# 2111.05754.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2111.05754.pdf
# Kích thước tệp: 373423 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Cắt tỉa một lần cho tất cả: Các mô hình ngôn ngữ tiền huấn luyện thưa thớt
Oﬁr Zafrir
Intel Labs, Israel
ofir.zafrir@intel.comAriel Larey
Intel Labs, Israel
ariel.larey@intel.comGuy Boudoukh
Intel Labs, Israel
guy.boudoukh@intel.com
Haihao Shen
Intel Corporation
haihao.shen@intel.comMoshe Wasserblat
Intel Labs, Israel
moshe.wasserblat@intel.com

Tóm tắt
Các mô hình ngôn ngữ dựa trên Transformer được áp dụng cho nhiều ứng dụng trong xử lý ngôn ngữ tự nhiên. Tuy nhiên, chúng không hiệu quả và khó triển khai. Trong những năm gần đây, nhiều thuật toán nén đã được đề xuất để tăng hiệu quả triển khai của các mô hình Transformer lớn trên phần cứng đích. Trong công trình này, chúng tôi trình bày một phương pháp mới để huấn luyện các mô hình ngôn ngữ Transformer tiền huấn luyện thưa thớt bằng cách tích hợp cắt tỉa trọng số và chưng cất mô hình. Những mô hình tiền huấn luyện thưa thớt này có thể được sử dụng để chuyển giao học tập cho nhiều tác vụ trong khi duy trì mô hình thưa thớt của chúng. Chúng tôi minh chứng phương pháp của mình với ba kiến trúc đã biết để tạo ra BERT-Base, BERT-Large và DistilBERT tiền huấn luyện thưa thớt. Chúng tôi cho thấy cách các mô hình tiền huấn luyện thưa thớt nén mà chúng tôi đã huấn luyện chuyển giao kiến thức của chúng đến năm tác vụ ngôn ngữ tự nhiên hạ nguồn khác nhau với mất mát độ chính xác tối thiểu. Hơn nữa, chúng tôi cho thấy cách nén thêm trọng số của các mô hình thưa thớt xuống độ chính xác 8bit bằng cách sử dụng huấn luyện nhận biết lượng tử hóa. Ví dụ, với BERT-Large tiền huấn luyện thưa thớt của chúng tôi được tinh chỉnh trên SQuADv1.1 và lượng tử hóa xuống 8bit, chúng tôi đạt được tỷ lệ nén 40X cho bộ mã hóa với mất mát độ chính xác dưới 1%. Theo hiểu biết của chúng tôi, kết quả của chúng tôi cho thấy tỷ lệ nén-độ chính xác tốt nhất cho BERT-Base, BERT-Large và DistilBERT.

1 Giới thiệu
Các mô hình ngôn ngữ tiền huấn luyện dựa trên Transformer (LM) như BERT [Devlin et al., 2019] và RoBERTa [Liu et al., 2019] đã trở thành phương pháp tiêu chuẩn cho nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP). Gần đây, chúng ta chứng kiến sự xuất hiện của các mô hình lớn hơn nhiều bậc độ lớn như GPT-2 [Radford et al., 2019], T-NLG [Rosset, 2020], GPT-3 [Brown et al., 2020], và Switch-C [Fedus et al., 2021]. Những mô hình này đã nâng cao kết quả tiên tiến trong một số tác vụ NLP như trả lời câu hỏi và phân loại văn bản. Tuy nhiên, xu hướng hướng tới các mô hình lớn hơn này đặt ra một số lo ngại. Khi tài nguyên tính toán và bộ nhớ cần thiết để chạy suy luận tăng theo kích thước mô hình, việc triển khai những mô hình này trong môi trường sản xuất và trên các thiết bị biên trở nên rất tốn kém và thách thức. Hơn nữa, những lượng lớn tài nguyên tính toán này gây ra chi phí môi trường cao [Strubell et al., 2019].

Nén mô hình của các LM lớn là một lĩnh vực nghiên cứu đang phát triển do những lo ngại này. Cắt tỉa trọng số là một phương pháp nén đã được chứng minh là rất hiệu quả trong việc giảm dấu chân bộ nhớ của mô hình [Han et al., 2015, Zhu and Gupta, 2018]. Tuy nhiên, cắt tỉa trọng số của các LM dựa trên Transformer lớn đến tỷ lệ thưa thớt cao đòi hỏi các phương pháp cắt tỉa chuyên biệt [Sanh et al.,

35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.arXiv:2111.05754v1  [cs.CL]  10 Nov 2021

--- TRANG 2 ---
2020, Chen et al., 2020, Gordon et al., 2020, Lagunas et al., 2021]. Hơn nữa, hầu hết các phương pháp cắt tỉa đòi hỏi các sửa đổi và điều chỉnh cụ thể cho tác vụ để tạo ra kết quả chất lượng.

Gordon et al. [2020] phát hiện rằng, về mặt độ chính xác, không quan trọng việc BERT được cắt tỉa trong giai đoạn tiền huấn luyện hay trong giai đoạn chuyển giao học tập. Điều này cho thấy rằng một LM có thể được cắt tỉa một lần trong quá trình tiền huấn luyện và sau đó được tinh chỉnh cho bất kỳ tác vụ hạ nguồn nào mà không cần điều chỉnh cụ thể cho tác vụ.

Trong bài báo này, chúng tôi trình bày một phương pháp mới, Prune Once for All (Prune OFA), tận dụng cắt tỉa trọng số và chưng cất mô hình để tạo ra các mô hình ngôn ngữ Transformer tiền huấn luyện với tỷ lệ thưa thớt cao. Chúng tôi áp dụng phương pháp của mình cho BERT-Base, BERT-Large và DistilBERT [Sanh et al., 2019] để tạo ra các mô hình tiền huấn luyện thưa thớt cho những kiến trúc mô hình này. Sau đó chúng tôi cho thấy cách những mô hình thưa thớt này có thể được tinh chỉnh để tạo ra các mô hình thưa thớt cụ thể cho tác vụ với mất mát độ chính xác tối thiểu cho SQuADv1.1 [Rajpurkar et al., 2016] cũng như cho bốn tác vụ từ GLUE Benchmark [Wang et al., 2018]. Chúng tôi cũng cho thấy rằng có thể nén thêm các mô hình bằng cách sử dụng huấn luyện nhận biết lượng tử hóa để đạt được kết quả tiên tiến về tỷ lệ nén-độ chính xác.

Những đóng góp chính của công trình này gồm ba phần: 1) Chúng tôi giới thiệu một phương pháp mới không phụ thuộc kiến trúc để huấn luyện các mô hình ngôn ngữ tiền huấn luyện thưa thớt. 2) Chúng tôi minh chứng cách tinh chỉnh những mô hình thưa thớt này trên các tác vụ hạ nguồn để tạo ra các mô hình thưa thớt và lượng tử hóa, loại bỏ gánh nặng cắt tỉa và điều chỉnh cho một tác vụ ngôn ngữ cụ thể. 3) Chúng tôi công bố thư viện nghiên cứu nén của chúng tôi với các script ví dụ để tái tạo công trình của chúng tôi cho các kiến trúc khác, cùng với các mô hình tiền huấn luyện thưa thớt của chúng tôi được trình bày trong bài báo này.

2 Công trình liên quan
Các mô hình ngôn ngữ lớn bị quá tham số hóa và khó triển khai. Do đó, vấn đề nén những mô hình này với mất mát độ chính xác tối thiểu cho các tác vụ hạ nguồn được khám phá rộng rãi.

Sanh et al. [2020] đề xuất phương pháp Movement Pruning được thiết kế đặc biệt cho chuyển giao học tập. Neural Magic triển khai Gradual Magnitude Pruning.¹ Cả hai phương pháp đều đề xuất cắt tỉa BERT-Base trong khi tinh chỉnh cho các tác vụ hạ nguồn kết hợp với chưng cất mô hình, và trình bày kết quả cho thấy độ thưa thớt 90% cho một số tác vụ. Tuy nhiên, cả hai phương pháp đều đòi hỏi thời gian tinh chỉnh dài cũng như điều chỉnh các siêu tham số liên quan đến cắt tỉa cho mỗi tác vụ. Mặt khác, phương pháp của chúng tôi không đòi hỏi điều chỉnh các siêu tham số cắt tỉa đặc biệt cho mỗi tác vụ vì chúng tôi cắt tỉa mô hình một lần cho tất cả các tác vụ. Hơn nữa, chúng tôi trình bày kết quả tốt hơn hoặc tương đương với ngân sách tính toán thấp hơn nhiều trong giai đoạn chuyển giao học tập. Gordon et al. [2020] khám phá tác động của cắt tỉa trọng số trong quá trình chuyển giao học tập và kết luận rằng cắt tỉa BERT-Base ở giai đoạn tiền huấn luyện không làm giảm hiệu suất của mô hình so với cắt tỉa ở giai đoạn tinh chỉnh. Chúng tôi cải thiện phương pháp được đề xuất và trình bày kết quả tốt hơn với tỷ lệ thưa thớt cao hơn nhiều. Chen et al. [2020] khám phá Giả thuyết Vé số [Frankle and Carbin, 2018] cho các mô hình tiền huấn luyện BERT. Cụ thể hơn, họ phân tích khả năng tìm kiếm vé trúng trong mô hình tiền huấn luyện BERT-Base có thể chuyển giao cho các tác vụ hạ nguồn khác. Các tác giả kết luận rằng vé trúng được tìm thấy trong khi tiền huấn luyện trên tác vụ Masked-LM, chuyển giao tốt cho các tác vụ hạ nguồn khác. Lagunas et al. [2021] trình bày một phương pháp cắt tỉa có cấu trúc, loại bỏ các hàng, cột và đầu attention, trong khi đạt được mất mát dưới 1% trong F1 cho kiến trúc BERT trên SQuADv1.1. Mishra et al. [2021] thực hiện cắt tỉa có cấu trúc 2:4 trên BERT trong khi tiếp tục tiền huấn luyện BERT; Phương pháp này tạo ra mô hình thưa thớt 50% có thể được tinh chỉnh mà không mất độ chính xác. Michel et al. [2019] khám phá tầm quan trọng của mỗi đầu trong cơ chế attention đa đầu của BERT và trình bày một phương pháp cắt tỉa các đầu attention cùng với các trọng số liên quan.

Các công trình khác đề xuất chưng cất kiến thức để nén các mô hình Transformer thành một đối tác dày đặc nhỏ hơn có thể được điều chỉnh cho các tác vụ hạ nguồn [Sanh et al., 2019, Jiao et al., 2020, Sun et al., 2020]. Lượng tử hóa các mô hình ngôn ngữ dựa trên Transformer cũng là một phương pháp nén được biết đến. Shen et al. [2020] đề xuất một phương pháp lượng tử hóa BERT với độ rộng bit khác nhau cho mỗi lớp. Các công trình khác triển khai huấn luyện nhận biết lượng tử hóa để lượng tử hóa BERT xuống 8bits [Kim et al., 2021, Zafrir et al., 2019]. Zhang et al. [2020] tạo ra một phương pháp tạo BERT trọng số ba ngôi. Kim and Hassan [2020] trình bày một pipeline nén cho các mô hình Transformer bao gồm chưng cất mô hình, lượng tử hóa và cắt tỉa đầu.

¹https://github.com/neuralmagic/sparseml/tree/main/integrations/huggingface-transformers

--- TRANG 3 ---
3 Cắt tỉa trọng số
Cắt tỉa trọng số là quá trình buộc một số trọng số của mạng neural về không. Cắt tỉa trọng số có thể là không có cấu trúc khi các trọng số riêng lẻ được cắt tỉa, hoặc có cấu trúc khi các nhóm trọng số có cấu trúc được cắt tỉa, ví dụ như khối, kênh, lớp. Cắt tỉa trọng số tạo ra các mạng neural thưa thớt giảm tính toán và dấu chân bộ nhớ của mô hình đã huấn luyện.

Trong bài báo này, chúng tôi tập trung vào cắt tỉa trọng số không có cấu trúc. Zhu and Gupta [2018] trình bày một phương pháp Gradual Magnitude Pruning (GMP) để dần dần cắt tỉa các trọng số có độ lớn thấp trong quá trình huấn luyện. Trong quá trình huấn luyện, mỗi f bước, các trọng số có độ lớn thấp nhất được cắt tỉa cho đến khi đạt tỷ lệ thưa thớt tạm thời st cho bước thời gian t, được định nghĩa bởi

st = sf + (si - sf) × (1 - (t - ts)/(te - ts))³                                    (1)

trong đó si và sf là tỷ lệ thưa thớt ban đầu và cuối cùng, và ts và te là các bước thời gian bắt đầu và kết thúc cắt tỉa.

Trong một bài báo gần đây, Renda et al. [2020] trình bày một thuật toán cắt tỉa dựa trên IMP (Iterative Magnitude Pruning) [Han et al., 2015] và Learning Rate Rewinding (LRR). IMP bao gồm hai bước: cắt tỉa một phần của mô hình và tiếp tục tinh chỉnh nó để phục hồi từ lỗi cắt tỉa được gây ra. Hai bước này được lặp lại cho đến khi đạt được tỷ lệ thưa thớt mong muốn. Trong LRR, bộ lập lịch tốc độ học được tua lại về trạng thái của nó trước bước cắt tỉa ở đầu bước tinh chỉnh. Chúng tôi đề xuất kết hợp nguyên tắc của learning rate rewinding vào GMP bằng cách tua lại bộ lập lịch tốc độ học về trạng thái của nó tại thời điểm ts mỗi f bước. Sau te, bộ lập lịch tiếp tục với cài đặt ban đầu cho đến khi kết thúc huấn luyện. Phụ lục C trực quan hóa cách LRR kết hợp với GMP thay đổi bộ lập lịch tốc độ học.

4 Chưng cất kiến thức
Chưng cất kiến thức, được giới thiệu bởi Hinton et al. [2015], là quá trình huấn luyện một mạng học sinh để tái tạo hành vi của một mô hình giáo viên. Khi chưng cất được sử dụng để khớp các dự đoán của mô hình giáo viên, mất mát cross-entropy mềm giữa xác suất mềm của học sinh và giáo viên được tính như sau:

Lkd = Σi ti log(si)                                                              (2)

trong đó si là xác suất mềm được ước tính bởi học sinh, và ti là xác suất mềm tương ứng được ước tính bởi giáo viên cho cùng một mẫu đầu vào. Các xác suất mềm được tính bằng hàm softmax với nhiệt độ T.

Thông thường, giáo viên là một mô hình lớn đạt hiệu suất cao, và học sinh dựa trên một kiến trúc nhỏ hơn. Trong bài báo này, chúng tôi đề xuất tận dụng phương pháp chưng cất mô hình cho quá trình cắt tỉa. Chúng tôi tập trung vào một cách tiếp cận trong đó cả giáo viên và học sinh chia sẻ cùng một kiến trúc, nhưng khác nhau về tỷ lệ thưa thớt. Trong trường hợp này, giáo viên là một mô hình dày đặc đã được huấn luyện trên một tác vụ mục tiêu, và học sinh là một mô hình với độ thưa thớt cố định hoặc đang trải qua cắt tỉa. Chưng cất-trong-quá-trình-cắt-tỉa có thể được áp dụng cho các mô hình ngôn ngữ trong cả giai đoạn tiền huấn luyện và tinh chỉnh. Trong giai đoạn tiền huấn luyện, giáo viên là một mô hình ngôn ngữ tiền huấn luyện, và trong giai đoạn tinh chỉnh, giáo viên là một mô hình ngôn ngữ được tinh chỉnh cho một tác vụ mục tiêu.

5 Prune Once for All
Khái niệm cắt tỉa các mô hình ngôn ngữ như BERT [Devlin et al., 2019] trong khi tiền huấn luyện đã được khám phá bởi Chen et al. [2020] và Gordon et al. [2020]. Tuy nhiên, tinh chỉnh mô hình thưa thớt cho một tác vụ ngôn ngữ cụ thể dẫn đến kết quả kém hoặc tỷ lệ thưa thớt thấp. Trong phần này, chúng tôi sẽ giới thiệu phương pháp mới của chúng tôi, Prune OFA, để tạo ra các mô hình ngôn ngữ tiền huấn luyện thưa thớt có thể được tinh chỉnh sau này cho các tác vụ hạ nguồn với mất mát độ chính xác tối thiểu ở tỷ lệ thưa thớt cao.

Một trực quan hóa về phương pháp của chúng tôi được trình bày trong Hình 1. Phương pháp này lấy một mô hình ngôn ngữ tiền huấn luyện làm đầu vào và đưa ra một mô hình ngôn ngữ thưa thớt có cùng kiến trúc. Phương pháp bao gồm hai bước, chuẩn bị giáo viên và cắt tỉa học sinh. Mô hình tiền huấn luyện thưa thớt mà chúng tôi huấn luyện là

--- TRANG 4 ---
Prune Once for All
Chuyển giao học tập  
[+ chưng cất]
Tập dữ liệu tiền huấn luyệnChuẩn bị  
giáo viên Mô hình ngôn ngữ  
tiền huấn luyệnCắt tỉa  
học sinh
Khởi tạoMô hình ngôn ngữ tiền huấn 
luyện được tinh chỉnhMô hình ngôn ngữ  
tiền huấn luyện thưa thớt
Chưng cất
Giáo viênMô hình thưa thớt
cuối cùng
Tập dữ liệu tác vụChưng cất
Giáo viên tác vụ Khóa mẫuHình 1: Phương pháp Prune OFA

mô hình chúng tôi sử dụng cho chuyển giao học tập trong khi duy trì mẫu thưa thớt của nó. Chúng tôi gọi phương pháp này là Prune Once for All vì chúng tôi cho thấy cách tinh chỉnh các mô hình tiền huấn luyện thưa thớt cho nhiều tác vụ ngôn ngữ trong khi chúng tôi chỉ cắt tỉa mô hình tiền huấn luyện một lần.

Chuẩn bị giáo viên Bước đầu tiên của Prune OFA là có được một mô hình được tối ưu hóa trên tập dữ liệu tiền huấn luyện cho một tác vụ tiền huấn luyện với mục tiêu LPT như được hiển thị trong Hình 1.² Cùng tập dữ liệu này sẽ được sử dụng để cắt tỉa học sinh trong bước tiếp theo. Mô hình này sẽ khởi tạo các mô hình học sinh và giáo viên trong bước cắt tỉa học sinh.

Cắt tỉa học sinh Một mô hình học sinh được khởi tạo từ giáo viên được chuẩn bị trong bước chuẩn bị giáo viên. Sau đó học sinh được tinh chỉnh trên một tổ hợp tuyến tính của tác vụ tiền huấn luyện, từ bước chuẩn bị giáo viên, và mục tiêu chưng cất kiến thức Lkd:

L = αPT LPT + αkd Lkd                                                           (3)

trong khi được cắt tỉa bằng các phương pháp GMP + LRR. Mô hình đầu ra của quá trình này là một LM tiền huấn luyện thưa thớt có thể được sử dụng mà không cần cắt tỉa bổ sung cho chuyển giao học tập để tạo ra các mô hình thưa thớt cho một tác vụ hạ nguồn cụ thể.

Khóa-mẫu Chúng tôi muốn giữ mẫu thưa thớt của mô hình tiền huấn luyện thưa thớt được tạo bởi Prune OFA tại chỗ trong quá trình tinh chỉnh. Chúng tôi đề xuất một phương pháp gọi là khóa-mẫu ngăn không cho các số không được tìm thấy trong mô hình thay đổi trong khi huấn luyện mô hình. Khóa-mẫu được mô tả chi tiết hơn trong Phụ lục B.

6 Thiết lập thí nghiệm
Tập dữ liệu Chúng tôi sử dụng tập dữ liệu Wikipedia tiếng Anh (2500M từ) để huấn luyện các mô hình trên tác vụ tiền huấn luyện. Chúng tôi chia dữ liệu thành tập huấn luyện (~95%) và tập xác thực (~5%). Cả hai tập đều được tiền xử lý như được mô tả trong các bài báo gốc của mô hình [Devlin et al., 2019, Sanh et al., 2019]. Chúng tôi xử lý dữ liệu để sử dụng độ dài chuỗi tối đa được cho phép bởi các mô hình, tuy nhiên, chúng tôi cho phép các chuỗi ngắn hơn với xác suất 0.1. Chúng tôi đánh giá các mô hình tiền huấn luyện thưa thớt của mình trên một số benchmark phổ biến cho chuyển giao học tập; một tác vụ trả lời câu hỏi, SQuADv1.1 chứa 89K ví dụ huấn luyện [Rajpurkar et al., 2016], và các tác vụ phân loại văn bản sau từ GLUE Benchmark: MNLI, QQP, QNLI và SST-2 chứa 393K, 364K, 105K, và 67K ví dụ huấn luyện tương ứng [Wang et al., 2018, Williams et al., 2018, Iyer et al., 2017, Socher et al., 2013].

Áp dụng Prune Once for All Chúng tôi giới thiệu phương pháp của mình bằng cách áp dụng Prune OFA trên ba kiến trúc khác nhau có kích thước khác nhau; BERT-Base, BERT-Large và DistilBERT. Vì chúng tôi không có dữ liệu huấn luyện được xử lý ban đầu được sử dụng để huấn luyện BERT-Base, BERT-Large và DistilBERT, chúng tôi chạy một bước bổ sung để tinh chỉnh các mô hình tiền huấn luyện bằng dữ liệu huấn luyện được xử lý mà chúng tôi chuẩn bị. Tiếp theo, chúng tôi thực hiện bước cắt tỉa học sinh để có được các mô hình tiền huấn luyện thưa thớt của chúng tôi. Chúng tôi cắt tỉa BERT-Base và DistilBERT đến tỷ lệ thưa thớt {85%; 90%} và BERT-Large đến tỷ lệ thưa thớt 90%. Cắt tỉa được áp dụng cho tất cả các lớp Linear trong bộ mã hóa Transformer bao gồm lớp pooler nếu nó tồn tại. Các siêu tham số chính xác và chi tiết bổ sung được tóm tắt trong Phụ lục E

²Ví dụ, tác vụ tiền huấn luyện cho BERT-Base là mô hình hóa ngôn ngữ có mặt nạ kết hợp với dự đoán câu tiếp theo.

--- TRANG 5 ---
Bảng 1: Kết quả Prune OFA BERT-Base so sánh với các phương pháp cắt tỉa khác
Mô hình ThưaThớtChuyển giao
với KDSQUAD MNLI (m/mm) SST-2 QNLI QQP
EM F1 Acc Acc Acc Acc Acc F1
Tham chiếu 0% 80.80 88.50 84.06 84.51 92.13 91.16 91.20 88.13
Chen et al. [2020] 70% N/A 86.54 82.59 N/A 91.86 89.44 90.03 N/A
Gordon et al. [2020] 80% N/A N/A 75.90 N/A 88.10 85.30 86.90 N/A
Prune OFA 85% 78.59 86.63 81.67 82.53 91.34 89.95 90.69 87.41
Cắt tỉa tinh chỉnh85%+ 78.00 86.16 82.45 83.05 88.82 87.79 90.87 87.65
Prune OFA + 81.10 88.42 82.71 83.67 91.46 90.34 91.15 88.00
Prune OFA +QAT 85% + 80.84 88.24 81.40 82.51 91.46 89.76 91.09 88.01
Neural Magic³
90%+ 79.40 87.20 N/A N/A N/A N/A N/A N/A
Sanh et al. [2020] + 76.60 84.90 81.20 81.80 N/A N/A 90.20 86.80
Prune OFA + 79.83 87.25 81.45 82.43 90.88 89.07 90.93 87.72

Bảng 2: Kết quả Prune OFA BERT-Large
Mô hình ThưaThớtSQuAD MNLI (m/mm) SST-2 QNLI QQP
EM F1 Acc Acc Acc Acc Acc F1
Tham chiếu 0% 83.99 90.93 86.39 86.58 93.54 92.42 91.59 88.67
Prune OFA 90% 83.35 90.20 83.74 84.20 92.95 91.39 91.48 88.43
Prune OFA + QAT 90% 83.22 90.02 83.47 84.08 92.72 91.45 91.41 88.36

Chuyển giao học tập Sau khi tạo ra các mô hình tiền huấn luyện thưa thớt của chúng tôi, chúng tôi tinh chỉnh chúng cho các tác vụ NLP sau: SQuADv1.1, QNLI, MNLI, SST-2 và QQP. Chúng tôi sử dụng các siêu tham số mặc định cho mỗi tác vụ và thực hiện tìm kiếm lưới cho tốc độ học, weight decay, tỷ lệ khởi động và số epoch huấn luyện. Đối với mỗi tác vụ, chúng tôi báo cáo trung bình của hai lần chạy khác nhau với các seed khác nhau đạt được kết quả tốt nhất trên tập phát triển của tác vụ. Chúng tôi cải thiện thêm kết quả của các mô hình thưa thớt bằng cách tích hợp chưng cất kiến thức. Đối với mỗi tác vụ và mô hình, chúng tôi tạo một giáo viên tác vụ dựa trên mô hình tiền huấn luyện dày đặc ban đầu được tinh chỉnh cho tác vụ. Đối với SQuADv1.1 và QQP, chúng tôi báo cáo kết quả tối đa hóa F1, và đối với MNLI, chúng tôi báo cáo kết quả tối đa hóa độ chính xác không khớp. Đối với các siêu tham số chính xác và chi tiết bổ sung, xem Phụ lục E.

So sánh với cắt tỉa tinh chỉnh Chúng tôi so sánh phương pháp Prune OFA của chúng tôi với cắt tỉa tinh chỉnh trong đó chúng tôi cắt tỉa mô hình tiền huấn luyện dày đặc trong quá trình tinh chỉnh cho một tác vụ hạ nguồn. Cho mục đích đó, chúng tôi triển khai cắt tỉa GMP kết hợp với chưng cất kiến thức và chạy thí nghiệm sử dụng cùng giáo viên và siêu tham số được sử dụng trong các thí nghiệm chuyển giao học tập Prune OFA.

Lượng tử hóa Chúng tôi triển khai huấn luyện nhận biết lượng tử hóa tương tự Q8BERT [Zafrir et al., 2019]. Để biết chi tiết về sự khác biệt giữa phương pháp của chúng tôi và Q8BERT, xem Phụ lục D. Đối với mỗi tác vụ, chúng tôi chọn mô hình hoạt động tốt nhất cho tác vụ này và thực hiện huấn luyện nhận biết lượng tử hóa trên nó. Chúng tôi sử dụng các siêu tham số hơi khác nhau cho phiên huấn luyện này như được mô tả trong Phụ lục E.2. Chúng tôi báo cáo trung bình của hai lần chạy khác nhau với các seed khác nhau đạt được kết quả tốt nhất.

7 Kết quả
Trong Bảng 1, chúng tôi trình bày các kết quả thí nghiệm của mình cho việc cắt tỉa BERT-Base đến tỷ lệ thưa thớt 85% và 90% bằng Prune OFA. Chúng tôi cũng trình bày kết quả của các phương pháp cắt tỉa khác được áp dụng cho BERT-Base cũng như kết quả của các thí nghiệm cắt tỉa tinh chỉnh mà chúng tôi thực hiện. Các kết quả không được đánh dấu trong cột Chuyển giao với KD không sử dụng chưng cất mô hình trong giai đoạn chuyển giao học tập. Kết quả tốt nhất trong mỗi danh mục được đánh dấu bằng chữ đậm. Chúng tôi quan sát thấy rằng phương pháp của chúng tôi đạt được kết quả tốt hơn so với các công trình cắt tỉa trước đây khác trong khi tiền huấn luyện với tỷ lệ thưa thớt cao hơn. Khi so sánh phương pháp Prune OFA của chúng tôi với các phương pháp cắt tỉa tinh chỉnh khác, chúng tôi quan sát thấy rằng phương pháp của chúng tôi tạo ra kết quả tốt nhất ở tỷ lệ thưa thớt 85% và 90%. Hơn nữa, chúng tôi cho thấy sự suy giảm độ chính xác thấp hơn

³Kết quả lấy từ kho mô hình thưa thớt của Neural Magic: https://sparsezoo.neuralmagic.com/

--- TRANG 6 ---
Bảng 3: Kết quả Prune OFA DistilBERT so sánh với cắt tỉa tinh chỉnh
Mô hình ThưaThớtSQuAD MNLI (m/mm) SST-2 QNLI QQP
EM F1 Acc Acc Acc Acc Acc F1
Tham chiếu 0% 77.70 85.80 82.20 N/A 91.30 89.20 N/A 88.50
Cắt tỉa tinh chỉnh85%76.16 84.55 81.22 81.92 88.88 86.60 90.18 86.80
Prune OFA 78.10 85.82 81.35 82.03 90.60 88.31 90.29 86.97
Prune OFA +QAT 85% 77.03 85.13 80.66 81.14 88.93 87.97 90.22 86.92
Cắt tỉa tinh chỉnh90%74.63 83.42 80.47 81.32 88.25 84.91 89.97 86.57
Prune OFA 76.91 84.82 80.68 81.47 90.02 87.66 90.05 86.67
Prune OFA +QAT 90% 75.62 83.87 78.80 80.40 88.47 87.20 89.97 86.63

1% so với kết quả của mô hình tiền huấn luyện dày đặc ở độ thưa thớt 85% với ngoại lệ của benchmark MNLI-matched. Lưu ý rằng đối với MNLI, các kết quả được báo cáo được chọn dựa trên độ chính xác không khớp tốt nhất của mô hình được tìm thấy trong tìm kiếm lưới của chúng tôi; khi tìm kiếm kết quả khớp tốt nhất, chúng tôi giảm khoảng cách độ chính xác xuống mất mát độ chính xác 1% với chi phí tăng mất mát độ chính xác cho không khớp: 83.09/83.36 (m/mm).

Kết quả cho việc cắt tỉa BERT-Large đến tỷ lệ thưa thớt 90% được trình bày trong Bảng 2. Những kết quả này nằm trong phạm vi mất mát độ chính xác 1% cho tất cả các tác vụ trừ tác vụ MNLI. Chúng tôi kết luận rằng mô hình BERT-Large thưa thớt 90% (~30.2M tham số khác không) mà chúng tôi huấn luyện có độ chính xác tốt hơn so với BERT-Base dày đặc (~85M tham số khác không).

Kết quả của chúng tôi cho việc cắt tỉa DistilBERT đến tỷ lệ thưa thớt 85% và 90% được trình bày trong Bảng 3 với kết quả của chúng tôi cho các thí nghiệm cắt tỉa tinh chỉnh mà chúng tôi thực hiện. Ở cả hai tỷ lệ thưa thớt, phương pháp của chúng tôi tạo ra kết quả độ chính xác tốt hơn so với cắt tỉa tinh chỉnh (kết quả tốt nhất trong mỗi danh mục được đánh dấu bằng chữ đậm). Hơn nữa, ở tỷ lệ thưa thớt 85%, kết quả của chúng tôi nằm trong phạm vi mất mát độ chính xác tương đối 1% trong tất cả các tác vụ trừ QQP.

Bảng 1, 2 và 3 trình bày kết quả lượng tử hóa, được chỉ định với hậu tố +QAT. Áp dụng huấn luyện nhận biết lượng tử hóa trên các mô hình thưa thớt kết quả của chúng tôi làm giảm độ chính xác của mô hình thêm trung bình 0.67% so với độ chính xác của mô hình độ chính xác đầy đủ. Kết quả cho mô hình thưa thớt 85% +QAT tốt hơn so với mô hình thưa thớt 90% với độ chính xác đầy đủ trong tất cả các tác vụ cho BERT-Base và trong 3/5 tác vụ cho DistilBERT. Hơn nữa, mô hình thưa thớt và lượng tử hóa 85% nhỏ hơn mô hình thưa thớt 90% theo hệ số 0.375.

Một nghiên cứu loại bỏ đã được thực hiện để kiểm tra cách mỗi thành phần của phương pháp Prune OFA ảnh hưởng đến khả năng của mô hình tiền huấn luyện chuyển giao kiến thức của nó đến các tác vụ hạ nguồn, như được mô tả trong Phụ lục A.

8 Kết luận và công việc tương lai
Chúng tôi đã giới thiệu Prune OFA, một phương pháp không phụ thuộc kiến trúc để tạo ra các mô hình ngôn ngữ tiền huấn luyện thưa thớt. Chúng tôi cũng cho thấy cách những mô hình thưa thớt này có thể được sử dụng để có được các mô hình thưa thớt được tinh chỉnh mà không có gánh nặng cắt tỉa cụ thể cho tác vụ. Kết quả của chúng tôi cho thấy rằng việc sử dụng những mô hình tiền huấn luyện thưa thớt này cho chuyển giao học tập tạo ra kết quả với sự suy giảm hiệu suất tối thiểu so với đối tác dày đặc của chúng cho nhiều tác vụ NLP khác nhau. Chúng tôi tiếp tục chứng minh rằng việc tích hợp lượng tử hóa có thể dẫn đến các mô hình thưa thớt và lượng tử hóa hiệu quả hơn với chi phí nhỏ cho độ chính xác của mô hình.

Một hướng có thể cho nghiên cứu tương lai là khám phá liệu một mô hình tiền huấn luyện lớn và thưa thớt có tốt hơn trong việc nắm bắt và chuyển giao kiến thức ngôn ngữ tự nhiên so với một mô hình dày đặc nhỏ hơn có cùng kiến trúc với số lượng tham số khác không tương tự hay không.

Chúng tôi hy vọng rằng việc phát hành mã và các mô hình tiền huấn luyện thưa thớt của chúng tôi cho cộng đồng sẽ giúp phát triển các mô hình hiệu quả hơn.

--- TRANG 7 ---
9 Lời cảm ơn
Chúng tôi biết ơn Ella Charlaix của HuggingFace về những bình luận và sửa chữa hữu ích của cô ấy.

Tài liệu tham khảo
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165, 2020.

T. Chen, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, and M. Carbin. The lottery ticket hypothesis
for pre-trained bert networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems, volume 33, pages 15834–15846.
Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
b6af2c9703f203a2794be03d443af2e3-Paper.pdf.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers), pages 4171–4186, 2019.

W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with
simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.

J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In
International Conference on Learning Representations, 2018.

M. Gordon, K. Duh, and N. Andrews. Compressing bert: Studying the effects of weight pruning on
transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, pages
143–155, 2020.

S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both weights and connections for efficient
neural network. In NIPS, pages 1135–1143, 2015. URL http://papers.nips.cc/paper/
5784-learning-both-weights-and-connections-for-efficient-neural-network.

G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS Deep
Learning and Representation Learning Workshop, 2015. URL http://arxiv.org/abs/1503.
02531.

S. Iyer, N. Dandekar, K. Csernai, et al. First quora dataset release: Question pairs. data. quora. com,
2017.

X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. Tinybert: Distilling bert for
natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings, pages 4163–4174, 2020.

S. Kim, A. Gholami, Z. Yao, M. W. Mahoney, and K. Keutzer. I-bert: Integer-only bert quantization.
ICML, 2021.

Y. J. Kim and H. Hassan. Fastformers: Highly efficient transformer models for natural language
understanding. In Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language
Processing, pages 149–158, 2020.

D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR (Poster), 2015.

F. Lagunas, E. Charlaix, V. Sanh, and A. M. Rush. Block pruning for faster transformers, 2021.

Q. Lhoest, A. V. del Moral, P. von Platen, T. Wolf, Y. Jernite, A. Thakur, L. Tunstall, S. Patil,
M. Drame, J. Chaumond, J. Plu, J. Davison, S. Brandeis, T. L. Scao, V. Sanh, K. C. Xu, N. Patry,
A. McMillan-Major, P. Schmid, S. Gugger, S. Liu, S. Lesage, L. Debut, T. Matussière, C. Delangue,
and S. Bekman. huggingface/datasets: 1.11.0, July 2021. URL https://doi.org/10.5281/
zenodo.5148649.

--- TRANG 8 ---
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoy-
anov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,
2019.

P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? Advances in Neural
Information Processing Systems, 32:14014–14024, 2019.

A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu, and P. Micikevicius.
Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021.

A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems, 32:8026–8037, 2019.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised
multitask learners. 2019.

P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100, 000+ questions for machine compre-
hension of text. In EMNLP, 2016.

A. Renda, J. Frankle, and M. Carbin. Comparing rewinding and fine-tuning in neural network pruning.
ICLR, 2020.

C. Rosset. Turing-nlg: A 17-billion-parameter language model by mi-
crosoft. 2020. URL https://www.microsoft.com/en-us/research/blog/
turing-nlg-a-17-billion-parameter-language-model-by-microsoft/.

V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster,
cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

V. Sanh, T. Wolf, and A. Rush. Movement pruning: Adaptive sparsity by fine-tuning. Advances in
Neural Information Processing Systems, 33, 2020.

S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer. Q-bert:
Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pages 8815–8821, 2020.

R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep
models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013
conference on empirical methods in natural language processing, pages 1631–1642, 2013.

E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in
nlp. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
pages 3645–3650, 2019.

Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou. Mobilebert: a compact task-agnostic bert
for resource-limited devices. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 2158–2170, 2020.

A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Glue: A multi-task benchmark
and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP
Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355,
2018.

A. Williams, N. Nangia, and S. R. Bowman. A broad-coverage challenge corpus for sentence
understanding through inference. In 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2018,
pages 1112–1122. Association for Computational Linguistics (ACL), 2018.

T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Fun-
towicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,
M. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing.
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-
ing: System Demonstrations, pages 38–45, Online, Oct. 2020. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.

--- TRANG 9 ---
Bảng 4: Kết quả nghiên cứu loại bỏ BERT-Base thưa thớt 85% Prune OFA
Chuẩn bị
giáo viênLRRChưng cất
tiền huấn luyệnChưng cất
chuyển giaơSQuAD MNLI (m/mm)
EM F1 Acc Acc
78.11 86.13 81.14 81.74
+ 78.00 86.31 81.22 82.01
+ + 78.41 86.51 81.39 82.01
+ + 78.30 86.41 81.57 82.13
+ + + 78.59 86.63 81.67 82.53
+ + 80.77 88.08 82.20 82.83
+ + + + 81.10 88.42 82.71 83.67

O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth
Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition
(EMC2-NIPS), pages 36–39, 2019. doi: 10.1109/EMC2-NIPS53020.2019.00016.

W. Zhang, L. Hou, Y. Yin, L. Shang, X. Chen, X. Jiang, and Q. Liu. Ternarybert: Distillation-aware
ultra-low bit bert. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 509–521, 2020.

M. Zhu and S. Gupta. To prune, or not to prune: exploring the efficacy of pruning for model
compression. ICLR, 2018.

A Nghiên cứu loại bỏ
Trong phần này, chúng tôi phân tích cách mỗi bước của phương pháp Prune OFA ảnh hưởng đến kết quả cuối cùng. Chúng tôi so sánh các mô hình theo cách tương tự như trong Phần 7, bằng cách so sánh các kết quả khác nhau của các mô hình tiền huấn luyện thưa thớt trên các tác vụ hạ nguồn. Trong nghiên cứu loại bỏ, chúng tôi tập trung vào BERT-Base được cắt tỉa đến 85% được tinh chỉnh cho SQuADv1.1 và MNLI. Tất cả các kết quả từ nghiên cứu loại bỏ được trình bày trong Bảng 4.

Chuẩn bị giáo viên Bước chuẩn bị giáo viên chỉ được thực hiện trong trường hợp dữ liệu huấn luyện được xử lý ban đầu của mô hình tiền huấn luyện không có sẵn. Vì mục tiêu của chúng tôi là cắt tỉa mô hình, luôn luôn tốt hơn khi bắt đầu từ một mô hình được tối ưu hóa tốt hơn cho dữ liệu được sử dụng để cắt tỉa, do đó bước chuẩn bị giáo viên. Để đo lường tác động của bước chuẩn bị giáo viên, chúng tôi cắt tỉa hai mô hình, một mô hình sử dụng mô hình tiền huấn luyện BERT-Base làm khởi tạo, và một mô hình sử dụng đầu ra của bước chuẩn bị giáo viên làm khởi tạo. Sau đó, chúng tôi tinh chỉnh cả hai cho các tác vụ SQuADv1.1 và MNLI và so sánh kết quả của chúng. Chúng tôi thấy cải thiện đáng chú ý khi thực hiện với bước chuẩn bị giáo viên trong cả hai tác vụ.

Cắt tỉa học sinh Chúng tôi so sánh kết quả của một mô hình được cắt tỉa với LRR với một mô hình được cắt tỉa mà không có LRR, có nghĩa là lịch trình tốc độ học vẫn là lịch trình giảm tuyến tính mặc định với lịch trình khởi động. Đối với SQuADv1.1, chúng tôi quan sát thấy cải thiện đáng kể trong cả hai benchmark. Tuy nhiên, trong trường hợp MNLI, chúng tôi không thấy bất kỳ cải thiện nào trong độ chính xác không khớp mà chúng tôi cố gắng tối đa hóa, nhưng có cải thiện đáng kể trong độ chính xác khớp. Chúng tôi quan sát thấy rằng việc áp dụng chưng cất kiến thức trong bước cắt tỉa học sinh cải thiện kết quả của cả hai tác vụ. Chưng cất kiến thức có vẻ ít quan trọng hơn trong trường hợp SQuADv1.1 và quan trọng hơn trong trường hợp MNLI. Ngoài ra, chúng tôi thấy rằng việc kết hợp LRR và chưng cất kiến thức đạt được kết quả tốt hơn so với việc sử dụng riêng lẻ từng phương pháp. Chúng tôi kết luận rằng việc áp dụng LRR trong khi cắt tỉa cải thiện kết quả tinh chỉnh và do đó là một phần quan trọng của thuật toán của chúng tôi.

Chuyển giao học tập với chưng cất kiến thức Chúng tôi thấy rằng việc sử dụng chưng cất kiến thức trong khi tinh chỉnh cho các tác vụ hạ nguồn cải thiện kết quả đáng kể. Chúng tôi kiểm tra xem phương pháp của chúng tôi có vẫn cải thiện kết quả độ chính xác của các mô hình thưa thớt khi được tinh chỉnh với chưng cất mô hình hay không. Từ các kết quả ở cuối Bảng 4, chúng tôi suy ra rằng phương pháp của chúng tôi trực giao với chưng cất kiến thức trong khi tinh chỉnh và cải thiện thêm kết quả độ chính xác của cả hai tác vụ.

--- TRANG 10 ---
[Hình 2: Lịch trình tốc độ học và độ thưa thớt. Cả hai hình đều hiển thị một bộ lập lịch tốc độ học giảm tuyến tính với các bước khởi động twu so với một bộ lập lịch độ thưa thớt được định nghĩa bởi Phương trình 1. (a) lịch trình học tập không có tua lại. (b) lịch trình học tập có tua lại]

B Chi tiết khóa-mẫu
Sau đây là mô tả chi tiết về phương pháp Khóa-mẫu được sử dụng khi tinh chỉnh các mô hình tiền huấn luyện thưa thớt của chúng tôi. Trước khi huấn luyện, phương pháp Khóa-mẫu khởi tạo một mặt nạ Ml cho mỗi lớp thưa thớt l với trọng số Wl, đại diện cho mẫu thưa thớt của lớp.

Ml_uv = {1 nếu Wl_uv ≠ 0
         {0 nếu Wl_uv = 0                                                    (4)

Sau đó, trong khi huấn luyện, gradient mất mát L đối với các trọng số được điều chỉnh thành

∂L/∂Wl_uv = {∂L/∂Wl_uv nếu Ml_uv = 1
             {0 nếu Ml_uv = 0                                                (5)

đảm bảo rằng một trọng số ban đầu bằng 0 sẽ vẫn bằng 0 trong suốt quá trình tinh chỉnh.

C Trực quan hóa Tua lại Tốc độ Học với Cắt tỉa Độ lớn Dần dần
Hình 2 minh chứng cách một bộ lập lịch tốc độ học giảm tuyến tính với khởi động được điều chỉnh với LRR so với cùng bộ lập lịch không có LRR.

D Sự khác biệt về phương pháp lượng tử hóa so với Q8BERT
Chúng tôi đã triển khai phiên bản huấn luyện nhận biết lượng tử hóa của riêng mình tương tự Q8BERT với những khác biệt sau: 1) Các activation được lượng tử hóa bằng lượng tử hóa bất đối xứng thay vì lượng tử hóa đối xứng. 2) Các vector embedding không được lượng tử hóa và được biểu diễn ở độ chính xác đầy đủ. 3) Các mô hình được lượng tử hóa sau khi tinh chỉnh cho một tác vụ hạ nguồn trong một phiên học tập riêng biệt.

E Khả năng tái tạo
E.1 Triển khai
Phương pháp Prune OFA, GMP, chưng cất mô hình và huấn luyện nhận biết lượng tử hóa của chúng tôi được triển khai trong Gói Nghiên cứu Nén Mô hình của chúng tôi bằng PyTorch [Paszke et al., 2019].⁴ Thư viện của chúng tôi cung cấp một số phương pháp cắt tỉa không phụ thuộc kiến trúc và các phương pháp nén khác có thể được cắm vào bất kỳ phiên huấn luyện nào chỉ với vài dòng mã. Chúng tôi mời cộng đồng nghiên cứu sử dụng thư viện của chúng tôi để đẩy nhanh nghiên cứu của họ trong cắt tỉa và nén mạng neural.

⁴https://github.com/IntelLabs/Model-Compression-Research-Package

--- TRANG 11 ---
Bảng 5: Siêu tham số được sử dụng với Prune OFA
Siêu tham số Giá trị
Tỷ lệ khởi động 0.01
Kích thước batch 256
Weight decay 0.01
Bước tối đa 100k
Giảm tốc độ học Tuyến tính + LRR
Độ dài chuỗi 512
αPT 0.5
αkd 0.5
Nhiệt độ 2.0
Bắt đầu cắt tỉa 0
Kết thúc chính sách cắt tỉa 50k
Kết thúc cắt tỉa 80k
Khoảng cắt tỉa 1k

Bảng 6: Siêu tham số được sử dụng cho chuyển giao học tập
Siêu tham số SQuAD GLUE
Tốc độ học {1.5e-4, 1.8e-4} {1e-4, 1.2e-4, 1.5e-5}
Kích thước batch 12 32
Weight decay {0, 0.01}
Epoch {8} {3, 6, 9}
Giảm tốc độ học Tuyến tính
Tỷ lệ khởi động {0, 0.01, 0.1}
Độ dài chuỗi 384 128
αPT 0.0
αkd 1.0
Nhiệt độ 2.0

Chúng tôi sử dụng thư viện HuggingFace/transformers và các script ví dụ có sẵn để huấn luyện các mô hình dựa trên Transformer của chúng tôi [Wolf et al., 2020]. Chúng tôi đã sửa đổi các script ví dụ để bao gồm các phương pháp của chúng tôi và cung cấp chúng trong các ví dụ của thư viện.

Tất cả các tập dữ liệu được đề cập trong bài báo được tải xuống và xử lý bằng thư viện HuggingFace/datasets [Lhoest et al., 2021].

E.2 Chi tiết huấn luyện & siêu tham số
Chuẩn bị giáo viên Chúng tôi thực hiện bước chuẩn bị giáo viên trên tất cả các mô hình. Các mục tiêu tiền huấn luyện cho cả các mô hình BERT và DistilBERT giống như trong bài báo gốc. Đối với các mô hình BERT, các mục tiêu là mô hình hóa ngôn ngữ có mặt nạ (MLM) và dự đoán câu tiếp theo (NSP), và đối với DistilBERT mục tiêu chỉ là MLM. Các siêu tham số được sử dụng được chi tiết trong Bảng 5. Chúng tôi sử dụng bộ tối ưu Adam [Kingma and Ba, 2015] với tốc độ học {5e-5, 1e-4, 1e-4} cho {BERT-Base, BERT-Large, DistilBERT}.

Cắt tỉa học sinh Chúng tôi chạy cắt tỉa học sinh với cùng các mục tiêu, siêu tham số và bộ tối ưu mà chúng tôi sử dụng ở bước chuẩn bị giáo viên (Bảng 5) với tốc độ học {1.5e-4, 1e-4, 1.5e-4} cho {BERT-Base, BERT-Large, DistilBERT}.

Chuyển giao học tập Đối với các thí nghiệm chuyển giao học tập của Prune OFA hoặc cắt tỉa tinh chỉnh, chúng tôi sử dụng các siêu tham số trong Bảng 6 kết hợp với bộ tối ưu Adam. Khi kết hợp chưng cất kiến thức trong giai đoạn chuyển giao học tập, trong các thí nghiệm của chúng tôi, chúng tôi phát hiện rằng tốt nhất là chỉ tối ưu hóa mục tiêu chưng cất kiến thức và bỏ qua các nhãn sự thật cơ bản.

--- TRANG 12 ---
Bảng 7: Siêu tham số được sử dụng cho huấn luyện nhận biết lượng tử hóa
Siêu tham số SQuAD GLUE
Tốc độ học {1e-6, 1e-5} {5e-8, 1e-7, 1e-6, 1e-5}
Kích thước batch 12 32
Weight decay {0, 0.01}
Epoch 2 3
Giảm tốc độ học Tuyến tính
Tỷ lệ khởi động {0, 0.01, 0.1}
Độ dài chuỗi 384 128
αPT 0.0
αkd 1.0
Nhiệt độ 2.0

Lượng tử hóa Đối với các thí nghiệm huấn luyện nhận biết lượng tử hóa của Prune OFA, chúng tôi sử dụng các siêu tham số trong Bảng 7 kết hợp với bộ tối ưu Adam.
