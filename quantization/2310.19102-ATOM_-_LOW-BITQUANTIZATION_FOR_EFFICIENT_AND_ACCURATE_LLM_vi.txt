# 2310.19102.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2310.19102.pdf
# Kích thước tệp: 793583 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
ATOM : LƯỢNG TỬ HÓA BIT THẤP CHO VIỆC PHỤC VỤ LLM HIỆU QUẢ VÀ CHÍNH XÁC

Yilong Zhao1 2 *Chien-Yu Lin2Kan Zhu2Zihao Ye2Lequn Chen2Size Zheng2 3 *
Luis Ceze2 4Arvind Krishnamurthy2Tianqi Chen4 5Baris Kasikci2

TÓM TẮT
Nhu cầu ngày càng tăng đối với các Mô hình Ngôn ngữ Lớn (LLM) trong các ứng dụng như tạo nội dung, chatbot thông minh và phân tích cảm xúc đặt ra những thách thức đáng kể cho các nhà cung cấp dịch vụ LLM. Để sử dụng hiệu quả tài nguyên GPU và tăng thông lượng, việc xử lý theo lô nhiều yêu cầu đã nổi lên như một mô hình phổ biến; để tăng tốc hơn nữa việc xử lý theo lô, các kỹ thuật lượng tử hóa LLM giảm tiêu thụ bộ nhớ và tăng khả năng tính toán. Tuy nhiên, các sơ đồ lượng tử hóa phổ biến (ví dụ: lượng tử hóa trọng số-kích hoạt 8-bit) không thể tận dụng đầy đủ khả năng của GPU hiện đại, như các toán tử số nguyên 4-bit, dẫn đến hiệu suất dưới mức tối ưu.

Để tối đa hóa thông lượng phục vụ của LLM, chúng tôi giới thiệu Atom, một phương pháp lượng tử hóa bit thấp đạt được cải thiện thông lượng cao với tổn thất độ chính xác không đáng kể. Atom tăng đáng kể thông lượng phục vụ bằng cách sử dụng các toán tử bit thấp và giảm đáng kể tiêu thụ bộ nhớ thông qua lượng tử hóa bit thấp. Nó đạt được độ chính xác cao bằng cách áp dụng một quy trình lượng tử hóa độ chính xác hỗn hợp và chi tiết mới. Chúng tôi đánh giá Atom trên lượng tử hóa trọng số-kích hoạt 4-bit trong bối cảnh phục vụ. Atom cải thiện thông lượng đầu cuối (token/s) lên đến 7.73× so với FP16 và 2.53× so với lượng tử hóa INT8, trong khi duy trì cùng mục tiêu độ trễ.

1 GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) ngày càng được tích hợp vào quy trình làm việc và cuộc sống hàng ngày của chúng ta, nơi chúng ta sử dụng chúng để tóm tắt, hoàn thành mã và ra quyết định. Các nghiên cứu báo cáo rằng ChatGPT có hơn 100 triệu người dùng, với hơn 1 tỷ lượt truy cập trang web mỗi tháng (Duarte, 2023). Hơn nữa, quy mô và khả năng của LLM tiếp tục phát triển để đáp ứng một loạt rộng hơn các nhiệm vụ. Nhu cầu suy luận cao và độ phức tạp của mô hình đã tăng đáng kể chi phí vận hành, tức là tính toán/bộ nhớ và năng lượng, cho các nhà cung cấp dịch vụ LLM lên gần 1 triệu đô la mỗi ngày (Elimian, 2023).

Không có gì ngạc nhiên, việc tối ưu hóa phục vụ LLM đang trở thành một mối quan tâm cấp bách. Hầu hết các nỗ lực đã tập trung vào việc cải thiện thông lượng phục vụ LLM, điều này thường được đạt được bằng cách xử lý theo lô các yêu cầu từ nhiều người dùng khác nhau (Yu et al., 2022; Chen, 2023; Kwon et al., 2023). Việc xử lý theo lô nhiều yêu cầu tăng cường độ tính toán và phân bổ chi phí tải ma trận trọng số, do đó cải thiện thông lượng. Các công trình trước đây đã khám phá các kỹ thuật lượng tử hóa LLM để cải thiện thêm hiệu quả xử lý theo lô. Các kỹ thuật này sử dụng các kiểu dữ liệu nhỏ hơn để thay thế các giá trị điểm nổi 16-bit (FP16), do đó giảm tiêu thụ bộ nhớ và tăng tốc tính toán (Lin et al., 2023; Xiao et al., 2023).

Tuy nhiên, các sơ đồ lượng tử hóa hiện tại không tận dụng hết mức độ khả năng được cung cấp bởi hỗ trợ phần cứng bit thấp hiệu quả mới nổi (ví dụ: Nvidia Ampere (Abdelkhalik et al., 2022) và Qualcomm Hexagon (Wikipedia contributors, 2023)). Chẳng hạn, một số phương pháp trước đây đã khám phá lượng tử hóa chỉ trọng số (Lin et al., 2023; Frantar et al., 2023). Trong các sơ đồ lượng tử hóa này, trọng số được lượng tử hóa thành biểu diễn bit thấp (ví dụ: INT3), trong khi các kích hoạt vẫn ở dạng biểu diễn điểm nổi (ví dụ: FP16). Do đó, trọng số phải được giải lượng tử hóa thành biểu diễn điểm nổi thích hợp (ví dụ: FP16) trước khi được nhân với các kích hoạt sử dụng biểu diễn điểm nổi. Do đó, mặc dù lượng tử hóa chỉ trọng số giảm tiêu thụ bộ nhớ, nó vẫn đòi hỏi phép toán điểm nổi tốn kém, điều này không hiệu quả, đặc biệt đối với kích thước lô lớn.

Một sơ đồ lượng tử hóa nổi bật khác là lượng tử hóa trọng số-kích hoạt, trong đó cả trọng số và kích hoạt đều được lượng tử hóa thành biểu diễn bit thấp. Trong sơ đồ này, trọng số và kích hoạt có thể được nhân trực tiếp bằng cách sử dụng các đơn vị tính toán độ chính xác thấp

--- TRANG 2 ---
Atom
Tĩnh
Sắp xếp lại
Nhóm
Lượng tử hóa
Động
Sắp xếp lại
Lượng tử hóa Nhóm
Bit thấp
Lượng tử hóa
Bit cao
Ngoại lai
Kích hoạt
Trọng số
GEMM
Hợp nhất
FlashInfer
Hợp nhất
Token tiếp theo
Token tiếp theo
KV Cache
Bit thấp
GEMM
Hợp nhất

Hình 1. Tổng quan về thiết kế của Atom. Đối với ma trận kích hoạt, chúng tôi sắp xếp lại động các kênh để chọn ra các ngoại lai. Sau đó, chúng tôi áp dụng lượng tử hóa nhóm bit thấp cho các giá trị bình thường trong khi sử dụng độ chính xác bit cao cho các ngoại lai. Đối với ma trận trọng số, quá trình lượng tử hóa có thể được thực hiện tĩnh. Chúng tôi thực hiện GEMM hợp nhất và FlashInfer hợp nhất (Ye et al., 2024) để tăng thông lượng. Chúng tôi cũng áp dụng KV-cache đã lượng tử hóa để giảm di chuyển bộ nhớ.

các đơn vị tính toán. Phương pháp lượng tử hóa này có tiềm năng lớn hơn để đạt được thông lượng suy luận cao hơn so với lượng tử hóa chỉ trọng số do hỗ trợ phần cứng bit thấp hiệu quả. Ví dụ, GPU A100 có thể đạt 1248 TOPS của INT4 và 624 TOPS của INT8 trái ngược với chỉ 312 TFLOPS cho FP16 với Tensor Cores (NVIDIA, a). Các công trình trước đây như LLM.INT8() (Dettmers et al., 2022) và SmoothQuant (Xiao et al., 2023) đã khám phá lượng tử hóa trọng số-kích hoạt INT8 và đạt được gần như không có tổn thất độ chính xác. Tuy nhiên, lượng tử hóa INT8 vẫn không thể sử dụng các phép toán bit thấp hơn như INT4 Tensor Cores (NVIDIA, b). Ngoài ra, lượng tử hóa INT8 vẫn dưới mức tối ưu để giảm tiêu thụ bộ nhớ lớn trong phục vụ LLM, nơi cả tham số mô hình và KV-cache theo lô đều tiêu thụ bộ nhớ lớn (Sheng et al., 2023; Zhang et al., 2023).

Đối với lượng tử hóa trọng số-kích hoạt bit thấp hơn, các công trình gần đây như OmniQuant (Shao et al., 2023) và QLLM (Liu et al., 2023a) đã đề xuất lượng tử hóa LLM xuống 4-bit. Tuy nhiên, các kỹ thuật của họ vẫn cho thấy sự gia tăng perplexity đáng kể so với baseline FP16 như được hiển thị trong Hình 2. Do đó, việc xác định cách lượng tử hóa chính xác LLM thành biểu diễn bit thấp trong khi duy trì hiệu quả phần cứng vẫn là một lĩnh vực nghiên cứu mở.

Trong công trình này, chúng tôi giới thiệu Atom, một lượng tử hóa trọng số-kích hoạt bit thấp chính xác cho LLM sử dụng hiệu quả phần cứng hiện đại. Để duy trì độ chính xác, Atom kết hợp ba thiết kế lượng tử hóa chính: (1) Nó áp dụng lượng tử hóa độ chính xác hỗn hợp, giữ lại một số lượng nhỏ nhưng quan trọng các kích hoạt và trọng số ở độ chính xác cao để bảo toàn độ chính xác. (2) Nó sử dụng lượng tử hóa nhóm chi tiết trên cả trọng số và kích hoạt, điều này tự nhiên giảm các lỗi lượng tử hóa. (3) Thay vì tính toán trước các tham số lượng tử hóa cho các kích hoạt, Atom lượng tử hóa động các kích hoạt để nắm bắt tốt nhất phân phối của mỗi đầu vào.

Mặc dù các tối ưu hóa lượng tử hóa này có thể cải thiện độ chính xác lượng tử hóa, chúng có thể không sử dụng phần cứng cơ bản một cách hiệu quả mà không có thiết kế bespoke. Ví dụ, kỹ thuật độ chính xác hỗn hợp có thể dẫn đến truy cập bộ nhớ không đều và làm chậm hiệu suất (Guo et al., 2023); phép nhân ma trận với lượng tử hóa nhóm không được hỗ trợ tốt trong các thư viện kernel; và lượng tử hóa động các kích hoạt phát sinh thêm tính toán (Xiao et al., 2023). Để đảm bảo hiệu quả phần cứng cao và giảm thiểu chi phí lượng tử hóa, Atom: (1) sắp xếp lại các kích hoạt và trọng số để duy trì truy cập bộ nhớ đều cho các phép toán độ chính xác hỗn hợp, (2) hợp nhất các phép toán lượng tử hóa và sắp xếp lại vào các toán tử hiện có để giảm thiểu chi phí, (3) tiếp tục lượng tử hóa các ngoại lai thành 8-bit để giữ cân bằng giữa độ chính xác và hiệu quả và (4) lượng tử hóa KV-cache thành biểu diễn bit thấp để giảm di chuyển bộ nhớ. Chúng tôi minh họa quy trình lượng tử hóa của Atom trong Hình 1.

Để xác thực tính khả thi của Atom, chúng tôi tích hợp nó vào một framework phục vụ đầu cuối (Chen et al., 2023). Đối với các phép nhân ma trận đặc biệt của chúng tôi với độ chính xác hỗn hợp và lượng tử hóa nhóm, chúng tôi triển khai các kernel CUDA tùy chỉnh sử dụng tensor cores bit thấp. Các thí nghiệm trên các tập dữ liệu phổ biến cho thấy Atom có tổn thất độ chính xác không đáng kể (giảm 1.4% độ chính xác zero-shot trung bình, tăng 0.3 perplexity WikiText2 cho Llama-65B) khi lượng tử hóa các mô hình xuống 4-bit (cho cả trọng số và kích hoạt), trong khi các công trình trước đây gặp phải tổn thất độ chính xác lớn hơn dưới cùng độ chính xác (xem Bảng 1).

Khi so sánh thông lượng phục vụ đầu cuối với các độ chính xác và sơ đồ lượng tử hóa khác nhau, Atom cải thiện thông lượng lên đến 7.7×, 5.5×, và 2.5× so với FP16, W4A16, và W8A8, tương ứng, trong khi đạt được độ trễ tương tự (xem Hình 10). Những kết quả này cho thấy Atom có thể lượng tử hóa chính xác LLM thành độ chính xác bit thấp trong khi đạt được thông lượng phục vụ cao.

Tóm lại, chúng tôi đóng góp như sau:
• Một phân tích hiệu suất toàn diện của workload phục vụ LLM

--- TRANG 3 ---
Atom
mà chỉ ra lợi ích hiệu quả của lượng tử hóa trọng số-kích hoạt bit thấp.
• Atom, một thuật toán lượng tử hóa trọng số-kích hoạt bit thấp chính xác kết hợp (1) độ chính xác hỗn hợp với sắp xếp lại kênh, (2) lượng tử hóa nhóm chi tiết, (3) lượng tử hóa kích hoạt động để giảm thiểu lỗi lượng tử hóa, và (4) lượng tử hóa KV-cache.
• Một framework phục vụ LLM tích hợp mà chúng tôi đồng thiết kế một quy trình suy luận hiệu quả, triển khai các kernel GPU bit thấp và chứng minh thông lượng và độ trễ đầu cuối thực tế của Atom.
• Một đánh giá toàn diện về Atom, cho thấy nó cải thiện thông lượng phục vụ LLM lên đến 7.7× với chỉ một tổn thất độ chính xác nhỏ.

2 KIẾN THỨC NỀN TẢNG
Các kỹ thuật lượng tử hóa sử dụng các giá trị bit thấp rời rạc để xấp xỉ các điểm nổi độ chính xác cao. Vì số nguyên đại diện cho một phạm vi đồng nhất, việc lượng tử hóa các giá trị điểm nổi thành số nguyên được phổ biến rộng rãi do tính đơn giản và hiệu quả phần cứng (Jacob et al., 2017; Han et al., 2016). Lượng tử hóa điển hình bao gồm hai bước: xác định các tham số lượng tử hóa (bao gồm scale và zero point) và tính toán tensor đã lượng tử hóa. Đối với lượng tử hóa bất đối xứng đồng nhất, scale s và zero point z được xác định bởi (Nagel et al., 2021):

s = max(X) - min(X) / (2^n - 1) · c, z = ⌊-min(X)/s⌉, (1)

trong đó X là tensor đầu vào, n là độ rộng bit lượng tử hóa, và c là hệ số cắt được sử dụng để giảm phạm vi động của lượng tử hóa nhằm giảm thiểu tác động của các giá trị ngoại lai. Các phần tử trong tensor đã lượng tử hóa có thể được tính bằng:

X̄ = clamp(⌊X/s⌉ + z, 0, 2^n - 1).

Chúng ta có thể đơn giản hóa thêm phương trình này cho lượng tử hóa đối xứng:

s = 2 · max(|X|) / (2^n - 1) · c
X̄ = clamp(⌊X/s⌉, -2^(n-1), 2^(n-1) - 1).

Các tham số lượng tử hóa s và z có thể được tính tĩnh bằng cách sử dụng dữ liệu hiệu chuẩn hoặc động trong thời gian suy luận với thống kê thời gian chạy. Do đó, các phương pháp lượng tử hóa có thể được phân loại là tĩnh hoặc động.

Đối với LLM, chúng ta có thể áp dụng lượng tử hóa trên cả ma trận kích hoạt và trọng số (lượng tử hóa trọng số-kích hoạt) hoặc chỉ ma trận sau (lượng tử hóa chỉ trọng số). Tuy nhiên, lượng tử hóa trọng số-kích hoạt bất đối xứng có thể dẫn đến các tính toán bổ sung trong phép nhân ma trận vì:

W · X = sW(W̄ - zW) · sx(X̄ - zx),

trong đó ba số hạng chéo bổ sung cần được tính toán để sử dụng các đơn vị tính toán bit thấp. Do đó, chúng tôi áp dụng lượng tử hóa đối xứng trong công trình này để có hiệu quả.

Các sự đánh đổi khác nhau giữa độ chính xác và hiệu quả có thể được đạt được bằng lượng tử hóa với độ chi tiết khác nhau: Đối với lượng tử hóa per-tensor, tất cả các giá trị trong tensor chia sẻ một bộ scale và zero-point (Nagel et al., 2021). Đối với lượng tử hóa per-channel (token), chúng ta tính toán scale và zero-point cho một hàng hoặc một cột của tensor (Xiao et al., 2023). Chúng ta ký hiệu kênh là chiều cuối cùng của ma trận đầu vào. Mỗi kênh có thể được chia thêm thành nhiều nhóm con, và lượng tử hóa được thực hiện riêng biệt trên mỗi nhóm, được gọi là lượng tử hóa per-group (Lin et al., 2023). Độ chi tiết càng mịn, lượng tử hóa càng chính xác, nhưng chi phí càng cao. Trong công trình này, chúng tôi áp dụng lượng tử hóa nhóm để có độ chính xác cao hơn với các kernel chuyên dụng để quản lý chi phí, như được hiển thị trong § 4.2.

3 PHÂN TÍCH HIỆU SUẤT CỦA PHỤC VỤ LLM BIT THẤP

Trong phần này, trước tiên chúng tôi phân tích nút thắt hiệu suất của suy luận LLM trong các tình huống phục vụ và sau đó thiết lập tầm quan trọng của lượng tử hóa trọng số-kích hoạt bit thấp.

Do nhu cầu cao, phục vụ LLM hướng đến thông lượng. Tuy nhiên, giai đoạn giải mã tự hồi quy của suy luận LLM chỉ lấy một token làm đầu vào và tạo ra token tiếp theo, do đó dựa vào phép nhân ma trận-vector (GEMV) (Agrawal et al., 2024). Vì GEMV cần tải một ma trận trọng số lớn trong khi chỉ thực hiện một vài phép nhân, nó bị ràng buộc mạnh bởi bộ nhớ. Do đó nó gây ra việc sử dụng GPU dưới mức, dẫn đến cường độ tính toán thấp

--- TRANG 4 ---
Atom

Ràng buộc
bộ nhớ
Ràng buộc
tính toán
Đỉnh
FP16
Ops
Đỉnh
INT4
Ops
FP16
4-bits
Lượng tử hóa
Trọng số-Kích hoạt
Ops
Elements
Ops/s

(a) Lượng tử hóa trọng số-kích hoạt

Ràng buộc
bộ nhớ
Ràng buộc
tính toán
Đỉnh
FP16
Ops
FP16
4-bits
Lượng tử hóa chỉ trọng số
Lớp self-attention
Lớp dense
Ops
Elements
Ops/s

×× (b) Lượng tử hóa chỉ trọng số

Hình 4. Mô hình roofline của các phương pháp lượng tử hóa khác nhau đặc trưng cho các toán tử bằng cường độ tính toán của chúng, được định nghĩa là Ops/Elements. Ở kích thước lô lớn, lớp dense là ràng buộc tính toán, có cường độ tính toán lớn, trong khi self-attention luôn thể hiện cường độ tính toán thấp hơn.

(tỷ lệ tính toán-to-IO) và do đó thông lượng thấp (Williams et al., 2009). Để giảm thiểu vấn đề này, xử lý theo lô được sử dụng rộng rãi bằng cách kết hợp đầu vào từ nhiều yêu cầu để thực hiện phép nhân ma trận lớp dense (tạo K,Q,V, phép chiếu O, và MLP) và tăng cường độ tính toán, do đó sử dụng GPU (Pope et al., 2022; Yu et al., 2022; Chen et al., 2023; Zhong et al., 2024).

Để khai thác thêm hiệu ứng xử lý theo lô và tăng thông lượng, các ma trận đầu vào của lớp dense của giai đoạn giải mã và prefill được xử lý theo lô cùng nhau để tạo thành các ma trận lớn hơn (Patel et al., 2023). Với kích thước lô lớn, lớp dense cuối cùng có phép nhân ma trận-ma trận ràng buộc tính toán (GEMM). Tuy nhiên, mặc dù các lớp self-attention trong giai đoạn giải mã cũng là các phép toán GEMV, chúng không thể hưởng lợi từ xử lý theo lô. Vì các yêu cầu suy luận khác nhau không chia sẻ KV-cache với các lịch sử ngữ cảnh khác nhau, dữ liệu cross-request không thể được xử lý theo lô để tái sử dụng, dẫn đến không có lợi ích hiệu quả. Ngay cả với một số tối ưu hóa như FlashAttention (Dao et al., 2022) hoặc Group Query Attention (Ainslie et al., 2023), các lớp self-attention vẫn bị ràng buộc bởi việc di chuyển bộ nhớ lớn của KV-cache.

Sau khi áp dụng kỹ thuật xử lý theo lô, chúng tôi đo phân tích thời gian của các toán tử khác nhau dưới các kích thước lô khác nhau. Như Hình 3 cho thấy, cả lớp dense và self-attention đều hoạt động như nút thắt đối với thông lượng, tiêu thụ hơn 90% thời gian xử lý. Do đó, chúng tôi sử dụng các cơ chế lượng tử hóa để đẩy nhanh cả lớp dense và self-attention.

Chúng tôi sử dụng mô hình Roofline (Williams et al., 2009) để đánh giá hiệu ứng của các phương pháp lượng tử hóa khác nhau trong các tình huống phục vụ. Như Hình 4(a) cho thấy, lượng tử hóa trọng số-kích hoạt có thông lượng tính toán lớp dense cao hơn do phép tính bit thấp hiệu quả của phần cứng. Nó cũng tăng thông lượng của lớp self-attention bằng cách giảm kích thước của KV-cache, do đó giảm di chuyển bộ nhớ. Tuy nhiên, như Hình 4(b) cho thấy, lượng tử hóa chỉ trọng số không thể cải thiện thông lượng lớp dense vì việc giải lượng tử hóa phải được thực hiện trước phép nhân ma trận, tạo ra các tính toán vẫn ở định dạng điểm nổi. Mặt khác, lượng tử hóa chỉ trọng số không thể lượng tử hóa KV-cache, không mang lại lợi ích cho các lớp self-attention. Chúng tôi định lượng thêm hiệu ứng của các kỹ thuật lượng tử hóa khác nhau trong Hình 11(a) và 11(b) trong §5 với profiling kernel.

Tóm lại, lượng tử hóa trọng số-kích hoạt bit thấp vượt trội hơn lượng tử hóa chỉ trọng số về việc tăng cường thông lượng trong tình huống phục vụ bởi vì nó tăng tốc cả lớp dense và self-attention. Trong các phần tiếp theo, chúng tôi chứng minh cách Atom mang lại thông lượng cao trong khi vẫn duy trì độ chính xác cao với lượng tử hóa trọng số-kích hoạt bit thấp.

4 THIẾT KẾ

Độ chính xác bit thấp cho phép sử dụng hiệu quả phần cứng cơ bản, dẫn đến tăng thông lượng. Tuy nhiên, thật khó khăn để duy trì độ chính xác cao với biểu diễn bit thấp. Để lượng tử hóa LLM xuống độ chính xác bit cực thấp trong khi giữ độ chính xác, chúng tôi kết hợp một bộ các cơ chế lượng tử hóa được điều chỉnh theo đặc điểm LLM. Các cơ chế này bao gồm lượng tử hóa độ chính xác hỗn hợp với sắp xếp lại kênh, lượng tử hóa nhóm chi tiết, và lượng tử hóa động. Chúng tôi chứng minh lợi ích độ chính xác nhờ các kỹ thuật này với nghiên cứu ablation trong Bảng 3. Atom cũng áp dụng lượng tử hóa bit thấp trên KV-cache, điều này tiếp tục tăng hiệu quả. Các phần phụ tiếp theo đi sâu vào chi tiết của từng cơ chế và lợi thế của nó, tiếp theo là mô tả chi tiết về quy trình đầu cuối.

4.1 Lượng tử hóa độ chính xác hỗn hợp

Các công trình trước đây đã quan sát thấy rằng một thách thức chính của lượng tử hóa LLM là hiện tượng ngoại lai trong các kích hoạt (Dettmers et al.,

--- TRANG 5 ---
Atom

  k_proj  v_proj
  q_proj  o_proj + gate_proj   up_proj
  down_proj+Layer Normalization
Sắp xếp lại & Yêu cầu lượng tử
FlashInfer
Layer Normalization
Sắp xếp lại & Yêu cầu lượng tử
Yêu cầu lượng tử
KV
Sắp xếp lại & Yêu cầu lượng tử
Giải lượng tử
Kích hoạt
Yêu cầu lượng tử
Lớp giải mã Llama FP16 Bit thấp Op hợp nhất Sắp xếp lại tĩnh
Giải lượng tử
Yêu cầu lượng tử
Giải lượng tử
Yêu cầu lượng tử
Giải lượng tử
Giải lượng tử
Giải lượng tử
Giải lượng tử
KV KV Cache đã lượng tử hóa

Hình 6. Tổng quan về quy trình Atom trên họ mô hình Llama. Atom quản lý cẩn thận chi phí của các toán tử lượng tử hóa bằng cách hợp nhất chúng vào các toán tử hiện có. Đối với các toán tử ràng buộc tính toán, Atom sử dụng hỗ trợ phần cứng bit thấp hiệu quả. Đối với lớp self-attention ràng buộc bộ nhớ, Atom lượng tử hóa KV-cache để tăng cường thêm thông lượng. Chúng tôi triển khai các kernel chuyên dụng cho từng toán tử hợp nhất.

11
33
A W1
0-27
713
-2=Sắp xếp lại
tĩnh
0
047
-99-1
411
331
00
047
-99-1
4
Sắp xếp lại động
Đầu ra của lớp cuối
Đầu ra
X
Giá trị
bình thường
Ngoại lai
Y

Hình 7. Atom sắp xếp lại động kích hoạt (A) để di chuyển các kênh ngoại lai đến cuối ma trận, với các chỉ số sắp xếp lại được xác định trong hiệu chuẩn offline. Ma trận trọng số (W) được sắp xếp lại tĩnh để vẫn căn chỉnh với các kênh kích hoạt tương ứng, đảm bảo tính đúng đắn của kết quả đầu ra.

2022; Lin et al., 2023). Như Hình 5(a) cho thấy, một vài kênh thể hiện độ lớn lớn gấp nhiều lần so với các kênh khác, được gọi là ngoại lai. Phạm vi động lớn của các ngoại lai này có thể tăng đáng kể lỗi lượng tử hóa. Do đó, việc xử lý hiệu quả các ngoại lai là rất quan trọng trong lượng tử hóa bit thấp.

Một cách trực quan để giảm thiểu hiệu quả thách thức này là lượng tử hóa riêng biệt các ngoại lai và giá trị bình thường, thành bit thấp và cao, được gọi là phương pháp độ chính xác hỗn hợp. Như Hình 5(b) cho thấy, sau khi chúng ta loại bỏ các ngoại lai, các kênh còn lại đồng nhất hơn nhiều, có thể được biểu diễn hiệu quả bằng các giá trị bit thấp. Kết quả của chúng tôi chỉ ra rằng các biểu diễn 8-bit, như FP8 (Micikevicius et al., 2022) và INT8, đủ để biểu diễn các ngoại lai (Xem Bảng 3). Vì INT8 được hỗ trợ rộng rãi bởi các triển khai phần cứng (ví dụ: NVIDIA Tensor Core (Abdelkhalik et al., 2022)), Atom áp dụng lượng tử hóa INT8 cho các ngoại lai.

Mối quan tâm chính với lượng tử hóa độ chính xác hỗn hợp là truy cập bộ nhớ không đều của nó (Dettmers et al., 2022; Guo et al., 2023), dẫn đến hiệu quả phần cứng kém. Để áp dụng lượng tử hóa độ chính xác hỗn hợp trong khi duy trì truy cập bộ nhớ đều, Atom tái sử dụng kỹ thuật sắp xếp lại được giới thiệu trong RPTQ (Yuan et al., 2023), nơi mục tiêu là cải thiện độ chính xác lượng tử hóa. Như Hình 7 cho thấy, Atom sắp xếp lại các kênh ngoại lai rải rác của các kích hoạt đến cuối ma trận, cho phép triển khai hiệu quả lượng tử hóa độ chính xác hỗn hợp. Để đảm bảo tính tương đương của kết quả tính toán, các ma trận trọng số cần được sắp xếp lại với các chỉ số sắp xếp lại tương ứng của các kích hoạt. Vì các kênh ngoại lai có thể được xác định offline bằng cách sử dụng dữ liệu hiệu chuẩn (Dettmers et al., 2022), việc sắp xếp lại các ma trận trọng số chỉ phát sinh chi phí một lần. Tuy nhiên, việc sắp xếp lại các ma trận kích hoạt vẫn cần được thực hiện online, có thể tốn kém. Để giảm thiểu điều này, Atom hợp nhất các toán tử sắp xếp lại ma trận kích hoạt vào các toán tử trước đó, điều này giảm đáng kể chi phí sắp xếp lại xuống dưới 0.5% thời gian chạy.

4.2 Lượng tử hóa nhóm chi tiết

Ngay cả khi Atom lượng tử hóa riêng biệt các ngoại lai và giá trị bình thường, phần sau vẫn khó thực hiện chính xác do khả năng biểu diễn hạn chế của độ chính xác 4-bit (Phần 5.4). Để tăng cường thêm độ chính xác, lượng tử hóa nhóm được áp dụng rộng rãi (Lin et al., 2023; Nagel et al., 2021), chia ma trận thành các nhóm con và thực hiện lượng tử hóa trong từng nhóm con. Ví dụ, kích thước nhóm 128 có nghĩa là mỗi chuỗi liên tiếp 128 phần tử được coi là một nhóm duy nhất, được lượng tử hóa độc lập.

Lượng tử hóa nhóm mang lại sự đánh đổi giữa cải thiện độ chính xác và chi phí giải lượng tử hóa, đặc biệt trong lượng tử hóa trọng số-kích hoạt. Các công trình trước đây chưa nghiên cứu cách kết hợp hiệu quả giải lượng tử hóa nhóm vào pipeline GEMM tinh tế, tức là pipeline MMA

--- TRANG 6 ---
Atom

+Giải lượng tử
Giải lượng tử
Nhóm 1
INT
FP16
1
2
3
Nhóm 2
Tensor Cores CUDA Cores

Hình 8. Tổng quan về toán tử GEMM hợp nhất. Phép nhân của mỗi nhóm trước tiên được tính bởi các đơn vị với hỗ trợ bit thấp hiệu quả, tức là Tensor Cores (Bước 1). Kết quả sau đó được giải lượng tử hóa và tiếp theo được tích lũy với các đơn vị FP16 điển hình (Bước 2, 3). Lưu ý rằng tất cả các hoạt động được hợp nhất trong một pipeline duy nhất.

(Thakkar et al., 2023). Atom đề xuất một kỹ thuật hợp nhất như được hiển thị trong Hình 8, góp phần vào một kernel GEMM hiệu quả với tăng tốc thực tế (Xem §5.3.1). Atom trước tiên tính toán phép nhân ma trận của các nhóm kích hoạt với các nhóm trọng số tương ứng và thu được kết quả tạm thời bằng cách sử dụng phần cứng bit thấp hiệu quả, tức là Tensor Cores (Bước 1). Atom sau đó cộng nhiều kết quả tạm thời lại với nhau để có kết quả GEMM. Tuy nhiên, vì Atom thực hiện lượng tử hóa chi tiết cho từng nhóm kích hoạt và trọng số, mỗi kết quả tạm thời có các tham số lượng tử hóa khác nhau. Do đó, Atom trước tiên giải lượng tử hóa tất cả kết quả tạm thời thành biểu diễn FP16 với CUDA Cores (Bước 2) và sau đó thực hiện phép cộng (Bước 3). Để quản lý chi phí, chúng tôi hợp nhất giải lượng tử hóa và tổng hợp vào kernel GEMM, cụ thể là vào pipeline MMA. Do đó, các hoạt động bổ sung có thể được thực hiện tại chỗ mà không cần di chuyển bộ nhớ thêm và được chồng chập với các lệnh MMA gốc. Chúng tôi chứng minh hiệu quả của toán tử GEMM hợp nhất trong §5.3.1.

Với kích thước nhóm 128 và kích thước kênh độ chính xác cao 128, Atom có bit hiệu quả 4.25¹ trên Llama-7b. Bit hiệu quả được định nghĩa là số bit trung bình được sử dụng cho mỗi phần tử, bao gồm các tham số lượng tử hóa. Chỉ số này được sử dụng rộng rãi trong các công trình trước đây về lượng tử hóa chỉ trọng số (Frantar et al., 2023; Lin et al., 2023), chủ yếu vì nó đại diện cho tỷ lệ nén thực tế và do đó tăng tốc trong thiết lập ràng buộc bộ nhớ. Tuy nhiên, lợi ích chính của lượng tử hóa trọng số-kích hoạt trong các tình huống phục vụ là hiệu quả tính toán của việc tận dụng các đơn vị tính toán bit thấp thay vì việc giảm bộ nhớ. Do đó, chúng tôi sẽ không sử dụng chỉ số này trong các thảo luận tiếp theo.

4.3 Quy trình lượng tử hóa động

Mặc dù lượng tử hóa chi tiết có thể bảo toàn tốt hơn các biến thiên cục bộ bên trong mỗi kênh của các kích hoạt, lợi thế này sẽ giảm đi nếu chúng ta tính toán tĩnh các tham số lượng tử hóa dựa trên dữ liệu hiệu chuẩn, vì đầu vào thực tế có thể có phân phối cục bộ khác.

Do đó, Atom áp dụng lượng tử hóa động, điều chỉnh các tham số lượng tử hóa cho từng ma trận kích hoạt trong quá trình suy luận. Để kiềm chế chi phí của lượng tử hóa động, chúng tôi hợp nhất các hoạt động lượng tử hóa vào toán tử trước đó, tương tự như triển khai của ZeroQuant (Yao et al., 2022). Vì toán tử bổ sung là theo phần tử (với một phép giảm và một phép chia theo phần tử), thời gian chạy của toán tử hợp nhất vẫn không đáng kể so với các lớp dense và self-attention tốn thời gian, như Hình 3 cho thấy.

Tuy nhiên, lượng tử hóa bất đối xứng có thể dẫn đến chi phí thời gian chạy đáng kể do tính toán bổ sung đáng kể (như được thảo luận trong §2). Để đạt cân bằng giữa thông lượng và độ chính xác, chúng tôi chọn lượng tử hóa đối xứng với ngưỡng cắt được chọn cẩn thận. Chúng tôi cũng kết hợp GPTQ (Frantar et al., 2023) khi lượng tử hóa ma trận trọng số vì đây là quá trình hoàn toàn offline và mang lại sự tăng cường độ chính xác mà không hy sinh hiệu quả thời gian chạy.

4.4 Lượng tử hóa KV-cache

Như được mô tả trong §3, lớp self-attention trong giai đoạn giải mã bị ràng buộc rất nhiều bởi bộ nhớ. Để giảm thiểu vấn đề này, Atom cũng áp dụng lượng tử hóa bit thấp cho KV-cache. Atom tải KV-cache ở độ chính xác bit thấp và trực tiếp giải lượng tử hóa nó trước khi thực hiện tính toán FP16, điều này tăng đáng kể thông lượng bằng cách giảm bộ nhớ lớn. Mặt khác, vì việc di chuyển bộ nhớ của KV-cache được lượng tử hóa bất đối xứng và đối xứng là tương tự, chúng hoạt động tương tự trên các lớp self-attention ràng buộc bộ nhớ. Do đó, Atom sử dụng lượng tử hóa bất đối xứng trên KV-cache vì nó có thể mang lại lợi ích độ chính xác.

So với các ma trận kích hoạt, chúng tôi lập luận rằng KV-cache dễ lượng tử hóa hơn. Để thực hiện self-attention, vector Query của token đến được nhân với K cache. Kết quả được chuẩn hóa bằng Softmax và tiếp tục được nhân với V cache để thu được đầu ra (Vaswani et al., 2023). Do việc chuẩn hóa của Softmax, lỗi lượng tử hóa của K cache có ít ảnh hưởng hơn đến đầu ra. Hơn nữa, profiling của chúng tôi trong Hình 9

--- TRANG 7 ---
Atom

chỉ ra rằng V cache thể hiện hiện tượng ngoại lai ít thường xuyên hơn, làm cho nó phù hợp hơn cho lượng tử hóa. Do đó, Atom trực tiếp áp dụng lượng tử hóa bit thấp bất đối xứng với độ chi tiết của attention head và bảo toàn độ chính xác cao như được hiển thị trong §5.4.

4.5 Triển khai quy trình lượng tử hóa

Để chứng minh tính khả thi của các lựa chọn thiết kế của chúng tôi, chúng tôi triển khai Atom trên các mô hình Llama (Touvron et al., 2023a), như được hiển thị trong Hình 6. Để tận dụng lợi ích của lượng tử hóa, Atom quản lý chi phí của các toán tử bổ sung bằng hợp nhất kernel: Atom hợp nhất các toán tử lượng tử hóa, bao gồm sắp xếp lại, lượng tử hóa và giải lượng tử hóa, vào các toán tử hiện có. Đối với lớp dense ràng buộc tính toán, Atom sử dụng các đơn vị bit thấp để tăng thông lượng. Đối với lớp self-attention ràng buộc bộ nhớ, Atom hợp nhất giải lượng tử hóa với thư viện kernel cho phục vụ LLM, FlashInfer (Ye et al., 2024), để chỉ các giá trị bit thấp từ KV-cache được tải. Atom cũng kết hợp PageAttention (Kwon et al., 2023) để sử dụng bộ nhớ hiệu quả nhằm cho phép kích thước lô lớn.

5 ĐÁNH GIÁ

Chúng tôi tiến hành đánh giá toàn diện về độ chính xác và hiệu quả của Atom. Đối với độ chính xác, chúng tôi đánh giá Atom trên các chỉ số được sử dụng rộng rãi, perplexity tạo sinh và độ chính xác zero-shot. Đối với hiệu quả, chúng tôi đánh giá Atom từ dưới lên, bắt đầu với hiệu suất per-kernel, tiếp theo là thông lượng và độ trễ đầu cuối. Chúng tôi cũng thực hiện các nghiên cứu ablation để hiểu cách các kỹ thuật khác nhau ảnh hưởng đến Atom, điều này chỉ ra sự đánh đổi giữa hiệu quả và độ chính xác của từng lựa chọn thiết kế.

5.1 Thiết lập lượng tử hóa

Atom sử dụng lượng tử hóa đối xứng trên trọng số và kích hoạt trong khi sử dụng lượng tử hóa bất đối xứng trên KV-cache. Chúng tôi đánh giá Atom sử dụng kích thước nhóm 128. Để xác định các kênh ngoại lai, chúng tôi sử dụng 128 câu được lấy mẫu ngẫu nhiên từ WikiText2 (Merity et al., 2016) làm dữ liệu hiệu chuẩn, theo các công trình trước đây (Lee et al., 2023; Shao et al., 2023; Liu et al., 2023a). Chúng tôi chọn 128 kênh có giá trị tổng bình phương cao nhất làm kênh ngoại lai và giữ chúng trong INT8. Sau đó chúng tôi sắp xếp lại ma trận kích hoạt và trọng số theo các chỉ số của kênh ngoại lai. Sau khi sắp xếp lại, Atom áp dụng GPTQ (Frantar et al., 2023) cho lượng tử hóa trên ma trận trọng số. Đối với cắt, chúng tôi sử dụng tìm kiếm lưới để tìm các hệ số cắt tối ưu 0.9 và 0.85 cho lượng tử hóa kích hoạt và trọng số, tương ứng.

Đối với tiền xử lý lượng tử hóa trọng số và xác định ngoại lai, chúng tôi chạy Atom trên một RTX Ada 6000 đơn lẻ và lượng tử hóa mô hình theo từng lớp. Đối với Llama-65B lớn, Atom mất khoảng 4 giờ để hoàn thành quá trình.

5.2 Đánh giá độ chính xác

Benchmarks. Chúng tôi đánh giá Atom trên các mô hình Llama (Touvron et al., 2023a) mã nguồn mở phổ biến. Chúng tôi tập trung vào các thiết lập bit thấp, lượng tử hóa trọng số-kích hoạt INT4 và INT3. Chúng tôi áp dụng các chỉ số độ chính xác mô hình được sử dụng phổ biến, perplexity và độ chính xác zero-shot. Đối với perplexity, chúng tôi đánh giá trên các tập dữ liệu WikiText2 (Merity et al., 2016), PTB (Marcus et al., 1994), và C4 (Raffel et al., 2020). Đối với các nhiệm vụ zero-shot, chúng tôi sử dụng lm-eval (Gao et al., 2021), dựa trên đó chúng tôi đánh giá Atom trên các nhiệm vụ PIQA (Bisk et al., 2019), ARC (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), và WinoGrande (Sakaguchi et al., 2019).

Baselines. Chúng tôi so sánh Atom với các kỹ thuật lượng tử hóa hậu huấn luyện được phát hành gần đây: SmoothQuant (Xiao et al., 2023), OmniQuant (Shao et al., 2023), và QLLM (Liu et al., 2023a). Đối với SmoothQuant, chúng tôi triển khai phiên bản riêng của mình vì mã chính thức không hỗ trợ các mô hình Llama và chỉ có lượng tử hóa W8A8. Chúng tôi tiến hành tìm kiếm lưới trên giá trị alpha được định nghĩa trong SmoothQuant và báo cáo các số tốt nhất cho mỗi benchmark. Đối với OmniQuant, chúng tôi sử dụng trọng số đã được lượng tử hóa trước của họ cho đánh giá W4A4 và đánh giá W3A3 bằng cách chạy mã chính thức của họ. Để có được kết quả W3A3 tốt nhất cho OmniQuant, chúng tôi tiến hành tìm kiếm siêu tham số và xác định lr = 1e−4 và alpha = 0.75 cho quá trình lượng tử hóa của họ. Chúng tôi bỏ qua W3A3 OmniQuant trên Llama-30B và Llama-65B do yêu cầu tài nguyên lớn của quá trình lượng tử hóa của nó. Đối với QLLM, chúng tôi báo cáo các số W4A4 trong bài báo của họ nhưng không đánh giá W3A3 vì mã của họ không khả dụng khi chúng tôi tiến hành thí nghiệm.

Độ chính xác zero-shot. Bảng 1 so sánh độ chính xác zero-shot của sáu nhiệm vụ giữa Atom và các baseline trên các mô hình Llama. Atom vượt trội đáng kể so với các phương pháp lượng tử hóa trọng số-kích hoạt khác. Đối với W4A4, Atom chỉ cho thấy 2.3%, 1.7%, 0.4% và 1.4% tổn thất độ chính xác trung bình cho Llama ở kích thước 7B, 13B, 30B và 65B khi so sánh với FP16. Cùng lúc đó, các công trình trước đây cho thấy tổn thất độ chính xác từ 9.6% đến 23.8% dưới cùng thiết lập.

Perplexity. Bảng 2 báo cáo kết quả perplexity của Atom và các baseline trên các mô hình Llama. Như bảng cho thấy, mặc dù các phương pháp gần đây như OmniQuant và QLLM thành công giảm perplexity của W4A4 xuống khoảng 10, tổn thất độ chính xác vẫn đáng kể. Atom tiếp tục giảm perplexity và đạt được ít hơn 0.4 tăng perplexity trên tất cả ba tập dữ liệu với Llama-65b. Đối với W3A3, Atom vẫn phần lớn duy trì perplexity, với mức tăng perplexity trung bình 2.3 cho Llama-65B. Cùng lúc đó, các công trình hiện có không đạt được perplexity chấp nhận được. Lưu ý rằng Atom có ít tổn thất độ chính xác hơn khi lượng tử hóa các mô hình lớn hơn.

--- TRANG 8 ---
[Bảng 1 và 2 với dữ liệu số liệu được giữ nguyên như trong bản gốc]

--- TRANG 9 ---
232425262728
Kích thước lô1234Thông lượng (kilo tokens/s)
FP16
W4A16
W8A8
W4A4(Atom)

(a) Thông lượng đầu cuối

232425262728
Kích thước lô20406080100120140160Độ trễ (ms)
Độ trễ chấp nhận được

(b) Độ trễ giải mã mỗi token

W16A16
FP16W4A16
AWQW8A8
SmoothQW4A4
Atom051015202530Thông lượng (tokens/giây) 
7.7x5.5x2.5xThông lượng
Độ trễ
515253545556575
Độ trễ(ms) 

(c) So sánh với bộ nhớ GPU cố định

Hình 10. Đánh giá đầu cuối của Atom. Các đường liền là đo lường chính xác, trong khi các đường đứt nét là ước tính do khả năng bộ nhớ hạn chế. (a) Số token được tạo ra mỗi giây. (b) Độ trễ giải mã trung bình mỗi token. Atom vượt trội so với tất cả các phương pháp lượng tử hóa khác cho cả thông lượng và độ trễ. (c) Hiệu suất được đánh giá dưới một lượng bộ nhớ GPU cố định. Lưu ý rằng Atom tăng thông lượng 2.5× nhiều hơn W8A8 vì nó cho phép kích thước lô lớn hơn, sử dụng hiệu ứng xử lý theo lô.

[Phần còn lại của trang 9 được dịch tiếp tục với nội dung về đánh giá hiệu quả, kernel evaluation, end-to-end evaluation và các kết quả performance]

--- TRANG 10 ---
[Nội dung về ablation study và các bảng kết quả được dịch tiếp]

--- TRANG 11 ---
[Phần thảo luận về tính tổng quát và các định dạng dữ liệu mới]

--- TRANG 12 ---
[Phần kết luận và lời cảm ơn được dịch tiếp]

--- TRANG 13-14 ---
[Danh sách tài liệu tham khảo được giữ nguyên]
