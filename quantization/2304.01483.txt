# 2304.01483.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2304.01483.pdf
# File size: 5245885 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Blockwise Compression of Transformer-based Models without Retraining
Gaochen Dong, Wei Chen
Tensorchip, Beijing, China
gaochendong buaa@outlook.com
abstract
Transformer-based models, exemplified by GPT-3, ChatGPT, and GPT-4, have recently garnered considerable
attention in both academia and industry due to their promising performance in general language tasks. Nevertheless,
these models typically involve computationally encoding processes, and in some cases, decoding processes as well,
both of which are fundamentally large-scale matrix multiplication. These operations bring the inevitable challenges
of massive computation resources and huge memory footprint, usually requiring at least 1023FLOPs and hundreds
of gigabytes, respectively. A common method to address this issue is to reduce the computational and memory re-
quirements by applying layerwise quantization to the transformer, replacing the usual fp32 data type with a low-bit
equivalent. Unfortunately, this method often leads to decreased model accuracy and necessitates time-consuming
retraining. Such retraining not only requires fine-tuning skills but also substantial computational resources, posing
challenges for users. To specifically tackle these issues, we propose BCT, a framework of blockwise compression for
transformers without retraining, aiming to facilitate model deployment. Unlike layerwise compression methods, BCT
achieves finer compression of the entire transformer by operating blockwise. This method mitigates data distribu-
tion deviation caused by quantization, eliminating the requirement for retraining. BCT effectively compresses all
components of the model, including but not limited to the embedding, matrix multiplication, GELU, Softmax, layer
normalization, and intermediate results. In a case study, an efficient model is compressed by BCT achieving up to
7.988x compression. Subsequently, we also evaluate it on several General Language Understanding Evaluation (GLUE)
datasets. Experimental results on the majority of GLUE benchmark demonstrate the effectiveness of our method, as
BCT achieves less than a 0.9% degradation in accuracy compared to the more than a 1% degradation seen with other
methods providing similar or inferior compression ratios.
Keywords: transformer; compression; blockwise; noretraining
1 Introduction
Transformer-based models[Vaswani et al., 2017], such as
GPT-3[Brown et al., 2020], ChatGPT and GPT-4, have
demonstrated considerable promise and state-of-the-art
performance in many fields, such as sentiment classifica-
tion, machine translation, document analysis, question an-
swering, text summarization, multi-round dialogue, image
classification, visual question answering, and visual com-
monsense reasoning. As a result, the demand for deploying
these models in business and scientific applications, such
as advanced search engines, AI-powered chatbots for en-
hanced customer service, and scientific research support,
has been rapidly increasing.
However, the widespread use of transformer-based
models is hindered by their substantial computation and
memory requirements, primarily due to the utilization of
the multi-head self-attention mechanism. Usually, they
require at least 1023FLOPs and hundreds of gigabytes,
respectively. High requirements for hardware platforms
bring incredible difficulties to deployment and application.
To address these challenges, model compression has
emerged as a feasible strategy to reduce the computational
and memory requirements of transformer-based modelswhile maintaining their predictive performance. Com-
pressed models offer faster calculations, reduced mem-
ory footprint, and lower bandwidth requirements, thereby
facilitating deployment and application. In this case,
several compression methods are proposed to compress
transformer-based models with an acceptable degrada-
tion of accuracy. However, most of them require exten-
sive retraining, which can take weeks or even months, to
fine-tune the parameters using a prepared-well calibration
dataset to match the original data distribution.
In this paper, we introduce Blockwise Compression of
Transformers without retraining (BCT), a novel frame-
work that compresses each component of the trans-
former. Unlike previous compression frameworks that
may only focus on specific components, BCT uti-
lize a blockwise method to compress the entire trans-
former, including the embedding, matrix multiplica-
tion, GELU[Hendrycks and Gimpel, 2016], Softmax, layer
normalization[Ba et al., 2016], and all the intermediate re-
sults. Fig.1 illustrates the framework of BCT. Exper-
iments demonstrate that, without retraining, BCT can
maintain an accuracy degradation of less than 0.9% in
most tasks on the GLUE[Wang et al., 2018] datasets.
This paper makes three major contributions: (1) We
1arXiv:2304.01483v2  [cs.CL]  17 Sep 2023

--- PAGE 2 ---
Fig. 1: The Framework of BCT
propose BCT, an efficient blockwise compression frame-
work for transformer-based models that eliminates the
need for retraining. (2) A practical and efficient methodol-
ogy is proposed to handle each transformer layer, including
matrix multiplication and nonlinear operations. (3) We
evaluate BCT on multiple GLUE datasets, demonstrat-
ing its effectiveness with an accuracy degradation of less
than 0.9% on most tasks, surpassing many existing meth-
ods. Overall, BCT provides an efficient and practical solu-
tion for deploying transformer-based models in resource-
constrained environments without retraining, significantly
reducing the barriers to their wider application.
2 Related Work
In recent years, the compression of transformer-based
models has emerged as a prominent area of research. Com-
pressed models can significantly reduce computational
resource requirements and memory footprints. Several
efficient and practical compression methods, including
pruning and quantization, have been proposed. For in-
stance, SparseGPT[Frantar and Alistarh, 2023] achieves
60% sparsity by pruning GPT without necessitating re-
training. Methods such as Q8BERT[Zafrir et al., 2019]
and Q-BERT[Shen et al., 2020] implement partial quanti-
zation on BERT, while using floating-point arithmetic for
the remainder, an method commonly referred to as ’fake
quantization’. However, above methods fail to achievethe optimal compression ratio. I-BERT[Kim et al., 2021]
quantizes BERT with integer-only arithmetic and infers
matrix multiplication with int8 and nonlinear operations
with int32. FQ-BERT[Liu et al., 2021] also fully quantizes
the whole BERT and can be deployed on FPGA. Both
methods achieve a high compression ratio, especially 7.94x
compression for FQ-BERT. However, they both need re-
training to fine-tune the parameters. In contrast to these
exsiting methods, our proposed framework, BCT, achieves
a high compression ratio while eliminating the need for re-
training. BCT can maintain an accuracy degradation of
less than 0.9% in most tasks with 7.988x compression, even
higher than FQ-BERT.
3 Method
In this paper, BCT compresses the parameters and inter-
mediate results of the model with blockwise compression
and performs matrix multiplication and nonlinear opera-
tions with low-bit arithmetic, as shown in Fig.1.
3.1 Blockwise Compression
BCT partitions the weights, biases and intermediate re-
sults of a model into blocks and compresses them accord-
ingly. The dimensions of these blocks are determined by
the specific hardware platform in use. It is essential to
strike a balance between hardware efficiency and model
2

--- PAGE 3 ---
Fig. 2: An Example of Blockwise and Layerwise Compression
Initialize a 4 * 4 matrix and set block size = 2.
The number in the center of the dashed box is the shift value corresponding to the number in the dashed box
Fig. 3: Comparison of Data Distribution through Box Plot
performance when selecting the appropriate block size.
Generally, 64 is a block size that is friendly to both model
parameter size and memory size. Unlike layerwise com-
pression, which affects the data distribution on a larger
scale, blockwise compression brings about changes at a
smaller scale and is less influenced by the data distribu-
tion of other blocks. Therefore, the distribution of block-
wise compressed data is closer to the original distribution.
Fig.2 provides an in-depth comparative study between
blockwise and layerwise compression algorithms. Fig.3,
by employing a box plot representation, effectively eluci-
dates the data distribution for the original data in con-
junction with both compression methods in Fig.2. Conse-
quently, the distribution of the blockwise compressed data
closely resembles the original distribution. Additionally,
blockwise compression takes into account the correlation
information among samples, resulting in lower accuracy
degradation further.
Due to the superior performance of blockwise compres-
sion, BCT can compress weight, bias and intermediate
results to low-bit integer data or low-bit floating point
data, thereby saving computation resources and reducing
memory footprints. Fig.4 shows the memory footprint of
various data types.
In this paper, we apply a symmetric shift quantiza-
tion method[Miyashita et al., 2016] to compress fp32 data
into low-bit integer data, which is more hardware-friendly
than linear compression appiled by previous compression
method. For k-bit compression of block x, the functionsare:
shift =⌊log2(2k−1
max(|x|))⌋ (1)
xc=clip([x << shift ], MIN, MAX ) (2)
where c means compression and MIN, and MAX are
calculated statistically on the calibration dataset by mea-
suring KL divergence. This enables each compressed
block, denoted as xc, to have its own shift value, which
can be seen as the exponent section of the block. In
contrast, when compressing data layerwise, all the com-
pressed data share a single exponent section, which means
coarser-grained quantization and greater data distribution
deviation. In addition to integer data, BCT can clip the
exponent and mantissa sections of fp32 data to obtain fp8
data. The representation ranges of the data types we use
are shown in Table 1.
3.2 Matrix Multiplication
The core concept lies in the observation that matrix multi-
plication can be performed as matrix block multiplication.
For instance, let us consider the equation Y=X·WT+B,
where X represents the input, Y represents the output,
W represents the weights, and B represents the biases.
In BCT, X, W, and B are partitioned into blocks, which
are then blockwise compressed to obtain cX, cW, and cB
respectively. Subsequently, BCT conducts matrix block
3

--- PAGE 4 ---
Fig. 4: The Bitwidth of Data Types
Table 1: The Representation of Data Types
Data Type Dynamic Range Min Positive Value
int4 -8 ∼7 1
int8 -128 ∼127 1
fp8(e4m3) -240 ∼240 1.95*10ˆ-3
fp8(e5m2) -57344 ∼57344 1.526*10ˆ-5
fp32 -3.4*10ˆ38 ∼3.4*10ˆ38 1.4*10ˆ-45
multiplication cX·cWTand adds the compressed bias cB
to yield the final output cY.
When performing matrix multiplication, there are cer-
tain crucial details to consider. Prior to accumulation or
addition, it is necessary to normalize the exponent sec-
tions(the shift values) of the blocks. This normalization
process is illustrated in Fig.5. Specifically, the exponent
section shift kof each block cXik·cWT
jkshould be ad-
justed uniformly to match the maximum exponent sectionshift max=max(shift k).
The overall process for each accumulation step can be
outlined as follows:
accij=qX
k=1
cXik·cWT
jk<<(shift max−shift k)
(3)
In addition, the exponent sections of accijandcCj
should be uniform before adding them.
Fig. 5: Normalize Exponent Sention Before Accumulation or Addition
In addition to mitigating distribution deviation and
consequently eliminating the need for retraining, imple-
menting block-wise compression and computation in ma-
trix multiplication operations proffers another significant
advantage. Specifically, it enhances the parallelism in-
herent in matrix multiplications. This process involves
customizing the block size and treating each block mul-
tiplication - involving the multiplication of an input data
block cXikwith a weight block cWT
jk- as an indepen-
dent entity. This independence allows for the simultane-
ous execution of block-wise matrix multiplication. As a
result, computational resources can be utilized more effi-
ciently, computational speed is increased, and overall per-formance is improved. Therefore, the block-wise method
not only optimizes resource allocation but also boosts the
cost-effectiveness and performance of transformer-based
models.
3.3 Nonlinear Operations
For nonlinear operations like GELU, Softmax, and Lay-
erNorm, a pragmatic method involves computation with
low-bit floating-point data such as fp8 or employing an
int8 lookup table. In the case of the latter, we confine
the input within a specific sampling range and treat the
compressed input as keys. Concurrently, the output cor-
responding to each input is compressed and regarded as
4

--- PAGE 5 ---
Fig. 6: Process Out-of-Range Data in Nonlinear Exponential Operation
values. Consequently, we construct a lookup table consist-
ing of 256 key-value pairs that can accommodate any int8
input. However, instead of directly obtaining the output
from this table, we deploy an interpolation algorithm to
approximate these nonlinear operations. This method re-
sults in a smaller degradation in accuracy. To illustrate,
consider two keys, ‘ key 0‘ and ‘ key 1‘, which are closest to
the input x. Upon looking up the table, we receive cor-
responding values ‘ value 0‘ and ‘ value 1‘. The output y is
then calculated using a weighted average formula:
y=x−key 1
key 0−key 1value 0+x−key 0
key 1−key 0value 1
This practical method enables a more accurate approx-
imation of nonlinear operations and contributes to the en-
hanced efficiency of our proposed blockwise compression
method.
3.3.1 GELU
The Gaussian Error Linear Unit (GELU), an activation
function widely involved in the transformer-based models,
is mathematically defined as follows:
GELU (x) =x·1
2h
1 +erf(x/√
2)i
(4)
Owing to its superb linear properties, analogous to the
Rectified Linear Unit (ReLU)[Xu et al., 2015], GELU be-
comes a suitable candidate for interpolation algorithms.
These exceptional characteristics enable us to compress
the input and output of GELU directly with minimal
degradation in accuracy. Consequently, by constructing
a lookup table and implementing the interpolation algo-
rithm based on the compressed input and output, we can
maintain a satisfactory level of accuracy while enhancing
computational efficiency.
3.3.2 Softmax
The Softmax function, pivotal in transformer-based mod-
els, is mathematically represented as:
Softmax (xi) =exp(xi)P
jexp(xj)(5)
Central to the Softmax operation is a nonlinear ex-
ponential component. We address this by initiating the
exponential operation via a table lookup method. Conse-
quently, the output of Softmax can be derived through
low-bit arithmetic operations. Prior to constructing alookup table for the exponential operation, it proves ben-
eficial to subtract all inputs by their respective maximum
value. This action constrains the output of the exponen-
tial operation to within the range of 0 and 1, without
altering the final output of the Softmax function. This
strategy effectively circumvents the issue of an excessively
broad output distribution from the exponential operation
which could complicate compression. Furthermore, out-
of-range data can be managed by leveraging the property
ex+y=ex×eyof the exponential operation. By taking two
inside-range numbers x and y as separate inputs, we can
obtain the exponential value for the potentially outside-
range sum, x+y, as depicted in Fig.6. This method allows
us to aptly handle out-of-range data and maintain an ef-
fective blockwise compression method.
3.3.3 LayerNorm
The Layer Normalization (LayerNorm) function is a popu-
lar normalization method used in transformer-based mod-
els, which is defined as:
LayerNorm (x) =x−E[x]p
V ar[x] +ε×γ+β (6)
At the core of the LayerNorm function lies a nonlin-
ear component — the square root operation. In line with
our methodology, we compress both the input and output
of the square root function to construct a corresponding
lookup table. Based on this table and employing the in-
terpolation algorithm, we can derive the output of Layer-
Norm using low-bit arithmetic. Just as for the exponential
operation, we devise a strategy to deal with data that falls
outside the designated sampling range. We leverage the
mathematical principle√x+y=√x∗√yto process such
out-of-range data. Furthermore, the square root function
also resides in the scale layer of the attention mechanism,
where we apply the same aforementioned method. This
consistent application across different components rein-
forces the efficacy and universality of BCT.
3.4 Intermediate Results
It is entirely feasible to partition intermediate results into
blocks and apply compression accordingly. An imperative
rule that governs this process stipulates that any transfer
of intermediate results between two layers must occur us-
ing low-bit data types. In circumstances where the output
from a layer does not meet this specification, it becomes
necessary to reapply the compression strategy, thereby en-
suring the output conforms to the low-bit data format.
5

--- PAGE 6 ---
Consequently, the intermediate results can be used as in-
put, being calculated in blocks with the blockwise parame-
ters of the next layer. This method aligns with our broader
methodology and contributes to maintaining robust com-
putational efficiency across all layers.
4 EXPERIMENTS
4.1 Dataset
For an extensive evaluation of BCT, we select several
datasets from GLUE benchmark. This includes one single-
sentence task (SST2), one similarity and paraphrase task
(STSB), and two inference tasks (MNLI and RTE). Such
a diverse selection of tasks enables us to thoroughly scru-
tinize BCT’s performance across a variety of application
scenarios, thereby providing a comprehensive understand-
ing of its capability, efficiency, and applicability in differ-
ent scenarios.
4.2 Setup
Given that ChatGPT and GPT4 are not open-source,
we have opted to employ the well-known BERT model
as the testing ground for BCT. By the way, it is worth
mentioning that other transformer-based models, such asChatGPT and GPT4, employ similar operators to BERT
in their model architectures. Therefore, the methodol-
ogy of compressing BERT using BCT is equally applica-
ble to these models. The baseline model used for com-
parisons is the BERT-base model furnished by Pytorch-
Transformers1. For a comprehensive analysis, we con-
trast BCT with two existing quantization methods -
Q8BERT and FQBERT. Specifically, we devise four dif-
ferent models, each employing a unique data type for
each layer, as detailed in Table 2. The BCT int8/fp32
model serves to validate the efficacy of blockwise com-
pression without retraining, making it an apt compar-
ison with Q8BERT. Notably, both BCT int8/fp32 and
Q8BERT exclusively quantize the embedding and Feed-
Forward Network (FFN) components of BERT. We de-
sign BCT int4/8 and BCT int8 to execute inference via
low-bit integer arithmetic operations. On the other hand,
our BCT fp8 model is configured to conduct inference
utilizing low-bit floating-point arithmetic, demanding the
least computational resources amongst the four models.
It is closely trailed by BCT int4/8 in resource efficiency.
Furthermore, we advocate for deploying the computation-
heavy encoder/decoder elements of transformers in the ac-
celerator. Concurrently, the embedding component, which
demands negligible computational resources but has a sig-
nificant impact on accuracy, can be computed in the CPU.
Table 2: Data Types of Four BCT Models
models embedding(w/i) Linear(w/b/i) MatMul(i) Softmax(i) LayerNorm(w/b/i) FFN(w/b/i) GELU(i)
BCT int8/fp32 int8/int8 fp32/fp32/fp32 fp32 fp32 fp32/fp32/fp32 int8/int8/int8 fp32
BCT int4/8 fp32/fp32 int4/int8/int8 int8 int8 int8/int8/int8 int4/int8/int8 int8
BCT int8 fp32/fp32 int8/int8/int8 int8 int8 int8/int8/int8 int8/int8/int8 int8
BCT fp8 fp32/fp32 fp8/fp8/fp8 fp8 fp8 fp8/fp8/fp8 fp8/fp8/fp8 fp8
we denote w as the data type of weight, b as the data type of bias and i as the data type of intermediate results.
4.3 Performance
We use the accuracy of SST-2, MNLI, RTE, and the Spear-
man correlation of STS-B as comparison metrics. Thebenchmark results of GLUE tasks are shown in Table 3.
Table 3: Benchmark Results of BCT
SST-2 STS-B MNLI-m RTE
Bert-base 91.74% 87.36% 83.61% 62.45%
BCT int8/fp32 91.86% 87.39% 83.44% 64.26%
BCT int8 92.2% 87.51% 82.14% 63.81%
BCT int4/8 90.94% 86.49% 80.08% 62.16%
BCT fp8 91.74% 87.36% 83.61% 62.45%
In the majority of tasks, BCT int8/fp32 exhibits a
lower decrease in accuracy compared to Q8BERT. This
can be attributed to the blockwise compression strat-
egy employed by BCT. Furthermore, BCT int4/8 demon-
strates a higher level of compression(7.988x) when com-pared to FQ-BERT(7.94x). For instance, while FQ-
BERT employs a 32-bit bias, BCT int4/8 utilizes an 8-
bit bias. Additionally, it is worth noting that BCT int4/8
showcases a mere 0.80% degradation in accuracy on the
SST-2 dataset, which is even less than the 0.81% re-
1https://github.com/huggingface/transformers
6

--- PAGE 7 ---
Table 4: Accuracy Degradation of Models
SST-2 STS-B MNLI-m RTE
Q8BERT -0.13% -0.65% - -1.32%
BCT int8/fp32 +0.12% +0.03% -0.17% +1.81%
FQ-BERT -0.81% - -3.61% -
BCT int8 +0.46% +0.15% -1.47% +1.36%
BCT int4/8 -0.80% -0.87% -3.53% -0.29%
BCT fp8 +0.00% +0.00% +0.00% +0.00%
duction observed in FQ-BERT, all without requiring re-
training. This substantiates the robustness and effective-
ness of BCT int4/8. Moreover, BCT int8 and BCT fp8
deliver outstanding performance across various datasets.
Through these advancements, BCT-based models have ex-
hibited a superior capability to compress models with min-
imal degradation in accuracy.
5 Conclusion
In this study, we introduced BCT that efficiently blockwise
compresses transformer-based models without retraining.
Unlike traditional compression methods that operate at
the layer level, BCT works at a finer block level across
the entire model. BCT’s compression using fp8 or int4
data types achieves a balance between performance and
resource consumption. It applies uniformly across all
components of the transformer model, effectively elim-
inating data distribution deviation caused by quantiza-
tion and negating the need for retraining. Furthermore,
BCT’s hardware-friendly shift compression method en-
hances computation efficiency. Practical benefits of BCT
are evident in deploying transformer-based models. A case
study showed BCT could compress a model to 7.988x of its
original size, maintaining competitive performance. Test-
ing on various GLUE datasets also demonstrated BCT’s
effectiveness, with less than a 0.9% degradation if accu-
racy in most tasks compared to alternative methods. Our
work confirms that BCT substantially lowers computa-
tional and memory demands while preserving near-original
model performance levels. In conclusion, BCT provides a
practical solution for the deployment of transformer-based
models, especially Language Learning Models (LLMs) like
ChatGPT and GPT4, overcoming technical and economic
challenges in numerous scenarios.
References
[Ba et al., 2016] Ba, J. L., Kiros, J. R., and Hinton,
G. E. (2016). Layer normalization. arXiv preprint
arXiv:1607.06450 .
[Brown et al., 2020] Brown, T., Mann, B., Ryder, N.,
Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan,
A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Lan-
guage models are few-shot learners. Advances in neural
information processing systems , 33:1877–1901.[Frantar and Alistarh, 2023] Frantar, E. and Alistarh, D.
(2023). Massive language models can be accurately
pruned in one-shot. arXiv preprint arXiv:2301.00774 .
[Hendrycks and Gimpel, 2016] Hendrycks, D. and Gim-
pel, K. (2016). Gaussian error linear units (gelus). arXiv
preprint arXiv:1606.08415 .
[Kim et al., 2021] Kim, S., Gholami, A., Yao, Z., Ma-
honey, M. W., and Keutzer, K. (2021). I-bert: Integer-
only bert quantization. In International conference on
machine learning , pages 5506–5518. PMLR.
[Liu et al., 2021] Liu, Z., Li, G., and Cheng, J. (2021).
Hardware acceleration of fully quantized bert for ef-
ficient natural language processing. In 2021 Design,
Automation & Test in Europe Conference & Exhibition
(DATE) , pages 513–516. IEEE.
[Miyashita et al., 2016] Miyashita, D., Lee, E. H., and
Murmann, B. (2016). Convolutional neural networks
using logarithmic data representation. arXiv preprint
arXiv:1603.01025 .
[Shen et al., 2020] Shen, S., Dong, Z., Ye, J., Ma, L., Yao,
Z., Gholami, A., Mahoney, M. W., and Keutzer, K.
(2020). Q-bert: Hessian based ultra low precision quan-
tization of bert. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 34, pages 8815–8821.
[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar,
N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L.,
and Polosukhin, I. (2017). Attention is all you need.
Advances in neural information processing systems , 30.
[Wang et al., 2018] Wang, A., Singh, A., Michael, J., Hill,
F., Levy, O., and Bowman, S. R. (2018). Glue: A multi-
task benchmark and analysis platform for natural lan-
guage understanding. arXiv preprint arXiv:1804.07461 .
[Xu et al., 2015] Xu, B., Wang, N., Chen, T., and Li, M.
(2015). Empirical Evaluation of Rectified Activations
in Convolutional Network. arXiv:1505.00853 [cs, stat].
[Zafrir et al., 2019] Zafrir, O., Boudoukh, G., Izsak, P.,
and Wasserblat, M. (2019). Q8bert: Quantized 8bit
bert. In 2019 Fifth Workshop on Energy Efficient Ma-
chine Learning and Cognitive Computing-NeurIPS Edi-
tion (EMC2-NIPS) , pages 36–39. IEEE.
7
