# 2309.14592.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2309.14592.pdf
# Kích thước file: 4799516 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
LƯỢNG TỬ HÓA SAU HUẤN LUYỆN HIỆU QUẢ VỚI ĐỊNH DẠNG FP8
Haihao Shen1Naveen Mellempudi2 *Xin He1Qun Gao1Chang Wang1Mengni Wang1

TÓM TẮT
Những tiến bộ gần đây trong các phương pháp học sâu như LLM và mô hình khuếch tán đã tạo ra nhu cầu về các phương pháp lượng tử hóa cải tiến có thể đáp ứng các yêu cầu tính toán của những kiến trúc hiện đại này trong khi vẫn duy trì độ chính xác. Hướng tới mục tiêu này, chúng tôi nghiên cứu các ưu điểm của định dạng dữ liệu FP8 cho lượng tử hóa sau huấn luyện trên 75 kiến trúc mạng độc đáo bao phủ một loạt rộng các nhiệm vụ, bao gồm dịch máy, mô hình ngôn ngữ, tạo văn bản, phân loại hình ảnh, tạo và phân đoạn. Chúng tôi xem xét ba biểu diễn FP8 khác nhau (E5M2, E4M3, và E3M4) để nghiên cứu ảnh hưởng của các mức độ đánh đổi khác nhau giữa dải động và độ chính xác đối với độ chính xác mô hình. Dựa trên nghiên cứu rộng rãi của chúng tôi, chúng tôi đã phát triển một quy trình lượng tử hóa tổng quát hóa trên các kiến trúc mạng khác nhau. Kết quả thực nghiệm của chúng tôi cho thấy định dạng FP8 vượt trội hơn INT8 ở nhiều khía cạnh, bao gồm độ bao phủ khối lượng công việc (92.64% so với 65.87%), độ chính xác mô hình và sự phù hợp cho một loạt rộng hơn các phép toán. Hơn nữa, những phát hiện của chúng tôi cho thấy E4M3 phù hợp hơn cho các mô hình NLP, trong khi E3M4 hoạt động tốt hơn một chút so với E4M3 trên các nhiệm vụ thị giác máy tính.

1 GIỚI THIỆU
Lượng tử hóa là quá trình giảm độ chính xác số của trọng số và kích hoạt của mạng neural để giảm chi phí tính toán của suy luận. Lượng tử hóa INT8 (Vanhoucke et al., 2011; Han et al., 2015a) là lựa chọn được chấp nhận rộng rãi nhất hiện nay do khả năng cung cấp hiệu suất suy luận cao trên phần cứng học sâu hiện đại trong khi duy trì độ chính xác mô hình hợp lý. Nó đặc biệt hiệu quả cho các nhiệm vụ thị giác máy tính như phát hiện đối tượng và phân loại hình ảnh, và đã được triển khai rộng rãi trong sản xuất cả ở quy mô trung tâm dữ liệu và trên các thiết bị biên có tài nguyên hạn chế. Tuy nhiên, INT8 gặp phải một số thách thức phát sinh do dải động hạn chế của nó. Một số kỹ thuật lượng tử hóa đã được phát triển để giải quyết những thách thức này. Ví dụ, lượng tử hóa bất đối xứng (Jacob et al., 2018; Krishnamoorthi, 2018; Bhalgat et al., 2020) phân bổ các số bit khác nhau cho dải dương và âm với một độ lệch khác không, để biểu diễn tốt hơn phân phối của các giá trị gốc. Các phương pháp lượng tử hóa không đồng nhất (Miyashita et al., 2016; Zhou et al., 2017; Cai et al., 2017; Fang et al., 2020; Li et al., 2020) cố gắng gán độ chính xác cao hơn cho các phần dữ liệu được coi là quan trọng hơn để giảm lỗi lượng tử hóa. Các phương pháp sử dụng tỷ lệ theo nhóm (Zhou et al., 2016; Mellempudi et al., 2017) hoặc theo kênh (Jacob et al., 2018; Krishnamoorthi, 2018) mở rộng dải động hiệu quả bằng cách sử dụng hệ số tỷ lệ độc lập cho mỗi nhóm phần tử được chọn. Dải động hạn chế của INT8 cũng dẫn đến biểu diễn kém các giá trị ngoại lệ thường được tìm thấy trong các kích hoạt. Điều này đặc biệt phổ biến trong các Mô hình Ngôn ngữ Lớn (LLM), nơi các giá trị ngoại lệ lớn hơn đáng kể so với phần còn lại của các kích hoạt. Cách tiếp cận phổ biến nhất để xử lý các giá trị ngoại lệ là cắt chúng bằng các giá trị ngưỡng được thu thập thông qua hiệu chuẩn (Sung et al., 2015; Zhao et al., 2019b) hoặc học trong quá trình huấn luyện (Bhalgat et al., 2020; Choi et al., 2018; Esser et al., 2020; Zhang et al., 2018a). Gần đây hơn (Wei et al., 2022; Xiao et al., 2022) đã đề xuất áp dụng các biến đổi toán học để phân phối lại độ lớn của các giá trị ngoại lệ giữa các tensor trọng số và kích hoạt để giảm thiểu tác động của chúng. Mặc dù có những tiến bộ này, các phương pháp INT8 vẫn không hiệu quả cho một loạt rộng các nhiệm vụ mô hình ngôn ngữ, nơi sự hiện diện của LayerNorm đã được chỉ ra làm khuếch đại sự xuất hiện của các giá trị ngoại lệ (Wei et al., 2022). Do đó, một tỷ lệ đáng kể của những khối lượng công việc này quay lại sử dụng độ chính xác cao hơn để bảo tồn độ chính xác mô hình.

Bài báo này lập luận rằng các định dạng điểm nổi 8-bit (FP8) là một giải pháp thay thế hiệu quả và năng suất hơn cho INT8 để lượng tử hóa mạng neural sâu. Chúng tôi đánh giá ba biểu diễn khác nhau (E5M2, E4M3, và E3M4) cung cấp các mức độ đánh đổi khác nhau giữa dải động và độ chính xác. Bảng 1 hiển thị chi tiết của định dạng nhị phân và mã hóa giá trị đặc biệt. Nghiên cứu tập trung vào lợi ích của các định dạng FP8 cho lượng tử hóa sau huấn luyện như cách tiếp cận được ưa chuộng sử dụng trong sản xuất. Chúng tôi đã phát triển các quy trình lượng tử hóa tổng quát hóa trên các kiến trúc mạng khác nhau và tiến hành thí nghiệm trên 75 mạng bao phủ một loạt rộng các lĩnh vực ứng dụng. Kết quả của chúng tôi cho thấy các định dạng FP8 tổng thể cung cấp độ chính xác cao hơn, độ bao phủ khối lượng công việc tốt hơn so với INT8 (92.64% so với 65.87%) và có thể xử lý nhiều phép toán hơn như LayerNorm và BatchNorm. Dữ liệu cũng cho thấy E4M3 phù hợp hơn cho một loạt rộng các mô hình NLP với độ bao phủ 96.32% so với E3M4 (92.11%), trong khi E3M4 hoạt động tốt hơn một chút trên các mô hình thị giác máy tính với độ bao phủ 78.95% so với E4M3 (73.68%). Đóng góp của chúng tôi như sau:

•Đề xuất một luồng lượng tử hóa FP8 thống nhất và có thể mở rộng hoạt động trên các lĩnh vực ứng dụng và kích thước mô hình khác nhau. Theo hiểu biết tốt nhất của chúng tôi, công trình của chúng tôi là đầu tiên nghiên cứu vấn đề này trên hơn 200 nhiệm vụ và hơn 75 mô hình thể hiện khả năng mở rộng của cách tiếp cận của chúng tôi.

•Chứng minh các ưu điểm của định dạng FP8 so với INT8, về độ bao phủ khối lượng công việc, độ chính xác mô hình và sự phù hợp cho một loạt rộng hơn các phép toán. Công trình của chúng tôi cũng là nghiên cứu đầu tiên thể hiện điều chỉnh mô hình tự động dựa trên độ chính xác cho lượng tử hóa.

•Đề xuất rằng E4M3 phù hợp hơn cho các mô hình NLP, trong khi E3M4 hoạt động tốt hơn một chút so với E4M3 trên các nhiệm vụ thị giác máy tính.

1.1 Công trình liên quan

Có một khối lượng nghiên cứu ngày càng tăng đang nghiên cứu việc sử dụng các định dạng điểm nổi 8-bit để tăng tốc các nhiệm vụ huấn luyện và suy luận học sâu. Các nghiên cứu ban đầu của (Wang et al., 2018) và (Mellempudi et al., 2019) tập trung vào định dạng E5M2 cho các nhiệm vụ huấn luyện do dải động rộng hơn của nó cần thiết để biểu diễn các giá trị gradient. (Sun et al., 2019) sau đó đề xuất sử dụng kết hợp hai định dạng nhị phân, E5M2 và E4M3, cho huấn luyện và mở rộng nghiên cứu của họ để bao gồm các nhiệm vụ suy luận. Họ cũng đề xuất sử dụng độ lệch số mũ để dịch chuyển dải số của định dạng E4M3 để xử lý các giá trị ngoại lệ trong kích hoạt. Các nghiên cứu sau này của (Noune et al., 2022) và (Kuzmin et al., 2022) đã mở rộng phạm vi này để bao gồm độ lệch số mũ biến đổi và các định dạng với ít bit số mũ hơn, như E3M4 và E2M5. Gần đây hơn, (Micikevicius et al., 2022) đã trình bày một phương pháp huấn luyện tổng quát sử dụng tỷ lệ theo tensor bằng định dạng E5M2 và E4M3. Họ cũng mở rộng các nghiên cứu suy luận để bao phủ các mô hình ngôn ngữ lớn như GPT-3 (6.7B).

Phần còn lại của bài báo này được tổ chức như sau. Phần 2 thảo luận về các ưu điểm của biểu diễn điểm nổi 8-bit trong việc xử lý các giá trị ngoại lệ. Phần 3 giới thiệu quy trình lượng tử hóa và các thành phần của một lược đồ lượng tử hóa tiêu chuẩn, mở rộng và một framework để điều chỉnh hiệu suất mô hình. Phần 4 phác thảo thiết lập thí nghiệm, trình bày kết quả độ chính xác, và đưa ra thảo luận về điều chỉnh hiệu suất. Phần 5 trình bày kết luận và công việc tương lai.

2 NỀN TẢNG

Phân phối giá trị FP8 và lỗi lượng tử hóa:
Các định dạng điểm nổi có thể biểu thị một dải động lớn các giá trị bằng cách sử dụng kết hợp mantissa và số mũ. Một tập hợp các số điểm nổi trong X∈R được biểu thị như sau:

x= (−1)s×22e−b×(1 +f1×2−1+f2×2−2+...+fm×2−m)(1)

trong đó s∈ {0,1} là dấu, e là độ rộng bit số mũ và fi∈ {0,1} là mantissa hoặc phần thập phân m-bit.

Dải động của một định dạng điểm nổi được xác định bởi độ rộng của số mũ của nó. Giá trị số mũ được biểu thị dưới dạng lũy thừa của 2 và phục vụ như một hệ số tỷ lệ cho mantissa. Điều này có nghĩa là các số điểm nổi không được phân bố đều, mà có kích bước nhỏ hơn quanh zero tăng lên theo độ lớn của giá trị được biểu diễn. Điều này cho phép các định dạng điểm nổi biểu diễn các giá trị nhỏ hơn với độ chính xác tốt hơn.

Độ rộng của mantissa xác định số lượng điểm lưới được biểu diễn cho mỗi bước gia tăng của số mũ, điều này lần lượt ảnh hưởng đến độ chính xác của định dạng. Những đặc tính này cho phép các định dạng điểm nổi hỗ trợ dải động cao hơn mà không làm giảm độ chính xác của các giá trị nhỏ hơn, khiến chúng phù hợp để biểu diễn nhiều mẫu dữ liệu thường xuyên xuất hiện trong các khối lượng công việc học sâu thể hiện phân phối chuẩn đuôi dài.

Hình 1 minh họa sự khác biệt trong phân phối các giá trị lượng tử hóa và tác động của các giá trị ngoại lệ đối với cả định dạng FP8 và INT8. Trong biểu đồ giữa, các định dạng FP8 cho thấy sự tập trung lớn hơn của các điểm lưới ở giữa phân phối, cho thấy một vùng có độ chính xác cao hơn gần zero. Dải độ chính xác cao rộng hơn đối với các định dạng có nhiều bit mantissa hơn, cho phép chúng biểu diễn tỷ lệ phần trăm lớn hơn của vùng 3σ của dữ liệu gốc với độ chính xác cao hơn. Ngược lại, lượng tử hóa INT8 hoạt động với kích bước cố định được xác định bởi giá trị lớn nhất có trong dữ liệu đầu vào. Điều này có nghĩa là các giá trị ngoại lệ có thể ảnh hưởng đáng kể đến kích bước bằng cách kéo dãn lưới lượng tử hóa, dẫn đến ít điểm lưới hơn dưới vùng 3σ. Điều này được phản ánh trong lỗi lượng tử hóa tổng thể (MSE) được hiển thị bên phải, nơi các định dạng E4M3 và E3M4 đã vượt trội đáng kể so với INT8, trong khi E5M2 hoạt động kém hơn vì nó có ít bit mantissa hơn.

--- TRANG 2 ---
Lượng tử hóa sau huấn luyện hiệu quả với định dạng FP8

Bảng 1. Định dạng nhị phân FP8: Ký hiệu EeMm biểu thị phân bổ bit cho Số mũ (e) và Mantissa (m) tương ứng. Các định dạng hỗ trợ bit dấu và bit dẫn đầu ngầm trong mantissa. E5M2 tuân theo các quy tắc mã hóa giống IEEE, trong khi E4M3 và E3M4 sử dụng mã hóa mở rộng để lấy lại ±Infinity cho mã hóa hữu ích, một chuỗi bit duy nhất của tất cả-ones biểu thị NaN.

[Bảng thông số kỹ thuật của E5M2, E4M3, E3M4]

3 QUY TRÌNH LƯỢNG TỬ HÓA

Có một số thách thức trong việc tạo ra một lược đồ lượng tử hóa tổng quát có thể được áp dụng cho các mạng trên nhiều lĩnh vực ứng dụng và liên quan đến nhiều định dạng dữ liệu. Các mạng có thể có các yêu cầu khác nhau về dải động, độ chính xác và có thể chứa các phép toán nhạy cảm với lượng tử hóa. Để tạo điều kiện tổng quát hóa, lược đồ lượng tử hóa phải có khả năng hỗ trợ một tập hợp rộng các phép toán phổ biến, đồng thời có khả năng thích ứng để đáp ứng các yêu cầu độc đáo của các ứng dụng khác nhau. Framework của chúng tôi hoàn thành điều này bằng cách kết hợp cả lược đồ lượng tử hóa tiêu chuẩn có thể được áp dụng rộng rãi, cũng như lược đồ lượng tử hóa mở rộng tối ưu hóa các phép toán cụ thể thông qua một quá trình điều chỉnh lặp. Hình 2 mô tả quy trình cấp cao cho lượng tử hóa FP8 sau huấn luyện. Lược đồ lượng tử hóa tiêu chuẩn là cấu hình mặc định được áp dụng cho tập hợp phổ biến các toán tử trên các kiến trúc khác nhau, trong khi lược đồ mở rộng cụ thể cho một kiến trúc và được áp dụng từng bước trong một vòng lặp phản hồi.

Sơ đồ luồng trong Hình 2 cũng bao gồm một bước Hiệu chuẩn BatchNorm bổ sung được áp dụng chỉ cho các mô hình thị giác máy tính. (Sun et al., 2019) đã chỉ ra rằng việc điều chỉnh lại các tham số BatchNorm (mean và variance) để bù đắp cho sự dịch chuyển phương sai do lượng tử hóa gây ra, đã cải thiện đáng kể độ chính xác suy luận. Thêm vào đó, xin lưu ý rằng E5M2 sử dụng lượng tử hóa trực tiếp và không yêu cầu Hiệu chuẩn Dải vì nó có dải động đủ để xử lý các giá trị ngoại lệ. Đối với các định dạng E4M3 và E3M4, chúng tôi thấy tỷ lệ max đơn giản là đủ để xử lý các giá trị ngoại lệ. Chúng tôi cũng đã xem xét các phương pháp hiệu chuẩn dải phức tạp hơn như phân kỳ KL (Darvish Rouhani et al., 2020; Migacz, 2017), lỗi MSE (Choukroun et al., 2019; Zhao et al., 2019a) và percentile (Gholami et al., 2021) nhưng không cung cấp bất kỳ lợi ích bổ sung nào.

3.1 Lược đồ lượng tử hóa tiêu chuẩn

Phần này phác thảo các thành phần của lược đồ lượng tử hóa tiêu chuẩn, được rút ra từ các nghiên cứu rộng rãi của chúng tôi được tiến hành trên một số nhiệm vụ học sâu trên nhiều lĩnh vực ứng dụng. Lược đồ này được áp dụng cho tập con phổ biến của các toán tử bao gồm Convolution, Linear và Embedding. Lược đồ này cũng giống hệt với lược đồ lượng tử hóa INT8, cho phép so sánh độ chính xác công bằng.

Tỷ lệ Trọng số và Kích hoạt: Chúng tôi khuyến nghị sử dụng tỷ lệ theo kênh cho trọng số trên tất cả các mạng. Mặc dù các định dạng FP8 có dải động đủ để xử lý các phân phối trọng số phổ biến, bằng chứng thực nghiệm cho thấy áp dụng tỷ lệ theo kênh có thể giảm lỗi làm tròn bằng cách sử dụng hiệu quả toàn bộ không gian mã hóa cho mỗi kênh. Tương tự, chúng tôi thấy tỷ lệ theo tensor là đủ để xử lý các giá trị ngoại lệ bằng định dạng FP8. Các hệ số tỷ lệ được tính như sau:

s= (float max/max T) (2)

trong đó float max là giá trị tối đa có thể biểu diễn của định dạng FP8 được chọn, và max T là giá trị absmax được hiệu chuẩn của tensor. Một số nghiên cứu gần đây (Xiao et al., 2022; Wei et al., 2022; Dettmers et al., 2022) đã chỉ ra rằng tỷ lệ kích hoạt theo kênh có thể có lợi cho lượng tử hóa INT8. Tuy nhiên, các phương pháp như vậy có thể yêu cầu triển khai kernel đặc biệt có khả năng phát sinh chi phí tính toán cao hơn, do đó chúng không được bao gồm trong nghiên cứu của chúng tôi.

Toán tử Đầu tiên và Cuối cùng: Các nghiên cứu trước đây (Han et al., 2015b; Choi et al., 2018; Micikevicius et al., 2022) về mạng convolution đã chỉ ra rằng convolution đầu tiên và các lớp fully-connected cuối cùng nhạy cảm hơn với lượng tử hóa. Hai toán tử này thường chiếm <1% tổng tính toán. Do đó, chúng tôi tiếp tục duy trì các lớp này ở độ chính xác cao hơn để bảo tồn độ chính xác mô hình. Xin lưu ý rằng ngoại lệ này chỉ áp dụng cho mạng neural convolution.

3.2 Lược đồ lượng tử hóa mở rộng

Phần này phác thảo lược đồ lượng tử hóa được áp dụng có chọn lọc để giải quyết các nhu cầu cụ thể của một ứng dụng. Các phương pháp này được áp dụng từng bước để tối đa hóa hiệu quả mô hình trong khi bảo tồn độ chính xác.

Mở rộng Phạm vi Toán tử: Mạng neural dành một phần đáng kể thời gian thực thi của chúng trong các phép toán bị ràng buộc bởi bộ nhớ như LayerNorm, BatchNorm và các toán tử theo phần tử như Add và Mul. Các nỗ lực trước đây Bhandare et al. (2019); Kim et al. (2021) để lượng tử hóa các toán tử này bằng xấp xỉ số nguyên không thành công trong việc duy trì độ chính xác mô hình. Các thí nghiệm của chúng tôi cho thấy các định dạng FP8 có khả năng xử lý các toán tử này mà không hy sinh độ chính xác mô hình.

Định dạng FP8 hỗn hợp: Các phân phối dữ liệu của trọng số và kích hoạt có thể khác nhau tùy thuộc vào kiến trúc của mô hình và tập dữ liệu mà nó được huấn luyện. Hình 3 hiển thị các phân phối điển hình của tensor trọng số và kích hoạt trong các khối lượng công việc NLP và thị giác máy tính. Các phân phối trọng số trong cả hai lớp mô hình có xu hướng tuân theo phân phối chuẩn với nhiều giá trị gần zero. Các tensor này yêu cầu nhiều bit mantissa hơn trong định dạng dữ liệu để biểu diễn phân phối một cách chính xác. Ngược lại, các kích hoạt của mô hình NLP cho thấy nhiều giá trị ngoại lệ đòi hỏi một dải động lớn hơn trong định dạng dữ liệu để đảm bảo các giá trị ngoại lệ được biểu diễn chính xác. Chúng tôi cân bằng sự đánh đổi này bằng cách gán định dạng E5M2 hoặc E4M3 cho các tensor bị ràng buộc dải và E3M4 cho các tensor bị ràng buộc độ chính xác.

Lượng tử hóa Tĩnh so với Động: Chúng tôi sử dụng lượng tử hóa tĩnh như phương pháp mặc định trong suốt nghiên cứu của chúng tôi vì nó hiệu quả hơn về mặt tính toán. Tuy nhiên, chúng tôi đã nghiên cứu tác động độ chính xác của lượng tử hóa động trên tất cả các định dạng FP8 và thấy rằng nó không mang lại lợi ích bổ sung cho E5M2 nhưng quan sát thấy một cải thiện đáng chú ý về độ chính xác cho các định dạng E4M3 và E3M4 trên các mô hình được chọn.

--- TRANG 3 ---
Lượng tử hóa sau huấn luyện hiệu quả với định dạng FP8

Hình 3. Phân phối Tensor: (trái) kích hoạt trong khối lượng công việc NLP chứa các giá trị ngoại lệ, do đó chúng bị ràng buộc dải, (giữa) Kích hoạt trong khối lượng công việc CV có xu hướng bị ràng buộc độ chính xác, (phải) Tensor trọng số từ cả mạng CV & NLP có xu hướng bị ràng buộc độ chính xác.

4 KẾT QUẢ

4.1 Thiết lập thí nghiệm

Chúng tôi thể hiện kết quả lượng tử hóa FP8 bằng cách sử dụng một framework mô phỏng phần mềm chứa hai thành phần chính, mô phỏng kiểu dữ liệu và lượng tử hóa mô hình. Để mô phỏng kiểu dữ liệu, chúng tôi sử dụng FP8 Emulation Toolkit, cung cấp triển khai tham chiếu chạy trên phần cứng FP32. Chúng tôi tận dụng Neural Compressor để thực hiện lượng tử hóa mô hình bằng cách kết hợp cả lược đồ lượng tử hóa tiêu chuẩn và mở rộng, cùng với các phương pháp lượng tử hóa cụ thể FP8 như hiệu chuẩn BatchNorm và hỗ trợ cho các định dạng FP8 hỗn hợp. Framework của chúng tôi hỗ trợ một loạt rộng các toán tử được lượng tử hóa, bao gồm các toán tử tính toán như Convolution, Linear, MatMul, BatchMatMul và các toán tử bộ nhớ như Embedding, BatchNorm, LayerNorm, Add và Mul.

Chúng tôi đánh giá các phương pháp lượng tử hóa của chúng tôi trên hơn 200 nhiệm vụ khác nhau, sử dụng 75 kiến trúc mô hình độc đáo và hơn 20 tập dữ liệu khác nhau. Các mô hình được chọn ngẫu nhiên từ một pool kết hợp giữa tính đa dạng và độ phổ biến từ các hub chính như Hugging Face Models và Torch Vision, cũng như các mô hình riêng lẻ từ Github dựa trên độ phổ biến của chúng. Sau đây là danh sách một phần các khối lượng công việc được phân loại rộng rãi dưới Xử lý Ngôn ngữ Tự nhiên (NLP) và Thị giác Máy tính (CV).

Văn bản và Xử lý Ngôn ngữ Tự nhiên: Chúng tôi đã đánh giá 38 mạng khác nhau trong danh mục này trên một loạt rộng các nhiệm vụ NLP, có thể được chia nhỏ thêm như sau:

• Mô hình ngôn ngữ tạo sinh. Chúng tôi đánh giá Bloom (Scao et al., 2022) và LLaMA (Touvron et al., 2023), hai LLM mã nguồn mở đại diện, và đánh giá độ chính xác bằng lambada-openai.

• Phân loại văn bản. Chúng tôi đánh giá hơn 30 mạng khác nhau (ví dụ, Bert-Large (Devlin et al., 2018), DistilBert (Sanh et al., 2019), Longformer (Beltagy et al., 2020)) trên nhiều nhiệm vụ khác nhau (ví dụ, mrpc, cola, sts-b, sst2).

• Tóm tắt. Chúng tôi đo độ chính xác của pegasus (Zhang et al., 2020) trên tập dữ liệu samsum.

• Các nhiệm vụ NLP khác. Một số mô hình được chọn khác như MarianMT (Junczys-Dowmunt et al., 2018) cho dịch máy neural và DialogGPT (Zhang et al., 2019) cho mô hình ngôn ngữ trên tập dữ liệu WMT ENRO và wikitext.

Hình ảnh và Thị giác Máy tính: Chúng tôi đánh giá 34 mạng khác nhau trên các nhiệm vụ thị giác máy tính khác nhau từ các danh mục sau.

• Tạo hình ảnh. Chúng tôi đánh giá Stable Diffusion, một mô hình khuếch tán latent text-to-image tiên tiến mã nguồn mở và đánh giá bằng FID (Heusel et al., 2017).

• Phân loại hình ảnh. Chúng tôi đánh giá một loạt rộng mạng neural convolution (CNN) như VGG (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al., 2015), ResNet (He et al., 2016), ShuffleNet (Zhang et al., 2018b), EfficientNet (Tan & Le, 2019), và các mô hình vision dựa trên Transformer như ViT (Dosovitskiy et al., 2020) trên ImageNet ILSVRC 2012 và CIFAR-10.

• Phân đoạn hình ảnh & phát hiện đối tượng. Chúng tôi chọn các mô hình điển hình như U-Net (Ronneberger et al., 2015) cho phân đoạn hình ảnh sử dụng tập dữ liệu từ Kaggle Carvana Image Masking Challenge (Shaler et al., 2017) và YoloV3 (Redmon & Farhadi, 2018) cho phát hiện đối tượng sử dụng COCO2014 (Lin et al., 2014).

Âm thanh và Xử lý Tiếng nói: Chúng tôi đánh giá hai mô hình HuBERT (Hsu et al., 2021) và wav2vec 2.0 (Baevski et al., 2020) cho nhận dạng giọng nói và đánh giá độ chính xác bằng LibriSpeech (Panayotov et al., 2015).

Hệ thống Gợi ý: Chúng tôi đánh giá Mô hình Gợi ý Học sâu (DLRM) (Naumov et al., 2019) và đo độ chính xác trên Criteo Terabyte.

4.2 Kết quả lượng tử hóa

4.2.1 Độ chính xác

Lưu ý rằng tỷ lệ đạt trong Bảng 2 là tỷ lệ phần trăm khối lượng công việc đáp ứng tiêu chí độ chính xác mất mát tương đối 1% so với baseline FP32. SmoothQuant Xiao et al. (2022) được kích hoạt trên các mô hình NLP với giá trị alpha làm mượt mặc định (điều chỉnh alpha nằm ngoài phạm vi của bài báo này). Hình 4 minh họa tính biến thiên của mất mát độ chính xác cho các định dạng dữ liệu khác nhau trên các khối lượng công việc CV và NLP.

Bảng 3 hiển thị độ chính xác của một số mẫu đại diện từ tất cả khối lượng công việc CV và NLP. Hình 5 hiển thị mất mát độ chính xác của tất cả khối lượng công việc được sắp xếp theo kích thước mô hình theo thứ tự tăng dần.

4.2.2 Chất lượng tạo sinh

Hình 6 hiển thị hình ảnh được tạo bởi Stable Diffusion với prompt "A photo of an astronaut riding a horse on Mars". Phân tích chủ quan của chúng tôi cho thấy các định dạng FP8 đạt được chất lượng hình ảnh vượt trội so với INT8, như được chỉ ra bởi mũi tên xanh. Thêm vào đó, E4M3 và E3M4 tạo ra hình ảnh mượt mà hơn và tạo ra các chi tiết phức tạp hơn, đặc biệt là trên phi hành gia. Chúng tôi sử dụng điểm FID để so sánh chất lượng hình ảnh được tạo (thấp hơn là tốt hơn) và thấy rằng điểm FID phù hợp với đánh giá chủ quan của chúng tôi. Thêm mẫu về Stable Diffusion được hiển thị trong Phụ lục A.2.

Bảng 4 hiển thị văn bản mẫu được tạo bởi Bloom trên prompt với 32 token đầu vào sử dụng beam search kích thước 4. Với prompt làm đầu vào, bạn có thể thấy E3M4 cho thấy phản hồi tốt hơn INT8 với nội dung toàn diện hơn và ít token lặp lại (ví dụ, saw many strange). Phụ lục A.3 hiển thị đầu ra đầy đủ trên định dạng dữ liệu và cách tiếp cận lượng tử hóa khác nhau.

4.3 Thảo luận

4.3.1 Lược đồ lượng tử hóa tiêu chuẩn

Lượng tử hóa Toán tử Đầu tiên và Cuối cùng: Đối với mạng convolution, lượng tử hóa toán tử đầu tiên và cuối cùng giảm Tỷ lệ đạt cho định dạng E5M2 và E4M3 lần lượt 25% và 15%. Tuy nhiên, E3M4 có thể duy trì Tỷ lệ đạt 70% ngay cả khi toán tử đầu tiên và cuối cùng được lượng tử hóa. Do đó, chúng tôi khuyến nghị việc kích hoạt toán tử đầu tiên và cuối cùng cho lượng tử hóa FP8 như một tùy chọn điều chỉnh.

Hiệu chuẩn BatchNorm: Chúng tôi sử dụng tăng cường dữ liệu để nâng cao tính đa dạng đặc trưng của dữ liệu hiệu chuẩn ảnh hưởng đến chất lượng thống kê BatchNorm và độ chính xác mô hình. Hình 7 so sánh hiệu quả của các phương pháp tăng cường dữ liệu huấn luyện và suy luận trong việc bảo tồn độ chính xác mô hình ở các kích thước mẫu dữ liệu hiệu chuẩn khác nhau. Chúng tôi thấy biến đổi huấn luyện hiệu quả hơn ngay cả ở kích thước mẫu nhỏ hơn (<3K). Tuy nhiên, chúng tôi khuyến nghị kích thước mẫu 3K với biến đổi huấn luyện để đạt kết quả tốt nhất trên một loạt rộng mạng.

4.3.2 Lược đồ lượng tử hóa mở rộng

Định dạng FP8 hỗn hợp: Hình 8 minh họa cách sử dụng định dạng FP8 hỗn hợp trên đầu vào có thể tác động đến lỗi lượng tử hóa của đầu ra của toán tử Linear từ mô hình BERT-base (MPRC). Các thí nghiệm của chúng tôi cho thấy sử dụng E4M3 cho kích hoạt và E3M4 cho trọng số tạo ra kết quả độ chính xác tốt nhất trên một loạt khối lượng công việc NLP. Các cải thiện độ chính xác đạt được bởi lược đồ này cho mô hình Bert, Funnel, và Longformer được trình bày trong Bảng 5.

Mở rộng Phạm vi Toán tử: Hình 9 có kết quả từ các nghiên cứu lượng tử hóa của chúng tôi mở rộng đến một loạt rộng hơn các toán tử như BatchMatMul, MatMul, Embedding và LayerNorm. Kết quả của chúng tôi cho thấy E4M3 đạt được độ chính xác tổng thể tốt hơn và biến thiên nhỏ hơn trong mất mát độ chính xác trên một loạt rộng các nhiệm vụ NLP.

Lượng tử hóa Tĩnh so với Động: Trong khi lượng tử hóa tĩnh là cách tiếp cận mặc định trong công thức của chúng tôi, chúng tôi cũng nghiên cứu tác động của lượng tử hóa động đối với độ chính xác mô hình. Kết quả cho thấy lượng tử hóa động có thể cải thiện độ chính xác của mô hình NLP khi lượng tử hóa với định dạng E4M3 và E3M4 như được hiển thị trong Bảng 6.

5 TÓM TẮT VÀ CÔNG VIỆC TƯƠNG LAI

Chúng tôi trình bày một tập hợp công thức lượng tử hóa sau huấn luyện cho suy luận FP8 và chứng minh hiệu quả trên 75 kiến trúc mạng độc đáo bao phủ một loạt rộng các nhiệm vụ như mô hình ngôn ngữ, tạo văn bản, phân loại và tạo hình ảnh. Chúng tôi khuyến nghị E3M4 và E4M3 như định dạng FP8 mặc định cho mô hình CV và NLP tương ứng, trong khi các công thức bổ sung như định dạng FP8 hỗn hợp và mở rộng phạm vi toán tử FP8 đáng để khám phá để tạo ra một mô hình FP8 tối ưu. Như công việc tương lai của chúng tôi, chúng tôi dự định áp dụng công thức lượng tử hóa FP8 cho các mô hình LLM đa dạng hơn (ví dụ, BioGPT (Luo et al., 2022), Llama2 Chat (Touvron et al., 2023), Code Llama (Rozière et al., 2023)), và đóng góp công thức và triển khai của chúng tôi cho cộng đồng mã nguồn mở.

[Phần còn lại của tài liệu bao gồm các bảng kết quả chi tiết, đồ thị, và tài liệu tham khảo]
