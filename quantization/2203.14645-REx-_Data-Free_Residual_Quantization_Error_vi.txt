Tài liệu tham khảo
[1] Kaiming He, Xiangyu Zhang, et al. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016.
[2] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV, pages 21-37. Springer, 2016.
[3] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.
[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[5] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018.
[6] Matthieu Courbariaux, Itay Hubara, et al. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. NeurIPS, 2016.
[7] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. ICLR, 2018.
[8] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In CVPR, pages 2704-2713, 2018.
[9] Jan Achterhold, Jan Mathias Koehler, Anke Schmeink, and Tim Genewein. Variational network quantization. In ICLR, 2018.
[10] Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling. Relaxed quantization for discretized neural networks. ICLR, 2018.
[11] Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, and Mickey Aleksic. A quantization-friendly separable convolution for mobilenets. In 2018 1st Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2), pages 14-18. IEEE, 2018.
[12] Kanghyun Choi, Hye Yoon Lee, Deokki Hong, Joonsang Yu, Noseong Park, Youngsok Kim, and Jinho Lee. It's all in the teacher: Zero-shot quantization brought closer to the teacher. In CVPR, pages 8311-8321, 2022.
[13] Yunshan Zhong, Mingbao Lin, Gongrui Nan, Jianzhuang Liu, Baochang Zhang, Yonghong Tian, and Rongrong Ji. Intraq: Learning synthetic images with intra-class heterogeneity for zero-shot network quantization. In CVPR, pages 12339-12348, 2022.
[14] Markus Nagel, Mart van Baalen, et al. Data-free quantization through weight equalization and bias correction. In ICCV, pages 1325-1334, 2019.
[15] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Recovering neural network quantization error through weight factorization. In ICML, pages 4486-4495, 2019.
[16] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In ICML, pages 7543-7552, 2019.
[17] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In CVPR, pages 13169-13178, 2020.
[18] Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantization. In CVPR, pages 15658-15667, 2021.
[19] Guo Cong et al. Squant: On-the-fly data-free quantization via diagonal hessian approximation. ICLR, 2022.
[20] Nvidia. Nvidia a100 tensor core gpu architecture. web tech report (https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf), 2021.
[21] Majid Rabbani. Jpeg2000: Image compression fundamentals, standards and practice. Journal of Electronic Imaging, 11(2):286, 2002.
[22] Stephane G Mallat. A theory for multiresolution signal decomposition: the wavelet representation. In Fundamental Papers in Wavelet Theory, pages 494-513. Princeton University Press, 2009.
[23] Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression. ICLR, 2017.

--- TRANG 8 ---
[24] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.
[25] Sangyun Oh, Hyeonuk Sim, Sugil Lee, and Jongeun Lee. Automated log-scale quantization for low-cost deep neural networks. In CVPR, pages 742-751, 2021.
[26] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel static input quantization. WACV, 2023.
[27] Yuhang Li, Feng Zhu, Ruihao Gong, Mingzhu Shen, Xin Dong, Fengwei Yu, Shaoqing Lu, and Shi Gu. Mixmix: All you need for data-free compression are feature and data mixing. In ICCV, pages 4410-4419, 2021.
[28] Boyuan Feng, Yuke Wang, Tong Geng, Ang Li, and Yufei Ding. Apnn-tc: Accelerating arbitrary precision neural networks on ampere gpu tensor cores. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-13, 2021.
[29] Cliff Robinson. Untether.ai boqueria 1458 risc-v core ai accelerator, Aug 2022.
[30] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.
[31] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
[32] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Red: Looking for redundancies for data-free structured compression of deep neural networks. NeurIPS, 2021.
[33] J. Deng, W. Dong, et al. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009.
[34] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html, 2012.
[35] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, pages 3213-3223, 2016.
[36] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355. Association for Computational Linguistics, 2018.
[37] Edouard Yvinec, Arnaud Dapogny, and Kevin Bailly. To fold or not to fold: a necessary and sufficient condition on batch-normalization layers folding. IJCAI, 2022.
[38] Intel. Intel® distribution of openvino™ toolkit. Intel, 2022.
[39] Nvidia. Nvidia distribution of tensorrt toolkit. Nvidia, 2022.
[40] Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan. Generative low-bitwidth data free quantization. In ECCV, pages 1-17. Springer, 2020.
[41] Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016.
[42] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
[43] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.
[44] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Powerquant: Automorphism search for non-uniform quantization. In ICLR, 2023.
[45] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. BMVC 2014, 2014.
[46] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In ICML, pages 7197-7206. PMLR, 2020.
[47] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. NeurIPS, 2021.
[48] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.

--- TRANG 9 ---
[49] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In AAAI, volume 34, pages 7432-7439, 2020.
[50] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.
[51] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.
[52] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.
[53] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.
[54] Mark Sandler, Andrew Howard, et al. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pages 4510-4520, 2018.
[55] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, pages 801-818, 2018.
[56] Erica Klarreich. Multiplication hits the speed limit. Communications of the ACM, 63(1):11-13, 2019.

--- TRANG 10 ---
A Hội tụ theo cấp số nhân
Sự hội tụ theo cấp số nhân có thể được chứng minh cho hai phương pháp: mở rộng và mở rộng thưa thớt. Trước tiên chúng tôi chứng minh nó cho việc mở rộng trên các mô hình tuần tự, sau đó tổng quát hóa kết quả cho các kiến trúc đa dạng hơn. Trước khi mô tả chi tiết bằng chứng của bổ đề A.1, chúng tôi thúc đẩy thực nghiệm giả định về tính đối xứng đối với phân phối giá trị trọng số. Trong Hình 4, chúng tôi vẽ phân phối của trọng số của một số lớp của ResNet 50 được huấn luyện trên ImageNet. Giả định thường được thỏa mãn trong thực tế. Hơn nữa, trong bất kỳ trường hợp nào mà nó sẽ không được thỏa mãn, nó có thể được thực thi bằng cách sử dụng lượng tử hóa bất đối xứng.

Bổ đề A.1. Cho f là một lớp với trọng số W ∈ ℝⁿ với phân phối đối xứng. Chúng ta ký hiệu R^(k) là trọng số lượng tử hóa thứ k từ lỗi dư tương ứng. Khi đó lỗi giữa W^(K) được tái tỷ lệ = Q^(-1)(R^(K)) và trọng số gốc W giảm theo cấp số nhân, tức là:

|w - Σ_(k=1)^K w^(k)| ≤ (1/(2^(b-1)-1))^(K-1) (s_R^(K))_i/2     (8)

trong đó w và w^(k) ký hiệu các phần tử của W và W^(K) và (s_R^(k))_i ký hiệu thừa số tái tỷ lệ theo hàng ở bậc k tương ứng với w, như được định nghĩa trong phương trình 1.

Chúng ta làm việc trên các lớp mở rộng tính toán

f^(K): x ↦ σ(Σ_(k=1)^K R^(k) Q(x)s_R^(k)/s_x + b)     (9)

Chứng minh. Giả sử K = 1, khi đó W^(1) là kết quả của phép hợp thành của toán tử lượng tử hóa nghịch đảo và toán tử lượng tử hóa, tức là W^(1) = s_W⌊W/s_W⌉. Theo định nghĩa của toán tử làm tròn, chúng ta biết rằng |⌊a⌉ - a| ≤ 0.5. Do đó chúng ta có |w - w^(1)| ≤ s_W/2. Bây giờ trong trường hợp k = 2, chúng ta có theo định nghĩa của lượng tử hóa của lỗi dư và thuộc tính của toán tử làm tròn

|⌊(w - w^(1))/s_R^(2)⌉ - (w - w^(1))/s_R^(2)| ≤ 1/2     (10)

trong đó s_R^(2) là thừa số tái tỷ lệ trong phần dư bậc hai R₂ được tính từ w - w^(1). Các trọng số lượng tử hóa do đó được cho bởi:

|w - Σ_(i=1)^2 w^(i)| ≤ s_R^(2)/2     (11)

Bởi vì phân phối trọng số đối xứng, chúng ta biết rằng đối với bất kỳ k nào, s_R^(K) = max{w - Σ_(k=1)^(K-1) w^(k)}/(2^(b-1)-1) hoặc bất kỳ định nghĩa nào khác của delta trong không gian độ chính xác đầy đủ. Cũng, theo định nghĩa chúng ta có max{w - Σ_(k=1)^(K-1) w^(k)} ≤ s_R^(K). Do đó:

|w - Σ_(k=1)^K w^(k)| ≤ (1/(2^(b-1)-1)) s_R^(K)/2     (12)

Chúng ta kết luận bằng cách sử dụng một bằng chứng quy nạp đơn giản.

Như một hệ quả tức thì, chúng ta có hệ quả sau đây biện minh cho thuật ngữ mở rộng:

Hệ quả A.2. Cho f là một lớp của trọng số có giá trị thực W với phân phối đối xứng và R^(k) là trọng số lượng tử hóa thứ k từ lỗi dư tương ứng. Khi đó,

E[||f - Σ_(k=1)^K f^(k)||] ≥ E[||f - Σ_(k=1)^(K+1) f^(k)||]     (13)

và f = Σ_(k=1)^∞ f^(k).

Bất đẳng thức đầu tiên kết quả từ việc mô tả chi tiết quy nạp trong bằng chứng trước. Thay vì một giới hạn trên về lỗi trên tất cả các giá trị vô hướng, chúng ta xem xét từng lỗi và chỉ ra sử dụng các thuộc tính tương tự rằng chúng giảm sau mỗi bước. f = Σ_(k=1)^∞ f^(k) là một hệ quả trực tiếp của phương trình 8.

--- TRANG 11 ---
50
40
30
20
10
−0.3−0.2−0.100.10.20.3

Hình 4: Phân phối của các giá trị trọng số vô hướng của các lớp khác nhau của ResNet 50 được huấn luyện trên ImageNet. Chúng ta quan sát rằng mọi phân phối đều đối xứng quanh 0.

Mở rộng thưa thớt Cho N_i^(k) ký hiệu chuẩn L1 của kênh đầu ra i của phần dư bậc k R^(k). Phần dư thưa thớt được định nghĩa là:

R_γ^(k),i = (R^(k))_i · 1_γ^(k)     (14)

trong đó · là phép nhân theo phần tử, 1_γ^(k) = 1{N_i^(k) ≥ τ_γ^(k)} và τ_γ^(k) là một ngưỡng được định nghĩa là phân vị γ của N^(k). Nói cách khác, chúng ta loại bỏ một tỷ lệ γ các kênh từ phần dư R^(k) ít quan trọng nhất, như được chỉ ra bởi chuẩn N^(k) của chúng. Tuy nhiên, lưu ý rằng những kênh bị cắt tỉa này có thể được mã hóa trong các phần dư tiếp theo, tức là R^(k'), với k' > k. Kết quả từ Bổ đề A.1 trở thành:

Bổ đề A.3. Cho f là một lớp của trọng số có giá trị thực W với phân phối đối xứng. Khi đó chúng ta có

|w - (Σ_(k=1)^(K-1) w^(k) + Q^(-1)(R_γ^(K)))| ≤ ||N^(K) · 1_γ^(K)||_∞ (s_R^(k))_i / ((2^(b-1)-1)^K 2)     (15)

trong đó ||·||_∞ là toán tử chuẩn vô cùng với quy ước rằng ||0||_∞ = 1 và (s_R^(k))_i ký hiệu thừa số tái tỷ lệ theo hàng ở bậc K tương ứng với w.

Chứng minh. Từ phương trình 8, chúng ta có:

|w - (Σ_(k=1)^(K-1) w^(k) + Q^(-1)(R^(K)_1))| ≤ (s_R^(K))_i / (2 · (1/(2^(b-1)-1))^K)     (16)

tương ứng với trường hợp trong đó γ_l = 1. Nếu γ_l < 1, chúng ta có hai khả năng cho w. Đầu tiên, tọa độ trong N^(K) liên kết với lớn hơn τ_γ^(K) thì chúng ta rơi vào trường hợp trong đó R_γ^(K) = R^(K) và như vậy chúng ta có kết quả từ phương trình 8 mạnh hơn phương trình 15. Thứ hai, tọa độ trong N^(K) liên kết với thấp hơn τ_γ^(K). Khi đó chúng ta có rằng sự khác biệt giữa trọng số cơ bản w và mở rộng mỏng được giới hạn bởi mở rộng của bậc thấp hơn và giá trị tối đa của chuẩn N^(K) dẫn đến kết quả trong phương trình 15.

Xác thực thực nghiệm: Trong bổ đề A.1 và A.3, chúng tôi đã nêu sự hội tụ theo cấp số nhân về 0 của lỗi xấp xỉ trên các giá trị trọng số. Để xác nhận thực nghiệm kết quả lý thuyết này, chúng tôi lượng tử hóa ResNet 50 được huấn luyện trên ImageNet trong các giá trị ba trạng thái cho các bậc K khác nhau. Như có thể thấy trong Hình 5, lỗi trung bình mỗi lớp, hội tụ theo cấp số nhân về 0 phù hợp với kỳ vọng của chúng tôi. Hình cũng xác nhận kết quả thực nghiệm về các chiến lược cho γ. Các lỗi cao hơn nằm ở các lớp cuối, do đó các lớp này đòi hỏi nhiều sự chú ý hơn.

--- TRANG 12 ---
100
10^-1
10^1
10^2
10^3
10^-2
0 1 0.2 0.4 0.6 0.8
độ sâu của lớp chuẩn nơ-ron trung bình

Hình 5: So sánh chuẩn trung bình của lỗi lượng tử hóa cho mỗi lớp của ResNet 50 được huấn luyện trên ImageNet. Chúng ta quan sát sự hội tụ theo cấp số nhân được nêu trong bổ đề A.1 và A.3.

B Giới hạn trên lỗi
Định lý B.1. Cho F là một DNN tuần tự L lớp được huấn luyện. Chúng ta ký hiệu σ_l là giá trị đơn lẻ lớn nhất của W_l - Σ_k R^(k), tức là chuẩn phổ của W_l - Σ_k R^(k). Khi đó chúng ta có

max_(||X||=1) ||F(X) - F^(K)(X)||_∞ ≤ U_res

U_res = ∏_(l=1)^L (Σ_(i=1)^l σ_i u_i^(K) + 1) - 1     (17)

trong đó u_l^(K) = (1/(2^(b-1)-1))^(K-1) (s_R^(K))_i/2 từ phương trình 8.

Chứng minh. Hãy xem xét L = 2, và F: X ↦ Bσ(Ax). Đối với bất kỳ X nào trong miền của F sao cho ||X|| = 1, chúng ta có

||F(X)||_2 ≤ σ_B + σ_A + σ_B σ_A     (18)

trong đó σ_B là giá trị đơn lẻ lớn nhất của B và σ_A là giá trị đơn lẻ lớn nhất của A. Theo định nghĩa của chuẩn 2 và chuẩn ∞, chúng ta có

σ_(A-A^(K)) ≤ σ_A u_A^(K)     (19)

trong đó σ_(A-A^(K)) là giá trị đơn lẻ lớn nhất của lỗi dư bậc K, A - A^(K) và u_A^(K) được dẫn xuất từ phương trình 8. Do đó, chúng ta có

||F(X) - F^(K)(X)||_2 ≤ σ_B u_B^(K) + σ_A u_A^(K) + σ_B u_B^(K) σ_A u_A^(K)     (20)

Mở rộng thưa thớt
Định lý B.2. Cho F là một DNN tuần tự L lớp được huấn luyện. Chúng ta ký hiệu σ_l là giá trị đơn lẻ lớn nhất của W_l - Σ_k R^(k), tức là chuẩn phổ của W_l - Σ_k R^(k). Khi đó chúng ta có

max_(||X||=1) ||F(X) - F^(K)(X)||_∞ ≤ U_sparse

U_sparse = ∏_(l=1)^L (Σ_(i=1)^l σ_i u_i^(K) + 1) - 1     (21)

trong đó u_l^(K) = ||N^(K) · 1_γ^(K)||_∞ (s_R^(k))_i / ((2^(b-1)-1)^K 2) từ phương trình 15.

Kết quả này được dẫn xuất trực tiếp từ Định lý B.1. Kết quả này có thể được mở rộng cho các kiến trúc phức tạp hơn. Để làm như vậy, chúng ta đơn giản cần giải quyết các thuộc tính cụ thể như kết nối bỏ qua, nối và các hàm kích hoạt khác.

Kết nối bỏ qua và nối Trong trường hợp kết nối bỏ qua, đồ thị được chia từ lớp bắt đầu l₁ và chia thành ít nhất hai nhánh được cộng sau lớp l₂ và l₃. Giả sử chúng ta có thể tính toán giới hạn trên cho mỗi nhánh (mạng con), chúng ta đơn giản cộng các lỗi con này. Trong trường hợp U-net, nơi kết nối bỏ qua chứa kết nối bỏ qua, chúng ta đơn giản thực hiện quá trình này một cách đệ quy.

Một cách tiếp cận tương tự có thể được áp dụng để giải quyết việc nối. Tuy nhiên trong trường hợp này, chúng ta giữ giá trị lớn nhất thay vì cộng chúng.

Khối Self-Attention và Cross-Attention Để tổng quát hóa cho các mô-đun attention, chúng ta cần tổng quát hóa công thức của chúng ta cho một tích của các lớp. Hãy xem xét các tensor trọng số của các key W_keys và query W_queries. Khi đó các điểm attention được tính như sau

Att(X) = (W_keys × X)^T × (W_queries × X)     (22)

Chúng ta muốn giới hạn lỗi lượng tử hóa trên cơ chế attention. Tuy nhiên, quá trình liên quan đến độ lớn của đầu vào X như chúng ta làm nổi bật

Error_Att(X) = ||(W_keys × X)^T × (W_queries × X) - (Σ_k R_keys^(k) × X)^T × (Σ_k R_queries^(k) × X)||     (23)

Nếu chúng ta ký hiệu σ_k và σ_q là chuẩn phổ của các lỗi dư của key và query tương ứng, khi đó chúng ta có thể đơn giản hóa công thức trước

Error_Att(X) = ||(σ_k × X)^T(W_queries × X) + (W_keys × X)^T(σ_q × X) + (σ_k × X)^T(σ_q × X)||     (24)

Để đo lường ảnh hưởng này trên softmax trong tình huống tệ nhất, chúng ta có thể đơn giản so sánh σ_k và σ_q với các giá trị đơn lẻ nhỏ nhất của W_queries và W_keys. Nếu chúng ta ký hiệu α_k và α_q là các giá trị đơn lẻ lớn nhất của W_keys và W_queries tương ứng, khi đó chúng ta có

Error_Att(X)/||X||≤1 ≤ σ_k α_q + σ_q α_k + σ_k σ_q     (25)

Nếu chúng ta ký hiệu ε = σ_k α_q + σ_q α_k + σ_k σ_q là giới hạn trên này, khi đó lỗi trên các điểm softmax trở thành

Error_Softmax(X)/||X||≤1 ≤ 1 - e^(-2ε)     (26)

Các hàm kích hoạt khác Mặc dù các kích hoạt ReLU chiếm ưu thế trong các DNN hiện đại, vẫn có nhiều hàm kích hoạt được sử dụng rộng rãi khác như SiLU, GeLU hoặc thậm chí sigmoid. SiLU và GeLU được giới hạn bởi ReLU ở phía dương là nơi các lỗi cao nhất xảy ra. Do đó, giới hạn trên không thay đổi đối với các hàm kích hoạt GeLU và SiLU (mặc dù dưới nhiều giả định hơn về hỗ trợ, giới hạn trên có thể được thắt chặt cho ReLU và nên được sửa đổi cho GeLU và SiLU). Mặt khác, đối với các kích hoạt sigmoid hoặc các kích hoạt tương tự (ví dụ: tanh), giới hạn trên trở thành một giới hạn trên trên X trong miền của F thay vì X trên vòng tròn đơn vị.

C Mở rộng thưa thớt vượt trội hơn mở rộng tiêu chuẩn
Bổ đề C.1. Cho f là một lớp của trọng số có giá trị thực W với phân phối đối xứng. Khi đó, đối với K' < K hai số nguyên, chúng ta có:

Err(R^(1) + Σ_(k=2)^(K') R_γ₁^(k)) ≥ Err(R^(1) + Σ_(k=2)^K R_γ₂^(k))     (27)

trong đó Err là lỗi lượng tử hóa (tức là sự khác biệt tuyệt đối giữa trọng số lượng tử hóa và gốc, như trong Phương trình 8) và K' × γ₁ = K × γ₂ = β.

Chứng minh. Hãy giả sử các lớp đầu ra hai kênh. Khi đó, chúng ta có γ₁ = 1 và γ₂ = 0.5. Chúng ta chỉ cần chứng minh kết quả cho k₁ = 2 và k₂ = 1 vì kết quả sẽ mở rộng tự nhiên từ trường hợp này. Ý tưởng của bằng chứng bao gồm việc chỉ ra rằng việc sử dụng các giá trị β thấp hơn cho phép nhiều khả năng mở rộng hơn có thể dẫn đến hiệu suất tốt hơn. Hãy ký hiệu (W)₁ và (W)₂ là các trọng số tương ứng với việc tính toán của kênh đầu ra thứ nhất và thứ hai tương ứng. Sử dụng γ₁ = 1, mở rộng bậc hai tương ứng với việc lượng tử hóa (W)₁ hoặc (W)₂. Giả sử (W)₁ được chọn cho R_γ₁^(2). Khi đó, R_γ₁^(3) sẽ hoặc lượng tử hóa lỗi từ (W)₂ hoặc tiếp tục lượng tử hóa lỗi từ R_γ₁^(2). Trong trường hợp đầu tiên, chúng ta kết thúc với R^(1) + Σ_(i=2)^(k₁) R_γ₁^(i) = R^(1) + Σ_(n=2)^(k₂) R_γ₂^(i). Ngược lại, Err(R^(1) + Σ_(i=2)^(k₁) R_γ₁^(i)) > Err(R^(1) + Σ_(i=2)^(k₂) R_γ₂^(i)).

D Chi tiết triển khai và tập dữ liệu
Chúng tôi xác thực phương pháp được đề xuất trên ba tác vụ thị giác máy tính đầy thử thách thường được sử dụng để so sánh các phương pháp lượng tử hóa. Đầu tiên, chúng tôi đánh giá trên phân loại ImageNet [33] (~1.2M hình ảnh train/50k test). Thứ hai, chúng tôi báo cáo kết quả về phát hiện đối tượng trên Pascal VOC 2012 [34] (~17k hình ảnh trong tập test). Thứ ba, chúng tôi benchmark trên phân đoạn hình ảnh trên tập dữ liệu CityScapes [35] (500 hình ảnh validation). Kết quả NLP của chúng tôi được thu được trên tác vụ transfer learning GLUE [36]. Chúng tôi cũng đánh giá OPT-13B [42] LLM trên các tập dữ liệu lý luận thông thường tiêu chuẩn: BoolQ [48], PIQA [49], HellaSwag [50], WinoGrande [51], ARC easy và challenge [52] và OpenBookQA [53]. Trong các thí nghiệm của chúng tôi, chúng tôi đã sử dụng MobileNet [54] và ResNet [1] trên ImageNet. Đối với phát hiện đối tượng Pascal VOC, chúng tôi sử dụng kiến trúc SSD [2] với backbone MobileNet. Trên CityScapes, chúng tôi sử dụng DeepLab V3+ [55] với backbone MobileNet. Chúng tôi cũng kiểm tra phương pháp của chúng tôi trên VGG 16 [45] và transformer như mô hình BERT [4] cũng như các mô hình ngôn ngữ lớn như OPT-13B [42].

Trong các thí nghiệm của chúng tôi, các đầu vào và kích hoạt được lượng tử hóa sử dụng cùng phương pháp như [14]. Chúng tôi đếm các phép toán bit-wise như sau: cho W là trọng số có giá trị thực của lớp tích chập d×d trên bản đồ đặc trưng đầu vào có hình dạng D×D×n_i và n_o đầu ra và stride s. Khi đó tích chập yêu cầu d²D²/s² n_i n_o phép nhân dấu phẩy động. Lớp lượng tử hóa yêu cầu hai phép toán tái tỷ lệ (cho lượng tử hóa của đầu vào và phép toán Q^(-1)) và một tích chập int-b, tức là n_i D² + D²/s² n_o phép nhân dấu phẩy động và d²D²/s² n_i n_o phép nhân int-b. Lưu ý rằng số lượng phép cộng vẫn không thay đổi. Theo [56], độ phức tạp thấp nhất cho phép nhân vô hướng b-chữ số là o(b log(b)) phép toán bit. Điều này được đạt được về mặt lý thuyết bằng cách sử dụng thuật toán Harvey-Hoeven (giới hạn tiệm cận cũng chưa được chứng minh). Chúng tôi sử dụng giá trị này vì đây là thiết lập ít thuận lợi nhất cho phương pháp được đề xuất. Kết quả là số lượng O_original phép toán bit cần thiết cho lớp gốc, O_R^(1) số lượng phép toán bit cho lớp lượng tử hóa ngây thơ và O_R^(k) cho mở rộng lượng tử hóa dư bậc i là

O_original = D²d²n_i n_o/s² 32 log(32)
O_R^(1) = D²[(n_i + n_o/s²)32 log(32) + d²n_i n_o/s² b log(b)]
O_R^(k-1) = D²[(n_i + n_o/s²)32 log(32) + kd²n_i n_o/s² b log(b)]     (28)

Sử dụng kết quả này, chúng ta có thể ước tính bậc mở rộng tối đa trước khi số lượng phép toán trong f^(k) vượt quá O_baseline. Lưu ý rằng trong trường hợp các lớp fully-connected, D = 1, s = 1 và d = 1. Trong phần tiếp theo, chúng tôi sử dụng metric cảm ứng về độ chính xác đối với tổng số phép toán bit-wise được thực hiện bởi DNN trên một đầu vào duy nhất. Metric này không xem xét thực tế rằng các phép toán được thêm vào có thể được thực hiện song song. Đối với SQuant [19], chúng tôi sử dụng triển khai của chúng tôi đạt được kết quả độ chính xác khác nhau do độ chính xác ban đầu khác nhau cho các mô hình cơ bản. Đối với ZeroQ [17], chúng tôi sử dụng kết quả được cung cấp bởi SQuant [19]. Tương tự như công việc trước [15,14,19], chúng tôi ký hiệu W·/A· thiết lập lượng tử hóa (số bit cho lượng tử hóa trọng số và số bit cho lượng tử hóa kích hoạt). Chúng tôi đã sử dụng các triển khai Tensorflow của các mô hình cơ bản từ kho lưu trữ chính thức khi có thể hoặc các tài nguyên có sẵn công khai khác khi cần thiết. MobileNet và ResNet cho ImageNet đến từ zoo mô hình tensorflow. Trong phát hiện đối tượng, chúng tôi đã kiểm tra mô hình SSD với backbone MobileNet từ kho lưu trữ git của Manish. Cuối cùng, trong phân đoạn ngữ nghĩa hình ảnh, mô hình DeepLab V3+ đến từ kho lưu trữ git của Bonlime. Các trọng số được huấn luyện trước của mạng cung cấp độ chính xác cơ bản tiêu chuẩn trên mỗi tác vụ. Việc tính toán các phần dư cũng như công việc được thực hiện trên các trọng số được thực hiện bằng thư viện python Numpy.

--- TRANG 13 ---
Bảng 6: Chi phí bổ sung gây ra bởi việc giảm tổng trong triển khai số nguyên đầy đủ của REx. Trong bảng này R có nghĩa là một phần dư đầy đủ không có độ thưa thớt.

mô hình W4+25% /A4 W4+50% /A4 W4+100% /A4 W4+1R/A4 W4+2R/A4
ResNet 0.035% 0.070% 0.138% 0.415% 0.830%
MobileNet v2 0.016% 0.032% 0.063% 0.189% 0.378%
BERT 0.004% 0.009% 0.018% 0.054% 0.107%

Bảng 7: Chi phí bổ sung gây ra bởi việc giảm tổng trong triển khai số nguyên đầy đủ của REx. Trong bảng này chúng tôi nghiên cứu tích chập 1 x 1 trên đầu vào có hình dạng 224×224×320 và đầu ra 1280 kênh cũng như tích chập depthwise trên đầu vào có hình dạng 224×224×96.

Bộ lọc bậc mở rộng thời gian chạy đầy đủ (ops) chi phí bổ sung (ops) chi phí tương đối
Conv 1x1 2 2.0×10⁷ 6.2×10⁴ 0.15
Conv 1x1 3 4.0×10⁷ 6.1×10⁴ 0.10
Conv 1x1 4 6.0×10⁷ 6.2×10⁴ 0.08
Depthwise Conv 3x3 2 2.7×10⁵ 3.0×10⁴ 5.49
Depthwise Conv 3x3 3 5.4×10⁵ 3.0×10⁴ 3.38
Depthwise Conv 3x3 4 8.7×10⁵ 2.9×10⁴ 2.43

E Giảm mở rộng trong bộ tích lũy
Hãy đi qua quy trình chi tiết mà chúng tôi đã áp dụng để đi từ lượng tử hóa mô phỏng với các thừa số tỷ lệ dấu phẩy động sang suy luận chỉ số nguyên. Chúng tôi dựa vào quy trình được giới thiệu trong [1]. Đầu tiên, hãy xem xét lượng tử hóa của một tensor duy nhất A. Lượng tử hóa được mô phỏng sử dụng A~ = s_A × ⌊A/s^(A)⌉ = s_A × A_Q. Trong tình huống hiện tại, A_Q được lượng tử hóa và thực sự phù hợp với bit-width đích trong khi s_A được lưu trữ như một giá trị dấu phẩy động. Để đạt được suy luận chỉ số nguyên, chúng ta cần chuyển đổi phép nhân bằng s_A thành phép nhân số nguyên. Từ [1], chúng tôi dựa vào phương trình (6) và, sử dụng các ký hiệu tương tự, chúng ta có s_A × A_Q ≈ M_A × 2^(-n) × A_Q. Tất cả các phép toán này là phép toán chỉ số nguyên. Tuy nhiên, trong thực tế, các phép toán này có thể thêm lỗi ngoài chính sơ đồ lượng tử hóa. Để đo lường lỗi này, chúng tôi đã tiến hành thí nghiệm riêng của mình và quan sát rằng lỗi bổ sung không thay đổi đầu ra lượng tử hóa của lớp; điều này là do thực tế rằng số hạng M_A có ít nhất 30 bit độ chính xác trong khi A_Q có 1, 4 hoặc 8 bit độ chính xác (tùy thuộc vào bit-width lượng tử hóa). Sự khác biệt về độ chính xác này đến từ việc sử dụng bộ tích lũy lớn hơn là tiêu chuẩn trong suy luận lượng tử hóa.

Bây giờ khi chúng tôi đã mô tả chi tiết cách lượng tử hóa, chúng tôi mô tả chi tiết cách cộng hai tensor riêng biệt A và B (điều này sẽ có tầm quan trọng đặc biệt để cộng các phần dư, như bạn đã chỉ ra). Trong lượng tử hóa mô phỏng, chúng ta sẽ có A + B ≈ s_A × ⌊A/s^(A)⌉ + s_B × ⌊B/s^(B)⌉ = s_A × A_Q + s_B × B_Q. Bây giờ, bằng cách áp dụng kỹ thuật tương tự như trên, chúng ta có s_A × A_Q + s_B × B_Q ≈ M_A × 2^(-n) × A_Q + M_{B/A} × 2^(-n) × B_Q, trong đó M_{B/A} là số nguyên gần nhất với M_B/M_A được mã hóa với 30 bit độ chính xác. Điều này đã được thảo luận trong Phụ lục A.2 trong [1]. Các tác giả nêu rằng phép toán này tốn kém vì nó yêu cầu thực hiện phép nhân số nguyên trước phép cộng. Đây là kết quả của việc chúng ta cần đi từ bộ tích lũy xuống bit-width lượng tử hóa và sau đó quay lại kích thước bộ tích lũy.

Trong pipeline của chúng tôi, chúng tôi hạn chế chi phí này bằng cách sử dụng phép toán kết hợp để giới thiệu chi phí thấp so với việc đơn giản sử dụng kích thước kernel lớn hơn. Một cách chính thức, chúng tôi đã sử dụng công thức nói trên trực tiếp trên kết quả nhân. Nói cách khác, chúng ta có công thức sau cho A và phần dư lượng tử hóa của nó R_A trong trường hợp lượng tử hóa sử dụng b bit: s_A × A_Q + s_{R_A} × R_A ≈ 2^(-n) × (M_A × A_Q + M_{R_A} × 2^(-b) × R_A). Do đó, chi phí bổ sung từ sơ đồ tổng phần dư được giới hạn ở một phép dịch bit trên phần dư trong quá trình giảm bộ tích lũy. Trong Bảng 6, chúng tôi báo cáo chi phí bổ sung tương đối được giới thiệu bởi phép dịch bit bổ sung này trong sơ đồ tổng phần dư đối với tổng chi phí suy luận. Ví dụ, chúng tôi liệt kê trong Bảng 7 một số kết quả với phần cứng Gap9: chúng tôi so sánh chi phí tính toán một số lớp tích chập với chi phí giảm các phần dư. Trên các tích chập, như mong đợi, chi phí hoàn toàn không đáng kể. Do khả năng song song hóa của phần cứng, khi bậc mở rộng tăng, chi phí bổ sung giảm. Hơn nữa, cần lưu ý rằng các lớp tích chập depthwise không được hỗ trợ tốt bởi hầu hết phần cứng cho đến ngày nay, do đó kết quả ít ấn tượng hơn nhưng chi phí bổ sung tuyệt đối tương tự.
