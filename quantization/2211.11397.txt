# 2211.11397.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2211.11397.pdf
# File size: 3292266 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Learning Low-Rank Representations for Model Compression
Zezhou Zhu,1Yucong Zhou,2Zhao Zhong2
1Beijing University of Posts and Telecommunications
2Huawei
zhuzezhou@bupt.edu.com, zhouyucong1, zorro.zhongzhao@huawei.com
Abstract
Vector Quantization (VQ) is an appealing model compres-
sion method to obtain a tiny model with less accuracy loss.
While methods to obtain better codebooks and codes under
ﬁxed clustering dimensionality have been extensively stud-
ied, optimizations of the vectors in favour of clustering per-
formance are not carefully considered, especially via the re-
duction of vector dimensionality. This paper reports our re-
cent progress on the combination of dimensionality compres-
sion and vector quantization, proposing a Low-Rank Repre-
sentation Vector Quantization (LR2VQ) method that outper-
forms previous VQ algorithms in various tasks and architec-
tures. LR2VQ joins low-rank representation with subvector
clustering to construct a new kind of building block that is
directly optimized through end-to-end training over the task
loss. Our proposed design pattern introduces three hyper-
parameters, the number of clusters k, the size of subvectors
mand the clustering dimensionality ~d. In our method, the
compression ratio could be directly controlled by m, and the
ﬁnal accuracy is solely determined by ~d. We recognize ~das
a trade-off between low-rank approximation error and clus-
tering error and carry out both theoretical analysis and ex-
perimental observations that empower the estimation of the
proper ~dbefore ﬁne-tunning. With a proper ~d, we evaluate
LR2VQ with ResNet-18/ResNet-50 on ImageNet classiﬁca-
tion datasets, achieving 2.8%/1.0% top-1 accuracy improve-
ments over the current state-of-the-art VQ-based compres-
sion algorithms with 43 /31compression factor.
Introduction
In recent years, deep neural networks have achieved remark-
able performance on different vision tasks like image classi-
ﬁcation, object detection, and semantic segmentation. How-
ever, this progress is fueled by wider and deeper network
architectures, which possess a large memory footprint and
computation overhead. These networks are hard to be de-
ployed on battery-powered and resource-constrained hard-
ware such as wearable devices and mobile phones. At the
same time, deep neural networks are redundant in parame-
ters and layer connections, which implies that it is possible
to compress them without much accuracy loss. Therefore,
compressing neural networks is essential for real-world ap-
plications.
There are several approaches to compressing deep neural
networks from different perspectives. Compact network de-sign(Zhang et al. 2017; Sandler et al. 2018; Howard et al.
2019; Tan and Le 2019) and neural architecture search
(NAS)(Zoph et al. 2018; Zhong et al. 2018; Pham et al.
2018; Zhang, Zhang, and Zhong 2020; Fang, Wang, and
Zhong 2019) focus on exploring lightweight architectures;
Pruning(LeCun, Denker, and Solla 1990) tends to elimi-
nate unnecessary connections or channels on heavy mod-
els directly; Quantization(Courbariaux, Bengio, and David
2016; Hubara et al. 2016) compresses each parameter from
FP32 to lower bits. Here we concentrate on Vector Quan-
tization(VQ)(Jegou, Douze, and Schmid 2011) to achieve a
higher compression-to-accuracy ratio.
VQ exploits ﬁlter redundancy to reduce storage cost with
codebook andcodes . Most of the prior works(Gong et al.
2014; Son, Nah, and Lee 2018; Wu et al. 2016; Stock
et al. 2020; Chen, Wang, and Cheng 2020; Martinez et al.
2021) demonstrate impressive performance on compression-
to-accuracy ratio. The key to the success of VQ is cluster-
ing and ﬁne-tuning, where clustering decreases the memory
demand for storage and computation, and ﬁne-tuning helps
recover the model performance by data-driven optimization.
The accuracy loss of the quantized network is closely related
to the clustering error. Great progress have been achieved on
ﬁnding more expressive codebooks and codes by reducing
the clustering error under a ﬁxed clustering dimensionality.
However, the clustering dimensionality is a critical factor in
impacting clustering error. Applying a distance-based clus-
tering method in high-dimensional space is more likely to
suffer from the curse of dimensionality(Indyk and Motwani
2000), which introduces signiﬁcant difﬁculties for clustering
to produce large clustering errors. On the contrary, clustering
in low-dimensional space is much easier to produce lower
clustering error. Speciﬁcally, the dimensionality of subvec-
tors in convolutional layers is always 9 or 18, raising the dif-
ﬁculty in ﬁnding expressive codebooks and codes in high di-
mensional space, and resulting in poor quantization perfor-
mance. On the question of dimensionality redunction, low-
rank representation methods are widely used for all sorts of
dimensionality reduction tasks. These remarkable methods
invoke the demand to investigate the possibility to design
signiﬁcantly better quantizers by the mixture of VQ and low-
rank representations.
To keep up with the above demand, this paper proposes
Low-Rank Representation Vector Quantization (LR2VQ),arXiv:2211.11397v1  [cs.CV]  21 Nov 2022

--- PAGE 2 ---
a new method that jointly considers the compression ratio
and the clustering dimensionality to achieve better quanti-
zation. LR2VQ utilizes low-rank representations (LRRs) to
approximate the original ﬁlters and clustering the subvectors
in LRRs to achieve quantization. Our method possesses an
extraordinary characteristic: the compressed model size is
controlled by m(the subvector size), and the quantization
performance is solely determined by ~d(the clustering di-
mensionality). This characteristic decouples mand~d, which
are always equal in previous VQ approaches. At this time,
~dcan vary in a wide range freely. The changeable ~dcon-
tributes to clustering in low-dimensional space, which bene-
ﬁts quantization with lower clustering error. Besides, it pro-
vides an opportunity to comprehensively research the di-
mensionality reduction and vector quantization without im-
pacting the compressed model size. Moreover, it introduces
a trade-off between the approximation error in low-rank rep-
resentation learning and the clustering error in quantiza-
tion. Based on theoretical analysis and experimental results,
we empower the estimation of ~dwhich appropriately bal-
ances errors to achieve better quantization performance. We
experiment LR2VQ on large-scale datasets like ImageNet
and COCO. Compared with the results in PQF(Martinez
et al. 2021), LR2VQ improves 2.8%/1.0% top-1 accuracy
for ResNet-18/ResNet-50 on ImageNet with 43 /31com-
pression factor, and 0.91% box AP for Mask R-CNN on
COCO with 26compression factor. In summary, our main
contributions are as follows:
• We propose a simple yet effective quantization method
called Low-Rank Representation Vector Quantization
(LR2VQ). The subvector size mand the clustering di-
mensionality ~dare mutually independent, making it pos-
sible to achieve dimensionality reduction for better quan-
tization performance.
• We identify the trade-off of ~d, and provide estimations to
~dwith both theoretical and empirical results to balance
the approximation error and the clustering error.
• We evaluate LR2VQ with various ~d, which produces
state-of-the-art performance on different datasets and ar-
chitectures.
Related Work
There is a large body of literature on deep neural network
compression. We review related literature from four per-
spectives: compact network design, punning, tensor decom-
position, and quantization.
Compact network design Light weight networks like
SqueezeNet(Iandola et al. 2016), NasNet(Zoph et al. 2018),
ShufﬂeNet(Zhang et al. 2017), MobileNets(Sandler et al.
2018; Howard et al. 2019) and EfﬁcientNets(Tan and Le
2019) are proposed to be computation and memory effe-
cient. However, these architectures are either hand-crafted
or produced by searching algorithms. It is inefﬁcient to de-
sign networks manually, and current searching approaches
demand vast computation resources. To avoid these issues,
another line of work directly operates on the existed networkarchitectures (like VGG or ResNets) to achieve compression
and acceleration.
Pruning The simplest pruning operates on ﬁlters by re-
moving the connections according to an importance crite-
ria until achieving desirable compression-to-accuracy trade-
off(LeCun, Denker, and Solla 1990; Guo, Yao, and Chen
2016; Han et al. 2015). However, most pruning methods dis-
card individual parameters, and the pruned networks are too
sparse to achieve acceleration on embedded devices. Hence,
some approaches focus on pruning unimportant channels
to realize compression and acceleration simutanously(He,
Zhang, and Sun 2017; Li et al. 2017; Luo, Wu, and Lin
2017).
Quantization Quantization amounts to reducing the
bitwidth of each parameter in neural networks. In this con-
text, we focus on Vector Quantization(VQ), which treats
each vector individually to decompose a high-dimensional
tensor with small-sized codebooks and low-bit codes.
BGD(Stock et al. 2020), P&G(Chen, Wang, and Cheng
2020), PQF(Martinez et al. 2021) and DKM(Cho et al. 2021)
are recently proposed vector quantization approaches. BGD
minimizes the reconstruction error of feature maps in the
quantized network and optimizes codebooks via layer-wise
distillation. P&G directly minimizes the reconstruction er-
ror of parameters to achieve improvements. PQF addresses
the invariance problem for quantization and formulates a
permutation-based method to ﬁnd a functional equivalent
network for easier clustering. Nevertheless, the permuta-
tion is also performed on high-dimensional space, which is
more likely to result in a large clustering error. DKM is an
attention-based method that learns to cluster during network
training. However, the differentiable clustering method re-
quires expensive end-to-end training.
Unlike these approaches, our method takes advantage
of learning LRRs to achieve changeable clustering dimen-
sionality, which beneﬁts the overall reconstruction error for
quantization. Based on this property, we exploit the trade-off
in LR2VQ to guide the searching for the proper clustering
dimensionality. The accuracy recovery process in LR2VQ is
also high efﬁciency, which needs a few ﬁne-tuning epochs
over the task loss. All these contributions result in an ef-
ﬁcient and effective method for deep neural network com-
pression.
Method
Our proposed method aims to achieve a target compression
ratio with codebooks and codes from different clustering di-
mensionality. Toward this goal, LR2VQ consists of three
steps:
1.Learning low-rank representations : This step learns
low-rank representations (LRRs) for all the convolutional
ﬁlters using gradient-based learning methods. We use
LRRs and linear transformations (LTs) to replace the
convolutional ﬁlters for computation.
2.Quantizing low-rank representations : The dimension-
ality of LRRs is usually different from the original sub-
vectors. After learning LRRs, we generate codebooks

--- PAGE 3 ---
CNN
Reshaped Matrix Conv . filters
Clustering
Codebook
CodesReshape
DecodeDecoded Matrix CNN
Task LossLearning LRR
ClusteringCodebook Codes
DecodeDecoded LRR
LT
LRR & L TFigure 1: The compression pipeline of LR2VQ and normal VQ. Nis the number of parameters in Conv. ﬁlters. Unlike normal
VQ, LR2VQ ﬁrst learns low-rank representation (LRR) and linear transformation (LT), then quantizes LRR with a smaller-
sized codebook and low-bit codes. The decoded LRR can be transformed to approximate the reshaped matrix for computation.
LR2VQ can perform clustering on various dimensionality, and the compression ratio is unchanged.
and codes by clustering the subvectors in LRRs. This op-
eration realizes the variation of clustering dimensionality.
3.Global ﬁne-tuning : We ﬁne-tune codebooks by min-
imizing the loss function over training datasets with
gradient-based optimization. After ﬁne-tuning, we merge
codebooks and linear transformations (LTs) to eliminate
the additional computation complexity during inference.
Learning Low-Rank Representations
This section presents how to generate convolutional ﬁlters
by LRRs and initialize LRRs for robust learning.
Deﬁnition Let us denote a convolutional ﬁlter W3
RCoutCinKKin uncompressed neural networks, with
Coutas the number of output channels, Cinas the num-
ber of input channels, and Kas the spatial size of the ﬁl-
ter. In normal VQ, Wshould be reshaped into a 2D matrix
Wr2RCoutCinKK
mm, wheremis the size of subvectors,
andCoutCinKK=m is the number of subvectors. The value
ofmdetermines the number of subvectors and compression
ratios. In our method, we construct Wby approximating
Wrwith two 2D matrices AandB:
WrW0=AB; (1)
where A2RCoutCinKK
m~dandB2R~dm. The dimen-
sionality of Ais~d2[1;m]. Once W0is computed, we can
reshape it into a 4D tensor of shape CoutCinKKto
replace Wfor computation.
Equation 1 is similar to low-rank matrix decomposition,
soAcan be treated as the LRR of W0andBis the cor-
responding linear transformation from ~dtom. Instead of
mathematical decomposing W, we directly learn AandB
by end-to-end optimization, which is more adaptive and ef-
ﬁcient for obtaining expressive LRR for W0.
Initialization Weights initialization strongly inﬂuences
the neural network’s optimization and ﬁnal performance.
As we replace the original weights by W0, the initializa-
tion of W0should be consistent with W. Thus, we expect
Var(W) =Var(W0) =  . Based on Equation 1, W0
is computed by the multiplication between AandB. Forsimplicity, we initialize Awith a zero-mean, variance
normal distribution. With the above assumptions, we only
need to compute the variance of Bfor initialization. Accord-
ing to the derivations in (He et al. 2015b), we can calculate
Var(B)with the following equations:
Var(A)mVar (B) =Var(W) =Var(W0)
)mVar (B) = 
)Var(B) =1
m:(2)
To preserve the magnitude in the backward pass, we multi-
plyVar(B)with its output dimension m. At this time, we
can initialize Bwith a zero-mean, 1=mvariance normal dis-
tribution.
Quantizing Low-Rank Representations
In this section, we ﬁrst demonstrate how we apply VQ on the
learned LRRs to obtain codebooks Cand codes I. Then, we
discuss the trade-off of ~din LR2VQ. Finally, we introduce
an analytic method to search for a coarse estimation of ~dthat
may result in a lower reconstruction error.
Codebook and codes In LR2VQ, LRRs take a major-
ity of network parameters. So we apply VQ on LRRs to
save storage with codebooks Cand codes I. In our deﬁ-
nition, Ais matrix with N=m rows and ~dcolumns, where
N=CoutCinKK= . We treat each row in Aas an indi-
vidual subvector to be compressed, so the total number of
subvectors for quantization is N=m , and the dimensionality
of subvectors is ~d. To save the storage of these subvectors,
we usekcentroids with size ~dfor approximation. We call
the set of centroids as codebook C=fc1;:::;ckg2Rk~d
where each row in Cis a centroid. Codes I2RN
mis a set
of assignments that identify the best approximation mapping
between subvectors and centroids
Ip=argmin
qjjAp Cqjj2
2; (3)
wherepandqare the row indexes in AandC, andAp2
R1~dis thepth subvector, Cq2R1~dis theqth centroid,

--- PAGE 4 ---
Ip2R1is a single code to Ap. WithCandI, we can
decode them to construct bAby looking up Cwith all the
codesI
bA=C(I) =fCI1;:::;CIN
mg2RN
m~d; (4)
then transform bAtom-dimensional space
W0cW0=bAB: (5)
All the codebooks and codes can be generated by clustering.
The subvector size in normal VQ is m, while the sub-
vector size in our method is ~d. Note that the clustering di-
mensionality equals the size of subvectors for all VQ meth-
ods. Therefore, ~dis the clustering dimensionality in LR2VQ,
which can vary within [1;m]. The number of subvectors in
LRR is also directly controlled by m, resulting in equiva-
lent number of subvectors to the original ﬁlters. As a result,
clustering on LRRs does not impact the compression ratio.
Figure 1 depicts the comparison between normal VQ and
LR2VQ.
LR2VQ is compatible with any clustering methods. Here,
we introduce how LR2VQ works with k-means clustering.
After initializing a codebook with the subvectors in A,k-
means loops within the following steps:
1. Update assignments via Equation 3 according to the sub-
vectors in LRRs and codebook;
2. Update codebook (centroids) according to assignments.
Once the loop ends, codes are ﬁxed as each subvector should
be replaced by a speciﬁc centroid. Although the Euclidean
errors between subvectors and centroids might be tiny, such
errors introduce a large gap between AandbA. Therefore,
extra network ﬁne-tuning is necessary to compensate for
performance loss.
Trade-off of ~dIn LR2VQ, ~dis the dimensionality for
low-rank approximation and vector quantization. This value
simultaneously inﬂuences the approximation error in low-
rank representation learning and the clustering error in vec-
tor quantization, which affects the overall reconstruction er-
ror for model compression. Here, we discuss the trade-off of
~din LR2VQ.
The conceptional relationship between the overall recon-
struction error Er, the approximation error Eaand the clus-
tering errorEcis described as follows:
Er/Ea+Ec; (6)
and bothEaandEcare affected by ~d. Therefore, we need
a proper ~dto reduce these two errors. For Ea, a larger ~d
means more parameters in the LRR network, indicating a
smallerEain LR2VQ. Especially when ~dis close tom, the
number of parameters in the LRR network is already suf-
ﬁcient to approximate the original network, which can pro-
duce a “zero” approximation error. So Eamonotonically de-
creases with a rising ~d. ForEc, high-dimensional subvectors
are hard to be clustered, resulting in a large clustering error,
while the low-dimensional subvectors are more favourable
for clustering. For example, when ~d= 1,Ecmay becomeextremely tiny because each subvector is a single scaler in
1D space, which is the lowest and the simplest space for
clustering; When ~d=m, the subvectors in LRR have the
same dimensionality as the original subvectors, leading to
signiﬁcant clustering difﬁculty and the largest clustering er-
ror. SoEcmonotonically increases with a rising ~d. Based
on the above discussion, we can conclude that EaandEc
are inversely correlated. Adding these two errors together,
we expect that the variation of Eris similar to Figure 2,
which ﬁrst declines then increases with a rising ~d. Such vari-
ation provides reliable insurance that there must be a proper
~dto produce a smaller reconstruction error. In summary, the
trade-off of ~dis the theoretical guarantee in our proposed
LR2VQ to achieve better quantization performance. We ex-
tensively experiment on various ~din later sections.
1 3 7
Figure 2: Possible variations for errors with m= 9.
Searching for proper ~dThe most effective way to choose
proper ~dis grid search, which is time and resources consum-
ing. Theoretically, an accurate analytical method is useful
to search a proper ~dafter low-rank representation learning.
Based on the assumption and analysis in PQF(Martinez et al.
2021), we apply a simple method to coarsely estimate ~dafter
LRR learning as a starting point.
As different architectures have different characteristics, it
is hard to express Eawith mathematics. Besides, Eais ﬁxed
after LRR learning and can be measured by the model per-
formance. Consequently, we choose the LRR networks com-
parable to the original network for searching ~dbecauseEa
in these networks is negligible. At this time, Eris dominated
byEc, and we only need to consider Ecin searching proper
~d. We note that Ecindicates the clustering error between
W0andcW0rather than AandbA. The essence of LR2VQ
is clustering the subvectors in W0, and we achieve this goal
with an indirect method, which is clustering on A. There-
fore, the clustering error between W0andcW0is the actual
Ecin LR2VQ. Based on the above analysis, the following
computations are all performed on W0rather than A.
According to PQF, we can infer that W0 N (0;),
where 2Rmmis the covariance of W0, and the lower

--- PAGE 5 ---
bound ofEcis
Eck 2
mmjj1
m: (7)
Similar to PQF, we minimize jjto reduceEc.
After LRR learning, each LRR network with a candidate
~dadopts the following computations:
1. Generating W0for all convolutional layers;
2. CalculatingjjforW0;
3. Adding all thejjto represent Ec.
The quantization regimes for all FC layers are the same, and
we do not apply LR2VQ to them. So the jjin FC layers
can be treated as equal, which can be neglected in each net-
work. After the above computation, the network with the
lowestjjprovides a coarse estimation for ~d, and quantizing
the corresponding LRR network is more likely to achieve a
better compression-to-accuracy ratio in LR2VQ. We demon-
strate the results of jjwith different ~din the later section
to validate the coarse estimations of our method.
Global Fine-tuning
After clustering, global ﬁne-tuning is necessary to compen-
sate for performance loss caused by Ec. In this step, we
ﬁx the assigned centroids to all subvectors and ﬁne-tune the
compressed network with the loss function and training data
in low-rank representation learning. During ﬁne-tuning, all
the subvectors will be replaced by Equation 4 and 5 for com-
putation. So the centroids are differentiable and can be up-
dated by gradients as follows:
c(i)l
t+1 c(i)l
t ^@^L
@c(i)t; (8)
where ^Lis the loss function, and the learning rate is mod-
iﬁed to ^. This procedure is potent in boosting the model
performance of the quantized networks.
Inference without B
The linear transformation (LT) Bis necessary for low-rank
representation learning. Nevertheless, once all the parame-
ters are ﬁxed after global ﬁne-tuning, we can eliminate B
and the computation in Equation 1. Such elimination is ac-
complished by the commutative property between LT and
Look-Up Table (LUT) operation. After the codebook Cis
ﬁxed, we can obtain a new codebook C0by
C0=CB2Rkm; (9)
and the computation of LR2VQ is modiﬁed to
WW0cW0=C0(I)
= (CB)(I) =C(I)B
=bAB:(10)
Equation 10 implies that only codes Iand the new code-
bookC0are expected to be stored, and the computation in
Equation 1 is completely removed by a simple LUT opera-
tionC0(I)during inference.Experiments
Experiments on ImageNet
In this section, we evaluate our LR2VQ with vanilla ResNet-
18, ResNet-50(He et al. 2015a) on ImageNet(Russakovsky
et al. 2015) classiﬁcation datasets.
Baselines We compare our method with PQF and vanilla
DKM as they are the most competitive VQ methods up to
the writing of this paper. As LR2VQ requires low-rank pre-
training before quantization, we do not use the pre-trained
models from the Pytorch model zoo. Instead, we implement
our training procedures and re-implement PQF and vanilla
DKM under the same code base for a fair comparison. As a
representative VQ method, PQF has conducted sufﬁcient ex-
periments and set solid baselines, so we follow its compres-
sion settings to compare different methods under identical
model sizes.
Compression setups Let’s denote cvfor33convolution,
pwfor11convolution and fcfor fully-connected layer.
To identify the hyperparameters in different convolutional
layers in LR2VQ, we use mcv;mpw;kcv;kpw;~dcv;~dpwto
representm,kand~din33and pointwise convolution. We
set two compression regimes to achieve different compres-
sion ratios. The detailed conﬁgurations are shown in Table
2. The large blocks regime means fewer codes and a smaller
model size for quantized networks. Speciﬁcally, the cluster-
ing dimensionality for 33and pointwise convolution in
PQF and vanilla DKM are equal to mcv;mpw, andkcv,kpw
are also the same as in LR2VQ. For FC layers, the dimen-
sionality of subvectors is 4, and k= 2048 for ResNet-18 and
k= 1024 for ResNet-50. For other settings, we follow (Mar-
tinez et al. 2021) that we clamp the number of centroids to
min(k;N= 4)for stability, and we do not compress the ﬁrst
77convolution in ResNets since they take less than 0.1%
model size.
Memory footprint Following PQF, we only compress the
weights in convolutional and FC layers and ignore the bias
in FC and batchnorm layers. We train networks with 32-
bit ﬂoats but store the codebooks with 16-bit ﬂoats. For
k= 256 , all the codes can be stored as 8-bit integers. These
settings effectively reduce model sizes with negligible accu-
racy loss.
Training details The low-rank representation learning
uses a total batch size of 1024 on 16 NVIDIA V100 GPUS to
train 100 epochs with SGD optimizer plus Nesterov and mo-
mentum 0.9. Weight decay is 0.0001, and the learning rate
is 0.4 with a cosine annealing scheduler. Label smooth is set
to 0.1. We train LRR networks with ~d2[3;7]because other
values produces either larger Eaor largerEcto harm quan-
tization. After LRR learning, we run our searching method
to estimate a coarse ~d, and start quantizing the correspond-
ing LRR network. We run k-means for 100 iterations to ob-
tain codebooks and codes, then ﬁne-tune the compressed
network with an Adam optimizer. The initial learning rate
of ﬁne-tuning is 0.001 and annealed by a cosine scheduler.
This procedure runs on 16 GPUs with batch size 2048 for 9
epochs, which takes half an hour for ResNets.

--- PAGE 6 ---
Table 1: ImageNet results for PQF, vannila DKM and LR2VQ. The value of ~dcvand~dpware shown after LR2VQ ’s results.
ModelOriginal
top-1Original
sizeComp.
sizeComp.
ratioPQF DKM LR2VQ (~dcv=~dpw)
ResNet-18 71.30 44.59M1.54M 29x 67.98 68.09 69.56 (4/4)
1.03M 43x 64.39 64.33 67.21 (4/4)
ResNet-50 77.75 97.49M5.09M 19x 74.93 oom 76.17 (5/4)
3.19M 31x 73.40 73.88 74.48 (5/4)
Table 2: Compression regimes with kcv=kpw= 256 .
Architecture Regime mcvmpw~dcv~dpw
ResNet-18Small blocks 9 4[1;mcv] [1;mpw]Large blocks 18 4
ResNet-50Small blocks 9 4[1;mcv] [1;mpw]Large blocks 18 8
Results Table 1 shows the comparison of our LR2VQ with
k-means clustering against PQF and vanilla DKM on stan-
dard ResNet-18 and ResNet-50 with the conﬁgurations in
Table 2. The table shows that LR2VQ outperforms PQF and
vanilla DKM across all conﬁgurations. For ResNet-18 with
large block compression, LR2VQ presents a deﬁnite 2.8%
improvement over PQF under 43 compression factor. For
ResNet-50, LR2VQ consistently outperforms baselines for
more than 1% top-1 accuracy under 31 compression fac-
tor. Speciﬁcally, we mark the proper ~dcvand~dpwafter the
results of LR2VQ. As can be seen, dcvis 4 or 5 in both com-
pression regimes, which is lower than dcv= 9or18in PQF
and vanilla DKM. This result conﬁrms that low-dimensional
clustering effectively reduces the clustering error and the re-
construction error. Besides, it proves the advances in jointly
considering the compression ratio and the clustering dimen-
sionality, which possess great potential in beneﬁting vector
quantization. Another result we want to discuss here is the
performance of vanilla DKM. Based on the computation of
DKM, we expect it to be a better clustering method com-
pared to k-means as the clusters are optimized end-to-end.
Contrary to expectations, vanilla DKM has a similar perfor-
mance to PQF. This phenomenon may be explained by the
optimization of DKM also suffers from high-dimensional
clustering, which suggests varying the dimensionality in dif-
ferent clustering methods.
Experiments on COCO
To generalize LR2VQ to different datasets and architectures,
we compress Mask R-CNN with LR2VQ and experiment
on COCO datasets. We ﬁrst propose low-rank representa-
tion learning from scratch to obtain LRRs, then clustering
the subvectors in LRRs with k-means and ﬁne-tuning the
overall network on COCO datasets. The pre-trained network
size is different from (Martinez et al. 2021) because we uti-
lize the architecture in Detectron2(Wu et al. 2019) instead of
Pytorch(Paszke et al. 2019). For a fair comparison, the com-pression conﬁgurations for PQF and LR2VQ are the same as
in (Martinez et al. 2021). The results are demonstrated in Ta-
ble 3. Our method obtains 38.20 box AP, which remarkably
surpasses PQF with 0.91 AP under 26 compression fac-
tor. These results illustrate the generalizability of LR2VQ to
different vision tasks and architectures.
Table 3: Compression results for Mask R-CNN on COCO.
Method Size Ratio Box AP Mask AP
Uncompressed 174.37M - 40.34 36.54
PQF 6.65M 26  37.29 34.24
LR2VQ 6.65M 26  38.20 34.93
Ablation on the trade-off of ~d
We have discussed the trade-off of ~dand proposed assump-
tions on the variation of errors in Figure 2. Here, we ex-
tensively experiment on ~dto investigate this trade-off and
prove our assumption. We experiment on ImageNet to com-
pare the model performance with different ~d. We iterate ~dcv
among [1;9](or[1;18]) with small (or large) block regime
in ResNet-18, ~dpwamong [1;8]with large block regime in
ResNet-50, and ~dcvamong [1;9]with small block regime
in Mask R-CNN. These settings ensure that the dimension-
ality of LRRs varies in a wide range. Other conﬁgurations
are the same as in Table 2. The baseline we compared here
is PQF. We plot the accuracy of low-rank pre-trained net-
works and their corresponding quantized networks. Surpris-
ingly, these curves demonstrate a similar trend across dif-
ferent conﬁgurations and architectures. As ~drises, the ac-
curacy of LRR pre-trained models rapidly increases, then
oscillates around the original uncompressed network. The
quantized networks’ curves also perform similarly across
different settings, which ﬁrst increase to the top and then
decline with a rising ~d. These tendency broadly support the
assumptions on EaandErin Figure 2 and further guaran-
tees the proper ~dcan appropriately balance EaandEcto
achieve lower Er. All the peaks of red curves are the em-
pirical results to the proper ~din LR2VQ. Speciﬁcally, the
accuracy of compressed and uncompressed ResNet-18 with
~d= 1 is extremely tiny, indicating that the clustering error
in 1D space is negligible, so EadominatesErto produce
a poor performance for the compressed and uncompressed
networks. Almost all the LRR pre-trained networks become

--- PAGE 7 ---
comparable to the original networks before reaching ~d=m,
which implies enormous parameter redundancy in convolu-
tions. We note that ~dcvis ﬁxed to 5 in ResNet-50 with large
blocks, which limits the power of 33convolutions. So
there is always a gap between LRR pre-trained networks and
the original uncompressed network. Taking all these results
together, the trade-off of ~dprovides reliable insurance that
there must be a proper ~din LR2VQ to achieve better quanti-
zation performance.
1 3 5 7 9
̃d (3×3 conv)6466687072Top-1 accuracy(%)ResNet-18 with small blocksLR2VQ pre-train LR2VQ+k-means PQF baseline Uncompressed
1357911131517
̃d (3×3 conv)6063666972Top-1 accuracy(%)ResNet-18 with large blocks
1 3 5 7
̃d (1×1 conv)72747678Top-1 accuracy(%)ResNet-50 with large blocks
1 3 5 7 9
̃d (3×3 conv)3335373941Box APMask R-CNN with small blocks
Figure 3: Top-1 accuracy with different dimensionality.
Lower intrinsic dimensionality One unanticipated ﬁnd-
ing in Figure 3 is that LR2VQ outperforms PQF even at
~dcv=mcv. Intuitively, the clustering difﬁculty for ~d=m
is similar to PQF, so the performance of LR2VQ and PQF
should be close. However, in such circumstances, LR2VQ
outperforms PQF in all architectures. This discrepancy may
be attributed to our low-rank representation learning, which
implicitly learns lower intrinsic dimensionality to make the
subvectors more favourable for clustering. To validate this
speculation, we plot the intrinsic dimensionality of LRRs
by principal components analysis (PCA) in each layer with
~dcv=mcvin Figure 4. The x-axis is the layer index, and
the y-axis is the intrinsic dimensionality with a variance ra-
tio of more than 99.99% after PCA. As the ﬁgure shows,
the intrinsic dimensionality of LRRs tends to be much lower
than the original ﬁlters. For example, 11 layers learn lower
intrinsic dimensionality among 16 layers in large block com-
pression. These results suggest that LRRs can automatically
learn lower intrinsic dimensionality to beneﬁt clustering.
Results of searching ~d
To reduce the ﬁne-tune overhead, a theoretical analysis to
reﬁne candidate ~ds is very valuable. Here, we demonstrate
the estimation of our coarse method for searching ~d. As de-
scribed in previous sections, we apply our searching method
with ~d2[3;7]. Figure 5 presents the summation of jj
1 4 7 10 13 16
Layer index456789Intrinsic dimensionalityResNet-18 with small blocks
Standard
LRR
1 4 7 10 13 16
Layer index81012141618ResNet-18 with large blocks
Standard
LRRFigure 4: Intrinsic dimension of subvectors before cluster-
ing.Standard andLRR means pre-raining with vanilla ﬁl-
ter and low-rank representation ﬁlter.
for different ~d. As a starting point, our coarse estimation of
the proper ~dusing Equation 7 shows agreements with ex-
perimental results. All these results are reasonable because
the distribution of subvectors also affects clustering, which
is consistent with the results in the previous section. With
a ﬁxed number of centroids, reducing ~dcan decrease the
clustering difﬁculty, but hard-for clustering subvectors also
produce signiﬁcant clustering errors. Fortunately, our ex-
periments show that the learned subvectors in LR2VQ are
favourable for clustering. Hence, our coarse estimation pro-
vides the right direction towards the proper ~d.
3 4 5 6 7
Empirical ̃d=4−27−26−25−24−23log(|Σ|))
Estimation in ResNet-18
3 4 5 6 7
Empirical ̃d=5−25−24−23
Estimation in ResNet-50
Figure 5: The estimation of our coarse analytical method.
Empirical means ~dis given by the experimental results from
Figure 3. ~dwith a lowerjjis the better estimation.
Conclusion
We propose a new method called LR2VQ, which ﬁrst learns
low-rank representations (LRRs) and then quantizes LRRs
to achieve compression. LR2VQ decouples the subvector
size and the clustering dimensionality by quantizing the sub-
vectors in the learned LRRs, making it possible to realize
the variation of clustering dimensionality under a ﬁxed com-
pressed model size. The changeable nature of clustering di-
mensionality introduces a trade-off between the approxima-
tion error and the clustering error, which implies that the
value of ~dis critical to the performance of LR2VQ. We pro-
vide theoretical analysis and empirical observations to offer
estimations of the proper ~dafter LRR learning. We evalu-
ate LR2VQ on different datasets and architectures, and all
the results demonstrate that LR2VQ leads the state-of-the-
art performance among competitors. This paper provides the
ﬁrst comprehensive assessment of reducing clustering di-
mensionality, which is trustworthy for vector quantization.

--- PAGE 8 ---
References
Chen, W.; Wang, P.; and Cheng, J. 2020. To-
wards Convolutional Neural Networks Compression via
Global&Progressive Product Quantization. In BMVC .
Cho, M.; Vahid, K. A.; Adya, S.; and Rastegari, M. 2021.
DKM: Differentiable K-Means Clustering Layer for Neural
Network Compression. arXiv:2108.12659.
Courbariaux, M.; Bengio, Y .; and David, J.-P. 2016. Bi-
naryConnect: Training Deep Neural Networks with binary
weights during propagations. arXiv:1511.00363.
Fang, M.; Wang, Q.; and Zhong, Z. 2019. BETANAS: Bal-
ancEd TrAining and selective drop for Neural Architecture
Search. CoRR , abs/1912.11191.
Gong, Y .; Liu, L.; Yang, M.; and Bourdev, L. 2014. Com-
pressing Deep Convolutional Networks using Vector Quan-
tization. arXiv:1412.6115.
Guo, Y .; Yao, A.; and Chen, Y . 2016. Dynamic Network
Surgery for Efﬁcient DNNs. arXiv:1608.04493.
Han, S.; Pool, J.; Tran, J.; and Dally, W. J. 2015. Learn-
ing both Weights and Connections for Efﬁcient Neural Net-
works. arXiv:1506.02626.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015a. Deep Resid-
ual Learning for Image Recognition. arXiv:1512.03385.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015b. Delving
Deep into Rectiﬁers: Surpassing Human-Level Performance
on ImageNet Classiﬁcation. arXiv:1502.01852.
He, Y .; Zhang, X.; and Sun, J. 2017. Channel
Pruning for Accelerating Very Deep Neural Networks.
arXiv:1707.06168.
Howard, A.; Sandler, M.; Chu, G.; Chen, L.-C.; Chen, B.;
Tan, M.; Wang, W.; Zhu, Y .; Pang, R.; Vasudevan, V .; et al.
2019. Searching for mobilenetv3. In Proceedings of the
IEEE International Conference on Computer Vision , 1314–
1324.
Hubara, I.; Courbariaux, M.; Soudry, D.; El-Yaniv, R.; and
Bengio, Y . 2016. Quantized Neural Networks: Training
Neural Networks with Low Precision Weights and Activa-
tions. arXiv:1609.07061.
Iandola, F. N.; Han, S.; Moskewicz, M. W.; Ashraf, K.;
Dally, W. J.; and Keutzer, K. 2016. SqueezeNet: AlexNet-
level accuracy with 50x fewer parameters and ¡0.5MB
model size. arXiv:1602.07360.
Indyk, P.; and Motwani, R. 2000. Approximate Nearest
Neighbors: Towards Removing the Curse of Dimensional-
ity.Conference Proceedings of the Annual ACM Symposium
on Theory of Computing , 604-613.
Jegou, H.; Douze, M.; and Schmid, C. 2011. Product Quan-
tization for Nearest Neighbor Search. IEEE Trans. Pattern
Anal. Mach. Intell. , 33(1): 117–128.
LeCun, Y .; Denker, J.; and Solla, S. 1990. Optimal Brain
Damage. In Touretzky, D., ed., Advances in Neural Infor-
mation Processing Systems , volume 2. Morgan-Kaufmann.
Li, H.; Kadav, A.; Durdanovic, I.; Samet, H.; and Graf,
H. P. 2017. Pruning Filters for Efﬁcient ConvNets.
arXiv:1608.08710.Luo, J.-H.; Wu, J.; and Lin, W. 2017. ThiNet: A Filter
Level Pruning Method for Deep Neural Network Compres-
sion. arXiv:1707.06342.
Martinez, J.; Shewakramani, J.; Liu, T. W.; B ˆarsan, I. A.;
Zeng, W.; and Urtasun, R. 2021. Permute, Quantize,
and Fine-tune: Efﬁcient Compression of Neural Networks.
arXiv:2010.15703.
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;
Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,
L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,
M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,
J.; and Chintala, S. 2019. PyTorch: An Imperative Style,
High-Performance Deep Learning Library. In Wallach, H.;
Larochelle, H.; Beygelzimer, A.; d'Alch ´e-Buc, F.; Fox, E.;
and Garnett, R., eds., Advances in Neural Information Pro-
cessing Systems 32 , 8024–8035. Curran Associates, Inc.
Pham, H.; Guan, M. Y .; Zoph, B.; Le, Q. V .; and Dean, J.
2018. Efﬁcient Neural Architecture Search via Parameter
Sharing. arXiv:1802.03268.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;
Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;
et al. 2015. Imagenet large scale visual recognition chal-
lenge. International journal of computer vision , 115(3):
211–252.
Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; and
Chen, L.-C. 2018. Mobilenetv2: Inverted residuals and lin-
ear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , 4510–4520.
Son, S.; Nah, S.; and Lee, K. M. 2018. Clustering Con-
volutional Kernels to Compress Deep Neural Networks. In
ECCV .
Stock, P.; Joulin, A.; Gribonval, R.; Graham, B.; and J ´egou,
H. 2020. And the Bit Goes Down: Revisiting the Quantiza-
tion of Neural Networks. arXiv:1907.05686.
Tan, M.; and Le, Q. V . 2019. Efﬁcientnet: Rethinking model
scaling for convolutional neural networks. arXiv preprint
arXiv:1905.11946 .
Wu, J.; Leng, C.; Wang, Y .; Hu, Q.; and Cheng, J. 2016.
Quantized Convolutional Neural Networks for Mobile De-
vices. arXiv:1512.06473.
Wu, Y .; Kirillov, A.; Massa, F.; Lo, W.-Y .; and Girshick,
R. 2019. Detectron2. https://github.com/facebookresearch/
detectron2.
Zhang, X.; Zhou, X.; Lin, M.; and Sun, J. 2017. ShufﬂeNet:
An Extremely Efﬁcient Convolutional Neural Network for
Mobile Devices. arXiv:1707.01083.
Zhang, Y .; Zhang, J.; and Zhong, Z. 2020. AutoBSS: An
Efﬁcient Algorithm for Block Stacking Style Search. In
NeurIPS .
Zhong, Z.; Yang, Z.; Deng, B.; Yan, J.; Wu, W.; Shao, J.; and
Liu, C.-L. 2018. BlockQNN: Efﬁcient Block-wise Neural
Network Architecture Generation. arXiv:1808.05584.
Zoph, B.; Vasudevan, V .; Shlens, J.; and Le, Q. V . 2018.
Learning Transferable Architectures for Scalable Image
Recognition. arXiv:1707.07012.
