# Nén Blockwise các Mô hình dựa trên Transformer mà không cần Huấn luyện lại
Gaochen Dong, Wei Chen
Tensorchip, Bắc Kinh, Trung Quốc
gaochendong buaa@outlook.com

## Tóm tắt
Các mô hình dựa trên Transformer, được minh họa bởi GPT-3, ChatGPT và GPT-4, gần đây đã thu hút được sự chú ý đáng kể từ cả học thuật và công nghiệp do hiệu suất đầy hứa hẹn của chúng trong các tác vụ ngôn ngữ tổng quát. Tuy nhiên, những mô hình này thường liên quan đến các quá trình mã hóa tính toán phức tạp, và trong một số trường hợp, cả quá trình giải mã nữa, cả hai đều về cơ bản là phép nhân ma trận quy mô lớn. Những hoạt động này mang lại những thách thức không thể tránh khỏi về tài nguyên tính toán khổng lồ và dung lượng bộ nhớ lớn, thường yêu cầu ít nhất 10^23 FLOPs và hàng trăm gigabyte tương ứng. Một phương pháp phổ biến để giải quyết vấn đề này là giảm yêu cầu tính toán và bộ nhớ bằng cách áp dụng lượng tử hóa theo lớp cho transformer, thay thế kiểu dữ liệu fp32 thông thường bằng một tương đương bit thấp. Thật không may, phương pháp này thường dẫn đến giảm độ chính xác của mô hình và đòi hỏi việc huấn luyện lại tốn thời gian. Việc huấn luyện lại như vậy không chỉ yêu cầu kỹ năng tinh chỉnh mà còn cần tài nguyên tính toán đáng kể, gây ra thách thức cho người dùng. Để giải quyết cụ thể những vấn đề này, chúng tôi đề xuất BCT, một khung nén blockwise cho transformer mà không cần huấn luyện lại, nhằm tạo điều kiện cho việc triển khai mô hình. Không giống như các phương pháp nén theo lớp, BCT đạt được nén tinh tế hơn của toàn bộ transformer bằng cách hoạt động theo khối. Phương pháp này giảm thiểu độ lệch phân phối dữ liệu do lượng tử hóa gây ra, loại bỏ yêu cầu huấn luyện lại. BCT nén hiệu quả tất cả các thành phần của mô hình, bao gồm nhưng không giới hạn ở embedding, phép nhân ma trận, GELU, Softmax, chuẩn hóa lớp và các kết quả trung gian. Trong một nghiên cứu trường hợp, một mô hình hiệu quả được nén bởi BCT đạt được nén lên tới 7.988x. Tiếp theo, chúng tôi cũng đánh giá nó trên một số bộ dữ liệu General Language Understanding Evaluation (GLUE). Kết quả thực nghiệm trên phần lớn benchmark GLUE chứng minh hiệu quả của phương pháp chúng tôi, khi BCT đạt được sự suy giảm độ chính xác ít hơn 0.9% so với sự suy giảm hơn 1% thấy được với các phương pháp khác cung cấp tỷ lệ nén tương tự hoặc kém hơn.

**Từ khóa:** transformer; nén; blockwise; không huấn luyện lại

## 1 Giới thiệu
Các mô hình dựa trên Transformer[Vaswani et al., 2017], chẳng hạn như GPT-3[Brown et al., 2020], ChatGPT và GPT-4, đã chứng minh tiềm năng đáng kể và hiệu suất tiên tiến trong nhiều lĩnh vực, như phân loại cảm xúc, dịch máy, phân tích tài liệu, trả lời câu hỏi, tóm tắt văn bản, đối thoại nhiều vòng, phân loại hình ảnh, trả lời câu hỏi hình ảnh và suy luận thông thường hình ảnh. Kết quả là, nhu cầu triển khai những mô hình này trong các ứng dụng kinh doanh và khoa học, chẳng hạn như công cụ tìm kiếm tiên tiến, chatbot được hỗ trợ AI để nâng cao dịch vụ khách hàng và hỗ trợ nghiên cứu khoa học, đã tăng nhanh chóng.

Tuy nhiên, việc sử dụng rộng rãi các mô hình dựa trên transformer bị cản trở bởi yêu cầu tính toán và bộ nhớ đáng kể của chúng, chủ yếu do việc sử dụng cơ chế tự chú ý đa đầu. Thông thường, chúng yêu cầu ít nhất 10^23 FLOPs và hàng trăm gigabyte tương ứng. Yêu cầu cao đối với nền tảng phần cứng mang lại khó khăn không thể tin được cho việc triển khai và ứng dụng.

Để giải quyết những thách thức này, nén mô hình đã nổi lên như một chiến lược khả thi để giảm yêu cầu tính toán và bộ nhớ của các mô hình dựa trên transformer trong khi vẫn duy trì hiệu suất dự đoán của chúng. Các mô hình nén cung cấp tính toán nhanh hơn, dung lượng bộ nhớ giảm và yêu cầu băng thông thấp hơn, từ đó tạo điều kiện cho việc triển khai và ứng dụng. Trong trường hợp này, một số phương pháp nén được đề xuất để nén các mô hình dựa trên transformer với sự suy giảm độ chính xác có thể chấp nhận được. Tuy nhiên, hầu hết chúng đều yêu cầu huấn luyện lại rộng rãi, có thể mất vài tuần hoặc thậm chí vài tháng, để tinh chỉnh các tham số sử dụng bộ dữ liệu hiệu chuẩn được chuẩn bị kỹ để khớp với phân phối dữ liệu gốc.

Trong bài báo này, chúng tôi giới thiệu Nén Blockwise của Transformer mà không huấn luyện lại (BCT), một khung mới nén từng thành phần của transformer. Không giống như các khung nén trước đó có thể chỉ tập trung vào các thành phần cụ thể, BCT sử dụng phương pháp blockwise để nén toàn bộ transformer, bao gồm embedding, phép nhân ma trận, GELU[Hendrycks and Gimpel, 2016], Softmax, chuẩn hóa lớp[Ba et al., 2016] và tất cả các kết quả trung gian. Hình 1 minh họa khung của BCT. Các thí nghiệm chứng minh rằng, mà không cần huấn luyện lại, BCT có thể duy trì sự suy giảm độ chính xác ít hơn 0.9% trong hầu hết các tác vụ trên bộ dữ liệu GLUE[Wang et al., 2018].

Bài báo này có ba đóng góp chính: (1) Chúng tôi đề xuất BCT, một khung nén blockwise hiệu quả cho các mô hình dựa trên transformer loại bỏ nhu cầu huấn luyện lại. (2) Một phương pháp thực tế và hiệu quả được đề xuất để xử lý từng lớp transformer, bao gồm phép nhân ma trận và các phép toán phi tuyến. (3) Chúng tôi đánh giá BCT trên nhiều bộ dữ liệu GLUE, chứng minh hiệu quả của nó với sự suy giảm độ chính xác ít hơn 0.9% trong hầu hết các tác vụ, vượt trội nhiều phương pháp hiện có. Nhìn chung, BCT cung cấp một giải pháp hiệu quả và thực tế để triển khai các mô hình dựa trên transformer trong môi trường hạn chế tài nguyên mà không cần huấn luyện lại, giảm đáng kể các rào cản cho ứng dụng rộng rãi hơn của chúng.

## 2 Công trình liên quan
Trong những năm gần đây, việc nén các mô hình dựa trên transformer đã nổi lên như một lĩnh vực nghiên cứu nổi bật. Các mô hình nén có thể giảm đáng kể yêu cầu tài nguyên tính toán và dung lượng bộ nhớ. Một số phương pháp nén hiệu quả và thực tế, bao gồm cắt tỉa và lượng tử hóa, đã được đề xuất. Ví dụ, SparseGPT[Frantar and Alistarh, 2023] đạt được 60% độ thưa thớt bằng cách cắt tỉa GPT mà không cần huấn luyện lại. Các phương pháp như Q8BERT[Zafrir et al., 2019] và Q-BERT[Shen et al., 2020] thực hiện lượng tử hóa một phần trên BERT, trong khi sử dụng số học dấu phẩy động cho phần còn lại, một phương pháp thường được gọi là 'lượng tử hóa giả'. Tuy nhiên, các phương pháp trên không đạt được tỷ lệ nén tối ưu. I-BERT[Kim et al., 2021] lượng tử hóa BERT với số học chỉ số nguyên và suy luận phép nhân ma trận với int8 và các phép toán phi tuyến với int32. FQ-BERT[Liu et al., 2021] cũng lượng tử hóa hoàn toàn toàn bộ BERT và có thể được triển khai trên FPGA. Cả hai phương pháp đều đạt được tỷ lệ nén cao, đặc biệt là nén 7.94x cho FQ-BERT. Tuy nhiên, cả hai đều cần huấn luyện lại để tinh chỉnh các tham số. Trái ngược với những phương pháp hiện có này, khung được đề xuất của chúng tôi, BCT, đạt được tỷ lệ nén cao trong khi loại bỏ nhu cầu huấn luyện lại. BCT có thể duy trì sự suy giảm độ chính xác ít hơn 0.9% trong hầu hết các tác vụ với nén 7.988x, thậm chí cao hơn FQ-BERT.

## 3 Phương pháp
Trong bài báo này, BCT nén các tham số và kết quả trung gian của mô hình với nén blockwise và thực hiện phép nhân ma trận và các phép toán phi tuyến với số học bit thấp, như được hiển thị trong Hình 1.

### 3.1 Nén Blockwise
BCT chia các trọng số, độ lệch và kết quả trung gian của một mô hình thành các khối và nén chúng tương ứng. Kích thước của những khối này được xác định bởi nền tảng phần cứng cụ thể đang sử dụng. Việc cân bằng giữa hiệu quả phần cứng và hiệu suất mô hình khi chọn kích thước khối phù hợp là điều cần thiết. Nói chung, 64 là kích thước khối thân thiện với cả kích thước tham số mô hình và kích thước bộ nhớ. Không giống như nén theo lớp, ảnh hưởng đến phân phối dữ liệu ở quy mô lớn hơn, nén blockwise mang lại những thay đổi ở quy mô nhỏ hơn và ít bị ảnh hưởng bởi phân phối dữ liệu của các khối khác. Do đó, phân phối dữ liệu nén blockwise gần với phân phối gốc hơn.

Hình 2 cung cấp một nghiên cứu so sánh sâu giữa các thuật toán nén blockwise và layerwise. Hình 3, bằng cách sử dụng biểu diễn box plot, làm sáng tỏ hiệu quả phân phối dữ liệu cho dữ liệu gốc kết hợp với cả hai phương pháp nén trong Hình 2. Do đó, phân phối dữ liệu nén blockwise giống với phân phối gốc. Ngoài ra, nén blockwise tính đến thông tin tương quan giữa các mẫu, dẫn đến suy giảm độ chính xác thấp hơn nữa.

Do hiệu suất vượt trội của nén blockwise, BCT có thể nén trọng số, độ lệch và kết quả trung gian thành dữ liệu số nguyên bit thấp hoặc dữ liệu dấu phẩy động bit thấp, từ đó tiết kiệm tài nguyên tính toán và giảm dung lượng bộ nhớ. Hình 4 hiển thị dung lượng bộ nhớ của các kiểu dữ liệu khác nhau.

Trong bài báo này, chúng tôi áp dụng phương pháp lượng tử hóa dịch chuyển đối xứng[Miyashita et al., 2016] để nén dữ liệu fp32 thành dữ liệu số nguyên bit thấp, thân thiện với phần cứng hơn nén tuyến tính được áp dụng bởi phương pháp nén trước đó. Đối với nén k-bit của khối x, các hàm là:

shift = ⌊log₂(2^(k-1)/max(|x|))⌋ (1)
x_c = clip([x << shift], MIN, MAX) (2)

trong đó c có nghĩa là nén và MIN, và MAX được tính toán thống kê trên bộ dữ liệu hiệu chuẩn bằng cách đo độ phân kỳ KL. Điều này cho phép mỗi khối nén, được ký hiệu là x_c, có giá trị dịch chuyển riêng, có thể được xem như phần mũ của khối. Trái lại, khi nén dữ liệu theo lớp, tất cả dữ liệu nén chia sẻ một phần mũ duy nhất, có nghĩa là lượng tử hóa thô hơn và độ lệch phân phối dữ liệu lớn hơn. Ngoài dữ liệu số nguyên, BCT có thể cắt các phần mũ và mantissa của dữ liệu fp32 để có được dữ liệu fp8. Phạm vi biểu diễn của các kiểu dữ liệu chúng tôi sử dụng được hiển thị trong Bảng 1.

### 3.2 Phép nhân Ma trận
Khái niệm cốt lõi nằm trong quan sát rằng phép nhân ma trận có thể được thực hiện như phép nhân khối ma trận. Ví dụ, hãy xem xét phương trình Y = X·W^T + B, trong đó X biểu diễn đầu vào, Y biểu diễn đầu ra, W biểu diễn trọng số và B biểu diễn độ lệch. Trong BCT, X, W và B được chia thành các khối, sau đó được nén blockwise để có được cX, cW và cB tương ứng. Tiếp theo, BCT tiến hành phép nhân khối ma trận cX·cW^T và thêm độ lệch nén cB để cho ra đầu ra cuối cùng cY.

Khi thực hiện phép nhân ma trận, có một số chi tiết quan trọng cần xem xét. Trước khi tích lũy hoặc cộng, cần chuẩn hóa các phần mũ (giá trị dịch chuyển) của các khối. Quá trình chuẩn hóa này được minh họa trong Hình 5. Cụ thể, phần mũ shift_k của mỗi khối cX_ik·cW^T_jk nên được điều chỉnh đồng nhất để khớp với phần mũ tối đa shift_max = max(shift_k).

Quá trình tổng thể cho mỗi bước tích lũy có thể được phác thảo như sau:
acc_ij = Σ(k=1 to q) cX_ik·cW^T_jk << (shift_max - shift_k) (3)

Ngoài ra, các phần mũ của acc_ij và cC_j nên được đồng nhất trước khi cộng chúng.

Ngoài việc giảm thiểu độ lệch phân phối và do đó loại bỏ nhu cầu huấn luyện lại, việc thực hiện nén và tính toán theo khối trong các phép toán nhân ma trận mang lại một ưu điểm đáng kể khác. Cụ thể, nó tăng cường tính song song vốn có trong phép nhân ma trận. Quá trình này liên quan đến việc tùy chỉnh kích thước khối và xử lý mỗi phép nhân khối - liên quan đến việc nhân một khối dữ liệu đầu vào cX_ik với một khối trọng số cW^T_jk - như một thực thể độc lập. Sự độc lập này cho phép thực hiện đồng thời phép nhân ma trận theo khối. Kết quả là, tài nguyên tính toán có thể được sử dụng hiệu quả hơn, tốc độ tính toán được tăng lên và hiệu suất tổng thể được cải thiện. Do đó, phương pháp theo khối không chỉ tối ưu hóa phân bổ tài nguyên mà còn tăng hiệu quả chi phí và hiệu suất của các mô hình dựa trên transformer.

### 3.3 Phép toán Phi tuyến
Đối với các phép toán phi tuyến như GELU, Softmax và LayerNorm, một phương pháp thực dụng liên quan đến tính toán với dữ liệu dấu phẩy động bit thấp như fp8 hoặc sử dụng bảng tra cứu int8. Trong trường hợp sau, chúng tôi giới hạn đầu vào trong một phạm vi lấy mẫu cụ thể và xử lý đầu vào nén như các khóa. Đồng thời, đầu ra tương ứng với mỗi đầu vào được nén và coi như giá trị. Do đó, chúng tôi xây dựng một bảng tra cứu gồm 256 cặp khóa-giá trị có thể chứa bất kỳ đầu vào int8 nào. Tuy nhiên, thay vì trực tiếp lấy đầu ra từ bảng này, chúng tôi triển khai thuật toán nội suy để xấp xỉ những phép toán phi tuyến này. Phương pháp này dẫn đến suy giảm độ chính xác nhỏ hơn. Để minh họa, xem xét hai khóa, 'key_0' và 'key_1', gần nhất với đầu vào x. Khi tra cứu bảng, chúng tôi nhận được các giá trị tương ứng 'value_0' và 'value_1'. Đầu ra y sau đó được tính toán bằng công thức trung bình có trọng số:

y = ((x - key_1)/(key_0 - key_1)) * value_0 + ((x - key_0)/(key_1 - key_0)) * value_1

Phương pháp thực tế này cho phép xấp xỉ chính xác hơn các phép toán phi tuyến và góp phần vào hiệu quả nâng cao của phương pháp nén blockwise được đề xuất của chúng tôi.

#### 3.3.1 GELU
Đơn vị Tuyến tính Lỗi Gaussian (GELU), một hàm kích hoạt được sử dụng rộng rãi trong các mô hình dựa trên transformer, được định nghĩa về mặt toán học như sau:

GELU(x) = x · (1/2)[1 + erf(x/√2)] (4)

Do tính chất tuyến tính vượt trội, tương tự như Đơn vị Tuyến tính Chỉnh lưu (ReLU)[Xu et al., 2015], GELU trở thành ứng cử viên phù hợp cho các thuật toán nội suy. Những đặc điểm đặc biệt này cho phép chúng tôi nén đầu vào và đầu ra của GELU trực tiếp với suy giảm độ chính xác tối thiểu. Do đó, bằng cách xây dựng bảng tra cứu và thực hiện thuật toán nội suy dựa trên đầu vào và đầu ra nén, chúng tôi có thể duy trì mức độ chính xác thỏa đáng trong khi nâng cao hiệu quả tính toán.

#### 3.3.2 Softmax
Hàm Softmax, quan trọng trong các mô hình dựa trên transformer, được biểu diễn về mặt toán học như:

Softmax(x_i) = exp(x_i) / Σ_j exp(x_j) (5)

Trung tâm của phép toán Softmax là một thành phần mũ phi tuyến. Chúng tôi giải quyết điều này bằng cách bắt đầu phép toán mũ thông qua phương pháp tra cứu bảng. Do đó, đầu ra của Softmax có thể được dẫn xuất thông qua các phép toán số học bit thấp. Trước khi xây dựng bảng tra cứu cho phép toán mũ, việc trừ tất cả đầu vào cho giá trị tối đa tương ứng của chúng tỏ ra có lợi. Hành động này giới hạn đầu ra của phép toán mũ trong phạm vi từ 0 đến 1, mà không thay đổi đầu ra cuối cùng của hàm Softmax. Chiến lược này hiệu quả tránh vấn đề phân phối đầu ra quá rộng từ phép toán mũ có thể làm phức tạp việc nén. Hơn nữa, dữ liệu ngoài phạm vi có thể được quản lý bằng cách tận dụng tính chất e^(x+y) = e^x × e^y của phép toán mũ. Bằng cách lấy hai số trong phạm vi x và y làm đầu vào riêng biệt, chúng tôi có thể có được giá trị mũ cho tổng có thể ngoài phạm vi, x+y, như được mô tả trong Hình 6. Phương pháp này cho phép chúng tôi xử lý dữ liệu ngoài phạm vi một cách thích hợp và duy trì phương pháp nén blockwise hiệu quả.

#### 3.3.3 LayerNorm
Hàm Chuẩn hóa Lớp (LayerNorm) là một phương pháp chuẩn hóa phổ biến được sử dụng trong các mô hình dựa trên transformer, được định nghĩa như:

LayerNorm(x) = ((x - E[x]) / √(Var[x] + ε)) × γ + β (6)

Ở trung tâm của hàm LayerNorm nằm một thành phần phi tuyến — phép toán căn bậc hai. Phù hợp với phương pháp của chúng tôi, chúng tôi nén cả đầu vào và đầu ra của hàm căn bậc hai để xây dựng bảng tra cứu tương ứng. Dựa trên bảng này và sử dụng thuật toán nội suy, chúng tôi có thể dẫn xuất đầu ra của LayerNorm bằng số học bit thấp. Giống như phép toán mũ, chúng tôi tạo ra một chiến lược để xử lý dữ liệu nằm ngoài phạm vi lấy mẫu được chỉ định. Chúng tôi tận dụng nguyên lý toán học √(x+y) = √x * √y để xử lý dữ liệu ngoài phạm vi như vậy. Hơn nữa, hàm căn bậc hai cũng tồn tại trong lớp tỷ lệ của cơ chế chú ý, nơi chúng tôi áp dụng cùng phương pháp nói trên. Ứng dụng nhất quán này trên các thành phần khác nhau củng cố hiệu quả và tính phổ quát của BCT.

### 3.4 Kết quả Trung gian
Hoàn toàn khả thi để chia các kết quả trung gian thành các khối và áp dụng nén tương ứng. Một quy tắc bắt buộc chi phối quá trình này quy định rằng bất kỳ việc chuyển kết quả trung gian nào giữa hai lớp phải xảy ra bằng cách sử dụng các kiểu dữ liệu bit thấp. Trong những trường hợp mà đầu ra từ một lớp không đáp ứng đặc tả này, việc áp dụng lại chiến lược nén trở nên cần thiết, từ đó đảm bảo đầu ra tuân theo định dạng dữ liệu bit thấp. Do đó, các kết quả trung gian có thể được sử dụng như đầu vào, được tính toán theo khối với các tham số blockwise của lớp tiếp theo. Phương pháp này phù hợp với phương pháp rộng hơn của chúng tôi và góp phần duy trì hiệu quả tính toán mạnh mẽ trên tất cả các lớp.

## 4 THỰC NGHIỆM

### 4.1 Bộ dữ liệu
Để đánh giá toàn diện BCT, chúng tôi chọn một số bộ dữ liệu từ benchmark GLUE. Điều này bao gồm một tác vụ câu đơn (SST2), một tác vụ tương tự và diễn giải (STSB), và hai tác vụ suy luận (MNLI và RTE). Việc lựa chọn đa dạng các tác vụ như vậy cho phép chúng tôi xem xét kỹ lưỡng hiệu suất của BCT trên nhiều tình huống ứng dụng, từ đó cung cấp hiểu biết toàn diện về khả năng, hiệu quả và tính ứng dụng của nó trong các tình huống khác nhau.

### 4.2 Thiết lập
Do ChatGPT và GPT4 không phải mã nguồn mở, chúng tôi đã chọn sử dụng mô hình BERT nổi tiếng làm nền tảng thử nghiệm cho BCT. Nhân tiện, cần đề cập rằng các mô hình dựa trên transformer khác, chẳng hạn như ChatGPT và GPT4, sử dụng các toán tử tương tự BERT trong kiến trúc mô hình của chúng. Do đó, phương pháp nén BERT bằng BCT có thể áp dụng tương tự cho những mô hình này. Mô hình cơ sở được sử dụng để so sánh là mô hình BERT-base được cung cấp bởi Pytorch-Transformers. Để phân tích toàn diện, chúng tôi đối chiếu BCT với hai phương pháp lượng tử hóa hiện có - Q8BERT và FQBERT. Cụ thể, chúng tôi thiết kế bốn mô hình khác nhau, mỗi mô hình sử dụng một kiểu dữ liệu duy nhất cho mỗi lớp, như được chi tiết trong Bảng 2. Mô hình BCT int8/fp32 phục vụ để xác thực hiệu quả của nén blockwise mà không cần huấn luyện lại, làm cho nó phù hợp để so sánh với Q8BERT. Đáng chú ý, cả BCT int8/fp32 và Q8BERT đều chỉ lượng tử hóa các thành phần embedding và Feed-Forward Network (FFN) của BERT. Chúng tôi thiết kế BCT int4/8 và BCT int8 để thực hiện suy luận thông qua các phép toán số học số nguyên bit thấp. Mặt khác, mô hình BCT fp8 của chúng tôi được cấu hình để tiến hành suy luận sử dụng số học dấu phẩy động bit thấp, đòi hỏi ít tài nguyên tính toán nhất trong số bốn mô hình. Nó được theo sát bởi BCT int4/8 về hiệu quả tài nguyên. Hơn nữa, chúng tôi ủng hộ việc triển khai các phần tử encoder/decoder tốn nhiều tính toán của transformer trong bộ gia tốc. Đồng thời, thành phần embedding, đòi hỏi tài nguyên tính toán không đáng kể nhưng có tác động đáng kể đến độ chính xác, có thể được tính toán trong CPU.

### 4.3 Hiệu suất
Chúng tôi sử dụng độ chính xác của SST-2, MNLI, RTE và tương quan Spearman của STS-B làm thước đo so sánh. Kết quả benchmark của các tác vụ GLUE được hiển thị trong Bảng 3.

Trong phần lớn các tác vụ, BCT int8/fp32 thể hiện sự giảm độ chính xác thấp hơn so với Q8BERT. Điều này có thể được quy cho chiến lược nén blockwise được BCT sử dụng. Hơn nữa, BCT int4/8 chứng minh mức độ nén cao hơn (7.988x) khi so sánh với FQ-BERT (7.94x). Ví dụ, trong khi FQ-BERT sử dụng độ lệch 32-bit, BCT int4/8 sử dụng độ lệch 8-bit. Ngoài ra, đáng chú ý là BCT int4/8 thể hiện chỉ 0.80% suy giảm độ chính xác trên bộ dữ liệu SST-2, thậm chí ít hơn 0.81% giảm quan sát thấy trong FQ-BERT, tất cả mà không yêu cầu huấn luyện lại. Điều này chứng thực tính mạnh mẽ và hiệu quả của BCT int4/8. Hơn nữa, BCT int8 và BCT fp8 mang lại hiệu suất xuất sắc trên các bộ dữ liệu khác nhau. Thông qua những tiến bộ này, các mô hình dựa trên BCT đã thể hiện khả năng vượt trội để nén mô hình với suy giảm độ chính xác tối thiểu.

## 5 Kết luận
Trong nghiên cứu này, chúng tôi đã giới thiệu BCT nén blockwise hiệu quả các mô hình dựa trên transformer mà không cần huấn luyện lại. Không giống như các phương pháp nén truyền thống hoạt động ở cấp độ lớp, BCT hoạt động ở cấp độ khối tinh tế hơn trên toàn bộ mô hình. Việc nén của BCT sử dụng các kiểu dữ liệu fp8 hoặc int4 đạt được sự cân bằng giữa hiệu suất và tiêu thụ tài nguyên. Nó áp dụng đồng nhất trên tất cả các thành phần của mô hình transformer, hiệu quả loại bỏ độ lệch phân phối dữ liệu do lượng tử hóa gây ra và phủ nhận nhu cầu huấn luyện lại. Hơn nữa, phương pháp nén dịch chuyển thân thiện với phần cứng của BCT tăng cường hiệu quả tính toán. Lợi ích thực tế của BCT rõ ràng trong việc triển khai các mô hình dựa trên transformer. Một nghiên cứu trường hợp cho thấy BCT có thể nén một mô hình xuống 7.988x kích thước gốc của nó, duy trì hiệu suất cạnh tranh. Thử nghiệm trên các bộ dữ liệu GLUE khác nhau cũng chứng minh hiệu quả của BCT, với suy giảm độ chính xác ít hơn 0.9% trong hầu hết các tác vụ so với các phương pháp thay thế. Công trình của chúng tôi xác nhận rằng BCT giảm đáng kể yêu cầu tính toán và bộ nhớ trong khi bảo tồn mức hiệu suất gần như mô hình gốc. Tóm lại, BCT cung cấp một giải pháp thực tế cho việc triển khai các mô hình dựa trên transformer, đặc biệt là các Mô hình Học Ngôn ngữ (LLM) như ChatGPT và GPT4, vượt qua thách thức kỹ thuật và kinh tế trong nhiều tình huống.
