# 2305.18513.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2305.18513.pdf
# File size: 314386 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2305.18513v1  [cs.CL]  29 May 2023SLIMFIT: Memory-Efﬁcient Fine-Tuning of
Transformer-based Models Using Training Dynamics
Arash Ardakani1Altan Haan1Shangyin Tan1Doru Thom Popovici2
Alvin Cheung1Costin Iancu2Koushik Sen1
University of California, Berkeley1Lawrence Berkeley National Laboratory2
{arash.ardakani,altanh,shangyin,akcheung,ksen}@berk eley.edu
{dtpopovici,cciancu}@lbl.gov
Abstract
Transformer-based models, such as BERT and ViT, have achiev ed state-of-the-art re-
sults across different natural language processing (NLP) a nd computer vision (CV)
tasks. However, these models are extremely memory intensiv e during their ﬁne-tuning
process, making them difﬁcult to deploy on GPUs with limited memory resources. To
address this issue, we introduce a new tool called S LIMFITthat reduces the memory
requirements of these models by dynamically analyzing thei r training dynamics and
freezing less-contributory layers during ﬁne-tuning. The layers to freeze are chosen
using a runtime inter-layer scheduling algorithm. S LIMFITadopts quantization and
pruning for particular layers to balance the load of dynamic activations and to mini-
mize the memory footprint of static activations, where stat ic activations refer to those
that cannot be discarded regardless of freezing. This allow s SLIMFITto freeze up to
95% of layers and reduce the overall on-device GPU memory usa ge of transformer-
based models such as ViT and BERT by an average of 2 .2×, across different NLP
and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-1 0, CIFAR-100
and ImageNet with an average degradation of 0 .2% in accuracy. For such NLP and
CV tasks, S LIMFITcan reduce up to 3 .1×the total on-device memory usage with
an accuracy degradation of only up to 0 .4%. As a result, while ﬁne-tuning of ViT
on ImageNet and BERT on SQuAD 2.0 with a batch size of 128 requi res 3 and 2
32GB GPUs respectively, S LIMFITenables their ﬁne-tuning on a single 32GB GPU
without any signiﬁcant accuracy degradation. The code of th is paper is available at
https://github.com/arashardakani/SlimFit .
1 Introduction
Over the past few years, various transformer-based models h ave been developed with the adoption of
the attention mechanism that weighs the importance of each p art of the input data differently. Pre-
training of such transformer-based models on large data has led to a signiﬁcant boost in accuracy when
ﬁne-tuned on various natural language processing (NLP) and computer vision (CV) downstream tasks
[1, 2]. Despite their great performance in achieving state- of-the-art (SOTA) accuracy, these models are
memory intensive and require a considerably large amount of on-device GPU memory during their ﬁne-
tuning phase when compared to the conventional convolution al and recurrent neural networks [3]. The
memory requirement of current transformer-based models ha s made them difﬁcult to ﬁne-tune even on
powerful GPUs. With the introduction of larger transformer -based models over the past few years, the
on-device GPU memory has become a major bottleneck for their ﬁne-tuning process [3, 4, 5].
The total on-device memory usage of GPUs consists primarily of activations, parameters, gradients,
optimizer states, and the CUDA context. Among these factors , activations account for most of the
memory usage due to batching, which makes them several order s of magnitude larger than other factors
(see Fig. 1). Therefore, activation compressed training (A CT) has emerged as the primary solution
for memory-efﬁcient ﬁne-tuning [6, 4]. This approach ﬁrst c ompresses activations during the forward
pass and then decompresses them during the backward pass. In this way, the memory footprint can be
Preprint.

--- PAGE 2 ---
Batch Size: 32 Batch Size: 64 Batch Size: 128051015
6.1
9.1
15.43.2
6.4
12.84·10−1
4·10−1
4·10−12.4
2.3
2.2Memory (GByte)
Total Activations Parameters Others
Figure 1: The breakdown of memory usage of BERT when
ﬁne-tuned on different batch sizes including 32, 64, and 128 .signiﬁcantly reduced by caching the
compressed activations. In ACT, quan-
tization [7, 8, 6, 4] has been a popular
choice to compress activations among
other compressors such as JPEG [9] or
pruning [5]. The current SOTA ACT
adaptively assigns quantization bits to
each layer for a given architecture [4].
While the SOTA ACT successfully re-
duces the memory footprint of activa-
tions, its overall on-device GPU mem-
ory reduction is not signiﬁcant. For in-
stance, the total on-device GPU memory reduction of the SOTA ACT is limited to 0.1GB despite its
6.4×reduction in the memory of activations when ﬁne-tuning BERT on CoLA dataset with a batch size
of 32. It is worth mentioning that we refer to the memory usage reported by “nvidia-smi” as the overall
on-device memory in this paper (see Appendix A for more infor mation on memory management).
Tensor rematerialization [3, 10, 11, 12], also known as grad ient checkpointing, is another prominent ap-
proach to reducing activation memory by trading computatio ns for memory. In tensor rematerialization,
only speciﬁc activations are stored during the forward pass , while the rest are recomputed in the back-
ward pass. Of course, recomputing activations requires mor e operations and signiﬁcantly prolongs the
ﬁne-tuning process [4]. Reduced precision training, as ano ther approach, performs the computations of
both forward and backward passes in low-precision [13, 14, 1 5, 16]. While these works can successfully
train conventional models, few-bit model ﬁne-tuning is not trivial. For instance, 8-bit quantization of
BERT for inference results in a signiﬁcant precision loss [1 7], which makes ﬁne-tuning on few bits a
challenging task.
Low-rank adaptation (LoRA) [18] is another key approach to r educing the overall on-device GPU mem-
ory where the transformer-based models are ﬁne-tuned by ins erting a small number of trainable param-
eters into each layer while keeping the pre-trained model pa rameters frozen. Such an approach enables
ﬁne-tuning transformer-based models with signiﬁcantly le ss number of trainable parameters, leading
to a reduction in the memory footprint of optimizer states an d gradients. Such a memory reduction
becomes signiﬁcant for extremely large transformer models such as GPT [19] with over hundred billion
parameters.
Different from these methods, we put forward a new approach t o reducing the overall on-device memory
usage by analyzing training dynamics. More precisely, we dy namically analyze the gradient contribu-
tions of layers in transformer-based models and perform par ameter updates for speciﬁc layers only
while the rest of layers are kept frozen. Training dynamics h ave been used to analyze the behavior of a
model during its training/ﬁne-tuning process [20, 21, 22]. However, our work uses training dynamics to
detect and discard unimportant activations during ﬁne-tun ing by freezing their associated layers, lead-
ing to a reduction of the memory footprint. Our method is orth ogonal to existing approaches including
rematerialization and LoRA, which could be composed for fur ther reductions.
Freezing layers or parameters has been studied in different domains, including transformer-based mod-
els to preserve previously learned information during ﬁne- tuning [23]. Freezing parameters have also
been used to regularize ﬁne-tuning (e.g., over-ﬁtting redu ction) in pre-trained models [24]. Recently,
freezing has been used to accelerate ﬁne-tuning by progress ively freezing model blocks [25, 26, 27].
However, since such an approach starts the ﬁne-tuning proce ss without freezing at least for a few train-
ing iterations, its overall on-device memory requirement r emains similar to that of training without
freezing. For instance, ﬁne-tuning ViT on ImageNet with a ba tch size of 128 using such a freezing
approach on a single 32GB GPU results in an out-of-memory err or (see Appendix B for more details).
To orchestrate effective layer-freezing decisions, we int roduce a runtime inter-layer scheduling (ILS)
algorithm. Our method ﬁnds and freezes a set of layers in tran sformer-based models that are less con-
tributory, i.e., layers with fewer updates in their paramet ers, to the ﬁne-tuning process at each iteration.
While the ILS algorithm successfully detects and freezes un important layers, its memory reduction is
not proportional to the freezing rate. The reason behind thi s disproportionality is twofold: the imbal-
anced number of activations among layers and the existence o f static activations. Static activations refer
to those that cannot be discarded regardless of freezing (e. g., activations of non-linear functions such as
GELU). We address these two issues using quantization and pr uning to even out the number of activa-
2

--- PAGE 3 ---
tions across all layers and to reduce the memory overhead of s tatic activations. We use quantization and
pruning for a few speciﬁc layers of transformer-based model s as opposed to reduced precision training
methods where all the layers are quantized. As a result, the i mpact of quantization and pruning on
accuracy is insigniﬁcant in our work. For instance, the accu racy degradation due to quantization and
pruning is only 0 .1% on the MRPC dataset.
By combining ILS with quantization and pruning, we introduc e a performance tool called S LIMFIT
for reducing the on-device GPU memory usage of transformer- based models during ﬁne-tuning. We
demonstrate the effectiveness of S LIMFITin reducing the memory footprint on popular models of BERT
and ViT. We show that S LIMFITcan freeze up to 95% of layers and reduce the overall on-devic e memory
usage by an average of 2 .2×when ﬁne-tuning BERT and ViT models on different benchmarks and
datasets, such as GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 and I mageNet with an average accuracy
degradation of 0 .2%. More precisely, S LIMFITreduces the overall on-device memory usage of the ﬁne-
tuning process on GLUE from 6.1GB to 4.0GB (1 .5×reduction) with a batch size of 32, on SQuAD
2.0 from 58.5GB to 19.1GB (3 .1×reduction) with a batch size of 128, on CIFAR-10 from 7.2GB to
4.3GB (1.7×reduction) with a batch size of 32, on CIFAR-100 from 7.2GB to 4.5GB (1.6×reduction)
with a batch size of 32, and on ImageNet from 77.4GB to 26.1GB ( 3.0×) with a batch size of 128 at the
cost of up to 0 .4% accuracy degradation. As a result, S LIMFITenables performing memory-intensive
ﬁne-tuning processes on a single 32GB GPU such as ﬁne-tuning ViT on ImageNet with a batch size of
128 while this normally requires three 32GB GPUs.
2 Preliminaries
Over the past few years, pre-training of attention-based mo dels has led to signiﬁcant advances on many
NLP and CV tasks with the popular BERT [1] and ViT [2] models. T he pre-training process provides
a good initialization point such that these models can bette r generalize on unseen data of downstream
tasks. Therefore, these models can achieve state-of-the-a rt results by ﬁne-tuning through small adjust-
ments to their parameters. Architecturally, these models c onsist of an initial embedding layer, followed
by repeated blocks of multi-head attention (MHA) fed into a f eed-forward network (FFN) module (see
Appendix C for more details). The base architectures of BERT and ViT contain over a hundred layers
built up in this manner.
Despite the large number of layers, not all need to be updated during ﬁne-tuning to achieve decent
performance on downstream tasks, as shown in [28]. Notably, the authors found that freezing approxi-
mately 60% of early attention layers in BERT led to negligibl e performance degradation. This suggests
that the ﬁne-tuned model tends to preserve generic features learned during pre-training. Motivated by
this study, we seek to analyze the training dynamics of pre-t rained models and to automatically detect
layers with less contributions to the ﬁne-tuning process.
3 Learning the Importance of Layers
Training dynamics is an active ﬁeld of research that provide s insight about the behavior of pre-trained
models when ﬁne-tuning on downstream tasks. The convergenc e proof of optimization algorithms such
as stochastic gradient descent [29] shows that the distance between the parameters and the optimal
solution is reduced over training iterations and according ly, the weight distance (or the weight update
amount) between consecutive iterations decreases. Theref ore, it is possible that some layers can only
receive minimal changes to their parameters as we approach t he end of the training process. Of course,
detecting and freezing such layers, when they show minimal u pdates, will not affect accuracy. Since
transformer-based models are pre-trained, they already sh ow small updates during ﬁne-tuning compared
to pre-training. As such, detecting and freezing layers wit h minimal updates (i.e., weight distance
values) will not signiﬁcantly affect the ﬁne-tuning proces s and accordingly the ﬁnal accuracy. Based
on the above observations, we consider the ℓ1-norm of the update received by parameters of each layer
through all the ﬁne-tuning iterations as the training dynam ics in this paper. It is also worth mentioning
that freezing layers has no impact on training convergence a s it causes a pause in the training procedure
of frozen layers as shown by our theoretical analysis in Appe ndix D.1.
3.1 Training Dynamics
Let us consider a pre-trained model with a set of parameters Wwhere the parameters associated with
theith layer at iteration tis denoted as Wt
i∈RM×I. The training dynamics of for the ith layer at iteration
tis deﬁned as the ℓ1-norm of the distance between Wt−1
iandWt
i, i.e.,
3

--- PAGE 4 ---
0 100 200 300 400 500 600 700 8000246
IterationDistance ValueQuery Weights (layer #1)
Query Weights (layer #5)
Query Weights (layer #11)
(a) CoLA0 50 100 150 200 250 3000510
IterationDistance ValueQuery Weights (layer #1)
Query Weights (layer #5)
Query Weights (layer #11)
(b) MRPC
Figure 2: The distance values of query weight matrix for the ﬁ rst, ﬁfth and eleventh attention layers of
BERT-base ﬁne-tuned on (a) CoLA and (b) MRPC datasets for 3 ep ochs.
dt
i=1
M×I/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleWt
i−Wt−1
i
Wt−1
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
ℓ1, (1)
where dt∈Rn
+containing all dis at iteration tis referred to as distance vector, and ndenotes the total
number of layers. In fact, Eq. (1) calculates the normalized change in the parameters of the ith layer.
3.2 Inter-Layer Scheduling Algorithm
We use the distance values as training dynamics to analyze th e ﬁne-tuning behavior of pre-trained mod-
els. For instance, consider the distance values across all t he ﬁne-tuning iterations for the CoLA [30] and
MRPC [31] datasets. Fig. 2a shows the distance values of the q uery weight matrix for the ﬁrst, ﬁfth
and eleventh attention layers of BERT-base ﬁne-tuned on CoL A dataset whereas Fig. 2b depicts those
of the same layers for BERT-based ﬁne-tuned on MRPC dataset.
We observe the following based on the experimental results o f these two datasets. First, the updated
amount for each layer becomes smaller over ﬁne-tuning itera tions. Second, the updated amount of each
layer is task-speciﬁc and is independent of its position. Th ird, there are some layers showing smaller
distance values w.r.t. other layers across almost all the it erations. Finally, layers with a higher distance
value in the beginning can become smaller over the ﬁne-tunin g iterations than layers starting with a
lower distance value.
Given the above observations, we introduce an ILS algorithm to decide on updating priority of layers
using their distance values. Fig. 3 shows the overview of the ILS algorithm. At each iteration ranging
from the ﬁrst iteration to the last iteration, our ILS algori thm selects those layers with large distance
values to be updated and those with small distance values to b e frozen. More precisely, layers are ﬁrst
ranked based on their distance values at each training itera tion and then those with small distance values
are kept frozen according to the freezing rate as a hyper-par ameter. The intuition is that layers with small
distance values are less contributory to the ﬁne-tuning pro cess as their parameters are not being updated
much. On the other hand, the layers with large distance value s are learning task-speciﬁc patterns by
making more signiﬁcant adjustments to their parameters. No te that freezing middle layers does not
interrupt the gradient propagation to the early layers of th e network as shown through an example in
Appendix D.2.
The freezing rate of the ILS algorithm can be decided based on the on-device GPU memory budget.
Of course, using an extremely high freezing rate may result i n a performance degradation depending
ILS ILS ILS ILS
Transformer Model Transformer Model Transformer Model Tra nsformer ModelTraining Iteration 0 Training Iteration 1 Training Iterati on 2 Training Iteration n-1
Frozen Active Freezing decision Training dynamics
Figure 3: The overview of ILS algorithm. ILS freezes a certai n number of layers depending on the freez-
ing rate at every single iteration throughout the ﬁne-tunin g process for the total of ntraining iterations.
4

--- PAGE 5 ---
Layer #1
Layer #2
Layer #3
Layer #4
Layer #5
Layer #6
Layer #7
Layer #810.5
12.3
8.5
9.4
11.4
3.5
6.7
13Transformer Model
Distance Vector
ILS
Iteration t−1Layer #1
Layer #2
Layer #3
Layer #4
Layer #5
Layer #6
Layer #7
Layer #88.1
5.3
8.5
9.4
7.1
3.5
6.7
11Transformer Model
Distance Vector
ILS
Iteration tLayer #1
Layer #2
Layer #3
Layer #4
Layer #5
Layer #6
Layer #7
Layer #84.2
5.3
4.9
6.4
7.1
3.5
6.7
10Transformer Model
Distance Vector
Iteration t+1
Frozen Frozen layers are not being updated. Active Active layers are being updated.
Figure 4: An example of the iterative freezing process using
our ILS algorithm.Algorithm 1 The pseudo code of the ILS
algorithm performing iterative freezing.
Input: model, number of iterations as
itr, number of layers as L, freezing rate
F
d= rand( L)
foriinitrdo
idx= argsort( d)[:int(L*F)]
forjinidxdo
model.layer[ j].requires_grad =
False
end for
model.train()
Update d
end for
on the downstream task, providing a worthwhile trade-off be tween accuracy and on-device GPU mem-
ory. On the other hand, while performance degradation is unl ikely with a very small freezing rate, the
memory reduction is insigniﬁcant as well.
Since there is no prior knowledge about the distance values o f each layer at the beginning of the ﬁne-
tuning process, our ILS algorithm initializes the distance vector with large random values. Depending on
the freezing rate, each layer along with its distance value a re updated during the ﬁrst few iterations once
until all random numbers in the distance vector are substitu ted with an actual distance value. Afterwards,
layers are kept frozen according to their actual distance va lue. The distance value of the active layers is
only updated at each iteration while that of the frozen layer s remains unchanged. The pseudo code of
our ILS algorithm performing iterative freezing is shown in Algorithm 1.
To better understand the ILS algorithm, we illustrate the it erative freezing process using an example as
shown in Fig. 4. Suppose we have an 8-layer transformer-base d model and accordingly an 8-element
distance vector at iteration t. Considering the freezing rate of 50% for this example, 4 lay ers with the
lowest distance values are kept frozen and the rest are updat ed at each iteration.
4 Inter-Layer Load-Balancing
So far, we have introduced our ILS algorithm that prioritize s updating particular layers while keeping
the rest of layers frozen according to their distance value. For the given freezing rate of 50% as an
example, we expect to see a 2 ×reduction in the memory footprint of activations. However, this is not
the case in transformer-based models due to the imbalanced t he number of activations across all the
layers. In fact, the imbalance in the number of activations u ndermines the ability of our ILS algorithm
in reducing the memory footprint during the ﬁne-tuning as sh own in Fig. 5.
Since the focus of this paper is on transformer-based models such as BERT and ViT, we analyze their
architecture for imbalanced layers. Table 1 summarizes the number of activations associated to the
input of layers with trainable parameters in BERT or ViT. Amo ng all trainable layers, there is only one
imbalanced layer in the attention block which contains 4 ×more activations than other layers.
To address the load-balancing issue in the number of activat ions for the aforementioned layer, we use
quantization. Since the imbalance factor among layers is 4 ×, we adopt 8-bit quantization for activations
of the imbalanced layer where 4 bits are used for both the inte ger and fractional parts. In this way, the
memory cost of the activations are evened out using quantiza tion. In our quantization scheme, we cache
the activations of the imbalanced layer using 8 bits during t he forward pass. In the backward pass, we
convert the 8-bit activations to 32-bit ﬂoating-point form at. Therefore, all the forward and backward
computations are still performed using single-precision ﬂ oating-point format. The conversion process
between 8-bit ﬁxed-point and 32-bit ﬂoating-point formats are provided in Appendix E.
5 Dynamic and Static Activations
The type of activations in transformer-based models can be d ivided into two categories: dynamic and
static. We refer to the activations that can be discarded by f reezing their layer as dynamic activa-
tions. On the other hand, static activations cannot be disca rded regardless of freezing. Among different
5

--- PAGE 6 ---
Layer #1 (Size: B * 128 * 768)
Layer #2 (Size: B * 128 * 3072)
Layer #3 (Size: B * 128 * 768)
Layer #4 (Size: B * 128 * 3072)Transformer-based Model
Frozen
Active
B: Batch Size
Memory Reduction = 1 .25×
Figure 5: An example of a model with imbal-
anced number of activations and its impact on
the memory reduction.Type of Layer Description # Activations Status
Dense attention.self.query B∗T∗H Balance
Dense attention.self.key B∗T∗H Balance
Dense attention.self.value B∗T∗H Balance
Dense attention.output B∗T∗H Balance
LayerNorm attention.output B∗T∗H Balance
Dense intermediate B∗T∗H Balance
Dense output B ∗T∗4∗H Imbalance
LayerNorm output B∗T∗H Balance
Table 1: The number of activations associated to the in-
put of layers with trainable parameters in BERT where
B,T,Hdenote the batch size, sequence length, hidden
size, respectively. ViT has the same structure with dif-
ferent descriptions.
types of layers, GELU, MatMul, Softmax and LayerNorm contai n static activations as shown Table 2.
Note that MatMul and Softmax share the same activations. For the backward computations of Softmax,
its output during the forward pass is saved as its activation s. On the other hand, the input to MatMul
is required for its backward computations as activations. S ince the output of Softmax is an input to
MatMul in the forward pass, they share the same activations.
GELU and MatMul/Softmax do not have any trainable parameter s and accordingly cannot be frozen.
Therefore, these two layers hold on to their activations thr oughout the ﬁne-tuning process. The best
approach to reduce their memory cost is quantization. We use 4 and 8 bits for quantization of activations
in GELU and MatMul/Softmax, respectively. Since there is no 4-bit tensor support in PyTorch, we store
each two 4-bit activations as a single 8-bit activations usi ng shift operations. Note that using such bit-
levels result in a negligible accuracy degradation while fu rther quantization of those activations incurs
a signiﬁcant accuracy loss.
As opposed to GELU and MatMul/Softmax, LayerNorm contains t rainable parameters and can be frozen
by the ILS algorithm. However, its activations are still sta tic. The forward pass of LayerNorm is
computed by:
/tildewidex=x−E(x)/radicalbig
Var(x)+ε, (2)
y=/tildewidex∗γ+β, (3)
where γandβare trainable parameters. The input and output to LayerNorm are denoted by x∈RHand
y∈RH, respectively. E(·)and Var(·)compute the average and variance, respectively. The deriva tive of
the loss with respect to γ(i.e.,/hatwideγ) is computed by
/hatwideγ=/tildewidex∗/hatwidey, (4)
and with respect to β(i.e.,/hatwideβ) by:
/hatwideβ=/hatwidey, (5)
where/hatwideydenotes the derivative of the loss w.r.t. y. We also need to compute the derivative of the loss
with respect to x(i.e.,/hatwidex) as:
g=γ∗/hatwidey
H∗/radicalbig
Var(x)+ε, (6)
/hatwidex=H∗g−∑
Hg−/tildewidex∗∑
H(g∗/tildewidex). (7)
When LayerNorm is frozen, there is no need to compute Eq. (4). However, the activations of this
layer cannot be discarded since they are still a part of the co mputations in Eq. (7). More precisely, the
standardized version of x(i.e.,/tildewidex) is required even when this layer is frozen.
The contribution of the last term in Eq. (7) (i.e., ∑H(g∗/tildewidex)) is signiﬁcant for large values of /tildewidexonly.
Therefore, the small values of /tildewidexcan be discarded. Ideally, we want to have all the activation s of this
layer to be discarded when this layer is frozen. However, thi s will results in an accuracy degradation. As
Table 2: The type of activations of layers in MHA and FFN of BER T and ViT.
Type of Layer # Activations Type of Activations Type of Layer # Activations Type of Activations
Dense B∗T∗H Dynamic LayerNorm B ∗T∗H Static
MatMul B ∗T∗H(2×) Static Dense B∗T∗H Dynamic
Softmax B ∗T∗T Static GELU B ∗T∗4∗H Static
MatMul B ∗T∗H&B∗T∗T Static Dense B∗T∗4∗H Dynamic
Dense B∗T∗H Dynamic LayerNorm B ∗T∗H Static
6

--- PAGE 7 ---
Table 3: The accuracy and memory performance of S LIMFITon the GLUE benchmark and SQuAD 2.0.
The batch size of 32 and 128 were used for GLUE benchmark and SQ uAD 2.0, respectively.
Method Metric MNLI mQQP QNLI SST-2 CoLA STS-B MRPC RTE SQuAD 2.0
BERT (Basline)Accuracy 83.4 90.8 90.5 92.1 58.9 89.5 86.4 70.2 74.0
Memory of Activations (GB) 3.2 3.2 3.2 3.2 3.2 3.2 3.2 3.2 55.1
Total On-chip GPU Memory (GB) 6.1 6.1 6.1 6.1 6.1 6.1 6.1 6.1 58 .5 (2 GPUs)
SLIMFITAccuracy 83.3 90.4 90.4 92.3 59.6 89.4 86.3 70.4 74.0
Freezing Rate (%) 80 80 95 95 90 85 91 90 80
Memory of Activations (GB) 0.7 0.7 0.5 0.5 0.6 0.7 0.6 0.6 10
Total On-chip GPU Memory (GB) 4.4 4.4 4.0 4.0 4.3 4.3 4.3 4.3 19 .1
such, we prune away the small values in /tildewidexand keep the top 10% largest values. In this way, the memory
load of activations is signiﬁcantly reduced. Of course, whe n this layer is not frozen, the backpropagation
is performed without any approximation. Such a trick conver ts LayerNorm from a static layer to a semi-
static one. It is worth mentioning that the indices to pruned activations are also stored along with
activations. The details of the pruning procedure is provid ed in Appendix F.
6 S LIMFIT
SLIMFITis a performance tool that exploits our ILS algorithm along w ith quantization and pruning to
reduce the memory footprint of activations through an itera tive freezing process. The total on-device
GPU memory reduction of S LIMFITis a result of the memory reduction in both dynamic and static
activations. Static activations contribute a ﬁxed amount o f memory whereas the memory usage of
dynamic activations depends on the freezing rate. Given a hi gh freezing rate, the memory footprint of
activations and accordingly the total on-device GPU memory usage can be signiﬁcantly reduced. The
choice of freezing rate depends on the memory budget of the us er. By increasing the freezing rate up to
a certain point, there will be no performance degradation. H owever, using an extremely high freezing
rate trades off memory for accuracy. Finding the breaking po int of the method is task dependent and
varies from one dataset to another.
7 Experimental Results
We use the base version of BERT and ViT for our experiments. We ﬁne-tune these two models using
SLIMFITwhich is implemented on PyTorch. We evaluate BERT [1] using t he GLUE benchmark [31]
and SQuAD 2.0 [32]. For ViT [2], we use CIFAR-10, CIFAR-100 an d ImageNet datasets [33, 34]
for evaluation purposes. We discuss the memory usage of acti vations and the overall on-device GPU
memory on the 32GB NVIDIA V100 GPU. We report the total on-dev ice GPU memory usage using
“nvidia-smi”. For all the experiments in this section, we us e 3 epochs for ﬁne-tuning. The details about
the CV/NLP tasks, measurements and hyper-parameter settin gs are provided in Appendix G.
7.1 Accuracy Evaluation on GLUE and SQuAD 2.0
To evaluate the language understanding ability of BERT mode ls, the GLUE benchmark is formed by
a series of downstream tasks including sentiment classiﬁca tion (SST-2), natural language inference
(RTE, QNLI, and MNLI), paraphrase detection (MRPC, QQP, and STS-B), and linguistic acceptability
(CoLA). We use Spearman correlation for STS-B, Matthew’s co rrelation for CoLA, percentage accuracy
for RTE, MRPC, SST-2, QQP, QNLI and MNLI m, and F1 score for SQuAD 2.0. In this work, we ﬁne-
tune the BERT-base model using S LIMFITon the downstream tasks of the GLUE benchmark as well
as the question answering task on SQuAD 2.0. Table 3 shows the accuracy on the validation set of
the aforementioned tasks and memory usage of S LIMFITcompared to the baseline. The results of the
baseline were obtained without freezing. We report the resu lts associated with the highest freezing rate
that can achieve a similar accuracy to that of the baseline by varying the learning rate. The experimental
results on the GLUE benchmark show that up to 95% of dynamic ac tivations can be discarded with
up to 0.4% accuracy degradation, leading to an average of 1.9GB redu ction in the total on-device GPU
memory usage. On the other hand, while ﬁne-tuning SQuAD 2.0 w ithout freezing requires the minimum
of 2 32GB NVIDIA V100 GPUs on a batch size of 128, S LIMFITenables its ﬁne-tuning on a single
32GB NVIDIA V100 GPU, reducing the total on-device memory re quirement of such a task from
58.5GB down to 19.1GB (3 .1×reduction).
Figure 6 shows the total on-device GPU memory usage of BERT wh en ﬁne-tuned using S LIMFITfor
different batch sizes at the freezing rate of 95% on the GLUE b enchmark and 80% on SQuAD 2.0.
According to the experimental results, S LIMFITenables a reduction ranging from 1 .5×to 3.1×in the
total on-device GPU memory on NLP tasks. The reduction in the total on-device memory usage is more
signiﬁcant for larger batch sizes since the activations dom inate the memory footprint.
7

--- PAGE 8 ---
GLUE SQuAD 2.0 CIFAR-10 CIFAR-100 ImageNet020406080
6.1
16.4
7.2
7.2
20.54
7.3
4.3
4.5
8.99.1
29.6
11.4
11.4
40.15.1
11.1
5.8
6.3
14.915.4
58.5
20.3
20.3
77.46.8
19.1
8.7
9.6
26.1Total On-device
GPU Memory (GB)
Baseline-32 SLIMFIT-32 Baseline-64 SLIMFIT-64 Baseline-128 SLIMFIT-128
Figure 6: The total on-device GPU memory usage of S LIMFITcompared to the baseline across different
batch sizes including 32, 64 and 128 on NLP and CV datasets.
Table 4: The top-1 accuracy and memory performance of S LIMFITon CV benchmarks using a batch
size of 32 for CIFAR datasets and 128 for ImageNet dataset.
Baseline S LIMFIT
Model Metric CIFAR-10 CIFAR-100 ImageNet CIFAR-10 CIFAR-1 00 ImageNet
ViTAccuracy (%) 98.8 91.2 83.3 98.5 91.0 83.3
Freezing Rate (%) NA NA NA 90 75 95
Memory of Activations (GB) 4.5 4.5 69.5 0.8 1.0 11.9
Total Memory (GB) 7.2 7.2 77.4 (3 GPUs) 4.3 4.5 26.1
7.2 Accuracy Evaluation on CIFAR and ImageNet
To assess the effectiveness of our method on CV tasks, we ﬁne- tune the ViT-base model on CIFAR-10,
CIFAR-100 and ImageNet datasets. We use the test set of CIFAR -10/CIFAR-100 and the validation set
of ImageNet to evaluate their accuracy on ViT. Table 4 shows t hat S LIMFITcan ﬁne-tune the ViT-base
model with the freezing rate of up to 95% with up to 0 .3% loss in accuracy while signiﬁcantly reducing
the overall on-device GPU memory usage. More speciﬁcally, S LIMFITreduces the overall memory
usage of the ﬁne-tuning process on CIFAR-10 from 7.2GB to 4.3 GB (1.7×reduction) with a batch size
of 32, on CIFAR-100 from 7.2GB to 4.5GB (1 .6×reduction) with a batch size of 32, and on ImageNet
from 77.4GB to 26.1GB (3 ×reduction) with a batch size of 128. Fig. 6 also shows the tota l on-device
GPU memory usage of S LIMFITacross different batch sizes on CV tasks.
8 Ablation Studies
In this section, we study different aspects of S LIMFITin ﬁne-tuning of transformer-based models
through a series of ablation studies. Due to limited space, w e discuss the impact of quantization/pruning
and total wall-clock time in Appendix H and Appendix I, respe ctively. For all the experiments in this
section, we use a batch size of 32 and 3 epochs for ﬁne-tuning.
8.1 Accuracy vs Freezing Rate
In Section (3.2), we discussed that our ILS algorithm orches trates the freezing schedule based on a
simple rule: layers with largest distance values are update d whereas those with lowest distance values
are kept frozen for the given freezing rate. Of course, such a n iterative freezing approach trades off
between accuracy and freezing rate. To better show this trad e-off, we measured and illustrated accuracy
of CoLA and MRPC datasets across different freezing rates in Fig. 7. The trade-off curve shows our
ILS algorithm can maintain the accuracy at the same level of t he baseline by freezing up to 95% of
layers.
Besides our ILS algorithm, the freezing schedule can be deci ded using random or progressive freezing
approaches. In the random scheduling method, frozen layers are randomly selected at each iteration.
In the progressive approach, on the other hand, early layers are progressively kept frozen whereas later
layers are being updated throughout the ﬁne-tuning process . Among these approaches, our ILS algo-
rithm signiﬁcantly stands out in terms of both accuracy and f reezing rate as shown in Fig. 7. The
0 20 40 60 80 100405060
Freezing Rate (%)Matthew’s CorrelationILS
Random
Progressive
(a) CoLA0 20 40 60 80 10060708090100
Freezing Rate (%)Accuracy (%)ILS
Random
Progressive
(b) MRPC
Figure 7: The trade-off curve between accuracy and freezing rate for three different iterative freezing
approaches (i.e., ILS, random and progressive methods) on ( a) CoLA and (b) MRPC datasets.
8

--- PAGE 9 ---
Table 5: Comparison with state-of-the-arts when ﬁne-tunin g BERT on CoLA dataset.
Model Metric Baseline 4-bit GACT (ICML’22 [4]) DropIT (ICLR ’23 [5]) S LIMFIT
BERTAccuracy (Matthew’s Correlation) 58.9 59.0 57.5 59.6
Freezing Rate (%) NA NA NA 90%
Memory of Activations (GB) 3.2 0.5 2.4 0.6
Total Memory (GB) 6.1 6.0 5.7 4.3
Latency (Seconds) 251 455 367 281
0 20 40 60 80
F reezing Rate (%)0
20
40
60
80
100Layers
(a) CoLA
0 20 40 60 80
F reezing Rate (%)0
20
40
60
80
100Layers
(b) MRPC
Figure 8: The frequency of update occurrence for each layer a s a
heatmap on (a) CoLA and (b) MRPC datasets. The description of lay-
ers corresponding the indices are provided in Appendix J.reason behind its superior per-
formance is that ILS allows
more updates for layers with
large distance values by keep-
ing layers with minimal dis-
tance values frozen for a spe-
ciﬁc number of iterations. On
the other hand, in the ran-
dom approach, the layers are
randomly selected to be up-
dated. Therefore, layers with
large distance values receive
less number of updates in the
random approach compared to
ILS. Of course, the chance of
layers with large distance val-
ues being randomly selected as active layers decreases as th e freezing rate increases, which explains the
accuracy gap between ILS and the random approach with freezi ng rate higher than 70% freezing rate.
In the progressive freezing approach, the early layers rece ive no update during the ﬁne-tuning process,
resulting in a signiﬁcant accuracy degradation for large fr eezing rates.
8.2 Frequency of Update Occurrence
To visualize the frequency of update occurrence for each lay er, we use a heatmap as shown in Fig. 8 for
both CoLA and MRPC datasets where larger counts are associat ed with darker colorings. As shown in
the heatmap, the dense layers inside the MHA module receive m ore updates than other layers for both
datasets. Moreover, the update patterns of these datasets a re similar for small freezing rates whereas
they become more task-speciﬁc for high freezing rates. In fa ct, the ILS algorithm prioritizes the update
of some speciﬁc layers over others for high freezing rates.
9 Comparison With State-of-the-Art Techniques and Limitat ions
Next, we compare S LIMFITwith state-of-the-art compression methods targeting memo ry reduction, i.e.,
GACT [4] and DropIT [5]. Table 5 summarizes the comparison re sults in terms of accuracy, memory
and latency. For fair comparison, we measure their performa nce under the same framework and hyper-
parameters (i.e., the batch size and the number of training e pochs) during ﬁne-tuning of BERT on CoLA.
The experimental results of GACT and DropIT were obtained us ing their ofﬁcial PyTorch libraries. Ac-
cording to the experimental results, GACT shows the lowest m emory amount for activations. However,
in terms of on-device GPU memory usage, S LIMFIToutperforms GACT. In terms of accuracy, all mod-
els show a comparable accuracy on CoLA w.r.t. the baseline. F inally, in terms of speed, S LIMFIT
shows the fastest ﬁne-tuning speed among existing works whi le it still falls short w.r.t. the baseline (see
Appendix I for more details on S LIMFIT’s computing speed). Despite the better accuracy of S LIMFIT
on CoLA, it shows up to 0 .4% degradation in accuracy across different CV/NLP tasks wh ich is another
limitation of S LIMFITbesides its ﬁne-tuning speed w.r.t. the baseline.
10 Conclusion
In this paper, we presented a performance tool called S LIMFITthat reduces the memory usage of activa-
tions and accordingly the overall on-device GPU memory usag e of transformer-based models through
an iterative freezing of layers during ﬁne-tuning. S LIMFITadopts an inter-layer scheduling method to
orchestrate the freezing schedule at each iteration. To bal ance the number of activations across all lay-
ers and to reduce the memory usage of static activations, S LIMFITuses quantization and pruning for a
few speciﬁc layers. We evaluated the performance of S LIMFITacross different NLP and CV tasks. We
showed that S LIMFITsigniﬁcantly reduces the on-device GPU memory usage of the ﬁ ne-tuning process
by up to 3 .1×when using a batch size of 128.
9

--- PAGE 10 ---
References
[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: P re-training of deep bidirectional
transformers for language understanding,” 2018, cite arxi v:1810.04805Comment: 13 pages.
[Online]. Available: http://arxiv.org/abs/1810.04805
[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn , X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houls by, “An image is worth 16x16
words: Transformers for image recognition at scale,” in International Conference on Learning
Representations , 2021. [Online]. Available: https://openreview.net/for um?id=YicbFdNTTy
[3] P. Jain, A. Jain, A. Nrusimha, A. Gholami, P. Abbeel, J. Go nzalez, K. Keutzer,
and I. Stoica, “Checkmate: Breaking the memory wall with opt imal tensor rema-
terialization,” in Proceedings of Machine Learning and Systems , I. Dhillon, D. Pa-
pailiopoulos, and V . Sze, Eds., vol. 2, 2020, pp. 497–511. [O nline]. Available:
https://proceedings.mlsys.org/paper/2020/ﬁle/084b6f bb10729ed4da8c3d3f5a3ae7c9-Paper.pdf
[4] X. Liu, L. Zheng, D. Wang, Y . Cen, W. Chen, X. Han, J. Chen, Z . Liu, J. Tang, J. Gonzalez,
M. Mahoney, and A. Cheung, “GACT: Activation compressed tra ining for generic network
architectures,” in Proceedings of the 39th International Conference on Machin e Learning , ser.
Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,
G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 17–23 Jul 2022, pp . 14 139–14 152. [Online].
Available: https://proceedings.mlr.press/v162/liu22v .html
[5] J. Chen, K. Xu, Y . Wang, Y . Cheng, and A. Yao, “DropIT: Drop ping intermediate tensors
for memory-efﬁcient DNN training,” in The Eleventh International Conference on Learning
Representations , 2023. [Online]. Available: https://openreview.net/for um?id=Kn6i2BZW69w
[6] J. Chen, L. Zheng, Z. Yao, D. Wang, I. Stoica, M. Mahoney, a nd J. Gonzalez, “Actnn: Reducing
training memory footprint via 2-bit activation compressed training,” in International Conference
on Machine Learning . PMLR, 2021, pp. 1803–1813.
[7] A. Chakrabarti and B. Moseley, “Backprop with approxima te activations for memory-efﬁcient
network training,” 2019. [Online]. Available: https://op enreview.net/forum?id=rJgfjjC9Ym
[8] F. Fu, Y . Hu, Y . He, J. Jiang, Y . Shao, C. Zhang, and B. Cui, “ Don’t waste your bits! Squeeze
activations and gradients for deep neural networks via Tiny Script,” in Proceedings of the 37th
International Conference on Machine Learning , ser. Proceedings of Machine Learning Research,
H. D. III and A. Singh, Eds., vol. 119. PMLR, 13–18 Jul 2020, pp . 3304–3314. [Online].
Available: https://proceedings.mlr.press/v119/fu20c. html
[9] R. D. Evans, L. Liu, and T. M. Aamodt, “Jpeg-act: Accelera ting deep learning via transform-
based lossy compression,” in 2020 ACM/IEEE 47th Annual International Symposium on Compu ter
Architecture (ISCA) , 2020, pp. 860–873.
[10] T. Chen, B. Xu, C. Zhang, and C. Guestrin, “Training deep nets with sublinear memory cost,”
arXiv preprint arXiv:1604.06174 , 2016.
[11] O. Beaumont, L. Eyraud-Dubois, and A. Shilova, “Efﬁcie nt combination of rematerial-
ization and ofﬂoading for training dnns,” in Advances in Neural Information Processing
Systems , M. Ranzato, A. Beygelzimer, Y . Dauphin, P. Liang, and J. W. V aughan,
Eds., vol. 34. Curran Associates, Inc., 2021, pp. 23 844–23 8 57. [Online]. Available:
https://proceedings.neurips.cc/paper/2021/ﬁle/c8461 bf13fca8a2b9912ab2eb1668e4b-Paper.pdf
[12] M. Kirisame, S. Lyubomirsky, A. Haan, J. Brennan, M. He, J. Roesch, T. Chen, and Z. Tatlock,
“Dynamic tensor rematerialization,” in International Conference on Learning Representations ,
2021. [Online]. Available: https://openreview.net/foru m?id=Vfs_2RnOD0H
[13] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Else n, D. Garcia, B. Ginsburg,
M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu, “Mixed prec ision training,” 2017, cite
arxiv:1710.03740Comment: Published as a conference paper at ICLR 2018. [Online]. Available:
http://arxiv.org/abs/1710.03740
[14] S. Wu, G. Li, F. Chen, and L. Shi, “Training and inference with integers in deep neural
networks,” in International Conference on Learning Representations , 2018. [Online]. Available:
https://openreview.net/forum?id=HJGXzmspb
10

--- PAGE 11 ---
[15] N. Wang, J. Choi, D. Brand, C.-Y . Chen, and K. Gopalakris hnan, “Training deep
neural networks with 8-bit ﬂoating point numbers,” in Advances in Neural Information
Processing Systems , S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-B ianchi,
and R. Garnett, Eds., vol. 31. Curran Associates, Inc., 2018 . [Online]. Available:
https://proceedings.neurips.cc/paper/2018/ﬁle/335d3 d1cd7ef05ec77714a215134914c-Paper.pdf
[16] R. Banner, I. Hubara, E. Hoffer, and D. Soudry, “Scalabl e methods for 8-bit training of neural
networks,” ser. NIPS’18. Red Hook, NY , USA: Curran Associat es Inc., 2018, p. 5151–5159.
[17] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat, “Q8 bert: Quantized 8bit bert,” in 2019 Fifth
Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing - NeurIPS Edition
(EMC2-NIPS) , 2019, pp. 36–39.
[18] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. W ang, L. Wang, and W. Chen,
“LoRA: Low-rank adaptation of large language models,” in International Conference on Learning
Representations , 2022. [Online]. Available: https://openreview.net/for um?id=nZeVKeeFYf9
[19] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. D hariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan,
R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandli sh, A. Radford,
I. Sutskever, and D. Amodei, “Language models are few-shot l earners,” in Advances in
Neural Information Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell, M. Bal-
can, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020 , pp. 1877–1901. [Online]. Available:
https://proceedings.neurips.cc/paper_ﬁles/paper/202 0/ﬁle/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
[20] S. Swayamdipta, R. Schwartz, N. Lourie, Y . Wang, H. Haji shirzi, N. A. Smith, and Y . Choi,
“Dataset cartography: Mapping and diagnosing datasets wit h training dynamics,” in Proceedings
of the 2020 Conference on Empirical Methods in Natural Langu age Processing (EMNLP) .
Online: Association for Computational Linguistics, Nov. 2 020, pp. 9275–9293. [Online].
Available: https://aclanthology.org/2020.emnlp-main. 746
[21] R. Teehan, M. Clinciu, O. Serikov, E. Szczechla, N. Seel am, S. Mirkin, and A. Gokaslan,
“Emergent structures and training dynamics in large langua ge models,” in Proceedings of
BigScience Episode #5 – Workshop on Challenges & Perspectiv es in Creating Large Language
Models . virtual+Dublin: Association for Computational Linguist ics, May 2022, pp. 146–159.
[Online]. Available: https://aclanthology.org/2022.bi gscience-1.11
[22] Z. Fang, M. Shahbazi, T. Probst, D. P. Paudel, and L. Van G ool, “Training dynamics aware neural
network optimization with stabilization,” in Proceedings of the Asian Conference on Computer
Vision (ACCV) , December 2022, pp. 4276–4292.
[23] Y . Lee, A. S. Chen, F. Tajwar, A. Kumar, H. Yao, P. Liang, a nd C. Finn, “Surgical ﬁne-tuning
improves adaptation to distribution shifts,” arXiv preprint arXiv:2210.11466 , 2022.
[24] V . V . Ramasesh, E. Dyer, and M. Raghu, “Anatomy of catast rophic forgetting: Hidden
representations and task semantics,” in International Conference on Learning Representations ,
2021. [Online]. Available: https://openreview.net/foru m?id=LhY8QdUGSuw
[25] Y . Liu, S. Agarwal, and S. Venkataraman, “Autofreeze: A utomatically freezing model
blocks to accelerate ﬁne-tuning,” CoRR , vol. abs/2102.01386, 2021. [Online]. Available:
https://arxiv.org/abs/2102.01386
[26] S. Li, G. Yuan, Y . Dai, Y . Zhang, Y . Wang, and X. Tang, “Sma rtFRZ: An efﬁcient training frame-
work using attention-based layer freezing,” in The Eleventh International Conference on Learning
Representations , 2023. [Online]. Available: https://openreview.net/for um?id=i9UlAr1T_xl
[27] C. He, S. Li, M. Soltanolkotabi, and S. Avestimehr, “Pip etransformer: Automated elastic
pipelining for distributed training of large-scale models ,” in Proceedings of the 38th International
Conference on Machine Learning , ser. Proceedings of Machine Learning Research, M. Meila
and T. Zhang, Eds., vol. 139. PMLR, 18–24 Jul 2021, pp. 4150–4 159. [Online]. Available:
https://proceedings.mlr.press/v139/he21a.html
[28] A. Merchant, E. Rahimtoroghi, E. Pavlick, and I. Tenney , “What happens to BERT embeddings
during ﬁne-tuning?” in Proceedings of the Third BlackboxNLP Workshop on Analyzing and
Interpreting Neural Networks for NLP . Online: Association for Computational Linguistics, Nov.
2020, pp. 33–44. [Online]. Available: https://aclantholo gy.org/2020.blackboxnlp-1.4
11

--- PAGE 12 ---
[29] S. Shalev-Shwartz and S. Ben-David, Understanding Machine Learning - From Theory to Algo-
rithms. Cambridge University Press, 2014.
[30] A. Warstadt, A. Singh, and S. R. Bowman, “Neural network acceptability judgments,” arXiv
preprint arXiv:1805.12471 , 2018.
[31] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bow man, “GLUE: A multi-task
benchmark and analysis platform for natural language under standing,” in Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreti ng Neural Networks for NLP .
Brussels, Belgium: Association for Computational Linguis tics, Nov. 2018, pp. 353–355. [Online].
Available: https://aclanthology.org/W18-5446
[32] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD : 100,000+ Questions for Machine
Comprehension of Text,” arXiv e-prints , p. arXiv:1606.05250, 2016.
[33] A. Krizhevsky, “Learning multiple layers of features f rom tiny images,” pp. 32–33, 2009.
[Online]. Available: https://www.cs.toronto.edu/~kriz /learning-features-2009-TR.pdf
[34] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fe i, “Imagenet: A large-scale hierarchical
image database,” in 2009 IEEE conference on computer vision and pattern recogni tion. Ieee,
2009, pp. 248–255.
[35] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. D eVito, Z. Lin, A. Desmaison,
L. Antiga, and A. Lerer, “Automatic differentiation in pyto rch,” in CUDA semantics - PyTorch 2.0
documentation , 2017. [Online]. Available: https://pytorch.org/docs/s table/notes/cuda.html
12

--- PAGE 13 ---
Appendix
A Memory Management
The on-device memory of modern GPUs is limited to a few tens of gigabytes depending on their model
(e.g., 32GB NVIDIA V100). If the memory requirement of the tr aining/ﬁne-tuning of neural networks
goes beyond the available memory on GPUs, an out-of-memory e rror will occur. The memory require-
ment of the under-run code on GPUs can be viewed by “nvidia-sm i”. For a training/ﬁne-tuning process,
this memory requirement is determined by the size of the mode l, cached activations, gradients, gradient
moments from the optimizer and CUDA contents after the ﬁrst t raining iterations. It is worth mention-
ing that the memory usage of the training/ﬁne-tuning proces s remains constant after the ﬁrst iteration
if the following iterations are performing the same computa tions (see Fig. 9). If the memory require-
ment of each iteration is different from others, PyTorch rep orts the memory requirement of the iteration
using the maximum memory among other iterations as the total on-device GPU memory usage of the
program in “nvidia-smi” [35]. Therefore, the unused memory of tensors in progressive memory opti-
mization over training iterations will still show as used in “nvidia-smi”. To reduce the overall memory
usage of the training/ﬁne-tuning process given the above ex planations, we need to balance the memory
usage of all iterations from the ﬁrst iterations to the last o ne (see Fig. 9). S LIMFITaims at reducing the
overall memory usage of large transformer-based models dur ing ﬁne-tuning.
0 5 10 156668707274
Training IterationMemory of
Activations (GB)Baseline
0 5 10 151111.51212.513
Training IterationMemory of
Activations (GB)SlimFit
(a) Memory of Activations
0 5 10 15020406080
Training IterationTotal On-device
memory (GB)Baseline
0 5 10 150102030
Training IterationTotal On-device
memory (GB)SlimFit
(b) Total On-device Memory
Figure 9: The total on-device GPU memory and memory of activa tions during different training itera-
tions when ﬁne-tuning ViT on ImageNet with a batch size of 128 with the freezing rate of 95% compared
to the baseline. S LIMFITbalances the memory usage of activations using freezing to r educe the total
on-device memory usage of the ﬁne-tuning process. While the memory usage of activations changes
at each iteration when using S LIMFIT, the changes are relatively small thanks to the load-balanc ing
technique described in Section 4.
B Comparison with Existing Freezing Approaches
Here, we describe the main differences between S LIMFITand other freezing approaches including
SmartFRZ [26], PipeTransformer [27] and AutoFreeze [25]. T he main difference is that the aforemen-
tioned works mainly focus on exploiting freezing to acceler ate the training/ﬁne-tuning process. Concep-
tually, SmartFRZ, PipeTransformer and AutoFreeze progres sively freeze layers as the training process
proceeds. In these methods, the ﬁrst training iteration sta rts without freezing where all layers are up-
dated. In the following iterations, these methods then prog ressively start freezing from the early layers
down to the latest layers in the model in an orderly fashion. F or instance, AutoFreeze performs the ﬁrst
epoch without freezing, the second epoch while freezing the ﬁrst 5 layers, the third epoch while freez-
ing the ﬁrst 8 layers and the fourth epoch while freezing the ﬁ rst 11 layers when ﬁne-tuning BERT. In
this example, the memory and computation requirement of eac h epoch is different from others as each
epoch presents a different degree of freezing. This allows t o exploit the unused computing and memory
13

--- PAGE 14 ---
resources to further accelerate the process by increasing b atch sizes as the memory decreases through-
out the training iterations [25] or increasing data-parall el width through pipelining [27]. Since the ﬁrst
training iteration (or even epoch) of these methods perform s the training process without freezing, their
overall memory requirement reported by “nvidia-smi” is sim ilar to that of training without freezing as
discussed in Appendix A. In other words, the under-use GPU mu st still be able to meet the memory
requirement of training without freezing. For instance, ﬁn e-tuning of ViT with a batch size of 128 on a
single 32GB NVIDIA V100 using such methods results in an out- of-memory error.
SLIMFIT, on the other hand, focuses on reducing the overall memory re quirement of the ﬁne-tuning
process using freezing. As opposed to the aforementioned me thods (i.e., SmartFRZ, PipeTransformer
and AutoFreeze), S LIMFITfreezes layers at every single training iterations from the ﬁrst iteration to
the last one with a ﬁxed freezing rate. With load-balancing u sing quantization, S LIMFITensures that
the memory requirement of every single iteration remains ro ughly the same throughout the ﬁne-tuning
process. This enables S LIMFITperforming memory-intensive ﬁne-tuning processes with la rge batch
sizes on a single 32GB GPU such as ViT on ImageNet with a batch s ize of 128 while this normally
requires three 32GB GPUs.
C Architecture of Transformer-based Models
Fig. 10 shows the overall architecture of transformer-base d models including an initial embedding
layer, followed by repeated blocks of multi-head attention (MHA) and feed-forward network (FFN).
The details of each layer inside the MHA and FFN modules are pr ovided in Table 6.
EmbeddingMulti-head attentionAdd & LayernormFeed-forward networkAdd & Layernorm
MHAFFN
×L
Figure 10: The main architecture of
BERT. Note that ViT has a similar archi-
tecture with LayerNorms located before
the MHA block. Ldenotes the number of
attention layers.Module Type of Layer Description # Activations
MHADense attention.query B∗T∗H
Dense attention.key B∗T∗H
Dense attention.value B∗T∗H
MatMul NA B∗T∗H(2×)
Softmax NA B∗T∗T
MatMul NA B∗T∗H&B∗T∗T
Dense attention.output B∗T∗H
LayerNorm attention.output B∗T∗H
FFNDense intermediate B∗T∗H
GELU NA B∗T∗4∗H
Dense output B∗T∗4∗H
LayerNorm output B∗T∗H
Table 6: The details of layers in MHA and FFN modules
of BERT where B,T,Hdenote the batch size, sequence
length, hidden size, respectively. ViT has the same struc-
ture with different descriptions.
D Theoretical Analysis
D.1 Convergence Analysis
In this section, we provide a convergence analysis for our fr eezing strategy. More precisely, we prove
convergence of stochastic gradient descent (SGD) when cons idering freezing during update iterations.
Given the loss function f, we assume that the parameters are initialized with some val ue and denoted as
the vector w0∈Rd. Given the training example, the parameters are updated by
wt+1=wt−γt∇f(wt), (8)
where wtdenotes the parameter vector at time t,γtis the learning rate, and ∇frepresents the gradient
of the loss function. We assume that the magnitude of the grad ient samples are bounded by a constant
G>0 for all xin the space such that
||∇f(x)||≤G. (9)
Also, we assume that there exists a constant L>0 for any vector u∈Rdwhere we have
|uT∇2f(x)u|≤L||u||2. (10)
14

--- PAGE 15 ---
Given Eq. (9) and Eq. (10), performing Taylor expansion on Eq . (8) similar to [29] results in
E[f(wt+1)]≤E[f(wt)]−γtE[||∇f(wt)||2]+γ2
tG2L
2, (11)
whereEdenotes the expected value.
Now, let us assume that the layer containing the parameter ve ctor is frozen at the training iteration t. In
this case, ∇f(wt)is equal to 0 and consequently wt+1is equal to wt. In this freezing scenario, Eq. (11)
still holds true sinceγ2
tG2L
2is greater than 0.
By rearranging the terms in Eq. (11), summing over Titerations and telescoping the sum, we obtain
T−1
∑
t=0γtE[||∇f(wt)||2]≤T−1
∑
t=0(E[f(wt)]−E[f(wt+1)])+T−1
∑
t=0γ2
tG2L
2, (12)
=f(w0)−f(wT)+G2L
2T−1
∑
t=0γ2
t, (13)
≤f(w0)−f(w∗)+G2L
2T−1
∑
t=0γ2
t, (14)
where w∗indicates an optimal solution. Given the above inequality, we showed that the convergence
proof of SGD remains intact while introducing freezing for s peciﬁc training iterations.
D.2 Backpropagation With a Frozen Layer
Here, we provide a simple example demonstrating how gradien ts are backpropagated to the ﬁrst layer of
a neural network while its middle layer is frozen. To this end , let us perform the backpropagation using
a 3-layer network as an example. Mathematically, the archit ecture of this network can be described as
follows:
y1=xW 1+b1, (15)
y2=y1W2+b2, (16)
y3=y2W3+b3, (17)
where W1,W2,W3,b1,b2andb3are the weights and biases of the network. In this example, x,y1and
y2are inputs to the ﬁrst layer, the second layer and the third la yer, respectively. Now, let us derive the
backpropagation equations with the loss Lusing the chain rule as follows (please note that we obtain
∂L
∂y3by computing the loss where ∂denotes the partial derivative):
∂L
∂W3=∂L
∂y3∂y3
∂W3=∂L
∂y3y2, (18)
∂L
∂b3=∂L
∂y3∂y3
∂b3=∂L
∂y31=∂L
∂y3, (19)
∂L
∂W2=∂L
∂y3∂y3
∂y2∂y2
∂W2=∂L
∂y3WT
3y1, (20)
∂L
∂b2=∂L
∂y3∂y3
∂y2∂y2
∂b2=∂L
∂y3WT
31=∂L
∂y3WT
3, (21)
∂L
∂W1=∂L
∂y3∂y3
∂y2∂y2
∂y1∂y1
∂W1=∂L
∂y3WT
3WT
2x, (22)
∂L
∂b1=∂L
∂y3∂y3
∂y2∂y2
∂y1∂y1
∂b1=∂L
∂y3WT
3WT
21=∂L
∂y3WT
3WT
2. (23)
Given the above equations, to update the network weights (i. e.,W1,W2, and W3), we need to store x,
y1andy2during the forward computations since they are required in E q. (18), Eq. (20) and Eq. (22)
during the back computations.
Now, suppose the middle layer is frozen. In this case, there i s no need to compute Eq. (20) and therefore
there is no need to store y1during the forward computations. Of course, discarding y1does not affect
the backward computations of the ﬁrst layer since Eq. (22) an d Eq. (23) are independent of y1.
15

--- PAGE 16 ---
E Conversion Between 8-bit Integer and 32-bit Floating-poi nt
Algorithm 2 shows the conversion process between 8-bit ﬁxed -point and 32-bit ﬂoating-point formats.
It is worth mentioning that the same procedure can be used for the conversion between 4-bit ﬁxed-point
and 32-bit ﬂoating-point formats. Moreover, the quantizat ion function is used to compress the cached
tensors during the forward propagation only. Of course, bot h the forward and backward computations
are still performed using 32-bit ﬂoating-point computatio ns as shown in Algorithm 4 where the “com-
press” function in this case is the quantization function (i .e., the conversion from 32-bit ﬂoating-point
to 8-bit integer) and the “decompress” function performs th e reverse computations (i.e., the conversion
from 8-bit integer to 32-bit ﬂoating-point).
Algorithm 2 The conversion between 8-bit integer and 32-bit ﬂoating-po int.
Description: number of integer bits as ib, number of fractional bits as f b, input x, output y
32 bits to 8 bits conversion:
y= clamp(round( x∗2f b),−2f b+ib−1, 2f b+ib−1−1)
8 bits to 32 bits conversion:
x=y
2f b
F Pruning Algorithm
The pruning algorithm is performed in a few steps. In the ﬁrst step, the input vector is sorted from largest
to smallest values along with their indices and the size of th e dense vector. We then only keep and cache
the top 10% largest values of the input vector for the backwar d computations as the second step. It
is worth mentioning that pruning beyond 90% results in a sign iﬁcant accuracy degradation. During
backpropagation, we create a zero-valued tensor using the s ize of the dense vector and then replace zero
values with the top 10% largest values using their correspon ding indices. Algorithm 3 shows the pruning
process during the forward computations and the restoring p rocess during the backward computations.
It is worth mentioning that the pruning function is used to co mpress the cached tensors during the
forward propagation only. Of course, both the forward and ba ckward computations are still performed
using 32-bit ﬂoating-point computations as shown in Algori thm 4 where the “compress” function in this
case is the pruning function and the “decompress” function p erforms the restoring computations.
G Details of CV/NLP Tasks, Measurements and Hyper-paramete r Settings
For language understanding tasks and CV tasks, we used BERT- base-cased and ViT-base throughout
this paper, respectively. The BERT-base and ViT-base pre-t rained on ImageNet-21k were conﬁgured
according to [1] and [2], respectively. We used AdamW ( β1=0.9,β2=0.999 and L2 weight decay
Algorithm 3 The description of the pruning process during the forward co mputations and the restoring
process during the backward computations.
Description: x : input vector, xs: sorted input vector, xidx: indices of the sorted input vector, ys: top
10% largest values, yidx: indices of top 10% largest values, y: output vector, sort: sorting function,
zeros: function to create zero-valued tensor, and scatter: function to replace zero values with the
tensor values from ysaccording to the indices.
Pruning process:
xs,xidx= sort( x)
ys,yidx=xs[0 :int(x.numel()∗0.1)],xidx[0 :int(x.numel()∗0.1)]
Restoring process:
y= zeros( x.numel())
y= scatter( y,ys,yidx)
16

--- PAGE 17 ---
of 0.01) as the optimizer and linear decay of the learning rat e with warmup ranging from 0 to 0.1
for both models. We evaluate BERT-base on several downstrea m tasks from the GLUE benchmark
and SQuAD 2.0. We use Spearman correlation for STS-B, Matthe ws correlation for CoLA, accuracy
for RTE, MRPC, SST-2 QQP, QNLI and MNLI m(matched), and F1 score for SQuAD 2.0. For the
downstream tasks from the GLUE benchmark, we used the sequen ce length of 128 whereas we adopted
the sequence length of 384 for the question answering task on SQuAD 2.0. For CIFAR-10, CIFAR-100
and ImageNet, we use top-1 accuracy as our evaluation metric . For the image classiﬁcation tasks, we
used the patch size of 16 with the resolution of 224 for CIFAR- 10/CIFAR-100 and the resolution of
384 for ImageNet. Depending on the task, the learning rate va ries from 4e-5 to 1.8e-4. For all the
experiments in this paper, we used 3 epochs for ﬁne-tuning. T he hyper-parameter settings of each task
are summarized in Table 7. It is worth mentioning that ViT mod els can also be ﬁne-tuned using SGD.
However, ﬁne-tuning ViT models using SGD requires more epoc hs w.r.t. AdamW to obtain a similar
accuracy.
In this paper, we measured our experimental results directl y from 32GB NVIDIA V100 GPU(s) without
any memory swapping between CPU and GPU(s). The total on-dev ice GPU memory usage of the ﬁne-
tuning process is measured using “nvidia-smi”. We measured the wall-clock time (i.e., latency) of the
ﬁne-tuning process using the CUDA’s event API in PyTorch (i. e., “torch.cuda.Event”). The memory
footprint of activations on the GPU(s) was measured using th e PyTorch’s memory management API
(i.e., “torch.cuda.memory_allocated”).
Table 7: The hyper-parameter settings of each NLP/CV task.
Dataset Model Optimizer Learning Rate Warmup Evaluation Me tric
MNLI m bert-base-cased AdamW 4e-5 0 percentage accuracy
QQP bert-base-cased AdamW 5e-5 0 percentage accuracy
QNLI bert-base-cased AdamW 5e-5 0 percentage accuracy
SST-2 bert-base-cased AdamW 8e-5 0 percentage accuracy
CoLA bert-base-cased AdamW 8e-5 0.1 Matthew’s correlation
STS-B bert-base-cased AdamW 8e-5 0 Spearman correlation
MRPC bert-base-cased AdamW 1.25e-4 0 percentage accuracy
RTE bert-base-cased AdamW 1.2e-4 0 percentage accuracy
SQuAD 2.0 bert-base-uncased AdamW 1.8e-4 0.1 F1 score
CIFAR-10 vit-base-patch16-224-in21k AdamW 7.5e-5 0 perce ntage accuracy
CIFAR-100 vit-base-patch16-224-in21k AdamW 5.5e-5 0 perc entage accuracy
ImageNet vit-base-patch16-384 AdamW 5e-5 0 percentage acc uracy
H Impact of Quantization and Pruning
In this work, we used quantization and pruning for a few speci ﬁc layers to balance the number of activa-
tions across all layers and to reduce the memory footprint of static activations. We used 8-bit quantiza-
tion for the activations of the imbalanced linear layer and M atMul. We also quantized the activations of
GELU using 4 bits. The pruning of LayerNorm was performed whe n this layer is kept frozen. It is worth
mentioning that both quantization and pruning have no impac t on the forward computations. They are
only used to compress activations for caching. To show the im pact of such compression methods, we
report the accuracy evaluation of BERT on CoLA and MRPC datas ets with and without quantization or
pruning in Table 8. The experimental results show no notable performance loss due to the compression
techniques.
Table 8: The impact of quantization and pruning on the accura cy evaluation.
Dataset BaselineQuantization of Pruning of All
Linear MatMul GELU LayerNorm together
CoLA 58.9 58.9 60.6 60.0 59.7 59.7
MRPC 86.4 86.4 86.3 86.3 86.3 86.3
I Discussion on Wall-Clock Time
Compared to training without freezing, S LIMFITintroduces extra computations and also skips weight
gradient computations for the frozen layers at the same time . The main source of computational over-
head in S LIMFITis quantization and pruning of activations. The quantizati on overhead is due to the
17

--- PAGE 18 ---
conversion between different precision levels (i.e., betw een 8 bits and 32-bit ﬂoating-point format) as
discussed in Appendix E. Pruning also requires sorting of va lues to keep their top 10% largest values,
which causes an additional computational overhead. Comput ing the weight distance metric is another
source of computational overhead.
Algorithm 4 The description of skipping weight gradient computations w hen the layer is frozen. In this
example, we assume the activations of the frozen layer requi re compression (e.g., an imbalanced linear
layer or LayerNorm). Activations are denoted as “input” and are cached using either quantization or
pruning depending on the type of the layer as a compression me thod. The compression and decompres-
sion functions are denoted as “compress” and “decompress”. Since weights are deﬁned as “Parameter”
in PyTorch, caching weights does not introduce any extra mem ory.
class ILSFunction(torch.autograd.Function):
@staticmethod
defforward(ctx, input: torch.Tensor, weight: torch.nn.Para meter, requires_grad):
# Compute forward computations to obtain out
ifrequires_grad:
ctx.save_for_backward(compress(input), weight)
else:
ctx.save_for_backward(weight)
ctx.requires_grad = requires_grad
return out
@staticmethod
defbackward(ctx, grad_output: torch.Tensor):
ifctx.requires_grad:
input, weight = decompress(ctx.saved_tensors[0]), ctx.s aved_tensors[1]
# Compute backward computations to obtain grad_input and gr ad_weight
else:
weight = ctx.saved_tensors[1]
grad_weight = None
# Compute backward computations to obtain grad_input
return grad_input, grad_weight, None
On the other hand, S LIMFITskips the weight gradients computations of frozen layers us ing PyTorch
“requires_grad” as shown in Algorithm 4. When an activate la yer is frozen, there is no need to compute
its weight gradients as discussed in Appendix D.2, which red uces the wall-clock time. The amount of
speedup due to the skipped computations highly depends on th e hyper-parameters of the networks such
as freezing rate. Therefore, the wall-clock time of each net work varies from one to another depending on
the hyper-parameters. For instance, Fig. 11 shows the wall- clock time of ﬁne-tuning ViT on ImageNet
using a batch size of 32 across different freezing rates. Acc ording to the experimental results, the
computational overhead of S LIMFITis dominant for small freezing rates. However, as the freezi ng rate
increases, the speedup of the skipped gradient computation s overcomes the computational overhead of
SLIMFITwhere S LIMFITwith the freezing rate of 95% results in a similar wall-clock time as of the
0 20 40 60 80 10018202224
Freezing Rate (%)Hours per epochSlimFit
Figure 11: Wall-clock time of ﬁne-tuning ViT on ImageNet wit h a batch size of 32 across different
freezing rates.
18

--- PAGE 19 ---
baseline. It is worth mentioning that the baseline is the poi nt at the freezing rate of 0 where no freezing
was used during the ﬁne-tuning process.
J Description of Layers in the Heatmap
The description of layers associated to the indices in Fig. 8 is provided in Algorithm 5. It is worth men-
tioning that the layers denoted by “bert.encoder.layer[i] .attention” belong to the MHA module whereas
the remaining layers inside the loop belong to the FFN module .
Algorithm 5 The description of layers associated to the indices in Fig. 8 .
bert.embeddings.word_embeddings.weight
bert.embeddings.position_embeddings.weight
bert.embeddings.token_type_embeddings.weight
bert.embeddings.LayerNorm.weight
fori=0 to 11: do
bert.encoder.layer[i].attention.self.query.weight
bert.encoder.layer[i].attention.self.key.weight
bert.encoder.layer[i].attention.self.value.weight
bert.encoder.layer[i].attention.output.dense.weight
bert.encoder.layer[i].attention.output.LayerNorm.we ight
bert.encoder.layer[i].intermediate.dense.weight
bert.encoder.layer[i].output.dense.weight
bert.encoder.layer[i].output.LayerNorm.weight
end for
bert.pooler.dense.weight
classiﬁer.weight
19
