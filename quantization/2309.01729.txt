# 2309.01729.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2309.01729.pdf
# File size: 1311966 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Softmax Bias Correction for Quantized Generative Models
Nilesh Prasad Pandey, Marios Fournarakis, Chirag Patel, Markus Nagel
Qualcomm AI Research*
{nileshpr, mfournar, cpatel, markusn }@qti.qualcomm.com
Abstract
Post-training quantization (PTQ) is the go-to com-
pression technique for large generative models, such as
stable diffusion or large language models. PTQ meth-
ods commonly keep the softmax activation in higher
precision as it has been shown to be very sensitive to
quantization noise. However, this can lead to a signif-
icant runtime and power overhead during inference on
resource-constraint edge devices. In this work, we in-
vestigate the source of the softmax sensitivity to quan-
tization and show that the quantization operation leads
to a large bias in the softmax output, causing accuracy
degradation. To overcome this issue, we propose an of-
fline bias correction technique that improves the quan-
tizability of softmax without additional compute during
deployment, as it can be readily absorbed into the quan-
tization parameters. We demonstrate the effectiveness
of our method on stable diffusion v1.5 and 125M-size
OPT language model, achieving significant accuracy
improvement for 8-bit quantized softmax.
1. Introduction
The increasing prevalence of large generative neural
networks, such as stable diffusion [4, 13, 24], ChatGPT,
and OPT [33], has revolutionized the fields of computer
vision and natural language processing. These mod-
els exhibit exceptional capabilities in generating real-
istic images and human-like text. However, deploying
them on edge devices is challenging due to their size and
computational demands. To address this issue, quan-
tization has emerged as the most promising technique
to optimize model deployment on resource-constrained
devices, with a plethora of work emerging for both vi-
sion [14, 3, 18] and language models [19, 8, 6].
Post-training quantization (PTQ) is the go-to method
*Qualcomm AI Research is an initiative of Qualcomm Technolo-
gies, Incfor quantizing such models because accessing original
training data and pipelines can be difficult, and training
them requires vast computing resources. However, acti-
vation quantization remains challenging because certain
layers, such as the softmax in transformers, are particu-
larly sensitive to quantization. This issue is even more
pronounced in diffusion models due to the iterative na-
ture of the denoising process leading to error accumu-
lation. For this reason, it is common practice to keep
the softmax unquantized or in higher precision leading
to significant latency overhead, especially in networks
with larger sequence lengths [29].
In this work, we systematically investigate the source
of the softmax sensitivity to quantization and show that
quantization operation leads to a large bias degrading
accuracy. We introduce a hardware-friendly bias cor-
rection that acts as an offset at the softmax output,
which can be absorbed into the quantization parame-
ters. Despite its simplicity, our method significantly im-
proves the SQNR and perplexity scores for diffusion [4]
and OPT [33] language models, respectively, with 8-bit
quantized softmax.
2. Background
2.1. Related Work
Quantization is one of the most effective methods
available for reducing latency and power consumption
in neural network inference. This is achieved not only
thanks to reduced model size but because fixed-point op-
erations are more efficient than their floating-point coun-
terparts. In this work, we focus on post-training quan-
tization (PTQ), which takes a pre-trained FP32 network
and converts it directly into a fixed-point network with-
out the need for the original training pipeline [20, 21,
15]. These methods require either no data or only a
small calibration dataset and are easier to use compared
to quantization-aware training (QAT) [5, 11, 10, 23]. For
more details on neural network quantization, we refer
the reader to [9, 22].
1arXiv:2309.01729v1  [cs.LG]  4 Sep 2023

--- PAGE 2 ---
As the success of language models has increased con-
currently with their size, a lot of recent work has focused
on quantizing these models [27, 32, 27, 7]. While few
methods have emerged to address the issue of outliers in
the output of transformers [1, 2, 30], our work is comple-
mentary as we focus on quantizing the attention weights.
Similarly, while recent work on the quantizing diffu-
sion models [17, 12, 26] have discussed various prob-
lems and methods to overcome quantization challenges,
most of these methods keep sensitive activations, such
as softmax, in higher-precision. However, softmax can
be the biggest latency bottleneck due to its inefficient
execution in hardware [29]. Our work is orthogonal to
existing methods as we focus on improving the quantiz-
ibility of softmax layers to lower bits.
2.2. Motivation
Softmax accounts for a significant fraction of the to-
tal runtime of transformers accounting for up to 40%
for sequence lengths larger than 2048 [29]. As a re-
sult, keeping softmax in low precision can accelerate in-
ference by reducing the size of the look-up tables re-
quired to estimate exponential functions. As modern
diffusion model, e.g. stable diffusion v1.51reach se-
quence lengths of 4096, low-bit softmax is imperative if
we want to achieve competitive on-device performance.
However, when quantizing the softmax in stable dif-
fusion to 8 bits, we observe a considerable deviation
in the generated images compared to the floating-point
model (see columns FP32 and W8A16-SM8 in figure 2).
On the contrary, when keeping softmax at 16 bits (col-
umn W8A16 in figure 2), the generated image matches
that of the floating-point model very closely.
To confirm our hypothesis that the softmax layers in
the diffusion process are particularly sensitive to quan-
tization, we perform the following sensitivity analysis:
we quantize individual attention tensors to 8 bits in the
denoising U-Net while keeping the rest of the network in
FP32 and measure the signal-to-quantization noise ratio
(SQNR) between the quantized and full-precision at the
end of the denoising process. We use a calibration set X
of 400 input latents sampled uniformly across all time
steps and report the mean SQNR in dB in table 1. We
calculate the SQNR using the following formula:
SQNRdB= 10 log Ex∥ϕ(x)∥2
2
∥q(ϕ(x))−ϕ(x)∥2
2
,(1)
where x∈ X,ϕ(·)is the output of the denoising U-Net,
and∥ · ∥ 2is the Forbenious norm.
1https://github.com/runwayml/
stable-diffusionActivation in 8 bits SQNR( ↑)
Query (Q) 32.36
Key (K) 29.77
Value (V) 26.58
Attention score (softmax input) 28.09
Softmax output 3.24
Table 1: Quantization sensitivity analysis for attention layers
in the denoising U-Net of stable diffusion. We quantize each
activation to 8 bits while keeping the rest of the network un-
quantized, and report mean SQNR( ↑): the higher, the better.
Figure 1: x-axis: sum of 8-bit quantized softmax vectors be-
fore bias correction while keeping rest of the network in FP32
Ex[q(softmax (x)]); y-axis: SQNR between full-precision
and quantized UNet outputs after final diffusion step.
We can see from table 1 that quantizing the softmax
output leads to an 8-fold degradation in SQNR com-
pared to the second most sensitive activation, the value
tensor (V).
2.3. Quantized softmax is biased
Why is the softmax output so sensitive to quantiza-
tion? Having a closer look at the values of the quan-
tized softmax, we found that up to 99% of the values are
rounded to zero. As all these values are rounded down,
the resulting quantization error is biased , and the soft-
max probabilities are not correctly normalized anymore,
which can degrade the model’s performance. In the scat-
ter plot of figure 1, we see that many quantized softmax
outputs do not add up to 1.0. In fact, the expected sum
of the softmax output over the calibration set can be as
low as 0.3. We also observe a high correlation between
quantization bias and degradation of the denoising pro-
cess: the larger gap from the expected softmax output
(1.0), the lower the SQNR at the U-Net output.
3. Quantized activation bias correction
In the previous section, we experimentally estab-
lished that quantizing the softmax in the transformer can
2

--- PAGE 3 ---
lead to highly-biased outputs causing significant distor-
tion in stable diffusion’s output. In this section, we out-
line a simple but effective method for correcting this bias
and improving performance.
We define quantization bias as the systematic discrep-
ancy between quantized and unquantized activation vec-
tory:
β(y;T) =E[Ty]−E[q(Ty)], (2)
where Tis the transformation function acting on y, and
q(·)is the quantization function. The transformation T
could be the identity or a simple linear transformation,
such as a reduction along a certain axis. We can now
correct for this bias by adding back to the quantized ac-
tivation yq=q(Ty), such that
E[Tyq+β(y;T)] =E[Ty]. (3)
In practice, we calculate an empirical estimate of the
bias,bβ, using the available calibration data.
3.1. Softmax bias correction
In the case of softmax activations, we know in ad-
vance that its output is normalized and should thus sum
to 1.0. Using the notation of equation (2), the transfor-
mation Tis an inner product with the vector of ones
along the normalization dimension: E
1⊤y
= 1. In
transformers, the input to the softmax layer is typically
three-dimensional X∈Rnheads×nseq×nseqand the soft-
max is applied across the last dimension. Depending
on the capabilities of the hardware available, we could
have a per-tensor orper attention-head correction fac-
tor, which would require reducing the output of the soft-
max output accordingly. For example, the per-tensor
correction factor is calculated by:
β=1
nseq−ExPnheads
iPnseq
jPnseq
kYi,j,k
nheadsn2seq,(4)
where Y=q(softmax (X))andβis added elemen-
twise to the whole quantized output Y. In later sec-
tions (cf. Sec 4.1), we perform an ablation study for
bias correction granularities.
3.2. Absorbing bias correction
An important benefit of our bias correction method
is that it can be easily absorbed into the offset of asym-
metric quantization. For a bbitwidth uniform quantizer
with scale sand zero-point z, asymmetric quantization
is defined as:
yq=q(y;s, z, b )
=s·h
clampjy
sm
+z,0,2b−1
−zi
=s·yint−c,(5)Type of correction SD (SQNR ↑)
None 3.17
Per-tensor 5.77
Per attention-head 6.05
Time-step aware, per-tensor 5.93
Time-step aware, per attention-head 6.06
Table 2: Granularity ablation study for bias correction: we
quantize the softmax to 8 bits, keeping the rest of the network
in FP32. We report SQNR for stable diffusion (SD) ( ↑): the
higher, the better
where c=s·zis a floating-point offset of the quan-
tization grid, as the scale sis typically a floating-point
number [15]. For bias correction, we only have to ab-
sorb the correction factor into the offset, c′=s·z−β,
while keeping everything else the same. Given that
activations are most commonly asymmetrically quan-
tized [1, 30, 22, 2], our bias correction leads to no ad-
ditional compute.
4. Experiments
In this section, we demonstrate the advantages of
softmax bias correction. We perform experiments on
stable diffusion v1.52and extend our analysis to a
transformer-based language model. We experiment with
the 125M-sized variant of OPT [33] pre-trained using
the causal language modeling (CLM) objective. We use
a validation pipeline for HuggingFace libraries [31, 16]
and evaluate on Wikipedia validation set2. We report
SQNR between the full-precision and quantized U-Net
output for stable diffusion (the higher, the better), and
CLM perplexity for OPT (the lower, the better).
We use PyTorch v1.11 and the AI Model Efficiency
Toolkit (AIMET)3[28] to quantize the models to desired
bitwidths. We implement per-tensor symmetric quanti-
zation for weights and asymmetric quantization for acti-
vations.
4.1. Granularity of bias correction
As mentioned in section 3.1, depending on the tar-
get hardware, we can apply bias correction at different
granularities of attention tensor, e.g.per-attention head
orper-tensor . Due to the iterative nature of the denois-
ing process in diffusion models, activation distributions
in the U-Net are time-step dependent, hence motivating
2https://github.com/huggingface/diffusers
2Specifically, we use the English subset of Wiki-40b, https://
huggingface.co/datasets/wiki40b , that contains cleaned-
up text of English Wikipedia and training/validation splits.
3AIMET is a product of Qualcomm Innovation Center, Inc., avail-
able on GitHub at https://github.com/quic/aimet
3

--- PAGE 4 ---
(a)First shot of the Milky Way
 (b)Tuscany, Italy, Country Side, Sunrise, Foggy, Dawn, Landscape,
Aerial view, Meadow, 5K
(c)skeleton in heaven dressed as renaissance artist painting a portrait of
a model dressed as a Saint, painting, renaissance art, detailed,oil painting
(d)highly detailed oil painting of a western dragon emerging from a hot
spring, fantasy, featured on art station
Figure 2: Visual comparison between FP32, W8A16, W8A16 with softmax quantized to 8 bits without and with per attention head
based bias correction generated using 20 diffusion steps on test prompts from LAION Aesthetics dataset [25].
us to use the time-step as an additional axis of granu-
larity to perform time-step-aware bias correction. We
compare the results from the different schemes in table
2.
We observe that the per-attention head correction
scheme performs on par or better than all other schemes,
making it a favorable choice for on-device deployment
due to its minimal computational overhead compared to
its time-aware counterpart.
4.2. Main Results
We extend our analysis to include the 125M-size
OPT language model, and we report results using per-
attention head bias correction in table 3. We quantize
softmax to 8 bits (SM8) and keep the rest of the net-
work at either full-precision (FP32) or 8-bit weights and
16-bit activations (W8A16). With bias correction, we
achieve over 2.7dB improvement for stable diffusion and
roughly 4.8 improvement in perplexity for OPT in both
quantization settings. In the case of diffusion, we also
demonstrate the improvement visually in figure 2, by
showing the generated images of the quantized diffu-
sion with and without bias correction. As we can see,
the generated image with bias correction very closely
resembles the full precision output.
5. Conclusions
In this work, we investigated a common prevail-
ing issue of softmax sensitivity to quantization in the
case of generative models. To understand the source
of the softmax sensitivity to quantization, we analyzedConfiguration SD (SQNR ↑) OPT (ppl ↓)
FP32 baseline - 27.73
FP32-SM8 3.17 34.98
FP32-SM8 + bias correction 6.05 30.19
W8A16 baseline 9.66 27.77
W8A16-SM8 3.05 35.11
W8A16-SM8 + bias correction 5.76 30.24
Table 3: Per-attention bias correction for stable diffusion (SD)
and 125M OPT model for different quantization configurations
(W8A16 & FP32), while keeping the softmax to 8 bits (SM8).
We report SQNR for stable diffusion (SD) and CLM perplexity
for OPT. ( ↑): the higher, the better; ( ↓): the lower, the better.
the softmax distributions and showed that the quanti-
zation operation led to a significant bias in the soft-
max output degrading the performance of generative
models when represented in lower precision. To over-
come this issue, we proposed a simple yet effective
hardware-friendly offset correction to improve the quan-
tizability of softmax layers to lower bits, which is key
to achieving competitive on-device performance, espe-
cially for transformer-based networks with longer se-
quence lengths. We demonstrated the effectiveness of
our method on stable diffusion v1.5 and 125M-size OPT
language model, achieving over 2.7dB improvement for
stable diffusion and roughly 4.8 improvement in per-
plexity for OPT respectively for the 8-bit weights and
16-bit activations (W8A16) quantization setting.
4

--- PAGE 5 ---
References
[1] Yelysei Bondarenko, Markus Nagel, and Tijmen
Blankevoort. Understanding and overcoming the
challenges of efficient transformer quantization. In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , pages
7947–7969, Online and Punta Cana, Dominican
Republic, November 2021. Association for Com-
putational Linguistics. 2, 3
[2] Yelysei Bondarenko, Markus Nagel, and Tijmen
Blankevoort. Quantizable transformers: Remov-
ing outliers by helping attention heads do nothing,
2023. 2, 3
[3] Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Ji-
uqiang Tang, Chuo-Ling Chang, Andrei Kulik, and
Matthias Grundmann. Speed is all you need: On-
device acceleration of large diffusion models via
gpu-aware optimizations. In Proceedings of the
IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 4650–4654, 2023. 1
[4] Prafulla Dhariwal and Alexander Nichol. Diffu-
sion models beat gans on image synthesis. Ad-
vances in neural information processing systems ,
34:8780–8794, 2021. 1
[5] Steven K Esser, Jeffrey L McKinstry, Deepika
Bablani, Rathinakumar Appuswamy, and Dhar-
mendra S Modha. Learned step size quantization.
arXiv preprint arXiv:1902.08153 , 2019. 1
[6] Elias Frantar and Dan Alistarh. Optimal brain
compression: A framework for accurate post-
training quantization and pruning. In Alice H.
Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural In-
formation Processing Systems , 2022. 1
[7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler,
and Dan Alistarh. Gptq: Accurate post-training
quantization for generative pre-trained transform-
ers, 2023. 2
[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler,
and Dan Alistarh. OPTQ: Accurate quantiza-
tion for generative pre-trained transformers. In
The Eleventh International Conference on Learn-
ing Representations , 2023. 1
[9] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei
Yao, Michael W. Mahoney, and Kurt Keutzer. A
survey of quantization methods for efficient neuralnetwork inference. CoRR , abs/2103.13630, 2021.
1
[10] Ruihao Gong, Xianglong Liu, Shenghu Jiang,
Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei
Yu, and Junjie Yan. Differentiable soft quanti-
zation: Bridging full-precision and low-bit neural
networks. International Conference on Computer
Vision (ICCV) , 2019. 1
[11] Tiantian Han, Dong Li, Ji Liu, Lu Tian, and
Yi Shan. Improving low-precision network quanti-
zation via bin regularization. In International Con-
ference on Computer Vision (ICCV) , 2021. 1
[12] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong
Zhou, and Bohan Zhuang. Ptqd: Accurate post-
training quantization for diffusion models. arXiv
preprint arXiv:2305.10657 , 2023. 2
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel.
Denoising diffusion probabilistic models. Ad-
vances in neural information processing systems ,
33:6840–6851, 2020. 1
[14] Jilei Hou and Ziad Asghar. World’s first on-device
demonstration of stable diffusion on an android
phone. 2023. 1
[15] Raghuraman Krishnamoorthi. Quantizing deep
convolutional networks for efficient inference: A
whitepaper. arXiv preprint arXiv:1806.08342 ,
2018. 1, 3
[16] Quentin Lhoest, Albert Villanova del Moral,
Yacine Jernite, Abhishek Thakur, Patrick von
Platen, Suraj Patil, Julien Chaumond, Mariama
Drame, Julien Plu, Lewis Tunstall, Joe Davison,
Mario ˇSaˇsko, Gunjan Chhablani, Bhavitvya Ma-
lik, Simon Brandeis, Teven Le Scao, Victor Sanh,
Canwen Xu, Nicolas Patry, Angelina McMillan-
Major, Philipp Schmid, Sylvain Gugger, Cl ´ement
Delangue, Th ´eo Matussi `ere, Lysandre Debut, Stas
Bekman, Pierric Cistac, Thibault Goehringer, Vic-
tor Mustar, Franc ¸ois Lagunas, Alexander M. Rush,
and Thomas Wolf. Datasets: A community library
for natural language processing, 2021. 3
[17] Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang,
Zhen Dong, Daniel Kang, Shanghang Zhang, and
Kurt Keutzer. Q-diffusion: Quantizing diffusion
models. arXiv preprint arXiv:2302.04304 , 2023. 2
[18] Yanyu Li, Huan Wang, Qing Jin, Ju Hu,
Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey
5

--- PAGE 6 ---
Tulyakov, and Jian Ren. Snapfusion: Text-to-
image diffusion model on mobile devices within
two seconds. arXiv preprint arXiv:2306.00980 ,
2023. 1
[19] Yijiang Liu, Huanrui Yang, Zhen Dong,
Kurt Keutzer, Li Du, and Shanghang Zhang.
Noisyquant: Noisy bias-enhanced post-training
activation quantization for vision transformers,
2023. 1
[20] Markus Nagel, Rana Ali Amjad, Mart Van Baalen,
Christos Louizos, and Tijmen Blankevoort. Up or
down? adaptive rounding for post-training quan-
tization. In International Conference on Machine
Learning , pages 7197–7206. PMLR, 2020. 1
[21] Markus Nagel, Mart van Baalen, Tijmen
Blankevoort, and Max Welling. Data-free
quantization through weight equalization and
bias correction. In Proceedings of the IEEE/CVF
International Conference on Computer Vision ,
pages 1325–1334, 2019. 1
[22] Markus Nagel, Marios Fournarakis, Rana Ali
Amjad, Yelysei Bondarenko, Mart van Baalen,
and Tijmen Blankevoort. A white paper on
neural network quantization. arXiv preprint
arXiv:2106.08295 , 2021. 1, 3
[23] Markus Nagel, Marios Fournarakis, Yelysei Bon-
darenko, and Tijmen Blankevoort. Overcoming os-
cillations in quantization-aware training. In Kama-
lika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvari, Gang Niu, and Sivan Sabato, editors,
Proceedings of the 39th International Conference
on Machine Learning , volume 162 of Proceed-
ings of Machine Learning Research , pages 16318–
16330. PMLR, 17–23 Jul 2022. 1
[24] Robin Rombach, Andreas Blattmann, Dominik
Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-
resolution image synthesis with latent diffusion
models. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition ,
pages 10684–10695, 2022. 1
[25] Christoph Schuhmann, Romain Beaumont,
Richard Vencu, Cade Gordon, Ross Wightman,
Mehdi Cherti, Theo Coombes, Aarush Katta,
Clayton Mullis, Mitchell Wortsman, et al. Laion-
5b: An open large-scale dataset for training
next generation image-text models. Advances
in Neural Information Processing Systems ,
35:25278–25294, 2022. 4[26] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe
Wu, and Yan Yan. Post-training quantization
on diffusion models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1972–1981, 2023. 2
[27] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma,
Zhewei Yao, Amir Gholami, Michael W Mahoney,
and Kurt Keutzer. Q-bert: Hessian based ultra
low precision quantization of bert. In Proceedings
of the AAAI Conference on Artificial Intelligence ,
volume 34, pages 8815–8821, 2020. 2
[28] Sangeetha Siddegowda, Marios Fournarakis,
Markus Nagel, Tijmen Blankevoort, Chirag Patel,
and Abhijit Khobare. Neural network quantization
with ai model efficiency toolkit (aimet). arXiv
preprint arXiv:2201.08442 , 2022. 3
[29] Jacob R Stevens, Rangharajan Venkatesan, Steve
Dai, Brucek Khailany, and Anand Raghunathan.
Softermax: Hardware/software co-design of an ef-
ficient softmax for transformers. In 2021 58th
ACM/IEEE Design Automation Conference (DAC) ,
pages 469–474. IEEE, 2021. 1, 2
[30] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiang-
guo Zhang, Ruihao Gong, Jinyang Guo, and Xian-
glong Liu. Outlier suppression+: Accurate quan-
tization of large language models by equivalent
and optimal shifting and scaling. arXiv preprint
arXiv:2304.09145 , 2023. 2, 3
[31] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan
Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu,
Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexan-
der Rush. Transformers: State-of-the-art natu-
ral language processing. In Proceedings of the
2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations ,
pages 38–45, Online, October 2020. Association
for Computational Linguistics. 3
[32] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and
Moshe Wasserblat. Q8bert: Quantized 8bit bert.
arXiv preprint arXiv:1910.06188 , 2019. 2
[33] Susan Zhang, Stephen Roller, Naman Goyal,
Mikel Artetxe, Moya Chen, Shuohui Chen,
6

--- PAGE 7 ---
Christopher Dewan, Mona Diab, Xian Li, Xi Vic-
toria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer,
Kurt Shuster, Daniel Simig, Punit Singh Koura,
Anjali Sridhar, Tianlu Wang, and Luke Zettle-
moyer. Opt: Open pre-trained transformer lan-
guage models, 2022. 1, 3
7
