# 2310.02410.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2310.02410.pdf
# Kích thước tệp: 425701 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Hỗn hợp các Chuyên gia Lượng tử hóa (MoQE):
Hiệu ứng Bổ sung của
Lượng tử hóa Bit thấp và Tính mạnh mẽ
Young Jin Kim∗
Microsoft
youki@microsoft.comRaffy Fahim∗
Microsoft
raffybekheit@microsoft.com
Hany Hassan Awadalla
Microsoft
hanyh@microsoft.com
Tóm tắt
Các mô hình Hỗn hợp Chuyên gia (MoE) lớn có thể đạt được chất lượng tiên tiến trên
các tác vụ ngôn ngữ khác nhau, bao gồm cả tác vụ dịch máy, nhờ vào khả năng mở rộng
mô hình hiệu quả với tính song song của chuyên gia (Fedus et al., 2021). Tuy nhiên,
nó đã mang lại một vấn đề cơ bản về tiêu thụ bộ nhớ lớn hơn và tăng cổ thắt băng thông
bộ nhớ tại thời điểm triển khai. Trong bài báo này, chúng tôi đề xuất Hỗn hợp các Chuyên gia
Lượng tử hóa (MoQE) là một phương pháp lượng tử hóa chỉ trọng số đơn giản áp dụng
lượng tử hóa bit cực thấp xuống đến 2-bit chỉ cho trọng số chuyên gia để giảm thiểu
các vấn đề tăng bộ nhớ và độ trễ của các mô hình MoE. Chúng tôi chỉ ra rằng lượng tử hóa
bit thấp cùng với kiến trúc MoE mang lại hiệu suất mô hình đáng tin cậy trong khi
giảm đáng kể kích thước bộ nhớ ngay cả khi không có bất kỳ huấn luyện bổ sung nào
trong hầu hết các trường hợp. Đặc biệt, các lớp chuyên gia trong các mô hình MoE
mạnh mẽ hơn nhiều đối với lượng tử hóa so với các lớp mạng lan truyền thuận (FFN)
thông thường. Trong phân tích toàn diện của chúng tôi, chúng tôi chỉ ra rằng các mô hình MoE
với trọng số chuyên gia 2-bit có thể mang lại hiệu suất mô hình tốt hơn so với mô hình
dày đặc được huấn luyện trên cùng tập dữ liệu. Kết quả của lượng tử hóa bit thấp,
chúng tôi chỉ ra kích thước mô hình có thể được giảm 79.6% so với mô hình MoE
điểm nổi nửa độ chính xác (fp16) gốc. Kết hợp với việc triển khai runtime GPU được
tối ưu hóa, nó cũng đạt được tăng tốc 1.24X trên GPU A100.

1 Giới thiệu
Kiến trúc Hỗn hợp-các-Chuyên gia (MoE) tăng hiệu quả số lượng tham số mô hình,
trong khi duy trì sự tăng dưới tuyến tính trong các yêu cầu tính toán bằng cách chỉ kích hoạt
một số ít chuyên gia nhỏ tại một thời điểm (Lepikhin et al., 2020; Fedus et al., 2021; Kim et al., 2021; Artetxe
et al., 2021). Kết quả là, các mô hình MoE có thể đạt được chất lượng cao hơn so với các mô hình
dày đặc bằng cách tăng kích thước mô hình đáng kể. Trong thiết lập huấn luyện phân tán
quy mô lớn, điều này có thể được mở rộng hiệu quả với tính song song của chuyên gia (Fedus et al., 2021).
Tuy nhiên, trong các tình huống suy luận, bất chấp sự tăng dưới tuyến tính trong tải tính toán,
có một sự gia tăng đáng chú ý trong yêu cầu băng thông bộ nhớ. Bảng 1 cho thấy bao nhiêu
chi phí băng thông bộ nhớ được giới thiệu, ngay cả khi chỉ sử dụng 32 chuyên gia mà không
có sự tăng tương ứng trong FLOP lý thuyết, như được triển khai với cổng top-1 (Fedus et al., 2021)
trên GPU NVIDIA A100.
∗Đóng góp bằng nhau.
Bản thảo. Đang được xem xét.arXiv:2310.02410v1  [cs.LG]  3 Oct 2023

--- TRANG 2 ---
Bảng 1: Đo lường tốc độ suy luận và kích thước mô hình của các mô hình dày đặc và MoE. Cả hai mô hình
chạy với kích thước lô 24 và thông lượng được đo bằng số câu được xử lý trong một giây.
Mô hình Thông lượng Kích thước mô hình % trọng số MoE
(câu/giây) (fp16) tính bằng GB
Dày đặc 14.02 1.18 -
MoE (32 chuyên gia) 5.37 9.91 92.8 %
Chênh lệch 0.38X 8.38X -

Bất chấp tiến bộ trong huấn luyện các mô hình MoE, chỉ có một số ít nghiên cứu
liên quan đến suy luận mô hình MoE. Rajbhandari et al. (2022) thiết kế một kiến trúc MoE
hiệu quả hơn và runtime phân tán. Kudugunta et al. (2021) sử dụng thông tin cụ thể tác vụ
để giảm kích thước mô hình tại thời điểm triển khai bằng cách chỉ tải các chuyên gia cụ thể tác vụ.
Kim et al. (2021) cắt tỉa một số chuyên gia tại thời điểm triển khai để giảm kích thước mô hình
bằng cách đánh đổi hiệu suất mô hình. Zoph et al. (2022) sử dụng kỹ thuật chưng cất kiến thức
để chưng cất một mô hình MoE lớn thành một mô hình dày đặc nhỏ hơn để giảm tiêu thụ bộ nhớ
và cải thiện thông lượng. Ngay cả với tất cả các kỹ thuật được đề xuất, vẫn chưa có giải pháp
để tăng tốc suy luận của các mô hình MoE trong khi duy trì độ chính xác.

Để giải quyết vấn đề một cách hiệu quả, chúng tôi chỉ ra thực nghiệm rằng trọng số chuyên gia
có tính mạnh mẽ cao đối với lượng tử hóa, do đó chúng có thể được lượng tử hóa xuống 3-bit
mà không cần huấn luyện bổ sung hoặc dữ liệu hiệu chuẩn và xuống 2-bit với Huấn luyện
Nhận thức Lượng tử hóa (QAT) dẫn đến giảm 79.6% kích thước bộ nhớ. Cũng với tối ưu hóa
runtime, chúng tôi chỉ ra rằng phương pháp này tăng tốc độ suy luận hơn 1.24X nhanh hơn trên GPU A100.

2 Tính mạnh mẽ lượng tử hóa của các lớp MoE

2.1 Phân phối số của trọng số mô hình
Khi lượng tử hóa ma trận, các giá trị ngoại lệ thường làm lệch phạm vi cần lượng tử hóa và
các yếu tố tỷ lệ trở nên quá lớn và dẫn đến chất lượng lượng tử hóa kém. Chúng tôi điều tra
xem liệu có tồn tại các giá trị ngoại lệ trong MoE và các lớp khác hay không. Hình 1 cho thấy
các biểu đồ hộp phân phối trọng số của các lớp tuyến tính trong các khối FFN của mô hình MoE.
Chúng tôi sử dụng khối FFN hai lớp bình thường từ bài báo Transformer (Vaswani et al., 2017).
Theo thực hành được sử dụng rộng rãi, một lớp MoE nằm ở mỗi lớp khác (Lepikhin et al., 2020; Fedus et al.,
2021; Kim et al., 2021). Các lớp số chẵn {0,2, ...} là các lớp FFN chuyên gia và các lớp số lẻ
{1,3, ...} là các lớp FFN dày đặc bình thường. Từ biểu đồ, các lớp FFN dày đặc có phạm vi
lớn hơn nhiều so với các lớp FFN MoE. Điều này chỉ ra rằng các lớp FFN dày đặc có nhiều
giá trị ngoại lệ hơn các lớp FFN MoE. Hiện tượng này phổ biến hơn trong các lớp tuyến tính
thứ hai đôi khi đạt xuống −8.0 như được hiển thị trong Hình 1b.

[Hình 1: Phân phối trọng số FFN qua các lớp với biểu đồ hộp]

--- TRANG 3 ---
2.1.1 Tính mạnh mẽ của các lớp chuyên gia đối với lượng tử hóa

Để hiểu rõ hơn về cách áp dụng lượng tử hóa trên các phần khác nhau của mô hình MoE
ảnh hưởng đến độ chính xác, chúng tôi tiến hành một loạt thí nghiệm với các bit lượng tử hóa
khác nhau. Chúng tôi chia mô hình MoE thành bốn phần: (i) FFN chuyên gia, (ii) các lớp FFN
dày đặc, (iii) các lớp tự chú ý và (iv) các lớp chú ý chéo. Dựa trên quan sát rằng lượng tử hóa
tuyến tính hoạt động tốt hơn với bit thấp hơn, chúng tôi sử dụng nó cho loạt thí nghiệm này.

Hình 2 cho thấy điểm số BLEU đánh giá là một trong những chỉ số chất lượng cho dịch máy
khi lượng tử hóa các phần khác nhau của mô hình MoE. Chúng tôi quan sát thấy rằng lượng tử hóa
các lớp FFN chuyên gia xuống 2-bit không ảnh hưởng nghiêm trọng đến chất lượng mô hình tổng thể.
Tuy nhiên, lượng tử hóa các phần khác của mô hình xuống 2-bit làm giảm đáng kể chất lượng đầu ra.
Các khối chú ý chéo và tự chú ý được lượng tử hóa vẫn có thể duy trì chất lượng với lượng tử hóa
3-bit, nhưng hiệu suất của chúng bị ảnh hưởng với lượng tử hóa 2-bit. Mặt khác, các lớp FFN
dày đặc bị ảnh hưởng đáng kể với lượng tử hóa bit thấp 2-bit và 3-bit. Với lượng tử hóa 3-bit,
điểm số mô hình giảm 23% so với điểm số ban đầu, và lượng tử hóa 2-bit trên các lớp FFN dày đặc
cho điểm số gần như bằng không. Chúng tôi cũng bao gồm nghiên cứu tương tự trên mô hình dày đặc
trong Phụ lục D, mô hình tương tự với lượng tử hóa 2 và 3 bit được quan sát.

[Hình 2: Tác động lượng tử hóa trên các phần khác nhau của mô hình MoE]

3 Thí nghiệm

Với các quan sát từ phần trước, chúng tôi áp dụng tích cực lượng tử hóa bit thấp chỉ trên
trọng số MoE dẫn đến MoQE (Hỗn hợp-các-Chuyên gia-Lượng tử hóa). Chúng tôi sử dụng
tác vụ dịch máy đa ngôn ngữ cho các thí nghiệm của mình. Chi tiết về tập dữ liệu, chỉ số
chất lượng và kiến trúc mô hình được mô tả trong Phụ lục B.

3.1 Kết quả hiệu suất MoQE

Chúng tôi áp dụng công thức lượng tử hóa MoQE cho một mô hình MoE và so sánh hiệu suất
với mô hình MoE cơ sở trong Bảng 2. Để tham khảo, một mô hình dày đặc cũng được huấn luyện
trên cùng tập dữ liệu với mô hình MoE. Đối với mô hình MoE, các thiết lập lượng tử hóa khác nhau
từ 8-bit đến 2-bit được đo cùng với hiệu suất fp16 ban đầu. Đối với lượng tử hóa 2-bit,
QAT bổ sung được áp dụng.

Trước hết, mô hình MoE đạt được cải thiện 2.87% trên điểm số BLEU trong khi tăng kích thước
mô hình lên 8.38X của mô hình dày đặc ban đầu. Khi áp dụng lượng tử hóa hậu huấn luyện 4-bit,
nó vẫn duy trì điểm số BLEU cao hơn 2.11% so với mô hình dày đặc ban đầu. Điều này làm giảm
tiêu thụ bộ nhớ 68% và tăng tốc suy luận 1.24X nhanh hơn so với mô hình MoE fp16. Với QAT 2-bit,
mô hình MoE vẫn có thể duy trì chất lượng cao hơn 1.88% so với mô hình dày đặc ban đầu,
nhưng kích thước mô hình bây giờ chỉ bằng 1.71X của mô hình dày đặc ban đầu.

--- TRANG 4 ---
Bảng 2: So sánh hiệu suất mô hình. Tất cả các mô hình được huấn luyện trên cùng dữ liệu đến khi hội tụ
với 200,000 bước cập nhật. Đường cơ sở là điểm số BLEU và tốc độ của mô hình dày đặc tương đương FLOP.
Loại mô hình Độ chính xác BLEU trung bình Thông lượng Kích thước
(chênh lệch %) (X lần) (X lần)
Dày đặc fp16 45.06 (0) - -
MoE Cơ sở fp16 46.35 (+2.87) 1.00X 1.00X
MoE 5.3B (32 chuyên gia) int8 46.34 (+2.85) 1.16X 0.54X
int4 46.18 (+2.49) 1.24X 0.32X
MoQE int3 46.01 (+2.11) Chưa tối ưu 0.26X
int2 45.90 (+1.88) Chưa tối ưu 0.20X QAT

3.2 So sánh tính mạnh mẽ giữa các mô hình MoE và dày đặc

Chúng tôi so sánh tính mạnh mẽ chống lại lượng tử hóa bit thấp giữa các mô hình MoE và dày đặc
sử dụng lượng tử hóa hậu huấn luyện mà không có QAT. Đối với mô hình dày đặc, lượng tử hóa
với các bit khác nhau được áp dụng cho các lớp FFN số chẵn. Phụ lục D cho thấy đây là lựa chọn
lớp tốt nhất cho mô hình dày đặc. Chúng tôi sử dụng hai tập dữ liệu khác nhau để xác minh
phương pháp lượng tử hóa được đề xuất hoạt động trong các thiết lập mô hình khác nhau.

[Hình 3: So sánh hiệu suất lượng tử hóa giữa các mô hình MoE và dày đặc]

Hình 3 trình bày thí nghiệm với mô hình được huấn luyện với tập dữ liệu dịch đa ngôn ngữ 20 hướng.
Nó cho thấy điểm số BLEU trung bình với độ chính xác lượng tử hóa khác nhau cho cả mô hình MoE
và dày đặc. Mô hình MoE có thể duy trì độ chính xác trong phạm vi -0.3 xuống đến 3-bit và -1.82
cho 2-bit. Mặt khác, mô hình dày đặc chỉ có thể bảo toàn độ chính xác xuống đến 4-bit, nhưng
bắt đầu mất độ chính xác đáng kể hơn 2 điểm số BLEU khi xuống đến 3-bit. Trong trường hợp 2-bit,
mô hình dày đặc mất hầu hết khả năng với -42.96 điểm số BLEU. Bảng 6 trong Phụ lục cho thấy
sự khác biệt điểm số theo lượng tử hóa cho cả mô hình MoE và dày đặc trên 10 bản dịch cặp ngôn ngữ khác nhau.

4 Kết luận và hạn chế

Bài báo này cho thấy các mô hình MoE mạnh mẽ như thế nào đối với lượng tử hóa bit thấp với
các thí nghiệm khác nhau. Bằng cách phân tích độ nhạy theo thành phần và các lựa chọn thiết kế
lượng tử hóa khác nhau, chúng tôi trình bày một cách hiệu quả và hiệu quả để giảm kích thước
mô hình dẫn đến giảm kích thước mô hình 4.9X. Với runtime được tối ưu hóa, mô hình lượng tử hóa
4-bit có thể chạy nhanh hơn 1.24X so với mô hình fp16.

Ngay cả với những phát hiện thú vị, nghiên cứu có một số hạn chế. Trước hết, chưa tồn tại
việc triển khai được tối ưu hóa cho lượng tử hóa thấp hơn 4-bit. Đây là một hướng nghiên cứu
tương lai tiềm năng tốt. Thứ hai, lượng tử hóa 2-bit vẫn yêu cầu QAT trong khi lượng tử hóa
3-bit hoặc bit cao hơn thì không. Cuối cùng, có thể có một cách tiếp cận lai để trộn các độ chính xác
lượng tử hóa khác nhau giữa các lớp MoE và các lớp khác có thể dẫn đến hiệu suất mô hình tối ưu hơn.

--- TRANG 5 ---
Tài liệu tham khảo

Alham Fikri Aji and Kenneth Heafield. Nén các mô hình dịch máy thần kinh với độ chính xác 4-bit.
Trong NGT, 2020.

Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria
Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Mô hình ngôn ngữ quy mô lớn hiệu quả
với hỗn hợp các chuyên gia. arXiv preprint arXiv:2112.10684, 2021.

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Mở rộng đến các mô hình
tham số nghìn tỷ với độ thưa thớt đơn giản và hiệu quả. arXiv preprint arXiv:2101.03961, 2021.

Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah A. Smith. Bộ mã hóa sâu,
bộ giải mã nông: Đánh giá lại dịch máy không tự hồi quy. Trong ICLR, 2021.

Guolin Ke, Di He, and Tie-Yan Liu. Suy nghĩ lại về mã hóa vị trí trong tiền huấn luyện ngôn ngữ.
ArXiv, abs/2006.15595, 2021.

Young Jin Kim, Marcin Junczys-Dowmunt, Hany Hassan, Alham Fikri Aji, Kenneth Heafield, Roman
Grundkiewicz, and Nikolay Bogoychev. Từ nghiên cứu đến sản xuất và ngược lại: Dịch máy thần kinh
nhanh một cách kỳ lạ. Trong Proceedings of the 3rd Workshop on Neural Generation and Translation,
pp. 280–288, 2019.

Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu,
Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Huấn luyện MoE
có thể mở rộng và hiệu quả cho các mô hình đa tác vụ đa ngôn ngữ. arXiv preprint arXiv:2109.10465, 2021.

Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang
Luong, and Orhan Firat. Vượt xa chưng cất: Hỗn hợp chuyên gia cấp tác vụ cho suy luận hiệu quả.
Trong EMNLP, 2021.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Mở rộng các mô hình khổng lồ với
tính toán có điều kiện và phân mảnh tự động. arXiv preprint arXiv:2006.16668, 2020.

Rui Liu, Young Jin Kim, Alexandre Muzio, and Hany Hassan. Gating dropout: Điều chỉnh hiệu quả
giao tiếp cho các transformer được kích hoạt thưa thớt. Trong International Conference on Machine
Learning, pp. 13782–13792. PMLR, 2022.

Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Am-
mar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Thúc đẩy suy luận và huấn luyện
hỗn hợp chuyên gia để cung cấp năng lượng cho quy mô AI thế hệ tiếp theo. Trong ICML, 2022.

Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Trong NIPS, 2017.

Yiren Wang, ChengXiang Zhai, and Hany Hassan Awadalla. Học đa tác vụ cho dịch máy thần kinh
đa ngôn ngữ. arXiv preprint arXiv:2010.02523, 2020.

Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
Yanyan Lan, Liwei Wang, and Tie-Yan Liu. Về chuẩn hóa lớp trong kiến trúc transformer.
Trong ICML, 2020.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and
William Fedus. Thiết kế các mô hình chuyên gia thưa thớt hiệu quả. arXiv preprint arXiv:2202.08906, 2022.

--- TRANG 6 ---
A Thuật toán lượng tử hóa

A.1 Kỹ thuật lượng tử hóa

Chúng tôi thử hai kỹ thuật lượng tử hóa, đó là (i) lượng tử hóa tuyến tính là ánh xạ các giá trị
số nguyên lượng tử hóa và giá trị float ban đầu một cách đồng nhất và (ii) lượng tử hóa dựa trên log
từ Aji & Heafield (2020) ánh xạ các phạm vi số nguyên và float theo thang log. Trong cả hai trường hợp,
chúng tôi chọn lượng tử hóa theo kênh thay vì lượng tử hóa theo ma trận dựa trên thí nghiệm trong Phụ lục C.

Lượng tử hóa tuyến tính với giá trị tuyệt đối cực đại. Kỹ thuật đầu tiên là lượng tử hóa tuyến tính,
với ma trận A và b bit, nó mã hóa A như sau:

sj=2×max(|A:,j|)
2b−1

Q:,j= int(A:,j
sj)

trong đó s là yếu tố tỷ lệ có thể được chọn theo kênh như được hiển thị hoặc cho toàn bộ tensor.
Tại thời điểm suy luận, Q lượng tử hóa được giải lượng tử hóa trở lại A′ với yếu tố tỷ lệ s như sau:

A′:,j=Q:,j×sj

Lượng tử hóa thang log. Kỹ thuật thứ hai là lượng tử hóa thang log trong đó 1 bit được giữ cho
dấu và (b−1) bit được sử dụng để mã hóa các giá trị thang log. Với ma trận A, công thức lượng tử hóa như sau:

P=sign(A)
T=clip(|A|
s,1,21−2b−1)
Q=⌈log2(2
3T)⌉

trong đó s có thể được chọn theo hai cách, hoặc (i) giá trị tuyệt đối cực đại hoặc (ii) giá trị tối ưu
để giảm thiểu sai số bình phương trung bình (MSE) giữa các giá trị lượng tử hóa và ban đầu được
mô tả trong Aji & Heafield (2020). Chúng tôi sử dụng thuật toán thứ hai mà chúng tôi quan sát
độ chính xác tốt hơn với lượng tử hóa. Tại thời điểm suy luận, các giá trị trọng số lượng tử hóa
được giải lượng tử hóa dựa trên công thức như sau:

A′=P×s×2Q
2 3

[Hình 4: Lượng tử hóa tuyến tính so với thang log với lượng tử hóa s tối ưu]

B Thiết lập thí nghiệm

Tác vụ. Chúng tôi sử dụng tác vụ dịch máy đa ngôn ngữ cho các thí nghiệm của mình với hai tập dữ liệu
khác nhau là 20 hướng ngôn ngữ và 10 hướng ngôn ngữ tương ứng. Chúng tôi sử dụng sacrebleu trên
đầu ra đã bỏ token hóa để đo độ chính xác của các mô hình. Một GPU NVIDIA A100 đơn chạy trong
container docker chạy Ubuntu 20.04 và CUDA 11.6 được sử dụng cho tất cả các thí nghiệm, và tất cả
mã được biên dịch với nvcc và gcc/g++ 9.3. Chúng tôi đo runtime đầu cuối của suy luận cho tập dữ liệu đánh giá.

Tập dữ liệu. Chúng tôi sử dụng hai tập dữ liệu khác nhau được mô tả dưới đây. Đối với thiết lập tập dữ liệu
lớn hơn, chúng tôi sử dụng tập dữ liệu được thu thập nội bộ bao gồm 6 ngôn ngữ khác nhau là Đức (de),
Pháp (fr), Ý (it), Tây Ban Nha (es), Hà Lan (nl) và Tiếng Anh (en). Chúng được thu thập từ web,
và mỗi cặp ngôn ngữ có ít nhất vài trăm triệu câu. Chúng tôi sử dụng từ vựng 128,000 từ phụ được xây dựng
với thư viện sentencepiece. Số câu huấn luyện được bao gồm trong Phụ lục F.

Đối với thiết lập tập dữ liệu nhỏ hơn, chúng tôi sử dụng tập dữ liệu benchmark WMT-10 được sử dụng
rộng rãi cho các benchmark công khai (Wang et al., 2020; Kim et al., 2021). Có 32.5 triệu cặp câu
cho 20 cặp ngôn ngữ lấy tiếng Anh làm trung tâm bao gồm Pháp (fr), Séc (cs), Đức (de), Phần Lan (fi),
Latvia (lt), Estonia (et), Romania (ro), Hindi (hi), Thổ Nhĩ Kỳ (tr) và Gujarati (gu).

Kiến trúc mô hình. Đối với tất cả các thí nghiệm với tập dữ liệu lớn, chúng tôi sử dụng 24 lớp mã hóa
transformer (Vaswani et al., 2017) và 12 lớp giải mã transformer theo thực hành mã hóa sâu hơn và
giải mã nông hơn (Kim et al., 2019; Kasai et al., 2021) để hiệu quả hơn trong giải mã tự hồi quy.
Chiều nhúng là 1,024 và chiều ẩn FFN là 4,096. Đối với mã hóa thông tin vị trí vào trạng thái ẩn,
chúng tôi sử dụng Transformer với Mã hóa Vị trí Không ràng buộc (TUPE) được đề xuất trong Ke et al. (2021)
thay vì nhúng vị trí sin thông thường. Một lựa chọn thiết kế khác là vị trí của chuẩn hóa lớp.
Để ổn định huấn luyện, chúng tôi sử dụng chuẩn hóa trước lớp được đề xuất trong Xiong et al. (2020)
thay vì chuẩn hóa sau lớp ban đầu từ (Vaswani et al., 2017). Chúng tôi huấn luyện các mô hình MoE
và dày đặc để so sánh. Các lựa chọn kiến trúc mô hình được đề cập ở đây là chung cho cả hai mô hình.
Sự khác biệt duy nhất giữa các mô hình dày đặc và MoE là số lượng chuyên gia. Chúng tôi sử dụng
32 chuyên gia cho mô hình MoE được huấn luyện với dữ liệu web lớn hơn. Chúng tôi sử dụng giải mã
tìm kiếm chùm với kích thước chùm 5. Đối với các thí nghiệm với tập dữ liệu nhỏ hơn, chúng tôi sử dụng
12 lớp mã hóa transformer và 6 lớp giải mã transformer. Chiều nhúng là 768 và chiều ẩn FFN là 3,072.
Trong thiết lập này, chúng tôi sử dụng các lớp MoE với 128 chuyên gia ở mỗi lớp khác.

Kiến trúc MoE. Đối với các thiết lập cụ thể của mô hình MoE, chúng tôi sử dụng cổng học top-1 từ
Fedus et al. (2021) và sử dụng lớp MoE ở mỗi lớp khác là các lớp số chẵn (Lepikhin et al., 2020;
Fedus et al., 2021; Kim et al., 2021). Trong quá trình huấn luyện các mô hình MoE, chúng tôi sử dụng
nhiễu jittering và mất cân bằng (tỷ lệ 0.01) được đề xuất trong Lepikhin et al. (2020); Fedus et al. (2021)
để phân phối sử dụng chuyên gia đồng đều hơn. Để ngăn chặn overfitting và điều chỉnh mô hình tốt hơn,
chúng tôi cũng sử dụng gating dropout (0.2) (Liu et al., 2022).

C Lượng tử hóa theo kênh so với theo ma trận

Các yếu tố tỷ lệ được tính toán bởi thuật toán lượng tử hóa và được lưu trữ trong các số điểm nổi
nửa độ chính xác (fp16) để giải lượng tử hóa các ma trận. Các yếu tố này có thể được chọn theo
thang kênh hoặc thang ma trận toàn bộ. Như được hiển thị trong hình 5, lượng tử hóa theo kênh
cho điểm số khá cao hơn so với theo tensor đặc biệt là với độ chính xác thấp. Tham số bổ sung
để lưu trữ các yếu tố tỷ lệ theo kênh là nhỏ, bởi vì chỉ cần một giá trị cho một kênh và ít hơn 1%
tổng tham số trong một ma trận. Do đó, chúng tôi sử dụng lượng tử hóa theo kênh cho tất cả
các thí nghiệm lượng tử hóa.

D Lượng tử hóa các lớp khác nhau trong mô hình dày đặc

Trong bài báo, chúng tôi so sánh một mô hình dày đặc và một mô hình MoE về tính mạnh mẽ lượng tử hóa.
Để có so sánh công bằng, chúng tôi xem xét lượng tử hóa chỉ một nửa FFN của các khối transformer dày đặc,
bởi vì chúng tôi chỉ lượng tử hóa trọng số chuyên gia chỉ tồn tại ở mỗi khối khác (số chẵn).
Chúng tôi so sánh ba cấu hình khác nhau - (1) chỉ lượng tử hóa FFN của các khối số chẵn, (2) chỉ lượng tử hóa
FFN của các khối số lẻ và (3) lượng tử hóa tất cả các lớp FFN. Như có thể thấy trong Hình D, lượng tử hóa
FFN của các khối số chẵn ảnh hưởng đến độ chính xác ít nhất, và lượng tử hóa tất cả các lớp FFN
cho kết quả tồi tệ nhất. Dựa trên thí nghiệm này, chúng tôi chỉ lượng tử hóa FFN của các khối transformer
số chẵn cho mô hình dày đặc trong tất cả các thí nghiệm và so sánh.

--- TRANG 7 ---
[Hình 5: Lượng tử hóa tuyến tính của FFN chuyên gia với các yếu tố tỷ lệ theo kênh và theo ma trận]

[Hình 6: Tác động lượng tử hóa của các lớp khác nhau trong mô hình dày đặc]

E Độ lệch của ma trận trọng số trong các mô hình MoE và dày đặc

Trong phân tích phân phối trọng số mô hình ở Phần 2, chúng tôi quan sát thấy rằng các lớp FFN
của mô hình dày đặc có xu hướng có nhiều giá trị ngoại lệ hơn so với các lớp FFN chuyên gia của MoE.
Chúng tôi đo độ lệch của phân phối trọng số của những cái đó trong Bảng 3.

Bảng 3: Độ lệch phân phối tham số của các lớp FFN chuyên gia so với không chuyên gia

[Bảng 3 với các giá trị độ lệch tham số]

F Tóm tắt tập dữ liệu dịch máy

Bảng 4 cho thấy số câu song song được sử dụng để huấn luyện các mô hình dày đặc và MoE.
Tất cả các ngôn ngữ có ít nhất 300 triệu câu và sự khác biệt về số lượng giữa các ngôn ngữ
ít hơn hai lần.

Bảng 4: Số câu song song bao gồm dữ liệu dịch ngược.

[Bảng 4 với số liệu câu song song]

G Sự khác biệt điểm số BLEU chi tiết với lượng tử hóa được áp dụng cho mô hình
được huấn luyện trên tập dữ liệu WMT công khai

Bảng 5 cho thấy các thay đổi điểm số BLEU cá nhân với các bit lượng tử hóa khác nhau
cho các mô hình MoE và dày đặc được huấn luyện trên tập dữ liệu WMT công khai.

H Sự khác biệt điểm số BLEU chi tiết với lượng tử hóa được áp dụng cho mô hình 5.3B.

Bảng 6 cho thấy các thay đổi điểm số BLEU cá nhân với các bit lượng tử hóa khác nhau
cho các mô hình MoE và dày đặc được đo với tập dữ liệu xác thực nội bộ. Bảng 7 cho thấy
hiệu suất đánh giá của cùng mô hình trên hai tập dữ liệu công khai WMT.

--- TRANG 8-11 ---
[Các bảng 5, 6, 7 với dữ liệu điểm số BLEU chi tiết]
