# 2308.07939.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2308.07939.pdf
# Kích thước tệp: 500682 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
ADA-QPACKNET - TỈA CẢI TIẾN VỚI GIẢM ĐỘ RỘNG BIT NHƯ MỘT PHƯƠNG PHÁP HỌC LIÊN TỤC HIỆU QUẢ KHÔNG QUÊN∗
Marcin Pietron
AGH-UST
Kraków
pietron@agh.edu.plDominik Zurek
AGH-UST
Kraków
dzurek@agh.edu.plKamil Faber
AGH-UST
Kraków
kfaber@agh.edu.plRoberto Corizzo
American University
Washington
corizzo@american.edu

TÓM TẮT
Học Liên tục (CL) là một quá trình trong đó vẫn còn khoảng cách lớn giữa hiệu quả của con người và mô hình học sâu. Gần đây, nhiều thuật toán CL đã được thiết kế. Hầu hết chúng có nhiều vấn đề với việc học trong môi trường động và phức tạp. Trong công trình này, phương pháp dựa trên kiến trúc mới Ada-QPacknet được mô tả. Nó kết hợp việc tỉa để trích xuất mạng con cho mỗi nhiệm vụ. Khía cạnh quan trọng trong các phương pháp CL dựa trên kiến trúc là dung lượng của chúng. Trong phương pháp được trình bày, kích thước của mô hình được giảm bằng phương pháp lượng tử hóa tuyến tính và phi tuyến hiệu quả. Phương pháp này giảm độ rộng bit của định dạng trọng số. Kết quả được trình bày cho thấy lượng tử hóa bit thấp đạt được độ chính xác tương tự như mạng con dấu phẩy động trên các kịch bản CL được biết đến. Theo kiến thức của chúng tôi, đây là chiến lược CL đầu tiên kết hợp cả hai kỹ thuật nén tỉa và lượng tử hóa để tạo ra các mạng con nhiệm vụ. Thuật toán được trình bày đã được thử nghiệm trên các tổ hợp tập phổ biến và so sánh với các thuật toán phổ biến nhất. Kết quả cho thấy phương pháp được đề xuất vượt trội hơn hầu hết các chiến lược CL trong các kịch bản gia tăng nhiệm vụ và lớp.
Từ khóa Học liên tục · Lượng tử hóa · Tỉa · Quên Thảm khốc

1 Giới thiệu
Học liên tục (CL) là một mô hình học máy mới nổi nhằm thiết kế các phương pháp và chiến lược mới để cung cấp phân tích chính xác trong các môi trường thực tế phức tạp và động [6]. Khi các mô hình phải đối mặt với nhiều nhiệm vụ trong suốt vòng đời của chúng, một tính chất mong muốn cho các chiến lược CL là duy trì hiệu suất cao trên tất cả các nhiệm vụ. Mô hình này đang nhận được sự chú ý ngày càng tăng từ các cộng đồng nghiên cứu, dẫn đến một số công trình được đề xuất [55,54,2,6,18,3]. Có ba loại chiến lược CL chính: luyện tập lại (còn được gọi là phát lại kinh nghiệm), điều hòa, và kiến trúc. Các chiến lược kiến trúc khá mạnh mẽ ở chỗ chúng cho phép thích ứng và phát triển mô hình để đáp ứng các nhiệm vụ mới, và nghiên cứu gần đây cho thấy các phương pháp như vậy nằm trong số những phương pháp hiệu quả và có hiệu lực nhất trong các kịch bản CL [21,32,30,18]. Các phương pháp không quên [21,32] đáp ứng các nhiệm vụ mới bằng cách gán một tập con của số lượng trọng số có sẵn cho mỗi nhiệm vụ. Hiệu quả của chúng phụ thuộc mạnh vào khả năng tận dụng hiệu quả dung lượng mô hình để đáp ứng càng nhiều nhiệm vụ càng tốt.

Một cạm bẫy quan trọng của các phương pháp kiến trúc không quên dựa trên tỉa hiện tại là chúng bị hạn chế trong khả năng khai thác hiệu quả độ thưa thớt của mô hình vì mỗi lớp được tỉa với cùng một mức độ thưa thớt không đổi. Kết quả là, chúng không điều chỉnh đúng cách số lượng trọng số cần loại bỏ trong khi vẫn bảo toàn độ chính xác phân loại [64,51]. Một hạn chế chung khác được chia sẻ bởi các phương pháp không quên là chúng dễ bị bão hòa nhanh chóng dung lượng mô hình có sẵn, ở chỗ chúng không thể gán nhiều hơn một giá trị cho mỗi trọng số và do đó bị giới hạn bởi số lượng trọng số có sẵn.

∗Bài báo được chấp nhận tại ECAI 2023arXiv:2308.07939v2 [cs.LG] 1 Oct 2023

--- TRANG 2 ---
Ada-QPacknet

Trong bài báo này, chúng tôi đề xuất Ada-QPacknet, một phương pháp không quên mới cho phân loại hình ảnh liên tục giải quyết những hạn chế này bằng cách kết hợp tỉa thích ứng với nhận dạng mức độ thưa thớt và lượng tử hóa trọng số phi tuyến thích ứng.

Cụ thể, để khai thác hiệu quả độ thưa thớt của mô hình, Ada-QPacknet được đề xuất của chúng tôi kết hợp tỉa thích ứng với các mức độ thưa thớt khác nhau cho mỗi lớp, trong đó tỷ lệ tỉa được chọn thông qua tìm kiếm vé số nhanh. Hơn nữa, để giải quyết vấn đề bão hòa nhanh chóng dung lượng mô hình, Ada-QPacknet thực hiện một giai đoạn lượng tử hóa thích ứng tách mỗi trọng số thành nhiều thành phần, mỗi thành phần sử dụng một tập con của 32 bit có sẵn. Việc tách này cho phép chúng tôi tái sử dụng một trọng số duy nhất cho nhiều hơn một nhiệm vụ, dẫn đến giảm sử dụng dung lượng của mô hình và cải thiện về mặt hiệu quả mô hình. Đồng thời, bằng cách gán độc quyền các thành phần đơn lẻ của trọng số cho các nhiệm vụ, chúng tôi vượt qua hạn chế thiên vị thường tồn tại trong các phương pháp không quên thông thường, gán nhiều nhiệm vụ cho cùng một giá trị trọng số.

Tóm lại, những đóng góp của bài báo chúng tôi như sau:
• Một phương pháp tỉa thích ứng tận dụng độ nhạy và tầm quan trọng của trọng số đặc trưng cho các lớp khác nhau trong các mô hình học sâu. Phương pháp này cho phép chúng tôi giảm tập hợp trọng số có sẵn được gán cho các nhiệm vụ riêng biệt, cho phép AdaQ-Packnet sử dụng hiệu quả một tập con các trọng số mà không mất đáng kể về độ chính xác.

• Một chiến lược lượng tử hóa cho học liên tục, cho phép chúng tôi chia trọng số thành các thành phần, mỗi thành phần có thể được gán cho một nhiệm vụ cụ thể. Khả năng này cho phép AdaQ-Packnet khai thác tốt hơn hiệu quả và dung lượng mô hình, tận dụng biểu diễn độ rộng bit giảm của trọng số.

• Một đánh giá thực nghiệm với ba kịch bản CL chuẩn, làm nổi bật rằng phương pháp được đề xuất của chúng tôi vượt trội hơn các chiến lược CL luyện tập lại, điều hòa và kiến trúc tiên tiến, cả về độ chính xác và khai thác dung lượng mô hình.

Bài báo được cấu trúc như sau. Phần 2 tóm tắt các công trình liên quan trong các chiến lược CL và phương pháp tỉa. Phần 3 mô tả phương pháp Ada-QPacknet được đề xuất của chúng tôi. Phần 4 mô tả thiết lập thực nghiệm của chúng tôi và trình bày kết quả được trích xuất trong các thực nghiệm của chúng tôi. Phần 5 kết thúc bài báo với một tóm tắt về kết quả thu được và phác thảo các hướng có liên quan cho công việc tương lai.

2 Các công trình liên quan
Các phương pháp học liên tục thường được phân loại thành luyện tập lại, điều hòa và kiến trúc [6]. Các phương pháp luyện tập lại sử dụng bộ nhớ để ghi nhớ một số dữ liệu từ các tập trước đó. Giải pháp đơn giản nhất là lưu trữ một phần dữ liệu huấn luyện trước đó và xen kẽ chúng với dữ liệu huấn luyện mới (Replay [6]). Các phương pháp luyện tập lại phổ biến khác bao gồm GEM [19], A-GEM [25], và GDumb [26]. Các phương pháp điều hòa thường sử dụng các ràng buộc trên hàm mất mát để tăng cường kết nối cho các mẫu đã học. Các phương pháp phổ biến nhất là Synaptic Intelligence (SI) [22], ước tính tầm quan trọng của trọng số dựa trên mất mát điều hòa có trọng số và gradient. Các phương pháp thay thế là LwF [28], và EWC [20]. Các công trình có liên quan nhất cho nghiên cứu này thuộc về danh mục kiến trúc, đại diện cho một hướng đầy hứa hẹn trong các công trình gần đây [21].

Các phương pháp kiến trúc tập trung vào cấu trúc của mô hình neural. Một trong những phương pháp được biết đến và hiệu quả nhất là CWRStar [31] thực hiện sao chép trọng số và khởi tạo lại trọng số ở lớp cuối cùng để đáp ứng các nhiệm vụ mới. Các phương pháp hiệu quả hơn là những phương pháp dựa trên tỉa giải quyết vấn đề tăng dung lượng mô hình và loại bỏ các trọng số không quan trọng. Các ví dụ đáng chú ý bao gồm PackNet [32], chia mô hình thành các mạng con độc lập để giải quyết các nhiệm vụ khác nhau. Phương pháp như vậy được cho là không quên vì các mạng con được ánh xạ trực tiếp tới các nhiệm vụ và không thay đổi trọng số. Tuy nhiên, một hạn chế lớn là nguy cơ tăng dung lượng mô hình. Vấn đề này gần đây đã khuyến khích các nhà nghiên cứu khám phá và thiết kế các phương pháp tỉa, để giảm dung lượng mô hình và yêu cầu lưu trữ.

Tỉa trong mạng neural có thể có cấu trúc và không có cấu trúc. Các công trình trong [39,78,77,73,74,76,80] tập trung vào tỉa có cấu trúc, bao gồm tỉa toàn bộ các kênh trong bộ lọc bằng các kỹ thuật khác nhau. Tỉa không có cấu trúc có thể được thực hiện trên các mô hình đã được huấn luyện trước mà không cần huấn luyện lại như trong [64,68], dẫn đến tăng hiệu quả, hoặc với huấn luyện lại, như trong tìm kiếm vé số [66], tỉa chuyển động [67], và dropout biến thiên [79]. Các phương pháp tỉa hiệu quả khác bao gồm các phương pháp dựa trên di truyền như trong [51], áp dụng mặt nạ không đổi và định nghĩa các toán tử lai tạo và đột biến riêng, và các kỹ thuật tăng cường như trong [52]. Tỉa có thể được tiến hành kết hợp với huấn luyện lại để tránh sụt giảm hiệu suất do loại bỏ trọng số [64]. Hai phương pháp học liên tục gần đây áp dụng phương pháp tỉa là SupSup [18] và Winning Subnetworks (WSN) [21]. SupSup tìm mặt nạ nhị phân tối ưu trên mạng xương sống cố định cho mỗi nhiệm vụ mới. Mặt khác, WSN đã giới thiệu cải tiến đáng kể so với Packnet, vì nó chỉ xem xét một tập con trọng số từ nhiệm vụ trước đó để tránh chuyển giao tiến về phía trước thiên vị. Một xu hướng gần đây khác là kiến trúc mô hình có thể mở rộng với nén. Phương pháp Dynamically Expandable Representation (DER) [7] thêm một mô-đun con mới vào mô hình chính mỗi khi một nhiệm vụ mới được trình bày, trong khi tận dụng tỉa như một chế độ bổ sung. Các tác giả trong [8] trình bày phương pháp Dytox, dựa trên các khối chú ý Transformer. Mô hình học liên tục bao gồm một phần bộ mã hóa - chung cho tất cả các nhiệm vụ và một bộ giải mã, cụ thể cho nhiệm vụ. Mặt khác, FOSTER [9] đề xuất một phương pháp tăng cường mở rộng mạng bằng một bộ trích xuất đặc trưng dư và một lớp phân loại tuyến tính. Cả Dytox [8] và FOSTER [9] đều kết hợp một chiến lược phát lại để cải thiện hiệu quả của mô hình.

Lượng tử hóa là một trong những kỹ thuật hiệu quả nhất cho việc nén các mô hình học sâu [68,58]. Các chiến lược phổ biến bao gồm lượng tử hóa tất cả các hệ số trong một lớp đơn với một số bit được chỉ định để biểu diễn các phần nguyên và phần thập phân [41,42] dựa trên phạm vi giá trị của tập hệ số. Một chiến lược khác là biểu diễn các hệ số và dữ liệu bằng các số nguyên với một hệ số tỷ lệ thích hợp. Nhiều phương pháp lượng tử hóa trong tài liệu áp dụng các phương pháp tuyến tính [68,58,42] hoặc phi tuyến bao gồm phân cụm [57]. Lượng tử hóa có thể được thực hiện trong quá trình huấn luyện mô hình [58] hoặc có thể được chạy trên một mô hình đã được huấn luyện trước [68,57]. Gần đây, một số phương pháp cho biểu diễn bit thấp đã được thiết kế [43, 44, 45, 46, 47]. Nhiều trong số chúng không thể được chạy mà không có một số suy giảm đáng kể về độ chính xác. Một lợi thế được biết đến của lượng tử hóa là thực tế nó tạo thuận lợi cho việc áp dụng mạng neural sâu trong các bộ gia tốc phần cứng chuyên dụng với độ rộng bit số học hạn chế và không gian bộ nhớ [51]. Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi, bài báo của chúng tôi là nỗ lực đầu tiên áp dụng lượng tử hóa cho các mô hình CL.

3 Thuật toán Ada-QPacknet
Trong phần này, chúng tôi mô tả Ada-QPacknet được đề xuất của chúng tôi – một phương pháp không quên tận dụng tỉa thích ứng và lượng tử hóa thích ứng để loại bỏ quên thảm khốc, khai thác hiệu quả độ thưa thớt mô hình và sử dụng hiệu quả dung lượng mô hình.

Hình 1 trình bày tổng quan về thuật toán Ada-QPacknet, minh họa quá trình lặp đi lặp lại của tỉa thích ứng và lượng tử hóa trên một kịch bản huấn luyện điển hình với nhiều nhiệm vụ.

Ada-QPacknet thích ứng quá trình tỉa thích ứng [51,64] để giảm kích thước của các mạng con nhiệm vụ, vì nhược điểm đáng kể nhất của các phương pháp không quên tiên tiến như PackNet [32] và WSN [21], là mức độ thưa thớt không đổi được áp dụng cho tất cả các lớp trong các nhiệm vụ liên tiếp. Điều này hạn chế đáng kể khả năng thích ứng của phương pháp và đặt ra nguy cơ sử dụng không hiệu quả dung lượng mô hình. Một đặc điểm quan trọng khác của Ada-QPacknet là lượng tử hóa thích ứng, cho phép chúng tôi gán nhiều hơn một nhiệm vụ cho mỗi trọng số, bằng cách chia dung lượng của nó thành các thành phần. Bản chất thích ứng của quá trình lượng tử hóa cho phép Ada-QPacknet xác định độ rộng bit tối ưu cho mỗi nhiệm vụ, dựa trên sự đánh đổi giữa số lượng bit được gán cho mỗi nhiệm vụ và hiệu suất mô hình. Trong các phần phụ sau, chúng tôi mô tả tỉa thích ứng và lượng tử hóa thích ứng chi tiết hơn.

3.1 Tỉa Thích ứng với Nhận dạng Mức độ Thưa thớt
Trong giai đoạn này, mục tiêu là thực hiện tỉa trong khi xác định các mức độ thưa thớt riêng biệt cho mỗi lớp. Các mức độ thưa thớt nên cao nhất có thể để giảm chiếm dụng bộ nhớ để lại nhiều dung lượng hơn cho các nhiệm vụ tương lai. Biểu diễn của mô hình đã tỉa Fp_Θ là tuple sau:

Fp_Θ = (FΘ, M), (1)

trong đó FΘ được định nghĩa là:

FΘ(X) = fθL(fθL-1...(fθ0(X))). (2)

FΘ là mô hình gốc với một tập hợp các lớp tích chập và kết nối đầy đủ fθi, được định nghĩa theo thứ tự được chỉ định. Tensor Θ được định nghĩa là:

Θ = {θ0, θ1, ..., θL}, (3)

và chứa trọng số của các lớp tích chập và kết nối đầy đủ. θi là một tensor trọng số giữa các lớp i và i+1. Một mục đơn lẻ wi,k,l đại diện cho một trọng số giữa neuron thứ k trong lớp i và neuron thứ l trong lớp i+1².

M là một tập hợp các mặt nạ:

M = {M0, M1, ..., ML}. (4)

² Để đơn giản hóa ký hiệu của chúng tôi, chúng tôi giả định rằng một giá trị trọng số duy nhất được gán cho tất cả các nhiệm vụ. Trong phương pháp của chúng tôi, nhiều giá trị cho một trọng số duy nhất có thể được gán để giải quyết nhiều nhiệm vụ.

--- TRANG 4 ---
Ada-QPacknet
Ada-QPacknet Mô hình
Ban đầu
Huấn luyện trên Nhiệm vụ 1...
Các Nhiệm vụ TiếpMô hình được Lượng tử hóa
Mô hình 
đã Tỉa
Mô hình
đã Tỉa
Lượng tử hóa
Thích ứng
Lượng tử hóa
Thích ứng
Mô hình được
Lượng tử hóa
Tỉa
Thích ứng
Tỉa
Thích ứng
Huấn luyện trên Nhiệm vụ 2
...
Mô hình
Cuối cùng

Hình 1: Tổng quan về phương pháp Ada-QPacknet được đề xuất. Ban đầu, mô hình có biểu diễn độ rộng bit đầy đủ được đại diện bởi các hình chữ nhật trắng. Phương pháp tuân theo một quy trình lặp bao gồm tỉa thích ứng trong quá trình huấn luyện và lượng tử hóa thích ứng. Tỉa thích ứng xác định các mức độ thưa thớt khác nhau cho mỗi lớp. Lượng tử hóa thích ứng xác định độ rộng bit tối ưu cho mỗi nhiệm vụ (được xác định bằng các màu khác nhau, tức là đỏ, xanh dương, tím và vàng) và lượng tử hóa trọng số, cải thiện việc khai thác dung lượng mô hình. Mô hình cuối cùng có bốn trọng số hỗ trợ bốn nhiệm vụ khác nhau, minh họa việc sử dụng hiệu quả độ thưa thớt và dung lượng mô hình.

Mỗi mặt nạ Mi được đặt giữa các lớp i và i+1 và có cùng hình dạng với θi. Mặt nạ được sử dụng để quyết định trọng số nào sẽ được sử dụng để giải quyết các nhiệm vụ cụ thể. Một mục đơn lẻ trong Mi chứa một tập hợp các số chỉ ra trong những nhiệm vụ nào trọng số được sử dụng. Do đó, mỗi trọng số có thể được gán cho một hoặc nhiều nhiệm vụ. Một biểu diễn đồ họa được hiển thị trong Hình 2.

Một mục đơn lẻ trong mặt nạ Mi được biểu diễn như một vector nhị phân:

Mi_k,l = {t1, t2, ..., tn}, (5)

trong đó k là neuron nguồn trong lớp i, l là neuron đích trong lớp i+1, ti ∈ {0,1} chỉ ra liệu một trọng số cho trước có được sử dụng bởi nhiệm vụ thứ i hay không.

Mức độ thưa thớt Υi là tỷ lệ giữa số lượng trọng số chưa được gán cho bất kỳ nhiệm vụ nào và số lượng tất cả trọng số trong mặt nạ Mi:

Υi = |Mi| - P|Mi|_k,l(|Mi_k,l| > 0) / |Mi| (6)

Độ thưa thớt có trọng số tổng thể của mô hình được định nghĩa là:

Υ = ΣL_i|θi| · Υi (7)

3.1.1 Tạo mặt nạ vé số ngẫu nhiên
Mỗi khi gặp một nhiệm vụ mới, đối với mỗi lớp, một số mặt nạ ứng viên PS được tạo ra theo thuật toán vé số ngẫu nhiên, có tính đến ràng buộc mức độ thưa thớt giữa các giá trị Vmin và Vmax. Ví dụ, mức độ thưa thớt 0.5 có nghĩa là một nửa tất cả trọng số sẽ được tận dụng để giải quyết nhiệm vụ. Bằng cách này, chúng tôi tạo ra một mặt nạ ứng viên cho nhiệm vụ t và lớp l:

CMt_l = {e|e ∈ Mi ∧ |e| < TL}, |CMt_l| < |Mi|, (8)

trong đó |e| là số lượng nhiệm vụ đã được gán cho trọng số e, và TL là một hằng số xác định dung lượng, tức là số lượng nhiệm vụ tối đa có thể được gán cho một trọng số duy nhất, theo lượng tử hóa mong muốn được sử dụng. Ví dụ, TL = 4 với dung lượng trọng số 32-bit và tỷ lệ lượng tử hóa là 8-bit. Công thức này cho phép chúng tôi lấy mẫu một tập con các trọng số có sẵn từ mặt nạ gốc (theo ràng buộc thưa thớt) với không hoặc nhiều gán trước cho các nhiệm vụ.

Tập hợp các mặt nạ ứng viên cho tất cả các lớp được định nghĩa là:

CMt = {CMt_0, CMt_1, ..., CMt_L} (9)

Khi tất cả các mặt nạ ứng viên được tạo ra, ứng viên tốt nhất theo độ chính xác kết quả cho lớp hiện tại được chọn.

Thuật toán 1 Tỉa Thích ứng cho nhiệm vụ t
Yêu cầu: PS – kích thước quần thể
Yêu cầu: Θ – trọng số của mô hình
Yêu cầu: α,β – tầm quan trọng của độ chính xác và độ thưa thớt trong lựa chọn
1: Θ ← khởi tạo các trọng số có sẵn trong Θ
2: CMt ← ∅ // Một tập hợp các mặt nạ ứng viên
3: Θt ← ∅ // Một tập hợp các trọng số đã che
4: for i = 0 to PS do
5:   CMt,i ← mặt nạ vé số ngẫu nhiên (eq: 9)
6:   Θt,i = Θ ⊙ CMt,i {Trọng số đã che}
7:   Thực hiện huấn luyện ngắn Θt,i trên nhiệm vụ t
8:   CMt ← CMt ∪ CMt,i
9:   Θt ← Θt ∪ Θt,i
10: end for
11: At ← ∀ CMt,i ∈ CMt độ chính xác của Θ ⊙ CMt,i trên nhiệm vụ t
12: Υt ← ∀ CMt,i ∈ CMt tính Υi cho CMt,i (eq: 7)
13: id = argmax_i∈{0,...,PS}(α·Ai/max(A) + β·Υi/max(Υ))
14: Mt ← CMt,id
15: Θ' ← Θt,id
16: Thực hiện huấn luyện đầy đủ của Θ' trên nhiệm vụ t
17: return Mt, Θ'

Thuật toán 1 mô tả quá trình tỉa nhiệm vụ cho một nhiệm vụ t. Chúng tôi bắt đầu với việc khởi tạo ngẫu nhiên các trọng số mô hình có sẵn (dòng 1). Hai tập hợp rỗng được tạo ra: i) CMt được sử dụng để lưu trữ các mặt nạ ứng viên (dòng 2); ii) Θt được sử dụng để lưu trữ các trọng số đã che (dòng 3). Sau đó, chúng tôi tạo ra PS (kích thước quần thể) mặt nạ ứng viên (dòng 4-10). Đối với mỗi mặt nạ ứng viên được tạo ra (dòng 5), chúng tôi huấn luyện các trọng số đã che (dòng 6-7) với một số lượng epoch rất hạn chế. Sau đó, chúng tôi thêm mặt nạ ứng viên được tạo ra và các trọng số đã huấn luyện vào các tập hợp tương ứng (dòng 8-9). Trong bước tiếp theo, chúng tôi tính độ chính xác At cho tất cả các ứng viên được tạo ra (dòng 11) và độ thưa thớt có trọng số Υt (dòng 12). Chúng tôi xác định mô hình tốt nhất theo phương trình trong dòng 13, trong đó đánh giá đồng thời xem xét độ chính xác và độ thưa thớt: giữa hai ứng viên có độ chính xác tương tự nhưng giá trị độ thưa thớt khác nhau đáng kể, cái có mức độ thưa thớt cao nhất được ưu tiên, vì chúng tôi muốn bảo tồn càng nhiều trọng số càng tốt cho các nhiệm vụ tương lai. Sau bước này, chúng tôi thực hiện giai đoạn huấn luyện đầy đủ của các trọng số đã che Θ' trên nhiệm vụ hiện tại t.

3.2 Lượng tử hóa Thích ứng
Lượng tử hóa là một giai đoạn quan trọng trong thuật toán Ada-QPacknet. Mục đích của nó có hai mặt: i) nó cho phép chúng tôi gán một trọng số duy nhất cho nhiều nhiệm vụ; ii) nó giúp giảm việc sử dụng dung lượng của mô hình với một số lượng nhiệm vụ được chỉ định. Biểu diễn trọng số đơn 32-bit tiêu chuẩn được chia thành nhiều thành phần với số lượng bit giảm, và mỗi thành phần có thể được gán cho một nhiệm vụ khác nhau, như được hiển thị trong Hình 2. Ví dụ, một biểu diễn trọng số đơn 32-bit có thể được chia thành bốn thành phần 8-bit được gán cho bất kỳ nhiệm vụ nào. Tuy nhiên, bản chất thích ứng của quá trình này không phụ thuộc vào các thành phần có độ rộng bằng nhau, mà cho phép chúng tôi gán một số lượng bit khác nhau cho mỗi thành phần, đánh giá sự đánh đổi giữa độ rộng bit và hiệu suất mô hình. Ví dụ, một biểu diễn trọng số đơn 32-bit cũng có thể được chia thành 3 thành phần với các độ rộng bit sau: 8, 10, 14, v.v.

Thuật toán 2 trình bày chi tiết về quá trình lượng tử hóa thích ứng. Nó bắt đầu với lượng tử hóa phi tuyến của mô hình đã tỉa nhiệm vụ với các giá trị tối thiểu của các tham số ω và ψ (độ rộng bit) (dòng 1). Chúng tôi tận dụng thuật toán lượng tử hóa phi tuyến được chi tiết trong Thuật toán 3. Sau đó, chúng tôi kiểm tra độ chính xác mô hình (dòng 2). Tiếp theo, nếu sự sụt giảm độ chính xác nhiều hơn

--- TRANG 6 ---
Ada-QPacknet

N1,1
N2,1
N1,3N1,2
N2,2
w1,3,2w0,1,1
w0,1,3
w0,2,2
w0,2,3
0Nhiệm vụ 1
N0,1
N0,2w0,1,2
w1,2,2w1,2,1w1,1,2w1,1,1
w0,2,1
1         0.3456
2         0.0145
3        -0.35760         0.4576
1         0.1417
2        -0.1145
3        -0.29670         0.355511011
110110
00
Sách mã lớp 2 - nhiệm vụ 1Sách mã lớp 1 - nhiệm vụ 10011
w1,3,1
N1,1
N2,1
N1,3N1,2
N2,2
w1,3,2w0,1,1
w0,1,3
w0,2,2
w0,2,3
0Nhiệm vụ 1
N0,1
N0,2w0,1,2
w1,2,2w1,2,1w1,1,2w1,1,1
w0,2,1
w1,3,1
1         0.3456
2         0.0145
3        -0.35760         0.4576
1         0.1417
2        -0.1145
3        -0.29670         0.355511011
110110
00
Sách mã lớp 2 - nhiệm vụ 1Sách mã lớp 1 - nhiệm vụ 100111
11
0
00
10Nhiệm vụ 2
1         -0.18980         0.2453Sách mã lớp 1 - nhiệm vụ 2
1         0.10230         0.4521Sách mã lớp 2 - nhiệm vụ 2

Hình 2: Hình này trình bày trạng thái của mô hình sau khi nó được huấn luyện, tỉa và lượng tử hóa cho Nhiệm vụ 1 (trái) và Nhiệm vụ 2 (phải). Trong giai đoạn huấn luyện cho Nhiệm vụ 1, Tỉa Thích ứng giải phóng một tập con trọng số mô hình (đường nét đứt), có thể được sử dụng sau này để đáp ứng các nhiệm vụ mới. Sau đó, Lượng tử hóa Thích ứng xác định độ rộng bit tối ưu và lượng tử hóa biểu diễn của trọng số. Trong ví dụ này, biểu diễn trọng số 32-bit được lượng tử hóa thành 2 bit (ô vuông đỏ). Các giá trị được biểu diễn trên các bit này là mã từ sách mã theo lớp và có thể được dịch thành trọng số 32-bit đầy đủ. Khả năng này cho phép Ada-QPacknet chia mỗi trọng số thành nhiều thành phần, mỗi thành phần có thể được gán cho bất kỳ nhiệm vụ nào. Cùng một quá trình được lặp lại khi Nhiệm vụ 2 được trình bày: Tỉa Thích ứng lượng tử hóa biểu diễn trọng số 32-bit thành một bit đơn (ô vuông xanh).

Thuật toán 2 Lượng tử hóa Thích ứng cho nhiệm vụ t.
Yêu cầu: ψ – Độ rộng bit ban đầu
Yêu cầu: Qq – Độ chính xác mô hình
Yêu cầu: Θ – Mô hình đã tỉa
Yêu cầu: δ – Mất mát tối đa về độ chính xác
1: Θq, K = nonlinear_quantization(ψ, Θ) {xem Thuật toán 3}
2: Qq = acc(F, Θq)
3: while Qq < Q - δ do
4:   ψ = ψ + 1
5:   Θq, K = nonlinear_quantization(ψ, Θ)
6:   Qq = acc(F, Θq)
7: end while
8: return ψ – độ rộng bit tối ưu cho nhiệm vụ, Θq – mô hình đã lượng tử hóa, K – sách mã

ngưỡng δ cho trước (dòng 3) quá trình lặp tăng độ rộng bit được bắt đầu (dòng 3-7) cho đến khi sự sụt giảm hiệu suất được coi là có thể chấp nhận được.

Phương pháp lượng tử hóa phi tuyến được thiết kế trong công trình này (xem Thuật toán 3) được lấy cảm hứng từ [57]. Cụ thể, chúng tôi bắt đầu với việc xác định số lượng centroid ω và tạo ra một sách mã rỗng K sẽ lưu trữ, cho mỗi lớp, một ánh xạ giữa mã và giá trị trọng số (dòng 1-2). Chúng tôi cũng tạo ra một bản sao của mô hình Θ, trong đó trọng số sẽ chứa mã từ sách mã (chỉ số centroid) thay vì giá trị thực. Sau đó, đối với mỗi lớp i, chúng tôi phân cụm các giá trị trọng số từ lớp θi tận dụng thuật toán phân cụm K-Means với một số cụm được chỉ định ω (dòng 5). Bước này trả về hai đầu ra: một danh sách centroid C và danh sách gán trọng số-với-chỉ số-centroid A. Chúng tôi tiến hành bằng cách gán chỉ số centroid tương ứng cho mỗi trọng số (dòng 6-8). Để giữ ánh xạ giữa chỉ số centroid và centroid, chúng tôi xây dựng một sách mã cho mỗi lớp (dòng 9-11). Thuật toán trả về bản sao của mô hình với trọng số đã lượng tử hóa được lấy làm mã từ sách mã, cũng như sách mã cần thiết để giải mã mã thành giá trị thực của trọng số. Chúng tôi lưu ý rằng số lượng centroid liên quan trực tiếp đến độ rộng bit mong muốn (như được hiển thị trong dòng 1). Một số lượng centroid thấp ngụ ý rằng bộ nhớ cần thiết để lưu trữ trọng số được giảm. Mỗi trọng số được lưu trữ như một chỉ số của centroid gần nhất. Một biểu diễn trực quan của sách mã được hiển thị trong Hình 2.

4 Kết quả
4.1 Thiết lập thực nghiệm
Các thực nghiệm được chạy cho ba kịch bản CL: Permuted MNIST (p-MNIST) [12], split CIFAR100 (s-CIFAR100) [13], và 5 datasets [10], một kịch bản gia tăng nhiệm vụ bao gồm MNIST, SVHN, FashionMNIST, CIFAR10, not-

--- TRANG 7 ---
Ada-QPacknet

Thuật toán 3 Lượng tử hóa phi tuyến
Yêu cầu: ψ – độ rộng bit mong muốn
Yêu cầu: Θ – trọng số của một mô hình
1: ω ← 2^ψ {Số lượng centroid}
2: K ← [K0, K1, ..., K|Θ|]; Ki = {} {Sách mã rỗng}
3: Θ' ← bản sao của Θ
4: for θi in Θ do
5:   C, A ← KMeans(θi, ω) {C: centroids, A: gán trọng số-với-chỉ số-centroid}
6:   for θi,j ∈ θi do
7:     θ'i,j ← A[θi,j] {Sử dụng chỉ số centroid thay vì giá trị trọng số}
8:   end for
9:   for ck ∈ C do
10:    Ki[k] ← ck {Xây dựng sách mã theo lớp}
11:  end for
12: end for
13: return Θ', K

MNIST, TinyImagenet [4], và Imagenet100 [5]. Kịch bản p-MNIST bao gồm 10 nhiệm vụ với các pixel được hoán vị ngẫu nhiên của MNIST gốc (10 nhiệm vụ với 10 lớp mỗi nhiệm vụ). s-CIFAR100 được chia thành 10 nhiệm vụ với 10 lớp mỗi nhiệm vụ. Kịch bản 5 datasets là một chuỗi 5 nhiệm vụ với các dataset khác nhau, mỗi nhiệm vụ có 10 lớp. Kịch bản TinyImagenet bao gồm 40 nhiệm vụ với 5 lớp được lấy mẫu ngẫu nhiên mỗi nhiệm vụ. Cuối cùng, kịch bản Imagenet100 bao gồm 10 nhiệm vụ với 10 lớp được lấy mẫu ngẫu nhiên mỗi nhiệm vụ.

Trong các thực nghiệm của chúng tôi, kích thước quần thể tìm kiếm tỉa PS được đặt thành 16. Các giá trị cho các tham số VMIN và VMAX được đặt thành 0.45 và 0.85 cho tất cả các lớp. Các tham số α và β được đặt thành 0.9 và 0.1. Huấn luyện ngắn trong quá trình tìm kiếm mặt nạ được thực hiện với 5 epoch, trong khi huấn luyện đầy đủ cho mặt nạ được chọn sử dụng 50 epoch. Đối với Ada-QPacknet, tỷ lệ học được đặt bằng cách sử dụng bộ lập lịch động bắt đầu từ 0.01 và giảm theo thời gian với tối thiểu 0.0001 cho tất cả các dataset ngoại trừ TinyImagenet (bắt đầu từ 0.001) và Imagenet100 (tỷ lệ học không đổi 0.0001). Các mô hình được áp dụng là mạng neural hai lớp với các lớp kết nối đầy đủ (p-MNIST), AlexNet rút gọn (s-CIFAR100) [27], Resnet-18 (5 datasets và Imagenet), TinyNet (TinyImagenet) phù hợp với các xương sống mô hình được sử dụng trong bài báo WSN [21]. Đối với khởi tạo trọng số, chúng tôi áp dụng bộ khởi tạo Xavier. Các thực nghiệm được thực hiện trên một máy trạm được trang bị GPU NVIDIA A100. Các thực nghiệm của chúng tôi bao gồm 5 lần chạy hoàn chỉnh cho mỗi chiến lược. Do đó, tổng số lần thực hiện tương ứng với số lượng nhiệm vụ trong mỗi kịch bản nhân với 5.

Về các chỉ số, chúng tôi sử dụng định nghĩa tiêu chuẩn nhất của Độ chính xác suốt đời theo mô tả trong [34]. Hơn nữa, chúng tôi cũng tính dung lượng (tổng bộ nhớ được chiếm bởi mỗi nhiệm vụ) được lấy cảm hứng từ tính toán dung lượng trong [21], được định nghĩa như sau:

CAPt = ΣLi(1-Υi)·|θi|·b + |L|·2b·(32+b) + ΣLi(1-Υi)·|Mi| (10)

Dung lượng có thể được coi như tổng của ba thành phần. Thành phần đầu tiên mô tả số lượng trọng số sau khi tỉa nhân với độ rộng bit b. Thành phần thứ hai mô tả kích thước sách mã. Thành phần thứ ba bao gồm dung lượng của tất cả các mặt nạ.

4.2 So sánh với các phương pháp khác
Bảng 1 trình bày kết quả về độ chính xác trung bình. Trong phân tích thực nghiệm của chúng tôi, chúng tôi đã xem xét các đại diện từ tất cả các danh mục chiến lược CL. Cụ thể, chúng tôi đã xem xét Packnet và WSN (kiến trúc), Cumulative và Replay (luyện tập lại), SI (điều hòa), và CWRStar (kết hợp kiến trúc và điều hòa). Chiến lược Naive là một chiến lược đơn giản trong đó mô hình được tinh chỉnh khi các nhiệm vụ mới được trình bày, và có thể được coi như một giới hạn dưới. Chiến lược Cumulative, mặt khác, lưu trữ dữ liệu từ tất cả các nhiệm vụ đã quan sát cho đến nay để huấn luyện lại mô hình, và được coi như một giới hạn trên.

Kết quả nhấn mạnh rằng Ada-QPacknet vượt trội hơn tất cả các chiến lược được xem xét. Trong p-MNIST, Ada-QPacknet đạt 97.1% (0.50% so với Cumulative), phương pháp WSN (96.41%) chỉ đứng sau phương pháp Cumulative (96.45%). Packnet đạt 96.31%, trong khi các phương pháp khác kém hơn đáng kể so với các chiến lược kiến trúc (phương pháp tiếp theo trong bảng xếp hạng là Replay với 62.22%). Trong s-CIFAR100, Ada-QPacknet đạt 74.1% độ chính xác trung bình

--- TRANG 8 ---
Ada-QPacknet

Bảng 1: Kết quả so sánh về độ chính xác trung bình cho các chiến lược CL trong các kịch bản CL khác nhau: Permuted MNIST (p-MNIST), split-CIFAR100 (s-CIFAR100), và 5 datasets.

p-MNIST s-CIFAR100 5 datasets TinyImagenet
Naive 60.12 17.32 33.08 20.27
CWRStar 31.31 20.84 36.16 24.00
SI 57.32 19.54 29.42 20.51
Replay 62.22 19.60 55.24 23.14
CUMULATIVE 96.45 36.52 84.44 27.53
Packnet 96.31 72.57 92.59 55.46
WSN 96.41 76.38 93.41 71.96
Ada-QPacknet 97.1 74.1 94.1 71.9

Bảng 2: So sánh dung lượng cho các phương pháp dựa trên tỉa

Datasets p-MNIST s-CIFAR100 5 datasets TinyImagenet
Packnet 96.38% 81.0% 82.86% 188.67%
WSN 77.73% 99.13% 86.10% 48.65%
Ada-QPacknet 81.25% 78.6% 33.7% 112.5%

bình, đạt hiệu suất tốt thứ hai. Phương pháp hoạt động tốt nhất là WSN, với mức tăng hiệu suất 2.28%, mặc dù phương pháp này có mức sử dụng dung lượng cao hơn khi so sánh với Ada-QPacknet, mang lại kết quả tốt nhất về dung lượng (78.6%). Chiến lược thứ ba đạt kết quả thỏa đáng là Packnet (81.0%). Các chiến lược tiếp theo trong bảng xếp hạng là Cumulative và CWRStar, với độ chính xác thấp hơn đáng kể (36.52% và 20.84%). Trong kịch bản 5 datasets, Ada-QPacknet (94.1%) có hiệu suất tốt hơn WSN đứng thứ hai (93.41%) và Packnet đứng thứ ba (92.59%) với cải thiện đáng kể về dung lượng (33.7%) so với Packnet (82.86%) và WSN (86.10%). Chúng tôi cho rằng kết quả này do sự tương tự thấp giữa các nhiệm vụ do các dataset khác nhau, khiến chiến lược tái sử dụng trọng số được áp dụng trong Packnet và WSN ít hiệu quả hơn Ada-QPacknet. Phương pháp Cumulative khá gần với các phương pháp không quên này với hiệu suất 84.44%. Các chiến lược khác đạt kết quả tệ hơn đáng kể. Trong kịch bản TinyImagenet, WSN đạt điểm hiệu suất tốt nhất (71.96%), vượt trội một chút so với Ada-QPacknet (71.9%), xuất hiện là phương pháp hoạt động tốt thứ hai. Một giải thích có thể là học chuyển giao thông qua tái sử dụng trọng số được thực hiện bởi các chiến lược khác hiệu quả hơn trong kịch bản này do sự tương tự nhiệm vụ cao hơn khi so sánh với 5 datasets. Tất cả các chiến lược khác, như Packnet (55.46%), SI (20.51%), Replay (23.14%) tệ hơn đáng kể và có vẻ không hiệu quả trong kịch bản này.

Kết quả được trình bày trong Bảng 2 cho thấy so sánh dung lượng của mô hình trong ba chiến lược CL kiến trúc khác nhau. Trên p-MNIST, Packnet và WSN sử dụng 96.38% và 77.73% dung lượng mô hình gốc, tương ứng. Đồng thời, Ada-QPacknet đại diện cho mức trung gian, sử dụng 81.25% dung lượng trong khi đạt điểm độ chính xác hoạt động tốt nhất. Trong trường hợp s-CIFAR100, chúng tôi cũng quan sát thấy các chiến lược không quên là những chiến lược hoạt động tốt nhất. Trong trường hợp này, Ada-QPacknet (độ chính xác 74.1%, dung lượng 78.6%) đại diện cho phương pháp tốt nhất về dung lượng khi so sánh với Packnet (độ chính xác 72.57%, dung lượng 81.0%) và WSN (độ chính xác 76.38%, dung lượng 99.13%), điều này cho thấy việc sử dụng dung lượng cao hơn là điều kiện cần thiết để đạt độ chính xác cao hơn trên dataset này, do độ phức tạp của nó. Về kịch bản 5-datasets, Ada-QPacknet đạt kết quả tốt hơn WSN và Packnet, trong khi sử dụng dung lượng mô hình thấp hơn đáng kể (33.7% so với hơn 80% của cả Packnet và WSN). Cuối cùng, về TinyImagenet, Ada-QPacknet có kết quả tốt thứ hai về sử dụng dung lượng (112.5%), đặt mình ở giữa WSN (48.5%) và Packnet (188.67%), cái xuất hiện đặc biệt không hiệu quả.

Nhìn chung, kết quả của chúng tôi nhấn mạnh rằng Ada-QPacknet đạt kết quả cạnh tranh và tỷ lệ nén trong tất cả các kịch bản. Đáng chú ý là Ada-QPacknet vượt trội hơn đáng kể các phương pháp khác trên kịch bản 5-datasets phức tạp nhất được xây dựng trên các dataset không đồng nhất. Một lý do tại sao Packnet và WSN có thể đạt kết quả tương tự cho p-MNIST và s-CIFAR100 có thể là khả năng chia sẻ trọng số giữa các nhiệm vụ, trong khi cả hai kịch bản nói trên đều có các nhiệm vụ đồng nhất xuất phát từ một dataset duy nhất. Do đó, chia sẻ trọng số cung cấp lợi thế lớn cho WSN và Packnet.

Bảng 3: So sánh với các phương pháp kiến trúc và replay SOTA trên Imagenet100 (10/10).

Methods Parameters Accuracy
FOSTER - 74.49
DyTox 10.73M 75.54
DER 112.27M 75.36
Ada-QPacknet 11.5M 72.26

--- TRANG 9 ---
Ada-QPacknet

Bảng 4: Dung lượng của mỗi thành phần và cả hai kết hợp (Ada-QPacknet), trung bình cho mỗi nhiệm vụ

Datasets p-MNIST s-CIFAR100 5 datasets
pruning 45.0% 50.0% 52.0%
quantization 12.5% 15.25% 12.5%
Ada-QPacknet 8.12% 7.7% 9.9%

Bảng 5: Độ chính xác của mỗi thành phần và cả hai kết hợp (Ada-QPacknet)

Datasets p-MNIST s-CIFAR100 5 datasets
pruning 97.1 75.3 94.4
quantization 98.5 75.8 95.2
Ada-QPacknet 97.1 74.1 94.1

Kết quả bổ sung về kịch bản Imagenet100 với các phương pháp lai được trình bày trong Bảng 3. Kết quả nhấn mạnh rằng Ada-QPacknet (72.26%) không thể vượt trội hơn các phương pháp kiến trúc được mở rộng với bộ nhớ dựa trên replay, như FOSTER (74.49%), DyTox (75.54%), và DER (75.36%). Hiện tượng này được mong đợi do độ phức tạp vốn có cao hơn của các phương pháp lai xuất phát từ thành phần bộ nhớ. Tuy nhiên, hiệu suất Ada-QPacknet vẫn tương đối gần với các phương pháp lai, với chiếm dụng dung lượng rất thấp (11.5M), có thể so sánh với DyTox (10.73M) và tốt hơn đáng kể so với DER (112.27M)³. Chúng tôi cũng lưu ý rằng các thực nghiệm này được chạy mà không thực hiện tối ưu hóa mở rộng cho các siêu tham số Ada-QPacknet (ví dụ, chúng tôi đã sử dụng cấu hình tỷ lệ học cố định) do yêu cầu rất tốn thời gian của kịch bản Imagenet100. Vì lý do này, chúng tôi mong đợi rằng một tập hợp siêu tham số tốt hơn có thể được xác định cho phương pháp được đề xuất của chúng tôi, điều này sẽ đặt độ chính xác của nó gần hơn với các phương pháp lai.

4.3 Nghiên cứu cắt bỏ
Chúng tôi tiến hành các thực nghiệm để xác minh rằng tất cả các giai đoạn của Ada-QPacknet đóng góp vào khả năng mô hình cuối cùng. Nghiên cứu cắt bỏ của chúng tôi có hai mặt và nhằm đánh giá: i) dung lượng mô hình (xem Bảng 4), và ii) độ chính xác mô hình (xem Bảng 5) khi một thành phần cụ thể của Ada-QPacknet được kích hoạt.

Tập trung vào yếu tố quan trọng nhất - dung lượng, kết quả trong Bảng 4 cho thấy rằng, trung bình, tỉa cho phép giải phóng một lượng đáng kể dung lượng mô hình (45.0% cho p-MNIST, 50.0% cho s-CIFAR100, 52% cho 5-datasets). Mặt khác, lượng tử hóa giải phóng, trung bình, một lượng dung lượng cao hơn (86.58%) cho cả ba kịch bản (p-MNIST, s-CIFAR100, và 5-datasets). Hành vi này được mong đợi vì lượng tử hóa có tiềm năng xác định một số lượng bit thấp giữ lại hầu hết độ chính xác mô hình, đây là công việc tinh vi hơn (xem xét rằng một số bit nhất định có thể được giải phóng từ mỗi trọng số) khi so sánh với tỉa thích ứng (chỉ có thể loại bỏ trọng số hoàn toàn). Khi kết hợp, hai giai đoạn mang lại giảm dung lượng đáng chú ý cho mỗi nhiệm vụ (91.88% cho p-MNIST, 92.30% cho s-CIFAR100, 90.10% cho 5-datasets).

Phân tích kết quả từ góc độ độ chính xác mô hình cho phép chúng tôi hiểu sự đánh đổi giữa dung lượng mô hình (được giải phóng bởi tỉa thích ứng và lượng tử hóa) và độ chính xác mô hình tương ứng. Kết quả trong Bảng 5 cho thấy lượng tử hóa mang lại độ chính xác cao hơn tỉa. Rõ ràng là việc loại bỏ một số trọng số có thể có tác động nhiều hơn việc giảm độ rộng bit của chúng theo cách thích ứng và phi tuyến. Ví dụ, đối với p-MNIST, mô hình với tỉa thích ứng mang lại độ chính xác 97.1%, và lượng tử hóa mang lại độ chính xác 98.5%. Các dataset khác tuân theo mẫu tương tự với lượng tử hóa mang lại 75.8% và 95.2% độ chính xác cho s-CIFAR100 và 5-datasets tương ứng, trong khi tỉa mang lại độ chính xác 75.3% và 94.4%. Về Ada-QPacknet cuối cùng, độ chính xác bằng hoặc thấp hơn so với một thành phần đơn lẻ: 97.1%, phù hợp với hiệu suất của tỉa, thấp hơn 98.5% (lượng tử hóa) cho p-MNIST, trong khi nó giảm từ 75.3% (tỉa) và 75.8% (lượng tử hóa) xuống 74.1% cho s-CIFAR100, từ 94.4% (tỉa) và 95.2% (lượng tử hóa) xuống 94.1% cho 5-datasets. Tuy nhiên, một mức độ giảm được mong đợi vì mô hình đang được đơn giản hóa bởi tỉa thích ứng và lượng tử hóa, và thật an tâm khi thấy rằng sự sụt giảm hiệu suất không đáng kể.

Nhìn chung, chúng tôi kết luận rằng cả tỉa thích ứng và lượng tử hóa đều đóng góp vào việc giảm dung lượng mô hình, cho phép mô hình đáp ứng các nhiệm vụ tương lai mà không ảnh hưởng đáng kể đến độ chính xác mô hình. Chúng tôi lưu ý rằng kết quả cho phương pháp của chúng tôi mà không có tỉa và lượng tử hóa giảm dung lượng bị thiếu, vì việc diễn giải chúng sẽ cồng kềnh. Về mặt khái niệm, Ada-QPacknet mà không có tỉa và lượng tử hóa có thể được tái công thức theo hai cách: i) hành vi tương tự như Naive, trong đó cùng một mô hình được huấn luyện lại đầy đủ khi một nhiệm vụ mới được quan sát, dẫn đến quên; ii) là một mô hình không quên, tuy nhiên, sẽ bão hòa hoàn toàn dung lượng của nó sau nhiệm vụ đầu tiên, làm mất hiệu lực khả năng học suốt đời.

Ngoài nghiên cứu cắt bỏ của chúng tôi, chúng tôi báo cáo một số mẫu xuất hiện trong các thực nghiệm của chúng tôi. Đối với hầu hết các nhiệm vụ kịch bản, sự sụt giảm hiệu suất quan sát thấy với lượng tử hóa 4-bit ít hơn 1% cho tất cả các nhiệm vụ (so với trọng số 32-bit đầy đủ). Ví dụ, trong p-MNIST, sự sụt giảm hiệu suất -0.5%, -1.9%, và -3.0% được quan sát thấy với 3, và 2 bit, tương ứng. Trong 5 datasets, các nhiệm vụ đã cho thấy mức độ phức tạp khác nhau từ thấp (3 bit được sử dụng cho p-MNIST, notMNIST, và s-CIFAR100), trung bình (4 bit cho Fashion MNIST), và cao (5 bit cho SVHN). Đối với Imagenet100, cấu hình ít nhất mang lại sự sụt giảm độ chính xác không đáng kể là 5 bit. Cần lưu ý rằng đối với TinyImagenet, dung lượng tổng thể được giảm bằng cách phân bổ 5 mạng con riêng biệt, mỗi mạng có 8 nhiệm vụ. Lựa chọn này cho phép chúng tôi giảm thiểu số lượng bit cho mỗi mặt nạ, nếu không sẽ là 40 bit do 40 nhiệm vụ được bao gồm trong dataset này.

Từ góc độ độ phức tạp tính toán, chúng tôi quan sát thấy rằng thời gian huấn luyện trong Ada-QPacknet bị chi phối bởi tìm kiếm vé số, trong khi lượng tử hóa thêm chi phí không đáng kể vào thời gian thực hiện vì nó được thực hiện sau huấn luyện. So sánh phương pháp của chúng tôi với WSN và PackNet, chúng tôi quan sát thấy một overhead khoảng 50% về thời gian thực hiện tổng thể. Kết quả này cho thấy rằng kết quả tích cực thu được về độ chính xác mô hình và dung lượng cho Ada-QPacknet được đạt với thời gian thực hiện hợp lý về cơ bản nằm trong cùng lớp độ phức tạp với các đối thủ trực tiếp của nó.

5 Kết luận và công việc tương lai
Trong bài báo này, chúng tôi đã đề xuất Ada-QPacknet, một phương pháp kiến trúc không quên mới cho học liên tục sử dụng tỉa thích ứng và lượng tử hóa thích ứng. Phương pháp tỉa thích ứng cho phép phương pháp xác định tập con trọng số gần tối ưu bảo toàn hiệu suất của mô hình trên các nhiệm vụ đơn lẻ trong khi giảm thiểu dung lượng cần thiết để giải quyết mỗi nhiệm vụ. Hơn nữa, chiến lược lượng tử hóa thích ứng được áp dụng trong phương pháp của chúng tôi cho phép sử dụng một trọng số duy nhất cho nhiều hơn một nhiệm vụ mà không có sự sụt giảm đáng kể về hiệu suất. Kết quả được trình bày cho thấy phương pháp được đề xuất của chúng tôi đạt kết quả cạnh tranh về độ chính xác và sử dụng dung lượng mô hình so với nhiều chiến lược học liên tục, bao gồm các phương pháp không quên phổ biến. Trong công việc tương lai, chúng tôi sẽ tập trung vào thiết kế khả năng mô hình hóa mạnh mẽ hơn để xử lý tốt hơn các kịch bản phức tạp (như Imagenet100) nơi Ada-QPacknet đạt hiệu suất không tối ưu. Cụ thể, chúng tôi sẽ tập trung vào ước tính độ nhạy lớp và tương quan giữa phân phối nhiệm vụ để ước tính ranh giới mức độ thưa thớt cho các nhiệm vụ mới. Hơn nữa, chúng tôi sẽ điều tra việc áp dụng chia sẻ trọng số, để đạt tỷ lệ nén cao hơn cho mỗi nhiệm vụ, cũng như lượng tử hóa lai. Cuối cùng, chúng tôi sẽ thiết kế một cơ chế để chọn mặt nạ phù hợp nhất trong số những mặt nạ có sẵn được phân bổ cho các nhiệm vụ đã học trước đó khi dung lượng mô hình đầy đủ được đạt tới.

Lời cảm ơn
Nghiên cứu này được hỗ trợ một phần bởi Cơ sở hạ tầng PLGrid.

Tài liệu tham khảo
[1]Joao Rafael, Ivo Correia, Alcides Fonseca, và Bruno Cabral, Dependency-Based Automatic Parallelization of Java Applications, Tháng 8 2014, DOI: 10.1007/978-3-319-14313-2_16

[2]Kamil Faber và Roberto Corizzo và Bartlomiej Sniezynski và Nathalie Japkowicz, VLAD: Task-agnostic VAE-based lifelong anomaly detection, Neural Networks, vol. 165, pp. 248-273, 2023

[3]Zhou, Da-Wei và Wang, Qi-Wei và Qi, Zhi-Hong và Ye, Han-Jia và Zhan, De-Chuan và Liu, Ziwei, Deep class-incremental learning: A survey, arXiv preprint arXiv:2302.03648, 2023

[4] Le, Ya và Yang, Xuan, Tiny imagenet visual recognition challenge

[5]Russakovsky, Olga và Deng, Jia và Su, Hao và Krause, Jonathan và Satheesh, Sanjeev và Ma, Sean và Huang, Zhiheng và Karpathy, Andrej và Khosla, Aditya và Bernstein, Michael, Imagenet large scale visual recognition challenge, International journal of computer vision, vol. 115, pp. 211–252, 2015, Springer

[6]Parisi, German I và Kemker, Ronald và Part, Jose L và Kanan, Christopher và Wermter, Stefan, Continual lifelong learning with neural networks: A review, Neural networks, vol. 113, pp. 54-71, 2019, Elsevier

[7]Yan, S. và Xie, J. và He, X., DER: Dynamically Expandable Representation for Class Incremental Learning, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021

[8]Douillard, A. và Rame, A. và Couairon, G. và Cord, M., DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022

--- TRANG 11 ---
Ada-QPacknet

[9]Wang, F.-Y. và Zhou, D.-W. và Ye, H.-J. và Zha, D.-C., FOSTER: Feature Boosting and Compression for Class-Incremental Learning, European Conference on Computer Vision (ECCV), 2022

[10] Ebrahimi, Sayna và Meier, Franziska và Calandra, Roberto và Darrell, Trevor và Rohrbach, Marcus, Adversarial continual learning, European Conference on Computer Vision, pp. 386-402, 2020, Springer

[11] Kour, George và Saabne, Raid, Real-time segmentation of on-line handwritten arabic script, Frontiers in Handwriting Recognition (ICFHR), pp. 417-422, 2014, IEEE

[12] Yann Le Cun, The MNIST database of handwritten digits, http://yann. lecun. com/exdb/mnist/

[13] Alex Krizhevsky, Learning Multiple Layers of Features from Tiny Images, 2009

[14] Hadash, Guy và Kermany, Einat và Carmeli, Boaz và Lavi, Ofer và Kour, George và Jacovi, Alon, Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications, arXiv preprint arXiv:1804.09028, 2018

[15] Bengio, Yoshua và LeCun, Yann, Scaling Learning Algorithms Towards AI, Large Scale Kernel Machines, MIT Press, 2007

[16] Hinton, Geoffrey E. và Osindero, Simon và Teh, Yee Whye, A Fast Learning Algorithm for Deep Belief Nets, Neural Computation, pp.1527-1554, vol. 18, 2006

[17] Bengio, Yoshua và Louradour, Jérôme và Collobert, Ronan và Weston, Jason, Curriculum learning, Proceedings of the 26th annual international conference on machine learning, pp.41-48, 2009

[18] Wortsman, Mitchell và Ramanujan, Vivek và Liu, Rosanne và Kembhavi, Aniruddha và Rastegari, Mohammad và Yosinski, Jason và Farhadi, Ali, Supermasks in Superposition, arXiv, https://arxiv.org/abs/2006.14769, 2020

[19] David Lopez-Paz, Marc'Aurelio Ranzato, journal = arXiv, Gradient Episodic Memory for Continual Learning, arXiv, https://arxiv.org/abs/1706.08840, 2017

[20] author = Kirkpatrick, James và Pascanu, Razvan và Rabinowitz, Neil và Veness, Joel và Desjardins, Guillaume và Rusu, Andrei và Milan, Kieran và Quan, John và Ramalho, Tiago và Grabska-Barwinska, Agnieszka và Hassabis, Demis và Clopath, Claudia và Kumaran, Dharshan và Hadsell, Raia, Overcoming catastrophic forgetting in neural networks, arXiv, https://arxiv.org/abs/1612.00796, 2016

[21] Kang, Haeyong và Mina, Rusty J. L. và Rizky, Sultan và Madjid, Hikmawan và Yoon, Jaehong và Hasegawa-Johnson, Mark và Ju-Hwang, Sung và Yoo, Chang D., Forget-free Continual Learning with Winning Subnetworks, ICML, 2022

[22] Zenke, Friedemann và Poole, Ben và Ganguli, Surya, Continual Learning Through Synaptic Intelligence, arXiv, https://arxiv.org/abs/1703.04200, 2017

[23] Sylvestre-Alvise, Rebuffi và Kolesnikov, Alexander và Lampert, Christoph H., iCaRL: Incremental classifier and representation learning, arXiv, arXiv:1611.07725,2016

[24] Masse, Nicolas Y. và Grant, Gregory D. và Freedman David J., Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization, Salk Institute for Biological Studies, 2018

[25] Chaudhry, Arslan và Ranzato, Marc'Aurelio và Rohrbach, Marcus và Elhoseiny, Mohamed, Efficient Lifelong Learning with A-GEM, arXiv:1812.00420, arXiv, 2019

[26] Ameya Prabhu, Philip H. S. Torr và Puneet K. Dokania, GDumb: A Simple Approach that Questions Our Progress in Continual Learning, Lecture Notes in Computer Science (LNIP), vol. 12347, 2020

[27] Saha, Gobinda và Garg, Isha và Roy, Kaushik, Gradient Projection Memory for Continual Learning, International Conference on Learning Representations, 2020

[28] Li, Zhizhong và Hoiem, Derek, Learning without forgetting, IEEE transactions on pattern analysis and machine intelligence, vol. 40, pp. 2935-2947, 2017

[29] M. van de Ven, Gido và Siegelmann, Hava T. và Tolias, Andreas S., Brain-inspired replay for continual learning with artificial neural networks, Nature Communications, 2020

[30] Rusu, Andrei A. và Rabinowitz, Neil C. và Desjardins, Guillaume và Soyer, Hubert và Kirkpatrick, James và Kavukcuoglu, Koray và Pascanu, Razvan và Hadsell, Raia, Progressive Neural Networks, arXiv, https://arxiv.org/abs/1606.04671, 2016

[31] Lomonaco, Vincenzo và Maltoni, Davide và Pellegrini, Lorenzo, Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches, 1st Workshop on Continual Learning in Computer Vision at CVPR2020, https://arxiv.org/abs/1907.03799, 2019

--- TRANG 12 ---
Ada-QPacknet

[32] Mallya, Arun và Lazebnik, Svetlana, PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning, arXiv, https://arxiv.org/abs/1711.05769, 2017

[33] Kirkpatrick, James và Pascanu, Razvan và Rabinowitz, Neil và Veness, Joel và Desjardins, Guillaume và Rusu, Andrei A và Milan, Kieran và Quan, John và Ramalho, Tiago và Grabska-Barwinska, Agnieszka, Overcoming catastrophic forgetting in neural networks, Proceedings of the national academy of sciences, vol. 114, pp. 3521-3526, 2017

[34] Díaz-Rodríguez, Natalia và Lomonaco, Vincenzo và Filliat, David và Maltoni, Davide, Don't forget, there is more than forgetting: new metrics for Continual Learning, arXiv preprint arXiv:1810.13166, 2018

[35] Cossu, Andrea và Graffieti, Gabriele và Pellegrini, Lorenzo và Maltoni, Davide và Bacciu, Davide và Carta, Antonio và Lomonaco, Vincenzo, Is Class-Incremental Enough for Continual Learning?, Frontiers in Artificial Intelligence, vol.5, 2022, Frontiers Media SA

[36] Lomonaco, Vincenzo và Maltoni, Davide, CORe50: a New Dataset and Benchmark for Continuous Object Recognition, Proceedings of the 1st Annual Conference on Robot Learning, pp. 17-26, vol. 78, 2017, Proceedings of Machine Learning Research

[37] Aljundi, Rahaf và Babiloni, Francesca và Elhoseiny, Mohamed và Rohrbach, Marcus và Tuytelaars, Tinne, Memory Aware Synapses: Learning What (not) to Forget, Computer Vision – ECCV 2018, Springer International Publishing, pp. 144-161

[38] Matthias De Lange và Rahaf Aljundi và Marc Masana và Sarah Parisot và Xu Jia và Aleks Leonardis và Gregory G. Slabaugh và Tinne Tuytelaars, A Continual Learning Survey: Defying Forgetting in Classification Tasks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, vol. 44, pp.3366-3385

[39] Aketi, S.A. và Roy, S. và Raghunathan, A. và Roy, K., Gradual channel pruning while training using feature relevance scores for convolutional neural networks, pp. 171924–171932, IEEE Access 8, 2020

[40] Van de Ven, Gido M và Tolias, Andreas S, Three scenarios for continual learning, arXiv preprint arXiv:1904.07734, 2019

[41] Anwar, S. và Hwang, K. và Sung, W., Fixed point optimization of deep convolutional neural networks for object recognition, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1131-1135, 2015

[42] Gysel, P. và Motamedi, M. và Ghiasi, S., Hardware-oriented approximation of convolutional neural networks, arXiv:1604.03168, 2016

[43] Park, E. và Ahn, J. và Yoo, S., Weighted-Entropy-Based Quantization for Deep Neural Networks, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017 July

[44] Zhang, D. và Yang, J. và Ye, D. và Hua, G., Learned quantization for highly accurate and compact deep neural networks, arXiv:1807.10029, 2018

[45] Jung, S. và Son, C. và Lee, S. và Son, J. và Kwak, Y. và Han, J.J. và Choi, C, Joint training of low-precision neural network with quantization interval parameters, arXiv:1808.05779, 2018

[46] McDonnell, M. D., Training wide residual networks for deployment using a single bit for each weight, ICLR, 2018

[47] Mishra, A. và Nurvitadhi, E., WRPN: Wide Reduced-Precision Networks, ICLR, 2018

[48] Markus Nagel và Rana A. Amjad và Mart van Baalen và Christos Louizos và Tijmen Blankevoort. Up or Down? Adaptive Rounding for Post-Training Quantization, Proceedings of ICML, 2020

[49] S. Mao và W.J. Dally, Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016

[50] A. Ren và T. Zhang và S. Ye và J. Li và W. Xu và X. Qian và X. Lin và Y. Wang, ADMM-NN: an algorithm-hardware co-design framework of dnns using alternating direction methods of multipliers, Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2019, pp. 925–938, April 13-17,

[51] K. Xu và D. Zhang, J. An và L. Liu và L. Liu và D. Wang, GenExp: Multi-objective pruning for deep neural network based on genetic algorithm, Neurocomputing, April, 2021 DOI: 10.1016/j.neucom.2021.04.022

[52] S. Han và W.Dally, Bandwidth-efficient deep learning, DAC, 2018

[53] A. Renda và J. Frankle và M. Carbin, Comparing fine-tuning and rewinding in neural network pruning, ICLR, 2020

--- TRANG 13 ---
Ada-QPacknet

[54] Faber, Kamil và Corizzo, Roberto và Sniezynski, Bartlomiej và Japkowicz, Nathalie, Active Lifelong Anomaly Detection with Experience Replay, IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA), pp. 1-10, 2022, IEEE

[55] Megan M. Baker và Alexander New và Mario Aguilar-Simon và Ziad Al-Halah và Sébastien M.R. Arnold và Ese Ben-Iwhiwhu và Andrew P. Brna và Ethan Brooks và Ryan C. Brown và Zachary Daniels và Anurag Daram và Fabien Delattre và Ryan Dellana và Eric Eaton và Haotian Fu và Kristen Grauman và Jesse Hostetler và Shariq Iqbal và Cassandra Kent và Nicholas Ketz và Soheil Kolouri và George Konidaris và Dhireesha Kudithipudi và Erik Learned-Miller và Seungwon Lee và Michael L. Littman và Sandeep Madireddy và Jorge A. Mendez và Eric Q. Nguyen và Christine Piatko và Praveen K. Pilly và Aswin Raghavan và Abrar Rahman và Santhosh Kumar Ramakrishnan và Neale Ratzlaff và Andrea Soltoggio và Peter Stone và Indranil Sur và Zhipeng Tang và Saket Tiwari và Kyle Vedder và Felix Wang và Zifan Xu và Angel Yanguas-Gil và Harel Yedidsion và Shangqun Yu và Gautam K. Vallabha, A domain-agnostic approach for characterization of lifelong learning systems, Neural Networks, vol. 160, pp. 274-296, 2023

[56] D. Lin và S. Talathi và V. Annapureddy, Fixed point quantization of deep convolutional networks, vol. 48, pp. 2849-2858, ICLR, 2016

[57] Pietron, Marcin và Karwatowski, Michal và Wielgosz, Maciej và Duda, Jerzy, Fast Compression and Optimization of Deep Learning Models for Natural Language Processing, CANDAR Workshops 2019, pp. 162-168, 2019

[58] Han, Song và Pool, Jeff và Tran, John và Dally, William, Learning both weights and connections for efficient neural network, Advances in neural information processing systems, pp. 1135-1143, 2015

[59] Gysel, Philipp, Ristretto: Hardware-oriented approximation of convolutional neural networks, arXiv preprint arXiv:1605.06402, 2016

[60] title=Pruning Filters while Training for Efficiently Optimizing Deep Learning Networks, author=Roy, Sourjya và Panda, Priyadarshini và Srinivasan, Gopalakrishnan và Raghunathan, Anand, journal=arXiv preprint arXiv:2003.02800, year=2020

[61] Zhao, Chenglong và Ni, Bingbing và Jian, Zhang và Zhao, Qiwei và Zhang, Wenjun và Tian, Qi, Variational Convolutional Neural Network Pruning, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), DOI: 10.1109/CVPR.2019.00289, 2019

[62] Krzysztof Wróbel và Michał Karwatowski và Maciej Wielgosz và Marcin Pietroń và Kazimierz Wiatr, Compression of Convolutional Neural Network for Natural Language Processing, Computer Science, vol. 21, 2020, doi: 10.7494/csci.2020.21.1.337

[63] Jongsoo Park. và Sheng Li và Wei Wen và Ping Tak Peter Tang và Hai Li và Yiran Chen và Pradeep Dubey, Faster CNNs with Direct Sparse Convolutions and Guided Pruning, 1608.01409, arXiv, 2016

[64] Marcin Pietron và Maciej Wielgosz, Retrain or not retrain? – efficient pruning methods of deep CNN networks, 2002.07051, arXiv, 2020

[65] M.Pietron và M.Karwatowski và M.Wielgosz và J.Duda, Fast Compression and Optimization of Deep Learning Models for Natural Language Processing, CANDAR Workshops, pp. 162-168, 2019

[66] J. Frankle và G.K. Dziugaite và D.M. Roy và M. Carbin, The Lottery Ticket Hypothesis at Scale, 1903.01611, arXiv, March 2019

[67] V. Sanh và T. Wolf và A. M. Rush, Movement Pruning: Adaptive Sparsity by Fine-Tuning, 2005.07683, arXiv, May 2020

[68] M. Al-Hami và M. Pietron và R. Casas và M. Wielgosz, Methodologies of Compressing a Stable Performance Convolutional Neural Networks in Image Classification, pp. 1-23, Neural Processing Letters, January 2020

[69] CIFAR dataset, https://paperswithcode.com/sota/image-classification-on-cifar-100, 2020

[70] Krizhevsky, Alex và Sutskever, Ilya và Hinton, Geoffrey E., ImageNet Classification with Deep Convolutional Neural Networks, vol. 60, pp. 84–90, June 2017, Association for Computing Machinery

[71] K. He và X. Zhang và S. Ren và J. Sun, Deep Residual Learning for Image Recognition, pp. 770-778, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016

[72] Simonyan, Karen và Zisserman, Andrew, Very Deep Convolutional Networks for Large-Scale Image Recognition, arXiv 1409.1556, 2014

[73] Li, H. và Kadav, A. và Durdanovic, I. và Samet, H. và Graf, H., Pruning filters for efficient convnets, arXiv preprint arXiv:1608.08710, 2016

--- TRANG 14 ---
Ada-QPacknet

[74] Liu, C. và Wu, H., Channel pruning based on mean gradient for accelerating convolutional neural networks, Signal Processing, pp. 84–91, 2019

[75] Zhao, C. và Ni, B. và Zhang, J. và Zhao, Q. và Zhang, W. và Tian, Q., Variational convolutional neural network pruning, in: Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2780–2789, 2019

[76] Wang, W. và Zhu, L. và Guo, B., Reliable identification of redundant kernels for convolutional neural network compression, Journal of Visual Communication and Image Representation, 2019

[77] Hu, Y. và Sun, S. và Li, S. và Wang, X. và Gu, Q., A novel channel pruning method for deep neural network compression, arXiv:1805.11394, 2018

[78] Chen, S. và Zhao, Q., Shallowing deep networks: Layer-wise pruning based on feature representations, IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (12), pp. 3048–3056, 2018

[79] Molchanov, D. và Ashukha, A. và Vetrov, D., Variational dropout sparsifies deep neural networks, in: International Conference on Machine Learning, pp. 2498–2507, ICML, 2017

[80] Zhuang, Z. và Tan, M. và Zhuang, B. và Liu, J. và Guo, Y. và Wu, Q. và Huang, J. và Zhu, J., Discrimination-aware channel pruning for deep neural networks, pp. 875–886, Advances in Neural Information Processing Systems (NeurIPs), 2018

[81] Huang, Q. và Zhou, K. và You, S. và Neumann, U., Learning to prune filters in convolutional neural networks, pp. 709–718, IEEE Winter Conference on Applications of Computer Vision (WACV), 2018
