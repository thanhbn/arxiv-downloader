# 2310.01382.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2310.01382.pdf
# Kích thước tệp: 1331635 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
NÉN LLM: SỰ THẬT HIẾM KHI THUẦN KHIẾT
VÀ KHÔNG BAO GIỜ ĐƠN GIẢN
Ajay Jaiswal1, Zhe Gan2, Xianzhi Du2, Bowen Zhang2, Zhangyang Wang1, Yinfei Yang2
1Đại học Texas tại Austin,2Apple
TÓM TẮT
Bất chấp những thành tựu đáng chú ý, các Mô hình Ngôn ngữ Lớn (LLM) hiện đại
phải đối mặt với chi phí tính toán và bộ nhớ cực lớn. Gần đây, một số nghiên cứu
đã cho thấy thành công đáng kể trong việc nén (cắt tỉa
và lượng tử hóa) LLM không cần đào tạo và không cần dữ liệu đạt được 50 - 60% độ thưa thớt và giảm
độ rộng bit xuống 3 hoặc 4 bit mỗi trọng số, với sự suy giảm độ phức tạp
không đáng kể so với đường cơ sở không nén. Khi các nỗ lực nghiên cứu gần đây tập trung vào phát
triển các phương pháp nén ngày càng tinh vi, công trình của chúng tôi lùi lại một bước và đánh giá lại
hiệu quả của các phương pháp nén SoTA hiện có,
vốn dựa trên một chỉ số khá đơn giản và rộng rãi bị nghi ngờ, độ phức tạp (thậm chí
đối với LLM dày đặc). Chúng tôi giới thiệu Bộ đánh giá LLM Nén Tập trung Kiến thức
(LLM-KICK), một tập hợp các nhiệm vụ được tuyển chọn cẩn thận để định nghĩa lại giao thức đánh
giá cho LLM nén, có sự phù hợp đáng kể với
các đối tác dày đặc của chúng và độ phức tạp không thể nắm bắt được những thay đổi tinh tế trong khả năng
thực sự của chúng. LLM-KICK tiết lộ nhiều ưu điểm thuận lợi và tình trạng đáng tiếc
của các phương pháp nén SoTA hiện tại: tất cả các phương pháp cắt tỉa đều chịu sự suy giảm hiệu suất đáng kể, đôi khi ở tỷ lệ thưa thớt tầm thường (ví dụ: 25-
30%), và thất bại đối với độ thưa thớt N:M trong các nhiệm vụ tập trung kiến thức; các phương pháp lượng
tử hóa hiện tại thành công hơn so với cắt tỉa; tuy nhiên, LLM được cắt tỉa ngay cả ở
≥50% độ thưa thớt vẫn là hệ thống truy xuất và tóm tắt trong ngữ cảnh mạnh mẽ; cùng với
những điều khác. LLM-KICK được thiết kế để đánh giá toàn diện khả năng của LLM nén đối
với hiểu biết ngôn ngữ, lý luận, sinh tạo, truy xuất trong ngữ cảnh, tóm tắt trong
ngữ cảnh, v.v. Chúng tôi hy vọng nghiên cứu của mình có thể thúc đẩy sự phát triển
của các phương pháp nén LLM tốt hơn. Mã nguồn tái tạo có sẵn tại
https://github.com/VITA-Group/llm-kick .

1 GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) có mặt khắp nơi, ảnh hưởng sâu sắc không chỉ đến bối cảnh
của NLP (Ram et al., 2023; Liu et al., 2023a; Sawada et al., 2023; Qin et al., 2023; Zhuo, 2023;
Lee et al., 2023), mà gần đây còn hỗ trợ nhiều thuật toán thị giác máy tính (Lian et al., 2023; Wang
et al., 2023; Lai et al., 2023; Lu et al., 2023) và mạng nơ-ron đồ thị (Ye et al., 2023; Chen
et al., 2023; Qian et al., 2023; Duan et al., 2023); đạt được hiệu suất xuất sắc trên
nhiều bảng xếp hạng nhiệm vụ khác nhau. Bất chấp nhiều khả năng chưa từng có, việc dân chủ hóa
chúng chủ yếu bị hạn chế bởi sự hiện diện của hàng tỷ tham số, phụ thuộc vào yêu cầu tính toán
và bộ nhớ đáng kinh ngạc. Ví dụ, GPT-175B yêu cầu 325 GB bộ nhớ GPU
chỉ để tải trọng số mô hình, và ít nhất năm GPU A100 (80GB) với các kỹ thuật song song tinh vi (Sheng et al., 2023).

Để dân chủ hóa LLM, những nỗ lực đáng kể đã được thực hiện để giảm thiểu chi phí tính toán cao
của chúng, chủ yếu chia thành hai hướng nghiên cứu: cắt tỉa mạng, và lượng tử hóa trọng số. Hướng
trước thu nhỏ kích thước mạng bằng cách loại bỏ các trọng số cụ thể khỏi mô hình – về cơ bản đặt
chúng bằng không, trong khi hướng sau nhằm lượng tử hóa các tham số thành các biểu diễn mức bit thấp hơn. Một số
thành công gần đây trong cắt tỉa mạng (Sun et al., 2023; Frantar & Alistarh, 2023; Jaiswal et al.,
2023a; Ma et al., 2023; Ji et al., 2023) và lượng tử hóa (Liu et al., 2023c; Kim et al., 2023; Dettmers
et al., 2023a; Frantar et al., 2022; Lin et al., 2023a; Dettmers et al., 2023c) (thảo luận công trình liên quan chi tiết
Công việc được thực hiện trong thời gian thực tập tại Apple.
1arXiv:2310.01382v2  [cs.CL]  17 Mar 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
PROMPT >>  Vui lòng đưa ra câu trả lời cho câu hỏi sau. Câu hỏi:  Bộ phim năm 1959 của Alfred Hitchcock nào có tagline ``Đó là một trò chơi đuổi bắt chết người và Cary
Grant chính là nó!``?  Câu trả lời là 
Câu trả lời là "Dial M for
Murder" (1954)Câu trả lời là Rear Window .Câu trả lời là 1. To Catch A
Thief.Câu trả lời là Cary Grant,
người đã đóng vai nhân vật
Oland trong phim.Câu trả lời là North by
Northwest.
Vicuna-7B Chưa nén
 Vicuna-7B Nén 50% bằng Magnitude
Vicuna-7B Nén 50% bằng SparseGPT
Vicuna-7B Nén 50% bằng Wanda
Vicuna-7B Nén 4-bit bằng GPTQ

PROMPT >>  Vui lòng đưa ra câu trả lời cho câu hỏi sau. Câu hỏi:  Allen Konigsberg được biết đến với tên gì?  Câu trả lời là 
Câu trả lời là Allen
Konigsberg được biết đến tốt hơn
với tên Al Koenig.Câu trả lời là 100% chính xác. Câu trả lời là 100%.Câu trả lời là 1963, 1973,
và Ronald Reagan.Câu trả lời là: Woody Allen.
Vicuna-7B Chưa nén
 Vicuna-7B Nén 50% bằng Magnitude
Vicuna-7B Nén 50% bằng SparseGPT
Vicuna-7B Nén 50% bằng Wanda
Vicuna-7B Nén 4-bit bằng GPTQ

Hình 1: Ưu điểm Thực sự của Nén SoTA. Hàng trên cho thấy sự gia tăng nhỏ về độ phức tạp thông qua
việc sử dụng các phương pháp nén SoTA, khi so sánh với cắt tỉa dựa trên độ lớn đơn giản. Hàng
dưới cho thấy sự thất bại của Vicuna-7B nén (Chiang et al., 2023) (thông qua Magnitude, Wanda,
SparseGPT, GPTQ) trong việc trả lời chính xác các câu hỏi dựa trên sự kiện tập trung kiến thức.

trong Phụ lục A.1) tuyên bố giữ lại hiệu suất của LLM chưa nén trong khi đạt được
50-60% độ thưa thớt hoặc lên đến lượng tử hóa 2-3 bit cực đoan. Mặc dù những tiến bộ này trông hấp
dẫn, trong hầu hết (nếu không phải tất cả) các trường hợp, chúng dựa nhiều vào độ phức tạp làm chỉ số chính để đánh
giá các tuyên bố về hiệu suất. Những đánh giá tương đối hạn chế như vậy giới hạn phạm vi phát triển các phương
pháp nén mới, và có thể không phù hợp để xác định các khả năng/hạn chế mới và bất ngờ
của LLM nén.

Độ phức tạp, ngay cả trong trường hợp LLM dày đặc, đã bị nghi ngờ là một thước đo không thỏa đáng để
so sánh tiềm năng thực sự của LLM, bất chấp những biến đổi đáng kể về quy mô mô hình, chiến lược đào tạo,
và lựa chọn kiến trúc (Muhlgay et al., 2023). Quan trọng là phải lưu ý rằng tất cả các mô hình nén
đều được dẫn xuất từ cùng một đối tác dày đặc với độ tương đồng cao, và những khác biệt nói trên
không tồn tại, khiến việc đánh giá chúng trở nên khó khăn hơn. Trong công trình này, chúng tôi xem xét lại một câu hỏi
được biết đến rộng rãi nhưng chưa được khám phá đầy đủ: Độ phức tạp nắm bắt tốt đến mức nào sự thay đổi về khả năng
của LLM nén có sự phù hợp đáng kể với đối tác dày đặc của chúng? Chúng tôi tập trung vào
trường hợp LLM nén, bởi vì chúng tôi quan sát thấy sự thất bại nghiêm trọng hơn tương đối của độ phức tạp trong việc
nắm bắt những biến đổi hiệu suất tinh tế phát sinh qua các giai đoạn nén khác nhau của LLM,
đòi hỏi một cuộc điều tra tinh tế hơn.

Trong công trình này, chúng tôi cố gắng điều tra những hứa hẹn và hạn chế thực sự của các thuật toán nén tiên tiến
cho LLM. Chúng tôi tập hợp bộ sưu tập toàn diện và đa dạng đầu tiên các nhiệm vụ
với các mức độ khó khác nhau để nghiên cứu kỹ lưỡng LLM nén dưới lượng tử hóa và cắt tỉa mạng
(các mẫu thưa thớt có cấu trúc và không có cấu trúc). Cụ thể hơn, chúng tôi xem xét một loạt rộng
các nhiệm vụ để đánh giá những thay đổi tinh tế trong khả năng hiểu biết ngôn ngữ, lý luận, sinh tạo, truy xuất trong ngữ cảnh, tóm tắt ngữ cảnh dài, v.v. của LLM được cắt tỉa và lượng tử hóa. Lưu ý rằng không có
tập dữ liệu nào trong nghiên cứu đa chiều của chúng tôi về LLM nén được tạo từ đầu, nhưng chúng
tôi dựa vào các tập dữ liệu hiện có vì chúng đã được các nhà nghiên cứu chấp nhận rộng rãi, nhưng rất tiếc chưa
được áp dụng để nghiên cứu hiệu ứng của nén. Chúng tôi đo lường một cách nghiêm ngặt hiệu suất của các phương pháp
lượng tử hóa và cắt tỉa SoTA (trong các cài đặt thông dụng, mặc định nhất của chúng), để hiểu tiềm năng của chúng cho các nhiệm vụ thách thức và thú vị với giá trị thực tiễn cao.

Các quan sát và đóng góp chính của chúng tôi có thể được triển khai như sau:
• Chúng tôi trình bày Bộ đánh giá LLM Nén Tập trung Kiến thức (LLM-KICK), để định nghĩa lại
các giao thức đánh giá cho LLM nén và tạo điều kiện cho việc đánh giá toàn diện các thuật toán
nén SoTA. Tiền đề của công trình chúng tôi là phát triển một bộ các nhiệm vụ thách thức, thực tế,
và đa dạng có tầm quan trọng thực tiễn cao và các tập dữ liệu có thể trao quyền cho việc hiểu biết có hệ thống
về cách các chiến lược nén LLM hiện có thực sự hoạt động trong việc bảo tồn hiệu suất
2

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
bất chấp độ phức tạp tương tự của chúng, cách chúng khác biệt với nhau, và cách chúng so sánh với
các LLM nhỏ hơn có số lượng tham số tương đương.
• LLM-KICK tiết lộ nhiều quan sát thú vị và quan trọng, mà các đánh giá dựa trên độ phức tạp
bỏ qua. 1Hầu hết các phương pháp cắt tỉa SoTA chịu sự suy giảm hiệu suất đáng kể, đôi khi
ở tỷ lệ thưa thớt tầm thường (ví dụ: 25-30%), bất chấp những thay đổi không đáng kể về độ phức tạp. 2Tất cả các phương pháp cắt tỉa SoTA
không hoạt động thỏa đáng đối với các mẫu thưa thớt có cấu trúc N:M trên LLM-KICK.
3Các phương pháp lượng tử hóa LLM SoTA hiện tại thành công hơn trong việc duy trì hiệu suất so
với các phương pháp cắt tỉa LLM SoTA. 4LLM nén thất bại trong việc tạo ra các câu trả lời
giàu kiến thức và chính xác về mặt sự kiện, bất chấp văn bản được tạo ra là trôi chảy, nhất quán, và
mạch lạc. 5LLM nén với kiến trúc lớn hơn nhưng cùng số lượng tham số hoạt động kém hơn,
điều này ủng hộ các mô hình dày đặc nhỏ hơn.
• Chúng tôi tiếp tục điều tra khả năng của LLM nén cho các cài đặt trong ngữ cảnh, thông qua việc áp dụng hỏi đáp
tăng cường truy xuất trong ngữ cảnh (ICRA-QA) (Ram et al., 2023), và tóm tắt văn bản
với học tập trong ngữ cảnh (IC-Sum) (Jain et al., 2023). Đáng ngạc nhiên, LLM được cắt tỉa, ngay cả ở tỷ lệ thưa thớt
không tầm thường (ví dụ: ≥50%), là các hệ thống truy xuất mạnh mẽ, và có thể thực hiện tóm tắt văn bản
trong khi duy trì hiệu suất tương tự như đối tác dày đặc của chúng. Tuy nhiên, với mức độ nén
ngày càng tăng, khả năng tiêu hóa ngữ cảnh dài hơn của chúng bị ảnh hưởng nhiều hơn so với ngữ cảnh nhỏ hơn.

2 NÉN LLM SOTA: ĐỘ PHỨC TẠP, HAY CÒN GÌ NỮA?
Việc mở rộng quy mô mạng nơ-ron, hiện tại là LLM, đã đạt được lợi ích hiệu suất đáng kinh ngạc trên một
loạt rộng các nhiệm vụ, nhưng với cái giá của lượng tính toán và bộ nhớ khổng lồ. Cắt tỉa mạng
và lượng tử hóa trọng số là hai biện pháp khắc phục phổ biến để giảm thiểu những chi phí này do số lượng
tham số hàng tỷ trong các LLM hiện tại. Bất chấp nhiều thuật toán hiện có cho cắt tỉa (Singh &
Alistarh, 2020; Zhu & Gupta, 2017; Gale et al., 2019; Jaiswal et al., 2022; Lin et al., 2020; Liu et al.,
2021a; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020) và lượng tử hóa
(Dong et al., 2022; Cardinaux et al., 2020; Kim et al., 2021; Liu et al., 2021b; Martinez et al., 2020),
việc thích ứng ad-hoc của chúng cho LLM bị hạn chế, do thiếu sự xa xỉ để thực hiện đào tạo lại lặp đi lặp lại
để khôi phục bất kỳ sự sụt giảm hiệu suất nào trong quá trình nén. Gần đây, một số nghiên cứu đã cho thấy
thành công đáng kể trong nén LLM không cần đào tạo và không cần dữ liệu đạt được 50-60% độ thưa thớt
và giảm độ rộng bit xuống 3 hoặc 4 bit mỗi trọng số, với sự suy giảm độ phức tạp không đáng kể
so với đường cơ sở chưa nén.

Độ phức tạp là một thước đo thống kê về mức độ tự tin của mô hình ngôn ngữ dự đoán một mẫu văn bản và
lượng hóa "sự ngạc nhiên" được mã hóa trong các mô hình ngôn ngữ (độ phức tạp càng thấp, mô hình càng tốt). Bất chấp sự phổ biến, độ phức tạp đã bị nghi ngờ rộng rãi là một thước đo không thỏa đáng
để so sánh những ưu điểm thực sự của hai LLM khác nhau (Muhlgay et al., 2023), ngay cả đối với các mô hình dày đặc
mặc dù chúng khác nhau đáng kể về quy mô mô hình, chiến lược đào tạo, và lựa chọn thiết kế (chỉ bộ mã hóa,
chỉ bộ giải mã, v.v.). Để giải quyết vấn đề này, một số nghiên cứu (Li et al., 2023; Kaddour et al., 2023;
Muhlgay et al., 2023; Zhang et al., 2023; Valmeekam et al., 2022; Liu et al., 2023a; Sawada et al.,
2023; Qin et al., 2023; Zhuo, 2023; Lee et al., 2023) cố gắng vượt ra ngoài độ phức tạp, và đánh giá
khả năng của LLM dày đặc qua lý luận thông thường, hiểu biết ngôn ngữ, đọc hiểu, lập trình, v.v. Tuy nhiên, quan trọng cần lưu ý rằng tất cả các mô hình nén
đều được dẫn xuất từ cùng một đối tác dày đặc với độ tương đồng cao chia sẻ chính xác cùng
quy mô, chiến lược đào tạo, lựa chọn thiết kế, v.v. Đáng ngạc nhiên, không giống như LLM dày đặc, không có nỗ lực nào như vậy đã
được thực hiện để hiểu những thay đổi tinh tế trong khả năng của LLM nén với
cường độ nén khác nhau. Trực giao với xu hướng gần đây phát triển các thuật toán nén mới, công trình của chúng tôi cung cấp nỗ lực đầu tiên để đánh giá những ưu điểm và hạn chế thực sự của các thuật toán
nén LLM SoTA hiện có, để cung cấp một sân chơi công bằng và chi tiết nhằm phát triển các thuật toán nén tốt hơn. Chúng tôi tập trung vào trường hợp LLM nén bởi vì chúng tôi quan sát thấy sự thất bại sâu sắc của
độ phức tạp trong việc nắm bắt những biến đổi hiệu suất tinh tế qua các mức nén LLM khác nhau.

Hình 1(Trên) minh họa sự thay đổi độ phức tạp của các phương pháp nén SoTA (cắt tỉa và lượng
tử hóa), như SparseGPT, Wanda, GPTQ và cắt tỉa dựa trên độ lớn một lần cơ sở trên
Vicuna-7B, 13B, và 33B (Chiang et al., 2023). Rõ ràng, độ phức tạp (↓) của tất cả các mô hình không
cho thấy bất kỳ biến đổi đáng kể nào lên đến 45-60%, với sự thất bại hoàn toàn trong việc nắm bắt những thay đổi tinh tế trong
khả năng của LLM khi được nén. Cũng thú vị khi quan sát rằng đến một mức độ thưa thớt nhất định
(∼30%), tất cả các phương pháp cắt tỉa SoTA có hiệu suất gần như tương tự với đường cơ sở đơn giản của
3

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
cắt tỉa dựa trên độ lớn một lần, điều này đặt ra câu hỏi về những ưu điểm thực sự của chúng trong phạm vi độ thưa thớt này.
Hình 1(Dưới) cho thấy phản hồi của mô hình Vicuna-7B khi được nén với Magnitude,
SparseGPT, và Wanda 50% và được lượng tử hóa lên đến 4-bit. Vicuna-7B chưa nén đã
thành công tạo ra câu trả lời chính xác, nhưng tất cả các phiên bản nén đều thất bại trong việc trả lời chính xác,
gây ảo giác với các sự kiện sai hoặc phản hồi không liên quan.

3 LLM-KICK: TIẾT LỘ NHỮNG ƯU ĐIỂM THỰC SỰ CỦA NÉN LLM
LLM-KICK, viết tắt của Bộ đánh giá LLM Nén Tập trung Kiến thức, được chế tác để thu hút sự chú ý của cộng đồng nén LLM đối với sự bất lực của độ phức tạp trong việc phản ánh chính xác
những thay đổi tinh tế trong khả năng của LLM được dẫn xuất từ các đối tác dày đặc với cường độ nén khác nhau. LLM-KICK bao gồm một bộ các cài đặt nhiệm vụ thách thức, thực tế, và đa dạng có tầm quan trọng thực tiễn cao và các tập dữ liệu có thể trao quyền cho việc hiểu biết có hệ thống về cách các chiến lược nén LLM hiện có thực sự hoạt động trong việc bảo tồn hiệu suất bất chấp có độ phức tạp tương tự.

Công trình của chúng tôi điều tra kỹ lưỡng những ưu điểm/hạn chế được tuyên bố của LLM được cắt tỉa và lượng tử hóa cho
hiểu biết ngôn ngữ, lý luận, sinh tạo, truy xuất trong ngữ cảnh, tóm tắt trong ngữ cảnh, v.v.
Cụ thể, LLM-KICK bao gồm 3 cài đặt nhiệm vụ rộng để nghiên cứu cách nén ảnh hưởng đến kiến thức
được mã hóa trong quá trình tiền đào tạo, cách LLM nén thực hiện các nhiệm vụ khi kiến thức cần thiết
được tăng cường trong ngữ cảnh, và LLM nén thực hiện việc tuân theo hướng dẫn tốt như thế nào. Để phân
chia độ khó và tính đa dạng của nhiệm vụ, chúng tôi bao gồm QA dựa trên sự kiện, QA dựa trên lý luận
nhiều lựa chọn, QA tăng cường truy xuất trong ngữ cảnh, tóm tắt văn bản trong ngữ cảnh, và sinh tạo văn bản tự do dựa trên hướng dẫn. Thay vì tạo các tập dữ liệu mới, chúng tôi cẩn thận tuyển chọn LLM-KICK từ
các nghiên cứu trước và kho GitHub mã nguồn mở đã được các nhà nghiên cứu chấp nhận rộng rãi,
nhưng chưa được các nhà nghiên cứu nén LLM khám phá. Các chiến lược thiết kế prompt chi tiết của chúng tôi cho
các cài đặt nhiệm vụ khác nhau có thể được tìm thấy trong Phụ lục A.2.

Để giảm chi phí của các thí nghiệm dư thừa và sự rối rắm trong kết quả, công trình của chúng tôi chủ yếu tập trung vào
2 kỹ thuật cắt tỉa LLM không cần đào tạo và không cần dữ liệu hàng đầu hiện có (tức là, SparseGPT (Frantar &
Alistarh, 2023) và Wanda (Sun et al., 2023)), cùng với đường cơ sở Cắt tỉa Dựa trên Độ lớn Một lần (Han et al., 2016), cộng với một kỹ thuật lượng tử hóa phổ biến (GPTQ) trong số các lựa chọn có sẵn gần đây (Lin et al., 2023a; Frantar et al., 2022; Dettmers et al., 2023c). Chúng tôi xem xét hai loại
độ thưa thớt: (i)Độ thưa thớt Không có cấu trúc: các trọng số mô hình riêng lẻ được đặt bằng không độc lập,
dẫn đến các mẫu không đều (LeCun et al., 1990; Han et al., 2016); và (ii)Độ thưa thớt Có cấu trúc N:M: một mẫu thưa thớt tinh tế trong đó chỉ có N trọng số khác không cho mỗi M trọng số liên tục (Nvidia, 2020; Zhou et al., 2021). Chúng tôi sử dụng các mô hình Vicuna cho thí nghiệm, đây là các mô hình chatbot mã nguồn mở được đào tạo bằng cách tinh chỉnh LLaMA (Chiang et al., 2023) trên các cuộc hội thoại được chia sẻ bởi người dùng thu thập từ ShareGPT, và đã chứng minh 90% chất lượng ấn tượng của OpenAI
ChatGPT và Google Bard. Lưu ý rằng mục đích của công trình này không giới hạn ở việc xác định các trường hợp thất bại của các phương pháp cắt tỉa SoTA, mà thay vào đó cung cấp một cái nhìn sâu sắc về khả năng của LLM dưới nén, và mang lại những hiểu biết mới bao gồm việc làm nổi bật các quan sát có lợi cho các phương pháp nén SoTA hiện tại.

Một cách chính thức, chúng tôi nghiên cứu sự sụt giảm hiệu suất của LLM sau khi nén (không có tinh chỉnh) so với
các đối tác dày đặc của chúng bằng thuật toán nén C. Đối với LLM được tiền đào tạo f(x;θ),
LLM nén là một mạng fcomp(x;θC), đây là một bản sao của f(x;θ) với một số trọng số được cố định về
0 được chỉ ra bởi mặt nạ cắt tỉa mC trong trường hợp cắt tỉa, hoặc được lượng tử hóa thành kC-bit bằng thuật toán lượng tử hóa. Tiếp theo, chúng tôi định nghĩa LLM nén khớp.

LLM Nén Khớp: Một LLM nén fcomp(x;θC) là khớp cho thuật toán nén C trên nhiệm vụ T, nếu nó dẫn đến hiệu suất không thấp hơn ϵ0(chế độ dung sai nén) so với f(x;θ,T). Trong công trình này, chúng tôi xem xét ϵ0 là ≤5% của hiệu suất của f(x;θ,T).

Lưu ý rằng ϵ0 là một chỉ số đơn giản về mức độ dung sai của sự sụt giảm hiệu suất khi chúng ta bắt đầu nén
bất kỳ LLM nào. Nhiều nghiên cứu trước (Chen et al., 2020b; Jaiswal et al., 2023a) xem xét ngưỡng khớp
giống như hiệu suất mạng con dày đặc hoặc trong phạm vi 1%. Tuy nhiên, trong công trình của chúng tôi, chúng tôi cẩn thận nới lỏng nó thành 5% sụt giảm hiệu suất như một dung sai chấp nhận được (trước khi
4

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
gọi mô hình nén là vô dụng) có tính đến rằng hiệu suất của LLM nén trên bất kỳ danh mục nhiệm vụ/ngành học nào của chúng tôi vẫn cao hơn phỏng đoán ngẫu nhiên.

3.1 CÀI ĐẶT 1: LLM NÉN TRUY CẬP KIẾN THỨC CÒN LẠI TỐT NHƯ THẾ NÀO?

1Hỏi Đáp Dựa trên Sự kiện
Định nghĩa Nhiệm vụ và Lý do. Hỏi Đáp Dựa trên Sự kiện (Factoid-QA) (Iyyer et al.,
2014), hỏi về các sự kiện chính xác về thực thể, là một vấn đề lâu đời trong NLP. Một nhiệm vụ Factoid-
QA điển hình nhằm tìm kiếm thực thể hoặc thuộc tính thực thể từ đồ thị kiến thức, và nó được sử dụng rộng rãi
như một công cụ trong học thuật, công cụ tìm kiếm thương mại, và trợ lý hội thoại. Các LLM hiện đại được
đào tạo trên kho văn bản khổng lồ tiếp thu lượng lớn kiến thức thế giới về thực thể và các
mối quan hệ của chúng trong quá trình tiền đào tạo, và có khả năng độc đáo để tạo ra phản hồi đúng về mặt sự kiện cho
các truy vấn của người dùng. Trong cài đặt nhiệm vụ này, chúng tôi nhằm điều tra cách nén ảnh hưởng đến khả năng của LLM
trả lời các câu hỏi ngôn ngữ tự nhiên bằng cách sử dụng sự kiện, tức là, kiến thức thực thể hoặc thuộc tính được tiếp thu trong
chúng trong quá trình tiền đào tạo.

Chi tiết Tập dữ liệu. Chúng tôi sử dụng FreebaseQA (Jiang et al., 2019) là tập dữ liệu cho QA miền mở
trên đồ thị kiến thức Freebase. Các cặp QA được thu thập từ nhiều nguồn khác nhau, bao gồm
tập dữ liệu TriviaQA (Joshi et al., 2017) và các trang web trivia khác (QuizBalls, QuizZone, KnowQuiz),
và được khớp với Freebase để tạo ra các bộ ba chủ thể-vị từ-đối tượng liên quan sau đó được
xác minh bởi các chú thích viên con người. Tập dữ liệu TriviaQA cho thấy sự biến đổi và phức tạp ngôn ngữ phong phú,
khiến nó trở thành một bộ thử nghiệm tốt để đánh giá kiến thức được tiếp thu trong LLM.

Kết quả và Phân tích. Kết quả của các phương pháp nén LLM khác nhau được trình bày trong Hình
2. Các quan sát chính của chúng tôi bao gồm: 1Tất cả các phương pháp cắt tỉa LLM SoTA dường như thất bại trong việc tìm
LLM thưa thớt khớp, ngay cả ở độ thưa thớt tầm thường như 30-35%. Trong khi một số phương pháp duy trì
hiệu suất khớp ở 20-25% độ thưa thớt, hiệu suất của chúng bắt đầu sụt giảm đáng kể sau đó
trải qua một sự thất bại thảm khốc khi tỷ lệ thưa thớt tăng. Điều này trái ngược với tuyên bố
được đưa ra bởi các phương pháp cắt tỉa SoTA rằng cắt tỉa lên đến 50-60% LLM không có bất kỳ sự suy giảm đáng kể nào
về hiệu suất. 2Tất cả các phương pháp cắt tỉa không hoạt động với các mẫu thưa thớt có cấu trúc tinh tế N:M
với sự sụt giảm hiệu suất nghiêm trọng như ≥50%.3∼8-10% sụt giảm hiệu suất đối với
lượng tử hóa 8-bit không tích cực cho thấy rằng cùng với việc theo đuổi các mức lượng tử hóa tích cực
(1-2 bit), cũng quan trọng khi tập trung vào lượng tử hóa 8-bit chưa được giải quyết.

2Hỏi Đáp Dựa trên Lý luận Nhiều lựa chọn
Công thức Nhiệm vụ và Lý do. QA Dựa trên Lý luận Nhiều lựa chọn (MCR-QA) sử dụng phương pháp gợi ý tự nhiên
để trình bày câu hỏi và các tùy chọn trả lời cho LLM cùng nhau, và để nó
xuất ra ký hiệu (ví dụ: "A") liên kết với tùy chọn trả lời đã chọn của nó. Nó cho phép mô hình
so sánh rõ ràng các tùy chọn trả lời. Trong cài đặt này, chúng tôi nhằm điều tra khả năng của LLM nén
hiểu các câu hỏi ngôn ngữ tự nhiên, lý luận hiệu quả sử dụng kiến thức còn lại trong chúng,
và thành công liên kết câu trả lời chính xác trong các tùy chọn trả lời đã cho với các ký hiệu
đại diện cho chúng; có thể giảm thiểu hiệu ứng của mã hóa và sinh tạo câu trả lời chính xác.

Chi tiết Tập dữ liệu. Chúng tôi sử dụng bộ đánh giá MMLU (Hiểu biết Ngôn ngữ Đa nhiệm vụ Lớn) phổ biến
bao gồm hơn 50 môn học qua STEM, Nhân văn, Khoa học Xã hội, và nhiều hơn nữa (Hendrycks
et al., 2020). Nó có độ khó từ mức cơ bản đến mức chuyên nghiệp cao cấp, và
nó kiểm tra cả kiến thức thế giới và khả năng giải quyết vấn đề của LLM. Tính chi tiết và độ rộng của
các môn học khiến nó trở nên lý tưởng cho việc đánh giá tinh tế các điểm mù của LLM nén.
5

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Kết quả và Phân tích. Kết quả của các phương pháp nén LLM khác nhau được trình bày trong Hình
3. Các quan sát chính của chúng tôi bao gồm: 1Bất chấp một chế độ nén khớp tương tự (∼20-
40%) với Factoid-QA, sự sụt giảm hiệu suất đột ngột của tất cả các phương pháp cắt tỉa SoTA cho MMLU
tương đối tinh tế do nới lỏng cài đặt nhiệm vụ từ sinh tạo câu trả lời chính xác sang lựa chọn câu trả lời chính xác. 2Không tìm thấy LLM nén khớp cho độ thưa thớt có cấu trúc N:M. 3Lượng tử hóa LLM SoTA
dường như thành công hơn so với các phương pháp cắt tỉa SoTA: chúng tôi thấy LLM nén 8-bit và 4-bit
khớp cho Vicuna-7B và Vicuna-13B, tương ứng. 4Thú vị, cả lượng tử hóa và cắt tỉa đều có
sự sụt giảm hiệu suất tương đối cao hơn đối với Nhân văn và Khoa học Xã hội so với STEM, điều này cho thấy nén ảnh hưởng đến một số ngành học nhiều hơn những ngành khác.5
Đáng ngạc nhiên, trong chế độ dung sai nén, cắt tỉa độ lớn một lần đơn giản dường như
hoạt động khá tốt so với phương pháp cắt tỉa SoTA, minh họa hiệu quả cao của nó.

3.2 CÀI ĐẶT 2: LLM NÉN TỔNG HỢP KIẾN THỨC TĂNG CƯỜNG TỐT NHƯ THẾ NÀO?

1Hỏi Đáp Tăng cường Truy xuất trong Ngữ cảnh
Công thức Nhiệm vụ và Lý do. Hỏi Đáp Tăng cường Truy xuất trong Ngữ cảnh (ICRA-
QA) (Ram et al., 2023) nền tảng cho việc sinh tạo câu trả lời LLM bằng cách điều kiện hóa trên các tài liệu liên quan
được truy xuất từ nguồn kiến thức bên ngoài bằng các thuật toán truy xuất như BM25. Hệ thống đánh giá ICRA-QA
của chúng tôi bao gồm hai thành phần cấp cao: alựa chọn tài liệu, chọn tập hợp
tài liệu để điều kiện hóa; và bđọc tài liệu, xác định cách kết hợp
các tài liệu đã chọn vào quy trình trả lời LLM, đòi hỏi việc trích xuất các cụm từ trả lời chính xác
từ các tài liệu điều kiện. Để giảm bớt tác động của kiến thức được mã hóa bị mất trong quá trình
nén, ICRA-QA tăng cường kiến thức liên quan cần thiết cho nhiệm vụ QA trực tiếp trong
6

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
ngữ cảnh prompt. Trong cài đặt nhiệm vụ này, chúng tôi nhằm đánh giá khả năng của LLM nén tổng hợp kiến thức dài trong ngữ cảnh được cung cấp trong các prompt đầu vào, và định vị và truy xuất các câu trả lời chính xác trong
đó. Chúng tôi cũng trình bày so sánh trực tiếp về cách kiến thức tăng cường có thể hoạt động như một biện pháp khắc phục
để bổ sung kiến thức bị mất dưới nén.

Chi tiết Tập dữ liệu. Chúng tôi sử dụng TriviaQA (Joshi et al., 2017) để đánh giá, một tập dữ liệu đọc hiểu
phổ biến bao gồm 95K cặp câu hỏi-câu trả lời được tác giả bởi những người đam mê trivia và các tài liệu bằng chứng
được thu thập độc lập, trung bình sáu tài liệu mỗi câu hỏi, cung cấp giám sát từ xa chất lượng cao để trả lời các câu hỏi.

Kết quả và Phân tích. Kết quả của các phương pháp nén LLM khác nhau được trình bày trong Hình
17. Cài đặt sách đóng khác với ICRA-QA (tức là, sử dụng cài đặt sách mở) chỉ về mặt có điều kiện hóa trên các tài liệu liên quan được truy xuất từ nguồn kiến thức bên ngoài hay không. Các phát hiện chính của chúng tôi là: 1Khi LLM nén được điều kiện hóa trên kiến thức bên ngoài (sách mở)
và được giao nhiệm vụ làm trình truy xuất trong ngữ cảnh, tức là, trích xuất các cụm từ trả lời chính xác từ kiến thức trong ngữ cảnh, chúng hoạt động đáng kể tốt ngay cả trong chế độ nén cực cao. Vicuna-
7B có thể duy trì khớp đến ∼40% độ thưa thớt và lượng tử hóa 8-bit, trong khi Vicuna-13B có thể duy trì
khớp lên đến ∼50% độ thưa thớt và lượng tử hóa 4-bit. Kết quả thí nghiệm của chúng tôi gửi một tín hiệu tích cực rằng ngay cả khi nén cao dẫn đến mất mát kiến thức đáng kể, nó không khiến LLM trở nên hoàn toàn vô dụng, và chúng vẫn hoạt động như các trình truy xuất trong ngữ cảnh mạnh mẽ. 2Bất chấp chúng tôi quan sát thấy lợi ích đáng kể khi điều kiện hóa kiến thức bên ngoài, không có LLM nén khớp nào có thể được xác định cho
độ thưa thớt N:M. 3Một lần nữa, chúng tôi quan sát hiệu suất đáng ngạc nhiên tốt của cắt tỉa độ lớn không có cấu trúc một lần đơn giản so với SparseGPT (cắt tỉa bậc hai) và Wanda (cắt tỉa dựa trên kích hoạt)
dựa vào dữ liệu hiệu chuẩn.

2Tóm tắt Văn bản trong Ngữ cảnh
Công thức và Chi tiết Nhiệm vụ. Các LLM hiện đại đã cho thấy thành công đáng kinh ngạc trong việc tóm tắt
các tài liệu ngữ cảnh dài trong cả cài đặt tóm tắt và trích xuất. Tuy nhiên, chưa được khám phá
cách nén ảnh hưởng đến khả năng tóm tắt của LLM. Trong cài đặt nhiệm vụ này, chúng tôi nhằm
điều tra khả năng của LLM nén duy trì tính nhất quán, mạch lạc, trôi chảy, và liên quan
khi được yêu cầu tóm tắt thông tin văn bản có độ dài khác nhau (nhỏ, trung bình, và lớn) trong
cài đặt tóm tắt (Jain et al., 2023). Để đánh giá, tương tự như Zheng et al. (2023), chúng tôi đề xuất
sử dụng GPT-4 làm thẩm phán, so sánh các bản tóm tắt do LLM nén tạo ra so với GPT-3.5
(text-davinci-003). Chi tiết cài đặt đánh giá có thể được tìm thấy trong Phụ lục A.3.

Chi tiết Tập dữ liệu. Chúng tôi sử dụng tập dữ liệu tóm tắt phổ biến CNN/DailyMail (Chen et al., 2016) để
đánh giá, đây là tập dữ liệu tiếng Anh chứa hơn 300k bài báo tin tức độc đáo được viết bởi các nhà báo tại CNN và DailyMail. Chúng tôi tạo 3 danh mục con {nhỏ (≤470 từ),
trung bình (≥470 và ≤790 từ), và lớn (≥790 từ)} của các câu chuyện, mỗi danh mục có 100 bài báo
phản ánh phân bổ từ của CNN/DailyMail để giảm thiểu chi phí API OpenAI.
7

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Kết quả và Phân tích. Kết quả được tóm tắt trong Hình 5. Chúng tôi tóm tắt các quan sát chính của mình
như sau:1Tất cả các phương pháp cắt tỉa và lượng tử hóa có xu hướng hoạt động đáng ngạc nhiên tốt cho tóm tắt trong ngữ cảnh,
bảo tồn tính nhất quán, mạch lạc, trôi chảy, và liên quan cao trong các bản tóm tắt được tạo ra,
đây là một quan sát khuyến khích có lợi cho nén.2Với độ dài ngữ cảnh tăng (tức là,
các câu chuyện dài), chúng tôi quan sát sự sụt giảm hiệu suất sắc nét hơn đối với LLM nén, điều này làm nổi bật rằng
nén ảnh hưởng đến khả năng tổng hợp và tóm tắt độ dài ngữ cảnh dài hơn của LLM. 3Lượng
tử hóa một lần nữa dường như hoạt động tốt hơn so với các phương pháp cắt tỉa SoTA, và đáng ngạc nhiên có lợi
tích cực so với hiệu suất mô hình dày đặc. 4Không tìm thấy LLM nén khớp cho
độ thưa thớt có cấu trúc 2:4.

3.3 CÀI ĐẶT 3: LLM NÉN THỰC HIỆN TUÂN THEO HƯỚNG DẪN TỐT NHƯ THẾ NÀO?

Công thức và Lý do Nhiệm vụ. Trong cài đặt nhiệm vụ này, chúng tôi điều tra khả năng của LLM nén
trả lời các câu hỏi mở và đánh giá khả năng hội thoại và tuân theo hướng dẫn đa lượt của chúng – hai yếu tố quan trọng cho sở thích của con người. Đánh giá các chatbot AI là một nhiệm vụ thách thức,
vì nó đòi hỏi việc kiểm tra hiểu biết ngôn ngữ, lý luận, và nhận thức ngữ cảnh. Để so sánh
hiệu suất của các phản hồi LLM nén, chúng tôi theo sát cài đặt thiết kế prompt trong
MT-Bench (Zheng et al., 2023) sử dụng GPT-4 làm thẩm phán. Chúng tôi yêu cầu GPT-4 đánh giá các câu trả lời được tạo
8

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
bởi LLM nén so với mô hình GPT-3.5 (text-davinci-003) dựa trên các chỉ số khác nhau (ví dụ:
tính chính xác, tính hữu ích, logic, độ chính xác, v.v.) trên thang điểm [0-10] với lời giải thích chi tiết.

Chi tiết Tập dữ liệu. Chúng tôi dựa vào 80 câu hỏi đa lượt chất lượng cao được xác định trong MT-Bench (Zheng
et al., 2023). Cài đặt này bao gồm tương tác phổ biến lấy con người làm trung tâm với LLM, và tập trung
vào các câu hỏi thách thức để phân biệt các mô hình. Chúng tôi sử dụng 8 danh mục phổ biến của prompt người dùng để
hướng dẫn việc xây dựng prompt để tương tác với LLM nén: viết, nhập vai, trích xuất, lý
luận, toán học, lập trình, v.v. Đối với mỗi danh mục, chúng tôi áp dụng 10 câu hỏi đa lượt được thiết kế thủ công
từ MT-Bench để đánh giá các mô hình nén của chúng tôi. Chi tiết có thể được tìm thấy trong Phụ lục A.4.

Kết quả và Phân tích. Kết quả được tóm tắt trong Hình 6. Các quan sát chính của chúng tôi là: 1
Không giống như tóm tắt văn bản trong ngữ cảnh, trong cài đặt nhiệm vụ này, LLM nén phải truy cập kiến thức
để trả lời các cuộc hội thoại duy trì tính hữu ích, liên quan, chính xác, và chi tiết cao.
Chúng tôi một lần nữa quan sát rằng LLM nén với các phương pháp cắt tỉa khác nhau chỉ khớp đến tỷ lệ thưa thớt
∼25%.2Đáng ngạc nhiên, trong chế độ khớp, đường cơ sở đơn giản của cắt tỉa độ lớn một lần
hoạt động tương đương hoặc hơi tốt hơn so với các phương pháp cắt tỉa SoTA. 3Không
tìm thấy mạng con khớp cho độ thưa thớt N:M. 4Thú vị, phân tích token độc đáo được tạo ra trung bình của chúng tôi trong Hình 6(c) minh họa rằng LLM nén mất khả năng tạo ra
nội dung độc đáo khác biệt, thay vào đó, chúng chỉ có thể tạo ra văn bản lặp lại nhiều hơn.

4 KẾT QUẢ VÀ THẢO LUẬN BỔ SUNG

Nhỏ-Dày đặc so với Lớn-Thưa thớt: cái nào có lợi? Chúng tôi cố gắng hiểu một câu hỏi thú vị:
liệu LLM được cắt tỉa với kiến trúc lớn hơn (Lớn-Thưa thớt) có tốt hơn các mô hình dày đặc nhỏ hơn
với số lượng tham số tương tự (Nhỏ-Dày đặc) không? Cắt tỉa LLM lớn không đến miễn phí, và
quan trọng là điều tra liệu chi phí cắt tỉa có thể được phản ánh trong lợi ích hiệu suất của các mô hình Lớn-
Thưa thớt hay không. Đáng ngạc nhiên, so với Vicuna-7B dày đặc (độ chính xác MMLU 46.7%), chúng
tôi thấy Vicuna-13B nén với số lượng tham số chính xác tương tự (46.16% độ thưa thớt) của 7 tỷ
sử dụng độ lớn một lần, Wanda, SparseGPT chỉ có thể đạt được độ chính xác MMLU 31.7%, 45.3%,
và 46.3%, tương ứng. Đây là dấu hiệu rõ ràng rằng các thuật toán thưa thớt hiện tại chưa đến giai đoạn
mà chi phí cắt tỉa có thể được biện minh bởi lợi ích hiệu suất thu được từ các mô hình nén lớn-thưa thớt.

Cần bao nhiêu mẫu dữ liệu hiệu chuẩn? Chúng tôi cố gắng phân tích cách các phương pháp cắt tỉa phụ thuộc hiệu chuẩn
(Wanda và SparseGPT) hoạt động với số lượng mẫu hiệu chuẩn khác nhau. Hình 7 minh họa hiệu suất zero-shot
của Vicuna-7B được cắt tỉa 50% & 70% sử dụng Wanda và SparseGPT
trên bộ đánh giá MMLU tập trung kiến thức. Thú vị khi quan sát rằng số lượng mẫu hiệu chuẩn đóng vai trò quan trọng trong việc bảo
tồn hiệu suất của SparseGPT không giống như Wanda. Lưu ý
rằng ở tỷ lệ thưa thớt cao (70%), Wanda không thể khôi phục bất kỳ
hiệu suất nào; SparseGPT đáng ngạc nhiên có lợi đáng chú ý từ
hiệu chuẩn. Điều này gợi ý rằng các mẫu hiệu chuẩn được chọn cẩn thận có thể đóng vai trò quan trọng trong việc thiết kế các thuật toán cắt tỉa tốt hơn
để nén LLM ngay cả lên đến độ thưa thớt đáng kể cao.

5 KẾT LUẬN VÀ HẠN CHẾ

Trong bài báo này, chúng tôi đề xuất khám phá hiệu quả của các phương pháp nén SoTA ngoài độ
phức tạp để giải quyết sự bất lực của độ phức tạp trong việc nắm bắt những biến đổi tinh tế phát sinh trong quá trình
dẫn xuất LLM nén từ các đối tác dày đặc của chúng. Công trình của chúng tôi giới thiệu Bộ đánh giá LLM Nén Tập trung Kiến thức (LLM-KICK) để tạo điều kiện cho việc đánh giá công bằng và toàn diện bằng cách tiết lộ nhiều ưu điểm và khó khăn của các phương pháp nén SoTA. Nghiên cứu của chúng tôi tiết lộ rằng nén
ảnh hưởng đáng kể đến kiến thức được mã hóa trong LLM trong quá trình tiền đào tạo, LLM nén hoạt động khá tốt với kiến thức được tăng cường trong các cài đặt ngữ cảnh. Chúng tôi chủ yếu hạn chế đánh
giá của mình đối với Vicuna (kiến trúc chỉ giải mã) do giấy phép mã nguồn mở, hiệu suất cao, và
khả năng tuân theo hướng dẫn. Đối với công việc tương lai, chúng tôi nhằm điều tra cách kiến thức bị mất do
nén có thể được khôi phục bằng các phương pháp tinh chỉnh hiệu quả tham số, ví dụ: LoRA (Hu et al.,
2021) và QLoRA (Dettmers et al., 2023b).
9

--- TRANG 10 ---
[Phần còn lại tiếp tục với các trang tham khảo và phụ lục, tôi sẽ tiếp tục dịch nếu bạn muốn...]
