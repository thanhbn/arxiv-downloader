# 2204.04215.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2204.04215.pdf
# File size: 483407 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Data-Free Quantization with Accurate Activation
Clipping and Adaptive Batch Normalization
Yefei He, Luoming Zhang, Weijia Wu, Hong Zhou
College of Biomedical Engineering & Instrument Science
Zhejiang University
Hangzhou, China
{billhe,zluoming,weijiawu}@zju.edu.cn, zhouh@mail.bme.zju.edu.cn
Abstract
Data-free quantization compresses the neural network to low bit-width without
access to original training data. Most existing data-free quantization methods
cause severe performance degradation due to inaccurate activation clipping range
and quantization error, especially for low bit-width. In this paper, we present a
simple yet effective data-free quantization method with accurate activation clipping
and adaptive batch normalization. Accurate activation clipping (AAC) improves
the model accuracy by exploiting accurate activation information from the full-
precision model. Adaptive batch normalization (ABN) Ô¨Årstly proposes to address
the quantization error from distribution changes by updating the batch normaliza-
tion layer adaptively. Extensive experiments demonstrate that the proposed data-
free quantization method can yield surprisingly performance, achieving 64:33%
top-1 accuracy of 4-bit ResNet18 on ImageNet dataset, with 3:7%absolute im-
provement outperforming the existing state-of-the-art methods.
1 Introduction
Deep learning has achieved breakthrough successes in many Ô¨Åelds, such as computer vision [6, 11]
and natural language processing [4]. Over-parameterization is an obvious feature of deep learning
models compared to traditional methods. Since its birth, deep learning models have the disadvantages
of massive parameters and high computational complexity. For example, ResNet18 [5] network has
11.7M parameters and 1.8GFLOPS calculations. These shortcomings limit the application of deep
learning models in edge devices such as mobile phones. To solve this problem, model compression
methods such as model quantization, distillation, and pruning have emerged in recent years. Among
them, model quantization is one of the most commonly used compression methods. Quantization
refers to mapping model parameters or activations from Ô¨Çoating-point numbers to integers according
to certain rules, thereby greatly reduce the size of the model and accelerate the inference process.
Quantizing a full-precision model to low bit-width directly often leads to a severe accuracy drop. To
address this, two quantization methods, i.e., quantization-aware-training (QAT) and post-training
quantization (PTQ) are proposed. The former aims to retrain the quantized model and consumes lots
of computation resources. The latter directly quantize the pre-trained Ô¨Çoating-point model and uses
part of the training data to calibrate it. Both methods are data-driven that require real data in their
quantization process. However, in many practical scenarios, the training dataset is not available for
privacy policy or security issues.
Fortunately, data-free quantization can compress models without accessing any original training data.
Many excellent prior studies [1,12] try to solve the task by the generative methods with synthetic data.
ZeroQ [1] and DSG [14] proposed to generate fake data from the distribution ( i.e., mean and standard
deviation) of BN layers. After using the fake data to update the range of quantized activations,
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2204.04215v2  [cs.LG]  22 Jun 2022

--- PAGE 2 ---
FP Model
Gaussian
DataABS Loss
AC
DataUpdateQuantized Model
Activation LayersFP Model
Gaussian
DataBNS Loss
BN
DataUpdateQuantized Model
BatchNorm Layers
(u, l) (Œº, œÉ)Data Generator
Quantized ModelConv LayersFP Model
QATTraining
‚ë†
‚ë°Figure 1: Proposed three-step pipeline for data-free quantization. Here, "AC" means activation
clipping and "FP Model" denotes full-precision model.
the method achieved competitive result compared to the previous QAT methods at 8-bit, but the
performance degrades signiÔ¨Åcantly at low bit-width (especially 4-bit or lower). Although the synthetic
data conforms to the distribution of BN layers, the activation range is actually determined by the peak
value and there is no necessary connection between the extreme values of the data and its distribution.
Therefore, we argut that the updated activation clipping range is inaccurate. Besides, the quantization
process inevitably brings deviations to both weights and activations, thus disturbing the distribution
of intermediate feature maps. The distribution mismatch between the feature maps and the Ô¨Åxed BN
statistics will further aggravate the performance decrease. However, this problem is disregarded by
previous data-free quantization methods and how to alleviate this mismatch remains an open question.
Furthermore, recent studies [2,12,15] proposed neural network-based data generators to Ô¨Åne-tune the
quantized model. After Ô¨Åne-tuning, their work achieved better results than [1] when models were
quantized to 4-bit. Nevertheless, training a generator requires a lot of time and computing resources,
and Ô¨Åne-tuning can be less useful or even harmful for 8-bit quantization. How to quantize models
generally and efÔ¨Åciently is still a problem.
To address the above issues, we present a simple yet effective data-free quantization method with two
core components, i.e., accurate activation clipping and adaptive batch normalization. Accurate activa-
tion clipping uses the classiÔ¨Åcation results of the teacher model and an absolute value loss function to
optimize the synthetic data, the generated data contains accurate activation range information of the
original dataset. Adaptive batch normalization takes the shift of quantized feature map distribution
into consideration. After determining the activation value range, we propose to adaptively update the
BN statistics of the quantized model, alleviating the distribution mismatch. Finally, we propose to
perform Ô¨Åne-tuning only when necessary to further improve accuracy. Even with limited computing
resources that can‚Äôt perform Ô¨Åne-tuning, our method can achieve competitive performance compared
to Ô¨Åne-tuning-based method.
We summarize our main contributions as follows:
‚Ä¢We rethink the determinants of activation range and propose a novel method to generate data
speciÔ¨Åcally for accurate activation clipping. This provides a new approach for extracting dataset‚Äôs
activation information from full-precision models.
‚Ä¢An adaptive batch normalization is proposed to alleviate the disturbance from distribution mismatch
of BN layers.
‚Ä¢Combining the above methods with optional Ô¨Åne-tuning, we get a brand new three-step data-
free quantization pipeline. Extensive experiments on the large scale ImageNet dataset prove
that quantization with our method can surpass the state-of-the-art methods with 3:7%absolute
improvement on 4-bit ResNet18.
2 Related Work
Data-free quantization was Ô¨Årst proposed by DFQ [7] and quickly became the focus of research.
There are two important questions in the Ô¨Åeld of data-free quantization: how to generate synthetic
data and how to apply these data to improve the quantized model. Updating gaussian input using
2

--- PAGE 3 ---
gradient backpropagation is a wildly adopted approach. Methods like ZeroQ [1] and DSG [14]
followed this scheme and extract information from BN layers to generate synthetic data for activation
clipping. In this case, the key problem is how to recover the activation information of the training
dataset from the full-precision model. Another common approach uses a network-based generator to
synthesize data [2, 12, 15]. After generating data, they Ô¨Åne-tuned the quantized model to improve the
accuracy. However, this can be much more time-consuming than the Ô¨Årst approach.
To further improve the limit of data-free quantization, we argue that the method of generating data
should be considered corporately with the method of using it. In other words, questions one and two
should be considered together. For activation clipping, we analyzed the cause of the peak value of
activations and proposed a new loss function LABS that can generate data dedicated to activation
clipping. Meanwhile, we propose a new method to apply the synthetic data, i.e., updating the BN
layers‚Äô statistics to alleviate the distribution mismatch. Both methods are fast and effective, and can
be further combined with Ô¨Åne-tuning-based methods.
3 Method
0 2 4 6 8 10 12 14
Layer5101520253035Activation threshold
Ours
ZeroQ
Best
Figure 2: Activation range comparison for ResNet18 on ImageNet when quantized to 4-bit. Among
them, the best clipping range is obtained by iterate through all values between [0;50](which basically
covering all activation values) while inputting real dataset into the model.
3.1 Preliminaries
We use uniform quantization in our study and experiments. For uniform quantization, given the
bit-widthband clip range [l;u]for weight or activation, quantization-dequantization process as
follows:
 =u l
2b 1(1)
Q(x) = round(x l
) (2)
D(x) =Q(x) +l (3)
where is the interval length, Q(x)is the quantized representation of the data and D(x)is the result
of dequantization process of a value Q(x).
3

--- PAGE 4 ---
There will be two main problems when it comes to data-free quantization. First, since the weight has
been trained, the range of the weight is its minimum/maximum value. However, the clip range for
activations of each layers depends on the speciÔ¨Åc input and is still unknown. Second, the statistics ( 
and) of BN layers depend on the input and feature maps of the network, and have been Ô¨Åxed in the
model. However, quantizing the model may shift the distribution of the intermediate feature maps,
which consequently become inconsistent with the BN statistics.
In this section, we propose two approaches, i.e., accurate activation clipping and adaptive batch
normalization to address the corresponding two problems, which can achieve remarkable results in a
short time. In low-bit quantization, we further use Ô¨Åne-tuning to improve the accuracy, thus having a
three-step pipeline for data-free quantization, as shown in Figure 1.
3.2 Accurate Activation Clipping
To determine the clipping range of activations ( i.e.uandlin Equation (1)), one common way is to
generate synthetic data and conduct a forward propagation with it. The peak value of activations
is stored as the clipping range parameter. While previous studies [1, 12] generate fake data with
the distribution of BN layers, it only provides a coarse prediction for the activation range. We
argue that it is not the optimal choice for updating the clipping range because the peaks are not
directly related to the distribution of the data. This is illustrated in Figure 2, where we plot the best
activation clipping range and the data-determined range for every layer in the ResNet18 model. With
distribution-consistent data from BN layers, ZeroQ [1] presents a dissatisÔ¨Åed performance compared
to the optimal activation value. So what exactly determines the activation clipping range?
Batch 
NormalizationConvolution ReLU ConvolutionClip & Quant
Figure 3: A common block structure of quantized CNNs
Figure 3 presents a common block structure of quantized CNNs. Before clipping, ReLU takes feature
map of convolution layers as input. Thus, the lower bound lis zero without doubt, while the upper
boundudepends on the maximum value of feature map. The recent study [16] revealed that feature
map is the response of the network to category features where high response is related to signiÔ¨Åcant
features and low response is related to irrelevant features such as background. Therefore, to emulate
the response of the real dataset, synthetic data should make the network to be highly responsive.
To this end, we propose a novel method to generate accurate activation clipping data. We notice
that deep learning models perform image classiÔ¨Åcation tasks on large-scale datasets well with rich
category-related information. Thus we try to let the model to learn the most responsive data by itself.
For example, given a target label "Ô¨Çowers" and a gaussian-random image, if we input the gaussian
image to the network directly, the result is unlikely to classify it as a Ô¨Çower. However, if we have an
appropriate loss function, calculate the loss according to the target label and backpropagate to the
image, the conÔ¨Ådence of classifying as "Ô¨Çowers" will increase as we iterate. Finally, the network
generates the "most Ô¨Çower-liked" image adaptively. With the high-quality generated image used as
input, the model will emerge higher response and emulate the real dataset‚Äôs activation peaks. In other
words, we have to solve the following optimization problems:
min
xEx;y[`(M(x);y)]; (4)
wherexis the synthetic input data, yis the target label, `is the loss function and Mis the full-precision
model.
To select an appropriate loss function, we take a deeper look at this problem. The Cross-Entropy (CE)
loss is the most commonly used loss function in image classiÔ¨Åcation tasks and it‚Äôs a good indicator
of how well the model classify input images. Therefore, we use the CE loss to judge the quality
of the generated data, where lower loss means better generated data. However, when it comes to
backpropagation, CE loss is not the best choice. A toy example is used to illustrate the problem, as
4

--- PAGE 5 ---
shown in Figure 4(a). We can see that using CE loss for backpropagation cannot optimize the loss
function to the optimum. Details of the toy experiment can be found in Appendix 6.1.
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013
/uni00000053
(a) Optimized by CE loss/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013
/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013
/uni00000053
(b) Optimized by ABS loss
Figure 4: Toy experiment of optimization using different loss functions. We take M(x)=x as the
model and optimize the input. prepresents prediction probability of target class.
The derivative computed by CE loss can be formulated as:
@LCE
@My(x)=@LCE
@py@py
@My(x)(5)
= 1
pypy(1 py) (6)
=py 1 (7)
wherepyis the probability of classify to label y ( i.e., the result of softmax operation). With the
increase ofpy, the derivative used for backpropagation will be smaller, which makes the optimization
process saturated before it reaches the optimal state.
It can be seen from Equation (7)that the problem is mainly caused by softmax operation, where
all the scores are brought into calculation. Actually, in this peak value problem, we only care about
the score corresponding to the target label and hope to reÔ¨Çect it on the input image. Scores of other
labels or their appearance on the input image are irrelevant to the model‚Äôs response. Therefore, we
propose a hard-label absolute value loss (ABS Loss):
LABS= My(x) (8)
Thus, only the regions that the model considers to have an impact on the "Ô¨Çower" features will be
enhanced, and the intermediate feature map will also produce a higher response. Using Equation (8)
as loss functions in the algorithm also make the Cross-Entropy loss drop to 0 quickly, making the
data generation process even faster (see Figure 4(b)). It can also be seen from the Figure 2 that the
data we generate is very close to the optimal value of the activation clipping range.
3.3 Adaptive Batch Normalization
Previous research [7] has found that when the convolutional layer is quantized, the output featuremap
distribution will have a certain offset compared to the original model. When we quantize models
to low bit-width, this phenomenon will be more obvious, as shown in Figure 5. In this case, since
the distribution of the input data does not match the statistic stored in the BN layer, the BN layer
not only fails to normalize the data, but may also have a side effect that harms the model accuracy.
This statistic inconsistency is caused by quantization errors. When we have the training dataset, we
can use QAT to retrain the weights and BN layers of the network. However, in the case of data-free
quantization, we do not have training data to optimize the quantization error. Hence, in this study
5

--- PAGE 6 ---
15
 10
 5
 0 5 10 15
(a)0.00.51.01.52.02.53.03.51e6
FP Model ( =-0.18, =1.09)
Quantized Model ( =-0.30, =1.44)
20
 15
 10
 5
 0 5 10 15
(b)0.000.250.500.751.001.251.501.751e6
FP Model ( =-0.45, =1.48)
Quantized Model ( =-0.54, =1.88)
Figure 5: Comparison between featuremaps of original model and quantized model. (a):
layer1.1.conv2, (b): layer2.0.conv2.
we look at this problem from another perspective. Since the model has a large number of weight
parameters, it‚Äôs difÔ¨Åcult to update without training data. But the BN layers contains few parameters
and only depends on the distribution of the input data. In order to alleviate the distribution mismatch
and improve the accuracy of the quantized model, we choose to update the statistics of BN layers and
let BN layers adapt to the errors caused by quantization.
Although the data generated in section 3.2 can restore the activation range very well, it is not suitable
for updating the statistics of the BN layer. As analyzed above, updating the activation range is an
extremum problem while updating the BN statistics is a data distribution problem. Therefore, we use
the distribution-consistent data proposed in [1] to update the BN statistics.
The BN statistics (BNS) loss function is deÔ¨Åned as follows:
LBNS=LX
i=0kbi ik22+kbi ik22(9)
Among them, i,iare the mean and standard deviation information stored in the i-th BN layer
in the full-precision model. biandbiare the mean and standard deviation of input data at layer i.
Similar to AAC, the random input image is backpropagated through the loss function (9), and the
generated data obtained contains the distribution information of the training dataset. By feeding this
data into the network, we can measure the distribution disturbance and update the BN statistics well.
3.4 Three-Step Pipeline
In the case of low bit-width (especially 4-bit or lower) quantization, it is difÔ¨Åcult to achieve ideal
results without Ô¨Åne-tuning the weights. Methods like GDFQ [12] propose to use a network-based
generator to generate fake data and perform retraining on the quantized model.
However, as analyzed in section3.1, determining activation range and BN statistics are key problems
in data-free quantization. While retraining the weights improve the accuracy, these two problems are
neglected by previous Ô¨Åne-tuning methods. Meanwhile, our corresponding methods proposed above
are effective and fast that can be completed within one minute. Therefore, we use our method as the
basis for Ô¨Åne-tuning, which makes up for their shortcomings and greatly improves the accuracy. At
this point, we get a three-step data-free quantization pipeline, as shown in Figure 1.
Fine-tuning can consume lots of time and computing resources while our method are efÔ¨Åcient.
Therefore, we try to seek a balance between time and effect. In the case of 8-bit, our two-step
method (without Ô¨Åne-tuning) has been able to achieve ideal results, and the improvement brought by
Ô¨Åne-tuning is subtle. In the case of low bit-width, although the improvement brought by Ô¨Åne-tuning
is still considerable and a complete three-step method is worth performing, the two-step method
6

--- PAGE 7 ---
Table 1: Comparisons of different network architecture on ImageNet. Here, "No Data" means no
training data is used, "No FT" stands for no retraining the weight, and "FP" means full-precision
pretrained model.
MODEL NODATA NOFT M ETHOD BIT-WIDTH (W/A) A CC.(%)
RESNET18  FP 32/32 71.47p pZEROQ [1] 4/4 26.04p pDSG [14] 4/4 34.53p pOURS 4/4 55.06p GDFQ [12] 4/4 60.60p OURS 4/4 64.33
  FP 32/32 71.47p pDFQ [7] 8/8 69.70p GDFQ [12] 8/8 70.80p pZEROQ [1] 8/8 71.43p pOURS 8/8 71.43p pDSG [14] 8/8 71.49
BN-VGG16  FP 32/32 74.28p pZEROQ [1] 4/4 1.15p GDFQ [12] 4/4 67.10p pOURS 4/4 68.59p OURS 4/4 70.10
INCEPTION V3  FP 32/32 78.80p pZEROQ [1] 4/4 26.84p pDSG [14] 4/4 34.89p pOURS 4/4 69.20p GDFQ [12] 4/4 70.39p OURS 4/4 72.74
MOBILE NETV2  FP 32/32 73.03p GDFQ [12] 4/4 59.24p OURS 4/4 59.50p pDFQ [7] 8/8 72.33p GDFQ [12] 8/8 72.88p pZEROQ [1] 8/8 72.91p pOURS 8/8 72.93
achieves a huge improvement over recent study DSG [14] and is comparable with Ô¨Åne-tuning based
method, which will be further analyzed in section 4.2.
4 Experiments
4.1 Implementation Details
This research is based on the Pytorch framework and all experiments are conducted using RTX3090
GPUs. For the generation of accurate activation clipping data, we used a learning rate of 0.2 to
optimize 200 iterations; for the generation of distribution-consistent data, we used a learning rate
of 0.5 to optimize 500 iterations. For data generator, we follow the settings introduced by [12]. To
make a fair comparison, the method of quantizing models is the same as [1] and [12], where all the
convolution layers and linear layers are quantized to the same bit-width.
We conduct all experiments on the large-scale benchmark dataset ImageNet (ILSVRC12) [3]. In order
to verify the versatility of our method, we tested several commonly used network structures, including
ResNet [5], VGG [9], Inception v3 [10] and MobileNetV2 [8]. All networks use the pre-trained
model provided by pytorchcv [13].
7

--- PAGE 8 ---
4.2 Comparison with State-of-the-arts
In this subsection, we evaluate our method by comparing it with existing state-of-the-arts data-free
quantization methods over various network architecture. Since the accuracy in 8-bit quantization is
very close to the full-precision model, we mainly report the results at 4-bit, as shown in Table1.
We can observe that for W4A4 ( i.e., quantize both weights and activations to 4-bit), our method
outperforms all other methods by large margins on different network architectures. This illustrates
the effectiveness of our accurate activation clipping and BN statistics update. Even if our method
does not use Ô¨Åne-tuning on many models, the effect is very close to or exceeds that of GDFQ, which
requires Ô¨Åne-tuning. As we all know, Ô¨Åne-tuning consumes a lot of GPU time. Our approach (without
FT, two-step) only takes about one minute to complete, strikes a better balance between time and
results.
For W8A8, the effects of various methods are very close to the full-precision model. In this case, we
Ô¨Ånd that if the generated data is used for Ô¨Åne-tuning, the effect is subtle or may even be worse than
the result of not using Ô¨Åne-tuning. We recommend not using Ô¨Åne-tuning for 8-bit quantization.
4.3 Ablation Study
4.3.1 Three-step pipeline
To further verify the effects of each part of three-step pipeline, we used the ResNet18 model to
perform ablation experiments on ImageNet dataset. Table 2 shows the performance under different
settings. We can see that all three steps have played an important role in improving accuracy. When
we combine them, the proposed three-steps pipeline obtained achieves the effect of SOTA, indicating
that the improvements brought by the three steps can be superimposed. The three steps consider a
better activation clipping range, updating BN statistics to adapt to the quantization error, and Ô¨Åne-
tuning the weight separately, that‚Äôs why our method can obtain better results. It is worth mentioning
that when only the activation clipping range is updated, our performance greatly surpasses ZeroQ [1],
indicating that our method can better restore the activation value range of the real dataset.
Table 2: Ablation study of three-step pipeline on 4-bit quantization. Here, "AC" donotes activation
clipping, "BN Update" means updating the statistics in BN layers of quantized model and "FT" means
using synthetic data to Ô¨Åne-tune the quantized model.
WITHAC W ITHBN UPDATE WITHFT A CC.(%)
p(ZEROQ)  26.04
   10.22p  43.03p p 55.06p p p64.33
What‚Äôs more, we valid the effects of proposed method over several state-of-the-art Ô¨Åne-tuning-based
methods. As shown in Table 3. In all cases, our method can further improve the accuracy, proving
the robustness of our method and that an accurate activation range is an important basis for the
Ô¨Åne-tuning-based method.
4.3.2 Selection of Loss Function
We further investigate the performance of generating accurate activation data using different loss
functions. The ResNet18 model is quantized to 4-bit. We iterate on the gaussian image for 600
epochs to generate synthetic data, and use this data to update the activation clipping range. Table
4 shows the performance under different settings, where ‚ÄúMAE‚Äù denotes mean average error and
"MSE" stands for mean square error. We feed the generated data into the model and calculate the
Cross-Entropy loss to measure how well the model responds to the data.
It can be seen that the data generated by the proposed ABS loss can make the model predict labels
exactly as we expected. This also achieved the highest classiÔ¨Åcation accuracy after updating the
quantized model.
8

--- PAGE 9 ---
Table 3: Results of 4-bit ResNet18 on ImageNet. The results are obtained with the ofÔ¨Åcial code from
authors
METHOD TOP-1 A CC.(%)
REAL DATA 67.90
GDFQ [12] 60.60
GDFQ+O URS 64.33 (+3.73)
QIMERA [2] 62.98
QIMERA +OURS 63.72 (+0.74)
INTRA Q [15] 66.47
INTRA Q+OURS 67.06 (+0.59)
Table 4: Selection of different loss functions
MODEL METHOD LOSS ACC.(%)
RESNET18ABS+BNS 2.86 28.46
CROSS -ENTROPY 1.77 33.68
MAE 1.10 41.13
MSE 0.25 41.39
ABS 0 43.03
An intuitive idea is to allow the synthetic data to make the model highly responsive while conforming
to the distribution determined by the BN statistics. However, in experiments we found that if we add
the two loss functions together, the optimization process becomes unstable and is unable to converge.
4.4 Quantization EfÔ¨Åciency
In this subsection, we compare our two-step and three-step pipeline with the existing data-free
quantization methods, as shwon in Table 5. Methods that update gaussian input to determine the
activation range is way faster than methods with Ô¨Åne-tuning. Among them, our method achieves
the best accuracy and brings 21% absolute improvement on 4-bit ResNet18. This is very useful
on devices that have limited computing power and cannot perform Ô¨Åne-tuning. When Ô¨Åne-tune is
required, our method can greatly improve the accuracy while barely increasing the time consumption.
Table 5: Comparison of time cost of 4-bit ResNet-18 with different data-free quantization methods.
METHOD WITHFT A CC.(%) T IME(MIN)
ZEROQ  26.08 0.83
DSG  34.53 -
OURS (TWO -STEP ) 55.06 1.38
GDFQp60.60 139.6
OURS (THREE -STEP )p64.33 141.2
5 Conclusion and Discussion
In this paper, we propose two novel techniques: accurate activation clipping and adaptive batch
normalization to improve the accuracy of data-free quantization. First, we analyze the origin of
activation peak values and construct a new scheme to generate synthetic data that can restore the
activation range of the original dataset, thus helping better quantizing the activations. Next, we
analyze the mismatch between quantized featuremap distribution and statistics stored in BN layers.
We propose to update the BN statistics adaptively and let the BN layers adapt to the errors caused by
quantization. Finally, we combine the above two methods with an optional Ô¨Åne-tune module and get
a three-step quantization pipeline, which can further improve the accuracy, allowing users to strike
a balance between time consumption and results. Extensive experiments prove that our methods
outperforms the existed data-free quantization methods by a large margin.
9

--- PAGE 10 ---
We also Ô¨Ånd that, since the existing methods mainly focus on updating the clipping range of activation
values, they do not work well with models like MobileNet that do not use traditional ReLU as the
activation function. How to quantize networks like MobileNet effectively is a question worth studying
in the future.
References
[1]Y . Cai, Z. Yao, Z. Dong, A. Gholami, and K. Keutzer. Zeroq: A novel zero shot quantization
framework. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) , 2020.
[2]K. Choi, D. Hong, N. Park, Y . Kim, and J. Lee. Qimera: Data-free quantization with synthetic
boundary supporting samples. Advances in Neural Information Processing Systems , 34, 2021.
[3] J. Deng. Imagenet : A large-scale hierarchical image database. Proc. CVPR, 2009 , 2009.
[4]J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. 2018.
[5]K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.
[6]Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s.
arXiv preprint arXiv:2201.03545 , 2022.
[7]M. Nagel, M. V . Baalen, T. Blankevoort, and M. Welling. Data-free quantization through weight
equalization and bias correction. In 2019 IEEE/CVF International Conference on Computer
Vision (ICCV) , 2019.
[8]M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. C. Chen. Inverted residuals and linear
bottlenecks: Mobile networks for classiÔ¨Åcation, detection and segmentation. 2018.
[9]K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. Computer Science , 2014.
[10] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architec-
ture for computer vision. IEEE , pages 2818‚Äì2826, 2016.
[11] C.-Y . Wang, A. Bochkovskiy, and H.-Y . M. Liao. Scaled-YOLOv4: Scaling cross stage
partial network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 13029‚Äì13038, June 2021.
[12] S. Xu, H. Li, B. Zhuang, J. Liu, J. Cao, C. Liang, and M. Tan. Generative low-bitwidth data
free quantization. 2020.
[13] A. You, X. Li, Z. Zhu, and Y . Tong. Torchcv: A pytorch-based framework for deep learning in
computer vision. https://github.com/donnyyou/torchcv , 2019.
[14] X. Zhang, H. Qin, Y . Ding, R. Gong, Q. Yan, R. Tao, Y . Li, F. Yu, and X. Liu. Diversifying sam-
ple generation for accurate data-free quantization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 15658‚Äì15667, 2021.
[15] Y . Zhong, M. Lin, G. Nan, J. Liu, B. Zhang, Y . Tian, and R. Ji. Intraq: Learning synthetic
images with intra-class heterogeneity for zero-shot network quantization. arXiv preprint
arXiv:2111.09136 , 2021.
[16] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for
discriminative localization. In CVPR , 2016.
6 Appendix
6.1 Implementation details of toy experiment
In the toy experiment shown in Figure 4, we let the model to be an identity transform. Given a target
label, we calculate different loss functions and backpropagate to the input, which will make the value
of target class higher during iteration. We run both experiments for 300 iterations. The algorithm is
summarized below.
10

--- PAGE 11 ---
Algorithm 1 Toy experiment of optimizing by different loss functions
Require: Identity transform model M, a loss functionL, random data x2Rnand a target label
t2[0;n 1].
fori= 1tonum _iterations do
Forward propagate M(x) =x
Calculate loss value according to the choice of loss function L
Backward propagate and update x
end for
Return: Target value xt
11
