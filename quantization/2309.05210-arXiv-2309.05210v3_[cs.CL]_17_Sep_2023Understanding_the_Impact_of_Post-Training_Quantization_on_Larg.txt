# 2309.05210.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2309.05210.pdf
# File size: 113213 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2309.05210v3  [cs.CL]  17 Sep 2023Understanding the Impact of Post-Training Quantization on Large Language
Models
Somnath Roy
Freshworks Inc
somnath.roy@freshworks.com
Abstract
Large language models (LLMs) are rapidly increasing in size ,
with the number of parameters becoming a key factor in the
success of many commercial models, such as ChatGPT, Claude,
and Bard. Even the recently released publicly accessible mo d-
els for commercial usage, such as Falcon and Llama2, come
equipped with billions of parameters. This signiﬁcant incr ease
in the number of parameters makes deployment and operation
very costly. The remarkable progress in the ﬁeld of quantiza -
tion for large neural networks in general and LLMs in particu -
lar, has made these models more accessible by enabling them t o
be deployed on consumer-grade GPUs. Quantized models gen-
erally demonstrate comparable performance levels to their un-
quantized base counterparts. Nonetheless, there exists a n otable
gap in our comprehensive understanding of how these quantiz ed
models respond to hyperparameters, such as temperature, ma x
new tokens, and top k, particularly for the next word prediction.
The present analysis reveals that nf4 and fp4 are equally
proﬁcient 4-bit quantization techniques, characterized b y sim-
ilar attributes such as inference speed, memory consumptio n,
and the quality of generated content. Nevertheless, these q uan-
tization methods exhibit distinct behaviors at varying tem per-
ature settings, both in the context of smaller and larger mod -
els. Furthermore, the study identiﬁes nf4 as displaying gre ater
resilience to temperature variations in the case of the llam a2
series of models at lower temperature, while fp4 and fp4-dq
proves to be a more suitable choice for falcon series of model s.
It is noteworthy that, in general, 4-bit quantized models of vary-
ing sizes exhibit higher sensitivity to temperature in the r ange of
0.5 to 0.8, unlike their unquantized counterparts. Additio nally,
int8 quantization is associated with signiﬁcantly slower i nfer-
ence speeds, whereas unquantized bﬂoat16 models consisten tly
yield the fastest inference speeds across models of all size s.
Index Terms : post-training quantization, LLM, nf4, fp4, nf4-
dq, fp4-dq
1. Introduction
With the emergence of the Transformer architecture [1], a si g-
niﬁcant breakthrough was achieved, enabling the effective re-
tention of extensive long-range dependencies in tasks rela ted
to natural language processing, speech, and vision. The tra ns-
former architecture enables highly parallel training due t o se-
quence parallelism, which makes it possible to pretrain LLM s
with hundreds of billions of parameters [2, 3, 4]. The Big-be nch
[5] introduced over 200 benchmarks designed to assess the ca -
pabilities of Large Language Models(LLMs) through quantiﬁ -
cation and extrapolation. This diverse and intricately ela borated
set of benchmarks signiﬁcantly contributed to the intensiﬁ cation
of the race surrounding LLM development and advancement.The widespread adoption of LLMs on a substantial scale
gained traction following the successful establishment of Chat-
GPT (including GPT-3 and subsequent iterations) [2]. The pr e-
training of large transformer language models with 7 billio n pa-
rameters and beyond demands a considerable amount of GPU
computation, which can translate to costs amounting to mill ions
of dollars. Such level of expenditure is beyond what academi c
research and small organizations can typically afford. Des pite
the high cost of deploying and operating large language mod-
els (LLMs), the recent release of the Falcon [6] and Llama2 [7 ]
models has sparked optimism among small organizations and
has increased their desire to deploy their own custom LLMs.
The efﬁcient deployment of decoder only LLMs are chal-
lenging in practice because the generative inference proce eds
sequentially, where the computation for each token depends
on the previously generated tokens [8]. It is noteworthy tha t
caching the attention key and value tensors of each layer can
signiﬁcantly improve the inference speed of smaller decode r-
only models that ﬁt on a single GPU memory. However, this
is not possible for models that do not ﬁt into the memory of a
single GPU. To address the need for expensive high-end GPUs
to support the deployment of these models, diverse forms of
quantization have been put forward as potential solutions. The
application of quantization methods to transformers emerg es as
a efﬁcacious approach for mitigating sampling latency, whi le
incurring minimal to negligible impact on overall performa nce
[9]. Quantization techniques can be mainly characterized i nto
three forms namely - i) quantization aware training [10, 11] , ii)
quantization aware ﬁne-tuning [12, 13, 14], and iii) post tr aining
quantization (PTQ) [15, 16, 17]. In [18], the investigation pri-
marily centers on evaluating the impact of diverse post-tra ining
quantization methods, employing perplexity scores as a ben ch-
mark. The perplexity scores are computed on datasets such as
Wiki [19], PTB [20], and C4 [21], which mostl likely have
served as foundational datasets during the training of most of
the LLMs. It should be noted that these datasets are predis-
posed to exhibit favorable perplexity scores across all mod els,
owing to their utilization in model training. Furthermore, it is
acknowledged that perplexity, as a metric, may not effectiv ely
capture instances of repetitive generation within LLMs. Fo l-
lowing outlines the primary contributions of the present st udy.
1. This study offers a systematic examination of the inﬂuenc e
exerted by three pivotal hyper-parameters, namely, max new
tokens, temperature, and top k, on LLMs that have un-
dergone quantization through widely adopted post-trainin g
quantization techniques such as [15]1(hereafter, gptq) and
1https://github.com/IST-DASLab/gptq

--- PAGE 2 ---
[14, 12]23(hereafter, bitsandbytes).
2. It explores how these hyper-parameters exert their inﬂue nce
across a range of model sizes, spanning from 3 billion to 70
billion parameters.
3. The process involves generating a total of 6,300 samples
for each quantization method, achieved by constructing ten
smaller prompts that encompass a diverse spectrum of do-
mains for every model.
4. LLMs typically exhibit a tendency towards repetitive gen er-
ation, and it is often challenging to discern such repetitio n
through perplexity scores. Therefore, to identify and quan -
tify repetitive generation, the primary metric employed is the
number of duplicate content words.
5. It scrutinizes quantization methods that share similar i nfer-
ence speeds but manifest differing effects on accuracy.
6. Finally, it aims to discern the optimal quantization meth od
for deployment, considering speciﬁc constraints and requi re-
ments.
2. Quantization
Quantization is a well deﬁned mechanism for reducing the num -
ber of bits used to represent a value. In the context of large n eu-
ral network models, quantization reduces the precision of t he
model’s parameters and/or activations. Moreover, it has be en
found that the quantized large models are often competitive to
its base ones in terms of accuracy while reducing the computa -
tional requirements.
In the context of LLMs, the quantization process can be divid ed
into two types namely i) simulated and, ii) pure quantizatio n.
In simulated quantization, some operations are performed i n
ﬂoating-point arithmetic, which requires the dequantizat ion of
quantized parameters back to full precision during inferen ce.
[22, 23, 24, 25]. Pure quantization uses integer-only quant iza-
tion, which eliminates the need for dequantization during i nfer-
ence [26, 27, 28, 12, 29]. The main difference between these
two process of quantization is shown below in Table 1. How-
Features Simulated
QuantizationPure Quanti-
zation
Operations Floating and
Fixed pointFixed-point
Need for de-
quantizationYes No
Inference speed Slower Comparatively
Faster
Table 1: General understanding of simulated vs. pure quantiza-
tion in transformer based LLMs
ever, it is crucial to note that pure quantization is a more ag gres-
sive approach and can also lead to a greater loss of accuracy.
On the other hand, simulated quantization is a conservative ap-
proach and can achieve signiﬁcant speedups without sacriﬁc ing
too much accuracy. Pure quantization can be further catego-
rized into W8A8 and W4A4, where the weights and activations
are quantized to 8-bit integers and 4-bit integers, respect ively
[29] [27].
2https://github.com/TimDettmers/bitsandbytes
3https://github.com/artidoro/qlora2.1. GPTQ
It is a layer-wise quantization method based on the Optimal
Brain Quantization (OBQ) [30]. The goal is to ﬁnd a quantized
weight matrix /tildewiderWthat minimizes the squared error between the
quantized layer output /tildewiderWX and the full-precision layer output
WX as shown below.
argmin
/tildewiderW/bardblWX−/tildewiderWX/bardbl2
The OBQ algorithm iteratively quantizes one weight at a time ,
while the GPTQ algorithm utilizes a vectorized implementat ion
that allows it to efﬁciently handle multiple rows of the weig ht
matrix in parallel. This makes GPTQ signiﬁcantly faster tha n
OBQ, especially for large models.
2.1.1. GPU Memory Consumption in 4-bit GPTQ Quantization
It is well-established that the goal of quantization is to de ploy
LLMs on consumer-grade GPUs having at most 24 GB.The dis-
tribution of GPU memory utilised by the models during GPTQ
4-bit quantization is shown below in Table 2. GPTQ quantiza-
tion has following limitations.
• It is very GPU memory intensive process.
• Even 4-bit quantization of 40B model throws out of mem-
ory (OOM) on 80GB A100 GPU machine. Moreover, it is
not possible to quantize 7B models on 24GB A10 GPU ma-
chines.
Model GPU Memory(GB)
stablelm 3b 19.54
redpajama 3b 9.58
falcon 7b 23.64
llama2 7b 24.83
llama2 13b 40.46
falcon 40b and
llama2 70bOOM on single A100 80GB
GPU
Table 2: Distribution of GPU memory consumed by GPTQ 4-
bits quantization for different models evaluated on Nvidia A100
80GB GPU machine
2.1.2. Layerwise Error induced by GPTQ
GPTQ 4-bit quantization reduces the size of a model by more
than 80%, i.e., a model of 14 GB is reduced to around 2 GB post
quantization. It is important to note that the quantization error
introduced by GPTQ is different for different models, as sho wn
in Table 3. This is because the models shown in Table 3 have
different architectures, including the number of heads, nu mber
of layers, embedding dimension, number of query groups in
multi-query attention, block size, and hidden dimension.
2.2. bitsandbytes Quantizations
bitsandbytes (bnb) provides implementation of ﬁve powerfu l
and state-of-the-art quantization techniques namely i) in t8, ii)
fp4, iii) nf4, iv) fp4-dq4, and v) nf4-dq. The int8 quantization
procedure[14] uses vector-wise quantization with separat e nor-
malization constants for each inner product in the matrix mu l-
tiplication. However, they have found around 0.1% dominant
4dq stands for double quantization

--- PAGE 3 ---
Model mlp.proj att.proj mlp.fc attn.attn
stablelm 3b 52850.7 12638.9 383200.9 844806.3
redpajama 3b 23448.1 1048.9 137061.9 138947.9
falcon 7b 19194.83 2362.39 149962.4 32886.3
llama2 7b 22773.0 3198.7 170837.6 248520.0
llama2 13b 27829.5 5470.5 247389.2 301002.0
Table 3: Quantization error introduced by GPTQ in mlp projection, at tention projection, fully connected and attention layers.
activation outliers that has the potential to degrade the qu ality
especially in bigger LLMs. Therefore, the precision for the se
dominant outliers are kept in ﬂoat16. This scheme isolates t he
outlier feature dimensions into a 16-bit matrix multiplica tion,
while still allowing more than 99.9% of the values to be multi -
plied in 8 bits.
QLoRA[12] introduced a new data type called 4-bit normal-
ﬂoat (nf4), which is optimal for normally distributed weigh ts,
double quantization to reduce the memory footprint, and pag ed
optimization to manage memory spikes. These techniques to-
gether yield excellent inference speed without sacriﬁcing the
quality of generation. In nf4 quantization, the base model
weights are stored in nf4 data type and computation is per-
formed in bﬂoat16. However, the model weights are dequan-
tized to bﬂoat16 in the foward pass for inference [31]. The bn b
quantizations compress the model footprint in the range of 4 0%
(int8) to 70% (nf4-dq). It is important to emphasize here tha t
int8 quantization for llama2 70B throws OOM error on A100
80GB GPU machine. Rest of the details of compressed model
size corresponding to bnb quantizations are described in th e fol-
lowing sections.
3. Experiment
This section provides a detailed description of the models,
prompts, decoding approach, and related hyper-parameters used
to generate the data for the analysis.
3.1. Model Description
A total of six pre-trained models with 3 billion to 70 billion
parameters were selected for next-word prediction. These m od-
els are decoder-only, and their architecture-speciﬁc deta ils are
shown in Table 4. As can be seen, these models differ from
each other in terms of the number of heads, number of layers,
embedding dimension, number of query groups used in multi-
query attention, sequence length, and intermediate size.
3.2. Prompts Selection and Proposed Hypothesis
Ten prompts are designed to access the quality and inference
speed of pre-trained models for next word generation. These
prompts are selected on the simple proposed hypothesis and
shown below in Table 5.
Hypothesis 1: All pre-trained LLMs trained on billions or
trillions of tokens can be ideally conceptualized as a large tree,
where each node represents a topic and the text continuation s
associated with that topic. As we traverse down the tree, the text
continuations become more speciﬁc and focused. Conversely ,
as we traverse up the tree, the text continuations become mor e
general and abstract.
Hypothesis 2: The quality of a pre-trained model can be
assessed based on its ability to accurately identify the cor rect
topic node and then traverse to the sub-topic node for focuse d
next word prediction.3.3. Decoder Description
The current experiment uses a bare top k sampling decoder
without any additional features, such as repetition penalt y. To
assess the model’s potential, we use a list of max new tokens,
temperature as well as top k. The max new tokens, tempera-
ture and top k are [50, 100, 150, 200, 250, 300, 350, 400, 450,
500], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] and [1, 5, 1 0, 20,
50, 100, 200] respectively. The completion text is generate d for
every quantized models using all the combinations of max new
tokens, temperature and top k. The reason for high top k such
as 200 is that it might allow models to choose more diverse, le ss
repetitive, and semantically coherent text.
4. Analysis
A total of 6300 (10 prompts ×10 max new tokens ×9 tem-
perature×7 top k) completion text is generated for each quan-
tized model except falcon 40b and llama2 70b for 16bit5. The
evaluation of these completion texts is conducted through t he
computation of counting the duplicate content words, servi ng
as a metric for assessing the quality of the generated text. T he
content words are the remaining words after removing the sto p
words. Additionally, the model’s size in gigabytes (GB) ser ves
as a key measure for quantifying GPU memory consumption,
while tokens/sec is employed as a metric to gauge the model’s
inference speed.
4.1. Memory Consumption and Inference Speed
The utilization of int8 quantization demonstrates a signiﬁ cant
reduction in memory consumption, approximately in the rang e
of 40% to 50%, when compared to bﬂoat16, as illustrated in
Table 6. Nonetheless, it is important to note that this enhan ce-
ment is accompanied by a corresponding trade-off in inferen ce
speed, with int8 exhibiting a slowdown of roughly 75% to 80%
in comparison to bﬂoat16, as indicated in Table 7.
When evaluating memory consumption between the fp4
and nf4 quantization approaches for model sizes up to 13 bil-
lion, their distinctions are negligible. However, nf4 quan tization
exhibits a slight advantage over fp4 in terms of memory con-
sumption for larger models such as falcon 40b and llama2 70b.
Nevertheless, fp4-dq is found to be better in memory consump -
tion (i.e., takes less memory) across the models compared to
its counterpart, as shown in Table 6. It is worth noting that
while double quantization offers a clear advantage in memor y
consumption, it results in an inference speed reduction of a p-
proximately 10% to 25% compared to the absence of double
quantization, as outlined in Table 7.
In conclusion, among the various quantization methods,
5Both falcon 40b and llama2 70b encounter OOM errors on an
80GB GPU machine. falcon 40b throws an OOM error when the max
new tokens exceeds 400, while llama2 70b faces an OOM error during
the loading process.

--- PAGE 4 ---
Model nhead nlayer embed dim nquery groups mqa block size intermediate size
stablelm 3b 32 16 4096 32 4096 16384
redpajama 3b 32 32 2560 32 2048 10240
falcon 7b 71 32 4544 1 2048 18176
llama2 7b 32 32 4096 32 4096 11008
llama2 13b 40 40 4096 32 5120 13824
falcon 40b 128 60 8192 8 2048 32768
llama2 70b 64 80 8192 8 5120 28672
Table 4: Descrption of most relevant architectural specs of the pre- trained models used during the experiment
Prompt General Expected Continuation
Life in London Travel/Cultural/Work-Related/London speciﬁc stuff
It is easy to be a techie Comparison of techie with other probable roles in tech secto r
Stock brokers are earning Stock brokers and their earning style, sources, etc
It looks like written by Shakespeare Shakespeare style text comparison
Hello, my name is Chat or Introduction
Global warming and AI Global Warming and AI in general as well as their +ive and -ive association
Current world order Essay/Discussion/Power and Politics related tow world ord er
Percentage of people adore actors and singers Stats on people following their favourite actors/singers a nd discussion on the related topic
Exercise and eating habits for Eating habits and exercise routine in general (pros and cons )
Millennial and genz Comparison and contrast between millennial and genz
Table 5: Prompts Description
bﬂoat16 stands out as the least efﬁcient in terms of memory
consumption. However, it excels in terms of inference speed ,
except in the case of stablelm 3b.
4.2. Temperature vs. Quality of Generation
A common pattern emerges within all quantization approache s,
wherein an increase in temperature correlates with an eleva tion
in number of duplicate content words except for bﬂoat16. How -
ever, it is worth noting that some models are more sensitive t o
even temperature lower than 0.5 compared to others.
When comparing the performance of stablelm 3b and red-
pajama 3b models, it becomes evident that the fp4 and nf4-dq
quantization methods exhibit suboptimal results, charact erized
by an increased occurrence of duplicate words at lower tem-
perature settings. However, the situation varies when cons ider-
ing falcon models, where nf4 quantization consistently dem on-
strates inferior performance across the entire temperatur e spec-
trum in comparison to other quantization methods.
In contrast, when assessing llama2 models, the situation be -
comes more nuanced, with most quantization approaches con-
tributing signiﬁcantly to repetitive generation. In this c ontext,
determining a clear front-runner among these methods prove s to
be a challenging task. Nevertheless, it is noteworthy that f or the
llama2 70b model, both fp4 and fp4-dq quantization methods
outshine the others in terms of performance.
The analysis reveals that the int8-quantized model demon-
strates effective control over the occurrence of duplicate content
words for both llama2 13b and llama2 70b, effectively limiting
them in the range of 40. In contrast, the bﬂoat16 models ex-
hibit a characteristic of independence from temperature sc aling,
as they consistently generate a comparable number of repeti tive
words across all temperature settings except redpajama 3b.
4.3. Max Returned Tokens vs. Quality of Generation
The term max returned tokens encompasses the combined value
of max new tokens and the length of the input prompt in terms oftokens. the analyis reveals that an the count of duplicate wo rds
generated linearly increases with the increase of max retur ned
tokens across all models and quantization methods.
4.4. Top k vs. Quality of Generation
The analysis offers a somewhat surprising insight, indicat ing
that setting top k equal to 1 tends to result in the lowest occur-
rence of duplicate words across models and quantization met h-
ods. Nonetheless, it’s noteworthy that this effect reaches a point
of saturation and loses distinctiveness when top k is equal to or
greater than 5.
4.5. Overall Comparison
In terms of the average number of duplicate content words6gen-
erated in absolute terms, our analysis reveals the followin g in-
sights:
• For fp4 and fp4-dq compared to nf4 and nf4-dq across var-
ious models (except llama2 series), there is a consistent re -
duction in repetitive generation, typically ranging from 1 2%
to 20% relative.
• In the case of nf4 and nf4-dq for llama2 models of differ-
ent sizes, there is a more noticeable advantage, with relati ve
reduction of 9% to 11% in repetitive generation.
• Int8 quantization has a more pronounced limitation on the
number of generated words, producing approximately 30-
50% fewer content words than 4-bit quantization. Addition-
ally, it produces 25-40% more duplicate content words rela-
tive to 4-bit quantization at normalized scale.
• When comparing bﬂoat16 with 4-bit quantization, it’s note -
worthy that bﬂoat16 generally produces more number of con-
tent words, often ranging from approximately 3% to 10%.
Nonetheless, bﬂoat16 tends to generate a marginally higher
6The total number of content words generated for the unquanti zed
model lies in the range of 1.34M to 1.45M and the maximum dupli cate
number of words is around 80K.

--- PAGE 5 ---
Model bnb.nf4 bnb.nf4-dq bnb.fp4 bnb.f4-dq bnb.int8 bﬂoat16
stablelm 3b 3.22 3.20 3.22 3.06 4.68 7.42
redpajama 3b 2.31 2.17 2.31 2.17 3.52 5.60
falcon 7b 5.72 5.37 5.72 5.37 8.71 14.50
llama2 7b 4.58 4.27 4.58 4.27 7.82 13.53
llama2 13b 8.83 7.8 8.83 7.8 14.2 26.23
falcon 40b 26.40 24.64 26.55 24.64 44.52 80.85
llama2 70b 40.23 38.2 40.4 38.2 70.44 -
Table 6: The distribution of memory consumed (lower is better) for al l the models for different quantization evaluated on Nvidia A100
80GB GPU machine.
Model bnb.nf4 bnb.nf4-dq bnb.fp4 bnb.f4-dq bnb.int8 bﬂoat 16
stablelm 3b (37.76, 62.79) (38.7, 53.37) (42.99, 63.11) (38.7, 53.03) (7.91, 16.81) (37.76, 49.88)
redpajama 3b (24.2, 32.29) (22.59, 27.04) (25.64, 31.37) (15.49, 27.0 8) (2.52, 3.24) (29.35, 37.85)
falcon 7b (29.09, 37.54) (22.71, 30.04) (24.77, 37.41) (22.23, 30. 7) (3.13, 12.63) (35.79, 48.05)
llama2 7b (23.09, 29.88) (19.32, 23.44) (23.01, 28.65) (17.85, 23. 41) (1.32, 8.87) (28.39, 36.35)
llama2 13b (15.9, 23.14) (13.22, 18.84) (12.0, 22.98) (10.83, 18.2 2) (6.49, 7.14) (24.12, 29.34)
falcon 40b (11.93, 16.59) (11.57, 14.51) (12.12, 16.61) (10.42, 12 .76) (3.56, 4.63) (12.37, 13.99)
llama2 70b (8.67, 10.39) (6.47, 9.07) (8.52, 10.23) (6.39, 8.82) (2 .79, 3.76) -
Table 7: The distribution of minimum and maximum inference speed (hi gher is better) in tokens/sec for different quantization ev aluated
on Nvidia A100 80GB GPU machine.
duplicate words, indicating relative inferiority of 1% to 3 .5%
with 4bit quantization.
The computation of average perplexity scores, with a token
stride of 512, is conducted for all quantization levels acro ss each
model. An examination of these scores reveals that the perpl ex-
ity values for all models reside within a relatively constra ined
range, typically ranging from 12 to 15. Consequently, it is d is-
cerned that perplexity, within this context, may not serve a s a
suitable metric for assessing the quality of the generated t ext.
5. Conclusions
In scenarios where GPU memory is not a limiting factor and
the utmost priority is placed on achieving both high inferen ce
speed and accuracy, it is advisable to prioritize the utiliz ation
of bﬂoat16 for models up to 7 billions. This preference arise s
due to its reduced susceptibility to variations in temperat ure and
max new tokens. Moreover, model upto 7 billion size effec-
tively ﬁts into a consumer grade GPU machine. Alternatively ,
nf4 and fp4 serves as the default choice for individuals seek -
ing a balance between GPU utilization, accuracy and inferen ce
speed, thus offering a middle-ground solution that combine s all
aspects effectively.
It’s worth noting that the adoption of double quantization,
such as fp4-dq and nf4-dq, can lead to a marginal reduction in
memory footprint. However, it is accompanied by a relativel y
decreased inference speed. Hence, the recommendation lean s
toward using quantization without the doubling approach. A d-
ditionally, when considering the nf4 and fp4 precision comb i-
nation, it is recommended to use a temperature of less than 0. 5,
exactly 1.0, or a combination of these values to achieve opti mal
performance.
The current evaluation does not consider int8 to be a fea-
sible alternative to other quantization methods. While int 8 re-
duces memory usage, it signiﬁcantly slows down inference an d
produces around 30-50% fewer words than other quantization
methods.It is important to note that the current experiment did not
achieve satisfactory results in terms of accuracy and infer ence
speed when using gptq 4-bit quantization. Further investig ation
is needed to replicate the comparable performance that has b een
reported in other studies7. Therefore, this result is not included
in the analysis presented.
6. Limitations and Future Work
The current study is conducted on 7 models ranging in size fro m
3 billion to 70 billion parameters, and 10 prompts are used fo r
next-word prediction using various combinations of hyperp a-
rameters. Further study with more models (containing ≤1 bil-
lion parameters) and prompts might provide more insights in to
the effects of these hyperparameters on relatively smaller quan-
tized LLMs.
Future work will focus on the primary causes of repetitive
generation and their relationship to Hypothesis 1 and Hypot he-
sis 2. Moreover, the results show that falcon has a faster inf er-
ence speed than llama2 in the 7B category. However, falcon ha s
a higher number of overall parameters than llama2. Therefor e,
future research will focus on model-speciﬁc factors that af fect
inference speed.
7. References
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones , A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you ne ed,”
Advances in neural information processing systems , vol. 30, 2017.
[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dh ari-
wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Lan-
guage models are few-shot learners,” Advances in neural informa-
tion processing systems , vol. 33, pp. 1877–1901, 2020.
[3] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann
et al. , “Palm: Scaling language modeling with pathways,” arXiv
preprint arXiv:2204.02311 , 2022.
7https://github.com/PanQiWei/AutoGPTQ

--- PAGE 6 ---
[4] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhan dari,
J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti
et al. , “Using deepspeed and megatron to train megatron-turing
nlg 530b, a large-scale generative language model,” arXiv preprint
arXiv:2201.11990 , 2022.
[5] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid,
A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-
Alonso et al. , “Beyond the imitation game: Quantifying and ex-
trapolating the capabilities of language models,” arXiv preprint
arXiv:2206.04615 , 2022.
[6] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cap-
pelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Lau-
nay, “The reﬁnedweb dataset for falcon llm: outperforming c u-
rated corpora with web data, and web data only,” arXiv preprint
arXiv:2306.01116 , 2023.
[7] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al. ,
“Llama 2: Open foundation and ﬁne-tuned chat models,” arXiv
preprint arXiv:2307.09288 , 2023.
[8] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury ,
J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efﬁciently scal-
ing transformer inference,” Proceedings of Machine Learning and
Systems , vol. 5, 2023.
[9] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and
J. Jumper, “Accelerating large language model decoding wit h
speculative sampling,” arXiv preprint arXiv:2302.01318 , 2023.
[10] G. Yang, D. Lo, R. Mullins, and Y . Zhao, “Dynamic stashin g
quantization for efﬁcient transformer training,” arXiv preprint
arXiv:2303.05295 , 2023.
[11] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi,
R. Krishnamoorthi, and V . Chandra, “Llm-qat: Data-free qua nti-
zation aware training for large language models,” arXiv preprint
arXiv:2305.17888 , 2023.
[12] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoye r,
“Qlora: Efﬁcient ﬁnetuning of quantized llms,” arXiv preprint
arXiv:2305.14314 , 2023.
[13] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim,
J.-W. Ha, N. Sung, and D. Lee, “Alphatuning: Quantization-a ware
parameter-efﬁcient adaptation of large-scale pre-traine d language
models,” arXiv preprint arXiv:2210.03858 , 2022.
[14] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer, “ Llm. int8
(): 8-bit matrix multiplication for transformers at scale, ”arXiv
preprint arXiv:2208.07339 , 2022.
[15] E. Frantar, S. Ashkboos, T. Hoeﬂer, and D. Alistarh, “Gp tq: Ac-
curate post-training quantization for generative pre-tra ined trans-
formers,” arXiv preprint arXiv:2210.17323 , 2022.
[16] Z. Yuan, L. Niu, J. Liu, W. Liu, X. Wang, Y . Shang, G. Sun,
Q. Wu, J. Wu, and B. Wu, “Rptq: Reorder-based post-
training quantization for large language models,” arXiv preprint
arXiv:2304.01089 , 2023.
[17] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “Awq :
Activation-aware weight quantization for llm compression and ac-
celeration,” arXiv preprint arXiv:2306.00978 , 2023.
[18] Z. Yao, C. Li, X. Wu, S. Youn, and Y . He, “A comprehensive
study on post-training quantization for large language mod els,”
arXiv preprint arXiv:2303.08302 , 2023.
[19] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointe r sentinel
mixture models,” arXiv preprint arXiv:1609.07843 , 2016.
[20] M. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Buil ding a
large annotated corpus of english: The penn treebank,” 1993 .
[21] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. M atena,
Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfe r
learning with a uniﬁed text-to-text transformer,” The Journal of
Machine Learning Research , vol. 21, no. 1, pp. 5485–5551, 2020.
[22] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Ma-
honey, and K. Keutzer, “Q-bert: Hessian based ultra low prec ision
quantization of bert,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , vol. 34, no. 05, 2020, pp. 8815–8821.[23] A. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos, “Gobo:
Quantizing attention-based nlp models for low latency and e n-
ergy efﬁcient inference,” in 2020 53rd Annual IEEE/ACM In-
ternational Symposium on Microarchitecture (MICRO) . IEEE,
2020, pp. 811–824.
[24] H. Bai, W. Zhang, L. Hou, L. Shang, J. Jin, X. Jiang, Q. Liu ,
M. Lyu, and I. King, “Binarybert: Pushing the limit of bert qu an-
tization,” arXiv preprint arXiv:2012.15701 , 2020.
[25] W. Zhang, L. Hou, Y . Yin, L. Shang, X. Chen, X. Jiang, and
Q. Liu, “Ternarybert: Distillation-aware ultra-low bit be rt,”arXiv
preprint arXiv:2009.12812 , 2020.
[26] S. Kim, A. Gholami, Z. Yao, M. W. Mahoney, and K. Keutzer, “I-
bert: Integer-only bert quantization,” in International conference
on machine learning . PMLR, 2021, pp. 5506–5518.
[27] Z. Yao, C. Li, X. Wu, S. Youn, and Y . He, “A comprehensive
study on post-training quantization for large language mod els,”
arXiv preprint arXiv:2303.08302 , 2023.
[28] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,
“Smoothquant: Accurate and efﬁcient post-training quanti zation
for large language models,” in International Conference on Ma-
chine Learning . PMLR, 2023, pp. 38 087–38 099.
[29] X. Wu, C. Li, R. Y . Aminabadi, Z. Yao, and Y . He, “Un-
derstanding int4 quantization for transformer models: La-
tency speedup, composability, and failure cases,” arXiv preprint
arXiv:2301.12017 , 2023.
[30] E. Frantar and D. Alistarh, “Optimal brain compression : A frame-
work for accurate post-training quantization and pruning, ”Ad-
vances in Neural Information Processing Systems , vol. 35, pp.
4475–4488, 2022.
[31] Y . Belkada, T. Dettmers, A. Pagnoni, S. Gugger, and
S. Mangrulkar, “Making llms even more accessible with bitsa nd-
bytes, 4-bit quantization and qlora,” 2023. [Online]. Avai lable:
https://huggingface.co/blog/4bit-transformers-bitsa ndbytes
