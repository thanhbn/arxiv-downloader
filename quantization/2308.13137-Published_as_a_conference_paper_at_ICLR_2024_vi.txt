# 2308.13137.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2308.13137.pdf
# Kích thước tệp: 1020535 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
OMNIQUANT : LƯỢNG TỬ HÓA HIỆU CHUẨN ĐA HƯỚNG
CHO CÁC MÔ HÌNH NGÔN NGỮ LỚN
Wenqi Shao†1, Mengzhao Chen†1, Zhaoyang Zhang3, Peng Xu1,2, Lirui Zhao1,
Zhiqian Li2,Kaipeng Zhang1,Peng Gao1,Yu Qiao1,Ping Luo∗1,2
1OpenGVLab, Phòng thí nghiệm AI Thượng Hải 2Đại học Hồng Kông
3Đại học Trung văn Hồng Kông

TÓM TẮT
Các mô hình ngôn ngữ lớn (LLM) đã cách mạng hóa các tác vụ xử lý ngôn ngữ tự nhiên. Tuy nhiên, việc triển khai thực tế của chúng bị cản trở bởi yêu cầu bộ nhớ và tính toán khổng lồ. Mặc dù các phương pháp lượng tử hóa sau huấn luyện (PTQ) gần đây có hiệu quả trong việc giảm dung lượng bộ nhớ và cải thiện hiệu quả tính toán của LLM, chúng tạo ra các tham số lượng tử hóa thủ công, dẫn đến hiệu suất thấp, đặc biệt trong lượng tử hóa bit cực thấp. Để giải quyết vấn đề này, chúng tôi giới thiệu kỹ thuật Lượng tử hóa hiệu chuẩn đa hướng (OmniQuant) cho LLM, đạt được hiệu suất tốt trong các cài đặt lượng tử hóa đa dạng trong khi duy trì hiệu quả tính toán của PTQ bằng cách tối ưu hóa hiệu quả các tham số lượng tử hóa khác nhau. OmniQuant bao gồm hai thành phần sáng tạo gồm Cắt trọng số có thể học (LWC) và Biến đổi tương đương có thể học (LET). LWC điều chỉnh các giá trị cực trị của trọng số bằng cách tối ưu hóa ngưỡng cắt. Trong khi đó, LET giải quyết các ngoại lệ kích hoạt bằng cách chuyển thách thức lượng tử hóa từ kích hoạt sang trọng số. Hoạt động trong khung có thể vi phân sử dụng tối thiểu hóa lỗi theo khối, OmniQuant có thể tối ưu hóa quá trình lượng tử hóa hiệu quả cho cả lượng tử hóa chỉ trọng số và lượng tử hóa trọng số-kích hoạt. Ví dụ, họ mô hình LLaMA-2 có kích thước 7-70B có thể được xử lý với OmniQuant trên một GPU A100-40G duy nhất trong vòng 1-16 giờ sử dụng 128 mẫu. Các thí nghiệm mở rộng xác nhận hiệu suất vượt trội của OmniQuant qua các cấu hình lượng tử hóa đa dạng như W4A4 (trọng số 4-bit, kích hoạt 4-bit), W6A6, W4A16, W3A16, và W2A16. Ngoài ra, OmniQuant thể hiện hiệu quả trong các mô hình được điều chỉnh theo hướng dẫn và mang lại cải thiện đáng kể về tốc độ suy luận và giảm bộ nhớ trên các thiết bị thực. Mã nguồn có sẵn tại https://github.com/OpenGVLab/OmniQuant .

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) như GPT-4 (Bubeck et al., 2023) và LLaMA (Touvron et al., 2023a), đã thể hiện hiệu suất ấn tượng qua các bài kiểm tra ngôn ngữ tự nhiên khác nhau (Hendrycks et al., 2020; Zellers et al., 2019). Hơn nữa, khả năng hiểu ngôn ngữ vốn có trong LLM có thể được chuyển giao thành công vào các mô hình đa phương thức (Mu et al., 2023; Xu et al., 2023; Zhang et al., 2023a; Huang et al., 2024; 2023). Do đó, LLM có thể được coi là tiền thân của trí tuệ nhân tạo tổng quát (Bubeck et al., 2023). Tuy nhiên, yêu cầu tính toán và bộ nhớ đáng kể của LLM đặt ra những thách thức lớn (Zhang et al., 2023b; Hu et al., 2023). Ví dụ, mô hình GPT-3 (Brown et al., 2020) yêu cầu 350G bộ nhớ để tải các tham số của nó ở định dạng FP16, tương ứng với yêu cầu ít nhất năm GPU A100-80G cho suy luận. Nhu cầu đáng kể này về tài nguyên tính toán và chi phí truyền thông liên quan cản trở việc triển khai thực tế LLM trong các ứng dụng thế giới thực.

Lượng tử hóa đã được chứng minh là có triển vọng để giảm thiểu cả chi phí tính toán và bộ nhớ trong LLM. Nói chung, nó có hai loại bao gồm lượng tử hóa sau huấn luyện (PTQ) và huấn luyện nhận biết lượng tử hóa (QAT). Mặc dù QAT có thể dẫn đến độ chính xác cạnh tranh hơn PTQ, nó không thực tế do chi phí huấn luyện cao vì toàn bộ mô hình được huấn luyện với nhận thức về quá trình lượng tử hóa. Kết quả là, PTQ thường được sử dụng trong các phương pháp lượng tử hóa hiện tại trên LLM. Ví dụ, nhiều phương pháp PTQ (Frantar et al., 2022; Lin et al., 2023; Dettmers et al., 2023b) giảm tiêu thụ bộ nhớ bằng lượng tử hóa chỉ trọng số, lượng tử hóa trọng số trong khi duy trì kích hoạt độ chính xác đầy đủ. Để giảm thêm chi phí tính toán, một hướng nghiên cứu khác (Xiao et al., 2023; Wei et al., 2022; Yuan et al., 2023; Wei et al., 2023; Liu et al., 2023a) sử dụng lượng tử hóa trọng số-kích hoạt, lượng tử hóa cả trọng số và kích hoạt thành các giá trị bit thấp để thực hiện phép nhân ma trận bit thấp.

Các phương pháp lượng tử hóa hiện có đã thể hiện những thành tựu đáng kể trong các tình huống khác nhau, bao gồm lượng tử hóa chỉ trọng số W4A16 (tức là trọng số 4-bit và kích hoạt 16-bit) như (Lin et al., 2023; Dettmers et al., 2023b; Lee et al., 2023), cũng như lượng tử hóa trọng số-kích hoạt W8A8 (Wei et al., 2023). Tuy nhiên, chúng thường biểu hiện suy giảm hiệu suất đáng kể khi đối mặt với lượng tử hóa bit thấp, như W2A16 và W4A4, như được minh họa trong Hình 1 (b & c). Sự thiếu hụt hiệu suất này trong lượng tử hóa bit thấp có thể được quy cho việc các phương pháp này (Frantar et al., 2022; Lin et al., 2023; Wei et al., 2023) chủ yếu dựa vào các tham số lượng tử hóa thủ công như cường độ di chuyển (Xiao et al., 2023) và tham số tỷ lệ (Wei et al., 2023), thường dẫn đến hiệu suất thấp hơn. Mặc dù Huấn luyện nhận biết lượng tử hóa (QAT) (Liu et al., 2023b) có hiệu quả trong việc xác định cấu hình lượng tử hóa tối ưu, nó giới thiệu chi phí huấn luyện đáng kể về cả hiệu quả huấn luyện và dữ liệu. Do đó, việc lượng tử hóa LLM bằng các kỹ thuật dựa trên QAT như LLMQAT (Liu et al., 2023b) rất khó khăn. Ví dụ, GPTQ (Frantar et al., 2022), một phương pháp PTQ, có thể hoàn thành lượng tử hóa LLaMA-13B trong một giờ sử dụng 128 mẫu trên một GPU A100, trong khi LLM-QAT (Liu et al., 2023b) yêu cầu 100k mẫu và hàng trăm giờ GPU. Điều này dẫn chúng ta đến một câu hỏi trung tâm: liệu chúng ta có thể đạt được hiệu suất của QAT, trong khi duy trì hiệu quả thời gian và dữ liệu của PTQ?

Bài báo này giới thiệu một kỹ thuật lượng tử hóa mới, OmniQuant, giải quyết hiệu quả câu hỏi trên. OmniQuant đạt được hiệu suất tiên tiến qua các tình huống lượng tử hóa khác nhau, đặc biệt trong các cài đặt bit thấp, trong khi bảo tồn hiệu quả thời gian và dữ liệu của PTQ, như được minh họa trong Hình 1. Khác với Huấn luyện nhận biết lượng tử hóa (QAT) (Liu et al., 2023b) liên quan đến tối ưu hóa trọng số phức tạp, OmniQuant đóng băng trọng số độ chính xác đầy đủ gốc và chỉ kết hợp một số tham số lượng tử hóa có thể học. Như được hiển thị trong Hình 2, OmniQuant bao gồm hai thành phần chính kết hợp các loại tham số lượng tử hóa có thể học khác nhau, bao gồm Cắt trọng số có thể học (LWC) và Biến đổi tương đương có thể học (LET). Cụ thể, LWC điều chỉnh các giá trị cực trị của trọng số bằng cách tối ưu hóa ngưỡng cắt. Trong khi đó, LET giải quyết các ngoại lệ kích hoạt bằng cách học các biến đổi tương đương toán học trong một bộ mã hóa transformer.

Thay vì tối ưu hóa đồng thời tất cả tham số qua LLM, OmniQuant lượng tử hóa tuần tự các tham số của một lớp trước khi chuyển sang lớp tiếp theo dưới khung tối thiểu hóa lỗi lượng tử hóa theo khối. Theo cách này, OminiQuant có thể được tối ưu hóa hiệu quả sử dụng thuật toán Gradient Descent Ngẫu nhiên (SGD) đơn giản. Nhờ tối ưu hóa có thể vi phân, LWC và LET có thể được tích hợp liền mạch vào lượng tử hóa. Chúng tôi nhận thấy rằng LWC có thể giảm thiểu khó khăn trong lượng tử hóa trọng số và LET tiếp tục chuyển thách thức lượng tử hóa từ kích hoạt sang trọng số, tạo điều kiện cho OmniQuant trở thành khung lượng tử hóa đa năng cho cả lượng tử hóa chỉ trọng số và lượng tử hóa trọng số-kích hoạt. Đáng chú ý, OmniQuant không giới thiệu thêm tính toán hoặc tham số cho mô hình đã lượng tử hóa vì ngưỡng cắt trong LWC và các yếu tố tương đương trong LET có thể được hợp nhất vào trọng số đã lượng tử hóa.

Các mô hình FP khó lượng tử hóa (7B-70B)
Các mô hình FP dễ lượng tử hóa (7B-70B)
Các mô hình đã lượng tử hóa
Biến đổi tương đương
Cắt trọng số
lượng tử hóa
GPU A100-40G đơn
128 mẫu huấn luyện
Huấn luyện 1-16 giờ
Có thể học | Cố định

Hình 2: Đặc điểm của OmniQuant trên họ LLaMA.

Như được mô tả trong Hình 2, OmniQuant dễ thực hiện ngay cả với tài nguyên hạn chế. Đặc biệt, lấy họ mô hình LLaMA-2 (7B-70B) làm ví dụ, tất cả các mô hình có thể được lượng tử hóa trên một GPU A100-40G duy nhất chỉ sử dụng 128 mẫu huấn luyện. Thời gian huấn luyện dao động từ 1 đến 16 giờ, tùy thuộc vào kích thước của mô hình được lượng tử hóa, dao động từ 7B đến 70B. Nhờ tích hợp liền mạch LWC và LET đạt được bằng tối ưu hóa có thể vi phân, OmniQuant thể hiện hiệu suất vượt trội so với các phương pháp dựa trên PTQ trước đó trong các cài đặt lượng tử hóa khác nhau. Ví dụ, khi LLaMA-13B được lượng tử hóa thành W2A16, OmniQuant đạt được độ phức tạp 13.21, trong khi GPTQ gây ra sự gia tăng đáng kể về độ phức tạp lên 3832, như được thể hiện trong Hình 1. Một tiến bộ hiệu suất tương tự cũng được quan sát trong lượng tử hóa W4A4.

Các đóng góp của OmniQuant được tóm tắt như sau. 1) Chúng tôi hình thành một đường ống lượng tử hóa mới cho LLM, OmniQuant, đóng băng trọng số độ chính xác đầy đủ gốc trong khi kết hợp một tập hợp tham số có thể học bị hạn chế. OmniQuant truyền lượng tử hóa với cập nhật gradient trong khi bảo tồn hiệu quả thời gian và dữ liệu của các phương pháp PTQ. 2) OmniQuant bao gồm Cắt trọng số có thể học (LWC) và Biến đổi tương đương có thể học (LET). Các chiến lược này làm cho trọng số và kích hoạt độ chính xác đầy đủ phù hợp hơn với lượng tử hóa. 3) Thông qua các thí nghiệm mở rộng, chúng tôi chứng minh rằng OmniQuant vượt trội hơn các phương pháp trước đó qua một loạt cài đặt lượng tử hóa (W416, W3A16, W2A16, W6A6, W4A4), các họ mô hình khác nhau (OPT, LLaMA, LLaMA-2, LLaMA-2-chat, Falcon), và một loạt kích thước mô hình (125M-180B). Tăng tốc tính toán và giảm bộ nhớ của OmniQuant cũng được thể hiện trên các thiết bị thực.

2 CÔNG TRÌNH LIÊN QUAN

2.1 CÁC PHƯƠNG PHÁP LƯỢNG TỬ HÓA.

Lượng tử hóa giảm độ chính xác bit của mạng neural, dẫn đến các mô hình nhỏ hơn và suy luận nhanh hơn. Các phương pháp hiện tại được chia chủ yếu thành Huấn luyện nhận biết lượng tử hóa (QAT)(Liu et al., 2023b) và Lượng tử hóa sau huấn luyện (PTQ)(Xiao et al., 2023; Frantar et al., 2022). Trong khi QAT duy trì hiệu suất bằng cách mô phỏng lượng tử hóa trong quá trình huấn luyện, chi phí huấn luyện của nó làm cho nó không phù hợp với LLM. Các kỹ thuật PTQ như AdaRound (Nagel et al., 2020) và BRECQ (Li et al., 2021) sử dụng tối ưu hóa gradient để xác định làm tròn tối ưu, nhưng điều chỉnh tất cả trọng số tốn thời gian cho các mô hình lớn hơn. Do đó, hầu hết các phương pháp lượng tử hóa LLM (Xiao et al., 2023; Frantar et al., 2022; Dettmers et al., 2023b; Lee et al., 2023; Wei et al., 2023) ưu tiên PTQ không cần huấn luyện, hạn chế hiệu suất trong các tình huống bit thấp hơn. Mục tiêu của chúng tôi là tích hợp cập nhật gradient trong lượng tử hóa LLM, phản ánh phương pháp của QAT, trong khi giữ lại hiệu quả của PTQ.

2.2 LƯỢNG TỬ HÓA CỦA LLM.

Xem xét đối tượng được lượng tử hóa, lượng tử hóa LLM hiện có có thể được phân loại thành hai lĩnh vực: lượng tử hóa chỉ trọng số và lượng tử hóa trọng số-kích hoạt.

Lượng tử hóa chỉ trọng số. Lượng tử hóa chỉ trọng số tập trung vào việc chuyển đổi trọng số thành các giá trị bit thấp. Ví dụ, GPTQ (Frantar et al., 2022) sử dụng tái tạo theo khối cho lượng tử hóa 3/4-bit. SpQR (Dettmers et al., 2023b), OWQ (Lee et al., 2023), và AWQ (Lin et al., 2023) nhấn mạnh tầm quan trọng của trọng số gắn liền với kích hoạt có độ lớn cao hơn. Do đó, SpQR và OWQ sử dụng lượng tử hóa độ chính xác hỗn hợp để bảo vệ trọng số quan trọng, trong khi AWQ chọn tỷ lệ theo kênh để tránh sự không hiệu quả phần cứng của độ chính xác hỗn hợp. Qlora (Dettmers et al., 2023a) và INT2.1 (Chee et al., 2023) khôi phục khả năng của mô hình đã lượng tử hóa thông qua tinh chỉnh hiệu quả tham số. Phương pháp của chúng tôi, ngược lại, nâng cao quá trình lượng tử hóa trực tiếp, làm cho OmniQuant bổ sung cho Qlora và INT2.1.

Lượng tử hóa trọng số-kích hoạt. Lượng tử hóa trọng số-kích hoạt nén cả trọng số và kích hoạt. SmoothQuant (Xiao et al., 2023), LLM.int8() (Dettmers et al., 2022), và Outlier Suppression (Wei et al., 2022) đạt được lượng tử hóa W8A8 bằng cách quản lý các ngoại lệ kích hoạt. LLM.int8() sử dụng phân tách độ chính xác hỗn hợp, trong khi hai phương pháp kia sử dụng tỷ lệ theo kênh. Hơn nữa, Outlier Suppression+(Wei et al., 2023) thêm dịch chuyển theo kênh để thúc đẩy lượng tử hóa W6A6. Khác với các thiết kế heuristic trước đó, chúng tôi sử dụng tối ưu hóa gradient và mở rộng các biến đổi tương đương cho cơ chế attention, nâng cao thêm lượng tử hóa cache K/V. Gần đây, RPTQ (Yuan et al., 2023) và LLM-QAT (Liu et al., 2023b) đã đạt được lượng tử hóa W4A4. Tuy nhiên, RPTQ áp dụng lượng tử hóa kích hoạt theo nhóm không thân thiện với triển khai, và LLM-QAT sử dụng QAT tốn thời gian. Khác với RPTQ và LLM-QAT, chúng tôi đạt được lượng tử hóa W4A4 thông qua lượng tử hóa theo token thân thiện với triển khai và duy trì hiệu quả PTQ.

3 OMNIQUANT

Thách thức của lượng tử hóa LLM. Hai khó khăn chính nằm trong việc lượng tử hóa một LLM. Đầu tiên, kích hoạt khó lượng tử hóa do sự tồn tại của các kênh ngoại lệ. Xem xét rằng phân phối trọng số phẳng và đồng đều, SmoothQuant (Xiao et al., 2023) và Outlier Suppression+ (Wei et al., 2023) giải quyết vấn đề này bằng cách di chuyển khó khăn lượng tử hóa từ kích hoạt sang trọng số với cường độ di chuyển được định trước hoặc tối ưu hóa dựa trên tìm kiếm lưới. Thứ hai, lỗi lượng tử hóa của trọng số cũng đóng vai trò then chốt trong hiệu suất cuối cùng do tầm quan trọng của trọng số tương ứng với kích hoạt. SqQR (Dettmers et al., 2023b) và OWQ (Lee et al., 2023) đề xuất giữ lại trọng số quan trọng ở độ chính xác đầy đủ, trong khi AWQ (Lin et al., 2023) bảo vệ những trọng số này bằng tỷ lệ theo kênh tìm kiếm lưới. Mặc dù các phương pháp này đã đạt được thành công nhất định trong việc nén các LLM khác nhau, chúng thường dẫn đến hiệu suất dưới tối ưu và không thể xử lý lượng tử hóa bit cực thấp do thiết kế thô của các tham số lượng tử hóa thủ công như cường độ di chuyển và yếu tố tỷ lệ.

Trong phần này, chúng tôi giới thiệu một kỹ thuật lượng tử hóa có thể vi phân cho LLM gọi là OmniQuant, trong đó các tham số lượng tử hóa được học với tính linh hoạt tốt hơn. Hướng tới mục tiêu này, OmniQuant được thực hiện với khung tối thiểu hóa lỗi lượng tử hóa theo khối như được trình bày trong Phần 3.1. Để giải quyết các thách thức nêu trên của lượng tử hóa LLM, chúng tôi đưa ra hai chiến lược mới cho các tham số lượng tử hóa có thể học bổ sung bao gồm cắt trọng số có thể học (LWC) để giảm thiểu khó khăn trong lượng tử hóa trọng số và biến đổi tương đương có thể học (LET) để tiếp tục chuyển thách thức lượng tử hóa từ kích hoạt sang trọng số. Chúng tôi giới thiệu LWC và LCT trong Phần 3.2 và Phần 3.3, tương ứng.

3.1 TỐI THIỂU HÓA LỖI LƯỢNG TỬ HÓA THEO KHỐI

Các phương pháp PTQ trước đó với tối ưu hóa gradient, như AdaRound (Nagel et al., 2020), BRECQ (Li et al., 2021) không thể được áp dụng trong các mô hình với hàng tỷ tham số vì chúng khó tối ưu hóa do không gian giải pháp khổng lồ. Thay vì điều chỉnh toàn bộ mô hình, chúng tôi đề xuất một đường ống tối ưu hóa mới với tối thiểu hóa lỗi lượng tử hóa theo khối, nơi các tham số lượng tử hóa bổ sung có thể được tối ưu hóa theo cách có thể vi phân. Chúng tôi hình thức hóa mục tiêu tối ưu hóa như sau:

arg min
Θ1,Θ2||F(W,X)− F
Qw(W; Θ1,Θ2), Qa(X,Θ2)
||, (1)

trong đó F đại diện cho hàm ánh xạ cho một khối transformer trong LLM, W và X là trọng số và kích hoạt độ chính xác đầy đủ, Qw(·) và Qa(·) đại diện cho bộ lượng tử hóa trọng số và kích hoạt, tương ứng, Θ1 và Θ2 là các tham số lượng tử hóa trong cắt trọng số có thể học (LWC) và biến đổi tương đương có thể học (LET), tương ứng. Lượng tử hóa theo khối trong Phương trình (1) lượng tử hóa tuần tự các tham số của một khối transformer trước khi chuyển sang khối tiếp theo.

Tối thiểu hóa theo khối trong Phương trình (1) có hai ưu điểm. Đầu tiên, được trang bị tối thiểu hóa theo khối trong Phương trình (1), OmniQuant có thể tối ưu hóa các tham số lượng tử hóa trong LWC và LET cùng nhau, làm cho nó đủ khả năng bao gồm cả lượng tử hóa chỉ trọng số và lượng tử hóa trọng số-kích hoạt. Thứ hai, tối thiểu hóa theo khối dễ tối ưu hóa với yêu cầu tài nguyên tối thiểu. OmniQuant chỉ xác định một số tham số lượng tử hóa với tính tối ưu, dễ hơn so với tối ưu hóa toàn bộ trọng số trong các phương pháp dựa trên PTQ trước đó (Nagel et al., 2020; Li et al., 2021). Theo kinh nghiệm, chúng tôi nhận thấy rằng tất cả các mô hình từ họ LLaMA-2 (Touvron et al., 2023b) có thể được lượng tử hóa trên một GPU A100-40G duy nhất chỉ sử dụng 128 mẫu huấn luyện.

3.2 CẮT TRỌNG SỐ CÓ THỂ HỌC

OmniQuant sử dụng một mô-đun cắt trọng số có thể học (LWC) để giảm khó khăn lượng tử hóa trọng số trong một LLM. Tương tự như các phương pháp trước đó với ngưỡng cắt có thể học (Esser et al., 2019; Liu et al., 2022; Choi et al., 2018), LWC cũng xác định phạm vi động tối ưu của trọng số bằng cách tối ưu hóa ngưỡng cắt. Tuy nhiên, chúng tôi nhận thấy rằng việc sử dụng trực tiếp các nghệ thuật trước đây như PACT (Choi et al., 2018) và LSQ (Esser et al., 2019) trong lượng tử hóa sẽ tạo ra hiệu suất không thỏa mãn, như được thể hiện trong Bảng A14 trong Phụ lục.

Thay vì học trực tiếp ngưỡng cắt như trong các phương pháp trước đó (Esser et al., 2019; Choi et al., 2018), LWC tối ưu hóa cường độ cắt như được hình thức hóa bởi

Wq= clamp( ⌊W/h⌉+z,0,2N−1),trong đó h=γmax(W)−βmin(W)/2N−1, z=−⌊βmin(W)/h⌉ (2)

trong đó ⌊·⌉ biểu thị phép toán làm tròn. N là số bit mục tiêu. Wq và W biểu thị trọng số đã lượng tử hóa và độ chính xác đầy đủ, tương ứng. h là yếu tố chuẩn hóa cho trọng số và z là giá trị điểm không. Phép toán clamp hạn chế giá trị trong phạm vi số nguyên N-bit, cụ thể [0,2N−1]. Trong Phương trình (2), γ∈[0,1] và β∈[0,1] là cường độ cắt có thể học cho ranh giới trên và dưới của trọng số, tương ứng. Chúng tôi cụ thể hóa γ và β bằng hàm sigmoid*. Do đó, Θ1={γ, β} trong Phương trình (1).

Lưu ý rằng LWC thoái hóa thành lược đồ lượng tử hóa MinMax vanilla được sử dụng trong các công trình hiện có (Xiao et al., 2023), Frantar et al. (2022) khi γ= 1 và β= 1. Bằng cách kế thừa lợi ích của lượng tử hóa Min-Max, LWC chỉ cần điều chỉnh cường độ cắt để xác định ngưỡng cắt tối ưu, sẽ giảm khó khăn tối ưu hóa. Được cắt bởi ngưỡng tối ưu, trọng số gốc sẽ dễ lượng tử hóa. Như được chỉ ra bởi các thí nghiệm trong Bảng 1, phương pháp cắt trọng số có thể học đề xuất của chúng tôi vượt trội đáng kể so với các kỹ thuật lượng tử hóa chỉ trọng số trước đó (Frantar et al., 2022; Lin et al., 2023)).

3.3 BIẾN ĐỔI TƯƠNG ĐƯƠNG CÓ THỂ HỌC

Ngoài LWC cho phép trọng số thân thiện với lượng tử hóa bằng cách tối ưu hóa ngưỡng cắt, chúng tôi tiếp tục giảm khó khăn lượng tử hóa trọng số-kích hoạt bằng biến đổi tương đương có thể học (LET). Xem xét rằng các ngoại lệ trong bản đồ kích hoạt có tính hệ thống và độc đáo với các kênh cụ thể, các phương pháp trước đó như SmoothQuant (Xiao et al., 2023) di chuyển khó khăn lượng tử hóa từ kích hoạt sang trọng số với biến đổi tương đương toán học. Tuy nhiên, chúng tạo ra các tham số tương đương thủ công, dẫn đến kết quả dưới tối ưu.

Nhờ việc bao gồm tối thiểu hóa lỗi lượng tử hóa theo khối, LET của chúng tôi có thể xác định các tham số tương đương tối ưu theo cách có thể vi phân. Lấy cảm hứng từ SmoothQuant (Xiao et al., 2023) và Outlier Suppression+ (Wei et al., 2023), chúng tôi áp dụng tỷ lệ theo kênh và dịch chuyển theo kênh để thao tác phân phối kích hoạt, cung cấp giải pháp hiệu quả cho vấn đề ngoại lệ. Cụ thể, chúng tôi điều tra biến đổi tương đương qua cả lớp tuyến tính và phép toán attention, như được minh họa trong Hình 3.

Lớp tuyến tính. Lớp tuyến tính nhận một chuỗi token đầu vào X∈RT×Cin trong đó T là độ dài token và là phép nhân của ma trận trọng số W∈RCin×Cout và vector bias B∈R1×Cout. Một lớp tuyến tính tương đương toán học được biểu thị như:

Y=XW +B= [(X−δ)⊘s|{z}
˜X]·[s⊙W|{z}
˜W] + [B+δW|{z}
˜B] (3)

trong đó Y đại diện cho đầu ra, s∈R1×Cin và δ∈R1×Cin là các tham số tỷ lệ và dịch chuyển theo kênh, tương ứng, ˜X, ˜W và ˜B là kích hoạt, trọng số và bias tương đương, tương ứng, '⊘' và '⊙' là phép chia và nhân theo phần tử. Bằng Phương trình (3), các kích hoạt được biến đổi để thân thiện với lượng tử hóa với chi phí tăng khó khăn lượng tử hóa trong trọng số. Theo nghĩa này, LWC trong Phần 3.2 có thể cải thiện hiệu suất lượng tử hóa trọng số-kích hoạt đạt được bởi LET vì nó làm cho trọng số thân thiện với lượng tử hóa. Cuối cùng, chúng tôi thực hiện lượng tử hóa trên kích hoạt và trọng số đã biến đổi, như được đưa ra bởi

Y=Qa(˜X)Qw(˜W) +eB, (4)

trong đó Qa là bộ lượng tử hóa MinMax vanilla và Qw là bộ lượng tử hóa MinMax với cắt trọng số có thể học (tức là LWC của chúng tôi).

Lưu ý rằng các tham số tỷ lệ và dịch chuyển trong ˜X có thể được hấp thụ vào lớp chuẩn hóa hoặc tuyến tính trước đó và các yếu tố tỷ lệ trong ˜W có thể được hợp nhất vào trọng số tuyến tính gốc W. Do đó, biến đổi tương đương trong Phương trình (3) có thể giảm hiệu quả lỗi lượng tử hóa mà không giới thiệu thêm tham số hoặc chi phí. Chúng tôi sử dụng biến đổi tương đương này trong tất cả các lớp tuyến tính của LLM ngoại trừ lớp tuyến tính thứ hai của FFN như được hiển thị trong Hình 3. Điều này có thể vì độ thưa thớt cao của các đặc trưng sau lớp phi tuyến (Liu et al., 2023c) dẫn đến gradient không ổn định khi áp dụng biến đổi tương đương có thể học.

Phép toán Attention. Ngoài lớp tuyến tính, phép toán attention cũng chiếm một tỷ lệ đáng kể của tính toán. Ngoài ra, mô hình tự hồi quy của LLM đòi hỏi lưu trữ cache key-value (KV) cho mỗi token, dẫn đến nhu cầu bộ nhớ đáng kể cho các chuỗi dài. Do đó, chúng tôi cũng lượng tử hóa ma trận Q/K/V thành bit thấp trong cài đặt lượng tử hóa trọng số-kích hoạt. Cụ thể, biến đổi tương đương có thể học của ma trận ái lực self-attention có thể được viết như:

P= Softmax( QKT) = Softmax(( Q⊘sa|{z}
˜Q)(sa⊙KT
|{z}
˜KT)). (5)

trong đó sa∈R1×Cout là yếu tố tỷ lệ trong ma trận ái lực. Tương tự như Phương trình (4), tính toán ma trận ái lực đã lượng tử hóa được biểu thị như P= Softmax( Qa(eQ)Qa(eKT)). Ở đây chúng tôi cũng sử dụng lược đồ lượng tử hóa Min-Max như Qa để lượng tử hóa ma trận ˜Q/˜K. Từ Phương trình (4) và Phương trình (5) chúng ta biết rằng Θ2={δ, s, s a} trong Phương trình (1).

Các yếu tố tỷ lệ theo kênh trong ˜Q và ˜K, như được thấy trong Phương trình (5), có thể được hấp thụ vào trọng số tuyến tính của phép chiếu query và key, tương ứng. Đáng chú ý rằng biến đổi rõ ràng của V được bỏ qua vì phân phối của nó đã được thay đổi theo kênh bởi biến đổi nghịch đảo liên quan đến lớp tuyến tính chiếu đầu ra.

4 THÍ NGHIỆM

4.1 CÀI ĐẶT

Lượng tử hóa. Chúng tôi thí nghiệm với cả lượng tử hóa chỉ trọng số và lượng tử hóa trọng số-kích hoạt. Đối với trường hợp đầu, cài đặt mặc định là lượng tử hóa trọng số INT4/INT3/INT2 theo kênh. Lượng tử hóa trọng số theo nhóm được đại diện bởi 'g', ví dụ, W3A16g128 có nghĩa là lượng tử hóa chỉ trọng số 3-bit với kích thước nhóm 128. Trong lượng tử hóa trọng số-kích hoạt, mặc định là lượng tử hóa trọng số INT6/INT4 theo kênh và kích hoạt theo token (Dettmers et al., 2022). Tất cả kích hoạt trung gian được lượng tử hóa thành bit thấp, loại trừ đầu ra SoftMax, được giữ ở độ chính xác đầy đủ do phân phối đuôi dài của nó khiến nó không phù hợp với lượng tử hóa đồng đều.

Huấn luyện Yếu tố tỷ lệ theo kênh được khởi tạo với SmoothQuant (Xiao et al., 2023), và yếu tố dịch chuyển theo kênh được khởi tạo sử dụng Outlier Suppression+ (Wei et al., 2023). Để tối ưu hóa các tham số có thể học, chúng tôi sử dụng bộ tối ưu hóa AdamW với suy giảm trọng số bằng không. Tỷ lệ học cho cắt trọng số có thể học và biến đổi tương đương được đặt như 5e−3 và 1e−2, tương ứng. Chúng tôi sử dụng bộ dữ liệu hiệu chuẩn bao gồm 128 đoạn 2048-token được chọn ngẫu nhiên từ WikiText2 (Merity et al., 2016). Toàn bộ quá trình huấn luyện được tạo điều kiện trên một GPU Nvidia A100 duy nhất, sử dụng kích thước batch là 1 qua 20 epoch, ngoại trừ lượng tử hóa W2A16 sử dụng 40 epoch. Đối với lượng tử hóa trọng số-kích hoạt, cả cắt trọng số có thể học và biến đổi tương đương đều được kích hoạt. Đối với chỉ trọng số, cả hai đều được sử dụng cho OPT, nhưng chỉ cắt cho LLaMA, vì Bảng A3 cho thấy lợi ích không đáng kể từ biến đổi tương đương cho LLaMA.

Mô hình. Chúng tôi kiểm tra trên OPT(125M-66B)(Zhang et al., 2022)), LLaMA(7B-65B) (Touvron et al., 2023a), LLaMA-2(7B-70B) (Touvron et al., 2023b), Falcon-180B (Penedo et al., 2023), và LLaMA-2-chat được điều chỉnh theo hướng dẫn (Touvron et al., 2023b) cho tính tổng quát. Trong khi bài báo chính nổi bật kết quả LLaMA, chi tiết toàn diện cho các mô hình khác có sẵn trong Phần A8 của Phụ lục.

Đánh giá. Theo công trình trước đó (Lin et al., 2023; Frantar et al., 2022), chúng tôi đánh giá các mô hình đã lượng tử hóa bằng cách báo cáo độ phức tạp của các thí nghiệm tạo ngôn ngữ, cụ thể trên WikiText2 (Merity et al., 2016), PTB (Marcus et al., 1994)), C4 (Raffel et al., 2020). Hơn nữa, độ chính xác được đánh giá trong các tác vụ zero-shot bao gồm PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), BoolQ (Clark et al., 2019), và HellaSwag (Clark et al., 2018). Chúng tôi tuân thủ cài đặt GPTQ (Frantar et al., 2022) cho các thí nghiệm tạo ngôn ngữ, và thực hiện lm-eval-harness (Gao et al., 2021) để thực thi tất cả các tác vụ zero-shot.

Baseline. Đối với lượng tử hóa chỉ trọng số, chúng tôi so sánh với lượng tử hóa làm tròn về gần nhất vanilla (RTN), GPTQ (Frantar et al., 2022), và AWQ (Lin et al., 2023). Đối với lượng tử hóa trọng số-kích hoạt, chúng tôi so sánh phương pháp của chúng tôi với SmoothQuant (Xiao et al., 2023), Outlier Supression + (Wei et al., 2023), RPTQ (Yuan et al., 2023), và phương pháp QAT gần đây LLM-QAT (Liu et al., 2023b). Lưu ý rằng chúng tôi tái tạo SmoothQuant và Outlier Suppression+ với lượng tử hóa trọng số theo kênh và lượng tử hóa kích hoạt theo token để so sánh công bằng.

4.2 KẾT QUẢ LƯỢNG TỬ HÓA CHỈ TRỌNG SỐ

Kết quả của họ LLaMA có thể được tìm thấy trong Bảng 1, trong khi kết quả cho OPT được trình bày trong Phần A8 của Phụ lục. Như được minh họa bởi các bảng, OmniQuant luôn vượt trội hơn phương pháp lượng tử hóa chỉ trọng số LLM trước đó qua các họ LLM khác nhau (OPT, LLaMA-1, LLaMA-2) và các cấu hình lượng tử hóa đa dạng, bao gồm W2A16, W2A16g128, W2A16g64, W3A16, W3A16g128, W4A16, và W4A16g128. Những phát hiện này gợi ý tính linh hoạt của OmniQuant, có thể thích ứng với nhiều cấu hình lượng tử hóa. Ví dụ, trong khi AWQ (Lin et al., 2023) đặc biệt hiệu quả với lượng tử hóa theo nhóm, OmniQuant thể hiện hiệu suất vượt trội qua cả lượng tử hóa theo kênh và theo nhóm. Hơn nữa, lợi ích hiệu suất của OmniQuant trở nên rõ rệt hơn khi kích thước bit lượng tử hóa giảm.

4.3 KẾT QUẢ LƯỢNG TỬ HÓA TRỌNG SỐ-KÍCH HOẠT

Trong lượng tử hóa trọng số-kích hoạt, trọng tâm chính của chúng tôi nằm ở lượng tử hóa W6A6 và W4A4. Chúng tôi loại trừ lượng tử hóa W8A8 vì SmoothQuant có thể gần như đạt được các mô hình đã lượng tử hóa W8A8 không mất mát khi so sánh với các mô hình độ chính xác đầy đủ tương ứng. Kết quả của họ LLaMA có thể được tìm thấy trong Bảng 2, trong khi kết quả cho OPT được trình bày trong Bảng A25 của Phụ lục. Bảng 2 minh họa độ chính xác tác vụ zero-shot của lượng tử hóa trọng số-kích hoạt LLaMA. Đáng chú ý, OmniQuant nâng cao đáng kể độ chính xác trung bình bởi +4.99% ∼+11.80% qua các mô hình khác nhau tại lượng tử hóa W4A4. Đáng chú ý, trong LLaMA-7B, OmniQuant thậm chí vượt qua phương pháp QAT gần đây, LLM-QAT (Liu et al., 2023b), với biên độ ấn tượng +6.22%. Cải thiện này thể hiện hiệu quả của việc kết hợp các tham số có thể học bổ sung, được chứng minh là có lợi hơn so với điều chỉnh trọng số toàn cục được sử dụng bởi QAT.

[Tiếp tục với phần còn lại của bản dịch...]
