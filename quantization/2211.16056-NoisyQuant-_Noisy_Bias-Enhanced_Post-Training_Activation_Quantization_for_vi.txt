NoisyQuant: Lượng tử hóa kích hoạt sau huấn luyện với thiên kiến nhiễu cho Vision Transformers

Yijiang Liu1, Huanrui Yang2, Zhen Dong2, Kurt Keutzer2, Li Du1, Shanghang Zhang3
1Đại học Nanjing, 2Đại học California, Berkeley
3Phòng thí nghiệm Trọng điểm Quốc gia về Xử lý Thông tin Đa phương tiện, Khoa Khoa học Máy tính, Đại học Bắc Kinh
liuyijiang@smail.nju.edu.cn, {huanrui, zhendong, keutzer}@berkeley.edu
ldu@nju.edu.cn, shanghang@pku.edu.cn

Tóm tắt
Kiến trúc phức tạp và chi phí huấn luyện cao của vision transformers thúc đẩy việc khám phá lượng tử hóa sau huấn luyện. Tuy nhiên, phân phối đuôi nặng của các kích hoạt vision transformer cản trở hiệu quả của các phương pháp lượng tử hóa sau huấn luyện trước đó, ngay cả với các thiết kế bộ lượng tử tiên tiến. Thay vì điều chỉnh bộ lượng tử để phù hợp tốt hơn với phân phối kích hoạt phức tạp, bài báo này đề xuất NoisyQuant, một cải tiến bất khả tri bộ lượng tử cho hiệu suất lượng tử hóa kích hoạt sau huấn luyện của vision transformers. Chúng tôi có một khám phá lý thuyết đáng ngạc nhiên rằng với một bộ lượng tử cho trước, việc thêm một thiên kiến nhiễu Uniform cố định vào các giá trị được lượng tử hóa có thể giảm đáng kể lỗi lượng tử hóa dưới các điều kiện có thể chứng minh. Dựa trên hiểu biết lý thuyết, NoisyQuant đạt được thành công đầu tiên trong việc chủ động thay đổi phân phối kích hoạt đuôi nặng với thiên kiến nhiễu cộng để phù hợp với một bộ lượng tử cho trước. Các thí nghiệm mở rộng cho thấy NoisyQuant cải thiện đáng kể hiệu suất lượng tử hóa sau huấn luyện của vision transformer với chi phí tính toán tối thiểu. Ví dụ, trên lượng tử hóa kích hoạt tuyến tính uniform 6-bit, NoisyQuant cải thiện độ chính xác top-1 SOTA trên ImageNet lên đến 1.7%, 1.1% và 0.5% cho ViT, DeiT, và Swin Transformer tương ứng, đạt hiệu suất ngang bằng hoặc thậm chí cao hơn so với lượng tử hóa phi tuyến, độ chính xác hỗn hợp trước đó.

1. Giới thiệu
Được truyền cảm hứng bởi thành công của các mô hình transformer dựa trên Self Attention (SA) trong các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên (NLP), các nghiên cứu gần đây đã đạt được tiến bộ đáng kể trong việc áp dụng các mô hình transformer vào lĩnh vực thị giác máy tính. Trong khi đó, thiết kế điển hình của các mô hình transformer gây ra kích thước mô hình lớn, tiêu thụ tính toán cao và thời gian huấn luyện dài. Ví dụ, mô hình DeiT-Base được sử dụng rộng rãi chứa 86M tham số, ghi lại 18G phép toán dấu phẩy động cho một đầu vào duy nhất, và yêu cầu 300 epoch huấn luyện trên tập dữ liệu ImageNet. Điều này dẫn đến những khó khăn đáng kể trong triển khai phần cứng.

Đối mặt với khó khăn như vậy, một số phương pháp nén và tăng tốc được áp dụng cho các mô hình vision transformer, bao gồm cắt tỉa, lượng tử hóa, và tìm kiếm kiến trúc mạng nơ-ron, v.v.

Trong số các phương pháp này, lượng tử hóa xuất hiện như một trong những cách hiệu quả và được áp dụng rộng rãi nhất. Quá trình lượng tử hóa sử dụng một hàm "bộ lượng tử" được định nghĩa trước để chuyển đổi biểu diễn liên tục của trọng số và kích hoạt thành một số lượng nhỏ các ký hiệu rời rạc, do đó cho phép biểu diễn độ chính xác thấp để tiết kiệm bộ nhớ một cách đơn giản. Đối với các mô hình DNN, lỗi xấp xỉ do bộ lượng tử gây ra không thể tránh khỏi dẫn đến suy giảm hiệu suất. Một loạt các công trình tập trung vào Huấn luyện Có Nhận thức Lượng tử hóa (QAT) để tinh chỉnh mô hình đã lượng tử hóa ở độ chính xác thấp. Tuy nhiên, với chi phí huấn luyện cao và đồ thị tính toán phức tạp của các mô hình vision transformer, việc huấn luyện lại mô hình ở độ chính xác thấp có thể tốn kém và không ổn định. Thay vào đó, Lượng tử hóa Sau Huấn luyện (PTQ) được ưa chuộng cho vision transformers vì nó loại bỏ nhu cầu huấn luyện lại hoặc tinh chỉnh mô hình đã lượng tử hóa, thay vào đó chỉ điều chỉnh thiết kế của bộ lượng tử dựa trên mô hình được huấn luyện trước với độ chính xác đầy đủ và một tập nhỏ dữ liệu hiệu chuẩn được lấy mẫu.

Ví dụ, các bộ lượng tử tuyến tính giảm lỗi lượng tử hóa bằng cách dịch chuyển, cắt bớt và chia tỷ lệ các giá trị được lượng tử hóa. Các bộ lượng tử phi tuyến tiếp tục điều chỉnh độ rộng và vị trí của mỗi thùng lượng tử hóa để phù hợp tốt hơn với phân phối.

Thật không may, mặc dù có tiến bộ trong việc thiết kế các bộ lượng tử PTQ tốt hơn, việc lượng tử hóa các mô hình vision transformer vẫn có vẻ là một thử thách đáng kể, đặc biệt là đối với lượng tử hóa kích hoạt. Các lớp transformer tạo ra hàng triệu giá trị kích hoạt với các phân phối phức tạp. Ví dụ, các đầu ra của hàm GELU được phân phối bất đối xứng, với các đỉnh trong biểu đồ tần số tại một số giá trị và một đuôi dài trong một phạm vi lớn (xem phía trên bên trái của Hình 1). Một số lớp chiếu tuyến tính cũng dẫn đến các giá trị kích hoạt lớn đáng kể được phân phối thưa thớt trong một đuôi rất dài. Do đó, PTQ độ chính xác thấp trên vision transformer bị suy giảm hiệu suất, ngay cả khi sử dụng các bộ lượng tử phi tuyến hoặc độ chính xác hỗn hợp với chi phí giao tiếp dữ liệu và tính toán bổ sung. Không có phương pháp PTQ uniform phi tuyến nào đạt được hiệu suất tốt trên các mô hình vision transformer.

Bài báo này cung cấp một góc nhìn hoàn toàn mới trong việc xử lý phân phối kích hoạt vision transformer phức tạp. Thay vì thêm nhiều thủ thuật hơn trong thiết kế bộ lượng tử để phù hợp với phân phối kích hoạt, công trình này khám phá tiềm năng thay đổi tích cực và hiệu quả về chi phí phân phối được lượng tử hóa, làm cho nó thân thiện hơn với một bộ lượng tử cho trước. Pipeline của phương pháp được đề xuất được minh họa trong Hình 1.

Cụ thể, chúng tôi có một khám phá đáng ngạc nhiên rằng với bất kỳ bộ lượng tử nào, lỗi lượng tử hóa có thể được giảm đáng kể bằng cách thêm một thiên kiến nhiễu cố định được lấy mẫu từ phân phối Uniform vào kích hoạt trước khi lượng tử hóa. Chúng tôi chứng minh lý thuyết điều kiện khi việc giảm lỗi lượng tử hóa có thể đạt được. Trên cơ sở này, chúng tôi đề xuất NoisyQuant, một cải tiến cắm và chạy, bất khả tri bộ lượng tử về lượng tử hóa kích hoạt sau huấn luyện của các mô hình vision transformer.

Đối với mỗi lớp, chúng tôi lấy mẫu một Thiên kiến Nhiễu dựa trên phân phối kích hoạt đầu vào theo phân tích lý thuyết của chúng tôi, và tính toán thiên kiến khử nhiễu tương ứng để giữ lại đầu ra chính xác của lớp. Tại thời gian suy luận, Thiên kiến Nhiễu được thêm vào kích hoạt đầu vào trước khi lượng tử hóa, và thiên kiến khử nhiễu được loại bỏ khỏi đầu ra lớp. Quá trình này giảm đáng kể lỗi lượng tử hóa với chi phí tính toán tối thiểu. NoisyQuant dẫn đến cải thiện đáng kể trong hiệu suất PTQ của các vision transformers tiên tiến. Áp dụng NoisyQuant trên lượng tử hóa tuyến tính uniform đạt hiệu suất ngang bằng với các phương pháp PTQ phi tuyến, độ chính xác hỗn hợp SOTA. Thêm NoisyQuant lên trên các bộ lượng tử phi tuyến này đạt được lợi ích hiệu suất thêm.

Theo hiểu biết tốt nhất của chúng tôi, bài báo này có những đóng góp mới lạ sau:

• Chứng minh lý thuyết khả năng và chứng minh các điều kiện khả thi để giảm lỗi lượng tử hóa của các phân phối đuôi nặng với một thiên kiến nhiễu cộng cố định;

• Đề xuất NoisyQuant, một cải tiến bất khả tri bộ lượng tử cho hiệu suất PTQ về lượng tử hóa kích hoạt. NoisyQuant đạt được thành công đầu tiên trong việc chủ động tinh chỉnh phân phối được lượng tử hóa để giảm lỗi lượng tử hóa theo kết quả lý thuyết về thiên kiến nhiễu cộng, với chi phí tính toán tối thiểu;

• Chứng minh cải thiện hiệu suất nhất quán bằng cách áp dụng NoisyQuant lên trên các bộ lượng tử PTQ hiện có. Đối với PTQ 6-bit, NoisyQuant cải thiện độ chính xác top-1 ImageNet của các mô hình vision transformer được lượng tử hóa tuyến tính uniform lên đến 1.7%, và cải thiện bộ lượng tử phi tuyến SOTA PTQ4ViT lên đến 0.7%.

2. Công trình liên quan

2.1. Các mô hình Transformer trong thị giác máy tính
Kể từ khi các mô hình transformer dựa trên Self-Attention (SA) đã cho thấy hiệu suất ấn tượng trong các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên, các nhà nghiên cứu đã chuyển ý tưởng này vào thị giác máy tính. Công trình tiên phong về Vision Transformer lần đầu tiên xây dựng một mô hình hoàn toàn dựa trên SA trong lĩnh vực thị giác máy tính và cho thấy hiệu quả của nó. Kể từ đó, nhiều mô hình thị giác dựa trên transformer đã xuất hiện với hiệu suất được tăng cường, tuyên bố tiên tiến trong nhiều nhiệm vụ thị giác máy tính. Những tiến bộ đáng chú ý bao gồm tăng cường dữ liệu và chưng cất bổ sung, kết hợp kiến trúc đa giai đoạn, và khám phá thiết kế attention mới lạ. Các mô hình Transformer cũng tiến bộ đến các nhiệm vụ ngoài phân loại, như phát hiện đối tượng, phân đoạn ngữ nghĩa, và tạo hình ảnh. Hơn nữa, sự phát triển của vision transformer mang lại sự thống nhất kiến trúc mô hình và biểu diễn qua hình ảnh và văn bản, cho phép kết nối thị giác-ngôn ngữ mạnh mẽ hơn thông qua học tập tương phản cặp hình ảnh-văn bản.

Sự phát triển nhanh chóng của các mô hình vision transformer thúc đẩy việc khám phá lượng tử hóa, vì lượng tử hóa mang lại tiết kiệm bộ nhớ và tính toán đơn giản bất khả tri với kiến trúc mô hình. So với các mô hình CNN, việc sử dụng các hàm kích hoạt tiên tiến như GELU và đồ thị tính toán phức tạp hơn như cơ chế attention làm cho vision transformers nhạy cảm hơn với lượng tử hóa. Công trình này nhằm giải quyết khó khăn của lượng tử hóa kích hoạt vision transformer sau huấn luyện do phân phối kích hoạt đuôi nặng gây ra, mà chúng tôi tin rằng sẽ tạo điều kiện cho việc triển khai các mô hình transformer tiên tiến vào các ứng dụng thị giác máy tính thực tế.

2.2. Các phương pháp lượng tử hóa sau huấn luyện
Lượng tử hóa sau huấn luyện (PTQ) là một phương pháp lượng tử hóa được ưa chuộng trong các tình huống với dữ liệu huấn luyện hạn chế hoặc tài nguyên tính toán hạn chế cho quá trình tinh chỉnh có nhận thức lượng tử hóa tốn kém. Các phương pháp trước đây đã nghiên cứu tận tình PTQ trên CNNs. Ví dụ, EasyQuant trình bày một cơ chế tìm kiếm nhanh để xác định phạm vi cắt phù hợp cho lượng tử hóa. ZeroQ đề xuất một cơ chế chưng cất để tạo ra hình ảnh đầu vào proxy, có thể tận dụng thống kê vốn có của các lớp chuẩn hóa batch để tiến hành PTQ. SQuant hiệu chuẩn mô hình cho lỗi lượng tử hóa thấp hơn trên cơ sở độ nhạy thu được bởi phổ Hessian. Trên vision transformers, Liu et al. trình bày một phương pháp PTQ tuyến tính sử dụng hệ số tương quan Pearson và mất mát xếp hạng để xác định các yếu tố tỷ lệ. PTQ4ViT giới thiệu một Lượng tử hóa Uniform Đôi phi tuyến dựa trên phân phối kích hoạt của vision transformers, thiết lập các yếu tố tỷ lệ khác nhau cho 1) kích hoạt dương và âm của GeLU, và 2) giá trị nhỏ và lớn của Softmax, có thể giảm lỗi lượng tử hóa kích hoạt ở một mức độ nào đó với chi phí của chi phí tính toán bổ sung. [19, 34] tái tham số hóa lớp LN để ức chế các giá trị ngoại lệ bằng cách thu nhỏ các giá trị kích hoạt.

Không giống như các phương pháp PTQ trước đây phân tích phân phối kích hoạt và điều chỉnh bộ lượng tử cho phù hợp, phương pháp của chúng tôi có một góc nhìn mới lạ về việc chủ động sửa đổi phân phối được lượng tử hóa với một thiên kiến nhiễu được lấy mẫu. Chúng tôi cho thấy phương pháp được đề xuất có thể mang lại giảm lỗi lượng tử hóa và cải thiện hiệu suất cho tất cả các bộ lượng tử PTQ cho lượng tử hóa kích hoạt vision transformer.

3. Phương pháp

Trong phần này, chúng tôi cung cấp một giới thiệu sơ bộ về ký hiệu và công thức điển hình của lượng tử hóa DNN trong Phần 3.1, phân tích lý thuyết tiềm năng giảm lỗi lượng tử hóa với thiên kiến nhiễu cộng được lấy mẫu trước trong Phần 3.2, và công thức hóa NoisyQuant, một phương pháp lượng tử hóa kích hoạt sau huấn luyện được tăng cường bởi thiên kiến nhiễu với lỗi lượng tử hóa giảm trong Phần 3.3.

3.1. Sơ bộ
Để bắt đầu, chúng tôi tóm tắt lại quá trình lượng tử hóa sau huấn luyện trên các mô hình DNN. Với sự thống trị của các lớp chiếu tuyến tính kết nối đầy đủ (FC) trong các mô hình transformer, ở đây chúng tôi tập trung thảo luận về lớp FC.

Tính toán được thực hiện trong một lớp FC với trọng số W∈Rk×m, kích hoạt đầu vào X∈Rm×n, và thiên kiến B∈Rk×1 có thể được công thức hóa như

f(X) = WX + B. (1)

Gánh nặng tính toán chính trong Eq. (1) nằm ở phép nhân ma trận của WX, yêu cầu tổng cộng k×m×n tính toán nhân-tích lũy (MACs). Lượng tử hóa sau huấn luyện nhằm giảm chi phí tính toán của phép nhân ma trận bằng cách chuyển đổi X và W thành các giá trị lượng tử hóa điểm cố định, do đó thay thế MACs dấu phẩy động bằng các phép toán điểm cố định rẻ hơn nhiều. Lớp FC đã lượng tử hóa do đó có thể được biểu diễn như

fq(X) = qW(W) qA(X) + B, (2)

trong đó qW(·) và qA(·) biểu thị hàm lượng tử của trọng số và kích hoạt tương ứng. Nghiên cứu trước đây đã quan sát thấy rằng phân phối kích hoạt đuôi nặng của X trong transformer đang gây ra lỗi lượng tử hóa đáng kể giữa X và qA(X) ở độ chính xác thấp, dẫn đến suy giảm hiệu suất đáng kể. Các phương pháp PTQ trước đây sửa đổi thiết kế của bộ lượng tử qA(·) để giảm thiểu lỗi lượng tử hóa.

Trong bài báo này, chúng tôi đề xuất một cách tiếp cận thay thế, trong đó chúng tôi sửa đổi phân phối của kích hoạt X với một thiên kiến nhiễu được lấy mẫu trước trước khi lượng tử hóa, và chứng minh nó là một cải tiến cắm và chạy cho bất kỳ thiết kế bộ lượng tử nào.

3.2. Phân tích lý thuyết về lỗi lượng tử hóa
Vì nghiên cứu trước đây chủ yếu đổ lỗi cho phân phối đuôi nặng của kích hoạt transformer là nguyên nhân chính gây ra lỗi lượng tử hóa lớn, trong phần này chúng tôi phân tích cách lỗi lượng tử hóa thay đổi khi chúng tôi chủ động thay đổi phân phối kích hoạt được lượng tử hóa. Một cách đơn giản để thay đổi phân phối kích hoạt đầu vào là thêm một "Thiên kiến Nhiễu" cố định được lấy mẫu từ phân phối ngẫu nhiên Uniform. Ký hiệu kích hoạt đầu vào là X và Thiên kiến Nhiễu là N, trong đó N và X có cùng kích thước, ở đây chúng tôi công thức hóa sự khác biệt lỗi lượng tử hóa giữa X và X+N cho một bộ lượng tử Q trong Eq. (3)

D(X; N) = QE(X+N) - QE(X) = ||Q(X+N) - X - N||²₂ - ||Q(X) - X||²₂. (3)

Định lý 1 phát biểu điều kiện trong đó D(x; N) ≤ 0 giữ cho mỗi ảnh chụp biểu đồ x của kích hoạt X.

Định lý 1. Xem xét một bộ lượng tử Q cho trước với độ rộng thùng lượng tử hóa 2b. Đối với mỗi ảnh chụp biểu đồ của X trong đó tất cả các phần tử Xi có cùng khoảng cách x từ trung tâm của thùng lượng tử hóa, và cho một Thiên kiến Nhiễu N được lấy mẫu từ NU(-n, n) trong đó x ≤ n ≤ 2b - x, chúng ta có

D(x; N) ≤ 0 iff 0 ≤ x ≤ n(1 - √(n/3b)). (4)

Chứng minh. Không mất tính tổng quát, chúng tôi xem xét thùng lượng tử hóa của bộ lượng tử Q mà x rơi vào được trung tâm tại điểm 0, do đó 0 ≤ x ≤ b.

Theo cách này, Q(Xi) = b cho tất cả các phần tử trong ảnh chụp biểu đồ, dẫn đến một lỗi lượng tử hóa

QE(X) = (b - x)². (5)

Xem xét việc thêm N ~ U(-n, n). X+N sẽ theo U(x-n, x+n). Trong trường hợp x ≤ n ≤ 2b - x, Q(Xi + Ni) = b nếu Ni ∈ [-x, n], và Q(Xi + Ni) = -b nếu Ni ∈ [-n, -x) cho tất cả các phần tử trong X+N. Mặc dù chúng tôi chỉ lấy mẫu một thể hiện duy nhất của N từ phân phối Uniform, với số lượng lớn các phần tử trong ảnh chụp biểu đồ, lỗi lượng tử hóa thực nghiệm QE(X+N) có thể được ước tính với một kỳ vọng trên tất cả N ~ U(-n, n) theo Định luật Yếu của Số Lớn, như

E_N[QE(X+N)] = (1/2n) ∫_{x-n}^{x+n} QE(x+z) dz
                = (1/2n) [∫_{-x}^{-n} (z+b)² dz + ∫_{0}^{x+n} (z-b)² dz]
                = (x²b/n) - x² + n²/(3nb) + b². (6)

Kết hợp Eq. (5) và Eq. (6), chúng ta có

D(x; N) = E_N[QE(X+N)] - QE(X)
         = (b/n)x² + 2bx + n²/(3nb). (7)

Chúng tôi xác minh việc dẫn xuất này một cách thực nghiệm trong Phần 4.

Có thể quan sát thấy rằng với b và n cho trước, D(x; N) là một hàm bậc hai đối với giá trị kích hoạt x. Bất đẳng thức D(x; N) ≤ 0; 0 ≤ x ≤ n có thể được giải dễ dàng đối với x như

0 ≤ x ≤ n(1 - √(n/3b)), (8)

luôn khả thi vì cả b và n đều dương.

Định lý 1 chỉ ra rằng việc thêm Thiên kiến Nhiễu luôn có thể giảm lỗi lượng tử hóa của các phần tử gần trung tâm của thùng lượng tử hóa, đây là nguồn gốc của lỗi lượng tử hóa lớn trong phân phối đuôi nặng.

Trong thực tế, để chọn một n phù hợp cho một lớp trong mô hình được huấn luyện trước, chúng tôi trước tiên có thể thu được phân phối thực nghiệm của kích hoạt X bằng cách truyền một lượng nhỏ dữ liệu hiệu chuẩn qua mô hình được huấn luyện trước. Với phân phối của X, chúng tôi có thể ước tính giảm lỗi lượng tử hóa dự kiến của lớp theo số như một hàm của n

L(n) = Σ_{x∈X} [D(x; N)]. (9)

Lưu ý rằng x ở đây biểu thị khoảng cách giữa mỗi giá trị kích hoạt và trung tâm thùng lượng tử hóa tương ứng. Mặc dù chúng tôi không thể tìm được một giải pháp dạng đóng cho cực tiểu của L(n), chúng tôi có thể dễ dàng tìm được một lựa chọn đủ tốt với một tìm kiếm tuyến tính trên một phạm vi nhỏ của n.

3.3. Công thức và pipeline NoisyQuant
Dựa trên phân tích lý thuyết của chúng tôi về việc giảm lỗi lượng tử hóa với Thiên kiến Nhiễu, chúng tôi đề xuất áp dụng Thiên kiến Nhiễu như một cải tiến cho các phương pháp PTQ của các mô hình vision transformer, mà chúng tôi đặt tên là "NoisyQuant". Pipeline của NoisyQuant được hình dung trong Hình 1. Cụ thể, trước khi thực hiện PTQ và triển khai mô hình, chúng tôi lấy mẫu một Thiên kiến Nhiễu duy nhất N ∈ R^{m×1} cho mỗi lớp từ phân phối Uniform, và cố định nó trong suốt quá trình suy luận. Phạm vi của N được xác định bởi mục tiêu hiệu chuẩn được định nghĩa trong Eq. (9). Tại thời gian suy luận, chúng tôi thêm Thiên kiến Nhiễu N vào kích hoạt đầu vào X theo quy tắc phát sóng trước khi đi qua bộ lượng tử kích hoạt, do đó chủ động thay đổi phân phối được lượng tử hóa để giảm lỗi lượng tử hóa đầu vào. Sau phép nhân kích hoạt-trọng số trong lớp tuyến tính, chúng tôi loại bỏ tác động của N bằng cách điều chỉnh hạng thiên kiến trong lớp tuyến tính với một thiên kiến khử nhiễu được tính từ N, do đó lấy lại đầu ra chính xác. Chính thức, NoisyQuant chuyển đổi tính toán FC đã lượng tử hóa được định nghĩa trong Eq. (2) thành

f^N_q(X) = q_W(W) q_A(X+N) + (B - q_W(W)N). (10)

Vì N được lấy mẫu trước khi triển khai mô hình và cố định trong suốt quá trình suy luận, thiên kiến đầu ra khử nhiễu B' = B - q_W(W)N chỉ cần được tính một lần trước khi triển khai, mà không có bất kỳ chi phí tính toán nào trong quá trình suy luận. Chi phí tính toán duy nhất mà NoisyQuant mang lại tại thời gian suy luận là phép cộng tức thời của X+N, không đáng kể so với chi phí của các phép nhân ma trận. Cả B' và N đều phải được lưu trữ với độ chính xác cao hơn. Trong thực tế, chúng tôi lưu trữ cả hai biến là INT16 để cho phép suy luận chỉ số nguyên.

So sánh với đầu ra của lớp FC dấu phẩy động được định nghĩa trong Eq. (1), lỗi lượng tử hóa đầu ra do lượng tử hóa kích hoạt gây ra được định nghĩa trong Eq. (2) là

QE_O(X) = ||f_q(X) - f(X)||²₂ = ||W[Q(X) - X]||²₂, (11)

trong đó Q(·) là viết tắt của q_A(·), và chúng tôi bỏ qua lượng tử hóa trọng số q_W(·) vì chúng tôi đang tập trung vào lượng tử hóa kích hoạt. Tương tự, lỗi lượng tử hóa đầu ra sau khi áp dụng NoisyQuant có thể được tính như

QE'_O(X) = ||f^N_q(X) - f(X)||²₂ = ||W[Q(X+N) - X - N]||²₂. (12)

Như chúng tôi chứng minh trong Định lý 1 rằng Thiên kiến Nhiễu cho phép X+N có lỗi lượng tử hóa thấp hơn X, chúng tôi có thể đạt được lỗi lượng tử hóa đầu ra thấp hơn trong Eq. (12) so với trong Eq. (11) khi NoisyQuant được áp dụng, do đó cải thiện hiệu suất PTQ của mô hình.

4. Xác minh hiểu biết lý thuyết

Trong phần này, chúng tôi cung cấp xác minh thực nghiệm về phân tích lý thuyết của chúng tôi về việc giảm lỗi lượng tử hóa kích hoạt với NoisyQuant. Chúng tôi xác minh số học Định lý 1 với dữ liệu mô phỏng trong Phần 4.1, và chứng minh sự giảm trong cả lỗi lượng tử hóa đầu vào và đầu ra dưới phân phối kích hoạt thời gian suy luận thực trong Phần 4.2.

4.1. Xác minh thực nghiệm Định lý 1
Ở đây chúng tôi theo các thiết lập trong Định lý 1 để xác minh thực nghiệm việc dẫn xuất lý thuyết của nó. Cụ thể, chúng tôi đặt phạm vi thùng lượng tử hóa b = 1, và khám phá cách sự khác biệt lỗi lượng tử hóa do Thiên kiến Nhiễu gây ra thay đổi với các lựa chọn khác nhau của giá trị kích hoạt x và phạm vi thiên kiến nhiễu n.

Đối với tất cả kết quả thực nghiệm, chúng tôi thử nghiệm với 10 thể hiện của Thiên kiến Nhiễu N được lấy mẫu độc lập, và báo cáo giá trị trung bình và độ lệch chuẩn của D(X; N) được định nghĩa trong Eq. (3) qua 10 thể hiện. Chúng tôi xem xét kích hoạt đầu vào X là một tensor với 20 chiều. Với hàng chục đến hàng trăm nghìn giá trị kích hoạt trong mỗi lớp transformer, có khả năng thấy hơn 20 phần tử kích hoạt có cùng giá trị. Vì chúng tôi dựa việc dẫn xuất lý thuyết trên Định luật Yếu của Số Lớn, việc có nhiều phần tử hơn có cùng giá trị sẽ dẫn đến ít phương sai hơn so với kết quả mô phỏng được cung cấp trong phần này.

Đối với thí nghiệm đầu tiên, chúng tôi cố định tất cả các phần tử trong X có giá trị x = 0.1, và thay đổi n trong phạm vi [0.1, 1.9]. Chúng tôi so sánh D(X; N) thực nghiệm và kết quả lý thuyết được dẫn xuất trong Eq. (7) trong Hình 2. Các kết quả được đánh giá tuân theo chặt chẽ đường lý thuyết. Ngay cả với chỉ 20 phần tử kích hoạt, độ lệch chuẩn qua Thiên kiến Nhiễu được lấy mẫu độc lập nhỏ hơn nhiều so với lợi ích lỗi lượng tử hóa mà việc thêm Thiên kiến Nhiễu mang lại.

Tương tự, chúng tôi cố định n = 1.4 và thay đổi giá trị kích hoạt x trong phạm vi [0, 0.6]. Kết quả được hiển thị trong Hình 3, nơi chúng ta thấy xu hướng tương tự về giá trị trung bình gần và phương sai nhỏ như trong thí nghiệm trước. Lỗi lượng tử hóa có thể được giảm bởi Thiên kiến Nhiễu khi x < 0.44, điều này tuân theo kết quả tính toán số của Eq. (4). Cả hai kết quả đều cho thấy tính đúng đắn và ổn định của việc dẫn xuất Eq. (7) và chứng minh Định lý 1.

4.2. Lỗi lượng tử hóa trong các lớp transformer
Như đã đề cập trong Phần 3.2, NoisyQuant cho phép giảm cả lỗi lượng tử hóa kích hoạt đầu vào (QE) và lỗi đầu ra (QE_O) của một lớp tuyến tính với một bộ lượng tử cho trước. Trong phần này, chúng tôi xác minh việc giảm lỗi trên phân phối kích hoạt thực của mô hình vision transformer. Thực tế, chúng tôi chạy đánh giá trên 5120 hình ảnh được chọn ngẫu nhiên của tập xác thực với bộ lượng tử PTQ tuyến tính EasyQuant làm baseline, và tính toán QE và QE_O có hoặc không thêm Thiên kiến Nhiễu. Thí nghiệm được chạy trên các loại lớp khác nhau, cụ thể là qkv, proj, fc1, và fc2. Kết quả được tính trung bình qua tất cả các lớp của mỗi loại. Như được hiển thị trong Bảng 1, sự khác biệt lỗi lượng tử hóa đầu vào D(X) được định nghĩa trong Eq. (3) luôn thấp hơn 0, điều này đáp ứng kỳ vọng của chúng tôi rằng Thiên kiến Nhiễu giảm lỗi lượng tử hóa đầu vào. Điều này tiếp tục dẫn đến sụt giảm đáng kể trong lỗi đầu ra giữa các lớp đã lượng tử hóa và dấu phẩy động cho tất cả các loại lớp. Lên đến 19% sụt giảm lỗi đầu ra trung bình có thể đạt được trong các lớp fc2.

Để hiểu rõ hơn về tác động mà NoisyQuant tạo ra, chúng tôi hình dung cả phân phối đầu vào và đầu ra của mỗi lớp. Chúng tôi bắt đầu với việc làm nổi bật fc2 vì nó đạt được sự giảm QE lớn nhất. Lớp fc2 nhận đầu vào từ các kích hoạt GELU, có phân phối phức tạp đáng kể như đã giới thiệu trong Phần 1. Hình 4 hình dung phân phối lỗi lượng tử hóa đầu vào có hoặc không có NoisyQuant. Chỉ với EasyQuant, một lượng đáng kể các phần tử kích hoạt cho thấy lỗi lượng tử hóa lớn, có lẽ do lượng tử hóa kém trên các khu vực được phân phối dày đặc. Thay vào đó, NoisyQuant nói chung làm cho QE được phân phối đều và cho hầu hết các phần tử gần zero, do đó dẫn đến sự giảm QE đáng kể tổng thể. Tiếp theo, chúng tôi vẽ biểu đồ tần số của các đầu ra lớp được chọn trong Hình 5. Đầu ra NoisyQuant liên tục tuân theo phân phối đầu ra lớp dấu phẩy động gần hơn so với EasyQuant, đặc biệt tại các giá trị có độ lớn lớn. Điều này dẫn đến giảm lỗi lên đến 60% trong đầu ra của một số lớp vision transformer.

5. Thí nghiệm

Phần này đánh giá tác động của NoisyQuant đối với hiệu suất PTQ của một số kiến trúc vision transformer được sử dụng phổ biến. Đầu tiên, chúng tôi đánh giá hiệu suất trên các nhiệm vụ phân loại hình ảnh với các mô hình ViT, DeiT và Swin transformer. Chúng tôi cũng đánh giá hiệu suất phát hiện đối tượng với mô hình DETR. Cuối cùng, chúng tôi cung cấp nghiên cứu ablation về cách NoisyQuant cải thiện hiệu suất PTQ khi được thêm vào mỗi loại lớp trong mô hình vision transformer.

5.1. Thiết lập thí nghiệm
Tập dữ liệu. Các nhiệm vụ phân loại được thực hiện trên tập dữ liệu phân loại ImageNet-2012, với 1.2 triệu hình ảnh huấn luyện và 50,000 hình ảnh xác thực trong 1000 lớp. Đối với nhiệm vụ phát hiện đối tượng, tập dữ liệu MSCOCO 2017 được sử dụng để đánh giá hiệu suất PTQ, chứa 118,000 hình ảnh huấn luyện và 5,000 hình ảnh xác thực. Đối với cả hai nhiệm vụ, chúng tôi lấy mẫu ngẫu nhiên 1024 hình ảnh từ tập huấn luyện làm dữ liệu hiệu chuẩn cho tất cả các phương pháp PTQ được thực hiện trong thí nghiệm của chúng tôi.

Kiến trúc mô hình được huấn luyện trước. Chúng tôi thực hiện PTQ uniform 6-bit và 8-bit trên các biến thể khác nhau của các mô hình vision transformer được huấn luyện trước. Đối với các nhiệm vụ phân loại, chúng tôi lượng tử hóa họ mô hình ViT, DeiT, và Swin được cung cấp bởi thư viện Timm, bao gồm các mô hình quy mô lớn với độ phân giải đầu vào 384×384 (được đánh dấu bằng "*"). Đối với phát hiện, chúng tôi thực hiện PTQ trên việc triển khai chính thức của DETR với backbone ResNet-50. Chúng tôi sử dụng checkpoint mô hình được huấn luyện trước được cung cấp bởi nguồn chính thức của mỗi mô hình, có hiệu suất dấu phẩy động được báo cáo trong các bảng kết quả và khớp với các bài báo gốc của chúng.

Chi tiết triển khai. Theo các phương pháp lượng tử hóa chính thống, chúng tôi lượng tử hóa tất cả các trọng số và đầu vào liên quan đến phép nhân ma trận. Cụ thể hơn, chúng tôi lượng tử hóa các trọng số của qkv-projectors, attention output projectors, MLPs, linear embeddings, và model heads. Ngoài ra, chúng tôi cũng lượng tử hóa các kích hoạt đầu vào của các lớp tuyến tính và các toán tử nhân ma trận. Theo [23, 26, 43, 45], chúng tôi giữ các phép toán layer normalization và softmax ở độ chính xác đầy đủ. Ngoài các ghi chú đặc biệt, chúng tôi thực hiện lượng tử hóa theo lớp đối xứng cho kích hoạt và lượng tử hóa theo kênh đối xứng cho trọng số. Cụ thể, trọng số được lượng tử hóa bởi các giá trị MinMax tuyệt đối mà không cần cắt. Chúng tôi triển khai NoisyQuant trên bộ lượng tử tuyến tính EasyQuant và bộ lượng tử phi tuyến PTQ4ViT. Sau khi chúng tôi áp dụng bộ lượng tử được chọn để xác định độ rộng và vị trí thùng lượng tử hóa trên dữ liệu hiệu chuẩn, chúng tôi quyết định các tham số của phân phối Thiên kiến Nhiễu thông qua tìm kiếm tuyến tính. Chúng tôi sử dụng lỗi lượng tử hóa kích hoạt thực nghiệm như được định nghĩa trong Eq. (9) làm mục tiêu tìm kiếm, nơi chúng tôi tìm tham số Thiên kiến Nhiễu có thể giảm thiểu lỗi lượng tử hóa.

5.2. Hiệu suất NoisyQuant
Ở đây chúng tôi so sánh NoisyQuant với các phương pháp PTQ tiên tiến, bao gồm percentile, bit-split, Liu et al., FQ-ViT, EasyQuant, và PTQ4ViT, trên ImageNet. Hiệu suất PTQ của các biến thể khác nhau của ViT, DeiT, và Swin transformer được cung cấp trong Bảng 2, Bảng 3, và Bảng 4, tương ứng. Thí nghiệm được tiến hành cho cả bộ lượng tử tuyến tính và phi tuyến.

Bộ lượng tử tuyến tính. Các phương pháp PTQ tuyến tính trước đây gặp phải suy giảm hiệu suất nghiêm trọng trên lượng tử hóa kích hoạt vision transformer. EasyQuant đạt hiệu suất tốt nhất trong số các bộ lượng tử tuyến tính, nhưng vẫn gây ra sụt giảm độ chính xác 2-6% dưới lượng tử hóa 6-bit so với baseline dấu phẩy động. Bằng cách áp dụng phương pháp NoisyQuant được đề xuất lên trên bộ lượng tử EasyQuant, bộ lượng tử NoisyQuant-Linear kết quả liên tục và đáng kể cải thiện độ chính xác PTQ trên tất cả các biến thể của các mô hình vision transformer. Dưới thiết lập W6A6, NoisyQuant-Linear đạt được cải thiện hiệu suất 1.73% trên ViT-S, 0.98% trên ViT-B*, 1.1% trên DeiT-S, và 0.5% trên SWIN-T so với EasyQuant. Dưới thiết lập W8A8, khi khoảng cách hiệu suất giữa EasyQuant và dấu phẩy động trở nên nhỏ hơn, NoisyQuant vẫn có vẻ có lợi trong việc cải thiện thêm hiệu suất PTQ. Đáng chú ý rằng mặc dù bộ lượng tử phi tuyến như PTQ4ViT liên tục đạt hiệu suất PTQ cao hơn so với bộ lượng tử tuyến tính EasyQuant, việc cải tiến mà NoisyQuant mang lại cho phép bộ lượng tử NoisyQuant-Linear đạt hiệu suất ngang bằng hoặc thậm chí tốt hơn so với PTQ4ViT phi tuyến, với chi phí thấp hơn nhiều trong triển khai phần cứng.

Bộ lượng tử phi tuyến. Vì chúng tôi tuyên bố NoisyQuant là một cải tiến bất khả tri bộ lượng tử của PTQ, chúng tôi tiếp tục triển khai NoisyQuant trên bộ lượng tử phi tuyến PTQ4ViT, cụ thể là NoisyQuant-PTQ4ViT. NoisyQuant-PTQ4ViT vượt trội hơn hầu hết kết quả của PTQ4ViT, đáng chú ý đạt được cải thiện 1.25% trên DeiT-S, 0.67% trên ViT-S, và 0.67% trên Swin-B trong thiết lập W6A6. Hiệu suất PTQ W6A6 của Swin-S và Swin-B* lần đầu tiên được cải thiện đến trong vòng 0.5% của baseline dấu phẩy động bởi bộ lượng tử NoisyQuant-PTQ4ViT, điều mà không thể đạt được mà không có sự giúp đỡ của NoisyQuant.

Ngoài các mô hình phân loại, chúng tôi báo cáo hiệu suất PTQ của mô hình phát hiện đối tượng DETR trong Bảng 5. NoisyQuant được triển khai trên bộ lượng tử EasyQuant cũng vượt trội hơn baseline EasyQuant, và tất cả các bộ lượng tử PTQ tuyến tính trước đây bao gồm percentile, bit-split, và Liu et al.

5.3. Tác động của NoisyQuant đối với các loại lớp khác nhau
Khi chúng tôi cho thấy hiệu quả của NoisyQuant trong việc cải thiện hiệu suất PTQ của toàn bộ mô hình, ở đây chúng tôi khám phá xem NoisyQuant có hữu ích cho tất cả các loại lớp trong mô hình vision transformer hay không. Bảng 6 thể hiện hiệu ứng của việc áp dụng NoisyQuant trên các loại lớp khác nhau. Dấu kiểm tra trong bảng có nghĩa là chúng tôi áp dụng NoisyQuant cho kích hoạt đầu vào của tất cả các lớp thuộc loại cụ thể đó, nếu không lớp được lượng tử hóa chỉ với EasyQuant. Đối với tất cả các loại lớp, việc áp dụng NoisyQuant liên tục cải thiện hiệu suất PTQ của toàn bộ mô hình. Cụ thể, các lớp 'fc2' có được lợi ích lớn nhất từ NoisyQuant, điều này tương ứng với phân tích trước đây của chúng tôi rằng phân phối kích hoạt sau hàm GELU mang lại thử thách chính cho PTQ, có thể được giải quyết với phương pháp NoisyQuant được đề xuất. Sau khi thêm NoisyQuant vào kích hoạt đầu vào của tất cả các lớp tuyến tính, mô hình đạt được sự thúc đẩy hiệu suất tối đa.

6. Kết luận
Công trình này đề xuất NoisyQuant, một phương pháp lượng tử hóa kích hoạt sau huấn luyện được tăng cường bởi thiên kiến nhiễu cho phân phối kích hoạt phức tạp của các mô hình vision transformer. Chúng tôi chứng minh lý thuyết việc giảm lỗi lượng tử hóa mà việc thêm thiên kiến nhiễu vào kích hoạt đầu vào trước khi lượng tử hóa mang lại, và chứng minh thực nghiệm hiệu quả của NoisyQuant trên cả bộ lượng tử tuyến tính và phi tuyến trên các mô hình vision transformer. Chúng tôi hy vọng công trình này mở ra một hướng mới để giảm lỗi lượng tử hóa PTQ thông qua việc chủ động thay đổi phân phối được lượng tử hóa, và truyền cảm hứng cho thiết kế hoặc phương pháp tạo ra phân phối Thiên kiến Nhiễu tốt hơn dựa trên đặc tính bộ lượng tử và kích hoạt.

Lời cảm ơn Công trình này được hỗ trợ một phần bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia của Trung Quốc dưới Grant 2022YFB4400900. Nhóm Berkeley ghi nhận sự hỗ trợ từ Berkeley Deep Drive, Intel Corporation, và Panasonic.
