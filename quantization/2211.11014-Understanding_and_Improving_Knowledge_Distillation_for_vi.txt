# 2211.11014.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2211.11014.pdf
# Kích thước tệp: 2562845 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hiểu và Cải thiện Chưng cất Tri thức cho
Huấn luyện Nhận biết Lượng tử hóa của các Bộ mã hóa Transformer Lớn
Minsoo Kim1, Sihwa Lee2, Sukjin Hong3, Du-Seong Chang3, và Jungwook Choi1,2
1Khoa Kỹ thuật Điện tử, Đại học Hanyang
2Khoa Trí tuệ Nhân tạo, Đại học Hanyang
Seoul, Cộng hòa Hàn Quốc
{minsoo2333, macto94, choij}@hanyang.ac.kr ,
3KT, Seoul, Cộng hòa Hàn Quốc
{sukjin.hong, dschang}@kt.com
Tóm tắt
Chưng cất tri thức (KD) đã trở thành một
phương pháp phổ biến để nén mô hình nhằm
tăng cường khả năng của mô hình nhẹ với
tri thức được chuyển giao từ giáo viên. Đặc
biệt, KD đã được sử dụng trong huấn luyện
nhận biết lượng tử hóa (QAT) của các bộ mã
hóa Transformer như BERT để cải thiện độ
chính xác của mô hình học sinh với các tham
số trọng số độ chính xác giảm. Tuy nhiên,
ít được hiểu biết về việc phương pháp KD
nào trong số các phương pháp khác nhau phù
hợp nhất với QAT của Transformers. Trong
công trình này, chúng tôi cung cấp một phân
tích sâu sắc về cơ chế của KD trong việc khôi
phục attention của các Transformer lượng tử
hóa lớn. Đặc biệt, chúng tôi tiết lộ rằng loss
MSE đã được áp dụng trước đây trên điểm
attention là không đủ để khôi phục thông tin
self-attention. Do đó, chúng tôi đề xuất hai
phương pháp KD; loss attention-map và
attention-output. Hơn nữa, chúng tôi khám
phá việc thống nhất cả hai loss để giải quyết
sự ưu tiên phụ thuộc vào nhiệm vụ giữa
attention-map và output loss. Kết quả thực
nghiệm trên các mô hình bộ mã hóa Transformer
khác nhau chứng minh rằng các phương pháp
KD được đề xuất đạt được độ chính xác tối
tân cho QAT với lượng tử hóa trọng số dưới
2-bit.
1 Giới thiệu
Chưng cất tri thức (KD) (Hinton et al., 2015)
là một khung học chuyển giao để truyền đạt tri
thức của một mô hình lớn (giáo viên) cho một
mô hình nhẹ (học sinh). Nhiều phương pháp KD
đã được phát triển liên quan đến nguồn tri thức
và mục tiêu chưng cất. Trong nhiều trường hợp,
phân kỳ Kullback-Leibler (KL-Div) được sử
dụng như một mục tiêu chưng cất mặc định để
khớp nhãn mềm của giáo viên và học sinh (Hinton
et al., 2015; Sanh et al., 2019). Nhưng các nghiên
cứu tiếp theo về KD cho thấy rằng các biểu diễn
nội tại cũng truyền đạt tri thức trung gian của
giáo viên (Sun et al., 2019; Aguilar et al., 2020).
Vì vậy việc tối thiểu hóa khoảng cách (ví dụ:
lỗi bình phương trung bình, MSE)
Tác giả liên hệ tri thức trạng thái ẩn (HSK) của các lớp
giữa giáo viên và học sinh cũng đã được đề xuất
(Sun et al., 2019; Liu et al., 2021a). KD trở thành
một kỹ thuật nén mô hình thiết yếu để triển khai
hiệu quả các mô hình ngôn ngữ dựa trên Transformer
quy mô lớn. Ví dụ, một mô hình bộ mã hóa
Transformer phổ biến, BERT, chứa hàng trăm triệu
tham số, gây ra chi phí bộ nhớ và tính toán sâu
sắc (Devlin et al., 2019). Những mô hình quy mô
lớn này yêu cầu nén cực kỳ để giảm dấu chân
mô hình từ 10 đến 100 lần. Do đó, các nghiên
cứu mở rộng đã được thực hiện để chưng cất
các mô hình học sinh hiệu quả (Sanh et al., 2019;
Sun et al., 2019; Jiao et al., 2020; Wang et al.,
2020, 2021), nhưng trọng tâm của chúng bị giới
hạn trong việc đạt được ít tham số hơn.

Huấn luyện nhận biết lượng tử hóa (QAT) nổi
bật vì thành công gần đây trong việc giảm không
chỉ các yêu cầu bộ nhớ mà còn độ phức tạp tính
toán của các mô hình Transformer (Bhandare et al.,
2019; Zafrir et al., 2019; Kim et al., 2021a). Mặc
dù QAT phản ánh các lỗi lượng tử hóa trong quá
trình tính toán chuyển tiếp của gradient ngẫu nhiên
giảm dần để huấn luyện một mô hình mạnh mẽ
hơn đối với các lỗi lượng tử hóa, việc lượng tử
hóa các tham số trọng số của Transformers với
độ chính xác thấp hơn 2-bit làm giảm độ chính
xác. Do đó, nhiều kỹ thuật QAT gần đây đã sử
dụng khung KD để chưng cất khả năng của giáo
viên độ chính xác đầy đủ cho mô hình học sinh
với các tham số độ chính xác giảm (Zhang et al.,
2020; Bai et al., 2021; Jin et al., 2021; Li et al.,
2022). Tuy nhiên, ít được hiểu biết về việc phương
pháp KD nào trong số các phương pháp khác nhau
phù hợp nhất với QAT của các Transformer lớn.
Không có sự biện minh cẩn thận, hầu hết các
công trình trước đây đã áp dụng chưng cất theo
lớp của điểm attention và đầu ra Transformer với
loss MSE ngoài loss KL-Div cơ bản trên đầu ra
mô hình. Do đó, không rõ liệu cài đặt KD như
vậy có hữu ích nhất cho QAT trên các bộ mã hóa
Transformer quy mô lớn như BERT-Large.arXiv:2211.11014v1 [cs.CL] 20 Nov 2022

--- TRANG 2 ---
Trong công trình này, chúng tôi cung cấp một
phân tích sâu sắc về KD trong việc khôi phục
attention cho QAT của Transformers về mặt
nguồn tri thức và mục tiêu. Chúng tôi đầu tiên
tiết lộ rằng KD tất cả các lớp của lớp Transformer
trung gian là thiết yếu cho QAT, trái ngược với
nén mô hình dựa trên KD. Trong trường hợp
BERT-Base, chúng tôi tiếp tục khám phá rằng
KD dựa trên KL-Div trên attention-map (được
gọi là attention-map loss) vượt trội hơn kỹ thuật
KD trước đây sử dụng loss MSE trên điểm attention.
Tuy nhiên, attention-map loss không đủ cho các
bộ mã hóa Transformer lớn vì lượng tử hóa trọng
số làm gián đoạn việc truyền attention cho các
nhiệm vụ NLP cụ thể khi có nhiều lớp. Do đó,
chúng tôi thiết kế một KD sâu sắc, loss MSE trên
đầu ra attention (được gọi là attention-output
loss), và giúp bảo tồn việc khôi phục attention
cùng với nhiều lớp. Các attention-map và output
loss được đề xuất và sự kết hợp của chúng được
đánh giá trên các mô hình bộ mã hóa Transformer
khác nhau (BERT-Base/Large và một mô hình
ngôn ngữ Hàn Quốc giống BERT (ULM). Kết
quả thực nghiệm chứng minh rằng các phương
pháp KD được đề xuất đáng kể tăng cường độ
chính xác mô hình vượt qua tối tân cho QAT với
lượng tử hóa trọng số dưới 2-bit tích cực.

Chúng tôi tóm tắt đóng góp của mình như sau:
• Chúng tôi cải thiện các kỹ thuật KD trước đây
cho QAT để tăng cường độ chính xác của các
bộ mã hóa Transformer lớn.
• Chúng tôi tiết lộ định lượng rằng attention-map
loss (dựa trên KL-Div) vượt trội hơn attention-
score loss hiện có (dựa trên MSE). Attention-map
loss được đề xuất đặc biệt có lợi cho mô hình
BERT-Base.
• Chúng tôi khám phá các đặc điểm attention
phụ thuộc vào nhiệm vụ, đặc biệt đáng chú ý
trong BERT-Large. Đặc biệt, chúng tôi tiết lộ
rằng các nhiệm vụ cụ thể trên các Transformer
lớn bị đồng nhất hóa đầu ra attention khi trọng
số được lượng tử hóa. Chúng tôi đề xuất một
phương pháp KD mới, attention-output loss,
để giải quyết vấn đề này.
• Chúng tôi tiếp tục khám phá tiềm năng thống
nhất attention-map và output loss để xử lý
các đặc điểm attention phụ thuộc vào nhiệm
vụ một cách phổ quát.
• Chúng tôi đánh giá các phương pháp KD được
đề xuất trên các bộ mã hóa Transformer quy
mô lớn và các nhiệm vụ NLP khác nhau, đạt
được độ chính xác tối tân cho QAT dưới 2-bit
tích cực.

Hình 1: Kiến trúc lớp Transformer.

2 Công trình Liên quan
2.1 Mô hình Bộ mã hóa Transformer
Các mô hình bộ mã hóa dựa trên Transformer như
BERT (Devlin et al., 2019) đã được áp dụng rộng
rãi cho các nhiệm vụ xử lý ngôn ngữ tự nhiên
(NLP) như trả lời câu hỏi và suy luận ngôn ngữ.
Như Hình 1 cho thấy, những mô hình này được
xây dựng với các lớp Transformer bao gồm Multi-
Head Attention (MHA) và Feed-Forward Network
(FFN) (Vaswani et al., 2017). Đầu vào cho lớp
Transformer thứ l là Xl ∈ R^{n×d} trong đó n và
d lần lượt là độ dài chuỗi và kích thước trạng thái
ẩn. Gọi NH là số đầu attention và dh = d/NH.
W^Q_h, W^K_h, W^V_h ∈ R^{d×dh} là các tham số
trọng số chuyển đổi Xl thành Query (Q = XlW^Q_h),
Key (K = XlW^K_h), và Value (V = XlW^V_h),
tương ứng. Sau đó, điểm attention (ASh = QK^T),
bản đồ attention (AMh = Softmax_h(ASh/√d)),
và ngữ cảnh attention (ACh = AMhV).

MHA được định nghĩa là:
MHA(Xl) = Concat(AC1;:::ACNH)W^O. (1)

Được thúc đẩy bởi (Kobayashi et al., 2020), MHA
có thể được viết lại cho mỗi token i:
MHA(Xl)(i) = Σ^n_{j=1} αi,j f(Xl(j)), (2)

trong đó f(x) := (xW^V + b^V)W^O và αi,j là xác
suất attention thứ j của token thứ i trong AMh.
Do đó, MHA có thể được phân tách thành hai phần:
tạo self-attention (SA-GEN) tương ứng với

--- TRANG 3 ---
bản đồ attention (α), và truyền self-attention
(SA-PROP) tương ứng với f(x). Hình 1 cho thấy
phần nào là SA-GEN và SA-PROP tương ứng.

FFN bao gồm hai lớp kết nối đầy đủ với các tham
số trọng số W1 và W2:
FFN(Yl) = GELU(YlW1 + b1)W2 + b2. (3)

Do đó, đầu ra của lớp Transformer Xl+1 được
định nghĩa là:
Yl = LayerNorm(Xl + MHA(Xl));
Xl+1 = LayerNorm(Yl + FFN(Yl)). (4)

Ở đây, Yl và Xl+1 được gọi là đầu ra attention
(AO) và đầu ra Transformer, tương ứng.

2.2 Chưng cất Tri thức cho Nén Mô hình
Transformer

Chưng cất tri thức (KD) (Hinton et al., 2015) là
một khung học chuyển giao mà một mô hình nhẹ
(học sinh) học từ tri thức được chưng cất từ một
mô hình cồng kềnh (giáo viên). Vì KD cung cấp
cho học sinh thông tin để đạt được khả năng của
giáo viên, KD đã được áp dụng rộng rãi cho việc
nén mô hình của các mô hình Transformer quy
mô lớn như BERT. Một phương pháp chưng cất
cơ bản là khớp phân phối xác suất tại đầu ra của
mô hình giáo viên và học sinh qua loss CE, như
trong DistilBERT (Sanh et al., 2019). Ngoài chưng
cất nhãn mềm này, PKD (Sun et al., 2019) đề xuất
KD trên đầu ra chuẩn hóa của mỗi lớp Transformer,
vì chưng cất trên các biểu diễn trung gian của
giáo viên có thể có lợi cho học sinh. MobileBERT
(Sun et al., 2020) cũng sử dụng KD theo đầu trên
bản đồ attention và kiến trúc tùy chỉnh cho các
tính toán Transformer hiệu quả. MiniLM và
MiniLMv2 (Wang et al., 2020, 2021) tiếp tục
chuyển giao tri thức quan hệ từ bản đồ self-
attention, nhưng chỉ tại một lớp Transformer
duy nhất (nằm ở cuối hoặc giữa trên). (Liu et al.,
2021a) tiếp tục khẳng định rằng chưng cất nhiều
biểu diễn trung gian hơn không nhất thiết giúp
cải thiện độ chính xác của học sinh.

Mặc dù những kỹ thuật nén dựa trên KD này đã
phát triển các cấu trúc BERT hiệu quả, vẫn có
hiểu biết hạn chế về KD trong lượng tử hóa mô
hình. Đặc biệt, chúng tôi là những người đầu
tiên tiết lộ định lượng rằng chưng cất nhiều biểu
diễn trung gian hơn giúp QAT giảm khoảng cách
độ chính xác giữa học sinh lượng tử hóa và giáo
viên độ chính xác đầy đủ.

2.3 Lượng tử hóa cho BERT

Lượng tử hóa là một kỹ thuật đầy hứa hẹn để
giảm chi phí suy luận cao của các mô hình quy
mô lớn mà không thay đổi cấu trúc mô hình. Thay
vì biểu diễn số trong dấu phẩy động 32-bit (FP32),
việc sử dụng biểu diễn điểm cố định, như lượng
tử hóa số nguyên 8-bit (INT8), đã đạt được tăng
tốc đáng kể và tiết kiệm lưu trữ cho BERT (Zafrir
et al., 2019; Kim et al., 2021a; Lin et al., 2021).
Tuy nhiên, lượng tử hóa trực tiếp các tham số
trọng số dẫn đến suy giảm độ chính xác mô hình
gốc khi độ chính xác bit lượng tử hóa thấp. Do
đó, huấn luyện nhận biết lượng tử hóa (QAT)
thường được áp dụng cho lượng tử hóa mô hình
độ chính xác cực thấp. Gần đây, QAT đã được
áp dụng để nén BERT với độ chính xác thấp hơn
2-bit. TernaryBERT (Zhang et al., 2020) biểu diễn
mỗi tham số trọng số thành một trong ba giá trị
{−1, 0, 1}. TernaryBERT tích cực kết hợp KD
vào QAT để cải thiện suy giảm độ chính xác. Để
giảm độ chính xác bit, BinaryBERT (Bai et al.,
2021) đề xuất một quy trình QAT đã sửa đổi với
khởi tạo trọng số cụ thể cho lượng tử hóa nhị
phân. DQ-BART (Li et al., 2022) tiếp tục kết hợp
nén mô hình (qua giảm lớp) và lượng tử hóa bằng
cách khai thác KD.

Mặc dù KD đã trở thành một kỹ thuật de-facto
cho QAT, vẫn thiếu hiểu biết về lý do tại sao.
Đặc biệt, các phương pháp QAT được đề cập
ở trên đều sử dụng KD theo lớp trên điểm self-
attention (ASl) và đầu ra Transformer (Xl) cùng
với KD trên nhãn mềm. Xem xét nhiều kỹ thuật
KD với các lựa chọn khác nhau cho nguồn tri
thức và mục tiêu, không rõ liệu công thức hiện
tại có giúp QAT nhiều nhất hay không. Công trình
này điều tra các kỹ thuật KD theo lớp trước đây
và cải thiện chúng với các mục tiêu và nguồn tri
thức mới.

3 Kỹ thuật KD Trước đây cho QAT

Trong phần này, chúng tôi điều tra các kỹ thuật
KD trước đây cho QAT được đánh giá trên BERT-
Base. Như đã thảo luận trước đó, các kỹ thuật
KD thường được sử dụng cho QAT bao gồm 1)
chưng cất tất cả các lớp và 2) chưng cất trên
SA-GEN. Đầu tiên, chúng tôi cung cấp biện minh
và cải thiện cho những kỹ thuật này. Sau đó
chúng tôi tiếp tục thể hiện hạn chế khi chúng
được áp dụng cho các bộ mã hóa Transformer
quy mô lớn.

--- TRANG 4 ---
Hình 2: Minh họa chiến lược lựa chọn lớp trong nén
mô hình và lượng tử hóa mô hình. Trái: Chiến lược
ánh xạ đồng nhất. Phải: Chưng cất tất cả các lớp.

3.1 Chưng cất Tất cả các Lớp cho QAT

Nói chung, biểu diễn nội tại của giáo viên, như
đầu ra lớp, được sử dụng rộng rãi cho chưng cất
tri thức cho nén mô hình (Aguilar et al., 2020).
Tuy nhiên, có sự khác biệt rõ rệt trong KD giữa
nén mô hình thông thường và lượng tử hóa. Ví
dụ, Hình 2 cho thấy hai ánh xạ lớp-đến-lớp đại
diện cho KD: chưng cất lớp được chọn cho nén
mô hình (trái) và chưng cất tất cả các lớp cho
QAT (phải). Trong trường hợp chưng cất lớp được
chọn, nghiên cứu cho thấy rằng tiện ích biên của
tri thức trạng thái ẩn (HSK) giảm dần khi nhiều
HSK hơn đã được chưng cất (Liu et al., 2021a).
Ngược lại, hầu hết các phương pháp QAT trước
đây áp dụng KD trên đầu ra Transformer của tất
cả các lớp. Sự tương đương cấu trúc của giáo
viên và học sinh của các phương pháp QAT làm
cho lựa chọn này trở nên tự nhiên, nhưng có ít
sự biện minh.

Chúng tôi phỏng đoán rằng lượng tử hóa được
áp dụng cho các tham số trọng số làm gián đoạn
chức năng của lớp Transformer, cần thiết hướng
dẫn theo lớp. Để xác thực phỏng đoán này, chúng
tôi đã tiến hành hai thí nghiệm. Đầu tiên, chúng
tôi so sánh độ chính xác của chưng cất lớp được
chọn đồng nhất với số lượng lớp được chưng cất
khác nhau. Như thể hiện trong Hình 3a, độ chính
xác tăng cùng với số lượng lớp được chưng cất,
và chưng cất tất cả các lớp vượt trội đáng kể so
với chưng cất lớp được chọn. Ngoài ra, chúng tôi
so sánh bề mặt loss của hai phương pháp chưng
cất sau QAT về mặt giá trị riêng tối đa Hessian
(Park và Kim, 2022). Chúng tôi áp dụng chưng
cất lựa chọn lớp đơn như một chiến lược chưng
cất lớp được chọn. Đặc biệt, chúng tôi sử dụng
phương pháp chọn lớp thứ 10 của mô hình BERT-
Base, điều này hữu ích nhất trong hiệu suất như
một chưng cất lựa chọn lớp đơn. Như thể hiện
trong Hình 3b, chưng cất tất cả các lớp cho thấy
độ lớn nhỏ hơn của

Hình 3: So sánh (a) độ chính xác RTE và (b) Phổ Giá
trị riêng tối đa Hessian giữa chưng cất lớp được chọn
và chưng cất tất cả các lớp trong BERT-Base.

Giá trị riêng, cho thấy bề mặt loss mượt mà hơn.
Do đó, chúng ta có thể kết luận rằng chưng cất
theo lớp giúp huấn luyện học sinh với các tham
số trọng số lượng tử hóa.

3.2 Cải thiện KD trên Tạo Self-Attention

Chúng tôi tiếp tục điều tra mục tiêu của KD tất
cả các lớp. Như đã thảo luận trước đó, các phương
pháp QAT trước đây sử dụng loss MSE trên điểm
attention (được gọi là attention-score loss) cho
KD tất cả các lớp, như sau:

Lscore = Σ^{L-1}_{l=0} MSE(AS^T_l, AS^S_l). (5)

Cho rằng bản đồ attention nắm bắt mối tương
quan của một token đến tất cả các token khác,
việc duy trì tầm quan trọng tương đối của các
token là thiết yếu. Tuy nhiên, lượng tử hóa về
bản chất kẹp và biểu diễn thô các tham số trọng
số, làm cho attention ít phân biệt hơn. Chúng tôi
mong đợi KD giúp duy trì sự chênh lệch, nhưng
attention-score loss không phải là một mục tiêu
thích hợp vì nó chủ yếu tập trung vào việc khớp
logit (Kim et al., 2021b).

Như một thay thế, chúng tôi đề xuất sử dụng
loss KL-Div trên attention-map (được gọi là
attention-map loss) được định nghĩa như sau:

Lmap = 1/NH Σ^{NH}_{h=1} Σ^n_{t=1} D_KL(AM^T_{l,h,t}||AM^S_{l,h,t}). (6)

Giả sử rằng tham số siêu nhiệt độ (τ) là một,
KL-Div tập trung vào khớp nhãn (Kim et al.,
2021b). Do đó tầm quan trọng tương đối của
attention qua các token được duy trì tốt hơn với
attention-map loss. Mặc dù attention-map loss
đã được sử dụng trước đây trong nén mô hình
(Wang et al., 2021), chúng tôi là những người
đầu tiên

--- TRANG 5 ---
Hình 4: Tỷ lệ độ dài bao phủ và ranking loss cho mỗi
đầu attention trong BERT-Base. Trục X: Số đầu attention.
Quantized: Mô hình lượng tử hóa không áp dụng QAT,
KL-Div: attention-map loss, MSE: attention-score loss

tiết lộ định lượng lợi ích của attention-map loss
trong bối cảnh QAT.

Chúng tôi đã giới thiệu hai chỉ số đặc trưng cho
bản đồ attention để đánh giá loss KD được đề
xuất một cách định lượng. Tỷ lệ độ dài bao phủ
nắm bắt độ lệch bản đồ attention của học sinh
từ giáo viên dựa trên phạm vi token Top-K.
Ranking loss (Liu et al., 2021b) cho thấy sự
tương tự trong thứ hạng attention của giáo viên
và học sinh. Hình 4 so sánh tỷ lệ độ dài bao
phủ và ranking loss của mọi đầu attention của
BERT-Base trên nhiệm vụ SST-2 (xu hướng tổng
thể giống nhau cho các lớp và nhiệm vụ khác).
Như thể hiện trong hình, lượng tử hóa tăng đáng
kể tỷ lệ độ dài bao phủ và ranking loss, cho thấy
rằng thứ hạng tương đối của attention bị biến
dạng nghiêm trọng. Attention-score loss giúp
giảm sự biến dạng như vậy, nhưng vẫn còn các
đỉnh. Ngược lại, attention-map loss thành công
trong việc ức chế các đỉnh, duy trì tầm quan
trọng tương đối của bản đồ attention. Chi tiết
hơn về tỷ lệ độ dài bao phủ và ranking loss được
mô tả trong Phụ lục A.1.

Để hiểu thêm về tác động của các mục tiêu KD
lên độ chính xác QAT, chúng tôi đã tiến hành
quét nhiệt độ của KL-Div. Vì gradient của loss
KL-Div có thể được đơn giản hóa thành gradient
của loss MSE khi nhiệt độ đủ lớn (Kim et al.,
2021b), chúng ta có thể quản lý hành vi của loss
KL-Div thông qua quét giá trị nhiệt độ (τ), trong
đó τ=1 và τ=∞ tương tự như attention-map và
attention-score loss,

tương ứng. Bảng 1 cho thấy độ chính xác QAT
của BERT-Base trên CoLA và STS-B với τ khác
nhau. Như thể hiện trong bảng, độ chính xác của
mô hình lượng tử hóa tăng khi thuật ngữ loss
trở nên tương tự như attention-map loss. Cải
thiện hiệu suất như vậy hỗ trợ hiểu biết của
chúng tôi rằng 1) khớp nhãn là quan trọng để
bù đắp QAT trên SA-GEN, và 2) attention-map
loss hiệu quả hơn cho khớp nhãn.

Tham số Siêu Nhiệt độ Phân kỳ KL (τ)
Nhiệm vụ KL Div τ=1 τ=5 τ=10 τ=20 MSE
CoLA 50.76 50.76 49.11 47.51 47.19 47.51
STS-B 87.78 87.69 87.20 87.19 87.29 87.55

Bảng 1: KD-QAT dựa trên Attention với quét Tham
số Siêu Nhiệt độ KL Div trên các nhiệm vụ CoLA và
STS-B với BERT-Base (Mỗi thí nghiệm được lặp lại
5 lần.)

4 KD cho QAT trên Transformers Lớn

Chúng tôi mở rộng điều tra các kỹ thuật KD cho
QAT trên các mô hình transformer lớn. Trong
phần này, chúng tôi đầu tiên tiết lộ hạn chế của
attention-map loss do các đặc điểm phụ thuộc
vào nhiệm vụ. Sau đó chúng tôi đề xuất một loss
KD mới, attention-output loss, để giải quyết thách
thức này. Cuối cùng, chúng tôi đề xuất sự kết
hợp của hai loss để xử lý các đặc điểm phụ thuộc
vào nhiệm vụ.

4.1 Đặc điểm Phụ thuộc vào Nhiệm vụ

Mặc dù cùng các mô hình được huấn luyện trước
được sử dụng cho việc tinh chỉnh downstream,
các đặc điểm của attention khác nhau tùy thuộc
vào nhiệm vụ (Kovaleva et al., 2019). Được thúc
đẩy bởi các thảo luận của (Bondarenko et al.,
2021) rằng các ngoại lệ trong kích hoạt của kết
nối dư (tức là SA-PROP) sắp xếp các mẫu attention
cụ thể, chúng tôi kiểm tra những ngoại lệ này
qua các đường cong min-max tại đầu ra attention
để hiểu các đặc điểm phụ thuộc vào nhiệm vụ
cho lượng tử hóa.

Hình 5 vẽ dải động của đầu ra attention (Yl)
qua các token đầu vào. Có hai trường hợp đại
diện: Trường hợp-1) nhiệm vụ với các giá trị
attention riêng biệt (đặc biệt cho các token đặc
biệt) như RTE, và Trường hợp-2) nhiệm vụ với
các giá trị attention đồng nhất như SST-2. Các
đặc điểm attention tổng thể của mỗi trường hợp
được tăng cường khi kích thước mô hình tăng.
Ví dụ, các tính năng riêng biệt của attention RTE
trở nên cực đoan hơn trên BERT-Large.

--- TRANG 6 ---
Vì lượng tử hóa kẹp và biểu diễn thô các giá trị,
việc duy trì attention riêng biệt cho các nhiệm
vụ trong Trường hợp-1 là thách thức. Như đã
thảo luận trong Phần 3.2, trong trường hợp BERT-
Base, attention-map loss có khả năng khôi phục
sự chênh lệch trong attention (Hình 5a-Trên).
Tuy nhiên, như thể hiện trong Hình 5a-Dưới,
attention-map loss thất bại trong việc điều chỉnh
attention trong trường hợp BERT-Large.

Chúng tôi phỏng đoán rằng attention-map loss
thất bại do số lượng lớp tăng của BERT-Large.
Chúng tôi áp dụng khung phân tích của (Kobayashi
et al., 2020) để phân tích riêng biệt hành vi theo
lớp của SA-GEN và SA-PROP. Hình 6 vẽ khoảng
cách trung bình của SA-GEN và SA-PROP của
một token đặc biệt [SEP] từ giáo viên cho RTE
(Trường hợp-1) và SST-2 (Trường hợp-2) với
BERT-Large. Lưu ý rằng attention-map loss ức
chế khoảng cách trong SA-GEN. Việc ức chế
SA-GEN này làm xấu đi đầu ra attention (c.f.,
attention-map loss hiệu quả cho SST-2). Với
nhiều lớp, lượng tử hóa dọc theo SA-PROP thất
bại KD với attention-map loss để khôi phục
attention đặc biệt. Do đó, chúng ta cần một loss
KD mới để xử lý sự gián đoạn từ SA-PROP.

4.2 Attention Output Loss

Quan sát từ Hình 6 ngụ ý rằng SA-PROP trở
thành nguồn gián đoạn cho QAT của BERT-Large
trên các nhiệm vụ Trường hợp-1. Một cách để
ức chế lỗi lượng tử hóa dọc theo SA-PROP là
áp dụng KD trực tiếp lên SA-PROP. Do đó, chúng
tôi thiết kế một loss KD mới, attention-output
loss như sau:

Loutput = Σ^{L-1}_{l=0} MSE(Y^T_l, Y^S_l). (7)

Lợi ích của attention-output loss là rõ ràng. Như
thể hiện trong Hình 5a, đầu ra attention với
attention-output loss theo attention đặc biệt của
giáo viên độ chính xác đầy đủ. Chúng ta có thể
hiểu cơ chế của attention-output loss qua Hình
6; attention-output loss cho phép sửa đổi SA-GEN
để điều chỉnh bản đồ attention sao cho đầu ra
attention kết quả khớp tốt hơn với giáo viên.
Lưu ý rằng sự thay đổi trong SA-GEN xảy ra tại
các lớp trên của mô hình Transformer; do đó,
attention-output loss có lợi hơn cho các mô hình
Transformer lớn.

Để hiểu thêm về các đặc điểm phụ thuộc vào
nhiệm vụ, chúng tôi quan sát thực nghiệm tác
động của attention-output loss lên xác suất self-
attention của bản đồ attention.

Hình 5: So sánh dải động theo token của đầu ra
attention (Yl) giữa BERT-Base (trên) và BERT-Large
(dưới) cho RTE và SST-2. Mỗi cặp đường cong mô
tả các giá trị min-max tại đầu ra attention của token.
FP-min-max và Map/Output-min-max tương ứng với
các đường cong min-max của mô hình giáo viên và
học sinh với attention map/output loss, tương ứng.

attention. Để định lượng sự sửa đổi trong bản
đồ attention, chúng tôi giới thiệu tỷ lệ thứ hạng,
được định nghĩa là thứ hạng của xác suất attention
của một token riêng lẻ được chuẩn hóa bởi độ
dài chuỗi. Hình 7 theo dõi tỷ lệ thứ hạng của
các token được chọn của mô hình giáo viên độ
chính xác đầy đủ và học sinh lượng tử hóa cho
mỗi đầu. Trong trường hợp RTE (tức là nhiệm
vụ Trường hợp-1), QAT với attention-output loss
thể hiện những thay đổi nhanh chóng trong thứ
hạng theo một hướng cụ thể hướng tới giảm
attention-output loss. Trong trường hợp SST-2,
tuy nhiên, tình huống rất khác; thứ hạng của
các token được chọn thay đổi đáng kể bất kể
KD cho QAT. Do đó, KD trên đầu ra attention
không thể điều khiển thứ hạng theo bất kỳ hướng
có ý nghĩa nào. Những quan sát này xác nhận
tầm quan trọng của việc xem xét các đặc điểm
phụ thuộc vào nhiệm vụ để KD thành công cho
QAT.

--- TRANG 7 ---
Hình 6: Khoảng cách trung bình của SA-GEN (xác
suất self-attention - α) và SA-PROP (truyền self-
attention - f(x)) từ mô hình giáo viên trong hai nhiệm
vụ (RTE, SST-2) với BERT-Large.

4.3 Loss Attention-Map và Output Thống nhất

Xem xét các đặc điểm attention phụ thuộc vào
nhiệm vụ của BERT-Large, chúng tôi tiếp tục
khám phá tiềm năng thống nhất attention-map
và output loss cho QAT. Lưu ý rằng sự ưu tiên
giữa attention-map và output loss khác nhau tùy
theo kích thước mô hình (ví dụ: BERT-Base vs.
Large) và nhiệm vụ (Trường hợp-1 vs. Trường
hợp-2). Để khám phá, chúng tôi công thức hóa
một loss attention-map và output thống nhất với
β như một tham số pha trộn như sau:

LSM1 = Lmap + βLoutput;
LSM2 = Loutput + βLmap;
trong đó β ∈ {0.1, 0.2, 0.3, ..., 0.9}. (8)

Như sẽ được thảo luận trong Phần 5.2, loss thống
nhất có thể tăng cường độ chính xác của loss KD
có hiệu suất tốt nhất (hoặc attention-map hoặc
output loss). Khi áp dụng loss thống nhất này
trong KD-QAT, chúng tôi xác định rằng mọi
nhiệm vụ đều có các tham số pha trộn thuận lợi
riêng cho điểm số của chúng, điều này cho thấy
các đặc điểm phụ thuộc vào nhiệm vụ. Thông
tin chi tiết về tham số pha trộn cho mỗi nhiệm
vụ có trong Phụ lục A.3.

Hình 7: Tỷ lệ thứ hạng cho mỗi đầu (48 đầu của ba
lớp cuối) trên nhiệm vụ SST-2 và RTE với BERT-Large.
Trái: Mô hình Giáo viên, Phải: Mô hình Học sinh

5 Thí nghiệm
5.1 Thiết lập Thí nghiệm

Chúng tôi sử dụng ba mô hình bộ mã hóa Transformer
(BERT-Base, BERT-Large, ULM-Encoder-Large)
để đánh giá các phương pháp KD được đề xuất.
BERT (Devlin et al., 2019) bao gồm lớp bộ mã
hóa Transformer, được tinh chỉnh cho các nhiệm
vụ downstream GLUE (Devlin et al., 2019). ULM-
Encoder-Large (Seo et al., 2022) là một mô hình
ngôn ngữ Hàn Quốc dựa trên T5 (Raffel et al.,
2019), được tinh chỉnh cho các nhiệm vụ downstream
KLUE (Park et al., 2021a).

Cấu hình của mỗi mô hình như sau:
1. BERT-Base. Đây là bộ mã hóa Transformer
12 lớp với chiều ẩn 768 sử dụng 12 đầu attention
và chứa khoảng 110M tham số.
2. BERT-Large. Nó bao gồm 24 lớp bộ mã hóa
Transformer, và sử dụng chiều ẩn 1024 với
16 đầu attention. Mô hình này chứa khoảng
340M tham số.
3. ULM-Encoder-Large. Nó cũng có cùng cấu
hình như BERT-large ngoại trừ chiều feed-
forward, là 2816 cho ULM-Encoder-Large
trong khi BERT-Large có 4096. Nó chứa
khoảng 280M tham số.

Chúng tôi khởi tạo QAT từ các mô hình được
tinh chỉnh cụ thể cho nhiệm vụ. Các thí nghiệm
của chúng tôi được thực hiện trên GPU A6000.
Việc triển khai của chúng tôi dựa trên

--- TRANG 8 ---
[Này là bảng hiệu suất phức tạp với nhiều cột và hàng, tôi sẽ dịch tiêu đề và phần quan trọng]

Nhiệm vụ GLUE RTE† CoLA† STS-B† SST-2? QNLI? MNLI? QQP? MRPC AVG
(Tập dữ liệu) (2.5k) (8.5k) (5.7k) (67k) (108k) (393k) (364k) (3.5k)
Độ chính xác đầy đủ 73.28 58.04 89.24 92.09 91.32 84.37 89.30 87.77 83.39
Baseline 68.53 ±1.69 49.61 ±0.79 87.55 ±0.14 92.01 ±0.29 90.65 ±0.05 84.21 ±0.10 89.06 ±0.40 88.58 ±0.40 81.28
Map 70.39 ±0.78 50.40 ±1.03 87.78 ±0.15 92.13 ±0.22 90.98 ±0.17 84.31 ±0.10 89.22 ±0.40 88.07 ±0.40 81.66
Output 70.65 ±1.27 49.05 ±0.50 87.77 ±0.14 92.13 ±0.22 90.58 ±0.07 84.24 ±0.01 89.17 ±0.20 87.01 ±0.43 81.33
Map+Output 71.68 ±1.19 50.50 ±0.45 87.73 ±0.16 92.39 ±0.18 90.91 ±0.14 84.33 ±0.06 89.28 ±0.10 88.18 ±0.53 81.87

Bảng 2: BERT-Base: Hiệu suất kết quả KD-QAT trên benchmark GLUE (kích hoạt 8-bit và lượng tử hóa trọng số ternary, tỷ lệ nén của BERT-Base lượng tử hóa là 14.9x). Các nhiệm vụ tập dữ liệu nhỏ (dưới 10k) được lặp lại 5 lần; những cái khác được lặp lại 3 lần. † và ? chỉ ra các nhiệm vụ GLUE Trường hợp-1 và Trường hợp-2 tương ứng.

[Bảng tương tự cho BERT-Large]

Bảng 3: BERT-Large: Hiệu suất kết quả KD-QAT trên benchmark GLUE (kích hoạt 8-bit và lượng tử hóa trọng số ternary, tỷ lệ nén của BERT-Large lượng tử hóa là 15.4x). Các nhiệm vụ tập dữ liệu nhỏ (dưới 10k) được lặp lại 5 lần; những cái khác được lặp lại 3 lần. † và ? chỉ ra các nhiệm vụ GLUE Trường hợp-1 và Trường hợp-2 tương ứng.

codebase PyTorch TernaryBERT.¹ Tất cả các tham số embedding và trọng số được ternarized và các kích hoạt được lượng tử hóa thành 8-bit cho QAT. chúng tôi sử dụng ternarization theo lớp cho trọng số trong các lớp Transformer trong khi theo hàng cho word embedding, giống như TernaryBERT (Zhang et al., 2020). Ngoài ra, tất cả các thí nghiệm được lặp lại 5 lần, trừ khi được nêu khác.

Để so sánh hiệu suất, chúng tôi xem xét các tùy chọn KD sau:
• Baseline. TernaryBERT tiêu chuẩn với attention-score và loss đầu ra Transformer cùng với KD trên nhãn mềm.
• Map. Sử dụng attention-map loss thay vì attention-score loss của TernaryBERT.
• Output. Sử dụng attention-output loss thay vì attention-score loss của TernaryBERT.
• Map+Output. Sử dụng loss attention-map và output thống nhất thay vì attention-score loss của TernaryBERT.

5.2 Thí nghiệm trên BERT-Base và Large

Bảng 2 và 3 cho thấy kết quả trên tập phát triển qua benchmark GLUE. Các quan sát đáng chú ý được tóm tắt như sau:

• Các nhiệm vụ GLUE có thể được phân loại thành hai trường hợp. Trường hợp-1(†): RTE, CoLA, STS-B. Trường hợp-2(?): SST-2, QNLI, MNLI, QQP.
• Trong trường hợp BERT-Base, attention-map loss có lợi cho tất cả các nhiệm vụ trong Trường hợp-1 và Trường hợp-2, trong khi attention-output loss không hiệu quả.
• Trong trường hợp BERT-Large, attention-map loss hữu ích một cách cận biên cho Trường hợp-1 và Trường hợp-2, trong khi attention-output loss tăng cường đáng kể độ chính xác của các nhiệm vụ Trường hợp-1.
• Tổng thể, loss thống nhất tạo thuận lợi cho độ chính xác QAT, ngoại trừ BERT-Large trên các nhiệm vụ Trường hợp-1 (trong đó attention-output loss hoạt động tốt nhất).
• MRPC là một trường hợp đặc biệt; độ chính xác QAT thường vượt trội hơn độ chính xác Độ chính xác Đầy đủ, ngụ ý rằng nhiễu lượng tử hóa điều chỉnh mô hình một cách thuận lợi cho nhiệm vụ này.

5.3 Thí nghiệm trên ULM-Encoder-Large

Bảng 4 tóm tắt kết quả trung bình ba lần đánh giá các phương pháp KD của chúng tôi cho QAT trên ULM-Encoder-Large. Tổng thể, ULM-Encoder-Large khá mạnh mẽ đối với lượng tử hóa, nhưng các phương pháp KD của chúng tôi

--- TRANG 9 ---
Nhiệm vụ KLUE-TC KLUE-STS NSMC AVG
(Tập dữ liệu) (45k) (11k) (150k)
Độ chính xác đầy đủ 85.76 92.11 91.87 89.91
Baseline 85.56 ±0.08 91.04 ±0.10 91.13 ±0.04 89.24
Map 85.41 ±0.10 91.44 ±0.23 91.24 ±0.10 89.36
Output 85.63 ±0.23 91.03 ±0.11 91.39 ±0.15 89.35
Map + Output 85.57 ±0.21 91.11 ±0.14 91.65 ±0.12 89.44

Bảng 4: ULM-Large: Hiệu suất kết quả KD-QAT trên tập dữ liệu dev KLUE và NSMC

Nhiệm vụ GLUE RTE† CoLA† STS-B† SST-2? QNLI?
Độ chính xác đầy đủ 70.39 60.31 89.83 92.32 92.29
Map 66.42 53.16 88.65 92.20 91.93
MHA loss 66.78 54.01 88.69 92.08 91.84
MHA loss + Residual 69.50 54.71 89.10 92.13 91.92

Bảng 5: Nghiên cứu ablation về attention-output loss phân tách các nguồn của nó thành MHA(Xl) và đường dư (Xl) với BERT-Large trên các nhiệm vụ GLUE. † và ? chỉ ra các nhiệm vụ GLUE Trường hợp-1 và Trường hợp-2 tương ứng.

vượt qua baseline (= TernaryBERT). Cụ thể hơn, attention-map loss hiệu quả hơn trên nhiệm vụ KLUE-STS, trong khi output loss vượt trội hơn map loss trong KLUE-TC và NSMC, như thể hiện trong bảng. Hơn nữa, loss thống nhất đạt được độ chính xác tốt nhất trên NSMC. Do đó, các loss KD được đề xuất có thể cải thiện độ chính xác của phương pháp QAT baseline.

5.4 Nghiên cứu Ablation

Trong Phần 4.2, chúng tôi đề xuất attention-output loss để ức chế lỗi lượng tử hóa dọc theo SA-PROP. Như thể hiện trong Hình 1 và được định nghĩa trong Eq. 4, attention-output loss tích hợp hai nguồn của SA-PROP: MHA(Xl) và kết nối dư (Xl). Chúng tôi điều tra hiệu quả của attention-output loss bằng cách sử dụng một trong hai phần của nó duy nhất như một mục tiêu hàm loss: MHA(Xl) (Chúng tôi gọi hàm loss này là MHA loss). Cụ thể, MHA loss sử dụng MHA(Xl) như một mục tiêu của hàm loss thay vì Yl trong Eq. 7.

Bảng 5 cho thấy rằng phương pháp MHA loss cải thiện hiệu suất một cách cận biên trong các nhiệm vụ Trường hợp-1. Khi kết nối dư được thêm vào mục tiêu MHA loss (MHA loss +Residual trong Bảng 5), tương đương với attention-output loss, hiệu suất của tất cả các nhiệm vụ tăng lên. (đặc biệt trong các nhiệm vụ GLUE Trường hợp-1). Những quan sát này cho thấy rằng việc kết hợp kết nối dư như một mục tiêu của attention-output loss là đáng kể trong việc khôi phục sự gián đoạn của SA-PROP dưới lượng tử hóa.

6 Kết luận

Trong công trình này, chúng tôi điều tra cơ chế của Chưng cất tri thức (KD) cho QAT của các Transformer lớn. Chúng tôi đề xuất hai phương pháp KD, attention-map và attention-output loss, để cải thiện việc khôi phục thông tin self-attention. Kết quả thực nghiệm trên các mô hình bộ mã hóa Transformer khác nhau chứng minh rằng các phương pháp KD được đề xuất và sự kết hợp của chúng đạt được độ chính xác tối tân cho QAT với lượng tử hóa trọng số dưới 2-bit. Mã của chúng tôi có sẵn tại https://github.com/MarsJacobs/kd-qat-large-enc.

7 Hạn chế

Công trình này điều tra cách KD hoạt động cho QAT trên Bộ mã hóa Transformer. Mặc dù các kỹ thuật phân tích được sử dụng trong công trình này tiết lộ nhiều hiểu biết thú vị, một phân tích lý thuyết hơn về tác động của lượng tử hóa dưới KD sẽ được đánh giá cao. Ngoài ra, chúng tôi khám phá tiềm năng thống nhất hai kỹ thuật KD được đề xuất; việc kết hợp cân bằng tự động của hai (hoặc nhiều hơn) loss KD sẽ là một hướng nghiên cứu tương lai thú vị.

Lời cảm ơn

Công trình này được hỗ trợ một phần bởi các khoản tài trợ của Viện Quy hoạch và Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2020-0-01373, Chương trình Trường Cao học Trí tuệ Nhân tạo (Đại học Hanyang), và Số 2022-0-00971, Tổng hợp Logic cho Kiến trúc Tính toán PIM dựa trên NVM).

Tài liệu tham khảo
[Phần tài liệu tham khảo sẽ được dịch nếu cần, nhưng thường giữ nguyên trong các bài báo khoa học]

--- TRANG 10 ---
[Các tài liệu tham khảo tiếp tục...]

--- TRANG 11 ---
[Các tài liệu tham khảo tiếp tục...]

--- TRANG 12 ---
A Phụ lục

A.1 Tỷ lệ Độ dài Bao phủ và Ranking Loss

Như thể hiện trong Phần 3.2, tỷ lệ độ dài bao phủ và ranking loss là các chỉ số cho thấy tầm quan trọng tương đối trong bản đồ attention đã lệch như thế nào dưới lượng tử hóa. Để có được tỷ lệ độ dài bao phủ, đầu tiên, sắp xếp bản đồ attention của học sinh và giáo viên theo thứ tự xác suất. Sau đó chúng ta có thể có được các token Top-K của bản đồ giáo viên nhận được nhiều attention nhất trong bản đồ attention của giáo viên. Sau đó, chúng ta tìm ra cần bao nhiêu token để xem xét để có được tất cả các token Top-K của bản đồ giáo viên từ bản đồ học sinh đã sắp xếp. Số lượng token mà mỗi token phải xem xét để bao phủ các token Top-K của bản đồ giáo viên được gọi là độ dài bao phủ, và chúng ta chuẩn hóa độ dài bao phủ bằng độ dài chuỗi. Chúng ta gọi chỉ số này là tỷ lệ độ dài bao phủ.

(Liu et al., 2021b) đã giới thiệu loss thứ hạng cặp để giữ thứ tự tương đối của các giá trị attention. Loss thứ hạng cặp cho thấy thứ tự tầm quan trọng attention khác nhau như thế nào giữa hai bản đồ attention. Với bất kỳ hai token nào, nếu thứ tự của hai token trong bản đồ học sinh khác với thứ tự đó trong bản đồ giáo viên, chúng ta thêm sự khác biệt giữa các giá trị của hai token vào loss.

Lranking^h = Σ(i=1 to n-1)Σ(j=i+1 to n)((AMS_i - AMS_j) × sign(AMT_i - AMT_j));
Lranking = Σ(h=1 to NH)Lranking^h; (9)

trong đó h là chỉ số của đầu attention.

A.2 Trực quan hóa Bản đồ Attention

Chúng tôi so sánh bản đồ self-attention của mô hình BERT-base độ chính xác đầy đủ được tinh chỉnh và mô hình lượng tử hóa trên nhiệm vụ RTE. Hình 8 cho thấy các bản đồ self-attention từ lớp Transformer thứ 3 (đầu thứ 8) của BERT-Base. Lưu ý rằng việc áp dụng lượng tử hóa làm biến dạng nghiêm trọng bản đồ self-attention của giáo viên. Như thể hiện trong hình, attention-map loss thành công khôi phục bản đồ self-attention của giáo viên, trong khi TernaryBERT thất bại trong việc nắm bắt một số tính năng đặc biệt.

Hình 8: Trực quan hóa bản đồ self-attention với BERT-Base trên Nhiệm vụ RTE (a) Bản đồ self-attention của giáo viên (b) Sau lượng tử hóa không có QAT (c) Sau KD-QAT TernayBERT (d) Attention-map loss

A.3 Khám phá Loss Attention-Map và Output Thống nhất

Như đã đề cập trong Phần 4.3, chúng tôi khám phá hiệu quả của việc thống nhất attention map và output loss cho QAT. Chúng tôi đã tiến hành các thí nghiệm bằng cách cố định một trong hai loss, attention-map và output loss, và thay đổi tham số pha trộn β cho loss khác theo Eq. 8. Bảng 7 và 8 cho thấy kết quả tốt nhất thay đổi tham số pha trộn β trong BERT-Base và BERT-Large. Tổng thể, duy trì attention-map loss (trường hợp SM1) hoạt động tốt hơn các trường hợp ngược lại (trường hợp SM2). Để thể hiện mức độ ảnh hưởng theo tham số pha trộn β, chúng tôi tóm tắt β cho thấy kết quả tốt nhất cho mỗi trường hợp trong Bảng 6. Bảng cho thấy rằng mọi nhiệm vụ đều có tham số pha trộn β thuận lợi riêng. Những xu hướng này có thể được kết nối với các đặc điểm attention phụ thuộc vào nhiệm vụ trong Phần 4.1.

[Các bảng và phần còn lại của phụ lục tiếp tục với chi tiết kỹ thuật về thiết lập thí nghiệm, tập dữ liệu và cài đặt huấn luyện]

--- TRANG 13 ---
[Phần cuối của tài liệu với các bảng hiệu suất chi tiết và cài đặt thí nghiệm]
