# BitNet: Mở Rộng Quy Mô Transformer 1-bit cho Mô Hình Ngôn Ngữ Lớn

Hongyu Wang*†‡Shuming Ma*†Li Dong†Shaohan Huang†
Huaijie Wang§Lingxiao Ma†Fan Yang†Ruiping Wang‡Yi Wu§Furu Wei†⋄
†Microsoft Research‡University of Chinese Academy of Sciences§Tsinghua University
https://aka.ms/GeneralAI

## Tóm Tắt

Kích thước ngày càng tăng của các mô hình ngôn ngữ lớn đã đặt ra những thách thức cho việc triển khai và gây ra mối quan ngại về tác động môi trường do tiêu thụ năng lượng cao. Trong nghiên cứu này, chúng tôi giới thiệu BitNet, một kiến trúc Transformer 1-bit có thể mở rộng và ổn định được thiết kế cho các mô hình ngôn ngữ lớn. Cụ thể, chúng tôi giới thiệu BitLinear như một thay thế trực tiếp cho lớp nn.Linear để huấn luyện trọng số 1-bit từ đầu. Kết quả thực nghiệm về mô hình hóa ngôn ngữ cho thấy BitNet đạt được hiệu suất cạnh tranh trong khi giảm đáng kể dung lượng bộ nhớ và tiêu thụ năng lượng so với các phương pháp lượng tử hóa 8-bit tiên tiến và các baseline Transformer FP16. Hơn nữa, BitNet thể hiện quy luật mở rộng tương tự như Transformer độ chính xác đầy đủ, cho thấy tiềm năng mở rộng hiệu quả đến các mô hình ngôn ngữ lớn hơn nữa trong khi duy trì lợi ích về hiệu quả và hiệu suất.

"Tôi không nghĩ có điều gì độc đáo về trí thông minh con người. Tất cả các nơ-ron trong não tạo ra nhận thức và cảm xúc đều hoạt động theo cách nhị phân.
William Henry Gates III"

## 1 Giới Thiệu

Sự tăng trưởng nhanh chóng của các mô hình ngôn ngữ lớn đã dẫn đến những cải tiến đáng kể trong nhiều tác vụ khác nhau. Tuy nhiên, việc lưu trữ các mô hình ngôn ngữ lớn rất tốn kém do chi phí suy luận và tiêu thụ năng lượng cao. Khi kích thước của các mô hình này tăng lên, băng thông bộ nhớ cần thiết để truy cập và xử lý các tham số mô hình trở thành nút thắt cổ chai chính, hạn chế hiệu suất suy luận tổng thể. Hơn nữa, khi triển khai các mô hình này trên hệ thống phân tán hoặc nền tảng đa thiết bị, chi phí giao tiếp giữa các thiết bị có thể tác động đáng kể đến độ trễ suy luận và tiêu thụ năng lượng. Lượng tử hóa mô hình đã nổi lên như một giải pháp đầy hứa hẹn, vì nó có thể giảm đáng kể dung lượng bộ nhớ và chi phí tính toán của các mô hình quy mô lớn trong khi duy trì hiệu suất cạnh tranh.

Hầu hết các phương pháp lượng tử hóa hiện có cho các mô hình ngôn ngữ lớn đều là lượng tử hóa sau huấn luyện. Chúng đơn giản và dễ áp dụng vì không yêu cầu thay đổi gì trong pipeline huấn luyện hoặc huấn luyện lại mô hình. Tuy nhiên, điều này sẽ dẫn đến mất mát độ chính xác đáng kể hơn, đặc biệt khi độ chính xác giảm xuống, vì mô hình không được tối ưu hóa cho biểu diễn lượng tử hóa trong quá trình huấn luyện.

Một hướng khác của việc lượng tử hóa mạng nơ-ron sâu là huấn luyện nhận biết lượng tử hóa. So với lượng tử hóa sau huấn luyện, nó thường mang lại độ chính xác tốt hơn, vì mô hình được huấn luyện để tính đến độ chính xác giảm từ đầu. Hơn nữa, nó cho phép mô hình tiếp tục huấn luyện hoặc tinh chỉnh, điều này rất quan trọng đối với các mô hình ngôn ngữ lớn. Thách thức của huấn luyện nhận biết lượng tử hóa chủ yếu nằm ở tối ưu hóa, tức là mô hình trở nên khó hội tụ hơn khi độ chính xác giảm xuống. Bên cạnh đó, chưa biết liệu huấn luyện nhận biết lượng tử hóa có tuân theo quy luật mở rộng của các mô hình ngôn ngữ nơ-ron hay không.

Trong nghiên cứu này, chúng tôi tập trung vào nhị phân hóa (tức là 1-bit), đây là trường hợp cực đoan của lượng tử hóa, được áp dụng cho các mô hình ngôn ngữ lớn. Các nghiên cứu trước đây về mạng nơ-ron nhị phân chủ yếu xoay quanh mạng nơ-ron tích chập. Gần đây, đã có một số nghiên cứu về Transformer nhị phân. Tuy nhiên, các nghiên cứu này tập trung vào dịch máy hoặc huấn luyện trước BERT, khá khác biệt so với các mô hình ngôn ngữ lớn. Ví dụ, dịch máy sử dụng kiến trúc mã hóa-giải mã, huấn luyện trước BERT sử dụng bộ mã hóa hai chiều, và các mô hình ngôn ngữ lớn sử dụng bộ giải mã một chiều. Hơn nữa, các mô hình ngôn ngữ lớn thường được mở rộng đến kích thước mô hình lớn hơn nhiều, trong khi các mô hình BERT và dịch máy không trải qua việc mở rộng rộng rãi như vậy.

Theo hiểu biết của chúng tôi, nghiên cứu này là nghiên cứu đầu tiên điều tra huấn luyện nhận biết lượng tử hóa cho các mô hình ngôn ngữ lớn 1-bit. Chúng tôi đề xuất BitNet, một kiến trúc Transformer 1-bit cho các mô hình ngôn ngữ lớn, nhằm mở rộng hiệu quả về cả bộ nhớ và tính toán. BitNet sử dụng trọng số nhị phân độ chính xác thấp và kích hoạt lượng tử hóa, trong khi duy trì độ chính xác cao cho các trạng thái tối ưu hóa và gradient trong quá trình huấn luyện. Phương pháp của chúng tôi được thiết kế để có thể mở rộng và ổn định, với khả năng xử lý các mô hình ngôn ngữ lớn một cách hiệu quả. Việc triển khai kiến trúc BitNet khá đơn giản, chỉ yêu cầu thay thế các phép chiếu tuyến tính (tức là nn.Linear trong PyTorch) trong Transformer. Hơn nữa, nó bổ sung cho các phương pháp tăng tốc khác cho các mô hình ngôn ngữ lớn, như PagedAttention, FlashAttention, và giải mã suy đoán.

Chúng tôi đánh giá BitNet trên một loạt các benchmark mô hình hóa ngôn ngữ, so sánh với các phương pháp lượng tử hóa tiên tiến và Transformer FP16. Kết quả thực nghiệm cho thấy BitNet đạt được hiệu suất cạnh tranh về cả perplexity và độ chính xác tác vụ downstream. Quan trọng hơn, BitNet giảm đáng kể dung lượng bộ nhớ và tiêu thụ năng lượng so với các baseline. Hơn nữa, chúng tôi cho thấy BitNet tuân theo quy luật mở rộng tương tự như Transformer độ chính xác đầy đủ, cho thấy nó có thể được mở rộng hiệu quả đến các mô hình ngôn ngữ lớn hơn nữa với những lợi ích tiềm năng về hiệu suất và hiệu quả.

## 2 BitNet

Như thể hiện trong Hình 2, BitNet sử dụng cùng layout như Transformer, xếp chồng các khối self-attention và mạng feed-forward. So với Transformer thông thường, BitNet sử dụng BitLinear (Eq. 11) thay vì phép nhân ma trận thông thường, sử dụng trọng số mô hình nhị phân (tức là 1-bit). Chúng tôi để các thành phần khác ở độ chính xác cao, ví dụ 8-bit trong các thí nghiệm của chúng tôi. Chúng tôi tổng kết các lý do như sau. Thứ nhất, các kết nối dư và chuẩn hóa lớp đóng góp chi phí tính toán không đáng kể cho các mô hình ngôn ngữ lớn. Thứ hai, chi phí tính toán của biến đổi QKV nhỏ hơn nhiều so với phép chiếu tham số khi mô hình tăng lên. Thứ ba, chúng tôi bảo toàn độ chính xác cho embedding đầu vào/đầu ra vì các mô hình ngôn ngữ phải sử dụng xác suất độ chính xác cao để thực hiện lấy mẫu.

### 2.1 BitLinear

Chúng tôi đầu tiên nhị phân hóa các trọng số thành +1 hoặc -1 với hàm signum. Theo nghiên cứu trước, chúng tôi tập trung các trọng số về zero-mean trước khi nhị phân hóa để tăng dung lượng trong phạm vi số học hạn chế. Một hệ số tỷ lệ beta được sử dụng sau nhị phân hóa để giảm lỗi l2 giữa trọng số giá trị thực và trọng số nhị phân. Việc nhị phân hóa của một trọng số W trong R^(n×m) có thể được công thức hóa như sau:

W̃ = Sign(W - α), (1)

Sign(W_ij) = +1, nếu W_ij > 0,
            -1, nếu W_ij ≤ 0, (2)

α = (1/nm) ∑_ij W_ij (3)

Chúng tôi tiếp tục lượng tử hóa các kích hoạt đến độ chính xác b-bit. Theo nghiên cứu trước, chúng tôi sử dụng lượng tử hóa absmax, điều này tỷ lệ các kích hoạt vào phạm vi [-Q_b, Q_b] (Q_b = 2^(b-1)) bằng cách nhân với Q_b và chia cho giá trị tuyệt đối lớn nhất của ma trận đầu vào:

x̃ = Quant(x) = Clip(x × Q_b/γ, -Q_b + ε, Q_b - ε), (4)

Clip(x, a, b) = max(a, min(b, x)), γ = ||x||_∞, (5)

trong đó ε là một số thực dấu phẩy động nhỏ ngăn chặn overflow khi thực hiện clipping.

Đối với các kích hoạt trước các hàm phi tuyến (ví dụ ReLU), chúng tôi tỷ lệ chúng vào phạm vi [0, Q_b] bằng cách trừ đi giá trị nhỏ nhất của đầu vào để tất cả các giá trị đều không âm:

x̃ = Quant(x) = Clip((x - η) × Q_b/γ, ε, Q_b - ε), η = min_ij x_ij. (6)

Trong nghiên cứu này, chúng tôi lượng tử hóa kích hoạt đến 8-bit và để độ chính xác thấp hơn cho nghiên cứu tương lai. Hơn nữa, việc lượng tử hóa được thực hiện trên từng tensor trong quá trình huấn luyện trong khi trên từng token trong quá trình suy luận để có cả tính ổn định và hiệu quả.

Với các phương trình lượng tử hóa ở trên, phép nhân ma trận có thể được viết như sau:

y = W̃x̃ (7)

Chúng tôi giả định rằng các phần tử trong W và x độc lập lẫn nhau và chia sẻ cùng phân phối, và W và x độc lập với nhau. Khi đó phương sai của đầu ra y được ước tính như sau:

Var(y) = n Var(W̃x̃) (8)
       = n E[W̃²]E[x̃²] (9)
       = n β²E[x̃²] ≈ E[x̃²] (10)

Đối với tính toán độ chính xác đầy đủ, phương sai của đầu ra Var(y) ở quy mô 1 với các phương pháp khởi tạo tiêu chuẩn (ví dụ khởi tạo Kaiming hoặc Xavier), có lợi ích lớn cho tính ổn định huấn luyện. Để bảo toàn phương sai sau lượng tử hóa, chúng tôi giới thiệu một hàm LayerNorm trước lượng tử hóa kích hoạt. Theo cách này, phương sai của đầu ra y sau đó được ước tính là Var(y) ≈ E[LN(x̃)²] = 1, có cùng độ lớn với đối tác độ chính xác đầy đủ Var(y). Trong bối cảnh Transformer, nó có triển khai chính xác như SubLN. Với SubLN và các phương pháp lượng tử hóa ở trên, chúng tôi có BitLinear, được công thức hóa như sau:

y = W̃x̃ = W̃ Quant(LN(x)) × (βγ/Q_b) (11)

LN(x) = (x - E(x))/√(Var(x) + ε), β = (1/nm)||W||₁ (12)

Hình 2 cung cấp minh họa về luồng tính toán của BitLinear. Sau thao tác SubLN, các kích hoạt được lượng tử hóa với hàm absmax. Phép nhân ma trận được thực hiện giữa trọng số 1-bit và các kích hoạt lượng tử hóa. Các kích hoạt đầu ra được tái tỷ lệ với {β, γ} để khử lượng tử hóa chúng về độ chính xác ban đầu.

**Model parallelism với Group Quantization và Normalization** Một kỹ thuật thiết yếu để mở rộng các mô hình ngôn ngữ lớn là song song mô hình, phân chia phép nhân ma trận trên nhiều thiết bị. Điều kiện tiên quyết cho các phương pháp song song mô hình hiện có là các tensor độc lập dọc theo chiều phân vùng. Tuy nhiên, tất cả các tham số α, β, γ, và η đều được tính toán từ toàn bộ tensor, phá vỡ điều kiện tiên quyết độc lập. Một giải pháp là giới thiệu một thao tác all-reduce cho mỗi tham số. Tuy nhiên, mặc dù việc giao tiếp cho mỗi tham số nhỏ, lượng đồng bộ hóa tăng lên khi mô hình trở nên sâu hơn, làm chậm đáng kể quá trình forward pass. Vấn đề này cũng tồn tại trong SubLN, nơi mean và variance nên được ước tính qua chiều phân vùng.

Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp đơn giản làm cho song song mô hình hiệu quả hơn. Chúng tôi chia các trọng số và kích hoạt thành các nhóm và sau đó ước tính độc lập các tham số của mỗi nhóm. Theo cách này, các tham số có thể được tính toán cục bộ mà không yêu cầu giao tiếp bổ sung. Phương pháp này, được gọi là Group Quantization, được công thức hóa như sau:

Đối với ma trận trọng số W trong R^(n×m), chúng tôi chia nó thành G nhóm dọc theo chiều phân vùng, và mỗi nhóm có kích thước n/G × m. Sau đó chúng tôi ước tính các tham số cho mỗi nhóm độc lập:

α_g = (G/nm) ∑_ij W_ij^(g), β_g = (G/nm)||W^(g)||₁, (13)

trong đó W^(g) biểu thị nhóm thứ g của ma trận trọng số. Tương tự, đối với các kích hoạt, chúng tôi có thể chia ma trận đầu vào x trong R^(n×m) thành G nhóm và tính toán các tham số cho mỗi nhóm:

γ_g = ||x^(g)||_∞, η_g = min_ij x_ij^(g) (14)

Đối với LN, chúng tôi có thể áp dụng kỹ thuật group normalization để tính toán mean và variance cho mỗi nhóm độc lập:

LN(x^(g)) = (x^(g) - E(x^(g)))/√(Var(x^(g)) + ε) (15)

Theo cách này, chúng tôi có thể triển khai hiệu quả song song mô hình với Group Quantization và Normalization, không yêu cầu giao tiếp bổ sung và có thể mở rộng đến các mô hình ngôn ngữ lớn.

### 2.2 Huấn Luyện Mô Hình

**Straight-through estimator.** Để huấn luyện mô hình 1-bit của chúng tôi, chúng tôi sử dụng straight-through estimator (STE) để xấp xỉ gradient trong quá trình backpropagation. Phương pháp này bỏ qua các hàm không khả vi, như hàm Sign (Eq. 2) và Clip (Eq. 5), trong quá trình backward pass. STE cho phép gradient chảy qua mạng mà không bị ảnh hưởng bởi các hàm không khả vi này, làm cho việc huấn luyện mô hình lượng tử hóa trở nên khả thi.

**Mixed precision training.** Trong khi các trọng số và kích hoạt được lượng tử hóa đến độ chính xác thấp, các gradient và trạng thái tối ưu hóa được lưu trữ ở độ chính xác cao để đảm bảo tính ổn định và độ chính xác của huấn luyện. Theo nghiên cứu trước, chúng tôi duy trì một trọng số tiềm ẩn ở định dạng độ chính xác cao cho các tham số có thể học để tích lũy các cập nhật tham số. Các trọng số tiềm ẩn được nhị phân hóa ngay lập tức trong quá trình forward pass và không bao giờ được sử dụng cho quá trình suy luận.

**Large learning rate.** Một thách thức cho tối ưu hóa là một cập nhật nhỏ trên các trọng số tiềm ẩn thường không tạo ra sự khác biệt nào trong các trọng số 1-bit. Điều này dẫn đến gradient và cập nhật thiên lệch được ước tính dựa trên các trọng số 1-bit. Vấn đề này thậm chí còn tệ hơn ở đầu quá trình huấn luyện, nơi các mô hình được cho là hội tụ nhanh nhất có thể. Để giải quyết thách thức này, chúng tôi khám phá các phương pháp khác nhau, kết luận rằng tăng learning rate là cách đơn giản và tốt nhất để tăng tốc tối ưu hóa. Các thí nghiệm của chúng tôi cho thấy BitNet có lợi từ learning rate lớn về mặt hội tụ, trong khi FP16 Transformer phân kỳ ở đầu quá trình huấn luyện với cùng learning rate. Chi tiết hơn có thể được tìm thấy trong Phần 3.

### 2.3 Hiệu Quả Tính Toán

Chúng tôi ước tính hiệu quả tính toán của BitNet về cả năng lượng thao tác số học và dung lượng bộ nhớ. Chúng tôi chủ yếu tập trung vào tính toán cho phép nhân ma trận, vì nó đóng góp nhiều nhất vào chi phí của các mô hình ngôn ngữ lớn.

**Năng lượng thao tác số học.** Theo mô hình năng lượng trong các nghiên cứu trước, tiêu thụ năng lượng cho các thao tác số học khác nhau có thể được ước tính như sau:

[Bảng 2: Tiêu thụ năng lượng ADD và MUL cho các biểu diễn bit khác nhau tại các nút quy trình 45nm và 7nm]

Trong Transformer thông thường, đối với phép nhân ma trận với các chiều m×n và n×p, tiêu thụ năng lượng có thể được tính như sau:

E_add = m × (n-1) × p × Ê_add (16)
E_mul = m × n × p × Ê_mul (17)

Đối với BitNet, tiêu thụ năng lượng của phép nhân ma trận được chi phối bởi các thao tác cộng, vì các trọng số là 1-bit. Các thao tác nhân chỉ được áp dụng để tỷ lệ đầu ra với các scalar β và γ/Q_b, vì vậy tiêu thụ năng lượng cho phép nhân có thể được tính như sau:

E_mul = (m×p + m×n) × Ê_mul (18)

nhỏ hơn đáng kể so với trong Transformer. Tiết kiệm năng lượng của W1A8 BitNet so với Transformer độ chính xác đầy đủ (32-32) và độ chính xác nửa (16-16) được thể hiện trong Bảng 1. Như có thể thấy, BitNet cung cấp tiết kiệm năng lượng đáng kể, đặc biệt cho các thao tác nhân, là thành phần chính của tiêu thụ năng lượng phép nhân ma trận.

## 3 So Sánh với FP16 Transformer

### 3.1 Thiết Lập

Chúng tôi huấn luyện một loạt các mô hình ngôn ngữ tự hồi quy với BitNet ở nhiều quy mô khác nhau, từ 125M đến 30B. Các mô hình được huấn luyện trên một corpus tiếng Anh, bao gồm dataset Pile, các snapshot Common Crawl, RealNews, và CC-Stories. Chúng tôi sử dụng tokenizer Sentencpiece để tiền xử lý dữ liệu và kích thước từ vựng là 16K. Bên cạnh BitNet, chúng tôi cũng huấn luyện các baseline Transformer với cùng dataset và cài đặt để so sánh công bằng. Chi tiết hơn có thể được tìm thấy trong phụ lục.

### 3.2 Quy Luật Mở Rộng Tối Ưu Suy Luận

Các mô hình ngôn ngữ nơ-ron đã được chứng minh là mở rộng có thể dự đoán được với kiến trúc Transformer thông thường. Loss mở rộng theo quy luật lũy thừa với lượng tính toán được sử dụng cho huấn luyện. Điều này cho phép chúng ta xác định việc phân bổ tối ưu của ngân sách tính toán cũng như dự đoán hiệu suất của các mô hình ngôn ngữ lớn từ các mô hình nhỏ hơn.

Để nghiên cứu quy luật mở rộng của Transformer nhị phân, chúng tôi bắt đầu bằng cách vẽ đường cong mở rộng của cả BitNet và baseline FP16 Transformer theo số lượng tham số. Chúng tôi cố định số token huấn luyện và thay đổi kích thước mô hình. Hình 3 cho thấy việc mở rộng loss của BitNet tương tự như FP16 Transformer, tuân theo quy luật lũy thừa. Sau đó chúng tôi fit quy luật mở rộng với một term loss không thể giảm:

L(N) = aN^b + c (19)

Để đánh giá liệu quy luật mở rộng có thể dự đoán chính xác loss hay không, chúng tôi chọn các mô hình từ 125M đến 6.7B để fit các tham số trong quy luật lũy thừa và sử dụng quy luật để dự đoán loss của 13B và 30B. Kết quả cho thấy quy luật mở rộng đã fit dự đoán loss của BitNet với độ chính xác cao. Bên cạnh đó, khoảng cách giữa BitNet và FP16 Transformer trở nên nhỏ hơn khi kích thước mô hình tăng lên.

Trong khi quy luật lũy thừa ở trên đo lường xu hướng mở rộng của BitNet, nó không mô hình hóa đúng mối quan hệ giữa loss và tính toán thực tế. Nghiên cứu trước ước tính tính toán bằng cách tính toán FLOPs. Tuy nhiên, nó không áp dụng cho các mô hình 1-bit có chi phí được chi phối bởi tính toán số nguyên. Hơn nữa, nó chủ yếu đo lường tính toán huấn luyện hơn là suy luận. Để hiểu rõ hơn về hiệu quả mở rộng của các mô hình ngôn ngữ nơ-ron, chúng tôi giới thiệu Quy Luật Mở Rộng Tối Ưu Suy Luận. Nó dự đoán loss theo tiêu thụ năng lượng. Chúng tôi tập trung vào chi phí năng lượng suy luận vì nó mở rộng theo việc sử dụng mô hình, trong khi chi phí huấn luyện chỉ diễn ra một lần. Chúng tôi ước tính tiêu thụ năng lượng như trong Phần 2.3. Hình 3 cho thấy đường cong mở rộng theo chi phí năng lượng suy luận tại các nút quy trình 7nm. Nó chứng minh rằng BitNet có hiệu quả mở rộng cao hơn nhiều. Với ngân sách tính toán cố định, BitNet đạt được loss tốt hơn đáng kể. Đồng thời, chi phí suy luận nhỏ hơn nhiều để đạt được cùng hiệu suất như các mô hình FP16.

### 3.3 Kết Quả trên Các Tác Vụ Downstream

Ngoài loss, chúng tôi cũng quan tâm đến khả năng theo việc mở rộng của BitNet. So với loss, khả năng khó dự đoán hơn do tính chất xuất hiện của các mô hình ngôn ngữ nơ-ron. Để đánh giá khả năng với các metric có thể diễn giải, chúng tôi kiểm tra cả kết quả 0-shot và 4-shot trên bốn tác vụ downstream, bao gồm Hellaswag, Winogrande, Winograd, và Storycloze. Hình 4 báo cáo kết quả trung bình của BitNet và FP16 Transformer với nhiều quy mô khác nhau. Tương tự như đường cong mở rộng loss, hiệu suất trên các tác vụ downstream có thể mở rộng khi ngân sách tính toán tăng lên. Bên cạnh đó, hiệu quả mở rộng của khả năng cao hơn nhiều so với baseline FP16 Transformer, về cả hiệu suất zero-shot và few-shot.

### 3.4 Kiểm Tra Tính Ổn Định

Thách thức chính cho việc huấn luyện Transformer bit thấp là tính ổn định trong tối ưu hóa. Do đó, chúng tôi thực hiện kiểm tra tính ổn định cho cả BitNet và baseline FP16 bằng cách huấn luyện một loạt mô hình với các learning rate đỉnh khác nhau. Hình 5a minh họa kết quả của kiểm tra tính ổn định. Nó cho thấy BitNet có thể hội tụ với learning rate lớn trong khi FP16 Transformer không thể, chứng minh tính ổn định huấn luyện tốt hơn của BitNet. Lợi thế này trong tối ưu hóa cho phép huấn luyện với learning rate lớn hơn. Hình 5b cho thấy BitNet có thể hưởng lợi từ việc tăng learning rate, đạt được hội tụ tốt hơn về PPL.

## 4 So Sánh với Lượng Tử Hóa Sau Huấn Luyện

### 4.1 Thiết Lập

Chúng tôi huấn luyện BitNet với cùng thiết lập như mô tả trong Phần 3.1. Chúng tôi so sánh BitNet với các phương pháp lượng tử hóa tiên tiến, bao gồm Absmax, SmoothQuant, GPTQ, và QuIP. Các phương pháp này là lượng tử hóa sau huấn luyện trên một mô hình FP16 Transformer, tuân theo cùng cài đặt huấn luyện và dữ liệu như BitNet. Trong số đó, Absmax và SmoothQuant lượng tử hóa cả trọng số và kích hoạt, trong khi GPTQ và QuIP chỉ giảm độ chính xác của trọng số. Chúng tôi áp dụng các phương pháp cho nhiều mức lượng tử hóa khác nhau. Đối với lượng tử hóa chỉ trọng số (tức là GPTQ và QuIP), chúng tôi thí nghiệm với W4A16 và W2A16. Đối với lượng tử hóa trọng số và kích hoạt (tức là Absmax và SmoothQuant), chúng tôi sử dụng chúng để lượng tử hóa FP16 Transformer thành W8A8, W4A4, và W1A8. Triển khai BitNet của chúng tôi là trọng số nhị phân kích hoạt 8-bit (W1A8), có bit thấp hơn hoặc bằng các baseline.

### 4.2 Kết Quả

Bảng 3 trình bày phân tích so sánh chi tiết về hiệu suất zero-shot của phương pháp đề xuất, BitNet, so với các phương pháp baseline khác nhau trên bốn dataset benchmark, cụ thể là Winogrande, Winograd, Storycloze, và Hellaswag. Tất cả các mô hình có kích thước 6.7B để so sánh công bằng.

Các phương pháp được đánh giá qua nhiều mức bit trọng số, từ 16 xuống 1. Bên cạnh độ chính xác zero-shot trên các tác vụ downstream, các metric đánh giá bao gồm perplexity mô hình ngôn ngữ trên tập validation, cung cấp hiểu biết toàn diện về hiệu suất của mỗi phương pháp.

Kết quả chứng minh hiệu quả của BitNet trong việc đạt được mức hiệu suất cạnh tranh so với các phương pháp baseline, đặc biệt ở các mức bit thấp. Điểm zero-shot của BitNet có thể so sánh với các mô hình 8-bit, trong khi chi phí suy luận thấp hơn nhiều. Đối với các mô hình 4-bit, các phương pháp lượng tử hóa chỉ trọng số vượt trội hơn các bộ lượng tử hóa trọng số và kích hoạt, chủ yếu vì kích hoạt khó lượng tử hóa hơn. BitNet, là một mô hình 1-bit, đạt được kết quả tốt hơn đáng kể so với cả phương pháp lượng tử hóa trọng số và kích hoạt và phương pháp chỉ trọng số. Đối với các mô hình bit thấp hơn, BitNet có điểm số vượt trội liên tục so với tất cả baseline. Điều này chứng minh lợi thế của các phương pháp huấn luyện nhận biết lượng tử hóa so với các phương pháp lượng tử hóa sau huấn luyện. Hình 6 tổng kết cả độ chính xác zero-shot và few-shot của phương pháp chúng tôi và các baseline trong khi mở rộng kích thước mô hình từ 1.3B đến 6.7B. Nó chứng minh rằng lợi thế này nhất quán qua các quy mô khác nhau.

## 5 Nghiên Cứu Ablation

Trong Bảng 4, chúng tôi trình bày một nghiên cứu ablation so với một số phương pháp thay thế. Chúng tôi ablate hiệu ứng của các lựa chọn trong phương pháp lượng tử hóa kích hoạt cũng như các kỹ thuật để ổn định việc huấn luyện mô hình. BitNet triển khai absmax để lượng tử hóa kích hoạt và sử dụng SubLN cho tính ổn định huấn luyện. Một thay thế lượng tử hóa là hàm elastic, động điều chỉnh các tỷ lệ với các tham số có thể học. Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng absmax có hiệu suất tốt hơn hàm elastic. Bên cạnh đó, hàm absmax dẫn đến huấn luyện ổn định hơn, cho phép learning rate lớn hơn cho BitNet. Chúng tôi tiếp tục so sánh SubLN với kiến trúc Pre-LN và BMT. Pre-LN là kiến trúc mặc định cho pretraining GPT, trong khi BMT đã được chứng minh cải thiện tính ổn định của các mô hình nhị phân. Các thí nghiệm của chúng tôi cho thấy SubLN vượt trội hơn cả Pre-LN và BMT. Do đó, chúng tôi chọn absmax và SubLN làm triển khai trong BitNet.

## 6 Kết Luận và Nghiên Cứu Tương Lai

Chúng tôi trình bày BitNet, một kiến trúc Transformer 1-bit mới cho các mô hình ngôn ngữ lớn. Phương pháp của chúng tôi được thiết kế để có thể mở rộng và ổn định, với khả năng xử lý các mô hình ngôn ngữ lớn một cách hiệu quả. Kết quả thực nghiệm cho thấy BitNet đạt được hiệu suất cạnh tranh về cả perplexity và hiệu suất tác vụ downstream, trong khi giảm đáng kể dung lượng bộ nhớ và tiêu thụ năng lượng so với các baseline. Hơn nữa, BitNet tuân theo quy luật mở rộng tương tự như Transformer độ chính xác đầy đủ, cho thấy nó có thể được mở rộng hiệu quả đến các mô hình ngôn ngữ lớn hơn nữa với những lợi ích tiềm năng về hiệu suất và hiệu quả. Trong tương lai, chúng tôi muốn mở rộng BitNet về kích thước mô hình và bước huấn luyện. Chúng tôi cũng quan tâm đến việc áp dụng BitNet trong các kiến trúc khác (ví dụ RetNet) để huấn luyện các mô hình ngôn ngữ lớn.
