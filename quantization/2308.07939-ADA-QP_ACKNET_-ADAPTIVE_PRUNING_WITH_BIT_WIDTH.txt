# 2308.07939.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2308.07939.pdf
# File size: 500682 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ADA-QP ACKNET -ADAPTIVE PRUNING WITH BIT WIDTH
REDUCTION AS AN EFFICIENT CONTINUAL LEARNING METHOD
WITHOUT FORGETTING∗
Marcin Pietron
AGH-UST
Kraków
pietron@agh.edu.plDominik Zurek
AGH-UST
Kraków
dzurek@agh.edu.plKamil Faber
AGH-UST
Kraków
kfaber@agh.edu.plRoberto Corizzo
American University
Washington
corizzo@american.edu
ABSTRACT
Continual Learning (CL) is a process in which there is still huge gap between human and deep
learning model efficiency. Recently, many CL algorithms were designed. Most of them have many
problems with learning in dynamic and complex environments. In this work new architecture based
approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network
for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented
method the size of the model is reduced by efficient linear and nonlinear quantisation approach.
The method reduces the bit-width of the weights format. The presented results shows that low bit
quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To
our knowledge it is the first CL strategy which incorporates both compression techniques pruning and
quantisation for generating task sub-networks. The presented algorithm was tested on well-known
episode combinations and compared with most popular algorithms. Results show that proposed
approach outperforms most of the CL strategies in task and class incremental scenarios.
Keywords Continual learning ·Quantisation ·Pruning ·Catastrophic Forgetting
1 Introduction
Continual learning (CL) is an emerging machine learning paradigm that aims at designing new methods and strategies
to provide accurate analysis in complex and dynamic real-world environments [ 6]. As models are challenged with
multiple tasks over their lifetime, a desired property for CL strategies is to maintain a high performance across all tasks.
This paradigm is receiving increasing attention from research communities, which led to a number of works being
proposed [ 55,54,2,6,18,3]. There are three main types of CL strategies: rehearsal (also known as experience replay),
regularization, and architectural. Architectural strategies are quite powerful in that they allow to adapt and evolve the
model to accommodate new tasks, and recent research shows that such approaches are among the most efficient and
effective in CL scenarios [ 21,32,30,18]. Forget-free methods [ 21,32] accommodate new tasks by assigning a subset
of the number of available weights to each task. Their effectiveness strongly depends on their ability to effectively
leverage model capacity to accommodate as many tasks as possible.
One important pitfall of existing pruning-based architectural forget-free methods is that they are limited in their ability
to efficiently exploit model sparsity since each layer is pruned with the same constant sparsity level. As a result, they
do not properly tune the number of weights to be removed while preserving classification accuracy [ 64,51]. Another
general limitation shared by forget-free methods is that they are prone to a quick saturation of the available model
capacity, in that they cannot assign more than one value to each weight and are therefore limited by the available number
of weights.
∗Paper accepted at ECAI 2023arXiv:2308.07939v2  [cs.LG]  1 Oct 2023

--- PAGE 2 ---
Ada-QPacknet
In this paper, we propose Ada-QPacknet, a novel forget-free method for continual image classification that solves
these limitations by incorporating adaptive pruning with sparsity-level identification and adaptive non-linear weights
quantization.
Specifically, to effectively exploit model sparsity, our proposed Ada-QPacknet incorporates adaptive pruning with
different sparsity levels for each layer, where pruning ratios are chosen via fast lottery ticket search. Moreover, to
deal with the issue of quickly saturating model capacity, Ada-QPacknet performs an adaptive quantization stage that
separates each weight into multiple components, each using a subset of the available 32 bits. This separation allows us
to reuse a single weight for more than one task, leading to reduced use of models’ capacity and improvements in terms
of model efficiency. At the same time, by exclusively assigning single components of weights to tasks, we overcome
the bias limitation that is typically existent in common forget-free methods, which assign multiple tasks to the same
weight value.
In summary, the contributions of our paper are the following:
•An adaptive pruning approach that leverages the sensitivity and weights’ importance characterizing different
layers in deep learning models. This approach allows us to reduce the available set of model’s weights assigned
to separate tasks, allowing AdaQ-Packnet to efficiently use a subset of the weights without significant losses in
accuracy.
•A quantization strategy for continual learning, which allows us to split weights into components, each of which
can be assigned to a specific task. This capability allows AdaQ-Packnet to better exploit model efficiency and
capacity, leveraging the reduced bit-width representation of the weights.
•An experimental evaluation with three benchmark CL scenarios, which highlights that our proposed method
outperforms state-of-the-art rehearsal, regularization, and architectural CL strategies, both in terms of accuracy
and exploitation of model capacity.
The paper is structured as follows. Section 2 summarizes related works in CL strategies and pruning methods. Section
3 describes our proposed Ada-QPacknet method. Section 4 describes our experimental setup, and presents the results
extracted in our experiments. Section 5 wraps up the paper with a summary of the results obtained and outlines relevant
directions for future work.
2 Related works
Continual learning methods are commonly categorized as rehearsal, regularization, and architectural [ 6]. Rehearsal
methods use memory to remember some data from past episodes. The simplest solution is to store part of the previous
training data and interleave them with new training data (Replay [ 6]). Other popular rehearsal methods include GEM
[19], A-GEM [ 25], and GDumb [ 26]. Regularization methods usually use constraints on the loss function to strengthen
connections for already learned patterns. The most popular methods are Synaptic Intelligence (SI) [ 22], which estimates
the importance of weights based on weighted regularization loss and gradient. Alternative approaches are LwF [ 28], and
EWC [ 20]. The most relevant works for this study belong to the architectural category, which represents a promising
direction in recent works [21].
The architectural methods concentrate on the topology of the neural model. One of the most known and effective
methods is CWRStar [ 31] which performs weights copy and re-initializes weights at the last layer to accommodate
new tasks. More efficient methods are those based on pruning which deals with the issue of increasing model capacity
and removes unimportant weights. Notable examples include PackNet [ 32], which divides the model into independent
sub-networks to address different tasks. Such an approach is thought to be forget-free since sub-networks are directly
mapped to tasks and are not subject to change weights. However, a major limitation is the risk of growing model
capacity. This issue has recently encouraged researchers to explore and devise pruning methods, to reduce model
capacity and storage requirements.
Pruning in neural networks can be structural and non-structural. Works in [ 39,78,77,73,74,76,80] concentrate on
structural pruning, which involves pruning entire channels in filters by means of different techniques. Non-structural
pruning can be carried out on pretrained models without retraining as in [ 64,68], which results in increased efficiency,
or with retraining, as in lottery tickets search [ 66], movement pruning [ 67], and variational dropout [ 79]. Other
efficient pruning methods include genetic-based approaches as in [ 51], which adopts constant masks and defines its own
crossover and mutation operators, and reinforcement techniques as in [ 52]. Pruning can be conducted in combination
with retraining to avoid performance drops caused by weights’ removal [ 64]. Two recent continual learning methods
adopting pruning approaches are SupSup [ 18] and Winning Subnetworks (WSN) [ 21]. SupSup finds the optimal binary
mask on a fixed backbone network for each new task. On the other hand, WSN introduced significant improvement
2

--- PAGE 3 ---
Ada-QPacknet
over Packnet, as it only considers a subset of weights from the previous task to avoid a biased forward transfer. Another
recent trend is that expandable model architectures with compression. The Dynamically Expandable Representation
(DER) method [ 7] adds a new submodule to the main model each time a new task is presented, while leveraging pruning
as an additional mode. Authors in [ 8] present the Dytox method, which is based on Transformer attention blocks. The
continual learning model consists of an encoder part - common for all tasks and a decoder, which is task-specific. On
the other hand, FOSTER [ 9] proposes a boosting approach that expands the network by means of a new residual feature
extractor and a linear classifier layer. Both Dytox [ 8] and FOSTER [ 9] incorporate a replay strategy to improve the
model’s efficiency.
Quantization is one of the most efficient techniques for the compression of deep learning models [ 68,58]. Common
strategies include the quantization of all coefficients in a single layer with a specified number of bits to represent
the integer and fractional parts [ 41,42] based on the range of values of the coefficients set. Another strategy is to
represent coefficients and data by integer numbers with an appropriate scaling factor. Many quantization approaches
in the literature adopt linear [ 68,58,42] or non-linear approaches including clustering [ 57]. Quantization can be
performed during model training [ 58] or can be run on a pre-trained model [ 68,57]. Recently, several methods for
low-bit representation have been designed [43, 44, 45, 46, 47]. Many of them can not be run without some significant
degradation in accuracy. One known advantage of quantization is the fact that it facilitates the adoption of deep neural
networks in specialized hardware accelerators with limited arithmetic bit-width and memory space [ 51]. However, to
the best of our knowledge, our paper is the first attempt to apply quantization to CL models.
3 Ada-QPacknet algorithm
In this section, we describe our proposed Ada-QPacknet – a forget-free method that leverages adaptive pruning and
adaptive quantization to eliminate catastrophic forgetting, effectively exploit model sparsity, and efficiently use model
capacity.
Figure 1 presents the overview of the Ada-QPacknet algorithm, illustrating the iterative process of adaptive pruning and
quantization on a typical training scenario with multiple tasks.
Ada-QPacknet adapts the adaptive pruning process [ 51,64] to reduce the size of task sub-networks, since the most
significant drawback of state-of-the-art forget-free methods such as PackNet [ 32] and WSN [ 21], is the constant sparsity
level applied to all layers in consecutive tasks. This significantly restricts the adaptability of the method and poses
a risk of inefficient use of model capacity. Another crucial feature of Ada-QPacknet is adaptive quantization, which
allows us to assign more than one task to each weight, by dividing its capacity into components. The adaptive nature of
the quantization process allows Ada-QPacknet to identify the optimal bit-width for each task, based on the trade-off
between the number of bits assigned to each task and the model performance. In the following subsections, we describe
adaptive pruning and adaptive quantization in more detail.
3.1 Adaptive Pruning with Sparsity Level Identification
In this stage, the goal is to perform pruning while identifying separate sparsity levels for each layer. Sparsity levels
should be as high as possible to reduce memory occupation leaving more capacity for future tasks. The representation
of the pruned model Fp
Θis the following tuple:
Fp
Θ= (FΘ, M), (1)
where FΘis defined as:
FΘ(X) =fθL(fθL−1...(fθ0(X))). (2)
FΘis the original model with a set of convolutional and fully-connected layers fθi, which are defined in the specified
order. The Θtensor is defined as:
Θ ={θ0, θ1, ..., θ L}, (3)
and contains weights of convolutional and fully-connected layers. θiis a tensor of weights between layers iandi+ 1.
A single entry wi,k,lrepresents a weight between k-th neuron in layer iandl-th neuron in layer i+ 12.
Mis a set of masks:
M={M0, M1, ..., ML}. (4)
2To simplify our notation, we assume that a single weight value is assigned to all tasks. In our method, multiple values for a
single weight can be assigned to solve multiple tasks.
3

--- PAGE 4 ---
Ada-QPacknet
Ada-QPacknetInitial
Model
Training on T ask 1...
Next TasksQuantized model
Pruned 
ModelPruned
Model
Adaptive
Quantization
Adaptive
Quantization
Quantized
Model
Adaptive
Pruning
Adaptive
Pruning
Training on T ask 2
...
Final
Model
Figure 1: Overview of the proposed Ada-QPacknet method. Initially, the model has a full bit-width representation
represented by white rectangles. The method follows an iterative process that includes adaptive pruning during training
and adaptive quantization. Adaptive pruning identifies different sparsity levels for each layer. Adaptive quantization
identifies the optimal bit-width for each task (identified by different colors, i.e. red, blue, purple, and yellow) and
quantizes weights, improving the exploitation of model capacity. The final model has four weights that support four
different tasks, illustrating an efficient use of model sparsity and capacity.
Each mask Miis positioned in between layers iandi+ 1and has the same shape as θi. Masks are used to decide
which weights will be used to solve specific tasks. A single entry in Micontains a set of numbers indicating in which
tasks the weight is used. Therefore, each weight can be assigned to one or more tasks. A graphical representation is
shown in Figure 2.
A single entry in the mask Miis represented as a binary vector:
Mi
k,l={t1, t2, . . . , t n}, (5)
where kis the source neuron in the layer i,lis the destination neuron in layer i+ 1,ti∈{0,1} indicates if a given
weight is used by the i-th task.
The sparsity level Υiis a ratio between the number of weights that are not yet assigned to any tasks and the number of
all weights in a mask Mi:
Υi=|Mi| −P|Mi|
k,l(|Mi
k,l|>0)
|Mi|(6)
The overall weighted sparsity of the model is defined as:
Υ =LX
i|θi| ·Υi (7)
3.1.1 Generating random lottery ticket mask
Each time a new task is encountered, for each layer, a number of candidate masks PSare generated according to the
random lottery ticker algorithm, taking into account a sparsity level constraint between VminandVmax values. For
example, a sparsity level of 0.5means that half of all weights will be leveraged to solve the task. This way, we generate
a candidate mask for task tand layer l:
CMt
l=n
e|e∈Mi∧ |e|< TLo
,|CMt
l|<|Mi|, (8)
where |e|is the number of tasks already assigned to a weight e, and TLis a constant that determines the capacity, i.e.
the maximum number of tasks that can be assigned to a single weight, according to the desired quantization used. For
4

--- PAGE 5 ---
Ada-QPacknet
instance, TL= 4with a weight capacity of 32-bit and a quantization rate is 8-bit. This formulation allows us to sample
a subset of available weights from the original mask (according to the sparsity constraint) with zero or more prior
assignments to tasks.
The set of candidate masks for all layers is defined as:
CMt={CMt
0, CMt
1, . . . , CMt
L} (9)
Once all candidate masks are generated, the best candidate according to the resulting accuracy for the current layer is
selected.
Algorithm 1 Adaptive Pruning for task t
Require: PS– population size
Require: Θ– weights of the model
Require: α,β– importance of accuracy and sparsity in selection
1:Θ←initialize the available weights in Θ
2:CMt← ∅ // A set of candidate masks
3:Θt← ∅ // A set of masked weights
4:fori= 0toPSdo
5: CMt,i←random lottery ticket mask (eq: 9)
6: Θt,i=Θ⊙CMt,i{Masked weights}
7: Execute short training Θt,ion task t
8: CMt←CMt∪CMt,i
9: Θt←Θt∪Θt,i
10:end for
11:At← ∀ CMt,i∈CMtaccuracy of Θ⊙CMt,ion task t
12:Υt← ∀ CMt,i∈CMtcompute ΥiforCMt,i(eq: 7)
13:id= argmax
i∈{0,...,P S}(α·Ai
max(A)+β·Υi
max(Υ))
14:Mt←CMt,idx
15:Θ′←Θt,idx
16:Execute full training of Θ′on task t
17:return Mt,Θ′
Algorithm 1 describes the task pruning process for a task t. We start with the random initialization of available model
weights (line 1). Two empty sets are created: i)CMtused for storing candidate masks (line 2); ii)Θtused for storing
masked weights (line 3). After that, we generate PS(population size) candidate masks (lines 4-10). For each generated
candidate mask (line 5), we train masked weights (lines 6-7) with a very limited number of epochs. Then, we add
the generated candidate mask and trained weights to the respective sets (lines 8-9). In the next step, we compute the
accuracy Atfor all generated candidates (line 11) and weighted sparsity Υt(line 12). We identify the best model
according to the equation in line 13, where the evaluation simultaneously considers accuracy and sparsity: between two
candidates with similar accuracy but dramatically different values of sparsity, the one with the highest level of sparsity
is preferred, as we want to preserve as many weights as possible for future tasks. After this step, we perform the full
training stage of the masked weights Θ′on the current task t.
3.2 Adaptive Quantization
Quantization is a key stage in the Ada-QPacknet algorithm. Its purpose is twofold: i)it allows us to assign a single
weight to multiple tasks; ii)it helps reducing the use of capacity of the model with a specified number of tasks. The
standard 32-bit single-weight representation is divided into multiple components with a reduced number of bits, and
each component can be assigned to a different task, as shown in Figure 2. For example, one 32-bit single-weight
representation may be divided into four 8-bit components that are assigned to any of the tasks. However, the adaptive
nature of this process is not dependent on equal-width components, but allows us to assign a varying number of
bits for each component, evaluating the trade-off between bit-width and model performance. For example, a 32-bit
single-weight representation may also be divided into 3 components with the following bit-widths: 8, 10, 14, etc.
Algorithm 2 presents the details of the adaptive quantization process. It starts with non-linear quantization of the task-
pruned model with minimal values of parameters ωandψ(bit-width) (line 1). We leverage the non-linear quantization
algorithm detailed in Algorithm 3. Then, we check the model accuracy (line 2). Next, if the drop in accuracy is more
5

--- PAGE 6 ---
Ada-QPacknet
N1,1
N2,1
N1,3N1,2
N2,2
w1,3,2w0,1,1
w0,1,3
w0,2,2
w0,2,3
0Task 1
N0,1
N0,2w0,1,2
w1,2,2w1,2,1w1,1,2w1,1,1
w0,2,1
1         0.3456
2         0.0145
3        -0.35760         0.4576
1         0.1417
2        -0.1 145
3        -0.29670         0.355511011
110110
00
Layer 2 codebook - task 1Layer 1 codebook - task 10011
w1,3,1
N1,1
N2,1
N1,3N1,2
N2,2
w1,3,2w0,1,1
w0,1,3
w0,2,2
w0,2,3
0Task 1
N0,1
N0,2w0,1,2
w1,2,2w1,2,1w1,1,2w1,1,1
w0,2,1
w1,3,1
1         0.3456
2         0.0145
3        -0.35760         0.4576
1         0.1417
2        -0.1 145
3        -0.29670         0.355511011
110110
00
Layer 2 codebook - task 1Layer 1 codebook - task 100111
11
0
00
10Task 2
1         -0.18980         0.2453Layer 1 codebook - task 2
1         0.10230         0.4521Layer 2 codebook - task 2
Figure 2: The figure presents the status of the model after it is trained, pruned, and quantized for Task 1 (left) and
Task 2 (right). During the training stage for Task 1, Adaptive Pruning releases a subset of model weights (dashed
lines), which could be used later on to accommodate new tasks. After that, Adaptive Quantization identifies the optimal
bit-width and quantizes the representation of the weights. In this example, weight representation of 32-bits is quantized
into 2 bits (red squares). Values represented on this bits are codes from a layer-wise codebook and can be translated
into full 32-bits weights. This capability allows Ada-QPacknet to divide each weight into multiple components, each of
which can be assigned to any of the tasks. The same process is repeated when Task 2 is presented: Adaptive Pruning
quantizes the 32-bits weight representation into a single bit (blue squares).
Algorithm 2 Adaptive Quantization for task t.
Require: ψ– Initial bit-width
Require: Qq– Model accuracy
Require: Θ– Pruned model
Require: δ– Maximum loss in accuracy
1:Θq, K= nonlinear_quantization( ψ,Θ) {see Algorithm 3}
2:Qq= acc( F,Θq)
3:while Qq< Q -δdo
4: ψ=ψ+ 1
5: Θq, K= nonlinear_quantization( ψ,Θ)
6: Qq= acc( F,Θq)
7:end while
8:return ψ– optimal bit-width for the task, Θq– quantized model, K– codebook
than the given threshold δ(line 3) the iterative process of increasing the bit-width is started (lines 3-7) until the drop in
performance is deemed to be acceptable.
The non-linear quantization approach devised in this work (see Algorithm 3) is inspired by [ 57]. Specifically, we start
with the identification of the number of centroids ωand the creation of an empty codebook Kwhich will store, for each
layer, a mapping between code and weight values (lines 1-2). We also create a copy of the model Θ, where weights will
contain codes from the codebook (centroid indices) instead of real values. Subsequently, for each layer i, we cluster
weights values from layer θileveraging the K-Means clustering algorithm with a specified number of clusters ω(line 5).
This step returns two outputs: a list of centroids Cand the list of weight-to-centroid-index assignment A. We proceed
by assigning the corresponding centroid index to each weight (lines 6-8). To keep the mapping between centroid indices
and centroids, we build a codebook for each layer (lines 9-11). The algorithm returns the copy of the model with
quantized weights obtained as codes from the codebook, as well as the codebook required to decipher codes to the real
value of the weight. We note that the number of centroids is directly related to the desired bit-width (as shown in line 1).
A low number of centroids implies that the memory needed for storing the weights is decreased. Each weight is stored
as an index of the closest centroid. A visual representation of the codebook is shown in Figure 2.
4 Results
4.1 Experimental setup
The experiments were run for three CL scenarios: Permuted MNIST (p-MNIST) [ 12], split CIFAR100 (s-CIFAR100)
[13], and 5 datasets [ 10], a task-incremental scenario consisting of MNIST, SVHN, FashionMNIST, CIFAR10, not-
6

--- PAGE 7 ---
Ada-QPacknet
Algorithm 3 Non-linear quantization
Require: ψ– desired bit-width
Require: Θ– weights of a model
1:ω←2ψ{Number of centroids}
2:K←[K0, K1, . . . , K |Θ|];Ki={}{Empty codebook}
3:Θ′←copy of Θ
4:forθiinΘdo
5: C, A←KMeans( θi,ω) {C: centroids, A: weight-to-centroid-index assignment}
6: forθi,j∈θido
7: θ′
i,j←A[θi,j]{Use centroid index instead of weight value}
8: end for
9: forck∈Cdo
10: Ki[k]←ck{Building layer-wise codebook}
11: end for
12:end for
13:return Θ′, K
MNIST, TinyImagenet [ 4], and Imagenet100 [ 5]. The p-MNIST scenario consists of 10 tasks with randomly permuted
pixels of the original MNIST (10 tasks with 10 classes each). s-CIFAR100 is divided into 10 tasks with 10 classes
each. The 5 datasets scenario is a sequence of 5 tasks with different datasets, each with 10 classes. The TinyImagenet
scenario consists of 40 tasks with 5 randomly sampled classes each. Finally, the Imagenet100 scenario consists of 10
tasks with 10 randomly sampled classes each.
In our experiments, the pruning search population PSis set to 16. Values for VMIN andVMAX parameters are set
to 0.45 and 0.85 for all layers. The αandβparameters are set to 0.9 and 0.1. The short training during mask search
is done with 5 epochs, while the full training for the selected mask uses 50 epochs. For Ada-QPacknet, the learning
rate is set resorting to a dynamic scheduler starting from 0.01 and decreasing over time with a minimum of 0.0001 for
all datasets except for TinyImagenet (starting from 0.001) and Imagenet100 (constant learning rate of 0.0001). The
adopted models are a two-layer neural network with fully connected layers (p-MNIST), reduced AlexNet (s-CIFAR100)
[27], Resnet-18 (5 datasets and Imagenet), TinyNet (TinyImagenet) in accordance with model backbones used in the
WSN paper [ 21]. For weight initialization, we adopt the Xavier initializer. Experiments are executed on a workstation
equipped with an NVIDIA A100 GPU. Our experiments involve 5 complete runs for each strategy. Therefore, the total
number of executions corresponds to the number of tasks in each scenario multiplied by 5.
As for the metrics, we use the most standard definition of lifelong Accuracy following the description in [ 34]. Moreover,
we also compute the capacity (total memory occupied by each task) inspired by the capacity computation in [ 21],
defined as follows:
CAP t=LX
i(1−Υi)· |θi| ·b+|L| ·2b·(32 + b) +LX
i(1−Υi)· |Mi| (10)
The capacity can be regarded as the sum of three components. The first one describes the number of the weights after
the pruning multiplied by the bit-width b. The second one describes the codebook size. The third one includes all
masks’ capacity.
4.2 Comparison to other methods
Table 1 presents results in terms of average accuracy. In our experimental analysis, we considered representatives from
all categories of CL strategies. Specifically, we considered Packnet and WSN (architectural) , Cumulative and Replay
(rehearsal), SI (regularization), and CWRStar (combined architectural and regularization). The Naive strategy is a
simple strategy where the model is fine-tuned as new tasks are presented, and can be considered as a lower bound. The
Cumulative strategy, on the other hand, stores data from all tasks observed so far to retrain the model, and is considered
as an upper bound.
Results highlight that the Ada-QPacknet outperforms all the considered strategies. In p-MNIST, Ada-QPacknet achieves
97.1%(0.50% above Cumulative), the WSN method ( 96.41%) is just behind the Cumulative approach ( 96.45%).
Packnet achieves 96.31%, whereas the other methods significantly underperform the architectural strategies (the
following method in the ranking is Replay with 62.22%). In s-CIFAR100, Ada-QPacknet achieves 74.1%average
7

--- PAGE 8 ---
Ada-QPacknet
Table 1: Comparative results in terms of average accuracy for CL strategies in different CL scenarios: Permuted MNIST
(p-MNIST), split-CIFAR100 (s-CIFAR100), and 5 datasets.
p-MNIST s-CIFAR100 5 datasets TinyImagenet
Naive 60.12 17.32 33.08 20.27
CWRStar 31.31 20.84 36,16 24.00
SI 57.32 19.54 29.42 20.51
Replay 62.22 19.60 55.24 23.14
CUMULATIVE 96.45 36.52 84.44 27.53
Packnet 96.31 72,57 92.59 55.46
WSN 96.41 76.38 93.41 71.96
Ada-QPacknet 97.1 74.1 94.1 71.9
Table 2: Capacity comparison for pruning-based methods
Datasets p-MNIST s-CIFAR100 5 datasets TinyImagenet
Packnet 96.38% 81.0% 82.86% 188.67%
WSN 77.73% 99.13% 86.10% 48.65%
Ada-QPacknet 81.25% 78.6% 33.7% 112.5%
accuracy, achieving second-best performance. The best-performing method is WSN, with a 2.28% increase in
performance, although this method presents a higher capacity utilization when compared to Ada-QPacknet, which
yields the best results in terms of capacity ( 78.6%). The third strategy achieving satisfactory results is Packnet ( 81.0%).
The next strategies in the ranking are Cumulative and CWRStar, with a significantly lower accuracy ( 36.52% and
20.84%). In the 5 datasets scenario, Ada-QPacknet ( 94.1%) presents a better performance than the second-ranked WSN
(93.41%) and the third-ranked Packnet ( 92.59%) with a significant improvement for capacity ( 33.7%) compared to
Packnet ( 82.86%) and WSN ( 86.10%). We attribute this result to the low between-task similarity due to the different
datasets, which makes the weight reutilization strategy adopted in Packnet and WSN less effective than Ada-QPacknet.
The Cumulative approach is quite close to these forget-free methods with a performance of 84.44%. The other strategies
achieve significantly worse results. In the TinyImagenet scenario, WSN achieves the best-performing score ( 71.96%),
slightly outperforming Ada-QPacknet ( 71.9%), which appears to be the second-best performing method. A possible
explanation is that transfer learning via weight reutilization performed by other strategies is more effective in this
scenario due to a higher task similarity when compared to 5 datasets. All the other strategies, such as Packnet ( 55.46%),
SI (20.51%), Replay ( 23.14%) are significantly worse and appear ineffective in this scenario.
The results presented in Table 2 show the comparison of the model’s capacity in three different architectural CL
strategies. On p-MNIST, Packnet and WSN use 96.38% and77.73% of the original model capacity, respectively.
At the same time, Ada-QPacknet represents the middle ground, using 81.25% of the capacity while achieving the
best-performing accuracy scores. In the case of s-CIFAR100, we also observe that forget-free strategies are the best
performing. In this case, Ada-QPacknet (accuracy 74.1%, capacity 78.6%) represents the best approach in capacity
when compared to Packnet (accuracy 72.57%, capacity 81.0%) and WSN (accuracy 76.38%, capacity 99.13%), which
suggests that the utilization of a higher capacity is a necessary condition to achieve higher accuracy on this dataset,
given its complexity. As for the 5-datasets scenario, Ada-QPacknet achieves better results than WSN and Packnet, while
using a significantly lower model capacity ( 33.7%in comparison to over 80% by both Packnet and WSN). Finally, as
for TinyImagenet, Ada-QPacknet presents second-best results in terms of capacity utilization ( 112.5%), positioning
itself in the middle between WSN ( 48.5%) and Packnet ( 188.67%), which appears particularly ineffective.
Overall, our results highlight that Ada-QPacknet achieves competitive results and compression ratios in all scenarios.
It is noteworthy that Ada-QPacknet vastly outperforms other methods on the most complex 5-datasets scenario built
upon heterogeneous datasets. One reason why Packnet and WSN manage to achieve similar results for p-MNIST
and s-CIFAR100 could be their ability to share weights between tasks, while both aforementioned scenarios have
homogeneous tasks originating from a single dataset. Therefore, weight sharing provides a huge advantage for WSN
and Packnet.
Table 3: Comparison with SOTA architectural and replay methods on Imagenet100 (10/10).
Methods Parameters Accuracy
FOSTER - 74.49
DyTox 10.73M 75.54
DER 112.27M 75.36
Ada-QPacknet 11.5M 72.26
8

--- PAGE 9 ---
Ada-QPacknet
Table 4: Capacity of each component and both combined (Ada-QPacknet), average per each task
Datasets p-MNIST s-CIFAR100 5 datasets
pruning 45.0% 50.0% 52.0%
quantization 12.5% 15.25% 12.5%
Ada-QPacknet 8.12% 7.7% 9.9%
Table 5: Accuracy of each component and both combined (Ada-QPacknet)
Datasets p-MNIST s-CIFAR100 5 datasets
pruning 97.1 75.3 94.4
quantization 98.5 75.8 95.2
Ada-QPacknet 97.1 74.1 94.1
Additional results on the Imagenet100 scenario with hybrid methods are presented in Table 3. The results highlight that
Ada-QPacknet ( 72.26%) is unable to outperform architectural methods extended with replay-based memory, such as
FOSTER ( 74.49%), DyTox ( 75.54%), and DER ( 75.36%). This phenomenon is expected due to the inherently higher
complexity of hybrid approaches deriving from the memory component. Nevertheless, Ada-QPacknet performance
appears still relatively close to hybrid methods, with a very low capacity occupation ( 11.5M), which is comparable
to DyTox ( 10.73M) and significantly better than DER (112.27M)3. We also note that these experiments were run
without performing an extensive optimization for Ada-QPacknet hyperparameters (e.g., we used a fixed learning rate
configuration) due to the extremely time-consuming requirements of the Imagenet100 scenario. For this reason, we
expect that a better set of hyperparameters can be identified for our proposed method, which would position its accuracy
closer to that of hybrid methods.
4.3 Ablation study
We conduct experiments to verify that all stages of Ada-QPacknet contribute to the final model capabilities. Our ablation
study is twofold and aims at assessing: i)model capacity (see Table 4), and ii)model accuracy (see Table 5) when a
specific component of Ada-QPacknet is enabled.
Focusing on the most crucial factor - capacity, results in Table 4 show that, on average, pruning allows to free a
significant amount of model capacity ( 45.0%for p-MNIST, 50.0%for s-CIFAR100, 52% for 5-datasets). On the
other hand, quantization frees, on average, a higher amount of capacity ( 86.58%) for all three scenarios (p-MNIST,
s-CIFAR100, and 5-datasets). This behavior is expected since quantization has the potential to identify a low number of
bits that retain most of the model accuracy, which is a finer-grained work (considering that a certain number of bits
can be freed from each weight) when compared to adaptive pruning (which can only remove weight in its entirety).
When combined, the two stages yield a remarkable capacity reduction per task ( 91.88% for p-MNIST, 92.30% for
s-CIFAR100, 90.10% for 5-datasets).
Analyzing the results from the model accuracy viewpoint allows us to understand the trade-off between the model
capacity (freed by adaptive pruning and quantization) and the corresponding model accuracy. Results in Table 5 show
that quantization yields higher accuracy than pruning. It is clear that the removal of some weights may have more impact
than reducing their bit-width in an adaptive and non-linear way. For example, for p-MNIST, the model with adaptive
pruning yields an accuracy of 97.1%, and the quantization yields an accuracy of 98.5%. The other datasets follow
a similar pattern with quantization yielding 75.8%and95.2%accuracy for s-CIFAR100 and 5-datasets respectively,
where pruning yields an accuracy of 75.3%and94.4%. As for the final Ada-QPacknet, the accuracy is equal or lower
than that of a single component: 97.1%, matching the performance of pruning, which is lower than 98.5%(quantization)
for p-MNIST, whereas it drops from 75.3%(pruning) and 75.8%(quantization) to 74.1%for s-CIFAR100, from 94.4%
(pruning) and 05.2%(quantization) to 94.1%for 5-datasets. However, a degree of reduction is expected since the model
is being simplified by adaptive pruning and quantization, and it is reassuring to see that the performance drop is not
significant.
Overall, we conclude that both adaptive pruning and quantization contribute to model capacity reduction, enabling
the model to accommodate future tasks without significantly impacting model accuracy. We note that results for our
method without capacity reduction pruning and quantization are missing, as their interpretation would be cumbersome.
Conceptually, Ada-QPacknet without pruning and quantization could be reformulated in two ways: i)behave similarly
to Naive, where the same model is fully retrained as a new task is observed, leading to forgetting; ii)being a forget-free
model, which, however, would fully saturate its capacity after the first task, invalidating its lifelong learning capabilities.
3For FOSTER, we do not report the number of parameters, since it was not available in the original publication.
9

--- PAGE 10 ---
Ada-QPacknet
In addition to our ablation study, we report some patterns that emerged during our experiments. For most of the
scenarios tasks, the performance drop observed with 4-bit quantization was less than 1%for all tasks (in comparison
to full 32-bit weights). For example, in p-MNIST, a performance drop of −0.5%,−1.9%, and−3.0%was observed
with 3, and 2 bits, respectively. In 5 datasets, tasks have shown a different degree of complexity ranging from low (3
bits were used for p-MNIST, notMNIST, and s-CIFAR100), medium (4 bits for Fashion MNIST), and high (5 bits
for SVHN). For Imagenet100, the least configuration yielding a negligible drop in accuracy was 5 bits. It should be
noted that for TinyImagenet, the overall capacity was decreased by allocating 5 separate subnetworks, each with 8 tasks.
This choice allowed us to minimize the number of bits per mask, which would otherwise be 40 bits due to the 40 tasks
included in this dataset.
From a computational complexity viewpoint, we observed that the training time in Ada-QPacknet is dominated by
lottery ticket search, whereas quantization adds a negligible cost to the execution time since it is done post-training.
Comparing our method with WSN and PackNet, we observed an overhead of about 50% in terms of overall execution
time. This result shows that the positive results obtained in terms of model accuracy and capacity for Ada-QPacknet are
achieved with a reasonable execution time that fundamentally lies in the same complexity class as its direct competitors.
5 Conclusions and future works
In this paper, we proposed Ada-QPacknet, a novel forget-free architectural method for continual learning that resorts
to adaptive pruning and adaptive quantization. The adaptive pruning approach allows the method to identify the near
optimal subset of weights that preserves the performance of the model on single tasks while minimizing the capacity
required to solve each task. Moreover, the adaptive quantization strategy adopted in our method allows the use of a
single weight for more than one task without any significant drop in performance. The presented results show that
our proposed method achieves competitive results in terms of accuracy and utilization of model capacity than many
continual learning strategies, including popular forget-free methods. In future work, we will focus on devising more
robust modeling capabilities to better deal with complex scenarios (such as Imagenet100) where Ada-QPacknet achieved
a suboptimal performance. Specifically, we will focus on estimating layer sensitivities and correlations between task
distributions in order to estimate sparsity-level boundaries for new tasks. Moreover, we will investigate the adoption of
weight sharing, to achieve higher compression ratios per task, as well as hybrid quantization. Finally, we will devise a
mechanism to select the most suitable mask among the available ones allocated to previously learned tasks once full
model capacity is reached.
Acknowledgments
This research was supported in part by the PLGrid Infrastructure.
References
[1]Joao Rafael, Ivo Correia, Alcides Fonseca, and Bruno Cabral, Dependency-Based Automatic Parallelization of Java
Applications , August 2014, DOI: 10.1007/978-3-319-14313-2_16
[2]Kamil Faber and Roberto Corizzo and Bartlomiej Sniezynski and Nathalie Japkowicz, VLAD: Task-agnostic
VAE-based lifelong anomaly detection , Neural Networks, vol. 165, pp. 248-273, 2023
[3]Zhou, Da-Wei and Wang, Qi-Wei and Qi, Zhi-Hong and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei, Deep
class-incremental learning: A survey , arXiv preprint arXiv:2302.03648, 2023
[4] Le, Ya and Yang, Xuan, Tiny imagenet visual recognition challenge
[5]Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and
Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael, Imagenet large scale visual
recognition challenge , International journal of computer vision, vol. 115, pp. 211–252, 2015, Springer
[6]Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan, Continual
lifelong learning with neural networks: A review , Neural networks, vol. 113, pp. 54-71, 2019, Elsevier
[7]Yan, S. and Xie, J. and He, X., DER: Dynamically Expandable Representation for Class Incremental Learning ,
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021
[8]Douillard, A. and Rame, A. and Couairon, G. and Cord, M., DyTox: Transformers for Continual Learning with
DYnamic TOken eXpansion , IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022
10

--- PAGE 11 ---
Ada-QPacknet
[9]Wang, F.-Y . and Zhou, D.-W. and Ye, H.-J. and Zha, D.-C., FOSTER: Feature Boosting and Compression for
Class-Incremental Learning , European Conference on Computer Vision (ECCV), 2022
[10] Ebrahimi, Sayna and Meier, Franziska and Calandra, Roberto and Darrell, Trevor and Rohrbach, Marcus,
Adversarial continual learning , European Conference on Computer Vision, pp. 386-402, 2020, Springer
[11] Kour, George and Saabne, Raid, Real-time segmentation of on-line handwritten arabic script , Frontiers in
Handwriting Recognition (ICFHR), pp. 417-422, 2014, IEEE
[12] Yann Le Cun, The MNIST database of handwritten digits , http://yann. lecun. com/exdb/mnist/
[13] Alex Krizhevsky, Learning Multiple Layers of Features from Tiny Images , 2009
[14] Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon, Estimate
and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications , arXiv preprint
arXiv:1804.09028, 2018
[15] Bengio, Yoshua and LeCun, Yann, Scaling Learning Algorithms Towards AI , Large Scale Kernel Machines, MIT
Press, 2007
[16] Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye, A Fast Learning Algorithm for Deep Belief Nets ,
Neural Computation, pp.1527-1554, vol. 18, 2006
[17] Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason, Curriculum learning , Proceed-
ings of the 26th annual international conference on machine learning, pp.41-48, 2009
[18] Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Moham-
mad and Yosinski, Jason and Farhadi, Ali, Supermasks in Superposition , arXiv, https://arxiv.org/abs/2006.14769,
2020
[19] David Lopez-Paz, Marc’Aurelio Ranzato, journal = arXiv, Gradient Episodic Memory for Continual Learning ,
arXiv, https://arxiv.org/abs/1706.08840, 2017
[20] author = Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume
and Rusu, Andrei and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka
and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia, Overcoming catastrophic
forgetting in neural networks , arXiv, https://arxiv.org/abs/1612.00796, 2016
[21] Kang, Haeyong and Mina, Rusty J. L. and Rizky, Sultan and Madjid, Hikmawan and Yoon, Jaehong and Hasegawa-
Johnson, Mark and Ju-Hwang, Sung and Yoo, Chang D., Forget-free Continual Learning with Winning Subnetworks ,
ICML, 2022
[22] Zenke, Friedemann and Poole, Ben and Ganguli, Surya, Continual Learning Through Synaptic Intelligence , arXiv,
https://arxiv.org/abs/1703.04200, 2017
[23] Sylvestre-Alvise, Rebuffi and Kolesnikov, Alexander and Lampert, Christoph H., iCaRL: Incremental classifier
and representation learning , arXiv, arXiv:1611.07725,2016
[24] Masse, Nicolas Y . and Grant, Gregory D. and Freedman David J., Alleviating catastrophic forgetting using
context-dependent gating and synaptic stabilization , Salk Institute for Biological Studies, 2018
[25] Chaudhry, Arslan and Ranzato, Marc’Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed, Efficient Lifelong
Learning with A-GEM , arXiv:1812.00420, arXiv, 2019
[26] Ameya Prabhu, Philip H. S. Torr and Puneet K. Dokania, GDumb: A Simple Approach that Questions Our
Progress in Continual Learning , Lecture Notes in Computer Science (LNIP), vol. 12347, 2020
[27] Saha, Gobinda and Garg, Isha and Roy, Kaushik, Gradient Projection Memory for Continual Learning , Interna-
tional Conference on Learning Representations, 2020
[28] Li, Zhizhong and Hoiem, Derek, Learning without forgetting , IEEE transactions on pattern analysis and machine
intelligence, vol. 40, pp. 2935-2947, 2017
[29] M. van de Ven, Gido and Siegelmann, Hava T. and Tolias, Andreas S., Brain-inspired replay for continual learning
with artificial neural networks , Nature Communications, 2020
[30] Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick,
James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia, Progressive Neural Networks , arXiv,
https://arxiv.org/abs/1606.04671, 2016
[31] Lomonaco, Vincenzo and Maltoni, Davide and Pellegrini, Lorenzo, Rehearsal-Free Continual Learning
over Small Non-I.I.D. Batches , 1st Workshop on Continual Learning in Computer Vision at CVPR2020,
https://arxiv.org/abs/1907.03799, 2019
11

--- PAGE 12 ---
Ada-QPacknet
[32] Mallya, Arun and Lazebnik, Svetlana, PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning ,
arXiv, https://arxiv.org/abs/1711.05769, 2017
[33] Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and
Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka,
Overcoming catastrophic forgetting in neural networks , Proceedings of the national academy of sciences, vol. 114,
pp. 3521-3526, 2017
[34] Díaz-Rodríguez, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide, Don’t forget, there is
more than forgetting: new metrics for Continual Learning , arXiv preprint arXiv:1810.13166, 2018
[35] Cossu, Andrea and Graffieti, Gabriele and Pellegrini, Lorenzo and Maltoni, Davide and Bacciu, Davide and Carta,
Antonio and Lomonaco, Vincenzo, Is Class-Incremental Enough for Continual Learning? , Frontiers in Artificial
Intelligence, vol.5, 2022, Frontiers Media SA
[36] Lomonaco, Vincenzo and Maltoni, Davide, CORe50: a New Dataset and Benchmark for Continuous Object
Recognition , Proceedings of the 1st Annual Conference on Robot Learning, pp. 17-26, vol. 78, 2017, Proceedings
of Machine Learning Research
[37] Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne,
Memory Aware Synapses: Learning What (not) to Forget , Computer Vision – ECCV 2018, Springer International
Publishing, pp. 144-161
[38] Matthias De Lange and Rahaf Aljundi and Marc Masana and Sarah Parisot and Xu Jia and Aleks Leonardis and
Gregory G. Slabaugh and Tinne Tuytelaars, A Continual Learning Survey: Defying Forgetting in Classification
Tasks , IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, vol. 44, pp.3366-3385
[39] Aketi, S.A. and Roy, S. and Raghunathan, A. and Roy, K., Gradual channel pruning while training using feature
relevance scores for convolutional neural networks , pp. 171924–171932, IEEE Access 8, 2020
[40] Van de Ven, Gido M and Tolias, Andreas S, Three scenarios for continual learning , arXiv preprint
arXiv:1904.07734, 2019
[41] Anwar, S. and Hwang, K. and Sung, W., Fixed point optimization of deep convolutional neural networks for object
recognition , IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1131-1135,
2015
[42] Gysel, P. and Motamedi, M. and Ghiasi, S., Hardware-oriented approximation of convolutional neural networks ,
arXiv:1604.03168, 2016
[43] Park, E. and Ahn, J. and Yoo, S., Weighted-Entropy-Based Quantization for Deep Neural Networks , IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2017 July
[44] Zhang, D. and Yang, J. and Ye, D. and Hua, G., Learned quantization for highly accurate and compact deep
neural networks , arXiv:1807.10029, 2018
[45] Jung, S. and Son, C. and Lee, S. and Son, J. and Kwak, Y . and Han, J.J. and Choi, C, Joint training of low-precision
neural network with quantization interval parameters , arXiv:1808.05779, 2018
[46] McDonnell, M. D., Training wide residual networks for deployment using a single bit for each weight , ICLR, 2018
[47] Mishra, A. and Nurvitadhi, E., WRPN: Wide Reduced-Precision Networks , ICLR, 2018
[48] Markus Nagel and Rana A. Amjad and Mart van Baalen and Christos Louizos and Tijmen Blankevoort. Up or
Down? Adaptive Rounding for Post-Training Quantization , Proceedings of ICML, 2020
[49] S. Mao and W.J. Dally, Deep compression: Compressing deep neural network with pruning, trained quantization
and huffman coding , 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016
[50] A. Ren and T. Zhang and S. Ye and J. Li and W. Xu and X. Qian and X. Lin and Y . Wang, ADMM-NN: an
algorithm-hardware co-design framework of dnns using alternating direction methods of multipliers , Proceedings of
the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating
Systems, ASPLOS 2019, pp. 925–938, April 13-17,
[51] K. Xu and D. Zhang, J. An and L. Liu and L. Liu and D. Wang, GenExp: Multi-objective pruning for deep neural
network based on genetic algorithm , Neurocomputing, April, 2021 DOI: 10.1016/j.neucom.2021.04.022
[52] S. Han and W.Dally, Bandwidth-efficient deep learning , DAC, 2018
[53] A. Renda and J. Frankle and M. Carbin, Comparing fine-tuning and rewinding in neural network pruning , ICLR,
2020
12

--- PAGE 13 ---
Ada-QPacknet
[54] Faber, Kamil and Corizzo, Roberto and Sniezynski, Bartlomiej and Japkowicz, Nathalie, Active Lifelong Anomaly
Detection with Experience Replay , IEEE 9th International Conference on Data Science and Advanced Analytics
(DSAA), pp. 1-10, 2022, IEEE
[55] Megan M. Baker and Alexander New and Mario Aguilar-Simon and Ziad Al-Halah and Sébastien M.R. Arnold
and Ese Ben-Iwhiwhu and Andrew P. Brna and Ethan Brooks and Ryan C. Brown and Zachary Daniels and Anurag
Daram and Fabien Delattre and Ryan Dellana and Eric Eaton and Haotian Fu and Kristen Grauman and Jesse
Hostetler and Shariq Iqbal and Cassandra Kent and Nicholas Ketz and Soheil Kolouri and George Konidaris and
Dhireesha Kudithipudi and Erik Learned-Miller and Seungwon Lee and Michael L. Littman and Sandeep Madireddy
and Jorge A. Mendez and Eric Q. Nguyen and Christine Piatko and Praveen K. Pilly and Aswin Raghavan and
Abrar Rahman and Santhosh Kumar Ramakrishnan and Neale Ratzlaff and Andrea Soltoggio and Peter Stone
and Indranil Sur and Zhipeng Tang and Saket Tiwari and Kyle Vedder and Felix Wang and Zifan Xu and Angel
Yanguas-Gil and Harel Yedidsion and Shangqun Yu and Gautam K. Vallabha, A domain-agnostic approach for
characterization of lifelong learning systems , Neural Networks, vol. 160, pp. 274-296, 2023
[56] D. Lin and S. Talathi and V . Annapureddy, Fixed point quantization of deep convolutional networks , vol. 48, pp.
2849-2858, ICLR, 2016
[57] Pietron, Marcin and Karwatowski, Michal and Wielgosz, Maciej and Duda, Jerzy, Fast Compression and
Optimization of Deep Learning Models for Natural Language Processing , CANDAR Workshops 2019, pp. 162-168,
2019
[58] Han, Song and Pool, Jeff and Tran, John and Dally, William, Learning both weights and connections for efficient
neural network, Advances in neural information processing systems, pp. 1135-1143, 2015
[59] Gysel, Philipp, Ristretto: Hardware-oriented approximation of convolutional neural networks , arXiv preprint
arXiv:1605.06402, 2016
[60] title=Pruning Filters while Training for Efficiently Optimizing Deep Learning Networks, author=Roy, Sourjya
and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Raghunathan, Anand, journal=arXiv preprint
arXiv:2003.02800, year=2020
[61] Zhao, Chenglong and Ni, Bingbing and Jian, Zhang and Zhao, Qiwei and Zhang, Wenjun and Tian, Qi, Variational
Convolutional Neural Network Pruning , 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), DOI: 10.1109/CVPR.2019.00289, 2019
[62] Krzysztof Wróbel and Michał Karwatowski and Maciej Wielgosz and Marcin Pietro ´n and Kazimierz Wiatr,
Compression of Convolutional Neural Network for Natural Language Processing , Computer Science, vol. 21, 2020,
doi: 10.7494/csci.2020.21.1.337
[63] Jongsoo Park. and Sheng Li and Wei Wen and Ping Tak Peter Tang and Hai Li and Yiran Chen and Pradeep Dubey,
Faster CNNs with Direct Sparse Convolutions and Guided Pruning , 1608.01409, arXiv, 2016
[64] Marcin Pietron and Maciej Wielgosz, Retrain or not retrain? – efficient pruning methods of deep CNN networks ,
2002.07051, arXiv, 2020
[65] M.Pietron and M.Karwatowski and M.Wielgosz and J.Duda, Fast Compression and Optimization of Deep Learning
Models for Natural Language Processing , CANDAR Workshops, pp. 162-168, 2019
[66] J. Frankle and G.K. Dziugaite and D.M. Roy and M. Carbin, The Lottery Ticket Hypothesis at Scale , 1903.01611,
arXiv, March 2019
[67] V . Sanh and T. Wolf and A. M. Rush, Movement Pruning: Adaptive Sparsity by Fine-Tuning , 2005.07683, arXiv,
May 2020
[68] M. Al-Hami and M. Pietron and R. Casas and M. Wielgosz, Methodologies of Compressing a Stable Performance
Convolutional Neural Networks in Image Classification , pp. 1-23, Neural Processing Letters, January 2020
[69] CIFAR dataset, https://paperswithcode.com/sota/image-classification-on-cifar-100 , 2020
[70] Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E., ImageNet Classification with Deep Convolutional
Neural Networks , vol. 60, pp. 84–90, June 2017, Association for Computing Machinery
[71] K. He and X. Zhang and S. Ren and J. Sun, Deep Residual Learning for Image Recognition , pp. 770-778, IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2016
[72] Simonyan, Karen and Zisserman, Andrew, Very Deep Convolutional Networks for Large-Scale Image Recognition ,
arXiv 1409.1556, 2014
[73] Li, H. and Kadav, A. and Durdanovic, I. and Samet, H. and Graf, H., Pruning filters for efficient convnets , arXiv
preprint arXiv:1608.08710, 2016
13

--- PAGE 14 ---
Ada-QPacknet
[74] Liu, C. and Wu, H., Channel pruning based on mean gradient for accelerating convolutional neural networks ,
Signal Processing, pp. 84–91, 2019
[75] Zhao, C. and Ni, B. and Zhang, J. and Zhao, Q. and Zhang, W. and Tian, Q., Variational convolutional neural
network pruning, in: Proc. of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 2780–2789,
2019
[76] Wang, W. and Zhu, L. and Guo, B., Reliable identification of redundant kernels for convolutional neural network
compression , Journal of Visual Communication and Image Representation, 2019
[77] Hu, Y . and Sun, S. and Li, S. and Wang, X. and Gu, Q., A novel channel pruning method for deep neural network
compression , arXiv:1805.11394, 2018
[78] Chen, S. and Zhao, Q., Shallowing deep networks: Layer-wise pruning based on feature representations , IEEE
Transactions on Pattern Analysis and Machine Intelligence 41 (12), pp. 3048–3056, 2018
[79] Molchanov, D. and Ashukha, A. and Vetrov, D., Variational dropout sparsifies deep neural networks, in: Interna-
tional Conference on Machine Learning , pp. 2498–2507, ICML, 2017
[80] Zhuang, Z. and Tan, M. and Zhuang, B. and Liu, J. and Guo, Y . and Wu, Q. and Huang, J. and Zhu, J.,
Discrimination-aware channel pruning for deep neural networks , pp. 875–886, Advances in Neural Information
Processing Systems (NeurIPs), 2018
[81] Huang, Q. and Zhou, K. and You, S. and Neumann, U., Learning to prune filters in convolutional neural networks ,
pp. 709–718, IEEE Winter Conference on Applications of Computer Vision (WACV), 2018
14
