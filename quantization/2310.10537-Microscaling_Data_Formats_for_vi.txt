# Định dạng dữ liệu Microscaling cho
Deep Learning
Bita Darvish Rouhani∗Ritchie Zhao Ankit More Mathew Hall Alireza Khodamoradi Summer Deng
Dhruv Choudhary Marius Cornea Eric Dellinger Kristof Denolf Stosic Dusan Venmugil Elango
Maximilian Golub Alexander Heinecke Phil James-Roxby Dharmesh Jani Gaurav Kolhe
Martin Langhammer Ada Li Levi Melnick Maral Mesmakhosroshahi Andres Rodriguez
Michael Schulte Rasoul Shafipour Lei Shao Michael Siu Pradeep Dubey Paulius Micikevicius
Maxim Naumov Colin Verrilli Ralph Wittig Doug Burger Eric Chung
Microsoft AMD Intel Meta NVIDIA Qualcomm Technologies Inc.

Tóm tắt
Các định dạng dữ liệu độ rộng bit hẹp là chìa khóa để giảm chi phí tính toán và lưu trữ của các ứng dụng deep learning hiện đại. Bài báo này đánh giá các định dạng dữ liệu Microscaling (MX) kết hợp một hệ số tỷ lệ trên mỗi khối với các kiểu floating-point và integer hẹp cho các phần tử riêng lẻ. Các định dạng MX cân bằng các nhu cầu cạnh tranh về hiệu quả phần cứng, độ chính xác mô hình và ma sát người dùng. Kết quả thực nghiệm trên hơn hai chục benchmark chứng minh tính thực tiễn của các định dạng dữ liệu MX như một sự thay thế trực tiếp cho FP32 cơ sở cho suy luận và huấn luyện AI với ma sát người dùng thấp. Chúng tôi cũng cho thấy trường hợp đầu tiên huấn luyện các mô hình ngôn ngữ sinh tạo với trọng số, kích hoạt và gradient dưới 8-bit với mất mát độ chính xác tối thiểu và không cần thay đổi công thức huấn luyện.

1 Giới thiệu
Những tiến bộ gần đây trong khả năng AI như trả lời câu hỏi đối thoại, hoàn thành mã thông minh và tạo text-to-image đã được áp dụng nhanh chóng trong các công nghệ thực tiễn. Những tiến bộ này đã được thực hiện chủ yếu thông qua việc mở rộng quy mô của mô hình deep learning cơ bản. Tuy nhiên, việc mở rộng quy mô này đã dẫn đến sự gia tăng đáng kể trong sức mạnh tính toán và dung lượng lưu trữ cần thiết để huấn luyện và triển khai các mô hình như vậy.

Một phương pháp để giảm chi phí tính toán và lưu trữ của các mô hình deep learning là sử dụng các định dạng dữ liệu độ rộng bit thấp thay vì FP32 thông thường. Những bước tiến lớn đã được thực hiện để cho phép huấn luyện sử dụng FP16, Bfloat16, và gần đây nhất là FP8 [1], cũng như thực hiện suy luận trong các định dạng integer hẹp như INT8. Hỗ trợ gốc cho các định dạng độ rộng bit thấp hiện đã trở nên phổ biến trong phần cứng hướng AI như GPU, TPU và các thiết bị suy luận edge. Các định dạng hẹp nhất, như FP8 và INT8, yêu cầu các hệ số tỷ lệ trên mỗi tensor để điều chỉnh theo phạm vi động của mỗi tensor. Tuy nhiên, việc tỷ lệ ở mức tensor đã được chứng minh là không đủ cho các định dạng dưới 8-bit do phạm vi động hạn chế của chúng. Nghiên cứu đã cho thấy rằng các định dạng dữ liệu micro scaled liên kết các hệ số tỷ lệ với các khối con chi tiết của tensor hiệu quả hơn trong chế độ dưới 8 bit (ví dụ: [2; 3; 4; 5]).

Bài báo này đánh giá các định dạng dữ liệu Microscaling (MX) [6] — tiêu chuẩn mở đầu tiên cho một họ các kiểu dữ liệu micro-scaled nhằm vào huấn luyện và suy luận deep learning. Tiêu chuẩn MX nhằm tạo ra một định dạng dữ liệu hiệu quả bằng cách đạt được sự cân bằng giữa ba yếu tố chính:

• Hiệu quả Phần cứng — Tối đa hóa hiệu quả tính toán và lưu trữ thông qua độ rộng bit giảm.
• Độ chính xác Mô hình — Giảm thiểu khoảng cách trong chất lượng kết quả so với FP32 cơ sở cho huấn luyện và suy luận AI.
• Ma sát Người dùng — Đảm bảo tích hợp liền mạch trong các framework huấn luyện và suy luận hiện có và tính tổng quát trên các khối lượng công việc khác nhau.

Chi tiết về tiêu chuẩn MX và các định dạng nhị phân cụ thể có thể được tìm thấy trong Đặc tả Microscaling OCP [6]. Bài báo này sẽ tập trung vào các kết quả thực nghiệm của việc sử dụng các định dạng MX cho suy luận trực tiếp, suy luận khuếch tán lỗi và suy luận tinh chỉnh, cũng như huấn luyện trên các benchmark khác nhau.

Kết quả của chúng tôi xác nhận hiệu quả của các định dạng MX trong việc cân bằng các yêu cầu cạnh tranh về hiệu quả phần cứng, độ chính xác mô hình và ma sát người dùng. Các định dạng MX 8-bit có thể thực hiện suy luận trực tiếp trên các mô hình pretrained FP32 với mất mát độ chính xác tối thiểu và không cần hiệu chuẩn hoặc tinh chỉnh. Suy luận với các định dạng MX 6-bit cũng rất gần với FP32 sau khi tinh chỉnh nhận biết lượng tử hóa hoặc sử dụng phương pháp lượng tử hóa sau huấn luyện. Sử dụng các định dạng MX 6-bit, chúng tôi chứng minh trường hợp đầu tiên huấn luyện các mô hình transformer lớn với trọng số, kích hoạt và gradient dưới 8-bit đến độ chính xác khớp với FP32 mà không cần thay đổi công thức huấn luyện. Đi xa hơn nữa, chúng tôi cho thấy rằng việc huấn luyện các transformer lớn có thể được thực hiện với trọng số định dạng MX 4-bit, chỉ phải chịu một sự sụt giảm độ chính xác nhỏ.

Thư viện CUDA tùy chỉnh để mô phỏng các định dạng MX trên GPU hiện có có thể được tìm thấy tại [7]. Thư viện này có thể được sử dụng để tái tạo các kết quả thực nghiệm được báo cáo trong bài báo này.

2 Microscaling

Một đơn vị dữ liệu cơ bản trong định dạng MX đại diện cho một vector của k số và bao gồm một tỷ lệ chia sẻ X và k phần tử vô hướng {Pi}k i=1 (xem Hình 1). Đơn vị dữ liệu này được gọi là khối MX và được định nghĩa bởi sự kết hợp của kích thước khối k, định dạng dữ liệu tỷ lệ và định dạng dữ liệu phần tử. Hai định dạng dữ liệu độc lập với nhau, và tất cả k phần tử chia sẻ cùng định dạng dữ liệu phần tử. Bố cục của khối MX không được quy định — một triển khai có thể lưu trữ X liền kề với hoặc tách biệt khỏi các phần tử.

X
(tỷ lệ chia sẻ)P1(phần tử)
P2(phần tử)
Pk(phần tử)…k phần tử 
vô hướng

Hình 1: Một khối đơn trong định dạng dữ liệu Microscaling. Khối mã hóa một vector của k số, mỗi số có giá trị XPi.

Cho {vi}k i=1 là k số thực được đại diện trong khối MX. Giá trị của mỗi số có thể được suy ra như sau:
• Nếu X=NaN, thì vi=NaN cho tất cả i
• Nếu |XPi|> V max Float 32 thì vi được định nghĩa bởi triển khai
• Ngược lại, vi=XPi

trong đó V max Float 32 đề cập đến độ lớn lớn nhất có thể biểu diễn trong IEEE Float32.

2.1 Mã hóa Giá trị Đặc biệt

Các định dạng MX có thể mã hóa NaN theo tối đa hai cách. Thứ nhất: nếu X là NaN, thì tất cả k giá trị trong khối MX là NaN bất kể mã hóa của Pi. Thứ hai: nếu X không phải là NaN, mỗi phần tử Pi có thể mã hóa NaN riêng lẻ.

Tùy thuộc vào định dạng phần tử, các định dạng MX có thể mã hóa Inf bằng cách để X là một số (tức là không phải NaN) và mỗi Pi mã hóa Inf riêng lẻ. Tỷ lệ chia sẻ X không mã hóa Inf.

2.2 Các Định dạng MX Cụ thể

Bảng 1 cho thấy các tham số định nghĩa các định dạng MX cụ thể, được đặt tên bằng cách thêm tiền tố "MX" vào tên của định dạng dữ liệu phần tử. Tất cả các định dạng MX cụ thể sử dụng E8M0 (số mũ 8-bit) như định dạng cho tỷ lệ chia sẻ. Các số mũ có thể biểu diễn của các định dạng này là tập hợp con của các số mũ có thể biểu diễn của FP32.

Chi tiết về các định dạng dữ liệu phần tử FP8 có thể được tìm thấy trong đặc tả FP8 OCP [1]. Chi tiết về các định dạng dữ liệu phần tử khác và định dạng tỷ lệ E8M0 có thể được tìm thấy trong Đặc tả Microscaling OCP [6].

Bảng 1: Các định dạng dữ liệu tuân thủ MX cụ thể và tham số của chúng.

Tên Định dạng | Kích thước Khối | Định dạng Dữ liệu Tỷ lệ | Bit Tỷ lệ | Định dạng Dữ liệu Phần tử | Độ rộng bit Phần tử
MXFP8 | 32 | E8M0 | 8 | FP8 (E4M3 / E5M2) | 8
MXFP6 | 32 | E8M0 | 8 | FP6 (E2M3 / E3M2) | 6
MXFP4 | 32 | E8M0 | 8 | FP4 (E2M1) | 4
MXINT8 | 32 | E8M0 | 8 | INT8 | 8

3 Chuyển đổi Float Vô hướng sang Định dạng MX

Trong bài báo này, chúng tôi sử dụng Thuật toán 1 để chuyển đổi từ định dạng floating-point vô hướng (ví dụ: FP32) sang định dạng MX. Thuật toán này tuân theo ngữ nghĩa được nêu trong Phần 6.3 của Đặc tả Microscaling OCP [6], và được cung cấp như một ví dụ hoạt động. Lưu ý rằng, đặc tả cho phép các công thức chuyển đổi khác được định nghĩa bởi triển khai — tức là, chuyển đổi sang định dạng MX không nhất thiết phải tuân theo Thuật toán 1.

Thuật toán 1 Chuyển đổi vector các float vô hướng {Vi}k i=1 thành khối MX {X,{Pi}k i=1}
Yêu cầu: emax elem = số mũ của số bình thường lớn nhất trong định dạng dữ liệu phần tử
1: shared_exp ← ⌊log2(max i(|Vi|))⌋ − emax elem
2: X ← 2shared_exp
3: for i = 1 to k do
4: Pi = quantize_to_element_format (Vi/X), clamping các số bình thường
5: end for
6: return X,{Pi}k i=1

Ở Dòng 1, shared_exp chứa một offset của emax_elem để ánh xạ số mũ đầu vào tối đa đến binade lớn nhất trong định dạng dữ liệu phần tử. Điều này cho phép sử dụng đầy đủ phạm vi số mũ của định dạng dữ liệu phần tử.

Ở Dòng 4, khi lượng tử hóa Vi/X, các số bình thường vượt quá phạm vi có thể biểu diễn của định dạng phần tử được kẹp về giá trị tối đa có thể biểu diễn, bảo toàn dấu. Inf và NaN không bị kẹp. Điều này phù hợp với đặc tả MX OCP.

Ở Dòng 4, Pi được đặt thành không nếu Vi đầu vào tương ứng là số subnormal Float32. Điều này không được mô tả trong đặc tả MX OCP và được thực hiện để đơn giản hóa thuật toán.

Khi chuyển đổi các tensor đa chiều, một trục chính phải được chọn cho tỷ lệ chia sẻ (thường là chiều giảm trong phép nhân ma trận). Đối với ma trận 2D, tỷ lệ có thể được chia sẻ bởi mỗi k phần tử trong một hàng hoặc cột. Chuyển vị ma trận 2D trong định dạng MX thay đổi trục của tỷ lệ chia sẻ — tức là, chuyển đổi sang định dạng MX và chuyển vị không phải là các phép toán giao hoán.

4 Kết quả Thực nghiệm

4.1 Luồng Tính toán

Hình 2 cho thấy một ví dụ về luồng tính toán cho huấn luyện sử dụng định dạng MX. Đối với các phép toán liên quan đến tích vô hướng (ví dụ: matmul và convolution) trong cả pass thuận và ngược, hai đầu vào được chuyển đổi sang định dạng MX, và phép toán được thực hiện sử dụng tích vô hướng hiệu quả từ Phần 6.2 của Đặc tả Microscaling OCP [6]. Các phép toán vector (ví dụ: layernorm, Softmax, GELU và residual add) được thực hiện trong định dạng floating-point vô hướng như Bfloat16 hoặc FP32. Các phép toán tích vô hướng tạo ra đầu ra trong định dạng scalar float. Một bản sao chính của các trọng số được giữ trong FP32, và bản sao này được cập nhật trong mỗi bước huấn luyện. Trong tất cả các ví dụ huấn luyện trong bài báo này, chúng tôi sử dụng luồng tính toán được minh họa trong Hình 2.

[Hình 2: Luồng tính toán với các định dạng MX (ký hiệu là MX*). Trong sơ đồ, MatMul bao gồm bất kỳ phép toán tích vô hướng nào như matmul, linear và convolution. Vector Ops bao gồm các phép toán không phải tích vô hướng như activations, normalization, Softmax và residual add.]

Do tính chất không giao hoán của chuyển vị và lượng tử hóa thành định dạng MX (xem Phần 3), các trọng số lượng tử hóa Wi và chuyển vị WT i của chúng phải được lưu trữ như hai tensor riêng biệt. Lưu ý rằng hai tensor không cần được lưu trữ trong bộ nhớ làm việc đồng thời trừ khi một sự xen kẽ rất chi tiết của các pass thuận và ngược được sử dụng.

4.2 Phương pháp

Chúng tôi đã sử dụng một thư viện tùy chỉnh để mô phỏng các định dạng MX trên GPU hiện có. Thư viện được triển khai như một extension CUDA tùy chỉnh trong PyTorch và thực hiện lượng tử hóa theo Hình 2. Cụ thể, chúng tôi đã khám phá bốn cài đặt:

• Suy luận Trực tiếp. Suy luận lượng tử hóa được thực hiện trên mô hình FP32 đã huấn luyện. Tất cả GeMM trong pass thuận được lượng tử hóa trừ khi được gọi ra rõ ràng (pass ngược hoàn toàn không được thực thi).

• Suy luận Khuếch tán Lỗi. Thuật toán khuếch tán lỗi là thuật toán Lượng tử hóa Sau Huấn luyện (PTQ) được dẫn xuất từ GPFQ [8]. Nó thực hiện lượng tử hóa sử dụng một tập dữ liệu hiệu chuẩn nhỏ. Trong thí nghiệm này, tất cả activations và weights trong pass thuận được lượng tử hóa thành cùng định dạng để đơn giản. Quá trình PTQ này là quá trình một pass nhanh mà không có vòng lặp huấn luyện hoặc cần bất kỳ tham số tuning nào.

• Suy luận Tinh chỉnh. Tinh chỉnh nhận biết lượng tử hóa được thực hiện trên mô hình FP32 đã huấn luyện trong một số epoch nhỏ. Đối với việc tinh chỉnh này, tất cả GeMM trong pass thuận được lượng tử hóa, trong khi pass ngược được thực hiện trong FP32. Khám phá siêu tham số được sử dụng để tìm các siêu tham số tinh chỉnh phù hợp.

• Huấn luyện. Một mô hình được huấn luyện từ đầu sử dụng luồng tính toán trong đó tất cả GeMM trong cả pass thuận và ngược đều được lượng tử hóa (xem Hình 2). Đối với huấn luyện độ chính xác hỗn hợp trong đó weights và activations sử dụng các định dạng dữ liệu khác nhau, các gradient (Ei trong Hình 2) được lượng tử hóa thành định dạng activation.

Bộ benchmark của chúng tôi chứa hai loại tác vụ: phân biệt và sinh tạo.

4.3 Suy luận Phân biệt

Trong phần này, chúng tôi kiểm tra các kết quả suy luận với định dạng MX trên nhiều tác vụ phân biệt bao gồm dịch ngôn ngữ, mã hóa văn bản, phân loại hình ảnh, nhận dạng giọng nói và các mô hình gợi ý. Bảng 2 tóm tắt các kết quả liên quan đến suy luận trực tiếp. Kết quả cho suy luận tinh chỉnh được báo cáo trong Bảng 4, và kết quả cho PTQ với suy luận khuếch tán lỗi được báo cáo trong Bảng 3.

Trong các thí nghiệm này, cùng các định dạng MX được sử dụng cho cả weights và activations theo Thuật toán 1. Round-half-to-nearest-even được sử dụng để chuyển đổi sang định dạng MX. Các kết quả được trình bày trong Bảng 2 xác nhận hiệu quả của MXINT8 như một sự thay thế trực tiếp cho FP32 với mất mát độ chính xác tối thiểu. Đối với MXFP8 và MXFP6, xu hướng chung là biến thể của định dạng với nhiều bit mantissa hơn tốt hơn cho suy luận trực tiếp. Với suy luận tinh chỉnh (Bảng 4), MXFP6_E2M3 có thể đạt được gần bằng với FP32.

[Bảng 2-6 được dịch với cấu trúc tương tự, giữ nguyên các số liệu và tên mô hình]

4.4 Suy luận Sinh tạo

Chúng tôi đã tận dụng LM Eval Harness mã nguồn mở của Eleuther AI để đánh giá các định dạng dữ liệu MX trong suy luận sinh tạo của OpenAI GPT3-175B và LLaMA-7B mã nguồn mở. Tất cả các benchmark đều được chạy dưới cài đặt zero-shot (tức là không có ví dụ nào được trình bày cho các mô hình trước khi đánh giá). Bộ benchmark của chúng tôi bao gồm các tập con sau:

Lambada — Lambada là một tác vụ dự đoán tầm xa, trong đó mô hình phải dự đoán từ cuối cùng trong một đoạn văn kể dài. Chúng tôi đã sử dụng phiên bản dữ liệu lambada được sử dụng để đánh giá GPT2 trong LM Harness.

Wikitext — Tác vụ wikitext dựa trên tập dữ liệu wikitext-2 và yêu cầu mô hình dự đoán các chuỗi dài dựa trên các bài viết Wikipedia chất lượng cao. GPT3-175B không được đánh giá trên tác vụ này vì dữ liệu Wikipedia là một phần của corpus huấn luyện của nó [17].

Tập dữ liệu ARC — Các tác vụ Arc đều là tác vụ multiple choice bao gồm gần 8000 câu hỏi thi khoa học, với tập dữ liệu được chia thành các câu hỏi dễ và khó hơn. Mô hình được giao nhiệm vụ chọn câu trả lời đúng từ nhiều lựa chọn.

Test Hendryck — Bộ test Hendryck là một tập hợp các tác vụ đo lường mức độ hiểu biết của mô hình trong 57 lĩnh vực khác nhau. Chúng tôi đã sử dụng computer science, international law và jurisprudence như một tập con cho nghiên cứu này. Các tác vụ này đều là câu hỏi multiple choice, trong đó mô hình phải chọn câu trả lời đúng từ các lựa chọn được trình bày.

[Các bảng kết quả và phân tích tiếp theo được dịch tương tự]

4.5 Huấn luyện Sinh tạo

Bảng 7 và Hình 3 cho thấy loss mô hình ngôn ngữ thu được từ việc huấn luyện các mô hình giống GPT với kích thước khác nhau (20M-1.5B) sử dụng MXFP6_e3m2 cho cả pass thuận và ngược (xem Hình 2). Việc huấn luyện được thực hiện sử dụng optimizer ADAM, với các siêu tham số được tuning cho FP32. Cùng các siêu tham số được sử dụng lại cho các lần chạy định dạng MX mà không có thay đổi. Tất cả các mô hình được huấn luyện đến hiệu quả với số bước được tính toán dựa trên các quy luật power scaling [18]. Round-half-away-from-zero rounding được sử dụng để chuyển đổi sang định dạng MX.

Các kết quả trong Bảng 7 và Hình 3 cho thấy rằng MXFP6_e3m2 có khả năng cung cấp chất lượng mô hình khớp với FP32 với footprint mạch thấp hơn nhiều. MXFP6 cung cấp chứng minh đầu tiên về việc huấn luyện các mô hình ngôn ngữ sinh tạo đến độ ngang bằng với FP32 sử dụng weights, activations và gradients 6-bit mà không cần thay đổi công thức huấn luyện.

Đẩy giới hạn xa hơn, Bảng 8 và Hình 4 cho thấy kết quả từ việc huấn luyện cùng các mô hình giống GPT, lần này dưới cài đặt độ chính xác hỗn hợp với weights MXFP4 và activations MXFP6_e3m2. Các gradient sử dụng cùng định dạng dữ liệu như activations. Các siêu tham số huấn luyện giống như trước. Kết quả của chúng tôi chứng minh rằng các mô hình ngôn ngữ sinh tạo có thể được huấn luyện với weights MXFP4 và activations và gradients MXFP6 chỉ phải chịu một penalty nhỏ trong loss mô hình. Điều này một lần nữa không cần thay đổi công thức huấn luyện.

[Các bảng và hình tiếp theo được dịch với cấu trúc tương tự]

5 Kết luận

Bài báo này đánh giá các định dạng dữ liệu MX tích hợp tỷ lệ mức khối trên các phần tử độ rộng bit hẹp. Các định dạng MX cụ thể được đánh giá cung cấp các lựa chọn thay thế hấp dẫn cho huấn luyện và suy luận FP32 với ma sát người dùng tối thiểu. Kết quả thực nghiệm cho thấy hiệu quả của các định dạng MX cho nhiều mô hình deep learning bao gồm các mô hình ngôn ngữ sinh tạo, phân loại hình ảnh, nhận dạng giọng nói, mô hình gợi ý và dịch thuật.

Cụ thể, MXINT8 là một sự thay thế hấp dẫn cho FP32 cho suy luận trực tiếp ma sát thấp. MXFP6 khớp chặt chẽ với FP32 cho suy luận sau tinh chỉnh nhận biết lượng tử hóa. MXFP6 cũng, lần đầu tiên, cho phép huấn luyện mô hình ngôn ngữ sinh tạo ở weights, activations và gradients dưới 8-bit mà không hy sinh độ chính xác mô hình hoặc cần thay đổi công thức huấn luyện. Giảm độ rộng bit xa hơn, chúng tôi trình bày việc huấn luyện với weights MXFP4 và activations và gradients MXFP6, chỉ phải chịu một penalty loss nhỏ cho các mô hình ngôn ngữ sinh tạo.

Lời cảm ơn

Các tác giả muốn cảm ơn những cá nhân sau đây vì sự hỗ trợ và đóng góp vô giá của họ: Ian Bratt, Nigel Stephens, Jelena Milanovic, John Brothers, Yuan Yu, Rani Borkar, Saurabh Dighe, Brian Harry, Matt Perry, Renee L'Heureux, Dimitry Melts, Jasmine Klar, và Steve Scott.

[Phần Tài liệu tham khảo được dịch tương tự với 18 tài liệu được liệt kê]
