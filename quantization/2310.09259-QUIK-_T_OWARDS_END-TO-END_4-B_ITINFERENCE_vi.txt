# 2310.09259.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2310.09259.pdf
# Kích thước tệp: 809499 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
QUIK: HƯỚNG TỚI SUY LUẬN 4-BIT ĐẦU CUỐI TRÊN CÁC MÔ HÌNH NGÔN NGỮ LỚN SINH TẠO

Saleh Ashkboos* 1Ilia Markov* 2Elias Frantar2Tingxuan Zhong3Xingchen Wang3Jie Ren4
Torsten Hoefler1Dan Alistarh2 5

TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLM) từ họ GPT đã trở nên cực kỳ phổ biến, dẫn đến cuộc đua giảm chi phí suy luận của chúng để cho phép tính toán cục bộ hiệu quả. Tuy nhiên, phần lớn công việc hiện tại chỉ tập trung vào lượng tử hóa chỉ trọng số, có thể giảm chi phí thời gian chạy trong cài đặt sinh tạo từng token bị giới hạn bởi bộ nhớ, nhưng không giải quyết chúng trong các kịch bản bị giới hạn bởi tính toán, chẳng hạn như suy luận theo lô hoặc xử lý prompt. Trong bài báo này, chúng tôi giải quyết vấn đề lượng tử hóa tổng quát, trong đó cả trọng số và kích hoạt đều nên được lượng tử hóa. Chúng tôi chỉ ra, lần đầu tiên, rằng phần lớn tính toán suy luận cho các mô hình sinh tạo lớn như LLaMA, OPT và Falcon có thể được thực hiện với cả trọng số và kích hoạt được đưa về 4 bit, theo cách dẫn đến tăng tốc thực tế, đồng thời duy trì độ chính xác tốt. Chúng tôi đạt được điều này thông qua chiến lược lượng tử hóa hybrid được gọi là QUIK, nén hầu hết trọng số và kích hoạt xuống 4-bit, trong khi giữ một số trọng số và kích hoạt ngoại lai ở độ chính xác cao hơn. Đặc điểm chính của sơ đồ của chúng tôi là nó được thiết kế với hiệu quả tính toán trong đầu: chúng tôi cung cấp các kernel GPU phù hợp với định dạng QUIK với thời gian chạy theo lớp có hiệu quả cao, dẫn đến cải thiện thông lượng đầu cuối thực tế lên đến 3.4x so với thực thi FP16. Chúng tôi cung cấp các nghiên cứu chi tiết cho các mô hình từ họ OPT, LLaMA-2 và Falcon, cũng như một thể hiện đầu tiên của suy luận chính xác sử dụng lượng tử hóa cộng với độ thưa thớt 2:4. Mã nguồn có sẵn tại: https://github.com/IST-DASLab/QUIK .

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) từ họ Generative Pre-trained Transformer (GPT) (Radford et al., 2019) cực kỳ phổ biến. Một yếu tố chính góp phần vào việc áp dụng chúng là khả năng nén chúng bằng các kỹ thuật tiên tiến, ví dụ, (Frantar et al., 2022; Dettmers et al., 2022; Lin et al., 2023; Yuan et al., 2023), cho phép lưu trữ cục bộ và suy luận sinh tạo hiệu quả cho các mô hình này, ngay cả trên máy tính cá nhân. Phần lớn công việc về lượng tử hóa LLM có thể được phân loại thành hai trường hợp:
•Các phương pháp lượng tử hóa chỉ trọng số (Frantar et al., 2022; Dettmers et al., 2022; Lin et al., 2023; Dettmers et al., 2023; Lin et al., 2023; Kim et al., 2023) giúp giảm chi phí truyền tải bộ nhớ khổng lồ của suy luận LLM. Tuy nhiên, các phương pháp này không giảm tính toán, và không thể cung cấp tăng tốc đáng kể cho các cài đặt bị giới hạn bởi tính toán, chẳng hạn như xử lý prompt hoặc suy luận theo lô.
•Các phương pháp lượng tử hóa trọng số-kích hoạt kết hợp, có thể cung cấp cải thiện tính toán, nhưng hoặc tập trung độc quyền vào trọng số và kích hoạt 8-bit (8W8A) (Xiao et al., 2022; Dettmers et al., 2022), hoặc thực thi với lượng lớn mất mát độ chính xác so với các đối tác không nén của chúng (Yuan et al., 2023; Shao et al., 2023).

Do đó, vẫn còn một khoảng cách đáng kể giữa các định dạng nén được hỗ trợ hiệu quả bởi phần cứng—cụ thể, GPU NVIDIA hỗ trợ nguyên bản phép nhân ma trận 4bit được tăng tốc trên cả kiến trúc Ampere và Lovelace (NVIDIA, 2023)—và các thuật toán lượng tử hóa với hỗ trợ tính toán sẽ cho phép suy luận được thực hiện chính xác trên các định dạng nén như vậy.

Đóng góp. Trong bài báo này, chúng tôi thực hiện một bước để thu hẹp khoảng cách này, và lần đầu tiên chỉ ra rằng một phần lớn tính toán trong các LLM hiện đại như OPT (Zhang et al., 2022), LLaMA-2 (Touvron et al., 2023) và Falcon (TII UAE, 2023) có thể được thực hiện chính xác và hiệu quả bằng cách sử dụng kích hoạt và trọng số 4-bit (4W4A).

Về mặt thuật toán, chúng tôi cho thấy kết quả được cải thiện đáng kể so với công việc trước đây về lượng tử hóa kết hợp trọng số và kích hoạt xuống 4 bit, thông qua một sơ đồ hybrid cho

--- TRANG 2 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

Lượng tử hóa thành INT4 với hỗ trợ Kernel GPU, được gọi là QUIK. Trong QUIK, các ma trận được chia thành trọng số và kích hoạt "cơ sở", được xử lý độc quyền ở độ chính xác 4-bit, và một số lượng nhỏ trọng số và kích hoạt "ngoại lai", được xử lý ở độ chính xác cao hơn như INT8 hoặc FP16. Sử dụng cách tiếp cận này, cũng như các insight bổ sung về độ nhạy cảm của lớp, chúng tôi xây dựng một framework có thể khôi phục độ chính xác trong vòng 0.3–0.5 điểm perplexity qua các kích thước mô hình (tương ứng với lỗi tương đối 6%-16%), trong khi thực thi một phần lớn suy luận trong INT4. Để minh họa, cho mô hình LLaMA2 nhạy cảm với 70B tham số, chúng tôi có thể khôi phục độ chính xác trong vòng 0.5 perplexity, trong khi thực thi 70% tính toán lớp tuyến tính trong INT4, dẫn đến tăng tốc đầu cuối 3.4x (xem Hình 1).

Về mặt hệ thống, đặc điểm chính của QUIK là nó có thể được triển khai hiệu quả thông qua các kernel GPU với chi phí thời gian chạy và bộ nhớ thấp so với phép nhân ma trận INT4 nguyên bản GPU (MatMul). Chúng tôi chứng minh điều này thông qua một triển khai tổng quát dẫn đến tăng tốc theo lớp và cải thiện thông lượng đầu cuối so với cả baseline FP16 và INT8. Cụ thể, chúng tôi chỉ ra rằng việc hỗ trợ một số lượng hạn chế các ngoại lai feature và trọng số có thể có chi phí không đáng kể bằng cách hợp nhất các thao tác lượng tử hóa và khử lượng tử hóa vào MatMul và bằng cách giảm thiểu chi phí của chúng trong các lớp tuyến tính thông qua các tối ưu hóa bổ sung.

Nhìn chung, QUIK tận dụng lượng tử hóa cho tăng tốc đầu cuối và giảm bộ nhớ đáng kể. Ví dụ, để xử lý một chuỗi 2048 token trên GPU RTX 3090 thương mại, chúng tôi đạt được tăng tốc đầu cuối từ 3.1x, cho các mô hình OPT-66B và Falcon-180B, đến 3.4x cho LLaMA2-70B, so với tối ưu lý thuyết ≈4x.

Ngoài ra, QUIK cần ít bộ nhớ GPU hơn nhiều, và do đó, ít GPU hơn, so với FP16. Ví dụ, QUIK cung cấp giảm bộ nhớ 3.6x cho OPT-66B, và nén 3x cho thực thi chính xác LLaMA2-70B, thực thi sau này trong ít hơn 50GB bộ nhớ GPU.

2 ĐỘNG LỰC

Phân tích Roofline. Để thúc đẩy việc tập trung vào trường hợp bị giới hạn bởi tính toán, chúng tôi bắt đầu phân tích thao tác tính toán cơ bản trong bối cảnh LLM, một phép nhân ma trận cho các số lượng token khác nhau. Chúng tôi profile một lớp tuyến tính có kích thước tiêu chuẩn (11K x 4K, tương ứng với MLP trong LLaMA-7B (Touvron et al., 2023)), sử dụng toolkit NVIDIA NSight Compute (NVIDIA), từ một token đến 16, 256 và 1024 token.

Kết quả, được minh họa trong Hình 2, rõ ràng cho thấy rằng trường hợp ít token (1 và 16) bị giới hạn bởi bộ nhớ, trong khi khối lượng công việc trở nên bị giới hạn bởi tính toán cho số lượng token lớn hơn, cụ thể là lớn hơn 128. Một triển khai LLM đầu cuối thực tế sẽ cần xem xét tối ưu hóa cả hai kịch bản, vì trường hợp xử lý prompt "prefill" thuộc về kịch bản số lượng token lớn, trong khi sinh từng token một thuộc về trường hợp trước. Hơn nữa, chạy phiên bản "theo lô" của khối lượng công việc một token, tức là cho nhiều người dùng, sẽ lại dẫn đến số lượng token lớn, quay trở lại trường hợp bị giới hạn bởi tính toán.

Hơn nữa, chúng tôi quan sát thấy rằng các phương pháp hiện tại cho lượng tử hóa chỉ trọng số, ví dụ (Frantar et al., 2022; Dettmers & Zettlemoyer, 2022; Lin et al., 2023) chỉ phục vụ để cải thiện cường độ số học của thao tác này, bằng cách giảm lượng dữ liệu cần được truyền tải mỗi thao tác, nhưng vẫn thực hiện tính toán ở độ chính xác gốc. Do đó, chúng không giúp ích trong trường hợp bị giới hạn bởi tính toán, và thực tế thậm chí còn tăng nhẹ lượng tính toán mỗi thao tác, do chi phí khử lượng tử hóa.

Tiềm năng Tăng tốc. Với việc tập trung vào trường hợp bị giới hạn bởi tính toán

--- TRANG 3 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

trường hợp, việc điều tra các tùy chọn phần cứng có sẵn dẫn đến tiềm năng tăng tốc là tự nhiên. Như được hiển thị trong Hình 3, lượng tử hóa là một cách tiếp cận tự nhiên trong trường hợp này, vì GPU NVIDIA có hỗ trợ nguyên bản cho các kiểu dữ liệu INT4 và INT8, cung cấp cải thiện thông lượng lớn qua các kích thước ma trận. Cụ thể, INT8 cung cấp cải thiện thông lượng có thể cao hơn một chút so với 2x so với FP16 trên MatMul thô, trong khi INT4 gần như tăng gấp đôi so với INT8. Tuy nhiên, để tận dụng các thao tác phần cứng này, cả đầu vào lớp (kích hoạt) và trọng số lớp phải được lượng tử hóa thành cùng kiểu dữ liệu nén.

Chúng tôi sẽ tập trung vào lượng tử hóa sau huấn luyện chính xác cho suy luận LLM, bằng cách nén cả trọng số và kích hoạt, chủ yếu thành các kiểu dữ liệu INT4. Như đã nêu, lượng tử hóa chỉ trọng số (Frantar et al., 2022; Lin et al., 2023) không chuyển sang cài đặt của chúng tôi, và lượng tử hóa kích hoạt là nổi tiếng khó khăn (Xiao et al., 2022). Hơn nữa, như được hiển thị trong Bảng 1, các phương pháp hiện tại để lượng tử hóa cả trọng số và kích hoạt trong LLM bị hỏng về mặt độ chính xác khi áp dụng cho nén 4bit.

3 PHƯƠNG PHÁP

3.1 Bối cảnh

Chúng tôi tập trung vào nhiệm vụ tăng tốc các lớp tuyến tính trong Mô hình Ngôn ngữ Lớn (LLM) bằng cách sử dụng lượng tử hóa 4-bit cho cả ma trận trọng số W và ma trận đầu vào X. Theo định nghĩa PyTorch (Paszke et al., 2019), một lớp tuyến tính thực hiện một phép biến đổi tuyến tính cùng với một vector bias b, có dạng XWT+b. Bây giờ chúng tôi mô tả bối cảnh và chi tiết của kỹ thuật.

Ngoại lai trong Lượng tử hóa Đầu vào. Được biết rằng các ma trận kích hoạt khó lượng tử hóa (Dettmers et al., 2022; Xiao et al., 2022; Yuan et al., 2023), chủ yếu do sự hiện diện của các feature ngoại lai trong các ma trận này, trong đó một số cột của X có cường độ lớn hơn lên đến 100x. LLM.int8() (Dettmers et al., 2022) xác định và trích xuất các cột ngoại lai của X trong quá trình forward pass và lượng tử hóa phần còn lại của các phần tử với 8-bit. Tuy nhiên, LLM.int8() không hiệu quả về thời gian chạy do chi phí tính toán được thêm vào việc xác định ngoại lai tại thời điểm chạy. Công việc gần đây (Xiao et al., 2022) đã chỉ ra rằng các feature ngoại lai được cố định cho mỗi lớp qua các tập dữ liệu, có nghĩa là chúng ta có thể trích xuất các chỉ số ngoại lai ngoại tuyến bằng cách sử dụng một tập hiệu chuẩn nhỏ.

Lượng tử hóa Trọng số GPTQ. GPTQ (Frantar et al., 2022) là một phương pháp lượng tử hóa chỉ trọng số bao gồm việc lượng tử hóa W trong khi giữ lại các kích hoạt X trong FP16. Để làm điều này, nó lặp qua các cột trọng số; đối với mỗi cột, nó lượng tử hóa tất cả các phần tử của nó một cách đồng thời. Sau khi lượng tử hóa một cột trọng số, GPTQ điều chỉnh các cột chưa được lượng tử hóa còn lại, ở bên phải của cột hiện tại, bằng cách sử dụng thông tin bậc hai để bù đắp cho lỗi lượng tử hóa được giới thiệu trong bước hiện tại. Quá trình này tích lũy các lỗi lượng tử hóa tại các cột cuối cùng, làm cho chúng nhạy cảm hơn với lượng tử hóa.

3.2 Lượng tử hóa QUIK

Tổng quan. Ở mức độ cao, QUIK hoạt động như sau. Đầu tiên, lưu ý rằng, trong quá trình biến đổi tuyến tính XWT, các cột ngoại lai trong X, mà chúng tôi có nghĩa là các cột có giá trị trung bình lớn được định nghĩa trước đây, sẽ luôn được nhân với các cột nhất định trong WT, như được minh họa trong Hình 4. Chúng tôi

--- TRANG 4 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

tận dụng quan sát này để cải thiện chất lượng lượng tử hóa GPTQ, trong một cài đặt mà chúng tôi cũng lượng tử hóa (một phần) các kích hoạt.

Vì các cột ngoại lai được cố định qua các tập dữ liệu, chúng tôi bắt đầu bằng cách trích xuất các chỉ số của các cột ngoại lai bằng một tập hiệu chuẩn. Sau đó, chúng tôi sắp xếp lại các cột trọng số (và các cột đầu vào tương ứng của chúng), để dịch chuyển các ngoại lai về phía cuối. Cuối cùng, chúng tôi thực hiện lượng tử hóa trên các cột trọng số lên đến chỉ số của các ngoại lai. Điều này phá vỡ lượng tử hóa của các cột "khó khăn" này. Nó cũng giúp lượng tử hóa GPTQ bằng cách 1) tập hợp các lỗi lượng tử hóa vào các cột chúng tôi giữ trong FP16, và 2) loại bỏ các ngoại lai trọng số tiềm năng khỏi thang đo lượng tử hóa 4bit.

Cắt Trọng số. Cắt trọng số cải thiện lượng tử hóa bằng cách cắt tỉa phân phối đầu vào trước khi làm tròn. Điều này có thể được thực hiện bằng cách huấn luyện toàn bộ mạng để tìm ngưỡng cắt tối ưu (Shao et al., 2023; Esser et al., 2019; Choi et al., 2018); hoặc sử dụng các phương pháp heuristic (Lin et al., 2023; Lee et al., 2023; Kim et al., 2023). Chúng tôi thấy rằng việc áp dụng tìm kiếm tuyến tính qua các ngưỡng cắt cho lượng tử hóa trọng số cải thiện perplexity cuối cùng.

Lượng tử hóa Một phần Dựa trên Độ nhạy cảm. Việc chọn chính xác các cột ngoại lai là chìa khóa cho QUIK. Theo (Xiao et al., 2022; Dettmers et al., 2022), chúng tôi chọn các cột có norm ℓ∞ lớn nhất làm ngoại lai. Vì việc tìm các cột này một cách động tại thời gian chạy tốn kém, chúng tôi theo (Xiao et al., 2022) trong việc xác định một tập ngoại lai được định trước cho mỗi lớp thông qua một tập hiệu chuẩn (xem Phần 4), và lượng tử hóa các trọng số ngoại tuyến. Chúng tôi sử dụng cùng các chỉ số ngoại lai để trích xuất các cột ngoại lai đầu vào trong quá trình forward pass.

Cách tiếp cận này đủ để lượng tử hóa chính xác các mô hình như OPT (Zhang et al., 2022) (xem Phần 4). Tuy nhiên, các mô hình khổng lồ có độ chính xác cao như LLaMA2-70B đặt ra một thách thức thêm do các lớp FeedForward của chúng, bao gồm ba phép biến đổi tuyến tính cùng với phép nhân theo phần tử, cũng như việc sử dụng các kích hoạt Sigmoid Linear Unit (SiLU). Cụ thể, phân tích norm ℓ∞ của chúng tôi được minh họa trong Hình 10, gợi ý rằng các lớp Down proj nhạy cảm hơn nhiều với lượng tử hóa. (Li et al. (2023) đến kết luận tương tự.) Do đó, chúng tôi mở rộng sơ đồ của mình để khôi phục độ chính xác bằng cách lượng tử hóa các lớp Down proj thành 8 bit thay vì 4, mà không có thay đổi khác nào đối với phương pháp của chúng tôi. Chúng tôi minh họa quy trình chọn ngoại lai chi tiết trong Phần 4.3.1. Hình 11 trình bày phân tích chi tiết về phân tích FLOP tổng thể với các độ chính xác khác nhau khi lượng tử hóa mô hình LLaMA2-70B thông qua QUIK.

3.3 Triển khai Suy luận Hiệu quả

Bây giờ chúng tôi cung cấp mô tả cấp cao về cách các mô hình ở định dạng QUIK được thực thi hiệu quả trên GPU. Chúng tôi minh họa quy trình làm việc trong Hình 5 và cung cấp mã giả trong Thuật toán 1. Bước đầu tiên và quan trọng nhất trong QUIK là chia ma trận đầu vào có hình dạng (#tokens, #features) theo cột, vì vậy qua các feature, thành hai tập con, một phần "độ chính xác đầy đủ" nhỏ (thường là half hoặc bfloat16) và một phần cơ sở lớn, sẽ được lượng tử hóa (xem dòng 3 trong mã giả). Phần độ chính xác đầy đủ được nhân với phần tương ứng (độ chính xác đầy đủ) của ma trận trọng số theo cách tiêu chuẩn, trong khi phần còn lại đi qua đường ống phép nhân ma trận được lượng tử hóa được mô tả tiếp theo.

Đường ống MatMul được lượng tử hóa bao gồm ba phần: 1) lượng tử hóa động các kích hoạt, 2) thực sự thực hiện MatMul của các kích hoạt và trọng số được lượng tử hóa, và 3) khử lượng tử hóa kết quả trở lại định dạng floating point.

Lượng tử hóa. Nói chung, chúng tôi lượng tử hóa trọng số một cách đối xứng (chỉ có thang đo) mỗi đầu ra và lượng tử hóa kích hoạt một cách bất đối xứng (thang đo và zero) mỗi token. Cái trước được thực hiện ngoại tuyến (xem Phần 3.2), trong khi cái sau phải được thực hiện trực tuyến dựa trên các giá trị kích hoạt hiện tại. Cụ thể, chúng tôi đầu tiên quét các kích hoạt để xác định giá trị min- và max- mỗi token, từ đó chúng tôi tính toán thang đo và điểm zero (dòng 12). Chúng được sử dụng để biến các kích hoạt floating point thành số nguyên, được viết ra lại dưới dạng các giá trị INT4 hoặc INT8 có dấu (do đó trừ halfRange trong dòng 14) ở định dạng đóng gói để xử lý hiệu quả tiếp theo (xem dòng 13-16).

Phép nhân Ma trận. MatMul thực tế được thực hiện bởi thư viện CUTLASS (NVIDIA, 2023), có khả năng sử dụng hiệu quả các tensor-core INT8/INT4 của phần cứng để thực hiện tính toán độ chính xác thấp nhanh, trong khi tích lũy kết quả ở định dạng INT32 rộng hơn.

Khử lượng tử hóa. Vì MatMul được thực hiện hoàn toàn với các giá trị INT được lượng tử hóa, chúng ta cần chuyển đổi trở lại định dạng floating point để tích hợp đúng thông tin thang đo và zero. Cụ thể, chúng ta cần nhân mỗi phần tử đầu ra oij với thang đo token đầu vào tương ứng scaleAct và thang đo trọng số đầu ra scaleWeight (dòng 22). Ngoài ra, chúng ta cũng cần tính đến điểm zero kích hoạt zeroAct. Để làm điều này, chúng ta xem xét một tích vô hướng ⟨w, x⟩ (đại diện cho một giá trị đầu ra duy nhất trong matmul tổng thể của chúng ta) trong đó một hằng số z được thêm vào mỗi xi:
y=X iwi(xi+z) =X iwixi+z·X iwi.(1)
Do đó, chúng ta phải dịch chuyển bằng z nhân với tổng trên các trọng số liên quan, cái sau là tĩnh và do đó có thể được tính toán trước dưới dạng wReduced; việc chuyển đổi INT có dấu thành không dấu cũng phải được xem xét (dòng 23 - 24). Cuối cùng, chúng ta thêm các giá trị được khử lượng tử hóa này vào kết quả ngoại lai gốc, tạo ra đầu ra cuối cùng (dòng 8).

--- TRANG 5 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

Thuật toán 1 Các kernel Lượng tử hóa và Khử lượng tử hóa.
1:function QUIK Matmul
2: Đầu vào: wInt, wFP, x, FPindices, scaleWeight, wReduced;
3: xFP, xQ ←−split(x, FPindices);
4: xINT, zeroAct, scaleAct ←−Quantization(xQ);
5: resultFP ←−FPmatmul(xFP, wFP);
6: resultInt ←−INTmatmul(xInt, wInt);
7: dequantFP ←− Dequantization(resultInt, zeroAct, scaleAct, scaleWeight, wReduced)
8: return dequantFP + resultFP;
9:end function
10:function Quantization
11: Đầu vào: dataFP;
12: zeroAct, scaleAct ←− findZeroScale(dataFP);
13: forelem∈dataFP, outElem ∈output do
14: // Sử dụng scale/zero tương ứng với token
15: outFP ←− (elem - zeroAct) / scaleAct - halfRange;
16: outElem ←− pack(outFP);
17: end for
18: return output, zeroAct, scaleAct;
19:end function
20:function Dequantization
21: Đầu vào: inputINT, zeroAct, scaleAct, scaleWeight, wReduced
22: forelem∈inputINT, outElem ∈outputFP do
23: // Sử dụng scale cho token và hàng trọng số, tương ứng
24: x ←− elem * scaleAct * scaleWeight;
25: shift ←− zeroAct + halfRange * scaleAct;
26: shift ←− shift * wReduced;
27: outElem ←− x + shift;
28: end for
29: return outputFP;
30:end function

3.4 Tối ưu hóa Hiệu suất

Xương sống tính toán của triển khai kernel QUIK là phép nhân ma trận CUTLASS độ chính xác thấp. Tuy nhiên, bản chất độ chính xác hỗn hợp của thuật toán áp đặt việc sử dụng các hàm phụ trợ, như chia tách dữ liệu đầu vào, tính toán metadata, lượng tử hóa và khử lượng tử hóa. Điều này cung cấp cơ hội cho các tối ưu hóa.

Hợp nhất Lượng tử hóa. Một triển khai naïve của đường ống chia tách và lượng tử hóa sẽ yêu cầu một lần đọc-và-ghi cho phần ngoại lai, một lần đọc-và-ghi khác cho phần cơ sở, hai lần đọc để xác định giá trị min-max mỗi token và một lần đọc-và-ghi nữa để thực sự thực hiện lượng tử hóa. Nhiều thao tác bị giới hạn bởi bộ nhớ chậm này có thể được tối ưu hóa thông qua việc hợp nhất operator cẩn thận dưới dạng các kernel tùy chỉnh.

Cụ thể, chúng tôi gán mỗi hàng đầu vào cho một khối CUDA và thực hiện 3 lần qua nó: reduction (tìm thông tin meta) trên các phần tử không phải ngoại lai, lượng tử hóa của các không phải ngoại lai và di chuyển các ngoại lai đến một mảnh bộ nhớ riêng biệt. Điều này loại bỏ hai lần đọc tốn kém (tính toán min-max và chia tách phần cơ sở) và một lần ghi (chia tách phần cơ sở), và chi phí của các lần khởi chạy kernel bổ sung.

Điều chỉnh Song song hóa. Để quy trình lượng tử hóa ở trên hiệu quả trên GPU hiện đại, chúng ta phải đảm bảo song song hóa tối ưu thông qua việc điều chỉnh cẩn thận các khối CUDA và số lượng thread. Tham số điều chỉnh quan trọng nhất là số hàng chúng ta xử lý với một khối CUDA. Ánh xạ một khối cho mỗi hàng mang lại chi phí khởi chạy bổ sung, trong khi ánh xạ quá nhiều hàng mỗi khối dẫn đến over-subscription khối và chiếm dụng GPU thấp hơn. Do đó, chúng tôi tối ưu hóa số hàng thích hợp mỗi khối cho các kích thước ma trận khác nhau (thường là các giá trị từ 8 đến 32). Điều này cải thiện tốc độ lượng tử hóa lên đến 30%.

Epilogue Khử lượng tử hóa. CUTLASS đầu tiên tích lũy kết quả MatMul trong các thanh ghi trước khi cam kết chúng vào bộ nhớ toàn cục (chậm). Chúng ta có thể tránh một lần ghi và đọc không cần thiết của kết quả matmul INT32 trung gian bằng cách trực tiếp thực hiện khử lượng tử hóa trong một epilogue tùy chỉnh được áp dụng trước khi cam kết bộ nhớ toàn cục, mà chúng ta tiếp tục tích lũy trực tiếp vào kết quả của MatMul ngoại lai. Nhìn chung, điều này xen kẽ hai thao tác tốn kém và tiết kiệm các lần khởi chạy kernel và chuyến đi bộ nhớ bổ sung.

Tác động Hiệu suất. Để minh họa tác động của các tối ưu hóa này, chúng tôi đánh dấu chúng là các phiên bản khác nhau của kernel của chúng tôi: phiên bản 1 có lượng tử hóa và khử lượng tử hóa không được hợp nhất; phiên bản 2 có lượng tử hóa được hợp nhất và khử lượng tử hóa không được hợp nhất; phiên bản 3 hợp nhất cả lượng tử hóa và khử lượng tử hóa.

Hình 6 cung cấp phân tích chi tiết về kết quả của mỗi tối ưu hóa này. Chúng tôi quan sát thấy rằng chúng đặc biệt hiệu quả cho các kích thước ma trận nhỏ, nơi chúng dẫn đến tăng tốc đầu cuối gần 2x. Tối ưu hóa lượng tử hóa được hợp nhất mang lại cải thiện thông lượng lên đến 40% và epilogue khử lượng tử hóa mang lại thêm 10% tăng tốc.

4 XÁC THỰC THỰC NGHIỆM

Thiết lập chung. Chúng tôi đánh giá phương pháp của mình trên các mô hình OPT (Zhang et al., 2022), LLaMA-2 (Touvron et al., 2023), và Falcon (TII UAE, 2023), sử dụng các triển khai HuggingFace (Wolf et al., 2019) của định nghĩa mô hình và tập dữ liệu. Theo SmoothQuant (Xiao et al., 2022), chúng tôi trích xuất các chỉ số ngoại lai bằng cách sử dụng 512 câu ngẫu nhiên từ tập dữ liệu Pile (Gao et al., 2020). Chúng tôi xem xét lên đến 5% (dựa trên kích thước mô hình) của các feature đầu vào làm ngoại lai trong các lớp tuyến tính. Trong quá trình lượng tử hóa trọng số GPTQ, chúng tôi chọn ngẫu nhiên 128 mẫu với độ dài chuỗi 2048 từ tập dữ liệu C4 (Raffel et al., 2020). Chúng tôi áp dụng lượng tử hóa đối xứng cho trọng số và lượng tử hóa bất đối xứng cho kích hoạt. Các ngưỡng cắt cho lượng tử hóa trọng số được tìm thông qua tìm kiếm tuyến tính trên sai số bình phương. Sơ đồ của chúng tôi lượng tử hóa một mô hình 70B trong ít hơn 2 giờ trên một GPU NVIDIA A100 duy nhất.

--- TRANG 6 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

4.1 Khôi phục Độ chính xác

So sánh Độ chính xác trên OPT. Đầu tiên chúng tôi so sánh độ chính xác của QUIK với các phương pháp lượng tử hóa 4W4A trước đây: SmoothQuant (Xiao et al., 2022), RPTQ (Yuan et al., 2023) và OmniQuant (Shao et al., 2023).

Bảng 1 hiển thị kết quả của tất cả các phương pháp cho 4 mô hình OPT lớn hơn trên nhiệm vụ WikiText2 (Merity et al., 2016). Chúng tôi quan sát thấy rằng, với QUIK, độ chính xác của các mô hình OPT vẫn nhất quán ngay cả khi sử dụng số lượng ngoại lai đồng nhất cho tất cả các lớp (thay vì sử dụng tỷ lệ phần trăm của các feature đầu vào). Do đó, chúng tôi sử dụng 256 ngoại lai trên tất cả các module tuyến tính (khoảng ≈3% kích thước ẩn của OPT-66B). Như có thể thấy, bằng cách tận dụng hiệu quả một lượng nhỏ các cột ngoại lai độ chính xác đầy đủ, QUIK có thể vượt trội đáng kể so với các phương pháp 4-bit trước đây, chỉ giảm 0.3 đến 0.5 điểm perplexity so với baseline độ chính xác đầy đủ. Chúng tôi nhấn mạnh rằng, để so sánh công bằng, QUIK lượng tử hóa tất cả các lớp backbone tuyến tính thành 4-bit ở đây. Kết quả bổ sung được trình bày trong Phụ lục A.

Độ chính xác trên Các mô hình LLaMA-2 và Falcon. Tiếp theo, chúng tôi chuyển sang các mô hình LLaMA-2 và Falcon. Xem Bảng 2 cho kết quả trên WikiText2. Như có thể thấy, QUIK-4B có thể bảo tồn độ chính xác trong tất cả các mô hình với nhiều nhất 0.5 mất mát perplexity cho các mô hình LLaMA-2, và 0.3 cho các mô hình Falcon.

Độ chính xác Zero-Shot. Tiếp theo, chúng tôi đánh giá tác động của QUIK đối với độ chính xác của các nhiệm vụ zero-shot. Để làm điều này, chúng tôi nghiên cứu độ chính xác trung bình của các mô hình LLaMA-2 và OPT lớn nhất trên năm nhiệm vụ zero-shot phổ biến: PIQA (Tata & Patel, 2003); WinoGrande (Sakaguchi et al., 2021); HellaSwag

--- TRANG 7 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

(Zellers et al., 2019); Arc (Easy và Challenge) (Boratko et al., 2018). Chúng tôi sử dụng LM Evaluation Harness (Gao et al., 2021) với các tham số mặc định trong các thí nghiệm của chúng tôi. Bảng 3 hiển thị độ chính xác trung bình của QUIK trên các nhiệm vụ zero-shot. Tương tự như nhiệm vụ sinh, QUIK bảo tồn độ chính xác của các nhiệm vụ zero-shot với nhiều nhất 1.5% giảm độ chính xác cho các mô hình LLaMA-2 và 1.1% cho các mô hình OPT.

Lượng tử hóa 8-Bit. Chúng tôi so sánh độ chính xác của QUIK-8B với SmoothQuant (Xiao et al., 2022) trên OPT, LLaMA-2, và Falcon. Chúng tôi sử dụng lượng tử hóa bất đối xứng per-token cho kích hoạt và lượng tử hóa đối xứng cho trọng số trong SmoothQuant (đây là các cài đặt cơ bản giống như cho QUIK). Bảng 4 hiển thị rằng mặc dù cả hai sơ đồ đều gần như không mất mát về mặt khác biệt perplexity so với FP16, QUIK tạo ra kết quả độ chính xác cao hơn trong hầu hết các trường hợp, so với SmoothQuant. Hơn nữa, không rõ liệu SmoothQuant có thể được áp dụng cho các mô hình với attention song song, như mô hình Falcon-7B, nơi các khối MLP và Attention chia sẻ cùng layer norm cho đầu vào của chúng, vì điều này ngăn cản việc hợp nhất yếu tố thang đo. Xem Phụ lục C để biết thêm kết quả.

Các lớp Không có Ngoại lai. Cuối cùng, chúng tôi nghiên cứu hiệu ứng của việc giữ nhiều lớp tuyến tính mà không có bất kỳ ngoại lai nào. Điều này có thể giúp tăng hiệu suất đầu cuối bằng cách loại bỏ tất cả các chi phí liên quan đến ngoại lai trong quá trình forward pass. (Mặc dù, như chúng tôi sẽ chỉ ra sau, những chi phí này là nhỏ.) Bảng 5 hiển thị cách độ chính xác của các mô hình khác nhau thay đổi khi chúng tôi sử dụng các giá trị ngưỡng tuyệt đối khác nhau (được hiển thị bởi T), được trích xuất bằng cách sử dụng tìm kiếm tuyến tính, cho các ngoại lai. Chúng tôi kết luận rằng không có ngưỡng phổ quát nào trên tất cả các mô hình, sẽ bảo tồn độ chính xác trên tất cả các mô hình. Ví dụ, Falcon-180B có thể đạt được độ chính xác hợp lý ngay cả khi 24% các lớp tuyến tính (115 trong số 480) chứa zero ngoại lai. Tuy nhiên, đây không phải là trường hợp cho các mô hình nhỏ hơn: LLaMA2-70B có thể khôi phục độ chính xác với lên đến 5% các lớp tuyến tính (30 trong số 560) có zero ngoại lai QUIK. Chúng tôi cung cấp các thí nghiệm bổ sung trong Phụ lục D.

4.2 Phân tích Hiệu suất

Bây giờ chúng tôi xem xét hiệu suất của triển khai QUIK bằng cách đánh giá các khía cạnh khác nhau của kernel của chúng tôi. Chúng tôi sử dụng PyTorch/1.13, CUDA/11.8, Huggingface Transformers/4.34. Chúng tôi chạy tất cả các thí nghiệm của mình trên GPU RTX 3090 vì mục tiêu chính của chúng tôi là tăng tốc suy luận LLM trên GPU thương mại. Phụ lục G hiển thị kết quả tương tự trên GPU RTX 3080.

Sử dụng Bộ nhớ Đỉnh. Đầu tiên, chúng tôi đánh giá việc sử dụng bộ nhớ đỉnh của các mô hình được lượng tử hóa của chúng tôi. Trong Bảng 6, chúng tôi đánh giá việc sử dụng bộ nhớ đỉnh trên các cấu hình khác nhau cho họ OPT và LLaMA-2. Đối với OPT-66B, các mô hình QUIK-8B và QUIK-4B chứng minh giảm bộ nhớ đỉnh khoảng 47% (so với giảm lý tưởng 50%) và 74% (so với giảm lý tưởng 75%), tương ứng. Đối với mô hình LLaMA2-70B, việc giảm là 32% cho QUIK-8B và 67% cho QUIK-4B. Điều này là do chúng tôi giữ down-projection ở 8-bit và sử dụng ngoại lai bổ sung. Chi phí bổ sung đến từ các buffer phụ trợ, khác nhau cho các kích thước lớp khác nhau.

Tăng tốc Lý tưởng và Theo lớp. Tiếp theo, chúng tôi đánh giá các tăng tốc lý tưởng, cũng như các tăng tốc thực tế chúng tôi đo được trong mỗi khối Transformer riêng biệt. Kết quả trong Hình 3 mô tả sức mạnh tính toán "lý tưởng" cho phép nhân ma trận theo lớp ở các mức độ chính xác khác nhau, mà không tính đến bất kỳ chi phí lượng tử hóa/khử lượng tử hóa nào. Ở đây, chúng tôi tập trung vào tăng tốc có thể thực hiện được khi thực thi Thuật toán 1, bao gồm phép nhân độ chính xác hỗn hợp cũng như các thao tác nén và giải nén.

Trong Hình 7, chúng tôi so sánh hiệu suất theo lớp của các lớp tuyến tính được lượng tử hóa (QUIK-4B sử dụng 256 ngoại lai mỗi lớp) so với FP16, cho một triển khai đầy đủ của thuật toán của chúng tôi. Các kích thước ma trận tương ứng với các lớp trong mô hình LLaMA. Chúng tôi quan sát thấy rằng QUIK-4B có thể đạt được tăng tốc hơi cao hơn 4× trên các lớp lớn và hơn 2× trên các lớp nhỏ hơn. Do đó, tăng tốc của tăng tốc matmul độ chính xác thấp thô có thể một phần "ẩn" các chi phí của QUIK.

Tăng tốc đầu cuối. Cuối cùng, chúng tôi cũng chứng minh lợi ích tăng tốc đầu cuối của các mô hình QUIK. Để mục đích này, chúng tôi tích hợp QUIK vào triển khai PyTorch HuggingFace được sử dụng rộng rãi, bằng cách thay thế các lớp tuyến tính bằng các triển khai lại QUIK 4-bit (và 8-bit). Đối với mô hình LLaMA, chúng tôi sử dụng FlashAttention (Dao et al., 2022) cho tất cả các mô hình (bao gồm cả FP16). Số lượng ngoại lai trong QUIK-4B được đặt thành 256 ngoại trừ trường hợp đặc biệt của các lớp down projection trong LLaMA và FC2 trong các mô hình Falcon, mà chúng tôi lượng tử hóa thành 8 bit với hơn 600 ngoại lai.

Trong Hình 9, chúng tôi so sánh cải thiện thông lượng của các lần prefill (cho các lô đơn với 2048 token) cho các mô hình được lượng tử hóa, so với phiên bản FP16 tương ứng. Biểu đồ cột hiển thị cải thiện thông lượng của QUIK-4B so với FP16. Các chú thích đến baseline đại diện cho các giá trị thông lượng thực tế của nó trong các thí nghiệm của chúng tôi. Ví dụ, OPT-66B sử dụng các lớp tuyến tính FP16 đạt được 439 token/s trong khi suy luận mô hình tương tự với các lớp tuyến tính QUIK-4B dẫn đến 1343 token/s. Điều này cho thấy rằng, ngoài việc giảm bộ nhớ gần 4×, giảm số lượng GPU cần thiết cho suy luận, QUIK cũng đạt được thông lượng cao hơn lên đến 3.4× so với FP16, với những cải thiện lớn nhất đạt được trên các mô hình lớn nhất (LLaMA2-70B), nơi tác động tương đối của chi phí là thấp nhất. Việc giảm bộ nhớ quan trọng trong trường hợp suy luận Falcon: chúng tôi không thể chạy Falcon-180B ở độ chính xác đầy đủ trên 8xRTX3090 GPU, vì đỉnh bộ nhớ tối đa của mô hình là hơn 360GB. Tuy nhiên, QUIK-4B cho phép chúng tôi chạy suy luận đầy đủ của mô hình 180B này trên một máy chủ duy nhất dẫn đến 542 token/giây. Do đó, chúng tôi ước tính tăng tốc cho mô hình FP16 180B trong Hình 9(c) dựa trên thời gian chạy của một khối Transformer duy nhất.

Chúng tôi nhấn mạnh rằng tăng tốc trong các thí nghiệm đầu cuối của chúng tôi là độc quyền thông qua các lớp tuyến tính được tăng tốc QUIK. Tất cả các hàm khác hoàn toàn giống nhau. Như được hiển thị trong Hình 8 (Phải), chi phí từ attention, softmax, hoặc các thao tác layernorm trở nên đáng kể khi một phần lớn tính toán xảy ra ở độ chính xác 4-bit.

Chi phí Hiệu suất Ngoại lai. Để minh họa các tác động hiệu suất của việc hỗ trợ ngoại lai, trong Hình 8 (trái) chúng tôi cung cấp tăng tốc đầu cuối cho các biến thể của tích hợp HuggingFace nơi chúng tôi trực tiếp sử dụng các kernel 8-bit và 4-bit, mà không bảo tồn độ chính xác (Lý tưởng 8-bit và 4-bit), so với các triển khai QUIK bảo tồn độ chính xác của chúng tôi.

Chúng tôi quan sát thấy rằng triển khai 8-bit cung cấp tăng tốc gần như lý tưởng, giảm số lượng GPU từ 7 xuống 5. QUIK-4B (tính đến ngoại lai) hoạt động ≈15% tốt hơn, tiếp tục giảm số lượng GPU cần thiết xuống 3, sử dụng ít hơn 50 GB bộ nhớ GPU. Tác động hiệu suất của việc chọn ngoại lai (do đó phép nhân ma trận độ chính xác hỗn hợp) và lượng tử hóa 8-bit có chọn lọc (cho lớp MLP down-projection) được hiển thị trong so sánh với Lý tưởng 4-bit. QUIK-4B nằm trong vòng 15% hiệu suất Lý tưởng 4-bit. (Tuy nhiên, hiện tại không biết làm thế nào một mô hình với trọng số và kích hoạt ở 4 bit có thể khôi phục độ chính xác.) Lý do cho tác động hiệu suất này được cung cấp trong Hình 8 (phải), nơi chúng tôi phân tích chi phí theo thao tác cho suy luận LLaMA2-70B. Cụ thể, chúng tôi quan sát ở đây và trong Hình 6 rằng chi phí của lượng tử hóa và phép nhân độ chính xác đầy đủ có thể chiếm một phần lớn thời gian thao tác tổng thể, đặc biệt là đối với các ma trận nhỏ hơn. Điều này minh họa sự đánh đổi giữa hiệu suất và độ chính xác cho một mô hình cụ thể.

4.3 Nghiên cứu Ablation

Bây giờ chúng tôi cung cấp các ví dụ chi tiết về việc sử dụng QUIK trên hai mô hình lớn: LLaMA2-70B, và Falcon-180B. Mô hình trước quan trọng vì nó cho thấy hiệu suất cao trên các nhiệm vụ khác nhau (Touvron et al., 2023). Mô hình sau là mô hình kiểu GPT lớn nhất có sẵn công khai.

4.3.1 Nghiên cứu Trường hợp 1: LLaMA2-70B

Đầu tiên, chúng tôi nghiên cứu phân tích FLOP qua các độ chính xác sử dụng QUIK-4B trên LLaMA2-70B. Tiếp theo, chúng tôi nghiên cứu hiệu ứng của các tham số chính của QUIK: Down-Projection 8-bit, và Số lượng Ngoại lai. Chúng tôi cung cấp ablation bổ sung trong Phụ lục B.

Down-Projection 8-bit. Trong module MLP của mô hình LLaMA2-70B, ba lớp tuyến tính có mặt, được gọi là "Up-Proj", "Gate-Proj", và "Down-Proj". "Up-Proj" và "Gate-Proj" chia sẻ một đầu vào (đầu vào MLP) và áp dụng các phép biến đổi tuyến tính tương ứng của chúng lên nó. Sau đó, đầu ra của "Gate-Proj" được áp dụng hàm kích hoạt SiLU. Cuối cùng, đầu vào cho lớp "Down-Proj" được xây dựng bằng cách lấy tích Hadamard của các đầu ra từ "Up-Proj" và "Gate-Proj".

Hình 10 hiển thị phương sai của đầu vào qua các lớp khác nhau trong LLaMA2-70B, mà chúng tôi sử dụng làm hướng dẫn để chọn cả số lượng ngoại lai và tập hợp các lớp được thực thi ở độ chính xác 8 bit. Cụ thể, có thể quan sát thấy rằng các lớp "Down-Proj" có phương sai đầu vào lớn, chủ yếu do tích Hadamard của hai đầu ra trước đó, dẫn đến độ chính xác kém cho lượng tử hóa 4-bit. Để giải quyết điều này, chúng tôi sử dụng lượng tử hóa 8-bit cho cả trọng số và kích hoạt trong các lớp "Down-Proj" của các mô hình LLaMA2. Bảng 7 hiển thị rằng việc giữ các lớp down-projection ở 8-bit là quan trọng cho độ chính xác cao trên LLaMA2, vì nó cải thiện perplexity hơn 2 điểm, qua tất cả các mô hình.

Phân tích FLOP/s. Hình 11 hiển thị tỷ lệ phần trăm FLOP/s chúng tôi giữ ở mỗi độ chính xác (INT4 cho trọng số cơ sở, FP16 cho ngoại lai, và INT8 cho các lớp down-projection) trong LLaMA2-70B. Chính xác hơn, cho 256 ngoại lai, chúng tôi thực hiện ≈70% các thao tác ở 4-bit và ≈27% sử dụng 8-bit.

Số lượng Ngoại lai. Cuối cùng, chúng tôi xem xét cách các số lượng ngoại lai khác nhau ảnh hưởng đến điểm số WikiText2 cho mô hình LLaMA2-70B. Trong Bảng 8, chúng tôi quan sát thấy rằng việc tăng ngoại lai từ 128 lên 1024 dẫn đến cải thiện perplexity 0.2. Chúng tôi cũng điều chỉnh các ngoại lai cho các lớp down-projection, đảm bảo có nhiều hơn 3.5x lần so với các lớp tuyến tính khác, để phù hợp với kích thước đầu vào. Kết quả của chúng tôi cho thấy rằng việc sử dụng 256 ngoại lai đã là một lựa chọn tốt cho các thí nghiệm của chúng tôi. Việc sử dụng ngoại lai bổ sung không cải thiện độ chính xác đáng kể.

4.3.2 Nghiên cứu Trường hợp 2: Falcon-180B

Trong phần này, chúng tôi xem xét lại việc áp dụng QUIK cho Falcon-180B, mô hình kiểu GPT lớn nhất có sẵn công khai. Mô hình yêu cầu ≈365GB bộ nhớ GPU cho suy luận, làm cho việc chạy suy luận trên máy chủ GPU với 8x nút RTX3090 (192 GB bộ nhớ) trở nên không thể, minh họa tầm quan trọng của việc giảm dấu chân bộ nhớ của mô hình này.

Kết quả trong Bảng 2 và 5, và Hình 9 đã trình bày kết quả độ chính xác và hiệu suất cho mô hình này cho các biến thể QUIK. Ở đây, chúng tôi điều tra việc tận dụng định dạng thưa thớt 2:4 + INT4 được hỗ trợ bởi phần cứng bằng cách kết hợp QUIK với độ thưa thớt 2:4 cho mô hình này.

Lượng tử hóa INT-4 và Thưa thớt hóa 2:4 Kết hợp. Một giải pháp đơn giản để đẩy giới hạn của nén mô hình là thưa thớt hóa mô hình đã được lượng tử hóa (hoặc ngược lại). Tuy nhiên, điều này dẫn đến giảm độ chính xác cao. Thay vào đó, chúng tôi mở rộng thuật toán SparseGPT (Frantar & Alistarh, 2023) để hỗ trợ sơ đồ ngoại lai của chúng tôi để đồng thời lượng tử hóa và thưa thớt hóa mô hình, trong khi giữ các feature ngoại lai ở FP16 dày đặc. Trong Bảng 9, chúng tôi trình bày kết quả của việc lượng tử hóa tất cả các lớp, nhưng có chọn lọc giữ một số loại lớp dày đặc. Cụ thể, chúng tôi thấy rằng việc cắt tỉa một lần các trọng số trong các khối attention theo mẫu 2:4 trong tất cả các lớp phần lớn bảo tồn độ chính xác, dẫn đến lợi ích bộ nhớ nhỏ. Chúng tôi trình bày kết quả 8-bit trong cùng cài đặt trong Phụ lục E.

5 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Chúng tôi đã trình bày một sơ đồ lượng tử hóa hybrid được gọi là QUIK, thực thi đa số lớn tính toán suy luận ở độ chính xác 4-bit, với hỗ trợ GPU hiệu quả. Chúng tôi đã chỉ ra tăng tốc đáng kể sử dụng QUIK qua nhiều loại LLM, trên phần cứng thương mại. Trong công việc tương lai, chúng tôi dự định xem xét một triển khai thống nhất sẽ hỗ trợ cả suy luận một token và nhiều token trên trọng số QUIK, tích hợp với giải mã suy đoán (Leviathan et al., 2023), và các mô hình bổ sung.

--- TRANG 11 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

TÀI LIỆU THAM KHẢO

Boratko, M., Padigela, H., Mikkilineni, D., Yuvraj, P., Das, R., McCallum, A., Chang, M., Fokoue-Nkoutche, A., Kapanipathi, P., Mattei, N., et al. Một phân loại hệ thống về kiến thức, lý luận và bối cảnh trong tập dữ liệu ARC. arXiv preprint arXiv:1806.00358, 2018.

Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srinivasan, V., và Gopalakrishnan, K. Pact: Kích hoạt cắt được tham số hóa cho mạng nơ-ron được lượng tử hóa. arXiv preprint arXiv:1805.06085, 2018.

Dao, T., Fu, D. Y., Ermon, S., Rudra, A., và Ré, C. FlashAttention: Attention chính xác nhanh và hiệu quả bộ nhớ với nhận thức io. arXiv preprint arXiv:2205.14135, 2022.

Dettmers, T. và Zettlemoyer, L. Trường hợp cho độ chính xác 4-bit: luật thang đo suy luận k-bit. arXiv preprint arXiv:2212.09720, 2022.

Dettmers, T., Lewis, M., Belkada, Y., và Zettlemoyer, L. LLM.int8(): Phép nhân ma trận 8-bit cho transformer ở quy mô. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.

Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., và Alistarh, D. Spqr: Một biểu diễn thưa-lượng tử cho nén trọng số llm gần như không mất mát. arXiv preprint arXiv:2306.03078, 2023.

Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., và Modha, D. S. Lượng tử hóa kích thước bước đã học. arXiv preprint arXiv:1902.08153, 2019.

Frantar, E. và Alistarh, D. Sparsegpt: Các mô hình ngôn ngữ khổng lồ có thể được cắt tỉa chính xác trong một lần. 2023.

Frantar, E., Ashkboos, S., Hoefler, T., và Alistarh, D. Gptq: Lượng tử hóa sau huấn luyện chính xác cho transformer được huấn luyện trước sinh tạo. arXiv preprint arXiv:2210.17323, 2022.

Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: Một tập dữ liệu 800gb văn bản đa dạng cho mô hình hóa ngôn ngữ. arXiv preprint arXiv:2101.00027, 2020.

Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., et al. Một framework cho đánh giá mô hình ngôn ngữ few-shot. Phiên bản v0. 0.1. Sept, 2021.

Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., và Keutzer, K. Squeezellm: Lượng tử hóa dày đặc và thưa thớt. arXiv preprint arXiv:2306.07629, 2023.

Lee, C., Jin, J., Kim, T., Kim, H., và Park, E. Owq: Bài học rút ra từ ngoại lai kích hoạt cho lượng tử hóa trọng số trong mô hình ngôn ngữ lớn. arXiv preprint arXiv:2306.02272, 2023.

Leviathan, Y., Kalman, M., và Matias, Y. Suy luận nhanh từ transformer thông qua giải mã suy đoán. Trong International Conference on Machine Learning, pp. 19274–19286. PMLR, 2023.

Li, Q., Zhang, Y., Li, L., Yao, P., Zhang, B., Chu, X., Sun, Y., Du, L., và Xie, Y. Fptq: Lượng tử hóa sau huấn luyện tinh tế cho mô hình ngôn ngữ lớn. arXiv preprint arXiv:2308.15987, 2023.

Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., và Han, S. Awq: Lượng tử hóa trọng số nhận thức kích hoạt cho nén và tăng tốc llm. arXiv preprint arXiv:2306.00978, 2023.

Merity, S., Xiong, C., Bradbury, J., và Socher, R. Mô hình hỗn hợp sentinel con trỏ. arXiv preprint arXiv:1609.07843, 2016.

NVIDIA. Nvidia nsight compute. URL https://developer.nvidia.com/nsight-compute.

NVIDIA. Thư viện nvidia cutlass, 2023. URL https://github.com/NVIDIA/cutlass/.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: Một thư viện học sâu kiểu mệnh lệnh, hiệu suất cao. Advances in neural information processing systems, 32, 2019.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., và Sutskever, I. Mô hình ngôn ngữ là những người học đa nhiệm không giám sát. OpenAI blog, 1(8):9, 2019.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., và Liu, P. Khám phá giới hạn của học chuyển giao với một transformer văn bản-thành-văn bản thống nhất. Journal of Machine Learning Research, 21(140):1–67, 2020.

Sakaguchi, K., Bras, R. L., Bhagavatula, C., và Choi, Y. Winogrande: Một thách thức schema winograd đối kháng ở quy mô. Communications of the ACM, 64(9):99–106, 2021.

Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., và Luo, P. Omniquant: Lượng tử hóa được hiệu chuẩn toàn hướng cho mô hình ngôn ngữ lớn, 2023.

Tata, S. và Patel, J. M. PiQA: Một đại số để truy vấn tập dữ liệu protein. Trong International Conference on Scientific and Statistical Database Management, 2003.

--- TRANG 12 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

TII UAE. Họ mô hình ngôn ngữ lớn Falcon. https://huggingface.co/tiiuae, Tháng 5 2023.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Mô hình nền tảng mở và mô hình chat được tinh chỉnh. arXiv preprint arXiv:2307.09288, 2023.

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Transformer của huggingface: Xử lý ngôn ngữ tự nhiên hiện đại. arXiv preprint arXiv:1910.03771, 2019.

Xiao, G., Lin, J., Seznec, M., Demouth, J., và Han, S. Smoothquant: Lượng tử hóa sau huấn luyện chính xác và hiệu quả cho mô hình ngôn ngữ lớn. arXiv preprint arXiv:2211.10438, 2022.

Yuan, Z., Niu, L., Liu, J., Liu, W., Wang, X., Shang, Y., Sun, G., Wu, Q., Wu, J., và Wu, B. Rptq: Lượng tử hóa sau huấn luyện dựa trên sắp xếp lại cho mô hình ngôn ngữ lớn. arXiv preprint arXiv:2304.01089, 2023.

Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., và Choi, Y. Hellaswag: Một máy có thể thực sự hoàn thành câu của bạn không? arXiv preprint arXiv:1905.07830, 2019.

Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. OPT: Mô hình transformer ngôn ngữ được huấn luyện trước mở. arXiv preprint arXiv:2205.01068, 2022.

--- TRANG 13 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

A KẾT QUẢ ĐỘ CHÍNH XÁC OPT ĐẦY ĐỦ

Bảng 10 hiển thị kết quả perplexity của các mô hình OPT. Chúng tôi sử dụng lượng tử hóa đối xứng cho trọng số trong tất cả các thí nghiệm của chúng tôi. Kết quả gợi ý rằng trong cài đặt 4-bit, việc xem xét các feature ngoại lai là quan trọng để bảo tồn độ chính xác ngay cả trong các mô hình nhỏ (như OPT-1.3b). Chúng tôi lưu ý rằng 256 ngoại lai tương đương với 12.5% kích thước ẩn của mô hình 1.3B (và 2.77% của mô hình 66B).

[Bảng 10 - KẾT QUẢ PERPLEXITY CỦA QUIK-4B TRÊN CÁC MÔ HÌNH OPT KHÁC NHAU VỚI CÁC NGOẠI LAI KHÁC NHAU TRÊN BA TẬP DỮ LIỆU]

B KẾT QUẢ ĐỘ CHÍNH XÁC LLAMA-2 ĐẦY ĐỦ

Bảng 11 hiển thị perplexity của QUIK trên các mô hình LLaMA-2. Chúng tôi cung cấp một danh sách các mẹo để cải thiện chất lượng của mô hình mà không có quá nhiều chi phí. Chúng tôi thấy rằng việc giữ lớp down-proj ở 8 bit có thể cải thiện perplexity khoảng 3 điểm. Ngoài ra, chúng tôi thấy weight clipping như một mẹo rẻ và hiệu quả để cải thiện độ chính xác của QUIK-4B.

[Bảng 11 - KẾT QUẢ PERPLEXITY LLAMA-2 TRÊN WIKITEXT2 SỬ DỤNG 256 NGOẠI LAI]

C KẾT QUẢ ĐỘ CHÍNH XÁC INT-8 ĐẦY ĐỦ

Bảng 12 hiển thị so sánh QUIK-8B với SmoothQuant trên tập dữ liệu WikiText2. Chúng tôi sử dụng lượng tử hóa per-token (per-column) cho kích hoạt (trọng số) trong SmoothQuant và chỉ áp dụng lượng tử hóa trên các lớp tuyến tính (đây cũng là trường hợp cho QUIK). Chúng tôi loại trừ mô hình Falcon-7B vì mô hình này có một layer-norm duy nhất cho cả khối MLP và Attention và không rõ làm thế nào trọng số của FC1 và KQV sẽ được cập nhật trong thuật toán SmoothQuant.

D KẾT QUẢ ĐẦY ĐỦ ZERO-OUTLIER

Bảng 13 hiển thị kết quả của việc giữ số lượng lớp khác nhau mà không có ngoại lai cho các mô hình khác nhau.

--- TRANG 14 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

[Bảng 12 - KẾT QUẢ ĐỘ CHÍNH XÁC CHO CÁC MÔ HÌNH 8BIT TRÊN WIKITEXT2]

[Bảng 13 - NGHIÊN CỨU CÀI ĐẶT ZERO OUTLIER TRÊN WIKITEXT2 SỬ DỤNG 256 NGOẠI LAI]

E ĐỘ THƯA THỚT 2:4 + LƯỢNG TỬ HÓA INT8

Bảng 14 hiển thị kết quả độ chính xác của việc áp dụng QUIK-8B với độ thưa thớt 2:4 trên tất cả các mô hình. Kết quả gợi ý rằng sự giảm độ chính xác chính là từ việc giới thiệu độ thưa thớt 2:4 vào các ma trận trọng số và việc giữ một số lớp ở dạng dày đặc là quan trọng để bảo tồn độ chính xác (Xem phần 4.3.2).

[Bảng 14 - KẾT QUẢ ĐỘ CHÍNH XÁC WIKITEXT2 CHO VIỆC ÁP DỤNG ĐỘ THƯA THỚT 2:4 VỚI QUIK-8B]

F BENCHMARK HIỆU SUẤT FALCON

Chúng tôi cũng khám phá cải thiện hiệu suất của các mô hình Falcon (TII UAE, 2023). Máy 8xRTX3090 chứa khoảng 190GB bộ nhớ GPU không đủ để chạy suy luận mô hình fp16.

--- TRANG 15 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

[Hình 12 - TĂNG TỐC THEO LỚP TRÊN RTX3080 DUY NHẤT CHO CÁC KÍCH THƯỚC LỚP VÀ LOẠI NÉN KHÁC NHAU]

G HIỆU SUẤT TRÊN GPU RTX3080

Để xác thực hiệu suất của QUIK trong các loại GPU khác, chúng tôi tiến hành benchmark trên GPU RTX3080. Kết quả được trình bày trong Hình 12. Chúng ta có thể thấy rằng QUIK-4B vẫn có thể đạt được tăng tốc hơn 4x trên loại GPU khác.

H HIỆU SUẤT VỚI CÁC KÍCH THƯỚC CHUỖI KHÁC NHAU

Chúng tôi chủ yếu tập trung công việc của mình vào các trường hợp "prefill" với kích thước chuỗi lớn (trong tất cả các thí nghiệm của chúng tôi kích thước chuỗi bằng 2048). Trong phần này chúng tôi khám phá hiệu suất của QUIK-4B với các kích thước chuỗi đầu vào khác. Trong Hình 13(a) và 13(b) chúng tôi thay đổi kích thước đầu vào từ 1 đến 8k. Trong thí nghiệm đầu tiên (Hình. 13(a)) chúng tôi chạy benchmark theo lớp, trong thí nghiệm thứ hai (Hình 13(b)) chúng tôi chạy suy luận của một khối Transformer duy nhất (trên một GPU duy nhất). Chúng ta thấy rằng ở kích thước chuỗi đầu vào nhỏ QUIK chậm hơn đáng kể cho kích thước lớp và mô hình nhỏ hơn. Điều này có thể được giải thích bởi thực tế là lợi ích của phép nhân ma trận độ chính xác thấp ở quy mô này không thể bù đắp chi phí lượng tử hóa. Tuy nhiên, ở kích thước lớp và mô hình lớn QUIK có tăng tốc lên đến 2x ngay cả với đầu vào một token. Trong trường hợp các chuỗi đầu vào lớn chúng ta thấy rằng hiệu suất giảm có nghĩa là phép nhân ma trận độ chính xác thấp bão hòa ở quy mô này.

[Hình 13 - HIỆU SUẤT TƯƠNG ĐỐI CỦA QUIK-4B VỚI NGOẠI LAI CHO CÁC KÍCH THƯỚC CHUỖI KHÁC NHAU TRÊN GPU RTX3090]

I HIỆU SUẤT VỚI SỐ LƯỢNG NGOẠI LAI KHÁC NHAU

Trong phần này chúng tôi khám phá hiệu ứng của số lượng ngoại lai đối với hiệu suất QUIK. Hình 14 gợi ý rằng thời gian của QUIK matmul giữ nguyên qua tất cả các kích thước lớp cho tất cả số lượng ngoại lai khác không. Sự vượt trội của trường hợp zero ngoại lai có thể được giải thích bởi thực tế là nó không có phép nhân ma trận độ chính xác đầy đủ bổ sung và di chuyển dữ liệu đầu vào. Tuy nhiên, những kết quả này cho thấy rằng QUIK cho phép tăng số lượng ngoại lai mà không hy sinh hiệu suất, điều này rất quan trọng

--- TRANG 16 ---
QUIK: Hướng tới Suy luận 4-Bit Đầu cuối trên Các Mô hình Ngôn ngữ Lớn Sinh tạo

cho việc khôi phục độ chính xác, như chúng tôi đã thảo luận trong Phần 4.3.1.

[Hình 14 - KẾT QUẢ THỜI GIAN CHO CÁC KÍCH THƯỚC LỚP QUIK-4B KHÁC NHAU VỚI SỐ LƯỢNG NGOẠI LAI KHÁC NHAU TRÊN GPU RTX3090]
