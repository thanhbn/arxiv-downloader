# 2309.05210.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2309.05210.pdf
# Kích thước tệp: 113213 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2309.05210v3  [cs.CL]  17 Sep 2023Hiểu về Tác Động của Lượng Tử Hóa Sau Huấn Luyện đối với Các Mô Hình Ngôn Ngữ Lớn
Somnath Roy
Freshworks Inc
somnath.roy@freshworks.com
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đang tăng nhanh về kích thước, với số lượng tham số trở thành yếu tố quan trọng trong thành công của nhiều mô hình thương mại, như ChatGPT, Claude, và Bard. Ngay cả các mô hình có thể truy cập công khai được phát hành gần đây để sử dụng thương mại, như Falcon và Llama2, cũng được trang bị hàng tỷ tham số. Sự gia tăng đáng kể về số lượng tham số này khiến việc triển khai và vận hành trở nên rất tốn kém. Tiến bộ đáng kể trong lĩnh vực lượng tử hóa cho các mạng neural lớn nói chung và LLM nói riêng, đã làm cho các mô hình này trở nên dễ tiếp cận hơn bằng cách cho phép chúng được triển khai trên GPU cấp tiêu dùng. Các mô hình được lượng tử hóa thường thể hiện mức hiệu suất tương đương với các đối tác cơ bản chưa được lượng tử hóa. Tuy nhiên, vẫn tồn tại một khoảng cách đáng chú ý trong hiểu biết toàn diện của chúng ta về cách các mô hình được lượng tử hóa này phản ứng với các siêu tham số, như nhiệt độ, số token mới tối đa, và top k, đặc biệt là cho việc dự đoán từ tiếp theo.

Phân tích hiện tại cho thấy nf4 và fp4 là các kỹ thuật lượng tử hóa 4-bit có hiệu quả ngang nhau, được đặc trưng bởi các thuộc tính tương tự như tốc độ suy luận, tiêu thụ bộ nhớ, và chất lượng nội dung được tạo ra. Tuy nhiên, các phương pháp lượng tử hóa này thể hiện các hành vi khác biệt ở các cài đặt nhiệt độ khác nhau, cả trong bối cảnh của các mô hình nhỏ hơn và lớn hơn. Hơn nữa, nghiên cứu xác định nf4 có sự bền bỉ hơn đối với các biến đổi nhiệt độ trong trường hợp dòng mô hình llama2 ở nhiệt độ thấp, trong khi fp4 và fp4-dq tỏ ra là lựa chọn phù hợp hơn cho dòng mô hình falcon. Đáng chú ý là, nói chung, các mô hình được lượng tử hóa 4-bit với kích thước khác nhau thể hiện độ nhạy cảm cao hơn với nhiệt độ trong khoảng từ 0.5 đến 0.8, không giống như các đối tác chưa được lượng tử hóa. Ngoài ra, lượng tử hóa int8 liên quan đến tốc độ suy luận chậm hơn đáng kể, trong khi các mô hình bfloat16 chưa được lượng tử hóa liên tục mang lại tốc độ suy luận nhanh nhất trên các mô hình có mọi kích thước.

Thuật ngữ chỉ mục: lượng tử hóa sau huấn luyện, LLM, nf4, fp4, nf4-dq, fp4-dq

1. Giới thiệu
Với sự xuất hiện của kiến trúc Transformer [1], một bước đột phá đáng kể đã được đạt được, cho phép việc giữ lại hiệu quả các phụ thuộc tầm xa rộng lớn trong các tác vụ liên quan đến xử lý ngôn ngữ tự nhiên, lời nói và thị giác. Kiến trúc transformer cho phép huấn luyện song song cao do tính song song tuần tự, điều này làm cho việc tiền huấn luyện LLM với hàng trăm tỷ tham số trở nên khả thi [2, 3, 4]. Big-bench [5] đã giới thiệu hơn 200 bộ tiêu chuẩn được thiết kế để đánh giá khả năng của Các Mô hình Ngôn ngữ Lớn (LLM) thông qua định lượng và ngoại suy. Bộ tiêu chuẩn đa dạng và được xây dựng phức tạp này đã đóng góp đáng kể vào việc tăng cường cuộc đua xung quanh việc phát triển và tiến bộ của LLM.

Việc áp dụng rộng rãi LLM ở quy mô lớn đã trở nên phổ biến sau khi ChatGPT (bao gồm GPT-3 và các phiên bản tiếp theo) được thiết lập thành công [2]. Việc tiền huấn luyện các mô hình ngôn ngữ transformer lớn với 7 tỷ tham số trở lên đòi hỏi một lượng lớn tính toán GPU, có thể dẫn đến chi phí lên tới hàng triệu đô la. Mức chi tiêu như vậy vượt quá khả năng mà nghiên cứu học thuật và các tổ chức nhỏ thường có thể chi trả. Mặc dù chi phí cao của việc triển khai và vận hành các mô hình ngôn ngữ lớn (LLM), việc phát hành gần đây các mô hình Falcon [6] và Llama2 [7] đã khơi dậy sự lạc quan trong các tổ chức nhỏ và tăng mong muốn triển khai LLM tùy chỉnh của riêng họ.

Việc triển khai hiệu quả các LLM chỉ có bộ giải mã là thách thức trong thực tế vì quá trình suy luận tạo sinh diễn ra tuần tự, trong đó việc tính toán cho mỗi token phụ thuộc vào các token đã được tạo trước đó [8]. Đáng chú ý là việc lưu trữ các tensor khóa và giá trị chú ý của mỗi lớp có thể cải thiện đáng kể tốc độ suy luận của các mô hình chỉ có bộ giải mã nhỏ hơn phù hợp với bộ nhớ GPU đơn. Tuy nhiên, điều này không khả thi đối với các mô hình không phù hợp với bộ nhớ của một GPU đơn. Để giải quyết nhu cầu về GPU cao cấp đắt tiền để hỗ trợ việc triển khai các mô hình này, các hình thức lượng tử hóa đa dạng đã được đưa ra như các giải pháp tiềm năng. Việc áp dụng các phương pháp lượng tử hóa cho transformer xuất hiện như một cách tiếp cận hiệu quả để giảm thiểu độ trễ lấy mẫu, trong khi phát sinh tác động tối thiểu đến không đáng kể đối với hiệu suất tổng thể [9]. Các kỹ thuật lượng tử hóa có thể được đặc trưng chủ yếu thành ba hình thức cụ thể là - i) huấn luyện có nhận thức lượng tử hóa [10, 11], ii) tinh chỉnh có nhận thức lượng tử hóa [12, 13, 14], và iii) lượng tử hóa sau huấn luyện (PTQ) [15, 16, 17]. Trong [18], cuộc điều tra chủ yếu tập trung vào việc đánh giá tác động của các phương pháp lượng tử hóa sau huấn luyện đa dạng, sử dụng điểm perplexity như một tiêu chuẩn. Điểm perplexity được tính toán trên các tập dữ liệu như Wiki [19], PTB [20], và C4 [21], có khả năng đã được sử dụng như các tập dữ liệu nền tảng trong quá trình huấn luyện hầu hết các LLM. Cần lưu ý rằng các tập dữ liệu này có xu hướng thể hiện điểm perplexity thuận lợi trên tất cả các mô hình, do việc sử dụng chúng trong huấn luyện mô hình. Hơn nữa, được thừa nhận rằng perplexity, như một chỉ số, có thể không nắm bắt hiệu quả các trường hợp tạo sinh lặp lại trong LLM. Sau đây phác thảo các đóng góp chính của nghiên cứu hiện tại.

1. Nghiên cứu này cung cấp một kiểm tra có hệ thống về ảnh hưởng được thực hiện bởi ba siêu tham số quan trọng, cụ thể là, số token mới tối đa, nhiệt độ, và top k, trên LLM đã trải qua lượng tử hóa thông qua các kỹ thuật lượng tử hóa sau huấn luyện được áp dụng rộng rãi như [15]¹ (sau đây, gptq) và

¹https://github.com/IST-DASLab/gptq

--- TRANG 2 ---
[14, 12]²³ (sau đây, bitsandbytes).
2. Nó khám phá cách các siêu tham số này tác động của chúng trên một loạt kích thước mô hình, từ 3 tỷ đến 70 tỷ tham số.
3. Quá trình bao gồm việc tạo ra tổng cộng 6,300 mẫu cho mỗi phương pháp lượng tử hóa, đạt được bằng cách xây dựng mười prompt nhỏ hơn bao gồm một phổ đa dạng các lĩnh vực cho mỗi mô hình.
4. LLM thường thể hiện xu hướng hướng tới việc tạo sinh lặp lại, và thường khó nhận biết sự lặp lại như vậy thông qua điểm perplexity. Do đó, để xác định và định lượng việc tạo sinh lặp lại, chỉ số chính được sử dụng là số từ nội dung trùng lặp.
5. Nó xem xét kỹ lưỡng các phương pháp lượng tử hóa có cùng tốc độ suy luận tương tự nhưng thể hiện các tác động khác nhau đối với độ chính xác.
6. Cuối cùng, nó nhằm phân biệt phương pháp lượng tử hóa tối ưu cho việc triển khai, xem xét các ràng buộc và yêu cầu cụ thể.

2. Lượng tử hóa
Lượng tử hóa là một cơ chế được định nghĩa rõ ràng để giảm số bit được sử dụng để biểu diễn một giá trị. Trong bối cảnh của các mô hình mạng neural lớn, lượng tử hóa giảm độ chính xác của các tham số và/hoặc kích hoạt của mô hình. Hơn nữa, đã được phát hiện rằng các mô hình lớn được lượng tử hóa thường cạnh tranh với các mô hình cơ bản của chúng về độ chính xác trong khi giảm các yêu cầu tính toán.

Trong bối cảnh của LLM, quá trình lượng tử hóa có thể được chia thành hai loại cụ thể là i) mô phỏng và, ii) lượng tử hóa thuần túy. Trong lượng tử hóa mô phỏng, một số phép toán được thực hiện trong số học dấu phẩy động, điều này đòi hỏi việc khử lượng tử hóa các tham số được lượng tử hóa trở lại độ chính xác đầy đủ trong quá trình suy luận [22, 23, 24, 25]. Lượng tử hóa thuần túy sử dụng lượng tử hóa chỉ có số nguyên, loại bỏ nhu cầu khử lượng tử hóa trong quá trình suy luận [26, 27, 28, 12, 29]. Sự khác biệt chính giữa hai quá trình lượng tử hóa này được hiển thị dưới đây trong Bảng 1. 

Tính năng | Lượng tử hóa Mô phỏng | Lượng tử hóa Thuần túy
---|---|---
Phép toán | Dấu phẩy động và Điểm cố định | Điểm cố định
Nhu cầu khử lượng tử hóa | Có | Không
Tốc độ suy luận | Chậm hơn | Tương đối Nhanh hơn

Bảng 1: Hiểu biết chung về lượng tử hóa mô phỏng so với lượng tử hóa thuần túy trong LLM dựa trên transformer

Tuy nhiên, điều quan trọng cần lưu ý là lượng tử hóa thuần túy là một cách tiếp cận tích cực hơn và cũng có thể dẫn đến mất mát độ chính xác lớn hơn. Mặt khác, lượng tử hóa mô phỏng là một cách tiếp cận bảo thủ và có thể đạt được tăng tốc đáng kể mà không hy sinh quá nhiều độ chính xác. Lượng tử hóa thuần túy có thể được phân loại thêm thành W8A8 và W4A4, trong đó trọng số và kích hoạt được lượng tử hóa thành số nguyên 8-bit và số nguyên 4-bit, tương ứng [29] [27].

²https://github.com/TimDettmers/bitsandbytes
³https://github.com/artidoro/qlora

2.1. GPTQ
Đây là một phương pháp lượng tử hóa theo lớp dựa trên Optimal Brain Quantization (OBQ) [30]. Mục tiêu là tìm ma trận trọng số được lượng tử hóa W̃ minimizing sai số bình phương giữa đầu ra lớp được lượng tử hóa W̃X và đầu ra lớp độ chính xác đầy đủ WX như được hiển thị dưới đây.

argmin ||WX - W̃X||²
  W̃

Thuật toán OBQ lặp đi lặp lại lượng tử hóa từng trọng số một, trong khi thuật toán GPTQ sử dụng một triển khai vector hóa cho phép nó xử lý hiệu quả nhiều hàng của ma trận trọng số song song. Điều này làm cho GPTQ nhanh hơn đáng kể so với OBQ, đặc biệt là đối với các mô hình lớn.

2.1.1. Tiêu thụ Bộ nhớ GPU trong Lượng tử hóa GPTQ 4-bit
Đã được thiết lập rõ ràng rằng mục tiêu của lượng tử hóa là triển khai LLM trên GPU cấp tiêu dùng có tối đa 24 GB. Phân phối bộ nhớ GPU được sử dụng bởi các mô hình trong quá trình lượng tử hóa GPTQ 4-bit được hiển thị dưới đây trong Bảng 2. Lượng tử hóa GPTQ có các hạn chế sau.

• Đây là quá trình rất tốn bộ nhớ GPU.
• Ngay cả lượng tử hóa 4-bit của mô hình 40B cũng gây ra lỗi hết bộ nhớ (OOM) trên máy GPU A100 80GB. Hơn nữa, không thể lượng tử hóa các mô hình 7B trên máy GPU A10 24GB.

Mô hình | Bộ nhớ GPU(GB)
---|---
stablelm 3b | 19.54
redpajama 3b | 9.58
falcon 7b | 23.64
llama2 7b | 24.83
llama2 13b | 40.46
falcon 40b và llama2 70b | OOM trên GPU A100 80GB đơn

Bảng 2: Phân phối bộ nhớ GPU được tiêu thụ bởi lượng tử hóa GPTQ 4-bit cho các mô hình khác nhau được đánh giá trên máy GPU Nvidia A100 80GB

2.1.2. Lỗi Theo Lớp được tạo bởi GPTQ
Lượng tử hóa GPTQ 4-bit giảm kích thước của một mô hình hơn 80%, tức là, một mô hình 14 GB được giảm xuống khoảng 2 GB sau lượng tử hóa. Điều quan trọng cần lưu ý là lỗi lượng tử hóa được giới thiệu bởi GPTQ khác nhau đối với các mô hình khác nhau, như được hiển thị trong Bảng 3. Điều này là do các mô hình được hiển thị trong Bảng 3 có các kiến trúc khác nhau, bao gồm số lượng head, số lượng lớp, chiều embedding, số lượng nhóm truy vấn trong chú ý đa truy vấn, kích thước khối, và chiều ẩn.

2.2. Lượng tử hóa bitsandbytes
bitsandbytes (bnb) cung cấp triển khai của năm kỹ thuật lượng tử hóa mạnh mẽ và tiên tiến cụ thể là i) int8, ii) fp4, iii) nf4, iv) fp4-dq⁴, và v) nf4-dq. Quy trình lượng tử hóa int8 [14] sử dụng lượng tử hóa theo vector với các hằng số chuẩn hóa riêng biệt cho mỗi tích vô hướng trong phép nhân ma trận. Tuy nhiên, họ đã phát hiện khoảng 0.1% những

⁴dq viết tắt của double quantization

--- TRANG 3 ---
Mô hình | mlp.proj | att.proj | mlp.fc | attn.attn
---|---|---|---|---
stablelm 3b | 52850.7 | 12638.9 | 383200.9 | 844806.3
redpajama 3b | 23448.1 | 1048.9 | 137061.9 | 138947.9
falcon 7b | 19194.83 | 2362.39 | 149962.4 | 32886.3
llama2 7b | 22773.0 | 3198.7 | 170837.6 | 248520.0
llama2 13b | 27829.5 | 5470.5 | 247389.2 | 301002.0

Bảng 3: Lỗi lượng tử hóa được giới thiệu bởi GPTQ trong phép chiếu mlp, phép chiếu chú ý, kết nối đầy đủ và các lớp chú ý.

outlier kích hoạt chiếm ưu thế có tiềm năng làm giảm chất lượng đặc biệt là trong các LLM lớn hơn. Do đó, độ chính xác cho các outlier chiếm ưu thế này được giữ ở float16. Sơ đồ này cô lập các chiều đặc trưng outlier thành một phép nhân ma trận 16-bit, trong khi vẫn cho phép hơn 99.9% các giá trị được nhân ở 8 bit.

QLoRA [12] đã giới thiệu một kiểu dữ liệu mới được gọi là 4-bit normalfloat (nf4), tối ưu cho trọng số phân phối chuẩn, lượng tử hóa kép để giảm dấu chân bộ nhớ, và tối ưu hóa phân trang để quản lý các đỉnh bộ nhớ. Những kỹ thuật này cùng nhau mang lại tốc độ suy luận xuất sắc mà không hy sinh chất lượng tạo sinh. Trong lượng tử hóa nf4, trọng số mô hình cơ bản được lưu trữ ở kiểu dữ liệu nf4 và tính toán được thực hiện ở bfloat16. Tuy nhiên, trọng số mô hình được khử lượng tử hóa thành bfloat16 trong lần truyền tiến cho suy luận [31]. Các lượng tử hóa bnb nén dấu chân mô hình trong khoảng từ 40% (int8) đến 70% (nf4-dq). Điều quan trọng cần nhấn mạnh ở đây là lượng tử hóa int8 cho llama2 70B gây ra lỗi OOM trên máy GPU A100 80GB. Phần còn lại của các chi tiết về kích thước mô hình nén tương ứng với lượng tử hóa bnb được mô tả trong các phần sau.

3. Thí nghiệm
Phần này cung cấp mô tả chi tiết về các mô hình, prompt, cách tiếp cận giải mã, và các siêu tham số liên quan được sử dụng để tạo dữ liệu cho phân tích.

3.1. Mô tả Mô hình
Tổng cộng sáu mô hình được tiền huấn luyện với 3 tỷ đến 70 tỷ tham số được chọn cho dự đoán từ tiếp theo. Những mô hình này chỉ có bộ giải mã, và các chi tiết cụ thể về kiến trúc của chúng được hiển thị trong Bảng 4. Như có thể thấy, những mô hình này khác nhau về số lượng head, số lượng lớp, chiều embedding, số lượng nhóm truy vấn được sử dụng trong chú ý đa truy vấn, độ dài chuỗi, và kích thước trung gian.

3.2. Lựa chọn Prompt và Giả thuyết Đề xuất
Mười prompt được thiết kế để truy cập chất lượng và tốc độ suy luận của các mô hình được tiền huấn luyện cho việc tạo từ tiếp theo. Những prompt này được chọn dựa trên giả thuyết đề xuất đơn giản và được hiển thị dưới đây trong Bảng 5.

Giả thuyết 1: Tất cả các LLM được tiền huấn luyện trên hàng tỷ hoặc hàng nghìn tỷ token có thể được khái niệm hóa lý tưởng như một cây lớn, trong đó mỗi nút đại diện cho một chủ đề và các phần tiếp tục văn bản liên quan đến chủ đề đó. Khi chúng ta đi xuống cây, các phần tiếp tục văn bản trở nên cụ thể và tập trung hơn. Ngược lại, khi chúng ta đi lên cây, các phần tiếp tục văn bản trở nên tổng quát và trừu tượng hơn.

Giả thuyết 2: Chất lượng của một mô hình được tiền huấn luyện có thể được đánh giá dựa trên khả năng xác định chính xác nút chủ đề đúng và sau đó đi đến nút chủ đề phụ cho dự đoán từ tiếp theo tập trung.

3.3. Mô tả Bộ giải mã
Thí nghiệm hiện tại sử dụng một bộ giải mã lấy mẫu top k thuần túy mà không có bất kỳ tính năng bổ sung nào, như hình phạt lặp lại. Để đánh giá tiềm năng của mô hình, chúng tôi sử dụng một danh sách các token mới tối đa, nhiệt độ cũng như top k. Các token mới tối đa, nhiệt độ và top k lần lượt là [50, 100, 150, 200, 250, 300, 350, 400, 450, 500], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0] và [1, 5, 10, 20, 50, 100, 200]. Văn bản hoàn thành được tạo ra cho mọi mô hình được lượng tử hóa bằng cách sử dụng tất cả các kết hợp của token mới tối đa, nhiệt độ và top k. Lý do cho top k cao như 200 là nó có thể cho phép các mô hình chọn văn bản đa dạng hơn, ít lặp lại hơn và nhất quán về mặt ngữ nghĩa.

4. Phân tích
Tổng cộng 6300 (10 prompt × 10 token mới tối đa × 9 nhiệt độ × 7 top k) văn bản hoàn thành được tạo ra cho mỗi mô hình được lượng tử hóa ngoại trừ falcon 40b và llama2 70b cho 16bit⁵. Việc đánh giá những văn bản hoàn thành này được thực hiện thông qua tính toán đếm số từ nội dung trùng lặp, phục vụ như một chỉ số để đánh giá chất lượng của văn bản được tạo ra. Các từ nội dung là những từ còn lại sau khi loại bỏ các từ dừng. Ngoài ra, kích thước của mô hình tính bằng gigabyte (GB) phục vụ như một thước đo chính để định lượng tiêu thụ bộ nhớ GPU, trong khi token/giây được sử dụng như một chỉ số để đo tốc độ suy luận của mô hình.

4.1. Tiêu thụ Bộ nhớ và Tốc độ Suy luận
Việc sử dụng lượng tử hóa int8 thể hiện việc giảm đáng kể về tiêu thụ bộ nhớ, khoảng trong phạm vi từ 40% đến 50%, so với bfloat16, như được minh họa trong Bảng 6. Tuy nhiên, điều quan trọng cần lưu ý là sự cải thiện này đi kèm với một sự đánh đổi tương ứng về tốc độ suy luận, với int8 thể hiện sự chậm lại khoảng 75% đến 80% so với bfloat16, như được chỉ ra trong Bảng 7.

Khi đánh giá tiêu thụ bộ nhớ giữa các cách tiếp cận lượng tử hóa fp4 và nf4 cho kích thước mô hình lên đến 13 tỷ, sự khác biệt của chúng là không đáng kể. Tuy nhiên, lượng tử hóa nf4 thể hiện một lợi thế nhỏ so với fp4 về tiêu thụ bộ nhớ đối với các mô hình lớn hơn như falcon 40b và llama2 70b. Tuy nhiên, fp4-dq được tìm thấy là tốt hơn về tiêu thụ bộ nhớ (tức là, tiêu tốn ít bộ nhớ hơn) trên các mô hình so với đối tác của nó, như được hiển thị trong Bảng 6. Đáng chú ý là trong khi lượng tử hóa kép mang lại lợi thế rõ ràng về tiêu thụ bộ nhớ, nó dẫn đến việc giảm tốc độ suy luận khoảng 10% đến 25% so với việc không có lượng tử hóa kép, như được nêu trong Bảng 7.

Kết luận, trong số các phương pháp lượng tử hóa khác nhau,

⁵Cả falcon 40b và llama2 70b đều gặp lỗi OOM trên máy GPU 80GB. falcon 40b gây ra lỗi OOM khi số token mới tối đa vượt quá 400, trong khi llama2 70b gặp lỗi OOM trong quá trình tải.

--- TRANG 4 ---
Mô hình | nhead | nlayer | embed dim | nquery groups | mqa | block size | intermediate size
---|---|---|---|---|---|---|---
stablelm 3b | 32 | 16 | 4096 | 32 | | 4096 | 16384
redpajama 3b | 32 | 32 | 2560 | 32 | | 2048 | 10240
falcon 7b | 71 | 32 | 4544 | 1 | | 2048 | 18176
llama2 7b | 32 | 32 | 4096 | 32 | | 4096 | 11008
llama2 13b | 40 | 40 | 4096 | 32 | | 5120 | 13824
falcon 40b | 128 | 60 | 8192 | 8 | | 2048 | 32768
llama2 70b | 64 | 80 | 8192 | 8 | | 5120 | 28672

Bảng 4: Mô tả các thông số kiến trúc liên quan nhất của các mô hình được tiền huấn luyện sử dụng trong thí nghiệm

Prompt | Tổng quát | Tiếp tục Mong đợi
---|---|---
Life in London | Du lịch/Văn hóa/Liên quan đến Công việc/Những thứ cụ thể về London
It is easy to be a techie | So sánh của techie với các vai trò có thể có khác trong lĩnh vực công nghệ
Stock brokers are earning | Nhà môi giới chứng khoán và phong cách kiếm tiền, nguồn của họ, v.v.
It looks like written by Shakespeare | So sánh văn bản theo phong cách Shakespeare
Hello, my name is | Trò chuyện hoặc Giới thiệu
Global warming and AI | Nóng lên toàn cầu và AI nói chung cũng như mối liên hệ tích cực và tiêu cực của chúng
Current world order | Tiểu luận/Thảo luận/Quyền lực và Chính trị liên quan đến trật tự thế giới
Percentage of people adore actors and singers | Thống kê về những người theo dõi diễn viên/ca sĩ yêu thích của họ và thảo luận về chủ đề liên quan
Exercise and eating habits for | Thói quen ăn uống và thói quen tập thể dục nói chung (ưu và nhược điểm)
Millennial and genz | So sánh và đối chiếu giữa thế hệ millennial và genz

Bảng 5: Mô tả Prompt

bfloat16 nổi bật là kém hiệu quả nhất về tiêu thụ bộ nhớ. Tuy nhiên, nó xuất sắc về tốc độ suy luận, ngoại trừ trong trường hợp của stablelm 3b.

4.2. Nhiệt độ vs. Chất lượng Tạo sinh
Một mẫu hình chung xuất hiện trong tất cả các cách tiếp cận lượng tử hóa, trong đó sự gia tăng nhiệt độ tương quan với sự nâng cao số từ nội dung trùng lặp ngoại trừ bfloat16. Tuy nhiên, đáng chú ý là một số mô hình nhạy cảm hơn với nhiệt độ thấp hơn 0.5 so với những mô hình khác.

Khi so sánh hiệu suất của các mô hình stablelm 3b và redpajama 3b, rõ ràng là các phương pháp lượng tử hóa fp4 và nf4-dq thể hiện kết quả không tối ưu, được đặc trưng bởi sự gia tăng xuất hiện từ trùng lặp ở cài đặt nhiệt độ thấp hơn. Tuy nhiên, tình huống thay đổi khi xem xét các mô hình falcon, trong đó lượng tử hóa nf4 liên tục thể hiện hiệu suất kém hơn trên toàn bộ phổ nhiệt độ so với các phương pháp lượng tử hóa khác.

Trái lại, khi đánh giá các mô hình llama2, tình huống trở nên phức tạp hơn, với hầu hết các cách tiếp cận lượng tử hóa đóng góp đáng kể vào việc tạo sinh lặp lại. Trong bối cảnh này, việc xác định một người dẫn đầu rõ ràng trong số các phương pháp này tỏ ra là một nhiệm vụ thách thức. Tuy nhiên, đáng chú ý là đối với mô hình llama2 70b, cả hai phương pháp lượng tử hóa fp4 và fp4-dq đều vượt trội hơn những phương pháp khác về hiệu suất.

Phân tích cho thấy mô hình được lượng tử hóa int8 thể hiện kiểm soát hiệu quả đối với sự xuất hiện của từ nội dung trùng lặp cho cả llama2 13b và llama2 70b, hiệu quả giới hạn chúng trong phạm vi 40. Trái lại, các mô hình bfloat16 thể hiện đặc tính độc lập với việc thay đổi tỷ lệ nhiệt độ, vì chúng liên tục tạo ra số lượng từ lặp lại tương đương trên tất cả các cài đặt nhiệt độ ngoại trừ redpajama 3b.

4.3. Token Trả về Tối đa vs. Chất lượng Tạo sinh
Thuật ngữ token trả về tối đa bao gồm giá trị kết hợp của token mới tối đa và độ dài của prompt đầu vào tính theo token. Phân tích cho thấy số lượng từ trùng lặp được tạo ra tăng tuyến tính với sự gia tăng của token trả về tối đa trên tất cả các mô hình và phương pháp lượng tử hóa.

4.4. Top k vs. Chất lượng Tạo sinh
Phân tích đưa ra một cái nhìn có phần bất ngờ, chỉ ra rằng việc đặt top k bằng 1 có xu hướng dẫn đến sự xuất hiện thấp nhất của từ trùng lặp trên các mô hình và phương pháp lượng tử hóa. Tuy nhiên, đáng chú ý là hiệu ứng này đạt đến điểm bão hòa và mất tính khác biệt khi top k bằng hoặc lớn hơn 5.

4.5. So sánh Tổng thể
Về số từ nội dung trùng lặp trung bình⁶ được tạo ra theo nghĩa tuyệt đối, phân tích của chúng tôi tiết lộ những cái nhìn sau:

• Đối với fp4 và fp4-dq so với nf4 và nf4-dq trên các mô hình khác nhau (ngoại trừ dòng llama2), có sự giảm thiểu nhất quán trong việc tạo sinh lặp lại, thường dao động từ 12% đến 20% tương đối.

• Trong trường hợp nf4 và nf4-dq cho các mô hình llama2 có kích thước khác nhau, có một lợi thế đáng chú ý hơn, với sự giảm thiểu tương đối từ 9% đến 11% trong việc tạo sinh lặp lại.

• Lượng tử hóa Int8 có một hạn chế rõ ràng hơn về số từ được tạo ra, sản xuất khoảng 30-50% ít từ nội dung hơn so với lượng tử hóa 4-bit. Ngoài ra, nó tạo ra 25-40% từ nội dung trùng lặp nhiều hơn tương đối so với lượng tử hóa 4-bit ở quy mô chuẩn hóa.

• Khi so sánh bfloat16 với lượng tử hóa 4-bit, đáng chú ý là bfloat16 thường tạo ra nhiều từ nội dung hơn, thường dao động từ khoảng 3% đến 10%. Tuy nhiên, bfloat16 có xu hướng tạo ra số từ trùng lặp cao hơn một chút, cho thấy sự kém cỏi tương đối từ 1% đến 3.5% với lượng tử hóa 4bit.

Việc tính toán điểm perplexity trung bình, với bước token là 512, được thực hiện cho tất cả các mức lượng tử hóa trên mỗi mô hình. Một kiểm tra các điểm số này cho thấy các giá trị perplexity cho tất cả các mô hình nằm trong một phạm vi tương đối hạn chế, thường dao động từ 12 đến 15. Do đó, được nhận biết rằng perplexity, trong bối cảnh này, có thể không phục vụ như một chỉ số phù hợp để đánh giá chất lượng của văn bản được tạo ra.

5. Kết luận
Trong các tình huống mà bộ nhớ GPU không phải là yếu tố hạn chế và ưu tiên hàng đầu được đặt vào việc đạt được cả tốc độ suy luận cao và độ chính xác, nên ưu tiên việc sử dụng bfloat16 cho các mô hình lên đến 7 tỷ. Sở thích này phát sinh do tính nhạy cảm giảm đối với các biến đổi nhiệt độ và token mới tối đa. Hơn nữa, mô hình lên đến kích thước 7 tỷ hiệu quả phù hợp với máy GPU cấp tiêu dùng. Thay vào đó, nf4 và fp4 phục vụ như lựa chọn mặc định cho những cá nhân tìm kiếm sự cân bằng giữa việc sử dụng GPU, độ chính xác và tốc độ suy luận, do đó cung cấp một giải pháp trung gian kết hợp tất cả các khía cạnh một cách hiệu quả.

Đáng chú ý là việc áp dụng lượng tử hóa kép, như fp4-dq và nf4-dq, có thể dẫn đến việc giảm thiểu nhỏ trong dấu chân bộ nhớ. Tuy nhiên, nó đi kèm với tốc độ suy luận giảm tương đối. Do đó, khuyến nghị nghiêng về việc sử dụng lượng tử hóa mà không có cách tiếp cận nhân đôi. Ngoài ra, khi xem xét sự kết hợp độ chính xác nf4 và fp4, khuyến nghị sử dụng nhiệt độ nhỏ hơn 0.5, chính xác 1.0, hoặc sự kết hợp của các giá trị này để đạt được hiệu suất tối ưu.

Đánh giá hiện tại không xem xét int8 là một lựa chọn thay thế khả thi cho các phương pháp lượng tử hóa khác. Trong khi int8 giảm việc sử dụng bộ nhớ, nó làm chậm đáng kể việc suy luận và tạo ra khoảng 30-50% ít từ hơn so với các phương pháp lượng tử hóa khác.

Điều quan trọng cần lưu ý là thí nghiệm hiện tại không đạt được kết quả thỏa đáng về độ chính xác và tốc độ suy luận khi sử dụng lượng tử hóa gptq 4-bit. Cần điều tra thêm để tái tạo hiệu suất tương đương đã được báo cáo trong các nghiên cứu khác⁷. Do đó, kết quả này không được bao gồm trong phân tích được trình bày.

6. Hạn chế và Công việc Tương lai
Nghiên cứu hiện tại được thực hiện trên 7 mô hình có kích thước từ 3 tỷ đến 70 tỷ tham số, và 10 prompt được sử dụng cho dự đoán từ tiếp theo bằng cách sử dụng các kết hợp khác nhau của siêu tham số. Nghiên cứu thêm với nhiều mô hình hơn (chứa ≤1 tỷ tham số) và prompt có thể cung cấp nhiều cái nhìn sâu sắc hơn về tác động của những siêu tham số này đối với LLM được lượng tử hóa tương đối nhỏ hơn.

Công việc tương lai sẽ tập trung vào các nguyên nhân chính của việc tạo sinh lặp lại và mối quan hệ của chúng với Giả thuyết 1 và Giả thuyết 2. Hơn nữa, kết quả cho thấy falcon có tốc độ suy luận nhanh hơn llama2 trong danh mục 7B. Tuy nhiên, falcon có số lượng tham số tổng thể cao hơn llama2. Do đó, nghiên cứu tương lai sẽ tập trung vào các yếu tố cụ thể của mô hình ảnh hưởng đến tốc độ suy luận.

7. Tài liệu tham khảo
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.

[2] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.

[3] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.

⁷https://github.com/PanQiWei/AutoGPTQ

--- TRANG 5 ---
Mô hình | bnb.nf4 | bnb.nf4-dq | bnb.fp4 | bnb.f4-dq | bnb.int8 | bfloat16
---|---|---|---|---|---|---
stablelm 3b | 3.22 | 3.20 | 3.22 | 3.06 | 4.68 | 7.42
redpajama 3b | 2.31 | 2.17 | 2.31 | 2.17 | 3.52 | 5.60
falcon 7b | 5.72 | 5.37 | 5.72 | 5.37 | 8.71 | 14.50
llama2 7b | 4.58 | 4.27 | 4.58 | 4.27 | 7.82 | 13.53
llama2 13b | 8.83 | 7.8 | 8.83 | 7.8 | 14.2 | 26.23
falcon 40b | 26.40 | 24.64 | 26.55 | 24.64 | 44.52 | 80.85
llama2 70b | 40.23 | 38.2 | 40.4 | 38.2 | 70.44 | -

Bảng 6: Phân phối bộ nhớ được tiêu thụ (thấp hơn là tốt hơn) cho tất cả các mô hình cho lượng tử hóa khác nhau được đánh giá trên máy GPU Nvidia A100 80GB.

Mô hình | bnb.nf4 | bnb.nf4-dq | bnb.fp4 | bnb.f4-dq | bnb.int8 | bfloat 16
---|---|---|---|---|---|---
stablelm 3b | (37.76, 62.79) | (38.7, 53.37) | (42.99, 63.11) | (38.7, 53.03) | (7.91, 16.81) | (37.76, 49.88)
redpajama 3b | (24.2, 32.29) | (22.59, 27.04) | (25.64, 31.37) | (15.49, 27.08) | (2.52, 3.24) | (29.35, 37.85)
falcon 7b | (29.09, 37.54) | (22.71, 30.04) | (24.77, 37.41) | (22.23, 30.7) | (3.13, 12.63) | (35.79, 48.05)
llama2 7b | (23.09, 29.88) | (19.32, 23.44) | (23.01, 28.65) | (17.85, 23.41) | (1.32, 8.87) | (28.39, 36.35)
llama2 13b | (15.9, 23.14) | (13.22, 18.84) | (12.0, 22.98) | (10.83, 18.22) | (6.49, 7.14) | (24.12, 29.34)
falcon 40b | (11.93, 16.59) | (11.57, 14.51) | (12.12, 16.61) | (10.42, 12.76) | (3.56, 4.63) | (12.37, 13.99)
llama2 70b | (8.67, 10.39) | (6.47, 9.07) | (8.52, 10.23) | (6.39, 8.82) | (2.79, 3.76) | -

Bảng 7: Phân phối tốc độ suy luận tối thiểu và tối đa (cao hơn là tốt hơn) tính theo token/giây cho lượng tử hóa khác nhau được đánh giá trên máy GPU Nvidia A100 80GB.

[4] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti et al., "Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model," arXiv preprint arXiv:2201.11990, 2022.

[5] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso et al., "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models," arXiv preprint arXiv:2206.04615, 2022.

[6] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, và J. Launay, "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only," arXiv preprint arXiv:2306.01116, 2023.

[7] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.

[8] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, và J. Dean, "Efficiently scaling transformer inference," Proceedings of Machine Learning and Systems, vol. 5, 2023.

[9] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, và J. Jumper, "Accelerating large language model decoding with speculative sampling," arXiv preprint arXiv:2302.01318, 2023.

[10] G. Yang, D. Lo, R. Mullins, và Y. Zhao, "Dynamic stashing quantization for efficient transformer training," arXiv preprint arXiv:2303.05295, 2023.

[11] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, và V. Chandra, "Llm-qat: Data-free quantization aware training for large language models," arXiv preprint arXiv:2305.17888, 2023.

[12] T. Dettmers, A. Pagnoni, A. Holtzman, và L. Zettlemoyer, "Qlora: Efficient finetuning of quantized llms," arXiv preprint arXiv:2305.14314, 2023.

[13] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-W. Ha, N. Sung, và D. Lee, "Alphatuning: Quantization-aware parameter-efficient adaptation of large-scale pre-trained language models," arXiv preprint arXiv:2210.03858, 2022.

[14] T. Dettmers, M. Lewis, Y. Belkada, và L. Zettlemoyer, "Llm.int8(): 8-bit matrix multiplication for transformers at scale," arXiv preprint arXiv:2208.07339, 2022.

[15] E. Frantar, S. Ashkboos, T. Hoefler, và D. Alistarh, "Gptq: Accurate post-training quantization for generative pre-trained transformers," arXiv preprint arXiv:2210.17323, 2022.

[16] Z. Yuan, L. Niu, J. Liu, W. Liu, X. Wang, Y. Shang, G. Sun, Q. Wu, J. Wu, và B. Wu, "Rptq: Reorder-based post-training quantization for large language models," arXiv preprint arXiv:2304.01089, 2023.

[17] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, và S. Han, "Awq: Activation-aware weight quantization for llm compression and acceleration," arXiv preprint arXiv:2306.00978, 2023.

[18] Z. Yao, C. Li, X. Wu, S. Youn, và Y. He, "A comprehensive study on post-training quantization for large language models," arXiv preprint arXiv:2303.08302, 2023.

[19] S. Merity, C. Xiong, J. Bradbury, và R. Socher, "Pointer sentinel mixture models," arXiv preprint arXiv:1609.07843, 2016.

[20] M. Marcus, B. Santorini, và M. A. Marcinkiewicz, "Building a large annotated corpus of english: The penn treebank," 1993.

[21] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, và P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485–5551, 2020.

[22] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney, và K. Keutzer, "Q-bert: Hessian based ultra low precision quantization of bert," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, 2020, pp. 8815–8821.

--- TRANG 6 ---
[23] A. H. Zadeh, I. Edo, O. M. Awad, và A. Moshovos, "Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference," in 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEE, 2020, pp. 811–824.

[24] H. Bai, W. Zhang, L. Hou, L. Shang, J. Jin, X. Jiang, Q. Liu, M. Lyu, và I. King, "Binarybert: Pushing the limit of bert quantization," arXiv preprint arXiv:2012.15701, 2020.

[25] W. Zhang, L. Hou, Y. Yin, L. Shang, X. Chen, X. Jiang, và Q. Liu, "Ternarybert: Distillation-aware ultra-low bit bert," arXiv preprint arXiv:2009.12812, 2020.

[26] S. Kim, A. Gholami, Z. Yao, M. W. Mahoney, và K. Keutzer, "I-bert: Integer-only bert quantization," in International conference on machine learning. PMLR, 2021, pp. 5506–5518.

[27] Z. Yao, C. Li, X. Wu, S. Youn, và Y. He, "A comprehensive study on post-training quantization for large language models," arXiv preprint arXiv:2303.08302, 2023.

[28] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, và S. Han, "Smoothquant: Accurate and efficient post-training quantization for large language models," in International Conference on Machine Learning. PMLR, 2023, pp. 38087–38099.

[29] X. Wu, C. Li, R. Y. Aminabadi, Z. Yao, và Y. He, "Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases," arXiv preprint arXiv:2301.12017, 2023.

[30] E. Frantar và D. Alistarh, "Optimal brain compression: A framework for accurate post-training quantization and pruning," Advances in Neural Information Processing Systems, vol. 35, pp. 4475–4488, 2022.

[31] Y. Belkada, T. Dettmers, A. Pagnoni, S. Gugger, và S. Mangrulkar, "Making llms even more accessible with bitsandbytes, 4-bit quantization and qlora," 2023. [Online]. Available: https://huggingface.co/blog/4bit-transformers-bitsandbytes
