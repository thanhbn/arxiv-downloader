# 2310.18313.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2310.18313.pdf
# File size: 980564 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FP8-LM: Training FP8 Large Language Models
Houwen Peng∗Kan Wu∗Yixuan Wei∗
Guoshuai Zhao Yuxiang Yang Ze Liu Yifan Xiong Ziyue Yang
Bolin Ni Jingcheng Hu Ruihang Li Miaosen Zhang Chen Li Jia Ning Ruizhe Wang Zheng Zhang
Shuguang Liu Joe Chau Han Hu†Peng Cheng†
Microsoft Azure and Microsoft Research
Abstract
In this paper, we explore FP8 low-bit data formats for efficient training of large language
models (LLMs). Our key insight is that most variables, such as gradients and optimizer
states,inLLMtrainingcanemploylow-precisiondataformatswithoutcompromisingmodel
accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8
automatic mixed-precision framework for training LLMs. This framework offers three levels
ofFP8utilizationto streamlinemixed-precisionanddistributedparalleltrainingfor LLMs.
It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an
incremental manner. Experiment results show that, during the training of GPT-175B model
onH100GPUplatform,ourFP8mixed-precisiontrainingframeworknotonlyachieveda
remarkable 39% reduction in real memory usage but also ran 75% faster than the widely
adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer
Engine by 37%. This largely reduces the training costs for large foundation models. Fur-
thermore,ourFP8mixed-precisiontrainingmethodologyisgeneric. Itcanbeseamlessly
applied to other tasks such as LLM instruction tuning and reinforcement learning with
human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training
framework is open-sourced at aka.ms/MS.AMP.
1 Introduction
Largelanguagemodels(LLMs)(Brownetal.,2020;Smithetal.,2022;Chowdheryetal.,2022; Zhangetal.,
2022) have demonstrated unprecedented capabilities in language comprehension and generation, leading to
breakthroughs in reasoning, math, science, and many other tasks (OpenAI, 2023; Anil et al., 2023). However,
training LLMs is extremely costly. For example, PaLM takes 6,144 TPUv4 chips to train a 540B model, while
GPT-3175Bconsumesseveralthousandpetaflop/s-daysofcomputeforpre-training(Chowdheryetal.,2022;
Brownetal.,2020). ThismotivatestheneedsofreducingthetrainingcostsofLLMs,especiallyforthescaling
of next-generation super-intelligent models.
Low-precision training is one of the most promising directions to reduce the costs, as it can provide high
speed, small memory footprint, and low communication overhead. Most existing training systems, e.g.,
Megatron-LM(Shoeybietal.,2019),MetaSeq(Zhangetal.,2022),andColossal-AI(Lietal.,2023a),train
LLMswitheitherFP32full-precisionorFP16/BF16mixed-precisionbydefault. Thisisnotessential,however,
Contributions for all the authors can be found in Section 5.
* equal work†contact: {hanhu | pengc}@microsoft.comarXiv:2310.18313v2  [cs.LG]  19 Dec 2023

--- PAGE 2 ---
0 200 400 600 800 1000
Model Size (B)8163264128Number of GPUsBF16
Our FP8Figure 1: An analysis of comparing the maximum model sizes attainable through the utilization of either the
prevalent BF16 or our FP8 mixed-precision training approach on a cluster of Nvidia H100 GPUs with 80GB
memory.
toachievefullaccuracyforlargemodels. WiththereleaseofNvidiaH100GPU,FP8isbecomingthenext-
generation datatype for low-precision representation (Nvidia, 2022a; Micikevicius et al., 2022). Theoretically,
FP8 can achieve 2×speed-up, 50% - 75% memory cost savings, and 50% - 75% communication savings
compared with current 16-bit and 32-bit floating point mixed-precision training, which is very promising for
scaling-up next-generation foundation models.
Unfortunately, the current support for FP8 training is rare and limited. The only usable framework is the
Nvidia Transformer Engine (TE) (Nvidia, 2022b), but it applies FP8 solely for GEMM computation and still
retains master weights and gradients using high precision, e.g., FP16 or FP32. As a result, the end-to-end
speed-up, memory and communication cost savings are very limited, which does not fully unveil the power
of FP8. To address this issue, we propose an extremely optimized FP8 mixed-precision framework for LLM
training. The core idea is to infiltrate FP8 compute, storage, and communication into the whole progress of
largemodeltraining,makingtheforwardandbackwardpassallusedthelow-precisionFP8,thuslargely
reducing system workloads compared to previous frameworks (Micikevicius et al., 2017; Nvidia, 2022b;
Micikeviciusetal.,2022). Specifically,wedesignthreeoptimizationlevelsthatutilizeFP8tostreamlinemixed-
precisionanddistributedtraining. Thethreelevelsgraduallyincorporate8-bitcollectivecommunication,
optimizer,anddistributedparalleltraininginanincrementalmanner. Thehigheroptimizationlevelindicates
using more FP8 during LLM training. Moreover, for large-scale training, such as GPT-175B trained on
thousandofGPUs,ourframeworkprovidesFP8low-bitparallelism,includingtensor,pipeline,andsequence
parallelism, paving the way to next-generation low-precision parallel training.
Training LLMs with FP8 is non-trivial. The challenges stem from issues such as data underflow or overflow,
coupled with quantization errors arising from the narrower dynamic range and reduced precision inherent
in FP8 data formats. These challenges cause numerical instabilities and irreversible divergences throughout
thetrainingprocess. Totacklethem,weproposetwotechniques: precisiondecoupling andautomaticscaling for
preventingthelossofcriticalinformation. Theformeroneinvolvesdecouplingtheinfluenceofdataprecision
on parameters such as weights, gradients, optimizer states, and assigning reduced precision to components
that are not precision sensitive. The latter one is to preserve gradient values within the representation range
of FP8 data formats through the dynamic adjustment of tensor scaling factors, thereby alleviating underflow
and overflow occurrences during all-reduce communication.
TovalidatetheproposedFP8low-precisionframework,weapplyittoGPT-stylemodeltraining,encompassing
both pre-training and supervised fine-tuning (SFT). The experimental results demonstrate the effectiveness
of our FP8 methodology, yielding substantial benefits including a 29% to 39% reduction in real memory
usage (e.g., 29% reduction for GPT-7B while 39% for GPT-175B ) and a notable 63% to 65% decrease in
weight-related communication overhead compared to the prevalent BF16 mixed-precision training approach.
Without changes to any hyper-parameters, such as learning rate and weight decay, the models trained using
FP8 exhibit performance equivalency to those employing BF16 high precision, both in pre-training and
2

--- PAGE 3 ---
downstream tasks. It is noteworthy that during the training of GPT-175B model, our FP8 mix-precision
frameworkreducestrainingtimeby37%comparedtoTE(Nvidia,2022b),whileconsuming42%lessmemory
onH100GPUplatform. Moreimportantly,thereductionincostsachievedthroughtheutilizationoflow-
precision FP8 can be further increased, as the scale of models continues to expand, which is presented in
Fig. 1.
For fine-tuning, we employ FP8 mixed-precision for instruction tuning and reinforcement learning with
human feedback (RLHF) to better align pre-trained LLMs with end tasks and user preferences. Specifically,
we fine-tune pre-trainedmodelson publicly user-shared instruction-following data (ShareGPT,2023). The
models tuned with our FP8 mixed-precision demonstrate comparable performance to those utilizing the
half-precision BF16 (Zheng et al., 2023) on the AlpacaEval (Li et al., 2023b) and MT-Bench (Zheng et al.,
2023)benchmarks,whileachieving27%improvementsintrainingspeed. Moreover,FP8mixed-precision
exhibitsconsiderablepotentialsinRLHF,aprocessthatnecessitatesloadingmultiplemodelsduringtraining.
Through the utilization of FP8 in training, the prevalent RLHF framework AlpacaFarm (Dubois et al., 2023)
can yield a 32% reduction in model weights and a 62% reduction in optimizer states’ memory consumption.
This further demonstrates the versatility and adaptability of our FP8 low-precision training framework.
We are making the following contributions to drive the design of next-generation FP8 low-precision training
for LLMs.
•A new FP8 mixed-precision training framework. It unlocks 8-bit weights, gradients, optimizer, and
distributedtraininggraduallyinanadd-onfashion,whichisconvenientinuse. This8-bitframework
can be used as a simple drop-in replacement for existing 16/32-bit mixed-precision counterparts,
without requiring any changes to the hyper-parameters and training receipts. Additionally, we
provide a Pytorch implementation that enables 8-bit low-precision training in a few lines of code.
•A new family of GPT-style models trained with FP8. We apply the proposed FP8 scheme to GPT pre-
training and fine-tuning ( i.e., SFT and RLHF), and demonstrate its potentials on a variety of model
scales ranging from 7B to 175B parameters. We equip prevalent parallel computation paradigms
with FP8 supports, including tensor, pipeline, and sequence parallelisms, enabling the utilization of
FP8 to train large foundation models. We open-source the first FP8 GPT training codebase based
upon Megatron-LM (Shoeybi et al., 2019) implementation.
We expect the release of our FP8 framework will establish a new paradigm for next-generation low-precision
training system dedicated to large foundation models.
2 FP8 LLMs
Mixed-precision (Micikevicius et al., 2017) has been widely used in LLM training to improve compute and
memoryefficiency. Themostpopularmixed-precisionschemesareFP16-FP32andBF16-FP32. Becauseof
the restricted numerical range of FP16, FP16-FP32 scheme has been known instabilities for training large
models (Rae et al., 2021; Zeng et al., 2022). Consequently, the community now commonly adopts BF16-FP32
fortrainingLLMs,suchasMegatron-TuringNLG-530B(Smithetal.,2022),Bloom-175B(Scaoetal.,2022)
and Gopher (Rae et al., 2021). The underlying reason is that BF16 has a wide dynamic range to maintain
numerical stability while matching the performance of the full-precision FP32. Moreover, BF16 employs half
thenumberofbitsascomparedtoFP32,thusreducingconsiderablememoryfootprintswhileimproving
compute efficiency.
FP8isanaturalevolutionfrom16-bitdataformatstofurtherreducingcomputingcosts. However,training
LLMswith reduced-precisionFP8poses newchallenges. Thedynamic rangeand representationprecision
of FP81are much lower than BF16 and FP16, which inevitably induces more training collapses, such as
loss spikes or even NaNs. To address the issues, tensor scaling techniques are proposed (Sun et al., 2019;
Micikevicius et al., 2022). The core idea is multiplying higher precision values with a scaling factor prior to
theircasting toFP8in ordertomove themintoa rangethatbetter overlapswith the representablerangeof
1The details of FP8 data formats are presented in Appendix A.1.
3

--- PAGE 4 ---
acorrespondingFP8format2(Micikeviciusetal.,2022). Suchaper-tensorscalingtechniquereducesdata
quantizationerrorswhileimprovingnumericalstabilityandaccuracy,thusenablingtheutilizationofthe
lower-precision FP8 for training large models.
Unfortunately,thecurrentsupportforFP8low-precisiontrainingisrestricted. NvidiaTE(Nvidia,2022b)only
supportsFP8computeforlinearlayersinTransformer(Vaswanietal.,2017),whileleavingallotheroperations,
suchas weight updateandgradient synchronization, still usinghigherprecision. In this work, we presentan
extremely optimized FP8mixed-precisionstrategyfor LLMtraining. The newFP8 optimization includes
three key perspectives: FP8 communication, FP8 optimizer, and FP8 distributed training. By integrating
theseaspects,thetrainingofLLMssuchasthe175BGPT-3modelcanfullyharnesstheadvantagesofFP8
low-precision and improve training efficiency.
2.1 FP8 Gradient and All-Reduce Communication
Existingmixed-precisiontrainingmethodologies(Micikeviciusetal.,2017;Nvidia,2022b)typicallyemploy16-
bitor32-bitdatatypeforthecomputationandstorageofgradients,resultinginahighbandwidthrequirement
for collective communication throughout the training process. We found that directly applying FP8 to
gradientsleadstoadecreaseinaccuracy. Thefundamentalissueliesintheunderflowandoverflowproblems
arising from the low-bit all-reduce operation. Specifically, there are two standard methods aggregating
gradients across GPUs during all-reduce: pre-scaling andpost-scaling . Pre-scaling divides the gradient gi
calculated on the i-th GPU by the total number of GPUs ( i.e.,N) before being summed, which is formulated
as:
g=g1/N+g2/N+···+gN/N. (1)
When Nis large, thisdivision can causedata underflow, especially for FP8low-precision representation of
gradients. Tomitigatethisissue,post-scalingperformsthegradientsummationfirst,followedbythedivision
scaling during the gradient collection process:
g= (g1+g2+···+gN)/N. (2)
This post-scaling approach keeps the gradients close to the maximum value of the FP8 datatype, effectively
alleviating the underflow issue. However, this approach encounters overflow issues when aggregating
gradients.
In contrast, we propose an automatic scaling technique to resolve both the underflow and overflow issues
inthepre-scalingandpost-scalingapproaches. Tobespecific, weintroduceanauto-scalingfactor µ, that
changes on the fly during the training, to reduce the occurrences of overflow and underflow in gradients:
g′
i=µ·gi. (3)
Astatisticalanalysisisconductedonthegradientvaluesof g′
i,withtheobjectiveofquantifyingtheproportion
of values that attains the maximum feasible value within the FP8 representation range. If the ratio of the
maximum value exceeds a specified threshold, i.e., 0.001%, µis set to 1/2in the subsequent training step,
therebymitigatingtheriskofoverflow. Conversely,whentheratioconsistentlyremainsthethreshold,we
opttoexponentiallyincrease µto2overthespanof1,000trainingsteps,therebyeffectivelymitigatingthe
risk of underflow occurrences.
AnotherkeyobstacleofFP8collectivecommunicationliesindevisinganeffectivestrategytomanagethe
tensor-wise scaling factors that are associated with each gradient tensor. The current NCCL implementation
(Nvidia,2020)lacksthecapabilityofperformingall-reduceoperationconsideringtheadditionaltensor-wise
scaling factors. Meanwhile, efficient implementation is also very challenging, especially considering that
theNCCLgradientsummationoperatesatsub-tensorlevel. Thiscomplexityincreasessignificantlywhen
incorporating updates for tensor-wise scaling factors. To overcome this issue, we propose a new mechanism
that scales FP8 gradients across GPUs using a single shared scalar. To be specific, let (g′
i, s′
i)denote a scaling
tensor which stores the weight gradient in the i-th GPU, where g′
iis a FP8 tensor and s′
iis the corresponding
scaling factor. The actual weight gradient is g′
i/s′
i. Prior to the all-reduce operation over gradient tensors,
2The details of FP8 tensor scaling are introduced in Appendix A.2.
4

--- PAGE 5 ---
wefirstgatherthescalingfactors s′
iofeachgradienttensoronallGPUsandcalculatetheglobalminimum
scaling factor s′
gas:
s′
g=min(s′
1, s′
2, . . . , s′
N), (4)
wheretheglobalminimumscalingfactor s′
gissharedacrossGPUs. Weusethissharedscalingfactor s′
gto
unify the rescaling of the gradient tensors across GPUs. In this way, all gradient tensors associated with the
same weight use the same shared scaling factor to quantize the tensors into FP8 format on all GPUs:
g′′
i=FP8 
s′
g·(g′
i/s′
i)
. (5)
Thisapproachreducescommunicationoverheadbytransmittingonlyasinglescalar s′
g,makingtheadditional
synchronizationstephighlyefficient. Astheinputtensorssharethesamescalingfactor,iteliminatestheneed
ofconsideringall-reducethescalingfactorsinparallelandallowsforstandardNCCLall-reduceoperationto
be performed. The final collected gradient is obtained as follows:
g=g′′
1+g′′
2+···+g′′
N, s =N·s′
g, (6)
where gisthefinalaggregatedgradientand sisthecorrespondingscalingfactor. Rescalingthescalingfactor
for the summed gradient gis equivalent to dividing gbyNin theory. By implementing the aforementioned
dual strategies of distributed and automated scaling, we can successfully realize FP8 low-bit gradient
communicationwhilepreservingmodelaccuracy. Furthermore,thisapproachentailsstoringgradientsin
FP8 and conducting communication in FP8 as well, thereby yielding reductions in GPU memory usage and
communication bandwidth consumption.
2.2 FP8 Optimizer
In the training of LLMs, Adam and its variants (Kingma and Ba, 2015; Loshchilov and Hutter, 2018) are the
mostfrequently-usedoptimizationmethods,thatmaintaincopiesofmodelweights,gradients,first-order
and second-order gradient moments for model updates. Mixed-precision training (Micikevicius et al., 2017)
with Adam optimizer typically stores master weights, gradients and gradient moments in 32-bit float format
for numerical stability (Shoeybi et al., 2019; Rajbhandari et al., 2020; Zhang et al., 2022; Scao et al., 2022).
Consequently, the Adam optimizer consumes 16 bytes of memory per parameter during training:
4|{z}
master weights+ 4|{z}
gradients+ 4 + 4|{z}
Adam states= 16bytes . (7)
When model size is large, the memory consumption of the variables in Adam will become a bottleneck.
Previous work (Rae et al., 2021; Zeng et al., 2022; Liu et al., 2022) has revealed that reducing precision of
the variables in optimizer to 16-bit leads to accuracy degradation when training billion-scale models3. This
prompts an evaluation of which variables in the optimizer should be allocated high precision and which can
be accommodated with low-precision.
To clarify, we decouple the influence of data precision on the variables in the optimizer and investigate
which one can be assigned lower precision, i.e.,precision decoupling . We find a guiding principle: the gradient
statistics can use lower precision, while the master weights necessitate high precision. More concretely, the
first-ordergradientmomentcantolerateahighquantizationerrorandcanbeassignedwithlow-precision
FP8,whilethesecond-ordermomentrequiresahigherprecision,asanalyzedinSec. 3.3. Thisstemsfromthe
fact that, during model updates in Adam, the direction of the gradient holds greater significance than its
magnitude. FP8 with tensor scaling can effectively preserve the distribution of the first-order moment as the
high-precision tensor, though it introduces precision degradation to some extend. Calculating the square of
gradients for the second-order gradient moment might lead to data underflow due to the typically small
gradient values. Therefore, allocating a 16-bit higher precision is necessary to preserve numerical accuracy.
On the other hand, we find that it is crucial to keep the master weights using high precision. The underlying
reason is that weight updates can sometimes become extremely small or large during training, higher
3BF16lackstheprecisionneededforaccuracy,whileFP16hasarestricteddynamicrange. Giventhesechallenges,
prevalent mixed-precision training methodologies rely on utilizing FP32 full-precision for master weights, gradients, and
gradient moments.
5

--- PAGE 6 ---
LayerNorm LayerNorm
Sequence Parallel Tensor Parallel Sequence Parallel
LayerNorm LayerNormFP8(   ) FP8(   )FP8(  ) FP8(  )Gelu GeluFP8(   ) FP8(   )Dropout Dropout
 and   : FP8 Weight      and   : ActivationFigure2: TransformerlayerwithFP8tensorandsequenceparallelism. TheFP8low-bitoperationishigh-
lightedwithorange. gisall-gatherinforwardpassandreduce-scatterinbackwardpass,while ¯gisreduce-
scatterinforwardpassandall-gatherinbackwardpass. Thegather-reduceoperation gbetweensequence
parallelandtensorparallelisexecutedutilizingFP8low-precisionactivation,thussavinghalfcommunication
costs.
precisionforthemasterweightshelpspreventlossofinformationwhenupdatingweights,ensuringmore
stableandaccuratetraining. Intheimplementation,themasterweightshavetwoviableoptions: utilizing
either FP32 full-precision or FP16 with tensor scaling. FP16 with tensor scaling offers the advantage of
conservingmemorywithoutcompromisingaccuracy. Consequently,ourdefaultchoiceistoemployFP16
withtensorscalingforstoringmasterweightsintheoptimizer. OurFP8mixed-precisionoptimizerconsumes
6 bytes of memory per parameter during training:
2|{z}
master weights+ 1|{z}
gradients+ 1 + 2|{z}
Adam states= 6bytes . (8)
Thisnewlow-bitoptimizerreducesmemoryfootprintsby2.6xincomparisontotheprevioussolution,as
exemplified in Eq. (7). Noteworthily, this is the first FP8 optimizer for LLM training. The experiments in
Sec. 3.2 show that FP8 optimizer can preserve model accuracy at various scales, ranging from 125M to 175B
parameters.
2.3 FP8 Distributed Parallel Training
Training LLMs like GPT-3 requires distributed learning strategies for parallelizing across GPUs. The
frequently-usedstrategiesincludedataparallelism,tensorparallelism,pipelineparallelism,andsequence
parallelism. Eachparallelismhasitsownmeritsandhasbeenusedinacomplementaryfashioninexisting
systems (Smith et al., 2022; Shoeybi et al., 2019; Zhang et al., 2022; Scao et al., 2022; Li et al., 2023a). For
FP8supportsofthesestrategies,neitherdataparallelismnorpipelineparallelismnecessitatesanyspecific
modifications, because they do not involve additional FP8 compute and communication when splitting data
batches or model layers into segments across devices.
Tensorparallelismpartitionsindividuallayersofamodelacrossmultipledevices,suchthattheshardsof
weight, gradient and activation tensors are placed on separate GPUs, instead of a single one. To equip tensor
parallelism with FP8, we convert the sharded weight and activation tensors to FP8 format for linear layer
computation, enabling the forward compute and backward gradient collective communication all using FP8.
On the other hand, sequence parallelism splits input sequences into multiple chunks and the sub-sequences
arefedto differentdevicestosave activationmemory. As showninFig. 2,sequenceandtensor parallelism
are performed in parallel to different parts of a Transformer model to make the best use of the available
memoryandimprovetrainingefficiency. Thereisaconverter gbetweensequenceandtensorparallelregions
to all-gather sequence partitions in the forward pass (or reduce-scatter tensor segments in the backward
pass). WeaddanFP8datatypeconversionpriorto g,suchthattheall-gather(orreduce-scatter)operation
uses FP8 low-bit activation to save communication cost across GPUs.
6

--- PAGE 7 ---
weight 1
weight 2
weight 3
GPU 1
GPU 2weight 1
weight 2
weight 3
GPU 1
GPU 2Figure 3: ZeRO tensor partitioning with and without scaling factors. Left: the original high-precision ZeRO
method, which splits a single tensor into multiple partitions and distributes them to different devices. Right:
the proposed FP8 ZeRO, which distributes each tensor in its entirety across devices while taking tensor
scaling into account.
In addition, Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) is another frequently-used
distributed learning technique in large model training. The core idea of ZeRO is to shade model states over
devices, such that each device only hold a fraction of data ( e.g., master weights, gradients, and optimizer
states) required for a training step. To reduce memory consumption, ZeRO method generally splits a single
tensorintomultiplepartitionsanddistributesthemtodifferentdevices. DirectlyapplyingFP8toZeROis
infeasible,becauseitisdifficulttohandlethescalingfactorsassociatedwiththeFP8partitions. Theper-tensor
scalingfactorsshouldbedistributedalongwithFP8partitions. Toaddressthisissue,weimplementanew
FP8distributionschemethatdistributeseachtensorasawholeacrossdevices,ratherthanpartitioningit
intomultiplesub-tensorsasinZeRO.ThedistributionofFP8tensorsisprocessedinagreedymanner,as
outlined in Alg. 1. Specifically, our method first sorts the tensors of model states according to their sizes,
and then distributes the tensors to different GPUs based upon the remaining memory size of each GPU.
The distribution follows the principle that the GPUs with larger remaining memory get a higher priority
in receiving new distributed tensors. In this way, the tensor scaling factors can be distributed along with
thetensorssmoothly,whilereducingcommunicationandcomputecomplexity. Figure3presentsavisual
illustration of the difference in ZeRO tensor partitioning between scenarios with and without scaling factors.
3 Experiment
Inthissection,weassesstheeffectivenessoftheproposedFP8mixed-precisiontrainingapproachonGPT-style
LLMs, including a wide range of model scales, from 125 million to 175 billion parameters. For performance
ablation, we compare GPT models trained with FP8 against those trained with half-precision BF16 and
Algorithm 1 Greedy Distribution Algorithm for ZeRO
Input:FP8tensorswiththeircorrespondingscalingfactors: T={(s1, t1),(s2, t2), . . . , (sn, tn)},where sdenotesscaling
factors while trepresents 8-bit tensors. The size of each tensor: C={c1, c2, . . . , c n}.
Output: Partitions representing scaling tensors assigned to each GPU.
1:SortTin descending order of their sizes to get T′={(s′
1, t′
1),(s′
2, t′
2), . . . , (s′
n, t′
n)}andC′={c′
1, c′
2, . . . , c′
n}, where
c′
1⩾c′
2⩾···⩾c′
n.
2: Initialize memory usage uj= 0and partition pj=∅for each GPU Gj.
3:fori= 1tondo
4: j←arg min juj ▷Find the GPU j∈[1, m]with the least memory usage.
5: pj←pj∪ {(s′
i, t′
i)} ▷Assign (s′
i, t′
i)toGj.
6: uj←uj+c′
i ▷Update the memory usage of Gj.
7:end for
8:returnPartitions P={p1, p2, . . . , p m}
7

--- PAGE 8 ---
params dimension nheads nlayers TP PP SP learning rate batch size ntokens
125M 768 12 12 1 1 ✓ 6.0e−41M 100B
7B 4096 32 32 1 1 ✓ 3.0e−44M 100B
13B 5120 40 40 2 1 ✓ 3.0e−44M 100B
175B 12288 96 96 8 4 ✓ 3.0e−51M 40B
Table1: Modelsizes,architectures,andtraininghyper-parameters. TP,PP,andSPindicatetensor,pipeline,
and sequence parallelism, respectively. To mitigate carbon emissions and save cost, we restrict the training of
the 175B model to a dataset comprising only 40B tokens, which has proven to be sufficient for evaluating
system performance.
full-precision FP32. For generality evaluation, we conduct experiments encompassing both FP8 low-bit
pre-training and fine-tuning, considering instruction tuning and human preference alignment.
3.1 Experimental Setup
3.1.1 Training Dataset
Ourpre-trainingdataisconstructedusingopen-sourcedlanguagecollectionsfromseveralsources,including
CommonCrawl4, The Pile (Gao et al., 2020), C4 (Raffel et al., 2020), OpenWebText (Radford et al., 2019;
Gokaslan and Cohen, 2019), CC-NEWS (Liu et al., 2019), CC-Stories (Trinh and Le, 2018), Redpajama
(Redpajama, 2023), and Wikipedia5. We apply fuzzy deduplication (Lee et al., 2022) across CommonCrawl
snapshots to enhance data quality. Tab. 10 in Appendix A.3 provides details of our pre-training data,
including information such as the number of tokens from each source and associated sampling weights. For
a more comprehensive understanding of the data and its cleaning pipeline, readers are encouraged to refer
to Appendix A.3.
Moreover, for instruction tuning, we follow the same settings as Vicuna-v1.1(VicunaTeam, 2023), which
usesapubliclyuser-sharedinstructionfollowingdata(ShareGPT,2023). Forreinforcementlearningwith
human feedback, the training data we used is a combination of the Anthropic’s Helpful and Harmless
dataset (Bai et al., 2022) and Open-Assistant dataset (Köpf et al., 2023). The training framework and
associated configurations align with the publicly available AlpacaFarm (Dubois et al., 2023).
3.1.2 Model Configuration
The model architecture we used is a decoder-only Transformer (Brown et al., 2020), which has been widely-
used in recentgenerative LLMs like PaLM(Chowdhery et al., 2022), OPT (Zhang etal., 2022), and LLaMA
(Touvron et al., 2023). In addition to the base architecture, we integrate several modifications proposed
recently to improve model efficiency and effectiveness. 1) Rotary Positional Embedding : Drawing inspiration
from recent successful experiments (Black et al., 2022; Touvron et al., 2023), we incorporate rotary positional
embeddings(RoPE)(Suetal.,2021)intoourapproach. Thisadditionenablesustocapturebothabsolute
andrelativepositionsinformation,enhancingperformanceespeciallywhenextrapolatingtolargercontext
windows. 2) Flash Attention : The standard attention implementation is bottlenecked by memory access
(Ivanov et al., 2021). Flash Attention (Dao et al., 2022) proposed an IO-aware exact attention algorithm
which uses tiling to reduce the amount of HBM accesses, achieving substantial acceleration.
WetrainthemodelsusingtheproposedFP8optimizer,whichisbuiltuponAdam(KingmaandBa,2015)
with decoupled weight decay (Loshchilov and Hutter, 2018), following the common practise with the decay
rates β1= 0.9, β2= 0.95, and weight decay = 0.1. The learning rate schedule is cosine-like, and the final
learningrate is10% ofthe maximallearning rate. We trainthe modelsfor 100Btokens intotal witha batch
size of 4M tokens, and the input sequence length is set to 2048. The model warm-up is conducted for 1,000
iterations. Tab. 1presentsthedetailsofmodelconfigurationsandthecorrespondingtrainingsettings. The
training is conducted on Azure NDv5 H100 GPU platform (Microsoft, 2023).
4https://commoncrawl.org
5https://wikipedia.org
8

--- PAGE 9 ---
(a) GPT-7B
 (b) GPT-13B
 (c) GPT-175B
Figure4: AcomparisonbetweenFP8andBF16: AnalyzingthetraininglossofGPTmodelswiththeparameters
ranging from 7 billion to 175 billion.
HS Lambada BoolQ PIQA COPA Winogrande Arc-C Arc-E ObQA Avg
GPT-7B model zero-shot performance
BF1661.3 61.4 61.2 75.0 79.0 58.5 32.9 59.7 36.4 58.4
FP860.0 61.8 62.0 74.2 78.0 59.8 32.9 58.7 34.6 58.0
GPT-13B model zero-shot performance
BF1664.8 64.9 63.4 75.9 82.0 61.0 35.2 61.5 40.6 61.0
FP864.1 63.4 63.9 76.2 81.0 61.6 34.9 61.3 36.8 60.4
Table 2: Zero-shot performance on downstream tasks. The models are trained with either the standard BF16
mixed-precision scheme (Shoeybi et al., 2019) or the proposed FP8 low-precision scheme.
3.2 Main Results
3.2.1 Model Performance
WefirstcomparetheperformanceofmodelstrainedusingFP8mixed-precisionwiththosetrainedusingBF16.
InFig. 4, the pre-trainingloss over tokens isdisplayedforGPT modelsof 7B,13B, and175B parameters. The
trainingconfigurationsandhyper-parametersremainconsistentacrossmodelstrainedwithFP8andBF16.
Theonlydifferenceliesin themixed-precisionschemesutilized. Asshown inFig. 4,theloss curvesalmost
overlap with each other. The results unequivocally demonstrate that the proposed FP8 mixed-precision
scheme can achieve equivalent performance to the prevalenthigher-precisionBF16 scheme (Shoeybi et al.,
2019;Raeetal.,2021;Hoffmannetal.,2022)acrossadiversearrayofmodelscales. Also, weevaluatethe
pre-trainedmodelsonawiderangeofdownstreamtasks,includingHellaSwag(HS)(Zellersetal.,2019),
Lambada (Paperno et al., 2016) BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), COPA (Roemmele et al.,
2011), Winogrande (Sakaguchi et al., 2021), Arc (Clark et al., 2018), and OpenbookQA (ObQA) (Mihaylov
et al., 2018). As reported in Tab. 2, the FP8 pre-trained models exhibit comparable zero-shot performance in
comparison to their BF16 counterparts. This result provides further validation that models pre-trained with
FP8 low-precision maintain both accuracy and intrinsic in-context learning capabilities at a level comparable
to their high-precision counterparts.
Furthermore, weleverage theproposed FP8mixed-precision approachfor fine-tuningLLMs ininstruction
following. Forafaircomparison,wefollowthesameinstructiontuningsettingsasVicuna-v1.1(VicunaTeam,
2023),whichadoptstheopen-sourcedLLaMA-7B(Touvronetal.,2023)asthebasemodelforfine-tuning.
Fig. 5presentsthefine-tuningloss,wherethecurvescorrespondingtoBF16andFP8displayanotabledegree
ofoverlap. Meanwhile,thewin-rateofourFP8fine-tunedmodelsagainstDavinci-003(OpenAI,2022)isalso
comparable to that of Vicuna-v1.1, which is fine-tuned using BF16 half-precision, as reported in Tab. 3. This
indicatesthatourFP8low-bittrainingschemeisversatile,asitisapplicablenotonlytopre-trainingphase
but also to downstream fine-tuning tasks.
In addition, we further apply the proposed FP8 mixed-precision scheme to reinforcement learning from
human feedback (RLHF), a more complex process to align LLMs with user preferences. Following the
9

--- PAGE 10 ---
Figure 5: SFT training loss.Mixed-precisionSystem Performance Model Performance
GPU Mem. (GB) Throughput AlpacaEval MT-Bench
BF16 51.1 103 66.15 5.75
FP8 44.0(-14%) 131( +27%)67.20 5.70
Table3: AcomparisonbetweenFP8andBF16forSFT.Forsys-
tem performance, we report results of GPU memory usage and
training throughput. For model performance, we present the
win-rateagainstDavinci-003onAlpacaEvalandGPT-4judged
scores on MT-Bench.
Figure 6: RLHF training loss.Mixed-precisionMemory Usage (MB) Model Performance
Weights Optimizer States AlpacaEval MT-Bench
BF16 15,082 15,116 72.05 6.16
FP8 10,292( -32%) 5,669( -62%) 72.42 6.04
Table4: AcomparisonofFP8andBF16RLHFalignment. Mem-
ory usage is assessed with a focus on weights and optimizer
states, while model performance is evaluated on AlpacaEval
considering win-rate against Davinci-003, and MT-Bench using
GPT-4 judged scores.
sametrainingsettingasAlpacaFarm(Duboisetal.,2023),arecentRLframeworkforLLMalignment,we
optimize policymodels with PPO algorithm (Schulmanet al., 2017). The solely difference lies inthechoice
ofmixed-precisiontrainingschemes, i.e.,BF16v.s.FP8. FromtheresultsreportedinFig. 6andTab. 4,we
observe a notable reduction in memory utilization, for instance, a 32% memory reduction concerning model
weightsanda62%reductionconcerningoptimizerstates. Consequently,itcanbeinferredthatFP8iscapable
ofreplicatingtheBF16mixed-precisionforRLHFtraining. Thisunderscoresthebroaderapplicabilityand
versatility of our FP8 low-bit training solution.
3.2.2 System Performance
In this section, we evaluate system-level performance of FP8 mixed-precision, considering communication
efficiency, memory utilization, and the overall speed, with an emphasis on cost savings. Our method
employs 8-bit gradients for all-reduce collective communication among GPUs. Theoretically, this results
in a 75% reduction in communication costs when compared to the mainstream 32-bit scheme (Despite
BF16 mixed-precision computing gradients using 16-bit precision, it still employs 32-bit precision for all-
reduce communication (Shoeybi et al., 2019)). Due to the impact of system transmission loss, the observed
practical reductionduring GPTmodel training fallswithin therange of63% to 65%,as indicatedin Table 5.
Furthermore, it is worth noting that the recent Nvidia Transformer Engine (TE) (Nvidia, 2022b) still relies
on full-precision FP32 for collective communication, resulting in the same level of reduction for our FP8
solution.
WhentrainingGPTmodelswithidenticalbatchsizes,FP8mixed-precisioncanleadtoareductioninmemory
footprint ranging from 28% to 39% when compared to BF16, as reported in Tab. 5. These reductions in
memory consumption are attributed to the FP8 gradient and FP8 optimizer techniques wehave introduced.
Moreover, compared with TE (Nvidia, 2022b), our solution is also very competitive, obtaining 36.1%, 36.0%,
and 42.1% additional memory reductions for different model sizes, i.e., GPT-7B, 13B, and 175B. Although TE
employs FP8 for compute, it still uses high-precision optimizer and gradients, which consumes much more
memorythanoursolution. Inaddition,thesavedmemoryinourmethodcanbeusedtotrainlargerbatch
size or longer sequence. For example, when employing 32 H100 GPUs with a memory capacity of 80GB, our
approach enables the training of models with a context of 4,096 tokens, accommodating up to 175 billion
parameters. Incontrast,TEcanonlyaccommodatemodelswithacontextof2,048tokens. Thisshowcases
the potential of integrating our FP8 mixed-precision training into existing LLMs, empowering them to train
longer sequences with the same GPU resources.
10

--- PAGE 11 ---
Model TP PP DPMicro Mixed GPU ThroughputTFLOPSMFU Weight-related Comm.
BS Precision Mem. (GB) (#samples/s) (%) Rate (%) Volume (GB)
GPT-7B 1 1 322 BF16 69.6 159.2 445 45.0 10.1 37.2
2 FP8 (TE) 77.3 224.5 627 31.7 9.7 37.2
2 FP8 (Ours) 49.4 ( -29%) 219.8 ( +38%) 615 31.1 7.9 13.9 ( -63%)
4 FP8 (Ours) 69.3 230.5 ( +45%) 645 32.6 10.4 13.9 ( -63%)
GPT-13B 2 1 162 BF16 68.2 79.3 420 42.5 11.1 34.3
2 FP8 (TE) 76.4 111.7 592 29.9 7.1 34.3
2 FP8 (Ours) 48.9 ( -28%) 109.5 ( +38%) 575 29.1 3.9 12.4 ( -64%)
4 FP8 (Ours) 67.8 121.5 ( +53%) 644 32.5 9.3 12.4 ( -64%)
GPT-175B 8 4 41 BF16 66.1 22.4 386 39.0 8.8 23.4
1 FP8 (TE) 69.6 28.7 493 24.9 3.9 23.4
1 FP8 (Ours) 40.3 ( -39%) 27.1 ( +21%) 473 23.9 2.5 8.2 ( -65%)
4 FP8 (Ours) 57.7 39.3 ( +75%) 677 34.2 10.9 8.2 ( -65%)
Table 5: System-level performance on Nvidia H100 GPUs 80GB. Here, TP, PP, and DP represent tensor,
pipeline, and data parallelism respectively. BS indicates batch size, while MFU denotes model FLOPs
utilization. Weight-relatedcommunicationcontainstheall-gatheroperatoronweightsandthereduce-scatter
operator on weight gradients.
0 5 10 15 20 25 30
Block ID050100150200SNRpre scale
post scale
auto scale
(a) SNR (Signal to Noise Ratio)
0 5 10 15 20 25 30
Block ID0204060Underflow Rate (%)pre scale
post scale
auto scale (b) Underflow rate
0 5 10 15 20 25 30
Block ID0.000.050.100.150.200.25Overflow Rate (%)pre scale
post scale
auto scale (c) Overflow rate
Figure 7: Comparing different strategies, i.e., pre-scaling, post-scaling, and auto-scaling, for FP8 gradient
all-reduce. WeinvestigateSNR,underflowrate,andoverflowrateacrossdifferentTransformerblocks. The
experiment is conducted using a GPT-7B model with a data parallelism factor of 128.
Moreover, our FP8 mixed-precision scheme shows a superior training throughput compared to the prevalent
BF16scheme,achievinganotablespeed-upof75%whenappliedtoGPT-175Bmodel. ThemodelFLOPS
utilization (MFU) of FP8 mixed-precision training is 34.2% on H100 GPUs, being 37.3% superior to TE.
These findings provide substantial evidence that our FP8 scheme effectively conserves memory, reduces
communicationcostsduringthetrainingoflargemodels,andultimatelyenhancessystemutilizationefficiency
on the latest H100 GPU platform.
3.3 Ablation Study
We ablate various design choices of FP8 mixed-precision training strategy for LLMs and report the per-
formance in Tab. 6 – 8 and Fig. 7 – 8. The ablation experiments are conducted on GPT models, whose
architecturesandtrainingsettingsareelaboratedinTab. 1. Importantly,ourablationstudyyieldsseveral
guidelines for the effective utilization of 8-bit datatype in LLM training, which can facilitate future research
on low-bit model training.
Communication . Wefirstanalyzethelimitationsoftheconventionalpre-scalingandpost-scalingmethods
whenaggregatinglow-bitgradientsduringtheall-reducecommunicationprocess. AsshowninFig. 7,we
conduct a statistical analysis on SNR, underflow rate, and overflow rate of weight gradients across different
Transformer blocks. It is observed that the pre-scaling method has relative larger underflow rate when
quantifyinggradientsfrom32-bitto8-bit,whilethepost-scalingmethodhashigheroverflowrate. Incontrast,
theproposedauto-scalingtechniquecandiminishboththeunderflowratioandtheoverflowratio,while
getting much better SNR, as shown in Fig. 7 (a). This demonstrates the effectiveness of auto-scaling method
in reducing quantization errors when utilizing 8-bit datatype for gradient all-reduce.
11

--- PAGE 12 ---
Low-bit
SettingsCompute
(GEMM)Comm.Master
WeightOptimizer
States
FP32 #0 FP32 FP32 FP32 FP32+FP32
BF16 #1 BF16 FP32 FP32 FP32+FP32
FP8 #2a FP8 FP8 FP16 FP8+FP16
FP8 #2b FP8 FP8 BF16 FP8+FP16
FP8 #3 FP8 FP8 FP8 FP8+FP16
FP8 #4 FP8 FP8 FP16 FP8+FP8
Table 6: Precision decoupling for the variables
withintheoptimizer. Here,ourfocusisonablating
the master weight and optimizer states, as these
components areprecision sensitive. Theoptimizer
statesincludebothfirst-orderandsecond-ordergra-
dientmoments. NotethattheFP16masterweight
uses tensor scaling.
0 20 40 60 80 100
Billions of T okens3.03.54.04.55.0Training Loss
FP32 #0
BF16 #1
FP8 #2a
FP8 #2b
FP8 #3
FP8 #4Figure 8: Training losses of GPT-125M models
with the settings presented in Tab. 6. The loss
curve for FP8 #4 has diverged.
Model TP PP DPMicro Mixed Act-related Comm.
BS Precision Rate (%) Volume (GB)
GPT-13B 2 1 16 2BF16 12.9 4.7
FP8 (Ours) 5.3 3.1
GPT-175B 8 4 4 1BF16 14.9 5.9
FP8 (Ours) 5.2 3.9
Table 7: Activation-related communication vol-
ume reduction in sequence and tensor paral-
lelism, including the all-gather operator on ac-
tivation and the reduce-scatter on activation gra-
dients.
Model TP PP DPMicro Mixed GPU Memory
BS Precision Min Max
GPT-7B 1 1 32 2BF16 69.07 69.63
FP8 (TE) 76.97 77.28
FP8 (Ours) 49.06 49.36
GPT-13B 2 1 16 2BF16 67.98 68.18
FP8 (TE) 73.68 76.36
FP8 (Ours) 48.45 48.85
GPT-175B 8 4 4 1BF16 65.60 66.12
FP8 (TE) 69.04 69.57
FP8 (Ours) 38.64 40.28
Table8: ComparingZeROdistributionmethods
in terms of memory load across GPUs. Here
“Min”and“Max”denotetheminimumandmax-
imummemoryutilizationobservedacrossGPUs.
OurFP8ZeROmethoduseslessmemorywhile
achieving memory-aware load balancing.Optimizer . We further ablate the impact of re-
ducedprecisionforthevariablesintheAdamW
optimizer. We set the BF16 mixed-precision op-
timizer as the baseline, since it has been widely
used in existing LLM training frameworks (Mi-
cikevicius et al., 2017; Shoeybi et al., 2019; Nvidia,
2022b). Tab. 6 presents the settings of reduced
precisionforthevariables,whileFig. 8plotsthe
corresponding training losses. We observe that:
1) FP8 master weight induces performance degra-
dation (see the #2a vs.#3 lines in Fig. 8), while
FP16 can maintain accuracy as FP32 (see #2a vs.
#0and#1)butrequiringusingtensorscaling. Itre-
veals that the master weight is precision-sensitive.
This can be attributed to the master weight’s role
inupdatingweights,whichtendtoexhibitsmall
magnitudes, necessitating high precision to main-
tain accuracy. 2) The training loss of BF16 master
weight is slightly higher than that of FP16 with
ascalingfactorbecauseBF16hasfewermantissa
bits, resulting in lower precision (see #2a vs.#2b).
3) The second-order gradient moment is more
precision-sensitive than the first-order one, be-
causethesquarecalculationiseasytocauseunder-
flow and leads to accuracy degradation. Utilizing
FP8 for the second-order gradient moment can
lead to divergent training loss (see the #4 dot in
Fig. 8).
Parallelism . InourFP8LLMtrainingframework,weintroduceFP8low-bitconvertorsintosequenceparallelism
and tensor parallelism to reduce activation communication costs across GPUs. Here we conduct an analysis
experiment to count the activation-related communication volume during GPT model training, andreport
the numbers in Tab. 7. It is observed that our FP8 parallel scheme results in a substantial reduction of 34% in
activation-relatedcommunicationcostscomparedtotheoriginalmethodutilizingBF16. Furthermore,in
ZeRO distributed training, our method distributes each FP8 tensor along with its associated scaling factor as
awhole,ratherthanpartitioningthetensorintosplitsacrossGPUs. Thisstrategynotonlyresultsinmore
GPU memory savings but also maintains a balanced memory load across GPUs, as demonstrated in Tab. 8.
12

--- PAGE 13 ---
4 Related Work
Mixed-precision Training. Efficient training through reduced mixed-precision has been widely used in
moderndeep learningto savecomputingcosts. Whilesomeworks havetaken bit-reductiontothe extreme,
i.e.1-bit binary networks (Hubara et al., 2016; Rastegari et al., 2016), they have not been successful in
maintaining model accuracy (Micikevicius et al., 2022). The most practical scheme now is the FP16 half-
precisionmethod(Micikeviciusetal.,2017),whichcanmaintainaccuracywhileimprovingtrainingefficiency.
Thecomputations duringforward passandback propagationuseFP16 whilethemaster weightsuseFP32.
SinceFP16hasanarrowerdynamicrange,FP16mixed-precisionentailslossscaling(Micikeviciusetal.,2017)
to prevent loss of accuracy. Fortunately, the need for loss scaling can be avoided by using BF16 datatype,
because BF16 maintains the same dynamic range as the full-precision FP32. This results in that large model
training now prefers to use BF16 mixed-precision scheme, which is more stable during training (Smith et al.,
2022; Scao et al., 2022; Zeng et al., 2022).
FP8isanaturalprogressionfrom16-bitdataformatstofurtherreducingcomputingcost. Earlypioneering
effortsinFP8low-bitmodeltraining(Wangetal.,2018;Sunetal.,2019;Dettmersetal.,2021)havelargely
remained at the simulation stage. Consequently, there exists a notable gap between the projected capabilities
of these approaches and their actual performance on hardware (Micikevicius et al., 2022). With the advent
of Nvidia Hopper GPU architecture (Nvidia, 2022a), FP8 is emerging as a viable and practical data type for
the next-generation low-precision training, as discussed in (Micikevicius et al., 2022). At present, the Nvidia
Transformer Engine (TE) (Nvidia, 2022b) serves as the primary framework for FP8 mixed-precision training.
However,itssupportforFP8usageremainssomewhatconstrained. TE’scurrentimplementationrestricts
FP8usagesolelytoweightcomputation,retainingthestorageofmodelweightsandgradientcalculations
with16-bitdatatypes. Consequently,theend-to-endspeed-up,memoryandcommunicationcostsavings
are limited. In contrast, our work infiltrates FP8 gradient, optimizer, and distributed training into the whole
progress of model training, fully unveiling the capabilities of FP8.
Large Language Models. Recent years have witnessed a substantial evolution in the field of LLMs. Autore-
gressive language modeling – predicting the future of a text sequence from its past – provides a simple yet
powerful objective that admits formulation of numerous tasks. While there exist alternative methodologies,
suchasmaskedlanguagemodeling(Devlinetal.,2019)andpermutationlanguagemodeling(Yangetal.,
2019), the autoregressive method now is more promising because of its strong performance. Following the
scalinglaws (Brownet al.,2020) andthe refinedlaws (Hoffmannet al.,2022), variousLLMs arehave been
proposed, including dense models: GPT-3 (Brown et al., 2020), Jurassic-1 (Lieber et al., 2021), Gopher (Rae
etal.,2021),Chinchilla(Hoffmannetal.,2022),Bloom(Scaoetal.,2022),OPT(Zhangetal.,2022)Megatron-
Turing NLG (Smith et al., 2022), PaLM (Chowdheryet al., 2022), LaMDA (Thoppilan et al., 2022), LLaMA
(Touvronetal.,2023),andsparsemodels: GLaM(Duetal.,2022),andSwitchtransformers(Fedusetal.,
2022). Each of them has demonstrated remarkably competitive few-shot performance across a wide range of
tasksatthetimeoftheirrespectivereleases. Nonetheless,thesemodelsstillencounterchallenges,suchas
overwhelming computational requirements and the need for acquiring more high-quality training data. In
this work, we delve into the utilization of low-precision techniques to mitigate the training costs, which is a
crucial step for the continued expansion of language models.
Low-precisiontraininghasbeenwidelyusedinLLMtrainingtoreducecomputecost. OPT(Zhangetal.,
2022)andGLM(Zengetal.,2022)utilizeFP16forforwardsandbackwardsandFP32foroptimizerstatesand
master weights, to reduce the GPU memory usage and improve training efficiency. Bloom (Scao et al., 2022)
findthatFP16cancausenumericalinstabilitiesandirreversibledivergences,especiallywhentrainingmodels
largerthan100Bparameters,becauseFP16’sdynamicrangeislimited. Consequently,BloomandotherLLMs,
suchasGopher(Raeetal.,2021)andChinchilla(Hoffmannetal.,2022),adoptBF16mixed-precision,because
BF16 hasa widedynamic range thatis the sameas FP32. LLM training andtuning with 8-bitlow-precision
were not well-explored in previous works, because the hardware support for FP8 is not available before
the release of Nvidia Hopper infrastructure. This work presents the first exploration of FP8 pre-training
and fine-tuning for LLMs, while proposing an extremely-optimized FP8 mixed-precision scheme. We hope
thisworkcouldfacilitatefutureresearchinFP8and,potentially,extendtoexploringevenlowerprecision
training, such as 4-bit and 1-bit.
13

--- PAGE 14 ---
5 Conclusion
Inthiswork,weexplore8-bittrainingforLLMs. WeintroduceanewFP8mixed-precisiontrainingframe-
work,whichincorporates8-bitcollectivecommunication,optimizer,anddistributedparalleltraininginan
incremental manner. To our best knowledge, this is the first work infiltrating FP8 compute, storage and
communicationintothewholeprogressoflargelanguagemodeltraining. Extensiveexperimentsdemonstrate
the proposed method effectively diminishes communication overhead and curtails memory utilization in the
context of GPT model training at various scales. In future work, we plan to scale up the size and training
stepsoftheFP8GPTmodelsandfurthertrainthemwithour8-bitmixed-precisionscheme. Moreover,we
willalsousetheproposedFP8schemetotrainmulti-modallargemodels, andexplorelow-bitdeploymentof
LLMs on various edge devices, such as smart phones.
Contribution and Acknowledgement
This project was initially proposed by Han Hu and Peng Cheng, who are the directional lead. Shuguang Liu
served as the product lead throughout the project.
The contributions for all the co-authors are detailed as follows:
FP8 Framework: Kan Wu, Houwen Peng, Ze Liu, Peng Cheng, Han Hu
System: Yifan Xiong, Ziyue Yang, Yuxiang Yang, Guoshuai Zhao, Peng Cheng
HardwareInfrastructure: GuoshuaiZhao,YuxiangYang,YifanXiong,PengCheng,ShuguangLiu,JoeChau
Data:Ruihang Li, Miaosen Zhang, Jia Ning, Chen Li, Ruizhe Wang, Houwen Peng, Han Hu
Pre-training: Yixuan Wei, Kan Wu, Ze Liu, Miaosen Zhang, Zheng Zhang, Houwen Peng, Han Hu
Alignment (SFT, RS, and RLHF): Bolin Ni, Jingcheng Hu, Yixuan Wei, Houwen Peng, Han Hu
Evaluation: Yixuan Wei, Bolin Ni, Jingcheng Hu
Product Engineering: Yuxiang Yang, Kan Wu, Yifan Xiong, Ziyue Yang, Guoshuai Zhao, Peng Cheng
We thank Eric Chung, Bita Darvish Rouhani, Yu Pei, Hyunseung Harry Yoo, Zhenghong Zhou, Gongrui
Zhang, and Zhirong Wu for helpful discussions.
We thank Baining Guo and Lidong Zhou for their guidance and support for this project.
14

--- PAGE 15 ---
References
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint
arXiv:2305.10403 , 2023.
YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,DawnDrain,Stanislav
Fort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmlessassistantwithreinforcement
learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.
Microsoft Bing. Bing webmaster tools. 2022. URL https://www.bing.com/webmasters/ .
YonatanBisk,RowanZellers,JianfengGao,YejinChoi,etal. Piqa: Reasoningaboutphysicalcommonsensein
natural language. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pages 7432–7439,
2020.
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language
model. In ProceedingsofBigScienceEpisode#5–WorkshoponChallenges&PerspectivesinCreatingLargeLanguage
Models, pages 95–136, 2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan,PranavShyam, GirishSastry,AmandaAskell, SandhiniAgarwal,ArielHerbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,
SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners.
InAdvances in Neural Information Processing Systems , volume 33, pages 1877–1901. Curran Associates, Inc.,
2020.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha
Tsvyashchenko,Joshua Maynez, AbhishekB Rao, ParkerBarnes, Yi Tay, NoamM. Shazeer, Vinodkumar
Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,
Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,
HenrykMichalewski,XavierGarcía,VedantMisra,KevinRobinson,LiamFedus,DennyZhou,Daphne
Ippolito,DavidLuan,HyeontaekLim,BarretZoph,AlexanderSpiridonov,RyanSepassi,DavidDohan,
ShivaniAgrawal,MarkOmernick,AndrewM.Dai,ThanumalayanSankaranarayanaPillai,MariePellat,
Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei
Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling
with pathways. ArXiv, abs/2204.02311, 2022.
ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristinaToutanova.
BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference
oftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,
Volume 1 (Long and Short Papers) , pages 2924–2936, Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300 .
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,andOyvind
Tafjord.Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge. arXiv:1803.05457v1 ,
2018.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient
exact attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344–16359, 2022.
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization.
InInternational Conference on Learning Representations , 2021.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter
oftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers) ,
15

--- PAGE 16 ---
pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423 .
NanDu,YanpingHuang,AndrewMDai,SimonTong,DmitryLepikhin,YuanzhongXu,MaximKrikun,
YanqiZhou,AdamsWeiYu,OrhanFirat,etal. Glam: Efficientscalingoflanguagemodelswithmixture-of-
experts. In International Conference on Machine Learning , pages 5547–5569. PMLR, 2022.
YannDubois,XuechenLi,RohanTaori,TianyiZhang,IshaanGulrajani,JimmyBa,CarlosGuestrin,Percy
Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from
human feedback. arXiv preprint arXiv:2305.14387 , 2023.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and efficient sparsity. The Journal of Machine Learning Research , 23(1):5232–5270, 2022.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace
He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027 , 2020.
Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus , 2019.
JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya, TrevorCai, ElizaRutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal
large language models. arXiv:2203.15556 , 2022.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural
networks. Advances in neural information processing systems , 29, 2016.
HuggingFace. wikipedia - datasets at hugging face. 2022. URL https://huggingface.co/datasets/
wikipedia .
AndreiIvanov,NikoliDryden,TalBen-Nun,ShigangLi,andTorstenHoefler. Datamovementisallyouneed:
A case study on optimizing transformers. Proceedings of Machine Learning and Systems , 3:711–732, 2021.
Armand Joulin, Édouard Grave, Piotr Bojanowski, and Tomáš Mikolov. Bag of tricks for efficient text
classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational
Linguistics: Volume 2, Short Papers , pages 427–431, 2017.
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. In 3rdInternationalConference
on Learning Representations , San Diego, CA, 2015. URL http://arxiv.org/abs/1412.6980 .
Denis Kocetkov, Raymond Li, LI Jia, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Muñoz
Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, et al. The stack: 3 tb of permissively licensed
source code. Transactions on Machine Learning Research , 2022.
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Ab-
dullahBarhoum,NguyenMinhDuc,OliverStanley,RichárdNagyfi,etal. Openassistantconversations–
democratizing large language model alignment. arXiv preprint arXiv:2304.07327 , 2023.
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and
Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8424–8445,
2022.
Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and
Yang You. Colossal-ai: A unified deep learning system for large-scale parallel training. In Proceedings of the
52nd International Conference on Parallel Processing , pages 766–775, 2023a.
XuechenLi,TianyiZhang,YannDubois,RohanTaori,IshaanGulrajani,CarlosGuestrin,PercyLiang,and
Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https:
//github.com/tatsu-lab/alpaca_eval , 2023b.
Opher Lieber, Or Sharir, Barak Lenz,and Yoav Shoham. Jurassic-1: Technical details andevaluation. White
Paper. AI21 Labs , 1, 2021.
16

--- PAGE 17 ---
Yinhan Liu,Myle Ott, NamanGoyal, Jingfei Du,Mandar Joshi, DanqiChen, Omer Levy,Mike Lewis, Luke
Zettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretrainingapproach. arXivpreprint
arXiv:1907.11692 , 2019.
Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,
LiDong,etal. Swintransformerv2: Scalingupcapacityandresolution. In ProceedingsoftheIEEE/CVF
conference on computer vision and pattern recognition , pages 12009–12019, 2022.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on
Learning Representations , 2018.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg,MichaelHouston,OleksiiKuchaiev,GaneshVenkatesh,etal. Mixedprecisiontraining. arXiv
preprint arXiv:1710.03740 , 2017.
PauliusMicikevicius,DusanStosic,NeilBurgess,MariusCornea,PradeepDubey,RichardGrisenthwaite,
Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv
preprint arXiv:2209.05433 , 2022.
Microsoft. Azure high-performance computing. 2023. URL https://azure.microsoft.com/en-us/
solutions/high-performance-computing .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a
new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing , pages 2381–2391, 2018.
Nvidia. Apex. 2018. URL https://nvidia.github.io/apex .
Nvidia. The nvidia collective communications library. 2020. URL https://developer.nvidia.com/nccl .
Nvidia. Nvidia h100 tensor core gpu architecture. 2022a. URL https://resources.nvidia.com/
en-us-tensor-core .
Nvidia. Nvidia transformer engine. 2022b. URL https://docs.nvidia.com/deeplearning/
transformer-engine/index.html .
Nvidia. Using fp8 with transformer engine. 2022c. URL https://docs.nvidia.com/deeplearning/
transformer-engine/user-guide/examples/fp8_primer.html .
OpenAI. Model index for researchers. 2022. URL https://platform.openai.com/docs/
model-index-for-researchers .
OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro
Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word predic-
tion requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages 1525–1534, 2016.
Shawn Presser. Books3. https://twitter.com/theshawwn/status/1320282149329784833 , 2020.
Alec Radford,Jeff Wu, RewonChild, DavidLuan, Dario Amodei, andIlya Sutskever. Languagemodels are
unsupervised multitask learners. 2019.
JackWRae,AnnaPotapenko,SiddhantMJayakumar,ChloeHillier,andTimothyPLillicrap. Compressive
transformers for long-range sequence modelling. In International Conference on Learning Representations ,
2019.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides,
Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis &
insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. The
Journal of Machine Learning Research , 21(1):5485–5551, 2020.
17

--- PAGE 18 ---
Samyam Rajbhandari, JeffRasley,Olatunji Ruwase, andYuxiongHe. Zero: Memoryoptimizationstoward
training trillion parameter models. In SC20: International Conference for High Performance Computing,
Networking, Storage and Analysis , pages 1–16. IEEE, 2020.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning , pages
8821–8831. PMLR, 2021.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification
using binary convolutional neural networks. In European conference on computer vision , pages 525–542.
Springer, 2016.
Redpajama. Redpajama-data: an open source recipe to reproduce llama training dataset. 2023. URL
https://github.com/togethercomputer/RedPajama-Data .
Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives: An
evaluation of commonsense causal reasoning. In AAAI Spring Symposium , 2011.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial
winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021.
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning
abilities of neural models. In International Conference on Learning Representations , 2018.
TevenLeScao,388Authors,andThomasWolf.BLOOM:A176B-parameteropen-accessmultilinguallanguage
model.ArXiv, abs/2211.05100, 2022.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov.Proximalpolicyoptimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
ShareGPT. Openchat: Advancing open-source language models with imperfect data. 2023. URL https:
//sharegpt.com/ .
MohammadShoeybi, MostofaPatwary, RaulPuri, PatrickLeGresley, JaredCasper, andBryanCatanzaro.
Megatron-lm: Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism. arXivpreprint
arXiv:1909.08053 , 2019.
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,
Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron
totrainmegatron-turingnlg530b,alarge-scalegenerativelanguagemodel. arXivpreprintarXiv:2201.11990 ,
2022.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Viji
Srinivasan,XiaodongCui,WeiZhang,andKailashGopalakrishnan. Hybrid8-bitfloatingpoint(hfp8)
training and inference for deep neural networks. Advances in neural information processing systems , 32, 2019.
Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani,
KaoutarElMaghraoui,VijayalakshmiVijiSrinivasan,andKailashGopalakrishnan. Ultra-lowprecision
4-bit training of deep neural networks. Advances in Neural Information Processing Systems , 33:1796–1807,
2020.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
AliciaJin,TaylorBos,LeslieBaker,YuDu,etal. Lamda: Languagemodelsfordialogapplications. arXiv
preprint arXiv:2201.08239 , 2022.
Jörg Tiedemann. Finding alternative translations in a large corpus of movie subtitle. In Proceedings of the
Tenth International Conference on Language Resources and Evaluation (LREC’16) , pages 3518–3522, 2016.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundation
language models. arXiv preprint arXiv:2302.13971 , 2023.
TrieuHTrinhandQuocVLe. Asimplemethodforcommonsensereasoning. arXivpreprintarXiv:1806.02847 ,
2018.
18

--- PAGE 19 ---
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Process-
ing Systems , pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7181-attention-is-all-you-need.pdf .
VicunaTeam. Vicuna: An open-source chatbot impressing gpt-4 with 90quality. 2023. URL https://lmsys.
org/blog/2023-03-30-vicuna/ .
NaigangWang,JungwookChoi,DanielBrand,Chia-YuChen,andKailashGopalakrishnan. Trainingdeep
neural networks with 8-bit floating point numbers. Advances in neural information processing systems , 31,
2018.
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand
Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data.
arXiv preprint arXiv:1911.00359 , 2019.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Gen-
eralized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf .
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine
really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics , pages 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472 .
AohanZeng,XiaoLiu,ZhengxiaoDu,ZihanWang,HanyuLai,MingDing,ZhuoyiYang,YifanXu,Wendi
Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. In The Eleventh International
Conference on Learning Representations , 2022.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 , 2022.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv
preprint arXiv:2306.05685 , 2023.
YukunZhu,RyanKiros,RichZemel,RuslanSalakhutdinov,RaquelUrtasun,AntonioTorralba,andSanja
Fidler. Aligningbooksandmovies: Towardsstory-likevisualexplanationsbywatchingmoviesandreading
books. In Proceedings of the IEEE international conference on computer vision , pages 19–27, 2015.
19

--- PAGE 20 ---
A Appendix
A.1 FP8 Data Formats
InSeptember2022,NVIDIA,ARM,andIntelpublishedFP8specificationforstandardizationasaninterchange
formatforAI(Micikeviciusetal.,2022). Theindustryhasmovedfrom32-bitprecisionto16-bit,andnoweven
8-bitprecisionforAImodeltraining. Thisdevelopmentreflectsabroaderindustrytrendthathastransitioned
fromhigh-precisiontolow-precisiontraining. Notably,theproposedFP8specificationintroducestwodistinct
data types, E5M2 and E4M3, which offer a trade-off between a larger range and higher precision of stored
values (Nvidia, 2022c).
•E4M3 consists of 1 sign bit, 4 exponent bits and 3 bits of mantissa. It can store values up to +/-448
and NaN.
•E5M2consistsof1signbit,5exponentbitsand2bitsofmantissa. Itcanstorevaluesupto+/-57344,
+/- inf and NaN.
The FP8 format (Micikevicius et al., 2022) roughly follows the IEEE 754 standard. Compared to higher
precision data formats such as FP16 and FP32, FP8 suffers from two kinds of representation degradation:
•Lower representation range. The representation range in a data format specifies the range between the
maximumandminimumvaluesthattheformatcanaccuratelyrepresent. Therearetwomodes,a
normal mode, which defines a regular range with relatively constant precision, and a subnormal
mode, which extends the range to represent smaller values with lower precision. The normal range
primarilydependsonthenumberofexponent( E)bits,withmore Ebitsresultinginalargernormal
range. Ontheotherhand,thesubnormalrangeisprimarilyinfluencedbythenumberofmantissa
(M) bits, where an increase in Mbits leads to a larger subnormal range. As illustrated in Tab. 9, the
representationrangeofFP8isnotablynarrowercomparedtothatofFP16andFP32, especiallyinthe
caseofthe S1E4M3sub-format( Sdenotesthesignbit). Thisdiscrepancyrepresentstheprimary
challenge when employing FP8 for training large models.
•Lower representation precision. The limited number of mantissa ( Mbits) leads to quantization rep-
resentation errors. Due to the considerably fewer Mbits in FP8, the representation precision of
FP8 is substantially lower than that of FP16, as depicted in Tab. 9. This challenge stands as another
significant hurdle when considering the use of FP8 for training large models.
FP8 consists of two sub-formats: S1E4M3andS1E5M2. The former offers a narrower representation range
buthigherprecision,whilethelatterprovidesalargerrangebutlowerprecision. Thesetwosub-formatsgive
users the flexibility to strike a balance between their requirements for range and precision in model training.
Table 9: Representation range and error for different data formats
Data format Representation Range Maximum Relative Error
Max normal Min normal Min subnormal Min - Max (normal) Min∼Max (subnormal)
FP32
(S1E8M23)3.40×10381.18×10−381.40×10−451.19×10−7∼5.96×10−85.00×10−1∼1.19×10−7
FP16
(S1E5M10)65,504 6.10×10−55.96×10−89.76×10−4∼4.89×10−45.00×10−1∼9.78×10−4
BF16
(S1E8M7)3.39×10381.18×10−389.18×10−417.75×10−3∼3.94×10−35.00×10−1∼7.94×10−3
FP8
(S1E4M3)448 1.56×10−21.95×10−31.11×10−1∼7.69×10−25.00×10−1∼1.67×10−1
FP8
(S1E5M2)57,344 6.10×10−51.53×10−52.00×10−1∼1.67×10−15.00×10−1∼5.00×10−1
20

--- PAGE 21 ---
A.2 FP8 Tensor Scaling
WenowdiscusstheunderlyingmechanismsforhowlargemodeltrainingwithFP8overcomesthechallenges
associated with representation range and precision degradation. The key technique behind is tensor scaling,
which scales the tensor values that originally locate out the representation range of a data format to its
comfort zone, as visualized in Fig. 9. The pioneer scaling techniques (Micikevicius et al., 2017; Nvidia, 2018)
apply a global scaling factor to the loss, such that gradients of all layers are scaled by a single adaptive factor.
The utilization of the global loss scaling technique, in conjunction with various other training strategies, has
facilitated the widespread adoption of FP16 mixed-precision training on V100 and A100 GPUs. Remarkably,
this approach has resulted in minimal to no degradation in accuracy, particularly for small to medium-sized
models(Micikeviciusetal.,2017). Nonetheless,whendealingwithsuper-largemodelsorcomplextasks,
suchasinthetrainingofmodelslikeDALL-E(Rameshetal.,2021),thegloballossscalingtechniquestill
encounters significant underflow issues. As a consequence, block-wise (Ramesh et al., 2021) and layer-wise
(Sun et al., 2020) gradient scaling are proposed.
WhiletheglobalscalingtechniqueenablesalmostnoaccuracydropforFP16training(witharangeof[5.96E-8,
6.55E+4]),thefine-grainedper-tensorscalingwillenablestablemodeltrainingusingevenshallowerrangeby
FP8(witharangeof[1.95E-3,448]for E4M3andarangeof[1.53E-5,5.73E+4]for E5M2). Fig. 9showsthat
the representation range of FP8 has been large enough to deal with general model training. In the per-tensor
scaling technique, various strategies are available for choosing the suitable scaling factor for a given FP8
tensor. Two common approaches are “just-in-time scaling" and “delayed scaling" (Nvidia, 2022c).
•Just-in-time scaling . This strategy involves determining the scaling factor based on the maximum
absolutevalue(amax)ofthetensorbeinggenerated. However,inpracticalapplications,thisapproach
is often infeasible because it necessitates multiple passes through the data. Specifically, the operator
first produces and writes out the output in higher precision, then calculates the maximum absolute
value of the output, and finally applies this scaling factor to all values to obtain the final FP8 output.
Thisprocessintroducesasignificantamountofoverhead,whichcansubstantiallyreducethebenefits
of using FP8.
•Delayedscaling . Thisstrategyinvolvesselectingthescalingfactorbasedonthemaximumabsolute
values observed in a certain number of preceding iterations. This approach allows for the full
performancebenefitsofFP8computationbutnecessitatesthestorageofahistoryofmaximumvalues
as additional parameters of the FP8 operators.
Figure 9: Scaling gradients to fall within the representation range of the FP8 datatype.
21

--- PAGE 22 ---
A.3 Pre-training Data
Tab. 10presents anoverviewof ourcollecteddata sourcesalongwith thecorrespondingsampling weights
employed in pre-training. The arXiv and StackExchange subsets are collected from Redpajama (Redpa-
jama,2023),whileBookCorpus2(Zhuetal.,2015),Books3(Presser,2020),DM-Math(Saxtonetal.,2018),
Gutenberg (Rae et al., 2019), HackerNews6, NIH ExPorter7, OpenSubtitles (Tiedemann, 2016), and USPTO8
subsets are extracted from The Pile (Gao et al., 2020). The Wikipedia data is downloaded from HuggingFace
(HuggingFace, 2022). We use the 20220301 dump, including 24 languages: bg, ca, cs, da, de, en, es, fr, hi, hr,
hu, it, jp, ko, nl, pl, pt, ro, ru, sl, sr, sv, uk, zh.
We pre-process 11 CommonCrawl snapshots, ranging from 2018 to 2023, with the CCNet pipeline (Wenzek
etal.,2019). Thisprocessinvolvesdatadeduplicationatthelinelevel,followedbylanguageidentification
utilizingafastTextlinearclassifier(Joulinetal.,2017)toeliminatenon-Englishpages. Afilteringmechanism
basedonann-gramlanguagemodelisemployedtoexcludelow-qualitycontent. Inaddition,wetrainalinear
classifier (Redpajama, 2023) to distinguish documents similar to Wikipedia pages from randomly sampled
CommonCrawl documents. Documents not classified as resembling Wikipedia are excluded. Finally, we
perform fuzzy deduplication (Lee et al., 2022) across all the processed snapshots from CommonCrawl.
We collect Python code data from Github using a repository list provided by Bing indexing (Bing, 2022).
The cleaning of the code data includes three steps. First, we remove control characters, except for \tand\n.
Next, weremovecopyright commentsinthecode. Analphanumericratefilteristhenapplied, removing
lines with a rate below 0.5 if they are comments, and discarding the entire file if its overall alphanumeric rate
islessthan0.98. Fileswithlessthan5linesoramaximumlinelengthexceeding1,000charactersarealso
discarded. Also, files with an average line length of more than 100 characters are discarded. Lastly, a pattern
searchisconductedtoidentifykeyPythonkeywords( e.g.,import,from,def,class,if,for,try,etc.) withinthe
code. Filescontaininglessthan3instancesofthesekeywordsareeliminated. Thiscomprehensiveprocess
ensures that the remaining Python code data is of high quality and suitable for use in academic research. We
additionallyaddPythoncodefromStack(Kocetkovetal.,2022),andperformfuzzydeduplicationwithinall
the collected Python code.
6https://news.ycombinator.com
7https://exporter.nih.gov
8https://bulkdata.uspto.gov
22

--- PAGE 23 ---
Dataset Sampling prop. Epochs Training Tokens (Billion)
Web Crawls
CommonCrawl 51.71% 0.16 51.71
C4 25.56% 0.16 25.56
OpenWebText 2.73% 0.16 2.73
Technical & Science content
arXiv 1.54% 0.05 1.54
StackExchange 1.42% 0.08 1.42
DM-Math 0.39% 0.05 0.39
USPTO 0.52% 0.05 0.52
NIH ExPorter 0.04% 0.05 0.04
Programming Languages
Python 4.50% 0.11 4.50
Other Curated Sources
Wikipedia 4.50% 0.16 4.50
Books 4.50% 0.09 4.50
News 2.00% 0.11 2.00
Dialogue 2.00% 0.27 2.00
Total 100.00
Table 10: Pre-training data. For each subset we list the sampling weight, number of epochs, and training
tokens. BooksdataincludesBookCorpus2(Zhuetal.,2015),Books3(Presser,2020),andGutenberg(Rae
etal.,2019). Dialoguedata includesHackerNewsandOpenSubtitles(Tiedemann,2016). Forexperiments
with a training token count of less than 100 billion, we employ the same sampling proportion.
23
