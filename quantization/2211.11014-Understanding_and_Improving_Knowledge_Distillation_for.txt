# 2211.11014.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2211.11014.pdf
# File size: 2562845 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Understanding and Improving Knowledge Distillation for
Quantization-Aware Training of Large Transformer Encoders
Minsoo Kim1, Sihwa Lee2, Sukjin Hong3, Du-Seong Chang3, and Jungwook Choi1,2
1Department of Electronic Engineering, Hanyang University
2Department of Artiﬁcial Intelligence, Hanyang University
Seoul, Republic of Korea
{minsoo2333, macto94, choij}@hanyang.ac.kr ,
3KT, Seoul, Republic of Korea
{sukjin.hong, dschang}@kt.com
Abstract
Knowledge distillation (KD) has been a ubiq-
uitous method for model compression to
strengthen the capability of a lightweight
model with the transferred knowledge from the
teacher. In particular, KD has been employed
in quantization-aware training (QAT) of Trans-
former encoders like BERT to improve the ac-
curacy of the student model with the reduced-
precision weight parameters. However, little
is understood about which of the various KD
approaches best ﬁts the QAT of Transformers.
In this work, we provide an in-depth analysis
of the mechanism of KD on attention recovery
of quantized large Transformers. In particular,
we reveal that the previously adopted MSE loss
on the attention score is insufﬁcient for recov-
ering the self-attention information. Therefore,
we propose two KD methods; attention-map
and attention-output losses. Furthermore, we
explore the uniﬁcation of both losses to address
task-dependent preference between attention-
map and output losses. The experimental re-
sults on various Transformer encoder models
demonstrate that the proposed KD methods
achieve state-of-the-art accuracy for QAT with
sub-2-bit weight quantization.
1 Introduction
Knowledge distillation (KD) (Hinton et al., 2015)
is a transfer learning framework to pass on knowl-
edge of a large model (the teacher) to a lightweight
model (the student). Numerous KD methods have
been developed regarding the source of knowl-
edge and distillation objectives. In many cases,
Kullback-Leibler divergence (KL-Div) is used as
a default distillation objective to match the soft la-
bels of the teacher and the student (Hinton et al.,
2015; Sanh et al., 2019). But further studies on
KD suggest that internal representations also con-
vey the intermediate knowledge of the teacher (Sun
et al., 2019; Aguilar et al., 2020). Thus minimiz-
ing the distance (e.g., mean squared error, MSE)
Corresponding Authorof hidden state knowledge (HSK) of layers be-
tween the teacher and the student has also been
proposed (Sun et al., 2019; Liu et al., 2021a). KD
becomes an essential model compression technique
for efﬁciently deploying large-scale Transformer-
based language models. For example, a popular
Transformer encoder model, BERT, contains hun-
dreds of millions of parameters, incurring profound
memory and computation overhead (Devlin et al.,
2019). These large-scale models require extreme
compression to reduce the model footprint by 10
to 100 times. Therefore, extensive studies have
been accomplished to distill efﬁcient student mod-
els (Sanh et al., 2019; Sun et al., 2019; Jiao et al.,
2020; Wang et al., 2020, 2021), but their focus is
limited to achieving fewer parameters.
Quantization-aware training (QAT) stands out
for its recent success in reducing not only the mem-
ory requirements but also the computational com-
plexity of Transformer models (Bhandare et al.,
2019; Zafrir et al., 2019; Kim et al., 2021a). Al-
though QAT reﬂects quantization errors during the
forward pass computation of stochastic gradient de-
scent to train a model more robust to quantization
errors, quantizing weight parameters of Transform-
ers to a precision lower than 2-bits degrades the
accuracy. Therefore, many recent QAT techniques
employed the KD framework to distill the capa-
bility of the full-precision teacher to the student
model with reduced-precision parameters (Zhang
et al., 2020; Bai et al., 2021; Jin et al., 2021; Li
et al., 2022). However, little is understood about
which of the various KD approaches best ﬁts the
QAT of large Transformers. Without careful justi-
ﬁcation, most prior works adopted the layer-wise
distillation of the attention score and the Trans-
former output with the MSE loss in addition to the
basic KL-Div loss on the model output. Therefore,
it is unclear if such KD setting is the most helpful
for QAT on large-scale Transformer encoders like
BERT-Large.arXiv:2211.11014v1  [cs.CL]  20 Nov 2022

--- PAGE 2 ---
In this work, we provide an in-depth analysis of
KD on attention recovery for QAT of Transformers
in terms of the knowledge sources and the objec-
tives. We ﬁrst reveal that all-layer KD of the inter-
mediate Transformer layer is essential for QAT, in
contrast to the KD-based model compression. In
the case of BERT-Base, we further discover that
the KL-Div-based KD on attention-map (called
attention-map loss) outperforms the prior KD tech-
nique that takes MSE loss on the attention score.
However, the attention-map loss is insufﬁcient for
the large Transformer encoders since weight quan-
tization disrupts attention propagation for speciﬁc
NLP tasks when there are many layers. Therefore,
we devise an insightful KD, MSE loss on attention
output (called attention-output loss), and help pre-
serve attention recovery along with many layers.
The proposed attention-map and output losses and
their combination are evaluated on various Trans-
former encoder models (BERT-Base/Large and a
BERT-like Korean language model (ULM). The
experimental results demonstrate that the proposed
KD methods signiﬁcantly boost the model accu-
racy surpassing the state-of-the-art for QAT with
aggressive sub-2-bit weight quantization.
We summarize our contributions as follows:
•We improve the prior KD techniques for QAT
to boost the accuracy of large Transformer
encoders.
•We quantitatively reveal that the attention-
map loss (based on KL-Div) outperforms the
existing attention-score loss (based on MSE).
The proposed attention-map loss is particu-
larly beneﬁcial for the BERT-Base model.
•We discover the task-dependent attention char-
acteristics, particularly noticeable in BERT-
Large. In particular, we reveal that speciﬁc
tasks on large Transformers suffer homoge-
nization of attention output when weights are
quantized. We propose a new KD method,
attention-output loss , to address this issue.
•We further explore the potential of unifying
the attention-map and output losses to handle
task-dependent attention characteristics ubiq-
uitously.
•We evaluate the proposed KD methods on
various large-scale Transformer encoders and
NLP tasks, achieving state-of-the-art accuracy
for sub-2-bit aggressive QAT.
Figure 1: The Transformer layer architecture.
2 Related Work
2.1 Transformer Encoder Model
Transformer-based encoder models like BERT (De-
vlin et al., 2019) has been widely adopted for natu-
ral language processing (NLP) tasks such as ques-
tion answering and language inference. As Fig. 1
shows, these models are built with Transformer
layers consisting of Multi-Head Attention (MHA)
and Feed-Forward Network (FFN) (Vaswani et al.,
2017). The input to the l-th Transformer layer is
Xl2Rndwherenanddare the sequence length
and hidden state size, respectively. Let NHbe
the number of attention heads and dh=d=NH.
WQ
h;WK
h;WV
h2Rddhare the weight parame-
ters converting Xlinto Query ( Q=XlWQ
h), Key
(K=XlWK
h), and Value ( V=XlWV
h), respec-
tively. Then, attention score ( ASh=QK>), atten-
tion map ( AMh=Softmax h(AShp
d)), and attention
context ( ACh=AMhV).
MHA is deﬁned as:
MHA (Xl) =Concat (AC 1;:::ACNH)WO:(1)
Motivated by (Kobayashi et al., 2020), MHA can
be re-written per each token i:
MHA (Xl)(i) =nX
j=1i;jf(Xl(j)); (2)
wheref(x) := (xWV+bV)WOandi;jisj’th
attention probability of i’th token in AMh. There-
fore, MHA can be decomposed into two parts: self-
attention generation (SA-GEN) corresponding to

--- PAGE 3 ---
the attention map ( ), and self-attention propaga-
tion (SA-PROP) corresponding to f(x). Fig. 1
shows which part is SA-GEN and SA-PROP re-
spectively.
FFN consists of two fully-connected layers with
weight parameters W1andW2:
FFN(Yl) =GELU (YlW1+b1)W2+b2:(3)
Therefore, output of a Transformer layer Xl+1is
deﬁned as:
Yl=LayerNorm (Xl+MHA (Xl));
Xl+1=LayerNorm (Yl+FFN(Yl)):(4)
Here, YlandXl+1are called attention output ( AO)
and Transformer output, respectively.
2.2 Knowledge Distillation for Compression
of Transformer Models
Knowledge distillation (KD) (Hinton et al., 2015)
is a transfer learning framework that a lightweight
model (the student) learns from the knowledge
distilled from a cumbersome model (the teacher).
Since KD provides the student information to
reach the teacher’s capability, KD has been widely
adopted for model compression of large-scale
Transformer models like BERT. A basic distillation
approach is to match the probability distribution at
the output of the teacher and student models via
CE loss, as in DistilBERT (Sanh et al., 2019). In
addition to this soft-label distillation, PKD (Sun
et al., 2019) suggested KD on the normalized out-
put of each Transformer layer, as distillation on the
teacher’s intermediate representations can beneﬁt
the student. MobileBERT (Sun et al., 2020) also
employed per-head KD on the attention map and
customized architecture for efﬁcient Transformer
computations. MiniLM and MiniLMv2 (Wang
et al., 2020, 2021) further transferred relational
knowledge from the self-attention map, but only
at a single Transformer layer (located at the last or
upper-middle). (Liu et al., 2021a) further claimed
that distilling more intermediate representations
does not necessarily help improve the accuracy of
the student.
Although these KD-based compression tech-
niques have developed efﬁcient BERT structures,
there has been limited understanding of KD on the
model quantization. In particular, we are the ﬁrst
to quantitatively reveal that more distillation of the
intermediate representations helps QAT reduce the
accuracy gap between the quantized student and
the full-precision teacher.2.3 Quantization for BERT
Quantization is a promising technique to reduce the
high inference cost of large-scale models without
changing the model structure. Instead of repre-
senting numbers in 32-bit ﬂoating-point (FP32),
employing ﬁxed-point representation, such as 8-bit
integer (INT8) quantization, has achieved signiﬁ-
cant speedup and storage savings for BERT (Zafrir
et al., 2019; Kim et al., 2021a; Lin et al., 2021).
However, direct quantization of weight parameters
leads to degradation of the original model accuracy
when quantization bit-precision is low. Therefore,
quantization-aware training (QAT) is commonly
applied for ultra-low precision model quantization.
Recently, QAT has been applied for compressing
BERT with a precision lower than 2-bit. Ternary-
BERT (Zhang et al., 2020) represents each weight
parameter into one of three values f 1;0;1g.
TernaryBERT actively incorporates KD into QAT
for improving accuracy degradation. To reduce the
bit-precision, BinaryBERT (Bai et al., 2021) pro-
posed a modiﬁed QAT procedure with a speciﬁc
weight initialization for binary quantization. DQ-
BART (Li et al., 2022) further combined model
compression (via layer reduction) and quantization
by exploiting KD.
Although KD has been a de-facto technique for
QAT, there is a lack of understanding about why.
In particular, the aforementioned QAT methods all
employed the layer-wise KD on the self-attention
score ( ASl) and Transformer output ( Xl) along
with the KD on soft labels. Considering numer-
ous KD techniques with various choices for the
knowledge sources and the objective, it is not clear
if the current recipe helps QAT the most. This work
investigates the prior layer-wise KD techniques and
improves them with new objectives and knowledge
sources.
3 Prior KD Techniques for QAT
In this section, we investigate prior KD techniques
for QAT evaluated on BERT-Base. As discussed
earlier, KD techniques commonly used for QAT
include 1) all-layer distillation and 2) distillation
on SA-GEN. First, we provide justiﬁcation and
improvement on these techniques. Then we further
showcase the limitation when they are applied to
large-scale Transformer encoders.

--- PAGE 4 ---
Figure 2: Illustration of layer selection strategy in model
compression and model quantization. Left: Uniform
mapping strategy. Right: All-layer distillation.
3.1 All-Layer Distillation for QAT
Generally, the internal representation of the teacher,
such as a layer output, is widely used for knowl-
edge distillation for model compression (Aguilar
et al., 2020). However, there is a distinct difference
in KD between typical model compression and
quantization. For example, Fig. 2 shows two repre-
sentative layer-to-layer mapping for KD: selected-
layer distillation for model compression (left) and
all-layer distillation for QAT (right). In the case of
selected-layer distillation, the study showed that the
marginal utilities of hidden state knowledge (HSK)
diminish as more HSK has been distilled(Liu et al.,
2021a). In contrast, most prior QAT methods ap-
plied KD on the Transformer output of all the lay-
ers. The structural equivalence of the teacher and
the student of QAT methods makes this choice nat-
ural, but there is little justiﬁcation.
We conjecture that quantization applied to the
weight parameters disrupts the functionality of the
Transformer layer, necessitating layer-wise guid-
ance. To validate this conjecture, we conducted two
experiments. First, we compared the accuracy of
the uniformly selected layer distillation with a vary-
ing number of distilled layers. As shown in Fig. 3a,
the accuracy grows along with the number of dis-
tilled layers, and all-layer distillation signiﬁcantly
outperforms selected-layer distillation. In addition,
we compared the loss surface of the two distilla-
tion approaches after QAT in terms of Hessian max
eigenvalues (Park and Kim, 2022). We adopt single
layer selection distillation as a strategy of selected
layer distillation. In particular, we used the method
of selecting the 10th layer of BERT-Base model,
which was most helpful in performance as a single
layer selection distillation. As shown in Fig. 3b,
all-layer distillation shows smaller magnitudes of
Figure 3: Comparison of (a) RTE accuracy and (b)
Hessian max Eigenvalues Spectra between the selected-
layer distillation and all-layer distillation in BERT-Base.
Eigenvalues, indicating a smoother loss surface.
Therefore, we can conclude that the layer-wise dis-
tillation helps train the student with the quantized
weight parameters.
3.2 Improve KD on Self-Attention
Generation
We further investigate the objective of all-layer KD.
As discussed earlier, prior QAT methods employed
MSE loss on the attention score (called attention-
score loss) for all-layer KD, as follows:
Lscore =L 1X
l=0MSE(AST
l;ASS
l): (5)
Given that the attention map captures the correla-
tion of one token to all the others, it is essential to
maintain the relative importance of tokens. How-
ever, quantization in nature clamps and coarsely
represents the weight parameters, making attention
less distinguishable. We expect KD to help main-
tain disparity, but the attention-score loss is not a
proper objective since it mainly focuses on logit
matching (Kim et al., 2021b).
As an alternative, we propose to use the KL-
Div loss on the attention-map (called attention-map
loss) deﬁned as follows:
Lmap=1
NhnNhX
h=1nX
t=1DKL(AMT
l;h;tkAMS
l;h;t):
(6)
Assuming that the temperature hyper-parameter ( )
is one, KL-Div focuses on label-matching (Kim
et al., 2021b). Thus the relative importance of
attention across tokens is better maintained with
attention-map loss. Although the attention-map
loss was previously employed in model compres-
sion (Wang et al., 2021), we are the ﬁrst to quan-

--- PAGE 5 ---
Figure 4: The cover length ratio and ranking loss per
attention head in BERT-Base. X-axis: Number of atten-
tion heads. Quantized: Quantized model without apply-
ing QAT, KL-Div: attention-map loss, MSE: attention-
score loss
titatively reveal the beneﬁts of the attention-map
loss in the context of QAT.
We introduced two metrics that characterize
the attention map to evaluate the proposed KD
loss quantitatively. The cover length ratio cap-
tures the student’s attention map deviation from
the teacher’s based on Top-K token coverage. The
ranking loss (Liu et al., 2021b) shows the simi-
larity in the attention rankings of the teacher and
the student. Fig. 4 compares the cover length ra-
tio and the ranking loss of every attention head
of BERT-Base on SST-2 task (the overall trend
is the same for the other layers and tasks). As
shown in the ﬁgure, quantization signiﬁcantly in-
creases the cover length ratio and the ranking loss,
indicating that the relative ratings of the attention
are seriously distorted. The attention-score loss
helps reduce such distortion, yet spikes still ex-
ist. In contrast, attention-map loss successfully
suppresses the spikes, maintaining the relative im-
portance of the attention map. More details about
cover length ratio and ranking loss are described in
Appendix. A.1.
To further understand the impact of the objec-
tives of KD on the QAT accuracy, we conducted the
temperature sweep of KL-Div. Since the gradients
of KL-Div loss can be simpliﬁed into the gradients
of MSE loss when the temperature is sufﬁciently
large (Kim et al., 2021b), we can manage the be-
havior of KL-Div loss through sweeping the tem-
perature value ( ), where=1 and= inf resem-
ble the attention-map and attention-score losses,respectively. Table 1 shows the QAT accuracy of
BERT-Base on CoLA and STS-B with varying .
As shown in the table, the accuracy of the quan-
tized model increases as the loss term becomes sim-
ilar to the attention-map loss. Such performance
improvement supports our understanding that 1)
label matching is crucial for compensating QAT
on SA-GEN, and 2) the attention-map loss is more
effective for label matching.
KL Divergence Temperature Hyper-Parameter ( )
Task KL Div =1=5=10=20 MSE
CoLA 50.76 50.76 49.11 47.51 47.19 47.51
STS-B 87.78 87.69 87.20 87.19 87.29 87.55
Table 1: Attention based KD-QAT with KL Div Temper-
ature Hyper Parameter sweeping on CoLA and STS-B
tasks with BERT-Base (Each experiment is repeated 5
times.)
4 KD for QAT on Large Transformers
We extend the investigation of KD techniques to
QAT on large transformer models. In this section,
we ﬁrst reveal the limitation of the attention-map
loss due to the task-dependent characteristics. Then
we propose a new KD loss, the attention-output
loss, to address this challenge. Lastly, we propose
a combination of the two losses to handle task-
dependent characteristics.
4.1 Task-Dependent Characteristics
Although the same pre-trained models are em-
ployed for the downstream ﬁne-tuning, the charac-
teristics of attention vary depending on the tasks
(Kovaleva et al., 2019). Motivated by the discus-
sions of (Bondarenko et al., 2021) that outliers in
activations of residual connections (i.e., SA-PROP)
arrange speciﬁc attention patterns, we examine
these outliers via min-max curves at attention out-
put to understand task-dependent characteristics
for quantization.
Fig. 5 plots the dynamic range of the attention
output (Yl) across the input tokens. There are two
representative cases: Case-1) the task with distinct
attention values (especially for special tokens) such
as RTE, and Case-2) the task with homogeneous
attention values such as SST-2. Each case’s overall
attention characteristics are intensiﬁed as the model
size increases. For example, distinct features of
RTE’s attention become more drastic on BERT-
Large.

--- PAGE 6 ---
Since the quantization clamps and coarsely rep-
resents the values, it is challenging to maintain the
distinct attention for the tasks in Case-1. As dis-
cussed in Sec. 3.2, in the case of BERT-Base, the
attention-map loss was capable of recovering the
disparity in the attention (Fig. 5a-Top). However,
as shown in Fig. 5a-Below, the attention-map loss
failed to adjust the attention in the case of BERT-
Large.
We conjecture that the attention-map loss fails
due to the increased number of layers of BERT-
Large. We adopt the analysis framework of
(Kobayashi et al., 2020) to separately analyze the
layer-wise behavior of SA-GEN and SA-PROP.
Fig. 6 plots the average distance of SA-GEN
and SA-PROP of a special token [SEP] from the
teacher for RTE (Case-1) and SST-2 (Case-2) with
BERT-Large. Note that the attention-map loss sup-
presses the distance in SA-GEN. This suppression
of SA-GEN deteriorates the attention output (c.f.,
the attention-map loss is effective for SST-2). With
many layers, quantization along SA-PROP fails
KD with attention-map loss to recover distinctive
attention. Therefore, we need a new KD loss to
handle disruption from SA-PROP.
4.2 Attention Output Loss
Observations from Fig. 6 imply that SA-PROP be-
comes a disruption source for QAT of BERT-Large
on Case-1 tasks. One way to suppress the quantiza-
tion error along SA-PROP is to apply KD directly
to SA-PROP. Therefore, we devise a new KD loss,
the attention-output loss as follows:
Loutput =L 1X
l=0MSE(YT
l;YS
l): (7)
The beneﬁts of the attention-output loss are ap-
parent. As shown in Fig. 5a, the attention output
with the attention-output loss follows the distinc-
tive attention of the full-precision teacher. We can
understand the mechanism of the attention-output
loss via Fig. 6; the attention-output loss allows
modiﬁcation of SA-GEN to adjust the attention
map so that the resulting attention output matches
better with the teacher. Note that the change in
SA-GEN occurs at the upper layers of the Trans-
former models; thus, the attention-output loss is
more beneﬁcial for large Transformer models.
To further understand the task-dependent char-
acteristics, we empirically observe the attention-
output loss’s impact on the attention map’s self-
Figure 5: Comparison of per-token dynamic ranges
of attention output ( Yl) between BERT-Base (top) and
BERT-Large (bottom) for RTE and SST-2. Each pair
of curves delineates min-max values at the token’s at-
tention output. FP-min-max and Map/Output-min-max
correspond to the min-max curves of the teacher model
and the student with the attention map/output loss, re-
spectively.
attention probability. To quantify the modiﬁcation
in the attention map, we introduce the ranking ratio,
deﬁned as a ranking of the attention probability of
an individual token normalized by the sequence
length. Fig. 7 tracks the ranking ratio of the se-
lected tokens of full-precision teacher and quan-
tized student models per each head. In the case
of RTE (i.e., Case-1 task), QAT with the attention-
output loss exhibits rapid changes in ranking in a
speciﬁc direction toward reduced attention-output
loss. In the case of SST-2, however, the situation
is very different; rankings of the selected tokens
change signiﬁcantly regardless of KD for QAT.
Thus, KD on the attention output cannot drive the
rankings in any meaningful direction. These ob-
servations conﬁrm the importance of considering
task-dependent characteristics for successful KD
for QAT.

--- PAGE 7 ---
Figure 6: Average distance of SA-GEN(self-attention
probability - ) and SA-PROP(self-attention propaga-
tion -f(x)) from the teacher model in two tasks (RTE,
SST-2) with BERT-Large.
4.3 Uniﬁed Attention-Map and Output Loss
Considering task-dependent attention characteris-
tics of BERT-Large, we further explore the po-
tential of unifying the attention-map and output
losses for QAT. Note that the preference between
the attention-map and output losses varies accord-
ing to the model size (e.g., BERT-Base vs. Large)
and tasks (Case-1 vs. Case-2). As for exploration,
we formulate a uniﬁed attention-map and output
loss withas a mixing parameter as follows:
LSM 1=Lmap+Loutput;
LSM 2=Loutput +Lmap;
where2f0:1;0:2;0:3;:::; 0:9g:(8)
As will be discussed in Sec.5.2, the uniﬁed loss
can boost the accuracy of the best performing KD
loss (either the attention-map or output loss). As
applying this uniﬁed loss in KD-QAT, we identiﬁed
that every tasks has its own score favorable mixing
Figure 7: Ranking ratio per head (Last three layer’s 48
heads) on the SST-2 and RTE task with BERT-Large.
Left: Teacher Model, Right: Student Model
parameters which shows task-dependent character-
istics. Detailed mixing parameter information for
each task is in Appendix. A.3.
5 Experiments
5.1 Experimental Setup
We employ three Transformer encoder models
(BERT-Base, BERT-Large, ULM-Encoder-Large)
to evaluate the proposed KD methods. BERT (De-
vlin et al., 2019) consists of Transformer encoder
layer, ﬁnetuned for GLUE downstream tasks (De-
vlin et al., 2019). ULM-Encoder-Large (Seo
et al., 2022) is a Korean language model based
on T5 (Raffel et al., 2019), ﬁnetuned for KLUE
downstream tasks (Park et al., 2021a).
The conﬁguration of each model is as follows:
1.BERT-Base. It is a 12-layer Transformer en-
coder with a hidden dimension of 768 using
12 attention heads and contains about 110M
parameters.
2.BERT-Large. It is composed of 24 Trans-
former encoder layers, and uses a hidden di-
mension of 1024 with 16 attention heads. This
model contains about 340M parameters.
3.ULM-Encoder-Large. It also has the same
conﬁguration as BERT-large except for feed-
forward dimension, which is 2816 for ULM-
Encoder-Large while BERT-Large has 4096.
It contains about 280M parameters.
We initiate QAT from the task-speciﬁc ﬁne-
tuned models. Our experiments were performed
on A6000 GPUs. Our implementation is based on

--- PAGE 8 ---
GLUE Task RTEyCoLAySTS-BySST-2?QNLI?MNLI?QQP?MRPC A VG
(Dataset) (2.5k) (8.5k) (5.7k) (67k) (108k) (393k) (364k) (3.5k)
Full-Prec 73.28 58.04 89.24 92.09 91.32 84.37 89.30 87.77 83.39
Baseline 68.53 ±1.69 49.61 ±0.79 87.55 ±0.14 92.01 ±0.29 90.65 ±0.05 84.21 ±0.10 89.06 ±0.40 88.58 ±0.40 81.28
Map 70.39 ±0.78 50.40 ±1.03 87.78 ±0.15 92.13 ±0.22 90.98 ±0.17 84.31 ±0.10 89.22 ±0.40 88.07 ±0.40 81.66
Output 70.65 ±1.27 49.05 ±0.50 87.77 ±0.14 92.13 ±0.22 90.58 ±0.07 84.24 ±0.01 89.17 ±0.20 87.01 ±0.43 81.33
Map+Output 71.68 ±1.19 50.50 ±0.45 87.73 ±0.16 92.39 ±0.18 90.91 ±0.14 84.33 ±0.06 89.28 ±0.10 88.18 ±0.53 81.87
Table 2: BERT-Base: Performance of KD-QAT Results on GLUE benchmark (8-bit activation and ternary weight
quantization, the compression rate of quantized BERT-Base is 14.9x). Small dataset (under 10k) tasks are repeated
5 times; the others are repeated 3 times. yand?indicate Case-1 and Case-2 GLUE tasks respectively.
GLUE Task RTEyCoLAySTS-BySST-2?QNLI?MNLI?QQP?MRPC A VG
(Dataset) (2.5k) (8.5k) (5.7k) (67k) (108k) (393k) (364k) (3.5k)
Full-Prec 70.39 60.31 89.83 92.32 92.29 86.49 89.55 88.43 83.70
Baseline 65.02 ±1.40 52.87 ±0.99 88.75 ±0.09 91.82 ±0.22 91.87 ±0.15 85.70 ±0.17 89.29 ±0.07 89.26 ±0.54 81.84
Map 66.42 ±0.75 53.16 ±0.53 88.65 ±0.11 92.20 ±0.30 91.93 ±0.13 86.10 ±0.13 89.53 ±0.07 88.67 ±0.37 81.28
Output 69.50 ±1.20 54.71 ±0.71 89.10 ±0.08 92.13 ±0.26 91.92 ±0.13 86.22 ±0.05 89.44 ±0.09 88.75 ±0.71 82.72
Map+Output 68.83 ±1.45 54.69 ±1.08 88.85 ±0.15 92.30 ±0.11 92.16 ±0.15 86.36 ±0.06 89.48 ±0.06 88.64 ±0.79 82.66
Table 3: BERT-Large: Performance of KD-QAT Results on GLUE benchmark (8-bit activation and ternary weight
quantization, the compression rate of quantized BERT-Large is 15.4x). Small dataset (under 10k) tasks are repeated
5 times; the others are repeated 3 times. yand?indicate Case-1 and Case-2 GLUE tasks respectively.
the TernaryBERT PyTorch codebase.1All embed-
ding and weight parameters are ternarized and the
activations are quantized to 8-bit for QAT. we use
layer-wise ternarization for weights in Transformer
layers while row-wise for the word embedding,
same as TernaryBERT (Zhang et al., 2020). Also,
all the experiments are repeated 5 times, unless
stated otherwise.
For performance comparison, we consider the
following KD options:
•Baseline. The standard TernaryBERT with
the attention-score and Transformer output
loss along with KD on soft labels.
•Map. Use the attention-map loss instead of
the attention-score loss of TernaryBERT.
•Output. Use the attention-output loss instead
of the attention-score loss of TernaryBERT.
•Map+Output. Use the uniﬁed attention-map
and output loss instead of the attention-score
loss of TernaryBERT.
5.2 Experiments on BERT-Base and Large
Tables 2 and 3 show the result on the development
set across the GLUE benchmark. Notable observa-
1https://github.com/huawei-noah/
Pretrained-Language-Model/tree/master/
TernaryBERTtions are summarized as follows:
•The GLUE tasks can be categorized into two
cases. Case-1(y): RTE, CoLA, STS-B. Case-
2(?): SST-2, QNLI, MNLI, QQP.
•In the case of BERT-Base, attention-map loss
beneﬁts all the tasks in Case-1 and Case-2,
whereas attention-output loss is ineffective.
•In the case of BERT-Large, the attention-map
loss is marginally helpful for Case-1 and Case-
2, while the attention-output loss signiﬁcantly
boosts the accuracy of Case-1 tasks.
•Overall, the uniﬁed loss facilitates QAT accu-
racy, except for BERT-Large on Case-1 tasks
(in which the attention-output loss works the
best).
•MRPC is a corner case; the QAT accuracy
often outperforms the Full-Precision accuracy,
implying that quantization noise regularizes
the model favorably for this task.
5.3 Experiments on ULM-Encoder-Large
Table 4 summarizes the average results three times
of evaluating our KD methods for QAT on ULM-
Encoder-Large. Overall, ULM-Encoder-Large is
quite robust to quantization, but our KD methods

--- PAGE 9 ---
Task KLUE-TC KLUE-STS NSMC A VG
(Dataset) (45k) (11k) (150k)
Full-Prec 85.76 92.11 91.87 89.91
Baseline 85.56 ±0.08 91.04 ±0.10 91.13 ±0.04 89.24
Map 85.41 ±0.10 91.44 ±0.23 91.24 ±0.10 89.36
Output 85.63 ±0.23 91.03 ±0.11 91.39 ±0.15 89.35
Map + Output 85.57 ±0.21 91.11 ±0.14 91.65 ±0.12 89.44
Table 4: ULM-Large: Performance of KD-QAT Results
on KLUE and NSMC dev dataset
GLUE Task RTEyCoLAySTS-BySST-2?QNLI?
Full-Prec 70.39 60.31 89.83 92.32 92.29
Map 66.42 53.16 88.65 92.20 91.93
MHA loss 66.78 54.01 88.69 92.08 91.84
MHA loss + Residual 69.50 54.71 89.10 92.13 91.92
Table 5: Ablation study on the attention-output loss
decomposing its sources into MHA( Xl) and residual
path ( Xl) with BERT-Large on the GLUE tasks. †and
?indicate Case-1 and Case-2 GLUE tasks respectively.
surpass the baseline (= TernaryBERT). More specif-
ically, the attention-map loss is more effective on
the KLUE-STS task, while the output loss outper-
forms the map loss in KLUE-TC and NSMC, as
shown in the table. Furthermore, the uniﬁed loss
achieves the best accuracy on NSMC. Therefore,
the proposed KD losses could improve the accuracy
of the baseline QAT method.
5.4 Ablation Study
In Sec. 4.2, we proposed attention-output loss
to suppress the quantization error along the
SA-PROP. As shown in Fig. 1 and deﬁned in Eq. 4,
attention-output loss integrates two sources of
SA-PROP: MHA (Xl) and residual connection( Xl).
We investigate attention-output loss’s effectiveness
by employing one of its two parts solely as a loss
function objective: MHA (Xl) (We call this loss
function MHA loss). Speciﬁcally, MHA loss uses
MHA (Xl) as a objective of loss function instead of
Ylin Eq. 7.
Table. 5 shows that the MHA loss method im-
proves performance marginally in Case-1 tasks.
When the residual connection is added to the MHA
loss objective (MHA loss +Residual in Table. 5),
which is equivalent to attention-output loss, the per-
formance of all tasks increases. (especially in Case-
1 GLUE tasks). These observations indicate that
incorporating residual connection as an objective
of attention-output loss is signiﬁcant in recovering
disruption of SA-PROP under the quantization.6 Conclusion
In this work, we investigate the mechanism of
Knowledge distillation (KD) for QAT of large
Transformers. We propose two KD methods,
attention-map, and attention-output losses, to im-
prove the recovery of the self-attention informa-
tion. The experimental results on various Trans-
former encoder models demonstrate that the pro-
posed KD methods and their combination achieve
state-of-the-art accuracy for QAT with sub-2-bit
weight quantization. Our code is available at
https://github.com/MarsJacobs/kd-qat-large-enc.
7 Limitation
This work investigates how KD works for QAT on
Transformer Encoders. Although the analysis tech-
niques employed in this work reveal many exciting
insights, a more theoretical analysis of the impact
of quantization under KD would be highly appre-
ciated. Also, we explore the potential of unifying
the two proposed KD techniques; incorporating au-
tomatic balancing of the two (or more) KD losses
would be an interesting future research direction.
Acknowledgement
This work was partly supported by Institute of In-
formation & communications Technology Planning
& Evaluation (IITP) grants funded by the Korea
government(MSIT) (No. 2020-0-01373, Artiﬁcial
Intelligence Graduate School Program (Hanyang
University), and No. 2022-0-00971, Logic Synthe-
sis for NVM-based PIM Computing Architecture).
References
Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao,
Xing Fan, and Chenlei Guo. 2020. Knowledge dis-
tillation from internal representations. In The Thirty-
Fourth AAAI Conference on Artiﬁcial Intelligence,
AAAI 2020, The Thirty-Second Innovative Applica-
tions of Artiﬁcial Intelligence Conference, IAAI 2020,
The Tenth AAAI Symposium on Educational Advances
in Artiﬁcial Intelligence, EAAI 2020, New York, NY,
USA, February 7-12, 2020 , pages 7350–7357. AAAI
Press.
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin,
Xin Jiang, Qun Liu, Michael Lyu, and Irwin King.
2021. BinaryBERT: Pushing the limit of BERT quan-
tization. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 4334–4348, Online. Association for Computa-
tional Linguistics.

--- PAGE 10 ---
Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada,
Vivek Menon, Sun Choi, Kushal Datta, and Vikram
Saletore. 2019. Efﬁcient 8-bit quantization of trans-
former neural machine language translation model.
arXiv preprint arXiv:1906.00532 .
Yelysei Bondarenko, Markus Nagel, and Tijmen
Blankevoort. 2021. Understanding and overcoming
the challenges of efﬁcient transformer quantization.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
7947–7969, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.
TinyBERT: Distilling BERT for natural language un-
derstanding. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pages 4163–
4174, Online. Association for Computational Lin-
guistics.
Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, and
Zhiliang Gan. 2021. Kdlsq-bert: A quantized bert
combining knowledge distillation with learned step
size quantization. arXiv preprint arXiv:2101.05938 .
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W
Mahoney, and Kurt Keutzer. 2021a. I-bert: Integer-
only bert quantization. International Conference on
Machine Learning (Accepted) .
Taehyeon Kim, Jaehoon Oh, Nakyil Kim, Sang-
wook Cho, and Se-Young Yun. 2021b. Compar-
ing kullback-leibler divergence and mean squared
error loss in knowledge distillation. CoRR ,
abs/2105.08919.
Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and
Kentaro Inui. 2020. Attention is not only a weight:
Analyzing transformers with vector norms. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7057–7075, Online. Association for Computa-
tional Linguistics.
Olga Kovaleva, Alexey Romanov, Anna Rogers, and
Anna Rumshisky. 2019. Revealing the dark secrets
of BERT. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages4365–4374, Hong Kong, China. Association for Com-
putational Linguistics.
Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati,
Parminder Bhatia, Andrew Arnold, Bing Xiang, and
Dan Roth. 2022. Dq-bart: Efﬁcient sequence-to-
sequence model via joint distillation and quantization.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers) , pages 203–211.
Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and
Shuchang Zhou. 2021. Fq-vit: Fully quantized vi-
sion transformer without retraining. arXiv preprint
arXiv:2111.13824 .
Yuanxin Liu, Fandong Meng, Zheng Lin, Weiping
Wang, and Jie Zhou. 2021a. Marginal utility dimin-
ishes: Exploring the minimum knowledge for BERT
knowledge distillation. CoRR , abs/2106.05691.
Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei
Ma, and Wen Gao. 2021b. Post-training quantiza-
tion for vision transformer. In Advances in Neural
Information Processing Systems .
Ilya Loshchilov and Frank Hutter. 2017. Fixing
weight decay regularization in adam. CoRR ,
abs/1711.05101.
Namuk Park and Songkuk Kim. 2022. How do vision
transformers work? In International Conference on
Learning Representations .
Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik
Cho, Ji Yoon Han, Jangwon Park, Chisung Song,
Junseong Kim, Youngsook Song, Taehwan Oh, et al.
2021a. Klue: Korean language understanding evalua-
tion. In Thirty-ﬁfth Conference on Neural Informa-
tion Processing Systems Datasets and Benchmarks
Track (Round 2) .
Sungjoon Park, Jihyung Moon, Sungdong Kim, Won-Ik
Cho, Jiyoon Han, Jangwon Park, Chisung Song, Jun-
seong Kim, Yongsook Song, Tae Hwan Oh, Joohong
Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong,
Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyun-
woo Kim, Myeonghwa Lee, Seongbo Jang, Seung-
won Do, Sunkyoung Kim, Kyungtae Lim, Jongwon
Lee, Kyumin Park, Jamin Shin, Seonghyun Kim,
Eunjeong Lucy Park, Alice Oh, Jung-Woo Ha, and
Kyunghyun Cho. 2021b. KLUE: korean language
understanding evaluation. CoRR , abs/2105.09680.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2019. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. CoRR , abs/1910.10683.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 .

--- PAGE 11 ---
Youngkyung Seo, Jehoon Lee, TaeHyeong Kim, Jun-
seok Koh, Hwijung Ryu, and Du-Seong Chang.
2022. External Knowledge-based Language Model
Enhancement. GTC.
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
CoRR , abs/1804.04235.
Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.
Patient knowledge distillation for BERT model com-
pression. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
4323–4332, Hong Kong, China. Association for Com-
putational Linguistics.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020. MobileBERT:
a compact task-agnostic BERT for resource-limited
devices. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 2158–2170, Online. Association for Computa-
tional Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems , pages 5998–6008.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,
and Furu Wei. 2021. MiniLMv2: Multi-head self-
attention relation distillation for compressing pre-
trained transformers. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 2140–2151, Online. Association for Computa-
tional Linguistics.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. In Proceedings of the
34th International Conference on Neural Information
Processing Systems , NIPS’20, Red Hook, NY , USA.
Curran Associates Inc.
Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8bert: Quantized 8bit bert. 2019
Fifth Workshop on Energy Efﬁcient Machine Learn-
ing and Cognitive Computing - NeurIPS Edition
(EMC2-NIPS) , pages 36–39.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, and Qun Liu. 2020. TernaryBERT:Distillation-aware ultra-low bit BERT. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
509–521, Online. Association for Computational Lin-
guistics.

--- PAGE 12 ---
A Appendix
A.1 Cover Length Ratio and Ranking Loss
As shown in Sec. 3.2, cover length ratio and ranking
loss are metrics indicating how much the relative
importance within the attention map has deviated
under the quantization. To obtain the cover length
ratio, ﬁrst, sort the student and teacher’s attention
map in probability order. Then we can get the
teacher map’s Top-K tokens that receive the most
attention in the teacher attention map. After that,
we ﬁnd out how many tokens we need to look at
to get all the teacher map’s Top-K tokens from the
sorted student map. The number of tokens that
each token has to look at for covering the teacher
map’s Top-K tokens is called cover length, and we
normalize cover length by sequence length. We
call this metric cover length ratio.
(Liu et al., 2021b) introduced pairwise ranking
loss to keep the relative order of attention values.
Pairwise ranking loss indicates how different the
order of attention importance is between two atten-
tion maps. With any of the two tokens, if the order
of the two tokens in the student map is different
from that in the teacher map, we add the difference
between the values of the two tokens to the loss.
Lh
ranking
=n 1X
i=1nX
j=i+1((AMS
i AMS
j)
sign(AMT
i AMT
j));
Lranking =NHX
h=1Lh
ranking;(9)
wherehis an index of attention head.
A.2 Visualization of Attention Map
We compare the self-attention map of ﬁne-tuned
full-precision BERT-base model and the quantized
model on the RTE task. Fig. 8 shows self-attention
maps from the 3rd Transformer layer (8th head) of
BERT-Base. Note that applying quantization dis-
torts the self-attention map of the teacher severely.
As shown in the ﬁgure, attention-map loss success-
fully recovers the self-attention map of the teacher,
whereas TernaryBERT fails to capture some dis-
tinctive features.
Figure 8: Visualization of self-attention map with BERT-
Base over RTE Task (a) Teacher self-attention map (b)
After quantization without QAT (c) After TernayBERT
KD-QAT (d) Attention-map loss
A.3 Uniﬁed Attention-Map and Output Loss
Exploration
As mentioned in Sec. 4.3, we explore the effec-
tiveness of unifying the attention map and output
losses for QAT. We conducted the experiments by
ﬁxing one of two losses, attention-map, and output
loss, and changing the mixing parameter for an-
other loss according to Eq. 8. Table 7 and 8 show
the best results changing mixing parameter in
BERT-Base and BERT-Large. Overall, maintain-
ing attention-map loss (case SM 1) perform better
than opposite cases (case SM 2). To show the de-
gree of effect according to mixing parameter , we
summarize the showing the best result for each
case in Table 6. The table shows that every task has
its own score favorable mixing parameter . These
trends can be connected to the task-dependent at-
tention characteristics in Sec 4.1.
GLUE BERT-Base BERT-Large
TaskSM 1SM 2SM 1SM 2
RTE 0.6 0.4 0.1 0.9
CoLA 0.4 0.2 0.1 0.5
STS-B 0.7 0.2 0.3 0.9
MRPC 0.1 0.1 0.1 0.9
SST-2 0.2 0.7 0.3 0.1
QNLI 0.8 0.8 0.8 0.6
MNLI 0.5 0.1 0.8 0.7
QQP 0.1 0.1 0.5 0.6
Table 6: Results of Map+Output mixing parameter 
exploration

--- PAGE 13 ---
GLUE Task RTE CoLA STS-B MRPC SST-2 QNLI MNLI QQP A VG
Full-Prec 73.28 58.04 89.24 87.77 92.09 91.32 84.37 89.30 83.39
SM 1 71.68 ±1.19 50.50 ±0.45 87.73 ±0.16 88.18 ±0.53 92.39 ±0.18 90.90 ±0.14 84.33 ±0.06 89.28 ±0.10 81.87
SM 2 71.48 ±0.96 50.10 ±1.02 87.71 ±0.09 88.22 ±0.39 92.32 ±0.11 90.91 ±0.07 84.24 ±0.04 89.22 ±0.08 81.83
Table 7: BERT-Base Map+Output performance results on GLUE benchmark. Small dataset (under 10k) tasks are
repeated 5 times; the others are repeated 3 times.
GLUE Task RTE CoLA STS-B MRPC SST-2 QNLI MNLI QQP A VG
Full-Prec 70.39 60.31 89.83 88.43 92.32 92.29 86.49 89.55 83.70
SM 1 68.83 ±1.45 54.69 ±1.08 88.85 ±0.15 88.64 ±0.79 92.31 ±0.11 92.16 ±0.15 86.32 ±0.03 89.48 ±0.05 82.64
SM 2 68.23 ±1.06 54.44 ±0.46 88.93 ±0.08 88.24 ±0.26 92.20 ±0.30 92.16 ±0.07 86.36 ±0.06 89.45 ±0.03 82.46
Table 8: BERT-Large Map+Output performance results on GLUE benchmark. Small dataset (under 10k) tasks are
repeated 5 times; the others are repeated 3 times.
A.4 Experimental Setup
Datasets
We evaluate our method on all datasets of the
GLUE benchmark (Wang et al., 2018) for BERT,
and two datasets of the KLUE (Park et al., 2021b),
which is one of the datasets to evaluate the natu-
ral language understanding capability of Korean
language models, and sentiment analysis NSMC2
datasets. Details are as follows.
1.GLUE. The General Language Understand-
ing Evaluation is a collection of resources for
training, evaluating, and analyzing natural lan-
guage understanding systems.
2.KLUE-TC. The KLUE Topic Classiﬁcation
is a single sentence classiﬁcation task, and
it classiﬁes which topic the input sentence
belongs to among the 7 representative topics.
We averaged the accuracy and F1 score as the
metric.
3.KLUE-STS. The KLUE Semantic Textual
Similarity is to measure the degree of seman-
tic similarity between two Korean sentences.
We averaged the Pearson Correlation Coefﬁ-
cient (PCC) and Spearman Correlation Coefﬁ-
cient (SCC) to measure the performance.
4.NSMC. The NA VER Sentiment Movie Cor-
pus has a collection of movie reviews scraped
from NA VER Movies3, including an annota-
tion of whether the evaluation of the movie is
positive or negative.
2https://github.com/e9t/nsmc
3https://movie.naver.com/movie/point/af/list.
naverTraining Settings
For evaluating our methods, we use batch size 16
for CoLA and 32 for other GLUE tasks. The learn-
ing rate starts from zero and gradually increases
to 2e-5 during the warm-up stage and decays lin-
early to 2e-9 for 3 epochs. The dropout proba-
bility was always kept at 0.1. For an optimizer,
we use BertAdam4, which is a variant of Adam.
For ULM-Large, we train for 10 epochs using
AdamW (Loshchilov and Hutter, 2017) for KLUE-
TC and Adafactor (Shazeer and Stern, 2018) for
KLUE-STS and NSMC. We empirically ﬁnd the
best hyperparameter setting for each task in the
following choices:
•Batch size: 16, 32, 64
•Learning rate: 1e-5, 2e-5, 5e-5
4https://github.com/huggingface/transformers/
blob/v0.6.2/pytorch_pretrained_bert/optimization.
py
