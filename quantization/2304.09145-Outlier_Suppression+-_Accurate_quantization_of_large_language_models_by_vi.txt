# 2304.09145.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2304.09145.pdf
# Kích thước tệp: 1534917 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Outlier Suppression+: Lượng tử hóa chính xác các mô hình ngôn ngữ lớn bằng phép dịch chuyển và tỷ lệ tương đương và hiệu quả
Xiuying Wei1, 2, 3, Yunchen Zhang2, 5, Yuhang Li4, Xiangguo Zhang2,
Ruihao Gong1, 2∗, Jinyang Guo1, Xianglong Liu1
1Phòng thí nghiệm trọng điểm nhà nước về môi trường phát triển phần mềm, Đại học Beihang
2SenseTime Research3Trường Khoa học Máy tính và Truyền thông, EPFL
4Đại học Yale,5UESTC
xiuying.wei@epfl.ch, yuhang.li@yale.edu, {jinyangguo, xlliu}@buaa.edu.cn
{zhangyunchen, zhangxiangguo, gongruihao}@sensetime.com

Tóm tắt
Lượng tử hóa hậu huấn luyện (PTQ) của các mô hình ngôn ngữ transformer phải đối mặt với những thách thức đáng kể do sự tồn tại của các outlier có hại trong activations. Chúng tôi quan sát thấy rằng các outlier này tập trung trong các kênh cụ thể và không đối xứng giữa các kênh. Để giải quyết vấn đề này, chúng tôi đề xuất khung Outlier Suppression+ (OS+), bao gồm phép dịch chuyển theo kênh cho tính không đối xứng và phép tỷ lệ theo kênh cho tập trung. Chúng tôi chỉ ra rằng các phép toán này có thể được di chuyển một cách liền mạch vào các module tiếp theo trong khi vẫn duy trì tính tương đương. Thứ hai, chúng tôi đề xuất một sơ đồ nhanh và ổn định để tính toán các giá trị dịch chuyển và tỷ lệ hiệu quả. Phép dịch chuyển theo kênh căn chỉnh tâm của mỗi kênh để loại bỏ tính không đối xứng của outlier. Phép tỷ lệ theo kênh đánh giá định lượng các thay đổi do di chuyển và lượng tử hóa mang lại để cân bằng tải lượng tử hóa tốt hơn. Chúng tôi xác thực OS+ của mình trong cả hai cài đặt lượng tử hóa tiêu chuẩn và tinh vi với các mô hình bao gồm BERT, OPT, BLOOM, BLOOMZ và LLaMA. Kết quả toàn diện trên các tác vụ khác nhau chứng minh tính ưu việt của phương pháp của chúng tôi. Đặc biệt, với lượng tử hóa tiêu chuẩn, OS+ có thể đạt được hiệu suất gần floating-point trên cả mô hình nhỏ và mô hình ngôn ngữ lớn ở 8-bit và 6-bit. Bên cạnh đó, chúng tôi thiết lập một trạng thái nghệ thuật mới cho BERT 4-bit với cải thiện 15.5%. Mã của chúng tôi có sẵn tại https://github.com/ModelTC/Outlier_Suppression_Plus.

1 Giới thiệu
Các mô hình ngôn ngữ Transformer (ví dụ: BERT, LLMs) đã thu hút sự chú ý đáng kể do hiệu suất đáng chú ý và kích thước mô hình có thể mở rộng của chúng. Các mô hình này đã phát triển từ hàng trăm triệu tham số (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2018) đến hàng trăm tỷ tham số (Brown et al., 2020; Zhang et al., 2022; Smith et al., 2022). Điều này đòi hỏi việc sử dụng các kỹ thuật nén (Han et al., 2015; Hinton et al., 2015; Zoph and Le, 2016; LeCun et al., 1989) để triển khai thực tế. Trong số các kỹ thuật này, lượng tử hóa (Jacob et al., 2018) đã nổi lên như một mô hình tổng quát và chính để giảm cả dung lượng bộ nhớ và chi phí tính toán.

Tuy nhiên, lượng tử hóa, đặc biệt là lượng tử hóa hậu huấn luyện (Choukroun et al., 2019; Banner et al., 2018; Wu et al., 2020) trong bối cảnh dữ liệu và tài nguyên GPU hạn chế, đã trở nên ngày càng thách thức trên các mô hình này (ví dụ: giảm 12% độ chính xác trong BERT (Bondarenko et al., 2021) và suy giảm thảm khốc trong OPT-175B (Dettmers et al., 2022)). Điều này được gây ra bởi sự hiện diện của các outlier có hại trong activation (ví dụ: phạm vi phân phối có thể là 80 trong BERT và thậm chí 140 trong OPTs), điều này ngăn cản các số rời rạc biểu diễn chính xác các số liên tục.

Để chống lại nút thắt cổ chai, các nhà nghiên cứu thực hiện điều tra sâu và phát hiện rằng outliers chủ yếu tập trung trong các kênh nhất định. Một số công trình (Bondarenko et al., 2021; Dettmers et al., 2022) đề xuất các sơ đồ lượng tử hóa tinh vi và cung cấp các mức bit bổ sung cho các kênh outlier. Những công trình khác (Wei et al., 2022b; Xiao et al., 2022) thực hiện tỷ lệ activation để tỷ lệ outliers và di chuyển các giá trị tỷ lệ đến các trọng số tiếp theo để có tính tương đương FP. Tuy nhiên, cách tiếp cận trước có thể làm tổn hại hiệu ứng tăng tốc lượng tử hóa trong khi cách sau xác định các giá trị tỷ lệ mà không xem xét việc giảm thiểu thay đổi được giới thiệu bởi di chuyển và lượng tử hóa, điều mà chúng tôi thấy là không tối ưu. Trong khi đó, chúng tôi cũng xác định một đặc điểm outlier mới mà các công trình trước đây đã bỏ qua nhưng cũng chịu tr책任 cho phạm vi tensor lớn.

Trong bài báo này, chúng tôi đề xuất khung Outlier Suppression+ bao gồm phép dịch chuyển theo kênh

--- TRANG 2 ---
và tỷ lệ để hiệu quả theo đuổi hiệu suất lượng tử hóa tốt hơn trong khi tương đương giữ đầu ra FP. Đầu tiên, chúng tôi tìm thấy một tính năng mới của outliers rằng chúng ở dạng không đối xứng giữa các kênh (ví dụ: trong Hình 1a, một kênh có vấn đề trên OPT-66B chiếm trục âm từ -97 đến -58 trong khi một kênh khác có các giá trị dương từ 5.7 đến 43). Cách trình bày outlier không đối xứng này có thể gây ra phân phối tensor rất rộng như 140 ngay cả khi được cấu thành từ các kênh có phạm vi tương đối nhỏ như 39. Do đó, chúng tôi đề xuất phép toán dịch chuyển theo kênh, có thể loại bỏ tác động của tính không đối xứng bằng cách thực hiện phép toán sau:

fX′=X−z, (1)

trong đó z đóng vai trò như một vector hàng (z∈Rn) và dịch chuyển activation cho mỗi kênh. Bằng cách này, với z được thiết kế cẩn thận mà chúng tôi sẽ giới thiệu trong Mục 4.2.1, tensor mới fX′ có thể loại bỏ thuộc tính không đối xứng outlier. Ví dụ, bằng cách căn chỉnh tâm của mỗi kênh trong Hình 1b, phạm vi có thể được giảm xuống 40 (phạm vi kênh tối đa) từ 140 (phạm vi tensor lớn). Cuối cùng, lưu ý rằng phép toán này không phải là phép toán dịch chuyển thông thường cho lượng tử hóa đối xứng, vì nó hoạt động theo kênh và cung cấp phân phối tốt hơn cho lượng tử hóa per-tensor.

Tỷ lệ theo kênh. Ngoài tính năng không đối xứng giữa các kênh, cũng tồn tại hiện tượng tập trung outlier (Wei et al., 2022b) rằng outliers chủ yếu tích tụ trong các kênh cụ thể trên các đầu vào khác nhau. Ví dụ, kênh thứ 8725 và kênh thứ 6354 trong Hình 1a có các giá trị hung dữ hơn những kênh khác. Do đó, sau khi dịch chuyển, chúng tôi trang bị tỷ lệ theo kênh để thu hẹp chúng xuống để giảm bớt thêm khó khăn lượng tử hóa.

fX= (X−z)⊘s. (2)

Trong phương trình trên, vector hàng s∈Rn tỷ lệ tensor đã dịch chuyển cho mỗi kênh và mang lại activation thân thiện với lượng tử hóa cuối cùng fX. Ví dụ, trong Hình 1c, một tensor có kích thước 10 có thể được thu được nếu chúng ta tỷ lệ xuống các kênh có tín hiệu trên 5. Tính toán chi tiết của s sẽ được đưa ra trong Mục 4.2.2.

Triển khai. Dễ dàng triển khai các phép toán này. Lấy đầu ra của LayerNorm Hình 2 làm ví dụ, chúng ta chỉ cần thay thế các tham số biến đổi tuyến tính β và γ bằng (β−z)⊘s và γ⊘s để đạt được hiệu ứng dịch chuyển và tỷ lệ. Đối với những thứ khác, chúng ta có thể cập nhật các tham số trong hàm DeQuant trước đó.

4.1.2 Mô hình di chuyển thống nhất
Như đã đề cập trong Phương trình (1) và Phương trình (2), chúng tôi trừ z và chia s để làm cho activation có vấn đề kháng với lượng tử hóa. Để giữ một mô hình FP tương đương, một mô hình di chuyển thống nhất được đề xuất để chuyển cả vector dịch chuyển và tỷ lệ đảo ngược đến các module tiếp theo. Chúng tôi chứng minh tính khả thi của thuật toán này trên hai cấu trúc phổ biến.

Lớp tuyến tính. Đầu tiên, chúng tôi xem xét một tình huống phổ biến trong đó một lớp tuyến tính (tích chập) theo ngay sau đó. Đảo ngược các phép toán trên (tức là (fX⊙s+z)W⊤+b) bằng cách cập nhật W∈Rm,n và b∈Rm trong lớp tiếp theo, được cho bởi

(fX⊙s+z)W⊤+b
= (fX⊙s)W⊤+zW⊤+b
=fX(W⊤⊙s⊤) + (zW⊤+b).(3)

Theo Phương trình (3), trọng số và bias có thể hấp thụ s và z, tương ứng, và do đó trở thành:

fW=W⊙[s1s2...sn; s1s2...sn; ...; s1s2...sn],
eb=zW⊤+b.(4)

Ví dụ, Hình 2(a) mô tả activation thách thức điển hình (đầu ra của LayerNorm) trong cấu trúc attention, tất cả các trọng số và bias theo sau có thể hấp thụ các tín hiệu dịch chuyển và tỷ lệ mà không có bất kỳ gánh nặng tính toán bổ sung nào.

Kết nối dư. Thứ hai, chúng tôi xem xét trường hợp trong đó một kết nối dư được áp dụng sau cấu trúc LayerNorm (Post-LN) và được đưa vào đầu vào được lượng tử hóa. Như được hiển thị trong Hình 2b, ngoài biến đổi lớp tuyến tính, hàm nhận dạng sẽ được thay thế bằng phép nhân và phép cộng theo kênh để duy trì tính tương đương. Chúng tôi chứng minh rằng những tính toán tăng thêm này sẽ chỉ tạo ra gánh nặng suy luận không đáng kể trong Mục 5.5.

Cuối cùng, vì s và z đóng vai trò như các tham số được chia sẻ giữa các token và batch dữ liệu, mô hình di chuyển thống nhất có thể được triển khai tốt và tạo ra cùng một đầu ra mà không có tính toán bổ sung hầu hết thời gian.

4.2 Dịch chuyển và tỷ lệ hiệu quả
Dựa trên các phép toán dịch chuyển và tỷ lệ tương đương, trong phần này, chúng tôi đề xuất một sơ đồ nhanh và ổn định để theo đuổi các giá trị hiệu quả.

4.2.1 Giá trị dịch chuyển
Thiết kế của vector dịch chuyển nên loại bỏ tác động của tính không đối xứng giữa các kênh. Do đó, chúng tôi thiết kế để căn chỉnh tâm của mỗi kênh về 0 để kênh outlier sẽ không chỉ chiếm phía dương hoặc âm. Chi tiết, z được định nghĩa là trung bình của tín hiệu tối thiểu và tối đa trong mỗi kênh, được cho bởi:

zj=max(X:,j) + min( X:,j)/2, (5)

Với dịch chuyển theo kênh bây giờ, phạm vi tensor giảm xuống phạm vi kênh lớn nhất, loại bỏ việc bị định nghĩa bởi các outlier không đối xứng.

4.2.2 Giá trị tỷ lệ
Thiết kế của vector tỷ lệ nên tỷ lệ xuống outliers thêm trong khi mang tác động biên lề đến lượng tử hóa trọng số tiếp theo. Các phần sau giới thiệu cách có được nó với mục tiêu tối ưu hóa và quy trình được đề xuất.

Thách thức. Nhớ lại rằng biến đổi tương đương Phương trình (4) cũng tỷ lệ trọng số và có thể dẫn đến lượng tử hóa trọng số kém hơn, điều này đòi hỏi chúng ta tính toán các giá trị tỷ lệ tinh vi để đạt được cân bằng lượng tử hóa giữa activation và trọng số. Tuy nhiên, chúng tôi thấy các công trình trước đây (Wei et al., 2022b; Xiao et al., 2022) hoặc bỏ qua trọng số tiếp theo bị ảnh hưởng hoặc thực hiện một cách heuristic đơn giản bằng cách cân bằng phạm vi của activation và trọng số. Không giống như họ, chúng tôi nghĩ điểm chính là giảm thiểu thay đổi đầu ra tương tác của chúng do di chuyển và lượng tử hóa (một phân tích chi tiết có sẵn trong Bảng 6). Do đó, một mục tiêu tối ưu hóa mới được đề xuất.

Mục tiêu tối ưu hóa. Chúng tôi đầu tiên nghiên cứu trường hợp đơn giản rằng activation có vấn đề hoạt động như đầu vào của một lớp tuyến tính (ví dụ: Hình 2b). Thay vì giảm thiểu lỗi lượng tử hóa của activation và trọng số riêng biệt (tức là minsE∥Q((X−z)⊘s)−(X−z)⊘s∥2F và minsE∥Q(W⊙s)−W⊙s∥2F), một quan điểm mất mát tác vụ được áp dụng bằng cách quan tâm đến đầu ra nhân ma trận của chúng. Chúng tôi đo lường thay đổi đầu ra sau khi tỷ lệ và lượng tử hóa trọng số và activation để theo đuổi các yếu tố hiệu quả, được cho bởi:

minsE[∥Q((X−z)⊘s)Q(W⊙s)⊤+eb|{z}đầu ra sau tỷ lệ và lượng tử hóa −(XW⊤+b)|{z}đầu ra FP gốc∥2F],(6)

trong đó sai số bình phương trung bình (MSE) được sử dụng để lượng hóa sự khác biệt.

Nhiều lớp tuyến tính: Hơn nữa, chúng tôi nghiên cứu trường hợp cho nhiều lớp tuyến tính như cấu trúc attention (Hình 2a), trong đó ba trọng số sẽ được nhân với cùng một vector tỷ lệ và được tính toán với cùng activation bị khoanh.

Trong tình huống này, các đầu ra nhân ma trận của chúng được tạo ra bởi các ma trận được tỷ lệ và lượng tử hóa được đánh dấu là eQq, fKq, eVq, (Các đầu ra gốc được ký hiệu là Q,K,V). Áp dụng Phương trình (6) cho ba lớp tuyến tính riêng biệt và đơn giản cộng các mất mát có thể làm cho việc minh họa tầm quan trọng và cách sử dụng khác nhau của chúng trở nên khó khăn. Do đó, chúng tôi sử dụng cơ chế attention như một hàm hậu xử lý để tổ chức hợp lý thông tin tỷ lệ và lượng tử hóa của chúng, được cho bởi:

minsE[∥softmax( eQqfK⊤q)eVq−softmax( QK⊤)V∥2F]. (7)

Chuẩn hóa và masking được bỏ qua để đơn giản ký hiệu, và có thể thấy rằng thông tin từ hai lớp tuyến tính đầu tiên đã được đóng gói trong bản đồ attention.

Quy trình tối ưu hóa. Hướng tới mục tiêu trên, một quy trình nhanh và ổn định được giới thiệu để tìm kiếm vector tỷ lệ. Đầu tiên, chúng tôi thấy rằng việc tỷ lệ xuống chỉ các kênh có outliers có thể mang lại hiệu suất tốt hơn. Bởi vì các kênh có activations bình thường có thể thể hiện nhiều biến thiên hơn trên các đầu vào khác nhau, có thể khó tìm một giá trị tỷ lệ phù hợp cho chúng. Ngoài ra, xem xét rằng chúng không chịu trách nhiệm cho hiệu suất lượng tử hóa thấp, việc tỷ lệ chúng là không cần thiết. Thứ hai, chúng tôi đề xuất tối ưu hóa một biến thay thế gọi là ngưỡng outlier t, sẽ nén chỉ các kênh có phạm vi activation trên t vào (−t, t) và giữ nguyên những kênh khác (Hình 2). Về cơ bản, có được sử dụng để chỉ định kênh nào để tỷ lệ xuống, phạm vi activation được tỷ lệ cuối cùng, cũng như các giá trị tỷ lệ trong các trọng số sau.

Kỹ thuật này đơn giản hóa vấn đề phức tạp với nhiều biến s thành một biến duy nhất t. Sau đó chúng tôi áp dụng tìm kiếm lưới đơn giản cho t để giảm thiểu mục tiêu Phương trình (6), Phương trình (7). Sau khi có được t hiệu quả, vector tỷ lệ được tính toán như:

sj= max(1.0,max(X:,j−zj)/t). (8)

5 Thí nghiệm
Các đánh giá được thiết kế để hiển thị: I. dự đoán thỏa đáng của OS+ của chúng tôi cho cả mô hình ngôn ngữ nhỏ và lớn với lượng tử hóa tiêu chuẩn; II. hiệu suất nhất quán của OS+ trên thậm chí bit thấp hơn với lượng tử hóa tinh vi; III. nghiên cứu loại bỏ; III. phân tích như độ phức tạp tính toán.

5.1 Thiết lập
Cài đặt lượng tử hóa. Cả lượng tử hóa tiêu chuẩn và tinh vi đều được xem xét. Đối với cái tiêu chuẩn, chúng tôi thực hiện các nút lượng tử hóa giống như trong Wei et al. (2022b); NVIDIA (2022), luôn áp dụng lượng tử hóa activation per-tensor, xem xét lượng tử hóa trọng số per-tensor (tốc độ nhanh nhất) và per-channel (hiệu suất cao). Đối với lượng tử hóa tinh vi, chúng tôi áp dụng lượng tử hóa per-token (Yao et al., 2022) và per-group (Yao et al., 2023).

Ký hiệu: Chúng tôi sử dụng INT8, INT6, INT4 để biểu thị độ rộng bit của activation và trọng số. Cụ thể, INT8* đề cập đến lượng tử hóa trọng số per-tensor. Và lượng tử hóa per-token và per-group sẽ được đánh dấu trong bảng dưới đây.

Mô hình và tác vụ. Chúng tôi tiến hành thí nghiệm trên cả mô hình ngôn ngữ nhỏ và lớn. Đầu tiên, các mô hình BERT (phiên bản base và large) được đánh giá trên benchmark GLUE (Wang et al., 2018a). Thứ hai, bốn OPT lớn nhất từ 13B đến 175B, BLOOM lớn nhất (Scao et al., 2022) và BLOOMZ (Muennighoff et al., 2022) có 176 tỷ tham số, và các mô hình LLaMA (Touvron et al., 2023) bao gồm kích thước 7B, 13B, 30B, 65B được chọn làm đại diện. Các tác vụ zero-shot bao gồm mô hình hóa ngôn ngữ, lựa chọn nhiều, lý luận thông thường, v.v. được chọn để đánh giá. Mã đánh giá dựa trên lm-harness-evaluation1.

Baseline. Đối với BERT, chúng tôi áp dụng các kỹ thuật PTQ cổ điển làm baseline, bao gồm MinMax, Percentile (Wu et al., 2020), OMSE (Choukroun et al., 2019), và các công trình gần đây về lượng tử hóa BERT bao gồm PEG (Bondarenko et al., 2021), và Outlier Suppression (Wei et al., 2022b). Đối với các mô hình lớn bao gồm OPT, BLOOM và LLaMA, chúng tôi chủ yếu so sánh với các công trình gần đây bao gồm ZeroQuant (Yao et al., 2022), và SmoothQuant (Xiao et al., 2022). Để biết chi tiết, độc giả có thể tham khảo Phụ lục C.

Triển khai. Chúng tôi ngẫu nhiên chọn 128 mẫu từ tập dữ liệu huấn luyện, dữ liệu trong miền cho benchmark GLUE, và tập dữ liệu PILE (Gao et al., 2020) cho các tác vụ zero-shot. Một batch dữ liệu đầu tiên được sử dụng để tính toán các vector dịch chuyển và tỷ lệ hiệu quả. Sau đó, hiệu chuẩn được tiến hành. Thêm chi tiết có thể được tìm thấy trong Phụ lục C.

5.2 Lượng tử hóa tiêu chuẩn với OS+
Trong phần này, chúng tôi cho thấy OS+ có thể giúp lượng tử hóa tiêu chuẩn đạt được kết quả thỏa đáng từ cả khía cạnh mô hình nhỏ và LLMs.

BERT. Bảng 1 đưa ra kết quả dự đoán của các thuật toán PTQ phổ biến. Hầu hết các phương pháp hoạt động tốt trên INT8* nhưng thất bại ở bit thấp hơn trong khi phương pháp của chúng tôi nhất quán đạt được kết quả vượt trội. So với Wei et al. (2022b), phương pháp của chúng tôi vượt trội 1.6% và 15.5% trên 6-bit và 4-bit, tương ứng. Tóm lại, phương pháp của chúng tôi có thể đạt được hiệu suất gần floating point trên bit cao và giảm khoảng cách hiệu suất xuống 5.6% trên 4-bit.

OPT và BLOOM. Với lượng tử hóa tiêu chuẩn, chúng tôi liệt kê độ chính xác 8-bit và 6-bit trong Bảng 2. Có thể quan sát thấy rằng OS+ vượt trội ZeroQuant với một biên độ lớn. Trong khi SmoothQuant gặp khó khăn với việc giảm độ chính xác không đáng kể trên các cài đặt khó khăn hơn nhiều như mô hình 175B 6-bit với outliers cực kỳ nghiêm trọng, của chúng tôi vẫn đưa ra kết quả thú vị, có 32.5% tăng trên tác vụ HellaSwag, 27.4% tăng trên PIQA. Kết quả của các mô hình BLOOM chỉ ra rằng thách thức lượng tử hóa của chúng ít nghiêm trọng hơn OPTs với việc giảm độ chính xác nhỏ hơn trên các phương pháp. Phương pháp của chúng tôi vẫn đánh bại tốt nhất của những phương pháp khác khoảng 2% điểm trên 6-bit. Để kết luận, với lượng tử hóa tiêu chuẩn, của chúng tôi thực sự gần với kết quả FP trên 8-bit và thể hiện khoảng 1 điểm suy giảm độ chính xác trên 6-bit.

5.3 Lượng tử hóa tinh vi với OS+
Ở đây, OS+ được kết hợp với lượng tử hóa tinh vi để xác thực ứng dụng rộng rãi của nó và đi đến cài đặt bit cực thấp như lượng tử hóa 4-bit.

Lượng tử hóa Per-token. Lượng tử hóa per-token (Yao et al., 2022), tùy chỉnh các tham số lượng tử hóa cho các token riêng lẻ, có thể mang lại dự đoán tốt hơn, đặc biệt là cho lượng tử hóa bit thấp hơn và đầu ra dài hơn như WikiText2 (Merity et al., 2017). Chúng tôi lựa chọn các mô hình LLaMA để xác thực. Đáng chú ý rằng cấu trúc của LLaMA khác với những mô hình khác trong thiết kế phép nhân element-wise của hai activations làm đầu vào cho lớp cuối cùng trong FFN, có thể dẫn đến tín hiệu rất lớn, thậm chí vượt quá 600. Với thách thức như vậy, chúng tôi cung cấp thí nghiệm cả có và không có lượng tử hóa của lớp này trong Bảng 3 và Bảng 10, tương ứng. Trong cả hai bảng, chúng tôi nhấn mạnh hiệu suất không mất mát của chúng tôi trên lượng tử hóa 6-bit trong khi SmoothQuant vẫn gặp khó khăn trong Bảng 10. Ngoài ra, nó cho thấy hiệu suất vượt trội của OS+ trên 4-bit (ví dụ: cải thiện 10.58% trên Winogrande, giảm 10.04 PPL trên WikiText2).

Lượng tử hóa Per-group. Thêm vào đó, lượng tử hóa per-group (Yao et al., 2023), điều chỉnh các tham số lượng tử hóa cho mỗi nhóm phần tử, là một cách tinh vi hơn. Nhận ra khó khăn của lượng tử hóa 4-bit cho OPTs, chúng tôi minh họa một ví dụ bằng cách áp dụng lượng tử hóa per-group với kích thước nhóm tương đối lớn là 1024 và 512. Hình 3 cho thấy rằng OS+ tiếp tục vượt trội các phương pháp khác và có thể cạnh tranh hơn trong các trường hợp khó khăn hơn như kích thước nhóm 1024.

5.4 Nghiên cứu loại bỏ
Lựa chọn thiết kế của giá trị tỷ lệ. Trong phần này, chúng tôi so sánh các thiết kế vector tỷ lệ khác nhau. Trong Bảng 4, hàng thứ hai hiển thị kết quả không có hậu xử lý attention Phương trình (7). Tổng các mất mát của nhiều lớp tuyến tính, như được hiển thị, chứng minh là không khôn ngoan, dẫn đến suy giảm hiệu suất khoảng 2% và 10% trên OPTs. Hàng thứ ba loại bỏ ngưỡng outlier và thay vào đó học các giá trị tỷ lệ trực tiếp. Chúng tôi thấy quá trình này không ổn định và đòi hỏi các siêu tham số phù hợp, gây ra thất bại trên LLMs. Như đã đề cập trong Mục 4.2.2, sự không ổn định này có thể xuất phát từ các giá trị tỷ lệ không tối ưu cho các kênh bình thường với độ lớn khác nhau.

Hiệu ứng của mỗi phép toán. Từ Bảng 5, có thể quan sát rõ ràng rằng bằng cách loại bỏ phép toán dịch chuyển, độ chính xác giảm khoảng 1%-3% trong các cài đặt khó khăn. Điều này là do, không có dịch chuyển theo kênh ban đầu làm mượt thách thức lượng tử hóa, các yếu tố tỷ lệ khó có thể khoanh outliers hiệu quả trong khi tạo ra gánh nặng lượng tử hóa trọng số có thể chịu đựng được. Hơn nữa, khi loại trừ hiệu ứng tỷ lệ, hiệu suất giảm đáng kể, với thậm chí các kết quả sụp đổ trên LLMs.

5.5 Phân tích
Tỷ lệ activation khác nhau. Bởi vì các giá trị tỷ lệ hoạt động trong cả activation và trọng số, việc giảm lỗi lượng tử hóa cho các tensor riêng lẻ không thể đảm bảo thay đổi đầu ra tối thiểu, điều này đóng gói thông tin của chúng đến quá trình forward pass sau. Ví dụ, trong Bảng 6, Outlier Suppression với các giá trị tỷ lệ cố định có lỗi lượng tử hóa nhỏ nhất cho trọng số. SmoothQuant với cách heuristic có lỗi lượng tử hóa nhỏ nhất cho activation. Tuy nhiên, cả hai đều không mang lại lỗi lượng tử hóa nhỏ nhất cho đầu ra. Điều này tiết lộ tầm quan trọng của việc tối ưu hóa trực tiếp theo đầu ra, đó chính xác là những gì phương pháp của chúng tôi làm. Do đó, chúng tôi có thể tận hưởng hiệu suất cuối cùng tốt nhất.

Lưu trữ mô hình và độ chính xác. Được truyền cảm hứng bởi nhiều mô hình với kích thước đa dạng, chúng tôi cũng nghiên cứu mối quan hệ giữa lưu trữ và độ chính xác của chúng trong các cài đặt lượng tử hóa. Tập trung vào một loại mô hình với độ rộng bit lượng tử hóa khác biệt, Hình 4 cho thấy rằng lượng tử hóa 8-bit cắt giảm lưu trữ khoảng một nửa, thường có thể duy trì hiệu suất ban đầu, và lượng tử hóa 6-bit có thể dẫn đến ít suy giảm hiệu suất hơn trên các mô hình lớn hơn. Hơn nữa, xem xét các ràng buộc lưu trữ cố định, chúng tôi phát hiện rằng các mô hình lớn được lượng tử hóa thường vượt trội các mô hình FP nhỏ. Những quan sát này có thể liên quan đến sự mạnh mẽ của mô hình, ngụ ý rằng các mô hình lớn có thể hưởng lợi từ nén nhiều hơn nếu các outlier đặc biệt được xử lý tốt.

Độ phức tạp tính toán. Chúng tôi giải thích độ phức tạp tính toán của chúng tôi trong các giai đoạn hiệu chuẩn và triển khai. Đối với quá trình hiệu chuẩn, OS+ hiệu quả, và có thể tạo ra các giá trị tỷ lệ và dịch chuyển trong khoảng 20 phút cho OPT-175B offline. Hơn nữa, do biến đổi tương đương, phương pháp của chúng tôi không đòi hỏi huấn luyện bổ sung và có thể được áp dụng trong cài đặt hậu huấn luyện. Đối với triển khai, chúng tôi thảo luận hiệu quả suy luận với hiệu suất độ trễ được đánh giá bằng cách sử dụng (NVIDIA, 2022). Như đã đề cập trước đó, dịch chuyển và tỷ lệ theo kênh của chúng tôi có thể được triển khai bằng cách cập nhật các tham số trước đó, và được di chuyển đến các trọng số tiếp theo. Đối với LLMs, biến đổi của chúng tôi không giới thiệu bất kỳ gánh nặng tính toán bổ sung nào và dẫn đến cải thiện độ trễ thuận lợi, như được chứng minh trong tăng tốc 1.5× trong Hình 5. Chỉ các mô hình BERT thêm vào thay thế hàm nhận dạng trong kết nối dư bằng phép nhân và phép cộng theo kênh. Chi phí như vậy là tối thiểu, như được hiển thị trong Hình 5, dẫn đến tăng tốc độ trễ có thể so sánh.

6 Kết luận
Chúng tôi trình bày khung Outlier Suppression+ để giải quyết các outlier không đối xứng và nhất quán trong LLMs và các transformer khác. Khung của chúng tôi đơn giản để sử dụng, bao gồm cả phép toán tỷ lệ và dịch chuyển, có thể được triển khai hiệu quả và hiệu quả. Thí nghiệm chứng minh hiệu quả của các phương pháp của chúng tôi để khoanh outliers.

Hạn chế
Trong khi chúng tôi đã quan sát các tính năng của outliers và thiết kế các phương pháp để đối phó với chúng, các lý do cơ bản cho sự xuất hiện và thuộc tính của chúng chưa được hiểu đầy đủ. Điều này có thể đòi hỏi một phân tích sâu về quy trình huấn luyện, bao gồm quy trình và siêu tham số. Những điều tra như vậy tốn thời gian nhưng có thể có lợi cho cả tình huống FP và lượng tử hóa.

Tuyên bố đạo đức
Khung Outlier Suppression+ của chúng tôi nhằm cải thiện hiệu suất lượng tử hóa của các mô hình ngôn ngữ transformer. Nó có thể thúc đẩy sự phát triển của máy học thực tế và xanh và không phát sinh thêm mối quan tâm đạo đức.

Lời cảm ơn
Chúng tôi chân thành cảm ơn các nhà đánh giá ẩn danh cho các đánh giá chân thành và đề xuất có giá trị của họ để làm cho điều này tốt hơn. Chúng tôi cũng cảm ơn Qi Zhang cho cuộc thảo luận sâu sắc và Jing Liu đã giúp xây dựng mã của LLaMA. Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62022009), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62306025), Phòng thí nghiệm trọng điểm nhà nước về môi trường phát triển phần mềm (SKLSDE-2022ZX-23).

--- TRANG 10 ---
Tài liệu tham khảo
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. 2020. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701.

Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. 2018. Aciq: analytical clipping for integer quantization of neural networks.

Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. 2021. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.

Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13169–13178.

Zhaowei Cai and Nuno Vasconcelos. 2020. Rethinking differentiable search for mixed-precision neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2349–2358.

Mengzhao Chen, Wenqi Shao, Peng Xu, Mingbao Lin, Kaipeng Zhang, Fei Chao, Rongrong Ji, Yu Qiao, and Ping Luo. 2023. Diffrate: Differentiable compression rate for efficient vision transformers. arXiv preprint arXiv:2305.17997.

Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. 2018. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085.

Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. 2019. Low-bit quantization of neural networks for efficient inference. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3009–3018. IEEE.

Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. Binaryconnect: Training deep neural networks with binary weights during propagations. Advances in neural information processing systems, 28.

Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm. int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339.

Tim Dettmers and Luke Zettlemoyer. 2022. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2019. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293–302.

Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. 2019. Learned step size quantization. arXiv preprint arXiv:1902.08153.

Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Herve Jegou, and Armand Joulin. 2020. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320.

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.

Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. 2019. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In The IEEE International Conference on Computer Vision (ICCV).

Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, and Minyi Guo. 2022. Squant: On-the-fly data-free quantization via diagonal hessian approximation. arXiv preprint arXiv:2202.07471.

Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2023. Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization. Matrix, 17(4.2):7–1.

Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.

--- TRANG 11 ---
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. 2021. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning, pages 4466–4475. PMLR.

Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

Qing Jin, Jian Ren, Richard Zhuang, Sumant Hanumante, Zhengang Li, Zhiyu Chen, Yanzhi Wang, Kaiyuan Yang, and Sergey Tulyakov. 2022. F8net: Fixed-point 8-bit only multiplication for network quantization. arXiv preprint arXiv:2202.05239.

Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 5506–5518. PMLR.

Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. 2021. Bert busters: Outlier dimensions that disrupt transformers. arXiv preprint arXiv:2105.06990.

Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort. 2022. Fp8 quantization: The power of the exponent. arXiv preprint arXiv:2208.09225.

Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain damage. Advances in neural information processing systems, 2.

Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng Gao, and Guodong Guo. 2022. Q-vit: Accurate and fully quantized low-bit vision transformer. arXiv preprint arXiv:2210.06707.

Yuhang Li, Xin Dong, and Wei Wang. 2019. Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks. arXiv preprint arXiv:1909.13144.

Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. 2021. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations.

Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978.

Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan Zhuang. 2023. Qllm: Accurate and efficient low-bitwidth quantization for large language models. arXiv preprint arXiv:2310.08041.

Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, and Bohan Zhuang. 2022. Ecoformer: Energy-saving attention with linear complexity. Advances in Neural Information Processing Systems, 35:10295–10308.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models.

Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. 2022. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786.

Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. 2020. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197–7206. PMLR.

Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. 2019. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325–1334.

NVIDIA. 2022. Faster transformer. https://github.com/NVIDIA/FasterTransformer.

Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dell'Orletta. 2022. Outliers dimensions that disrupt transformers are driven by frequency. arXiv preprint arXiv:2205.11380.

Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xianglong Liu. 2022. Bibert: Accurate fully binarized bert. arXiv preprint arXiv:2203.06390.

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.

--- TRANG 12 ---
Mingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li, Chuming Li, Chen Lin, Fengwei Yu, Junjie Yan, and Wanli Ouyang. 2021. Once quantization-aware training: High performance extremely low-bit architecture search. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5340–5349.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815–8821.

Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.

Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. 2022. Compression of generative pre-trained language models via quantization. arXiv preprint arXiv:2203.10705.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018a. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.

Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. 2018b. Training deep neural networks with 8-bit floating point numbers. Advances in neural information processing systems, 31.

Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. 2020. Towards accurate post-training network quantization via bit-split and stitching. In International Conference on Machine Learning, pages 9847–9856. PMLR.

Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. 2022a. Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization. In International Conference on Learning Representations.

Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. 2022b. Outlier suppression: Pushing the limit of low-bit transformer language models. arXiv preprint arXiv:2209.13325.

Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. 2020. Integer quantization for deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602.

Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouthe, and Song Han. 2022. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438.

Ke Xu, Lei Han, Ye Tian, Shangshang Yang, and Xingyi Zhang. 2023. Eq-net: Elastic quantization neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1505–1514.

Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861.

Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. 2023. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. arXiv preprint arXiv:2303.08302.

Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. 2021. Ptq4vit: Post-training quantization framework for vision transformers. arXiv preprint arXiv:2111.12293.

Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36–39. IEEE.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.

Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. 2018. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 365–382.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.

Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint arXiv:2009.12812.

Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and Xianglong Liu. 2021. Diversifying sample generation for accurate data-free quantization. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. 2019. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543–7552. PMLR.

Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.

--- TRANG 13 ---
A Công trình liên quan
Lượng tử hóa. Nén đã trở nên ngày càng phổ biến những ngày này (Han et al., 2015; Hinton et al., 2015; Hu et al., 2021; Liu et al., 2022; Xu et al., 2023; Chen et al., 2023). Một trong những kỹ thuật hiệu quả của nó gọi là lượng tử hóa (Jacob et al., 2018) sử dụng biểu diễn bit thấp cho activation và trọng số trong mạng neural. Các nhà nghiên cứu phân loại phương pháp này thành hai quy trình: lượng tử hóa hậu huấn luyện (PTQ) và huấn luyện nhận thức lượng tử hóa (QAT). QAT (Courbariaux et al., 2015; Choi et al., 2018; Esser et al., 2019; Li et al., 2019; Gong et al., 2019; Shen et al., 2021; Zhang et al., 2018) huấn luyện mô hình được lượng tử hóa end-to-end, cần thiết tài nguyên GPU đáng kể và toàn bộ tập dữ liệu huấn luyện. Ngược lại, PTQ (Choukroun et al., 2019; Wu et al., 2020; Banner et al., 2018; Wang et al., 2020; Zhao et al., 2019; Nagel et al., 2019) chỉ yêu cầu hàng trăm mẫu và tiêu thụ tài nguyên hạn chế, tạo ra một mô hình được hiệu chuẩn nhanh chóng. Gần đây, một số công trình (Nagel et al., 2020; Hubara et al., 2021; Li et al., 2021; Wei et al., 2022a) đề xuất điều chỉnh mô hình nhẹ để cải thiện hiệu suất PTQ. Bên cạnh đó, các loại lượng tử hóa khác bao gồm lượng tử hóa zero-shot không có dữ liệu hiệu chuẩn thực (Cai et al., 2020; Zhang et al., 2021; Guo et al., 2022), độ chính xác hỗn hợp với độ rộng bit hỗn hợp (Dong et al., 2019; Cai and Vasconcelos, 2020), và kiểu dữ liệu FP8 (Wang et al., 2018b; Kuzmin et al., 2022; Micikevicius et al., 2022; Jin et al., 2022).

Lượng tử hóa của các mô hình ngôn ngữ transformer. Gần đây, đã có sự quan tâm ngày càng tăng đối với lượng tử hóa của các mô hình ngôn ngữ transformer. Trong bối cảnh QAT, Zafrir et al. (2019) đầu tiên khám phá lượng tử hóa 8-bit cho các mô hình giống BERT. Shen et al. (2020) giới thiệu lượng tử hóa theo nhóm và nghiên cứu lượng tử hóa độ chính xác hỗn hợp dựa trên thông tin Hessian. Bai et al. (2020); Zhang et al. (2020); Qin et al. (2022) kết hợp các chiến lược chưng cất với lượng tử hóa. Kim et al. (2021) xấp xỉ hàm phi tuyến trong kiến trúc transformer để cho phép suy luận chỉ số nguyên. Fan et al. (2020) kết hợp nhiễu lượng tử hóa để tăng cường. Thêm vào đó, Tao et al. (2022) điều tra các thách thức của việc lượng tử hóa các mô hình sinh.

Trong lĩnh vực PTQ, các nhà nghiên cứu đã phát hiện rằng hiệu suất kém của các mô hình này nên được quy cho các outliers cực đoan trong activations. Những outliers này thể hiện các đặc điểm đặc biệt từ cả khía cạnh kênh và token. Về mặt kênh, outliers nhất quán xuất hiện trong các kênh nhất định trên các đầu vào khác nhau. Bondarenko et al. (2021) sử dụng sơ đồ lượng tử hóa per-embedding-group sử dụng các tham số lượng tử hóa khác nhau cho các nhóm kênh riêng biệt, trong khi Dettmers et al. (2022) đề xuất sử dụng biểu diễn FP16 cho các kênh có vấn đề có tín hiệu trên 6. Wei et al. (2022b) xác định tính năng này nằm trong đầu ra của LayerNorm và di chuyển tham số tỷ lệ của LayerNorm đến các module tiếp theo để làm giảm outliers. Xiao et al. (2022) đề xuất tính toán các giá trị tỷ lệ bằng cách cân bằng phạm vi giữa activations và trọng số và đánh giá trên các mô hình ngôn ngữ lớn. Guo et al. (2023) loại bỏ các giá trị bình thường liền kề với outliers, tạo chỗ cho outliers với hỗ trợ GPU tùy chỉnh. So với họ, chúng tôi thiết kế các yếu tố tỷ lệ quan tâm đến kết quả tương tác của activation có vấn đề và trọng số theo sau để tỷ lệ xuống các kênh có outliers offline. Ngoài ra, chúng tôi nhận thấy sự trình bày không đối xứng của outliers và thiết kế một phép toán dịch chuyển. Trong khi chúng tôi hoạt động trên các kênh tương ứng giữa trọng số và activation, một công trình sau này (Liu et al., 2023) áp dụng các phép toán chia tách và hợp nhất để chuyển giao gánh nặng lượng tử hóa của các kênh outlier đến các kênh đối diện của trọng số, điều này có thể khuyến khích chúng tôi thiết kế một kỹ thuật linh hoạt hơn mà không có yêu cầu chỉ số kênh giống hoặc đối diện. Về mặt token, các token khác nhau thể hiện các mức độ outliers khác nhau. Dettmers et al. (2022); Yao et al. (2022) giới thiệu một sơ đồ mới gọi là lượng tử hóa per-token tính toán động các tham số lượng tử hóa cho mỗi token. Wei et al. (2022b) điều tra tác động cắt của outliers và khuyến nghị tìm một phạm vi cắt thích hợp theo cách token-wise.

Bên cạnh đó, một số nghiên cứu tập trung vào lượng tử hóa trọng số, chẳng hạn như Dettmers and Zettlemoyer (2022); Frantar et al. (2022); Zeng et al. (2022); Lin et al. (2023) và một số bao gồm Yuan et al. (2021); Li et al. (2022), điều tra lượng tử hóa của các mô hình Vision Transformer (ViT). Thú vị, một số nghiên cứu (Kovaleva et al., 2021; Puccetti et al., 2022) khám phá các lý do cơ bản cho việc xuất hiện outliers và truy tìm chúng trở lại giai đoạn tiền huấn luyện.

B Thí nghiệm bổ sung
BERT-base. Chúng tôi cung cấp kết quả chi tiết của các mô hình BERT-base trên benchmark GLUE trong Bảng 7. Thú vị, chúng tôi thấy rằng các mô hình nhạy cảm với các siêu tham số học tập khác nhau trong giai đoạn fine-tuning, chẳng hạn như CoLA và RTE, cũng thể hiện kết quả lượng tử hóa kém thuận lợi hơn. Điều này gợi ý một mối quan hệ có thể có giữa lượng tử hóa và sự mạnh mẽ.

BERT-large. Chúng tôi cũng tiến hành thí nghiệm trên các mô hình BERT-large trong Bảng 8. Kết quả trên các phương pháp chỉ ra rằng lượng tử hóa các mô hình BERT-large thách thức hơn (ví dụ: MinMax gặp phải sự giảm độ chính xác đáng kể (khoảng 13%) trên INT8* so với BERT-base, và Outlier Suppression cũng thất bại trên cài đặt 6-bit). May mắn thay, với Outlier Suppression+, kết quả có thể được cải thiện, mang lại 18.7% tăng cường.

OPT. Ở đây, chúng tôi cung cấp kết quả của OPTs trên nhiều tác vụ hơn. Bảng 9 là phần bổ sung cho Bảng 2, tiếp tục cho thấy cải thiện hiệu suất nhất quán của OS+.

LLaMA. Nhớ lại rằng chúng tôi tiến hành thí nghiệm trên LLaMA với hai cài đặt khác nhau trong phần lượng tử hóa tinh vi. Bảng 10 đưa ra kết quả khi lượng tử hóa cấu trúc đặc biệt và thách thức (lớp cuối cùng của FFN) trong các mô hình LLaMA. Có thể quan sát thấy rằng của chúng tôi vẫn kiếm được hiệu suất gần floating-point trên lượng tử hóa 6-bit và đánh bại những phương pháp khác khoảng 5% ∼14% về độ chính xác trung bình của bốn tác vụ đầu tiên, và thậm chí giảm PPL gấp bốn lần cho WikiText2. Bằng cách so sánh với cài đặt dễ hơn Bảng 3, chúng tôi thấy rằng cấu trúc đặc biệt với tín hiệu lớn thực sự dẫn đến kết quả 4-bit thấp hơn nhiều trên các phương pháp, đặc biệt là cho MinMax và SmoothQuant, điều này khiến chúng tôi nghĩ về thiết kế mô hình, kỹ thuật huấn luyện, và fine-tuning hiệu quả cho lượng tử hóa.

C Chi tiết triển khai
C.1 OS+
Trong phần này, chúng tôi cung cấp mô tả chi tiết về triển khai của chúng tôi với phần cốt lõi được chưng cất trong thuật toán 1.

BERT. Trên benchmark GLUE, các mô hình FP được fine-tuned được sử dụng để lượng tử hóa. Chúng tôi ngẫu nhiên chọn 128 mẫu và đặt kích thước batch là 32. Đầu tiên, một batch dữ liệu được sử dụng để tính toán các tín hiệu dịch chuyển và tỷ lệ hiệu quả cho các activations có vấn đề, đặc biệt là đầu ra sau LayerNorm ở đây. Sau đó các vector dịch chuyển và tỷ lệ được hợp nhất vào các phép toán trước đó và được hấp thụ trong các module sau. Trên các mô hình được hợp nhất, chúng tôi áp dụng quy trình hiệu chuẩn. Đặc biệt, trên các mô hình BERT, do sự biến thiên lớn của phạm vi token như đã thảo luận trong Yao et al. (2022); Wei et al. (2022b), chúng tôi kết hợp Token-Wise Clipping được đề xuất trong Outlier Suppression là một kỹ thuật trực giao và làm yếu outliers từ khía cạnh token.

OPTs. Đối với OPTs, chúng tôi lượng tử hóa các mô hình được tiền huấn luyện và đánh giá chúng trên các tác vụ zero-shot. 128 mẫu được trích xuất ngẫu nhiên từ một trong các tập dữ liệu train, cụ thể là tập dữ liệu PILE. Như chúng tôi đã quan sát rằng LayerNorm tạo ra các outliers không đối xứng nghiêm trọng trên các kênh nhất định, phương pháp được đề xuất được áp dụng ở đây. Sau khi có được một mô hình thân thiện với lượng tử hóa hơn, thuật toán MinMax thu thập thống kê phân phối. Vì các token đa dạng không có outliers có mức độ khác nhau trên các mô hình này, các kỹ thuật cắt nâng cao không được liên quan.

BLOOM và BLOOMZ. Quy trình chính tương tự như OPTs. Ngoại lệ duy nhất là sử dụng Token-Wise Clipping như phương pháp hiệu chuẩn vì các mô hình này có outliers khác nhau giữa các token khác nhau. Các tỷ lệ cắt được tìm kiếm là 0.5% và 1.5% cho BLOOM 8-bit và 6-bit, và 0.0% và 0.5% trên BLOOMZ.

LLaMA. Quy trình chính tương tự như OPTs với một số khác biệt nhỏ. Đầu tiên, chúng tôi sử dụng tập dữ liệu WikiText2 để hiệu chuẩn. Thứ hai, vì LLaMA không có bias, việc giới thiệu dịch chuyển theo kênh có thể phát sinh một chút chi phí. Do đó, để so sánh công bằng, chúng tôi đơn giản bỏ qua dịch chuyển theo kênh cho LLaMA ở đây. Thứ ba, khi lấy cài đặt khó khăn hơn để lượng tử hóa lớp cuối cùng trong FFN, tỷ lệ theo kênh cũng được tiến hành do đó cập nhật thang đo lượng tử hóa của up proj và các tham số trọng số của down proj, điều này không mang lại chi phí tính toán trong quá trình suy luận. Cuối cùng, không giống như OPTs, đối với các tác vụ có các chỉ số độ chính xác được chuẩn hóa, chúng tôi báo cáo chỉ số độ chính xác được chuẩn hóa thay vì chỉ số độ chính xác để căn chỉnh với bài báo gốc (Touvron et al., 2023). Điểm này cũng đã được chỉ ra trong mỗi bảng dưới đây.

C.2 Baseline
Chúng tôi giới thiệu chi tiết triển khai của baseline ở đây. MinMax thu được thống kê tối thiểu và tối đa của tensor cho phạm vi cắt lượng tử hóa. Percentile (Wu et al., 2020) sử dụng phân vị phân phối activation làm phạm vi cắt lượng tử hóa. Sử dụng tập dev, chúng tôi tìm kiếm các siêu tham số của nó trong [0.999, 0.9999, 0.99999]. OMSE (Choukroun et al., 2019) giảm thiểu sai số bình phương trung bình giữa các tín hiệu lượng tử hóa và FP. PEG (Bondarenko et al., 2021) áp dụng lượng tử hóa tinh vi cho activation có vấn đề từ góc độ kênh. Outlier Suppression (OS) (Wei et al., 2022b) sử dụng các yếu tố tỷ lệ cố định để khoanh outliers và tiếp tục cắt outliers theo cách token-wise. ZeroQuant (Yao et al., 2022) sử dụng lượng tử hóa per-token, gán các tham số lượng tử hóa khác nhau cho các token khác nhau. Sơ đồ tinh vi này từ khía cạnh token cũng đòi hỏi lượng tử hóa động. Trong khi đó, đối với INT8*, chúng tôi triển khai lượng tử hóa trọng số per-group theo mô tả của nó. SmoothQuant (Xiao et al., 2022) di chuyển các yếu tố tỷ lệ đến các module sau để làm mượt activation có vấn đề. Các yếu tố tỷ lệ của chúng bằng phạm vi giữa activation và trọng số. Đối với bit thấp hơn, chúng tôi cũng tìm kiếm siêu tham số α của nó theo mô tả của nó để có hiệu suất tốt hơn.

--- TRANG 14 ---
Thuật toán 1: Outlier Suppression+
Đầu vào: Đầu ra có vấn đề X của LayerNorm với các tham số γ,β, module tiếp theo M với trọng số W và bias b, số lần lặp tìm kiếm lưới K.
{1. Dịch chuyển và tỷ lệ hiệu quả:}
z=min(X:,j)+max( X:,j)/2▷Vector dịch chuyển hiệu quả.
loss∗=INF
for k= 1 to K do
t= max( X−z)·k/K, ▷Liệt kê ngưỡng outlier.
sj= max(1.0,max(X:,j−zj)/t)
Tính toán lossk dựa trên Phương trình (6), Phương trình (7).
if loss∗> lossk then
loss∗=lossk,s∗=s ▷Các yếu tố tỷ lệ hiệu quả.
{2. Dịch chuyển và tỷ lệ tương đương:}
eβ= (β−z)⊘s∗,eγ=γ⊘s∗▷Hợp nhất z,s∗ vào các phép toán trước đó.
eb=zW⊤+b,fW=W⊙s∗▷Cập nhật các module theo sau.
return LayerNorm đã biến đổi và module tiếp theo;
