# 2310.09259.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/quantization/2310.09259.pdf
# File size: 809499 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
QUIK: T OWARDS END-TO-END 4-B ITINFERENCE
ONGENERATIVE LARGE LANGUAGE MODELS
Saleh Ashkboos* 1Ilia Markov* 2Elias Frantar2Tingxuan Zhong3Xingchen Wang3Jie Ren4
Torsten Hoefler1Dan Alistarh2 5
ABSTRACT
Large Language Models (LLMs) from the GPT family have become extremely popular, leading to a race towards
reducing their inference costs to allow for efficient local computation. Yet, the vast majority of existing work
focuses on weight-only quantization, which can reduce runtime costs in the memory-bound one-token-at-a-time
generative setting, but does not address them in compute-bound scenarios, such as batched inference or prompt
processing. In this paper, we address the general quantization problem, where both weights and activations
should be quantized. We show, for the first time, that the majority of inference computations for large generative
models such as LLaMA, OPT, and Falcon can be performed with both weights and activations being cast to 4
bits, in a way that leads to practical speedups, while at the same time maintaining good accuracy. We achieve
this via a hybrid quantization strategy called QUIK, which compresses most of the weights and activations to
4-bit, while keeping some outlier weights and activations in higher-precision. The key feature of our scheme is
that it is designed with computational efficiency in mind: we provide GPU kernels matching the QUIK format
with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to
3.4x relative to FP16 execution. We provide detailed studies for models from the OPT, LLaMA-2 and Falcon
families, as well as a first instance of accurate inference using quantization plus 2:4 sparsity. Code is available at:
https://github.com/IST-DASLab/QUIK .
1 I NTRODUCTION
Large language models (LLMs) from the Generative Pre-
trained Transformer (GPT) family (Radford et al., 2019)
are massively popular. One key contributor to their adop-
tion has been the ability to compress them using advanced
techniques, e.g., (Frantar et al., 2022; Dettmers et al., 2022;
Lin et al., 2023; Yuan et al., 2023), enabling local storage
and efficient generative inference for these models, even on
personal computers. The vast majority of work on LLM
quantization can be categorized into two cases:
â€¢Weight-only quantization methods (Frantar et al., 2022;
Dettmers et al., 2022; Lin et al., 2023; Dettmers et al.,
2023; Lin et al., 2023; Kim et al., 2023) that help reduce
the massive memory-transfer costs of LLM inference.
Yet, these methods do not reduce computation, and cannot
provide significant speedup for computationally-bound
*Equal contribution1ETH Zurich2Institute of Science
and Technology Austria3Xidian University4KAUST
5Neural Magic, Inc.. Correspondence to: Saleh Ashk-
boos <saleh.ashkboos@inf.ethz.ch>, Dan Alistarh
<dan.alistarh@ist.ac.at>.
Accepted to the 3rd NeurIPS Workshop on Efficient Natural Lan-
guage and Speech Processing (ENLSP) , 2023.settings, such as prompt processing or batch inference.
â€¢Joint weight-activation quantization methods , which can
provide computational improvements, but either focus ex-
clusively on 8-bit weights and activations (8W8A) (Xiao
et al., 2022; Dettmers et al., 2022), or execute with large
amounts of accuracy loss relative to their uncompressed
counterparts (Yuan et al., 2023; Shao et al., 2023).
Thus, there is still a significant gap between compressed
formats efficiently supported by hardwareâ€”specifically,
NVIDIA GPUs natively support accelerated 4bit matrix
multiplication on both the Ampere and Lovelace architec-
tures (NVIDIA, 2023)â€”and quantization algorithms with
computational support which would allow inference to be
performed accurately on such compressed formats.
Contribution. In this paper, we take a step towards bridging
this gap, and show for the first time that a large fraction of
the computation in modern LLMs such as OPT (Zhang et al.,
2022), LLaMA-2 (Touvron et al., 2023) and Falcon (TII
UAE, 2023) can be performed accurately and efficiently
using 4-bit activations and weights (4W4A) .
On the algorithmic side, we show significantly improved
results relative to prior work on joint quantization of
weights and activations to 4 bits, via a hybrid scheme forarXiv:2310.09259v2  [cs.LG]  2 Nov 2023

--- PAGE 2 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
		
	
	
	
2.61x3.4x2.48xspeedup
Figure 1. Accuracy and speedups for QUIK at different model
sizes, on the LLaMA family of models. QUIK achieves up to 3.4x
speedup with minor accuracy degradation on LLaMA-2 models.
QUantization to INT4 with GPU Kernel support, called
QUIK . In QUIK, matrices are split into â€œbaseâ€ weights
and activations, which are processed exclusively at 4-bit
precision, and a small number of â€œoutlierâ€ weights and ac-
tivations, which are processed at higher precision such as
INT8 or FP16. Using this approach, as well as additional
insights into layer sensitivity, we build a framework which
can recover accuracy within 0.3â€“0.5 perplexity points across
model sizes (corresponding to 6%-16% relative error), while
executing a large fraction of the inference in INT4. For
illustration, for the sensitive LLaMA2 model with 70B pa-
rameters, we can recover accuracy within 0.5 perplexity,
while executing 70% of the linear layer computations in
INT4, leading to 3.4x end-to-end speedups (see Figure 1).
On the systems side, the key feature of QUIK is that it can
be implemented efficiently via GPU kernels with low run-
time and memory overheads relative to GPU-native INT4
matrix multiplication (MatMul). We demonstrate this via a
general implementation leading to per-layer speedups and
end-to-end throughput improvements relative to both FP16
and INT8 baselines. Specifically, we show that supporting
a limited number of feature and weight outliers can have
negligible overhead by fusing the quantization and dequan-
tization operations into the MatMul and by mitigating their
costs in linear layers via additional optimizations.
Overall, QUIK leverages quantization for significant end-
to-end speedups and memory reductions. For example, for
processing a sequence of 2048 tokens on a commodity RTX
3090 GPU, we achieve end-to-end speedups between 3.1x,
for the OPT-66B and Falcon-180B models, and 3.4x for
LLaMA2-70B, relative to a theoretical optimum of â‰ˆ4x.
In addition, QUIK requires much less GPU memory, and
therefore, less GPUs, relative to FP16. For instance, QUIK
provides 3.6x memory reduction for OPT-66B, and 3x com-
0 1 10 100 1000
Arithmetic Intensity, FLOP/byte116.3Performance, TFLOP/sMemory bound
Compute bound
Input size 1Input size 16
Input size 128Input size 256
Input size 1024Figure 2. Roofline analysis of a standard LLM MatMul operation,
for a matrix of size 8K x 8K, in FP32, on an NVIDIA GPU.
Markers denote the results of profiling with different token counts
(from 1 to 1024). Small counts (1 and 16) are memory-bound,
whereas larger counts (from 128 to 1024) are compute-bound.
pression for accurate execution of LLaMA2-70B, executing
the latter in less than 50GB of GPU memory.
2 M OTIVATION
Roofline Analysis. To motivate our focus on the compute-
bound case, we begin an analysis of the basic computational
operation in the context of LLMs, a matrix multiplication
for different numbers of tokens. We profile a linear layer
of standard size (11K x 4K, corresponding to the MLP
in LLaMA-7B (Touvron et al., 2023)), using the NVIDIA
NSight Compute toolkit (NVIDIA), from a single token to
16, 256 and 1024 tokens.
The results, illustrated in Figure 2, clearly show that the
case of few tokens (1 and 16) is memory-bound, whereas
the workload becomes compute-bound for the larger token
counts, specifically larger than 128. A realistic end-to-end
LLM deployment would need to consider optimizing both
scenarios, as the prompt processing â€œprefillâ€ case falls into
the large token count scenario, whereas generating one-
token-at-a-time falls into the former case. Moreover, run-
ning a â€œbatchedâ€ version of the single-token workload, i.e.
for multiple users, would again result in large token counts,
returning to the compute-bound case.
Further, we observe that existing methods for weight-only
quantization, e.g. (Frantar et al., 2022; Dettmers & Zettle-
moyer, 2022; Lin et al., 2023) only serve to improve
the arithmetic intensity of this operation, by reducing the
amount of data which needs to be transferred per operation,
but still perform the computation in the original precision.
Thus, they do not help in the compute-bound case, and in
fact even slightly increase the amount of computation per
operation, due to the de-quantization overheads.
Speedup Potential. Given our focus on the compute-bound

--- PAGE 3 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
25651210242048307240965120614471688192921610240 11264122881331214336
Matrix Dimension (M=N=K)0100200300400TFLOP/s
W4A4
W8A8
FP16
FP32
Figure 3. Ideal matrix multiplication performance for different
layer sizes and data precision on RTX3090.
case, it is natural to investigate the available hardware op-
tions leading to potential speedups. As shown in Figure 3,
quantization is a natural approach in this case, given that
NVIDIA GPUs have native support for INT4 and INT8 data
types, providing major throughput improvements across
matrix sizes. Specifically, INT8 provides throughput im-
provements that can be slightly higher than 2x relative to
FP16 on raw MatMuls, whereas INT4 almost doubles over
INT8. However, to leverage these hardware operations,
both layer inputs (activations) and layer weights must be
quantized to the same compressed data type.
We will focus on accurate post-training quantization for
LLM inference, by compressing both weights and activa-
tions, primarily to INT4 data types. As stated, weight-only
quantization (Frantar et al., 2022; Lin et al., 2023) does not
transfer to our setting, and activation quantization is notori-
ously challenging (Xiao et al., 2022). Moreover, as shown
in Table 1, existing methods for quantizing both weights
and activations in LLMs break down in terms of accuracy
when applied to 4bit compression.
3 M ETHOD
3.1 Background
We focus on the task of accelerating linear layers within
Large Language Models (LLMs) by employing 4-bit quanti-
zation for both the weight matrix Wand the input matrix
X. Following the PyTorch definition (Paszke et al., 2019),
a linear layer carries out a linear transformation along with
a bias vector b, taking the form of XWT+b. We now
describe the background and details of the technique.
Outliers in Input Quantization. It is known that the acti-
vation matrices are hard to quantize (Dettmers et al., 2022;
Xiao et al., 2022; Yuan et al., 2023), mainly due to the pres-
ence of outlier features in these matrices, where some of
X W
WStep 1 : The outlier columns are
characterized based on the inputs.
Step 2 : The
outlier columns
will be pushed
toward the end.Â 
Â  Â  Â  Â  W
INT4 FP16X
Step 3 : GPTQ quantizes
the majority of the weights
using the re-ordered
Hessian matrix and
accumulates the errors in
the outlier columns.Figure 4. Outlier-aware quantization with QUIK. Outlier weight
columns are extracted based on outlier columns in the input. We
permute the outlier columns toward the end of the matrix before
applying GPTQ quantization (using the re-ordered Hessian matrix)
to accumulate the quantization errors in the FP16 columns.
the columns have up to 100x larger magnitudes. LLM.int8()
(Dettmers et al., 2022) identifies and extracts the outlier
columns of Xduring the forward pass and quantizes the
rest of the elements with 8-bit. However, LLM.int8() is not
efficient at runtime due to the added computational cost of
determining outliers on-the-fly. Recent work (Xiao et al.,
2022) has shown that the outlier features are fixed for each
layer across datasets, which means that we can extract out-
lier indices offline using a small calibration set.
GPTQ Weight Quantization. GPTQ (Frantar et al., 2022)
is a weight-only quantization method which involves the
quantization of Wwhile retaining the activations Xin FP16.
To do this, it iterates over the weight columns; for each
column, it quantizes all of its elements simultaneously. Fol-
lowing the quantization of a weight column, GPTQ adjusts
the remaining unquantized columns, to the right of the cur-
rent one, by using second-order information to compensate
for the introduced quantization error in the current step.
This process accumulates the quantization errors at the last
columns , making them more sensitive to quantization.
3.2 QUIK Quantization
Overview. At a high level, QUIK works as follows. First,
note that, during the linear transformation XWT, the outlier
columns in X, by which we mean the columns with large
average values defined previously, will always be multiplied
by certain columns in WT, as illustrated in Figure 4. We

--- PAGE 4 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
leverage this observation to improve the quality of GPTQ
quantization, in a setting where we quantize (part of) the
activations as well.
Since the outlier columns are fixed across datasets, we begin
by extracting the indices of the outlier columns by means
of a calibration set. Then, we rearrange the weight columns
(and their corresponding input columns), to shift the outliers
toward the end. Finally, we perform quantization on the
weight columns up to the index of the outliers. This cir-
cumvents quantization of these â€œdifficultâ€ columns. It also
helps GPTQ quantization by 1) aggregating the quantization
errors to the columns we keep in FP16, and 2) removing
potential weight outliers from the 4bit quantization scale.
Weight Clipping. Weight clipping improves quantization
by trimming the input distribution before rounding. This
could be done by either training the whole network to find
the optimal clipping thresholds (Shao et al., 2023; Esser
et al., 2019; Choi et al., 2018); or employing heuristic meth-
ods (Lin et al., 2023; Lee et al., 2023; Kim et al., 2023). We
found that applying linear search over the clipping thresh-
olds for weight quantization improves final perplexity.
Sensitivity-Based Partial Quantization. Accurately select-
ing outlier columns is key for QUIK. Following (Xiao et al.,
2022; Dettmers et al., 2022), we select the columns with the
largest â„“âˆžnorm as outliers. Since finding these columns dy-
namically at runtime is costly, we follow (Xiao et al., 2022)
in identifying a predefined set of outliers for each layer via
a calibration set (see Section 4), and quantize the weights
offline. We use the same outlier indices for extracting the
input outlier columns during the forward pass.
This approach is sufficient for accurate quantization of mod-
els such as OPT (Zhang et al., 2022) (see Section 4). How-
ever, highly-accurate massive models such as LLaMA2-70B
present a further challenge due to their FeedForward lay-
ers, which involve three linear transformations along with
element-wise multiplication, as well as the use of the Sig-
moid Linear Unit (SiLU) activations. Specifically, our â„“âˆž
norm analysis illustrated in Figure 10, suggests that the
Down projlayers are much more sensitive to quantization.
(Li et al. (2023) arrived at a similar observation.) Thus, we
extend our scheme to recover accuracy by quantizing the
Down projlayers to 8 bits instead of 4, without other changes
to our method. We illustrate the outlier selection procedure
in detail in Section 4.3.1. Figure 11 presents a detailed anal-
ysis of the overall FLOP breakdown to various precisions
when quantizing the LLaMA2-70B model via QUIK.
3.3 Efficient Inference Implementation
We now provide a high-level description of how models
in the QUIK format are executed efficiently on GPU. We
illustrate the workflow in Figure 5 and provide pseudocodein Algorithm 1. The first and most important step in QUIK
is splitting the input matrix of shape (#tokens, #features)
column-wise, so across features, into two sub-sets, a small
â€œfull precisionâ€ part (usually half or bfloat16) and a large
base part, which will be quantized (see line 3 in the pseu-
docode). The full-precision part is multiplied with the cor-
responding (full-precision) part of the weight matrix in stan-
dard fashion, while the rest goes through the quantized
matrix multiplication pipeline described next.
The quantized MatMul pipeline consists of three parts: 1)
dynamically quantizating the activations, 2) actually per-
forming the MatMul of quantized activations and weights,
and 3) dequantizing the result back to floating point format.
Quantization. In general, we quantize weights symmetri-
cally (only scale) per output and quantize activations asym-
metrically (scale and zero) per token. The former is done
offline (see Section 3.2), while the latter must be done on-
linebased on the current activation values. Specifically, we
first scan the activations to determine the per-token min- and
max-value, from which we calculate the scale and zero point
(line 12). These are then used to turn the floating point acti-
vations into integers, which are written out again as signed
(hence the halfRange subtraction in line 14) INT4 or INT8
values in a packed format for efficient further processing
(see lines 13-16).
Matrix Multiplication. The actual MatMul is performed
by the CUTLASS (NVIDIA, 2023) library, which is able to
effectively utilize the hardwareâ€™s INT8/INT4 tensor-cores to
perform fast low-precision calculations, while accumulating
results in a wider INT32 format.
Dequantization. As the MatMul was carried out purely
with quantized INT values, we need to convert back to a
floating point format in order to properly integrate scale
and zero information. Concretely, we need to multiply each
output element oijby its corresponding input token scale
scaleAct and output weight scale scaleWeight (line
22). Additionally, we also need to account for the activation
zero-point zeroAct . To do this, we consider a scalar
product âŸ¨w, xâŸ©(representing a single output value in our
overall matmul) where a constant zis added to each xi:
y=X
iwi(xi+z) =X
iwixi+zÂ·X
iwi.(1)
Consequently, we must shift by ztimes the sum over rele-
vant weights , the latter of which is static and can thus be
precomputed as wReduced ; the signed to unsigined INT
conversion must be considered as well (lines 23 - 24). Fi-
nally, we add these dequantized values to the original outlier
result, yielding the final output (line 8).

--- PAGE 5 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
Algorithm 1 Quantization and Dequantization kernels.
1:function QUIK Matmul
2: Input: wInt, wFP, x, FPindices, scaleWeight, wRe-
duced;
3: xFP, xQ â†âˆ’split(x, FPindices);
4: xINT, zeroAct, scaleAct â†âˆ’Quantization(xQ);
5: resultFP â†âˆ’FPmatmul(xFP, wFP);
6: resultInt â†âˆ’INTmatmul(xInt, wInt);
7: dequantFP â†âˆ’ Dequantization(resultInt, zeroAct,
scaleAct, scaleWeight, wReduced)
8: return dequantFP + resultFP;
9:end function
10:function Quantization
11: Input: dataFP;
12: zeroAct, scaleAct â†âˆ’ findZeroScale(dataFP);
13: forelemâˆˆdataFP, outElem âˆˆoutput do
14: // Use scale/zero corresponding to token
15: outFP â†âˆ’ (elem - zeroAct) / scaleAct - halfRange;
16: outElem â†âˆ’ pack(outFP);
17: end for
18: return output, zeroAct, scaleAct;
19:end function
20:function Dequantization
21: Input: inputINT, zeroAct, scaleAct, scaleWeight, wRe-
duced
22: forelemâˆˆinputINT, outElem âˆˆoutputFP do
23: // Use scales for token and weight row, respectively
24: x â†âˆ’ elem * scaleAct * scaleWeight;
25: shift â†âˆ’ zeroAct + halfRange * scaleAct;
26: shift â†âˆ’ shift * wReduced;
27: outElem â†âˆ’ x + shift;
28: end for
29: return outputFP;
30:end function
3.4 Performance Optimizations
The computational backbone of the QUIK kernel implemen-
tation is the low-precision CUTLASS matrix multiplication.
However, the mixed precision nature of the algorithm im-
poses the use of auxiliary functions, such as input data
splitting, metadata computation, quantization and dequanti-
zation. This provides opportunities for optimizations.
Quantization Fusion. A naive implementation of the split-
ting and quantization pipeline would require one read-and-
write pass for the outlier-part, another read-and-write pass
for the base-part, two read passes to determine per-token
min-max values and one more read-and-write pass for actu-
ally carrying out quantization. Many of these slow memory-
bound operations can be optimized away via careful operator
fusion in the form of bespoke kernels.
Specifically, we assign each input row to a CUDA block
and perform 3 passes over it: reduction (finding meta infor-
mation) over the non-outliers elements, quantization of the
non-outliers and moving the outliers to a separate piece of
memory. This eliminates two costly read (min-max calcula-tion and base-part splitting) and one write pass (base-part
splitting), and overheads of additional kernel launches.
Parallelization Tuning. For the above quantization proce-
dure to be efficient on a modern GPU, we have to ensure
optimal parallelization via careful tuning of CUDA blocks
and threadcounts. The most critical tuning parameter is the
number of rows we process with one CUDA block. Map-
ping one block per each row brings additional launching
overheads, while mapping too many rows per block results
in block over-subscription and lower occupancy of the GPU.
Hence, we optimized the appropriate number of rows per
block for different matrix sizes (usually values between 8
and 32). This improved quantization speed by up to 30%.
Dequantization Epilogue. CUTLASS first accumulates
MatMul results in registers before committing them to
(slow) global memory. We can avoid an unnecessary write
and read pass of intermediate INT32 matmul results by di-
rectly performing dequantization in a custom epilogue that
is applied before the global memory commit, which we
further directly accumulate into the results of the outlier
MatMul. Overall, this interleaves two expensive operations
and saves additional kernel launches and memory trips.
Performance Impact. To illustrate the impact of these opti-
mizations, we mark them as different versions of our kernel:
version 1 has unfused quantization and dequantization; ver-
sion 2 has fused quantization and unfused dequantization;
version 3 fuses both quantization and dequantization.
Figure 6 provides a detailed breakdown of the results of
each of these optimizations. We observe that they are espe-
cially effective for the small matrix sizes, where they lead to
end-to-end speedups of almost 2x. Fused quantization opti-
mization gives up to 40% throughput improvement and the
dequantization epilogue yields an additional 10% speedup.
4 E XPERIMENTAL VALIDATION
General setup. We evaluate our method on OPT (Zhang
et al., 2022), LLaMA-2 (Touvron et al., 2023), and Fal-
con (TII UAE, 2023) models, using HuggingFace (Wolf
et al., 2019) implementations of model definitions and
datasets. Following SmoothQuant (Xiao et al., 2022), we
extract outlier indices using 512 random sentences from the
Pile dataset (Gao et al., 2020). We consider up to 5% (based
on the model size) of the input features as outliers in the
linear layers. During the GPTQ weight quantization, we
randomly select 128 samples with 2048 sequence length
from the C4 dataset (Raffel et al., 2020). We apply sym-
metric quantization to weights and asymmetric quantization
to activations. Clipping thresholds for weight quantization
are found via a linear search over the squared error. Our
scheme quantizes a 70B model in less than 2 hours on a
single NVIDIA A100 GPU.

--- PAGE 6 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
Outlier ColumnsX
4-bit Per-T oken
Quantization
INT4 MatMulOutlier Extraction
Quantizable Columns
Dequantize and
cast to FP16FP16Outlier W eights
FP16 MatMul
YQuantizedÂ 
Weights
INT4Â  W
Transpose V iewTranspose V iewWeight MatrixÂ  Input MatrixÂ 
Figure 5. Schematic for the forward pass of a linear layer ( XWT)
with QUIK-4B. In the first step, the input outlier features are
extracted based on the pre-defined indices and the rest of the input
values will be quantized using per-token quantization. The INT4
MatMul will be applied using the quantized weights, calculated
offline (see Figure 4). Finally, the output will be dequantized, cast
to FP16, and added to the result of FP16 MatMul.
(8192, 1024) (8192, 8192) (28672, 8192)
Layer size0.00.20.40.60.81.01.2Time relative to V1V1
V2V3V1
V2
V3V1
V2V3Data Split
MetaQuantization
INT MatmulFP Matmul
Dequantization
Figure 6. Operation timings in different QUIK-4B versions with
256 outliers relative to the first version for different matrix sizes.
Hatched bars represent fused operations. Experiment executed
with input size 2048 on an RTX3090 GPU.
4.1 Accuracy Recovery
Accuracy Comparison on OPT. We first compare the ac-
curacy of QUIK with prior 4W4A quantization methods:
SmoothQuant (Xiao et al., 2022), RPTQ (Yuan et al., 2023)
and OmniQuant (Shao et al., 2023).
Table 1 shows the results of all methods for 4 larger OPT
models on the WikiText2 task (Merity et al., 2016). We
observed that, with QUIK, the accuracy of OPT modelsModelOPT
6.7B 13B 30B 66B
Baseline 10.86 10.13 9.56 9.34
SmoothQuant 1.8e4 7.4e3 1.2e4 2.2e5
RPTQ 17.83 17.83 11.50 11.16
OmniQuant 12.24 11.65 10.60 10.29
QUIK (ours) 11.18 10.78 10.08 9.66
Table 1. Perplexity of 4-bit OPT models on the WikiText2 dataset.
SmoothQuant, RPTQ, and OmniQuant results are taken from Shao
et al. (2023), RPTQ denotes their improved numbers. Note that for
the 66B model, all prior schemes keep 0.71% of the linear layer
operations in FP16 (the Head), while, by excluding outliers from
quantization, we retain 2.78% of operations in FP16.
remains consistent even when employing a uniform number
of outliers for all layers (instead of using a percentage of the
input features). Consequently, we employed 256 outliers
across all linear modules (which is â‰ˆ3%of OPT-66Bâ€™s
hidden size). As can be seen, by effectively leveraging a
small amount of full-precision outlier columns, QUIK can
significantly outperform prior 4-bit methods, dropping only
0.3to0.5points in perplexity relative to the full precision
baseline. We emphasize that, for a fair comparison, QUIK
quantizes alllinear backbone layers to 4-bit here. Additional
results are presented in Appendix A.
Accuracy on LLaMA-2 and Falcon Models. Next, we
move to LLaMA-2 and Falcon models. See Table 2 for the
results on WikiText2. As can be seen, QUIK-4B can pre-
serve the accuracy in all models with at most 0.5perplexity
loss for the LLaMA-2 models, and 0.3for Falcon models.
ModelLLaMA-2 Falcon
7B 13B 70B 7B 40B 180B
Baseline 5.47 4.88 3.20 6.59 5.23 3.30
SmoothQuant 83.12 35.88 - - - -
OmniQuant 14.26 12.30 - - - -
QUIK-4B 5.84 5.28 3.74 6.90 5.46 3.61
Table 2. Perplexity results of QUIK for 4-bit LLaMA-2 and Falcon
models on WikiText2. We use 256 outliers for all linear layers.
For the down-projection (in LLaMA-2 models) and FC2 layers
(in Falcon models), we use 8-bit quantization, and increase the
number of outliers (in FP16) proportionally to the number of input
features of these layers (which is not the case for other schemes).
Results for SmoothQuant and OmniQuant follow Shao et al. (2023).
OmniQuant does not present results for the Falcon family and
LLaMA2-70B in 4-bit. RPTQ does not present any results for
LLaMA-2 and Falcon families.
Zero-Shot Accuracy. Next, we evaluate the impact of
QUIK on the accuracy of zero-shot tasks. To this end, we
study the average accuracy of the largest LLaMA-2 and OPT
models on five popular zero-shot tasks: PIQA (Tata & Patel,
2003); WinoGrande (Sakaguchi et al., 2021); HellaSwag

--- PAGE 7 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
Model Bits Arc Challenge Arc Easy HellaSwag PIQA WinoGrande Avg. Score
OPT-30BFP16 38.05 65.36 72.28 78.13 68.43 64.45
QUIK-4B 36.69 64.39 70.84 77.75 67.01 63.34
OPT-66BFP16 40.02 67.26 74.87 79.82 68.82 66.16
QUIK-4B 38.82 64.73 73.68 79.43 68.82 65.10
LLaMA2-13BFP16 48.98 77.44 79.38 80.52 72.22 71.70
QUIK-4B 48.04 74.92 78.36 79.22 71.90 70.49
LLaMA2-70BFP16 57.34 80.98 83.81 82.75 77.98 76.57
QUIK-4B 56.14 79.00 81.57 81.56 76.56 74.97
Table 3. LM eval harness results of QUIK on OPT, LLaMA-2, and Falcon families. using 256 outliers.
(Zellers et al., 2019); Arc (Easy and Challenge) (Boratko
et al., 2018). We use the LM Evaluation Harness (Gao et al.,
2021) with default parameters in our experiments. Table 3
shows the averaged accuracy of QUIK over zero-shot tasks.
Similar to the generation task, QUIK preserves the accuracy
of zero-shot tasks with at most a 1.5% accuracy drop for
LLaMA-2 models and 1.1% for OPT models.
8-Bit Quantization. We compare the accuracy of QUIK-8B
with SmoothQuant (Xiao et al., 2022) on OPT, LLaMA-
2, and Falcon. We use asymmetric per-token quantization
for activations and symmetric quantization for the weights
in SmoothQuant (these are the same basic settings as for
QUIK). Table 4 shows that although both schemes are close
to lossless in terms of perplexity difference to FP16, QUIK
produces higher accuracy results in most cases, compared to
SmoothQuant. Further, it is unclear whether SmoothQuant
can be applied to models with parallel attention , such as
the Falcon-7B model, where the MLP and Attention blocks
share the same layer norm for their input, as this prevents
scale factor fusion. See Appendix C for further results.
Table 4. Accuracy results for 8-bit models on WikiText2. We use
256 outliers in QUIK experiments. Following the SmoothQuant
paper, we use Î±= 0.8for LLaMA-2 models and Î±= 0.5for
OPT and Falcon families.
ModelOPT LLaMA-2 Falcon
30B 66B 13B 70B 40B 180B
FP16 9.56 9.34 4.88 3.20 5.23 3.30
SmoothQuant 9.59 9.80 4.94 3.48 5.26 3.30
QUIK-8B 9.51 9.29 4.89 3.33 5.23 3.31
Outlier-Free Layers. Finally, we study the effect of keep-
ing multiple linear layers without any outliers. This might
help boost end-to-end performance by removing all the
outlier-related overheads during the forward pass. (Al-
though, as we show later, these overheads are minor.) Ta-
ble 5 shows how the accuracy of different models changes
when we use different absolute threshold values (shown by
T), extracted using a linear search, for the outliers. We con-Model T LLaMA2-70B Falcon-180B
FP16 - 3.2 3.30
QUIK-4B0 3.74 (0) 3.61 (0)
2.0 3.75 (10) 3.61 (3)
3.0 3.85 (30) 3.61 (4)
4.0 5.15 (58) 3.72 (14)
8.0 5.92 (219) 3.73 (115)
Table 5. Study of zero outlier setting on WikiText2 using 256 out-
liers. We use zero outliers when the maximum of scale is less than
threshold T. For each experiment, the number of linear layers with
zero outliers is written in parentheses.
clude that there is no universal threshold across all models,
which would preserve accuracy across all models. For ex-
ample, Falcon-180B can achieve reasonable accuracy even
if 24% of the linear layers (115 out of 480) contain zero
outliers. However, this is not the case for smaller models:
LLaMA2-70B can recover accuracy with up to 5% of the
linear layers (30 out of 560) having zero QUIK outliers. We
provide additional experiments in Appendix D.
4.2 Performance Analysis
We now examine the performance of the QUIK implemen-
tation by evaluating different aspects of our kernel. We use
PyTorch/1.13, CUDA/11.8, Huggingface Transformers/4.34.
We run all our experiments on RTX 3090 GPUs as our main
goal is to accelerate LLM inference on commodity GPUs.
Appendix G shows similar results on RTX 3080 GPUs.
Peak Memory Usage. First, we assess the memory usage
of our quantized models. In Table 6, we evaluate the peak
memory usage across different configurations for the OPT
and LLaMA-2 families. For OPT-66B, the QUIK-8B and
QUIK-4B models demonstrate peak memory reductions of
approximately 47% (compared to the ideal 50% reduction)
and 74% (compared to the ideal 75% reduction), respec-
tively. For the LLaMA2-70B model, the reductions are 32%
for QUIK-8B and 67% for QUIK-4B. This is because we
keep the down-projection in 8-bits and use additional out-

--- PAGE 8 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
ModelOPT LLaMA-2
13B 30B 66B 7B 13B 70B
Baseline 30.5 67.4 162.1 14.9 28.0 147.1
QUIK-8B 16.1 39.3 81.2 14.6 25.2 99.3
QUIK-4B 10.7 24.6 45.1 7.1 12.1 49.1
Table 6. Peak memory usage (in GB) in an end-to-end benchmark.
In total, the outliers take 2.71 GB and 4.06 GB for OPT-66B and
LLaMA2-70B models respectively.
liers. Additional overheads come from auxiliary buffers,
which differ for various layer sizes.
Ideal and Layer-wise Speedups. Next, we evaluate the
ideal speedups, as well as the actual speedups we measure in
each Transformer block separately. The results in Figure 3
depict â€œidealâ€ computational power for layer-wise matrix
multiplications at different precision levels, without tak-
ing into account any quantization/dequantization overheads.
Here, we focus on realizable speedups when executing Al-
gorithm 1, which includes mixed-precision multiplication
as well as compression and decompression operations.
In Figure 7, we compare the layer-wise performance of quan-
tized linear layers (QUIK-4B uses 256 outliers per layer)
relative to FP16, for a full implementation of our algorithm.
The matrix sizes correspond to layers in LLaMA models.
We observe that QUIK-4B can achieve slightly higher than
4Ã—speedup on large layers and over 2Ã—on smaller ones.
Thus, the speedups of raw low-precision matmul speedups
can partially â€œhideâ€ the overheads of QUIK.
End-to-end speedups. Finally, we also demonstrate the
end-to-end speedup benefits of QUIK models. For this pur-
pose, we integrate QUIK into the widely used HuggingFace
PyTorch implementation, by replacing linear layers with
4-bit (and 8-bit) QUIK re-implementations. For the LLaMA
model, we use FlashAttention (Dao et al., 2022) for all mod-
els (including FP16). The number of outliers in QUIK-4B
is set to 256 except for the special case of down projection
layers in LLaMA and FC2 in the Falcon models, which we
quantize to 8 bits with more than 600 outliers.
In Figure 9, we compare the throughput improvements of
prefill passes (for single batches with 2048 tokens) for quan-
tized models, relative to the corresponding FP16 version.
The bar plot shows throughput improvements of QUIK-4B
compared to FP16. The annotations to the baseline repre-
sent its actual throughput values in our experiments. For
instance, OPT-66B using FP16 linear layers achieved 439
tokens/s whereas the same model inference with QUIK-4B
linear layers resulted in 1343 tokens/s. This shows that,
in addition to a close to 4Ã—memory reduction, which re-
duces the number of required GPUs for inference, QUIK
(4096, 4096) (8192, 1024) (11008, 4096) (5120, 5120) (8192, 8192) (28672, 8192)
Matrix size01234SpeedupBaseline
QUIK-8B
QUIK-4BFigure 7. Layer-wise speedups on a single RTX3090 for different
layer sizes and compression types. QUIK-4B with 256 outliers,
QUIK-8B without outliers.
0123456Speedup
0.5k t/s1.4k t/s1.4k t/s1.7k t/s1.9k t/sFP16 Baseline
SmoothQuant QUIK-8B
Ideal 8 BitsQUIK-4B
Ideal 4 Bits
7%6%
61%7%19%
Quantization
FP MatMul
INT MatMul
FlashAttn
Other
Figure 8. Performance results and overhead breakdown on
LLaMA2-70B on a machine with 8x RTX 3090 GPUs. Left:
Speedup vs. FP16 and vs. an ideal implementation, without over-
heads, for 4-bit and 8-bit QUIK kernels with absolute throughput
values. Right: Performance breakdown of end-to-end inference
benchmark for QUIK-4B with outliers in terms of MatMul time
vs. quantization overheads.
also achieves up to 3.4Ã—higher throughput relative to FP16,
with the biggest improvements attained on the largest mod-
els (LLaMA2-70B), where the relative impact of overheads
is lowest. The memory reduction is important in the Falcon
inference case: we were not able to run Falcon-180B in
full precision on 8xRTX3090 GPUs, as the max memory
peak of the model is more than 360GB. However, QUIK-
4B allows us to run full inference of this 180B model on a
single server resulting in 542 tokens/second. Therefore, we
estimated speedups for the FP16 180B model in Figure 9(c)
based on the runtime of a single Transformer block.
We emphasize that the speedups in our end-to-end exper-
iments are exclusively through QUIK accelerated linear
layers. All other functions are precisely the same. As shown
in Figure 8 (Right), the overheads from attention, softmax,
or layernorm operations become significant when a large
fraction of the computation occurs in 4-bit precision.
Outlier Performance Costs. To illustrate the performance
implications of supporting outliers, in Figure 8 (left) we pro-

--- PAGE 9 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
6.7B 13B 30B 66B
Model0.00.51.01.52.02.53.0Speedup
3491 tkns/s
2065 tkns/s
956 tkns/s
439 tkns/sBaseline
QUIK-4B
(a) OPT
7B 13B 70B
Model0.00.51.01.52.02.53.03.5Speedup
4039 tkns/s
2409 tkns/s
480 tkns/sBaseline
QUIK-4B (b) LLaMA-2
7B 40B 180B*
Model0.00.51.01.52.02.53.0Speedup
2995 tkns/s
633 tkns/sBaseline
QUIK-4B (c) Falcon
Figure 9. End-to-end inference speedups for QUIK-4B with outliers relative to the FP16 baseline, on NVIDIA RTX 3090 GPUs. Falcon-
180B results are from single Transformer block inference benchmark.
01020304050607080
Layer10âˆ’210âˆ’1100101102103Variance
Q/K/V Out-Proj Up/Gate-Proj Down-Proj
Figure 10. The variance of the inputs in different layers of
LLaMA2-70B. The "Down-Proj" layers have significantly larger
variances, resulting in poor 4-bit quantization.
vide end-to-end speedups for variants of the HuggingFace
integration where we directly use 8-bit and 4-bit kernels,
without preserving accuracy (Ideal 8-bit and 4-bit), relative
to our accuracy-preserving QUIK implementations.
We observe that the 8-bit implementation provides close to
ideal speedups, reducing the number of GPUs from 7 to
5. QUIK-4B (taking outliers into account) performs â‰ˆ15%
better, further reducing the number of required GPUs to
3, using less than 50 GB of GPU memory. The perfor-
mance impact of outlier selection (hence mixed precision
matrix multiplication) and selective 8-bit quantization (for
down-projection MLP layer) is shown in the comparison
with Ideal 4-bit. QUIK-4B is within 15% of Ideal 4-bit
performance. (However, it is currently not known how a
model with weights and activations in 4 bits could recover
accuracy.) The justification for this performance impact is
provided in Figure 8 (right), where we break down the per-
operation overheads for LLaMA2-70B inference. Specifi-
cally, we observe here and in Figure 6 that the overheads
of quantization and full precision multiplication can take
QKV Out-Proj Up-Proj Gate-Proj Down-Proj LM-Head Sum
Module020406080100FLOP/s (%)LLaMA2-70B Linear Modules
FP16
INT8
INT4Figure 11. FLOP/s analysis of the LLaMA2-70B linear layers with
QUIK. We use 3.125% outliers (256 outliers in all layers and 896
for the down-projection layer) and 2048 sequence length.
up a large fraction of the overall operation time, especially
for smaller matrices. This illustrates the trade-offs between
performance and accuracy for a specific model.
4.3 Ablation Studies
We now provide in-depth examples for using QUIK on
two large models: LLaMA2-70B, and Falcon-180B. The
former model is important as it shows high performance
across different tasks (Touvron et al., 2023). The latter is
the largest openly-available GPT-type model.
4.3.1 Case Study 1: LLaMA2-70B
First, we study the FLOP breakdown across precisions using
QUIK-4B on LLaMA2-70B. Next, we study the effect of
key parameters of QUIK: 8-bit Down-Projection, and Out-
lier Counts. We provide additional ablation in Appendix B.
8-bit Down-Projection. Within the MLP module of the
LLaMA2-70B model, three linear layers are present, re-

--- PAGE 10 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
ferred to as "Up-Proj", "Gate-Proj", and "Down-Proj". "Up-
Proj" and "Gate-Proj" share an input (MLP input) and apply
their respective linear transformations to it. Subsequently,
the output of "Gate-Proj" is subjected to a SiLU activation
function. Lastly, the input for the "Down-Proj" layer is
constructed by taking the Hadamard product of the outputs
from "Up-Proj" and "Gate-Proj".
LLaMA-2 7B 13B 70B
Baseline 5.47 4.88 3.20
QUIK-4B 5.84 5.28 3.74
4-bit Down-Proj 8.87 7.78 6.91
Table 7. Ablation for keeping the down-projection layer in 4-bits.
Figure 10 shows the variance of the input across various
layers in LLaMA2-70B, which we use as a guide to choose
both the number of outliers and the set of layers to be exe-
cuted in 8 bit precision. Specifically, it can be observed that
the "Down-Proj" layers have large input variance, mainly
due to the Hadamard product of the previous two outputs,
resulting in poor accuracy for 4-bit quantization. To address
this, we employ 8-bit quantization for both the weights
and activations within the "Down-Proj" layers of LLaMA2
models. Table 7 shows that keeping the down-projection
layers in 8-bit is critical for high accuracy on LLaMA2, as
it improves perplexity by >2 points, across all models.
FLOP/s Analysis. Figure 11 shows the percentage of the
FLOP/s we keep in each precision (INT4 for base weights,
FP16 for outliers, and INT8 for down-projection layers) in
LLaMA2-70B. More precisely, for 256 outliers, we perform
â‰ˆ70% of the operations in 4-bit and â‰ˆ27% using 8-bits.
Method OutliersDown-Proj WikiText2
Outliers (PPL)
Baseline - - 3.20
QUIK-4B128 448 3.80
256 896 3.74
512 1792 3.67
1024 3584 3.62
Table 8. Ablation study of different outlier numbers in QUIK for
the LLaMA2-70B model.
Outlier Count. Finally, we look at how different outlier
counts affect the WikiText2 score for the LLaMA2-70B
model. In Table 8, we observe that increasing the outliers
from 128 to 1024 results in a 0.2 perplexity improvement.
We also adjusted the outliers for down-projection layers,
ensuring there are 3.5x times more than the other linear
layers, to match input size. Our results show that using 256
outliers is already a good choice for our experiments. Using
additional outliers does not significantly improve accuracy.Precision SparsityDense WikiText2 Mem. Peak
Layers (PPL) (rel to FP16)
FP160% All 3.30 100%
2:4 None 6.13 -
QUIK-4B0% All 3.61 38 %
2:4 None 6.62 25%
2:4 Attn. Blocks 6.34 26%
2:4 MLP Blocks 3.93 36%
Table 9. Accuracy results for quantized + 2:4 sparsified on Falcon-
180B. For the quantized experiments, we apply quantization on
all layers with 256 outliers but keep some of the layers in dense
(mentioned in the Table). By memory peak we mean the maximal
amount of allocated memory (in GB) during the inference of a
single Transformer block.
4.3.2 Case Study 2: Falcon-180B
In this section, we revisit applying QUIK to Falcon-180B,
the largest GPT-style openly-available model. The model
requires â‰ˆ365GB of GPU memory for the inference, which
makes it impossible to run inference on a GPU server with
8x RTX3090 nodes (192 GB memory), illustrating the im-
portance of reducing the memory footprint of this model.
The results in Tables 2 and 5, and Figure 9 already presented
accuracy and performance results for this model for QUIK
variants. Here, we investigate leveraging the hardware-
supported 2:4 sparse + INT4 format by combining QUIK
with 2:4 sparsity for this model.
Joint INT-4 Quantization and 2:4 Sparsification. A sim-
ple solution for pushing the limits of the model compression
is to sparsify the already quantized model (or vice-versa).
However, this results in high accuracy drops. Instead, we
extend the SparseGPT algorithm (Frantar & Alistarh, 2023)
to support our outlier scheme to jointly quantize and sparsify
the model, while keeping the outlier features in dense FP16.
In Table 9, we present the results of quantizing all layers,
but selectively keep certain layer types dense. Specifically,
we found that one-shot pruning of the weights in the atten-
tion blocks to the 2:4 pattern throughout all layers largely
preserves accuracy, leading to small memory gains. We
present 8-bit results in the same setting in Appendix E.
5 C ONCLUSION AND FUTURE WORK
We presented a hybrid quantization scheme called QUIK,
executing a large majority of inference computation in 4-bit
precision, with efficient GPU support. We have shown sig-
nificant speedups using QUIK across several LLM types,
on commodity hardware. In future work, we plan to ex-
amine a unified implementation which would support both
single-token and multi-token inference on top of QUIK
weights, integration with speculative decoding (Leviathan
et al., 2023), and additional models.

--- PAGE 11 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
REFERENCES
Boratko, M., Padigela, H., Mikkilineni, D., Yuvraj, P., Das,
R., McCallum, A., Chang, M., Fokoue-Nkoutche, A., Ka-
panipathi, P., Mattei, N., et al. A systematic classification
of knowledge, reasoning, and context within the ARC
dataset. arXiv preprint arXiv:1806.00358 , 2018.
Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srini-
vasan, V ., and Gopalakrishnan, K. Pact: Parameterized
clipping activation for quantized neural networks. arXiv
preprint arXiv:1805.06085 , 2018.
Dao, T., Fu, D. Y ., Ermon, S., Rudra, A., and RÃ©, C. FlashAt-
tention: Fast and memory-efficient exact attention with
io-awareness. arXiv preprint arXiv:2205.14135 , 2022.
Dettmers, T. and Zettlemoyer, L. The case for 4-bit pre-
cision: k-bit inference scaling laws. arXiv preprint
arXiv:2212.09720 , 2022.
Dettmers, T., Lewis, M., Belkada, Y ., and Zettlemoyer, L.
LLM.int8(): 8-bit matrix multiplication for transformers
at scale. Advances in Neural Information Processing
Systems 35: Annual Conference on Neural Information
Processing Systems 2022, NeurIPS 2022 , 2022.
Dettmers, T., Svirschevski, R., Egiazarian, V ., Kuznedelev,
D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T.,
and Alistarh, D. Spqr: A sparse-quantized representation
for near-lossless llm weight compression. arXiv preprint
arXiv:2306.03078 , 2023.
Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R.,
and Modha, D. S. Learned step size quantization. arXiv
preprint arXiv:1902.08153 , 2019.
Frantar, E. and Alistarh, D. Sparsegpt: Massive language
models can be accurately pruned in one-shot. 2023.
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:
Accurate post-training quantization for generative pre-
trained transformers. arXiv preprint arXiv:2210.17323 ,
2022.
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,
et al. The pile: An 800gb dataset of diverse text for
language modeling. arXiv preprint arXiv:2101.00027 ,
2020.
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
C., Golding, L., Hsu, J., McDonell, K., Muennighoff,
N., et al. A framework for few-shot language model
evaluation. Version v0. 0.1. Sept , 2021.
Kim, S., Hooper, C., Gholami, A., Dong, Z., Li,
X., Shen, S., Mahoney, M. W., and Keutzer, K.
Squeezellm: Dense-and-sparse quantization. arXiv
preprint arXiv:2306.07629 , 2023.Lee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Lessons
learned from activation outliers for weight quantization in
large language models. arXiv preprint arXiv:2306.02272 ,
2023.
Leviathan, Y ., Kalman, M., and Matias, Y . Fast inference
from transformers via speculative decoding. In Inter-
national Conference on Machine Learning , pp. 19274â€“
19286. PMLR, 2023.
Li, Q., Zhang, Y ., Li, L., Yao, P., Zhang, B., Chu, X., Sun,
Y ., Du, L., and Xie, Y . Fptq: Fine-grained post-training
quantization for large language models. arXiv preprint
arXiv:2308.15987 , 2023.
Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and
Han, S. Awq: Activation-aware weight quantization
for llm compression and acceleration. arXiv preprint
arXiv:2306.00978 , 2023.
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843 , 2016.
NVIDIA. Nvidia nsight compute. URL https://
developer.nvidia.com/nsight-compute .
NVIDIA. Nvidia cutlass library, 2023. URL https://
github.com/NVIDIA/cutlass/ .
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. Advances in neural information
processing systems , 32, 2019.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. OpenAI blog , 1(8):9, 2019.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., and Liu, P. Exploring
the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research , 21
(140):1â€“67, 2020.
Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y .
Winogrande: An adversarial winograd schema challenge
at scale. Communications of the ACM , 64(9):99â€“106,
2021.
Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
Zhang, K., Gao, P., Qiao, Y ., and Luo, P. Omniquant:
Omnidirectionally calibrated quantization for large lan-
guage models, 2023.
Tata, S. and Patel, J. M. PiQA: An algebra for querying pro-
tein data sets. In International Conference on Scientific
and Statistical Database Management , 2003.

--- PAGE 12 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
TII UAE. The Falcon family of large language models.
https://huggingface.co/tiiuae , May 2023.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., et al. Llama 2: Open foundation and fine-
tuned chat models. arXiv preprint arXiv:2307.09288 ,
2023.
Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C.,
Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,
et al. Huggingfaceâ€™s transformers: State-of-the-art natural
language processing. arXiv preprint arXiv:1910.03771 ,
2019.
Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han,
S. Smoothquant: Accurate and efficient post-training
quantization for large language models. arXiv preprint
arXiv:2211.10438 , 2022.
Yuan, Z., Niu, L., Liu, J., Liu, W., Wang, X., Shang, Y ., Sun,
G., Wu, Q., Wu, J., and Wu, B. Rptq: Reorder-based post-
training quantization for large language models. arXiv
preprint arXiv:2304.01089 , 2023.
Zellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi,
Y . Hellaswag: Can a machine really finish your sentence?
arXiv preprint arXiv:1905.07830 , 2019.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., et al.
OPT: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 , 2022.

--- PAGE 13 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
A F ULL OPT A CCURACY RESULTS
Table 10 shows the perplexity results of OPT models. We use symmetric quantization for the weights in all our experiments.
The results suggest that in a 4-bit setting, considering outlier features is crucial to preserve the accuracy even in small models
(like OPT-1.3b). We note that 256 outliers is equivalent to 12.5% of the 1.3B modelâ€™s hidden size (and 2.77% of the 66B
modelâ€™s hidden size).
Model OPT-1.3b OPT-6.7b OPT-13b OPT-30b OPT-66b
Task WIKI PT C4 WIKI PT C4 WIKI PT C4 WIKI PT C4 WIKI PT C4
Baseline 14.63 16.96 14.72 10.86 13.09 11.74 10.13 12.34 11.20 9.56 11.84 10.69 9.34 11.36 10.28
GPTQ-4B 15.89 18.83 15.90 11.43 13.81 12.21 10.38 12.65 11.41 9.60 12.02 10.83 9.65 11.63 10.56
0 Outliers 15k 9k 10k 10k 9k 9k 9k 12k 9k 12k 13k 17k 12k 13k 10k
64 Outliers 26.259 27.143 22.981 11.473 13.888 12.348 11.031 13.305 11.971 10.283 12.557 11.267 9.851 11.965 10.742
128 Outliers 17.638 19.709 16.799 11.671 13.809 12.314 10.964 13.241 11.894 10.339 12.564 11.279 9.805 11.842 10.653
256 Outliers 17.358 19.525 16.607 11.184 13.811 12.262 10.779 13.175 11.847 10.078 12.465 11.226 9.662 11.793 10.635
Table 10. Perplexity scores of QUIK-4B over various OPT models with different outliers on three datasets: WikiText2 (WIKI), Pen
Treebank (PT), and C4. GPTQ-4B only quantizes the weights (using int-4 symmetric quantization) and keeps the activations in FP16.
B F ULL LLAMA-2 A CCURACY RESULTS
Table 11 shows the perplexity of QUIK on LLaMA-2 models. We provide a list of tricks to improve the quality of the model
without too much overhead. We found that keeping the down-proj layer in 8 bits can improve the perplexity by about 3
points. Also, we found weight clipping as a cheap and efficient trick for improving the accuracy of QUIK-4B.
LLaMA-2 Down-Proj Clipping 7B 13B 70B
FP16 W16A16 - 5.47 4.88 3.2
GPTQ-4B W4A16 - 6.24 5.25 3.68
QUIK-4B W4A4 - 8.78 7.78 6.91
QUIK-4B W4A16 - 6.09 5.49 3.98
QUIK-4B W4A8 - 6.11 5.5 4.0
QUIK-4B W8A8 - 5.98 5.37 3.87
QUIK-4B W8A8 âœ“ 5.84 5.28 3.74
Table 11. LLaMA-2 perplexity results on WikiText2 using 256 outliers. We apply clipping only during the weight quantization.
C F ULL INT-8 A CCURACY RESULTS
Table 12 shows QUIK-8B comparison against SmoothQuant on the WikiText2 dataset. We use per-token (per-column)
quantization for the activations (weights) in SmoothQuant and only apply the quantization on the linear layers (which is the
case for QUIK also). We exclude the Falcon-7B model as this model has a single layer-norm for both MLP and Attention
blocks and it is not clear how the weights of the FC1 and KQV will be updated in the SmoothQuant algorithm.
D Z ERO-OUTLIER FULL RESULTS
Table 13 shows the results of keeping different numbers of layers without outliers for different models.

--- PAGE 14 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
ModelOPT LLaMA-2 Falcon
1.3b 6.7B 13B 30B 66B 7B 13B 70B 40B 180B
FP16 14.63 10.84 10.13 9.56 9.34 5.47 4.88 3.20 5.23 3.30
SmoothQuant 14.70 10.89 10.37 9.59 9.80 5.58 4.94 3.48 5.26 3.30
QUIK-8B 14.62 10.84 10.13 9.51 9.29 5.48 4.89 3.33 5.23 3.31
Table 12. Accuracy results for 8bit models on WikiText2. We use 256 outliers in QUIK experiments. Following the SmoothQuant paper,
we use Î±= 0.8hyperparameter for LLaMA-2 models and Î±= 0.5for OPT and Falcon families.
Model TLLaMA-2 Falcon
7B 13B 70B 7B 40B 180B
FP16 - 5.47 4.88 3.2 6.59 5.23 3.30
QUIK-4B0 5.84 (0) 5.28 (0) 3.74 (0) 6.90 (0) 5.46 (0) 3.61 (0)
2.0 5.91 (5) 5.33 (3) 3.75 (10) 6.90 (3) 5.46 (1) 3.61 (3)
3.0 6.09 (11) 5.34 (8) 3.85 (30) 6.91 (14) 5.46 (2) 3.61 (4)
4.0 6.13 (21) 5.36 (17) 5.15 (58) 6.93 (27) 10.56 (8) 3.72 (14)
8.0 12.93 (55) 21.85 (66) 5.92 (219) 6.94 (57) 10.61 (33) 3.73 (115)
Table 13. Study of zero outlier setting on WikiText2 using 256 outliers. We use zero outliers when the maximum of scale is less than
threshold T. For each experiment, the number of linear layers with zero outliers is written in parentheses.
E 2:4 S PARSITY + INT8 Q UANTIZATION
Table 14 shows the accuracy results of applying QUIK-8B with 2:4 sparsity across all models. The results suggest that the
main accuracy drop is from introducing 2:4 sparsity to the weight matrices and keeping some of the layers in dense is crucial
to preserve the accuracy (See section 4.3.2).
Model SparsityOPT LLaMA-2 Falcon
1.3b 6.7B 13B 30B 66B 7B 13B 70B 7B 40B 180B
FP16 0% 14.63 10.84 10.13 9.56 9.34 5.47 4.88 3.20 6.59 5.23 3.30
SparseGPT 2:4 24.08 14.15 12.93 10.93 10.08 10.97 8.78 5.70 12.33 12.33 6.13
QUIK-8B0% 14.62 10.84 10.13 9.51 9.29 5.48 4.89 3.33 6.59 5.23 3.31
2:4 22.69 14.59 12.87 11.06 10.24 11.07 8.66 5.89 11.07 8.09 6.19
Table 14. WikiText2 accuracy results for applying 2:4 sparsity with QUIK-8B. We use 256 outliers in all experiments.
F F ALCON PERFORMANCE BENCHMARK
We also explore the performance improvements of Falcon (TII UAE, 2023) models. The 8xRTX3090 machine contains
around 190GB GPU memory which is not enough to run fp16 model inference.

--- PAGE 15 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
(4096, 4096) (8192, 1024) (11008, 4096) (5120, 5120) (8192, 8192) (28672, 8192)
Matrix size01234SpeedupBaseline
QUIK-8B
QUIK-4B
Figure 12. Layer-wise speedups on a single RTX3080 for different layer sizes and compression types. QUIK-4B with 256 outliers,
QUIK-8B without outliers.
G P ERFORMANCE ON RTX3080 GPU S
To validate the performance of QUIK in other types of GPUs we conducted benchmarks on RTX3080 GPUs. The results are
presented in Figure 12. We can see that QUIK-4B still can get more that 4x speedup on another type of GPU.
H P ERFORMANCE AT DIFFERENT SEQUENCE SIZES
We mainly focus our work on the â€œprefillâ€ cases with large sequence sizes (in all our experiments sequence size is equal
to 2048). In this section we explore the performance of the QUIK-4B with other input sequence sizes. In Figures 13(a)
and 13(b) we vary input size from 1 to 8k. In the first expeeriment (Figure. 13(a)) we ran layer-wise benchmark, in the
second (Figure 13(b)) we ran inference of a single Transformer block (on a single GPU). We see that at small input sequence
sizes QUIK is noticably slower for smaller layer size and models. It can be explained by the fact that the gains of low
precision matrix multiplication at this scale can not compensate the quantization overheads. However, at large layer and
model sizes QUIK has up to 2x speedup even with single token input. In case of the large input sequences we see that
performance decreases meaning that low precision matrix multiplication saturates at this scale.
1 16 256 2048 8192
Input size1234Time relative to fp16(4096, 4096)
(8192, 1024)
(8192, 8192)
(28672, 8192)
(a) Layerwise Performance.
1 16 256 2048 8192
Input size0.81.01.21.41.61.82.02.2Throughput relative to fp167B
13B
70B (b) LLaMA Block performance.
Figure 13. Relative performance of QUIK-4B with outliers for different sequence sizes (batch size = 1) on RTX3090 GPU
I P ERFORMANCE WITH VARIOUS OUTLIER NUMBER
In this section we explore the effect of outliers numbers on the QUIK performances. Figure 14 suggests that the timing
of QUIK matmul stays the same across all layer sizes for all non-zero outlier numbers. The zero outliers case superiority
can be explained by the fact that it does not have additional full precision matrix multiplication and input data movements.
However, these results show that QUIK allow increase the outlier number without performance sacrifices which is crucial

--- PAGE 16 ---
QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models
for the accuracy recovery, as we discussed in the Section 4.3.1.
0 32 64 128 256 512 704 1024
Number of outliers0.30.40.50.60.70.8Time per matmul, ms(4096, 4096)
(8192, 1024)(11008, 4096)
(5120, 5120)
Figure 14. Timing results for different QUIK-4B layers sizes with various number of outliers on RTX3090 GPU.
