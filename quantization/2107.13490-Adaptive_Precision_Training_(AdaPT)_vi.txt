# 2107.13490.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2107.13490.pdf
# Kích thước tệp: 1411361 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Adaptive Precision Training (AdaPT):
Một phương pháp huấn luyện lượng tử hóa động cho DNN
Lorenz Kummer12Kevin Sidak13Tabea Reichmann14Wilfried Gansterer15
Tháng 8 năm 2021
Tóm tắt
Lượng tử hóa là một kỹ thuật để giảm thời gian huấn luyện và suy luận của mạng nơ-ron sâu (DNN), điều này rất quan trọng cho việc huấn luyện trong các môi trường hạn chế tài nguyên hoặc các ứng dụng mà suy luận là yếu tố thời gian quan trọng. Các phương pháp lượng tử hóa tiên tiến hiện tại (SOTA) tập trung vào lượng tử hóa sau huấn luyện, tức là lượng tử hóa các DNN đã được huấn luyện trước để tăng tốc suy luận. Mặc dù có nghiên cứu về huấn luyện lượng tử hóa, hầu hết các phương pháp đều yêu cầu tinh chỉnh ở độ chính xác đầy đủ (thường là độ chính xác đơn) trong giai đoạn huấn luyện cuối cùng hoặc áp dụng một độ dài từ toàn cục trên toàn bộ DNN. Điều này dẫn đến việc phân bổ không tối ưu độ rộng bit cho các lớp và do đó, sử dụng tài nguyên không tối ưu. Trong nỗ lực khắc phục những hạn chế như vậy, chúng tôi giới thiệu AdaPT, một chiến lược huấn luyện thưa thớt lượng tử hóa điểm cố định mới. AdaPT quyết định về việc chuyển đổi độ chính xác giữa các epoch huấn luyện dựa trên các điều kiện lý thuyết thông tin. Mục tiêu là xác định trên cơ sở từng lớp độ chính xác thấp nhất không gây mất thông tin do lượng tử hóa trong khi giữ độ chính xác đủ cao sao cho các bước học tương lai không bị ảnh hưởng bởi gradient biến mất. Các lợi ích của DNN hoàn toàn lượng tử hóa kết quả được đánh giá dựa trên một mô hình hiệu suất phân tích mà chúng tôi phát triển. Chúng tôi minh họa rằng có thể đạt được tăng tốc trung bình 1,27 so với huấn luyện tiêu chuẩn trong float32 với tăng độ chính xác trung bình 0,98% cho AlexNet/ResNet trên CIFAR10/100 và chúng tôi tiếp tục chứng minh các mô hình được huấn luyện bằng AdaPT đạt được tăng tốc suy luận trung bình 2,33 với giảm kích thước mô hình 0,52.

1 Giới thiệu
Với xu hướng chung trong học máy hướng tới kích thước mô hình lớn để giải quyết các vấn đề ngày càng phức tạp, suy luận trong các ứng dụng quan trọng về thời gian hoặc huấn luyện dưới các ràng buộc tài nguyên và/hoặc năng suất đang trở nên ngày càng thách thức. Các ứng dụng mà các mô hình hiệu quả về thời gian và không gian là quan trọng bao gồm robot học, thực tế tăng cường, xe tự lái, ứng dụng di động, ứng dụng chạy trên phần cứng tiêu dùng, hoặc nghiên cứu khoa học nơi thường cần một số lượng lớn các mô hình đã huấn luyện để tối ưu hóa siêu tham số. Cùng với điều này, một số kiến trúc DNN phổ biến nhất, như AlexNet [1] hoặc ResNet [2], đã bị ảnh hưởng bởi tình trạng quá tham số hóa và overfitting [3, 4].

Các giải pháp có thể cho những vấn đề nêu trên bao gồm tỉa (xem, ví dụ, [5, 6, 7, 8, 9]), hoặc lượng tử hóa. Trong khi tỉa mạng, nhằm giảm số lượng tham số trong một kiến trúc DNN nhất định (ví dụ bằng cách thưa hóa tensor trọng số và sử dụng định dạng tensor thưa), là một chiến lược thành công để giảm kích thước mạng và thời gian chạy, nó thường không cố gắng điều chỉnh độ rộng bit tham số để khai thác các lợi thế có sẵn trong phép toán độ rộng bit thấp, do đó bỏ qua tiềm năng này để giảm tiêu thụ tài nguyên tính toán. Chúng tôi sử dụng tỉa dưới dạng thưa hóa nhưng trọng tâm của chúng tôi là sử dụng lượng tử hóa tốt hơn. Khi lượng tử hóa, độ chính xác của các tham số và của phép tính được giảm và độ rộng bit thô hơn kết quả cho phép sử dụng hiệu quả hơn tài nguyên tính toán và giảm thời gian chạy. Tuy nhiên, lượng tử hóa phải được thực hiện một cách thận trọng, vì các phương pháp ngây thơ hoặc độ rộng bit quá thấp có thể có tác động tiêu cực đến độ chính xác của mạng và khả năng hội tụ trong quá trình huấn luyện, điều này không thể chấp nhận được cho các trường hợp sử dụng quan trọng. Ví dụ, lượng tử hóa nhị phân như được đề xuất trong [10] giảm yêu cầu bộ nhớ và có thể tăng tốc hiệu quả phép tính, vì phép nhân có thể được thực hiện dưới dạng dịch chuyển bit, nhưng độ chính xác bị ảnh hưởng nhẹ từ phương pháp này và sự hội tụ được chỉ ra là chậm hơn nhiều so với huấn luyện float32.

Các phương pháp lượng tử hóa hiện tại không khai thác đầy đủ tiềm năng của huấn luyện lượng tử hóa. Chúng không tính đến sự khác biệt mà lượng tử hóa có thể có trên các lớp khác nhau trong quá trình huấn luyện, cũng không tăng hoặc giảm độ chính xác được sử dụng một cách động. AdaPT mở rộng các phương pháp này và giới thiệu một cơ chế chuyển đổi độ chính xác mới dựa trên Divergence Kullback-Leibler (KLD) [11], tính toán số bit trung bình bị mất do chuyển đổi độ chính xác. Điều này được thực hiện không chỉ cho toàn bộ mạng, mà trên cơ sở từng lớp, do đó xem xét các hiệu ứng lượng tử hóa khác nhau trên các lớp khác nhau, dẫn đến Huấn luyện Độ chính xác Thích ứng của DNN theo thời gian trong quá trình huấn luyện. Phương pháp của chúng tôi không cần giai đoạn tinh chỉnh nào ở độ chính xác đầy đủ và tạo ra một mạng đã được lượng tử hóa điểm cố định có thể được triển khai trên các phần cứng mạch tích hợp chuyên dụng hiệu suất cao (ASIC) hoặc mảng cổng lập trình trường (FPGA).

1.1 Công trình liên quan
Trong các nghiên cứu chung về độ nhạy của mạng nơ-ron (NN), phân tích nhiễu loạn đã được áp dụng trong lịch sử với trọng tâm vào độ nhạy lý thuyết của các nơ-ron đơn hoặc perceptron đa lớp [12, 13, 14], nhưng gần đây cũng đã mang lại kết quả thú vị cho độ nhạy của các kiến trúc hiện đại đơn giản bằng cách cung cấp các thuật toán hiệu quả để đánh giá mạng có thể so sánh với độ phức tạp của LeNet-5 [15, 16]. Mặc dù các nghiên cứu này chắc chắn cải thiện hiểu biết của chúng ta về tác động của nhiễu loạn lên NN, chúng thiếu tính ứng dụng thực tế cho huấn luyện DNN lượng tử hóa do tính chất phân tích chủ yếu.

Để khám phá sự suy giảm độ chính xác do lượng tử hóa trọng số, hàm kích hoạt và gradient, [17] và [18] đã giới thiệu các framework TensorQuant và QPyTorch, có khả năng mô phỏng các lượng tử hóa phổ biến nhất cho các tác vụ huấn luyện và suy luận trên cơ sở float32. Cả hai framework đều cho phép tự do chọn số mũ và mantissa cho biểu diễn điểm nổi, độ dài từ và bit phân số cho biểu diễn điểm cố định và độ dài từ cho biểu diễn điểm nổi khối cũng như biểu diễn có dấu/không dấu. Vì lượng tử hóa chỉ được mô phỏng trong các framework này, không thể đạt được tăng tốc thời gian chạy trên cơ sở này.

Một số phương pháp đã cố gắng giảm thiểu sự suy giảm độ chính xác suy luận do lượng tử hóa trọng số và/hoặc hàm kích hoạt trong khi tận dụng tăng hiệu suất liên quan. Quantization Aware Training (QAT) [19, 20] kết hợp lượng tử hóa mô phỏng vào huấn luyện mô hình và huấn luyện chính mô hình để bù đắp các lỗi được giới thiệu. Một phương pháp tương tự được thực hiện bởi Uniform Noise Injection for Non-Uniform Quantization (UNIQ) [21], mô phỏng một bộ lượng tử hóa không đồng nhất để tiêm nhiễu tại thời gian huấn luyện để có được một mô hình phù hợp cho suy luận lượng tử hóa. Learned Quantization Nets (LQ-Nets) [22] học các sơ đồ lượng tử hóa tối ưu thông qua việc huấn luyện chung DNN và các bộ lượng tử hóa liên quan. Framework Reinforcement-Learning (ReLeQ) được giới thiệu bởi [23] sử dụng một agent học tăng cường để học độ chính xác phân loại cuối cùng w.r.t. độ rộng bit của mỗi lớp DNN để tìm phân công tối ưu từ lớp đến độ rộng bit cho suy luận. Các phép toán thân thiện với lượng tử hóa chuyên dụng được sử dụng trong [24]. Một phương pháp khác được thực hiện bởi Variational Network Quantization (VNQ) [25] sử dụng huấn luyện dropout biến phân [26] với một prior cảm ứng thưa thớt có cấu trúc [27] để công thức hóa lượng tử hóa sau huấn luyện như vấn đề suy luận biến phân tìm kiếm posterior tối ưu hóa KLD. Tối thiểu hóa entropy bậc cao cho nén mạng nơ-ron (HEMP) [28] giới thiệu một bộ điều chỉnh dựa trên mã hóa entropy tối thiểu hóa entropy tham số lượng tử hóa trong học dựa trên gradient và kết hợp nó với một sơ đồ tỉa riêng biệt [29] để đạt được mức độ nén mô hình cao sau huấn luyện.

Các framework học máy như PyTorch hoặc TensorFlow đã cung cấp khả năng lượng tử hóa tích hợp sẵn. Các phương pháp lượng tử hóa này tập trung vào QAT hoặc lượng tử hóa sau huấn luyện [30, 31]. Cả hai phương pháp này chỉ lượng tử hóa mô hình sau khi huấn luyện và do đó không cung cấp lợi ích hiệu quả trong quá trình huấn luyện. Cần các bước xử lý bổ sung trong hoặc sau huấn luyện làm tăng chi phí tính toán.

Từ góc độ lý thuyết, huấn luyện lượng tử hóa đã được nghiên cứu trong [32] với trọng tâm đặc biệt vào các phương pháp làm tròn và đảm bảo hội tụ trong khi [33] cung cấp phân tích về hành vi của bộ ước lượng thẳng (STE) [34] trong quá trình huấn luyện lượng tử hóa. Đối với huấn luyện phân tán trên môi trường đa node, Quantized Stochastic Gradient Descent (QSGD) [35] kết hợp một họ các sơ đồ nén gradient nhằm giảm giao tiếp giữa các node xảy ra trong quá trình cập nhật gradient của SGD. Điều này tạo ra giảm đáng kể thời gian chạy và có thể đảm bảo hội tụ huấn luyện mạng dưới các giả định tiêu chuẩn [36]. Huấn luyện độ rộng bit rất thấp trên node đơn (trọng số và/hoặc kích hoạt nhị phân, tam phân hoặc tương tự được lượng tử hóa), thường kết hợp với STE, đã được chỉ ra mang lại giảm không tầm thường về thời gian chạy và kích thước mô hình nhưng thường với chi phí tương đối cao cho độ chính xác mô hình, giảm tốc độ hội tụ hoặc không lượng tử hóa hoàn toàn mạng [10, 37, 38, 39, 40, 41] và đại diện có khả năng nhất của nó, Quantized tensor train neural networks (QTTNet) [42], kết hợp phân rã tensor và lượng tử hóa độ rộng bit thấp đầy đủ trong quá trình huấn luyện để giảm đáng kể yêu cầu thời gian chạy và bộ nhớ, vẫn bị suy giảm độ chính xác lên đến 2,5% so với baseline tương ứng. Một phương pháp huấn luyện (lại) lượng tử hóa số nguyên độ rộng bit thấp báo cáo giảm độ chính xác 1% hoặc ít hơn so với baseline float32 đã được giới thiệu bởi [43], nhưng nó yêu cầu các mạng được huấn luyện trước ở độ chính xác đầy đủ làm điểm khởi đầu và mục tiêu được nêu rõ ràng là giảm chi phí tính toán và kích thước mô hình trong quá trình suy luận. Để tăng tốc huấn luyện trên node đơn thông qua lượng tử hóa điểm nổi khối, [44] đã giới thiệu một sơ đồ lượng tử hóa huấn luyện động (Multi Precision Policy Enforced Training, MuPPET). Sau huấn luyện lượng tử hóa, nó tạo ra một mô hình cho suy luận float32.

1.2 Đóng góp
Các giải pháp huấn luyện và suy luận lượng tử hóa hiện tại để lại chỗ cho cải thiện trong một số lĩnh vực. QAT, LQ-Nets, ReLeQ, UNIQ, HEMP và VNQ có khả năng tạo ra các mạng sao cho suy luận có thể được thực hiện dưới lượng tử hóa với sự suy giảm độ chính xác nhỏ (so với baseline, không thể so sánh và không rõ chúng được tối ưu hóa tốt như thế nào), nhưng yêu cầu huấn luyện float32 tốn kém về mặt tính toán và bản thân các thuật toán cũng phát sinh một chi phí nhất định. Huấn luyện QSGD chỉ lượng tử hóa gradient và tập trung vào việc giảm chi phí giao tiếp trong môi trường đa node. Huấn luyện nhị phân, tam phân hoặc tương tự được lượng tử hóa đã được chỉ ra thường đi kèm với chi phí giảm độ chính xác, giảm tốc độ hội tụ hoặc chỉ lượng tử hóa trọng số chứ không phải kích hoạt và đại diện có khả năng nhất của nó QTTNet vẫn không đạt được iso-accuracy.

MuPPET yêu cầu ít nhất N epochs cho N mức lượng tử hóa (độ rộng bit được xác định trước áp dụng toàn cục cho tất cả các lớp) bởi vì chuyển đổi độ chính xác chỉ được kích hoạt ở cuối một epoch và thuật toán cần đi qua tất cả các mức độ chính xác. Các lợi thế tiềm năng từ việc có các mức độ chính xác khác nhau tại các lớp khác nhau của mạng không được khai thác. Tiêu chí chuyển đổi độ chính xác chỉ dựa trên sự đa dạng của gradient của k epochs cuối cùng, không có metric nào được sử dụng để đo lượng thông tin bị mất bằng cách áp dụng một lượng tử hóa nhất định cho trọng số. Các mức độ chính xác chỉ có thể tăng trong quá trình huấn luyện và không bao giờ giảm. Hơn nữa, MuPPET xuất ra một mạng float32 sao cho suy luận phải được thực hiện trong độ chính xác đầy đủ đắt đỏ.

Chúng tôi tiến bộ SOTA bằng cách cung cấp một giải pháp dễ sử dụng cho huấn luyện lượng tử hóa DNN bằng cách sử dụng một cơ chế chuyển đổi độ chính xác intra-epoch dựa trên lý thuyết thông tin có khả năng tăng và giảm độ chính xác của mạng một cách động trên cơ sở từng lớp. Trọng số và kích hoạt được lượng tử hóa đến độ rộng bit thấp nhất có thể mà không mất thông tin theo heuristic của chúng tôi và một mức độ thưa thớt nhất định được cảm ứng trong khi đồng thời giữ độ rộng bit đủ cao để các bước học tiếp theo thành công. Điều này dẫn đến một mạng có lợi thế về kích thước mô hình và thời gian cho huấn luyện và suy luận. Bằng cách huấn luyện AlexNet và ResNet20 trên các bộ dữ liệu CIFAR10/100, chúng tôi chứng minh trên cơ sở một mô hình phân tích cho chi phí tính toán rằng so với baseline float32 AdaPT có tính cạnh tranh về độ chính xác và tạo ra giảm không tầm thường chi phí tính toán (tăng tốc). So với MuPPET, AdaPT cũng có những lợi thế phương pháp luận nội tại nhất định. Sau huấn luyện AdaPT, mô hình được lượng tử hóa hoàn toàn và thưa thớt ở một mức độ nhất định sao cho, không giống như trường hợp với MuPPET, xuất ra một mô hình float32, AdaPT mang lại lợi thế của nó đến giai đoạn suy luận.

2 Bối cảnh

2.1 Lượng tử hóa
Biểu diễn số mô tả cách các số được lưu trữ trong bộ nhớ (được minh họa bằng hình 1) và cách các phép toán số học trên những số đó được thực hiện. Thường có sẵn trên phần cứng tiêu dùng là biểu diễn điểm nổi và số nguyên trong khi biểu diễn điểm cố định hoặc điểm nổi khối được sử dụng trong ASIC hiệu suất cao hoặc FPGA. Độ chính xác số được sử dụng bởi một biểu diễn số nhất định đề cập đến lượng bit được phân bổ cho biểu diễn của một số duy nhất, ví dụ một số thực được lưu trữ trong float32 đề cập đến biểu diễn điểm nổi trong độ chính xác 32-bit. Với những định nghĩa về biểu diễn số và độ chính xác này, nói một cách tổng quát nhất, lượng tử hóa là khái niệm chạy một phép tính hoặc các phần của một phép tính ở độ chính xác số giảm hoặc biểu diễn số khác nhau với ý định giảm chi phí tính toán và tiêu thụ bộ nhớ. Tuy nhiên, thực thi lượng tử hóa của một phép tính có thể dẫn đến việc giới thiệu lỗi thông qua biểu diễn lượng tử hóa machine epsilon mach quá lớn (underflow) để mô tả chính xác các giá trị thực kết quả hoặc phạm vi biểu diễn quá nhỏ để lưu trữ kết quả (overflow).

Lượng tử hóa điểm nổi Giá trị v của một số điểm nổi được cho bởi v=s×b^(p-1)×b^e trong đó s là significand (mantissa), p là độ chính xác (số chữ số trong s), b là cơ số và e là số mũ [45]. Do đó lượng tử hóa sử dụng biểu diễn điểm nổi có thể đạt được bằng cách giảm số bit có sẵn cho mantissa và số mũ, ví dụ chuyển từ biểu diễn float32 sang float16, và được cung cấp sẵn bởi các framework học máy phổ biến cho lượng tử hóa sau huấn luyện [46, 47].

Lượng tử hóa số nguyên Biểu diễn số nguyên có sẵn cho lượng tử hóa sau huấn luyện và QAT (int8, int16 do có sẵn trên phần cứng tiêu dùng) trong các framework học máy phổ biến [46, 47]. Tuy nhiên, huấn luyện lượng tử hóa không được hỗ trợ do kích hoạt lượng tử hóa số nguyên không thể vi phân một cách có ý nghĩa, làm cho lan truyền ngược tiêu chuẩn không thể áp dụng [33]. Các trường hợp đặc biệt của lượng tử hóa số nguyên là lượng tử hóa 1-bit và 2-bit, thường được gọi là lượng tử hóa nhị phân và tam phân trong tài liệu.

Lượng tử hóa điểm nổi khối Điểm nổi khối biểu diễn mỗi số như một cặp số nguyên có dấu WL (độ dài từ) bit x và một hệ số tỷ lệ s sao cho giá trị v được biểu diễn như v=x×b^s với cơ số b=2 hoặc b=10. Hệ số tỷ lệ s được chia sẻ trên nhiều biến (khối), do đó có tên điểm nổi khối, và thường được xác định sao cho modulus của phần tử lớn nhất là 2^[1-b;1] [48]. Phép toán điểm nổi khối được sử dụng trong các trường hợp mà các biến không thể được biểu diễn với độ chính xác đủ trên phần cứng điểm cố định gốc.

Lượng tử hóa điểm cố định Các số điểm cố định có một số chữ số thập phân cố định được gán và do đó mọi phép tính phải được đóng khung sao cho kết quả nằm trong các ranh giới nhất định của biểu diễn [49]. Theo định nghĩa của [50], một số điểm cố định có dấu của độ dài world WL = i + s + 1 có thể được biểu diễn bởi một 3-tuple ⟨s,i,pi⟩ trong đó s biểu thị liệu số có dấu hay không, i biểu thị số bit số nguyên và p biểu thị số bit phân số.

2.2 MuPPET
MuPPET là một thuật toán huấn luyện DNN độ chính xác hỗn hợp kết hợp việc sử dụng biểu diễn điểm nổi khối và điểm nổi. Thuật toán lưu trữ hai bản sao trọng số của mạng: một bản sao master float32 của trọng số được cập nhật trong quá trình truyền ngược và một bản sao lượng tử hóa được sử dụng cho truyền thuận. MuPPET sử dụng biểu diễn điểm nổi khối như được nêu trong mục 2.1 với cơ số b=2 cho lượng tử hóa. Mức độ chính xác i của một lớp l của tất cả các lớp L được định nghĩa là:

qi^l = ⟨WLnet, sweights^l, sact^l⟩i

với WLnet là toàn cục trên mạng, và hệ số tỷ lệ s (cho trọng số và hàm kích hoạt) thay đổi theo lớp, được xác định mỗi khi chuyển đổi độ chính xác được kích hoạt. Hệ số tỷ lệ cho một ma trận X được cho bởi:

sweights,act = ⌊log2(min((UB + 0.5)/X{weights,act}max, (LB - 0.5)/X{weights,act}min))⌋

với X{weights,act}max,min mô tả giá trị tối đa hoặc tối thiểu trong ma trận trọng số hoặc feature maps của lớp. UB và LB mô tả các giới hạn trên và giới hạn dưới của độ dài từ WLnet. Các phần tử riêng lẻ x của nó được lượng tử hóa:

x{weights,act}quant = ⌊x{weights,act} × 2^s{weights,act} + Unif(-0.5, 0.5)⌋

Unif(a,b) là việc lấy mẫu từ phân phối đồng nhất trong khoảng [a,b]. Các tham số -0.5 và 0.5 để thêm vào các giới hạn được chọn để sử dụng tối đa WLnet. Trong quá trình huấn luyện, độ chính xác của trọng số lượng tử hóa được tăng lên bằng cách sử dụng một heuristic chuyển đổi độ chính xác dựa trên đa dạng gradient [51], trong đó gradient của mini-batch cuối cùng ∇f^j_l(w) của mỗi lớp l∈L tại epoch j được lưu trữ và sau một số epoch nhất định (r), đa dạng gradient intra-epoch χs tại epoch j được tính bằng:

χs(w)j = (Σ∀l∈L Σ^j_{k=j-r} ||∇f^k_l(w)||^2_2) / (||Σ^j_{k=j-r} ∇f^k_l(w)||^2_2 × |L|)

Tại epoch j tồn tại một tập hợp các đa dạng gradient S(j) = {χs(w)i ∀ei < j} (e biểu thị epoch mà nó được chuyển vào sơ đồ lượng tử hóa) trong đó tỷ lệ p = maxS(j)/χs(w)i được tính toán. Nếu p vi phạm một ngưỡng trong một số lần nhất định, một chuyển đổi độ chính xác được kích hoạt. Tăng tốc được tuyên bố bởi MuPPET dựa trên một mô hình hiệu suất ước tính mô phỏng phép toán điểm cố định sử dụng NVIDIA CUTLASS [52] để tương thích với GPU, chỉ hỗ trợ phép toán điểm nổi và số nguyên.

Chúng tôi đã chọn MuPPET làm baseline để so sánh AdaPT vì trong số các công trình liên quan được xem xét cho tác vụ huấn luyện lượng tử hóa, MuPPET đến gần nhất với mục tiêu iso-accuracy.

3 AdaPT

3.1 Khởi tạo thân thiện với lượng tử hóa
Mặc dù nghiên cứu tài liệu chuyên sâu và theo hiểu biết tốt nhất của chúng tôi, việc khởi tạo trọng số như chiến lược phản công cho gradient biến mất/bùng nổ tác động đến huấn luyện lượng tử hóa như thế nào chưa được khám phá. Tuy nhiên, kết quả thí nghiệm sơ bộ của cuộc điều tra đầu tiên của chúng tôi về tác động của các chiến lược khởi tạo trọng số khác nhau đã cho thấy khả năng chống chịu của DNN được huấn luyện dưới một sơ đồ lượng tử hóa số nguyên truyền thuận cố định (int2, int4, int8, int16) với bản sao master float32 cho tính toán gradient có mối tương quan mạnh với lựa chọn bộ khởi tạo (hình 2).

Sử dụng Adam [53] làm bộ tối ưu hóa, chúng tôi đã huấn luyện LeNet-5 trên MNIST/FMNIST [54] và AlexNet trên CIFAR10/100 [55] và xem xét mức độ mà các mạng lượng tử hóa kém hơn về độ chính xác so với các mạng baseline được huấn luyện trong float32 phụ thuộc vào lựa chọn bộ khởi tạo (Random Normal, Truncated Normal, Random Uniform, Glorot Normal/Uniform [56], He Normal/Uniform [57], Variance Scaling [58], Lecun Normal/Uniform [59, 60]) và các tham số bộ khởi tạo. Chúng tôi thấy rằng DNN được khởi tạo bởi fan-in truncated normal variance scaling (TNVS) suy giảm ít nhất dưới huấn luyện lượng tử hóa với sơ đồ lượng tử hóa số nguyên cố định như được mô tả ở trên. Điều này tương quan với kết quả được xuất bản bởi [61], người đã giới thiệu một tham số tỷ lệ có thể học được trong sơ đồ lượng tử hóa bất đối xứng của họ để đạt được huấn luyện ổn định hơn. Do đó chúng tôi khởi tạo mạng với TNVS và một hệ số tỷ lệ được chọn theo kinh nghiệm s trong đó nl là số đơn vị đầu vào của tensor trọng số Wl trước huấn luyện lượng tử hóa với AdaPT.

Wl ~ N(μ = √(s/nl), σ = 0, τ = √(3s/nl))

Việc xem xét cách các chiến lược phản công khác (ví dụ khuếch đại gradient, [62], chuẩn hóa gradient [63], chuẩn hóa trọng số [64]) ảnh hưởng đến huấn luyện lượng tử hóa vẫn là một câu hỏi mở.

3.2 Mức độ chính xác
AdaPT sử dụng lượng tử hóa điểm cố định như được định nghĩa trong mục 2.1 bởi vì không giống như điểm nổi khối như được sử dụng bởi MuPPET với độ dài từ toàn cục trên toàn bộ mạng và số mũ được chia sẻ theo lớp, chúng tôi đoán rằng độ dài từ và phân số tối ưu là các thuộc tính cục bộ của mỗi lớp dưới giả thuyết rằng các lớp khác nhau chứa lượng thông tin khác nhau trong các thời điểm khác nhau trong quá trình huấn luyện. Chúng tôi quyết định không sử dụng lượng tử hóa điểm nổi vì điểm cố định cho chúng tôi kiểm soát tốt hơn độ chính xác số và có sẵn trong ASIC hiệu suất cao là nền tảng mục tiêu của chúng tôi và không sử dụng lượng tử hóa số nguyên, đặc biệt là các dạng cực đoan của nó nhị phân hóa và tam phân hóa, bởi vì mục tiêu của chúng tôi là iso-accuracy mà tài liệu cho thấy khó đạt được với lượng tử hóa thô như vậy. Tuy nhiên về nguyên tắc, khái niệm AdaPT có thể được mở rộng cho các biểu diễn khác ngoài điểm cố định. Hơn nữa, AdaPT sử dụng làm tròn ngẫu nhiên, mà kết hợp với biểu diễn điểm cố định đã được chỉ ra bởi [50] liên tục vượt trội hơn làm tròn gần nhất, để lượng tử hóa các số float32. Cho rằng AdaPT không phụ thuộc vào việc một số có dấu hay không, chúng tôi biểu diễn mức độ chính xác của mỗi l∈L đơn giản là ⟨WLl, FLl⟩ trong đó độ dài phân số FLl biểu thị số bit phân số. Đối với một số ngẫu nhiên P∈[0,1], x được làm tròn ngẫu nhiên bởi:

SR(x) = {⌊x⌋, if P ≥ (x - ⌊x⌋)/εmach
         ⌊x⌋ + 1, if P < (x - ⌊x⌋)/εmach}

3.3 Cơ chế chuyển đổi độ chính xác
Chuyển đổi độ chính xác trong huấn luyện DNN lượng tử hóa là nhiệm vụ cân bằng cẩn thận nhu cầu giữ độ chính xác thấp nhất có thể để cải thiện thời gian chạy và kích thước mô hình, nhưng vẫn giữ đủ độ chính xác để mạng tiếp tục học. Trong AdaPT, chúng tôi đã mã hóa những lợi ích đối lập này trong hai phép toán, PushDown Operation và PushUp Operation.

PushDown Operation Xác định lượng thông tin bị mất nếu độ chính xác của biểu diễn điểm cố định của tensor trọng số của một lớp bị giảm, có thể được thực hiện bằng phương pháp heuristic bằng cách diễn giải chuyển đổi độ chính xác như một thay đổi mã hóa. Giả sử một tensor trọng số WlQl của lớp l∈L với đối tác lượng tử hóa ̃WlPl, trong đó Pl, Ql là các phân phối tương ứng. Thì Divergence Kullback-Leibler liên tục [11] (1) biểu diễn số bit trung bình bị mất thông qua việc thay đổi mã hóa của l từ Ql thành Pl, với p và q biểu thị xác suất, và w là các phần tử của tensor trọng số:

D(Pl||Ql) = ∫^∞_{-∞} pl(w)log(pl(w)/ql(w))dw

Sử dụng rời rạc hóa thông qua binning, chúng tôi có được Pl và Ql ở độ phân giải rl thông qua hàm phân phối thực nghiệm:

̃F^rl(w) = (1/(rl+1)) Σ^rl_{i=1} 1_{W^l_i ≤ w} (1)

sau đó có thể được sử dụng trong Divergence Kullback-Leibler rời rạc (2).

KL(Pl||Ql) = Σ_{w∈Wl} Pl(w)log(Pl(w)/Ql(w)) (2)

Sử dụng phương pháp chia đôi, AdaPT hiệu quả tìm lượng tử hóa nhỏ nhất ⟨WL^l_{min}, FL^l_{min}⟩ của Wl sao cho KL(Pl||Ql) ≈ 0 ∀l∈L

PushUp Operation Tuy nhiên, xác định độ chính xác của biểu diễn điểm cố định của Wl tại batch j, sao cho thông tin bị mất thông qua lượng tử hóa là tối thiểu nhưng vẫn có đủ độ chính xác cho các batch tiếp theo j+1 để tiếp tục học, là một nhiệm vụ không tầm thường. Chỉ lượng tử hóa Wl ở đầu huấn luyện thành biểu diễn điểm cố định độ chính xác thấp (ví dụ ⟨WLl, FLl⟩ = ⟨8,4⟩) sẽ dẫn đến mạng không thể học, vì ở các mức độ chính xác thấp như vậy, gradient sẽ biến mất rất sớm trong quá trình truyền ngược. Do đó, AdaPT theo dõi cho mỗi lớp một heuristic thứ hai trong j batch cuối cùng để xác định cần bao nhiêu độ chính xác để mạng tiếp tục học. Nếu gradient được lượng tử hóa, một heuristic dựa trên đa dạng gradient được sử dụng (3), (4).

χs(w)^l_j = (Σ^j_{k=j-r} ||∇f^l_k(w)||_2) / (||Σ^j_{k=j-r} ∇f^l_k(w)||_2) (3)

̃χs(w)^l_j = {log χs(w)^l_j if 0 < χs(w)^l_j < 1
              1 otherwise

Nếu ̃χs(w)^l_j > 0, hai đề xuất cho việc tăng độ chính xác được tính toán, s^l_1 = max(⌈1/log χs(w)^l_j - 1⌉, 1) và s^l_2 = max(min(32 - log_2 χs(w)^l_j - 1, 32) - FL^l_{min}, 1) và đề xuất cuối cùng được tính toán phụ thuộc vào chiến lược toàn cục st thông qua

s^l = {min(s^l_1, s^l_2) if st = min
       ⌈0.5(s^l_1 + s^l_2)⌉ if st = mean  (4)
       max(s^l_1, s^l_2) if st = max

Ngược lại, tức là ̃χs(w)^l_j ≤ 0, s^l = 1. Lượng tử hóa điểm cố định mới của lớp l sau đó được có được bằng FLl = min(FL^l_{min} + s^l, 32), WLl = min(max(WL^l_{min}, FL^l_{min}) + 1, 32)).

Xử lý phạm vi hạn chế của điểm cố định Như được nêu trong mục 2.1, các phép tính điểm cố định phải được đóng khung sao cho kết quả phù hợp trong các ranh giới nhất định. Chúng tôi tiếp cận điều này bằng cách thêm một số bit đệm buff vào độ dài từ của mỗi lớp, tức là ở cuối PushUp, FLl = min(FL^l_{min}, 32 - buff), WLl = max(min(FL^l_{min} + buff, 32), WL^l_{min}). Ngoài ra, chúng tôi chuẩn hóa gradient để hạn chế tăng trưởng trọng số và giảm khả năng trọng số trở thành không thể biểu diễn trong độ chính xác nhất định sau một bước cập nhật.

∇fl(w) = ∇fl(w) / ||∇fl(w)||_2

Chiến lược, Độ phân giải và Lookback Để thích ứng chiến lược st được đề cập trong (4), chúng tôi sử dụng một heuristic đơn giản dựa trên loss. Đầu tiên chúng tôi tính lookback trung bình trên tất cả các lớp lbavg = |L|^{-1} Σ^{|L|}_{i=0} lb^l và loss trung bình Lavg = |L|^{-1} Σ^{lbavg}_{i=0} Li trong lbavg batch cuối cùng. Sau đó thông qua (5), chiến lược được thích ứng.

st = {max if |Lavg| > |Li| and st = 'mean'
      mean if |Lavg| > |Li| and st = 'min'  (5)
      min if |Lavg| ≤ |Lj|}

Bởi vì số lượng gradient được thu thập cho mỗi lớp ảnh hưởng đến kết quả của heuristic dựa trên đa dạng gradient (3), (4), chúng tôi giới thiệu một tham số lookback lb^l bị giới hạn bởi các siêu tham số lb^l_{wr} ≤ lb^l ≤ lb^l_{upr} được ước tính tại thời gian chạy. Đầu tiên, lb^l_{new} được tính toán:

lb^l_{new} = {min(max(⌈lb^l_{upr}/χs(w)^l_j⌉, lb^l_{wr}), lb^l_{upr}) if 0 < χs(w)^l_j
              lb^l_{upr} otherwise

Sau đó, để ngăn jitter, một momentum đơn giản được áp dụng để có được lb^l cập nhật = ⌈αlb^l_{new} + (1-α)lb^l⌉ với α∈[0,1].

Tương tự, số lượng bin được sử dụng trong bước rời rạc hóa (1) ảnh hưởng đến kết quả của Divergence Kullback-Leibler rời rạc (2). Chúng tôi kiểm soát số lượng bin thông qua một tham số được gọi là độ phân giải rl, được suy ra tại thời gian chạy và bị giới hạn bởi các siêu tham số rl_{wr} ≤ rl ≤ r^l_{upr}.

rl = {min(max(rl + 1, rl_{wr}), r^l_{upr}) if lb^l = lb^l_{upr}
      min(max(rl - 1, rl_{wr}), r^l_{upr}) if lb^l = lb^l_{wr} (5)

3.4 AdaPT-SGD (ASGD)
Mặc dù AdaPT về nguyên tắc có thể được kết hợp với bất kỳ bộ tối ưu hóa dựa trên gradient lặp nào (ví dụ Adam), chúng tôi đã chọn triển khai AdaPT với Stochastic Gradient Descent (SGD), vì nó tổng quát hóa tốt hơn các thuật toán gradient thích ứng [65]. Việc triển khai AdaPT sử dụng cơ chế chuyển đổi độ chính xác và biểu diễn số được mô tả trong mục 3, bằng cách chia nó thành hai phép toán: PushDown Operation (alg. 3), tìm biểu diễn điểm cố định nhỏ nhất cho một lớp nhất định không gây mất thông tin, và PushUp Operation (alg. 4), tìm kiếm độ chính xác cần thiết cho một lớp nhất định để mạng tiếp tục học. Việc tích hợp hai phép toán này vào cơ chế chuyển đổi độ chính xác được mô tả trong alg. 2, và việc tích hợp vào quá trình huấn luyện SGD được mô tả trong alg. 1.

Cảm ứng thưa thớt Ngoài cơ chế chuyển đổi độ chính xác AdaPT, chúng tôi đã sử dụng một bộ điều chỉnh thưa hóa L1 [66, 67, 68] để có được tensor trọng số thưa và đặc biệt thân thiện với lượng tử hóa, kết hợp tuyến tính với điều chỉnh L2 để có độ chính xác tốt hơn theo cách tương tự như được đề xuất bởi [69]. Ngoài ra sử dụng một kỹ thuật tương tự như [5], chúng tôi phạt các bước học dẫn đến tăng độ dài từ hoặc giảm thưa thớt bằng:

P = WL^l/32 × sp^l

trong đó sp^l là tỷ lệ phần trăm phần tử khác không của lớp l. Do đó loss trong ASGD được tính bằng:

L̂(Wl) = L + λ₁||Wl||₁ + λ₂||Wl||₂² + P

Thuật toán Khi AdaPT-SGD (alg. 1) được khởi động trên một DNN float32 chưa được huấn luyện, nó đầu tiên khởi tạo trọng số các lớp DNN (được ký hiệu là L̂) với TNVS (alg. 1, dòng 1). Tiếp theo ánh xạ lượng tử hóa Q được khởi tạo, gán cho mỗi l∈L một tuple ⟨WLl, FLl⟩, một lookback lb^l và một độ phân giải rl (alg. 1, dòng 2). Sau đó một bản sao master float32 L của L̂ được tạo (alg. 1, dòng 3). Trong quá trình huấn luyện cho mỗi lần truyền thuận trên một batch, gradient lượng tử hóa Ĝ và loss L được tính với một lần truyền thuận sử dụng các lớp lượng tử hóa L̂ (alg. 1, dòng 4-6). Cơ chế chuyển đổi độ chính xác được mô tả trong mục 3 sau đó được gọi (alg. 1, dòng 7) và sau khi thích ứng chiến lược push up như được mô tả trong mục 3, nó lặp qua l∈L (alg. 2 dòng 2), đầu tiên thích ứng độ phân giải rl và lookback lb^l (alg. 2 dòng 4-5) và sau đó thực thi PushDown và PushUp trên lớp l (alg. 2 dòng 6-10) để cập nhật ⟨WLl, FLl⟩ ∈ Q. Khi PushDown được gọi trên l, nó giảm ánh xạ lượng tử hóa theo cách chia đôi cho đến khi KL chỉ ra một lượng tử hóa thô hơn sẽ gây mất thông tin ở độ phân giải rl (3, dòng 1-5). Sau khi tính toán lượng tử hóa thô nhất ⟨WL^l_{min}, FL^l_{min}⟩ ∈ Q không gây mất thông tin trong l, PushUp được gọi trên l để tăng lượng tử hóa đến điểm mà mạng dự kiến sẽ tiếp tục học, dựa trên đa dạng gradient của lb^l batch cuối cùng (alg. 4, dòng 1-6). Sau PushUp, PrecisionSwitch trả về Q cập nhật cho vòng lặp huấn luyện, và một lần truyền ngược SGD thông thường cập nhật bản sao master float32 L, sử dụng gradient lượng tử hóa Ĝ (alg. 1, dòng 7,8). Cuối cùng, trọng số L hiện đã cập nhật được lượng tử hóa sử dụng Q cập nhật và được ghi lại vào L̂ để sử dụng trong lần truyền thuận tiếp theo (alg. 1, dòng 9-11).

4 Đánh giá thực nghiệm

4.1 Thiết lập
Để đánh giá thực nghiệm AdaPT, chúng tôi đã huấn luyện AlexNet và ResNet20 trên các bộ dữ liệu CIFAR-10/100 với lập lịch tốc độ học giảm trên plateau (ROP) sẽ giảm tốc độ học theo một hệ số nhất định nếu loss không giảm trong một số epoch nhất định [46]. Do không có sẵn phần cứng điểm cố định, chúng tôi đã sử dụng QPyTorch để mô phỏng phép toán điểm cố định và mô hình hiệu suất riêng của chúng tôi (mục 4.1.2) để mô phỏng tăng tốc, giảm kích thước mô hình và tiêu thụ bộ nhớ. Các thí nghiệm được thực hiện trên Nvidia DGX-1. Nó có 8 x Tesla V100 GPU, có khả năng thực hiện phép toán số nguyên và điểm nổi [70].

4.1.1 Siêu tham số
Tất cả các lớp l∈L được lượng tử hóa với ⟨WLl, FLl⟩ = ⟨8,4⟩ ở đầu huấn luyện AdaPT. Các siêu tham số khác cụ thể cho AdaPT được đặt thành rl_{wr} = 50, r^l_{upr} = 150, lb^l_{wr} = 25, lb^l_{upr} = 100, momentum lookback α = 0.33 cho tất cả các thí nghiệm, bit đệm được đặt thành buff = 4 để huấn luyện AlexNet trên CIFAR10 và buff = 8 để huấn luyện ResNet và AlexNet trên CIFAR100. Các siêu tham số không cụ thể cho AdaPT (lr, L1 decay, L2 decay, ROP patience, ROP threshold, batch size, accumulation steps) được chọn bằng tìm kiếm lưới và xác thực chéo 10-fold và chúng tôi tham khảo kho mã của chúng tôi¹ cho các tệp cấu hình chính xác của mỗi thí nghiệm.

4.1.2 Mô hình hiệu suất
Tăng tốc và giảm kích thước mô hình được tính toán bằng một mô hình hiệu suất tính đến các lần truyền thuận và ngược, cũng như kích thước batch, tích lũy gradient, độ chính xác số và thưa thớt. Mô hình hiệu suất của chúng tôi tính toán các phép toán theo lớp (MAdds, sau đó được gọi là ops), cân nhất chúng với độ dài word của lớp và tỷ lệ phần trăm phần tử khác không của tensor tại một giai đoạn cụ thể trong huấn luyện để mô phỏng lượng tử hóa và định dạng tensor thưa, và tổng hợp chúng để có được chi phí tính toán tổng thể phát sinh của tất cả các lần truyền thuận và ngược. Ngoài ra, mô hình hiệu suất ước tính chi phí của AdaPT cho mỗi lớp l push up operation pu^l và push down operation pd^l bằng:

ops^l_{pd,i} ≈ 2log_2(32-8) × r^l_i × 3 × ∏_{dim∈l} dim (6)

ops^l_{pu,i} ≈ (lb^l_i + 1) × ∏_{dim∈l} dim + 1 (7)

Sử dụng (6), (7) và giả định đơn giản hóa rằng một lần truyền ngược phát sinh nhiều phép toán như một lần truyền thuận lượng tử hóa nhưng được thực hiện ở độ chính xác đầy đủ tức là độ dài word 32 bit với gradient không thưa, chi phí huấn luyện AdaPT sau đó được giới hạn bởi:

costs_{train} ≈ ∑^n_{i=1} ∑^{|L|}_{l=1} ops^l_i × sp^l_i × WL^l_i + 32/accs (8)

và chi phí của AdaPT được giới hạn bởi:

costs_{AdaPT} ≈ ∑^n_{i=1} ∑^{|L|}_{l=1} 32 × sp^l_i × (ops^l_{pd,i} + ops^l_{pu,i})/accs × lb^l_i (9)

trong đó n là số bước huấn luyện, L là các lớp của mạng và WL^l_i là độ dài word của lớp l thứ i tại bước huấn luyện i và sp^l_i là tỷ lệ phần trăm phần tử khác không của lớp l tại bước i. Sử dụng (8), (9), chúng tôi có được tổng chi phí thông qua costs = cost_{train} + costs_{AdaPT} và tiếp tục tăng tốc của phương pháp huấn luyện của chúng tôi so với MuPPET hoặc baseline float32 kết hợp kích thước batch bs, bước tích lũy gradient accs và chi phí tính toán costs được có được thông qua:

SU = (bs_{other} × costs_{other})/(bs_{ours} × costs_{ours})

trong đó tự nhiên chúng tôi loại trừ chi phí của AdaPT khi tính toán costs_{other}. Giảm kích thước mô hình SZ được tính toán bằng cách đầu tiên tính toán kích thước mô hình riêng lẻ sz:

sz = ∑^{|L|}_{l=1} sp^l_i × WL^l_i

với i = n và sau đó tạo thành thương số SZ = sz_{other}/sz_{ours}. Tương tự, tiêu thụ bộ nhớ trung bình trong quá trình huấn luyện MEM = mem_{other}/mem_{ours} được tạo thành bởi thương số tiêu thụ bộ nhớ mem của mỗi mô hình:

mem = (∑^n_{i=1} ∑^{|L|}_{l=1} sp^l_i × WL^l_i + 32) × (1/n)

Bởi vì sz và mem bỏ qua kích thước tensor, chúng không thể được diễn giải như các giá trị tuyệt đối. Tuy nhiên cho rằng trong các thương số MEM và SZ các hiệu ứng của kích thước tensor sẽ triệt tiêu khi so sánh các kiến trúc giống hệt nhau (tức là AlexNet float32 và AlexNet lượng tử hóa), những giá trị này cung cấp các biện pháp tương đối hợp lệ cho tiêu thụ bộ nhớ và kích thước mô hình dưới ràng buộc này.

4.2 Kết quả

4.2.1 Huấn luyện
Bảng 1 cho thấy độ chính xác xác nhận top-1 và sự khác biệt độ chính xác đạt được bởi AdaPT và MuPPET cho cả huấn luyện lượng tử hóa trên CIFAR100, và cơ sở float32. Bảng 2 cho thấy kết quả cho huấn luyện trên CIFAR10. Như có thể thấy trong các bảng, AdaPT có khả năng huấn luyện lượng tử hóa không chỉ đến độ chính xác có thể so sánh với huấn luyện float32 mà thậm chí đạt hoặc vượt qua iso-accuracy trong tất cả các trường hợp được xem xét. Các thí nghiệm được chọn hơn nữa minh họa rằng AdaPT cung cấp hiệu suất này độc lập với kiến trúc DNN cơ bản hoặc bộ dữ liệu được sử dụng. Hình 3 và 4 hiển thị việc sử dụng độ dài word theo thời gian cho các lớp riêng lẻ cho ResNet20 và AlexNet trên CIFAR100. Trong cả hai trường hợp, chúng tôi quan sát rằng các lớp riêng lẻ có các sở thích độ chính xác khác nhau phụ thuộc vào tiến trình của quá trình huấn luyện, một hiệu ứng đặc biệt rõ ràng trong AlexNet. Đối với ResNet20, độ dài word thú vị là giảm đáng kể ở giữa quá trình huấn luyện và sau đó giảm, mà chúng tôi đoán là do điều chỉnh thưa hóa L1 cũng như hình phạt wordlength/sparsity được giới thiệu trong mục 3 dẫn đến giảm trọng số không liên quan, mà nếu không sẽ bù đắp heuristic KL được sử dụng trong PushDown Operation. Bảng 4 và 3 cho thấy tăng tốc ước tính của AdaPT so với baseline float32 của chúng tôi (100 epoch, cùng số bước tích lũy gradient và kích thước batch như các mô hình được huấn luyện AdaPT) và baseline float32 của MuPPET. AdaPT đạt được tăng tốc có thể so sánh với các giải pháp SOTA trên baseline của chúng tôi và vượt trội hơn MuPPET trên baseline của MuPPET trong mọi tình huống. Thật không may, cơ sở mã của MuPPET không thể được thực thi nên chúng tôi không thể áp dụng MuPPET cho baseline AdaPT hiệu quả hơn và bị giới hạn so sánh với kết quả được cung cấp bởi các tác giả gốc. Chúng tôi đã sử dụng mô hình hiệu suất của chúng tôi để mô phỏng hiệu suất của MuPPET dựa trên các chuyển đổi độ chính xác được nêu trong bài báo MuPPET bởi vì các tác giả MuPPET không xuất bản mô hình hiệu suất của họ. Một tổng quan về việc giảm chi phí tính toán thông qua ASGD so với baseline float32 được cung cấp bởi hình 8 cũng như Hình 5 và hình 6 minh họa việc cảm ứng thưa thớt trong quá trình huấn luyện AdaPT. Thú vị là trong một số trường hợp, chúng tôi quan sát mức độ thưa thớt tăng khi quá trình huấn luyện AdaPT tiến triển, với một số lớp đạt 80% thưa thớt hoặc hơn ở cuối huấn luyện như trong trường hợp AlexNet được huấn luyện trên CIFAR100, trong khi ít thưa thớt hơn có thể được cảm ứng trong ít lớp hơn khi huấn luyện ResNet trên CIFAR10/100. Như có thể thấy trong bảng 5, AdaPT cảm ứng thưa thớt nhiều nhất trong AlexNet, với 45% thưa thớt mô hình cuối cùng và 34% thưa thớt intra-training trung bình cho huấn luyện trên CIFAR100. Trong kiến trúc residual ResNet, chúng tôi quan sát rằng AdaPT giới thiệu 7% thưa thớt trong mô hình cuối cùng và 3% thưa thớt intra-training. Như hình 6 minh họa thưa thớt tăng về cuối huấn luyện, chúng tôi đoán rằng đối với các kiến trúc giống ResNet, hiệu ứng cảm ứng thưa thớt có thể rõ ràng hơn nếu huấn luyện được thực hiện trong số epoch cao hơn.

Một tác dụng phụ được minh họa bởi hình 7 và bảng 4 và bảng 2 của AdaPT để giảm chi phí tính toán của huấn luyện mạng (hình 8) là tiêu thụ bộ nhớ intra-training tăng của nó được gây ra bởi AdaPT duy trì một bản sao master float32 hoàn chỉnh của tensor trọng số mô hình được sử dụng cho cập nhật truyền ngược. Tuy nhiên hiệu ứng này chỉ có mặt trong quá trình huấn luyện vì các bản sao master bị loại bỏ một khi mô hình được triển khai và sử dụng cho suy luận.

4.2.2 Suy luận
Cho rằng các mạng được huấn luyện AdaPT được lượng tử hóa và thưa thớt hoàn toàn, chúng có lợi thế mở rộng vượt ra ngoài giai đoạn huấn luyện vào giai đoạn suy luận. Bảng 6 cho thấy rằng tăng tốc suy luận cho các mạng được huấn luyện AdaPT dao động từ 1,63 đến 3,56. Tăng tốc đạt được trong giai đoạn suy luận thậm chí còn cao hơn những tăng tốc đạt được trong quá trình huấn luyện bởi vì trong quá trình suy luận, không có lần truyền ngược đắt đỏ nào phải được thực hiện và không có chi phí được gây ra bởi AdaPT xảy ra. Đây là một lợi thế không tầm thường so với MuPPET không cung cấp bất kỳ lợi thế sau huấn luyện nào về tăng tốc hoặc tiêu thụ bộ nhớ do mạng kết quả là float32.

5 Kết luận
Với AdaPT chúng tôi giới thiệu một thuật toán huấn luyện thưa hóa DNN lượng tử hóa mới có thể được kết hợp với các sơ đồ huấn luyện dựa trên gradient lặp. AdaPT khai thác các phép toán điểm cố định nhanh cho các lần truyền thuận, trong khi thực hiện các lần truyền ngược ở độ chính xác cao float32. Bằng cách cân bằng cẩn thận nhu cầu giảm thiểu độ chính xác điểm cố định để đạt tăng tốc tối đa và yêu cầu bộ nhớ tối thiểu, trong khi đồng thời giữ độ chính xác đủ cao để mạng tiếp tục học, phương pháp của chúng tôi tạo ra độ chính xác top-1 có thể so sánh hoặc tốt hơn các kỹ thuật float32 SOTA hoặc các thuật toán huấn luyện lượng tử hóa khác. Hơn nữa AdaPT có lợi thế phương pháp luận nội tại không chỉ huấn luyện mạng theo cách lượng tử hóa, mà chính mạng được huấn luyện cũng được lượng tử hóa và thưa thớt sao cho nó có thể được triển khai lên phần cứng ASIC nhanh với chi phí bộ nhớ giảm và được thực thi với chi phí tính toán giảm so với mô hình độ chính xác đầy đủ. Ngoài ra chúng tôi đóng góp một mô hình hiệu suất cho huấn luyện lượng tử hóa điểm cố định.

6 Công việc tương lai
AdaPT như được trình bày sử dụng biểu diễn điểm cố định để cho phép chuyển đổi độ chính xác chi tiết. Tuy nhiên, phần cứng điểm cố định không phổ biến như phần cứng điểm nổi, vì vậy chúng tôi dự định mở rộng khái niệm sang lượng tử hóa điểm nổi sao cho AdaPT trở thành tương thích với phần cứng tiêu dùng float16/float32. Hơn nữa, chúng tôi dự định khám phá liệu AdaPT có thể được sử dụng để tạo ra các mạng độ rộng bit rất thấp (nhị phân hóa/tam phân hóa) thông qua việc giảm dần không gian tìm kiếm lượng tử hóa trong quá trình huấn luyện. Ngoài ra, chúng tôi đoán các heuristic được sử dụng bởi AdaPT có thể được sử dụng cho tỉa DNN intra-training, điều này sẽ là chủ đề của nghiên cứu tương lai. Chúng tôi lên kế hoạch thử nghiệm ablation để giảm độ phức tạp của AdaPT. Một ứng dụng thú vị khác của AdaPT mà chúng tôi dự định khám phá là tính hữu ích của nó trong các lĩnh vực nghiên cứu khác như Khám phá Thuốc.

7 Lời cảm ơn
Chúng tôi cảm ơn Giáo sư Torsten Möller đã tài trợ cho Cụm GPU DGX-1 được sử dụng trong các thí nghiệm của chúng tôi và chúng tôi cảm ơn Giáo sư Sebastian Tschiatschek về phản hồi và thảo luận hữu ích. Cả hai đều là nhân viên của Khoa Khoa học Máy tính của Đại học Vienna tại thời điểm viết.
