# 2308.15987.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2308.15987.pdf
# Kích thước tệp: 661833 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
FPTQ: LƯỢNG TỬ HÓA HẬU HUẤN LUYỆN TINH CHỈNH CHO CÁC MÔ HÌNH NGÔN NGỮ LỚN

Qingyuan Li†1, Yifan Zhang†∗1,2, Liang Li1, Peng Yao1, Bo Zhang1, Xiangxiang Chu1, Yerui Sun1,
Li Du2, và Yuchen Xie1
1Meituan
2Đại học Nanjing

TÓM TẮT
Trong kỷ nguyên của các mô hình ngôn ngữ quy mô lớn, kích thước tham số đáng kể đặt ra những thách thức nghiêm trọng cho việc triển khai. Là một kỹ thuật nén phổ biến, lượng tử hóa đã nổi lên như là thực hành chính để giải quyết vấn đề này, chủ yếu tập trung vào hai công thức W8A8 và W4A16 (tức là trọng số và kích hoạt trong các độ rộng bit như vậy). Trong nghiên cứu này, chúng tôi đề xuất một phương pháp lượng tử hóa hậu huấn luyện W4A8 mới cho các LLM mã nguồn mở có sẵn, kết hợp ưu điểm của cả hai công thức. Do đó, chúng tôi có thể tận dụng lợi ích trong việc sử dụng I/O của lượng tử hóa trọng số 4-bit và tăng tốc do tính toán ma trận 8-bit. Tuy nhiên, W4A8 đối mặt với sự suy giảm hiệu suất nghiêm trọng. Như một biện pháp khắc phục, chúng tôi áp dụng các chiến lược lượng tử hóa kích hoạt theo lớp có tính năng cân bằng logarithmic mới cho các lớp khó xử lý nhất, và chúng tôi kết hợp chúng với lượng tử hóa trọng số tinh chỉnh. Không cần những điều phức tạp, chúng tôi loại bỏ sự cần thiết cho việc tinh chỉnh thêm và đạt được hiệu suất lượng tử hóa W4A8 tối tân trên BLOOM, LLaMA, và LLaMA-2 trên các điểm chuẩn tiêu chuẩn. Chúng tôi xác nhận rằng lượng tử hóa W4A8 có thể đạt được cho việc triển khai các mô hình ngôn ngữ lớn, thúc đẩy các ứng dụng thực tế rộng rãi của chúng.

1 GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) được phân biệt bởi khả năng tri thức nổi bật đặc biệt của chúng (Wei et al., 2022), cho phép chúng thực hiện xuất sắc trên nhiều loại nhiệm vụ ngôn ngữ. Tuy nhiên, quy mô khổng lồ của chúng đặt ra một rào cản đáng kể cho việc triển khai do yêu cầu lưu trữ đáng kể và lượng tính toán khổng lồ. Thách thức này đặc biệt rõ rệt trong các môi trường có tài nguyên hạn chế như thiết bị tính toán biên và thiết bị cá nhân, nơi các ràng buộc có thể cản trở việc áp dụng rộng rãi những mô hình ngôn ngữ tiên tiến này.

Để giải quyết vấn đề này, một số chiến lược nén mô hình đã được đề xuất, bao gồm cắt tỉa (Ma et al., 2023; Frantar & Alistarh, 2023; Sun et al., 2023), chưng cất (Zhang et al., 2023), lượng tử hóa (Frantar et al., 2022; Xiao et al., 2023), và phân tách thứ hạng thấp (Yao et al., 2023). Mỗi phương pháp này đều có những hạn chế riêng. Ví dụ, cắt tỉa có thể đạt được tỷ lệ nén hợp lý nhưng có thể yêu cầu tinh chỉnh đáng kể hoặc gắn chặt với các kiến trúc phần cứng cụ thể. Ngược lại, các kỹ thuật lượng tử hóa, mặc dù có khả năng áp dụng phổ quát, thường đối mặt với vấn đề lỗi lượng tử hóa đáng kể, đặc biệt với kích thước tham số ngày càng tăng (Dettmers et al., 2022).

Gần đây, sự chú ý nghiên cứu đã chuyển hướng sang một phương pháp cân bằng hơn cho lượng tử hóa, cụ thể là việc sử dụng độ rộng bit thấp hơn cho trọng số và độ rộng bit cao hơn cho kích hoạt, như W4A16 trong GPTQ (Frantar et al., 2022). Điều này giới thiệu một góc nhìn mới để giải quyết các khía cạnh tính toán và bộ nhớ chuyên sâu của LLM, thường bao gồm các cấu trúc Transformer Decoder (Vaswani et al., 2017). Trong quá trình suy luận, nó có thể được chia thành giai đoạn giải mã ngữ cảnh chuyên sâu tính toán và giai đoạn tự giải mã chuyên sâu bộ nhớ, mỗi giai đoạn đều trình bày những thách thức và cơ hội độc đáo để tối ưu hóa thêm.

∗Công việc được thực hiện như một thực tập sinh tại Meituan.†Đóng góp Bằng nhau.
1arXiv:2308.15987v1  [cs.CL]  30 Aug 2023

--- TRANG 2 ---
Tuy nhiên, có một sự thiếu hụt rõ rệt trong nghiên cứu khám phá sự kết hợp hiệp đồng của hai công thức lượng tử hóa W8A8 và W4A16. Bài báo này nhằm thu hẹp khoảng cách bằng cách đề xuất một phương pháp Lượng tử hóa Hậu Huấn luyện Tinh chỉnh (được gọi là FPTQ) sáng tạo kết hợp lợi ích của cả hai, do đó cung cấp một giải pháp W4A8 hiệu quả và có hiệu lực cho việc triển khai nhiều mô hình ngôn ngữ lớn có sẵn được thử nghiệm trên vô số nhiệm vụ ngôn ngữ tự nhiên.

0 10 20 30 40 50 60 70 80
Phân phối kích hoạt trước LAE101102103104105106107Tần số
0 10 20 30 40 50 60 70 80
Phân phối kích hoạt sau LAE101102103104105106107108Tần số

Hình 1: Phân phối kích hoạt trước và sau cân bằng logarithmic trên BLOOM-7B1.

Chúng tôi đầu tiên điều tra độ khó lượng tử hóa bằng cách minh họa các phân phối kích hoạt trong các lớp khác nhau, phát hiện rằng phạm vi của chúng khác nhau đáng kể điều này thúc đẩy chúng tôi cho một chiến lược theo lớp. Tiếp theo, chúng tôi cung cấp một kỹ thuật cân bằng kích hoạt độc đáo để xử lý các outlier khó xử lý (Hình 1), và cải thiện hiệu suất tổng thể với lượng tử hóa trọng số tinh chỉnh.

Tóm lại, chúng tôi đóng góp một số điểm quan trọng cho lĩnh vực nén và triển khai LLM:

1.Nén W4A8 hiệu suất cao và chi phí thấp: Chúng tôi là những người đầu tiên đạt được nén PTQ W4A8 (trọng số INT4 và kích hoạt INT8) hiệu suất cao cho các mô hình ngôn ngữ lớn, duy trì độ chính xác của mô hình gốc. Là một kỹ thuật lượng tử hóa hậu huấn luyện, nó đơn giản hóa đáng kể quy trình sản xuất của LLM.

2.Sơ đồ lượng tử hóa mới: Dựa trên phân tích toàn diện của chúng tôi về phân phối kích hoạt của LLM, chúng tôi sử dụng một chiến lược theo lớp để đối phó với các mức độ khó lượng tử hóa khác nhau. Đặc biệt, chúng tôi phát minh ra một cân bằng kích hoạt logarithmic ngoại tuyến để tạo ra một phân phối thân thiện với lượng tử hóa cho các lớp khó xử lý trước đây.

3.Thân thiện với suy luận: Phương pháp của chúng tôi hài hòa hiệu quả bộ nhớ và tính toán cho phép lưu trữ trọng số ở định dạng 4-bit trong khi thực hiện suy luận INT8, do đó thúc đẩy cả việc truy cập bộ nhớ và tính toán.

2 CÔNG VIỆC LIÊN QUAN

2.1 CÁC MÔ HÌNH NGÔN NGỮ LỚN
Vài năm qua đã chứng kiến sự bùng nổ của các mô hình ngôn ngữ được huấn luyện trước. BERT (Devlin et al., 2019) được thiết kế để hiểu ngữ cảnh của các từ trong câu và đã được sử dụng cho các nhiệm vụ như phân tích cảm xúc và trả lời câu hỏi. RoBERTa (Liu et al., 2019) là một phiên bản cải tiến của BERT với kỹ thuật huấn luyện trước tốt hơn và dữ liệu huấn luyện lớn hơn. T5 (Raffel et al., 2020) được thiết kế để thực hiện một loạt rộng các nhiệm vụ xử lý ngôn ngữ tự nhiên, bao gồm dịch ngôn ngữ và tóm tắt. XLNet (Yang et al., 2019) được thiết kế để xử lý các chuỗi văn bản dài và đã đạt được kết quả tối tân trên một số nhiệm vụ xử lý ngôn ngữ tự nhiên. GPT-3 (Brown et al., 2020) là một trong những LLM tiên tiến nhất với 175 tỷ tham số, có khả năng thực hiện một loạt rộng các nhiệm vụ xử lý ngôn ngữ tự nhiên. Cùng với những mô hình mã nguồn mở như GLM (Du et al., 2021), BLOOM (Laurençon et al., 2022), OPT (Zhang et al., 2022) và dòng LLaMa (Touvron et al., 2023), LLM đã cách mạng hóa đáng kể lĩnh vực xử lý ngôn ngữ tự nhiên và đang được sử dụng trong một loạt rộng các ứng dụng.

2

--- TRANG 3 ---
Tuy nhiên, LLM có hàng tỷ tham số và thường được huấn luyện trước trên lượng lớn dữ liệu văn bản, điều này yêu cầu tài nguyên tính toán đáng kể để huấn luyện và triển khai. Có một lời kêu gọi cho thời gian suy luận nhanh hơn và yêu cầu bộ nhớ thấp hơn để làm cho LLM thực tế hơn.

2.2 LƯỢNG TỬ HÓA TRÊN LLM
Việc áp dụng lượng tử hóa cho các mô hình ngôn ngữ lớn đặt ra những thách thức độc đáo. Các sơ đồ PTQ truyền thống đã đạt được thành công lớn trong Mạng Nơ-ron Tích chập (CNN) (Nagel et al., 2019; Wu et al., 2020; Nagel et al., 2021; Yao et al., 2021), nhưng việc áp dụng trực tiếp cho các mô hình ngôn ngữ lớn thường dẫn đến mất mát độ chính xác nghiêm trọng, điều này thường do sự hiện diện của nhiều outlier trong các giá trị kích hoạt của các mô hình lớn (Dettmers et al., 2022).

Một số phương pháp đã được đề xuất để giải quyết những vấn đề này. Ví dụ, LLM.int8() (Dettmers et al., 2022) chia các giá trị kích hoạt đầu vào thành hai phần: các chiều không phải outlier được tính toán với INT8, và outlier được tính toán với FP16. GPTQ (Frantar et al., 2022) và AWQ (Lin et al., 2023) tránh khó khăn này bằng cách áp dụng kích hoạt FP16 và lượng tử hóa chỉ trọng số INT4. Tuy nhiên, những phương pháp này cũng có hạn chế riêng, như chi phí tính toán và không thể thực sự tận dụng tăng tốc phần cứng.

Các phương pháp khác như SmoothQuant (Xiao et al., 2023), RPTQ (Yuan et al., 2023), và ZeroQuant-V2 (Yao et al., 2023) đề xuất các chiến lược khác nhau để đạt được lượng tử hóa trong khi giảm thiểu mất mát độ chính xác và chi phí tính toán. Tuy nhiên, SmoothQuant chỉ là một giải pháp W8A8 và nó gặp phải hiệu suất kém trên W4A8. Phần còn lại giải quyết thách thức W4A8 nhưng chúng có bộ thách thức riêng như sắp xếp lại trọng số, lượng tử hóa bất đối xứng, và kích hoạt theo nhóm, có thể làm phức tạp công việc kỹ thuật và có thể không hỗ trợ tốt cho phần cứng. Dưới ánh sáng của những vấn đề này, chúng tôi được thúc đẩy để đạt được lượng tử hóa W4A8 mà không dựa vào QAT hoặc các phương pháp chưng cất, mở đường cho việc triển khai hiệu quả LLM.

3 PHƯƠNG PHÁP

3.1 TẠI SAO W4A8?
Suy luận sinh của LLM có thể được chia thành hai giai đoạn: giải mã ngữ cảnh tạo ra một token đầu ra cho một lời nhắc đầu vào (được nhúng như một chuỗi token), và tự giải mã dự đoán lặp đi lặp lại token tiếp theo trong một chuỗi, xem Hình 2 (a). Giai đoạn trước bị ràng buộc tính toán do tính toán vòng đầu của các chuỗi đầu vào dài và giai đoạn sau bị ràng buộc bộ nhớ do xử lý tuần tự, do đó hai triển khai khác nhau được yêu cầu.

LLMGiải mã Ngữ cảnh<out1><t1><t2>…<tn>Tự Giải mãRàng buộc tính toánRàng buộc bộ nhớLLM<out1><out2>…<out2>(a)Giải mã Ngữ cảnhTự Giải mãGiải mã Ngữ cảnhTự Giải mãGiải mã Ngữ cảnh Tự Giải mãW8A8W4A16W4A8Chi phí thời gian(b)

Hình 2: (a)Hai giai đoạn suy luận LLM trong đó giải mã ngữ cảnh bị ràng buộc tính toán và tự giải mã bị ràng buộc bộ nhớ. (b)W4A8 tăng tốc cả hai giai đoạn và nhanh hơn hai giai đoạn kia.

Bảng 1: So sánh hiệu quả giai đoạn giải mã cho các phương pháp lượng tử hóa khác nhau. CD: Giải mã Ngữ cảnh, SD: Tự Giải mã

Phương pháp CD Hiệu quả SD Hiệu quả
ZeroQuant Không Không
SmoothQuant Có Không
GPTQ Không Có
AWQ Không Có
Của chúng tôi Có Có

Các phương pháp lượng tử hóa trước đây như Smoothquant (Xiao et al., 2023) có tính năng W8A8, trong khi AWQ (Lin et al., 2023) và GPTQ (Frantar et al., 2022) sử dụng W4A16. Cả hai công thức đều thỏa hiệp một giai đoạn cho giai đoạn khác, dẫn đến hiệu suất tổng thể kém hơn, trong khi chỉ W4A8 có thể tăng tốc cả hai giai đoạn, xem Hình 2 (b) và Bảng 1. Đó là, giải mã ngữ cảnh

3

--- TRANG 4 ---
hưởng lợi từ tăng tốc sử dụng phép nhân ma trận 8-bit, trong khi tự giải mã cũng được tăng tốc thông qua giảm truy cập bộ nhớ sử dụng trọng số 4-bit.

Có một vài nghiên cứu W4A8 hiện tại. ZeroQuant (Yao et al., 2022) sử dụng độ chính xác hỗn hợp cho trọng số self-attention (W8) và không được thử nghiệm trên các mô hình lớn hơn, ZeroQuantV2 (Yao et al., 2023) sử dụng lượng tử hóa kích hoạt tinh chỉnh không khả thi trong thực tế. ZeroQuant-FP (Wu et al., 2023) giảm bớt sự suy giảm bằng cách sử dụng tính toán FP8 độ chính xác cao hơn nhưng nó phụ thuộc vào phần cứng cụ thể (ví dụ NVIDIA H100). LLM-QAT (Liu et al., 2023a) áp dụng QAT để cải thiện hiệu suất W4A8 nhưng nó yêu cầu huấn luyện tốn kém và dễ bị điều chỉnh siêu tham số tẻ nhạt. Do đó, cần thiết phải cải thiện độ chính xác của mô hình W4A8 trong khi không làm tổn hại tốc độ suy luận của nó. Phương pháp cũng phải được làm cho chi phí thấp và có thể tổng quát hóa cho hầu hết các LLM cập nhật.

3.2 PHÂN TÍCH PHÂN PHỐI KÍCH HOẠT TRÊN LLM
Với mục tiêu của chúng tôi trong tâm trí, chúng tôi được thúc đẩy để thiết kế một phương pháp PTQ mạnh mẽ. Để bắt đầu, chúng tôi nghiên cứu tại sao lượng tử hóa W4A8 vanilla khó khăn cho các LLM hiện tại. Chúng tôi đầu tiên vẽ phân phối kích hoạt của LLaMA-7B trong Hình 3 để tìm các hành vi khác biệt của các lớp khác nhau. Ví dụ, oproj có phân phối compact trong khi down proj trải rộng. Hiện tượng này tái diễn trong nhiều LLM khác, xem Phụ lục A.3.

0 10 20 30
Lớp101
100101Giá trị kích hoạtgiá trị kích hoạt o_proj
0 10 20 30
Lớp101
100101102103Giá trị kích hoạtgiá trị kích hoạt down_proj

Hình 3: Trực quan hóa phân phối kích hoạt của oproj và down proj trên LLaMA-7B.

Như chúng ta có thể thấy từ phân tích trên, phạm vi dao động tối đa của các giá trị kích hoạt đầu vào cho một số lớp nhất định dao động từ hàng chục đến hàng nghìn. Sử dụng lượng tử hóa tĩnh per-tensor sẽ dẫn đến lỗi lượng tử hóa đáng kể, nhưng sử dụng lượng tử hóa động per-token cho tất cả các lớp sẽ không mang lại tăng tốc phần cứng đầy đủ. Do đó, nó tự nhiên kêu gọi một chính sách cụ thể theo lớp để xác định độ chi tiết của lượng tử hóa.

3.3 FPTQ: LƯỢNG TỬ HÓA HẬU HUẤN LUYỆN TINH CHỈNH
Được thúc đẩy bởi phân tích trên, chúng tôi đề xuất phương pháp lượng tử hóa hậu huấn luyện của chúng tôi sử dụng chiến lược lượng tử hóa theo lớp liên quan đến các phân phối kích hoạt khác biệt. Quy trình hoàn chỉnh của chúng tôi được đưa ra trong Thuật toán 1. Các thành phần chính được thảo luận chi tiết.

3.3.1 CHIẾN LƯỢC LƯỢNG TỬ HÓA KÍCH HOẠT THEO LỚP
Chìa khóa để giải quyết khó khăn lượng tử hóa kích hoạt nằm trong việc xử lý outlier. Theo kinh nghiệm, chúng ta có thể sử dụng các chiến lược lượng tử hóa kích hoạt khác nhau cho các lớp khác nhau, như được hiển thị trong Bảng 2.

Đối với phạm vi giá trị kích hoạt trong hàng chục (được ký hiệu là v0), lượng tử hóa tĩnh per-tensor có thể được sử dụng an toàn. Tuy nhiên, để tránh mất mát lượng tử hóa cho phạm vi kích hoạt trên hàng trăm (được ký hiệu là v1), lượng tử hóa động per-token sẽ được đặt vào vị trí mặc dù hơi hy sinh lợi ích tăng tốc phần cứng. Đối với hầu hết các lớp có phạm vi trong hàng trăm, tức là (v0, v1), nó đòi hỏi một chiến lược đặc biệt đồng thời giảm lỗi lượng tử hóa trong khi không làm tổn hại tốc độ suy luận.

Xiao et al. (2023) phát hiện rằng khi các outlier lớn hơn thống trị phân phối, các bit lượng tử hóa hiệu quả của inlier bị thu hẹp đáng kể. Đối với lượng tử hóa per-tensor 8-bit, nó trở thành 28·mi/m trong đó mi là biên độ tối đa của kênh i và m là giá trị tối đa của toàn bộ tensor. Họ cũng quan sát thấy rằng outlier ở lại trong các kênh cố định. Dựa trên hai phát hiện này, chúng ta được phép thực hiện triệt tiêu outlier per-channel trên kích hoạt. SmoothQuant (Xiao et al., 2023) cố gắng 'làm mịn' phân phối per-channel bằng cách chia kích hoạt với một thang đo

4

--- TRANG 5 ---
Thuật toán 1 FPTQ: Lượng tử hóa Hậu Huấn luyện Tinh chỉnh
Đầu vào: Một LLM được huấn luyện trước
Đầu ra: Một LLM được lượng tử hóa
1:Cân chỉnh LLM được huấn luyện trước với tập dữ liệu được xác định trước
2:Thực hiện phân tích phân phối kích hoạt
3:for mỗi lớp- l trong cấu trúc Transformer ( L lớp tổng cộng) do
4: if Phạm vi kích hoạt v≤v0 then
5: Đặt chính sách lượng tử hóa kích hoạt của lớp l thành tĩnh per-tensor
6: else if Phạm vi kích hoạt v0< v < v1 then
7: Thực hiện cân bằng kích hoạt logarithmic
8: Đặt chính sách lượng tử hóa kích hoạt của lớp l thành tĩnh per-tensor
9: else
10: Đặt chính sách lượng tử hóa kích hoạt của lớp l thành động per-token
11: end if
12: Đặt chính sách lượng tử hóa trọng số của mỗi lớp như tinh chỉnh
13:end for
14:Cập nhật trọng số và kích hoạt của LLM w.r.t. chính sách lượng tử hóa được chọn
15:Nhận LLM được lượng tử hóa hiệu suất cao

Bảng 2: Chiến lược lượng tử hóa kích hoạt cho các phạm vi khác nhau của giá trị kích hoạt.
Phạm vi Giá trị Kích hoạt Chiến lược Lượng tử hóa Hiệu quả Phần cứng Hoạt động Điển hình
v≤v0 per-tensor, tĩnh Cao Dense
v0< v < v1 LAE + per-tensor, tĩnh Cao QKV , FC1
v≥v1 per-token, động Trung bình FC2

si= max( |xi|)/max(|wi|), trong đó xi và wi là kích hoạt và trọng số của kênh i tương ứng.
AWQ (Lin et al., 2023) giới thiệu các siêu tham số được tìm kiếm lưới α và β để đặt tầm quan trọng cho kích hoạt và trọng số riêng biệt, nơi họ thấy rằng đóng góp của trọng số là nhỏ và đề xuất nhận thức kích hoạt là quan trọng nhất. Trong bài báo này, chúng tôi lập luận rằng không cần thiết phải xem xét trọng số để tính toán thang đo "làm mịn" kích hoạt. Bên cạnh đó, điều quan trọng là giữ lại tất cả các giá trị kích hoạt với một ánh xạ không mất mát phi tuyến, nhưng nó phải thỏa mãn hai tiêu chí (1)chạm nhẹ nhàng với inlier (2)triệt tiêu mạnh mẽ outlier. Về mặt này, chúng tôi xác minh rằng hàm logarithmic phù hợp đúng với mục đích này.

Cân bằng Kích hoạt Logarithmic. Để tạo ra một phân phối kích hoạt thân thiện với lượng tử hóa, chúng tôi đề xuất một phương pháp cân bằng kích hoạt logarithmic (LAE) ngoại tuyến mới điều hòa các phân phối kích hoạt theo cách phi tuyến. Cụ thể, chúng tôi tính toán thang đo kênh thứ i si như giá trị kích hoạt tối đa max(|Xi|) chia cho ánh xạ logarithmic của nó với một dịch chuyển là 2 (để có tối thiểu thang đo 1), được hiển thị trong Phương trình 1. Công thức giữ lại thông tin gốc trong khi nó nén các phân phối khác nhau một cách có thể so sánh. Hình 1 thể hiện phân phối kết quả của nó.

si= max( |xi|)/log2(2 + max( |xi|)); xi=xi/si (1)

Một khi thang đo s được thu được, chúng ta có thể cập nhật trọng số và kích hoạt tương ứng như sau,
W′=diag(s)W;X′=Xdiag(s)−1s.t. X′W′=XW (2)

Do đó, cập nhật này được thực hiện tại chỗ vì nó tương đương về mặt toán học. Đáng chú ý, s có thể dễ dàng được hợp nhất vào trọng số của lớp trước đó. Trong trường hợp của chúng tôi, chỉ có hai loại hoạt động (QKV và FC1) có phạm vi kích hoạt trong (v0, v1). Để áp dụng LAE ngoại tuyến, các cập nhật kích hoạt của chúng được hợp nhất vào hoạt động tiền nhiệm LayerNorm (Ba et al., 2016).

5

--- TRANG 6 ---
3.3.2 LƯỢNG TỬ HÓA TRỌNG SỐ
Do sự phức tạp của LLM, không thể xử lý chỉ sử dụng chiến lược per-channel vanilla, như được hiển thị trong Hình 4 (a). ZeroQuant (Yao et al., 2022) áp dụng lượng tử hóa trọng số nhóm tinh chỉnh (Shen et al., 2020) giải quyết khó khăn lượng tử hóa của các LLM nhỏ hơn như GPT-3 (Brown et al., 2020). Vì hai chiến lược có chi phí giống hệt nhau từ góc độ kỹ thuật, chúng tôi áp dụng lượng tử hóa trọng số tinh chỉnh trong đó thang đo được tính toán theo nhóm cho tất cả các lớp để có được hiệu suất tốt hơn, được mô tả trong Hình 4 (b).

(a)(b)NTrọng số Lượng tử hóa(K ×N)Chiều vàoChiều raThang đoK/NhómNhómTrọng số Lượng tử hóa(K ×N)Chiều vàoChiều raThang đo1NLAE
(c)QKVKcacheVcacheDenseLayerNormLAE
(d)UpGateDownLayerNorm

Hình 4: (a)Lượng tử hóa trọng số per-channel. (b)Lượng tử hóa per-channel tinh chỉnh. (c, d) Self-attention và FFN trong hầu hết LLM. Xanh nhạt: lượng tử hóa kích hoạt tĩnh per-tensor. Tím: lượng tử hóa kích hoạt động per-token. Tất cả trọng số được lượng tử hóa theo cách tinh chỉnh.

Khi dòng LLaMA (Touvron et al., 2023) nổi lên thành trọng tâm chính, chúng tôi minh họa sơ đồ lượng tử hóa cụ thể của chúng tôi cho kiến trúc của nó trong Hình 4 (c) và (d). Thú vị, chúng tôi phát hiện rằng xu hướng phân phối kích hoạt LLaMA giữ nguyên cho tất cả dòng mô hình, do đó sơ đồ lượng tử hóa của chúng tôi có thể được tái sử dụng trực tiếp. Cân bằng kích hoạt logarithmic được thực hiện ngoại tuyến (thang đo cho kích hoạt sau đó được hợp nhất vào LayerNorm) cho QKV và Up/Gate. Cũng đáng chú ý rằng KV cache được lượng tử hóa được áp dụng để tiết kiệm chi phí I/O.

4 THỰC NGHIỆM

4.1 TẬP DỮ LIỆU
Chúng tôi xác thực sơ đồ lượng tử hóa của mình trên một số tập dữ liệu, bao gồm LAMBADA (Paperno et al., 2016), MMLU (Hendrycks et al., 2020), và một tập các nhiệm vụ Common Sense QA (Talmor et al., 2019) như WinoGrande (Sakaguchi et al., 2021), PIQA (Tata & Patel, 2003), HellaSwag (Zellers et al., 2019), ARC e. Đối với các nhiệm vụ CommonSense QA, chúng tôi sử dụng công cụ Language Model Evaluation Harness (Gao et al., 2021) để đánh giá các mô hình của chúng tôi. Đối với tập cân chỉnh, chúng tôi lấy mẫu ngẫu nhiên 512 mẫu từ tập dữ liệu Pile (Gao et al., 2020).

4.2 TRIỂN KHAI

Mô hình Gốc SmoothQuant FPTQ
FP16 W8A8 W4A8
BLOOM-7B1 57.9080% 59.6352% 58.2185%
LLaMA-7B 73.7435% 73.7823% 73.8017%
LLaMA-13B 76.1886% 76.3633% 75.7423%
LLaMA-65B 79.1966% 78.6920% 79.1384%
LLaMA-2-7B 73.7046% 74.1510% 72.4820%
LLaMA-2-13B 76.6350% 75.5288% 75.3154%
LLaMA-2-70B 79.5653% 78.7891% 78.7114%

Bảng 3: So sánh trên Tập dữ liệu LAMBADA.

Baseline. Trong các thực nghiệm của chúng tôi, chúng tôi đã chọn SmoothQuant (Xiao et al., 2023) và GPTQ (Frantar et al., 2022) làm baseline của chúng tôi, được đưa ra trạng thái là các sơ đồ lượng tử hóa W8A8 và W4A16 phổ biến nhất. Những phương pháp này đã được áp dụng rộng rãi trong nhiều ứng dụng khác nhau và hiệu suất của chúng đã được xác thực rộng rãi, thiết lập chúng như các điểm chuẩn đáng tin cậy trong lĩnh vực lượng tử hóa LLM. Đồng thời, để tiếp tục chứng minh tiềm năng của FPTQ, chúng tôi so sánh nó với phương pháp QAT, đặc biệt với LLM-QAT (Liu et al., 2023b). Đáng chú ý rằng QAT giới thiệu chi phí tài nguyên tính toán đáng kể; ngược lại, phương pháp của chúng tôi chỉ phát sinh chi phí không đáng kể so với nó.

6

--- TRANG 7 ---
Triển khai. Chúng tôi thấy rằng đối với các LLM được điều tra trong bài báo của chúng tôi, ranh giới kích hoạt v0 có thể thường được đặt là 15 và v1 là 150.

4.3 KẾT QUẢ THỰC NGHIỆM TRÊN LAMBADA
Chúng tôi ban đầu tiến hành các thực nghiệm của mình trên tập dữ liệu LAMBADA (Paperno et al., 2016). Mặc dù thực tế là LAMBADA có thể không phản ánh hiệu quả khả năng toàn diện của mô hình, nó phục vụ như một công cụ có giá trị để xác thực nhanh chóng độ chính xác mô hình và định lượng tác động đến hiệu suất mô hình. Phương pháp của chúng tôi, Fine-grained Post-training Quantization (FPTQ), đã đạt được các mô hình lượng tử hóa W4A8 thể hiện độ chính xác nổi bật tương tự như các đối tác điểm nổi của chúng trên cả BLOOM-7B1 (Scao et al., 2022) và tất cả các mô hình trong dòng LLaMA (Touvron et al., 2023). Đây là một quan sát rất khuyến khích, gợi ý hiệu lực của phương pháp của chúng tôi.

Mô hình HyperParam MMLU Common Sense QA
Phương pháp BW Hums. STEM Social Other Avg WG PIQA HS ARC eAvg
BLOOM-7B1 FP16 W16A16 26.10 26.84 24.21 26.34 25.90 63.93 72.91 57.24 57.74 62.96
SmoothQuant W8A8 26.04 27.80 24.50 25.82 26.03 61.96 72.52 56.66 57.41 62.14
GPTQ W4A16 26.06 26.47 25.28 26.50 26.08 63.38 72.42 55.98 56.86 62.16
FPTQ W4A8 25.87 26.71 23.76 26.56 25.74 63.22 72.80 55.98 57.32 62.33
LLaMA-7B FP16 W16A16 33.60 31.10 38.20 38.40 35.20 69.85 79.16 76.10 72.80 74.48
SmoothQuant W8A8 33.88 30.32 37.63 39.08 35.14 70.09 79.00 75.17 72.22 74.12
GPTQ W4A16 32.39 30.35 35.03 36.15 33.40 68.03 77.69 72.95 69.44 72.02
FPTQ W4A8 30.20 29.95 32.76 35.87 32.02 70.01 78.40 74.46 70.79 73.42
LLaMA-13B FP16 W16A16 44.60 37.10 54.00 53.50 47.10 72.77 80.09 79.07 74.71 76.66
SmoothQuant W8A8 44.14 36.51 54.05 52.65 46.64 72.06 79.71 78.34 73.91 76.00
GPTQ W4A16 46.01 39.00 54.01 53.36 47.96 73.16 80.25 78.60 74.37 76.59
FPTQ W4A8 40.96 34.19 49.72 49.75 43.46 72.14 79.33 77.50 72.69 75.41
LLaMA-65B FP16 W16A16 61.80 52.00 73.30 67.60 63.50 77.35 82.32 84.15 79.76 80.90
SmoothQuant W8A8 61.32 50.50 71.69 66.90 62.56 74.90 81.07 82.32 77.4 78.92
GPTQ W4A16 60.23 52.09 72.15 66.75 62.60 77.43 82.32 83.57 79.88 80.80
FPTQ W4A8 59.85 49.24 71.50 65.89 61.52 75.77 81.45 83.44 78.45 79.78
LLaMA-2-7B FP16 W16A16 43.40 37.00 51.80 52.40 46.00 69.06 79.11 75.98 74.58 74.68
SmoothQuant W8A8 42.49 36.65 50.67 51.33 45.06 69.06 77.97 75.91 75.98 74.58
GPTQ W4A16 42.66 36.45 51.25 50.99 45.13 68.51 78.67 0.75.96 71.68 73.45
FPTQ W4A8 41.15 35.79 49.37 50.77 44.02 69.38 77.97 74.89 72.85 73.77
LLaMA-2-13B FP16 W16A16 54.40 44.30 63.40 60.80 55.70 72.22 80.52 79.38 77.44 77.39
SmoothQuant W8A8 52.67 43.07 63.15 60.39 54.69 72.06 79.54 79.28 77.31 77.05
GPTQ W4A16 51.99 43.57 63.05 60.49 54.56 72.30 79.60 78.79 77.23 76.98
FPTQ W4A8 51.65 42.54 62.27 59.90 53.92 70.56 79.43 78.06 75.76 75.95
LLaMA-2-70B FP16 W16A16 65.20 57.80 80.40 74.60 69.10 77.98 82.75 83.81 80.98 81.38
SmoothQuant W8A8 63.23 56.46 79.23 72.42 67.40 78.14 82.37 82.60 80.72 80.96
GPTQ W4A16 62.93 57.65 79.62 74.12 68.04 78.06 82.92 83.37 80.89 81.31
FPTQ W4A8 62.83 55.27 78.23 72.49 66.81 77.03 82.37 82.58 79.88 80.47

Bảng 4: So sánh trên MMLU và Common Sense QA. BW: BitWidth

4.4 KẾT QUẢ TRÊN MMLU VÀ COMMON SENSE QA
MMLU (Hendrycks et al., 2020) và Common Sense QA (Talmor et al., 2019) là các tập dữ liệu nổi tiếng hiện tại phản ánh toàn diện hiệu suất của LLM. Chúng tôi tiến hành các thực nghiệm rộng rãi trên những tập dữ liệu này, bao gồm đánh giá so sánh với hai giải pháp tối tân: SmoothQuant cho W8A8, và GPTQ cho W4A16.

Trên tập dữ liệu MMLU, phương pháp của chúng tôi thể hiện khoảng cách hiệu suất trong vòng 1% cho hầu hết các mô hình so với SmoothQuant. Các outlier đáng chú ý bao gồm LLaMA-7B và LLaMA-13B, cho thấy sự giảm rõ rệt hơn. Tuy nhiên, điều quan trọng cần lưu ý rằng tập dữ liệu MMLU, với thành phần chủ đạo của câu hỏi trắc nghiệm, có thể thể hiện thiên vị trong ước tính độ chính xác khi khả năng vốn có của mô hình bị hạn chế.

Trên Common Sense QA, phương pháp của chúng tôi thể hiện chỉ 1% khoảng cách độ chính xác với mô hình FP16 trên gần như tất cả các mô hình, bao gồm các mô hình kém hiệu suất đã được xác định trước đây LLaMA-7B và LLaMA-13B trên MMLU. Quan sát này nhấn mạnh tính mạnh mẽ của phương pháp chúng tôi.

7

--- TRANG 8 ---
4.5 SO SÁNH VỚI LLM-QAT TRÊN COMMON SENSE QA

Mô hình Phương pháp Cài đặt WG PIQA HS ARC e Trung bình
LLaMA-7B Gốc FP16 69.85 79.16 44.40 72.81 74.51
LLM-QAT W4A8 68.80 77.40 73.00 68.40 71.90
FPTQ W4A8 70.09 78.62 74.45 70.37 73.38
LLaMA-13B Gốc FP16 72.22 80.52 79.38 77.44 77.39
LLM-QAT W4A8 70.60 79.10 77.50 73.00 75.05
FPTQ W4A8 72.85 80.09 78.20 76.09 76.81

Bảng 5: So sánh với LLM-QAT trên LLaMA-7B.

Do sự thiếu hụt của các công việc Lượng tử hóa Hậu huấn luyện (PTQ) khác sử dụng lượng tử hóa W4A8, chúng tôi tiến hành một nghiên cứu so sánh với phương pháp Quantization-Aware Training (QAT), LLM-QAT (Liu et al., 2023b), trên tập dữ liệu Common Sense QA. Phương pháp của chúng tôi đạt được độ chính xác gần với mô hình FP16 hơn đáng kể so với LLM-QAT. Tuy nhiên, do dữ liệu hạn chế có sẵn công khai từ LLM-QAT, chúng tôi trình bày ở đây kết quả thực nghiệm chỉ cho LLaMA-7B và LLaMA-13B. Có thể quan sát thấy rằng phương pháp của chúng tôi mang lại kết quả hơi vượt trội trên mọi tập con của tập dữ liệu so với LLM-QAT, làm nổi bật hiệu quả của phương pháp học của chúng tôi.

5 NGHIÊN CỨU ABLATION

5.1 SO SÁNH VỚI LƯỢNG TỬ HÓA KHÔNG DỮ LIỆU
Chúng tôi thừa nhận rằng tập dữ liệu cân chỉnh có thể là một trong những yếu tố ảnh hưởng đến hiệu suất của mô hình được lượng tử hóa. Do đó, để duy trì công bằng, chúng tôi sử dụng tập dữ liệu Pile (Gao et al., 2020) như một tập dữ liệu cân chỉnh trong các thực nghiệm trước đây của chúng tôi. Tuy nhiên, để chứng minh tính mạnh mẽ của phương pháp chúng tôi, chúng tôi áp dụng các token được tạo ngẫu nhiên cho cân chỉnh mô hình. Chúng tôi tiến hành các nghiên cứu ablation trên BLOOM-7B1, LLaMA-7B và LLaMA-2-7B dưới cài đặt độ rộng bit W8A8 và W4A8 trong Bảng 6. Thật phấn khích khi lưu ý rằng, đã được phát hiện rằng sử dụng tập dữ liệu ngẫu nhiên thường dẫn đến kết quả vượt trội trong hầu hết các trường hợp. Điều này chứng thực rằng phương pháp của chúng tôi có thể áp dụng trong các tình huống không có dữ liệu.

Mô hình HyperParam MMLU Common Sense QA
Cân chỉnh BW Hums. STEM Social Other Avg WG PIQA HS ARC eAvg
LLaMA-7B Pile W8A8 33.88 30.32 37.63 39.08 35.14 70.09 79.00 75.17 72.22 74.12
Random W8A8 32.33 29.85 36.46 38.25 34.07 70.01 78.62 75.48 72.69 74.20
Pile W4A8 30.20 29.95 32.76 35.87 32.02 70.01 78.40 74.46 70.79 73.42
Random W4A8 31.20 31.05 36.37 37.01 33.64 68.67 78.62 74.62 71.21 73.28
LLaMA-2-7B Pile W8A8 42.49 36.65 50.67 51.33 45.06 69.06 77.97 75.91 75.98 74.58
Random W8A8 42.55 36.28 51.41 51.63 45.24 67.80 79.22 75.98 74.28 74.32
Pile W4A8 41.15 35.79 49.37 50.77 44.02 69.38 77.97 74.89 72.85 73.77
Random W4A8 41.32 35.42 47.97 50.19 43.56 67.88 78.07 75.46 73.11 73.63
BLOOM-7B1 Pile W8A8 26.04 27.80 24.50 25.82 26.03 63.93 72.91 57.24 57.74 62.96
Random W8A8 25.80 27.60 25.06 26.96 26.29 63.77 72.80 56.65 57.45 62.67
Pile W4A8 25.87 26.71 23.76 26.56 25.74 61.96 72.52 56.66 57.41 62.14
Random W4A8 26.29 27.04 23.37 27.08 25.99 61.88 72.42 56.17 56.94 61.85

Bảng 6: Nghiên cứu ablation trên tập dữ liệu cân chỉnh trên MMLU và Common Sense QA.

5.2 LƯỢNG TỬ HÓA TRỌNG SỐ VỚI GPTQ
Chúng tôi quan sát thấy rằng phương pháp GPTQ, bù đắp trọng số dựa trên ma trận Hessian, trực giao với phương pháp hiện tại của chúng tôi. Do đó, chúng tôi cố gắng tinh chỉnh trọng số sử dụng phương pháp GPTQ sau khi tiến hành cân bằng kích hoạt logarithmic (LAE) trên mô hình, để điều tra tiềm năng tăng độ chính xác. Tuy nhiên, các thực nghiệm của chúng tôi trong Bảng 7 chứng minh rằng việc thêm hoạt động GPTQ thường dẫn đến tác động tiêu cực đến độ chính xác trong hầu hết các trường hợp. Chúng tôi khuyến khích các nhà nghiên cứu tương lai tiến hành các khám phá thú vị hơn trong lĩnh vực này.

8

--- TRANG 9 ---
Mô hình HyperParam MMLU Common Sense QA
Phương pháp BW Hums. STEM Social Other Avg WG PIQA HS ARC eAvg
LLaMA-7B FP16 W16A16 33.60 31.10 38.20 38.40 35.20 69.85 79.16 76.21 72.81 74.51
FPTQ W4A8 30.20 29.95 32.76 35.87 32.02 70.01 78.40 74.46 70.79 73.42
FPTQ GPTQ W4A8 28.40 28.33 30.84 33.22 30.03 68.82 78.13 72.88 66.96 71.70
LLaMA-2-7B FP16 W16A16 43.40 37.00 51.80 52.40 46.00 69.06 79.11 75.98 74.58 74.68
FPTQ W4A8 41.15 35.79 49.37 50.77 44.02 69.38 77.97 74.89 72.85 73.77
FPTQ GPTQ W4A8 40.57 35.42 48.55 48.86 43.13 67.56 78.35 74.90 72.94 73.44
BLOOM-7B1 FP16 W16A16 26.10 26.84 24.21 26.34 25.90 63.93 72.91 57.24 57.74 62.96
FPTQ W4A8 25.87 26.71 23.76 26.56 25.74 61.96 72.52 56.66 57.41 62.14
FPTQ GPTQ W4A8 26.21 28.20 25.28 26.37 26.47 62.90 72.31 55.39 57.28 61.97

Bảng 7: Ablation trên MMLU và Common Sense QA. FPTQ GPTQ : trọng số được cập nhật bởi GPTQ trước.

6 THẢO LUẬN VÀ HƯỚNG TƯƠNG LAI

Phân tích về hiệu quả tính toán. GPU hiện đại, như NVIDIA A100, hỗ trợ tính toán ma trận khối song song và xử lý pipeline. Lượng tử hóa trọng số tinh chỉnh hưởng lợi từ tính toán khối như vậy và giới thiệu ít chi phí. Hiện tại, tăng tốc W4A16 dựa trên kernel GPU FP16INT4 GEMM (Kim et al., 2022), triển khai tính toán ma trận kiểu hỗn hợp. Trọng số INT4 đầu tiên được chuyển đổi thành FP16, và tính toán ma trận sau đó được thực hiện với FP16. Tính toán cơ bản vẫn sử dụng đơn vị tính toán điểm nổi của GPU, vì vậy trong trường hợp đầu vào dài và batch lớn, kernel FP16INT4 thậm chí có tác dụng tiêu cực so với tính toán FP16 trực tiếp do chuyển đổi bổ sung. Tăng tốc tính toán W8A8 dựa trên kernel GPU INT8 GEMM, sử dụng INT8 Tensor Cores cho tính toán cơ bản. Có tăng tốc đáng chú ý trong giai đoạn giải mã ngữ cảnh, nhưng trong giai đoạn tự giải mã, nút thắt cổ chai chủ yếu nằm ở truy cập bộ nhớ.

Để đồng thời giải quyết các vấn đề tăng tốc trong cả giai đoạn giải mã ngữ cảnh và tự giải mã, chúng ta có thể thiết kế một kernel INT8INT4, có lợi từ INT8 Tensor Cores cho tăng tốc trong giai đoạn giải mã ngữ cảnh, trong khi giữ trọng số được tải như INT4 để giảm thời gian truy cập bộ nhớ trong giai đoạn tự giải mã.

Lượng tử hóa không dữ liệu. Chúng tôi phát hiện rằng việc rút ngẫu nhiên các mẫu từ từ vựng token như trong Bảng 6 là đầy hứa hẹn. Chúng tôi tin rằng vẫn còn chỗ cho cải tiến trong khía cạnh này.

Tính toán thang đo chỉ yêu cầu kích hoạt. Đối với lượng tử hóa kích hoạt, phương pháp của chúng tôi hoàn toàn loại bỏ trọng số cho việc tính toán si điều này phản hồi các phát hiện trong Lin et al. (2023). Để làm cho chiến lược của chúng tôi tổng quát hơn, chúng tôi giới thiệu siêu tham số α để kiểm soát mức độ triệt tiêu, xem A.2. Tuy nhiên có thể phát minh ra các hàm ánh xạ phi tuyến khác không có siêu tham số và trong khi đó dẫn đến hiệu suất tốt hơn.

7 KẾT LUẬN
Tóm lại, công việc của chúng tôi trình bày một bước tiến đáng kể trong lĩnh vực nén Mô hình Ngôn ngữ Lớn (LLM). Sau một cái nhìn tổng quan về các sơ đồ lượng tử hóa hiện tại, chúng tôi giới thiệu một phương pháp lượng tử hóa hậu huấn luyện mới có thể làm cho suy luận của LLM hiệu quả hơn, mà không làm tổn hại hiệu suất của chúng. Chúng tôi thành công đạt được hiệu suất cao và hiệu quả cho W4A8, có sử dụng tối ưu tài nguyên tính toán tăng cường tốc độ của cả giai đoạn giải mã nội dung và tự giải mã. Hơn nữa, việc loại bỏ nhu cầu tinh chỉnh trong quá trình huấn luyện đơn giản hóa đáng kể pipeline triển khai. Điều này chứng thực rằng phương pháp của chúng tôi cung cấp một giải pháp triển khai hiệu quả cho LLM mà không hy sinh độ chính xác của chúng. Trong khi tiến bộ của chúng tôi đáng khuyến khích, chúng tôi thừa nhận tiềm năng cho việc khám phá và tinh chỉnh thêm trong lĩnh vực này. Chúng tôi dự đoán rằng công việc của chúng tôi sẽ truyền cảm hứng cho các nỗ lực nghiên cứu tương lai nhằm làm cho LLM thậm chí hiệu quả và thực tế hơn.

9

--- TRANG 10 ---
TÀI LIỆU THAM KHẢO
Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Trong Conference on Neural Information Processing Systems (NeurIPS) , 2020.

Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 , 2022.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong North American Chapter of the Association for Computational Linguistics (NAACL) , 2019.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, và Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360 , 2021.

Elias Frantar và Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628 .

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.

Young Jin Kim, Rawn Henry, Raffy Fahim, và Hany Hassan Awadalla. Who says elephants can't run: Bringing large scale moe models into cloud scale production. arXiv preprint arXiv:2211.10017 , 2022.

Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The BigScience corpus: A 1.6 TB composite multilingual dataset. 2022.

Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, và Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2023.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.

Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, và Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888 , 2023a.

Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, và Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models, 2023b.

10

--- TRANG 11 ---
Xinyin Ma, Gongfan Fang, và Xinchao Wang. Llm-pruner: On the structural pruning of large language models, 2023.

Markus Nagel, Mart van Baalen, Tijmen Blankevoort, và Max Welling. Data-free quantization through weight equalization and bias correction. Trong International Conference on Computer Vision (ICCV) , 2019.

Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, và Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295 , 2021.

Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, và Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031 , 2016.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, và Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. Trong Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pp. 8815–8821, 2020.

Mingjie Sun, Zhuang Liu, Anna Bair, và J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695 , 2023.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, và Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge, 2019.

Sandeep Tata và Jignesh M Patel. PiQA: An algebra for querying protein data sets. Trong International Conference on Scientific and Statistical Database Management , 2003.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Conference on Neural Information Processing Systems (NeurIPS) , 2017.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 , 2022.

Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, và Paulius Micikevicius. Integer quantization for deep learning inference: Principles and empirical evaluation, 2020.

Xiaoxia Wu, Zhewei Yao, và Yuxiong He. Zeroquant-fp: A leap forward in llms post-training w4a8 quantization using floating-point formats, 2023.

Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, và Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. Trong International Conference on Machine Learning , pp. 38087–38099. PMLR, 2023.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems , 32, 2019.

11

--- TRANG 12 ---
Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, et al. HAWQ-v3: Dyadic neural network quantization. Trong International Conference on Machine Learning (ICML) , 2021.

Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, và Yuxiong He. ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.

Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, và Yuxiong He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation, 2023.

Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, và Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models, 2023.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.

Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, và Dawei Song. Lifting the curse of capacity gap in distilling language models, 2023.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.

12

--- TRANG 13 ---
A PHỤ LỤC

A.1 KIẾN THỨC SƠ BỘ VỀ LƯỢNG TỬ HÓA
Lượng tử hóa là một quá trình ánh xạ các giá trị liên tục thành các giá trị rời rạc bằng cách chia tỷ lệ. Hệ số tỷ lệ cũng được gọi là kích thước bước lượng tử hóa. Trong thực tế, điểm nổi độ chính xác cao hơn được sử dụng cho huấn luyện và phiên bản lượng tử hóa được sử dụng cho suy luận. Xem xét lượng tử hóa số nguyên b-bit, đối với tensor thực x trong phạm vi (min, max ), nó có thể được chuyển đổi thành tensor số nguyên x′ trong khoảng (−2b−1,2b−1−1) bằng lượng tử hóa đều đối xứng như,

scale =max(|x|)/(2b−1−1) (3)
x′=⌊(x/scale )⌉ (4)

Lượng tử hóa trọng số và lượng tử hóa kích hoạt. Thông thường, trọng số được lượng tử hóa như các giá trị số nguyên. Lượng tử hóa kích hoạt đề cập đến lượng tử hóa của các bản đồ tính năng kích hoạt trung gian.

Lượng tử hóa tĩnh so với lượng tử hóa động. Đối với lượng tử hóa tĩnh, thống kê kích hoạt ngoại tuyến được thu thập để tính toán thang đo và nó được giữ tĩnh trong quá trình suy luận. Đối với lượng tử hóa động, các thống kê như vậy được tính toán tại thời gian chạy.

Per-tensor so với per-token. Trong sơ đồ per-tensor, ma trận tensor được xem xét như một tổng thể để tính toán thang đo lượng tử hóa. Trong sơ đồ per-token, mỗi token đầu vào tương ứng với một thang đo được tính toán trên tất cả các kênh kích hoạt của token cụ thể. Về bản chất, sơ đồ per-token tinh chỉnh hơn.

Per-channel so với nhóm. Trong sơ đồ per-channel, thang đo lượng tử hóa được tính toán theo kênh. Trong sơ đồ nhóm, mỗi kênh được chia thành nhiều nhóm và thang đo của chúng cũng vậy.

A.2 DẠNG TỔNG QUÁT CỦA LAE
Chúng tôi đưa ra dạng tổng quát của hàm cân bằng kích hoạt logarithmic. Đối với đa số LLM, chúng tôi sử dụng α= 1.

scale = (log2(2 +scale ))α(5)

A.3 THÊM PHÂN PHỐI KÍCH HOẠT CỦA LLM
Chúng tôi trực quan hóa các phân phối kích hoạt của dòng LLaMA trong Hình 5, 6, 7, 8, 9, và 10. Thật thú vị khi thấy rằng các mô hình LLaMA ở các quy mô khác nhau chia sẻ các phân phối tương tự trong các hoạt động giống nhau, dẫn đến một sơ đồ lượng tử hóa phổ quát.

13

--- TRANG 14 ---
0 10 20 30
Lớp100101Giá trị kích hoạtgiá trị kích hoạt q_proj
0 10 20 30
Lớp101
100101Giá trị kích hoạtgiá trị kích hoạt o_proj
0 10 20 30
Lớp100101Giá trị kích hoạtgiá trị kích hoạt up_proj
0 10 20 30
Lớp101
100101102103Giá trị kích hoạtgiá trị kích hoạt down_proj

Hình 5: Trực quan hóa phân phối kích hoạt của oproj và down proj trên LLaMA-2-7B.

0 10 20 30
Lớp100101Giá trị kích hoạtgiá trị kích hoạt q_proj
0 10 20 30
Lớp101
100101Giá trị kích hoạtgiá trị kích hoạt o_proj
0 10 20 30
Lớp100101Giá trị kích hoạtgiá trị kích hoạt up_proj
0 10 20 30
Lớp101
100101102103Giá trị kích hoạtgiá trị kích hoạt down_proj

Hình 6: Trực quan hóa phân phối kích hoạt của oproj và down proj trên LLaMA-2-13B.

14

--- TRANG 15 ---
0 10 20 30 40 50 60 70
Lớp100101Giá trị kích hoạtgiá trị kích hoạt q_proj
0 10 20 30 40 50 60 70
Lớp101
100101Giá trị kích hoạtgiá trị kích hoạt o_proj
0 10 20 30 40 50 60 70
Lớp100101Giá trị kích hoạtgiá trị kích hoạt up_proj
0 10 20 30 40 50 60 70
Lớp101
100101102103104Giá trị kích hoạtgiá trị kích hoạt down_proj

Hình 7: Trực quan hóa phân phối kích hoạt của oproj và down proj trên LLaMA-2-70B.

15

--- TRANG 16 ---
0 10 20 30
Lớp100101Giá trị kích hoạtgiá trị kích hoạt q_proj
0 10 20 30
Lớp101
100101Giá trị kích hoạtgiá trị kích hoạt o_proj
0 10 20 30
Lớp100101Giá trị kích hoạtgiá trị kích hoạt up_proj
0 10 20 30
Lớp101
100101102103Giá trị kích hoạtgiá trị kích hoạt down_proj

Hình 8: Trực quan hóa phân phối kích hoạt của oproj và down proj trên LLaMA-7B.

0 10 20 30
Lớp100101Giá trị kích hoạtgiá trị kích hoạt q_proj
0 10 20 30
Lớp101
100101Giá trị kích hoạtgiá trị kích hoạt o_proj
0 10 20 30
Lớp100101Giá trị kích hoạtgiá trị kích hoạt up_proj
0 10 20 30
Lớp101
100101102103Giá trị kích hoạtgiá trị kích hoạt down_proj

Hình 9: Trực quan hóa phân phối kích hoạt của oproj và down proj trên LLaMA-13B.

16

--- TRANG 17 ---
0 10 20 30 40 50 60 70
Lớp100101Giá trị kích hoạtgiá trị kích hoạt q_proj
0 10 20 30 40 50 60 70
Lớp101
100101Giá trị kích hoạtgiá trị kích hoạt o_proj
0 10 20 30 40 50 60 70
Lớp100101Giá trị kích hoạtgiá trị kích hoạt up_proj
0 10 20 30 40 50 60 70
Lớp101
100101102103Giá trị kích hoạtgiá trị kích hoạt down_proj

Hình 10: Trực quan hóa phân phối kích hoạt của oproj và down proj trên LLaMA-65B.

17
