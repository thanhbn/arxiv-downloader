# 2308.09723.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2308.09723.pdf
# Kích thước file: 665123 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
FineQuant: Mở khóa hiệu quả với lượng tử hóa chỉ trọng số mịn cho các mô hình ngôn ngữ lớn
Young Jin Kim∗
Microsoft
youki@microsoft.comRawn Henry∗
NVIDIA
rhenry@nvidia.com
Raffy Fahim
Microsoft
raffybekheit@microsoft.comHany Hassan Awadalla
Microsoft
hanyh@microsoft.com

Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) đã đạt được hiệu suất tối ưu trên các tác vụ ngôn ngữ khác nhau nhưng gặp thách thức trong việc triển khai thực tế do yêu cầu bộ nhớ đáng kể. Hơn nữa, các mô hình sinh mới nhất gặp phải chi phí suy luận cao do nút cổ chai băng thông bộ nhớ trong quá trình giải mã tự hồi quy. Để giải quyết những vấn đề này, chúng tôi đề xuất một phương pháp lượng tử hóa chỉ trọng số hiệu quả giúp giảm tiêu thụ bộ nhớ và tăng tốc suy luận cho LLMs. Để đảm bảo suy giảm chất lượng tối thiểu, chúng tôi giới thiệu một phương pháp heuristic đơn giản và hiệu quả chỉ sử dụng trọng số mô hình của một mô hình đã được huấn luyện trước. Phương pháp này có thể áp dụng cho cả mô hình Mixture-of-Experts (MoE) và mô hình dày đặc mà không cần fine-tuning bổ sung. Để chứng minh hiệu quả của phương pháp đề xuất, đầu tiên chúng tôi phân tích các thách thức và vấn đề liên quan đến lượng tử hóa LLM. Tiếp theo, chúng tôi trình bày phương pháp heuristic của mình, có thể tìm kiếm thích ứng độ chi tiết của lượng tử hóa, giải quyết hiệu quả những vấn đề này. Hơn nữa, chúng tôi triển khai các GEMM GPU có hiệu quả cao thực hiện phép nhân ma trận và giải lượng tử hóa ngay lập tức, hỗ trợ phép nhân của kích hoạt fp16 hoặc bf16 với trọng số int8 hoặc int4. Chúng tôi đánh giá phương pháp của mình trên các mô hình mã nguồn mở quy mô lớn như OPT-175B và các mô hình MoE nội bộ, cho thấy mất mát độ chính xác tối thiểu trong khi đạt được thông lượng cao hơn lên đến 3,65 lần trên cùng số lượng GPU.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs) đã chứng minh hiệu quả của chúng trong các tác vụ ngôn ngữ khác nhau bằng cách tăng số lượng tham số có thể huấn luyện và huấn luyện trước các mô hình trên dữ liệu quy mô lớn để sử dụng trong các tác vụ downstream khác nhau (Devlin et al., 2018; Radford et al., 2018; Liu et al., 2019; Raffel et al., 2020). Với sự tiến bộ của các phương pháp huấn luyện quy mô lớn phân tán (Shazeer et al., 2018; Rasley et al., 2020; Ren et al., 2021; Baines et al., 2021) và thu thập dữ liệu quy mô lớn (Raffel et al., 2020; Hoffmann et al., 2022), các mô hình đã phát triển lớn hơn và đạt được hiệu suất tối ưu với khả năng học tăng cường, thể hiện khả năng học trong ngữ cảnh (Brown et al., 2020; Zhang et al., 2022; Chowdhery et al., 2022) có thể được sử dụng cho các tác vụ ngôn ngữ khác nhau ngay cả mà không cần cập nhật tham số cho các tác vụ cụ thể. Zhang et al. (2022)

Tuy nhiên, việc triển khai các mô hình lớn như vậy đi kèm với chi phí đáng kể tăng tỷ lệ thuận với kích thước mô hình. Sự tăng trưởng kích thước mô hình đã tăng nhiều bậc độ lớn trong vài năm qua (lớn hơn 1.588 lần từ BERT large - 340 triệu đến PaLM 540 tỷ)(Devlin et al., 2018; Chowdhery et al., 2022), và mà không cải thiện hiệu quả suy luận, chi phí suy luận và độ trễ sẽ tăng mạnh.

∗Đóng góp ngang nhau.
Preprint. Đang được xem xét.arXiv:2308.09723v1  [cs.LG]  16 Aug 2023

--- TRANG 2 ---
Lượng tử hóa là một kỹ thuật nén giảm kích thước mô hình và tăng tốc suy luận bằng cách xấp xỉ các số dấu phẩy động với các số có độ chính xác nhỏ hơn. Nhiều nghiên cứu đã chứng minh hiệu quả của lượng tử hóa trong việc tăng tốc suy luận mô hình mạng thần kinh (Rodriguez et al., 2018; Stock et al., 2019; Choukroun et al., 2019; Gholami et al., 2022), đặc biệt trong sinh ngôn ngữ tự nhiên, như dịch máy (Kim et al., 2019; Aji và Heafield, 2020; Fan et al., 2021; Park et al., 2022; Kim et al., 2022) và các tác vụ hiểu ngôn ngữ tự nhiên (Kim và Awadalla, 2020). Tuy nhiên, vẫn chưa được khám phá kỹ về cách lượng tử hóa chỉ trọng số có thể được sử dụng hiệu quả trong bối cảnh các mô hình ngôn ngữ lớn. Ngoài ra, các phương pháp hiện tại giới thiệu các thủ tục phức tạp và tốn kém như Huấn luyện Nhận biết Lượng tử hóa (QAT) bổ sung và/hoặc hiệu chuẩn trên dữ liệu bổ sung. Nếu không, chúng làm tổn hại tốc độ hoặc độ chính xác. Để giải quyết thách thức hiệu quả hơn, chúng tôi tập trung vào phương pháp lượng tử hóa chỉ trọng số đơn giản không yêu cầu huấn luyện bổ sung trong nghiên cứu này vì nó có nhiều ưu điểm - (i) độ chính xác có thể được duy trì tốt vì tính toán số học cơ bản được thực hiện ở độ chính xác dấu phẩy động chính xác hơn. Do đó, chúng ta có thể đẩy độ chính xác xuống các dải bit rất thấp một cách hiệu quả. (ii) nó có thể được sử dụng cho các kiến trúc phần cứng và GPU khác nhau mà không cần các hướng dẫn phần cứng cụ thể xử lý phép nhân bit thấp. (iii) nó có thể tránh các bước huấn luyện bổ sung tốn kém. Khi đó, các câu hỏi nghiên cứu chính là làm thế nào để khai thác hiệu quả lượng tử hóa bit thấp này mà không mất độ chính xác và làm thế nào để triển khai hiệu quả một GEMM chấp nhận các loại khác nhau trên GPU hiện đại.

Trong bài báo này, chúng tôi đưa ra những đóng góp sau:
1. Phân tích toàn diện về hành vi lượng tử hóa: Chúng tôi cung cấp phân tích toàn diện về hành vi lượng tử hóa trên Mô hình Mô hình Ngôn ngữ (LLMs). Chúng tôi điều tra tác động của việc áp dụng lượng tử hóa bit thấp (xuống 3-bit) lên độ chính xác LLM.
2. Thuật toán lượng tử hóa mịn: Chúng tôi đề xuất một thuật toán lượng tử hóa mịn kết hợp lượng tử hóa theo nhóm và lựa chọn thích ứng độ chi tiết. Phương pháp này giúp bảo toàn độ chính xác dấu phẩy động ban đầu ngay cả khi có mất mát do lượng tử hóa.
3. Kernel GPU hiệu quả cao: Chúng tôi triển khai các kernel GPU hiệu quả cao và tiến hành phân tích hiệu suất kỹ lưỡng, xem xét các kích thước batch và độ dài ngữ cảnh khác nhau. Phân tích này cho phép chúng tôi xác định việc sử dụng tối ưu của phương pháp đề xuất trên GPU thực.
4. Suy luận tăng tốc với mô hình quy mô lớn: Chúng tôi chứng minh hiệu quả của phương pháp đề xuất bằng cách áp dụng nó lên một mô hình transformer dày đặc mã nguồn mở quy mô lớn gọi là OPT. Với 175 tỷ tham số và các mô hình MoE nội bộ sử dụng kernel GPU tối ưu hóa, phương pháp của chúng tôi cho phép triển khai mô hình 175 tỷ tham số chỉ trên 2 GPU, dẫn đến giảm đáng kể chi phí phụ và chi phí 64%. Hơn nữa, phương pháp của chúng tôi đạt được thông lượng cao hơn 3,65 lần trên cùng số lượng GPU.

Những đóng góp này cùng nhau thúc đẩy hiểu biết về hành vi lượng tử hóa trong LLMs, đề xuất một thuật toán lượng tử hóa hiệu quả, tối ưu hóa triển khai GPU, và chứng minh lợi ích thực tế về giảm yêu cầu tài nguyên và cải thiện thông lượng suy luận.

2 Nền tảng - Thách thức của việc lượng tử hóa LLMs

2.1 Thách thức cơ bản của suy luận LLMs sinh
Tăng chi phí giao tiếp. Chúng ta phải thực hiện một all reduce sau mỗi khối attention và FFN khi thực hiện suy luận với tensor parallelism. Trong khi các công nghệ như NVLink và NCCL tăng tốc mạnh giao tiếp GPU với GPU, việc sử dụng ít GPU nhất có thể để giảm thiểu chi phí này là mong muốn.

Trọng số lớn với kích hoạt nhỏ. Sự tăng kích thước mô hình khiến các phép nhân ma trận trong giai đoạn giải mã của LLMs bị tắc nghẽn bởi băng thông bộ nhớ. Trọng số thường chiếm ưu thế trong lưu lượng bộ nhớ vì các kích hoạt có xu hướng chỉ có một vài token một khi ngữ cảnh đã được sử dụng để tạo cache attention KV. Khi số lượng tham số tăng, lượng dữ liệu phải di chuyển từ HBM đến lõi GPU tăng lên, đặt áp lực lớn hơn lên hệ thống con bộ nhớ. Trong các bộ xử lý hiện đại, tính toán nhanh hơn nhiều so với bộ nhớ nên việc giảm nút cổ chai bộ nhớ là mong muốn.

Với những quan sát đó, việc giảm dấu chân bộ nhớ là rất quan trọng.

2.2 Thách thức lượng tử hóa
Lượng tử hóa là một chủ đề nghiên cứu tích cực để tăng tốc suy luận và giảm dấu chân bộ nhớ của LLMs. Tuy nhiên, vẫn còn nhiều thách thức, và đặc biệt không có phương pháp nào có thể duy trì độ chính xác và cải thiện hiệu quả cùng lúc mà không giới thiệu các thủ tục phức tạp để chuyển đổi và thực hiện suy luận.

Khó duy trì độ chính xác tốt khi áp dụng lượng tử hóa trên LLMs. Người ta biết rằng các phương pháp lượng tử hóa ngây thơ có thể làm suy giảm đáng kể độ chính xác so với các mô hình gốc (Frantar et al., 2022). Một lý do cho điều này là các outlier trong kích hoạt dựa trên các nghiên cứu trước (Dettmers et al., 2022; Xiao et al., 2022). Dettmers et al. (2022); Xiao et al. (2022) đề xuất các phương pháp để giảm thiểu vấn đề này bằng cách xử lý các outlier riêng biệt trong phép tính dấu phẩy động hoặc bằng cách chuyển hệ số nhân từ kích hoạt sang trọng số mô hình.

Khó đạt được hiệu quả cao. Ngay cả khi một số thuật toán có thể duy trì độ chính xác của các mô hình dấu phẩy động gốc, việc triển khai hiệu quả phương pháp đề xuất trong thực tế cũng không đơn giản. Điều này đòi hỏi các triển khai kernel đặc biệt trên GPU. Ví dụ, Dettmers et al. (2022) có thể đạt được độ chính xác tốt với lượng tử hóa, nhưng cải thiện hiệu quả là nhỏ. Ngoài ra, OPTQ Frantar et al. (2022) không cung cấp kernel suy luận hiệu quả ngoài kích thước batch 1. Tuy nhiên, chúng tôi lưu ý rằng các kernel GPU hiệu quả của chúng tôi có thể được sử dụng với trọng số được lượng tử hóa bởi OPTQ., cho phép người ta hưởng lợi từ tốc độ của kernel của chúng tôi và độ chính xác của OPTQ.

Thêm phức tạp để giải quyết vấn đề. Để khắc phục các vấn đề về sụt giảm độ chính xác và thiếu hiệu quả của thời gian chạy, đã có một số nghiên cứu được đề xuất. Những phương pháp này yêu cầu các thủ tục tốn kém và phức tạp để đạt được mục tiêu, đặc biệt với bộ dữ liệu cụ thể cho tác vụ mục tiêu để hiệu chuẩn. Yao et al. (2022) sử dụng các bước chưng cất kiến thức bổ sung để khôi phục sụt giảm độ chính xác từ lượng tử hóa. Park et al. (2022) sử dụng lượng tử hóa mã hóa nhị phân và thực hiện tối ưu hóa số lặp để tìm sơ đồ mã hóa nhị phân tốt nhất cho một mô hình và tác vụ nhất định, điều này không đơn giản. Frantar et al. (2022) sử dụng Optimal Brain Quantization (OBQ) để duy trì độ chính xác của mô hình dấu phẩy động gốc, xáo trộn trọng số mô hình dựa trên thông tin ma trận Hessian bậc hai xấp xỉ. Tất cả những phương pháp đó đã giới thiệu các thủ tục thuật toán không đơn giản và cụ thể cho bộ dữ liệu. Đặc biệt, chi phí của những thuật toán này tăng cùng với kích thước của các mô hình cơ sở.

Trong công trình này, mục tiêu của chúng tôi là tìm một phương pháp lượng tử hóa có thể mở rộng, chính xác và hiệu quả mà không giới thiệu chi phí bổ sung cho việc chuyển đổi mô hình.

3 Thiết kế phương pháp lượng tử hóa cho LLMs - Lượng tử hóa mịn thích ứng

Phần này đi sâu vào hiện tượng quan sát được trong lượng tử hóa LLM, cụ thể tập trung vào các vấn đề tiềm năng có thể dẫn đến suy giảm chất lượng, đặc biệt liên quan đến phạm vi lượng tử hóa. Chúng tôi kiểm tra kỹ lưỡng những vấn đề này và khám phá các chiến lược tiềm năng để giảm thiểu chúng trong khi đảm bảo kiểm soát hiệu quả phạm vi lượng tử hóa. Dựa trên phân tích của chúng tôi, chúng tôi đề xuất một thuật toán heuristic được thiết kế để tự động xác định phạm vi lượng tử hóa phù hợp.

3.1 Phương pháp lượng tử hóa: cài đặt cơ bản
Tính đồng nhất của lượng tử hóa
Chúng tôi tiến hành thí nghiệm với hai kỹ thuật lượng tử hóa tập trung vào tính đồng nhất của phạm vi lượng tử hóa. Đầu tiên, chúng tôi sử dụng lượng tử hóa tuyến tính, ánh xạ đồng nhất các giá trị số nguyên lượng tử hóa với các giá trị float gốc tương ứng. Thứ hai, chúng tôi khám phá lượng tử hóa dựa trên log, lấy cảm hứng từ Aji và Heafield (2020), trong đó cả phạm vi số nguyên và float đều được ánh xạ theo thang logarit. Trong cả hai trường hợp, chúng tôi áp dụng lượng tử hóa theo cột để đánh giá tác động của tính đồng nhất lượng tử hóa

--- TRANG 3 ---
lên độ chính xác mô hình. Các công thức chi tiết cho hai kỹ thuật này được mô tả trong Phụ lục A.

Hình 1 minh họa so sánh hiệu suất giữa hai kỹ thuật lượng tử hóa được áp dụng cho các lớp FFN sử dụng bit thấp. Đối với 3 và 4 bit, cả hai kỹ thuật đều thể hiện hiệu suất tương tự. Tuy nhiên, với lượng tử hóa 2-bit, lượng tử hóa thang log cho thấy sự giảm đáng kể về độ chính xác. Xem xét những quan sát này và sự đơn giản tính toán, chúng tôi chọn sử dụng lượng tử hóa đồng nhất cho tất cả các thí nghiệm tiếp theo.

Tính đối xứng - phân phối số của trọng số mô hình
Để xác định phương pháp lượng tử hóa phù hợp nhất, chúng tôi đã tiến hành phân tích thêm về phân phối tham số trọng số trên các lớp khác nhau. Hình 2 trình bày các phân phối ví dụ của trọng số mô hình, thường thể hiện phân phối chuẩn tập trung quanh số không. Tuy nhiên, trong một số trường hợp, các outlier có thể làm méo mó phân phối trọng số, có thể dẫn đến phạm vi lượng tử hóa không chính xác. Dựa trên quan sát của chúng tôi và xem xét hiệu quả triển khai, chúng tôi chọn sử dụng lượng tử hóa đối xứng quanh số không.

3.2 Độ chi tiết của lượng tử hóa
Xem xét các lựa chọn thiết kế được thực hiện trước đó trong phần này, độ chi tiết của lượng tử hóa nổi lên như thành phần quan trọng nhất của thuật toán lượng tử hóa. Vì mục đích tính toán hiệu quả và giảm tiêu thụ bộ nhớ, thông thường có 1 thang lượng tử hóa cho mỗi tensor hoặc 1 thang lượng tử hóa cho mỗi cột trong tensor. Tuy nhiên, để duy trì xấp xỉ gần với các giá trị số gốc với các giá trị lượng tử hóa, mong muốn có các nhóm tham số nhỏ hơn chia sẻ thang. Điều này cần thiết vì các outlier trong phân phối có khả năng làm lệch dữ liệu đáng kể, dẫn đến giảm độ chính xác lượng tử hóa, đặc biệt cho các giá trị số nhỏ hơn.

3.2.1 Sự sụp đổ thảm khốc của hiệu suất mô hình
Trong suốt quan sát của chúng tôi, chúng tôi đã ghi nhận sự suy giảm đáng kể về hiệu suất khi sử dụng lượng tử hóa theo ma trận so với lượng tử hóa theo cột trên các lớp khác nhau, như được chứng minh trong Phụ lục B. Do đó, lượng tử hóa theo cột đóng vai trò là đường cơ sở cho các thí nghiệm của chúng tôi. Tuy nhiên, ngay cả với lượng tử hóa theo cột, chúng tôi đã gặp phải các trường hợp sụp đổ thảm khốc trong hiệu suất LLM, đặc biệt khi các outlier nhất định tồn tại trong trọng số mô hình. Hình 3a mô tả mối quan hệ giữa Sai số Bình phương Trung bình (MSE) của các giá trị lượng tử hóa và điểm BLEU dịch khi chúng tôi thay đổi kích thước nhóm trong mô hình OPT 30B. Trong khi tăng độ chi tiết dẫn đến sự tăng dần trong giá trị MSE, mô hình nhanh chóng mất khả năng về điểm BLEU tác vụ vượt quá một điểm nhất định. Do đó, việc xác định độ chi tiết tối ưu cho mỗi ma trận để bảo toàn hiệu suất tác vụ trong khi tối đa hóa kích thước của các nhóm tham số chia sẻ thang là rất quan trọng.

3.3 Lượng tử hóa mịn thích ứng
Sau khi điều tra thêm về sự thất bại thảm khốc của một mô hình lượng tử hóa, chúng tôi đã phát hiện ra rằng sự thất bại có thể được khắc phục bằng cách điều chỉnh độ chi tiết của bốn ma trận cụ thể trong số 288 ma trận lượng tử hóa. Chỉ cần tăng độ chi tiết của bốn ma trận này lên một hệ số hai đã cho phép khôi phục hơn 94% độ chính xác bị mất. Dựa trên quan sát này, chúng tôi đã phát triển một phương pháp dựa trên heuristic đơn giản để gán độ chi tiết khác nhau cho các ma trận trọng số mô hình khác nhau.

Trong quá trình lượng tử hóa một ma trận, chúng tôi bắt đầu từ lượng tử hóa theo cột và tính toán phạm vi của các giá trị phải được lượng tử hóa. Sau đó chúng tôi giảm một nửa kích thước nhóm lượng tử hóa và tính toán phạm vi của mỗi nhóm. Nếu đối với bất kỳ nhóm nào, new_range/old_range > α chúng tôi giảm một nửa kích thước nhóm lượng tử hóa một lần nữa. Chúng tôi lặp lại quá trình này cho đến khi sự khác biệt phạm vi lượng tử hóa giữa hai độ chi tiết trở nên nhỏ hơn α. Hình 3b minh họa tác động của kích thước nhóm thích ứng lên điểm BLEU và kích thước mô hình tính bằng gigabyte (GB). Với phương pháp lượng tử hóa mịn thích ứng, chỉ có sự khác biệt nhỏ 0,1% trong điểm BLEU, trong khi kích thước mô hình được giảm xuống chỉ 26% kích thước mô hình FP16 gốc.

4 Thí nghiệm

4.1 Thiết lập thí nghiệm
Các thí nghiệm độ trễ và thông lượng của chúng tôi được tiến hành sử dụng GPU NVIDIA A100 SXM4 bên trong container Docker chạy Ubuntu 20.04 và CUDA 11.8. Tất cả mã được biên dịch sử dụng nvcc 11.8.89 và gcc/g++ 9.3. Để thực hiện các thí nghiệm, chúng tôi sử dụng phiên bản sửa đổi của FasterTransformer2 v5.3. Các kernel lượng tử hóa chỉ trọng số cho lượng tử hóa theo cột đã là mã nguồn mở.

Tác vụ và bộ dữ liệu. Đối với các mô hình dày đặc, chúng tôi sử dụng các tác vụ ngôn ngữ mã nguồn mở khác nhau, bao gồm LAMBADA, HellaSwag, PiQA, WinoGrande, OpenBookQA, RTE, COPA từ lm-evaluation harness (Gao et al., 2021), cũng như tác vụ dịch máy WMT (WMT16 Đức và Anh)3. Đối với các mô hình MoE, chúng tôi sử dụng tác vụ dịch máy đa ngôn ngữ bao gồm 10 hướng dịch ngôn ngữ từ và sang tiếng Anh bao gồm Đức (de), Pháp (fr), Ý (it), Tây Ban Nha (es), Hà Lan (nl), và Anh (en). Chúng tôi sử dụng từ vựng con 128.000 từ, được xây dựng với thư viện sentencepiece4. Số lượng câu huấn luyện được bao gồm trong Phụ lục E. Để đo độ chính xác của các mô hình, chúng tôi sử dụng sacrebleu5 trên đầu ra đã được bỏ token.

Kiến trúc mô hình dày đặc. Đối với các thí nghiệm mô hình dày đặc, chúng tôi sử dụng các mô hình ngôn ngữ lớn mã nguồn mở khác nhau chia sẻ kiến trúc tương tự, bao gồm decoder-only với nhiều lớp transformer. Để đánh giá độ chính xác của các mô hình này, chúng tôi bao gồm GPT-2-XL (1.5B) (Radford et al., 2019), OPT (13B và 30B) (Zhang et al., 2022), và OPT-IML (Max 30B và Max 175B) (Iyer et al., 2022). Số lượng tham số mô hình từ 1,5 tỷ đến 175 tỷ. Số lượng lớp và kích thước ẩn chi tiết có thể được tìm thấy trong các bài báo gốc.

Kiến trúc mô hình MoE. Đối với các thí nghiệm mô hình MoE của chúng tôi, chúng tôi sử dụng các mô hình MoE nội bộ đã được huấn luyện trước (5.3B) với một vài sửa đổi cho kiến trúc mô hình transformer (Vaswani et al., 2017). Những sửa đổi này bao gồm: (i) một encoder sâu gồm 24 lớp transformer và một decoder nông gồm 12 lớp transformer, (ii) áp dụng Transformer with Untied Positional Encoding (TUPE) được đề xuất trong Ke et al. (2021) thay vì embedding vị trí sinusoidal thông thường, và (iii) triển khai chuẩn hóa tiền lớp từ Xiong et al. (2020). Đối với các mô hình MoE, chúng tôi sử dụng gating học top-1 từ Fedus et al. (2021) và một lớp MoE với 32 expert ở mỗi lớp khác, cụ thể là các lớp có số chẵn, như được sử dụng trong Lepikhin et al. (2020); Fedus et al. (2021); Kim et al. (2021). Ngoài ra, chúng tôi áp dụng jittering noise, balancing loss (tỷ lệ 0,01) (Lepikhin et al., 2020; Fedus et al., 2021) để phân phối sử dụng expert đồng đều hơn và gating dropout (0,2) (Liu et al., 2022) để ngăn overfitting và cải thiện regularization.

Triển khai kernel GPU. Chúng tôi sử dụng các triển khai kernel được phát triển bởi Kim et al. (2022), dựa vào CUTLASS để tạo các kernel hiệu quả cho phép giải lượng tử hóa và nhân ma trận kết hợp. Các kernel này có thể xử lý kích hoạt FP16 hoặc BF16, một vector thang cùng loại dữ liệu với kích hoạt, và trọng số int8 hoặc int4. Các kernel giải lượng tử hóa trọng số để khớp với loại dữ liệu của kích hoạt và thực hiện phép toán tensor core dấu phẩy động. Đầu ra cuối cùng của kernel cũng cùng loại dữ liệu với kích hoạt đầu vào. Các kernel này có sẵn dưới dạng mã nguồn mở trong FasterTransformer. Để hỗ trợ nhiều hệ số tỷ lệ cho mỗi cột, chúng tôi mở rộng các kernel này để xử lý ma trận thang, cho phép chúng tôi triển khai kernel lượng tử hóa khối int4. Chúng tôi đặt kích thước khối là 64 cho tất cả phân tích hiệu suất dưới đây, vì nó khớp với kích thước K tile của kernel gemm + dequantize kết hợp của chúng tôi.

Trong các trường hợp bị ràng buộc tính toán như encoder hoặc giai đoạn tạo ngữ cảnh của GPT, việc chuyển đổi từ số nguyên sang float làm tắc nghẽn kernel của chúng tôi, thay vì phép toán tensor core. Do đó, GEMM lượng tử hóa chỉ trọng số của chúng tôi chậm hơn GEMM FP16xFP16 tương đương trong các trường hợp bị ràng buộc tính toán nhưng cung cấp tăng tốc đáng kể trong các trường hợp bị ràng buộc bộ nhớ như thấy trong Hình 4. Chúng tôi lập luận rằng kernel này hữu ích vì:

1. Các mô hình ngôn ngữ lớn (LLMs) thường dành nhiều thời gian hơn trong giai đoạn giải mã bị ràng buộc bộ nhớ hơn trong giai đoạn tạo ngữ cảnh bị ràng buộc tính toán, đặc biệt khi độ dài chuỗi đầu ra dài.
2. LLM thường được phục vụ với kích thước batch nhỏ trong hầu hết các trường hợp thực tế, điều này gây áp lực đáng kể lên hệ thống bộ nhớ trong phép nhân ma trận vì trọng số cần được đọc từ HBM của GPU. Tuy nhiên, kernel của chúng tôi sử dụng nén int4, giảm số byte cần tải trọng số lên đến 4X. Chi phí tải thang nhỏ, ngay cả cho lượng tử hóa khối với kích thước khối 64 như thấy trong Hình 4.

Phương pháp lượng tử hóa. Tất cả thí nghiệm lượng tử hóa có một hệ số tỷ lệ cho mỗi cột của ma trận trọng số, trừ khi kích thước khối B được chỉ định. Trong trường hợp đó, mỗi khối liên tiếp gồm B phần tử trong một cột nhất định có hệ số tỷ lệ riêng. Điều này có nghĩa là chúng tôi có nhiều hệ số tỷ lệ cho mỗi cột.

4.2 Kết quả hiệu suất mô hình dày đặc

4.2.1 Độ chính xác
Bảng 1 trình bày tác động của lượng tử hóa lên các tác vụ ngôn ngữ tự nhiên khác nhau sử dụng các mô hình khác nhau. Kết quả cho thấy rằng, nói chung, lượng tử hóa chỉ trọng số 8-bit không ảnh hưởng đáng kể đến độ chính xác so với fp16. Điều này được quan sát trên các tác vụ ngôn ngữ khác nhau, cho thấy các mô hình tạo ra đầu ra tương tự. Tuy nhiên, lượng tử hóa 4-bit với độ chi tiết theo cột dẫn đến một số suy giảm độ chính xác do outlier trong phân phối trọng số, như được thảo luận trong Phần 3.2. Để khôi phục độ chính xác, chúng tôi áp dụng chiến lược lượng tử hóa theo nhóm, cho thấy độ chính xác tương tự với fp16 gốc.

--- TRANG 4 ---
Chúng tôi cũng cho thấy các thí nghiệm tương tự cho OPT-IML cho dịch máy. Bảng 5 cho thấy các số độ chính xác với lượng tử hóa bit khác nhau trên các mô hình OPT-IML 30B và 175B. Với phương pháp lượng tử hóa theo nhóm, các mô hình có thể bảo toàn độ chính xác trong khi lượng tử hóa xuống 4-bit và 3-bit cho một số phần.

4.2.2 Microbenchmarks
Để hiểu cách lượng tử hóa chỉ trọng số của chúng tôi tăng tốc phép nhân ma trận, chúng tôi thu thập microbenchmark từ OPT-13B và OPT-30b và trình bày kết quả trong Hình 4. Chúng tôi thấy rằng phép nhân ma trận có thể được tăng tốc lên đến 2,5X cho các mô hình đó khi số lượng token trong kích hoạt nhỏ. Điều này thường xảy ra cho phần tự hồi quy của LLM có xu hướng chiếm ưu thế trong thời gian chạy tổng thể.

4.2.3 Benchmark End to End
Chúng tôi xây dựng Bảng 3 làm tham chiếu để tính toán thời gian end to end cho các độ dài đầu vào và đầu ra khác nhau cho OPT-175B trên 8, 4 và 2 GPU. Bảng của chúng tôi cho thấy giai đoạn ngữ cảnh chậm lại chủ yếu do chạy trên ít GPU hơn. Ngoài ra, kernel lượng tử hóa chỉ trọng số của chúng tôi có một số chậm lại cho các trường hợp bị ràng buộc tính toán. Tuy nhiên, chúng tôi cho thấy rằng thời gian mỗi bước decoder thường trong vòng 20% của FP16 mặc dù sử dụng ít hơn 2X hoặc 4x GPU. Độ trễ mỗi token không tỷ lệ với số lượng GPU vì ít GPU hơn cần giao tiếp và kernel của chúng tôi cung cấp tăng tốc đáng kể (như thấy trong Bảng 4) trong giai đoạn decoder.

Bảng 4 cho thấy thời gian end to end (được xây dựng từ Bảng 3) và tăng thông lượng liên quan. Để tính toán tăng thông lượng, chúng tôi giả định mô hình FP16 gốc được chia trên 8-GPU trong một node duy nhất và cùng node đó được sử dụng để phục vụ các mô hình INT8 hoặc INT4. Chúng tôi đo thông lượng mỗi node bằng cách giả định rằng mô hình được nhân bản hai lần trên node cho INT8 và 4 lần cho INT4 (64) và các yêu cầu được phục vụ cho các instance mô hình độc lập đồng thời. Chúng tôi nhấn mạnh rằng kỹ thuật nén của chúng tôi cho phép phục vụ 4 instance của OPT-175B trên một node A100 duy nhất với 8 GPU.

4.3 Kết quả hiệu suất mô hình MoE
Chúng tôi đánh giá hiệu suất của phương pháp lượng tử hóa chỉ trọng số trên mô hình MoE và báo cáo kết quả trong Bảng 6. Chúng tôi điều tra tác động của các độ chính xác lượng tử hóa khác nhau, từ 8-bit đến 3-bit. Do tính mạnh mẽ của các lớp MoE FFN, độ chính xác của mô hình được bảo toàn khá tốt ngay cả với độ chính xác 3-bit và 4-bit, so với độ chính xác fp16 gốc.

Hình 5 cho thấy cải thiện tốc độ end-to-end với các kích thước batch khác nhau với lượng tử hóa 8-bit và 4-bit.

5 Kết luận và Hạn chế
Bài báo này trình bày một phương pháp tăng tốc các mô hình ngôn ngữ lớn thông qua việc sử dụng lượng tử hóa bit thấp. Kỹ thuật lượng tử hóa chỉ trọng số được đề xuất thể hiện kết quả đầy hứa hẹn trong việc nén các mô hình rất lớn với lên đến 175 tỷ tham số, trong khi vẫn duy trì độ chính xác. Để giải quyết vấn đề outlier ảnh hưởng đến phân phối trọng số lượng tử hóa, lượng tử hóa mịn được sử dụng.

Mặc dù có điểm mạnh, nghiên cứu có một vài hạn chế. Đầu tiên, kernel GPU được tối ưu hóa chỉ được triển khai cho kích thước nhóm 64. Tuy nhiên, chúng tôi dự định mở rộng hỗ trợ cho bất kỳ kích thước nhóm lũy thừa 2 nào lớn hơn 16. Thứ hai, đánh giá hiệu suất được tiến hành chỉ trên GPU A100, vì vậy cải thiện tốc độ có thể khác nhau trên các kiến trúc GPU khác. Cuối cùng, phương pháp đề xuất không tận dụng các hướng dẫn số nguyên ngay cả khi chúng có sẵn. Những hạn chế này gợi ý các hướng tiềm năng cho nghiên cứu tương lai.

Một hướng đặc biệt đầy hứa hẹn cho công việc tương lai liên quan đến việc khám phá độ chính xác và hiệu quả của việc sử dụng kích hoạt int8 và trọng số int4 với thang số nguyên cho lượng tử hóa mịn. Phương pháp này có khả năng nâng cao hơn nữa hiệu quả của các mô hình.

--- TRANG 5 ---
--- TRANG 6 ---
--- TRANG 7 ---
--- TRANG 8 ---
--- TRANG 9 ---
--- TRANG 10 ---
--- TRANG 11 ---
Tài liệu tham khảo
Alham Fikri Aji và Kenneth Heafield. 2020. Nén các mô hình dịch máy thần kinh với độ chính xác 4-bit. Trong NGT.

Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam Sheiffer, et al. 2021. Fairscale: Một thư viện pytorch mô-đun mục đích chung cho huấn luyện hiệu suất cao và quy mô lớn.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Các mô hình ngôn ngữ là những người học few-shot. Advances in neural information processing systems, 33:1877–1901.

Yoni Choukroun, Eli Kravchik, và Pavel Kisilev. 2019. Lượng tử hóa bit thấp của mạng thần kinh cho suy luận hiệu quả. 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 3009–3018.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Mở rộng mô hình hóa ngôn ngữ với pathways. arXiv preprint arXiv:2204.02311.

Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. 2022. Llm.int8(): Phép nhân ma trận 8-bit cho transformer ở quy mô. ArXiv, abs/2208.07339.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Huấn luyện trước transformer hai chiều sâu để hiểu ngôn ngữ. arXiv preprint arXiv:1810.04805.

Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Hervé Jégou, và Armand Joulin. 2021. Huấn luyện với nhiễu lượng tử hóa cho nén mô hình cực đoan. ArXiv, abs/2004.07320.

William Fedus, Barret Zoph, và Noam Shazeer. 2021. Switch transformers: Mở rộng đến các mô hình trillion tham số với sparsity đơn giản và hiệu quả. arXiv preprint arXiv:2101.03961.

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. 2022. Gptq: Lượng tử hóa sau huấn luyện chính xác cho transformer sinh được huấn luyện trước. arXiv preprint arXiv:2210.17323.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. 2021. Một framework cho đánh giá mô hình ngôn ngữ few-shot.

Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, và Kurt Keutzer. 2022. Một khảo sát về các phương pháp lượng tử hóa cho suy luận mạng thần kinh hiệu quả. ArXiv, abs/2103.13630.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Huấn luyện các mô hình ngôn ngữ lớn tối ưu tính toán. arXiv preprint arXiv:2203.15556.

Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Mở rộng meta learning hướng dẫn mô hình ngôn ngữ qua lăng kính tổng quát hóa. arXiv preprint arXiv:2212.12017.

Guolin Ke, Di He, và Tie-Yan Liu. 2021. Suy nghĩ lại về mã hóa vị trí trong huấn luyện trước ngôn ngữ. ArXiv, abs/2006.15595.

Young Jin Kim và Hany Hassan Awadalla. 2020. Fastformers: Các mô hình transformer hiệu quả cao cho hiểu ngôn ngữ tự nhiên. arXiv preprint arXiv:2010.13382.

Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, và Hany Hassan Awadalla. 2021. Huấn luyện moe có thể mở rộng và hiệu quả cho các mô hình đa tác vụ đa ngôn ngữ. arXiv preprint arXiv:2109.10465.

--- TRANG 12 ---
Young Jin Kim, Rawn Henry, Raffy Fahim, và Hany Hassan Awadalla. 2022. Ai nói voi không thể chạy: Đưa các mô hình moe quy mô lớn vào sản xuất quy mô cloud. arXiv preprint arXiv:2211.10017.

Young Jin Kim, Marcin Junczys-Dowmunt, Hany Hassan, Alham Fikri Aji, Kenneth Heafield, Roman Grundkiewicz, và Nikolay Bogoychev. 2019. Từ nghiên cứu đến sản xuất và ngược lại: Dịch máy thần kinh nhanh đến mức buồn cười. Trong Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 280–288.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. 2020. Gshard: Mở rộng các mô hình khổng lồ với tính toán có điều kiện và sharding tự động. arXiv preprint arXiv:2006.16668.

Rui Liu, Young Jin Kim, Alexandre Muzio, và Hany Hassan. 2022. Gating dropout: Regularization hiệu quả giao tiếp cho transformer được kích hoạt thưa thớt. Trong International Conference on Machine Learning, pages 13782–13792. PMLR.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: Một phương pháp huấn luyện trước bert được tối ưu hóa mạnh mẽ. arXiv preprint arXiv:1907.11692.

Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, và Dongsoo Lee. 2022. nuqmm: Matmul lượng tử hóa cho suy luận hiệu quả của các mô hình ngôn ngữ sinh quy mô lớn. arXiv preprint arXiv:2206.09557.

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Cải thiện hiểu biết ngôn ngữ bằng huấn luyện trước sinh.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Các mô hình ngôn ngữ là những người học đa tác vụ không giám sát. OpenAI blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Khám phá giới hạn của transfer learning với một transformer văn bản đến văn bản thống nhất. J. Mach. Learn. Res., 21(140):1–67.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. 2020. Deepspeed: Tối ưu hóa hệ thống cho phép huấn luyện các mô hình deep learning với hơn 100 tỷ tham số. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.

Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyang Yang, Minjia Zhang, Dong Li, và Yuxiong He. 2021. Zero-offload: Dân chủ hóa huấn luyện mô hình quy mô tỷ. Trong USENIX Annual Technical Conference.

Andres Rodriguez, Eden Segal, Etay Meiri, Evarist Fomenko, Young Jin Kim, Haihao Shen, và Barukh Ziv. 2018. Suy luận và huấn luyện deep learning với độ chính xác số thấp hơn.

Noam M. Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, và Blake A. Hechtman. 2018. Mesh-tensorflow: Deep learning cho siêu máy tính. ArXiv, abs/1811.02084.

Pierre Stock, Armand Joulin, Rémi Gribonval, Benjamin Graham, và Hervé Jégou. 2019. Và bit giảm xuống: Xem xét lại lượng tử hóa mạng thần kinh. arXiv preprint arXiv:1907.05686.

Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong NIPS.

Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, và Song Han. 2022. Smoothquant: Lượng tử hóa sau huấn luyện chính xác và hiệu quả cho các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2211.10438.

Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, và Tie-Yan Liu. 2020. Về chuẩn hóa lớp trong kiến trúc transformer. Trong ICML.

--- TRANG 13 ---
Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, và Yuxiong He. 2022. Zeroquant: Lượng tử hóa sau huấn luyện hiệu quả và giá cả phải chăng cho transformer quy mô lớn. ArXiv, abs/2206.01861.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Các mô hình transformer ngôn ngữ được huấn luyện trước mở. arXiv preprint arXiv:2205.01068.

--- TRANG 14 ---
A Công thức phương pháp lượng tử hóa
Lượng tử hóa tuyến tính với tối đa tuyệt đối. Chúng tôi sử dụng lượng tử hóa tuyến tính với tối đa tuyệt đối như phương pháp chính. Cho một ma trận A và b bit, phương pháp này mã hóa A như sau:
sj=2×max(|A:,j|)/(2^b−1)
Q:,j= int(A:,j/sj)

Ở đây, s là hệ số tỷ lệ, có thể được chọn cho mỗi kênh, như thể hiện, hoặc cho toàn bộ tensor. Tại thời điểm suy luận, Q được lượng tử hóa được giải lượng tử hóa trở lại A' với hệ số tỷ lệ s như sau:
A':, j=Q:, j×sj

Lượng tử hóa thang log. Một phương pháp lượng tử hóa khác mà chúng tôi thử nghiệm là lượng tử hóa thang log trong đó 1 bit được giữ cho dấu và (b−1) bit được sử dụng để mã hóa các giá trị thang log. Cho một ma trận A, công thức lượng tử hóa như sau:
P=sign(A)
T=clip(|A|/s,1,2^(1−2^(b−1)))
Q=⌈log2(2^3T)⌉

trong đó s có thể được chọn theo hai cách, hoặc (i) tối đa tuyệt đối hoặc (ii) giá trị tối ưu để giảm thiểu sai số bình phương trung bình (MSE) giữa các giá trị lượng tử hóa và gốc được mô tả trong Aji và Heafield (2020). Chúng tôi sử dụng thuật toán thứ hai mà chúng tôi quan sát độ chính xác tốt hơn với lượng tử hóa. Tại thời điểm suy luận, các giá trị trọng số lượng tử hóa được giải lượng tử hóa dựa trên công thức như sau:
A′=P×s×2^Q

Hình 1 cho thấy so sánh hiệu suất của hai phương pháp lượng tử hóa.

B Lượng tử hóa theo kênh vs theo ma trận
Các hệ số tỷ lệ được tính toán bởi thuật toán lượng tử hóa và được lưu trữ trong các số dấu phẩy động có độ chính xác một nửa (fp16) để giải lượng tử hóa các ma trận. Các hệ số này có thể được chọn ở quy mô kênh hoặc quy mô toàn bộ ma trận. Như thể hiện trong hình 6, lượng tử hóa theo kênh cho điểm số khá cao hơn lượng tử hóa theo tensor đặc biệt cho độ chính xác thấp. Các tham số bổ sung để lưu trữ các hệ số tỷ lệ theo kênh là nhỏ, vì chỉ cần một giá trị cho một kênh và ít hơn 1% tổng tham số trong một ma trận. Do đó, chúng tôi sử dụng lượng tử hóa theo kênh cho tất cả các thí nghiệm lượng tử hóa.

C Lượng tử hóa các lớp khác nhau trong mô hình dày đặc
Để so sánh với các mô hình MoE luân phiên các loại khối khác nhau là khối expert và khối dày đặc, chúng tôi xem xét lượng tử hóa chỉ một nửa các FFN của khối transformer dày đặc, vì chúng tôi lượng tử hóa trọng số expert chỉ trên các mô hình MoE tồn tại chỉ ở mỗi khối khác (số chẵn). Chúng tôi so sánh ba cấu hình khác nhau - (1) chỉ lượng tử hóa FFN của các khối số chẵn, (2) chỉ lượng tử hóa FFN của các khối số lẻ và (3) lượng tử hóa tất cả các lớp FFN. Như có thể thấy trong Hình 7, lượng tử hóa FFN của các khối số chẵn ảnh hưởng đến độ chính xác ít nhất, và lượng tử hóa tất cả các lớp FFN cho kết quả tồi tệ nhất. Dựa trên thí nghiệm này, chúng tôi chỉ lượng tử hóa FFN của các khối transformer số chẵn cho mô hình dày đặc trong tất cả các thí nghiệm và so sánh.

D Độ lệch của ma trận trọng số trong mô hình MoE và dày đặc
Trong phân tích phân phối trọng số mô hình ở Phần 3, chúng tôi quan sát thấy các lớp FFN của mô hình dày đặc có xu hướng có nhiều outlier hơn các lớp FFN expert của MoE. Chúng tôi đo độ lệch của phân phối trọng số của những cái này trong Bảng 7.

E Tóm tắt bộ dữ liệu dịch máy
Bảng 8 cho thấy số lượng câu song song được sử dụng để huấn luyện các mô hình dày đặc và MoE. Tất cả các ngôn ngữ có ít nhất 300 triệu câu và sự khác biệt về số lượng giữa các ngôn ngữ ít hơn hai lần.

F So sánh tính mạnh mẽ giữa mô hình MoE và dày đặc
Chúng tôi so sánh tính mạnh mẽ chống lại lượng tử hóa bit thấp giữa các mô hình MoE và dày đặc sử dụng lượng tử hóa sau huấn luyện mà không có bất kỳ QAT nào. Đối với mô hình dày đặc, lượng tử hóa với các bit khác nhau được áp dụng cho các lớp FFN số chẵn. Phụ lục C cho thấy đây là lựa chọn lớp tốt nhất cho

--- TRANG 15 ---
mô hình dày đặc. Chúng tôi sử dụng hai bộ dữ liệu khác nhau để xác minh phương pháp lượng tử hóa được đề xuất hoạt động trong các cài đặt mô hình khác nhau.

Hình 8 trình bày thí nghiệm với mô hình được huấn luyện với bộ dữ liệu lớn hơn. Nó cho thấy điểm BLEU trung bình với độ chính xác lượng tử hóa khác nhau cho cả mô hình MoE và dày đặc. Mô hình MoE có thể duy trì độ chính xác trong vòng -0,3 xuống 3-bit và -1,82 cho 2-bit. Mặt khác, mô hình dày đặc chỉ có thể bảo toàn độ chính xác xuống 4-bit, nhưng bắt đầu mất độ chính xác đáng kể hơn 2 điểm BLEU khi xuống 3-bit. Trong trường hợp 2-bit, mô hình dày đặc mất hầu hết khả năng với -42,96 điểm BLEU.

Hình 9 trình bày thí nghiệm với mô hình được huấn luyện với bộ dữ liệu nhỏ hơn. Trong cài đặt này, mỗi expert riêng lẻ nhỏ hơn, nhưng có gấp 4 lần expert hơn trong một lớp MoE. Và, chúng được huấn luyện với bộ dữ liệu nhỏ hơn, vì vậy chúng không có kiến thức tương đương với mô hình trước được huấn luyện trên bộ dữ liệu lớn hơn. Như có thể thấy trong Hình, hiệu suất lượng tử hóa cho thấy mô hình tương tự. Mô hình MoE bảo toàn độ chính xác ngay cả khi được lượng tử hóa xuống 2 hoặc 3 bit. Tuy nhiên, mô hình dày đặc nhanh chóng mất hiệu suất khi được lượng tử hóa xuống thấp hơn 4-bit. Một lần nữa, mô hình MoE mạnh mẽ hơn nhiều đối với lượng tử hóa so với mô hình dày đặc.

F.1 Tính mạnh mẽ của các lớp MoE FFN đối với lượng tử hóa
Đối với các mô hình MoE, chúng tôi cũng tiến hành một loạt thí nghiệm với các bit lượng tử hóa khác nhau. Chúng tôi chia một mô hình MoE thành bốn phần: (i) FFN expert, (ii) các lớp FFN dày đặc, (iii) các lớp self-attention và (iv) các lớp cross-attention.

Hình 10 cho thấy điểm BLEU đánh giá khi các phần khác nhau của mô hình MoE được lượng tử hóa. Quan sát thấy rằng lượng tử hóa các lớp FFN expert xuống 2-bit không ảnh hưởng đáng kể đến chất lượng mô hình tổng thể. Tuy nhiên, lượng tử hóa các phần khác của mô hình thành 2-bit làm tổn hại đáng kể chất lượng đầu ra. Các khối cross-attention và self-attention được lượng tử hóa vẫn có thể duy trì chất lượng với lượng tử hóa 3-bit, nhưng hiệu suất của chúng bị ảnh hưởng với lượng tử hóa 2-bit. Mặt khác, các lớp FFN dày đặc bị ảnh hưởng đáng kể bởi lượng tử hóa bit thấp hơn của 2-bit và 3-bit. Với lượng tử hóa 3-bit, điểm mô hình giảm 23% so với điểm gốc, và lượng tử hóa 2-bit trên các lớp FFN dày đặc cho điểm gần như bằng không. Cùng một nghiên cứu cũng được bao gồm trên một mô hình dày đặc trong Phụ lục C, và một mô hình tương tự với lượng tử hóa 2 và 3 bit được quan sát.

--- TRANG 16 ---
--- TRANG 17 ---
