# 2302.10899.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2302.10899.pdf
# Kích thước tệp: 1508639 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
1
Hỗ trợ Ái lực Đặc trưng trong Chưng cất Tri thức
và Lượng tử hóa Mạng Nơ-ron Sâu trên
Dữ liệu Không có Nhãn
Zhijian Li, Biao Yang, Penghang Yin, Yingyong Qi, và Jack Xin
Tóm tắt —Trong bài báo này, chúng tôi đề xuất một phương pháp chưng cất tri thức (KD) được hỗ trợ bởi ái lực đặc trưng (FA) để cải thiện việc huấn luyện nhận thức lượng tử hóa của mạng nơ-ron sâu (DNN). Tổn thất FA trên các bản đồ đặc trưng trung gian của DNN đóng vai trò dạy các bước giữa của một giải pháp cho học sinh thay vì chỉ đưa ra câu trả lời cuối cùng trong KD thông thường nơi tổn thất tác động lên các logit mạng ở mức đầu ra. Kết hợp tổn thất logit và tổn thất FA, chúng tôi phát hiện rằng mạng học sinh được lượng tử hóa nhận được sự giám sát mạnh hơn so với từ dữ liệu nhãn chân lý cơ sở. FAQD kết quả có khả năng nén mô hình trên dữ liệu không có nhãn, mang lại lợi ích thực tế ngay lập tức vì các mô hình giáo viên được huấn luyện trước có sẵn và dữ liệu không có nhãn rất phong phú. Ngược lại, việc gắn nhãn dữ liệu thường tốn công sức và đắt đỏ. Cuối cùng, chúng tôi đề xuất một tổn thất ái lực đặc trưng nhanh (FFA) xấp xỉ chính xác tổn thất FA với độ phức tạp tính toán bậc thấp hơn, giúp tăng tốc huấn luyện cho đầu vào hình ảnh độ phân giải cao.
Thuật ngữ Chỉ mục —Lượng tử hóa, Mạng Nơ-ron Tích chập, Chưng cất Tri thức, Nén Mô hình, Phân loại Hình ảnh

I. GIỚI THIỆU
Lượng tử hóa là một trong những phương pháp phổ biến nhất để nén mạng nơ-ron sâu, bằng cách chiếu trọng số mạng và hàm kích hoạt xuống độ chính xác thấp hơn để tăng tốc tính toán và giảm tiêu thụ bộ nhớ. Tuy nhiên, có sự mất mát độ chính xác không thể tránh khỏi trong chế độ bit thấp. Một cách để giảm thiểu vấn đề này là thông qua chưng cất tri thức (KD [10]). Trong bài báo này, chúng tôi nghiên cứu một KD được hỗ trợ ái lực đặc trưng để mạng học sinh và giáo viên không chỉ cố gắng khớp logit của chúng ở mức đầu ra mà còn khớp các bản đồ đặc trưng ở các giai đoạn trung gian. Điều này tương tự như dạy học sinh thông qua các bước trung gian của một giải pháp thay vì chỉ cho thấy câu trả lời cuối cùng (như trong KD thông thường [10]). Phương pháp của chúng tôi không dựa vào nhãn chân lý cơ sở trong khi tăng cường việc học mạng học sinh và thu hẹp khoảng cách giữa các mô hình độ chính xác đầy đủ và thấp.

A. Lượng tử hóa Trọng số của Mạng Nơ-ron
Huấn luyện nhận thức lượng tử hóa (QAT) tìm kiếm trọng số mô hình tối ưu trong huấn luyện. Cho một mục tiêu L, sơ đồ QAT cổ điển ([6], [21]) được công thức hóa là
(
wt+1=wt− ∇ uL(ut),
ut+1=Quant( wt+1),(1)
trong đó Quant là phép chiếu lên không gian lượng tử hóa độ chính xác thấp.
Yin et al. [28] đề xuất BinaryRelax, một dạng thư giãn của QAT, thay thế cập nhật thứ hai của (1) bằng
ut+1=wt+1+λt+1Quant( wt+1)
1 +λt+1,
λt+1=ηλt với η >1.(2)
Darkhorn et al. [7] cải thiện thêm (2) bằng cách thiết kế một sơ đồ tăng trưởng có thể học phức tạp hơn cho λt và thêm một tham số có thể học vào Quant( ·). Polino et al. [19] đề xuất chưng cất lượng tử hóa (QD), một khung QAT tận dụng chưng cất tri thức cho lượng tử hóa. Dưới QD, mô hình lượng tử hóa nhận giám sát từ cả nhãn chân lý cơ sở (GT) và một giáo viên được huấn luyện ở độ chính xác float (FP). Hàm mục tiêu có dạng tổng quát ( α∈(0,1)):
LQD=αLKD+ (1−α)LGT (3)
trong đó LKD là tổn thất phân kỳ Kullback–Leibler (KL), và LGT là tổn thất log likelihood âm (NLL). Để so sánh các phương pháp khác nhau một cách công bằng, chúng tôi giới thiệu hai thuật ngữ kỹ thuật: lượng tử hóa đầu cuối đến đầu cuối và lượng tử hóa tinh chỉnh. Lượng tử hóa đầu cuối đến đầu cuối là huấn luyện một mô hình lượng tử hóa từ đầu, và lượng tử hóa tinh chỉnh là huấn luyện một mô hình lượng tử hóa từ một mô hình độ chính xác float được huấn luyện trước (FP). Với cùng một phương pháp, phương pháp sau thường cho kết quả tốt hơn phương pháp trước.
Li et al. [15] đề xuất một lượng tử hóa hỗn hợp (a.k.a. BRECQ) lấy một mô hình được huấn luyện trước và huấn luyện lại một phần mô hình trên một tập con nhỏ dữ liệu.

B. Lượng tử hóa Kích hoạt
Ngoài lượng tử hóa trọng số, việc suy luận của mạng nơ-ron có thể được tăng tốc thêm thông qua lượng tử hóa kích hoạt. Cho một độ phân giải α >0, một hàm kích hoạt ReLU lượng tử hóa có độ rộng bit b∈N là σ=σ(x, α):
σ=

0 x <0
kα (k−1)α≤x < kα, 1≤k≤2b−1
(2b−1)α x≥(2b−1)α
(4)
trong đó tham số độ phân giải α được học từ dữ liệu. Một biểu đồ của ReLU lượng tử hóa 2-bit được hiển thị trong Hình 1. Tuy nhiên, hàm kích hoạt lượng tử hóa như vậy dẫn đến gradient biến mất trong huấn luyện, làm cho lan truyền ngược tiêu chuẩn không áp dụng được. Thực vậy, rõ ràng là ∂σ/∂x= 0 hầu hết mọi nơi. Bengio et al. [2] đề xuất sử dụng một ước lượng thẳng (STE) trong bước lan truyền ngược để xử lý vấn đề gradient bằng không. Ý tưởng là đơn giản thay thế ∂σ/∂x biến mất bằng một đạo hàm không tầm thường ∂˜σ/∂x của một hàm thay thế ˜σ(x, α). Các nghiên cứu lý thuyết về STE và các vấn đề hội tụ so với tái diễn của các thuật toán huấn luyện đã được tiến hành trong ([17], [27]). Trong nhiều lựa chọn STE khác nhau, một STE được sử dụng rộng rãi là x-đạo hàm của cái gọi là clipped ReLU [3] ˜α(x, α) = min {max{x,0},(2b−1)α}, cụ thể,
∂˜σ/∂x=(
1 0 < x < (2b−1)α
0 khác.
Ngoài ra, một vài proxy của ∂σ/∂α đã được đề xuất ([4], [29]). Trong công trình này, chúng tôi theo [29] và sử dụng proxy ba giá trị:
∂σ/∂α≈

0 x≤0
2b−1 0< x < (2b−1)α
2b−1 x≥(2b−1)α.(5)

C. Chưng cất Tri thức
Một số công trình đã đề xuất áp đặt sự gần gũi của các phân phối xác suất giữa mạng giáo viên và học sinh, ví dụ sự tương đồng giữa các bản đồ đặc trưng. Một ma trận thủ tục dòng giải pháp (FSP) trong [26] đo lường việc trao đổi thông tin giữa hai lớp của một mô hình cho trước. Sau đó tổn thất l2 điều hòa khoảng cách giữa các ma trận FSP của giáo viên và học sinh trong chưng cất tri thức. Một tổn thất biến đổi chú ý (AT) [30] đo lường trực tiếp khoảng cách của các bản đồ đặc trưng được xuất ra bởi giáo viên và học sinh, điều này tăng cường việc học của học sinh từ giáo viên. Tương tự, tổn thất ái lực đặc trưng (FA) [24] đo lường khoảng cách của hai bản đồ đặc trưng. Trong một khung học đối ngẫu cho phân đoạn ngữ nghĩa [24], tổn thất FA được áp dụng trên các bản đồ đặc trưng đầu ra của một bộ giải mã phân đoạn và một bộ giải mã độ phân giải cao. Trong [25], tổn thất FA được áp dụng trên các đường dẫn đa độ phân giải trong chưng cất tri thức của các mô hình phân đoạn ngữ nghĩa. Nó cải thiện Độ chính xác Trung bình trung bình của mô hình học sinh nhẹ. Cho hai bản đồ đặc trưng có cùng chiều cao và chiều rộng (nội suy nếu khác), FS∈RC1×H×W và FT∈RC2×H×W, trước tiên chúng tôi chuẩn hóa bản đồ đặc trưng dọc theo chiều kênh. Cho một pixel của bản đồ đặc trưng Fi∈RC, chúng tôi xây dựng một ma trận ái lực S∈RWH×WH là:
Sij=∥Fi−Fj∥θ:= cos θij=⟨Fi,Fj⟩/(||Fi||||Fj||).
trong đó θij đo lường góc giữa Fi và Fj. Do đó, tổn thất FA đo lường sự tương đồng của khoảng cách góc theo cặp giữa các pixel của hai bản đồ đặc trưng, có thể được công thức hóa là
Lfa(FS,FT) =1/(W2H2)||ST−SS||2².(6)

D. Đóng góp
Trong bài báo này, những đóng góp chính của chúng tôi là:
1) Chúng tôi phát hiện rằng sử dụng lỗi bình phương trung bình (MSE) cho hiệu suất tốt hơn KL trên QAT, đây là một cải thiện đáng kể của QD ([19]).
2) Chúng tôi liên tục cải thiện độ chính xác của các mạng học sinh lượng tử hóa khác nhau bằng cách áp đặt tổn thất FA trên bản đồ đặc trưng của mỗi khối tích chập. Chúng tôi cũng tiết lộ nền tảng lý thuyết của tổn thất ái lực đặc trưng về mặt bổ đề Johnson-Lindenstrass nổi tiếng cho các nhúng chiều thấp.
3) Chúng tôi đạt được độ chính xác lượng tử hóa tiên tiến trên CIFAR-10, CIFAR-100, và Tiny ImageNet. Khung FAQD của chúng tôi có thể huấn luyện một mạng học sinh lượng tử hóa trên dữ liệu không có nhãn lên đến hoặc vượt quá độ chính xác của đối tác độ chính xác đầy đủ của nó.
4) Chúng tôi đề xuất một tổn thất FA Nhanh (FFA) ngẫu nhiên để tăng tốc tính toán của tổn thất huấn luyện, và chứng minh sự hội tụ và giới hạn lỗi của nó.

E. Tổ chức
Bài báo này được tổ chức như sau: Trong Phần II, chúng tôi giới thiệu mục tiêu chính của FAQD. Đặc biệt, chúng tôi trình bày tổn thất ái lực đặc trưng và xem xét so sánh giữa tổn thất MSE và KL. Trong Phần III, chúng tôi xác minh bằng số rằng FAQD vượt trội hơn các phương pháp cơ sở. Trong Phần IV, chúng tôi giới thiệu tổn thất ái lực đặc trưng nhanh và xác minh việc tăng tốc của nó đối với FAQD.

II. CHƯNG CẤT VÀ LƯỢNG TỬ HÓA HỖ TRỢ ÁI LỰC ĐẶC TRƯNG

A. Tổn thất Ái lực Đặc trưng
Trong thiết lập lượng tử hóa, việc yêu cầu FS gần với FT là không hợp lý, vì chúng thường ở trong các không gian khác nhau (FS∈Q trong lượng tử hóa đầy đủ) và có các chiều khác nhau. Tuy nhiên, FS có thể được xem như một nén của FT trong chiều, và việc bảo toàn thông tin dưới sự nén như vậy đã được nghiên cứu trong cảm biến nén. Các nhà nghiên cứu ([20], [22]) đã đề xuất nén nhúng đồ thị xuống chiều thấp hơn để có thể tính toán tích chập đồ thị một cách hiệu quả. Trong vấn đề phân cụm K-means, một số phương pháp ([1], [18]) đã được thiết kế để chiếu dữ liệu vào một không gian chiều thấp sao cho
||Proj(x)−Proj(y)|| ≈ || x−y||,∀(x,y), (7)

--- TRANG 3 ---
3
và vì vậy khoảng cách theo cặp từ các điểm dữ liệu đến tâm có thể được tính toán với chi phí thấp hơn.
Xem các bản đồ đặc trưng của mô hình học sinh như một nén của bản đồ đặc trưng của giáo viên, chúng tôi áp đặt một tính chất tương tự về mặt khoảng cách góc theo cặp:
||FS_i−FS_j||θ≈ ||FT_i−FT_j||θ,∀(i, j)
điều này được thực hiện bằng cách tối thiểu hóa tổn thất ái lực đặc trưng. Mặt khác, một bổ đề giống Johnson–Lindenstrauss (JL [11]) có thể đảm bảo rằng chúng ta có ma trận ái lực đặc trưng của học sinh gần với của giáo viên, với điều kiện số kênh của mạng học sinh không quá nhỏ. Ngược lại, bổ đề JL cổ điển phát biểu rằng một tập hợp các điểm trong không gian chiều cao có thể được nhúng vào một không gian có chiều thấp hơn nhiều theo cách mà khoảng cách Euclide giữa các điểm được bảo toàn gần như nguyên vẹn. Để điều chỉnh nó cho ứng dụng của chúng tôi, chúng tôi chứng minh bổ đề giống JL sau trong trường hợp khoảng cách góc:

Định lý 2.1 (Bổ đề Johnson–Lindenstrauss, Trường hợp Góc):
Cho bất kỳ ϵ∈(0,1), một ma trận nhúng F∈Rn×d, với k∈(16ϵ−2lnn, d), tồn tại một ánh xạ tuyến tính T(F)∈Rn×k sao cho
(1−ϵ)||Fi−Fj||θ≤ ||T(F)i−T(F)j||θ ≤(1 +ϵ)||Fi−Fj||θ,∀1≤i, j≤n (8)
trong đó ||Fi−Fj||θ=⟨Fi,Fj⟩/(∥Fi∥∥Fj∥) là khoảng cách góc.

Do đó có thể giảm chiều nhúng từ d xuống k, trong khi gần như bảo toàn khoảng cách góc theo cặp giữa các điểm. Trong một mạng nơ-ron tích chập, chúng ta có thể xem các bản đồ đặc trưng trung gian như FS∈RHW×C1 và FT∈RHW×C2, và tổn thất ái lực đặc trưng sẽ giúp học sinh học một nhúng đặc trưng nén. Tổn thất FA có thể được đặt một cách linh hoạt giữa giáo viên và học sinh ở các vị trí khác nhau (bộ mã hóa/giải mã, khối dư, v.v.) cho các mô hình khác nhau. Trong việc thực hiện tiêu chuẩn của ResNet, các khối dư có cùng số kênh đầu ra được nhóm thành một lớp tuần tự. Chúng tôi áp dụng tổn thất FA cho các đặc trưng của các lớp như vậy.

LFA=∑L(l=1)Lfa(FT_l,FS_l)
trong đó FT_l và FS_l là các bản đồ đặc trưng của giáo viên và học sinh tương ứng. Ví dụ, họ mạng dư của ResNet20, ResNet56, ResNet110, và ResNet164 có L= 3, trong khi họ ResNet18, ResNet34, và ResNet50 có L= 4.

B. Lựa chọn Hàm Tổn thất
Trong công trình này, chúng tôi đề xuất hai bộ lựa chọn hàm tổn thất cho lượng tử hóa đầu cuối đến đầu cuối và lượng tử hóa được huấn luyện trước, trong đó lượng tử hóa đầu cuối đến đầu cuối đề cập đến việc có một mô hình học sinh chưa được huấn luyện với trọng số được khởi tạo ngẫu nhiên. Chúng tôi điều tra cả hai kịch bản lượng tử hóa và đề xuất hai chiến lược khác nhau cho mỗi kịch bản.

Phân kỳ Kullback–Leibler (KL) là một thước đo của sự tương đồng giữa hai phân phối xác suất. Cho một phân phối chân lý cơ sở P, nó tính toán entropy tương đối của một phân phối Q cho trước từ P:
LKL(P||Q) = ∑(x∈X)P(x) ln(P(x)/Q(x)). (9)

Trong khi KD thường được kết hợp với tổn thất KL ([10], [19]), việc chọn các hàm tổn thất khác không phải là bất thường. Kim et al. [14] cho thấy rằng MSE, trong một số trường hợp nhất định, có thể vượt trội hơn KL trong thiết lập chưng cất tri thức giáo viên-học sinh cổ điển. Tổn thất KL cũng được sử dụng rộng rãi để cân bằng giữa độ chính xác và tính bền vững dưới các cuộc tấn công đối nghịch, có thể được coi là chưng cất tri thức tự thân. Cho một bộ phân loại f, một điểm dữ liệu gốc x và ví dụ đối nghịch x′ của nó, TRADES [31] được công thức hóa là
LTRADES =LCE(f(x),y) +LKL(f(x)||f(x′)).

Li et al. [16] cho thấy LCE(f(x′), y) vượt trội hơn LKL(f(x)||f(x′)) cả về mặt thực nghiệm và lý thuyết.
Được truyền cảm hứng từ các nghiên cứu trên, chúng tôi tiến hành thí nghiệm về các lựa chọn khác nhau của hàm tổn thất. Chúng tôi so sánh KD trên lượng tử hóa từ đầu (đầu cuối đến đầu cuối). Như được hiển thị trong Bảng I, MSE vượt trội hơn KL trong lượng tử hóa.

Học sinh Giáo viên 1-bit 2-bit 4-bit
LKD=tổn thất KL trong (3)
ResNet20 ResNet110 89.06% 90.86% 92.01%
LKD=MSE trong (3)
ResNet20 ResNet110 90.00% 91.01% 92.17%

BẢNG I: So sánh tổn thất KL và tổn thất MSE trên tập dữ liệu CIFAR-10. Tất cả giáo viên đều là các mô hình FP được huấn luyện trước, và tất cả học sinh đều là các mô hình ban đầu (lượng tử hóa đầu cuối đến đầu cuối).

Mặt khác, chúng tôi thấy rằng tổn thất KL hoạt động tốt hơn cho lượng tử hóa tinh chỉnh. Một giải thích có thể là khi huấn luyện từ đầu, số hạng ln(P(x)/Q(x)) lớn. Tuy nhiên, đạo hàm của logarithm nhỏ ở các giá trị lớn, làm cho nó hội tụ chậm hơn và có khả năng tệ hơn. Mặt khác, khi P(x)/Q(x) gần bằng 1, logarithm có độ dốc sắc và hội tụ nhanh.

C. Chưng cất và Lượng tử hóa Hỗ trợ Ái lực Đặc trưng
Được truyền cảm hứng từ các nghiên cứu trước ([13], [15], [19]), chúng tôi đề xuất một chưng cất lượng tử hóa hỗ trợ ái lực đặc trưng (FAQD). Hàm mục tiêu lượng tử hóa đầu cuối đến đầu cuối được công thức hóa là:
L=αLKD+βLFA+γLGT
=αLMSE(fT(x), fS(x))+β∑L(l=1)Lfa(FT_l,FS_l)+γLNLL(fS(x), y).(10)

Trong lượng tử hóa tinh chỉnh, chúng tôi thay thế tổn thất MSE trong (10) bằng tổn thất phân kỳ KL. Trong FAQD, mô hình học sinh học không chỉ các logit cuối cùng của giáo viên mà còn các bản đồ đặc trưng trích xuất trung gian của giáo viên bằng cách sử dụng chuẩn ái lực đặc trưng được tính toán như trong [24].

--- TRANG 4 ---
4

Hình 2: Khung FAQD. Các bản đồ đặc trưng trung gian được giám sát bởi tổn thất FA, và các logit thô bởi tổn thất MSE.

Ngoài (10), chúng tôi cũng đề xuất một mục tiêu không có nhãn không yêu cầu kiến thức về nhãn:
Llabel-free =αLMSE(fT(x), fS(x))+β∑L(l=1)Lfa(FT_l,FS_l).(11)

Mặc dù các mô hình thị giác máy tính được huấn luyện trước có sẵn từ dịch vụ đám mây như AWS và dữ liệu hình ảnh/video được thu thập phong phú, việc gắn nhãn dữ liệu vẫn đắt đỏ và tốn thời gian. Do đó, một khung lượng tử hóa không có nhãn có giá trị đáng kể trong thế giới thực. Trong công trình này, chúng tôi xác minh rằng tổn thất FA có thể cải thiện đáng kể hiệu suất KD. Tổn thất không có nhãn trong Eq. (11) có thể vượt trội hơn các phương pháp cơ sở trong Bảng II cũng như QD có giám sát trước đó trong (3).

III. KẾT QUẢ THỰC NGHIỆM
Trong Bảng II, chúng tôi liệt kê hiệu suất của các phương pháp trước đó được đề cập trong phần giới thiệu. Chúng tôi muốn lưu ý rằng kết quả BRECQ là từ lượng tử hóa theo kênh. Cụ thể, mỗi kênh của một lớp tích chập có bộ chia tỷ lệ float và bản đồ chiếu riêng. Tất cả các kết quả khác trong Bảng II là lượng tử hóa theo lớp.

Tất cả các thí nghiệm được báo cáo ở đây đã được tiến hành trên một máy tính để bàn với card GPU Nvidia RTX6000 8GB tại UC Irvine.

Phương pháp 1-bit 2-bit 4-bit
Mô hình: ResNet20
QAT ([6], [21]) 87.07% 90.26% 91.47%
BinaryRelax [28] 88.64% 90.47% 91.75%
QD [19] 89.06% 90.86% 92.01%
DSQ [8] 90.24% 91.06% 91.92%
BRECQ∗[15] N/A 88.10% 89.01%

BẢNG II: Độ chính xác lượng tử hóa đầu cuối đến đầu cuối của một số phương pháp huấn luyện nhận thức lượng tử hóa hiện có trên tập dữ liệu CIFAR-10. Để tuân thủ công trình gốc, chúng tôi áp dụng lượng tử hóa theo kênh trong BRECQ, được ký hiệu bằng ∗. Tất cả các phương pháp khác đều dưới lượng tử hóa theo lớp.

A. Lượng tử hóa Trọng số
Trong phần này chúng tôi thử nghiệm FAQD trên tập dữ liệu CIFAR-10. Đầu tiên, chúng tôi thí nghiệm trên lượng tử hóa tinh chỉnh. ResNet110 độ chính xác float (FP) dạy ResNet20 và ResNet56. Giáo viên có độ chính xác 93.91%, và hai mô hình được huấn luyện trước có độ chính xác 92.11% và 93.31% tương ứng. Trong khi cả tối ưu hóa SGD và Adam đều hoạt động tốt trên bài toán, chúng tôi thấy tổn thất KL với Adam hơi vượt trội hơn SGD trong kịch bản này.
Mục tiêu là
L=LKL+LFA
cho lượng tử hóa không có nhãn. Khi hiệu chỉnh nhãn chân lý cơ sở, tổn thất cross-entropy LNLL được sử dụng làm tiêu chí giám sát.

Cifar-10
Giáo viên ResNet110: 93.91%
ResNet20 FP được huấn luyện trước: 92.21%
Phương pháp 1-bit 2-bit 4-bit
FAQD không có nhãn 89.97% 91.40% 92.55%
FAQD với Giám sát 90.92% 91.93% 92.74%

Cifar-100
Giáo viên ResNet164: 74.50%
ResNet110 FP được huấn luyện trước:72.96%
Phương pháp 1-bit 2-bit 4-bit
FAQD không có nhãn 73.33% 75.02% 75.78%
FAQD với Giám sát 73.35% 75.24% 76.10%

Tiny ImageNet
Giáo viên ResNet34: 65.60%
ResNet18 FP được huấn luyện trước: 64.23%
Phương pháp 1-bit 2-bit 4-bit
FAQD không có nhãn 64.89% 65.02% 65.69%
FAQD với Giám sát 65.77% 66.49% 66.62%

BẢNG III: Chưng cất tri thức tinh chỉnh cho lượng tử hóa của tất cả các lớp tích chập.

Đối với lượng tử hóa đầu cuối đến đầu cuối, chúng tôi thấy rằng tổn thất MSE hoạt động tốt hơn tổn thất KL. Tối ưu hóa Adam gặp khó khăn để đạt được hiệu suất chấp nhận được trên lượng tử hóa đầu cuối đến đầu cuối (với tổn thất KL hoặc MSE). Chúng tôi thử nghiệm thêm hiệu suất của FAQD trên tập dữ liệu lớn hơn CIFAR-100 nơi một ResNet 164 FP dạy một ResNet110 lượng tử hóa. Chúng tôi báo cáo độ chính xác cho cả giám sát không có nhãn và có nhãn. Chúng tôi đánh giá FAQD trên cả lượng tử hóa tinh chỉnh và lượng tử hóa đầu cuối đến đầu cuối. Trong thí nghiệm CIFAR-100, giáo viên ResNet164 có độ chính xác kiểm tra 74.50%. Đối với FAQD được huấn luyện trước, học sinh FP ResNet110 có độ chính xác 72.96%. Như được hiển thị trong Bảng III và Bảng IV, FAQD có hiệu suất vượt trội đáng ngạc nhiên trên CIFAR-100. Học sinh nhị phân hóa gần như đạt độ chính xác của mô hình FP, và mô hình 4-bit vượt qua giáo viên FP.

Cifar-10
Giáo viên ResNet110: 93.91%
Phương pháp 1-bit 2-bit 4-bit
Học sinh FP ResNet20: 92.21 %
FAQD không có nhãn 89.88% 91.23% 92.19%
FAQD với Giám sát 90.56% 91.65% 92.43%

Cifar-100
Giáo viên ResNet164: 74.50%
Phương pháp 1-bit 2-bit 4-bit
FAQD không có nhãn 72.78% 74.35% 74.90%
FAQD với Giám sát 73.35% 74.40% 75.31%

Tiny ImageNet
Giáo viên ResNet34: 65.60%
Phương pháp 1-bit 2-bit 4-bit
FAQD không có nhãn 64.37% 65.05% 65.40%
FAQD với Giám sát 65.13% 65.67% 65.92%

BẢNG IV: FAQD đầu cuối đến đầu cuối của ResNet110 trên CIFAR-100. Độ chính xác của lượng tử hóa 4-bit không có nhãn vượt qua 72.96% của ResNet110 FP và gần với ResNet164 FP.

B. Lượng tử hóa Đầy đủ
Trong phần này, chúng tôi mở rộng kết quả của chúng tôi đến lượng tử hóa đầy đủ nơi hàm kích hoạt cũng được lượng tử hóa. Trong Bảng V, chúng tôi liệt kê kết quả tinh chỉnh từ các phương pháp nêu trên. Trong số các phương pháp trong Bảng V, chỉ có Quantized Distillation (QD) ổn định dưới lượng tử hóa đầy đủ đầu cuối đến đầu cuối. Chúng tôi mở rộng kết quả của chúng tôi đến tập dữ liệu Tiny ImageNet nhỏ, chứa 100K hình ảnh 64 ×64 được thu nhỏ trên 200 lớp để huấn luyện. Để mô phỏng ImageNet, chúng tôi nội suy độ phân giải trở lại 224 ×224 gốc. Như được hiển thị trong Bảng VI, lượng tử hóa tinh chỉnh 4W4A có độ chính xác tương tự với float ResNet20. Trong khi đó, chúng tôi thu hẹp khoảng cách hiệu suất tồn tại lâu dài [9] khi giảm độ chính xác kích hoạt xuống 1-bit, vì sự giảm độ chính xác là tuyến tính (theo độ chính xác kích hoạt) và nhỏ. Khi tinh chỉnh một mô hình lượng tử hóa đầy đủ, chúng tôi theo một quy trình hai bước. Đầu tiên, chúng tôi huấn luyện một mô hình lượng tử hóa kích hoạt với trọng số điểm phẩy. Sau đó, chúng tôi áp dụng lượng tử hóa đầy đủ bằng FAQD. Kỹ thuật này tỏ ra thiết yếu, đặc biệt khi mở rộng quy mô tập dữ liệu Tiny ImageNet. Trong các thí nghiệm của chúng tôi, chúng tôi quan sát hiện tượng sau khi thay thế tất cả các hàm kích hoạt ReLU bằng ReLU Lượng tử hóa 1-bit. Đối với một mô hình ResNet20 32A32W được huấn luyện trước, ban đầu được huấn luyện trên CIFAR-10, độ chính xác giảm xuống 80.03% từ độ chính xác ban đầu 92.21%. Tuy nhiên, khi làm việc với một mô hình ResNet-18 được huấn luyện trước trên tập dữ liệu Tiny ImageNet, độ chính xác giảm mạnh xuống 0.62% từ độ chính xác ban đầu 64.23%.

Phương pháp 1W4A 4W4A
Mô hình: ResNet20
BinaryRelax [28] 89.22% 91.37%
QD [19] 90.15% 92.06%
BCGD [29] 89.98% 91.65%
BRECQ∗[15] N/A 88.71%

BẢNG V: Kết quả lượng tử hóa đầy đủ tinh chỉnh của các phương pháp hiện có trên CIFAR-10. ∗ có nghĩa giống như trong Bảng 2.

CIFAR-10
ResNet20 được huấn luyện trước: 1A32W-91.89%, 4A32W-92.01%
Mô hình được huấn luyện trước 1W1A 4W4A
ResNet20 Không N/A 91.07%
ResNet20 Có 89.70% 92.53%

CIFAR-100
ResNet56 được huấn luyện trước: 1A32W-70.96%, 4A32W-71.42%
Mô hình được huấn luyện trước 1W1A 4W4A
ResNet56 Không N/A 68.84%
ResNet56 Có 68.18 73.53%

Tiny ImageNet
ResNet18 được huấn luyện trước: 1A32W-63.82%, 4A32W-64.15%
Mô hình được huấn luyện trước 1W1A 4W4A
ResNet18 Không N/A 64.67%
ResNet18 Có 65.01 65.55%

BẢNG VI: Lượng tử hóa đầy đủ đầu cuối đến đầu cuối và tinh chỉnh trên CIFAR-10, CIFAR-100 và Tiny ImageNet, với mạng giáo viên giống như trong Bảng 4.

IV. TỔN THẤT ÁI LỰC ĐẶC TRƯNG NHANH

A. Phương pháp Đề xuất
Mặc dù có sự gia tăng đáng kể của hiệu suất KD, chúng tôi lưu ý rằng việc giới thiệu tổn thất FA sẽ tăng thời gian huấn luyện. Nếu chúng ta chuẩn hóa các bản đồ đặc trưng theo hàng trước, việc tính toán tổn thất FA giữa nhiều bản đồ đặc trưng trung gian có thể tốn kém.

--- TRANG 6 ---
6

Hình 3: Tổn thất ái lực đặc trưng nhanh với ma trận ngẫu nhiên hạng thấp Z.

Lfa(F1, F2) =∥F1F1^T−F2F2^T∥2².(12)
Vì chúng ta đóng băng giáo viên được huấn luyện trước, bản đồ đặc trưng của mô hình giáo viên F1=fT(x) là một hằng số, trái ngược với bản đồ đặc trưng học sinh F2=fS(Θ,x). Ký hiệu S1=F1F1^T∈R^(WH×WH) và g(Θ,x) =fS(Θ,x)[fS(Θ,x)]^T. Ái lực đặc trưng có thể được công thức hóa là
Lfa(Θ) =1/|X|∑(x∈X)∥S1−g(Θ,x)∥2².(13)

Tính toán S1 và g(Θ, X) yêu cầu độ phức tạp O(W²H²C) mỗi cái (C là số kênh), khá tốn kém. Chúng tôi giới thiệu một ước lượng ngẫu nhiên của Lffa(Θ):
Lffa(F1, F2,z) =1/|X|∑(x∈X)∥(S1−g(Θ,x))z∥2²,(14)
trong đó z∈R^(HW) là một vector với các thành phần chuẩn đơn vị i.i.d N(0,1). Chúng tôi chỉ ra dưới đây rằng Eq. (14) là một ước lượng không thiên vị của tổn thất FA (13).

Mệnh đề 1:
Ez∼N(0,1)[Lffa(F1, F2,z)] =Lfa(Θ).

Ước lượng này có thể đạt được độ phức tạp tính toán O(HWC ) bằng cách thực hiện hai phép nhân ma trận-vector F1F1^T z.
Chúng tôi định nghĩa tổn thất Ái lực Đặc trưng Nhanh (FFA) là k tập hợp của (14):
Lffa,k(Θ) =1/|X|∑(x∈X)1/k∥(S1−g(Θ,x))Zk∥2²(15)
trong đó Zk∈R^(HW×k) với các thành phần i.i.d N(0,1), và chúng ta có k≪WH. Độ phức tạp tính toán của Lffa,k(Θ) là O(kWHC).

Cuối cùng, chúng tôi lưu ý rằng tổn thất FFA có thể tăng tốc tính toán khoảng cách Euclide theo cặp trong giảm chiều như trong (7). Cách phổ biến để tính toán khoảng cách theo cặp của các hàng cho một ma trận A∈R^(n×c) là phát sóng vector của chuẩn hàng và tính toán AA^T. Cho vector chuẩn hàng v= (∥A1∥², ···,∥An∥²), ma trận tương đồng (Sij), Sij=∥Ai−Aj∥², được tính toán là
S=1⊗v−2AA^T+v⊗1.
Số hạng 2AA^T có thể được xấp xỉ hiệu quả bởi tổn thất FFA.

B. Kết quả Thực nghiệm
Chúng tôi thử nghiệm tổn thất FA Nhanh trên CIFAR-10 và Tiny ImageNet. Như đã đề cập trong phần trước, ResNet-20 có 3 khối dư. Chiều rộng và chiều cao tương ứng cho các bản đồ đặc trưng là 32, 16, và 8, H=W cho tất cả các nhóm, vì vậy chiều (HW) của các ma trận tương đồng là 1024, 256, và 64. Chúng tôi thử nghiệm tổn thất FA nhanh với số tập hợp k=1, 5, và 15. Kết quả được hiển thị trong Bảng VII. Trong khi đó, FFA có thêm thời gian huấn luyện cho mỗi bước. Khi k= 1, độ chính xác không nhất quán do phương sai lớn. Với quá ít mẫu trong ước lượng, chuẩn FA nhanh quá nhiễu và làm hại chưng cất. Ở k= 5, tổn thất FA nhanh ổn định và độ chính xác cải thiện hướng tới của cơ sở, L=LMSE +LCE trong Bảng I. Khi k tăng lên 15, hiệu suất của tổn thất FA nhanh có thể so sánh với của tổn thất FA chính xác. Hơn nữa, chúng tôi thí nghiệm với tiêu thụ thời gian để tính toán tổn thất FA và tổn thất FFA. Chúng tôi vẽ thời gian theo thang logarit so với H, (H=W) cho các bản đồ đặc trưng. Độ phức tạp thời gian lý thuyết để tính toán tổn thất FA chính xác là O(H⁴) và của tổn thất FFA là O(H²). Hình 4(a) cho thấy sự đồng ý với ước tính lý thuyết. H càng lớn, tổn thất FFA càng có lợi. Đối với các hình ảnh (y tế) có độ phân giải hàng nghìn, tổn thất FFA sẽ có những tiết kiệm tính toán đáng kể. Trong Bảng VII, chúng tôi báo cáo thời gian huấn luyện trên mỗi epoch. Chúng tôi huấn luyện các mô hình 200 epoch với tỷ lệ học tập annealing cosine.

Tập dữ liệu CIFAR-10 Tiny ImageNet
Mô hình ResNet20 ResNet18
Số Tập hợp k Độ chính xác Tổn thất FA Nhanh
1 (ResNet20)/1 (ResNet18) 88.89±2.95% 52.32±4.35%
5/40 90.55% 56.12%
15/80 90.72% 61.12%
Số Tập hợp k Thời gian Huấn luyện Tổn thất FA Nhanh Trên Mỗi Epoch
1/1 29.71s 5m19s
5/40 29.77s 5m32s
15/80 30.74s 5m51s
Số Tập hợp k Thời gian Huấn luyện Tổn thất FA Chính xác Trên Mỗi Bước
N/A 36.17s 7m36s

BẢNG VII: Độ chính xác FFA 4A4W và thời gian huấn luyện trên mỗi epoch cho ResNet20 trên CIFAR-10 và ResNet18 trên Tiny ImageNet, với mạng giáo viên giống như trong Bảng 4. Tổn thất FFA tăng tốc huấn luyện và tiếp cận hiệu suất của tổn thất FA chính xác với lựa chọn phù hợp của số tập hợp k.

C. Phân tích Lý thuyết của Tổn thất FFA
Như được hiển thị trong Mệnh đề 4.1, tổn thất FFA là một ước lượng không thiên vị k-tập hợp của tổn thất FA. Bởi luật mạnh của số lớn, tổn thất FFA hội tụ đến tổn thất FA chính xác với xác suất 1.

Định lý 4.1: Cho Θ cho trước, giả sử rằng |Lfa(Θ)|<∞, thì
∀ϵ >0,∃N sao cho ∀k > N, |Lffa,k(Θ)− Lfa(Θ)|< ϵ.
Cụ thể, tổn thất FFA hội tụ đến tổn thất FA theo điểm:
∀Θ,lim(k→∞)Lffa,k(Θ) = Lfa(Θ).

Chúng tôi cũng thiết lập giới hạn lỗi sau cho k hữu hạn.

--- TRANG 7 ---
7

(a) Biểu đồ log-log cho thời gian suy luận của tổn thất FA và tổn thất FFA.
(b) Biểu đồ phóng to cho thời gian suy luận của tổn thất FA và tổn thất FFA.

Hình 4: Biểu đồ cho thời gian suy luận của tổn thất FA và tổn thất FFA với k= 1.

Mệnh đề 2:
P(|Lffa,k(Θ)− Lfa(Θ)|> ϵ) ≤C/(ϵ²k),
trong đó C≤3∥Lfa(Θ)∥₂⁴.

Mệnh đề 4.2 nói rằng xác suất mà ước tính FFA có lỗi vượt quá một giá trị mục tiêu giảm như O(1/k). Phân tích đảm bảo độ chính xác của tổn thất FFA như một ước lượng hiệu quả của tổn thất FA. Một câu hỏi khác người ta có thể hỏi là liệu việc tối thiểu hóa tổn thất FFA có tương đương với việc tối thiểu hóa tổn thất FA hay không. Ký hiệu Θ∗= arg min Lfa(Θ) và Θ∗ₖ= arg min Lffa,k(Θ), và giả sử cực tiểu là duy nhất cho mỗi hàm. Để thay thế tổn thất FA bằng tổn thất FFA, người ta hy vọng rằng Θ∗ₖ hội tụ đến Θ∗. Thật không may, sự hội tụ theo điểm trong Định lý 4.1 không đủ để đảm bảo sự hội tụ của các điểm tối ưu, vì một phản ví dụ có thể dễ dàng được xây dựng. Trong phần còn lại của phần này, chúng tôi chỉ ra rằng sự hội tụ như vậy có thể được thiết lập dưới một giả định bổ sung.

Định lý 4.2 (Hội tụ trong trường hợp tổng quát): Giả sử rằng Lffa,k(Θ) hội tụ đến Lfa(Θ) đồng đều, tức là
∀ϵ >0,∃N sao cho ∀k > N, |Lffa,k(Θ)− Lfa(Θ)|< ϵ
và |Lfa(Θ)|<∞,∀Θ. Thì
lim(k→∞)||Θ∗ₖ−Θ∗||₂= 0.(16)

Giả định hội tụ đồng đều có thể được nới lỏng nếu Lfa là lồi trong Θ. Một hệ quả của Định lý 4.2 như dưới đây.

Hệ quả 4.2.1 (Hội tụ trong trường hợp lồi): Cho Lfa:Rⁿ→R là lồi và L-smooth, và rằng ∃hằng số M > 0 sao cho ||Θ∗ₖ|| ≤M,∀k. Thì Lffa,k cũng lồi cho bất kỳ k nào, và lim(k→∞)||Θ∗ₖ−Θ∗||₂= 0.

V. KẾT LUẬN
Chúng tôi đã trình bày FAQD, một phương pháp chưng cất tri thức hỗ trợ đặc trưng (FA) cho huấn luyện nhận thức lượng tử hóa. Nó kết hợp tổn thất MSE với tổn thất FA và cải thiện đáng kể độ chính xác của mạng học sinh lượng tử hóa. FAQD áp dụng cho cả lượng tử hóa chỉ trọng số và lượng tử hóa đầy đủ, và vượt trội hơn Resnets cơ sở trên CIFAR-10/100 và Tiny ImageNet. Chúng tôi cũng phân tích một xấp xỉ ngẫu nhiên hiệu quả (FFA) cho tổn thất FA cho các bản đồ đặc trưng chiều lớn, cung cấp nền tảng lý thuyết cho tổn thất FFA để có lợi cho việc huấn luyện mô hình tương lai trên hình ảnh độ phân giải cao trong các ứng dụng.

VI. PHỤ LỤC
Chứng minh Định lý 2.1: Đủ để chứng minh rằng cho bất kỳ tập hợp n vector đơn vị trong Rd, có một ánh xạ tuyến tính gần như bảo toàn khoảng cách góc theo cặp, vì khoảng cách góc là bất biến tỷ lệ.
Cho T là một biến đổi tuyến tính được cảm sinh bởi một ma trận Gaussian ngẫu nhiên (1/√k)A∈Rᵏˣᵈ sao cho T(F) =FA^T. Định nghĩa các sự kiện A⁻ᵢⱼ={T: (1−ϵ)∥Fi−Fj∥²≤ ∥T(F)i−T(F)j∥²≤ (1+ϵ)∥Fi−Fj∥² thất bại} và A⁺ᵢⱼ={T: (1−ϵ)∥Fi+Fj∥²≤ ∥T(F)i+T(F)j∥²≤(1 +ϵ)∥Fi+Fj∥² thất bại}.
Theo chứng minh của bổ đề JL cổ điển trong trường hợp Euclide [23], chúng ta có:
P(A⁻ᵢⱼ)≤2e^(−(ϵ²−ϵ³)k/4), P(A⁺ᵢⱼ)≤2e^(−(ϵ²−ϵ³)k/4).(17)

Cho Bij={T:|Fi·Fj−T(F)i·T(F)j|> ϵ}, trong đó · là viết tắt cho tích vô hướng. Chúng tôi chỉ ra rằng Bij⊂A⁻ᵢⱼ∪ A⁺ᵢⱼ cho ∥Fi∥=∥Fj∥= 1 bằng cách chỉ ra A⁻ᵢⱼᶜ∩ A⁺ᵢⱼᶜ⊂ BCᵢⱼ.
Nếu A⁻ᵢⱼᶜ∩ A⁺ᵢⱼᶜ đúng, chúng ta có
4T(F)i·T(F)j
=∥T(F)i+T(F)j∥²− ∥T(F)i−T(F)j∥²
≤(1 +ϵ)∥Fi+Fj∥²−(1−ϵ)∥Fi−Fj∥²
=4Fi·Fj+ 2ϵ(∥Fi∥²+∥Fj∥²)
=4Fi·Fj+ 4ϵ.
Do đó, Fi·Fj−T(F)i·T(F)j≥ −ϵ. Bằng một lập luận tương tự, chúng ta có Fi·Fj−T(F)i·T(F)j≤ϵ. Sau đó chúng ta có A⁻ᵢⱼᶜ∩ A⁺ᵢⱼᶜ⊂ BCᵢⱼ, và do đó
P(Bij)≤P(A⁻ᵢⱼ∪A⁺ᵢⱼ)≤4 exp{−(ϵ²−ϵ³)k/4}
và
P(∪i<jBij)≤∑(i<j)P(Bij)≤4n² exp{−(ϵ⁴−ϵ³)k/4}.
Xác suất này nhỏ hơn 1 nếu chúng ta lấy k >16 lnn/ϵ².
Do đó, phải tồn tại một T sao cho ∩i<jBCᵢⱼ đúng, điều này hoàn thành chứng minh.

--- TRANG 8 ---
8

Chứng minh Mệnh đề 4.1: Cho N=WH, aij= (F1F1^T)ij, và bij= (F2F2^T)ij trong phương trình (14), chúng ta có:
Ez Lffa(F1, F2; 2) = Ez ∑(i=1 đến N)(∑(j=1 đến N)|aij−bij|zj)²
=Ez ∑(i=1 đến N)(∑(j=1 đến N)|aij−bij|²zj²+2∑(j≠k)|aij−bij||aik−bik|zjzk)
=Ez ∑(i=1 đến N)∑(j=1 đến N)|aij−bij|²zj²+2∑(i=1 đến N)∑(j≠k)|aij−bij||aik−bik|zjzk
=∑(i=1 đến N)∑(j=1 đến N)|aij−bij|²Ez zj²
+ 2∑(i=1 đến N)∑(j≠k)|aij−bij||aik−bik|Ez zjzk
=∑(i=1 đến N)∑(j=1 đến N)|aij−bij|²=Lfa(F1, F2; 2).

Chứng minh Định lý 4.1: Cho một ma trận Gaussian Zk= [z1,···,zk]∈Rⁿˣᵏ,
Lffa,k(Θ) =1/k ∑(l=1 đến k)Lffa(F1, F2,zl).
Với bất kỳ Θ cố định nào, Lffa(F1, F2,zl),l= 1,···, k, là các biến ngẫu nhiên i.i.d. Giả sử moment đầu tiên của mỗi biến ngẫu nhiên là hữu hạn, bởi luật mạnh của số lớn,
Lffa,k(Θ) hội tụ đến E[Lffa(F1, F2,z1)] gần như chắc chắn. Nói cách khác, lim(k→∞)Lffa,k(Θ) = Lfa(Θ) với xác suất 1.

Chứng minh Mệnh đề 4.2: Bằng bất đẳng thức Chebyshev, chúng ta có
P(Lffa,k(Θ)−E[Lffa,k(Θ)]> ϵ) ≤ Var(Lffa,k(Θ))/ϵ²=Var(Lffa(F1, F2,z1))/(ϵ²k).(18)

Để ước tính
Var(Lffa(F1, F2,z1) = E[L²ffa(F1, F2,z1)]− E[Lffa(F1, F2,z1)]²,(19)
đủ để ước tính
E[L²ffa(F1, F2,z1)] = Ez[∑(i=1 đến N)∑(j=1 đến N)|aij−bij|²zj²+∑(i=1 đến N)∑(j≠k)|aij−bij||aik−bik|zjzk]²
mà bằng (vì các số hạng chéo bằng không):
Ez[∑(i=1 đến N)∑(j=1 đến N)|aij−bij|²zj²]²+[∑(i=1 đến N)∑(j≠k)|aij−bij||aik−bik|zjzk]².

Tính toán trực tiếp cho:
∑(i=1 đến N)∑(j=1 đến N)|aij−bij|⁴zj⁴+∑(i=1 đến N)∑(j=1 đến N)∑(l≠i)|aij−bij|²|alj−blj|²zj⁴
+2∑(i=1 đến N)∑(j=1 đến N)∑(l≠j)|aij−bij|²|ail−bil|²zj²zl²
+∑(i=1 đến N)∑(j=1 đến N)∑(k=1 đến N)∑(l≠j)|aij−bij|²|akl−bkl|²zj²zl²

Lưu ý rằng E[zi⁴] = 3. Lấy E[·], chúng ta dẫn ra giới hạn trên 3∥Lfa∥₂⁴.

Chứng minh Định lý 4.2: Vì lim(k→∞)Lffa,k(Θ*) = Lfa(Θ*), đủ để chỉ ra rằng
lim(k→∞)inf_Θ Lffa,k(Θ) = Lfa(Θ*).
Lưu ý rằng
∀Θ,lim(k→∞)Lffa,k(Θ) = Lfa(Θ)≤ Lfa(Θ*).
Thì,
Lfa(Θ*)≥lim(k→∞)inf_Θ Lffa,k(Θ).
Mặt khác, với ϵ tùy ý >0, chúng ta có:
∃N sao cho ∀k > N |Lffa,k(Θ)− Lfa(Θ)|<ϵ/2,∀Θ
và tồn tại một dãy {Θk} sao cho
Lffa,k(Θk)<inf_Θ Lffa,k(Θ) +ϵ/2.
Lưu ý rằng |Lffa,k(Θk)− Lfa(Θk)|<ϵ/2 với k > N, vì vậy:
Lfa(Θ*)−ϵ≤ Lfa(Θk)−ϵ <inf_Θ Lffa,k(Θ),∀k > N.
Vì ϵ là tùy ý, lấy k→ ∞, chúng ta có
Lfa(Θ*)≤lim(k→∞)inf_Θ Lffa,k(Θ).

Chứng minh Hệ quả 4.2.1: Để dễ đọc, chúng ta viết tắt: Lffa,k =fk và Lfa=f. Cho
H=∇²f/(∇Θ∇Θ^T)≽0∈ Rⁿˣⁿ
là ma trận Hessian của tổn thất FA, là nửa xác định dương bởi tính lồi của Lfa. Thì,
∇²fk/(∇Θ∇Θ^T)=Zk^T HZk≽0∈Rᵏˣᵏ
điều này ngụ ý tính lồi của fk cho tất cả k. Hơn nữa, rõ ràng là fk là smooth cho tất cả k vì
∥∇fk(x)− ∇fk(y)∥=∥Zk(∇f(x)− ∇f(y))∥ ≤L· ∥Zk∥ · ∥x−y∥.(20)
Chúng tôi lưu ý rằng fk cũng smooth. Mặc dù chúng ta không thể khẳng định equi-smoothness vì chúng ta không thể giới hạn ∥Zk∥ đồng đều trong k, điều trên đủ để chúng ta chứng minh kết quả mong muốn.

--- TRANG 9 ---
9

Với ∀k, cho bất kỳ tham số ban đầu Θ0, bởi tính smooth và lồi của fk, đã biết rằng
∥Θt_k−Θ*_k∥ ≤ ∥ Θ0−Θ*_k∥
trong đó Θt_k là tham số chúng ta đạt được sau t bước của gradient descent. Do đó, chúng ta có thể chọn một tập compact K=BR(Θ*) cho R đủ lớn sao cho {Θk}∞_{k=1}⊂K (ký hiệu Θ*_∞= Θ*).
Bây giờ, đủ để chứng minh fk hội tụ đến f đồng đều trên K.
Thực tế, fk hội tụ đến f trên bất kỳ tập compact nào. Để bắt đầu, chúng tôi phát biểu một kết quả đã biết từ phân tích hàm ([5], [12]):

Bổ đề 6.1: (Tính bị chặn đồng đều và equi-Lipschitz) Cho F là một họ hàm lồi trên Rⁿ và K⊂Rⁿ là một tập con compact. Thì, F là equi-bounded và equi-Lipschitz trên K.

Kết quả này được thiết lập trong bất kỳ không gian Banach nào trong [12], vì vậy nó tự động đúng trong không gian Euclide chiều hữu hạn. Bằng Bổ đề 6.1, chúng ta có rằng dãy {fk}∞_{k=1}, trong đó f∞= f, là equi-Lipschitz. ∀>0,∃δ >0 sao cho |fk(x)−fk(y)|< ϵ cho tất cả k và x, y∈K khi |x−y|< δ. Vì {B(x, δ)}_{x∈K} tạo thành một cover mở cho K, chúng ta có một sub-cover hữu hạn {B(xj, δ)}^m_{j=1} của K. Vì có hữu hạn điểm xj, tồn tại Nϵ sao cho
∀k > N ϵ,|fk(xj)−f(xj)|< ϵ, cho j= 1,···, m.
Với bất kỳ x∈K, x∈B(xj*, δ) cho một số j*. Với tất cả k > N ϵ, chúng ta có
|fk(x)−f(x)| ≤ |fk(x)−fk(xj*)|+|fk(xj*)−f(xj*)|+|f(xj*)−f(x)| ≤(2L̃+ 1)ϵ(21)
trong đó L̃ là hằng số Lipschitz cho họ equi-Lipschitz.
Do đó, fk hội tụ đến f đồng đều trên K.

TÀI LIỆU THAM KHẢO
[1] Luca Becchetti, Marc Bury, Vincent Cohen-Addad, Fabrizio Grandoni, và Chris Schwiegelshohn. Oblivious dimension reduction for k-means: beyond subspaces and the Johnson-Lindenstrauss lemma. Trong Proceedings of the 51st annual ACM SIGACT symposium on theory of computing, trang 1039–1050, 2019.
[2] Yoshua Bengio, Nicholas Léonard, và Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
[3] Zhaowei Cai, Xiaodong He, Jian Sun, và Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 5918–5926, 2017.
[4] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, và Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.
[5] S Cobzas. Lipschitz properties of convex mappings. Adv. Oper. Theory, 2(1):21–49, 2017.
[6] Matthieu Courbariaux, Yoshua Bengio, và Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. Advances in neural information processing systems, 28, 2015.
[7] Tim Dockhorn, Yaoliang Yu, Eyyüb Sari, Mahdi Zolnouri, và Vahid Partovi Nia. Demystifying and generalizing binaryconnect. Advances in Neural Information Processing Systems, 34:13202–13216, 2021.
[8] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, và Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 4852–4861, 2019.
[9] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, và Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 4852–4861, 2019.
[10] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.
[11] William B Johnson, Joram Lindenstrauss, và Gideon Schechtman. Extensions of Lipschitz maps into Banach spaces. Israel Journal of Mathematics, 54(2):129–138, 1986.
[12] Mohamed Jouak và Lionel Thibault. Equicontinuity of families of convex and concave-convex operators. Canadian Journal of Mathematics, 36(5):883–898, 1984.
[13] Jangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel, và Nojun Kwak. Qkd: Quantization-aware knowledge distillation. arXiv preprint arXiv:1911.12491, 2019.
[14] Taehyeon Kim, Jaehoon Oh, NakYil Kim, Sangwook Cho, và Se-Young Yun. Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation. trang 2628–2635, 2021.
[15] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, và Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.
[16] Zhijian Li, Bao Wang, và Jack Xin. An integrated approach to produce robust deep neural network models with high efficiency. Trong International Conference on Machine Learning, Optimization, and Data Science, trang 451–465. Springer, 2021.
[17] Ziang Long, Penghang Yin, và Jack Xin. Recurrence of optimum for training weight and activation quantized networks. Applied and Computational Harmonic Analysis, 62:41–65, 2023.
[18] Konstantin Makarychev, Yury Makarychev, và Ilya Razenshteyn. Performance of Johnson-Lindenstrauss transform for k-means and k-medians clustering. Trong Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, trang 1027–1038, 2019.
[19] Antonio Polino, Razvan Pascanu, và Dan Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018.
[20] Dinesh Ramasamy và Upamanyu Madhow. Compressive spectral embedding: sidestepping the svd. Advances in neural information processing systems, 28, 2015.
[21] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, và Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. Trong European conference on computer vision, trang 525–542. Springer, 2016.

--- TRANG 10 ---
10

[22] Nicolas Tremblay, Gilles Puy, Rémi Gribonval, và Pierre Vandergheynst. Compressive spectral clustering. Trong International conference on machine learning, trang 1002–1011. PMLR, 2016.
[23] Santosh S Vempala. The random projection method, volume 65. American Mathematical Soc., 2005.
[24] Li Wang, Dong Li, Yousong Zhu, Lu Tian, và Yi Shan. Dual super-resolution learning for semantic segmentation. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 3774–3783, 2020.
[25] Biao Yang, Fanghui Xue, Yinyong Qi, và Jack Xin. Improving efficient semantic segmentation networks by enhancing multi-scale feature representation via resolution path based knowledge distillation and pixel shuffle. Trong Proceedings of the 16th International Symposium on Visual Computing, trang 325–336. Springer, Cham, 2021.
[26] Junho Yim, Donggyu Joo, Jihoon Bae, và Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 4133–4141, 2017.
[27] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, và Jack Xin. Understanding straight-through estimator in training activation quantized neural nets. trong Proceedings of International Conference on Learning Representations (ICLR), 2019.
[28] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, và Jack Xin. Binaryrelax: A relaxation approach for training deep neural networks with quantized weights. SIAM Journal on Imaging Sciences, 11(4):2205–2223, 2018.
[29] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, và Jack Xin. Blended coarse gradient descent for full quantization of deep neural networks. Research in the Mathematical Sciences, 6(1):1–23, 2019.
[30] Sergey Zagoruyko và Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016.
[31] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, và Michael Jordan. Theoretically principled trade-off between robustness and accuracy. Trong International conference on machine learning, trang 7472–7482. PMLR, 2019.
