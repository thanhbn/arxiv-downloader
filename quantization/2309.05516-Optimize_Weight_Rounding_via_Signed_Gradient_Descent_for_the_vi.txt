# 2309.05516.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/quantization/2309.05516.pdf
# Kích thước tệp: 2271477 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tối ưu hóa Làm tròn Trọng số thông qua Gradient Descent Có dấu cho
Lượng tử hóa các Mô hình Ngôn ngữ Lớn
Wenhua Cheng*và Weiwei Zhang và Haihao Shen và Yiyang Cai
Xin He và Kaokao Lv và Yi Liu
Intel
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) đã thể hiện năng lực đặc biệt trong các nhiệm vụ liên quan đến ngôn ngữ, nhưng việc triển khai chúng đặt ra những thách thức đáng kể do yêu cầu bộ nhớ và lưu trữ lớn. Lượng tử hóa chỉ trọng số đã nổi lên như một giải pháp hứa hẹn, giảm đáng kể nhu cầu bộ nhớ và lưu trữ mà không hy sinh quá nhiều hiệu suất. Trong nghiên cứu này, chúng tôi giới thiệu SignRound, một phương pháp tận dụng gradient descent có dấu (SignSGD) để tối ưu hóa các giá trị làm tròn và cắt ngắn trọng số chỉ trong 200 bước. SignRound tích hợp các ưu điểm của Quantization-Aware Training (QAT) và Post-Training Quantization (PTQ), mang lại kết quả đặc biệt từ 2 đến 4 bit trong khi giảm thiểu chi phí điều chỉnh và tránh thêm chi phí suy luận. Ví dụ, SignRound đạt được cải thiện độ chính xác tuyệt đối trung bình từ 6.91% đến 33.22% ở 2 bit, được đo bằng độ chính xác zero-shot trung bình trên 11 nhiệm vụ. Nó cũng thể hiện khả năng tổng quát hóa mạnh mẽ trong các mô hình gần đây, đạt được lượng tử hóa 4-bit gần như không mất mát trong hầu hết các tình huống. Mã nguồn được công bố tại https://github.com/intel/auto-round .

1 Giới thiệu
Trong những năm gần đây, đã có sự gia tăng đáng kể trong việc áp dụng các Mô hình Ngôn ngữ Lớn (LLMs), dẫn đến nhu cầu triển khai rộng rãi chúng ngay cả trên các thiết bị có tài nguyên hạn chế. Tuy nhiên, việc triển khai LLMs trên các thiết bị này đặt ra những thách thức đáng kể do yêu cầu bộ nhớ và lưu trữ lớn. Ngoài ra, nhu cầu tính toán của các mô hình này tạo ra rào cản cho các ứng dụng thời gian thực. Do đó, việc nghiên cứu các kỹ thuật như lượng tử hóa là rất quan trọng để cho phép triển khai hiệu quả các LLMs. Các kỹ thuật lượng tử hóa có thể được phân loại thành hai loại chính: quantization-aware training (QAT) *Correspondence:wenhua.cheng@intel.com(Esser et al., 2020; Zhuang et al., 2021; Lee et al., 2021; Liu et al., 2023b) và post-training quantization (PTQ) (Nagel et al., 2019; Xiao et al., 2023; Frantar et al., 2022; Nagel et al., 2020).

QAT bao gồm việc huấn luyện mô hình với lượng tử hóa trong tâm trí, sử dụng các biểu diễn độ chính xác thấp hơn được mô phỏng để cho phép mô hình học hỏi và thích ứng với các hiệu ứng của lượng tử hóa. Cách tiếp cận này thường mang lại độ chính xác tốt hơn so với PTQ. Tuy nhiên, QAT có những nhược điểm, bao gồm tăng độ phức tạp huấn luyện, thời gian huấn luyện dài hơn và cần điều chỉnh các siêu tham số. Việc áp dụng QAT cho LLMs có thể đặc biệt tốn tài nguyên, mặc dù có những nỗ lực gần đây (Hu et al., 2021; Dettmers et al., 2023) để cải thiện hiệu quả của việc fine-tuning LLMs.

Mặt khác, PTQ lượng tử hóa trực tiếp mô hình mà không có bất kỳ huấn luyện mô phỏng hoặc fine-tuning nào. Trong khi PTQ là một cách tiếp cận đơn giản hơn, nó dễ bị giảm độ chính xác đáng kể. Điều này nhấn mạnh tầm quan trọng của những tiến bộ tiếp theo trong các phương pháp PTQ để nâng cao khả năng bảo tồn độ chính xác của chúng.

Lượng tử hóa thường áp dụng cho hai loại tensor: activations và weights. Lượng tử hóa activations cho LLMs có thể khó khăn (Wei et al., 2023; Xiao et al., 2023; Bondarenko et al., 2024), làm cho lượng tử hóa chỉ trọng số trở thành một lựa chọn thực tế hơn. Hơn nữa, nút thắt cổ chai chính trong việc tạo ra các token mới cho LLMs thường phát sinh từ các hạn chế băng thông bộ nhớ (Kim et al., 2023a), nhấn mạnh ưu điểm của lượng tử hóa chỉ trọng số.

Nghiên cứu này tập trung vào lượng tử hóa chỉ trọng số. Trong việc lượng tử hóa trọng số, một bước quan trọng bao gồm việc làm tròn, chủ yếu được thực hiện thông qua rounding-to-nearest (RTN). RTN lượng tử hóa từng trọng số độc lập bằng cách làm tròn nó đến số nguyên gần nhất, nhưng nó bỏ qua các mối quan hệ giữa các trọng số và giữa trọng số và activations. Adaptive Rounding (Nagel et al., 2020) đã khám phá tiềm năng cho một chiến lược làm tròn nâng cao để cải thiện độ chính xác.arXiv:2309.05516v5  [cs.CL]  8 Oct 2024

--- TRANG 2 ---
Họ đã tiếp cận nhiệm vụ làm tròn bằng cách xây dựng nó như một bài toán tối ưu hóa nhị phân không có ràng buộc bậc hai và xấp xỉ mất mát bằng cách sử dụng khai triển chuỗi Taylor. Tuy nhiên, việc chỉ dựa vào số hạng bậc hai có thể không mang lại kết quả chính xác, vì việc làm tròn có thể thay đổi đáng kể trọng số, làm cho các số hạng bậc khác trở nên không thể bỏ qua.

Chúng tôi chọn SignSGD(Balles et al., 2020; Li et al., 2023a; Safaryan and Richtárik, 2021) như phương pháp tối ưu hóa để tiếp cận giải pháp làm tròn tối ưu trong một số bước hạn chế. Động lực đằng sau lựa chọn này, được trình bày chi tiết trong Phần 3, xuất phát từ các ranh giới được xác định rõ của không gian giải pháp và tính đơn giản vốn có của phương pháp chỉ cần điều chỉnh siêu tham số tối thiểu. Hình 1 cung cấp tổng quan về phương pháp của chúng tôi. Các đóng góp của chúng tôi chủ yếu nằm ở ba khía cạnh:

•Chúng tôi giới thiệu một phương pháp ngắn gọn nhưng hiệu quả để tối ưu hóa lượng tử hóa chỉ trọng số, kết hợp những ưu điểm của cả QAT và PTQ. Cách tiếp cận của chúng tôi tận dụng SignSGD để điều chỉnh việc làm tròn với việc cắt ngắn trọng số, mà không giới thiệu bất kỳ chi phí bổ sung nào trong quá trình suy luận.

•Các kết quả thực nghiệm của chúng tôi thể hiện sự nâng cao hiệu suất đáng kể so với các nghiên cứu gần đây trên các cấu hình lượng tử hóa khác nhau, từ 2-bit đến 4-bit.

•Chúng tôi chứng minh rằng hiệu suất của SignRound có thể được nâng cao thêm bằng cách fine-tuning các siêu tham số cụ thể của mô hình trong một không gian hạn chế. Hơn nữa, phương pháp của chúng tôi thể hiện khả năng tổng quát hóa mạnh mẽ trên các mô hình khác nhau và mang lại kết quả gần như không mất mát trên phần lớn các tình huống sử dụng lượng tử hóa 4-bit.

2 Nghiên cứu Liên quan
Quantization Aware Training. Các phương pháp QAT đã trở nên phổ biến rộng rãi trong nén mô hình, vì chúng cho phép quá trình fine-tuning (Esser et al., 2020; Zhuang et al., 2021; Lee et al., 2021), thường dẫn đến độ chính xác vượt trội so với phương pháp PTQ.

Post-training Quantization (PTQ). Các phương pháp PTQ đơn giản hóa quá trình lượng tử hóa mà không cần huấn luyện bổ sung. (Nagel et al., 2019;Liu et al., 2021; Frantar and Alistarh, 2022; Hassibi et al., 1993; Yao et al., 2021). Do yêu cầu tài nguyên thấp, PTQ đặc biệt phù hợp cho việc lượng tử hóa các Mô hình Ngôn ngữ Lớn.

Large Language Models Quantization. Những bước tiến đáng kể đã được thực hiện trong việc giải quyết nhu cầu cấp bách về lượng tử hóa các mô hình ngôn ngữ lớn (LLMs). GPT3.int8() (Dettmers et al., 2022) giới thiệu một cách tiếp cận độ chính xác hỗn hợp để bảo tồn các kênh quan trọng ở độ chính xác cao. AQLM (Mao et al., 2024) xây dựng dựa trên Additive Quantization, một thuật toán cổ điển từ họ Multi-Codebook Quantization, thích ứng nó cho lượng tử hóa LLM. ZeroQuantV2 (Yao et al., 2024) sử dụng các ma trận hạng thấp để nâng cao khôi phục chất lượng mô hình. RPTQ (Yuan et al., 2023) giải quyết sự khác biệt phạm vi giữa các kênh bằng cách sắp xếp lại và lượng tử hóa chúng theo cụm. LLM-QAT (Liu et al., 2023b) sử dụng QAT để nâng cao hiệu suất. Một số phương pháp khác, như SPIQ (Yvinec et al., 2023b), SmoothQuant (Xiao et al., 2023), và Outlier Suppression+ (Wei et al., 2023), sử dụng các phép biến đổi tương đương được thiết kế thủ công để giảm thiểu lỗi lượng tử hóa. Các phương pháp này dựa vào kiến trúc mô hình để hợp nhất các phép toán biến đổi tương đương. LRQ (Lee et al., 2024) chỉ cần học ít tham số hơn đáng kể trong khi cho phép chia tỷ lệ riêng lẻ của trọng số, do đó tăng cường khả năng tổng quát hóa của các LLMs đã lượng tử hóa.

Weight Only Quantization. Lượng tử hóa chỉ trọng số giảm dấu chân bộ nhớ và nhu cầu băng thông bằng cách chỉ lượng tử hóa trọng số trong khi giữ activations ở độ chính xác điểm nổi, cung cấp sự cân bằng hứa hẹn giữa độ chính xác và nén. GPTQ (Frantar et al., 2022) tối ưu hóa trọng số bằng kỹ thuật Optimal Brain Surgeon (Hassibi et al., 1993), đạt được lượng tử hóa bit thấp trên LLMs với chi phí điều chỉnh tối thiểu. AWQ (Lin et al., 2023) theo cách tiếp cận biến đổi tương đương với điều chỉnh bổ sung trong không gian hạn chế, chia sẻ những hạn chế tương tự với SmoothQuant (Xiao et al., 2023). TEQ (Cheng et al., 2023) và OmniQuant (Shao et al., 2023) đều sử dụng biến đổi tương đương có thể huấn luyện, trong khi OmniQuant sử dụng điều chỉnh cắt ngắn trọng số bổ sung. HQQ (Badri and Shaji, 2023) tăng tốc lượng tử hóa cho các mô hình lớn bằng cách loại bỏ nhu cầu dữ liệu hiệu chuẩn, làm cho quá trình lượng tử hóa cực kỳ nhanh. Một số nghiên cứu khác đã tích hợp các phương pháp tối ưu hóa với chi phí suy luận bổ sung để cải thiện độ chính xác lượng tử hóa, như các kỹ thuật phân tách dense-and-sparse trong SqueezeLLM (Kim et al., 2023a) và EasyQuant (Tang et al., 2023), cũng như các phương pháp lượng tử hóa không đồng nhất trong NUPES (Yvinec et al., 2023a), QuIP# (Tseng et al., 2024),(Gong et al., 2024), AQLM (Mao et al., 2024), v.v. Ngoài ra, FineQuant (Kim et al., 2023b) giới thiệu một cách tiếp cận lượng tử hóa trọng số heuristic đơn giản để xác định tự động độ chi tiết lượng tử hóa. Trong nghiên cứu này, chúng tôi tập trung vào các cách tiếp cận không giới thiệu chi phí trong quá trình suy luận.

Rounding Methods. Adaptive Rounding (Nagel et al., 2020) đã chỉ ra tiềm năng của một chiến lược làm tròn tiên tiến để nâng cao độ chính xác (Li et al., 2021; Wei et al., 2022). Họ đã sử dụng nhiệm vụ làm tròn như một bài toán tối ưu hóa nhị phân không có ràng buộc bậc hai bằng cách xấp xỉ mất mát nhiệm vụ thông qua khai triển chuỗi Taylor. Tuy nhiên, chỉ xem xét số hạng bậc hai có thể không mang lại kết quả chính xác. Điều này là vì giá trị làm tròn được nhân với một hệ số chia tỷ lệ trong quá trình de-quantization, có khả năng giới thiệu những thay đổi trọng số đáng kể làm cho các số hạng bậc khác trở nên không thể bỏ qua. FlexRound (Lee et al., 2023) giới thiệu một cách tiếp cận linh hoạt hơn cho việc làm tròn bằng cách tích hợp phép chia theo từng phần tử. Tuy nhiên, nó không dễ dàng mở rộng để áp dụng cho LLMs do nhu cầu về các siêu tham số chuyên biệt cho từng mô hình và nhiệm vụ cụ thể. Hơn nữa, Oscillation-free (Liu et al., 2023a) gợi ý rằng việc giới thiệu các tham số có thể học được có thể dẫn đến các vấn đề dao động trọng số. AQuant (Li et al., 2022) giới thiệu một cách tiếp cận động mà ranh giới trở thành một hàm phụ thuộc vào giá trị activation để giảm lỗi lượng tử hóa của activation.

Signed Gradient Descent. Signed gradient descent không được sử dụng phổ biến và thường được áp dụng trong các tình huống cụ thể, chẳng hạn như giảm chi phí giao tiếp. Điều này là vì signed gradient mang ít thông tin hơn đáng kể so với gradient gốc. Các nghiên cứu gần đây đã làm sáng tỏ những ưu điểm của các phương pháp dựa trên dấu so với gradient descent trong một số điều kiện nhất định. Balles et al. (Balles et al., 2020) phát hiện rằng các phương pháp dựa trên dấu tốt hơn khi ma trận Hessian tập trung trên đường chéo và giá trị riêng lớn nhất lớn hơn nhiều so với giá trị riêng trung bình. Li et al. (Li et al., 2023a) đã nghiên cứu một biến thể của gradient descent dựa trên dấu thể hiện sự hội tụ nhanh hơn. Safaryan et al. (Safaryan and Richtárik, 2021) đề xuất một stochastic sign descent với momentum, hội tụ dưới giả định phương sai bị chặn tiêu chuẩn với tốc độ tiệm cận tối ưu. Những phát hiện này góp phần hiểu rõ hơn về những lợi ích tiềm năng và ứng dụng của các phương pháp signed gradient descent.

3 Phương pháp
Chúng tôi bắt đầu với tổng quan về lượng tử hóa trước khi đi sâu vào chi tiết của cách tiếp cận của chúng tôi. Các phép toán sau đây có thể được sử dụng để lượng tử hóa và de-quantize trọng số W:

fW=s∗clip(W/s+zp, n, m ), n, m∈N(1)

trong đó phép toán làm tròn ⌊·⌉thường được thực hiện bằng phương pháp RTN. Mặc dù RTN là một cách tiếp cận đơn giản, nó lượng tử hóa từng phần tử độc lập, dẫn đến mất khả năng mô hình hóa tương quan giữa các trọng số khác nhau hoặc activations. s đại diện cho tỷ lệ lượng tử hóa, có thể thu được bằng phương trình sau, và zp là điểm zero.

s=(max(W)−min(W))/(2^bit−1)(2)

Để cải thiện hiệu quả của phép toán lượng tử hóa làm tròn, chúng tôi xây dựng dựa trên nghiên cứu trước đó (Nagel et al., 2020) bằng cách giới thiệu một tham số có thể huấn luyện V để điều chỉnh các giá trị làm tròn.

fW=s∗clip((W/s+zp+V), n, m ), n, m∈N(3)

Ngoài ra, theo các nghiên cứu gần đây (Lin et al., 2023; Shao et al., 2023), chúng tôi giới thiệu hai tham số có thể huấn luyện bổ sung, ký hiệu là α∈[0,1] và β∈[0,1], để fine-tune tỷ lệ cắt ngắn trọng số. Các tham số này được tích hợp vào phương trình như sau:

s=(max(W)∗α−min(W)∗β)/(2^bit−1)(4)

Những thay đổi này cho phép một quá trình lượng tử hóa thích ứng hơn. Chúng tôi sử dụng tái tạo đầu ra theo khối để huấn luyện các tham số này qua optimizer, do đó tạo khung cho tối ưu hóa như sau.

min α,β,V ∥WX−fWX∥²F (5)

trong đó X là đầu vào của khối và ||·||F biểu thị chuẩn Frobenius.

Phương pháp của chúng tôi phân biệt chủ yếu bằng cách tận dụng SignSGD, tối ưu hóa các tham số dựa trên dấu của gradient như sau:

Wt+1=Wt−lrt∗sign(gt) (6)

trong đó t đại diện cho bước, lr là tốc độ học và g biểu thị gradient. Động lực được chi tiết dưới đây. Thứ nhất, các giá trị tối ưu cho việc làm tròn lên và xuống thường nằm trong một vùng lớn thay vì một điểm thập phân duy nhất, vì chỉ có ngưỡng để thay đổi giá trị làm tròn là quan trọng. Điều này loại bỏ sự cần thiết cho độ lớn gradient để hội tụ chính xác đến một điểm duy nhất. Thứ hai, do các ranh giới bị giới hạn, tức là [−0.5,0.5] cho làm tròn và [0,1] cho cắt ngắn trọng số, SignSGD cho phép điều hướng hiệu quả trong không gian này trong số bước hạn chế. Ngược lại, các optimizer như Adam (Kingma and Ba, 2014) có thể gặp khó khăn do sự biến động đáng kể trong độ lớn gradient, khiến việc hội tụ đến giá trị tối ưu trong số bước hạn chế trở nên khó khăn. Thứ ba, SignSGD vốn dĩ trực quan, tạo điều kiện dễ dàng điều chỉnh kích thước bước (tốc độ học). Ví dụ, chúng tôi đã sử dụng cùng các siêu tham số optimizer trên tất cả các thí nghiệm trừ khi được nêu rõ ràng, bao gồm 200 bước và tốc độ học 5e-3 với suy giảm trọng số tuyến tính. Dựa trên Eq. 6, điều chỉnh tối đa cho mỗi tham số là tổng của các tốc độ học trên tất cả các bước, tức là 200×0.005/2 = 0.5. Kết quả là, điều chỉnh có thể bao phủ phạm vi [-0.5,0.5] khi khởi tạo tại 0 cho làm tròn, và phạm vi [0.5,1.0] khi khởi tạo tại 1 và cắt ngắn đến ≤1.0 cho cắt ngắn trọng số, hoạt động tốt trong thực tế. Thứ tư, SignSGD phân biệt bởi thiết kế nhẹ nhàng, đòi hỏi ít tài nguyên bộ nhớ và tính toán hơn so với các optimizer như Adam (Kingma and Ba, 2014).

Hình 1 cung cấp hình minh họa về cách tiếp cận của chúng tôi. Và Pseudocode 1 trình bày chi tiết hơn về SignRound.

4 Thí nghiệm
Phần này trình bày đánh giá toàn diện về SignRound từ nhiều góc độ. Chúng tôi bắt đầu với tổng quan ngắn gọn về các kiến trúc LLM và các nhiệm vụ được bao gồm trong đánh giá của chúng tôi. Tiếp theo, chúng tôi cung cấp so sánh chi tiết giữa phương pháp của chúng tôi và một số cách tiếp cận hiện có, nhấn mạnh những ưu điểm độc đáo của SignRound. Hơn nữa, chúng tôi tiến hành các nghiên cứu ablation để củng cố hiệu quả của các lựa chọn của chúng tôi và điều tra độ nhạy của các siêu tham số. Cuối cùng, chúng tôi đánh giá khả năng tạo sinh của phương pháp chúng tôi trên các mô hình gần đây khác nhau. Các so sánh chi phí điều chỉnh được cung cấp trong Phụ lục A.

4.1 Cài đặt Thí nghiệm
Đánh giá và Nhiệm vụ. Chúng tôi đánh giá nhiều nhiệm vụ ngôn ngữ để giải quyết cài đặt task-agnostic. Cụ thể, chúng tôi trình bày kết quả độ chính xác trung bình cho 11 nhiệm vụ zero-shot, bao gồm HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), PIQA (Bisk et al., 2020), LAMBADA (Paperno et al., 2016), TruthfulQA (Lin et al., 2022), OpenBookQA (Mihaylov et al., 2018), BoolQ (Clark et al., 2019), RTE (Dagan et al., 2010), ARC-Easy, ARC-Challenge (Clark et al., 2018), và MMLU (Hendrycks et al., 2020). Chúng tôi sử dụng lm-eval-harness (Gao et al., 2023) cho tất cả các nhiệm vụ trên. Hơn nữa, chúng tôi bổ sung đánh giá của mình với phân tích perplexity (PPL) trên Wikitext2 (Merity et al., 2016), PTB (Marcus et al., 1993), và C4 (Raffel et al., 2020), theo cách thực hiện¹ của GPTQ và Wikitext2 (Merity et al., 2016) sử dụng lm-eval-harness (Gao et al., 2023). Tuy nhiên, chúng tôi lập luận rằng perplexity bị ảnh hưởng đáng kể bởi các outliers, như được minh họa trong Bảng 14 cho các thuật toán khác nhau. Tính nhạy cảm này có thể xuất phát từ biểu thức toán học PPL(X) = exp(−1/t∑ᵢ₌₁ᵗ log pθ(xᵢ|x<ᵢ)), trong đó việc gán xác suất thấp cho thậm chí một token có thể làm tăng đáng kể điểm perplexity. Do đó, chúng tôi ưu tiên độ chính xác của 11 nhiệm vụ được đề cập ở trên làm chỉ số chính, với dữ liệu perplexity phục vụ như tài liệu tham khảo bổ sung.

Cấu hình Lượng tử hóa. Phù hợp với GPTQ (Frantar et al., 2022), trọng tâm của chúng tôi cụ thể là lượng tử hóa chỉ trọng số, nhắm đến các lớp tuyến tính trong các khối transformer. Các lớp như lớp embedding và thường là lớp tuyến tính cuối cùng như 'lm-head' được loại trừ khỏi quá trình lượng tử hóa. Đánh giá của chúng tôi chủ yếu tập trung vào các cấu hình W4G-1, W4G128, W3G128 và W2G128, trong đó W4 chỉ ra lượng tử hóa trọng số với 4 bit và G đại diện cho nhóm chi tiết hơn như được mô tả trong (Park et al., 2022; Frantar et al., 2022). Chúng tôi áp dụng lượng tử hóa bất đối xứng. Để giảm thiểu overfitting trên các bộ dữ liệu WikiText và C4, đối với tất cả các phương pháp cần hiệu chuẩn, chúng tôi chọn ngẫu nhiên 512 mẫu hiệu chuẩn với cùng seed từ bộ dữ liệu pile-10k² có sẵn, bao gồm 10k mẫu đầu tiên từ pile (Gao et al., 2020). Chúng tôi đã sử dụng¹ https://github.com/IST-DASLab/gptq ²https://huggingface.co/datasets/NeelNanda/pile-10k độ dài chuỗi 2048 cho hiệu chuẩn, trong khi đối với các phương pháp khác, chúng tôi tuân thủ cài đặt chính thức của chúng.

Mô hình Ngôn ngữ Lớn. Chúng tôi so sánh các thuật toán khác nhau trên các mô hình thường được sử dụng như LLaMA-V1 (Touvron et al., 2023a), LLaMA-V2 (Touvron et al., 2023b), và Mistral-7B-v0.1 (Jiang et al., 2023). So sánh của chúng tôi bao phủ một phạm vi rộng các tham số LLM, từ 7B đến 70B, để đảm bảo phạm vi và phân tích toàn diện.

Siêu tham số SignRound. Trừ khi được nêu rõ ràng, quá trình điều chỉnh bao gồm việc điều chỉnh từng khối trong 200 bước với tốc độ học 5×10⁻³, kích thước batch là 8, và suy giảm tốc độ học tuyến tính. Ngoài ra, chúng tôi đã sử dụng automatic mixed precision (AMP) để tăng tốc việc điều chỉnh.

4.2 So sánh với Các Phương pháp Gần đây
Trong phần này, chúng tôi so sánh các phương pháp của chúng tôi với những phương pháp đã thể hiện kết quả đáng chú ý và không áp đặt chi phí bổ sung trên các mô hình được thử nghiệm trong lượng tử hóa chỉ trọng số cho LLMs, bao gồm GPTQ (Frantar et al., 2022), AWQ (Lin et al., 2023), HQQ (Badri and Shaji, 2023), OmniQuant (Shao et al., 2023) với phương pháp naïve RTN.

Để đảm bảo so sánh công bằng nhất có thể, chúng tôi đã kích hoạt act-order và true-sequential trong GPTQ và cũng kích hoạt static_group trong các tình huống với group_size. Ký hiệu GPTQ+ chỉ ra rằng chúng tôi đã điều chỉnh random seed hoặc tiền xử lý dữ liệu để giải quyết các vấn đề liên quan đến ma trận Hessian không định dương hoặc các vấn đề khác. Đối với OmniQuant(Shao et al., 2023), chúng tôi tuân thủ cài đặt chính thức, bao gồm chạy 20 epoch bao gồm cả W2G128 để tiết kiệm thời gian và vô hiệu hóa 'let'. Chúng tôi đã tiến hành các thử nghiệm hiệu chuẩn sử dụng kích thước mẫu 512 và 128, cũng như kích thước mẫu 512 với kích thước batch 4. Các phát hiện của chúng tôi cho thấy rằng việc sử dụng kích thước mẫu 512 thường dẫn đến hiệu suất tương đương hoặc cao hơn một chút cho các mô hình nhỏ hơn hoặc bằng 13B. Do đó, chúng tôi trình bày kết quả dựa trên kích thước mẫu 512. Đối với các mô hình 70B, do vấn đề mất mát Not a Number (NAN) và để giảm chi phí điều chỉnh của OmniQuant, chúng tôi đã áp dụng 128 mẫu cho hiệu chuẩn.

Chúng tôi trình bày kết quả tóm tắt của Mistral-7B và LLaMA V2 trong Bảng 1, kết quả chi tiết của Mistral-7B trong Bảng 2, và các kết quả chi tiết bổ sung được cung cấp trong Phụ lục C do hạn chế không gian. Tóm lại, cách tiếp cận của chúng tôi thể hiện hiệu suất vượt trội so với GPTQ (Frantar et al., 2022), đạt điểm 30/32, AWQ (Lin et al., 2023) với 27/32, HQQ (Badri and Shaji, 2023) với 15/16, và OmniQuant (Shao et al., 2023) với điểm 29/32 trên LLaMA V1/LLaMA V2/Mistral-7B trên các cài đặt lượng tử hóa khác nhau, bao gồm W4G-1, W4G128, W3G128, và W2G128. Các đánh giá này dựa trên độ chính xác trung bình của 11 nhiệm vụ zero-shot.

Đáng chú ý rằng khi độ sâu bit giảm, những ưu điểm của SignRound trở nên đáng chú ý hơn. Ví dụ, như được hiển thị trong Bảng 2, SignRound có thể mang lại cải thiện độ chính xác tuyệt đối trung bình từ 6.91% đến 33.22% ở W2G128. Hơn nữa, chúng tôi có thể nâng cao hiệu suất bằng cách điều chỉnh các siêu tham số của mô hình từ tám lựa chọn, ký hiệu là ours*. Các lựa chọn này bao gồm số bước (200, 1000), tốc độ học cắt ngắn trọng số (1.0/steps, 2.0/steps), và tùy chọn để kích hoạt hoặc vô hiệu hóa đầu vào đã lượng tử hóa, tức là sử dụng đầu ra từ khối đã lượng tử hóa trước đó hoặc khối gốc trước đó.

4.3 So sánh với Các Phương pháp Làm tròn
Trong phần này, chúng tôi tiến hành phân tích so sánh giữa SignRound, FlexRound(Lee et al., 2023), và AdaRound(Nagel et al., 2020). Đáng chú ý, trong thí nghiệm, không có triển khai chính thức nào có sẵn cho FlexRound và AdaRound cho LLMs. Do đó, chúng tôi tham khảo các triển khai³ ⁴ để biết thêm chi tiết. Tuy nhiên, điều quan trọng cần nhấn mạnh là do thiếu hỗ trợ AMP và các tối ưu hóa khác, việc triển khai chậm đáng kể, đặc biệt khi tuân thủ cài đặt chính thức, bao gồm điều chỉnh 5000 bước, như được trình bày trong Bảng 9. Do đó, so sánh của chúng tôi bị giới hạn ở các mô hình có kích thước 13B hoặc nhỏ hơn. Chúng tôi đặt tốc độ học là 2e-4 cho LLaMA-v2-7b và Mistral-7B, và 1e-4 cho LLaMA-v2-13b để phù hợp với cài đặt chính thức càng gần càng tốt. Như được hiển thị trong Bảng 3, SignRound đạt được kết quả tốt hơn chỉ trong 200 bước so với 5000 bước cần thiết bởi các phương pháp làm tròn khác.

4.4 Nghiên cứu Ablation
SignSGD so với Adam. Để xác thực hiệu quả của SignSGD, Bảng 4 so sánh nó với optimizer Adam (Kingma and Ba, 2014). SignSGD sử dụng tốc độ học cố định 5e-3 trong tất cả các thí nghiệm, bao gồm 200 bước, với suy giảm trọng số tuyến tính. Đối với Adam, chúng tôi khám phá các tốc độ học từ 2.5e-3 đến 2e-2. Chúng tôi chọn lượng tử hóa các mô hình 13B hoặc nhỏ hơn với W4G-1 do chi phí thí nghiệm. SignSGD thể hiện ưu điểm rõ rệt trong các chỉ số độ chính xác trung bình trên 11 nhiệm vụ, chứng minh ưu điểm độc đáo của signed gradient descent trong tình huống này.

Điều chỉnh Round và Weight Clip. Để xác thực đóng góp của điều chỉnh làm tròn và điều chỉnh cắt ngắn trọng số, chúng tôi đã tiến hành nghiên cứu ablation trên ba mô hình với hai cấu hình lượng tử hóa. Như được hiển thị trong Bảng 5, mỗi thành phần cung cấp lợi ích so với RTN, với điều chỉnh làm tròn mang lại ưu điểm lớn hơn. Tuy nhiên, khi kết hợp, điều chỉnh cắt ngắn trọng số đôi khi có thể dẫn đến độ chính xác thấp hơn trong một số trường hợp nhất định ở W2G128.

Độ nhạy Siêu tham số. Để xác thực độ nhạy của các siêu tham số trong SignRound, chúng tôi đã tiến hành nghiên cứu ablation về độ dài chuỗi cho hiệu chuẩn, số lượng mẫu cho hiệu chuẩn, kích thước batch điều chỉnh, các bước điều chỉnh, và tốc độ học điều chỉnh. Kết quả được trình bày trong Bảng 6. Nhìn chung, các siêu tham số mặc định của chúng tôi đạt được kết quả cân bằng.

4.5 Tổng quát hóa cho Các Mô hình Khác
Để đánh giá tính tổng quát của phương pháp chúng tôi trên LLMs, chúng tôi đánh giá SignRound trên các LLMs mainstream khác nhau như Gemma (Team et al., 2024), Phi (Li et al., 2023b), Mistral (Jiang et al., 2023), Mixtral (Jiang et al., 2024) và Llama3 (Touvron et al., 2024). Bảng 7 chứng minh rằng tất cả các mô hình int4 duy trì độ giảm độ chính xác trong phạm vi 1% so với độ chính xác FP16 hoặc BF16 bằng cách sử dụng 1000 bước điều chỉnh và các siêu tham số theo mô hình trong số 4 lựa chọn được chi tiết trong Phần 4.1. Kết quả chi tiết được cung cấp trong Phụ lục C. Đáng chú ý, các thí nghiệm tổng quát hóa đã sử dụng phiên bản cập nhật (0.4.0+) của lm-eval-harness (Gao et al., 2023) và các mô hình đã lượng tử hóa thực sự, có thể dẫn đến sự khác biệt nhỏ so với dữ liệu benchmark khác.

5 Kết luận
Trong bài báo này, chúng tôi giới thiệu SignRound, một cách tiếp cận hiệu quả và ngắn gọn để tối ưu hóa làm tròn trọng số trong lượng tử hóa các mô hình ngôn ngữ lớn. SignRound sử dụng signed gradient descent để điều chỉnh giá trị làm tròn và cắt ngắn trọng số trong 200 bước, hoàn thành lượng tử hóa LLAMA-V2-70B trong khoảng 2.5 giờ. Các thí nghiệm rộng rãi của chúng tôi cho thấy SignRound vượt trội hơn các phương pháp lượng tử hóa khác trên các mô hình và bit trọng số khác nhau trong phần lớn các tình huống. Ngoài ra, SignRound cho thấy khả năng tạo sinh hứa hẹn trong các mô hình gần đây và đạt được hiệu suất nâng cao thông qua điều chỉnh siêu tham số cụ thể của mô hình.

6 Hạn chế
Mặc dù có những ưu điểm, chúng tôi quan sát thấy một khoảng cách đáng chú ý trong hiệu suất độ chính xác cho lượng tử hóa bit cực thấp, đặc biệt với lượng tử hóa 2-bit, so với mô hình gốc. Thách thức này có thể được giải quyết bằng cách khám phá lượng tử hóa không đồng nhất và lượng tử hóa độ chính xác hỗn hợp, điều mà chúng tôi để dành cho nghiên cứu tương lai.

7 Tuyên bố Đạo đức
Nghiên cứu của chúng tôi nhằm thúc đẩy kiến thức trong lượng tử hóa LLM. SignRound sử dụng các mô hình mã nguồn mở và bộ dữ liệu có sẵn công khai, và không gắn liền với các ứng dụng cụ thể, chỉ cần các bước fine-tuning tối thiểu trên các mô hình gốc. Điều này đảm bảo rằng các chi tiết kỹ thuật của phương pháp chúng tôi không mang ý nghĩa đạo đức tiềm ẩn nào. Chúng tôi thừa nhận những đóng góp của những người tạo ra và duy trì các tài nguyên này và cung cấp trích dẫn đến các nguồn gốc.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài được dịch tương tự...]

[Các bảng và hình ảnh giữ nguyên format và được dịch caption/tiêu đề]
