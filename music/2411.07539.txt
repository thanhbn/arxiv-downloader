# 2411.07539.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/music/2411.07539.pdf
# File size: 10923289 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Harmonizing Pixels and Melodies: Maestro-Guided
Film Score Generation and Composition Style Transfer
Fan Qi
Tianjin University
of TechnologyLiFeng Ni
Tianjin University
of TechnologyChangSheng Xu
Institute of Automation,
Chinese Academy of Sciences
Abstract
We introduce a film score generation framework to harmonize visual pixels and
music melodies utilizing a latent diffusion model. Our framework processes film
clips as input and generates music that aligns with a general theme while offering
the capability to tailor outputs to a specific composition style. Our model directly
produces music from video, utilizing a streamlined and efficient tuning mechanism
on ControlNet. It also integrates a film encoder adept at understanding the film’s
semantic depth, emotional impact, and aesthetic appeal. Additionally, we introduce
a novel, effective yet straightforward evaluation metric to evaluate the originality
and recognizability of music within film scores. To fill this gap for film scores,
we curate a comprehensive dataset of film videos and legendary original scores,
injecting domain-specific knowledge into our data-driven generation model. Our
model outperforms existing methodologies in creating film scores, capable of
generating music that reflects the guidance of a maestro’s style, thereby redefining
the benchmark for automated film scores and laying a robust groundwork for
future research in this domain. The code and generated samples are available at
https://anonymous.4open.science/r/HPM.
1 Introduction
A film score - the original music accompanying a film - plays a pivotal role in enriching the film’s
emotional landscape, deepening narrative complexity, character arcs, and thematic exploration. Creat-
ing a film score requires a harmonious, multidisciplinary effort that includes composers, orchestrators,
musicians, sound engineers, and music editors working in concert to blend composition, arrangement,
and recording with the film’s visual narrative. Automating the film score production process through
artificial intelligence research represents a significant stride toward cost efficiency and innovation in
film score production.
Translating the visual modality into music has been a promising area in cross-modal generative
modeling [ 52;7;54;47]. Some works rely on Codebook [ 5] and have less flexibility to generate
novel sounds outside it. Some works use pre-defined symbolic musical representations such as MIDI
(Musical Instrument Digital Interface), REMI (revamped MIDI-derived events), and Piano-Roll that
can be autoregressively generated [ 8;7;10]. Nevertheless, they require an excellent MIDI synth
to render audio from generated MIDI and struggle to model timbre and expressiveness. Recently,
some works directly produce spectrograms based on diffusion models [ 9;11;30] with textual
prompts. Diffusion model-generated audio exhibits fidelity to prominent prompt features, including
genre, tempo, instrumentation, and mood, while also capturing the fine-grained semantics of the
prompt. Yu et al. [ 47] utilize a latent conditional diffusion probabilistic model to synthesize long-term
conditional waveforms and generate soundtracks for dances and sports scenarios. Recent work1
1https://huggingface.co/spaces/fffiloni/image-to-music-v2
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.arXiv:2411.07539v1  [cs.MM]  12 Nov 2024

--- PAGE 2 ---
involves translating visual content into textual descriptions [ 24;27], utilizing robust text-to-music
diffusion models [ 19;30;11] for music generation. However, this two-step process (visual-to-text
and then text-to-music) introduces additional complexity and points of potential error. Key visual
elements crucial for emotion conveyance, like colors, lighting, and composition, may be inadequately
represented in text. In contrast, a direct visual-to-music translation could more effectively capture and
convert these visual cues into musical elements, preserving the original emotional tone of the visuals.
While conceptually straightforward, generating music from film diffusion models faces notable chal-
lenges. 1) The field significantly lacks datasets that carefully pair film clips with their corresponding
music. Compiling such datasets is challenging and resource-intensive. 2) Achieving the thematic
musical pieces align with the film’s narrative and emotional tone presents a complex challenge,
introducing integration difficulties within the current frameworks of diffusion models. 3) There is an
absence of objective metrics to measure the quality of music generated for film clips, complicating
the evaluation of progress and the refinement of models.
To bridge the existing void in automated film scores, we establish a comprehensive dataset - Film-
ScoreDB. FilmScoreDB contains 32,520 film clip-music pairs, totaling 90.35 hours, featuring com-
positions from renowned film composers. This collection serves to infuse our data-driven diffusion
model with targeted domain-specific insights. We present HPM, a novel approach tailored for gener-
ating film scores and transferring composition styles. Leveraging a diffusion model, our framework
introduces a low-rank, parameter-efficient fine-tuning mechanism to ControlNet, complemented by a
film encoder designed to assimilate semantic, emotional, and aesthetic dimensions of film content.
Additionally, we introduce an enhanced metric incorporating originality and recognizability to assess
the quality of generated music. Utilizing the design above, our model framework adeptly generates
film scores, operating independently and under specific styles’ control, demonstrating a significant
advancement in film score generation. In conclusion, our main contributions are three-fold:
•We are the first to tackle the automatic film score and composition style transfer challenge,
concentrating on generating music for specific film segments.
•We introduce a novel benchmark that includes a comprehensive film score dataset, a refined
set of evaluation metrics, and a well-defined baseline model to foster subsequent research
efforts.
•Through comprehensive experimental analysis, our framework demonstrates exceptional
capability in generating film scores and in transferring composition styles. Our framework
outperforms existing methods across all evaluated metrics and sets a new benchmark for the
musical arts community.
2 Dataset
As shown in Tab. 1, diverse video-music datasets serve specialized research needs. Datasets fo-
cusing on dance [ 26] and skating [ 46] explore the alignment of music rhythm with body motion.
Table 1: Diverse Video-Music Datasets.
Dataset Video Content Size Open Video Total Duration
URMP [23] Percussion Performance 44 ✓ ✓ 1.3h
AtinPiano [34] Piano Performance 257 ✓ ✓ 17.2h
MUSIC [50] Performance Video 685 ✓ ✓ 45.7h
EmoMV [41] Music Video 5,986 ✓ ✓ 44.33h
MuVi [4] Music Video 811 ✓ ✓ 13.52h
SymMV [55] Music Video 1,140 ✓ × 76h
MVDB [36] Music Video 1,985 ✓ ✓ 16.54h
FS1000 [45] Skate Video 1,604 ✓ ✓ 89.1h
FisV [46] Skate Video 500 ✓ ✓ 23.6h
AIST++ [29] Dance Video 60 ✓ ✓ 4h
TikTok [28] Dance Video 445 ✓ ✓ 1.55h
FilmScoreDB Film Video 32,520 ✓ ✓ 90.35hConversely, music performance datasets [ 50]
aim to synchronize the human body and fin-
ger movements with multi-instrumental music.
Using music video datasets for video-based
music generation aligns with our goal of the-
matic and emotional resonance. However, mu-
sic video datasets differ significantly from film
score datasets, with fundamental differences in
objectives: Music video directors visually pro-
mote songs, whereas film score composers en-
hance a film’s emotional and narrative depth. To
further elaborate, there are some specific differ-
ences. Technical Complexity : Film scores require deep musical theory, orchestration, and narrative
alignment, contrasting with music videos’ emphasis on visual creativity and editing skills. Cultural
and Historical Significance : Film scores, drawing from classical music traditions, possess significant
historical depth and cultural significance, in contrast to music videos that are shaped by and contribute
to contemporary trends. Therefore, developing large-scale, high-quality datasets characterized by
renowned film composers can drive research on video music.
2

--- PAGE 3 ---
Figure 1: Illustration of our HPM framework. a) During the training stage, our model incorporates
the video feature as a global control input, alongside the local control signal of melody and dynamics.
b) During the inference stage, the local controls can be one composition style of one specific master,
guiding the Film Score ControlNet to produce a Mel-spectrogram, subsequently converted into audio
via a vocoder. c) The film encoder processes to extract emotional, semantic, and aesthetic embeddings,
enriching the model’s interpretative depth.
For this purpose, we compile FilmScoreDB, a comprehensive dataset featuring film video clips and
their corresponding legendary original scores. The collected FilmScoreDB contains 32,520 samples,
sourced from nearly 300 famous films worldwide, each 10 seconds long. We split FilmScoreDB into
a training set (26,730 pairs), a validation set (2,895 pairs), and a test set (2,895 pairs). We start by
gathering a list of films with Best Original Score Nominees. To obtain such paired data, we correctly
obtain and download about 300 films on platforms such as YouTube, Netflix, Disney+, Prime Video,
etc., invited five connoisseurs to watch, and extract film clips, including scores. In addition, each
data pair is labeled with relevant film score composers and styles. For this dataset, we only provide
relevant information about the films used, including film names, composers, score information, and
clip timestamps. We also evaluate our model using the EmoMV [ 41] dataset to compare it fairly with
prior work.
3 Methodology
Our overall goal is to learn a conditional generative model, p(s|cfilm, Cstyle), for film scores. Here,
cfilm represents the conditioning information obtained from a given film clip through the Film
Encoder (time-independent), and Cstyle is a set of music style control signals (melody, dynamic).
We incorporate the obtained music style control signals into a ControlNet, inspired by the Uni-
ControlNet [ 51], and optimize the model training using the low-rank adaptation (LORA) [ 17]. We
generate new film scores with different styles based on the conditioning cfilm, and the music style
control signals Cstyle during inference.
3.1 Film Encoder
We extract semantic, aesthetic, and emotional features from film videos as model guidance to mitigate
the complexity and computational demands of using raw video frames for music generation. Semantic
features are derived using the image encoder of CLIP [ 37], averaging extracted frame features to
capture video semantics. Aesthetic features, assessing visual qualities like lighting and color, are
obtained using a pre-trained TA V AR model [ 25], which evaluates visual attributes and theme-related
aesthetics through a combination of networks and feature mapping. For emotional features, we
employ the pre-trained WECL [ 49] model with the subjective nature of emotions in videos by
utilizing intra-modal and inter-modal relationships. We employ a lightweight attention feature fusion
method to integrate these diverse features effectively, optimizing music generation by learning fusion
3

--- PAGE 4 ---
weights for a harmonized feature combination. Due to space constraints, the detailed process for this
section is included in the supplementary materials.
3.2 Film Score ControlNet
To realize film score generation and Maestro-guided Composition Style Transfer, inspired by Uni-
ControlNet [ 51], we propose Film Score ControlNet, a unified framework that allows for the si-
multaneous utilization of different local controls (e.g., melody and dynamic) and global controls
(e.g., video semantic features, emotional features, and aesthetic features) in a single model, enabling
flexible and compositional style-controllable film score generation.
In this task, given the global video control cfilm and a set of local controls Cstyle that control the
music style, our overall objective is to learn a conditional generative model pθ(z0|cfilm, Cstyle)on
the compressed representation zof the music mel-spectrogram X.
3.2.1 Foundational Film Score Generation Model
We utilize a probabilistic generative model to estimate the true conditional data distribution
q(z0|cfilm), where the model distribution is denoted as pθ(z0|cfilm). Here, z0∈RC×T
r×F
rrepre-
sents the prior audio sample xin the latent space compressed from the mel-spectrogram X∈RT×F,
where rdenotes the compression level, Crepresents the channels of the latent representation, Tand
Frepresent the time and frequency dimensions of the mel-spectrogram X, respectively.
The diffusion model consists of two processes: a forward process that gradually adds a small amount
of high-level Gaussian noise to the sample z0overMsteps, and a corresponding backward process
that predicts the added noise during the forward process and eliminates the noise to recover the input
latent. To enhance the synchronization between video and music, we fine-tune the AudioLDM [ 30]
employing a dedicated video-to-music dataset, HIMV-200k [ 16]. This optimization transitions the
foundation model from a text-music space to a video-music space. We refer to this pre-trained model
asVM-Unet in the following.
3.2.2 Adding Style Controls to Foundational Model
As shown in Fig. 1, similar to ControlNet [ 51], we fix the weights of VM-Unet . We replicate the
structure and weights of the Down Block and Middle Block from VM-Unet and add some new
zero-convolution layers, called S-Control Branch . During the training stage, we take the music style
of melody and dynamic as the local control, and film feature as global control into S-Control Branch.
The compressed representation of the mel-spectrogram is fed into VM-Unet . Only the parameters of
theS-Control Branch are trainable.
Specifically, let f(x(m,l−1), m, c film, Cstyle)denote the lthblock of the S-Control Branch , where
mis the diffusion time step, x(m,l−1)contains the features of the noised latent after l−1blocks, and
cfilm,Cstyle are the global and local control, respectively. The style control is incorporated via:
ˇf(l)(xm,l−1, m, c film, Cstyle) :=Zout(fl(x(m,l−1)+Cstyle, m, c film)), (1)
where Zoutis the newly attached zero convolution layer and f(l)is initialized from the lthencoder
block of the pre-trained video-conditioned UNet.
For global control, cross-attention layers are employed to capture visual information cfilm from the
input film clip. In our setup, ztrepresents incoming noise features, Wq,Wk, and Wvare projection
matrices. The Q,K, and Vin cross-attention can be denoted as:
Q=Wq(zt), K=Wk(cfilm), V=Wv(cfilm). (2)
For local control, we start by aligning the melody and dynamic control signals with the input noise
features’ resolution using two convolutional blocks and a newly added zero convolution layer. Then,
we integrate them with the normalized input noise features through a shortcut in the following
sequence:
C=norm (zt) +Zin(conv mel(cmel)) +Zin(conv dyn(cdyn)), (3)
where Zinis also a new input zero convolution layer. For Melody (cmel∈RT×12), we adapt the
chromagram using a window size of 260 and a hop size of 160, condensing energy from Ffrequency
4

--- PAGE 5 ---
bins into 12 pitch classes. Then, we enhance it by selecting the most dominant pitch class through
argmax at each time step. For Dynamic (cdyn∈RT×1), we derive loudness by summing energy from
frequency bins in each time frame of the linear spectrogram and converting the values to decibels
(dB), which aligns closely with human perception of loudness. To reduce rapid fluctuations caused
by note onsets or percussion, and to align our dynamic control with perceived musical intensity, we
smooth the values by using a second-order context window, namely the Savitzky-Golay filter [42].
3.2.3 Low-Rank Adaptation For Improving Training
Despite ControlNet being half the size of the stable diffusion model, it still possesses too many
parameters for consumer GPUs to handle efficiently. The LoRA [ 17] is a low-resource fine-tuning
approach for large models. Given a pre-trained model layer with weights W0∈Rd×k, where dis the
input dimension and kis the output dimension, LoRA decomposes ∆Was:
∆W=BA, (4)
where B∈Rd×randA∈Rr×kwithr≪min(d, k).min(d, k)is a small rank that constrains the
update to a low dimensional subspace. By freezing W0and only optimizing the smaller matrices A
andB, LoRA achieves massive reductions in trainable parameters. During inference, ∆Wcan be
merged into W0with no overhead by a LoRA scaling factor α:
W=W0+α∆W. (5)
Subsequently, we inject trainable layers (low-rank decomposition matrices) into each transformer
block in the S-Control Branch . Experimental results demonstrate that utilizing LoRA to fine-
tune enhances the efficiency of the process, resulting in a faster performance with a reduction in
computational demand.
3.2.4 Inference
During inference, the condition of local style control is set to zero for film score generation. For
composition style transfer, we can control the style based on melody, dynamics, or both, allowing for
flexible adaptation of the composition’s style to the desired characteristics.
4 Experiments
This section provides a detailed evaluation of our Harmonized Pixel and Melody Film Score Diffusion
(HPM) model, integrating pixel-level and melody-level aspects. We employ objective and subjective
metrics on two different datasets to assess our model. To evaluate generated music’s originality,
we introduce the originality vs. recognizability framework [ 2]. Additionally, we demonstrate the
effectiveness of using LoRA for training acceleration within our framework.
4.1 Implementation Details
Considering the computational efficiency, we use AudioLDM [ 30] as our basic backbone. We fix the
parameters of Music V AE in the AudioLDM. For all visual features, we use a frame rate of 10 fps,
following the standard on Film Score (ours) and EmoMV . We use the default hyper-parameters of
AudioLDM and employ a base learning rate of 0.0001 . The film score generation diffusion model
training over 200,000gradient steps with a batch size of 2is executed on 4NVIDIA GeForce RTX
3090 GPUs, taking approximately 40hours to complete over 60epochs. We apply the AdamW
optimizer [ 32] with β1= 0.9,β2= 0.999, and a weight decay of 0.01. During inference stage,
we adopt DDIM [ 39] for sampling, with the number of timesteps set to 200and the classifier free
guidance scale [ 15] set to 7.5. We use the L2 loss and AdamW optimizer for fine-tuning our Film
Score ControlNet until convergence for 12hours, using 4NVIDIA GeForce RTX 3090 GPUs. During
inference stage, we use 100-step DDIM sampling and only use classifier-free guidance on global
video control. Due to the constraints of paper length, details regarding the film encoder are provided
in the supplementary materials.
5

--- PAGE 6 ---
4.2 Evaluation Methods
4.2.1 Baseline
For a comprehensive evaluation of our HPM framework, we conduct comparative experiments on
two tasks. For film score generation , we select seven high-performing methods with available
code as baselines. Specifically, we compare our HPM with LORIS [ 47], D2M-Gan model [ 53],
CDCD model [ 54] based on Codebook, and DIFF-Foley model [ 33] based on Mel-Spectrogram, we
employ the official implementation. We further fine-tune a video-to-music generation model, dubbed
Tango-VM, building upon the text-to-music model, Tango [ 11]. Due to the difficulty of converting
film music into MIDI without annotations, we exclude MIDI-generation-based methods, such as
CMT [ 7], Foley Music [ 10], Audeo [ 40], and Video2Music [ 20] from our baseline comparisons. For
Composition Style Transfer , we compare our model with the DITTO [ 35] trained with melody and
dynamics controls. In EmoMV , each emotion is designated as a distinct style. Similarly, each film
score style within FilmScoreDB is identified as a unique style. The DITTO model realizes text-driven
music stylization, leveraging melody and intensity conditioning through inference-time optimization
of the initial noise latent space in a pre-trained text-to-music diffusion model. Inspired by DITTO,
we evaluate our VM-net model across three scenarios: (i) melody-only, excluding melodies, (ii)
dynamics-only, excluding dynamics, and (iii) combined melody and dynamic control.
4.2.2 Evalation Metric
Objective : To assess rhythm correspondence in our study, we utilize modified versions of the Beats
Coverage Score (BCS) [ 6], Beats Hit Score (BHS) [ 22], and F1 scores for a thorough evaluation
of rhythm accuracy, CSD and HSD (the standard deviations of BCS and BHS) to evaluate gener-
ation stability. For music quality, the Frechet Audio Distance (FAD) [ 21], Inception Score, and
Kullback-Leibler (KL) [ 13] divergence are employed, leveraging the VGGish classifier for FAD
to measure similarity and PANN for IS and KL to evaluate quality. For style transfer, we measure
Melody Accuracy and Dynamics Correlation [ 44], analyzing the alignment of pitch classes and the
Pearson correlation of dynamics between input controls and generated output, with micro and macro
correlations providing insights into dynamic control fidelity and consistency across generations.
Subjective : In addition, we also used the subjective measure of Mean Opinion Score (MOS) in the
video-to-music generation task. MOS is obtained by calculating the average score of user research on
overall music quality. Here, we recruit 20 participants to listen to 30 pieces of music generated by
HPM and five baseline methods (5 samples per method) and then evaluate the overall music quality
by giving scores ranging from 1 to 5 in terms of music quality, clarity, and presence or absence of
delay.
4.2.3 The Originality vs. Recognizability Framework
Figure 2: Originality vs. Recognizability Comparison.
The big data point represents a single model, encapsulating
the average scores of originality and recognizability across
all categories, while the surrounding smaller data points
illustrate the scores within each category for that model.Originality within film scoring is a crit-
ical metric, necessitating the creation
of compositions that exhibit distinctive-
ness when compared with prior back-
ground music. This principle under-
scores the importance of innovation in
the composition process, ensuring that
each score contributes a unique audi-
tory experience that enhances the nar-
rative and emotional depth of the film.
Therecognizability of a film score refers
to its capacity to be discerned by audi-
ences as embodying a specific musical
style, thereby facilitating the identifica-
tion of the score’s stylistic lineage or
intent. This attribute is pivotal in eval-
uating the effectiveness of a score in
invoking intended emotional responses
or thematic associations. To compute the Originality , we use the following equation, σρ=
6

--- PAGE 7 ---
Table 2: Quantitative evaluation results for the film score generation task on the FilmScoreDB and
EmoMV datasets.
Dataset MethodRhythm Correspondence Music Quality Generate Stability Subjective Evaluation
BCS ↑BHS ↑F1 scores ↑IS↑KL↓FAD ↓CSD ↓ HSD ↓ Mos↑
D2M-Gan 60.1 61.1 62.2 1.3 5.5 14.3 24.1 25.7 2.9
CDCD 57.2 60 61.1 1.6 8.6 15.6 22.4 23.8 2.7
FilmScoreDBTango-VM 62.1 57.3 61.4 3.7 6.7 10.8 25.4 22.5 3.3
LORIS 58.3 50.1 60.3 4.0 5.7 7.5 20.4 22.3 3.1
DIFF-Foley 64.2 59.7 61.6 3.3 5.4 8.3 19.1 18.7 3.4
HPM 66.7 62.2 64.4 4.4 5.3 7.1 18 16.2 3.9
D2M-Gan 62.4 63.2 65.4 1.8 5.2 12.1 22.5 23.2 3,3
CDCD 59.3 62.6 63.2 1.9 7.3 13.7 21.3 20.9 3.1
EmoMVTango-VM 64.5 59.4 63.7 4.3 5.9 8.7 23.5 20.3 3.6
LORIS 59.5 52.3 63.8 4.5 5.4 7.5 19.3 20.6 3.0
DIFF-Foley 67.1 60.2 64.4 3.7 5.6 9.4 18.2 17.5 3.5
HPM 70.1 64.3 66.2 5.2 4.9 7 16.7 15.6 3.8
Pnc
j=1r
1
N−1PN
i=1
f(mj
i)−1
NPN
i=1f(mj
i)2
, where ncdenotes class number, f(mj
i)denotes
extract feature for mel-spectrogram mj
ifrom pre-trained model. To evaluate recognizability , we use
a one-shot classification model designed for musical styles, measuring the classification accuracy
of each generated sample. The average accuracy across the test set is used as the recognizability
metric. Fig. 2 presents a comparison between our approach and these baselines in terms of originality
and recognizability. In comparison, D2M-Gan [ 53] and CDCD [ 54] learned to make identical music
of the specific style (i.e., low originality but high accuracy). Despite their proficiency in accurately
learning the distribution of the Ground Truth, which results in high recognizability, they fall short in
generating new music compositions. This suggests that both diffusion models and GANs are unable
to overcome the limitations imposed by the Codebook. The waveform-based LORIS [ 47], by learning
from the most original representation of music, is capable of continually creating its own music.
However, this leads to the generated music deviating from the distribution of Ground Truth, resulting
in lower recognizability. Our model, along with Tango-VM, effectively balances the relationship
between originality and recognizability, ensuring a favorable outcome in both innovation and stylistic
expression in music. Since Tango [ 11] has larger parameters and achieves higher spectral resolution,
it results in greater creativity compared with our model.
4.3 Main Results
4.3.1 Film Score Generation
The quantitative experimental results of our film score generation model are shown in Tab. 2. Our
model outperforms all baselines in terms of rhythm consistency, music quality, and generation
stability. The baselines are designed for dance/sport music generation, focusing on modeling the
video pose/motion to the rhythm of music. It is not suitable for film music with semantic and
emotion serveing as bridges to film and score. To be noted, LORIS [ 47] method has the lower BHS
and F1 scores compared with other methods. The extraction of critical pose features, as a crucial
visual rhythm condition for LORIS [ 47], poses challenges within our FilmDB due to the variability
and complexity of tasks and scene transitions characteristic of film videos. Therefore, baselines
are designed for dance/sport music generation by aligning video motion to rhythm, fail to cater to
film music’s unique requirements, where semantic and emotional connections are essential, making
them unsuitable for our film scoring context. Nevertheless, the waveform-based LORIS method
outperforms CDCD [ 54] and D2M-Gan [ 53] , as well as the spectrum-based Tango-VM, in terms of
overall music quality scores. This suggests that waveforms synthesized by the generative model better
handle musical details, ensuring an improvement in music quality. From Tab. 2, we observe that all
methods perform well in terms of rhythm consistency on the MV dataset, indicating that films present
more challenging rhythm scenarios. Nonetheless, our model still achieves significant improvements
compared with other methods on both datasets. These results validate that our framework can
consistently generate high-quality music tracks with accurate rhythm alignment for both films and
music videos.
In Fig. 3, we present examples of generating film scores, showcasing the video frames along with the
corresponding emotional and aesthetic scores and the melody and dynamic of the generated music.
7

--- PAGE 8 ---
Figure 3: Examples of our Film Score Generation, including video frame (top), emotion & aesthetic
score (middle), and generated melody & dynamic (bottom).
Table 3: Quantitative evaluation results for the composition style transfer task on FilmDB and
EmoMV datasets.
Dataset Control Model Melody acc (%)Dynamics corr(r, in %) Music Quality
Micro Macro IS↑KL↓FAD ↓
Melody DITTO 51.3 1.7 3.2 4.8 4.2 6.1
only Ours 57.6 2.8 3.9 5.8 5.4 7.4
FilmScoreDBDynamics DITTO 4.5 54.3 79.3 5.1 4.5 5.9
only Ours 8.4 60.3 84.7 5.4 5.3 7.2
Melody DITTO 52.3 61.7 82.4 5.4 4.8 6.7
Dynamics Ours 57.7 64.5 87.2 6.1 5.6 7.9
Melody DITTO 52.5 2.4 5.2 5.3 4.7 6.8
only Ours 58.2 3.6 5.1 6.4 5.2 7.9
EmoMVDynamics DITTO 4.8 55.3 79.9 5.7 5.3 6.4
only Ours 9.6 63.4 86.8 6.2 5.4 7.7
Melody DITTO 53.4 62.5 83.4 6 5.4 7.2
Dynamics Ours 58.6 66.3 89.1 6.7 5.6 8.3
Emotion and aesthetics represent two distinct feature dimensions for characterizing films, with the
former reflecting the perceived strength of emotion conveyed by the subjects or the scene, and the
latter can refer to the overall composition, balance, and visual appeal of the frame. The generated
dynamics and emotional intensity values are positively correlated. Emotion and aesthetics collectively
determine the melody.
4.3.2 Composition Style Transfer
The quantitative experimental results of our composition style transfer are shown in Tab. 3. We estab-
lish two style-controlled test sets from the FilmScoreDB and EmoMV datasets. The FilmScoreDB
test set categorizes music by film genres (e.g., Plot, Love and Action) and selects control signals of
melody and dynamics from different genres to form a diverse control set. In contrast, the EmoMV
test set, with six emotion categories, uses a similar approach but increases the control samples from
2 to 10 per category to enrich the experimental control set, aiming to assess the model’s ability to
generate music across varied styles and emotional tones. Under three different conditions, although
there is a significant difference in the quality of generated outputs between the two methods, our
control scores are better than DITTO’s method. Additionally, HPM more accurately responds to
melody and dynamic controls. This suggests that the optimization-based approach during inference
is better suited for accurately controlling melody and dynamics in complex music compared with
DITTO’s method. In Fig. 4, we display examples of outputs from models under the control of styles
(melody and dynamics) extracted from target music. Comparing the mel spectrograms, melody,
and dynamic structures of original, generated, and target music reveals that while there may not be
complete overlap, the comparison sufficiently shows our model’s ability to generate new music in a
distinct style from the Ground Truth under specified controls.
8

--- PAGE 9 ---
Figure 4: Examples of our composition style transfer, including spectrogram (top), melody (middle),
and dynamic (bottom).
Table 4: The results of the ablation study of Film Encoder and the comparative experiment of the
accelerated with LORA on the FilmScoreDB dataset.
(a) The impact of LoRA.
Method Trainable Parameters Train Time
HPM w/o LORA 87M 48 hours
HPM w/ LORA 20M 12 hours
Method Melody acc(%)Dynamics corr(r,in %)
Micro Macro
HPM w/o LORA 57.7 64.5 87.2
HPM w/ LORA 57.3 64.7 87.6(b) Different Film Encoder conditions.
MethodMusic Quality Generate Stability
IS↑KL↓FAD ↓CSD ↓ HSD ↓
HPM w/ S,A 4.6 5.5 7.3 18.1 16.1
HPM w/ A,E 4.5 5.4 7.2 17.9 16.0
HPM w/ E,S 4.5 5.4 7.2 18.0 16.1
HPM w/ S 4.5 5.4 7.2 17.8 16.0
HPM w/ E 4.6 5.5 7.3 18.0 16.1
HPM w/ A 4.7 5.6 7.4 18.3 16.2
HPM 4.4 5.3 7.1 18 16.2
4.3.3 Low-rank Adaptation for Accelerated Training
To prove the efficiency of LORA, we evaluate two model variants: HPM with/without LORA in
Tab. 4b, noting differences in trainable parameters and training durations. Remarkably, HPM with
LORA reduces parameters from 87 million to 20 million and cuts training time from 48 to 12 hours
without compromising performance. HPM without LORA marginally outperformed in melody
accuracy, while the HPM with LORA has slightly better dynamics correlation scores. This indicates
LORA’s efficacy in streamlining the model and quickening training while maintaining or slightly
improving dynamics assessment.
4.4 Ablation Study
We conduct an ablation study to test the influence of film encoder and film score ControlNet on
the FilmScoreDB dataset. For the film encoder, we test different combinations of visual conditions
(Semantic, Aesthetic, Emotion) on the performance of the HPM model in Tab. 4a. Each row represents
a variant of the model with specific combinations of visual conditions. We observe that incorporating
different combinations of visual conditions leads to variations in performance across all evaluation
metrics compared with the complete model (HPM). Models with combined visual conditions generally
exhibit higher performance compared with models with individual visual condition. The performance
of the HPM model with different combinations of visual conditions remains relatively consistent
across various metrics, indicating that each combination contributes to enhancing music generation
tasks. Overall, these findings highlight the importance of considering multiple visual conditions
simultaneously to achieve optimal performance in music generation tasks using the HPM model.
5 Discussion
In this paper, we investigate the task of automatic film scoring through the introduction of a
comprehensive 90.35-hour film-music dataset, alongside the establishment of a baseline model
(HPM) and an evaluative metric focus on Originality vs. Recognizability . We acknowledge certain
constraints and delineate avenues for future enhancement: A primary constraint of our current
framework is rigidity in producing music of fixed durations, limiting its adaptability to diverse
emotional expressions, narrative forms, or situational demands. We intend to adjust sampling rates
and density to vary output lengths while preserving the music’s integrity.
9

--- PAGE 10 ---
References
[1]Bar-Tal, O., Yariv, L., Lipman, Y ., Dekel, T.: Multidiffusion: Fusing diffusion paths for
controlled image generation (2023)
[2]Boutin, V ., Singhal, L., Thomas, X., Serre, T.: Diversity vs. recognizability: Human-like
generalization in one-shot generative models. Advances in Neural Information Processing
Systems 35, 20933–20946 (2022)
[3]Çano, E., Morisio, M., et al.: Music mood dataset creation based on last. fm tags. In: 2017
International Conference on Artificial Intelligence and Applications, Vienna, Austria. pp. 15–26
(2017)
[4]Chua, P., Makris, D., Herremans, D., Roig, G., Agres, K.: Predicting emotion from music videos:
exploring the relative contribution of visual and auditory information to affective responses.
arXiv preprint arXiv:2202.10453 (2022)
[5]Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y ., Défossez, A.: Simple
and controllable music generation. arXiv preprint arXiv:2306.05284 (2023)
[6]Davis, A., Agrawala, M.: Visual rhythm and beat. ACM Transactions on Graphics (TOG) 37(4),
1–11 (2018)
[7]Di, S., Jiang, Z., Liu, S., Wang, Z., Zhu, L., He, Z., Liu, H., Yan, S.: Video background music
generation with controllable music transformer. In: Proceedings of the 29th ACM International
Conference on Multimedia. pp. 2037–2045 (2021)
[8]Dong, H.W., Hsiao, W.Y ., Yang, L.C., Yang, Y .H.: Musegan: Multi-track sequential generative
adversarial networks for symbolic music generation and accompaniment. In: Proceedings of the
AAAI Conference on Artificial Intelligence. vol. 32 (2018)
[9]Forsgren, S., Martiros, H.: Riffusion - Stable diffusion for real-time music generation (2022),
https://riffusion.com/about
[10] Gan, C., Huang, D., Chen, P., Tenenbaum, J.B., Torralba, A.: Foley music: Learning to generate
music from videos. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XI 16. pp. 758–775. Springer (2020)
[11] Ghosal, D., Majumder, N., Mehrish, A., Poria, S.: Text-to-audio generation using instruction-
tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731 (2023)
[12] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition. pp. 770–778
(2016)
[13] Hershey, S., Chaudhuri, S., Ellis, D.P., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M.,
Platt, D., Saurous, R.A., Seybold, B., et al.: Cnn architectures for large-scale audio classification.
In: 2017 ieee international conference on acoustics, speech and signal processing (icassp). pp.
131–135. IEEE (2017)
[14] Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y ., Cohen-Or, D.: Prompt-to-
prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022)
[15] Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022)
[16] Hong, S., Im, W., Yang, H.S.: Content-based video-music retrieval using soft intra-modal
structure constraint. arXiv preprint arXiv:1704.06761 (2017)
[17] Hu, E.J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., Chen, W.: Lora:
Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)
[18] Hu, F., Chen, A., Wang, Z., Zhou, F., Dong, J., Li, X.: Lightweight attentional feature fusion:
A new baseline for text-to-video retrieval. In: European Conference on Computer Vision. pp.
444–461. Springer (2022)
10

--- PAGE 11 ---
[19] Huang, Q., Park, D.S., Wang, T., Denk, T.I., Ly, A., Chen, N., Zhang, Z., Zhang, Z., Yu, J.,
Frank, C., et al.: Noise2music: Text-conditioned music generation with diffusion models. arXiv
preprint arXiv:2302.03917 (2023)
[20] Kang, J., Poria, S., Herremans, D.: Video2music: Suitable music generation from videos using
an affective multimodal transformer model. arXiv preprint arXiv:2311.00968 (2023)
[21] Kilgour, K., Zuluaga, M., Roblek, D., Sharifi, M.: Fréchet audio distance: A reference-free
metric for evaluating music enhancement algorithms. In: INTERSPEECH. pp. 2350–2354
(2019)
[22] Lee, H.Y ., Yang, X., Liu, M.Y ., Wang, T.C., Lu, Y .D., Yang, M.H., Kautz, J.: Dancing to music.
Advances in neural information processing systems 32(2019)
[23] Li, B., Liu, X., Dinesh, K., Duan, Z., Sharma, G.: Creating a multitrack classical music
performance dataset for multimodal music analysis: Challenges, insights, and applications.
IEEE Transactions on Multimedia 21(2), 522–535 (2018)
[24] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)
[25] Li, L., Huang, Y ., Wu, J., Yang, Y ., Li, Y ., Guo, Y ., Shi, G.: Theme-aware visual attribute
reasoning for image aesthetics assessment. IEEE Transactions on Circuits and Systems for
Video Technology (2023)
[26] Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance
generation with aist++. In: Proceedings of the IEEE/CVF International Conference on Computer
Vision. pp. 13401–13412 (2021)
[27] Li, Y ., Wang, C., Jia, J.: Llama-vid: An image is worth 2 tokens in large language models. arXiv
preprint arXiv:2311.17043 (2023)
[28] Lin, T.Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.:
Microsoft coco: Common objects in context. In: Computer Vision–ECCV 2014: 13th European
Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740–755.
Springer (2014)
[29] Lin, Y .B., Tseng, H.Y ., Lee, H.Y ., Lin, Y .Y ., Yang, M.H.: Exploring cross-video and cross-
modality signals for weakly-supervised audio-visual video parsing. Advances in Neural Infor-
mation Processing Systems 34, 11449–11461 (2021)
[30] Liu, H., Chen, Z., Yuan, Y ., Mei, X., Liu, X., Mandic, D., Wang, W., Plumbley, M.D.: Audioldm:
Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503 (2023)
[31] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B.: Swin transformer:
Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF
international conference on computer vision. pp. 10012–10022 (2021)
[32] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017)
[33] Luo, S., Yan, C., Hu, C., Zhao, H.: Diff-foley: Synchronized video-to-audio synthesis with
latent diffusion models. Advances in Neural Information Processing Systems 36(2024)
[34] Moryossef, A., Elazar, Y ., Goldberg, Y .: At your fingertips: Automatic piano fingering detection
(2019)
[35] Novack, Z., McAuley, J., Berg-Kirkpatrick, T., Bryan, N.J.: Ditto: Diffusion inference-time
t-optimization for music generation. arXiv preprint arXiv:2401.12179 (2024)
[36] Pandeya, Y .R., Lee, J.: Deep learning-based late fusion of multimodal information for emotion
classification of music video. Multimedia Tools and Applications 80, 2887–2905 (2021)
11

--- PAGE 12 ---
[37] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,
A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language
supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021)
[38] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis
with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. pp. 10684–10695 (2022)
[39] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 (2020)
[40] Su, K., Liu, X., Shlizerman, E.: Audeo: Audio generation for a silent performance video.
Advances in Neural Information Processing Systems 33, 3325–3337 (2020)
[41] Thao, H.T.P., Roig, G., Herremans, D.: Emomv: Affective music-video correspondence learning
datasets for classification and retrieval. Information Fusion 91, 64–79 (2023)
[42] Virtanen, P., Gommers, R., Oliphant, T., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E.,
Peterson, P., Weckesser, W., Bright, J., et al.: Fundamental algorithms for scientific computing
in python and scipy 1.0 contributors. scipy 1.0. Nat. Methods 17, 261–272 (2020)
[43] Wallace, B., Gokul, A., Ermon, S., Naik, N.: End-to-end diffusion latent optimization improves
classifier guidance. In: ICCV. pp. 7246–7256. IEEE (2023)
[44] Wu, S.L., Donahue, C., Watanabe, S., Bryan, N.J.: Music controlnet: Multiple time-varying
controls for music generation. arXiv preprint arXiv:2311.07069 (2023)
[45] Xie, P., Zhang, Q., Li, Z., Tang, H., Du, Y ., Hu, X.: Vector quantized diffusion model with
codeunet for text-to-sign pose sequences generation. arXiv preprint arXiv:2208.09141 (2022)
[46] Xu, C., Fu, Y ., Zhang, B., Chen, Z., Jiang, Y .G., Xue, X.: Learning to score figure skating sport
videos. IEEE transactions on circuits and systems for video technology 30(12), 4578–4590
(2019)
[47] Yu, J., Wang, Y ., Chen, X., Sun, X., Qiao, Y .: Long-term rhythmic video soundtracker. In:
International Conference on Machine Learning (ICML) (2023)
[48] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models
[49] Zhang, Z., Wang, L., Yang, J.: Weakly supervised video emotion detection and prediction
via cross-modal temporal erasing network. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 18888–18897 (2023)
[50] Zhao, H., Gan, C., Rouditchenko, A., V ondrick, C., McDermott, J., Torralba, A.: The sound of
pixels. In: Proceedings of the European conference on computer vision (ECCV). pp. 570–586
(2018)
[51] Zhao, S., Chen, D., Chen, Y .C., Bao, J., Hao, S., Yuan, L., Wong, K.Y .K.: Uni-controlnet: All-
in-one control to text-to-image diffusion models. Advances in Neural Information Processing
Systems 36(2024)
[52] Zhao, S., Yao, H., Wang, F., Jiang, X., Zhang, W.: Emotion based image musicalization. In:
2014 IEEE International conference on multimedia and expo workshops (ICMEW). pp. 1–6.
IEEE (2014)
[53] Zhu, Y ., Olszewski, K., Wu, Y ., Achlioptas, P., Chai, M., Yan, Y ., Tulyakov, S.: Quantized
gan for complex music generation from dance videos. In: European Conference on Computer
Vision. pp. 182–199. Springer (2022)
[54] Zhu, Y ., Wu, Y ., Olszewski, K., Ren, J., Tulyakov, S., Yan, Y .: Discrete contrastive diffusion
for cross-modal music and image generation. In: The Eleventh International Conference on
Learning Representations (2022)
[55] Zhuo, L., Wang, Z., Wang, B., Liao, Y ., Bao, C., Peng, S., Han, S., Zhang, A., Fang, F., Liu, S.:
Video background music generation: Dataset, method and evaluation. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 15637–15647 (2023)
12

--- PAGE 13 ---
Table 5: Overview of the video2music method.
Model Music Representation Architecture Mertics Dataset
Foley Music [10] MIDI Transformer NDB URMP,Atin Piano,MUSIC
CMT [7] MIDI Transformer PCHE,GPS,ST LPD
D2M-Gan [53] Codebook Gan BCS,BHS,Genre Accurary,MOS AIST++,TikTok
CDCD [54] Codebook Diffusion BCS,BHS,Genre Accurary,MOS AIST++,TikTok
V-Musprod [55] MIDI Transformer VMCP,SC,PE,PCE,EBR,IOI SymMV
LORIS [47] Waveform Diffusion MOS,BCS,BHS,F1 Scores FisV ,AIST++,FS1000,Finegym
Video2Music [20] MIDI Transformer Hits@k scores,EML MuVi-sync
DIFF-Foley [33] Mel-Spectrogram Diffusion IS,FID,MKL,Align Accs VGGSound,AudioSet
Ours Mel-Spectrogram ControlNet MOS,BCS,BHS,F1 Scores,FAD,KL,OR FilmScoreDB(ours),EmoMV
Figure 5: Top 20 composers data distribution
 Figure 6: Top 20 movie styles data distribution
A Related Work
A.1 Video2Music Generation
As illustrated in Tab. 5, there has been some progress in the video-to-music generation task. Some
works [ 10;7;55;20] are based on the transformer architecture to generate the MIDI file for music
generation autoregressively. However, the limitation lies in the simplicity of the music recorded in the
MIDI file, which prevents the model from generating complex music. Some pipelines [ 53;54] use the
Codebook as music representation but are limited by their diversity due to the Codebook. LORIS [ 47]
is a long-term music generation framework that utilizes a Latent Diffusion Model [ 38], mainly focused
on figure skating and dance video soundtracks, and lacks other application scenarios. However,
DIFF-Foley [ 33] utilizes the temporal and semantic alignment features learned through contrastive
audio-visual pre-training (CA VP) as conditions and then uses LDM to generate the Mel-spectrogram.
It effectively demonstrates the ability of the diffusion model and Mel-spectrogram representation in
the video-to-music task. Unlike prior works, our framework is capable of automatically generating
complex film scores at affordable costs.
A.2 Controllable Generation
Although conditional guided generation has achieved impressive results in diffusion, text or video
alone cannot finely control the generated content, so controllable generation is gradually emerging.
ControlNet [48] uses lightweight adapters to incorporate conditional inputs (Canny edges, Hough
lines, user scribbles, etc.) into latent diffusion models, which can better control the image generation
process and generate more specific and compliant images. Uni-ControlNet [51] can accept multiple
pixel-level controls through a single adapter branch without specifying all controls simultaneously,
while ControlNet requires a separate adapter branch for each control. Meanwhile, Inference-time
Guidance-based [ 14;1]/Optimization-based [ 43;35] Control allows for real-time adjustments in
the direction of the generative process, offering users greater control over the output.
13

--- PAGE 14 ---
Figure 7: Illustration of samples in FilmScoreDB. The information displayed for each sample
includes the movie title, music score title, and composer.
B Dataset
B.1 FilmScoreDB
We annotate 280 films and and obtain 4887 raw film segments. The duration of these segments
ranges from 5 seconds to 18 minutes. For our film score generation, we discard segments less than
10 seconds in duration and split those greater than 10 seconds into 10-second intervals. Our dataset
covers 134 renowned composers, including Hans Zimmer, John Williams, and Danny Elfman, among
others. These composers are universally recognized as top film composers and their works frequently
receive nominations for Oscars and Grammys, which ensures the high quality of our film scoring
dataset. In Fig. 5, we present the distribution of sample data for the top twenty composers. The
distribution demonstrates a long-tail effect, encompassing composers who have fewer but highly
classic works. This diversity assists the model in learning film score across various styles and
techniques. In Fig. 6, we depict the distribution of sample data for the top twenty film genres. Our
dataset encompasses a wide range of film genres, including plot, action, love, and more. Typically,
each film has multiple film genre annotations. Although dominant genres have a higher proportion,
the long-tail effect ensures the inclusion of numerous niche film genres, facilitating the capture of
more subtle differences and challenging scenarios. Overall, our dataset is sizable, with substantial
data for top composers and film genres, providing ample support for model training.
In Fig. 7, we present a visualization of our FilmScoreDB dataset, which comprises video-music
pairs. Each sample contains not only the movie title, audio, video, music score title, and composer,
but also the corresponding movie genre label. Our dataset encompasses a diverse range of movie
categories. At the same time, we also eliminate all vocal content from the audio data using Demucs2
to enhance the quality of the model-generated outputs. Demucs is a highly effective Music Source
Separation model that separates the input music into accompaniment and vocal files. Here, we select
the accompaniment as our final audio data. We do not own any copyrights to the films and music
referenced in the dataset.
B.2 HIMV and EmoMV
The HIMV dataset[ 16] is a large-scale music video dataset that provides approximately 100k YouTube
links. We utilize these links to download around 40k music videos with Yt-dlp3. Subsequently,
2https://github.com/facebookresearch/demucs
3https://github.com/yt-dlp/yt-dlp
14

--- PAGE 15 ---
we select two 10-second segments from the middle of each video to construct an approximate 80k
video-music dataset. These data are preprocessed using Demucs and used to train our VM-Net.
The EmoMV dataset consists of data from the MVED dataset[36], the Music Mood Dataset[3], and
some original music videos from movies collected on YouTube. In total, the dataset includes 5,370
30s data points and 616 10s data points. Subsequently, we clip the 30s data into three segments,
resulting in a final set of 16,726 video-music pairs for experimentation.
C Film Encoder
C.1 Semantic Feature
CLIP [ 37] is a pre-trained model for language-image tasks, consisting of a text encoder ftext(·)and
an image encoder fimage (·). It is trained on large-scale image-text datasets using contrastive learning
to enhance the similarity between images and text. As a result, CLIP4can generate semantic priors
not only for images but also for videos, which can be utilized to facilitate music generation.
To generate video semantic priors, we first extract 512-dimensional features from video frames using
fimage (·)at a frame rate of 10 frames per second. Then, we average the feature sequence along the
time dimension to obtain the visual semantic feature cs∈R1×512of the entire video. The cscontains
different semantic representations in the video, such as scenes, characters, and objects, which can
guide the generation of music.
C.2 Aesthetic Feature
We employ the TA V AR5model [ 25] for aesthetic feature extraction. It comprises three key compo-
nents: The Visual Attribute Analysis Network (V AAN), The Theme Understanding Network (TUN),
Bilevel Aesthetic Reasoning.
1)Visual Attribute Analysis Network (V AAN), which is used to learn visual attributes for aesthetic
perception. It uses a ResNet50 [ 12] architecture with the fully connected layers removed, allowing
feature extraction to be shared across branches. Then, we utilize six Multilayer Perceptrons (MLPs)
to further map the shared features to six visual attributes. Specifically, we extract 10frames from
our movie clip, denoted as ai(i= 1,2, ...,10), at a frame rate of 1 frame per second. For each input
image ai, the hidden feature hi
ais obtained from the shared feature extraction network Fa(·)as:
hi
a=Fa(ai). (6)
Then, six MLPs are used to construct six attribute branches, further mapping the hidden features hi
a
of each image to visual attributes ˆai, which are defined as follows:
ˆai=MLP m(hi
a), (7)
where m= 1,2, . . . ,6denotes 6attribute branch MLPs, and ˆai=
ˆai
1,ˆai
2, . . . ,ˆai
6	
denotes six
predicted visual attributes. We take the average of 10 frames to obtain 6 distinct attributes (the mean
of different video frames) as the visual attributes ˆajfor the entire video. These visual attributes are
then used as input for the module of Bilevel Aesthetic Reasoning.
2)Theme Understanding Network (TUN), which utilizes a ResNet-50 backbone [ 12] to initially predict
the theme category of an image. Then, a Multi-Layer Perceptron (MLP) with PReLU activation
function maps the input image to the predicted theme category, where the last fully connected layer
produces six outputs representing six theme categories. Finally, a softmax non-linearity operation
is performed to generate the predicted theme probabilities. Since this module not only predicts the
theme category of an image but also generates theme features, we feed the 10 frames of a movie
video into this module to extract theme features for each frame, which are defined as:
ˆbi=MLP (Ft(ai)). (8)
Then, we average the theme features of these 10 frames to obtain the theme feature ˆbfor the movie
video. This feature is also inputted into the module of Bilevel Aesthetic Reasoning. 3)Bilevel
4https://v-iashin.github.io/video_features/models/clip
5https://github.com/yipoh/TA V AR
15

--- PAGE 16 ---
Aesthetic Reasoning. Firstly, considering the relationship between image themes and visual attributes,
the theme features serve as central nodes, and all attribute nodes are connected to the central theme
node. Finally, node features are obtained through a graph convolutional network(GCN). Additionally,
based on the relationship between theme-aware visual attributes and general aesthetics, general
aesthetic features are treated as the central node, and all theme-aware visual attribute nodes are
connected to the central node. By utilizing the attribute aesthetics graph, updated node features that
integrate theme-aware visual attributes and aesthetic features are obtained. Here, aesthetic features
are extracted using a Swin Transformer [31].
Lastly, we append an FC layer to map the updated node features to an overall aesthetic quality score.
The overall aesthetic quality score is then embedded to obtain aesthetic features ca∈R1×512, which
guide music generation.
C.3 Emotion Feature
Here, we employ a pre-trained Weakly Supervised Video Emotion Detection and Prediction(WECL6)
model [ 49], to predict emotional information within the videos.We embed one-hot categorical labels
E into the Emotion feature ce∈R1×512via linear projection:
ce=Embed (E). (9)
C.4 Feature Fusion
For the semantic feature cs, aesthetic feature ca, and emotional feature ce, if we use the typical feature
vector concatenation, we will face the curse of dimensionality and a lack of interaction between
features. Therefore, we have employed a simplified feature fusion block called Lightweight Attention
Feature Fusion (LAFF) [ 18]. In this approach, we combine the semantic, emotional, and aesthetic
features of the video in a specific LAFF block using a convex combination, where the fusion weights
are learned to optimize the generation of movie music.
D Objective Metric
D.1 BCS, BHS, F1 scores, CSD and HSD
Here, we use improved versions of the Bar Coverage Score (BCS) and Bar Hit Score (BHS) for rhythm
relevance evaluation. BCS and BHS are typically evaluated by computing the aligned rhythm beats
between synthesized and real music, but due to these metrics being only applicable to short-length
music (2 6s), two main issues arise when evaluating long music: 1) The second-wise rhythm detection
algorithm would lead to extremely sparse vectors for any long music sequences, thus consistently low
BCS and BHS values fail to reflect the actual performance. 2) If the generated music involves more
rhythm beats than the ground truth, BHS can easily exceed 1, which would make the metric values
appear perfect while the per-sample performance is far from satisfactory. We use two modified BCS
and BHS similar to LORIS [ 47]: First, adjust the parameters of the audio onset detection algorithm to
avoid sparse rhythm vectors. Second, compute BCS by dividing the aligned beats by the total beats
of the generated music (Ba/Bg), where BCS and BHS serve as recall and precision respectively.
Additionally, we use the F1 scores of BCS and BHS as a comprehensive evaluation, and CSD and
HSD (the standard deviations of BCS and BHS) to evaluate generation stability.
D.2 Melody acc and Dynamics corr
Melody Accuracy assesses the match between frame-wise pitch classes ( C, C#,. . . , B ; totaling
12) of input melody controls and those extracted from the generated output. Dynamics Correlation
measures the Pearson correlation between frame-wise input dynamic values and those computed
from the generation. Two types of correlations are computed: micro and macro. Micro calculates
r’s for each generation separately, while macro collects input/generation dynamic values across all
generations and computes a single r. Micro correlation examines whether relative dynamic control
values are respected within a generation, while macro correlation assesses the same attribute across
multiple generations.
6https://github.com/nku-zhichengzhang/CTEN
16

--- PAGE 17 ---
E Additional Examples
In Fig. 8, we show three additional examples of composition style transfer with HPM. In these three
examples, our model can highlight the melody of style music (red rectangle) by learning the main
melody of style music while enhancing music segments that match the video dance beat (circled in
yellow) and eliminating noise between different notes (circled in red). This makes the generated
music more compact and compatible with video content.
(a)
(b)
(c)
Figure 8: More Composition Style Transfer Results on FilmScoreDB. We visualize the spectrograms
of Style Music and Generated Music.
17
