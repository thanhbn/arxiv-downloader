# 2411.07539.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/music/2411.07539.pdf
# Kích thước tệp: 10923289 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hòa Hợp Điểm Ảnh và Giai Điệu: Tạo Nhạc Phim Được Hướng Dẫn Bởi Nghệ Sĩ
và Chuyển Đổi Phong Cách Sáng Tác
Fan Qi
Đại học Công nghệ Thiên TânLiFeng Ni
Đại học Công nghệ Thiên TânChangSheng Xu
Viện Tự động hóa,
Viện Hàn lâm Khoa học Trung Quốc
Tóm tắt
Chúng tôi giới thiệu một khung tạo nhạc phim để hòa hợp các điểm ảnh thị giác và
giai điệu âm nhạc sử dụng mô hình khuếch tán tiềm ẩn. Khung của chúng tôi xử lý
các đoạn phim làm đầu vào và tạo ra âm nhạc phù hợp với chủ đề chung đồng thời
cung cấp khả năng điều chỉnh đầu ra theo một phong cách sáng tác cụ thể. Mô hình
của chúng tôi trực tiếp tạo ra âm nhạc từ video, sử dụng cơ chế điều chỉnh được tối
ưu hóa và hiệu quả trên ControlNet. Nó cũng tích hợp một bộ mã hóa phim thành
thạo trong việc hiểu độ sâu ngữ nghĩa, tác động cảm xúc và sức hấp dẫn thẩm mỹ
của phim. Ngoài ra, chúng tôi giới thiệu một chỉ số đánh giá mới, hiệu quả nhưng
đơn giản để đánh giá tính độc đáo và khả năng nhận diện của âm nhạc trong nhạc
phim. Để lấp đầy khoảng trống này cho nhạc phim, chúng tôi tuyển chọn một bộ dữ
liệu toàn diện về các video phim và nhạc gốc huyền thoại, tiêm kiến thức chuyên
ngành vào mô hình tạo sinh dựa trên dữ liệu của chúng tôi. Mô hình của chúng tôi
vượt trội so với các phương pháp hiện có trong việc tạo nhạc phim, có khả năng tạo
ra âm nhạc phản ánh sự hướng dẫn của phong cách nghệ sĩ, từ đó định nghĩa lại
tiêu chuẩn cho nhạc phim tự động và đặt nền tảng vững chắc cho nghiên cứu tương
lai trong lĩnh vực này. Mã nguồn và các mẫu được tạo có sẵn tại
https://anonymous.4open.science/r/HPM.

1 Giới thiệu
Nhạc phim - âm nhạc gốc đi kèm với phim - đóng vai trò then chốt trong việc làm phong phú
cảnh quan cảm xúc của phim, làm sâu sắc thêm độ phức tạp của câu chuyện, các cung bậc nhân
vật và khám phá chủ đề. Tạo ra nhạc phim đòi hỏi một nỗ lực đa ngành hài hòa bao gồm các nhà
soạn nhạc, người sắp xếp dàn nhạc, nhạc sĩ, kỹ sư âm thanh và biên tập viên âm nhạc làm việc
cùng nhau để pha trộn sáng tác, sắp xếp và ghi âm với câu chuyện thị giác của phim. Tự động hóa
quá trình sản xuất nhạc phim thông qua nghiên cứu trí tuệ nhân tạo đại diện cho một bước tiến
đáng kể hướng tới hiệu quả chi phí và đổi mới trong sản xuất nhạc phim.

Dịch chuyển phương thức thị giác thành âm nhạc đã là một lĩnh vực đầy hứa hẹn trong mô hình
hóa tạo sinh đa phương thức [52;7;54;47]. Một số công trình dựa vào Codebook [5] và có ít linh
hoạt hơn để tạo ra những âm thanh mới bên ngoài nó. Một số công trình sử dụng các biểu diễn
âm nhạc tượng trưng được định nghĩa trước như MIDI (Giao diện Nhạc cụ Số), REMI (các sự
kiện được cải tiến từ MIDI), và Piano-Roll có thể được tạo ra một cách tự hồi quy [8;7;10]. Tuy
nhiên, chúng đòi hỏi một bộ tổng hợp MIDI xuất sắc để hiển thị âm thanh từ MIDI được tạo và
gặp khó khăn trong việc mô hình hóa âm sắc và tính biểu cảm. Gần đây, một số công trình trực
tiếp tạo ra phổ đồ dựa trên các mô hình khuếch tán [9;11;30] với các lời nhắc văn bản. Âm thanh
được tạo bởi mô hình khuếch tán thể hiện độ trung thực với các đặc trưng nổi bật của lời nhắc,
bao gồm thể loại, nhịp độ, nhạc cụ và tâm trạng, đồng thời cũng nắm bắt được ngữ nghĩa chi tiết
của lời nhắc. Yu et al. [47] sử dụng mô hình xác suất khuếch tán có điều kiện tiềm ẩn để tổng hợp
các dạng sóng có điều kiện dài hạn và tạo ra nhạc nền cho các tình huống khiêu vũ và thể thao.
Công trình gần đây1

1https://huggingface.co/spaces/fffiloni/image-to-music-v2

Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.arXiv:2411.07539v1 [cs.MM] 12 Nov 2024

--- TRANG 2 ---
liên quan đến việc dịch nội dung thị giác thành các mô tả văn bản [24;27], sử dụng các mô hình
khuếch tán văn bản-sang-âm nhạc mạnh mẽ [19;30;11] để tạo âm nhạc. Tuy nhiên, quy trình hai
bước này (thị giác-sang-văn bản và sau đó văn bản-sang-âm nhạc) giới thiệu thêm độ phức tạp và
các điểm lỗi tiềm ẩn. Các yếu tố thị giác quan trọng để truyền tải cảm xúc, như màu sắc, ánh sáng
và bố cục, có thể được thể hiện không đầy đủ trong văn bản. Ngược lại, việc dịch chuyển thị giác-
sang-âm nhạc trực tiếp có thể hiệu quả hơn trong việc nắm bắt và chuyển đổi những tín hiệu thị
giác này thành các yếu tố âm nhạc, bảo tồn tông cảm xúc ban đầu của hình ảnh.

Mặc dù về mặt khái niệm đơn giản, việc tạo âm nhạc từ các mô hình khuếch tán phim gặp phải
những thách thức đáng chú ý. 1) Lĩnh vực này thiếu đáng kể các bộ dữ liệu ghép nối cẩn thận các
đoạn phim với âm nhạc tương ứng. Biên soạn các bộ dữ liệu như vậy là thách thức và tốn nhiều
tài nguyên. 2) Việc đạt được các tác phẩm âm nhạc theo chủ đề phù hợp với câu chuyện và tông
cảm xúc của phim đặt ra một thách thức phức tạp, giới thiệu các khó khăn tích hợp trong các khung
hiện tại của các mô hình khuếch tán. 3) Thiếu các chỉ số khách quan để đo lường chất lượng âm
nhạc được tạo cho các đoạn phim, làm phức tạp việc đánh giá tiến bộ và tinh chỉnh các mô hình.

Để bắc cầu cho khoảng trống hiện có trong nhạc phim tự động, chúng tôi thiết lập một bộ dữ liệu
toàn diện - FilmScoreDB. FilmScoreDB chứa 32.520 cặp đoạn phim-âm nhạc, tổng cộng 90,35
giờ, có các sáng tác từ những nhà soạn nhạc phim nổi tiếng. Bộ sưu tập này phục vụ để truyền
những hiểu biết chuyên ngành có mục tiêu vào mô hình khuếch tán dựa trên dữ liệu của chúng tôi.
Chúng tôi trình bày HPM, một phương pháp mới được thiết kế riêng để tạo nhạc phim và chuyển
đổi phong cách sáng tác. Tận dụng mô hình khuếch tán, khung của chúng tôi giới thiệu một cơ chế
điều chỉnh hiệu quả về tham số thứ bậc thấp cho ControlNet, được bổ sung bởi một bộ mã hóa
phim được thiết kế để đồng hóa các khía cạnh ngữ nghĩa, cảm xúc và thẩm mỹ của nội dung phim.
Ngoài ra, chúng tôi giới thiệu một chỉ số được cải tiến kết hợp tính độc đáo và khả năng nhận diện
để đánh giá chất lượng âm nhạc được tạo. Sử dụng thiết kế trên, khung mô hình của chúng tôi
thành thạo tạo ra nhạc phim, hoạt động độc lập và dưới sự kiểm soát của các phong cách cụ thể,
thể hiện một bước tiến đáng kể trong việc tạo nhạc phim. Tóm lại, những đóng góp chính của
chúng tôi có ba khía cạnh:

• Chúng tôi là những người đầu tiên giải quyết thách thức nhạc phim tự động và chuyển đổi
phong cách sáng tác, tập trung vào việc tạo âm nhạc cho các phân đoạn phim cụ thể.
• Chúng tôi giới thiệu một tiêu chuẩn mới bao gồm một bộ dữ liệu nhạc phim toàn diện, một
bộ chỉ số đánh giá được tinh chỉnh và một mô hình cơ sở được định nghĩa rõ ràng để thúc
đẩy các nỗ lực nghiên cứu tiếp theo.
• Thông qua phân tích thực nghiệm toàn diện, khung của chúng tôi thể hiện khả năng đặc biệt
trong việc tạo nhạc phim và chuyển đổi phong cách sáng tác. Khung của chúng tôi vượt trội
so với các phương pháp hiện có trên tất cả các chỉ số được đánh giá và thiết lập một tiêu
chuẩn mới cho cộng đồng nghệ thuật âm nhạc.

2 Bộ dữ liệu
Như được hiển thị trong Bảng 1, các bộ dữ liệu video-âm nhạc đa dạng phục vụ nhu cầu nghiên
cứu chuyên biệt. Các bộ dữ liệu tập trung vào khiêu vũ [26] và trượt băng [46] khám phá sự phù
hợp của nhịp điệu âm nhạc với chuyển động cơ thể.

Bảng 1: Các Bộ Dữ Liệu Video-Âm Nhạc Đa Dạng.
Bộ dữ liệu | Nội dung Video | Kích thước | Video Mở | Tổng Thời lượng
URMP [23] | Biểu diễn Bộ gõ | 44 | ✓ | ✓ | 1,3h
AtinPiano [34] | Biểu diễn Piano | 257 | ✓ | ✓ | 17,2h
MUSIC [50] | Video Biểu diễn | 685 | ✓ | ✓ | 45,7h
EmoMV [41] | Video Âm nhạc | 5.986 | ✓ | ✓ | 44,33h
MuVi [4] | Video Âm nhạc | 811 | ✓ | ✓ | 13,52h
SymMV [55] | Video Âm nhạc | 1.140 | ✓ | × | 76h
MVDB [36] | Video Âm nhạc | 1.985 | ✓ | ✓ | 16,54h
FS1000 [45] | Video Trượt băng | 1.604 | ✓ | ✓ | 89,1h
FisV [46] | Video Trượt băng | 500 | ✓ | ✓ | 23,6h
AIST++ [29] | Video Khiêu vũ | 60 | ✓ | ✓ | 4h
TikTok [28] | Video Khiêu vũ | 445 | ✓ | ✓ | 1,55h
FilmScoreDB | Video Phim | 32.520 | ✓ | ✓ | 90,35h

Ngược lại, các bộ dữ liệu biểu diễn âm nhạc [50] nhằm đồng bộ hóa chuyển động cơ thể và ngón tay với âm nhạc đa nhạc cụ. Sử dụng các bộ dữ liệu video âm nhạc để tạo âm nhạc dựa trên video phù hợp với mục tiêu của chúng tôi về sự cộng hưởng theo chủ đề và cảm xúc. Tuy nhiên, các bộ dữ liệu video âm nhạc khác biệt đáng kể so với các bộ dữ liệu nhạc phim, với những khác biệt cơ bản về mục tiêu: Đạo diễn video âm nhạc quảng bá trực quan cho các bài hát, trong khi các nhà soạn nhạc phim nâng cao độ sâu cảm xúc và câu chuyện của phim. Để làm rõ thêm, có một số khác biệt cụ thể. Độ Phức tạp Kỹ thuật: Nhạc phim đòi hỏi lý thuyết âm nhạc sâu sắc, dàn nhạc và sự phù hợp với câu chuyện, tương phản với sự nhấn mạnh của video âm nhạc vào sáng tạo thị giác và kỹ năng biên tập. Ý nghĩa Văn hóa và Lịch sử: Nhạc phim, rút ra từ truyền thống âm nhạc cổ điển, sở hữu độ sâu lịch sử và ý nghĩa văn hóa đáng kể, trái ngược với video âm nhạc được định hình bởi và đóng góp vào các xu hướng đương đại. Do đó, việc phát triển các bộ dữ liệu quy mô lớn, chất lượng cao được đặc trưng bởi các nhà soạn nhạc phim nổi tiếng có thể thúc đẩy nghiên cứu về âm nhạc video.

--- TRANG 3 ---
Hình 1: Minh họa khung HPM của chúng tôi. a) Trong giai đoạn huấn luyện, mô hình của chúng tôi kết hợp đặc trưng video như một đầu vào điều khiển toàn cục, cùng với tín hiệu điều khiển cục bộ của giai điệu và động lực. b) Trong giai đoạn suy luận, các điều khiển cục bộ có thể là một phong cách sáng tác của một nghệ sĩ cụ thể, hướng dẫn Film Score ControlNet để tạo ra Mel-spectrogram, sau đó được chuyển đổi thành âm thanh qua vocoder. c) Bộ mã hóa phim xử lý để trích xuất các embedding cảm xúc, ngữ nghĩa và thẩm mỹ, làm phong phú độ sâu diễn giải của mô hình.

Vì mục đích này, chúng tôi biên soạn FilmScoreDB, một bộ dữ liệu toàn diện có các đoạn video phim và nhạc gốc huyền thoại tương ứng. FilmScoreDB được thu thập chứa 32.520 mẫu, có nguồn từ gần 300 bộ phim nổi tiếng trên toàn thế giới, mỗi mẫu dài 10 giây. Chúng tôi chia FilmScoreDB thành tập huấn luyện (26.730 cặp), tập kiểm định (2.895 cặp) và tập kiểm tra (2.895 cặp). Chúng tôi bắt đầu bằng việc thu thập danh sách các bộ phim có Đề cử Nhạc Gốc Hay Nhất. Để có được dữ liệu ghép nối như vậy, chúng tôi đúng cách lấy và tải xuống khoảng 300 bộ phim trên các nền tảng như YouTube, Netflix, Disney+, Prime Video, v.v., mời năm chuyên gia xem và trích xuất các đoạn phim, bao gồm cả nhạc phim. Ngoài ra, mỗi cặp dữ liệu được gắn nhãn với các nhà soạn nhạc và phong cách nhạc phim liên quan. Đối với bộ dữ liệu này, chúng tôi chỉ cung cấp thông tin liên quan về các bộ phim được sử dụng, bao gồm tên phim, nhà soạn nhạc, thông tin nhạc phim và dấu thời gian đoạn phim. Chúng tôi cũng đánh giá mô hình của mình bằng bộ dữ liệu EmoMV [41] để so sánh công bằng với công trình trước đó.

3 Phương pháp
Mục tiêu tổng thể của chúng tôi là học một mô hình tạo sinh có điều kiện, p(s|cfilm, Cstyle), cho nhạc phim. Ở đây, cfilm đại diện cho thông tin điều kiện thu được từ một đoạn phim cho trước thông qua Film Encoder (không phụ thuộc thời gian), và Cstyle là một tập hợp các tín hiệu điều khiển phong cách âm nhạc (giai điệu, động lực). Chúng tôi kết hợp các tín hiệu điều khiển phong cách âm nhạc thu được vào ControlNet, lấy cảm hứng từ Uni-ControlNet [51], và tối ưu hóa việc huấn luyện mô hình sử dụng low-rank adaptation (LORA) [17]. Chúng tôi tạo ra nhạc phim mới với các phong cách khác nhau dựa trên điều kiện cfilm và các tín hiệu điều khiển phong cách âm nhạc Cstyle trong quá trình suy luận.

3.1 Film Encoder
Chúng tôi trích xuất các đặc trưng ngữ nghĩa, thẩm mỹ và cảm xúc từ video phim làm hướng dẫn mô hình để giảm thiểu độ phức tạp và nhu cầu tính toán của việc sử dụng các khung hình video thô để tạo âm nhạc. Các đặc trưng ngữ nghĩa được rút ra bằng cách sử dụng bộ mã hóa hình ảnh của CLIP [37], lấy trung bình các đặc trưng khung được trích xuất để nắm bắt ngữ nghĩa video. Các đặc trưng thẩm mỹ, đánh giá chất lượng thị giác như ánh sáng và màu sắc, được thu thập bằng cách sử dụng mô hình TAVAR được huấn luyện trước [25], đánh giá các thuộc tính thị giác và thẩm mỹ liên quan đến chủ đề thông qua sự kết hợp của mạng và ánh xạ đặc trưng. Đối với các đặc trưng cảm xúc, chúng tôi sử dụng mô hình WECL được huấn luyện trước [49] với bản chất chủ quan của cảm xúc trong video bằng cách sử dụng các mối quan hệ nội-phương thức và liên-phương thức. Chúng tôi sử dụng một phương pháp fusion đặc trưng attention nhẹ để tích hợp các đặc trưng đa dạng này một cách hiệu quả, tối ưu hóa việc tạo âm nhạc bằng cách học trọng số fusion cho sự kết hợp đặc trưng hài hòa. Do hạn chế về không gian, quy trình chi tiết cho phần này được bao gồm trong tài liệu bổ sung.

3.2 Film Score ControlNet
Để thực hiện việc tạo nhạc phim và Chuyển đổi Phong cách Sáng tác được Hướng dẫn bởi Nghệ sĩ, lấy cảm hứng từ Uni-ControlNet [51], chúng tôi đề xuất Film Score ControlNet, một khung thống nhất cho phép sử dụng đồng thời các điều khiển cục bộ khác nhau (ví dụ: giai điệu và động lực) và điều khiển toàn cục (ví dụ: đặc trưng ngữ nghĩa video, đặc trưng cảm xúc và đặc trưng thẩm mỹ) trong một mô hình duy nhất, cho phép tạo nhạc phim linh hoạt và có thể điều khiển phong cách sáng tác.

Trong nhiệm vụ này, với điều khiển video toàn cục cfilm và một tập hợp các điều khiển cục bộ Cstyle điều khiển phong cách âm nhạc, mục tiêu tổng thể của chúng tôi là học một mô hình tạo sinh có điều kiện ptheta(z0|cfilm, Cstyle) trên biểu diễn nén z của mel-spectrogram âm nhạc X.

3.2.1 Mô hình Tạo Nhạc Phim Cơ bản
Chúng tôi sử dụng một mô hình tạo sinh xác suất để ước tính phân phối dữ liệu có điều kiện thực q(z0|cfilm), trong đó phân phối mô hình được ký hiệu là ptheta(z0|cfilm). Ở đây, z0 trong RC×Tr×Fr đại diện cho mẫu âm thanh tiên nghiệm x trong không gian tiềm ẩn được nén từ mel-spectrogram X trong RT×F, trong đó r biểu thị mức độ nén, C đại diện cho các kênh của biểu diễn tiềm ẩn, T và F đại diện cho các chiều thời gian và tần số của mel-spectrogram X, tương ứng.

Mô hình khuếch tán bao gồm hai quá trình: một quá trình tiến tích lũy thêm một lượng nhỏ nhiễu Gaussian cấp cao vào mẫu z0 qua M bước, và một quá trình ngược tương ứng dự đoán nhiễu được thêm vào trong quá trình tiến và loại bỏ nhiễu để khôi phục đầu vào tiềm ẩn. Để tăng cường đồng bộ hóa giữa video và âm nhạc, chúng tôi tinh chỉnh AudioLDM [30] sử dụng một bộ dữ liệu video-sang-âm nhạc chuyên dụng, HIMV-200k [16]. Tối ưu hóa này chuyển đổi mô hình nền tảng từ không gian văn bản-âm nhạc sang không gian video-âm nhạc. Chúng tôi gọi mô hình được huấn luyện trước này là VM-Unet trong phần tiếp theo.

3.2.2 Thêm Điều khiển Phong cách vào Mô hình Cơ bản
Như được hiển thị trong Hình 1, tương tự như ControlNet [51], chúng tôi cố định trọng số của VM-Unet. Chúng tôi sao chép cấu trúc và trọng số của Down Block và Middle Block từ VM-Unet và thêm một số lớp zero-convolution mới, được gọi là S-Control Branch. Trong giai đoạn huấn luyện, chúng tôi lấy phong cách âm nhạc của giai điệu và động lực làm điều khiển cục bộ, và đặc trưng phim làm điều khiển toàn cục vào S-Control Branch. Biểu diễn nén của mel-spectrogram được đưa vào VM-Unet. Chỉ các tham số của S-Control Branch có thể huấn luyện được.

Cụ thể, gọi f(x(m,l-1), m, cfilm, Cstyle) biểu thị khối thứ l của S-Control Branch, trong đó m là bước thời gian khuếch tán, x(m,l-1) chứa các đặc trưng của tiềm ẩn nhiễu sau l-1 khối, và cfilm, Cstyle là điều khiển toàn cục và cục bộ, tương ứng. Điều khiển phong cách được kết hợp qua:

ˇf(l)(xm,l-1, m, cfilm, Cstyle) := Zout(fl(x(m,l-1)+Cstyle, m, cfilm)), (1)

trong đó Zout là lớp zero convolution mới được gắn và f(l) được khởi tạo từ khối mã hóa thứ l của UNet có điều kiện video được huấn luyện trước.

Đối với điều khiển toàn cục, các lớp cross-attention được sử dụng để nắm bắt thông tin thị giác cfilm từ đoạn phim đầu vào. Trong thiết lập của chúng tôi, zt đại diện cho các đặc trưng nhiễu đến, Wq, Wk, và Wv là các ma trận chiếu. Q, K, và V trong cross-attention có thể được ký hiệu là:

Q=Wq(zt), K=Wk(cfilm), V=Wv(cfilm). (2)

Đối với điều khiển cục bộ, chúng tôi bắt đầu bằng việc căn chỉnh các tín hiệu điều khiển giai điệu và động lực với độ phân giải của các đặc trưng nhiễu đầu vào bằng hai khối tích chập và một lớp zero convolution mới được thêm vào. Sau đó, chúng tôi tích hợp chúng với các đặc trưng nhiễu đầu vào được chuẩn hóa thông qua một shortcut theo trình tự sau:

C=norm(zt) +Zin(convmel(cmel)) +Zin(convdyn(cdyn)), (3)

trong đó Zin cũng là một lớp zero convolution đầu vào mới. Đối với Melody (cmel trong RT×12), chúng tôi điều chỉnh chromagram sử dụng kích thước cửa sổ 260 và kích thước hop 160, nén năng lượng từ F thùng tần số thành 12 lớp cao độ. Sau đó, chúng tôi tăng cường nó bằng cách chọn lớp cao độ chiếm ưu thế nhất thông qua argmax tại mỗi bước thời gian. Đối với Dynamic (cdyn trong RT×1), chúng tôi rút ra độ lớn bằng cách tổng năng lượng từ các thùng tần số trong mỗi khung thời gian của spectrogram tuyến tính và chuyển đổi các giá trị thành decibel (dB), phù hợp chặt chẽ với nhận thức của con người về độ lớn. Để giảm dao động nhanh do khởi phát nốt nhạc hoặc bộ gõ gây ra, và để căn chỉnh điều khiển động lực của chúng tôi với cường độ âm nhạc được nhận thức, chúng tôi làm mượt các giá trị bằng cách sử dụng cửa sổ ngữ cảnh bậc hai, cụ thể là bộ lọc Savitzky-Golay [42].

3.2.3 Thích ứng Thứ bậc Thấp để Cải thiện Huấn luyện
Mặc dù ControlNet có kích thước bằng một nửa mô hình stable diffusion, nó vẫn sở hữu quá nhiều tham số để các GPU tiêu dùng xử lý hiệu quả. LoRA [17] là một phương pháp tinh chỉnh tài nguyên thấp cho các mô hình lớn. Với một lớp mô hình được huấn luyện trước có trọng số W0 trong Rd×k, trong đó d là chiều đầu vào và k là chiều đầu ra, LoRA phân tách ΔW như:

ΔW=BA, (4)

trong đó B trong Rd×r và A trong Rr×k với r≪min(d, k). min(d, k) là một thứ bậc nhỏ hạn chế việc cập nhật vào một không gian con chiều thấp. Bằng cách đóng băng W0 và chỉ tối ưu hóa các ma trận nhỏ hơn A và B, LoRA đạt được việc giảm lớn về tham số có thể huấn luyện. Trong quá trình suy luận, ΔW có thể được hợp nhất vào W0 mà không có overhead bằng một hệ số tỷ lệ LoRA alpha:

W=W0+alpha·ΔW. (5)

Tiếp theo, chúng tôi tiêm các lớp có thể huấn luyện (ma trận phân tách thứ bậc thấp) vào mỗi khối transformer trong S-Control Branch. Kết quả thực nghiệm chứng minh rằng việc sử dụng LoRA để tinh chỉnh tăng cường hiệu quả của quá trình, dẫn đến hiệu suất nhanh hơn với việc giảm nhu cầu tính toán.

3.2.4 Suy luận
Trong quá trình suy luận, điều kiện của điều khiển phong cách cục bộ được đặt thành zero cho việc tạo nhạc phim. Đối với chuyển đổi phong cách sáng tác, chúng tôi có thể điều khiển phong cách dựa trên giai điệu, động lực, hoặc cả hai, cho phép thích ứng linh hoạt phong cách của sáng tác theo các đặc tính mong muốn.

4 Thực nghiệm
Phần này cung cấp đánh giá chi tiết về mô hình Harmonized Pixel and Melody Film Score Diffusion (HPM) của chúng tôi, tích hợp các khía cạnh cấp điểm ảnh và cấp giai điệu. Chúng tôi sử dụng các chỉ số khách quan và chủ quan trên hai bộ dữ liệu khác nhau để đánh giá mô hình của chúng tôi. Để đánh giá tính độc đáo của âm nhạc được tạo, chúng tôi giới thiệu khung độc đáo so với khả năng nhận diện [2]. Ngoài ra, chúng tôi chứng minh hiệu quả của việc sử dụng LoRA để tăng tốc huấn luyện trong khung của chúng tôi.

4.1 Chi tiết Triển khai
Xem xét hiệu quả tính toán, chúng tôi sử dụng AudioLDM [30] làm backbone cơ bản. Chúng tôi cố định các tham số của Music VAE trong AudioLDM. Đối với tất cả các đặc trưng thị giác, chúng tôi sử dụng tần số khung hình 10 fps, theo tiêu chuẩn trên Film Score (của chúng tôi) và EmoMV. Chúng tôi sử dụng các siêu tham số mặc định của AudioLDM và sử dụng tỷ lệ học cơ bản 0,0001. Việc huấn luyện mô hình khuếch tán tạo nhạc phim qua 200.000 bước gradient với kích thước batch 2 được thực hiện trên 4 GPU NVIDIA GeForce RTX 3090, mất khoảng 40 giờ để hoàn thành qua 60 epoch. Chúng tôi áp dụng bộ tối ưu hóa AdamW [32] với beta1= 0,9, beta2= 0,999, và weight decay 0,01. Trong giai đoạn suy luận, chúng tôi áp dụng DDIM [39] để lấy mẫu, với số timestep được đặt thành 200 và thang classifier free guidance [15] được đặt thành 7,5. Chúng tôi sử dụng loss L2 và bộ tối ưu hóa AdamW để tinh chỉnh Film Score ControlNet của chúng tôi cho đến khi hội tụ trong 12 giờ, sử dụng 4 GPU NVIDIA GeForce RTX 3090. Trong giai đoạn suy luận, chúng tôi sử dụng lấy mẫu DDIM 100 bước và chỉ sử dụng classifier-free guidance trên điều khiển video toàn cục. Do hạn chế về độ dài bài báo, chi tiết về film encoder được cung cấp trong tài liệu bổ sung.

--- TRANG 6 ---
4.2 Phương pháp Đánh giá

4.2.1 Baseline
Để đánh giá toàn diện khung HPM của chúng tôi, chúng tôi tiến hành các thực nghiệm so sánh trên hai nhiệm vụ. Đối với tạo nhạc phim, chúng tôi chọn bảy phương pháp hiệu suất cao có mã nguồn khả dụng làm baseline. Cụ thể, chúng tôi so sánh HPM của chúng tôi với LORIS [47], mô hình D2M-Gan [53], mô hình CDCD [54] dựa trên Codebook, và mô hình DIFF-Foley [33] dựa trên Mel-Spectrogram, chúng tôi sử dụng triển khai chính thức. Chúng tôi tiếp tục tinh chỉnh một mô hình tạo video-sang-âm nhạc, được gọi là Tango-VM, dựa trên mô hình văn bản-sang-âm nhạc, Tango [11]. Do khó khăn trong việc chuyển đổi âm nhạc phim thành MIDI mà không có chú thích, chúng tôi loại trừ các phương pháp dựa trên tạo MIDI, như CMT [7], Foley Music [10], Audeo [40], và Video2Music [20] khỏi so sánh baseline của chúng tôi. Đối với Chuyển đổi Phong cách Sáng tác, chúng tôi so sánh mô hình của chúng tôi với DITTO [35] được huấn luyện với điều khiển giai điệu và động lực. Trong EmoMV, mỗi cảm xúc được chỉ định như một phong cách riêng biệt. Tương tự, mỗi phong cách nhạc phim trong FilmScoreDB được xác định như một phong cách độc nhất. Mô hình DITTO thực hiện stylization âm nhạc hướng văn bản, tận dụng điều kiện giai điệu và cường độ thông qua tối ưu hóa thời gian suy luận của không gian tiềm ẩn nhiễu ban đầu trong mô hình khuếch tán văn bản-sang-âm nhạc được huấn luyện trước. Lấy cảm hứng từ DITTO, chúng tôi đánh giá mô hình VM-net của chúng tôi trên ba tình huống: (i) chỉ giai điệu, loại trừ giai điệu, (ii) chỉ động lực, loại trừ động lực, và (iii) điều khiển giai điệu và động lực kết hợp.

4.2.2 Chỉ số Đánh giá
Khách quan: Để đánh giá tương ứng nhịp điệu trong nghiên cứu của chúng tôi, chúng tôi sử dụng các phiên bản được sửa đổi của Beats Coverage Score (BCS) [6], Beats Hit Score (BHS) [22], và điểm F1 để đánh giá toàn diện độ chính xác nhịp điệu, CSD và HSD (độ lệch chuẩn của BCS và BHS) để đánh giá tính ổn định tạo sinh. Đối với chất lượng âm nhạc, Frechet Audio Distance (FAD) [21], Inception Score, và phân kỳ Kullback-Leibler (KL) [13] được sử dụng, tận dụng bộ phân loại VGGish cho FAD để đo tương đồng và PANN cho IS và KL để đánh giá chất lượng. Đối với chuyển đổi phong cách, chúng tôi đo Melody Accuracy và Dynamics Correlation [44], phân tích sự phù hợp của các lớp cao độ và tương quan Pearson của động lực giữa điều khiển đầu vào và đầu ra được tạo, với tương quan micro và macro cung cấp thông tin về độ trung thực điều khiển động lực và tính nhất quán trên các thế hệ.

Chủ quan: Ngoài ra, chúng tôi cũng sử dụng đo lường chủ quan của Mean Opinion Score (MOS) trong nhiệm vụ tạo video-sang-âm nhạc. MOS được thu thập bằng cách tính điểm trung bình của nghiên cứu người dùng về chất lượng âm nhạc tổng thể. Ở đây, chúng tôi tuyển 20 người tham gia để nghe 30 tác phẩm âm nhạc được tạo bởi HPM và năm phương pháp baseline (5 mẫu mỗi phương pháp) và sau đó đánh giá chất lượng âm nhạc tổng thể bằng cách cho điểm từ 1 đến 5 về chất lượng âm nhạc, độ rõ ràng và có hay không có độ trễ.

4.2.3 Khung Độc đáo so với Khả năng Nhận diện

Hình 2: So sánh Độc đáo so với Khả năng Nhận diện.
Điểm dữ liệu lớn đại diện cho một mô hình duy nhất, gói gọn điểm trung bình của độc đáo và khả năng nhận diện trên tất cả các danh mục, trong khi các điểm dữ liệu nhỏ hơn xung quanh minh họa điểm trong mỗi danh mục cho mô hình đó.

Tính độc đáo trong nhạc phim là một chỉ số quan trọng, đòi hỏi việc tạo ra các sáng tác thể hiện sự khác biệt khi so sánh với âm nhạc nền trước đó. Nguyên tắc này nhấn mạnh tầm quan trọng của sự đổi mới trong quá trình sáng tác, đảm bảo rằng mỗi bản nhạc đóng góp một trải nghiệm thính giác độc nhất tăng cường độ sâu câu chuyện và cảm xúc của phim. Khả năng nhận diện của một bản nhạc phim đề cập đến khả năng được khán giả nhận biết như thể hiện một phong cách âm nhạc cụ thể, từ đó tạo điều kiện cho việc xác định dòng dõi phong cách hoặc ý định của bản nhạc. Thuộc tính này có vai trò quan trọng trong việc đánh giá hiệu quả của một bản nhạc trong việc gợi lên các phản ứng cảm xúc hoặc liên tưởng chủ đề dự định. Để tính toán Độc đáo, chúng tôi sử dụng phương trình sau, sigma_rho=

--- TRANG 7 ---
Bảng 2: Kết quả đánh giá định lượng cho nhiệm vụ tạo nhạc phim trên các bộ dữ liệu FilmScoreDB và EmoMV.

[Bảng với các cột: Dataset, Method, Rhythm Correspondence (BCS ↑, BHS ↑, F1 scores ↑), Music Quality (IS↑, KL↓, FAD ↓), Generate Stability (CSD ↓, HSD ↓), Subjective Evaluation (Mos↑)]

[Các hàng dữ liệu cho FilmScoreDB và EmoMV với các phương pháp khác nhau như D2M-Gan, CDCD, Tango-VM, LORIS, DIFF-Foley, HPM]

Σ(j=1 to nc) r * (1/(N-1)) * Σ(i=1 to N) (f(mj_i) - (1/N) * Σ(i=1 to N) f(mj_i))^2, trong đó nc biểu thị số lớp, f(mj_i) biểu thị đặc trưng trích xuất cho mel-spectrogram mj_i từ mô hình được huấn luyện trước. Để đánh giá khả năng nhận diện, chúng tôi sử dụng một mô hình phân loại one-shot được thiết kế cho các phong cách âm nhạc, đo độ chính xác phân loại của mỗi mẫu được tạo. Độ chính xác trung bình trên tập kiểm tra được sử dụng làm chỉ số khả năng nhận diện. Hình 2 trình bày so sánh giữa phương pháp của chúng tôi và các baseline này về độc đáo và khả năng nhận diện. Trong so sánh, D2M-Gan [53] và CDCD [54] đã học tạo ra âm nhạc giống hệt của phong cách cụ thể (tức là độc đáo thấp nhưng độ chính xác cao). Mặc dù thành thạo trong việc học chính xác phân phối của Ground Truth, dẫn đến khả năng nhận diện cao, chúng thiếu sót trong việc tạo ra các sáng tác âm nhạc mới. Điều này cho thấy rằng cả mô hình khuếch tán và GAN đều không thể vượt qua các hạn chế do Codebook áp đặt. LORIS [47] dựa trên dạng sóng, bằng cách học từ biểu diễn độc đáo nhất của âm nhạc, có khả năng liên tục tạo ra âm nhạc riêng của mình. Tuy nhiên, điều này dẫn đến âm nhạc được tạo ra lệch khỏi phân phối Ground Truth, dẫn đến khả năng nhận diện thấp hơn. Mô hình của chúng tôi, cùng với Tango-VM, hiệu quả cân bằng mối quan hệ giữa độc đáo và khả năng nhận diện, đảm bảo kết quả thuận lợi trong cả sự đổi mới và biểu hiện phong cách trong âm nhạc. Vì Tango [11] có nhiều tham số hơn và đạt độ phân giải phổ cao hơn, nó dẫn đến sự sáng tạo lớn hơn so với mô hình của chúng tôi.

4.3 Kết quả Chính

4.3.1 Tạo Nhạc Phim
Kết quả thực nghiệm định lượng của mô hình tạo nhạc phim của chúng tôi được hiển thị trong Bảng 2. Mô hình của chúng tôi vượt trội so với tất cả baseline về tính nhất quán nhịp điệu, chất lượng âm nhạc và tính ổn định tạo sinh. Các baseline được thiết kế cho việc tạo âm nhạc khiêu vũ/thể thao, tập trung vào việc mô hình hóa tư thế/chuyển động video theo nhịp điệu của âm nhạc. Nó không phù hợp cho âm nhạc phim với ngữ nghĩa và cảm xúc phục vụ như cầu nối giữa phim và nhạc phim. Cần lưu ý, phương pháp LORIS [47] có điểm BHS và F1 thấp hơn so với các phương pháp khác. Việc trích xuất các đặc trưng tư thế quan trọng, như một điều kiện nhịp điệu thị giác quan trọng cho LORIS [47], đặt ra thách thức trong FilmDB của chúng tôi do tính biến đổi và phức tạp của các nhiệm vụ và chuyển đổi cảnh đặc trưng của video phim. Do đó, các baseline được thiết kế cho việc tạo âm nhạc khiêu vũ/thể thao bằng cách căn chỉnh chuyển động video với nhịp điệu, không đáp ứng được yêu cầu độc đáo của âm nhạc phim, nơi các kết nối ngữ nghĩa và cảm xúc là thiết yếu, khiến chúng không phù hợp cho bối cảnh nhạc phim của chúng tôi. Tuy nhiên, phương pháp LORIS dựa trên dạng sóng vượt trội so với CDCD [54] và D2M-Gan [53], cũng như Tango-VM dựa trên phổ, về điểm chất lượng âm nhạc tổng thể. Điều này cho thấy rằng các dạng sóng được tổng hợp bởi mô hình tạo sinh xử lý tốt hơn các chi tiết âm nhạc, đảm bảo cải thiện chất lượng âm nhạc. Từ Bảng 2, chúng tôi quan sát thấy rằng tất cả các phương pháp hoạt động tốt về tính nhất quán nhịp điệu trên bộ dữ liệu MV, cho thấy rằng phim trình bày các tình huống nhịp điệu thách thức hơn. Tuy nhiên, mô hình của chúng tôi vẫn đạt được các cải thiện đáng kể so với các phương pháp khác trên cả hai bộ dữ liệu. Những kết quả này xác nhận rằng khung của chúng tôi có thể tạo ra một cách nhất quán các bản nhạc chất lượng cao với sự căn chỉnh nhịp điệu chính xác cho cả phim và video âm nhạc.

Trong Hình 3, chúng tôi trình bày các ví dụ về việc tạo nhạc phim, trưng bày các khung video cùng với điểm cảm xúc và thẩm mỹ tương ứng và giai điệu và động lực của âm nhạc được tạo.

--- TRANG 8 ---
Hình 3: Ví dụ về Tạo Nhạc Phim của chúng tôi, bao gồm khung video (trên), điểm cảm xúc & thẩm mỹ (giữa), và giai điệu & động lực được tạo (dưới).

Bảng 3: Kết quả đánh giá định lượng cho nhiệm vụ chuyển đổi phong cách sáng tác trên các bộ dữ liệu FilmDB và EmoMV.

[Bảng với các cột: Dataset, Control, Model, Melody acc (%), Dynamics corr(r, in %), Music Quality (IS↑, KL↓, FAD ↓)]

[Dữ liệu cho FilmScoreDB và EmoMV với các điều khiển khác nhau: Melody only, Dynamics only, Melody Dynamics, so sánh DITTO và Ours]

Cảm xúc và thẩm mỹ đại diện cho hai chiều đặc trưng riêng biệt để đặc trưng hóa phim, với chiều trước phản ánh cường độ cảm xúc được nhận thức truyền tải bởi các chủ thể hoặc cảnh, và chiều sau có thể đề cập đến bố cục tổng thể, sự cân bằng và sức hấp dẫn thị giác của khung hình. Động lực được tạo và các giá trị cường độ cảm xúc có tương quan dương. Cảm xúc và thẩm mỹ cùng nhau quyết định giai điệu.

4.3.2 Chuyển đổi Phong cách Sáng tác
Kết quả thực nghiệm định lượng của chuyển đổi phong cách sáng tác của chúng tôi được hiển thị trong Bảng 3. Chúng tôi thiết lập hai tập kiểm tra có điều khiển phong cách từ các bộ dữ liệu FilmScoreDB và EmoMV. Tập kiểm tra FilmScoreDB phân loại âm nhạc theo thể loại phim (ví dụ: Plot, Love và Action) và chọn tín hiệu điều khiển giai điệu và động lực từ các thể loại khác nhau để tạo thành một tập điều khiển đa dạng. Ngược lại, tập kiểm tra EmoMV, với sáu danh mục cảm xúc, sử dụng phương pháp tương tự nhưng tăng mẫu điều khiển từ 2 lên 10 mỗi danh mục để làm phong phú tập điều khiển thực nghiệm, nhằm đánh giá khả năng của mô hình trong việc tạo âm nhạc trên các phong cách và tông cảm xúc đa dạng. Dưới ba điều kiện khác nhau, mặc dù có sự khác biệt đáng kể về chất lượng đầu ra được tạo giữa hai phương pháp, điểm điều khiển của chúng tôi tốt hơn phương pháp của DITTO. Ngoài ra, HPM phản ứng chính xác hơn với các điều khiển giai điệu và động lực. Điều này cho thấy rằng phương pháp dựa trên tối ưu hóa trong quá trình suy luận phù hợp hơn để điều khiển chính xác giai điệu và động lực trong âm nhạc phức tạp so với phương pháp của DITTO. Trong Hình 4, chúng tôi hiển thị các ví dụ về đầu ra từ các mô hình dưới sự điều khiển của phong cách (giai điệu và động lực) được trích xuất từ âm nhạc mục tiêu. So sánh các mel spectrogram, cấu trúc giai điệu và động lực của âm nhạc gốc, được tạo và mục tiêu cho thấy rằng mặc dù có thể không có sự chồng lấp hoàn toàn, việc so sánh đủ cho thấy khả năng của mô hình chúng tôi trong việc tạo ra âm nhạc mới theo phong cách riêng biệt từ Ground Truth dưới các điều khiển được chỉ định.

--- TRANG 9 ---
Hình 4: Ví dụ về chuyển đổi phong cách sáng tác của chúng tôi, bao gồm spectrogram (trên), giai điệu (giữa), và động lực (dưới).

Bảng 4: Kết quả của nghiên cứu ablation về Film Encoder và thực nghiệm so sánh được tăng tốc với LORA trên bộ dữ liệu FilmScoreDB.

(a) Tác động của LoRA.
Method | Trainable Parameters | Train Time
HPM w/o LORA | 87M | 48 hours
HPM w/ LORA | 20M | 12 hours

Method | Melody acc(%) | Dynamics corr(r,in %)
       |               | Micro | Macro
HPM w/o LORA | 57.7 | 64.5 | 87.2
HPM w/ LORA | 57.3 | 64.7 | 87.6

(b) Các điều kiện Film Encoder khác nhau.
Method | Music Quality | Generate Stability
       | IS↑ | KL↓ | FAD ↓ | CSD ↓ | HSD ↓
HPM w/ S,A | 4.6 | 5.5 | 7.3 | 18.1 | 16.1
HPM w/ A,E | 4.5 | 5.4 | 7.2 | 17.9 | 16.0
HPM w/ E,S | 4.5 | 5.4 | 7.2 | 18.0 | 16.1
HPM w/ S | 4.5 | 5.4 | 7.2 | 17.8 | 16.0
HPM w/ E | 4.6 | 5.5 | 7.3 | 18.0 | 16.1
HPM w/ A | 4.7 | 5.6 | 7.4 | 18.3 | 16.2
HPM | 4.4 | 5.3 | 7.1 | 18 | 16.2

4.3.3 Thích ứng Thứ bậc Thấp để Tăng tốc Huấn luyện
Để chứng minh hiệu quả của LORA, chúng tôi đánh giá hai biến thể mô hình: HPM có/không có LORA trong Bảng 4b, lưu ý sự khác biệt về tham số có thể huấn luyện và thời gian huấn luyện. Đáng chú ý, HPM với LORA giảm tham số từ 87 triệu xuống 20 triệu và cắt giảm thời gian huấn luyện từ 48 xuống 12 giờ mà không làm giảm hiệu suất. HPM không có LORA vượt trội một chút về độ chính xác giai điệu, trong khi HPM với LORA có điểm tương quan động lực tốt hơn một chút. Điều này cho thấy hiệu quả của LORA trong việc tối ưu hóa mô hình và tăng tốc huấn luyện trong khi duy trì hoặc cải thiện một chút đánh giá động lực.

4.4 Nghiên cứu Ablation
Chúng tôi tiến hành nghiên cứu ablation để kiểm tra ảnh hưởng của film encoder và film score ControlNet trên bộ dữ liệu FilmScoreDB. Đối với film encoder, chúng tôi kiểm tra các kết hợp khác nhau của điều kiện thị giác (Semantic, Aesthetic, Emotion) về hiệu suất của mô hình HPM trong Bảng 4a. Mỗi hàng đại diện cho một biến thể của mô hình với các kết hợp cụ thể của điều kiện thị giác. Chúng tôi quan sát thấy rằng việc kết hợp các điều kiện thị giác khác nhau dẫn đến các biến đổi về hiệu suất trên tất cả các chỉ số đánh giá so với mô hình hoàn chỉnh (HPM). Các mô hình với điều kiện thị giác kết hợp thường thể hiện hiệu suất cao hơn so với các mô hình với điều kiện thị giác riêng lẻ. Hiệu suất của mô hình HPM với các kết hợp khác nhau của điều kiện thị giác vẫn tương đối nhất quán trên các chỉ số khác nhau, cho thấy rằng mỗi kết hợp đóng góp vào việc nâng cao các nhiệm vụ tạo âm nhạc. Nhìn chung, những phát hiện này làm nổi bật tầm quan trọng của việc xem xét nhiều điều kiện thị giác đồng thời để đạt được hiệu suất tối ưu trong các nhiệm vụ tạo âm nhạc sử dụng mô hình HPM.

5 Thảo luận
Trong bài báo này, chúng tôi điều tra nhiệm vụ nhạc phim tự động thông qua việc giới thiệu một bộ dữ liệu phim-âm nhạc toàn diện 90,35 giờ, cùng với việc thiết lập một mô hình baseline (HPM) và một chỉ số đánh giá tập trung vào Độc đáo so với Khả năng Nhận diện. Chúng tôi thừa nhận những hạn chế nhất định và vạch ra các hướng để nâng cao trong tương lai: Một hạn chế chính của khung hiện tại của chúng tôi là tính cứng nhắc trong việc tạo ra âm nhạc có thời lượng cố định, hạn chế khả năng thích ứng của nó với các biểu hiện cảm xúc đa dạng, các hình thức câu chuyện hoặc yêu cầu tình huống. Chúng tôi dự định điều chỉnh tỷ lệ lấy mẫu và mật độ để thay đổi độ dài đầu ra trong khi bảo tồn tính toàn vẹn của âm nhạc.

--- TRANG 10 ---
Tài liệu tham khảo
[1]Bar-Tal, O., Yariv, L., Lipman, Y., Dekel, T.: Multidiffusion: Fusing diffusion paths for controlled image generation (2023)
[2]Boutin, V., Singhal, L., Thomas, X., Serre, T.: Diversity vs. recognizability: Human-like generalization in one-shot generative models. Advances in Neural Information Processing Systems 35, 20933-20946 (2022)
[3]Çano, E., Morisio, M., et al.: Music mood dataset creation based on last.fm tags. In: 2017 International Conference on Artificial Intelligence and Applications, Vienna, Austria. pp. 15-26 (2017)
[4]Chua, P., Makris, D., Herremans, D., Roig, G., Agres, K.: Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses. arXiv preprint arXiv:2202.10453 (2022)
[5]Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., Défossez, A.: Simple and controllable music generation. arXiv preprint arXiv:2306.05284 (2023)
[6]Davis, A., Agrawala, M.: Visual rhythm and beat. ACM Transactions on Graphics (TOG) 37(4), 1-11 (2018)
[7]Di, S., Jiang, Z., Liu, S., Wang, Z., Zhu, L., He, Z., Liu, H., Yan, S.: Video background music generation with controllable music transformer. In: Proceedings of the 29th ACM International Conference on Multimedia. pp. 2037-2045 (2021)
[8]Dong, H.W., Hsiao, W.Y., Yang, L.C., Yang, Y.H.: Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 32 (2018)
[9]Forsgren, S., Martiros, H.: Riffusion - Stable diffusion for real-time music generation (2022), https://riffusion.com/about
[10] Gan, C., Huang, D., Chen, P., Tenenbaum, J.B., Torralba, A.: Foley music: Learning to generate music from videos. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI 16. pp. 758-775. Springer (2020)
[11] Ghosal, D., Majumder, N., Mehrish, A., Poria, S.: Text-to-audio generation using instruction-tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731 (2023)
[12] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)
[13] Hershey, S., Chaudhuri, S., Ellis, D.P., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., et al.: Cnn architectures for large-scale audio classification. In: 2017 ieee international conference on acoustics, speech and signal processing (icassp). pp. 131-135. IEEE (2017)
[14] Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022)
[15] Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022)
[16] Hong, S., Im, W., Yang, H.S.: Content-based video-music retrieval using soft intra-modal structure constraint. arXiv preprint arXiv:1704.06761 (2017)
[17] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)
[18] Hu, F., Chen, A., Wang, Z., Zhou, F., Dong, J., Li, X.: Lightweight attentional feature fusion: A new baseline for text-to-video retrieval. In: European Conference on Computer Vision. pp. 444-461. Springer (2022)

--- TRANG 11 ---
[19] Huang, Q., Park, D.S., Wang, T., Denk, T.I., Ly, A., Chen, N., Zhang, Z., Zhang, Z., Yu, J., Frank, C., et al.: Noise2music: Text-conditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917 (2023)
[20] Kang, J., Poria, S., Herremans, D.: Video2music: Suitable music generation from videos using an affective multimodal transformer model. arXiv preprint arXiv:2311.00968 (2023)
[21] Kilgour, K., Zuluaga, M., Roblek, D., Sharifi, M.: Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms. In: INTERSPEECH. pp. 2350-2354 (2019)
[22] Lee, H.Y., Yang, X., Liu, M.Y., Wang, T.C., Lu, Y.D., Yang, M.H., Kautz, J.: Dancing to music. Advances in neural information processing systems 32(2019)
[23] Li, B., Liu, X., Dinesh, K., Duan, Z., Sharma, G.: Creating a multitrack classical music performance dataset for multimodal music analysis: Challenges, insights, and applications. IEEE Transactions on Multimedia 21(2), 522-535 (2018)
[24] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)
[25] Li, L., Huang, Y., Wu, J., Yang, Y., Li, Y., Guo, Y., Shi, G.: Theme-aware visual attribute reasoning for image aesthetics assessment. IEEE Transactions on Circuits and Systems for Video Technology (2023)
[26] Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance generation with aist++. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13401-13412 (2021)
[27] Li, Y., Wang, C., Jia, J.: Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043 (2023)
[28] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740-755. Springer (2014)
[29] Lin, Y.B., Tseng, H.Y., Lee, H.Y., Lin, Y.Y., Yang, M.H.: Exploring cross-video and cross-modality signals for weakly-supervised audio-visual video parsing. Advances in Neural Information Processing Systems 34, 11449-11461 (2021)
[30] Liu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D., Wang, W., Plumbley, M.D.: Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503 (2023)
[31] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012-10022 (2021)
[32] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)
[33] Luo, S., Yan, C., Hu, C., Zhao, H.: Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems 36(2024)
[34] Moryossef, A., Elazar, Y., Goldberg, Y.: At your fingertips: Automatic piano fingering detection (2019)
[35] Novack, Z., McAuley, J., Berg-Kirkpatrick, T., Bryan, N.J.: Ditto: Diffusion inference-time t-optimization for music generation. arXiv preprint arXiv:2401.12179 (2024)
[36] Pandeya, Y.R., Lee, J.: Deep learning-based late fusion of multimodal information for emotion classification of music video. Multimedia Tools and Applications 80, 2887-2905 (2021)

--- TRANG 12 ---
[37] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)
[38] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)
[39] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)
[40] Su, K., Liu, X., Shlizerman, E.: Audeo: Audio generation for a silent performance video. Advances in Neural Information Processing Systems 33, 3325-3337 (2020)
[41] Thao, H.T.P., Roig, G., Herremans, D.: Emomv: Affective music-video correspondence learning datasets for classification and retrieval. Information Fusion 91, 64-79 (2023)
[42] Virtanen, P., Gommers, R., Oliphant, T., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., et al.: Fundamental algorithms for scientific computing in python and scipy 1.0 contributors. scipy 1.0. Nat. Methods 17, 261-272 (2020)
[43] Wallace, B., Gokul, A., Ermon, S., Naik, N.: End-to-end diffusion latent optimization improves classifier guidance. In: ICCV. pp. 7246-7256. IEEE (2023)
[44] Wu, S.L., Donahue, C., Watanabe, S., Bryan, N.J.: Music controlnet: Multiple time-varying controls for music generation. arXiv preprint arXiv:2311.07069 (2023)
[45] Xie, P., Zhang, Q., Li, Z., Tang, H., Du, Y., Hu, X.: Vector quantized diffusion model with codeunet for text-to-sign pose sequences generation. arXiv preprint arXiv:2208.09141 (2022)
[46] Xu, C., Fu, Y., Zhang, B., Chen, Z., Jiang, Y.G., Xue, X.: Learning to score figure skating sport videos. IEEE transactions on circuits and systems for video technology 30(12), 4578-4590 (2019)
[47] Yu, J., Wang, Y., Chen, X., Sun, X., Qiao, Y.: Long-term rhythmic video soundtracker. In: International Conference on Machine Learning (ICML) (2023)
[48] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models
[49] Zhang, Z., Wang, L., Yang, J.: Weakly supervised video emotion detection and prediction via cross-modal temporal erasing network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18888-18897 (2023)
[50] Zhao, H., Gan, C., Rouditchenko, A., Vondrick, C., McDermott, J., Torralba, A.: The sound of pixels. In: Proceedings of the European conference on computer vision (ECCV). pp. 570-586 (2018)
[51] Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems 36(2024)
[52] Zhao, S., Yao, H., Wang, F., Jiang, X., Zhang, W.: Emotion based image musicalization. In: 2014 IEEE International conference on multimedia and expo workshops (ICMEW). pp. 1-6. IEEE (2014)
[53] Zhu, Y., Olszewski, K., Wu, Y., Achlioptas, P., Chai, M., Yan, Y., Tulyakov, S.: Quantized gan for complex music generation from dance videos. In: European Conference on Computer Vision. pp. 182-199. Springer (2022)
[54] Zhu, Y., Wu, Y., Olszewski, K., Ren, J., Tulyakov, S., Yan, Y.: Discrete contrastive diffusion for cross-modal music and image generation. In: The Eleventh International Conference on Learning Representations (2022)
[55] Zhuo, L., Wang, Z., Wang, B., Liao, Y., Bao, C., Peng, S., Han, S., Zhang, A., Fang, F., Liu, S.: Video background music generation: Dataset, method and evaluation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15637-15647 (2023)

--- TRANG 13 ---
Bảng 5: Tổng quan về phương pháp video2music.
Model | Music Representation | Architecture | Mertics | Dataset
Foley Music [10] | MIDI | Transformer | NDB | URMP,Atin Piano,MUSIC
CMT [7] | MIDI | Transformer | PCHE,GPS,ST | LPD
D2M-Gan [53] | Codebook | Gan | BCS,BHS,Genre Accurary,MOS | AIST++,TikTok
CDCD [54] | Codebook | Diffusion | BCS,BHS,Genre Accurary,MOS | AIST++,TikTok
V-Musprod [55] | MIDI | Transformer | VMCP,SC,PE,PCE,EBR,IOI | SymMV
LORIS [47] | Waveform | Diffusion | MOS,BCS,BHS,F1 Scores | FisV,AIST++,FS1000,Finegym
Video2Music [20] | MIDI | Transformer | Hits@k scores,EML | MuVi-sync
DIFF-Foley [33] | Mel-Spectrogram | Diffusion | IS,FID,MKL,Align Accs | VGGSound,AudioSet
Ours | Mel-Spectrogram | ControlNet | MOS,BCS,BHS,F1 Scores,FAD,KL,OR | FilmScoreDB(ours),EmoMV

Hình 5: Phân phối dữ liệu 20 nhà soạn nhạc hàng đầu

Hình 6: Phân phối phong cách phim top 20

A Công trình Liên quan

A.1 Tạo Video2Music
Như được minh họa trong Bảng 5, đã có một số tiến bộ trong nhiệm vụ tạo video-sang-âm nhạc. Một số công trình [10;7;55;20] dựa trên kiến trúc transformer để tạo tệp MIDI cho việc tạo âm nhạc một cách tự hồi quy. Tuy nhiên, hạn chế nằm ở sự đơn giản của âm nhạc được ghi lại trong tệp MIDI, điều này ngăn cản mô hình tạo ra âm nhạc phức tạp. Một số pipeline [53;54] sử dụng Codebook làm biểu diễn âm nhạc nhưng bị hạn chế bởi tính đa dạng của chúng do Codebook. LORIS [47] là một khung tạo âm nhạc dài hạn sử dụng Latent Diffusion Model [38], chủ yếu tập trung vào nhạc nền cho trượt băng và video khiêu vũ, và thiếu các tình huống ứng dụng khác. Tuy nhiên, DIFF-Foley [33] sử dụng các đặc trưng căn chỉnh thời gian và ngữ nghĩa được học thông qua pre-training đối lập audio-visual (CAVP) làm điều kiện và sau đó sử dụng LDM để tạo Mel-spectrogram. Nó hiệu quả chứng minh khả năng của mô hình khuếch tán và biểu diễn Mel-spectrogram trong nhiệm vụ video-sang-âm nhạc. Khác với các công trình trước, khung của chúng tôi có khả năng tự động tạo ra nhạc phim phức tạp với chi phí phải chăng.

A.2 Tạo sinh Có thể Điều khiển
Mặc dù tạo sinh có hướng dẫn có điều kiện đã đạt được kết quả ấn tượng trong khuếch tán, văn bản hoặc video một mình không thể điều khiển tinh vi nội dung được tạo, vì vậy tạo sinh có thể điều khiển đang dần nổi lên. ControlNet [48] sử dụng các adapter nhẹ để kết hợp các đầu vào có điều kiện (cạnh Canny, đường Hough, nét vẽ của người dùng, v.v.) vào các mô hình khuếch tán tiềm ẩn, có thể điều khiển tốt hơn quá trình tạo hình ảnh và tạo ra những hình ảnh cụ thể và tuân thủ hơn. Uni-ControlNet [51] có thể chấp nhận nhiều điều khiển cấp điểm ảnh thông qua một nhánh adapter duy nhất mà không cần chỉ định tất cả các điều khiển đồng thời, trong khi ControlNet yêu cầu một nhánh adapter riêng biệt cho mỗi điều khiển. Đồng thời, Điều khiển dựa trên Hướng dẫn thời gian Suy luận [14;1]/dựa trên Tối ưu hóa [43;35] cho phép điều chỉnh thời gian thực trong hướng của quá trình tạo sinh, cung cấp cho người dùng kiểm soát lớn hơn đối với đầu ra.

--- TRANG 14 ---
Hình 7: Minh họa các mẫu trong FilmScoreDB. Thông tin hiển thị cho mỗi mẫu bao gồm tiêu đề phim, tiêu đề nhạc phim và nhà soạn nhạc.

B Bộ dữ liệu

B.1 FilmScoreDB
Chúng tôi chú thích 280 bộ phim và thu được 4887 phân đoạn phim thô. Thời lượng của các phân đoạn này từ 5 giây đến 18 phút. Để tạo nhạc phim, chúng tôi loại bỏ các phân đoạn dưới 10 giây và chia những phân đoạn trên 10 giây thành các khoảng 10 giây. Bộ dữ liệu của chúng tôi bao gồm 134 nhà soạn nhạc nổi tiếng, bao gồm Hans Zimmer, John Williams và Danny Elfman, trong số những người khác. Những nhà soạn nhạc này được công nhận rộng rãi là những nhà soạn nhạc phim hàng đầu và các tác phẩm của họ thường xuyên nhận được đề cử Oscar và Grammy, điều này đảm bảo chất lượng cao của bộ dữ liệu nhạc phim của chúng tôi. Trong Hình 5, chúng tôi trình bày phân phối dữ liệu mẫu cho hai mươi nhà soạn nhạc hàng đầu. Phân phối thể hiện hiệu ứng đuôi dài, bao gồm các nhà soạn nhạc có ít tác phẩm hơn nhưng cực kỳ kinh điển. Sự đa dạng này hỗ trợ mô hình trong việc học nhạc phim trên các phong cách và kỹ thuật khác nhau. Trong Hình 6, chúng tôi mô tả phân phối dữ liệu mẫu cho hai mươi thể loại phim hàng đầu. Bộ dữ liệu của chúng tôi bao gồm một loạt các thể loại phim, bao gồm cốt truyện, hành động, tình yêu và nhiều hơn nữa. Thông thường, mỗi bộ phim có nhiều chú thích thể loại phim. Mặc dù các thể loại chiếm ưu thế có tỷ lệ cao hơn, hiệu ứng đuôi dài đảm bảo việc bao gồm nhiều thể loại phim đặc thù, tạo điều kiện nắm bắt những khác biệt tinh tế hơn và các tình huống thách thức. Nhìn chung, bộ dữ liệu của chúng tôi có quy mô lớn, với dữ liệu đáng kể cho các nhà soạn nhạc và thể loại phim hàng đầu, cung cấp hỗ trợ đầy đủ cho việc huấn luyện mô hình.

Trong Hình 7, chúng tôi trình bày một hình ảnh trực quan của bộ dữ liệu FilmScoreDB của chúng tôi, bao gồm các cặp video-âm nhạc. Mỗi mẫu chứa không chỉ tiêu đề phim, âm thanh, video, tiêu đề nhạc phim và nhà soạn nhạc, mà còn có nhãn thể loại phim tương ứng. Bộ dữ liệu của chúng tôi bao gồm một loạt đa dạng các danh mục phim. Đồng thời, chúng tôi cũng loại bỏ tất cả nội dung giọng hát khỏi dữ liệu âm thanh bằng Demucs2 để nâng cao chất lượng của đầu ra được tạo bởi mô hình. Demucs là một mô hình Tách nguồn Âm nhạc cực kỳ hiệu quả tách âm nhạc đầu vào thành các tệp đệm và giọng hát. Ở đây, chúng tôi chọn đệm làm dữ liệu âm thanh cuối cùng của chúng tôi. Chúng tôi không sở hữu bất kỳ bản quyền nào đối với các bộ phim và âm nhạc được tham chiếu trong bộ dữ liệu.

B.2 HIMV và EmoMV
Bộ dữ liệu HIMV [16] là một bộ dữ liệu video âm nhạc quy mô lớn cung cấp khoảng 100k liên kết YouTube. Chúng tôi sử dụng những liên kết này để tải xuống khoảng 40k video âm nhạc với Yt-dlp3. Tiếp theo, chúng tôi chọn hai phân đoạn 10 giây từ giữa mỗi video để xây dựng một bộ dữ liệu video-âm nhạc khoảng 80k. Những dữ liệu này được tiền xử lý bằng Demucs và được sử dụng để huấn luyện VM-Net của chúng tôi.

Bộ dữ liệu EmoMV bao gồm dữ liệu từ bộ dữ liệu MVED [36], Music Mood Dataset [3], và một số video âm nhạc gốc từ phim được thu thập trên YouTube. Tổng cộng, bộ dữ liệu bao gồm 5.370 điểm dữ liệu 30s và 616 điểm dữ liệu 10s. Tiếp theo, chúng tôi cắt dữ liệu 30s thành ba phân đoạn, dẫn đến một tập cuối cùng gồm 16.726 cặp video-âm nhạc để thực nghiệm.

2https://github.com/facebookresearch/demucs
3https://github.com/yt-dlp/yt-dlp

--- TRANG 15 ---
C Film Encoder

C.1 Đặc trưng Ngữ nghĩa
CLIP [37] là một mô hình được huấn luyện trước cho các nhiệm vụ ngôn ngữ-hình ảnh, bao gồm một bộ mã hóa văn bản ftext(·) và một bộ mã hóa hình ảnh fimage(·). Nó được huấn luyện trên các bộ dữ liệu hình ảnh-văn bản quy mô lớn sử dụng học tương phản để tăng cường sự tương đồng giữa hình ảnh và văn bản. Kết quả là, CLIP4 có thể tạo ra các tiên nghiệm ngữ nghĩa không chỉ cho hình ảnh mà còn cho video, có thể được sử dụng để tạo điều kiện cho việc tạo âm nhạc.

Để tạo ra các tiên nghiệm ngữ nghĩa video, đầu tiên chúng tôi trích xuất các đặc trưng 512 chiều từ các khung video bằng fimage(·) với tần số khung hình 10 khung mỗi giây. Sau đó, chúng tôi lấy trung bình chuỗi đặc trưng theo chiều thời gian để thu được đặc trưng ngữ nghĩa thị giác cs trong R1×512 của toàn bộ video. cs chứa các biểu diễn ngữ nghĩa khác nhau trong video, như cảnh, nhân vật và đối tượng, có thể hướng dẫn việc tạo âm nhạc.

C.2 Đặc trưng Thẩm mỹ
Chúng tôi sử dụng mô hình TAVAR5 [25] để trích xuất đặc trưng thẩm mỹ. Nó bao gồm ba thành phần chính: Mạng Phân tích Thuộc tính Thị giác (VAAN), Mạng Hiểu biết Chủ đề (TUN), Lý luận Thẩm mỹ Hai cấp.

1) Mạng Phân tích Thuộc tính Thị giác (VAAN), được sử dụng để học các thuộc tính thị giác cho nhận thức thẩm mỹ. Nó sử dụng kiến trúc ResNet50 [12] với các lớp kết nối đầy đủ được loại bỏ, cho phép trích xuất đặc trưng được chia sẻ trên các nhánh. Sau đó, chúng tôi sử dụng sáu Perceptron Đa lớp (MLP) để ánh xạ thêm các đặc trưng được chia sẻ thành sáu thuộc tính thị giác. Cụ thể, chúng tôi trích xuất 10 khung từ đoạn phim của chúng tôi, ký hiệu là ai (i = 1,2, ...,10), với tần số khung hình 1 khung mỗi giây. Đối với mỗi hình ảnh đầu vào ai, đặc trưng ẩn hi_a được thu thập từ mạng trích xuất đặc trưng được chia sẻ Fa(·) như:

hi_a=Fa(ai). (6)

Sau đó, sáu MLP được sử dụng để xây dựng sáu nhánh thuộc tính, ánh xạ thêm các đặc trưng ẩn hi_a của mỗi hình ảnh thành các thuộc tính thị giác âi, được định nghĩa như sau:

âi=MLPm(hi_a), (7)

trong đó m = 1,2, ...,6 biểu thị 6 MLP nhánh thuộc tính, và âi = {âi_1, âi_2, ..., âi_6} biểu thị sáu thuộc tính thị giác được dự đoán. Chúng tôi lấy trung bình của 10 khung để thu được 6 thuộc tính riêng biệt (trung bình của các khung video khác nhau) làm thuộc tính thị giác âj cho toàn bộ video. Những thuộc tính thị giác này sau đó được sử dụng làm đầu vào cho module Lý luận Thẩm mỹ Hai cấp.

2) Mạng Hiểu biết Chủ đề (TUN), sử dụng backbone ResNet-50 [12] để ban đầu dự đoán danh mục chủ đề của một hình ảnh. Sau đó, một Perceptron Đa lớp (MLP) với hàm kích hoạt PReLU ánh xạ hình ảnh đầu vào thành danh mục chủ đề được dự đoán, trong đó lớp kết nối đầy đủ cuối cùng tạo ra sáu đầu ra đại diện cho sáu danh mục chủ đề. Cuối cùng, một hoạt động phi tuyến softmax được thực hiện để tạo ra các xác suất chủ đề được dự đoán. Vì module này không chỉ dự đoán danh mục chủ đề của một hình ảnh mà còn tạo ra các đặc trưng chủ đề, chúng tôi đưa 10 khung của video phim vào module này để trích xuất đặc trưng chủ đề cho mỗi khung, được định nghĩa là:

b̂i=MLP(Ft(ai)). (8)

Sau đó, chúng tôi lấy trung bình các đặc trưng chủ đề của 10 khung này để thu được đặc trưng chủ đề b̂ cho video phim. Đặc trưng này cũng được đưa vào module Lý luận Thẩm mỹ Hai cấp. 3) Lý luận Thẩm mỹ Hai cấp. Đầu tiên, xem xét mối quan hệ giữa chủ đề hình ảnh và thuộc tính thị giác, các đặc trưng chủ đề phục vụ như các nút trung tâm, và tất cả các nút thuộc tính được kết nối với nút chủ đề trung tâm. Cuối cùng, các đặc trưng nút được thu thập thông qua mạng tích chập đồ thị (GCN). Ngoài ra, dựa trên mối quan hệ giữa các thuộc tính thị giác nhận thức chủ đề và thẩm mỹ chung, các đặc trưng thẩm mỹ chung được xử lý như nút trung tâm, và tất cả các nút thuộc tính thị giác nhận thức chủ đề được kết nối với nút trung tâm. Bằng cách sử dụng đồ thị thẩm mỹ thuộc tính, các đặc trưng nút được cập nhật tích hợp các thuộc tính thị giác nhận thức chủ đề và đặc trưng thẩm mỹ được thu thập.

4https://v-iashin.github.io/video_features/models/clip
5https://github.com/yipoh/TAVAR

Cuối cùng, chúng tôi thêm một lớp FC để ánh xạ các đặc trưng nút được cập nhật thành một điểm chất lượng thẩm mỹ tổng thể. Điểm chất lượng thẩm mỹ tổng thể sau đó được nhúng để thu được các đặc trưng thẩm mỹ ca trong R1×512, hướng dẫn việc tạo âm nhạc.

C.3 Đặc trưng Cảm xúc
Ở đây, chúng tôi sử dụng một mô hình Phát hiện và Dự đoán Cảm xúc Video Được giám sát Yếu (WECL6) được huấn luyện trước [49], để dự đoán thông tin cảm xúc trong video. Chúng tôi nhúng các nhãn phân loại one-hot E vào đặc trưng Cảm xúc ce trong R1×512 qua chiếu tuyến tính:

ce=Embed(E). (9)

C.4 Fusion Đặc trưng
Đối với đặc trưng ngữ nghĩa cs, đặc trưng thẩm mỹ ca và đặc trưng cảm xúc ce, nếu chúng tôi sử dụng nối vector đặc trưng điển hình, chúng tôi sẽ đối mặt với lời nguyền của chiều và thiếu tương tác giữa các đặc trưng. Do đó, chúng tôi đã sử dụng một khối fusion đặc trưng đơn giản được gọi là Lightweight Attention Feature Fusion (LAFF) [18]. Trong phương pháp này, chúng tôi kết hợp các đặc trưng ngữ nghĩa, cảm xúc và thẩm mỹ của video trong một khối LAFF cụ thể sử dụng một kết hợp lồi, trong đó các trọng số fusion được học để tối ưu hóa việc tạo âm nhạc phim.

D Chỉ số Khách quan

D.1 BCS, BHS, điểm F1, CSD và HSD
Ở đây, chúng tôi sử dụng các phiên bản cải tiến của Bar Coverage Score (BCS) và Bar Hit Score (BHS) để đánh giá liên quan nhịp điệu. BCS và BHS thường được đánh giá bằng cách tính toán các nhịp đập nhịp điệu được căn chỉnh giữa âm nhạc được tổng hợp và thực, nhưng do những chỉ số này chỉ áp dụng cho âm nhạc độ dài ngắn (≤ 6s), hai vấn đề chính phát sinh khi đánh giá âm nhạc dài: 1) Thuật toán phát hiện nhịp điệu theo giây sẽ dẫn đến các vector cực kỳ thưa thớt cho bất kỳ chuỗi âm nhạc dài nào, do đó các giá trị BCS và BHS thấp một cách nhất quán không phản ánh được hiệu suất thực tế. 2) Nếu âm nhạc được tạo ra có nhiều nhịp đập nhịp điệu hơn ground truth, BHS có thể dễ dàng vượt quá 1, điều này sẽ làm cho các giá trị chỉ số xuất hiện hoàn hảo trong khi hiệu suất mỗi mẫu còn lâu mới đạt được. Chúng tôi sử dụng hai BCS và BHS được sửa đổi tương tự LORIS [47]: Đầu tiên, điều chỉnh các tham số của thuật toán phát hiện khởi phát âm thanh để tránh các vector nhịp điệu thưa thớt. Thứ hai, tính BCS bằng cách chia các nhịp đập được căn chỉnh cho tổng số nhịp đập của âm nhạc được tạo (Ba/Bg), trong đó BCS và BHS phục vụ như recall và precision tương ứng. Ngoài ra, chúng tôi sử dụng điểm F1 của BCS và BHS như một đánh giá toàn diện, và CSD và HSD (độ lệch chuẩn của BCS và BHS) để đánh giá tính ổn định tạo sinh.

D.2 Melody acc và Dynamics corr
Melody Accuracy đánh giá sự phù hợp giữa các lớp cao độ khung-wise (C, C#, ..., B; tổng cộng 12) của điều khiển giai điệu đầu vào và những cái được trích xuất từ đầu ra được tạo. Dynamics Correlation đo tương quan Pearson giữa các giá trị động lực khung-wise đầu vào và những giá trị được tính toán từ thế hệ. Hai loại tương quan được tính toán: micro và macro. Micro tính toán r cho mỗi thế hệ riêng biệt, trong khi macro thu thập các giá trị động lực đầu vào/thế hệ trên tất cả các thế hệ và tính toán một r duy nhất. Tương quan micro kiểm tra liệu các giá trị điều khiển động lực tương đối có được tôn trọng trong một thế hệ hay không, trong khi tương quan macro đánh giá cùng một thuộc tính trên nhiều thế hệ.

6https://github.com/nku-zhichengzhang/CTEN

--- TRANG 16 ---
E Ví dụ Bổ sung
Trong Hình 8, chúng tôi hiển thị ba ví dụ bổ sung về chuyển đổi phong cách sáng tác với HPM. Trong ba ví dụ này, mô hình của chúng tôi có thể làm nổi bật giai điệu của âm nhạc phong cách (hình chữ nhật đỏ) bằng cách học giai điệu chính của âm nhạc phong cách trong khi tăng cường các phân đoạn âm nhạc phù hợp với nhịp khiêu vũ video (được khoanh tròn màu vàng) và loại bỏ nhiễu giữa các nốt nhạc khác nhau (được khoanh tròn màu đỏ). Điều này làm cho âm nhạc được tạo ra gọn gàng hơn và tương thích với nội dung video.

(a)
(b) 
(c)

Hình 8: Kết quả Chuyển đổi Phong cách Sáng tác Thêm trên FilmScoreDB. Chúng tôi trực quan hóa các spectrogram của Style Music và Generated Music.
