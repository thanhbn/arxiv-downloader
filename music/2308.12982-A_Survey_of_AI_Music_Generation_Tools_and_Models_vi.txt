# 2308.12982.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/music/2308.12982.pdf
# Kích thước tệp: 665952 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Khảo Sát Về Các Công Cụ và Mô Hình Tạo Nhạc AI
Yueyue Zhu∗Jared Baca†Banafsheh Rekabdar‡
Reza Rawassizadeh§
28 tháng 8, 2023
Tóm tắt
Trong nghiên cứu này, chúng tôi cung cấp một khảo sát toàn diện về các công cụ tạo nhạc AI,
bao gồm cả các dự án nghiên cứu và ứng dụng thương mại. Để thực hiện phân tích, chúng tôi
đã phân loại các phương pháp tạo nhạc thành ba danh mục: lớp dựa trên tham số,
dựa trên văn bản và dựa trên hình ảnh. Khảo sát của chúng tôi làm nổi bật các khả năng
đa dạng và tính năng chức năng của các công cụ này, phục vụ cho nhiều đối tượng người dùng,
từ những người nghe thường đến các nhạc sĩ chuyên nghiệp. Chúng tôi quan sát thấy rằng mỗi công cụ
đều có những ưu điểm và hạn chế riêng. Do đó, chúng tôi đã tổng hợp một danh sách
toàn diện các yếu tố này cần được xem xét trong quá trình lựa chọn công cụ.
Hơn nữa, khảo sát của chúng tôi đưa ra những hiểu biết sâu sắc về các cơ chế cơ bản và
thách thức của việc tạo nhạc AI.

1 Giới thiệu
Âm nhạc là một phần không thể thiếu của văn hóa nhân loại đã phát triển đáng kể qua các
thế kỷ, thích ứng với các nền văn hóa, phong cách và công nghệ khác nhau [18]. Việc tạo nhạc bởi
các mô hình cũng đã trải qua một sự thay đổi mô hình với những tiến bộ trong AI và học máy
[38]. Các công cụ tạo nhạc trí tuệ nhân tạo (AI) cung cấp cho các nhạc sĩ và
nhà soạn nhạc những cách thức mới và sáng tạo để tạo ra âm nhạc, điều này không chỉ tạo điều kiện
cho việc thể hiện ý định âm nhạc của người dùng mà còn có tác động đáng kể đến quyền sở hữu sáng tạo
và sự tự tin của họ trong khả năng hợp tác với công nghệ AI [10, 56, 107].
Các công cụ này [5, 57, 43, 3, 24] sử dụng các thuật toán học máy để học từ các
tập dữ liệu âm nhạc lớn và nhằm tạo ra các tác phẩm âm nhạc mới không thể phân biệt được với
âm nhạc do con người tạo ra.

Sự xuất hiện của các mạng nơ-ron sâu, hay còn gọi là học sâu, từ năm 2012, đã cách mạng hóa
một số lĩnh vực khoa học máy tính [52], bao gồm cả việc tạo nhạc AI. Một số mô hình học sâu
có thể tạo ra các chuỗi nốt ngắn hạn, nhưng việc tạo ra các giai điệu dài hơn đã trở thành
khả thi thông qua việc phát triển các kiến trúc mạng nơ-ron gần đây, chẳng hạn như

∗Khoa Khoa học Máy tính, Đại học Metropolitan, Đại học Boston, Boston, MA, USA.
Email: yueyuez@bu.edu
†Khoa Viết Lách và Sản xuất Đương đại, Đại học Âm nhạc Berklee, Boston, MA, USA.
Email: jbaca@berklee.edu
‡Khoa Khoa học Máy tính, Đại học Kỹ thuật và Khoa học Máy tính Maseeh, Đại học Bang Portland,
Portland, OR, USA. Email: rekabdar@pdx.edu
§Khoa Khoa học Máy tính, Đại học Metropolitan, Đại học Boston, Boston, MA, USA.
Email: rezar@bu.edu

1arXiv:2308.12982v1 [cs.SD] 24 Aug 2023

--- TRANG 2 ---
như MusicVAE [76] và TransformerVAE [46], và các mô hình sinh như Denoising
Diffusion Probabilistic Models [38]. Tuy nhiên, các mô hình này tạo ra các giai điệu đa âm dài hơn
không nhất thiết phải tuân theo một chủ đề trung tâm và có thể thiếu cảm giác định hướng.
Các mô hình học sâu đã được sử dụng để hòa âm, tạo ra hòa âm đi kèm với một
giai điệu cho trước, và các kỹ thuật chuyển đổi phong cách đã được sử dụng để biến đổi
âm nhạc theo một phong cách nhất định thành phong cách khác. Briot, Jean-Pierre và cộng sự [9] đã thảo luận về
các hạn chế trong việc áp dụng trực tiếp học sâu vào tạo nhạc, bao gồm việc thiếu
sáng tạo và kiểm soát và sự cần thiết của tương tác con người trong quá trình sáng tác.
Các mô hình AI dựa trên dữ liệu đôi khi có xu hướng tạo ra các biến thể của
các mẫu hiện có thay vì các tác phẩm hoàn toàn nguyên bản [9]. Ràng buộc này bắt nguồn từ sự phụ thuộc
của chúng vào dữ liệu đào tạo, vốn làm giới hạn đầu ra sáng tạo của chúng một cách vốn có.

Trong khảo sát này, đầu tiên, chúng tôi giải thích các thuật ngữ cơ bản phổ biến trong tạo nhạc,
cũng được áp dụng trong âm nhạc do AI tạo ra. Sau đó, chúng tôi sẽ khám phá tình trạng hiện tại
của các công cụ và mô hình tạo nhạc AI, đánh giá chức năng của chúng và thảo luận về
các hạn chế của chúng. Cuối cùng, bằng cách phân tích các công cụ và kỹ thuật mới nhất, chúng tôi nhằm cung cấp
một hiểu biết toàn diện về tiềm năng của việc sáng tác âm nhạc dựa trên AI và
các thách thức phải được giải quyết để cải thiện hiệu suất của chúng.

Khảo sát này nhằm cung cấp tổng quan về các công cụ và mô hình tạo nhạc AI,
khả năng và hạn chế của chúng. Chúng tôi bắt đầu bằng việc giải thích các khái niệm cho những độc giả
không quen thuộc với sáng tác âm nhạc. Sau đó, chúng tôi mô tả các phương pháp của chúng tôi. Cụ thể,
chúng tôi bắt đầu bằng việc giải thích phương pháp thu thập dữ liệu của chúng tôi. Sau đó, chúng tôi liệt kê các phương pháp truyền thống
không có mạng nơ-ron và tạo nhạc. Tiếp theo, chúng tôi sẽ xem xét các
công cụ tạo nhạc dựa trên AI phổ biến có sẵn ngày nay. Các công cụ này là mã nguồn mở và đã được
sử dụng bởi một số nhà nghiên cứu và nhà phát triển để tạo ra âm nhạc do AI tạo ra. Tuy nhiên, chỉ
một số mô hình mà chúng tôi xem xét là mã nguồn mở, và trong những trường hợp như vậy, chúng tôi dựa vào
các bản demo hoặc giải thích chính thức để so sánh. Phạm vi của nghiên cứu này được giới hạn trong các công cụ và mô hình tạo nhạc AI
sử dụng các thuật toán học máy để tạo ra âm nhạc.
Chúng tôi sẽ không bao gồm các ứng dụng rộng hơn của AI trong âm nhạc, chẳng hạn như phân loại âm nhạc,
hệ thống khuyến nghị và phân tích âm nhạc.

2 Các Khái niệm Sáng tác Âm nhạc
Trong phần này, chúng tôi sẽ khám phá các khái niệm cơ bản góp phần vào cấu trúc
và tổ chức của một tác phẩm âm nhạc. Việc hiểu sự tương tác của chúng là rất quan trọng cho
việc phát triển các công cụ tạo nhạc AI.

Âm điệu đại diện cho một âm thanh có cao độ xác định, được đặc trưng bởi tần số, biên độ
và tông màu của nó [80]. Nó đóng vai trò là đơn vị cơ bản trong sáng tác âm nhạc, cho phép
tạo ra các giai điệu, hợp âm và các cấu trúc âm nhạc khác.

Cao độ (Khóa) biểu thị tần số được cảm nhận của một âm thanh, xác định vị trí của nó trên
quang phổ cao-thấp [77]. Một tác phẩm âm nhạc thường được sáng tác theo một cao độ cụ thể,
thiết lập trung tâm âm điệu và điều chỉnh các mối quan hệ giữa các nốt [51].

Tông màu thường được gọi là màu sắc âm điệu hoặc chất lượng của âm thanh, là đặc tính
phân biệt các nguồn âm thanh khác nhau ngay cả khi chúng có cùng cao độ và
âm lượng [75, 84].

Hòa âm đề cập đến sự kết hợp đồng thời của các cao độ hoặc âm điệu khác nhau tạo ra
một cảm giác thính giác dễ chịu cho người nghe [65].

Hợp âm đề cập đến các nhóm nốt được chơi đồng thời, tạo thành nền tảng của hòa âm
trong âm nhạc.

--- TRANG 3 ---
Nhịp độ biểu thị tốc độ mà một tác phẩm âm nhạc được biểu diễn, thường được đo
bằng số nhịp mỗi phút (BPM) [55]. Nhịp độ có thể ảnh hưởng đáng kể đến tâm trạng
và tác động cảm xúc của một tác phẩm, với nhịp độ nhanh hơn thường được liên kết với sự hứng khởi hoặc năng lượng
và nhịp độ chậm hơn liên quan đến sự bình tĩnh hoặc buồn [89]. Các công cụ tạo nhạc AI có thể
điều chỉnh nhịp độ một cách chiến lược để gợi lên những cảm xúc cụ thể ở người nghe, tùy chỉnh các
tác phẩm được tạo ra để phù hợp với tâm trạng hoặc cảm xúc mong muốn.

Âm lượng biểu thị độ lớn được cảm nhận của âm thanh, có liên quan chặt chẽ đến biên độ
hoặc cường độ của nó. Đây là một đại lượng vô hướng đại diện cho cường độ của năng lượng âm thanh
được truyền, thường được đo bằng decibel (dB) [60].

Phong cách bao gồm các đặc điểm và kỹ thuật đặc biệt được sử dụng bởi một nhà soạn nhạc hoặc
người biểu diễn, do đó hình thành bản sắc độc đáo của các tác phẩm âm nhạc của họ [59]. Khi phong cách
được áp dụng cho các công cụ tạo nhạc AI, việc phân tích và học hỏi từ âm nhạc hiện có do nhạc sĩ
tạo ra cho phép mô phỏng các phong cách từ các nhà soạn nhạc hoặc thể loại khác nhau, do đó
tạo ra các tác phẩm mới phản ánh các đặc điểm nghệ thuật độc đáo của nghệ sĩ
hoặc thời đại lịch sử.

Điệp khúc đề cập đến một phần lặp lại trong một bài hát, thường có giai điệu đáng nhớ
và lời bài hát truyền tải chủ đề trung tâm của tác phẩm [16].

Âm nhạc đa âm đề cập đến âm nhạc bao gồm nhiều đường giai điệu độc lập
được chơi hoặc hát đồng thời. Các đường giai điệu này tương tác với nhau để tạo ra hòa âm,
đối âm và kết cấu phong phú và phức tạp hơn so với âm nhạc đơn âm,
bao gồm một đường giai điệu duy nhất.

MIDI (Musical Instrument Digital Interface) là một giao thức tiêu chuẩn để giao tiếp
giữa các nhạc cụ điện tử, máy tính và các thiết bị kỹ thuật số khác. MIDI cho phép
trao đổi thông tin âm nhạc, chẳng hạn như nốt, vận tốc và thông điệp điều khiển,
giữa các thiết bị và ứng dụng phần mềm khác nhau. Nó cho phép các nhạc sĩ và nhà sản xuất
điều khiển và đồng bộ hóa các nhạc cụ và thiết bị khác nhau, và ghi lại và chỉnh sửa
các buổi biểu diễn âm nhạc với độ chính xác và linh hoạt.

Vận tốc phím còn được gọi là vận tốc đánh phím hoặc vận tốc gõ phím, là một phép đo
về mức độ mạnh mẽ của việc nhấn phím trên bàn phím MIDI hoặc bộ điều khiển MIDI khác. Giá trị này
thường được biểu thị bằng một số từ 0 đến 127, với 0 cho biết phím
không được nhấn, và 127 cho biết phím được nhấn với lực tối đa.

ABC hoặc ký hiệu abc [1] là một hệ thống ký hiệu tắt để viết nhạc bằng
ký tự ASCII. Được phát triển trong những năm 1970 và 1980, nó thường được sử dụng trong
các truyền thống âm nhạc Celtic và dân gian để chia sẻ và phân phối âm nhạc truyền thống. Nó sử dụng
các chữ cái và ký hiệu để biểu diễn các nốt nhạc, nhịp điệu và các yếu tố khác, làm cho nó
trở thành một phương pháp dễ học và được sử dụng rộng rãi để chia sẻ âm nhạc trong
các định dạng kỹ thuật số như qua Internet.

Pianoroll là một giao diện trong Digital Audio Workstations (DAWs) [53] cho phép thao tác
dữ liệu Musical Instrument Digital Interface (MIDI). Nó sử dụng một lưới nơi
trục X biểu thị thời gian và trục Y biểu thị cao độ. Thời lượng và cường độ
(vận tốc) của các nốt có thể điều chỉnh được, làm cho chúng trở thành một phần không thể thiếu của cấu trúc
của các tác phẩm âm nhạc và quan trọng cho việc phát triển các công cụ tạo nhạc AI.

Chromagram là một hình ảnh hóa minh họa cường độ của các cao độ khác nhau trong một
tác phẩm âm nhạc theo thời gian. Như được thể hiện trong Hình 1, mỗi lớp cao độ (C đến B) tương ứng
với một hàng duy nhất trên trục y và trục x biểu thị thời gian. Màu sắc tại mỗi
điểm trong không gian 2D này tiết lộ mức năng lượng của một lớp cao độ tại thời điểm đã cho,
màu sắc sáng hơn cho biết năng lượng cao hơn. Hình 1 Chromagram mô tả giai điệu của "Twinkle
Twinkle Little Star".

--- TRANG 4 ---
Phần đệm trong âm nhạc đề cập đến các yếu tố hỗ trợ, thường là hòa âm, cung cấp
nền tảng cho giai điệu hoặc chủ đề chính của một bài hát.

Hình 1: Chromagram của "Twinkle Twinkle Little Star", minh họa năng lượng lớp cao độ
theo thời gian.

2.1 Sự tương tác của các khái niệm trong Âm nhạc do AI tạo ra
Các công cụ tạo nhạc AI có thể tạo ra các tác phẩm với các tiến trình nhất quán và
có tính thẩm mỹ bằng cách kết hợp kiến thức về hòa âm và hợp âm. Ngoài ra, việc xác định
các mẫu trong điệp khúc của các bài hát phổ biến cho phép các công cụ tạo nhạc AI tạo ra
các bài hát mới với các hook giai điệu hấp dẫn và đáng nhớ cộng hưởng với người nghe.

Kết luận, việc hiểu sự tương tác của các khái niệm âm nhạc cơ bản này là
quan trọng để phát triển các công cụ tạo nhạc AI tinh vi có khả năng tạo ra các tác phẩm
giống con người có tính biểu cảm về mặt cảm xúc và phù hợp về mặt bối cảnh. Bằng cách
mô hình hóa và sử dụng hiệu quả các khái niệm này, một mô hình AI có thể tạo ra các tác phẩm
có khả năng làm phong phú cảnh quan âm nhạc với các cấu trúc và chủ đề sáng tạo,
chứng minh một hướng đi hứa hẹn cho tương lai của việc tạo nhạc và công nghệ.
Hơn nữa, kiến thức này thu hẹp khoảng cách giữa việc tạo nhạc truyền thống và âm nhạc
do AI tạo ra, tạo điều kiện cho hợp tác liên ngành và thúc đẩy những tiến bộ trong
công nghệ âm nhạc.

3 Phương pháp
3.1 Thu thập Dữ liệu
Để tạo ra một danh sách toàn diện về các công cụ và mô hình tạo nhạc AI, chúng tôi đã sử dụng
phương pháp tìm kiếm từ khóa trên một số nền tảng, cùng với sự hỗ trợ của ChatGPT
v3.5, ChatGPT v4 và Bard¹ để tinh chỉnh danh sách từ khóa tìm kiếm và tài nguyên web. Để
xác định danh sách từ khóa, đầu tiên, ba tác giả của bài báo tổng hợp danh sách từ khóa
riêng biệt, sau đó họ tích lũy kết quả. Tiếp theo, chúng tôi sử dụng các Mô hình Ngôn ngữ Lớn (LLMs)
đã liệt kê để tinh chỉnh danh sách từ khóa và tinh chỉnh danh sách tài nguyên web
để tìm kiếm các từ khóa này.

¹https://bard.google.com

--- TRANG 5 ---
Danh sách tài nguyên web: Google Search, Google News, Bing News, Google Scholar,
Twitter, GitHub, YouTube, Reddit², Hacker News³, và Huggingface⁴

Danh sách từ khóa: AI music, AI music generation, Diffusion music generation,
Neural Networks Music Generation, Machine Learning Music, Music Generation Models,
Music Generation Algorithms, Music AI, Music Technology, Computer-generated Music,
Deep Learning Music.

Lời nhắc chúng tôi đã sử dụng trên nền tảng LLM như sau: Tôi đang tìm kiếm âm nhạc
được tạo ra bởi AI trong các nền tảng sau; [tài nguyên web] bằng cách sử dụng các từ khóa sau;
[từ khóa]. Bạn có đề xuất nào về nền tảng hoặc từ khóa mà tôi đã bỏ lỡ không?

3.2 Phân loại Công cụ Tạo Nhạc
Chúng tôi cung cấp một biểu diễn trực quan về sự tiến triển của các mô hình tạo nhạc này
từ khi bắt đầu đến nay trong Hình 2. Dòng thời gian theo dõi sự phát triển
từ các phương pháp không có mạng nơ-ron ban đầu đến các mô hình không có tham số
được hỗ trợ bởi AI mới nhất, minh họa rõ ràng những tiến bộ đáng kể trong công nghệ tạo nhạc
qua các năm.

Hình 2: Dòng thời gian của các Mô hình Tạo Nhạc

Theo truyền thống, các công cụ AI tạo nhạc sử dụng nhiều phương pháp khác nhau, bao gồm chuỗi Markov
[33], mô hình dựa trên quy tắc [94, 90], và thuật toán tiến hóa [107, 29, 102]. Các

²https://reddit.com
³https://news.ycombinator.com
⁴https://huggingface.co

--- TRANG 6 ---
phương pháp này thường dựa trên tham số, đòi hỏi đầu vào của con người về các tham số
hoặc cấu hình liên quan để hướng dẫn quá trình tạo nhạc.

Các mô hình sau đó đã dựa vào mạng nơ-ron. Vì chất lượng của nhạc được tạo ra
thông qua mạng nơ-ron tốt hơn đáng kể so với các phương pháp truyền thống, chúng tôi tách
giải thích của chúng trong khảo sát này. Chúng tôi phân loại các mô hình mạng nơ-ron thành hai loại:
mô hình dựa trên tham số và mô hình không có tham số. Mô hình dựa trên tham số là những mô hình
yêu cầu các tham số đầu vào cụ thể, chẳng hạn như 'nhịp độ' hoặc 'khóa' mong muốn, để tạo nhạc.
Mặt khác, mô hình không có tham số không yêu cầu bất kỳ tham số đầu vào cụ thể nào,
và chúng được chia thành hai danh mục con: mô hình dựa trên gợi ý và mô hình dựa trên hình ảnh.
Mô hình dựa trên gợi ý cho phép người dùng nhập văn bản mô tả làm gợi ý để tạo
nhạc, trong khi mô hình dựa trên hình ảnh sử dụng đầu vào hình ảnh như hình ảnh hoặc video.

Hơn nữa, Hình 3 trình bày một phân loại phân cấp của các mô hình tạo nhạc,
làm sáng tỏ thêm về các phân loại và danh mục con của các mô hình này. Cấu trúc cây này
cho phép hiểu biết chi tiết hơn về sự khác biệt và mối quan hệ
giữa các mô hình khác nhau, cũng như sự tiến hóa của chúng theo thời gian.

Hình 3: Phân loại Phân cấp của các Mô hình Tạo Nhạc

3.2.1 Phương pháp Không có Mạng Nơ-ron
Dựa trên các nghiên cứu của chúng tôi và sự hỗ trợ của các mô hình ngôn ngữ lớn (Bard⁶, ChatGPT 3.5
⁷, Claude1⁸), chúng tôi đã xác định có ba phương pháp không có mạng nơ-ron để xây dựng
âm nhạc; bao gồm, Chuỗi Markov, mô hình dựa trên Quy tắc, và thuật toán Tiến hóa. Bảng
1 minh họa các phương pháp này, chức năng chính của chúng và các mô hình cơ bản.
Cột Phân loại trong các bảng của chúng tôi, như đã thảo luận trong Phần Phân loại Công cụ Tạo
Nhạc, phân loại các công cụ tạo nhạc dựa trên loại đầu vào mà chúng
yêu cầu để tạo nhạc.

⁶https://bard.google.com
⁷https://openai.com/blog/chatgpt
⁸https://www.anthropic.com/product

--- TRANG 7 ---
Bảng 1: So sánh các công cụ tạo nhạc không có mạng nơ-ron

Công cụ/Phương pháp | Chức năng Chính | Mô hình Không có Mạng Nơ-ron | Phân loại
---|---|---|---
Markov Melody Generator [39] | Phân tích các mẫu trong nhạc hiện có để tạo ra các tác phẩm mới | Chuỗi Markov [85, 6, 71] | Dựa trên tham số
Melisma Stochastic Melody Generator [88] | Tạo ra các chuỗi giai điệu bằng cách sử dụng các quá trình ngẫu nhiên | Chuỗi Markov [88] | Dựa trên tham số
MuseScore [83] | Cho phép sáng tác dựa trên các mẫu do người dùng định nghĩa | Hệ thống Dựa trên Quy tắc [83] | Dựa trên tham số
APOPCALEAPS [90] | Tạo ra nhạc pop dựa trên các quy tắc cơ hội | Hệ thống Dựa trên Quy tắc [94, 90] | Dựa trên tham số
GenJam [7] | Sử dụng lựa chọn, đột biến và lai ghép các chuỗi âm nhạc để tạo ra nhạc mới | Thuật toán Di truyền [7] | Dựa trên tham số
Procedural Piano Composition [17] | Tiến hóa các tác phẩm nhạc ngẫu nhiên sử dụng mẫu tâm trạng cho đến khi tâm trạng của chúng phù hợp với mẫu | Thuật toán Di truyền [17] | Dựa trên tham số
Marques' Music Generator [58] | Sử dụng các khái niệm lý thuyết giai điệu và âm nhạc như một hàm fitness trong thuật toán di truyền | Thuật toán Di truyền [58] | Dựa trên tham số

--- TRANG 8 ---
Bảng 2: So sánh các công cụ tạo nhạc mạng nơ-ron

Tên Công cụ | Chức năng Chính | Giao diện Truy cập | Mô hình Mạng Nơ-ron | Phân loại
---|---|---|---|---
Magenta [5] | Tạo giai điệu, trống, piano, hợp âm sử dụng học sâu | Plug-in, dòng lệnh, Python, jscript | RNN-LSTM [61, 86], VAE [48], GANSynth [27], WaveNet [97], Mạng Transformer [100] | Dựa trên tham số
Jukebox [23, 62] | Tạo nhạc với tổng hợp giọng hát | Python | VQ-VAE [98] | Dựa trên tham số
Music Generation with Sentiment [30] | Tạo nhạc với phân tích cảm xúc | Python | mLSTM [50] | Dựa trên tham số
MuseNet [64] | Tạo nhạc với khả năng chuyển đổi phong cách | Dựa trên Web | Transformer [100] (Chỉ giải mã) | Dựa trên tham số
Music Transformer [41] | Tạo nhạc với cấu trúc dài hạn | Python | Transformer [100] | Dựa trên tham số
SDMuse [104] | Khung chỉnh sửa và tạo nhạc | Chưa phát hành | SDE [93], Gaussian Diffusion Model⁵ | Dựa trên tham số
Riffusion [31] | Tạo clip âm thanh từ văn bản và hình ảnh | Giao diện Web | Stable Diffusion v1.5 [78] | Dựa trên gợi ý
Moˆ usai [82] | Tạo nhạc từ gợi ý văn bản | Python | Diffusion Magnitude Autoencoder (DMAE) [66] | Dựa trên gợi ý
Noise2Music [44] | Tạo âm thanh nhạc chất lượng cao từ gợi ý văn bản | Chưa phát hành | Diffusion Model [72, 91] | Dựa trên gợi ý
MusicLM [3] | Tạo nhạc độ trung thực cao từ gợi ý văn bản | Chưa phát hành | SoundStream, w2vBERT, và MuLan | Dựa trên gợi ý
CMT [24] | Tạo nhạc nền video | Python | Transformer | Dựa trên hình ảnh (video)
V-MusProd [109] | Tạo nhạc từ video với việc tách rời hợp âm, giai điệu và phần đệm | Chưa phát hành | Tùy chỉnh | Dựa trên hình ảnh (video)
Foley Music [32] | Tạo nhạc từ các clip video sử dụng âm thanh foley | PyTorch | Encoder-decoder | Dựa trên hình ảnh (video)
Công cụ Thương mại | Tạo nhạc với các tham số có thể tùy chỉnh | Giao diện Web | Không xác định | Dựa trên tham số và dựa trên gợi ý

--- TRANG 9 ---
Chuỗi Markov là các mô hình toán học được sử dụng để phân tích và dự đoán hành vi
của các hệ thống thể hiện một thuộc tính gọi là thuộc tính Markov [36]. Thuộc tính này
nói rằng trạng thái tương lai của một hệ thống chỉ phụ thuộc vào trạng thái hiện tại của nó,
và không phụ thuộc vào lịch sử quá khứ. Bằng cách dự đoán xác suất của các chuyển đổi
giữa các nốt nhạc hoặc sự kiện khác nhau, chúng đã được sử dụng trong việc tạo giai điệu
để tạo ra các giai điệu mới trôi chảy và tự nhiên [6]. Sau đó, chuỗi Markov cũng đã được
sử dụng để tạo ra âm nhạc tùy chỉnh dựa trên đầu vào thủ công của người dùng về tâm trạng [71].
Gần đây, Shapiro và cộng sự [85] sử dụng chuỗi Markov để tạo nhạc để phân tích
các mẫu trong nhạc hiện có và tạo ra các tác phẩm mới.

Một nhóm công trình khác tập trung vào Tạo nhạc dựa trên Quy tắc, sử dụng các quy tắc
được định nghĩa trước để tạo ra dữ liệu âm nhạc theo các mẫu hoặc phong cách cụ thể.
Một ví dụ về phương pháp này được cung cấp bởi Spangler [94], người đã giới thiệu một hệ thống
để đệm thời gian thực. Hệ thống này trích xuất và sử dụng một tập hợp các quy tắc hòa âm
từ các ví dụ âm nhạc để tạo ra các hòa âm mới phản ứng với một giai điệu đầu vào,
chứng minh việc áp dụng các tập hợp quy tắc xác định trong việc tạo nhạc. Theo một hướng khác,
Sneyers và De Schreye [90] đã phát triển APOPCALEAPS, một hệ thống để tạo ra
và học nhạc pop tận dụng một dạng quy tắc xác suất độc đáo. Các quy tắc này, được học
tự động từ các ví dụ, cho phép một loạt các kết quả âm nhạc tiềm năng, cung cấp một
yếu tố ngẫu nhiên trong các tham số được xác định. Phương pháp dựa trên cơ hội này đã được
chứng minh là thúc đẩy tính linh hoạt và sáng tạo trong việc tạo nhạc.

Một nhóm công trình thứ ba tập trung vào thuật toán tiến hóa, cụ thể là thuật toán di truyền,
trong lĩnh vực tạo nhạc. Các thuật toán này tham gia vào việc xác định có chọn lọc
các chuỗi âm nhạc tốt nhất, tinh chỉnh chúng thông qua đột biến và lai ghép,
do đó thúc đẩy việc tạo ra các tác phẩm mới. Năm 1994, Biles đã giới thiệu GenJam (Genetic
Jammer) [7], một thuật toán di truyền tương tác tiến hóa các bản solo jazz improvisation
bằng cách tiến hóa các quần thể biện pháp âm nhạc, với thuật toán sử dụng phản hồi
thời gian thực từ một người cố vấn là hàm fitness. Sau đó vào năm 2000, Marques và cộng sự
[58] đã áp dụng thuật toán di truyền và tiến hóa vào việc tạo nhạc, trong đó các khái niệm
lý thuyết giai điệu và âm nhạc được sử dụng như một hàm fitness, cho phép điều hướng
hiệu quả qua các không gian tìm kiếm lớn để tìm ra các giải pháp sáng tạo. Gần đây hơn,
vào năm 2021, Rocha de Azevedo Santos và cộng sự [17] đã sử dụng thuật toán di truyền
để tạo ra một hệ thống sáng tác nhạc piano sử dụng mẫu tâm trạng. Hệ thống này tiến hóa
các tác phẩm nhạc ngẫu nhiên cho đến khi tâm trạng của chúng phù hợp với mẫu, sử dụng
các đặc điểm thống kê MIDI để tính toán khoảng cách nhạc-mẫu. Mặc dù có một số ràng buộc,
chẳng hạn như cảm giác được tạo ra bởi máy tính do thiếu tính đều đặn và đồng bộ của nhịp điệu,
các ví dụ này nhấn mạnh tiềm năng của thuật toán tiến hóa trong việc tạo nhạc tự động
và vai trò của chúng trong việc mô phỏng các chiến lược sáng tạo của các nhà soạn nhạc hiện đại.

3.2.2 Tạo Nhạc dựa trên Mạng Nơ-ron
Trong phần sau, chúng tôi thực hiện một kiểm tra có hệ thống về các mô hình tạo nhạc
được điều khiển bởi mạng nơ-ron. Cuộc thảo luận được chia thành ba phần con riêng biệt,
mỗi phần tập trung vào một loại mô hình cụ thể: mô hình dựa trên tham số, mô hình dựa trên gợi ý,
và mô hình dựa trên hình ảnh. Trong mỗi trường hợp, chúng tôi cung cấp một kiểm tra kỹ lưỡng
về kiến trúc mô hình, phân định các tính năng và khả năng chính của nó. Ngoài ra, chúng tôi đưa ra
một đánh giá cân bằng về các lợi ích và hạn chế tiềm năng của mỗi mô hình. Mục tiêu của chúng tôi
là cung cấp một cái nhìn toàn diện về bối cảnh của các mô hình tạo nhạc dựa trên mạng nơ-ron,
làm sáng tỏ các cơ chế khác nhau mà chúng hoạt động và
những ưu điểm và nhược điểm tương ứng.

Bảng 2 cung cấp tóm tắt về các công cụ tạo nhạc khác nhau dựa trên chức năng chính,
giao diện truy cập, mô hình mạng nơ-ron và phân loại. Lưu ý rằng một số mô hình không thể
truy cập được do các hạn chế được thực hiện bởi tập đoàn chủ sở hữu.

4 Công cụ Tạo Nhạc Dựa trên Tham số
Tạo Nhạc Dựa trên Tham số là một danh mục cụ thể của các mô hình yêu cầu một số tham số
đầu vào để hoạt động hiệu quả. Các tham số được đề cập có thể dao động từ các thuộc tính
âm nhạc như 'nhịp độ' hoặc 'khóa' đến các đầu vào trừu tượng hơn như 'tâm trạng' hoặc 'nhiệt độ'.
Không giống như các đối tác Không có Tham số, hoạt động tự động mà không cần
đầu vào cụ thể, các mô hình Dựa trên Tham số cung cấp mức độ kiểm soát cao hơn
cho người dùng, cho phép họ điều hướng nhạc được tạo ra theo hướng phù hợp với
sở thích của họ. Điều này được thực hiện bằng cách tùy chỉnh các tham số đầu vào
hướng dẫn quá trình tạo nhạc.

4.1 Magenta

4.1.1 Tổng quan
Magenta [5], một sáng kiến nghiên cứu mã nguồn mở, là một khung mở rộng khám phá
việc kết hợp học máy vào các quá trình sáng tạo nghệ thuật và âm nhạc. Các tác giả
mô tả rằng trọng tâm của họ là khám phá khả năng phát triển các công cụ thông minh
và giao diện tạo điều kiện cho việc tích hợp các mô hình tạo nhạc vào
quá trình sáng tạo của nghệ sĩ và nhạc sĩ. Mục tiêu của họ không phải là thay thế
quá trình sáng tạo mà là nâng cao nó.

Các kho lưu trữ chính của Magenta, bao gồm thư viện Python TensorFlow phổ biến,
có thể được tìm thấy trên GitHub⁹, cung cấp một loạt các mô hình mạng nơ-ron. Chúng bao gồm
nhiều mô hình Sequential (RNN/LSTM [87, 86, 61]) được thiết kế riêng cho việc tạo
chuỗi trống và giai điệu, MusicVAE [76], và GANSynth [27] sử dụng mạng đối nghịch
sinh [35] để tổng hợp âm thanh.

Trong bài báo này, chúng tôi trình bày một kiểm tra về một số mô hình tạo nhạc được tạo
bởi Magenta, cụ thể là Music VAE, NSynth, Melody RNN, và Drums RNN. Các mô hình này
được chọn do ảnh hưởng được công nhận trong lĩnh vực và sự liên quan của chúng
đến các xu hướng đương đại trong việc tạo nhạc. Ước tính về độ phổ biến này được thực hiện
thông qua đánh giá gián tiếp sử dụng các mô hình ngôn ngữ lớn, như ChatGPT, để xác định
tần suất và bối cảnh của các đề cập của chúng trong tài liệu liên quan và
diễn ngôn kỹ thuật số.

4.1.2 Kiến trúc
Magenta chứa một số mô hình mạng nơ-ron cho việc tạo nhạc, có thể được
phân loại thành ba loại mô hình riêng biệt: mô hình tuần tự [87] như mạng nơ-ron
hồi quy (RNN) và mạng Long Short-Term Memory, autoencoder biến phân
(VAE) [76], và bộ tổng hợp nơ-ron (NSynth) [28]. Chúng tôi sẽ giải thích ngắn gọn
về mỗi mô hình trong phần này.

⁹https://github.com/magenta

--- TRANG 10 ---
Các mô hình tạo nhạc dựa trên chuỗi, như Melody RNN [20], Improv RNN
[19], và Polyphony RNN [21], được đào tạo để học phân phối của các mẫu và cấu trúc
âm nhạc trong một tập dữ liệu cho trước. Các mô hình này có khả năng tạo ra nhạc mới
bằng cách dự đoán nốt tiếp theo trong một chuỗi nốt nhạc dựa trên những nốt trước đó.
Phân phối được học về các mẫu và cấu trúc âm nhạc từ dữ liệu đào tạo cho phép các
mô hình tạo ra nhạc trong nhiều phong cách và thể loại khác nhau.

VAE là các mô hình sinh xác suất học phân phối xác suất của tập dữ liệu đầu vào [48].
Music VAE [76], một VAE hồi quy phân cấp để tạo nhạc, và nó có khả năng tạo ra
nhạc mới bằng cách lấy mẫu từ phân phối đã học. Mô hình được đào tạo trên một tập dữ liệu
các tác phẩm âm nhạc khoảng 1,5 triệu tệp MIDI được tác giả thu thập [76], và nó học
một biểu diễn chiều thấp của dữ liệu đầu vào, có thể được sử dụng để tạo ra nhạc mới
với các phong cách và đặc điểm khác nhau. Chất lượng tái tạo cũng được đánh giá trên
Lakh MIDI Dataset [68] công khai.

NSynth [28] là một mô hình tự hồi quy nơ-ron để tạo nhạc [26], sử dụng WaveNet
[97] và một autoencoder để tạo ra nhạc chất lượng cao với các đặc tính âm thanh
phức tạp và đa dạng. Các mô hình tự hồi quy học phân phối xác suất của nốt tiếp theo
trong một chuỗi dựa trên những nốt trước đó. Tuy nhiên, chúng yêu cầu một lượng lớn
tài nguyên tính toán và một tập dữ liệu lớn các ví dụ âm nhạc để đào tạo.

4.1.3 Tính năng và khả năng
Nền tảng cốt lõi của Magenta xoay quanh khái niệm chuỗi nốt nhạc,
là một biểu diễn trừu tượng của một chuỗi nốt với các cao độ, nhạc cụ,
và vận tốc đánh khác nhau, tương tự như MIDI. Như được thể hiện trên trang web chính thức [4],
việc sử dụng chính của Magenta bao gồm việc tạo ra nhạc kiểu MIDI với giai điệu một track.

NoteSequence là một cấu trúc dữ liệu nắm bắt các khía cạnh khác nhau của âm nhạc,
như thông tin về thời gian, cao độ và nhạc cụ. Thư viện NoteSequence là một
thành phần thiết yếu của dự án Magenta, và nó tập trung vào biểu diễn NoteSequence
có thể serializable và cung cấp nhiều tiện ích để làm việc với dữ liệu âm nhạc.
Các chức năng chính của thư viện bao gồm tạo chuỗi nốt từ các định dạng khác nhau
(ví dụ: MIDI, abc [1], MusicXML [74]), thao tác chuỗi nốt (ví dụ: cắt, lượng tử hóa),
trích xuất các thành phần từ chuỗi nốt (ví dụ: giai điệu, track trống, hợp âm),
xuất chuỗi nốt sang các định dạng khác nhau (ví dụ: MIDI hoặc âm thanh) và chuyển đổi
chuỗi nốt sang các biểu diễn hữu ích cho việc đào tạo mô hình (ví dụ: tensor one-hot).

Tóm tắt các chức năng của các mô hình Magenta được trình bày trong Bảng 3.

Trong phần sau, chúng tôi giải thích các mô hình mạng nơ-ron khác nhau của Magenta
để đạt được các tác vụ khác nhau và chức năng tương ứng của chúng.

Nsynth: Mô hình Neural Audio Synthesis (NSynth) sử dụng autoencoder dựa trên WaveNet
để tạo ra âm thanh [28]. Không giống như các bộ tổng hợp truyền thống tạo ra âm thanh từ
các thành phần được thiết kế bằng tay, như bộ dao động và bảng sóng, NSynth sử dụng
mạng nơ-ron sâu để tạo ra âm thanh ở cấp độ của các mẫu cá nhân. Bằng cách học trực tiếp
từ dữ liệu, NSynth cung cấp cho các nhạc sĩ khả năng kiểm soát trực quan về tông màu và
động lực, cho phép họ khám phá các âm thanh mới khó có thể hoặc không thể tạo ra
với một bộ tổng hợp được điều chỉnh bằng tay.

Một trong những tính năng chính của NSynth là khả năng phân tích việc tạo ra âm nhạc
thành các nốt và các đặc tính âm nhạc khác, như tông màu. Nó giả định rằng việc tạo ra

--- TRANG 11 ---
Bảng 3: Danh sách các Mô hình Magenta trong Thư mục Mô hình [22]

Mô hình | Mô tả
---|---
Arbitrary Image Stylization | Một hệ thống học máy để thực hiện chuyển đổi phong cách nghệ thuật nhanh có thể hoạt động trên các phong cách hội họa tùy ý.
Coconet | Đào tạo một CNN để hoàn thành các bản nhạc từng phần.
Drums RNN | Áp dụng mô hình hóa ngôn ngữ vào việc tạo track trống sử dụng LSTM.
GANSynth | Sử dụng mô hình dựa trên GAN để tạo ra âm thanh [28].
Image Stylization | Một "Multistyle Pastiche Generator" tạo ra các biểu diễn nghệ thuật của ảnh. Được mô tả trong "A Learned Representation For Artistic Style".
Improv RNN | Tạo ra giai điệu được điều kiện trên một tiến trình hợp âm cơ bản sử dụng LSTM.
Melody RNN | Áp dụng mô hình hóa ngôn ngữ vào việc tạo giai điệu sử dụng LSTM.
Music Transformer | Tạo ra các buổi biểu diễn âm nhạc, không có điều kiện hoặc có điều kiện trên một bản nhạc.
Music VAE | Một autoencoder biến phân hồi quy phân cấp cho âm nhạc.
NSynth | Neural Audio Synthesis với WaveNet Autoencoders.
Onsets and Frames | Mô hình phiên âm âm nhạc piano tự động với phương pháp mục tiêu kép.
Performance RNN | Áp dụng mô hình hóa ngôn ngữ vào âm nhạc đa âm sử dụng kết hợp các sự kiện bật/tắt nốt, dịch chuyển thời gian và thay đổi vận tốc.
Piano Genie | Học một biểu diễn rời rạc chiều thấp của nhạc piano sử dụng RNN encoder-decoder.
Pianoroll RNN-NADE | Áp dụng mô hình hóa ngôn ngữ vào việc tạo nhạc đa âm sử dụng LSTM kết hợp với NADE.
Polyphony RNN | Áp dụng mô hình hóa ngôn ngữ vào việc tạo nhạc đa âm sử dụng LSTM.
RL Tuner | Nâng cao LSTM được đào tạo để tạo giai điệu đơn âm sử dụng học tăng cường (bổ sung deep Q-learning với phần thưởng cross-entropy).
Score2Perf | Tạo ra các buổi biểu diễn âm nhạc, không có điều kiện hoặc có điều kiện trên một bản nhạc.

--- TRANG 12 ---
âm nhạc có thể được mô hình hóa bằng cách phân tích chất lượng của các nốt và các đặc tính âm nhạc khác.
Để tạo điều kiện cho việc này, các nhà phát triển đã xây dựng tập dữ liệu NSynth¹⁰, là một
bộ sưu tập lớn các nốt nhạc được chú thích lấy mẫu từ các nhạc cụ cá nhân trên một phạm vi
cao độ và vận tốc. Cùng với tập dữ liệu, các nhà phát triển cũng đã phát hành mô hình
autoencoder kiểu WaveNet, học các mã có ý nghĩa biểu diễn không gian âm thanh nhạc cụ.

Melody-rnn: Melody RNN có bốn cấu hình: cơ bản, mono, lookback, và attention.
Cấu hình cơ bản sử dụng mã hóa one-hot để biểu diễn các giai điệu được trích xuất làm
đầu vào cho LSTM. Để đào tạo, tất cả các ví dụ được chuyển vị sang phạm vi cao độ MIDI
[48, 84], và đầu ra cũng trong phạm vi này. Cấu hình mono tương tự như cấu hình cơ bản
nhưng có thể sử dụng toàn bộ 128 cao độ MIDI. Cấu hình lookback giới thiệu các đầu vào
và nhãn tùy chỉnh cho phép mô hình nhận biết các mẫu xảy ra trong một và hai thanh và
giúp mô hình nhận biết các mẫu liên quan đến vị trí của một sự kiện trong biện pháp.
Cuối cùng, cấu hình attention sử dụng cơ chế attention để truy cập thông tin quá khứ
mà không cần lưu trữ thông tin đó trong trạng thái của ô RNN. Điều này cho phép mô hình
học các phụ thuộc dài hạn và tạo ra giai điệu với các chủ đề cong dài hơn.

Để tối ưu hóa hiệu suất của mô hình Melody RNN, một số siêu tham số có thể được điều chỉnh.
Các siêu tham số này bao gồm kích thước batch, xác định số lượng chuỗi được sử dụng trong
mỗi lần lặp đào tạo, cũng như số bước thời gian trong chuỗi được tạo ra, tỷ lệ học,
và số đơn vị ẩn trong mỗi lớp LSTM. Hơn nữa, xác suất giữ dropout được sử dụng để
xác định xác suất mà một đơn vị ẩn cho trước sẽ được giữ lại trong quá trình đào tạo,
và độ dài attention chỉ định độ dài của cơ chế attention. Ngoài ra, số bước đào tạo
có thể được chỉ định để xác định thời điểm dừng vòng lặp đào tạo, trong khi siêu tham số
eval ratio được sử dụng để xác định phần các ví dụ được dành riêng cho đánh giá trong
quá trình đào tạo. Bằng cách điều chỉnh các siêu tham số này, người dùng có thể tùy chỉnh
hiệu suất của mô hình để tạo ra giai điệu phù hợp hơn với nhu cầu của họ.

Music VAE: MusicVAE là một mô hình Magenta khác dựa trên Virtual Auto Encoder (VAE)
[48], cung cấp các chế độ tạo nhạc tương tác khác nhau, bao gồm lấy mẫu ngẫu nhiên từ
phân phối prior, nội suy giữa các chuỗi hiện có và thao tác các chuỗi hiện có thông qua
các vector thuộc tính hoặc mô hình ràng buộc tiềm ẩn. Các biểu diễn giai điệu/bass-line
và trống dựa trên những gì được sử dụng bởi MelodyRNN và DrumsRNN (được mô tả trong
các phần sau).

MusicVAE sử dụng các checkpoint được đào tạo trước cho các cấu hình khác nhau, được sử dụng
để tạo ra đầu ra từ dòng lệnh.

GrooVAE là một biến thể của MusicVAE để tạo ra và kiểm soát các buổi biểu diễn trống
biểu cảm. Nó có thể được đào tạo với Groove MIDI Dataset [34] của các buổi biểu diễn trống.

Drums RNN: Mô hình này tạo ra các track trống. Khi mô hình hóa các track trống, nó được
coi là một chuỗi sự kiện duy nhất bằng cách ánh xạ tất cả các trống MIDI khác nhau lên
một số nhỏ hơn các lớp trống và biểu diễn mỗi sự kiện như một giá trị duy nhất. Giá trị này
trình bày tập hợp các lớp trống được đánh. Không giống như giai điệu, các track trống là
đa âm, và nhiều trống có thể được đánh đồng thời.

Mô hình cung cấp hai thiết lập, một trống và bộ trống, trong đó thiết lập trước mã hóa
tất cả trống thành một lớp duy nhất, trong khi thiết lập sau kết hợp bộ trống chín mảnh,
bao gồm các loại trống khác nhau được mã hóa như vector one-hot độ dài 512, và các bộ đếm
nhị phân bổ sung đầu vào của mô hình. Drums RNN có thể hoạt động với mô hình được đào tạo
trước, nơi người dùng nhập các tham số cần thiết và có thể chỉ định trống primer, hoặc
đào tạo một mô hình mới bằng cách chuyển đổi các tệp MIDI thành NoteSequences, chuyển đổi
thành các ví dụ Sequence để đào tạo và đánh giá.

¹⁰https://magenta.tensorflow.org/datasets/nsynth

--- TRANG 13 ---
4.1.4 Ưu điểm và Hạn chế
Một ưu điểm chính của Magenta là nó là một dự án nghiên cứu mã nguồn mở nhằm
khám phá vai trò của học máy như một công cụ trong quá trình sáng tạo nghệ thuật và âm nhạc.
Nó đã phát triển một loạt các mô hình và thư viện từ năm 2016 cung cấp nhiều
chức năng khác nhau để tạo ra, thao tác và tạo nhạc và nghệ thuật.

Magenta cung cấp một loạt các tài nguyên và thư viện (được liệt kê trong 2), cho phép
tính linh hoạt trong việc nhập và thao tác dữ liệu âm nhạc và nghệ thuật. Ngoài ra, mỗi mô hình
có cách tiếp cận độc đáo riêng và có thể tạo ra các loại nhạc khác nhau. Điều này cung cấp
vô số khả năng và chức năng cho người dùng khám phá và sử dụng¹¹. Ngoài ra, Magenta
hỗ trợ nhiều định dạng, như MIDI và MusicXML, cho phép khả năng tương tác với
phần mềm âm nhạc và nghệ thuật khác.

Tuy nhiên, Magenta cũng có một số hạn chế. Đây là một dự án phức tạp yêu cầu
một mức độ kiến thức kỹ thuật và chuyên môn nhất định để sử dụng và hiểu. Một số mô hình
và thư viện có thể hướng đến các trường hợp sử dụng cụ thể, hạn chế tính linh hoạt tổng thể
của chúng. Các nhạc sĩ có thể gặp khó khăn trong việc học và chọn đúng công cụ do
phạm vi mô hình và thư viện có sẵn rộng lớn. Cần lưu ý rằng đầu ra chính của Magenta
có dạng chuỗi nhạc kiểu MIDI chỉ có thể được sử dụng dễ dàng bởi những người
có kiến thức trước về sản xuất âm nhạc. Magenta không thể luôn tạo ra kết quả
mong muốn, và âm nhạc được tạo ra có thể yêu cầu điều chỉnh và tinh chỉnh thủ công
thêm sau đó.

4.2 JukeBox

4.2.1 Tổng quan
Jukebox [23, 62] là một mô hình dựa trên mạng nơ-ron khác để tạo nhạc. Nó bao gồm
các yếu tố giọng hát trong một loạt các thể loại và phong cách âm nhạc đa dạng. Đầu ra
của mô hình Jukebox được trình bày dưới dạng âm thanh thô. Mô hình này sử dụng
VQ-VAE và Transformers để tạo ra các tác phẩm nguyên bản. Đáng chú ý là Jukebox
có khả năng tạo ra ca hát, đây là một tính năng độc đáo so với các mô hình tạo nhạc khác
vào thời điểm phát hành.

4.2.2 Kiến trúc
Jukebox sử dụng kiến trúc VQ-VAE phân cấp [23] để nén âm nhạc thành các mã rời rạc.
Phương pháp này tương tự như mô hình VQ-VAE-2 [73] được sử dụng để tạo hình ảnh,
nhưng đã được sửa đổi để phù hợp hơn với nhu cầu tạo nhạc. Mô hình sử dụng
khởi động lại ngẫu nhiên để ngăn chặn sự sụp đổ codebook, đây là một vấn đề phổ biến
cho VQ-VAE. Nó tách các decoder để tối đa hóa việc sử dụng các cấp trên cùng bị
thắt cổ chai trong VQ-VAE phân cấp, và một spectral loss để cho phép tái tạo
các tần số cao hơn. Mô hình có ba cấp, mỗi cấp nén âm thanh với một hệ số khác nhau
và giảm chi tiết trong âm thanh. Tuy nhiên, nó vẫn có thể giữ lại thông tin quan trọng
về cao độ, tông màu và âm lượng của âm thanh.

Sau quá trình nén này, một tập hợp các mô hình prior, bao gồm prior cấp cao nhất
và hai prior upsampling, được đào tạo để học phân phối của các mã âm nhạc trong
không gian nén. Các mô hình prior này sử dụng một biến thể đơn giản của Sparse Transformers,
một dạng self-attention, để đào tạo hiệu quả. Đóng vai trò là các biểu diễn xác suất
của cấu trúc cơ bản trong không gian âm nhạc nén, các mô hình prior cho phép Jukebox
tạo ra các mẫu âm nhạc mới bằng cách nắm bắt các mẫu toàn cầu và chi tiết tinh vi hơn
thông qua một phương pháp phân cấp.

Để đào tạo mô hình, một tập dữ liệu 1,2 triệu bài hát đã được thu thập, cùng với
lời bài hát và metadata tương ứng. Ngoài ra, mô hình có thể được điều kiện trên nghệ sĩ
và thể loại, cho phép nó tạo ra nhạc theo phong cách cụ thể. Nghệ sĩ và thể loại có thể
được phân cụm lại với nhau bằng cách sử dụng t-SNE [99], phản ánh sự tương đồng
và tiết lộ các liên kết bất ngờ giữa chúng.

4.2.3 Tính năng và Khả năng
Jukebox có hai loại mô hình; một có lời và một không có lời. Các mô hình thường được
đặt tên với tiền tố cho biết số lượng upsampler tham số, như "5b" hoặc "5b lyrics".
Quy ước đặt tên này làm nổi bật việc nâng cấp từ phiên bản "1b" trước đó, với giá trị
số đại diện cho sự gia tăng upsampler tham số để nâng cao hiệu suất.

Nó có một số siêu tham số, và chúng tôi liệt kê một số tham số quan trọng. Một trong
các siêu tham số là speed upsampling, cho phép upsampling nhanh hơn nhưng có thể
dẫn đến các mẫu hơi "giật cục". Một siêu tham số khác là mode, có thể là "primed"
hoặc "ancestral". Chế độ primed tiếp tục một bài hát hiện có, trong khi chế độ ancestral
tạo ra một bài hát từ đầu. Nếu chế độ primed được chọn, một tệp âm thanh được yêu cầu
để chỉ định bài hát mà Jukebox sẽ tiếp tục.

Jukebox cũng cung cấp khả năng chọn nghệ sĩ và thể loại của âm nhạc được tạo ra.
Các tùy chọn có sẵn cho nghệ sĩ và thể loại có thể được tìm thấy trong kho lưu trữ GitHub
cho¹². Các mô hình 5b lyrics và 5b sử dụng danh sách thể loại v2, trong khi mô hình
1b lyrics sử dụng danh sách thể loại v3. Có thể kết hợp tối đa năm thể loại v2,
nhưng không thể kết hợp các thể loại v3.

Cuối cùng, Jukebox cung cấp một siêu tham số gọi là sampling temperature, xác định
tính sáng tạo và năng lượng của âm nhạc được tạo ra. Nhiệt độ càng cao, kết quả càng
hỗn loạn và dữ dội. Khuyến nghị giữ nhiệt độ giữa .96 và .999, nhưng người dùng có thể
thử nghiệm với các giá trị khác nhau để đạt được kết quả mong muốn.

4.2.4 Ưu điểm và Hạn chế
Mô hình Jukebox có một số ưu điểm khiến nó trở thành một công cụ mạnh mẽ để tạo nhạc.
Một ưu điểm chính là việc sử dụng kỹ thuật VQ-VAE, cho phép nén hiệu quả âm nhạc
thành các mã rời rạc. Điều này cho phép tạo ra các bài hát mới trong khi giữ lại thông tin
quan trọng về cao độ, tông màu và âm lượng.

Mô hình Jukebox cũng có khả năng được điều kiện trên nghệ sĩ và thể loại, cho phép
nó tạo ra nhạc theo phong cách cụ thể. Điều này cho phép kiểm soát nhiều hơn đối với
âm nhạc được tạo ra và có thể hữu ích cho các ứng dụng như tạo nhạc nền tùy chỉnh
hoặc danh sách phát cá nhân hóa.

Một ưu điểm khác của mô hình Jukebox là việc sử dụng tập dữ liệu 1,2 triệu bài hát
để đào tạo, được ghép nối với lời bài hát và metadata tương ứng, cho phép nó học
một loạt các phong cách và mẫu âm nhạc.

Tuy nhiên, cũng có một số hạn chế đối với mô hình Jukebox. Một hạn chế là quá trình
downsampling được sử dụng để nén âm thanh có thể dẫn đến mất chi tiết trong âm nhạc
được tạo ra. Ngoài ra, khả năng tạo nhạc theo phong cách cụ thể của mô hình bị hạn chế
bởi sự đa dạng của tập dữ liệu mà nó được đào tạo. Nếu một phong cách âm nhạc cụ thể
không được đại diện tốt trong tập dữ liệu, mô hình có thể gặp khó khăn trong việc tạo ra
nhạc theo phong cách đó.

Mặc dù Jukebox có chung hạn chế của các mô hình tạo nhạc dựa trên dữ liệu trong việc
tạo ra các biến thể của các mẫu hiện có, thách thức độc đáo của nó là độ phức tạp tính toán
liên quan đến phương pháp phân cấp của nó. Quá trình tạo ra có thể mất một lượng
tài nguyên đáng kể cho các máy thường có thể truy cập. Điều này có thể là một rào cản
cho các nhà phát triển và nhà nghiên cứu không có quyền truy cập vào các cụm GPU của
tập đoàn lớn.

¹²https://github.com/openai/jukebox

--- TRANG 14 ---
4.3 MuseNet

4.3.1 Tổng quan
MuseNet [64] là một mạng nơ-ron sâu được thiết kế với khả năng tạo ra các tác phẩm
âm nhạc dài 4 phút với tối đa 10 nhạc cụ khác nhau và có tiềm năng kết hợp nhiều phong cách
khác nhau.

4.3.2 Kiến trúc
MuseNet [64] sử dụng mô hình transformer [92, 100], tương tự như GPT-2, dự đoán
các token tiếp theo trong một chuỗi, có thể áp dụng cho cả âm thanh và văn bản. Sử dụng
các kernel được tối ưu hóa của Sparse Transformer [11], một mạng 72 lớp với 24 attention head
được xây dựng, tập trung vào một context bao gồm 4096 token.

Cửa sổ context lớn của mô hình có thể góp phần vào khả năng nắm bắt và duy trì
các yếu tố cấu trúc dài hạn trong âm nhạc, rất quan trọng trong việc tạo ra các tác phẩm
âm nhạc nhất quán [64]. Dữ liệu tuần tự được sử dụng để đào tạo, với mục đích dự đoán
(các) nốt tiếp theo dựa trên một tập hợp các nốt [64]. Tập dữ liệu được sử dụng để đào tạo
được lấy từ các bộ sưu tập khác nhau bao gồm ClassicalArchives [54], BitMidi [2],
và tập dữ liệu MAESTRO [37].

Nhiều chiến lược mã hóa khác nhau đã được sử dụng để chuyển đổi các tệp MIDI thành
định dạng thân thiện với mô hình. Một sơ đồ mã hóa kết hợp thông tin cao độ, âm lượng
và nhạc cụ thành một token duy nhất cuối cùng đã được áp dụng, cân bằng tính biểu cảm
và ngắn gọn [64].

Hơn nữa, kiến trúc kết hợp một số embedding cho ngữ cảnh cấu trúc, bao gồm
embedding vị trí, embedding theo dõi thời gian và embedding cụ thể hợp âm [64].
Trong quá trình đào tạo, các kỹ thuật tăng cường dữ liệu như chuyển vị, tăng cường
âm lượng và thời gian, và mixup trong không gian embedding token đã được sử dụng [106].
Ngoài ra, một cơ chế "inner critic" được sử dụng, đào tạo mô hình để phân biệt giữa
các mẫu tập dữ liệu gốc và các thế hệ quá khứ của chính nó [64].

4.3.3 Tính năng và Khả năng
MuseNet cung cấp cho người dùng cơ hội khám phá và tương tác với khả năng của nó
thông qua hai chế độ: "đơn giản" và "nâng cao". Chế độ đơn giản cung cấp cho người dùng
một tập hợp các mẫu không được tuyển chọn được tạo ra trước, cho phép họ thử nghiệm
với các nhà soạn nhạc, phong cách và điểm khởi đầu khác nhau. Tính năng này cho phép
người dùng khám phá phạm vi đa dạng của các phong cách âm nhạc mà mô hình có thể tạo ra.
Mặt khác, chế độ nâng cao cung cấp trải nghiệm tương tác hơn nơi người dùng có thể
tương tác trực tiếp với mô hình để tạo ra các tác phẩm hoàn toàn mới. Mặc dù chế độ này
có thể yêu cầu thời gian hoàn thành lâu hơn, nó cung cấp khả năng tạo ra một tác phẩm
độc đáo và nguyên bản. Bằng cách cung cấp hai chế độ này, MuseNet cho phép người dùng
thử nghiệm với khả năng của nó và làm cho nó có thể tiếp cận với cả người dùng mới và
có kinh nghiệm.

Để tạo ra các thế hệ được kiểm soát nhiều hơn, các token nhà soạn nhạc và nhạc cụ
đã được giới thiệu trong giai đoạn đào tạo. Các token này cung cấp các tín hiệu ngữ cảnh
cho mô hình về phong cách và nhạc cụ dự kiến của âm nhạc, cho phép người dùng hướng dẫn
đầu ra của mô hình theo hướng mong muốn [64].

4.3.4 Ưu điểm và Hạn chế
MuseNet thể hiện khả năng đáng chú ý trong việc giữ lại cấu trúc dài hạn của một
tác phẩm âm nhạc, do đó mô phỏng hiệu quả phong cách hoặc nhạc sĩ với độ chính xác cao.
Sự thành thạo này được đạt được thông qua kiến trúc của nó, đã được thiết kế tỉ mỉ để
tạo ra các tác phẩm âm nhạc nhất quán và có tính thẩm mỹ khi các tham số phù hợp
với sơ đồ đào tạo và mã hóa của nó được sử dụng. Các tham số này bao gồm các gợi ý
thích hợp như token nhà soạn nhạc và nhạc cụ và mã hóa hợp nhất các đặc điểm âm nhạc
như thông tin cao độ, âm lượng và nhạc cụ thành một token duy nhất. Sự kết hợp chính xác
của các thành phần này trao cho mô hình khả năng tạo ra các chuỗi âm nhạc biểu cảm
nhưng ngắn gọn, mô phỏng hiệu quả các tác phẩm tự nhiên.

Tuy nhiên, có những hạn chế cần được xem xét khi sử dụng MuseNet. Mặc dù người dùng
có thể đề xuất các nhạc cụ cụ thể, MuseNet tạo ra mỗi nốt bằng cách tính toán xác suất
trên tất cả các nốt và nhạc cụ có thể. Do đó, mô hình có thể chọn các nhạc cụ khác
với những gì người dùng đề xuất. Hơn nữa, MuseNet cần trợ giúp với các cặp phong cách
và nhạc cụ kỳ lạ, như Chopin với bass và trống. Do đó, các thế hệ sẽ tự nhiên hơn
nếu các nhạc cụ gần nhất với phong cách thường gặp của nhà soạn nhạc hoặc ban nhạc
được chọn. Cuối cùng, MuseNet không đảm bảo rằng âm nhạc được tạo ra không có
các tuyên bố bản quyền bên ngoài.

4.4 Music Transformer

4.4.1 Tổng quan
Music Transformer là một mô hình mạng nơ-ron khác được thiết kế để tạo ra các chuỗi
âm nhạc dài với cấu trúc dài hạn [41]. Nó sử dụng kiến trúc transformer và có khả năng
mở rộng quy mô đến các chuỗi âm nhạc theo thứ tự phút.

4.4.2 Kiến trúc
Mô hình Music Transformer sử dụng các token rời rạc để biểu diễn âm nhạc, với từ vựng
cụ thể được xác định bởi tập dữ liệu đào tạo. Decoder Transformer là một mô hình sinh,
dựa vào các cơ chế self-attention với thông tin vị trí được học hoặc sinusoidal.

--- TRANG 15 ---
Mỗi lớp bao gồm một lớp con self-attention theo sau là một lớp con feedforward.
Lớp attention sử dụng cơ chế attention tích dot có tỷ lệ, và lớp con feedforward
thực hiện hai lớp dày đặc theo điểm. Mô hình sử dụng các biểu diễn vị trí tương đối
để cho phép attention được thông báo bởi khoảng cách giữa các vị trí trong một chuỗi.
Music Transformer giảm yêu cầu bộ nhớ trung gian của attention gần bằng một
thủ tục "skewing" [41]. Ngoài ra, nó sử dụng attention cục bộ tương đối cho các chuỗi
rất dài bằng cách chia chuỗi đầu vào thành các khối không chồng lắp.

4.4.3 Tính năng và Khả năng
Music Transformer có sẵn trong cả phiên bản TensorFlow¹³ và PyTorch¹⁴ với các API
dễ sử dụng. Nó có thể tạo ra âm nhạc với cấu trúc dài hạn và mở rộng quy mô đến
các chuỗi âm nhạc theo thứ tự phút. Ngoài ra, nó có dấu chân bộ nhớ giảm,
cho phép nó tạo ra các chuỗi âm nhạc dài hơn. Mô hình được đào tạo trên dữ liệu tuần tự
và có thể tạo ra các tác phẩm tự nhiên khi các siêu tham số chính xác được chọn.

4.4.4 Ưu điểm và hạn chế
Music Transformer có ưu điểm tạo ra các chuỗi âm nhạc dài với cấu trúc dài hạn
và đã được sửa đổi để giảm yêu cầu bộ nhớ. Tuy nhiên, giai điệu được tạo ra bởi
công cụ có thể đơn điệu nếu underfitting hoặc dữ liệu đào tạo không đủ tồn tại.
Điều này có thể hạn chế tính hữu ích của công cụ cho các tác vụ tạo nhạc phức tạp
và có thể yêu cầu tối ưu hóa bổ sung hoặc đào tạo với các tập dữ liệu lớn hơn và
đa dạng hơn để cải thiện chất lượng của âm nhạc được tạo ra.

4.5 SDMuse

4.5.1 Tổng quan
SDMuse là một khung chỉnh sửa và tạo nhạc vi phân ngẫu nhiên thống nhất có thể
tạo ra và sửa đổi các tác phẩm âm nhạc hiện có ở độ chi tiết tinh vi được đề xuất
vào năm 2022 [104, 105].

4.5.2 Kiến trúc
Khung này tuân theo một quy trình hai giai đoạn với biểu diễn kết hợp của pianoroll
và MIDI-event. Trong giai đoạn đầu, SDMuse tạo ra và chỉnh sửa pianoroll thông qua
một quá trình khử nhiễu lặp đi lặp lại dựa trên một phương trình vi phân ngẫu nhiên
(SDE) sử dụng một prior mô hình sinh khuếch tán [93]. Trong giai đoạn thứ hai,
khung này tinh chỉnh pianoroll được tạo ra và dự đoán các token MIDI-event
một cách tự hồi quy.

4.5.3 Tính năng và Khả năng
SDMuse có thể sáng tác một tác phẩm âm nhạc hoàn chỉnh từ đầu và sửa đổi các
tác phẩm âm nhạc hiện có theo nhiều cách khác nhau, như kết hợp, tiếp tục, inpainting,
và chuyển đổi phong cách. Khung này cũng có thể trích xuất các tín hiệu điều khiển
tinh vi từ chính tác phẩm âm nhạc mà không yêu cầu chú thích dữ liệu bổ sung,
làm cho nó hiệu quả hơn.

Khung được đề xuất tận dụng hai biểu diễn âm nhạc phổ biến nhất, pianoroll
và MIDI-event. Các phương pháp dựa trên Pianoroll sử dụng pianoroll để biểu diễn
bản nhạc, trong khi các phương pháp dựa trên MIDI-event chuyển đổi các tác phẩm
âm nhạc thành một chuỗi token MIDI-event. Biểu diễn kết hợp được sử dụng trong
SDMuse phù hợp hơn để trích xuất và kiểm soát thông tin cảm nhận như cấu trúc
trong khi tạo ra và mô hình hóa các chi tiết biểu diễn âm nhạc chính xác, như vận tốc
và vị trí khởi đầu tinh vi.

SDMuse liên quan đến một số tín hiệu điều khiển tinh vi, bao gồm mật độ nốt,
phân phối cao độ và chuỗi tiến trình hợp âm trong quá trình đào tạo của mô hình
khuếch tán để cho phép tạo/chỉnh sửa âm nhạc không điều kiện và có điều kiện
cùng một lúc. Các tín hiệu điều khiển này có thể được trích xuất từ chính tác phẩm
âm nhạc, làm cho nó hiệu quả hơn.

4.5.4 Ưu điểm và Hạn chế
Mặc dù chúng tôi không thể thử nghiệm với SDMuse một cách thực tế do thiếu
phiên bản demo có sẵn để sử dụng, khả năng của mô hình trong việc tạo ra và
chỉnh sửa âm nhạc đa track đã được chứng minh trong bài báo học thuật. Cụ thể,
mô hình có thể thực hiện chỉnh sửa tinh vi bằng cách sử dụng tạo ra và chỉnh sửa
dựa trên nét vẽ, inpainting, outpainting, kết hợp và chuyển đổi phong cách. Hơn nữa,
hiệu quả của SDMuse trong việc tạo ra và chỉnh sửa âm nhạc đã được đánh giá trong
các thí nghiệm khác nhau sử dụng tập dữ liệu âm nhạc pop ailabs1k7 [40]. Mặc dù
thiếu thử nghiệm trực tiếp, các kết quả được báo cáo và phương pháp học cung cấp
hiểu biết về tiềm năng của SDMuse như một công cụ tạo ra và chỉnh sửa âm nhạc.

Nhìn chung, hiệu quả của SDMuse trong việc tạo ra và chỉnh sửa các tác phẩm âm nhạc
đã được chứng minh thông qua các thí nghiệm về các tác vụ chỉnh sửa và tạo nhạc
khác nhau sử dụng tập dữ liệu âm nhạc pop ailabs1k7.

4.6 Tạo Nhạc với Cảm xúc dựa trên mLSTM

4.6.1 Tổng quan
Music Generation with Sentiment [30] là một mô hình có thể sáng tác âm nhạc với
một cảm xúc được chỉ định. Nó dựa trên mLSTM, ban đầu được thiết kế để tạo ra
các đánh giá sản phẩm Amazon dựa trên cảm xúc tích cực hoặc tiêu cực [67]. Trong
mô hình này, một tác phẩm âm nhạc được biểu diễn như một chuỗi các từ và dấu câu
từ một từ vựng biểu diễn các sự kiện được trích xuất từ tệp MIDI. Mô hình cũng có thể
được sử dụng để phân tích cảm xúc của âm nhạc ký hiệu, cho phép người dùng phân loại
âm nhạc dựa trên nội dung cảm xúc.

4.6.2 Kiến trúc
Mô hình được đào tạo trên một tập dữ liệu gọi là VGMIDI, bao gồm 823 tác phẩm được
trích xuất từ nhạc nền trò chơi điện tử ở định dạng MIDI. Các tác phẩm là những
bản sắp xếp piano của nhạc nền và có độ dài từ 26 giây đến 3 phút. Trong số
các tác phẩm này, 95 được chú thích theo một mô hình 2 chiều biểu diễn cảm xúc
sử dụng một cặp valence-arousal. Valence cho biết cảm xúc tích cực so với tiêu cực,
và arousal cho biết cường độ cảm xúc [81]. Mô hình valence-arousal cũng là một trong
những mô hình chiều phổ biến nhất được sử dụng để gắn nhãn cảm xúc trong âm nhạc.

Nó sử dụng mLSTM và hồi quy logistic để sáng tác âm nhạc với một cảm xúc.
Cảm xúc trong âm nhạc được xác định bởi các đặc tính khác nhau như giai điệu, hòa âm,
nhịp độ và tông màu. Ngoài ra, cùng dữ liệu được gắn nhãn có thể được sử dụng để
khám phá việc sáng tác âm nhạc thuật toán có tình cảm trong cả các bài toán phân loại
(đa lớp và/hoặc nhị phân) và hồi quy.

¹³https://github.com/jason9693/MusicTransformer-tensorflow2.0
¹⁴https://github.com/jason9693/musictransformer-pytorch

--- TRANG 16 ---
4.6.3 Tính năng và Khả năng
Mô hình tạo ra âm nhạc phù hợp với một tâm trạng hoặc cảm xúc cụ thể, cung cấp
cho người dùng một công cụ tiện lợi để tạo ra âm nhạc với cảm xúc mong muốn. Hơn nữa,
mô hình có thể được sử dụng để phân tích cảm xúc của âm nhạc ký hiệu, tạo điều kiện
cho người dùng kiểm tra và phân loại âm nhạc dựa trên nội dung cảm xúc.

4.6.4 Ưu điểm và Hạn chế
Theo kết quả trong bài báo, mô hình mLSTM sinh kết hợp với hồi quy logistic đã
chứng minh độ chính xác phân loại đáng kể (89,83%); mô hình đạt được độ chính xác
84% để tạo ra các tác phẩm âm nhạc tích cực và 67% độ chính xác cho các tác phẩm
tiêu cực. Tuy nhiên, các tác phẩm tiêu cực được tìm thấy là mơ hồ hơn, cho thấy
có chỗ để cải thiện thêm. Các nhà phát triển thừa nhận điều này là một hạn chế
của mô hình và có kế hoạch cải thiện khả năng tạo ra các tác phẩm tiêu cực ít mơ hồ hơn
trong công việc tương lai. Ngoài ra, các tác giả đề xuất rằng công việc tương lai của họ
mở rộng khả năng của mô hình để tạo ra âm nhạc với các cảm xúc cụ thể (ví dụ: vui,
buồn, hồi hộp) và các cặp valence-arousal (số thực), và cũng sáng tác nhạc nền
thời gian thực cho các trải nghiệm kể chuyện miệng.

5 Công cụ Tạo Nhạc Dựa trên Gợi ý
Tạo nhạc dựa trên gợi ý chuyển đổi đầu vào văn bản thành âm nhạc. Các công cụ này
sử dụng học máy và các thuật toán xử lý ngôn ngữ để giải thích các gợi ý văn bản,
biến đổi ngữ nghĩa và cảm xúc được nhúng thành các tác phẩm âm nhạc hài hòa.

5.1 Riffusion

5.1.1 Tổng quan
Riffusion [31] là một mô hình khuếch tán được tinh chỉnh tạo ra các clip âm thanh
từ các gợi ý dựa trên văn bản và hình ảnh của các spectrogram. Kiến trúc mô hình
dựa trên mô hình Stable Diffusion, một mô hình AI mã nguồn mở tạo ra hình ảnh
từ văn bản.

5.1.2 Kiến trúc
Kiến trúc của Riffusion chủ yếu được xây dựng trên mô hình Stable Diffusion v1.5¹⁵,
một kiến trúc học sâu xuất sắc trong các tác vụ sinh. Về bản chất, mô hình này sử dụng
một biến thể của denoising autoencoder kết hợp với một quá trình khuếch tán để
mô hình hóa phân phối dữ liệu. Kiến trúc này đã được tinh chỉnh cho tác vụ độc đáo
của việc tạo ra âm thanh từ hình ảnh spectrogram.

Xương sống của đầu vào mô hình là các spectrogram biểu diễn âm thanh dưới dạng
hình ảnh. Các spectrogram này được xử lý bởi một kiến trúc mạng nơ-ron được thiết kế
để hiểu và tạo ra các hình ảnh phức tạp. Với bản chất của spectrogram, với các
cấu trúc phức tạp của chúng biểu diễn các tần số âm thanh khác nhau theo thời gian,
một mạng có thể xử lý độ phức tạp dữ liệu như vậy là rất quan trọng. Do đó, kiến trúc
của Stable Diffusion, đã được chứng minh là tạo ra các cấu trúc hình ảnh phức tạp,
trở thành một lựa chọn phù hợp.

Kiến trúc mô hình được trang bị khả năng chuyển đổi Image-to-Image, một tính năng
quan trọng cho phép mô hình điều kiện đầu ra của nó không chỉ dựa trên gợi ý văn bản
mà còn trên các hình ảnh khác. Tính năng này có hiệu quả cho phép kiến trúc thích ứng
với các sắc thái cấu trúc của các hình ảnh đầu vào khác nhau, thúc đẩy khả năng tạo ra
nhiều đầu ra âm thanh khác nhau. Ngoài ra, kiến trúc cũng bao gồm các cơ chế Looping
và Interpolation, yêu cầu mô hình điều hướng mượt mà qua không gian tiềm ẩn,
do đó bảo tồn sự nhất quán trong đầu ra âm thanh.

5.1.3 Tính năng và Khả năng
Riffusion cung cấp giao diện người dùng dựa trên web cho phép người dùng chọn
một gợi ý hoặc nhập văn bản tùy chỉnh chứa các từ khóa, như "jazz" hoặc "guitar,"
và tạo ra nhịp dựa trên các từ khóa này. Nền tảng có khả năng tạo ra vô số
biến thể gợi ý dựa trên các seed khác nhau. Ngoài ra, Riffusion có thể tạo ra
cả nhạc ngắn và jam có thể lặp lại, và các checkpoint mô hình được cung cấp
để thuận tiện cho người dùng.

Giao diện người dùng dựa trên web của Riffusion¹⁶ cung cấp một cách tiếp cận
có thể tiếp cận và thân thiện với người dùng để tạo ra nhịp. Người dùng có thể
điều chỉnh các tham số như hình ảnh seed và mức độ khử nhiễu để thay đổi âm nhạc
họ tạo ra và lưu âm nhạc như các riff ngắn.

5.1.4 Ưu điểm và Hạn chế
Riffusion, như một công cụ tạo nhạc dựa trên mô hình khuếch tán, có một số ưu điểm.
Thứ nhất, nó có giao diện thân thiện với người dùng cho phép người dùng dễ dàng
tạo ra âm nhạc từ đầu vào văn bản mà không cần mã hóa hoặc thủ tục cài đặt phức tạp.
Ngoài ra, Riffusion tạo ra âm nhạc chất lượng cao với nhiễu tối thiểu, làm cho nó
trở thành một lựa chọn tuyệt vời cho những người tìm kiếm đầu ra âm nhạc chất lượng cao.

Tuy nhiên, kiến trúc vốn có của nền tảng Riffusion dẫn đến những hạn chế nhất định
trong khả năng kiểm soát của người dùng đối với đầu ra âm nhạc. Cụ thể, âm nhạc được
tạo ra chủ yếu dựa trên đầu vào văn bản được cung cấp và hình ảnh seed, hướng dẫn
mô hình khuếch tán ổn định trong việc tạo ra đầu ra âm thanh. Sự phụ thuộc này vào
các gợi ý văn bản và hình ảnh seed có nghĩa là khả năng kiểm soát chi tiết của người dùng
đối với đầu ra âm nhạc phụ thuộc vào chất lượng và sự liên quan của các đầu vào này
đối với đầu ra mong muốn. Hơn nữa, số lượng hình ảnh seed có sẵn để tạo ra âm nhạc
bị hạn chế, có thể dẫn đến thiếu đa dạng trong âm nhạc được tạo ra. Một lĩnh vực khác
để cải thiện của Riffusion là không thể tải xuống các tác phẩm dài hơn, với người dùng
chỉ có thể tải xuống các track ngắn. Ngoài ra, tính năng lặp lại của các jam ngắn
có thể dẫn đến tính lặp lại.

Mặc dù sự thiếu linh hoạt của Riffusion có thể không phù hợp với người dùng yêu cầu
mức độ tùy chỉnh cao, điều quan trọng cần lưu ý là lựa chọn thiết kế này được thực hiện
để tạo ra một công cụ tạo nhạc có thể tiếp cận và đơn giản có thể nhanh chóng tạo ra
âm nhạc dựa trên đầu vào của người dùng, mà không yêu cầu kiến thức âm nhạc rộng lớn
hoặc kỹ năng kỹ thuật.

¹⁵https://huggingface.co/runwayml/stable-diffusion-v1-5
¹⁶https://www.riffusion.com/

--- TRANG 17 ---
5.2 Noise2Music

5.2.1 Tổng quan
Noise2Music khám phá việc sử dụng các mô hình khuếch tán trong việc tạo ra âm thanh
âm nhạc chất lượng cao từ các gợi ý văn bản [44]. Tương tự như MusicLM, mô hình
cũng dựa trên MuLan [43], là một mô hình embedding kết hợp nối âm nhạc với
các mô tả âm nhạc ngôn ngữ tự nhiên không bị ràng buộc.

5.2.2 Kiến trúc
Kiến trúc Khuếch tán End-to-End sử dụng một loạt các mô hình khuếch tán liên tiếp
để tạo ra clip âm nhạc cuối cùng. Quá trình này bao gồm việc sử dụng pseudo-labeling
để tạo ra dữ liệu đào tạo, nơi một tập lớn âm thanh âm nhạc không được gắn nhãn
được sử dụng để đào tạo hai mô hình sâu. Một mô hình ngôn ngữ lớn được sử dụng
để tạo ra một bộ sưu tập lớn các câu mô tả âm nhạc chung làm ứng cử viên caption,
trong khi một mô hình embedding kết hợp âm nhạc-văn bản được đào tạo trước
được sử dụng để gán các caption cho mỗi clip âm nhạc thông qua phân loại zero-shot
[63]. Phương pháp này tạo ra tập đào tạo bằng pseudo-labeling, cho phép mô hình
xử lý ngữ nghĩa phức tạp, tinh vi vượt ra ngoài việc điều kiện nhãn âm nhạc đơn giản.

Các mô hình Noise2Music đã được chứng minh có khả năng sinh mạnh mẽ, có thể
tạo ra các tác phẩm âm nhạc vượt ra ngoài việc điều kiện nhãn âm nhạc cơ bản,
sử dụng nội dung ngữ nghĩa của các caption được cung cấp để tạo ra âm nhạc
phức tạp và chi tiết.

5.2.3 Tính năng và Khả năng
Noise2Music là một mô hình tạo nhạc thể hiện khả năng sinh dựa trên các gợi ý
văn bản có nội dung ngữ nghĩa phong phú. Nó có thể tạo ra âm nhạc dựa trên một
loạt các thuộc tính âm nhạc chính, bao gồm thể loại, nhạc cụ, nhịp độ, tâm trạng,
đặc điểm giọng hát và thời đại. Ngoài ra, nó có thể tạo ra âm nhạc dựa trên
các gợi ý sáng tạo.

5.2.4 Ưu điểm và Hạn chế
Noise2Music có ưu điểm trong việc cung cấp âm nhạc chất lượng cao từ các gợi ý
văn bản phong phú, mang lại sự sáng tạo cho các nghệ sĩ và nhà tạo nội dung.
Tuy nhiên, có những hạn chế đối với mô hình, như các thiên lệch tiềm ẩn được học
từ các tập đào tạo, có thể biểu hiện theo những cách tinh vi và không thể dự đoán được.
Hơn nữa, việc sử dụng sai mục đích cũng là một rủi ro khi nội dung được tạo ra
khớp với các ví dụ của dữ liệu đào tạo. Do đó, các thực hành phát triển mô hình
có trách nhiệm, kiểm tra trùng lặp và nỗ lực xác định và giải quyết các vấn đề
an toàn tiềm ẩn là cần thiết để cải thiện các mô hình sinh này. Mô hình không được
phát hành do các hạn chế và rủi ro.

5.3 Moˆ usai

5.3.1 Tổng quan
Moˆ usai [82] là một công cụ tạo nhạc từ văn bản dựa trên mô hình khuếch tán
tiềm ẩn [79].

5.3.2 Kiến trúc
Moˆ usai sử dụng phương pháp khuếch tán cascading hai giai đoạn để tạo ra âm nhạc
stereo chất lượng cao từ các mô tả văn bản. Hệ thống tạo ra âm nhạc ở 48kHz,
kéo dài nhiều phút.

Giai đoạn đầu tiên sử dụng Diffusion Magnitude Autoencoder (DMAE) [66] để nén
dạng sóng âm thanh với hệ số 64. Việc giảm kích thước dữ liệu đáng kể này không
làm tổn hại đến độ trung thực cao của việc tái tạo âm thanh. DMAE điều kiện
quá trình khuếch tán trên một vector tiềm ẩn nén của đầu vào, do đó tạo ra
một decoder sinh mạnh mẽ.

Giai đoạn thứ hai của Moˆ usai tạo ra một không gian tiềm ẩn mới sử dụng
mô hình khuếch tán, được điều kiện trên embedding văn bản. Các embedding này
được tạo ra từ một mô tả cho trước sử dụng một mô hình ngôn ngữ transformer
đông lạnh. Hai giai đoạn của Moˆ usai sử dụng kiến trúc U-Net một chiều hiệu quả
với các cấu hình khác nhau. Giai đoạn khuếch tán tiềm ẩn văn bản-thành-âm thanh
áp dụng khuếch tán lên không gian nén được tạo ra trong giai đoạn đầu tiên
và sử dụng các khối cross-attention để cung cấp embedding văn bản điều kiện.

Moˆ usai được đào tạo trên một tập dữ liệu đa dạng gồm 2.500 giờ âm nhạc stereo,
được lấy mẫu ở 48kHz. Metadata như tiêu đề, tác giả, album, thể loại và năm phát hành
được sử dụng làm mô tả văn bản. Để tăng cường tính mạnh mẽ của việc điều kiện,
mỗi yếu tố của metadata được bỏ qua với xác suất 0,1. Việc đào tạo có thể được
hoàn thành trong vòng một tuần sử dụng GPU A100 với kích thước batch 32.
Trong quá trình suy luận, một nguồn âm thanh mới khoảng 88 giây có thể được
tổng hợp trong ít hơn 88 giây sử dụng GPU tiêu dùng.

5.3.3 Tính năng và Khả năng
Tính năng nổi bật của Moˆ usai là khả năng tạo ra nhiều phút âm nhạc stereo
chất lượng cao ở độ phân giải 48kHz hoàn toàn từ các mô tả văn bản. Khả năng này
phục vụ để thu hẹp khoảng cách giữa ý tưởng văn bản và việc tạo ra âm nhạc,
cung cấp một nền tảng phong phú để chuyển đổi các khái niệm tưởng tượng thành
âm nhạc hấp dẫn về mặt thính giác.

Đầu vào của mô hình có thể bao gồm không chỉ văn bản mô tả mà còn các đặc điểm
âm nhạc như chiều thời gian, cấu trúc dài hạn, các lớp âm thanh chồng lắp
và các sắc thái tinh vi. Cách tiếp cận toàn diện này đối với việc phân tích đầu vào
cho phép hiểu biết toàn diện về ý định âm nhạc, dẫn đến đầu ra âm nhạc độ trung thực cao.

Việc đào tạo trên một tập dữ liệu rộng lớn và đa dạng gồm 2.500 giờ mẫu âm nhạc stereo
trang bị cho Moˆ usai một 'từ vựng' rộng về các yếu tố âm nhạc. Điều này, cùng với
khả năng tạo ra một loạt âm nhạc đa dạng, nâng cao tính linh hoạt và khả năng
thích ứng của mô hình.

Các mẫu âm nhạc mã nguồn mở được tạo ra bằng Moˆ usai được cung cấp như một
minh chứng cho khả năng của nó¹⁷.

5.3.4 Ưu điểm và Hạn chế
Ưu điểm chính của Moˆ usai nằm ở khả năng tạo ra âm nhạc stereo chất lượng cao
dài từ các mô tả văn bản. Khả năng chuyển đổi ngữ nghĩa ngôn ngữ thành ngôn ngữ
âm nhạc này mở ra những khả năng to lớn cho sự sáng tạo và đổi mới âm nhạc.

Tuy nhiên, độ phức tạp của mô hình và nhu cầu về tài nguyên tính toán đáng kể
có thể tạo ra thách thức cho người dùng với sức mạnh xử lý hạn chế. Hơn nữa,
khả năng diễn giải của Moˆ usai có thể là một trở ngại, với độ phức tạp và
bản chất không trực quan của các quá trình khuếch tán nó sử dụng để tạo ra âm nhạc.

Khả năng của mô hình trong việc tạo ra các thể loại âm nhạc cụ thể cũng có thể
bị hạn chế do phân phối của dữ liệu đào tạo. Mặc dù nó đã được tiếp xúc với
một loạt các thể loại như electronic, pop, metal và hip hop, nó có ít kinh nghiệm
với các thể loại khác như blues và classical. Do đó, chất lượng của âm nhạc được
tạo ra trong những thể loại ít được tiếp xúc này có thể không mạnh mẽ như vậy.

¹⁷URL: https://anonymous0.notion.site/anonymous0/Mo-sai-Text-to-Audio-with-Long-Context-Latent-Diffusion-b43dbc71caf94b5898f9e8de714ab5dc

--- TRANG 18 ---
5.4 MusicLM

5.4.1 Tổng quan
MusicLM là một mô hình tạo nhạc được phát triển gần đây [3] có khả năng tạo ra
âm nhạc độ trung thực cao từ các mô tả văn bản phong phú. Mô hình đã thu hút
sự chú ý do khả năng tạo ra âm nhạc có thể kéo dài hàng phút và hiệu suất
chất lượng âm thanh cao. Đáng tiếc, mô hình MusciLM chưa được chính thức phát hành
như một dự án mã nguồn mở, nhóm đã cung cấp các demo âm thanh và tập dữ liệu
MusicCaps chứa 5,5k cặp âm nhạc-văn bản. Tuy nhiên, một số triển khai mã nguồn mở
của kiến trúc MusicLM đã được đề xuất như: [8].

5.4.2 Kiến trúc
Mô hình MusicLM của Agostinelli và cộng sự [3] được cấu thành từ ba mô hình
được đào tạo trước, SoundStream [103], w2vBERT [13], và MuLan [42]. Các mô hình
này được sử dụng để trích xuất các biểu diễn âm thanh để sử dụng trong việc tạo
nhạc có điều kiện văn bản. Cụ thể, SoundStream được sử dụng để trích xuất
các biểu diễn âm thanh tự giám sát từ dữ liệu âm thanh đơn âm. Các token
âm thanh này được sử dụng để tổng hợp độ trung thực cao. w2vBERT được sử dụng
để trích xuất các token ngữ nghĩa từ dữ liệu âm thanh, tạo điều kiện cho việc
tạo ra nhất quán dài hạn. MuLan được sử dụng để biểu diễn việc điều kiện cho
việc tạo nhạc. Trong quá trình đào tạo, MusicLM sử dụng embedding âm nhạc MuLan,
trong khi tại thời điểm suy luận, embedding văn bản MuLan được sử dụng.

Các biểu diễn âm thanh rời rạc từ SoundStream và các biểu diễn văn bản rời rạc
từ MuLan được sử dụng trong một tác vụ mô hình hóa chuỗi-thành-chuỗi phân cấp
để tạo nhạc có điều kiện văn bản. Giai đoạn đầu tiên học ánh xạ từ các token
âm thanh MuLan đến các token ngữ nghĩa. Các token ngữ nghĩa này, được dẫn xuất
từ các mô hình dữ liệu âm thanh được đào tạo trước, tạo điều kiện cho việc
biểu diễn và mô hình hóa các cấu trúc âm nhạc rộng lớn. Giai đoạn thứ hai
dự đoán các token âm thanh được điều kiện trên cả token âm thanh MuLan và
token ngữ nghĩa. Decoder-only Transformers được sử dụng để mô hình hóa cả
hai giai đoạn, và giai đoạn mô hình hóa âm thanh được chia thành các giai đoạn
mô hình hóa thô và tinh để tránh các chuỗi token dài.

Trong quá trình đào tạo, SoundStream và w2vBERT được đào tạo trên tập dữ liệu
Free Music Archive (FMA)¹⁸. Ngược lại, các tokenizer và mô hình tự hồi quy
cho các giai đoạn mô hình hóa ngữ nghĩa và âm thanh được đào tạo trên một
tập dữ liệu lớn chứa năm triệu clip âm thanh. Các giai đoạn được đào tạo
với nhiều lần đi qua dữ liệu đào tạo, sử dụng các crop ngẫu nhiên 30 và 10 giây
của âm thanh đích cho các giai đoạn ngữ nghĩa và âm thanh tương ứng. Trong
quá trình suy luận, embedding văn bản MuLan được sử dụng như tín hiệu điều kiện,
và lấy mẫu nhiệt độ được sử dụng cho việc lấy mẫu tự hồi quy trong tất cả
các giai đoạn. Các giá trị nhiệt độ được chọn dựa trên kiểm tra chủ quan để
cung cấp một sự cân bằng tốt giữa sự đa dạng và tính nhất quán thời gian
của âm nhạc được tạo ra.

5.4.3 Tính năng và Khả năng
Trang web MusicLM giới thiệu khả năng của một mô hình tạo nhạc được hỗ trợ bởi AI
để tạo ra âm thanh dựa trên các mô tả văn bản ngắn gọn, bao gồm thông tin về
thể loại âm nhạc, nhạc cụ, nhịp độ và cảm xúc. Mô hình đã chứng minh tiềm năng
của nó trong việc tạo ra cả các tác phẩm âm nhạc ngắn (30 giây) và dài (5 phút).
Ngoài ra, mô hình cung cấp tính năng chế độ câu chuyện, nơi nó tạo ra âm nhạc
dựa trên một chuỗi gợi ý văn bản truyền tải những thay đổi trong tâm trạng
hoặc cốt truyện. Tính năng này chứng minh khả năng của mô hình trong việc chuyển đổi
một cách liền mạch giữa các mức độ tâm trạng khác nhau trong âm nhạc được tạo ra.

Hơn nữa, mô hình thể hiện tính linh hoạt trong việc điều kiện, cho phép tạo ra
âm nhạc từ một giai điệu được cung cấp phù hợp với mô tả văn bản được cung cấp,
cũng như từ các bức tranh với caption đi kèm. Mô hình cũng cung cấp khả năng
tạo ra âm nhạc cho các cấp độ kinh nghiệm nhạc sĩ khác nhau.

5.4.4 Ưu điểm và Hạn chế
Mô hình MusicLM đã thu hút sự chú ý đáng kể do khả năng tạo ra âm nhạc có thể
kéo dài hàng phút và hiệu suất chất lượng âm thanh cao. Một trong những ưu điểm
chính của MusicLM là nó có thể tạo ra âm nhạc phức tạp có cấu trúc và sự nhất quán
tương tự như âm nhạc được tạo ra bởi các nhà soạn nhạc. Điều này có nghĩa là
mô hình có tiềm năng được sử dụng cho nhiều ứng dụng khác nhau, từ việc tạo ra
nhạc nền cho video và trò chơi đến hỗ trợ các nhà soạn nhạc trong quá trình sáng tạo.

Một ưu điểm khác của MusicLM là khả năng tạo ra các tác phẩm âm nhạc dài,
điều này không phải lúc nào cũng có thể với các mô hình tạo nhạc khác. Điều này
đặc biệt hữu ích cho các ứng dụng như tạo ra nhạc nền yêu cầu dòng chảy âm nhạc
liên tục trong một khoảng thời gian dài. Ngoài ra, mô hình có thể tạo ra âm thanh
chất lượng cao, điều này rất quan trọng để đảm bảo rằng âm nhạc được tạo ra
đáp ứng các tiêu chuẩn mong muốn.

Tuy nhiên, hạn chế đáng kể của MusicLM là mô hình chính thức và các checkpoint
chưa được phát hành như một dự án mã nguồn mở do rủi ro bản quyền.

¹⁸https://github.com/mdeff/fma

--- TRANG 19 ---
5.5 MusicGen

5.5.1 Tổng quan
MusicGen [14], một thành phần của thư viện Audiocraft [15], là một mô hình tạo nhạc
gần đây được phát triển bằng cách sử dụng kiến trúc transformer. Mô hình này tạo ra
các mẫu âm nhạc dựa trên mô tả văn bản hoặc đặc điểm giai điệu, do đó thiết lập
một khung thống nhất cho việc hiểu ngôn ngữ và âm nhạc. Các ứng dụng tiềm năng
của nó bao gồm nhiều lĩnh vực, bao gồm sáng tác âm nhạc và tạo nhạc nền.

5.5.2 Kiến trúc
MusicGen là một mô hình Transformer tự hồi quy một giai đoạn [14], được đào tạo
trên một tokenizer EnCodec 32kHz với bốn codebook được lấy mẫu ở 50 Hz. Nó khác
với các phương pháp như MusicLM bằng cách không yêu cầu biểu diễn ngữ nghĩa
tự giám sát, và tạo ra tất cả bốn codebook đồng thời. Bằng cách kết hợp một
độ trễ nhỏ giữa các codebook, việc dự đoán song song của các codebook là có thể,
giảm các bước tự hồi quy xuống 50 mỗi giây âm thanh.

Nó bao gồm điều kiện Văn bản, được thực hiện bằng cách tính toán tensor điều kiện
từ một mô tả văn bản khớp cho âm thanh đầu vào. Các phương pháp biểu diễn văn bản
khác nhau được sử dụng, bao gồm các bộ mã hóa văn bản được đào tạo trước như T5 [70],
mô hình ngôn ngữ dựa trên hướng dẫn FLAN-T5 [12], và biểu diễn văn bản-âm thanh
kết hợp CLAP [101].

Điều kiện giai điệu của nó áp dụng phương pháp tinh chỉnh lặp đi lặp lại, kiểm soát
cấu trúc giai điệu bằng cách điều kiện trên chromagram và mô tả văn bản của đầu vào.
Một thắt cổ chai thông tin, được giới thiệu bằng cách chọn bin thời gian-tần số
chiếm ưu thế tại mỗi timestep, giúp ngăn chặn overfitting và loại bỏ nhu cầu
về dữ liệu được giám sát.

5.5.3 Chức năng và Khả năng
MusicGen cho phép người dùng tạo ra các mẫu âm nhạc dựa trên mô tả văn bản
hoặc đặc điểm giai điệu. Nó cung cấp khả năng kiểm soát quá trình tạo ra,
phục vụ các sở thích và yêu cầu âm nhạc khác nhau. Mô hình hỗ trợ nhiều phong cách
âm nhạc và có thể xử lý các mức độ trừu tượng khác nhau trong mô tả văn bản
trong khi cho phép điều chỉnh giai điệu trong quá trình tạo ra.

MusicGen bao gồm bốn mô hình được đào tạo trước: small (300M), medium (1.5B),
melody (1.5B, với khả năng văn bản-thành-âm nhạc và văn bản+giai điệu-thành-âm nhạc),
và large (3.3B, chỉ văn bản-thành-âm nhạc). Phạm vi mô hình này cho phép người dùng
chọn mô hình phù hợp nhất với các trường hợp sử dụng cụ thể và hạn chế tài nguyên.

MusicGen được chọn và tích hợp liền mạch như thành phần tạo nhạc của AudioCraft [15],
một bộ công cụ âm thanh AI. Các giao diện được triển khai để tương tác với MusicGen
có sẵn thông qua Jupyter notebook¹⁹, Gradio cục bộ²⁰, và HuggingFace Space²¹.
Để vận hành mô hình cục bộ, cần có GPU, với khuyến nghị về bộ nhớ 16GB, nhưng
các GPU nhỏ hơn vẫn có thể tạo ra các chuỗi ngắn hơn hoặc hoạt động với mô hình nhỏ.

5.5.4 Ưu điểm và Hạn chế
MusicGen nổi bật như một mô hình tạo nhạc có thể kiểm soát một giai đoạn tiên tiến,
với khả năng được điều kiện trên cả văn bản và giai điệu. Nó sử dụng các chiến lược
chèn codebook đơn giản để tạo ra âm nhạc chất lượng cao, cũng dẫn đến việc giảm
số lượng bước thời gian tự hồi quy so với phương pháp làm phẳng truyền thống hơn.
Hơn nữa, hiệu suất của MusicGen đã được phân tích kỹ lưỡng trên các kích thước
mô hình khác nhau, phương pháp điều kiện và kỹ thuật tiền xử lý văn bản,
chứng minh tính linh hoạt của nó [14].

Trong lĩnh vực tạo nhạc từ văn bản, MusicGen đã cho thấy sự vượt trội so với
các mô hình khác như Mousai [82], Riffusion [31], MusicLM [3], và Noise2Music [44].
Đầu ra âm thanh chất lượng cao của nó thể hiện sự tuân thủ tốt hơn với mô tả văn bản
được cung cấp. Khi nói đến việc tạo ra giai điệu, MusicGen hiệu quả tạo ra âm nhạc
được điều kiện trên một giai điệu cho trước, ngay cả khi chroma bị bỏ qua tại thời điểm
suy luận, một thuộc tính mạnh mẽ phân biệt thêm nó.

MusicGen đã nhận được điểm cao trong cả các chỉ số đánh giá khách quan và chủ quan,
bao gồm Fréchet Audio Distance (FAD) [47], Kullback-Leibler Divergence (KL),
và điểm CLAP [101, 45]. Các phát hiện định lượng này được củng cố bởi các đánh giá
định tính từ những người nghe đánh giá cao chất lượng âm thanh và sự liên quan
với đầu vào văn bản.

Tuy nhiên, MusicGen có một số hạn chế. Phương pháp tạo ra của nó không cung cấp
khả năng kiểm soát tinh vi về việc tuân thủ của âm nhạc được tạo ra với việc điều kiện;
điều này phần lớn phụ thuộc vào khung điều kiện (CF). Ngoài ra, trong khi việc tăng cường
dữ liệu cho điều kiện văn bản là đơn giản, điều kiện âm thanh yêu cầu điều tra thêm
về các phương pháp tăng cường dữ liệu, loại và lượng hướng dẫn cần thiết.

Về mặt đạo đức, MusicGen đặt ra những thách thức nhất định tương tự như nhiều
mô hình sinh quy mô lớn. Tập dữ liệu đào tạo được sử dụng có tỷ lệ đáng kể
âm nhạc phong cách phương Tây, có thể dẫn đến thiếu đa dạng tiềm ẩn trong
âm nhạc được tạo ra. Tuy nhiên, thông qua việc đơn giản hóa thiết kế, như sử dụng
mô hình ngôn ngữ một giai đoạn và số lượng bước tự hồi quy giảm, mô hình hy vọng
mở rộng ứng dụng của nó đến các tập dữ liệu mới.

¹⁹https://github.com/facebookresearch/audiocraft/blob/main/demo.ipynb
²⁰https://colab.research.google.com/drive/1fxGqfg96RBUvGxZ1XXN07s3DthrKUl4-?usp=sharing
²¹https://huggingface.co/spaces/facebook/MusicGen

--- TRANG 20 ---
6 Công cụ Tạo Nhạc Dựa trên Hình ảnh
Các công cụ Tạo Nhạc Dựa trên Hình ảnh biểu thị một xu hướng mới trong lĩnh vực này,
sử dụng các đầu vào hình ảnh như hình ảnh hoặc video để tạo ra các tác phẩm âm nhạc.
Các mô hình này vượt qua ranh giới tạo nhạc truyền thống, tạo ra sự phản chiếu
thính giác của các kích thích hình ảnh. Như một phần của danh mục mô hình không có
tham số lớn hơn, chúng tạo ra âm nhạc một cách tự động từ các gợi ý hình ảnh
được cung cấp, mang lại những khả năng độc đáo trong bối cảnh tạo nhạc.

6.1 Controllable Music Transformer

6.1.1 Tổng quan
Controllable Music Transformer (CMT) [24] là một phương pháp dựa trên transformer
chuyên về việc tạo ra nhạc nền phù hợp với video đã cho về mặt nhịp điệu
và tâm trạng. Nó đề xuất một mối quan hệ nhịp điệu mới giữa video và âm nhạc,
kết nối thời gian, tốc độ chuyển động và tính nổi bật của chuyển động từ video
với nhịp, mật độ simu-note và cường độ simu-note từ âm nhạc tương ứng.
Bằng cách này, CMT có thể tạo ra nhạc nền phù hợp với video đã cho về mặt
nhịp điệu và tâm trạng.

6.1.2 Kiến trúc
CMT sử dụng phương pháp dựa trên transformer với 12 lớp self-attention và
tám attention head để phân tích nhịp điệu của video và âm nhạc và thiết lập
kết nối giữa chúng. Mô hình được đào tạo trên Lakh Pianoroll Dataset [25, 69],
chứa hơn 174.000 pianoroll đa track được sử dụng để biểu diễn âm nhạc MIDI.
Mô hình CMT kết hợp các đặc điểm nhịp điệu được trích xuất từ cả video và
âm nhạc MIDI, được biểu diễn như các từ ghép, và nhúng chúng với mã hóa
beat-timing để tạo ra một chuỗi các token âm nhạc. Sau đó, mô hình transformer
dự đoán các thuộc tính của mỗi token, như cao độ, thời lượng và loại nhạc cụ,
để tạo ra âm nhạc cuối cùng. Cuối cùng, trong giai đoạn suy luận, CMT thay thế
các đặc điểm nhịp điệu bằng những đặc điểm từ video để tạo ra âm nhạc phù hợp
với nhịp điệu của video.

6.1.3 Tính năng và Khả năng
CMT có thể tạo ra nhạc nền phù hợp với video đã cho về mặt nhịp điệu và tâm trạng.
Nó cho phép kiểm soát cục bộ các đặc điểm nhịp điệu và kiểm soát toàn cầu
các thể loại âm nhạc và nhạc cụ. Sau khi hoàn thành đào tạo, mô hình CMT đã
hiểu ý nghĩa đằng sau các thuộc tính cường độ và mật độ, và do đó chúng ta
chỉ cần thay thế hai thuộc tính đó khi thích hợp trong giai đoạn suy luận để
làm cho âm nhạc hài hòa hơn với video đã cho. CMT giới thiệu một siêu tham số
để kiểm soát khả năng tương thích giữa âm nhạc và video, làm cho âm nhạc được
tạo ra phù hợp hơn so với âm nhạc do con người tạo ra. Hơn nữa, CMT giới thiệu
mã hóa beat-timing để tận dụng thông tin thời gian hoặc độ dài từ video và
lựa chọn loại thể loại/nhạc cụ để chọn các token khởi đầu khác nhau trong
giai đoạn suy luận.

Ưu điểm và Hạn chế Mô hình CMT được đề xuất đã chứng minh hiệu suất thỏa đáng [24]
trong việc tạo ra nhạc nền phù hợp chặt chẽ với nhịp điệu và tâm trạng của video
đã cho. Nó đạt được điều này bằng cách kiểm soát các thuộc tính mật độ và cường độ
của âm nhạc, góp phần tạo ra sự kết hợp âm nhạc-video hài hòa hơn. Ngoài ra,
CMT giới thiệu một siêu tham số cho phép người dùng kiểm soát mức độ tương thích
giữa âm nhạc và video. Tuy nhiên, điều quan trọng cần lưu ý là việc tạo ra
nhạc nền cho các video dài hơn hai phút có thể tốn kém về mặt tính toán, có thể
hạn chế khả năng áp dụng của nó đối với một số trường hợp sử dụng nhất định.
Hơn nữa, trong khi CMT tạo ra âm nhạc có khả năng tương thích cao với video,
tính du dương của âm nhạc được tạo ra vẫn còn kém hơn so với âm nhạc trong
tập đào tạo.

6.2 V-MusProd: Mô hình AI Sản xuất Âm nhạc Video

6.2.1 Tổng quan
V-MusProd [109] là một mô hình được thiết kế riêng để tạo ra một tác phẩm âm nhạc
phù hợp cho một video đã cho, tận dụng phương pháp tách rời độc đáo cho hợp âm,
giai điệu và phần đệm. Phương pháp này trích xuất và chuyển đổi các đặc điểm
ngữ nghĩa, màu sắc và chuyển động từ video để hướng dẫn quá trình tạo nhạc,
đảm bảo mối quan hệ hài hòa giữa nội dung hình ảnh của video và âm nhạc được AI tạo ra.

6.2.2 Kiến trúc
V-MusProd, khung tạo nhạc mới của chúng tôi, bao gồm một bộ điều khiển video
và một bộ tạo nhạc. Bộ điều khiển video trích xuất các đặc điểm hình ảnh và
nhịp điệu, sau đó phục vụ như đầu vào ngữ cảnh cho bộ tạo nhạc. Bộ tạo nhạc
tuân theo một quá trình tách rời để tạo ra âm nhạc, với ba giai đoạn được đào tạo
độc lập: Hợp âm, Giai điệu và Phần đệm. Tác phẩm âm nhạc cuối cùng được tạo thành
bằng cách kết hợp các track giai điệu và phần đệm được tạo ra tại thời điểm suy luận.

Bộ điều khiển video sử dụng các kỹ thuật trích xuất đặc điểm có ý nghĩa từ video
để đơn giản hóa quá trình học. Chúng tôi trích xuất các đặc điểm ngữ nghĩa, màu sắc
và chuyển động riêng biệt để hướng dẫn mô hình tạo nhạc. Các đặc điểm ngữ nghĩa
được trích xuất sử dụng mô hình CLIP2Video được đào tạo trước để mã hóa các
frame video thô thành các token đặc điểm ngữ nghĩa. Các đặc điểm màu sắc,
biểu diễn phân phối màu sắc trong một đa tạp phi tuyến, phục vụ như tín hiệu
điều khiển cho việc tạo ra hợp âm. Các đặc điểm chuyển động, được xác định
bằng cách tính toán sự khác biệt RGB, được sử dụng để xác định nhịp độ âm nhạc.

Các đặc điểm ngữ nghĩa và màu sắc được xử lý thông qua các transformer encoder
riêng biệt và sau đó được nối với nhau theo chiều dài. Một embedding có thể học
được thêm vào để phân biệt giữa các token đặc điểm màu sắc hoặc đặc điểm ngữ nghĩa,
sau đó được đưa vào một transformer encoder để fusion liên phương thức và thời gian.
Đầu ra được fusion phục vụ như key và value của cross-attention trong Chord Transformer.

Bộ tạo nhạc, bao gồm Chord Transformer, Melody Transformer và Accompaniment Transformer,
được thiết kế để tạo ra âm nhạc ký hiệu được điều kiện trên đặc điểm video được trích xuất.

Chord Transformer áp dụng kiến trúc transformer decoder để học phụ thuộc dài hạn
của các chuỗi đặc điểm video đầu vào. Melody Transformer, tuân theo kiến trúc
transformer encoder-decoder, tạo ra một chuỗi nốt như đầu ra giai điệu dựa trên
chuỗi hợp âm đầu vào. Accompaniment Transformer, tương tự sử dụng transformer
encoder-decoder, tạo ra chuỗi phần đệm dựa trên cả hợp âm và giai điệu.
Tác phẩm âm nhạc cuối cùng được hình thành bằng cách kết hợp phần đệm được tạo ra
với giai điệu.

6.2.3 Tính năng và Khả năng
V-MusProd đại diện cho một bước tiến đáng kể trong việc tạo nhạc có điều kiện video.
Nó kết hợp các tín hiệu hình ảnh từ nội dung video và tạo nhạc theo một cách
chưa từng có, mang lại cho âm nhạc được tạo ra một cảm giác phong phú và
có nền tảng ngữ cảnh.

Thuộc tính đặc biệt của V-MusProd là việc phân chia các yếu tố âm nhạc thành
hợp âm, giai điệu và phần đệm. Cách tiếp cận này cho phép thao tác và kiểm soát
sâu sắc từng thành phần cá nhân, tăng cường tính linh hoạt và độ chính xác
của quá trình tạo nhạc.

V-MusProd cũng tận dụng một loạt các đặc điểm đa dạng từ chính nội dung video.
Nó sử dụng các thuộc tính ngữ nghĩa, màu sắc và chuyển động để phát triển
một liên kết mạnh mẽ giữa nội dung video và âm nhạc được tạo ra. Điều này
đảm bảo rằng âm nhạc được tạo ra không chỉ là một đầu ra độc lập mà thực sự
là một biểu diễn có thể nghe được của đầu vào video.

Hơn nữa, kiến trúc của mô hình được thiết kế để thích ứng với một cài đặt
không điều kiện, mở rộng khả năng của nó ra ngoài việc tạo nhạc có điều kiện
video. Điều này làm nổi bật tính linh hoạt của mô hình và tiềm năng cho
các ứng dụng rộng hơn.

6.2.4 Ưu điểm và Hạn chế
V-MusProd thể hiện những cải tiến đáng kể so với các mô hình tạo nhạc video
trước đây [32, 49, 95, 96, 108]. Các mô hình trước đây này chủ yếu dựa vào
các đặc điểm nhịp điệu video hoặc bị hạn chế đối với các loại video cụ thể.
Tuy nhiên, V-MusProd chứng minh khả năng áp dụng rộng hơn. Nó có thể tạo ra
âm nhạc trực tiếp từ đầu vào video mà không cần dữ liệu video-âm nhạc được ghép đôi
hoặc chú thích bổ sung, đánh dấu một tiến bộ đáng kể trong lĩnh vực này.
Khả năng này không chỉ đơn giản hóa quá trình tạo ra mà còn đứng như một
minh chứng cho thiết kế sáng tạo và mạnh mẽ của mô hình.

Các đánh giá khách quan làm nổi bật hiệu suất nâng cao của V-MusProd so với
CMT [24] về mặt tương ứng video-âm nhạc và chất lượng âm nhạc. Các đánh giá
chủ quan, bao gồm một nghiên cứu người dùng liên quan đến các chuyên gia sáng tác
âm nhạc và không phải chuyên gia, cũng củng cố phát hiện này, với V-MusProd
vượt trội hơn CMT trong gần như tất cả các chỉ số.

Tuy nhiên, V-MusProd có một số hạn chế. Hiện tại, tập dữ liệu và âm nhạc được
tạo ra bởi V-MusProd chỉ chứa các track piano, cho thấy thiếu khám phá
các nhạc cụ khác như trống, guitar và dây. Hơn nữa, mô hình không tính đến
một cách rõ ràng các đặc điểm cảm xúc cấp cao hoặc cấu trúc lặp lại của
các cụm từ âm nhạc, một lĩnh vực tiềm năng để cải thiện thêm. Ba giai đoạn
của phương pháp V-MusProd được đào tạo riêng biệt thay vì end-to-end, một
yếu tố có thể ảnh hưởng đến hiệu suất tổng thể và hiệu quả của mô hình.
Mặc dù có những hạn chế này, V-MusProd phục vụ như một baseline hứa hẹn
cho nghiên cứu tương lai trong lĩnh vực tạo nhạc có điều kiện video.

--- TRANG 21 ---
6.3 Foley Music

6.3.1 Tổng quan
Foley Music [32] là một công cụ tạo ra âm nhạc phù hợp với nội dung hình ảnh
của một clip video. Công cụ này sử dụng một mạng nơ-ron sâu để học mối quan hệ
giữa các hiệu ứng âm thanh và âm nhạc mà nó tạo ra.

6.3.2 Kiến trúc
Trong nghiên cứu này, các tác giả đề xuất việc tạo ra âm nhạc từ video bằng cách
xác định hai biểu diễn trung gian quan trọng: các điểm chính cơ thể và các sự kiện MIDI.
Quá trình tạo ra được hình thành như một bài toán dịch chuyển động-thành-MIDI,
và một khung Graph-Transformer được trình bày để dự đoán các chuỗi sự kiện MIDI
tương ứng chính xác với các chuyển động cơ thể. Hệ thống được đề xuất bao gồm
ba thành phần chính: một bộ mã hóa hình ảnh, một bộ giải mã MIDI và một bộ tổng hợp
âm thanh. Để bắt đầu, bộ mã hóa hình ảnh lấy các frame video làm đầu vào và
trích xuất tọa độ các điểm chính bằng cách sử dụng Graph Convolutional Network (GCN).
Điều này giúp nắm bắt động lực cơ thể và tạo ra một biểu diễn tiềm ẩn theo thời gian.
Sau đó, bộ giải mã MIDI lấy biểu diễn chuỗi video này và tạo ra một chuỗi
các sự kiện MIDI. Cuối cùng, chuỗi sự kiện MIDI được tạo ra được chuyển đổi
thành dạng sóng tương ứng với một công cụ tổng hợp âm nhạc có sẵn.

6.3.3 Tính năng và Khả năng
Hệ thống Foley Music được xây dựng trên PyTorch, một framework học sâu phổ biến,
và có khả năng tạo ra các clip âm nhạc cho nhiều nhạc cụ khác nhau bao gồm
accordion, bass, bassoon, cello, guitar, piano, tuba, ukulele và violin.
Để chuẩn bị dữ liệu âm thanh cho việc tạo nhạc, các sự kiện MIDI được trích xuất
từ các bản ghi âm thanh. Hiệu suất của hệ thống được chứng minh trong một
thí nghiệm đào tạo nó trên một clip video 6 giây từ một tập dữ liệu video
biểu diễn âm nhạc quy mô lớn. Quá trình đào tạo yêu cầu sử dụng các optimizer
và regularizer để đảm bảo kết quả chất lượng cao. Như đã đề cập, cần có
một bộ tổng hợp phần mềm để có được dạng sóng âm nhạc được tạo ra cuối cùng.

6.3.4 Ưu điểm và Hạn chế
Kết quả thí nghiệm tiết lộ rằng mô hình có khả năng hoạt động hiệu quả trên
các loại video biểu diễn âm nhạc khác nhau. Kết quả cho thấy rằng khung
thành công thiết lập mối tương quan giữa tín hiệu hình ảnh và âm nhạc sử dụng
các điểm chính cơ thể và biểu diễn MIDI.

Đáng chú ý, các biểu diễn MIDI được sử dụng trong khung là trong suốt và
hoàn toàn có thể diễn giải, cung cấp tính linh hoạt trong chỉnh sửa âm nhạc.
Biểu diễn MIDI có thể được sửa đổi một cách thuận tiện để tạo ra âm nhạc
theo nhiều phong cách đa dạng bằng cách sử dụng các biểu diễn MIDI. Tuy nhiên,
điều này yêu cầu các bộ tổng hợp âm nhạc bổ sung để render âm nhạc, vì khung
không bao gồm một bộ tổng hợp nơ-ron.

7 Công cụ Tạo Nhạc Thương mại
Thị trường thương mại tràn ngập với nhiều công cụ tạo nhạc phục vụ cho những người dùng
muốn tạo nhạc dễ dàng và nhanh chóng, nhưng có thể không có kiến thức âm nhạc
trước đó hoặc kỹ năng lập trình. Các công cụ này thường được trang bị giao diện
người dùng dựa trên web cho phép người dùng thao tác các tham số như cảm xúc, nhịp độ
và độ dài để tạo ra âm nhạc theo sở thích của họ. Tuy nhiên, các công cụ này có thể
được phân loại rộng rãi dựa trên giao diện người dùng và mô hình tương tác của chúng:
Công cụ Tham số Cài đặt sẵn, Công cụ Tạo ra Tương tác và Công cụ Tự động tạo ra.

Các mô hình dựa trên tham số bao gồm các nền tảng cho phép người dùng chọn
các tham số khác nhau như tâm trạng, thể loại hoặc hoạt động làm yếu tố hướng dẫn
cho việc tạo nhạc. Một số nền tảng trong danh mục này cung cấp một lớp tùy chỉnh
nâng cao hơn, cho phép người dùng điều chỉnh cấu trúc và thành phần của âm nhạc
được tạo ra, như Mubert²², Boomy²³, Ecrett Music²⁴, và Soundraw.io²⁵.

Các mô hình dựa trên gợi ý, như SongR²⁶, cung cấp một tương tác độc đáo nơi
người dùng có thể nhập lời bài hát hoặc ý tưởng giai điệu của họ để hướng dẫn
quá trình tạo nhạc.

Các mô hình dựa trên phong cách thích ứng quá trình sáng tác dựa trên các ảnh hưởng
do người dùng cung cấp, có thể bao gồm một tệp âm nhạc hiện có hoặc một tác động
cảm xúc cụ thể. Ngoài ra, các mô hình này cũng có thể cung cấp các phong cách
cài đặt sẵn để hướng dẫn việc sáng tác, như Aiva.ai²⁷.

Mặc dù dễ sử dụng và có thể tiếp cận, các công cụ này thường không cung cấp
thông tin chi tiết về kiến trúc mô hình cơ bản của chúng, khiến người dùng không
biết về các phương pháp hoặc thuật toán đằng sau âm nhạc họ tạo ra. Sự thiếu
minh bạch này, kết hợp với các tùy chọn tùy chỉnh có thể hạn chế, có thể có nghĩa
là các công cụ này không đáp ứng được cho người dùng có nhu cầu âm nhạc
chuyên biệt hoặc phức tạp hơn.

Tuy nhiên, các công cụ tạo nhạc thương mại có giá trị trong khả năng nhanh chóng
và thuận tiện tạo ra âm nhạc. Sự vắng mặt của các mô tả mô hình sâu sắc,
tuy nhiên, làm cho việc đánh giá toàn diện hiệu suất hoặc hiệu quả của các
công cụ này trở nên thách thức. Mặc dù bài báo này sẽ không đi sâu vào
chi tiết của các mô hình âm nhạc thương mại, đóng góp của chúng trong việc
làm cho việc tạo nhạc trở nên dễ tiếp cận và thuận tiện hơn cho người dùng
được công nhận.

8 Kết luận
Trong khảo sát này, chúng tôi đã mô tả các mô hình cho phép tạo nhạc dựa trên
tham số, gợi ý và clip video. Phân tích của chúng tôi làm nổi bật những ưu điểm
và hạn chế độc đáo của mỗi công cụ, như tính linh hoạt, độ tinh vi và chất lượng
của âm nhạc được tạo ra. Ví dụ, một số công cụ cung cấp tính linh hoạt trong
việc phát triển các tệp MIDI nhưng yêu cầu bộ tổng hợp phần mềm bổ sung để
xử lý âm nhạc. Các công cụ khác (đặc biệt là các mô hình khuếch tán dựa trên gợi ý)
tạo ra âm nhạc tinh vi nhưng cần tính linh hoạt hơn trong việc tạo ra âm nhạc
với các nhạc cụ khác nhau. Một thách thức vẫn còn là khả năng tạo ra các tác phẩm
dài hơn với một mẫu âm nhạc tốt. Chúng tôi cũng thừa nhận rằng đánh giá này
không hoàn toàn đầy đủ do bản chất phát triển nhanh chóng của lĩnh vực này.
Tuy nhiên, phạm vi các công cụ tạo nhạc AI có sẵn cho thấy triển vọng trong
việc cách mạng hóa ngành công nghiệp âm nhạc, tăng cường sự sáng tạo và mở rộng
phạm vi biểu đạt âm nhạc. Chúng tôi dự đoán sự xuất hiện của các mô hình tinh vi
hơn sẽ vượt qua những hạn chế hiện tại và cung cấp các công cụ tạo nhạc AI linh hoạt,
thân thiện với người dùng và chất lượng cao hơn.

²²https://mubert.com
²³https://boomy.com
²⁴https://ecrettmusic.com
²⁵https://soundraw.io
²⁶https://app.songr.ai
²⁷https://www.aiva.ai

--- TRANG 22 ---
Tài liệu tham khảo

[1] ABC Notation. ABC Notation. https://abcnotation.com/, Truy cập năm 2023.

[2] Feross Aboukhadijeh. Bitmidi: Free midi files, 2023. Truy cập: 2023-05-15.

[3] Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. Musiclm: Generating music from text, 2023.

[4] Google AI. Getting started, n.d.

[5] Google AI. Magenta. https://magenta.tensorflow.org/, n.d. Truy cập 10 tháng 2 năm 2023.

[6] Charles Ames. The markov process as a compositional model: A survey and tutorial. Leonardo, 22(2):175–187, 1989.

[7] John A Biles. Genjam: A genetic algorithm for generating jazz solos. Proceedings of the international computer music conference, pages 131–137, 1994.

[8] Louis Bouchard. Musiclm - pytorch. https://github.com/lucidrains/musiclm-pytorch, February 2023. Truy cập: March 15, 2023.

[9] Jean-Pierre Briot and François Pachet. Deep learning for music generation: Challenges and directions. Neural Comput. Appl., 32(4):981–993, feb 2020.

[10] Filippo Carnovalini and Antonio Rodà. Computational creativity and music generation systems: An introduction to the state of the art. Frontiers in Artificial Intelligence, 3, 2020.

[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019.

[12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.

[13] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training, 2021.

[14] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. Simple and controllable music generation, 2023.

[15] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. Simple and controllable music generation. arXiv preprint arXiv:2306.05284, 2023.

--- TRANG 23 ---
[16] John Covach and Andrew Flory. What's that sound? An introduction to rock and its history. WW Norton & Company, 2005.

[17] Luisa Rocha de Azevedo Santos, Carlos Silla Jr., and Marjory Da Costa Abreu. A methodology for procedural piano music composition with mood templates using genetic algorithms. In 11th International Conference of Pattern Recognition Systems (ICPRS 2021), pages 1–6. IET, October 2021.

[18] Elena Denisova-Schmidt. The Evolution of Music: Culture, Technology, and Society. Routledge, 2017.

[19] Magenta Developers. Magenta improv rnn. https://github.com/magenta/magenta/blob/main/magenta/models/improv_rnn/README.md, 2021. Truy cập ngày 17 tháng 4 năm 2023.

[20] Magenta Developers. Magenta melody rnn. https://github.com/magenta/magenta/blob/main/magenta/models/melody_rnn/README.md, 2021. Truy cập ngày 17 tháng 4 năm 2023.

[21] Magenta Developers. Magenta polyphony rnn. https://github.com/magenta/magenta/blob/main/magenta/models/polyphony_rnn/README.md, 2021. Truy cập ngày 17 tháng 4 năm 2023.

[22] Magenta Developers. Magenta model directory. https://github.com/magenta/magenta/tree/main/magenta/models, 2023.

[23] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music, 2020.

[24] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, and Shuicheng Yan. Video background music generation with controllable music transformer. In Proceedings of the 29th ACM International Conference on Multimedia. ACM, oct 2021.

[25] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI), 2018.

[26] A. DuBreuil. Hands-On Music Generation with Magenta: Explore the role of deep learning in music generation and assisted music composition. Packt Publishing, 2020.

[27] Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam Roberts. GANSynth: Adversarial neural audio synthesis. In International Conference on Learning Representations, 2019.

[28] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi. Neural audio synthesis of musical notes with wavenet autoencoders, 2017.

--- TRANG 24 ---
[29] Majid Farzaneh and Rahil Mahdian Toroghi. Music generation using an interactive evolutionary algorithm. In Pattern Recognition and Artificial Intelligence, pages 207–217. Springer International Publishing, December 2019.

[30] Lucas N. Ferreira and Jim Whitehead. Learning to generate music with sentiment, 2021.

[31] Seth* Forsgren and Hayk* Martiros. Riffusion - Stable diffusion for real-time music generation, 2022.

[32] Chuang Gan, Deng Huang, Peihao Chen, Joshua B. Tenenbaum, and Antonio Torralba. Foley music: Learning to generate music from videos. In Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI, page 758–775, Berlin, Heidelberg, 2020. Springer-Verlag.

[33] Sudhanshu Gautam and Sarita Soni. Music composition with artificial intelligence system based on markov chain and genetic algorithm. 2018.

[34] Jon Gillick, Adam Roberts, Jesse Engel, Douglas Eck, and David Bamman. Learning to groove with inverse sequence transformations. In International Conference on Machine Learning (ICML), 2019.

[35] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.

[36] Venkat N. Gudivada, Dhana Rao, and Vijay V. Raghavan. Chapter 9 - big data driven natural language processing research and applications. In Venu Govindaraju, Vijay V. Raghavan, and C.R. Rao, editors, Big Data Analytics, volume 33 of Handbook of Statistics, pages 203–238. Elsevier, 2015.

[37] Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna Huang, Sander Dieleman, Erich Elsen, Jesse Engel, and Douglas Eck. The maestro dataset. arXiv preprint arXiv:1810.12247, 2018.

[38] Carlos Hernandez-Olivan and Jose R. Beltran. Music composition with deep learning: A review, 2021.

[39] Simone Hill. Markov melody generator. University of Massachusetts Lowell, 2023.

[40] Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, and Yi-Hsuan Yang. Compound word transformer: Learning to compose full-song music over dynamic directed hypergraphs, 2021.

[41] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer, 2018.

--- TRANG 25 ---
[42] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel P. W. Ellis. Mulan: A joint embedding of music audio and natural language. In Proceedings of the the 23rd International Society for Music Information Retrieval Conference (ISMIR), 2022.

[43] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel PW Ellis. Mulan: A joint embedding of music audio and natural language. arXiv preprint arXiv:2208.12415, 2022.

[44] Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, Jesse Engel, Quoc V. Le, William Chan, Zhifeng Chen, and Wei Han. Noise2music: Text-conditioned music generation with diffusion models, 2023.

[45] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models, 2023.

[46] Junyan Jiang, Gus G. Xia, Dave B. Carlton, Chris N. Anderson, and Ryan H. Miyakawa. Transformer vae: A hierarchical model for structure-aware and interpretable music representation learning. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 516–520, 2020.

[47] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fréchet audio distance: A metric for evaluating music enhancement algorithms, 2019.

[48] Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.

[49] A.S. Koepke, O. Wiles, Y. Moses, and A. Zisserman. Sight to sound: An end-to-end approach for visual piano transcription. In International Conference on Acoustics, Speech, and Signal Processing, 2020.

[50] Ben Krause, Iain Murray, Steve Renals, and Liang Lu. Multiplicative LSTM for sequence modelling. ICLR Workshop track, 2017.

[51] Steven G Laitz. The complete musician: An integrated approach to tonal theory, analysis, and listening. Oxford University Press, 2012.

[52] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.

[53] Colby Leider. Digital audio workstation. McGraw-Hill New York, 2004.

[54] Classical Archives LLC. Classical archives: The largest classical music site in the world, 2023. Truy cập: 2023-05-15.

[55] Justin London. Hearing in time: Psychological aspects of musical meter. Oxford University Press, 2004.

--- TRANG 26 ---
[56] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J. Cai. Novice-ai music co-creation via ai-steering tools for deep generative models. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI '20, page 1–13, New York, NY, USA, 2020. Association for Computing Machinery.

[57] Magenta. Magenta/magenta: Magenta: Music and art generation with machine intelligence, n.d.

[58] Manuel Marques, V Oliveira, S Vieira, and AC Rosa. Music composition using genetic evolutionary algorithms. In Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No. 00TH8512), volume 1, pages 714–719. IEEE, 2000.

[59] Leonard B Meyer. Style and music: Theory, history, and ideology. University of Pennsylvania Press, 1989.

[60] Michael Möser. Engineering acoustics. Nova York (Estados Unidos): Springer Publishing, 2009.

[61] Christopher Olah. Understanding lstm networks, 2015.

[62] OpenAI, Apr 2020.

[63] Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton, and Tom M Mitchell. Zero-shot learning with semantic output codes. Advances in neural information processing systems, 22, 2009.

[64] Christine Payne. Musenet. https://openai.com/blog/musenet/, 2019. Truy cập: 2023-05-15.

[65] Walter Piston. Harmony. WW Norton & Company, 1987.

[66] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10609–10619, 2022.

[67] Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444, 2017.

[68] Colin Raffel. Learning-based methods for comparing sequences, with applications to audio-to-MIDI alignment and matching, June 2016.

[69] Colin Raffel. Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching. PhD thesis, 2016.

[70] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2020.

[71] Adhika Sigit Ramanto and Nur Ulfa Maulidevi. Markov chain based procedural music generator with user chosen mood compatibility. International Journal of Asia Digital Art and Design, 21:19–24, 2017.

--- TRANG 27 ---
[72] Roger Ratcliff, Pablo Gómez, and Gail McKoon. A diffusion model account of response time and accuracy in a brightness discrimination task: Fitting real data and failing to fit fake but plausible data. Psychological Review, 111(4):931–959, 2004.

[73] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2, 2019.

[74] Recordare LLC. MusicXML. https://www.musicxml.com/, Truy cập năm 2023.

[75] Jean-Claude Risset and David L. Wessel. 5 - exploration of timbre by analysis and synthesis. In Diana Deutsch, editor, The Psychology of Music (Second Edition), Cognition and Perception, pages 113–169. Academic Press, San Diego, second edition edition, 1999.

[76] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck. A hierarchical latent vector model for learning long-term structure in music, 2019.

[77] Juan G Roederer. The physics and psychophysics of music: An introduction. Springer Science & Business Media, 2013.

[78] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684–10695, June 2022.

[79] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022.

[80] Thomas D. Rossing, F. Richard Moore, and Paul A. Wheeler. The Science of Sound. Addison-Wesley, 2002.

[81] James Russell. A circumplex model of affect. Journal of Personality and Social Psychology, 39:1161–1178, 12 1980.

[82] Flavio Schneider, Zhijing Jin, and Bernhard Schölkopf. Moˆ usai: Text-to-music generation with long-context latent diffusion, 2023.

[83] Werner Schweer and Others. Musescore. MuseScore BVBA, 2011.

[84] W.A. Sethares. Tuning, Timbre, Spectrum, Scale. Springer London, 2005.

[85] Ilana Shapiro and Mark Huber. Markov chains for computer music generation. Journal of Humanistic Mathematics, 11(2):167–195, July 2021.

[86] Alex Sherstinsky. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D: Nonlinear Phenomena, 404:132306, mar 2020.

[87] Alex Sherstinsky. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. Physica D: Nonlinear Phenomena, 404:132306, 2020.

[88] Daniel Sleator and David Temperley. Melisma stochastic melody generator. Melisma Music Analyzer, 2003.

--- TRANG 28 ---
[89] John A Sloboda. Exploring the musical mind: Cognition, emotion, ability, function. Oxford University Press, 2005.

[90] Jon Sneyers and Danny De Schreye. Apopcaleaps: Automatic music generation with chrism. 10 2010.

[91] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256–2265, Lille, France, 07–09 Jul 2015. PMLR.

[92] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. Release strategies and the social impacts of language models, 2019.

[93] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021.

[94] Randall Richard Spangler. Rule-based analysis and generation of music. PhD thesis, 1999.

[95] Kun Su, Xiulong Liu, and Eli Shlizerman. Audeo: Audio generation for a silent performance video. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 3325–3337. Curran Associates, Inc., 2020.

[96] Didac Suris, Carl Vondrick, Bryan Russell, and Justin Salamon. It's time for artistic correspondence in music and video, 2022.

[97] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio, 2016.

[98] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018.

[99] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.

[100] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.

[101] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation, 2023.

--- TRANG 29 ---
[102] Ali Çağatay Yüksel, Mehmet Melih Karci, and A. Şima Uyar. Automatic music generation using evolutionary algorithms and neural networks. In 2011 International Symposium on Innovations in Intelligent Systems and Applications, pages 354–358, 2011.

[103] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec, 2021.

[104] Chen Zhang, Yi Ren, Kejun Zhang, and Shuicheng Yan. Sdmuse: Stochastic differential music editing and generation via hybrid representation, 2022.

[105] Chen Zhang, Yi Ren, Kejun Zhang, and Shuicheng Yan. Stochastic differential music editing and generation via hybrid representation, Sep 2022.

[106] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.

[107] Ziyi Zhao, Hanwei Liu, Song Li, Junwei Pang, Maoqing Zhang, Yi Qin, Lei Wang, and Qidi Wu. A review of intelligent music generation systems, 2022.

[108] Ye Zhu, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei Chai, Yan Yan, and Sergey Tulyakov. Quantized gan for complex music generation from dance videos. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.

[109] Le Zhuo, Zhaokai Wang, Baisen Wang, Yue Liao, Stanley Peng, Chenxi Bao, Miao Lu, Xiaobo Li, and Si Liu. Video background music generation: Dataset, method and evaluation, 2022.

39
