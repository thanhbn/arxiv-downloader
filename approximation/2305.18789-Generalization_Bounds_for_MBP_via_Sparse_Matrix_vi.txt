# Giới hạn Tổng quát hóa cho MBP thông qua Phác thảo Ma trận Thưa

Etash Kumar Guha
Trường Tin học
Viện Công nghệ Georgia
Atlanta, GA, 30332
etash@gatech.edu

Prasanjit Dubey
Trường Kỹ thuật Công nghiệp và Hệ thống
Viện Công nghệ Georgia
Atlanta, GA, 30332
pdubey31@gatech.edu

Xiaoming Huo
Trường Kỹ thuật Công nghiệp và Hệ thống
Viện Công nghệ Georgia
Atlanta, GA, 30332
huo@gatech.edu

## Tóm tắt

Trong bài báo này, chúng tôi đưa ra một giới hạn mới về lỗi tổng quát hóa của các mạng nơ-ron quá tham số khi chúng đã trải qua Cắt tỉa Dựa trên Độ lớn (MBP). Công trình của chúng tôi dựa trên các giới hạn trong Arora et al. [2018], nơi lỗi phụ thuộc vào một, xấp xỉ được tạo ra bởi cắt tỉa, và hai, số lượng tham số trong mô hình đã cắt tỉa, và cải thiện so với các giới hạn tổng quát hóa dựa trên chuẩn tiêu chuẩn. Các ước lượng đã cắt tỉa thu được bằng MBP gần với các hàm chưa cắt tỉa với xác suất cao, điều này cải thiện tiêu chí đầu tiên. Sử dụng Phác thảo Ma trận Thưa, không gian của các ma trận đã cắt tỉa có thể được biểu diễn hiệu quả trong không gian của các ma trận dày đặc có kích thước nhỏ hơn nhiều, từ đó cải thiện tiêu chí thứ hai. Điều này dẫn đến giới hạn tổng quát hóa mạnh hơn nhiều phương pháp tiên tiến, từ đó tạo ra bước đột phá mới trong phát triển thuật toán cho cắt tỉa và giới hạn lỗi tổng quát hóa của các mô hình quá tham số. Ngoài ra, chúng tôi mở rộng kết quả để thu được giới hạn tổng quát hóa cho Cắt tỉa Lặp lại [Frankle và Carbin, 2018]. Chúng tôi xác minh thực nghiệm sự thành công của phương pháp mới này trên Mạng Truyền thẳng kích hoạt ReLU trên tập dữ liệu MNIST và CIFAR10.

## 1 Giới thiệu

Các mạng nơ-ron quá tham số thường được sử dụng trong thực tế vì chúng đạt được lỗi tổng quát hóa đáng chú ý [Goodfellow et al., 2016]. Tuy nhiên, kích thước khổng lồ của chúng khiến chúng chạy chậm và tốn kém trong quá trình suy luận [Han et al., 2015]. Các nhà thực hành học máy (ML) thường sử dụng cắt tỉa Dựa trên Độ lớn (MBP) để khắc phục độ phức tạp tính toán này. Sau khi huấn luyện các mạng nơ-ron lớn, các tham số hoặc phần tử ma trận trong mô hình có độ lớn nhỏ nhất được đặt về 0. Điều này làm giảm đáng kể yêu cầu bộ nhớ của mô hình và thời gian suy luận. Tuy nhiên, MBP cũng đã được chứng minh là tạo ra ít lỗi tổng quát hóa và thực tế, thường làm giảm lỗi tổng quát hóa so với mô hình gốc [Han et al., 2015, Li et al., 2016, Cheng et al., 2017]. Việc kiểm tra nơi và tại sao sự tổng quát hóa mạnh xảy ra có thể giúp xây dựng các thuật toán học và mô hình tổng quát hóa tốt hơn [Foret et al., 2020, Le et al., 2018]. Tuy nhiên, các phân tích lý thuyết về tại sao MBP đạt được lỗi tổng quát hóa mạnh vẫn cần được thực hiện. Việc cung cấp các phân tích như vậy là thách thức vì nhiều lý do. Thứ nhất, việc loại bỏ các trọng số nhỏ nhất là một phép toán tương đối ít được nghiên cứu trong đại số tuyến tính, và chỉ có ít công cụ có sẵn để phân tích các tính chất của ma trận đã cắt tỉa. Thứ hai, khó để đặc trưng phân phối của trọng số sau huấn luyện và cắt tỉa.

Tuy nhiên, Arora et al. [2018] đã cung cấp một công cụ phân tích toàn diện hơn về lỗi tổng quát hóa của các mô hình có ít tham số hơn một cách hiệu quả. Cụ thể, họ đã đặt cận trên cho lỗi tổng quát hóa của một mạng nơ-ron lớn khi được nén. Chúng ta có thể áp dụng trực tiếp kết quả này cho các mô hình đã cắt tỉa vì các mô hình đã cắt tỉa về bản chất có ít tham số hơn. Giới hạn của họ được chia thành hai phần: lượng lỗi được mô hình tạo ra thông qua nén và số lượng tham số khác nhau trong mô hình đã nén. Chúng tôi sử dụng công cụ chính này để chỉ ra rằng các đầu ra từ MBP tổng quát hóa tốt vì chúng không tạo ra nhiều lỗi và có ít tham số hơn. Chúng tôi chứng minh cả hai hiện tượng này với một vài giả định hợp lý. Cụ thể, với một số giả định có thể biện minh về phân phối của các tham số trọng số đã huấn luyện, chúng tôi phát triển một cận trên của lượng lỗi mà mạng nơ-ron đã cắt tỉa chịu với xác suất cao. Chúng tôi cũng chứng minh rằng số lượng tham số cần thiết để biểu diễn đầy đủ không gian của các mô hình đã cắt tỉa là tương đối nhỏ. Cụ thể, chúng tôi chỉ ra rằng tập hợp các ma trận đã cắt tỉa của chúng tôi có thể được biểu diễn hiệu quả trong không gian của các ma trận dày đặc có kích thước nhỏ hơn nhiều thông qua Phác thảo Ma trận Thưa.

Kết hợp hai phần của giới hạn, chúng tôi thu được một giới hạn lỗi tổng quát hóa mới có tính cạnh tranh với các giới hạn tổng quát hóa tiên tiến. Hơn nữa, theo hiểu biết của chúng tôi, đây là giới hạn tổng quát hóa đầu tiên cho MBP sử dụng Giới hạn Nén. Chúng tôi xác minh thực nghiệm sự thành công của phương pháp mới này trên tập dữ liệu MNIST và CIFAR10 nơi giới hạn của chúng tôi tốt hơn vài bậc độ lớn (ít nhất 10^7 lần tốt hơn trên CIFAR10, tham khảo Hình 1b) so với các giới hạn tiêu chuẩn nổi tiếng của Neyshabur et al. [2015], Bartlett et al. [2017], và Neyshabur et al. [2017]. Chúng tôi mở rộng khung work để chỉ ra rằng việc sử dụng Cắt tỉa Độ lớn Lặp lại (IMP) hoặc Vé số [Frankle và Carbin, 2018] cũng tổng quát hóa. Cụ thể, Malach et al. [2020] chỉ ra rằng IMP tạo ra kết quả với lỗi nhỏ và ít tham số khác không. Chúng tôi sử dụng phác thảo ma trận để đếm hiệu quả số lượng tham số theo cách có thể sử dụng được cho giới hạn tổng quát hóa của chúng tôi. Điều này dẫn đến một giới hạn tổng quát hóa mạnh mà theo hiểu biết của chúng tôi, chỉ được phân tích thực nghiệm [Bartoldson et al., 2020, Jin et al., 2022].

**Đóng góp** Chúng tôi liệt kê chính thức các đóng góp ở đây. Đầu tiên chúng tôi chứng minh lỗi được tạo ra bởi MBP là nhỏ so với mô hình gốc. Hơn nữa, chúng tôi chứng minh rằng MBP của chúng tôi đạt được độ thưa đủ, tức là, tương đối ít tham số khác không còn lại sau cắt tỉa. Để thắt chặt giới hạn tổng quát hóa, chúng tôi chỉ ra rằng các ma trận đã cắt tỉa từ MBP có thể được phác thảo thành các ma trận dày đặc nhỏ hơn. Chúng tôi kết hợp các kết quả trên để chứng minh rằng lỗi tổng quát hóa của các mô hình đã cắt tỉa là nhỏ. Chúng tôi mở rộng khung chứng minh trên để thiết lập giới hạn lỗi tổng quát hóa cho IMP. Theo hiểu biết của chúng tôi, đây là những kết quả đầu tiên nghiên cứu tổng quát hóa của các mô hình đã cắt tỉa thông qua MBP hoặc IMP. Chúng tôi xác minh thực nghiệm rằng giới hạn tổng quát hóa của chúng tôi cải thiện so với nhiều giới hạn lỗi tổng quát hóa tiêu chuẩn cho MLP trên tập dữ liệu CIFAR10 và MNIST.

## 2 Các Công trình Liên quan

### 2.1 Giới hạn Tổng quát hóa Dựa trên Chuẩn

Trong những năm gần đây, nhiều công trình đã nghiên cứu cách sử dụng đếm tham số và chuẩn trọng số để tạo ra các giới hạn tổng quát hóa chặt chẽ hơn như một sự tiến hóa từ Độ phức tạp Rademacher và chiều VC cổ điển. Galanti et al. [2023] sử dụng Độ phức tạp Rademacher để phát triển giới hạn tổng quát hóa cho các mạng tự nhiên thưa như những mạng từ chính quy hóa thưa. Neyshabur et al. [2015] nghiên cứu một lớp tổng quát của giới hạn dựa trên chuẩn cho mạng nơ-ron. Hơn nữa, Bartlett và Mendelson [2002] đã sử dụng Độ phức tạp Rademacher và Gaussian để tạo ra giới hạn tổng quát hóa. Long và Sedghi [2020] đưa ra giới hạn lỗi tổng quát hóa cho Mạng Nơ-ron Tích chập (CNN) sử dụng khoảng cách từ trọng số ban đầu và số lượng tham số độc lập với chiều của bản đồ đặc trưng và số pixel trong đầu vào. Daniely và Granot [2019] sử dụng độ dài mô tả xấp xỉ như một dạng trực quan cho đếm tham số.

### 2.2 Kỹ thuật Cắt tỉa

Trong khi MBP là một trong những dạng cắt tỉa phổ biến nhất trong thực tế, các dạng khác cũng tồn tại. Collins và Kohli [2014] tạo ra độ thưa trong CNN của họ bằng cách sử dụng chính quy hóa ℓ1 trong huấn luyện. Molchanov et al. [2017] phát triển khung cắt tỉa lặp lại để nén CNN sâu sử dụng cắt tỉa dựa trên tiêu chí tham lam dựa trên khai triển Taylor và tinh chỉnh bằng lan truyền ngược. Liu et al. [2017] sử dụng Độ thưa Bộ lọc cùng với Network Slimming để tăng tốc trong CNN của họ. Ullrich et al. [2017] đặt tên soft-weight sharing như một phương pháp tạo ra độ thưa trong giới hạn của họ. Hơn nữa, Hooker et al. [2019] đã nghiên cứu thực nghiệm mẫu dữ liệu nào của các mô hình đã cắt tỉa sẽ khác biệt đáng kể so với mô hình gốc. Nhiều công trình sử dụng các phương pháp cắt tỉa ít phổ biến hơn như coresets [Mussay et al., 2019] hoặc hiện tượng IMP [Frankle và Carbin, 2018, Malach et al., 2020].

## 3 Chuẩn bị

### 3.1 Ký hiệu

Chúng tôi xem xét một bài toán phân loại đa lớp tiêu chuẩn nơi với một mẫu x cho trước, chúng ta dự đoán lớp y, là một số nguyên từ 1 đến k. Chúng ta giả định rằng mô hình của chúng ta sử dụng một thuật toán học tạo ra một tập hợp L ma trận M={A1, . . . ,AL} nơi Ai∈ℝ^{d1^i×d2^i}. Ở đây, d1^i, d2^i là các chiều của lớp thứ i. Do đó, với một đầu vào x nào đó, đầu ra của mô hình được ký hiệu là M(x) được định nghĩa là

M(x) = ALφL−1(AL−1φL−2(. . .A2φ1(A1x))),

từ đó ánh xạ x đến M(x) trong ℝ^k. Ở đây, φi là hàm kích hoạt cho lớp thứ i có Độ trơn Lipschitz Li. Khi không mơ hồ, chúng ta sẽ sử dụng ký hiệu x0=x và x1=A1x và x2=A2φ1(A1x) và tiếp tục như vậy. Với bất kỳ phân phối dữ liệu D nào, mất mát biên dự kiến cho một biên γ > 0 được định nghĩa là

Rγ(M) = P(x,y)∼D[M(x)[y] ≤ γ + max_{j≠y} M(x)[j]].

Rủi ro quần thể R(M) được thu được như một trường hợp đặc biệt của Rγ(M) bằng cách đặt γ = 0. Mất mát biên thực nghiệm cho một bộ phân loại được định nghĩa là

R̂γ(M) = (1/|S|) ∑_{(x,y)∈S} I[M(x)[y] − max_{j≠y}(M(x)[j]) ≥ γ],

cho một biên γ > 0 nào đó nơi S là tập dữ liệu được cung cấp (khi γ = 0, điều này trở thành mất mát phân loại). Trực quan, R̂γ(M) ký hiệu số lượng phần tử mà bộ phân loại M dự đoán đúng y với biên lớn hơn hoặc bằng γ. Hơn nữa, chúng ta định nghĩa kích thước của S là |S| = n. Chúng ta sẽ ký hiệu M̂ = {Â1, . . . , ÂL} là mô hình đã nén thu được sau cắt tỉa M. Lỗi tổng quát hóa của mô hình đã cắt tỉa sau đó là R0(M̂). Hơn nữa, chúng ta sẽ định nghĩa ma trận khác biệt tại lớp l là Δl = Al − Âl. Bây giờ chúng ta đã định nghĩa chính thức ký hiệu, chúng ta sẽ tóm tắt ngắn gọn công cụ tổng quát hóa chính trong toàn bộ bài báo này.

### 3.2 Giới hạn Nén

Vì giới hạn nén là một trong những công cụ lý thuyết chính được sử dụng trong toàn bộ bài báo này, chúng ta sẽ tóm tắt ngắn gọn các giới hạn được trình bày trong Arora et al. [2018]. Cho rằng chúng ta đưa một mô hình f vào một thuật toán nén, tập hợp các đầu ra có thể là một tập hợp các mô hình GA,s nơi A là một tập hợp các cấu hình tham số có thể và s là một số thông tin khởi đầu được cung cấp cho thuật toán nén. Chúng ta sẽ gọi gA là một mô hình như vậy tương ứng với cấu hình tham số A trong A. Hơn nữa, nếu tồn tại một mô hình đã nén gA trong GA,s sao cho với bất kỳ đầu vào nào trong tập dữ liệu S, các đầu ra từ gA và f khác nhau nhiều nhất γ, chúng ta nói f có thể (γ,S) nén được. Chính thức, chúng ta làm rõ điều này trong định nghĩa sau.

**Định nghĩa 3.1.** Nếu f là một bộ phân loại và GA,s = {gA|A ∈ A} là một lớp các bộ phân loại với một tập hợp các cấu hình tham số có thể huấn luyện A và chuỗi cố định s. Chúng ta nói rằng f có thể (γ,S)-nén được thông qua GA,s nếu tồn tại A ∈ A sao cho với bất kỳ x ∈ S, chúng ta có với mọi y, |f(x)[y] − gA(x)[y]| ≤ γ.

Bây giờ chúng ta giới thiệu giới hạn nén của chúng ta. Lỗi tổng quát hóa của các mô hình đã nén theo kỳ vọng, nhiều nhất, là lỗi tổng quát hóa thực nghiệm của mô hình gốc nếu mô hình gốc có biên γ. Sử dụng các bất đẳng thức tập trung tiêu chuẩn, chúng ta áp dụng giới hạn này cho tất cả các kết quả mô hình đã cắt tỉa có thể. Giới hạn tổng quát hóa kết quả phụ thuộc vào cả biên và số lượng tham số trong mô hình đã cắt tỉa, như trong định lý sau.

**Định lý 3.1.** [Arora et al., 2018] Giả sử GA,s = {gA,s|A ∈ A} nơi A là một tập hợp q tham số mỗi tham số có thể có nhiều nhất r giá trị rời rạc và s là một chuỗi trợ giúp. Cho S là một tập huấn luyện với n mẫu. Nếu bộ phân loại đã huấn luyện f có thể (γ,S)-nén được thông qua GA,s, với chuỗi trợ giúp s, thì tồn tại A ∈ A với xác suất cao trên tập huấn luyện,

R0(gA) ≤ R̂γ(f) + O(√(q log r / n)).

Cần lưu ý rằng định lý trên cung cấp một giới hạn tổng quát hóa cho bộ phân loại đã nén gA, không phải cho bộ phân loại đã huấn luyện f. Do đó, hai phần của việc tạo ra giới hạn tổng quát hóa chặt chẽ hơn cho một thuật toán nén nhất định bao gồm việc giới hạn lỗi được tạo ra bởi nén, γ trong R̂γ(f), và số lượng tham số q sau nén. Chúng ta chứng minh rằng chúng ta có thể đạt được cả hai với MBP truyền thống.

### 3.3 Giả định Chuẩn bị

Việc phân tích các hiệu ứng của cắt tỉa là khó khăn mà không hiểu trước từ phân phối nào mà trọng số của một mô hình đã huấn luyện nằm. Đây là một câu hỏi phức tạp và mở nói chung. Tuy nhiên, Han et al. [2015] đã thực hiện quan sát thực nghiệm rằng trọng số thường nằm trong một phân phối Gaussian có mean bằng 0, như trong Hình 7 của Han et al. [2015]. Do đó chúng ta sẽ giả định điều này là đúng, rằng phân phối của trọng số tuân theo phân phối chuẩn với mean 0 và phương sai Ψ. Ở đây, chúng ta nêu các giả định chuẩn bị chính mà chúng ta sẽ sử dụng sau này.

**Giả định 3.1.** Với bất kỳ l ∈ [L], i, j ∈ [d1^l] × [d2^l], Al_{i,j} ∼ N(0,Ψ).

Giả định này phát biểu rằng mỗi nguyên tử trong một ma trận của mô hình đã học tuân theo gần đúng một phân phối Gaussian tập trung tại 0 với phương sai Ψ. Mặc dù là một giả định mạnh, điều này phổ biến. Thực tế, Qian và Klabjan [2021] giả định rằng trọng số tuân theo phân phối đều để phân tích trọng số của các mô hình đã cắt tỉa. Chúng ta giả định phân phối Gaussian vì điều này hợp lý hơn so với giả định phân phối đều. Bây giờ chúng ta có thể trình bày thuật toán MBP mà chúng ta sẽ phân tích trong toàn bộ bài báo này.

## 4 Thuật toán Cắt tỉa Dựa trên Độ lớn

Mặc dù tồn tại nhiều phiên bản của các thuật toán MBP, tất cả đều dựa trên khung tổng quát của việc loại bỏ các trọng số có độ lớn nhỏ để giảm số lượng tham số trong khi đảm bảo mô hình đã cắt tỉa không khác biệt lớn so với mô hình gốc. Chúng ta muốn chọn một thuật toán cắt tỉa dựa trên khung này thường được các nhà thực hành sử dụng trong khi đồng thời dễ phân tích. Chúng ta phát triển thuật toán để mô phỏng MBP ngẫu nhiên thường thấy trong các công trình như Han et al. [2015], Qian và Klabjan [2021]. Mặc dù thuật ngữ bên trong biến ngẫu nhiên Bernoulli được sử dụng như một chỉ báo cho cắt tỉa hơi khác so với tài liệu trước đây, đây là một thay đổi nhỏ cho phép chúng ta rời khỏi giả định phân phối đều từ Qian và Klabjan [2021] sang một giả định Gaussian thuận lợi hơn. Chúng ta chính thức trình bày thuật toán trong Thuật toán 1 dưới đây.

**Thuật toán 1: MBP**
Dữ liệu: {A1, . . . ,AL}, d
Kết quả: {Â1, . . . ,ÂL}
với l ∈ [L] thực hiện
    với i, j ∈ [d1^l] × [d2^l] và i ≠ j thực hiện
        X := Bernoulli(exp(−[Al_{i,j}]^2 / (dΨ)))
        Âl_{i,j} := 0 nếu X = 1 ngược lại Al_{i,j}
    kết thúc
kết thúc

**Nhận xét 4.1.** Chúng ta không cắt tỉa các phần tử đường chéo trong Thuật toán 1. Mặc dù không tiêu chuẩn, điều này cho phép sử dụng Phác thảo Ma trận sau này để có giới hạn tổng quát hóa tốt hơn. Tuy nhiên, trong Dasarathy et al. [2013], họ lưu ý sự cần thiết cho các phần tử đường chéo khác không là để dễ trình bày chứng minh, và Phác thảo Ma trận vẫn nên có thể với việc cắt tỉa các phần tử đường chéo.

Xác suất của nguyên tử bị nén tương đối nhỏ đối với các nguyên tử lớn hơn. Xác suất bị nén lớn hơn đối với các nguyên tử nhỏ hơn gần 0. Ở đây, d là một siêu tham số hữu ích để điều chỉnh cường độ nén của chúng ta. Sử dụng thuật toán nén này, mô hình đã cắt tỉa có khả năng duy trì các kết nối giữa các nguyên tử lớn hơn trong khi loại bỏ nhiều tham số nhỏ hơn. Để sử dụng giới hạn tổng quát hóa từ Phần 3.2, chúng ta cần chỉ ra rằng Thuật toán 1 tạo ra một mô hình đã cắt tỉa M̂ tạo ra các đầu ra tương tự như mô hình gốc M. Chúng ta cũng cần chỉ ra rằng số lượng tham số khác không trong các mô hình đã cắt tỉa là nhỏ. Chúng ta chứng minh điều này trong các phần dưới đây.

### 4.1 Chứng minh Lỗi

Chúng ta bắt đầu bằng cách giới hạn sự khác biệt giữa các đầu ra của các lớp tương ứng trong các mô hình đã cắt tỉa và gốc để chứng minh rằng sự khác biệt dự kiến giữa các mô hình đã cắt tỉa và gốc là nhỏ. Giả định tính chuẩn từ Giả định 3.1 làm cho điều này dễ tính toán hơn nhiều. Thực vậy, mỗi nguyên tử của ma trận khác biệt Δl = Âl − Al là một biến ngẫu nhiên độc lập và phân phối giống hệt nhau. Việc giới hạn chuẩn ℓ2 của ma trận như vậy chỉ dựa vào tài liệu phong phú nghiên cứu các chuẩn của ma trận ngẫu nhiên. Thực tế, từ Latala [2005], chúng ta chỉ cần một moment bậc hai và bậc bốn bị chặn của phân phối của mỗi nguyên tử. Để sử dụng giới hạn này, chúng ta chỉ cần chứng minh rằng ma trận khác biệt Δl và mô hình đã cắt tỉa thu được bằng sơ đồ nén Thuật toán 1 có các nguyên tử có moment bị chặn và có mean bằng không. Với thuật toán nén được chọn và phân phối trọng số sau huấn luyện sử dụng Giả định 3.1, ma trận Δl thực sự thỏa mãn các tính chất như vậy. Chúng ta chứng minh chúng trong bổ đề sau.

**Bổ đề 4.1.** Giá trị Kỳ vọng của bất kỳ phần tử Δl_{ij} nào của ma trận Δl = Âl − Al được cho bởi E(Δl_{ij}) = 0 với bất kỳ i, j ∈ [d1^l] × [d2^l], l ∈ [L]. Do đó, E(Δl) = 0 là một ma trận đầy số 0. Hơn nữa, chúng ta có E((Δl_{ij})^2) = d^{3/2}Ψ/(d+2)^{3/2}. Hơn nữa, moment bậc bốn của bất kỳ phần tử (Δl_{ij})^4 nào của Δl được cho bởi E((Δl_{ij})^4) = 3d^{5/2}Ψ^2/(d+2)^{5/2}.

Với các tính chất này của ma trận Δl, chúng ta có thể sử dụng các bất đẳng thức tập trung đơn giản để giới hạn lỗi tích lũy tại bất kỳ lớp nào giữa các mô hình đã cắt tỉa và gốc. Nếu chúng ta mô phỏng một đầu vào mẫu x đi qua mô hình đã cắt tỉa, chúng ta có thể chỉ ra rằng lỗi tích lũy qua toàn bộ mô hình được giới hạn thông qua quy nạp. Chúng ta chính thức trình bày bổ đề như vậy ở đây.

**Bổ đề 4.2.** Với bất kỳ lớp l ∈ [L] cho trước, chúng ta có với xác suất ít nhất 1 − 1/εl
‖(Âl − Al)‖2 ≤ εlΓl nơi Γl = C[√(d^{3/2}Ψ/(d+2)^{3/2})√(d1^l + d2^l) + (3d1^l d2^l d^{5/2}Ψ^2/(d+2)^{5/2})^{1/4}].

Ở đây, Âl được tạo ra bởi Thuật toán 1 và C là một hằng số dương phổ quát.

Bây giờ chúng ta có hiểu biết chính thức về cách cắt tỉa một lớp nhất định trong mô hình gốc ảnh hưởng đến kết quả của lớp đó. Chúng ta có thể trình bày giới hạn lỗi cho toàn bộ mạng thưa.

**Bổ đề 4.3.** Sự khác biệt giữa các đầu ra của mô hình đã cắt tỉa và mô hình gốc trên bất kỳ đầu vào x nào được giới hạn bởi, với xác suất ít nhất 1 − ∑_{l}ε_l^{−1},

‖x̂L − xL‖ ≤ e^{d0_1} ∏_{l=1}^L Ll‖Al‖2 ∑_{l=1}^L (εlΓl/‖Al‖2).

Giới hạn này phát biểu rằng lỗi tích lũy bởi mô hình đã cắt tỉa thu được bằng Thuật toán 1 chỉ phụ thuộc vào chiều của mô hình, hằng số Lipschitz của các hàm kích hoạt, phương sai của các phần tử và lỗi của nén. Giới hạn như vậy là trực quan vì lỗi được tích lũy lặp lại qua các lớp. Chúng ta cung cấp một phác thảo chứng minh ngắn gọn ở đây.

**Phác thảo Chứng minh.** Bằng Giới hạn Nhiễu từ Neyshabur et al. [2017], chúng ta có thể giới hạn lượng lỗi tích lũy qua mô hình sử dụng chuẩn của ma trận khác biệt từ Bổ đề 4.2. Chúng ta có thể tạo ra giới hạn chặt chẽ hơn bằng cách xem xét giá trị tối đa dự kiến của (Âl − Al)x với xác suất cao. Nếu d2^l < d1^l, chúng ta quan sát rằng ma trận Âl − Al có nhiều nhất d2^l giá trị kỳ dị khác không. Để biết thêm chi tiết, vui lòng xem Phụ lục C.

Tuy nhiên, giới hạn lỗi này nhiều hơn là cần thiết để chứng minh giới hạn tổng quát hóa mạnh. Chúng ta yêu cầu số lượng mô hình có thể sau huấn luyện và nén là hữu hạn để sử dụng giới hạn nén. Do đó, chúng ta cần áp dụng rời rạc hóa cho mô hình đã nén để đảm bảo rằng số lượng mô hình là hữu hạn. Tuy nhiên, điều này tương đối đơn giản với nền tảng lý thuyết đã được cung cấp.

### 4.2 Rời rạc hóa

Bây giờ chúng ta chỉ ra rằng lỗi dự đoán giữa một rời rạc hóa của mô hình đã cắt tỉa và mô hình gốc cũng được giới hạn. Phương pháp rời rạc hóa của chúng ta chỉ đơn giản là làm tròn mỗi giá trị trong lớp l đến bội số gần nhất của ρl. Chúng ta sẽ gọi mô hình đã cắt tỉa đã rời rạc hóa là M̃ nơi lớp thứ l sẽ được ký hiệu là Ãl. Chúng ta cung cấp bổ đề sau giới hạn chuẩn của sự khác biệt của các lớp giữa mô hình đã cắt tỉa và mô hình đã rời rạc hóa. Sử dụng trực quan này, chúng ta có thể chứng minh rằng lỗi được tạo ra bởi rời rạc hóa là nhỏ.

**Bổ đề 4.4.** Chuẩn của sự khác biệt giữa lớp đã cắt tỉa và lớp đã rời rạc hóa được giới hạn trên là ‖Ãl − Âl‖2 ≤ ρlJl nơi Jl là số lượng tham số khác không trong Âl (Jl được sử dụng cho ngắn gọn ở đây và sẽ được phân tích sau). Với xác suất ít nhất 1 − ∑_{l=1}^L ε_l^{−1}, cho rằng tham số ρl cho mỗi lớp được chọn sao cho ρl ≤ (1/(L‖Al‖2) − εlΓl)/Jl, chúng ta có rằng lỗi được tạo ra bởi cả rời rạc hóa và cắt tỉa được giới hạn bởi

‖xL − x̃L‖2 ≤ e^{d0_1} ∏_{l=1}^L Ll‖Al‖2 ∑_{l=1}^L (εlΓl + ρlJl)/‖Al‖2.

Bây giờ, chúng ta có một giới hạn lỗi đủ cho thuật toán MBP của chúng ta. Do đó, như bước tiếp theo, chúng ta tập trung vào việc giới hạn số lượng tham số mà mô hình đã nén của chúng ta sẽ có. Để làm điều này, chúng ta giới thiệu công cụ lý thuyết quan trọng tiếp theo: Phác thảo Ma trận.

## 5 Phác thảo Ma trận Thưa

Như thấy trong Định lý 3.1, lỗi tổng quát hóa phụ thuộc mạnh vào số lượng tham số. Chúng ta cố gắng đếm số lượng tham số hóa có thể của mô hình đã cắt tỉa M̂ có thể đạt được bằng cách kết hợp thuật toán học và Thuật toán 1. Trong phụ lục, chúng ta thảo luận một cách tiếp cận ngây thơ bằng cách đếm số lượng ma trận thưa có thể được tạo ra bởi sự kết hợp của một thuật toán học và Thuật toán 1. Điều này đạt được một giới hạn tổng quát hóa kém mong muốn, tạo ra động lực cho Phác thảo Ma trận. Bây giờ chúng ta giới thiệu các kiến thức cơ bản và thúc đẩy nhu cầu cho phác thảo ma trận.

### 5.1 Cơ bản về Phác thảo Ma trận

Ở đây chúng ta giới thiệu các khái niệm cơ bản của phác thảo ma trận. Cụ thể, chúng ta có thể biểu diễn một ma trận thưa X ∈ ℝ^{p1×p2} như Y ∈ ℝ^{m×m} nơi p1, p2 ≥ m. Ý tưởng của phác thảo ma trận là tạo ra một nhúng cho ma trận này như Y = AXB^T. Ở đây, các ma trận A ∈ {0,1}^{m×p1}, B ∈ {0,1}^{m×p2} được chọn trước khi phác thảo được thực hiện. Để khôi phục ma trận gốc, chúng ta giải bài toán tối ưu

min_{X̃∈ℝ^{p1×p2}} ‖X̃‖1 s.t. Y = AX̃B^T. (1)

Nếu bài toán từ Phương trình (1) có một minimum duy nhất và minimum duy nhất đó là X thực, chúng ta có thể nói rằng sơ đồ phác thảo này là không mất mát. Trong trường hợp như vậy, tất cả thông tin trong X được mã hóa trong Y. Với ánh xạ như vậy, chúng ta có thể sử dụng ánh xạ một-một này giữa Y và X để đếm số lượng tham số hóa của X sử dụng Y, có kích thước nhỏ hơn. Chúng ta sử dụng các tính chất từ tài liệu này để giúp phát triển và chứng minh các giới hạn lỗi tổng quát hóa được cải thiện.

Chúng ta khẳng định với phác thảo ma trận rằng chúng ta có thể biểu diễn không gian của các ma trận thưa lớn có chiều p với tập hợp các ma trận dày đặc nhỏ có chiều √(jp log p) nơi j là số lượng phần tử khác không tối đa trong bất kỳ hàng hoặc cột nào. Việc đếm số lượng tham số trong các ma trận dày đặc nhỏ hiệu quả hơn về mặt tham số so với việc đếm số lượng ma trận thưa lớn, do đó cung cấp một cách tránh sự bùng nổ tổ hợp. Chúng ta chính thức hóa điều này trong phần sau.

### 5.2 Trường hợp Thưa

Để áp dụng tài liệu phác thảo ma trận cho các ma trận thưa của chúng ta, chúng ta cần chứng minh một số tính chất của các ma trận thu được bằng sơ đồ nén Thuật toán 1. Chúng ta giới thiệu một cấu trúc như vậy gọi là độ thưa phân tán jr, jc, đảm bảo rằng phác thảo có thể được áp dụng cho các ma trận. Trực quan, tính chất như vậy đảm bảo rằng bất kỳ hàng hoặc cột nào của ma trận thưa của chúng ta không chứa quá nhiều phần tử khác không. Chúng ta định nghĩa chính thức trực quan như vậy ở đây.

**Định nghĩa 5.1.** Một ma trận có độ thưa phân tán jr, jc nếu nhiều nhất jr phần tử trong bất kỳ cột nào khác không, jc phần tử trong bất kỳ hàng nào khác không, và các phần tử đường chéo đều khác không.

Kiến thức chính khác là cách tạo ra A, B cho phác thảo ma trận thưa. Nếu người đọc quan tâm, chúng ta thảo luận cách tạo ra A và B cùng với một số trực quan đằng sau phác thảo ma trận trong Phụ lục E.1.

### 5.3 Giới hạn cho Phác thảo Ma trận Thưa

Từ Dasarathy et al. [2013], có thể chứng minh rằng phác thảo tập hợp các ma trận thưa phân tán jr, jc chỉ yêu cầu m nhỏ. Với một lựa chọn của m và thuật ngữ xác suất δ, ta có thể chỉ ra rằng nghiệm của Phương trình (1) khớp với giá trị chưa nén với xác suất cao. Điều này chủ yếu được chỉ ra bằng cách đầu tiên chứng minh rằng một nghiệm tồn tại và nghiệm tốt nhất cho A^{−1}YB^{−1} = X̃ là nghiệm duy nhất tối thiểu hóa chuẩn ℓ1 với xác suất cao.

**Định lý 5.1.** (Từ Định lý 1 của Dasarathy et al. [2013]) Cho p = max(d1^l, d2^l). Giả sử rằng A ∈ {0,1}^{m×d1^l}, B ∈ {0,1}^{m×d2^l} được rút độc lập và đều từ tập hợp δ-random bipartite. Sau đó, miễn là m = O(p√(max(jcd1^l, jrd2^l) log(p))) và δ = O(log(p)), tồn tại c ≥ 0 sao cho với bất kỳ ma trận thưa phân tán jr, jc cho trước X, phác thảo AXB thành X̃ dẫn đến một phác thảo duy nhất cho mỗi X. Phát biểu này đúng với xác suất 1 − p^{−c}.

**Nhận xét 5.1.** Phát biểu định lý cho Định lý 5.1 nêu rằng c ≥ 0. Tuy nhiên, trong chứng minh, họ chứng minh khẳng định mạnh hơn rằng c ≥ 2. Do đó, xác suất mà Định lý 5.1 đúng ít nhất là 1 − p^{−2}. Khi p tăng, xác suất mà định lý này đúng tiếp cận 1.

### 5.4 Lỗi Tổng quát hóa từ Phác thảo

Để sử dụng các công cụ lý thuyết trên của phác thảo ma trận, chúng ta phải chỉ ra rằng các đầu ra từ thuật toán nén Thuật toán 1 thỏa mãn các định nghĩa của độ thưa phân tán jr, jc. Khẳng định như vậy là trực quan và các tính chất tương tự đã được chỉ ra cho các ma trận ngẫu nhiên tuân theo các phân phối khác nhau. Cho rằng các ma trận đã huấn luyện của chúng ta thỏa mãn phân phối Gaussian, một hàng hoặc cột không có khả năng chứa nhiều phần tử khác không. Ở đây, chúng ta chứng minh trong bổ đề sau rằng mô hình đã cắt tỉa sử dụng Thuật toán 1 thỏa mãn điều kiện của độ thưa phân tán sử dụng Giả định 3.1.

**Bổ đề 5.1.** Với xác suất ít nhất 1 − 1/λl − (d1^l)^{−1/3} − (d2^l)^{−1/3}, chúng ta có rằng các đầu ra từ Thuật toán 1 có độ thưa phân tán jr, jc nơi max(jr, jc) ≤ 3λl max(d1^l, d2^l)χ và λl ∈ ℝ. Ở đây, χ = (√(d+2) − √d)/√(d+2).

Với việc định lượng trên về không gian của các ma trận thưa và giới hạn của lỗi mô hình, chúng ta có thể áp dụng giới hạn nén từ Arora et al. [2018]. Giới hạn nén như vậy trực quan phụ thuộc chủ yếu vào hai giá trị này.

**Định lý 5.2.** Với mỗi ma trận Âl, định nghĩa jl là max(jr, jc) nơi jr và jc là các hệ số độ thưa phân tán cho Âl. Hơn nữa, với mỗi ma trận Âl, định nghĩa pl = max(d1^l, d2^l). Sau đó chúng ta có

R0(gA) ≤ R̂γ(f) + O(√(∑l 3λlχd1^l d2^l log^2(pl) log(1/ρl) / n)).

Điều này đúng khi d được chọn sao cho γ ≥ e^{d0_1} ∏_{l=1}^L Ll‖Al‖2 ∑_{l=1}^L (εlΓl + ρlJl)/‖Al‖2 nơi Jl ≤ O(χd1^l d2^l). Khẳng định này đúng với xác suất ít nhất 1 − [∑_{l=1}^L λl^{−1} + εl^{−1} + p^{−c}].

Ở đây, χ phụ thuộc vào siêu tham số d của chúng ta. Thực vậy, giới hạn tổng quát hóa này cải thiện rất nhiều so với một ứng dụng tầm thường của giới hạn nén như trong Bổ đề D.1. Đáng chú ý, điều này loại bỏ bản chất tổ hợp của giới hạn ngây thơ trong Bổ đề D.1 với một chi phí nhỏ của √(log^2(pl)/n).

## 6 Tổng quát hóa của Vé số

Khung chứng minh lỗi tổng quát hóa như vậy cho cắt tỉa áp dụng cho nhiều hơn chỉ cắt tỉa Dựa trên độ lớn. Một cách tiếp cận cắt tỉa thú vị là chỉ đơn giản tạo ra một mô hình rất lớn G sao cho một mô hình mục tiêu nhỏ hơn M được ẩn bên trong G và có thể được tìm thấy bằng cắt tỉa. Công thức vé số này cho cắt tỉa đã thấy nhiều lợi ích thực nghiệm. Chính thức, chúng ta sẽ gọi vé số của chúng ta trong G là một mạng con trọng số Ĝ của G. Ĝ này là một phiên bản đã cắt tỉa của G gốc. Thực tế, Malach et al. [2020] chỉ ra rằng với một G đủ lớn, tồn tại với xác suất cao một cắt tỉa G̃ sao cho G̃ và hàm mục tiêu M khác nhau nhiều nhất ε. Hơn nữa, G̃ này sẽ có khoảng cùng số lượng tham số khác không như mạng mục tiêu gốc M. Điều này được trình bày chính thức trong Định lý 6.1.

**Định lý 6.1.** Cố định một số ε, δ ∈ (0,1). Cho M là một mạng mục tiêu có độ sâu L sao cho với mọi i ∈ [L] chúng ta có ‖Ai‖2 ≤ 1, ‖Ai‖_{max} ≤ 1/√{d1,i}. Hơn nữa, cho nM là chiều ẩn tối đa của M. Cho G là một mạng nơi mỗi chiều ẩn được giới hạn trên bởi poly(d1,0, nM, L, 1/ε, log(1/δ)) := DG và độ sâu 2L, nơi chúng ta khởi tạo Ai từ phân phối đều U([−1,1]). Sau đó, w.p ít nhất 1 − δ tồn tại một mạng con trọng số Ĝ của G sao cho:

sup_{x∈S} |Ĝ(x) − M(x)| ≤ ε.

Hơn nữa, số lượng trọng số hoạt động (khác không) trong Ĝ là O(d0,1DG + D^2_GL).

**Định lý 6.2.** Với xác suất ít nhất 1 − δ − LD^{−c}_G có lỗi tổng quát hóa của

R0(G̃) ≤ R̂_{ε+ερ}(G̃) + O(√([nMd0,1 log(DG)^2 + Ln^2_M log(DG)^2] log(1/ρ) / n)).

Ở đây, ερ là lỗi nhỏ được tạo ra bởi rời rạc hóa.

Ở đây, chúng ta có một giới hạn tổng quát hóa cho mô hình đã cắt tỉa. Một điều thú vị cần lưu ý là giới hạn này chỉ là một yếu tố nhỏ của log(DG) tệ hơn so với nếu chúng ta đã áp dụng giới hạn nén cho một mô hình có kích thước của hàm mục tiêu M. Theo hiểu biết của chúng ta, đây là một trong những phân tích tổng quát hóa đầu tiên cho vé số.

## 7 Phân tích Thực nghiệm

**Mã** Chúng tôi đã cung cấp mã của chúng tôi ở đây để tái tạo. Điều này dựa trên một fork của LaBonte [2023].

Chúng ta nghiên cứu giới hạn tổng quát hóa thu được bằng Thuật toán 1 cắt tỉa với một số giới hạn tổng quát hóa dựa trên chuẩn tiêu chuẩn nổi tiếng của Neyshabur et al. [2015], Bartlett et al. [2017], và Neyshabur et al. [2017] được sử dụng làm cơ sở. Các thí nghiệm của chúng ta so sánh lỗi tổng quát hóa thu được bởi các giới hạn này, giới hạn tổng quát hóa của thuật toán chúng ta (như được cung cấp trong 5.2), và lỗi tổng quát hóa thực của mô hình đã nén, đã huấn luyện. Chúng ta cũng cung cấp một thí nghiệm chứng minh cách giới hạn tổng quát hóa của chúng ta mở rộng khi tăng chiều ẩn của các mô hình.

Các mô hình của chúng ta là Mô hình Perceptron Đa lớp (MLP) với các lớp kích hoạt ReLU có 5 lớp. Chúng ta huấn luyện thuật toán với tốc độ học 0.02 với bộ tối ưu Adam [Kingma và Ba, 2014] trong 300 epoch. Chúng ta tiến hành thí nghiệm trên hai tập dữ liệu phân loại hình ảnh khác nhau: MNIST [LeCun và Cortes, 2010] và CIFAR10 [Krizhevsky et al.]. Chúng ta sử dụng MLP với chiều ẩn 784 để so sánh giới hạn với các giới hạn tổng quát hóa khác. Đối với thí nghiệm về mở rộng với kích thước mô hình, chúng ta thử nghiệm trên các chiều ẩn 500, 1000, 1500, 2000, và 2500 nơi độ sâu được giữ không đổi.

**Kết quả** Giới hạn của chúng ta tốt hơn vài bậc độ lớn so với các giới hạn tổng quát hóa tiên tiến cơ bản, như có thể suy ra từ Hình 1a và Hình 1b ở trên. Trong cả hai thí nghiệm, giới hạn gần nhất với chúng ta là của Bartlett et al. [2017], vẫn ít nhất 10^3 và 10^7 lần lớn hơn giới hạn của chúng ta trên tập dữ liệu MNIST và CIFAR10 tương ứng. Hơn nữa, giới hạn tổng quát hóa của chúng ta luôn tốt hơn Bartlett et al. [2017] khi chiều ẩn tăng Hình 1c. Điều này chứng minh rằng trên nhiều tập dữ liệu, giới hạn của chúng ta chặt chẽ hơn các giới hạn tổng quát hóa dựa trên chuẩn truyền thống và mở rộng tốt hơn với các chiều ẩn. Kết quả, mặc dù đáng chú ý, không đáng ngạc nhiên chủ yếu do việc sử dụng thuật toán cắt tỉa của chúng ta, đảm bảo lỗi do nén thấp, và sử dụng Phác thảo Ma trận Thưa, làm giảm đáng kể chiều của mô hình đã cắt tỉa, một yếu tố quan trọng khi tính toán giới hạn tổng quát hóa.

## 8 Kết luận

Bài báo này đã đạt được tiến bộ trong bài toán đưa ra giới hạn tổng quát hóa của các mạng nơ-ron quá tham số. Với thuật toán cắt tỉa hiệu quả và Phác thảo Ma trận Thưa, chúng ta đã thu được giới hạn cho các mô hình đã cắt tỉa tốt hơn đáng kể so với các giới hạn tổng quát hóa dựa trên chuẩn nổi tiếng và xác minh thực nghiệm hiệu quả của phương pháp này trên dữ liệu thực tế. Chúng ta hy vọng những kết quả này sẽ thúc đẩy nghiên cứu thêm trong học sâu để hiểu rõ hơn cách và tại sao các mô hình tổng quát hóa. Sẽ thú vị khi thấy liệu phác thảo ma trận có thể được sử dụng để chứng minh tổng quát hóa cho các loại cắt tỉa khác nhau, như coresets. Hơn nữa, cũng có thể có lợi khi thấy liệu phác thảo ma trận có thể được sử dụng cùng với giới hạn PAC-Bayes để mang lại giới hạn tổng quát hóa. Về vấn đề này, chúng ta đã mở rộng khung tổng quát của bài báo này để đưa ra giới hạn tổng quát hóa cho vé số trong Phần 6, một kết quả mà theo hiểu biết của chúng ta, là đầu tiên thuộc loại này. Một khả năng khác sẽ là khám phá cách các giới hạn tổng quát hóa khác nhau có thể được tạo ra cho các phân phối dữ liệu khác nhau từ các thuật toán huấn luyện khác nhau.

**Hạn chế** Thuật toán cắt tỉa dựa trên độ lớn của chúng ta không cắt tỉa đường chéo của ma trận, điều này không tiêu chuẩn. Hơn nữa, sau huấn luyện, chúng ta giả định rằng mỗi nguyên tử thuộc về một phân phối Gaussian i.i.d, điều này có thể không luôn đúng. Ngoài ra, tương tự như các giới hạn tiêu chuẩn, giới hạn tổng quát hóa của chúng ta vẫn trống rỗng, không giải thích đầy đủ tổng quát hóa của các mô hình.
