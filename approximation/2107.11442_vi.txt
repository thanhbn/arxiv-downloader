# 2107.11442.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2107.11442.pdf
# Kích thước tệp: 1409520 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Nén Mạng Nơ-ron: Hướng tới
Xác định Phân tách Tối ưu theo Từng Lớp
Lucas Liebenwein
MIT CSAIL
lucas@csail.mit.eduAlaa Maalouf
University of Haifa
alaamalouf12@gmail.com
Oren Gal
University of Haifa
orengal@alumni.technion.ac.ilDan Feldman
University of Haifa
dannyf.post@gmail.comDaniela Rus
MIT CSAIL
rus@csail.mit.edu
Tóm tắt
Chúng tôi trình bày một khung nén toàn cục mới cho mạng nơ-ron sâu tự động
phân tích từng lớp để xác định tỷ lệ nén tối ưu cho mỗi lớp, đồng thời đạt được
mức nén tổng thể mong muốn. Thuật toán của chúng tôi dựa trên ý tưởng nén
mỗi lớp tích chập (hoặc kết nối đầy đủ) bằng cách chia các kênh thành nhiều
nhóm và phân tách từng nhóm thông qua phân tách hạng thấp. Cốt lõi của thuật
toán chúng tôi là việc suy ra các ràng buộc lỗi theo từng lớp từ định lý Eckart–Young–Mirsky.
Sau đó chúng tôi tận dụng các ràng buộc này để đặt vấn đề nén như một bài toán
tối ưu hóa trong đó chúng tôi muốn cực tiểu hóa lỗi nén tối đa trên các lớp và
đề xuất một thuật toán hiệu quả để đạt được giải pháp. Các thí nghiệm của chúng
tôi cho thấy phương pháp của chúng tôi vượt trội so với các phương pháp nén
hạng thấp hiện có trên nhiều loại mạng và bộ dữ liệu. Chúng tôi tin rằng kết quả
của chúng tôi mở ra những hướng nghiên cứu mới cho việc đánh đổi hiệu suất-kích
thước toàn cục của các mạng nơ-ron hiện đại.

1 Giới thiệu
20.0% 40.0% 60.0% 80.0%
Tỷ lệ Nén (Tham số)-3.0%-2.0%-1.0%0.0%+1.0% Delta Độ chính xác Kiểm tra Top1
resnet18, ImageNet
20.0% 40.0% 60.0% 80.0%
Tham số Bị cắt-80.0%-60.0%-40.0%-20.0%0.0%Delta Độ chính xác Top1
resnet20, CIFAR10
ALDS (Của chúng tôi)
PCA
SVD-Energy
SVD
L-Rank
FT
PFP
Hình 1: ALDS, Automatic Layer-wise Decomposition Selector, có thể nén lên đến
60% tham số trên ResNet18 (ImageNet), nhiều gấp 3 lần so với các phương pháp
cơ sở. Kết quả chi tiết được mô tả trong Phần 3.

Nén mạng nơ-ron bao gồm việc lấy một mô hình hiện có và giảm dấu chân
tính toán và bộ nhớ của nó để cho phép triển khai các mạng quy mô lớn
trong môi trường hạn chế tài nguyên. Ngoài hiệu quả thời gian suy luận,
nén có thể mang lại những hiểu biết mới về thiết kế (Liu et al., 2019b),
huấn luyện (Liebenwein et al., 2021a,b), và tính chất lý thuyết (Arora et al., 2018)
của mạng nơ-ron.

Trong số các kỹ thuật nén hiện có – bao gồm lượng tử hóa (Wu et al., 2016),
chưng cất (Hinton et al., 2015), và cắt tỉa (Han et al., 2015) – nén hạng thấp
nhằm phân tách tensor trọng số của một lớp thành một bộ các tensor hạng
thấp nhỏ hơn. Các kỹ thuật nén như vậy có thể dựa trên tài liệu phong phú
về phân tách hạng thấp và nhiều ứng dụng bên ngoài học sâu như giảm chiều
(Laparra et al., 2015) hoặc phân cụm phổ (Peng et al., 2015). Hơn nữa, nén
hạng thấp có thể dễ dàng được triển khai trong bất kỳ khung học máy nào bằng
cách thay thế lớp hiện có bằng một tập hợp các lớp nhỏ hơn mà không cần,
ví dụ, hỗ trợ đại số tuyến tính thưa thớt.

Trong học sâu, chúng ta gặp phải hai thách thức liên quan nhưng khác biệt khi
áp dụng nén hạng thấp. Một mặt, mỗi lớp nên được phân tách hiệu quả ("bước
cục bộ") và, mặt khác, chúng ta cần cân bằng lượng nén trong mỗi lớp để đạt
được tỷ lệ nén tổng thể mong muốn với mất mát tối thiểu trong khả năng dự đoán
của mạng ("bước toàn cục"). Trong khi "bước cục bộ", tức là thiết kế phương
pháp phân tách hiệu quả nhất theo từng lớp, truyền thống đã nhận được nhiều sự
chú ý (Denton et al., 2014; Garipov et al., 2016; Jaderberg et al., 2014; Kim et al.,
2015b; Lebedev et al., 2015; Novikov et al., 2015), "bước toàn cục" chỉ gần đây
mới trở thành trọng tâm của nghiên cứu, ví dụ, xem các công trình gần đây của
Alvarez và Salzmann (2017); Idelbayev và Carreira-Perpinán (2020); Xu et al. (2020).

Trong bài báo này, chúng tôi đặt mục tiêu thiết kế một khung đồng thời tính đến
cả bước cục bộ và toàn cục. Giải pháp được đề xuất của chúng tôi, được gọi là
Automatic Layer-wise Decomposition Selector (ALDS), giải quyết thách thức này
bằng cách tối ưu hóa lặp đi lặp lại cho phương pháp phân tách của mỗi lớp (bước
cục bộ) và chính phép nén hạng thấp trong khi tính đến lỗi tối đa phát sinh trên
các lớp (bước toàn cục). Trong Hình 1, chúng tôi cho thấy ALDS vượt trội so với
các phương pháp hiện có trên điểm chuẩn ResNet18 (ImageNet) phổ biến (60%
nén so với 20% cho các phương pháp cơ sở).

Phân tách hiệu quả theo từng lớp. Khung của chúng tôi dựa trên phân tách dựa
trên SVD đơn giản của mỗi lớp. Lấy cảm hứng từ Denton et al. (2014); Idelbayev
và Carreira-Perpinán (2020); Jaderberg et al. (2014) và những người khác, chúng
tôi phân tách mỗi lớp bằng cách đầu tiên gấp tensor trọng số thành một ma trận
trước khi áp dụng SVD và mã hóa cặp ma trận kết quả thành hai lớp riêng biệt.

Cải tiến phân tách thông qua nhiều tập con. Một sự tổng quát hóa tự nhiên của
các phương pháp phân tách hạng thấp bao gồm việc chia ma trận thành nhiều tập
con (không gian con) trước khi nén từng tập con riêng lẻ. Trong bối cảnh học sâu,
điều này đã được điều tra trước đây cho các lớp riêng lẻ (Denton et al., 2014),
bao gồm các lớp nhúng (Chen et al., 2018; Maalouf et al., 2021). Chúng tôi đưa
ý tưởng này đi xa hơn và tích hợp nó vào phương pháp phân tách theo từng lớp
của chúng tôi như siêu tham số bổ sung về số lượng tập con. Do đó, bước cục bộ
của chúng tôi, tức là phân tách theo từng lớp, bao gồm việc chọn số lượng tập con
(k`) cho mỗi lớp và hạng (j`).

Hướng tới giải pháp toàn cục cho nén hạng thấp. Chúng ta có thể mô tả giải pháp
tối ưu cho nén hạng thấp như tập hợp các siêu tham số (số lượng không gian con
k` và hạng j` cho mỗi lớp trong trường hợp của chúng tôi) cực tiểu hóa sự sụt
giảm độ chính xác của mạng nén. Trong khi việc tìm giải pháp tối ưu toàn cục là
NP-đầy đủ, chúng tôi đề xuất ALDS như một lựa chọn thay thế có thể giải quyết
hiệu quả cho phép chúng tôi tìm kiếm giải pháp tối ưu cục bộ về lỗi tương đối
tối đa phát sinh trên các lớp. Để đạt được điều này, chúng tôi suy ra các ràng buộc
chuẩn phổ dựa trên Định lý Eckhart-Young-Mirsky cho phương pháp phân tách
theo từng lớp của chúng tôi để mô tả sự đánh đổi giữa nén lớp và lỗi phát sinh.
Tận dụng các ràng buộc của chúng tôi, sau đó chúng tôi có thể tối ưu hóa hiệu
quả trên tập hợp các phân tách có thể cho mỗi lớp. Tổng quan về ALDS được
hiển thị trong Hình 2.

2 Phương pháp

Trong phần này, chúng tôi giới thiệu khung nén của chúng tôi bao gồm phương
pháp phân tách theo từng lớp (Phần 2.1), cơ chế lựa chọn toàn cục để đồng thời
nén tất cả các lớp của mạng (Phần 2.2), và quy trình tối ưu hóa (ALDS) để giải
quyết vấn đề lựa chọn (Phần 2.3).

2

--- TRANG 2 ---
Hình 2: Tổng quan ALDS. Khung bao gồm một bước toàn cục và cục bộ, xem Phần 2.

tral clustering (Peng et al., 2015). Hơn nữa, nén hạng thấp có thể dễ dàng được triển khai trong
bất kỳ khung học máy nào bằng cách thay thế lớp hiện có bằng một tập hợp các lớp nhỏ hơn mà
không cần, ví dụ, hỗ trợ đại số tuyến tính thưa thớt.

Trong học sâu, chúng ta gặp phải hai thách thức liên quan nhưng khác biệt khi áp dụng nén hạng
thấp. Một mặt, mỗi lớp nên được phân tách hiệu quả ("bước cục bộ") và, mặt khác, chúng ta cần
cân bằng lượng nén trong mỗi lớp để đạt được tỷ lệ nén tổng thể mong muốn với mất mát tối thiểu
trong khả năng dự đoán của mạng ("bước toàn cục"). Trong khi "bước cục bộ", tức là thiết kế
phương pháp phân tách hiệu quả nhất theo từng lớp, truyền thống đã nhận được nhiều sự chú ý
(Denton et al., 2014; Garipov et al., 2016; Jaderberg et al., 2014; Kim et al., 2015b; Lebedev et al.,
2015; Novikov et al., 2015), "bước toàn cục" chỉ gần đây mới trở thành trọng tâm của nghiên cứu,
ví dụ, xem các công trình gần đây của Alvarez và Salzmann (2017); Idelbayev và Carreira-Perpinán
(2020); Xu et al. (2020).

Trong bài báo này, chúng tôi đặt mục tiêu thiết kế một khung đồng thời tính đến cả bước cục bộ
và toàn cục. Giải pháp được đề xuất của chúng tôi, được gọi là Automatic Layer-wise Decomposition
Selector (ALDS), giải quyết thách thức này bằng cách tối ưu hóa lặp đi lặp lại cho phương pháp
phân tách của mỗi lớp (bước cục bộ) và chính phép nén hạng thấp trong khi tính đến lỗi tối đa
phát sinh trên các lớp (bước toàn cục). Trong Hình 1, chúng tôi cho thấy ALDS vượt trội so với
các phương pháp hiện có trên điểm chuẩn ResNet18 (ImageNet) phổ biến (60% nén so với 20%
cho các phương pháp cơ sở).

Phân tách hiệu quả theo từng lớp. Khung của chúng tôi dựa trên phân tách dựa trên SVD đơn
giản của mỗi lớp. Lấy cảm hứng từ Denton et al. (2014); Idelbayev và Carreira-Perpinán (2020);
Jaderberg et al. (2014) và những người khác, chúng tôi phân tách mỗi lớp bằng cách đầu tiên gấp
tensor trọng số thành một ma trận trước khi áp dụng SVD và mã hóa cặp ma trận kết quả thành
hai lớp riêng biệt.

Cải tiến phân tách thông qua nhiều tập con. Một sự tổng quát hóa tự nhiên của các phương pháp
phân tách hạng thấp bao gồm việc chia ma trận thành nhiều tập con (không gian con) trước khi
nén từng tập con riêng lẻ. Trong bối cảnh học sâu, điều này đã được điều tra trước đây cho các
lớp riêng lẻ (Denton et al., 2014), bao gồm các lớp nhúng (Chen et al., 2018; Maalouf et al., 2021).
Chúng tôi đưa ý tưởng này đi xa hơn và tích hợp nó vào phương pháp phân tách theo từng lớp
của chúng tôi như siêu tham số bổ sung về số lượng tập con. Do đó, bước cục bộ của chúng tôi,
tức là phân tách theo từng lớp, bao gồm việc chọn số lượng tập con (k`) cho mỗi lớp và hạng (j`).

Hướng tới giải pháp toàn cục cho nén hạng thấp. Chúng ta có thể mô tả giải pháp tối ưu cho
nén hạng thấp như tập hợp các siêu tham số (số lượng không gian con k` và hạng j` cho mỗi lớp
trong trường hợp của chúng tôi) cực tiểu hóa sự sụt giảm độ chính xác của mạng nén. Trong khi
việc tìm giải pháp tối ưu toàn cục là NP-đầy đủ, chúng tôi đề xuất ALDS như một lựa chọn thay
thế có thể giải quyết hiệu quả cho phép chúng tôi tìm kiếm giải pháp tối ưu cục bộ về lỗi tương
đối tối đa phát sinh trên các lớp. Để đạt được điều này, chúng tôi suy ra các ràng buộc chuẩn phổ
dựa trên Định lý Eckhart-Young-Mirsky cho phương pháp phân tách theo từng lớp của chúng tôi
để mô tả sự đánh đổi giữa nén lớp và lỗi phát sinh. Tận dụng các ràng buộc của chúng tôi, sau đó
chúng tôi có thể tối ưu hóa hiệu quả trên tập hợp các phân tách có thể cho mỗi lớp. Tổng quan về
ALDS được hiển thị trong Hình 2.

2 Phương pháp

Trong phần này, chúng tôi giới thiệu khung nén của chúng tôi bao gồm phương pháp phân tách
theo từng lớp (Phần 2.1), cơ chế lựa chọn toàn cục để đồng thời nén tất cả các lớp của mạng
(Phần 2.2), và quy trình tối ưu hóa (ALDS) để giải quyết vấn đề lựa chọn (Phần 2.3).

2

--- TRANG 3 ---
Hình 3: Trái: tích chập 2D. phải: phân tách được sử dụng cho ALDS. Đối với tích chập f×c×1×2
với f bộ lọc, c kênh, và kernel 1×2, phân tách mỗi lớp của chúng tôi bao gồm: (1) k tích chập
song song j×c/k×1×2; (2) một tích chập duy nhất f×kj×1×1 được áp dụng trên đầu ra (xếp chồng)
của lớp đầu tiên.

2.1 Nén Lớp Cục bộ

Chúng tôi trình bày chi tiết sơ đồ nén hạng thấp của chúng tôi cho các lớp tích chập dưới đây và
lưu ý rằng nó dễ dàng áp dụng cho các lớp kết nối đầy đủ như một trường hợp đặc biệt của tích
chập với kernel 1×1.

Nén tích chập thông qua SVD. Cho một lớp tích chập của f bộ lọc, c kênh, và kernel 1×2, chúng
tôi ký hiệu tensor trọng số tương ứng bằng W ∈ Rf×c×1×2. Theo Denton et al. (2014); Idelbayev
và Carreira-Perpinán (2020); Wen et al. (2017) và những người khác, sau đó chúng tôi có thể giải
thích lớp như một lớp tuyến tính có hình dạng f×c×1×2 và xấp xỉ hạng j tương ứng như hai lớp
tuyến tính liên tiếp có hình dạng f×j và j×c×1×2. Ánh xạ trở lại tích chập, điều này tương ứng
với tích chập j×c×1×2 theo sau bởi tích chập f×j×1×1.

Nhiều không gian con. Theo trực giác được phác thảo trong Phần 1, chúng tôi đề xuất phân cụm
các cột của ma trận trọng số của lớp thành k ≥ 2 không gian con riêng biệt trước khi áp dụng SVD
cho mỗi tập con. Để đạt được điều này, chúng tôi có thể xem xét bất kỳ phương pháp phân cụm
nào, chẳng hạn như k-means hoặc phân cụm phép chiếu (Chen et al., 2018; Maalouf et al., 2021).
Tuy nhiên, các phương pháp như vậy yêu cầu các thuật toán xấp xỉ đắt đỏ có thể hạn chế khả năng
tích hợp chúng vào khung nén dựa trên tối ưu hóa như được phác thảo trong Phần 2.2. Ngoài ra,
phân cụm tùy ý có thể yêu cầu xáo trộn lại các tensor đầu vào có thể dẫn đến sự chậm trễ đáng
kể trong quá trình suy luận. Thay vào đó, chúng tôi đã chọn một phương pháp phân cụm đơn giản,
đó là chia cắt kênh, trong đó chúng tôi chỉ đơn giản chia c kênh đầu vào của lớp thành k tập con,
mỗi tập con chứa tối đa ⌈c/k⌉ kênh đầu vào liên tiếp. Không giống như các phương pháp khác,
chia cắt kênh có thể triển khai hiệu quả, ví dụ, như tích chập nhóm trong PyTorch (Paszke et al.,
2017) và đảm bảo tăng tốc thực tế sau khi nén mạng.

Tổng quan về phân tách mỗi lớp. Tóm lại, cho các số nguyên j, k ≥ 1 và tensor 4D W ∈ Rf×c×1×2
đại diện cho một tích chập, phương pháp nén mỗi lớp tiến hành như sau:

1. PHÂN CHIA các kênh của lớp tích chập thành k tập con, trong đó mỗi tập con có tối đa ⌈c/k⌉
kênh liên tiếp, dẫn đến k tensor tích chập {Wi}ki=1 trong đó Wi ∈ Rf×ci×1×2, và ∑ki=1 ci = c.

2. PHÂN TÁCH mỗi tensor Wi, i ∈ [k], bằng cách xây dựng ma trận trọng số tương ứng Wi ∈
Rf×ci1×2, c.f. Hình 3, tính toán xấp xỉ hạng j của nó, và phân tích nó thành một cặp ma trận
nhỏ hơn Ui có f hàng và j cột và Vi có j hàng và ci1×2 cột.

3. THAY THẾ lớp gốc trong mạng bằng 2 lớp. Lớp đầu tiên bao gồm k tích chập song song, trong
đó lớp song song thứ i, i ∈ [k], được mô tả bởi tensor Vi ∈ Rj×ci×1×2 có thể được xây dựng
từ ma trận Vi (j bộ lọc, ci kênh, kernel 1×2). Lớp thứ hai được xây dựng bằng cách định hình
lại mỗi ma trận Ui, i ∈ [k], để có được tensor Ui ∈ Rf×j×1×1, và sau đó xếp chồng kênh tất
cả k tensor U1, ..., Uk để có được một tensor duy nhất có hình dạng f×kj×1×1.

Lớp phân tách được mô tả trong Hình 3. Cặp lớp kết quả có j×c×1×2 và j×f×k tham số, tương
ứng, điều này ngụ ý việc giảm tham số từ f×c×1×2 xuống j(f×k + c×1×2).

2.2 Nén Mạng Toàn cục

Trong phần trước, chúng tôi đã giới thiệu sơ đồ nén lớp của chúng tôi. Chúng tôi lưu ý rằng trong
thực tế, chúng tôi thường muốn nén toàn bộ mạng gồm L lớp lên đến mức giảm tương đối được
chỉ định trước trong tham số ("tỷ lệ nén" hoặc CR). Tuy nhiên, thường không rõ ràng mỗi lớp
` ∈ [L] nên được nén bao nhiêu để đạt được CR mong muốn trong khi phát sinh mức tăng tối thiểu
trong mất mát. Thật không may, bài toán tối ưu hóa này là NP-đầy đủ vì chúng ta sẽ phải kiểm tra
mọi kết hợp nén lớp dẫn đến CR mong muốn để nén tối ưu từng lớp. Mặt khác, các phương pháp
heuristic đơn giản, ví dụ, tỷ lệ nén không đổi cho mỗi lớp, có thể dẫn đến kết quả không tối ưu,
xem Phần 3. Để đạt được điều này, chúng tôi đề xuất một khung nén toàn cục có thể giải quyết
hiệu quả dựa trên việc cực tiểu hóa lỗi tương đối tối đa phát sinh trên các lớp. Chúng tôi mô tả
chi tiết hơn từng thành phần của quy trình tối ưu hóa của chúng tôi dưới đây.

Lỗi tương đối theo từng lớp như đại diện cho mất mát tổng thể. Vì chi phí thực sự (mất mát bổ
sung phát sinh sau nén) sẽ dẫn đến bài toán NP-đầy đủ, chúng tôi thay thế chi phí thực sự bằng
một đại diện hiệu quả hơn. Cụ thể, chúng tôi xem xét lỗi tương đối tối đa ε := max`∈[L] ε` trên
các lớp, trong đó ε` biểu thị lỗi tương đối tối đa lý thuyết trong lớp thứ ` như được mô tả trong
Định lý 1 dưới đây. Chúng tôi chọn cực tiểu hóa chi phí cụ thể này vì: (i) việc cực tiểu hóa lỗi
tương đối tối đa đảm bảo rằng không có lớp nào phát sinh lỗi lớn một cách bất hợp lý có thể bị
lan truyền hoặc khuếch đại; (ii) việc dựa vào khái niệm lỗi tương đối thay vì tuyệt đối được ưa
thích vì việc chia tỷ lệ giữa các lớp có thể thay đổi tùy ý, ví dụ, do chuẩn hóa batch, và do đó
quy mô tuyệt đối của lỗi lớp có thể không chỉ ra sự tăng mất mát; và (iii) lỗi tương đối mỗi lớp
đã được chứng minh là có liên kết nội tại với lỗi nén lý thuyết, ví dụ, xem các công trình của
Arora et al. (2018) và Baykal et al. (2019a) do đó đại diện cho một đại diện tự nhiên cho chi phí.

Định nghĩa lỗi tương đối mỗi lớp. Gọi W` ∈ Rf`×c`×`1×`2 và W̄` ∈ Rf`×c`×`1×`2 biểu thị
tensor trọng số và ma trận gấp tương ứng của lớp `, tương ứng. Lỗi tương đối mỗi lớp ε` được
định nghĩa như sự khác biệt tương đối trong chuẩn toán tử giữa ma trận Ŵ` (tương ứng với tensor
trọng số nén Ŵ`) và ma trận trọng số gốc W̄` trong lớp `, tức là,

ε` := ‖Ŵ` - W̄`‖/‖W̄`‖.                                                    (1)

Lưu ý rằng trong khi trong thực tế phương pháp của chúng tôi phân tách lớp gốc thành một tập
hợp các lớp riêng biệt (xem Phần 2.1), cho mục đích suy ra lỗi kết quả, chúng tôi tái kết hợp các
lớp nén thành toán tử ma trận tổng thể Ŵ`, tức là, Ŵ` = [U`1V`1 ... U`k`V`k`], trong đó U`iV`i là
phân tích của cụm thứ i (tập hợp các cột) trong lớp thứ `, với mọi ` ∈ [L] và i ∈ [k`], xem tài
liệu bổ sung để biết thêm chi tiết. Chúng tôi lưu ý rằng chuẩn toán tử ‖·‖ cho một lớp tích chập
do đó biểu thị lỗi tương đối tối đa phát sinh cho một patch đầu ra riêng lẻ ("pixel") trên tất cả
các kênh đầu ra.

Suy ra ràng buộc lỗi tương đối. Bây giờ chúng tôi suy ra một ràng buộc lỗi cho phép chúng tôi
mô tả lỗi tương đối mỗi lớp theo các siêu tham số nén j` và k`, tức là, ε` = ε`(k`, j`). Điều này
sẽ hữu ích sau này vì chúng ta phải truy vấn lỗi tương đối liên tục trong quy trình tối ưu hóa của
chúng tôi. Ràng buộc lỗi được mô tả như sau.

Định lý 1. Cho ma trận lớp W̄` và xấp xỉ hạng thấp tương ứng Ŵ`, lỗi tương đối ε` := ‖Ŵ` - W̄`‖/‖W̄`‖
bị ràng buộc bởi

ε` ≤ √k max i∈[k] σi,j+1/σ1,                                               (2)

trong đó σi,j+1 là giá trị kỳ dị lớn thứ j+1 của ma trận W̄`i, với mọi i ∈ [k], và σ1 = ‖W̄`‖ là
giá trị kỳ dị lớn nhất của W̄`.

Chứng minh. Đầu tiên, chúng tôi nhớ lại các ma trận W̄`1, ..., W̄`k và chúng tôi ký hiệu phân tích
SVD cho mỗi ma trận bằng: W̄`i = Ũ`iΣ̃`iṼ`i. Bây giờ, quan sát rằng với mọi i ∈ [k], ma trận Ŵ`i
là xấp xỉ hạng j của W̄`i. Do đó, phân tích SVD của Ŵ`i có thể được viết như Ŵ`i = Ũ`iΣ̂`iṼ`T
i , trong
đó Σ̂`i ∈ Rf×d là ma trận đường chéo sao cho j mục đường chéo đầu tiên bằng j mục đầu tiên trên
đường chéo của Σ̃`i, và phần còn lại là số không. Do đó,

W̄` - Ŵ` = [W̄`1 - Ŵ`1, ..., W̄`k - Ŵ`k] = [Ũ`1(Σ̃`1 - Σ̂`1)Ṽ`1, ..., Ũ`k(Σ̃`k - Σ̂`k)Ṽ`k]
= [Ũ`1 ... Ũ`k] diag((Σ̃`1 - Σ̂`1)Ṽ`1, ..., (Σ̃`k - Σ̂`k)Ṽ`k).          (3)

Bởi (3) và bởi bất đẳng thức tam giác, chúng ta có

‖W̄` - Ŵ`‖ ≤ ‖[Ũ`1 ... Ũ`k]‖ ‖diag((Σ̃`1 - Σ̂`1)Ṽ`1, ..., (Σ̃`k - Σ̂`k)Ṽ`k)‖.    (4)

Bây giờ, chúng ta quan sát rằng

‖[Ũ`1 ... Ũ`k]‖2 = ‖[Ũ`1 ... Ũ`k][Ũ`1 ... Ũ`k]T‖ = ‖diag(Ik, ..., Ik)‖ = √k.    (5)

Cuối cùng, chúng tôi chỉ ra rằng

‖diag((Σ̃`1 - Σ̂`1)Ṽ`1, ..., (Σ̃`k - Σ̂`k)Ṽ`k)‖ = max i∈[k] ‖(Σ̃`i - Σ̂`i)Ṽ`i‖    (6)
= max i∈[k] ‖(Σ̃`i - Σ̂`i)‖ = max i∈[k] σi,j+1,                           (7)

trong đó đẳng thức thứ hai đúng vì các cột của V là trực giao và đẳng thức cuối cùng đúng theo
Định lý Eckhart-Young-Mirsky (Định lý 2.4.8 của Golub và Van Loan (2013)). Đặt (7) và (5)
vào (4) kết thúc chứng minh.

Kích thước mạng kết quả. Gọi Θ = {W`}L`=1 biểu thị tập hợp trọng số cho L lớp và lưu ý rằng
số lượng tham số trong lớp ` được cho bởi |W`| = f`c``1`2 và |Θ| = ∑`∈[L] |W`|. Hơn nữa,
lưu ý rằng |Ŵ`| = j`(k`f` + c``1`2) nếu phân tách, Θ̂ = {Ŵ`}L`=1, và |Θ̂| = ∑`∈[L] |Ŵ`|.
Tỷ lệ nén tổng thể do đó được cho bởi 1 - |Θ̂|/|Θ| trong đó chúng tôi bỏ qua các tham số khác
để dễ trình bày. Quan sát rằng ngân sách lớp |Ŵ`| hoàn toàn được xác định bởi k`, j` giống như
ràng buộc lỗi.

Nén Mạng Toàn cục. Tổng hợp mọi thứ lại với nhau, chúng tôi có được công thức sau cho ngân
sách tối ưu mỗi lớp:

εopt = min {j`,k`}L`=1 max `∈[L] ε`(k`, j`)                                  (8)

với điều kiện 1 - |Θ̂(k1, j1, ..., kL, jL)|/|Θ| ≥ CR,

trong đó CR biểu thị tỷ lệ nén tổng thể mong muốn. Do đó, việc phân bổ tối ưu ngân sách mỗi
lớp bao gồm việc tìm số lượng không gian con k` và hạng j` tối ưu cho mỗi lớp bị ràng buộc bởi
tỷ lệ nén tổng thể mong muốn CR.

2.3 Automatic Layer-wise Decomposition Selector (ALDS)

Chúng tôi đề xuất giải quyết (8) bằng cách tối ưu hóa lặp đi lặp lại k1, ..., kL và j1, ..., jL cho
đến khi hội tụ tương tự như thuật toán giống EM như được hiển thị trong Thuật toán 1 và Hình 2.

Cụ thể, cho một tập hợp trọng số và tỷ lệ nén mong muốn CR, đầu tiên chúng tôi khởi tạo ngẫu
nhiên số lượng không gian con k1, ..., kL cho mỗi lớp (Dòng 2). Dựa trên các giá trị đã cho cho
mỗi k`, sau đó chúng tôi giải quyết cho các hạng tối ưu j1, ..., jL sao cho tỷ lệ nén tổng thể được
thỏa mãn (Dòng 4). Lưu ý rằng lỗi tối đa ε được cực tiểu hóa nếu tất cả các lỗi đều bằng nhau.
Do đó, việc giải quyết cho các hạng trong Dòng 4 bao gồm việc đoán một giá trị cho ε, tính toán
kích thước mạng kết quả, và lặp lại quy trình cho đến khi CR mong muốn được thỏa mãn, ví dụ
thông qua tìm kiếm nhị phân.

Thuật toán 1 ALDS(Θ, CR, nseed)
Đầu vào: Θ: tham số mạng; CR: tỷ lệ nén tổng thể; nseed: số lượng hạt giống ngẫu nhiên để khởi tạo
Đầu ra: k1, ..., kL: số lượng không gian con cho mỗi lớp; j1, ..., jL: hạng mong muốn mỗi không gian con cho mỗi lớp

1: for i ∈ [nseed] do
2:     k1, ..., kL ← RANDOM_INIT()
3:     while not converged do
4:         j1, ..., jL ← OPTIMAL_RANKS(CR, k1, ..., kL). Bước toàn cục: chọn sao cho ε1 = ... = εL
5:         for ` ∈ [L] do
6:             b` ← j`(k`f` + c``1`2). ngân sách lớp kết quả
7:             k` ← OPTIMAL_SUBSPACES(b`). Bước cục bộ: cực tiểu hóa ràng buộc lỗi cho ngân sách lớp đã cho
8:         end for
9:     end while
10:    εi ← RECORD_ERROR(k1, ..., kL, j1, ..., jL)
11: end for
12: return k1, ..., kL, j1, ..., jL from ibest = argmini εi

5

--- TRANG 4 ---
Sau đó, chúng tôi gán lại số lượng không gian con k` cho mỗi lớp bằng cách lặp qua tập hợp hữu
hạn các giá trị có thể cho k` (Dòng 7) và chọn giá trị cực tiểu hóa lỗi tương đối cho ngân sách
lớp hiện tại b` (được tính toán trong Dòng 6). Lưu ý rằng chúng tôi có thể ước lượng hiệu quả
lỗi tương đối bằng cách tận dụng Định lý 1. Sau đó chúng tôi lặp đi lặp lại cả hai bước cho đến
khi hội tụ (Dòng 3-8). Để cải thiện chất lượng của tối ưu cục bộ, chúng tôi khởi tạo quy trình
với nhiều hạt giống ngẫu nhiên (Dòng 1-11) và chọn phân bổ có lỗi thấp nhất (Dòng 12).

Chúng tôi lưu ý rằng chúng tôi thực hiện các lời gọi lặp đi lặp lại đến subroutine phân tách của
chúng tôi (tức là SVD; Dòng 4, 7) nhấn mạnh sự cần thiết cho nó phải hiệu quả và rẻ để đánh giá.
Hơn nữa, chúng tôi có thể giảm thêm độ phức tạp tính toán bằng cách tận dụng Định lý 1 như
đã đề cập ở trên.

Chi tiết bổ sung liên quan đến ALDS được cung cấp trong tài liệu bổ sung.

Mở rộng. Ở đây, chúng tôi sử dụng SVD với nhiều không gian con như phương pháp nén mỗi
lớp. Tuy nhiên, chúng tôi lưu ý rằng ALDS có thể dễ dàng được mở rộng cho bất kỳ tập hợp
kỹ thuật nén hạng thấp mong muốn nào. Cụ thể, chúng tôi có thể thay thế bước cục bộ của Dòng
7 bằng tìm kiếm trên các phương pháp khác nhau, ví dụ, phân tách Tucker, PCA, hoặc các sơ
đồ nén SVD khác, và trả về phương pháp tốt nhất cho ngân sách đã cho. Nói chung, chúng tôi có
thể kết hợp ALDS với bất kỳ nén hạng thấp nào miễn là chúng tôi có thể đánh giá hiệu quả lỗi
mỗi lớp của sơ đồ nén. Trong tài liệu bổ sung, chúng tôi thảo luận về một số kết quả sơ bộ nhấn
mạnh hiệu suất đầy hứa hẹn của các mở rộng như vậy.

3 Thí nghiệm

Mạng và tập dữ liệu. Chúng tôi nghiên cứu các kiến trúc mạng tiêu chuẩn và tập dữ liệu khác
nhau. Đặc biệt, chúng tôi kiểm tra khung nén của chúng tôi trên ResNet20 (He et al., 2016),
DenseNet22 (Huang et al., 2017), WRN16-8 (Zagoruyko và Komodakis, 2016), và VGG16 (Simonyan
và Zisserman, 2015) trên CIFAR10 (Torralba et al., 2008); ResNet18 (He et al., 2016), AlexNet
(Krizhevsky et al., 2012), và MobileNetV2 (Sandler et al., 2018) trên ImageNet (Russakovsky et al.,
2015); và trên Deeplab-V3 (Chen et al., 2017) với backbone ResNet50 trên dữ liệu phân đoạn Pascal
VOC (Everingham et al., 2015).

Phương pháp cơ sở. Chúng tôi so sánh ALDS với một tập hợp đa dạng các kỹ thuật nén hạng
thấp. Cụ thể, chúng tôi đã triển khai PCA (Zhang et al., 2015b), SVD với phân bổ lớp dựa trên
năng lượng (SVD-Energy) theo Alvarez và Salzmann (2017); Wen et al. (2017), và SVD đơn giản
với nén không đổi mỗi lớp (Denton et al., 2014). Ngoài ra, chúng tôi cũng đã triển khai cơ chế
lựa chọn hạng học được gần đây (L-Rank) của Idelbayev và Carreira-Perpinán (2020). Cuối cùng,
chúng tôi đã triển khai hai phương pháp cắt tỉa bộ lọc gần đây, tức là FT của Li et al. (2016) và
PFP của Liebenwein et al. (2020), như các kỹ thuật nén thay thế cho mạng nén dày đặc. Các so
sánh bổ sung trên ImageNet được cung cấp trong Phần 3.2.

Huấn luyện lại. Cho các thí nghiệm của chúng tôi, chúng tôi nghiên cứu học một lần và tua lại
tốc độ học lặp đi lặp lại lấy cảm hứng từ Renda et al. (2020) cho các lượng huấn luyện lại khác
nhau. Cụ thể, chúng tôi xem xét pipeline nén-huấn luyện lại thống nhất sau đây trên tất cả các
phương pháp:

1. HUẤN LUYỆN trong e epoch theo lịch trình huấn luyện tiêu chuẩn cho mạng tương ứng.
2. NÉN mạng theo phương pháp đã chọn.

[THIS IS FIGURE/CHART: Four graphs showing one-shot compress+retrain experiments on CIFAR10 with baseline comparisons for DenseNet22, VGG16, and WRN16-8]

Hình 4: Thí nghiệm nén một lần + huấn luyện lại trên CIFAR10 với so sánh phương pháp cơ sở.

6

--- TRANG 5 ---
[THIS IS FIGURE: Six graphs showing size-accuracy trade-offs for various compression ratios, methods, and networks for ResNet20 (CIFAR10) and ResNet18 (ImageNet)]

Hình 5: Sự đánh đổi kích thước-độ chính xác cho các tỷ lệ nén, phương pháp và mạng khác nhau. Nén được thực hiện sau huấn luyện và mạng được huấn luyện lại một lần với lượng được chỉ định (một lần). (a, b, d, e): sự khác biệt trong độ chính xác kiểm tra cho các lượng huấn luyện lại cố định. (c, f): tỷ lệ nén tối đa với độ giảm độ chính xác dưới 1% cho các lượng huấn luyện lại thay đổi.

3. HUẤN LUYỆN LẠI mạng trong r epoch sử dụng các siêu tham số huấn luyện từ epoch [e-r, e].
4. LẶP LẠI các bước 1.-3. sau khi chiếu các lớp phân tách trở lại (tùy chọn).

Báo cáo số liệu. Chúng tôi báo cáo độ chính xác kiểm tra Top-1, Top-5, và IoU tùy theo nhiệm vụ tương ứng. Đối với mỗi mạng nén, chúng tôi cũng báo cáo tỷ lệ nén, tức là mức giảm tương đối, về tham số và phép toán dấu phẩy động được ký hiệu bằng CR-P và CR-F, tương ứng. Mỗi thí nghiệm được lặp lại 3 lần và chúng tôi báo cáo trung bình và độ lệch chuẩn.

3.1 Nén Một Lần trên CIFAR10, ImageNet, và VOC với Phương pháp Cơ sở

Chúng tôi huấn luyện các mạng tham chiếu trên CIFAR10, ImageNet, và VOC, sau đó nén và huấn luyện lại mạng một lần với r = e cho các so sánh phương pháp cơ sở và tỷ lệ nén khác nhau.

CIFAR10. Trong Hình 4, chúng tôi cung cấp kết quả cho DenseNet22, VGG16, và WRN16-8 trên CIFAR10. Đáng chú ý, phương pháp của chúng tôi có thể vượt trội so với các phương pháp cơ sở hiện có trên một phạm vi rộng các tỷ lệ nén được kiểm tra. Cụ thể, trong vùng mà các mạng chỉ phát sinh sụt giảm tối thiểu về độ chính xác (ΔTop1 ≤ 1%) ALDS đặc biệt hiệu quả.

ResNet (CIFAR10 và ImageNet). Hơn nữa, chúng tôi đã kiểm tra ALDS trên ResNet20 (CIFAR10) và ResNet18 (ImageNet) như được hiển thị trong Hình 5. Đối với các thí nghiệm này, chúng tôi đã thực hiện tìm kiếm lưới

[THIS IS FIGURE: Three graphs showing one-shot compress+retrain experiments on various architectures and datasets]

Hình 6: Thí nghiệm nén một lần + huấn luyện lại trên các kiến trúc và tập dữ liệu khác nhau với so sánh phương pháp cơ sở.

7

--- TRANG 6 ---
[THIS IS TABLE: Baseline results showing performance metrics for different models and compression methods across CIFAR10, ImageNet, and VOC datasets]

Bảng 1: Kết quả phương pháp cơ sở cho ∆Top1 ≤ 0.5% cho một lần với CR-P và CR-F cao nhất trong số các phương pháp phân tách tensor được in đậm cho mỗi mạng. Kết quả trùng khớp với Hình 4, 5, 6b.

trên cả nhiều tỷ lệ nén và lượng huấn luyện lại. Ở đây, chúng tôi nhấn mạnh rằng ALDS vượt trội so với các phương pháp cơ sở ngay cả với việc huấn luyện lại ít hơn đáng kể. Trên Resnet 18 (ImageNet) ALDS có thể nén hơn 50% tham số với việc huấn luyện lại tối thiểu (1% huấn luyện lại) và độ giảm độ chính xác dưới 1% so với các phương pháp so sánh tốt nhất (40% nén với 50% huấn luyện lại).

[THIS IS TABLE: AlexNet and ResNet18 Benchmarks on ImageNet showing Top-1, Top-5 accuracy and FLOP reduction percentages]

Bảng 2: Điểm chuẩn AlexNet và ResNet18 trên ImageNet. Chúng tôi báo cáo độ chính xác Top-1, Top-5 và phần trăm giảm FLOP (CR-F). Kết quả tốt nhất với độ giảm độ chính xác dưới 0.5% được in đậm.

MobileNetV2 (ImageNet). Tiếp theo, chúng tôi đã kiểm tra và so sánh ALDS trên kiến trúc MobileNetV2 cho ImageNet như được hiển thị trong Hình 6a. Không giống như các mạng khác, MobileNetV2 là một mạng đã được tối ưu hóa cụ thể cho triển khai hiệu quả và bao gồm các cấu trúc lớp như các hoạt động tích chập theo chiều sâu và theo kênh. Do đó việc tìm thấy sự dư thừa trong kiến trúc là thách thức hơn. Chúng tôi thấy rằng ALDS có thể vượt trội so với các phương pháp phân tách tensor hiện có trong tình huống này cũng vậy.

VOC. Cuối cùng, chúng tôi đã kiểm tra cùng thiết lập trên DeeplabV3 với backbone ResNet50 được huấn luyện trên dữ liệu phân đoạn Pascal VOC 2012, xem Hình 6b. Chúng tôi lưu ý rằng ALDS luôn vượt trội so với các phương pháp cơ sở khác trong thiết lập này cũng vậy (60% CR-P so với 20% mà không có sụt giảm độ chính xác).

Kết quả dạng bảng. Kết quả một lần của chúng tôi được tóm tắt lại trong Bảng 1 trong đó chúng tôi báo cáo CR-P và CR-F cho ∆Top1 ≤ 0.5%. Chúng tôi quan sát rằng ALDS luôn cải thiện so với công trình trước đây. Chúng tôi lưu ý rằng cắt tỉa thường mất từ giây đến phút cho CIFAR và ImageNet, tương ứng, thường nhanh hơn ngay cả một epoch huấn luyện duy nhất.

3.2 Điểm chuẩn ImageNet

Tiếp theo, chúng tôi kiểm tra khung của chúng tôi trên hai điểm chuẩn ImageNet phổ biến, ResNet18 và AlexNet. Chúng tôi tuân theo pipeline nén-huấn luyện lại được phác thảo ở đầu phần và lặp lại nó một cách lặp đi lặp lại

8

--- TRANG 7 ---
để có được tỷ lệ nén cao hơn. Cụ thể, sau khi huấn luyện lại và trước bước nén tiếp theo, chúng tôi chiếu các lớp phân tách trở lại lớp gốc. Bằng cách này, chúng tôi tránh việc đệ quy trên các lớp phân tách.

Kết quả của chúng tôi được báo cáo trong Bảng 2 trong đó chúng tôi so sánh với nhiều điểm chuẩn nén có sẵn (kết quả được điều chỉnh trực tiếp từ các bài báo tương ứng). Phần giữa và phần dưới của bảng cho mỗi mạng được tổ chức thành các phương pháp nén hạng thấp và cắt tỉa bộ lọc, tương ứng. Lưu ý rằng các sự khác biệt báo cáo về độ chính xác (∆Top1 và ∆Top5) là tương đối so với độ chính xác cơ sở của chúng tôi. Trên ResNet18, chúng tôi có thể giảm số lượng FLOP 65% với sụt giảm tối thiểu về độ chính xác so với phương pháp cạnh tranh tốt nhất (MUSCO, 58.67%). Với sụt giảm độ chính xác cao hơn một chút (-1.37%), chúng tôi thậm chí có thể nén 76% FLOP. Trên AlexNet, khung của chúng tôi tìm thấy các mạng với sự khác biệt -0.21% và -0.41% về độ chính xác với hơn 77% và 81% FLOP ít hơn. Điều này tạo thành sự cải thiện hơn 10% về FLOP so với công nghệ tiên tiến hiện tại (L-Rank) cho các sụt giảm độ chính xác tương tự.

3.3 Nghiên cứu Phân tích

[THIS IS FIGURE: A graph showing one-shot ablation study of ALDS for Resnet20 (CIFAR10) with multiple curves comparing different variants]

Hình 7: Nghiên cứu phân tích một lần của ALDS cho Resnet20 (CIFAR10).

Để điều tra các tính năng khác nhau của phương pháp chúng tôi, chúng tôi đã chạy các thí nghiệm nén sử dụng nhiều biến thể được suy ra từ phương pháp của chúng tôi, xem Hình 7. Đối với phiên bản đơn giản nhất của phương pháp chúng tôi, chúng tôi xem xét tỷ lệ nén không đổi mỗi lớp và cố định giá trị k thành 3 hoặc 5 cho tất cả các lớp được ký hiệu bằng ALDS-Simple3 và ALDS-Simple5, tương ứng. Lưu ý rằng ALDS-Simple với k = 1 tương ứng với phương pháp so sánh SVD. Đối với phiên bản được ký hiệu bằng ALDS-Error3, chúng tôi cố định số lượng không gian con mỗi lớp (k = 3) và chỉ chạy bước toàn cục của ALDS (Dòng 4 của Thuật toán 1) để xác định tỷ lệ nén tối ưu mỗi lớp. Kết quả nghiên cứu phân tích của chúng tôi trong Hình 7 cho thấy phương pháp của chúng tôi rõ ràng có lợi từ sự kết hợp của cả bước toàn cục và cục bộ về số lượng không gian con (k) và hạng mỗi không gian con (j).

Chúng tôi cũng so sánh phân cụm không gian con của chúng tôi (chia cắt kênh) với kỹ thuật phân cụm của Maalouf et al. (2021), phân cụm các cột ma trận sử dụng phân cụm phép chiếu. Cụ thể, chúng tôi thay thế việc chia cắt kênh của ALDS-Simple3 bằng phân cụm phép chiếu (Messi3 trong Hình 7). Như mong đợi, Messi cải thiện hiệu suất so với ALDS-Simple nhưng chỉ một chút và sự khác biệt về cơ bản là không đáng kể. Cùng với các bất lợi tính toán của các phương pháp phân cụm giống Messi (không có cấu trúc, NP-hard; xem Phần 2.1), việc chia cắt kênh đơn giản dựa trên ALDS do đó là lựa chọn ưa thích trong bối cảnh của chúng tôi.

4 Công trình Liên quan

Công trình của chúng tôi dựa trên công trình trước đây về nén mạng nơ-ron. Chúng tôi thảo luận công trình liên quan tập trung vào cắt tỉa, nén hạng thấp, và các khía cạnh toàn cục của nén.

Cắt tỉa không có cấu trúc. Các kỹ thuật cắt tỉa trọng số (Lin et al., 2020b; Molchanov et al., 2016, 2019; Singh và Alistarh, 2020; Wang et al., 2021; Yu et al., 2018) nhằm giảm số lượng trọng số riêng lẻ, ví dụ, bằng cách loại bỏ các trọng số có giá trị tuyệt đối dưới ngưỡng (Han et al., 2015; Renda et al., 2020), hoặc bằng cách sử dụng một mini-batch các điểm dữ liệu để xấp xỉ ảnh hưởng của mỗi tham số đến hàm mất mát (Baykal et al., 2019a,b). Tuy nhiên, vì các phương pháp này tạo ra các mô hình thưa thớt thay vì nhỏ hơn, chúng yêu cầu một số hình thức hỗ trợ đại số tuyến tính thưa thớt để tăng tốc thời gian chạy.

Cắt tỉa có cấu trúc. Cắt tỉa các cấu trúc như bộ lọc trực tiếp thu nhỏ mạng (Chen et al., 2020; Li et al., 2019b; Lin et al., 2020a; Liu et al., 2019a; Luo và Wu, 2020; Ye et al., 2018). Các bộ lọc có thể được cắt tỉa sử dụng điểm số cho mỗi bộ lọc, ví dụ, dựa trên trọng số (He et al., 2018, 2017) hoặc thông tin dữ liệu (Liebenwein et al., 2020; Yu et al., 2018), và loại bỏ những bộ lọc có điểm số dưới ngưỡng. Đáng chú ý rằng cắt tỉa bộ lọc bổ sung cho nén hạng thấp.

Nén hạng thấp (bước cục bộ). Một phương pháp phổ biến cho nén hạng thấp bao gồm phân tách tensor bao gồm phân tách Tucker (Kim et al., 2015b), phân tách CP (Lebedev et al., 2015), Tensor-Train (Garipov et al., 2016; Novikov et al., 2015) và các phương pháp khác (Denil et al., 2013; Ioannou et al., 2017; Jaderberg et al., 2014). Các phương pháp giống phân tách khác bao gồm chia sẻ trọng số, phép chiếu ngẫu nhiên, và băm đặc trưng (Arora et al., 2018; Chen et al., 2015a,b; Shi et al., 2009; Ullrich et al., 2017; Weinberger et al., 2009). Thay thế, nén hạng thấp có thể được thực hiện thông qua phân tách ma trận (ví dụ, SVD) trên các tensor được làm phẳng như được thực hiện bởi Denton et al. (2014); Sainath et al. (2013); Tukan et al. (2020); Xue et al. (2013); Yu et al. (2017) cùng những người khác. Chen et al. (2018); Denton et al. (2014); Maalouf et al. (2021) cũng khám phá việc sử dụng phân cụm không gian con trước khi áp dụng nén hạng thấp cho mỗi cụm để cải thiện lỗi xấp xỉ. Đáng chú ý, hầu hết công trình trước đây dựa vào một số hình thức thuật toán xấp xỉ đắt đỏ – ngay cả để chỉ giải quyết nén hạng thấp mỗi lớp, ví dụ, phân cụm hoặc phân tách tensor. Trong bài báo này, thay vào đó chúng tôi tập trung vào vấn đề nén toàn cục và chỉ ra rằng các kỹ thuật nén đơn giản (SVD với chia cắt kênh) có lợi thế trong bối cảnh này vì chúng tôi có thể sử dụng chúng như các subroutine hiệu quả. Chúng tôi lưu ý rằng chúng tôi thậm chí có thể mở rộng thuật toán của chúng tôi cho nhiều loại phân tách mỗi lớp khác nhau.

Nén nhận thức mạng (bước toàn cục). Để xác định hạng (hoặc tỷ lệ nén) của mỗi lớp, công trình trước đây đề xuất tính đến nén trong quá trình huấn luyện (Alvarez và Salzmann, 2017; Ioannou et al., 2016, 2015; Wen et al., 2017; Xu et al., 2020), ví dụ, bằng cách huấn luyện mạng với hình phạt khuyến khích các ma trận trọng số có hạng thấp. Những người khác đề xuất lựa chọn các hạng sử dụng phân tích nhân tử ma trận Bayesian biến phân (Kim et al., 2015b). Trong bài báo gần đây của họ, Chin et al. (2020) đề xuất tạo ra toàn bộ tập hợp các mạng nén với các sự đánh đổi độ chính xác/tốc độ khác nhau. Bài báo của chúng tôi cũng được lấy cảm hứng từ một dòng công trình gần đây hướng tới việc tự động chọn hoặc học hạng của mỗi lớp (Gusak et al., 2019; Idelbayev và Carreira-Perpinán, 2020; Li và Shi, 2018; Tiwari et al., 2021; Zhang et al., 2015b,c). Chúng tôi đưa các phương pháp như vậy đi xa hơn và đề xuất một khung nén toàn cục tích hợp nhiều kỹ thuật phân tách với nhiều hơn một siêu tham số mỗi lớp (số lượng không gian con và hạng của mỗi lớp). Phương pháp này tăng số lượng cực tiểu cục bộ về lý thuyết và giúp cải thiện hiệu suất trong thực tế.

5 Thảo luận và Kết luận

Lợi ích thực tế. Bằng cách tiến hành nhiều thí nghiệm đa dạng trên nhiều tập dữ liệu và mạng, chúng tôi đã chỉ ra tính hiệu quả và tính linh hoạt của khung nén của chúng tôi so với các phương pháp hiện có. Thời gian chạy của ALDS không đáng kể so với huấn luyện lại và do đó có thể được tích hợp hiệu quả vào các pipeline nén-huấn luyện lại.

ALDS như khung nén modular. Bằng cách xem xét riêng biệt sơ đồ nén hạng thấp cho mỗi lớp (bước cục bộ) và nén hạng thấp thực tế (bước toàn cục), chúng tôi đã cung cấp một khung có thể tìm kiếm hiệu quả trên một tập hợp các siêu tham số mong muốn mô tả nén hạng thấp. Tự nhiên, khung của chúng tôi do đó có thể được tổng quát hóa cho các sơ đồ nén khác (như phân tách tensor) và chúng tôi hy vọng khám phá những khía cạnh này trong công trình tương lai.

Ràng buộc lỗi dẫn đến hiểu biết toàn cục. Cốt lõi của đóng góp của chúng tôi là phân tích lỗi của chúng tôi cho phép chúng tôi liên kết các khía cạnh toàn cục và cục bộ của các kỹ thuật nén theo từng lớp. Chúng tôi tận dụng các ràng buộc lỗi của chúng tôi trong thực tế để nén mạng hiệu quả hơn thông qua quy trình lựa chọn hạng tự động mà không cần điều chỉnh siêu tham số tẻ nhạt bổ sung. Tuy nhiên, chúng tôi cũng phải dựa vào định nghĩa đại diện (lỗi tương đối tối đa) của lỗi nén để cho phép giải pháp có thể xử lý mà chúng tôi có thể triển khai hiệu quả. Chúng tôi hy vọng những quan sát này sẽ khuyến khích nghiên cứu tương lai về các kỹ thuật nén đi kèm với các ràng buộc lỗi chặt chẽ – có khả năng thậm chí xem xét huấn luyện lại – sau đó có thể được gói một cách tự nhiên vào khung nén toàn cục.

Lời cảm ơn

Nghiên cứu này được tài trợ bởi Phòng thí nghiệm Nghiên cứu Không quân Hoa Kỳ và Máy gia tốc Trí tuệ Nhân tạo Không quân Hoa Kỳ và được hoàn thành dưới Số Thỏa thuận Hợp tác FA8750-19-2-1000. Các quan điểm và kết luận có trong tài liệu này là của các tác giả và không nên được giải thích là đại diện cho các chính sách chính thức, được thể hiện rõ ràng hoặc ngụ ý, của Không quân Hoa Kỳ hoặc Chính phủ Hoa Kỳ. Chính phủ Hoa Kỳ được ủy quyền sao chép và phân phối các bản sao cho mục đích Chính phủ bất chấp bất kỳ ký hiệu bản quyền nào ở đây. Công trình này được hỗ trợ thêm bởi Tài trợ Văn phòng Nghiên cứu Hải quân (ONR) N00014-18-1-2830.

10

--- TRANG 8 ---
Tài liệu tham khảo

Jose M Alvarez và Mathieu Salzmann. Huấn luyện nhận thức nén của mạng sâu. Trong Advances in Neural Information Processing Systems, trang 856–867, 2017.

Sanjeev Arora, Rong Ge, Behnam Neyshabur, và Yi Zhang. Ràng buộc tổng quát hóa mạnh hơn cho mạng sâu thông qua phương pháp nén. Trong International Conference on Machine Learning, trang 254–263, 2018.

Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, và Daniela Rus. Coresets phụ thuộc dữ liệu để nén mạng nơ-ron với ứng dụng cho ràng buộc tổng quát hóa. Trong International Conference on Learning Representations, 2019a. URL https://openreview.net/forum?id=HJfwJ2A5KX.

Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, và Daniela Rus. Nhấm nháp mạng nơ-ron: Cắt tỉa mạng nơ-ron có thể chứng minh dựa trên thông tin độ nhạy. arXiv preprint arXiv:1910.05422, 2019b.

Jianda Chen, Shangyu Chen, và Sinno Jialin Pan. Cắt tỉa kênh linh hoạt động và hiệu quả lưu trữ thông qua học tăng cường sâu. Advances in Neural Information Processing Systems, 33, 2020.

Liang-Chieh Chen, George Papandreou, Florian Schroff, và Hartwig Adam. Suy nghĩ lại về tích chập atrous cho phân đoạn ảnh ngữ nghĩa. arXiv preprint arXiv:1706.05587, 2017.

Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, và Cho-jui Hsieh. GroupReduce: Xấp xỉ hạng thấp theo khối để thu nhỏ mô hình ngôn ngữ nơ-ron. Advances in Neural Information Processing Systems, 2018-December:10988–10998, jun 2018. URL http://arxiv.org/abs/1806.06950.

Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, và Yixin Chen. Nén mạng nơ-ron với thủ thuật băm. Trong International conference on machine learning, trang 2285–2294, 2015a.

Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, và Yixin Chen. Nén mạng nơ-ron tích chập. CoRR, abs/1506.04449, 2015b. URL http://arxiv.org/abs/1506.04449.

Ting-Wu Chin, Ruizhou Ding, Cha Zhang, và Diana Marculescu. Hướng tới nén mô hình hiệu quả thông qua xếp hạng toàn cục đã học. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 1518–1528, 2020.

Kenneth L Clarkson và David P Woodruff. Tính thưa thớt đầu vào và độ cứng cho xấp xỉ không gian con vững chắc. Trong 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, trang 310–329. IEEE, 2015.

Arthur P Dempster, Nan M Laird, và Donald B Rubin. Khả năng tối đa từ dữ liệu không đầy đủ thông qua thuật toán em. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1–22, 1977.

Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, và Nando de Freitas. Dự đoán tham số trong học sâu. Trong Advances in Neural Information Processing Systems 26, trang 2148–2156, 2013.

Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, và Rob Fergus. Khai thác Cấu trúc Tuyến tính Trong Mạng Tích chập để Đánh giá Hiệu quả. Advances in Neural Information Processing Systems, 2(January):1269–1277, apr 2014. URL http://arxiv.org/abs/1404.0736.

Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, và Chenggang Yan. Cắt tỉa bộ lọc oracle xấp xỉ cho tối ưu hóa chiều rộng CNN phá hủy. Trong International Conference on Machine Learning, trang 1607–1616. PMLR, 2019.

11

--- TRANG 9 ---
Xuanyi Dong, Junshi Huang, Yi Yang, và Shuicheng Yan. Nhiều hơn là ít hơn: Một mạng phức tạp hơn với độ phức tạp suy luận ít hơn. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 5840–5848, 2017.

Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, và Andrew Zisserman. Thử thách các lớp đối tượng trực quan pascal: Một cái nhìn hồi tưởng. International journal of computer vision, 111(1):98–136, 2015.

Xitong Gao, Yiren Zhao, Łukasz Dudziak, Robert Mullins, và Cheng-zhong Xu. Cắt tỉa kênh động: Tăng cường và triệt tiêu đặc trưng. arXiv preprint arXiv:1810.05331, 2018.

Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, và Dmitry Vetrov. Tensorization tối thượng: nén các lớp tích chập và fc giống nhau. arXiv preprint arXiv:1611.03214, 2016.

Gene H Golub và Charles F Van Loan. Tính toán ma trận, tập 3. JHU press, 2013.

Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, và Kaiming He. SGD minibatch lớn chính xác: Huấn luyện imagenet trong 1 giờ. arXiv preprint arXiv:1706.02677, 2017.

Julia Gusak, Maksym Kholiavchenko, Evgeny Ponomarev, Larisa Markeeva, Philip Blagoveschensky, Andrzej Cichocki, và Ivan Oseledets. Nén đa giai đoạn tự động của mạng nơ-ron. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, trang 0–0, 2019.

Song Han, Huizi Mao, và William J. Dally. Nén sâu: Nén mạng nơ-ron sâu với cắt tỉa, lượng tử hóa được huấn luyện và mã hóa huffman. CoRR, abs/1510.00149, 2015. URL http://arxiv.org/abs/1510.00149.

Bharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, và Jitendra Malik. Đường viền ngữ nghĩa từ các detector nghịch đảo. Trong 2011 International Conference on Computer Vision, trang 991–998. IEEE, 2011.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Học tàn dư sâu để nhận dạng ảnh. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016.

Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, và Yi Yang. Cắt tỉa bộ lọc mềm để tăng tốc mạng nơ-ron tích chập sâu. Trong Proceedings of the 27th International Joint Conference on Artificial Intelligence, trang 2234–2240. AAAI Press, 2018.

Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, và Yi Yang. Cắt tỉa bộ lọc thông qua trung vị hình học để tăng tốc mạng nơ-ron tích chập sâu. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 4340–4349, 2019.

Yihui He, Xiangyu Zhang, và Jian Sun. Cắt tỉa kênh để tăng tốc mạng nơ-ron rất sâu. Trong Proceedings of the IEEE International Conference on Computer Vision, trang 1389–1397, 2017.

Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Chưng cất kiến thức trong mạng nơ-ron. arXiv preprint arXiv:1503.02531, 2015.

Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, và G Edward Suh. Mạng nơ-ron cổng kênh. arXiv preprint arXiv:1805.12549, 2018.

Gao Huang, Zhuang Liu, Laurens Van Der Maaten, và Kilian Q Weinberger. Mạng tích chập kết nối dày đặc. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 4700–4708, 2017.

Yerlan Idelbayev và Miguel A Carreira-Perpinán. Nén hạng thấp của mạng nơ-ron: Học hạng của mỗi lớp. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 8049–8059, 2020.

Y Ioannou, D Robertson, J Shotton, R Cipolla, và A Criminisi. Huấn luyện cnn với bộ lọc hạng thấp để phân loại ảnh hiệu quả. Trong 4th International Conference on Learning Representations, ICLR 2016-Conference Track Proceedings, 2016.

12

--- TRANG 10 ---
Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, và Antonio Criminisi. Huấn luyện cnn với bộ lọc hạng thấp để phân loại ảnh hiệu quả. arXiv preprint arXiv:1511.06744, 2015.

Yani Ioannou, Duncan Robertson, Roberto Cipolla, và Antonio Criminisi. Rễ sâu: Cải thiện hiệu quả cnn với các nhóm bộ lọc phân cấp. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 1231–1240, 2017.

Max Jaderberg, Andrea Vedaldi, và Andrew Zisserman. Tăng tốc mạng nơ-ron tích chập với mở rộng hạng thấp. Trong Proceedings of the British Machine Vision Conference. BMVA Press, 2014.

Hyeji Kim, Muhammad Umar Karim Khan, và Chong-Min Kyung. Nén mạng nơ-ron hiệu quả. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 12569–12577, 2019.

Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, và Dongjun Shin. Nén Mạng Nơ-ron Tích chập Sâu cho Ứng dụng Di động Nhanh và Tiêu thụ Ít Điện. 4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings, nov 2015a. URL http://arxiv.org/abs/1511.06530.

Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, và Dongjun Shin. Nén mạng nơ-ron tích chập sâu cho ứng dụng di động nhanh và tiêu thụ ít điện. arXiv preprint arXiv:1511.06530, 2015b.

Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton. Phân loại imagenet với mạng nơ-ron tích chập sâu. Trong F. Pereira, C. J. C. Burges, L. Bottou, và K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, trang 1097–1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.

Valero Laparra, Jesús Malo, và Gustau Camps-Valls. Giảm chiều thông qua hồi quy trong ảnh siêu phổ. IEEE Journal of Selected Topics in Signal Processing, 9(6):1026–1036, 2015.

Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan V. Oseledets, và Victor S. Lempitsky. Tăng tốc mạng nơ-ron tích chập sử dụng phân tách cp được tinh chỉnh. Trong ICLR (Poster), 2015. URL http://arxiv.org/abs/1412.6553.

Chong Li và CJ Shi. Xấp xỉ hạng thấp dựa trên tối ưu hóa có ràng buộc của mạng nơ-ron sâu. Trong Proceedings of the European Conference on Computer Vision (ECCV), trang 732–747, 2018.

Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Cắt tỉa bộ lọc cho convnet hiệu quả. arXiv preprint arXiv:1608.08710, 2016.

Jiashi Li, Qi Qi, Jingyu Wang, Ce Ge, Yujian Li, Zhangzhang Yue, và Haifeng Sun. Oicsr: Điều chuẩn độ thưa thớt ngoài-trong-kênh cho mạng nơ-ron sâu nhỏ gọn. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 7046–7055, 2019a.

Yawei Li, Shuhang Gu, Luc Van Gool, và Radu Timofte. Học cơ sở bộ lọc để nén mạng nơ-ron tích chập. Trong Proceedings of the IEEE International Conference on Computer Vision, trang 5623–5632, 2019b.

Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, và Daniela Rus. Cắt tỉa bộ lọc có thể chứng minh cho mạng nơ-ron hiệu quả. Trong International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=BJxkOlSYDH.

Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, và Daniela Rus. Mất trong cắt tỉa: Các hiệu ứng của việc cắt tỉa mạng nơ-ron ngoài độ chính xác kiểm tra. Proceedings of Machine Learning and Systems, 3, 2021a.

Lucas Liebenwein, Ramin Hasani, Alexander Amini, và Daniela Rus. Dòng chảy thưa thớt: Cắt tỉa mô hình độ sâu liên tục. arXiv preprint arXiv:2106.12718, 2021b.

13

--- TRANG 11 ---
Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, và Ling Shao. Hrank: Cắt tỉa bộ lọc sử dụng bản đồ đặc trưng hạng cao. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 1529–1538, 2020a.

Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, và Martin Jaggi. Cắt tỉa mô hình động với phản hồi. Trong International Conference on Learning Representations, 2020b. URL https://openreview.net/forum?id=SJem8lSFwB.

Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, và Jian Sun. Metapruning: Meta learning cho cắt tỉa kênh mạng nơ-ron tự động. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 3296–3305, 2019a.

Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Học mạng tích chập hiệu quả thông qua thu nhỏ mạng. Trong Proceedings of the IEEE International Conference on Computer Vision, trang 2736–2744, 2017.

Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. Suy nghĩ lại giá trị của việc cắt tỉa mạng. Trong International Conference on Learning Representations, 2019b. URL https://openreview.net/forum?id=rJlnB3C5Ym.

Jian-Hao Luo và Jianxin Wu. Autopruner: Một phương pháp cắt tỉa bộ lọc có thể huấn luyện đầu cuối đến cuối cho suy luận mô hình sâu hiệu quả. Pattern Recognition, 107:107461, 2020.

Alaa Maalouf, Harry Lang, Daniela Rus, và Dan Feldman. Học sâu gặp phân cụm phép chiếu. Trong International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=EQfpYwF3-b.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan Kautz. Cắt tỉa mạng nơ-ron tích chập cho suy luận hiệu quả tài nguyên. arXiv preprint arXiv:1611.06440, 2016.

Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, và Jan Kautz. Ước lượng tầm quan trọng cho cắt tỉa mạng nơ-ron. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 11264–11272, 2019.

Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, và Dmitry Vetrov. Tensorizing mạng nơ-ron. Trong Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, trang 442–450, 2015.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, và Adam Lerer. Phân biệt tự động trong pytorch. Trong NIPS-W, 2017.

Xi Peng, Zhang Yi, và Huajin Tang. Phân cụm không gian con vững chắc thông qua hồi quy ridge ngưỡng. Trong Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.

Alex Renda, Jonathan Frankle, và Michael Carbin. So sánh tinh chỉnh và tua lại trong cắt tỉa mạng nơ-ron. Trong International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=S1gSj0NKvB.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, và Li Fei-Fei. Thử thách Nhận dạng Trực quan Quy mô Lớn ImageNet. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.

Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, và Bhuvana Ramabhadran. Phân tích nhân tử ma trận hạng thấp cho huấn luyện mạng nơ-ron sâu với mục tiêu đầu ra chiều cao. Trong 2013 IEEE international conference on acoustics, speech and signal processing, trang 6655–6659. IEEE, 2013.

Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, và Liang-Chieh Chen. Mobilenetv2: Tàn dư nghịch đảo và cổ chai tuyến tính. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 4510–4520, 2018.

14

--- TRANG 12 ---
Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, và SVN Vishwanathan. Kernel băm cho dữ liệu có cấu trúc. Journal of Machine Learning Research, 10(Nov):2615–2637, 2009.

Karen Simonyan và Andrew Zisserman. Mạng nơ-ron tích chập rất sâu cho nhận dạng ảnh quy mô lớn. arXiv preprint arXiv:1409.1556, 2014.

Karen Simonyan và Andrew Zisserman. Mạng nơ-ron tích chập rất sâu cho nhận dạng ảnh quy mô lớn. Trong International Conference on Learning Representations, 2015.

Sidak Pal Singh và Dan Alistarh. Woodfisher: Xấp xỉ bậc hai hiệu quả cho nén mô hình. arXiv preprint arXiv:2004.14340, 2020.

Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Mạng nơ-ron tích chập với điều chuẩn hạng thấp. arXiv preprint arXiv:1511.06067, 2015.

Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, và Deepak Gupta. Chipnet: Cắt tỉa nhận thức ngân sách với xấp xỉ liên tục heaviside. Trong International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=xCxXwTzx4L1.

Antonio Torralba, Rob Fergus, và William T Freeman. 80 triệu hình ảnh nhỏ: Một tập dữ liệu lớn cho nhận dạng đối tượng và cảnh phi tham số. IEEE transactions on pattern analysis and machine intelligence, 30(11):1958–1970, 2008.

Murad Tukan, Alaa Maalouf, Matan Weksler, và Dan Feldman. Mạng sâu nén: Tạm biệt svd, xin chào xấp xỉ hạng thấp vững chắc. arXiv preprint arXiv:2009.05647, 2020.

Karen Ullrich, Edward Meeds, và Max Welling. Chia sẻ trọng số mềm cho nén mạng nơ-ron. arXiv preprint arXiv:1702.04008, 2017.

Huan Wang, Can Qin, Yulun Zhang, và Yun Fu. Cắt tỉa nơ-ron thông qua điều chuẩn tăng trưởng. Trong International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=o966_Is_nPA.

Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, và Josh Attenberg. Băm đặc trưng cho học đa nhiệm quy mô lớn. Trong Proceedings of the 26th annual international conference on machine learning, trang 1113–1120, 2009.

Wei Wen, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran Chen, và Hai Li. Điều phối Bộ lọc cho Mạng Nơ-ron Sâu Nhanh hơn. Proceedings of the IEEE International Conference on Computer Vision, 2017-Octob:658–666, mar 2017. URL http://arxiv.org/abs/1703.09746.

Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, và Jian Cheng. Mạng Nơ-ron Tích chập Lượng tử cho Thiết bị Di động. Trong Proceedings of the International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, và Hongkai Xiong. TRP: Cắt tỉa Hạng Được Huấn luyện cho Mạng Nơ-ron Sâu Hiệu quả. IJCAI International Joint Conference on Artificial Intelligence, 2021-Janua:977–983, apr 2020. URL http://arxiv.org/abs/2004.14566.

Jian Xue, Jinyu Li, và Yifan Gong. Tái cấu trúc mô hình âm thanh mạng nơ-ron sâu với phân tách giá trị kỳ dị. Trong Interspeech, trang 2365–2369, 2013.

Jianbo Ye, Xin Lu, Zhe Lin, và James Z. Wang. Suy nghĩ lại giả định chuẩn nhỏ hơn-thông tin ít hơn trong cắt tỉa kênh của các lớp tích chập. Trong International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HJ94fqApW.

Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, và Larry S Davis. Nisp: Cắt tỉa mạng sử dụng lan truyền điểm số tầm quan trọng nơ-ron. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 9194–9203, 2018.

15

--- TRANG 13 ---
Xiyu Yu, Tongliang Liu, Xinchao Wang, và Dacheng Tao. Về nén mô hình sâu bằng phân tách hạng thấp và thưa thớt. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 7370–7379, 2017.

Sergey Zagoruyko và Nikos Komodakis. Mạng tàn dư rộng. arXiv preprint arXiv:1605.07146, 2016.

Xiangyu Zhang, Jianhua Zou, Kaiming He, và Jian Sun. Tăng tốc Mạng Tích chập Rất Sâu cho Phân loại và Phát hiện. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(10):1943–1955, may 2015a. URL http://arxiv.org/abs/1505.06798.

Xiangyu Zhang, Jianhua Zou, Kaiming He, và Jian Sun. Tăng tốc mạng tích chập rất sâu cho phân loại và phát hiện. IEEE transactions on pattern analysis and machine intelligence, 38(10):1943–1955, 2015b.

Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, và Jian Sun. Xấp xỉ hiệu quả và chính xác của mạng tích chập phi tuyến. Trong Proceedings of the IEEE Conference on Computer Vision and pattern Recognition, trang 1984–1992, 2015c.

Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, và Jinhui Zhu. Cắt tỉa kênh nhận thức phân biệt cho mạng nơ-ron sâu. arXiv preprint arXiv:1810.11809, 2018.

16

--- TRANG 14 ---
Tài liệu Bổ sung

Mục lục
A Chi tiết Phương pháp Bổ sung 18
A.1 Kiến thức Cơ bản Phương pháp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 Phương pháp Phân cụm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 Nén thông qua SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.4 Triển khai Hiệu quả của ALDS (Thuật toán 1) . . . . . . . . . . . . . . . . . . . . . . . . 20
A.5 Thảo luận Bổ sung về ALDS (Thuật toán 1) . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.6 Mở rộng của ALDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

B Thiết lập Thí nghiệm và Siêu tham số 23
B.1 Thiết lập Thí nghiệm cho CIFAR10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B.2 Thiết lập Thí nghiệm cho ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B.3 Thiết lập Thí nghiệm cho Pascal VOC . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.4 Phương pháp Cơ sở . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.5 Pipeline Nén-Huấn luyện lại . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

C Kết quả Thí nghiệm Bổ sung 26
C.1 Bảng Đầy đủ cho Thí nghiệm Nén Một lần từ Phần 3.1 . . . . . . . . . . . . . . . . . . 26
C.2 Kết quả Điểm chuẩn ImageNet Đầy đủ từ Phần 3.2 . . . . . . . . . . . . . . . . . . . . 27
C.3 Nghiên cứu Phân tích . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C.4 Mở rộng của ALDS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

17

--- TRANG 15 ---
Hình 8: Tích chập đến phép nhân ma trận. Một lớp tích chập của f = 20 bộ lọc, c = 6 kênh, và kernel 2×2 (l1 = l2 = 2). Hình dạng tensor đầu vào là 6×3×3. Ma trận trọng số tương ứng có f = 20 hàng (một hàng mỗi bộ lọc) và 24 cột (c×l1×l2), còn đối với ma trận đặc trưng tương ứng, nó có 24 hàng và 4 cột, ở đây 4 là số lượng cửa sổ tích chập (tức là số lượng pixel/mục trong mỗi bản đồ đặc trưng đầu ra). Sau khi nhân các ma trận đó, chúng ta định hình lại chúng thành hình dạng mong muốn để có được các bản đồ đặc trưng đầu ra mong muốn.

A Chi tiết Phương pháp Bổ sung

Trong phần này, chúng tôi cung cấp thêm chi tiết liên quan đến phương pháp của chúng tôi.

A.1 Kiến thức Cơ bản Phương pháp

Kỹ thuật nén theo từng lớp của chúng tôi dựa trên hiểu biết rằng bất kỳ lớp tuyến tính nào có thể được biểu diễn như phép nhân ma trận, điều này cho phép chúng tôi dựa vào SVD như subroutine nén. Tập trung vào tích chập, chúng tôi cho thấy cách một lớp như vậy có thể được biểu diễn lại như phép nhân ma trận. Các phương pháp tương tự đã được sử dụng bởi Denton et al. (2014); Idelbayev và Carreira-Perpinán (2020); Wen et al. (2017) cùng những người khác.

Tích chập đến phép nhân ma trận. Đối với một lớp tích chập đã cho của f bộ lọc, c kênh, kernel l1×l2 và bản đồ đặc trưng đầu vào với c đặc trưng, mỗi kích thước m1×m2, chúng tôi ký hiệu tensor trọng số bằng W ∈ Rf×c×l1×l2 và tensor đầu vào bằng X ∈ Rc×m1×m2. Hơn nữa, gọi W̄ ∈ Rf×cl1l2 biểu thị toán tử ma trận được mở của lớp được xây dựng từ W bằng cách làm phẳng c×l1×l2 kernel của mỗi bộ lọc thành một hàng và xếp chồng các hàng để tạo thành một ma trận. Cuối cùng, gọi p biểu thị tổng số khối trượt và X̄ ∈ Rcl1l2×p biểu thị ma trận đầu vào được mở, được xây dựng từ tensor đầu vào X như sau: trong khi mô phỏng tích chập bằng cách trượt W dọc theo X, chúng tôi trích xuất các khối cục bộ trượt của X trên tất cả các kênh bằng cách làm phẳng mỗi khối thành vector cột cl1×l2 chiều và nối chúng lại với nhau để tạo thành X̄. Như được minh họa trong Hình 8, bây giờ chúng ta có thể biểu diễn tích chập Y = W ⊛ X như phép nhân ma trận Ȳ = W̄X̄, trong đó Y ∈ Rf×p1×p2 và Ȳ ∈ Rf×p tương ứng với biểu diễn tensor và ma trận của các bản đồ đặc trưng đầu ra, và p1, p2 biểu thị các chiều không gian của Y. Sự tương đương của Y và Ȳ có thể dễ dàng được thiết lập thông qua phép định hình lại thích hợp vì p = p1p2.

Phân tách tensor hiệu quả thông qua SVD. Được trang bị khái niệm tương ứng giữa tích chập và phép nhân ma trận, mục tiêu của chúng tôi là phân tách lớp thông qua toán tử ma trận W̄ ∈ Rf×cl1l2. Để đạt được điều này, chúng tôi tính toán xấp xỉ hạng j của W̄ sử dụng SVD và phân tích nó thành một cặp ma trận nhỏ hơn U ∈ Rf×j và V ∈ Rj×cl1l2. Chi tiết hơn về cách tính toán U và V được đưa ra trong Phần A. Sau đó chúng tôi có thể thay thế tích chập gốc, được biểu diễn bởi W̄, bằng hai tích chập nhỏ hơn, được biểu diễn bởi V và U cho lớp đầu tiên và thứ hai, tương ứng. Giống như lớp gốc, chúng ta có thể thiết lập một lớp tích chập tương đương cho cả U và V như được mô tả trong Hình 9. Để thiết lập sự tương đương, chúng tôi lưu ý rằng (a) mỗi hàng của các ma trận V và U tương ứng với một bộ lọc được làm phẳng của tích chập tương ứng, và (b) số lượng kênh trong mỗi lớp bằng số lượng kênh trong tensor đầu vào tương ứng của nó. Do đó, lớp đầu tiên, được biểu diễn bởi V ∈ Rj×cl1l2 có j bộ lọc, c.f. (a), mỗi bộ gồm c kênh, c.f. (b), với kích thước kernel l1×l2. Lớp thứ hai tương ứng với U có f bộ lọc, c.f. (a), j kênh, c.f. (b), và kernel 1×1 và có thể được biểu diễn tương đương như tensor U ∈ Rf×j×1×1. Lưu ý rằng số lượng trọng số được giảm từ f×c×l1×l2 xuống j(f + c×l1×l2).

A.2 Phương pháp Phân cụm

Như đã giải thích trước đây trong Phần 2.1, người ta có thể phân cụm các cột của ma trận trọng số tương ứng W̄, thay vì phân cụm các kênh của lớp tích chập. Ở đây, phân cụm kênh có thể được định nghĩa như phân cụm có ràng buộc của các cột này, trong đó các cột bao gồm các mục tương ứng với cùng một kernel (ví dụ, 4 cột đầu tiên trong W̄ từ Hình 9) được đảm bảo trong cùng một cụm.

Sự tổng quát hóa này dễ dàng thích ứng với các phương pháp phân cụm khác tạo ra tập hợp giải pháp rộng hơn, ví dụ, k-means đã biết. Một lựa chọn trực giác cho trường hợp của chúng tôi là phân cụm phép chiếu và các biến thể của nó. Mục tiêu của phân cụm phép chiếu là tính toán một tập hợp k không gian con, mỗi không gian có chiều j, cực tiểu hóa tổng bình phương khoảng cách từ mỗi cột trong W̄ đến không gian con gần nhất từ tập hợp này. Sau đó, chúng ta có thể phân chia các cột của W̄ thành k tập con theo không gian con gần nhất của chúng từ tập hợp này. Đây là một mở rộng tự nhiên của SVD giải quyết vấn đề này cho trường hợp k = 1. Tuy nhiên, vấn đề này được biết là NP-hard, do đó cần các thuật toán xấp xỉ đắt đỏ để giải quyết nó, hoặc thay thế, có thể thu được giải pháp cực tiểu cục bộ sử dụng phương pháp Expectation-Maximization (EM) (Dempster et al., 1977).

Một phiên bản vững chắc hơn của phương pháp trước đây là cực tiểu hóa tổng khoảng cách không bình phương từ các điểm đến các không gian con. Phương pháp này được biết là vững chắc hơn đối với các outlier ("điểm xa"). Tương tự như biến thể gốc (trường hợp khoảng cách bình phương), chúng ta có thể sử dụng phương pháp EM để có được "đoán tốt" cho giải pháp của vấn đề này, tuy nhiên, phương pháp EM yêu cầu một thuật toán giải quyết vấn đề cho trường hợp k = 1, tức là tính toán không gian con cực tiểu hóa tổng khoảng cách (không bình phương) từ các cột đó (đối với trường hợp tổng khoảng cách bình phương, SVD là thuật toán này). Thật không may, chỉ có các thuật toán xấp xỉ (Clarkson và Woodruff, 2015; Tukan et al., 2020) cho trường hợp này, và các phiên bản tất định đắt đỏ về thời gian chạy.

Hơn nữa, và có lẽ quan trọng hơn, tất cả các phương pháp này không thể được coi là nén có cấu trúc vì phân cụm tùy ý có thể yêu cầu xáo trộn lại các tensor đầu vào có thể dẫn đến sự chậm trễ đáng kể trong quá trình suy luận. Ví dụ, khi nén một lớp kết nối đầy đủ, phân cụm tùy ý có thể dẫn đến các nơ-ron không liên tiếp từ lớp đầu tiên được kết nối với cùng một nơ-ron trong lớp thứ hai, trong khi các nơ-ron ở giữa chúng thì không. Do đó, các lớp này chỉ có thể có biểu diễn lớn, thưa thớt thay vì nhỏ, dày đặc.

Để đạt được điều này, chúng tôi chọn sử dụng chia cắt kênh, tức là chúng tôi chỉ đơn giản chia các kênh của lớp tích chập thành k khối, trong đó mỗi khối có tối đa c/k kênh liên tiếp. Việc chia các kênh thành các tập con liên tiếp (mà không cho phép bất kỳ phân cụm tùy ý nào) và áp dụng phân tích nhân tử trên mỗi tập con dẫn đến một lớp nén có cấu trúc mà không cần hỗ trợ phần mềm/phần cứng đặc biệt. Hơn nữa, phương pháp này nhanh nhất trong số tất cả các phương pháp khác. Cuối cùng, trong khi các phương pháp khác có thể đưa ra đoán ban đầu tốt hơn cho mạng nén về lý thuyết, trong thực tế thì không phải như vậy; xem Hình 10. Điều này là do khung nén toàn cục (Phần 2.2) sử dụng lặp đi lặp lại việc chia cắt kênh.

Chúng ta thấy rằng trong thực tế, phương pháp của chúng tôi cải thiện so với các kỹ thuật hiện đại và thu được các mạng nhỏ hơn với độ chính xác cao hơn mà không sử dụng những phương pháp phức tạp có thể dẫn đến mạng thưa thớt nhưng không nhỏ hơn; xem Phần C.3.

A.3 Nén thông qua SVD

Như đã giải thích trong Phần 2.1, các trọng số của một lớp tích chập (hoặc lớp dày đặc) có thể được mã hóa thành ma trận W ∈ Rf×ck1k2, trong đó f và c là số lượng bộ lọc và kênh trong lớp, tương ứng, và k1k2 là kích thước của kernel. Để nén ma trận W (và do đó lớp tương ứng của nó), chúng tôi nhằm phân tích nó thành một cặp ma trận nhỏ hơn U ∈ Rf×j và V ∈ Rj×ck1k2, sao cho UV xấp xỉ toán tử ma trận gốc W.

Cách tính toán các ma trận U và V? Để đơn giản, gọi d = ck1k2. Chúng tôi phân tích ma trận W ∈ Rf×d thông qua SVD để có được W = ŨΣ̃Ṽ, trong đó Ũ là ma trận trực giao f×f, Σ̃ là ma trận đường chéo hình chữ nhật f×d với các số thực không âm trên đường chéo, và Ṽ là ma trận trực giao d×d.

Để tính toán xấp xỉ hạng j của W, chúng ta có thể đơn giản định nghĩa 3 ma trận sau: U ∈ Rf×j, V ∈ Rj×d, và Σ ∈ Rj×j, trong đó U được xây dựng bằng cách lấy j cột đầu tiên của Ũ, V bằng cách lấy j hàng đầu tiên của Ṽ, và Σ là ma trận đường chéo sao cho j mục trên đường chéo của nó bằng j mục đầu tiên trên đường chéo của Σ̃. Bây giờ chúng ta có UΣV là xấp xỉ hạng j của W.

Cuối cùng, chúng ta có thể định nghĩa V̂ = ΣV để có được phân tích của xấp xỉ hạng j của W là UV̂.

A.4 Triển khai Hiệu quả của ALDS (Thuật toán 1)

Thuật toán của chúng tôi được đề xuất trong Phần 2.2 nhằm cực tiểu hóa lỗi tương đối tối đa ε := max`∈[L] ε` trên L lớp của mạng như đại diện cho chi phí thực sự, trong đó ε` là lỗi tương đối tối đa lý thuyết trong lớp thứ `:

ε` := ‖Ŵ` - W̄`‖/‖W̄`‖.

Thông qua Thuật toán 1, với mọi ` ∈ [L], chúng ta cần tính toán lặp đi lặp lại ε` như một hàm của j` và k`. Tại Dòng 4, chúng ta được cho một đoán cho các giá trị tối ưu của k1, ..., kL, và mục tiêu của chúng ta là tính toán các giá trị j1, ..., jL sao cho các lỗi kết quả ε1, ..., εL (xấp xỉ) bằng nhau để cực tiểu hóa lỗi tối đa max`∈[L] ε` trong khi đạt được tỷ lệ nén toàn cục mong muốn. Để đạt được điều này, chúng tôi đoán một giá trị cho ε và với k1, ..., kL đã cho, chọn j1, ..., jL tương ứng sao cho ε tạo thành ràng buộc trên chặt chẽ cho lỗi tương đối trong mỗi lớp. Dựa trên ngân sách hiện tại (và do đó tỷ lệ nén), bây giờ chúng ta có thể cải thiện đoán của chúng ta về ε, ví dụ, thông qua tìm kiếm nhị phân hoặc các loại thuật toán tìm nghiệm khác, cho đến khi chúng ta hội tụ đến một giá trị của ε tương ứng với tỷ lệ nén tổng thể mong muốn của chúng ta.

Sau đó, đối với mỗi lớp, chúng ta được cho các giá trị cụ thể của k` và j`, điều này ngụ ý rằng chúng ta được cho một ngân sách b` cho mỗi lớp ` ∈ [L]. Sau đó, chúng tôi gán lại số lượng không gian con k` và hạng j` của chúng cho mỗi lớp bằng cách lặp qua tập hợp hữu hạn các giá trị có thể cho k` (Dòng 7) và chọn kết hợp j`, k` cực tiểu hóa lỗi tương đối cho ngân sách lớp hiện tại b` (được tính toán trong Dòng 6).

Sau đó chúng tôi lặp đi lặp lại cả hai bước cho đến khi hội tụ (Dòng 3-8).

Do đó, thay vì tính toán chi phí của mỗi lớp tại mỗi bước, chúng ta có thể lưu một bảng tra cứu lưu trữ các lỗi ε` cho các giá trị có thể của k` và j` của mỗi lớp. Đối với mỗi lớp ` ∈ [L], chúng tôi lặp qua tập hợp hữu hạn các giá trị của k`, và chúng tôi chia ma trận W̄` thành k` ma trận (theo phương pháp chia cắt kênh được giải thích trong Phần 2.1), sau đó chúng tôi tính toán phân tích SVD của mỗi ma trận từ k` ma trận này, và cuối cùng, tính toán ε` tương ứng với một j` cụ thể (k` đã được cho) trong thời gian O(fd), trong đó f là số lượng hàng trong ma trận trọng số tương ứng với lớp thứ ` và d là số lượng cột.

Hơn nữa, thay vì tính toán mỗi tùy chọn của ε` trong thời gian O(fd), chúng tôi sử dụng ràng buộc trên được suy ra trong Định lý 1 để tính toán nó trong thời gian O(k) và lưu nó trong bảng tra cứu. Cụ thể, chúng ta có thể biểu diễn lỗi tương đối như một hàm của hạng và do đó chúng ta chỉ cần giải quyết SVD cơ bản cho mỗi lớp một lần cho mỗi giá trị của k`. Nếu không có Định lý 1, chúng ta sẽ cần tính toán lỗi tương đối (chuẩn toán tử) cho mỗi cặp j`, k` riêng biệt. Điều này sẽ dẫn đến sự chậm trễ đáng kể trong thời gian chạy của Thuật toán 1. Do đó, việc sử dụng kết hợp bảng tra cứu và ứng dụng Định lý 1 đảm bảo triển khai hiệu quả hơn của Thuật toán 1.

A.5 Thảo luận Bổ sung về ALDS (Thuật toán 1)

Dưới đây, chúng tôi bao gồm chi tiết bổ sung và làm rõ về Thuật toán 1.

Tổng quan
Ở cấp độ cao, Thuật toán 1 nhằm tìm một tối ưu cục bộ cho quy trình tối ưu hóa được mô tả trong Phương trình (8). Chúng tôi tối ưu hóa lặp đi lặp lại cho k1, ..., kL và j1, ..., jL. Bước mà chúng ta cố định tập hợp k và tối ưu hóa cho tập hợp j là Dòng 4, trong khi ở Dòng 7 chúng ta cố định ngân sách lớp và tối ưu hóa cho tập hợp k. Tại mỗi bước, mục tiêu được cực tiểu hóa. Do đó đối với một seed cố định, ALDS hội tụ đến một tối ưu cục bộ của (8). Sau đó chúng tôi lặp lại toàn bộ quy trình nhiều lần với các seed ngẫu nhiên khác nhau để cải thiện chất lượng của tối ưu cục bộ.

Dòng 4: OPTIMAL_RANKS(CR, k1, ..., kL)
Tại bước này, chúng ta được cho một đoán cho các giá trị tối ưu của k1, ..., kL, và mục tiêu của chúng ta là tính toán các giá trị j1, ..., jL cực tiểu hóa hàm mục tiêu được mô tả trong Phương trình (8), tức là lỗi tối đa max`∈[L] ε`, trong khi đạt được tỷ lệ nén toàn cục mong muốn CR.

Để tìm giải pháp tối ưu, chúng tôi lưu ý điều sau. Nhớ lại rằng k được cố định.
1. Lỗi tối đa được cực tiểu hóa chính xác khi tất cả các lỗi đều bằng nhau. Để thấy rằng điều này thực sự đúng, chúng ta có thể tiến hành bằng phản chứng. Giả sử chúng ta tìm thấy một giải pháp tối ưu trong đó tất cả các lỗi không bằng nhau. Sau đó chúng ta có thể sử dụng một số ngân sách nén của chúng ta để thêm nhiều tham số hơn vào lớp có lỗi tối đa trong khi loại bỏ cùng lượng tham số từ lớp có lỗi tối thiểu. Vì việc thêm nhiều tham số hơn cải thiện lỗi, chúng ta vừa hạ thấp lỗi tối đa bằng cách thêm nhiều tham số hơn vào lớp có lỗi tối đa. Do đó, điều này dẫn đến mâu thuẫn chứng minh tuyên bố ban đầu của chúng ta.

2. Một lỗi không đổi nhất định trên các lớp tương ứng với một tỷ lệ nén cố định. Điều này rất đơn giản để thấy. Cụ thể, đối với một lỗi lớp nhất định, chúng ta có thể tìm hạng tương ứng và hạng ngụ ý số lượng tham số mà lớp nén sẽ có. Điều này sau đó ngụ ý một tỷ lệ nén cố định. Hơn nữa, lưu ý rằng mối quan hệ này là đơn điệu.

Cả (1.) và (2.) cùng nhau ngụ ý rằng chúng ta có thể sử dụng tìm kiếm nhị phân hoặc một số thuật toán tìm nghiệm khác để xác định lỗi không đổi tương ứng cho tỷ lệ nén mong muốn OPTIMAL_RANKS. Giải pháp của tìm kiếm nhị phân của chúng ta sau đó sẽ là tập hợp j (hạng) tương ứng cho mỗi lớp cực tiểu hóa lỗi tối đa cho tỷ lệ nén mong muốn và tập hợp k đã cho (nhớ lại rằng chúng ta tối ưu hóa cho k riêng biệt).

Dòng 7: OPTIMAL_SUBSPACES(b`)
Bước này khá đơn giản để theo dõi. Đầu tiên, chúng tôi lưu ý rằng đối với bước này, chúng tôi tiến hành trên cơ sở từng lớp. Ở đây, đối với mỗi lớp, chúng ta được cho các giá trị cụ thể của k` và j`, điều này ngụ ý rằng chúng ta được cho một ngân sách b` cho mỗi lớp ` ∈ [L]. Sau đó, chúng tôi gán lại số lượng không gian con k` và hạng j` của chúng cho mỗi lớp như sau: Chúng tôi lặp qua tập hợp hữu hạn các giá trị có thể cho k`, đối với mỗi giá trị k` như vậy, chúng tôi chọn j` tương ứng sao cho tổng kích thước (số lượng tham số) của lớp này (xấp xỉ) bằng ngân sách đã cho b`. Bây giờ, đối với mỗi cặp ứng viên k` và j`, chúng tôi tính toán lỗi tương đối trên lớp này gây ra sau nén đối với các giá trị này. Cuối cùng, chúng tôi chọn kết hợp j`, k` cực tiểu hóa lỗi tương đối cho ngân sách lớp hiện tại b`. Sau đó chúng tôi loại bỏ các giá trị được tìm thấy cho j1, ..., jL và tối ưu hóa lại chúng trong lần lặp tiếp theo của OPTIMAL_RANKS.

Lưu ý rằng đối với OPTIMAL_SUBSPACES, không có mối quan hệ đơn điệu giữa giá trị của k` và lỗi tương ứng như có giữa giá trị của j` và lỗi. Do đó, chúng tôi tiến hành trên cơ sở từng lớp trong đó chúng tôi giữ ngân sách từng lớp không đổi trong OPTIMAL_SUBSPACES như được mô tả ở trên.

Tính tối ưu
Từ chi tiết của hai bước, rất rõ ràng rằng chi phí đang giảm ở mỗi bước trong quy trình tối ưu hóa và do đó chúng ta có thể kết luận rằng đối với mỗi seed ngẫu nhiên, Thuật toán 1 hội tụ đến một tối ưu cục bộ (tại thời điểm đó chi phí sẽ không tăng).

Nhận xét thêm
Lưu ý rằng ở trên đối với OPTIMAL_RANKS, chúng tôi giả định rằng các lỗi (ε`) là liên tục nhưng chúng thực sự rời rạc vì chúng là hàm của hạng là rời rạc. Tuy nhiên, miễn là chúng ta có thể đảm bảo rằng mục tiêu giảm ở mỗi lần lặp, chúng ta vẫn có thể đạt được cực tiểu cục bộ.

Thay thế, chúng ta có thể giải quyết phiên bản thả lỏng liên tục của vấn đề trên và sử dụng phương pháp làm tròn ngẫu nhiên để có được giải pháp xấp xỉ tối ưu.

Tuy nhiên, trong thực tế, chúng tôi thấy rằng không cần thiết phải thêm bước phức tạp bổ sung này vì đủ để mục tiêu chi phí giảm ở mỗi bước thời gian và chúng ta không thể hy vọng có được tối ưu toàn cục (chúng ta chỉ có thể xấp xỉ nó bằng cách lặp lại quy trình tối ưu hóa với nhiều seed ngẫu nhiên, mà chúng tôi làm, xem Thuật toán 1).

A.6 Mở rộng của ALDS

Như đã đề cập trong Phần 2.3, ALDS có thể dễ dàng được mở rộng cho bất kỳ tập hợp kỹ thuật nén hạng thấp mong muốn nào. Cụ thể, chúng ta có thể thay thế bước cục bộ của Dòng 7 bằng tìm kiếm trên các phương pháp khác nhau, ví dụ, phân tách Tucker, PCA, hoặc các sơ đồ nén SVD khác, và trả về phương pháp tốt nhất cho ngân sách đã cho. Nói chung, chúng ta có thể kết hợp ALDS với bất kỳ nén hạng thấp nào miễn là chúng ta có thể đánh giá hiệu quả lỗi từng lớp của sơ đồ nén. Lưu ý rằng điều này về cơ bản trang bị cho chúng ta một khung để tự động chọn kỹ thuật phân tách từng lớp hoàn toàn tự động.

Để đạt được điều này, chúng tôi kiểm tra một mở rộng của ALDS trong đó ngoài việc tìm kiếm trên nhiều giá trị của k`, chúng tôi đồng thời tìm kiếm trên các sơ đồ làm phẳng khác nhau để chuyển đổi tensor tích chập thành ma trận trước khi áp dụng SVD.

Như trước đây, gọi W ∈ Rf×c×l1×l2 biểu thị tensor trọng số cho lớp tích chập với f bộ lọc, c kênh đầu vào, và kernel l1×l2. Hơn nữa, gọi j biểu thị hạng mong muốn của phân tách. Chúng tôi xem xét các sơ đồ sau để tự động tìm kiếm:

• SƠ ĐỒ 0: làm phẳng tensor thành ma trận có hình dạng f×cl1l2. Các lớp phân tách tương ứng với tích chập j×c×l1×l2 theo sau bởi tích chập f×j×1×1. Đây là cùng sơ đồ như được sử dụng trong ALDS.

• SƠ ĐỒ 1: làm phẳng tensor thành ma trận có hình dạng fl1×cl2. Các lớp phân tách tương ứng với tích chập j×c×l1×l2 theo sau bởi tích chập f×j×1×1.

• SƠ ĐỒ 2: làm phẳng tensor thành ma trận có hình dạng fl2×cl1. Các lớp phân tách tương ứng với tích chập j×c×1×l1 theo sau bởi tích chập f×j×l2×1.

• SƠ ĐỒ 3: làm phẳng tensor thành ma trận có hình dạng fl1l2×c. Các lớp phân tách tương ứng với tích chập j×c×1×1 theo sau bởi tích chập f×j×l1×l2.

Chúng tôi ký hiệu phương pháp này bằng ALDS+ và cung cấp kết quả sơ bộ trong Phần C.4. Chúng tôi lưu ý rằng vì ALDS+ là sự tổng quát hóa của ALDS, hiệu suất của nó ít nhất bằng ALDS gốc. Hơn nữa, kết quả sơ bộ của chúng tôi thực sự cho thấy rằng mở rộng này cải thiện rõ ràng hiệu suất thực nghiệm của ALDS.

22

--- TRANG 16 ---
B Thiết lập Thí nghiệm và Siêu tham số

Các đánh giá thí nghiệm của chúng tôi dựa trên nhiều kiến trúc mạng, tập dữ liệu và pipeline nén khác nhau. Sau đây, chúng tôi cung cấp tất cả các siêu tham số cần thiết để tái tạo các thí nghiệm của chúng tôi cho mỗi tập dữ liệu và kiến trúc mạng tương ứng.

Tất cả các mạng được huấn luyện, nén và đánh giá trên cụm tính toán với GPU NVIDIA Titan RTX và NVIDIA RTX 2080Ti. Các thí nghiệm được tiến hành với PyTorch 1.7 và mã của chúng tôi hoàn toàn mã nguồn mở³.

Tất cả các mạng được huấn luyện theo các siêu tham số được phác thảo trong các bài báo gốc tương ứng. Trong quá trình huấn luyện lại, được mô tả trong Phần B.5, chúng tôi tái sử dụng cùng các siêu tham số.

Hơn nữa, mỗi thí nghiệm được lặp lại 3 lần và chúng tôi báo cáo trung bình và trung bình, độ lệch chuẩn trong các bảng và hình, tương ứng.

Đối với mỗi tập dữ liệu, chúng tôi sử dụng tập phát triển có sẵn công khai làm tập kiểm tra và sử dụng chia 90%/5%/5% trên tập huấn luyện để có được tập huấn luyện riêng biệt và hai tập xác thực. Một tập xác thực được sử dụng cho các phương pháp nén phụ thuộc dữ liệu, ví dụ, PCA (Zhang et al., 2015a); tập khác được sử dụng để dừng sớm trong quá trình huấn luyện.

B.1 Thiết lập Thí nghiệm cho CIFAR10

Tất cả các siêu tham số liên quan được phác thảo trong Bảng 3. Đối với mỗi mạng, chúng tôi sử dụng các siêu tham số huấn luyện được phác thảo trong các bài báo gốc tương ứng, tức là như được mô tả bởi He et al. (2016), Simonyan và Zisserman (2014), Huang et al. (2017), và Zagoruyko và Komodakis (2016) cho ResNet, VGG, DenseNet, và WideResNet (WRN), tương ứng.

Chúng tôi thêm một giai đoạn khởi động ở đầu trong đó chúng tôi mở rộng tuyến tính tốc độ học từ 0 đến tốc độ học danh nghĩa để đảm bảo hiệu suất huấn luyện phù hợp trong các thiết lập huấn luyện phân tán (Goyal et al., 2017).

Trong quá trình huấn luyện, chúng tôi sử dụng chiến lược tăng cường dữ liệu tiêu chuẩn cho CIFAR: (1) đệm không từ 32x32 đến 36x36; (2) cắt ngẫu nhiên đến 32x32; (3) lật ngang ngẫu nhiên; (4) chuẩn hóa theo kênh. Trong quá trình suy luận, chỉ có chuẩn hóa (4) được áp dụng.

Các tỷ lệ nén được chọn theo chuỗi hình học với tỷ số chung được ký hiệu bằng α trong Bảng 3, tức là tỷ lệ nén cho lần lặp i được xác định bởi 1 - αⁱ. Tham số nén nseed biểu thị số lượng seed được sử dụng để khởi tạo Thuật toán 1 để nén với PP.

B.2 Thiết lập Thí nghiệm cho ImageNet

Chúng tôi báo cáo các siêu tham số liên quan trong Bảng 4. Đối với ImageNet, chúng tôi xem xét các kiến trúc mạng Resnet18 (He et al., 2016), AlexNet (Krizhevsky et al., 2012), và MobileNetV2 (Sandler et al., 2018).

Trong quá trình huấn luyện, chúng tôi sử dụng tăng cường dữ liệu sau: (1) thay đổi kích thước và cắt ngẫu nhiên đến 224x224; (2) lật ngang ngẫu nhiên; (3) chuẩn hóa theo kênh. Trong quá trình suy luận, chúng tôi sử dụng cắt trung tâm đến 224x224 trước khi áp dụng (3).

Lưu ý rằng đối với MobileNetV2, chúng tôi triển khai tốc độ học ban đầu thấp hơn trong quá trình huấn luyện lại. Nếu không, tất cả các siêu tham số vẫn giữ nguyên trong quá trình huấn luyện lại.

³Kho mã: https://github.com/lucaslie/torchprune

23

--- TRANG 17 ---
Bảng 3: Các siêu tham số thí nghiệm cho huấn luyện, nén và huấn luyện lại cho các kiến trúc mạng CIFAR10 được kiểm tra. "LR" và "LR decay" ở đây biểu thị tốc độ học và (phép nhân) giảm tốc độ học, tương ứng, được triển khai tại các epoch như được chỉ định. "@{x,...}" cho biết rằng tốc độ học được giảm mỗi x epoch.

[THIS IS TABLE: CIFAR10 Hyperparameters showing details for VGG16, Resnet20, DenseNet22, and WRN-16-8 including training parameters, test accuracy, loss, optimizer settings, etc.]

Bảng 4: Các siêu tham số thí nghiệm cho huấn luyện, nén và huấn luyện lại cho các kiến trúc mạng ImageNet được kiểm tra. "LR" và "LR decay" ở đây biểu thị tốc độ học và (phép nhân) giảm tốc độ học, tương ứng, được triển khai tại các epoch như được chỉ định. "@{x,...}" cho biết rằng tốc độ học được giảm mỗi x epoch.

[THIS IS TABLE: ImageNet Hyperparameters showing details for ResNet18, AlexNet, and MobileNetV2 including training parameters, accuracy metrics, optimizer settings, etc.]

B.3 Thiết lập Thí nghiệm cho Pascal VOC

Ngoài CIFAR và ImageNet, chúng tôi cũng xem xét nhiệm vụ phân đoạn từ Pascal VOC 2012 Everingham et al. (2015). Chúng tôi tăng cường dữ liệu huấn luyện danh nghĩa sử dụng các nhãn bổ sung như được cung cấp bởi Hariharan et al. (2011). Là kiến trúc mạng, chúng tôi xem xét DeeplabV3 Chen et al. (2017) với backbone ResNet50 được huấn luyện trước trên ImageNet.

Trong quá trình huấn luyện, chúng tôi sử dụng pipeline tăng cường dữ liệu sau: (1) thay đổi kích thước ngẫu nhiên (256x256 đến 1024x1024) và cắt đến 513x513; (2) lật ngang ngẫu nhiên; (3) chuẩn hóa theo kênh. Trong quá trình suy luận, chúng tôi thay đổi kích thước chính xác đến 513x513 trước khi chuẩn hóa (3) được áp dụng.

Chúng tôi báo cáo cả độ chính xác intersection-over-union (IoU) và Top1 cho mỗi mạng nén và không nén. Các siêu tham số thí nghiệm được tóm tắt trong Bảng 5.

B.4 Phương pháp Cơ sở

Chúng tôi triển khai và so sánh với các phương pháp nén sau cho các thí nghiệm cơ sở của chúng tôi:

1. PCA (Zhang et al., 2015a) phân tách mỗi lớp dựa trên phân tích thành phần chính của tiền kích hoạt (đầu ra của lớp tuyến tính). Chúng tôi triển khai phiên bản đối xứng, tuyến tính của phương pháp của họ. Tỷ lệ nén từng lớp dựa trên giải pháp tham lam để cực tiểu hóa

24

--- TRANG 18 ---
Bảng 5: Các siêu tham số thí nghiệm cho huấn luyện, nén và huấn luyện lại cho kiến trúc mạng VOC được kiểm tra. "LR" và "LR decay" ở đây biểu thị tốc độ học và giảm tốc độ học, tương ứng. Lưu ý rằng tốc độ học được giảm đa thức sau mỗi bước.

[THIS IS TABLE: Pascal VOC 2012 - Segmentation Hyperparameters for DeeplabV3-ResNet50, showing various training parameters including IoU Test accuracy (69.84%), Top 1 Test accuracy (94.25%), and other settings like loss, optimizer, epochs, etc.]

tích của năng lượng từng lớp, trong đó năng lượng được định nghĩa là tổng các giá trị kỳ dị trong lớp nén, xem Phương trình (14) của Zhang et al. (2015a).

2. SVD-ENERGY (Alvarez và Salzmann, 2017; Wen et al., 2017) phân tách mỗi lớp thông qua gấp ma trận tương tự như phân tách dựa trên SVD của chúng tôi. Tỷ lệ nén từng lớp được tìm thấy bằng cách giữ mức giảm năng lượng tương đối không đổi trên các lớp, trong đó năng lượng được định nghĩa là tổng các giá trị kỳ dị bình phương.

3. SVD (Denton et al., 2014) phân tách mỗi lớp thông qua gấp ma trận tương tự như phân tách dựa trên SVD của chúng tôi. Tuy nhiên, chúng tôi cố định k` = 1 cho tất cả các lớp ` ∈ [L] để cung cấp so sánh danh nghĩa tương tự như phân tách tensor "tiêu chuẩn". Tỷ lệ nén từng lớp được giữ không đổi trên tất cả các lớp.

4. L-RANK (Idelbayev và Carreira-Perpinán, 2020) phân tách mỗi lớp thông qua gấp ma trận tương tự như phân tách dựa trên SVD của chúng tôi. Nén từng lớp được xác định bằng cách cực tiểu hóa mục tiêu chi phí kết hợp của năng lượng và chi phí tính toán của mỗi lớp, xem Phương trình (5) của Idelbayev và Carreira-Perpinán (2020) để biết chi tiết.

5. FT Li et al. (2016) cắt tỉa các bộ lọc (hoặc nơ-ron) trong mỗi lớp với chuẩn `2 theo từng phần tử thấp nhất. Tỷ lệ nén từng lớp được đặt thủ công (không đổi trong triển khai của chúng tôi).

6. PFP Liebenwein et al. (2020) cắt tỉa các kênh với độ nhạy thấp nhất, trong đó độ nhạy phụ thuộc dữ liệu dựa trên khái niệm có thể chứng minh về cắt tỉa kênh. Tỷ lệ cắt tỉa từng lớp được xác định dựa trên các đảm bảo lỗi lý thuyết liên quan.

B.5 Pipeline Nén-Huấn luyện lại

Nhớ lại rằng các thí nghiệm cơ sở của chúng tôi dựa trên pipeline nén-huấn luyện lại thống nhất sau đây trên tất cả các phương pháp nén:

1. HUẤN LUYỆN trong e epoch theo lịch trình huấn luyện tiêu chuẩn cho mạng tương ứng.
2. NÉN mạng theo phương pháp đã chọn.
3. HUẤN LUYỆN LẠI mạng trong r epoch mong muốn sử dụng các siêu tham số huấn luyện gốc từ các epoch trong phạm vi [e-r, e].
4. LẶP LẠI các bước 1.-3. sau khi chiếu các lớp phân tách trở lại (tùy chọn).

Ngoài ra, chúng tôi cũng xem xét các thí nghiệm trong thiết lập tua lại tốc độ học lặp đi lặp lại, trong đó các bước 2 và 3 được lặp lại một cách lặp đi lặp lại (bước 4 tùy chọn).

25

--- TRANG 19 ---
Trong khi các bài báo khác nhau kết hợp các phương pháp nén của họ với các lịch trình huấn luyện lại khác nhau, chúng tôi thống nhất pipeline nén-huấn luyện lại trên tất cả các phương pháp được kiểm tra cho các thí nghiệm cơ sở của chúng tôi để đảm bảo rằng kết quả có thể so sánh được. Lưu ý rằng pipeline nén-huấn luyện lại được triển khai như được giới thiệu ban đầu bởi Renda et al. (2020) đã được chỉ ra mang lại kết quả nén tốt một cách nhất quán trên các thiết lập nén/cắt tỉa khác nhau (không có cấu trúc, có cấu trúc) và nhiệm vụ (thị giác máy tính, NLP). Do đó, chúng tôi chọn tập trung vào pipeline cụ thể đó.

C Kết quả Thí nghiệm Bổ sung

Trong phần này, chúng tôi cung cấp kết quả bổ sung của các đánh giá thí nghiệm của chúng tôi.

C.1 Bảng Đầy đủ cho Thí nghiệm Nén Một lần từ Phần 3.1

[THIS IS TABLE: Complex table showing compression ratios for different models on CIFAR10 with various accuracy thresholds (0.0%, 0.5%, 1.0%, 2.0%, 3.0%). The table includes results for ResNet20, VGG16, DenseNet22, and WRN16-8 models with different pruning methods (ALDS, PCA, SVD-Energy, SVD, L-Rank, FT, PFP). Values shown are for Top1 accuracy, CR-P (compression ratio parameters), and CR-F (compression ratio FLOPs).]

Bảng 6: Tỷ lệ nén tối đa mà sụt giảm độ chính xác kiểm tra nhiều nhất là một mức được chỉ định trước δ trên CIFAR10. Bảng báo cáo tỷ lệ nén về tham số và FLOP, được ký hiệu bằng CR-P và CR-F, tương ứng. Khi δ mong muốn không đạt được cho bất kỳ tỷ lệ nén nào trong phạm vi, các trường được để trống. Các giá trị cao nhất đạt được cho CR-P và CR-F được in đậm.

[THIS IS TABLE: Second table showing compression ratios for ResNet20 on CIFAR10 with different amounts of retraining (r=0%e through r=100%e). Similar structure to the previous table but focusing on retraining effects.]

Bảng 7: Tỷ lệ nén tối đa mà sụt giảm độ chính xác kiểm tra nhiều nhất là δ = 1.0% cho ResNet20 (CIFAR10) với các lượng huấn luyện lại khác nhau (như được chỉ ra). Bảng báo cáo tỷ lệ nén về tham số và FLOP, được ký hiệu bằng CR-P và CR-F, tương ứng. Khi δ mong muốn không đạt được cho bất kỳ tỷ lệ nén nào trong phạm vi, các trường được để trống. Các giá trị cao nhất đạt được cho CR-P và CR-F được in đậm.

26

--- TRANG 20 ---
[THIS IS TABLE: Table 8 showing compression ratios for ResNet18 and MobileNetV2 on ImageNet with different accuracy thresholds]

[THIS IS TABLE: Table 9 showing compression ratios for ResNet18 on ImageNet with different retraining amounts]

[THIS IS TABLE: Table 10 showing compression ratios for DeeplabV3-ResNet50 on Pascal VOC2012]

C.2 Kết quả Điểm chuẩn ImageNet Đầy đủ từ Phần 3.2

Kết quả được cung cấp trong Bảng 11.

C.3 Nghiên cứu Phân tích

Để hiểu rõ hơn về các khía cạnh khác nhau của phương pháp chúng tôi, chúng tôi xem xét một nghiên cứu phân tích trong đó chúng tôi có chọn lọc tắt các tính năng khác nhau của ALDS. Cụ thể, chúng tôi so sánh phiên bản đầy đủ của ALDS với các biến thể sau:

1. ALDS-ERROR giải quyết cho các hạng tối ưu (Dòng 4 của Thuật toán 1) cho một tập hợp giá trị mong muốn cho k1, ..., kL. Chúng tôi kiểm tra k` = 3, ∀` ∈ [L]. Biến thể này kiểm tra lợi ích của việc thay đổi số lượng không gian con so với việc cố định chúng thành một giá trị mong muốn.

2. SVD-ERROR tương ứng với ALDS-Error với k` = 1, ∀` ∈ [L]. Biến thể này kiểm tra lợi ích của việc có nhiều không gian con ngay từ đầu trong bối cảnh phân bổ tỷ lệ nén từng lớp dựa trên lỗi.

27

--- TRANG 21 ---
[THIS IS TABLE: Table 11 showing AlexNet and ResNet18 Benchmarks on ImageNet with Top-1, Top-5 accuracy and reduction percentages]

3. ALDS-SIMPLE chọn các hạng trong mỗi lớp cho một tập hợp giá trị mong muốn của k1, ..., kL sao cho tỷ lệ nén từng lớp là không đổi. Chúng tôi kiểm tra k` = 3, ∀` ∈ [L], và k` = 5, ∀` ∈ [L]. Biến thể này kiểm tra lợi ích của việc phân bổ tỷ lệ nén từng lớp theo lỗi lớp so với phương pháp heuristic không đổi đơn giản.

4. MESSI tiến hành như ALDS-Simple nhưng thay thế phân cụm không gian con bằng phân cụm phép chiếu (Maalouf et al., 2021). Chúng tôi kiểm tra k` = 3, ∀` ∈ [L]. Biến thể này kiểm tra bất lợi của việc có kỹ thuật phân cụm không gian con đơn giản (chia cắt kênh) so với việc sử dụng kỹ thuật tinh vi hơn.

Chúng tôi lưu ý rằng ALDS-Simple với k` = 1, ∀` ∈ [L] tương ứng với phương pháp so sánh SVD từ các phần trước.

Chúng tôi nghiên cứu các biến thể trên ResNet20 được huấn luyện trên CIFAR10 trong hai thiết lập: chỉ nén và nén một lần + huấn luyện lại. Kết quả được trình bày trong Hình 10. Chúng tôi nhấn mạnh rằng biến thể hoàn chỉnh của thuật toán chúng tôi (ALDS) luôn vượt trội so với các biến thể yếu hơn, cung cấp bằng chứng thực nghiệm về tính hiệu quả của từng thành phần cốt lõi của ALDS.

28

--- TRANG 22 ---
[THIS IS FIGURE: Three graphs showing test accuracy differences for various compression methods on ResNet20, CIFAR10 with different retraining scenarios]

Hình 10: Sự khác biệt về độ chính xác kiểm tra ("Delta Top1 Test Accuracy") cho các tỷ lệ nén mục tiêu khác nhau, các phương pháp dựa trên ALDS/liên quan đến ALDS, và mạng trên CIFAR10.

Chúng tôi lưu ý rằng việc thay đổi số lượng không gian con cho mỗi lớp để gán tối ưu một giá trị k` trong mỗi lớp là rất quan trọng trong việc cải thiện hiệu suất của chúng tôi. Điều này rõ ràng từ so sánh giữa ALDS, ALDS-Error, và SVD-Error: việc có giá trị cố định cho k mang lại kết quả không tối ưu.

Việc chọn khái niệm chi phí thích hợp (lỗi tương đối tối đa) hơn nữa được ưa thích hơn các phương pháp heuristic đơn giản như tỷ lệ nén không đổi từng lớp. Cụ thể, sự khác biệt chính giữa ALDS-Error và ALDS-Simple là cách thức xác định các hạng cho một tập hợp k đã cho: ALDS-Error tối ưu hóa cho hàm chi phí dựa trên lỗi trong khi ALDS-Simple dựa vào phương pháp heuristic tỷ lệ nén không đổi từng lớp đơn giản. Trong thực tế, ALDS-Error vượt trội so với ALDS-Simple trong tất cả các tình huống được kiểm tra.

Cuối cùng, chúng tôi kiểm tra bất lợi của việc sử dụng phương pháp phân cụm không gian con đơn giản. Để đạt được điều này, chúng tôi so sánh ALDS-Simple và Messi cho các giá trị cố định của k. Trong khi trong một số tình huống, đặc biệt là không có huấn luyện lại, Messi cung cấp cải thiện khiêm tốn so với ALDS-Simple, sự cải thiện là không đáng kể đối với hầu hết các thiết lập. Hơn nữa, lưu ý rằng Messi yêu cầu thuật toán xấp xỉ đắt đỏ như được giải thích trong Phần A.2. Điều này sẽ ngăn cản chúng tôi tích hợp Messi vào khung ALDS đầy đủ một cách hiệu quả về mặt tính toán. Tuy nhiên, như rõ ràng từ nghiên cứu phân tích, chúng tôi thể hiện những lợi ích hiệu suất nhất đối với các tính năng liên quan đến xem xét toàn cục thay vì cải thiện cục bộ, từng lớp. Ngoài ra, chúng tôi cũng nên lưu ý rằng Messi không phát ra tham số hóa lại có cấu trúc do đó yêu cầu phần mềm hoặc phần cứng chuyên biệt để có được tăng tốc. Do đó, chúng tôi có thể kết luận rằng chia cắt kênh là kỹ thuật phân cụm thích hợp trong bối cảnh của chúng tôi.

29

--- TRANG 23 ---
C.4 Mở rộng của ALDS

Chúng tôi kiểm tra và so sánh ALDS với ALDS+ (xem Phần A.6) để điều tra những lợi ích hiệu suất chúng ta có thể thu được từ việc tổng quát hóa bước cục bộ của chúng tôi để tìm kiếm trên nhiều sơ đồ phân tách. Chúng tôi chạy các thí nghiệm nén một lần chỉ nén trên ResNet20 (CIFAR10) và ResNet18 (ImageNet).

Kết quả được hiển thị trong Hình 11. Chúng tôi thấy rằng ALDS+ có thể tăng đáng kể sự đánh đổi hiệu suất-kích thước so với phương pháp ALDS tiêu chuẩn của chúng tôi. Điều này được mong đợi vì bằng cách tổng quát hóa bước cục bộ của ALDS, chúng tôi đang tăng không gian tìm kiếm của các giải pháp phân tách có thể. Sử dụng khung ALDS của chúng tôi, chúng ta có thể tìm kiếm hiệu quả và tự động trên không gian giải pháp tăng. Chúng tôi hình dung rằng những quan sát của chúng tôi sẽ khuyến khích nghiên cứu tương lai về khả năng không chỉ chọn tỷ lệ nén tối ưu từng lớp mà còn sơ đồ nén tối ưu.

[THIS IS FIGURE: Two graphs side by side showing test accuracy differences for ALDS+ vs ALDS on ResNet20 (CIFAR10) and ResNet18 (ImageNet)]

Hình 11: Sự khác biệt về độ chính xác kiểm tra ("Delta Top1 Test Accuracy") cho các tỷ lệ nén mục tiêu khác nhau, các phương pháp dựa trên ALDS/liên quan đến ALDS, và mạng trên CIFAR10. Các mạng được nén một lần và không được huấn luyện lại sau đó.

30
