# 2205.13571.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2205.13571.pdf
# Kích thước tệp: 2727789 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
VÉ SỐ THẤP HẠNG:
TÌM KIẾM MẠNG NEURAL THẤP HẠNG HIỆU QUẢ QUA
PHƯƠNG TRÌNH VI PHÂN MA TRẬN

Steffen Schotthöfer
Viện Công nghệ Karlsruhe
76131 Karlsruhe (Đức)
steffen.schotthoefer@kit.edu

Emanuele Zangrando
Viện Khoa học Gran Sasso
67100 L'Aquila (Ý)
emanuele.zangrando@gssi.it

Jonas Kusch
Đại học Innsbruck
6020 Innsbruck (Áo)
jonas.kusch1@gmail.com

Gianluca Ceruti
EPF Lausanne
1015 Lausanne (Thụy Sĩ)
gianluca.ceruti@epfl.ch

Francesco Tudisco
Viện Khoa học Gran Sasso
67100 L'Aquila (Ý)
francesco.tudisco@gssi.it

TÓM TẮT
Mạng neural đã đạt được thành công to lớn trong nhiều ứng dụng đa dạng. Tuy nhiên, dung lượng bộ nhớ và nhu cầu tính toán của chúng có thể khiến việc ứng dụng trở nên không thực tế trong các thiết lập có tài nguyên phần cứng hoặc năng lượng hạn chế. Trong công trình này, chúng tôi đề xuất một thuật toán mới để tìm kiếm các mạng con thấp hạng hiệu quả. Đáng chú ý là những mạng con này được xác định và điều chỉnh ngay trong giai đoạn huấn luyện và tổng thời gian cũng như tài nguyên bộ nhớ cần thiết cho cả việc huấn luyện và đánh giá đều được giảm đáng kể. Ý tưởng chính là hạn chế các ma trận trọng số vào một đa tạp thấp hạng và cập nhật các nhân tử thấp hạng thay vì ma trận đầy đủ trong quá trình huấn luyện. Để rút ra các cập nhật huấn luyện bị hạn chế trong đa tạp được quy định, chúng tôi sử dụng các kỹ thuật từ việc giảm bậc mô hình động cho phương trình vi phân ma trận. Điều này cho phép chúng tôi cung cấp các đảm bảo về xấp xỉ, ổn định và giảm dần. Hơn nữa, phương pháp của chúng tôi tự động và linh hoạt điều chỉnh các hạng trong quá trình huấn luyện để đạt được độ chính xác xấp xỉ mong muốn. Hiệu quả của phương pháp được đề xuất được chứng minh thông qua nhiều thí nghiệm số trên các mạng kết nối đầy đủ và tích chập.

1 Giới thiệu
Mặc dù cho thấy hiệu suất tuyệt vời về mặt kỷ lục phân loại, hầu hết các mạng neural tiên tiến đều đòi hỏi một lượng tính toán và lưu trữ bộ nhớ khổng lồ cho cả giai đoạn huấn luyện và đánh giá [27]. Những yêu cầu này không chỉ làm tăng chi phí cơ sở hạ tầng và tiêu thụ năng lượng, mà còn khiến việc triển khai mạng neural nhân tạo trên các cơ sở hạ tầng có tài nguyên hạn chế như điện thoại di động hoặc thiết bị thông minh trở nên không khả thi. Mặt khác, đã được biết rõ rằng trọng số của mạng chứa các cấu trúc và dư thừa có thể được khai thác để giảm chiều không gian tham số mà không ảnh hưởng đáng kể đến độ chính xác tổng thể [9, 3, 17].

Cắt tỉa mạng là một hướng nghiên cứu phổ biến giải quyết vấn đề này bằng cách loại bỏ các tham số dư thừa khỏi các mô hình đã được huấn luyện trước. Thông thường, mạng ban đầu lớn và chính xác, và mục tiêu là tạo ra một mạng nhỏ hơn với độ chính xác tương tự. Các phương pháp trong lĩnh vực này bao gồm thưa thớt hóa trọng số [20,24,49] và lượng tử hóa [60,10], với các kỹ thuật cắt tỉa khác nhau, bao gồm heuristic dựa trên tìm kiếm [24], học tăng cường [2,23] và thuật toán di truyền [43]. Các công trình gần đây hơn đã xem xét

Những tác giả này đóng góp ngang nhau cho công trình này. arXiv:2205.13571v2 [cs.LG] 18 Oct 2022

--- TRANG 2 ---
cắt tỉa trong quá trình huấn luyện, bằng cách xây dựng cắt tỉa như một bài toán tối ưu hóa dựa trên dữ liệu [20,26,27]. "Cắt tỉa động" kết quả này đưa về một giai đoạn huấn luyện bị ràng buộc tham số, tuy nhiên, cho đến nay chủ yếu tập trung vào yêu cầu trọng số thưa thớt hoặc nhị phân.

Thay vì áp đặt tính thưa thớt hoặc biến nhị phân, trong công trình này chúng tôi ràng buộc không gian tham số vào đa tạp của các ma trận thấp hạng. Các ma trận tham số của mạng neural và các ma trận dữ liệu lớn nói chung hiếm khi có hạng đầy đủ [53,56,48,15]. Việc ràng buộc các tham số này nằm trên một đa tạp được xác định bởi các ma trận thấp hạng do đó là một cách tiếp cận khá tự nhiên. Bằng cách diễn giải bài toán huấn luyện như một dòng gradient liên tục theo thời gian, chúng tôi đề xuất một thuật toán huấn luyện dựa trên việc mở rộng các thuật toán Xấp xỉ Thấp Hạng Động (DLRA) gần đây [5,6,4]. Cách tiếp cận này cho phép chúng tôi sử dụng các bộ tích phân số thấp hạng cho Phương trình Vi phân Thường (ODE) ma trận để có được các giai đoạn huấn luyện thuận và ngược được sửa đổi chỉ sử dụng các nhân tử hạng nhỏ trong biểu diễn thấp hạng của các ma trận tham số và ổn định đối với các giá trị kỳ dị nhỏ. Đây là một sự khác biệt nổi bật so với các sơ đồ huấn luyện thấp hạng "vanilla" thay thế gần đây [57,31] chỉ đơn giản phân tích các ma trận trọng số thành tích của hai nhân tử thấp hạng UV^T và áp dụng thuật toán giảm dần luân phiên trên hai biến U và V.

Chúng tôi thực hiện một số đánh giá thí nghiệm trên các mạng kết nối đầy đủ và tích chập cho thấy rằng mô hình huấn luyện thấp hạng động kết quả mang lại các kiến trúc mạng neural tham số thấp mà so với các đối tác hạng đầy đủ của chúng đều ít đòi hỏi hơn đáng kể về mặt lưu trữ bộ nhớ và đòi hỏi ít chi phí tính toán hơn nhiều để được huấn luyện. Hơn nữa, các mạng neural thấp hạng được huấn luyện đạt được độ chính xác có thể so sánh với kiến trúc đầy đủ gốc. Quan sát này gợi nhớ đến giả thuyết vé số được gọi - các mạng neural dày đặc chứa các mạng con thưa thớt đạt độ chính xác cao [17] - và gợi ý sự hiện diện của các vé trúng thấp hạng: các mạng con thấp hạng có hiệu suất cao của các mạng dày đặc. Đáng chú ý, chiến lược huấn luyện thấp hạng động của chúng tôi dường như có thể tìm thấy các vé trúng thấp hạng trực tiếp trong giai đoạn huấn luyện độc lập với việc khởi tạo.

2 Công trình liên quan về các phương pháp thấp hạng
Phân tích thừa số thấp hạng sử dụng SVD và các kỹ thuật phân tích ma trận khác đã được nghiên cứu rộng rãi trong các cộng đồng tính toán khoa học và học máy. Thách thức nén và tăng tốc các mạng neural quy mô lớn sử dụng các phương pháp thấp hạng đã khơi dậy sự quan tâm nghiên cứu rộng rãi trong những năm gần đây và đã có nỗ lực đáng kể hướng tới việc phát triển các chiến lược phân tích thừa số thấp hạng cho mạng neural sâu.

Các công trình trước đây có thể được phân loại thô vào các cách tiếp cận với hạng thấp cố định và hạng thấp biến đổi trong thời gian huấn luyện. Các cách tiếp cận hạng cố định phân tích các ma trận trọng số sử dụng SVD hoặc phân tích tensor của các mạng đã được huấn luyện trước và tinh chỉnh mạng được phân tích [50,12,55,38], ràng buộc các ma trận trọng số có hạng thấp cố định trong quá trình huấn luyện [30,57,31], hoặc tạo các lớp như một tổ hợp tuyến tính của các lớp có hạng khác nhau [29]. Do đó, những phương pháp này đưa hạng của việc phân tích ma trận như một siêu tham số khác cần được tinh chỉnh. Các phương pháp thích ứng hạng giảm thiểu vấn đề này bằng cách xác định và điều chỉnh tự động cấu trúc thấp hạng sau khi huấn luyện. Đặc biệt, [33,34] áp dụng các heuristic để xác định hạng của việc phân tích ma trận trước thời gian, trong khi [59] khuyến khích trọng số thấp hạng thông qua một hàm mất mát có phạt phụ thuộc vào các hạng ma trận được xấp xỉ.

Một số phương pháp đã được đề xuất gần đây để điều chỉnh các hạng của ma trận trọng số song song với giai đoạn huấn luyện mạng chính. Trong [40], các tác giả thiết lập việc huấn luyện mạng neural như một bài toán tối ưu hóa có ràng buộc với giới hạn trên về hạng của các trọng số, được giải quyết theo cách tiếp cận luân phiên dẫn đến một chương trình số nguyên hỗn hợp NP-khó. Các tác giả của [28] xây dựng một bài toán tối ưu hóa có ràng buộc tương tự dẫn đến một sơ đồ tối ưu hóa hỗn hợp rời rạc-liên tục giải quyết đồng thời các hạng và các phần tử của ma trận. Tuy nhiên, cả hai cách tiếp cận này đều đòi hỏi kiến thức về ma trận trọng số đầy đủ (và phân tích giá trị kỳ dị của nó) trong quá trình huấn luyện và tổng thể đòi hỏi tính toán nhiều hơn so với huấn luyện tiêu chuẩn.

Trong công trình này, chúng tôi vượt qua các vấn đề trên và đề xuất một thuật toán huấn luyện với yêu cầu bộ nhớ và tính toán giảm. Để đạt được điều này, chúng tôi diễn giải lại bài toán tối ưu hóa của mạng neural như một dòng gradient của các ma trận trọng số mạng và do đó như một ODE ma trận. Việc xây dựng liên tục này cho phép chúng tôi sử dụng những tiến bộ gần đây trong các phương pháp DLRA cho ODE ma trận nhằm phát triển giải pháp

--- TRANG 3 ---
của phương trình vi phân trên đa tạp thấp hạng. Ý tưởng chính của DLRA [35], có nguồn gốc từ nguyên lý biến phân Dirac-Frenkel [14,18], là xấp xỉ giải pháp thông qua phân tích thừa số thấp hạng và rút ra các phương trình phát triển cho các nhân tử riêng lẻ. Qua đó, giải pháp đầy đủ không cần được lưu trữ và chi phí tính toán có thể được giảm đáng kể. Để đảm bảo tính mạnh mẽ của phương pháp, các bộ tích phân ổn định đã được đề xuất trong [44] và [6]. Thay vì phát triển các nhân tử thấp hạng riêng lẻ theo thời gian, những phương pháp này phát triển tích của các nhân tử thấp hạng, mang lại các tính chất ổn định và chính xác đáng chú ý [32], cả trong thiết lập ma trận và tensor [36,46,45,8]. Trong công trình này, chúng tôi sử dụng bộ tích phân "cập nhật cơ sở phi truyền thống & bước Galerkin" [6] cũng như phần mở rộng thích ứng hạng của nó [4], xem thêm [37,7]. Bộ tích phân phi truyền thống thích ứng hạng chọn các hạng xấp xỉ theo động lực huấn luyện liên tục theo thời gian và cho phép chúng tôi tìm các mạng con thấp hạng có hiệu suất cao trực tiếp trong giai đoạn huấn luyện, đồng thời đòi hỏi chi phí huấn luyện và lưu trữ bộ nhớ giảm.

3 Huấn luyện thấp hạng qua dòng gradient
Xem xét một mạng neural kết nối đầy đủ thuận N(x) = z_M, với z_0 = x ∈ R^{n_0}, z_k = σ_k(W_k z_{k-1} + b_k) ∈ R^{n_k}, k = 1,...,M (thiết lập tích chập được thảo luận trong tài liệu bổ sung §6.6). Chúng tôi xem xét việc huấn luyện N dựa trên việc tối ưu hóa hàm mất mát L(W_1,...,W_M; N(x), y) bằng thuật toán giảm dần dựa trên gradient. Ví dụ, khi sử dụng giảm gradient, các trọng số của N tại lần lặp t ∈ N được cập nhật qua

W_k^{t+1} = W_k^t - α∇_{W_k}L(W_1,...,W_M; N(x), y) ∀k = 1,...,M (1)

với tốc độ học α. Khi các ma trận trọng số W_k dày đặc, cả việc đánh giá thuận và gradient của mạng đều đòi hỏi một số lượng lớn phép nhân ma trận đầy đủ, với chi phí tính toán cao và dung lượng bộ nhớ lớn. Điều này khiến việc huấn luyện và sử dụng mạng neural quy mô lớn trở thành thách thức khó khăn trên các thiết bị có tài nguyên hạn chế. Đồng thời, nhiều bằng chứng cho thấy rằng các mạng dày đặc thường được tham số hóa quá mức và hầu hết các trọng số học được theo cách này đều không cần thiết [48,15]. Để giảm chi phí bộ nhớ và tính toán của việc huấn luyện, chúng tôi đề xuất một phương pháp thực hiện việc tối thiểu hóa trên đa tạp của các ma trận thấp hạng.

Để đạt được điều này, chúng tôi giả định rằng W_k lý tưởng có thể được xấp xỉ tốt bởi một ma trận có hạng r_k ≪ min{n_k, n_{k+1}} dạng U_k S_k V_k^T ∈ R^{n_k × n_{k-1}}, trong đó U_k ∈ R^{n_k × r_k}, V_k ∈ R^{n_{k-1} × r_k} là các ma trận mỏng và cao có các cột trực chuẩn trải rộng các không gian con tối ưu nắm bắt các tính chất thiết yếu của tham số, và S_k ∈ R^{r_k × r_k} là một ma trận nhỏ hạng đầy đủ cho phép chúng tôi ngoại suy thông tin hữu ích từ các không gian con đã học U_k và V_k.

Các thuật toán giảm dần truyền thống như (1) không đảm bảo việc bảo toàn cấu trúc thấp hạng U_k S_k V_k^T khi cập nhật trọng số trong quá trình huấn luyện và đòi hỏi kiến thức về toàn bộ W_k thay vì các nhân tử U_k, S_k, V_k. Ở đây chúng tôi diễn giải lại việc tối thiểu hóa mất mát như một dòng gradient liên tục theo thời gian và rút ra một phương pháp huấn luyện mới vượt qua tất cả các hạn chế nêu trên.

Việc tối thiểu hóa hàm mất mát đối với W_k tương đương với việc đánh giá hành vi thời gian dài của ODE ma trận sau đây cho phép chúng tôi diễn giải giai đoạn huấn luyện như một quá trình liên tục:

Ẇ_k(t) = -∇_{W_k}L(W_1,...,W_M; N(x), y); (2)

trong đó "dấu chấm" biểu thị đạo hàm theo thời gian. Gọi M_{r_k} biểu thị đa tạp của các ma trận có hạng r_k và giả sử tại thời điểm t_0 nào đó các trọng số nằm trong đa tạp, tức là W_k(t_0) ∈ M_{r_k}. Việc sử dụng diễn giải liên tục theo thời gian này cho phép chúng tôi rút ra một chiến lược để phát triển các trọng số theo động lực trong (2) sao cho W_k(t) ∈ M_{r_k} cho mọi t ≥ t_0. Để đạt được điều này, trong §3.1 chúng tôi khai thác thực tế là W_k(t) thừa nhận một phân tích phụ thuộc thời gian [13] W_k(t) = U_k(t)S_k(t)V_k(t)^T để viết lại (2) như một hệ ODE ma trận cho mỗi nhân tử riêng lẻ U_k, S_k và V_k. Sau đó, trong §4 chúng tôi đề xuất một thuật toán để tích phân hiệu quả hệ ODE. Chúng tôi chỉ ra trong §4.1 rằng thuật toán như vậy giảm mất mát một cách đơn điệu và chính xác, theo nghĩa là U_k S_k V_k^T ≈ W_k, tức là các không gian con thấp chiều đã học U_k và V_k phù hợp tốt với hành vi của giải pháp mạng hạng đầy đủ W_k của (2), thông qua tác động của S_k đã học. Đáng chú ý, việc sử dụng động lực của các nhân tử riêng lẻ cũng sẽ cho phép chúng tôi điều chỉnh thích ứng hạng r_k trong suốt quá trình huấn luyện liên tục theo thời gian.

3

--- TRANG 4 ---
3.1 Động lực kết hợp của các nhân tử thấp hạng qua DLRA
Chúng tôi xem xét hệ động lực của một ma trận trọng số W_k đơn lẻ, trong khi các ma trận trọng số còn lại được cố định theo thời gian và được xem như các tham số cho gradient. Trong phần sau, chúng tôi bỏ qua việc viết các tham số này để hiệu quả trình bày. Giả sử W_k(t) ∈ M_{r_k}, chúng tôi có thể xây dựng (2) như

min_{Ẇ_k(t)} {‖Ẇ_k(t) + ∇_{W_k}L(W_k(t))‖_F : Ẇ_k(t) ∈ T_{W_k(t)}M_{r_k}} (3)

trong đó T_{W_k(t)}M_{r_k} là không gian tiếp tuyến của M_{r_k} tại vị trí W_k(t). Để giải (3), chúng tôi tiếp tục quan sát rằng (3) có thể được xây dựng tương đương như điều kiện Galerkin sau [35]:

⟨Ẇ_k(t) + ∇_{W_k}L(W_k(t)), W̃_k⟩ = 0 ∀W̃_k ∈ T_{W_k(t)}M_{r_k}. (4)

Từ W_k = U_k S_k V_k^T, một phần tử chung W̃_k của không gian tiếp tuyến T_{W_k(t)}M_{r_k} có thể được viết như

W̃_k = Ũ_k S_k V_k^T + U_k S̃_k V_k^T + U_k S_k Ṽ_k^T,

trong đó Ũ_k và Ṽ_k là các phần tử chung của không gian tiếp tuyến của đa tạp Stiefel với r_k cột trực chuẩn tại các điểm U_k và V_k, tương ứng, và S̃_k là một ma trận r_k × r_k chung, xem ví dụ [35, §2] để biết chi tiết. Ngoài ra, các điều kiện Gauge U_k^T Ũ_k = 0 và V_k^T Ṽ_k = 0 phải được áp đặt để đảm bảo tính trực giao của các ma trận cơ sở, và tính duy nhất của biểu diễn của các phần tử không gian tiếp tuyến. Tương tự, bằng quy tắc chuỗi được áp dụng nhiều lần, chúng tôi có

Ẇ_k = d/dt[U_k S_k V_k^T] = U̇_k S_k V_k^T + U_k Ṡ_k V_k^T + U_k S_k V̇_k^T.

Bây giờ, điều kiện Galerkin (4) trở thành

⟨U̇_k S_k V_k^T + U_k Ṡ_k V_k^T + U_k S_k V̇_k^T + ∇_{W_k}L(W_k(t)), W̃_k⟩ = 0, ∀W̃_k ∈ T_{W_k(t)}M_{r_k} (5)

với U_k^T U̇_k = 0 và V_k^T V̇_k = 0. Nếu chúng tôi chọn W̃_k = Ũ_k S̃_k Ṽ_k^T trong (5), chúng tôi nhận được

⟨U_k^T U̇_k S_k V_k^T V_k + U_k^T U_k Ṡ_k V_k^T V_k + U_k^T U_k S_k V̇_k^T V_k + U_k^T ∇_{W_k}L(W_k(t))V_k, S̃_k⟩ = 0.

Do đó, sử dụng các điều kiện Gauge, chúng tôi nhận được ⟨Ṡ_k + U_k^T ∇_{W_k}L(W_k(t))V_k, S̃_k⟩ = 0, phải đúng cho một ma trận r_k × r_k chung S̃_k. Chúng tôi nhận được theo cách này một phương trình phát triển cho nhân tử S_k(t). Tương tự, chỉ định (5) cho hai lựa chọn W̃_k = Ũ_k S_k V_k^T và W̃_k = U_k S_k Ṽ_k^T, cho phép chúng tôi nhận được hệ phương trình vi phân sau cho các nhân tử riêng lẻ của W_k:

{
Ṡ_k = -U_k^T ∇_{W_k}L(W_k(t))V_k;
U̇_k = -(I - U_k U_k^T)∇_{W_k}L(W_k(t))V_k S_k^{-1};
V̇_k = -(I - V_k V_k^T)∇_{W_k}L(W_k(t))^T U_k S_k^{-T}.
} (6)

4 Thuật toán huấn luyện dựa trên KLS
Để thực hiện một bước huấn luyện bị ràng buộc hạng hiệu quả và mạnh mẽ, chúng tôi tích phân số hệ ODE (6). Cách tiếp cận của chúng tôi dựa trên "bộ tích phân KLS phi truyền thống" [6] và phiên bản thích ứng hạng của nó [4]. Mã giả của chiến lược huấn luyện được đề xuất được trình bày trong Thuật toán 1. Ý tưởng chính của thuật toán KLS là luân phiên biểu diễn tích W_k = U_k S_k V_k^T như W_k = K_k V_k^T và W_k = U_k L_k^T, xem xét các ODE kết hợp tương ứng từ (6), và sau đó thực hiện ba bước chính:

1,2. Các bước K&L (song song). Cập nhật K_k và L_k hiện tại bằng cách tích phân các phương trình vi phân

K̇_k(t) = -∇_{W_k}L(K_k(t)V_k^T)V_k; K_k(0) = U_k S_k;
L̇_k(t) = -∇_{W_k}L(U_k L_k(t)^T)^T U_k; L_k(0) = V_k S_k^T; (7)

từ t = 0 đến t = τ; sau đó tạo các ma trận cơ sở trực chuẩn mới Ũ_k và Ṽ_k trải rộng phạm vi của K_k(τ) và L_k(τ) đã tính.

4

--- TRANG 5 ---
3. Bước S. Cập nhật S_k hiện tại bằng cách tích phân phương trình vi phân

Ṡ_k(t) = -Ũ_k^T ∇_{W_k}L(Ũ_k S_k(t)Ṽ_k^T)Ṽ_k (8)

từ t = 0 đến t = τ, với điều kiện giá trị ban đầu S_k(0) = Ũ_k^T U_k S_k V_k^T Ṽ_k.

Một tính năng quan trọng của thuật toán này là nó có thể được mở rộng đến khả năng thích ứng hạng một cách tương đối đơn giản [4], cho phép chúng tôi phát triển linh hoạt hạng của S_k (và do đó hạng của W_k) trong quá trình tính toán. Điều này đặc biệt hữu ích, vì chúng tôi có thể mong đợi các ma trận trọng số có hạng thấp nhưng chúng tôi có thể không biết "hạng tốt nhất" cho mỗi lớp là gì. Thông thường, việc điều chỉnh linh hoạt các hạng của một sơ đồ tối ưu hóa thấp hạng là một vấn đề thách thức vì việc di chuyển từ đa tạp M_{r_k} đến M_{r_k±1} giới thiệu các điểm kỳ dị [19,1]. Thay vào đó, việc xem bài toán huấn luyện như một hệ phương trình vi phân ma trận cho phép chúng tôi vượt qua vấn đề này bằng một thủ thuật đơn giản: tại mỗi bước của bộ tích phân KLS, chúng tôi nhân đôi kích thước của các ma trận cơ sở Ũ_k và Ṽ_k được tính trong các bước K- và L- bằng cách tính các cơ sở trực chuẩn trải rộng [K_k(τ)|U_k] và [L_k(τ)|V_k], tương ứng, tức là bằng cách bổ sung cơ sở hiện tại với cơ sở được tính trong bước thời gian trước đó. Sau đó, sau khi ma trận S_k mới được tính qua bước S, một bước cắt ngắn được thực hiện, loại bỏ khỏi ma trận S_k mới tính tất cả các giá trị kỳ dị dưới ngưỡng nhất định ε.

Tất nhiên, việc thêm khả năng thích ứng hạng vào bộ tích phân có chi phí. Trong trường hợp đó, mỗi bước đòi hỏi phải thực hiện phân tích SVD với kích thước gấp đôi hạng hiện tại của S để có thể đặt ngưỡng các giá trị kỳ dị. Hơn nữa, kích thước của các cơ sở U_k và V_k có thể tăng, điều này cũng có thể đòi hỏi nỗ lực tính toán bổ sung. Tuy nhiên, nếu các hạng vẫn nhỏ trong suốt động lực, chi phí tính toán này có thể bỏ qua, như chúng tôi sẽ thảo luận thêm trong §4.3 và §5.

4.1 Phân tích lỗi và hội tụ
Trong phần này chúng tôi trình bày các kết quả lý thuyết chính, cho thấy rằng (a) các ma trận thấp hạng U_k S_k V_k^T được hình thành bởi các nhân tử trọng số được tính với Thuật toán 1 gần với giải pháp thực của (2), và (b) rằng hàm mất mát giảm trong quá trình DLRT, với điều kiện ngưỡng giá trị kỳ dị ε không quá lớn, tức là được giới hạn bởi một hằng số nhân với bình phương kích thước bước thời gian (xem Định lý 1). Trong phiên bản chúng tôi trình bày ở đây, một phần các phát biểu được trình bày một cách không chính thức vì lý do ngắn gọn. Chúng tôi tham khảo tài liệu bổ sung §6.1 để biết chi tiết và các chứng minh.

Giả sử dòng gradient F_k(Z) = -∇_{W_k}L(W_1,...,Z,...,W_M; N(x), y) trong (2) được giới hạn cục bộ và liên tục Lipschitz cục bộ, với các hằng số C_1 và C_2, tương ứng. Khi đó,

Định lý 1. Cố định x và y, gọi W_k(t) là giải pháp liên tục theo thời gian (hạng đầy đủ) của (2) và gọi U_k, S_k, V_k là các nhân tử được tính với Thuật toán 1 sau t bước. Giả sử các bước K, L, S (7) và (8) được tích phân chính xác từ 0 đến τ. Giả sử hơn nữa rằng, đối với bất kỳ Z ∈ M_{r_k} đủ gần W_k(t), toàn bộ dòng gradient F_k(Z) "ε-gần" với M_{r_k}. Khi đó,

‖U_k S_k V_k^T - W_k(tτ)‖_F ≤ c_1ε + c_2τ + c_3ε = O(ε + τ) k = 1,...,M

trong đó các hằng số c_1, c_2 và c_3 chỉ phụ thuộc vào C_1 và C_2. Đặc biệt, giới hạn xấp xỉ không phụ thuộc vào các giá trị kỳ dị của giải pháp chính xác cũng như xấp xỉ.

Quan sát rằng, trong khi hàm mất mát L giảm đơn điệu dọc theo bất kỳ giải pháp liên tục theo thời gian W_k(t) nào của (2), không rõ ràng rằng mất mát giảm khi việc tích phân được thực hiện trên đa tạp thấp hạng qua Thuật toán 1. Kết quả tiếp theo cho thấy rằng điều này thực sự đúng, lên đến các số hạng bậc của dung sai cắt ngắn ε. Chính xác hơn, chúng tôi có

Định lý 2. Gọi W_k^t = U_k^t S_k^t (V_k^t)^T là ma trận trọng số hạng thấp được tính tại bước t của Thuật toán 1 và gọi L(t) = L(W_1^t,...,W_M^t; N(x), y). Khi đó, đối với bước thời gian đủ nhỏ, chúng tôi có

L(t+1) ≤ L(t) - βτ + γε

trong đó β và γ là các hằng số dương không phụ thuộc vào t, τ và ε.

4.2 Thực hiện hiệu quả các gradient
Tất cả ba bước K, L, S đều đòi hỏi việc đánh giá dòng gradient của hàm mất mát đối với toàn bộ ma trận W_k. Các cách tiếp cận khác nhau để tính toán hiệu quả gradient này có thể được sử dụng. Chiến lược chúng tôi

5

--- TRANG 6 ---
Thuật toán 1: Sơ đồ Huấn luyện Thấp Hạng Động (DLRT)
Đầu vào: Các nhân tử thấp hạng ban đầu S_k^0 ∈ R^{r_k^0 × r_k^0}; U_k^0 ∈ R^{n_k × r_k^0}; V_k^0 ∈ R^{n_{k-1} × r_k^0} cho k = 1,...,M;
iter: số lần lặp giảm dần tối đa mỗi epoch;
adaptive: cờ Boolean quyết định có động cập nhật các hạng hay không;
ε: ngưỡng giá trị kỳ dị cho quy trình thích ứng.

1 foreach epoch do
2   for t = 0 to t = iter do
3     foreach layer k do
4       K_k^t ← U_k^t S_k^t                        /* Bước K */
5       K_k^{t+1} ← one-step-integrate{K̇(t) = -∇_K L(K(t)(V_k^t)^T z_{k-1} + b_k^t), K(0) = K_k^t}
6       L_k^t ← V_k^t (S_k^t)^T                    /* Bước L */
7       L_k^{t+1} ← one-step-integrate{L̇(t) = -∇_L L(U_k^t L(t)^T z_{k-1} + b_k^t), L(0) = L_k^t}
8       if adaptive then                            /* Bước bổ sung cơ sở */
9         K_k^{t+1} ← [K_k^{t+1} | U_k^t]
10        L_k^{t+1} ← [L_k^{t+1} | V_k^t]
11      U_k^{t+1} ← orthonormal basis for the range of K_k^{t+1}  /* Bước S */
12      M_k ← (U_k^{t+1})^T U_k^t
13      V_k^{t+1} ← orthonormal basis for the range of L_k^{t+1}
14      N_k ← (V_k^{t+1})^T V_k^t
15      S̃_k^t ← M_k S_k^t N_k^T
16      S_k^{t+1} ← one-step-integrate{Ṡ(t) = -∇_S L(U_k^{t+1} S(t)(V_k^{t+1})^T z_{k-1} + b_k^t), S(0) = S̃_k^t}
17      if adaptive then                            /* Bước nén hạng */
18        P, Σ, Q ← SVD(S_k^{t+1})
19        S_k^{t+1} ← truncate using the singular value threshold ε
20        U_k^{t+1} ← U_k^{t+1} P̃ where P̃ = [first r_k^{t+1} columns of P]
21        V_k^{t+1} ← V_k^{t+1} Q̃ where Q̃ = [first r_k^{t+1} columns of Q]
                                                    /* Bước cập nhật bias */
22      b_k^{t+1} ← one-step-integrate{ḃ(t) = -∇_b L(U_k^{t+1} S_k^{t+1} (V_k^{t+1})^T z_{k-1} + b(t)); b(0) = b_k^t}

thảo luận dưới đây nhằm giảm chi phí bộ nhớ và tính toán bằng cách tránh việc tính toán gradient đầy đủ, thay vào đó làm việc với gradient đối với các nhân tử thấp hạng.

Để đạt được điều này, chúng tôi lưu ý rằng đối với bước K, nó giữ ∇_{W_k} L(K_k(t)V_k^T)V_k = ∇_{K_k} L(K_k(t)V_k^T). Do đó, toàn bộ gradient có thể được tính thông qua một lần chạy thuận của mạng đối với K_k

z_k = σ_k(K_k(t)V_k^T z_{k-1} + b_k); k = 1,...,M (9)

và ghi lại gradient đối với K_k. Theo cách này, gradient đầy đủ không cần được tính toán và tổng chi phí tính toán bao gồm việc chạy một đánh giá thuận trong khi ghi lại gradient đối với K_k, tương tự như thuật toán lan truyền ngược truyền thống. Các bước L- và S- có thể được đánh giá hiệu quả theo cách tương tự, bằng cách đánh giá mạng trong khi ghi lại gradient đối với L_k và S_k, tương ứng. Do đó, thay vì một băng gradient đơn (hoặc đánh giá quy tắc chuỗi) của mạng ma trận trọng số đầy đủ, chúng tôi có ba băng gradient, một cho mỗi bước hạng thấp, mà dung lượng tính toán kết hợp ít hơn băng ma trận đầy đủ. Chúng tôi cung cấp các công thức chi tiết cho cả ba băng gradient trong tài liệu bổ sung §6.5.

4.3 Chi tiết thực hiện, chi phí tính toán và hạn chế
Mỗi bước của Thuật toán 1 đòi hỏi việc tính toán hai cơ sở trực chuẩn cho phạm vi của K_k^{t+1} và L_k^{t+1}.

6

--- TRANG 7 ---
Tất nhiên có các kỹ thuật khác nhau để tính toán các ma trận trực chuẩn như vậy. Trong việc thực hiện của chúng tôi, chúng tôi sử dụng thuật toán QR, được biết là một trong những cách tiếp cận hiệu quả và ổn định nhất cho mục đích này. Trong chiến lược thích ứng, các giá trị kỳ dị của S_k^{t+1} được cắt ngắn theo tham số ε. Để đạt được điều này, trong việc thực hiện của chúng tôi, chúng tôi sử dụng chuẩn Frobenius của Σ. Chính xác, chúng tôi cắt ngắn Σ = diag(σ_i) tại bước 19 của Thuật toán 1 bằng cách chọn ma trận con chính r × r nhỏ nhất sao cho (∑_{i=r+1}^∞ σ_i^2)^{1/2} ≤ ε.

Cuối cùng, one-step-integrate biểu thị một quy trình số tích phân ODE tương ứng từ thời gian t = 0 đến t = τ. Trong thực tế, có thể sử dụng các bộ tích phân số khác nhau, mà không ảnh hưởng đến khả năng của thuật toán giảm hàm mất mát (xem [4, Thm. 5]) trong khi duy trì cấu trúc thấp hạng. Trong việc thực hiện của chúng tôi, chúng tôi đã sử dụng hai phương pháp:

1. Euler rõ ràng. Phương pháp này áp dụng cho dòng gradient trùng với một bước của Stochastic Gradient Descent (SGD), áp dụng cho ba nhân tử K_k; L_k; S_k độc lập.

2. Adam. Ở đây chúng tôi chính thức tính toán các nhân tử mới bằng cách sửa đổi bước Euler rõ ràng như trong phương pháp tối ưu Adam. Lưu ý rằng SGD tăng tốc Nesterov được biết là trùng với một bộ tích phân ODE đa bước tuyến tính cụ thể [51]. Mặc dù Adam không trực tiếp tương ứng với một bộ tích phân số theo hiểu biết của chúng tôi, trong các thử nghiệm của chúng tôi, nó dẫn đến việc giảm mất mát nhanh hơn so với cả Euler (SGD) và SGD tăng tốc Nesterov.

Đối với cả hai lựa chọn, bước thời gian mục tiêu tương ứng với giá trị của tốc độ học, mà chúng tôi đặt là 0.2 cho Euler. Đối với Adam, chúng tôi sử dụng cập nhật động mặc định, đặt 0.001 làm giá trị bắt đầu.

Chi phí tính toán. Để có được chi phí tính toán và yêu cầu bộ nhớ tối thiểu cho bước K, thứ tự đánh giá K_k V_k^T z_{k-1} trong (9) rất quan trọng. Đầu tiên, chúng tôi tính ẑ := V_k^T z_{k-1} ∈ R^{r_k} đòi hỏi O(r_k n_{k-1}) phép toán. Thứ hai, chúng tôi tính K_k ẑ đòi hỏi O(r_k n_k) phép toán. Thêm số hạng bias và đánh giá hàm kích hoạt đòi hỏi O(n_k) phép toán. Do đó, kết hợp trên tất cả các lớp, chúng tôi có chi phí tiệm cận O(∑_k r_k(n_k + n_{k+1})). Việc ghi lại đánh giá thuận để tính gradient đối với K_k như đã thảo luận trong §4.2 không ảnh hưởng đến chi phí tiệm cận, tức là chi phí tính toán bước K tại lớp k giả sử một điểm dữ liệu x đòi hỏi C_K · ∑_k r_k(n_k + n_{k+1}) phép toán. Theo cách tương tự, chúng tôi có chi phí tính toán của các bước L- và S-, lại là C_{L,S} · ∑_k r_k(n_k + n_{k+1}). Hơn nữa, các phân tích QR được sử dụng trong bước K- và L- đòi hỏi O(∑_k r_k^2(n_k + n_{k-1})) phép toán và tính toán SVD trong bước cắt ngắn có chi phí trường hợp xấu nhất O(∑_k r_k^3). Do đó, giả sử r_k ≪ n_k, n_{k+1}, chi phí mỗi bước của phương pháp thấp hạng của chúng tôi là C_{DLRA} · ∑_k r_k^2(n_k + n_{k-1}), trái ngược với huấn luyện mạng dày đặc, đòi hỏi C_{dense} · ∑_k n_k n_{k+1} phép toán. Về chi phí bộ nhớ, lưu ý rằng chúng tôi chỉ cần lưu trữ r_k(1 + n_k + n_{k+1}) tham số mỗi lớp trong thuật toán, tương ứng với các ma trận S_k^t; U_k^t; V_k^t. Hơn nữa, ở cuối việc huấn luyện, chúng tôi có thể nén thêm bộ nhớ bằng cách lưu trữ tích của các nhân tử trọng số được huấn luyện U_k S_k, thay vì các ma trận riêng lẻ.

Hạn chế. Một yêu cầu cho hiệu quả của DLRT là r_k ≪ n_k, n_{k+1}. Khi ngưỡng cắt ngắn ε quá nhỏ, Thuật toán 1 không cung cấp lợi thế so với huấn luyện tiêu chuẩn. Điều này cũng được thể hiện bởi Hình 1. Hơn nữa, theo thuật ngữ của [54], DLRT được thiết kế để giảm chi phí huấn luyện tương ứng với tham số mô hình và bộ tối ưu. Để giảm thêm chi phí kích hoạt, DLRT có thể được kết hợp với các cách tiếp cận micro-batching hoặc checkpointing. Cuối cùng, việc lựa chọn ε giới thiệu một siêu tham số bổ sung hiện tại đòi hỏi kiến thức bên ngoài để điều chỉnh. Tuy nhiên, các thí nghiệm của chúng tôi trong §5 cho thấy rằng các giá trị tương đối lớn của ε mang lại hiệu suất cạnh tranh so với một số đường cơ sở, bao gồm huấn luyện tiêu chuẩn.

5 Kết quả Số
Chúng tôi minh họa hiệu suất của Thuật toán DLRT 1 trên một số trường hợp thử nghiệm. Mã được thực hiện trong cả Tensorflow (https://github.com/CSMMLab/DLRANet) và PyTorch (https://github.com/COMPiLELab/DLRT). Các mạng được huấn luyện trên CPU AMD Ryzen 9 3950 X và GPU Nvidia RTX 3090. Thời gian được đo trên thực thi CPU thuần túy.

5.1 Phân tích hiệu suất trên tập dữ liệu MNIST
Chúng tôi phân chia tập dữ liệu MNIST [11] thành các tập huấn luyện-xác thực-kiểm tra được lấy mẫu ngẫu nhiên có kích thước 50K-10K-10K. Hình ảnh được chuẩn hóa theo pixel; không sử dụng thêm tăng cường dữ liệu hoặc chính quy hóa.

7

--- TRANG 8 ---
(a) Thời gian huấn luyện
(b) Thời gian dự đoán

Hình 1: So sánh thời gian thực thi batch và huấn luyện của mạng thấp hạng 5 lớp, 5120 neuron với các hạng khác nhau và một mạng tham chiếu không phân tích với cùng kiến trúc trên tập dữ liệu MNIST. Thời gian huấn luyện được hiển thị tương ứng với một epoch cho một batch 256 điểm dữ liệu. Thời gian dự đoán thay vào đó tham chiếu đến toàn bộ tập dữ liệu. Tất cả các thời gian là trung bình trên 1000 lần chạy.

Thời gian mạng kết nối đầy đủ feed-forward hạng cố định. Đầu tiên, chúng tôi so sánh thời gian huấn luyện của Thuật toán DLRT thích ứng 1 trên mạng kết nối đầy đủ 5 lớp [5120;5120;5120;5120;10] cố định hạng của các lớp 1-4, tức là chọn một hạng bắt đầu r_k^0 cụ thể cho các nhân tử trọng số đầu vào và cắt ngắn Σ tại dòng 19 của Thuật toán 1 thành ma trận con chính r_k^0 × r_k^0, thay vì qua ngưỡng. Tiếp theo, chúng tôi đo thời gian dự đoán trung bình trên toàn bộ tập dữ liệu MNIST trên 1000 lần chạy. Hình 1(a) và 1(b) cho thấy rằng cả hai thời gian đều mở rộng tuyến tính với hạng của các phân tích, và rằng đối với các hạng đủ nhỏ, DLRT nhanh hơn so với đường cơ sở hạng đầy đủ cả về mặt huấn luyện và dự đoán.

Phát triển hạng và hiệu suất của DLRT cho các ngưỡng giá trị kỳ dị khác nhau. Tiếp theo, chúng tôi chứng minh khả năng của DLRT xác định hạng của các ma trận trọng số mạng tự động trong quá trình huấn luyện mạng sử dụng Thuật toán 1. Bộ tối ưu Adam với tốc độ học mặc định được sử dụng cho cập nhật gradient. Chúng tôi huấn luyện mạng kết nối đầy đủ 5 lớp, trong đó 4 lớp đầu được thay thế bằng các lớp thấp hạng trong các thử nghiệm tiếp theo. Hàm kích hoạt được chọn là ReLU cho các lớp ẩn, và softmax cho lớp đầu ra. Hàm mất mát huấn luyện là sparse categorical cross entropy và chúng tôi đo thêm độ chính xác của mô hình. Chúng tôi sử dụng kích thước batch 256 và huấn luyện trong 250 epoch. Chúng tôi chọn ε = λ‖Σ‖, do đó chúng tôi cắt ngắn các giá trị kỳ dị của S_k^t hiện tại bằng một phần của tổng chuẩn Frobenius. λ càng nhỏ, càng nhiều giá trị kỳ dị được giữ lại.

Hình 2 (a) và (b) cho thấy sự phát triển của các lớp thích ứng hạng của mạng 5 lớp 500-neuron trong một nghiên cứu trường hợp thời gian dài cho λ = 0.05 và λ = 0.15. Chúng tôi có thể thấy rằng trong epoch đầu tiên, các hạng ma trận đầy đủ ban đầu được giảm đáng kể, xuống 27 cho λ = 0.15, và xuống 85 cho λ = 0.05 tương ứng. Trong 50 epoch đầu tiên, các hạng lớp đã gần với các hạng cuối cùng của chúng. Điều này cho thấy rằng thuật toán thích ứng hạng chỉ cần thiết cho vài epoch huấn luyện đầu tiên, và sau đó có thể được thay thế bằng huấn luyện thấp hạng cố định rẻ hơn về mặt tính toán (bằng cách đặt biến Boolean adaptive thành False trong Thuật toán 1). Hình 3 so sánh độ chính xác kiểm tra trung bình của mạng 5 lớp với 500 và 784 neuron với các mức nén thấp hạng khác nhau, trên năm lần chạy độc lập với các tập huấn luyện-kiểm tra-xác thực được lấy mẫu ngẫu nhiên. Các mạng có thể được nén qua huấn luyện thấp hạng động hơn 95%, trong khi chỉ mất ít hơn 1% độ chính xác kiểm tra so với mạng tham chiếu dày đặc được đánh dấu màu đỏ.

Lưu ý rằng việc hạn chế không gian của các mạng có thể đến một hạng cho trước sẽ điều chỉnh bài toán, vì hạn chế như vậy có thể được hiểu là thêm một số hạng chính quy hóa PCR vào hàm mất mát. Điều này có thể được thấy từ xu hướng không bị overfitting và đạt được độ chính xác kiểm tra được cải thiện so với mạng dày đặc tương ứng cho tỷ lệ nén vừa phải. Cũng lưu ý rằng huấn luyện thấp hạng thích ứng loại bỏ nhu cầu tìm kiếm lưới siêu tham số về mặt trọng số lớp, do việc điều chỉnh hạng tự động. Động lực hạng cho tất cả các cấu hình có thể được thấy trong tài liệu bổ sung §6.3. Cuối cùng, trong tài liệu bổ sung §6.4 chúng tôi so sánh việc sử dụng DLRT với cách tiếp cận vanilla chỉ đơn giản đặt ngưỡng các giá trị kỳ dị của mạng hạng đầy đủ. Kết quả của chúng tôi cho thấy rằng các vé trúng thấp hạng có lợi tồn tại, nhưng không dễ tìm. Thực tế, các mạng con thấp hạng vanilla hoạt động rất kém. Từ quan điểm này, cách tiếp cận của chúng tôi có thể được xem như một kỹ thuật cắt tỉa động hiệu quả, có thể xác định các mạng con thấp hạng có hiệu suất cao trong một mạng dày đặc cho trước. Đáng chú ý, các

8

--- TRANG 9 ---
(a) Phát triển hạng cho λ = 0.15
(b) Phát triển hạng cho λ = 0.05

Hình 2: Phát triển hạng (lớp 1-4) của mạng kết nối đầy đủ 5 lớp [500,500,500,500,10] trên MNIST.

Hình 3: Độ chính xác kiểm tra trung bình trên số lượng tham số và tỷ lệ nén cho 5 lần chạy với các tập huấn luyện-kiểm tra-xác thực được lấy mẫu ngẫu nhiên trên mạng kết nối đầy đủ 5 lớp. Các chấm đỏ biểu thị đường cơ sở hạng đầy đủ.

thí nghiệm số của chúng tôi gợi ý rằng các vé trúng thấp hạng có thể được huấn luyện từ đầu và không phụ thuộc nhiều vào dự đoán trọng số ban đầu.

Các lớp tích chập: LeNet5. Ở đây chúng tôi so sánh sơ đồ huấn luyện thấp hạng động được đề xuất trên LeNet5 [39] trên MNIST, với tham chiếu hạng đầy đủ và một số đường cơ sở. SVD prune [61] và LRNN [28] là các cách tiếp cận gần nhất với DLRT của chúng tôi: chúng huấn luyện động các lớp thấp hạng bằng cách thêm phạt hạng vào hàm mất mát, và bằng cách bổ sung bước huấn luyện tiêu chuẩn qua một bước chiếu SVD trong trường hợp sau và một bước cắt tỉa trong trường hợp trước. Trong khi tính toán các nhân tử thấp hạng cho mỗi lớp, do đó giảm lưu trữ bộ nhớ của mạng, cách tiếp cận huấn luyện này đắt hơn so với huấn luyện mạng đầy đủ. GAL [42], SSL [62], và NISP [58] là các phương pháp cắt tỉa nhằm học các trọng số thưa thớt tối ưu (thay vì thấp hạng) bằng cách thêm các số hạng chính quy hóa thúc đẩy thưa thớt vào hàm mất mát huấn luyện. Như đối với LRNN, các phương pháp này không giảm chi phí tính toán của giai đoạn huấn luyện (như được chỉ ra với <0% trong Bảng 1). Tương tự như [28], kỹ thuật huấn luyện thấp hạng thích ứng của chúng tôi được áp dụng cho các lớp tích chập bằng cách làm phẳng tensor đại diện cho kernel tích chập thành một ma trận. Chi tiết được cung cấp trong tài liệu bổ sung §6.6. Tất cả các mô hình được huấn luyện trong 120 epoch sử dụng SGD với tốc độ học cố định 0.2. Kết quả trong Bảng 1 cho thấy rằng thuật toán DLRT có thể tìm các mạng con thấp hạng với lên đến 96.4% ít tham số hơn so với tham chiếu đầy đủ, trong khi giữ độ chính xác kiểm tra trên 95%. So với các phương pháp đường cơ sở, chúng tôi đạt được tỷ lệ nén tốt hơn nhưng quan sát độ chính xác thấp hơn. Tuy nhiên, không giống như các tham chiếu đường cơ sở, DLRT tự động cắt tỉa các giá trị kỳ dị trong quá trình huấn luyện, mà không cần giải quyết bất kỳ bài toán tối ưu hóa bổ sung nào, do đó cải thiện đáng kể hiệu quả thời gian và bộ nhớ của cả giai đoạn thuận và ngược, so với tham chiếu đầy đủ.

Tính mạnh mẽ đối với các giá trị kỳ dị nhỏ và so sánh với tham số hóa thấp hạng vanilla. Một cách trực tiếp để thực hiện huấn luyện áp đặt hạng cố định cho các ma trận trọng số là tham số hóa mỗi trọng số như W_k = U_k V_k^T và luân phiên huấn luyện đối với U_k và V_k. Đây là chiến lược được sử dụng ví dụ trong [57,31]. Cách tiếp cận tham số hóa thấp hạng vanilla này có một số nhược điểm so với DLRT, ngoài việc lựa chọn hạng không thích ứng rõ ràng. Đầu tiên, DLRT đảm bảo xấp xỉ và giảm dần qua Định lý 1 và 2. Thứ hai, chúng tôi quan sát rằng phân tích vanilla tạo ra một phương pháp tối ưu hóa không ổn định khi các giá trị kỳ dị nhỏ xuất hiện.

9

--- TRANG 10 ---
[Hình biểu đồ với 4 đồ thị: 2 đồ thị trên cùng về Test Accuracy, 2 đồ thị dưới cùng về Train Loss, so sánh DLRT và UVT factorization trong điều kiện "no decay" và "decay"]

Hình 4: Đường cong học trung bình với độ lệch chuẩn của Lenet5 trên MNIST qua 10 lần chạy của DLRT so với phân tích lớp vanilla W_k = U_k V_k^T. Cả hai phương pháp đều được thực hiện với tốc độ học cố định 0.01, và kích thước batch 128. Các ma trận trọng số được khởi tạo hoàn toàn ngẫu nhiên ("no decay") hoặc được khởi tạo với lựa chọn ngẫu nhiên buộc có sự suy giảm theo cấp số nhân trên các giá trị kỳ dị ("decay").

Bảng 1: Kết quả huấn luyện LeNet5 trên tập dữ liệu MNIST. Tham số hiệu quả đại diện cho số lượng tham số chúng ta phải lưu để đánh giá mạng và những tham số chúng ta cần để huấn luyện qua Thuật toán DLRT 1. Tỷ lệ nén (c.r.) là tỷ lệ phần trăm giảm tham số so với mô hình đầy đủ (<0% chỉ ra rằng tỷ lệ âm). "ft" chỉ ra rằng mô hình đã được tinh chỉnh. "LeNet5" biểu thị kiến trúc LeNet5 tiêu chuẩn được huấn luyện với SGD.

[THIS IS TABLE: Bảng so sánh hiệu suất các phương pháp với độ chính xác test, hạng, số tham số và tỷ lệ nén cho cả đánh giá và huấn luyện]

Vấn đề này đặc trưng cho chính đa tạp thấp hạng [35,16], có độ cong cục bộ tỷ lệ nghịch với giá trị kỳ dị nhỏ nhất của các ma trận trọng số. Ngược lại, chiến lược tích phân số làm cơ sở cho DLRT được thiết kế để tận dụng cấu trúc của đa tạp và mạnh mẽ đối với các giá trị kỳ dị nhỏ [32]. Điều này có thể được thấy từ giới hạn của Định lý 1, trong đó các hằng số độc lập với các giá trị kỳ dị của các ma trận trọng số, và được minh họa bởi Hình 4, trong đó DLRT cho thấy tốc độ hội tụ nhanh hơn nhiều so với SGD vanilla được thực hiện trên mỗi nhân tử của tham số hóa U_k V_k^T, khi áp dụng để huấn luyện LeNet5 trên MNIST. Cả hai phương pháp đều được thực hiện với cùng tốc độ học cố định.

5.2 Kết quả trên ImageNet1K và Cifar10 với ResNet-50, AlexNet, và VGG16
Cuối cùng, chúng tôi đánh giá khả năng nén các kiến trúc khác nhau trên các tập huấn luyện quy mô lớn. Chúng tôi huấn luyện một mô hình đường cơ sở hạng đầy đủ và so sánh nó với DLRT sử dụng cùng trọng số bắt đầu trên GPU Nvidia A-100. Bộ tối ưu được sử dụng là SGD với hệ số momentum 0.1 và không sử dụng kỹ thuật tăng cường dữ liệu. Chúng tôi so sánh kết quả trên các mô hình ResNet-50, VGG16, và AlexNet, trên các tập dữ liệu Cifar10 và ImageNet1k, và so với một số phương pháp đường cơ sở thay thế tham số thấp. Đối với DLRT, các lớp cuối cùng của mạng đã được điều chỉnh để phù hợp với các nhiệm vụ phân loại tương ứng. Kết quả chi tiết được báo cáo trong Bảng 2, nơi chúng tôi hiển thị độ chính xác kiểm tra (được báo cáo như sự khác biệt so với đường cơ sở đầy đủ) cũng như tỷ lệ nén. Với Cifar10, chúng tôi đạt được nén huấn luyện 77.5% với mất mát độ chính xác chỉ 1.89% cho VGG16 và nén huấn luyện 84.2% với mất mát độ chính xác 1.79% cho AlexNet. Trong benchmark ImageNet1k, chúng tôi đạt được tỷ lệ nén huấn luyện 14.2%, với mất mát độ chính xác kiểm tra 0.5% trong độ chính xác top-5 trên ResNet-50 và nén huấn luyện 78.4% với mất mát độ chính xác top-5 2.19% trên VGG16.

10

--- TRANG 11 ---
Bảng 2: Kết quả trên ImageNet1k (trái) và Cifar10 (phải). Tỷ lệ nén là tỷ lệ phần trăm giảm tham số so với mô hình đầy đủ. DLRT được sử dụng với λ = 0.1. Số lượng tham số của các mô hình đầy đủ là: 33.6M (VGG16); 23.6M (AlexNet); 29.6M (ResNet-50). Chúng tôi báo cáo sự khác biệt trong độ chính xác kiểm tra (độ chính xác kiểm tra top-5 cho ImageNet1k) so với các đường cơ sở đầy đủ.

[THIS IS TABLE: Bảng hiển thị kết quả so sánh độ chính xác và tỷ lệ nén cho ImageNet1k và Cifar10]

6 Phụ lục
6.1 Chứng minh các kết quả chính
Chúng tôi cung cấp ở đây chứng minh Định lý 1 và 2. Chứng minh dựa trên một số kết quả cổ điển cũng như những tiến bộ gần đây trong lý thuyết DLRA, bao gồm [35, 44, 32, 4, 6].

Nhớ lại rằng, đối với một lớp k cố định, chúng tôi diễn giải lại giai đoạn huấn luyện như một sự phát triển liên tục theo thời gian của các trọng số trên đa tạp của các ma trận thấp hạng, như được minh họa trong Hình 5(a-b). Điều này đưa về việc giải phương trình vi phân ma trận bị ràng buộc đa tạp

min_{Ẇ_k(t)} {‖Ẇ_k(t) - F_k(W_k(t))‖ : Ẇ_k(t) ∈ T_{W_k(t)}M_{r_k}} (10)

trong đó ‖·‖ là chuẩn Frobenius và F_k biểu thị dòng gradient của mất mát đối với biến ma trận thứ k, cụ thể

F_k(Z) = -∇_{W_k}L(W_1,...,Z,...,W_M; N(x), y).

Vì lý do đơn giản và để có ký hiệu sạch hơn, vì tất cả các kết quả chúng tôi sẽ trình bày đều đúng cho một k chung, chúng tôi bỏ chỉ số k từ bây giờ. Đặc biệt, chúng tôi giả sử W là ma trận trọng số của một lớp ẩn chung với n neuron đầu vào và m neuron đầu ra.

Để việc rút ra của chúng tôi đúng, chúng tôi yêu cầu hai tính chất sau:

(a) Cập nhật trọng số thời gian rời rạc
(b) Cập nhật trọng số thời gian liên tục
(c) Điều kiện Galerkin

Hình 5: Các panel (a)-(b): Diễn giải đồ họa lại của bước cập nhật trọng số như một quá trình liên tục theo thời gian. Panel (c): Chiếu trực giao lên không gian tiếp tuyến của đa tạp thấp hạng M_r. Đường đứt nét mô tả chiếu dẫn đến Ẇ_k(t), là phần tử tiếp tuyến tối thiểu hóa khoảng cách giữa ∇_{W_k}L(W_k(t)) và không gian tiếp tuyến T_{W_k(t)}M_r tại xấp xỉ W_k(t).

11

--- TRANG 12 ---
P1. Dòng gradient F được giới hạn cục bộ và liên tục Lipschitz cục bộ, với các hằng số C₁ và C₂, tương ứng. Cụ thể, chúng tôi giả sử tồn tại C₁, C₂ > 0 (độc lập với k) sao cho

‖F(Z)‖ ≤ C₁, ‖F(Z) - F(Z̃)‖ ≤ C₂‖Z - Z̃‖

cho mọi Z, Z̃ ∈ Rᵐˣⁿ.

P2. Toàn bộ dòng gradient "không quá xa" khỏi đa tạp hạng-r M_r. Chính xác, chúng tôi giả sử rằng đối với bất kỳ Z ∈ M_r tùy ý gần W(t), toàn bộ dòng gradient F(Z) gần t sao cho

‖(I - P(Z))F(Z)‖ ≤ ε;

trong đó P(Z) biểu thị chiếu trực giao lên T_Z M_r.

Lưu ý rằng cả hai giả định đều hợp lệ cho huấn luyện mạng neural thấp hạng. Đặc biệt, tính liên tục Lipschitz và tính bị chặn của gradient là các giả định tiêu chuẩn trong tối ưu hóa và được thỏa mãn bởi gradient của các hàm mất mát mạng neural thường được sử dụng. Hơn nữa, việc giả sử dòng gradient gần với đa tạp thấp hạng là một quan sát thực nghiệm thường gặp trong mạng neural [53, 48, 15].

Để rút ra chứng minh Định lý 1 và 2, trước tiên chúng tôi trình bày một số bổ đề nền tảng liên quan. Bổ đề đầu tiên cho thấy rằng không gian con được tạo ra bởi bước K trong Thuật toán 1 sau phân tích QR là O((τ + ε)) gần với phạm vi của giải pháp chính xác, trong đó τ là bước thời gian của bộ tích phân và ε là dung sai cắt ngắn giá trị riêng.

Bổ đề 1 ([6, Bổ đề 2]). Gọi W₁ là giải pháp tại thời gian t = τ của bài toán đầy đủ (2) với điều kiện ban đầu W₀. Gọi U₁ là ma trận thu được với bước K của Thuật toán 1 hạng cố định, sau một bước. Dưới các giả định P1 và P2 ở trên, chúng ta có

‖U₁U₁ᵀW₁ - W₁‖ ≤ ξ

trong đó
ξ = C₁C₂(4e^(C₂τ) + 9)τ² + (3e^(C₂τ) + 4)ε.

Chứng minh. Phân tích lỗi cục bộ của [32] cho thấy rằng tồn tại L₁ sao cho

‖U₁L₁ᵀ - W₁‖ ≤ ξ.

Suy ra rằng,

‖U₁L₁ᵀ - W₁‖² = ‖U₁L₁ᵀ - U₁U₁ᵀW₁ + U₁U₁ᵀW₁ - W₁‖²
                = ‖U₁U₁ᵀ(U₁L₁ᵀ - W₁) + (I - U₁U₁ᵀ)(W₁)‖²
                = ‖U₁U₁ᵀ(U₁L₁ᵀ - W₁)‖² + ‖(I - U₁U₁ᵀ)W₁‖².

Do đó,

‖U₁U₁ᵀ(U₁L₁ᵀ - W₁)‖² + ‖(I - U₁U₁ᵀ)W₁‖² ≤ 2ξ².

Do đó, vì cả hai số hạng phải được giới hạn bởi 2ξ² riêng lẻ, chúng ta thu được kết quả đã nêu.

Trong bổ đề tiếp theo chúng tôi chỉ ra rằng không gian được tạo ra bởi bước L cũng gần với giải pháp chính xác. Cụ thể, kết hợp với kết quả trước đó, chúng ta có

Bổ đề 2 ([6, Bổ đề 3]). Gọi W₁, U₁ được định nghĩa như trên. Gọi V₁ là ma trận thu được từ bước L của Thuật toán 1 hạng cố định, sau một bước. Ước lượng sau đây đúng:

‖U₁U₁ᵀW₁V₁V₁ᵀ - W₁‖ ≤ 2ξ.

Chứng minh. Bước L được thu được như bước K áp dụng cho hàm chuyển vị G(Y) = F(Yᵀ)ᵀ. Do tính bất biến của chuẩn Frobenius dưới chuyển vị, tính chất P1 đúng. Tương tự, tính chất P2 tiếp tục được thỏa mãn vì

‖(I - P(Y))G(Y)‖ = ‖(I - P(Yᵀ))F(Yᵀ)‖ ≤ ε;

trong đó đẳng thức P(Y)Zᵀ = (P(Yᵀ)Z)ᵀ đã được sử dụng [35, §4]. Suy ra từ Bổ đề 1 rằng

‖U₁U₁ᵀW₁ - W₁‖ ≤ ξ;
‖V₁V₁ᵀW₁ᵀ - W₁ᵀ‖ ≤ ξ. (11)

Điều này ngụ ý rằng

‖U₁U₁ᵀW₁V₁V₁ᵀ - W₁‖ ≤ ‖U₁U₁ᵀW₁V₁V₁ᵀ - W₁V₁V₁ᵀ + W₁V₁V₁ᵀ - W₁‖
                                ≤ ‖U₁U₁ᵀW₁V₁V₁ᵀ - W₁V₁V₁ᵀ‖ + ‖W₁V₁V₁ᵀ - W₁‖
                                ≤ ‖(U₁U₁ᵀW₁ - W₁)V₁V₁ᵀ‖ + ‖V₁V₁ᵀW₁ᵀ - W₁ᵀ‖
                                ≤ ‖U₁U₁ᵀW₁ - W₁‖‖V₁V₁ᵀ‖₂ + ‖V₁V₁ᵀW₁ᵀ - W₁ᵀ‖.

Vì ‖V₁V₁ᵀ‖₂ = 1, kết quả đã nêu suy ra từ (11).

Với các bổ đề trước đó, chúng ta có thể rút ra giới hạn lỗi cục bộ cho bộ tích phân KLS hạng cố định của Phần 4.

Bổ đề 3 (Lỗi Cục bộ, [6, Bổ đề 4]). Gọi W₁; U₁; V₁ được định nghĩa như trên và gọi S₁ là ma trận thu được với bước S của Thuật toán 1 sau một bước. Giới hạn lỗi cục bộ sau đây đúng:

‖U₁S₁V₁ᵀ - W₁‖ ≤ (ĉ₁ε + ĉ₂τ);

trong đó các hằng số ĉᵢ độc lập với các giá trị kỳ dị của W₁ và S₁.

Chứng minh. Từ Bổ đề 2 và đẳng thức Y₁ = U₁S₁V₁ᵀ, chúng ta có

‖Y₁ - W₁‖ ≤ ‖Y₁ - U₁U₁ᵀW₁V₁V₁ᵀ‖ + ‖U₁U₁ᵀW₁V₁V₁ᵀ - W₁‖
            ≤ ‖U₁(S₁ - U₁ᵀW₁V₁)V₁ᵀ‖ + 2ξ
            ≤ ‖S₁ - U₁ᵀW₁V₁‖ + 2ξ.

Phân tích lỗi cục bộ được giảm về việc giới hạn số hạng ‖S₁ - U₁ᵀW₁V₁‖. Đối với 0 ≤ t ≤ τ, chúng tôi do đó giới thiệu đại lượng phụ sau:

S̃(t) := U₁ᵀW(t)V₁.

Chúng tôi quan sát rằng số hạng W(t) có thể được viết lại như

W(t) = U₁U₁ᵀW(t)V₁V₁ᵀ + (W(t) - U₁U₁ᵀW(t)V₁V₁ᵀ)
     = U₁S̃(t)V₁ᵀ + R(t);

trong đó R(t) biểu thị số hạng trong ngoặc lớn. Đối với 0 ≤ t ≤ τ, suy ra từ Bổ đề 2 và giới hạn C₁ của hàm F rằng

‖W(t) - W(τ)‖ ≤ ∫₀ᵗ ‖Ẇ(s)‖ds = ∫₀ᵗ ‖F(W(s))‖ds ≤ C₁τ.

Do đó, số hạng R(t) được giới hạn bởi

‖R(t)‖ ≤ ‖R(t) - R(τ)‖ + ‖R(τ)‖ ≤ 2C₁τ + 2ξ.

Chúng tôi viết lại hàm F(W(t)) như

F(W(t)) = F(U₁S̃(t)V₁ᵀ + R(t)) = F(U₁S̃(t)V₁ᵀ) + D(t)

trong đó khiếm khuyết D(t) được cho bởi

D(t) := F(U₁S̃(t)V₁ᵀ + R(t)) - F(U₁S̃(t)V₁ᵀ).

Qua hằng số Lipschitz C₂ của hàm F, khiếm khuyết được giới hạn bởi

‖D(t)‖ ≤ C₂‖R(t)‖ ≤ 2C₂(C₁τ + ξ).

13

--- TRANG 13 ---
Bây giờ, chúng tôi so sánh hai phương trình vi phân

S̃̇(t) = -U₁ᵀF(U₁S̃(t)V₁ᵀ)V₁ + U₁ᵀD(t)V₁; S̃(0) = U₁ᵀW₀V₁;
Ṡ(t) = -U₁ᵀF(U₁S(t)V₁ᵀ)V₁; S(0) = U₁ᵀW₀V₁.

Giải pháp S₁ thu được trong phương trình vi phân thứ hai giống như được cho bởi bước S của bộ tích phân KLS của Phần 4. Theo cấu trúc, giải pháp thu được trong phương trình vi phân đầu tiên tại thời gian t = τ là S̃(τ) = U₁ᵀW₁V₁. Với bất đẳng thức Gronwall chúng tôi thu được

‖S₁ - U₁ᵀW₁V₁‖ ≤ ∫₀ᵗ e^(C₂(τ-s))‖D(s)‖ds ≤ Le^(C₂τ)2C₂(C₁τ + ξ).

Kết quả mang lại phát biểu của định lý sử dụng định nghĩa của ξ.

Bây giờ chúng ta có thể kết luận chứng minh Định lý 1.

Chứng minh Định lý 1. Trong Bổ đề 3, lỗi cục bộ cho bộ tích phân hạng cố định của §4 đã được cung cấp. Lỗi cục bộ theo thời gian của phiên bản thích ứng hạng được thu được trực tiếp qua bất đẳng thức tam giác:

‖U₁S₁V₁ᵀ - W(τ)‖ ≤ ĉ₁ε + ĉ₂τ² + ε;

trong đó ε là tham số dung sai được chọn cho quy trình cắt ngắn. Ở đây, chúng tôi lạm dụng ký hiệu và chúng tôi duy trì cùng danh pháp U₁; S₁; và V₁ cũng cho xấp xỉ thấp hạng mới thu được qua quy trình cắt ngắn.

Do đó, chúng tôi kết luận chứng minh sử dụng tính liên tục Lipschitz của hàm F. Chúng tôi di chuyển từ lỗi cục bộ theo thời gian đến lỗi toàn cục theo thời gian bằng một luận cứ tiêu chuẩn của quạt Lady Windermere [21, Phần II.3]. Do đó, lỗi sau t bước của Thuật toán 1 thích ứng hạng được cho bởi

‖UᵗSᵗVᵗᵀ - W(tτ)‖ ≤ c₁ε + c₂τ + c₃ε = O(ε + τ).

Để kết luận, chúng tôi chứng minh rằng sau một bước, thuật toán DLRT thích ứng hạng được đề xuất giảm dọc theo các xấp xỉ thấp hạng. Chúng tôi nhắc nhở rằng chỉ cần giả định tính chất P1 ở đây.

Chứng minh Định lý 2. Gọi Ỹ(t) = U₁S(t)V₁ᵀ. Ở đây, S(t) biểu thị giải pháp cho t ∈ [0, τ] của bước S của bộ tích phân thích ứng hạng. Suy ra rằng

d/dt L(Ỹ(t)) = ⟨∇L(Ỹ(t)), Ỹ̇(t)⟩
                = ⟨∇L(Ỹ(t)), U₁Ṡ(t)V₁ᵀ⟩
                = ⟨U₁ᵀ∇L(Ỹ(t))V₁, Ṡ(t)⟩
                = -⟨U₁ᵀ∇L(Ỹ(t))V₁, U₁ᵀ∇L(Ỹ(t))V₁⟩ = -‖U₁ᵀ∇L(Ỹ(t))V₁‖².

Các đẳng thức cuối cùng theo định nghĩa của bước S. Đối với t ∈ [0, τ] chúng ta có

d/dt L(Ỹ(t)) ≤ -β²                                                           (12)

trong đó β = min₀≤τ≤₁ ‖U₁ᵀ∇L(Ỹ(τ))V₁‖. Tích phân (12) từ t = 0 đến t = τ, chúng tôi thu được

L(Ỹ₁) ≤ L(Ỹ₀) - β²τ.

Vì không gian con U₁ và V₁ chứa theo cấu trúc phạm vi và phạm vi đồng của giá trị ban đầu, chúng ta có Ỹ₀ = U₀S₀V₀ᵀ [4, Bổ đề 1]. Việc cắt ngắn sao cho ‖Y₁ - Ỹ₁‖ ≤ ε. Do đó,

L(Y₁) ≤ L(Ỹ₁) + γε

trong đó γ = max₀≤λ≤₁ ‖∇L(λY₁ + (1-λ)Ỹ₁)‖. Do đó, kết quả đã nêu được thu được.

14

--- TRANG 14 ---
6.2 Đo lường thời gian chi tiết
Bảng 3 hiển thị thời gian huấn luyện batch trung bình của mạng dày đặc 5 lớp, 5120 neuron trên tập dữ liệu MNIST, với kích thước batch 500 mẫu. Chúng tôi lấy trung bình thời gian trên 200 batch và hiển thị thêm độ lệch chuẩn của thời gian tương ứng với các hạng lớp. Thời gian batch đo toàn bộ các bước K, L và S, bao gồm lan truyền ngược và cập nhật gradient, cũng như đánh giá mất mát và metric.

Bảng 3: Thời gian huấn luyện batch trung bình cho huấn luyện thấp hạng cố định của mạng kết nối đầy đủ 5 lớp với độ rộng lớp [5120;5120;5120;5120;10]. Các phân tích thấp hạng khác nhau được so sánh.

[THIS IS TABLE: Shows ranks, mean time, and standard deviation for different network configurations]

Bảng 4 cho thấy thời gian kiểm tra trung bình của mạng dày đặc 5 lớp, 5120 neuron, cho các phân tích thấp hạng khác nhau và mạng tham chiếu hạng đầy đủ. Thời gian được lấy trung bình trên 1000 đánh giá của tập dữ liệu huấn luyện MNIST 60K mẫu. Chúng tôi đo đánh giá thuận bước K của các mạng thấp hạng cũng như đánh giá mất mát và độ chính xác dự đoán.

Bảng 4: Thời gian dự đoán tập dữ liệu trung bình cho huấn luyện thấp hạng cố định của mạng kết nối đầy đủ 5 lớp với độ rộng lớp [5120;5120;5120;5120;10]. Các phân tích thấp hạng khác nhau được so sánh.

[THIS IS TABLE: Shows ranks, mean time, and standard deviation for different network configurations during prediction]

6.3 Hiệu suất huấn luyện chi tiết của mạng thấp hạng thích ứng
Bảng 5 và 6 hiển thị tổng quan chi tiết về kết quả thấp hạng thích ứng của §5.1. Các hạng được hiển thị là hạng của thuật toán đã hội tụ. Sự phát triển hạng của trường hợp thử nghiệm 5-Lớp, 500-Neuron có thể được thấy trong Hình 6. Số lượng tham số Đánh giá tương ứng với các tham số của bước K của thuật toán thấp hạng động, vì tất cả các ma trận khác không còn cần thiết trong giai đoạn đánh giá. Số lượng tham số huấn luyện được đánh giá như số lượng tham số của bước S của huấn luyện thấp hạng thích ứng, với mở rộng cơ sở tối đa 2r, trong đó r là hạng hiện tại của mạng. Chúng tôi sử dụng các hạng hội tụ của huấn luyện thấp hạng thích ứng để tính các tham số huấn luyện. Lưu ý rằng trong các epoch huấn luyện đầu tiên, số lượng tham số thường cao hơn cho đến khi việc giảm hạng đạt được mức đủ thấp.

15

--- TRANG 15 ---
a) Phát triển hạng cho λ = 0.17
b) Phát triển hạng cho λ = 0.15
c) Phát triển hạng cho λ = 0.13
d) Phát triển hạng cho λ = 0.11
e) Phát triển hạng cho λ = 0.09
f) Phát triển hạng cho λ = 0.07
g) Phát triển hạng cho λ = 0.05
h) Phát triển hạng cho λ = 0.03

Hình 6: Phát triển hạng của thuật toán huấn luyện thấp hạng thích ứng động cho kiến trúc dày đặc 5 lớp, 500-neuron.

16

--- TRANG 16 ---
Bảng 5: Huấn luyện thấp hạng động cho mạng 5 lớp 500-neurons. c.r. biểu thị tỷ lệ nén so với mạng dày đặc hạng đầy đủ.

[THIS IS TABLE: Hiển thị kết quả NN metrics cho Evaluation và Train với các giá trị λ khác nhau, bao gồm test acc., ranks, params, và c.r.]

Bảng 6: Huấn luyện thấp hạng động cho mạng 5 lớp 784-neurons. c.r. biểu thị tỷ lệ nén so với mạng dày đặc hạng đầy đủ.

[THIS IS TABLE: Hiển thị kết quả NN metrics cho Evaluation và Train với các giá trị λ khác nhau, bao gồm test acc., ranks, params, và c.r.]

6.3.1 Thí nghiệm Lenet5
Trong Bảng 7 chúng tôi báo cáo kết quả của năm lần chạy độc lập của sơ đồ huấn luyện thấp hạng động trên Lenet5; chúng tôi tham khảo §5.1 để biết thêm chi tiết. Đối với mỗi cột của bảng, chúng tôi báo cáo giá trị trung bình cùng với độ lệch chuẩn tương đối của nó. Không có seed nào được áp dụng để chia tập dữ liệu và tạo cấu hình trọng số ban đầu.

Bảng 7: Kết quả trung bình và độ lệch chuẩn tương đối của thuật toán huấn luyện thấp hạng động qua năm lần chạy độc lập trên Lenet5. Tốc độ học thích ứng 0.05 với thuế suy giảm theo cấp số nhân 0.96.

[THIS IS TABLE: Hiển thị kết quả NN metrics cho Evaluation và Train với các giá trị λ khác nhau]

6.4 Hiệu suất huấn luyện chi tiết của cắt tỉa thấp hạng
Thuật toán huấn luyện thấp hạng được đề xuất không cần được áp dụng để huấn luyện mạng từ các dự đoán trọng số ban đầu ngẫu nhiên. Khi một mạng đã được huấn luyện có sẵn, phương pháp được đề xuất có thể được sử dụng như một chiến lược cắt tỉa hiệu quả bộ nhớ. Một cách tiếp cận đơn giản để giảm mạng kết nối đầy đủ đã được huấn luyện thành mạng hạng r là tính SVD cho tất cả các ma trận trọng số và cắt ngắn những phân tích đó tại hạng r. Tuy nhiên, mặc dù lựa chọn này tối ưu để trình bày các ma trận trọng số, nó có thể giảm đáng kể độ chính xác của mạng. Do đó, việc huấn luyện lại mạng con thấp hạng được xác định thường cần thiết để có được các tính chất độ chính xác mong muốn. Ba khía cạnh chính quan trọng để có được một phương pháp cắt tỉa hiệu quả cho các phương pháp thấp hạng:

1. Việc huấn luyện lại bảo toàn cấu trúc thấp hạng của mạng con.

17

--- TRANG 17 ---
2. Việc huấn luyện lại không thể hiện dung lượng bộ nhớ của mạng kết nối đầy đủ.
3. Việc huấn luyện lại tìm thấy mạng tối ưu trong số các mạng hạng thấp có thể.

Hãy lưu ý rằng điểm hút của các phương trình phát triển thấp hạng động được đề xuất đáp ứng ba yêu cầu này. Nhớ lại rằng đối với các phương trình phát triển chúng ta có (3):

min_{Ẇ_k(t)} {‖Ẇ_k(t) + ∇_{W_k}L(W_k(t))‖_F : Ẇ_k(t) ∈ T_{W_k(t)}M_{r_k}} (13)

Điều kiện Ẇ_k(t) ∈ T_{W_k(t)}M_{r_k} đảm bảo rằng các ma trận trọng số vẫn có hạng thấp. Hơn nữa, như đã thảo luận trước đây, phương pháp huấn luyện chỉ đòi hỏi khả năng bộ nhớ để lưu trữ các nhân tử thấp hạng. Tại điểm hút, tức là khi Ẇ_k = 0, điều kiện cuối cùng đảm bảo rằng điểm hút tối thiểu hóa ‖∇_{W_k}L(W_k(t))‖_F. Tức là, điểm hút là mạng con thấp hạng tối ưu theo nghĩa rằng nó chọn mạng với gradient tối thiểu. Để nhấn mạnh tính hiệu quả của phương pháp thấp hạng của chúng tôi như một kỹ thuật cắt tỉa, chúng tôi lấy mạng kết nối đầy đủ từ Bảng 6. Để chứng minh độ chính xác xác thực kém khi chỉ đơn giản thực hiện SVD trên các ma trận trọng số 784x784 đầy đủ và cắt ngắn tại một hạng nhỏ hơn cho trước, chúng tôi thực hiện thí nghiệm này cho các hạng r ∈ {10,20,30,40,50,60,70,80,90,100}. Hóa ra mặc dù giảm yêu cầu bộ nhớ, chiến lược này dẫn đến độ chính xác không thỏa đáng khoảng 10%, xem cột đầu tiên của Bảng 8. Sau đó, chúng tôi sử dụng các phương pháp huấn luyện thấp hạng được đề xuất với hạng cố định r để huấn luyện lại mạng. Như điểm bắt đầu, chúng tôi sử dụng các mạng thấp hạng đã được xác định bởi SVD cắt ngắn. Việc huấn luyện lại sau đó đạt được độ chính xác mong muốn có thể so sánh với các mạng thấp hạng đã được xác định trước đó trong Bảng 6.

Bảng 8: Phương pháp cắt tỉa với 784 Neurons mỗi lớp

[THIS IS TABLE: Bảng hiển thị kết quả so sánh độ chính xác test giữa SVD và low-rank training, cùng với thông số đánh giá]

6.5 Rút gọn chi tiết của gradient
Trong phần này, chúng tôi rút gọn tính toán các gradient trong các bước K, L và S một cách chi tiết. Để làm điều này, chúng ta hãy bắt đầu với gradient đầy đủ, tức là gradient của mất mát đối với ma trận trọng số W_k. Chúng ta có

[Theo sau là một loạt các phương trình toán học phức tạp về tính toán gradient]

18

--- TRANG 18 ---
và lưu ý rằng đối với ℓ ≠ k

∂W_k^ℓ/∂W_k^ij {∑_{i_{ℓ-1}} W_{ii_{ℓ-1}}^ℓ z_{ℓ-1}^{i_{ℓ-1}} + b_i^ℓ} = ∑_{i_{ℓ-1}} W_{ii_{ℓ-1}}^ℓ ∂W_k^ij z_{ℓ-1}^{i_{ℓ-1}}, (16)

trong khi đối với ℓ = k chúng ta có

∂W_k^ij/∂W_k^ij {∑_{i_{k-1}=1}^{n_{k-1}} W_{ii_{k-1}}^k z_{k-1}^{i_{k-1}} + b_i^k} = ∑_{i_{k-1}} δ_{ij}δ_{ii_{k-1}} z_{k-1}^{i_{k-1}}. (17)

Do đó, việc cắm đệ quy (15), (16) và (17) vào (14) mang lại

∂W_k^ij L = ∑_{i_M} ∂z_M^{i_M}/∂L σ'_{M,i_M} ∑_{i_{M-1}} W_{i_M i_{M-1}}^M ∂W_k^ij z_{M-1}^{i_{M-1}}

= ∑_{i_M} ∂z_M^{i_M}/∂L σ'_{M,i_M} ∑_{i_{M-1}} W_{i_M i_{M-1}}^M σ'_{M-1,i_{M-1}} ∑_{i_{M-2}} W_{i_{M-1} i_{M-2}}^{M-1} ∂W_k^ij z_{M-2}^{i_{M-2}} = ...

= ∑_{i_k,...,i_M} ∂z_M^{i_M}/∂L ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} ∂W_k^ij σ_k {∑_{i_{k-1}=1}^{n_{k-1}} W_{i_k i_{k-1}}^k z_{k-1}^{i_{k-1}} + b_{i_k}^k} (18)

= ∑_{i_k,...,i_M} ∂z_M^{i_M}/∂L ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} ∑_{i_{k-1}} δ_{ij}δ_{ki_{k-1}} z_{k-1}^{i_{k-1}}

= ∑_{i_{k+1},...,i_M} ∂z_M^{i_M}/∂L ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,j} δ_{ki_k} z_{k-1}^j

Được viết trong ký hiệu ma trận và sử dụng tích Hadamard được định nghĩa là y ⊙ Ax = (y_i A_{ij} x_j)_{ij}, cho A ∈ R^{m×n}, x ∈ R^n và y ∈ R^m, chúng ta có:

∂W_k L/∂W_k = ∂z_M L/∂z_M ⊙ (∏_{ℓ=k+1}^M W_ℓ^T σ'_ℓ)^T z_{k-1}^T

Bây giờ, chúng ta hãy chuyển sang việc rút gọn các bước K, L và S cho huấn luyện thấp hạng động. Đối với bước K, chúng tôi biểu diễn ma trận trọng số W_k như W_{ki_k i_{k-1}} = ∑_m K_{ki_k m} V_{ki_{k-1} m}. Do đó, tái sử dụng kết quả trung gian (18) mang lại

∂K_{kj} L/∂K_{kj} = ∑_{i_k,...,i_M} ∂z_M^{i_M}/∂L ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} ∂K_{kj}/∂K_{kj} σ_k {∑_{i_{k-1}=1}^{n_{k-1}} ∑_m K_{ki_k m} V_{ki_{k-1} m} z_{k-1}^{i_{k-1}} + b_{i_k}^k}

= ∑_{i_k,...,i_M} ∂z_M^{i_M}/∂L ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} ∑_{i_{k-1}=1}^{n_{k-1}} ∑_m δ_{ij}δ_{km} V_{ki_{k-1} m} z_{k-1}^{i_{k-1}}

= ∑_{i_{k+1},...,i_M} ∂z_M^{i_M}/∂L ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} ∑_{i_{k-1}=1}^{n_{k-1}} δ_{ij} V_{ki_{k-1} k} z_{k-1}^{i_{k-1}}

Trong ký hiệu ma trận chúng ta thu được

∂K_k L/∂K_k = ∂z_M L/∂z_M ⊙ (∏_{ℓ=k+1}^M W_ℓ^T σ'_ℓ)^T V_k^T z_{k-1}^T

= ∂W_k L/∂W_k V_k;

19

--- TRANG 19 ---
đó chính xác là vế phải của bước K. Do đó, bước K có thể được tính bằng một đánh giá thuận của L và ghi lại băng gradient đối với K_k. Tương tự, đối với bước L, chúng tôi biểu diễn W_k như W_{ki_k i_{k-1}} = ∑_m U_{ki_k m} L_{ki_{k-1} m}. Do đó,

∂L_{kjk} L = ∑_{i_k,...,i_M} \frac{∂z_M^{i_M}}{∂L} ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} \frac{∂L_{kjk}}{∂L_{kjk}} σ_k \left( ∑_{i_{k-1}=1}^{n_{k-1}} ∑_m U_{ki_k m} L_{ki_{k-1} m} z_{k-1}^{i_{k-1}} + b_{i_k}^k \right)

= ∑_{i_k,...,i_M} \frac{∂z_M^{i_M}}{∂L} ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} ∑_{i_{k-1}=1}^{n_{k-1}} ∑_m U_{ki_k m} δ_{i_{k-1}jk} δ_{km} z_{k-1}^{i_{k-1}}

= ∑_{i_k,...,i_M} \frac{∂z_M^{i_M}}{∂L} ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} U_{ki_k m} z_{k-1}^j.

Trong ký hiệu ma trận, chúng ta thu được

∂L_k L = σ'_k \left( U_k^T \frac{∂z_M L}{∂z_M} ⊙ \left( ∏_{ℓ=k+1}^M W_ℓ^T σ'_ℓ \right)^T z_{k-1}^T \right)^T

= \left( \frac{∂W_k L}{∂W_k} \right)^T U_k.

Cuối cùng, đối với bước S chúng ta viết W_{ki_k i_{k-1}} = ∑_{n,m} U_{ki_k m} S_{mn} V_{ki_{k-1} n}. Khi đó,

∂S_{kjk} L = ∑_{i_k,...,i_M} \frac{∂z_M^{i_M}}{∂L} ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} \frac{∂S_{kjk}}{∂S_{kjk}} \left( ∑_{n,m} U_{ki_k m} S_{mn} V_{ki_{k-1} n} \right)

= ∑_{i_k,...,i_M} \frac{∂z_M^{i_M}}{∂L} ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} ∑_{i_{k-1}=1}^{n_{k-1}} ∑_m U_{ki_k m} δ_{mjk} V_{ki_{k-1} k} z_{k-1}^{i_{k-1}}

= ∑_{i_k,...,i_M} \frac{∂z_M^{i_M}}{∂L} ∏_{ℓ=k+1}^M σ'_{ℓ,i_ℓ} W_{i_ℓ i_{ℓ-1}}^ℓ σ'_{k,i_k} U_{ki_k j} V_{ki_{k-1} k} z_{k-1}^{i_{k-1}}.

Trong ký hiệu ma trận, chúng ta có

∂S_k L = U_k^T \frac{∂z_M L}{∂z_M} ⊙ \left( ∏_{ℓ=k+1}^M W_ℓ^T σ'_ℓ \right)^T V_k^T z_{k-1}^T

= U_k^T \frac{∂W_k L}{∂W_k} V_k.

6.6 Biểu diễn ma trận thấp hạng và thực hiện các lớp tích chập
Một bộ lọc tích chập tổng quát là một tensor bốn mode W ∈ R^{F×C×J×K} bao gồm F bộ lọc có hình dạng C×J×K, được áp dụng cho một batch của N tín hiệu hình ảnh kênh C đầu vào Z có kích thước không gian U×V như ánh xạ tuyến tính,

(Z * W)(n,f,u,v) = ∑_{j=1}^J ∑_{k=1}^K ∑_{c=1}^C W(f,c,j,k)Z(n,c,u-j,v-k). (19)

Để huấn luyện bộ lọc tích chập trên đa tạp ma trận thấp hạng, chúng tôi định hình lại tensor W thành một ma trận hình chữ nhật W_{resh} ∈ R^{F×CJK}. Việc định hình lại này cũng được xem xét trong ví dụ [28]. Một tùy chọn là, xem tích chập như sự co lại giữa một tensor ba mode Z_{unfolded} của các miếng vá và ma trận kernel được định hình lại W_{resh} sử dụng hàm fold-unfold của Pytorch. Chúng ta có thể xây dựng unfold bằng cách xếp chồng phiên bản vector hóa của các mẫu trượt của kernel trên đầu vào gốc, thu được theo cách này một tensor Z_{unfolded} ∈ R^{N×CJK×L}, trong đó L biểu thị kích thước của phiên bản phẳng của đầu ra của tích chập 2-D. Do đó, phương trình 19 có thể được viết lại như một tích tensor mode:

(Z * W)(n,f,u,v) = ∑_{j=1}^J ∑_{k=1}^K ∑_{c=1}^C W_{resh}(f,(c,j,k))Z_{unfolded}(n,(c,j,k),(u,v))

= ∑_{p=1}^r U(f,p) ∑_{q=1}^r S(p,q) ∑_{j=1}^J ∑_{k=1}^K ∑_{c=1}^C V((c,j,k),q)Z_{unfolded}(n,(c,j,k),(u,v)) (20)

Như được thể hiện trong (20), chúng ta có thể phân tích trọng số bắt đầu W_{resh} = USV^T và sau đó thực hiện toàn bộ quy trình huấn luyện như một hàm của các nhân tử (U,S,V), mà không bao giờ tái tạo kernel. Sau đó chúng ta có thể áp dụng các xem xét của các lớp kết nối đầy đủ.

Lời cảm ơn. Công trình của S. Schotthöfer được tài trợ bởi Chương trình Ưu tiên SPP2298 "Cơ sở Lý thuyết của Học Sâu" của Deutsche Forschungsgemeinschaft (DFG). Công trình của J. Kusch được tài trợ bởi Deutsche Forschungsgemeinschaft (DFG) - 491976834. Công trình của G. Ceruti được hỗ trợ bởi dự án nghiên cứu SNSF "Thuật toán nhanh từ cập nhật thấp hạng", số tài trợ 200020-178806. Công trình của F. Tudisco và E. Zangrando được tài trợ bởi dự án MUR-PNRR "Học máy tham số thấp". Cảm ơn đặc biệt Giáo sư Martin Frank cho việc hướng dẫn PhD của Steffen Schottöfer.

Tài liệu tham khảo
[1] P.-A. Absil, R. Mahony, và R. Sepulchre. Thuật toán tối ưu hóa trên đa tạp ma trận. Princeton University Press, 2009.

[2] A. Ashok, N. Rhinehart, F. Beainy, và K. M. Kitani. Học N2n: Nén mạng đến mạng qua học tăng cường gradient chính sách. Trong International Conference on Learning Representations, 2018.

[3] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, và J. Guttag. Tình trạng của cắt tỉa mạng neural là gì? Proceedings of machine learning and systems, 2:129-146, 2020.

[4] G. Ceruti, J. Kusch, và C. Lubich. Một bộ tích phân mạnh mẽ thích ứng hạng cho xấp xỉ thấp hạng động. BIT Numerical Mathematics, 2022.

[5] G. Ceruti và C. Lubich. Tích phân thời gian của ma trận thấp hạng đối xứng và phản đối xứng và tensor Tucker. BIT Numerical Mathematics, 60(3):591-614, 2020.

[6] G. Ceruti và C. Lubich. Một bộ tích phân mạnh mẽ phi truyền thống cho xấp xỉ thấp hạng động. BIT. Numerical Mathematics, 62(1):23-44, 2022.

[7] G. Ceruti, C. Lubich, và D. Sulz. Tích phân thời gian thích ứng hạng của mạng tensor cây. arXiv:2201.10291, 2022.

[8] G. Ceruti, C. Lubich, và H. Walach. Tích phân thời gian của mạng tensor cây. SIAM Journal on Numerical Analysis, 59(1):289-313, 2021.

[9] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, và S.-F. Chang. Một khám phá về dư thừa tham số trong mạng sâu với chiếu vòng tròn. Trong Proceedings of the IEEE international conference on computer vision, trang 2857-2865, 2015.

[10] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, và Y. Bengio. Mạng neural nhị phân: Huấn luyện mạng neural sâu với trọng số và kích hoạt bị ràng buộc +1 hoặc -1. Advances in neural information processing systems, 2016.

[11] L. Deng. Cơ sở dữ liệu MNIST của hình ảnh chữ số viết tay cho nghiên cứu học máy. IEEE Signal Processing Magazine, 29(6):141-142, 2012.

[12] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, và R. Fergus. Khai thác cấu trúc tuyến tính trong mạng tích chập để đánh giá hiệu quả. Advances in neural information processing systems, 27, 2014.

[13] L. Dieci và T. Eirola. Về phân tích mượt của ma trận. SIAM Journal on Matrix Analysis and Applications, 20(3):800-819, 1999.

21

--- TRANG 20 ---
[14] P. A. M. Dirac et al. Các nguyên lý của cơ học lượng tử. Số 27. Oxford university press, 1981.

[15] R. Feng, K. Zheng, Y. Huang, D. Zhao, M. Jordan, và Z.-J. Zha. Giảm hạng trong mạng neural sâu. arXiv:2206.06072, 2022.

[16] F. Feppon và P. F. Lermusiaux. Một cách tiếp cận hình học cho việc giảm bậc mô hình động. SIAM Journal on Matrix Analysis and Applications, 39(1):510-538, 2018.

[17] J. Frankle và M. Carbin. Giả thuyết vé số: Tìm kiếm mạng neural thưa thớt, có thể huấn luyện. Trong International Conference on Learning Representations, 2018.

[18] J. Frenkel. Cơ học sóng, lý thuyết tổng quát nâng cao, tập 1. Oxford, 1934.

[19] B. Gao và P.-A. Absil. Một phương pháp thích ứng hạng riemannian cho hoàn thành ma trận thấp hạng. Computational Optimization and Applications, 81(1):67-90, 2022.

[20] Y. Guo, A. Yao, và Y. Chen. Phẫu thuật mạng động cho DNN hiệu quả. Advances in neural information processing systems, 29, 2016.

[21] E. Hairer, S. P. Nørsett, và G. Wanner. Giải phương trình vi phân thường. I. Các vấn đề không cứng, tập 8 của Springer Series in Computational Mathematics. Springer-Verlag, Berlin, ấn bản thứ hai, 1993.

[22] Y. He, G. Kang, X. Dong, Y. Fu, và Y. Yang. Cắt tỉa bộ lọc mềm để tăng tốc mạng tích chập sâu, 2018.

[23] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, và S. Han. AMC: AutoML cho nén và tăng tốc mô hình trên thiết bị di động. Trong Proceedings of the European conference on computer vision, trang 784-800, 2018.

[24] Y. He, X. Zhang, và J. Sun. Cắt tỉa kênh để tăng tốc mạng rất sâu. Trong IEEE International Conference on Computer Vision, trang 1389-1397, 2017.

[25] Y. He, X. Zhang, và J. Sun. Cắt tỉa kênh để tăng tốc mạng rất sâu, 2017.

[26] G. Huang, Z. Liu, L. Van Der Maaten, và K. Q. Weinberger. Mạng tích chập kết nối dày đặc. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 4700-4708, 2017.

[27] Z. Huang và N. Wang. Lựa chọn cấu trúc thưa thớt dựa trên dữ liệu cho mạng neural sâu. Trong Proceedings of the European conference on computer vision (ECCV), trang 304-320, 2018.

[28] Y. Idelbayev và M. A. Carreira-Perpiñán. Nén thấp hạng của mạng neural: Học hạng của mỗi lớp. Trong IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 8046-8056, 2020.

[29] Y. Ioannou, D. Robertson, J. Shotton, R. Cipolla, và A. Criminisi. Huấn luyện CNN với bộ lọc thấp hạng để phân loại hình ảnh hiệu quả. Trong International Conference on Learning Representations, 2016.

[30] M. Jaderberg, A. Vedaldi, và A. Zisserman. Tăng tốc mạng tích chập với mở rộng thấp hạng. Trong Proceedings of the British Machine Vision Conference. BMVA Press, 2014.

[31] M. Khodak, N. Tenenholtz, L. Mackey, và N. Fusi. Khởi tạo và chính quy hóa các lớp neural được phân tích. Trong International Conference on Learning Representations, 2021.

[32] E. Kieri, C. Lubich, và H. Walach. Xấp xỉ thấp hạng động rời rạc hóa với sự hiện diện của các giá trị kỳ dị nhỏ. SIAM Journal on Numerical Analysis, 54(2):1020-1038, 2016.

[33] H. Kim, M. U. K. Khan, và C.-M. Kyung. Nén mạng neural hiệu quả. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 12569-12577, 2019.

[34] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, và D. Shin. Nén mạng tích chập sâu cho ứng dụng di động nhanh và ít năng lượng, 2015.

[35] O. Koch và C. Lubich. Xấp xỉ thấp hạng động. SIAM Journal on Matrix Analysis and Applications, 29(2):434-454, 2007.

[36] O. Koch và C. Lubich. Xấp xỉ tensor động. SIAM Journal on Matrix Analysis and Applications, 31(5):2360-2375, 2010.

22

--- TRANG 21 ---
[37] J. Kusch và P. Stammer. Một phương pháp nguồn va chạm mạnh mẽ cho xấp xỉ thấp hạng động thích ứng hạng trong xạ trị. arXiv:2111.07160, 2021.

[38] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, và V. Lempitsky. Tăng tốc mạng tích chập sử dụng phân tích cp được tinh chỉnh. Trong International Conference on Learning Representations, 2015.

[39] Y. Lecun, L. Bottou, Y. Bengio, và P. Haffner. Học dựa trên gradient áp dụng cho nhận dạng tài liệu. Proceedings of the IEEE, 86(11):2278-2324, 1998.

[40] C. Li và C. J. R. Shi. Xấp xỉ thấp hạng dựa trên tối ưu hóa có ràng buộc của mạng neural sâu. Trong Proceedings of the European Conference on Computer Vision (ECCV), Tháng 9 2018.

[41] J. Lin, Y. Rao, J. Lu, và J. Zhou. Cắt tỉa neural thời gian chạy. Trong I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett, biên tập, Advances in Neural Information Processing Systems, tập 30. Curran Associates, Inc., 2017.

[42] S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, và D. Doermann. Hướng tới cắt tỉa CNN có cấu trúc tối ưu qua học đối kháng sinh. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 2790-2799, 2019.

[43] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, và K. Kavukcuoglu. Biểu diễn phân cấp cho tìm kiếm kiến trúc hiệu quả. Trong International Conference on Learning Representations, 2018.

[44] C. Lubich và I. V. Oseledets. Một bộ tích phân phân chia chiếu cho xấp xỉ thấp hạng động. BIT Numerical Mathematics, 54(1):171-188, 2014.

[45] C. Lubich, T. Rohwedder, R. Schneider, và B. Vandereycken. Xấp xỉ động bằng tensor Tucker phân cấp và tensor-train. SIAM Journal on Matrix Analysis and Applications, 34(2):470-494, 2013.

[46] C. Lubich, B. Vandereycken, và H. Walach. Tích phân thời gian của tensor Tucker bị ràng buộc hạng. SIAM Journal on Numerical Analysis, 56(3):1273-1290, 2018.

[47] J.-H. Luo, J. Wu, và W. Lin. Thinet: Một phương pháp cắt tỉa mức bộ lọc cho nén mạng neural sâu, 2017.

[48] C. H. Martin và M. W. Mahoney. Tự chính quy hóa ngầm trong mạng neural sâu: Bằng chứng từ lý thuyết ma trận ngẫu nhiên và ý nghĩa cho học tập. Journal of Machine Learning Research, 22(165):1-73, 2021.

[49] P. Molchanov, S. Tyree, T. Karras, T. Aila, và J. Kautz. Cắt tỉa mạng tích chập cho suy luận hiệu quả tài nguyên. Trong International Conference on Learning Representations, 2017.

[50] T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, và B. Ramabhadran. Phân tích ma trận thấp hạng cho huấn luyện mạng neural sâu với mục tiêu đầu ra chiều cao. Trong 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, trang 6655-6659, 2013.

[51] D. Scieur, V. Roulet, F. Bach, và A. d'Aspremont. Phương pháp tích phân và thuật toán tối ưu hóa tăng tốc. Trong Advances In Neural Information Processing Systems, 2017.

[52] P. Singh, V. Kumar Verma, P. Rai, và V. P. Namboodiri. Chơi và cắt tỉa: Cắt tỉa bộ lọc thích ứng cho nén mô hình sâu. Trong Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, trang 3460-3466. International Joint Conferences on Artificial Intelligence Organization, 7 2019.

[53] S. P. Singh, G. Bachmann, và T. Hofmann. Cái nhìn phân tích về cấu trúc và hạng của ánh xạ Hessian mạng neural. Trong Advances in Neural Information Processing Systems, tập 34, 2021.

[54] N. S. Sohoni, C. R. Aberger, M. Leszczynski, J. Zhang, và C. Ré. Huấn luyện mạng neural bộ nhớ thấp: Một báo cáo kỹ thuật. arXiv:1904.10631, 2019.

[55] A. Tjandra, S. Sakti, và S. Nakamura. Nén mạng neural tái phát với tensor train. Trong 2017 International Joint Conference on Neural Networks (IJCNN), trang 4451-4458. IEEE, 2017.

[56] M. Udell và A. Townsend. Tại sao các ma trận dữ liệu lớn xấp xỉ thấp hạng? SIAM Journal on Mathematics of Data Science, 1(1):144-160, 2019.

23

--- TRANG 22 ---
[57] H. Wang, S. Agarwal, và D. Papailiopoulos. Pufferfish: các mô hình hiệu quả giao tiếp mà không tốn thêm chi phí. Proceedings of Machine Learning and Systems, 3:365-386, 2021.

[58] W. Wen, C. Wu, Y. Wang, Y. Chen, và H. Li. Học tính thưa thớt có cấu trúc trong mạng neural sâu. Advances in neural information processing systems, 29, 2016.

[59] W. Wen, C. Xu, C. Wu, Y. Wang, Y. Chen, và H. Li. Phối hợp các bộ lọc cho mạng neural sâu nhanh hơn. Trong Proceedings of the IEEE International Conference on Computer Vision, trang 658-666, 2017.

[60] J. Wu, C. Leng, Y. Wang, Q. Hu, và J. Cheng. Mạng tích chập lượng tử hóa cho thiết bị di động. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 4820-4828, 2016.

[61] H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. Li, và Y. Chen. Học mạng neural sâu thấp hạng qua chính quy hóa trực giao vector kỳ dị và thưa thớt hóa giá trị kỳ dị. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, trang 678-679, 2020.

[62] J. Ye, X. Lu, Z. Lin, và J. Z. Wang. Suy nghĩ lại về giả định chuẩn nhỏ hơn-ít thông tin hơn trong cắt tỉa kênh của các lớp tích chập. Trong International Conference on Learning Representations, 2018.

24
