# 2312.17244.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2312.17244.pdf
# Kích thước tệp: 662408 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
BÁC SĨ PHẪU THUẬT LLM
Tycho F.A. van der Ouderaa1∗, Markus Nagel2, Mart van Baalen2,
Yuki M. Asano3, Tijmen Blankevoort2
1Imperial College London ,2Qualcomm AI Research†,3QUV A Lab, University of Amsterdam
TÓM TẮT
Các mô hình ngôn ngữ hiện đại đang trở nên ngày càng lớn nhằm đạt được hiệu suất cao nhất trên các kho văn bản lớn có sẵn. Tuy nhiên, kích thước khổng lồ của kiến trúc Transformer khiến việc triển khai mô hình trong các ràng buộc về tính toán, môi trường hoặc thiết bị cụ thể trở nên khó khăn. Chúng tôi khám phá việc nén dựa trên dữ liệu của các mô hình đã được huấn luyện sẵn như một giải pháp thay thế cho việc huấn luyện các mô hình nhỏ hơn từ đầu. Để làm điều này, chúng tôi mở rộng các xấp xỉ độ cong được phân tích Kronecker của cảnh quan mất mát mục tiêu cho các mô hình ngôn ngữ lớn. Bằng cách này, chúng tôi có thể tính toán cả việc phân bổ động các cấu trúc có thể được loại bỏ cũng như các cập nhật của các trọng số còn lại để tính đến việc loại bỏ. Chúng tôi cung cấp một khung tổng quát cho việc cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc và cải thiện các cập nhật trọng số để nắm bắt nhiều tương quan hơn giữa các trọng số, đồng thời vẫn hiệu quả về mặt tính toán. Về mặt thực nghiệm, phương pháp của chúng tôi có thể cắt tỉa các hàng và cột từ một loạt các mô hình OPT và Llamav2-7B từ 20%-30%, với mất mát hiệu suất không đáng kể, và đạt được kết quả hiện đại trong việc cắt tỉa không có cấu trúc và bán cấu trúc của các mô hình ngôn ngữ lớn.
Mã nguồn có sẵn tại: https://github.com/Qualcomm-AI-research/llm-surgeon.

1 GIỚI THIỆU
Những tiến bộ gần đây trong mô hình hóa ngôn ngữ (Vaswani et al., 2017) cho phép khớp các mô hình ngôn ngữ lớn (LLM) với hàng triệu hoặc thậm chí hàng tỷ tham số (như OPT (Zhang et al., 2022) và Llama 2 (Touvron et al., 2023)) trên các kho văn bản lớn để đạt hiệu suất cao. Thật không may, kích thước của các LLM này thường khiến việc triển khai chúng trong các ràng buộc thực tế trở nên khó khăn. Triển khai dựa trên đám mây có thể trở nên rất đắt đỏ đối với các mô hình lớn hơn, và các thiết bị hiệu quả như điện thoại thường bị hạn chế về kích thước bộ nhớ để lưu trữ một mô hình.

Một nhánh văn học mở rộng từ cuối những năm 1980, ví dụ, Optimal Brain Damage (OBD, LeCun et al. (1989)) và Optimal Brain Surgeon (OBS, Hassibi & Stork (1992)), coi việc cắt tỉa như một bài toán tối ưu hóa có ràng buộc để giảm dung lượng và yêu cầu thời gian chạy của mô hình. Hessian cần thiết cho phương pháp này tăng theo bình phương của số lượng tham số, và chỉ có thể được tính toán trong thực tế cho các mạng không thực tế nhỏ. Để khắc phục vấn đề này, Eigendamage (Wang et al., 2019) giới thiệu một phân tích Kronecker của một xấp xỉ đường chéo theo khối của Hessian. Các công trình gần đây, như Optimal Brain Compression (Frantar & Alistarh, 2022), SparseGPT (Frantar & Alistarh, 2023), chứng minh việc cắt tỉa sau huấn luyện thực tế của LLM, nhưng chỉ xem xét độ cong mất mát của lỗi tái tạo đầu ra bình phương của lớp được cắt tỉa, bỏ qua các gradient liên quan đến chi phí loại bỏ cục bộ với mất mát mục tiêu. Kết quả là, xấp xỉ của họ đối với cảnh quan mất mát mục tiêu không chính xác, dẫn đến suy giảm hiệu suất đáng kể cho các LLM được cắt tỉa. Hơn nữa, các phương pháp này không dễ dàng mở rộng sang việc cắt tỉa có cấu trúc.

∗Công việc được thực hiện trong thời gian thực tập tại Qualcomm AI Research
†Qualcomm AI Research là một sáng kiến của Qualcomm Technologies, Inc.

Nén có cấu trúc (hàng và cột) Nén không có cấu trúc (các phần tử ma trận)
1.3b 2.7b 6.7b kích thước mô hình:14.62
12.47
10.86
perplexity
tập kiểm tra
đã huấn luyện sẵn
K-OBD
LLM Surgeon
1.3b 2.7b 6.7b kích thước mô hình:14.62
12.47
10.86
perplexity
tập kiểm tra
đã huấn luyện sẵn
SparseGPT
LLM Surgeon

Hình 1: LLM Surgeon cho phép nội suy kích thước mô hình giữa các mô hình đã được huấn luyện sẵn hiện có.

1arXiv:2312.17244v2  [cs.LG]  20 Mar 2024

--- TRANG 2 ---
Công trình này giới thiệu LLM Surgeon, một khung tổng quát cho việc cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc của LLM. Tại thời điểm nộp bài, chúng tôi coi đây là phương pháp đầu tiên thực hiện thành công việc cắt tỉa có cấu trúc của LLM. Công trình đồng thời của Ashkboos et al. (2024) cũng xem xét việc cắt tỉa có cấu trúc của LLM nhưng bỏ qua thông tin gradient, dẫn đến hiệu suất cuối cùng thấp hơn.

Hiệu suất vượt trội của LLM Surgeon đạt được bằng cách mở rộng các xấp xỉ đường chéo theo khối được phân tích Kronecker đối với Fisher thực nghiệm từ Eigendamage lên LLM. Chúng tôi tiếp tục mở rộng công trình bằng cách rút ra các chi phí cắt tỉa trọng số và cập nhật kiểu OBS cho việc cắt tỉa có cấu trúc của nhiều hàng và cột, và cung cấp một khung tổng quát cũng kết hợp việc cắt tỉa bán cấu trúc và không có cấu trúc. Thay vì coi các cập nhật trọng số riêng lẻ một cách độc lập, chúng tôi cố gắng xem xét càng nhiều tương quan giữa các trọng số càng tốt trong thực tế và rút ra các cập nhật trọng số kết hợp để cắt tỉa nhiều trọng số (hoặc nhiều tập hợp trọng số có cấu trúc) cùng một lúc. Không giống như các công trình trước đó trong việc cắt tỉa LLM, LLM Surgeon cắt tỉa trong nhiều lần, cập nhật trọng số và ước tính độ cong giữa các lần. Chúng tôi sử dụng ngưỡng toàn cục cho việc cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc, tức là thay vì cắt tỉa các lớp theo một lượng cố định, các lớp nhạy cảm hơn được cắt tỉa ít hơn so với những lớp mạnh mẽ hơn. Cuối cùng, chúng tôi đề xuất giảm thiểu các gradient bậc một có thể không bằng không bằng cách sử dụng các cập nhật bậc một hạng thấp tùy chọn giữa các lần. Một lợi thế chính của LLM Surgeon là nó cho phép đánh đổi tính toán bổ sung trong quá trình nén để có độ chính xác tốt hơn bằng cách tăng số lượng tương quan và/hoặc lần cắt tỉa. Phương pháp của chúng tôi đưa ra kết quả có thể sử dụng thực tế đầu tiên cho việc cắt tỉa có cấu trúc của LLM - chúng có thể được cắt tỉa lên đến 30% với suy giảm hiệu suất nhỏ. Hơn nữa, chúng tôi đạt được kết quả hiện đại trong việc cắt tỉa LLM không có cấu trúc và bán cấu trúc.

2 BỐI CẢNH VÀ CÔNG TRÌNH LIÊN QUAN

Việc cắt tỉa mạng nơ-ron nhằm loại bỏ các tham số khỏi mô hình trong khi giảm thiểu tác động tiêu cực đến hiệu suất cuối cùng. Một cách chính thức hơn, chúng tôi ký hiệu P tham số mô hình như vector θ∗=
vec(W∗
1,W∗
2, . . .W∗
L)∈RP, bằng cách làm phẳng L ma trận trọng số của các khối attention và fully-connected,
với θ∗≈arg minθL(θ)đã được khớp với dữ liệu D để tối thiểu hóa mất mát negative likelihood
L(θ)=−logp(θ|D). Để nén mô hình, chúng ta đang tìm kiếm một vector đã cắt tỉa ˆθ:

ˆθ= arg minθL(θ)s.t. các ràng buộc cắt tỉa dựa trên θ∗(1)

trong đó các ràng buộc được chọn xác định cấu trúc của các trọng số nén ˆθ. Trong việc cắt tỉa không có cấu trúc, một phần của tổng số phần tử trọng số được đặt bằng không. Trong việc cắt tỉa bán cấu trúc của M:N, chúng ta có M trọng số trong mỗi N trọng số liên tiếp bằng không (Zhou et al., 2021; Hubara et al., 2021). Và trong việc cắt tỉa có cấu trúc (Louizos et al., 2017), toàn bộ các hàng và cột được đặt bằng không. Việc cắt tỉa có cấu trúc dẫn đến những lợi ích tức thì nhất về bộ nhớ và tính toán, vì nó trực tiếp giảm kích thước của các ma trận cần được biểu diễn rõ ràng nhưng được coi là khó nén nhất. Duy trì hiệu suất cao thường dễ dàng hơn trong các sơ đồ khác nhưng đòi hỏi phép toán chuyên biệt khai thác cấu trúc thưa thớt để có lợi ích khi triển khai. Chúng tôi xem xét tất cả các loại cắt tỉa ở trên, tập trung vào việc cắt tỉa có cấu trúc cho LLM.

Thông thường, phương trình (1) không thể được giải trực tiếp, vì không gian của các cấu hình cắt tỉa có thể vượt quá những gì có thể được đánh giá trong thực tế. Để minh họa, một tìm kiếm qua tất cả các mặt nạ cắt tỉa không có cấu trúc có thể của một LLM 125 triệu tham số sẽ yêu cầu 2P=2125m≈1037628749 đánh giá. Do đó, ý tưởng là tìm ˆθ bằng cách sử dụng một đại lý của cảnh quan mất mát q dễ làm việc hơn:

L(θ) =−logp(D |θ)≈ −logq(θ) (2)

Nếu người ta chọn một dạng Gaussian cụ thể cho đại lý q của chúng ta, thì các giải pháp cho các ràng buộc cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc có thể được rút ra dưới dạng đóng (phụ lục A).

2.1 KHAI TRIỂN TAYLOR

Làm thế nào để chúng ta có được một đại lý tốt của mất mát q? Một trong những phương pháp dễ nhất là mở rộng cục bộ log loss thông qua khai triển Taylor bậc hai xung quanh các trọng số đã được huấn luyện sẵn θ∗, tạo ra:

−logq(θ)≈ −logp(D|θ∗)−(θ−θ∗)T∇L(θ∗)−1
2(θ−θ∗)THθ∗(θ−θ∗) (3)

trong đó [∇L(θ∗)]i=∂
∂θiL(θ∗
i) ký hiệu Jacobian và [Hθ]ij=∂2
∂θiθjL(θij) ký hiệu Hessian.
Số hạng bậc một biến mất [∇L(θ∗)]i=0 tại điểm tối ưu. Lưu ý rằng trong thực tế số hạng bậc một

2

--- TRANG 3 ---
Hình 2: Cắt tỉa như tối ưu hóa có ràng buộc đẳng thức của xấp xỉ bậc hai của cảnh quan mất mát (trái), hoặc tương đương, tối đa hóa likelihood dưới xấp xỉ Laplace (phải).

có thể không biến mất. Trong khi chúng tôi tuân theo giả định này ban đầu, chúng tôi xem xét các sửa chữa bậc một xen kẽ để giảm thiểu vấn đề trong mục 3.6. Khai triển bậc hai của phương trình (3) tạo thành cơ sở của các phương pháp cắt tỉa optimal brain damage (LeCun et al., 1989) và optimal brain surgeon (Hassibi & Stork, 1992). Lưu ý rằng từ góc độ xác suất, một xấp xỉ bậc hai của log likelihood ngụ ý một xấp xỉ Gaussian của likelihood, như cũng được quan sát bởi (Wang et al., 2019) và được minh họa trong hình 2. Điều này được biết đến rộng rãi (Bishop & Nasrabadi, 2006), (MacKay, 2003) như xấp xỉ Laplace q(θ) =N(θ|θ∗+∇L(θ∗),H−1
θ∗), với các trọng số đã được huấn luyện sẵn là trung bình và Hessian nghịch đảo cục bộ là ma trận hiệp phương sai nắm bắt các tương quan giữa các trọng số.

2.2 MA TRẬN THÔNG TIN FISHER THEO KHỐI

Đối với một mạng được huấn luyện với mất mát negative log-likelihood, Hessian giống hệt với ma trận Fisher:

Hθ=Fθ=XN
n=1Ey∼pθ(y|xn)
∇θlogpθ(y|xn)∇θlogpθ(y|xn)T
(4)

có lợi ích luôn luôn là positive semi-definite, với nghịch đảo do đó tạo thành một ma trận hiệp phương sai thích hợp cho q, và có thể được xấp xỉ với các mẫu Monte Carlo của pθ(y|xn). Đối với hầu hết các LLM, điều này sẽ coi đầu ra softmax của mạng như phân phối categorical pθ(y|xn), và lấy mẫu từ đó. Trong thực tế, chúng tôi sử dụng 'Fisher thực nghiệm' thay thế kỳ vọng trên y bằng dữ liệu mục tiêu yn (Kunstner et al., 2019). Fisher đầy đủ (thực nghiệm) Fθ∈RP×P tăng theo bậc hai theo số lượng tham số P. Để khắc phục điều này, Fisher thường được viết theo các khối theo lớp Flk=PN
n=1E
vec(∇Wllogpθ(y|xn))vec(∇Wklogpθ(y|xn))T
, và được xấp xỉ bằng cách chỉ coi các lớp một cách độc lập (Martens & Grosse, 2015; Botev et al., 2017):

Fθ=diag(F11,F22, . . . ,FLL), Fl=XN
n=1Eh
(gl,ngT
l,n)⊗(al,naT
l,n)
| {z }
RC×RCi
(5)

trong đó ⊗ ký hiệu tích Kronecker và vec (·) phép toán vector hóa ma trận. Bởi vì chúng tôi bỏ qua các tương tác giữa các lớp, chúng tôi viết Fl thay vì Fll cho các khối Fisher liên quan đến ma trận trọng số Wl∈RR×C tạo ra đầu ra yl,n=Wlal,n∈RR từ đầu vào al,n∈RC, cho mỗi lớp l và điểm dữ liệu n. Do đó, chúng ta có thể tính toán các khối Fisher từ các kích hoạt đầu vào al,n∈RC của dữ liệu được truyền xuôi xn và các gradient đầu ra gl,n=∇yl,nL∈RR từ backpropagation.

2.3 CẮT TỈA NHƯ TỐI ƯU HÓA CÓ RÀNG BUỘC

Optimal brain surgery dựa vào việc loại bỏ và điều chỉnh các trọng số sao cho mất mát bị ảnh hưởng tiêu cực ít nhất, do đó chúng ta nên viết bài toán như một bài toán tối ưu hóa có ràng buộc. Từ xấp xỉ Gaussian được thảo luận trong mục 2.1 thu được bằng cách khai triển bậc hai log likelihood loss −logp≈1
2θTFθ, cập nhật tối ưu ∆θ=ˆθ−θ (và do đó cũng là ˆθ=θ+∆θ) trở thành bài toán tối ưu hóa bậc hai có ràng buộc đẳng thức sau (Hassibi & Stork, 1992):

arg min
∆θ1
2∆θTF∆θ (6)
s.t.eT
k∆θ+eT
kθ= 0,∀k∈ K

trong đó F là positive semi-definite và K là tập hợp K chỉ số được cắt tỉa (tức là đặt bằng không).

3

--- TRANG 4 ---
Thuật toán 1 LLM Surgeon (có cấu trúc)
Đầu vào: trọng số ban đầu θ0, kích thước mục tiêu α, và dữ liệu D
Cho shot t trong [1, 2, . . . , T]
Tính toán: độ cong xấp xỉ G,A từ dữ liệu D ▷ mục 3.1
Tính toán: chi phí mỗi hàng/cột Lr,Lc từ G,A ▷ mục 3.2
Tính toán: ngưỡng τ sử dụng Lr và Lc cho kích thước mục tiêu αt ▷ mục 3.3
Chọn: các hàng và cột để loại bỏ ER,EC dựa trên τ ▷ mục 3.3
Tính toán: cập nhật trọng số ∆θt−1 dựa trên ER,EC và G,A ▷ mục 3.4
Cập nhật: các trọng số còn lại θt←θt−1+ ∆θt−1▷ mục 3.5
Tùy chọn: θt←cập nhật hạng thấp (θt) ▷ mục 3.6
Đầu ra: trọng số nén ˆθ=θT

Giải pháp tổng quát Chúng tôi ký hiệu EK= [e1e2. . .eK]T∈[0,1]K×P như một ma trận mà các vector hàng là các vector cơ sở canonical ek∈RP chọn các phần tử được cắt tỉa. Một trong những phương pháp tiêu chuẩn nhất để giải phương trình (6) là sử dụng nhân tử Langrange, dẫn đến một giải pháp dạng đóng tổng quát cho sự gia tăng dự kiến trong mất mát L và cập nhật trọng số tối ưu ∆θ:

L=1
2(EKθ∗)T
EKF−1ET
K−1EKθ (7)

∆θ=−F−1ET
K
EKF−1ET
K−1EKθ (8)

mà chúng tôi sử dụng để rút ra không có cấu trúc, bán cấu trúc, có cấu trúc cho các xấp xỉ Fisher hiện đại (xem phụ lục A.2 đến A.4). Cùng dạng tổng quát của phương trình (7) và (8) xuất hiện trong công trình cắt tỉa LLM trước đó Kurtic et al. (2022), nhưng chỉ cho việc cắt tỉa theo lớp đơn giản hơn nhiều và không có cắt tỉa có cấu trúc.

3 LLM SURGEON

Mục này mô tả các thành phần của phương pháp của chúng tôi, LLM Surgeon, được tóm tắt trong thuật toán 1.

3.1 ƯỚC TÍNH ĐỘ CONG CẢNH QUAN MẤT MÁT

Cắt tỉa chính xác dựa vào việc xấp xỉ độ cong cục bộ một cách chính xác trong khi khắc phục chi phí bộ nhớ liên quan đến việc lưu trữ độ cong thực. Cụ thể, ngay cả với xấp xỉ theo khối của phương trình (5), F∈RRC×RC yêu cầu tổng N ma trận RC×RC lớn, quá lớn để thực tế vừa trong bộ nhớ. Thay vào đó, chúng tôi điều chỉnh xấp xỉ KFAC (Martens & Grosse, 2015) giả định tính độc lập của các kích hoạt và đạo hàm, xấp xỉ một kỳ vọng của các tích Kronecker như một tích Kronecker của hai kỳ vọng E[gl,ngT
l,n⊗al,naT
l,n]≈E[gl,ngT
l,n]⊗E[al,naT
l,n], cho phép các khối Fisher theo lớp được xấp xỉ như Fl≈eFl, trong đó

eFl=Gl⊗Al, với Gl=1√
NXN
n=1gl,ngT
l,n và Al=1√
NXN
n=1al,naT
l,n (9)

được xây dựng từ các kích hoạt al,n∈RC từ các lần truyền xuôi và gradient gl,n∈RR từ các lần truyền ngược (Eschenhagen et al., 2024). Xấp xỉ này xuất phát từ văn học tối ưu hóa, nhưng gần đây đã trở nên phổ biến cho các bài toán khác yêu cầu xấp xỉ độ cong (Immer et al., 2022; van der Ouderaa et al., 2023), bao gồm cắt tỉa có cấu trúc trong Wang et al. (2019).

Một lợi thế bổ sung của việc xấp xỉ các khối Fisher như các tích Kronecker là nghịch đảo trở nên đặc biệt dễ tính toán eF−1=G−1⊗A−1, do đó chỉ yêu cầu nghịch đảo các nhân tử. Thực tế này cho phép chúng ta không bao giờ xây dựng rõ ràng các ma trận RC×RC lớn trong bộ nhớ tạo nên eF và eF−1, mà thay vào đó làm việc trực tiếp với các ma trận G và A nhỏ hơn nhiều.

3.2 TÍNH TOÁN CHI PHÍ TRONG MẤT MÁT CUỐI CÙNG

Số lượng các tổ hợp có thể trong đó các trọng số có thể được loại bỏ tăng (siêu-)mũ theo số lượng tham số, khiến việc ước tính một chi phí L riêng biệt cho mỗi việc loại bỏ như vậy trở nên không khả thi. Do đó, một chiến lược phổ biến là coi các trọng số một cách độc lập khi tính toán chi phí loại bỏ L. Chúng tôi cũng tuân theo chiến lược này, nhưng lưu ý rằng điều này không nhất thiết ngụ ý rằng chúng ta phải đưa ra giả định độc lập mạnh tương tự cho các cập nhật trọng số ∆θ sau khi chọn các trọng số để loại bỏ.

4

--- TRANG 5 ---
Không giống như hầu hết các công trình trước đó, chúng tôi trình bày các cập nhật trọng số có tương quan bằng cách tính đến các phần tử ngoài đường chéo của xấp xỉ Fisher trong mục 3.4.

Đối với bán cấu trúc và không có cấu trúc, chúng tôi sử dụng chi phí độc lập cho các phần tử trọng số riêng lẻ k∈[1, RC], và đối với có cấu trúc sử dụng chi phí độc lập cho tất cả các hàng r∈[1, R] và cột c∈[1, C]. Chúng tôi thấy rằng chúng ta có thể rút ra các chi phí thích hợp từ công thức chi phí tổng quát phương trình (7) bằng cách để E=ek∈RRC trong đó phần tử one-hot duy nhất tại chỉ số k của vector cơ sở canonical ek chọn trọng số để loại bỏ. Đối với cắt tỉa có cấu trúc, chúng ta tương tự chọn các hàng r và cột c, bằng cách đặt E=eT
r⊗I∈RC×RC hoặc E=I⊗ec∈RR×RC với er∈RR,ec∈RC. Thay vào phương trình (7), chúng ta tìm thấy:

Lk=1
2(θk)2
[G−1⊗A−1]kk, Lr=1
2θT
rAθr
[G−1]rr,Lc=1
2θT
cGθc
[A−1]cc(10)

Các rút gốn đầy đủ có thể được tìm thấy trong phụ lục A.2 và A.3. Chi phí cho các phần tử đơn Lk tương đương với những chi phí được tìm thấy trong optimal brain surgeon (Hassibi & Stork, 1992) và Lr và Lc gần giống với structured brain surgeon của (Wang et al., 2019), nhưng trong trường hợp của chúng tôi được rút ra cho các hàng và cột ma trận (xem phụ lục A.3). Cho các ước tính độ cong, chi phí cho việc loại bỏ tất cả trọng số hoặc tất cả hàng và cột có thể được tính toán song song. Ngoài ra, chúng tôi rút ra chi phí cho tổng xấp xỉ nhân tử Kronecker tổng quát hơn eF≈G1⊗A1+G2⊗A2 trong phụ lục I thông qua một phân tích eigendecomposition.

3.3 PHÂN BỔ TRỌNG SỐ ĐỘNG VỚI NGƯỠNG TOÀN CỤC

W gốc
chi phí hàng r, 
chi phí cột c
có cấu trúc
W+W
chi phí phần tử k
bán cấu trúc
W+W
chi phí phần tử k
không có cấu trúc
W+W

Trọng số đã cắt tỉa Cập nhật trọng số còn lại

Hình 3: Khung tổng quát cho nén có cấu trúc, bán cấu trúc và không có cấu trúc.

Không giống như các công trình trước đây cắt tỉa từng lớp một (Frantar & Alistarh, 2023), chúng tôi sử dụng một ngưỡng toàn cục τ cho phép phân bổ động các mức độ thưa thớt giữa các lớp, cắt tỉa nhiều nhất ở nơi nó gây tổn hại ít nhất. Phương pháp của chúng tôi có thể nén một mô hình đến một kích thước mục tiêu được chọn cụ thể α, được định nghĩa là phần trăm trọng số nên còn lại, tức là vẫn khác không sau khi nén. Trong tất cả việc cắt tỉa có cấu trúc, bán cấu trúc và không có cấu trúc (hình 3), chúng tôi chọn nhiều trọng số để loại bỏ sao cho kích thước mục tiêu α được đạt đến và gây ra chi phí L ít nhất có thể, như được tính toán theo mục 3.2. Đối với cắt tỉa không có cấu trúc, điều này đơn giản như sắp xếp các chi phí cho tất cả trọng số Lk trong mạng và đặt một ngưỡng toàn cục τ sao cho phần α của trọng số nằm trong ngưỡng Lk≤τ. Đối với cắt tỉa bán cấu trúc M:N, chúng tôi sắp xếp M chi phí của mỗi N trọng số liên tiếp và chọn M trọng số với chi phí thấp nhất. Trong trường hợp lịch trình nhiều shot (xem mục 3.5), chúng tôi cũng tổng M chi phí thấp nhất trong mỗi khối để tìm chi phí mỗi khối, sắp xếp chi phí mỗi khối trên toàn bộ mạng, và tương tự như trường hợp không có cấu trúc đặt một ngưỡng toàn cục τ sao cho phần α của trọng số nằm trong ngưỡng. Cuối cùng cho cắt tỉa có cấu trúc, chúng tôi thực hiện sắp xếp được trọng số thích hợp bởi số lượng phần tử tạo nên một hàng hoặc cột và đặt ngưỡng toàn cục τ sao cho phần α của tất cả trọng số nằm trong ngưỡng. Sau đó chúng tôi loại bỏ tất cả các hàng và cột nằm trong ngưỡng Lr,Lc≤τ.

3.4 CẬP NHẬT TRỌNG SỐ CÓ TƯƠNG QUAN

Giống như hầu hết các phương pháp cắt tỉa khác, chúng tôi cắt tỉa nhiều trọng số cùng một lúc (Frantar & Alistarh, 2023; Wang et al., 2019). Để đạt được chi phí cắt tỉa và cập nhật trọng số cho việc cắt tỉa nhiều trọng số, việc thông thường là tính toán chi phí và cập nhật cho các trọng số riêng lẻ (hoặc tập hợp trọng số) một cách độc lập và cộng chúng lại để đạt được chi phí cắt tỉa kết hợp. Trong LLM Surgeon, chúng tôi lập luận rằng tốt hơn là xem xét các cập nhật trọng số kết hợp thay vì độc lập. Sau khi chọn tập hợp trọng số để cắt tỉa, chúng ta thường có thể đủ khả năng để tính toán một cập nhật trọng số có tương quan duy nhất liên quan đến việc loại bỏ kết hợp của nhiều trọng số, thay vì cộng một cách ngây thơ các cập nhật trọng số liên quan đến các loại bỏ riêng lẻ. Chúng tôi rút ra các cập nhật trọng số có tương quan như vậy dưới đây. Lưu ý rằng, đối với việc tính toán chi phí dự kiến, chúng tôi giả định rằng chi phí hàng, cột hoặc trọng số là độc lập, vì số lượng tổ hợp có thể của trọng số để cắt tỉa tăng quá lớn để tính toán trong thời gian hợp lý.

Cập nhật trọng số có tương quan không có cấu trúc / bán cấu trúc nhanh Về mặt toán học, chúng tôi biểu diễn các trọng số đã cắt tỉa như EK= [e1e2. . .eR′]T∈RK×RS, trong đó er∈RR′ là các

5

--- TRANG 6 ---
vector cơ sở canonical one-hot chọn các trọng số để loại bỏ. Vì mỗi phần tử k có một chỉ số hàng r và cột c liên quan duy nhất, chúng ta do đó cũng có thể sử dụng các vector cơ sở canonical cho các hàng ER∈RK×R và cột EC∈RK×C tương ứng (tức là, chúng ta có [ER]i⊗[EC]i=[EK]i được thỏa mãn cho tất cả i). Chúng tôi rút ra các cập nhật trọng số không có cấu trúc trong phụ lục A.2, bằng cách xem xét các eigendecomposition G=K1S1KT
1,A=K2S2K2 của xấp xỉ Fisher F≈G⊗A, từ phương trình (8) tạo ra:

∆W=G−1
K1
KT
1W−1K2⊘S| {z }
K×K−1
K2
A−1(11)

trong đó ⊘ là phép chia theo phần tử, và để ngắn gọn sử dụng ký hiệu thanh K1=EKK1,K2=EKK2, θ=EKθ, và S=diag(S1)diag(S2)T∈RR×C, và diag (·) vector hóa các đường chéo ma trận.

Về mặt lập trình, chúng tôi luôn tránh biểu diễn rõ ràng các ma trận lớn eF và eF−1 trong bộ nhớ, mà thay vào đó tính toán các đại lượng liên quan từ các nhân tử của chúng. Tương tự, chúng tôi không bao giờ biểu diễn các ma trận thưa EK,ER hoặc EC trong bộ nhớ, mà thay vào đó làm việc với danh sách các chỉ số của các phần tử one-hot trực tiếp. Ví dụ, chúng ta có thể xây dựng rẻ K1=ERK1∈RK×R và K2=ECK2∈RK×C, bằng cách sao chép các vector hàng, và vector θ=EKθ=ERWET
C∈RK bằng cách lập chỉ mục tất cả các trọng số đã cắt tỉa.

Số lượng tối đa trọng số có tương quan Nút thắt tính toán chính là nghịch đảo ma trận K×K trong phương trình (11). Để kiểm soát tốc độ nén, chúng ta có thể chia các trọng số đã cắt tỉa thành các tập con rời rạc K=K1∪K2∪. . ., sao cho mỗi tập con Ki không vượt quá số lượng tối đa trọng số có tương quan Ki≤m, và tổng các cập nhật độc lập liên quan. Sử dụng ít tương quan hơn bằng cách đặt m thấp hơn cho phép đánh đổi chất lượng nén với tốc độ.

Cập nhật trọng số có tương quan có cấu trúc nhanh Không giống như trường hợp tổng quát yêu cầu nghịch đảo ma trận K×K cho K trọng số có tương quan, chúng tôi thấy rằng các cập nhật trọng số với xấp xỉ Fisher được phân tích Kronecker ˜F=G⊗A chỉ yêu cầu nghịch đảo ma trận R′×R′ khi loại bỏ R′ hàng hoặc ma trận C′×C′ khi loại bỏ C′ cột. Các cập nhật rẻ hơn nhiều so với những gì chúng ta mong đợi dựa trên số lượng trọng số hiệu quả trong những hàng và cột đó, điều này sẽ ngụ ý nghịch đảo các ma trận R′C×R′C hoặc RC′×RC′. Trong thực tế, điều này dẫn đến tăng tốc đáng kể cho việc cắt tỉa có cấu trúc và các cập nhật trọng số tính đến tương quan giữa các hàng hoặc cột.

Khi loại bỏ R′ hàng, r1, r2, . . . rR′, hoặc C′ cột c1, c2, . . . , cC′, với 1<R′< R và 1<C′<C, chúng tôi ký hiệu các vector one-hot chọn tất cả hàng và cột được loại bỏ tương ứng là ER′= [e1e2. . .eR′]T∈RR′×R và EC′= [e1e2. . .eC′]T∈RC′×C. Chúng tôi tìm thấy các cập nhật trọng số liên quan đến việc loại bỏ R′ hàng bằng cách đặt EK=ER′⊗I hoặc EK=I⊗EC′:

loại bỏ nhiều R′ hàng:
loại bỏ nhiều C′ cột: ∆W=−W(EC′A−1ET
C′)−1(A−1ET
C′)
∆W=−G−1ET
R′(ER′G−1ET
R′)−1W(12)

Từ đây, rõ ràng rằng trường hợp đặc biệt của việc loại bỏ một hàng r hoặc cột c duy nhất dưới xấp xỉ Kronecker liên quan đến việc nghịch đảo ma trận 1×1, và do đó chỉ yêu cầu phép chia vô hướng:

loại bỏ hàng r duy nhất: ∆θ=−G−1er⊗θr
[G−1]rr, hoặc cột c duy nhất: ∆θ=−θc⊗A−1ec
[A−1]cc(13)

phù hợp với các cập nhật có cấu trúc độc lập trong Wang et al. (2019), cho các bộ lọc tích chập. Chúng tôi đã mở rộng các cập nhật trọng số có cấu trúc hiện có cho các hàng và cột, và rút ra các quy tắc cập nhật cũng xem xét tương quan giữa các nhóm có cấu trúc (trong trường hợp của chúng tôi là các hàng và cột).

3.5 LỊCH TRÌNH CẮT TỈA NHIỀU SHOT

Để cải thiện tỷ lệ hiệu suất so với độ thưa thớt, chúng tôi đề xuất cắt tỉa trong nhiều shot. Chúng tôi biện minh lý thuyết cho phương pháp nhiều shot này bằng cách lưu ý rằng cảnh quan mất mát đại lý q dựa vào khai triển Taylor (phương trình (3)) chỉ giữ cục bộ và do đó trở nên không đáng tin cậy cho các bước nhảy lớn hơn ∆θ trong không gian tham số. Chúng tôi giảm thiểu điều này bằng cách cắt tỉa trong nhiều T>1 shot, t∈[1,2, . . . , T], mỗi shot dẫn đến một cập nhật trọng số nhỏ hơn ∆θ sau đó độ cong của bề mặt mất mát có thể được ước tính lại.

Khi cắt tỉa đến kích thước mục tiêu α, tức là loại bỏ 1−α của tổng trọng số, chúng tôi chọn một lịch trình αt bắt đầu từ α0= 1 và kết thúc với αT=α, sao cho sau T shot, chính xác phần α của tổng trọng số còn lại. Về mặt thực nghiệm, chúng tôi thấy rằng một lịch trình tuyến tính cho αt, như được công thức hóa trong mục 4, cải thiện một cách đơn điệu

6

--- TRANG 7 ---
Bảng 1: Nén có cấu trúc của các mô hình ngôn ngữ lớn trên dữ liệu wikitext-2.

Hiệu suất kiểm tra (PPL)
Phương pháp Kích thước mục tiêu OPT (125m) OPT (1.3b) OPT (2.7b) OPT (6.7b) Llama-v2 (7b)
Baseline 100% 27.65 14.62 12.47 10.86 5.12
Magnitude 90% 767.2 894.4 1229 3464 36746
I⊗I 80% 4685 (1278) 2788 16747 347960
70% 17970 (3098) 9255 17312 41373
L-OBD 90% 33.3 20.76 17.69 27.20 14259
diag(I⊗A) 80% 94.14 1392 3236 7570 15630
nhiều shot 70% 545.6 2147 7233 7628 21386
K-OBD 90% 27.97 14.68 11.96 10.53 5.48
diag(G⊗A) 80% 29.89 15.63 12.47 11.28 9.14
nhiều shot 70% 36.54 18.29 14.53 13.03 15.43
60% 47.54 24.65 18.09 16.21 28.03
50% 75.95 37.68 26.68 25.54 46.64
LLM Surgeon (của chúng tôi) 90% 28.29 14.73 12.00 10.82 5.43
G⊗A 80% 29.37 15.27 12.37 11.22 7.29
trong hàng/cột cor. ∆ 70% 32.46 16.60 13.16 11.83 10.85
60% 39.82 19.40 14.79 12.94 16.67
50% 51.48 23.81 18.01 15.38 25.62
LLM Surgeon (của chúng tôi) 90% 28.01 14.70 12.02 10.77 5.25
G⊗A 80% 28.73 15.12 12.27 11.02 6.18
đầy đủ cor. ∆ 70% 31.82 16.24 12.92 11.64 7.83
60% 38.47 18.45 14.23 12.58 10.39
50% 49.78 22.95 17.15 14.90 15.38

hiệu suất cắt tỉa với nhiều shot hơn, và các mức độ thưa thớt cao hơn thường yêu cầu nhiều shot hơn (xem phụ lục F.1). Cắt tỉa nhiều shot cho phép người ta chi tiêu (tuyến tính theo T) nhiều tính toán hơn để cải thiện hiệu suất nén cuối cùng.

3.6 CÁC SỬA CHỮA BẬC MỘT HẠNG THẤP XEN KẼ

Chúng tôi đề xuất các sửa chữa bậc một hạng thấp xen kẽ tùy chọn để cải thiện thêm hiệu suất nén. Cho đến nay, chúng tôi giả định các tham số ở trong một tối ưu cục bộ khi tìm giải pháp dạng đóng cho bài toán ràng buộc bậc hai. Tuy nhiên, trong thực tế, giả định này có thể không đúng vì (i) mạng nơ-ron có thể không được tối ưu hóa đến mức tối thiểu, (ii) một mất mát khác có thể được sử dụng cho nén so với được sử dụng cho huấn luyện, hoặc (iii) chúng ta cắt tỉa trong nhiều shot (mục 3.5) không tránh khỏi làm cho các trọng số phân kỳ khỏi tối ưu. Để giảm thiểu điều này, chúng tôi xem xét các sửa chữa bậc một bằng cách xen kẽ các shot cắt tỉa với các điều chỉnh hạng thấp của trọng số Wl+UV (LoRA, bởi (Hu et al., 2021)), thường được sử dụng trong fine-tuning LLM. Chúng tôi luôn hấp thụ các cập nhật sau mỗi shot, để ước tính mất mát q tiếp theo gần hơn với tối ưu và các giả định cơ bản có khả năng đúng hơn. Bằng cách hấp thụ các cập nhật LoRA giữa các shot, tổng các cập nhật hạng thấp có thể có hạng cao hơn các cập nhật riêng lẻ. Đó là, chúng ta có rank (U1V1+U2V2+. . .+UTVT)≥ rank(UtVt) cho các cập nhật UtVt tại bất kỳ shot t nào, với đẳng thức chỉ xảy ra nếu các cập nhật nằm chính xác trong cùng một không gian con mà khó có thể xảy ra trong thực tế. Thông tin này cũng có thể được sử dụng trong fine-tuning LoRA thông thường và do đó có thể hữu ích bên ngoài bối cảnh nén mô hình để cho phép điều chỉnh mô hình hạng thấp biểu cảm hơn, với chi phí không đáng kể.

4 KẾT QUẢ

Chúng tôi so sánh hiệu suất nén của LLM Surgeon trên các tác vụ mô hình hóa ngôn ngữ trên các họ mô hình OPT (Zhang et al., 2022) và Llama-v2 (Touvron et al., 2023), sử dụng dữ liệu từ tập dữ liệu wikitext-2 (phụ lục B.2). Để nén, chúng tôi sử dụng 128 chuỗi với độ dài chuỗi 2048 token từ tập dữ liệu huấn luyện và đánh giá test perplexity (PPL) trên phần test tiêu chuẩn. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng lịch trình thưa thớt tuyến tính αt=1−t(1−α
T) tại mỗi shot s trước khi đạt đến thưa thớt cuối cùng α. Chúng tôi sử dụng 40 shot ở α=0.5 thưa thớt và báo cáo các tỷ lệ nén trung gian, hiệu quả sử dụng T=8 shot cho α=0.9, T=16 cho α=0.8, T=24 cho α=0.7, và T=32 cho α=0.6. Chúng tôi so sánh với các baseline magnitude pruning, L-OBD, SparseGPT và K-OBD. K-OBD và LLM Surgeon sử dụng quy trình nhiều shot của mục 3.5 sử dụng T=40 shot cho cắt tỉa có cấu trúc và T=5 shot cho cắt tỉa bán cấu trúc và không có cấu trúc. Chi tiết thêm được tìm thấy trong phụ lục B.

7

--- TRANG 8 ---
4.1 NÉN CÓ CẤU TRÚC

Nén có cấu trúc của các hàng và cột cho phép tiết kiệm trực tiếp về bộ nhớ và tính toán thông qua việc giảm thẳng kích thước ma trận trong mô hình. Đối với LLM surgeon, chúng tôi xem xét trong mục 3.4 các cập nhật trọng số với các mức độ tương quan khác nhau: giới hạn trong các tương quan trong hàng và cột, và tương quan cả trong và giữa các hàng và cột. Chúng tôi tiếp tục so sánh với magnitude pruning, chỉ sử dụng độ lớn trọng số, L-OBD, chỉ sử dụng kích hoạt, và K-OBD, cũng sử dụng độ cong được phân tích Kronecker nhưng giả định độc lập hoàn toàn và do đó chỉ cắt tỉa mà không cập nhật các trọng số còn lại. Chúng tôi báo cáo kết quả trong bảng 1, và quan sát thấy nhiều tương quan hơn dẫn đến hiệu suất tốt hơn, với những cải thiện lớn nhất cho họ mô hình Llama-v2.

Trong khi nén có cấu trúc 50% không tốt hơn một mô hình nhỏ hơn có kích thước tương tự, LLM Surgeon cho phép chúng ta giảm kích thước mô hình lên đến 30% với mất mát tối thiểu, mà không cần huấn luyện một mô hình nhỏ hơn từ đầu (hình 1). Trong các thí nghiệm nén có cấu trúc của chúng tôi, phương pháp LLM Surgeon được đề xuất vượt trội hơn tất cả các baseline và đạt hiệu suất tốt nhất cho mỗi kích thước nén mục tiêu.

4.2 CẬP NHẬT HẠNG THẤP XEN KẼ

Bảng 2: Nén có cấu trúc của OPT-125m trên wikitext-2 sử dụng cập nhật LoRA xen kẽ

Kích thước không có với
mục tiêu LoRA LoRA
Đã huấn luyện sẵn 100% 27.65 23.35
LLM Surgeon 90% 28.01 24.16
(của chúng tôi) 80% 28.73 25.25
G⊗A 70% 31.82 28.86
đầy đủ cor. ∆ 60% 38.47 31.26
50% 49.78 36.50

Ngoài ra, chúng tôi đánh giá hiệu suất nén kết hợp với các sửa chữa bậc một được đề xuất sử dụng điều chỉnh hạng thấp xen kẽ được mô tả trong mục 3.6. Chúng tôi thấy rằng LoRA cải thiện hiệu suất nén trong mô hình 125m nhỏ nhất, nhưng không trong các mô hình lớn hơn. Chúng tôi giả thuyết rằng các mô hình lớn hơn dễ bị overfitting trên tương đối ít batch dữ liệu wikitext-2 được sử dụng để nén mô hình. Tuy nhiên, chúng tôi kết luận rằng LoRA xen kẽ có thể hữu ích trong các trường hợp, và khuyến nghị trước tiên sử dụng phương pháp được đề xuất mà không có cập nhật xen kẽ và, nếu có đủ dữ liệu để nén, tùy chọn sử dụng nó nếu nó cải thiện hiệu suất.

4.3 NÉN BÁN CẤU TRÚC

Đối với cắt tỉa bán cấu trúc 2:4, chúng tôi so sánh LLM Surgeon với magnitude pruning, chỉ sử dụng độ lớn trọng số, L-OBD một shot, chỉ sử dụng kích hoạt, và K-OBD một shot, cũng sử dụng độ cong được phân tích Kronecker nhưng giả định độc lập hoàn toàn và do đó chỉ cắt tỉa mà không cập nhật các trọng số còn lại cũng như SparseGPT hiện đại (Frantar & Alistarh, 2023). Chúng tôi báo cáo hiệu suất kiểm tra sau nén bán cấu trúc 50% (2:4) trên dữ liệu wikitext-2 trong bảng 3. Chúng tôi thấy rằng việc xem xét nhiều tương quan trọng số hơn dẫn đến hiệu suất cuối cùng được cải thiện sau khi nén. LLM Surgeon được đề xuất của chúng tôi có tính cạnh tranh với các công trình trước đó, vượt trội hơn tất cả các baseline về test set perplexity (PPL).

Bảng 3: Nén bán cấu trúc 2:4 cho các mô hình ngôn ngữ lớn trên dữ liệu wikitext-2.

Hiệu suất kiểm tra (PPL)
Phương pháp F≈ Kích thước mục tiêu OPT (125m) OPT (1.3b) OPT (2.7b) OPT (6.7b)
Baseline 100% 27.65 14.62 12.47 10.86
Magnitude I⊗I 50% 342.04 379.57 1106.01 187.29
L-OBD diag (I⊗A) 50% 87.26 44.92 41.40 27.36
K-OBD diag (G⊗A) 50% 68.74 27.22 20.23 15.55
SparseGPT I⊗A 50% 45.51 29.44 14.92 13.01
LLM Surgeon (của chúng tôi) G⊗A 50% 44.64 25.10 14.64 12.10

4.4 NÉN KHÔNG CÓ CẤU TRÚC

Đối với cắt tỉa không có cấu trúc, chúng tôi lặp lại cùng các thí nghiệm như trường hợp cắt tỉa có cấu trúc được mô tả trong mục 4.1. Trong bảng 4, chúng tôi báo cáo hiệu suất kiểm tra cuối cùng về perplexity (PPL) trên wikitext-2 sau khi nén LLM có kích thước khác nhau của họ OPT và Llama-v2. Nhìn chung, chúng tôi thấy rằng các phương pháp với xấp xỉ chính xác hơn của cảnh quan độ cong và tính đến nhiều tương quan hơn hoạt động tốt hơn. LLM Surgeon được đề xuất vượt trội hơn tất cả các baseline, đạt hiệu suất kiểm tra cao nhất trên các kích thước mục tiêu.

8

--- TRANG 9 ---
Bảng 4: Nén không có cấu trúc của các mô hình ngôn ngữ lớn trên dữ liệu wikitext-2.

Hiệu suất kiểm tra (PPL)
Phương pháp Kích thước mục tiêu OPT (125m) OPT (1.3b) OPT (2.7b) OPT (6.7b) Llama-v2 (7b)
Baseline 100% 27.65 14.62 12.47 10.86 5.12
Magnitude 90% 27.62 14.69 12.60 10.88 5.18
I⊗I 80% 28.53 15.68 13.18 11.26 5.37
70% 52.88 140.2 15.22 12.22 6.03
L-OBD 90% 29.70 16.24 14.44 13.43 6.09
diag(I⊗A) 80% 32.18 21.92 23.35 39.85 116.2
một shot 70% 49.08 204.7 274.8 810.4 6549
K-OBD 90% 27.64 14.62 12.09 36.89 5.13
G⊗A 80% 27.62 14.37 130220 39928 5.19
một shot 70% 27.92 220.1 23097 19506 5.60
60% 29.24 13783 10331 33896 9.20
50% 34.43 7311 10495 91506 118.6
SparseGPT 90% 27.93 14.69 12.00 10.86 5.49
I⊗A 80% 28.18 15.07 12.05 10.86 5.58
70% 28.93 22.77 12.17 10.89 5.71
60% 30.20 25.07 12.37 10.98 5.94
50% 33.17 26.77 12.88 11.92 6.51
LLM Surgeon (của chúng tôi) 90% 27.69 14.62 12.01 10.86 5.13
G1⊗A1 80% 27.83 14.66 12.14 10.87 5.20
đầy đủ cor. ∆ 70% 28.35 14.81 12.25 10.82 5.36
nhiều shot 60% 28.98 14.91 12.28 10.83 5.66
50% 30.30 15.47 12.68 10.97 6.08

4.5 CẤU TRÚC THƯA THỚT ĐÃ HỌC

Phương pháp được đề xuất có thể phân bổ động các mức độ thưa thớt giữa các lớp thông qua các ngưỡng toàn cục được mô tả trong mục 3.3. Trong Hình 4.5, chúng tôi so sánh tổng các mức độ thưa thớt được phân bổ theo độ sâu lớp và theo loại lớp sau khi nén mô hình OPT-125m đã được huấn luyện sẵn. Chúng tôi thấy rằng LLM Surgeon cắt tỉa tương đối nhiều hơn ở lớp đầu tiên và ít hơn ở các lớp giữa. Hơn nữa, chúng tôi quan sát thấy rằng một phần lớn hơn của trọng số được loại bỏ trong các khối fully-connected so với các khối attention, nhưng các sai lệch ít hơn so với các phương pháp khác. Phân bổ động cho phép cắt tỉa nhiều nhất ở nơi nó gây tổn hại ít nhất.

123456789101112
lớp0%20%40%60%80%100%trọng số còn lạiThưa thớt theo độ sâu lớp

Q K V O FC1 FC2
loại lớp0%20%40%60%80%100%Thưa thớt theo loại lớp

LLM Surgeon (đầy đủ cor)
LLM Surgeon (trong hàng/cột cor.)
Magnitude
L-OBD
K-OBD

Hình 4: Các mức độ thưa thớt thu được với cắt tỉa có cấu trúc trên OPT-125m theo độ sâu lớp và loại.

5 KẾT LUẬN

Trong công trình này, chúng tôi đã giới thiệu thuật toán LLM Surgeon cho nén không có cấu trúc, bán cấu trúc và có cấu trúc của mạng nơ-ron. Công trình xây dựng dựa trên các phương pháp nén mạng nơ-ron cổ điển xuất phát từ đầu những năm 1990 nhằm tìm cắt tỉa tối ưu bằng cách mở rộng độ cong của cảnh quan mất mát. Phương pháp sử dụng các xấp xỉ Fisher hiện đại để mở rộng cắt tỉa chính xác đến lĩnh vực các mô hình ngôn ngữ lớn (LLM) với hàng tỷ tham số, trong khi vẫn thực tế về cả bộ nhớ và tính toán. Không giống như hầu hết các công trình trước đó về nén LLM dựa trên dữ liệu, chúng tôi không chỉ sử dụng độ lớn trọng số và kích hoạt từ các lần truyền xuôi, mà còn sử dụng thông tin gradient từ các lần truyền ngược để liên quan chi phí loại bỏ trọng số với mục tiêu cuối cùng thực sự. Chúng tôi cải thiện các công trình trước đó thông qua các xấp xỉ chính xác hơn đối với độ cong cảnh quan mất mát và xem xét nhiều tương quan trọng số hơn để cập nhật các trọng số còn lại. Tăng số lượng tương quan và sử dụng nhiều shot cho phép chúng ta đánh đổi tính toán bổ sung với độ chính xác tốt hơn. Cuối cùng, LLM Surgeon đưa ra kết quả có thể sử dụng thực tế đầu tiên cho cắt tỉa có cấu trúc của LLM và đạt kết quả hiện đại trong cắt tỉa mô hình ngôn ngữ lớn không có cấu trúc và bán cấu trúc.

9

--- TRANG 10 ---
TÀI LIỆU THAM KHẢO

Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, và James
Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv
preprint arXiv:2401.15024 , 2024.

Christopher M Bishop và Nasser M Nasrabadi. Pattern recognition and machine learning , tập
4. Springer, 2006.

Aleksandar Botev, Hippolyt Ritter, và David Barber. Practical gauss-newton optimisation for deep
learning. Trong International Conference on Machine Learning , trang 557–565. PMLR, 2017.

Runa Eschenhagen, Alexander Immer, Richard Turner, Frank Schneider, và Philipp Hennig.
Kronecker-factored approximate curvature for modern neural network architectures. Advances
in Neural Information Processing Systems , 36, 2024.

Elias Frantar và Dan Alistarh. Optimal brain compression: A framework for accurate post-training
quantization and pruning. Advances in Neural Information Processing Systems , 35:4475–4488,
2022.

Elias Frantar và Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in
one-shot. 2023.

Gene H Golub và Charles F Van Loan. Matrix computations . JHU press, 2013.

Babak Hassibi và David Stork. Second order derivatives for network pruning: Optimal brain
surgeon. Advances in neural information processing systems , 5, 1992.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
và Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 , 2021.

Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, và Daniel Soudry. Accurate post training
quantization with small calibration sets. Trong International Conference on Machine Learning , trang
4466–4475. PMLR, 2021.

Alexander Immer, Tycho van der Ouderaa, Gunnar R ¨atsch, Vincent Fortuin, và Mark van der Wilk.
Invariance learning in deep neural networks with differentiable laplace approximations. Advances
in Neural Information Processing Systems , 35:12449–12463, 2022.

Abdoulaye Koroko, Ani Anciaux-Sedrakian, Ibtihel Ben Gharbia, Val ´erie Gar `es, Mounir Haddou,
và Quang Huy Tran. Efficient approximations of the fisher matrix in neural networks using
kronecker product singular value decomposition. arXiv preprint arXiv:2201.10285 , 2022.

Frederik Kunstner, Philipp Hennig, và Lukas Balles. Limitations of the empirical fisher approx-
imation for natural gradient descent. Advances in neural information processing systems , 32,
2019.

Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael
Goin, và Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning
for large language models. arXiv preprint arXiv:2203.07259 , 2022.

Yann LeCun, John Denker, và Sara Solla. Optimal brain damage. Advances in neural information
processing systems , 2, 1989.

Christos Louizos, Max Welling, và Diederik P Kingma. Learning sparse neural networks through
l0regularization. arXiv preprint arXiv:1712.01312 , 2017.

David JC MacKay. Information theory, inference and learning algorithms . Cambridge university
press, 2003.

James Martens và Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. Trong International conference on machine learning , trang 2408–2417. PMLR, 2015.

10

--- TRANG 11 ---
Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843 , 2016.

Mingjie Sun, Zhuang Liu, Anna Bair, và J Zico Kolter. A simple and effective pruning approach
for large language models. arXiv preprint arXiv:2306.11695 , 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.

Tycho van der Ouderaa, Alexander Immer, và Mark van der Wilk. Learning layer-wise equivari-
ances automatically using gradients. Advances in Neural Information Processing Systems , 36,
2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems , 30, 2017.

Chaoqi Wang, Roger Grosse, Sanja Fidler, và Guodong Zhang. Eigendamage: Structured pruning
in the kronecker-factored eigenbasis. Trong International conference on machine learning , trang 6566–
6575. PMLR, 2019.

Wikipedia. Wikipedia . PediaPress, 2004.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Huggingface's transformers:
State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer
language models. arXiv preprint arXiv:2205.01068 , 2022.

Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, và Hong-
sheng Li. Learning n: m fine-grained structured sparse neural networks from scratch. arXiv
preprint arXiv:2102.04010 , 2021.

11

--- TRANG 12 ---
A RÚT GỌN CHO CẮT TỈA

Cho rằng chúng ta sử dụng xấp xỉ Gaussian của mất mát p≈q=N thông qua xấp xỉ bậc hai của log likelihood −logp≈1
2(θ∗)TFθ∗, nén tối ưu nhất trở thành nghiệm của bài toán tối ưu hóa có ràng buộc sau:

arg min
∆θ∗1
2∆(θ∗)TF∆θ∗(14)
s.t.eT
k∆θ∗+eT
kθ∗= 0,∀k∈Q

trong đó Q là tập hợp Q chỉ số được cắt tỉa.

A.1 GIẢI PHÁP TỔNG QUÁT

Theo (Kurtic et al., 2022), chúng tôi ký hiệu các phần tử đã cắt tỉa như EK= [eq1eq2. . .]T∈
[0,1]|Q|×P và sử dụng thực tế rằng giải phương trình (6) thông qua sử dụng nhân tử Langrange cho giải pháp dạng đóng tổng quát cho chi phí L và cập nhật trọng số ∆θ:

L=1
2(EKθ∗)T
EKF−1ET
K−1EKθ∗(15)

∆θ∗=F−1ET
K
EKF−1ET
K−1EKθ∗(16)

A.2 LOẠI BỎ MỘT PHẦN TỬ DUY NHẤT

Optimal brain surgeon (OBS) Để loại bỏ một phần tử duy nhất với chỉ số q, chúng ta đơn giản đặt EK=eT
k:

L=1
2(EKθ∗)T
EKF−1ET
K−1EKθ
=1
2θT
k1
[F−1]kkθk
=1
2(θk)2
[F−1]kk,∆θ=−F−1ET
K
EKF−1ET
K−1EKθ
=−F−1ek
eT
kF−1ek−1eT
Kθ
=−θk
[F−1]kkF−1ek(17)

tương ứng chính xác với mất mát và cập nhật của optimal brain surgeon (Hassibi & Stork, 1992).

Optimal brain damage (OBD) Chúng ta cũng có thể xem xét rằng các phần tử độc lập và Fisher là đường chéo. Sau khi lưu ý rằng điều này ngụ ý rằng các phần tử đường chéo của Fisher nghịch đảo là nghịch đảo vô hướng của các phần tử trong Fisher [F−1]kk=1
[F]kk, các công thức đơn giản hóa thành:

L= [F]kk(θk)2, ∆θ=−θkek (18)

tương ứng chính xác với mất mát và cập nhật của optimal brain damage (LeCun et al., 1989).

VECTOR HÓA

Để mục đích thực hiện, có thể thuận tiện khi có ký hiệu vector hóa Lθ∈RRC hoặc LW∈RR×C để tính toán tất cả các mất mát dự kiến song song:

Cho OBD:
Cho OBS: Lθ=1
2θ∗⊙θ∗⊙diag(F)
Lθ=1
2θ∗⊙θ∗⊘diag(F−1),LW=1
2W∗⊙W∗⊙mat(diag(F))
LW=1
2W∗⊙W∗⊘mat(diag(F−1))(19)

A.3 LOẠI BỎ MỘT HÀNG HOẶC CỘT DUY NHẤT

Structured OBS Nếu chúng ta xem xét xấp xỉ F≈G⊗A với nghịch đảo đã biết (G⊗A)−1=
G−1⊗A−1, thì để loại bỏ một hàng tại chỉ số r∈[0, R], chúng ta phải tính đến tương quan trong

12

--- TRANG 13 ---
các phần tử của hàng đó. Đó là, chúng ta viết ma trận EK= (eT
r⊗I) chứa các vector hàng one-hot cho tất cả các phần tử trong hàng r. Thay vào giải pháp tổng quát phương trình (7), chúng ta tìm thấy:

L=1
2EKθT
EKF−1ET
K−1EKθ∗
=1
2((eT
r⊗I)θ∗)T
(eT
r⊗I)(G⊗A)−1(eT
r⊗I)T−1(eT
r⊗I)θ∗
=1
2θT
r
eT
rG−1er⊗IA−1I−1θr
=1
2θT(eT
r⊗I)
[G−1]rr
⊗A−1−1(er⊗I)θr
=1
2θT
rAθr
[G−1]rr(20)

trong đó chúng ta viết θr=eT
rW∗∈RC cho vector hàng thứ r trong W. Tương tự, chúng ta thu được cập nhật trọng số liên quan:

∆θ=−F−1ET
K
EKF−1ET
K−1EKθ∗
=−(G⊗A)−1(eT
r⊗I)T
(eT
r⊗I) (G⊗A)−1(eT
r⊗I)T−1
(eT
r⊗I)θ∗
=−
G−1⊗A−1
(er⊗I)
eT
rG−1er⊗A−1−1θr
=−1
[G−1]rr
G−1er⊗A−1IA−1I
θr
=−G−1er⊗θr
[G−1]rr(21)

đạt được một cập nhật cắt tỉa có cấu trúc tương tự như được rút ra trong (Wang et al., 2019) cho các bộ lọc tích chập. Chúng ta có thể rút ra tương đương mất mát dự kiến và cập nhật cho cột, bằng cách xem xét EK=(I⊗eT
c). Nếu chúng ta làm như vậy, chúng ta tìm thấy các cập nhật có cấu trúc cho một hàng r hoặc cột c:

Loại bỏ hàng r:
Loại bỏ cột c: L=1
2θT
rAθr
[G−1]rr
L=1
2θT
cGθc
[A−1]cc∆θ=−G−1er⊗θr
[G−1]rr
∆θ=−θc⊗A−1ec
[A−1]cc(22)

Structured OBD Chúng ta cũng có thể giả định rằng, khi loại bỏ một hàng r, các phần tử riêng lẻ trong hàng cũng độc lập điều này sẽ ngụ ý [A]ii=1
[A−1]ii. Tương tự, [G]ii=1
[G−1]ii khi loại bỏ một cột c. Do đó, chúng ta có thể đơn giản hóa thành:

Loại bỏ hàng r:
Loại bỏ cột c: L=1
2GrrθT
rAθr
L=1
2AccθT
cGθc∆θ=−erθT
r
∆θ=−θceT
c(23)

dạng tương tự với các mất mát và cập nhật structured OBD như được rút ra trong (Wang et al., 2019) cho các bộ lọc tích chập. Các rút gọn hơi khác ở chỗ chúng ta bắt đầu từ giải pháp tổng quát phương trình (8), tránh được nhu cầu rút ra lại nhân tử Langrange cho mỗi cấu trúc có thể.

A.4 CẮT TỈA NHIỀU HÀNG VÀ CỘT (CÓ TƯƠNG QUAN)

Hãy xem xét việc loại bỏ R′ hàng r1, r2, . . . r′
R hàng hoặc C′ cột với chỉ số c1, c2, . . . , cC′, với 1<R′< R và 1<C′<C. Chúng tôi ký hiệu các ma trận chứa các vector one-hot chọn tất cả hàng và cột được loại bỏ tương ứng như:

ER′= [e1e2. . .eR′]T∈RR′×R EC′= [e1e2. . .eC′]T∈RC′×C(24)

Sau đó, ma trận EK chứa các vector hàng one-hot chọn tất cả các phần tử được loại bỏ có thể được viết như:

Nhiều hàng:
Nhiều cột: EK= (ER′⊗IC)∈RQ×RC, (với Q=R′C)
EK= (IR⊗EC′)∈RQ×RC, (với Q=RC′)(25)

13

--- TRANG 14 ---
Để đồng thời loại bỏ hàng và cột, chúng ta có thể xếp chồng các ma trận với các vector hàng trùng lặp đã được loại bỏ:

Nhiều hàng và cột: EK
ER′⊗IC
IR⊗EC′
∈RQ×RC với các hàng trùng lặp đã được loại bỏ (26)

Việc loại bỏ các hàng trùng lặp là cần thiết do một số phần tử R′C′ chồng lấp giữa hàng và cột, sau đó tổng số hàng do đó trở thành Q=R′C+C′R−R′C′. Chúng tôi sử dụng các ma trận đơn vị có kích thước thích hợp IR∈RR×R và IC∈RC×C. Để ngắn gọn, chúng tôi viết vector hoặc ma trận của các trọng số đã cắt tỉa θ:=EKθ∈RQ.

Đầu tiên, chúng tôi rút ra việc loại bỏ cho R′ hàng bằng cách định nghĩa ma trận loại bỏ như EK=ER′⊗I và định nghĩa W:=ER′W∈RR′×C. Cập nhật trọng số hoàn chỉnh cho việc loại bỏ nhiều hàng trở thành:

∆θ=−F−1ET
K
EKF−1ET
K−1EKθ∗
=−(G⊗A)−1(ER′⊗I)T
(ER′⊗I)(G⊗A)−1(ER′⊗I)T−1(ER′⊗I)θ∗
=−(G−1ET
R′⊗A−1)
ER′G−1ET
R′⊗A−1−1θ∗
=−(G−1ET
R′⊗A−1)
(ER′G−1ET
R′)−1⊗A
θ∗
∆W=−G−1ET
R′
(ER′G−1ET
R′)−1WA
A−1
=−G−1ET
R′(ER′G−1ET
R′)−1W (27)

Tương tự, chúng tôi rút ra việc loại bỏ C′ cột bằng cách định nghĩa ma trận loại bỏ như EK=I⊗EC′ và định nghĩa W:=EC′W∈RR×C′. Cập nhật trọng số hoàn chỉnh cho việc loại bỏ nhiều cột trở thành:

∆θ=−F−1ET
K
EKF−1ET
K−1EKθ∗
=−(G⊗A)−1(I⊗EC′))T
(I⊗EC′)(G⊗A)−1(I⊗EC′)T−1(I⊗EC′)θ∗
=−(G⊗A)−1(I⊗EC′))T
(I⊗EC′)(G⊗A)−1(I⊗EC′)T−1(I⊗EC′)θ∗
=−(G−1⊗A−1ET
C′)
G⊗EC′A−1ET
C′−1θ
∆W=−G−1GW(EC′A−1ET
C′)−1(A−1ET
C′)
=−W(EC′A−1ET
C′)−1(A−1ET
C′) (28)

14

--- TRANG 15 ---
B CHI TIẾT THỰC NGHIỆM.

Mã nguồn có sẵn tại: https://github.com/Qualcomm-AI-research/llm-surgeon.

B.1 CÁC MÔ HÌNH

Các mô hình OPT Từ họ mô hình OPT ((Zhang et al., 2022)), chúng tôi xem xét các mô hình với số lượng tham số sau: 125 triệu (125m), 1.3 tỷ (1.3b), 2.7 tỷ (2.7b), 6.7 tỷ (6.7b) mô hình. Chúng tôi bỏ qua mô hình 350 triệu do layer norm khác biệt. Chúng tôi thu được các checkpoint đã được huấn luyện sẵn tiêu chuẩn sử dụng Huggingface (Wolf et al., 2019) và sử dụng điều này làm baseline và khởi tạo cho nén.

Các mô hình Llama-v2 Từ họ mô hình Llama-v2 ((Touvron et al., 2023)), chúng tôi xem xét một mô hình với 7 tỷ (7b) tham số và một mô hình với 13 tỷ (13b) tham số. Chúng tôi thu được các checkpoint đã được huấn luyện sẵn tiêu chuẩn sử dụng Huggingface (Wolf et al., 2019) và sử dụng điều này làm baseline và khởi tạo cho nén.

B.2 TẬP DỮ LIỆU

Tiếng Anh / Wikitext-2 Phần lớn kết quả được thu được trên tập dữ liệu Wikitext-2 chứa các tập con đã được phân tích của Wikipedia tiếng Anh (Merity et al., 2016; Wikipedia, 2004), sử dụng các tập huấn luyện và kiểm tra mặc định. Để khớp, chúng tôi sử dụng 128 batch của 2048 ký tự và để kiểm tra chúng tôi sử dụng tập kiểm tra tiêu chuẩn chứa 4358 ký tự.

Tiếng Pháp / Wikipedia Đối với các thí nghiệm dữ liệu tiếng Pháp, chúng tôi sử dụng một tập con của Wikipedia tiếng Pháp (Wikipedia, 2004). Để khớp, chúng tôi sử dụng 128 batch của 2048 ký tự và để kiểm tra chúng tôi sử dụng một tập kiểm tra được chọn ngẫu nhiên chứa 1067888 ký tự.

Tiếng Đức / Wikipedia Đối với các thí nghiệm dữ liệu tiếng Ý, chúng tôi sử dụng một tập con của Wikipedia tiếng Đức (Wikipedia, 2004). Để khớp, chúng tôi sử dụng 128 batch của 2048 ký tự và để kiểm tra chúng tôi sử dụng một tập kiểm tra được chọn ngẫu nhiên chứa 1112372 ký tự.

Tiếng Ý / Wikipedia Đối với các thí nghiệm dữ liệu tiếng Ý, chúng tôi sử dụng một tập con của Wikipedia tiếng Ý (Wikipedia, 2004). Để khớp, chúng tôi sử dụng 128 batch của 2048 ký tự và để kiểm tra chúng tôi sử dụng một tập kiểm tra được chọn ngẫu nhiên chứa 633177 ký tự.

B.3 TƯƠNG ĐƯƠNG MẶT NẠ

Khi so sánh tương đương của các mặt nạ cắt tỉa thu được giữa hai mô hình θA và θB thu được bởi hai phương pháp nén A và B. Chúng tôi luôn xem xét trường hợp cắt tỉa 50%, và định nghĩa tương đương mặt nạ như phần trăm của cùng các trọng số được đặt bằng không trong cả hai mô hình:

tương đương mặt nạ =PX
i=11([θA]i= 0 và [θB]i= 0)
P. (29)

trong đó 1 ký hiệu hàm chỉ thị trả về 1 nếu cả hai trọng số [θA]i và [θB]i đều bằng không, và trả về 0 nếu ngược lại.

B.4 SPARSE GPT VÀ ĐÁNH GIÁ CÁC BASELINE

Đối với baseline SparseGPT, chúng tôi đã sử dụng kho mã chính thức SparseGPT (Frantar & Alistarh, 2023) cho phép huấn luyện và đánh giá trên wikitext-2. Các kết quả thu được có thể khác với những kết quả được báo cáo trong bài báo gốc vì tập dữ liệu C4 đã được sử dụng ở đó.

Trong công trình này, các mô hình được huấn luyện với cùng 128 batch của tập huấn luyện wikitext-2 như có sẵn trong codebase SparseGPT và được đánh giá trên tập kiểm tra wikitext-2 sử dụng cùng quy trình đánh giá chính xác.

15

--- TRANG 16 ---
C CHI TIẾT KỸ THUẬT

C.1 MÃ GIẢ

Thuật toán 2 LLM Surgeon (có cấu trúc)
Đầu vào: kích thước mục tiêu α
Đầu vào: trọng số ban đầu θ0
Cho shot t trong [1, 2, . . . , T]
Tính toán: độ cong xấp xỉ G1,A1 từ dữ liệu (tùy chọn cũng G2,A2)▷ mục 3.1
Tính toán: chi phí mỗi hàng/cột Lr,Lc từ G1,A1,(G2,A2) ▷ mục 3.2
Tính toán: ngưỡng τ sử dụng Lr và Lc cho kích thước mục tiêu α ▷ mục 3.3
Chọn: các hàng và cột để loại bỏ ER,EC dựa trên τ ▷ mục 3.3
Tính toán: cập nhật trọng số ∆θt−1 dựa trên ER,EC và G1,A1,(G2,A2) ▷ mục 3.4
Cập nhật: các trọng số còn lại θt←θt−1+ ∆θt−1▷ mục 3.5
Tùy chọn: θt←cập nhật hạng thấp (θt)
Đầu ra: trọng số nén ˆθ=θT

Thuật toán 3 LLM Surgeon (bán cấu trúc / không có cấu trúc)
Đầu vào: kích thước mục tiêu α
Đầu vào: trọng số ban đầu θ0
Cho shot t trong [1, 2, . . . , T]
Tính toán: độ cong xấp xỉ G1,A1 từ dữ liệu (tùy chọn cũng G2,A2)▷ mục 3.1
Tính toán: chi phí mỗi phần tử Lk từ G1,A1,(G2,A2) ▷ mục 3.2
Tính toán: ngưỡng τ từ Lk và kích thước mục tiêu αt (không có cấu trúc/bán cấu trúc) ▷ mục 3.3
Chọn: các phần tử để loại bỏ EK dựa trên τ (không có cấu trúc/bán cấu trúc) ▷ mục 3.3
Tính toán: cập nhật trọng số ∆θt−1 dựa trên EK và G1,A1,(G2,A2) ▷ mục 3.4
Cập nhật: các trọng số còn lại θt←θt−1+ ∆θt−1▷ mục 3.5
Tùy chọn: θt←cập nhật hạng thấp (θt)
Đầu ra: trọng số nén ˆθ=θT

C.2 GIẢM CHẤN

Trong thực tế, chúng tôi giảm chấn các ma trận G và A bằng cách thêm một số hạng đường chéo G+λGI và A+λAI. Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng các giá trị trong khoảng [0.01, 0.1] nhân với các số hạng đường chéo trung bình thường hoạt động tốt. Chúng tôi tuân theo (Frantar & Alistarh, 2023) và luôn sử dụng λA=0.01diag(A) để nhất quán với công trình trước đó và cho phép so sánh công bằng với các baseline. Hơn nữa, chúng tôi sử dụng λG=0.1diag(G) cho các thí nghiệm có cấu trúc và λG=0.01diag(G) trong các thí nghiệm bán cấu trúc và không có cấu trúc.

16

--- TRANG 17 ---
D HIỆU SUẤT TÁC VỤ DOWNSTREAM

Chúng tôi cũng đánh giá phương pháp của mình trên các tác vụ downstream vì các số liệu perplexity không nhất thiết tương quan với hiệu suất downstream. Hơn nữa, chúng tôi cũng lặp lại thí nghiệm này sử dụng tập dữ liệu C4 như dữ liệu tham khảo cho nén, vì điều này được sử dụng trong công trình trước đó (Frantar & Alistarh, 2023) và vì điều này có thể được coi là một tập dữ liệu tham khảo tổng quát hơn. Trong bảng 5 và 6, chúng tôi báo cáo hiệu suất kiểm tra 0-shot của cắt tỉa có cấu trúc cho LLM surgeon và baseline K-OBD.

Bảng 5: Hiệu suất tác vụ downstream sử dụng Wikitext-2 để cắt tỉa.

Cắt tỉa có cấu trúc
(với wikitext-2) Kích thước mô hình wikitext word ppl boolq piqa hallaswag winogrande arc easy arc challenge openbookq copa lambada openai wsc273 TRUNG BÌNH wikitext2
Baseline dày đặc 100% 9.24 77.74 79.11 75.99 69.14 74.58 46.25 44.20 86.00 73.92 85.71 71.26
LLM Surgeon (của chúng tôi) 90% 9.63 76.21 78.56 75.39 67.64 74.12 46.50 43.60 85.00 72.64 84.98 70.46
80% 12.16 72.97 77.09 71.30 66.30 71.36 41.89 41.80 87.00 56.43 80.22 66.66
70% 16.91 61.25 73.56 60.72 61.09 63.09 36.69 38.80 81.00 28.33 76.56 58.11
60% 25.15 44.98 69.26 48.04 54.38 52.31 30.29 36.80 78.00 11.72 68.50 49.43
50% 43.68 39.60 64.36 40.29 52.57 44.91 26.28 30.80 74.00 6.52 61.54 44.09
K-OBD 90% 9.89 76.67 78.02 74.80 68.11 75.17 46.33 44.60 86.00 72.71 82.78 70.52
80% 17.62 74.34 75.24 67.85 64.64 63.80 40.27 41.60 83.00 30.23 82.42 62.34
70% 32.72 65.29 71.82 53.07 56.83 51.05 33.11 37.80 79.00 12.21 70.70 53.09
60% 68.63 60.80 65.67 43.99 53.20 41.79 28.50 34.00 75.00 7.04 60.44 47.04
50% 136.33 61.56 60.66 36.84 53.04 36.11 26.71 33.00 72.00 4.70 61.17 44.58

Bảng 6: Hiệu suất tác vụ downstream sử dụng C4 để cắt tỉa.

Cắt tỉa có cấu trúc
(với C4) Kích thước mô hình wikitext word ppl boolq piqa hallaswag winogrande arc easy arc challenge openbookq copa lambada openai wsc273 TRUNG BÌNH wikitext2
Baseline dày đặc 100% 9.24 77.74 79.11 75.99 69.14 74.58 46.25 44.20 86.00 73.92 85.71 71.26
LLM Surgeon (của chúng tôi) 90% 9.90 77.03 78.45 74.95 68.27 73.19 45.99 44.60 84.00 72.81 82.78 70.21
80% 14.42 75.60 76.82 69.71 63.85 70.29 41.30 42.80 87.00 45.53 82.42 65.53
70% 25.16 66.39 72.85 58.11 56.83 62.16 34.47 38.40 80.00 22.69 69.96 56.19
60% 45.35 62.48 68.93 48.10 55.64 51.56 27.99 35.20 70.00 12.56 61.54 49.40
50% 77.30 62.60 65.02 41.70 54.22 42.55 24.23 31.20 71.00 7.26 60.44 46.02
K-OBD 90% 10.59 75.47 78.18 73.61 66.46 72.52 44.37 43.60 87.00 71.22 82.42 69.48
80% 20.12 73.36 75.14 66.11 62.43 62.84 38.23 41.00 86.00 21.50 78.39 60.50
70% 56.92 63.30 68.44 52.31 55.64 46.72 31.31 34.60 77.00 5.69 68.86 50.39
60% 112.85 62.23 64.47 46.36 52.17 40.53 29.52 32.40 72.00 2.91 63.00 46.56
50% 272.16 62.42 61.70 38.47 50.43 33.29 26.96 31.80 65.00 0.91 59.34 43.03

Chúng tôi thấy rằng phương pháp của chúng tôi không chỉ hoạt động tốt về test perplexity mà còn tương quan tốt với hiệu suất downstream, vượt trội hơn các baseline trên các tác vụ downstream này.

E CÁC THÍ NGHIỆM BỔ SUNG TRÊN LLAMA-V2 13B.

Để đánh giá hiệu suất trên các mô hình 13B tham số lớn hơn, chúng tôi cũng báo cáo nén có cấu trúc trên mô hình Llama-v2 13B và đánh giá hiệu suất tác vụ downstream. Test perplexities (thấp hơn là tốt hơn) có thể được tìm thấy trong bảng 7 dưới đây:

Bảng 7: Cắt tỉa mô hình Llama-v2 13B.

Baseline Kích thước mô hình đã cắt tỉa
Dày đặc 100% 90% 80% 70% 60% 50%
K-OBD 4.547 4.908 6.294 10.08 13.06 16.06
LLM Surgeon 4.547 4.692 5.286 6.207 7.245 9.428

cũng như kết quả đánh giá trên các benchmark downstream (cao hơn là tốt hơn) trong bảng 8 dưới đây.

Bảng 8: Hiệu suất tác vụ downstream sau khi cắt tỉa mô hình Llama-v2 13B lớn.

Llama-v2 13B Kích thước mô hình wikitext word ppl boolq piqa hallaswag winogrande arc easy arc challenge openbookq copa lambada openai wsc273 TRUNG BÌNH wikitext2
Baseline dày đặc 100% 8.23 80.52% 80.52% 79.38% 72.14% 77.53% 49.23% 45.20% 90.00% 76.77% 89.38% 74.07%
LLM Surgeon (của chúng tôi) 90% 8.57 81.07% 79.87% 79.24% 72.38% 76.30% 49.91% 47.20% 92.00% 75.47% 89.38% 74.28%
80% 10.08 80.86% 79.00% 77.09% 70.56% 75.93% 46.76% 46.80% 90.00% 67.79% 86.45% 72.12%
70% 12.74 74.50% 76.50% 71.52% 68.67% 69.74% 40.27% 45.00% 91.00% 54.40% 83.52% 67.51%
60% 16.00 64.62% 73.01% 65.04% 65.75% 63.80% 37.12% 39.60% 90.00% 44.50% 81.32% 62.48%
50% 23.75 65.66% 68.77% 56.19% 63.22% 56.19% 31.83% 36.60% 85.00% 35.16% 77.29% 57.59%
K-OBD 90% 8.79 81.31% 79.76% 79.12% 72.22% 76.94% 47.95% 47.80% 91.00% 75.26% 88.64% 74.00%
80% 11.79 80.80% 79.16% 76.80% 70.56% 73.74% 46.93% 48.60% 88.00% 58.99% 87.55% 71.11%
70% 20.00 66.76% 74.43% 64.18% 64.96% 56.23% 36.01% 39.00% 88.00% 38.54% 79.49% 60.76%
60% 27.74 55.66% 70.24% 55.52% 60.46% 49.62% 32.68% 35.80% 80.00% 30.06% 73.63% 54.37%
50% 37.38 59.79% 66.54% 48.39% 57.46% 46.59% 30.72% 34.00% 77.00% 24.61% 69.96% 51.50%

Chúng tôi thấy rằng LLM Surgeon cũng vượt trội hơn các baseline trên các mô hình Llama-v2 13B hiện có. Chúng tôi nhấn mạnh rằng những kết quả này được thu được trên cắt tỉa có cấu trúc của các hàng và cột, được coi là cấu trúc cắt tỉa khó nhất và bị ràng buộc nhất. Tuy nhiên, chúng ta có thể nén Llama 13B 20% với sự sụt giảm ít hơn 2% trong hiệu suất tác vụ downstream. Nó cũng vượt trội đáng kể so với baseline cho tất cả các tỷ lệ nén, cả về test perplexity và hiệu suất tác vụ downstream.

17

--- TRANG 18 ---
F ABLATIONS

F.1 SHOTS

Bảng 9: Ablation của số lượng shot T cho LLM Surgeon có cấu trúc nén mô hình OPT-1.3b.

Kích thước mục tiêu Shots T wikitext-2 PPL Shots T wikitext-2 PPL Shots T wikitext-2 PPL
90% 6 14.70 8 14.70 10 14.72
80% 12 15.14 16 15.12 20 15.08
70% 18 16.21 24 16.24 30 16.23
60% 24 18.53 32 18.45 40 18.49
50% 30 23.32 40 22.95 50 22.68

F.2 NÉN THEO TÁC VỤ CỤ THỂ

Bảng 10: Hiệu suất giao tác vụ và tương đương mặt nạ của mô hình OPT-125m nén 50% sử dụng LLM Surgeon có cấu trúc trên các tập con ngôn ngữ.

tập dữ liệu đánh giá tương đương mặt nạ (%)
mục tiêu EN FR DE IT EN FR DE IT
Đã huấn luyện sẵn 27.66 22.54 24.32 27.66
EN 47.46 172.9 181.1 169.1 1.00 0.74 0.70 0.72
FR 113.4 28.44 35.02 34.90 0.74 1.00 0.87 0.90
DE 142.1 35.15 27.49 38.49 0.70 0.87 1.00 0.87
IT 123.7 31.85 33.78 30.58 0.72 0.90 0.87 1.00

LLM Surgeon sử dụng dữ liệu để tìm một mô hình nén có tác động tiêu cực ít nhất đến hiệu suất kiểm tra cuối cùng. Trong mục này, chúng tôi khám phá mức độ mà phương pháp có thể sử dụng dữ liệu để nén cụ thể cho tác vụ hiện tại. Chúng tôi làm điều này bằng cách so sánh hiệu suất kiểm tra và tương đương giữa các mặt nạ cắt tỉa kết quả cho các ngôn ngữ mô hình hóa ngôn ngữ khác nhau: Tiếng Anh (EN/wikitext-2), Tiếng Pháp (FR) và Tiếng Ý (IT) và Tiếng Đức (DE). Chúng tôi xem xét nén không có cấu trúc 50% sử dụng LLM Surgeon với các cập nhật trọng số có tương quan. Đối với mỗi mô hình nén, chúng tôi so sánh hiệu suất trên tất cả các ngôn ngữ và so sánh tương đương giữa các mặt nạ cắt tỉa kết quả (chi tiết trong phụ lục B.3), và báo cáo kết quả trong bảng 10. Giống như các phương pháp khác sử dụng dữ liệu để nén (Hassibi & Stork, 1992; Frantar & Alistarh, 2023; Wang et al., 2019), chúng tôi mong đợi thấy một số tương quan giữa dữ liệu được sử dụng để huấn luyện và dữ liệu có hiệu suất kiểm tra tốt, điều này được phản ánh trong cả hiệu suất kiểm tra và mặt nạ. Điều quan trọng cần lưu ý là hiệu suất cuối cùng sau khi nén sẽ phụ thuộc vào chất lượng của tập dữ liệu được sử dụng để nén. Hơn nữa, thí nghiệm chứng minh rằng phương pháp có thể được sử dụng cho nén theo tác vụ cụ thể được điều chỉnh theo dữ liệu được sử dụng để nén và tổng quát hóa đến hiệu suất kiểm tra cao trên dữ liệu kiểm tra liên quan.

18

--- TRANG 19 ---
G VỀ SO SÁNH CÔNG BẰNG

Tất cả kết quả trong công trình này (bao gồm cả SparseGPT) đều được huấn luyện trên Wikitext-2 để so sánh công bằng. Để làm điều này, chúng tôi đã sử dụng cùng dataloader và script đánh giá như repo SparseGPT chính thức và chạy lại tất cả kết quả SparseGPT để được huấn luyện trên Wikitext-2. Trong một số trường hợp, điều này dẫn đến điểm số tốt hơn cho baseline SparseGPT so với kết quả được huấn luyện trên C4 được báo cáo trong bài báo SparseGPT gốc. Tuy nhiên, chúng tôi thấy rằng phương pháp của chúng tôi sử dụng các ước tính độ cong được cải thiện vẫn vượt trội hơn các baseline về hiệu suất kiểm tra cuối cùng.

H HIỆU SUẤT TÍNH TOÁN

Chúng tôi báo cáo chi phí tính toán về thời gian cắt tỉa trong bảng 11 và bộ nhớ GPU trong bảng 12.

Bảng 11: Hiệu suất thời gian.

Hiệu suất kiểm tra
Thời gian chạy Mạng Thời gian PPL 90% PPL 80% PPL 70% PPL 60% PPL 50%
Baseline không có cấu trúc (SparseGPT) Llama-v2 7B <5m 5.49 5.58 5.71 5.94 6.51
LLM Surgeon không có cấu trúc (của chúng tôi) Llama-v2 7B 2d8h16m 5.13 5.20 5.36 5.66 6.08
Baseline có cấu trúc (K-OBD) Llama-v2 7B 16h58m 5.48 9.14 15.43 28.03 46.64
LLM Surgeon có cấu trúc (của chúng tôi) Llama-v2 7B 17h08m 5.25 6.18 7.83 10.39 15.38
Baseline có cấu trúc (K-OBD) Llama-v2 13B 1d6h5m 4.908 6.294 10.08 13.06 16.06
LLM Surgeon có cấu trúc (của chúng tôi) Llama-v2 13B 1d9h26m 4.692 5.286 6.207 7.245 9.428

Phương pháp của chúng tôi hiệu quả nhất cho cắt tỉa có cấu trúc, nhưng cần lưu ý rằng các nỗ lực kỹ thuật có thể cải thiện thêm tốc độ cho cắt tỉa không có cấu trúc. Trọng tâm của bài báo là cắt tỉa có cấu trúc, mà chúng tôi đạt được tỷ lệ nén hiện đại. Quan trọng là, việc nén LLM chỉ cần xảy ra một lần sau đó mô hình đã cắt tỉa có thể được triển khai vô số lần mà không có chi phí thêm. Điều này thúc đẩy phương pháp của chúng tôi mất nhiều thời gian hơn để chạy nhưng đạt hiệu suất kiểm tra cuối cùng tốt hơn.

Bảng 12: Hiệu suất bộ nhớ.

Mạng SparseGPT (baseline) LLM-Surgeon không có cấu trúc (của chúng tôi)
Llama-7B <5m / 1 GPU (32GB) 2d8h16m / 4xH100 80 GB
K-OBD (baseline) LLM-Surgeon có cấu trúc (của chúng tôi)
Llama-7B 16h58m / 4xH100 80 GB 17h08m / 4xH100 80 GB
Llama-13B 1d6h5m / 8xH100 80 GB 1d9h26m / 8xH100 80 GB

Chúng tôi lập luận rằng sự khác biệt trong hiệu suất và thời gian chạy của các phương pháp cắt tỉa phần lớn có thể được quy cho các giả định cơ bản về tương quan giữa các trọng số. Đáng chú ý, các thuật toán xem xét ít tương quan, đôi khi đến mức hoàn toàn bỏ qua tất cả thông tin gradient, có thể dẫn đến các thuật toán cắt tỉa rất nhanh cho cắt tỉa không có cấu trúc và bán cấu trúc nhưng thường không đủ linh hoạt để thực hiện cắt tỉa có cấu trúc của hàng và cột. Ví dụ về các thuật toán nhẹ như vậy cho LLM là (Sun et al., 2023) và SparseGPT (Frantar & Alistarh, 2023), như cũng có thể quan sát từ bảng 11. Phương pháp của chúng tôi đưa ra các giả định ít mạnh hơn về độ cong của mất mát và kết quả là vượt trội hơn tất cả các baseline trên tất cả cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc. Hơn nữa, độ cong được cải thiện cũng đủ điều kiện cho phân bổ động của việc loại bỏ trọng số và các cập nhật trọng số có tương quan được cải thiện. Trong thực tế, chúng tôi luôn khuyến nghị sử dụng phương pháp của chúng tôi cho cắt tỉa có cấu trúc. Đối với cắt tỉa không có cấu trúc và bán cấu trúc, chúng tôi lưu ý một sự đánh đổi quan trọng giữa độ chính xác kiểm tra cuối cùng mong muốn và ngân sách tính toán có sẵn. Ở đây, phương pháp được đề xuất của chúng tôi có thể đạt được hiệu suất mô hình cuối cùng cao nhất nhưng yêu cầu nhiều tài nguyên tính toán hơn và mất nhiều thời gian hơn để chạy. Cần lưu ý rằng cắt tỉa chỉ cần xảy ra một lần sau đó mô hình có thể được triển khai vô số lần thời gian này, điều này phụ thuộc vào tài nguyên tính toán có sẵn cũng có thể hợp pháp hóa việc dành thời gian cắt tỉa bổ sung ngay cả khi điều này cao hơn nhiều so với các thuật toán khác về mặt tương đối. Về mặt tuyệt đối, việc sử dụng nhiều GPU lớn là thực hành phổ biến trong lĩnh vực mô hình ngôn ngữ lớn và nhiều GPU hơn thường được sử dụng để huấn luyện và triển khai các mô hình ngôn ngữ lớn. Hơn nữa, xấp xỉ độ cong một cách ngây thơ có thể song song hóa dữ liệu trong trường hợp cần tăng tốc thêm hoặc các mô hình lớn hơn. Chúng tôi hy vọng điều này cung cấp bối cảnh và nhấn mạnh sự đánh đổi giữa hiệu suất và tính toán trong thực tế.

19

--- TRANG 20 ---
I MỞ RỘNG ƯỚC TÍNH ĐỘ CONG

Thay vì sử dụng một tích Kronecker duy nhất, chúng ta có thể xem xét cải thiện xấp xỉ thông qua tổng của nhiều nhân tử Kronecker:

F≈eF=G1⊗A1+G2⊗A2 (30)

Phụ lục cuối cùng này đề cập đến câu hỏi làm thế nào người ta có thể tính toán các xấp xỉ như vậy và cách sử dụng chúng trong khung cắt tỉa mạng nơ-ron.

I.1 TÍCH KRONECKER GẦN NHẤT HOẶC TỔNG CÁC TÍCH KRONECKER

Thay vì giả định tính độc lập của kích hoạt và đạo hàm như trong mục 3.1, theo KFAC cổ điển của (Martens & Grosse, 2015), chúng ta có thể muốn tìm xấp xỉ tích Kronecker gần nhất F≈eG⊗eA gần nhất với Fisher về norm Frobenius:

eGl,eAl= arg min
Gl,Al||Fl−Gl⊗Al||F (31)

Tìm tổng gần nhất của các nhân tử Kronecker có thể được diễn đạt lại như một bài toán eigenvalue cổ điển của việc tìm ma trận hạng-1 gần nhất. Golub & Van Loan (2013).

||F−˜G⊗˜A||F≡ ||R (F)−vec(eG)vec(eA)T||F (32)

Phương pháp power và deflation Sau khi xem xét việc reshape, chúng ta có thể sử dụng các lặp power để giải và tìm các nhân tử Kronecker gần nhất G1,A1=solve(F).

Tìm với các lặp power:
eG1,eA1=solve(F) = arg min
G,A||F−G⊗A||F Deflation:
eGr,eAr=solve(F−Xr−1
r′=1(eGr′⊗eAr′))

Mô tả mở rộng hơn về phương pháp power solve (·) có thể được tìm thấy trong thuật toán 4. Ở đầu thuật toán, chúng tôi khởi tạo các lặp power như vector với những số một 1= [1 1 . . . 1]. Sau mỗi shot, chúng ta có thể khởi tạo vector như ước tính cuối cùng được tìm thấy trong shot trước đó.

Thuật toán 4 Phương pháp power Kronecker. Tìm eG,eA tích Kronecker gần nhất ||F−eG⊗eA||F.
Đầu vào: Khởi tạo eg0=1,ea0=1 (hoặc sử dụng ước tính từ shot trước đó).
Đầu vào: Đặt lặp I (hoặc I=1 nếu sử dụng ước tính từ shot trước đó)
Đầu ra: eG,eA
cho lặp i trong [1, 2, . . . , I] thực hiện
Tính toán: egi=R(eF)eai−1
||R(eF)eai−1||2, với R(eF)eai−1=1
NPN
n=1aT
neAi−1anvec(gngT
n)
Tính toán: eai=R(eF)Tegi
||R(eF)Tegi||2, với R(eF)Tegi=1
NPN
n=1gT
neGignvec(anaT
n)
Tính toán: σi=||eai||2
kết thúc cho
Trả về: eG=√
σimat(eg),eA=√
σimat(ea).

Fisher thực
rmse: 0.13
rmse diag: 0.19 KFAC cổ điển (IAD)
rmse: 0.12
rmse diag: 0.15 KFAC gần nhất RK=1
rmse: 0.11
rmse diag: 0.15 KFAC gần nhất RK=2
rmse: 0.09
rmse diag: 0.14 KFAC gần nhất RK=3
rmse: 0.04
rmse diag: 0.14 KFAC gần nhất RK=9

Hình 5: Ví dụ minh họa các xấp xỉ nhân tử Kronecker gần nhất eF≈PRK
r=1Gi⊗Ai, so sánh với KFAC cổ điển với giả định IAD. RK lớn hơn tạo ra các xấp xỉ tốt hơn cho Fisher thực F cho RK lớn hơn, được đo bằng root mean squared error (rmse).

20

--- TRANG 21 ---
I.2 CÁC XẤP XỈ ĐỘ CONG MỞ RỘNG

Đối với KFAC cổ điển với IAD hoặc xấp xỉ Kronecker gần nhất RK=1 dạng eF=G⊗A, nghịch đảo đơn giản trở thành (G⊗A)−1=G−1⊗A−1. Thật không may, chúng ta không thể sử dụng đồng nhất thức nghịch đảo nổi tiếng này cho tổng các nhân tử Kronecker, đó là lý do tại sao chúng ta quay lại các eigendecomposition G=E1S1ET
1 và A=E2S2ET
2, cho phép chúng ta phân tích Fisher thành:

eF=K1S1KT
1⊗K2S2KT
2= (K1⊗K2)(I⊗I+S1⊗S2)(KT
1⊗KT
2) (33)

trong đó K1 và K2 cụ thể có thể được tìm thấy trong App. B của Martens & Grosse (2015), mà chúng tôi đã tuân theo chặt chẽ trong các rút gọn của chúng tôi. Bởi vì K1 và K2 là trực giao và S1 và S2 đường chéo, Fisher nghịch đảo trở thành:

eF−1= (K1⊗K2)(I⊗I+S1⊗S2)−1(KT
1⊗KT
2) (34)

Trong bối cảnh huấn luyện mạng nơ-ron, bài toán trở nên hơi khó hơn vì chúng ta muốn xây dựng ước tính eGi và eAi một cách tăng dần từ các mẫu riêng lẻ al,n,gl,n tạo nên F, mà không cần lưu trữ đồng thời nhiều hơn một hoặc batch kích hoạt đầu vào al,n hoặc gradient đầu ra gl,n trong bộ nhớ. Mặc dù bài toán phân tích thành phần chính tích Kronecker trực tuyến này phần lớn vẫn là một bài toán nghiên cứu mở, cách tiếp cận của chúng tôi tuân theo chặt chẽ công trình gần đây của (Koroko et al., 2022) sử dụng các xấp xỉ tương tự trong bối cảnh tối ưu hóa. Tổng của nhiều nhân tử Kronecker RK>1 sẽ tạo ra các xấp xỉ gần hơn, nhưng cũng tăng tuyến tính yêu cầu bộ nhớ với RK cao hơn và làm cho việc nghịch đảo F−1 khó khăn hơn đáng kể.

Công thức để tính toán chi phí và cập nhật trọng số. Đối với tổng các nhân tử Kronecker, chúng tôi thấy rằng giải pháp tối ưu hóa có ràng buộc cho chi phí ∆L phương trình (7) và cập nhật trọng số ∆θ phương trình (8) trở thành tích vô hướng và tích ma trận-vector sau:

Lk=1
2⟨θ∗,Uθ∗⟩= (θ∗)TU(θ∗)∈R (35)

∆θ=eF−1ET
Ku=K1
KT
1UK2⊘
11T+s1sT
2
KT
2∈RRC(36)

với trung tâm của tất cả là ma trận U= [EKF−1ET
K]−1 nắm bắt tương quan giữa các trọng số:

U=
EK
K1⊗K2
I⊗I+S1⊗S2−1
KT
1⊗KT
2
ET
K−1
(37)

trong đó (I⊗I+S1⊗S2) là đường chéo và nghịch đảo do đó có thể được tính toán theo phần tử. Nghịch đảo còn lại có kích thước K×K, cho K trọng số có tương quan.

Lưu ý về tổng các nhân tử Kronecker Về mặt thực nghiệm, chúng tôi không thấy lợi ích về hiệu suất khi sử dụng tổng xấp xỉ nhân tử Kronecker gần nhất hai, hoặc thấy nó quá chậm. Do đó, chúng tôi tập trung trong văn bản chính vào LLM Surgeon với xấp xỉ KFAC tích Kronecker đơn nhanh để xấp xỉ độ cong cảnh quan mất mát. Tuy nhiên, chúng tôi chọn bao gồm phụ lục này vì chúng tôi tin rằng có thể hữu ích trong các bối cảnh khác hoặc truyền cảm hứng cho công trình tương lai nhằm cải thiện thêm chất lượng của các xấp xỉ độ cong.

21

--- TRANG 22 ---
J MÃ NGUỒN

Mã nguồn có sẵn tại: https://github.com/Qualcomm-AI-research/llm-surgeon.

22
