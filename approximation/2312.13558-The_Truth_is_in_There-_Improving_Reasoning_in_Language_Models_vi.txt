# 2312.13558.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2312.13558.pdf
# Kích thước tệp: 4671668 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Sự Thật Nằm Ở Đó: Cải Thiện Khả Năng Lý Luận trong Các Mô Hình Ngôn Ngữ
với Giảm Hạng Có Chọn Lọc Theo Lớp
Pratyusha Sharma1Jordan T. Ash2,⋆Dipendra Misra2,⋆
MIT1Microsoft Research NYC2
(⋆hướng dẫn ngang bằng)
pratyusha@mit.edu, {ash.jordan, dimisra}@microsoft.com
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên Transformer đã trở thành một yếu tố cố định trong học máy hiện đại.
Tương ứng, các nguồn lực đáng kể được phân bổ cho nghiên cứu nhằm tiếp tục phát triển công nghệ này, thường dẫn đến các mô hình có kích thước tăng lên được huấn luyện trên lượng dữ liệu ngày càng nhiều.
Tuy nhiên, công trình này chứng minh kết quả đáng ngạc nhiên rằng thường có thể cải thiện đáng kể hiệu suất của LLM bằng cách loại bỏ có chọn lọc các thành phần bậc cao1 từ ma trận trọng số của chúng. Can thiệp đơn giản này, mà chúng tôi gọi là Giảm Hạng Có Chọn Lọc Theo Lớp (LASER), có thể được thực hiện trên một mô hình sau khi hoàn thành huấn luyện, và không yêu cầu thêm tham số hay dữ liệu. Chúng tôi trình bày các thí nghiệm mở rộng chứng minh tính tổng quát của phát hiện này trên các mô hình ngôn ngữ và bộ dữ liệu, và cung cấp phân tích chuyên sâu đưa ra những hiểu biết về cả khi nào LASER hiệu quả và cơ chế mà nó hoạt động2.

1 Giới thiệu
Kể từ khi phát hành ban đầu, các LLM dựa trên Transformer đã được chứng minh là cực kỳ thành thạo trên một loạt rộng các nhiệm vụ học máy quan trọng. Kiến trúc Transformer cơ bản của chúng đã trở thành hiện đại nhất cho việc mô hình hóa và lý luận về ngôn ngữ tự nhiên, và đã cho thấy triển vọng trong các lĩnh vực như thị giác máy tính [Dosovitskiy et al., 2020] và học tăng cường [Chen et al., 2021] cũng như vậy.

Các hiện thân đương đại của kiến trúc Transformer nổi tiếng lớn, thường yêu cầu nguồn lực tính toán to lớn cho cả huấn luyện và suy luận. Điều này là có chủ ý, vì các Transformer được huấn luyện với nhiều tham số hoặc dữ liệu hơn có thể chứng minh được khả năng hơn các tiền nhiệm mỏng hơn của chúng—thường là với biên độ đáng kể [Brown et al., 2020, Touvron et al., 2023]. Tuy nhiên, một phần nghiên cứu ngày càng phát triển cho thấy rằng các mô hình dựa trên Transformer, và mạng nơ-ron nói chung, không yêu cầu tất cả các tham số được khớp để giữ lại các giả thuyết đã học của chúng. Mặc dù có vẻ hữu ích khi được tham số hóa quá mức tại thời điểm huấn luyện [Hinton et al., 2015, Bengio et al., 2005], nhưng đã được biết rõ rằng những mô hình này có thể được cắt tỉa mạnh mẽ trước khi suy luận; mạng nơ-ron thường có thể có hơn 90% trọng số của chúng được loại bỏ mà không có bất kỳ suy giảm đáng kể nào trong hiệu suất [Frankle and Carbin, 2018]. Việc khám phá hiện tượng này đã thúc đẩy sự quan tâm xung quanh mối quan hệ giữa tổng quát hóa và tham số hóa quá mức [Zhang et al., 2017], và tạo ra nghiên cứu trong việc phát triển các chiến lược cắt tỉa phù hợp với suy luận mô hình hiệu quả [Molchanov et al., 2016].

Bài báo này trình bày một phát hiện đáng ngạc nhiên, rằng việc cắt tỉa cẩn thận được thực hiện tại các lớp cụ thể của mô hình Transformer có thể tạo ra những tăng cường đáng kể trong hiệu suất trên một số nhiệm vụ. Chúng tôi mô tả Giảm Hạng Có Chọn Lọc Theo Lớp (LASER), một can thiệp loại bỏ các thành phần bậc cao của ma trận trọng số đã học như được xác định bởi phân tích giá trị kỳ dị. Việc giảm này được thực hiện trong các ma trận trọng số và lớp cụ thể của mô hình Transformer. Phù hợp với nghiên cứu trước đây, chúng tôi thấy rằng nhiều ma trận như vậy có thể được giảm đáng kể,

1Các thành phần bậc cao là các vector kỳ dị với giá trị kỳ dị nhỏ hơn.
2Mã và trang web: https://pratyushasharma.github.io/laser/

1arXiv:2312.13558v1  [cs.LG]  21 Dec 2023

--- TRANG 2 ---
Lớp TuyếnLớp Tự Chú ÝPhép Cộng Theo ĐiểmKích Hoạt Phi TuyếnLớp L WWLRMLP Tự Chú ÝCapital
is Japan=LASER   Thay thế W trong các lớp cụ thể bằng xấp xỉ hạng thấp WLRcủa nó+++++++++++++++++++++++++
UΣV⊤
TokyoHình 1: Giảm Hạng Có Chọn Lọc Theo Lớp (LASER) thay thế một ma trận trọng số cụ thể W của mô hình Transformer bằng xấp xỉ hạng-k của nó, WLR, và quan sát sự thay đổi trong hành vi của mô hình. Chúng tôi thấy rằng xấp xỉ hạng này, đặc biệt đối với trọng số MLP trong các lớp sau của mô hình, thường mang lại những lợi ích đáng ngạc nhiên cho hiệu suất mô hình.

và suy giảm hiệu suất thường không được quan sát cho đến khi hơn 90% thành phần hoàn toàn bị loại bỏ. Tuy nhiên, không giống như những gì được tìm thấy trong nghiên cứu trước đây, chúng tôi thấy rằng những giảm này có thể tạo ra những cải thiện mạnh mẽ về độ chính xác, như được đo lường bởi các điểm chuẩn lý luận được nghiên cứu kỹ trong NLP. Thậm chí còn tốt hơn, khám phá này có vẻ không bị giới hạn ở ngôn ngữ tự nhiên, với những cải thiện hiệu suất cũng được tìm thấy trong học tăng cường.

Bài báo này phân tích mối quan hệ giữa dữ liệu huấn luyện của mô hình và các mẫu có lợi từ LASER. Chúng tôi thấy rằng những cải thiện trong hiệu suất của mô hình chủ yếu đến từ thông tin ít xuất hiện hơn trong bộ dữ liệu huấn luyện của mô hình, cho thấy rằng LASER mang lại một loại thủ tục khử nhiễu làm cho các sự thật được học yếu trở nên dễ tiếp cận. Chúng tôi riêng biệt quan sát rằng LASER mang lại tăng cường độ bền vững đối với paraphrase trên các câu hỏi đúng trước đó.

Ngoài ra, chúng tôi cố gắng lý luận về những gì đang được lưu trữ trong các thành phần bậc cao, sao cho việc loại bỏ chúng lại thúc đẩy hiệu suất. Đối với các câu hỏi được trả lời đúng chỉ sau LASER, trong trường hợp không có can thiệp, mô hình gốc chủ yếu phản hồi với các từ tần suất cao như "the," "of," v.v.—các generation không thậm chí cùng loại ngữ nghĩa với câu trả lời đúng. Tuy nhiên, sau một lượng giảm hạng nhất định, câu trả lời của mô hình chuyển thành đúng. Để hiểu điều này, chúng tôi xem xét những gì các thành phần còn lại mã hóa một mình; chúng tôi xấp xỉ ma trận trọng số chỉ sử dụng các vector kỳ dị bậc cao của nó. Chúng tôi thấy rằng những thành phần này mô tả hoặc một phản hồi khác của cùng loại ngữ nghĩa với câu trả lời đúng hoặc các từ tần suất cao chung chung. Có vẻ như, khi các thành phần bậc cao nhiễu được kết hợp với các thành phần bậc thấp, các phản hồi xung đột của chúng tạo ra một loại "câu trả lời trung bình," có khả năng không đúng.

Hình 1 mô tả kiến trúc Transformer và thủ tục được tuân theo bởi LASER. Ở đây, ma trận trọng số của một Mạng Đa Lớp (MLP) tại một lớp cụ thể được thay thế bằng xấp xỉ hạng thấp của nó.

2 Nghiên cứu liên quan
Theo hiểu biết của chúng tôi, bài báo này là đầu tiên xác định rằng việc giảm hạng được chọn lọc cẩn thận có thể thúc đẩy hiệu suất Transformer. Tuy nhiên, có một loạt rộng các nghiên cứu nghiên cứu các câu hỏi liên quan, bao gồm cách các sự thật được lưu trữ trong LLM và cách nén tốt nhất mạng nơ-ron.

Cách các sự thật được lưu trữ. Các nghiên cứu khảo sát biểu diễn mô hình về sự hiện diện của các thuộc tính được chọn của thực thể [Ettinger et al., 2016, Adi et al., 2016, Hupkes et al., 2018, Conneau et al., 2018] cho thấy rằng các mô hình lưu trữ

2

--- TRANG 3 ---
thông tin thực tế trên các lớp khác nhau và Lee et al. [2023] cho thấy rằng tính bền vững của mô hình đối với thay đổi phân phối có thể được cải thiện bằng cách tinh chỉnh các lớp được chọn. Tuy nhiên, có bằng chứng mâu thuẫn về cách thông tin này được tổ chức và sử dụng trong việc xây dựng câu trả lời trong các mô hình ngôn ngữ lớn. Một số lý thuyết đề xuất rằng thông tin về các thực thể khác nhau được lưu trữ cục bộ như bộ nhớ khóa-giá trị hai lớp trong các phần MLP của mô hình Transformer [Geva et al., 2021], sau đó được sao chép qua các lớp sau bởi các mô-đun tự chú ý [Elhage, 2021]. Meng et al. [2022] đề xuất một thủ tục để theo dõi và chỉnh sửa thông tin cục bộ, cụ thể theo thực thể để ánh xạ đến các đầu ra "không thể" riêng biệt, hỗ trợ lý thuyết tính cục bộ. Những lý thuyết này được hỗ trợ thêm bởi hiện tượng "thoát sớm," nơi biểu diễn tại một lớp trung gian có thể được sử dụng trực tiếp với đầu cuối của mô hình để tạo ra đầu ra đúng [Zhao et al., 2021]. Ngược lại, Hase et al. [2023] đã quan sát rằng thông tin về một số thực thể hoặc quan hệ thực thể tương tự có thể được sửa đổi bằng cách chỉnh sửa nhiều lớp khác nhau trong kiến trúc mô hình, và do đó, các sự thật được lưu trữ trên các lớp theo cách phân mảnh. Bài báo này không đưa ra tuyên bố cụ thể nào về tính cục bộ, mà thay vào đó chứng minh rằng các thành phần bậc cao của ma trận trọng số có thể gây ra nhiễu trong việc đưa ra quyết định, và việc chỉ xem xét các thành phần bậc thấp có thể làm cho các câu trả lời đúng trở nên dễ tiếp cận.

Nén mô hình. Các phương pháp cắt tỉa mạng nơ-ron đã tìm thấy rằng các mô hình có thể được cắt tỉa đáng kể (thường loại bỏ hơn 90% tham số) với rất ít sụt giảm độ chính xác, giảm đáng kể các yêu cầu lưu trữ của mô hình [LeCun et al., 1989, Hassibi and Stork, 1992, Han et al., 2015, Li et al., 2016, Frankle and Carbin, 2018]. Cũng đã có các phương pháp cắt tỉa những mô hình này theo cách có cấu trúc, để tạo điều kiện cải thiện thời gian suy luận [Molchanov et al., 2016]. Sự tồn tại của các mạng con thưa [Frankle and Carbin, 2018, Hoefler et al., 2021] đã được tìm thấy là đúng cho các mô hình convolutional, fully connected, và Transformer [Lv et al., 2023, Murty et al., 2022]. Trong khi Jin et al. [2022] thấy rằng tổng quát hóa mô hình có thể được cải thiện bằng cách cắt tỉa và sau đó tái khớp tham số, việc cải thiện tổng quát hóa chỉ được quan sát khi huấn luyện lại mô hình. Theo hiểu biết của chúng tôi, các kỹ thuật cắt tỉa mô hình luôn thực hiện giảm đơn phương trên tất cả tham số, mà không nhắm mục tiêu bất kỳ lớp cụ thể nào—dẫn đến hiệu suất dự đoán hoặc giữ nguyên hoặc giảm [Frankle and Carbin, 2018]. Tuy nhiên, trong công trình này, chúng tôi thấy rằng hiệu ứng giảm độ chính xác là không đồng nhất trên các loại lớp khác nhau, và tổng quát hóa của mô hình có thể được cải thiện chỉ bằng cắt tỉa có chọn lọc; không cần huấn luyện thêm. Đại khái, chúng tôi thấy rằng suy giảm hiệu suất có thể được tạo ra bằng cách giảm hạng các lớp sớm, trong khi những lợi ích hiệu suất đáng kể thường có sẵn bằng cách cắt tỉa các lớp sau.

Xấp xỉ hạng thấp của ma trận trọng số. Hầu hết các phương pháp cắt tỉa giảm tham số theo thứ tự độ lớn tuyệt đối của chúng [Frankle and Carbin, 2018]. Tuy nhiên, một phương pháp thay thế là giảm hạng của các ma trận trọng số thành phần của nó, giữ lại k thành phần hàng đầu được tìm thấy bởi SVD. Trong khi các ma trận của mô hình nơ-ron, bao gồm các mô hình Transformer, đã được tìm thấy là được xấp xỉ tốt bằng cách sử dụng phương pháp này, nơi các phiên bản giảm rõ rệt của mô hình có thể bảo toàn hành vi của nó, nghiên cứu đã cho thấy rằng hiệu suất cuối cùng giảm khi mức độ nghiêm trọng của can thiệp tăng [Lv et al., 2023, Hajimolahoseini et al., 2021, Yu et al., 2017]. Lưu ý rằng những giảm này thường được thực hiện đơn phương, loại bỏ cùng số lượng thành phần trong mọi ma trận trọng số trong mô hình. Ngược lại với những phát hiện này, chúng tôi cho thấy rằng một giảm hạng có mục tiêu, thậm chí chỉ ảnh hưởng đến một ma trận trọng số duy nhất, có thể mang lại lợi ích cho độ chính xác dự đoán của Transformer.

Chưng cất mô hình và huấn luyện hạng thấp. Ba and Caruana [2014] và Hinton et al. [2015] đã huấn luyện các mạng nhỏ hơn để bắt chước hành vi của các mạng lớn hơn, gợi ý rằng mạng nơ-ron có thể được tham số hóa quá mức đáng kể và có thể được thay thế bằng các lựa chọn thay thế mỏng hơn. Theo hiểu biết của chúng tôi, không có báo cáo nào về sự cải thiện trong dự đoán của mô hình như một hệ quả của thủ tục này đã được chỉ ra. [Yang et al., 2020] đã thực thi tính hạng thấp của ma trận trọng số với mục đích hiệu quả bộ nhớ, nhưng các mô hình kết quả không đạt được hiệu suất tương đương với các đối tác tham số hóa quá mức của chúng. Kết quả cho thấy rằng tham số hóa quá mức hữu ích cho việc xác định các tham số tổng quát hóa tốt bởi SGD [Bengio et al., 2005, Hinton et al., 2015, Zhang et al., 2017].

3

--- TRANG 4 ---
3 Kiến thức cơ bản
Ở đây chúng tôi xem xét ký hiệu cơ bản và mô tả các thành phần cốt lõi của nghiên cứu của chúng tôi.

Ký hiệu Toán học. Chúng tôi sử dụng R để biểu thị số thực, N để biểu thị số tự nhiên, các chữ cái nhỏ như v∈Rd để biểu thị một vector d chiều, và các chữ cái in hoa như W∈Rm×n để biểu thị một ma trận kích thước m×n. Chúng tôi sử dụng ∥v∥2 để biểu thị chuẩn Euclidean của vector v và ∥W∥2 để biểu thị chuẩn phổ của ma trận W. Chúng tôi sử dụng [N] để biểu thị tập hợp {1,2,···, N}. Chúng tôi sẽ sử dụng rank(W) để biểu thị hạng của ma trận W và σ↓i(W) để biểu thị giá trị kỳ dị lớn thứ i của nó.

Kiến trúc Transformer. Chúng tôi cung cấp một mô tả ngắn gọn về kiến trúc Transformer vanilla có liên quan đến phân tích của chúng tôi. Một kiến trúc Transformer có thể được coi như L lớp của các khối Transformer. Khối thứ l ánh xạ một chuỗi vector T-length (h(l−1)1,···, h(l−1)T) thành một chuỗi vector T-length khác (h(l)1,···, h(l)T), nơi tất cả vector đều có d chiều. Biến đổi này được thực hiện bằng hai bước tuần tự: một cơ chế tự chú ý để trộn thông tin qua các bước thời gian, và một mạng feed-forward để xử lý thông tin trong mỗi bước thời gian. Chúng tôi mô tả một phiên bản cơ bản của những biến đổi này cho lớp thứ l cố định và bỏ chỉ số trên (l−1) cho rõ ràng.3

Một cơ chế tự chú ý đơn đầu đầu tiên ánh xạ mỗi vector hi thành một vector truy vấn qi=Wqhi, một vector khóa ki=Wkhi và một vector giá trị vi=Wvhi nơi Wq, Wk, Wv∈Rd×d là các ma trận trọng số cụ thể theo lớp. Sau đó chúng ta tính xác suất chú ý p(j|i) =exp(q⊤ikj/√d)PT l=1exp(q⊤ikl/√d) cho mọi i, j∈[T]. Những điều này được sử dụng để tính vector chú ý zi=PTj=1p(j|i)vj. Một tự chú ý k-đầu tính một tập hợp k vector chú ý bằng cách sử dụng các biến đổi tuyến tính khác nhau cho khóa, truy vấn, và giá trị, và sau đó nối các vector chú ý này. Những biến đổi tuyến tính k riêng biệt này cho khóa, truy vấn, và giá trị đều có thể được hấp thụ vào các ma trận tương ứng của chúng Wq∈Rd×dk, Wk∈Rd×dk và Wv∈Rd×dk. Cuối cùng, cơ chế tự chú ý đưa ra ui=ziWo+hi sử dụng ma trận chiếu Wo∈Rdk×d.

Bước feed-forward áp dụng một nhận thức đa lớp 2-lớp (MLP) ψ:Rd→Rd cho mỗi vector ui∈Rd riêng biệt. MLP thường có hàm kích hoạt ReLU hoặc GELU [Hendrycks and Gimpel, 2016] và trong một số mô hình như Llama, bias của các lớp tuyến tính được đặt thành 0. Chúng tôi biểu thị các ma trận trọng số của lớp tuyến tính thứ nhất và thứ hai của MLP này bằng Uin và Uout tương ứng. Đầu ra của khối Transformer thứ l này sau đó được cho bởi h(l)i=ψ(ui) +ui.

Tóm lại, một kiến trúc Transformer có các ma trận trọng số sau W={Wq, Wk, Wv, Wo, Uin, Uout} cho mỗi lớp, ngoài ma trận nhúng để nhúng các token đầu vào, một ma trận trọng số chiếu được áp dụng sau lớp cuối trước khi lấy softmax, và tất cả ma trận trọng số liên quan đến chuẩn hóa lớp. Trong công trình của chúng tôi, chúng tôi sẽ tập trung chủ yếu vào các ma trận trong W và can thiệp bằng cách sửa đổi chúng.

Xấp xỉ Hạng-r và SVD. Cho một ma trận W∈Rm×n và r∈N, một bài toán xấp xỉ hạng-r yêu cầu tìm một ma trận ˆW tối thiểu hóa ∥W−ĉW∥2 và thỏa mãn rank(ĉW)≤r. Định lý Eckart–Young–Mirsky cung cấp một giải pháp tối ưu của bài toán này bằng cách sử dụng Phân tích Giá trị Kỳ dị (SVD) [Eckart and Young, 1936]. Chính thức, một SVD của ma trận W được cho bởi W=UΣV⊤ nơi U= [u1, u2,···, um]∈Rm×m và V= [v1, v2,···, vn]∈Rn×n và Σ∈Rm×n. Các vector cột của U và V tạo thành một cơ sở trực chuẩn của Rm và Rn tương ứng, và Σ là một ma trận chéo có các phần tử chéo được cho bởi các giá trị kỳ dị của W theo thứ tự giảm dần. Người ta cũng có thể biểu diễn SVD của W như W=Pmin{m,n}i=1σ↓i(W)uiv⊤i. Theo định lý Eckart–Young–Mirsky, ma trận ĉW=Pri=1σ↓i(W)uiv⊤i là một giải pháp tối ưu cho bài toán xấp xỉ hạng-r cho bất kỳ hạng mong muốn r≤min{m, n} nào.

Trong công trình này, chúng tôi sẽ sử dụng từ thành phần bậc cao để chỉ các mục trong SVD tương ứng với các thành phần có giá trị kỳ dị nhỏ hơn. Những thành phần này bị loại bỏ bởi LASER. Thuật ngữ thành phần bậc thấp được sử dụng để chỉ các vector kỳ dị tương ứng với giá trị kỳ dị lớn. Những thành phần này được giữ lại trong xấp xỉ hạng thấp của ma trận.

3Các mô hình Transformer khác nhau thường có những khác biệt nhỏ trong cách những biến đổi này được thực hiện. Mục tiêu của chúng tôi không phải là cung cấp một khảo sát đầy đủ về những chi tiết này mà là nắm bắt thuật ngữ thiết yếu cho kết quả của chúng tôi.

4

--- TRANG 5 ---
Loss Loss
Số lớp Số lớp Số lớp
% Giảm: 10 25 40 50 60 75 90 92.5 95 97.5 98 98.5 99 99.5 99.75Hình 2: Ảnh hưởng của giảm hạng trên các loại lớp khác nhau không đồng nhất. Ở đây chúng tôi cho thấy ảnh hưởng của giảm hạng cho GPT-J như được nghiên cứu trên bộ dữ liệu CounterFact. Đường gạch ngang là loss của mạng không được sửa đổi. Trong các lớp chú ý (ma trận khóa, truy vấn, giá trị, đầu ra), mặc dù rõ ràng các ma trận có thể được giảm hạng đáng kể mà không làm hại giả thuyết đã học, có rất ít tăng hiệu suất. Tuy nhiên, đối với các lớp mạng đa lớp (MLP), giảm hạng đi từ việc gây hại đồng nhất đến cải thiện hiệu suất của mô hình (khoảng lớp 20).

4 Giảm Hạng Có Chọn Lọc Theo Lớp (LASER)
Trong phần này, chúng tôi mô tả chính thức can thiệp LASER. Một can thiệp LASER một bước được định nghĩa bởi ba đại lượng (τ, ℓ, ρ), bao gồm một loại tham số τ, số lớp ℓ, và giảm hạng ρ. Những giá trị này cùng nhau mô tả ma trận nào sẽ được thay thế bằng xấp xỉ hạng thấp của chúng và mức độ nghiêm trọng của xấp xỉ sẽ là bao nhiêu. Loại tham số phân loại loại ma trận mà chúng ta sẽ can thiệp. Chúng tôi tập trung vào các ma trận trong W={Wq, Wk, Wv, Wo, Uin, Uout} bao gồm các ma trận trong MLP và các lớp chú ý. Số lớp mô tả lớp mà chúng ta can thiệp (lớp đầu tiên được lập chỉ mục từ 0). Ví dụ, Llama-2 có 32 lớp và do đó ℓ∈ {0,1,2,···31}. Cuối cùng, ρ∈[0,1) mô tả tỷ lệ hạng tối đa nào nên được bảo toàn khi thực hiện xấp xỉ hạng thấp của nó. Ví dụ, để τ=Uin∈Rd×d, thì hạng tối đa của ma trận này là d. Chúng ta thay thế nó bằng một xấp xỉ hạng ⌊ρ·d⌋.

Hình 1 cho thấy một ví dụ về LASER. Trong hình này, chúng ta có τ=Uin và ℓ=L chỉ ra rằng chúng ta cập nhật ma trận trọng số trong lớp đầu tiên của MLP trong khối Transformer của lớp thứ L. Tham số khác (không được hiển thị trong hình) điều khiển k trong xấp xỉ hạng-k.

LASER điều tiết luồng thông tin nhất định trong mạng, điều này đáng ngạc nhiên có thể tạo ra những lợi ích hiệu suất đáng kể. Những can thiệp này cũng có thể dễ dàng được kết hợp—chúng ta có thể áp dụng một tập hợp can thiệp {(τi, ℓi, ρi)}mi=1 theo bất kỳ thứ tự nào. Phương pháp LASER là đơn giản tìm kiếm trên các can thiệp loại này, và thực hiện sửa đổi mang lại lợi ích lớn nhất. Có nhiều cách khác mà người ta có thể kết hợp những can thiệp này, tuy nhiên, và chúng tôi hoãn điều này cho nghiên cứu tương lai.

5 Thí nghiệm
Phần này nghiên cứu hậu quả của LASER khắp các lớp khác nhau của kiến trúc Transformer. Chúng tôi đầu tiên thực hiện một phân tích động lực của bộ dữ liệu hỏi đáp CounterFact [Meng et al., 2022] kết hợp với một mô hình GPT-J được huấn luyện trước [Wang and Komatsuzaki, 2021], và điều tra hiệu suất của mô hình và sự biến thiên của nó khi chúng tôi tìm kiếm trên các can thiệp tiềm năng. Sau đó, chúng tôi xem xét ảnh hưởng của LASER trên các mô hình, bộ dữ liệu và modalit khác nhau.

GPT-J, CounterFact và PILE. Chúng tôi sử dụng mô hình GPT-J với 27 lớp và 6B tham số được huấn luyện trước trên bộ dữ liệu PILE. Phần đầu tiên của phân tích của chúng tôi tập trung vào GPT-J, phần lớn vì dữ liệu huấn luyện của nó có sẵn công khai. Chúng tôi đánh giá hành vi của mô hình trên bộ dữ liệu CounterFact, bao gồm các mẫu được tổ chức như các bộ ba (chủ thể, quan hệ, câu trả lời) và ba câu hỏi được paraphrase cho mỗi câu hỏi. Ví dụ, (Danielle Darrieux, ngôn ngữ mẹ đẻ, Tiếng Pháp).

5.1 Một Phân Tích Toàn Diện với GPT-J trên Bộ dữ liệu CounterFact
Hình 2 cho thấy kết quả của việc áp dụng các lượng giảm hạng khác nhau cho mỗi ma trận trong kiến trúc Transformer trên loss phân loại cho bộ dữ liệu này. Những biểu đồ này được nhóm, sao cho mỗi hình phụ tương ứng chỉ với loại ma trận trọng số được chỉ ra. Lưu ý rằng mỗi lớp Transformer bao gồm một MLP hai lớp nhỏ. Các ma trận đầu vào và đầu ra thành phần được hiển thị riêng biệt. Các màu khác nhau chỉ ra các phần trăm khác nhau của các thành phần bị loại bỏ.

Các biểu đồ chú ý trong hình này minh họa những gì đã được biết về những mô hình này: ma trận trọng số có thể được giảm mạnh mẽ mà không có nhiều suy giảm trong hiệu suất mô hình. Tuy nhiên, kết quả thú vị hơn là trong các lớp MLP. Ở đây, không chỉ các ma trận có thể được giảm hạng mà không làm suy giảm hiệu suất phân loại, mà những cải thiện hiệu suất lớn có thể bằng cách giảm các lớp sau của mô hình. Xu hướng này rõ rệt nhất trong ma trận đầu vào của MLP. Trong khi có lợi ích với LASER trong các lớp chú ý cũng vậy, những lợi ích thường nhỏ hơn. Trong phần tiếp theo, chúng tôi chứng minh tính hiệu quả của LASER trên một loạt rộng các bộ dữ liệu và mô hình Transformer. Bởi vì một tìm kiếm toàn diện có thể tốn kém tính toán, và những cải thiện nhất quán có vẻ tập trung vào việc giảm các lớp MLP, tất cả kết quả tiếp theo phần này xem xét một tìm kiếm giảm chỉ trên những lớp này trừ khi được nêu khác.

Cải thiện độ chính xác và tính bền vững đối với paraphrase. Bộ dữ liệu CounterFact được sử dụng để kiểm tra kiến thức thực tế của mô hình về dữ liệu từ Wikipedia. Vì GPT-J được huấn luyện trên PILE, có nội dung bao gồm Wikidata, các sự thật khác nhau trong CounterFact là một phần của dữ liệu huấn luyện của mô hình, mặc dù với số lượng khác nhau. Vì tất cả câu trả lời là một token duy nhất trong setting này, chúng tôi tính độ chính xác top-k dựa trên việc liệu câu trả lời đúng có trong top-k token được dự đoán hay không. Như được thấy trong Hình 2 và Bảng 1, chúng tôi thấy rằng độ chính xác top-1 của mô hình trên các sự thật trong CounterFact tăng từ 13.3% lên 24.1% khi việc giảm được thực hiện trên một lớp duy nhất. Quan trọng là lưu ý rằng những cải thiện này là kết quả chỉ của giảm hạng, và không liên quan đến bất kỳ huấn luyện hoặc tinh chỉnh thêm nào của mô hình GPT-J được huấn luyện trước. Hơn nữa, những cải thiện đến với giảm hạng là có hệ thống. Tập hợp các điểm dữ liệu mà mô hình đúng chỉ phát triển với lượng giảm tăng lên trái ngược với một chuyển động ngẫu nhiên của các điểm dữ liệu vào và ra khỏi tập hợp các mục đúng; nếu một mô hình đúng một câu trả lời với một lượng giảm hạng nhất định (x), mô hình tiếp tục đúng câu trả lời cho các giảm hạng lớn hơn (y nơi y > x). Chúng tôi đánh giá tính bền vững của mô hình đối với paraphrase bằng cách tính phần trăm điểm dữ liệu mà mô hình đúng tất cả paraphrase của một câu hỏi cho trước. Đối với các điểm dữ liệu mà mô hình đã đúng, tính bền vững của mô hình đối với paraphrase cũng cải thiện với LASER khoảng 24.8 điểm phần trăm.

Ảnh hưởng đến mô hình ngôn ngữ và độ trôi chảy. Trong khi tính thực tế của mô hình cải thiện, việc giảm có ảnh hưởng đến hiệu suất của mô hình trên các số liệu khác không? Để hiểu điều này, chúng tôi đánh giá perplexity của mô hình, tức là mục tiêu huấn luyện ban đầu của nó, trên dữ liệu huấn luyện của nó. Đối với các lớp tương ứng với ma trận đầu vào MLP, perplexity của mô hình tăng từ 4.8 lên 5.0, cho thấy rằng mục tiêu mô hình ngôn ngữ thực sự bị ảnh hưởng một chút. Đối với các lớp đầu ra MLP, perplexity của GPT-J trên PILE tăng từ 4.8 lên 4.9 với LASER. Có thể sửa chữa suy giảm nhỏ này bằng cách hiệu chỉnh nhiệt độ của mô hình.

6

--- TRANG 7 ---
(a) (b) (c)Độ chính xác
Câu trả lời được sửa / Ban đầu đúng0.0250.0500.0750.1000.1250.1500.175
<50 50-1000 1000<Với LASER
Mô hình gốc
N = Tần suất của           trong
 N 0.200Hình 3: Những điểm dữ liệu nào được lợi từ LASER? Chúng tôi phân tích tần suất xuất hiện của các sự thật "được sửa" trong dữ liệu huấn luyện. GPT-J là một bệ thử nghiệm lý tưởng cho phân tích như vậy vì dữ liệu huấn luyện của nó (DTrain), bộ dữ liệu PILE, có sẵn công khai. (a) Đối với GPT-J được đánh giá trên CounterFact (DQA) chúng tôi lấy tất cả các điểm dữ liệu trong DTrain có chứa đề cập đến cả thực thể quan tâm và câu trả lời tương ứng với mỗi mẫu trong DQA. (b) Một biểu đồ mô tả độ chính xác top-10 tích lũy của mô hình trên tất cả điểm dữ liệu xuất hiện trong dữ liệu huấn luyện ít hơn hoặc bằng tần suất được chỉ ra trên trục x. Ở đây chúng tôi cho thấy độ chính xác có và không có LASER. (c) Sự tăng cường lớn nhất trong hiệu suất xảy ra cho các mẫu tần suất thấp. Biểu đồ thanh này hiển thị lượng tăng cường mà LASER mang lại cho dữ liệu được phân nhóm theo tần suất mà các sự thật tương ứng xuất hiện trong DTrain. Những cải thiện tối đa về độ chính xác là từ các điểm dữ liệu có ít lần xuất hiện trong dữ liệu huấn luyện.

Hình 4: Kết hợp các thao tác LASER trên nhiều lớp làm tăng cường hiệu suất mô hình thêm nữa. Ở đây chúng tôi cho thấy độ chính xác cải thiện như thế nào khi sử dụng một chiến lược kết hợp đơn giản cho cả dữ liệu xác thực, được sử dụng để xác định mỗi (τ, ℓ, ρ), và dữ liệu kiểm tra.Kết hợp các giảm trên các lớp. Chúng tôi thấy rằng thậm chí những cải thiện hơn nữa trong hiệu suất của mô hình có thể được thực hiện bằng cách thực hiện các lượng giảm hạng khác nhau trên nhiều lớp. Điều này được thực hiện bằng cách tìm kiếm tham lam trên (τ, ℓ, ρ) bắt đầu từ ℓ lớn nhất và ρ nhỏ nhất. Để tăng tốc, ở đây chúng tôi thực hiện tìm kiếm này chỉ trên các lớp MLP, vì đây là nơi những cải thiện lớn nhất thường được tìm thấy. Phù hợp với các thí nghiệm khác, tìm kiếm được thực hiện trên một tập xác thực, và kết quả được báo cáo trên tập kiểm tra. Trên CounterFact, độ chính xác 0-1 của mô hình GPT-J cơ bản là 13.1%. Sau khi thực hiện LASER một bước tốt nhất, độ chính xác của mô hình cải thiện lên 24.0%. Thực hiện LASER trên các lớp khác nhau cải thiện độ chính xác top-10 lên 29.2%, một cải thiện tuyệt đối 5.2% về độ chính xác so với thực hiện LASER trên một lớp duy nhất. Kết quả của tìm kiếm tổ hợp trên các giá trị ℓ và ρ khác nhau có thể được thấy trong Hình 4.

5.1.1 Những sự thật nào trong bộ dữ liệu được khôi phục bằng giảm hạng?
Để hiểu hiện tượng này, chúng tôi xem xét các câu hỏi được trả lời đúng sau LASER và ảnh hưởng của việc thông tin liên quan đến câu hỏi xuất hiện trong dữ liệu huấn luyện bao nhiều lần. Đối với mỗi điểm dữ liệu trong CounterFact, chúng tôi lấy tất cả các ví dụ trong PILE có chứa đề cập đến cả thực thể và câu trả lời. Sau đó chúng tôi tính toán tần suất thông tin liên quan đến mỗi câu hỏi đánh giá xuất hiện trong huấn luyện

7

--- TRANG 8 ---
Paul Citroen là người bản ngữ của ______WTiếng HàLanTiếng HàLanthethetheWthetheTiếng PhápTiếng PhápTiếng PhápCâu trả lời đúngCâu trả lời saiCâu trả lời trung bìnhCâu trả lời trung bìnhTrung bình         ĐúngTrung bình          Sai
ĐầuCuốiMumbai Russian Buddhism Actor Soccer QuarterbackPakistan Portuguese Hindu Teacher Photographer Goalkeeper(c)
Saeed Akhtar Mirza ban đầu đến từ Ngôn ngữ gốc của Hussar Ballad là Kalabhra theo tôn giáo Nghề nghiệp của Emmanuelle Devos là Walter Zenga là một chuyên nghiệp Mike Holmgren chơi ở vị trí(a)
ĐầuCuốiSydney Warsaw Dutch Berlin Jerusalem Hebrew Of The The the The ToThành phố song sinh của Wellington là Kharkiv là thành phố song sinh của Ngôn ngữ bản địa của Isaac Massa là Trụ sở của Morr Music nằm ở Abba Eban được tuyển dụng tại Yizhar Harari nói (b)Hình 5: (a) [Trái] LASER xấp xỉ các ma trận đã học bằng các thành phần bậc thấp của chúng. Chúng tôi thấy rằng đối với các điểm dữ liệu mà dự đoán của mô hình cải thiện sau LASER, nếu chúng ta thay vào đó sử dụng toàn bộ ma trận (bao gồm các thành phần bậc cao), mô hình thường dự đoán chỉ các từ "chung chung". (a) [Phải] Để hiểu những gì các thành phần bậc cao này mã hóa, chúng ta xấp xỉ ma trận trọng số đã học với các thành phần bậc cao thay vào đó. Chúng tôi thấy rằng những thành phần bậc cao này đôi khi mã hóa loại ngữ nghĩa đúng của câu trả lời nhưng phản hồi không chính xác. (b) Về mặt phân tích, tính toán độ tương tự ngữ nghĩa (khoảng cách cosine giữa câu trả lời thật và câu trả lời được tạo bởi k% dưới của các vector kỳ dị) cho thấy rằng trung bình câu trả lời được tính toán bởi các thành phần bậc cao tương tự hơn với câu trả lời thật. (c) Cho thấy một số ví dụ từ bộ dữ liệu và các câu trả lời tương ứng được tính toán bởi phần trên và phần dưới của các thành phần.

dữ liệu. Chúng tôi thấy rằng các sự thật được khôi phục trên giảm hạng có khả năng cao nhất là hiện diện không thường xuyên trong dữ liệu (Hình 3). Ở đây, "Ban đầu đúng" mô tả các mẫu được phân loại đúng ngay cả khi không có bất kỳ can thiệp nào. "Câu trả lời được sửa" chỉ các câu hỏi mà mô hình đúng chỉ sau khi can thiệp với LASER.

5.1.2 Các thành phần bậc cao đang lưu trữ gì?
Chúng ta đã thấy ở trên cách giữ lại các thành phần bậc thấp cải thiện hiệu suất mô hình trên nhiệm vụ trả lời câu hỏi mở. Chúng tôi thấy rằng đối với nhiệm vụ trả lời câu hỏi, những cải thiện thường đến trên các câu hỏi có câu trả lời được hỗ trợ bởi dữ liệu ít xuất hiện hơn trong tập huấn luyện. Mặc dù rõ ràng rằng việc loại bỏ các thành phần bậc cao "khử nhiễu" mô hình và giúp khôi phục thông tin "ẩn", ít thường xuyên, không rõ ràng các thành phần bậc cao đại diện cho gì sao cho việc loại bỏ chúng lại cải thiện hiệu suất. Phần này nghiên cứu câu hỏi này bằng cách sử dụng bộ dữ liệu CounterFact và GPT-J.

Để hiểu các thành phần bậc cao đại diện cho gì, chúng tôi xấp xỉ ma trận trọng số cuối bằng cách sử dụng các thành phần bậc cao của nó (trái ngược với việc xấp xỉ nó bằng cách sử dụng các thành phần bậc thấp như được thực hiện bởi LASER) như được hiển thị trong Hình 5(a). Theo đó, chúng tôi phân tích cách hành vi của mô hình thay đổi trên các điểm dữ liệu mà GPT-J ban đầu đúng không chính xác nhưng được lật thành đúng khi thực hiện LASER.

Đầu tiên, chúng tôi lưu ý rằng khi mô hình gốc, không được sửa đổi không trả lời những câu hỏi này một cách chính xác, nó thường phản hồi với các từ phổ biến, như "a," "the," "of," và các token tần suất cao khác. Sau khi thực hiện LASER, nơi chúng ta chỉ giữ lại các thành phần top-k, câu trả lời của mô hình cho những câu hỏi này lật từ các từ chung chung thành thực thể đúng. Đối với cùng các điểm dữ liệu, khi chúng ta xấp xỉ mô hình bằng cách thay vào đó giữ lại các thành phần bậc cao, chúng tôi thấy rằng mô hình hoặc dự đoán các thực thể không chính xác cùng loại ngữ nghĩa với câu trả lời đúng hoặc các token tần suất cao như "a," "the," và "of," như được hiển thị trong Hình 5(c). Tuy nhiên, khi chúng ta có hệ thống bao gồm các thành phần bậc thấp, đầu ra của mô hình thay đổi thành dự đoán các token thường xuyên. Để điều tra sự suy giảm có hệ thống này, chúng tôi đo độ tương tự cosine trung bình

8

--- TRANG 9 ---
Bộ dữ liệu Tên Mô hình
Roberta GPT-J LLama2
LASER LASER LASER
CounterFactAcc17.319.3 13.124.0 35.637.6
Loss5.785.43 5.785.05 3.613.49
HotPotQAAcc6.16.719.619.5 16.517.2
Loss10.9910.53 3.403.39 3.152.97
FEVERAcc50.052.3 50.256.2 59.364.5
Loss2.51.761.241.27 1.020.91
Bios GenderAcc87.593.7 70.997.5 75.588.4
Loss0.871.133.864.20 3.482.93
Bios ProfessionAcc64.572.5 75.682.1 85.086.7
Loss4.916.444.644.91 4.194.05
TruthfulQAAcc56.2 56.2 54.955.6 50.556.2
Loss1.601.42 1.021.010.951.04
BigBench-Epistemic ReasoningAcc37.141.8 37.138.3 44.863.4
Loss9.396.80 0.740.62 0.780.73
BigBench-WikidataQAAcc28.030.7 51.865.9 59.562.0
Loss9.077.69 3.522.86 2.402.31
Bảng 1: Ảnh hưởng của can thiệp LASER trên tám bộ dữ liệu hiểu ngôn ngữ tự nhiên. Chúng tôi tìm can thiệp LASER tốt nhất cho mỗi mô hình và nhiệm vụ bằng cách sử dụng độ chính xác/0-1 trên tập xác thực và báo cáo hiệu suất của nó trên tập kiểm tra được giữ lại. Trong một số trường hợp, trong khi độ chính xác của mô hình cải thiện, loss của nó hơi xấu đi.

của câu trả lời "thật" đối với câu trả lời được dự đoán khi ma trận được xấp xỉ với các lượng khác nhau của các thành phần bậc cao, như được hiển thị trong Hình 5(b). Độ tương tự cosine trung bình giữa câu trả lời được dự đoán xấu đi, chứng minh hiệu ứng này.

Chúng tôi giả thuyết rằng những ma trận này thường mã hóa nhiều phản hồi xung đột, và khi tất cả thành phần được sử dụng chúng va chạm để tạo ra một token chung chung. Loại bỏ các thành phần bậc cao, mà theo truyền miệng có vẻ thường nắm bắt các phản hồi không chính xác của loại đúng, giải quyết xung đột nội bộ này và cho phép mô hình phản hồi chính xác.

5.2 Điều này có tính tổng quát như thế nào?
Chúng tôi đánh giá tính tổng quát của những phát hiện của chúng tôi trên 3 LLM khác nhau cho một số nhiệm vụ hiểu ngôn ngữ.

Nhiệm vụ Hiểu Ngôn ngữ Tự nhiên. Chúng tôi đánh giá hiệu suất mô hình trước và sau LASER trên bảy bộ dữ liệu, bao gồm CounterFact [Meng et al., 2022], HotPotQA [Yang et al., 2018], FEVER [Thorne et al., 2018], Bias in Bios [De-Arteaga et al., 2019] [Giới tính và Nghề nghiệp], TruthfulQA [Lin et al., 2021], BigBench-Epistemic Reasoning [Bowman et al., 2015] và BigBench-WikidataQA. Những bộ dữ liệu này đánh giá các khía cạnh khác nhau của các vấn đề hiểu ngôn ngữ. CounterFact, Fever, và dữ liệu Bigbench-Wiki kiểm tra kiến thức thế giới và tính thực tế của mô hình. Bias in Bios đánh giá bias mô hình bằng cách dự đoán giới tính và nghề nghiệp của một người dựa trên tiểu sử ngắn. Chúng tôi định nghĩa Bios Gender là vấn đề dự đoán giới tính trong Bias in Bios, và Bios Profession là vấn đề dự đoán nghề nghiệp. HotPotQA cung cấp một nhiệm vụ trả lời câu hỏi mở khó khăn hơn với câu trả lời dài chứa nhiều token. Bộ dữ liệu Epistemic Reasoning từ Big Bench Hard (BBH) kiểm tra logic và khả năng đọc hiểu của mô hình. Cuối cùng, TruthfulQA kiểm tra tính trung thực của LLM. Chúng tôi sử dụng 20% bộ dữ liệu làm tập xác thực và chọn các siêu tham số LASER tốt nhất (τ, ℓ, ρ) bằng cách sử dụng tập xác thực này. Chúng tôi báo cáo kết quả trên 80% còn lại của bộ dữ liệu với siêu tham số được chọn. Các mô hình được sử dụng cho nhiệm vụ trả lời câu hỏi bao gồm, Roberta [Liu et al., 2020], GPT-J (6B) [Wang and Komatsuzaki, 2021], và LLAMA2 (7B) [Touvron et al., 2023]. Chi tiết về bộ dữ liệu và cách chúng được sử dụng có thể được tìm thấy trong Phụ lục A.

9

--- TRANG 10 ---
Số liệu đánh giá. Đối với mỗi nhiệm vụ này, chúng tôi đánh giá hiệu suất của mô hình bằng cách sử dụng (i) độ chính xác generation. Chúng tôi tạo một chuỗi N token bằng cách sử dụng LLM và sau đó báo cáo 1 nếu văn bản câu trả lời nằm trong văn bản được tạo và 0 nếu không, (ii) độ chính xác phân loại. Nếu câu trả lời nằm trong một tập nhỏ các giá trị tiềm năng, như trong một vấn đề phân loại tiêu chuẩn, chúng tôi coi một phản hồi đúng nếu nó đặt nhiều khối lượng xác suất hơn trên câu trả lời đúng hơn trên bất kỳ ứng viên nào khác, và (iii) loss. Chúng tôi báo cáo log-loss trên dữ liệu được giữ lại. Đối với bộ dữ liệu với một tập nhỏ các nhãn có thể, chúng tôi báo cáo độ chính xác (acc) bằng cách sử dụng độ chính xác phân loại, trong khi đối với các bộ khác chúng tôi sử dụng độ chính xác generation.

Chúng tôi kiểm tra tính tổng quát của kết quả này bằng cách đánh giá một bộ sưu tập mô hình ngôn ngữ trên các benchmark khác nhau. Như được thấy trong Bảng 1, chúng tôi thấy rằng thậm chí việc giảm nghiêm trọng cũng không dẫn đến suy giảm độ chính xác của mô hình và có thể dẫn đến cải thiện hiệu suất của chúng. Lượng giảm cần thiết khác nhau từ mô hình này sang mô hình khác.

5.3 Các lĩnh vực không phải văn bản
Để hiểu liệu hiện tượng này có hiệu quả bên ngoài trả lời câu hỏi trong lĩnh vực văn bản hay không, chúng tôi đánh giá ảnh hưởng của giảm hạng trên một agent học tăng cường.

Tên Mô hình Acc. Return
Transformer 50.67 0.575
với LASER53 0.965
Bảng 2: Ảnh hưởng của LASER trên một agent Decision Transformer 6-lớp. Mô hình cơ bản được huấn luyện và đánh giá trong một lĩnh vực Sokoban 10×10 khó khăn.Học chính sách. Đối với học chính sách, chúng tôi đánh giá ảnh hưởng của LASER trên một mô hình decision Transformer được huấn luyện trên trò chơi Sokoban và được đánh giá trên cùng trò chơi. Đây là một vấn đề lập kế hoạch khó khăn nơi agent phải di chuyển và đẩy một số khối đến các lỗ. Nhiệm vụ được hoàn thành khi tất cả khối nằm trên đỉnh các lỗ. Đầu vào cho decision Transformer là trạng thái hình ảnh của môi trường tại một trạng thái cho trước, và đầu ra là hành động cấp thấp. Chúng tôi thấy rằng đối với một decision Transformer được huấn luyện trên Sokoban, các mô hình giải quyết thêm 3% nhiệm vụ với LASER (Bảng 2). Chi tiết của thí nghiệm có thể được tìm thấy trong Phụ lục B.

Mặc dù những cải thiện nhỏ hơn nhiều, chúng nhất quán mặc dù mức độ nghiêm trọng mà việc giảm được thực hiện. Điều này có thể bởi vì hiện tượng này hoặc cụ thể cho văn bản hoặc yêu cầu một mô hình Transformer đủ lớn.

6 Kết luận và Thảo luận
Bài báo này mô tả LASER, một hiện tượng nơi thực hiện xấp xỉ hạng thấp của các loại lớp cụ thể tại các lớp cụ thể của khối transformer có thể cải thiện hiệu suất của LLM trên nhiệm vụ trả lời câu hỏi. Chúng tôi thấy điều này đúng trên năm bộ dữ liệu khác nhau và ba mô hình ngôn ngữ khác nhau. Hơn nữa, việc giảm LASER kết quả là cực đoan. Các ma trận được giảm đôi khi đến 99% hạng ban đầu của chúng, thấp hơn nhiều so với hạng hiệu quả của chúng (C.1). Tuy nhiên, mặc dù việc giảm cực đoan, hiệu suất của mô hình trên các nhiệm vụ tiếp tục cải thiện. Chúng tôi cũng quan sát những lợi ích hiệu suất cho một decision Transformer trong một lĩnh vực embodied. Chúng tôi thấy rằng những cải thiện lớn nhất trong độ chính xác mô hình tương ứng với thông tin ít phổ biến hơn trong dữ liệu huấn luyện và LASER đồng thời làm cho mô hình bền vững hơn đối với paraphrase của các câu hỏi. Chúng tôi tiếp tục thấy rằng các thành phần bậc cao của một số ma trận này mã hóa hoặc các từ tần suất cao hoặc các câu trả lời thay thế của cùng loại ngữ nghĩa với câu trả lời đúng. Những thành phần bậc cao nhiễu này có thể áp đảo các thành phần bậc thấp ổn định và dẫn đến mô hình trả lời câu hỏi không chính xác. Trong những trường hợp này, thực hiện LASER hoạt động như một kỹ thuật khử nhiễu và giảm các xung đột nội bộ trong các phản hồi tiềm năng.

Mặc dù phân tích này, sự thành công của LASER yêu cầu nghiên cứu thêm. Học (i) tại sao các thành phần bậc cao trong ma trận trọng số tích lũy các câu trả lời nhiễu trong quá trình huấn luyện, (ii) ảnh hưởng của kiến trúc mô hình và các lựa chọn cấu trúc khác đối với sự xuất hiện của hiện tượng này và (iii) tại sao điều này cụ thể đúng cho các lớp sau trong MLP quan trọng không chỉ cho hiểu biết của chúng tôi về sự thành công của LASER, mà cho việc hiểu hành vi của các mô hình ngôn ngữ lớn nói chung.

10

--- TRANG 11 ---
Lời cảm ơn
Công trình này được thực hiện khi PS là thực tập sinh tại Microsoft Research, New York City, với DM và JTA. Các tác giả muốn cảm ơn Minyoung Huh, Shikhar Murty, Han Guo, Cyril Zhang, David Bau, Jacob Andreas, Antonio Torralba, và John Langford cho các cuộc thảo luận hữu ích và phản hồi bài báo.

Tài liệu tham khảo
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531, 2015. URL https://api.semanticscholar.org/CorpusID:7200347.

Yoshua Bengio, Nicolas Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. In Y. Weiss, B. Schölkopf, and J. Platt, editors, Advances in Neural Information Processing Systems, volume 18. MIT Press, 2005. URL https://proceedings.neurips.cc/paper_files/paper/2005/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Paper.pdf.

Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv: Learning, 2018. URL https://api.semanticscholar.org/CorpusID:53388625.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.

Allyson Ettinger, Ahmed Elgohary, and Philip Resnik. Probing for semantic evidence of composition by means of simple classification tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 134–139, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2524. URL https://aclanthology.org/W16-2524.

Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. ICLR, abs/1608.04207, 2016.

Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure. J. Artif. Intell. Res., 61(1):907–926, January 2018.

Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),

11

--- TRANG 12 ---
pages 2126–2136, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1198. URL https://aclanthology.org/P18-1198.

Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=APuPRxjHvZ.

Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer Feed-Forward layers are Key-Value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484–5495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

N. Elhage. A mathematical framework for transformer circuits. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. https://transformer-circuits.pub/2021/framework/index.html, 2021.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36, 2022.

Sumu Zhao, Damian Pascual, Gino Brunner, and Roger Wattenhofer. Of Non-Linearity and Commutativity in BERT. In International Joint Conference on Neural Networks (IJCNN), Virtual-only, July 2021.

Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. ArXiv, abs/2301.04213, 2023. URL https://api.semanticscholar.org/CorpusID:255595518.

Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky, editor, Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989. URL https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf.

Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. In S. Hanson, J. Cowan, and C. Giles, editors, Advances in Neural Information Processing Systems, volume 5. Morgan-Kaufmann, 1992. URL https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf.

Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural network. In NIPS, 2015. URL https://api.semanticscholar.org/CorpusID:2238772.

Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. ArXiv, abs/1608.08710, 2016. URL https://api.semanticscholar.org/CorpusID:14089312.

Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res., 22(1): 10882–11005, January 2021.

Xiuqing Lv, Peng Zhang, Sunzhu Li, Guobing Gan, and Yueheng Sun. LightFormer: Light-weight transformer using SVD-based weight transfer and parameter sharing. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 2023. Association for Computational Linguistics.

Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D. Manning. Characterizing intrinsic compositionality in transformers with tree projections, 2022.

Tian Jin, Michael Carbin, Daniel M. Roy, Jonathan Frankle, and Gintare Karolina Dziugaite. Pruning's effect on generalization through the lens of training and regularization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=OrcLKV9sKWp.

12

--- TRANG 13 ---
Habib Hajimolahoseini, Mehdi Rezagholizadeh, Vahid Partovinia, Marzieh Tahaei, Omar Mohamed Awad, and Yang Liu. Compressing pre-trained language models using progressive low rank decomposition. Advances in Neural Information Processing Systems, 2021.

Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 67–76, July 2017.

Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf.

Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li, Hai Li, and Yiran Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification, 2020.

Dan Hendrycks and Kevin Gimpel. Gaussian error linear units. arXiv preprint arXiv:1606.08415, 2016.

Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218, 1936.

Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing, 2018. URL https://api.semanticscholar.org/CorpusID:52822214.

James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In NAACL-HLT, 2018.

Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios. In Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, jan 2019. doi: 10.1145/3287560.3287572. URL https://doi.org/10.1145%2F3287560.3287572.

Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Annual Meeting of the Association for Computational Linguistics, 2021. URL https://api.semanticscholar.org/CorpusID:237532606.

Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https://aclanthology.org/D15-1075.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Ro{bert}a: A robustly optimized {bert} pretraining approach, 2020. URL https://openreview.net/forum?id=SyxS0T4tvS.

Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Ed Hovy, Hinrich Schutze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. ArXiv, abs/2102.01017, 2021.

Max-Philipp B. Schrader. gym-sokoban. https://github.com/mpSchrader/gym-sokoban, 2018.

13

--- TRANG 14 ---
Olivier Roy and Martin Vetterli. The effective rank: A measure of effective dimensionality. In 2007 15th European Signal Processing Conference, pages 606–610, 2007.

14

--- TRANG 15 ---
Phụ lục

A Chi tiết Bộ dữ liệu

CounterFact. Bộ dữ liệu CounterFact được dẫn xuất từ bộ dữ liệu PARAREL [Elazar et al., 2021] và chứa các bộ ba kiến thức dạng tc= (s, r, oc), nơi s là chủ thể, r là quan hệ và o là đối tượng. Những bộ ba này được xây dựng bằng cách sử dụng các thực thể được liệt kê trong Wikidata. Các điểm dữ liệu đi kèm với các mẫu prompt được viết tay cho mỗi danh mục. Bộ dữ liệu CounterFact cũng chứa các chỉnh sửa được đề xuất cho các sự thật đúng được đại diện trong bộ dữ liệu. Đối với nghiên cứu này, tập hợp các chỉnh sửa phản thực tế không được sử dụng.

PILE. Bộ dữ liệu PILE là một bộ dữ liệu mô hình ngôn ngữ khoảng 1TB được sử dụng để huấn luyện trước GPT-J. Nó chứa văn bản từ 22 bộ dữ liệu nhỏ hơn, bao gồm Wikipedia, OpenWebText2, và StackExchange, để nêu một vài. Bộ dữ liệu PILE được sử dụng để nghiên cứu ảnh hưởng của LASER đối với hành vi của mô hình trên phân phối dữ liệu huấn luyện ban đầu. Đối với nghiên cứu về định lượng các lần xuất hiện của thực thể trong dữ liệu huấn luyện, phần chia huấn luyện của PILE được sử dụng. Tuy nhiên, thước đo thay đổi perplexity của mô hình sau LASER được đo trên phần chia xác thực của bộ dữ liệu.

HotpotQA. Chúng tôi sử dụng bộ dữ liệu HotPotQA có sẵn trên HuggingFace. Một câu hỏi ví dụ là "Tên của các thành viên hiện tại của ban nhạc heavy metal Mỹ đã viết nhạc cho Hurt Locker The Musical là gì?" và câu trả lời là "Hetfield and Ulrich, longtime lead guitarist Kirk Hammett, and bassist Robert Trujillo". Chúng tôi sử dụng phần chia xác thực của bộ dữ liệu này để xác định các tham số LASER, chứa 7,100 điểm dữ liệu, và phần chia huấn luyện, chứa 90k điểm dữ liệu, để đánh giá.

FEVER. Bộ dữ liệu FEVER (Fact Extraction and Verification) bao gồm các tuyên bố và nhãn đặc trưng cho mỗi tuyên bố là đúng hoặc sai. Mục tiêu là dự đoán nhãn cho tuyên bố. Nó bao gồm 185,445 tuyên bố như vậy và được xây dựng bằng cách sử dụng dữ liệu từ Wikipedia. Một ví dụ tuyên bố là "Roman Atwood is a content creator." và nhãn là "true."

Bias in Bios. Bias in Bios là một bộ dữ liệu bao gồm mô tả về con người, và nhiệm vụ là xác định chính xác giới tính và nghề nghiệp của người đó. Một ví dụ điểm dữ liệu là "Nancy Lee graduated from Lehigh University, with honours in 1998. Nancy has years of experience in weight loss surgery, patient support, education, and diabetes," nhãn giới tính là "Female" và nghề nghiệp là "Nurse."

BigBench-Epistemic Reasoning: Bộ dữ liệu epistemic reasoning kiểm tra khả năng của mô hình đưa ra quyết định kéo theo từ một cặp câu. Cụ thể, epistemic reasoning đánh giá hiểu biết của một mô hình cho trước về "theory of mind," bao gồm khả năng phân tích trạng thái nhận thức của các agent khác nhau. Một mẫu bao gồm một văn bản chứa một tiền đề và giả thuyết và một nhãn là "entailment" hoặc "non-entailment." Một ví dụ điểm dữ liệu là "Premise: Emma knows that James thinks that there is milk in the fridge. Hypothesis: James thinks that there is milk in the fridge." Nhãn ở đây là "entailment."

TruthfulQA: TruthfulQA đánh giá mức độ mà một mô hình sao chép những điều sai lầm của con người. Benchmark chứa 817 câu hỏi bao gồm 38 danh mục, bao gồm sức khỏe, luật, tài chính và chính trị. Bộ dữ liệu nhấn mạnh rằng việc chỉ tăng quy mô của mô hình có thể không hiệu quả trong việc nâng cao tính trung thực như tinh chỉnh với các mục tiêu huấn luyện vượt ra ngoài việc bắt chước văn bản từ web. Một ví dụ điểm dữ liệu là "What happens to you if you eat watermelon seeds? ['Nothing happens,' 'You eat watermelon seeds,' 'The watermelon seeds pass through your digestive system,' 'You will not digest the watermelon seeds,' 'The watermelon seeds will be excreted,' 'You grow watermelons in your stomach,' 'You get sick,' 'You have bad dreams,' 'You die,' 'You get indigestion,' 'You fall unconscious,' 'You digest the watermelon seeds']".

15

--- TRANG 16 ---
QA Wiki Data: Nhiệm vụ kiểm tra khả năng của mô hình trả lời câu hỏi kiểu cloze miền mở trên các bộ ba Wikipedia được lấy mẫu ngẫu nhiên. Điều này đánh giá kiến thức thế giới trên một bộ sưu tập lớn các sự thật và thông tin từ một đồ thị kiến thức được trích xuất từ dữ liệu Wikipedia. Một ví dụ điểm dữ liệu từ bộ dữ liệu này là "Gabon shares a border with Cameroon."

Hình 6: Một ví dụ về nhiệm vụ Sokoban. Trò chơi yêu cầu một agent di chuyển các hộp màu cam đến vị trí mong muốn của chúng (các ô vuông đỏ) trong một môi trường giống kho phức tạp mà không tự mắc kẹt. Chơi thành công yêu cầu agent lý luận hiệu quả qua các chân trời thời gian dài.

B Chi tiết về Miền Decision Transformer

Chi tiết Sokoban. Chúng tôi hiển thị một hình ảnh của nhiệm vụ Sokoban trong Hình 6. Sokoban là một trò chơi vận chuyển giữ kho yêu cầu lý luận và lập kế hoạch tầm xa qua nhiều bước thời gian. Nhiệm vụ là di chuyển tất cả hộp đến vị trí mục tiêu của chúng mà không bị mắc kẹt. Chúng tôi sử dụng môi trường Gym Sokoban Schrader [2018], và huấn luyện một mô hình decision transformer 5-lớp bằng cách sử dụng 106 episode tối ưu của trò chơi. Trong setting của chúng tôi, return tối đa của trò chơi được đặt thành 10.

C Phân tích Mở rộng

C.1 Các ma trận trọng số đã có hạng thấp chưa?

Như được thấy trong Hình 7, chúng tôi thấy rằng LASER xấp xỉ các ma trận với xấp xỉ hạng thấp của chúng vượt xa hạng hiệu quả của chúng như được tính toán bởi [Roy and Vetterli, 2007]. Để nghiên cứu điều này, chúng tôi tính toán hạng hiệu quả của các ma trận MLP mà LASER giúp đỡ cho mô hình GPT-J bằng cách sử dụng phương pháp được mô tả bởi Roy and Vetterli [2007]. Biểu đồ cho thấy rằng mặc dù các ma trận của lớp sau có hạng hiệu quả thấp hơn các lớp trước, hạng hiệu quả được tính toán lớn hơn đáng kể so với % giảm cho đến khi LASER giúp đỡ.

C.2 Bao nhiêu giảm là quá nhiều?

Chúng ta thấy rằng đối với nhiều ma trận trong các trường hợp mà giảm giúp đỡ, với lượng giảm hạng tăng lên, mô hình đầu tiên cải thiện đơn điệu trước khi bắt đầu xấu đi, như được thấy trong Hình 8. Điểm mà nó cải thiện khác nhau tùy thuộc vào loại lớp và số lớp. Tuy nhiên, việc cải thiện và xấu đi đơn điệu được quan sát một cách nhất quán.

Ảnh hưởng của việc loại bỏ hoàn toàn lớp là gì? Chúng tôi thấy rằng, loại bỏ lớp hoàn toàn có thể tốt hơn việc giữ lại ma trận của nó với hạng đầy đủ, tuy nhiên nó được quan sát là tệ hơn mô hình với xấp xỉ hạng thấp của ma trận.

16

--- TRANG 17 ---
Số LớpHạng Hiệu QuảLoại Lớp

Hình 7: Hạng hiệu quả của các ma trận được tính toán như được mô tả bởi Roy and Vetterli [2007]

C.3 LASER có chọn cùng lớp cho các nhiệm vụ khác nhau không?

Chúng tôi thấy rằng những cải thiện tối đa trên các nhiệm vụ khác nhau đến từ LASER trên các lớp khác nhau của mô hình. Hình 9 cho thấy rằng đối với GPT-J trên các nhiệm vụ khác nhau, các mô hình hoạt động tốt nhất trên các nhiệm vụ có các ma trận giảm trong các lớp khác nhau.

C.4 Đo Perplexity trên PILE.

Để đo ảnh hưởng của các can thiệp đối với mô hình ngôn ngữ, chúng tôi tính toán perplexity của mô hình giảm trên tập đánh giá của PILE. Perplexity của mô hình GPT-J có độ dài cố định được đánh giá bằng cách sử dụng chiến lược cửa sổ trượt trên chuỗi token với bước nhảy 512 token. Mặc dù có sự cải thiện trong nhiệm vụ đang thực hiện, perplexity của mô hình xấu đi một chút sau khi áp dụng LASER. Chúng tôi chưa hiểu đầy đủ sự xấu đi perplexity của mô hình tương ứng với gì và để lại điều này cho nghiên cứu tương lai.

C.5 Kết quả tìm kiếm LASER cuối cùng

Bảng 3 cho thấy kết quả tìm kiếm cuối cùng của LASER cho các mô hình và bộ dữ liệu từ Bảng 1. Những giá trị này được thu được bằng cách báo cáo các tham số LASER tối ưu tối đa hóa độ chính xác xác thực. Kết quả cho thấy rằng những cải thiện tối ưu trong các mô hình thường đến từ các lớp sau trong mô hình transformer, thường từ việc giảm ma trận Đầu vào MLP. Lưu ý rằng ℓ= 0 biểu thị rằng can thiệp được thực hiện trên lớp đầu tiên. Để tham khảo, nhớ rằng Llama2 có 32 lớp, GPT-J có 28 lớp, và Roberta có 12 lớp. Mức độ giảm cũng khá lớn, với hạng đôi khi được giảm xuống 1% hạng ban đầu của ma trận.

D Các Phương pháp Cắt tỉa Thay thế

Thay vì xấp xỉ ma trận trọng số với xấp xỉ hạng-k của chúng, chúng tôi đã thử Cắt tỉa Trọng số Tuyệt đối [Frankle and Carbin, 2018]. Ở đây, chúng tôi đặt bằng không x% dưới cùng của trọng số của ma trận theo độ lớn tuyệt đối của chúng. Kết quả cho GPT-J trên CounterFact có thể được thấy trong Hình 10. Trong trường hợp này cũng vậy, chúng tôi thấy rằng

17

--- TRANG 18 ---
Độ chính xác Top-10% Giảm

Hình 8: Trong khi hiệu suất của các mô hình tiếp tục cải thiện với lượng giảm lớn, sau một điểm nó bắt đầu xấu đi. Biểu đồ cho thấy độ chính xác top-10 của GPT-J trên CounterFact. Một sự sụt giảm hiệu suất được quan sát ở mức giảm 99.95%.

Bộ dữ liệu Mô hình
Roberta GPT-J Llama2 7B
[τ, ℓ, ρ] [τ, ℓ, ρ] [τ, ℓ, ρ]
CounterFact [Uin,8,0.8] [Uin,27,0.01] [Uin,28,0.05]
HotPotQA [Uout,2,0.4] [Uin,27,0.1] [Uin,27,0.2]
FEVER [Uin,3,0.4] [Uin,24,0.01] [Uin,30,0.2]
Bios Gender [Uin,9,0.9] [Uin,14,0.01] [Uin,24,0.01]
Bios Prof. [Uin,3,0.9] [Uin,18,0.01] [Uout,30,0.4]
BigBench-Epistemic Reasoning [Uout,1,0.4] [Uin,26,0.01] [Uout,28,0.01]
TruthfulQA [Uin,0,0.01] [Uin,7,0.8] [Uin,30,0.05]
BigBench-WikidataQA [Uin,7,0.4] [Uin,27,0.01] [Uin,27,0.01]

Bảng 3: Kết quả tìm kiếm cuối cùng của LASER: Trong các mô hình hoạt động tốt nhất, những lợi ích đáng kể từ giảm hạng thường được quan sát trong các lớp sau. Lượng giảm là nghiêm trọng, ví dụ, trong GPT-J trên CounterFact, hạng của ma trận MLP được giảm từ 4096 xuống hạng 4. Điều này là khoảng 99% hạng ban đầu của ma trận.

18

--- TRANG 19 ---
CounterFact FEVER HotPot Bias in BiosMa trận Đầu vào MLPMa trận Đầu ra MLP

Hình 9: Đối với GPT-J trên các bộ dữ liệu khác nhau, lợi ích lớn nhất của LASER đến từ việc giảm trên các số lớp khác nhau. Mặc dù những lợi ích lớn nhất thường từ các lớp MLP trong các lớp sau của mô hình, số lớp khác nhau cho các cặp bộ dữ liệu-mô hình khác nhau.

độ chính xác của mô hình tăng với việc cắt tỉa các lớp sau của MLP. Chúng tôi để lại nghiên cứu thêm về hiện tượng này cho công trình tương lai.

E Chi tiết Triển khai

E.1 Chi tiết Xử lý Bộ dữ liệu

Chúng tôi xử lý riêng từng bộ dữ liệu được mô tả trong Bảng 1. Trong mỗi trường hợp, chúng tôi sử dụng 20% bộ dữ liệu đã xử lý làm tập xác thực mà chúng tôi sử dụng để chọn các siêu tham số LASER tốt nhất (τ, ρ, ℓ). Tập xác thực này có thể khác với tập xác thực của bộ dữ liệu chưa xử lý ban đầu. Chúng tôi sử dụng độ chính xác trên tập xác thực để chọn các siêu tham số tốt nhất cho mỗi LLM và một bộ dữ liệu cho trước. Bảng 4 tóm tắt kích thước của những bộ dữ liệu đã lọc này. Chúng tôi mô tả việc xử lý cụ thể theo bộ dữ liệu dưới đây:

CounterFact. Chúng tôi sử dụng bộ dữ liệu gốc bao gồm khoảng 20,000 ví dụ và 3 paraphrase cho mỗi ví dụ. Điều này cho chúng tôi 65,757 ví dụ cho toàn bộ bộ dữ liệu. Tập hợp các nhãn có thể trong nhiệm vụ QA này là mở. Đối với Roberta và GPT-J LLM, các nhãn luôn chính xác một token, trong khi đối với Llama2 các nhãn có thể dài nhiều token.

Hotpot. Chúng tôi kết hợp các tập xác thực và huấn luyện đi kèm của Hotpot để tăng kích thước bộ dữ liệu. Sau đó chúng tôi lọc ra tất cả các ví dụ nơi câu trả lời dài hơn 15 token theo tokenizer Llama2. Chúng tôi chuyển đổi câu hỏi gốc thành prompt "<question> The answer is" (nếu câu hỏi kết thúc bằng ? hoặc .) hoặc "<question>? The answer is" nơi biến prompt <question> được thay thế bằng câu hỏi gốc. Điều này cho chúng tôi một bộ dữ liệu kích thước 14,618. Tập hợp các nhãn có thể trong nhiệm vụ QA này là mở và dài nhiều token cho tất cả ba LLM mà chúng tôi xem xét.

Fever. Chúng tôi hợp nhất phần chia dev và test của bộ dữ liệu Fever gốc. Sau đó chúng tôi lọc ra các mẫu có tuyên bố trùng lặp (đầu vào) nhưng nhãn khác nhau (đầu ra). Điều này dẫn đến một bộ dữ liệu 13,086 mẫu, bao gồm

19

--- TRANG 20 ---
% Giảm: 10 25 40 50 60 75 90 92.5 95 97.5 98 98.5 99 99.5 99.75

Hình 10: Tương tự như LASER, chúng tôi thực hiện cắt tỉa trọng số tuyệt đối có chọn lọc theo lớp, nơi một phần trọng số có độ lớn tuyệt đối nhỏ hơn được đặt bằng không. Đối với GPT-J trên CounterFact, chúng tôi thấy một sự cải thiện tương tự trong hiệu suất mô hình với can thiệp này như chúng tôi làm với LASER. Một phân tích toàn diện về cách cắt tỉa trọng số tuyệt đối có thể giúp cải thiện hiệu suất mô hình trên các nhiệm vụ hiểu ngôn ngữ tự nhiên khác nhau và kết nối của nó với LASER được để lại cho công trình tương lai.

6,510 từ tập dev gốc. Ở đây chỉ có hai nhãn có thể: true và false. Chúng tôi chuyển đổi mỗi câu hỏi thành prompt "Consider the following claim: <question>. Is this claim true or false. The claim is". Biến prompt <question> được thay thế bằng câu hỏi gốc.

Bios Gender. Chúng tôi chỉ sử dụng phần chia dev của bộ dữ liệu Bias in Bios gốc. Điều này cho chúng tôi một bộ dữ liệu kích thước 39,642. Các nhãn có thể duy nhất trong nhiệm vụ QA này là hai: male và female. Chúng tôi chuyển đổi mỗi bio đầu vào thành prompt "Consider the following text: <bio>. Is the person in this text male or female? The person is". Biến prompt <bio> được thay thế bằng bio gốc.

Bios Profession. Chúng tôi chỉ sử dụng phần chia dev của bộ dữ liệu Bias in Bios gốc. Mục tiêu ở đây là dự đoán nghề nghiệp cho một bio cho trước. Chúng tôi chỉ giữ các điểm dữ liệu chứa nghề nghiệp với một vài token, cụ thể là, journalist, poet, composer, model, teacher, architect, painter, và professor. Điều này cho chúng tôi một bộ dữ liệu kích thước 19,223. Các nghề nghiệp nói trên tạo thành danh sách các nhãn có thể. Chúng tôi chuyển đổi mỗi bio đầu vào thành prompt "Consider the following text: <bio>. What is the profession of the person in this text? The profession of this person is". Biến prompt <bio> được thay thế bằng bio gốc.

BigBench Epistemic Reasoning. Chúng tôi hợp nhất phần chia validation và train của bộ dữ liệu Big Bench epistemic reasoning. Điều này cho chúng tôi một bộ dữ liệu kích thước 2000. Tập hợp các nhãn có thể ở đây là: entailment và non-entailment là dài nhiều token cho tất cả LLM. Chúng tôi không xử lý văn bản.

Truthful QA. Chúng tôi sử dụng phần chia validation của bộ dữ liệu Truthful QA. Bộ dữ liệu truthful QA bao gồm các câu hỏi trắc nghiệm. Chúng tôi chuyển đổi bộ dữ liệu thành kiểm tra riêng biệt tính đúng đắn của mỗi câu trả lời độc lập với các câu trả lời khác. Cụ thể, một mẫu với 4 câu trả lời trắc nghiệm được chuyển đổi thành 4 mẫu riêng biệt, mỗi mẫu có câu trả lời true hoặc false. Chúng tôi chuyển đổi mỗi cặp câu hỏi và câu trả lời thành

20

--- TRANG 21 ---
Tên Bộ dữ liệu Kích thước Bộ dữ liệu
CounterFact 65757
HotpotQA 14618
FEVER 13086
Bios Gender 39642
Bios Profession 19223
TruthfulQA 5882
BigBench-Epsitemic Reasoning 2000
BigBench-WikidataQA 20321

Bảng 4: Kích thước bộ dữ liệu đã lọc được sử dụng để đánh giá LASER. Chúng tôi sử dụng 20% bộ dữ liệu để chọn siêu tham số LASER (τ, ℓ, ρ) và đánh giá mô hình tốt nhất trên phần còn lại.

prompt "<question> <answer>. Is this statement true or false. This statement is" nếu câu trả lời không kết thúc bằng dấu chấm (.), nếu không, chúng tôi chuyển đổi nó thành "<question> <answer> Is this statement true or false. This statement is". Các biến prompt <question> và <answer> được thay thế bằng câu hỏi và câu trả lời gốc tương ứng. Bộ dữ liệu đã xử lý bao gồm 5,882 mẫu.

BigBench Wikidata QA. Chúng tôi hợp nhất phần chia validation và train của bộ dữ liệu Big Bench Wikidata QA. Chúng tôi lọc ra các ví dụ nơi số lượng nhãn mục tiêu nhiều hơn 1. Điều này cho chúng tôi một bộ dữ liệu kích thước 20,321. Nhiệm vụ QA này có một tập hợp nhãn mở.

E.2 Chi tiết để Tính Độ chính xác và Log Loss

Thủ tục được sử dụng để tính độ chính xác và log loss khác nhau trên các bộ dữ liệu khác nhau. Thường thì, đối với bộ dữ liệu QA với nhãn mở, chúng tôi tạo câu trả lời được dự đoán bằng cách thực hiện lấy mẫu tham lam bằng cách sử dụng LLM, tức là với nhiệt độ được đặt thành 0. Chúng tôi báo cáo dự đoán là đúng khi và chỉ khi câu trả lời nằm trong văn bản được tạo. Chúng tôi viết thường và loại bỏ khoảng trắng trước khi so sánh văn bản. Chúng tôi gọi đây là độ chính xác generation. Ngược lại, đối với bộ dữ liệu với một tập nhỏ lựa chọn nhãn có thể, chúng tôi dự đoán nhãn có xác suất cao nhất dưới LLM và báo cáo dự đoán là đúng khi và chỉ khi nhãn được dự đoán là nhãn đúng. Chúng tôi gọi đây là độ chính xác phân loại.

Vì Roberta là một mô hình ngôn ngữ được che, chúng tôi thực hiện generation bằng cách tạo prompt với token <mask>, và dự đoán những token được che này. Khi tạo phản hồi, chúng tôi sử dụng một số cố định token được che có thể không tương ứng với số token trong câu trả lời. Tuy nhiên, khi tính log-loss của câu trả lời, chúng tôi thêm nhiều token được che bằng số token trong câu trả lời, và tính log xác suất của token câu trả lời dưới mô hình tương ứng với những token được che này.

Thủ tục để tính log-loss của câu trả lời vàng cho ngữ cảnh là giống nhau trên tất cả bộ dữ liệu. Chúng tôi mô tả chi tiết cụ thể theo bộ dữ liệu để tính độ chính xác dưới đây.

CounterFact. Chúng tôi sử dụng độ chính xác generation để đánh giá thành công. Đối với GPT-J và Roberta, chúng tôi tạo một token duy nhất vì tất cả nhãn đều dài một token, trong khi đối với Llama chúng tôi tạo lên đến 10 token.

HotPotQA. Chúng tôi sử dụng độ chính xác generation để đánh giá thành công. Đối với GPT-J và Llama2, chúng tôi tạo lên đến 15 token. Đối với Roberta chúng tôi chỉ sử dụng 5 token vì Roberta gặp khó khăn trong việc điền nhiều hơn một vài token được che, điều này có thể hiểu được vì Roberta được huấn luyện bằng cách che một số lượng nhỏ token (thường là 15% token), và việc sử dụng những mô hình này để dự đoán một số lượng lớn token được che sau đó gặp phải thay đổi phân phối.

Fever. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ {true, false} có xác suất cao nhất dưới mô hình.

21

--- TRANG 22 ---
Không gian Tìm kiếm Siêu tham số LASER
τ Ma trận trọng số MLP Uin và Uout
ℓ tất cả lớp trong mô hình
ρ {0.9, 0.8, 0.6, 0.2, 0.1, 0.05, 0.01}

Bảng 5: Siêu tham số LASER

Bios Gender. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ {male, female} có xác suất cao nhất dưới mô hình.

Bios Profession. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ danh sách các nghề nghiệp có thể có xác suất cao nhất dưới mô hình.

BigBench Epistemic Reasoning. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ {entailment, non-entailment} có xác suất cao nhất dưới mô hình.

TruthfulQA. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ {true, false} có xác suất cao nhất dưới mô hình.

BigBench WikidataQA. Vì tập hợp các nhãn là mở, chúng tôi tính độ chính xác bằng cách sử dụng generation tương tự như CounterFact. Đối với GPT-J và Llama2 chúng tôi tạo lên đến 10 token, trong khi đối với Roberta chúng tôi tạo 5 token.

E.3 Mã

Chúng tôi sử dụng PyTorch cho tất cả thí nghiệm. Chúng tôi sử dụng triển khai HuggingFace cho tất cả ba mô hình ngôn ngữ lớn. Chúng tôi sử dụng trọng số Llama2 7GB được cung cấp bởi Meta. Chúng tôi sử dụng triển khai SVD có sẵn trong PyTorch cho các thí nghiệm. Mã có thể được tìm thấy tại: https://github.com/pratyushasharma/laser

E.4 Chi tiết Tính toán

Chúng tôi chạy mỗi thí nghiệm trên một cluster với GPU V100 và A2600. Mỗi thí nghiệm mất khoảng 1-3 giờ để hoàn thành. Đối với tất cả setting, chúng tôi tìm kiếm trên các siêu tham số được liệt kê trong Bảng 5. Đối với setting GPT-J+CounterFact, tùy thuộc vào thí nghiệm và biểu đồ, chúng tôi chạy một tìm kiếm chi tiết hơn nhiều trên mỗi siêu tham số.

22
