# 2312.07046.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/approximation/2312.07046.pdf
# File size: 102169 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2312.07046v1  [cs.LG]  12 Dec 2023Rethinking Compression: Reduced Order Modelling
of Latent Features in Large Language Models
Arnav Chavan∗1,2, Nahush Lele∗1, Deepak Gupta2
1Nyun AI2Transmute AI Lab
arnav.chavan@nyunai.com, guptadeepak2806@gmail.com
Abstract
Due to the substantial scale of Large Language Models (LLMs) , the direct ap-
plication of conventional compression methodologies prov es impractical. The
computational demands associated with even minimal gradie nt updates present
challenges, particularly on consumer-grade hardware. Thi s paper introduces an
innovative approach for the parametric and practical compr ession of LLMs based
on reduced order modelling, which entails low-rank decompo sition within the fea-
ture space and re-parameterization in the weight space. Not ably, this compression
technique operates in a layer-wise manner, obviating the ne ed for a GPU device
and enabling the compression of billion-scale models withi n stringent constraints
of both memory and time. Our method represents a signiﬁcant a dvancement in
model compression by leveraging matrix decomposition, dem onstrating superior
efﬁcacy compared to the prevailing state-of-the-art struc tured pruning method.
1 Introduction
Recent advances in generative modeling have led to a notable increase in the construction of large
language models (LLMs), some of which consist of hundreds of billions of parameters. Despite
their commendable accuracy, the associated computational demands are considerable, particularly
in terms of GPU memory for inference. In practical applicati ons, there is a growing need to compress
these models while minimizing the accompanying performanc e degradation.
Promising approaches to compress LLMs include pruning (Fra ntar & Alistarh, 2023; Sun et al.,
2023; Ma et al., 2023), quantization (Frantar et al., 2022; D ettmers et al., 2023) and knowledge dis-
tillation (Wu et al., 2023; Gu et al., 2023). Current LLM quan tization methods require speciﬁc
hardware-level support and are not able to reduce MACs and sp eed up inference time due to ex-
pensive quant-dequant operations in LLMs. Knowledge disti llation has been shown to perform
well in a training-aware fashion on standard deep learning m odels. However, the massive com-
putational resources needed for distillation limits the ap plicability of such approaches. Recently,
(Ma et al., 2023) presented a structured pruning approach de signed for LLMs. While this approach
is capable of pruning the LLMs with no need for ﬁne-tuning, th e drop in performance is signiﬁcant,
and clearly there is a need to explore further in this directi on. Moreover, the pruning strategy is not
universal and signiﬁcant effort is needed per neural archit ecture to identify the prunable structures.
In this paper, we present a novel, practical and training-fr ee approach to model compression which
is speciﬁcally for large models including LLMs. Referred fu rther as LLM-ROM, our approach per-
forms localized reduced order modelling of the latent featu res through low-rank decomposition in
the feature space and re-parameterization in the weight spa ce. Since LLM-ROM operates layerwise,
it does not require any massive model updates and can be execu ted on small GPU/CPU resources.
The simplicity of LLM-ROM facilitates the compression of bi llion-scale models within stringent
∗Equal contribution. Work done while Nahush was an Intern at N yun AI
Preprint. Under review.

--- PAGE 2 ---
constraints of both memory and time. Our early experiments d emonstrate that LLM-ROM outper-
forms existing approaches and can compress LLMs without any ﬁne-tuning.
2 Method
LLM-ROM builds reduced-order model (ROM) layerwise, and fo r a model with Llayers, the de-
composition of the latent feature maps is done in a sequentia l manner using a calibration data
X∈RB×d1, whereBandd1denote the batch-size and number of input channels, respect ively.
For theithlayer, denoted as Liwith weights Wi∈Rd2×d1whered2denotes the output channels,
we compute the feature map Yi=WiXi∈RB×d2. Following this, the principal components of
Yiare then computed through eigenvalue decomposition of the s ymmetric covariance matrix of Yi.
These components can be represented as Vj∈Rd2∀j∈[1,d2], and the principal component
matrix can be represented as V∈Rd2×d2, with each row denoting a principal component arranged
in the descending order of their eigenvalue.
Depending upon the target rank of the layer, we select only th e toprprincipal components ranked
by their respective eigenvalues. Thus, we index Vr=V[1→r,:]∈Rr×d2. Thus, the ROM of
this layer can be denoted as Yi=VT
rVrWiXi. Upon re-parameterization into low-rank matrices,
Wi1=VT
r∈Rd2×randWi2=VrWi∈Rr×d1, the layer can be decomposed into a sequential
combination of two smaller linear layers with weights Wi1andWi2respectively. We consider
the ROM of the previous layer to generate inputs for the next l ayer so that the next layers have
prior information of the error introduced in the previous la yers for decomposition. Note that the
ROM operations are performed on CPU with no requirement for a GPU, and the computational cost
associated with it is very small.
2.1 Layerwise rank computation
The LLaMA-7B (Touvron et al., 2023) model consists of 32 iden tical decoder modules (these mod-
ules comprise >96% of the total model parameters), each of wh ich consists of seven decomposable
weight matrices. Our initial studies showcased that settin g a uniform compression budget for all the
modules from the very beginning of the model lead to a signiﬁc ant deterioration in model perfor-
mance; for this reason, we restrict the application of our co mpression process to a subset of modules.
Further, decomposition of the layers of a module introduces errors in the outputs of that layer which
get compounded as we move forward in the network; to minimize this, we only compress mod-
ules towards the end of the model. Based on these heuristics w e perform experiments compressing
varying numbers of modules from the end depending on the budg et that we need to satisfy for the
entire model. The speciﬁc number of modules to be compressed is empirically determined for each
budget. For instance, to achieve an overall budget of 80%, we conducted experiments compressing
only the last 8 modules uniformly with a budget of 0.20, the la st 12 modules with a budget of 0.46,
and the last 16 modules with a budget of 0.60. Our ﬁndings indi cated that compressing the last
12 modules yielded the most favorable results for 80% budget . Similar experiments for 90% and
50% budget yield the best results when we compress the last 8 m odules with a budget of 0.60 and
the last 24 modules with a budget of 0.33 respectively, the re sults of which are listed in Table 1.
Each module is originally composed of 4 weight matrices Wi∈R4096×4096from the self attention
block and 3 weight matrices Wj∈R4096×11008from the feed-forward network (although one of
these is transposed, it does not change the computed rank). A t the end of compression, each weight
matrix in the self-attention block is decomposed into two lo w-rank matrices: Wi1∈R4096×rand
Wi2∈Rr×4096, wherertakes values of 1228, 954, and 675. Additionally, the weight matrices in
the feed-forward network are decomposed into Wj1∈R4096×randWj2∈Rr×11008withrvalues
of 1791, 1373, and 985. These decompositions correspond to t hree module budget settings of 60%,
46%, and 33%, resulting in overall model budgets of 90%, 80%, and 50%, respectively.
3 Experiments
3.1 Zero-shot performance
To evaluate the performance of the model in a task-agnostic s etting, we employ LLaMA’s
(Touvron et al., 2023) assessment methodology, performing zero shot task classiﬁcation across com-
2

--- PAGE 3 ---
mon sense reasoning datasets, including BoolQ (Clark et al. , 2019), PIQA (Bisk et al., 2020), Hel-
laSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al. , 2021), ARC-easy (Clark et al., 2018),
and ARC-challenge (Clark et al., 2018). We use a batch-size o f 512 for the calibration data (Section
3.3) from the training splits of the aforementioned dataset s, ensuring no data leakage, and set a max-
imum sequence length of 128. We set target compression rates of 80%, and 50% and compare with
LLM-Pruner2with and without ﬁne-tuning in Table 1.
Our LLM-ROM method consistently outperforms LLM-Pruner at 80% and 50% compression with-
out any ﬁne-tuning. It is noteworthy that at 80% budget our me thod even outperforms ﬁne-tuned
LLM-Pruner model, signifying that ROM is able to better extr act smaller neural structures and
weights from larger counterparts without any gradient upda tes on the extracted weights.
Method Finetune #Params #MACs BoolQ PIQA HellaSwag WinoGra nde ARC-e ARC-c Average
LLaMA-7B - 6.7B 423.93G 76.5 79.8 76.1 70.1 72.8 47.6 70.5
LLM-Pruner ✗ 5.4B 339.60G 59.4 75.6 65.3 61.3 59.2 37.1 59.7
LLM-Pruner ✓ 5.4B 339.60G 69.5 76.4 68.1 65.1 63.4 37.9 63.4
LLM-ROM ✗ 5.4B 339.99G 74.5 73.8 66.6 68.1 67.2 39.8 65.0
LLM-Pruner ✗ 3.4B 206.59G 52.3 59.6 35.6 53.2 33.5 27.2 43.6
LLM-Pruner ✓ 3.4B 206.59G 60.3 69.3 47.1 53.4 46.0 29.2 50.9
LLM-ROM ✗ 3.5B 215.61G 62.0 62.5 35.3 57.7 39.3 27.6 47.4
Table 1: Comprehensive comparison of our method with LLM-Pr uner on LLaMA-7B model.
3.2 Effect of Batch Size and Sequence Length
The eigenvalue decomposition of the covariance matrix and t he subsequent selection of the principal
components require a computation of the outputs of that laye r. The batch used to compute this out-
put is a key factor that can inﬂuence the generalizability of the layers obtained after decomposition.
Principal components computed on a larger sample size will e xhibit closer alignment with those of
the true distribution. To corroborate this hypothesis, we c onducted experiments along two orthog-
onal directions: one by solely varying the batch-size, and t he other with the variation of sequence
length, the results for the same are presented in Table 2 and 3 respectively.
Batch Size BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c Aver age
512 74.5 73.8 66.6 68.1 67.2 39.8 65.0
128 72.6 72.4 63.2 66.3 61.1 37.7 62.2
32 70.2 68.4 58.7 67.2 55.7 35.7 59.3
Table 2: Effect of batch size on model performance at a sequen ce length 128.
Seq. Length BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c Ave rage
128 74.5 73.8 66.6 68.1 67.2 39.8 65.0
64 66.6 74.4 65.7 67.6 65.4 40.1 63.3
32 66.2 73.4 65.1 67.8 64.6 39.5 62.7
Table 3: Effect of sequence length on model performance at ba tch size 512.
From Tables 2 and 3 it is evident that a larger batch is beneﬁci al and results in signiﬁcantly better
model generalization and at the same time longer sequence le ngth also aids in maintaining model
performance post compression.
3.3 Choice of calibration dataset
Given that the activations of data from the calibration data set are used to calculate the covariance ma-
trix, which is subsequently utilized for eigendecompositi on, it is reasonable to infer that the model’s
2We pick the best performing pruning method from LLM-Pruner i.e.Block Pruning.
3

--- PAGE 4 ---
performance is sensitive to the choice of this dataset. This inference is supported by our ﬁndings in
the conducted studies where we use three different datasets , namely ARC-challenge (Clark et al.,
2018), BookCorpus(Zhu et al., 2015) and a combination of all the common sense task prompts
i.e each batch contains an equal number of samples from the si x common sense reasoning tasks’
datasets used for benchmarking namely BoolQ (Clark et al., 2 019), PIQA (Bisk et al., 2020), Hel-
laSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al. , 2021), ARC-easy (Clark et al., 2018),
and ARC-challenge (Clark et al., 2018) , as our calibration d atasets at a budget of 80% keeping other
hyperparameters such as batch-size and sequence length con stant. When creating the calibration
datasets we choose samples from a data split which is disjoin t from the set upon which evaluation is
performed, ensuring there is no data leak. The results of the se studies are compiled in Table 4.
Dataset BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c Averag e
Combination 74.5 73.8 66.6 68.1 67.2 39.8 65.0
ARC-c 64.6 72.5 63.8 67.0 67.8 40.9 62.8
Book Corpus 63.2 73.6 65.6 67.7 63.6 38.0 61.9
Table 4: Comparison of model performance with respect to cho ice of calibration dataset
The results presented above show the inﬂuence of the calibra tion dataset choice on model perfor-
mance. It is unsurprising that the dataset, which consists o f the combination of all common sense
tasks used for benchmarking, exhibits the most favorable re lative performance.
4 Computational Cost
We conduct ROM of LLaMA-7B (Touvron et al., 2023) on a CPU serv er with 128 GB RAM and 48-
core/96-thread processor. Our current implementation loa ds the complete model at once; however, it
is trivial to perform ROM layerwise and hence can be done in un der 10 GB of peak RAM given that
only inputs and weights of current layer are loaded and proce ssed into the memory. On an average
it takes 13 seconds to perform ROM of each layer of LLaMA-7B (T ouvron et al., 2023) which has a
total of 224 layers. Overall, it takes 15.8 minutes, 21.8 min utes and 28.9 minutes for 90%, 80% and
50% compression rates respectively.
5 Conclusion
In this paper we presented a new direction for LLM compressio n leveraging reduced order modeling
of the latent features. Based on the concept of identifying t he ﬁnite set of most useful latent feature
modes, LLM-ROM is capable of compressing LLMs without the ne ed for any ﬁne-tuning. With
no requirement of a GPU during the compression process, LLM- ROM can be efﬁciently run on a
simple CPU machine. Moreover, unlike pruning, LLM-ROM is ve ry generic and does not require
manual interference for different model architectures. Ba sed on the presented results, we hope to
have paved way for a novel approach to design compressed LLMs in a resource-efﬁcient manner.
References
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al . Piqa: Reasoning about physical
commonsense in natural language, 2020.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatko wski, Michael Collins, and Kristina
Toutanova. BoolQ: Exploring the surprising difﬁculty of na tural yes/no questions. In Jill
Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computati onal Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Papers) , pp. 2924–2936, Minneapolis, Min-
nesota, June 2019. Association for Computational Linguist ics. doi: 10.18653/v1/N19-1300. URL
https://aclanthology.org/N19-1300 .
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashis h Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? t ry arc, the ai2 reasoning challenge.
arXiv preprint arXiv:1803.05457 , 2018.
4

--- PAGE 5 ---
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zett lemoyer. Qlora: Efﬁcient ﬁnetuning
of quantized llms. arXiv preprint arXiv:2305.14314 , 2023.
Elias Frantar and Dan Alistarh. Sparsegpt: Massive languag e models can be accurately pruned in
one-shot. In International Conference on Machine Learning , pp. 10323–10337. PMLR, 2023.
Elias Frantar, Saleh Ashkboos, Torsten Hoeﬂer, and Dan Alis tarh. Gptq: Accurate post-training
quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.
Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge dis tillation of large language models.
arXiv preprint arXiv:2306.08543 , 2023.
X. Ma, G. Fang, and X. Wang. Llm-pruner: On the structural pru ning of large language models.
NeurIPS , 2023.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An ad-
versarial winograd schema challenge at scale. Commun. ACM , 64(9):99–106, aug 2021. ISSN
0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381 .
Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simpl e and effective pruning approach
for large language models. arXiv preprint arXiv:2306.11695 , 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mart inet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisa l Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Op en and efﬁcient foundation
language models, 2023.
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mage ed, and Alham Fikri Aji.
Lamini-lm: A diverse herd of distilled models from large-sc ale instructions. arXiv preprint
arXiv:2304.14402 , 2023.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-
chine really ﬁnish your sentence? In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , pp. 4791–4800, 2019.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdin ov, Raquel Urtasun, Anto-
nio Torralba, and Sanja Fidler. Aligning books and movies: T owards story-like visual ex-
planations by watching movies and reading books. CoRR , abs/1506.06724, 2015. URL
http://arxiv.org/abs/1506.06724 .
5
