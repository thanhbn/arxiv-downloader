# 2210.17357.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2210.17357.pdf
# Kích thước file: 2373580 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
L-G RECO: NÉN GRADIENT THÍCH ỨNG THEO TẦNG CHO
DEEP LEARNING HIỆU QUẢ VÀ CHÍNH XÁC
Mohammadreza Alimohammadi1 *Ilia Markov1 *Elias Frantar1Dan Alistarh1 2
TÓM TẮT
Huấn luyện phân tán song song dữ liệu của mạng nơ-ron sâu (DNN) đã được áp dụng rất rộng rãi, nhưng vẫn có thể gặp phải tắc nghẽn truyền thông. Để giải quyết vấn đề này, toàn bộ các họ cơ chế nén đã được phát triển, bao gồm lượng tử hóa, thưa hóa và xấp xỉ hạng thấp, một số trong đó đang được áp dụng thực tế đáng kể. Mặc dù có tiến bộ này, hầu như tất cả các sơ đồ nén đã biết đều áp dụng nén đồng nhất trên các tầng DNN, mặc dù các tầng là không đồng nhất về số lượng tham số và tác động của chúng đến độ chính xác mô hình. Trong công trình này, chúng tôi cung cấp một khung tổng quát để thích ứng mức độ nén trên các tầng của mô hình một cách động trong quá trình huấn luyện, cải thiện nén tổng thể, trong khi dẫn đến tăng tốc đáng kể, mà không hy sinh độ chính xác. Khung của chúng tôi, được gọi là L-GreCo, dựa trên một thuật toán thích ứng, tự động chọn các tham số nén tối ưu cho các tầng mô hình đảm bảo tỷ lệ nén tốt nhất trong khi thỏa mãn ràng buộc lỗi. Các thí nghiệm rộng rãi trên các nhiệm vụ phân loại hình ảnh và mô hình hóa ngôn ngữ cho thấy L-GreCo có hiệu quả trên tất cả các họ phương pháp nén hiện có, và đạt được tăng tốc huấn luyện lên đến 2.5× và cải thiện nén lên đến 5× so với các triển khai hiệu quả của các phương pháp hiện có, trong khi khôi phục toàn bộ độ chính xác. Hơn nữa, L-GreCo bổ sung cho các thuật toán thích ứng hiện có, cải thiện tỷ lệ nén của chúng 50% và thông lượng thực tế 66%. Một triển khai ẩn danh có sẵn tại https://github.com/LGrCo/L-GreCo.

1 GIỚI THIỆU
Sự tăng trưởng lớn về kích thước mô hình và tập dữ liệu cho học sâu đã làm cho phân tán trở thành phương pháp tiêu chuẩn để huấn luyện. Chiến lược phổ biến nhất là phân tán song song dữ liệu đồng bộ, chia dữ liệu giữa các worker song song, mỗi worker tính toán gradient ngẫu nhiên trên dữ liệu của họ, sau đó tính trung bình gradient của các worker trong một bước đồng bộ. Quy trình này có một số ưu điểm, nhưng gây ra hai chi phí chính: chi phí đồng bộ hóa của đồng bộ hóa kiểu barrier ở mỗi bước, và chi phí truyền thông của việc trao đổi gradient theo kiểu all-to-all.

Đã có công việc đáng kể về việc giảm thiểu những chi phí này. Cụ thể, một phương pháp phổ biến để giảm chi phí truyền thông gradient, đó là trọng tâm chính của bài báo của chúng tôi, là nén gradient có mất mát (Seide et al., 2014; Alistarh et al., 2017; Dryden et al., 2016; Vogels et al., 2019), giảm số bit được truyền mỗi lần lặp. Hàng trăm kỹ thuật như vậy đã được đề xuất, có thể được phân loại thô thành ba họ phương pháp. Phương pháp tổng quát đầu tiên là lượng tử hóa (Seide et al., 2014; Alistarh et al., 2017; Wen et al., 2017), giảm độ rộng bit của các gradient được truyền theo cách nhận biết phương sai, để bảo toàn sự hội tụ. Thứ hai là thưa hóa (Strom, 2015; Dryden et al., 2016; Lin et al., 2017), giảm số lượng thành phần gradient được cập nhật ở mỗi bước, được chọn thông qua các số liệu saliency khác nhau. Phương pháp thứ ba và gần đây nhất là xấp xỉ hạng thấp (Wang et al., 2018; Vogels et al., 2019), tận dụng cấu trúc hạng thấp của tensor gradient để giảm thiểu chi phí truyền thông.

Trên thực tế, những phương pháp này đi kèm với sự đánh đổi không tầm thường về nén so với dễ sử dụng. Ví dụ, lượng tử hóa gradient dễ triển khai và triển khai, nhưng chỉ cung cấp nén hạn chế trước khi giảm độ chính xác; thưa hóa và xấp xỉ hạng thấp có thể cung cấp cải thiện nén theo thứ tự độ lớn, nhưng đi kèm với chi phí bổ sung về duy trì sửa lỗi và điều chỉnh siêu tham số cẩn thận để có kết quả tốt nhất. Những sự đánh đổi trên đã được cộng đồng nghiên cứu kỹ lưỡng, cùng với các phương pháp nén thích ứng mới (Agarwal et al., 2021b; Markov et al., 2022; Faghri et al., 2020), sử dụng nén "đã học", điều chỉnh nén theo lỗi phát sinh trong các giai đoạn khác nhau của huấn luyện DNN.

Mặc dù có lượng nghiên cứu rộng lớn này, sự tương tác giữa những phương pháp này, triển khai hệ thống của chúng, và động lực huấn luyện cơ bản, đã nhận được ít sự chú ý hơn đáng kể. Cụ thể, hầu như tất cả các công trình hiện có xem mô hình DNN như một tập tham số đồng nhất, và áp dụng nén hoặc toàn cục cho toàn bộ mô hình, ví dụ bằng cách thực hiện lựa chọn Top-K trên gradient khi thưa hóa (Chen et al., 2018), hoặc đồng nhất, áp dụng cùng mức độ nén cho mỗi tầng, độc lập với kích thước tầng hoặc tác động đến mất mát, tức "độ nhạy" (Vogels et al., 2019). Quan điểm này có thể dẫn đến hai sai lầm chính. Thứ nhất, từ phía ứng dụng, điều này bỏ lỡ những cơ hội tối ưu hóa đáng kể: các mô hình sâu hiện đại, đặc biệt là Transformer (Vaswani et al., 2017) có thể rất không đồng nhất về cả kích thước tầng và độ nhạy tầng đối với nén gradient, và nén gradient có thể có tác động khác nhau trong các giai đoạn huấn luyện (Achille et al., 2018). Thứ hai, từ phía hệ thống, hầu hết các khung huấn luyện hiệu quả chồng chéo truyền thông và tính toán, truyền gradient tầng ngay khi chúng được tạo ra. Do đó, việc có được một cái nhìn toàn cục nhất quán về tham số, cần thiết để thực hiện lựa chọn Top-K toàn cục là không thể, hoặc rất tốn kém để triển khai và áp dụng.

Những ví dụ cụ thể này chỉ ra một khoảng cách đáng kể giữa các kỹ thuật nén thuật toán, cần thiết cho huấn luyện song song dữ liệu hiệu quả, và triển khai thực tế của chúng. Cụ thể hơn, việc hỏi là tự nhiên: cho một mô hình tùy ý và một kỹ thuật nén, có cách hiệu quả nào để cân bằng các ràng buộc ứng dụng, tức độ nhạy tầng, một mặt, và các ràng buộc truyền thông hệ thống, tức kích thước tầng, mặt khác, một cách động trong quá trình huấn luyện, để tối đa hóa tăng tốc, mà không hy sinh độ chính xác cuối cùng của mô hình?

Để giải quyết những câu hỏi này, chúng tôi giới thiệu L-GreCo, một khung hiệu quả và tổng quát cho tham số hóa theo tầng của các sơ đồ nén GRadiEnt COmpression. Về mặt thuật toán, L-GreCo dựa trên một sự hình thức hóa mới của bài toán nén thích ứng theo tầng, xác định các tham số nén mỗi tầng, ví dụ cài đặt thưa hóa mỗi tầng hoặc mức lượng tử hóa, tối đa hóa lượng nén, dưới ràng buộc cố định về tổng lỗi do nén gradient, được đặt sao cho không mất độ chính xác. Ở cấp độ hệ thống, L-GreCo hoạt động bằng cách tích hợp với các khung huấn luyện tiêu chuẩn, như torch.distributed, để khai thác tính không đồng nhất của mô hình về cả cấu trúc mỗi tầng và độ nhạy mỗi tầng, xác định ngay lập tức mức độ nén gradient tầng riêng lẻ để tối đa hóa nén hoặc thời gian huấn luyện đầu cuối.

Chúng tôi xác nhận L-GreCo trên tất cả các họ chiến lược nén hiện có: lượng tử hóa, thưa hóa và nén hạng thấp, và trên nhiều nhiệm vụ thị giác và ngôn ngữ. Các thí nghiệm của chúng tôi cho thấy L-GreCo liên tục đạt tỷ lệ nén cao hơn các chiến lược thủ công hoặc thích ứng hiện có, và đặc biệt hiệu quả đối với các mô hình dựa trên Transformer không đồng nhất và nhạy cảm. Cụ thể, khung cung cấp lợi ích cho tất cả các chiến lược nén hiện có theo cách hộp đen, và dẫn đến lợi ích nén và tăng tốc thực tế đáng kể, so với các sơ đồ tham số hóa thủ công tốt nhất đã biết, khi thực hiện các benchmark phân loại hình ảnh tiêu chuẩn hoặc mô hình hóa ngôn ngữ trong cài đặt đơn và đa node.

Chúng tôi tóm tắt đóng góp của mình như sau:
• Chúng tôi chỉ ra rằng các sơ đồ nén gradient có thể tận dụng cấu trúc theo tầng không đồng nhất của DNN để giảm chi phí truyền thông.
• Chúng tôi thiết kế và triển khai khung L-GreCo, đảm bảo mức nén theo tầng tối ưu về sự đánh đổi nén-độ chính xác, dựa trên một thước đo độ nhạy nén tầng được biện minh về lý thuyết.
• Chúng tôi cung cấp một đánh giá thực nghiệm rộng rãi trên các mạng nơ-ron khác nhau (ResNet18, ResNet50, Transformer-XL, Transformer-LM) với các tập dữ liệu khác nhau (CIFAR-100, ImageNet, Wikitext-103) cho thấy L-GreCo giảm truyền thông lên đến 5× và đạt tăng tốc lên đến 2.5× mà không mất độ chính xác hoặc điều chỉnh đáng kể, trên các triển khai đơn và đa node.
• Chúng tôi tiến hành nghiên cứu chi tiết đầu tiên cho cả số liệu độ nhạy tầng và số liệu hiệu suất. Chúng tôi thấy rằng các số liệu độ nhạy dựa trên lỗi lượng tử hóa tương đương với các số liệu dựa trên mất mát đầu ra. Đồng thời, từ góc độ hiệu suất, các số liệu tìm cách tối đa hóa tỷ lệ nén dẫn đến kết quả tương tự như những số liệu dựa trên thời gian.
• Cuối cùng, chúng tôi chỉ ra rằng L-GreCo tương thích với các sơ đồ nén thích ứng khác: cụ thể, nó có thể được mở rộng để sử dụng thông tin về các giai đoạn khác nhau của huấn luyện (Agarwal et al., 2021b), dẫn đến cải thiện hiệu suất thêm.

2 CÔNG TRÌNH LIÊN QUAN
Phương pháp nén. Nén gradient thường sử dụng ba chiến lược: lượng tử hóa, thưa hóa và phân rã hạng thấp. Các phương pháp lượng tử hóa (Seide et al., 2014; Alistarh et al., 2017; Wen et al., 2017; Lim et al., 2018; Ramezani-Kebrya et al., 2021) sử dụng độ chính xác thấp hơn của mỗi thành phần gradient, giảm số bit được truyền. Chúng dễ triển khai và hoạt động dưới các siêu tham số ổn định (Alistarh et al., 2017; Xu et al., 2021; Markov et al., 2022); tuy nhiên, nén của chúng bị hạn chế bởi thực tế là ít nhất một bit mỗi mục thường phải được truyền. Các kỹ thuật thưa hóa (Strom, 2015; Dryden et al., 2016; Lin et al., 2017; Karimireddy et al., 2019) khắc phục điều này bằng cách xác định các thành phần quan trọng trong gradient và chỉ truyền những tập con đó. Cuối cùng, các thuật toán

--- TRANG 2 ---
phân rã gradient (Vogels et al., 2019; Wang et al., 2018) sử dụng thực tế rằng các tensor gradient theo tầng được biết là có thể xấp xỉ tốt thông qua ma trận hạng thấp, và nhằm thiết kế các phương pháp chiếu nhẹ cũng cung cấp lỗi thấp. Các kỹ thuật thưa hóa và hạng thấp thường yêu cầu bộ đệm sửa lỗi để bảo toàn sự hội tụ tốt, cũng như điều chỉnh siêu tham số không tầm thường.

Như chúng tôi chỉ ra thực nghiệm, L-GreCo tương thích với tất cả các phương pháp này và có thể cung cấp tiết kiệm băng thông bổ sung đáng kể cho mỗi chiến lược như vậy, mà không hy sinh độ chính xác mô hình và không cần điều chỉnh.

Nén thích ứng. Ý tưởng tổng quát về việc thích ứng mức độ nén trong quá trình huấn luyện đã được nghiên cứu bởi AdaComp (Chen et al., 2018), đề xuất một phương pháp nén thích ứng tự điều chỉnh; tuy nhiên, phương pháp của họ không thích ứng tham số nén theo tầng, và không thể được kết hợp với các phương pháp nén khác. Faghri et al. (2020) thích ứng lưới lượng tử hóa với phân bố gradient; tuy nhiên, phương pháp của họ được điều chỉnh cụ thể cho lượng tử hóa, và không quan tâm đến tính không đồng nhất tầng.

Sahu et al. (2021) tối ưu hóa tổng lỗi qua các bước cho nén dựa trên thưa hóa và đề xuất các bộ thưa hóa toàn cục ngưỡng, được chỉ ra đạt tỷ lệ nén cao hơn nén đồng nhất mỗi tầng trên các nhiệm vụ thị giác nhỏ (ví dụ ResNet18 trên CIFAR10/100). Tuy nhiên, phương pháp của họ bị hạn chế đối với thưa hóa và để lại việc điều chỉnh ngưỡng cho các mô hình quy mô lớn, nhạy cảm như Transformer hoặc mô hình quy mô ImageNet không rõ ràng. Đặc biệt, chúng tôi không thể có được kết quả tốt với phương pháp này trên các mô hình như Transformer-XL hoặc Transformer-LM. Trong phần 5.3 chúng tôi trình bày so sánh với phương pháp của họ trên ResNet18/CIFAR-100, cho thấy phương pháp của chúng tôi mang lại cả độ chính xác cao hơn và nén cao hơn.

Accordion (Agarwal et al., 2021a) thích ứng các tham số nén của thưa hóa và nén hạng thấp dựa trên các chế độ quan trọng của huấn luyện. Thuật toán luân phiên giữa hai mức nén ("thấp" và "cao"), được người dùng cung cấp và dễ bị mất độ chính xác. Phương pháp của chúng tôi cải thiện so với Accordion về tăng tốc, nhưng cũng có thể kết hợp phương pháp của chúng tôi với Accordion và đạt được lợi ích cao hơn nữa. Một biến thể của bài toán nén theo tầng tương tự như cái chúng tôi xem xét ở đây đã được đề xuất bởi CGX (Markov et al., 2022). Tuy nhiên, họ nghiên cứu một heuristic dựa trên kmeans, mà chúng tôi chỉ ra thực nghiệm là không tối ưu.

Theo hiểu biết của chúng tôi, chiến lược lập trình động của chúng tôi chưa được sử dụng trong bối cảnh nén gradient thích ứng. Các phương pháp liên quan đã được nghiên cứu trong bối cảnh nén trọng số cho DNN, xem ví dụ (Wu et al., 2020; Aflalo et al., 2020; Frantar & Alistarh, 2022; Shen et al., 2022). Tuy nhiên, có những khác biệt kỹ thuật đáng kể: thứ nhất, các số liệu lỗi và mục tiêu tăng tốc khác nhau trong trường hợp nén trọng số; thứ hai, chúng tôi thực hiện trực tuyến, tại thời điểm huấn luyện, có nghĩa là thuật toán của chúng tôi phải cực kỳ hiệu quả, và thích ứng với đầu vào động.

3 THIẾT LẬP BÀI TOÁN
Mục tiêu. Giả sử chúng ta được cho một mô hình DNN M với L tầng và một kỹ thuật nén, chúng ta sẽ muốn tìm một lựa chọn tham số nén cℓ, một cho mỗi tầng ℓ ∈ {1,2, . . . , L} mà sẽ giảm thiểu một số liệu đại diện cho thiệt hại của chất lượng huấn luyện được giới thiệu bởi nén trong khi giảm thiểu tổng số bit được truyền. Tuy nhiên, mô tả trực quan này để lại một loạt chi tiết, như 1) khái niệm về số liệu theo tầng tương ứng với hiệu ứng nén cho một tập tham số nhất định; 2) công thức bài toán chính xác, ràng buộc hiệu ứng nén hoặc tỷ lệ nén; và 3) một triển khai hiệu quả của thuật toán như vậy. Chúng tôi giải quyết những câu hỏi này tiếp theo.

Số liệu. Việc chọn số liệu độ nhạy đúng là chìa khóa để khôi phục độ chính xác. Cho việc này, chúng tôi đã thử một số phương pháp. Đầu tiên, chúng tôi lưu ý rằng độ nhạy của một tầng đối với nén gradient có thể được đo bằng phản ứng mất mát mô hình đối với nén. Để đánh giá số liệu, chúng tôi thiết lập thí nghiệm sau. Chúng tôi lưu các checkpoint mô hình ở các giai đoạn khác nhau của huấn luyện không nén. Sau đó tiến hành nhiều lần chạy huấn luyện ngắn (lên đến 50 bước) với cùng dữ liệu bắt đầu từ checkpoint nhưng thay đổi tham số nén gradient. Sau đó, chúng tôi sử dụng sự khác biệt mất mát có và không có nén như số liệu.

Bây giờ, câu hỏi là chúng ta muốn sử dụng tham số nén nào (vector có kích thước bằng số tầng được nén) để đánh giá số liệu của mỗi tầng. Vì chúng ta muốn thấy phản ứng mô hình đối với nén tầng riêng lẻ, đối với mỗi tầng chúng tôi thay đổi tham số nén để lại gradient của tất cả các tầng khác không bị thay đổi. Với điều đó, chúng tôi thu thập sự khác biệt mất mát cho mỗi tầng và tham số nén và sử dụng chúng như một số liệu. Phương pháp khác dựa trên các công trình lý thuyết trước đó, đề xuất rằng lỗi ℓ2 bình phương của nén là một thước đo tốt về tác động hội tụ của kỹ thuật nén. Ở đây, chúng tôi tập hợp gradient trong quá trình huấn luyện, sau đó nén gradient đã tập hợp cho mỗi tham số nén cho mỗi tầng riêng lẻ và sử dụng độ lớn của lỗi như một số liệu.

Như được chỉ ra trong Phần 5.4, hai phương pháp có tương quan mạnh và các tham số tối ưu kết quả gần nhau. Tuy nhiên, phương pháp dựa trên mất mát không áp dụng được trong thế giới thực vì nó yêu cầu đánh giá ngoài huấn luyện làm thay đổi quy trình huấn luyện gốc và mất thời gian tương đương với thời gian huấn luyện gốc. Đồng thời, phương pháp dựa trên lỗi có thể được sử dụng trong quá trình huấn luyện dễ dàng tích hợp vào huấn luyện và có chi phí thời gian không đáng kể. Xem xét điều này, trong suốt bài báo chúng tôi sử dụng chuẩn L2 của lỗi nén như số liệu độ nhạy tầng chính.

Bài toán tối ưu có ràng buộc. Chúng tôi hình thức hóa bài toán tối ưu của mình như sau. Cho một mô hình M với L tầng ℓ ∈ {1,2, . . . , L} và một kỹ thuật nén, cung cấp một tập lựa chọn nén C={c1, c2, . . . ck} cho mỗi tầng. Chúng tôi nhấn mạnh rằng, để đơn giản, chúng tôi xem xét một kỹ thuật nén duy nhất và cùng lựa chọn/mức nén cho mỗi tầng, nhưng phương pháp của chúng tôi cũng sẽ hoạt động cho các kỹ thuật khác nhau được áp dụng cho cùng mô hình, và lựa chọn nén không đồng nhất.

Trong bối cảnh này, phương pháp của chúng tôi nhận như đầu vào một hàm lỗi error(ℓ, cj) cung cấp chuẩn L2 của lỗi nén tại tầng ℓ cho lựa chọn nén cj, và một hàm size(ℓ, cℓ) đo chi phí truyền của tầng ℓ cho lựa chọn cℓ. Ngoài ra, chúng tôi giả định được cho một ngưỡng lỗi tối đa cố định Emax mà thuật toán không nên vi phạm. Sau đó, chúng tôi muốn tìm một cài đặt theo tầng của tham số nén c1, . . . , cL với mục tiêu

minimize ∑(L,ℓ=1) size(ℓ, cℓ) s.t. ∑(L,ℓ=1) error(ℓ, cℓ) ≤ Emax.

Về mặt thực tế, công thức này nhằm giảm thiểu tổng chi phí truyền cho các tensor gradient dưới ràng buộc cộng tối đa về lỗi nén gradient. Một giả định ngầm là số liệu error(·,·) là cộng trên các tầng, và có thể có được ràng buộc lỗi "tham chiếu" mà không dẫn đến mất độ chính xác. Như chúng ta thấy tiếp theo, đây là trường hợp đối với số liệu lỗi chúng tôi áp dụng.

Ràng buộc lỗi. Câu hỏi còn lại là cách chọn Emax. Chúng tôi quyết định chọn ràng buộc lỗi này để theo dõi của một phương pháp nén tham chiếu được biết là không mất độ chính xác so với baseline. Ở đây, chúng tôi tận dụng thực tế rằng tài liệu cung cấp các tham số cho phép đạt khôi phục độ chính xác đầy đủ cho các mô hình và tập dữ liệu khác nhau. Ví dụ, đối với lượng tử hóa, chúng ta có thể sử dụng lượng tử hóa 4-bit, được biết là khôi phục cho hầu như mọi mô hình (Alistarh et al., 2017; Markov et al., 2022). Đối với thưa hóa (Lin et al., 2017; Renggli et al., 2019) cũng như phân rã ma trận (Vogels et al., 2019) chúng tôi phải sử dụng các tham số tham chiếu khác nhau cho huấn luyện khác nhau theo baseline của chúng (Để biết chi tiết tham khảo Bảng 2 và 3).

4 KHUNG L-GRECO
Tổng quan. Bây giờ chúng tôi mô tả một thuật toán tổng quát để giải quyết bài toán tối ưu có ràng buộc từ phần trước. Thuật toán của chúng tôi đưa ra quyết định theo tầng để cân bằng độ lớn của lỗi nén và kích thước nén của mô hình. Như đầu vào, thuật toán của chúng tôi nhận kích thước tầng không nén size(ℓ,⊥), một tập G gradient tích lũy mỗi tầng (sẽ được sử dụng để kiểm tra lỗi nén), cũng như ràng buộc lỗi cố định Emax. Cụ thể, tại một bước quyết định nhất định, mục tiêu là tìm một ánh xạ tối ưu của mỗi tầng ℓ đến một mức nén

--- TRANG 3 ---
cℓ, sao cho chuẩn của tổng lỗi nén, được tính trên tập gradient tích lũy G không vượt quá Emax, nhưng tổng kích thước nén của mô hình ∑(L,ℓ=1) size(ℓ, cℓ) là tối thiểu cho ràng buộc lỗi này.

Công thức này gợi nhớ đến bài toán knapsack: lỗi là kích thước của knapsack, và kích thước nén là giá trị chúng ta muốn tối ưu. Trong công thức này, bài toán sẽ có một thuật toán hiệu quả và tối ưu, sử dụng lập trình động (DP). Tuy nhiên, lỗi L2 bình phương không rời rạc, vì vậy chúng ta không thể trực tiếp áp dụng phương pháp này. Thay vào đó, ý tưởng là giảm điều này thành một bài toán có thể giải được bằng cách rời rạc hóa tập giá trị lỗi có thể có. Cụ thể, vì lỗi là đơn điệu và chúng ta có thể sử dụng rời rạc hóa rất mịn mà không mất hiệu quả đáng kể, rất khó có khả năng chúng ta sẽ bỏ lỡ giải pháp tối ưu một lượng đáng kể. Để minh họa, trong triển khai của chúng tôi, chúng tôi sử dụng D = 10000 như một yếu tố rời rạc hóa (tức là các bước có kích thước Emax/D).

Thuật toán. Thuật toán, được trình bày đầy đủ trong Thuật toán 1, hoạt động như sau. Đầu tiên, chúng ta tính toán dữ liệu cần thiết cho thuật toán cho tất cả các tầng và tất cả các tham số nén được xem xét (dòng 1-10), tương ứng với lỗi và nén cho mỗi lựa chọn có thể. Sau đó chúng ta thực hiện một thuật toán lập trình động giải quyết bài toán sau. Chúng ta muốn tính toán kích thước tổng tối thiểu cho tổng lỗi nén E trong ℓ tầng đầu tiên

compressedsize(ℓ, E) = min ℓcompressedsize(ℓ−1, E− error(ℓ, cℓ)) +size(ℓ, cℓ).

Để đạt được điều này, đối với mỗi tầng chúng ta muốn xem xét, chúng ta chạy qua tất cả các gia tăng lỗi và tất cả các tham số nén có thể, và giảm thiểu tổng kích thước nén cho tổng lỗi nén hiện tại (dòng 12-22) lưu tham số nén mà chúng ta đạt được tối thiểu. Sau đó, trong dòng 23-27, chúng ta tìm gia tăng lỗi mà chúng ta đạt được tổng kích thước nén thấp nhất và tái tạo ánh xạ tham số nén—như vậy, chúng ta đã có được kết quả.

Triển khai. Như được hiển thị trong Hình 2, chúng tôi tích hợp L-GreCo ở giữa mã huấn luyện người dùng và hệ thống truyền thông chịu trách nhiệm nén gradient và đồng bộ hóa của chúng. Chúng tôi chạy thuật toán trên theo định kỳ, ví dụ mỗi epoch huấn luyện một lần, trên một worker được chỉ định duy nhất; trừ khi được nêu khác, worker này thực hiện tất cả các bước. Giữa các lần chạy thuật toán, chúng tôi tích lũy gradient mỗi tầng trong các bộ đệm phụ trợ. Sau đó chúng tôi xây dựng một bảng lỗi L2, cho mỗi tầng, và cho mọi tham số nén trong phạm vi do người dùng cung cấp và cho tập tham số nén tham chiếu. Để tìm lỗi, chúng tôi mô phỏng nén/giải nén của mỗi tầng với tham số nén cho trước mà không áp dụng phản hồi lỗi và tính toán khoảng cách L2 giữa vector gốc và được khôi phục. Sau đó chúng tôi chạy thuật toán DP. Điều này cung cấp cho chúng tôi ánh xạ nén tối ưu,

Thuật toán 1 Nén thích ứng L-GreCo
Đầu vào: Các tầng mô hình Li, gradient tích lũy Gi, tham số nén có thể C={c1, c2, . . . ck}, tham số nén mặc định tĩnh chúng ta cố gắng cải thiện Cd_i, yếu tố rời rạc hóa D
Đầu ra: Gán tham số nén cℓ ∈ C cho mỗi tầng ℓ

1: N = số tầng
2: Tính Emax cho tham số nén mặc định Cd_i
3: Tính bước rời rạc hóa Emax/D.
4: Ma trận Chi phí N × |C| trong đó vị trí i, j có giá trị kích thước của tầng i được nén với tham số nén cj.
5: Ma trận Lỗi N × |C| trong đó vị trí i, j có giá trị L2 rời rạc hóa của lỗi nén khi gradient tích lũy của tầng i được nén với tham số cj.
6: Ma trận DP N×(D+ 1) được điền với giá trị ∞.
7: Ma trận PD N×(D+ 1).
8: // Khởi tạo bảng chi phí:
9: for c ∈ C do
10: DP[1][Errors[1][c]] = Costs[1][c]
11: PD[1][Errors[1][c]] = c
12: end for
13: // Thuật toán lập trình động
14: for Tầng li := 2..N do
15: for ci ∈ C do
16: for ei := Errors[li][ci]..D do
17: t = DP[li−1][ei−Errors[li][ci]] + Costs[li][ci]
18: if t < DP[li][ei] then
19: DP[li][ei] = t
20: PD[li][ei] = ci
21: end if
22: end for
23: end for
24: end for
25: errmin = argmin(DP[N])
26: // Tái tạo tham số tối ưu
27: for li = N..1 do
28: result[li] = PD[li][errmin]
29: errmin = errmin − Errors[li][result[li]]
30: end for
31: return result

mà worker được chỉ định phát sóng đến các worker khác. Sau đó, trên mỗi worker, chúng tôi lưu ánh xạ tham số nén trong engine truyền thông.

Chi phí tính toán và bộ nhớ. Thuật toán giả định rằng chúng ta tích lũy gradient trong các bộ đệm bổ sung, chiếm bộ nhớ kích thước mô hình. Thuật toán DP có độ phức tạp thời gian O(D|L||C|) và độ phức tạp bộ nhớ O(|L|D). Thời gian thực tế cho thuật toán được trình bày trong Bảng 1. Chi phí bao gồm hai phần: 1. Tính toán lỗi 2. Thuật toán lập trình động. Chúng ta có thể thấy rằng phần lập trình động chỉ mất một phần nhỏ phần trăm thời gian huấn luyện trong khi hầu hết chi phí được gây ra bởi tính toán. Tuy nhiên, cả hai chi phí đều không đáng kể so với tăng tốc được cung cấp bởi L-GreCo (xem Hình 4 và 5).

Chi tiết truyền thông. Từ quan điểm triển khai

--- TRANG 4 ---
song song dữ liệu phổ biến, gradient trở nên có sẵn ngay sau khi kết thúc lan truyền ngược của tầng tương ứng. Để giảm độ trễ truyền thông, gradient được nhóm thành một số bộ đệm (được gọi là bucket trong pytorch). Một tối ưu hóa quan trọng là việc chồng chéo truyền thông gradient với tính toán. Điều này ngụ ý rằng chi phí truyền thông của các bucket đầu tiên có khả năng bị "ẩn" hoàn toàn bởi tính toán, trong khi đồng bộ hóa bucket cuối cùng trở thành một phần đáng kể của độ trễ thời gian giữa các bước huấn luyện (xem Hình 10(b)). Điều này có nghĩa là tốc độ truyền thông của các phần khác nhau của mô hình (tức là nhóm tầng) có tác động khác nhau đến tốc độ huấn luyện. Do đó, tối ưu hóa tỷ lệ nén có thể đưa ra kết quả không tối ưu.

Tối ưu hóa thời gian. Với điều đó trong tâm trí, chúng tôi đã tích hợp một tiện ích đo thời gian mỗi bucket cần để đồng bộ hóa. Với công cụ này, chúng ta có thể tái công thức hóa một bài toán tối ưu: thực tế, chúng ta muốn giảm thiểu khoảng thời gian giữa bắt đầu đồng bộ hóa bucket đầu tiên và kết thúc đồng bộ hóa bucket cuối cùng (tức là thời gian đồng bộ hóa gradient) thay vì tỷ lệ nén. Cho điều đó, chúng tôi đo thời gian đồng bộ hóa gradient cho các kết hợp khác nhau của tỷ lệ nén mỗi bucket, lưu kích thước bucket được truyền. Sau đó, chúng tôi huấn luyện một mô hình hồi quy tuyến tính để học mối quan hệ giữa kích thước bucket được truyền và thời gian đồng bộ hóa gradient. Từ đó chúng tôi thu được các hệ số mỗi bucket T(b) mà chúng ta có thể áp dụng cho mỗi tầng trong bucket tương ứng. Sau đó chúng ta thay đổi mục tiêu trong bài toán tối ưu (xem Công thức 3) thành:

minimize ∑(L,ℓ=1) size(ℓ, cℓ) ∗ T(ℓ) s.t. ∑(L,ℓ=1) error(ℓ, cℓ) ≤ Emax.

Điều này cung cấp cho chúng ta các tham số có đặc tính nén giống hoặc tệ hơn thuật toán dựa trên tỷ lệ nhưng cải thiện thời gian truyền thông ước tính.

5 XÁC NHẬN THỰC NGHIỆM
Chúng tôi đánh giá thực nghiệm L-GreCo trên tất cả các chiến lược nén hiện có: lượng tử hóa sử dụng QSGD, thưa hóa TopK, và xấp xỉ hạng thấp thông qua PowerSGD.

5.1 Thiết lập thí nghiệm
Cơ sở hạ tầng. Đánh giá của chúng tôi sử dụng các workstation phổ thông với 4 hoặc 8 GPU NVIDIA RTX3090. Trong cài đặt đa node, chúng tôi sử dụng 4 instance cloud với 4xRTX3090 GPU, được cung cấp bởi Genesis Cloud. Đo băng thông cho thấy các giá trị băng thông inter-GPU nằm trong khoảng 13 đến 16 Gbps, và băng thông inter-node trong cloud lên đến 10 Gbps. Chúng tôi sử dụng Pytorch 1.10, openmpi/4.1.4, CUDA 11.3, NCCL 2.8.4, và cudnn/8.1.1.

Triển khai. Chúng tôi triển khai L-GreCo trong PyTorch sử dụng hook torch.distributed cho PowerSGD và tận dụng khung CGX mã nguồn mở (Markov et al., 2022) cho lượng tử hóa và thưa hóa.

Tập dữ liệu và mô hình. Chúng tôi kiểm tra hai nhiệm vụ học DNN khác nhau: 1) phân loại hình ảnh trên tập dữ liệu CIFAR100 (Krizhevsky, 2009) và ImageNet (Deng et al., 2009), và 2) mô hình hóa ngôn ngữ trên WikiText-103 (Merity et al., 2016). Chúng tôi sử dụng các triển khai mô hình và tham số tiên tiến được cung cấp bởi phiên bản PyTorch của benchmark NVIDIA Training Examples (Nvidia, 2020) và thư viện fairseq ví dụ PyTorch (Ott et al., 2019). Chúng tôi sử dụng ResNet-18 cho huấn luyện CIFAR-100 với kích thước batch 256, ResNet-50 trong chế độ độ chính xác hỗn hợp cho ImageNet với kích thước batch 2048, và Transformer-XL và Transformer-LM được huấn luyện ở độ chính xác đầy đủ cho WikiText-103, với kích thước batch tương ứng 256 và 2048. Tất cả các thí nghiệm của chúng tôi sử dụng các công thức huấn luyện gốc, mà không có bất kỳ điều chỉnh siêu tham số nào để tính đến huấn luyện nén gradient.

Baseline. Baseline tự nhiên đầu tiên là huấn luyện không nén, đặt baseline độ chính xác của chúng tôi. Khớp với MLPerf (Mattson et al., 2020), chúng tôi đặt ngưỡng độ chính xác của mình là 1% tương đối so với huấn luyện không nén. Baseline tự nhiên thứ hai là các công thức nén gradient được tạo thủ công tốt nhất hiện có. Nói chung, các phương pháp hiện có đề xuất nén đồng nhất mỗi tầng đến một ngưỡng nhất định, ví dụ (Alistarh et al., 2017; Wen et al., 2017; Renggli et al., 2019; Vogels et al., 2019). Đối với những baseline như vậy, chúng tôi muốn cải thiện kích thước nén và tốc độ huấn luyện, có thể cũng cải thiện độ chính xác mô hình cuối cùng. Chúng tôi thấy rằng lựa chọn tốt nhất của tham số nén cho gán đồng nhất mỗi tầng phụ thuộc vào phương pháp nén, tập dữ liệu và nhiệm vụ. Đối với một số thí nghiệm, chúng tôi phải điều chỉnh tham số nén đồng nhất để khớp kết quả baseline (không nén) (xem Bảng 2 và 3).

Phạm vi tham số. L-GreCo yêu cầu một phạm vi tham số nén có thể như đầu vào. Chúng tôi luôn chọn phạm vi này để bao gồm tham số nén mặc định được sử dụng trong tài liệu. Hơn nữa, chúng tôi để lại khoảng cách giữa tham số mặc định và tham số nén cao nhất có thể

--- TRANG 5 ---
(giới hạn phạm vi bên phải) — nếu không, chúng ta bị hạn chế khớp lỗi L2; và một khoảng cách giữa tham số mặc định và tham số nén thấp nhất có thể (giới hạn phạm vi bên trái) — nếu không, chúng ta sẽ không cải thiện nén.

Phạm vi phụ thuộc vào phương pháp nén. Phương pháp lượng tử hóa và hạng thấp có số lượng tham số rời rạc hạn chế (số bit mỗi giá trị và hạng phân rã tương ứng) trong khi phạm vi cho thưa hóa lớn hơn. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng phương pháp đơn giản sau. Giả sử một tham số nén đồng nhất mặc định D, ví dụ 4 bit lượng tử hóa. Đối với phương pháp lượng tử hóa và hạng thấp, không gian tìm kiếm được định nghĩa là [D/2,2∗D] với bước gia tăng là 1. Đối với thưa hóa, chúng tôi chọn [D/10,10∗D] với gia tăng D/10.

Hai tham số đầu vào khác của L-GreCo là tần suất chạy thuật toán và thời gian khởi động sau đó nén được bật. Tham số đầu tiên thường được chọn bằng chu kỳ đánh giá từ các công thức huấn luyện (thường là 1 epoch). Như chúng ta sẽ thấy (Hình 9a) tỷ lệ nén của các sơ đồ được trả về bởi L-GreCo tương đối ổn định, vì vậy nó không cần điều chỉnh lại thường xuyên. Trong các thí nghiệm của chúng tôi, thời gian khởi động bằng thời gian khởi động tốc độ học mặc định.

5.2 Kết quả đánh giá
Khôi phục độ chính xác. Trước tiên chúng tôi kiểm tra độ chính xác mô hình sử dụng các công thức tiêu chuẩn cho huấn luyện đầu cuối. Đối với mỗi thí nghiệm, chúng tôi thực hiện 3 lần chạy với các seed khác nhau. Chúng tôi so sánh khôi phục độ chính xác L-GreCo với không nén (baseline) và tham số nén đồng nhất mỗi tầng tốt nhất (đồng nhất). Kết quả được trình bày trong Bảng 2 và 3. Tất cả độ chính xác và perplexity được trình bày với biến thiên seed. Tỷ lệ nén đại diện cho tiết kiệm chi phí truyền thực tế so với baseline không nén. Đối với mỗi phương pháp nén và nhiệm vụ huấn luyện, chúng tôi lấy tham số mặc định cung cấp tỷ lệ nén cao nhất trong khi khôi phục độ chính xác mô hình cuối cùng, tức là cải thiện nén thêm với cài đặt đồng nhất chỉ dẫn đến hội tụ tệ hơn.

Nhìn chung, kết quả cho thấy L-GreCo nằm trong giới hạn khôi phục độ chính xác 1% lỗi nhân (Mattson et al., 2020) cho hầu hết các nhiệm vụ, thường rất gần với baseline đồng nhất, trong khi liên tục tăng tỷ lệ nén, trên tất cả các nhiệm vụ và kỹ thuật nén được xem xét. Chúng tôi nhấn mạnh rằng chúng tôi không thực hiện điều chỉnh tham số cụ thể nhiệm vụ. Lợi ích đặc biệt cao cho các kỹ thuật thưa hóa và hạng thấp, nơi không gian tìm kiếm và do đó cũng là tiềm năng tiết kiệm của L-GreCo cao hơn. Ví dụ, đối với Transformer-LM, chúng tôi đạt được lên đến 5x nén cao hơn so với baseline đồng nhất, với tác động độ chính xác không đáng kể. Đồng thời, chúng tôi lưu ý rằng L-GreCo gây ra mất mát nhân >1% đối với lượng tử hóa và nén hạng thấp cho mô hình Transformer-LM rất nhạy cảm. Điều này là do phạm vi nén mặc định của chúng tôi quá tích cực trong trường hợp này; điều này có thể dễ dàng được giải quyết bằng cách điều chỉnh phạm vi — chúng tôi chọn không làm điều đó để tính nhất quán.

Để khám phá thêm sự đánh đổi độ chính xác-nén, chúng tôi thay đổi tham số nén đồng nhất mặc định, cụ thể điều chỉnh hạng powerSGD mục tiêu cho nhiệm vụ Transformer-XL/WikiText-103. Hình 3 cho thấy L-GreCo cung cấp sự đánh đổi tốt hơn đáng kể so với nén đồng nhất. Chúng ta có thể thấy L-GreCo cung cấp khôi phục perplexity tốt hơn trong khi cải thiện tốc độ huấn luyện của mô hình.

Profiling. Để khám phá chi phí nén, chúng tôi chạy profiling huấn luyện. Kết quả được trình bày trong Hình 8. Ở đó chúng tôi so sánh thời gian hoạt động cho huấn luyện gốc (không nén) và huấn luyện nơi gradient được nén với PowerSGD, hạng 32. Chúng ta có thể thấy rằng nén tương đối đắt (PowerSGD tốn thời gian hơn QSGD và TopK được tối ưu) mất ít hơn 10% thời gian bước.

Kết quả tăng tốc. Đối với cải thiện tăng tốc huấn luyện đầu cuối, chúng tôi sử dụng siêu tham số huấn luyện không nén tiêu chuẩn cho ResNet và Transformer. Chúng tôi xem xét weak scaling, tức là tăng kích thước batch toàn cục trong khi tăng số node. (Cải thiện hiệu suất cao hơn cho strong scaling.) Chúng tôi bắt đầu bằng cách kiểm tra kết quả thông lượng huấn luyện cho các thí nghiệm đa node trong Hình 4 và 5, được thực hiện trong môi trường cloud. Cài đặt này gặp phải tắc nghẽn băng thông ngay cả ở số node thấp hơn, điều này rõ ràng cho hiệu suất kém của baseline không nén cho cả hai mô hình. Nén đồng nhất được điều chỉnh loại bỏ tắc nghẽn này ở mức độ đáng kể: ví dụ, PowerSGD/ResNet50 đồng nhất có thể đạt 75% scaling lý tưởng trên 4 node.

Do đó khá ngạc nhiên rằng nén không đồng nhất tự động vẫn có thể cung cấp cải thiện đáng kể trong cài đặt này: so với nén đồng nhất, L-GreCo mang lại cải thiện thông lượng lên đến 1.45x cho ResNet50, và lên đến 2.4x tăng tốc trên Transformer-XL. Điều này cho thấy nén không đồng nhất có thể là một chiến lược hiệu quả trong tình huống này, đặc biệt đối với các mô hình không đồng nhất tầng như Transformer.

Tiếp theo chúng tôi kiểm tra kết quả cho scaling đơn node từ 1 đến 8 GPU, được trình bày trong Hình 6 và 7. Đây là tình huống thách thức hơn cho nén truyền thông vì băng thông ít bị tắc nghẽn hơn trong cài đặt đơn node. Chúng tôi bắt đầu bằng cách kiểm tra kết quả cho mô hình Transformer-XL. Đối với PowerSGD và TopK, L-GreCo dẫn đến lợi ích lên đến 25% tăng tốc đầu cuối so với đồng nhất, với sự khác biệt độ chính xác không đáng kể. Đối với QSGD, không gian tìm kiếm rất hạn chế: đồng nhất đã sử dụng 4 bit, và cung cấp scaling rất tốt. Phương pháp thích ứng của chúng tôi vẫn cung cấp 2% tăng tốc so với nén đồng nhất được điều chỉnh tốt của chúng tôi, và 50% tăng tốc so với huấn luyện không nén, đạt ≥90% scaling lý tưởng. Ở đây, chúng tôi quan sát rằng, đối với ResNet50, PowerSGD và QSGD có thể khôi phục độ chính xác ResNet50 với tham số đồng nhất gần với các tham số tối ưu lý thuyết (ví dụ hạng-4 cho PowerSGD, trong khi tối thiểu là hạng-1), để lại ít chỗ cho cải thiện cho các cơ chế thích ứng. Tuy nhiên, L-GreCo cung cấp cải thiện nhất quán trong trường hợp mô hình Transformer-XL không đồng nhất (Hình 6).

Nhìn chung, chúng tôi lưu ý rằng L-GreCo cung cấp cải thiện hiệu suất có ý nghĩa thống kê so với nén đồng nhất tĩnh (đặc biệt cho các mô hình không đồng nhất) khi áp dụng cho tất cả các phương pháp nén được xem xét, với tác động không đáng kể đến độ chính xác.

--- TRANG 6 ---
5.3 So sánh với các phương pháp thích ứng khác
Cho đến nay, chúng tôi đã sử dụng nén đồng nhất làm baseline. Bây giờ chúng tôi so sánh L-GreCo với các công trình trước đây về việc chọn tham số nén một cách thích ứng. Chúng tôi xem xét các phương pháp Accordion (Agarwal et al., 2021b) và CGX (Markov et al., 2022), vì chúng gần nhất về phạm vi và tổng quát nhất về các phương pháp nén áp dụng được. Chúng tôi thực hiện so sánh trên mô hình Transformer-XL trên WikiText-103, vì 1) đây là mô hình nhạy cảm với nén gradient, 2) có tính không đồng nhất cao của các tầng, và 3) gặp phải tắc nghẽn băng thông (xem Hình 4). PowerSGD được chọn làm phương pháp nén, vì Accordion tối ưu hóa cụ thể cho nó, và phương pháp này có thể dẫn đến lợi ích nén thực tế cao nhất. Chúng tôi chạy các thí nghiệm trong hai cài đặt phân tán: đơn node với 8 GPU và đa node bao gồm 4 máy với 4 GPU mỗi máy. Chúng tôi so sánh nén và thông lượng (mẫu được xử lý mỗi giây) cho mỗi phương pháp.

Trong các thí nghiệm trong phần này, chúng tôi khác với phương pháp của mình đối với việc chọn phạm vi tìm kiếm của tham số nén cho thuật toán L-GreCo, được mô tả trong Phần 5.1. Ở đây, chúng tôi nhằm có được tăng tốc nhiều nhất mà không mất độ chính xác mô hình cuối cùng. Do đó, đối với mỗi thuật toán chúng tôi xem xét trong phần này, chúng tôi điều chỉnh tham số nén cho phương pháp nén đã chọn và nhiệm vụ huấn luyện (mà không thay đổi siêu tham số huấn luyện, ví dụ tốc độ học, weight decay, v.v.) để chúng tôi có được kết quả thời gian tốt nhất với độ chính xác mô hình cuối cùng trong giới hạn cho phép của MLPerf. Phạm vi tham số tốt nhất cho L-GreCo hóa ra là hạng [8,64] (xem Bảng 4) với hạng mặc định 32.

Bảng 4. So sánh L-GreCo với các thuật toán thích ứng khác trên Transformer-XL sử dụng PowerSGD.

[THIS IS TABLE:
Thuật toán thích ứng | Phạm vi tham số | Tỷ lệ | Đơn node Tokens/s | Đa node Tokens/s
Đồng nhất | 32 | 14.1 | 110k | 72k
L-GreCo | 8 - 64 | 23.5 | 144k | 150k
Accordion | 16, 64 | 23.9 | 114k | 107k
CGX, kmeans | 8 - 64 | 21.6 | 124k | 112k
L-GreCo + Accordion | 8 - 128 | 36.9 | 138k | 176k]

So sánh Global TopK. Bài toán tối ưu - giảm thiểu độ lớn lỗi gradient cho tỷ lệ nén mong muốn - có thể được giải quyết bằng global topK trong trường hợp thưa hóa gradient. Thực vậy, global topK được áp dụng cho gradient của toàn bộ mô hình nên nó chọn các phần tử có độ lớn cao nhất, giảm thiểu tổng

--- TRANG 7 ---
lỗi. Tuy nhiên, phương pháp này có một số nhược điểm. Đầu tiên, global topK yêu cầu điều chỉnh và tìm kiếm siêu tham số thích hợp để hội tụ khi sử dụng mật độ thấp. L-GreCo lại không cố gắng giảm thiểu lỗi nén toàn cục – nó cố gắng khớp nó với lỗi nén của nén theo tầng đồng nhất mà khôi phục độ chính xác. Ngoài ra, L-GreCo có tham số nén thấp nhất có thể. Điều này có nghĩa là mỗi tầng có đóng góp của nó trong đồng bộ hóa gradient. Điều này có thể không phải là trường hợp đối với global topK - ở mật độ cao một số tầng có thể bị bỏ qua cập nhật cho một số bước tối ưu. Điều này có tác động xấu đến chất lượng mô hình cuối cùng. Nhược điểm thứ hai của global topK là tăng tốc thực tế mà nó có thể cung cấp. Như chúng tôi đã thảo luận trong Phần 4, trong các khung DataParallel hiện đại, gradient được đồng bộ hóa song song với tính toán vì hiệu quả - người ta có thể ẩn chi phí truyền thông sau tính toán. Tuy nhiên, trong trường hợp global topK, vì phải chờ đến khi tất cả gradient sẵn sàng, sau đó thực hiện nén và truyền thông. Có thể hóa ra trong trường hợp này chúng ta có nhiều truyền thông không chồng chéo hơn trong nén theo tầng hoặc thậm chí trong trường hợp không nén (xem Hình 10).

Để xác nhận nghi ngờ của chúng tôi về global topK, chúng tôi đã triển khai thuật toán sử dụng hook torch.distributed và chạy huấn luyện RN18/CIFAR100. Chúng tôi phát hiện ra rằng global topK chậm hơn L-GreCo 10% khi áp dụng với mật độ toàn cục tương tự trong trường hợp này - 0.25%.

So sánh Accordion. Accordion thích ứng nén bằng cách phát hiện các chế độ quan trọng trong quá trình huấn luyện. Nó chấp nhận hai chế độ nén có thể (tương ứng với nén thấp và cao), và có tham số ngưỡng lỗi η. Nó thu thập gradient, và định kỳ quyết định tham số sử dụng dựa trên thông tin gradient cho mỗi tầng. Chúng tôi đã triển khai Accordion sử dụng hook torch.distributed, được sử dụng cho PowerSGD. Đối với tham số η, chúng tôi chọn giá trị 0.5 được đề xuất bởi các tác giả và cố gắng điều chỉnh thủ công cặp tham số nén thấp và cao tốt nhất cho mỗi mô hình, mà huấn luyện hội tụ đến độ chính xác trong giới hạn MLPerf. Chúng tôi chạy thuật toán này trên Transformer-XL/Wikitext-103, và thấy rằng cặp tham số tốt nhất (về thời gian huấn luyện mà không mất độ chính xác) là hạng nén cao 16, và hạng nén thấp 64.

Trong Hình 9, người ta có thể thấy động lực của tỷ lệ nén trung bình trong quá trình huấn luyện của Accordion và L-GreCo. Chúng tôi lưu ý rằng Accordion chọn hạng nén thấp cho hầu như tất cả các tầng trong giai đoạn đầu của huấn luyện và hạng nén cao cho phần còn lại của thời gian huấn luyện, dẫn đến nén đồng nhất hoàn toàn lưỡng cực. Điều này cho thấy Accordion có thể không thực sự khai thác bản chất không đồng nhất của mô hình DNN. Do đó, các tối ưu hóa của Accordion và L-GreCo, tương ứng, có thể được xem là trực giao: Accordion tập trung vào việc thay đổi lượng nén trung bình trong quá trình huấn luyện, trong khi L-GreCo tìm cách tối ưu để đạt mức trung bình này bằng cách đặt mục tiêu theo tầng.

Với điều này trong tâm trí, chúng tôi kết hợp hai thuật toán này, như sau. Trước tiên chúng tôi thực hiện Accordion để có được tham số được đề xuất cho mỗi tầng và sử dụng các tham số này như tập tham số mặc định trong thuật toán L-GreCo. Tập tham số mặc định được sử dụng để định nghĩa lỗi tối đa của thuật toán DP (xem dòng 2 trong Thuật toán 1). Trong phương pháp này, Accordion xác định độ nhạy mô hình đối với nén gradient tại các điểm khác nhau trong huấn luyện, trong khi L-GreCo tìm ánh xạ tốt nhất của tham số nén mỗi tầng. Trong Hình 9, chúng ta thấy rằng sự kết hợp kết quả (L-GreCo với phạm vi [8,128] và Accordion với high=16, low=64) cung cấp kết quả vượt trội về tỷ lệ nén, mà không hy sinh độ chính xác.

Chúng tôi cũng so sánh hiệu suất của hai thuật toán riêng lẻ (xem Bảng 4). Chúng tôi quan sát rằng, mặc dù thực tế là tỷ lệ nén lý thuyết được đề xuất bởi Accordion về cơ bản giống với L-GreCo, thông lượng Accordion ít hơn khoảng 30%. Điều này được giải thích bởi thực tế rằng L-GreCo nén các tầng được truyền cuối cùng (bucket) đến mức cao hơn, dẫn đến cải thiện đáng kể thời gian truyền tổng. Cụ thể, trong Hình 9b, chúng tôi quan sát rằng L-GreCo truyền ít phần tử gấp đôi trong bucket cuối cùng so với Accordion. Hơn nữa, việc kết hợp L-GreCo với Accordion

--- TRANG 8 ---
cải thiện tỷ lệ nén 50%, và thời gian huấn luyện lên đến 66% so với Accordion.

So sánh Rethink-GS. Sahu et al. (2021) đề xuất một phương pháp thưa hóa về mặt kỹ thuật thích ứng - thưa hóa hard-threshold thay đổi số phần tử được truyền dựa trên phân bố gradient. Chúng tôi đã chạy huấn luyện L-GreCo của ResNet18/CIFAR-100 trong thiết lập được mô tả trong bài báo. Chúng tôi sử dụng mật độ 1% như tham số mặc định cho L-GreCo và phạm vi tìm kiếm là [0.1%,10%]. Đối với Rethink-GS chúng tôi sử dụng tham số λ = 4.72×10^-3.

Kết quả chúng tôi thấy rằng L-GreCo cải thiện thuật toán Rethink-GS 17% về tỷ lệ nén (6.7× so với 5.7×) trong khi cũng cải thiện độ chính xác cuối cùng - 71.7% so với 71.4% (các số khác với những gì chúng tôi hiển thị trong Bảng 2 vì ở đây chúng tôi sử dụng thiết lập từ (Sahu et al., 2021)). Chúng tôi lưu ý rằng khung của chúng tôi không yêu cầu bất kỳ điều chỉnh siêu tham số nào cho thí nghiệm này, trong khi Rethink-GS yêu cầu điều chỉnh cẩn thận tham số hard-threshold λ.

So sánh CGX. Nén thích ứng của CGX dựa trên kmeans và ánh xạ các tầng vào không gian 2 chiều (kích thước tầng vs. lỗi L2). Thuật toán phân cụm các tầng thành một số nhóm và gán tham số nén được định nghĩa trước cho các tầng trong nhóm. Chúng tôi đã triển khai logic này với nén PowerSGD. Chúng tôi sử dụng hạng 32 làm mặc định, và phạm vi tốt nhất (về nén) là từ 8 đến 64, sử dụng 6 cụm tầng. Kết quả được hiển thị trong Bảng 4. Tóm lại, L-GreCo cải thiện so với phương pháp kmeans lên đến 33%. Trong Hình 9b, chúng tôi quan sát rằng L-GreCo chọn tham số sao cho bucket lớn nhất và cuối cùng được nén nhiều nhất, trong khi thuật toán kmeans chọn tham số nén tệ hơn cho những tầng đó trong một số lần lặp.

Nhìn chung, L-GreCo cải thiện nén thực tế so với các kỹ thuật trước đây. Đáng chú ý, tỷ lệ nén cao nhất được đạt bởi phương pháp lai Accordion + L-GreCo, tận dụng thông tin theo tầng về cả độ nhạy và động lực huấn luyện.

5.4 Số liệu độ chính xác dựa trên mất mát
Như chúng tôi đã thảo luận trong phần 3, có thể sử dụng số liệu khác nhau để đo độ nhạy tầng đối với nén. Cái chúng tôi sử dụng trong tất cả các thí nghiệm là độ lớn lỗi. Cái khác có thể là dựa trên mất mát. trong đó chúng tôi thu thập sự khác biệt mất mát mô hình giữa huấn luyện không nén và huấn luyện với nén gradient của các tầng nhất định trong khi các tầng khác giữ nguyên và sử dụng sự khác biệt mất mát như một số liệu độ nhạy.

So sánh hai phương pháp này, chúng tôi đánh giá hệ số tương quan của các số liệu mà hai phương pháp này cung cấp. Chúng ta có thể thấy trong hình 11a, các giá trị số liệu từ hai phương pháp có tương quan cao. Hơn nữa, chúng tôi phát hiện ra trong các thí nghiệm của mình rằng các tham số nén cũng như tỷ lệ nén tổng kết quả cho cả hai số liệu rất gần nhau. Do đó, cho thực tế rằng việc thu thập số liệu dựa trên mất mát yêu cầu các lần chạy ngoài huấn luyện bổ sung, việc sử dụng số liệu dựa trên lỗi của chúng tôi được biện minh.

5.5 Tối ưu hóa dựa trên thời gian
Xem xét thực tế rằng các bucket truyền thông có tác động khác nhau đến hiệu suất huấn luyện, người ta có thể lưu ý rằng tỷ lệ nén không luôn mang lại cải thiện tốc độ mong đợi. Với điều đó trong tâm trí, chúng tôi đã sửa đổi thuật toán L-GreCo để nó tối ưu hóa thời gian dự kiến của truyền thông thay vì tỷ lệ nén. Như chúng tôi đã giải thích trong Phần 4, trong phương pháp này chúng tôi đo thời gian truyền thông thay đổi tỷ lệ nén cho mỗi bucket được truyền và huấn luyện mô hình hồi quy tuyến tính, chạy huấn luyện trong 50 bước với 5 bước khởi động cho mỗi tập tham số nén. Với mô hình chúng tôi có được trọng số của các bucket tương ứng với mối quan hệ giữa số bit được truyền mỗi bucket và thời gian truyền thông. Sau đó chúng tôi sử dụng những hệ số đó trong số liệu chúng tôi cố gắng giảm thiểu trong Thuật toán 1.

Chúng tôi đã chạy thuật toán trên benchmark chính của chúng tôi - PowerSGD trong huấn luyện Transformer-XL/WikiText-103. Mô hình tuyến tính được xây dựng trên dữ liệu thời gian chúng tôi thu thập (5000 mẫu - tập tỷ lệ nén mỗi bucket) có điểm gần bằng 1 có nghĩa là chúng tôi đã quản lý để dự đoán thời gian truyền thông sử dụng kích thước bucket được truyền gần như hoàn hảo. Chúng tôi phát hiện ra rằng các hệ số mỗi bucket từ mô hình tuyến tính gần nhau (xem Hình 11b). Điều này có nghĩa là tác động của mỗi bucket đến thời gian truyền thông tỷ lệ với kích thước bucket. Chúng tôi lưu ý rằng các tham số chúng tôi nhận được với thuật toán sửa đổi rất gần với các tham số từ thuật toán L-GreCo gốc. Cho thực tế đó chúng tôi tìm ra rằng trong trường hợp Transformer-XL/WikiText-103, thuật toán gốc mang lại các tham số tối ưu nhất về thời gian.

6 KẾT LUẬN
Chúng tôi đề xuất L-GreCo, một thuật toán nén gradient thích ứng tự động xác định các tham số nén theo tầng tối ưu, cho một ràng buộc lỗi cố định. Thuật toán L-GreCo tìm ánh xạ tham số nén sao cho 1) tổng lỗi nén L2 khớp với một mục tiêu được biết là khôi phục độ chính xác, và 2) tổng kích thước nén là tối thiểu cho mục tiêu này.

Xác nhận thực nghiệm của chúng tôi trên tất cả các họ phương pháp nén gradient cho thấy huấn luyện với các tham số theo tầng được đề xuất bởi L-GreCo khôi phục độ chính xác baseline trong khi tỷ lệ nén gradient được tăng đáng kể. L-GreCo cải thiện hiệu suất huấn luyện lên đến 2.5×, tiết kiệm lên đến 5.2x truyền thông so với nén vanilla, và lên đến 122x so với huấn luyện không nén. Nhìn chung, công trình của chúng tôi cung cấp một phương pháp mới để cải thiện các phương pháp nén gradient hiện có, với chi phí gần như bằng không về thời gian và mất độ chính xác.

Phương pháp của chúng tôi cho thấy rằng nén gradient mỗi tầng có thể là một hướng thú vị cho công việc tiếp theo trong lĩnh vực này. Cụ thể, phương pháp của chúng tôi có thể được tăng cường với thông tin thời gian thực tế về chi phí truyền gradient tầng, có thể dẫn đến lợi ích thực tế thêm. Một phần mở rộng có thể khác sẽ xem xét các chiến lược lai, cho phép kết hợp các kỹ thuật nén khác nhau (ví dụ thưa hóa và hạng thấp) bên trong cùng mô hình.

Chúng tôi đề xuất L-GreCo, một thuật toán nén gradient thích ứng tự động xác định các tham số nén theo tầng tối ưu, cho một ràng buộc lỗi cố định. Thuật toán L-GreCo tìm ánh xạ tham số nén sao cho 1) tổng lỗi nén L2 khớp với một mục tiêu được biết là khôi phục độ chính xác, và 2) tổng kích thước nén là tối thiểu cho mục tiêu này.

Phương pháp của chúng tôi được bổ sung bởi một khám phá sâu sắc về các số liệu "đúng" nắm bắt tác động độ chính xác và hiệu suất của nén, ở cấp độ mỗi tầng. Cụ thể, chúng tôi trình bày bằng chứng rằng việc giảm thiểu lỗi lượng tử hóa cục bộ, mỗi tầng dẫn đến kết quả rất tương tự với việc giảm thiểu các số liệu toàn cục như mất mát đầu ra. Hơn nữa, việc tối đa hóa tỷ lệ nén mỗi tầng tương quan rất tốt với việc giảm thiểu cụ thể tổng thời gian truyền trên triển khai song song dữ liệu hiện có.

Chúng tôi bổ sung những đóng góp thuật toán và phân tích này với một xác nhận thực nghiệm rộng rãi, trên tất cả các họ phương pháp nén gradient cho thấy huấn luyện với các tham số theo tầng được đề xuất bởi L-GreCo khôi phục độ chính xác baseline trong khi tỷ lệ nén gradient được tăng đáng kể. L-GreCo cải thiện hiệu suất huấn luyện lên đến 2.5×, tiết kiệm lên đến 5.2x truyền thông so với nén vanilla, và lên đến 122x so với huấn luyện không nén. Nhìn chung, công trình của chúng tôi cung cấp một phương pháp mới để cải thiện các phương pháp nén gradient hiện có, với chi phí gần như bằng không về thời gian và mất độ chính xác.

Các phần mở rộng có thể của phương pháp của chúng tôi có thể xem xét các chiến lược lai, cho phép kết hợp các kỹ thuật nén khác nhau (ví dụ thưa hóa và hạng thấp) bên trong cùng mô hình.

LỜI CẢM ơN
Các tác giả chân thành cảm ơn tài trợ từ Hội đồng Nghiên cứu Châu Âu (ERC) dưới chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu (thỏa thuận tài trợ số 805223 ScaleML), cũng như hỗ trợ thực nghiệm từ bộ phận IT của IST Austria, đặc biệt là Stefano Elefante, Andrei Hornoiu, và Alois Schloegl.

--- TRANG 9 ---
[Các hình và biểu đồ về thông lượng được dịch tương tự với các chú thích]

--- TRANG 10 ---
[Tiếp tục các hình và biểu đồ về thông lượng]

--- TRANG 11 ---
[Các hình về so sánh thuật toán thích ứng và phân tích buckets]

--- TRANG 12 ---
[Tiếp tục các phân tích và so sánh]

--- TRANG 13 ---
[Các hình về tương quan số liệu và hệ số]

--- TRANG 14 ---
TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được dịch sang tiếng Việt với cấu trúc tương tự]

--- TRANG 15 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 16 ---
[Phần phụ lục A về ưu tiên bucket]

--- TRANG 17 ---
[Phần phụ lục B về tính toán lỗi hạng thấp và các thiết lập thí nghiệm chi tiết]
