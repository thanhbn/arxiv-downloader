# 2312.07046.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2312.07046.pdf
# Kích thước tệp: 102169 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2312.07046v1  [cs.LG]  12 Dec 2023Suy nghĩ lại về Nén: Mô hình Bậc Giảm
của Đặc trưng Tiềm ẩn trong Mô hình Ngôn ngữ Lớn
Arnav Chavan∗1,2, Nahush Lele∗1, Deepak Gupta2
1Nyun AI2Transmute AI Lab
arnav.chavan@nyunai.com, guptadeepak2806@gmail.com
Tóm tắt
Do quy mô đáng kể của Mô hình Ngôn ngữ Lớn (LLM), việc áp dụng trực tiếp các phương pháp nén thông thường trở nên không thực tế. Các yêu cầu tính toán liên quan đến ngay cả những cập nhật gradient tối thiểu cũng đặt ra thách thức, đặc biệt trên phần cứng dành cho người tiêu dùng. Bài báo này giới thiệu một phương pháp sáng tạo để nén tham số và thực tế của LLM dựa trên mô hình bậc giảm, bao gồm phân rã bậc thấp trong không gian đặc trưng và tái tham số hóa trong không gian trọng số. Đáng chú ý, kỹ thuật nén này hoạt động theo cách từng lớp, loại bỏ nhu cầu về thiết bị GPU và cho phép nén các mô hình tỷ tham số trong giới hạn nghiêm ngặt về cả bộ nhớ và thời gian. Phương pháp của chúng tôi đại diện cho một bước tiến đáng kể trong nén mô hình bằng cách tận dụng phân rã ma trận, chứng minh hiệu quả vượt trội so với phương pháp cắt tỉa có cấu trúc tiên tiến hiện tại.

1 Giới thiệu
Những tiến bộ gần đây trong mô hình hóa sinh tạo đã dẫn đến sự gia tăng đáng kể trong việc xây dựng các mô hình ngôn ngữ lớn (LLM), một số trong đó bao gồm hàng trăm tỷ tham số. Mặc dù có độ chính xác đáng khen ngợi, các yêu cầu tính toán liên quan là đáng kể, đặc biệt về mặt bộ nhớ GPU để suy luận. Trong các ứng dụng thực tế, có nhu cầu ngày càng tăng để nén các mô hình này trong khi giảm thiểu sự suy giảm hiệu suất đi kèm.

Các phương pháp đầy hứa hẹn để nén LLM bao gồm cắt tỉa (Frantar & Alistarh, 2023; Sun et al., 2023; Ma et al., 2023), lượng tử hóa (Frantar et al., 2022; Dettmers et al., 2023) và chưng cất kiến thức (Wu et al., 2023; Gu et al., 2023). Các phương pháp lượng tử hóa LLM hiện tại yêu cầu hỗ trợ cấp phần cứng cụ thể và không thể giảm MAC và tăng tốc thời gian suy luận do các hoạt động quant-dequant đắt đỏ trong LLM. Chưng cất kiến thức đã được chứng minh là hoạt động tốt theo cách nhận biết huấn luyện trên các mô hình học sâu tiêu chuẩn. Tuy nhiên, các tài nguyên tính toán khổng lồ cần thiết cho chưng cất hạn chế khả năng áp dụng của các phương pháp như vậy. Gần đây, (Ma et al., 2023) đã trình bày một phương pháp cắt tỉa có cấu trúc được thiết kế cho LLM. Mặc dù phương pháp này có khả năng cắt tỉa LLM mà không cần điều chỉnh tinh, việc giảm hiệu suất là đáng kể, và rõ ràng là cần khám phá thêm trong hướng này. Hơn nữa, chiến lược cắt tỉa không phổ quát và cần nỗ lực đáng kể cho mỗi kiến trúc nơ-ron để xác định các cấu trúc có thể cắt tỉa.

Trong bài báo này, chúng tôi trình bày một phương pháp mới, thực tế và không cần huấn luyện để nén mô hình được thiết kế đặc biệt cho các mô hình lớn bao gồm LLM. Được gọi là LLM-ROM, phương pháp của chúng tôi thực hiện mô hình bậc giảm cục bộ của các đặc trưng tiềm ẩn thông qua phân rã bậc thấp trong không gian đặc trưng và tái tham số hóa trong không gian trọng số. Vì LLM-ROM hoạt động theo từng lớp, nó không yêu cầu bất kỳ cập nhật mô hình lớn nào và có thể được thực hiện trên tài nguyên GPU/CPU nhỏ. Sự đơn giản của LLM-ROM tạo điều kiện cho việc nén các mô hình tỷ tham số trong giới hạn nghiêm ngặt về cả bộ nhớ và thời gian. Các thí nghiệm ban đầu của chúng tôi chứng minh rằng LLM-ROM vượt trội hơn các phương pháp hiện có và có thể nén LLM mà không cần bất kỳ điều chỉnh tinh nào.

∗Đóng góp bằng nhau. Công việc được thực hiện khi Nahush là Thực tập sinh tại Nyun AI
Bản in sẵn. Đang được xem xét.

--- TRANG 2 ---
Các thí nghiệm ban đầu của chúng tôi chứng minh rằng LLM-ROM vượt trội hơn các phương pháp hiện có và có thể nén LLM mà không cần bất kỳ điều chỉnh tinh nào.

2 Phương pháp
LLM-ROM xây dựng mô hình bậc giảm (ROM) theo từng lớp, và cho một mô hình với L lớp, việc phân rã các bản đồ đặc trưng tiềm ẩn được thực hiện theo cách tuần tự sử dụng dữ liệu hiệu chuẩn X∈RB×d1, trong đó B và d1 tương ứng biểu thị kích thước batch và số kênh đầu vào.

Đối với lớp thứ i, ký hiệu là Li với trọng số Wi∈Rd2×d1 trong đó d2 biểu thị các kênh đầu ra, chúng tôi tính toán bản đồ đặc trưng Yi=WiXi∈RB×d2. Theo sau đó, các thành phần chính của Yi được tính toán thông qua phân rã giá trị riêng của ma trận hiệp phương sai đối xứng của Yi. Các thành phần này có thể được biểu diễn dưới dạng Vj∈Rd2∀j∈[1,d2], và ma trận thành phần chính có thể được biểu diễn dưới dạng V∈Rd2×d2, với mỗi hàng biểu thị một thành phần chính được sắp xếp theo thứ tự giảm dần của giá trị riêng của chúng.

Tùy thuộc vào bậc mục tiêu của lớp, chúng tôi chỉ chọn các thành phần chính hàng đầu r được xếp hạng theo các giá trị riêng tương ứng của chúng. Do đó, chúng tôi lập chỉ mục Vr=V[1→r,:]∈Rr×d2. Như vậy, ROM của lớp này có thể được ký hiệu là Yi=VT rVrWiXi. Khi tái tham số hóa thành các ma trận bậc thấp, Wi1=VT r∈Rd2×r và Wi2=VrWi∈Rr×d1, lớp có thể được phân rã thành sự kết hợp tuần tự của hai lớp tuyến tính nhỏ hơn với trọng số Wi1 và Wi2 tương ứng. Chúng tôi xem xét ROM của lớp trước để tạo đầu vào cho lớp tiếp theo để các lớp tiếp theo có thông tin trước về lỗi được giới thiệu trong các lớp trước để phân rã. Lưu ý rằng các hoạt động ROM được thực hiện trên CPU mà không yêu cầu GPU, và chi phí tính toán liên quan đến nó là rất nhỏ.

2.1 Tính toán bậc theo từng lớp
Mô hình LLaMA-7B (Touvron et al., 2023) bao gồm 32 module giải mã giống hệt nhau (các module này chiếm >96% tổng số tham số mô hình), mỗi module bao gồm bảy ma trận trọng số có thể phân rã. Các nghiên cứu ban đầu của chúng tôi đã chỉ ra rằng việc thiết lập một ngân sách nén đồng đều cho tất cả các module từ chính bắt đầu của mô hình dẫn đến sự suy giảm đáng kể trong hiệu suất mô hình; vì lý do này, chúng tôi hạn chế việc áp dụng quy trình nén của mình cho một tập con các module. Hơn nữa, việc phân rã các lớp của một module giới thiệu lỗi trong các đầu ra của lớp đó, lỗi này được tích lũy khi chúng ta tiến về phía trước trong mạng; để giảm thiểu điều này, chúng tôi chỉ nén các module về phía cuối của mô hình. Dựa trên các phương pháp heuristic này, chúng tôi thực hiện các thí nghiệm nén số lượng module khác nhau từ cuối tùy thuộc vào ngân sách mà chúng tôi cần thỏa mãn cho toàn bộ mô hình. Số lượng module cụ thể cần được nén được xác định theo kinh nghiệm cho mỗi ngân sách. Ví dụ, để đạt được ngân sách tổng thể 80%, chúng tôi đã tiến hành các thí nghiệm nén chỉ 8 module cuối cùng đồng đều với ngân sách 0.20, 12 module cuối cùng với ngân sách 0.46, và 16 module cuối cùng với ngân sách 0.60. Các phát hiện của chúng tôi chỉ ra rằng việc nén 12 module cuối cùng mang lại kết quả thuận lợi nhất cho ngân sách 80%. Các thí nghiệm tương tự cho ngân sách 90% và 50% mang lại kết quả tốt nhất khi chúng tôi nén 8 module cuối cùng với ngân sách 0.60 và 24 module cuối cùng với ngân sách 0.33 tương ứng, các kết quả này được liệt kê trong Bảng 1.

Mỗi module ban đầu được cấu thành từ 4 ma trận trọng số Wi∈R4096×4096 từ khối tự chú ý và 3 ma trận trọng số Wj∈R4096×11008 từ mạng truyền thẳng (mặc dù một trong số này được chuyển vị, nó không thay đổi bậc được tính toán). Vào cuối quá trình nén, mỗi ma trận trọng số trong khối tự chú ý được phân rã thành hai ma trận bậc thấp: Wi1∈R4096×r và Wi2∈Rr×4096, trong đó r nhận các giá trị 1228, 954, và 675. Ngoài ra, các ma trận trọng số trong mạng truyền thẳng được phân rã thành Wj1∈R4096×r và Wj2∈Rr×11008 với các giá trị r là 1791, 1373, và 985. Các phân rã này tương ứng với ba cài đặt ngân sách module 60%, 46%, và 33%, dẫn đến ngân sách mô hình tổng thể lần lượt là 90%, 80%, và 50%.

3 Thí nghiệm
3.1 Hiệu suất zero-shot
Để đánh giá hiệu suất của mô hình trong một cài đặt không phụ thuộc vào tác vụ, chúng tôi sử dụng phương pháp đánh giá của LLaMA (Touvron et al., 2023), thực hiện phân loại tác vụ zero shot trên các tập dữ liệu lý luận thông thường, bao gồm BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-easy (Clark et al., 2018), và ARC-challenge (Clark et al., 2018). Chúng tôi sử dụng kích thước batch 512 cho dữ liệu hiệu chuẩn (Mục 3.3) từ các phần huấn luyện của các tập dữ liệu nói trên, đảm bảo không có rò rỉ dữ liệu, và thiết lập độ dài chuỗi tối đa là 128. Chúng tôi thiết lập tỷ lệ nén mục tiêu là 80% và 50% và so sánh với LLM-Pruner2 có và không có điều chỉnh tinh trong Bảng 1.

Phương pháp LLM-ROM của chúng tôi luôn vượt trội hơn LLM-Pruner ở mức nén 80% và 50% mà không cần bất kỳ điều chỉnh tinh nào. Đáng chú ý rằng ở ngân sách 80%, phương pháp của chúng tôi thậm chí còn vượt trội hơn mô hình LLM-Pruner đã điều chỉnh tinh, báo hiệu rằng ROM có thể trích xuất tốt hơn các cấu trúc nơ-ron và trọng số nhỏ hơn từ các đối tác lớn hơn mà không cần bất kỳ cập nhật gradient nào trên các trọng số được trích xuất.

[THIS IS TABLE: Bảng 1 showing performance comparison with columns for Method, Finetune, #Params, #MACs, and various benchmark scores]

3.2 Ảnh hưởng của Kích thước Batch và Độ dài Chuỗi
Phân rã giá trị riêng của ma trận hiệp phương sai và việc lựa chọn tiếp theo các thành phần chính yêu cầu tính toán các đầu ra của lớp đó. Batch được sử dụng để tính toán đầu ra này là một yếu tố quan trọng có thể ảnh hưởng đến khả năng tổng quát hóa của các lớp được thu được sau phân rã. Các thành phần chính được tính toán trên kích thước mẫu lớn hơn sẽ thể hiện sự liên kết gần hơn với những thành phần của phân phối thực. Để chứng thực giả thuyết này, chúng tôi đã tiến hành các thí nghiệm theo hai hướng trực giao: một bằng cách chỉ thay đổi kích thước batch, và hướng khác với sự thay đổi của độ dài chuỗi, các kết quả cho điều tương tự được trình bày trong Bảng 2 và 3 tương ứng.

[THIS IS TABLE: Bảng 2 showing effect of batch size on model performance]

[THIS IS TABLE: Bảng 3 showing effect of sequence length on model performance]

Từ Bảng 2 và 3, rõ ràng rằng batch lớn hơn là có lợi và dẫn đến khả năng tổng quát hóa mô hình tốt hơn đáng kể và đồng thời độ dài chuỗi dài hơn cũng hỗ trợ trong việc duy trì hiệu suất mô hình sau khi nén.

3.3 Lựa chọn tập dữ liệu hiệu chuẩn
Cho rằng các kích hoạt dữ liệu từ tập dữ liệu hiệu chuẩn được sử dụng để tính toán ma trận hiệp phương sai, sau đó được sử dụng để phân rã giá trị riêng, có thể suy luận một cách hợp lý rằng hiệu suất của mô hình nhạy cảm với việc lựa chọn tập dữ liệu này. Suy luận này được hỗ trợ bởi các phát hiện của chúng tôi trong các nghiên cứu được tiến hành, nơi chúng tôi sử dụng ba tập dữ liệu khác nhau, cụ thể là ARC-challenge (Clark et al., 2018), BookCorpus (Zhu et al., 2015) và sự kết hợp của tất cả các lời nhắc tác vụ thông thường, tức là mỗi batch chứa một số lượng mẫu bằng nhau từ sáu tập dữ liệu tác vụ lý luận thông thường được sử dụng để đánh giá chuẩn, cụ thể là BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-easy (Clark et al., 2018), và ARC-challenge (Clark et al., 2018), làm tập dữ liệu hiệu chuẩn của chúng tôi ở ngân sách 80% giữ các siêu tham số khác như kích thước batch và độ dài chuỗi không đổi. Khi tạo các tập dữ liệu hiệu chuẩn, chúng tôi chọn các mẫu từ một phần dữ liệu tách biệt với tập được thực hiện đánh giá, đảm bảo không có rò rỉ dữ liệu. Các kết quả của những nghiên cứu này được tập hợp trong Bảng 4.

[THIS IS TABLE: Bảng 4 showing comparison of model performance with respect to calibration dataset choice]

Các kết quả được trình bày ở trên cho thấy ảnh hưởng của việc lựa chọn tập dữ liệu hiệu chuẩn đến hiệu suất mô hình. Không có gì ngạc nhiên rằng tập dữ liệu bao gồm sự kết hợp của tất cả các tác vụ thông thường được sử dụng để đánh giá chuẩn thể hiện hiệu suất tương đối thuận lợi nhất.

4 Chi phí Tính toán
Chúng tôi tiến hành ROM của LLaMA-7B (Touvron et al., 2023) trên máy chủ CPU với RAM 128 GB và bộ xử lý 48-core/96-thread. Việc triển khai hiện tại của chúng tôi tải toàn bộ mô hình cùng một lúc; tuy nhiên, việc thực hiện ROM theo từng lớp là tầm thường và do đó có thể được thực hiện trong RAM đỉnh dưới 10 GB cho rằng chỉ các đầu vào và trọng số của lớp hiện tại được tải và xử lý vào bộ nhớ. Trung bình mất 13 giây để thực hiện ROM của mỗi lớp của LLaMA-7B (Touvron et al., 2023) có tổng cộng 224 lớp. Nhìn chung, mất 15.8 phút, 21.8 phút và 28.9 phút cho tỷ lệ nén 90%, 80% và 50% tương ứng.

5 Kết luận
Trong bài báo này, chúng tôi đã trình bày một hướng mới cho việc nén LLM tận dụng mô hình bậc giảm của các đặc trưng tiềm ẩn. Dựa trên khái niệm xác định tập hữu hạn các chế độ đặc trưng tiềm ẩn hữu ích nhất, LLM-ROM có khả năng nén LLM mà không cần bất kỳ điều chỉnh tinh nào. Không yêu cầu GPU trong quá trình nén, LLM-ROM có thể được chạy hiệu quả trên máy CPU đơn giản. Hơn nữa, không giống như cắt tỉa, LLM-ROM rất tổng quát và không yêu cầu can thiệp thủ công cho các kiến trúc mô hình khác nhau. Dựa trên các kết quả được trình bày, chúng tôi hy vọng đã mở đường cho một phương pháp mới để thiết kế LLM nén theo cách hiệu quả về tài nguyên.

Tài liệu tham khảo

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language, 2020.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

--- TRANG 5 ---
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.

Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pp. 10323–10337. PMLR, 2023.

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023.

X. Ma, G. Fang, and X. Wang. Llm-pruner: On the structural pruning of large language models. NeurIPS, 2023.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106, aug 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381.

Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.

Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions. arXiv preprint arXiv:2304.14402, 2023.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791–4800, 2019.

Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. CoRR, abs/1506.06724, 2015. URL http://arxiv.org/abs/1506.06724.

5
