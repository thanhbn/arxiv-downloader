# THẺDỤNG CỤ PHẪU THUẬT LLM

Tycho F.A. van der Ouderaa1∗, Markus Nagel2, Mart van Baalen2,
Yuki M. Asano3, Tijmen Blankevoort2
1Imperial College London ,2Qualcomm AI Research†,3QUVA Lab, University of Amsterdam

TÓM TẮT
Các mô hình ngôn ngữ tiên tiến nhất đang ngày càng trở nên lớn hơn trong nỗ lực đạt được hiệu suất cao nhất trên kho dữ liệu văn bản lớn có sẵn. Tuy nhiên, kích thước khổng lồ của kiến trúc Transformer khiến việc triển khai mô hình trong các ràng buộc về tính toán, môi trường hoặc thiết bị cụ thể trở nên khó khăn. Chúng tôi khám phá việc nén dựa trên dữ liệu của các mô hình được đào tạo trước hiện có như một giải pháp thay thế cho việc đào tạo các mô hình nhỏ hơn từ đầu. Để làm điều này, chúng tôi mở rộng các xấp xỉ độ cong được phân tích Kronecker của cảnh quan mất mát mục tiêu đến các mô hình ngôn ngữ lớn. Khi làm như vậy, chúng tôi có thể tính toán cả việc phân bổ động các cấu trúc có thể được loại bỏ cũng như các cập nhật của các trọng số còn lại để tính đến việc loại bỏ. Chúng tôi cung cấp một khung tổng quát cho việc cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc và cải thiện các cập nhật trọng số để nắm bắt nhiều mối tương quan giữa các trọng số hơn, đồng thời vẫn hiệu quả về mặt tính toán. Thực nghiệm, phương pháp của chúng tôi có thể cắt tỉa các hàng và cột từ một loạt các mô hình OPT và Llamav2-7B từ 20%-30%, với mất mát hiệu suất không đáng kể, và đạt được kết quả tiên tiến trong việc cắt tỉa không có cấu trúc và bán cấu trúc của các mô hình ngôn ngữ lớn.
Mã nguồn có sẵn tại: https://github.com/Qualcomm-AI-research/llm-surgeon.

1 GIỚI THIỆU
Những tiến bộ gần đây trong mô hình hóa ngôn ngữ (Vaswani et al., 2017) cho phép khớp các mô hình ngôn ngữ lớn (LLM) với hàng triệu hoặc thậm chí hàng tỷ tham số (như OPT (Zhang et al., 2022) và Llama 2 (Touvron et al., 2023)) trên các kho dữ liệu văn bản lớn đạt được hiệu suất cao. Thật không may, kích thước của những LLM này thường khiến việc triển khai chúng trong các ràng buộc thực tế trở nên khó khăn. Triển khai dựa trên đám mây có thể trở nên rất đắt đỏ đối với các mô hình lớn hơn, và các thiết bị hiệu quả như điện thoại thường bị hạn chế về kích thước bộ nhớ để lưu trữ một mô hình.

Một nhánh tài liệu mở rộng từ cuối những năm 1980, ví dụ, Optimal Brain Damage (OBD, LeCun et al. (1989)) và Optimal Brain Surgeon (OBS, Hassibi & Stork (1992)), diễn đạt việc cắt tỉa như một bài toán tối ưu hóa có ràng buộc để giảm dấu chân và yêu cầu thời gian chạy của mô hình. Hessian cần thiết cho phương pháp này tăng theo bình phương số lượng tham số, và chỉ có thể được tính toán trong thực tế cho các mạng nhỏ một cách không thực tế. Để khắc phục vấn đề này, Eigendamage (Wang et al., 2019) giới thiệu một phân tích Kronecker của một xấp xỉ đường chéo theo khối của Hessian. Các công trình gần đây, như Optimal Brain Compression (Frantar & Alistarh, 2022), SparseGPT (Frantar & Alistarh, 2023), chứng minh việc cắt tỉa hậu đào tạo thực tế của LLM, nhưng chỉ xem xét một độ cong mất mát của lỗi tái tạo đầu ra bình phương của lớp được cắt tỉa, bỏ qua các gradient liên quan đến chi phí loại bỏ cục bộ với mất mát mục tiêu. Kết quả là, xấp xỉ của họ đối với cảnh quan mất mát mục tiêu là không chính xác, dẫn đến suy giảm hiệu suất đáng kể cho các LLM được cắt tỉa. Hơn nữa, những phương pháp này không dễ dàng mở rộng đến việc cắt tỉa có cấu trúc.

[THIS IS FIGURE: Hình 1: LLM Surgeon cho phép nội suy kích thước mô hình giữa các mô hình được đào tạo trước hiện có.]

Công trình này giới thiệu LLM Surgeon, một khung tổng quát cho việc cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc của LLM. Tại thời điểm nộp bài, chúng tôi coi đây là phương pháp đầu tiên thực hiện thành công việc cắt tỉa có cấu trúc của LLM. Công trình đồng thời của Ashkboos et al. (2024) cũng xem xét việc cắt tỉa có cấu trúc của LLM nhưng bỏ qua thông tin gradient, dẫn đến hiệu suất cuối cùng thấp hơn. Hiệu suất vượt trội của LLM Surgeon đạt được bằng cách mở rộng các xấp xỉ đường chéo theo khối được phân tích Kronecker đến Fisher thực nghiệm từ Eigendamage đến LLM. Chúng tôi tiếp tục mở rộng công trình bằng cách dẫn xuất các chi phí và cập nhật cắt tỉa trọng số giống như OBS cho việc cắt tỉa có cấu trúc của nhiều hàng và cột, và cung cấp một khung tổng quát cũng kết hợp việc cắt tỉa bán cấu trúc và không có cấu trúc. Thay vì xử lý các cập nhật trọng số riêng lẻ một cách độc lập, chúng tôi phấn đấu xem xét nhiều mối tương quan giữa các trọng số nhất có thể trong thực tế và dẫn xuất các cập nhật trọng số chung để cắt tỉa nhiều trọng số (hoặc nhiều tập hợp trọng số có cấu trúc) cùng một lúc. Không giống như công trình trước đây trong việc cắt tỉa LLM, LLM Surgeon cắt tỉa trong nhiều lần, cập nhật trọng số và ước tính độ cong giữa các lần. Chúng tôi sử dụng ngưỡng toàn cục cho việc cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc, tức là thay vì cắt tỉa các lớp theo một lượng cố định, các lớp nhạy cảm hơn được cắt tỉa ít hơn những lớp mạnh mẽ hơn. Cuối cùng, chúng tôi đề xuất giảm thiểu các gradient bậc nhất có thể không bằng không bằng cách sử dụng các cập nhật bậc nhất thứ hạng thấp tùy chọn giữa các lần cắt tỉa. Một lợi thế chính của LLM Surgeon là nó cho phép đánh đổi tính toán bổ sung trong quá trình nén để có độ chính xác tốt hơn bằng cách tăng số lượng mối tương quan và/hoặc số lần cắt tỉa. Phương pháp của chúng tôi đưa ra kết quả có thể sử dụng thực tế đầu tiên cho việc cắt tỉa có cấu trúc của LLM – chúng có thể được cắt tỉa lên đến 30% với suy giảm hiệu suất nhỏ. Hơn nữa, chúng tôi đạt được kết quả tiên tiến trong việc cắt tỉa LLM không có cấu trúc và bán cấu trúc.

2 NỀN TẢNG VÀ CÔNG TRÌNH LIÊN QUAN

Việc cắt tỉa mạng nơ-ron nhằm loại bỏ các tham số khỏi mô hình trong khi giảm thiểu tác động tiêu cực đến hiệu suất cuối cùng. Một cách chính thức hơn, chúng ta ký hiệu các tham số mô hình P là vector θ∗= vec(W∗₁,W∗₂, . . .W∗_L)∈R^P, bằng cách làm phẳng các ma trận trọng số L của các khối attention và kết nối đầy đủ, với θ∗≈arg min_θL(θ) đã được khớp với dữ liệu D để tối thiểu hóa một mất mát khả năng âm L(θ)=−log p(θ|D). Để nén mô hình, chúng ta đang tìm kiếm một vector được cắt tỉa θ̂:

θ̂= arg min_θL(θ) s.t. các ràng buộc cắt tỉa dựa trên θ∗ (1)

trong đó các ràng buộc được chọn xác định cấu trúc của các trọng số nén θ̂. Trong việc cắt tỉa không có cấu trúc, một phần của tổng số phần tử trọng số được đặt thành không. Trong việc cắt tỉa bán cấu trúc của M:N, chúng ta có M trọng số của mỗi N trọng số liên tiếp là không (Zhou et al., 2021; Hubara et al., 2021). Và trong việc cắt tỉa có cấu trúc (Louizos et al., 2017), toàn bộ các hàng và cột được đặt thành không. Việc cắt tỉa có cấu trúc dẫn đến lợi ích tức thì nhất trong bộ nhớ và tính toán, vì nó trực tiếp giảm chiều của các ma trận cần được biểu diễn một cách rõ ràng nhưng được coi là khó nén nhất. Duy trì hiệu suất cao thường dễ dàng hơn trong các sơ đồ khác nhưng đòi hỏi số học chuyên biệt khai thác cấu trúc thưa thớt để có lợi ích khi triển khai. Chúng tôi xem xét tất cả các loại cắt tỉa ở trên, với trọng tâm vào việc cắt tỉa có cấu trúc cho LLM.

Thông thường, phương trình (1) không thể được giải trực tiếp, vì không gian của các cấu hình cắt tỉa có thể vượt quá những gì có thể được đánh giá trong thực tế. Để minh họa, một tìm kiếm trên tất cả các mặt nạ cắt tỉa không có cấu trúc có thể của một LLM 125 triệu tham số sẽ yêu cầu 2^P=2^125m≈10^37628749 đánh giá. Do đó, ý tưởng là tìm θ̂ bằng cách sử dụng một đại diện của cảnh quan mất mát q dễ làm việc hơn:

L(θ) =−log p(D|θ)≈ −log q(θ) (2)

Nếu một người chọn một dạng Gaussian cụ thể cho đại diện q của chúng ta, thì các giải pháp cho các ràng buộc cắt tỉa không có cấu trúc, bán cấu trúc và có cấu trúc có thể được dẫn xuất dưới dạng đóng (phụ lục A).

2.1 KHAI TRIỂN TAYLOR

Làm thế nào chúng ta có được một đại diện tốt của mất mát q? Một trong những phương pháp dễ nhất là mở rộng log mất mát cục bộ thông qua một khai triển Taylor bậc hai xung quanh các trọng số được đào tạo trước θ∗, mang lại:

−log q(θ)≈ −log p(D|θ∗)−(θ−θ∗)^T∇L(θ∗)−½(θ−θ∗)^TH_θ∗(θ−θ∗) (3)

trong đó [∇L(θ∗)]_i=∂/∂θ_i L(θ∗_i) ký hiệu Jacobian và [H_θ]_ij=∂²/∂θ_iθ_j L(θ_ij) ký hiệu Hessian. Số hạng bậc nhất biến mất [∇L(θ∗)]_i=0 tại điểm tối ưu. Lưu ý rằng trong thực tế số hạng bậc nhất có thể không biến mất. Mặc dù chúng tôi tuân theo giả định này ban đầu, chúng tôi xem xét các hiệu chỉnh bậc nhất xen kẽ để giảm thiểu vấn đề trong phần 3.6. Khai triển bậc hai của phương trình (3) tạo thành cơ sở của các phương pháp cắt tỉa optimal brain damage (LeCun et al., 1989) và optimal brain surgeon (Hassibi & Stork, 1992). Lưu ý rằng từ góc độ xác suất, một xấp xỉ bậc hai của log khả năng ngụ ý một xấp xỉ Gaussian của khả năng, như cũng được quan sát bởi (Wang et al., 2019) và được minh họa trong hình 2. Điều này được biết đến (Bishop & Nasrabadi, 2006), (MacKay, 2003) như xấp xỉ Laplace q(θ) =N(θ|θ∗+∇L(θ∗),H⁻¹_θ∗), với các trọng số được đào tạo trước là trung bình và Hessian nghịch đảo cục bộ là ma trận hiệp phương sai nắm bắt các mối tương quan giữa các trọng số.

[THIS IS FIGURE: Hình 2: Việc cắt tỉa như tối ưu hóa có ràng buộc đẳng thức của xấp xỉ bậc hai của cảnh quan mất mát (trái), hoặc tương đương, tối đa hóa khả năng dưới xấp xỉ Laplace (phải).]

2.2 MA TRẬN THÔNG TIN FISHER THEO KHỐI

Đối với một mạng được đào tạo với mất mát log khả năng âm, Hessian giống hệt với ma trận Fisher:

H_θ=F_θ=∑_{n=1}^N E_{y∼p_θ(y|x_n)} [∇_θlog p_θ(y|x_n)∇_θlog p_θ(y|x_n)^T] (4)

điều này có lợi ích là luôn bán xác định dương, với nghịch đảo do đó tạo thành một ma trận hiệp phương sai thích hợp cho q, và có thể được xấp xỉ với các mẫu Monte Carlo của p_θ(y|x_n). Đối với hầu hết LLM, điều này sẽ là xử lý đầu ra softmax của mạng như phân phối phân loại p_θ(y|x_n), và lấy mẫu từ đó. Trong thực tế, chúng tôi sử dụng 'Fisher thực nghiệm' thay thế kỳ vọng trên y với dữ liệu mục tiêu y_n (Kunstner et al., 2019). Fisher đầy đủ (thực nghiệm) F_θ∈R^{P×P} tỷ lệ bậc hai theo số lượng tham số P. Để khắc phục điều này, Fisher thường được viết theo các khối theo lớp F_{lk}=∑_{n=1}^N E[vec(∇_{W_l}log p_θ(y|x_n))vec(∇_{W_k}log p_θ(y|x_n))^T], và được xấp xỉ bằng cách chỉ xử lý các lớp một cách độc lập (Martens & Grosse, 2015; Botev et al., 2017):

F_θ=diag(F_{11},F_{22}, . . . ,F_{LL}), F_l=∑_{n=1}^N E[(g_{l,n}g^T_{l,n})⊗(a_{l,n}a^T_{l,n})] ∈ R^{RC×RC} (5)

trong đó ⊗ ký hiệu tích Kronecker và vec(·) phép toán vector hóa ma trận. Bởi vì chúng ta bỏ qua các tương tác chéo lớp, chúng ta viết F_l thay vì F_{ll} cho các khối Fisher liên kết với ma trận trọng số W_l∈R^{R×C} tạo ra đầu ra y_{l,n}=W_la_{l,n}∈R^R từ đầu vào a_{l,n}∈R^C, cho mỗi lớp l và điểm dữ liệu n. Do đó, chúng ta có thể tính toán các khối Fisher từ các kích hoạt đầu vào a_{l,n}∈R^C của dữ liệu được truyền tiến x_n và các gradient đầu ra g_{l,n}=∇_{y_{l,n}}L∈R^R từ lan truyền ngược.

2.3 CẮT TỈA NHƯ TỐI ƯU HÓA CÓ RÀNG BUỘC

Optimal brain surgery dựa vào việc loại bỏ và điều chỉnh các trọng số sao cho mất mát bị ảnh hưởng tiêu cực ít nhất, do đó chúng ta nên viết bài toán như một bài toán tối ưu hóa có ràng buộc. Từ xấp xỉ Gaussian được thảo luận trong phần 2.1 thu được bằng cách khai triển bậc hai log khả năng mất mát −log p≈½θ^TFθ, cập nhật tối ưu Δθ=θ̂−θ (và do đó cũng θ̂=θ+Δθ) trở thành bài toán tối ưu hóa bậc hai có ràng buộc đẳng thức sau (Hassibi & Stork, 1992):

arg min_{Δθ} ½Δθ^TFΔθ (6)
s.t. e^T_kΔθ+e^T_kθ= 0,∀k∈ K

trong đó F là bán xác định dương và K là tập hợp các chỉ số K được cắt tỉa (tức là đặt thành không).

[CONTINUED IN NEXT PART DUE TO LENGTH LIMIT]
