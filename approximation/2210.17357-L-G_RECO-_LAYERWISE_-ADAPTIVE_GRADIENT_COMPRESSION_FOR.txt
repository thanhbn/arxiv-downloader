# 2210.17357.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/approximation/2210.17357.pdf
# File size: 2373580 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
L-G RECO: LAYERWISE -ADAPTIVE GRADIENT COMPRESSION FOR
EFFICIENT AND ACCURATE DEEPLEARNING
Mohammadreza Alimohammadi1 *Ilia Markov1 *Elias Frantar1Dan Alistarh1 2
ABSTRACT
Data-parallel distributed training of deep neural networks (DNN) has gained very widespread adoption, but can
still experience communication bottlenecks. To address this issue, entire families of compression mechanisms have
been developed, including quantization, sparsification, and low-rank approximation, some of which are seeing
significant practical adoption. Despite this progress, almost all known compression schemes apply compression
uniformly across DNN layers, although layers are heterogeneous in terms of parameter count and their impact
on model accuracy. In this work, we provide a general framework for adapting the degree of compression
across the model’s layers dynamically during training, improving the overall compression, while leading to
substantial speedups, without sacrificing accuracy. Our framework, called L-GreCo , is based on an adaptive
algorithm, which automatically picks the optimal compression parameters for model layers guaranteeing the
best compression ratio while satisfying an error constraint. Extensive experiments over image classification and
language modeling tasks shows that L-GreCo is effective across all existing families of compression methods ,
and achieves up to 2.5 ×training speedup and up to 5 ×compression improvement over efficient implementations
of existing approaches, while recovering full accuracy. Moreover, L-GreCo is complementary to existing
adaptive algorithms, improving their compression ratio by 50% and practical throughput by 66%. An anonymized
implementation is available at https://github.com/LGrCo/L-GreCo .
1 I NTRODUCTION
The massive growth in model and dataset sizes for deep
learning has made distribution a standard approach to train-
ing. The most popular strategy is synchronous data-parallel
distribution , which splits the data between parallel workers,
each of which computes stochastic gradients over their data,
and then averages the workers’ gradients in a synchronous
step. This procedure has several advantages, but induces
two main overheads: the synchronization cost of barrier-like
synchronization at every step, and the communication cost
of exchanging the gradients in an all-to-all fashion.
There has been significant work on mitigating these over-
heads. Specifically, a popular approach for reducing the
cost of gradient communication, which is the main focus
of our paper, is lossy gradient compression (Seide et al.,
2014; Alistarh et al., 2017; Dryden et al., 2016; V ogels et al.,
2019), which reduces the number of communicated bits
per iteration. Hundreds of such techniques have been pro-
posed, which can be roughly categorized into three method
families. The first general approach is quantization (Seide
et al., 2014; Alistarh et al., 2017; Wen et al., 2017), which
*Equal contribution1Institute of Science and Technology Aus-
tria2NeuralMagic.reduces the bit width of the communicated gradients in a
variance-aware fashion, in order to preserve convergence.
The second is sparsification (Strom, 2015; Dryden et al.,
2016; Lin et al., 2017), reducing the number of gradient
components updated at every step, which are chosen via var-
ious saliency metrics. The third and most recent approach
islow-rank approximation (Wang et al., 2018; V ogels et al.,
2019), which leverages the low-rank structure of gradient
tensors to minimize communication cost.
In practice, these approaches come with non-trivial trade-
offs in terms of compression versus ease-of-use. For in-
stance, gradient quantization is easy to implement and de-
ploy, but only provides limited compression before accuracy
degradation; sparsification and low-rank approximation can
provide order-of-magnitude compression improvements, but
come with additional costs in terms of maintaining error
correction and careful hyper-parameter tuning for the best
results. The above trade-offs have been thoroughly investi-
gated by the community, along with new adaptive compres-
sion methods (Agarwal et al., 2021b; Markov et al., 2022;
Faghri et al., 2020), which use “learned” compression, ad-
justing the compression to the error incurred during various
phases of DNN training.
Despite this vast amount of research, the interplay between
these methods, their systems implementation, and the under-arXiv:2210.17357v2  [cs.LG]  9 Jun 2023

--- PAGE 2 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
lying training dynamics, has received significantly less atten-
tion. Specifically, almost all existing works view the DNN
model as a uniform set of parameters , and apply compres-
sion either globally to the entire model, e.g. by performing
Top-K selection over the gradients when sparsifying (Chen
et al., 2018), or uniformly , applying the same degree of com-
pression to every layer, independently of layer size or impact
on the loss, i.e. “sensitivity” (V ogels et al., 2019). This view
can lead to two main fallacies. First, from the application
side, this misses significant opportunities for optimization:
modern deep models, in particular Transformers (Vaswani
et al., 2017) can be highly heterogeneous in terms of both
layer sizes and layer sensitivity to gradient compression,
and gradient compression can have a different impact dur-
ing stages of training (Achille et al., 2018). Second, from
the systems side, most efficient training frameworks overlap
communication and computation, transmitting layer gra-
dients as soon as they are generated. Thus, obtaining a
consistent global view of parameters, necessary to perform
global Top-K selection is either impossible, or very costly
to implement and apply.
These concrete examples point to a significant gap between
algorithmic compression techniques, necessary for efficient
data-parallel training, and their practical implementations.
More concretely, it is natural to ask: given an arbitrary
model and a compression technique, is there an efficient
way to balance the application constraints, i.e. the layer
sensitivities , on the one hand, and the system communica-
tion constraings, i.e. layer sizes , on the other, dynamically
during training, in order to maximize speedup, without sac-
rificing final model accuracy?
To address these questions, we introduce L-GreCo , an effi-
cient and general framework for Layer-wise parametrization
of GRadiEnt COmpression schemes. Algorithmically, L-
GreCo is based on a new formalization of the layer-wise
adaptive compression problem, which identifies per-layer
compression parameters, e.g. per-layer sparsity settings or
quantization levels, maximizing the amount of compression,
under a fixed constraint on the total error due to gradient
compression, set so there is not accuracy loss. At the system
level, L-GreCo works by integrating with standard training
frameworks, such as torch.distributed , to exploit
model heterogeneity in terms of both per-layer structure and
per-layer sensitivity, determining on-the-fly by how much
to compress individual layer gradients in order to maximize
compression or end-to-end training times.
We validate L-GreCo across all existing families of com-
pression strategies: quantization, sparsification, and low-
rank compression, and across a variety of vision and lan-
guage tasks. Our experiments show that L-GreCo con-
sistently achieves higher compression rates than existing
manual or adaptive strategies, and is particularly-effectivefor heterogeneous and sensitive Transformer-based models.
Specifically, the framework provides gains for all existing
compression strategies in a black-box fashion, and results in
significant compression and practical speedup gains, com-
pared to the best-known manual parametrization schemes,
when executing standard image classification or language
modeling benchmarks in single- and multi-node settings.
We summarize our contributions as follows:
•We show that gradient compression schemes can take
advantage of the heterogeneous layer-wise structure of
DNNs in order to reduce communication overhead.
•We design and implement the L-GreCo framework,
which guarantees optimal layer-wise compression lev-
els in terms of the compression-accuracy tradeoff,
based on a theoretically-justified measure of layer com-
pression sensitivity.
•We provide an extensive empirical evaluation on
different neural networks (ResNet18, ResNet50,
Transformer-XL, Transformer-LM) with different
datasets (CIFAR-100, ImageNet, Wikitext-103) show-
ing that L-GreCo reduces communication by up to
5×and achieves speedups up to 2.5 ×without loss
of accuracy or significant tuning, on single and multi-
node deployments.
•We conduct the first detailed study for both layer sen-
sitivity metrics and for performance metrics. We find
that sensitivity metrics based on quantization error are
quivalent to metrics based on output loss. At the same
time, from the performance perspective, metrics which
seek to maximize the compression ratio lead to similar
results to timing-based ones.
•Finally, we show that L-GreCo is compatible with
other adaptive compression schemes: specifically, it
can be extended to use information about the different
stages of training (Agarwal et al., 2021b), leading to
further performance improvements.
2 R ELATED WORK
Compression methods. Gradient compression usually em-
ploys three strategies: quantization, sparsification, and low-
rank decomposition. Quantization methods (Seide et al.,
2014; Alistarh et al., 2017; Wen et al., 2017; Lim et al.,
2018; Ramezani-Kebrya et al., 2021) use lower precision of
each gradient component, reducing the number of transmit-
ted bits. They are easy to implement, and work under stable
hyper-parameters (Alistarh et al., 2017; Xu et al., 2021;
Markov et al., 2022); however, their compression is limited
by the fact that at least one bit per entry must usually be
transmitted. Sparsification techniques (Strom, 2015; Dry-
den et al., 2016; Lin et al., 2017; Karimireddy et al., 2019)
circumvent this by identifying salient components in the
gradient and only transmit such subsets. Finally, gradient

--- PAGE 3 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
Layers8163264RanksUniform-32
perplexity 24.05L-GreCo-32-[8:64]
perplexity 24.08
1031041051061071081091010
Layer size
Figure 1. Profile of L-GreCo rank choices for PowerSGD com-
pression on Transformer-XL. The red line represents uniform
compression, while the blue line represents the L-GreCo pro-
file. Transparent bars show layer sizes. Layers are indexed in the
order they are communicated. The annotated number is the final
test perplexity (ppl) for the experiment (lower is better). Here, the
average compression of L-GreCo is1.5x higher than uniform.
decomposition algorithms (V ogels et al., 2019; Wang et al.,
2018) use the fact that the layer-wise gradient tensors are
known to be well-approximable via low-rank matrices, and
aim to design light-weight projection approaches that also
provide low error. Sparsification and low-rank techniques
usually require error correction buffers to preserve good
convergence, as well as non-trivial hyper-parameter tuning.
As we show experimentally, L-GreCo is compatible with
all of these approaches and can provide significant addi-
tional bandwidth savings for each such strategy, without
sacrificing model accuracy and without tuning.
Adaptive Compression. The general idea of adapting the
degree of compression during training has been investigated
by AdaComp (Chen et al., 2018), which proposes a self-
tuning adaptive compression method; yet, their method does
not adapt compression parameters per layer, and cannot be
combined with other compression approaches. Faghri et al.
(2020) adapts the quantization grid to the gradient distribu-
tion; yet, their approach is specifically-tuned to quantization,
and oblivious to layer heterogeneity.
Sahu et al. (2021) optimize the total error over steps for
sparsification-based compression and suggest threshold
global sparsifiers, which are shown to reach higher compres-
sion rates than uniform per-layer compression on small vi-
sion tasks (e.g. ResNet18 on CIFAR10/100). However, their
approach is restricted to sparsification and leaves unclear
how to tune the threshold for large-scale, sensitive models
such as Transformers or ImageNet-scale models. In particu-
lar, we were unable to obtain good results with this approach
on models such as Transformer-XL or Transformer-LM. In
section 5.3 we present a comparison with their approach on
ResNet18/CIFAR-100, showing that our method yields bothhigher accuracy and higher compression.
Accordion (Agarwal et al., 2021a) adapts the compres-
sion parameters of sparsification and low-rank compression
based on the critical regimes of training. The algorithm alter-
nates between two compression levels (“low” and “high”),
provided by the user and is prone to accuracy loss. Our
approach improves upon Accordion in terms of speedup,
but also that we can combine our method with Accordion
and obtain even higher gains. A variant of the layer-wise
compression problem similar to the one we consider here
was proposed by CGX (Markov et al., 2022). However,
they investigate a kmeans-based heuristic, which we show
experimentally to be sub-optimal.
To our knowledge, our dynamic programming strategy has
not been employed in the context of adaptive gradient com-
pression. Related approaches have been investigated in the
context of weight compression for DNNs, see e.g. (Wu et al.,
2020; Aflalo et al., 2020; Frantar & Alistarh, 2022; Shen
et al., 2022). Yet, there are significant technical differences:
first, the error metrics and speedup objectives are different in
the case of weight compression; second, we execute online,
at training time, which means that our algorithm has to be
extremely efficient, and adapt to dynamic inputs.
3 P ROBLEM SETUP
Goals. Assuming we are given a DNN model MwithL
layers and a compression technique, we would intuitively
like to find a choice of compression parameters cℓ, one for
each layer ℓ∈ {1,2, . . . , L }which would minimize a met-
ric representing a damage of the training quality introduced
by compression while minimizing the total number of bits
transmitted. Yet, this intuitive description leaves open a
range of details, such as 1) the notion of layer-wise metric
that corresponds to the compression effect for a given set of
parameters; 2) the exact problem formulation, constraining
the compression affect or the compression ratio; and 3) an
efficient implementation of such an algorithm. We address
these questions next.
Metric. Choosing the right sensitivity metric is a key for ac-
curacy recovery. For this, we have tried several approaches.
First, we note that the sensitivity of a layer to the gradient
compression can be measured by a model loss reaction to
the compression. In order to evaluate the metric we setup
the following experiment. We saved model checkpoints at
different stages of the uncompressed training. Then con-
ducted multiple short (up to 50 steps) training runs with the
same data starting from the checkpoint but varying gradient
compression parameters. After that, we used the difference
of loss with and without compression as metric.
Now, the question is what compression parameters (vectors

--- PAGE 4 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
of the size equal to number of the compressed layers) we
want to use to evaluate each layer’s metric. As long as we
wanted to see the model reaction to individual layer com-
pression, for each layer we varied compression parameter
leaving all other layers’ gradients untouched. With that we
collect the differences of loss for each layer and compres-
sion parameter and use them as a metric. Another approach
is based on prior theoretical works, which suggest that the
squared ℓ2error of compression is a good measure of the
convergence impact of compression technique. Here, we
aggregate gradients during the training, then compress the
aggregated gradients for each compression parameter for
each layer individually and use the magnitude of the error
as a metric.
As shown in Section 5.4, the two approaches are strongly
correlated and the resulting optimal parameters are close to
each other. However, the loss-based approach is not appli-
cable in the real world as it requires off-training evaluation
which alters the original training pipeline and takes a time
comparable to the original training time. At the same time,
the error-based approach can be used during the training
easily integrates into the training and has negligible timing
overheads. Taking this into account, throughout the paper
we use L2 norm of the compression error as the main layer
sensitivity metric.
The Constrained Optimization Problem. We formalize our
optimization problem as follows. Given a model MwithL
layers ℓ∈ {1,2, . . . , L }and a compression technique, pro-
viding a set of compression choices C={c1, c2, . . . ck}for
each layer. We emphasize that, for simplicity, we consider
a single compression technique and the same compression
choices/levels for each layer, but our approach would also
work for different techniques being applied to the same
model, and heterogeneous compression choices.
In this context, our method receives as input an error func-
tion error (ℓ, cj)which provides the L2 norm of the com-
pression error at layer ℓfor compression choice cj, and a
function size(ℓ, cℓ)which measures the transmission cost
of layer ℓfor choice cℓ. In addition, we assume to be given
a fixed maximal error threshold Emaxwhich the algorithm
should not violate. Then, we wish to find a layer-wise setting
of compression parameters c1, . . . , c Lwith the objective
minimizeLX
ℓ=1size(ℓ, cℓ)s.t.LX
ℓ=1error (ℓ, cℓ)≤ Emax.
In practical terms, this formulation aims to minimize the
total transmission cost for the gradient tensors under a max-
imum additive constraint on the gradient compression error.
One implicit assumption is that the metric error (·,·)isaddi-
tiveover layers, and that it is possible to obtain a “reference”
Training script User code
Distribute and save
new optimal
parametersL-GreCo
Compressed 
communicationDDP
FrameworkAccumulate
gradients
Find optimal
parametersFigure 2. Schema of L-GreCo integration.
error upper bound which does not result in accuracy loss.
As we see next, this is the case for the error metric we adopt.
The Error Bound. The remaining question is how to choose
Emax. We decided to pick this error bound to track that of
a reference compression approach which is known not to
lose accuracy relative to the baseline . Here, we leverage
the fact that the literature provides parameters which allow
reaching full accuracy recovery for different models and
datasets. For instance, for quantization, we can use 4-bit
quantization , which is known to recover for virtually every
model (Alistarh et al., 2017; Markov et al., 2022). For
sparsification (Lin et al., 2017; Renggli et al., 2019) as well
as for matrix decomposition (V ogels et al., 2019) we had
to use different reference parameters for different training
according to their baselines (For the details refer to the
Tables 2 and 3).
4 T HEL-G RECOFRAMEWORK
Overview. We now describe a general algorithm to solve
the constrained optimization problem from the previous
section. Our algorithm makes layer-wise decisions in order
to balance the magnitude of the compression error and the
compressed size of the model. As inputs, our algorithm
takes in the uncompressed layer sizes size(ℓ,⊥), a set G
of accumulated gradients per layer (which will be used to
examine compression error), as well as a fixed error bound
Emax. Specifically, at a given decision step, the objective is
to find an optimal mapping of each layer ℓto a compression

--- PAGE 5 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
levelcℓ, such that the norm of the total compression error,
computed over the set of accumulated gradients Gdoes not
surpass Emax, but the total compressed size of the modelPL
ℓ=1size(ℓ, cℓ)is minimal for this error bound.
This formulation is reminiscent of the knapsack problem :
the error is the size of the knapsack, and the compressed
size is the value we wish to optimize. In this formulation,
the problem would have an efficient and optimal algorithm,
using dynamic programming (DP). However, the squared
L2error is not discrete, so we cannot directly apply this
approach. Instead, the idea is to reduce this to a solvable
problem by discretizing the possible set of error values.
Specifically, since errors are monotonic and we can use a
very fine discretization without significant efficiency loss, it
is highly unlikely that we would miss the optimal solution by
a significant amount. For illustration, in our implementation,
we use D= 10000 as a discretization factor (i.e. steps of
sizeEmax/D).
The Algorithm. The algorithm, presented in full in Al-
gorithm 1, works as follows. First, we compute the data
needed for the algorithm for all layers and all considered
compression parameters (lines 1-10), corresponding to er-
rors and compressions for each possible choice. Then we
execute a dynamic programming algorithm solving the fol-
lowing problem. We want to compute the minimum total
size given total compression error Ein the first ℓlayers
compressedsize (ℓ, E) = min ℓcompressedsize (ℓ−1, E−
error (ℓ, cℓ)) +size(ℓ, cℓ). To achieve this, for each layer
we want to consider, we run over all error increments and
all possible compression parameters, and minimize the to-
tal compressed size for the current total compression error
(lines 12-22) saving the compression parameter with which
we obtain the minimum. Then, in lines 23-27, we find
the error increment with which we achieve the lowest total
compressed size and reconstruct the compression parameter
mapping—thus, we have obtained the result.
Implementation. As shown in the Figure. 2 we integrate
theL-GreCo in the middle between user training code and
communication system that is responsible for compression
of the gradient and their synchronization. We run the above
algorithm periodically, e.g. once per training epoch, on a sin-
gle designated worker; unless otherwise stated, this worker
performs all steps. In between runs of the algorithm, we
accumulate per-layer gradients in auxiliary buffers. We then
build an L2error table, for each layer, and for every com-
pression parameter in the user-provided range and for the
reference compression parameters set. To find the error, we
simulate the compression/decompression of each layer with
the given compression parameter without applying error
feedback and compute the L2distance between the origi-
nal and recovered vectors. Then we run the DP algorithm.
This provides us with the optimal compression mapping,Algorithm 1 L-GreCo adaptive compression
Input: Model Layers Li, accumulated gradients Gi, possible
compression parameters C={c1, c2, . . . , ck}, static default
compression parameters we try to improve Cd
i, discretization
factor D
Output: Compression parameters assignments cℓ∈Cfor each
layerℓ
1: N = number of layers
2: Compute Emaxfor the default compression parameters Cd
i
3: Compute discretization step Emax/D.
4:Costs matrix N× |C|where position i, jhas a value of the
size of layer icompressed with compression parameter cj.
5:Errors matrix N× |C|where position i, jhas a value of the
discretized L2of the compression error when the accumulated
gradients of layer iare compressed with parameter cj.
6:DP matrix N×(D+ 1) filled with ∞values.
7:PD matrix N×(D+ 1) .
8: // Initialization of the cost tables:
9:forc∈Cdo
10: DP[1][Errors [1][c]] =Costs [1][c]
11: PD[1][Errors [1][c]] =c
12:end for
13: // Dynamic programming algorithm
14:forLayer li:= 2..Ndo
15: forci∈Cdo
16: forei:=Errors [li][ci]..Ddo
17: t=DP[li−1][ei−Errors [li][ci]] +Costs [li][ci]
18: ift < DP [li][ei]then
19: DP[li][ei] =t
20: PD[li][ei] =ci
21: end if
22: end for
23: end for
24:end for
25:errmin=argmin (DP[N])
26: // Reconstruction of the optimal parameters
27:forli=N..1do
28: result [li] =PD[li][errmin]
29: errmin=errmin−Errors [li][result [li]]
30:end for
31:return result
which the designated worker broadcasts to the other workers.
Then, on each worker, we save the compression parameters
mapping in the communication engine.
Computational and memory costs. The algorithm assumes
that we accumulate gradients in additional buffers, occu-
pying the model size memory. The DP algorithm has
O(D|L||C|)time complexity and O(|L|D)memory com-
plexity. The actual timings for the algorithm are presented
in Table 1. The overheads consist of two parts: 1. Error
computation 2. Dynamic programming algorithm. We can
see that the dynamic programming part only takes a frac-
tion of a percent of the training time whereas most of the
overhead is caused by computation. However, both over-
heads are negligible compared to the speedups provided by
L-GreCo (see Figures 4 and 5).
Communication details. From the point of view of the com-

--- PAGE 6 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
Table 1. Timing overheads for L-GreCo in relation to the total
training time. Numbers in brackets represent error computation.
Compression
MethodTransformer-XL ResNet50
PowerSGD 0.56%[0 .49%] 0.15%[0 .14%]
QSGD 0.14%[0 .13%] 0.04%[0 .03%]
TopK 0.38%[0 .35%] 0.33%[0 .30%]
mon data-parallel implementation, the gradients become
available right after the end of the backward propagation
of the corresponding layer. In order to reduce communi-
cation latency, gradients are grouped into several buffers
(called buckets in pytorch ). An important optimization
is the overlapping of gradient communication with com-
putation. This implies that the cost of the communication
of the first buckets is likely to be completely “hidden” by
computation, whereas the synchronization of the last bucket
becomes a significant part of the timing delay between the
training steps (see Figure. 10(b)). This means that the com-
munication speed of different parts of the model (i.e., layer
groups) has a different impact on the training speed. Hence,
compression ratio optimization might output suboptimal
results.
Time optimization. Having that in mind, we have inte-
grated an utility that measures time each bucket takes to
synchronize. With this tool, we can reformulate an opti-
mization problem: in fact, we want to minimize the time
range between the start of first and end of the last bucket
synchronization (i.e. gradient synchronization time) rather
than the compression ratio. For that, we measure gradients
synchronization time for various combinations of compres-
sion ratios per bucket, saving the communicated buckets
sizes. Then, we train a linear regression model to learn
the relation between the transmitted bucket sizes and the
gradient synchronization time. From that we obtain per-
bucket coefficients T(b)that we can apply for each layer in
a respective bucket. Then we change the objective in the
optimization problem(see Formula. 3) to:
minimizeLX
ℓ=1size(ℓ, cℓ)∗T(ℓ)s.t.LX
ℓ=1error (ℓ, cℓ)≤ Emax.
This gives us the parameters that have the same or worse
compression characteristics than ratio-based algorithm but
improving the estimated communication time.
5 E XPERIMENTAL VALIDATION
We experimentally evaluate L-GreCo across all existing
compression strategies: quantization using QSGD, TopK
sparsification, and low-rank approximation via PowerSGD.5.1 Experimental setup
Infrastructure. Our evaluation uses commodity worksta-
tions with 4 or 8 NVIDIA RTX3090 GPUs. In the multi-
node setting, we use 4 cloud instances with 4xRTX3090
GPUs, provided by Genesis Cloud. Bandwidth measure-
ments show that inter-GPU bandwidth values lie in between
13 to 16 Gbps, and inter-node bandwidth in the cloud is up
to 10 Gbps. We used Pytorch 1.10, openmpi/4.1.4, CUDA
11.3, NCCL 2.8.4, and cudnn/8.1.1.
Implementation. We implement L-GreCo in PyTorch
using torch.distributed hooks for PowerSGD and
leveraging open-source CGX framework (Markov et al.,
2022) for quantization and sparsification.
Datasets and models. We examine two different
DNN learning tasks: 1) image classification on the CI-
FAR100 (Krizhevsky, 2009) and ImageNet (Deng et al.,
2009) datasets, and 2) language modeling on WikiText-
103 (Merity et al., 2016). We used state-of-the-art model im-
plementations and parameters provided by the PyTorch ver-
sion of the NVIDIA Training Examples benchmark (Nvidia,
2020) and the fairseq library PyTorch examples (Ott
et al., 2019). We used ResNet-18 for CIFAR-100 train-
ing with batch size 256, ResNet-50 in the mixed-precision
regime for ImageNet with batch size 2048, and Transformer-
XL and Transformer-LM trained in full-precision for
WikiText-103, with batch sizes 256 and 2048, respec-
tively. All our experiments use the original training
recipes, without any hyperparameter tuning to account
for gradient-compressed training.
Baselines. The first natural baseline is uncompressed
training, which sets our accuracy baseline. Matching
MLPerf (Mattson et al., 2020), we set our accuracy thresh-
old to 1% relative to uncompressed training. The second
natural baseline is the best existing manually-generated gra-
dient compression recipes. By and large, existing methods
propose uniform per-layer compression to a given threshold,
e.g. (Alistarh et al., 2017; Wen et al., 2017; Renggli et al.,
2019; V ogels et al., 2019). For such baselines, we want to
improve compressed size and training speed, possibly also
improving final model accuracy. We found that the best
choice of compression parameters for uniform per-layer as-
signment depends on the compression method, dataset, and
task. For some experiments we had to tune the uniform com-
pression parameters to match baseline (non-compressed)
results (see Tables 2 and 3).
Parameter ranges. L-GreCo requires a range of possible
compression parameters as input. We have always chosen
this range to include the default compression parameters
used in the literature. Moreover, we left a gap between
the default parameter and the highest possible compression

--- PAGE 7 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
Table 2. Accuracy recovery and compression ratios for different compression methods with uniform and adaptive schemes on image
classification tasks. The compression ratios measure actual transmission savings. Values in brackets for L-GreCo compression ratios
stand for improvements relative to the corresponding uniform compression.
Compression
approachParameter
choiceResNet18 on CIFAR-100 ResNet50 on ImageNet
Default
paramAccuracy Compression
ratioDefault
paramAccuracy Compression
ratio
Baseline N/A - 76.60±0.40 1.0 - 76.88±0.16 1.0
QSGDuniform4 bit76.80±0.40 7.84 bit77.38±0.10 7.7
L-Greco 76.46±0.21 8.6[1.10×] 76.77±0.25 11.0[1.41×]
TopKuniform1%75.73±0.46 48.11%76.85±0.06 45.6
L-Greco 75.66±0.35 182.0 [3.78×] 77.04±0.27 122.0 [2.67×]
PowerSGDuniformrank 476.36±0.28 72.2rank 476.50±0.37 66.5
L-Greco 76.43±0.37 133.9 [1.85×] 76.33±0.27 96.2[1.44×]
Table 3. Accuracy recovery and compression ratios for different compression methods with uniform and adaptive schemes on language
modeling tasks. The compression ratios measure actual transmission savings. Values in brackets for L-GreCo compression ratios stand
for improvements relative to the corresponding uniform compression.
Compression
approachCompression
parametersTransformerXL on WIKITEXT-103 TransformerLM on WIKITEXT-103
Default
paramPerplexity Compression
ratioDefault
paramPerplexity Compression
ratio
Baseline N/A - 23.82±0.10 1.0 - 29.34±0.12 1.0
QSGDuniform4 bit23.82±0.1 7.84 bit29.39±0.10 7.8
L-Greco 24.11±0.09 9.1[1.16×] 30.03±0.16 9.9[1.26×]
TopKuniform10%24.13±0.14 4.910%29.29±0.09 4.9
L-Greco 24.19±0.13 12.8[2.61×] 29.08±0.20 25.6[5.2×]
PowerSGDuniformrank 3224.08±0.12 14.0rank 3229.98±0.09 15.0
L-Greco 24.09±0.15 20.8[1.48×] 30.19±0.09 26.5[1.76×]
parameter (the right range bound) — otherwise, we are
limited to matching L2error; and a gap between the default
parameter and the lowest possible compression parameter
(the left range bound)—otherwise, we will not improve
compression.
The range depends on the compression method. Quantiza-
tion and low-rank methods have a limited number of discrete
parameters (number of bits per value and decomposition
rank respectively) whereas the range for sparsification is
larger. In our experiments, we use the following simple
approach. Assume a default uniform compression param-
eterD, e.g. 4 quantization bits. For quantization and low-
rank methods, the search space was defined as [D/2,2∗D]
with an incremental step of 1. For sparsification, we chose
[D/10,10∗D]with an increment of D/10.
The other two input parameters of L-GreCo are how fre-
quently the algorithm is run and the warm-up period after
which the compression is turned on. The first parameter is
usually chosen to be equal to the evaluation period from the
training recipes (typically, 1 epoch). As we will see (Fig-
ure 9a) the compression ratio of the schemes returned byL-GreCo is relatively stable, so it does not need a frequent
re-adjustment. In our experiments, the warm-up period is
equal to the default learning rate warm-up period.
5.2 Evaluation results
Accuracy recovery. We first examine model accuracies us-
ing standard recipes for end-to-end training. For each exper-
iment, we performed 3 runs with different seeds. We com-
pareL-GreCo accuracy recovery with the uncompressed
(baseline) and the best uniform per-layer compression pa-
rameters (uniform). The results are presented in Tables 2
and 3. All accuracies and perplexities are presented with
seed variability. The compression ratio represents actual
transmission cost savings versus the uncompressed baseline.
For each compression method and training task, we took the
default parameter which provides the highest compression
ratio while recovering the final model accuracy, i.e. further
compression improvement with uniform setting only leads
toworse convergence.
Overall, results show that L-GreCo stays within the ac-
curacy recovery limit of 1% multiplicative error (Mattson

--- PAGE 8 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
0.36 0.37 0.38 0.39 0.40
Time per step, s24.1524.2024.2524.3024.35Perplexity
Uniform
L-GreCo
Ranks
4
6
8
16
Figure 3. Perplexity (lower is better) vs. time per step (smaller
is better) for different ranks of PowerSGD compression for the
Transformer-XL wikitext-103 task with uniform or L-GreCo
suggested compression schemes. Single node, 8 RTX3090 GPUs.
et al., 2020) for most tasks, often being very close to the
uniform baseline, while consistently increasing the compres-
sion ratio, across all the tasks and compression techniques
considered. We stress that we did not perform task-specific
parameter tuning. The gains are remarkably high for spar-
sification and low-rank techniques, where the search space
and therefore also the savings potential of L-GreCo are
higher. For instance, for Transformer-LM, we obtain up
to 5x higher compression relative to the uniform baseline,
with negligible accuracy impact. At the same time, we note
that L-GreCo induces >1%multiplicative loss on quanti-
zation and low-rank compression for the highly-sensitive
Transformer-LM model.1This is because our default com-
pression range is too aggressive in this case; this can be
easily addressed by adjusting the range—we chose not to
do it for consistency.
To further explore the accuracy-compression trade-off,
we varied the uniform default compression parameters,
specifically throttling the target powerSGD rank for the
Transformer-XL/WikiText-103 task. Figure 3 shows that
L-GreCo provides a markedly better trade-off than uni-
form compression. We can see that L-GreCo provides better
perplexity recovery while improving training speed of the
model.
Profiling. In order to explore the compression overhead
we run the profiling the training. The result is presented
in Figure. 8. There we compare operation timings for the
original(uncompressed) training and training where the gra-
dients are compressed with PowerSGD, rank 32. We can see
that relatively expensive compression (PowerSGD is more
1Specifically, our loss is of at most 0.85perplexity relative to
the uncompressed baseline. For this model, however, even basic
FP16 training loses more than 1 point of perplexity vs FP32.time-consuming than QSGD and optimized TopK) takes
less than 10% of the step time.
Speedup results. For end-to-end training speedup improve-
ments, we used standard uncompressed training hyperpa-
rameters for ResNets and Transformers. We consider weak
scaling , i.e. increase the global batch size while increasing
the node count. (Performance improvements are higher for
strong scaling.) We begin by examining training throughput
results for multi-node experiments in Figures 4 and 5, exe-
cuted in the cloud environment. This setting encounters a
bandwidth bottleneck even at a lower node count, which is
apparent given the poor performance of the uncompressed
baseline for both models. Tuned uniform compression re-
moves this bottleneck to a significant extent: for instance,
uniform PowerSGD/ResNet50 can reach 75% of ideal scal-
ing on 4 nodes.
It is therefore fairly surprising that automatic non-uniform
compression can still provide significant improvements in
this setting: relative to uniform compression, L-GreCo
gives up to 1.45x throughput improvement for ResNet50,
and up to 2.4x speedup on Transformer-XL. This suggests
that non-uniform compression can be an effective strategy
in this scenario, especially for layer-heterogeneous models
such as Transformers.
We next examine results for single-node scaling from 1
to 8 GPUs, presented in Figures 6 and 7. This is a more
challenging scenario for communication compression since
bandwidth is less of a bottleneck in a single-node setting.
We begin by examining the results for the Transformer-XL
model. For PowerSGD and TopK, L-GreCo leads to gains
up to 25% end-to-end speedup compared to uniform, with
negligible accuracy difference. For QSGD, the search space
is very limited: uniform already uses 4 bits, and provides
very good scaling. Our adaptive method still provides 2%
speedup compared to our well-tuned uniform compression,
and 50% speedup compared to non-compressed training,
reaching ≥90% of ideal scaling. Here, we observe that,
for ResNet50, PowerSGD and QSGD are able to recover
ResNet50 accuracy with uniform parameters that are close
to theoretically-optimal ones (e.g. rank-4 for PowerSGD,
whereas the minimum is rank-1), leaving little room for
improvement given adaptive mechanisms. Nevertheless,
L-GreCo provides consistent improvements in the case of
the heterogeneous Transformer-XL model (Figure 6).
Overall, we note that L-GreCo provides statistically-
significant performance improvements over static uniform
compression (especially given heterogeneous models) when
applied to all considered compression methods, with negli-
gible impact on accuracy.

--- PAGE 9 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
1 2 4
Num Nodes20k40k60k80k100k120kTokens/secbaseline
uniform
L-GreCo
(a) PowerSGD
1 2 4
Num Nodes20k40k60k80k100k120kTokens/secbaseline
uniform
L-GreCo (b) QSGD
1 2 4
Num Nodes20k40k60k80k100k120k140kTokens/secbaseline
uniform
L-GreCo (c) TopK
Figure 4. Throughput for Transformer-XL (TXL) on WikiText-103. Multi node, each node has 4 RTX3090 GPUs.
1 2 4
Num Nodes1k2k3k4k5k6k7k8k9k10k11k12k13k14kImages/secbaseline
uniform
L-GreCo
(a) PowerSGD
1 2 4
Num Nodes1k2k3k4k5k6k7kImages/secbaseline
uniform
L-GreCo (b) QSGD
1 2 4
Num Nodes1k2k3k4k5k6k7k8k9k10k11k12k13k14kImages/secbaseline
uniform
L-GreCo (c) TopK
Figure 5. Throughput for ResNet50/ImageNet. Multi node, each node has 4 RTX3090 GPUs.
5.3 Comparison with other adaptive methods
So far, we have used uniform compression as our base-
line. We now compare L-GreCo with prior works on adap-
tively choosing compression parameters. We consider the
Accordion (Agarwal et al., 2021b) and CGX (Markov
et al., 2022) approaches, as they are the closest in terms of
scope, and the most general in terms of applicable com-
pression methods. We perform our comparison on the
Transformer-XL model on WikiText-103, as 1) it is a model
that is sensitive to gradient compression, 2) has high hetero-
geneity of layers, and 3) suffers from bandwidth bottlenecks
(see Figure 4). PowerSGD was chosen as a compression
method, as Accordion specifically optimizes for it, and
this approach can lead to the highest practical compression
gains. We run the experiments in two distributed settings:
single node with 8 GPUs and multi-node which includes
4 machines with 4 GPUs each. We compare compression
and throughput (processed samples per second) for each
method.
In the experiments in this section we deviate from our ap-
proach to the choice of search range of the compression
parameters for the L-GreCo algorithm, described in Sec-
tion 5.1. Here, we aim to get the most speedup without
losing the final model accuracy. Therefore, for each algo-
rithm we consider in this section we tune the compressionparameters for the chosen compression method and train-
ing task (without changing the training hyperparameters,
e.g. learning rate, weight decay, etc.) so that we get the
best timing results with the final model accuracy within
MLPerf allowed bounds. The best range of parameters for
L-GreCo turns out to be ranks [8,64](see Table 4) with
default rank 32.
Table 4. Comparison of L-GreCo with other adaptive algorithms
on Transformer-XL using PowerSGD.
Adaptive
algorithmParam.
RangeRatioSingle
node
Tokens/sMulti-
node
Tokens/s
Uniform 32 14.1 110k 72k
L-GreCo 8 - 64 23.5 144k 150k
Accordion 16, 64 23.9 114k 107k
CGX,
kmeans8 - 64 21.6 124k 112k
L-GreCo +
Accordion8 - 128 36.9 138k 176k
Global TopK Comparison. The optimization problem -
minimizing of gradient error magnitude given the desired
compression ratio - can be resolved by global topK in case
of gradient sparcification. Indeed, global topK is applied
to the gradients of the entire model so it selects the ele-
ments with the highest magnitudes, minimizing the total

--- PAGE 10 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
1 2 4 8
Num GPUs20k40k60k80k100k120kTokens/secbaseline
uniform
L-GreCo
(a) PowerSGD
1 2 4 8
Num GPUs20k40k60k80k100k120k140k160k180kTokens/secbaseline
uniform
L-GreCo (b) QSGD
1 2 4 8
Num GPUs20k40k60k80k100k120k140k160kTokens/secbaseline
uniform
L-GreCo (c) TopK
Figure 6. Throughput for Transformer-XL (TXL) on WikiText-103. Single node, 8 RTX3090 GPUs.
1 2 4 8
Num GPUs1k2k3k4k5k6k7kImages/secbaseline
uniform
L-GreCo
(a) PowerSGD
1 2 4 8
Num GPUs1k2k3k4k5k6k7k8kImages/secbaseline
uniform
L-GreCo (b) QSGD
1 2 4 8
Num GPUs1k2k3k4k5k6k7k8kImages/secbaseline
uniform
L-GreCo (c) TopK
Figure 7. Throughput for ResNet50/ImageNet. Single node, RTX3090 GPUs.
Forward Backward Communication Compression0100200300400500Time, msUncompressed
PowerSGD
Figure 8. Profiling of the training without compression vs Pow-
erSGD compression, rank 32. Transformer-XL model on WikiText-
103 dataset. Single node, RTX3090 GPUs.
error. However, this method has several drawbacks. First,
the global topK requires proper fine-tuning and search for
hyperparameters in order to converge when low densities
are used. L-GreCo in its turn does not try to minimize the
global compression error – it tries to match it to the com-
pression error of the uniform layer-wise compression that
recovers the accuracy. Also, L-GreCo has lowest possiblecompression parameter. It means that each layer has its
contribution in gradient synchronization. Which might not
be a case for the global topK - at high densities some layers
could be left without updates for several optimization steps.
This has a bad impact on the final model quality. The sec-
ond disadvantage of the global topK is an actual speedup it
might provide. As we discussed in the Section 4, in modern
DataParallel frameworks the gradients are synchronized in
parallel with computation in the sake of the efficiency - one
can hide the communication costs behind the computation.
However, in case of the global topK, as long as one has to
wait till all the gradients are ready, then perform compres-
sion and communication. It might turn out that in this case
we have more non-overlapped communication than in the
layer-wise compression or even in uncompressed case (see
Figure 10).
In order to confirm our doubts about the global topK we have
implemented the algorithm using torch.distributed
hooks and ran the RN18/CIFAR100 training. We found out
that the global topK is 10% slower than L-GreCo when
applied with the similar global density in this case - 0.25%.
Accordion Comparison. Accordion adapts compression
by detecting critical regimes during training. It accepts
two possible compression modes (corresponding to low
and high compression), and has a threshold error parame-

--- PAGE 11 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
0 5 10 15 20 25 30 35
Num steps, x1000102030405060Compression ratio
14.123.9
23.536.9
21.2Uniform 32
Accordion [16:64]
L-GreCo [8:64]Accordion + L-GreCo
CGX-kmeans
(a) Compression ratio over time.
1 2 3 4 5 6 7 8 9
Buckets2468Number of transmitted elements1e6
Uniform
CGX, kmeans
Accordion
L-GreCo
L-GreCo + Accordion (b) Compression per layer group (bucket).
Figure 9. Adaptive compression using L-GreCo versus other methods, for PowerSGD compression on Transformer-XL. The left plot
shows the dynamics of the compression ratio during training, marking the average compression ratio . The right plot presents the
transmitted number of elements per bucket averaged over time. Buckets are in communication order.
B0BackpropagationB1 B2
C(B0|B1|B2)
Compression + Communication
B0 Backpropagation B1 B2
C(B0) Compression + Communication C(B1) C(B2)Time
Timea)
b)
Figure 10. Scheme of communication in case of (a) Global TopK
compression and (b) standard bucket-wise communication. Global
TopK has to collect entire model gradients in buckets before it
compresses and synchronizes them, whereas standard approach
allows to overlap compression and communication with backprop-
agation.
terη. It collects the gradients, and periodically decides
the parameter to use based on gradient information for
each layer. We have implemented Accordion using the
torch.distributed hook, used for PowerSGD. For
the parameter η, we chose the value of 0.5suggested by the
authors and tried to hand-tune the best pair of low and high
compression parameters for each model, with which training
converges to an accuracy that is within MLPerf bounds. We
ran this algorithm on Transformer-XL/Wikitext-103, and
found that the best pair of parameters (in terms of training
time without losing accuracy) are high compression rank
16, and low compression rank 64.
In Figure 9, one can see the dynamics of the average
compression ratio over the training of Accordion and
L-GreCo . We notice that Accordion chooses a low
compression rank for almost all layers during the first pe-
riod of training and a high compression rank for the rest ofthe training time, leading to completely bimodal uniform
compression. This suggests that Accordion may not really
exploit the heterogeneous nature of DNN models. Therefore,
the optimizations of Accordion andL-GreCo , respec-
tively, could be seen as orthogonal :Accordion focuses
on varying the amount of average compression during train-
ing, whereas L-GreCo finds an optimal way of reaching
this average level by setting layer-wise targets.
With this in mind, we combined these two algorithms, as
follows. We first executed Accordion to get the sug-
gested parameters for each layer and used these parameters
as the default set of parameters in L-GreCo algorithm. The
default set of parameters is used to define the maximal er-
ror of the DP algorithm (see line 2 in Algorithm 1). In
this approach, Accordion determines the model sensitiv-
ity to gradient compression at different points in training,
while L-GreCo finds the best mapping of compression pa-
rameters per layer. In Figure 9, we see that the resulting
combination ( L-GreCo with range [8,128] and Accordion
with high=16, low=64) provides superior results in terms of
compression ratio, without sacrificing accuracy.
We also compare the performance of the two algorithms
in isolation (see Table 4). We observe that, despite the
fact that the theoretical compression ratio suggested by
Accordion is essentially the same as that of L-GreCo ,
theAccordion throughput is less by around 30%. This
is explained by the fact that L-GreCo compressed the last
transmitted layers (buckets) to higher levels, leading to
significantly-improved total transmission time. Specifically,
in Figure 9b, we observe that L-GreCo transmits twice
fewer elements in the last bucket relative to Accordion .
Moreover, combining L-GreCo with Accordion im-

--- PAGE 12 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
proves the compression ratio by 50%, and training time
by up to 66% compared to Accordion .
Rethink-GS comparison. Sahu et al. (2021) suggest a
sparcification method which is technically adaptive - hard-
threshold sparsification changes number of transmitted ele-
ments based on the gradient distribution. We have run the
L-GreCo training of ResNet18/CIFAR-100 in the setup de-
scribed in the paper. We used 1% density as a default param-
eter for L-GreCo and the search range was [0.1%,10%] .
ForRethink-GS we used parameter λ= 4.72×10−3.
As a result we saw that L-GreCo improves Rethink-GS
algorithm by 17% in terms of compression ratio ( 6.7×vs
5.7×) while also improving the final accuracy - 71.7% vs
71.4% (the numbers differ from the ones we show in Ta-
ble. 2 as here we used the setup from (Sahu et al., 2021)).
We note that our framework did not require any hyperparam-
eter tuning at all for this experiment, whereas Rethink-GS
requires careful tuning of the hard-threshold λparameter.
CGX Comparison. The adaptive compression of CGX is
based on kmeans and maps layers into a 2-dimensional
space (layer size vs. L2-error). The algorithm clusters lay-
ers into several groups and assigns predefined compression
parameters to the layers in the groups. We have imple-
mented this logic with PowerSGD compression. We used
rank 32 as default, and the best (in terms of compression)
range was from 8 to 64, using 6 layer clusters. The results
are shown in the Table 4. In short, L-GreCo improves upon
the kmeans approach by up to 33%. In Figure 9b, we ob-
serve that L-GreCo picks parameters such that the largest
and the last bucket are compressed the most, whereas the
kmeans algorithm chooses worse compression parameters
for those layers in some iterations.
Overall, L-GreCo improved practical compression rela-
tive to prior techniques. Of note, the highest compression
ratio is achieved by the hybrid Accordion +L-GreCo
method, which leverages layer-wise insights in terms of
both sensitivity and training dynamics.
5.4 Loss-based accuracy metric
As we discussed in the 3 it is possible to use different metric
for measuring the layer sensitivity to compression. The one
we used in the all experiments is error magnitude. The other
possible one is loss-based. in which we collect the model
loss differences between uncompressed training and training
with the gradient compression of certain layers while other
layers stay intact and use the loss difference as a sensitivity
metric.
Comparing these two approaches, we evaluate the correla-
tion coefficients of the metrics these two approaches provide.
We can see in the figure 11a, the metric values from twoapproaches have a high correlation. Moreover, we found
out in our experiments that the compression parameters as
well as the total compression ratio resulted for both metrics
are very close to each other. Hence, given the fact that col-
lecting loss-based metrics requires additional off-training
runs, our usage of error-based metric is justified.
5.5 Timing-based optimization
Considering the fact that communication buckets have dif-
ferent impact on training performance, one may notice that
compression ratio does not always give an expected speed
improvement. Having that in mind we modified L-GreCo
algorithm so that it optimizes the expected time of the com-
munication rather than compression ratio. As we explained
in Section 4, in this approach we measured the communica-
tion time varying compression ratio for each communicated
bucket and trained linear regression model, running the
training for 50 steps with 5 steps warmup for each set of
compression parameters. With the model we get the weights
of the buckets that correspond to a relation between number
of communicated bits per bucket and the communication
time. Then we use those coefficients in the metric we try to
minimize in the Algorithm 1.
We have run the algorithm on our workhorse benchmark -
PowerSGD in Transformer-XL/WikiText-103 training. The
linear model built on the timing data we collected (5000
samples - sets of compression ratios per bucket) has a score
close to 1 meaning that we managed to predict the com-
munication time using communicated bucket sizes almost
perfectly. We found out that the per-bucket coefficients from
the linear model are close to each other (see Figure. 11b). It
means that each bucket’s impact on communication time is
proportional to the bucket size. We noticed that the parame-
ters we get with the modified algorithm are very close to the
parameters from the original L-GreCo algorithm. Given
that we figure that in the case of Transformer-XL/WikiText-
103 the original algorithm gives the most optimal in terms
of timing parameters.
6 C ONCLUSION
We proposed L-GreCo , an adaptive gradient compression
algorithm which automatically identifies optimal layer-wise
compression parameters, given a fixed error constraint. The
L-GreCo algorithm finds the mapping of compression pa-
rameters such that 1) the total L2compression error matches
a target known to recover accuracy, and 2) the total com-
pressed size is minimal for this target.
Our experimental validation across all families of gradient
compression methods show that training with the layer-wise
parameters suggested by L-GreCo recovers the baseline
accuracy while the gradient compression ratio is substan-

--- PAGE 13 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
12 14 16 18 20 22 24 26 28 30
Compression parameter0.00.20.40.60.81.0Correlation coeﬃcient
(a) Metrics correlation
2 4 6 8
Buckets0123456Coeﬃcient×10−6 (b) Coefficients
Figure 11. Correlation coefficients between metric values (a) for PowerSGD using loss-based and error-magnitude approaches as the
sensitivity metrics. Timing coefficients per bucket (b) for timing-based approach. Transformer-XL/WikiText-103 model training with
PowerSGD method.
tially increased. L-GreCo improves training performance
up to 2.5×, saving up to 5.2x communication compared
to vanilla compression, and up to 122x relative to uncom-
pressed training. Overall, our work provides a new approach
for improving existing gradient compression methods, at
almost zero cost in terms of time and accuracy loss.
Our approach suggests that per-layer gradient compression
may be an interesting direction for further work in this
area. Specifically, our method can be augmented with actual
timing information regarding the layer gradient transmission
cost, which could lead to further practical gains. Another
possible extension would be considering hybrid strategies,
which allow for combining different compression techniques
(e.g. sparsification and low-rank) inside the same model.
We proposed L-GreCo , an adaptive gradient compression
algorithm which automatically identifies optimal layer-wise
compression parameters, given a fixed error constraint. The
L-GreCo algorithm finds the mapping of compression pa-
rameters such that 1) the total L2compression error matches
a target known to recover accuracy, and 2) the total com-
pressed size is minimal for this target.
Our approach is complemented by an in-depth exploration
of the “correct” metrics which capture the accuracy and
performance impact of compression, at the per-layer level.
Specifically, we present evidence that minimizing local, per-
layer quantization errors leads to very similar results to
minimizing global metrics such as output loss. Moreover,
maximizing per-layer compression rates correlates very well
to specifically minimizing total transmission time on exist-
ing data-parallel implementation.We complemented these algorithmic and analytic contribu-
tions with an extensive experimental validation, across all
families of gradient compression methods show that training
with the layer-wise parameters suggested by L-GreCo re-
covers the baseline accuracy while the gradient compression
ratio is substantially increased. L-GreCo improves training
performance up to 2.5×, saving up to 5.2x communication
compared to vanilla compression, and up to 122x relative to
uncompressed training. Overall, our work provides a new
approach for improving existing gradient compression meth-
ods, at almost zero cost in terms of time and accuracy loss.
Possible extensions of our method could consider hybrid
strategies, which allow for combining different compression
techniques (e.g. sparsification and low-rank) inside the same
model.
ACKNOWLEDGMENTS
The authors gratefully acknowledge funding from the Euro-
pean Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant
agreement No 805223 ScaleML), as well as experimental
support from the IST Austria IT department, in particular
Stefano Elefante, Andrei Hornoiu, and Alois Schloegl.

--- PAGE 14 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
REFERENCES
Achille, A., Rovere, M., and Soatto, S. Critical learning
periods in deep networks. In International Conference
on Learning Representations , 2018.
Aflalo, Y ., Noy, A., Lin, M., Friedman, I., and Zelnik, L.
Knapsack pruning with inner distillation. arXiv preprint
arXiv:2002.08258 , 2020.
Agarwal, S., Wang, H., Lee, K., Venkataraman, S., and Pa-
pailiopoulos, D. Adaptive gradient communication via
critical learning regime identification. In Smola, A., Di-
makis, A., and Stoica, I. (eds.), Proceedings of Machine
Learning and Systems , volume 3, pp. 55–80, 2021a.
Agarwal, S., Wang, H., Lee, K., Venkataraman, S., and
Papailiopoulos, D. Adaptive gradient communication via
critical learning regime identification. Proceedings of
Machine Learning and Systems , 3:55–80, 2021b.
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and V ojnovic,
M. QSGD: Communication-efficient sgd via gradient
quantization and encoding. Advances in Neural Informa-
tion Processing Systems , 30:1709–1720, 2017.
Chen, C. Y ., Choi, J., Brand, D., Agrawal, A., Zhang, W.,
and Gopalakrishnan, K. Adacomp: Adaptive residual
gradient compression for data-parallel distributed training.
In32nd AAAI Conference on Artificial Intelligence, AAAI
2018 , 32nd AAAI Conference on Artificial Intelligence,
AAAI 2018, pp. 2827–2835, 2018.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In2009 IEEE conference on computer vision and pattern
recognition , pp. 248–255. IEEE, 2009.
Dryden, N., Jacobs, S. A., Moon, T., and Van Essen, B.
Communication quantization for data-parallel training of
deep neural networks. In Proceedings of the Workshop
on Machine Learning in High Performance Computing
Environments , pp. 1–8. IEEE Press, 2016.
Faghri, F., Tabrizian, I., Markov, I., Alistarh, D., Roy, D. M.,
and Ramezani-Kebrya, A. Adaptive gradient quantization
for data-parallel sgd. Advances in neural information
processing systems , 33:3174–3185, 2020.
Frantar, E. and Alistarh, D. SPDY: Accurate pruning with
speedup guarantees. In Chaudhuri, K., Jegelka, S., Song,
L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Pro-
ceedings of the 39th International Conference on Ma-
chine Learning , volume 162 of Proceedings of Machine
Learning Research , pp. 6726–6743. PMLR, 17–23 Jul
2022. URL https://proceedings.mlr.press/
v162/frantar22a.html .Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M.
Error feedback fixes signsgd and other gradient compres-
sion schemes. In International Conference on Machine
Learning , pp. 3252–3261. PMLR, 2019.
Krizhevsky, A. Learning multiple layers of fea-
tures from tiny images. pp. 32–33, 2009. URL
https://www.cs.toronto.edu/ ˜kriz/
learning-features-2009-TR.pdf .
Lim, H., Andersen, D. G., and Kaminsky, M.
3lc: Lightweight and effective traffic compression
for distributed machine learning. arXiv preprint
arXiv:1802.07389 , 2018.
Lin, Y ., Han, S., Mao, H., Wang, Y ., and Dally, W. J.
Deep gradient compression: Reducing the communica-
tion bandwidth for distributed training. arXiv preprint
arXiv:1712.01887 , 2017.
Markov, I., Ramezanikebrya, H., and Alistarh, D. Cgx:
Adaptive system support for communication-efficient
deep learning, 2022. URL https://arxiv.org/
abs/2111.08617 .
Mattson, P., Reddi, V . J., Cheng, C., Coleman, C., Di-
amos, G., Kanter, D., Micikevicius, P., Patterson, D.,
Schmuelling, G., Tang, H., et al. Mlperf: An industry
standard benchmark suite for machine learning perfor-
mance. IEEE Micro , 40(2):8–16, 2020.
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843 , 2016.
Nvidia. Nvidia deep learning examples for tensor cores,
2020. URL https://github.com/NVIDIA/
DeepLearningExamples .
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,
N., Grangier, D., and Auli, M. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of NAACL-
HLT 2019: Demonstrations , 2019.
Ramezani-Kebrya, A., Faghri, F., Markov, I., Aksenov,
V ., Alistarh, D., and Roy, D. M. NUQSGD: Provably
communication-efficient data-parallel sgd via nonuniform
quantization. Journal of Machine Learning Research , 22
(114):1–43, 2021.
Renggli, C., Ashkboos, S., Aghagolzadeh, M., Alistarh,
D., and Hoefler, T. Sparcml: High-performance sparse
communication for machine learning. In Proceedings
of the International Conference for High Performance
Computing, Networking, Storage and Analysis , 2019.

--- PAGE 15 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
Sahu, A., Dutta, A., M. Abdelmoniem, A., Banerjee, T.,
Canini, M., and Kalnis, P. Rethinking gradient sparsifica-
tion as total error minimization. In Advances in Neural
Information Processing Systems , volume 34, pp. 8133–
8146. Curran Associates, Inc., 2021.
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. 1-bit stochas-
tic gradient descent and its application to data-parallel
distributed training of speech dnns. In Fifteenth annual
conference of the international speech communication
association . Citeseer, 2014.
Shen, M., Yin, H., Molchanov, P., Mao, L., Liu, J., and
Alvarez, J. M. Structural pruning via latency-saliency
knapsack. arXiv preprint arXiv:2210.06659 , 2022.
Strom, N. Scalable distributed dnn training using commod-
ity gpu cloud computing. In Sixteenth Annual Conference
of the International Speech Communication Association ,
2015.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tention is all you need. Advances in neural information
processing systems , 30, 2017.
V ogels, T., Karinireddy, S. P., and Jaggi, M. Powersgd:
Practical low-rank gradient compression for distributed
optimization. Advances In Neural Information Processing
Systems 32 (Nips 2019) , 32, 2019.
Wang, H., Sievert, S., Charles, Z., Liu, S., Wright, S., and
Papailiopoulos, D. ATOMO: Communication-efficient
learning via atomic sparsification. 2018.
Wen, W., Xu, C., Yan, F., Wu, C., Wang, Y ., Chen, Y .,
and Li, H. Terngrad: Ternary gradients to reduce com-
munication in distributed deep learning. arXiv preprint
arXiv:1705.07878 , 2017.
Wu, Y .-C., Liu, C.-T., Chen, B.-Y ., and Chien, S.-Y .
Constraint-aware importance estimation for global filter
pruning under multiple resource constraints. In Con-
ference on Computer Vision and Pattern Recognition
(CVPR) Workshops , 2020.
Xu, H., Ho, C.-Y ., Abdelmoniem, A. M., Dutta, A., Bergou,
E. H., Karatsenidis, K., Canini, M., and Kalnis, P. Grace:
A compressed communication framework for distributed
machine learning. In 2021 IEEE 41st international con-
ference on distributed computing systems (ICDCS) , pp.
561–572. IEEE, 2021.

--- PAGE 16 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
A B UCKETS PRIORITIZATION
Considering the fact that communication buckets have dif-
ferent impact on training performance, we modified the
L-GreCo algorithm so that the last buckets in transmission
order, corresponding to the earlier layers, were compressed
more. This compensates for the compression error caused
by picking lower compression parameters for the first buck-
ets, i.e. the last layers. In practical terms, we have added
linear priorities to the layers in Algorithm 1, multiplying
the size of each layer by the index of the bucket the layer is
communicated in. The profile of communicated elements
per buckets is shown in the Figure 12. We observe the linear
shift of higher compression ratios towards the last buck-
ets. However, bucket prioritization has worse performance
than original L-GreCo . It means that the effect of the first
big buckets transmission is higher than the effect of better
compression the last buckets.
1 2 3 4 5 6 7 8 9
Buckets0.51.01.52.0Number of transmitted elements1e6
L-GreCo[8:64]
LGreco[8:64] + Linear
Figure 12. Communicated elements per bucket for L-GreCo and
L-GreCo with linear bucket prioritizing. Transformer-XL with
PowerSGD.
B L OW-RANK ERROR COMPUTATION
As discussed in Section 4, one of the main steps of our algo-
rithm is to compute the error matrix for different possible
compression parameters. Table 1 suggests that this is the
most time-consuming part of our framework. Specifically
for the PowerSGD, we need to compute low-rank errors for
a wide range of ranks. There are two possible solutions to
do so.
B.1 Singular Value Decomposition
The first way to compute errors is to use singular value
decomposition and compute singular values for a particu-
lar layer, and calculate the approximation error for rank
r < min (m, n)by calculating er=qPmin(m,n)
i=r+1σ2
i,Table 5. Hyperparameters for ResNet-18/CIFAR-100
Parameter Value
Number of workers 8
Optimizer SGD with momentum
Global batch size 128
Momentum 0.9
Base LR 0.1
LR decay/10 at epoch 150 and
250
Epochs 200
Weight decay 10−4
which can be done efficiently for all ranks. Specifically,
it is sufficient to compute squared singular values once,
and then compute all the errors by a single matrix product.
Thus, the bottleneck is computing singular values requiring
O(mn·min(m, n))time and O(n2+mn)space.
B.2 Power Iteration Steps
The second approach is to calculate the approximation error
for each rank separately by doing a few power steps (without
the communication parts); as V ogels et al. (2019) claims,
this approach converges to the SVD-suggested matrix. On
the practical side, we have observed that applying only 5
power steps is enough to have a small error relative to the
optimal low-rank approximation suggested by SVD. This
approach needs O(mnr)time and O((m+n)·r)space
for calculating rank r approximation error and therefore
O(mnr2
max)to compute errors for all r∈[rmin, rmax].
B.3 The Best of Both Worlds
Comparing computational complexity and memory require-
ments of two methods suggests it is better to use the power
method when the rank range is small, e.g., ResNet50 on
ImageNet or ResNet18 on Cifar100, and to use the SVD
method when the rank range is large, e.g., TransformerXL
and TransformerLM on WIKITEXT-103.
C C OMBINATION OF POWER SGD AND
L-G RECO
We note that in all of our wide-range experiments, the com-
pression ratio when L-GreCo is applied to PowerSGD gen-
erally increases during the training (see Figure 13). This is
also aligned with the intuition behind the results of (Agarwal
et al., 2021a). This suggests that in this scenario, L-GreCo
is able to increase the compression in the less crucial learn-
ing periods, (e.g.last epochs).

--- PAGE 17 ---
L-GreCo: Layerwise-Adaptive Gradient Compression
10 20 30 40 50 60 70 80 90
Epoch9092949698Compression Ratio
Figure 13. Compression ratio of the scheme suggested by
L-GreCo during the training. ResNet50 with PowerSGD.
Table 6. Hyperparameters on ResNet-50/ImageNet
Parameter Value
Number of workers 8
Optimizer SGD with momentum
Global batch size 2048
Momentum 0.875
LR warmupLinear for 8 epochs,
starting from 0.256
LR schedule cosine
LR decay/10 at epoch 150 and
250
Epochs 90
Weight decay 1/32768
Label smoothing 0.1
Table 7. Hyperparameters on Transformer-XL/WikiText-103
Parameter Value
Number of workers 8
Optimizer LAMB
Global batch size 256
LR warmup Linear for 1000 steps
LR schedule cosine
Number of steps 40k
Weight decay 0.0
D D ETAILED EXPERIMENTAL SETTINGS
For all the experiments we used the standard hyperparame-
ters, datasets and data preprocessing. The detailed hyperpa-
rameters are shown in the tables 5, 6, 7, and 8.
For preprocessing the images of CIFAR-100 datasets, we
follow the standard data augmentation and normalization
routines. Random cropping and horizontal random flipping
were applied for data augmentation. We also normalized
each color with the following mean and standard devia-
tion values for each channel: (0.4914, 0.4822, 0.4465) and
(0.2023, 0.1994, 0.2010).Table 8. Hyperparameters on Transformer-LM/WikiText-103
Parameter Value
Number of workers 8
Optimizer Adam
Adam betas (0.9, 0.98)
Global batch size 2048
LR warmupLinear for 4000 steps
starting from 10−7
LR schedule inverse sqrt
Number of steps 50k
Weight decay 0.01
ResNet50 model uses the following data augmentation. We
perform rand resized crop to 224×224, scale from 8% to
100% and do a random horizontal flip. Also, we do nor-
malization with means (0.485, 0.456, 0.406) and standard
deviations (0.229, 0.224, 0.225).
For wikitext-103 preprocessing we used the standard pre-
processing tools and tokenizers provided by Nvidia Exam-
ples (Nvidia, 2020) and FairSeq library (Ott et al., 2019).
