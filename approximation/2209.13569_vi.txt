# 2209.13569.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2209.13569.pdf
# Kích thước tệp: 345709 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Khám phá Huấn luyện Thứ hạng Thấp của Mạng Nơ-ron Sâu
Siddhartha Rao Kamalakara* 1 2Acyr Locatelli* 2Bharat Venkitesh* 1Jimmy Ba3Yarin Gal4
Aidan N. Gomez1 2 4
Tóm tắt
Huấn luyện mạng nơ-ron sâu ở thứ hạng thấp, tức là với các lớp được phân tích nhân tử, đặc biệt được cộng đồng quan tâm: nó mang lại hiệu quả hơn so với huấn luyện không phân tích nhân tử về cả mức tiêu thụ bộ nhớ và thời gian huấn luyện. Công trình trước đây đã tập trung vào xấp xỉ thứ hạng thấp của các mạng đã được huấn luyện trước và huấn luyện trong không gian thứ hạng thấp với các mục tiêu bổ sung, đưa ra nhiều giải thích ngẫu nhiên cho thực hành được chọn. Chúng tôi phân tích các kỹ thuật hoạt động tốt trong thực tế, và thông qua các phép loại bỏ rộng rãi trên các mô hình như GPT2, chúng tôi cung cấp bằng chứng bác bỏ các niềm tin phổ biến trong lĩnh vực này, gợi ý trong quá trình về những cơ hội nghiên cứu thú vị vẫn cần được giải đáp.

1. Giới thiệu
Các phát triển gần đây trong huấn luyện các mô hình thị giác và ngôn ngữ rất lớn (Brown et al., 2020; Fedus et al., 2021; Dosovitskiy et al., 2020) đã dẫn đến nhu cầu ngày càng tăng về các mô hình huấn luyện hiệu quả. Phân tích nhân tử ma trận thứ hạng thấp của các lớp trong mạng nơ-ron sâu có thể mang lại tăng tốc huấn luyện đáng kể (lên tới 2x) và tiêu thụ ít bộ nhớ hơn so với đối tác không phân tích nhân tử. Trong khi phân tích nhân tử ma trận đã được nghiên cứu rộng rãi trong bối cảnh của mạng tuyến tính và ứng dụng của chúng cho các vấn đề cảm biến ma trận và hoàn thành ma trận, các hiệu ứng của các lớp phân tích nhân tử đối với tối ưu hóa là không tầm thường. Do đó, công trình trước đây trong không gian này chủ yếu tập trung vào huấn luyện thứ hạng thấp với các mục tiêu huấn luyện bổ sung, hoặc liên quan đến việc tính toán xấp xỉ phân tích nhân tử sau huấn luyện. Đã có công trình trước đây hạn chế tập trung vào động lực học huấn luyện cho mạng nơ-ron sâu thứ hạng thấp.

Các đóng góp của chúng tôi: chúng tôi xem xét các phát triển gần đây trong huấn luyện mạng thứ hạng thấp và đặt câu hỏi về các niềm tin hiện tại về lý do tại sao các kỹ thuật như khởi tạo dựa trên phân tích giá trị suy biến (SVD) và điều chỉnh L2 được sửa đổi lại hiệu quả. Chúng tôi bắt đầu với các kỹ thuật khởi tạo dựa trên SVD đã được tìm thấy là hiệu quả trong cả văn học thứ hạng thấp và thưa thớt (Lee et al., 2019). Chúng tôi nhìn vào lý thuyết ma trận ngẫu nhiên để định nghĩa chính thức phân phối của các giá trị suy biến tại khởi tạo trong mạng nơ-ron hiện đại và thách thức các giả định trước đây về tầm quan trọng của chúng. Chúng tôi tiết lộ những hiểu biết thực nghiệm mới về động lực học của các giá trị suy biến trong quá trình huấn luyện của mạng có điều chỉnh L2 và trình bày giả thuyết về lý do tại sao điều chỉnh L2 trên ma trận tái tổ hợp hoạt động tốt hơn so với điều chỉnh L2 trên các nhân tử của nó. Chúng tôi cũng điều tra các niềm tin hiện tại về kích thước bước hiệu quả và mối tương quan của nó với hiệu suất. Hơn nữa, chúng tôi phân tích và trình bày các thí nghiệm với tiền huấn luyện như một chiến lược để huấn luyện mạng thứ hạng thấp hoạt động tốt hơn. Chúng tôi trình bày một loạt các thí nghiệm rộng rãi để hỗ trợ lập luận của chúng tôi và để chứng minh tính hiệu quả và tính thực tế của việc huấn luyện mạng nơ-ron thứ hạng thấp.

100 150 200 250 300 350
Giờ TPU3132333435363738Perplexity
Baseline
Spectral
Spectral Ones
Hình 1. Giờ tính toán TPU so với Hiệu suất của GPT-2 trên LM1B khi mô hình được mở rộng. Mỗi điểm trên đường tương ứng với một kích thước mô hình khác nhau bắt đầu từ 1024 chiều ẩn (ở góc trên bên trái) đến 2560 (ở góc dưới bên phải) với bước tăng 256.

2. Bối cảnh
Hầu hết các công trình trong không gian thứ hạng thấp tập trung vào hiệu quả và tăng tốc đã xem xét xấp xỉ hậu hoc của các mạng đã huấn luyện. (Yu et al., 2017) đã thực hiện một phương pháp không có SVD để tái tạo bản đồ đặc trưng bằng cách tối thiểu hóa một mục tiêu áp đặt cấu trúc thứ hạng thấp thưa thớt. (Jaderberg et al., 2014) cũng xem xét một mạng đã huấn luyện mà trên đó cấu trúc thứ hạng thấp được áp đặt thông qua các mục tiêu tái tạo bộ lọc và dữ liệu. (Tai et al., 2016) tập trung vào huấn luyện thứ hạng thấp của CNN từ đầu; họ đề xuất phân tích nhân tử bộ lọc ngang và dọc của kernel tích chập và chiếu lại vào các vector trực giao tại mỗi bước. Một trong những lý do tại sao công trình trước đây tập trung vào xấp xỉ thứ hạng thấp sau huấn luyện là vì động lực học huấn luyện của mạng nơ-ron được hiểu kém. Hơn nữa, đã được tìm thấy rằng huấn luyện một cách ngây thơ trong không gian thứ hạng thấp từ đầu gặp phải khoảng cách trong hiệu suất - phần 4. Để giải quyết điều này ở một mức độ nào đó, nhiều nỗ lực gần đây đã được thực hiện để hiểu thiên vị ngầm của gradient descent (GD) trong phân tích nhân tử ma trận trong cả mạng tuyến tính và phi tuyến tính. (Arora et al., 2019) điều tra hành vi của GD trong mạng tuyến tính sâu và phát hiện rằng khi độ sâu của phân tích nhân tử tăng, GD có xu hướng tìm các giải pháp thứ hạng thấp. Họ cũng trình bày bằng chứng cho giả thuyết rằng ngôn ngữ của các chuẩn như chuẩn nuclear, chuẩn Frobenius, v.v., có thể không đủ để mô tả hành vi của GD. (Martin & Mahoney, 2018) trình bày phân tích thực nghiệm của các kiến trúc thường được sử dụng và đặc trưng hóa động lực học của GD trong mạng phi tuyến tính sâu theo Phân phối Phổ Thực nghiệm (ESD) và các giai đoạn của huấn luyện. Họ định nghĩa một tập hợp các biện pháp thứ hạng, mà chúng tôi sử dụng trong công trình của mình để phân tích huấn luyện thứ hạng thấp được đặt cạnh nhau với phân tích về huấn luyện không phân tích nhân tử. (Wang et al., 2021) sử dụng huấn luyện thứ hạng thấp với tiền huấn luyện không phân tích nhân tử trong bối cảnh giao tiếp hiệu quả trong môi trường phân tán. (Khodak et al., 2021) đề xuất quy trình huấn luyện thứ hạng thấp bằng cách điều tra khởi tạo và điều chỉnh trong các lớp phân tích nhân tử. Họ phân tích khởi tạo dựa trên SVD (Khởi tạo Phổ) và các tính chất của điều chỉnh L2 mà chúng tôi nghiên cứu độc lập trong công trình của mình. Họ phỏng đoán rằng có sự tương tác giữa chuẩn hóa và suy giảm trọng số và chính thức hóa hành vi này thông qua các phương trình cập nhật phân tích nhân tử.

3. Huấn luyện Thứ hạng Thấp
Trong phần này, chúng tôi trình bày công thức chúng tôi chọn để phân tích nhân tử các lớp. Chúng tôi thảo luận và phê phán các giả định và phỏng đoán liên quan đến công thức thứ hạng thấp trong bối cảnh khởi tạo SVD và điều chỉnh L2.

3.1. Phân tích nhân tử
Trong tất cả các thí nghiệm và phân tích của chúng tôi, chúng tôi phân tích nhân tử ma trận trọng số W tại mỗi lớp thành hai thành phần U và V sao cho W = UV>.

Chúng tôi tập trung vào độ sâu phân tích nhân tử là 2, có xem xét đến sự đánh đổi giữa bộ nhớ và tăng tốc: Khi độ sâu của phân tích nhân tử tại mỗi lớp tăng, nhiều kích hoạt hơn cần được lưu trữ trong bộ nhớ để lan truyền ngược. Độ sâu hai cung cấp tăng tốc trên tất cả các thí nghiệm của chúng tôi trong khi đảm bảo chi phí bộ nhớ kích hoạt tối thiểu.

Xem xét sự khác biệt giữa cập nhật gradient descent vanilla (không phân tích nhân tử) Wt+1 = Wt - η∇W và cập nhật được thực hiện trong cài đặt phân tích nhân tử:

Wt+1 = Ut+1V>t+1
Wt+1 = (Ut - η∇U)(Vt - η∇V)>
Wt+1 = Wt - η(∇WtVtV>t + UtU>t∇Wt)|{z}rt + η²∇WtWt∇W>t (1)

(Khodak et al., 2021) mở rộng phương trình cập nhật trên cho các lớp được chuẩn hóa. Hầu hết các kiến trúc hiện đại dựa vào các lớp chuẩn hóa để huấn luyện mạng khái quát hóa tốt. Điều này bao gồm chuẩn hóa batch (Ioffe & Szegedy, 2015) trong ResNets và chuẩn hóa lớp (Ba et al., 2016) trong Transformers. Chúng tôi giới thiệu người đọc đến (Khodak et al., 2021) để thảo luận chi tiết hơn về loại và vai trò của chuẩn hóa trong các lớp phân tích nhân tử và sử dụng công thức của họ về phương trình cập nhật được chuẩn hóa, được đưa ra bởi

ŵt+1 = ŵt - η ||W||²F (I - mn ŵt ŵt>) vec(r̂t) + O(η²) (2)

trong đó r̂t là rt với các gradient được tính đối với ma trận trọng số được chuẩn hóa Ŵ = W/||W||F và ŵ = vec(Ŵ).

Chúng ta thấy rằng gradient descent trong cài đặt phân tích nhân tử không hoàn toàn phù hợp với cập nhật gradient descent vanilla. Trong các phần tiếp theo, chúng tôi khám phá thực nghiệm và làm việc để vượt qua các thiên vị ngầm của cập nhật phân tích nhân tử này để chúng tôi có thể làm cho huấn luyện thứ hạng thấp trở thành một phương pháp huấn luyện hiệu quả và hiệu quả.

3.1.1. LỚP KẾT NỐI ĐẦY ĐỦ
Cho W ∈ Rm×n là ma trận trọng số của một lớp kết nối đầy đủ. Chúng tôi phân tích nhân tử W như W = UVT với U ∈ Rm×r và VT ∈ Rr×n, trong đó 0 < r ≤ min(m; n). Tại suy luận, khi r < mn/(m+n), phân tích nhân tử ma trận trọng số kết nối đầy đủ dẫn đến giảm dấu chân bộ nhớ cũng như các phép toán điểm nổi (flops) từ O(mn) đến O(mr + rn). Đối với huấn luyện, yêu cầu bộ nhớ thay đổi từ O(mn + n) đến O(mr + rn + n + r) vì chúng ta cần lưu trữ các kích hoạt trung gian để lan truyền ngược.

3.1.2. LỚP TÍCH CHẬP
Chúng tôi phân tích nhân tử kernel tích chập theo cách hỗ trợ viết lại tích chập đơn thành hai tích chập.

Chúng tôi chọn phân tích nhân tử kernel tích chập W ∈ Rh×w×cin×cout như W = UVT với U ∈ Rh×w×cin×r và VT ∈ R1×1×r×cout trong đó h, w đại diện cho chiều cao và chiều rộng kernel tương ứng, cin và cout đại diện cho số kênh đầu vào và đầu ra tương ứng và r đại diện cho thứ hạng của phân tích. Trong phân tích thứ hạng thấp, r ≤ min(hwcin; cout). Điều này dẫn đến giảm flops từ O(hwcincout) đến O(hwcinr + rcout).

3.2. Khởi tạo Phổ
(Khodak et al., 2021) điều tra tính hữu ích của khởi tạo phổ trong các công thức thứ hạng thấp của kiến trúc học sâu và đề xuất một vài giả thuyết về lý do tại sao nó hoạt động. Chúng tôi sử dụng cùng một sơ đồ khởi tạo SVD bị cắt ngắn, được định nghĩa như sau:

SVDr(W) = Û:r Σr V̂>:r, (3)

U = Û:r √r,
V = V̂:r √r,

trong đó W là ma trận có hình dạng N × M, U có hình dạng N × r, V có hình dạng M × r, Σ là ma trận đường chéo của các giá trị suy biến và r là thứ hạng chúng tôi chọn để phân tích nhân tử. Chúng tôi lưu ý rằng U và V là các ma trận hình chữ nhật trừ khi được chỉ định khác.

(Khodak et al., 2021) phân tích khởi tạo dựa trên SVD trong bối cảnh Phương trình cập nhật 1 và cung cấp hai giả thuyết về lý do tại sao kỹ thuật này hoạt động, cả hai chúng tôi đều bác bỏ.

• U0U>0 = V0V>0 = I/r.
Trong bối cảnh thứ hạng thấp, U và V là các ma trận hình chữ nhật thu được từ SVD bị cắt ngắn làm cho U và V trở thành các ma trận trực giao theo cột. Do đó, UU> và VV> không thể bằng I/r và các số hạng ∇WtVtV>t + UtU>t∇Wt trong Phương trình 1 không thể được đơn giản hóa.

• Các giá trị suy biến của một tập hợp Gaussian có tỷ lệ 1/√n được phân phối gần như xung quanh 1.
Chúng tôi nhìn vào lý thuyết Marchenko-Pastur (được mô tả trong Phụ lục A.1) để hiểu phân phối của các giá trị suy biến của ma trận tập hợp Gaussian có kích thước N × M, trong đó nêu rằng phân phối của các giá trị suy biến phụ thuộc vào tỷ lệ của khởi tạo ngẫu nhiên σ² và tỷ lệ kích thước N/M của lớp.

Chúng tôi tin rằng khởi tạo phổ hoạt động vì những lý do khác với những lý do được nêu trong công trình trước đây. Trong Phần 4.1, chúng tôi trình bày một thí nghiệm loại bỏ gợi ý về lý do tại sao sơ đồ khởi tạo này hoạt động tốt hơn.

3.3. Điều chỉnh L2
Nhiều kiến trúc dựa vào điều chỉnh L2 để khái quát hóa tốt hơn. Phương pháp đơn giản để áp đặt điều chỉnh L2 trong mạng phân tích nhân tử là áp dụng hình phạt chuẩn Frobenius cho các nhân tử U và V – tức là, λ/2(||U||²F + ||V||²F). (Srebro & Shraibman, 2005) chỉ ra rằng hình phạt này thực sự tối thiểu hóa chuẩn nuclear của ma trận tái tổ hợp UV>.

Để giải quyết điều này, (Khodak et al., 2021) đề xuất phạt chuẩn Frobenius của ma trận tái tổ hợp UV>, mà họ gọi là suy giảm Frobenius. Họ lập luận rằng suy giảm Frobenius giúp giữ kích thước bước hiệu quả cao trong suốt quá trình huấn luyện trong đó kích thước bước hiệu quả là số hạng η/||W||²F trong Phương trình 2. Chúng tôi chỉ ra, thông qua một nghiên cứu loại bỏ, rằng kích thước bước hiệu quả là một lập luận không đầy đủ để biện minh cho tính hiệu quả của suy giảm Frobenius so với điều chỉnh L2. Chúng tôi chỉ ra rằng động lực học của huấn luyện thứ hạng thấp với điều chỉnh L2 không thể được hiểu chỉ bằng cách xem xét Phương trình cập nhật được chuẩn hóa 2. Điều này bỏ qua các số hạng O(η²) phát sinh từ hình phạt chuẩn Frobenius có tác động không tầm thường đến tối ưu hóa. Chúng tôi thấy rằng tính hiệu quả của suy giảm Frobenius so với điều chỉnh L2 có thể được giải thích tốt hơn bằng cách xem xét thứ hạng hiệu quả của mạng. Chúng tôi sử dụng biện pháp thứ hạng được đề xuất trong (Martin & Mahoney, 2018) định nghĩa thứ hạng hiệu quả của ma trận W là:

||W||*/||W||op.

Tức là, tỷ lệ giữa chuẩn nuclear và chuẩn toán tử. Trong trường hợp của chúng tôi, chúng tôi quan tâm đến thứ hạng hiệu quả của UV>.

3.4. Tiền huấn luyện
Các giai đoạn đầu của huấn luyện được tin rộng rãi là quan trọng cho hiệu suất tốt trong mạng nơ-ron (Achille et al., 2017) (Frankle et al., 2019a). Điều này thúc đẩy chúng tôi khám phá huấn luyện trong một phần của tổng số bước huấn luyện trong không gian không phân tích nhân tử trước khi chuyển sang thay thế thứ hạng thấp của các lớp không phân tích nhân tử này. Chúng tôi áp dụng sơ đồ SVD bị cắt ngắn được mô tả trong Phương trình 3 cho các trọng số đã được huấn luyện một phần để thu được các nhân tử của lớp. Phần 4.3 mô tả tác động của tiền huấn luyện đến hiệu suất trên các thí nghiệm thị giác và ngôn ngữ của chúng tôi và phân tích bản chất của các giải pháp được tìm thấy với tiền huấn luyện khi so sánh với các giải pháp được tìm thấy bởi các mạng thứ hạng thấp được huấn luyện từ đầu (Evci et al., 2019) (Frankle et al., 2019b).

4. Thí nghiệm và Kết quả
Chúng tôi tiến hành các thí nghiệm rộng rãi trên cả mô hình thị giác và ngôn ngữ. Đối với mô hình thị giác, chúng tôi sử dụng Wide-ResNet-28 (Zagoruyko & Komodakis, 2016) trên CIFAR-100 và ResNet-50 (He et al., 2015) trên bộ dữ liệu ImageNet. Đối với tác vụ mô hình hóa ngôn ngữ, chúng tôi tiến hành thí nghiệm trên bộ dữ liệu điểm chuẩn một triệu từ (LM1B) (Chelba et al., 2013) và sử dụng kiến trúc GPT-2 (Radford et al., 2019). Chi tiết về thiết lập thí nghiệm đầy đủ của chúng tôi có thể được tìm thấy trong Phụ lục A.2. Trong các phần sau, chúng tôi so sánh các sơ đồ khởi tạo khác nhau và nghiên cứu các hiệu ứng của điều chỉnh L2 và suy giảm Frobenius. Cuối cùng, chúng tôi chứng minh tính hiệu quả của - và phân tích bản chất của các giải pháp được tìm thấy bởi - tiền huấn luyện.

4.1. Khởi tạo
Chúng tôi chỉ ra rằng khởi tạo phổ mang lại hiệu suất tương đương khi so sánh với các sơ đồ khởi tạo truyền thống. Sau đó, chúng tôi chỉ ra thực nghiệm rằng các giá trị suy biến không đóng vai trò chính trong việc cải thiện hiệu suất và rằng hướng của các vector suy biến mới quan trọng. Phát hiện này trái ngược với niềm tin trước đây (Khodak et al., 2021) về vai trò của các giá trị suy biến trong việc duy trì tỷ lệ khởi tạo. Chúng tôi thiết lập điều này bằng cách đặt các giá trị suy biến thành một trong Phương trình 3. Bảng 2, 3, 4 so sánh kết quả trên các sơ đồ khởi tạo trên CIFAR100, ImageNet và LM1B tương ứng. Chúng tôi quan sát thấy rằng spectral ones dẫn đến độ chính xác tốt hơn trên CIFAR-100, perplexity thấp hơn trên LM1B và hiệu suất tương đương trên ImageNet.

4.2. Điều chỉnh L2
Chúng tôi điều tra giả thuyết kích thước bước hiệu quả bằng cách huấn luyện hai mạng, một với tốc độ học η và cái kia với η/2. Vì vậy, kích thước bước hiệu quả của các mạng này là η/||W||²F và η/(2||W||²F) tương ứng, dựa trên Phương trình 2. Nếu giả thuyết rằng kích thước bước hiệu quả cao hơn dẫn đến hiệu suất tốt hơn là đúng, chúng ta nên thấy rằng việc giảm một nửa kích thước bước hiệu quả sẽ dẫn đến hiệu suất thấp hơn nhưng chúng tôi thấy rằng η/2 dẫn đến các mô hình ít nhất là tốt bằng các mô hình được huấn luyện với tốc độ học η.

Bảng 5, 6 và 7 so sánh tác động của kích thước bước hiệu quả đến hiệu suất trên CIFAR-100, ImageNet và LM1B tương ứng. Phân tích sự tiến hóa của các giá trị suy biến trong mạng được huấn luyện với điều chỉnh L2 và suy giảm Frobenius tiết lộ rằng các giá trị suy biến bị ảnh hưởng không cân xứng trong trường hợp điều chỉnh L2. Chúng tôi quan sát hiện tượng "người giàu trở nên giàu hơn, người nghèo trở nên nghèo hơn" trong các mạng điều chỉnh L2 gây ra thứ hạng hiệu quả ||UV>||*/||UV>||op của mạng giảm xuống vì sự gia tăng không cân xứng trong chuẩn toán tử của mỗi lớp. Chúng tôi báo cáo thứ hạng hiệu quả trung bình (trên các lớp) ở cuối huấn luyện cho các thí nghiệm của chúng tôi trong Bảng 1.

0 2 4 6 8 10
Bước Nội suy01020304050607080Độ chính xác trên Tập Kiểm traThứ hạng Thấp
pretrain: 5
pretrain: 10
pretrain: 15
pretrain: 20
pretrain: 25
pretrain: 30
pretrain: 40
Hình 2. So sánh nội suy của mạng thứ hạng thấp và tiền huấn luyện cho ResNet-50 trên ImageNet với thứ hạng 50%.

Mô hình	Bộ dữ liệu	Suy giảm Frobenius	L2
WRN	CIFAR-100	39.87	16.4
ResNet-50	ImageNet	68.72	58.00
Transformer	LM1B	206.93	205.70
Bảng 1. Các biện pháp thứ hạng hiệu quả cho các mô hình khác nhau

4.3. Tiền huấn luyện
Chúng tôi điều tra tiền huấn luyện mạng trong một phần của tổng số bước huấn luyện và quan sát thấy rằng điều này dẫn đến hiệu suất được cải thiện đáng kể trong các thí nghiệm mô hình ngôn ngữ của chúng tôi như được hiển thị trong Hình 1 và 3 khi chúng tôi mở rộng mô hình. Chúng tôi tiền huấn luyện trong không gian không phân tích nhân tử trong 40.000 bước và tiếp tục huấn luyện trong không gian phân tích nhân tử trong 200.000 bước. Chúng tôi kết hợp tiền huấn luyện với các kỹ thuật nói trên tức là suy giảm Frobenius và tiếp tục với các phân tích thu được từ Spectral và Spectral ones như được mô tả trong 3.4. Chúng tôi thấy rằng tiền huấn luyện không mang lại hiệu suất được cải thiện so với mạng thứ hạng thấp được huấn luyện từ đầu trong các thí nghiệm thị giác của chúng tôi như được hiển thị trong Bảng 8 và 9. Hơn nữa, chúng tôi nhận thấy rằng các giải pháp được tìm thấy với tiền huấn luyện gần hơn trong không gian tham số với các mô hình cơ sở (không phân tích nhân tử) tương ứng của chúng. Chúng tôi chứng minh điều này bằng cách thực hiện nội suy tuyến tính, được hiển thị trong Hình 2, 4 và 5, giữa tiền huấn luyện và trọng số cơ sở bằng cách sử dụng phương trình sau: θ = (1-t)θb + tθl cho t ∈ [0.0; 1.0] với bước tăng 0.1 trong đó t là hệ số nội suy, θb là tham số từ mô hình cơ sở và θl là tham số từ mô hình thứ hạng thấp với tiền huấn luyện.

5. Kết luận
Chúng tôi chứng minh thực nghiệm rằng khởi tạo Spectral và điều chỉnh L2 trên UV> cải thiện huấn luyện thứ hạng thấp nhưng được hiểu kém. Chúng tôi trình bày phân tích giá trị suy biến và các nghiên cứu loại bỏ hoạt động như các phản ví dụ cho niềm tin trước đây về lý do tại sao các kỹ thuật này hoạt động. Chúng tôi hy vọng đưa ra những lý do lý thuyết đằng sau tính hiệu quả của các kỹ thuật này trong công trình tương lai. Ngoài ra, chúng tôi chứng minh tiền huấn luyện như một chiến lược hiệu quả để cải thiện hiệu suất thứ hạng thấp và trình bày những hiểu biết về bản chất của các giải pháp được tìm thấy bởi các mạng với tiền huấn luyện.

Tài liệu tham khảo
Achille, A., Rovere, M., and Soatto, S. Critical learning periods in deep neural networks. CoRR, abs/1711.08856, 2017. URL http://arxiv.org/abs/1711.08856.

Arora, S., Cohen, N., Hu, W., and Luo, Y. Implicit regularization in deep matrix factorization, 2019.

Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization, 2016.

Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020.

Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., and Koehn, P. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020. URL https://arxiv.org/abs/2010.11929.

Evci, U., Pedregosa, F., Gomez, A. N., and Elsen, E. The difficulty of training sparse neural networks. CoRR, abs/1906.10732, 2019. URL http://arxiv.org/abs/1906.10732.

Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/abs/2101.03961.

Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. The lottery ticket hypothesis at scale. CoRR, abs/1903.01611, 2019a. URL http://arxiv.org/abs/1903.01611.

Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Linear mode connectivity and the lottery ticket hypothesis. CoRR, abs/1912.05671, 2019b. URL http://arxiv.org/abs/1912.05671.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift, 2015.

Jaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up convolutional neural networks with low rank expansions, 2014.

Khodak, M., Tenenholtz, N. A., Mackey, L., and Fusi, N. Initialization and regularization of factorized neural layers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=KTlJT1nof6d.

Lee, N., Ajanthan, T., Gould, S., and Torr, P. H. S. A signal propagation perspective for pruning neural networks at initialization. CoRR, abs/1906.06307, 2019. URL http://arxiv.org/abs/1906.06307.

Martin, C. H. and Mahoney, M. W. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.

Srebro, N. and Shraibman, A. Rank, trace-norm and max-norm. In Auer, P. and Meir, R. (eds.), Learning Theory, pp. 545–560, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg. ISBN 978-3-540-31892-7.

Tai, C., Xiao, T., Zhang, Y., Wang, X., and E, W. Convolutional neural networks with low-rank regularization, 2016.

Wang, H., Agarwal, S., and Papailiopoulos, D. Pufferfish: Communication-efficient models at no extra cost, 2021.

Yu, X., Liu, T., Wang, X., and Tao, D. On compressing deep models by low rank and sparse decomposition. pp. 67–76, 2017. doi: 10.1109/CVPR.2017.15.

Zagoruyko, S. and Komodakis, N. Wide residual networks. CoRR, abs/1605.07146, 2016. URL http://arxiv.org/abs/1605.07146.

--- TRANG 6 ---
Khám phá Huấn luyện Thứ hạng Thấp của Mạng Nơ-ron Sâu

A. Phụ lục

A.1. Lý thuyết Marchenko-Pastur
Lý thuyết Marchenko-Pastur (MP) định nghĩa phân phối của các giá trị suy biến của ma trận ngẫu nhiên Gaussian trong giới hạn vô cùng nhưng có thể áp dụng cho ma trận hữu hạn với các ràng buộc sai số rất hợp lý. Lý thuyết MP định nghĩa phân phối là:

ρ(λ) = { (N/2πσ²)√((λ₊ - λ)(λ - λ₋)) nếu λ ∈ [λ₋; λ₊]
         { 0 nếu khác                                           (4)

λ± = σ²(1 ± √(M/N))², (5)

A.2. Chi tiết Thí nghiệm
Đối với tác vụ mô hình hóa ngôn ngữ, chúng tôi tiến hành thí nghiệm trên bộ dữ liệu điểm chuẩn một triệu từ (LM1B) (Chelba et al., 2013) và sử dụng thiết lập sau: độ dài chuỗi đầu vào được cố định ở 256 và 1152 token cho huấn luyện và đánh giá tương ứng và kích thước từ vựng được giới hạn ở 32K từ con và huấn luyện tất cả các mô hình đến 240K bước. Chúng tôi triển khai mô hình ngôn ngữ transformer trên Tensorflow và chạy tất cả các thí nghiệm trên cloud TPU. Để có tiết kiệm tốt hơn về tính toán và bộ nhớ, chúng tôi kết hợp việc tạo query, key, value thành một ma trận trọng số. Đối với mỗi lớp transformer, chúng tôi phân tích ba phép toán ma trận; tạo Q, K, V và hai lớp kết nối đầy đủ. Chúng tôi bỏ qua việc phân tích nhân tử lớp chiếu đầu ra và lớp kết hợp kết hợp các đầu ra của attention (đây là ma trận vuông và chúng tôi chỉ thấy lợi ích bộ nhớ và tính toán cho các thứ hạng rất nhỏ). Đối với tất cả các lần chạy transformer, chúng tôi chọn thứ hạng 62,5% và giảm một nửa tốc độ học cơ sở. Đối với tiền huấn luyện, chúng tôi huấn luyện không phân tích nhân tử trong 40K bước sau đó chuyển sang huấn luyện phân tích nhân tử thứ hạng thấp trong 200K bước còn lại và giảm một nửa tốc độ học.

Đối với tác vụ phân loại hình ảnh, chúng tôi tiến hành thí nghiệm với CIFAR-100 và ImageNet. Đối với CIFAR-100, chúng tôi sử dụng phân chia huấn luyện/kiểm tra tiêu chuẩn với sơ đồ tăng cường đơn giản - Random Crop và Horizontal Flips. Chúng tôi huấn luyện WideResNet-28 (Zagoruyko & Komodakis, 2016) trong 200 epoch với SGD với momentum (0.9) và kích thước batch 128. Đối với điều chỉnh, chúng tôi sử dụng hệ số suy giảm trọng số 5e-4 và không dropout. Đối với các lần chạy huấn luyện thứ hạng thấp, chúng tôi phân tích nhân tử mọi lớp tích chập ngoài lớp đầu tiên theo sơ đồ phân tích nhân tử được mô tả ở trên và thứ hạng được chọn. Đối với các thí nghiệm ImageNet, chúng tôi sử dụng kiến trúc ResNet-50 tiêu chuẩn và huấn luyện trên TPU v2-8 với kích thước batch mỗi core là 128 và tuân theo cùng các siêu tham số và lịch trình tốc độ học được mô tả trong (He et al., 2015).

A.3. Kết quả Khởi tạo
Thứ hạng	Khởi tạo	Độ chính xác
Baseline (N/A)	He	81.08
0.1	He	77.94
	spectral	79.84
	spectral ones	79.07
0.2	He	80.37
	spectral	81.35
	spectral ones	81.27
0.3	He	80.87
	spectral	81.53
	spectral ones	81.61
Bảng 2. Kết quả khởi tạo của Wide Resnets trên Cifar-100

Thứ hạng	Khởi tạo	Top-1	Top-5
Baseline (N/A)	He	76.39	93.21
0.3	He	75.26	92.56
	spectral	75.77	92.87
	spectral ones	75.71	92.82
0.5	He	75.97	92.84
	spectral	76.13	93.09
	spectral ones	75.98	92.97
Bảng 3. Kết quả khởi tạo của ResNet trên Image Net

Thứ hạng	Khởi tạo	Perplexity
Baseline (N/A)	He	37.67
0.62	He	39.6
	spectral	38.78
	spectral ones	38.47
Bảng 4. Kết quả khởi tạo của Transformers trên LM1B

A.4. Kết quả Điều chỉnh
Thứ hạng	Điều chỉnh	Tỷ lệ lr	Độ chính xác
0.1	L2	0.5	73.12
		1.0	72.59
	Suy giảm Frobenius	0.5	79.84
		1.0	79.79
0.2	L2	0.5	78.22
		1.0	77.56
	Suy giảm Frobenius	0.5	81.35
		1.0	81.61
Bảng 5. So sánh giữa Suy giảm Frobenius và điều chỉnh L2 trên Cifar-100

--- TRANG 7 ---
Khám phá Huấn luyện Thứ hạng Thấp của Mạng Nơ-ron Sâu

Thứ hạng	Điều chỉnh	Tỷ lệ lr	Top-1	Top-5
0.3	L2	0.5	75.11	92.42
		1.0	74.9	92.24
	Suy giảm Frobenius	0.5	75.22	92.49
		1.0	75.77	92.87
0.5	L2	0.5	75.04	92.36
		1.0	74.83	92.25
	Suy giảm Frobenius	0.5	75.97	92.85
		1.0	76.13	93.09
Bảng 6. So sánh giữa Suy giảm Frobenius và điều chỉnh L2 trên Imagenet

Thứ hạng	Điều chỉnh	Tỷ lệ lr	Perplexity
0.62	L2	0.5	38.87
		1.0	39.01
	Suy giảm Frobenius	0.5	38.78
		1.0	39.2
Bảng 7. So sánh giữa Suy giảm Frobenius và điều chỉnh L2 trên LM1B

A.5. Kết quả Tiền huấn luyện
Thứ hạng	Epoch Tiền huấn luyện	Độ chính xác
0.2	0	81.35
	15	81.33
	30	81.56
	40	81.53
	50	81.39
	75	81.53
0.3	0	81.53
	15	81.73
	30	81.51
	40	81.67
	50	82.0
	75	81.44
Bảng 8. Kết quả tiền huấn luyện cho Wide ResNets trên CIFAR-100

Thứ hạng	Số epoch Tiền huấn luyện	Top-1	Top-5
0.5	5	76.07	92.88
	10	75.96	93.04
	15	76.12	92.96
	20	76.08	92.94
	25	76.15	93.00
	30	76.05	92.9
	35	76.24	93.06
	40	76.21	93.09
	45	76.29	93.12
Bảng 9. Kết quả tiền huấn luyện cho ResNet50 trên ImageNet

100 200 300 400 500
Tổng Tham số (Triệu)3132333435363738Perplexity
Baseline
Spectral
Spectral Ones
Hình 3. Tổng tham số so với Hiệu suất của GPT-2 trên LM1B khi mô hình được mở rộng. Mỗi điểm trên đường tương ứng với một kích thước mô hình khác nhau bắt đầu từ 1024 chiều ẩn (ở góc trên bên trái) đến 2560 (ở góc dưới bên phải) với bước tăng 256.

0 2 4 6 8 10
Bước Nội suy01020304050607080Độ chính xác trên Tập Kiểm traThứ hạng Thấp
pretrain: 10
pretrain: 15
pretrain: 20
pretrain: 30
pretrain: 40
pretrain: 50
pretrain: 75
Hình 4. So sánh nội suy của mạng thứ hạng thấp và tiền huấn luyện cho WideResNet-28 trên CIFAR-100 với thứ hạng 30%.

0 2 4 6 8 10
Bước Nội suy8
7
6
5
4
Log Likelihood
Thứ hạng Thấp
Pretrain: 40K
Pretrain: 120K
Hình 5. So sánh nội suy của mạng thứ hạng thấp và tiền huấn luyện cho transformer LM.
