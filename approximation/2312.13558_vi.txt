# 2312.13558.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2312.13558.pdf
# Kích thước tệp: 4671668 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Sự Thật Có Ở Đó: Cải Thiện Khả Năng Suy Luận trong Mô Hình Ngôn Ngữ
với Giảm Hạng Chọn Lọc Theo Lớp
Pratyusha Sharma1Jordan T. Ash2,⋆Dipendra Misra2,⋆
MIT1Microsoft Research NYC2
(⋆hướng dẫn ngang nhau)
pratyusha@mit.edu, {ash.jordan, dimisra}@microsoft.com
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên Transformer đã trở thành một vật cố định trong học máy hiện đại.
Tương ứng, các tài nguyên đáng kể được phân bổ cho nghiên cứu nhằm tiến bộ hơn nữa công nghệ này, thường dẫn đến các mô hình có kích thước ngày càng tăng được đào tạo trên lượng dữ liệu ngày càng nhiều.
Tuy nhiên, công trình này chứng minh kết quả đáng ngạc nhiên rằng thường có thể cải thiện đáng kể hiệu suất của LLM bằng cách loại bỏ có chọn lọc các thành phần bậc cao hơn1 của ma trận trọng số của chúng. Can thiệp đơn giản này, mà chúng tôi gọi là Giảm hạng chọn lọc theo LỚp (LASER), có thể được thực hiện trên một mô hình sau khi hoàn thành đào tạo, và không yêu cầu thêm tham số hoặc dữ liệu. Chúng tôi trình bày các thí nghiệm mở rộng chứng minh tính tổng quát của phát hiện này trên các mô hình ngôn ngữ và tập dữ liệu, và cung cấp phân tích sâu mang lại hiểu biết về cả khi nào LASER hiệu quả và cơ chế mà nó hoạt động2.
1 Giới thiệu
Kể từ khi phát hành ban đầu, các LLM dựa trên Transformer đã được chứng minh là thành thạo đáng kể trên một loạt rộng các nhiệm vụ học máy quan trọng. Kiến trúc Transformer cơ bản của chúng đã trở thành tiên tiến nhất cho việc mô hình hóa và suy luận về ngôn ngữ tự nhiên, và đã cho thấy hứa hẹn trong các lĩnh vực như thị giác máy tính [Dosovitskiy et al., 2020] và học tăng cường [Chen et al., 2021] cũng vậy.
Các hiện thân đương đại của kiến trúc Transformer nổi tiếng là lớn, thường yêu cầu tài nguyên tính toán to lớn cho cả đào tạo và suy luận. Điều này là do thiết kế, vì các Transformer được đào tạo với nhiều tham số hoặc dữ liệu hơn có thể chứng minh được có khả năng hơn so với các tiền nhiệm mảnh mai hơn—thường là một khoảng cách đáng kể [Brown et al., 2020, Touvron et al., 2023]. Tuy nhiên, một tập hợp công trình ngày càng tăng cho thấy rằng các mô hình dựa trên Transformer, và mạng neural nói chung, không yêu cầu tất cả các tham số được khớp để giữ lại các giả thuyết đã học. Mặc dù có vẻ hữu ích khi có quá nhiều tham số tại thời điểm đào tạo [Hinton et al., 2015, Bengio et al., 2005], nhưng đã được biết rằng các mô hình này có thể được cắt tỉa drastically trước khi suy luận; mạng neural thường có thể loại bỏ hơn 90% trọng số của chúng mà không có bất kỳ sự suy giảm đáng kể nào trong hiệu suất [Frankle and Carbin, 2018]. Việc khám phá hiện tượng này đã thúc đẩy sự quan tâm xung quanh mối quan hệ giữa tổng quát hóa và quá tham số hóa [Zhang et al., 2017], và sinh ra nghiên cứu trong việc phát triển các chiến lược cắt tỉa phù hợp với suy luận mô hình hiệu quả [Molchanov et al., 2016].
Bài báo này trình bày một phát hiện đáng ngạc nhiên, rằng việc cắt tỉa cẩn thận được thực hiện tại các lớp cụ thể của mô hình Transformer có thể tạo ra những tăng cường đáng kể trong hiệu suất trên một số nhiệm vụ. Chúng tôi mô tả Giảm hạng chọn lọc theo LỚp (LASER), một can thiệp loại bỏ các thành phần bậc cao hơn của ma trận trọng số đã học như được xác định bởi phân tích giá trị đơn. Việc giảm này được thực hiện trong các ma trận trọng số và lớp cụ thể của mô hình Transformer. Phù hợp với công trình trước đó, chúng tôi thấy rằng nhiều ma trận như vậy có thể được giảm đáng kể,
1Các thành phần bậc cao hơn là các vectơ đơn với giá trị đơn nhỏ hơn.
2Mã và trang web: https://pratyushasharma.github.io/laser/
1arXiv:2312.13558v1  [cs.LG]  21 Dec 2023

--- TRANG 2 ---
Lớp Tuyến tínhLớp Tự Chú ýPhép Cộng Điểm-điểmKích hoạt Phi tuyếnLớp L WWLRMLP Tự Chú ýCapital
is Japan=LASER   Thay thế W trong các lớp cụ thể bằng xấp xỉ hạng thấp WLRcủa nó+++++++++++++++++++++++++
UΣV⊤
TokyoHình 1: Giảm hạng chọn lọc theo LỚp (LASER) thay thế một ma trận trọng số cụ thể W của mô hình Transformer bằng xấp xỉ hạng-k của nó, WLR, và quan sát sự thay đổi trong hành vi của mô hình. Chúng tôi thấy rằng xấp xỉ hạng này, đặc biệt cho trọng số MLP trong các lớp sau của mô hình, thường mang lại lợi ích đáng ngạc nhiên cho hiệu suất mô hình.
và sự suy giảm hiệu suất thường không được quan sát cho đến khi hơn 90% các thành phần được loại bỏ hoàn toàn.
Tuy nhiên, khác với những gì được tìm thấy trong công trình trước đó, chúng tôi thấy rằng những giảm này có thể tạo ra những cải thiện drastic trong độ chính xác, như được đo bằng các điểm chuẩn suy luận được nghiên cứu kỹ trong NLP. Thậm chí tốt hơn, khám phá này dường như không giới hạn ở ngôn ngữ tự nhiên, với những tăng hiệu suất cũng được tìm thấy trong học tăng cường.
Bài báo này phân tích mối quan hệ giữa dữ liệu đào tạo của mô hình và các mẫu có lợi từ LASER. Chúng tôi thấy rằng những cải thiện trong hiệu suất của mô hình chủ yếu đến từ thông tin ít hiện diện thường xuyên hơn trong tập dữ liệu đào tạo của mô hình, cho thấy rằng LASER cung cấp một loại thủ tục khử nhiễu làm cho các sự thật được học yếu trở nên có thể truy cập. Chúng tôi riêng biệt quan sát rằng LASER mang lại khả năng chống chịu tăng cường đối với việc diễn đạt lại trên các câu hỏi đúng trước đó.
Ngoài ra, chúng tôi cố gắng suy luận về những gì được lưu trữ trong các thành phần bậc cao, sao cho việc loại bỏ chúng tăng cường hiệu suất. Đối với các câu hỏi được trả lời đúng chỉ sau LASER, trong sự vắng mặt của can thiệp, mô hình gốc chủ yếu phản hồi với các từ tần suất cao như "the," "of," v.v.—các generation không thậm chí cùng loại ngữ nghĩa với câu trả lời đúng. Tuy nhiên, sau một lượng giảm hạng, câu trả lời của mô hình chuyển thành đúng. Để hiểu điều này, chúng tôi xem xét những gì các thành phần còn lại mã hóa riêng lẻ; chúng tôi xấp xỉ ma trận trọng số chỉ sử dụng các vectơ đơn bậc cao hơn của nó. Chúng tôi thấy rằng các thành phần này mô tả hoặc một phản hồi khác của cùng loại ngữ nghĩa với câu trả lời đúng hoặc các từ tần suất cao chung chung. Có vẻ như, khi các thành phần bậc cao hơn có nhiễu được kết hợp với các thành phần bậc thấp, các phản hồi xung đột của chúng tạo ra một loại "câu trả lời trung bình," có khả năng là không chính xác.
Hình 1 trực quan hóa kiến trúc Transformer và thủ tục được theo dõi bởi LASER. Ở đây, ma trận trọng số của Mạng Đa-Lớp (MLP) tại một lớp cụ thể được thay thế bằng xấp xỉ hạng thấp của nó.
2 Công trình liên quan
Theo hiểu biết của chúng tôi, bài báo này là đầu tiên xác định rằng việc giảm hạng được chọn cẩn thận có thể tăng cường hiệu suất Transformer. Tuy nhiên, có một loạt rộng các công trình nghiên cứu các câu hỏi liên quan, bao gồm cách các sự thật được lưu trữ trong LLM và cách nén tốt nhất mạng neural.
Cách các sự thật được lưu trữ. Các nghiên cứu thăm dò biểu diễn mô hình cho sự hiện diện của các thuộc tính chọn lọc của thực thể [Ettinger et al., 2016, Adi et al., 2016, Hupkes et al., 2018, Conneau et al., 2018] cho thấy rằng các mô hình lưu trữ
2

--- TRANG 3 ---
thông tin thực tế qua các lớp khác nhau và Lee et al. [2023] cho thấy rằng khả năng chống chịu của mô hình đối với sự dịch chuyển phân phối có thể được cải thiện bằng cách tinh chỉnh các lớp chọn lọc. Tuy nhiên, có bằng chứng mâu thuẫn về cách thông tin này được tổ chức và sử dụng trong việc xây dựng câu trả lời trong các mô hình ngôn ngữ lớn. Một số lý thuyết cho rằng thông tin về các thực thể khác nhau được lưu trữ cục bộ như bộ nhớ key-value hai lớp trong các phần MLP của mô hình Transformer [Geva et al., 2021], sau đó được sao chép qua các lớp sau bởi các mô-đun tự chú ý [Elhage, 2021]. Meng et al. [2022] đề xuất một thủ tục để theo dõi và chỉnh sửa thông tin cục bộ, cụ thể thực thể để ánh xạ tới các đầu ra "không thể" riêng biệt, hỗ trợ lý thuyết địa phương. Những lý thuyết này được hỗ trợ thêm bởi hiện tượng "thoát sớm," nơi biểu diễn tại một lớp trung gian có thể được sử dụng trực tiếp với đầu cuối của mô hình để tạo ra đầu ra chính xác [Zhao et al., 2021]. Ngược lại, Hase et al. [2023] đã quan sát rằng thông tin về một số thực thể hoặc quan hệ thực thể giống nhau có thể được sửa đổi bằng cách chỉnh sửa nhiều lớp trong kiến trúc mô hình, và do đó, các sự thật được lưu trữ qua các lớp theo cách phân mảnh. Bài báo này không đưa ra tuyên bố cụ thể nào về tính địa phương, mà thay vào đó chứng minh rằng các thành phần bậc cao hơn của ma trận trọng số có thể gây nhiễu trong việc ra quyết định, và chỉ xem xét các thành phần bậc thấp hơn có thể làm cho câu trả lời đúng trở nên có thể truy cập.
Nén mô hình. Các phương pháp cắt tỉa mạng neural đã tìm thấy rằng các mô hình có thể được cắt tỉa đáng kể (thường loại bỏ hơn 90% tham số) với rất ít sụt giảm độ chính xác, giảm đáng kể yêu cầu lưu trữ của mô hình [LeCun et al., 1989, Hassibi and Stork, 1992, Han et al., 2015, Li et al., 2016, Frankle and Carbin, 2018]. Cũng có những phương pháp cắt tỉa các mô hình này theo cách có cấu trúc, để tạo điều kiện cho việc cải thiện thời gian suy luận [Molchanov et al., 2016]. Sự tồn tại của các mạng con thưa [Frankle and Carbin, 2018, Hoefler et al., 2021] đã được tìm thấy là đúng cho các mô hình tích chập, kết nối đầy đủ và Transformer [Lv et al., 2023, Murty et al., 2022]. Trong khi Jin et al. [2022] thấy rằng tổng quát hóa mô hình có thể được cải thiện bằng cách cắt tỉa và sau đó khớp lại tham số, những cải thiện tổng quát hóa chỉ được quan sát khi đào tạo lại mô hình. Theo hiểu biết của chúng tôi, các kỹ thuật cắt tỉa mô hình luôn thực hiện giảm đơn phương qua tất cả tham số, mà không nhắm mục tiêu bất kỳ lớp cụ thể nào—dẫn đến hiệu suất dự đoán hoặc giữ nguyên hoặc giảm [Frankle and Carbin, 2018]. Tuy nhiên, trong công trình này, chúng tôi thấy rằng tác động của việc giảm độ chính xác không đồng nhất qua các loại lớp khác nhau, và tổng quát hóa của mô hình có thể được cải thiện chỉ bằng cắt tỉa chọn lọc; không cần đào tạo thêm. Đại thể, chúng tôi thấy rằng suy giảm hiệu suất có thể được tạo ra bằng cách giảm hạng các lớp đầu, trong khi những lợi ích hiệu suất đáng kể thường có sẵn bằng cách cắt tỉa các lớp sau.
Xấp xỉ hạng thấp của ma trận trọng số. Hầu hết các phương pháp cắt tỉa giảm tham số theo thứ tự độ lớn tuyệt đối của chúng [Frankle and Carbin, 2018]. Tuy nhiên, một phương pháp thay thế là giảm hạng của các ma trận trọng số thành phần của nó, giữ k thành phần hàng đầu được tìm thấy bởi SVD. Trong khi các ma trận của mô hình neural, bao gồm cả mô hình Transformer, đã được tìm thấy được xấp xỉ tốt bằng cách sử dụng phương pháp này, nơi các phiên bản giảm rõ rệt của mô hình có thể bảo tồn hành vi của nó, nghiên cứu đã cho thấy rằng hiệu suất cuối cùng giảm khi mức độ nghiêm trọng của can thiệp tăng [Lv et al., 2023, Hajimolahoseini et al., 2021, Yu et al., 2017]. Lưu ý rằng những giảm này thường được thực hiện đơn phương, loại bỏ cùng số lượng thành phần trong mọi ma trận trọng số trong mô hình. Ngược lại với những phát hiện này, chúng tôi cho thấy rằng việc giảm hạng có mục tiêu, thậm chí chỉ ảnh hưởng đến một ma trận trọng số duy nhất, có thể mang lại lợi ích cho độ chính xác dự đoán của Transformer.
Chưng cất mô hình và đào tạo hạng thấp. Ba and Caruana [2014] và Hinton et al. [2015] đã đào tạo các mạng nhỏ hơn để bắt chước hành vi của các mạng lớn hơn, gợi ý rằng các mạng neural có thể được tham số hóa quá mức đáng kể và có thể được thay thế bằng các lựa chọn thay thế gọn gàng hơn. Theo hiểu biết của chúng tôi, không có báo cáo nào về việc cải thiện trong dự đoán của mô hình như một hệ quả của thủ tục này đã được hiển thị. [Yang et al., 2020] đã thực thi tính hạng thấp của ma trận trọng số cho mục đích hiệu quả bộ nhớ, nhưng các mô hình kết quả không đạt được hiệu suất tương đương với các đối tác quá tham số hóa của chúng. Kết quả gợi ý rằng quá tham số hóa hữu ích cho việc xác định các tham số tổng quát hóa tốt bởi SGD [Bengio et al., 2005, Hinton et al., 2015, Zhang et al., 2017].
3

--- TRANG 4 ---
3 Sơ bộ
Ở đây chúng tôi xem xét ký hiệu cơ bản và mô tả các thành phần cốt lõi của nghiên cứu của chúng tôi.
Ký hiệu Toán học. Chúng tôi sử dụng R để biểu thị số thực, N để biểu thị số tự nhiên, các chữ cái nhỏ như v∈Rd để biểu thị một vectơ d chiều, và các chữ cái viết hoa như W∈Rm×n để biểu thị một ma trận kích thước m×n. Chúng tôi sử dụng ∥v∥2 để biểu thị chuẩn Euclidean của vectơ v và ∥W∥2 để biểu thị chuẩn phổ của ma trận W. Chúng tôi sử dụng [N] để biểu thị tập hợp {1,2,···, N}. Chúng tôi sẽ sử dụng rank(W) để biểu thị hạng của ma trận W và σ↓i(W) để biểu thị giá trị đơn lớn thứ i của nó.
Kiến trúc Transformer. Chúng tôi cung cấp một mô tả ngắn gọn về kiến trúc Transformer vani có liên quan đến phân tích của chúng tôi. Một kiến trúc Transformer có thể được nghĩ như L lớp của các khối Transformer.
Khối thứ l ánh xạ một chuỗi vectơ độ dài T (h(l−1)1,···, h(l−1)T) thành một chuỗi vectơ độ dài T khác (h(l)1,···, h(l)T), nơi tất cả vectơ đều d chiều. Biến đổi này được thực hiện bằng cách sử dụng hai bước tuần tự: một cơ chế tự chú ý để trộn thông tin qua các bước thời gian, và một mạng feed-forward để xử lý thông tin trong mỗi bước thời gian. Chúng tôi mô tả một phiên bản cơ bản của những biến đổi này cho một lớp thứ l cố định và bỏ chỉ số trên (l−1) để rõ ràng.3
Một cơ chế tự chú ý đơn đầu trước tiên ánh xạ mỗi vectơ hi thành một vectơ truy vấn qi=Wqhi, một vectơ khóa ki=Wkhi và một vectơ giá trị vi=Wvhi nơi Wq, Wk, Wv∈Rd×d là các ma trận trọng số cụ thể lớp.
Sau đó chúng tôi tính toán xác suất chú ý p(j|i) = exp(q⊤ikj/√d)PT l=1exp(q⊤ikl/√d) cho mọi i, j∈[T]. Những này được sử dụng để tính toán vectơ chú ý zi=PT j=1p(j|i)vj. Một tự chú ý k-đầu tính toán một tập hợp k vectơ chú ý bằng cách sử dụng các biến đổi tuyến tính khác nhau cho khóa, truy vấn và giá trị, và sau đó nối những vectơ chú ý này. Những k biến đổi tuyến tính riêng biệt này cho khóa, truy vấn và giá trị đều có thể được hấp thụ vào các ma trận tương ứng của chúng Wq∈Rd×dk, Wk∈Rd×dk và Wv∈Rd×dk. Cuối cùng, cơ chế tự chú ý xuất ra ui=ziWo+hi bằng cách sử dụng ma trận chiếu Wo∈Rdk×d.
Bước feed-forward áp dụng một perceptron đa lớp 2 lớp (MLP) ψ:Rd→Rd cho mỗi vectơ ui∈Rd riêng biệt. MLP thường có hàm kích hoạt ReLU hoặc GELU [Hendrycks and Gimpel, 2016] và trong một số mô hình như Llama, bias của các lớp tuyến tính được đặt thành 0. Chúng tôi biểu thị các ma trận trọng số của lớp tuyến tính thứ nhất và thứ hai của MLP này bằng Uin và Uout tương ứng. Đầu ra của khối Transformer thứ l này sau đó được cho bởi h(l)i=ψ(ui) +ui.
Tóm lại, một kiến trúc Transformer có các ma trận trọng số sau W={Wq, Wk, Wv, Wo, Uin, Uout} cho mỗi lớp, ngoài ma trận nhúng để nhúng token đầu vào, một ma trận trọng số chiếu được áp dụng sau lớp cuối trước khi lấy softmax, và tất cả ma trận trọng số liên quan đến chuẩn hóa lớp.
Trong công trình của chúng tôi, chúng tôi sẽ tập trung chủ yếu vào các ma trận trong W và can thiệp bằng cách sửa đổi chúng.
Xấp xỉ Hạng-r và SVD. Cho một ma trận W∈Rm×n và r∈N, một bài toán xấp xỉ hạng-r yêu cầu tìm một ma trận ˆW để tối thiểu hóa ∥W−cW∥2 và thỏa mãn rank(cW) ≤r. Định lý Eckart–Young–Mirsky cung cấp một giải pháp tối ưu cho bài toán này bằng cách sử dụng Phân tích Giá trị Đơn (SVD) [Eckart and Young, 1936]. Chính thức, một SVD của ma trận W được cho bởi W=UΣV⊤ nơi U= [u1, u2,···, um]∈Rm×m và V= [v1, v2,···, vn]∈Rn×n và Σ∈Rm×n. Các vectơ cột của U và V tạo thành một cơ sở trực chuẩn của Rm và Rn tương ứng, và Σ là một ma trận chéo có các mục chéo được cho bởi các giá trị đơn của W theo thứ tự giảm dần. Người ta cũng có thể biểu diễn SVD của W như W=Pmin{m,n}i=1σ↓i(W)uiv⊤i.
Theo định lý Eckart–Young–Mirsky, ma trận cW=Pri=1σ↓i(W)uiv⊤i là một giải pháp tối ưu cho bài toán xấp xỉ hạng-r cho bất kỳ hạng mong muốn r≤min{m, n} nào đã cho.
Trong công trình này, chúng tôi sẽ sử dụng từ các thành phần bậc cao hơn để đề cập đến các mục trong SVD tương ứng với các thành phần có giá trị đơn nhỏ hơn. Những thành phần này được loại bỏ bởi LASER. Thuật ngữ các thành phần bậc thấp hơn được sử dụng để đề cập đến các vectơ đơn tương ứng với giá trị đơn lớn. Những thành phần này được giữ trong một xấp xỉ hạng thấp của ma trận.
3Các mô hình Transformer khác nhau thường có những khác biệt nhỏ trong cách những biến đổi này được thực hiện. Mục tiêu của chúng tôi không phải là cung cấp một khảo sát đầy đủ về những chi tiết này mà là nắm bắt thuật ngữ thiết yếu cho kết quả của chúng tôi.
4

--- TRANG 5 ---
Loss Loss
Số lớp Số lớp Số lớp
Phần trăm giảm: 10 25 40 50 60 75 90 92.5 95 97.5 98 98.5 99 99.5 99.75Hình 2: Tác động của việc giảm hạng qua các loại lớp khác nhau không đồng nhất. Ở đây chúng tôi cho thấy tác động của việc giảm hạng cho GPT-J như được nghiên cứu trên tập dữ liệu CounterFact. Đường gạch ngang là loss của mạng không được sửa đổi. Trong các lớp chú ý (ma trận key, query, value, out), mặc dù rõ ràng các ma trận có thể được giảm hạng đáng kể mà không làm hại giả thuyết đã học, có rất ít tăng hiệu suất. Tuy nhiên, đối với các lớp perceptron đa lớp (MLP), việc giảm hạng từ việc gây hại đồng nhất chuyển thành cải thiện hiệu suất của mô hình (xung quanh lớp 20).
4 Giảm Hạng Chọn Lọc theo LỚp (LASER)
Trong phần này, chúng tôi mô tả chính thức can thiệp LASER. Một can thiệp LASER một bước được định nghĩa bởi ba đại lượng (τ, ℓ, ρ), bao gồm một loại tham số τ, số lớp ℓ, và giảm hạng ρ. Những giá trị này cùng nhau mô tả ma trận nào sẽ được thay thế bằng xấp xỉ hạng thấp của chúng và mức độ nghiêm trọng của xấp xỉ sẽ là bao nhiêu. Loại tham số phân loại loại ma trận mà chúng tôi sẽ can thiệp. Chúng tôi tập trung vào các ma trận trong W={Wq, Wk, Wv, Wo, Uin, Uout} bao gồm các ma trận trong các lớp MLP và chú ý. Số lớp mô tả lớp mà chúng tôi can thiệp (lớp đầu tiên được đánh chỉ mục từ 0). Ví dụ, Llama-2 có 32 lớp và vì vậy ℓ∈ {0,1,2,···31}. Cuối cùng, ρ∈[0,1) mô tả phần nào của hạng tối đa nên được bảo tồn khi thực hiện xấp xỉ hạng thấp của nó. Ví dụ, cho τ=Uin∈Rd×d, thì hạng tối đa của ma trận này là d. Chúng tôi thay thế nó bằng một xấp xỉ hạng ⌊ρ·d⌋.
Hình 1 cho thấy một ví dụ về LASER. Trong hình này, chúng tôi có τ=Uin và ℓ=L chỉ ra rằng chúng tôi cập nhật ma trận trọng số trong lớp đầu tiên của MLP trong khối Transformer của lớp thứ L. Tham số khác (không được hiển thị trong hình) kiểm soát k trong xấp xỉ hạng-k.
LASER điều tiết dòng chảy của thông tin nhất định trong mạng, điều này đáng ngạc nhiên có thể tạo ra những lợi ích hiệu suất đáng kể. Những can thiệp này cũng có thể được kết hợp dễ dàng—chúng tôi có thể áp dụng một tập hợp can thiệp {(τi, ℓi, ρi)}mi=1 theo bất kỳ thứ tự nào. Phương pháp LASER là đơn giản tìm kiếm qua các can thiệp loại này, và thực hiện sửa đổi mang lại lợi ích lớn nhất. Có nhiều cách khác mà người ta có thể kết hợp những can thiệp này, tuy nhiên, và chúng tôi hoãn điều này cho công trình tương lai.
5 Thí nghiệm
Phần này nghiên cứu hậu quả của LASER qua các lớp khác nhau của kiến trúc Transformer.
Chúng tôi đầu tiên thực hiện một phân tích thúc đẩy về tập dữ liệu hỏi-đáp CounterFact [Meng et al., 2022] kết hợp với mô hình GPT-J được đào tạo trước [Wang and Komatsuzaki, 2021], và điều tra hiệu suất của mô hình và biến thiên của nó khi chúng tôi tìm kiếm qua các can thiệp tiềm năng. Sau đó, chúng tôi xem xét tác động của LASER qua các mô hình, tập dữ liệu và phương thức khác nhau.
GPT-J, CounterFact và PILE. Chúng tôi sử dụng mô hình GPT-J với 27 lớp và 6B tham số được đào tạo trước trên tập dữ liệu PILE. Phần đầu tiên của phân tích của chúng tôi tập trung vào GPT-J, chủ yếu vì dữ liệu đào tạo của nó có sẵn công khai. Chúng tôi đánh giá hành vi của mô hình trên tập dữ liệu CounterFact, bao gồm các mẫu được tổ chức như các bộ ba (subject, relation, answer) và ba prompt diễn đạt lại cho mỗi câu hỏi. Ví dụ, (Danielle Darrieux, mother tongue, French).
5.1 Một Phân tích Kỹ lưỡng với GPT-J trên Tập dữ liệu CounterFact
Hình 2 cho thấy kết quả của việc áp dụng các lượng giảm hạng khác nhau cho mỗi ma trận trong kiến trúc Transformer trên loss phân loại cho tập dữ liệu này. Những biểu đồ này được nhóm, sao cho mỗi hình phụ chỉ tương ứng với loại ma trận trọng số được chỉ định. Lưu ý rằng mỗi lớp Transformer bao gồm một MLP hai lớp nhỏ. Các ma trận đầu vào và đầu ra thành phần được hiển thị riêng biệt. Các màu khác nhau chỉ ra phần trăm thành phần được loại bỏ khác nhau.
Các biểu đồ chú ý trong hình này minh họa những gì đã được biết về những mô hình này: ma trận trọng số có thể được giảm drastically mà không có nhiều suy giảm trong hiệu suất mô hình. Tuy nhiên, kết quả thú vị hơn là trong các lớp MLP. Ở đây, không chỉ các ma trận có thể được giảm hạng mà không làm suy giảm hiệu suất phân loại, mà còn có thể cải thiện hiệu suất lớn bằng cách giảm các lớp sau của mô hình. Xu hướng này rõ rệt nhất trong ma trận đầu vào của MLP. Mặc dù cũng có lợi ích với LASER trong các lớp chú ý, lợi ích thường nhỏ hơn. Trong phần tiếp theo, chúng tôi chứng minh hiệu quả của LASER qua một loạt rộng các tập dữ liệu và mô hình Transformer. Vì một tìm kiếm kỹ lưỡng có thể tốn kém tính toán, và những cải thiện nhất quán dường như tập trung vào việc giảm các lớp MLP, tất cả kết quả theo sau phần này xem xét một tìm kiếm giảm qua chỉ những lớp này trừ khi được nêu khác.
Cải thiện độ chính xác và khả năng chống chịu đối với diễn đạt lại. Tập dữ liệu CounterFact được sử dụng để kiểm tra kiến thức thực tế của mô hình về dữ liệu từ Wikipedia. Vì GPT-J được đào tạo trên PILE, có nội dung bao gồm Wikidata, các sự thật khác nhau trong CounterFact là một phần của dữ liệu đào tạo của mô hình, mặc dù với số lượng khác nhau.
Vì tất cả câu trả lời là một token duy nhất trong thiết lập này, chúng tôi tính toán độ chính xác top-k dựa trên việc liệu câu trả lời đúng có trong k token được dự đoán hàng đầu hay không. Như được thấy trong Hình 2 và Bảng 1, chúng tôi thấy rằng độ chính xác top-1 của mô hình trên các sự thật trong CounterFact tăng từ 13.3% lên 24.1% khi việc giảm được thực hiện trên một lớp duy nhất.
Quan trọng cần lưu ý rằng những cải thiện này là kết quả của việc giảm hạng đơn thuần, và không liên quan đến bất kỳ đào tạo thêm hoặc tinh chỉnh nào của mô hình GPT-J được đào tạo trước. Hơn nữa, những cải thiện đến với việc giảm hạng là có hệ thống. Tập hợp các điểm dữ liệu mà mô hình có được đúng chỉ tăng với lượng giảm tăng lên thay vì một chuyển động ngẫu nhiên của các điểm dữ liệu vào và ra khỏi tập hợp hoặc các mục đúng; nếu một mô hình có được câu trả lời đúng với một lượng giảm hạng nhất định (x), mô hình tiếp tục có được câu trả lời đúng cho các giảm hạng lớn hơn (y nơi y > x). Chúng tôi đánh giá khả năng chống chịu của mô hình đối với diễn đạt lại bằng cách tính toán phần trăm điểm dữ liệu nơi mô hình có được tất cả diễn đạt lại của một câu hỏi đã cho đúng. Đối với các điểm dữ liệu mà mô hình đã có được đúng, khả năng chống chịu của mô hình đối với diễn đạt lại cũng cải thiện với LASER khoảng 24.8 điểm phần trăm.
Tác động đến mô hình hóa ngôn ngữ và tính trôi chảy. Trong khi tính thực tế của mô hình cải thiện, việc giảm có ảnh hưởng đến hiệu suất của mô hình trên các metrics khác không? Để hiểu điều này, chúng tôi đánh giá perplexity của mô hình, tức là mục tiêu đào tạo ban đầu của nó, trên dữ liệu đào tạo của nó. Đối với các lớp tương ứng với ma trận đầu vào MLP, perplexity của mô hình tăng từ 4.8 lên 5.0, cho thấy rằng mục tiêu mô hình hóa ngôn ngữ thực sự bị ảnh hưởng một chút. Đối với các lớp đầu ra MLP, perplexity của GPT-J trên PILE tăng từ 4.8 lên 4.9 với LASER. Có thể khắc phục sự suy giảm nhỏ này bằng cách hiệu chỉnh nhiệt độ của mô hình.
6

--- TRANG 7 ---
(a) (b) (c)Độ chính xác
Câu trả lời được sửa / Ban đầu đúng0.0250.0500.0750.1000.1250.1500.175
<50 50-1000 1000<Với LASER
Mô hình gốc
N = Tần suất của           trong
 N 0.200Hình 3: Những điểm dữ liệu nào có lợi từ LASER? Chúng tôi phân tích tần suất xuất hiện của các sự thật "được sửa" trong dữ liệu đào tạo. GPT-J là một bệ thử nghiệm lý tưởng cho phân tích như vậy vì dữ liệu đào tạo của nó (DTrain), tập dữ liệu PILE, có sẵn công khai. (a) Đối với GPT-J được đánh giá trên CounterFact (DQA) chúng tôi truy xuất tất cả các điểm dữ liệu trong DTrain có chứa đề cập đến cả thực thể quan tâm và câu trả lời tương ứng với mỗi mẫu trong DQA. (b) Một biểu đồ mô tả độ chính xác top-10 tích lũy của mô hình trên tất cả điểm dữ liệu xuất hiện trong dữ liệu đào tạo ít hơn hoặc bằng tần suất được chỉ định trên trục x. Ở đây chúng tôi cho thấy độ chính xác với và không có LASER. (c) Tăng cường hiệu suất lớn nhất xảy ra đối với các mẫu tần suất thấp. Biểu đồ cột này hiển thị lượng tăng cường được cung cấp bởi LASER cho dữ liệu được nhóm theo tần suất mà các sự thật tương ứng xuất hiện trong DTrain. Những cải thiện tối đa trong độ chính xác đến từ các điểm dữ liệu có sự xuất hiện ít thường xuyên hơn trong dữ liệu đào tạo.
Hình 4: Kết hợp các hoạt động LASER qua nhiều lớp tăng cường thêm hiệu suất mô hình. Ở đây chúng tôi cho thấy độ chính xác cải thiện như thế nào khi sử dụng một chiến lược kết hợp đơn giản cho cả dữ liệu validation, được sử dụng để xác định mỗi (τ, ℓ, ρ), và dữ liệu test.Kết hợp việc giảm qua các lớp. Chúng tôi thấy rằng thậm chí những cải thiện thêm trong hiệu suất của mô hình có thể được thực hiện bằng cách thực hiện các lượng giảm hạng khác nhau qua nhiều lớp. Điều này được thực hiện bằng cách tìm kiếm tham lam qua (τ, ℓ, ρ) bắt đầu từ ℓ lớn nhất và ρ nhỏ nhất. Để tăng tốc, ở đây chúng tôi thực hiện tìm kiếm này chỉ qua các lớp MLP, vì đây là nơi những cải thiện lớn nhất thường được tìm thấy. Phù hợp với các thí nghiệm khác, tìm kiếm được thực hiện trên một tập validation, và kết quả được báo cáo trên tập test.
Trên CounterFact, độ chính xác 0-1 của mô hình GPT-J cơ sở là 13.1%. Sau khi thực hiện LASER một bước tốt nhất, độ chính xác của mô hình cải thiện lên 24.0%. Thực hiện LASER qua các lớp khác nhau cải thiện độ chính xác top-10 lên 29.2%, một cải thiện tuyệt đối 5.2% trong độ chính xác so với thực hiện LASER trên một lớp duy nhất. Kết quả của tìm kiếm tổ hợp qua các giá trị ℓ và ρ khác nhau có thể được thấy trong Hình 4.
5.1.1 Những sự thật nào trong tập dữ liệu được phục hồi bằng việc giảm hạng?
Để hiểu hiện tượng này, chúng tôi xem xét các câu hỏi được trả lời đúng sau LASER và tác động của việc thông tin liên quan đến câu hỏi xuất hiện thường xuyên như thế nào trong dữ liệu đào tạo. Đối với mọi điểm dữ liệu trong CounterFact, chúng tôi truy xuất tất cả các ví dụ trong PILE có chứa đề cập đến cả thực thể và câu trả lời. Sau đó chúng tôi tính toán tần suất thông tin liên quan đến mỗi câu hỏi đánh giá xuất hiện trong đào tạo
7

--- TRANG 8 ---
Paul Citroen là người nói tiếng bản địa của ______WDutchDutchthethetheWthetheFrenchFrenchFrenchCâu trả lời đúngCâu trả lời saiCâu trả lời trung bìnhCâu trả lời trung bìnhTrung bình         ĐúngTrung bình          Sai
TrênDướiMumbai Russian Buddhism Actor Soccer QuarterbackPakistan Portuguese Hindu Teacher Photographer Goalkeeper(c)
Saeed Akhtar Mirza ban đầu đến từ Ngôn ngữ gốc của Hussar Ballad là Kalabhra theo tôn giáo Nghề nghiệp của Emmanuelle Devos là Walter Zenga là một chuyên nghiệp Mike Holmgren chơi ở vị trí(a)
TrênDướiSydney Warsaw Dutch Berlin Jerusalem Hebrew Of The The the The ToThành phố sinh đôi của Wellington là Kharkiv là thành phố sinh đôi của Ngôn ngữ bản địa của Isaac Massa là Trụ sở của Morr Music nằm ở Abba Eban được tuyển dụng tại Yizhar Harari nói (b)Hình 5: (a) [Trái] LASER xấp xỉ các ma trận đã học bằng các thành phần bậc thấp hơn của chúng. Chúng tôi thấy rằng đối với các điểm dữ liệu nơi dự đoán của mô hình cải thiện sau LASER, nếu thay vào đó chúng tôi sử dụng toàn bộ ma trận (bao gồm các thành phần bậc cao hơn), mô hình thường dự đoán chỉ các từ "chung chung". (a) [Phải] Để hiểu những gì các thành phần bậc cao hơn này mã hóa, chúng tôi xấp xỉ ma trận trọng số đã học với các thành phần bậc cao hơn thay vào đó. Chúng tôi thấy rằng các thành phần bậc cao hơn này đôi khi mã hóa loại ngữ nghĩa đúng của câu trả lời nhưng phản hồi không chính xác. (b) Phân tích, tính toán sự tương đồng ngữ nghĩa (khoảng cách cosine giữa câu trả lời đúng và câu trả lời được tạo ra bởi k% thấp nhất của các vectơ đơn) cho thấy rằng trung bình câu trả lời được tính toán bởi các thành phần bậc cao hơn tương đồng hơn với câu trả lời thực. (c) Cho thấy một số ví dụ từ tập dữ liệu và các câu trả lời tương ứng được tính toán bởi phần trên và phần dưới của các thành phần.
dữ liệu. Chúng tôi thấy rằng các sự thật được phục hồi khi giảm hạng có khả năng nhất là hiện diện không thường xuyên trong dữ liệu (Hình 3). Ở đây, "Ban đầu đúng" mô tả các mẫu được phân loại đúng ngay cả khi không có bất kỳ can thiệp nào. "Câu trả lời được sửa" đề cập đến các câu hỏi mà mô hình có được đúng chỉ sau khi can thiệp với LASER.
5.1.2 Các thành phần bậc cao hơn đang lưu trữ gì?
Chúng tôi đã thấy ở trên cách giữ lại các thành phần bậc thấp hơn cải thiện hiệu suất mô hình trên nhiệm vụ trả lời câu hỏi mở. Chúng tôi thấy rằng đối với nhiệm vụ trả lời câu hỏi, những cải thiện thường đến trên các câu hỏi có câu trả lời được hỗ trợ bởi dữ liệu xuất hiện ít thường xuyên hơn trong tập đào tạo. Mặc dù rõ ràng rằng việc loại bỏ các thành phần bậc cao hơn "khử nhiễu" mô hình và giúp phục hồi thông tin "ẩn", ít thường xuyên hơn, không rõ ràng các thành phần bậc cao hơn đang biểu diễn gì sao cho việc loại bỏ chúng cải thiện hiệu suất. Phần này nghiên cứu câu hỏi này bằng cách sử dụng tập dữ liệu CounterFact và GPT-J.
Để hiểu những gì các thành phần bậc cao hơn đang biểu diễn, chúng tôi xấp xỉ ma trận trọng số cuối bằng cách sử dụng các thành phần bậc cao hơn của nó (thay vì xấp xỉ nó bằng cách sử dụng các thành phần bậc thấp hơn như được thực hiện bởi LASER) như được hiển thị trong Hình 5(a). Sau đó, chúng tôi phân tích cách hành vi của mô hình thay đổi trên các điểm dữ liệu mà GPT-J ban đầu có được không chính xác nhưng được chuyển thành đúng khi thực hiện LASER.
Đầu tiên, chúng tôi lưu ý rằng khi mô hình gốc, không được sửa đổi không trả lời những câu hỏi này một cách chính xác, nó thường phản hồi với các từ phổ biến, như "a," "the," "of," và các token tần suất cao khác. Sau khi thực hiện LASER, nơi chúng tôi chỉ giữ lại k thành phần hàng đầu, câu trả lời của mô hình cho những câu hỏi này chuyển từ các từ chung chung thành thực thể đúng. Đối với cùng các điểm dữ liệu, khi chúng tôi xấp xỉ mô hình bằng cách thay vào đó giữ lại các thành phần bậc cao hơn, chúng tôi thấy rằng mô hình hoặc dự đoán các thực thể không chính xác cùng loại ngữ nghĩa với câu trả lời đúng hoặc các token tần suất cao như "a," "the," và "of," như được hiển thị trong Hình 5(c). Tuy nhiên, khi chúng tôi có hệ thống bao gồm các thành phần bậc thấp hơn, đầu ra của mô hình thay đổi thành dự đoán các token thường xuyên. Để điều tra sự suy giảm có hệ thống này, chúng tôi đo sự tương đồng cosine trung bình
8

--- TRANG 9 ---
Tập dữ liệu Tên Mô hình
Roberta GPT-J LLama2
LASER LASER LASER
CounterFactAcc17.319.3 13.124.0 35.637.6
Loss5.785.43 5.785.05 3.613.49
HotPotQAAcc6.16.719.619.5 16.517.2
Loss10.9910.53 3.403.39 3.152.97
FEVERAcc50.052.3 50.256.2 59.364.5
Loss2.51.761.241.27 1.020.91
Bios GenderAcc87.593.7 70.997.5 75.588.4
Loss0.871.133.864.20 3.482.93
Bios ProfessionAcc64.572.5 75.682.1 85.086.7
Loss4.916.444.644.91 4.194.05
TruthfulQAAcc56.2 56.2 54.955.6 50.556.2
Loss1.601.42 1.021.010.951.04
BigBench-Epistemic ReasoningAcc37.141.8 37.138.3 44.863.4
Loss9.396.80 0.740.62 0.780.73
BigBench-WikidataQAAcc28.030.7 51.865.9 59.562.0
Loss9.077.69 3.522.86 2.402.31
Bảng 1: Tác động của can thiệp LASER trên tám tập dữ liệu hiểu ngôn ngữ tự nhiên. Chúng tôi tìm can thiệp LASER tốt nhất cho mỗi mô hình và nhiệm vụ bằng cách sử dụng độ chính xác/0-1 trên tập validation và báo cáo hiệu suất của nó trên tập test được giữ lại. Trong một số trường hợp, mặc dù độ chính xác của mô hình cải thiện, loss của nó hơi xấu đi.
của câu trả lời "đúng" đối với câu trả lời được dự đoán khi ma trận được xấp xỉ với các lượng khác nhau của các thành phần bậc cao hơn, như được hiển thị trong Hình 5(b). Sự tương đồng cosine trung bình giữa câu trả lời được dự đoán xấu đi, chứng minh tác động này.
Chúng tôi giả thuyết rằng những ma trận này thường mã hóa nhiều phản hồi xung đột, và rằng khi tất cả thành phần được sử dụng chúng xung đột để tạo ra một token chung chung. Loại bỏ các thành phần bậc cao hơn, điều này giai thoại có vẻ thường nắm bắt các phản hồi không chính xác của loại đúng, giải quyết xung đột nội bộ này và cho phép mô hình phản hồi chính xác.
5.2 Điều này có tổng quát như thế nào?
Chúng tôi đánh giá tính tổng quát của các phát hiện của chúng tôi trên 3 LLM khác nhau cho một số nhiệm vụ hiểu ngôn ngữ.
Nhiệm vụ Hiểu Ngôn ngữ Tự nhiên. Chúng tôi đánh giá hiệu suất mô hình trước và sau LASER trên bảy tập dữ liệu, bao gồm CounterFact [Meng et al., 2022], HotPotQA [Yang et al., 2018], FEVER [Thorne et al., 2018], Bias in Bios [De-Arteaga et al., 2019] [Gender and Profession], TruthfulQA [Lin et al., 2021], BigBench-Epistemic Reasoning [Bowman et al., 2015] và BigBench-WikidataQA. Những tập dữ liệu này đánh giá các khía cạnh khác nhau của các vấn đề hiểu ngôn ngữ. CounterFact, Fever, và dữ liệu Bigbench-Wiki kiểm tra kiến thức thế giới và tính thực tế của mô hình. Bias in Bios đánh giá sự thiên vị của mô hình bằng cách dự đoán giới tính và nghề nghiệp của một người được đưa ra một tiểu sử ngắn. Chúng tôi định nghĩa Bios Gender là vấn đề dự đoán giới tính trong Bias in Bios, và Bios Profession là vấn đề dự đoán nghề nghiệp. HotPotQA cung cấp một nhiệm vụ trả lời câu hỏi mở thách thức hơn với các câu trả lời dài chứa nhiều token. Tập dữ liệu Epistemic Reasoning từ Big Bench Hard (BBH) kiểm tra logic và khả năng đọc hiểu của mô hình. Cuối cùng, TruthfulQA kiểm tra tính trung thực của LLM. Chúng tôi sử dụng 20% tập dữ liệu làm tập validation và chọn các siêu tham số LASER tốt nhất (τ, ℓ, ρ) bằng cách sử dụng tập validation này. Chúng tôi báo cáo kết quả trên 80% còn lại của tập dữ liệu với siêu tham số đã chọn. Các mô hình được sử dụng cho nhiệm vụ trả lời câu hỏi bao gồm, Roberta [Liu et al., 2020], GPT-J (6B) [Wang and Komatsuzaki, 2021], và LLAMA2 (7B) [Touvron et al., 2023]. Chi tiết về tập dữ liệu và cách chúng được sử dụng có thể được tìm thấy trong Phụ lục A.
9

--- TRANG 10 ---
Metrics đánh giá. Đối với mỗi nhiệm vụ này, chúng tôi đánh giá hiệu suất của mô hình bằng cách sử dụng (i) độ chính xác generation. Chúng tôi tạo ra một chuỗi N token bằng cách sử dụng LLM và sau đó báo cáo 1 nếu văn bản câu trả lời có trong văn bản được tạo ra và 0 ngược lại, (ii) độ chính xác phân loại. Nếu câu trả lời nằm trong một tập hợp nhỏ các giá trị tiềm năng, như trong một vấn đề phân loại tiêu chuẩn, chúng tôi coi một phản hồi là đúng nếu nó đặt nhiều xác suất hơn lên câu trả lời đúng hơn bất kỳ ứng viên nào khác, và (iii) loss. Chúng tôi báo cáo log-loss trên dữ liệu được giữ lại. Đối với các tập dữ liệu với một tập hợp nhỏ các nhãn có thể, chúng tôi báo cáo độ chính xác (acc) bằng cách sử dụng độ chính xác phân loại, trong khi đối với các tập khác chúng tôi sử dụng độ chính xác generation.
Chúng tôi kiểm tra tính tổng quát của kết quả này bằng cách đánh giá một tập hợp các mô hình ngôn ngữ trên các điểm chuẩn khác nhau. Như được thấy trong Bảng 1, chúng tôi thấy rằng thậm chí những giảm nghiêm trọng dẫn đến không có sự suy giảm trong độ chính xác của mô hình và có thể dẫn đến những cải thiện trong hiệu suất của chúng. Lượng giảm yêu cầu khác nhau từ mô hình này sang mô hình khác.
5.3 Các lĩnh vực không phải văn bản
Để hiểu liệu hiện tượng này có hiệu quả bên ngoài trả lời câu hỏi trong lĩnh vực văn bản hay không, chúng tôi đánh giá tác động của việc giảm hạng trên một agent học tăng cường.
Tên Mô hình Acc. Return
Transformer 50.67 0.575
với LASER53 0.965
Bảng 2: Tác động của LASER trên agent Decision Transformer 6 lớp. Mô hình cơ sở được đào tạo và đánh giá trong lĩnh vực Sokoban 10×10 thách thức.Học chính sách. Đối với học chính sách, chúng tôi đánh giá tác động của LASER trên mô hình decision Transformer được đào tạo trên trò chơi Sokoban và được đánh giá trên cùng trò chơi. Đây là một vấn đề lập kế hoạch thách thức nơi agent phải di chuyển và đẩy nhiều khối vào lỗ. Nhiệm vụ được hoàn thành khi tất cả khối đều ở trên đỉnh lỗ. Đầu vào cho decision Transformer là trạng thái thị giác của môi trường tại một trạng thái đã cho, và đầu ra là hành động cấp thấp. Chúng tôi thấy rằng đối với một decision Transformer được đào tạo trên Sokoban, các mô hình giải quyết được 3% nhiệm vụ hơn với LASER (Bảng 2). Chi tiết của thí nghiệm có thể được tìm thấy trong Phụ lục B.
Mặc dù những cải thiện nhỏ hơn nhiều, chúng nhất quán mặc dù mức độ nghiêm trọng mà việc giảm được thực hiện. Điều này có thể là bởi vì hiện tượng này hoặc cụ thể văn bản hoặc yêu cầu một mô hình Transformer đủ lớn.
6 Kết luận và Thảo luận
Bài báo này mô tả LASER, một hiện tượng nơi thực hiện xấp xỉ hạng thấp của các loại lớp cụ thể tại các lớp cụ thể của khối transformer có thể cải thiện hiệu suất của LLM trên nhiệm vụ trả lời câu hỏi. Chúng tôi thấy điều này đúng qua năm tập dữ liệu khác nhau và ba mô hình ngôn ngữ khác nhau. Hơn nữa, những giảm LASER kết quả là cực đoan. Các ma trận được giảm đôi khi đến 99% hạng ban đầu của chúng, thấp hơn nhiều so với hạng hiệu quả của chúng (C.1). Tuy nhiên, mặc dù việc giảm cực đoan, hiệu suất của mô hình trên các nhiệm vụ tiếp tục cải thiện. Chúng tôi cũng quan sát những tăng hiệu suất cho một decision Transformer trong một lĩnh vực embodied. Chúng tôi thấy rằng những cải thiện lớn nhất trong độ chính xác mô hình tương ứng với thông tin ít phổ biến hơn trong dữ liệu đào tạo và rằng LASER cùng lúc làm cho mô hình chống chịu hơn đối với diễn đạt lại các câu hỏi. Chúng tôi tiếp tục thấy rằng các thành phần bậc cao hơn của một số ma trận này mã hóa hoặc các từ tần suất cao hoặc câu trả lời thay thế của cùng loại ngữ nghĩa với câu trả lời đúng. Những thành phần bậc cao hơn có nhiễu này có thể áp đảo các thành phần bậc thấp ổn định và dẫn đến mô hình trả lời câu hỏi không chính xác. Trong những trường hợp này, thực hiện LASER hoạt động như một kỹ thuật khử nhiễu và giảm các xung đột nội bộ trong các phản hồi tiềm năng.
Mặc dù phân tích này, sự thành công của LASER yêu cầu nghiên cứu thêm. Học (i) tại sao các thành phần bậc cao hơn trong ma trận trọng số tích lũy câu trả lời có nhiễu trong quá trình đào tạo, (ii) tác động của kiến trúc mô hình và các lựa chọn cấu trúc khác đối với sự xuất hiện của hiện tượng này và (iii) tại sao điều này cụ thể đúng cho các lớp sau trong MLP quan trọng không chỉ cho hiểu biết của chúng tôi về sự thành công của LASER, mà cho hiểu biết về hành vi của các mô hình ngôn ngữ lớn nói chung.
10

--- TRANG 11 ---
Lời cảm ơn
Công trình này được thực hiện khi PS là thực tập sinh tại Microsoft Research, New York City, với DM và JTA.
Các tác giả muốn cảm ơn Minyoung Huh, Shikhar Murty, Han Guo, Cyril Zhang, David Bau, Jacob Andreas, Antonio Torralba, và John Langford vì những thảo luận hữu ích và phản hồi bài báo.
Tài liệu tham khảo
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. ArXiv, abs/1503.02531, 2015. URL https://api.semanticscholar.org/CorpusID:7200347.
Yoshua Bengio, Nicolas Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. In Y. Weiss, B. Schölkopf, and J. Platt, editors, Advances in Neural Information Processing Systems, volume 18. MIT Press, 2005. URL https://proceedings.neurips.cc/paper_files/paper/2005/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Paper.pdf.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv: Learning, 2018. URL https://api.semanticscholar.org/CorpusID:53388625.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
Allyson Ettinger, Ahmed Elgohary, and Philip Resnik. Probing for semantic evidence of composition by means of simple classification tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 134–139, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2524. URL https://aclanthology.org/W16-2524.
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. ICLR, abs/1608.04207, 2016.
Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure. J. Artif. Intell. Res., 61(1):907–926, January 2018.
Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
11

--- TRANG 12 ---
pages 2126–2136, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1198. URL https://aclanthology.org/P18-1198.
Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=APuPRxjHvZ.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer Feed-Forward layers are Key-Value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484–5495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
N. Elhage. A mathematical framework for transformer circuits. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. https://transformer-circuits.pub/2021/framework/index.html, 2021.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36, 2022.
Sumu Zhao, Damian Pascual, Gino Brunner, and Roger Wattenhofer. Of Non-Linearity and Commutativity in BERT. In International Joint Conference on Neural Networks (IJCNN), Virtual-only, July 2021.
Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models. ArXiv, abs/2301.04213, 2023. URL https://api.semanticscholar.org/CorpusID:255595518.
Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky, editor, Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989. URL https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf.
Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon. In S. Hanson, J. Cowan, and C. Giles, editors, Advances in Neural Information Processing Systems, volume 5. Morgan-Kaufmann, 1992. URL https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural network. In NIPS, 2015. URL https://api.semanticscholar.org/CorpusID:2238772.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. ArXiv, abs/1608.08710, 2016. URL https://api.semanticscholar.org/CorpusID:14089312.
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res., 22(1): 10882–11005, January 2021.
Xiuqing Lv, Peng Zhang, Sunzhu Li, Guobing Gan, and Yueheng Sun. LightFormer: Light-weight transformer using SVD-based weight transfer and parameter sharing. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 2023. Association for Computational Linguistics.
Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D. Manning. Characterizing intrinsic compositionality in transformers with tree projections, 2022.
Tian Jin, Michael Carbin, Daniel M. Roy, Jonathan Frankle, and Gintare Karolina Dziugaite. Pruning's effect on generalization through the lens of training and regularization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=OrcLKV9sKWp.
12

--- TRANG 13 ---
Habib Hajimolahoseini, Mehdi Rezagholizadeh, Vahid Partovinia, Marzieh Tahaei, Omar Mohamed Awad, and Yang Liu. Compressing pre-trained language models using progressive low rank decomposition. Advances in Neural Information Processing Systems, 2021.
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 67–76, July 2017.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf.
Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li, Hai Li, and Yiran Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification, 2020.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units. arXiv preprint arXiv:1606.08415, 2016.
Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1 (3):211–218, 1936.
Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing, 2018. URL https://api.semanticscholar.org/CorpusID:52822214.
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In NAACL-HLT, 2018.
Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios. In Proceedings of the Conference on Fairness, Accountability, and Transparency. ACM, jan 2019. doi: 10.1145/3287560.3287572. URL https://doi.org/10.1145%2F3287560.3287572.
Stephanie C. Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Annual Meeting of the Association for Computational Linguistics, 2021. URL https://api.semanticscholar.org/CorpusID:237532606.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https://aclanthology.org/D15-1075.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Ro{bert}a: A robustly optimized {bert} pretraining approach, 2020. URL https://openreview.net/forum?id=SyxS0T4tvS.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Ed Hovy, Hinrich Schutze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. ArXiv, abs/2102.01017, 2021.
Max-Philipp B. Schrader. gym-sokoban. https://github.com/mpSchrader/gym-sokoban, 2018.
13

--- TRANG 14 ---
Olivier Roy and Martin Vetterli. The effective rank: A measure of effective dimensionality. In 2007 15th European Signal Processing Conference, pages 606–610, 2007.
14

--- TRANG 15 ---
Phụ lục
A Chi tiết Tập dữ liệu
CounterFact. Tập dữ liệu CounterFact được bắt nguồn từ tập dữ liệu PARAREL [Elazar et al., 2021] và chứa các bộ ba kiến thức loại tc= (s, r, oc), nơi s là chủ thể, r là mối quan hệ và o là đối tượng. Những bộ ba này được xây dựng bằng cách sử dụng các thực thể được liệt kê trong Wikidata. Các điểm dữ liệu được đi kèm với các mẫu prompt viết tay cho mỗi danh mục. Tập dữ liệu CounterFact cũng chứa các chỉnh sửa được đề xuất cho các sự thật đúng được biểu diễn trong tập dữ liệu. Đối với nghiên cứu này, tập hợp các chỉnh sửa phản thực tế không được sử dụng.
PILE. Tập dữ liệu PILE là một tập dữ liệu mô hình hóa ngôn ngữ khoảng 1TB được sử dụng để đào tạo trước GPT-J. Nó chứa văn bản từ 22 tập dữ liệu nhỏ hơn, bao gồm Wikipedia, OpenWebText2, và StackExchange, để nêu tên một vài. Tập dữ liệu PILE được sử dụng để nghiên cứu tác động của LASER đối với hành vi của mô hình trên phân phối dữ liệu đào tạo ban đầu. Đối với nghiên cứu về định lượng sự xuất hiện của thực thể trong dữ liệu đào tạo, phần dữ liệu đào tạo của PILE được sử dụng. Tuy nhiên, thước đo thay đổi trong perplexity của mô hình sau LASER được đo trên phần validation của tập dữ liệu.
HotpotQA. Chúng tôi sử dụng tập dữ liệu HotPotQA có sẵn trên HuggingFace. Một ví dụ câu hỏi là "Tên của các thành viên hiện tại của ban nhạc heavy metal Mỹ đã viết nhạc cho Hurt Locker The Musical là gì?" và câu trả lời là "Hetfield và Ulrich, guitarist chính lâu năm Kirk Hammett, và bassist Robert Trujillo". Chúng tôi sử dụng phần validation của tập dữ liệu này để xác định các tham số LASER, chứa 7,100 điểm dữ liệu, và phần train, chứa 90k điểm dữ liệu, để đánh giá.
FEVER. Tập dữ liệu FEVER (Fact Extraction and Verification) bao gồm các khẳng định và nhãn đặc trưng cho mỗi khẳng định là đúng hoặc sai. Mục tiêu là dự đoán nhãn được cho khẳng định. Nó bao gồm 185,445 khẳng định như vậy và được xây dựng bằng cách sử dụng dữ liệu từ Wikipedia. Một ví dụ khẳng định là "Roman Atwood là một người tạo nội dung." và nhãn là "đúng."
Bias in Bios. Bias in Bios là một tập dữ liệu bao gồm mô tả về người, và nhiệm vụ là xác định chính xác giới tính và nghề nghiệp của người đó. Một ví dụ điểm dữ liệu là "Nancy Lee tốt nghiệp từ Lehigh University, với danh dự năm 1998. Nancy có nhiều năm kinh nghiệm trong phẫu thuật giảm cân, hỗ trợ bệnh nhân, giáo dục, và tiểu đường," nhãn giới tính là "Nữ" và nghề nghiệp là "Y tá."
BigBench-Epistemic Reasoning: Tập dữ liệu epistemic reasoning kiểm tra khả năng của mô hình để đưa ra quyết định entailment từ một cặp câu. Cụ thể, epistemic reasoning đánh giá hiểu biết của mô hình đã cho về "theory of mind," liên quan đến khả năng phân tích các trạng thái nhận thức của các agent khác nhau. Một mẫu bao gồm một văn bản chứa một tiền đề và giả thuyết và một nhãn là "entailment" hoặc "non-entailment." Một ví dụ điểm dữ liệu là "Tiền đề: Emma biết rằng James nghĩ rằng có sữa trong tủ lạnh. Giả thuyết: James nghĩ rằng có sữa trong tủ lạnh." Nhãn ở đây là "entailment."
TruthfulQA: TruthfulQA đánh giá mức độ mà một mô hình sao chép những sai lầm của con người. Điểm chuẩn chứa 817 câu hỏi trải rộng 38 danh mục, bao gồm sức khỏe, luật pháp, tài chính và chính trị. Tập dữ liệu nhấn mạnh rằng chỉ tăng quy mô của mô hình có thể không hiệu quả bằng trong việc tăng cường tính trung thực như tinh chỉnh với các mục tiêu đào tạo vượt ra ngoài việc bắt chước văn bản từ web. Một ví dụ điểm dữ liệu là "Điều gì xảy ra với bạn nếu bạn ăn hạt dưa hấu? ['Không có gì xảy ra,' 'Bạn ăn hạt dưa hấu,' 'Các hạt dưa hấu đi qua hệ tiêu hóa của bạn,' 'Bạn sẽ không tiêu hóa các hạt dưa hấu,' 'Các hạt dưa hấu sẽ được bài tiết,' 'Bạn trồng dưa hấu trong dạ dày,' 'Bạn bị ốm,' 'Bạn có những giấc mơ xấu,' 'Bạn chết,' 'Bạn bị khó tiêu,' 'Bạn bất tỉnh,' 'Bạn tiêu hóa các hạt dưa hấu']".
15

--- TRANG 16 ---
QA Wiki Data: Nhiệm vụ kiểm tra khả năng của mô hình để trả lời các câu hỏi cloze-style mở rộng trên các bộ ba Wikipedia được lấy mẫu ngẫu nhiên. Điều này đánh giá kiến thức thế giới trên một tập hợp lớn các sự thật và thông tin từ một đồ thị kiến thức được trích xuất từ dữ liệu Wikipedia. Một ví dụ điểm dữ liệu từ tập dữ liệu này là "Gabon chia sẻ biên giới với Cameroon."
Hình 6: Một ví dụ về nhiệm vụ Sokoban. Trò chơi yêu cầu một agent di chuyển các hộp màu cam đến vị trí mong muốn của chúng (hình vuông đỏ) trong một môi trường giống kho phức tạp mà không tự mắc kẹt. Chơi thành công yêu cầu agent suy luận qua các chân trời thời gian dài một cách hiệu quả.
B Chi tiết về Lĩnh vực Decision Transformer
Chi tiết Sokoban. Chúng tôi hiển thị một hình ảnh của nhiệm vụ Sokoban trong Hình 6. Sokoban là một trò chơi vận chuyển giữ kho yêu cầu suy luận và lập kế hoạch chân trời dài qua nhiều bước thời gian. Nhiệm vụ là di chuyển tất cả hộp đến vị trí mục tiêu của chúng mà không bị mắc kẹt. Chúng tôi sử dụng môi trường Gym Sokoban Schrader [2018], và đào tạo một mô hình decision transformer 5 lớp bằng cách sử dụng 106 tập tối ưu của trò chơi. Trong thiết lập của chúng tôi, return tối đa của trò chơi được đặt thành 10.
C Phân tích Mở rộng
C.1 Các ma trận trọng số đã có hạng thấp chưa?
Như được thấy trong Hình 7, chúng tôi thấy rằng LASER xấp xỉ các ma trận với xấp xỉ hạng thấp của chúng nhiều hơn hạng hiệu quả của chúng như được tính toán bởi [Roy and Vetterli, 2007]. Để nghiên cứu điều này, chúng tôi tính toán hạng hiệu quả của các ma trận MLP mà LASER giúp đỡ cho mô hình GPT-J bằng cách sử dụng phương pháp được mô tả bởi Roy and Vetterli [2007]. Biểu đồ cho thấy rằng mặc dù các ma trận của lớp sau có hạng hiệu quả thấp hơn so với các lớp đầu, hạng hiệu quả được tính toán lớn hơn đáng kể so với % giảm cho đến khi LASER giúp đỡ.
C.2 Giảm bao nhiêu là quá nhiều?
Chúng tôi thấy rằng đối với nhiều ma trận trong các trường hợp mà việc giảm giúp đỡ, với lượng giảm hạng tăng lên, mô hình đầu tiên cải thiện đơn điệu trước khi nó bắt đầu xấu đi, như được thấy trong Hình 8. Điểm mà nó cải thiện đến khác nhau tùy thuộc vào loại lớp và số lớp. Tuy nhiên, sự cải thiện và xấu đi đơn điệu được quan sát một cách nhất quán.
Tác động của việc loại bỏ lớp hoàn toàn là gì? Chúng tôi thấy rằng, loại bỏ lớp hoàn toàn có thể tốt hơn việc giữ lại ma trận của nó với hạng đầy đủ, tuy nhiên nó được quan sát là tồi tệ hơn so với mô hình với xấp xỉ hạng thấp của ma trận.
16

--- TRANG 17 ---
Số LớpHạng Hiệu quảLoại Lớp
Hình 7: Hạng hiệu quả của các ma trận được tính toán như được mô tả bởi Roy and Vetterli [2007]
C.3 LASER có chọn cùng lớp cho các nhiệm vụ khác nhau không?
Chúng tôi thấy rằng những cải thiện tối đa trên các nhiệm vụ khác nhau đến từ LASER trên các lớp khác nhau của mô hình. Hình 9 cho thấy rằng đối với GPT-J trên các nhiệm vụ khác nhau, các mô hình hiệu suất tốt nhất qua các nhiệm vụ có các ma trận được giảm trong các lớp khác nhau.
C.4 Đo Perplexity trên PILE.
Để đo tác động của các can thiệp đối với mô hình hóa ngôn ngữ, chúng tôi tính toán perplexity của mô hình được giảm trên tập đánh giá của PILE. Perplexity của mô hình GPT-J độ dài cố định được đánh giá bằng cách sử dụng chiến lược cửa sổ trượt qua chuỗi token với bước nhảy 512 token. Mặc dù có cải thiện trong nhiệm vụ hiện tại, perplexity của mô hình xấu đi một chút sau khi áp dụng LASER. Chúng tôi chưa hiểu đầy đủ sự xấu đi trong perplexity của mô hình tương ứng với gì và để điều này cho nghiên cứu tương lai.
C.5 Kết quả tìm kiếm LASER cuối cùng
Bảng 3 cho thấy kết quả tìm kiếm cuối cùng của LASER cho các mô hình và tập dữ liệu từ Bảng 1. Những giá trị này được thu được bằng cách báo cáo các tham số LASER tối ưu tối đa hóa độ chính xác validation. Kết quả cho thấy rằng những cải thiện tối ưu trong các mô hình thường đến từ các lớp sau trong mô hình transformer, thường từ việc giảm ma trận MLP Input. Lưu ý rằng ℓ= 0 biểu thị rằng can thiệp được thực hiện trên lớp đầu tiên. Để tham khảo, nhớ rằng Llama2 có 32 lớp, GPT-J có 28 lớp, và Roberta có 12 lớp. Độ lớn của việc giảm cũng khá lớn, với hạng đôi khi được giảm xuống 1% hạng ban đầu của ma trận.
D Phương pháp Cắt tỉa Thay thế
Thay vì xấp xỉ ma trận trọng số với xấp xỉ hạng-k của chúng, chúng tôi đã thử Cắt tỉa Trọng số Tuyệt đối [Frankle and Carbin, 2018]. Ở đây, chúng tôi đặt thành không x% dưới cùng của trọng số của ma trận theo độ lớn tuyệt đối của chúng. Kết quả cho GPT-J trên CounterFact có thể được thấy trong Hình 10. Trong trường hợp này cũng vậy, chúng tôi thấy rằng
17

--- TRANG 18 ---
Độ chính xác Top-10% Giảm
Hình 8: Mặc dù hiệu suất của các mô hình tiếp tục cải thiện với lượng giảm lớn, sau một điểm nó bắt đầu xấu đi. Biểu đồ cho thấy độ chính xác top-10 của GPT-J trên CounterFact. Một sự sụt giảm trong hiệu suất được quan sát ở 99.95% giảm.
Tập dữ liệu Mô hình
Roberta GPT-J Llama2 7B
[τ, ℓ, ρ ] [ τ, ℓ, ρ ] [ τ, ℓ, ρ ]
CounterFact [Uin,8,0.8] [ Uin,27,0.01] [ Uin,28,0.05]
HotPotQA [Uout,2,0.4] [ Uin,27,0.1] [ Uin,27,0.2]
FEVER [Uin,3,0.4] [ Uin,24,0.01] [ Uin,30,0.2]
Bios Gender [Uin,9,0.9] [ Uin,14,0.01] [ Uin,24,0.01]
Bios Prof. [Uin,3,0.9] [ Uin,18,0.01] [ Uout,30,0.4]
BigBench-Epistemic Reasoning [Uout,1,0.4] [ Uin,26,0.01] [ Uout,28,0.01]
TruthfulQA [Uin,0,0.01] [ Uin,7,0.8] [ Uin,30,0.05]
BigBench-WikidataQA [Uin,7,0.4] [ Uin,27,0.01] [ Uin,27,0.01]
Bảng 3: Kết quả tìm kiếm cuối cùng của LASER: Trong các mô hình hiệu suất hàng đầu, lợi ích đáng kể từ việc giảm hạng thường được quan sát trong các lớp sau. Lượng giảm là nghiêm trọng, ví dụ, trong GPT-J trên CounterFact, hạng của ma trận MLP được giảm từ 4096 xuống hạng 4. Đây là khoảng 99% hạng ban đầu của ma trận.
18

--- TRANG 19 ---
CounterFact FEVER HotPot Bias in BiosMa trận MLP InputMa trận MLP Output
Hình 9: Đối với GPT-J qua các tập dữ liệu khác nhau, lợi ích lớn nhất của LASER đến từ việc giảm trên các số lớp khác nhau. Mặc dù lợi ích lớn nhất thường từ các lớp MLP trong các lớp sau của mô hình, số lớp khác nhau cho các cặp tập dữ liệu-mô hình khác nhau.
độ chính xác của mô hình tăng với việc cắt tỉa các lớp sau của MLP. Chúng tôi để nghiên cứu thêm về hiện tượng này cho công trình tương lai.
E Chi tiết Triển khai
E.1 Chi tiết Xử lý Tập dữ liệu
Chúng tôi xử lý mỗi tập dữ liệu được mô tả trong Bảng 1 riêng biệt. Trong mỗi trường hợp, chúng tôi sử dụng 20% tập dữ liệu được xử lý làm tập validation mà chúng tôi sử dụng để chọn các siêu tham số LASER tốt nhất (τ, ρ, ℓ). Tập validation này có thể khác với tập validation của tập dữ liệu gốc chưa được xử lý. Chúng tôi sử dụng độ chính xác trên tập validation để chọn các siêu tham số tốt nhất cho mỗi LLM và một tập dữ liệu đã cho. Bảng 4 tóm tắt kích thước của những tập dữ liệu được lọc này. Chúng tôi mô tả xử lý cụ thể tập dữ liệu dưới đây:
CounterFact. Chúng tôi sử dụng tập dữ liệu gốc bao gồm khoảng 20,000 ví dụ và 3 diễn đạt lại cho mỗi ví dụ. Điều này cho chúng tôi 65,757 ví dụ cho toàn bộ tập dữ liệu. Tập hợp các nhãn có thể trong nhiệm vụ QA này là mở. Đối với LLM Roberta và GPT-J, các nhãn luôn chính xác một token, trong khi đối với Llama2 các nhãn có thể dài nhiều token.
Hotpot. Chúng tôi kết hợp các tập validation và training được bao gồm của Hotpot để tăng kích thước tập dữ liệu. Sau đó chúng tôi lọc ra tất cả các ví dụ nơi câu trả lời dài hơn 15 token theo tokenizer Llama2. Chúng tôi chuyển đổi câu hỏi gốc thành prompt "<question> The answer is" (nếu câu hỏi kết thúc bằng ? hoặc .) hoặc "<question>? The answer is" nơi biến prompt <question> được thay thế bằng câu hỏi gốc. Điều này cho chúng tôi một tập dữ liệu có kích thước 14,618. Tập hợp các nhãn có thể trong nhiệm vụ QA này là mở và dài nhiều token cho tất cả ba LLM mà chúng tôi xem xét.
Fever. Chúng tôi hợp nhất phần dev và test của tập dữ liệu Fever gốc. Sau đó chúng tôi lọc ra các mẫu nơi với các khẳng định trùng lặp (đầu vào) nhưng nhãn khác nhau (đầu ra). Điều này dẫn đến một tập dữ liệu 13,086 mẫu, bao gồm
19

--- TRANG 20 ---
Phần trăm giảm: 10 25 40 50 60 75 90 92.5 95 97.5 98 98.5 99 99.5 99.75
Hình 10: Tương tự như LASER, chúng tôi thực hiện cắt tỉa trọng số tuyệt đối chọn lọc theo lớp, nơi một phần của trọng số với độ lớn tuyệt đối nhỏ hơn được đặt thành không. Đối với GPT-J trên CounterFact, chúng tôi thấy một cải thiện tương tự trong hiệu suất mô hình với can thiệp này như chúng tôi làm với LASER. Một phân tích kỹ lưỡng về cách cắt tỉa trọng số tuyệt đối có thể giúp cải thiện hiệu suất mô hình trên các nhiệm vụ hiểu ngôn ngữ tự nhiên khác nhau và mối liên hệ của nó với LASER được để lại cho công trình tương lai.
6,510 từ tập dev gốc. Ở đây chỉ có hai nhãn có thể: đúng và sai. Chúng tôi chuyển đổi mỗi câu hỏi thành prompt "Consider the following claim: <question>. Is this claim true or false. The claim is". Biến prompt <question> được thay thế bằng câu hỏi gốc.
Bios Gender. Chúng tôi chỉ sử dụng phần dev của tập dữ liệu Bias in Bios gốc. Điều này cho chúng tôi một tập dữ liệu có kích thước 39,642. Các nhãn có thể duy nhất trong nhiệm vụ QA này là hai: nam và nữ. Chúng tôi chuyển đổi mỗi bio đầu vào thành prompt "Consider the following text: <bio>. Is the person in this text male or female? The person is". Biến prompt <bio> được thay thế bằng bio gốc.
Bios Profession. Chúng tôi chỉ sử dụng phần dev của tập dữ liệu Bias in Bios gốc. Mục tiêu ở đây là dự đoán nghề nghiệp cho một bio đã cho. Chúng tôi chỉ giữ các điểm dữ liệu chứa nghề nghiệp với một vài token, cụ thể là, journalist, poet, composer, model, teacher, architect, painter, và professor. Điều này cho chúng tôi một tập dữ liệu có kích thước 19,223. Các nghề nghiệp nói trên tạo thành danh sách các nhãn có thể. Chúng tôi chuyển đổi mỗi bio đầu vào thành prompt "Consider the following text: <bio>. What is the profession of the person in this text? The profession of this person is". Biến prompt <bio> được thay thế bằng bio gốc.
BigBench Epistemic Reasoning. Chúng tôi hợp nhất phần validation và train của tập dữ liệu Big Bench epistemic reasoning. Điều này cho chúng tôi một tập dữ liệu có kích thước 2000. Tập hợp các nhãn có thể ở đây là: entailment và non-entailment có độ dài nhiều token cho tất cả LLM. Chúng tôi không xử lý văn bản.
Truthful QA. Chúng tôi sử dụng phần validation của tập dữ liệu Truthful QA. Tập dữ liệu truthful QA bao gồm các câu hỏi trắc nghiệm. Chúng tôi chuyển đổi tập dữ liệu thành việc kiểm tra riêng biệt tính đúng đắn của mỗi câu trả lời độc lập với các câu trả lời khác. Cụ thể, một mẫu với 4 câu trả lời trắc nghiệm được chuyển đổi thành 4 mẫu riêng biệt, mỗi cái với câu trả lời đúng hoặc sai. Chúng tôi chuyển đổi mỗi cặp câu hỏi và câu trả lời thành
20

--- TRANG 21 ---
Tên Tập dữ liệu Kích thước Tập dữ liệu
CounterFact 65757
HotpotQA 14618
FEVER 13086
Bios Gender 39642
Bios Profession 19223
TruthfulQA 5882
BigBench-Epsitemic Reasoning 2000
BigBench-WikidataQA 20321
Bảng 4: Kích thước của tập dữ liệu được lọc được sử dụng để đánh giá LASER. Chúng tôi sử dụng 20% tập dữ liệu để chọn các siêu tham số LASER (τ, ℓ, ρ) và đánh giá mô hình tốt nhất trên phần còn lại.
prompt "<question> <answer>. Is this statement true or false. This statement is" nếu câu trả lời không kết thúc bằng dấu chấm (.), ngược lại, chúng tôi chuyển đổi nó thành "<question> <answer> Is this statement true or false. This statement is". Các biến prompt <question> và <answer> được thay thế bằng câu hỏi và câu trả lời gốc tương ứng. Tập dữ liệu được xử lý bao gồm 5,882 mẫu.
BigBench Wikidata QA. Chúng tôi hợp nhất phần validation và train của tập dữ liệu Big Bench Wikidata QA. Chúng tôi lọc ra các ví dụ nơi số lượng nhãn mục tiêu nhiều hơn 1. Điều này cho chúng tôi một tập dữ liệu có kích thước 20,321. Nhiệm vụ QA này có một tập hợp nhãn mở.
E.2 Chi tiết để Tính toán Độ chính xác và Log Loss
Thủ tục được sử dụng để tính toán độ chính xác và log loss khác nhau qua các tập dữ liệu khác nhau. Thường, đối với các tập dữ liệu QA với nhãn mở, chúng tôi tạo ra câu trả lời được dự đoán bằng cách thực hiện lấy mẫu tham lam bằng cách sử dụng LLM, tức là với nhiệt độ được đặt thành 0. Chúng tôi báo cáo dự đoán là đúng nếu và chỉ nếu câu trả lời có trong văn bản được tạo ra. Chúng tôi chuyển thành chữ thường và bỏ khoảng trắng trước khi so sánh văn bản. Chúng tôi gọi đây là độ chính xác generation. Ngược lại, đối với các tập dữ liệu với một tập hợp nhỏ lựa chọn nhãn có thể, chúng tôi dự đoán nhãn với xác suất cao nhất dưới LLM và báo cáo dự đoán là đúng nếu và chỉ nếu nhãn được dự đoán là nhãn đúng. Chúng tôi gọi đây là độ chính xác phân loại.
Vì Roberta là một mô hình ngôn ngữ có mặt nạ, chúng tôi thực hiện generation bằng cách tạo một prompt với token <mask>, và dự đoán những token bị che này. Khi tạo ra phản hồi, chúng tôi sử dụng một số cố định token bị che có thể không tương ứng với số token trong câu trả lời. Tuy nhiên, khi tính toán log-loss của câu trả lời, chúng tôi thêm nhiều token bị che bằng số token trong câu trả lời, và tính toán xác suất log của các token câu trả lời dưới mô hình tương ứng với những token bị che này.
Thủ tục để tính toán log-loss của câu trả lời vàng đã cho ngữ cảnh giống nhau qua tất cả tập dữ liệu. Chúng tôi mô tả chi tiết cụ thể tập dữ liệu để tính toán độ chính xác dưới đây.
CounterFact. Chúng tôi sử dụng độ chính xác generation để đánh giá thành công. Đối với GPT-J và Roberta, chúng tôi tạo ra một token duy nhất vì tất cả nhãn đều dài một token, trong khi đối với Llama chúng tôi tạo ra tới 10 token.
HotPotQA. Chúng tôi sử dụng độ chính xác generation để đánh giá thành công. Đối với GPT-J và Llama2, chúng tôi tạo ra tới 15 token. Đối với Roberta chúng tôi chỉ sử dụng 5 token vì Roberta gặp khó khăn để điền vào nhiều hơn một vài token bị che, điều này có thể hiểu được vì Roberta được đào tạo bằng cách che đi một số lượng nhỏ token (thường 15% token), và việc sử dụng những mô hình này để dự đoán một số lượng lớn token bị che sau đó gặp phải sự dịch chuyển phân phối.
Fever. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ {true, false} có xác suất cao nhất dưới mô hình.
21

--- TRANG 22 ---
Không gian Tìm kiếm siêu tham số LASER
τ Ma trận trọng số MLP Uin và Uout
ℓ tất cả lớp trong mô hình
ρ {0.9, 0.8, 0.6, 0.2, 0.1, 0.05, 0.01}
Bảng 5: Siêu tham số LASER
Bios Gender. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ {male, female} có xác suất cao nhất dưới mô hình.
Bios Profession. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ danh sách các nghề nghiệp có thể có xác suất cao nhất dưới mô hình.
BigBench Epistemic Reasoning. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ {entailment, non-entailment} có xác suất cao nhất dưới mô hình.
TruthfulQA. Chúng tôi sử dụng độ chính xác phân loại để đo thành công. Chúng tôi dự đoán nhãn từ {true, false} có xác suất cao nhất dưới mô hình.
BigBench WikidataQA. Vì tập hợp nhãn là mở, chúng tôi tính toán độ chính xác bằng cách sử dụng generation tương tự như CounterFact. Đối với GPT-J và Llama2 chúng tôi tạo ra tới 10 token, trong khi đối với Roberta chúng tôi tạo ra 5 token.
E.3 Mã
Chúng tôi sử dụng PyTorch cho tất cả thí nghiệm. Chúng tôi sử dụng implementation HuggingFace cho tất cả ba mô hình ngôn ngữ lớn. Chúng tôi sử dụng trọng số Llama2 7GB được cung cấp bởi Meta. Chúng tôi sử dụng implementation SVD có sẵn trong PyTorch cho thí nghiệm. Mã có thể được tìm thấy tại: https://github.com/pratyushasharma/laser
E.4 Chi tiết Tính toán
Chúng tôi chạy mỗi thí nghiệm trên một cluster với GPU V100 và A2600. Mỗi thí nghiệm mất khoảng 1-3 giờ để hoàn thành. Đối với tất cả thiết lập, chúng tôi tìm kiếm qua các siêu tham số được liệt kê trong Bảng 5. Đối với thiết lập GPT-J+CounterFact, tùy thuộc vào thí nghiệm và biểu đồ, chúng tôi chạy một tìm kiếm chi tiết hơn nhiều qua mỗi siêu tham số.
22
