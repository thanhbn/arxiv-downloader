# 2205.13571.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/approximation/2205.13571.pdf
# File size: 2727789 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LOW-RANK LOTTERY TICKETS :
FINDING EFFICIENT LOW -RANK NEURAL NETWORKS VIA
MATRIX DIFFERENTIAL EQUATIONS
Steffen Schotth√∂fer
Karlsruhe Institute of Technology
76131 Karlsruhe (Germany)
steffen.schotthoefer@kit.eduEmanuele Zangrando
Gran Sasso Science Institute
67100 L‚ÄôAquila (Italy)
emanuele.zangrando@gssi.it
Jonas Kusch
University of Innsbruck
6020 Innsbruck (Austria)
jonas.kusch1@gmail.comGianluca Ceruti
EPF Lausanne
1015 Lausanne (Switzerland)
gianluca.ceruti@epfl.chFrancesco Tudisco
Gran Sasso Science Institute
67100 L‚ÄôAquila (Italy)
francesco.tudisco@gssi.it
ABSTRACT
Neural networks have achieved tremendous success in a large variety of applications.
However, their memory footprint and computational demand can render them impractical
in application settings with limited hardware or energy resources. In this work, we
propose a novel algorithm to Ô¨Ånd efÔ¨Åcient low-rank subnetworks. Remarkably, these
subnetworks are determined and adapted already during the training phase and the overall
time and memory resources required by both training and evaluating them are signiÔ¨Åcantly
reduced. The main idea is to restrict the weight matrices to a low-rank manifold and
to update the low-rank factors rather than the full matrix during training. To derive
training updates that are restricted to the prescribed manifold, we employ techniques
from dynamic model order reduction for matrix differential equations. This allows us
to provide approximation, stability, and descent guarantees. Moreover, our method
automatically and dynamically adapts the ranks during training to achieve the desired
approximation accuracy. The efÔ¨Åciency of the proposed method is demonstrated through
a variety of numerical experiments on fully-connected and convolutional networks.
1 Introduction
While showing great performance in terms of classiÔ¨Åcation records, most state-of-the-art neural networks
require an enormous amount of computation and memory storage both for the training and the evaluation
phases [ 27]. These requirements not only increase infrastructure costs and energy consumption, but also
make the deployment of artiÔ¨Åcial neural networks to infrastructures with limited resources such as mobile
phones or smart devices prohibitive. On the other hand, it is well-known that networks‚Äô weights contain
structures and redundancies that can be exploited for reducing the parameter space dimension without
signiÔ¨Åcantly affecting the overall accuracy [9, 3, 17].
Network pruning is a popular line of research that addresses this problem by removing redundant pa-
rameters from pre-trained models. Typically, the initial network is large and accurate, and the goal is to
produce a smaller network with similar accuracy. Methods within this area include weight sparsiÔ¨Åcation
[20,24,49] and quantization [ 60,10], with different pruning techniques, including search-based heuris-
tics [ 24], reinforcement learning [ 2,23] and genetic algorithms [ 43]. More recent work has considered
These authors contributed equally to this work.arXiv:2205.13571v2  [cs.LG]  18 Oct 2022

--- PAGE 2 ---
pruning during training, by formulating pruning as a data-driven optimization problem [ 20,26,27]. The
resulting ‚Äúdynamical pruning‚Äù boils down to a parameter-constrained training phase which, however, has
been mostly focused on requiring sparse or binary weights so far.
Rather than enforcing sparsity or binary variables, in this work we constrain the parameter space to the
manifold of low-rank matrices. Neural networks‚Äô parameter matrices and large data matrices in general are
seldom full rank [ 53,56,48,15]. Constraining these parameters to lie on a manifold deÔ¨Åned by low-rank
matrices is thus a quite natural approach. By interpreting the training problem as a continuous-time
gradient Ô¨Çow, we propose a training algorithm based on the extension of recent Dynamical Low-Rank
Approximation (DLRA) algorithms [ 5,6,4]. This approach allows us to use low-rank numerical integrators
for matrix Ordinary Differential Equations (ODEs) to obtain modiÔ¨Åed forward and backward training
phases that only use the small-rank factors in the low-rank representation of the parameter matrices and
that are stable with respect to small singular values. This is a striking difference with respect to recent
alternative ‚Äúvanilla‚Äù low-rank training schemes [ 57,31] which simply factorize the weight matrices as the
product of two low-rank factors UV>and apply a descent algorithm alternatively on the two variables U
andV.
We perform several experimental evaluations on fully-connected and convolutional networks showing that
the resulting dynamical low-rank training paradigm yields low-parametric neural network architectures
which compared to their full-rank counterparts are both remarkably less-demanding in terms of memory
storage and require much less computational cost to be trained. Moreover, the trained low-rank neural
networks achieve comparable accuracy to the original full architecture. This observation is reminiscent of
the so-called lottery tickets hypothesis ‚Äî dense neural networks contain sparse subnetworks that achieve
high accuracy [ 17] ‚Äî and suggests the presence of low-rank winning tickets : highly-performing low-rank
subnetworks of dense networks. Remarkably, our dynamical low-rank training strategy seems to be able
to Ô¨Ånd the low-rank winning tickets directly during the training phase independent of initialization.
2 Related work on low-rank methods
Low-rank factorization using the SVD and other matrix decomposition techniques have been extensively
studied in the scientiÔ¨Åc computing and machine learning communities. The challenge of compressing
and speeding up large-scale neural networks using low-rank methods has sparked wide-spread research
interest in recent years and signiÔ¨Åcant effort has been put towards developing low-rank factorization
strategies for deep neural networks.
Previous works can roughly be categorized in approaches with Ô¨Åxed low rank and variable low rank during
training time. Fixed rank approaches decompose weight matrices using SVD or tensor decompositions
of pre-trained networks and Ô¨Åne-tune the factorized network [ 50,12,55,38], constrain weight matrices
to have a Ô¨Åxed low rank during training [ 30,57,31], or create layers as a linear combination of layers
of different rank [ 29]. Hence, these methods introduce the rank of the matrix decomposition as another
hyperparameter to be Ô¨Åne-tuned. Rank-adaptive methods mitigate this issue by automatic determination
and adaption of the low-rank structure after training. In particular, [ 33,34] apply heuristics to determine
the rank of the matrix decomposition ahead of time, whereas [ 59] encourages low-rank weights via a
penalized loss that depends on approximated matrix ranks.
Few methods have been proposed recently that adapt the ranks of the weight matrix alongside the
main network training phase. In [ 40], the authors set up the neural network training as a constrained
optimization problem with an upper bound on the ranks of the weights, which is solved in an alternating
approach resulting in an NP-hard mixed integer program. The authors of [ 28] formulate a similar
constrained optimization problem resulting in a mixed discrete-continuous optimization scheme which
jointly addresses the ranks and the elements of the matrices. However, both these approaches require
knowledge of the full weight matrix (and of its singular value decomposition) during training and overall
are more computational demanding than standard training.
In this work we overcome the above issues and propose a training algorithm with reduced memory and
computational requirements. To this end, we reinterpret the optimization problem of a neural network as
a gradient Ô¨Çow of the network weight matrices and thus as a matrix ODE. This continuous formulation
allows us to use recent advances in DLRA methods for matrix ODEs which aim at evolving the solution
2

--- PAGE 3 ---
of the differential equation on a low-rank manifold. The main idea of DLRA [ 35], which originates
from the Dirac-Frenkel variational principle [ 14,18], is to approximate the solution through a low-rank
factorization and derive evolution equations for the individual factors. Thereby, the full solution does not
need to be stored and the computational costs can be signiÔ¨Åcantly reduced. To ensure robustness of the
method, stable integrators have been proposed in [ 44] and [ 6]. Instead of evolving individual low-rank
factors in time, these methods evolve products of low-rank factors, which yields remarkable stability and
exactness properties [ 32], both in the matrix and the tensor settings [ 36,46,45,8]. In this work, we employ
the ‚Äúunconventional‚Äù basis update & Galerkin step integrator [ 6] as well as its rank-adaptive extension [ 4],
see also [ 37,7]. The rank-adaptive unconventional integrator chooses the approximation ranks according
to the continuous-time training dynamics and allows us to Ô¨Ånd highly-performing low-rank subnetworks
directly during the training phase, while requiring reduced training cost and memory storage.
3 Low-rank training via gradient Ô¨Çow
Consider a feed-forward fully-connected neural network N(x) =zM, withz0=x2Rn0,
zk=k(Wkzk 1+bk)2Rnk,k= 1;:::;M (the convolutional setting is discussed in the sup-
plementary material ¬ß6.6). We consider the training of Nbased on the optimization of a loss function
L(W1;:::;WM;N(x);y)by means of a gradient-based descent algorithm. For example, when using
gradient descent, the weights of Nat iterationt2Nare updated via
Wt+1
k=Wt
k rWkL(W1;:::;WM;N(x);y)8k= 1;:::;M (1)
with a learning rate . When the weight matrices Wkare dense, both the forward and gradient evaluations
of the network require a large number of full matrix multiplications, with high computational expense and
large memory footprint. This renders the training and the use of large-scale neural networks a difÔ¨Åcult
challenge on limited-resource devices. At the same time, a wealth of evidence shows that dense networks
are typically overparameterized and that most of the weights learned this way are unnecessary [ 48,15]. In
order to reduce the memory and computation costs of training, we propose a method that performs the
minimization over the manifold of low-rank matrices.
To this end, we assume that the ideal Wkcan be well-approximated by a matrix of rank rknk;nk+1of
the formUkSkV>
k2Rnknk 1, whereUk2Rnkrk,Vk2Rnk 1rkare thin and tall matrices having
orthonormal columns spanning optimal subspaces which capture essential properties of parameters, and
Sk2Rrkrkis a tiny full-rank matrix that allows us to extrapolate the useful information from the learned
subspacesUkandVk.
Traditional descent algorithms such as (1)do not guarantee the preservation of the low-rank structure
UkSkV>
kwhen updating the weights during training and require knowledge of the whole Wkrather than
the factorsUk;Sk;Vk. Here we reinterpret the loss minimization as a continuous-time gradient Ô¨Çow and
derive a new training method that overcomes all aforementioned limitations.
Minimizing the loss function with respect to Wkis equivalent to evaluating the long time behaviour of the
following matrix ODE that allows us to interpret the training phase as a continuous process:
_Wk(t) = rWkL(W1;:::;WM;N(x);y); (2)
where the ‚Äúdot‚Äù denotes the time-derivative. Let Mrkdenote the manifold of matrices with rank rkand
assume at a certain time t0the weights are in the manifold, i.e., Wk(t0)2Mrk. Using this continuous-
time interpretation allows us to derive a strategy to evolve the weights according to the dynamics in
(2)so thatWk(t)2Mrkfor alltt0. To this end, in ¬ß3.1 we exploit the fact that Wk(t)admits a
time-dependent factorization [ 13]Wk(t) =Uk(t)Sk(t)Vk(t)>to rewrite (2)as a system of matrix ODEs
for each of the individual factors Uk,SkandVk. Then, in ¬ß4 we propose an algorithm to efÔ¨Åciently
integrate the system of ODEs. We show in ¬ß4.1 that such algorithm reduces the loss monotonically and
is accurate, in the sense that UkSkV>
kWk, i.e. the learned low-dimensional subspaces UkandVk
well-match the behaviour of the full-rank network Wksolution of (2), through the action of the learned
Sk. Remarkably, using the dynamics of the individual factors will also allow us to adaptively adjust the
rankrkthroughout the continuous-time training process.
3

--- PAGE 4 ---
3.1 Coupled dynamics of the low-rank factors via DLRA
We consider the dynamical system of a single weight matrix Wk, while the remaining weight matrices
are Ô¨Åxed in time and are treated as parameters for the gradient. In the following, we omit writing these
parameters down for efÔ¨Åciency of exposition. Assuming Wk(t)2Mrk, we can formulate (2) as
minn
k_Wk(t) +rWkL(Wk(t))kF:_Wk(t)2TWk(t)Mrko
(3)
whereTWk(t)Mrkis the tangent space of Mrkat positionWk(t). In order to solve (3), we further observe
that (3) can be equivalently formulated as the following Galerkin condition [35]:
h_Wk(t) +rWkL(Wk(t));Wki= 08Wk2TWk(t)Mrk: (4)
FromWk=UkSkV>
k, a generic element Wkof the tangent space TWk(t)Mrkcan be written as
Wk=UkSkV>
k+UkSkV>
k+UkSkV>
k;
whereUkandVkare generic elements of the tangent space of the Stiefel manifold with rkorthonormal
columns at the points UkandVk, respectively, and Skis a genericrkrkmatrix, see e.g. [ 35, ¬ß2] for
details. Additionally, the Gauge conditions U>
kUk= 0 andV>
kVk= 0 must be imposed to ensure
orthogonality of the basis matrices, and the uniqueness of the representation of the tangent space elements.
Similarly, by the chain rule applied several times we have
_Wk=d
dt
UkSkV>
k	
=_UkSkV>
k+Uk_SkV>
k+UkSk_V>
k:
Now, the Galerkin condition (4) becomes
h_UkSkV>
k+Uk_SkV>
k+UkSk_V>
k+rWkL(Wk(t));Wki= 0;8Wk2TWk(t)Mrk(5)
withU>
k_Uk= 0andV>
k_Vk= 0. If we choose Wk=UkSkV>
kin (5), we obtain
hU>
k_UkSkV>
kVk+U>
kUk_SkV>
kVk+U>
kUkSk_V>
kVk+U>
krWkL(Wk(t))Vk;Ski= 0:
Thus, using the Gauge conditions, we obtain h_Sk+U>
krWkL(Wk(t))Vk;Ski= 0, which has to hold
for a generic rkrkmatrixSk. We obtain this way an evolution equation for the Sk(t)factor. Similarly,
specifying (5)for the two choices Wk=UkSkV>
kandWk=UkSkV>
k, allows us to obtain the
following system of differential equations for the individual factors of Wk:
8
<
:_Sk= U>
krWkL(Wk(t))Vk;
_Uk= (I UkU>
k)rWkL(Wk(t))VkS 1
k;
_Vk= (I VkV>
k)rWkL(Wk(t))>UkS >
k:(6)
4 KLS-based training algorithm
In order to perform an efÔ¨Åcient and robust rank-constrained training step, we numerically integrate
the system of ODEs (6). Our approach is based on the ‚Äúunconventional KLS integrator‚Äù [ 6] and its
rank-adaptive version [ 4]. The pseudocode of the proposed training strategy is presented in Algorithm 1.
The main idea of the KLS algorithm is to alternately represent the product Wk=UkSkV>
kasWk=
KkV>
kandWk=UkL>
k, consider the corresponding coupled ODEs from (6), and then perform three
main steps:
1,2. K&L-steps (in parallel). Update the current KkandLkby integrating the differential equations
_Kk(t) = rWkL(Kk(t)V>
k)Vk; Kk(0) =UkSk;
_Lk(t) = rWkL(UkLk(t)>)>Uk; Lk(0) =VkS>
k;(7)
fromt= 0tot=; then form new orthonormal basis matrices eUkandeVkspanning the range of the
computedKk()andLk().
4

--- PAGE 5 ---
3.S-step . Update the current Skby integrating the differential equation
_Sk(t) = eU>
krWkL(eUkSk(t)eV>
k)eVk (8)
fromt= 0tot=, with initial value condition Sk(0) =eU>
kUkSkV>
keVk.
An important feature of this algorithm is that it can be extended to rank adaptivity in a relatively
straightforward manner [ 4], letting us dynamically evolve the rank of Sk(and thus the rank of Wk) during
the computation. This is particularly useful, as we may expect the weight matrices to have low ranks but
we may not know what the ‚Äúbest‚Äù ranks for each layer are. Typically, dynamically adapting the ranks of a
low-rank optimization scheme is a challenging problem as moving from the manifold MrktoMrk1
introduces singular points [ 19,1]. Instead, treating the training problem as a system of matrix differential
equations allows us to overcome this issue with a simple trick: at each step of the KLS integrator we
double the dimension of the basis matrices eUkandeVkcomputed in the K- and L-steps by computing
orthonormal bases spanning [Kk()jUk]and[Lk()jVk], respectively, i.e. by augmenting the current
basis with the basis computed in the previous time step. Then, after the new Skmatrix is computed via
the S-step, a truncation step is performed, removing from the newly computed Skmatrix all the singular
values that are under a certain threshold #.
Of course, adding the rank-adaptivity to the integrator comes at a cost. In that case, each step requires
to perform an SVD decomposition of twice the size of the current rank of Skin order to be able to
threshold the singular values. Moreover, the dimension of the bases UkandVkmay grow, which also may
require additional computational effort. However, if the ranks remain small throughout the dynamics, this
computational overhead is negligible, as we will further discuss in ¬ß4.3 and ¬ß5.
4.1 Error analysis and convergence
In this section we present our main theoretical results, showing that (a) the low-rank matrices UkSkV>
kformed by the weights‚Äô factors computed with Alg. 1 are close to the true solution of (2), and (b) that
the loss function decreases during DLRT, provided the singular value threshold #is not too large, i.e.,
is bounded by a constant times the square of the time-step size (see Theorem 1). In the version we
present here, part of the statements are presented in an informal way for the sake of brevity. We refer to
the supplementary material ¬ß6.1 for details and for the proofs.
Assume the gradient Ô¨Çow Fk(Z) = rWkL(W1;:::;Z;:::;W M;N(x);y)in(2)is locally bounded
and locally Lipschitz continuous, with constants C1andC2, respectively. Then,
Theorem 1. Fixedxandy, letWk(t)be the (full-rank) continuous-time solution of (2)and letUk;Sk;Vk
be the factors computed with Algorithm 1 after tsteps. Assume that the K,L,S steps (7)and (8)are
integrated exactly from 0to. Assume moreover that, for any Z2MrksufÔ¨Åciently close to Wk(t), the
whole gradient Ô¨Çow Fk(Z)is ‚Äú"-close‚Äù toMrk. Then,
kUkSkV>
k Wk(t)kFc1"+c2+c3#= k = 1;:::;M
where the constants c1,c2andc3depend only on C1andC2. In particular, the approximation bound does
not depend on the singular values of the exact nor the approximate solution.
Observe that, while the loss function Ldecreases monotonically along any continuous-time solution
Wk(t)of(2), it is not obvious that the loss decreases when the integration is done onto the low-rank
manifold via Algorithm 1. The next result shows that this is indeed the case, up to terms of the order of
the truncation tolerance #. More precisely, we have
Theorem 2. LetWt
k=Ut
kSt
k(Vt
k)>be the low rank weight matrix computed at step tof Algorithm 1 and
letL(t) =L(Wt
1;:::;Wt
M;N(x);y). Then, for a small enough time-step we have
L(t+ 1)L(t) +#
whereandare positive constants that do not depend on t,and#.
4.2 EfÔ¨Åcient implementation of the gradients
All the three K,L,S-steps require the evaluation of the gradient Ô¨Çow of the loss function with respect to the
whole matrix Wk. Different approaches to efÔ¨Åciently compute this gradient may be used. The strategy we
5

--- PAGE 6 ---
Algorithm 1: Dynamic Low Rank Training Scheme (DLRT)
Input : Initial low-rank factors S0
kr0
kr0
k;U0
knkr0
k;V0
knk 1r0
kfork= 1;:::;M ;
iter : maximal number of descent iterations per epoch;
adaptive : Boolean Ô¨Çag that decides whether or not to dynamically update the ranks;
#: singular value threshold for adaptive procedure.
1foreach epoch do
2 fort= 0tot=iter do
3 foreach layerkdo
4 Kt
k Ut
kSt
k/* K-step */
5 Kt+1
k one-step-integrate_K(t) = rKL(K(t)(Vt
k)>zk 1+bt
k),K(0) =Kt
k	
6 Lt
k Vt
k(St
k)>/* L-step */
7 Lt+1
k one-step-integrate_L(t) = rLL(Ut
kL(t)>zk 1+bt
k),L(0) =Lt
k	
8 ifadaptive then /* Basis augmentation step */
9 Kt+1
k [Kt+1
kjUt
k]
10 Lt+1
k [Lt+1
kjVt
k]
11 Ut+1
k orthonormal basis for the range of Kt+1
k/* S-step */
12 Mk (Ut+1
k)>Ut
k
13 Vt+1
k orthonormal basis for the range of Lt+1
k
14 Nk (Vt+1
k)>Vt
k
15eSt
k MkSt
kN>
k
16 St+1
k one-step-integrate_S(t)= rSL 
Ut+1
kS(t) (Vt+1
k)>zk 1+bt
k
;S(0)=eSt
k	
17 ifadaptive then /* Rank compression step */
18 P;;Q SVD(St+1
k)
19 St+1
k truncate using the singular value threshold #
20 Ut+1
k Ut+1
kePwhereeP= [Ô¨Årstrt+1
kcolumns ofP]
21 Vt+1
k Vt+1
keQwhereeQ= [Ô¨Årstrt+1
kcolumns ofQ]
/* Bias update step */
22 bt+1
k one-step-integrate_b(t)= rbL(Ut+1
kSt+1
k(Vt+1
k)>zk 1+b(t));b(0)=bt
k	
discuss below aims at reducing memory and computational costs by avoiding the computation of the full
gradient, working instead with the gradient with respect to the low-rank factors.
To this end, we note that for the K-step it holds rWkL(Kk(t)V>
k)Vk=rKkL(Kk(t)V>
k). Hence, the
whole gradient can be computed through a forward run of the network with respect to Kk
zk=k
Kk(t)V>
kzk 1+bk
; k = 1;:::;M (9)
and taping the gradient with respect to Kk. In this way, the full gradient does not need to be computed
and the overall computational costs are comprised of running a forward evaluation while taping gradients
with respect to Kk, analogously to the traditional back-propagation algorithm. The L- and S-steps can
be evaluated efÔ¨Åciently in the same manner, by evaluating the network while taping the gradients with
respect toLkandSk, respectively. Hence, instead of a single gradient tape (or chain rule evaluation) of
the full weight matrix network, we have three gradient tapes, one for each low rank step, whose combined
computational footprint is less than the full matrix tape. We provide detailed formulas for all the three
gradient tapes in the supplementary material ¬ß6.5.
4.3 Implementation details, computational costs and limitations
Each step of Alg. 1 requires the computation of two orthonormal bases for the ranges of Kt+1
kandLt+1
k.
There are of course different techniques to compute such orthonormal matrices. In our implementation
6

--- PAGE 7 ---
we use the QR algorithm, which is known to be one of the most efÔ¨Åcient and stable approaches for this
purpose. In the adaptive strategy the singular values of St+1
kare truncated according to a parameter #. To
this end, in our implementation, we use the Frobenius norm of . Precisely, we truncate  = diag(i)
at step 19 of Alg. 1 by selecting the smallest principal rrsubmatrix such that (P
ir+12
i)1=2#.
Finally, one-step-integrate denotes a numerical procedure that integrates the corresponding ODE from time
t= 0tot=. In practice one can employ different numerical integrators, without affecting the ability of
the algorithm to reduce the loss function (see [ 4, Thm. 5]) while maintaining the low-rank structure. In
our implementation we used two methods:
1.Explicit Euler. This method applied to the gradient Ô¨Çow coincides with one step of Stochastic Gradient
Descent (SGD), applied to the three factors Kk;Lk;Skindependently.
2.Adam. Here we formally compute the new factors by modifying the explicit Euler step as in the Adam
optimization method. Note that Nesterov accelerated SGD is known to coincide with a particular linear
multistep ODE integrator [ 51]. While Adam does not directly correspond to a numerical integrator
to our knowledge, in our tests it resulted in a faster decrease of the loss than both Euler (SGD) and
Nesterov accelerated SGD.
For both choices, the target time step corresponds to the value of the learning rate, which we set to 0:2
for Euler. For Adam, we use the default dynamical update, setting 0:001as starting value.
Computational cost. To obtain minimal computational costs and memory requirements for the K-step,
the ordering of evaluating KkV>
kzk 1in(9)is important. First, we compute ez:=V>
kzk 12Rrk
which requires O(rknk 1)operations. Second, we compute Kkezwhich requires O(rknk)operations.
Adding the bias term and evaluating the activation function requires O(nk)operations. Hence, combined
over all layers we have asymptotic cost of O(P
krk(nk+nk+1)). Taping the forward evaluation to
compute the gradient with respect to Kkas discussed in ¬ß4.2 does not affect the asymptotic costs, i.e.
the costs of computing the K-step at layer kassuming a single data point xrequiresCK.P
krk(nk+
nk+1)operations. In a similar manner, we obtain the computational costs of the L- and S-steps, which
are againCL;S.P
krk(nk+nk+1). Moreover, the QR decompositions used in the K- and L-step
requireO P
kr2
k(nk+nk 1)
operations and computing the SVD in the truncation step has worst-
case cost of O P
kr3
k
. Hence, assuming rknk;nk+1, the cost per step of our low-rank method
isCDLRA.P
kr2
k(nk+nk 1), opposed to the dense network training, which requires Cdense.P
knknk+1operations. In terms of memory cost, note that we only need to store rk(1 +nk+nk+1)
parameters per layer during the algorithm, corresponding to the matrices St
k;Ut
k;Vt
k. Moreover, at the
end of the training we can further compress memory by storing the product of the trained weight factors
UkSk, rather than the individual matrices.
Limitations. A requirement for DLRT‚Äôs efÔ¨Åciency is that rknk;nk+1. When the truncation threshold
#is too small, Alg. 1 does not provide advantages with respect to standard training. This is also shown by
Fig. 1. Moreover, in the terminology of [ 54], DLRT is designed to reduce training costs corresponding to
model parameters and to the optimizer. To additionally decrease activation costs, DLRT can be combined
with micro-batching or checkpointing approaches. Finally, the choice of #introduces one additional
hyperparameter which at the moment requires external knowledge for tuning. However, our experiments
in ¬ß5 show that relatively large values of #yield competing performance as compared to a number of
baselines, including standard training.
5 Numerical Results
We illustrate the performance of DLRT Algorithm 1 on several test cases. The code
is implemented in both TensorÔ¨Çow (https://github.com/CSMMLab/DLRANet) and PyTorch
(https://github.com/COMPiLELab/DLRT). The networks are trained on an AMD Ryzen 9 3950 X CPU
and a Nvidia RTX 3090 GPU. Timings are measured on pure CPU execution.
5.1 Performance analysis on MNIST dataset
We partition MNIST dataset [ 11] in randomly sampled train-validation-test sets of size 50K-10K-10K.
Images are pixelwise normalized; no further data augmentation or regularization has been used.
7

--- PAGE 8 ---
(a) Training timings
 (b) Prediction timings
Figure 1: Comparison of batch execution and training times of 5-layer, 5120 -neurons low-rank networks
of different ranks and a non-factorized reference network with the same architecture on the MNIST dataset.
Training times shown correspond to one epoch for a batch of 256 datapoints. Prediction times refer instead
to the whole dataset. All the times are the average over 1000 runs.
Fixed-rank fully-connected feed-forward network timings. First, we compare the training time of
the adaptive DLRT Alg. 1 on a 5-layer fully-connected [5120;5120;5120;5120;10]network Ô¨Åxing the
ranks of layers 1-4, i.e. choosing a speciÔ¨Åc starting rank r0
kfor the input weight factors and truncating 
at line 19 of Alg. 1 to the principal r0
kr0
ksubmatrix, rather than via a threshold. Next, we measure the
average prediction time on the whole MNIST dataset over 1000 runs. Fig. 1(a) and 1(b) show that both
timings scale linearly with the rank of the factorizations, and that for sufÔ¨Åciently small ranks DLRT is
faster than the full-rank baseline both in terms of training and of prediction.
Rank evolution and performance of DLRT for different singular value thresholds. Next, we
demonstrate the capabilities of DLRT to determine the rank of the network‚Äôs weight matrices auto-
matically during the network training using Algorithm 1. The Adam optimizer with default learning rate
is used for the gradient update. We train fully connected 5-layer networks, of which the Ô¨Årst 4are replaced
by low-rank layers in the subsequent tests. The activation function is chosen to be ReLU for the hidden
layers, and softmax for the output layer. The training loss is sparse categorical cross entropy and we
additionally measure the model‚Äôs accuracy. We use batch size 256and train for 250epochs. We choose
#=kk, thus we truncate the singular values of the current St
kby a fraction of the total Frobenius
norm. The smaller , the more singular values are kept.
Figures 2 (a) and (b) show the evolution of the rank adaptive layers of a 5-layer 500-neuron network
in a long time case study for = 0:05and= 0:15. We can see that within the Ô¨Årst epoch the initial
full matrix ranks are reduced signiÔ¨Åcantly, to 27for= 0:15, and to85for= 0:05respectively.
Within the Ô¨Årst 50epochs, the layer ranks are already close to their Ô¨Ånal ranks. This indicates that
the rank adaptive algorithm is only needed for the Ô¨Årst few training epochs, and can then be replaced
by the computationally cheaper Ô¨Åxed-low-rank training (by setting the Boolean variable adaptive to
False in Algorithm 1). Figure 3 compares the mean test accuracy of 5-layer networks with 500and784
neurons with different levels of low-rank compression, over Ô¨Åve independent runs with randomly sampled
train-test-val sets. The networks can be compressed via dynamical low-rank training by more than 95%,
while only losing little more than 1%test accuracy compared to the dense reference network marked in red.
Remark that restricting the space of possible networks to a given rank regularizes the problem, since such
a restriction can be understood as adding a PCR regularization term to the loss function. This can be seen
from the tendency of not overÔ¨Åtting and reaching improved test accuracies compared to the corresponding
dense network for moderate compression ratios. Also note that adaptive-low rank training eliminates
the need for hyperparameter grid search in terms of layer-weights, due to automatic rank adaptation.
The rank dynamics for all conÔ¨Ågurations can be seen in the supplementary material ¬ß6.3. Finally, in
the supplementary material ¬ß6.4 we compare the use of DLRT with the vanilla approach which simply
thresholds the singular values of the full-rank network. Our results show that advantageous low-rank
winning tickets exist, but are not easy to Ô¨Ånd. In fact, the vanilla low-rank subnetworks perform very
poorly. From this point of view, our approach can be seen as an efÔ¨Åcient dynamical pruning technique,
able to determine high-performing low-rank subnetworks in a given dense network. Remarkably, our
8

--- PAGE 9 ---
(a) Rank evolution for = 0:15
 (b) Rank evolution of = 0:05
Figure 2: Rank evolution (layers 1-4) of 5-layer [500,500,500,500,10] fully-connected net on MNIST.
Figure 3: Mean test accuracy over parameters‚Äô number and compression rate for 5 runs with randomly
sampled train-test-val sets on 5-layer fully-connected nets. Red dots denote the full-rank baseline.
numerical experiments suggest that low-rank winning tickets can be trained from the start and do not to
heavily depend on the initial weight guess.
Convolutional layers: LeNet5. Here we compare the proposed dynamical low-rank training scheme
on LeNet5 [ 39] on MNIST, against the full-rank reference and several baselines. SVD prune [ 61] and
LRNN [28] are the closest approaches to our DLRT: they dynamically train low-rank layers by adding a
rank-penalty to the loss function, and by complementing the standard training step via an SVD projection
step in the latter and a pruning step in the former. While computing low-rank factors for each layer, thus
reducing memory storage of the network, this training approach is more expensive than training the full
network. GAL [ 42], SSL [ 62], and NISP [ 58] are pruning methods which aim at learning optimal sparse
weights (rather than low-rank) by adding sparsity-promoting regularization terms to the training loss. As
for LRNN, these methods do not reduce the computational cost of the training phase (as indicated with
the<0%in Table 1). Analogously to [ 28], our adaptive low-rank training technique is applied to the
convolutional layers by Ô¨Çattening the tensor representing the convolutional kernel into a matrix. Details
are provided in the supplementary material ¬ß6.6. All the models are trained for 120epochs using SGD
with a Ô¨Åxed learning rate of 0:2. Results in Table 1 show that the DLRT algorithm is able to Ô¨Ånd low-rank
subnetworks with up to 96:4%less parameters than the full reference, while keeping the test accuracy
above 95%. Compared to the baseline methods, we achieve better compression rates but observe lower
accuracy. However, unlike the baseline references, DLRT automatically prunes the singular values during
training, without requirement to solve any additional optimization problem, thus signiÔ¨Åcantly improving
time and memory efÔ¨Åciency of both forward and backward phases, with respect to the full reference.
Robustness with respect to small singular values and comparison with vanilla low-rank
parametrization. A direct way to perform training enforcing a Ô¨Åxed rank for the weight matrices
is to parameterize each weight as Wk=UkV>
kand alternating training with respect to Ukand toVk. This
is the strategy employed for example in [ 57,31]. This vanilla low-rank parametrization approach has a
number of disadvantages with respect to DLRT, on top of the obvious non-adaptive choice of the rank.
First, DLRT guarantees approximation and descent via Theorems 1 and 2. Second, we observe that the
vanilla factorization gives rise to an ill-conditioned optimization method when small singular values occur.
9

--- PAGE 10 ---
0 2 4 6 8
epoch20406080100test accuracy [%]
no decay
DLRT
UVT factorization
0 2 4 6 8
epoch
decay
DLRT
UVT factorization
0 2 4 6 8
epoch0.00.51.01.52.02.5train loss
no decay
DLRT
UVT factorization
0 2 4 6 8
epoch
decay
DLRT
UVT factorization        T est Accuracy                                                                                            Train LossFigure 4: Mean learning curves with standard deviation of Lenet5 on MNIST over 10 runs of DLRT
compared to a vanilla layer factorization Wk=UkV>
k. Both methods are implemented with Ô¨Åxed learning
rate of 0:01, and batch size of 128. The weight matrices are either completely randomly initialized (‚Äúno
decay‚Äù) or are initialized with a random choice forced to have an exponential decay on the singular values
(‚Äúdecay‚Äù).
Table 1: Results of the training of LeNet5 on MNIST dataset. Effective parameters represent the number
of parameters we have to save for evaluating the network and those we need in order to train via the DLRT
Alg.1. The compression ratio (c.r.) is the percentage of parameter reduction with respect to the full model
(<0%indicates that the ratio is negative). ‚Äúft‚Äù indicates that the model has been Ô¨Åne-tuned. ‚ÄúLeNet5‚Äù
denotes the standard LeNet5 architecture trained with SGD.
NN metrics Evaluation Train
method test acc. ranks params c.r. params c.r.
LeNet5 99:2% [20;50;500;10] 430500 0% 430500 0%DLRT= 0:11 98:0% [15 ;46;13;10] 47975 88 :86% 50585 88 :25%
= 0:15 97:8% [13 ;31;9;10] 34435 92 :0% 35746 91 :7%
= 0:2 97:2% [10 ;20;7;10] 25650 94 :04% 26299 93 :89%
= 0:3 95:3% [6 ;9;4;10] 15520 96:4% 15753 96:34%
SSL [62] (ft) 99:18% 110000 74 :4% <0%
NISP [58] (ft) 99:0% 100000 76 :5% <0%
GAL [42] 98:97% 30000 93 :0% <0%
LRNN [28] 98:67% [3 ;3;9;9] 18075 95 :8% <0%
SVD prune [61] 94:0% [2 ;5;89;10] 123646 71 :2% <0%
This problem is peculiar to the low-rank manifold itself [ 35,16], whose local curvature is proportional to
the inverse of the smallest singular value of the weight matrices. In contrast, the numerical integration
strategy at the basis of DLRT is designed to take advantage of the structure of the manifold and is robust
with respect to small singular values [ 32]. This can be seen from the bound of Theorem 1, where the
constants are independent of the singular values of the weight matrices, and is illustrated by Figure 4,
where DLRT shows a much faster convergence rate with respect to vanilla SGD performed on each factor
of the parametrization UkV>
k, when applied to train LeNet5 on MNIST. Both methods are implemented
with the same Ô¨Åxed learning rate.
5.2 Results on ImageNet1K and Cifar10 with ResNet-50, AlexNet, and VGG16
Finally, we assess the capability of compressing different architectures on large scale training sets. We
train a full-rank baseline model and compare it to DLRT using the same starting weights on an Nvidia
A-100 GPU. The used optimizer is SGD with momentum factor 0:1and no data-augmentation techniques
are used. We compare the results on ResNet-50, VGG16, and AlexNet models, on the Cifar10 and
ImageNet1k datasets, and with respect to a number of low-parametric alternative baselines methods. For
DLRT, the last layers of the networks have been adapted to match the corresponding classiÔ¨Åcation tasks.
Detailed results are reported in Table 2, where we show the test accuracy (reported as the difference with
respect to the full baseline) as well as compression ratios. With Cifar10, we archive a train compression of
77:5%with an accuracy loss of just 1:89% for VGG16 and 84:2%train compression at 1:79% accuracy
loss for AlexNet. In the ImageNet1k benchmark, we achieve a train compression rate of 14:2%, with an
test accuracy loss of 0:5%in top-5 accuracy on ResNet-50 and 78:4%train compression with 2:19top-5
accuracy loss on VGG16.
10

--- PAGE 11 ---
Table 2: Results on ImageNet1k (left) and Cifar10 (right). The compression ratio is the percentage of
parameter reduction with respect to the full model. DLRT is used with = 0:1. The number of parameters
of the full models are: 33.6M (VGG16); 23.6M (AlexNet); 29.6M (ResNet-50). We report difference in
test accuracy (top-5 test accuracy for ImageNet1k) with respect to the full baselines.
ImageNet1k
test acc.[%] compression rate
method (to baseline) eval[%] train[%]ResNet-50DLRT  0:56 54:1 14:2PP-2[52]  0:8 52:2<0
PP-1[52]  0:2 44:2<0
CP[25]  1:4 50:0<0
SFP[22]  0:2 41:8<0
ThiNet[47]  1:5 36:9<0VGG16DLRT  2:19 86 78 :4PP-1[52]  0:19 80:2<0
CP[25]  1:80 80:0<0
ThiNet[47]  0:47 69:04<0
RNP(3X)[41]  2:43 66:67<0Cifar10
test acc.[%] compression rate
method (to baseline) eval[%] train[%]VGG16DLRT  1:89 56 77 :5GAL[42]  1:87 77 <0
LRNN[28]  1:9 60 <0AlexNetDLRT  1:79 86:3 84:2NISP[58]  1:06  <0
6 Appendix
6.1 Proofs of the main results
We provide here a proof of Theorems 1 and 2. The proof is based on a number of classical results as well
as recent advances in DLRA theory, including [35, 44, 32, 4, 6].
Recall that, for a Ô¨Åxed layer k, we reinterpret the training phase as a continuous-time evolution of the
weights on the manifold of low-rank matrices, as illustrated in Fig. 5(a-b). This boils down to solving the
manifold-constrained matrix differential equation
minn
k_Wk(t) Fk(Wk(t))k:_Wk(t)2TWk(t)Mrko
; (10)
wherekk is the Frobenius norm and Fkdenotes the gradient Ô¨Çow of the loss with respect to the k-th
matrix variable, namely
Fk(Z) = rWkL(W1;:::;Z;:::;W M;N(x);y):
For the sake of simplicity and for a cleaner notation, as all the results we will present hold for a generic
k, we drop the subscript kfrom now on. In particular, we assume Wis the weight matrix of a generic
hidden layer with ninput andmoutput neurons.
In order for our derivation to hold, we require the following two properties:
(a) Discrete time weight update
(b) Continuous time weight
update
(c) Galerkin condition
Figure 5: Panels (a)-(b): Graphical re-interpretation of the weight update step as a time-continuous process.
Panel (c): Orthogonal projection onto the tangent space of the low-rank manifold Mr. The dashed line
depicts the projection resulting in _Wk(t), which is the tangent element minimizing the distance between
rWkL(Wk(t))and the tangent space TWk(t)Mrat the approximation Wk(t).
11

--- PAGE 12 ---
P1.The gradient Ô¨ÇowFis locally bounded and locally Lipschitz continuous, with constants C1andC2,
respectively. Namely, we assume there exist C1;C2>0(independent of k) such that
kF(Z)kC1kF(Z) F(~Z)kC2kZ ~Zk
for allZ;~Z2Rmn.
P2.The whole gradient Ô¨Çow is ‚Äúnot too far‚Äù from the rank- rmanifoldMr. Precisely, we assume that for
anyZ2Mrarbitrary close to W(t), the whole gradient Ô¨Çow F(Z)neartis such that
k(I P(Z))F(Z)k";
where P(Z)denotes the orthogonal projection onto TZMr.
Note that both assumptions are valid for low-rank neural network training. In particular, Lipschitz
continuity and boundedness of the gradient are standard assumptions in optimization and are satisÔ¨Åed by
the gradient of commonly used neural networks‚Äô losses. Moreover, assuming the gradient Ô¨Çow to be close
to the low-rank manifold is an often encountered empirical observation in neural networks [53, 48, 15].
In order to derive the proof of Theorems 1 and 2 we Ô¨Årst present a number of relevant background
lemmas. The Ô¨Årst lemma shows that the subspace generated by the K-step in Algorithm 1 after the
QR-decomposition is O((+"))close to the range of the exact solution, where is the time-step of the
integrator and "is the eigenvalue truncation tolerance.
Lemma 1 ([6, Lemma 2]) .LetW1be the solution at time t=of the full problem (2) with initial
conditionW0. LetU1be the matrix obtained with the K-step of the Ô¨Åxed-rank Algorithm 1, after one step.
Under the assumptions P1 and P2 above, we have
kU1U1;>W1 W1k
where
=C1C2(4eC2+ 9)2+ (3eC2+ 4)":
Proof. The local error analysis of [32] shows that there exists L1such that
kU1L1;> W1k:
It follows that,
kU1L1;> W1k2=kU1L1;> U1U1;>W1+U1U1;>W1 W1k2
=kU1U1;>(U1L1;> W1) + (I U1U1;>)( W1)k2
=kU1U1;>(U1L1;> W1)k2+k(I U1U1;>)W1k2:
Therefore,
kU1U1;>(U1L1;> W1)k2+k(I U1U1;>)W1k22:
Hence, since both terms must be bounded by 2individually, we obtain the stated result.
In the next lemma we show that also the space generated by the Lstep is close by the exact solution.
Namely, combined with the previous result, we have
Lemma 2 ([6, Lemma 3]) .LetW1,U1be deÔ¨Åned as above. Let V1be the matrix obtained from the
L-step of the Ô¨Åxed-rank Algorithm 1, after one step. The following estimate holds:
kU1U1;>W1V1V1;> W1k2:
Proof. The L-step is obtained as the K-step applied to the transposed function G(Y) =F(Y>)>. Due
to the invariance of the Frobenius norm under transposition, property P1 holds. Similarly, property P2
continues to be satisÔ¨Åed because
k(I P(Y))G(Y)k=k(I P(Y>))F(Y>)k";
12

--- PAGE 13 ---
where the equality P(Y)Z>=
P(Y>)Z>has been used [35, ¬ß4]. It follows from Lemma 1 that
kU1U1;>W1 W1k;
kV1V1;>W1;> W1;>k:(11)
This implies that
kU1U1;>W1V1V1;> W1kkU1U1;>W1V1V1;> W1V1V1;>+W1V1V1;> W1k
kU1U1;>W1V1V1;> W1V1V1;>k+kW1V1V1;> W1k
k 
U1U1;>W1 W1
V1V1;>k+kV1V1;>W1;> W1;>k
kU1U1;>W1 W1kkV1V1;>k2+kV1V1;>W1;> W1;>k:
BecausekV1V1;>k2= 1, the stated result follows from (11).
With the previous lemmas, we are in the position to derive the local error bound for the Ô¨Åxed-rank KLS
integrator of Section 4.
Lemma 3 (Local Error, [ 6, Lemma 4]) .LetW1;U1;V1be deÔ¨Åned as above and let S1be the matrix
obtained with the S-step of Algorithm 1 after one step. The following local error bound holds:
kU1S1V1;> W1k(^c1"+ ^c2);
where the constants ^ciare independent of the singular values of W1andS1.
Proof. From Lemma 2 and the equality Y1=U1S1V1;>, we have that
kY1 W1kkY1 U1U1;>W1V1V1;>k+kU1U1;>W1V1V1;> W1k
kU1(S1 U1;>W1V1)V1;>k+ 2
kS1 U1;>W1V1k+ 2:
The local error‚Äôs analysis is reduced to bound the term kS1 U1;>W1V1k. For 0t, we thus
introduce the following auxiliary quantity:
eS(t) :=U1;>W(t)V1:
We observe that the term W(t)can be re-written as
W(t) =U1U1;>W(t)V1V1;>+
W(t) U1U1;>W(t)V1V1;>
=U1eS(t)V1;>+R(t);
whereR(t)denotes the term in big brackets. For 0t, it follows from Lemma 2 and the bound C1
of the functionFthat
kW(t) W()kZ
0k_W(s)kds=Z
0kF(W(s))kdsC1:
Therefore, the term R(t)is bounded by
kR(t)kkR (t) R()k+kR()k2C1+ 2:
We re-cast the function F 
W(t)
as
F 
W(t)
=F 
U1eS(t)V1;>+R(t)
=F 
U1eS(t)V1;>
+D(t)
where the defectD(t)is given by
D(t) :=F 
U1eS(t)V1;>+R(t)
 F 
U1eS(t)V1;>
:
Via the Lipschitz constant C2of the functionF, the defect is bounded by
kD(t)kC2kR(t)k2C2(C1+):
13

--- PAGE 14 ---
Now, we compare the two differential equations
_eS(t) =U1;>F 
U1eS(t)V1;>
V1+U1;>D(t)V1;eS(0) =U1;>W0V1;
_S(t) =U1;>F 
U1S(t)V1;>
V1; S (0) =U1;>W0V1:
The solution S1obtained in the second differential equation is the same as given by the S-step of the KLS
integrator of Section 4. By construction, the solution obtained in the Ô¨Årst differential equation at time
t=iseS() =U1;>W1V1. With the Gronwall inequality we obtain
kS1 U1;>W1V1kZ
0eC2( s)kD(s)kdseL2C2(C1+):
The result yields the statement of the theorem using the deÔ¨Ånition of .
We are now in the position to conclude the proof of Theorem 1.
Proof of Theorem 1. In Lemma 3, the local error for the Ô¨Åxed-rank integrator of ¬ß4 has been provided.
The local error in time of the rank-adaptive version is directly obtained via a triangle inequality:
kU1S1V1;> W()k^c1"+ ^c22+#;
where#is the tolerance parameter chosen for the truncation procedure. Here, we abuse the notation and
we maintain the same nomenclature U1;S1;andV1also for the novel low-rank approximation obtained
via the truncation procedure.
Thus, we conclude the proof using the Lipschitz continuity of the function F. We move from the local
error in time to the global error in time by a standard argument of Lady Windermere‚Äôs fan [ 21, Section
II.3]. Therefore, the error after tsteps of the rank-adaptive Algorithm 1 is given by
kUtStVt;> W(t)kc1"+c2+c3#=:
To conclude with, we prove that after one step the proposed rank-adaptive DLRT algorithm decreases
along the low-rank approximations. We remind that only property P1 needs to be assumed here.
Proof of Theorem 2. LetbY(t) =U1S(t)V1;>. Here,S(t)denotes the solution for t2[0;]of the S-step
of the rank-adaptive integrator . It follows that
d
dtL(bY(t)) =hrL(bY(t));_bY(t)i
=hrL(bY(t));U1_S(t)V1;>i
=hU1;>rL(bY(t))V1;_S(t)i
=hU1;>rL(bY(t))V1; U1;>rL(bY(t))V1i= kU1;>rL(bY(t))V1k2:
The last identities follow by deÔ¨Ånition of the S-step. For t2[0;]we have
d
dtL(bY(t)) 2(12)
where= min 01kU1;>rL bY()
V1k. Integrating (12) from t= 0untilt=, we obtain
L(bY1)L(bY0) 2:
Because the subspace U1andV1contain by construction the range and co-range of the initial value, we
have thatbY0=U0S0V0;>[4, Lemma 1]. The truncation is such that kY1 bY1k#. Therefore,
L(Y1)L(bY1) +#
where= max 01krL 
Y1+ (1 )bY1
k. Hence, the stated result is obtained.
14

--- PAGE 15 ---
6.2 Detailed timing measurements
Table 3 displays the average batch training times of a 5-layer, 5120 -neuron dense network on the MNIST
dataset, with a batch size of 500samples. We average the timings over 200batches and additionally
display the standard deviation of the timings corresponding to the layer ranks. The batch timing measures
the full K,L and S steps, including back-propagation and gradient updates, as well as the loss and metric
evaluations.
Table 3: Average batch training times for Ô¨Åxed low-rank training of a 5-layer fully-connected network
with layer widths [5120;5120;5120;5120;10]. Different low-rank factorizations are compared
ranks mean time [s] std. deviation [s]
full-rank 0:3200:005227
[320;320;320;320;320] 0 :8550:006547
[160;160;160;160;10] 0 :3870:005657
[80;80;80;80;10] 0 :1980:004816
[40;40;40;40;10] 0 :1330:005984
[20;20;20;20;10] 0 :0980:005650
[10;10;10;10;10] 0 :0870:005734
[5;5;5;5;10] 0 :0710:005369
Table 4 shows the average test time of a 5-layer, 5120 -neuron dense network, for different low-rank
factorizations and the full rank reference network. The timings are averaged over 1000 evaluations of
the60Ksample MNIST training data set. We measure the Kstep forward evaluation of the low-rank
networks as well as the loss and prediction accuracy evaluations.
Table 4: Average dataset prediction times for Ô¨Åxed low-rank training of a 5-layer fully-connected network
with layer widths [5120;5120;5120;5120;10]. Different low-rank factorizations are compared.
ranks mean time [s] std. deviation [s]
full-rank 1:24760:0471
[2560;2560;2560;2560;10] 1 :42970:0400
[1280;1280;1280;1280;10] 0 :79660:0438
[640;640;640;640;10] 0 :48020:0436
[320;320;320;320;10] 0 :32860:0442
[160;160;160;160;10] 0 :26590:0380
[80;80;80;80;10] 0 :25220:0346
[40;40;40;40;10] 0 :24800:0354
[20;20;20;20;10] 0 :25010:0274
[10;10;10;10;10] 0 :24870:0276
[5;5;5;5;10] 0 :24720:0322
6.3 Detailed training performance of adaptive low-rank networks
Tables 5 and 6 display a detailed overview of the adaptive low-rank results of ¬ß5.1. The displayed ranks
are the ranks of the converged algorithm. The rank evolution of the 5-Layer, 500-Neuron test case can
be seen in Fig. 6. The Evaluation parameter count corresponds to the parameters of the Kstep of the
dynamical low-rank algorithm, since all other matrices are no longer needed in the evaluation phase. The
training parameter count is evaluated as the number of parameters of the Sstep of the adaptive dynamical
low rank training, with maximal basis expansion by 2r, whereris the current rank of the network. We
use the converged ranks of the adaptive low-rank training to compute the training parameters. Note that
during the very Ô¨Årst training epochs, the parameter count is typically higher until the rank reduction has
reached a sufÔ¨Åciently low level.
15

--- PAGE 16 ---
a) Rank evolution for = 0:17
 b) Rank evolution for = 0:15
c) Rank evolution for = 0:13
 d) Rank evolution for = 0:11
e) Rank evolution for = 0:09
 f) Rank evolution for = 0:07
g) Rank evolution for = 0:05
 h) Rank evolution for = 0:03
Figure 6: Rank evolution of the dynamic adaptive low-rank training algorithm for the 5-layer, 500-neuron
dense architecture.16

--- PAGE 17 ---
Table 5: Dynamical low rank training for 5-layer 500-neurons network. c.r. denotes the compression rate
relative to the full rank dense network.
NN metrics Evaluation Train
 test acc. ranks params c.r. params c.r.
full-rank 98:540:03% [500 ;500;500;500;10] 1147000 0% 1147000 0%
0:03 98 :490:02% [176 ;170;171;174;10] 745984 34 :97% 1964540 -71:27%
0:05 98 :560:02% [81 ;104;111;117;10] 441004 61 :56% 1050556 8 :40%
0:07 98 :520:08% [52 ;67;73;72;10] 283768 75 :26% 633360 44 :78%
0:09 98 :340:14% [35 ;53;51;46;10] 199940 82 :57% 429884 62 :52%
0:11 98 :110:46% [27 ;40;37;38;10] 154668 86 :52% 324904 71 :67%
0:13 97 :500:23% [20 ;31;32;30;10] 123680 89 :22% 255500 77 :72%
0:15 97 :220:29% [17 ;25;26;24;10] 101828 91 :13% 207320 81 :92%
0:17 96 :900:45% [13 ;21;24;20;10] 86692 92 :45% 174728 84 :76%
Table 6: Dynamical low rank training for 5-layer 784-neurons network. c.r. denotes the compression rate
relative to the full rank dense network.
NN metrics Evaluation Train
 test acc. ranks params c.r. params c.r.
full-rank 98:530:04% [784 ;784;784;784;10] 2466464 0% 2466464 0%
0:03 98 :610:07% [190 ;190;190;190;10] 1199520 51 :37% 2968800 -20:36%
0:05 98 :590:06% [124 ;120;125;126;10] 784000 68 :22% 1805268 26 :80%
0:07 98 :580:03% [76 ;86;85;83;10] 525280 78 :71% 1151864 53 :29%
0:09 98 :490:05% [56 ;67;63;59;10] 392000 84 :41% 836460 66 :08%
0:11 98 :120:21% [35 ;49;47;43;10] 280672 88 :63% 584240 76 :31%
0:13 97 :950:23% [29 ;35;38;34;10] 221088 91 :04% 453000 81 :63%
0:15 97 :810:17% [22 ;29;27;27;10] 172480 93 :01% 348252 85 :88%
0:17 97 :400:25% [17 ;23;22;23;10] 141120 94 :28% 281724 88 :57%
6.3.1 Lenet5 experiment
In Table 7 we report the results of Ô¨Åve independent runs of the dynamic low-rank training scheme on
Lenet5; we refer to ¬ß5.1 for further details. For each column of the table, we report the mean value
together with its relative standard deviations. No seed has been applied for splitting the dataset and
generating the initial weights conÔ¨Åguration.
Table 7: Mean results and standard relative deviations of the dynamic low-rank training algorithm over
Ô¨Åve independent runs on Lenet5. Adaptive learning rate of 0:05with0:96 exponentially decaying tax.
NN metrics Evaluation Train
 test acc. ranks params c.r. params c.r.
0:11 95:4201:865% [15 ;46;13;10] 47975 88 :9% 50585 88 :2%
0:15 95:5271:297% [13 ;31;9;10] 34435 92 :0% 35746 91 :69%
0:2 95:0091:465% [10 ;20;7;10] 25650 94 :04% 26299 93 :89%
0:3 92:4341:757% [6 ;9;4;10] 15520 96 :39% 15753 96 :34%
6.4 Detailed training performance of low-rank pruning
The proposed low-rank training algorithm does not need to be applied to train a network from random
initial weight guesses. When an already trained network is available, the proposed method can be
employed as a memory-efÔ¨Åcient pruning strategy. A straightforward approach to reduce a trained fully-
connected network to a rank rnetwork is to compute an SVD for all weight matrices and to truncate
those decompositions at rank r. However, while this choice is optimal to present weight matrices, it might
signiÔ¨Åcantly reduce the accuracy of the network. Hence, retraining the determined low-rank subnetwork
is commonly necessary to obtain desirable accuracy properties. Three key aspects are important to obtain
an efÔ¨Åcient pruning method for low-rank methods:
1. Retraining preserves the low-rank structure of the subnetwork.
17

--- PAGE 18 ---
2. Retraining does not exhibit the memory footprint of the fully connected network.
3. Retraining Ô¨Ånds the optimal network among possible low rank networks.
Let us note that the attractor of the proposed dynamical low-rank evolution equations fulÔ¨Ålls these three
requirements. Recall that for the evolution equations we have (3):
minn
k_Wk(t) +rWkL(Wk(t))kF:_Wk(t)2TWk(t)Mrko
: (13)
The condition _Wk(t)2TWk(t)Mrkensures that the weight matrices remain of low-rank. Moreover, as
previously discussed, the training method only requires memory capacities to store low-rank factors. At
the attractor, i.e., when _Wk= 0, the last condition ensures that the attractor minimizes krWkL(Wk(t))kF.
That is, the attractor is the optimal low-rank subnetwork in the sense that it picks the network with minimal
gradient. To underline the effectiveness of our low-rank method as a pruning technique, we take the
fully connected network from Table 6. To demonstrate the poor validation accuracy when simply doing
an SVD on the full 784by784weight matrices and truncating at a given smaller rank, we perform
this experiment for ranks r2f10;20;30;40;50;60;70;80;90;100g. It turns out that though reducing
memory requirements, this strategy leads to unsatisfactory accuracy of about 10%, see the Ô¨Årst column of
Table 8. Then, we use the proposed low-rank training methods with Ô¨Åxed rank rto retrain the network.
As starting points, we use the low-rank networks which have been determined by the truncated SVD.
Retraining then reaches desired accuracies that are comparable to the previously determined low-rank
networks in Table 6.
Table 8: Pruning methods with 784 Neurons per layer
test accuracy Evaluation
SVD low-rank training ranks params c.r.
98:63% 98:63% [784 ;784;784;784;10] 2466464 0%
9:91% 98 :16% [100 ;100;100;100;10] 635040 74 :25%
9:67% 98 :44% [90 ;90;90;90;10] 572320 76 :80%
9:15% 98 :47% [80 ;80;80;80;10] 509600 79 :34%
9:83% 98 :58% [70 ;70;70;70;10] 446880 81 :88%
9:67% 98 :41% [60 ;60;60;60;10] 384160 84 :42%
9:83% 98 :39% [50 ;50;50;50;10] 321440 86 :97%
10:64% 98:24% [40 ;40;40;40;10] 258720 89 :51%
10:3% 98:24% [30 ;30;30;30;10] 196000 92 :05%
9:15% 97 :47% [20 ;20;20;20;10] 133280 94 :60%
10:9% 95:36% [10 ;10;10;10;10] 70560 97 :14%
6.5 Detailed derivation of the gradient
In this section, we derive the computation of the gradients in the K, L and S steps in detail. For this, let us
start with the full gradient, i.e., the gradient of the loss with respect to the weight matrix Wk. We have
@W`
jkL=nMX
iM=1@zM
iML@W`
jkzM
iM=nMX
iM=1@zM
iML@W`
jkM0
@X
iM 1WiMiM 1zM 1
iM 1+bM
iM1
A
=nMX
iM=1@zM
iML0
M0
@X
iM 1WiMiM 1zM 1
iM 1+bM
iM1
A@W`
jk0
@X
iM 1WiMiM 1zM 1
iM 11
A: (14)
For a general , let us deÔ¨Åne
0
;i:=0
0
@X
i 1W
ii 1z 1
i 1+b
i1
A (15)
18

--- PAGE 19 ---
and note that for 6=`
@W`
jk0
@X
i 1W
ii 1z 1
i 11
A=X
i 1W
ii 1@W`
jkz 1
i 1; (16)
whereas for =`we have
@W`
jk0
@n 1X
i 1=1W
ii 1z 1
i 11
A=X
i 1jiki 1z 1
i 1: (17)
Therefore, recursively plugging (15), (16) and (17) into (14) yields
@W`
jkL=nMX
iM=1@zM
iML0
M;iMX
iM 1W
iMiM 1@W`
jkzM 1
iM 1
=nMX
iM=1@zM
iML0
M;iMX
iM 1W
iMiM 10
M 1;iM 1X
iM 2W
iM 1iM 2@W`
jkzM 2
iM 2=
=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`@W`
jk0
@n` 1X
i` 1=1W`
i`i` 1z` 1
i` 11
A (18)
=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`X
i` 1ji`ki` 1z` 1
i` 1
=X
i`+1;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;jji`z` 1
k
Written in matrix notation and making use of the Hadamard product deÔ¨Åned as yAx= (yiAijxj)ij,
forA2Rmn,x2Rnandy2Rm, we have:
@W`L=@zML> 
0
`MY
=`+1W>
0
!>
z>
` 1
Now, let us move to deriving the K, L and S-steps for the dynamical low-rank training. For the K-step, we
represent the weight matrix W`asW`
i`i` 1=P
mK`
i`mV`
i` 1m. Hence, reusing the intermediate result
(18) yields
@K`
jkL=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`@K`
jk0
@n` 1X
i` 1=1X
mK`
i`mV`
i` 1mz` 1
i` 11
A
=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`n` 1X
i` 1=1X
mji`kmV`
i` 1mz` 1
i` 1
=X
i`+1;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`n` 1X
i` 1=1ji`V`
i` 1kz` 1
i` 1
In matrix notation we obtain
@K`L=@zML> 
0
`MY
=`+1W>
0
!>
V>
`z` 1>
=@W`LV`;
19

--- PAGE 20 ---
which is exactly the right-hand side of the K-step. Hence, the K-step can be computed by a forward
evaluation ofLand recording the gradient tape with respect to K`. Similarly, for the L-step, we represent
W`asW`
i`i` 1=P
mU`
i`mL`
i` 1m. Hence,
@L`
jkL=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`@L`
jk0
@n` 1X
i` 1=1X
mU`
i`mL`
i` 1m1
A
=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`n` 1X
i` 1=1X
mU`
i`mji` 1kmz` 1
i` 1
=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`U`
i`mz` 1
j:
In matrix notation, we obtain
@L`L=0
@U>
`@zML> 
0
`MY
=`+1W>
0
!>
z>
` 11
A>
= (@W`L)>U`:
Lastly, for the S-step we write W`
i`i` 1=P
n;mU`
i`mSmnV`
i` 1n. Then,
@S`
jkL=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`@S`
jk X
n;mU`
i`mSmnV`
i` 1n!
=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`n` 1X
i` 1=1X
mU`
i`mjmknV`
i` 1nz` 1
i` 1
=X
i`;;iM@zM
iMLMY
=`+10
;iW
ii 10
`;i`U`
i`jV`
i` 1kz` 1
i` 1:
In matrix notation, we have
@S`L=U>
`@zML> 
0
`MY
=`+1W>
0
!>
V>
`z` 1>
=U>
`@W`LV`:
6.6 Low-rank matrix representation and implementation of convolutional layers
A generalized convolutional Ô¨Ålter is a four-mode tensor W2RFCJKconsisting of FÔ¨Ålters of shape
CJK, which is applied to a batch of NinputC channels image signals Zof spatial dimensions
UVas the linear mapping,
(ZW)(n;f;u;v ) =JX
j=1KX
k=1CX
c=1W(f;c;j;k )Z(n;c;u j;v k): (19)
In order to train the convolutional Ô¨Ålter on the low-rank matrix manifold, we reshape the tensor Winto a
rectangular matrix Wresh2RFCJK. This reshaping is also considered in e.g. [ 28]. An option is, to see
the convolution as the contraction between an three-mode tensor Zunfoldedof patches and the reshaped
kernel matrix Wreshusing Pytorch‚Äôs fold-unfold function. We can construct the unfold by stacking the
vectorized version of sliding patterns of the kernel on the original input, obtaining in this way a tensor
Zunfolded2RNCJKL, whereLdenotes the dimension of Ô¨Çatten version of the output of the 2-D
20

--- PAGE 21 ---
convolution. Thus, equation 19 can be rewritten as a tensor mode product:
(ZW)(n;f;u;v ) =JX
j=1KX
k=1CX
c=1Wresh(f;(c;j;k ))Zunfolded(n;(c;j;k );(u;v))
=rX
p=U(f;p)rX
q=1S(p;q)JX
j=1KX
k=1CX
c=1V((c;j;k );q)Zunfolded(n;(c;j;k );(u;v))(20)
As it is shown in (20), we can a decompose the starting weight Wresh=USV>and then do all the
training procedure as a function of the factors (U;S;V ), without ever reconstructing the kernel. Then we
can apply the considerations of fully connected layers.
Acknowledgements. The work of S. Schotth√∂fer was funded by the Priority Programme SPP2298
‚ÄúTheoretical Foundations of Deep Learning‚Äù by the Deutsche Forschungsgemeinschaft (DFG). The work
of J. Kusch was funded by the Deutsche Forschungsgemeinschaft (DFG) ‚Äì 491976834. The work of
G. Ceruti was supported by the SNSF research project ‚ÄúFast algorithms from low-rank updates‚Äù, grant
number 200020-178806. The work of F. Tudisco and E. Zangrando was funded by the MUR-PNRR
project ‚ÄúLow-parametric machine learning‚Äù. Special thanks to Prof. Martin Frank for the PhD mentorship
of Steffen Schott√∂fer.
References
[1]P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds . Princeton
University Press, 2009.
[2]A. Ashok, N. Rhinehart, F. Beainy, and K. M. Kitani. N2n learning: Network to network compression
via policy gradient reinforcement learning. In International Conference on Learning Representations ,
2018.
[3]D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag. What is the state of neural network
pruning? Proceedings of machine learning and systems , 2:129‚Äì146, 2020.
[4]G. Ceruti, J. Kusch, and C. Lubich. A rank-adaptive robust integrator for dynamical low-rank
approximation. BIT Numerical Mathematics , 2022.
[5]G. Ceruti and C. Lubich. Time integration of symmetric and anti-symmetric low-rank matrices and
Tucker tensors. BIT Numerical Mathematics , 60(3):591‚Äì614, 2020.
[6]G. Ceruti and C. Lubich. An unconventional robust integrator for dynamical low-rank approximation.
BIT. Numerical Mathematics , 62(1):23‚Äì44, 2022.
[7]G. Ceruti, C. Lubich, and D. Sulz. Rank-adaptive time integration of tree tensor networks.
arXiv:2201.10291 , 2022.
[8]G. Ceruti, C. Lubich, and H. Walach. Time integration of tree tensor networks. SIAM Journal on
Numerical Analysis , 59(1):289‚Äì313, 2021.
[9]Y . Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S.-F. Chang. An exploration of
parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE
international conference on computer vision , pages 2857‚Äì2865, 2015.
[10] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y . Bengio. Binarized neural networks:
Training deep neural networks with weights and activations constrained to +1 or -1. Advances in
neural information processing systems , 2016.
[11] L. Deng. The MNIST database of handwritten digit images for machine learning research. IEEE
Signal Processing Magazine , 29(6):141‚Äì142, 2012.
[12] E. L. Denton, W. Zaremba, J. Bruna, Y . LeCun, and R. Fergus. Exploiting linear structure within
convolutional networks for efÔ¨Åcient evaluation. Advances in neural information processing systems ,
27, 2014.
[13] L. Dieci and T. Eirola. On smooth decompositions of matrices. SIAM Journal on Matrix Analysis
and Applications , 20(3):800‚Äì819, 1999.
21

--- PAGE 22 ---
[14] P. A. M. Dirac et al. The principles of quantum mechanics . Number 27. Oxford university press,
1981.
[15] R. Feng, K. Zheng, Y . Huang, D. Zhao, M. Jordan, and Z.-J. Zha. Rank diminishing in deep neural
networks. arXiv:2206.06072 , 2022.
[16] F. Feppon and P. F. Lermusiaux. A geometric approach to dynamical model order reduction. SIAM
Journal on Matrix Analysis and Applications , 39(1):510‚Äì538, 2018.
[17] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
InInternational Conference on Learning Representations , 2018.
[18] J. Frenkel. Wave mechanics, advanced general theory , volume 1. Oxford, 1934.
[19] B. Gao and P.-A. Absil. A riemannian rank-adaptive method for low-rank matrix completion.
Computational Optimization and Applications , 81(1):67‚Äì90, 2022.
[20] Y . Guo, A. Yao, and Y . Chen. Dynamic network surgery for efÔ¨Åcient dnns. Advances in neural
information processing systems , 29, 2016.
[21] E. Hairer, S. P. N√∏rsett, and G. Wanner. Solving ordinary differential equations. I. Nonstiff problems ,
volume 8 of Springer Series in Computational Mathematics . Springer-Verlag, Berlin, second edition,
1993.
[22] Y . He, G. Kang, X. Dong, Y . Fu, and Y . Yang. Soft Ô¨Ålter pruning for accelerating deep convolutional
neural networks, 2018.
[23] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. AMC: AutoML for model compression and
acceleration on mobile devices. In Proceedings of the European conference on computer vision ,
pages 784‚Äì800, 2018.
[24] Y . He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In IEEE
International Conference on Computer Vision , pages 1389‚Äì1397, 2017.
[25] Y . He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks, 2017.
[26] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages
4700‚Äì4708, 2017.
[27] Z. Huang and N. Wang. Data-driven sparse structure selection for deep neural networks. In
Proceedings of the European conference on computer vision (ECCV) , pages 304‚Äì320, 2018.
[28] Y . Idelbayev and M. A. Carreira-Perpi√±√°n. Low-rank compression of neural nets: Learning the rank
of each layer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages
8046‚Äì8056, 2020.
[29] Y . Ioannou, D. Robertson, J. Shotton, R. Cipolla, and A. Criminisi. Training CNNs with low-rank
Ô¨Ålters for efÔ¨Åcient image classiÔ¨Åcation. In International Conference on Learning Representations ,
2016.
[30] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low
rank expansions. In Proceedings of the British Machine Vision Conference. BMVA Press , 2014.
[31] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized
neural layers. In International Conference on Learning Representations , 2021.
[32] E. Kieri, C. Lubich, and H. Walach. Discretized dynamical low-rank approximation in the presence
of small singular values. SIAM Journal on Numerical Analysis , 54(2):1020‚Äì1038, 2016.
[33] H. Kim, M. U. K. Khan, and C.-M. Kyung. EfÔ¨Åcient neural network compression. In Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition , pages 12569‚Äì12577, 2019.
[34] Y .-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Compression of deep convolutional
neural networks for fast and low power mobile applications, 2015.
[35] O. Koch and C. Lubich. Dynamical low-rank approximation. SIAM Journal on Matrix Analysis and
Applications , 29(2):434‚Äì454, 2007.
[36] O. Koch and C. Lubich. Dynamical tensor approximation. SIAM Journal on Matrix Analysis and
Applications , 31(5):2360‚Äì2375, 2010.
22

--- PAGE 23 ---
[37] J. Kusch and P. Stammer. A robust collision source method for rank adaptive dynamical low-rank
approximation in radiation therapy. arXiv:2111.07160 , 2021.
[38] V . Lebedev, Y . Ganin, M. Rakhuba, I. Oseledets, and V . Lempitsky. Speeding-up convolutional
neural networks using Ô¨Åne-tuned cp-decomposition. In International Conference on Learning
Representations , 2015.
[39] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278‚Äì2324, 1998.
[40] C. Li and C. J. R. Shi. Constrained optimization based low-rank approximation of deep neural
networks. In Proceedings of the European Conference on Computer Vision (ECCV) , September
2018.
[41] J. Lin, Y . Rao, J. Lu, and J. Zhou. Runtime neural pruning. In I. Guyon, U. V . Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information
Processing Systems , volume 30. Curran Associates, Inc., 2017.
[42] S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, and D. Doermann. Towards optimal
structured CNN pruning via generative adversarial learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 2790‚Äì2799, 2019.
[43] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu. Hierarchical representations
for efÔ¨Åcient architecture search. In International Conference on Learning Representations , 2018.
[44] C. Lubich and I. V . Oseledets. A projector-splitting integrator for dynamical low-rank approximation.
BIT Numerical Mathematics , 54(1):171‚Äì188, 2014.
[45] C. Lubich, T. Rohwedder, R. Schneider, and B. Vandereycken. Dynamical approximation by
hierarchical Tucker and tensor-train tensors. SIAM Journal on Matrix Analysis and Applications ,
34(2):470‚Äì494, 2013.
[46] C. Lubich, B. Vandereycken, and H. Walach. Time integration of rank-constrained Tucker tensors.
SIAM Journal on Numerical Analysis , 56(3):1273‚Äì1290, 2018.
[47] J.-H. Luo, J. Wu, and W. Lin. Thinet: A Ô¨Ålter level pruning method for deep neural network
compression, 2017.
[48] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. Journal of Machine Learning Research ,
22(165):1‚Äì73, 2021.
[49] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks for
resource efÔ¨Åcient inference. In International Conference on Learning Representations , 2017.
[50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix
factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE
International Conference on Acoustics, Speech and Signal Processing , pages 6655‚Äì6659, 2013.
[51] D. Scieur, V . Roulet, F. Bach, and A. d‚ÄôAspremont. Integration methods and accelerated optimization
algorithms. In Advances In Neural Information Processing Systems , 2017.
[52] P. Singh, V . Kumar Verma, P. Rai, and V . P. Namboodiri. Play and prune: Adaptive Ô¨Ålter pruning
for deep model compression. In Proceedings of the Twenty-Eighth International Joint Conference
on ArtiÔ¨Åcial Intelligence, IJCAI-19 , pages 3460‚Äì3466. International Joint Conferences on ArtiÔ¨Åcial
Intelligence Organization, 7 2019.
[53] S. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural
network Hessian maps. In Advances in Neural Information Processing Systems , volume 34, 2021.
[54] N. S. Sohoni, C. R. Aberger, M. Leszczynski, J. Zhang, and C. R√©. Low-memory neural network
training: A technical report. arXiv:1904.10631 , 2019.
[55] A. Tjandra, S. Sakti, and S. Nakamura. Compressing recurrent neural network with tensor train. In
2017 International Joint Conference on Neural Networks (IJCNN) , pages 4451‚Äì4458. IEEE, 2017.
[56] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM Journal on
Mathematics of Data Science , 1(1):144‚Äì160, 2019.
23

--- PAGE 24 ---
[57] H. Wang, S. Agarwal, and D. Papailiopoulos. PufferÔ¨Åsh: communication-efÔ¨Åcient models at no extra
cost. Proceedings of Machine Learning and Systems , 3:365‚Äì386, 2021.
[58] W. Wen, C. Wu, Y . Wang, Y . Chen, and H. Li. Learning structured sparsity in deep neural networks.
Advances in neural information processing systems , 29, 2016.
[59] W. Wen, C. Xu, C. Wu, Y . Wang, Y . Chen, and H. Li. Coordinating Ô¨Ålters for faster deep neural
networks. In Proceedings of the IEEE International Conference on Computer Vision , pages 658‚Äì666,
2017.
[60] J. Wu, C. Leng, Y . Wang, Q. Hu, and J. Cheng. Quantized convolutional neural networks for mobile
devices. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages
4820‚Äì4828, 2016.
[61] H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. Li, and Y . Chen. Learning low-rank deep
neural networks via singular vector orthogonality regularization and singular value sparsiÔ¨Åcation. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops ,
pages 678‚Äì679, 2020.
[62] J. Ye, X. Lu, Z. Lin, and J. Z. Wang. Rethinking the smaller-norm-less-informative assumption in
channel pruning of convolution layers. In International Conference on Learning Representations ,
2018.
24
