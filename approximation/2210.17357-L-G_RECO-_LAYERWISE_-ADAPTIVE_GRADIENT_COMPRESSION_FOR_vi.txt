# 2210.17357.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/approximation/2210.17357.pdf
# Kích thước tệp: 2373580 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
L-G RECO: NÉN GRADIENT THÍCH ỨNG THEO LỚP CHO
VIỆC HỌC SÂU HIỆU QUẢ VÀ CHÍNH XÁC
Mohammadreza Alimohammadi1 *Ilia Markov1 *Elias Frantar1Dan Alistarh1 2
TÓM TẮT
Huấn luyện phân tán song song dữ liệu của mạng nơ-ron sâu (DNN) đã được áp dụng rộng rãi, nhưng vẫn có thể gặp phải các nghẽn cổ chai giao tiếp. Để giải quyết vấn đề này, toàn bộ các họ cơ chế nén đã được phát triển, bao gồm lượng tử hóa, thưa thớt hóa và xấp xỉ hạng thấp, một số trong đó đang được áp dụng thực tế đáng kể. Mặc dù có tiến bộ này, hầu như tất cả các sơ đồ nén đã biết đều áp dụng nén đồng nhất trên các lớp DNN, mặc dù các lớp không đồng nhất về số lượng tham số và tác động của chúng đến độ chính xác của mô hình. Trong công trình này, chúng tôi cung cấp một khung tổng quát để thích ứng mức độ nén trên các lớp của mô hình một cách động trong quá trình huấn luyện, cải thiện nén tổng thể, đồng thời dẫn đến việc tăng tốc đáng kể mà không hy sinh độ chính xác. Khung của chúng tôi, được gọi là L-GreCo, dựa trên một thuật toán thích ứng, tự động chọn các tham số nén tối ưu cho các lớp mô hình đảm bảo tỷ lệ nén tốt nhất trong khi thỏa mãn ràng buộc lỗi. Các thí nghiệm mở rộng trên các tác vụ phân loại hình ảnh và mô hình hóa ngôn ngữ cho thấy L-GreCo hiệu quả trên tất cả các họ phương pháp nén hiện tại, và đạt được tăng tốc huấn luyện lên đến 2.5× và cải thiện nén lên đến 5× so với các triển khai hiệu quả của các phương pháp hiện tại, trong khi khôi phục độ chính xác đầy đủ. Hơn nữa, L-GreCo bổ sung cho các thuật toán thích ứng hiện tại, cải thiện tỷ lệ nén của chúng 50% và thông lượng thực tế 66%. Một triển khai ẩn danh có sẵn tại https://github.com/LGrCo/L-GreCo .

1 GIỚI THIỆU
Sự tăng trưởng lớn về kích thước mô hình và tập dữ liệu cho học sâu đã làm cho phân tán trở thành phương pháp tiêu chuẩn để huấn luyện. Chiến lược phổ biến nhất là phân tán song song dữ liệu đồng bộ, chia dữ liệu giữa các worker song song, mỗi worker tính toán gradient ngẫu nhiên trên dữ liệu của họ, và sau đó tính trung bình gradient của các worker trong một bước đồng bộ. Quy trình này có một số ưu điểm, nhưng gây ra hai chi phí chính: chi phí đồng bộ hóa của đồng bộ hóa giống như rào cản ở mỗi bước, và chi phí giao tiếp của việc trao đổi gradient theo kiểu tất cả-với-tất cả.

Đã có công việc đáng kể về việc giảm thiểu những chi phí này. Cụ thể, một phương pháp phổ biến để giảm chi phí giao tiếp gradient, đây là trọng tâm chính của bài báo chúng tôi, là nén gradient có tổn thất (Seide et al., 2014; Alistarh et al., 2017; Dryden et al., 2016; Vogels et al., 2019), giảm số bit được giao tiếp mỗi lần lặp. Hàng trăm kỹ thuật như vậy đã được đề xuất, có thể được phân loại thô thành ba họ phương pháp. Phương pháp tổng quát đầu tiên là lượng tử hóa (Seide et al., 2014; Alistarh et al., 2017; Wen et al., 2017), giảm độ rộng bit của gradient được giao tiếp theo cách nhận biết phương sai, để bảo toàn sự hội tụ. Thứ hai là thưa thớt hóa (Strom, 2015; Dryden et al., 2016; Lin et al., 2017), giảm số thành phần gradient được cập nhật ở mỗi bước, được chọn qua các số liệu tầm quan trọng khác nhau. Phương pháp thứ ba và gần đây nhất là xấp xỉ hạng thấp (Wang et al., 2018; Vogels et al., 2019), tận dụng cấu trúc hạng thấp của tensor gradient để giảm thiểu chi phí giao tiếp.

Trong thực tế, những phương pháp này đi kèm với các đánh đổi không tầm thường về nén so với dễ sử dụng. Ví dụ, lượng tử hóa gradient dễ triển khai và triển khai, nhưng chỉ cung cấp nén hạn chế trước khi giảm độ chính xác; thưa thớt hóa và xấp xỉ hạng thấp có thể cung cấp cải thiện nén theo bậc độ lớn, nhưng đi kèm với chi phí bổ sung về duy trì sửa lỗi và điều chỉnh siêu tham số cẩn thận để có kết quả tốt nhất. Các đánh đổi trên đã được cộng đồng điều tra kỹ lưỡng, cùng với các phương pháp nén thích ứng mới (Agarwal et al., 2021b; Markov et al., 2022; Faghri et al., 2020), sử dụng nén "đã học", điều chỉnh nén theo lỗi phát sinh trong các giai đoạn khác nhau của huấn luyện DNN.

Mặc dù có lượng nghiên cứu khổng lồ này, sự tương tác giữa những phương pháp này, triển khai hệ thống của chúng, và động lực huấn luyện cơ bản, đã nhận được ít sự chú ý hơn đáng kể. Cụ thể, hầu như tất cả các công trình hiện tại xem mô hình DNN như một tập tham số đồng nhất, và áp dụng nén hoặc toàn cục cho toàn bộ mô hình, ví dụ bằng cách thực hiện lựa chọn Top-K trên gradient khi thưa thớt hóa (Chen et al., 2018), hoặc đồng nhất, áp dụng cùng mức độ nén cho mỗi lớp, độc lập với kích thước lớp hoặc tác động đến mất mát, tức "độ nhạy cảm" (Vogels et al., 2019). Quan điểm này có thể dẫn đến hai sai lầm chính. Thứ nhất, từ phía ứng dụng, điều này bỏ lỡ các cơ hội tối ưu hóa đáng kể: các mô hình sâu hiện đại, đặc biệt là Transformer (Vaswani et al., 2017) có thể rất không đồng nhất về cả kích thước lớp và độ nhạy cảm lớp đối với nén gradient, và nén gradient có thể có tác động khác nhau trong các giai đoạn huấn luyện (Achille et al., 2018). Thứ hai, từ phía hệ thống, hầu hết các khung huấn luyện hiệu quả chồng chéo giao tiếp và tính toán, truyền gradient lớp ngay khi chúng được tạo ra. Do đó, việc có được một cái nhìn toàn cục nhất quán về tham số, cần thiết để thực hiện lựa chọn Top-K toàn cục là không thể, hoặc rất tốn kém để triển khai và áp dụng.

Những ví dụ cụ thể này chỉ ra một khoảng cách đáng kể giữa các kỹ thuật nén thuật toán, cần thiết cho huấn luyện song song dữ liệu hiệu quả, và các triển khai thực tế của chúng. Cụ thể hơn, việc đặt câu hỏi là tự nhiên: cho một mô hình tùy ý và một kỹ thuật nén, có cách hiệu quả nào để cân bằng các ràng buộc ứng dụng, tức độ nhạy cảm lớp, một mặt, và các ràng buộc giao tiếp hệ thống, tức kích thước lớp, mặt khác, một cách động trong quá trình huấn luyện, để tối đa hóa tăng tốc mà không hy sinh độ chính xác cuối cùng của mô hình?

Để giải quyết những câu hỏi này, chúng tôi giới thiệu L-GreCo, một khung hiệu quả và tổng quát cho tham số hóa theo lớp của các sơ đồ nén GRadiEnt. Về mặt thuật toán, L-GreCo dựa trên một công thức hóa mới của bài toán nén thích ứng theo lớp, xác định các tham số nén theo lớp, ví dụ cài đặt thưa thớt theo lớp hoặc mức lượng tử hóa, tối đa hóa lượng nén, dưới ràng buộc cố định về tổng lỗi do nén gradient, được đặt để không có mất mát độ chính xác. Ở cấp độ hệ thống, L-GreCo hoạt động bằng cách tích hợp với các khung huấn luyện tiêu chuẩn, như torch.distributed, để khai thác tính không đồng nhất của mô hình về cả cấu trúc theo lớp và độ nhạy cảm theo lớp, xác định tức thì mức độ nén gradient lớp riêng lẻ để tối đa hóa nén hoặc thời gian huấn luyện đầu cuối.

Chúng tôi xác thực L-GreCo trên tất cả các họ chiến lược nén hiện tại: lượng tử hóa, thưa thớt hóa và nén hạng thấp, và trên nhiều tác vụ thị giác và ngôn ngữ. Các thí nghiệm của chúng tôi cho thấy L-GreCo đạt được tỷ lệ nén cao hơn một cách nhất quán so với các chiến lược thủ công hoặc thích ứng hiện tại, và đặc biệt hiệu quả cho các mô hình dựa trên Transformer không đồng nhất và nhạy cảm. Cụ thể, khung cung cấp lợi ích cho tất cả các chiến lược nén hiện tại theo kiểu hộp đen, và dẫn đến lợi ích nén và tăng tốc thực tế đáng kể, so với các sơ đồ tham số hóa thủ công được biết đến tốt nhất, khi thực hiện các điểm chuẩn phân loại hình ảnh tiêu chuẩn hoặc mô hình hóa ngôn ngữ trong cài đặt đơn và đa nút.

Chúng tôi tóm tắt các đóng góp của mình như sau:
• Chúng tôi cho thấy rằng các sơ đồ nén gradient có thể tận dụng cấu trúc theo lớp không đồng nhất của DNN để giảm chi phí giao tiếp.
• Chúng tôi thiết kế và triển khai khung L-GreCo, đảm bảo mức nén theo lớp tối ưu về đánh đổi nén-độ chính xác, dựa trên một thước đo độ nhạy cảm nén lớp được biện minh lý thuyết.
• Chúng tôi cung cấp một đánh giá thực nghiệm mở rộng trên các mạng nơ-ron khác nhau (ResNet18, ResNet50, Transformer-XL, Transformer-LM) với các tập dữ liệu khác nhau (CIFAR-100, ImageNet, Wikitext-103) cho thấy L-GreCo giảm giao tiếp lên đến 5× và đạt tăng tốc lên đến 2.5× mà không mất độ chính xác hoặc điều chỉnh đáng kể, trên các triển khai đơn và đa nút.
• Chúng tôi tiến hành nghiên cứu chi tiết đầu tiên cho cả số liệu độ nhạy cảm lớp và số liệu hiệu suất. Chúng tôi phát hiện rằng số liệu độ nhạy cảm dựa trên lỗi lượng tử hóa tương đương với số liệu dựa trên mất mát đầu ra. Đồng thời, từ góc độ hiệu suất, số liệu tìm cách tối đa hóa tỷ lệ nén dẫn đến kết quả tương tự như những số liệu dựa trên thời gian.
• Cuối cùng, chúng tôi cho thấy L-GreCo tương thích với các sơ đồ nén thích ứng khác: cụ thể, nó có thể được mở rộng để sử dụng thông tin về các giai đoạn huấn luyện khác nhau (Agarwal et al., 2021b), dẫn đến cải thiện hiệu suất thêm.

2 CÔNG VIỆC LIÊN QUAN
Phương pháp nén. Nén gradient thường sử dụng ba chiến lược: lượng tử hóa, thưa thớt hóa và phân tách hạng thấp. Phương pháp lượng tử hóa (Seide et al., 2014; Alistarh et al., 2017; Wen et al., 2017; Lim et al., 2018; Ramezani-Kebrya et al., 2021) sử dụng độ chính xác thấp hơn của mỗi thành phần gradient, giảm số bit được truyền. Chúng dễ triển khai và hoạt động dưới siêu tham số ổn định (Alistarh et al., 2017; Xu et al., 2021; Markov et al., 2022); tuy nhiên, nén của chúng bị hạn chế bởi thực tế là thường phải truyền ít nhất một bit mỗi mục. Kỹ thuật thưa thớt hóa (Strom, 2015; Dryden et al., 2016; Lin et al., 2017; Karimireddy et al., 2019) vượt qua điều này bằng cách xác định các thành phần quan trọng trong gradient và chỉ truyền những tập con như vậy. Cuối cùng, thuật toán

--- TRANG 2 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

Lớp8163264HạngĐồng nhất-32
perplexity 24.05L-GreCo-32-[8:64]
perplexity 24.08

1031041051061071081091010
Kích thước lớp

Hình 1. Hồ sơ lựa chọn hạng L-GreCo cho nén PowerSGD trên Transformer-XL. Đường màu đỏ biểu thị nén đồng nhất, trong khi đường màu xanh biểu thị hồ sơ L-GreCo. Các thanh trong suốt hiển thị kích thước lớp. Các lớp được lập chỉ mục theo thứ tự chúng được giao tiếp. Số được chú thích là perplexity thử nghiệm cuối cùng (ppl) cho thí nghiệm (thấp hơn là tốt hơn). Ở đây, nén trung bình của L-GreCo cao hơn 1.5x so với đồng nhất.

phân tách gradient (Vogels et al., 2019; Wang et al., 2018) sử dụng thực tế rằng tensor gradient theo lớp được biết là có thể xấp xỉ tốt qua ma trận hạng thấp, và nhằm thiết kế các phương pháp chiếu nhẹ cũng cung cấp lỗi thấp. Kỹ thuật thưa thớt hóa và hạng thấp thường yêu cầu bộ đệm sửa lỗi để bảo toàn sự hội tụ tốt, cũng như điều chỉnh siêu tham số không tầm thường.

Như chúng tôi cho thấy thực nghiệm, L-GreCo tương thích với tất cả những phương pháp này và có thể cung cấp tiết kiệm băng thông bổ sung đáng kể cho mỗi chiến lược như vậy, mà không hy sinh độ chính xác mô hình và không cần điều chỉnh.

Nén Thích Ứng. Ý tưởng tổng quát về việc thích ứng mức độ nén trong quá trình huấn luyện đã được điều tra bởi AdaComp (Chen et al., 2018), đề xuất một phương pháp nén thích ứng tự điều chỉnh; tuy nhiên, phương pháp của họ không thích ứng tham số nén theo lớp, và không thể kết hợp với các phương pháp nén khác. Faghri et al. (2020) thích ứng lưới lượng tử hóa với phân bố gradient; tuy nhiên, phương pháp của họ được điều chỉnh cụ thể cho lượng tử hóa, và không nhận biết tính không đồng nhất lớp.

Sahu et al. (2021) tối ưu hóa tổng lỗi trên các bước cho nén dựa trên thưa thớt hóa và đề xuất bộ thưa thớt toàn cục ngưỡng, được cho thấy đạt tỷ lệ nén cao hơn so với nén đồng nhất theo lớp trên các tác vụ thị giác nhỏ (ví dụ ResNet18 trên CIFAR10/100). Tuy nhiên, phương pháp của họ bị hạn chế với thưa thớt hóa và để không rõ cách điều chỉnh ngưỡng cho các mô hình quy mô lớn, nhạy cảm như Transformer hoặc mô hình quy mô ImageNet. Đặc biệt, chúng tôi không thể có được kết quả tốt với phương pháp này trên các mô hình như Transformer-XL hoặc Transformer-LM. Trong phần 5.3 chúng tôi trình bày so sánh với phương pháp của họ trên ResNet18/CIFAR-100, cho thấy phương pháp của chúng tôi mang lại cả độ chính xác cao hơn và nén cao hơn.

Accordion (Agarwal et al., 2021a) thích ứng các tham số nén của thưa thớt hóa và nén hạng thấp dựa trên các chế độ quan trọng của huấn luyện. Thuật toán xen kẽ giữa hai mức nén ("thấp" và "cao"), được cung cấp bởi người dùng và dễ bị mất độ chính xác. Phương pháp của chúng tôi cải thiện so với Accordion về tăng tốc, nhưng cũng cho thấy chúng tôi có thể kết hợp phương pháp của mình với Accordion và có được lợi ích cao hơn nữa. Một biến thể của bài toán nén theo lớp tương tự như bài toán chúng tôi xem xét ở đây đã được đề xuất bởi CGX (Markov et al., 2022). Tuy nhiên, họ điều tra một heuristic dựa trên kmeans, mà chúng tôi cho thấy thực nghiệm là dưới tối ưu.

Theo hiểu biết của chúng tôi, chiến lược lập trình động của chúng tôi chưa được sử dụng trong bối cảnh nén gradient thích ứng. Các phương pháp liên quan đã được điều tra trong bối cảnh nén trọng số cho DNN, xem ví dụ (Wu et al., 2020; Aflalo et al., 2020; Frantar & Alistarh, 2022; Shen et al., 2022). Tuy nhiên, có những khác biệt kỹ thuật đáng kể: thứ nhất, số liệu lỗi và mục tiêu tăng tốc khác nhau trong trường hợp nén trọng số; thứ hai, chúng tôi thực hiện trực tuyến, tại thời điểm huấn luyện, có nghĩa là thuật toán của chúng tôi phải cực kỳ hiệu quả, và thích ứng với đầu vào động.

3 THIẾT LẬP BÀI TOÁN
Mục tiêu. Giả sử chúng ta được cho một mô hình DNN M với L lớp và một kỹ thuật nén, chúng ta sẽ muốn tìm một lựa chọn tham số nén cℓ theo trực quan, một cho mỗi lớp ℓ ∈ {1,2, . . . , L} sẽ giảm thiểu một số liệu đại diện cho thiệt hại của chất lượng huấn luyện được giới thiệu bởi nén trong khi giảm thiểu tổng số bit được truyền. Tuy nhiên, mô tả trực quan này để mở một loạt chi tiết, như 1) khái niệm số liệu theo lớp tương ứng với hiệu ứng nén cho một tập tham số nhất định; 2) công thức bài toán chính xác, ràng buộc ảnh hưởng nén hoặc tỷ lệ nén; và 3) một triển khai hiệu quả của thuật toán như vậy. Chúng tôi giải quyết những câu hỏi này tiếp theo.

Số liệu. Việc chọn số liệu độ nhạy cảm đúng là chìa khóa cho việc khôi phục độ chính xác. Cho điều này, chúng tôi đã thử một số phương pháp. Đầu tiên, chúng tôi lưu ý rằng độ nhạy cảm của một lớp đối với nén gradient có thể được đo bằng phản ứng mất mát mô hình đối với nén. Để đánh giá số liệu, chúng tôi thiết lập thí nghiệm sau. Chúng tôi lưu các checkpoint mô hình ở các giai đoạn khác nhau của huấn luyện không nén. Sau đó tiến hành nhiều lần chạy huấn luyện ngắn (lên đến 50 bước) với cùng dữ liệu bắt đầu từ checkpoint nhưng thay đổi tham số nén gradient. Sau đó, chúng tôi sử dụng sự khác biệt của mất mát có và không có nén làm số liệu.

Bây giờ, câu hỏi là chúng ta muốn sử dụng tham số nén nào (vector có kích thước bằng số lớp được nén) để đánh giá số liệu của mỗi lớp. Vì chúng ta muốn thấy phản ứng mô hình đối với nén lớp riêng lẻ, đối với mỗi lớp chúng tôi thay đổi tham số nén để lại gradient của tất cả các lớp khác không bị chạm đến. Với điều đó, chúng tôi thu thập sự khác biệt của mất mát cho mỗi lớp và tham số nén và sử dụng chúng làm số liệu. Phương pháp khác dựa trên các công trình lý thuyết trước đó, đề xuất rằng lỗi ℓ2 bình phương của nén là một thước đo tốt về tác động hội tụ của kỹ thuật nén. Ở đây, chúng tôi tổng hợp gradient trong quá trình huấn luyện, sau đó nén gradient được tổng hợp cho mỗi tham số nén cho mỗi lớp riêng lẻ và sử dụng độ lớn của lỗi làm số liệu.

Như được hiển thị trong Phần 5.4, hai phương pháp có tương quan mạnh và các tham số tối ưu kết quả gần với nhau. Tuy nhiên, phương pháp dựa trên mất mát không áp dụng được trong thế giới thực vì nó yêu cầu đánh giá ngoài huấn luyện làm thay đổi quy trình huấn luyện gốc và mất thời gian tương đương với thời gian huấn luyện gốc. Đồng thời, phương pháp dựa trên lỗi có thể được sử dụng trong quá trình huấn luyện dễ dàng tích hợp vào huấn luyện và có chi phí thời gian không đáng kể. Tính đến điều này, trong suốt bài báo chúng tôi sử dụng chuẩn L2 của lỗi nén làm số liệu độ nhạy cảm lớp chính.

Bài Toán Tối Ưu Có Ràng Buộc. Chúng tôi chính thức hóa bài toán tối ưu của mình như sau. Cho một mô hình M với L lớp ℓ ∈ {1,2, . . . , L} và một kỹ thuật nén, cung cấp một tập lựa chọn nén C={c1, c2, . . . ck} cho mỗi lớp. Chúng tôi nhấn mạnh rằng, để đơn giản, chúng tôi xem xét một kỹ thuật nén đơn và cùng lựa chọn/mức nén cho mỗi lớp, nhưng phương pháp của chúng tôi cũng sẽ hoạt động cho các kỹ thuật khác nhau được áp dụng cho cùng mô hình, và lựa chọn nén không đồng nhất.

Trong bối cảnh này, phương pháp của chúng tôi nhận đầu vào là một hàm lỗi error(ℓ, cj) cung cấp chuẩn L2 của lỗi nén tại lớp ℓ cho lựa chọn nén cj, và một hàm size(ℓ, cℓ) đo chi phí truyền của lớp ℓ cho lựa chọn cℓ. Ngoài ra, chúng tôi giả sử được cho một ngưỡng lỗi tối đa cố định Emax mà thuật toán không được vi phạm. Sau đó, chúng tôi muốn tìm một cài đặt theo lớp của tham số nén c1, . . . , cL với mục tiêu
minimize∑L ℓ=1 size(ℓ, cℓ) s.t. ∑L ℓ=1 error(ℓ, cℓ) ≤ Emax.

Về mặt thực tế, công thức này nhằm giảm thiểu tổng chi phí truyền cho tensor gradient dưới ràng buộc cộng dồn tối đa về lỗi nén gradient. Một giả định ngầm là số liệu error(·,·) có tính cộng dồn trên các lớp, và có thể có được giới hạn lỗi "tham chiếu" không dẫn đến mất độ chính xác. Như chúng ta sẽ thấy tiếp theo, đây là trường hợp cho số liệu lỗi chúng tôi áp dụng.

Giới Hạn Lỗi. Câu hỏi còn lại là cách chọn Emax. Chúng tôi quyết định chọn giới hạn lỗi này để theo dõi giới hạn của một phương pháp nén tham chiếu được biết là không mất độ chính xác so với đường cơ sở. Ở đây, chúng tôi tận dụng thực tế rằng tài liệu cung cấp các tham số cho phép đạt được khôi phục độ chính xác đầy đủ cho các mô hình và tập dữ liệu khác nhau. Ví dụ, đối với lượng tử hóa, chúng ta có thể sử dụng lượng tử hóa 4-bit, được biết là khôi phục cho hầu như mọi mô hình (Alistarh et al., 2017; Markov et al., 2022). Đối với thưa thớt hóa (Lin et al., 2017; Renggli et al., 2019) cũng như phân tách ma trận (Vogels et al., 2019) chúng tôi phải sử dụng các tham số tham chiếu khác nhau cho huấn luyện khác nhau theo đường cơ sở của chúng (Để biết chi tiết tham khảo Bảng 2 và 3).

4 KHUNG L-G RECO
Tổng quan. Bây giờ chúng tôi mô tả một thuật toán tổng quát để giải quyết bài toán tối ưu có ràng buộc từ phần trước. Thuật toán của chúng tôi đưa ra quyết định theo lớp để cân bằng độ lớn của lỗi nén và kích thước nén của mô hình. Làm đầu vào, thuật toán của chúng tôi nhận kích thước lớp không nén size(ℓ,⊥), một tập G gradient tích lũy theo lớp (sẽ được sử dụng để kiểm tra lỗi nén), cũng như giới hạn lỗi cố định Emax. Cụ thể, tại một bước quyết định nhất định, mục tiêu là tìm ánh xạ tối ưu của mỗi lớp ℓ tới một mức nén

--- TRANG 3 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

cℓ, sao cho chuẩn của tổng lỗi nén, được tính trên tập gradient tích lũy G không vượt quá Emax, nhưng tổng kích thước nén của mô hình ∑L ℓ=1 size(ℓ, cℓ) là tối thiểu cho giới hạn lỗi này.

Công thức này gợi nhớ đến bài toán túi xách: lỗi là kích thước của túi xách, và kích thước nén là giá trị chúng ta muốn tối ưu. Trong công thức này, bài toán sẽ có một thuật toán hiệu quả và tối ưu, sử dụng lập trình động (DP). Tuy nhiên, lỗi L2 bình phương không rời rạc, nên chúng ta không thể áp dụng trực tiếp phương pháp này. Thay vào đó, ý tưởng là giảm thiểu điều này thành một bài toán có thể giải được bằng cách rời rạc hóa tập giá trị lỗi có thể.

Cụ thể, vì lỗi có tính đơn điệu và chúng ta có thể sử dụng rời rạc hóa rất mịn mà không mất hiệu quả đáng kể, rất khó có khả năng chúng ta sẽ bỏ lỡ giải pháp tối ưu một lượng đáng kể. Để minh họa, trong triển khai của chúng tôi, chúng tôi sử dụng D = 10000 làm hệ số rời rạc hóa (tức các bước có kích thước Emax/D).

Thuật Toán. Thuật toán, được trình bày đầy đủ trong Thuật toán 1, hoạt động như sau. Đầu tiên, chúng tôi tính toán dữ liệu cần thiết cho thuật toán cho tất cả các lớp và tất cả các tham số nén được xem xét (dòng 1-10), tương ứng với lỗi và nén cho mỗi lựa chọn có thể. Sau đó chúng tôi thực hiện một thuật toán lập trình động giải quyết bài toán sau. Chúng ta muốn tính toán kích thước tổng tối thiểu cho tổng lỗi nén E trong ℓ lớp đầu tiên compressedsize(ℓ, E) = minℓ compressedsize(ℓ−1, E− error(ℓ, cℓ)) + size(ℓ, cℓ). Để đạt được điều này, đối với mỗi lớp chúng ta muốn xem xét, chúng tôi chạy qua tất cả các gia số lỗi và tất cả các tham số nén có thể, và giảm thiểu tổng kích thước nén cho tổng lỗi nén hiện tại (dòng 12-22) lưu tham số nén mà chúng ta đạt được mức tối thiểu. Sau đó, trong dòng 23-27, chúng tôi tìm gia số lỗi mà chúng ta đạt được kích thước nén tổng thấp nhất và tái tạo ánh xạ tham số nén—do đó, chúng ta đã có được kết quả.

Triển khai. Như được hiển thị trong Hình 2, chúng tôi tích hợp L-GreCo ở giữa mã huấn luyện người dùng và hệ thống giao tiếp chịu trách nhiệm nén gradient và đồng bộ hóa của chúng. Chúng tôi chạy thuật toán trên định kỳ, ví dụ một lần mỗi epoch huấn luyện, trên một worker được chỉ định duy nhất; trừ khi được nêu khác, worker này thực hiện tất cả các bước. Giữa các lần chạy thuật toán, chúng tôi tích lũy gradient theo lớp trong các bộ đệm phụ trợ. Sau đó chúng tôi xây dựng một bảng lỗi L2, cho mỗi lớp, và cho mọi tham số nén trong phạm vi do người dùng cung cấp và cho tập tham số nén tham chiếu. Để tìm lỗi, chúng tôi mô phỏng nén/giải nén của mỗi lớp với tham số nén đã cho mà không áp dụng phản hồi lỗi và tính toán khoảng cách L2 giữa vector gốc và phục hồi. Sau đó chúng tôi chạy thuật toán DP. Điều này cung cấp cho chúng tôi ánh xạ nén tối ưu,

Thuật toán 1 Nén thích ứng L-GreCo
Đầu vào: Lớp mô hình Li, gradient tích lũy Gi, tham số nén có thể C={c1, c2, . . . ck}, tham số nén mặc định tĩnh chúng tôi cố gắng cải thiện Cd i, hệ số rời rạc hóa D
Đầu ra: Gán tham số nén cℓ∈C cho mỗi lớp ℓ

1: N = số lớp
2: Tính Emax cho tham số nén mặc định Cd i
3: Tính bước rời rạc hóa Emax/D.
4: Ma trận Chi phí N× |C| nơi vị trí i, j có giá trị kích thước của lớp i được nén với tham số nén cj.
5: Ma trận Lỗi N× |C| nơi vị trí i, j có giá trị L2 rời rạc hóa của lỗi nén khi gradient tích lũy của lớp i được nén với tham số cj.
6: Ma trận DP N×(D+ 1) được điền với giá trị ∞.
7: Ma trận PD N×(D+ 1).
8: // Khởi tạo bảng chi phí:
9: for c∈C do
10: DP[1][Errors[1][c]] = Costs[1][c]
11: PD[1][Errors[1][c]] = c
12: end for
13: // Thuật toán lập trình động
14: for Lớp li:= 2..N do
15: for ci∈C do
16: for ei:=Errors[li][ci]..D do
17: t=DP[li−1][ei−Errors[li][ci]] + Costs[li][ci]
18: if t < DP[li][ei] then
19: DP[li][ei] = t
20: PD[li][ei] = ci
21: end if
22: end for
23: end for
24: end for
25: errmin=argmin(DP[N])
26: // Tái tạo tham số tối ưu
27: for li=N..1 do
28: result[li] = PD[li][errmin]
29: errmin=errmin−Errors[li][result[li]]
30: end for
31: return result

mà worker được chỉ định phát sóng tới các worker khác. Sau đó, trên mỗi worker, chúng tôi lưu ánh xạ tham số nén trong động cơ giao tiếp.

Chi phí tính toán và bộ nhớ. Thuật toán giả định rằng chúng ta tích lũy gradient trong các bộ đệm bổ sung, chiếm bộ nhớ kích thước mô hình. Thuật toán DP có độ phức tạp thời gian O(D|L||C|) và độ phức tạp bộ nhớ O(|L|D). Thời gian thực tế cho thuật toán được trình bày trong Bảng 1. Chi phí bao gồm hai phần: 1. Tính toán lỗi 2. Thuật toán lập trình động. Chúng ta có thể thấy rằng phần lập trình động chỉ mất một phần phần trăm thời gian huấn luyện trong khi hầu hết chi phí được gây ra bởi tính toán. Tuy nhiên, cả hai chi phí đều không đáng kể so với tăng tốc được cung cấp bởi L-GreCo (xem Hình 4 và 5).

Chi tiết giao tiếp. Từ quan điểm triển khai song song dữ liệu chung, gradient trở nên khả dụng ngay sau khi kết thúc lan truyền ngược của lớp tương ứng. Để giảm độ trễ giao tiếp, gradient được nhóm thành một số bộ đệm (được gọi là bucket trong pytorch). Một tối ưu hóa quan trọng là chồng chéo giao tiếp gradient với tính toán. Điều này có nghĩa là chi phí giao tiếp của các bucket đầu tiên có khả năng được "ẩn" hoàn toàn bởi tính toán, trong khi đồng bộ hóa bucket cuối cùng trở thành một phần đáng kể của độ trễ thời gian giữa các bước huấn luyện (xem Hình 10(b)). Điều này có nghĩa là tốc độ giao tiếp của các phần khác nhau của mô hình (tức các nhóm lớp) có tác động khác nhau đến tốc độ huấn luyện. Do đó, tối ưu hóa tỷ lệ nén có thể đưa ra kết quả dưới tối ưu.

Tối ưu hóa thời gian. Với điều đó trong tâm trí, chúng tôi đã tích hợp một tiện ích đo thời gian mỗi bucket mất để đồng bộ hóa. Với công cụ này, chúng ta có thể tái công thức hóa bài toán tối ưu: thực tế, chúng ta muốn giảm thiểu phạm vi thời gian giữa bắt đầu đồng bộ hóa bucket đầu tiên và kết thúc đồng bộ hóa bucket cuối cùng (tức thời gian đồng bộ hóa gradient) thay vì tỷ lệ nén. Cho điều đó, chúng tôi đo thời gian đồng bộ hóa gradient cho các kết hợp khác nhau của tỷ lệ nén mỗi bucket, lưu kích thước bucket được giao tiếp. Sau đó, chúng tôi huấn luyện một mô hình hồi quy tuyến tính để học mối quan hệ giữa kích thước bucket được truyền và thời gian đồng bộ hóa gradient. Từ đó chúng ta có được các hệ số theo bucket T(b) mà chúng ta có thể áp dụng cho mỗi lớp trong bucket tương ứng. Sau đó chúng tôi thay đổi mục tiêu trong bài toán tối ưu (xem Công thức 3) thành:
minimize ∑L ℓ=1 size(ℓ, cℓ) ∗ T(ℓ) s.t. ∑L ℓ=1 error(ℓ, cℓ) ≤ Emax.

Điều này cho chúng ta các tham số có đặc tính nén tương tự hoặc tệ hơn so với thuật toán dựa trên tỷ lệ nhưng cải thiện thời gian giao tiếp ước tính.

5 KIỂM CHỨNG THỰC NGHIỆM
Chúng tôi đánh giá thực nghiệm L-GreCo trên tất cả các chiến lược nén hiện tại: lượng tử hóa sử dụng QSGD, thưa thớt hóa TopK và xấp xỉ hạng thấp qua PowerSGD.

5.1 Thiết lập thực nghiệm
Cơ sở hạ tầng. Đánh giá của chúng tôi sử dụng các máy trạm thông thường với 4 hoặc 8 GPU NVIDIA RTX3090. Trong cài đặt đa nút, chúng tôi sử dụng 4 phiên bản đám mây với GPU 4xRTX3090, được cung cấp bởi Genesis Cloud. Đo băng thông cho thấy giá trị băng thông giữa GPU nằm trong khoảng 13 đến 16 Gbps, và băng thông giữa nút trong đám mây lên tới 10 Gbps. Chúng tôi sử dụng Pytorch 1.10, openmpi/4.1.4, CUDA 11.3, NCCL 2.8.4, và cudnn/8.1.1.

Triển khai. Chúng tôi triển khai L-GreCo trong PyTorch sử dụng hook torch.distributed cho PowerSGD và tận dụng khung CGX mã nguồn mở (Markov et al., 2022) cho lượng tử hóa và thưa thớt hóa.

Tập dữ liệu và mô hình. Chúng tôi kiểm tra hai tác vụ học DNN khác nhau: 1) phân loại hình ảnh trên tập dữ liệu CIFAR100 (Krizhevsky, 2009) và ImageNet (Deng et al., 2009), và 2) mô hình hóa ngôn ngữ trên WikiText-103 (Merity et al., 2016). Chúng tôi sử dụng các triển khai mô hình và tham số tiên tiến được cung cấp bởi phiên bản PyTorch của điểm chuẩn NVIDIA Training Examples (Nvidia, 2020) và thư viện fairseq của các ví dụ PyTorch (Ott et al., 2019). Chúng tôi sử dụng ResNet-18 cho huấn luyện CIFAR-100 với kích thước lô 256, ResNet-50 trong chế độ độ chính xác hỗn hợp cho ImageNet với kích thước lô 2048, và Transformer-XL và Transformer-LM được huấn luyện với độ chính xác đầy đủ cho WikiText-103, với kích thước lô 256 và 2048, tương ứng. Tất cả các thí nghiệm của chúng tôi sử dụng công thức huấn luyện gốc, không có bất kỳ điều chỉnh siêu tham số nào để tính đến huấn luyện nén gradient.

Đường cơ sở. Đường cơ sở tự nhiên đầu tiên là huấn luyện không nén, thiết lập đường cơ sở độ chính xác của chúng tôi. Phù hợp với MLPerf (Mattson et al., 2020), chúng tôi đặt ngưỡng độ chính xác của mình là 1% so với huấn luyện không nén. Đường cơ sở tự nhiên thứ hai là các công thức nén gradient được tạo thủ công tốt nhất hiện tại. Nhìn chung, các phương pháp hiện tại đề xuất nén đồng nhất theo lớp đến một ngưỡng nhất định, ví dụ (Alistarh et al., 2017; Wen et al., 2017; Renggli et al., 2019; Vogels et al., 2019). Đối với những đường cơ sở như vậy, chúng tôi muốn cải thiện kích thước nén và tốc độ huấn luyện, có thể cũng cải thiện độ chính xác mô hình cuối cùng. Chúng tôi phát hiện rằng lựa chọn tốt nhất của tham số nén cho gán đồng nhất theo lớp phụ thuộc vào phương pháp nén, tập dữ liệu và tác vụ. Đối với một số thí nghiệm, chúng tôi phải điều chỉnh tham số nén đồng nhất để phù hợp với kết quả đường cơ sở (không nén) (xem Bảng 2 và 3).

Phạm vi tham số. L-GreCo yêu cầu một phạm vi tham số nén có thể làm đầu vào. Chúng tôi luôn chọn phạm vi này để bao gồm tham số nén mặc định được sử dụng trong tài liệu. Hơn nữa, chúng tôi để lại khoảng cách giữa tham số mặc định và tham số nén cao nhất có thể

--- TRANG 4 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

Bảng 1. Chi phí thời gian cho L-GreCo liên quan đến tổng thời gian huấn luyện. Số trong ngoặc đại diện cho tính toán lỗi.

Phương pháp
Nén | Transformer-XL | ResNet50
PowerSGD | 0.56%[0.49%] | 0.15%[0.14%]
QSGD | 0.14%[0.13%] | 0.04%[0.03%]
TopK | 0.38%[0.35%] | 0.33%[0.30%]

(giới hạn phạm vi bên phải) — nếu không, chúng tôi bị hạn chế khớp lỗi L2; và khoảng cách giữa tham số mặc định và tham số nén thấp nhất có thể (giới hạn phạm vi bên trái)—nếu không, chúng tôi sẽ không cải thiện nén.

Phạm vi phụ thuộc vào phương pháp nén. Lượng tử hóa và phương pháp hạng thấp có số lượng tham số rời rạc hạn chế (số bit mỗi giá trị và hạng phân tách tương ứng) trong khi phạm vi cho thưa thớt hóa lớn hơn. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng phương pháp đơn giản sau. Giả sử một tham số nén đồng nhất mặc định D, ví dụ 4 bit lượng tử hóa. Đối với lượng tử hóa và phương pháp hạng thấp, không gian tìm kiếm được định nghĩa là [D/2,2∗D] với bước tăng 1. Đối với thưa thớt hóa, chúng tôi chọn [D/10,10∗D] với gia số D/10.

Hai tham số đầu vào khác của L-GreCo là tần suất chạy thuật toán và thời gian khởi động sau đó nén được bật. Tham số đầu tiên thường được chọn bằng thời kỳ đánh giá từ công thức huấn luyện (thường là 1 epoch). Như chúng ta sẽ thấy (Hình 9a) tỷ lệ nén của các sơ đồ được trả về bởi L-GreCo tương đối ổn định, nên không cần điều chỉnh lại thường xuyên. Trong các thí nghiệm của chúng tôi, thời gian khởi động bằng thời gian khởi động tốc độ học mặc định.

5.2 Kết quả đánh giá
Khôi phục độ chính xác. Đầu tiên chúng tôi kiểm tra độ chính xác mô hình sử dụng công thức tiêu chuẩn cho huấn luyện đầu cuối. Đối với mỗi thí nghiệm, chúng tôi thực hiện 3 lần chạy với seed khác nhau. Chúng tôi so sánh khôi phục độ chính xác L-GreCo với không nén (đường cơ sở) và tham số nén đồng nhất theo lớp tốt nhất (đồng nhất). Kết quả được trình bày trong Bảng 2 và 3. Tất cả độ chính xác và perplexity được trình bày với biến động seed. Tỷ lệ nén đại diện cho tiết kiệm chi phí truyền thực tế so với đường cơ sở không nén.

Đối với mỗi phương pháp nén và tác vụ huấn luyện, chúng tôi lấy tham số mặc định cung cấp tỷ lệ nén cao nhất trong khi khôi phục độ chính xác mô hình cuối cùng, tức cải thiện nén thêm với cài đặt đồng nhất chỉ dẫn đến hội tụ tồi tệ hơn.

Nhìn chung, kết quả cho thấy L-GreCo duy trì trong giới hạn khôi phục độ chính xác 1% lỗi nhân (Mattson et al., 2020) đối với hầu hết các tác vụ, thường rất gần với đường cơ sở đồng nhất, trong khi tăng tỷ lệ nén một cách nhất quán, trên tất cả các tác vụ và kỹ thuật nén được xem xét. Chúng tôi nhấn mạnh rằng chúng tôi không thực hiện điều chỉnh tham số cụ thể cho tác vụ. Lợi ích đặc biệt cao cho kỹ thuật thưa thớt hóa và hạng thấp, nơi không gian tìm kiếm và do đó cả tiềm năng tiết kiệm của L-GreCo cao hơn. Ví dụ, đối với Transformer-LM, chúng tôi có được lên đến 5x nén cao hơn so với đường cơ sở đồng nhất, với tác động độ chính xác không đáng kể. Đồng thời, chúng tôi lưu ý rằng L-GreCo gây ra mất mát nhân >1% đối với lượng tử hóa và nén hạng thấp cho mô hình Transformer-LM rất nhạy cảm.¹ Điều này là do phạm vi nén mặc định của chúng tôi quá tích cực trong trường hợp này; điều này có thể dễ dàng được giải quyết bằng cách điều chỉnh phạm vi—chúng tôi chọn không làm điều đó để nhất quán.

Để khám phá thêm đánh đổi độ chính xác-nén, chúng tôi thay đổi tham số nén đồng nhất mặc định, cụ thể là điều tiết hạng powerSGD mục tiêu cho tác vụ Transformer-XL/WikiText-103. Hình 3 cho thấy L-GreCo cung cấp đánh đổi tốt hơn đáng kể so với nén đồng nhất. Chúng ta có thể thấy rằng L-GreCo cung cấp khôi phục perplexity tốt hơn trong khi cải thiện tốc độ huấn luyện của mô hình.

Lập hồ sơ. Để khám phá chi phí nén chúng tôi chạy lập hồ sơ huấn luyện. Kết quả được trình bày trong Hình 8. Ở đó chúng tôi so sánh thời gian hoạt động cho huấn luyện gốc (không nén) và huấn luyện nơi gradient được nén với PowerSGD, hạng 32. Chúng ta có thể thấy rằng nén tương đối đắt (PowerSGD tốn thời gian hơn QSGD và TopK được tối ưu hóa) mất ít hơn 10% thời gian bước.

Kết quả tăng tốc. Đối với cải thiện tăng tốc huấn luyện đầu cuối, chúng tôi sử dụng siêu tham số huấn luyện không nén tiêu chuẩn cho ResNet và Transformer. Chúng tôi xem xét mở rộng yếu, tức tăng kích thước lô toàn cục trong khi tăng số nút. (Cải thiện hiệu suất cao hơn cho mở rộng mạnh.) Chúng tôi bắt đầu bằng cách kiểm tra kết quả thông lượng huấn luyện cho các thí nghiệm đa nút trong Hình 4 và 5, được thực hiện trong môi trường đám mây. Cài đặt này gặp phải nghẽn cổ chai băng thông ngay cả ở số nút thấp hơn, điều này rõ ràng do hiệu suất kém của đường cơ sở không nén cho cả hai mô hình. Nén đồng nhất được điều chỉnh loại bỏ nghẽn cổ chai này ở mức độ đáng kể: ví dụ, PowerSGD/ResNet50 đồng nhất có thể đạt 75% mở rộng lý tưởng trên 4 nút.

Do đó khá ngạc nhiên rằng nén không đồng nhất tự động vẫn có thể cung cấp cải thiện đáng kể trong cài đặt này: so với nén đồng nhất, L-GreCo cho lên đến 1.45x cải thiện thông lượng cho ResNet50, và lên đến 2.4x tăng tốc trên Transformer-XL. Điều này cho thấy rằng nén không đồng nhất có thể là một chiến lược hiệu quả trong kịch bản này, đặc biệt cho các mô hình không đồng nhất lớp như Transformer.

Tiếp theo chúng tôi kiểm tra kết quả cho mở rộng nút đơn từ 1 đến 8 GPU, được trình bày trong Hình 6 và 7. Đây là kịch bản thách thức hơn cho nén giao tiếp vì băng thông ít là nghẽn cổ chai hơn trong cài đặt nút đơn. Chúng tôi bắt đầu bằng cách kiểm tra kết quả cho mô hình Transformer-XL. Đối với PowerSGD và TopK, L-GreCo dẫn đến lợi ích lên đến 25% tăng tốc đầu cuối so với đồng nhất, với sự khác biệt độ chính xác không đáng kể. Đối với QSGD, không gian tìm kiếm rất hạn chế: đồng nhất đã sử dụng 4 bit, và cung cấp mở rộng rất tốt. Phương pháp thích ứng của chúng tôi vẫn cung cấp 2% tăng tốc so với nén đồng nhất được điều chỉnh tốt của chúng tôi, và 50% tăng tốc so với huấn luyện không nén, đạt ≥90% mở rộng lý tưởng. Ở đây, chúng tôi quan sát rằng, đối với ResNet50, PowerSGD và QSGD có thể khôi phục độ chính xác ResNet50 với tham số đồng nhất gần với những tham số tối ưu lý thuyết (ví dụ hạng-4 cho PowerSGD, trong khi mức tối thiểu là hạng-1), để lại ít chỗ cho cải thiện cho các cơ chế thích ứng. Tuy nhiên, L-GreCo cung cấp cải thiện nhất quán trong trường hợp mô hình Transformer-XL không đồng nhất (Hình 6).

Nhìn chung, chúng tôi lưu ý rằng L-GreCo cung cấp cải thiện hiệu suất có ý nghĩa thống kê so với nén đồng nhất tĩnh (đặc biệt cho các mô hình không đồng nhất) khi áp dụng cho tất cả các phương pháp nén được xem xét, với tác động không đáng kể đến độ chính xác.

--- TRANG 5 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

1 2 4
Số Nút | 20k40k60k80k100k120k | Token/giây | đường cơ sở
đồng nhất
L-GreCo
(a) PowerSGD

1 2 4
Số Nút | 20k40k60k80k100k120k | Token/giây | đường cơ sở
đồng nhất
L-GreCo (b) QSGD

1 2 4
Số Nút | 20k40k60k80k100k120k140k | Token/giây | đường cơ sở
đồng nhất
L-GreCo (c) TopK

Hình 4. Thông lượng cho Transformer-XL (TXL) trên WikiText-103. Đa nút, mỗi nút có 4 GPU RTX3090.

1 2 4
Số Nút | 1k2k3k4k5k6k7k8k9k10k11k12k13k14k | Ảnh/giây | đường cơ sở
đồng nhất
L-GreCo
(a) PowerSGD

1 2 4
Số Nút | 1k2k3k4k5k6k7k | Ảnh/giây | đường cơ sở
đồng nhất
L-GreCo (b) QSGD

1 2 4
Số Nút | 1k2k3k4k5k6k7k8k9k10k11k12k13k14k | Ảnh/giây | đường cơ sở
đồng nhất
L-GreCo (c) TopK

Hình 5. Thông lượng cho ResNet50/ImageNet. Đa nút, mỗi nút có 4 GPU RTX3090.

5.3 So sánh với các phương pháp thích ứng khác

Cho đến nay, chúng tôi đã sử dụng nén đồng nhất làm đường cơ sở. Bây giờ chúng tôi so sánh L-GreCo với các công trình trước đây về việc chọn tham số nén thích ứng. Chúng tôi xem xét các phương pháp Accordion (Agarwal et al., 2021b) và CGX (Markov et al., 2022), vì chúng gần nhất về phạm vi, và tổng quát nhất về phương pháp nén áp dụng được. Chúng tôi thực hiện so sánh trên mô hình Transformer-XL trên WikiText-103, vì 1) đây là mô hình nhạy cảm với nén gradient, 2) có tính không đồng nhất cao của các lớp, và 3) bị nghẽn cổ chai băng thông (xem Hình 4). PowerSGD được chọn làm phương pháp nén, vì Accordion tối ưu hóa cụ thể cho nó, và phương pháp này có thể dẫn đến lợi ích nén thực tế cao nhất. Chúng tôi chạy các thí nghiệm trong hai cài đặt phân tán: nút đơn với 8 GPU và đa nút bao gồm 4 máy với 4 GPU mỗi máy. Chúng tôi so sánh nén và thông lượng (mẫu được xử lý mỗi giây) cho mỗi phương pháp.

Trong các thí nghiệm trong phần này, chúng tôi lệch khỏi phương pháp của mình về việc chọn phạm vi tìm kiếm của tham số nén cho thuật toán L-GreCo, được mô tả trong Phần 5.1. Ở đây, chúng tôi nhằm có được tăng tốc nhiều nhất mà không mất độ chính xác mô hình cuối cùng. Do đó, đối với mỗi thuật toán chúng tôi xem xét trong phần này, chúng tôi điều chỉnh tham số nén cho phương pháp nén đã chọn và tác vụ huấn luyện (mà không thay đổi siêu tham số huấn luyện, ví dụ tốc độ học, weight decay, v.v.) để chúng tôi có được kết quả thời gian tốt nhất với độ chính xác mô hình cuối cùng trong giới hạn cho phép của MLPerf. Phạm vi tham số tốt nhất cho L-GreCo hóa ra là hạng [8,64] (xem Bảng 4) với hạng mặc định 32.

Bảng 4. So sánh L-GreCo với các thuật toán thích ứng khác trên Transformer-XL sử dụng PowerSGD.

Thuật toán
Thích ứng | Param.
Range | Tỷ lệ | Nút đơn
Token/s | Đa
nút
Token/s
Đồng nhất | 32 | 14.1 | 110k | 72k
L-GreCo | 8 - 64 | 23.5 | 144k | 150k
Accordion | 16, 64 | 23.9 | 114k | 107k
CGX,
kmeans | 8 - 64 | 21.6 | 124k | 112k
L-GreCo +
Accordion | 8 - 128 | 36.9 | 138k | 176k

So sánh Global TopK. Bài toán tối ưu - giảm thiểu độ lớn lỗi gradient cho tỷ lệ nén mong muốn - có thể được giải quyết bằng global topK trong trường hợp thưa thớt hóa gradient. Thật vậy, global topK được áp dụng cho gradient của toàn bộ mô hình nên nó chọn các phần tử có độ lớn cao nhất, giảm thiểu tổng

--- TRANG 6 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

1 2 4 8
Số GPU | 20k40k60k80k100k120k | Token/giây | đường cơ sở
đồng nhất
L-GreCo
(a) PowerSGD

1 2 4 8
Số GPU | 20k40k60k80k100k120k140k160k180k | Token/giây | đường cơ sở
đồng nhất
L-GreCo (b) QSGD

1 2 4 8
Số GPU | 20k40k60k80k100k120k140k160k | Token/giây | đường cơ sở
đồng nhất
L-GreCo (c) TopK

Hình 6. Thông lượng cho Transformer-XL (TXL) trên WikiText-103. Nút đơn, 8 GPU RTX3090.

1 2 4 8
Số GPU | 1k2k3k4k5k6k7k | Ảnh/giây | đường cơ sở
đồng nhất
L-GreCo
(a) PowerSGD

1 2 4 8
Số GPU | 1k2k3k4k5k6k7k8k | Ảnh/giây | đường cơ sở
đồng nhất
L-GreCo (b) QSGD

1 2 4 8
Số GPU | 1k2k3k4k5k6k7k8k | Ảnh/giây | đường cơ sở
đồng nhất
L-GreCo (c) TopK

Hình 7. Thông lượng cho ResNet50/ImageNet. Nút đơn, GPU RTX3090.

Lan truyền tiến | Lan truyền lùi | Giao tiếp | Nén | 0100200300400500 | Thời gian, ms | Không nén
PowerSGD

Hình 8. Lập hồ sơ huấn luyện không nén vs nén PowerSGD, hạng 32. Mô hình Transformer-XL trên tập dữ liệu WikiText-103. Nút đơn, GPU RTX3090.

lỗi. Tuy nhiên, phương pháp này có một số nhược điểm. Đầu tiên, global topK yêu cầu điều chỉnh và tìm kiếm siêu tham số thích hợp để hội tụ khi mật độ thấp được sử dụng. L-GreCo ngược lại không cố gắng giảm thiểu lỗi nén toàn cục – nó cố gắng khớp nó với lỗi nén của nén đồng nhất theo lớp khôi phục độ chính xác. Cũng như vậy, L-GreCo có tham số nén thấp nhất có thể. Điều đó có nghĩa là mỗi lớp có đóng góp của nó trong đồng bộ hóa gradient. Điều này có thể không phải là trường hợp cho global topK - ở mật độ cao một số lớp có thể bị bỏ lại mà không có cập nhật trong một số bước tối ưu. Điều này có tác động xấu đến chất lượng mô hình cuối cùng. Nhược điểm thứ hai của global topK là tăng tốc thực tế mà nó có thể cung cấp. Như chúng tôi đã thảo luận trong Phần 4, trong các khung DataParallel hiện đại, gradient được đồng bộ hóa song song với tính toán vì hiệu quả - người ta có thể ẩn chi phí giao tiếp đằng sau tính toán. Tuy nhiên, trong trường hợp global topK, vì người ta phải đợi cho đến khi tất cả gradient sẵn sàng, sau đó thực hiện nén và giao tiếp. Có thể xảy ra trong trường hợp này chúng ta có nhiều giao tiếp không chồng chéo hơn so với nén theo lớp hoặc thậm chí trong trường hợp không nén (xem Hình 10).

Để xác nhận nghi ngờ của chúng tôi về global topK, chúng tôi đã triển khai thuật toán sử dụng hook torch.distributed và chạy huấn luyện RN18/CIFAR100. Chúng tôi phát hiện rằng global topK chậm hơn 10% so với L-GreCo khi áp dụng với mật độ toàn cục tương tự trong trường hợp này - 0.25%.

So sánh Accordion. Accordion thích ứng nén bằng cách phát hiện các chế độ quan trọng trong quá trình huấn luyện. Nó chấp nhận hai chế độ nén có thể (tương ứng với nén thấp và cao), và có một tham số ngưỡng lỗi η. Nó thu thập gradient, và định kỳ quyết định tham số để sử dụng dựa trên thông tin gradient cho mỗi lớp. Chúng tôi đã triển khai Accordion sử dụng hook torch.distributed, được sử dụng cho PowerSGD. Đối với tham số η, chúng tôi chọn giá trị 0.5 được đề xuất bởi các tác giả và cố gắng điều chỉnh thủ công cặp tham số nén thấp và cao tốt nhất cho mỗi mô hình, với đó huấn luyện hội tụ đến độ chính xác trong giới hạn MLPerf. Chúng tôi chạy thuật toán này trên Transformer-XL/Wikitext-103, và phát hiện rằng cặp tham số tốt nhất (về thời gian huấn luyện mà không mất độ chính xác) là hạng nén cao 16, và hạng nén thấp 64.

Trong Hình 9, người ta có thể thấy động lực của tỷ lệ nén trung bình trong quá trình huấn luyện của Accordion và L-GreCo. Chúng tôi nhận thấy rằng Accordion chọn hạng nén thấp cho hầu như tất cả các lớp trong giai đoạn đầu tiên của huấn luyện và hạng nén cao cho thời gian huấn luyện còn lại, dẫn đến nén đồng nhất hoàn toàn hai chiều. Điều này cho thấy Accordion có thể không thực sự khai thác bản chất không đồng nhất của mô hình DNN. Do đó, các tối ưu hóa của Accordion và L-GreCo, tương ứng, có thể được xem là trực giao: Accordion tập trung vào việc thay đổi lượng nén trung bình trong quá trình huấn luyện, trong khi L-GreCo tìm cách tối ưu để đạt được mức trung bình này bằng cách đặt mục tiêu theo lớp.

Với điều này trong tâm trí, chúng tôi đã kết hợp hai thuật toán này, như sau. Chúng tôi đầu tiên thực hiện Accordion để có được tham số được đề xuất cho mỗi lớp và sử dụng những tham số này làm tập tham số mặc định trong thuật toán L-GreCo. Tập tham số mặc định được sử dụng để định nghĩa lỗi tối đa của thuật toán DP (xem dòng 2 trong Thuật toán 1). Trong phương pháp này, Accordion xác định độ nhạy cảm mô hình đối với nén gradient tại các điểm khác nhau trong huấn luyện, trong khi L-GreCo tìm ánh xạ tốt nhất của tham số nén mỗi lớp. Trong Hình 9, chúng ta thấy rằng sự kết hợp kết quả (L-GreCo với phạm vi [8,128] và Accordion với high=16, low=64) cung cấp kết quả vượt trội về tỷ lệ nén, mà không hy sinh độ chính xác.

Chúng tôi cũng so sánh hiệu suất của hai thuật toán riêng lẻ (xem Bảng 4). Chúng tôi quan sát rằng, mặc dù thực tế rằng tỷ lệ nén lý thuyết được đề xuất bởi Accordion về cơ bản giống như của L-GreCo, thông lượng Accordion ít hơn khoảng 30%. Điều này được giải thích bởi thực tế rằng L-GreCo nén các lớp được truyền cuối cùng (bucket) đến mức cao hơn, dẫn đến thời gian truyền tổng cải thiện đáng kể. Cụ thể, trong Hình 9b, chúng tôi quan sát rằng L-GreCo truyền ít phần tử gấp đôi trong bucket cuối cùng so với Accordion.

Hơn nữa, việc kết hợp L-GreCo với Accordion

--- TRANG 7 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

0 5 10 15 20 25 30 35
Số bước, x1000 | 102030405060 | Tỷ lệ nén
14.123.9
23.536.9
21.2 | Đồng nhất 32
Accordion [16:64]
L-GreCo [8:64] | Accordion + L-GreCo
CGX-kmeans
(a) Tỷ lệ nén theo thời gian.

1 2 3 4 5 6 7 8 9
Bucket | 2468 | Số phần tử được truyền | 1e6
Đồng nhất
CGX, kmeans
Accordion
L-GreCo
L-GreCo + Accordion (b) Nén mỗi nhóm lớp (bucket).

Hình 9. Nén thích ứng sử dụng L-GreCo so với các phương pháp khác, cho nén PowerSGD trên Transformer-XL. Biểu đồ bên trái cho thấy động lực của tỷ lệ nén trong quá trình huấn luyện, đánh dấu tỷ lệ nén trung bình. Biểu đồ bên phải trình bày số phần tử được truyền mỗi bucket trung bình theo thời gian. Bucket theo thứ tự giao tiếp.

B0 | Lan truyền lùi | B1 B2
C(B0|B1|B2)
Nén + Giao tiếp
B0 Lan truyền lùi B1 B2
C(B0) Nén + Giao tiếp C(B1) C(B2) | Thời gian
Thời gian | a)
b)

Hình 10. Sơ đồ giao tiếp trong trường hợp (a) nén Global TopK và (b) giao tiếp theo bucket tiêu chuẩn. Global TopK phải thu thập toàn bộ gradient mô hình trong bucket trước khi nén và đồng bộ hóa chúng, trong khi phương pháp tiêu chuẩn cho phép chồng chéo nén và giao tiếp với lan truyền lùi.

cải thiện tỷ lệ nén 50%, và thời gian huấn luyện lên đến 66% so với Accordion.

So sánh Rethink-GS. Sahu et al. (2021) đề xuất một phương pháp thưa thớt hóa về mặt kỹ thuật là thích ứng - thưa thớt hóa ngưỡng cứng thay đổi số phần tử được truyền dựa trên phân bố gradient. Chúng tôi đã chạy huấn luyện L-GreCo của ResNet18/CIFAR-100 trong thiết lập được mô tả trong bài báo. Chúng tôi sử dụng mật độ 1% làm tham số mặc định cho L-GreCo và phạm vi tìm kiếm là [0.1%,10%]. Đối với Rethink-GS chúng tôi sử dụng tham số λ = 4.72×10−3.

Kết quả chúng tôi thấy rằng L-GreCo cải thiện thuật toán Rethink-GS 17% về tỷ lệ nén (6.7× vs 5.7×) trong khi cũng cải thiện độ chính xác cuối cùng - 71.7% vs 71.4% (các số khác với những gì chúng tôi hiển thị trong Bảng 2 vì ở đây chúng tôi sử dụng thiết lập từ (Sahu et al., 2021)). Chúng tôi lưu ý rằng khung của chúng tôi không yêu cầu bất kỳ điều chỉnh siêu tham số nào cho thí nghiệm này, trong khi Rethink-GS yêu cầu điều chỉnh cẩn thận tham số ngưỡng cứng λ.

So sánh CGX. Nén thích ứng của CGX dựa trên kmeans và ánh xạ các lớp vào không gian 2 chiều (kích thước lớp vs. lỗi L2). Thuật toán phân cụm các lớp thành một số nhóm và gán tham số nén được định trước cho các lớp trong nhóm. Chúng tôi đã triển khai logic này với nén PowerSGD. Chúng tôi sử dụng hạng 32 làm mặc định, và phạm vi tốt nhất (về nén) là từ 8 đến 64, sử dụng 6 cụm lớp. Kết quả được hiển thị trong Bảng 4. Tóm lại, L-GreCo cải thiện so với phương pháp kmeans lên đến 33%. Trong Hình 9b, chúng tôi quan sát rằng L-GreCo chọn tham số sao cho bucket lớn nhất và cuối cùng được nén nhiều nhất, trong khi thuật toán kmeans chọn tham số nén tệ hơn cho những lớp đó trong một số lần lặp.

Nhìn chung, L-GreCo cải thiện nén thực tế so với các kỹ thuật trước đây. Đáng chú ý, tỷ lệ nén cao nhất được đạt bởi phương pháp kết hợp Accordion + L-GreCo, tận dụng những hiểu biết theo lớp về cả độ nhạy cảm và động lực huấn luyện.

5.4 Số liệu độ chính xác dựa trên mất mát
Như chúng tôi đã thảo luận trong phần 3, có thể sử dụng số liệu khác nhau để đo độ nhạy cảm lớp đối với nén. Số liệu chúng tôi sử dụng trong tất cả các thí nghiệm là độ lớn lỗi. Số liệu có thể khác là dựa trên mất mát, trong đó chúng tôi thu thập sự khác biệt mất mát mô hình giữa huấn luyện không nén và huấn luyện với nén gradient của các lớp nhất định trong khi các lớp khác giữ nguyên và sử dụng sự khác biệt mất mát làm số liệu độ nhạy cảm.

So sánh hai phương pháp này, chúng tôi đánh giá hệ số tương quan của số liệu mà hai phương pháp này cung cấp. Chúng ta có thể thấy trong hình 11a, giá trị số liệu từ hai phương pháp có tương quan cao. Hơn nữa, chúng tôi phát hiện trong các thí nghiệm của mình rằng tham số nén cũng như tỷ lệ nén tổng kết quả cho cả hai số liệu rất gần với nhau. Do đó, cho thực tế rằng việc thu thập số liệu dựa trên mất mát yêu cầu chạy bổ sung ngoài huấn luyện, việc sử dụng số liệu dựa trên lỗi của chúng tôi là chính đáng.

5.5 Tối ưu hóa dựa trên thời gian
Xem xét thực tế rằng các bucket giao tiếp có tác động khác nhau đến hiệu suất huấn luyện, người ta có thể nhận thấy rằng tỷ lệ nén không luôn cho cải thiện tốc độ mong đợi. Có điều đó trong tâm trí, chúng tôi đã sửa đổi thuật toán L-GreCo để nó tối ưu hóa thời gian mong đợi của giao tiếp thay vì tỷ lệ nén. Như chúng tôi giải thích trong Phần 4, trong phương pháp này chúng tôi đo thời gian giao tiếp thay đổi tỷ lệ nén cho mỗi bucket được giao tiếp và huấn luyện mô hình hồi quy tuyến tính, chạy huấn luyện trong 50 bước với 5 bước khởi động cho mỗi tập tham số nén. Với mô hình chúng ta có được trọng số của các bucket tương ứng với mối quan hệ giữa số bit được giao tiếp mỗi bucket và thời gian giao tiếp. Sau đó chúng tôi sử dụng những hệ số đó trong số liệu chúng tôi cố gắng giảm thiểu trong Thuật toán 1.

Chúng tôi đã chạy thuật toán trên điểm chuẩn chính của chúng tôi - PowerSGD trong huấn luyện Transformer-XL/WikiText-103. Mô hình tuyến tính được xây dựng trên dữ liệu thời gian chúng tôi thu thập (5000 mẫu - tập tỷ lệ nén mỗi bucket) có điểm gần 1 có nghĩa là chúng tôi đã quản lý để dự đoán thời gian giao tiếp sử dụng kích thước bucket được giao tiếp gần như hoàn hảo. Chúng tôi phát hiện rằng các hệ số mỗi bucket từ mô hình tuyến tính gần với nhau (xem Hình 11b). Điều đó có nghĩa là tác động của mỗi bucket đến thời gian giao tiếp tỷ lệ với kích thước bucket. Chúng tôi nhận thấy rằng các tham số chúng tôi có được với thuật toán đã sửa đổi rất gần với các tham số từ thuật toán L-GreCo gốc. Cho điều đó chúng tôi nhận ra rằng trong trường hợp Transformer-XL/WikiText-103, thuật toán gốc cho các tham số tối ưu nhất về thời gian.

6 KẾT LUẬN
Chúng tôi đề xuất L-GreCo, một thuật toán nén gradient thích ứng tự động xác định tham số nén tối ưu theo lớp, cho một ràng buộc lỗi cố định. Thuật toán L-GreCo tìm ánh xạ tham số nén sao cho 1) tổng lỗi nén L2 khớp với mục tiêu được biết để khôi phục độ chính xác, và 2) tổng kích thước nén là tối thiểu cho mục tiêu này.

Xác thực thực nghiệm của chúng tôi trên tất cả các họ phương pháp nén gradient cho thấy huấn luyện với tham số theo lớp được đề xuất bởi L-GreCo khôi phục độ chính xác đường cơ sở trong khi tỷ lệ nén gradient được tăng lên đáng kể. L-GreCo cải thiện hiệu suất huấn luyện lên đến 2.5×, tiết kiệm lên đến 5.2× giao tiếp so với nén vanilla, và lên đến 122× so với huấn luyện không nén. Nhìn chung, công việc của chúng tôi cung cấp một phương pháp mới để cải thiện các phương pháp nén gradient hiện tại, với chi phí gần như bằng không về thời gian và mất độ chính xác.

Phương pháp của chúng tôi gợi ý rằng nén gradient theo lớp có thể là một hướng thú vị cho công việc thêm trong lĩnh vực này. Cụ thể, phương pháp của chúng tôi có thể được tăng cường với thông tin thời gian thực tế về chi phí truyền gradient lớp, có thể dẫn đến lợi ích thực tế thêm. Một mở rộng có thể khác sẽ là xem xét các chiến lược kết hợp, cho phép kết hợp các kỹ thuật nén khác nhau (ví dụ thưa thớt hóa và hạng thấp) bên trong cùng mô hình.

Chúng tôi đề xuất L-GreCo, một thuật toán nén gradient thích ứng tự động xác định tham số nén tối ưu theo lớp, cho một ràng buộc lỗi cố định. Thuật toán L-GreCo tìm ánh xạ tham số nén sao cho 1) tổng lỗi nén L2 khớp với mục tiêu được biết để khôi phục độ chính xác, và 2) tổng kích thước nén là tối thiểu cho mục tiêu này.

Phương pháp của chúng tôi được bổ sung bởi một khám phá sâu sắc về các số liệu "đúng" nắm bắt tác động độ chính xác và hiệu suất của nén, ở cấp độ theo lớp. Cụ thể, chúng tôi trình bày bằng chứng rằng việc giảm thiểu lỗi lượng tử hóa cục bộ, theo lớp dẫn đến kết quả rất tương tự với việc giảm thiểu số liệu toàn cục như mất mát đầu ra. Hơn nữa, việc tối đa hóa tỷ lệ nén theo lớp tương quan rất tốt với việc giảm thiểu cụ thể tổng thời gian truyền trên triển khai song song dữ liệu hiện tại.

Chúng tôi bổ sung những đóng góp thuật toán và phân tích này với một xác thực thực nghiệm mở rộng, trên tất cả các họ phương pháp nén gradient cho thấy huấn luyện với tham số theo lớp được đề xuất bởi L-GreCo khôi phục độ chính xác đường cơ sở trong khi tỷ lệ nén gradient được tăng lên đáng kể. L-GreCo cải thiện hiệu suất huấn luyện lên đến 2.5×, tiết kiệm lên đến 5.2× giao tiếp so với nén vanilla, và lên đến 122× so với huấn luyện không nén. Nhìn chung, công việc của chúng tôi cung cấp một phương pháp mới để cải thiện các phương pháp nén gradient hiện tại, với chi phí gần như bằng không về thời gian và mất độ chính xác.

Các mở rộng có thể của phương pháp chúng tôi có thể xem xét các chiến lược kết hợp, cho phép kết hợp các kỹ thuật nén khác nhau (ví dụ thưa thớt hóa và hạng thấp) bên trong cùng mô hình.

LỜI CẢM ƠN
Các tác giả trân trọng cảm ơn tài trợ từ Hội đồng Nghiên cứu Châu Âu (ERC) dưới chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu (thỏa thuận tài trợ số 805223 ScaleML), cũng như hỗ trợ thực nghiệm từ bộ phận IT của IST Austria, đặc biệt là Stefano Elefante, Andrei Hornoiu, và Alois Schloegl.

--- TRANG 8 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

TÀI LIỆU THAM KHẢO

Achille, A., Rovere, M., and Soatto, S. Critical learning periods in deep networks. In International Conference on Learning Representations, 2018.

Aflalo, Y., Noy, A., Lin, M., Friedman, I., and Zelnik, L. Knapsack pruning with inner distillation. arXiv preprint arXiv:2002.08258, 2020.

Agarwal, S., Wang, H., Lee, K., Venkataraman, S., and Papailiopoulos, D. Adaptive gradient communication via critical learning regime identification. In Smola, A., Dimakis, A., and Stoica, I. (eds.), Proceedings of Machine Learning and Systems, volume 3, pp. 55–80, 2021a.

Agarwal, S., Wang, H., Lee, K., Venkataraman, S., and Papailiopoulos, D. Adaptive gradient communication via critical learning regime identification. Proceedings of Machine Learning and Systems, 3:55–80, 2021b.

Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. QSGD: Communication-efficient sgd via gradient quantization and encoding. Advances in Neural Information Processing Systems, 30:1709–1720, 2017.

Chen, C. Y., Choi, J., Brand, D., Agrawal, A., Zhang, W., and Gopalakrishnan, K. Adacomp: Adaptive residual gradient compression for data-parallel distributed training. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pp. 2827–2835, 2018.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. IEEE, 2009.

Dryden, N., Jacobs, S. A., Moon, T., and Van Essen, B. Communication quantization for data-parallel training of deep neural networks. In Proceedings of the Workshop on Machine Learning in High Performance Computing Environments, pp. 1–8. IEEE Press, 2016.

Faghri, F., Tabrizian, I., Markov, I., Alistarh, D., Roy, D. M., and Ramezani-Kebrya, A. Adaptive gradient quantization for data-parallel sgd. Advances in neural information processing systems, 33:3174–3185, 2020.

Frantar, E. and Alistarh, D. SPDY: Accurate pruning with speedup guarantees. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 6726–6743. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/frantar22a.html.

Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. Error feedback fixes signsgd and other gradient compression schemes. In International Conference on Machine Learning, pp. 3252–3261. PMLR, 2019.

Krizhevsky, A. Learning multiple layers of features from tiny images. pp. 32–33, 2009. URL https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.

Lim, H., Andersen, D. G., and Kaminsky, M. 3lc: Lightweight and effective traffic compression for distributed machine learning. arXiv preprint arXiv:1802.07389, 2018.

Lin, Y., Han, S., Mao, H., Wang, Y., and Dally, W. J. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887, 2017.

Markov, I., Ramezanikebrya, H., and Alistarh, D. cgx: Adaptive system support for communication-efficient deep learning, 2022. URL https://arxiv.org/abs/2111.08617.

Mattson, P., Reddi, V. J., Cheng, C., Coleman, C., Diamos, G., Kanter, D., Micikevicius, P., Patterson, D., Schmuelling, G., Tang, H., et al. Mlperf: An industry standard benchmark suite for machine learning performance. IEEE Micro, 40(2):8–16, 2020.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

Nvidia. Nvidia deep learning examples for tensor cores, 2020. URL https://github.com/NVIDIA/DeepLearningExamples.

Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.

Ramezani-Kebrya, A., Faghri, F., Markov, I., Aksenov, V., Alistarh, D., and Roy, D. M. NUQSGD: Provably communication-efficient data-parallel sgd via nonuniform quantization. Journal of Machine Learning Research, 22(114):1–43, 2021.

Renggli, C., Ashkboos, S., Aghagolzadeh, M., Alistarh, D., and Hoefler, T. Sparcml: High-performance sparse communication for machine learning. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2019.

--- TRANG 9 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

Sahu, A., Dutta, A., M. Abdelmoniem, A., Banerjee, T., Canini, M., and Kalnis, P. Rethinking gradient sparsification as total error minimization. In Advances in Neural Information Processing Systems, volume 34, pp. 8133–8146. Curran Associates, Inc., 2021.

Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Fifteenth annual conference of the international speech communication association. Citeseer, 2014.

Shen, M., Yin, H., Molchanov, P., Mao, L., Liu, J., and Alvarez, J. M. Structural pruning via latency-saliency knapsack. arXiv preprint arXiv:2210.06659, 2022.

Strom, N. Scalable distributed dnn training using commodity gpu cloud computing. In Sixteenth Annual Conference of the International Speech Communication Association, 2015.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Vogels, T., Karinireddy, S. P., and Jaggi, M. Powersgd: Practical low-rank gradient compression for distributed optimization. Advances In Neural Information Processing Systems 32 (Nips 2019), 32, 2019.

Wang, H., Sievert, S., Charles, Z., Liu, S., Wright, S., and Papailiopoulos, D. ATOMO: Communication-efficient learning via atomic sparsification. 2018.

Wen, W., Xu, C., Yan, F., Wu, C., Wang, Y., Chen, Y., and Li, H. Terngrad: Ternary gradients to reduce communication in distributed deep learning. arXiv preprint arXiv:1705.07878, 2017.

Wu, Y.-C., Liu, C.-T., Chen, B.-Y., and Chien, S.-Y. Constraint-aware importance estimation for global filter pruning under multiple resource constraints. In Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020.

Xu, H., Ho, C.-Y., Abdelmoniem, A. M., Dutta, A., Bergou, E. H., Karatsenidis, K., Canini, M., and Kalnis, P. Grace: A compressed communication framework for distributed machine learning. In 2021 IEEE 41st international conference on distributed computing systems (ICDCS), pp. 561–572. IEEE, 2021.

--- TRANG 10 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

A ƯU TIÊN BUCKET

Xem xét thực tế rằng các bucket giao tiếp có tác động khác nhau đến hiệu suất huấn luyện, chúng tôi đã sửa đổi thuật toán L-GreCo để các bucket cuối cùng trong thứ tự truyền, tương ứng với các lớp trước đó, được nén nhiều hơn. Điều này bù đắp cho lỗi nén được gây ra bởi việc chọn tham số nén thấp hơn cho các bucket đầu tiên, tức các lớp cuối cùng. Về mặt thực tế, chúng tôi đã thêm ưu tiên tuyến tính cho các lớp trong Thuật toán 1, nhân kích thước của mỗi lớp với chỉ số của bucket mà lớp được giao tiếp. Hồ sơ của các phần tử được giao tiếp mỗi bucket được hiển thị trong Hình 12. Chúng tôi quan sát sự dịch chuyển tuyến tính của tỷ lệ nén cao hơn về phía các bucket cuối cùng. Tuy nhiên, ưu tiên bucket có hiệu suất tệ hơn so với L-GreCo gốc. Điều đó có nghĩa là hiệu ứng của việc truyền bucket lớn đầu tiên cao hơn hiệu ứng của nén tốt hơn các bucket cuối cùng.

1 2 3 4 5 6 7 8 9
Bucket | 0.51.01.52.0 | Số phần tử được truyền | 1e6
L-GreCo[8:64]
LGreco[8:64] + Tuyến tính

Hình 12. Phần tử được giao tiếp mỗi bucket cho L-GreCo và L-GreCo với ưu tiên bucket tuyến tính. Transformer-XL với PowerSGD.

B TÍNH TOÁN LỖI HẠNG THẤP

Như đã thảo luận trong Phần 4, một trong những bước chính của thuật toán chúng tôi là tính toán ma trận lỗi cho các tham số nén có thể khác nhau. Bảng 1 cho thấy đây là phần tốn thời gian nhất của khung chúng tôi. Cụ thể đối với PowerSGD, chúng ta cần tính toán lỗi hạng thấp cho một phạm vi rộng của hạng. Có hai giải pháp có thể để làm như vậy.

B.1 Phân Tách Giá Trị Số Ít

Cách đầu tiên để tính toán lỗi là sử dụng phân tách giá trị số ít và tính toán giá trị số ít cho một lớp cụ thể, và tính toán lỗi xấp xỉ cho hạng r < min(m, n) bằng cách tính toán er=√∑min(m,n) i=r+1 σ²ᵢ,

Bảng 5. Siêu tham số cho ResNet-18/CIFAR-100
Tham số | Giá trị
Số worker | 8
Optimizer | SGD với momentum
Kích thước lô toàn cục | 128
Momentum | 0.9
LR cơ sở | 0.1
Giảm LR | /10 tại epoch 150 và 250
Epoch | 200
Weight decay | 10⁻⁴

có thể được thực hiện hiệu quả cho tất cả hạng. Cụ thể, đủ để tính toán giá trị số ít bình phương một lần, và sau đó tính toán tất cả lỗi bằng một tích ma trận đơn. Do đó, nghẽn cổ chai là tính toán giá trị số ít yêu cầu thời gian O(mn·min(m, n)) và không gian O(n²+mn).

B.2 Bước Lặp Lũy Thừa

Phương pháp thứ hai là tính toán lỗi xấp xỉ cho mỗi hạng riêng biệt bằng cách thực hiện một vài bước lũy thừa (không có phần giao tiếp); như Vogels et al. (2019) tuyên bố, phương pháp này hội tụ đến ma trận được đề xuất bởi SVD. Về mặt thực tế, chúng tôi đã quan sát rằng việc áp dụng chỉ 5 bước lũy thừa là đủ để có lỗi nhỏ so với xấp xỉ hạng thấp tối ưu được đề xuất bởi SVD. Phương pháp này cần thời gian O(mnr) và không gian O((m+n)·r) để tính toán lỗi xấp xỉ hạng r và do đó O(mnr²max) để tính toán lỗi cho tất cả r∈[rmin, rmax].

B.3 Tốt Nhất Của Cả Hai Thế Giới

So sánh độ phức tạp tính toán và yêu cầu bộ nhớ của hai phương pháp cho thấy tốt hơn là sử dụng phương pháp lũy thừa khi phạm vi hạng nhỏ, ví dụ ResNet50 trên ImageNet hoặc ResNet18 trên Cifar100, và sử dụng phương pháp SVD khi phạm vi hạng lớn, ví dụ TransformerXL và TransformerLM trên WIKITEXT-103.

C KẾT HỢP POWER SGD VÀ L-G RECO

Chúng tôi lưu ý rằng trong tất cả các thí nghiệm phạm vi rộng của chúng tôi, tỷ lệ nén khi L-GreCo được áp dụng cho PowerSGD thường tăng trong quá trình huấn luyện (xem Hình 13). Điều này cũng phù hợp với trực giác đằng sau kết quả của (Agarwal et al., 2021a). Điều này cho thấy rằng trong kịch bản này, L-GreCo có thể tăng nén trong các giai đoạn học ít quan trọng hơn, (ví dụ epoch cuối).

--- TRANG 11 ---
L-GreCo: Nén Gradient Thích Ứng Theo Lớp

10 20 30 40 50 60 70 80 90
Epoch | 9092949698 | Tỷ lệ nén

Hình 13. Tỷ lệ nén của sơ đồ được đề xuất bởi L-GreCo trong quá trình huấn luyện. ResNet50 với PowerSGD.

Bảng 6. Siêu tham số trên ResNet-50/ImageNet
Tham số | Giá trị
Số worker | 8
Optimizer | SGD với momentum
Kích thước lô toàn cục | 2048
Momentum | 0.875
Khởi động LR | Tuyến tính trong 8 epoch, bắt đầu từ 0.256
Lịch LR | cosine
Giảm LR | /10 tại epoch 150 và 250
Epoch | 90
Weight decay | 1/32768
Làm mượt nhãn | 0.1

Bảng 7. Siêu tham số trên Transformer-XL/WikiText-103
Tham số | Giá trị
Số worker | 8
Optimizer | LAMB
Kích thước lô toàn cục | 256
Khởi động LR | Tuyến tính trong 1000 bước
Lịch LR | cosine
Số bước | 40k
Weight decay | 0.0

D CÀI ĐẶT THỰC NGHIỆM CHI TIẾT

Đối với tất cả các thí nghiệm, chúng tôi sử dụng siêu tham số tiêu chuẩn, tập dữ liệu và tiền xử lý dữ liệu. Siêu tham số chi tiết được hiển thị trong các bảng 5, 6, 7, và 8.

Đối với tiền xử lý ảnh của tập dữ liệu CIFAR-100, chúng tôi tuân theo các quy trình tăng cường dữ liệu và chuẩn hóa tiêu chuẩn. Cắt ngẫu nhiên và lật ngang ngẫu nhiên được áp dụng cho tăng cường dữ liệu. Chúng tôi cũng chuẩn hóa mỗi màu với các giá trị trung bình và độ lệch chuẩn sau cho mỗi kênh: (0.4914, 0.4822, 0.4465) và (0.2023, 0.1994, 0.2010).

Bảng 8. Siêu tham số trên Transformer-LM/WikiText-103
Tham số | Giá trị
Số worker | 8
Optimizer | Adam
Adam beta | (0.9, 0.98)
Kích thước lô toàn cục | 2048
Khởi động LR | Tuyến tính trong 4000 bước bắt đầu từ 10⁻⁷
Lịch LR | nghịch đảo sqrt
Số bước | 50k
Weight decay | 0.01

Mô hình ResNet50 sử dụng tăng cường dữ liệu sau. Chúng tôi thực hiện cắt có kích thước ngẫu nhiên đến 224×224, tỷ lệ từ 8% đến 100% và thực hiện lật ngang ngẫu nhiên. Cũng như vậy, chúng tôi thực hiện chuẩn hóa với trung bình (0.485, 0.456, 0.406) và độ lệch chuẩn (0.229, 0.224, 0.225).

Đối với tiền xử lý wikitext-103, chúng tôi sử dụng các công cụ tiền xử lý tiêu chuẩn và tokenizer được cung cấp bởi Nvidia Examples (Nvidia, 2020) và thư viện FairSeq (Ott et al., 2019).
