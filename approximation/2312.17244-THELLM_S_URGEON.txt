# 2312.17244.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/approximation/2312.17244.pdf
# File size: 662408 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
THELLM S URGEON
Tycho F.A. van der Ouderaa1∗, Markus Nagel2, Mart van Baalen2,
Yuki M. Asano3, Tijmen Blankevoort2
1Imperial College London ,2Qualcomm AI Research†,3QUV A Lab, University of Amsterdam
ABSTRACT
State-of-the-art language models are becoming increasingly large in an effort to
achieve the highest performance on large corpora of available textual data. How-
ever, the sheer size of the Transformer architectures makes it difficult to deploy
models within computational, environmental or device-specific constraints. We
explore data-driven compression of existing pretrained models as an alternative
to training smaller models from scratch. To do so, we scale Kronecker-factored
curvature approximations of the target loss landscape to large language models.
In doing so, we can compute both the dynamic allocation of structures that can
be removed as well as updates of remaining weights that account for the removal.
We provide a general framework for unstructured, semi-structured and structured
pruning and improve upon weight updates to capture more correlations between
weights, while remaining computationally efficient. Experimentally, our method
can prune rows and columns from a range of OPT models and Llamav2-7B by
20%-30%, with a negligible loss in performance, and achieve state-of-the-art re-
sults in unstructured and semi-structured pruning of large language models.
Code is available at: https://github.com/Qualcomm-AI-research/llm-surgeon.
1 I NTRODUCTION
Recent advancements in language modeling (Vaswani et al., 2017) allow fitting large language mod-
els (LLMs) with millions or even billions of parameters (such as OPT (Zhang et al., 2022) and
Llama 2 (Touvron et al., 2023)) on big text corpora achieving high performance. Unfortunately,
the size of these LLMs often makes it hard to deploy them within practical constraints. Cloud-
based deployment can get very expensive for larger models, and efficient devices such as phones are
frequently limited in the memory size to host a model.
A body of literature extending back to the late 1980s, e.g., Optimal Brain Damage (OBD, LeCun
et al. (1989)) and Optimal Brain Surgeon (OBS, Hassibi & Stork (1992)), phrases pruning as a
constraint optimization problem to reduce a model’s footprint and runtime requirements. The Hes-
sian required for this approach grows with the square of the number of parameters, and can only
be computed in practice for unrealistically small networks. To overcome this issue, Eigendamage
(Wang et al., 2019) introduces a Kronecker factorization of a blockwise-diagonal approximation of
the Hessian. Recent works, like Optimal Brain Compression (Frantar & Alistarh, 2022), SparseGPT
(Frantar & Alistarh, 2023), demonstrate practical post-training pruning of LLMs, but only consider
a loss curvature of a pruned layer’s squared output reconstruction error, ignoring gradients that relate
local removal costs to the target loss. As a result, their approximation to the target loss landscape
is inaccurate, leading to a significant performance degradation for pruned LLMs. Further, these
methods do not readily extend to structured pruning.
∗Work done while doing an internship at Qualcomm AI Research
†Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.
Structured compression (rows and columns) Unstructured compression (matrix elements)
1.3b 2.7b 6.7b model sizes:14.62
12.47
10.86
test set
perplexity
pretrained
K-OBD
LLM Surgeon
1.3b 2.7b 6.7b model sizes:14.62
12.47
10.86
test set
perplexity
pretrained
SparseGPT
LLM Surgeon
Figure 1: LLM Surgeon allows interpolation of model size between existing pretrained models.
1arXiv:2312.17244v2  [cs.LG]  20 Mar 2024

--- PAGE 2 ---
This work introduces LLM Surgeon, a general framework for unstructured, semi-structured and
structured pruning of LLMs. At paper submission, we deemed this the first method to successfully
perform structured pruning of LLMs. Concurrent work by Ashkboos et al. (2024) also considers
structured pruning of LLMs but ignores gradient information, resulting in lower final performance.
The superior performance of LLM Surgeon is achieved by scaling up the block-diagonal Kronecker-
factorized approximations to the empirical Fisher from Eigendamage to LLMs. We further expand
upon the work by deriving OBS-like weight pruning costs and updates for structured pruning of
multiple rows and columns, and provide a general framework that also incorporates semi-structured
and unstructured pruning. Instead of treating individual weight updates independently, we strive
to consider as many correlations between weights as practically possible and derive joint weight
updates for pruning multiple weights (or multiple sets of structured weights) at once. Unlike prior
work in LLM pruning, LLM Surgeon prunes in multiple shots, updating weights and curvature es-
timates between shots. We use global thresholding for unstructured, semi-structured and structured,
i.e., instead of pruning layers by a fixed amount, more sensitive layers are pruned less than those
that are more robust. Lastly, we propose to mitigate possible first-order gradients not being zero by
using optional low-rank first-order updates between shots. A key advantage of LLM Surgeon is that
it allows trading off additional compute during compression for better accuracy by increasing the
number of correlations and/or shots. Our method gives the first practically usable results for struc-
tured pruning of LLMs – they can be pruned by up to 30% with minor performance degradation.
Furthermore, we achieve state-of-the-art results in unstructured and semi-structured LLM pruning.
2 B ACKGROUND AND RELATED WORK
Neural network pruning aims to remove parameters from a model while minimizing negative im-
pact on final performance. More formally, we denote the Pmodel parameters as vector θ∗=
vec(W∗
1,W∗
2, . . .W∗
L)∈RP, by flattening the Lweight matrices of attention and fully-connected
blocks, with already fitted θ∗≈arg minθL(θ)to data Dto minimise a negative likelihood loss
L(θ)=−logp(θ|D). To compress the model, we are looking for a pruned vector ˆθ:
ˆθ= arg minθL(θ)s.t. pruning constraints based on θ∗(1)
where chosen constraints determine the structure of compressed weights ˆθ. Inunstructured prun-
ing, a fraction of total weight elements is set to zero. In semi-structured pruning of M:N we have
that M weights of every N consecutive weights are zero (Zhou et al., 2021; Hubara et al., 2021).
And in structured pruning (Louizos et al., 2017), entire rows and columns are set to zero. Struc-
tured pruning leads to the most immediate gains in memory and computing, as it directly reduces
the dimensions of matrices that need to be represented explicitly but is regarded as the most difficult
to compress. Maintaining high performance is often easier in the other schemes but requires spe-
cialised arithmetic exploiting the sparsity structure to benefit at deployment. We consider all pruning
types above, with a focus on structured pruning for LLMs.
Typically, eq. (1) can not be solved directly, as the space of possible pruning configurations exceeds
what can be evaluated in practice. To illustrate, a search over all possible unstructured pruning
masks of a 125 million parameter LLM would require 2P=2125m≈1037628749evaluations. The
idea, therefore, is to find ˆθusing a surrogate of the loss landscape qthat is easier to work with:
L(θ) =−logp(D |θ)≈ −logq(θ) (2)
If one chooses a particular Gaussian form for our surrogate q, then solutions for unstructured, semi-
structured, and structured pruning constraints can be derived in closed-form (appendix A).
2.1 T AYLOR EXPANSION
How do we obtain a good surrogate of the loss q? One of the easiest approaches is to locally expand
the log loss through a second-order Taylor expansion around the pretrained weights θ∗, yielding:
−logq(θ)≈ −logp(D|θ∗)−(θ−θ∗)T∇L(θ∗)−1
2(θ−θ∗)THθ∗(θ−θ∗) (3)
where [∇L(θ∗)]i=∂
∂θiL(θ∗
i)denotes the Jacobian and [Hθ]ij=∂2
∂θiθjL(θij)denotes the Hessian.
The first-order term vanishes [∇L(θ∗)]i=0at the optimum. Note that in practice the first order
2

--- PAGE 3 ---
Figure 2: Pruning as equality constrained optimization of quadratic approximation of the loss land-
scape (left), or equivalently, maximising the likelihood under a Laplace approximation (right).
term may not vanish. While we follow this assumption initially, we consider interleaved first-order
corrections to mitigate the issue in section 3.6. The quadratic expansion of eq. (3) forms the basis of
the optimal brain damage (LeCun et al., 1989) and optimal brain surgeon (Hassibi & Stork, 1992)
pruning methods. Note that from a probabilistic perspective, a quadratic approximation of the log
likelihood implies a Gaussian approximation of the likelihood, as also observed by (Wang et al.,
2019) and illustrated in fig. 2. This is well-known (Bishop & Nasrabadi, 2006), (MacKay, 2003)
as the Laplace approximation q(θ) =N(θ|θ∗+∇L(θ∗),H−1
θ∗), with pretrained weights are the
mean and the local inverse Hessian is the covariance matrix capturing correlations between weights .
2.2 B LOCK FISHER INFORMATION MATRIX
For a network trained with negative log-likehood loss, the Hessian is identical to the Fisher matrix:
Hθ=Fθ=XN
n=1Ey∼pθ(y|xn)
∇θlogpθ(y|xn)∇θlogpθ(y|xn)T
(4)
which has the benefit of always being positive semi-definite, with the inverse thus forming a proper
covariance matrix for q, and can be approximated with Monte Carlo samples of pθ(y|xn). For
most LLMs, this would be treating the softmax output of the network as categorical distribution
pθ(y|xn), and sampling from that. In practice, we use the ‘empirical Fisher’ replacing the expec-
tation over ywith target data yn(Kunstner et al., 2019). The full (empirical) Fisher Fθ∈RP×P
scales quadratically in the number of parameters P. To overcome this, the Fisher is often written in
terms of layer-wise blocks Flk=PN
n=1E
vec(∇Wllogpθ(y|xn))vec(∇Wklogpθ(y|xn))T
, and
approximated by only treating layers independently (Martens & Grosse, 2015; Botev et al., 2017):
Fθ=diag(F11,F22, . . . ,FLL), Fl=XN
n=1Eh
(gl,ngT
l,n)⊗(al,naT
l,n)
| {z }
RC×RCi
(5)
where ⊗denotes the Kronecker product and vec (·)the matrix vectorisation operation. Because
we disregard cross-layer interactions we write Flinstead of Fllfor Fisher blocks associated with
the weight matrix Wl∈RR×Cproducing outputs yl,n=Wlal,n∈RRfrom inputs al,n∈RC, for
each layer land datapoint n. Consequently, we can compute Fisher blocks from input activations
al,n∈RCof forward-passed data xnand output gradients gl,n=∇yl,nL∈RRfrom backpropagation.
2.3 P RUNING AS CONSTRAINED OPTIMIZATION
Optimal brain surgery relies on removing and adapting weights such that the loss is least negatively
affected, thus it behooves us to write the problem as a constrained optimization problem. From
the Gaussian approximation discussed in section 2.1 obtained by quadratically expanding the log
likelihood loss −logp≈1
2θTFθ, the optimal update ∆θ=ˆθ−θ(and thus also ˆθ=θ+∆θ) becomes
the following equality constrained quadratic optimization problem (Hassibi & Stork, 1992):
arg min
∆θ1
2∆θTF∆θ (6)
s.t.eT
k∆θ+eT
kθ= 0,∀k∈ K
where Fis positive semi-definite and Kis the set of Kindices that are pruned (i.e., set to zero).
3

--- PAGE 4 ---
Algorithm 1 LLM Surgeon ( structured )
Input: initial weights θ0, target size α, and data D
Forshottin [1, 2, . . . , T]
Compute: approximate curvature G,Afrom data D ▷section 3.1
Compute: costs per row/column Lr,LcfromG,A ▷section 3.2
Compute: threshold τusingLrandLcgiven target size αt ▷section 3.3
Select: rows and columns to remove ER,ECbased on τ ▷ section 3.3
Compute: weight update ∆θt−1based on ER,ECandG,A ▷section 3.4
Update: remaining weights θt←θt−1+ ∆θt−1▷section 3.5
Optionally: θt←low-rank update (θt) ▷section 3.6
Output: compressed weights ˆθ=θT
General solution We denote EK= [e1e2. . .eK]T∈[0,1]K×Pas a matrix of which the
row vectors are canonical basis vectors ek∈RPthat select the elements to be pruned. One of the
most standard approaches to solve eq. (6) is using Langrange multipliers, which results in a general
closed-form solution for the expected increase in loss Land optimal weight update ∆θ:
L=1
2(EKθ∗)T 
EKF−1ET
K−1EKθ (7)
∆θ=−F−1ET
K 
EKF−1ET
K−1EKθ (8)
which we use to derive unstructured, semi-structured, structured for modern Fisher approximations
(see appendices A.2 to A.4). The same general form of eqs. (7) and (8) appears in prior LLM pruning
work Kurtic et al. (2022), but only for much simpler layer-wise pruning and no structured pruning.
3 LLM S URGEON
This section describes the components of our method, LLM Surgeon , summarised in algorithm 1.
3.1 E STIMATING LOSS LANDSCAPE CURVATURE
Accurate pruning relies on approximating the local curvature accurately while overcoming the mem-
ory cost associated with storing the true curvature. Specifically, even with the block-wise approxima-
tion of eq. (5), F∈RRC×RCrequires summing Nlarge RC×RCmatrices, too large to practically
fit in memory. Instead, we adapt the KFAC approximation (Martens & Grosse, 2015) that assumes
independence of activations and derivatives, approximating an expectation of Kronecker products
as a Kronecker product of two expectations E[gl,ngT
l,n⊗al,naT
l,n]≈E[gl,ngT
l,n]⊗E[al,naT
l,n],
allowing layer-wise Fisher blocks to be approximated as Fl≈eFl, where
eFl=Gl⊗Al, withGl=1√
NXN
n=1gl,ngT
l,nandAl=1√
NXN
n=1al,naT
l,n (9)
constructed from activations al,n∈RCfrom forward passes and gradients gl,n∈RRfrom back-
ward passes (Eschenhagen et al., 2024). The approximation originates from optimization literature,
but has recently gained popularity for other problems that require curvature approximations (Immer
et al., 2022; van der Ouderaa et al., 2023), including structured pruning in Wang et al. (2019).
An additional advantage of approximating Fisher blocks as Kronecker products is that the inverse
becomes particularly easy to compute eF−1=G−1⊗A−1, thus only requires inverting the factors.
This fact allows us to never explicitly construct large RC×RCmatrices in memory that make up eF
andeF−1, but rather directly work with the much smaller matrices GandA.
3.2 C OMPUTING COSTS IN FINAL LOSS
The number of possible combinations in which weights can be removed grows (supra-)exponentially
in parameter count, making it infeasible to estimate a separate cost Lfor each such removal. A
common strategy, therefore, is to treat weights independently when computing removal costs L. We
also follow this strategy, but note that this does not necessarily imply that we have to make such same
strong independence assumption for the weight updates ∆θafter selecting weights to be removed.
4

--- PAGE 5 ---
Unlike most prior work, we present correlated weight updates by taking into account off-diagonal
elements of the Fisher approximation in section 3.4.
For semi-structured and unstructured we use independent costs for individual weight elements
k∈[1, RC], and for structured use independent costs for all rows r∈[1, R]and columns c∈[1, C].
We find that we can derive the appropriate costs from the general cost formula eq. (7) by letting
E=ek∈RRCwhere the single one-hot element at index kof canonical basis vector ekselects
the weight to remove. For structured pruning, we similarly select rows rand columns c, by setting
E=eT
r⊗I∈RC×RCorE=I⊗ec∈RR×RCwither∈RR,ec∈RC. Plugging into eq. (7), we find:
Lk=1
2(θk)2
[G−1⊗A−1]kk, Lr=1
2θT
rAθr
[G−1]rr,Lc=1
2θT
cGθc
[A−1]cc(10)
Full derivations can be found in appendices A.2 and A.3. The costs for single elements Lkare
equivalent to those found in optimal brain surgeon (Hassibi & Stork, 1992) and LrandLcclosely
resemble structured brain surgeon of (Wang et al., 2019), but in our case derived for matrix rows
and columns (see appendix A.3). Given curvature estimates, costs for either removing all weights
or all rows and columns can be computed in parallel. In addition, we derive costs for the more
general sum of Kronecker factor approximation eF≈G1⊗A1+G2⊗A2in appendix I through
an eigendecomposition.
3.3 D YNAMIC WEIGHT ALLOCATION WITH GLOBAL THRESHOLD
original W
row cost r, 
column cost c
structured
W+W
element costs k
semi-structured
W+W
element costs k
unstructured
W+W
Pruned weights Updated remaining weights
Figure 3: General framework for structured, semi-
structured and unstructured compression.Unlike prior works that compress layer-by-
layer (Frantar & Alistarh, 2023), we use a
global threshold τenabling a dynamic alloca-
tion of sparsity levels across layers , pruning
most where it hurts the least. Our method can
compress a model to a specifically chosen target
sizeα, defined as the fraction of weights that
should remain, i.e. stay non-zero after com-
pression. In all structured, semi-structured, and
unstructured pruning (fig. 3), we select as many
weights for removal so that the target size αis reached that inflict the least possible costs L, as com-
puted according to section 3.2. For unstructured pruning, this is as simple as sorting the costs for all
weights Lkin the network and setting a global threshold τsuch that αfraction of weights fall within
the threshold Lk≤τ. For M:N semi-structured pruning, we sort the M costs of each N consecutive
weights and select the M weights with lowest cost. In case of a multi shot schedule (see section 3.5)
we also sum the M lowest costs in each block to find a cost per block, sort costs per block across the
entire network, and similar to the unstructured case set a global threshold τsuch that an αfraction
of weights fall within threshold. Lastly for structured pruning, we perform a sorting appropriately
weighted by the number of elements that make up a row or column and set the global threshold τ
such that αfraction of all weights fall within the threshold. Then we remove all rows and columns
that fall within the threshold Lr,Lc≤τ.
3.4 C ORRELATED WEIGHT UPDATES
Like most other pruning methods, we prune multiple weights at once (Frantar & Alistarh, 2023;
Wang et al., 2019). To arrive at pruning costs and weight updates for pruning multiple weights, it
is common to compute costs and updates for individual weights (or sets of weights) independently
and add them together to arrive at a joint pruning cost. In LLM Surgeon, we argue that it’s better
to consider weight updates jointly instead of independently. After selecting the set of weights for
pruning, we can often afford to compute a single correlated weight update associated to the joint
removal of multiple weights, instead of naively summing weight updates associated to individual
removals. We derive such correlated weight updates below. Note that, for the expected cost compu-
tation, we do assume that the row, column or weight costs are independent, as the number of possible
combinations of weights to prune grows too large to compute within reasonable time.
Fast unstructured / semi-structured correlated weight updates Mathematically, we represent
pruned weights as EK= [e1e2. . .eR′]T∈RK×RS, where er∈RR′are one-hot canonical ba-
5

--- PAGE 6 ---
sis vectors selecting the weights for removal. As each element khas a unique associated row r
and column cindex, we can consequently also use canonical basis vectors for these respective rows
ER∈RK×Rand columns EC∈RK×C(i.e., we have [ER]i⊗[EC]i=[EK]iis satisfied for all i).
We derive unstructured weight updates in appendix A.2, by considering eigendecompositions G=
K1S1KT
1,A=K2S2K2of the Fisher approximation F≈G⊗A, which from eq. (8) yields:
∆W=G−1
K1
KT
1W−1K2⊘S| {z }
K×K−1
K2
A−1(11)
where ⊘is element-wise division, and for brevity use bar notation K1=EKK1,K2=EKK2,
θ=EKθ, andS=diag(S1)diag(S2)T∈RR×C, and diag (·)vectorises matrix diagonals.
Programmatically, we always avoid explicitly representing large matrices eFandeF−1in memory,
but rather compute relevant quantities from their factors. Likewise, we never represent sparse ma-
tricesEK,ERorECin memory, but instead work with a lists of indices of the one-hot elements
directly. For example, we can cheaply construct K1=ERK1∈RK×RandK2=ECK2∈RK×C,
by copying row vectors, and the vector θ=EKθ=ERWET
C∈RKby indexing all pruned weights.
Maximum number of correlated weights The main computational bottleneck is the K×Kma-
trix inverse in eq. (11). To control compression speed, we can split pruned weights into disjoint
subsets K=K1∪K2∪. . ., such that each subset Kidoes not exceed the set maximum number of
correlated weights Ki≤m, and sum associated independent updates. Using less correlation by set-
ting a lower mallows trading compression quality for speed.
Fast structured correlated weight updates Unlike the general case which requires inverting a
K×Kmatrix for Kcorrelated weights, we find that weight updates with the Kronecker factored
Fisher approximation ˜F=G⊗Aonly require inverting a R′×R′matrix when removing R′rows
or aC′×C′matrix when removing C′columns. The updates are much cheaper than we would have
expected based on the effective number of weights in those rows and columns, which would imply
inverting R′C×R′CorRC′×RC′matrices. In practice, this leads to a significant speed-up for
structured pruning and weight updates that take into account correlations between rows or columns.
When removing R′rows, r1, r2, . . . r R′, or the C′columns c1, c2, . . . , c C′, with 1<R′< R and
1<C′<C, we denote one-hot vectors selecting all rows and columns to be removed respectively
asER′= [e1e2. . .eR′]T∈RR′×RandEC′= [e1e2. . .eC′]T∈RC′×C. We find
weight updates associated to removing the R′rows by setting EK=ER′⊗IorEK=I⊗EC′:
remove multiple R′rows:
remove multiple C′columns:∆W=−W(EC′A−1ET
C′)−1(A−1ET
C′)
∆W=−G−1ET
R′(ER′G−1ET
R′)−1W(12)
From here, it is clear that the special case of removing a single row ror column cunder Kronecker
approximation involves inverting a 1×1matrix, and thus only requires scalar division:
remove single row r:∆θ=−G−1er⊗θr
[G−1]rr, or single column c:∆θ=−θc⊗A−1ec
[A−1]cc(13)
in accordance to independent structured updates in Wang et al. (2019), for convolutional filters. We
have thus extended existing structured weight updates to rows and columns, and derived update rules
that also consider correlation between structured groups (in our case the rows and columns).
3.5 M ULTI SHOT PRUNING SCHEDULE
To improve the performance-to-sparsity ratio, we propose pruning in multiple shots. We theoreti-
cally justify this multi-shot approach by noting that the surrogate loss landscape qrelies on a Taylor
expansion (eq. (3)) that only holds locally and thus becomes unreliable for larger jumps ∆θin pa-
rameter space. We mitigate this by pruning in multiple T>1shots, t∈[1,2, . . . , T ], each resulting
in a smaller weight update ∆θafter which the curvature of the loss surface can be re-estimated.
When pruning to target size α, ie. removing 1−αof total weights, we choose a schedule αtstarting
atα0= 1and ends with αT=α, such that after Tshots, exactly αfraction of the total weight remain.
Empirically, we find that a linear schedule for αt, as formulated in section 4, monotonically improves
6

--- PAGE 7 ---
Table 1: Structured compression of large language models on wikitext-2 data.
Test performance (PPL)
Method Target size OPT (125m) OPT (1.3b) OPT (2.7b) OPT (6.7b) Llama-v2 (7b)
Baseline 100% 27.65 14.62 12.47 10.86 5.12
Magnitude 90% 767.2 894.4 1229 3464 36746
I⊗I 80% 4685 (1278) 2788 16747 347960
70% 17970 (3098) 9255 17312 41373
L-OBD 90% 33.3 20.76 17.69 27.20 14259
diag(I⊗A) 80% 94.14 1392 3236 7570 15630
multi shot 70% 545.6 2147 7233 7628 21386
K-OBD 90% 27.97 14.68 11.96 10.53 5.48
diag(G⊗A) 80% 29.89 15.63 12.47 11.28 9.14
multi shot 70% 36.54 18.29 14.53 13.03 15.43
60% 47.54 24.65 18.09 16.21 28.03
50% 75.95 37.68 26.68 25.54 46.64
LLM Surgeon ( ours ) 90% 28.29 14.73 12.00 10.82 5.43
G⊗A 80% 29.37 15.27 12.37 11.22 7.29
within row/col cor. ∆ 70% 32.46 16.60 13.16 11.83 10.85
60% 39.82 19.40 14.79 12.94 16.67
50% 51.48 23.81 18.01 15.38 25.62
LLM Surgeon ( ours ) 90% 28.01 14.70 12.02 10.77 5.25
G⊗A 80% 28.73 15.12 12.27 11.02 6.18
full cor. ∆ 70% 31.82 16.24 12.92 11.64 7.83
60% 38.47 18.45 14.23 12.58 10.39
50% 49.78 22.95 17.15 14.90 15.38
pruning performance with more shots, and that higher sparsity levels typically require more shots
(see appendix F.1). Multi-shot pruning allows one to spend (linearly in T) more computation to
improve the final compression performance.
3.6 I NTERLEAVED LOW -RANK FIRST -ORDER CORRECTIONS
We propose optional interleaved low-rank first-order corrections to further improve compression
performance. So far, we assumed parameters are in a local optimum when finding a closed-form
solution to the quadratic constraint problem. In practice, however, this assumption likely does not
hold since (i) the neural network may not be optimised to the minimum, (ii) a different loss may
be used for compression than used for training, or (iii) we prune in multiple shots (section 3.5)
inevitably causing weights to diverge from the optimum. To mitigate this, we consider first-order
corrections by interleaving pruning shots with low-rank adaptations of weights Wl+UV (LoRA, by
(Hu et al., 2021)), commonly used in LLM finetuning. We always absorb updates after each shot, so
that the next loss estimate qis closer to the optimum and underlying assumptions are likely to hold
more closely. By absorbing LoRA updates between shots, the sum of low-rank updates can have
a higher rank than individual updates. That is, we have rank (U1V1+U2V2+. . .+UTVT)≥
rank(UtVt)for the updates UtVtat any shot t, with equality only arising if updates lie exactly in
the same subspace which is unlikely to ever occur in practice. This insight could also be used during
regular LoRA finetuning and may therefore be useful outside the context of model compression to
allow more expressive low-rank model adaptation, at negligible cost.
4 R ESULTS
We compare compression performance of LLM Surgeon on language modeling tasks on OPT
(Zhang et al., 2022) and Llama-v2 (Touvron et al., 2023) model families, using data from wikitext-2
dataset (appendix B.2). For compression, we use 128 sequences with a sequence length of 2048
tokens from the training data set and evaluate test perplexity (PPL) on the standard test split. In our
experiments, we use a linear sparsity schedule αt=1−t(1−α
T)at each shot sbefore reaching the final
sparsity α. We use 40 shots at α=0.5sparsity and report intermediate compression rates, effectively
using T=8shots for α=0.9,T=16 forα=0.8,T=24 forα=0.7, and T=32 forα=0.6. We com-
pare against magnitude pruning, L-OBD, SparseGPT and K-OBD baselines. The K-OBD and LLM
Surgeon use the multi shot procedure of section 3.5 using T=40 shots for structured pruning and
T=5shots for semistructured and unstructured pruning. Further details are found in appendix B.
7

--- PAGE 8 ---
4.1 S TRUCTURED COMPRESSION
Structured compression of rows and columns enables direct savings in memory and compute through
a straight reduction of matrix dimensions in the model. For LLM surgeon, we consider in section 3.4
weight updates with different levels of correlations: limited to correlations within rows and columns,
and correlations both within and between rows and columns. We further compare against magnitude
pruning, which only uses weight magnitudes, L-OBD, which only uses activations, and K-OBD,
which also uses Kronecker-factored curvature but assumes full independence and thus only prunes
without updating remaining weights. We report results in table 1, and observe that more correlations
results in better performance, with the largest improvements for the Llama-v2 model family.
While a 50% structured compression is not better than a smaller model of similar size, LLM Surgeon
allows us to reduce model size by up to 30% with minimal loss, without training a smaller model
from scratch fig. 1. In our structured compression experiments our proposed LLM Surgeon method
outperforms all baselines and achieves the best performance for each compression target size.
4.2 I NTERLEAVED LOW -RANK UPDATES
Table 2: Structured compression of OPT-125m
on wikitext-2 using interleaved LoRA updates
Target without with
Size LoRA LoRA
Pretrained 100% 27.65 23.35
LLM Surgeon 90% 28.01 24.16
(ours ) 80% 28.73 25.25
G⊗A 70% 31.82 28.86
full cor. ∆ 60% 38.47 31.26
50% 49.78 36.50Additionally, we assess compression performance
in conjunction with the proposed first-order cor-
rections using the interleaved low-rank adaptation
described in section 3.6. We find that LoRA im-
proves compression performance in the smallest
125m model, but not in larger models. We hypoth-
esise that larger models are more prone to overfit-
ting on the relatively few batches of wikitext-2 data
used to compress the model. Nevertheless, we con-
clude that interleaved LoRA can be useful in cases,
and recommend first using the proposed method without interleaved updates and, if enough data is
available for compression, optionally using it if it improves performance.
4.3 S EMI-STRUCTURED COMPRESSION
For 2:4 semi-structured pruning, we compare LLM Surgeon with magnitude pruning, which only
uses weight magnitudes, single-shot L-OBD, which only uses activations, and single-shot K-OBD,
which also uses Kronecker-factored curvature but assumes full independence and thus only prunes
without updating remaining weights as well as the recent state-of-the-art SparseGPT (Frantar & Alis-
tarh, 2023). We report test performance after 50 % (2:4) semi-structured compression on wikitext-2
data in table 3. We empirically find that considering more weight correlations results in improved
final performance after compression. Our proposed LLM Surgeon is competitive with prior work
outperforming all baselines in terms of test set perplexity (PPL).
Table 3: Semi-structured 2:4 compression for large language models on wikitext-2 data.
Target Test performance (PPL)
Method F≈ size OPT (125m) OPT (1.3b) OPT (2.7b) OPT (6.7b)
Baseline 100% 27.65 14.62 12.47 10.86
Magnitude I⊗I 50% 342.04 379.57 1106.01 187.29
L-OBD diag (I⊗A) 50% 87.26 44.92 41.40 27.36
K-OBD diag (G⊗A) 50% 68.74 27.22 20.23 15.55
SparseGPT I⊗A 50% 45.51 29.44 14.92 13.01
LLM Surgeon ( ours ) G⊗A 50% 44.64 25.10 14.64 12.10
4.4 U NSTRUCTURED COMPRESSION
For unstructured pruning, we repeat the same experiments as structured pruning case described in
section 4.1. In table 4, we report final test performance in terms of perplexity (PPL) on wikitext-
2 after compressing LLMs of different sizes of OPT and Llama-v2 family. Overall, we find that
methods with more accurate approximations of the curvature landscape and that account for more
correlations perform better. The proposed LLM Surgeon outperforms all baselines, reaching the
highest test performance across target sizes.
8

--- PAGE 9 ---
Table 4: Unstructured compression of large language models on wikitext-2 data.
Target Test performance (PPL)
Method size OPT (125m) OPT (1.3b) OPT (2.7b) OPT (6.7b) Llama-v2 (7b)
Baseline 100% 27.65 14.62 12.47 10.86 5.12
Magnitude 90% 27.62 14.69 12.60 10.88 5.18
I⊗I 80% 28.53 15.68 13.18 11.26 5.37
70% 52.88 140.2 15.22 12.22 6.03
L-OBD 90% 29.70 16.24 14.44 13.43 6.09
diag(I⊗A) 80% 32.18 21.92 23.35 39.85 116.2
single shot 70% 49.08 204.7 274.8 810.4 6549
K-OBD 90% 27.64 14.62 12.09 36.89 5.13
G⊗A 80% 27.62 14.37 130220 39928 5.19
single shot 70% 27.92 220.1 23097 19506 5.60
60% 29.24 13783 10331 33896 9.20
50% 34.43 7311 10495 91506 118.6
SparseGPT 90% 27.93 14.69 12.00 10.86 5.49
I⊗A 80% 28.18 15.07 12.05 10.86 5.58
70% 28.93 22.77 12.17 10.89 5.71
60% 30.20 25.07 12.37 10.98 5.94
50% 33.17 26.77 12.88 11.92 6.51
LLM Surgeon ( ours ) 90% 27.69 14.62 12.01 10.86 5.13
G1⊗A1 80% 27.83 14.66 12.14 10.87 5.20
full cor. ∆ 70% 28.35 14.81 12.25 10.82 5.36
multi shot 60% 28.98 14.91 12.28 10.83 5.66
50% 30.30 15.47 12.68 10.97 6.08
4.5 L EARNED SPARSITY STRUCTURE
The proposed method can dynamically allocate sparsity across layers through global thresholds
described in section 3.3. In Fig. 4.5, we compare total allocated sparsity levels per layer depth and
per layer type after compressing a pretrained OPT-125m model. We find that the LLM Surgeon
prunes relatively more in the first layer and less in middle layers. Further, we observe that a larger
portions of weights are removed in fully-connected compared to attention blocks, but deviations are
less compared to other methods. Dynamic allocation allows for most pruning where it hurts least.
123456789101112
layer0%20%40%60%80%100%remaining weightsSparsity per layer depth
Q K V O FC1 FC2
layer type0%20%40%60%80%100%Sparsity per layer type
LLM Surgeon (full. cor )
LLM Surgeon (within row/col cor. )
Magnitude
L-OBD
K-OBD
Figure 4: Sparsity levels obtained with structured pruning on OPT-125m by layer depth and type.
5 C ONCLUSION
In this work, we have introduced the LLM Surgeon algorithm for unstructured, semi-structured and
structured compression of neural networks. The work builds upon classic neural network compres-
sion approaches originating from the early 1990’s that aim to find optimal pruning by expanding
the curvature of the loss landscape. The method utilises modern Fisher approximations to scale
accurate pruning to the realm of large language models (LLMs) with billions of parameters, while
remaining practical in both memory and compute. Unlike most prior work on data-based LLM
compression, we not only use weight magnitude and activations from forward passes, but also use
gradient information from backward passes to relate weight removal costs to the true final objective.
We improve upon prior work through more accurate approximations to the loss landscape curvature
and considering more weight correlations to update remaining weights. Increasing the number of
correlations and using multiple shots allows us trading off additional compute for better accuracy.
Lastly, LLM Surgeon gives the first practically usable results for structured pruning of LLMs and
achieves state-of-the-art results in unstructured and semi-structured large language model pruning.
9

--- PAGE 10 ---
REFERENCES
Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James
Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv
preprint arXiv:2401.15024 , 2024.
Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning , vol-
ume 4. Springer, 2006.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep
learning. In International Conference on Machine Learning , pp. 557–565. PMLR, 2017.
Runa Eschenhagen, Alexander Immer, Richard Turner, Frank Schneider, and Philipp Hennig.
Kronecker-factored approximate curvature for modern neural network architectures. Advances
in Neural Information Processing Systems , 36, 2024.
Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training
quantization and pruning. Advances in Neural Information Processing Systems , 35:4475–4488,
2022.
Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in
one-shot. 2023.
Gene H Golub and Charles F Van Loan. Matrix computations . JHU press, 2013.
Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain
surgeon. Advances in neural information processing systems , 5, 1992.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 , 2021.
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training
quantization with small calibration sets. In International Conference on Machine Learning , pp.
4466–4475. PMLR, 2021.
Alexander Immer, Tycho van der Ouderaa, Gunnar R ¨atsch, Vincent Fortuin, and Mark van der Wilk.
Invariance learning in deep neural networks with differentiable laplace approximations. Advances
in Neural Information Processing Systems , 35:12449–12463, 2022.
Abdoulaye Koroko, Ani Anciaux-Sedrakian, Ibtihel Ben Gharbia, Val ´erie Gar `es, Mounir Haddou,
and Quang Huy Tran. Efficient approximations of the fisher matrix in neural networks using
kronecker product singular value decomposition. arXiv preprint arXiv:2201.10285 , 2022.
Frederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical fisher approx-
imation for natural gradient descent. Advances in neural information processing systems , 32,
2019.
Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael
Goin, and Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning
for large language models. arXiv preprint arXiv:2203.07259 , 2022.
Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information
processing systems , 2, 1989.
Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through
l0regularization. arXiv preprint arXiv:1712.01312 , 2017.
David JC MacKay. Information theory, inference and learning algorithms . Cambridge university
press, 2003.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning , pp. 2408–2417. PMLR, 2015.
10

--- PAGE 11 ---
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843 , 2016.
Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach
for large language models. arXiv preprint arXiv:2306.11695 , 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Tycho van der Ouderaa, Alexander Immer, and Mark van der Wilk. Learning layer-wise equivari-
ances automatically using gradients. Advances in Neural Information Processing Systems , 36,
2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems , 30, 2017.
Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning
in the kronecker-factored eigenbasis. In International conference on machine learning , pp. 6566–
6575. PMLR, 2019.
Wikipedia. Wikipedia . PediaPress, 2004.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers:
State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer
language models. arXiv preprint arXiv:2205.01068 , 2022.
Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hong-
sheng Li. Learning n: m fine-grained structured sparse neural networks from scratch. arXiv
preprint arXiv:2102.04010 , 2021.
11

--- PAGE 12 ---
A D ERIVATIONS FOR PRUNING
Given that we use a Gaussian approximation of our loss p≈q=Nthrough a quadratic approxima-
tion of our log likelihood −logp≈1
2(θ∗)TFθ∗, the most optimal compression becomes the solution
to the following constrained optimization problem:
arg min
∆θ∗1
2∆(θ∗)TF∆θ∗(14)
s.t.eT
k∆θ∗+eT
kθ∗= 0,∀k∈Q
where Qis the set of Qindices that are pruned.
A.1 G ENERAL SOLUTION
Following (Kurtic et al., 2022), we denote pruned elements as EK= [eq1eq2. . .]T∈
[0,1]|Q|×Pand use the fact that solving eq. (6) through use of Langrange multipliers gives the
general closed-form solution for cost Land weight update ∆θ:
L=1
2(EKθ∗)T 
EKF−1ET
K−1EKθ∗(15)
∆θ∗=F−1ET
K 
EKF−1ET
K−1EKθ∗(16)
A.2 R EMOVING A SINGLE ELEMENT
Optimal brain surgeon (OBS) To remove a single element with index q, we simply set EK=eT
k:
L=1
2(EKθ∗)T 
EKF−1ET
K−1EKθ
=1
2θT
k1
[F−1]kkθk
=1
2(θk)2
[F−1]kk,∆θ=−F−1ET
K 
EKF−1ET
K−1EKθ
=−F−1ek 
eT
kF−1ek−1eT
Kθ
=−θk
[F−1]kkF−1ek(17)
which exactly correspond to the loss and updates of optimal brain surgeon (Hassibi & Stork, 1992).
Optimal brain damage (OBD) We may also consider that elements are independent and the
Fisher is diagonal. After noting that this implies that diagonal elements of the inverse Fisher are
scalar inverses of elements in the Fisher [F−1]kk=1
[F]kk, the formula’s simplify to:
L= [F]kk(θk)2, ∆θ=−θkek (18)
which exactly corresponds to loss and updates of optimal brain damage (LeCun et al., 1989).
VECTORISED
For implementation purposes, it might be convenient to have a vectorised notation Lθ∈RRCor
LW∈RR×Cto calculate all expected losses in parallel:
For OBD:
For OBS:Lθ=1
2θ∗⊙θ∗⊙diag(F)
Lθ=1
2θ∗⊙θ∗⊘diag(F−1),LW=1
2W∗⊙W∗⊙mat(diag(F))
LW=1
2W∗⊙W∗⊘mat(diag(F−1))(19)
A.3 R EMOVING A SINGLE ROW OR COLUMN
Structured OBS If we consider the approximation F≈G⊗Awith known inverse (G⊗A)−1=
G−1⊗A−1, then to remove a row at index r∈[0, R], we must take into account correlations within
12

--- PAGE 13 ---
elements of that row. That is, we write matrix EK= (eT
r⊗I)containing one-hot row-vectors for
all elements in row r. Plugging into the general solution eq. (7), we find:
L=1
2EKθT 
EKF−1ET
K−1EKθ∗
=1
2((eT
r⊗I)θ∗)T 
(eT
r⊗I)(G⊗A)−1(eT
r⊗I)T−1(eT
r⊗I)θ∗
=1
2θT
r 
eT
rG−1er⊗IA−1I−1θr
=1
2θT(eT
r⊗I) 
[G−1]rr
⊗A−1−1(er⊗I)θr
=1
2θT
rAθr
[G−1]rr(20)
where we write θr=eT
rW∗∈RCfor the r’th row-vector in W. Similarly, we obtain the associated
weight update:
∆θ=−F−1ET
K 
EKF−1ET
K−1EKθ∗
=−(G⊗A)−1(eT
r⊗I)T
(eT
r⊗I) (G⊗A)−1(eT
r⊗I)T−1
(eT
r⊗I)θ∗
=− 
G−1⊗A−1
(er⊗I) 
eT
rG−1er⊗A−1−1θr
=−1
[G−1]rr 
G−1er⊗A−1IA−1I
θr
=−G−1er⊗θr
[G−1]rr(21)
arriving at a similar structured pruning update as derived in (Wang et al., 2019) for convolutional
filters. We can equivalently derive expected loss and update for columns, by considering EK=
(I⊗eT
c). If we do so, we find the structured updates for a row ror column c:
Remove row r:
Remove column c:L=1
2θT
rAθr
[G−1]rr
L=1
2θT
cGθc
[A−1]cc∆θ=−G−1er⊗θr
[G−1]rr
∆θ=−θc⊗A−1ec
[A−1]cc(22)
Structured OBD We may also assume that, when removing a row r, the individual elements
within the row are also independent which would imply [A]ii=1
[A−1]ii. Similarly, [G]ii=1
[G−1]ii
when removing a column c. Consequently, we can simplify to:
Remove row r:
Remove column c:L=1
2GrrθT
rAθr
L=1
2AccθT
cGθc∆θ=−erθT
r
∆θ=−θceT
c(23)
similar form to structured OBD losses and updates as derived in (Wang et al., 2019) for convolutional
filters. The derivations slightly differ in that we start from the general solution eq. (8), circumventing
the need to rederive a Langrange multipliers for each possible structure.
A.4 P RUNING MULTIPLE (CORRELATED )ROWS AND COLUMNS
Let us consider the removal of R′rowsr1, r2, . . . r′
Rrows or C′columns with indices c1, c2, . . . , c C′,
with1<R′< R and1<C′<C. We denote matrices containing one-hot vectors selecting all rows
and columns to be removed respectively as:
ER′= [e1e2. . .eR′]T∈RR′×REC′= [e1e2. . .eC′]T∈RC′×C(24)
Then, the matrix EKcontaining one-hot row vectors selecting all elements to be removed can be
written as:
Multiple rows:
Multiple columns:EK= (ER′⊗IC)∈RQ×RC, (with Q=R′C)
EK= (IR⊗EC′)∈RQ×RC, (with Q=RC′)(25)
13

--- PAGE 14 ---
To simultaneously remove rows and columns, we can stack the matrices with duplicate row vectors
removed:
Multiple rows and columns: EK
ER′⊗IC
IR⊗EC′
∈RQ×RCwith duplicate rows removed (26)
The removal of duplicate rows is required due to the few R′C′overlapping elements between rows
and columns, after which the total number of rows thus becomes Q=R′C+C′R−R′C′. We used
appropriately sized identity matrices IR∈RR×RandIC∈RC×C. For brevity, we write the vector
or matrix of pruned weights θ:=EKθ∈RQ.
First, we derive the removal for R′rows by defining removal matrix as EK=ER′⊗Iand define
W:=ER′W∈RR′×C. The complete weight update for the removal of multiple rows becomes:
∆θ=−F−1ET
K 
EKF−1ET
K−1EKθ∗
=−(G⊗A)−1(ER′⊗I)T 
(ER′⊗I)(G⊗A)−1(ER′⊗I)T−1(ER′⊗I)θ∗
=−(G−1ET
R′⊗A−1) 
ER′G−1ET
R′⊗A−1−1θ∗
=−(G−1ET
R′⊗A−1) 
(ER′G−1ET
R′)−1⊗A
θ∗
∆W=−G−1ET
R′
(ER′G−1ET
R′)−1WA
A−1
=−G−1ET
R′(ER′G−1ET
R′)−1W (27)
Similarly, we derive the removal of C′columns by defining removal matrix as EK=I⊗EC′and
defineW:=EC′W∈RR×C′. The complete weight update for multiple column removal becomes:
∆θ=−F−1ET
K 
EKF−1ET
K−1EKθ∗
=−(G⊗A)−1(I⊗EC′))T 
(I⊗EC′)(G⊗A)−1(I⊗EC′)T−1(I⊗EC′)θ∗
=−(G⊗A)−1(I⊗EC′))T 
(I⊗EC′)(G⊗A)−1(I⊗EC′)T−1(I⊗EC′)θ∗
=−(G−1⊗A−1ET
C′) 
G⊗EC′A−1ET
C′−1θ
∆W=−G−1GW(EC′A−1ET
C′)−1(A−1ET
C′)
=−W(EC′A−1ET
C′)−1(A−1ET
C′) (28)
14

--- PAGE 15 ---
B E XPERIMENTAL DETAILS .
Code is available at: https://github.com/Qualcomm-AI-research/llm-surgeon.
B.1 M ODELS
OPT models From the OPT model family ((Zhang et al., 2022)), we consider models with the
following number of parameters: 125 million (125m), 1.3 billion (1.3b), 2.7 billion (2.7b), 6.7
billion (6.7b) models. We omit 350 million model due to different layer norm. We obtain the
standard pre-trained checkpoints using Huggingface (Wolf et al., 2019) and use this as a baseline
and initialisation for compression.
Llama-v2 models From the Llama-v2 model family ((Touvron et al., 2023)), we consider a model
with 7 billion (7b) parameters and a model with 13 billion (13b) parameters. We obtain the stan-
dard pre-trained checkpoints using Huggingface (Wolf et al., 2019) and use this as a baseline and
initialisation for compression.
B.2 D ATASETS
English / Wikitext-2 The majority of the results are obtained on the Wikitext-2 dataset containing
parsed subsets of the English Wikipedia (Merity et al., 2016; Wikipedia, 2004), using the default
training and test sets. For fitting, we use 128 batches of 2048 characters and for testing we use the
standard test set containing 4358 characters.
French / Wikipedia For French data experiments, we use a subset of French wikipedia
(Wikipedia, 2004). For fitting, we use 128 batches of 2048 characters and for testing we use a
randomly selected test set containing 1067888 characters.
German / Wikipedia For the Italian data experiments, we use a subset of the German wikipedia
(Wikipedia, 2004). For fitting, we use 128 batches of 2048 characters and for testing we use a
randomly selected test set containing 1112372 characters.
Italian / Wikipedia For the Italian data experiments, we use a subset of the Italian wikipedia
(Wikipedia, 2004). For fitting, we use 128 batches of 2048 characters and for testing we use a
randomly selected test set containing 633177 characters.
B.3 M ASK EQUIVALENCE
When comparing the equivalence of obtained pruning masks between two models θAandθBob-
tained by two compression methods AandB. We always consider the case of 50% pruning, and
define the mask equivalence as the fraction of same weights that are set two zero in both models:
mask equivalence =PX
i=11([θA]i= 0and[θB]i= 0)
P. (29)
where 1denotes an indicator function that returns 1 if both weights [θA]iand[θB]iare zero, and
returns 0 otherwise.
B.4 S PARSE GPT AND EVALUATION OF BASELINES
For the SparseGPT baseline, we used the official code SparseGPT code repository (Frantar & Alis-
tarh, 2023) which allows for training and evaluation on wikitext-2. The obtained results may differ
from those reported in the original paper as the C4 dataset was used there.
In this work, models were trained with the same 128 batches of the wikitext-2 training set as avail-
able in the SparseGPT codebase and are evaluated on the wikitext-2 test set using the exact same
evaluation procedure.
15

--- PAGE 16 ---
C T ECHNICAL DETAILS
C.1 P SEUDOCODES
Algorithm 2 LLM Surgeon ( structured )
Input: target size α
Input: initial weights θ0
Forshottin [1, 2, . . . , T]
Compute: approximate curvature G1,A1from data (optionally also G2,A2)▷section 3.1
Compute: costs per row/column Lr,LcfromG1,A1,(G2,A2) ▷section 3.2
Compute: threshold τusingLrandLcgiven target size α ▷ section 3.3
Select: rows and columns to remove ER,ECbased on τ ▷ section 3.3
Compute: weight update ∆θt−1based on ER,ECandG1,A1,(G2,A2) ▷section 3.4
Update: remaining weights θt←θt−1+ ∆θt−1▷section 3.5
Optionally: θt←low-rank update (θt)
Output: compressed weights ˆθ=θT
Algorithm 3 LLM Surgeon ( semi-structured / unstructured )
Input: target size α
Input: initial weights θ0
Forshottin [1, 2, . . . , T]
Compute: approximate curvature G1,A1from data (optionally also G2,A2)▷section 3.1
Compute: costs per element LkfromG1,A1,(G2,A2) ▷section 3.2
Compute: threshold τfromLkand target size αt(unstructured/semistructured) ▷section 3.3
Select: elements to remove EKbased on τ(unstructured/semistructured) ▷section 3.3
Compute: weight update ∆θt−1based on EKandG1,A1,(G2,A2) ▷section 3.4
Update: remaining weights θt←θt−1+ ∆θt−1▷section 3.5
Optionally: θt←low-rank update (θt)
Output: compressed weights ˆθ=θT
C.2 D AMPENING
In practice, we dampen the GandAmatrices by adding a diagonal term G+λGIandA+λAI.
In our experiments, we found that values in the range [0.01, 0.1] multiplied by mean diagonal terms
generally works well. We follow (Frantar & Alistarh, 2023) and always use λA=0.01diag(A)to
be consistent with prior work and allow for a fair comparison with baselines. Further, we use
λG=0.1diag(G)for structured experiments and λG=0.01diag(G)in semi-structured and unstruc-
tured experiments.
16

--- PAGE 17 ---
D D OWNSTREAM TASK PERFORMANCE
We also evaluate our method on downstream tasks as perplexity metrics do not necessarily correlate
with downstream performance. Further, we also repeat this experiment using the C4 dataset as
reference data for compression, as this is used in prior work (Frantar & Alistarh, 2023) and as this
can be regarded a more general reference dataset. In tables 5 and 6 we report 0-shot test performance
of structured pruning for LLM surgeon and K-OBD baseline.
Table 5: Downstream task performance using Wikitext-2 for pruning.
Structured pruning
(with wikitext-2) Model size wikitext word ppl boolq piqa hallaswag winogrande arc easy arc challenge openbookq copa lambada openai wsc273 A VERAGE wikitext2
Dense baseline 100% 9.24 77.74 79.11 75.99 69.14 74.58 46.25 44.20 86.00 73.92 85.71 71.26
LLM Surgeon (ours )90% 9.63 76.21 78.56 75.39 67.64 74.12 46.50 43.60 85.00 72.64 84.98 70.46
80% 12.16 72.97 77.09 71.30 66.30 71.36 41.89 41.80 87.00 56.43 80.22 66.66
70% 16.91 61.25 73.56 60.72 61.09 63.09 36.69 38.80 81.00 28.33 76.56 58.11
60% 25.15 44.98 69.26 48.04 54.38 52.31 30.29 36.80 78.00 11.72 68.50 49.43
50% 43.68 39.60 64.36 40.29 52.57 44.91 26.28 30.80 74.00 6.52 61.54 44.09
K-OBD 90% 9.89 76.67 78.02 74.80 68.11 75.17 46.33 44.60 86.00 72.71 82.78 70.52
80% 17.62 74.34 75.24 67.85 64.64 63.80 40.27 41.60 83.00 30.23 82.42 62.34
70% 32.72 65.29 71.82 53.07 56.83 51.05 33.11 37.80 79.00 12.21 70.70 53.09
60% 68.63 60.80 65.67 43.99 53.20 41.79 28.50 34.00 75.00 7.04 60.44 47.04
50% 136.33 61.56 60.66 36.84 53.04 36.11 26.71 33.00 72.00 4.70 61.17 44.58
Table 6: Downstream task performance using C4 for pruning.
Structured pruning
(with C4) Model size wikitext word ppl boolq piqa hallaswag winogrande arc easy arc challenge openbookq copa lambada openai wsc273 A VERAGE wikitext2
Dense baseline 100% 9.24 77.74 79.11 75.99 69.14 74.58 46.25 44.20 86.00 73.92 85.71 71.26
LLM Surgeon (ours )90% 9.90 77.03 78.45 74.95 68.27 73.19 45.99 44.60 84.00 72.81 82.78 70.21
80% 14.42 75.60 76.82 69.71 63.85 70.29 41.30 42.80 87.00 45.53 82.42 65.53
70% 25.16 66.39 72.85 58.11 56.83 62.16 34.47 38.40 80.00 22.69 69.96 56.19
60% 45.35 62.48 68.93 48.10 55.64 51.56 27.99 35.20 70.00 12.56 61.54 49.40
50% 77.30 62.60 65.02 41.70 54.22 42.55 24.23 31.20 71.00 7.26 60.44 46.02
K-OBD 90% 10.59 75.47 78.18 73.61 66.46 72.52 44.37 43.60 87.00 71.22 82.42 69.48
80% 20.12 73.36 75.14 66.11 62.43 62.84 38.23 41.00 86.00 21.50 78.39 60.50
70% 56.92 63.30 68.44 52.31 55.64 46.72 31.31 34.60 77.00 5.69 68.86 50.39
60% 112.85 62.23 64.47 46.36 52.17 40.53 29.52 32.40 72.00 2.91 63.00 46.56
50% 272.16 62.42 61.70 38.47 50.43 33.29 26.96 31.80 65.00 0.91 59.34 43.03
We find that our method not only performs well in terms of test perplexity but also correlates well
with downstream performance, outperforming the baselines on these downstream tasks.
E A DDITIONAL EXPERIMENTS ON LLAMA -V2 13B.
To assess performance on larger 13B parameter models, we also report structured compression on
the Llama-v2 13B model and evaluate downstream task performance. Test perplexities (lower is
better) can be found in table 7 below:
Table 7: Pruning Llama-v2 13B model.
Baseline Pruned model sizes
Dense 100% 90% 80% 70% 60% 50%
K-OBD 4.547 4.908 6.294 10.08 13.06 16.06
LLM Surgeon 4.547 4.692 5.286 6.207 7.245 9.428
as well as evaluated results on downstream benchmarks (higher is better) in table 8 below.
Table 8: Downstream task performance after pruning large Llama-v2 13B model.
Llama-v2 13B Model size wikitext word ppl boolq piqa hallaswag winogrande arc easy arc challenge openbookq copa lambada openai wsc273 A VERAGE wikitext2
Dense baseline 100% 8.23 80.52% 80.52% 79.38% 72.14% 77.53% 49.23% 45.20% 90.00% 76.77% 89.38% 74.07%
LLM Surgeon (ours )90% 8.57 81.07% 79.87% 79.24% 72.38% 76.30% 49.91% 47.20% 92.00% 75.47% 89.38% 74.28%
80% 10.08 80.86% 79.00% 77.09% 70.56% 75.93% 46.76% 46.80% 90.00% 67.79% 86.45% 72.12%
70% 12.74 74.50% 76.50% 71.52% 68.67% 69.74% 40.27% 45.00% 91.00% 54.40% 83.52% 67.51%
60% 16.00 64.62% 73.01% 65.04% 65.75% 63.80% 37.12% 39.60% 90.00% 44.50% 81.32% 62.48%
50% 23.75 65.66% 68.77% 56.19% 63.22% 56.19% 31.83% 36.60% 85.00% 35.16% 77.29% 57.59%
K-OBD 90% 8.79 81.31% 79.76% 79.12% 72.22% 76.94% 47.95% 47.80% 91.00% 75.26% 88.64% 74.00%
80% 11.79 80.80% 79.16% 76.80% 70.56% 73.74% 46.93% 48.60% 88.00% 58.99% 87.55% 71.11%
70% 20.00 66.76% 74.43% 64.18% 64.96% 56.23% 36.01% 39.00% 88.00% 38.54% 79.49% 60.76%
60% 27.74 55.66% 70.24% 55.52% 60.46% 49.62% 32.68% 35.80% 80.00% 30.06% 73.63% 54.37%
50% 37.38 59.79% 66.54% 48.39% 57.46% 46.59% 30.72% 34.00% 77.00% 24.61% 69.96% 51.50%
We find that LLM Surgeon also outperforms baselines on existing Llama-v2 13B models. We stress
that these results are obtained on structured pruning of rows and columns, which are regarded the
hardest and most constrained pruning structure. Yet, we can compress Llama 13B by 20% with less
than 2% drop in downstream task performance. It also significantly outperforms the baseline for all
compression rates, both in terms of test perplexity and downstream task performance.
17

--- PAGE 18 ---
F A BLATIONS
F.1 S HOTS
Table 9: Ablation of shot counts Tfor structured LLM Surgeon compressing OPT-1.3b model.
Target size Shots T wikitext-2 PPL Shots T wikitext-2 PPL Shots T wikitext-2 PPL
90% 6 14.70 8 14.70 10 14.72
80% 12 15.14 16 15.12 20 15.08
70% 18 16.21 24 16.24 30 16.23
60% 24 18.53 32 18.45 40 18.49
50% 30 23.32 40 22.95 50 22.68
F.2 T ASK-SPECIFIC COMPRESSION
Table 10: Cross-task performance and mask
equivalences of 50% compressed OPT-125m
model using structured LLM Surgeon on language
subsets.
evaluation dataset mask equivalence (%)
target EN FR DE IT EN FR DE IT
Pretrained 27.66 22.54 24.32 27.66
EN 47.46 172.9 181.1 169.1 1.00 0.74 0.70 0.72
FR 113.4 28.44 35.02 34.90 0.74 1.00 0.87 0.90
DE 142.1 35.15 27.49 38.49 0.70 0.87 1.00 0.87
IT 123.7 31.85 33.78 30.58 0.72 0.90 0.87 1.00LLM Surgeon uses data to find a compressed
model that has the least negative impact on fi-
nal test performance. In this section, we ex-
plore the extent to which the method can use
data to compress specifically to the task at hand.
We do so by comparing test performance and
equivalences between resulting pruning masks
for different language modeling languages: En-
glish (EN/wikitext-2), French (FR) and Italian
(IT) and the German (DE). We consider 50%
unstructured compression using LLM Surgeon with correlated weight updates. For each compressed
model, we compare performance on all languages and compare the equivalences between resulting
pruning masks (details in appendix B.3), and report results in table 10. Like other methods that use
data for compression (Hassibi & Stork, 1992; Frantar & Alistarh, 2023; Wang et al., 2019), we ex-
pect to see some correlation between the data used for training and data with good test performance,
which is reflected in both test performance and masks. It is important to note that the final perfor-
mance after compression will depend on the quality of the used dataset for compression. Further, the
experiment demonstrates that the method can be used for task-specific compression tailored towards
the data used for compression and generalises to high test performance on the associated test data.
18

--- PAGE 19 ---
G O N FAIR COMPARISON
All results in this work (including the SparseGPT) were trained on Wikitext-2 for fair comparison.
To do so, we used the same dataloader and evaluation script as the official SparseGPT repo and reran
all SparseGPT results to be trained on Wikitext-2. In some cases, this resulted in better scores for the
SparseGPT baseline compared to the C4-trained results reported in the original SparseGPT paper.
Yet, we find that our method using improved curvature estimates still outperformed the baselines in
terms of final test performance.
H C OMPUTATIONAL PERFORMANCE
We report computational cost in terms of pruning time in table 11 and GPU memory in table 12.
Table 11: Time performance.
Test performance
Runtime Network Time PPL 90% PPL 80% PPL 70% PPL 60% PPL 50%
Unstructured baseline (SparseGPT) Llama-v2 7B <5m 5.49 5.58 5.71 5.94 6.51
Unstructured LLM Surgeon ( ours ) Llama-v2 7B 2d8h16m 5.13 5.20 5.36 5.66 6.08
Structured baseline (K-OBD) Llama-v2 7B 16h58m 5.48 9.14 15.43 28.03 46.64
Structured LLM Surgeon ( ours ) Llama-v2 7B 17h08m 5.25 6.18 7.83 10.39 15.38
Structured baseline (K-OBD) Llama-v2 13B 1d6h5m 4.908 6.294 10.08 13.06 16.06
Structured LLM Surgeon ( ours ) Llama-v2 13B 1d9h26m 4.692 5.286 6.207 7.245 9.428
Our method is most efficient for structured pruning, but it must be noted that engineering efforts may
further improve speed for unstructured pruning. The focus of the paper is structured pruning, on
which we achieve state-of-the-art compression rates. Importantly, compression of LLMs only needs
to happen once after which a pruned model can be deployed infinitely many times without further
cost. This motivates our method which takes longer to run but reaches better final test performance.
Table 12: Memory performance.
Network SparseGPT (baseline) Unstructured LLM-Surgeon (ours)
Llama-7B <5m / 1 GPU (32GB) 2d8h16m / 4xH100 80 GB
K-OBD (baseline) Structured LLM-Surgeon (ours)
Llama-7B 16h58m / 4xH100 80 GB 17h08m / 4xH100 80 GB
Llama-13B 1d6h5m / 8xH100 80 GB 1d9h26m / 8xH100 80 GB
We argue that differences in the performance and the runtime of pruning methods can largely be
attributed to underlying assumptions on correlations between weights. Notably, algorithms that
consider few correlations, sometimes to the extent of completely disregarding all gradient informa-
tion, can result in very fast pruning algorithms for unstructured and semi-structured pruning but are
often not flexible enough to perform structured pruning of rows and columns. Examples of such
lightweight algorithms for LLMs are (Sun et al., 2023) and SparseGPT (Frantar & Alistarh, 2023),
as can also be observed from table 11. Our approach makes less strong assumptions on the cur-
vature of the loss and as a result outperforms all baselines on all unstructured, semi-structured and
structured pruning. Further, the improved curvature is also eligible for dynamic allocation of weight
removal and improved correlated weight updates. In practice, we always recommend using our
method for structured pruning. For unstructured and semi-structured pruning, we note an important
trade-off between the desired final test accuracy and the available computational budget. Here, our
proposed method can achieve the highest final model performance but requires more computational
resources and takes longer to run. It should be noted that pruning only needs to happen once after
which a model can be deployed infinitely many times this time, which dependent on the available
computational resources can also legitimise spending additional pruning time even if this is much
higher compared to other algorithms in relative terms. In absolute terms, the use of multiple large
GPUs is common practice in the field of large language models and many more GPUs are typically
used to train and deploy large language models. Moreover, the curvature approximation is naively
amenable to data parallelism in case further speed-ups or larger models are required. We hope this
provides context and emphasises the trade-off between performance and compute in practice.
19

--- PAGE 20 ---
I E XTENDING CURVATURE ESTIMATES
Instead of using a single Kronecker product, we might consider improving the approximation
through a sum of multiple Kronecker factors:
F≈eF=G1⊗A1+G2⊗A2 (30)
This last appendix deals with the question how one may computationally find such approximations
and how to utilise them in the neural network pruning framework.
I.1 N EAREST KRONECKER PRODUCT OR SUM OF KRONECKER PRODUCTS
Instead of assuming independence of activations and derivatives as in section 3.1, following the
classic KFAC of (Martens & Grosse, 2015), we might want to find the nearest Kronecker product
approximation F≈eG⊗eAthat is closest to the Fisher in terms of the Frobenius norm:
eGl,eAl= arg min
Gl,Al||Fl−Gl⊗Al||F (31)
Finding the nearest sum of Kronecker factors can be rephrased as a classic eigenvalue problem of
finding the nearest rank-1 matrix. Golub & Van Loan (2013).
||F−˜G⊗˜A||F≡ ||R (F)−vec(eG)vec(eA)T||F (32)
Power method and deflation After considering the reshaping, we can use power iterations to
solve for and find the nearest Kronecker factors G1,A1=solve(F).
Find with power iterations:
eG1,eA1=solve(F) = arg min
G,A||F−G⊗A||FDeflation:
eGr,eAr=solve(F−Xr−1
r′=1(eGr′⊗eAr′))
A more extensive description of the power method solve (·)can be found in algorithm 4. At the start
of the algorithm, we initialise power iterations as vector with one’s 1= [1 1 . . . 1]. After each
shot we can initialise the vector as the final estimate found during the previous shot.
Algorithm 4 Kronecker power method. Finds eG,eAnearest Kronecker product ||F−eG⊗eA||F.
Input: Initialise eg0=1,ea0=1(or using estimates of previous shot).
Input: Set iterations I(orI=1if using estimates from previous shot)
Output: eG,eA
foriteration iin [1, 2, . . . , I]do
Compute: egi=R(eF)eai−1
||R(eF)eai−1||2, withR(eF)eai−1=1
NPN
n=1aT
neAi−1anvec(gngT
n)
Compute: eai=R(eF)Tegi
||R(eF)Tegi||2, withR(eF)Tegi=1
NPN
n=1gT
neGignvec(anaT
n)
Compute: σi=||eai||2
end for
Return: eG=√
σimat(eg),eA=√
σimat(ea).
True Fisher
rmse: 0.13
rmse diag: 0.19Classic KFAC (IAD)
rmse: 0.12
rmse diag: 0.15Nearest KFAC RK=1
rmse: 0.11
rmse diag: 0.15Nearest KFAC RK=2
rmse: 0.09
rmse diag: 0.14Nearest KFAC RK=3
rmse: 0.04
rmse diag: 0.14Nearest KFAC RK=9
Figure 5: Example illustration of nearest Kronecker factor approximations eF≈PRK
r=1Gi⊗Ai,
compared to classical KFAC with the IAD assumption. Larger RKyields better approximations to
the true Fisher Ffor larger RK, as measured by the root mean squared error (rmse).
20

--- PAGE 21 ---
I.2 E XTENDED CURVATURE APPROXIMATIONS
For classic KFAC with IAD or RK=1nearest Kronecker approximations of the form eF=G⊗A,
the inverse simply becomes (G⊗A)−1=G−1⊗A−1. Unfortunately, we can not use this famous
inverse identity for sum of Kronecker factors, which is why we fall back on eigendecompositions
G=E1S1ET
1andA=E2S2ET
2, allowing us to decompose the Fisher into:
eF=K1S1KT
1⊗K2S2KT
2= (K1⊗K2)(I⊗I+S1⊗S2)(KT
1⊗KT
2) (33)
where specific K1andK2can be found in App. B of Martens & Grosse (2015), which we closely
followed in our derivations. Because K1andK2are orthogonal and S1andS2diagonal, the inverse
Fisher becomes:
eF−1= (K1⊗K2)(I⊗I+S1⊗S2)−1(KT
1⊗KT
2) (34)
In the context of neural network training, the problem gets slightly harder since we want to incre-
mentally construct estimates eGiandeAifrom individual samples al,n,gl,nthat make up F, without
having to simultaneously store more than a single or batch of input activations al,nor output gradi-
entsgl,nin memory. Although this online Kronecker-product principal component analysis problem
largely remains an open research problem, we our approach closely follows the recent work by (Ko-
roko et al., 2022) that uses similar approximations in the context of optimisation. A sum of multiple
RK>1Kronecker factors will yield closer approximations, but also linearly increase memory re-
quirements with higher RKand makes inverting F−1considerably more difficult.
Formulas to compute cost and weight updates. For sum of Kronecker factors, we find that the
constrained optimization solution of for costs ∆Leq. (7) and weight updates ∆θeq. (8) become the
following inner-product and matrix-vector product:
Lk=1
2⟨θ∗,Uθ∗⟩= (θ∗)TU(θ∗)∈R (35)
∆θ=eF−1ET
Ku=K1
KT
1UK2⊘
11T+s1sT
2
KT
2∈RRC(36)
with at the heart of it all a matrix U= [EKF−1ET
K]−1that captures correlations between weights:
U=
EK
K1⊗K2
I⊗I+S1⊗S2−1
KT
1⊗KT
2
ET
K−1
(37)
where (I⊗I+S1⊗S2)is diagonal and the inverse can thus be computed element-wise. The
remaining inverse is of size K×K, forKcorrelated weights.
Note on sum of Kronecker factors Experimentally, we did not find a benefit in performance
when using a sum of two nearest Kronecker factor approximation, or found it too slow. Therefore,
we focus in the main text on LLM Surgeon with fast single Kronecker product KFAC approximation
to approximate the loss landsscape curvature. Nevertheless, we choose to include this appendix as
we believe could prove useful in other contexts or inspire future work that aim to further improve
the quality of curvature approximations.
21

--- PAGE 22 ---
J C ODE
Code is available at: https://github.com/Qualcomm-AI-research/llm-surgeon.
22
