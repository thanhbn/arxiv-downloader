#!/usr/bin/env python3
"""
Script d·ªãch c√°c file paper t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát
S·ª≠ d·ª•ng OpenAI API ƒë·ªÉ d·ªãch thu·∫≠t chuy√™n nghi·ªáp
"""

import os
import glob
import openai
from pathlib import Path
import time
import json
import logging

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PaperTranslator:
    def __init__(self):
        # L·∫•y API key t·ª´ environment
        self.client = openai.OpenAI()
        self.base_dir = Path(".")
        os.chdir(self.base_dir)  # Change working directory to arxiv-downloader
        
        # Prompt d·ªãch thu·∫≠t chuy√™n nghi·ªáp
        self.translation_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω d·ªãch thu·∫≠t chuy√™n nghi·ªáp, c√≥ kh·∫£ nƒÉng d·ªãch vƒÉn b·∫£n h·ªçc thu·∫≠t v√† k·ªπ thu·∫≠t m·ªôt c√°ch ch√≠nh x√°c.

Nhi·ªám v·ª• c·ªßa b·∫°n l√† d·ªãch to√†n b·ªô n·ªôi dung sau sang ti·∫øng Vi·ªát.

T·∫≠p trung v√†o vi·ªác:
1. D·ªãch thu·∫≠t ch√≠nh x√°c c√°c thu·∫≠t ng·ªØ AI/ML v√† khoa h·ªçc m√°y t√≠nh.
2. Gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng c·ªßa vƒÉn b·∫£n g·ªëc (URLs arXiv, DOI, v.v.).
3. S·ª≠ d·ª•ng vƒÉn phong h·ªçc thu·∫≠t, trang tr·ªçng.
4. ƒê·∫£m b·∫£o b·∫£n d·ªãch m∆∞·ª£t m√† v√† d·ªÖ hi·ªÉu trong ng·ªØ c·∫£nh ti·∫øng Vi·ªát.
5. Gi·ªØ nguy√™n c√°c t·ª´ ti·∫øng Anh trong ngo·∫∑c ƒë∆°n sau thu·∫≠t ng·ªØ ti·∫øng Vi·ªát n·∫øu c·∫ßn thi·∫øt.
6. ƒê·∫∑c bi·ªát ch√∫ √Ω d·ªãch ƒë√∫ng c√°c ch·ªß ƒë·ªÅ nghi√™n c·ª©u AI nh∆∞ RAG, Chain-of-Thought, Multimodal, v.v.

VƒÉn b·∫£n c·∫ßn d·ªãch:
"""

    def find_untranslated_files(self):
        """T√¨m c√°c file .txt ch∆∞a c√≥ b·∫£n d·ªãch _vi.txt"""
        # T√¨m trong th∆∞ m·ª•c hi·ªán t·∫°i v√† c√°c collection folders
        all_txt_files = []
        
        # T√¨m files .txt trong th∆∞ m·ª•c g·ªëc
        root_txt_files = glob.glob("*.txt")
        all_txt_files.extend(root_txt_files)
        
        # T√¨m files arxiv_links.txt trong c√°c collection folders
        collection_dirs = [d for d in os.listdir('.') if os.path.isdir(d) and not d.startswith('.')]
        for collection_dir in collection_dirs:
            links_file = os.path.join(collection_dir, "arxiv_links.txt")
            if os.path.exists(links_file):
                all_txt_files.append(links_file)
        
        untranslated = []
        for txt_file in all_txt_files:
            if not txt_file.endswith("_vi.txt"):
                vi_file = txt_file.replace(".txt", "_vi.txt")
                if not os.path.exists(vi_file):
                    untranslated.append(txt_file)
        
        return untranslated

    def read_file_content(self, file_path):
        """ƒê·ªçc n·ªôi dung file v·ªõi encoding UTF-8"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # Th·ª≠ encoding kh√°c n·∫øu UTF-8 kh√¥ng ho·∫°t ƒë·ªông
            with open(file_path, 'r', encoding='latin-1') as f:
                return f.read()

    def translate_text_chunks(self, text, max_chunk_size=6000):
        """Chia text th√†nh chunks nh·ªè ƒë·ªÉ d·ªãch v·ªõi chi·∫øn l∆∞·ª£c th√¥ng minh"""
        chunks = []
        current_chunk = ""
        
        paragraphs = text.split('\n\n')
        threshold_size = int(max_chunk_size * 1.5)  # 150% of max chunk size
        
        i = 0
        while i < len(paragraphs):
            paragraph = paragraphs[i]
            
            # Calculate remaining text size from current paragraph onwards
            remaining_text = '\n\n'.join(paragraphs[i:])
            current_plus_paragraph = current_chunk + paragraph
            
            # Check if adding this paragraph exceeds max_chunk_size
            if len(current_plus_paragraph) > max_chunk_size:
                if current_chunk:
                    # Smart decision: if remaining text (including current paragraph) is < 150% of max_chunk_size,
                    # include everything in current chunk instead of creating a small leftover chunk
                    if len(current_chunk + remaining_text) <= threshold_size:
                        current_chunk += remaining_text
                        chunks.append(current_chunk.strip())
                        break  # All remaining text processed
                    else:
                        chunks.append(current_chunk.strip())
                        current_chunk = paragraph + "\n\n"
                else:
                    # Paragraph qu√° d√†i, chia nh·ªè h∆°n
                    sentences = paragraph.split('. ')
                    j = 0
                    while j < len(sentences):
                        sentence = sentences[j]
                        remaining_sentences = '. '.join(sentences[j:])
                        current_plus_sentence = current_chunk + sentence
                        
                        if len(current_plus_sentence) > max_chunk_size:
                            if current_chunk:
                                # Smart decision for sentences too
                                if len(current_chunk + remaining_sentences) <= threshold_size:
                                    current_chunk += remaining_sentences
                                    j = len(sentences)  # Exit sentence loop
                                else:
                                    chunks.append(current_chunk.strip())
                                    current_chunk = sentence + ". "
                            else:
                                chunks.append(sentence)
                                current_chunk = ""
                        else:
                            current_chunk += sentence + ". "
                        j += 1
            else:
                current_chunk += paragraph + "\n\n"
            
            i += 1
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks

    def translate_chunk(self, chunk):
        """D·ªãch m·ªôt chunk text"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": self.translation_prompt},
                    {"role": "user", "content": chunk}
                ],
                temperature=0.1,
                max_tokens=50000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"L·ªói khi d·ªãch chunk: {e}")
            return f"[L·ªñI D·ªäCH] {chunk}"

    def translate_file(self, input_file):
        """D·ªãch m·ªôt file"""
        logger.info(f"B·∫Øt ƒë·∫ßu d·ªãch file: {input_file}")
        
        # ƒê·ªçc n·ªôi dung file
        content = self.read_file_content(input_file)
        
        # Chia th√†nh chunks
        chunks = self.translate_text_chunks(content)
        logger.info(f"Chia th√†nh {len(chunks)} chunks")
        
        # D·ªãch t·ª´ng chunk
        translated_chunks = []
        for i, chunk in enumerate(chunks):
            logger.info(f"D·ªãch chunk {i+1}/{len(chunks)}")
            translated_chunk = self.translate_chunk(chunk)
            translated_chunks.append(translated_chunk)
            
            # T·∫°m d·ª´ng ƒë·ªÉ tr√°nh rate limit
            time.sleep(1)
        
        # Gh√©p l·∫°i th√†nh vƒÉn b·∫£n ho√†n ch·ªânh
        translated_content = "\n\n".join(translated_chunks)
        
        # L∆∞u file d·ªãch
        output_file = input_file.replace(".txt", "_vi.txt")
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(translated_content)
        
        logger.info(f"ƒê√£ l∆∞u b·∫£n d·ªãch: {output_file}")
        return output_file

    def translate_all_files(self):
        """D·ªãch t·∫•t c·∫£ c√°c file ch∆∞a c√≥ b·∫£n d·ªãch"""
        untranslated_files = self.find_untranslated_files()
        
        if not untranslated_files:
            logger.info("Kh√¥ng c√≥ file n√†o c·∫ßn d·ªãch!")
            return
        
        logger.info(f"T√¨m th·∫•y {len(untranslated_files)} file c·∫ßn d·ªãch")
        
        for i, file_path in enumerate(untranslated_files):
            try:
                logger.info(f"Ti·∫øn ƒë·ªô: {i+1}/{len(untranslated_files)}")
                self.translate_file(file_path)
                logger.info(f"Ho√†n th√†nh d·ªãch file: {file_path}")
                
                # T·∫°m d·ª´ng gi·ªØa c√°c file
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"L·ªói khi d·ªãch file {file_path}: {e}")
                continue
        
        logger.info("Ho√†n th√†nh d·ªãch t·∫•t c·∫£ c√°c file!")

def main():
    """H√†m ch√≠nh"""
    # Ki·ªÉm tra API key
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå Vui l√≤ng thi·∫øt l·∫≠p OPENAI_API_KEY trong environment variables")
        return
    
    translator = PaperTranslator()
    
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh d·ªãch c√°c file paper...")
    translator.translate_all_files()
    print("‚úÖ Ho√†n th√†nh!")

if __name__ == "__main__":
    main()