# 2208.01313.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/normalization/2208.01313.pdf
# Kích thước tệp: 7626159 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Chuẩn hóa Thống nhất để Tăng tốc và Ổn định
Transformers
Qiming Yang
yangqiming5@hikvision.com
Viện Nghiên cứu Hikvision
Hàng Châu, Trung QuốcKai Zhang
zhangkai23@hikvision.com
Viện Nghiên cứu Hikvision
Hàng Châu, Trung QuốcChaoxiang Lan
lanchaoxiang@hikvision.com
Viện Nghiên cứu Hikvision
Hàng Châu, Trung Quốc
Zhi Yang
yanzhi13@hikvision.com
Viện Nghiên cứu Hikvision
Hàng Châu, Trung QuốcZheyang Li
lizheyang@hikvision.com
Viện Nghiên cứu Hikvision &
Đại học Chiết Giang
Hàng Châu, Trung QuốcWenming Tan
tanwenming@hikvision.com
Viện Nghiên cứu Hikvision
Hàng Châu, Trung Quốc
Jun Xiao
junx@cs.zju.edu.cn
Đại học Chiết Giang
Hàng Châu, Trung QuốcShiliang Pu∗
pushiliang.hri@hikvision.com
Viện Nghiên cứu Hikvision
Hàng Châu, Trung Quốc

TÓM TẮT
Những kết quả vững chắc từ Transformers đã khiến chúng trở thành kiến trúc thịnh hành trong nhiều tác vụ ngôn ngữ tự nhiên và thị giác. Như một thành phần mặc định trong Transformers, Chuẩn hóa Lớp (LN) chuẩn hóa các kích hoạt trong mỗi token để tăng cường độ bền vững. Tuy nhiên, LN yêu cầu tính toán thống kê ngay lập tức trong suy luận cũng như các phép chia và căn bậc hai, dẫn đến việc không hiệu quả trên phần cứng. Hơn nữa, việc thay thế LN bằng các sơ đồ chuẩn hóa hiệu quả về phần cứng khác (ví dụ: Chuẩn hóa Batch) dẫn đến hiệu suất kém hơn, thậm chí sụp đổ trong quá trình huấn luyện. Chúng tôi phát hiện rằng tình thế khó xử này được gây ra bởi các hành vi bất thường của thống kê kích hoạt, bao gồm những biến động lớn qua các lần lặp và các ngoại lệ cực đoan trên các lớp. Để giải quyết những vấn đề này, chúng tôi đề xuất Chuẩn hóa Thống nhất (UN), có thể tăng tốc suy luận bằng cách được hợp nhất với các phép toán tuyến tính khác và đạt được hiệu suất tương đương với LN. UN cố gắng tăng cường hiệu suất bằng cách hiệu chỉnh thống kê kích hoạt và gradient với một chiến lược làm mượt biến động được thiết kế riêng. Trong khi đó, một chiến lược lọc ngoại lệ thích ứng được áp dụng để tránh sụp đổ trong quá trình huấn luyện có hiệu quả được chứng minh lý thuyết và xác minh thực nghiệm trong bài báo này. Chúng tôi chứng minh rằng UN có thể là một thay thế hiệu quả và dễ dàng thay thế cho LN bằng cách tiến hành các thí nghiệm mở rộng trên các tác vụ ngôn ngữ và thị giác. Bên cạnh đó, chúng tôi đánh giá hiệu quả của phương pháp trên GPU. Transformers được trang bị UN hưởng lợi khoảng 31% tăng tốc suy luận
∗Tác giả liên hệ.
Quyền tạo ra các bản sao kỹ thuật số hoặc cứng của toàn bộ hoặc một phần công trình này cho mục đích sử dụng cá nhân hoặc lớp học được cấp miễn phí với điều kiện các bản sao không được tạo ra hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của công trình này thuộc sở hữu của những người khác ngoài ACM phải được tôn trọng. Tóm tắt có ghi công nguồn được cho phép. Để sao chép khác, hoặc tái xuất bản, để đăng trên máy chủ hoặc để phân phối lại vào danh sách, yêu cầu quyền cụ thể trước và/hoặc một khoản phí. Yêu cầu quyền từ permissions@acm.org.
MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha
©2022 Hiệp hội Máy tính.
ACM ISBN 978-1-4503-9203-7/22/10. . . $15.00
https://doi.org/10.1145/3503161.3547860tăng tốc và gần 18% giảm bộ nhớ. Mã nguồn sẽ được phát hành tại https://github.com/hikvision-research/Unified-Normalization.

KHÁI NIỆM CCS
•Phương pháp tính toán →Tác vụ thị giác máy tính ;Dịch máy .

TỪ KHÓA
mạng nơ-ron, chuẩn hóa, transformers

Định dạng Tham chiếu ACM:
Qiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming
Tan, Jun Xiao, và Shiliang Pu. 2022. Chuẩn hóa Thống nhất để Tăng tốc và Ổn định Transformers. Trong Proceedings of the 30th ACM International Conference on Multimedia (MM '22), 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha. ACM, New York, NY, USA, 11 trang. https://doi.org/10.1145/3503161.3547860

1 GIỚI THIỆU
Transformers [ 38] ban đầu được giới thiệu cho các tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP) [ 10,28]. Vì Transformers đưa ra ít giả định về sự thiên vị cấu trúc của dữ liệu đầu vào, những kiến trúc này có thể được áp dụng một cách linh hoạt và toàn diện trong các tình huống khác, như các tác vụ đa phương thức và giọng nói [ 20,40]. Các mô-đun cơ bản của Transformers là chú ý multi-head tự chú ý (MHSA) và mạng feed-forward (FFN) có thể xếp chồng lên nhau để cho phép nắm bắt các phụ thuộc dài hạn giữa các token. Trong những mô-đun cơ bản này, Chuẩn hóa Lớp (LN) [ 2] được chọn làm thành phần mặc định giải phóng quá trình huấn luyện khỏi sự phụ thuộc nặng nề vào các mẫu mini-batch để xử lý đầu vào có độ dài thay đổi. Tuy nhiên, LN yêu cầu chi phí tính toán và bộ nhớ bổ sung trong quá trình suy luận vì thống kê ngay lập tức, cũng như các phép chia và căn bậc hai. Do đó LN không hiệu quả và khó đáp ứng nhu cầu công nghiệp [ 34,45]. Tuy nhiên, việc thay thế LN bằng Chuẩn hóa Batch (BN) [ 19] dẫn đến hiệu suất kém hơn trong các tác vụ NLP [ 35]. Shen et al. [35] cho thấy rằng những biến động lớn trong Transformers trong arXiv:2208.01313v1  [cs.CV]  2 Aug 2022

--- TRANG 2 ---
MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha Qiming Yang et al.

Hình 1: So sánh hiệu suất trên Transformer (IWSLT14), T2T-ViT-14 (ImageNet), và Swin-T (COCO) trong quá trình huấn luyện. Các phương pháp ngoại tuyến có thể được hợp nhất vào các phép toán tuyến tính khác trong suy luận đều được vẽ bằng đường liền nét. PN ban đầu đi kèm với một lớp layer-scale, được loại bỏ trong PN∗.

thống kê kích hoạt dẫn đến hiệu suất kém. Gần đây, Transformers được lan rộng một cách rộng rãi đến các tác vụ Thị giác Máy tính (CV) đã dẫn đến một loạt những đột phá trong phân loại hình ảnh, phát hiện đối tượng, phân đoạn thể hiện, v.v., còn được gọi là Vision Transformers (ViTs) [ 4,7,12–14,26,36,48,49]. LN được kế thừa trực tiếp từ Transformer gốc như một thành phần thiết yếu trong những ViTs này mặc dù có đầu vào có độ dài cố định. Tương tự như các tác vụ NLP, sự suy giảm hiệu suất đáng kể (thậm chí sụp đổ trong quá trình huấn luyện) cũng sẽ được kích hoạt bằng cách thay thế LN bằng BN trong Transformers trong các tác vụ CV [ 6,33,46]. Shao et al. [33] cho rằng BN có hại cho ViTs dẫn đến suy giảm hiệu suất. Trong các nghiên cứu trước [ 6,46], các tác giả khẳng định rằng việc thay thế tất cả LN bằng BN trong ViTs dẫn đến các vấn đề hội tụ. Vì việc thay thế ngây thơ cho LN dẫn đến hiệu suất kém hơn và không ổn định, việc triển khai Transformers vẫn gặp khó khăn từ việc tính toán thống kê ngay lập tức.

Có hai phương pháp chính để cải thiện hiệu quả phần cứng của chuẩn hóa trong Transformers. 1) Đơn giản hóa việc tính toán thống kê trực tuyến [ 35,50] và loại bỏ các phép toán không hiệu quả (ví dụ: căn bậc hai) [ 23]. Mặc dù hiệu suất được duy trì, việc tính toán động cho thống kê trực tuyến vẫn tồn tại. 2) Loại bỏ việc tính toán cho thống kê trực tuyến sử dụng thống kê cố định trong quá trình suy luận như một phương pháp ngoại tuyến, chẳng hạn như BN [ 19]. Theo cách này, suy luận có thể được tăng tốc bằng cách tránh việc tính toán thống kê dư thừa, loại bỏ các phép chia và căn bậc hai. Vì không có bữa trưa miễn phí, sự sụt giảm hiệu suất đáng kể và các vấn đề hội tụ được báo cáo trong những nghiên cứu này [ 35,45,46]. MABN [ 45] và PN [ 35] sử dụng các chiến lược trung bình động để giảm thiểu những biến động trong Transformers trong các tác vụ NLP. Tuy nhiên, những phương pháp được đề cập này chỉ dành riêng cho tác vụ mà hiệu suất kém hơn và tính không ổn định vẫn tồn tại trong ViTs.

Để giải quyết các vấn đề trên, chúng tôi phân tích các hành vi bất thường của thống kê trong Transformers. Chúng tôi điều tra thống kê kích hoạt dưới các chiến lược trung bình động trong quá trình huấn luyện Transformers. Chúng tôi khám phá rằng những biến động của thống kê kích hoạt mạnh mẽ hơn so với thống kê gradient trong quá trình huấn luyện. Hơn nữa, chúng tôi thấy rằng phạm vi của thống kê kích hoạt không phụ thuộc vào tác vụ tiếp tục tăng theo cả độ sâu và tiến trình huấn luyện, trong đó nguy cơ ngoại lệ phát sinh. Theo nghĩa này, các ngoại lệ cực đoan gần như không thể tránh khỏi và liên tục làm xấu đi tính nhất quán giữa thống kê kích hoạt và gradient. Những quan sát này cho thấy rằng hiệu suất kém hơn và tính không ổn định (được hiển thị trong Hình 1) rất có khả năng sôi sục xuống từ các hành vi bất thường của thống kê kích hoạt trong Transformers.

Trong bài báo này, chúng tôi nhắm đến việc thay thế LN bằng một phương pháp ngoại tuyến để thúc đẩy các ứng dụng cho Transformers trong các tác vụ ngôn ngữ và thị giác. Chúng tôi đề xuất Chuẩn hóa Thống nhất (UN) để tăng tốc suy luận trong Transformers và đạt được hiệu suất tương đương với LN. Cụ thể, chúng tôi thiết kế một chiến lược làm mượt biến động được thiết kế riêng để đối phó với những biến động có mức độ khác nhau trong thống kê kích hoạt và gradient. Đồng thời, một chiến lược lọc ngoại lệ thích ứng được giới thiệu để đảm bảo hội tụ ổn định, nơi tác động của các ngoại lệ có thể được chứng minh là được giảm đáng kể cả về lý thuyết và thí nghiệm. Các thí nghiệm mở rộng chứng minh hiệu quả của UN. Tóm lại, những đóng góp của chúng tôi như sau:

•Chúng tôi phân tích các hành vi bất thường của thống kê kích hoạt trong Transformers và tìm thấy những biến động lớn và ngoại lệ cực đoan chịu trách nhiệm cho hiệu suất kém hơn và tính không ổn định.

•Một chiến lược làm mượt biến động được thiết kế riêng được thiết kế để hiệu chỉnh thống kê kích hoạt và gradient và tăng cường hiệu suất.

•Một chiến lược lọc ngoại lệ thích ứng được giới thiệu để giảm tác động của các ngoại lệ cực đoan dựa trên phân tích lý thuyết.

•Các đánh giá mở rộng trong dịch máy nơ-ron, phân loại hình ảnh, phát hiện đối tượng, và phân đoạn thể hiện minh họa sự ưu việt của phương pháp chúng tôi, có khả năng trở thành một thay thế dễ dàng thay thế cho LN trong Transformers. Hơn nữa, chúng tôi cho thấy rằng Transformers được trang bị UN đạt được gần 18% giảm bộ nhớ và hơn 31% tăng tốc trong suy luận trên GPU.

2 NGHIÊN CỨU LIÊN QUAN

2.1 Transformers và Vision Transformers
Transformers [ 38] ban đầu cho thấy khả năng đáng ngạc nhiên trong mô hình hóa chuỗi và dịch máy nơ-ron. Nhờ tính linh hoạt cao, Transformers [ 3,10,15,47] đã trở thành những kiến trúc thống trị gần đây nhất trên nhiều tác vụ NLP và tác vụ giọng nói. Năm 2020, Carion et al. [4] đề xuất bộ phát hiện đầu cuối dựa trên Transformer đầu tiên DETR. Sau đó, ViT [ 13] được đề xuất như Transformer thuần túy đầu tiên trong các tác vụ CV. Những năm tiếp theo đã chứng kiến sự phát triển bùng nổ của Transformers trong các tác vụ CV. T2T [ 49] giới thiệu mô-đun token-to-token kết hợp các token liền kề ở giai đoạn đầu để mô hình hóa thông tin cục bộ. Swin [ 26] và Swin V2 [ 25] đã áp dụng chú ý dựa trên cửa sổ để giảm chi phí tính toán trong MHSA và đạt được hiệu suất tối ưu.

--- TRANG 3 ---
Chuẩn hóa Thống nhất MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha

Hình 2: Các phương pháp chuẩn hóa. Mỗi hình phụ hiển thị một tensor bản đồ đặc trưng, trong đó B là trục batch, N là trục số token (hoặc trục độ dài chuỗi), và C là trục kênh (còn được gọi là kích thước nhúng).

Gần đây nhất, Ding et al. [11] khám phá sự chú ý trong cả token không gian và kênh để đề xuất một backbone mạnh mẽ DaViT cho các tác vụ thị giác. Trong những Transformers và Vision Transformers được đề cập ở trên, LN là lựa chọn được ưa chuộng cho chuẩn hóa.

Ngoài ra, một số nghiên cứu [ 7,14,18] ban đầu xây dựng Transformers với BN, trong khi những nghiên cứu này cần thiết kế tinh tế về các phép toán tích chập để ổn định quá trình huấn luyện đã có những thay đổi đáng kể so với ViT gốc [13].

2.2 Các Phương pháp Chuẩn hóa
Chuẩn hóa được sử dụng rộng rãi để ổn định quá trình huấn luyện và tăng cường hiệu suất trong mạng nơ-ron sâu [ 17]. Như minh họa trong Hình 2, các phương pháp chuẩn hóa liên quan có thể được phân loại thành các phương pháp trực tuyến và ngoại tuyến tùy theo liệu thống kê suy luận có thể được hợp nhất hay không.

2.2.1 Các Phương pháp Trực tuyến. Các phương pháp trực tuyến yêu cầu tính toán thống kê ngay lập tức trong quá trình huấn luyện cũng như suy luận. IN [ 37], GN [ 41], và LN [ 2] là những phương pháp trực tuyến tiêu biểu tính toán thống kê trong các chiều khác nhau như được hiển thị trong Hình 2. Switchable Normalization [ 27] học cách chuyển đổi giữa các loại chuẩn hóa khác nhau bằng cách học trọng số quan trọng của chúng. Người ta tin rằng LN được tùy chỉnh cho các mẫu NLP có độ dài thay đổi [ 35]. Với sự gia tăng của Vision Transformers (ViTs) [ 13,26,39,49], LN cũng đã trở thành lựa chọn được ưa chuộng cho các tác vụ CV. Gần đây, DTN [ 33] khai thác mối liên kết trong các token liền kề để cải thiện hiệu suất của LN trong ViTs. Để làm cho LN hiệu quả hơn về phần cứng, một số nghiên cứu [ 23,44,50] cố gắng giảm chi phí tính toán trong LN. Zhang et al. [50] đề xuất một phương pháp đơn giản hơn RMSNorm mà tỷ lệ đầu vào bằng căn bậc hai trung bình. Tuy nhiên, việc tính toán động không hiệu quả cho thống kê trực tuyến vẫn không được loại bỏ về cơ bản.

2.2.2 Các Phương pháp Ngoại tuyến. Các phương pháp ngoại tuyến sử dụng thống kê suy luận ước tính có thể được đóng băng cho các đầu vào tùy ý. Chỉ cần một phép cộng và nhân theo điểm trong quá trình suy luận cho phép hợp nhất các phương pháp ngoại tuyến với các phép toán tuyến tính liền kề. Theo cách này, các phương pháp ngoại tuyến có thể được loại bỏ hoàn toàn khỏi mô hình và đạt được suy luận hiệu quả [ 45,46]. Tuy nhiên, một khi những phương pháp này hợp tác với Transformers, những biến động lớn qua các lần lặp sẽ dẫn đến suy giảm hiệu suất và thậm chí sụp đổ trong quá trình huấn luyện [ 6,33,35,46]. Yao et al. [46] tìm thấy Transformers được huấn luyện với

Thuật toán 1 Hợp nhất Chuẩn hóa
Đầu vào:𝛾,𝛽,𝜇,𝜎2∈R𝐶//trong Phương trình 1
W∈R𝐶out×𝐶,𝑏∈R𝐶out//tham số trong lớp tiếp theo
Đầu ra: W′∈R𝐶out×𝐶,𝑏′∈R𝐶out//tham số đã hợp nhất
1:˜𝛾=𝛾/𝜎
2:˜𝛽=𝛽−˜𝛾·𝜇
3:𝑏′=𝑏+W×˜𝛽//𝑦=W(𝑁𝑜𝑟𝑚(𝑥))+𝑏tương đương với
4:W′=W·(1𝐶out×˜𝛾𝑇) // 𝑦=W′𝑥+𝑏′

BN rất không ổn định và gặp sự cố không đều. Chen et al. [6] cố gắng thay thế một phần LN bằng BN trong FFN để ổn định quá trình huấn luyện Transformers. Để giảm thiểu tác động của những biến động lớn, MABN [ 45] tận dụng thống kê trung bình động theo số mũ trong thống kê kích hoạt, và tương ứng sử dụng thống kê trung bình động đơn giản trong thống kê gradient để ước tính gradient. Tương tự, Shen et al. [35] đề xuất PN∗sử dụng thống kê trung bình động theo số mũ trong cả thống kê kích hoạt và gradient. Các nghiên cứu trước đó nhằm cải thiện hiệu quả của LN trong Transformers nhưng vẫn gặp phải hiệu suất kém hơn và tính không ổn định. Do đó, việc thiết kế một phương pháp hiệu quả và mạnh mẽ hơn là có giá trị cho cộng đồng.

3 PHƯƠNG PHÁP
Trong phần này, chúng tôi mô tả quá trình thiết kế Chuẩn hóa Thống nhất (UN). Đầu tiên, chúng tôi phát triển một khung thống nhất để tận dụng các phương pháp ngoại tuyến. Dựa trên khung này, tiếp theo chúng tôi áp dụng một chiến lược làm mượt biến động được thiết kế riêng để giảm thiểu những biến động và một chiến lược lọc ngoại lệ thích ứng để ổn định quá trình huấn luyện.

3.1 Khung Thống nhất
Chúng tôi phát triển một khung thống nhất để áp dụng các phương pháp ngoại tuyến trong Transformers. Trong quy trình này, thống kê suy luận được cố định để chúng có thể được hợp nhất với các phép toán tuyến tính khác để tăng tốc.

Đối với một lớp chuẩn hóa, gọi X∈R𝐵×𝐶và Y∈R𝐵×𝐶biểu thị đầu vào và đầu ra, trong đó 𝐵là kích thước batch và 𝐶cho biết số kênh. Lưu ý rằng số token 𝑁, có thể được nén vào 𝐵, được bỏ qua trong phần này để rõ ràng. Đối với đầu vào tùy ý trong suy luận, tất cả các phương pháp ngoại tuyến thực hiện theo cách thống nhất

Y=𝛾·X−𝜇√
𝜎2+𝜖+𝛽. (1)

Ở đây,𝜖là một hằng số nhỏ, và 𝛾,𝛽∈R𝐶là các tham số có thể học. Thống kê suy luận 𝜇,𝜎2∈R𝐶được ước tính trong quá trình huấn luyện và độc lập với đầu vào. Vì thống kê và tham số được cố định trong quá trình suy luận, chuẩn hóa ngoại tuyến có thể được hợp nhất. Mã giả để hợp nhất chuẩn hóa ngoại tuyến với phép toán tuyến tính liền kề có thể được tìm thấy trong Thuật toán 1. Ngược lại, LN yêu cầu tính toán cho thống kê ngay lập tức 𝜇𝐿𝑁=𝜇𝐿𝑁(X),𝜎2
𝐿𝑁=𝜎2
𝐿𝑁(X) tiêu tốn thời gian tính toán bổ sung.

Trong lan truyền tiến của quá trình huấn luyện, quy trình chuẩn hóa được hiển thị như sau,

Z𝑡=X𝑡−^𝜇𝑡√︃
^𝜎2
𝑡+𝜖, (2)

Y𝑡=𝛾·Z𝑡+𝛽. (3)

--- TRANG 4 ---
MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha Qiming Yang et al.

Gọi Z𝑡biểu thị phiên bản được chuẩn hóa thay cho đầu vào X𝑡tại lần lặp 𝑡. Thống kê huấn luyện để chuẩn hóa được đánh dấu là ^𝜇𝑡và^𝜎2
𝑡, được cho bởi

^𝜇𝑡=Θ𝜇(𝜇𝑡,···,𝜇𝑡−𝑀+1), (4)
^𝜎2
𝑡=Θ𝜎2(𝜎2
𝑡,···,𝜎2
𝑡−𝑀+1). (5)

Ở đây,𝜇𝑡,···,𝜇𝑡−𝑀+1và𝜎2
𝑡,···,𝜎2
𝑡−𝑀+1là các chuỗi thống kê được ghi lại từ 𝑀lần lặp gần đây. Chúng tôi xem xét 𝜇𝑡và𝜎2
𝑡là thống kê moment thứ nhất và moment thứ hai cho đầu vào hiện tại X𝑡. Nói chung, thống kê huấn luyện có thể được sử dụng để cập nhật thống kê suy luận bằng cách áp dụng trung bình động. Trong lan truyền ngược, gradient của loss 𝐿truyền như:

𝜕𝐿
𝜕Z𝑡=𝛾·𝜕𝐿
𝜕Y𝑡, (6)

𝜕𝐿
𝜕X𝑡=1√︃
^𝜎2
𝑡+𝜖(𝜕𝐿
𝜕Z𝑡−𝜓^𝜇𝑡−Z𝑡·𝜓^𝜎2
𝑡). (7)

Cho gradient𝜕𝐿
𝜕Y𝑡,𝜓^𝜇𝑡và𝜓^𝜎2
𝑡cho biết thống kê gradient được sử dụng để ước tính𝜕𝐿
𝜕X𝑡. Trong khung này, gradient ước tính được có được từ các hàm trung bình Θ𝑔𝜇vàΘ𝑔𝜎2,

𝜓^𝜇𝑡=Θ𝑔𝜇(𝑔^𝜇𝑡,···,𝑔^𝜇𝑡−𝑀+1), (8)
𝜓^𝜎2
𝑡=Θ𝑔𝜎2(𝑔^𝜎2
𝑡,···,𝑔^𝜎2
𝑡−𝑀+1). (9)

Gradient được truyền từ ^𝜇𝑡và^𝜎2
𝑡được ký hiệu là 𝑔^𝜇𝑡và𝑔^𝜎2
𝑡.

Các Phương pháp Ngoại tuyến trong Khung Thống nhất. Với sự trợ giúp của khung thống nhất, các phương pháp ngoại tuyến có thể được biểu thị bằng cách chọn các đối tượng thống kê khác nhau và các hàm trung bình Θ.

Ví dụ, BN có thể được công thức hóa bằng cách chọn trung bình và phương sai cho thống kê moment thứ nhất và moment thứ hai, tức là,
𝜇𝑡=1
𝐵Í𝐵
𝑖=1x𝑖, 𝜎2
𝑡=1
𝐵Í𝐵
𝑖=1(𝑥𝑖−𝜇𝑡)2. (10)

Sau đó đặt các hàm trung bình như:
^𝜇𝑡=𝜇𝑡,^𝜎2
𝑡=𝜎2
𝑡, 𝜓 ^𝜇𝑡=𝑔^𝜇𝑡, 𝜓^𝜎2
𝑡=𝑔^𝜎2
𝑡. (11)

Ở đây, các hàm trung bình chỉ tập trung vào thống kê của lần lặp hiện tại𝑡và bỏ qua 𝑀−1thống kê cuối trong các chuỗi. Trong quá trình huấn luyện, thống kê suy luận được cập nhật như,

𝜇=𝛼𝜇+(1−𝛼)^𝜇𝑡, 𝜎2=𝛼𝜎2+(1−𝛼)^𝜎2
𝑡. (12)

Như minh họa trong Phương trình 11, BN chỉ tập trung vào các kích hoạt trong lần lặp hiện tại. Điều này làm cho BN dễ vỡ đối với những biến động lớn qua các lần lặp.

Đối với MABN [ 45], các tác giả giảm số lượng thống kê để ổn định quá trình huấn luyện và loại bỏ thống kê moment thứ nhất. Do đó, trung bình bình phương được chọn là thống kê moment thứ hai:

𝜎2
𝑡=1
𝐵Í𝐵
𝑖=1x2
𝑖. (13)

Hình 3: PNAC trung bình của thống kê kích hoạt và gradient qua các lần lặp trong Transformer. PNAC cao hơn cho thấy những biến động nhẹ hơn.

Thuật toán 2 Làm mượt Biến động
Lan truyền Tiến
Đầu vào: X𝑡∈R𝐵×𝐶
Đầu ra: Y𝑡∈R𝐵×𝐶
1:𝜎2
𝑡=1
𝐵Í𝐵
𝑖=1x2
𝑡,𝑖//trung bình bình phương mini-batch
2:^𝜎2
𝑡=𝑀√︃Î𝑀−1
𝑖=0𝜎2
𝑡−𝑖//trung bình hình học
3:Z𝑡=X𝑡√
^𝜎2
𝑡+𝜖//chuẩn hóa
4:Y𝑡=𝛾·Z𝑡+𝛽 //tái tỷ lệ và dịch chuyển
5:𝜎2=𝛼𝜎2+(1−𝛼)^𝜎2
𝑡//cập nhật cho suy luận

Lan truyền Ngược
Đầu vào:𝜕𝐿
𝜕Y𝑡∈R𝐵×𝐶
Đầu ra:𝜕𝐿
𝜕X𝑡∈R𝐵×𝐶
1:𝜕𝐿
𝜕Z𝑡=𝛾·𝜕𝐿
𝜕Y𝑡
2:𝑔^𝜎2
𝑡=1
𝐵Í𝐵
𝑖=1𝜕𝐿
𝜕z𝑖z𝑖 //gradient từ ^𝜎2
𝑡
3:𝜓^𝜎2
𝑡=𝛼𝜓^𝜎2
𝑡−1+(1−𝛼)1
𝑀Í𝑀−1
𝑖=0𝑔^𝜎2
𝑡−𝑖//ước tính gradient
4:𝜕𝐿
𝜕X𝑡=1√
^𝜎2
𝑡+𝜖(𝜕𝐿
𝜕Z𝑡−Z𝑡·𝜓^𝜎2
𝑡)

Các hàm trung bình cho MABN được đặt như sau,
^𝜇𝑡=0,^𝜎2
𝑡=𝐸𝑀𝐴𝑆1, 𝜓 ^𝜇𝑡=0, 𝜓^𝜎2
𝑡=𝑆𝑀𝐴𝑆2. (14)

Thống kê suy luận 𝜎2cho MABN được cập nhật giống như Phương trình 12. Với việc chỉ áp dụng EMAS trong thống kê kích hoạt, MABN khó tránh khỏi ảnh hưởng từ các ngoại lệ cực đoan.

3.2 Làm mượt Biến động
Phân tích với Kiểm tra Tính chuẩn. Chúng tôi đi sâu hơn để phân tích các hành vi bất thường của thống kê kích hoạt trong Transformers. Để điều tra mức độ biến động trong thống kê kích hoạt và gradient, chúng tôi phân tích định lượng các hành vi bất thường với trung bình bình phương như thống kê moment thứ hai. Đối với thống kê𝜎2
𝑡∈R𝐶, chúng tôi tiến hành kiểm tra tính chuẩn[ 8] trên một chuỗi 𝜎2
𝑡,···,𝜎2
𝑡−𝑀+1để thiết lập liệu chuỗi có đến từ một quần thể phân phối chuẩn hay không, sau đó tính phần trăm kênh được duy trì cho tính chuẩn. Chúng tôi định nghĩa Phần trăm Tính chuẩn trên Tất cả Kênh (PNAC) đo lường mức độ biến động trong thống kê:

𝑃𝑁𝐴𝐶 =|{𝑐𝑖|Kiểm tra Tính chuẩn(𝑐𝑖),𝑝>0.05}|
𝐶×100%,𝑖=1,···,𝐶.
(15)

PNAC thấp hơn, những biến động lớn hơn trong thống kê. Theo cách này, chúng tôi tính PNAC cho mỗi lớp được trung bình qua các lần lặp và vẽ nó trong Hình 3. Ở đầu quá trình huấn luyện, cả thống kê kích hoạt và gradient đều biến động nhẹ qua các lần lặp. Ở cuối quá trình huấn luyện, có một sự sụt giảm đáng kể trong PNAC của thống kê kích hoạt có nghĩa là những biến động lớn tồn tại trong thống kê kích hoạt. Ngược lại, chúng tôi thấy rằng có những biến động nhẹ hơn trong thống kê gradient.

Hơn nữa, tiếp theo chúng tôi trực quan hóa thống kê kích hoạt và gradient trong các lớp và kênh khác nhau, như được hiển thị trong Hình 4. Phạm vi của thống kê kích hoạt trở nên lớn hơn theo độ sâu và quá trình huấn luyện. Chúng tôi thấy rằng phân phối xiên của thống kê kích hoạt chứa các ngoại lệ cực đoan có thể tác động đến trung bình số học. Do đó chúng tôi chuyển sang áp dụng trung bình hình học (GM) với độ nhạy cảm ít hơn đối với các ngoại lệ thay vì trung bình số học (AM) để có được một đại diện tốt hơn của thống kê kích hoạt trong một phân phối xiên. Các hàm trung bình được định nghĩa như:

^𝜇𝑡=0,^𝜎2
𝑡=𝑀√︃Î𝑀−1
𝑖=0𝜎2
𝑡−𝑖, (16)

𝜓^𝜇𝑡=0, 𝜓^𝜎2
𝑡=𝛼𝜓^𝜎2
𝑡−1+(1−𝛼)1
𝑀Í𝑀−1
𝑖=0𝑔^𝜎2
𝑡−𝑖. (17)

Bằng cách áp dụng trung bình bình phương như thống kê moment thứ hai, chúng tôi trực quan hóa GM và AM của thống kê kích hoạt trong Hình 4:trong một phân phối gần như chuẩn (tức là, những biến động nhẹ), GM gần với AM; trong phân phối xiên (tức là, những biến động lớn), các ngoại lệ cực đoan ảnh hưởng lớn đến AM, trong khi GM vẫn gần với đa số. Do thống kê gradient 𝑔^𝜎2
𝑡là thống kê moment thứ nhất và không tuân theo các ràng buộc không âm, không thể sử dụng GM trực tiếp. Do đó, chúng tôi sử dụng AM trong thống kê gradient hơn nữa với một momentum cho ước tính gradient trong lan truyền ngược. Đặc biệt, chúng tôi tận dụng trung bình bình phương như thống kê moment thứ hai trong phương pháp của chúng tôi để giảm số lượng thống kê đảm bảo tính ổn định trong việc áp dụng các chiến lược trung bình động, như được chứng minh trong [ 45]. Chiến lược của chúng tôi có thể được công thức hóa như được hiển thị trong Thuật toán 2. Bỏ qua tác động của trọng số được cập nhật qua các lần lặp khác nhau, UN được đặt với các kích thước cửa sổ vừa phải.

3.3 Lọc Ngoại lệ
Mặc dù việc làm mượt biến động được tận dụng để hiệu chỉnh thống kê kích hoạt, các ngoại lệ cực đoan được quan sát và bằng cách nào đó dẫn đến tính không ổn định trong quá trình huấn luyện (như được hiển thị trong Hình 1). Với các chiến lược trung bình động (trong Phương trình 4 và 5) được áp dụng cho thống kê kích hoạt, không thể tính toán gradient chính xác từ các lần lặp trước [ 17]. Một khi các ngoại lệ cực đoan làm xấu đi lỗi ước tính gradient, nguy cơ không ổn định tăng lên. Dựa trên giả định này, chúng tôi cố gắng đi xa hơn một bước bằng cách giới thiệu một chiến lược lọc ngoại lệ thích ứng. Cụ thể hơn, mục tiêu chính của việc lọc ngoại lệ là quyết định khi nào áp dụng các chiến lược trung bình động. Để xác định các ngoại lệ, chúng tôi đặt một ngưỡng thích ứng cho việc lọc ngoại lệ với bất đẳng thức 𝐴𝑀−𝐺𝑀 [1]. Gọi Ω𝑡=(𝜎2
𝑡,𝜎2
𝑡−1,···,𝜎2
𝑡−𝑀+1)biểu thị 𝑀thống kê kích hoạt gần đây được ghi lại trong lan truyền tiến tại lần lặp 𝑡, trong đó𝑀>1, sau đó chúng ta có

𝐸(Ω𝑡)−Π(Ω𝑡)≤𝑀·𝑉(Ω1
2
𝑡), (18)

trong đó Ω1
2
𝑡=(𝜎𝑡,···,𝜎𝑡−𝑀+1)và𝑉(·),𝐸(·),Π(·)là các toán tử tính toán phương sai, trung bình số học, và trung bình hình học cho đầu vào tương ứng. Các ngoại lệ cực đoan sẽ làm tăng phương sai. Do đó chúng tôi sử dụng cận trên của lần lặp cuối để phát hiện các ngoại lệ cho lần lặp hiện tại. Do đó, 𝑀·𝑉(Ω1
2
𝑡−1)có thể được sử dụng như một ngưỡng thích ứng cho việc lọc ngoại lệ. Nói cách khác, một khi mini-batch được coi là chứa các ngoại lệ cực đoan lớn và tất cả các chiến lược trung bình động sẽ được loại bỏ trong một lớp chuẩn hóa cụ thể,

(
^𝜎2
𝑡=𝜎2
𝑡, 𝜓^𝜎2
𝑡=𝑔^𝜎2
𝑡nếu𝐸(Ω𝑡)−Π(Ω𝑡)>𝑀·𝑉(Ω1
2
𝑡−1)
Phương trình (16) và (17) nếu không.
(19)

--- TRANG 5 ---
Chuẩn hóa Thống nhất MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha

Hình 4: Thống kê kích hoạt (hàng 1 và 2) và gradient (hàng 3 và 4) trong kênh C của lớp chuẩn hóa L. Trong thống kê kích hoạt, chúng tôi hiển thị GM và AM của thống kê kích hoạt trong các đường liền nét 'xanh' và 'cam' tương ứng.

Thống kê hiện tại 𝜎2
𝑡và𝑔^𝜎2
𝑡sẽ được sử dụng cho lan truyền tiến và lan truyền ngược một khi các ngoại lệ được tìm thấy. Trong Phương trình 19, ngưỡng cho việc lọc ngoại lệ độc lập với đầu vào cụ thể𝑋𝑡, làm cho chiến lược này thích ứng hơn với thống kê kích hoạt luôn thay đổi trong quá trình huấn luyện. Bên cạnh đó, 𝜎2và𝜓^𝜎2
𝑡được sử dụng để cập nhật thống kê được ghi lại tại lần lặp 𝑡để thụ động hóa các ngoại lệ trong trung bình động. Phần còn lại của các phép toán chỉ giống như Thuật toán 2.

Bổ đề 3.1. Gọi𝐴𝑡=(𝑎𝑡,𝑎𝑡−1,...,𝑎𝑡−𝑀+1)và𝐴𝑡−1=(𝑎𝑡−1,𝑎𝑡−2, ...,𝑎𝑡−𝑀)là hai vector thỏa mãn 𝑎𝑖>0và𝑎𝑖<𝑎𝑡,∀𝑎𝑖∈𝐴𝑡−1. 𝐸(·),Π(·), và𝑉(·)biểu thị việc tính toán trung bình số học, trung bình hình học, và phương sai cho một vector tùy ý. Nếu 𝑀·𝑉(𝐴1
2
𝑡−1)< 𝐸(𝐴𝑡)−Π(𝐴𝑡)được thỏa mãn, thì𝜆=Π(𝐴𝑡)
𝑎𝑡<1.

Chứng minh. Từ bất đẳng thức AM-GM và điều kiện bổ đề, chúng ta có bất đẳng thức sau

Π(𝐴𝑡)−Π(𝐴𝑡−1)<𝐸(𝐴𝑡)−𝐸(𝐴𝑡−1). (20)

Vì𝐸(𝐴𝑡)−𝐸(𝐴𝑡−1)=𝑎𝑡−𝑎𝑡−𝑀
𝑀, sau đó

𝑀·(Π(𝐴𝑡)−Π(𝐴𝑡−1))<𝑎𝑡−𝑎𝑡−𝑀. (21)

Với𝑎𝑡>0, bất đẳng thức trên có thể được biến đổi thành

𝑀·𝜆(1−𝑀√︂𝑎𝑡−𝑀
𝑎𝑡)<1−𝑎𝑡−𝑀
𝑎𝑡. (22)

Do đó,

𝜆<1
𝑀·1−𝑎𝑡−𝑀
𝑎𝑡
1−𝑀√︃𝑎𝑡−𝑀
𝑎𝑡<1
𝑀·𝑀=1. (23)

Bất đẳng thức cuối cùng được thỏa mãn vì hàm 𝑓(𝑥)=1−𝑥
1−𝑀√𝑥tăng đơn điệu trong [0, 1). □

Hệ quả 3.2. Đối với một Mạng sử dụng UN không có lọc ngoại lệ, gọi𝑔𝜎2
𝑡biểu thị gradient thực tế được tính dựa trên quy tắc chuỗi và ˜𝑔𝜎2
𝑡biểu thị gradient ước tính được cho bởi một hàm trung bình. Nếu𝑥𝑡chứa một ngoại lệ, thì

𝑔𝜎2
𝑡
˜𝑔𝜎2
𝑡<1
𝑀. (24)

Chứng minh. UN sử dụng trung bình hình học để ước tính thống kê huấn luyện, điều này thay đổi thống kê mini-batch hiện tại nhưng tách rời khỏi đường truyền ngược. Do đó,

𝑔𝜎2
𝑡=𝜕𝐿
𝜕𝜎2
𝑡=𝜕𝐿
𝜕^𝜎2
𝑡·𝜕^𝜎2
𝑡
𝜕𝜎2
𝑡=˜𝑔𝜎2
𝑡𝜕^𝜎2
𝑡
𝜕𝜎2
𝑡. (25)

Kết hợp với Bổ đề 3.1, chúng ta có

𝑔𝜎2
𝑡
˜𝑔𝜎2
𝑡=𝜕^𝜎2
𝑡
𝜕𝜎2
𝑡=1
𝑀·^𝜎2
𝑡
𝜎2
𝑡=1
𝑀·Π(Ω𝑡)
𝜎2
𝑡<1
𝑀. (26)

□

Với chiến lược lọc ngoại lệ thích ứng được đề xuất trong phần này, chúng ta có thể tránh một lỗi ước tính gradient thảm khốc. Dựa trên Hệ quả 3.2, chúng tôi chứng minh rằng lỗi ước tính gradient sẽ được thu nhỏ bởi một hệ số 1/𝑀khi một ngoại lệ được tìm thấy.

Bảng 1: Hiệu suất (BELU [31], cao hơn là tốt hơn) của Transformers trên dịch máy nơ-ron. 'Offline' cho biết một phương pháp có thể được hợp nhất trong suy luận. 'NoNorm' có nghĩa là các mô hình không có chuẩn hóa. 'FAIL' cho biết sụp đổ trong quá trình huấn luyện. PN∗là PN không có lớp layer-scale.

Phương pháp OfflineIWSLT14 WMT14
BLEU△ BELU△
LN [2] % 35.3 40.0
RMSNorm [50] % 35.3 0.0 39.8 -0.2
PN [35] % 35.3 0.0 39.8 -0.2
NoNorm / FAIL / 32.8 -7.2
BN [19] ! 31.1 -4.2 35.1 -4.9
MABN [45] ! 35.4 +0.1 36.5 -3.5
PN∗[35] ! 35.0 -0.3 39.7 -0.3
UN ! 35.4 +0.1 39.9 -0.1

Bảng 2: Hiệu suất (Độ chính xác Top-1 %) của phân loại hình ảnh trên ImageNet-1K và CIFAR10/100.

Phương pháp OfflineSwin-T T2T-ViT-14†
ImageNet ImageNet CIFAR10 CIFAR100
Top1△ Top1△ Top1△ Top1△
LN [2] % 81.3 81.5 98.3 88.4
BN [19] ! 80.8 -0.5 79.8 -1.7 96.6 -1.7 88.2 -0.2
MABN [45] ! 80.9 -0.4 FAIL / / / / /
PN∗[35] ! 80.9 -0.4 FAIL / / / / /
UN ! 81.0 -0.3 80.9 -0.6 98.3 0.0 88.9 +0.5

†: Trên CIFAR10/100, các mô hình được khởi tạo với trọng số đã được huấn luyện trước từ ImageNet-1K. Lưu ý rằng T2T-ViT-14 hợp tác với MABN và PN∗gặp sự cố trong quá trình huấn luyện trên ImageNet, do đó chúng tôi không báo cáo các kết quả tương ứng trên CIFAR10/100.

4 THÍ NGHIỆM

4.1 Chi tiết Triển khai
Để đặt tất cả các thí nghiệm trên cùng một nền tảng, chúng tôi chỉ đơn giản thay thế tất cả LN trong các kiến trúc tương ứng bằng các đối tác thay thế dễ dàng, mà không thay đổi vị trí của lớp chuẩn hóa hoặc thêm các toán tử bổ sung. Tất cả các mô hình được huấn luyện và kiểm tra với cùng cấu hình. Để đơn giản hóa các cài đặt, momentum của UN được đặt là𝛼=0.9(giống như BN) để tránh việc điều chỉnh siêu tham số nhiều lần trên các tác vụ khác nhau. Tương tự như [ 45], chúng tôi đặt một bước khởi động cho UN, mặc định là 4K. Để hiển thị kết quả mạnh mẽ, chúng tôi báo cáo hiệu suất trung bình từ kết quả 5 lần chạy cho các tập dữ liệu nhỏ, IWSLT14, CIFAR10, và CIFAR100. Xem Phụ lục B để biết thêm chi tiết thí nghiệm.

4.2 Kết quả

4.2.1 Dịch Máy Nơ-ron. So sánh giữa các phương pháp trực tuyến và ngoại tuyến được liệt kê trong Bảng 1. Mặc dù RMSNorm [ 50] và PN đạt được kết quả tương đương với LN, các phương pháp trực tuyến không thể truy cập triển khai hiệu quả trên phần cứng. Chúng tôi báo cáo hiệu suất của Transformers được huấn luyện mà không có các lớp chuẩn hóa (được đánh dấu là 'NoNorm'). Tính không ổn định trong quá trình huấn luyện và hiệu suất giảm làm nổi bật tính cần thiết của việc áp dụng chuẩn hóa. Bên cạnh đó, các phương pháp ngoại tuyến trước đó, như BN, MABN, và

--- TRANG 6 ---
MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha Qiming Yang et al.

Bảng 3: Phát hiện đối tượng trên COCO val2017 với Faster R-CNN sử dụng Swin-T làm backbone. Tất cả các mô hình được huấn luyện với 36 epochs.

Phương pháp Offline APBox△ APBox
50APBox
75APBox𝑠 APBox𝑚 APBox
𝑙
LN [2] % 45.5 67.5 50.2 31.5 48.8 58.4
BN [19] ! 44.6 -0.9 66.8 48.9 30.4 48.0 57.8
MABN [45] ! 44.8 -0.7 66.8 49.0 29.0 47.9 57.3
PN∗[35] ! 44.3 -1.2 66.7 48.4 29.6 47.4 57.2
UN ! 45.2 -0.3 67.2 49.7 30.1 48.4 58.2

PN∗gặp phải sự suy giảm hiệu suất. Thay vào đó, phương pháp của chúng tôi vượt trội hơn các phương pháp ngoại tuyến khác và đạt được kết quả cân bằng hơn tương đương với các phương pháp trực tuyến trên cả IWSLT14 và WMT14.

4.2.2 Phân loại Hình ảnh. Bảng 2 báo cáo kết quả của T2T-ViT-14 và Swin-T trên ImageNet. Trong T2T-ViT-14, MABN và PN∗tận dụng các chiến lược trung bình động vani trải qua sự phân kỳ trong quá trình huấn luyện. BN xuất hiện tính không ổn định từ giai đoạn đầu của quá trình huấn luyện (được hiển thị trong Hình 1), dẫn đến suy giảm độ chính xác. UN hưởng lợi từ tính ổn định trong quá trình huấn luyện và có được sự cải thiện 1.1%so với BN. Trong Swin-T, độ chính xác top-1 của BN giảm 0.5%. UN vượt qua các phương pháp ngoại tuyến khác và khôi phục độ chính xác lên 81.0%. Sau đó, chúng tôi đánh giá UN trên các tác vụ phân loại hạ nguồn (CIFAR10/100). Loss huấn luyện biến động mạnh mẽ và không đều trong T2T-ViT-14 được tinh chỉnh với BN, dẫn đến sự sụt giảm đáng kể về độ chính xác. UN hội tụ ổn định mà không cần điều chỉnh bất kỳ cài đặt nào và vượt trội hơn LN về độ chính xác top-1.

4.2.3 Phát hiện Đối tượng và Phân đoạn Thể hiện. Trong Bảng 3, phương pháp của chúng tôi khôi phục hiệu suất từ các phương pháp ngoại tuyến khác, chỉ với sự giảm nhẹ 0.3%mAP so với LN. Trong Bảng 4, kết quả cũng cho thấy rằng phương pháp của chúng tôi đạt được hiệu suất tương đương với LN. Ở đây, chúng tôi cho thấy UN vượt qua các phương pháp ngoại tuyến khác rút ra sức mạnh từ việc làm mượt biến động và lọc ngoại lệ. Với những kết quả này, kết luận dễ dàng đến là UN có thể được tổng quát hóa cho các tác vụ thị giác khác hơn chỉ phân loại hình ảnh.

4.3 Phân tích

4.3.1 Nghiên cứu Loại bỏ. Trong Bảng 5, chúng tôi loại bỏ UN để xác minh đóng góp của các thành phần cơ bản. Lưu ý rằng trong việc làm mượt biến động của UN, GM, AM, và momentum 𝛼đều được sử dụng như các chiến lược trung bình động. Chúng tôi loại bỏ các thành phần cơ bản từng cái một, như được hiển thị trong Exp2-5. Trong Exp2, hiệu suất xấu đi mà không có bất kỳ chiến lược di chuyển nào được áp dụng cho thống kê kích hoạt. Trong ước tính gradient, chúng tôi tiến hành một chiến lược di chuyển hỗn hợp cho thống kê gradient, bao gồm trung bình số học và momentum 𝛼. Trong exp3-4, hiệu suất giảm sau khi loại bỏ bất kỳ phần cơ bản nào của chiến lược di chuyển hỗn hợp, điều này ngụ ý cả hai đều đóng góp cho ước tính gradient tốt hơn. Để so sánh với Exp2, chúng tôi loại bỏ tất cả các chiến lược di chuyển từ thống kê gradient trong exp5. Hóa ra các mô hình gặp phải mất mát hiệu suất mà không có ước tính gradient. Trong kết quả này, chúng tôi cho thấy việc làm mượt biến động được đề xuất trong bài báo này đã trao quyền cho UN để có được hiệu suất vững chắc. Ngoài ra, chúng tôi cũng báo cáo nghiên cứu loại bỏ trên IWSLT14,

Hình 5: Các bước tích lũy của việc loại bỏ trung bình động trong lọc ngoại lệ trong quá trình huấn luyện. Chúng tôi vẽ kết quả bằng màu đỏ, cam, và xanh lá cây sau khi huấn luyện 12, 24, và 36 epochs, tương ứng.

Hình 6: So sánh sự tương đồng của bản đồ đặc trưng trên T2T-ViT-14 (ImageNet) với các lớp nông được làm nổi bật trong hộp đỏ. Sự tương đồng được đo bằng Centered Kernel Alignment (CKA) [29] trên các lớp bao gồm các lớp chuẩn hóa, MHSA, và FFN trong T2T-ViT-14. Tất cả các lớp LN trong T2T-ViT-14 được thay thế đơn giản bằng BN và UN.

có chi tiết có thể được tìm thấy trong Phụ lục C.1. Trên IWSLT14, các mô hình vẫn có thể hưởng lợi từ việc làm mượt biến động để cải thiện thêm.

4.3.2 Ảnh hưởng của Kích thước Cửa sổ. Chúng tôi so sánh các kích thước cửa sổ khác nhau𝑀∈{2,4,6,8,10}trên COCO val2017. Bảng 6 báo cáo ảnh hưởng của kích thước cửa sổ trong UN. Khi UN được đặt với kích thước cửa sổ vừa phải, các mô hình hội tụ ổn định và có được hiệu suất cạnh tranh ở cuối.

4.3.3 Lọc Ngoại lệ. Như minh họa trong Hình 1, dễ dàng thấy rằng việc lọc ngoại lệ ổn định quá trình huấn luyện của UN trong T2T-ViT trong khi các phương pháp ngoại tuyến khác có xu hướng gặp sự cố trong quá trình huấn luyện. Bảng 7 trình bày kết quả có và không có lọc ngoại lệ. Trong Transformers, những biến động trong thống kê kích hoạt tăng theo độ sâu. Hình 5 hiển thị các bước tích lũy của các lần lặp đã tìm thấy ngoại lệ. Ngoại lệ có xu hướng xuất hiện từ một lớp sâu hơn. Khi mô hình được huấn luyện với nhiều epochs hơn, phần trăm tăng lên. Quan sát này ngụ ý rằng Transformers được huấn luyện với các phương pháp ngoại tuyến có thể có những biến động lớn hơn khi mở rộng quy mô độ sâu. Kết quả này cũng cho thấy rằng những biến động cũng tăng theo quá trình huấn luyện.

4.3.4 Sự Tương đồng Đặc trưng. Với việc sử dụng Centered Kernel Alignment (CKA), một thước đo tương đồng biểu diễn được sử dụng rộng rãi, chúng ta có thể

--- TRANG 7 ---
Chuẩn hóa Thống nhất MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha

Bảng 4: Phát hiện đối tượng và phân đoạn ngữ nghĩa trên COCO val2017 với Mask R-CNN sử dụng Swin-T làm backbone. Tất cả các mô hình được huấn luyện với 36 epochs.

Phương pháp Offline APBox△ APBox
50APBox
75APBox𝑠 APBox𝑚 APBox
𝑙APMask△ APMask
50APMask
75APMask𝑠 APMask𝑚 APMask
𝑙
LN [2] % 46.0 68.1 50.3 31.2 49.2 60.1 41.6 65.1 44.9 25.9 45.1 56.9
BN [19] ! 44.9 -1.1 67.2 49.0 29.6 48.4 58.3 40.8 -0.8 64.0 43.8 24.9 44.4 55.3
MABN [45] ! 45.1 -0.9 67.2 49.6 30.0 48.3 57.7 41.0 -0.6 64.2 44.1 24.9 44.7 55.0
PN∗[35] ! 44.6 -1.4 66.8 48.9 29.1 47.6 57.6 40.7 -0.9 63.7 43.6 24.1 43.8 54.9
UN ! 45.6 -0.4 67.6 50.4 29.6 49.2 58.9 41.4 -0.2 64.8 44.5 25.2 45.1 55.7

Bảng 5: Nghiên cứu loại bỏ trên COCO val2017. Mask R-CNN với Swin-T được huấn luyện trong 12 epochs.

ExpFP BP COCO‡
GM AM𝛼 APBoxΔ APMaskΔ
1!! ! 42.8 39.2
2 ! ! 42.2 -0.6 38.8 -0.4
3! ! 42.4 -0.4 38.8 -0.4
4!! 42.4 -0.4 39.0 -0.2
5! 42.5 -0.3 39.0 -0.2

Bảng 6: Ảnh hưởng của việc sử dụng các kích thước cửa sổ khác nhau ( 𝑀) trong UN được đánh giá trên COCO val2017.

𝑀 2 4 6 8 10
APBox42.8 42.8 42.8 42.8 42.5
APMask39.2 39.3 39.1 39.2 39.2

Bảng 7: Ảnh hưởng của việc sử dụng lọc ngoại lệ được đánh giá trên ImageNet và COCO val2017.

Lọc
Ngoại lệT2T-ViT-14 Swin-T
ImageNet ImageNet Faster RCNN Mask RCNN
Top1 Top1 APBoxAPBoxAPMask
w/ 80.9 81.0 42.2 42.8 39.2
w/o FAIL 80.7 FAIL FAIL FAIL

nghiên cứu các biểu diễn nội bộ giữa các mô hình khác nhau. Như được mô tả trong Hình 6, chúng tôi so sánh sự tương đồng của bản đồ đặc trưng trung gian giữa các mô hình được huấn luyện với các phương pháp chuẩn hóa khác nhau. Chúng tôi chủ yếu tập trung vào các pixel đường chéo trong bản đồ nhiệt cho biết sự tương đồng trên các lớp có cùng độ sâu. UN cho thấy sự tương đồng đáng chú ý hơn với LN so với BN, đặc biệt là trong các lớp nông. Kết quả này bằng cách nào đó giải thích sự ưu việt của phương pháp chúng tôi.

4.3.5 Hiệu quả. Transformers được áp dụng rộng rãi cho các tác vụ thị giác và cố gắng đạt được triển khai hiệu quả. LN đi kèm với chi phí tính toán và bộ nhớ bổ sung dẫn đến suy luận không hiệu quả. Bên cạnh đó, LN không thể được hỗ trợ trên nhiều thiết bị edge, ví dụ: NXP i.MX Series và TITDA4x. Vẫn còn

Bảng 8: So sánh hiệu quả suy luận giữa LN và UN trong Swin-T. MEM (MB) là bộ nhớ được phân bổ tối đa trong quá trình suy luận. TPUT (Img./Sec.) hiển thị throughput trung bình được tính trong 1000 batches. Chúng tôi đặt kích thước batch là 512 cho ImageNet và 2 cho COCO (với Mask R-CNN).

Tác vụ Phương pháp MEM Giảm TPUT Tăng tốc
ImageNetLN 9978 - 1179.5 -
UN 8213 17.7% 1547.8 31.2%
COCOLN 955 - 17.8 -
UN 897 6.1% 22.1 24.2%

chỗ cho Transformers để được cải thiện thêm để đạt được triển khai hiệu quả về phần cứng. Trong bài báo này, chúng tôi tập trung vào việc cải thiện lớp chuẩn hóa cho Transformers để đạt được sự cân bằng tốt hơn giữa hiệu suất và tốc độ suy luận. Với việc hợp nhất UN vào các lớp tuyến tính khác, các phép chia và căn bậc hai cũng được loại bỏ trong suy luận. Về mặt thực nghiệm, chúng tôi kiểm tra hiệu quả trên GeForce RTX 3090 với Swin-T như được báo cáo trong Bảng 8. Đối với phân loại, chúng tôi cho thấy rằng khi phương pháp của chúng tôi được hợp nhất với các phép toán tuyến tính khác, nó đạt được khoảng 18% giảm bộ nhớ và hơn 31% cải thiện throughput. Đối với phát hiện đối tượng, Mask R-CNN với Swin-T được tích hợp với các thành phần khác như FPN và head, trong khi LN chỉ được sử dụng trong backbone. Kết quả là, sự gia tăng tốc độ bị hạn chế.

5 KẾT LUẬN
Trong bài báo này, chúng tôi xem xét cách triển khai Transformers một cách hiệu quả bằng cách thay thế LN bằng một phương pháp ngoại tuyến. Các phương pháp ngoại tuyến trước đó gặp phải hiệu suất kém hơn và tính không ổn định do những biến động lớn và ngoại lệ cực đoan trong thống kê kích hoạt. Dựa trên phân tích của chúng tôi, chúng tôi đề xuất UN bao gồm các chiến lược làm mượt biến động và lọc ngoại lệ để giải quyết những thách thức này. Các thí nghiệm mở rộng trên các tác vụ NLP và CV chứng minh rằng phương pháp của chúng tôi vượt trội đáng kể so với các phương pháp ngoại tuyến trước đó. Hơn nữa, phương pháp của chúng tôi cung cấp hiệu suất tương đương với LN, với tốc độ tăng tốc hơn 31% trong suy luận. Chúng tôi tin rằng phương pháp của chúng tôi sẽ là một thành phần chung trong Transformers cho triển khai hiệu quả.

LỜI CẢM ƠN
Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số U19B2043).

--- TRANG 8 ---
MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha Qiming Yang et al.

TÀI LIỆU THAM KHẢO
[1]Jesús Munárriz Aldaz. 2012. Các cận chặt cho sự khác biệt giữa trung bình số học và hình học. Archiv der Mathematik 99, 4 (2012), 393–399.
[2]Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E. Hinton. 2016. Chuẩn hóa Lớp. arXiv preprint arXiv:1607.06450 (2016).
[3]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al .2020. Các mô hình ngôn ngữ là những người học few-shot. Advances in neural information processing systems 33 (2020), 1877–1901.
[4]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. 2020. Phát hiện đối tượng đầu cuối với transformers. Trong European Conference on Computer Vision . Springer, 213–229.
[5]Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al .2019. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019).
[6]Xinlei Chen, Saining Xie, và Kaiming He. 2021. Một nghiên cứu thực nghiệm về huấn luyện vision transformers tự giám sát. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision . 9640–9649.
[7]Zhengsu Chen, Lingxi Xie, Jianwei Niu, et al .2021. Visformer: Transformer thân thiện với thị giác. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision . 589–598.
[8]RALPH D'AGOSTINO và Egon S Pearson. 1973. Kiểm tra sự khởi hành khỏi tính chuẩn. Biometrika 60, 3 (1973), 613–622.
[9]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. 2009. Imagenet: Một cơ sở dữ liệu hình ảnh phân cấp quy mô lớn. Trong 2009 IEEE conference on computer vision and pattern recognition . Ieee, 248–255.
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Huấn luyện trước transformers hai chiều sâu để hiểu ngôn ngữ. arXiv preprint arXiv:1810.04805 (2018).
[11] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, và Lu Yuan. 2022. DaViT: Dual Attention Vision Transformers. https://doi.org/10.48550/ ARXIV.2204.03645
[12] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, et al .2021. Cswin transformer: Một backbone vision transformer chung với các cửa sổ hình chữ thập. arXiv preprint arXiv:2107.00652 (2021).
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al .2020. Một hình ảnh đáng giá 16x16 từ: Transformers cho nhận dạng hình ảnh ở quy mô. arXiv preprint arXiv:2010.11929 (2020).
[14] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, và Matthijs Douze. 2021. LeViT: một Vision Transformer trong ConvNet's Clothing cho Suy luận Nhanh hơn. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision . 12259–12269.
[15] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al .2020. Conformer: Transformer tăng cường tích chập cho nhận dạng giọng nói. arXiv preprint arXiv:2005.08100 (2020).
[16] Kaiming He, Georgia Gkioxari, Piotr Dollár, và Ross Girshick. 2017. Mask r-cnn. Trong Proceedings of the IEEE international conference on computer vision . 2961–2969.
[17] Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, và Ling Shao. 2020. Các kỹ thuật chuẩn hóa trong huấn luyện dnns: Phương pháp, phân tích và ứng dụng. arXiv preprint arXiv:2009.12836 (2020).
[18] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, và Bin Fu. 2021. Shuffle Transformer: Suy nghĩ lại Spatial Shuffle cho Vision Transformer. arXiv preprint arXiv:2106.03650 (2021).
[19] Sergey Ioffe và Christian Szegedy. 2015. Chuẩn hóa batch: Tăng tốc huấn luyện mạng sâu bằng cách giảm sự dịch chuyển hiệp biến nội bộ. Trong International conference on machine learning . PMLR, 448–456.
[20] Tianyang Lin, Yuxin Wang, Xiangyang Liu, và Xipeng Qiu. 2021. Một khảo sát về transformers. arXiv preprint arXiv:2106.04554 (2021).
[21] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, và Serge Belongie. 2017. Mạng kim tự tháp đặc trưng cho phát hiện đối tượng. Trong Proceedings of the IEEE conference on computer vision and pattern recognition . 2117–2125.
[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C Lawrence Zitnick. 2014. Microsoft coco: Các đối tượng phổ biến trong ngữ cảnh. Trong European conference on computer vision . Springer, 740–755.
[23] Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, và Jingbo Zhu. 2020. Hướng tới suy luận số nguyên 8-bit đầy đủ cho mô hình transformer. arXiv preprint arXiv:2009.08034 (2020).
[24] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, và Jiawei Han. 2020. Hiểu Khó khăn của Huấn luyện Transformers. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020) .
[25] Ze Liu, Han Hu, Yutong Lin, et al .2021. Swin Transformer V2: Mở rộng Quy mô Dung lượng và Độ phân giải. arXiv preprint arXiv:2111.09883 (2021).
[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. 2021. Swin transformer: Vision transformer phân cấp sử dụng các cửa sổ dịch chuyển. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision . 10012–10022.
[27] Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, và Jingyu Li. 2018. Học cách chuẩn hóa có thể phân biệt qua chuẩn hóa có thể chuyển đổi. arXiv preprint arXiv:1806.10779 (2018).
[28] Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, và Hannaneh Hajishirzi. 2020. Delight: Transformer sâu và nhẹ. arXiv preprint arXiv:2008.00623 (2020).
[29] Thao Nguyen, Maithra Raghu, và Simon Kornblith. 2020. Các mạng rộng và sâu có học những điều giống nhau không? khám phá cách các biểu diễn mạng nơ-ron thay đổi theo chiều rộng và độ sâu. arXiv preprint arXiv:2010.15327 (2020).
[30] Myle Ott, Sergey Edunov, Alexei Baevski, et al .2019. fairseq: Một bộ công cụ nhanh, mở rộng cho mô hình hóa chuỗi. arXiv preprint arXiv:1904.01038 (2019).
[31] Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: một phương pháp đánh giá tự động dịch máy. Trong Proceedings of the 40th annual meeting of the Association for Computational Linguistics . 311–318.
[32] Shaoqing Ren, Kaiming He, Ross Girshick, và Jian Sun. 2015. Faster r-cnn: Hướng tới phát hiện đối tượng thời gian thực với mạng đề xuất vùng. Advances in neural information processing systems 28 (2015).
[33] Wenqi Shao, Yixiao Ge, Zhaoyang Zhang, Xuyuan Xu, Xiaogang Wang, Ying Shan, và Ping Luo. 2021. Chuẩn hóa Token Động cải thiện Vision Transformer. arXiv preprint arXiv:2112.02624 (2021).
[34] Wenqi Shao, Tianjian Meng, Jingyu Li, et al .2019. Ssn: Học chuẩn hóa có thể chuyển đổi thưa thớt qua sparsestmax. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 443–451.
[35] Sheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, và Kurt Keutzer. 2020. Powernorm: Suy nghĩ lại chuẩn hóa batch trong transformers. Trong International Conference on Machine Learning . PMLR, 8741–8751.
[36] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, và Hervé Jégou. 2021. Đi sâu hơn với image transformers. arXiv preprint arXiv:2103.17239 (2021).
[37] Dmitry Ulyanov, Andrea Vedaldi, và Victor Lempitsky. 2016. Chuẩn hóa instance: Thành phần thiếu cho stylization nhanh. arXiv preprint arXiv:1607.08022 (2016).
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[39] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, và Ling Shao. 2021. Pyramid vision transformer: Một backbone đa năng cho dự đoán dày đặc mà không có tích chập. arXiv preprint arXiv:2102.12122 (2021).
[40] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, và Tsubasa Ochiai. 2018. ESPnet: Bộ công cụ Xử lý Giọng nói Đầu cuối. Trong Proceedings of Interspeech . 2207–2211. https: //doi.org/10.21437/Interspeech.2018-1456
[41] Yuxin Wu và Kaiming He. 2018. Chuẩn hóa nhóm. Trong ECCV . 3–19.
[42] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al . 2016. Hệ thống dịch máy nơ-ron của Google: Kết nối khoảng cách giữa dịch máy và con người. arXiv preprint arXiv:1609.08144 (2016).
[43] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, và Tieyan Liu. 2020. Về chuẩn hóa lớp trong kiến trúc transformer. Trong International Conference on Machine Learning . PMLR, 10524–10533.
[44] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, và Junyang Lin. 2019. Hiểu và cải thiện chuẩn hóa lớp. Advances in Neural Information Processing Systems 32 (2019).
[45] Junjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, và Jian Sun. 2020. Hướng tới ổn định thống kê batch trong lan truyền ngược của chuẩn hóa batch. arXiv preprint arXiv:2001.06838 (2020).
[46] Zhuliang Yao, Yue Cao, Yutong Lin, Ze Liu, Zheng Zhang, và Han Hu. 2021. Tận dụng chuẩn hóa batch cho vision transformers. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision . 413–422.
[47] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, và Xin Lei. 2021. WeNet: Bộ công cụ Nhận dạng Giọng nói Đầu cuối Hướng sản xuất Streaming và Non-streaming. Trong Proc. Interspeech . IEEE, Brno, Czech Republic.
[48] Xiaodong Yu, Dahu Shi, Xing Wei, Ye Ren, Tingqun Ye, và Wenming Tan. 2021. SOIT: Phân đoạn Đối tượng với Transformers Nhận thức Instance. arXiv preprint arXiv:2112.11037 (2021).
[49] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, và Shuicheng Yan. 2021. Tokens-to-token vit: Huấn luyện vision transformers từ đầu trên imagenet. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision . 558–567.
[50] Biao Zhang và Rico Sennrich. 2019. Chuẩn hóa lớp căn bậc hai trung bình. Advances in Neural Information Processing Systems 32 (2019).

--- TRANG 9 ---
MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha Qiming Yang et al.

Hình 7: Bản đồ mật độ xác suất của PNAC trong các lớp khác nhau (độ sâu) qua các lần lặp. Bên trái: trực quan hóa ở đầu quá trình huấn luyện. Bên phải: trực quan hóa ở cuối quá trình huấn luyện. Chúng tôi thu thập các mẫu từ 150 bước thời gian. Ước tính mật độ kernel (KDE) được sử dụng để trực quan hóa. Lưu ý rằng thống kê kích hoạt và gradient được vẽ bằng màu 'aquamarine' và 'xám', tương ứng.

A TRỰC QUAN HÓA

A.1 Biến động Lớn trong Thống kê Kích hoạt
Hình 7 trình bày bản đồ mật độ xác suất của PNAC trong quá trình huấn luyện minh họa xu hướng chung của thống kê cho tất cả các lớp chuẩn hóa trong Transformer. Ở đầu quá trình huấn luyện, hầu hết các kênh trong thống kê kích hoạt và gradient giữ PNAC cao, cho thấy những biến động nhẹ. Sau khi huấn luyện một vài epochs, có một sự thay đổi lớn trong thống kê kích hoạt rằng nhiều kênh có được PNAC thấp hơn nhiều, có nghĩa là những biến động lớn xuất hiện trong thống kê kích hoạt. Trong bài báo này, một chiến lược làm mượt biến động được thiết kế riêng được sử dụng để có được một đại diện tốt hơn của những thống kê luôn thay đổi này.

B CHI TIẾT CHO THÍ NGHIỆM

B.1 Thiết lập cho Dịch Máy Nơ-ron
Chúng tôi đánh giá phương pháp của chúng tôi với Transformer trên hai tập dữ liệu: (1) IWSLT14 De-En (IWSLT14) chứa 0.18M cặp câu; (2) WMT14 En-Fr (WMT14) [ 42] chứa 36M cặp câu. Thiết lập để tiền xử lý dữ liệu thô giống như [ 28]. Đối với IWSLT14, chúng tôi sao chép các chiến lược huấn luyện và đánh giá trong [ 24]. Đối với WMT14, chúng tôi tuân theo thiết lập huấn luyện và đánh giá trong [ 28] và trung bình 5 checkpoints cuối cho kiểm tra. Ở đây, tất cả các thí nghiệm được triển khai lại trên cơ sở mã của Fairseq [ 30] với thiết lập pre-normalization [43].

B.2 Thiết lập cho Phân loại Hình ảnh
Trong phần này, chúng tôi tiến hành phân loại hình ảnh trên ImageNet-1K [ 9] với hai Vision Transformers tiên tiến, T2T-ViT-14 [ 49] và Swin-T [ 26]. ImageNet-1K là một tập dữ liệu phân loại hình ảnh được sử dụng rộng rãi, chứa 1000 danh mục, 1.28M mẫu huấn luyện, và 50K mẫu xác thực. Theo thiết lập trong [ 26,49], tất cả các mô hình được huấn luyện từ đầu trong 300 epochs với kích thước đầu vào được cắt 224×224. Chúng tôi thay thế tất cả các lớp LN trong các kiến trúc gốc bằng BN/MABN/PN và phương pháp được đề xuất của chúng tôi UN. Sau khi huấn luyện trước các mô hình trên ImageNet, chúng tôi chuyển các mô hình đến các tập dữ liệu phân loại hạ nguồn

Bảng 9: Loại bỏ các thành phần khác nhau của UN được đánh giá trên IWSLT14 với Transformer.

Exp PhươngphápFP BP IWSLT14
GM AM𝛼 BLEU Δ
1 UN !! ! 35.4
2 ! ! 34.5 -0.9
3 ! ! 35.3 -0.1
4 !! 18.3 -17.1
5 ! FAIL /

tập dữ liệu phân loại, CIFAR10 và CIFAR100, tập trung vào phân loại đối tượng chung. Chúng tôi tuân theo công thức huấn luyện trong [ 49]. Tất cả các mô hình được cung cấp với đầu vào được thay đổi kích thước 224×224và được tinh chỉnh trong 60 epochs.

B.3 Thiết lập cho Phát hiện Đối tượng và Phân đoạn Thể hiện
Chúng tôi đánh giá phương pháp của chúng tôi trên COCO [ 22]. Theo thiết lập tiêu chuẩn trong [ 26], phát hiện đối tượng được tiến hành trên Faster R-CNN [ 32] với FPN [ 21]. Đối với phân đoạn thể hiện, chúng tôi đánh giá phương pháp của chúng tôi với một khung phổ biến Mask R-CNN [ 16]. Thiết lập cho huấn luyện và đánh giá tuân theo các cấu hình gốc trên [ 26], tất cả các mô hình được huấn luyện với 36 epochs. Kích thước đầu vào là 1333×800và tổng kích thước batch được đặt là 16. Backbone (Swin-T) được khởi tạo với trọng số đã được huấn luyện trước được huấn luyện trên ImageNet-1K. Tất cả các thí nghiệm được triển khai lại dựa trên mmdetection [5].

C KẾT QUẢ BỔ SUNG

C.1 Nghiên cứu Loại bỏ trên Dịch Máy Nơ-ron
Bảng 9, chúng tôi loại bỏ UN trên IWSLT14 để xác minh đóng góp của các thành phần cơ bản. Vì Trung bình Hình học (GM), Trung bình Số học(AM), và momentum 𝛼đều được sử dụng trong việc làm mượt biến động, chúng tôi cố gắng loại bỏ các thành phần cơ bản từng cái một, như được hiển thị trong Exp2-5. Không có bất kỳ chiến lược di chuyển nào được áp dụng trong thống kê kích hoạt, có một sự sụt giảm đáng kể về BELU. Trong lan truyền ngược, chúng tôi tiến hành một chiến lược di chuyển hỗn hợp cho ước tính gradient, bao gồm trung bình số học và momentum 𝛼. Kết quả cho thấy mỗi cái đều quan trọng cho hiệu suất cuối cùng. Đặc biệt, momentum 𝛼rất hữu ích cho việc ổn định quá trình huấn luyện trên ISWLT14. Trong Exp5, một khi chúng tôi loại bỏ tất cả các chiến lược di chuyển trong BP, mô hình sẽ không thể hội tụ. Kết quả cho thấy lợi thế của việc làm mượt biến động trong IWSLT14.

C.2 Ảnh hưởng của 𝛼
Trong UN, chúng tôi tận dụng một momentum 𝛼cho cả việc ước tính thống kê suy luận trong lan truyền tiến và ước tính gradient trong lan truyền ngược. Chúng tôi điều tra ảnh hưởng của 𝛼như được hiển thị trong Bảng 10. Bằng cách điều chỉnh 𝛼trong một phạm vi lớn {0.6,0.7,0.8,0.9}, chúng tôi thấy các mô hình vẫn hội tụ ổn định trên COCO với hiệu suất gần nhau. Chọn 𝛼từ{0.7,0.8,0.9}cũng tốt với IWSLT14. Một khi chúng tôi đặt nó với một tỷ lệ nhỏ, như 𝛼=0.6, kết quả

--- TRANG 10 ---
Chuẩn hóa Thống nhất MM '22, 10-14 tháng 10, 2022, Lisboa, Bồ Đào Nha

Bảng 10: Ảnh hưởng của 𝛼trong UN được đánh giá trên COCO val2017 (AP) và IWSLT14 (BELU).

𝛼 0.9 0.8 0.7 0.6
APBox42.8 42.6 42.7 42.8
APMask39.2 39.1 39.2 39.4
BELU 35.4 35.4 35.3 FAIL

là dành riêng cho tác vụ ở chỗ quá trình huấn luyện sụp đổ trên IWSLT14. Ở một mức độ nào đó, điều đó tương tự như những gì chúng tôi hiển thị trong Bảng 9 (loại bỏ momentum trong BP). Kết quả là, chúng tôi tin rằng 𝛼=0.9sẽ là một lựa chọn tốt cho các tác vụ khác nhau đồng thời cũng cho phép so sánh công bằng với các phương pháp khác.
