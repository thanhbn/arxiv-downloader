# 2210.15042.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/privacy/2210.15042.pdf
# File size: 458121 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Privately Fine-Tuning Large Language Models with
Differential Privacy
Rouzbeh Behnia
School of Information Systems
and Management
University of South Florida
Sarasota, USA
behnia@usf.eduMohammadreza (Reza) Ebrahimi
School of Information Systems
and Management
University of South Florida
Tampa, USA
ebrahimim@usf.eduJason Pacheco
Department of Computer Science
University of Arizona
Tucson, USA
pachecoj@cs.arizona.edu
Balaji Padmanabhan
School of Information Systems
and Management
University of South Florida
Tampa, USA
bp@usf.edu
Abstract ‚ÄîPre-trained Large Language Models (LLMs) are
an integral part of modern AI that have led to breakthrough
performances in complex AI tasks. Major AI companies with
expensive infrastructures are able to develop and train these large
models with billions and millions of parameters from scratch.
Third parties, researchers, and practitioners are increasingly
adopting these pre-trained models and Ô¨Åne-tuning them on their
private data to accomplish their downstream AI tasks. However,
it has been shown that an adversary can extract/reconstruct the
exact training samples from these LLMs, which can lead to
revealing personally identiÔ¨Åable information. The issue has raised
deep concerns about the privacy of LLMs. Differential privacy
(DP) provides a rigorous framework that allows adding noise in
the process of training or Ô¨Åne-tuning LLMs such that extracting
the training data becomes infeasible (i.e., with a cryptograph-
ically small success probability). While the theoretical privacy
guarantees offered in most extant studies assume learning models
from scratch through many training iterations in an asymptotic
setting, this assumption does not hold in Ô¨Åne-tuning scenarios in
which the number of training iterations is signiÔ¨Åcantly smaller.
To address the gap, we present EW-Tune , a DP framework
for Ô¨Åne-tuning LLMs based on Edgeworth accountant with
Ô¨Ånite-sample privacy guarantees. Our results across four well-
established natural language understanding (NLU) tasks show
that while EW-Tune adds privacy guarantees to LLM Ô¨Åne-tuning
process, it directly contributes to decreasing the induced noise to
up to 5.6% and improves the state-of-the-art LLMs performance
by up to 1.1% across all NLU tasks. We have open-sourced our
implementations for wide adoption and public testing purposes.
Index Terms ‚ÄîDifferential privacy, large language models, Ô¨Åne-
tuning, Edgeworth accountant
I. I NTRODUCTION
Large language models (LLMs) have become an integral
component of modern AI. Deep learning architectures with bil-
lions of parameters are often designed based on transformers,
a building block Ô¨Årst introduced by Google‚Äôs BERT [1]. LLMs
provide breakthrough performance in complex AI tasks such
Equally contributing authors (alphabetically ordered by the last name.)as dialogue systems [2] and text/automated story generation
[3]. Being equipped with the hardware infrastructure, major AI
companies such as Open AI and Facebook provide new LLMs
trained on the public data from the Internet [4], [5]. Common
examples include, RoBERTa [4] and GPT [5]. RoBERTa‚Äôs
training dataset includes English Wikipedia and millions of
online news crawled from the internet. Similarly, GPT was
trained on outbound links from Reddit.
AI researchers and practitioners often Ô¨Åne-tune these pre-
trained models on their downstream AI tasks using their
own private data to accomplish downstream tasks such as
malware detection [6], text-to-image generation [7]. However,
recently, it has been shown that these pre-trained models are
vulnerable to privacy attacks [8]. This problem is mainly
due to the model‚Äôs tendency to memorize training samples
without overÔ¨Åtting, also known as the ‚Äùmemorization issue‚Äù
[9]. This issue could lead to three major types of privacy
attacks: membership inference, model inversion, and training
data extraction.
Membership inference [10]: determines whether a certain
user‚Äôs data was included in the training.
Model inversion [11]: approximate the reconstruction of
the training data.
Training data extraction [8]: aims to exactly reveal the
training samples which makes this type of attack the most
powerful one with the most adverse consequences for the
users.
While all three types of attacks can jeopardize the privacy
of the users whose information is in the training data, training
data extraction directly targets users‚Äô personally identiÔ¨Åable
information and can endanger users‚Äô identity via revealing
important information such as their address, social security
number, phone number, etc. The Ô¨Åne-tuned LLMs used by
third parties on their private data will face the same privacy
1arXiv:2210.15042v3  [cs.CR]  20 Mar 2023

--- PAGE 2 ---
concerns. These privacy concerns around the issue necessitate
privacy-preserving approaches for Ô¨Åne-tuning LLMs. Such an
approach will allow third parties to privately Ô¨Åne-tune the
LLMs on their private data without any information leak about
their private training samples.
Differential Privacy (DP) is a promising approach to ensure
the training data privacy with theoretical guarantees [12]. DP
provides a mathematically rigorous framework with privacy
guarantees that enables Stochastic Gradient Descent (SGD),
the cornerstone of learning in LLMs, in a private setting.
In such a setting, SGD can be applied as a randomized
mechanism multiple times in each iteration of the training.
Most DP methods provide asymptotic guarantees. For theo-
retical guarantees, the number of SGD applications (known as
compositions) is often assumed to be unlimited in most privacy
studies. This assumption leads to asymptotic guarantees in
these studies (i.e., inÔ¨Ånite compositions of SGD in the limit).
However, in LLM Ô¨Åne-tuning the number of SGD iterations
is not only limited but also quite small (i.e., in the order of
several thousand) [13].
In this study, through a DP lens, and thanks to the Ô¨Ånite sam-
ple guarantee achieved by Edgeworth expansion [14], we pro-
pose a novel LLM Ô¨Åne-tuning framework, called EW-Tune ,
with Ô¨Ånite-sample guarantees. EW-Tune operates based on
an effective DP accounting approach known as Edgeworth
accountant, proposed in [14]. Edgeworth accountant computes
the amount of noise that is required to be added to the
gradients in SGD to guarantee a certain privacy budget (see
Section II-B). EW-Tune also leverages the latest efÔ¨Åcient
reparametrization technique proposed in [15].
A. Our contribution
While EW-Tune is a general framework, we showcase its
performance by focusing on its application to enhance the
privacy of LLM during Ô¨Åne-tuning. Our contribution to the
LLM‚Äôs private Ô¨Åne-tuning is two-fold:
Our study serves as the Ô¨Årst step towards Ô¨Åne-tuning
LLMs in a differentially private setting when the number
of compositions (i.e., the applications of differentially
private SGD) is Ô¨Ånite and limited to only several thousand
(less than 4,000 times in our experiments). Compared to
the existing methods that provide an asymptotic bound
on the privacy budget, through utilizing Edgeworth ac-
countant, EW-Tune is able to provide a non-asymptotic
privacy bound by using Berry-Esseen bound derived from
the Edgeworth approximation. In the case of Ô¨Åne-tuning
LLMs, given the Ô¨Ånite number of compositions, for
the same privacy budget, EW-Tune induces less noise
to SGD compared to the state-of-the-art. This directly
improves the learning and the accuracy of the model.
It is known that while Ô¨Åne-tuning via DP enhances the
model‚Äôs privacy, it can negatively affect the model‚Äôs
utility (i.e., performance) [12]. Our experiments show that
EW-Tune signiÔ¨Åcantly contributes to the state of the art
by enhancing the privacy of LLMs while preserving their
utility/accuracy compared to multiple recent alternativemethods across several important downstream benchmark
tasks including text classiÔ¨Åcation, entailment detection,
and question answering. Overall, EW-Tune decreases
the noise-induced to SGD up to 5.6%. EW-Tune also
enhances the state-of-the-art model‚Äôs accuracy by up to
1.1%.
II. B ACKGROUND AND RELATED WORK
We review three areas of the literature: (1) LLMs to identify
the state-of-the-art in language modeling and their Ô¨Åne-tuning.
(2) Differentially private deep learning as the overarching
framework to rigorously guarantee the privacy of Ô¨Åne-tuning
LLMs. (3) Edgeworth accountant as an emerging accountant
method that provides Ô¨Åne-sample guarantees, which could be
a useful tool for Ô¨Åne-tuning LLMs.
A. Large Language Models (LLMs)
Large language models are deep neural network archi-
tectures with billions of parameters [16]‚Äì[18]. They often
beneÔ¨Åt from an encoder-decoder architecture that generates
high-quality representations from sequence data (text, image,
malware, genes, etc.). Most LLMs use speciÔ¨Åc types of
layers with self-attention mechanisms known as transformers
to dynamically assign weights to input elements based on
their surrounding context [16]. Transformers enable LLMs to
provide high-quality representations of the input sequence. At
a high level, LLMs can be categorized into two types: masked
and autoregressive.
Masked language models are trained to predict a masked
token based on its surroundings. Highly effective examples
of masked language models include BERT [1] and RoBERTa
[16]. On the contrary, autoregressive language models learn to
predict the next token based on the previously generated ones,
which makes them suitable for text generation tasks [4], [19].
Due to their ability to produce high-quality representations
from input, masked language models are widely used in major
downstream AI tasks including text classiÔ¨Åcation, question
answering, semantic entailment detection, and speech recog-
nition.
Pre-trained LLMs are often Ô¨Åne-tuned on speciÔ¨Åc tasks and
datasets, through which the weights of the original model are
updated to better tune for the domain-speciÔ¨Åc data and task in
hand.
B. Differentially Private Deep Learning
Differential privacy [20], formally deÔ¨Åned in DeÔ¨Ånition 1,
computes a privacy guarantee when the results of an algorithm,
run on private data, are made public. When applied to machine
learning, a differentially private (DP) mechanism allows for
the public release of the model parameters while ensuring the
privacy of the original training data.
DeÔ¨Ånition 1: A randomized mechanism M:X !Y is
(;)-DP, if for all adjacent datasets X;X02X, differing in
a single element only, and all Y Y ,P(M(X)2Y)
eP(M(X0)2Y) +holds.
In DeÔ¨Ånition 1, (;)is often referred to as the privacy
budget.deÔ¨Ånes the distance between the two sides of
2

--- PAGE 3 ---
the equation and deÔ¨Ånes a failure probability. Differential
privacy enjoys from properties such as robustness to auxiliary
information and composability. The former guarantees privacy
even with the emergence of new side-information to the adver-
sary and the latter allows for modular design of mechanisms.
Essentially, composability implies that if two mechanisms
M1()andM2()are DP, and M(X) = (M1(X);M2(X)),
thenMis also differentially private.
Differentially private Stochastic Gradient Descent (DP-
SGD). The gold standard to achieve differential privacy in
deep learning is to update the neural network parameters with
noisy gradients. This is achieved by a randomized algorithm
called DP-SGD [12] via the two following steps:
‚ÄìGradient clipping: given a clipping norm C, the gra-
dient of each sample x,g(x), is clipped g0(x) 
g(x)=max(1;kg(x)k2
C).
‚ÄìGaussian mechanism: the clipped gradients are ag-
gregated and then an isotropic Gaussian noise from
N(0;C22), withas a noise multiplier, will be added
to the gradients.
The noise multiplier in DP-SGD is determined by the privacy
budget (;), the number of training rounds m, and the
sampling probability q=B=N for batch size BandNas
the total number of samples.
In their seminal work, Abadi et al. [12] introduced a
method called Moments accountant (MA) for computing an
upper bound for the privacy curve of compositions of DP
algorithms. The method was then used to track the privacy loss
of DP-SGD by computing the privacy curve for each training
iteration with itself mtimes, where mis the total number
of iterations. In [21], the MA framework is instantiated with
Renyi Differential Privacy (RDP). However, these algorithms,
while being efÔ¨Åcient (runtime is independent of m), provide
an upper bound which is rather impractical.
The Gaussian Differential Privacy (GDP) framework [22],
[23] also called f-DP is devised based on the central limit
theorem (CLT). The GDP framework offers a nice char-
acterization of differential privacy using hypothesis testing
interpretation [24]. GDP can only provide an approximation
for the privacy curve and it was shown to underreport the true
epsilon value in [25].
Using the notion of privacy loss random variables (PRV)
[26], Meiser and Mohammadi [27] introduced an algorithm
called privacy bucket for approximately composing privacy
curves. By employing the notion of PRV , one can utilize
the nice property of PRV to compute the composition of m
mechanisms M=M1M2Mmby simply summing
their corresponding PRVs D=Pm
i=1Di. The distribution of
Dcan then be approximated by computing the convolution of
its underlying distributions D1;:::;Dm. Koskela et al. [28]
used fast Fourier transform (FFT) to compute the convolution
efÔ¨Åciently. Following [28], Gopi et al. [25] leveraged FFT
to numerically compose trade-off functions. Their accountant,
called the PRV accountant addresses the underestimation of
f-DP and provides an upper-bound and lower-bound on the
leakage of.C. Edgeworth Accountant
As noted, at the heart of EW-Tune is Edgeworth ac-
countant [14]. Edgeworth accountant relies on f-DP [23],
which as discussed above, offers a full characterization of
differential privacy by utilizing hypothesis testing interpre-
tation. Informally, differential privacy measures the hardness
in distinguishing any pair of (neighboring) datasets based
on the information obtained from a mechanism M. In [23],
the authors formulated the notion of indistinguishably as a
hypothesis testing problem for two neighboring datasets Sand
S0. Therefore, the hypotheses are formed as H0: the underlying
dataset isSandH1: the underlying dataset is S0, where the
output ofMis the basis for conducting a hypothesis testing
problem. Let PandQdenote the probability distributions
ofM(S)andM(S0), respectively. Now, for a rejection rule
01, and hypotheses H0:PandH1:Q, the trade-
off function f=T(P;Q)() = inff:gdeÔ¨Ånes
the mapping from the Type-I error to Type-II error, where
=EP[]and= 1 EQ[]. To compute the composition
of the trade-off functions of the form f=Nm
i=1fi, let
us realize the i-th composition by two hypothesis H0;i=
wiPiandH1;i=wiQi. Now, to evaluate the
trade-off function f=Nm
i=1fi, we distinguish between two
composite hypothesis H0;i=wP1P2Pmand
H1;i=wQ1Q2Qmforw= (w1;:::;wm).
Edgeworth accountant [14] deÔ¨Ånes random variables called
privacy-loss log-likelihood ratios (PLLRs) to enable the loss-
less conversion of the f-DP guarantee into a collection of
(;)-DP guarantees. PLLRs are deÔ¨Åned as a Radon-Nikodym
derivatives of the hypotheses above as XilogdQi(i)
dPi(i)and
YilogdQi(i)
dPi(i)forPiandQi. The authors in
[14] showed the primal-dual relationship between f-DP and
a collection of (;())-DP via= 1 FY;m() e(1 
FX;m())whereFX;m andFY;m are the CDFs ofPm
i=1Xi
andPm
i=1Yi, respectively. To compute PLLRs through com-
posite hypotheses, the Edgeworth accountant uses a family
of PLLR sequences to compose the tightest possible trade-off
function that satisÔ¨Åes all f()-DP.
Assuming that for each , one can Ô¨Ånd a series of
PLLRs corresponding to f(), we can compute a collection
of(;()())-DP guarantee1. Then one has to compute an
approximate CDF as a random variable X=Pm
i=1Xiusing
Edgeworth expansion to output the Edgeworth accountant
approximates as FX;m andFY;m.
III. P ROPOSED METHOD
A. Threat Model
Adversary capabilities and objectives . We consider an adver-
saryAto have black-box access to the language model. In this
work, following [8], we assume that Adoes not have access
to the model‚Äôs speciÔ¨Åc weights and hidden states but is able to
obtain next-word predictions and compute the probability of
arbitrary sequences for instance via access to auto-complete
1The authors in [14] discuss how one can invert the equation to compute
afor a given
3

--- PAGE 4 ---
Edgeworth
Accountant
Private DataPrivacy 
Parameters (ùùê,ùúπ)EW-Tune
Public DataPrivately Fine -Tuned LLM
Noise Multiplier
(ùùà)Pre-trained LLM
DP-SGD
with RGPFig. 1. Abstract View of the Proposed EW-Tune Framework
models. The ultimate goal of the adversary is to extract (the
memorized) training data from the model. The severity of an
attack is increased if more examples could be extracted from
the model.
Adversary‚Äôs target and tasks . While EW-Tune is a general
framework that can be applied to enhance the privacy of any
LLM during Ô¨Åne-tuning. For speciÔ¨Åcity, we focus on one
of the most highly-adopted masked language models in AI
tasks, a successor of Google‚Äôs BERT, named roBERTa [16].
roBERTa mainly owes its popularity to its ability to learn
the bidirectional representation of the sentence. These high-
quality representations that are not available in autoregres-
sive language models such as GPT, speciÔ¨Åcally contribute
to breakthrough results in common downstream natural lan-
guage understanding (NLU) tasks such as sentiment analysis
and text categorization. Also, to show the generalizability
ofEW-Tune , we will test its utility and privacy guarantees
across four important and complex NLU tasks (all included in
the well-known General Language Understanding Evaluation
(GLUE) benchmark dataset [29]). Each task is associated with
a well-established dataset:
MNLI [30]: The Multi-Genre Natural Language Inference
(MNLI) is a collection of 433,000 sentence pairs that are
annotated with semantic entailment information [30]. The
LLM‚Äôs task in this corpus is to identify the semantic rela-
tionships between a given pair of sentences (entailment,
contradiction, or neutral relationship).
QNLI [29]: The Question-answering Naural Language
Inference (QNLI) is a natural language inference dataset
collected from Wikipedia that consists of 110,400
question-paragraph pairs, where only one of the sentences
in the paragraph is the answer to the corresponding
question. The LLM‚Äôs task is to determine whether a
sentence includes the answer to a given question.
QQP [29]: The Quora Question Pairs (QQP) dataset
includes over 400,000 question pairs. Each question pair
is annotated to indicate whether these questions aresemantically equivalent (i.e., paraphrase of each other).
The LLM‚Äôs task is to determine whether either of the
questions is the paraphrase of the other one.
SST-2 [31]: The Stanford Sentiment Treebank (SST-
2) includes 68,800 sentences from movie reviews and
annotations of their sentiment. The LLM‚Äôs task is to
predict the sentiment (positive or negative) of a given
sentence.
To operationalize defense against the above threat model,
we propose EW-Tune , a general framework for Ô¨Åne-tuning
LLMs for different downstream tasks. Figure 1 depicts the
components of our proposed EW-Tune framework.
Algorithm 1 EW-Tune Framework
1:Input: ExamplesfxigN
i=1, mechanismsfMigm
i=1, a
weight matricesfW(l)gH
l=1, warm-up steps Tw, group size
B, gradient clipping bound C, the failure probability and
an initial privacy budget and ak-th order Edgeworth
expansion.
2:Given the sampling probability q=B=N , for a given
, for each mechanism and all encode the PLLRs
[(X
i;Y
i)]and the cumulants up to order k+ 2
3:For eachcompute the Edgeworth approximation and
calculate()()and supremum sup()()
4:Given aninitand an arbitrary initial arb(e.g.,= 10 )
5:while (<initANDr< )do
6: Recompute(Steps 2-4) and reÔ¨Åne and reduce using
an initial reducing factor r(e.g.,r= 0:5)
7:end while
8:Choose a gradient-carrier matrix fW(l)gH
l=1according to
[15].
9:Sample a batch of examples with probability q.
10:Compute historical update to Ô¨Ånd gradient carrier and
decompose and compute the low-rank gradient carrier L
andRvia [15, Algorithm 2]
11:Run reparametrized forward/backward process and com-
pute individual gradients f@iL(l)
t;@iR(l)
tgl2[H];i2St
12:GivenCand, clip and add noise to the individual
gradients as in Section II-B to get ~@L(l)
tand~@R(l)
t
13:Construct ~@W(l)
t= (~@L)R+L(~@R) LLT(~@L)R
14:Output:h~@W(l)
t;(;)i
As seen in Figure 1, the pre-trained LLM is learned on
a public dataset (e.g., internet) from scratch by a major AI
company (e.g., Google and OpenAI) (shown in the left side
of Figure 1). Subsequently, the pre-trained model is used as
an input to EW-Tune to Ô¨Åne-tune with privacy guarantees ex-
pressed by privacy parameters (i.e., privacy budget 2[0;1)
and failure probability 2[0;1]) (as shown in the right
side of Figure 1). The privacy parameters (;)are provided
by the user/practitioner. A smaller indicates better privacy
preservation (and lower utility/performance). denotes the
probability that the training examples are accidentally being
leaked. In the context of LLM, a suitable value for is between
5 to 8, and is recommended to take a value in the order of
4

--- PAGE 5 ---
TABLE I
PERFORMANCE COMPARISON OF EW-T U N E AGAINST STATE OF THE ART DP METHODS ACROSS FOUR DIFFERENT NLU TASKS CONDUCTED BY
ROBERT ALLM
MethodMNLI QNLI QQP SST-2 MNLI QNLI QQP SST-2
Accuracy Noise Multiplier
RDP 81.25% 86.63% 84.60% 89.24% 0.65 0.829 0.6575 0.921
PRV 81.22% 86.79% 84.78% 91.82% 0.607 0.768 0.6135 0.8485
EW-Tune 81.81 % 87.71 % 84.91 % 92.19 % 0.573 0.739 0.579 0.8215
Performance reported for = 1e 6for MNLI, QNLI, and QQP; = 1e 5for SST-2; batch size = 2000
the inverse of the size of the training samples [13].
After the user provides the privacy parameters, the Edge-
worth accountant algorithm [14] is utilized to (1) compute the
number of compositions (i.e., applications of DP-SGD) for a
given dataset. (2) compute the amount of noise that guarantees
the given privacy budget. Subsequently, any variation of DP-
SGD [12], [15], as described in Section II-B, can be used to
Ô¨Åne-tune the LLM on the private dataset based on the appro-
priate noise multiplier obtained in the previous step. Due
to its breakthrough performance, we utilized a recent version
of the DP-SGD algorithm that is based on a new method
called reparameterized gradient perturbation (RGP) [15]. In the
original DP-SGD [12], the noise introduced highly depends on
the model parameters, and the per-example gradient clipping
results in very high memory and computation overhead. RGP
addresses the problems of DP-SGD by reparameterizing each
layer‚Äôs weight matrix Winto two low ranks gradient-carrier
matrices LandRand a residual weight matrix ~W. Finally,
all the transformer layers of the LLM will be Ô¨Åne-tuned
on the private data through DP-SGD. The output of the
EW-Tune framework is the Ô¨Åne-tuned LLM as shown in the
right side of Figure 1.
Algorithm 1 presents a more detailed version of our frame-
work ( EW-Tune ). The algorithm starts by computing an 
for a givenbased on the Edgeworth accountant explained in
Section II-C (Steps 2-4) by Ô¨Årst computing the PLLRs and then
approximating their CDF using Edgeworth expansion. Then
we compute ()()and its supremum. Next, in Steps 5-8, we
compute our Ô¨Ånal and an appropriate noise multiplier to
be used in our reparametrized gradient perturbation (i.e., noise
addition as in Section II-B). In Step 5, with each iteration
of the loop we compute = rand the initial tuning
factorris reduced by a constant factor (e.g., we use r=10in
our code). Finally, given the , the RGP algorithm efÔ¨Åciently
perturbs the updated parameters (Steps 9-14). For each update
with the weight matrix W, the algorithm works in four main
steps. In the Ô¨Årst step the gradient carrier matrices LandR
are generated via the decomposition method proposed in [15,
Algorithm 2]. The output of this step is the orthonormalize
version (via Gram-Schmidt orthonormalization process) of
the gradient carrier matrices. Next, the weight matrices are
reparametrized to compute and store individual gradients via
the forward/backward process presented in [15, Section 2]. In
the third step, the gradients are clipped and made noisy similar
to the DP-SGD method presented in Section II-B. Lastly, in
Step 14, the noisy aggregated gradients of the carrier matricesh~@W(l)
tare used to compute the gradients of the original
weights.
IV. E XPERIMENTS
We have open-sourced our optimized framework for wide
adoption and testing purposes at the following link.
https://github.com/star-ailab/LLM Tune
A. Experiment Setup
EW-Tune was developed and run on a single NVIDIA
GeForce RTX 3090 with 10,496 CUDA cores and 24 GB of
internal memory. To ensure that the LLM loading and Ô¨Åne-
tuning can take place in the internal memory we selected
the roBERTa.base pre-trained model with 125 million param-
eters. This LLM can be accessed from https://github.com/
facebookresearch/fairseq/tree/main/examples/roberta.
1) Parameter settings: Consistent with [29], we set the train
and test partition for each dataset: (MNLI: 393,000 for training
and 20,000 for testing; QNLI: 105,000 for training and 5400
for testing; QQP: 364,000 for training and 391,000 for testing,
SST-2: 67,000 for training and 1800 for testing).
To facilitate comparison, we set the privacy parameters and
consistent with [13]. Accordingly, we set = 8,= 1e 6
for larger datasets (i.e., MNLI, QNLI, and QQP; each with
several hundred thousand of samples) and = 1e 5for the
smaller dataset (i.e., SST-2; with tens of thousands of samples).
2) Benchmark Experiments: Following [13], [14] We eval-
uated the performance of the proposed EW-Tune framework
against two widely-used state-of-the-art DP alternatives: Renyi
Differential Privacy (RDP) [12], [21] and Privacy Loss Ran-
dom Variables (PRV) [25]. To rigorously evaluate EW-Tune ,
we conduct two sets of experiments (Section IV-B).
In Experiment 1, we evaluate the accuracy of the LLM
in solving the four mentioned NLU tasks (MNLI, QNLI,
QQP, SST-2) after Ô¨Åne-tuning with EW-Tune , RDP and
PRV . In Experiment 2, we evaluate the amount of noise
induced by alternative privacy accountant algorithms to that
ofEW-Tune for different values of .
B. Results
1) Experiment 1: Table I shows the results of comparing
the accuracy and noise multiplier of the proposed EW-Tune
against RDP and PRV across four NLU datasets (MNLI,
QNLI, QQP, and SST-2) carried out by the roBERTa LLM.
To report the performance, we repeated each experiment 3
times and reported the average. The highest accuracy in
5

--- PAGE 6 ---
Fig. 2. The Changes of Noise Multiplier based on different values of across
three Privacy Accounting Algorithms (RDP, PRV , and EW-Tune .)
performing each task appears in bold font. As seen in Table I,
EW-Tune yields the highest performance (81.81% on MNLI,
87.71% on QNLI, 84.91% on QQP, and 92.19% on SST-
2) among its counterparts. At the heart of EW-Tune , the
Edgeworth accountant utilizes an accurate privacy computation
method called f-differential privacy ( f-DP) along with Edge-
worth approximation, in place of CLT, that enjoys from a much
better convergence rate. This allows EW-Tune to achieve the
privacy guarantees by applying less noise to the transformer
layers of LLMs during training. The noise multiplier (i.e., the
standard deviation of Gaussian noise distribution) is shown on
the right side of Tabel I. As shown in Tabel I, EW-Tune yields
the lowest noise multiplier. More speciÔ¨Åcally, as compared to
the state-of-the-art, the noise multiplier is up to 4%, 3.2%,
5.6%, and 3.2% lower (for 58) for MNLI, QNLI,
QQP, and SST-2, respectively. In Tabel I, the highest perfor-
mance numbers and lowest noise multipliers are indicated in
boldface. We note that if instead of utilizing DP-SGD with
RGP [15], one were to instantiate these LLM‚Äôs with the
original DP-SGD [12], the smaller noise multiplier numbers
inEW-Tune would result in a much higher performance gap
with our counterparts.
2) Experiment 2: Experiment 2 evaluates the amount of
noise induced by EW-Tune and other benchmark privacy
accountant algorithms at = 1e 6for MNLI, QNLI, QQP,
and= 1e 5for SST-2. As noted, it is desirable to achieve
the same privacy budget ( ) by applying less noise to the
transformer layers of the LLM during the Ô¨Åne-tuning. As
shown in Figure IV-B2, EW-Tune yields the lowest amount of
noise across the benchmark privacy accounting methods (i.e.,
RDP and PRV). As shown in Figure IV-B2, when changes
from 5 to 8, EW-Tune induces the lowest amount of noise
into the SGD process for all four NLU tasks (MNLI, QNLI,
QQP, and SST-2).
Overall, The results of Experiments 1 and 2 on four complex
NLU task shows that EW-Tune is able to enhance the
performance thorough applying less noise to the SGD process,while achieving the same privacy budgets as its counterpart al-
gorithms. EW-Tune outperforms the other privacy accountant
methods for different values of privacy budget.
V. C ONCLUSION AND FUTURE WORK
In this work we presented a new framework called
EW-Tune , speciÔ¨Åcally designed for Ô¨Åne-tuning LLMs. By
utilizing the state-of-the-art privacy accountant and gradient
perturbation methods, EW-Tune is able to provide Ô¨Ånite-
sample privacy guarantee by introducing less noise as com-
pared to the existing methods. EW-Tune introduces up to 6%
less noise when privately training large language models which
contributes to up to 1.1% performance improvement. This can
contribute to addressing the gap in privacy and accuracy trade-
off in the realm of data privacy and AI.
An interesting future work would be to further study the
relationship between the introduced noise and training accu-
racy by focusing on the model‚Äôs total number of parameters,
dataset size, task objectives, and the number of compositions.
ACKNOWLEDGMENT
We would like to thank Hua Wang from the Statistics
Department at University of Pennsylvania for illuminating
discussions on Edgeworth accountant and helpful comments
on its implementation.
REFERENCES
[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training
of deep bidirectional transformers for language understanding,‚Äù arXiv
preprint arXiv:1810.04805 , 2018.
[2] D. Ham, J.-G. Lee, Y . Jang, and K.-E. Kim, ‚ÄúEnd-to-end neural pipeline
for goal-oriented dialogue systems using gpt-2,‚Äù in Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics ,
2020, pp. 583‚Äì592.
[3] L. Fang, T. Zeng, C. Liu, L. Bo, W. Dong, and C. Chen, ‚ÄúTransformer-
based conditional variational autoencoder for controllable story genera-
tion,‚Äù arXiv preprint arXiv:2101.00828 , 2021.
[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , ‚ÄúLanguage mod-
els are few-shot learners,‚Äù Advances in neural information processing
systems , vol. 33, pp. 1877‚Äì1901, 2020.
[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,
‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog ,
vol. 1, no. 8, p. 9, 2019.
[6] J. L. Hu, M. Ebrahimi, and H. Chen, ‚ÄúSingle-shot black-box adversarial
attacks against malware detectors: A causal language model approach,‚Äù
in2021 IEEE International Conference on Intelligence and Security
Informatics (ISI) . IEEE, 2021, pp. 1‚Äì6.
[7] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen,
and I. Sutskever, ‚ÄúZero-shot text-to-image generation,‚Äù in International
Conference on Machine Learning . PMLR, 2021, pp. 8821‚Äì8831.
[8] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee,
A. Roberts, T. B. Brown, D. Song, ¬¥U. Erlingsson, A. Oprea, and
C. Raffel, ‚ÄúExtracting training data from large language models,‚Äù in
30th USENIX Security Symposium, USENIX Security 2021, August 11-
13, 2021 , M. Bailey and R. Greenstadt, Eds. USENIX Association,
2021, pp. 2633‚Äì2650.
[9] N. Carlini, C. Liu, ¬¥U. Erlingsson, J. Kos, and D. Song, ‚ÄúThe secret
sharer: Evaluating and testing unintended memorization in neural net-
works,‚Äù in 28th USENIX Security Symposium (USENIX Security 19) ,
2019, pp. 267‚Äì284.
[10] S. Hisamoto, M. Post, and K. Duh, ‚ÄúMembership inference attacks on
sequence-to-sequence models: Is my data in your machine translation
system?‚Äù Transactions of the Association for Computational Linguistics ,
vol. 8, pp. 49‚Äì63, 2020.
6

--- PAGE 7 ---
[11] M. Fredrikson, S. Jha, and T. Ristenpart, ‚ÄúModel inversion attacks
that exploit conÔ¨Ådence information and basic countermeasures,‚Äù in
Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security , 2015, pp. 1322‚Äì1333.
[12] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, ‚ÄúDeep learning with differential privacy,‚Äù
inProceedings of the 2016 ACM SIGSAC Conference on Computer
and Communications Security, Vienna, Austria, October 24-28, 2016 ,
E. R. Weippl, S. Katzenbeisser, C. Kruegel, A. C. Myers, and
S. Halevi, Eds. ACM, 2016, pp. 308‚Äì318. [Online]. Available:
https://doi.org/10.1145/2976749.2978318
[13] D. Yu, S. Naik, A. Backurs, S. Gopi, H. A. Inan, G. Kamath, J. Kulkarni,
Y . T. Lee, A. Manoel, L. Wutschitz et al. , ‚ÄúDifferentially private Ô¨Åne-
tuning of language models,‚Äù arXiv preprint arXiv:2110.06500 , 2021.
[14] H. Wang, S. Gao, H. Zhang, M. Shen, and W. J. Su, ‚ÄúAnalytical
composition of differential privacy via the edgeworth accountant,‚Äù arXiv
preprint arXiv:2206.04236 , 2022.
[15] D. Yu, H. Zhang, W. Chen, J. Yin, and T.-Y . Liu, ‚ÄúLarge scale private
learning via low-rank reparametrization,‚Äù in International Conference on
Machine Learning . PMLR, 2021, pp. 12 208‚Äì12 218.
[16] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V . Stoyanov, ‚ÄúRoberta: A robustly optimized bert
pretraining approach,‚Äù arXiv preprint arXiv:1907.11692 , 2019.
[17] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, ‚Äúwav2vec 2.0: A frame-
work for self-supervised learning of speech representations,‚Äù Advances
in Neural Information Processing Systems , vol. 33, pp. 12 449‚Äì12 460,
2020.
[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, P. J. Liu et al. , ‚ÄúExploring the limits of transfer learning
with a uniÔ¨Åed text-to-text transformer.‚Äù J. Mach. Learn. Res. , vol. 21,
no. 140, pp. 1‚Äì67, 2020.
[19] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and
Q. V . Le, ‚ÄúXlnet: Generalized autoregressive pretraining for language
understanding,‚Äù Advances in neural information processing systems ,
vol. 32, 2019.
[20] C. Dwork, F. McSherry, K. Nissim, and A. D. Smith, ‚ÄúCalibrating
noise to sensitivity in private data analysis,‚Äù in Theory of Cryptography,
Third Theory of Cryptography Conference, TCC 2006, New York, NY,
USA, March 4-7, 2006, Proceedings , ser. Lecture Notes in Computer
Science, S. Halevi and T. Rabin, Eds., vol. 3876. Springer, 2006, pp.
265‚Äì284. [Online]. Available: https://doi.org/10.1007/11681878 14
[21] I. Mironov, ‚ÄúRenyi differential privacy,‚Äù CoRR , vol. abs/1702.07476,
2017. [Online]. Available: http://arxiv.org/abs/1702.07476
[22] Z. Bu, J. Dong, Q. Long, and W. J. Su, ‚ÄúDeep learning with gaussian
differential privacy,‚Äù CoRR , vol. abs/1911.11607, 2019. [Online].
Available: http://arxiv.org/abs/1911.11607
[23] J. Dong, A. Roth, and W. J. Su, ‚ÄúGaussian differential privacy,‚Äù
CoRR , vol. abs/1905.02383, 2019. [Online]. Available: http://arxiv.org/
abs/1905.02383
[24] P. Kairouz, S. Oh, and P. Viswanath, ‚ÄúThe composition theorem for
differential privacy,‚Äù in International conference on machine learning .
PMLR, 2015, pp. 1376‚Äì1385.
[25] S. Gopi, Y . T. Lee, and L. Wutschitz, ‚ÄúNumerical composition of
differential privacy,‚Äù in Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , M. Ranzato,
A. Beygelzimer, Y . N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021,
pp. 11 631‚Äì11 642. [Online]. Available: https://proceedings.neurips.cc/
paper/2021/hash/6097d8f3714205740f30debe1166744e-Abstract.html
[26] C. Dwork and G. N. Rothblum, ‚ÄúConcentrated differential privacy,‚Äù
CoRR , vol. abs/1603.01887, 2016. [Online]. Available: http://arxiv.org/
abs/1603.01887
[27] S. Meiser and E. Mohammadi, ‚ÄúTight on budget?: Tight bounds for
r-fold approximate differential privacy,‚Äù in Proceedings of the 2018
ACM SIGSAC Conference on Computer and Communications Security,
CCS 2018, Toronto, ON, Canada, October 15-19, 2018 , D. Lie,
M. Mannan, M. Backes, and X. Wang, Eds. ACM, 2018, pp. 247‚Äì264.
[Online]. Available: https://doi.org/10.1145/3243734.3243765
[28] A. Koskela, J. J ¬®alk¬®o, and A. Honkela, ‚ÄúComputing tight differential
privacy guarantees using FFT,‚Äù in The 23rd International Conference
on ArtiÔ¨Åcial Intelligence and Statistics, AISTATS 2020, 26-28
August 2020, Online [Palermo, Sicily, Italy] , ser. Proceedings of
Machine Learning Research, S. Chiappa and R. Calandra, Eds.,
vol. 108. PMLR, 2020, pp. 2560‚Äì2569. [Online]. Available:
http://proceedings.mlr.press/v108/koskela20b.html[29] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R.
Bowman, ‚ÄúGLUE: A multi-task benchmark and analysis platform
for natural language understanding,‚Äù in International Conference
on Learning Representations , 2019. [Online]. Available: https:
//openreview.net/forum?id=rJ4km2R5t7
[30] A. Williams, N. Nangia, and S. R. Bowman, ‚ÄúA broad-coverage
challenge corpus for sentence understanding through inference,‚Äù arXiv
preprint arXiv:1704.05426 , 2018.
[31] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng, and
C. Potts, ‚ÄúRecursive deep models for semantic compositionality over a
sentiment treebank,‚Äù in Proceedings of the 2013 conference on empirical
methods in natural language processing , 2013, pp. 1631‚Äì1642.
7
