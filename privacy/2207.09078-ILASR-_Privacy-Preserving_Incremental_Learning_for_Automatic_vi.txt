# 2207.09078.pdf
# ÄÆ°á»£c chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/privacy/2207.09078.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 955290 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
ILASR: Há»c TÄƒng Dáº§n Báº£o Máº­t Quyá»n RiÃªng TÆ° cho Nháº­n Diá»‡n Giá»ng NÃ³i Tá»± Äá»™ng á»Ÿ Quy MÃ´ Sáº£n Xuáº¥t
Gopinath Chennupati
Milind Rao
Gurpreet Chadha
Aaron Eakin
Amazon Alexa
USAAnirudh Raju
Gautam Tiwari
Anit Kumar Sahu
Ariya Rastrow
Jasha Droppo
Amazon Alexa
USAAndy Oberlin
Buddha Nandanoor
Prahalad Venkataramanan
Zheng Wu
Pankaj Sitpure
Amazon Alexa
USA

TÃ“M Táº®T
Há»c tÄƒng dáº§n lÃ  má»™t mÃ´ hÃ¬nh Ä‘á»ƒ kÃ­ch hoáº¡t xÃ¢y dá»±ng vÃ  cáº­p nháº­t mÃ´ hÃ¬nh á»Ÿ quy mÃ´ lá»›n vá»›i dá»¯ liá»‡u streaming. Äá»‘i vá»›i cÃ¡c nhiá»‡m vá»¥ nháº­n diá»‡n giá»ng nÃ³i tá»± Ä‘á»™ng (ASR) Ä‘áº§u cuá»‘i, viá»‡c thiáº¿u nhÃ£n Ä‘Æ°á»£c chÃº thÃ­ch bá»Ÿi con ngÆ°á»i cÃ¹ng vá»›i nhu cáº§u vá» cÃ¡c chÃ­nh sÃ¡ch báº£o máº­t quyá»n riÃªng tÆ° cho viá»‡c xÃ¢y dá»±ng mÃ´ hÃ¬nh khiáº¿n nÃ³ trá»Ÿ thÃ nh má»™t thÃ¡ch thá»©c Ä‘Ã¡ng gá»m. ÄÆ°á»£c thÃºc Ä‘áº©y bá»Ÿi nhá»¯ng thÃ¡ch thá»©c nÃ y, trong bÃ i bÃ¡o nÃ y chÃºng tÃ´i sá»­ dá»¥ng má»™t khung cÃ´ng tÃ¡c dá»±a trÃªn Ä‘Ã¡m mÃ¢y cho cÃ¡c há»‡ thá»‘ng sáº£n xuáº¥t Ä‘á»ƒ trÃ¬nh bÃ y nhá»¯ng hiá»ƒu biáº¿t tá»« há»c tÄƒng dáº§n báº£o máº­t quyá»n riÃªng tÆ° cho nháº­n diá»‡n giá»ng nÃ³i tá»± Ä‘á»™ng (ILASR). Báº±ng báº£o máº­t quyá»n riÃªng tÆ°, chÃºng tÃ´i cÃ³ nghÄ©a lÃ  viá»‡c sá»­ dá»¥ng dá»¯ liá»‡u táº¡m thá»i khÃ´ng Ä‘Æ°á»£c chÃº thÃ­ch bá»Ÿi con ngÆ°á»i. Há»‡ thá»‘ng nÃ y lÃ  má»™t bÆ°á»›c tiáº¿n cho cÃ¡c mÃ´ hÃ¬nh ASR cáº¥p sáº£n xuáº¥t cho há»c tÄƒng dáº§n/liÃªn tá»¥c mÃ  cung cáº¥p mÃ´i trÆ°á»ng thá»­ nghiá»‡m gáº§n thá»i gian thá»±c Ä‘á»ƒ thÃ­ nghiá»‡m trong Ä‘Ã¡m mÃ¢y cho ASR Ä‘áº§u cuá»‘i, trong khi tuÃ¢n thá»§ cÃ¡c chÃ­nh sÃ¡ch báº£o máº­t quyá»n riÃªng tÆ°. ChÃºng tÃ´i cho tháº¥y ráº±ng há»‡ thá»‘ng Ä‘Æ°á»£c Ä‘á» xuáº¥t cÃ³ thá»ƒ cáº£i thiá»‡n cÃ¡c mÃ´ hÃ¬nh sáº£n xuáº¥t Ä‘Ã¡ng ká»ƒ (3%) trong má»™t khoáº£ng thá»i gian má»›i sÃ¡u thÃ¡ng ngay cáº£ khi khÃ´ng cÃ³ nhÃ£n Ä‘Æ°á»£c chÃº thÃ­ch bá»Ÿi con ngÆ°á»i vá»›i cÃ¡c má»©c Ä‘á»™ giÃ¡m sÃ¡t yáº¿u khÃ¡c nhau vÃ  kÃ­ch thÆ°á»›c lÃ´ lá»›n trong há»c tÄƒng dáº§n. Sá»± cáº£i thiá»‡n nÃ y lÃ  20% trÃªn cÃ¡c bá»™ thá»­ nghiá»‡m vá»›i tá»« vÃ  cá»¥m tá»« má»›i trong khoáº£ng thá»i gian má»›i. ChÃºng tÃ´i chá»©ng minh hiá»‡u quáº£ cá»§a viá»‡c xÃ¢y dá»±ng mÃ´ hÃ¬nh theo cÃ¡ch tÄƒng dáº§n báº£o máº­t quyá»n riÃªng tÆ° cho ASR trong khi tiáº¿p tá»¥c khÃ¡m phÃ¡ tÃ­nh há»¯u Ã­ch cá»§a viá»‡c cÃ³ má»™t mÃ´ hÃ¬nh giÃ¡o viÃªn hiá»‡u quáº£ vÃ  sá»­ dá»¥ng kÃ­ch thÆ°á»›c lÃ´ lá»›n.

KHÃI NIá»†M CCS
â€¢PhÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n â†’Nháº­n diá»‡n giá»ng nÃ³i ;Máº¡ng nÆ¡-ron ;CÃ i Ä‘áº·t há»c bÃ¡n giÃ¡m sÃ¡t ;â€¢Báº£o máº­t vÃ  quyá»n riÃªng tÆ°â†’Giao thá»©c báº£o máº­t quyá»n riÃªng tÆ° .

Tá»ª KHÃ“A
Há»c TÄƒng Dáº§n, Nháº­n Diá»‡n Giá»ng NÃ³i Tá»± Äá»™ng, Há»c MÃ¡y Báº£o Máº­t Quyá»n RiÃªng TÆ°

ÄÆ°á»£c phÃ©p táº¡o báº£n sao ká»¹ thuáº­t sá»‘ hoáº·c in cá»§a toÃ n bá»™ hoáº·c má»™t pháº§n cÃ´ng trÃ¬nh nÃ y Ä‘á»ƒ sá»­ dá»¥ng cÃ¡ nhÃ¢n hoáº·c lá»›p há»c mÃ  khÃ´ng tÃ­nh phÃ­ vá»›i Ä‘iá»u kiá»‡n cÃ¡c báº£n sao khÃ´ng Ä‘Æ°á»£c táº¡o ra hoáº·c phÃ¢n phá»‘i vÃ¬ lá»£i nhuáº­n hoáº·c lá»£i tháº¿ thÆ°Æ¡ng máº¡i vÃ  cÃ¡c báº£n sao mang thÃ´ng bÃ¡o nÃ y vÃ  trÃ­ch dáº«n Ä‘áº§y Ä‘á»§ trÃªn trang Ä‘áº§u tiÃªn. Báº£n quyá»n cho cÃ¡c thÃ nh pháº§n cá»§a cÃ´ng trÃ¬nh nÃ y thuá»™c sá»Ÿ há»¯u cá»§a nhá»¯ng ngÆ°á»i khÃ¡c ngoÃ i ACM pháº£i Ä‘Æ°á»£c tÃ´n trá»ng. TÃ³m táº¯t cÃ³ ghi cÃ´ng Ä‘Æ°á»£c cho phÃ©p. Äá»ƒ sao chÃ©p náº¿u khÃ´ng, hoáº·c xuáº¥t báº£n láº¡i, Ä‘á»ƒ Ä‘Äƒng trÃªn mÃ¡y chá»§ hoáº·c Ä‘á»ƒ phÃ¢n phá»‘i láº¡i Ä‘áº¿n danh sÃ¡ch, cáº§n cÃ³ sá»± cho phÃ©p cá»¥ thá»ƒ trÆ°á»›c vÃ /hoáº·c má»™t khoáº£n phÃ­. YÃªu cáº§u quyá»n tá»« permissions@acm.org.
KDD '22, August 14â€“18, 2022, Washington, DC, USA
Â©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00
https://doi.org/10.1145/3534678.3539174

Äá»‹nh dáº¡ng Tham chiáº¿u ACM:
Gopinath Chennupati, Milind Rao, Gurpreet Chadha, Aaron Eakin, Anirudh
Raju, Gautam Tiwari, Anit Kumar Sahu, Ariya Rastrow, Jasha Droppo,
vÃ  Andy Oberlin, Buddha Nandanoor, Prahalad Venkataramanan, Zheng
Wu, Pankaj Sitpure. 2022. ILASR: Há»c TÄƒng Dáº§n Báº£o Máº­t Quyá»n RiÃªng TÆ°
cho Nháº­n Diá»‡n Giá»ng NÃ³i Tá»± Äá»™ng á»Ÿ Quy MÃ´ Sáº£n Xuáº¥t. Trong Tuyá»ƒn táº­p
Há»™i nghá»‹ láº§n thá»© 28 cá»§a ACM SIGKDD vá» KhÃ¡m phÃ¡ Tri thá»©c vÃ  Khai thÃ¡c Dá»¯ liá»‡u
(KDD '22), August 14â€“18, 2022, Washington, DC, USA. ACM, New York, NY,
USA, 10 trang. https://doi.org/10.1145/3534678.3539174

1 GIá»šI THIá»†U
Há»c mÃ¡y báº£o máº­t quyá»n riÃªng tÆ° [1] Ä‘Ã£ á»Ÿ tiá»n tuyáº¿n, do cáº£ sá»± quan tÃ¢m gia tÄƒng vá» quyá»n riÃªng tÆ° vÃ  kháº£ nÄƒng nháº¡y cáº£m tiá»m áº©n cá»§a máº¡ng nÆ¡-ron sÃ¢u Ä‘á»‘i vá»›i rÃ² rá»‰ vÃ  táº¥n cÃ´ng. Há»c LiÃªn káº¿t (FL) [44] lÃ  má»™t ká»¹ thuáº­t há»c mÃ¡y liÃªn quan Ä‘áº¿n viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn cÃ¡c thiáº¿t bá»‹ biÃªn, nÆ¡i dá»¯ liá»‡u khÃ´ng cáº§n rá»i khá»i thiáº¿t bá»‹, vÃ  cÃ³ thá»ƒ khÃ´ng Ä‘á»“ng nháº¥t vÃ  khÃ´ng phÃ¢n phá»‘i Ä‘á»™c láº­p vÃ  giá»‘ng há»‡t nhau (non-IID). Trong FL, nhiá»u cáº­p nháº­t mÃ´ hÃ¬nh tá»« má»™t sá»‘ thiáº¿t bá»‹ tham gia Ä‘Æ°á»£c tá»•ng há»£p. Máº·c dÃ¹ dá»¯ liá»‡u thÃ´ khÃ´ng rá»i khá»i thiáº¿t bá»‹ biÃªn, FL Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t hiá»‡n lÃ  dá»… bá»‹ táº¥n cÃ´ng Ä‘áº£o ngÆ°á»£c gradient [65,66]. Äá»ƒ Ä‘Ã¡p á»©ng, nhiá»u cÆ¡ cháº¿ báº£o máº­t quyá»n riÃªng tÆ° khÃ¡c nhau nhÆ° quyá»n riÃªng tÆ° vi sai vÃ  tá»•ng há»£p an toÃ n [19, 58] Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ chá»‘ng láº¡i rÃ² rá»‰ dá»¯ liá»‡u vÃ  tuÃ¢n thá»§ cÃ¡c cÆ¡ cháº¿ báº£o máº­t quyá»n riÃªng tÆ°. HÆ¡n ná»¯a, viá»‡c thiáº¿u nhÃ£n cho dá»¯ liá»‡u cÃ³ máº·t trong cÃ¡c thá»±c thá»ƒ tham gia, khiáº¿n FL trá»Ÿ nÃªn khÃ³ khÄƒn hÆ¡n Ä‘á»‘i vá»›i cÃ¡c á»©ng dá»¥ng nhÆ° nháº­n diá»‡n giá»ng nÃ³i tá»± Ä‘á»™ng (ASR). Háº§u háº¿t nghiÃªn cá»©u trong FL cho Ä‘áº¿n nay táº­p trung vÃ o viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»« Ä‘áº§u.

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i táº­p trung vÃ o há»c tÄƒng dáº§n báº£o máº­t quyá»n riÃªng tÆ° (IL), trong bá»‘i cáº£nh xÃ¢y dá»±ng mÃ´ hÃ¬nh sáº£n xuáº¥t Ä‘áº§u cuá»‘i á»Ÿ quy mÃ´ lá»›n trong cÃ¡c khoáº£ng thá»i gian má»Ÿ rá»™ng. Há»c tÄƒng dáº§n [8,62] Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i Ä‘á»ƒ cáº­p nháº­t mÃ´ hÃ¬nh tÄƒng dáº§n khi Ä‘ang cháº¡y thay vÃ¬ huáº¥n luyá»‡n chÃºng tá»« Ä‘áº§u. Há»c tÄƒng dáº§n nhÆ° váº­y khÃ´ng báº£o máº­t quyá»n riÃªng tÆ°.

Máº·c dÃ¹ cÃ³ nhá»¯ng tiáº¿n bá»™ trÃªn, theo hiá»ƒu biáº¿t tá»‘t nháº¥t cá»§a chÃºng tÃ´i, Ã­t khung cÃ´ng tÃ¡c tá»“n táº¡i cho viá»‡c huáº¥n luyá»‡n tÄƒng dáº§n báº£o máº­t quyá»n riÃªng tÆ° cá»§a cÃ¡c mÃ´ hÃ¬nh nháº­n diá»‡n giá»ng nÃ³i tá»± Ä‘á»™ng Ä‘áº§u cuá»‘i. CÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y vá» há»c liÃªn káº¿t cho cÃ¡c nhiá»‡m vá»¥ dá»±a trÃªn giá»ng nÃ³i [13,16,23] vÃ  ASR Ä‘áº§u cuá»‘i [18,26], táº­p trung vÃ o cÃ¡c tiÃªu chuáº©n chuáº©nÂ¹ vÃ  khÃ´ng pháº£i trÃªn dá»¯ liá»‡u sáº£n xuáº¥t quy mÃ´ lá»›n. IL báº£o máº­t quyá»n riÃªng tÆ° trÃªn thiáº¿t bá»‹ cho ASR Ä‘áº§u cuá»‘i Ä‘áº·t ra má»™t sá»‘ thÃ¡ch thá»©c. CÃ¡c há»‡ thá»‘ng ASR Ä‘áº§u cuá»‘i cá»¡ sáº£n xuáº¥t [11,24] tá»‘n kÃ©m Ä‘á»ƒ huáº¥n luyá»‡n ngay cáº£ trong thiáº¿t láº­p phÃ¢n tÃ¡n truyá»n thá»‘ng, huáº¥n luyá»‡n trÃªn thiáº¿t bá»‹ cáº§n thÃªm cÃ´ng viá»‡c [7] Ä‘á»ƒ thÃ­ch á»©ng vá»›i cÃ¡c rÃ ng buá»™c bá»™ nhá»› vÃ  tÃ­nh toÃ¡n háº¡n cháº¿. Táº¡o nhÃ£n huáº¥n luyá»‡n tá»©c lÃ  báº£n chÃ©p lá»i giá»ng nÃ³i, gáº§n thá»i gian thá»±c, trÃªn cÃ¡c thiáº¿t bá»‹ lÃ  má»™t thÃ¡ch thá»©c khÃ¡c. Äá»ƒ giáº£m bá»›t sá»± khÃ´ng kháº£ dá»¥ng cá»§a báº£n chÃ©p lá»i giá»ng nÃ³i gáº§n thá»i gian thá»±c, báº£n chÃ©p giÃ¡o viÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng theo cÃ¡ch bÃ¡n giÃ¡m sÃ¡t vÃ /hoáº·c tá»± há»c. VÃ­ dá»¥, xem xÃ©t váº¥n Ä‘á» cáº£i thiá»‡n cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c triá»ƒn khai trong cÃ¡c thiáº¿t bá»‹ biÃªn cháº¡y trá»£ lÃ½ giá»ng nÃ³i. Trong nhá»¯ng trÆ°á»ng há»£p nhÆ° váº­y, sá»‘ lÆ°á»£ng thiáº¿t bá»‹ hÃ ng triá»‡u, dáº«n Ä‘áº¿n quy mÃ´ lá»›n dá»¯ liá»‡u streaming Ä‘Æ°á»£c táº¡o ra. ChÃºng tÃ´i Ä‘á» xuáº¥t sá»­ dá»¥ng xá»­ lÃ½ lÃ´ lá»›n cho cÃ¡c phÃ¡t ngÃ´n Ä‘Æ°á»£c thu tháº­p táº¡i cÃ¡c thiáº¿t bá»‹ biÃªn vÃ  gá»­i Ä‘áº¿n Ä‘Ã¡m mÃ¢y Ä‘á»ƒ xá»­ lÃ½, vÃ  dá»¯ liá»‡u chá»‰ Ä‘Æ°á»£c lÆ°u trá»¯ táº¡m thá»i. Tuy nhiÃªn, viá»‡c triá»ƒn khai táº¥t cáº£ hoáº·c má»™t pháº§n cá»§a cÃ¡c thÃ nh pháº§n trÃªn vÃ o cÃ¡c thiáº¿t bá»‹ giá»ng nÃ³i háº¡n cháº¿ tÃ i nguyÃªn (nhÆ° Alexa, Google Assistant vÃ  nhá»¯ng thiáº¿t bá»‹ khÃ¡c) lÃ  thÃ¡ch thá»©c.

ChÃºng tÃ´i xÃ¢y dá»±ng vÃ  sá»­ dá»¥ng má»™t há»‡ thá»‘ng dá»±a trÃªn Ä‘Ã¡m mÃ¢y, Há»c TÄƒng Dáº§n cho Nháº­n Diá»‡n Giá»ng NÃ³i Tá»± Äá»™ng (ILASR) Ä‘á»ƒ huáº¥n luyá»‡n vÃ  cáº­p nháº­t cÃ¡c mÃ´ hÃ¬nh ASR sáºµn sÃ ng sáº£n xuáº¥t. ILASR tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ pipeline cá»§a há»c tÄƒng dáº§n theo cÃ¡ch báº£o máº­t quyá»n riÃªng tÆ°. Äá»ƒ thá»±c thi cÃ¡c khÃ­a cáº¡nh báº£o máº­t quyá»n riÃªng tÆ° trong bá»‘i cáº£nh ASR, chÃºng tÃ´i thá»±c thi viá»‡c gÃ¡n nhÃ£n cÃ¡c phÃ¡t ngÃ´n thÃ´ng qua cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c khÃ´ng cÃ³ chÃº thÃ­ch cá»§a con ngÆ°á»i. ILASR xá»­ lÃ½ má»™t phÃ¡t ngÃ´n má»™t láº§n trÆ°á»›c khi cáº­p nháº­t mÃ´ hÃ¬nh, báº£o tá»“n thá»© tá»± thá»i gian cá»§a dá»¯ liá»‡u. Vá» Ä‘iá»u Ä‘Ã³, nhá»¯ng Ä‘Ã³ng gÃ³p cá»§a bÃ i bÃ¡o lÃ :

â€¢Má»™t há»‡ thá»‘ng IL dá»±a trÃªn Ä‘Ã¡m mÃ¢y má»›i Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ASR sáºµn sÃ ng sáº£n xuáº¥t gáº§n thá»i gian thá»±c, vá»›i má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u streaming Ä‘Æ°á»£c khá»­ danh tÃ­nh, mÃ  khÃ´ng cáº§n pháº£i chÃ©p tay hoáº·c lÆ°u trá»¯ Ã¢m thanh.

â€¢ChÃºng tÃ´i cung cáº¥p nhá»¯ng hiá»ƒu biáº¿t má»›i vá» viá»‡c sá»­ dá»¥ng xá»­ lÃ½ lÃ´ lá»›n trong ILASR ráº±ng nÃ³ khÃ´ng cÃ³ tÃ¡c Ä‘á»™ng cÃ³ háº¡i Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m so vá»›i nhá»¯ng phÃ¡t hiá»‡n mÃ¢u thuáº«n [20, 33,37,41,42,52] (trÃªn CNN ImageNet). ChÃºng tÃ´i cÃ³ thá»ƒ thÃ­ch á»©ng vá»›i tá»· lá»‡ há»c cá»‘ Ä‘á»‹nh vÃ  tá»‘i Æ°u hÃ³a siÃªu tham sá»‘ tá»‘i thiá»ƒu [34] cÃ¹ng vá»›i huáº¥n luyá»‡n lÃ´ lá»›n. Vá»›i táº§n suáº¥t hÃ ng thÃ¡ng cá»§a cÃ¡c cáº­p nháº­t mÃ´ hÃ¬nh tÄƒng dáº§n, chÃºng tÃ´i quan sÃ¡t tháº¥y cÃ¡c mÃ´ hÃ¬nh sáº£n xuáº¥t (há»™i tá»¥ trÃªn dá»¯ liá»‡u cÅ©) cáº£i thiá»‡n gáº§n thá»i gian thá»±c trÃªn dá»¯ liá»‡u má»›i thuá»™c khoáº£ng thá»i gian sÃ¡u thÃ¡ng

â€¢ChÃºng tÃ´i thiáº¿t láº­p thá»±c nghiá»‡m trong sÃ¡u thÃ¡ng dá»¯ liá»‡u ráº±ng thá»© tá»± thá»i gian so vá»›i thá»© tá»± ngáº«u nhiÃªn cá»§a viá»‡c xá»­ lÃ½ phÃ¡t ngÃ´n khÃ´ng táº¡o ra báº¥t ká»³ sá»± khÃ¡c biá»‡t cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c trong hiá»‡u suáº¥t.

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ ILASR trÃªn ba kiáº¿n trÃºc transducer máº¡ng nÆ¡-ron há»“i quy sinh viÃªn (RNN-T) [24]. PhÆ°Æ¡ng phÃ¡p há»c bÃ¡n giÃ¡m sÃ¡t (SSL) táº¡o ra báº£n chÃ©p mÃ¡y sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh ASR giÃ¡o viÃªn lá»›n hÆ¡n. CÃ¡c sinh viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn dá»¯ liá»‡u khá»­ danh tÃ­nh ná»™i bá»™ cho Ä‘áº¿n 2020. ThÃ´ng qua huáº¥n luyá»‡n trong ILASR, chÃºng tÃ´i quan sÃ¡t tháº¥y sá»± cáº£i thiá»‡n 3âˆ’7% trong tá»· lá»‡ lá»—i tá»« (WER) so vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c khi nhá»¯ng sinh viÃªn nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n tÄƒng dáº§n trÃªn khoáº£ng thá»i gian má»›i sÃ¡u thÃ¡ng trong 2021. Sá»± cáº£i thiá»‡n trong WER Ä‘Æ°á»£c gá»i lÃ  giáº£m tá»· lá»‡ lá»—i tá»« tÆ°Æ¡ng Ä‘á»‘i (WERR). Äiá»u nÃ y tÄƒng lÃªn 20% trÃªn cÃ¡c bá»™ thá»­ nghiá»‡m vá»›i tá»« vÃ  cá»¥m tá»« má»›i trong 2021. TÆ°Æ¡ng tá»±, khi cÃ¡c mÃ´ hÃ¬nh sinh viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n tÄƒng dáº§n má»—i thÃ¡ng, chÃºng tÃ´i quan sÃ¡t tháº¥y sá»± cáº£i thiá»‡n WER, cÅ©ng nhÆ° hiá»‡n tÆ°á»£ng cÃ¡c mÃ´ hÃ¬nh trá»Ÿ nÃªn cÅ© ká»¹ mÃ  khÃ´ng cÃ³ cáº­p nháº­t thÃªm.

BÃ i bÃ¡o Ä‘Æ°á»£c tá»• chá»©c nhÆ° sau: pháº§n 2 mÃ´ táº£ cÃ¡c khÃ¡i niá»‡m thiáº¿t yáº¿u Ä‘Æ°á»£c sá»­ dá»¥ng trong bÃ i bÃ¡o; pháº§n 3 giáº£i thÃ­ch há»‡ thá»‘ng Ä‘Æ°á»£c Ä‘á» xuáº¥t; pháº§n 4 mÃ´ táº£ cÃ¡c cÃ i Ä‘áº·t thÃ­ nghiá»‡m; pháº§n 5 trÃ¬nh bÃ y káº¿t quáº£; pháº§n 6 tÃ³m táº¯t tÃ i liá»‡u liÃªn quan vÃ  cuá»‘i cÃ¹ng, pháº§n 7 káº¿t luáº­n vÃ  khuyáº¿n nghá»‹ cÃ¡c hÆ°á»›ng tÆ°Æ¡ng lai.

2 Ná»€N Táº¢NG
Trong pháº§n nÃ y chÃºng tÃ´i tÃ³m táº¯t kiáº¿n trÃºc RNN-T vÃ  huáº¥n luyá»‡n lÃ´ lá»›n vá»›i gradient descent ngáº«u nhiÃªn (SGD).

2.1 Kiáº¿n trÃºc mÃ´ hÃ¬nh RNN-T
HÃ¬nh 1 cho tháº¥y kiáº¿n trÃºc RNN-T [24] Ä‘Æ°á»£c sá»­ dá»¥ng trong nháº­n diá»‡n giá»ng nÃ³i thá»i gian thá»±c. MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n xÃ¡c suáº¥t ğ‘ƒ(y|x) cá»§a nhÃ£n y=(ğ‘¦1,...,ğ‘¦ğ‘ˆ) Ä‘Æ°á»£c cho cÃ¡c Ä‘áº·c trÆ°ng Ã¢m thanh x=(ğ‘¥1,...,ğ‘¥ğ‘‡). NÃ³ cÃ³ má»™t bá»™ mÃ£ hÃ³a, má»™t máº¡ng dá»± Ä‘oÃ¡n, vÃ  má»™t máº¡ng káº¿t há»£p. Bá»™ mÃ£ hÃ³a tÆ°Æ¡ng tá»± nhÆ° má»™t mÃ´ hÃ¬nh Ã¢m thanh nháº­n má»™t chuá»—i cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o Ã¢m thanh vÃ  xuáº¥t ra cÃ¡c biá»ƒu diá»…n áº©n Ä‘Æ°á»£c mÃ£ hÃ³a. Máº¡ng dá»± Ä‘oÃ¡n tÆ°Æ¡ng á»©ng vá»›i má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ cháº¥p nháº­n cÃ¡c dá»± Ä‘oÃ¡n nhÃ£n Ä‘áº§u ra trÆ°á»›c Ä‘Ã³, vÃ  Ã¡nh xáº¡ chÃºng thÃ nh cÃ¡c biá»ƒu diá»…n áº©n. Máº¡ng káº¿t há»£p lÃ  má»™t DNN feed forward nháº­n cáº£ Ä‘áº§u ra bá»™ mÃ£ hÃ³a vÃ  máº¡ng dá»± Ä‘oÃ¡n, vÃ  dá»± Ä‘oÃ¡n xÃ¡c suáº¥t nhÃ£n Ä‘áº§u ra cuá»‘i cÃ¹ng vá»›i chuáº©n hÃ³a softmax.

HÃ¬nh 1: Kiáº¿n trÃºc mÃ´ hÃ¬nh ASR RNN-T

2.2 Tá»•ng quan vá» há»c vá»›i kÃ­ch thÆ°á»›c lÃ´ lá»›n
Khi huáº¥n luyá»‡n vá»›i SGD, cÃ¡c mini batch vá»›i lá»‹ch trÃ¬nh tá»· lá»‡ há»c suy giáº£m Ä‘Æ°á»£c táº¡o tÃ¡c ká»¹ lÆ°á»¡ng thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng thay vÃ¬ sá»­ dá»¥ng cÃ¡c lÃ´ lá»›n. CÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y trong [33] Ä‘Ã£ chá»©ng minh sá»± sá»¥t giáº£m khÃ¡i quÃ¡t hÃ³a khi sá»­ dá»¥ng cÃ¡c lÃ´ lá»›n, do Ä‘Ã³ khuyÃªn nÃªn SGD mini-batch vá»›i tá»· lá»‡ há»c suy giáº£m. Tuy nhiÃªn, nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y trong huáº¥n luyá»‡n lÃ´ lá»›n cáº£ vá»›i quy táº¯c má»Ÿ rá»™ng tuyáº¿n tÃ­nh cá»§a tá»· lá»‡ há»c [22] vÃ  tá»· lá»‡ há»c háº±ng sá»‘ [53], huáº¥n luyá»‡n lÃ´ lá»›n Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng tá»± nhÆ° Ä‘á»‘i tÃ¡c mini-batch cá»§a nÃ³.

Má»™t quan sÃ¡t láº·p láº¡i trong tÃ i liá»‡u [20,33,37,41,42,52] lÃ  huáº¥n luyá»‡n lÃ´ lá»›n (cho ImageNet, >1000) dáº«n Ä‘áº¿n suy giáº£m Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m. Máº·c dÃ¹ cÃ³ giai Ä‘oáº¡n khá»Ÿi Ä‘á»™ng trong [22], cho ImageNet, Ä‘á»™ chÃ­nh xÃ¡c tá»‘t nháº¥t Ä‘Æ°á»£c quan sÃ¡t Ä‘áº¿n mini-batch lá»›n cá»§a 8192 hÃ¬nh áº£nh.

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á»‘i phÃ³ vá»›i nhá»¯ng thÃ¡ch thá»©c cá»§a 1) huáº¥n luyá»‡n vá»›i cÃ¡c lÃ´ lá»›n trong há»c tÄƒng dáº§n vÃ  2) há»c bÃ¡n giÃ¡m sÃ¡t Ä‘á»ƒ giáº£m bá»›t sá»± khÃ´ng kháº£ dá»¥ng cá»§a chÃº thÃ­ch vÃ  nhÃ£n cá»§a con ngÆ°á»i. Äá»‘i vá»›i nháº­n diá»‡n giá»ng nÃ³i tá»± Ä‘á»™ng (ASR), vá»›i kÃ­ch thÆ°á»›c lÃ´ lá»›n (>3ğ‘’5 phÃ¡t ngÃ´n) sá»­ dá»¥ng lá»‹ch trÃ¬nh tá»· lá»‡ há»c cá»‘ Ä‘á»‹nh, chÃºng tÃ´i quan sÃ¡t tháº¥y Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m tá»‘t hÆ¡n, trÃ¡i ngÆ°á»£c vá»›i sá»± suy giáº£m trong tÃ i liá»‡u, trong khi huáº¥n luyá»‡n vá»›i báº£n chÃ©p giÃ¡o viÃªn cho dá»¯ liá»‡u Ã¢m thanh tÄƒng dáº§n.

--- TRANG 2 ---
KDD '22, August 14â€“18, 2022, Washington, DC, USA Gopinath Chennupati et al.

Ä‘Ã o táº¡o phÃ¢n tÃ¡n, Ä‘Ã o táº¡o trÃªn thiáº¿t bá»‹ cáº§n thÃªm cÃ´ng viá»‡c [7] Ä‘á»ƒ thÃ­ch á»©ng vá»›i cÃ¡c rÃ ng buá»™c bá»™ nhá»› vÃ  tÃ­nh toÃ¡n háº¡n cháº¿. Táº¡o nhÃ£n huáº¥n luyá»‡n tá»©c lÃ  báº£n chÃ©p lá»i giá»ng nÃ³i, gáº§n thá»i gian thá»±c, trÃªn cÃ¡c thiáº¿t bá»‹ lÃ  má»™t thÃ¡ch thá»©c khÃ¡c. Äá»ƒ giáº£m bá»›t sá»± khÃ´ng kháº£ dá»¥ng cá»§a báº£n chÃ©p lá»i giá»ng nÃ³i gáº§n thá»i gian thá»±c, báº£n chÃ©p giÃ¡o viÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng theo cÃ¡ch bÃ¡n giÃ¡m sÃ¡t vÃ /hoáº·c tá»± há»c. VÃ­ dá»¥, xem xÃ©t váº¥n Ä‘á» cáº£i thiá»‡n cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c triá»ƒn khai trong cÃ¡c thiáº¿t bá»‹ biÃªn cháº¡y trá»£ lÃ½ giá»ng nÃ³i. Trong nhá»¯ng trÆ°á»ng há»£p nhÆ° váº­y, sá»‘ lÆ°á»£ng thiáº¿t bá»‹ hÃ ng triá»‡u, dáº«n Ä‘áº¿n quy mÃ´ lá»›n dá»¯ liá»‡u streaming Ä‘Æ°á»£c táº¡o ra. ChÃºng tÃ´i Ä‘á» xuáº¥t sá»­ dá»¥ng xá»­ lÃ½ lÃ´ lá»›n cho cÃ¡c phÃ¡t ngÃ´n Ä‘Æ°á»£c thu tháº­p táº¡i cÃ¡c thiáº¿t bá»‹ biÃªn vÃ  gá»­i Ä‘áº¿n Ä‘Ã¡m mÃ¢y Ä‘á»ƒ xá»­ lÃ½, vÃ  dá»¯ liá»‡u chá»‰ Ä‘Æ°á»£c lÆ°u trá»¯ táº¡m thá»i. Tuy nhiÃªn, viá»‡c triá»ƒn khai táº¥t cáº£ hoáº·c má»™t pháº§n cá»§a cÃ¡c thÃ nh pháº§n trÃªn vÃ o cÃ¡c thiáº¿t bá»‹ giá»ng nÃ³i háº¡n cháº¿ tÃ i nguyÃªn (nhÆ° Alexa, Google Assistant vÃ  nhá»¯ng thiáº¿t bá»‹ khÃ¡c) lÃ  thÃ¡ch thá»©c.

ChÃºng tÃ´i xÃ¢y dá»±ng vÃ  sá»­ dá»¥ng má»™t há»‡ thá»‘ng dá»±a trÃªn Ä‘Ã¡m mÃ¢y, Há»c TÄƒng Dáº§n cho Nháº­n Diá»‡n Giá»ng NÃ³i Tá»± Äá»™ng (ILASR) Ä‘á»ƒ huáº¥n luyá»‡n vÃ  cáº­p nháº­t cÃ¡c mÃ´ hÃ¬nh ASR sáºµn sÃ ng sáº£n xuáº¥t. ILASR tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ pipeline cá»§a há»c tÄƒng dáº§n theo cÃ¡ch báº£o máº­t quyá»n riÃªng tÆ°. Äá»ƒ thá»±c thi cÃ¡c khÃ­a cáº¡nh báº£o máº­t quyá»n riÃªng tÆ° trong bá»‘i cáº£nh ASR, chÃºng tÃ´i thá»±c thi viá»‡c gÃ¡n nhÃ£n cÃ¡c phÃ¡t ngÃ´n thÃ´ng qua cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c khÃ´ng cÃ³ chÃº thÃ­ch cá»§a con ngÆ°á»i. ILASR xá»­ lÃ½ má»™t phÃ¡t ngÃ´n má»™t láº§n trÆ°á»›c khi cáº­p nháº­t mÃ´ hÃ¬nh, báº£o tá»“n thá»© tá»± thá»i gian cá»§a dá»¯ liá»‡u. Vá» Ä‘iá»u Ä‘Ã³, nhá»¯ng Ä‘Ã³ng gÃ³p cá»§a bÃ i bÃ¡o lÃ :

â€¢Má»™t há»‡ thá»‘ng IL dá»±a trÃªn Ä‘Ã¡m mÃ¢y má»›i Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ASR sáºµn sÃ ng sáº£n xuáº¥t gáº§n thá»i gian thá»±c, vá»›i má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u streaming Ä‘Æ°á»£c khá»­ danh tÃ­nh, mÃ  khÃ´ng cáº§n pháº£i chÃ©p tay hoáº·c lÆ°u trá»¯ Ã¢m thanh.

â€¢ChÃºng tÃ´i cung cáº¥p nhá»¯ng hiá»ƒu biáº¿t má»›i vá» viá»‡c sá»­ dá»¥ng xá»­ lÃ½ lÃ´ lá»›n trong ILASR ráº±ng nÃ³ khÃ´ng cÃ³ tÃ¡c Ä‘á»™ng cÃ³ háº¡i Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m so vá»›i nhá»¯ng phÃ¡t hiá»‡n mÃ¢u thuáº«n [20, 33,37,41,42,52] (trÃªn CNN ImageNet). ChÃºng tÃ´i cÃ³ thá»ƒ thÃ­ch á»©ng vá»›i tá»· lá»‡ há»c cá»‘ Ä‘á»‹nh vÃ  tá»‘i Æ°u hÃ³a siÃªu tham sá»‘ tá»‘i thiá»ƒu [34] cÃ¹ng vá»›i huáº¥n luyá»‡n lÃ´ lá»›n. Vá»›i táº§n suáº¥t hÃ ng thÃ¡ng cá»§a cÃ¡c cáº­p nháº­t mÃ´ hÃ¬nh tÄƒng dáº§n, chÃºng tÃ´i quan sÃ¡t tháº¥y cÃ¡c mÃ´ hÃ¬nh sáº£n xuáº¥t (há»™i tá»¥ trÃªn dá»¯ liá»‡u cÅ©) cáº£i thiá»‡n gáº§n thá»i gian thá»±c trÃªn dá»¯ liá»‡u má»›i thuá»™c khoáº£ng thá»i gian sÃ¡u thÃ¡ng

â€¢ChÃºng tÃ´i thiáº¿t láº­p thá»±c nghiá»‡m trong sÃ¡u thÃ¡ng dá»¯ liá»‡u ráº±ng thá»© tá»± thá»i gian so vá»›i thá»© tá»± ngáº«u nhiÃªn cá»§a viá»‡c xá»­ lÃ½ phÃ¡t ngÃ´n khÃ´ng táº¡o ra báº¥t ká»³ sá»± khÃ¡c biá»‡t cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c trong hiá»‡u suáº¥t.

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ ILASR trÃªn ba kiáº¿n trÃºc transducer máº¡ng nÆ¡-ron há»“i quy sinh viÃªn (RNN-T) [24]. PhÆ°Æ¡ng phÃ¡p há»c bÃ¡n giÃ¡m sÃ¡t (SSL) táº¡o ra báº£n chÃ©p mÃ¡y sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh ASR giÃ¡o viÃªn lá»›n hÆ¡n. CÃ¡c sinh viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn dá»¯ liá»‡u khá»­ danh tÃ­nh ná»™i bá»™ cho Ä‘áº¿n 2020. ThÃ´ng qua huáº¥n luyá»‡n trong ILASR, chÃºng tÃ´i quan sÃ¡t tháº¥y sá»± cáº£i thiá»‡n 3âˆ’7% trong tá»· lá»‡ lá»—i tá»« (WER) so vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c khi nhá»¯ng sinh viÃªn nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n tÄƒng dáº§n trÃªn khoáº£ng thá»i gian má»›i sÃ¡u thÃ¡ng trong 2021. Sá»± cáº£i thiá»‡n trong WER Ä‘Æ°á»£c gá»i lÃ  giáº£m tá»· lá»‡ lá»—i tá»« tÆ°Æ¡ng Ä‘á»‘i (WERR). Äiá»u nÃ y tÄƒng lÃªn 20% trÃªn cÃ¡c bá»™ thá»­ nghiá»‡m vá»›i tá»« vÃ  cá»¥m tá»« má»›i trong 2021. TÆ°Æ¡ng tá»±, khi cÃ¡c mÃ´ hÃ¬nh sinh viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n tÄƒng dáº§n má»—i thÃ¡ng, chÃºng tÃ´i quan sÃ¡t tháº¥y sá»± cáº£i thiá»‡n WER, cÅ©ng nhÆ° hiá»‡n tÆ°á»£ng cÃ¡c mÃ´ hÃ¬nh trá»Ÿ nÃªn cÅ© ká»¹ mÃ  khÃ´ng cÃ³ cáº­p nháº­t thÃªm.

BÃ i bÃ¡o Ä‘Æ°á»£c tá»• chá»©c nhÆ° sau: pháº§n 2 mÃ´ táº£ cÃ¡c khÃ¡i niá»‡m thiáº¿t yáº¿u Ä‘Æ°á»£c sá»­ dá»¥ng trong bÃ i bÃ¡o; pháº§n 3 giáº£i thÃ­ch há»‡ thá»‘ng Ä‘Æ°á»£c Ä‘á» xuáº¥t; pháº§n 4 mÃ´ táº£ cÃ¡c cÃ i Ä‘áº·t thÃ­ nghiá»‡m; pháº§n 5 trÃ¬nh bÃ y káº¿t quáº£; pháº§n 6 tÃ³m táº¯t tÃ i liá»‡u liÃªn quan vÃ  cuá»‘i cÃ¹ng, pháº§n 7 káº¿t luáº­n vÃ  khuyáº¿n nghá»‹ cÃ¡c hÆ°á»›ng tÆ°Æ¡ng lai.

3 ILASR: Há»ŒC TÄ‚NG Dáº¦N CHO NHáº¬N DIá»†N GIá»ŒNG NÃ“I Tá»° Äá»˜NG
Pháº§n nÃ y mÃ´ táº£ kiáº¿n trÃºc ILASR vÃ  thuáº­t toÃ¡n há»c tÄƒng dáº§n tÆ°Æ¡ng á»©ng. ILASR cung cáº¥p huáº¥n luyá»‡n ASR Ä‘áº§u cuá»‘i quy mÃ´ lá»›n vá»›i kháº£ nÄƒng cáº­p nháº­t tÄƒng dáº§n cÃ¡c mÃ´ hÃ¬nh trong cÃ¡c cá»­a sá»• thá»i gian do ngÆ°á»i dÃ¹ng Ä‘á»‹nh nghÄ©a. ILASR tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ chu ká»³ sá»‘ng cá»§a táº¡o dá»¯ liá»‡u, láº¥y máº«u, gÃ¡n nhÃ£n, phÃ¡t triá»ƒn mÃ´ hÃ¬nh, Ä‘Ã¡nh giÃ¡ vÃ  triá»ƒn khai cho dá»¯ liá»‡u Ã¢m thanh gáº§n thá»i gian thá»±c.

HÃ¬nh 2: Khung xÆ°Æ¡ng cáº¥p cao cá»§a kiáº¿n trÃºc ILASR

3.1 Kiáº¿n trÃºc ILASR
HÃ¬nh 2 cho tháº¥y tá»•ng quan kiáº¿n trÃºc cá»§a há»‡ thá»‘ng ILASR. Há»‡ thá»‘ng bao gá»“m ba thÃ nh pháº§n chÃ­nh: (1) Bá»™ tiá»n xá»­ lÃ½ dá»¯ liá»‡u â€“ lÃ  má»™t dá»‹ch vá»¥ runtime Ä‘Ã¡m mÃ¢y xá»­ lÃ½ Ã¢m thanh gáº§n thá»i gian thá»±c tá»« thiáº¿t bá»‹; (2) IL Core chá»‹u trÃ¡ch nhiá»‡m cho huáº¥n luyá»‡n mÃ´ hÃ¬nh, tÃ­nh toÃ¡n cÃ¡c cáº­p nháº­t mÃ´ hÃ¬nh vÃ  suy luáº­n; vÃ  (3) IL Orchestrator tá»•ng há»£p cÃ¡c gradient tÃ­ch lÅ©y, cáº­p nháº­t mÃ´ hÃ¬nh, thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡ vÃ  hoÃ n thiá»‡n cáº­p nháº­t mÃ´ hÃ¬nh dá»±a trÃªn káº¿t quáº£ Ä‘Ã¡nh giÃ¡.

Train launcher khá»Ÿi táº¡o huáº¥n luyá»‡n ASR Ä‘áº§u cuá»‘i trong ILASR. BÆ°á»›c Ä‘áº§u tiÃªn lÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u Ä‘á»ƒ chá»n má»™t táº­p con cÃ¡c thiáº¿t bá»‹ vÃ  phÃ¡t ngÃ´n tham gia vÃ o vÃ²ng láº·p huáº¥n luyá»‡n. Viá»‡c lá»±a chá»n cÃ³ thá»ƒ ngáº«u nhiÃªn hoáº·c dá»±a trÃªn heuristics nháº±m cáº£i thiá»‡n mÃ´ hÃ¬nh theo má»™t cÃ¡ch cá»¥ thá»ƒ. Äiá»ƒm tin cáº­y thu Ä‘Æ°á»£c trong quÃ¡ trÃ¬nh suy luáº­n Ä‘Æ°á»£c sá»­ dá»¥ng [29,30] káº¿t há»£p vá»›i heuristics nhÆ° sá»± hiá»‡n diá»‡n cá»§a tá»« hiáº¿m hoáº·c tháº» ngá»¯ nghÄ©a vÃ  Ã½ Ä‘á»‹nh quan tÃ¢m. Viá»‡c lá»±a chá»n nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ táº­n dá»¥ng cÃ¡c tÃ­n hiá»‡u yáº¿u tá»« pháº£n há»“i ngÆ°á»i dÃ¹ng nhÆ° ngÆ°á»i dÃ¹ng chá»‰ ra liá»‡u hÃ nh Ä‘á»™ng Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi trá»£ lÃ½ lÃ  tÃ­ch cá»±c hay tiÃªu cá»±c hoáº·c phÃ¡t hiá»‡n ma sÃ¡t nhÆ° yÃªu cáº§u láº·p láº¡i hoáº·c há»§y bá». CÃ¡c Ä‘áº·c trÆ°ng Ã¢m thanh Ä‘Æ°á»£c trÃ­ch xuáº¥t vÃ  tÄƒng cÆ°á»ng [48] cho cÃ¡c phÃ¡t ngÃ´n Ä‘Æ°á»£c chá»n Ä‘á»ƒ huáº¥n luyá»‡n. Báº£n chÃ©p mÃ¡y Ä‘Æ°á»£c táº¡o ra báº±ng má»™t mÃ´ hÃ¬nh ASR giÃ¡o viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c sá»­ dá»¥ng huáº¥n luyá»‡n phÃ¢n tÃ¡n tiÃªu chuáº©n. MÃ´ hÃ¬nh ASR giÃ¡o viÃªn Ä‘áº§u cuá»‘i dá»±a trÃªn Conformer [25] giáº£i mÃ£ Ã¢m thanh Ä‘áº§u vÃ o (ğ‘‹) Ä‘á»ƒ táº¡o ra báº£n chÃ©p mÃ¡y (ğ‘Œ). Nhá»¯ng cáº·p (ğ‘‹,ğ‘Œ) nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh. Báº£n chÃ©p mÃ¡y hoáº¡t Ä‘á»™ng nhÆ° nhÃ£n sá»± tháº­t cÆ¡ báº£n. ILASR táº¡o ra cÃ¡c phiÃªn chÃ©p thÃ´ng qua tá»± Ä‘á»™ng hÃ³a an toÃ n mÃ  khÃ´ng cÃ³ sá»± can thiá»‡p hoáº·c xem xÃ©t cá»§a con ngÆ°á»i. CÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c trÃ­ch xuáº¥t cÃ¹ng vá»›i báº£n chÃ©p mÃ¡y trong bÆ°á»›c nÃ y Ä‘Æ°á»£c káº¿t há»£p Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh sinh viÃªn sá»­ dá»¥ng IL Core.

IL Core system cÃ³ má»™t giao diá»‡n láº­p trÃ¬nh á»©ng dá»¥ng (API) há»— trá»£ tÃ­ch lÅ©y gradient cá»¥c bá»™ trÃªn má»—i mÃ¡y chá»§ trong fleet, vÃ  má»™t engine suy luáº­n ASR. IL Core API há»— trá»£ FedSGD vÃ  FedAvg [44] vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ há»— trá»£ cÃ¡c trÃ¬nh tá»‘i Æ°u liÃªn káº¿t khÃ¡c nhÆ° FedProx [51], FedMA [59], FedNova [60], vÃ  trÃ¬nh tá»‘i Æ°u liÃªn káº¿t thÃ­ch á»©ng [50]. IL Orchestrator Ä‘iá»u phá»‘i huáº¥n luyá»‡n trÃªn fleet ILASR. IL Orchestrator chá»©a bá»™ xuáº¥t báº£n gradient, bá»™ tá»•ng há»£p vÃ  cáº­p nháº­t mÃ´ hÃ¬nh tÄƒng dáº§n. Bá»™ tá»•ng há»£p gradient thu tháº­p gradient tá»« má»—i instance IL Core, tá»•ng há»£p chÃºng vÃ  sau Ä‘Ã³ Ã¡p dá»¥ng chÃºng cho mÃ´ hÃ¬nh hiá»‡n táº¡i. Khi cáº­p nháº­t mÃ´ hÃ¬nh hoÃ n thÃ nh, cÃ¡c gradient Ä‘Æ°á»£c thu tháº­p sáº½ bá»‹ loáº¡i bá» vÃ  khÃ´ng Ä‘Æ°á»£c lÆ°u trá»¯ trong há»‡ thá»‘ng Ä‘iá»u nÃ y giÃºp giáº£m nguy cÆ¡ táº¥n cÃ´ng Ä‘áº£o ngÆ°á»£c gradient. Má»™t Ä‘Ã¡nh giÃ¡ nháº¹ Ä‘á»‹nh ká»³ cá»§a mÃ´ hÃ¬nh Ä‘áº£m báº£o ráº±ng mÃ´ hÃ¬nh Ä‘ang cáº£i thiá»‡n theo hÆ°á»›ng Ä‘Ãºng. MÃ´ hÃ¬nh toÃ n cá»¥c Ä‘Æ°á»£c cáº­p nháº­t trong má»™t vÃ²ng nháº¥t Ä‘á»‹nh khi hiá»‡u suáº¥t cáº£i thiá»‡n so vá»›i vÃ²ng trÆ°á»›c Ä‘Ã³. Äá»ƒ giáº£m xÃ¡c suáº¥t cáº­p nháº­t mÃ´ hÃ¬nh dáº«n Ä‘áº¿n hiá»‡u suáº¥t tá»‡ hÆ¡n, ILASR cÃ³ thá»ƒ Ä‘Æ°á»£c cháº¡y song song vá»›i cÃ¡c siÃªu tham sá»‘ khÃ¡c nhau. Trong ká»‹ch báº£n nÃ y, má»™t trong nhá»¯ng mÃ´ hÃ¬nh káº¿t quáº£ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng náº¿u nÃ³ dáº«n Ä‘áº¿n hiá»‡u suáº¥t cáº£i thiá»‡n. Sau má»™t sá»‘ vÃ²ng Ä‘á»§, mÃ´ hÃ¬nh cuá»‘i cÃ¹ng Ä‘Æ°á»£c lÆ°u trá»¯ cho báº£n phÃ¡t hÃ nh mÃ´ hÃ¬nh tiáº¿p theo sau má»™t bÆ°á»›c xÃ¡c thá»±c mÃ´ hÃ¬nh chi tiáº¿t.

ILASR giáº£i quyáº¿t cÃ¡c má»‘i quan tÃ¢m vá» báº£o máº­t vÃ  quyá»n riÃªng tÆ° vá»›i cÃ¡c má»©c Ä‘á»™ chi tiáº¿t khÃ¡c nhau. VÃ¬ ILASR lÃ  má»™t há»‡ thá»‘ng dá»±a trÃªn Ä‘Ã¡m mÃ¢y cho IL báº£o máº­t quyá»n riÃªng tÆ° á»Ÿ quy mÃ´ lá»›n, mÃ£ hÃ³a Ã¢m thanh gá»“m hai lá»›p. á» giai Ä‘oáº¡n Ä‘áº§u tiÃªn, mÃ£ hÃ³a TLS [15] Ä‘Æ°á»£c Ã¡p dá»¥ng cho truyá»n Ã¢m thanh tiáº¿p theo lÃ  mÃ£ hÃ³a key-master [40] cáº¥p á»©ng dá»¥ng. Quan trá»ng lÃ , Ã¢m thanh Ä‘Æ°á»£c xÃ³a trong vÃ i phÃºt (â‰¤10), trong Ä‘Ã³ cÃ¡c cáº­p nháº­t mÃ´ hÃ¬nh Ä‘Æ°á»£c tÃ­nh toÃ¡n.

3.2 ILASR: Há»c TÄƒng Dáº§n
Thuáº­t toÃ¡n 1 Thuáº­t toÃ¡n há»c tÄƒng dáº§n ILASR
YÃªu cáº§u: K mÃ¡y chá»§, L hÃ m máº¥t mÃ¡t, ğ‘ sá»‘ bÆ°á»›c cá»¥c bá»™ má»—i vÃ²ng,
B kÃ­ch thÆ°á»›c lÃ´ cá»¥c bá»™, (ğœ‚) tá»· lá»‡ há»c, ğ‘ƒğ‘Ÿğ‘˜ phÃ¡t ngÃ´n gáº§n Ä‘Ã¢y Ä‘Æ°á»£c kÃ©o bá»Ÿi
mÃ¡y chá»§ ğ‘˜ trong vÃ²ng ğ‘Ÿ, Dğ‘’ğ‘£ğ‘ğ‘™ táº­p Ä‘Ã¡nh giÃ¡ vÃ  Dâ„ğ‘¡ dá»¯ liá»‡u phiÃªn chÃ©p quÃ¡ khá»© náº¿u
Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n Ã´n táº­p.
Äáº£m báº£o: ğ‘¤ğ‘Ÿğº mÃ´ hÃ¬nh toÃ n cá»¥c Ä‘Æ°á»£c cáº­p nháº­t tÄƒng dáº§n vÃ  ğ‘¤ğ‘’ğ‘Ÿğ‘Ÿ tá»· lá»‡ lá»—i tá»«
trÃªn táº­p Ä‘Ã¡nh giÃ¡ sau ğ‘Ÿ vÃ²ng
1: Khá»Ÿi táº¡o. ğ‘¤0ğº // báº¯t Ä‘áº§u huáº¥n luyá»‡n vá»›i má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c
2: ğ‘¤ğ‘’ğ‘Ÿ0 = ğ‘ğ‘ ğ‘Ÿ_ğ‘–ğ‘›ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’_ğ‘’ğ‘›ğ‘”ğ‘–ğ‘›ğ‘’(Dğ‘’ğ‘£ğ‘ğ‘™, ğ‘¤Tğº)
3: for má»—i vÃ²ng ğ‘Ÿ = 1, 2, . . . do
4: for má»—i mÃ¡y chá»§ ğ‘˜ âˆˆ Fleet ILASR song song do
5: ğ‘¤ğ‘Ÿğ‘˜ = ğ‘¤ğ‘Ÿâˆ’1ğº
6: Dğ‘ ğ‘ ğ‘™ â† (lá»c ğ‘ƒğ‘Ÿğ‘˜ dá»±a trÃªn tiÃªu chÃ­ lá»±a chá»n phÃ¡t ngÃ´n vÃ  táº¡o
báº£n chÃ©p mÃ¡y, tham kháº£o thuáº­t toÃ¡n 2)
7: Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› â† (trá»™n Dğ‘ ğ‘ ğ‘™ vÃ  Dâ„ğ‘¡ náº¿u Dâ„ğ‘¡ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ã´n táº­p, náº¿u khÃ´ng
chá»‰ Dğ‘ ğ‘ ğ‘™)
8: Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› â† (chia Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› thÃ nh ğ‘ lÃ´ cÃ³ kÃ­ch thÆ°á»›c B)
9: for má»—i lÃ´ ğ‘ğ‘– tá»« ğ‘1 Ä‘áº¿n ğ‘ğ‘ do
10: ğ‘¤ğ‘Ÿğ‘˜ â† optimizerğ‘˜.update(ğœ‚, âˆ‡L(ğ‘¤; ğ‘ğ‘–))
11: end for
12: end for
13: ğ‘¤ğ‘Ÿğº â† 1/K âˆ‘Kğ‘˜=1 ğ‘¤ğ‘Ÿğ‘˜
14: ğ‘¤ğ‘’ğ‘Ÿğ‘Ÿ â† ğ‘ğ‘ ğ‘Ÿ_ğ‘–ğ‘›ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’_ğ‘’ğ‘›ğ‘”ğ‘–ğ‘›ğ‘’(Dğ‘’ğ‘£ğ‘ğ‘™, ğ‘¤ğ‘Ÿğº)
15: ğ‘¤ğ‘Ÿğº = ğ‘¤ğ‘Ÿâˆ’1ğº náº¿u ğ‘¤ğ‘’ğ‘Ÿğ‘Ÿ > ğ‘¤ğ‘’ğ‘Ÿğ‘Ÿâˆ’1 // Trá»Ÿ láº¡i mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³ náº¿u khÃ´ng
pháº£i lÃ  mÃ´ hÃ¬nh tá»‘t hÆ¡n.
16: end for

--- TRANG 3 ---
ILASR: Há»c TÄƒng Dáº§n Báº£o Máº­t Quyá»n RiÃªng TÆ° cho Nháº­n Diá»‡n Giá»ng NÃ³i Tá»± Äá»™ng á»Ÿ Quy MÃ´ Sáº£n Xuáº¥t KDD '22, August 14â€“18, 2022, Washington, DC, USA

Thuáº­t toÃ¡n 1 cho tháº¥y chÃ­nh sÃ¡ch há»c tÄƒng dáº§n trong khung cÃ´ng tÃ¡c ILASR. MÃ´ hÃ¬nh má»›i thu Ä‘Æ°á»£c trong má»—i vÃ²ng chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng náº¿u nÃ³ hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n mÃ´ hÃ¬nh tá»« vÃ²ng trÆ°á»›c Ä‘Ã³. CÃ¡c láº§n cháº¡y song song cá»§a thuáº­t toÃ¡n vá»›i cÃ¡c siÃªu tham sá»‘ khÃ¡c nhau Ä‘á»ƒ huáº¥n luyá»‡n má»™t ensemble cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c cáº­p nháº­t tÄƒng dáº§n cÃ³ thá»ƒ Ä‘áº£m báº£o ráº±ng cÃ³ Ã­t nháº¥t má»™t mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n mÃ´ hÃ¬nh tá»« vÃ²ng trÆ°á»›c Ä‘Ã³. Má»™t cÃ¢n nháº¯c thÃº vá»‹ khÃ¡c lÃ  hiá»‡u á»©ng cá»§a viá»‡c quÃªn tháº£m khá»‘c [17,21,43] trong há»c tÄƒng dáº§n cá»§a khung cÃ´ng tÃ¡c ILASR, nÆ¡i hÃ nh vi Ä‘Ã£ há»c trÆ°á»›c Ä‘Ã³ cá»§a má»™t mÃ´ hÃ¬nh bá»‹ quÃªn vá»›i cÃ¡c cáº­p nháº­t má»›i. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£m thiá»ƒu vá»›i viá»‡c Ã´n táº­p [2] cá»§a viá»‡c huáº¥n luyá»‡n trÃªn má»™t táº­p con dá»¯ liá»‡u lá»‹ch sá»­ Ä‘Æ°á»£c chÃº thÃ­ch cÃ¹ng vá»›i dá»¯ liá»‡u má»›i.

ChÃºng tÃ´i mÃ´ táº£ phÆ°Æ¡ng phÃ¡p táº¡o dá»¯ liá»‡u SSL trong thuáº­t toÃ¡n 2. ChÃºng tÃ´i láº¥y máº«u ngáº«u nhiÃªn má»™t táº­p con Ã¢m thanh gáº§n thá»i gian thá»±c, Ä‘á»ƒ chuáº©n bá»‹ má»™t pool dá»¯ liá»‡u (P), vÃ  tÃ­nh toÃ¡n sá»‘ lÆ°á»£ng phÃ¡t ngÃ´n má»¥c tiÃªu (U) cáº§n Ä‘Æ°á»£c láº¥y máº«u tá»« P, nÆ¡i má»—i phÃ¡t ngÃ´n bao gá»“m má»™t giÃ¡ trá»‹ tin cáº­y Ä‘Æ°á»£c tÃ­nh toÃ¡n trÆ°á»›c [57]. Äá»‘i vá»›i má»—i bin tin cáº­y, vÃ­ dá»¥ tin cáº­y trong (600,700] nÆ¡i tin cáº­y Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn thang Ä‘iá»ƒm tá»« 0 Ä‘áº¿n 1000, cÃ¡c phÃ¡t ngÃ´n Ä‘Æ°á»£c lá»c Ä‘á»ƒ tuÃ¢n thá»§ tiÃªu chÃ­ tin cáº­y. CÃ¡c phÃ¡t ngÃ´n Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn tá»« trÃªn Ä‘Æ°á»£c Ä‘áº·t Ä‘á»ƒ láº¥y sá»‘ lÆ°á»£ng phÃ¡t ngÃ´n má»¥c tiÃªu vÃ  gá»­i Ä‘áº¿n IL core Ä‘á»ƒ huáº¥n luyá»‡n, chÃºng Ä‘Æ°á»£c xÃ³a ngay khi mÃ´ hÃ¬nh thá»±c hiá»‡n má»™t láº§n qua nÃ³ láº§n Ä‘áº§u tiÃªn. CÃ¡c tiÃªu chÃ­ bá»• sung nhÆ° sá»± hiá»‡n diá»‡n cá»§a tá»« hiáº¿m, sá»± hiá»‡n diá»‡n cá»§a tháº» ngá»¯ nghÄ©a mong muá»‘n cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng.

Thuáº­t toÃ¡n 2 Quy trÃ¬nh lá»±a chá»n dá»¯ liá»‡u SSL
YÃªu cáº§u: ğœ danh sÃ¡ch cÃ¡c bin tin cáº­y phÃ¡t ngÃ´n
Äáº£m báº£o: X táº­p dá»¯ liá»‡u
1: P = ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘œğ‘š_ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’() // chuáº©n bá»‹ má»™t pool dá»¯ liá»‡u ngáº«u nhiÃªn
2: P = ğ‘¡ğ‘’ğ‘ğ‘â„ğ‘’ğ‘Ÿ_ğ‘‘ğ‘’ğ‘ğ‘œğ‘‘ğ‘’(P) // táº¡o báº£n chÃ©p mÃ¡y
3: U = ğ‘ğ‘ğ‘™ğ‘_ğ‘¢ğ‘¡ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘ğ‘’_ğ‘ğ‘œğ‘›ğ‘“ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘ğ‘’(P).
4: Q = [] // má»™t bin cho má»—i pháº¡m vi tin cáº­y
5: for ğ‘ âˆˆ ğœ do
6: Q[ğ‘] = ğ‘“ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ_ğ‘¢ğ‘¡ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘ğ‘’ğ‘ (U, ğ‘)
7: X = ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡_ğ‘¢ğ‘¡ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘ğ‘’ğ‘ (Q) // cÃ³ thá»ƒ bao gá»“m cÃ¡c tiÃªu chÃ­ bá»• sung
nhÆ° sá»± hiá»‡n diá»‡n cá»§a tá»« hiáº¿m hoáº·c tháº» ngá»¯ nghÄ©a mong muá»‘n
8: end for

4 THá»°C NGHIá»†M
ChÃºng tÃ´i mÃ´ táº£ cÃ¡c táº­p dá»¯ liá»‡u, cáº¥u hÃ¬nh mÃ´ hÃ¬nh vÃ  cÃ i Ä‘áº·t thÃ­ nghiá»‡m Ä‘Æ°á»£c sá»­ dá»¥ng trong bÃ i bÃ¡o nÃ y, Ä‘á»ƒ cung cáº¥p hiá»ƒu biáº¿t vÃ  nghiÃªn cá»©u há»c tÄƒng dáº§n báº£o máº­t quyá»n riÃªng tÆ° thÃ´ng qua ILASR.

4.1 Táº­p dá»¯ liá»‡u
Táº¥t cáº£ dá»¯ liá»‡u giá»ng nÃ³i Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ Ä‘á»u Ä‘Æ°á»£c khá»­ danh tÃ­nh.

Táº­p huáº¥n luyá»‡n CÃ¡c luá»“ng Ã¢m thanh Ä‘Æ°á»£c chuáº©n bá»‹ thÃ nh cÃ¡c táº­p dá»¯ liá»‡u huáº¥n luyá»‡n offline. CÃ¡c táº­p dá»¯ liá»‡u huáº¥n luyá»‡n sau Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thÃ­ nghiá»‡m:

Táº­p dá»¯ liá»‡u huáº¥n luyá»‡n trÆ°á»›c: Má»™t táº­p dá»¯ liá»‡u huáº¥n luyá»‡n trÆ°á»›c 480k giá» Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Ä‘iá»ƒm khá»Ÿi Ä‘áº§u cho huáº¥n luyá»‡n tÄƒng dáº§n vá»›i há»‡ thá»‘ng ILASR. Äiá»u nÃ y bao gá»“m hai táº­p dá»¯ liá»‡u:
(1) 120K giá» HT: Dá»¯ liá»‡u Ä‘Æ°á»£c chÃ©p bá»Ÿi con ngÆ°á»i (HT) tá»« 2020 vÃ  nhá»¯ng nÄƒm trÆ°á»›c Ä‘Ã³
(2) 360K giá» SSL: Dá»¯ liá»‡u Ä‘Æ°á»£c chÃ©p bá»Ÿi mÃ¡y trong 2020

Táº­p dá»¯ liá»‡u huáº¥n luyá»‡n tÄƒng dáº§n: ChÃºng tÃ´i coi cuá»‘i nÄƒm 2020 lÃ  ngÃ y báº¯t Ä‘áº§u cho huáº¥n luyá»‡n tÄƒng dáº§n cá»§a cÃ¡c mÃ´ hÃ¬nh ASR.
(1) 180K giá» ILASR SSL: Dá»¯ liá»‡u Ä‘Æ°á»£c chÃ©p bá»Ÿi mÃ¡y Ä‘Æ°á»£c táº¡o ra trong khoáº£ng thá»i gian sÃ¡u thÃ¡ng trong 2021 (ThÃ¡ng 1 Ä‘áº¿n ThÃ¡ng 6) vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n gáº§n thá»i gian thá»±c cá»§a há»‡ thá»‘ng ILASR.

Táº­p thá»­ nghiá»‡m: ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh trÃªn cÃ¡c táº­p thá»­ nghiá»‡m Ä‘Æ°á»£c chÃ©p bá»Ÿi con ngÆ°á»i (HT) ná»™i bá»™.

Chung: Bao gá»“m ba táº­p dá»¯ liá»‡u HT tá»« cÃ¡c pháº¡m vi thá»i gian khÃ¡c nhau Ä‘áº¡i diá»‡n cho trÆ°á»ng há»£p sá»­ dá»¥ng chung. NÃ³ bao gá»“m má»™t táº­p thá»­ nghiá»‡m 37 giá» tá»« 2021, má»™t táº­p thá»­ nghiá»‡m 10 giá» tá»« 2020 vÃ  má»™t táº­p thá»­ nghiá»‡m 96 giá» tá»« 2018-2019.

Hiáº¿m: Bao gá»“m ba táº­p dá»¯ liá»‡u HT tá»« cÃ¡c pháº¡m vi thá»i gian khÃ¡c nhau, nÆ¡i cÃ¡c phiÃªn chÃ©p chá»©a Ã­t nháº¥t má»™t tá»« hiáº¿m. Tá»« hiáº¿m lÃ  nhá»¯ng tá»« trong Ä‘uÃ´i dÃ i cá»§a tá»« vá»±ng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi táº§n suáº¥t tá»«. Äiá»u nÃ y bao gá»“m má»™t táº­p thá»­ nghiá»‡m 44 giá» tá»« 2021, má»™t táº­p thá»­ nghiá»‡m 44 giá» tá»« 2020, vÃ  má»™t táº­p thá»­ nghiá»‡m 27 giá» tá»« 2018-2019.

Delta: Äiá»u nÃ y bao gá»“m má»™t táº­p thá»­ nghiá»‡m HT 22 giá» ghi láº¡i sá»± thay Ä‘á»•i trong táº§n suáº¥t cá»§a tá»« trong 2021 so vá»›i 2020. CÃ¡c phiÃªn chÃ©p Ä‘Æ°á»£c lá»c dá»±a trÃªn 1-gram, 2-gram vÃ  3-gram cÃ³ táº§n suáº¥t cao hÆ¡n 5 láº§n trong 2021 so vá»›i 2020. Táº­p thá»­ nghiá»‡m nÃ y náº¯m báº¯t nhá»¯ng thay Ä‘á»•i trong phÃ¢n phá»‘i dá»¯ liá»‡u vÃ  ráº¥t liÃªn quan Ä‘á»ƒ Ä‘o lÆ°á»ng tÃ¡c Ä‘á»™ng cá»§a há»c tÄƒng dáº§n vá»›i ILASR.

Nháº¯n tin: Bao gá»“m hai táº­p dá»¯ liá»‡u HT bao gá»“m dá»¯ liá»‡u miá»n nháº¯n tin vÃ  truyá»n thÃ´ng. NÃ³ bao gá»“m má»™t thá»­ nghiá»‡m HT 2.7 giá» tá»« 2020 vÃ  má»™t táº­p thá»­ nghiá»‡m HT 45.5 giá» tá»« 2018-2019.

Táº­p dá»¯ liá»‡u hÃ ng thÃ¡ng (2021): ChÃºng tÃ´i sá»­ dá»¥ng sÃ¡u táº­p thá»­ nghiá»‡m hÃ ng thÃ¡ng tá»« ThÃ¡ng 1 Ä‘áº¿n ThÃ¡ng 6 2021 Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ thiáº¿t láº­p há»c tÄƒng dáº§n cá»§a ILASR. Má»—i táº­p dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c gá»i lÃ  (Jan, Feb, Â·Â·Â·, June) vÃ  má»—i thÃ¡ng cÃ³ trung bÃ¬nh 70 giá» dá»¯ liá»‡u. ChÃºng tÃ´i tiáº¿p tá»¥c bÃ¡o cÃ¡o káº¿t quáº£ trÃªn cÃ¡c táº­p dá»¯ liá»‡u 3 thÃ¡ng ğ½ğ‘ğ‘›âˆ’ğ‘€ğ‘ğ‘Ÿ bao gá»“m dá»¯ liá»‡u tá»« Jan, Feb, Mar vÃ  ğ´ğ‘ğ‘Ÿâˆ’ğ½ğ‘¢ğ‘› bao gá»“m dá»¯ liá»‡u tá»« Apr, May, June.

4.2 Chi tiáº¿t mÃ´ hÃ¬nh
Äáº·c trÆ°ng: CÃ¡c Ä‘áº·c trÆ°ng Ã¢m thanh lÃ  nÄƒng lÆ°á»£ng filter-bank log-mel 64 chiá»u [46] Ä‘Æ°á»£c tÃ­nh toÃ¡n trong cá»­a sá»• 25ms, vá»›i Ä‘á»™ dá»‹ch chuyá»ƒn 10ms. CÃ¡c Ä‘áº·c trÆ°ng Ä‘Æ°á»£c tÃ­nh toÃ¡n trÃªn 3 khung 10ms liÃªn tiáº¿p Ä‘Æ°á»£c xáº¿p chá»“ng vÃ  láº¥y máº«u phá»¥ Ä‘á»ƒ táº¡o ra cÃ¡c Ä‘áº·c trÆ°ng 192 chiá»u á»Ÿ tá»‘c Ä‘á»™ khung 30ms, vÃ  Ä‘Æ°á»£c cung cáº¥p lÃ m Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh ASR. CÃ¡c phiÃªn chÃ©p sá»± tháº­t cÆ¡ báº£n Ä‘Æ°á»£c tokenize thÃ nh 2500 Ä‘Æ¡n vá»‹ sub-word sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ uni-gram [35].

MÃ´ hÃ¬nh: MÃ´ hÃ¬nh giÃ¡o viÃªn: MÃ´ hÃ¬nh giÃ¡o viÃªn Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra báº£n chÃ©p mÃ¡y SSL. ChÃºng tÃ´i cÃ³ ba mÃ´ hÃ¬nh giÃ¡o viÃªn cÃ³ sáºµn: ğ‘‡3 lÃ  má»™t mÃ´ hÃ¬nh giÃ¡o viÃªn (má»™t há»‡ thá»‘ng ASR hybrid RNN-HMM thÃ´ng thÆ°á»ng [6]) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 100k giá» dá»¯ liá»‡u chá»‰ Ä‘áº¿n 2019. Báº£n chÃ©p mÃ¡y tá»« ğ‘‡3 Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ bootstrap vÃ  cung cáº¥p phiÃªn chÃ©p cho táº­p dá»¯ liá»‡u huáº¥n luyá»‡n trÆ°á»›c SSL 360k giá» gáº§n Ä‘Ã¢y hÆ¡n. Táº­p dá»¯ liá»‡u huáº¥n luyá»‡n trÆ°á»›c 480k giá», bao gá»“m táº­p dá»¯ liá»‡u SSL 360k giá» dá»±a trÃªn ğ‘‡3 vÃ  táº­p dá»¯ liá»‡u HT 120k giá», Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n hai mÃ´ hÃ¬nh giÃ¡o viÃªn Ä‘Æ°á»£c cáº­p nháº­t: (1) ğ‘‡1: Má»™t kiáº¿n trÃºc ASR dá»±a trÃªn conformer lá»›n hÆ¡n [25] Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 480k giá». ğ‘‡1 cÃ³ 122M tham sá»‘, má»™t bá»™ mÃ£ hÃ³a vá»›i 17Ã—512 lá»›p LSTM, 8 Ä‘áº§u attention vá»›i kernel convolution 32 chiá»u. Máº¡ng dá»± Ä‘oÃ¡n sá»­ dá»¥ng 2Ã—1024 lá»›p LSTM. (2) ğ‘‡2 lÃ  má»™t há»‡ thá»‘ng ASR hybrid RNN-HMM thÃ´ng thÆ°á»ng [6] vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¹ng táº­p dá»¯ liá»‡u 480k giá». Cuá»‘i cÃ¹ng, cÃ¡c mÃ´ hÃ¬nh sinh viÃªn cho táº¥t cáº£ thÃ­ nghiá»‡m trong bÃ i bÃ¡o Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c táº­p dá»¯ liá»‡u SSL sá»­ dá»¥ng mÃ´ hÃ¬nh giÃ¡o viÃªn ğ‘‡1 gáº§n Ä‘Ã¢y nháº¥t.

--- TRANG 4 ---
KDD '22, August 14â€“18, 2022, Washington, DC, USA Gopinath Chennupati et al.

Trong pháº§n 5.1.3, nháº±m má»¥c Ä‘Ã­ch ablation so sÃ¡nh cÃ¡c giÃ¡o viÃªn khÃ¡c nhau, chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh sinh viÃªn trÃªn cÃ¡c táº­p dá»¯ liá»‡u SSL dá»±a trÃªn ğ‘‡2 vÃ  ğ‘‡3.

MÃ´ hÃ¬nh sinh viÃªn: CÃ¡c mÃ´ hÃ¬nh sinh viÃªn dá»±a trÃªn cÃ¡c kiáº¿n trÃºc RNN-T dá»±a trÃªn LSTM khÃ¡c nhau. ChÃºng khÃ¡c nhau vá» sá»‘ lÆ°á»£ng lá»›p bá»™ mÃ£ hÃ³a vÃ  tá»‘c Ä‘á»™ khung Ä‘áº·c trÆ°ng. Hai mÃ´ hÃ¬nh sinh viÃªn Ä‘Æ°á»£c mÃ´ táº£ nhÆ° sau. ğ‘Ÿğ‘›ğ‘›ğ‘¡_60ğ‘š chá»©a 60M tham sá»‘ vá»›i bá»™ mÃ£ hÃ³a LSTM 5Ã—1024, máº¡ng dá»± Ä‘oÃ¡n LSTM 2Ã—1024 vÃ  má»™t máº¡ng káº¿t há»£p feed-forward vá»›i kÃ­ch hoáº¡t tanh. CÃ¡c embedding Ä‘áº§u vÃ o cá»§a máº¡ng dá»± Ä‘oÃ¡n lÃ  512 chiá»u. SpecAugment [48] Ä‘Æ°á»£c sá»­ dá»¥ng trÃªn cÃ¡c Ä‘áº·c trÆ°ng Ã¢m thanh. ğ‘Ÿğ‘›ğ‘›ğ‘¡_90ğ‘š chá»©a 90M tham sá»‘ vá»›i bá»™ mÃ£ hÃ³a lá»›p LSTM 8Ã—1024, má»™t máº¡ng dá»± Ä‘oÃ¡n cÃ³ kÃ­ch thÆ°á»›c 2Ã—1024, vÃ  má»™t máº¡ng káº¿t há»£p feed-forward vá»›i kÃ­ch hoáº¡t tanh. CÃ¡c embedding Ä‘áº§u vÃ o cá»§a máº¡ng dá»± Ä‘oÃ¡n sá»­ dá»¥ng embedding 512 chiá»u vÃ  má»™t tokenizer sub-word 2500 tá»« má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ uni-gram. SpecAugment Ä‘Æ°á»£c sá»­ dá»¥ng trÃªn cÃ¡c Ä‘áº·c trÆ°ng Ã¢m thanh. Bá»™ mÃ£ hÃ³a sá»­ dá»¥ng RNN Ä‘a lá»›p giáº£m thá»i gian [54] dá»±a trÃªn LSTM (cho tá»‘c Ä‘á»™ huáº¥n luyá»‡n vÃ  suy luáº­n) vá»›i tá»‘c Ä‘á»™ khung Ä‘áº·c trÆ°ng Ä‘Æ°á»£c Ä‘áº·t thÃ nh 3 lá»›p. Má»—i lá»›p khung Ä‘áº·c trÆ°ng nÃ y cÃ³ 1536 Ä‘Æ¡n vá»‹ vÃ  projection LSTM vá»›i kÃ­ch thÆ°á»›c 512.

CÃ¡c mÃ´ hÃ¬nh ğ‘Ÿğ‘›ğ‘›ğ‘¡_90ğ‘š vÃ  ğ‘Ÿğ‘›ğ‘›ğ‘¡_60ğ‘š Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn cáº£ dá»¯ liá»‡u HT 120k giá» vÃ  340k giá» dá»¯ liá»‡u SSL Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡c nhÃ£n Ä‘Æ°á»£c giáº£i mÃ£ bá»Ÿi giÃ¡o viÃªn (ğ‘‡1). Dá»¯ liá»‡u Ä‘Æ°á»£c chÃ©p bá»Ÿi con ngÆ°á»i Ä‘Æ°á»£c sá»­ dá»¥ng trong huáº¥n luyá»‡n trÆ°á»›c sá»­ dá»¥ng dá»¯ liá»‡u Ä‘áº¿n cuá»‘i nÄƒm 2020, trong khi dá»¯ liá»‡u SSL lÃ  trong nÄƒm 2020. Äá»‘i vá»›i cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i tiáº¿p tá»¥c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh sinh viÃªn RNN-T Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c á»Ÿ trÃªn sá»­ dá»¥ng tá»•ng sá»‘ 180k giá» dá»¯ liá»‡u SSL (nhÃ£n Ä‘Æ°á»£c táº¡o bá»Ÿi giÃ¡o viÃªn) cÃ³ sáºµn trong cá»­a sá»• thá»i gian 6 thÃ¡ng trong 2021.

Chi tiáº¿t huáº¥n luyá»‡n: ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c tham sá»‘ sau Ä‘á»ƒ huáº¥n luyá»‡n cáº£ mÃ´ hÃ¬nh giÃ¡o viÃªn vÃ  sinh viÃªn. Há»‡ thá»‘ng Ä‘Æ°á»£c cháº¡y trÃªn má»™t fleet gá»“m 200 node. ChÃºng tÃ´i Ã¡p dá»¥ng má»™t lá»‹ch trÃ¬nh tá»· lá»‡ há»c warm-up nÆ¡i ğ‘™ğ‘Ÿ = 1ğ‘’âˆ’7 cho 3000 bÆ°á»›c Ä‘áº§u tiÃªn, tiáº¿p theo lÃ  tá»· lá»‡ há»c háº±ng sá»‘ 5ğ‘’âˆ’4 Ä‘áº¿n 50k bÆ°á»›c, sau Ä‘Ã³ suy giáº£m exponential (ğ‘™ğ‘Ÿ = 1ğ‘’âˆ’5) tá»« 50k Ä‘áº¿n 750k bÆ°á»›c vá»›i trÃ¬nh tá»‘i Æ°u Adam (siÃªu tham sá»‘ lÃ  ğ›½1 = 0.9, ğ›½2 = 0.99).

ChÃºng tÃ´i thÃ­ nghiá»‡m vá»›i nhiá»u kÃ­ch thÆ°á»›c lÃ´ lá»›n (9k, 18k, 73k, 147k, 215k, 307k) thÃ´ng qua tÃ­ch lÅ©y gradient. LÆ°u Ã½ ráº±ng nhá»¯ng tÃ­ch lÅ©y nÃ y cÃ³ hiá»‡u á»©ng ngáº§m cá»§a viá»‡c thay Ä‘á»•i cÃ¡c giÃ¡ trá»‹ gradient do tá»•ng cÃ¡c gradient trÃªn má»™t lÃ´ lá»›n. ChÃºng tÃ´i xá»­ lÃ½ cÃ¡c lÃ´ lá»›n mÃ  khÃ´ng thay Ä‘á»•i lá»‹ch trÃ¬nh ğ‘™ğ‘Ÿ trong khi tÃ­ch lÅ©y cÃ¡c gradient. Hiá»‡u suáº¥t cá»§a nhá»¯ng mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c Ä‘o báº±ng giáº£m tá»· lá»‡ lá»—i tá»« tÆ°Æ¡ng Ä‘á»‘i (WERR) so vá»›i cÃ¡c Ä‘Æ°á»ng cÆ¡ sá»Ÿ tÆ°Æ¡ng á»©ng. WER lÃ  tá»· lá»‡ cá»§a khoáº£ng cÃ¡ch chá»‰nh sá»­a vá»›i Ä‘á»™ dÃ i chuá»—i, nÆ¡i khoáº£ng cÃ¡ch chá»‰nh sá»­a lÃ  Ä‘á»™ dÃ i cá»§a chuá»—i ngáº¯n nháº¥t cá»§a cÃ¡c phÃ©p toÃ¡n chÃ¨n, xÃ³a vÃ  thay tháº¿ trÃªn viá»‡c biáº¿n Ä‘á»•i má»™t chuá»—i Ä‘Æ°á»£c dá»± Ä‘oÃ¡n thÃ nh má»¥c tiÃªu.

5 Káº¾T QUáº¢ & THáº¢O LUáº¬N
Trong pháº§n nÃ y, chÃºng tÃ´i phÃ¢n tÃ­ch hiá»‡u suáº¥t cá»§a há»c tÄƒng dáº§n trong ILASR. Cá»¥ thá»ƒ, chÃºng tÃ´i phÃ¢n tÃ­ch hiá»‡u suáº¥t cá»§a há»c tÄƒng dáº§n trong ILASR vá» giáº£m tá»· lá»‡ lá»—i tá»« tÆ°Æ¡ng Ä‘á»‘i (WERR) so vá»›i cÃ¡c mÃ´ hÃ¬nh sinh viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c ban Ä‘áº§u lÃ m Ä‘Æ°á»ng cÆ¡ sá»Ÿ.

Tá»« Báº£ng 1, chÃºng ta tháº¥y ráº±ng ILASR cáº£i thiá»‡n má»™t mÃ´ hÃ¬nh cÆ¡ sá»Ÿ Ä‘Æ°á»£c huáº¥n luyá»‡n máº¡nh lÃªn Ä‘áº¿n 3% trÃªn cÃ¡c táº­p thá»­ nghiá»‡m trong 2021 mÃ  leo lÃªn 20% trÃªn táº­p dá»¯ liá»‡u delta bao gá»“m tá»« vÃ  cá»¥m tá»« má»›i hoáº·c thá»‹nh hÃ nh.

Báº£ng 1: Cáº£i thiá»‡n WER tÆ°Æ¡ng Ä‘á»‘i % tá»« mÃ´ hÃ¬nh ban Ä‘áº§u khi Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i há»‡ thá»‘ng ILASR

ILASR
Thá»i gian Táº­p thá»­ nghiá»‡m replay no replay
2021 Rare 0.72% 0.66%
Delta 20.10% 23.99%
General 1.23% 0.41%
Jan-Mar 1.25% 1.50%
Apr-Jun 2.73% 3.09%
2020 Rare 0.62% 0.62%
General 0.00% -0.72%
Message -0.83% -2.04%
2018-2019 Rare -0.63% -0.63%
General -1.21% -2.6%
Message -2.82% -3.42%

[HÃ¬nh 3: WERR hÃ ng thÃ¡ng (%) cho há»c tÄƒng dáº§n trong ILASR cho ğ‘Ÿğ‘›ğ‘›ğ‘¡_60ğ‘š trÃªn sÃ¡u táº­p thá»­ nghiá»‡m hÃ ng thÃ¡ng (ğ½ğ‘ğ‘›â€“ğ½ğ‘¢ğ‘›) khi Ä‘Æ°á»£c Ä‘o so vá»›i mÃ´ hÃ¬nh báº¯t Ä‘áº§u so vá»›i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n tÄƒng dáº§n trong má»—i thÃ¡ng.]

Äá»“ng thá»i, hiá»‡u suáº¥t trÃªn cÃ¡c táº­p thá»­ nghiá»‡m chung vÃ  Ä‘uÃ´i cÅ© hÆ¡n khÃ´ng tháº¥y nhiá»u suy giáº£m.

QuÃªn tháº£m khá»‘c lÃ  má»™t trong nhá»¯ng váº¥n Ä‘á» mÃ  há»c tÄƒng dáº§n cáº§n vÆ°á»£t qua Ä‘á»ƒ cÃ³ hiá»‡u suáº¥t nháº¥t quÃ¡n trÃªn cáº£ dá»¯ liá»‡u cÅ© vÃ  má»›i. Trong Báº£ng 1, chÃºng tÃ´i so sÃ¡nh hiá»‡u suáº¥t cá»§a há»c tÄƒng dáº§n dá»±a trÃªn replay, nÆ¡i má»™t pháº§n Ä‘Æ°á»£c láº¥y máº«u phá»¥ cá»§a dá»¯ liá»‡u Ä‘Æ°á»£c chÃ©p bá»Ÿi con ngÆ°á»i 120K giá» cÅ©ng Ä‘Æ°á»£c tiÃªu thá»¥ trong huáº¥n luyá»‡n mÃ´ hÃ¬nh trong khi Ä‘á»‘i tÃ¡c no replay khÃ´ng liÃªn quan Ä‘áº¿n Ä‘iá»u Ä‘Ã³. NhÆ° Ä‘Æ°á»£c chá»©ng minh trong Báº£ng 1, huáº¥n luyá»‡n dá»±a trÃªn replay cÃ³ xu hÆ°á»›ng vÆ°á»£t trá»™i hÆ¡n Ä‘á»‘i tÃ¡c no replay cá»§a nÃ³ trÃªn cÃ¡c táº­p thá»­ nghiá»‡m cÅ© hÆ¡n nhÆ° mong Ä‘á»£i tá»« tÃ i liá»‡u IL.

Tiáº¿p theo, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh ILASR Ä‘Æ°á»£c huáº¥n luyá»‡n tÄƒng dáº§n trÃªn cÃ¡c táº­p thá»­ nghiá»‡m chi tiáº¿t Ä‘Æ°á»£c chuáº©n bá»‹ trong má»—i thÃ¡ng trong sÃ¡u thÃ¡ng (Jan-Jun) cá»§a 2021, xem HÃ¬nh 3. Äá»‘i vá»›i táº¥t cáº£ cÃ¡c Ä‘Ã¡nh giÃ¡ trong HÃ¬nh 3, chÃºng tÃ´i bÃ¡o cÃ¡o WERR trong má»—i thÃ¡ng so vá»›i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c ban Ä‘áº§u (vÃ­ dá»¥, WERR trong ğ‘€ğ‘ğ‘¦ lÃ  sá»± khÃ¡c biá»‡t tÆ°Æ¡ng Ä‘á»‘i giá»¯a WER cá»§a mÃ´ hÃ¬nh ğ‘€ğ‘ğ‘¦ vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c). Káº¿t quáº£ cho tháº¥y cáº£i thiá»‡n tÄƒng dáº§n trong hiá»‡u suáº¥t trÃªn táº¥t cáº£ sÃ¡u táº­p thá»­ nghiá»‡m hÃ ng thÃ¡ng tá»« thÃ¡ng nÃ y sang thÃ¡ng khÃ¡c trong huáº¥n luyá»‡n ILASR. Äiá»u nÃ y gá»£i Ã½

--- TRANG 5 ---
KDD '22, August 14â€“18, 2022, Washington, DC, USA Gopinath Chennupati et al.

[HÃ¬nh 4: Äá»‘i vá»›i ğ‘Ÿğ‘›ğ‘›ğ‘¡_60ğ‘š, mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u cÃ³ sáºµn Ä‘áº¿n 12/2019. Huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c nÃ y á»Ÿ cháº¿ Ä‘á»™ tÄƒng dáº§n cho chÃ­n thÃ¡ng tiáº¿p theo (ğ½ğ‘ğ‘›âˆ’ğ‘†ğ‘’ğ‘) trong 2020. Trá»¥c x cho tháº¥y mÃ´ hÃ¬nh tÄƒng dáº§n hÃ ng thÃ¡ng, nÆ¡i mÃ´ hÃ¬nh tá»« thÃ¡ng trÆ°á»›c Ä‘Æ°á»£c tinh chá»‰nh trÃªn dá»¯ liá»‡u trong thÃ¡ng hiá»‡n táº¡i; trá»¥c y cho tháº¥y WER tÆ°Æ¡ng Ä‘á»‘i trong má»—i thÃ¡ng w.r.t mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c ban Ä‘áº§u. Má»—i Ä‘Æ°á»ng cong Ä‘áº¡i diá»‡n cho táº­p thá»­ nghiá»‡m cá»§a thÃ¡ng tÆ°Æ¡ng á»©ng.]

ráº±ng huáº¥n luyá»‡n tÄƒng dáº§n giÃºp náº¯m báº¯t cÃ¡c xu hÆ°á»›ng má»›i trong cÃ¡c khoáº£ng thá»i gian trong khi mÃ´ hÃ¬nh Ä‘ang thÃ­ch á»©ng vá»›i nhá»¯ng thay Ä‘á»•i tÄƒng dáº§n trong dá»¯ liá»‡u. CÅ©ng Ä‘Ã¡ng chÃº Ã½ lÃ  sá»± cáº£i thiá»‡n tÄƒng dáº§n khÃ´ng Ä‘áº¿n vá»›i chi phÃ­ cá»§a viá»‡c quÃªn tháº£m khá»‘c. ThÃº vá»‹ hÆ¡n, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u Ä‘áº¿n ğ‘€ğ‘ğ‘¦/ğ½ğ‘¢ğ‘›ğ‘’ lÃ m suy giáº£m hiá»‡u suáº¥t trÃªn táº­p thá»­ nghiá»‡m ğ½ğ‘¢ğ‘›ğ‘’, Ä‘iá»u nÃ y cáº£i thiá»‡n sau khi mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u cÃ³ sáºµn tá»« ğ‘€ğ‘ğ‘¦/ğ½ğ‘¢ğ‘›ğ‘’. Äiá»u nÃ y gá»£i Ã½ rÃµ rÃ ng vá» báº£n cháº¥t thÃ­ch á»©ng cá»§a viá»‡c náº¯m báº¯t nhá»¯ng thay Ä‘á»•i trong dá»¯ liá»‡u trong cÃ¡c khoáº£ng thá»i gian má»›i trong ILASR.

Äá»ƒ tÄƒng cÆ°á»ng hÆ¡n ná»¯a cÃ¡c tuyÃªn bá»‘ há»c tÄƒng dáº§n, chÃºng tÃ´i phÃ¢n tÃ­ch cÃ¡c máº«u há»c tÄƒng dáº§n trong thá»i gian dÃ i hÆ¡n trong cÃ¡c khoáº£ng thá»i gian giá»¯a ğ½ğ‘ğ‘›âˆ’ğ‘†ğ‘’ğ‘ trong 2020. HÃ¬nh 4 cho tháº¥y cÃ¡c máº«u há»c trÃªn cÆ¡ sá»Ÿ hÃ ng quÃ½ cho ba quÃ½ Ä‘áº§u tiÃªn (Q1â€“Q3) cá»§a 2020. Trong 2020 ğ‘„1 vÃ  ğ‘„2, WERR cáº£i thiá»‡n ban Ä‘áº§u vÃ  sau Ä‘Ã³ giáº£m khi huáº¥n luyá»‡n mÃ´ hÃ¬nh tÄƒng dáº§n tiáº¿n triá»ƒn trÃªn cÆ¡ sá»Ÿ thÃ¡ng qua thÃ¡ng. Sá»± suy giáº£m (trong khi tá»‘t hÆ¡n Ä‘Æ°á»ng cÆ¡ sá»Ÿ) lÃ  má»™t minh chá»©ng cá»§a viá»‡c quÃªn khi cÃ¡c cáº­p nháº­t má»›i hÆ¡n Ä‘Æ°á»£c Æ°u tiÃªn hÆ¡n cÃ¡c táº­p thá»­ nghiá»‡m cÅ© nhiá»u thÃ¡ng. Do Ä‘Ã³, trong 2020 ğ‘„3, hiá»‡u suáº¥t cáº£i thiá»‡n mÃ  khÃ´ng cÃ³ báº¥t ká»³ xu hÆ°á»›ng giáº£m nÃ o, Ä‘iá»u nÃ y lÃ  do thá»±c táº¿ ráº±ng cÃ¡c mÃ´ hÃ¬nh tiáº¿p tá»¥c há»c thÃ¡ng qua thÃ¡ng trong khi cÃ¡c táº­p thá»­ nghiá»‡m cÅ©ng thuá»™c vá» cÃ¹ng cÃ¡c khoáº£ng thá»i gian. Nhá»¯ng xu hÆ°á»›ng nÃ y gá»£i Ã½ ráº±ng cÃ¡c ká»¹ thuáº­t Ä‘Æ°á»£c Ä‘á» xuáº¥t giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t tÄƒng dáº§n ngay cáº£ trong cÃ¡c khoáº£ng thá»i gian dÃ i hÆ¡n trong khi háº¡n cháº¿ cÃ¡c há»“i quy trÃªn dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ cÅ© hÆ¡n.

Tiáº¿p theo, chÃºng tÃ´i khÃ¡m phÃ¡ má»™t sá»‘ lá»±a chá»n thiáº¿t káº¿ Ä‘Ã³ng vai trÃ² quan trá»ng trong hiá»‡u suáº¥t cá»§a ILASR vÃ  chia sáº» hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i vá» cÃ¡c lá»±a chá»n thiáº¿t káº¿.

5.1 Lá»±a chá»n Thiáº¿t káº¿: ILASR
ChÃºng tÃ´i khÃ¡m phÃ¡ cÃ¡c lá»±a chá»n thiáº¿t káº¿ sau trong bá»‘i cáº£nh khung cÃ´ng tÃ¡c ILASR: 1) hiá»‡u á»©ng cá»§a kÃ­ch thÆ°á»›c lÃ´ lá»›n trÃªn hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh sinh viÃªn; 2) hiá»‡u á»©ng thá»i gian trÃªn viá»‡c xá»­ lÃ½ dá»¯ liá»‡u trong ILASR; 3) phÃ¢n tÃ­ch táº§m quan trá»ng cá»§a cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn khÃ¡c nhau trong ILASR.

5.1.1 Huáº¥n luyá»‡n bá»n vá»¯ng vá»›i kÃ­ch thÆ°á»›c lÃ´ lá»›n. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c lÃ´ lá»›n trong ILASR qua tÃ­ch lÅ©y gradient. Khi kÃ­ch thÆ°á»›c lÃ´ hiá»‡u quáº£ tÄƒng, sá»‘ bÆ°á»›c tá»‘i Æ°u hÃ³a hoáº·c cáº­p nháº­t giáº£m khi cÃ¹ng má»™t lÆ°á»£ng dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½. KÃ­ch thÆ°á»›c lÃ´ lá»›n hÆ¡n sáº½ yÃªu cáº§u Ã­t bÆ°á»›c tá»‘i Æ°u hÃ³a hÆ¡n vÃ  ngÆ°á»£c láº¡i cho cÃ¹ng má»™t lÆ°á»£ng dá»¯ liá»‡u. Viá»‡c sá»­ dá»¥ng cÃ¡c lÃ´ lá»›n tÄƒng tá»‘c huáº¥n luyá»‡n (Ä‘Æ°á»£c hiá»ƒn thá»‹ trong [64]),

Báº£ng 2: Hiá»‡u á»©ng cá»§a cÃ¡c lÃ´ lá»›n trÃªn sá»± cáº£i thiá»‡n hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘á»‘i (vá» WERR, %) cá»§a táº¥t cáº£ ba mÃ´ hÃ¬nh khi Ä‘Æ°á»£c tinh chá»‰nh trong ILASR.

[THIS IS TABLE: Shows effect of different batch sizes (9k to 307k) on WERR performance across different time periods and test sets for rnnt_60m model]

Ä‘iá»u nÃ y tÆ°Æ¡ng tá»± trong ILASR. LÃ½ do kÃ­ch thÆ°á»›c lÃ´ lá»›n cÃ³ liÃªn quan trong há»‡ thá»‘ng ILASR lÃ  cÃ³ nhá»¯ng háº¡n cháº¿ vá» viá»‡c cÃ¡c gradient cÃ³ thá»ƒ Ä‘Æ°á»£c tá»•ng há»£p nhanh nhÆ° tháº¿ nÃ o vÃ  mÃ´ hÃ¬nh toÃ n cá»¥c Ä‘Æ°á»£c phÃ¢n phá»‘i Ä‘áº¿n cÃ¡c mÃ¡y chá»§ trong fleet. Do Ä‘Ã³, má»™t sá»‘ bÆ°á»›c cáº­p nháº­t háº¡n cháº¿ cÃ³ thá»ƒ diá»…n ra trong má»™t khoáº£ng thá»i gian so vá»›i huáº¥n luyá»‡n phÃ¢n tÃ¡n offline dá»±a trÃªn GPU. HÆ¡n ná»¯a, khi dá»¯ liá»‡u Ä‘áº¿n theo cÃ¡ch streaming vÃ  khÃ´ng Ä‘Æ°á»£c lÆ°u trá»¯, nÃ³ cáº§n Ä‘Æ°á»£c tiÃªu thá»¥ khi vÃ  khi nÃ³ Ä‘áº¿n, gáº§n thá»i gian thá»±c. Äá»‘i vá»›i má»—i sá»‘ lÆ°á»£ng cáº­p nháº­t háº¡n cháº¿, má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u streaming cÃ³ sáºµn.

ChÃºng tÃ´i khÃ¡m phÃ¡ sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a cÃ¡c lÃ´ lá»›n vÃ  hiá»‡u suáº¥t mÃ´ hÃ¬nh. Báº£ng 2 cho tháº¥y hiá»‡u á»©ng cá»§a cÃ¡c lÃ´ lá»›n trÃªn hiá»‡u suáº¥t cá»§a má»™t mÃ´ hÃ¬nh sinh viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n trong ILASR. Hiá»‡u suáº¥t (WERR) lÃ  tÆ°Æ¡ng Ä‘á»‘i so vá»›i mÃ´ hÃ¬nh sinh viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c tÆ°Æ¡ng á»©ng. ÄÆ°á»ng cÆ¡ sá»Ÿ nÃ y yáº¿u hÆ¡n, do Ä‘Ã³ cáº£i thiá»‡n lá»›n hÆ¡n. ChÃºng tÃ´i tháº¥y ráº±ng viá»‡c tÄƒng multiplier lÃ´ (kÃ­ch thÆ°á»›c lÃ´ hiá»‡u quáº£) cÃ³ hiá»‡u á»©ng khÃ´ng Ä‘Ã¡ng ká»ƒ trÃªn WER. Khi kÃ­ch thÆ°á»›c lÃ´ tÄƒng tá»« 9K Ä‘áº¿n 300K phÃ¡t ngÃ´n, sá»± khÃ¡c biá»‡t trong Ä‘á»™ chÃ­nh xÃ¡c lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ.

Quan trá»ng hÆ¡n, phÃ¡t hiá»‡n nÃ y trÃ¡i ngÆ°á»£c vá»›i cÃ¡c hiá»‡u á»©ng suy giáº£m Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m Ä‘Æ°á»£c bÃ¡o cÃ¡o trong tÃ i liá»‡u [20,22,33,37,41,42,52, 56] vá»›i viá»‡c sá»­ dá»¥ng cÃ¡c lÃ´ lá»›n. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng sá»± suy giáº£m nhÆ° váº­y khÃ´ng rÃµ rÃ ng Ä‘á»‘i vá»›i huáº¥n luyá»‡n mÃ´ hÃ¬nh trong ILASR. Máº·c dÃ¹, cÃ¡c ná»— lá»±c trong tÃ i liá»‡u khÃ´ng cÃ³ sá»± biá»‡n minh toÃ¡n há»c máº¡nh, Goyal et al. [22] lÃ½ giáº£i sá»± suy giáº£m hiá»‡u suáº¥t cho cÃ¡c váº¥n Ä‘á» tá»‘i Æ°u hÃ³a, do Ä‘Ã³ sá»­ dá»¥ng warm-up Ä‘á»ƒ giáº£m thiá»ƒu sá»± suy giáº£m. TÆ°Æ¡ng tá»±, trong trÆ°á»ng há»£p cá»§a chÃºng tÃ´i, chÃºng tÃ´i quy cÃ¡c lá»£i Ã­ch vÃ /hoáº·c khÃ´ng suy giáº£m hiá»‡u suáº¥t cho yáº¿u tá»‘ sau. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o lÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã£ há»™i tá»¥ trÃªn dá»¯ liá»‡u tá»« khoáº£ng thá»i gian trÆ°á»›c Ä‘Ã³ trÃ¡i ngÆ°á»£c vá»›i khá»Ÿi táº¡o ngáº«u nhiÃªn trong huáº¥n luyá»‡n lÃ´ lá»›n trong

--- TRANG 6 ---
ILASR: Há»c TÄƒng Dáº§n Báº£o Máº­t Quyá»n RiÃªng TÆ° cho Nháº­n Diá»‡n Giá»ng NÃ³i Tá»± Äá»™ng á»Ÿ Quy MÃ´ Sáº£n Xuáº¥t KDD '22, August 14â€“18, 2022, Washington, DC, USA

Báº£ng 3: TÃ¡c Ä‘á»™ng cá»§a thá»© tá»± thá»i gian (thá»i gian so vá»›i ngáº«u nhiÃªn) cá»§a viá»‡c xá»­ lÃ½ dá»¯ liá»‡u huáº¥n luyá»‡n trong ILASR cho cáº£ vá»›i vÃ  khÃ´ng cÃ³ replay cá»§a cÃ¡c phiÃªn chÃ©p con ngÆ°á»i.

[THIS IS TABLE: Shows impact of temporal order on training data processing in ILASR, with columns for Time, Test-set, and Chrono vs. random (replay/no replay)]

tÃ i liá»‡u, thÆ°á»ng, nhá»¯ng mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u (máº·c dÃ¹ cÃ³ vÃ i epoch ban Ä‘áº§u trong warm-up) trong tÃ i liá»‡u.

5.1.2 TÃ¡c Ä‘á»™ng cá»§a dá»¯ liá»‡u Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»i gian. Má»™t khÃ­a cáº¡nh quan trá»ng cá»§a IL lÃ  dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½ theo thá»i gian nhÆ° cÃ³ sáºµn, theo thá»i gian. ChÃºng tÃ´i phÃ¢n tÃ­ch hiá»‡u á»©ng cá»§a thá»© tá»± xá»­ lÃ½ (thá»i gian so vá»›i ngáº«u nhiÃªn) cho sÃ¡u thÃ¡ng trong 2021. LÆ°u Ã½, thá»© tá»± ngáº«u nhiÃªn giá»‘ng nhÆ° xÃ¡o trá»™n dá»¯ liá»‡u trong huáº¥n luyá»‡n phÃ¢n tÃ¡n thÃ´ng thÆ°á»ng cá»§a cÃ¡c mÃ´ hÃ¬nh sÃ¢u. Dá»¯ liá»‡u thá»i gian khÃ´ng pháº£i IID theo thá»i gian khi cÃ¡c phÃ¡t ngÃ´n cÃ³ sá»± tÆ°Æ¡ng quan vá»›i thá»i gian trong ngÃ y (vÃ­ dá»¥, yÃªu cáº§u bÃ¡o láº¡i bÃ¡o thá»©c vÃ o buá»•i sÃ¡ng hoáº·c báº­t Ä‘Ã¨n thÃ´ng minh sau hoÃ ng hÃ´n). ChÃºng tÃ´i tháº¥y ráº±ng khÃ´ng cÃ³ sá»± khÃ¡c biá»‡t trong hiá»‡u suáº¥t cá»§a viá»‡c xá»­ lÃ½ dá»¯ liá»‡u theo thá»i gian so vá»›i ngáº«u nhiÃªn nhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong Báº£ng 3. HÆ¡n ná»¯a, trong cáº£ hai trÆ°á»ng há»£p thá»i gian vÃ  ngáº«u nhiÃªn, cÃ¡c cáº£i thiá»‡n so vá»›i Ä‘Æ°á»ng cÆ¡ sá»Ÿ ban Ä‘áº§u Ä‘á»u rÃµ rÃ ng (xem Báº£ng 1).

Báº£ng 4: Hiá»‡u suáº¥t (vá» WERR, %) cá»§a giÃ¡o viÃªn ASR hybrid RNN-HMM (ğ‘‡2) vÃ  giÃ¡o viÃªn ASR hybrid RNN-HMM hai chiá»u (ğ‘‡3) so vá»›i giÃ¡o viÃªn Conformer (ğ‘‡1). Dáº¥u Ã¢m (-) thá»ƒ hiá»‡n ráº±ng ğ‘‡1 hoáº¡t Ä‘á»™ng tá»‡ hÆ¡n trong khi pháº§n cÃ²n láº¡i cho tháº¥y ráº±ng ğ‘‡1 lÃ  mÃ´ hÃ¬nh giÃ¡o viÃªn hoáº¡t Ä‘á»™ng tá»‘t nháº¥t.

[THIS IS TABLE: Shows performance comparison between different teacher models T1, T2, and T3]

5.1.3 Ablations vá»›i giÃ¡o viÃªn vÃ  sinh viÃªn. ChÃºng tÃ´i thÃ­ nghiá»‡m vá»›i ba mÃ´ hÃ¬nh giÃ¡o viÃªn khÃ¡c nhau Ä‘Æ°á»£c huáº¥n luyá»‡n cho cÃ¡c pháº¡m vi thá»i gian khÃ¡c nhau vá»›i cÃ¡c kiáº¿n trÃºc khÃ¡c nhau. ThÃ­ nghiá»‡m nÃ y giÃºp chÃºng tÃ´i khÃ¡m phÃ¡ táº§m quan trá»ng cá»§a viá»‡c giá»¯ má»™t giÃ¡o viÃªn Ä‘Æ°á»£c cáº­p nháº­t vÃ  hiá»‡u quáº£ hÆ¡n. Ba giÃ¡o viÃªn lÃ : ğ‘‡1 lÃ  dá»±a trÃªn Conformer Ä‘Æ°á»£c giáº£i thÃ­ch trÆ°á»›c Ä‘Ã³ trong pháº§n 4.2; ğ‘‡2 lÃ  má»™t mÃ´ hÃ¬nh hybrid RNN-HMM thÃ´ng thÆ°á»ng [6]; ğ‘‡3 lÃ  má»™t mÃ´ hÃ¬nh ASR hybrid RNN-HMM hai chiá»u thÃ´ng thÆ°á»ng. ğ‘‡1 vÃ  ğ‘‡2 Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¹ng lÆ°á»£ng dá»¯ liá»‡u Ä‘áº¿n cuá»‘i nÄƒm 2020 trong khi ğ‘‡3 Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u (tá»•ng cá»™ng âˆ¼100k giá» dá»¯ liá»‡u HT) cÃ³ sáºµn Ä‘áº¿n cuá»‘i nÄƒm 2019.

Báº£ng 4 so sÃ¡nh hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn. Trung bÃ¬nh, ğ‘‡1 tá»‘t hÆ¡n pháº§n cÃ²n láº¡i cá»§a hai giÃ¡o viÃªn, ğ‘‡1 > ğ‘‡2 > ğ‘‡3 trÃªn dá»¯ liá»‡u má»›i pháº£n Ã¡nh táº§m quan trá»ng cá»§a viá»‡c giá»¯ mÃ´ hÃ¬nh giÃ¡o viÃªn Ä‘Æ°á»£c cáº­p nháº­t. GiÃ¡o viÃªn dá»±a trÃªn Conformer, ğ‘‡1 tá»‘t hÆ¡n pháº§n cÃ²n láº¡i cá»§a hai giÃ¡o viÃªn cÃ²n láº¡i. Sá»± khÃ¡c biá»‡t hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘á»‘i, khi Ä‘Æ°á»£c Ä‘o trÃªn bá»‘n táº­p thá»­ nghiá»‡m tiÃªu chuáº©n lÃ , ğ‘‡1 tá»‘t hÆ¡n ğ‘‡2 vÃ  ğ‘‡3 vá»›i 11.85% vÃ  7.96% WERR, tÆ°Æ¡ng á»©ng.

Báº£ng 5: Hiá»‡u suáº¥t (vá» WERR) cá»§a cÃ¡c mÃ´ hÃ¬nh sinh viÃªn khi Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i báº£n chÃ©p mÃ¡y Ä‘Æ°á»£c táº¡o ra tá»« má»—i mÃ´ hÃ¬nh giÃ¡o viÃªn khÃ¡c nhau trong ba mÃ´ hÃ¬nh.

[THIS IS TABLE: Shows performance of student models when trained with machine transcripts from different teacher models]

Báº£ng 5 cho tháº¥y WERR cá»§a hai mÃ´ hÃ¬nh sinh viÃªn (ğ‘Ÿğ‘›ğ‘›ğ‘¡_90ğ‘š vÃ  ğ‘Ÿğ‘›ğ‘›ğ‘¡_60ğ‘š) khi Ä‘Æ°á»£c huáº¥n luyá»‡n sá»­ dá»¥ng báº£n chÃ©p mÃ¡y Ä‘Æ°á»£c táº¡o ra tá»« ba mÃ´ hÃ¬nh giÃ¡o viÃªn. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng cáº£ hai sinh viÃªn Ä‘á»u tÆ°Æ¡ng tá»± vá» hiá»‡u suáº¥t. Trung bÃ¬nh, cho ğ‘Ÿğ‘›ğ‘›ğ‘¡_90ğ‘š, huáº¥n luyá»‡n dá»±a trÃªn ğ‘‡1 tá»‘t hÆ¡n ğ‘‡2 vÃ  ğ‘‡3, vá»›i 4.66% vÃ  3.25% WERR, tÆ°Æ¡ng á»©ng. Cho ğ‘Ÿğ‘›ğ‘›ğ‘¡_60ğ‘š, ğ‘‡1 tá»‘t hÆ¡n ğ‘‡2 vÃ  ğ‘‡3 vá»›i 5.96 vÃ  2.18% cáº£i thiá»‡n WERR tÆ°Æ¡ng Ä‘á»‘i tÆ°Æ¡ng á»©ng. CÃ¡c cáº£i thiá»‡n lá»›n hÆ¡n so vá»›i Báº£ng 1 vÃ¬ nhá»¯ng thÃ­ nghiá»‡m nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i 3 thÃ¡ng dá»¯ liá»‡u sá»­ dá»¥ng Ä‘Æ°á»ng cÆ¡ sá»Ÿ yáº¿u hÆ¡n. Thá»±c táº¿, cáº£ hai sinh viÃªn cÃ³ cÃ¹ng thá»© tá»± hiá»‡u suáº¥t nhÆ° cÃ¡c giÃ¡o viÃªn, Ä‘Ã³ lÃ  ğ‘‡1 > ğ‘‡2 > ğ‘‡3 ngay cáº£ sau khi huáº¥n luyá»‡n trong IL trÃªn dá»¯ liá»‡u má»›i. Quan trá»ng hÆ¡n, má»©c Ä‘á»™ cáº£i thiá»‡n trong huáº¥n luyá»‡n sinh viÃªn (Ä‘Ãºng cho cáº£ hai mÃ´ hÃ¬nh sinh viÃªn) khÃ´ng cÃ³ cÃ¹ng quy mÃ´ nhÆ° sá»± khÃ¡c biá»‡t trong giÃ¡o viÃªn. VÃ­ dá»¥, giÃ¡o viÃªn dá»±a trÃªn Conformer (ğ‘‡1) tá»‘t hÆ¡n ğ‘‡3 7.96%, trong khi sinh viÃªn ğ‘Ÿğ‘›ğ‘›ğ‘¡_90ğ‘š Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i phiÃªn chÃ©p Conformer (ğ‘‡1) tá»‘t hÆ¡n 3.25% so vá»›i mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i phiÃªn chÃ©p ğ‘‡3. Äiá»u nÃ y gá»£i Ã½ ráº±ng cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn tá»‘t hÆ¡n dáº«n Ä‘áº¿n viá»‡c cáº£i thiá»‡n hiá»‡u suáº¥t sinh viÃªn nhÆ°ng sá»± khÃ¡c biá»‡t (cÃ¹ng sinh viÃªn Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn khÃ¡c nhau) háº¹p hÆ¡n. NÃ³i cÃ¡ch khÃ¡c, má»™t mÃ´ hÃ¬nh giÃ¡o viÃªn tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ cÃ³ thá»ƒ cÃ³ tÃ¡c Ä‘á»™ng háº¡n cháº¿ trong viá»‡c cáº£i thiá»‡n cÃ¡c mÃ´ hÃ¬nh sinh viÃªn trong ILASR.

6 CÃ”NG TRÃŒNH LIÃŠN QUAN
SGD gradients mini vÃ  lá»›n: Stochastic gradient descent (SGD) Ä‘iá»u khiá»ƒn viá»‡c huáº¥n luyá»‡n máº¡ng nÆ¡-ron vá»›i mini batch. CÃ¡c mini batch lá»›n [22,27,53,64] giáº£m sá»‘ lÆ°á»£ng cáº­p nháº­t vá»›i kÃ­ch thÆ°á»›c bÆ°á»›c lá»›n. ÄÆ¡n giáº£n tÄƒng kÃ­ch thÆ°á»›c lÃ´ giáº£m Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m [33] khi cÃ¡c gradient Ä‘Æ°á»£c tÃ­ch há»£p. Äá»™ chÃ­nh xÃ¡c táº­p thá»­ nghiá»‡m cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n vá»›i cÃ¡c lÃ´ lá»›n tá»· lá»‡ thuáº­n vá»›i tá»· lá»‡ há»c. Quy táº¯c má»Ÿ rá»™ng tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n nÃ y khÃ´ng hiá»‡u quáº£, Ä‘iá»u nÃ y cáº§n thiáº¿t má»™t giai Ä‘oáº¡n warm-up [22]. Thay vÃ¬ suy giáº£m tá»· lá»‡ há»c, viá»‡c tÄƒng kÃ­ch thÆ°á»›c lÃ´ trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n [53] giÃºp giáº£m cÃ¡c bÆ°á»›c truyá»n thÃ´ng Ä‘á»ƒ cáº­p nháº­t mÃ´ hÃ¬nh vÃ  cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m. Federated averaging [44] (FedAvg) theo má»™t chiáº¿n lÆ°á»£c tÆ°Æ¡ng tá»± cá»§a viá»‡c cáº­p nháº­t gradient Ä‘á»“ng bá»™. Do Ä‘Ã³, mÃ´ hÃ¬nh táº­p trung Ä‘Æ¡n giáº£n tá»•ng há»£p cÃ¡c cáº­p nháº­t tá»« nhiá»u client khÃ¡c nhau. Do Ä‘Ã³, chÃºng tÃ´i Ã¡p dá»¥ng nhá»¯ng cáº­p nháº­t lÃ´ Ä‘á»“ng bá»™ lá»›n nÃ y (nhÆ° trong [53]) cho mÃ´ hÃ¬nh trong cÃ i Ä‘áº·t liÃªn káº¿t (thiáº¿t káº¿ tÆ°Æ¡ng tá»± Ä‘Æ°á»£c Ä‘á» xuáº¥t trong [5]) cáº£ trong algoritm SGD vÃ  averaging liÃªn káº¿t. Xem xÃ©t cÃ¡c hiá»‡u á»©ng tiÃªu cá»±c cá»§a huáº¥n luyá»‡n lÃ´ lá»›n trÃªn Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m trong tÃ i liá»‡u [9,20,33,37,41,42,52], má»™t post-local SGD Ä‘Æ°á»£c Ä‘á» xuáº¥t [38], láº¥y cáº£m há»©ng tá»« FedAvg, nÆ¡i há» Ã¡p dá»¥ng huáº¥n luyá»‡n SGD mini-batch dá»±a trÃªn warm-up [22] cho huáº¥n luyá»‡n ban Ä‘áº§u trÆ°á»›c khi khá»Ÿi cháº¡y FedAvg. TÆ°Æ¡ng tá»±, SGD phÃ¢n tÃ¡n cho giá»ng nÃ³i [55] vÃ  huáº¥n luyá»‡n quy mÃ´ lá»›n vá»›i hÃ ng triá»‡u giá» giá»ng nÃ³i [49] Ä‘Ã£ giÃºp tÄƒng tá»‘c sáº£n xuáº¥t cho cÃ¡c mÃ´ hÃ¬nh ASR.

Há»c BÃ¡n giÃ¡m sÃ¡t trong ASR: Viá»‡c há»c bÃ¡n giÃ¡m sÃ¡t Ä‘Æ°á»£c mÃ´ táº£ trong [31,32] sá»­ dá»¥ng auto-encoder Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng giá»ng nÃ³i vÃ  vÄƒn báº£n tá»« dá»¯ liá»‡u vÄƒn báº£n vÃ  giá»ng nÃ³i khÃ´ng ghÃ©p cáº·p. ASR bÃ¡n giÃ¡m sÃ¡t vá»›i cÃ¡c Ä‘áº·c trÆ°ng filter-bank [39] sá»­ dá»¥ng cÃ¡c biá»ƒu diá»…n Ã¢m thanh cÃ³ bá»‘i cáº£nh sÃ¢u vá»›i lÆ°á»£ng nhá» dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n. Viá»‡c chÆ°ng cáº¥t yáº¿u cá»§a cÃ¡c cáº·p Ã¢m thanh-vÄƒn báº£n dáº«n tá»« cÃ¡c ká»¹ thuáº­t khÃ´ng giÃ¡m sÃ¡t trong [36] giÃºp cáº£i thiá»‡n ASR Ä‘áº§u cuá»‘i. CÃ¡c phÆ°Æ¡ng phÃ¡p bÃ¡n giÃ¡m sÃ¡t trong [61] káº¿t há»£p tÄƒng cÆ°á»ng dá»¯ liá»‡u thÃ´ng qua spectral augment [48] vÃ  Ä‘iá»u chá»‰nh tÃ­nh nháº¥t quÃ¡n Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t. Dropout cung cáº¥p sá»©c máº¡nh cá»§a ensemble, cÃ¡c ná»— lá»±c dropout bÃ¡n giÃ¡m sÃ¡t trong [14] cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c nhÃ£n giáº£ vÃ  hiá»‡u suáº¥t mÃ´ hÃ¬nh trong ASR. Gáº§n Ä‘Ã¢y, cÃ´ng trÃ¬nh trong [63] sá»­ dá»¥ng há»c bÃ¡n giÃ¡m sÃ¡t tÆ°Æ¡ng pháº£n vá»›i pseudo-labeling trong viá»‡c chÃ©p láº¡i ná»™i dung video. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i sá»­ dá»¥ng pseudo-label Ä‘Æ°á»£c táº¡o ra tá»« má»™t mÃ´ hÃ¬nh giÃ¡o viÃªn trong cÃ i Ä‘áº·t liÃªn káº¿t vá»›i kÃ­ch thÆ°á»›c lÃ´ lá»›n.

Há»c KhÃ´ng giÃ¡m sÃ¡t trong ASR: Má»™t lÄ©nh vá»±c liÃªn quan lÃ  viá»‡c huáº¥n luyá»‡n biá»ƒu diá»…n, ná»n táº£ng, hoáº·c cÃ¡c mÃ´ hÃ¬nh upstream tá»« Ä‘áº§u sá»­ dá»¥ng khá»‘i lÆ°á»£ng lá»›n dá»¯ liá»‡u khÃ´ng gÃ¡n nhÃ£n. MÃ´ hÃ¬nh nÃ y sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh cho cÃ¡c trÆ°á»ng há»£p sá»­ dá»¥ng downstream nhÆ° ASR, nháº­n dáº¡ng ngÆ°á»i nÃ³i, trong sá»‘ nhá»¯ng cÃ¡i khÃ¡c. MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c Ä‘á»‘i chiáº¿u vá»›i trÆ°á»ng há»£p sá»­ dá»¥ng cÃ¡c cáº­p nháº­t tÄƒng dáº§n cho mÃ´ hÃ¬nh ASR Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Æ°á»£c trÃ¬nh bÃ y trong cÃ´ng trÃ¬nh nÃ y. Má»™t kháº£o sÃ¡t toÃ n diá»‡n vá» cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° váº­y cho viá»‡c há»c biá»ƒu diá»…n giá»ng nÃ³i trong [45]. MÃ´ hÃ¬nh upstream Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i má»™t nhiá»‡m vá»¥ pretext nhÆ° má»™t phÆ°Æ¡ng phÃ¡p sinh Ä‘á»ƒ dá»± Ä‘oÃ¡n hoáº·c tÃ¡i táº¡o Ä‘áº§u vÃ o Ä‘Æ°á»£c cho má»™t gÃ³c nhÃ¬n háº¡n cháº¿ (vÃ­ dá»¥ dá»¯ liá»‡u quÃ¡ khá»©, masking) nhÆ° autoregressive predictive coding [12]. Trong má»™t phÆ°Æ¡ng phÃ¡p tÆ°Æ¡ng pháº£n, má»™t biá»ƒu diá»…n Ä‘Æ°á»£c há»c gáº§n vá»›i má»™t máº«u tÃ­ch cá»±c vÃ  xa hÆ¡n tá»« cÃ¡c máº«u tiÃªu cá»±c; wav2vec 2.0 [3] lÃ  má»™t máº«u Ä‘iá»ƒn hÃ¬nh nÆ¡i biá»ƒu diá»…n Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ gáº§n vá»›i má»™t vector má»¥c tiÃªu Ä‘Æ°á»£c lÆ°á»£ng tá»­ hÃ³a. Cuá»‘i cÃ¹ng, trong cÃ¡c phÆ°Æ¡ng phÃ¡p dá»± Ä‘oÃ¡n [4,10,28], nhiá»‡m vá»¥ pretext lÃ  dá»± Ä‘oÃ¡n cho cÃ¡c khung thá»i gian Ä‘áº§u vÃ o Ä‘Æ°á»£c mask, má»™t phÃ¢n phá»‘i trÃªn má»™t tá»« vá»±ng rá»i ráº¡c nhÆ° cÃ¡c Ä‘áº·c trÆ°ng log-mel Ä‘Æ°á»£c phÃ¢n cá»¥m. CÃ¡c mÃ´ hÃ¬nh ASR Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c sá»­ dá»¥ng nhá»¯ng ká»¹ thuáº­t nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c cáº­p nháº­t sá»­ dá»¥ng ILASR.

7 Káº¾T LUáº¬N
ChÃºng tÃ´i Ä‘á» xuáº¥t khung cÃ´ng tÃ¡c ILASR cho há»c tÄƒng dáº§n báº£o máº­t quyá»n riÃªng tÆ° cá»§a cÃ¡c há»‡ thá»‘ng nháº­n diá»‡n giá»ng nÃ³i tá»± Ä‘á»™ng Ä‘áº§u cuá»‘i. ILASR lÃ  má»™t bÆ°á»›c tiáº¿n lá»›n cho cÃ¡c há»‡ thá»‘ng ASR cáº¥p sáº£n xuáº¥t, Ä‘áº·c biá»‡t cho cÃ¡c cáº­p nháº­t tÄƒng dáº§n tá»± Ä‘á»™ng cá»§a nhá»¯ng há»‡ thá»‘ng nÃ y. Trong nghiÃªn cá»©u huáº¥n luyá»‡n gáº§n thá»i gian thá»±c nÃ y vá»›i ILASR, chÃºng tÃ´i há»c Ä‘Æ°á»£c ráº±ng ngay cáº£ cÃ¡c mÃ´ hÃ¬nh ASR cáº¥p sáº£n xuáº¥t Ä‘Ã£ há»™i tá»¥: 1) cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ theo cÃ¡ch tÄƒng dáº§n vá»›i 3% cáº£i thiá»‡n chung cÃ³ thá»ƒ lÃªn Ä‘áº¿n 20% trÃªn cÃ¡c táº­p thá»­ nghiá»‡m vá»›i tá»« hoáº·c cá»¥m tá»« má»›i; 2) huáº¥n luyá»‡n vá»›i cÃ¡c lÃ´ lá»›n phÃ¡t sinh nhÆ° káº¿t quáº£ cá»§a cÃ¡c rÃ ng buá»™c truyá»n thÃ´ng khÃ´ng dáº«n Ä‘áº¿n suy giáº£m; 3) huáº¥n luyá»‡n replay bá»™ nhá»› hiá»‡u quáº£ trong viá»‡c giáº£m thiá»ƒu viá»‡c quÃªn tháº£m khá»‘c trÃªn cÃ¡c táº­p thá»­ nghiá»‡m cÅ© hÆ¡n; 4) khÃ´ng cÃ³ tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ cá»§a viá»‡c xá»­ lÃ½ dá»¯ liá»‡u theo thá»i gian so vá»›i ngáº«u nhiÃªn trong IL cho nháº­n dáº¡ng giá»ng nÃ³i trong khoáº£ng thá»i gian sÃ¡u thÃ¡ng; vÃ  cuá»‘i cÃ¹ng; 5) cÃ³ sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra báº£n chÃ©p mÃ¡y khÃ´ng chuyá»ƒn thÃ nh cÃ¹ng quy mÃ´ cáº£i thiá»‡n trong sinh viÃªn.

Trong tÆ°Æ¡ng lai, chÃºng tÃ´i sáº½ khÃ¡m phÃ¡ tÃ­nh há»¯u Ã­ch cá»§a sinh viÃªn á»“n Ã o cho viá»‡c tá»± há»c láº·p thay vÃ¬ dá»±a vÃ o cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn trong ILASR. Nháº­n dáº¡ng giá»ng nÃ³i trÃªn thiáº¿t bá»‹ háº¡n cháº¿ tÃ i nguyÃªn thá»i gian thá»±c váº«n lÃ  má»™t thÃ¡ch thá»©c khÃ³ khÄƒn. á» Ä‘Ã¢y, chÃºng tÃ´i dá»± Ä‘á»‹nh khÃ¡m phÃ¡ thÃªm cÃ¡c hÆ°á»›ng khÃ¡c nhau nhÆ° tÃ¬m cÃ¡c siÃªu tham sá»‘ tá»‘t nháº¥t [34], kiá»ƒm soÃ¡t gradient rÃ² rá»‰ [66], ngÄƒn cháº·n Ä‘áº£o ngÆ°á»£c gradient vÃ  cÃ¡c cuá»™c táº¥n cÃ´ng rÃ² rá»‰ dá»¯ liá»‡u [58], cÃ¡ nhÃ¢n hÃ³a ASR tÃ¹y thuá»™c vÃ o bá»‘i cáº£nh thiáº¿t bá»‹, vÃ  sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh giÃ¡o viÃªn nhá» hÆ¡n hoáº·c tá»± gÃ¡n nhÃ£n cÃ³ thá»ƒ Ä‘Æ°á»£c cháº¡y trÃªn thiáº¿t bá»‹. CÃ¡c ká»¹ thuáº­t tÃ­nh toÃ¡n gradient gáº§n Ä‘Ãºng cÃ³ thá»ƒ Ä‘Æ°á»£c yÃªu cáº§u vá»›i nhá»¯ng háº¡n cháº¿ tÃ i nguyÃªn tÃ­nh toÃ¡n nghiÃªm trá»ng. HÆ¡n ná»¯a, khÃ¡m phÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p tÃ­ch há»£p thÃ´ng tin giÃ¡m sÃ¡t yáº¿u tá»« pháº£n há»“i ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c suy luáº­n hoáº·c rÃµ rÃ ng tá»« má»™t phiÃªn tÆ°Æ¡ng tÃ¡c cÅ©ng nhÆ° cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c cáº­p nháº­t bÃªn ngoÃ i lÃ  nhá»¯ng con Ä‘Æ°á»ng nghiÃªn cá»©u thÃªm.

Lá»œI Cáº¢M Æ N
ChÃºng tÃ´i cáº£m Æ¡n Kishore Nandury, Fred Weber, vÃ  Anand Mohan vÃ¬ cÃ¡c cuá»™c tháº£o luáº­n liÃªn quan Ä‘áº¿n ASR sáº£n xuáº¥t vÃ  heuristics lá»±a chá»n phÃ¡t ngÃ´n. Valentin Mendelev há»— trá»£ vá»›i viá»‡c xÃ¢y dá»±ng táº­p thá»­ nghiá»‡m delta Ä‘á»ƒ Ä‘o lÆ°á»ng tÃ¡c Ä‘á»™ng cá»§a IL trÃªn dá»¯ liá»‡u má»›i. ChÃºng tÃ´i cáº£m Æ¡n Bach Bui, Ehry MacRostie, Chul Lee, Nikko Strom, vÃ  Shehzad Mevawalla vÃ¬ cÃ¡c cuá»™c tháº£o luáº­n há»¯u Ã­ch, Ä‘Ã¡nh giÃ¡ vÃ  há»— trá»£. ChÃºng tÃ´i mang Æ¡n nhÃ³m Nháº­n dáº¡ng Giá»ng nÃ³i Alexa vÃ¬ cÃ¡c bÃ¬nh luáº­n, xÃ¢y dá»±ng táº­p dá»¯ liá»‡u vÃ  phÃ¡t triá»ƒn cÆ¡ sá»Ÿ háº¡ táº§ng huáº¥n luyá»‡n.

TÃ€I LIá»†U THAM KHáº¢O
[1] Mohammad Al-Rubaie vÃ  J Morris Chang. Privacy-preserving machine learning: Threats and solutions. IEEE Security & Privacy, 17(2):49â€“58, 2019.
[2] Robins Anthony. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123â€“146, 1995.
[3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, vÃ  Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33:12449â€“12460, 2020.
[4] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, vÃ  Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022.
[5] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub KoneÄnÃ½, Stefano Mazzocchi, H Brendan McMahan, et al. Towards federated learning at scale: System design. arXiv preprint arXiv:1902.01046, 2019.
[6] Herve A Bourlard vÃ  Nelson Morgan. Connectionist speech recognition: a hybrid approach, volume 247. Springer Science & Business Media, 2012.
[7] Han Cai, Chuang Gan, Ligeng Zhu, vÃ  Song Han. Tinytl: Reduce memory, not parameters for efficient on-device learning. Advances in Neural Information Processing Systems, 33:11285â€“11297, 2020.
[8] Francisco M Castro, Manuel J MarÃ­n-JimÃ©nez, NicolÃ¡s Guil, Cordelia Schmid, vÃ  Karteek Alahari. End-to-end incremental learning. Trong Proceedings of the European conference on computer vision (ECCV), pages 233â€“248, 2018.
[9] Kai Chen vÃ  Qiang Huo. Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering. Trong 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5880â€“5884, 2016. doi: 10.1109/ICASSP.2016.7472805.
[10] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv preprint arXiv:2110.13900, 2021.

--- TRANG 7 ---
KDD '22, August 14â€“18, 2022, Washington, DC, USA Gopinath Chennupati et al.

[11] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Gonina, et al. State-of-the-art speech recognition with sequence-to-sequence models. Trong 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4774â€“4778. IEEE, 2018.
[12] Yu-An Chung vÃ  James Glass. Generative pre-training for speech with autoregressive predictive coding. Trong ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3497â€“3501. IEEE, 2020.
[13] Xiaodong Cui, Songtao Lu, vÃ  Brian Kingsbury. Federated acoustic modeling for automatic speech recognition. CoRR, abs/2102.04429, 2021.
[14] Subhadeep Dey, Petr Motlicek, Trung Bui, vÃ  Franck Dernoncourt. Exploiting semi-supervised training through a dropout regularization in end-to-end speech recognition. arXiv preprint arXiv:1908.05227, 2019.
[15] Tim Dierks vÃ  Eric Rescorla. The transport layer security (tls) protocol version 1.2. 2008.
[16] Dimitrios Dimitriadis, Kenichi Kumatani, Robert Gmyr, Yashesh Gaur, vÃ  Sefik Emre Eskimez. A federated approach in training acoustic models. Trong Proc. Interspeech, 2020.
[17] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128â€“135, 1999.
[18] Yan Gao, Titouan Parcollet, Javier Fernandez-Marques, Pedro P. B. de Gusmao, Daniel J. Beutel, vÃ  Nicholas D. Lane. End-to-end speech recognition from federated acoustic models, 2021.
[19] Robin C Geyer, Tassilo Klein, vÃ  Moin Nabi. Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557, 2017.
[20] Noah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, Amir Gholami, Kai Rothauge, Michael W Mahoney, vÃ  Joseph Gonzalez. On the computational inefficiency of large batch sizes for stochastic gradient descent. arXiv preprint arXiv:1811.12941, 2018.
[21] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, vÃ  Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013.
[22] Priya Goyal, Piotr DollÃ¡r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, vÃ  Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
[23] Filip Granqvist, Matt Seigel, Rogier van Dalen, A'ine Cahill, Stephen Shum, vÃ  Matthias Paulik. Improving on-device speaker verification using federated learning with privacy, 2020.
[24] Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711, 2012.
[25] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020.
[26] Dhruv Guliani, FranÃ§oise Beaufays, vÃ  Giovanni Motta. Training speech recognition models with federated learning: A quality/cost framework. Trong ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3080â€“3084. IEEE, 2021.
[27] Elad Hoffer, Itay Hubara, vÃ  Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741, 2017.
[28] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, vÃ  Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451â€“3460, 2021.
[29] Hui Jiang. Confidence measures for speech recognition: A survey. Speech communication, 45(4):455â€“470, 2005.
[30] Kaustubh Kalgaonkar, Chaojun Liu, Yifan Gong, vÃ  Kaisheng Yao. Estimating confidence scores on asr results using recurrent neural networks. Trong 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4999â€“5003. IEEE, 2015.
[31] Shigeki Karita, Shinji Watanabe, Tomoharu Iwata, Atsunori Ogawa, vÃ  Marc Delcroix. Semi-supervised end-to-end speech recognition. Trong Interspeech, pages 2â€“6, 2018.
[32] Shigeki Karita, Shinji Watanabe, Tomoharu Iwata, Marc Delcroix, Atsunori Ogawa, vÃ  Tomohiro Nakatani. Semi-supervised end-to-end speech recognition using text-to-speech and autoencoders. Trong ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6166â€“6170. IEEE, 2019.
[33] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, vÃ  Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
[34] Mikhail Khodak, Tian Li, Liam Li, M Balcan, Virginia Smith, vÃ  Ameet Talwalkar. Weight sharing for hyperparameter optimization in federated learning. Trong Int. Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2020, 2020.
[35] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. Trong ACL, 2018.
[36] Bo Li, Tara N Sainath, Ruoming Pang, vÃ  Zelin Wu. Semi-supervised training for end-to-end models via weak distillation. Trong ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2837â€“2841. IEEE, 2019.
[37] Mu Li, Tong Zhang, Yuqiang Chen, vÃ  Alexander J. Smola. Efficient mini-batch training for stochastic optimization. Trong Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14, page 661â€“670. ACM, 2014.
[38] Tao Lin, Sebastian U. Stich, vÃ  Martin Jaggi. Don't use large mini-batches, use local sgd. ArXiv, abs/1808.07217, 2020.
[39] Shaoshi Ling, Yuzong Liu, Julian Salazar, vÃ  Katrin Kirchhoff. Deep contextualized acoustic representations for semi-supervised speech recognition. Trong ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6429â€“6433. IEEE, 2020.
[40] Prerna Mahajan vÃ  Abhishek Sachdeva. A study of encryption algorithms aes, des and rsa for security. Global Journal of Computer Science and Technology, 2013.
[41] Dominic Masters vÃ  Carlo Luschi. Revisiting small batch training for deep neural networks. arXiv preprint arXiv:1804.07612, 2018.
[42] Sam McCandlish, Jared Kaplan, Dario Amodei, vÃ  OpenAI Dota Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018.
[43] Michael McCloskey vÃ  Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Trong Psychology of learning and motivation, volume 24, pages 109â€“165. Elsevier, 1989.
[44] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, vÃ  Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. Trong Artificial Intelligence and Statistics, pages 1273â€“1282. PMLR, 2017.
[45] Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D Havtorn, Joakim Edin, Christian Igel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars MaalÃ¸e, et al. Self-supervised speech representation learning: A review. arXiv preprint arXiv:2205.10643, 2022.
[46] Climent Nadeu CamprubÃ­, Francisco Javier Hernando PericÃ¡s, vÃ  Monica Gorricho Moreno. On the decorrelation of filter-bank energies in speech recognition. Trong EUROSPEECH'95: 4th European Conference on Speech Communication and Technology: Madrid, Spain: 18-21 September 1995, pages 1381â€“1384, 1995.
[47] Vassil Panayotov, Guoguo Chen, Daniel Povey, vÃ  Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. Trong 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206â€“5210. IEEE, 2015.
[48] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, vÃ  Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019.
[49] Sree Hari Krishnan Parthasarathi vÃ  Nikko Strom. Lessons from building acoustic models with a million hours of speech. Trong ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6670â€“6674. IEEE, 2019.
[50] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub KoneÄnÃ½, Sanjiv Kumar, vÃ  H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.
[51] Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, vÃ  Virginia Smith. On the convergence of federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 3:3, 2018.
[52] Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, vÃ  George E Dahl. Measuring the effects of data parallelism on neural network training. arXiv preprint arXiv:1811.03600, 2018.
[53] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, vÃ  Quoc V. Le. Don't decay the learning rate, increase the batch size, 2018.
[54] H. Soltau, H. Liao, vÃ  H. Sak. Reducing the computational complexity for whole word models. 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 63â€“68, 2017.
[55] Nikko Strom. Scalable distributed dnn training using commodity gpu cloud computing. Trong Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[56] Peng Sun, Wansen Feng, Ruobing Han, Shengen Yan, vÃ  Yonggang Wen. Optimizing network performance for distributed dnn training on gpu clusters: Imagenet/alexnet training in 1.5 minutes, 2019.
[57] Prakhar Swarup, Roland Maas, Sri Garimella, Sri Harish Mallidi, vÃ  BjÃ¶rn Hoffmeister. Improving asr confidence scores for alexa using acoustic and hypothesis embeddings. Trong Interspeech, pages 2175â€“2179, 2019.
[58] Aleksei Triastcyn vÃ  Boi Faltings. Federated generative privacy. IEEE Intelligent Systems, 35(4):50â€“57, 2020.
[59] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, vÃ  Yasaman Khazaeni. Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020.
[60] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, vÃ  H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization.

--- TRANG 8 ---
KDD '22, August 14â€“18, 2022, Washington, DC, USA Gopinath Chennupati et al.

arXiv preprint arXiv:2007.07481, 2020.
[61] Felix Weninger, Franco Mana, Roberto Gemello, JesÃºs AndrÃ©s-Ferrer, vÃ  Puming Zhan. Semi-supervised learning with data augmentation for end-to-end asr. arXiv preprint arXiv:2007.13876, 2020.
[62] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, vÃ  Yun Fu. Large scale incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 374â€“382, 2019.
[63] Alex Xiao, Christian Fuegen, vÃ  Abdelrahman Mohamed. Contrastive semi-supervised learning for asr. Trong ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3870â€“3874. IEEE, 2021.
[64] Yang You, Igor Gitman, vÃ  Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training. arXiv preprint arXiv:1708.03888, 6:12, 2017.
[65] Bo Zhao, Konda Reddy Mopuri, vÃ  Hakan Bilen. idlg: Improved deep leakage from gradients. arXiv preprint arXiv:2001.02610, 2020.
[66] Ligeng Zhu, Zhijian Liu, vÃ  Song Han. Deep leakage from gradients. Advances in Neural Information Processing Systems, 32, 2019.
