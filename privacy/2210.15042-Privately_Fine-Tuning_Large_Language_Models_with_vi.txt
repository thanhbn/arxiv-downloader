# 2210.15042.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/privacy/2210.15042.pdf
# Kích thước tệp: 458121 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Tinh Chỉnh Riêng Tư các Mô hình Ngôn ngữ Lớn với
Quyền riêng tư Vi phân
Rouzbeh Behnia
Trường Hệ thống Thông tin
và Quản lý
Đại học South Florida
Sarasota, USA
behnia@usf.eduMohammadreza (Reza) Ebrahimi
Trường Hệ thống Thông tin
và Quản lý
Đại học South Florida
Tampa, USA
ebrahimim@usf.eduJason Pacheco
Khoa Khoa học Máy tính
Đại học Arizona
Tucson, USA
pachecoj@cs.arizona.edu

Balaji Padmanabhan
Trường Hệ thống Thông tin
và Quản lý
Đại học South Florida
Tampa, USA
bp@usf.edu

Tóm tắt —Các Mô hình Ngôn ngữ Lớn (LLM) được huấn luyện trước là
một phần không thể thiếu của AI hiện đại đã dẫn đến những thành tựu đột phá
trong các nhiệm vụ AI phức tạp. Các công ty AI lớn với cơ sở hạ tầng đắt tiền
có thể phát triển và huấn luyện những mô hình lớn này với hàng tỷ và hàng triệu
tham số từ đầu. Các bên thứ ba, nhà nghiên cứu và người thực hành ngày càng
áp dụng những mô hình được huấn luyện trước này và tinh chỉnh chúng trên dữ liệu
riêng của họ để hoàn thành các nhiệm vụ AI hạ lưu của họ. Tuy nhiên,
đã được chứng minh rằng một kẻ thù có thể trích xuất/tái tạo lại các
mẫu huấn luyện chính xác từ những LLM này, điều này có thể dẫn đến
việc tiết lộ thông tin nhận dạng cá nhân. Vấn đề này đã làm dấy lên
những lo ngại sâu sắc về quyền riêng tư của LLM. Quyền riêng tư vi phân
(DP) cung cấp một khung nghiêm ngặt cho phép thêm nhiễu trong
quá trình huấn luyện hoặc tinh chỉnh LLM sao cho việc trích xuất
dữ liệu huấn luyện trở nên không khả thi (tức là, với xác suất thành công
nhỏ về mặt mật mã). Trong khi các đảm bảo quyền riêng tư lý thuyết
được cung cấp trong hầu hết các nghiên cứu hiện tại giả định việc học các mô hình
từ đầu thông qua nhiều lần lặp huấn luyện trong một thiết lập tiệm cận,
giả định này không đúng trong các tình huống tinh chỉnh trong
đó số lần lặp huấn luyện nhỏ hơn đáng kể.
Để giải quyết khoảng cách này, chúng tôi trình bày EW-Tune, một khung DP
để tinh chỉnh LLM dựa trên kế toán Edgeworth với
các đảm bảo quyền riêng tư mẫu hữu hạn. Kết quả của chúng tôi trên bốn nhiệm vụ
hiểu ngôn ngữ tự nhiên (NLU) được thiết lập tốt cho thấy
rằng trong khi EW-Tune thêm các đảm bảo quyền riêng tư vào quá trình tinh chỉnh
LLM, nó trực tiếp góp phần giảm nhiễu được tạo ra lên đến
5,6% và cải thiện hiệu suất LLM tiên tiến lên đến 1,1% trên tất cả
các nhiệm vụ NLU. Chúng tôi đã mở mã nguồn các triển khai của mình để
được áp dụng rộng rãi và phục vụ mục đích thử nghiệm công khai.
Từ khóa Chỉ mục —Quyền riêng tư vi phân, mô hình ngôn ngữ lớn, tinh
chỉnh, kế toán Edgeworth

I. GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã trở thành một thành phần
không thể thiếu của AI hiện đại. Các kiến trúc học sâu với hàng
tỷ tham số thường được thiết kế dựa trên transformer,
một khối xây dựng lần đầu được giới thiệu bởi BERT của Google [1]. LLM
cung cấp hiệu suất đột phá trong các nhiệm vụ AI phức tạp như
các tác giả đóng góp bằng nhau (được sắp xếp theo thứ tự bảng chữ cái theo họ.)hệ thống đối thoại [2] và tạo văn bản/câu chuyện tự động [3]. Được trang bị cơ sở hạ tầng phần cứng, các công ty AI lớn như Open AI và Facebook cung cấp các LLM mới được huấn luyện trên dữ liệu công khai từ Internet [4], [5]. Các ví dụ phổ biến bao gồm RoBERTa [4] và GPT [5]. Bộ dữ liệu huấn luyện của RoBERTa bao gồm Wikipedia tiếng Anh và hàng triệu tin tức trực tuyến được thu thập từ internet. Tương tự, GPT được huấn luyện trên các liên kết ra ngoài từ Reddit.

Các nhà nghiên cứu và người thực hành AI thường tinh chỉnh những mô hình được huấn luyện trước này trên các nhiệm vụ AI hạ lưu của họ bằng cách sử dụng dữ liệu riêng của chính họ để hoàn thành các nhiệm vụ hạ lưu như phát hiện phần mềm độc hại [6], tạo văn bản thành hình ảnh [7]. Tuy nhiên, gần đây, đã được chứng minh rằng những mô hình được huấn luyện trước này dễ bị tấn công quyền riêng tư [8]. Vấn đề này chủ yếu do xu hướng của mô hình ghi nhớ các mẫu huấn luyện mà không bị quá khớp, còn được gọi là "vấn đề ghi nhớ" [9]. Vấn đề này có thể dẫn đến ba loại tấn công quyền riêng tư chính: suy luận thành viên, đảo ngược mô hình và trích xuất dữ liệu huấn luyện.

Suy luận thành viên [10]: xác định liệu dữ liệu của một người dùng nhất định có được bao gồm trong quá trình huấn luyện hay không.

Đảo ngược mô hình [11]: xấp xỉ việc tái tạo dữ liệu huấn luyện.

Trích xuất dữ liệu huấn luyện [8]: nhằm tiết lộ chính xác các mẫu huấn luyện điều này làm cho loại tấn công này trở thành mạnh nhất với những hậu quả bất lợi nhất đối với người dùng.

Trong khi cả ba loại tấn công đều có thể gây nguy hiểm cho quyền riêng tư của những người dùng có thông tin trong dữ liệu huấn luyện, việc trích xuất dữ liệu huấn luyện nhắm mục tiêu trực tiếp vào thông tin nhận dạng cá nhân của người dùng và có thể gây nguy hiểm cho danh tính của người dùng thông qua việc tiết lộ thông tin quan trọng như địa chỉ, số an sinh xã hội, số điện thoại, v.v. Các LLM được tinh chỉnh được sử dụng bởi các bên thứ ba trên dữ liệu riêng của họ sẽ đối mặt với cùng những lo ngại về quyền riêng tư
1arXiv:2210.15042v3  [cs.CR]  20 Mar 2023

--- TRANG 2 ---
lo ngại. Những lo ngại về quyền riêng tư xung quanh vấn đề này đòi hỏi các phương pháp bảo vệ quyền riêng tư cho việc tinh chỉnh LLM. Một phương pháp như vậy sẽ cho phép các bên thứ ba tinh chỉnh riêng tư các LLM trên dữ liệu riêng của họ mà không có bất kỳ rò rỉ thông tin nào về các mẫu huấn luyện riêng của họ.

Quyền riêng tư Vi phân (DP) là một phương pháp đầy hứa hẹn để đảm bảo quyền riêng tư dữ liệu huấn luyện với các đảm bảo lý thuyết [12]. DP cung cấp một khung nghiêm ngặt về mặt toán học với các đảm bảo quyền riêng tư cho phép Gradient Descent Ngẫu nhiên (SGD), nền tảng của việc học trong LLM, trong một thiết lập riêng tư. Trong thiết lập như vậy, SGD có thể được áp dụng như một cơ chế ngẫu nhiên nhiều lần trong mỗi lần lặp của quá trình huấn luyện.

Hầu hết các phương pháp DP cung cấp các đảm bảo tiệm cận. Đối với các đảm bảo lý thuyết, số lần ứng dụng SGD (được gọi là các tổ hợp) thường được giả định là không giới hạn trong hầu hết các nghiên cứu về quyền riêng tư. Giả định này dẫn đến các đảm bảo tiệm cận trong những nghiên cứu này (tức là, các tổ hợp vô hạn của SGD trong giới hạn). Tuy nhiên, trong việc tinh chỉnh LLM, số lần lặp SGD không chỉ bị giới hạn mà còn khá nhỏ (tức là, theo thứ tự của vài nghìn) [13].

Trong nghiên cứu này, thông qua lăng kính DP, và nhờ vào đảm bảo mẫu hữu hạn đạt được bởi khai triển Edgeworth [14], chúng tôi đề xuất một khung tinh chỉnh LLM mới, được gọi là EW-Tune, với các đảm bảo mẫu hữu hạn. EW-Tune hoạt động dựa trên một phương pháp kế toán DP hiệu quả được gọi là kế toán Edgeworth, được đề xuất trong [14]. Kế toán Edgeworth tính toán lượng nhiễu cần thiết được thêm vào các gradient trong SGD để đảm bảo một ngân sách quyền riêng tư nhất định (xem Phần II-B). EW-Tune cũng tận dụng kỹ thuật tái tham số hóa hiệu quả mới nhất được đề xuất trong [15].

A. Đóng góp của chúng tôi
Trong khi EW-Tune là một khung tổng quát, chúng tôi thể hiện hiệu suất của nó bằng cách tập trung vào ứng dụng của nó để tăng cường quyền riêng tư của LLM trong quá trình tinh chỉnh. Đóng góp của chúng tôi vào việc tinh chỉnh riêng tư của LLM có hai mặt:

Nghiên cứu của chúng tôi đóng vai trò là bước đầu tiên hướng tới việc tinh chỉnh LLM trong một thiết lập quyền riêng tư vi phân khi số lượng tổ hợp (tức là, các ứng dụng của SGD quyền riêng tư vi phân) là hữu hạn và chỉ giới hạn ở vài nghìn (ít hơn 4.000 lần trong các thí nghiệm của chúng tôi). So với các phương pháp hiện tại cung cấp giới hạn tiệm cận trên ngân sách quyền riêng tư, thông qua việc sử dụng kế toán Edgeworth, EW-Tune có thể cung cấp giới hạn quyền riêng tư không tiệm cận bằng cách sử dụng giới hạn Berry-Esseen được rút ra từ xấp xỉ Edgeworth. Trong trường hợp tinh chỉnh LLM, với số lượng tổ hợp hữu hạn, đối với cùng một ngân sách quyền riêng tư, EW-Tune tạo ra ít nhiễu hơn cho SGD so với các phương pháp tiên tiến. Điều này trực tiếp cải thiện việc học và độ chính xác của mô hình.

Điều được biết là trong khi tinh chỉnh thông qua DP tăng cường quyền riêng tư của mô hình, nó có thể ảnh hưởng tiêu cực đến tiện ích (tức là, hiệu suất) của mô hình [12]. Các thí nghiệm của chúng tôi cho thấy rằng EW-Tune đóng góp đáng kể vào hiện trạng bằng cách tăng cường quyền riêng tư của LLM trong khi bảo tồn tiện ích/độ chính xác của chúng so với nhiều phương pháp thay thế gần đây qua một số nhiệm vụ đánh giá quan trọng bao gồm phân loại văn bản, phát hiện kéo theo và trả lời câu hỏi. Tổng thể, EW-Tune giảm nhiễu được tạo ra cho SGD lên đến 5,6%. EW-Tune cũng tăng cường độ chính xác của mô hình tiên tiến lên đến 1,1%.

II. BỐI CẢNH VÀ CÔNG TRÌNH LIÊN QUAN
Chúng tôi xem xét ba lĩnh vực của văn hệ: (1) LLM để xác định hiện trạng trong mô hình hóa ngôn ngữ và việc tinh chỉnh của chúng. (2) Học sâu quyền riêng tư vi phân như khung tổng thể để đảm bảo nghiêm ngặt quyền riêng tư của việc tinh chỉnh LLM. (3) Kế toán Edgeworth như một phương pháp kế toán mới nổi cung cấp các đảm bảo mẫu hữu hạn, có thể là một công cụ hữu ích cho việc tinh chỉnh LLM.

A. Các Mô hình Ngôn ngữ Lớn (LLM)
Các mô hình ngôn ngữ lớn là các kiến trúc mạng nơ-ron sâu với hàng tỷ tham số [16]–[18]. Chúng thường được hưởng lợi từ kiến trúc mã hóa-giải mã tạo ra các biểu diễn chất lượng cao từ dữ liệu chuỗi (văn bản, hình ảnh, phần mềm độc hại, gen, v.v.). Hầu hết LLM sử dụng các loại lớp cụ thể với cơ chế tự chú ý được gọi là transformer để gán trọng số động cho các phần tử đầu vào dựa trên bối cảnh xung quanh của chúng [16]. Transformer cho phép LLM cung cấp các biểu diễn chất lượng cao của chuỗi đầu vào. Ở mức độ cao, LLM có thể được phân loại thành hai loại: che mặt nạ và tự hồi quy.

Các mô hình ngôn ngữ che mặt nạ được huấn luyện để dự đoán một token bị che dựa trên môi trường xung quanh của nó. Các ví dụ hiệu quả cao của mô hình ngôn ngữ che mặt nạ bao gồm BERT [1] và RoBERTa [16]. Ngược lại, các mô hình ngôn ngữ tự hồi quy học cách dự đoán token tiếp theo dựa trên những token được tạo trước đó, điều này làm cho chúng phù hợp cho các nhiệm vụ tạo văn bản [4], [19].

Do khả năng tạo ra các biểu diễn chất lượng cao từ đầu vào, các mô hình ngôn ngữ che mặt nạ được sử dụng rộng rãi trong các nhiệm vụ AI hạ lưu chính bao gồm phân loại văn bản, trả lời câu hỏi, phát hiện kéo theo ngữ nghĩa và nhận dạng giọng nói.

Các LLM được huấn luyện trước thường được tinh chỉnh trên các nhiệm vụ và bộ dữ liệu cụ thể, thông qua đó các trọng số của mô hình gốc được cập nhật để điều chỉnh tốt hơn cho dữ liệu cụ thể của miền và nhiệm vụ hiện tại.

B. Học Sâu Quyền riêng tư Vi phân
Quyền riêng tư vi phân [20], được định nghĩa chính thức trong Định nghĩa 1, tính toán đảm bảo quyền riêng tư khi kết quả của một thuật toán, chạy trên dữ liệu riêng, được công khai. Khi áp dụng cho học máy, một cơ chế quyền riêng tư vi phân (DP) cho phép công bố công khai các tham số mô hình trong khi đảm bảo quyền riêng tư của dữ liệu huấn luyện gốc.

Định nghĩa 1: Một cơ chế ngẫu nhiên M:X !Y là (;)-DP, nếu đối với tất cả các bộ dữ liệu lân cận X;X02X, chỉ khác nhau ở một phần tử duy nhất, và tất cả Y Y ,P(M(X)2Y) eP(M(X0)2Y) + giữ.

Trong Định nghĩa 1, (;) thường được gọi là ngân sách quyền riêng tư.  định nghĩa khoảng cách giữa hai bên của

--- TRANG 3 ---
phương trình và δ định nghĩa xác suất thất bại. Quyền riêng tư vi phân có các tính chất như tính mạnh mẽ đối với thông tin phụ trợ và tính tổ hợp. Tính chất đầu đảm bảo quyền riêng tư ngay cả khi có thông tin bên ngoài mới xuất hiện cho kẻ thù và tính chất sau cho phép thiết kế mô đun của các cơ chế. Về cơ bản, tính tổ hợp có nghĩa là nếu hai cơ chế M1() và M2() là DP, và M(X) = (M1(X);M2(X)), thì M cũng là quyền riêng tư vi phân.

Gradient Descent Ngẫu nhiên quyền riêng tư vi phân (DP-SGD). Tiêu chuẩn vàng để đạt được quyền riêng tư vi phân trong học sâu là cập nhật các tham số mạng nơ-ron với các gradient nhiễu. Điều này được đạt được bởi một thuật toán ngẫu nhiên được gọi là DP-SGD [12] thông qua hai bước sau:

–Cắt gradient: cho một chuẩn cắt C, gradient của mỗi mẫu x, g(x), được cắt g'(x) = g(x)/max(1;||g(x)||2/C).

–Cơ chế Gaussian: các gradient đã cắt được tổng hợp và sau đó một nhiễu Gaussian đẳng hướng từ N(0;C²σ²), với σ như một hệ số nhân nhiễu, sẽ được thêm vào các gradient.

Hệ số nhân nhiễu trong DP-SGD được xác định bởi ngân sách quyền riêng tư (ε;δ), số vòng huấn luyện m, và xác suất lấy mẫu q=B/N cho kích thước lô B và N là tổng số mẫu.

Trong công trình đột phá của họ, Abadi và cộng sự [12] đã giới thiệu một phương pháp được gọi là kế toán Moments (MA) để tính toán giới hạn trên cho đường cong quyền riêng tư của các tổ hợp thuật toán DP. Phương pháp này sau đó được sử dụng để theo dõi mất mát quyền riêng tư của DP-SGD bằng cách tính toán đường cong quyền riêng tư cho mỗi lần lặp huấn luyện với chính nó m lần, trong đó m là tổng số lần lặp. Trong [21], khung MA được khởi tạo với Renyi Differential Privacy (RDP). Tuy nhiên, những thuật toán này, mặc dù hiệu quả (thời gian chạy độc lập với m), cung cấp một giới hạn trên khá không thực tế.

Khung Gaussian Differential Privacy (GDP) [22], [23] còn được gọi là f-DP được thiết kế dựa trên định lý giới hạn trung tâm (CLT). Khung GDP cung cấp một đặc tính hóa tốt của quyền riêng tư vi phân sử dụng cách diễn giải kiểm định giả thuyết [24]. GDP chỉ có thể cung cấp một xấp xỉ cho đường cong quyền riêng tư và đã được chứng minh là báo cáo thấp giá trị epsilon thực trong [25].

Sử dụng khái niệm biến ngẫu nhiên mất mát quyền riêng tư (PRV) [26], Meiser và Mohammadi [27] đã giới thiệu một thuật toán được gọi là bucket quyền riêng tư để tổ hợp xấp xỉ các đường cong quyền riêng tư. Bằng cách sử dụng khái niệm PRV, người ta có thể tận dụng tính chất tốt đẹp của PRV để tính toán tổ hợp của m cơ chế M=M1∘M2∘...∘Mm bằng cách đơn giản là tổng các PRV tương ứng của chúng D=∑ᵢ₌₁ᵐDᵢ. Phân phối của D sau đó có thể được xấp xỉ bằng cách tính toán tích chập của các phân phối cơ bản D1,...,Dm. Koskela và cộng sự [28] đã sử dụng biến đổi Fourier nhanh (FFT) để tính toán tích chập một cách hiệu quả. Theo [28], Gopi và cộng sự [25] đã tận dụng FFT để tổ hợp số các hàm đánh đổi. Kế toán của họ, được gọi là kế toán PRV, giải quyết việc đánh giá thấp của f-DP và cung cấp giới hạn trên và giới hạn dưới về rò rỉ của ε.

C. Kế toán Edgeworth
Như đã lưu ý, trung tâm của EW-Tune là kế toán Edgeworth [14]. Kế toán Edgeworth dựa vào f-DP [23], như đã thảo luận ở trên, cung cấp một đặc tính hóa đầy đủ của quyền riêng tư vi phân bằng cách sử dụng cách diễn giải kiểm định giả thuyết. Một cách không chính thức, quyền riêng tư vi phân đo lường độ khó trong việc phân biệt bất kỳ cặp bộ dữ liệu (lân cận) nào dựa trên thông tin thu được từ một cơ chế M. Trong [23], các tác giả đã hình thức hóa khái niệm không thể phân biệt như một bài toán kiểm định giả thuyết cho hai bộ dữ liệu lân cận S và S'. Do đó, các giả thuyết được hình thành như H0: bộ dữ liệu cơ bản là S và H1: bộ dữ liệu cơ bản là S', trong đó đầu ra của M là cơ sở để tiến hành bài toán kiểm định giả thuyết. Hãy để P và Q biểu thị các phân phối xác suất của M(S) và M(S'), tương ứng. Bây giờ, đối với một quy tắc từ chối ϕ∈[0,1], và các giả thuyết H0:P và H1:Q, hàm đánh đổi f=T(P;Q)(α) = inf_{ϕ:α(ϕ)≤α}β(ϕ) định nghĩa ánh xạ từ lỗi Type-I đến lỗi Type-II, trong đó α=E_P[ϕ] và β= 1−E_Q[ϕ]. Để tính toán tổ hợp của các hàm đánh đổi có dạng f=⊗ᵢ₌₁ᵐfᵢ, hãy thực hiện tổ hợp thứ i bởi hai giả thuyết H_{0,i}=∏ᵢwᵢPᵢ và H_{1,i}=∏ᵢwᵢQᵢ. Bây giờ, để đánh giá hàm đánh đổi f=⊗ᵢ₌₁ᵐfᵢ, chúng ta phân biệt giữa hai giả thuyết tổ hợp H_{0,i}=∏w P₁⊗P₂⊗...⊗Pₘ và H_{1,i}=∏w Q₁⊗Q₂⊗...⊗Qₘ cho w= (w₁,...,wₘ).

Kế toán Edgeworth [14] định nghĩa các biến ngẫu nhiên được gọi là tỷ số log-likelihood mất mát quyền riêng tư (PLLR) để cho phép chuyển đổi không mất mát đảm bảo f-DP thành một tập hợp các đảm bảo (ε;δ)-DP. PLLR được định nghĩa như các đạo hàm Radon-Nikodym của các giả thuyết trên như Xᵢ≜log dQᵢ(ωᵢ)/dPᵢ(ωᵢ) và Yᵢ≜log dQᵢ(ωᵢ)/dPᵢ(ωᵢ) cho Pᵢ và Qᵢ. Các tác giả trong [14] đã chỉ ra mối quan hệ nguyên-đối ngẫu giữa f-DP và một tập hợp (ε;δ(ε))-DP thông qua δ= 1−F_{Y,m}(ε)−e^ε(1−F_{X,m}(ε)) trong đó F_{X,m} và F_{Y,m} là các CDF của ∑ᵢ₌₁ᵐXᵢ và ∑ᵢ₌₁ᵐYᵢ, tương ứng. Để tính toán PLLR thông qua các giả thuyết tổ hợp, kế toán Edgeworth sử dụng một họ các dãy PLLR để tổ hợp hàm đánh đổi chặt chẽ nhất có thể thỏa mãn tất cả f(α)-DP.

Giả định rằng đối với mỗi α, người ta có thể tìm một chuỗi PLLR tương ứng với f(α), chúng ta có thể tính toán một tập hợp đảm bảo (ε;δ(ε)(α))-DP¹. Sau đó người ta phải tính toán một CDF xấp xỉ như một biến ngẫu nhiên X=∑ᵢ₌₁ᵐXᵢ sử dụng khai triển Edgeworth để đầu ra của kế toán Edgeworth xấp xỉ như F_{X,m} và F_{Y,m}.

III. PHƯƠNG PHÁP ĐỀ XUẤT

A. Mô hình Mối đe dọa
Khả năng và mục tiêu của kẻ thù. Chúng tôi xem xét một kẻ thù A có quyền truy cập hộp đen vào mô hình ngôn ngữ. Trong công trình này, theo [8], chúng tôi giả định rằng A không có quyền truy cập vào các trọng số cụ thể và trạng thái ẩn của mô hình nhưng có thể thu được các dự đoán từ tiếp theo và tính toán xác suất của các chuỗi tùy ý ví dụ thông qua quyền truy cập vào các mô hình tự động hoàn thành

¹Các tác giả trong [14] thảo luận về cách người ta có thể đảo ngược phương trình để tính toán α cho δ đã cho
3

--- TRANG 4 ---
Kế toán
Edgeworth

Dữ liệu Riêng tư    Tham số Quyền riêng tư (ε,δ)    EW-Tune

Dữ liệu Công khai                                     LLM Được Tinh chỉnh Riêng tư

                                                      Hệ số Nhân Nhiễu
                                                      (σ)
LLM Được Huấn luyện Trước

                    DP-SGD
                    với RGP

Hình 1. Góc nhìn Trừu tượng của Khung EW-Tune Đề xuất

mô hình. Mục tiêu cuối cùng của kẻ thù là trích xuất dữ liệu huấn luyện (đã được ghi nhớ) từ mô hình. Mức độ nghiêm trọng của một cuộc tấn công tăng lên nếu có nhiều ví dụ hơn có thể được trích xuất từ mô hình.

Mục tiêu và nhiệm vụ của kẻ thù. Trong khi EW-Tune là một khung tổng quát có thể được áp dụng để tăng cường quyền riêng tư của bất kỳ LLM nào trong quá trình tinh chỉnh. Để cụ thể hóa, chúng tôi tập trung vào một trong những mô hình ngôn ngữ che mặt nạ được áp dụng cao nhất trong các nhiệm vụ AI, một người kế thừa của BERT của Google, tên là roBERTa [16]. roBERTa chủ yếu nợ sự phổ biến của nó vào khả năng học biểu diễn hai chiều của câu. Những biểu diễn chất lượng cao này không có sẵn trong các mô hình ngôn ngữ tự hồi quy như GPT, đặc biệt đóng góp vào kết quả đột phá trong các nhiệm vụ hiểu ngôn ngữ tự nhiên (NLU) hạ lưu phổ biến như phân tích cảm xúc và phân loại văn bản. Ngoài ra, để cho thấy tính tổng quát của EW-Tune, chúng tôi sẽ kiểm tra tiện ích và đảm bảo quyền riêng tư của nó trên bốn nhiệm vụ NLU quan trọng và phức tạp (tất cả đều được bao gồm trong bộ dữ liệu đánh giá General Language Understanding Evaluation (GLUE) nổi tiếng [29]). Mỗi nhiệm vụ được liên kết với một bộ dữ liệu được thiết lập tốt:

MNLI [30]: Multi-Genre Natural Language Inference (MNLI) là một tập hợp 433.000 cặp câu được chú thích với thông tin kéo theo ngữ nghĩa [30]. Nhiệm vụ của LLM trong kho ngữ liệu này là xác định mối quan hệ ngữ nghĩa giữa một cặp câu đã cho (kéo theo, mâu thuẫn, hoặc mối quan hệ trung tính).

QNLI [29]: Question-answering Natural Language Inference (QNLI) là một bộ dữ liệu suy luận ngôn ngữ tự nhiên được thu thập từ Wikipedia bao gồm 110.400 cặp câu hỏi-đoạn văn, trong đó chỉ một trong những câu trong đoạn văn là câu trả lời cho câu hỏi tương ứng. Nhiệm vụ của LLM là xác định liệu một câu có bao gồm câu trả lời cho một câu hỏi đã cho hay không.

QQP [29]: Bộ dữ liệu Quora Question Pairs (QQP) bao gồm hơn 400.000 cặp câu hỏi. Mỗi cặp câu hỏi được chú thích để chỉ ra liệu những câu hỏi này có tương đương về mặt ngữ nghĩa (tức là, paraphrase của nhau) hay không. Nhiệm vụ của LLM là xác định liệu một trong hai câu hỏi có phải là paraphrase của câu hỏi kia hay không.

SST-2 [31]: Stanford Sentiment Treebank (SST-2) bao gồm 68.800 câu từ các đánh giá phim và chú thích về cảm xúc của chúng. Nhiệm vụ của LLM là dự đoán cảm xúc (tích cực hoặc tiêu cực) của một câu đã cho.

Để vận hành hóa việc phòng thủ chống lại mô hình mối đe dọa trên, chúng tôi đề xuất EW-Tune, một khung tổng quát để tinh chỉnh LLM cho các nhiệm vụ hạ lưu khác nhau. Hình 1 mô tả các thành phần của khung EW-Tune đề xuất của chúng tôi.

Thuật toán 1 Khung EW-Tune
1:Đầu vào: Các ví dụ {xᵢ}ᴺᵢ₌₁, các cơ chế {Mᵢ}ᵐᵢ₌₁, các ma trận trọng số {W⁽ˡ⁾}ᴴₗ₌₁, các bước khởi động Tᵤ, kích thước nhóm B, giới hạn cắt gradient C, xác suất thất bại δ và ngân sách quyền riêng tư ban đầu ε và khai triển Edgeworth bậc k.
2:Cho xác suất lấy mẫu q=B/N, đối với δ đã cho, đối với mỗi cơ chế và tất cả α mã hóa PLLR [{Xᵢ;Yᵢ}] và các cumulant lên đến bậc k+ 2
3:Đối với mỗi α tính toán xấp xỉ Edgeworth và tính δ(ε)(α) và supremum sup δ(ε)(α)
4:Cho ε_init và một ε_arb tùy ý ban đầu (ví dụ, ε= 10⁻¹⁰)
5:while (δ<δ_init AND ε_r< ε)do
6: Tính toán lại (Bước 2-4) và tinh chỉnh ε và giảm ε sử dụng yếu tố giảm ban đầu ε_r (ví dụ, ε_r= 0.5)
7:end while
8:Chọn một ma trận mang gradient {W⁽ˡ⁾}ᴴₗ₌₁ theo [15].
9:Lấy mẫu một lô ví dụ với xác suất q.
10:Tính toán cập nhật lịch sử để tìm mang gradient và phân tách và tính toán mang gradient hạng thấp L và R thông qua [15, Thuật toán 2]
11:Chạy quá trình tiến/lùi tái tham số hóa và tính toán các gradient riêng lẻ {∂ᵢL⁽ˡ⁾ₜ,∂ᵢR⁽ˡ⁾ₜ}ₗ∈[H],i∈Sₜ
12:Cho C và σ, cắt và thêm nhiễu vào các gradient riêng lẻ như trong Phần II-B để có ~∂L⁽ˡ⁾ₜ và ~∂R⁽ˡ⁾ₜ
13:Xây dựng ~∂W⁽ˡ⁾ₜ= (~∂L)R+L(~∂R)−L^T(~∂L)R
14:Đầu ra:⟨~∂W⁽ˡ⁾ₜ;(ε;δ)⟩

Như thấy trong Hình 1, LLM được huấn luyện trước được học trên một bộ dữ liệu công khai (ví dụ, internet) từ đầu bởi một công ty AI lớn (ví dụ, Google và OpenAI) (được hiển thị ở phía bên trái của Hình 1). Tiếp theo, mô hình được huấn luyện trước được sử dụng như một đầu vào cho EW-Tune để tinh chỉnh với các đảm bảo quyền riêng tư được thể hiện bởi các tham số quyền riêng tư (tức là, ngân sách quyền riêng tư ε∈[0,1) và xác suất thất bại δ∈[0,1]) (như được hiển thị ở phía bên phải của Hình 1). Các tham số quyền riêng tư (ε;δ) được cung cấp bởi người dùng/người thực hành. Một ε nhỏ hơn chỉ ra việc bảo vệ quyền riêng tư tốt hơn (và tiện ích/hiệu suất thấp hơn). δ biểu thị xác suất các ví dụ huấn luyện vô tình bị rò rỉ. Trong bối cảnh LLM, một giá trị phù hợp cho ε là từ 5 đến 8, và δ được khuyến nghị lấy giá trị theo thứ tự

--- TRANG 5 ---
BẢNG I
SO SÁNH HIỆU SUẤT CỦA EW-TUNE CHỐNG LẠI CÁC PHƯƠNG PHÁP DP TIÊN TIẾN TRÊN BỐN NHIỆM VỤ NLU KHÁC NHAU ĐƯỢC THỰC HIỆN BỞI LLM ROBERTA

Phương pháp    MNLI    QNLI    QQP    SST-2    MNLI    QNLI    QQP    SST-2
                      Độ chính xác                    Hệ số Nhân Nhiễu
RDP           81.25%  86.63%  84.60%  89.24%   0.65    0.829   0.6575  0.921
PRV           81.22%  86.79%  84.78%  91.82%   0.607   0.768   0.6135  0.8485
EW-Tune       81.81%  87.71%  84.91%  92.19%   0.573   0.739   0.579   0.8215

Hiệu suất được báo cáo cho δ = 1e-6 cho MNLI, QNLI, và QQP; δ = 1e-5 cho SST-2; kích thước lô = 2000

nghịch đảo của kích thước các mẫu huấn luyện [13].

Sau khi người dùng cung cấp các tham số quyền riêng tư, thuật toán kế toán Edgeworth [14] được sử dụng để (1) tính toán số lượng tổ hợp (tức là, các ứng dụng của DP-SGD) cho một bộ dữ liệu đã cho. (2) tính toán lượng nhiễu đảm bảo ngân sách quyền riêng tư đã cho. Tiếp theo, bất kỳ biến thể nào của DP-SGD [12], [15], như được mô tả trong Phần II-B, có thể được sử dụng để tinh chỉnh LLM trên bộ dữ liệu riêng dựa trên hệ số nhân nhiễu thích hợp thu được trong bước trước. Do hiệu suất đột phá của nó, chúng tôi đã sử dụng một phiên bản gần đây của thuật toán DP-SGD dựa trên một phương pháp mới được gọi là nhiễu gradient tái tham số hóa (RGP) [15]. Trong DP-SGD gốc [12], nhiễu được giới thiệu phụ thuộc rất nhiều vào các tham số mô hình, và việc cắt gradient cho từng ví dụ dẫn đến chi phí bộ nhớ và tính toán rất cao. RGP giải quyết các vấn đề của DP-SGD bằng cách tái tham số hóa ma trận trọng số W của mỗi lớp thành hai ma trận mang gradient hạng thấp L và R và một ma trận trọng số dư ~W. Cuối cùng, tất cả các lớp transformer của LLM sẽ được tinh chỉnh trên dữ liệu riêng thông qua DP-SGD. Đầu ra của khung EW-Tune là LLM được tinh chỉnh như được hiển thị ở phía bên phải của Hình 1.

Thuật toán 1 trình bày một phiên bản chi tiết hơn của khung của chúng tôi (EW-Tune). Thuật toán bắt đầu bằng cách tính toán một ε cho δ đã cho dựa trên kế toán Edgeworth được giải thích trong Phần II-C (Bước 2-4) bằng cách đầu tiên tính toán PLLR và sau đó xấp xỉ CDF của chúng sử dụng khai triển Edgeworth. Sau đó chúng ta tính toán δ(ε)(α) và supremum của nó. Tiếp theo, trong Bước 5-8, chúng ta tính toán ε cuối cùng và một hệ số nhân nhiễu thích hợp để được sử dụng trong nhiễu gradient tái tham số hóa của chúng ta (tức là, thêm nhiễu như trong Phần II-B). Trong Bước 5, với mỗi lần lặp của vòng lặp chúng ta tính toán ε=εr và yếu tố điều chỉnh ban đầu εr được giảm bởi một yếu tố không đổi (ví dụ, chúng ta sử dụng εr=10 trong mã của chúng tôi). Cuối cùng, cho ε, thuật toán RGP làm nhiễu hiệu quả các tham số được cập nhật (Bước 9-14). Đối với mỗi cập nhật với ma trận trọng số W, thuật toán hoạt động trong bốn bước chính. Trong bước đầu tiên, các ma trận mang gradient L và R được tạo ra thông qua phương pháp phân tách được đề xuất trong [15, Thuật toán 2]. Đầu ra của bước này là phiên bản trực chuẩn hóa (thông qua quá trình trực chuẩn hóa Gram-Schmidt) của các ma trận mang gradient. Tiếp theo, các ma trận trọng số được tái tham số hóa để tính toán và lưu trữ các gradient riêng lẻ thông qua quá trình tiến/lùi được trình bày trong [15, Phần 2]. Trong bước thứ ba, các gradient được cắt và làm nhiễu tương tự như phương pháp DP-SGD được trình bày trong Phần II-B. Cuối cùng, trong Bước 14, các gradient tổng hợp nhiễu của các ma trận mang {~∂W(l)t} được sử dụng để tính toán các gradient của các trọng số gốc.

IV. THÍ NGHIỆM
Chúng tôi đã mở mã nguồn khung được tối ưu hóa của chúng tôi để được áp dụng rộng rãi và phục vụ mục đích thử nghiệm tại liên kết sau.
https://github.com/star-ailab/LLM_Tune

A. Thiết lập Thí nghiệm
EW-Tune được phát triển và chạy trên một NVIDIA GeForce RTX 3090 duy nhất với 10.496 lõi CUDA và 24 GB bộ nhớ trong. Để đảm bảo rằng việc tải và tinh chỉnh LLM có thể diễn ra trong bộ nhớ trong, chúng tôi đã chọn mô hình roBERTa.base được huấn luyện trước với 125 triệu tham số. LLM này có thể được truy cập từ https://github.com/facebookresearch/fairseq/tree/main/examples/roberta.

1) Thiết lập tham số: Phù hợp với [29], chúng tôi đặt phân vùng huấn luyện và kiểm tra cho mỗi bộ dữ liệu: (MNLI: 393.000 cho huấn luyện và 20.000 cho kiểm tra; QNLI: 105.000 cho huấn luyện và 5400 cho kiểm tra; QQP: 364.000 cho huấn luyện và 391.000 cho kiểm tra, SST-2: 67.000 cho huấn luyện và 1800 cho kiểm tra).

Để tạo điều kiện so sánh, chúng tôi đặt các tham số quyền riêng tư ε và δ phù hợp với [13]. Theo đó, chúng tôi đặt ε = 8, δ = 1e-6 cho các bộ dữ liệu lớn hơn (tức là, MNLI, QNLI, và QQP; mỗi bộ có vài trăm nghìn mẫu) và δ = 1e-5 cho bộ dữ liệu nhỏ hơn (tức là, SST-2; với hàng chục nghìn mẫu).

2) Thí nghiệm Đánh giá: Theo [13], [14] Chúng tôi đánh giá hiệu suất của khung EW-Tune đề xuất so với hai phương pháp thay thế DP tiên tiến được sử dụng rộng rãi: Renyi Differential Privacy (RDP) [12], [21] và Privacy Loss Random Variables (PRV) [25]. Để đánh giá nghiêm ngặt EW-Tune, chúng tôi tiến hành hai bộ thí nghiệm (Phần IV-B).

Trong Thí nghiệm 1, chúng tôi đánh giá độ chính xác của LLM trong việc giải quyết bốn nhiệm vụ NLU đã nêu (MNLI, QNLI, QQP, SST-2) sau khi tinh chỉnh với EW-Tune, RDP và PRV. Trong Thí nghiệm 2, chúng tôi đánh giá lượng nhiễu được tạo ra bởi các thuật toán kế toán quyền riêng tư thay thế so với EW-Tune cho các giá trị ε khác nhau.

B. Kết quả
1) Thí nghiệm 1: Bảng I cho thấy kết quả so sánh độ chính xác và hệ số nhân nhiễu của EW-Tune đề xuất so với RDP và PRV trên bốn bộ dữ liệu NLU (MNLI, QNLI, QQP, và SST-2) được thực hiện bởi LLM roBERTa. Để báo cáo hiệu suất, chúng tôi đã lặp lại mỗi thí nghiệm 3 lần và báo cáo trung bình. Độ chính xác cao nhất trong

--- TRANG 6 ---
Hình 2. Sự Thay đổi của Hệ số Nhân Nhiễu dựa trên các giá trị ε khác nhau trên ba Thuật toán Kế toán Quyền riêng tư (RDP, PRV, và EW-Tune.)

việc thực hiện mỗi nhiệm vụ xuất hiện bằng chữ đậm. Như thấy trong Bảng I, EW-Tune mang lại hiệu suất cao nhất (81.81% trên MNLI, 87.71% trên QNLI, 84.91% trên QQP, và 92.19% trên SST-2) trong số các đối tác của nó. Trung tâm của EW-Tune, kế toán Edgeworth sử dụng một phương pháp tính toán quyền riêng tư chính xác được gọi là quyền riêng tư vi phân f (f-DP) cùng với xấp xỉ Edgeworth, thay cho CLT, có tỷ lệ hội tụ tốt hơn nhiều. Điều này cho phép EW-Tune đạt được các đảm bảo quyền riêng tư bằng cách áp dụng ít nhiễu hơn cho các lớp transformer của LLM trong quá trình huấn luyện. Hệ số nhân nhiễu (tức là, độ lệch chuẩn của phân phối nhiễu Gaussian) được hiển thị ở phía bên phải của Bảng I. Như được hiển thị trong Bảng I, EW-Tune mang lại hệ số nhân nhiễu thấp nhất. Cụ thể hơn, so với hiện trạng, hệ số nhân nhiễu thấp hơn lên đến 4%, 3.2%, 5.6%, và 3.2% (với δ ∈ [5×10⁻⁸]) cho MNLI, QNLI, QQP, và SST-2, tương ứng. Trong Bảng I, các số hiệu suất cao nhất và hệ số nhân nhiễu thấp nhất được chỉ ra bằng chữ đậm. Chúng tôi lưu ý rằng nếu thay vì sử dụng DP-SGD với RGP [15], người ta khởi tạo những LLM này với DP-SGD gốc [12], các số hệ số nhân nhiễu nhỏ hơn trong EW-Tune sẽ dẫn đến khoảng cách hiệu suất cao hơn nhiều với các đối tác của chúng tôi.

2) Thí nghiệm 2: Thí nghiệm 2 đánh giá lượng nhiễu được tạo ra bởi EW-Tune và các thuật toán kế toán quyền riêng tư đánh giá khác tại δ = 1e-6 cho MNLI, QNLI, QQP, và δ = 1e-5 cho SST-2. Như đã lưu ý, việc đạt được cùng ngân sách quyền riêng tư (ε) bằng cách áp dụng ít nhiễu hơn cho các lớp transformer của LLM trong quá trình tinh chỉnh là mong muốn. Như được hiển thị trong Hình IV-B2, EW-Tune mang lại lượng nhiễu thấp nhất trên các phương pháp kế toán quyền riêng tư đánh giá (tức là, RDP và PRV). Như được hiển thị trong Hình IV-B2, khi ε thay đổi từ 5 đến 8, EW-Tune tạo ra lượng nhiễu thấp nhất vào quá trình SGD cho tất cả bốn nhiệm vụ NLU (MNLI, QNLI, QQP, và SST-2).

Tổng thể, Kết quả của Thí nghiệm 1 và 2 trên bốn nhiệm vụ NLU phức tạp cho thấy rằng EW-Tune có thể tăng cường hiệu suất thông qua việc áp dụng ít nhiễu hơn vào quá trình SGD, trong khi đạt được cùng ngân sách quyền riêng tư như các thuật toán đối tác của nó. EW-Tune vượt trội hơn các phương pháp kế toán quyền riêng tư khác cho các giá trị ngân sách quyền riêng tư khác nhau.

V. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Trong công trình này, chúng tôi đã trình bày một khung mới được gọi là EW-Tune, được thiết kế đặc biệt để tinh chỉnh LLM. Bằng cách sử dụng các phương pháp kế toán quyền riêng tư và nhiễu gradient tiên tiến, EW-Tune có thể cung cấp đảm bảo quyền riêng tư mẫu hữu hạn bằng cách giới thiệu ít nhiễu hơn so với các phương pháp hiện tại. EW-Tune giới thiệu ít nhiễu hơn lên đến 6% khi huấn luyện riêng tư các mô hình ngôn ngữ lớn điều này đóng góp vào việc cải thiện hiệu suất lên đến 1.1%. Điều này có thể đóng góp vào việc giải quyết khoảng cách trong sự đánh đổi giữa quyền riêng tư và độ chính xác trong lĩnh vực quyền riêng tư dữ liệu và AI.

Một công việc tương lai thú vị sẽ là nghiên cứu thêm mối quan hệ giữa nhiễu được giới thiệu và độ chính xác huấn luyện bằng cách tập trung vào tổng số tham số của mô hình, kích thước bộ dữ liệu, mục tiêu nhiệm vụ, và số lượng tổ hợp.

LỜI CẢM ỌN
Chúng tôi muốn cảm ơn Hua Wang từ Khoa Thống kê tại Đại học Pennsylvania vì những cuộc thảo luận sáng suốt về kế toán Edgeworth và những bình luận hữu ích về việc triển khai nó.

TÀI LIỆU THAM KHẢO
[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[2] D. Ham, J.-G. Lee, Y. Jang, and K.-E. Kim, "End-to-end neural pipeline for goal-oriented dialogue systems using gpt-2," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 583–592.
[3] L. Fang, T. Zeng, C. Liu, L. Bo, W. Dong, and C. Chen, "Transformer-based conditional variational autoencoder for controllable story generation," arXiv preprint arXiv:2101.00828, 2021.
[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.
[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019.
[6] J. L. Hu, M. Ebrahimi, and H. Chen, "Single-shot black-box adversarial attacks against malware detectors: A causal language model approach," in 2021 IEEE International Conference on Intelligence and Security Informatics (ISI). IEEE, 2021, pp. 1–6.
[7] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, "Zero-shot text-to-image generation," in International Conference on Machine Learning. PMLR, 2021, pp. 8821–8831.
[8] N. Carlini, F. Tramèr, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song, Ú. Erlingsson, A. Oprea, and C. Raffel, "Extracting training data from large language models," in 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, M. Bailey and R. Greenstadt, Eds. USENIX Association, 2021, pp. 2633–2650.
[9] N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song, "The secret sharer: Evaluating and testing unintended memorization in neural networks," in 28th USENIX Security Symposium (USENIX Security 19), 2019, pp. 267–284.
[10] S. Hisamoto, M. Post, and K. Duh, "Membership inference attacks on sequence-to-sequence models: Is my data in your machine translation system?" Transactions of the Association for Computational Linguistics, vol. 8, pp. 49–63, 2020.

--- TRANG 7 ---
[11] M. Fredrikson, S. Jha, and T. Ristenpart, "Model inversion attacks that exploit confidence information and basic countermeasures," in Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, 2015, pp. 1322–1333.
[12] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, "Deep learning with differential privacy," in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016, E. R. Weippl, S. Katzenbeisser, C. Kruegel, A. C. Myers, and S. Halevi, Eds. ACM, 2016, pp. 308–318. [Online]. Available: https://doi.org/10.1145/2976749.2978318
[13] D. Yu, S. Naik, A. Backurs, S. Gopi, H. A. Inan, G. Kamath, J. Kulkarni, Y. T. Lee, A. Manoel, L. Wutschitz et al., "Differentially private fine-tuning of language models," arXiv preprint arXiv:2110.06500, 2021.
[14] H. Wang, S. Gao, H. Zhang, M. Shen, and W. J. Su, "Analytical composition of differential privacy via the edgeworth accountant," arXiv preprint arXiv:2206.04236, 2022.
[15] D. Yu, H. Zhang, W. Chen, J. Yin, and T.-Y. Liu, "Large scale private learning via low-rank reparametrization," in International Conference on Machine Learning. PMLR, 2021, pp. 12 208–12 218.
[16] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.
[17] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representations," Advances in Neural Information Processing Systems, vol. 33, pp. 12 449–12 460, 2020.
[18] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu et al., "Exploring the limits of transfer learning with a unified text-to-text transformer." J. Mach. Learn. Res., vol. 21, no. 140, pp. 1–67, 2020.
[19] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining for language understanding," Advances in neural information processing systems, vol. 32, 2019.
[20] C. Dwork, F. McSherry, K. Nissim, and A. D. Smith, "Calibrating noise to sensitivity in private data analysis," in Theory of Cryptography, Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006, Proceedings, ser. Lecture Notes in Computer Science, S. Halevi and T. Rabin, Eds., vol. 3876. Springer, 2006, pp. 265–284. [Online]. Available: https://doi.org/10.1007/11681878_14
[21] I. Mironov, "Renyi differential privacy," CoRR, vol. abs/1702.07476, 2017. [Online]. Available: http://arxiv.org/abs/1702.07476
[22] Z. Bu, J. Dong, Q. Long, and W. J. Su, "Deep learning with gaussian differential privacy," CoRR, vol. abs/1911.11607, 2019. [Online]. Available: http://arxiv.org/abs/1911.11607
[23] J. Dong, A. Roth, and W. J. Su, "Gaussian differential privacy," CoRR, vol. abs/1905.02383, 2019. [Online]. Available: http://arxiv.org/abs/1905.02383
[24] P. Kairouz, S. Oh, and P. Viswanath, "The composition theorem for differential privacy," in International conference on machine learning. PMLR, 2015, pp. 1376–1385.
[25] S. Gopi, Y. T. Lee, and L. Wutschitz, "Numerical composition of differential privacy," in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 11 631–11 642. [Online]. Available: https://proceedings.neurips.cc/paper/2021/hash/6097d8f3714205740f30debe1166744e-Abstract.html
[26] C. Dwork and G. N. Rothblum, "Concentrated differential privacy," CoRR, vol. abs/1603.01887, 2016. [Online]. Available: http://arxiv.org/abs/1603.01887
[27] S. Meiser and E. Mohammadi, "Tight on budget?: Tight bounds for r-fold approximate differential privacy," in Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, CCS 2018, Toronto, ON, Canada, October 15-19, 2018, D. Lie, M. Mannan, M. Backes, and X. Wang, Eds. ACM, 2018, pp. 247–264. [Online]. Available: https://doi.org/10.1145/3243734.3243765
[28] A. Koskela, J. Jälkö, and A. Honkela, "Computing tight differential privacy guarantees using FFT," in The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], ser. Proceedings of Machine Learning Research, S. Chiappa and R. Calandra, Eds., vol. 108. PMLR, 2020, pp. 2560–2569. [Online]. Available: http://proceedings.mlr.press/v108/koskela20b.html
[29] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, "GLUE: A multi-task benchmark and analysis platform for natural language understanding," in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/forum?id=rJ4km2R5t7
[30] A. Williams, N. Nangia, and S. R. Bowman, "A broad-coverage challenge corpus for sentence understanding through inference," arXiv preprint arXiv:1704.05426, 2018.
[31] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts, "Recursive deep models for semantic compositionality over a sentiment treebank," in Proceedings of the 2013 conference on empirical methods in natural language processing, 2013, pp. 1631–1642.
7
