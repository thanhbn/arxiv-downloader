# 2207.09078.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/privacy/2207.09078.pdf
# File size: 955290 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ILASR: Privacy-Preserving Incremental Learning for Automatic
Speech Recognition at Production Scale
Gopinath Chennupati
Milind Rao
Gurpreet Chadha
Aaron Eakin
Amazon Alexa
USAAnirudh Raju
Gautam Tiwari
Anit Kumar Sahu
Ariya Rastrow
Jasha Droppo
Amazon Alexa
USAAndy Oberlin
Buddha Nandanoor
Prahalad Venkataramanan
Zheng Wu
Pankaj Sitpure
Amazon Alexa
USA
ABSTRACT
Incremental learning is one paradigm to enable model building and
updating at scale with streaming data. For end-to-end automatic
speech recognition (ASR) tasks, the absence of human annotated
labels along with the need for privacy preserving policies for model
building makes it a daunting challenge. Motivated by these chal-
lenges, in this paper we use a cloud based framework for production
systems to demonstrate insights from privacy preserving incremen-
tal learning for automatic speech recognition (ILASR). By privacy
preserving, we mean, usage of ephemeral data which are not hu-
man annotated. This system is a step forward for production level
ASR models for incremental/continual learning that offers near real-
time test-bed for experimentation in the cloud for end-to-end ASR,
while adhering to privacy-preserving policies. We show that the
proposed system can improve the production models significantly
(3%) over a new time period of six months even in the absence of
human annotated labels with varying levels of weak supervision
and large batch sizes in incremental learning. This improvement
is20%over test sets with new words and phrases in the new time
period. We demonstrate the effectiveness of model building in a
privacy-preserving incremental fashion for ASR while further ex-
ploring the utility of having an effective teacher model and use of
large batch sizes.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíSpeech recognition ;Neural
networks ;Semi-supervised learning settings ;‚Ä¢Security and pri-
vacy‚ÜíPrivacy-preserving protocols .
KEYWORDS
Incremental Learning, Automatic Speech Recognition, Privacy-
preserving Machine Learning
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA
¬©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00
https://doi.org/10.1145/3534678.3539174ACM Reference Format:
Gopinath Chennupati, Milind Rao, Gurpreet Chadha, Aaron Eakin, Anirudh
Raju, Gautam Tiwari, Anit Kumar Sahu, Ariya Rastrow, Jasha Droppo,
and Andy Oberlin, Buddha Nandanoor, Prahalad Venkataramanan, Zheng
Wu, Pankaj Sitpure. 2022. ILASR: Privacy-Preserving Incremental Learning
for Automatic Speech Recognition at Production Scale. In Proceedings of
the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô22), August 14‚Äì18, 2022, Washington, DC, USA. ACM, New York, NY,
USA, 10 pages. https://doi.org/10.1145/3534678.3539174
1 INTRODUCTION
Privacy preserving machine learning [ 1] has been at forefront, due
to both increased interest in privacy and the potential susceptibility
of deep neural networks to leaks and attacks. Federated Learning
(FL) [ 44] is a machine learning technique that involves training
models on edge devices, where data need not leave the device,
and can be heterogeneous and non-identically and independently
distributed (non-IID). In FL, multiple model updates from a number
of participating devices are aggregated. In spite of raw data not
leaving the edge device, FL has found to be susceptible to gradient
inversion attacks [ 65,66]. In response, various privacy-preserving
mechanisms such as differential privacy and secure aggregation [ 19,
58] have been proposed to counter data leakage and conform to
privacy preserving mechanisms. Moreover, the lack of labels for the
data present in the participating entities, makes FL more challenging
for applications such as automatic speech recognition (ASR). Most
research in FL until now focuses on training models from scratch.
In this work, we focus on privacy-preserving incremental learning
(IL), in the context of end-to-end production model building at scale
over extended time periods. Incremental learning [ 8,62] has been
extensively used to incrementally update models on the fly instead
of training them from scratch. Incremental learning as such is not
privacy-preserving.
Despite the above advances, to the best of our knowledge, few
frameworks exist for privacy-preserving incremental training of
end-to-end automatic speech recognition models. Prior work on
federated learning for speech-based tasks [ 13,16,23] and end-to-
end ASR [ 18,26], focus on standard benchmarks1and not on large
scale production data. Privacy-preserving IL on device for end-to-
end ASR poses a number of challenges. Production-sized end-to-
end ASR systems [ 11,24] are expensive to train even in traditional
1e.g. LibriSpeech [ 47] is a small sized dataset ( ‚àº1000 hours) recorded in a controlled
environmentarXiv:2207.09078v2  [cs.CL]  22 Jul 2022

--- PAGE 2 ---
KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Gopinath Chennupati et al.
distributed setup, on-device training needs more work [ 7] to accom-
modate restrictive memory and computational constraints. Gener-
ating training labels i.e. speech transcripts, in near real-time, on
the devices is another challenge. To alleviate unavailability of near
real-time speech transcripts, teacher transcripts can be used in a
semi-supervised and/or self-learning fashion. For example, consider
the problem of improving models deployed in edge devices that
run voice assistants. In such cases, the number of devices is in the
millions, which results in a large scale of streaming data being gen-
erated. We propose to use large batch processing for the utterances
being collected at the edge devices and sent to the cloud for process-
ing, and the data is only stored ephemerally. However, deploying
all or part of the above components on resource constrained speech
devices (such as Alexa, Google Assistant and others) is challenging.
We build and use a cloud-based system, Incremental Learning for
Automatic Speech Recognition (ILASR) to train and update pro-
duction ready ASR models. ILASR automates the entire pipeline of
incremental learning in a privacy-preserving manner. To enforce
privacy-preserving aspects in the context of ASR, we enforce la-
belling of the utterances through pre-trained teacher models with
no human annotations. ILASR processes an utterance once before
updating the model, preserving the chronological order of data. To
that end, the contributions of the paper are:
‚Ä¢A novel cloud-based IL system to train production ready ASR
models in near real-time, with a large amount of streaming
de-identified data, without having to manually transcribe or
persist the audio.
‚Ä¢We provide new insights in terms of usage of large batch pro-
cessing in ILASR that it does not have detrimental impact on
test accuracy as compared to the contradicting findings [ 20,
33,37,41,42,52] (on CNN ImageNet). We could accom-
modate fixed learning rates and minimal hyper-parameter
optimization [ 34] along with large batch training. With a
monthly frequency of incremental model updates, we ob-
serve that the production models (converged on old data)
improve in near real-time on new data belonging to a period
of six months
‚Ä¢We empirically establish over six months of data that chrono-
logical vs randomized order of processing utterances does
not produce any observable difference in performance.
We evaluate ILASR on three student recurrent neural network trans-
ducer (RNN-T) [ 24] architectures. The semi-supervised learning
(SSL) approach produces machine transcripts using a larger teacher
ASR model. The students are pre-trained on in-house de-identified
data until 2020 . Through training in ILASR, we observe an im-
provement of 3‚àí7%in word error rate (WER) over the pre-trained
baselines when these students are trained incrementally on a new
time period of six months in 2021 . The improvement in WER is
termed relative word error rate reduction (WERR). This increases
to20%on test sets with new words and phrases in 2021. Similarly,
when the student models are trained incrementally each month,
we observe WER improvements, as well the phenomenon where
models get stale without further updates.
The paper is organized as follows: section 2 describes the essen-
tial concepts used in the paper; section 3 explains the proposedsystem; section 4 describes the experimental settings; section 5
presents the results; section 6 summarizes the related literature and
finally, section 7 concludes and recommends future directions.
2 BACKGROUND
In this section we summarize the RNN-T architecture and large
batch training with stochastic gradient descent (SGD).
2.1 RNN-T model architecture
Figure 1 shows the RNN-T [ 24] architecture used in real-time speech
recognition. The model predicts the probability ùëÉ(y|x)of labels
y=(ùë¶1,...,ùë¶ ùëà)given acoustic features x=(ùë•1,...,ùë• ùëá). It has an
encoder, a prediction network, and a joint network. The encoder is
analogous to an acoustic model that takes a sequence of acoustic
input features and outputs encoded hidden representations. The
prediction network corresponds to a language model that accepts
the previous output label predictions, and maps them to hidden
representations. The joint network is a feed forward DNN that takes
both the encoder and prediction network outputs, and predicts the
final output label probabilities with softmax normalization.
Figure 1: RNN-T ASR model architecture
2.2 Overview of learning with large batch size
When training with SGD, mini batches with a well crafted decaying
learning rate schedule are commonly used as opposed to using large
batches. Previous work in [ 33] has demonstrated a generalization
drop when using large batches, thus recommending mini-batch
SGD with decaying learning rate. However, recent advances in large
batch training both with a linear scaling rule of the learning rate
[22] and constant learning rate[ 53], large batch training has been
shown to achieve similar performance as its mini-batch counterpart.
A recurrent observation in the literature [ 20,33,37,41,42,52] is
that large batch training (for ImageNet, >1000 ) results in test
accuracy degradation. Despite the warm-up in [22], for ImageNet,
the best accuracies are observed up to a large mini-batch of 8192
images.
In this paper, we deal with the challenges of 1) training with large
batches in incremental learning and 2) semi-supervised learning
to alleviate unavailability of human annotation and labels. For
automatic speech recognition (ASR), with large batch sizes ( >3ùëí5
utterances) using a fixed learning rate schedule, we observe better
test accuracies, as opposed to the degradation in literature, while
training with teacher transcripts for the incremental audio data.

--- PAGE 3 ---
ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA
3 ILASR: INCREMENTAL LEARNING FOR
AUTOMATIC SPEECH RECOGNITION
This section describes the ILASR architecture and the corresponding
incremental learning algorithm. ILASR offers large scale end-to-end
ASR training with the ability to incrementally update the models
in user-defined time windows. ILASR automates the whole life-
cycle of data generation, sampling, labeling, model development,
evaluation and deployment for audio data in near real-time.
Figure 2: High-level skeleton of the ILASR architecture
3.1 ILASR Architecture
Figure 2 shows the architectural overview of ILASR system. The
system comprises three primary components: (1) Data preprocessor ‚Äì
is a cloud runtime service that processes near real-time audio from
device; (2) IL Core is responsible for model training, computing
model updates and inference; and (3) IL Orchestrator aggregates
the accumulated gradients, updates the model, performs evaluation
and finalizes the model update based on the evaluation result.
Train launcher initiates the end-to-end ASR training in ILASR. The
first step is data preprocessing to select a subset of devices and ut-
terances to participate in the training loop. The selection could
be random or based on heuristics aimed at improving the model
in a particular way. Confidence scores obtained during inference
are used [ 29,30] coupled with heuristics such as presence of rare
words or semantic tags and intents of interest. This selection can be
extended to leverage weak signals from user feedback such as user
indicating whether the action taken by the assistant is positive or
negative or detecting friction such as repeated requests or cancella-
tions. Acoustic features are extracted and augmented [ 48] for the
selected utterances for training. Machine transcripts are generated
using a teacher ASR model pre-trained using standard distributed
training. The Conformer [ 25] based end-to-end ASR teacher model
decodes the input audio ( ùëã) to produce machine transcripts ( ùëå).
These paired ( ùëã,ùëå) instances are used to train the model. The
machine transcripts act as ground truth labels. ILASR produce tran-
scriptions through secure automation without human intervention
or review. The extracted features together with machine transcripts
in this step are combined to train the student models using IL Core .
TheIL Core system has an application programming interface (API)
that supports local gradient accumulation on each of the servers inthe fleet, and an ASR inference engine. The IL Core API supports
FedSGD and FedAvg [ 44] and can be extended to support other fed-
erated optimizers such as FedProx [ 51], FedMA [ 59], FedNova [ 60],
and adaptive federated optimizer [ 50]. The IL Orchestrator coordi-
nates training across the ILASR fleet. IL Orchestrator contains the
gradient publisher, aggregator and updates the model incrementally.
The gradient aggregator collects gradients from each of the IL Core
instances, aggregates them and then applies them to the current
model. Once the model update is done, the collected gradients are
discarded and not stored in the system which helps with reducing
the risk of gradient inversion attack. A periodic light-weight evalua-
tion of the model ensures that the model is directionally improving.
The global model is updated in a given round when the performance
improves over that of the previous round. To reduce the probability
of a model update resulting in worse performance, ILASR can be
run in parallel with differing hyperparameters. In this scenario, one
of the resulting models can be utilized should it result in improved
performance. After a sufficient number of rounds, the final model is
stored for the next model release after a detailed model validation
step.
ILASR addresses security and privacy concerns with different levels
of granularity. Since ILASR is a cloud-based system for privacy-
preserving IL at scale, the audio encryption is two fold. In the first
stage, TLS [ 15] encryption is applied on audio transmission followed
by an application level key-master [ 40] encryption. Importantly,
the audio is purged in a few minutes ( ‚â§10), within which the model
updates are calculated.
3.2 ILASR: Incremental Learning
Algorithm 1 ILASR incremental learning algorithm
Require:Kservers,Lloss function, ùëÅnumber of local steps per round,
Blocal batch size,(ùúÇ)learning rate, ùëÉùëü
ùëòrecent utterances pulled by
server ùëòin round ùëü,Dùëíùë£ùëéùëô eval set andD‚Ñéùë°past transcribed data if
used for rehearsal training.
Ensure: ùë§ùëü
Gincrementally updated global model and ùë§ùëíùëüùëüword error rate
on the eval set after ùëürounds
1:Init.ùë§0
G// start training with a pre-trained model
2:ùë§ùëíùëü 0=ùëéùë†ùëü_ùëñùëõùëì ùëíùëüùëíùëõùëêùëí _ùëíùëõùëîùëñùëõùëí(Dùëíùë£ùëéùëô, ùë§T
G)
3:foreach round ùëü=1,2, . . .do
4: foreach server ùëò‚ààILASR Fleet in parallel do
5: ùë§ùëü
ùëò=ùë§ùëü‚àí1
G
6:Dùë†ùë†ùëô‚Üê(filter ùëÉùëü
ùëòbased on utterance selection criteria and gener-
ate machine transcript, refer algorithm 2)
7:Dùë°ùëüùëéùëñùëõ‚Üê(mixDùë†ùë†ùëôandD‚Ñéùë°ifD‚Ñéùë°is used for rehearsal, else
justDùë†ùë†ùëô)
8:Dùë°ùëüùëéùëñùëõ‚Üê( splitDùë°ùëüùëéùëñùëõ intoùëÅbatches of sizeB)
9: foreach batch ùëèùëñfrom ùëè1toùëèùëÅdo
10: ùë§ùëü
ùëò‚Üêoptimizerùëò.update( ùúÇ,‚àáL( ùë§;ùëèùëñ))
11: end for
12: end for
13: ùë§ùëü
G‚Üê1
K√çK
ùëò=1ùë§ùëü
ùëò
14: ùë§ùëíùëüùëü‚Üêùëéùë†ùëü_ùëñùëõùëì ùëíùëüùëíùëõùëêùëí _ùëíùëõùëîùëñùëõùëí(Dùëíùë£ùëéùëô, ùë§ùëü
G)
15: ùë§ùëü
G=ùë§ùëü‚àí1
Gifùë§ùëíùëüùëü>ùë§ùëíùëüùëü‚àí1// Revert to the previous model if not
a better model.
16:end for

--- PAGE 4 ---
KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Gopinath Chennupati et al.
Algorithm 1 shows the incremental learning policy in ILASR frame-
work. The new model obtained in each round is used only if it
performs better than the model from the previous round. Parallel
runs of the algorithm with differing hyper parameters to train an
ensemble of incrementally updated models can ensure that there
is at least one model that performs better than the model from
the previous round. Another interesting consideration is the effect
of catastrophic forgetfulness [ 17,21,43] in incremental learning
of ILASR framework, where the previous learned behaviour of a
model is forgotten with new updates. This can be mitigated with
the rehearsal [ 2] of training on a subset of annotated historical data
along with the new data.
We describe the SSL data generation method in algorithm 2.
We randomly sample a subset of the audio in near real-time, to
prepare a data pool ( P), and calculate target number of utterances
(U) to be sampled from P, where each of the utterances include a
pre-calculated confidence value [ 57]. For each confidence bin, for
example confidence in (600,700] where confidence is evaluated on
a scale from 0 to 1000, utterances are filtered to conform to the
confidence criterion. The randomly sampled utterances from above
are set to get target number of utterances and sent to IL core for
training, which are deleted as soon as the model takes a pass over it
for the first time. Additional criteria such as presence of rare words,
presence of desired semantic tags can also be utilized.
Algorithm 2 SSL data selection procedure
Require:ùúèlist of utterance confidence bins
Ensure:Xdata set
1:P=ùëüùëéùëõùëëùëúùëö _ùë†ùëéùëöùëùùëôùëí()// prepare a random pool of data
2:P=ùë°ùëíùëéùëê‚Ñéùëíùëü _ùëëùëíùëêùëúùëëùëí(P)// generate machine transcripts
3:U=ùëêùëéùëôùëê_ùë¢ùë°ùë°ùëíùëüùëéùëõùëêùëí _ùëêùëúùëõùëìùëñùëëùëíùëõùëêùëí(P).
4:Q=[]// a bin for each confidence range
5:forùëê‚ààùúèdo
6:Q[ùëê]=ùëìùëñùëôùë°ùëíùëü _ùë¢ùë°ùë°ùëíùëüùëéùëõùëêùëíùë†(U,ùëê)
7:X=ùë†ùëíùëôùëíùëêùë° _ùë¢ùë°ùë°ùëíùëüùëéùëõùëêùëíùë†(Q)// can include additional criteria
like presence of rare words or desired semantic tags
8:end for
4 EXPERIMENTS
We describe the datasets, model configurations and experimental
settings used in this paper, to provide insights and study privacy-
preserving incremental learning through ILASR.
4.1 Datasets
All speech data used for training and evaluation are de-identified.
Train sets The audio streams are prepared into offline training
datasets. The following training datasets are used for experimenta-
tion:
Pre-training datasets : A480ùëò-hour pre-training dataset is utilized
for building pre-training models. This pre-trained model is used
as a starting point for incremental training with the ILASR system.
This comprises two datasets:
(1)120K-hour HT : Human-transcribed (HT) data from 2020 and
previous years
(2)360K-hour SSL : Machine-transcribed data in 2020Incremental training dataset : We consider the end of 2020 as
the start date for incremental training of ASR models.
(1)180K-hour ILASR SSL : Machine-transcribed data is generated
over a period of six months in 2021 (Jan to June) and is used
for near real-time training of the ILASR system.
Test sets : We evaluate the models on in-house human transcribed
(HT) test sets.
General : Includes three HT datasets from different time ranges
representing the general use case. It comprises a 37-hour test set
from 2021, a10-hour test set from 2020 and a 96-hour test set from
2018‚àí2019.
Rare : Includes three HT datasets from different time ranges, where
the transcriptions contain at least one rare word. Rare words are
those in the long-tail of the vocabulary determined by word fre-
quency. This includes a 44-hour test set from 2021, a44-hour test
set from 2020, and a 27-hour test set from 2018‚àí2019.
Delta : This consists of a 22-hour HT test set that records a change
in frequency of words in 2021 over 2020 . The transcriptions are
filtered based on 1-gram, 2-gram and 3-grams that are 5x more
frequent in 2021 than 2020 . This test set captures changes in the
data distribution and is very relevant to measure the impact of
incremental learning with ILASR.
Messaging : Includes two HT datasets that comprise of messaging
and communications domain data. It includes a 2.7-hour HT test
from 2020 and a 45.5-hour HT test set from 2018‚àí2019.
Monthly datasets (2021) : We use six monthly test sets from Jan
to June 2021 to evaluate the incremental learning setup of ILASR.
Each of these datasets are refered to as (Jan, Feb, ¬∑¬∑¬∑,June) and each
month has on average 70-hours of data. We further report results
on 3-month datasets ùêΩùëéùëõ‚àíùëÄùëéùëü including data from Jan, Feb, Mar
andùê¥ùëùùëü‚àíùêΩùë¢ùëõincluding data from Apr, May, June.
4.2 Model details
Features : The audio features are 64dimensional log-mel filter-bank
energies [ 46] computed over a 25ms window, with a 10ms shift. The
features computed on 3consecutive 10ms frames are stacked and
sub-sampled to result in 192dimensional features at a 30ms frame
rate, and are provided as input to the ASR model. The ground truth
transcripts are tokenized to 2500 sub-word units using a uni-gram
language model [35].
Models :Teacher models : Teacher models are used to generate SSL
machine transcripts. We have three teacher models available: ùëá3is
a teacher model (a conventional RNN-HMM hybrid ASR system[ 6])
that is trained on 100ùêæ-hours of data until 2019 only. The machine-
transcripts from ùëá3are utilized to bootstrap and provide transcripts
for the more recent 360ùêæ-hour SSL pre-training dataset. The 480ùêæ-
hour pre-training dataset, including the 360ùêæ-hour SSL dataset
based onùëá3and the 120ùêæ-hour HT dataset, is utilized to train two
updated teacher models: (1) ùëá1: A larger conformer based ASR
architecture [ 25] trained on 480ùêæ-hours.ùëá1has 122M parameters,
an encoder with 17√ó512LSTM layers, 8attention heads with
32dimensional convolution kernel. The prediction network uses
2√ó1024 LSTM layers. (2) ùëá2is a conventional RNN-HMM hybrid
ASR system [ 6] and is trained on the same 480ùêæ-hour dataset.
Finally, the student models for all experiments in the paper are
trained on SSL datasets that use the most recent ùëá1teacher model.

--- PAGE 5 ---
ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA
In section 5.1.3, for the purpose of ablations comparing various
teachers, we train student models on SSL datasets that are based
onùëá2andùëá3.
Student models : The student models are based on different LSTM
based RNN-T architectures. These vary in the number of encoder
layers and the feature frame rates. Two student models are described
as follows.ùëüùëõùëõùë° _60ùëöcontains 60M parameters with 5√ó1024 LSTM
encoder, 2√ó1024 LSTM prediction network and a feed-forward
joint network with tanh activation. The input embeddings of the
prediction network are 512dimensional. SpecAugment [ 48] is used
on the audio features. ùëüùëõùëõùë° _90ùëöcontains 90M parameters with
8√ó1024 LSTM layer encoder, a prediction network of size 2√ó1024,
and a feed-forward joint network with tanh activation. The input
embeddings of the prediction network use 512dimensional embed-
dings and a 2500 sub-word tokenizer from a uni-gram language
model. SpecAugment is used on the audio features. The encoder
uses an LSTM based time-reduced [ 54] RNN multi-layer (for speed
of training and inference) with feature frame rate set to 3layers.
Each of these feature frame layers have 1536 units and the LSTM
projection with a size of 512.
The models ùëüùëõùëõùë°_90ùëöandùëüùëõùëõùë°_60ùëöare pre-trained on both the
HT data of 120ùêæhours and 340ùêæhours of SSL data generated using
the teacher ( ùëá1) decoded labels. The human transcribed data used
in the pre-training utilizes data up to the end of 2020, while the SSL
data is in 2020. For our experiments in this paper, we further train
the above pre-trained RNN-T student models using a total amount
of180ùëòhours of SSL data (teacher generated labels) available in a
time-window of 6months in 2021.
Training details : We use the following parameters to train both
the teacher and student models. The system is run on a fleet con-
sisting of 200nodes. We adopt a learning rate schedule of warm-up
whereùëôùëü=1ùëí‚àí7for the first 3000 steps, followed by constant learn-
ing rate of 5ùëí‚àí4till50ùëòsteps, then exponential decay ( ùëôùëü=1ùëí‚àí5)
from 50ùëòto750ùëòsteps with Adam optimizer (hyperparameters are
ùõΩ1=0.9,ùõΩ2=0.99).
We experiment with multiple large batch sizes ( 9ùëò,18ùëò,73ùëò,147ùëò,
215ùëò,307ùëò) through gradient accumulations. Note that these accu-
mulations have an implicit effect of changing the gradient values
due to the summation of gradients across a large batch. We process
large batches without altering the ùëôùëüschedule while accumulating
the gradients. The performance of these models is measured in
terms of relative word error rate reduction (WERR) over the corre-
sponding baselines. WER is the ratio of edit distance to sequence
length, where edit distance is the length of the shortest sequence
of insert, delete and substitution operation on transforming a pre-
dicted sequence to target.
5 RESULTS & DISCUSSION
In this section, we analyze the performance of incremental learning
in ILASR. In particular, we analyze the performance of incremental
learning in ILASR in terms of relative word error rate reduction
(WERR) in comparison with the initial pre-trained student models
as baselines.
From Table 1, we see that ILASR improves a strongly trained
base model by up to 3% on test sets in 2021 which climbs to 20%
on the delta dataset that consists of new or trending words andTable 1: Relative % WER improvements from the initial
model when trained with the ILASR system
ILASR
Time Test-set replay no replay
2021Rare 0.72% 0.66%
Delta 20.10% 23.99%
General 1.23% 0.41%
Jan-Mar 1.25% 1.50%
Apr-Jun 2.73% 3.09%
2020Rare 0.62% 0.62%
General 0.00% -0.72%
Message -0.83% -2.04%
2018-2019Rare -0.63% -0.63%
General -1.21% -2.6%
Message -2.82% -3.42%
Jan Feb Mar Apr May Jun
Incremental model in each month (2021)‚àí4‚àí2024681012WERR (%)
rnnt 60m
test sets
Jan
FebMar
AprMay
Jun
Figure 3: Monthly WERR (%) for incremental learning in
ILASR forùëüùëõùëõùë°_60ùëöon six of the monthly test sets ( ùêΩùëéùëõ‚ÄìùêΩùë¢ùëõ)
when measured relative to the starting model versus the one
trained incrementally in each month.
phrases. At the same time, performance on older general and tail
test sets do not see much degradation.
Catastrophic forgetting is one of the issues incremental learning
needs to circumvent in order to have consistent performance across
both old and new data. In Table 1, we compare the performance of
replay based incremental learning, where a sub-sampled portion
of 120K-hour human-transcribed data is also consumed in model
training while the no replay counterpart does not involve that. As
demonstrated in Table 1, replay based training tends to outperform
itsno replay counterpart on older test sets as expected from IL
literature.
Next, we evaluate the incrementally trained ILASR models on
fine-grained test sets that are prepared in each of the six months
(Jan-Jun) of 2021, see Figure 3. For all the evaluations in Figure 3, we
report the WERR in each month relative to the initial pre-trained
model (for example, WERR in ùëÄùëéùë¶ is the relative difference between
the WERs of ùëÄùëéùë¶ model and the pre-trained). The results show
incremental improvements in performance on all the six monthly
test sets from month to month in the ILASR training. This suggests

--- PAGE 6 ---
KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Gopinath Chennupati et al.
Jan Feb Mar Apr May Jun Jul Aug Sep
Incremental model in each month (2020)‚àí202468WERR (%)
rnnt 60m (2020 Q1)
test sets
Jan Feb Mar
(a)2020ùëÑ1
Jan Feb Mar Apr May Jun Jul Aug Sep
Incremental model in each month (2020)‚àí202468WERR (%)
rnnt 60m (2020 Q2)
test sets
Apr May Jun (b)2020ùëÑ2
Jan Feb Mar Apr May Jun Jul Aug Sep
Incremental model in each month (2020)‚àí202468WERR (%)
rnnt 60m (2020 Q3)
test sets
Jul Aug Sep(c)2020ùëÑ3
Figure 4: For the ùëüùëõùëõùë°_60ùëö, the pre-trained model is trained on the data available until 12/2019. Training this pre-trained model
in incremental mode for the next nine months ( ùêΩùëéùëõ‚àíùëÜùëíùëù) in 2020. The x-axis shows the monthly incremental model, where
the model from previous month is fine-tuned on the data in current month; y-axis shows the relative WER in each month
w.r.t the initial pre-trained model. Each of the curves represent the test set of the corresponding month.
that the incremental training helps in capturing the new trends
in time periods while the model is adapting to the incremental
changes in the data. It is also noteworthy that the incremental
improvement does not come at the cost of catastrophic forgetting.
More interestingly, the models trained with the data until ùëÄùëéùë¶ /ùêΩùë¢ùëõùëí
degrade the performance on ùêΩùë¢ùëõùëí test-set, which improves after
the model is trained on the data available from ùëÄùëéùë¶/ùêΩùë¢ùëõùëí . This
clearly suggests the adaptive nature of capturing the shifts in data
in the new time periods in ILASR.
To further strengthen the incremental learning claims, we an-
alyze the incremental learning patterns for a longer duration in
the time-periods between ùêΩùëéùëõ‚àíùëÜùëíùëùin2020 . Figure 4 shows the
learning patterns on a quarterly basis for the first three quarters
(Q1‚ÄìQ3) of 2020. In 2020ùëÑ1andùëÑ2, the WERR improves initially
and then decreases as the incremental model training progresses on
a month-over-month basis. The degradation (whilst better than the
baseline) is a demonstration of forgetting as newer updates are pri-
oritized over the months old test sets. Consequently, in 2020ùëÑ3, the
performance improves without any downward trends, which is due
to the fact that the models keep learning month over month while
the test sets also belong to the same time-periods. These trends
suggest that the proposed techniques help in incrementally improv-
ing the performance even in longer time-periods while limiting the
regressions on the older eval data.
Next, we explore several design choices which play a key role in
the performance of ILASR and share our insights in terms of the
design choices.
5.1 Design Choices: ILASR
We explore the following design choices in the context of the ILASR
framework: 1) effect of large batch sizes on performance of the
student models; 2) temporal effects on processing the data in ILASR;
3) analyze the importance of different teacher models in ILASR.
5.1.1 Training is robust to large batch sizes. We use large batches
in ILASR via gradient accumulations. As the effective batch size in-
creases, the number of optimization or update steps reduces as the
same amount of data is processed. Larger batch sizes would require
fewer optimization steps and vice versa for the same amount of
data. Use of large batches accelerates the training (shown in [ 64]),Table 2: Effect of large batches on the relative improvement
in performance (in terms of WERR, %) of all the three mod-
els when fine-tuned in ILASR.
Time Test-seteffective batch size
9k 18k 73k 147k 215k 307k
ùëüùëõùëõùë° _60ùëö
2021 Jan‚ÄìMar 2.74% 1.49% 2.37% 2.37% 2.24% 2.24%
2018‚Äì Rare 3.58% 3.58% 3.72% 3.65% 3.78% 3.72%
2019 Message 6.46% 6.05% 9.46% 9.46% 9.28% 9.37%
General 15.14% 15.12% 14.70% 14.56% 14.41% 14.26%
which is similar in ILASR. The reason large batch sizes is relevant
in the ILASR system is that there are limitations about how quickly
gradients can be aggregated and the global model distributed to
the servers in the fleet. Hence, a limited number of update steps
can take place in a time period compared to GPU-based offline dis-
tributed training. Moreover, as data arrives in a streaming fashion
and is not persisted, it needs to be consumed as and when it arrives,
in near real-time. For each of the limited number of updates, a large
amount of streaming data is available.
We explore the trade-off between large batches and model perfor-
mance. Table 2 shows the effect of large batches on performance of
a student models trained in ILASR. The performance (WERR) is rel-
ative to the corresponding pre-trained student model. This baseline
is weaker, hence improvements are larger. We find that increasing
the batch multiplier (effective batch size) has insignificant effect
on WER. As batch sizes increase from 9K to 300K utterances, the
difference in the accuracies is insignificant.
More importantly, this finding is in contrast to the test accuracy
degradation effects reported in literature [ 20,22,33,37,41,42,52,
56] with the use of large batches. We observe that such degradation
is not evident for model training in ILASR. Although, the attempts
in the literature have no strong mathematical justification, Goyal
et al. [ 22] reasoned the performance degradation to optimization
issues, thereby using warm-up to mitigate the degradation. Sim-
ilarly, in our case, we attribute the gains and/or no performance
degradation to the following factor. The initialized models are pre-
trained that have converged on the data from a previous time period
as opposed to random initialization in the large batch training in

--- PAGE 7 ---
ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA
Table 3: Impact of the temporal order (chronological versus
random) of processing the training data in ILASR for both
with and without replay of the human transcriptions.
Time Test-set Chrono vs. random
replay no replay
2021Rare -0.62% -1.16%
Delta -1.68% -0.73%
General 0.15% 1.47%
Jan-Mar -0.53% -0.24%
Apr-Jun -0.47% 0.29%
2020Rare -0.56% -0.90%
General -0.55% 1.61%
Message 0.35% 0.48%
2019-2019Rare -0.46% -1.26%
General 0.32% 0.67%
Message -0.11% -0.87%
literature, usually, these models are trained from scratch (despite
the few initial epochs in warm-up) in the literature.
5.1.2 Impact of chronologically ordered data. One important aspect
of IL is the data being processed in time as is available, chronologi-
cally. We analyze the effect of processing order (chronological vs
random) for the six months in 2021 . Note, random order is same
as shuffling the data in regular distributed training of deep mod-
els. Chronological data is not IID across time as utterances have a
correlation with the time of day (for example, requests to snooze
alarms in the morning or turning smart lights on after sundown).
We found that there is no difference in performance of processing
the data chronologically as compared to randomly as depicted in
Table 3. Moreover, in both the cases of chronological and random-
ized, the improvements over initial baselines are clearly evident
(see Table 1).
Table 4: Performance (in terms of WERR, %) of the RNN-
HMM hybrid ASR teacher ( ùëá2) and bidirectional RNN-HMM
hybrid ASR ( ùëá3) based teacher models with respect to the
Conformer teacher ( ùëá1). The negative (-) sign represents that
ùëá1performs worse while the rest shows that ùëá1is the best
performing teacher model.
Time Test-setùëá1vsùëá3ùëá1vsùëá2
2021 Jan‚ÄìMar 16.63% 0.14%
2018‚Äì2019Rare 8.75% 12.02%
Message 7.34% 14.92%
General‚àí0.89% 20.51%
5.1.3 Ablations with teachers and students. We experiment with
three different teacher models that are trained for different time
ranges with different architectures. This experiment helps us ex-
plore the importance of keeping an updated and more effective
teacher. The three teachers are: ùëá1is the Conformer based that is
explained earlier in section 4.2; ùëá2is a RNN-HMM conventionalhybrid model [ 6];ùëá3is a bidirectional RNN-HMM conventional
hybrid ASR model. ùëá1andùëá2are trained on the same amount of
data until the end of 2020 whileùëá3is trained on the data (a total of
‚àº100ùëòhours of HT data) available till the end of 2019.
Table 4 compares the performance of the teacher models. On an
average,ùëá1is better than the rest of the two teachers, ùëá1>ùëá2>ùëá3
on new data reflecting the importance of keeping the teacher model
up-to-date. Conformer based teacher, ùëá1is better than the rest of
the remaining two teachers. The relative performance differences,
when measured on the four standard test sets are, ùëá1is better than
ùëá2andùëá3with 11.85%and 7.96%WERR, respectively.
Table 5: The performance (in terms of WERR) of the student
models when trained with the machine transcripts gener-
ated from each of the three different teacher models.
Time Test-setùëüùëõùëõùë° _90ùëö ùëüùëõùëõùë° _60ùëö
ùëá1 ùëá2 ùëá3 ùëá1 ùëá2 ùëá3
2021 Jan‚ÄìMar 10.5% 8.54% 7.41% 7.78% 6.27% 2.87%
2018‚Äì Rare 5.48% 5.71% 5.60% 4.21% 3.89% 3.58%
2019 Message 4.12% 4.88% 7.12% 3.07% 3.74% 6.05%
General 8.25% 7.88% 7.30% 5.07% 5.03% 6.55%
Table 5 shows the WERR of two student models ( ùëüùëõùëõùë°_90ùëöand
ùëüùëõùëõùë°_60ùëö) when trained using the machine transcripts generated
from the three teacher models. We observe that both the students
are similar in terms of performance. On an average, for ùëüùëõùëõùë°_90ùëö,
ùëá1based training is better than ùëá2andùëá3, with 4.66%and 3.25%
WERR, respectively. For ùëüùëõùëõùë°_60ùëö,ùëá1is better than ùëá2andùëá3
with 5.96and 2.18%relative WERR improvement respectively. The
improvements are larger than in Table 1 as these experiments were
done with 3 months of data using a weaker baseline. In fact, both
the students have same order of performance as the teachers, that
isùëá1>ùëá2>ùëá3even after training in IL on new data. More im-
portant, the magnitude of improvement in student (true for both
the student models) training is not of same scale as the difference
in teachers. For example, Conformer based teacher ( ùëá1) is better
thanùëá3by7.96%, whereasùëüùëõùëõùë°_90ùëöstudent trained with Con-
former transcripts ( ùëá1) is3.25%better than the one trained with ùëá3
transcripts. This suggests that better teacher models result in im-
proving the student performance but the difference (same student
trained with different teacher models) is narrower. In other words,
a significantly better teacher model can have a limited impact in
improving students models in ILASR.
6 RELATED WORK
SGD gradients mini and large: Stochastic gradient descent (SGD)
drives the training of neural nets with mini batches. Large mini
batches [ 22,27,53,64] reduce the number of updates with a large
step size. Simply increasing the batch size reduces the test accu-
racy [ 33] as the gradients get integrated. Test set accuracy can be
improved with large batches that are proportional to the learning
rate. This simple linear scaling is inefficient, which necessitates a
warm-up phase [ 22]. Instead of decaying the learning rate, increas-
ing the batch size during training [ 53] helps to reduce the commu-
nication steps to update the model and improves the test accuracy.
Federated averaging [ 44] (FedAvg) follows a similar strategy of

--- PAGE 8 ---
KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Gopinath Chennupati et al.
synchronously updating the gradients. Thus, the centralized model
simply aggregates the updates from various clients. Therefore, we
apply these large synchronous batch updates (as in [ 53]) to the
model in federated settings (similar designs were proposed in [ 5])
both in federated SGD and averaging algorithms. Considering the
negative effects of large-batch training on test accuracy in litera-
ture [ 9,20,33,37,41,42,52], a post-local SGD was proposed [ 38],
inspired from the FedAvg, where they adopt a warm-up [ 22] based
mini-batch SGD training for initial training before launching Fe-
dAvg. Similarly, distributed SGD for speech [ 55] and large scale
training with million hours of speech [ 49] have helped accelerate
the production for ASR models.
Semi-supervised Learning in ASR: The semi-supervised learn-
ing described in [ 31,32] employed auto-encoders to extract speech
and text features from unpaired text and speech data. Semi-supervised
ASR with filter-bank features [ 39] use deep contextualized acous-
tic representations with small amounts of labeled data. The weak
distillation of audio-text pairs resulted from the unsupervised tech-
niques in [ 36] helped in improving the end to end ASR. The semi-
supervised approaches in [ 61] combined the data augmentation
through spectral augment [ 48] and consistency regularization to
improve the performance. Dropout offer the power of ensembles,
the semi-supervised dropout attempts in [ 14] improved the pseudo
label accuracy and model performance in ASR. Recently, the work
in [63] employed contrastive semi-supervised learning with pseudo-
labeling in transcribing the video content. In this paper, we use the
pseudo-labels generated from a teacher model in federated setting
with large batch sizes.
Unsupervised Learning in ASR: A related area is the training
of representation, foundation, or upstream models from scratch
using large volumes of unlabelled data. This model can then be
fine-tuned for downstream use cases such as ASR, speaker recogni-
tion, among others. This paradigm is contrasted with the use case
of incremental updates to a pre-trained ASR model presented in
this work. A comprehensive survey of such methods for speech
representation learning are in [ 45]. The upstream model is trained
with a pretext task such as a generative approach to predict or recon-
struct the input given a limited view (eg past data, masking) such
as autoregressive predictive coding [ 12]. In a contrastive approach,
a representation is learned that is close to a positive sample and
further away from negative samples; wav2vec 2.0 [ 3] is an exemplar
where the representation is trained to be close to a quantized target
vector. Finally, in predictive approaches [ 4,10,28], the pretext task
is to predict for masked input timeframes, a distribution over a
discrete vocabulary such as clustered log-mel features. ASR models
pre-trained using these techniques can be updated using ILASR.
7 CONCLUSIONS
We proposed the ILASR framework for privacy preserving incre-
mental learning of end-to-end automatic speech recognition sys-
tems. ILASR is a big step forward for production level ASR systems,
especially for automatic incremental updates of these systems. In
this study of near-real time training with ILASR, we learned that
even the converged production level ASR models: 1) can be im-
proved significantly in an incremental fashion with 3% general
improvements that can go up to 20% on test sets with new wordsor phrases; 2) training with large batches arising as a result of com-
munication constraints does not result in degradation; 3) memory
replay training is effective at mitigating catastrophic forgetting on
older test sets; 4) there is no significant impact of chronological
versus random processing of data in IL for speech recognition over
a period of six months; and finally; 5) having a significant improve-
ment in teacher models used to generate machine transcripts does
not translate to the same scale of improvements in students.
In the future, we will explore the utility of noisy students for
iterative self-learning instead of relying on teacher models in ILASR.
Real-time resource-constrained on-device speech recognition is still
a hard challenge. Here, we plan to further explore different direc-
tions such as finding the best hyper parameters [ 34], controlling
leaky gradients [ 66], stopping gradient inversion and data leakage
attacks [ 58], personalizing ASR depending on the device context,
and using smaller teacher models or self-labelling that can be run
on device. Approximate gradient computation techniques may be
required with severe compute resource limitations. Further, explor-
ing methods of integrating weak supervision information from
inferred or explicit user feedback from a session of interactions as
well as externally updated language models are avenues of further
research.
ACKNOWLEDGMENTS
We thank Kishore Nandury, Fred Weber, and Anand Mohan for dis-
cussions related to production ASR and utterance selection heuris-
tics. Valentin Mendelev assisted with delta testset construction to
measure the impact of IL on new data. We thank Bach Bui, Ehry
MacRostie, Chul Lee, Nikko Strom, and Shehzad Mevawalla for help-
ful discussions, review and support. We are indebted to the Alexa
Speech Recognition group for comments, dataset construction and
training infrastructure development.
REFERENCES
[1]Mohammad Al-Rubaie and J Morris Chang. Privacy-preserving machine learning:
Threats and solutions. IEEE Security & Privacy , 17(2):49‚Äì58, 2019.
[2]Robins Anthony. Catastrophic forgetting, rehearsal and pseudorehearsal. Con-
nection Science , 7(2):123‚Äì146, 1995.
[3]Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec
2.0: A framework for self-supervised learning of speech representations. Advances
in Neural Information Processing Systems , 33:12449‚Äì12460, 2020.
[4]Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael
Auli. Data2vec: A general framework for self-supervised learning in speech,
vision and language. arXiv preprint arXiv:2202.03555 , 2022.
[5]Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Koneƒçn `y, Stefano Mazzocchi,
H Brendan McMahan, et al. Towards federated learning at scale: System design.
arXiv preprint arXiv:1902.01046 , 2019.
[6]Herve A Bourlard and Nelson Morgan. Connectionist speech recognition: a hybrid
approach , volume 247. Springer Science & Business Media, 2012.
[7]Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory,
not parameters for efficient on-device learning. Advances in Neural Information
Processing Systems , 33:11285‚Äì11297, 2020.
[8]Francisco M Castro, Manuel J Mar√≠n-Jim√©nez, Nicol√°s Guil, Cordelia Schmid, and
Karteek Alahari. End-to-end incremental learning. In Proceedings of the European
conference on computer vision (ECCV) , pages 233‚Äì248, 2018.
[9]Kai Chen and Qiang Huo. Scalable training of deep learning machines by in-
cremental block training with intra-block parallel optimization and blockwise
model-update filtering. In 2016 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 5880‚Äì5884, 2016. doi: 10.1109/ICASSP.2016.
7472805.
[10] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,
Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-
scale self-supervised pre-training for full stack speech processing. arXiv preprint
arXiv:2110.13900 , 2021.

--- PAGE 9 ---
ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA
[11] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick
Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina
Gonina, et al. State-of-the-art speech recognition with sequence-to-sequence
models. In 2018 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 4774‚Äì4778. IEEE, 2018.
[12] Yu-An Chung and James Glass. Generative pre-training for speech with autore-
gressive predictive coding. In ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages 3497‚Äì3501. IEEE, 2020.
[13] Xiaodong Cui, Songtao Lu, and Brian Kingsbury. Federated acoustic modeling
for automatic speech recognition. CoRR , abs/2102.04429, 2021.
[14] Subhadeep Dey, Petr Motlicek, Trung Bui, and Franck Dernoncourt. Exploiting
semi-supervised training through a dropout regularization in end-to-end speech
recognition. arXiv preprint arXiv:1908.05227 , 2019.
[15] Tim Dierks and Eric Rescorla. The transport layer security (tls) protocol version
1.2. 2008.
[16] Dimitrios Dimitriadis, Kenichi Kumatani, Robert Gmyr, Yashesh Gaur, and Se-
fik Emre Eskimez. A federated approach in training acoustic models. In Proc.
Interspeech , 2020.
[17] Robert M French. Catastrophic forgetting in connectionist networks. Trends in
cognitive sciences , 3(4):128‚Äì135, 1999.
[18] Yan Gao, Titouan Parcollet, Javier Fernandez-Marques, Pedro P. B. de Gusmao,
Daniel J. Beutel, and Nicholas D. Lane. End-to-end speech recognition from
federated acoustic models, 2021.
[19] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated
learning: A client level perspective. arXiv preprint arXiv:1712.07557 , 2017.
[20] Noah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, Amir Gholami,
Kai Rothauge, Michael W Mahoney, and Joseph Gonzalez. On the computational
inefficiency of large batch sizes for stochastic gradient descent. arXiv preprint
arXiv:1811.12941 , 2018.
[21] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio.
An empirical investigation of catastrophic forgetting in gradient-based neural
networks. arXiv preprint arXiv:1312.6211 , 2013.
[22] Priya Goyal, Piotr Doll√°r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large
minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677 ,
2017.
[23] Filip Granqvist, Matt Seigel, Rogier van Dalen, A‚Äôine Cahill, Stephen Shum,
and Matthias Paulik. Improving on-device speaker verification using federated
learning with privacy, 2020.
[24] Alex Graves. Sequence transduction with recurrent neural networks. arXiv
preprint arXiv:1211.3711 , 2012.
[25] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui
Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer:
Convolution-augmented transformer for speech recognition. arXiv preprint
arXiv:2005.08100 , 2020.
[26] Dhruv Guliani, Fran√ßoise Beaufays, and Giovanni Motta. Training speech recog-
nition models with federated learning: A quality/cost framework. In ICASSP
2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pages 3080‚Äì3084. IEEE, 2021.
[27] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better:
closing the generalization gap in large batch training of neural networks. arXiv
preprint arXiv:1705.08741 , 2017.
[28] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech
representation learning by masked prediction of hidden units. IEEE/ACM Trans-
actions on Audio, Speech, and Language Processing , 29:3451‚Äì3460, 2021.
[29] Hui Jiang. Confidence measures for speech recognition: A survey. Speech
communication , 45(4):455‚Äì470, 2005.
[30] Kaustubh Kalgaonkar, Chaojun Liu, Yifan Gong, and Kaisheng Yao. Estimating
confidence scores on asr results using recurrent neural networks. In 2015 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages
4999‚Äì5003. IEEE, 2015.
[31] Shigeki Karita, Shinji Watanabe, Tomoharu Iwata, Atsunori Ogawa, and Marc
Delcroix. Semi-supervised end-to-end speech recognition. In Interspeech , pages
2‚Äì6, 2018.
[32] Shigeki Karita, Shinji Watanabe, Tomoharu Iwata, Marc Delcroix, Atsunori
Ogawa, and Tomohiro Nakatani. Semi-supervised end-to-end speech recognition
using text-to-speech and autoencoders. In ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 6166‚Äì6170.
IEEE, 2019.
[33] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
and Ping Tak Peter Tang. On large-batch training for deep learning: Generaliza-
tion gap and sharp minima. arXiv preprint arXiv:1609.04836 , 2016.
[34] Mikhail Khodak, Tian Li, Liam Li, M Balcan, Virginia Smith, and Ameet Talwalkar.
Weight sharing for hyperparameter optimization in federated learning. In Int.
Workshop on Federated Learning for User Privacy and Data Confidentiality in
Conjunction with ICML 2020 , 2020.[35] Taku Kudo. Subword regularization: Improving neural network translation
models with multiple subword candidates. In ACL, 2018.
[36] Bo Li, Tara N Sainath, Ruoming Pang, and Zelin Wu. Semi-supervised training for
end-to-end models via weak distillation. In ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 2837‚Äì2841.
IEEE, 2019.
[37] Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J. Smola. Efficient mini-batch
training for stochastic optimization. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining , KDD ‚Äô14, page
661‚Äì670. ACM, 2014.
[38] Tao Lin, Sebastian U. Stich, and Martin Jaggi. Don‚Äôt use large mini-batches, use
local sgd. ArXiv , abs/1808.07217, 2020.
[39] Shaoshi Ling, Yuzong Liu, Julian Salazar, and Katrin Kirchhoff. Deep contextual-
ized acoustic representations for semi-supervised speech recognition. In ICASSP
2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pages 6429‚Äì6433. IEEE, 2020.
[40] Prerna Mahajan and Abhishek Sachdeva. A study of encryption algorithms aes,
des and rsa for security. Global Journal of Computer Science and Technology , 2013.
[41] Dominic Masters and Carlo Luschi. Revisiting small batch training for deep
neural networks. arXiv preprint arXiv:1804.07612 , 2018.
[42] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An
empirical model of large-batch training. arXiv preprint arXiv:1812.06162 , 2018.
[43] Michael McCloskey and Neal J Cohen. Catastrophic interference in connection-
ist networks: The sequential learning problem. In Psychology of learning and
motivation , volume 24, pages 109‚Äì165. Elsevier, 1989.
[44] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. Communication-efficient learning of deep networks
from decentralized data. In Artificial Intelligence and Statistics , pages 1273‚Äì1282.
PMLR, 2017.
[45] Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D Havtorn, Joakim
Edin, Christian Igel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars Maal√∏e,
et al. Self-supervised speech representation learning: A review. arXiv preprint
arXiv:2205.10643 , 2022.
[46] Climent Nadeu Camprub√≠, Francisco Javier Hernando Peric√°s, and Monica Gorri-
cho Moreno. On the decorrelation of filter-bank energies in speech recognition.
InEUROSPEECH‚Äô95: 4th European Conference on Speech Communication and
Technology: Madrid, Spain: 18-21 September 1995 , pages 1381‚Äì1384, 1995.
[47] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Lib-
rispeech: an asr corpus based on public domain audio books. In 2015 IEEE
international conference on acoustics, speech and signal processing (ICASSP) , pages
5206‚Äì5210. IEEE, 2015.
[48] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D
Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for
automatic speech recognition. arXiv preprint arXiv:1904.08779 , 2019.
[49] Sree Hari Krishnan Parthasarathi and Nikko Strom. Lessons from building
acoustic models with a million hours of speech. In ICASSP 2019-2019 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP) ,
pages 6670‚Äì6674. IEEE, 2019.
[50] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Koneƒçn `y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated
optimization. arXiv preprint arXiv:2003.00295 , 2020.
[51] Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and
Virginia Smith. On the convergence of federated optimization in heterogeneous
networks. arXiv preprint arXiv:1812.06127 , 3:3, 2018.
[52] Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy
Frostig, and George E Dahl. Measuring the effects of data parallelism on neural
network training. arXiv preprint arXiv:1811.03600 , 2018.
[53] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don‚Äôt decay
the learning rate, increase the batch size, 2018.
[54] H. Soltau, H. Liao, and H. Sak. Reducing the computational complexity for
whole word models. 2017 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU) , pages 63‚Äì68, 2017.
[55] Nikko Strom. Scalable distributed dnn training using commodity gpu cloud com-
puting. In Sixteenth Annual Conference of the International Speech Communication
Association , 2015.
[56] Peng Sun, Wansen Feng, Ruobing Han, Shengen Yan, and Yonggang Wen. Op-
timizing network performance for distributed dnn training on gpu clusters:
Imagenet/alexnet training in 1.5 minutes, 2019.
[57] Prakhar Swarup, Roland Maas, Sri Garimella, Sri Harish Mallidi, and Bj√∂rn
Hoffmeister. Improving asr confidence scores for alexa using acoustic and hy-
pothesis embeddings. In Interspeech , pages 2175‚Äì2179, 2019.
[58] Aleksei Triastcyn and Boi Faltings. Federated generative privacy. IEEE Intelligent
Systems , 35(4):50‚Äì57, 2020.
[59] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
Yasaman Khazaeni. Federated learning with matched averaging. arXiv preprint
arXiv:2002.06440 , 2020.
[60] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling
the objective inconsistency problem in heterogeneous federated optimization.

--- PAGE 10 ---
KDD ‚Äô22, August 14‚Äì18, 2022, Washington, DC, USA Gopinath Chennupati et al.
arXiv preprint arXiv:2007.07481 , 2020.
[61] Felix Weninger, Franco Mana, Roberto Gemello, Jes√∫s Andr√©s-Ferrer, and Puming
Zhan. Semi-supervised learning with data augmentation for end-to-end asr.
arXiv preprint arXiv:2007.13876 , 2020.
[62] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo,
and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 374‚Äì382, 2019.
[63] Alex Xiao, Christian Fuegen, and Abdelrahman Mohamed. Contrastive semi-
supervised learning for asr. In ICASSP 2021-2021 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP) , pages 3870‚Äì3874. IEEE, 2021.
[64] Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for
imagenet training. arXiv preprint arXiv:1708.03888 , 6:12, 2017.
[65] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage
from gradients. arXiv preprint arXiv:2001.02610 , 2020.
[66] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. Advances
in Neural Information Processing Systems , 32, 2019.
