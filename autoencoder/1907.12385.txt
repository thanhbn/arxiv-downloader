# 1907.12385.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/autoencoder/1907.12385.pdf
# File size: 3847400 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
Xiao Li1 2Chenghua Lin2Ruizhe Li2Chaozheng Wang1Frank Guerin3
Abstract
We tackle the problem disentangling the latent
space of an autoencoder in order to separate la-
belled attribute information from other character-
istic information. This then allows us to change
selected attributes while preserving other informa-
tion. Our method, matrix subspace projection, is
much simpler than previous approaches to latent
space factorisation, for example not requiring mul-
tiple discriminators or a careful weighting among
their loss functions. Furthermore our new model
can be applied to autoencoders as a plugin, and
works across diverse domains such as images or
text. We demonstrate the utility of our method
for attribute manipulation in autoencoders trained
across varied domains, using both human eval-
uation and automated methods. The quality of
generation of our new model (e.g. reconstruction,
conditional generation) is highly competitive to a
number of strong baselines.
1. Introduction
We investigate the problem of manipulating multiple at-
tributes of data samples. This can be applied to image data,
for example to manipulate a picture of a face to add a beard,
change gender, or age. It can also be applied to text, for ex-
ample to change the style or sentiment of a text. We assume
that we have a training dataset where attributes are labelled.
However there is an unsupervised aspect because we do
not have samples of the same individual with different at-
tribute combinations, e.g., the same person with and without
a beard. Furthermore the training samples have some at-
tribute combinations that are highly correlated, while other
combinations are completely absent; e.g., in the CelebA
dataset blond hair and earrings are highly correlated with
female (Torfason et al., 2016), while a female with beard is
1Department of Computing Science, University of Aberdeen,
UK2Department of Computer Science, University of Shefﬁeld,
UK3Department of Computer Science, University of Surrey, UK.
Correspondence to: Chenghua Lin <c.lin@shefﬁeld.ac.uk >.
Proceedings of the 37thInternational Conference on Machine
Learning , Online, PMLR 119, 2020. Copyright 2020 by the au-
thor(s).absent. Nevertheless we would like our system to somehow
isolate the explanatory factors in pixel space, to understand,
e.g., that blond hair corresponds only to colour changes to
hair pixels, and no change elsewhere in the face.
This challenge of isolating multiple explanatory factors
poses interesting problems for generative models. In im-
ages of faces for example, even if the training data has no
bearded lady, a good generative model should be able to
‘imagine’ novel examples that combine attributes in ways
not present in training data. As noted by Higgins et al.
(2016) “Models are unable to generalise to data outside of
the convex hull of the training distribution . . . unless they
learn about the data generative factors and recombine them
in novel ways.” Ideally we should fully disentangle and
isolate the data generative factors, so that we can represent
the generative factors of a sample with a vector that has one
part labelled attribute information, and another part with
the other characteristic information of the sample. This is
in a small way part of a general trend to try to move deep
neural network research towards explanatory models of the
world (LeCun, 2013; Lake et al., 2016; Yuille & Liu, 2018),
which requires disentanglement. The problem is important
because isolating explanatory factors is a way to overcome
the combinatorial explosion of required training examples
if such factors are not isolated (Yuille & Liu, 2018).
A typical approach to the problem uses an autoencoder (AE)
which encodes a given input (e.g. picture, text, etc.) into a
latent vector , and then restores (decodes) the latent vector to
the given input (Lample et al., 2017; Hu et al., 2017; Xiao
et al., 2018; Li et al., 2019). The latent vector contains the
attribute information as well as other characteristic informa-
tion of the input. If one can change the attribute information
in the latent space, then one can generate examples with the
altered attributes. The difﬁculty here is twofold: (1) learn
a latent space representation which separates the attributes
from all other characteristic information, and (2) fully dis-
entangle the attributes. If we fail in the separation part,
then efforts to generate with speciﬁc attributes may conﬂict
with other information in the latent space (as in Kingma
et al. (2014) etc., see x2). If we fail in the second part then
examples generated with speciﬁed attributes will also be
contaminated with spurious attributes (see Fig. 1 Left).
Many recent approaches make use of auxiliary neural net-arXiv:1907.12385v3  [cs.LG]  14 Aug 2020

--- PAGE 2 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
Figure 1. Left: from RelGAN (Wu et al., 2019), where the only
attribute changed is hair colour, but we see signiﬁcant changes in
skin colour, eyebrows, eyes, and lips. Right: from Fader (Lample
et al., 2017), where female was changed to male, but female eye-
brows are retained above the male ones, due to skip connections.
work structures with adversarial training in the style of Gen-
erative Adversarial Networks (GANs). These new networks
can be used to remove attribute information from the latent
space (Lample et al., 2017), or to feedback a loss term to
impose the attributes they want to appear in the output (He
et al., 2019). These adversarial approaches have competing
loss terms (for example reconstruction loss, attribute clas-
siﬁcation loss, realistic output loss), and require a careful
choice of hyperparameters to weight these loss functions.
In the case of Lample et al. (2017) a slowly increasing loss
was critical. These hyperparameters and training schedules
must be determined by trial and error, to avoid training
instability. Even after successful training we have found
that some models ignore the desired attributes and put too
much weight on reconstruction and realistic output (see x4).
This is partly because we push systems to the very difﬁcult
setting of training for multiple attributes together (e.g. 40
attributes for CelebA). This is a very demanding setting
for disentanglement, e.g. to dissociate lipstick, make-up,
and blond hair from female, and to dissociate beard, bushy
eyebrows, and 5 o’clock shadow from male.
We propose a simple and generic method, Matrix Subspace
Projection (MSP), which directly separates the attribute
information from all other non-attribute information, with-
out relying on weighting loss terms from auxiliary neural
networks. Our variables representing attributes are fully
disentangled, with one isolated variable for each attribute
of the training set. Therefore, when we do conditional gen-
eration, we can assign pure attributes combined with other
latent data which does not conﬂict, so that the generated pic-
tures are of high quality and not contaminated with spurious
attributes. Meanwhile, our model is a universal plugin. In
theory, it can be applied to any existing AEs (if and only if
the AEs use a latent vector). If the AE is a generative model
(such as V AE), with our approach, it becomes a conditional
generative model that generates content based on the given
condition constraints. In the case of images, we add a Patch-
GAN at the end of our generator to sharpen the image, but
this is not connected with the attribute manipulation taskand is not core to our model; it could be replaced with any
super resolution and sharpening method.
Our plugin has two uses: (1) samples can be generated from
a random seed, but with given attributes; (2) a given sample
can be modiﬁed to have desired speciﬁed attributes. Our
key contributions are: (1) A simple and universal plugin
for conditional generation and content replacement, directly
applicable to any AE architectures (e.g., image or text).
(2) Strong performance on learning disentangled latent rep-
resentations of multiple (e.g. 40) attributes. (3) A principled
weighting strategy for combining loss terms for training.
The code for our model is available online1.
2. Related Work
The ﬁrst approaches to control of generation by attributes
(conditional V AEs (Kingma et al., 2014; Sohn et al., 2015;
Yan et al., 2016)) simply added attribute information as an
extra input to the encoder or the decoder. These approaches
generate using a latent vector zand also an attribute vector
y, where the zoften conﬂicts with y, because attribute
information has not been removed from z. With conﬂicting
inputs the best the V AE can do is to produce a blurry image.
Generative Adversarial Networks (GANs) can be aug-
mented with encoders. IcGAN trains separate encoders
for the yandzvectors, but does not try to remove poten-
tially conﬂicting information (Perarnau et al., 2016). The
IcGAN authors also note that it can fail to generate unusual
attribute combinations such as a woman with a moustache,
because the GAN discriminator discourages the generator
from generating samples outside the training distribution.
More recent work tackled the problem of separating the
attribute information from the latent vector, using a new
auxiliary network (like a GAN discriminator) (Lample et al.,
2017; Creswell et al., 2017; Klys et al., 2018), which at-
tempts to guess the attribute of the latent vector z, and
penalise the generator if attribute information remains. A
signiﬁcant drawback of these adversarial approaches is that
great care must be taken in training so that the loss from
the discriminator (which is trying to remove attribute in-
formation) does not disturb the training to produce a good
reconstruction. In the case of Fader networks (Lample et al.,
2017) it was necessary to start with a discriminator loss
weight of zero, and linearly increase to 0.0001 over the ﬁrst
500,000 iterations; the authors state “ This scheduling turned
out to be critical in our experiments. Without it, we observed
that the encoder was too affected by the loss coming from
the discriminator, even for low values of [loss coefﬁcient] .”
While this adversarial approach can successfully remove
attribute information from z, there is nothing to stop the
1Code: https://xiao.ac/proj/msp

--- PAGE 3 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
(a) Moustache/facial hair
(b) Glasses and facial hair subspace(c) Glasses subspace(d) Glasses subspace(a) Moustache/facial hair
(b) Glasses and facial hair subspace(c) Glasses subspace(d) Glasses subspace(a) Moustache/facial hair
(b) Glasses and facial hair subspace(c) Glasses subspace(d) Glasses subspace(a) Moustache/facial hair
(b) Glasses and facial hair subspace(c) Glasses subspace(d) Glasses subspace(a) Moustache/facial hair
(b) Glasses and facial hair subspace(c) Glasses subspace(d) Glasses subspace
(a) Moustache/facial hair
(b) Glasses and facial hair subspace(c) Glasses subspace(d) Glasses subspace(a) Moustache/facial hair
(b) Glasses and facial hair subspace(c) Glasses subspace(d) Glasses subspace(a) Moustache/facial hair(b) Glasses and facial hair subspace(c) Glasses subspace(d) Glasses subspace
Figure 2. Figure showing the difﬁculty of disentangling attributes
for supervised ‘adversarial’ approaches. (a) from Creswell et al.
(2017) shows signiﬁcant change in the eyebrows and eyes when
adding facial hair. (b,c,d) from Klys et al. (2018). (b) moving
across the glasses and facial hair subspace, from the female on the
left, brings signiﬁcant changes in eyebrows and eyes, and the shape
of cheeks, making the face more masculine. (c) moving in glasses
subspace shows changes around the eyes and mouth, looking older.
(d) also moving in glasses subspace shows a narrower smile and a
more masculine lower face. Note all of the pictures are generated
by V AE, none are original photographs.
decoder (generator) from associating other spurious infor-
mation with the attribute. For example the decoder might
associate the attribute intended to be for ‘glasses’ with an
older or more masculine face. This is what we see in the
results of two of the adversarial approaches (see Fig. 2).
Most of the results in Creswell et al. (2017) focus on the at-
tribute ‘smiling’ (not reproduced here), and this is very well
disentangled. It is only when the training dataset associates
other attributes with the trained attribute that entanglement
will arise. In Creswell et al. (2017) the attribute vector is a
single binary variable so that the system can only be trained
to control (or classify) one attribute. It is not unexpected
that a generator will associate spurious information with an
attribute if the association is present in the training data and
the system has been trained only on examples labelling a
single attribute, e.g., glasses. The system cannot know that
it should isolate ‘wearing glasses’, and not ‘wearing glasses
and older’. Fader Networks (Lample et al., 2017) can train
for multiple attributes together, however He et al. (2019)
state that “Although Fader Networks is capable for multi-
ple attribute editing with one model, in practice, multiple
attribute setting makes the results blurry.”
The most recent works (2018-19) are GAN-based. They do
not try to remove attribute information from the latent space,
but instead add an additional attribute classiﬁer after genera-
tion, and impose an attribute classiﬁcation loss. This is in
addition to a typical GAN discriminator for realistic images.
AttGAN (He et al., 2019) uses an endoder, decoder (genera-
tor), and the attribute classiﬁer and discriminator applied to
the output of the generator. StarGAN (Choi et al., 2018) and
RelGAN (Wu et al., 2019) use no encoder, but use a singe
generator twice, in a cycle; the ﬁrst direction alters attributeslike a conditional GAN, the second one attempts to recon-
struct the image (using original attributes), and so requires
that non-attribute information has been preserved. StarGAN
uses a discriminator and attribute classiﬁer, like AttGAN,
while RelGAN adds a third network for interpolation.
All the works cited from 2017 to 2019 have an adversar-
ial component (in the style of a GAN); they train auxiliary
classiﬁers to feed back loss terms, to ensure they remove
undesirable attributes, or enforce desired ones. They need a
careful weighting among loss terms, but there is no princi-
pled method for determining these weighting hyperparame-
ters. Our work does not rely on an adversarial component
to manipulate attributes; we use a more direct method of
matrix projection onto subspaces, in order to factorise the
latent representation and separate attributes from other in-
formation. Furthermore, unlike the above works2we do not
use any skip connections. Skip connections can introduce
errors when a region of the source and target image is quite
different, we illustrate this further in Fig. 1 Right.
In addition to the above works using labelled attributes there
is also work on the more difﬁcult problem of unsupervised
learning of disentangled generative factors of data (Chen
et al., 2016; Higgins et al., 2017; Kumar et al., 2018). How-
ever the supervised (labelled) approaches generate much
clearer samples of selected attributes, and superior disentan-
glement. An alternative approach to controlled generation is
to simply train a deep convolutional network and do linear
interpolation in deep feature space (Upchurch et al., 2017).
This shows surprisingly good results, but in changing an
attribute that should only affect a local area it can affect
more image regions, and can produce unrealistic results for
more rare face poses.
3. Method
3.1. Problem Formulation
We are interested in factorising and manipulating multiple
attributes from a latent representation learned by an arbi-
trary Autoencoder (AE). Suppose we are given a dataset
Dof elements (x;y)withx2Rnandy2Y=f0;1gk
representing kattributes of x.
Let an arbitrary AE be represented by z=F(x)and
x0=G(z), whereF()is the encoder, G()is the decoder, z
is the latent vector encoding x, andx0is the reconstruction
ofx(see Fig. 3). Note that when x0is a good approx-
imation of x(i.e.,x0x), the attribute information of
xrepresented in ywill also be captured in the latent en-
coding z. Attribute manipulation means that we replace
the attributes ycaptured by zwith new attributes yn. Let
2Not mentioned in the Fader networks paper, but in the pub-
lished code: https://github.com/facebookresearch/FaderNetworks

--- PAGE 4 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
𝑥𝑧𝑥!
FG
H
M𝑦𝑥𝑧̂𝑥!
H·FG·H-1
M𝑦
(a) General Architecture(b) Simplified Architecture𝑧̂AutoEncoderInvertible Classifier
Figure 3. (a) The general architecture of our model (MSP); (b) the
simpliﬁed architecture of MSP.
K()be a replacement function, then we have new latent
space zn=K(z;yn)andxn=G(K(z;yn)), where the
attribute information encoded in yncan be predicted from
xnand the non-attribute information of xwill be preserved .
To give a concrete example, given an image of a face, x,
we wish to manipulate xw.r.t. the presence or absence of a
set of desired attributes encoded in yn(e.g., a face with or
without smiles, wearing or not wearing glasses), producing
the manipulated image xn, without changing the identify of
the face (i.e., preserving the non-attribute information of x).
3.2. Learning Disentangled Latent Representations via
Matrix Subspace Projection
To tackle the problem formulated in x3:1, we propose a
generic method to factor out the information about attributes
yfromzbased on the idea of performing orthogonal matrix
projection onto subspaces. Our model works as a universal
plugin and in theory, it can be applied to any existing AEs.
The general architecture of the proposed MSP model is
depicted in Fig. 3 (a). Given a latent vector zencoding x
and an arbitrarily complex invertible function H(),H()
transforms zto a new linear space ( ^ z=H(z)) such that
one can ﬁnd a matrix Mwhere (a) the projection of ^ zon
M(denoted by ^ y) approaches y(i.e.,^ ycaptures attribute
information),
M^ z=^ y;^ y!y (1)
and (b) there is an orthogonal matrix U[M;N], where
Nis the null space ofM(i.e.,M?N) and the projection
of^ zonN(denoted by ^ s) captures non-attribute information.
AsUis orthogonal, we also have UTU 1.
Fig. 3 (b) presents a simpliﬁed architecture of our MSP
model, which is equivalent to the general architecture. This
simpliﬁcation exists because as explained earlier H()is
invertible. So when the encoder and decoder of an AE have
enough capacity, they can essentially absorb HandH 1.
In other words, rather than ﬁtting FandG, the encoder and
decoder will ﬁt HF()andGH 1()instead. As M
itself is an incomplete orthogonal matrix, similar operations
cannot be applied to M.Our main learning objective, in addition to the original
AE objective (i.e. reconstruction loss LAE; seex3.3 and
Eq. 8), is then to estimate Mwhich is nontrivial. We turn
the problem of ﬁnding an optimal solution for Minto an
optimisation problem, in which we need to (1) enforce ^ y
to be as close to y(i.e., the vector encoding the ground
truth attributes) as possible; and (2) minimise jj^ sjj2so that
^ scontains as little information from ^ zas possible. This can
be formulated into the following loss function
LMSP=L1+L2 (2)
L1=jj^ y yjj2=jjM^ z yjj2(3)
L2=jj^ sjj2(4)
HereL1andL2encode the above two constraints, respec-
tively, and ^ yis the predicted attributes. Given that the AE
relies on the information of ^ zto reconstruct x, the optimi-
sation constraints of LAEandL2essentially introduce an
adversarial process: on the one hand, it discourages any
information of ^ zto be stored in ^ sdue to the penalty from
L2; on the other hand, the AE requires information from ^ z
to reconstruct x. So, the best solution is to only restore the
essential information for reconstruction (except the attribute
information) in ^ s. By optimisingLMSP, we cause ^ zto be
factorised, with the attribute information stored in ^ y, while
^ sonly retains non-attribute information.
The ﬁrst part of our loss function L1is relatively straight-
forward. The main obstacle here is to compute L2as^ sis
unknown. We develop a strategy to compute jj^ sjj2indirectly.
According to the deﬁnition of ^ yand^ swe can derive:
L2=jj^ sjj2=jj^ s 0jj2
=jj[^ y;^ s] [^ y;0]jj2Identical deformation
=jjU^ z [^ y;0]jj2(5)
where the square brackets represent the vector concatenation.
Because Uis orthogonal, we have
L2=jjU^ z [^ y;0]jj2
=jjU 1(U^ z [^ y;0])jj2
=jj^ z U 1[^ y;0]jj2=jj^ z UT[^ y;0]jj2
=jj^ z [M;N]T[^ y;0]jj2
=jj^ z MT^ yjj2jj^ z MTyjj2(6)
With Eq. 6 (which makes use of the properties of orthogo-
nal matrices), we avoid computing ^ sandNdirectly when
minimisingjj^ sjj2, and turn the minimisation problem into
optimising Minstead. Finally, we have:
LMSP=L1+L2
=jjM^ z yjj2+jj^ z MT^ yjj2
jjM^ z yjj2+jj^ z MTyjj2(7)

--- PAGE 5 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
The loss function in Eq. 7 also guarantees that after training,
the solution for Mwill be part of the orthogonal matrix U
(seex4.5). WhenLMSP is small, the transposition of M
becomes the inverse of M.
3.3. Applying the Matrix Subspace Projection
Framework to an AE
To apply our matrix subspace projection (MSP) framework
to an existing AE, one only needs to derive a ﬁnal loss
function by combining the loss of the AE and the loss of our
MSP framework.
L=LAE+LMSP (8)
whereis the weight forLMSP. As illustrated in Fig. 3
(a) and (b), one should note that applying our framework
will not change the structure of the AE, where our MSP
component simply takes the latent vector ^ zof the AE as
input.LAEhopes that ^ scan store more information to
reconstruct x, butLMSP wants ^ sto contain less information.
Whenis small, the model becomes a standard AE. When
is too large, the non-attribute information in ^ zis reduced
excessively, resulting in the generated products tending to
the average of the training samples. Therefore, another
challenge we face is how to set appropriately.
We propose a principled strategy for effectively determining
the value of (this strategy is used in all experiments in
this paper). Since LAEandLMSP essentially represent a
competing relationship for ^ zresources, we specify that LAE
andLMSP have the same inﬂuence on updating ^ s. We use
to represent the “intensity” with which the AE updates
^ zduring each back-propagation process. This intensity de-
pends on the structure of the model and the loss function
used by the model. For example, suppose an AE (for picture
generation) uses a CNN decoder and L2-loss. During the
training process, the error of each pixel between the gener-
ated picture and the true picture is backpropagated to ^ zas
the gradients of ^ z. The sum of these gradients is the ﬁnal
gradient of ^ z(i.e., corresponding to LAE). For a picture
withhwpixels andccolour channels, there are hwc
parts of gradients accumulated, so the intensity is hwc.
The intensity of ^ yfor updating ^ zis the total amount of
attributes (i.e. the dimension of ^ y), because the error for
each attribute is propagated back to ^ zand accumulated (i.e.,
corresponding toLMSP ). Therefore, in order to balance the
inﬂuence ofLAEandLMSP on updating ^ zduring training,
we have:
hwc
size(attribute ) +size(^ z)(9)
When using the cross-entropy-loss (or NLL loss etc.), which
is usually for textual generative models (e.g. the seq2seq
model), each generated word produces only one intensity,regardless of the word embedding size. Meanwhile, loss
values returned by the cross-entropy-loss are proportional
to the error, but the loss values returned by the MSP loss
(which is a MSE loss) are proportional to the error’s square.
Therefore, for a sentence of length k, the intensity of the
entire sentence to ^ zisk2, so that for cross-entropy-loss,
k2
size(attribute ) +size(^ z)(10)
3.4. Content Replacement and Conditional Generation
After MSP is trained (i.e., Mis estimated), there are two
ways to perform content replacement or change of attributes.
One way is to derive the orthogonal matrix U= [M;N]
by solving the null space NofM(i.e., the null space is
constituted of all the speciﬁc solutions for nw.r.t. equation
Mn= 0, where nis an independent variable). Given an
inputx, we ﬁrst encode it as ^ z. We then use Uto obtain
the attribute vector ^ yofxand the non-attribute information
vector ^ sas follows.
[^ y;^ s] = [M;N]^ z=U^ z=Uencoder (x) (11)
At this point, we can directly replace [^ y;^ s]with[yn;^ s],
where ynis the new attribute vector. With [yn;^ s]andUT
(note that UTapproaches U 1during training), we can
derive the new latent code znand then decode it into xn,
which ultimately captures the desired new attributes.
xn=decoder (zn) =decoder (UT[yn;^ s]) (12)
Alternatively, we can avoid explicitly calculating matrix N
(i.e. avoid calculating ^ s), for content replacement. Accord-
ing to Eqs.11 and 12, we deﬁne das the distance between ^ z
andzn.
d=^ z zn=UT[^ y;^ s] UT[yn;^ s]
=UT([^ y;^ s] [yn;^ s]) =UT[^ y yn;0]
= [M;N]T[^ y yn;0]
=MT(^ y yn) =MT(M^ z yn) (13)
It should be noted that here ^ z6=MTM^ zbecause the
reconstruction loss does not allows ^ sto be zero. Thus, we
have:
zn=^ z d=^ z MT(M^ z yn) (14)
xn=decoder (^ z MT(M^ z yn)) (15)
If the AE itself is a generative model (such as V AE), then
the AE+MSP structure becomes a conditional generative
model. Given a randomly sampled srand an attribute vector
yr, the model can generate new sample xrwith the desired
attributes with Eq.12.

--- PAGE 6 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glass
input imageMSPFader NetworksAttGan♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glass♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glass
input imageMSPFader NetworksAttGan♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glass♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glass♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glass
input imageMSPFader NetworksAttGan♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glass♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glass♂+beard♂+mkupopen +smile♂+baldhair -glass♀+beard♂-mkupopen -smile♂+bangshair-glass♂-beard♀+mkupshut +smile♀+baldhair +glass♀-beard♀-mkupshut -smile♀+bangshair+glassOur ModelBaselineBaseline
Our ModelBaselineBaseline
Our ModelBaselineBaseline
Figure 4. Examples of image attributes transformations using MSP (our model), Fader Networks and AttGAN.
4. Evaluation
Here we evaluate the ability to disentangle. We also evaluate
the orthogonality of Mas it is an important indicator of how
well our algorithm can approximate M.4.1. Matrix Subspace Projection in VAE
We apply our model on a vanilla VAE (Kingma & Welling,
2013) with the standard CNN encoder and decoder (the ar-
chitectures are same as Lample et al. (2017)). We used the

--- PAGE 7 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
Seq2seq V AE V AE+GAN
Better quality 34.5% 12.6% 17.9%
with AE only
Better quality 37.6% 12.0% 15.2%
with AE+MSP
both similar quality 27.9% 75.4% 66.9%
Table 1. Evaluation results of generation quality. Numbers in the
table denote percentage of participants under the column heading
who felt images were better with or without MSP.
MSP(ours) Fader AttGAN
male x beard 0.78 0.42 0.45
female x beard 0.52 0.03 0.41
male x no-beard 0.86 0.40 0.42
female x no-beard 0.90 0.61 0.63
male x makeup 0.52 0.02 0.35
male x no-makeup 0.89 0.50 0.47
female x makeup 0.87 0.63 0.52
female x no-makeup 0.67 0.42 0.47
smile x open-mouth 0.89 0.59 0.63
no-smile x open-mouth 0.66 0.11 0.29
smile x calm-mouth 0.95 0.34 0.33
no-smile x calm-mouth 0.76 0.43 0.38
male x bald 0.78 0.10 0.29
male x bangs 0.56 0.05 0.19
female x bald 0.29 0.01 0.17
female x bangs 0.68 0.21 0.20
no-glasses x black-hair 0.74 0.38 0.53
no-glasses x golden-hair 0.86 0.36 0.79
glasses x black-hair 0.82 0.21 0.32
glasses x golden-hair 0.77 0.19 0.33
Table 2. The classiﬁcation accuracy (ResNet-CNN classiﬁer) of
generated images using MSP, Fader Networks and AttGAN.
ADAM optimiser with learning rate = 0.0002, mini-batch
size of 256, and images are upsampled to 256 256. We
add an additional PatchGAN (Li & Wand, 2016) to make the
produced images sharp. The architecture of the PatchGAN
discriminator also adopts the version of Lample et al. (2017).
Our baselines are Fader networks (Lample et al., 2017) and
AttGAN (He et al., 2019), based on their published code
and settings. We did not compare StarGAN (Choi et al.,
2018) because we feel it is superseded by AttGAN, which
demonstrated better performance. We did not compare Rel-
GAN (Wu et al., 2019) as it does not disentangle attributes
(see Fig. 1 (left), also RelGAN cannot add a moustache or
beard to a female face, instead it will transform it to a male
face with beard).
We evaluated on the CelebA dataset (Liu et al., 2015)
(202,600 images) and trained one model on all 40 labelled
attributes. The generated examples are shown in Fig. 4.
Qualitatively we see clear examples of what Fader networks
and AttGan cannot do: For the woman with glasses (mid-
dle) FaderNetwork and AttGan show complete inability toTarget Inﬂuence on MSP Fader AttGAN
attribute
changedother attributes (ours)
gender beard 0.01 0.28 0.09
beard gender 0.07 0.11 0.02
gender makeup 0.02 0.07 0.05
makeup gender 0.05 0.09 0.14
smile mouth-open 0.01 0.20 0.07
mouth-open smile 0.02 0.07 0.09
Table 3. Quantitative evaluation of disentanglement (using ResNet-
CNN classiﬁer).
mouth open / smiling attributes morphing
Fader
NetworkAttGAN V AE+GAN
MSP (ours)
perfect 36.7% 47.5% 68.3%
recognizable 20.8% 15.3% 4.9%
unreco/unchang 42.5% 37.2% 26.8%
male / beard attributes morphing
Fader
NetworkAttGAN V AE+GAN
MSP (ours)
perfect 38.3% 55.9% 74.4%
recognizable 8.3% 11.2% 11.6%
unreco/unchang 53.3% 32.9% 14.0%
Table 4. Manual evaluation results of disentanglement. Numbers
in the table denote percentage of participants under the column
heading who felt the images represented the speciﬁed attribute (e.g.
smiling) in a way that was perfect, recognisable, or unrecognis-
able/unchanged.
remove the glasses; FaderNetwork completely fails to add
glasses to the other two faces, and AttGan can only manage
weak rims on the ﬁnal woman (bottom). FaderNetwork in
general struggles to change attributes, especially for the two
women, while AttGan does better, but struggles with certain
attributes, e.g. mostly it fails to change the ﬁnal woman to
male, and struggles to remove makeup.
For a quantitative evaluation we trained a classiﬁer (ResNet-
CNN) to measure the accuracy with which attributes are
changed. Table 2 shows that our MSP approach outper-
forms the competitors. Finally we calculated the average
Fr´echet Inception Distance (FID) (Heusel et al., 2017) for
each method: MSP=35.0, Fader=26.3, AttGAN=7.3 (lower
is better, 0 is the best). The FID score tries to calculate the
similarity of original images and generated images. Clearly
AttGAN is a winner for quality while our MSP is a winner
for accuracy of attribute modiﬁcation. When AttGAN can-
not handle the attribute modiﬁcation it generates unchanged
images and can get lower FID scores.
The results of attribute manipulation (both qualitative and
quantitative) are surprisingly bad for Fader networks and

--- PAGE 8 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
AttGAN, especially relative to the examples displayed in
their original papers. The primary reason for this is that
we trained those models on all 40 attributes together. Fader
networks works best when trained on a single attribute, as
noted in Sec. 2. The original AttGAN paper trained on 13 at-
tributes, and indeed it performs better at attribute manipula-
tion than Fader in our pictures. For the 40-attribute-together
version, when any attribute is changed all others must be
unchanged. For example, when we transition from male
to female (Fig. 4 left), it is implicit that the female should
keep no make-up or lipstick, etc. In the direction from fe-
male to male the male should keep no bushy eyebrows or 5
o’clock shadow. The original Fader network paper displays
a beautiful example of interpolating between male and fe-
male, but the female does have make-up and lipstick and the
male does have bushy eyebrows and 5 o’clock shadow. Our
difﬁcult 40-attribute setting is central to our aims, as stated
in our introduction: we want to fully disentangle multiple
attributes, because this gives a generative model the ability
to ‘imagine’ novel examples that combine attributes in ways
not present in training data.
4.2. Human Evaluation of Generated Example Quality
We evaluated whether our MSP model reduces the quality
of generated examples, using human evaluation via Amazon
Mechanical Turk (hiring 150 participants in total).
For each model, we randomly generated 1,000 example
pairs. Each pair contains a reconstructed example (from
AE) and an example generated by AE+MSP with one or two
random attribute modiﬁcation (attributes were changed to
 1if they were originally >0, or changed to 1 if they were
<0)3. The participants were shown the examples pair-by-
pair in the blind test, and they were asked to please choose
the one with better text/image quality, or choose both if you
think they perform similarly . The participants were told
that the text quality means the ﬂuency, semantic accuracy,
and syntactic accuracy, and the image quality means the
clarity and (face) recognisability. The results are shown in
Table 1. We treat the scores (i.e. participants’ choices) of
the result as a Likert scale, and we set our null hypothesis to
beH0:generation quality of AE+MSP is worse than using
the AE only , and,H1:generation quality of AE+MSP is
equal or higher than using the AE only . The hypotheses
are tested by a discrete Mann-Whitney U-test, rejecting H0
withp<0:03.
4.3. Evaluation of Disentanglement
Disentanglement is also an important feature of our model.
It means that when an attribute is modiﬁed, other at-
3The generated examples were automatically ﬁltered to pre-
vent conﬂict attributes; e.g. images of woman with beard are not
provided to the participants in this experiment.tributes remain unchanged. We make the three models
(V AE+GAN+MSP, AttGAN, and Fader Networks) generate
images by manipulating two groups of highly correlated
attributes, openness of mouth / smiling, and male / beard.
For the two groups, the three models should respectively
generate the images with closed mouth no smiling, closed
mouthsmiling, opened mouth no smiling, opened
mouthsmiling, femaleno beard, femalebeard, male
no beard, and male beard. We hired 50 participants in
Amazon Mechanical Turk; each of them was given 40 image
blocks. A block contains four images, which were from the
mouth / smiling group or the male / beard group, and which
were generated by one of the three models. The participants
were told which image should represent which attributes,
and the participants evaluated whether it did for each im-
age in the block by using a 3-level Likert scale (perfect,
recognisable, and unrecognisable/unchanged). The results
are shown in Table 4. It shows that our model performs
signiﬁcantly better than the baseline. ( p<0:0001 ).
In addition to human evaluation, we also conducted a quan-
titative evaluation to test how well a model can change an
attribute in isolation. For some selected highly correlated
attributes we change one target attribute, and measure the
change in another non-target attribute. For instance (the row
of gender/beard in Table 3), when the gender attribute is
changed manually, we measure the amount by which the
beard attribute is consequently changed. The results are
shown in Table 3. Note that, the scores show how much the
non-target attributes are affected, but not whether the target
attributes are correctly changed in the generated pictures.
Therefore the scores need to be read in conjunction with
Table 2. According to both Table 2 and Table 3, we can
conclude that in both of the aspects of the manipulation of
attributes and avoiding inﬂuence on non-target attributes,
the performance of our model exceeds the baselines.
4.4. Matrix Subspace Projection in Seq2seq
We apply our model to a classic seq2seq model for textual
content replacement, in order to determine if we can replace
words according to the given attributes and keep other words
unchanged. In this task, we adopt the E2E corpus (Du ˇsek
et al., 2019), which contains 50k+ reviews of restaurants
(E2E dataset is developed for Natural Language Generation,
but here we use it for content replacement). Each review is
a single sentence that is labelled by the attribute-value pairs,
for example, “name=[The Eagle]”, “food=[French]”, and
“customerRating=[3/5]”. We regard each attribute-value pair
as a unique label. All the attributes constitute ywhose
entries are 1 or 0 to represent each value (the correct texts
of the attribute name or value are NOT used).
Both the encoder and decoder of the seq2seq model are
formed by two-layer LSTMs. The model is trained for 1000

--- PAGE 9 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
Example 1 Example 2
Orig-attribute eatType[pub], customer-rating[5-out-of-5],
name[Blue-Spice], near[Crowne-Plaza-Hotel]familyFriendly[yes], area[city-centre], eatType[pub],
food[Japanese], near[Express-by-Holiday-Inn],
name[Green-Man]
Orig-text the blue spice pub , near crowne plaza hotel ,
has a customer rating of 5 out of 5 .near the express by holiday inn in the city centre is
green man . it is a japanese pub that is
family-friendly .
New-attribute eatType[coffee-shop], customer-rating[5-out-
of-5], name[Blue-Spice], near[Avalon]familyFriendly[no], area[riverside],
eatType[coffee-shop], food[French],
near[The-Six-Bells], name[Green-Man]
New-text the blue spice coffee shop , near avalon has a
customer rating of 5 out of 5 .near the six bells in the riverside area is a green man .
it is a french coffee shop that is not family-friendly .
Table 5. Results of changing attributes in E2E corpus.
(a)(b)(c)(d)
01530456075015304560750200400600800100002004006008001000
051015202530350510152025303502004006008001000020040060080010001.00.80.60.40.20.0-0.2-0.4
Figure 5. Measuring orthogonality: Heat map of MTMandUTUfor Seq2seq+MSP (a,b), and V AE+GAN+MSP (c,d)
epochs (on a Tesla T4 around 12 hours). After training, we
reconstruct the review sentences with randomly replaced
attributes, for example replacing “name=[The Eagle]” by
“name=[Burger King]”, “customerRating=[3/5]” by “cus-
tomerRating=[1/5]”. 50% of attributes are changed in each
sentence. The outcomes are shown in Table 5.
4.5. Orthogonality Evaluation
The ability to disentangle attributes is ensured by the or-
thogonality of Min our model. Instead of directly using an
orthogonal matrix, we train Mto be orthogonal. Thus, we
evaluate how close Mis to the orthogonal matrix. Fig. 5
shows the heat map of MTMandUTU, which indi-
cates that the production is fairly close to a unit matrix. It
visualises MTMin the seq2seq version of our model
(Fig. 5 (a)) and in the V AE version (Fig. 5 (c)). The matrix
UTU(Uis formed by Mconcatenating its null space)
is also visualised (in Fig. 5 (b) and (d)). It is clear that
when the matrices are multiplied by their transposes, the
products do approximate the unit matrix. Although Fig. 5
(c) shows that a small number of attributes remain slightly
entangled (by the green and deep purple pixels), this is
mainly caused by the few conﬂicting attributes in CelebA,
for example the receding-hairline baldbangs, and thestraight-hairwavy-hair. Thus, Mis indeed trained to be
a (partial) orthogonal matrix.
5. Conclusion
We propose a matrix projection plugin that can be attached
to various autoencoders (e.g. Seq2seq, V AE) to make the
latent space factorised and disentangled, based on labelled
attribute information, which ensures that manipulation in
the latent space is much easier. We test the attribute manip-
ulation ability of our model on an image dataset and text
corpus, obtaining results that show clean disentanglement.
In addition our model involves a simpler training process
than adversarial approaches which need a long training with
a very low weight on the loss coming from the discrimina-
tor that removes attribute information, to avoid the encoder
being too affected by this loss term (Lample et al., 2017).
Acknowledgement
We would like to thank all the anonymous reviewers for
their insightful comments. This work is supported by the
award made by the UK Engineering and Physical Sciences
Research Council (Grant number: EP/P011829/1).

--- PAGE 10 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
References
Chen, X., Duan, Y ., Houthooft, R., Schulman, J., Sutskever,
I., and Abbeel, P. Infogan: Interpretable representation
learning by information maximizing generative adversar-
ial nets. In Lee, D. D., Sugiyama, M., Luxburg, U. V .,
Guyon, I., and Garnett, R. (eds.), Advances in Neural In-
formation Processing Systems 29 , pp. 2172–2180. Curran
Associates, Inc., 2016.
Choi, Y ., Choi, M., Kim, M., Ha, J., Kim, S., and Choo,
J. Stargan: Uniﬁed generative adversarial networks for
multi-domain image-to-image translation. In 2018 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018 ,
pp. 8789–8797. IEEE Computer Society, 2018. doi: 10.
1109/CVPR.2018.00916.
Creswell, A., Bharath, A. A., and Sengupta, B. Conditional
autoencoders with adversarial information factorization.
CoRR , abs/1711.05175, 2017. URL http://arxiv.
org/abs/1711.05175 .
Duˇsek, O., Novikova, J., and Rieser, V . Evaluating the state-
of-the-art of end-to-end natural language generation: The
E2E NLG Challenge. arXiv preprint arXiv:1901.11528 ,
January 2019. URL https://arxiv.org/abs/
1901.11528 .
He, Z., Zuo, W., Kan, M., Shan, S., and Chen, X. Attgan:
Facial attribute editing by only changing what you want.
IEEE Transactions on Image Processing , 28(11):5464–
5478, Nov 2019. ISSN 1941-0042. doi: 10.1109/TIP.
2019.2916751.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and
Hochreiter, S. Gans trained by a two time-scale update
rule converge to a local nash equilibrium. In Proceedings
of the 31st International Conference on Neural Informa-
tion Processing Systems , NIPS17, pp. 66296640, Red
Hook, NY , USA, 2017. Curran Associates Inc. ISBN
9781510860964.
Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blun-
dell, C., Mohamed, S., and Lerchner, A. Early visual
concept learning with unsupervised deep learning. CoRR ,
abs/1606.05579, 2016. URL http://arxiv.org/
abs/1606.05579 .
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot,
X., Botvinick, M., Mohamed, S., and Lerchner, A.
beta-vae: Learning basic visual concepts with a con-
strained variational framework. In 5th International
Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Pro-
ceedings , 2017. URL https://openreview.net/
forum?id=Sy2fzU9gl .Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., and Xing,
E. P. Toward controlled generation of text. In Pre-
cup, D. and Teh, Y . W. (eds.), Proceedings of the 34th
International Conference on Machine Learning , vol-
ume 70 of Proceedings of Machine Learning Research ,
pp. 1587–1596, International Convention Centre, Syd-
ney, Australia, 06–11 Aug 2017. PMLR. URL http://
proceedings.mlr.press/v70/hu17e.html .
Kingma, D. P. and Welling, M. Auto-encoding variational
bayes. arXiv preprint arXiv:1312.6114 , 2013.
Kingma, D. P., Mohamed, S., Jimenez Rezende, D., and
Welling, M. Semi-supervised learning with deep gener-
ative models. In Ghahramani, Z., Welling, M., Cortes,
C., Lawrence, N. D., and Weinberger, K. Q. (eds.), Ad-
vances in Neural Information Processing Systems 27 , pp.
3581–3589. Curran Associates, Inc., 2014.
Klys, J., Snell, J., and Zemel, R. Learning latent subspaces
in variational autoencoders. In Bengio, S., Wallach, H.,
Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Gar-
nett, R. (eds.), Advances in Neural Information Process-
ing Systems 31 , pp. 6444–6454. Curran Associates, Inc.,
2018.
Kumar, A., Sattigeri, P., and Balakrishnan, A. Variational
inference of disentangled latent concepts from unlabeled
observations. In 6th International Conference on Learn-
ing Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings ,
2018. URL https://openreview.net/forum?
id=H1kG7GZAW .
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gersh-
man, S. J. Building machines that learn and think like
people. CoRR , abs/1604.00289, 2016.
Lample, G., Zeghidour, N., Usunier, N., Bordes, A., De-
noyer, L., et al. Fader networks: Manipulating images
by sliding attributes. In Advances in Neural Information
Processing Systems , pp. 5967–5976, 2017.
LeCun, Y . The power and limits of deep learning. Re-
searchTechnology Management , 61(6):22–27, 2013. doi:
10.1080/08956308.2018.1516928.
Li, C. and Wand, M. Precomputed real-time texture synthe-
sis with markovian generative adversarial networks. In
European Conference on Computer Vision , pp. 702–716.
Springer, 2016.
Li, R., Li, X., Lin, C., Collinson, M., and Mao, R. A stable
variational autoencoder for text modelling. In Proceed-
ings of the 12th International Conference on Natural
Language Generation , pp. 594–599, 2019.

--- PAGE 11 ---
Latent Space Factorisation and Manipulation via Matrix Subspace Projection
Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning
face attributes in the wild. In Proceedings of the IEEE
international conference on computer vision , pp. 3730–
3738, 2015.
Perarnau, G., van de Weijer, J., Raducanu, B., and ´Alvarez,
J. M. Invertible conditional gans for image editing. CoRR ,
abs/1611.06355, 2016. URL http://arxiv.org/
abs/1611.06355 .
Sohn, K., Lee, H., and Yan, X. Learning structured output
representation using deep conditional generative models.
In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama,
M., and Garnett, R. (eds.), Advances in Neural Infor-
mation Processing Systems 28 , pp. 3483–3491. Curran
Associates, Inc., 2015.
Torfason, R., Agustsson, E., Rothe, R., and Timofte,
R. From face images and attributes to attributes. In
13th Asian Conference on Computer Vision, Taipei,
Taiwan, November 20-24 , 11 2016. doi: 10.1007/
978-3-319-54187-7 21.
Upchurch, P., Gardner, J. R., Pleiss, G., Pless, R., Snavely,
N., Bala, K., and Weinberger, K. Q. Deep feature in-
terpolation for image content changes. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 , pp.
6090–6099, 2017. doi: 10.1109/CVPR.2017.645. URL
https://doi.org/10.1109/CVPR.2017.645 .
Wu, P.-W., Lin, Y .-J., Chang, C.-H., Chang, E. Y ., and Liao,
S.-W. Relgan: Multi-domain image-to-image transla-
tion via relative attributes. In The IEEE International
Conference on Computer Vision (ICCV) , October 2019.
Xiao, Y ., Zhao, T., and Wang, W. Y . Dirichlet
variational autoencoder for text modeling. CoRR ,
abs/1811.00135, 2018. URL http://arxiv.org/
abs/1811.00135 .
Yan, X., Yang, J., Sohn, K., and Lee, H. Attribute2image:
Conditional image generation from visual attributes. In
Computer Vision - ECCV 2016 - 14th European Confer-
ence, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part IV , pp. 776–791, 2016. doi: 10.1007/
978-3-319-46493-0 n47. URL https://doi.org/
10.1007/978-3-319-46493-0_47 .
Yuille, A. L. and Liu, C. Deep nets: What have they ever
done for vision? CoRR , abs/1805.04025, 2018.
