# BYTE-LEVEL RECURSIVE CONVOLUTIONAL AUTO-ENCODER FOR TEXT
# Bộ Mã Hóa Tự Động Tích Chập Đệ Quy Cấp Byte Cho Văn Bản

Xiang Zhang
Courant Institute of Mathematical Sciences, New York University
Facebook AI Research, Facebook Inc.
xiang@cs.nyu.edu

Yann LeCun
Courant Institute of Mathematical Sciences, New York University
Center for Data Science, New York University
Facebook AI Research, Facebook Inc.
yann@cs.nyu.edu

## TÓM TẮT

Bài báo này đề xuất mã hóa tự động văn bản ở cấp độ byte sử dụng mạng tích chập với kiến trúc đệ quy. Động lực là khám phá liệu có thể có việc tạo sinh văn bản có thể mở rộng và đồng nhất ở cấp độ byte theo cách không tuần tự thông qua nhiệm vụ đơn giản của mã hóa tự động hay không. Chúng tôi chỉ ra rằng việc tạo sinh văn bản không tuần tự từ một biểu diễn có độ dài cố định không chỉ có thể thực hiện được, mà còn đạt được kết quả mã hóa tự động tốt hơn nhiều so với mạng hồi quy. Mô hình được đề xuất là khung mã hóa-giải mã tích chập sâu đa giai đoạn sử dụng kết nối dư (He et al., 2016), chứa tới 160 lớp có tham số. Mỗi bộ mã hóa hoặc giải mã chứa một nhóm mô-đun chia sẻ bao gồm các lớp gộp hoặc lớp tăng mẫu, làm cho mạng trở thành đệ quy về mặt mức độ trừu tượng trong biểu diễn. Kết quả cho 6 bộ dữ liệu đoạn văn quy mô lớn được báo cáo, bằng 3 ngôn ngữ bao gồm tiếng Ả Rập, tiếng Trung và tiếng Anh. Các phân tích được tiến hành để nghiên cứu một số tính chất của mô hình được đề xuất.

## 1 GIỚI THIỆU

Gần đây, việc tạo sinh văn bản sử dụng mạng tích chập (ConvNets) bắt đầu trở thành một lựa chọn thay thế cho mạng hồi quy trong học chuỗi-tới-chuỗi (Gehring et al., 2017). Giả định chủ đạo cho cả hai phương pháp này là văn bản được tạo sinh từng từ một. Quá trình tạo sinh tuần tự như vậy mang rủi ro của vấn đề biến mất hoặc bùng nổ đầu ra hoặc gradient (Bengio et al., 1994), điều này giới hạn độ dài của kết quả được tạo sinh. Giới hạn về khả năng mở rộng như vậy thúc đẩy chúng tôi khám phá liệu việc tạo sinh văn bản không tuần tự có khả thi hay không.

Trong khi đó, việc xử lý văn bản từ các cấp độ thấp hơn từ - như ký tự (Zhang et al., 2015) (Kim et al., 2016) và byte (Gillick et al., 2016) (Zhang & LeCun, 2017) - cũng đang được khám phá do tiềm năng trong việc xử lý các ngôn ngữ khác biệt theo cùng một cách. Đặc biệt, công trình của Zhang & LeCun (2017) cho thấy việc mã hóa one-hot đơn giản trên byte có thể cho kết quả tốt nhất cho phân loại văn bản trong nhiều ngôn ngữ khác nhau. Lý do là nó đạt được sự cân bằng tốt nhất giữa hiệu năng tính toán và độ chính xác phân loại. Được truyền cảm hứng từ những kết quả này, bài báo này khám phá mã hóa tự động cho văn bản sử dụng mạng tích chập cấp byte có cấu trúc đệ quy, như một bước đầu tiên hướng tới việc tạo sinh văn bản cấp thấp và không tuần tự.

Đối với nhiệm vụ mã hóa tự động văn bản, chúng ta nên tránh sử dụng các cơ chế chú ý phổ biến như những cơ chế được sử dụng trong dịch máy Bahdanau et al. (2015), bởi vì chúng luôn cung cấp đường dẫn thông tin trực tiếp cho phép bộ mã hóa tự động sao chép trực tiếp từ đầu vào. Điều này làm giảm mục đích nghiên cứu khả năng biểu diễn của các mô hình khác nhau. Do đó, tất cả các mô hình được xem xét trong bài báo này sẽ mã hóa thành và giải mã từ một biểu diễn vector có độ dài cố định.

Bài báo của Zhang et al. (2017) là một kết quả trước đó về việc sử dụng mạng tích chập cấp từ cho mã hóa tự động văn bản. Bài báo này khác biệt với nó ở một số cách chính trong việc sử dụng mạng tích chập. Trước hết, các mô hình của chúng tôi hoạt động từ cấp độ byte thay vì từ, điều này có thể làm cho bài toán trở nên thách thức hơn. Thứ hai, mạng của chúng tôi là động với cấu trúc đệ quy mà mở rộng theo độ dài của văn bản đầu vào, điều này theo thiết kế có thể tránh các giải pháp tầm thường cho mã hóa tự động như hàm đồng nhất. Thứ ba, bằng cách sử dụng các heuristics thiết kế mới nhất như kết nối dư (He et al., 2016), mạng của chúng tôi có thể mở rộng lên tới vài trăm lớp sâu, so với một mạng tĩnh chứa một vài lớp.

Trong bài báo này, một số tính chất của mô hình mã hóa tự động được nghiên cứu. Sau đây là danh sách.

1. Áp dụng mô hình cho 3 ngôn ngữ - tiếng Ả Rập, tiếng Trung và tiếng Anh - cho thấy mô hình có thể xử lý tất cả các ngôn ngữ khác nhau theo cùng một cách với độ chính xác tốt như nhau.

2. So sánh với bộ nhớ ngắn hạn dài (LSTM) (Hochreiter & Schmidhuber, 1997) cho thấy lợi thế đáng kể của việc sử dụng mạng tích chập cho mã hóa tự động văn bản.

3. Chúng tôi xác định rằng một bộ giải mã tích chập đệ quy như của chúng tôi có thể tạo ra chính xác byte kết thúc chuỗi, mặc dù quá trình giải mã là không tuần tự.

4. Bằng cách nghiên cứu lỗi mã hóa tự động khi các mẫu chứa byte nhiễu ngẫu nhiên, chúng tôi cho thấy mô hình không thoái hóa thành hàm đồng nhất. Tuy nhiên, nó cũng không thể khử nhiễu đầu vào rất tốt.

5. Cấu trúc đệ quy yêu cầu một lớp gộp. Chúng tôi so sánh giữa gộp trung bình, gộp L2 và gộp max, và xác định rằng gộp max là lựa chọn tốt nhất.

6. Lợi thế của đệ quy được thiết lập bằng so sánh với một mô hình tĩnh không có nhóm mô-đun chia sẻ. Điều này cho thấy các heuristics ngôn ngữ học như đệ quy hữu ích cho việc thiết kế mô hình cho xử lý ngôn ngữ.

7. Chúng tôi cũng khám phá các mô hình có kích thước khác nhau bằng cách thay đổi độ sâu mạng tối đa từ 40 đến 320. Kết quả cho thấy các mô hình sâu hơn cho kết quả tốt hơn.

## 2 BỘ MÃ HÓA TỰ ĐỘNG TÍCH CHẬP ĐỆ QUY CẤP BYTE

Trong phần này, chúng tôi giới thiệu thiết kế của mô hình mã hóa tự động tích chập với cấu trúc đệ quy. Mô hình bao gồm 6 nhóm mô-đun, với 3 cho bộ mã hóa và 3 cho bộ giải mã. Mô hình đầu tiên mã hóa một đầu vào có độ dài biến đổi thành một vector có độ dài cố định kích thước 1024, sau đó giải mã trở lại cùng độ dài đầu vào. Kiến trúc bộ giải mã là một gương ngược của bộ mã hóa. Tất cả các lớp tích chập trong bài báo này đều có zero-padding được thêm vào để đảm bảo rằng mỗi lớp tích chập xuất ra cùng độ dài với đầu vào. Chúng cũng đều có kích thước đặc trưng 256 và kích thước kernel 3. Tất cả các lớp có tham số trong mô hình của chúng tôi sử dụng ReLU (Nair & Hinton, 2010) làm phi tuyến tính.

Trong bộ mã hóa, nhóm mô-đun đầu tiên bao gồm n lớp tích chập thời gian (1-D). Nó chấp nhận một chuỗi byte được mã hóa one-hot làm đầu vào, trong đó mỗi byte được mã hóa như một vector 256 chiều. Nhóm mô-đun đầu tiên này biến đổi đầu vào thành một biểu diễn nội bộ. Chúng tôi gọi nhóm mô-đun này là nhóm tiền tố. Nhóm mô-đun thứ hai bao gồm n lớp tích chập thời gian cộng với một lớp max-pooling kích thước 2. Nhóm này giảm độ dài đầu vào theo hệ số 2, và nó có thể được áp dụng lại và lại để giảm đệ quy độ dài biểu diễn. Do đó, chúng tôi đặt tên nhóm thứ hai này là nhóm đệ quy. Nhóm đệ quy được áp dụng cho đến khi kích thước biểu diễn trở thành 1024, thực chất là một đặc trưng có chiều 256 và độ dài 4. Sau đó, tiếp theo nhóm đệ quy cuối cùng là một nhóm hậu tố gồm n lớp tuyến tính cho biến đổi đặc trưng.

Bộ giải mã là một gương ngược đối xứng của bộ mã hóa. Nhóm tiền tố bộ giải mã bao gồm n lớp tuyến tính, tiếp theo là một nhóm đệ quy bộ giải mã mở rộng độ dài biểu diễn theo hệ số 2. Việc mở rộng này được thực hiện ở lớp tích chập đầu tiên của nhóm này, nơi nó xuất ra 512 đặc trưng sẽ được định hình lại thành 256 đặc trưng. Quá trình định hình lại mà chúng tôi sử dụng đảm bảo rằng các giá trị đặc trưng tương ứng với trường nhìn gần kề trong đầu vào, điều này tương tự với ý tưởng của tích chập sub-pixel (hoặc xáo trộn pixel) (Shi et al., 2016). Hình 3 mô tả quá trình định hình lại này để biến đổi biểu diễn có kích thước đặc trưng 4 và độ dài 8 thành kích thước đặc trưng 2 và độ dài 16. Sau khi nhóm đệ quy này được áp dụng nhiều lần (giống như của nhóm đệ quy bộ mã hóa), một nhóm hậu tố bộ giải mã gồm n lớp tích chập được áp dụng để giải mã các đặc trưng đệ quy thành một chuỗi byte.

Đầu ra cuối cùng của bộ giải mã được hiểu là xác suất của byte sau khi qua hàm softmax. Do đó, hàm mất mát chúng tôi sử dụng đơn giản là negative-log likelihood trên các đầu ra softmax riêng lẻ. Đáng chú ý rằng điều này không ngụ ý rằng các byte đầu ra độc lập vô điều kiện với nhau. Đối với bộ giải mã văn bản không tuần tự của chúng tôi, sự độc lập giữa các byte đầu ra được điều kiện hóa bởi biểu diễn từ bộ mã hóa, có nghĩa là sự phụ thuộc lẫn nhau của chúng được mô hình hóa bởi chính bộ giải mã. Hình 2 minh họa sự khác biệt giữa việc tạo sinh văn bản tuần tự và không tuần tự sử dụng mô hình đồ thị.

Tùy thuộc vào độ dài đầu vào và kích thước của biểu diễn được mã hóa, mô hình của chúng tôi có thể cực kỳ sâu. Ví dụ, với n = 8 và chiều mã hóa 1024 (giảm xuống độ dài 4 với 256 đặc trưng), cho một mẫu có độ dài 1024 byte, toàn bộ mô hình có 160 lớp có tham số. Huấn luyện một mô hình động sâu như vậy có thể rất thách thức khi sử dụng gradient descent ngẫu nhiên (SGD) do vấn đề biến mất gradient (Bengio et al., 1994). Do đó, chúng tôi sử dụng ý tưởng được đề xuất gần đây về kết nối dư (He et al., 2016) để làm cho tối ưu hóa dễ dàng hơn. Đối với mỗi cặp lớp có tham số liền kề, biểu diễn đặc trưng đầu vào được truyền qua đến đầu ra bằng phép cộng. Chúng tôi không thể huấn luyện mô hình được thiết kế theo cách này mà không có các kết nối dư như vậy.

Đối với tất cả các mô hình của chúng tôi, chúng tôi sử dụng biểu diễn được mã hóa có chiều 1024 (đệ quy xuống độ dài 4 với 256 đặc trưng). Đối với một mẫu đầu vào có độ dài tùy ý l, chúng tôi đầu tiên thêm byte null kết thúc chuỗi vào nó, và sau đó đệm nó đến độ dài 2^⌈log₂(l+1)⌉ với tất cả vector không. Điều này làm cho độ dài đầu vào là một lũy thừa cơ số 2 của một số nguyên nào đó, vì các nhóm đệ quy trong cả bộ mã hóa và giải mã đều giảm hoặc mở rộng độ dài biểu diễn theo hệ số 2. Nếu l < 4, nó được đệm đến kích thước 4 và không đi qua các nhóm đệ quy. Dễ thấy rằng độ sâu của mạng động này cho một mẫu có độ dài l theo thứ tự log₂l, có khả năng làm cho các biểu diễn ẩn hiệu quả hơn và dễ học hơn so với mạng hồi quy có thứ tự tuyến tính về độ sâu.

## 3 KẾT QUẢ CHO MÃ HÓA TỰ ĐỘNG ĐA NGÔN NGỮ

Trong phần này, chúng tôi trình bày kết quả của bộ mã hóa tự động tích chập đệ quy cấp byte.

### 3.1 BỘ DỮ LIỆU

Tất cả các bộ dữ liệu của chúng tôi đều ở cấp độ đoạn văn. Tiền xử lý tối thiểu được áp dụng cho chúng vì mô hình của chúng tôi có thể được áp dụng cho tất cả ngôn ngữ theo cùng một cách. Chúng tôi cũng xây dựng một bộ dữ liệu với các mẫu được trộn lẫn trong cả ba ngôn ngữ để kiểm tra khả năng xử lý dữ liệu đa ngôn ngữ của mô hình.

**enwiki.** Bộ dữ liệu này chứa các đoạn văn từ Wikipedia tiếng Anh, được xây dựng từ bản dump ngày 1 tháng 6 năm 2016. Chúng tôi có thể thu được 8,484,895 bài viết, và sau đó chia 7,634,438 cho huấn luyện và 850,457 cho kiểm tra. Số lượng đoạn văn cho huấn luyện và kiểm tra do đó là 41,256,261 và 4,583,893 tương ứng.

**hudong.** Bộ dữ liệu này chứa các đoạn văn từ trang web bách khoa toàn thư Trung Quốc baike.com. Chúng tôi thu thập 1,799,095 mục bài viết từ đó và sử dụng 1,618,817 cho huấn luyện và 180,278 cho kiểm tra. Số lượng đoạn văn cho huấn luyện và kiểm tra là 53,675,117 và 5,999,920.

**argiga.** Bộ dữ liệu này chứa các đoạn văn từ phiên bản thứ năm Arabic Gigaword (Parker et al., 2011a), là tập hợp các bài báo thông tấn tiếng Ả Rập. Tổng cộng có 3,346,167 bài viết, và chúng tôi sử dụng 3,011,403 cho huấn luyện và 334,764 cho kiểm tra. Kết quả là, chúng tôi có 27,989,646 đoạn văn cho huấn luyện và 3,116,719 cho kiểm tra.

**engiga.** Bộ dữ liệu này chứa các đoạn văn từ phiên bản thứ năm English Gigaword (Parker et al., 2011c), là tập hợp các bài báo thông tấn tiếng Anh. Tổng cộng có 9,876,096 bài viết, và chúng tôi sử dụng 8,887,583 cho huấn luyện và 988,513 cho kiểm tra. Kết quả là, chúng tôi có 116,456,520 đoạn văn cho huấn luyện và 12,969,170 cho kiểm tra.

**zhgiga.** Bộ dữ liệu này chứa các đoạn văn từ phiên bản thứ năm Chinese Gigaword (Parker et al., 2011b), là tập hợp các bài báo thông tấn tiếng Trung. Tổng cộng có 5,664,377 bài viết, và chúng tôi sử dụng 5,097,198 cho huấn luyện và 567,179 cho kiểm tra. Kết quả là, chúng tôi có 38,094,390 đoạn văn cho huấn luyện và 4,237,643 cho kiểm tra.

**allgiga.** Vì ba bộ dữ liệu Gigaword rất tương tự nhau, chúng tôi kết hợp chúng để tạo thành một bộ dữ liệu đa ngôn ngữ về các đoạn văn bài báo thông tấn. Trong bộ dữ liệu này, có 18,886,640 bài viết với 16,996,184 cho huấn luyện và 1,890,456 cho kiểm tra. Số lượng đoạn văn cho huấn luyện và kiểm tra là 182,540,556 và 20,323,532 tương ứng.

Bảng 1 là tóm tắt của các bộ dữ liệu này. Đối với các bộ dữ liệu lớn như vậy, thời gian kiểm tra có thể không thể chấp nhận được. Do đó, chúng tôi báo cáo tất cả kết quả dựa trên 1,000,000 mẫu được lấy mẫu ngẫu nhiên từ tập con huấn luyện hoặc kiểm tra tùy thuộc vào tình huống. Rất ít overfitting được quan sát ngay cả đối với mô hình lớn nhất của chúng tôi.

### 3.2 KẾT QUẢ

Bất kể bộ dữ liệu nào, tất cả các bộ mã hóa tự động văn bản của chúng tôi đều được huấn luyện với cùng siêu tham số sử dụng gradient descent ngẫu nhiên (SGD) với momentum (Polyak, 1964) (Sutskever et al., 2013). Mô hình chúng tôi sử dụng có n = 8 - nghĩa là có 8 lớp có tham số trong mỗi nhóm mô-đun tiền tố, đệ quy và hậu tố, cho cả bộ mã hóa và giải mã. Mỗi epoch huấn luyện chứa 1,000,000 bước, và mỗi bước được huấn luyện trên một mẫu được chọn ngẫu nhiên với độ dài lên tới 1024 byte. Do đó, độ sâu mô hình tối đa là 160. Chúng tôi chỉ lan truyền ngược qua các byte hợp lệ trong đầu ra. Lưu ý rằng mỗi mẫu chứa một byte kết thúc chuỗi ("byte null") theo thiết kế.

Chúng tôi đặt tốc độ học ban đầu là 0.001, và giảm một nửa mỗi 10 epoch. Momentum 0.9 được áp dụng để tăng tốc huấn luyện. Weight decay nhỏ 0.00001 được sử dụng để ổn định huấn luyện. Tùy thuộc vào độ dài của mỗi mẫu, các nhóm đệ quy bộ mã hóa hoặc giải mã được áp dụng một số lần nhất định. Chúng tôi thấy rằng việc chia gradient của các nhóm đệ quy này cho số lượng bản sao chia sẻ có thể tăng tốc huấn luyện. Quá trình huấn luyện dừng ở epoch thứ 100.

Lưu ý rằng vì các bộ dữ liệu engiga và allgiga có hơn 100,000,000 mẫu huấn luyện, khi huấn luyện dừng, mô hình chưa thấy toàn bộ dữ liệu huấn luyện. Tuy nhiên, huấn luyện thêm không đạt được cải thiện nào có thể quan sát được.

Bảng 2 chi tiết các lỗi cấp byte cho mô hình của chúng tôi trên tất cả các bộ dữ liệu đã nêu. Những kết quả này chỉ ra rằng các mô hình của chúng tôi có thể đạt được tỷ lệ lỗi rất tốt cho mã hóa tự động trong các ngôn ngữ khác nhau. Kết quả cho bộ dữ liệu allgiga cũng chỉ ra rằng mô hình không gặp khó khăn trong việc học từ các bộ dữ liệu đa ngôn ngữ chứa các mẫu của các ngôn ngữ rất khác nhau.

## 4 THẢO LUẬN

Phần này cung cấp so sánh với mạng hồi quy, và nghiên cứu về một tập hợp các tính chất khác nhau của mô hình mã hóa tự động được đề xuất. Hầu hết các kết quả này được thực hiện sử dụng bộ dữ liệu enwiki.

### 4.1 SO SÁNH VỚI MẠNG HỒI QUY

Chúng tôi xây dựng một mạng hồi quy cơ sở đơn giản sử dụng các đơn vị bộ nhớ ngắn hạn dài "vanilla" (Hochreiter & Schmidhuber, 1997). Trong mô hình này, cả byte đầu vào và đầu ra đều được nhúng vào vector có chiều 1024 để chúng tôi có thể sử dụng biểu diễn ẩn có chiều 1024. Bộ mã hóa đọc văn bản theo thứ tự ngược, điều này đã được Sutskever et al. (2014) quan sát là việc đảo ngược chuỗi đầu vào có thể cải thiện chất lượng đầu ra. Đầu ra ẩn 1024 chiều của cell cuối cùng được sử dụng làm đầu vào cho bộ giải mã.

Bộ giải mã cũng có byte đầu vào và đầu ra được nhúng vào vector có chiều 1024 và sử dụng biểu diễn ẩn có chiều 1024. Trong quá trình giải mã, byte được tạo sinh gần đây nhất được đưa vào bước thời gian tiếp theo. Điều này được gọi là "teacher forcing" được quan sát là cải thiện kết quả mã hóa tự động trong trường hợp của chúng tôi. Quá trình giải mã sử dụng thuật toán tìm kiếm chùm tia kích thước 2. Trong quá trình học, chúng tôi chỉ lan truyền ngược qua chuỗi có khả năng cao nhất sau tìm kiếm chùm tia.

Bảng 3 chi tiết kết quả cho LSTM. Các lỗi cấp byte lớn đến mức kết quả của các mô hình của chúng tôi trong bảng 2 tốt hơn ít nhất một bậc độ lớn. Giới hạn cơ bản của mạng hồi quy là bất kể cấp độ thực thể (từ, ký tự hoặc byte), chúng có thể nhớ chính xác khoảng tới 50 trong số chúng, và sau đó thất bại trong việc dự đoán chính xác chúng. Theo cấu trúc, quá trình tạo sinh văn bản đệ quy không tuần tự của chúng tôi có thể hy vọng là một giải pháp thay thế cho điều này, như đã rõ ràng trong kết quả ở đây.

### 4.2 KẾT THÚC CHUỖI

Một điều làm nên sự khác biệt giữa việc tạo sinh văn bản tuần tự và không tuần tự là cách quyết định khi nào kết thúc chuỗi byte được tạo sinh. Đối với quá trình tạo sinh tuần tự như bộ giải mã hồi quy, chúng ta có thể dừng khi một ký hiệu kết thúc chuỗi nào đó được tạo sinh. Đối với quá trình tạo sinh không tuần tự, chúng ta có thể coi ký hiệu kết thúc chuỗi đầu tiên gặp phải là dấu hiệu kết thúc, mặc dù nó sẽ không thể tránh khỏi việc tạo sinh một số ký hiệu thêm sau nó. Sau đó, một câu hỏi tự nhiên cần hỏi là, cách đơn giản này để xác định kết thúc chuỗi có hiệu quả không?

Để trả lời câu hỏi này, chúng tôi tính toán sự khác biệt của các ký hiệu kết thúc chuỗi giữa văn bản được tạo sinh và sự thật cơ sở của nó cho 1,000,000 mẫu, cho cả tập con huấn luyện và kiểm tra của bộ dữ liệu enwiki. Điều chúng tôi khám phá ra là phân phối của sự khác biệt độ dài tập trung cao ở 0, ở 99.63% cho cả huấn luyện và kiểm tra. Hình 4 cho thấy biểu đồ đầy đủ, trong đó các sự khác biệt độ dài khác 0 hầu như không thể nhìn thấy. Điều này gợi ý rằng quá trình tạo sinh văn bản không tuần tự của chúng tôi có thể mô hình hóa vị trí kết thúc chuỗi khá chính xác. Một lý do cho điều này là đối với mỗi mẫu chúng tôi có một ký hiệu kết thúc chuỗi - "byte null" - để mạng đã học mô hình hóa nó khá sớm trong quá trình huấn luyện.

### 4.3 HOÁN VỊ NGẪU NHIÊN CỦA CÁC MẪU

Một vấn đề tiềm ẩn cụ thể cho nhiệm vụ mã hóa tự động là rủi ro học giải pháp thoái hóa - hàm đồng nhất. Một cách để kiểm tra điều này là biến đổi các byte đầu vào một cách ngẫu nhiên và xem liệu tỷ lệ lỗi có khớp với xác suất biến đổi hay không. Chúng tôi thử nghiệm với xác suất biến đổi từ 0 đến 1 với khoảng cách 0.1, và cho mỗi trường hợp chúng tôi kiểm tra các lỗi cấp byte cho 100,000 mẫu trong cả tập con huấn luyện và kiểm tra của bộ dữ liệu enwiki.

Lưu ý rằng chúng ta có thể tính toán các lỗi cấp byte theo 2 cách. Cách đầu tiên là tính toán các lỗi với respect to các mẫu sự thật cơ sở. Nếu giải pháp thoái hóa thành hàm đồng nhất, thì các lỗi cấp byte nên tương quan với xác suất biến đổi. Cách thứ hai là tính toán các lỗi với respect to các mẫu đã biến đổi. Nếu giải pháp thoái hóa thành hàm đồng nhất, thì các lỗi cấp byte nên gần 0 bất kể xác suất biến đổi. Hình 5 cho thấy kết quả theo 2 cách tính toán lỗi này, và kết quả chỉ ra mạnh mẽ rằng mô hình không thoái hóa thành việc học hàm đồng nhất.

Đáng chú ý rằng các lỗi với respect to các mẫu sự thật cơ sở trong hình 5 cũng chứng minh rằng mô hình của chúng tôi thiếu khả năng khử nhiễu các mẫu đã biến đổi. Điều này có thể thấy từ hiện tượng rằng các lỗi cho mỗi xác suất biến đổi cao hơn giá trị đường chéo tham chiếu, thay vì thấp hơn. Điều này do thiếu tiêu chí khử nhiễu trong quá trình huấn luyện của chúng tôi.

### 4.4 ĐỘ DÀI MẪU

Chúng tôi cũng tiến hành thí nghiệm để cho thấy các lỗi cấp byte thay đổi như thế nào với respect to độ dài mẫu. Hình 7 cho thấy biểu đồ của độ dài mẫu cho tất cả bộ dữ liệu. Nó chỉ ra rằng đa số các mẫu đoạn văn có thể được mô hình hóa tốt dưới 1024 byte. Hình 6 cho thấy lỗi cấp byte của các mô hình của chúng tôi với respect to độ dài của các mẫu. Hình này được tạo ra bằng cách kiểm tra 1,000,000 mẫu từ mỗi tập con huấn luyện và kiểm tra của bộ dữ liệu enwiki. Mỗi bin trong biểu đồ đại diện cho một phạm vi 64 với giới hạn trên được chỉ ra. Ví dụ, lỗi tại 512 chỉ ra các lỗi được tổng hợp cho các mẫu có độ dài 449 đến 512.

Một hiện tượng thú vị là các lỗi có tương quan cao với số lượng nhóm đệ quy được áp dụng cho cả bộ mã hóa và bộ giải mã. Trong biểu đồ, các bin 64, 128, 192-256, 320-512, 576-1024 đại diện cho các mức đệ quy 4, 5, 6, 7, 8 tương ứng. Các lỗi cho cùng mức đệ quy gần như giống nhau, mặc dù có sự khác biệt độ dài lớn khi các mức đệ quy trở nên sâu. Lý do cho điều này cũng liên quan đến thực tế rằng có xu hướng có nhiều văn bản ngắn hơn văn bản dài hơn trong bộ dữ liệu, như được chứng minh trong hình 7.

### 4.5 CÁC LỚP GỘPING

Phần này chi tiết một thí nghiệm trong việc nghiên cứu các lỗi huấn luyện và kiểm tra thay đổi như thế nào với sự lựa chọn các lớp gộp trong mạng bộ mã hóa. Các thí nghiệm được tiến hành trên mô hình đã đề cập với n = 8, và thay thế lớp max-pooling trong bộ mã hóa bằng các lớp average-pooling hoặc L2-pooling. Bảng 4 chi tiết kết quả. Các con số chỉ ra mạnh mẽ rằng max-pooling là lựa chọn tốt nhất. Max-pooling chọn các giá trị lớn nhất trong trường nhìn của nó, giúp mạng đạt được optima tốt hơn (Boureau et al., 2010).

### 4.6 ĐỆ QUY

Việc sử dụng đệ quy trong mô hình được đề xuất xuất phát từ trực giác ngôn ngữ học rằng cấu trúc có thể giúp mô hình học các biểu diễn tốt hơn. Tuy nhiên, không có đảm bảo rằng trực giác như vậy có thể hữu ích cho mô hình trừ khi so sánh được thực hiện với một mô hình tĩnh nhận đầu vào có độ dài cố định và đi qua một mạng có cùng kiến trúc của các nhóm đệ quy mà không có chia sẻ trọng số.

Hình 8 cho thấy các lỗi huấn luyện và kiểm tra khi huấn luyện một mô hình tĩnh với cùng siêu tham số. Mô hình tĩnh nhận 1024 byte, và vector không được đệm nếu độ dài mẫu nhỏ hơn. Nhóm đệ quy do đó được áp dụng 8 lần trong cả bộ mã hóa và giải mã, mặc dù trọng số của chúng không được chia sẻ. Kết quả chỉ ra rằng một mô hình đệ quy không chỉ học nhanh hơn, mà còn có thể đạt được kết quả tốt hơn. Bảng 5 liệt kê các lỗi cấp byte.

### 4.7 ĐỘ SÂU MÔ HÌNH

Phần này khám phá liệu việc thay đổi kích thước mô hình có thể tạo ra sự khác biệt về kết quả hay không. Bảng 6 liệt kê các lỗi huấn luyện và kiểm tra của các độ sâu mô hình khác nhau với n ∈ {2,4,8,16}. Kết quả chỉ ra rằng tỷ lệ lỗi tốt nhất được đạt được với mô hình lớn nhất, với rất ít overfitting. Điều này một phần do thực tế rằng các bộ dữ liệu của chúng tôi khá lớn cho các mô hình được đề cập.

## 5 KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất mã hóa tự động văn bản sử dụng mạng tích chập đệ quy. Mô hình chứa 6 phần - 3 cho bộ mã hóa và 3 cho bộ giải mã. Bộ mã hóa và bộ giải mã đều chứa một nhóm mô-đun tiền tố và một nhóm mô-đun hậu tố cho biến đổi đặc trưng. Một nhóm mô-đun đệ quy được bao gồm ở giữa tiền tố và hậu tố cho mỗi bộ mã hóa và giải mã, nhóm này đệ quy thu nhỏ hoặc mở rộng độ dài biểu diễn. Kết quả là, mô hình của chúng tôi về bản chất tạo sinh văn bản theo cách không tuần tự.

Các thí nghiệm sử dụng mô hình này được thực hiện trên 6 bộ dữ liệu quy mô lớn bằng tiếng Ả Rập, tiếng Trung và tiếng Anh. So sánh với mạng hồi quy được cung cấp để cho thấy mô hình của chúng tôi đạt được kết quả tuyệt vời trong mã hóa tự động văn bản. Các tính chất của mô hình được đề xuất được nghiên cứu, bao gồm khả năng tạo ra ký hiệu kết thúc chuỗi, liệu mô hình có thoái hóa thành hàm đồng nhất hay không, và các biến thể của các lớp gộp, đệ quy và độ sâu của mô hình. Trong tương lai, chúng tôi hy vọng mở rộng các mô hình của mình thành các mô hình tạo sinh không tuần tự mà không có đầu vào, và sử dụng nó cho nhiều nhiệm vụ chuỗi-tới-chuỗi hơn như dịch máy.

## TÀI LIỆU THAM KHẢO

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.

Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. Trans. Neur. Netw., 5(2):157–166, March 1994. ISSN 1045-9227. doi: 10.1109/72.279181. URL http://dx.doi.org/10.1109/72.279181.

Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling in visual recognition. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 111–118, 2010.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional Sequence to Sequence Learning. In Proc. of ICML, 2017.

Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Subramanya. Multilingual language processing from bytes. In Proceedings of NAA-HLT, pp. 1296–1306, 2016.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997.

Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.

Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807–814, 2010.

Robert Parker, David Graff, Ke Chen, Junbo Kong, and Kazuaki Maeda. Arabic gigaword fifth edition ldc2011t11, 2011a. Web Download.

Robert Parker, David Graff, Ke Chen, Junbo Kong, and Kazuaki Maeda. Chinese gigaword fifth edition ldc2011t13, 2011b. Web Download.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition ldc2011t07, 2011c. Web Download.

B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1 – 17, 1964. ISSN 0041-5553.

Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1874–1883, 2016.

Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139–1147, 2013.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014.

Xiang Zhang and Yann LeCun. Which encoding is the best for text classification in chinese, english, japanese and korean? CoRR, abs/1708.02657, 2017.

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28, pp. 649–657. 2015.

Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, and Lawrence Carin. Deconvolutional paragraph representation learning. CoRR, abs/1708.04729, 2017.
