# 2311.10638.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/autoencoder/2311.10638.pdf
# File size: 646418 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
Jingyun Feng1Lin Zhang2Lili Yang1
Abstract
In disentangled representation learning, the goal
is to achieve a compact representation that con-
sists of all interpretable generative factors in the
observational data. Learning disentangled repre-
sentations for graphs becomes increasingly im-
portant as graph data rapidly grows. Existing ap-
proaches often rely on Variational Auto-Encoder
(V AE) or its causal structure learning-based refine-
ment, which suffer from sub-optimality in V AEs
due to the independence factor assumption and
unavailability of concept labels, respectively. In
this paper, we propose an unsupervised solution,
dubbed concept-free causal disentanglement, built
on a theoretically provable tight upper bound ap-
proximating the optimal factor. This results in an
SCM-like causal structure modeling that directly
learns concept structures from data. Based on
this idea, we propose Concept-free Causal VGAE
(CCVGAE) by incorporating a novel causal dis-
entanglement layer into Variational Graph Auto-
Encoder. Furthermore, we prove concept consis-
tency under our concept-free causal disentangle-
ment framework, hence employing it to enhance
the meta-learning framework, called concept-free
causal Meta-Graph (CC-Meta-Graph). We con-
duct extensive experiments to demonstrate the su-
periority of the proposed models: CCVGAE and
CC-Meta-Graph, reaching up to 29% and11%
absolute improvements over baselines in terms of
AUC, respectively.
1. Introduction
Graph data becomes ubiquitous, in both natural and human-
made scenarios, along with the rise of deep learning, gain-
ing increasing attention and making learning on graphs an
emerging research field, with the goal of understanding
graphs and dealing with downstream applications, such as
1Department of Statistics and Data Science, Southern Univer-
sity of Science and Technology, Shenzhen, Guangdong, China.
2Gongsheng Matrix, Shenzhen, China.
????‡∑®ùêÜùêß
 G
Sampling
Adapt MergeEncoder
DecoderCausal
ùêÄ,ùêó
ùêÄ‚Ä≤,ùêó‚Ä≤Œµ
Converge
Ground Truth Concepts ·à∑Z
Approximated ConceptsGenerate
Learning in Sub-graphs
ùêÜ
ùêß
ùêÜ
ùüè
ùêÜ
ùüê
Figure 1. An illustration of our proposed ideas. The left component
(shown in light green) demonstrates a causal disentangle process
in a single graph, i.e., CCVGAE which takes an adjacency matrix
and node features as inputs. The right-hand component shows the
consistency property of generative factors obtained by CCVGAE.
This property implies that merging generative factors from other
graphs can be adapted to others, and thus leads to an extension of
CCVGAE, called CC-Meta-Graph. Here, we assume all graphs
are sampled from the same graph (i.e., G).ÀúGnrepresents the
approximated concepts by merging the individual concepts ( Gi)
obtained from the samples ( Gi).
drug discovery (You et al., 2018), traffic forecasting (Jiang
& Luo, 2022), recommender systems (Wu et al., 2020), and
others (Zhou et al., 2020). Of particular importance is graph
representation learning (Hamilton, 2020), but it remains an
outstanding research problem due to graphs‚Äô non-IID and
non-Euclidean properties. There is growing attention to the
disentanglement learning to address this problem.
The goal of disentanglement learning is to acquire the repre-
sentations that capture all interpretable generative factors,
called disentangled representations (Bengio et al., 2013;
Higgins et al., 2018). A significant challenge of disentangle-
ment learning is that we often only have raw observations
while not allowing any supervision on generative factors
(i.e., causes) (Kumar et al., 2017). Earlier attempts (Paige
et al., 2017; Yang et al., 2021) often demand adequate la-
bels for training and hence can not fit the above realistic
setting. This motivates us to focus on the unsupervisedarXiv:2311.10638v1  [cs.LG]  17 Nov 2023

--- PAGE 2 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
setting. Recent advances in unsupervised disentanglement
learning have mostly focused on Variational Auto-encoders
(V AEs) (Li & Mandt, 2018) and Generative Adversarial
Networks (GANs) (Kocaoglu et al., 2017). In particular,
the V AE framework is preferred in graphs because of its
stability in contrast to mode collapse in GANs due to its
implicit modeling of the distribution, which is especially dif-
ficult to learn the distribution of graphs. So in this work, our
focus is on the V AE framework to explore disentanglement
for graph representation learning, i.e., Variational Graph
Auto-Encoders (VGAE) (Kipf & Welling, 2016a).
Despite the recent growth of disentanglement learning, most
state-of-the-art methods within the V AE framework have
assumed that the distributions in the hidden space are in-
dependent Gaussian (Kim & Mnih, 2018) and thus lead to
suboptimal solutions (Tr ¬®auble et al., 2021). Studies (Lo-
catello et al., 2020; Trauble et al., 2020) have shown that
disentanglement of representations is nearly impossible un-
der the independent assumption when the data demonstrates
intrinsic correlations. In contrast, modeling the structure for
underlying factors enhances disentanglement, particularly
causal structure learning (Sch ¬®olkopf & von K ¬®ugelgen, 2022).
However, when leveraging the V AE framework, there is no
adequate research on the optimal solution while imposing a
causal structure on the latent factors.
In this paper, we attempt to address unsupervised causal dis-
entangled representation learning in the VGAE framework,
including theoretical analysis and practical methodologies.
We prove a tight upper bound on approximating the optimal
latent factor via causal structure learning. It indicates that a
linear causal modeling function can approximate the opti-
mal latent factor with high confidence. With this, we then
develop a practical causal disentanglement method without
requiring concept labels, called concept-free causal disen-
tanglement. In this way, we achieve a data-driven causal
structure modeling that directly learns concept structures
from data. Building on this, we introduce a novel causal
disentanglement layer and then integrate it with VGAE, re-
sulting in our first model, called Concept-free Causal VGAE
(CCV AGE). Besides, we uncover the consistency of our ob-
tained concepts due to the data-driven style, making them
suitable for capturing underlying global information with
little data. Towards this, we propose a meta-learning model
that transfers global-aware concepts to newly arrived data,
resulting in our second model, called CC-Meta-Graph. In
Figure 1, we present an illustration of the proposed ideas.
We highlight the contributions of this paper:
1, In this paper, we theoretically prove a tight bound on the
approximation of optimal factors and offer a practical causal
disentanglement method on top of it, called concept-free
causal disentanglement.
2, We propose two causal disentanglement-enhanced mod-els: one is to support causal disentanglement in VGAE, and
the other is to validate the proposed consistency property.
3, We conduct extensive experiments with synthetic and
real-world graph data to demonstrate the efficiency of our
proposed models in terms of link prediction, achieving up
to29% and11% absolute improvements for CCVGAE and
CC-Meta-Graph, respectively.1
2. Related work
Disentanglement Learning. The concept of disentangle-
ment was first introduced by Bengio et al. (2013) as a
property of representation and its formal definition Higgins
et al. (2018) is: if a representation can be decomposed into
several independent features, which means only one of these
features will change when change one factor of data input,
then we call it ‚Äùdisentangled representation‚Äù . Some stud-
ies (Eastwood & Williams, 2018) consider a more rigorous
definition that only if each dimension of the representation
can capture at most one true generative factor, we can call
this representation a ‚Äùdisentangle representation‚Äù. In order
to encourage potential factors to learn disentangled repre-
sentations while optimizing the inherent task objectives,
disentangled representation learning is designed to capture
interpretable, controllable and robust representations.
In graph disentangled representation learning, most frame-
works are GNN-based. DisenGCN (Ma et al., 2019) utilizes
neighbourhood routing to identify the latent factor that may
caused edges, FactorGCN (Yang et al., 2020) disentangle
graph into several sub-graphs, each sub-graph represent
graph composed of one type of edges. However, Fan et al.
(2022) noticed that GNNs always suffer from spurious cor-
relation, even if the causal correlation always exists. They
proposed DisC to learn causal substructure and bias sub-
structure. DisC requires some of input graph nodes represent
concepts. However, such graphs are always unavailable in
real world.
Most typical disentangled representation learning meth-
ods are generative models, especially Variational Auto-
Encoder(V AE) (Higgins et al., 2016; Kumar et al., 2017;
Yang et al., 2021; Zhu et al., 2021). V AE use a variational
posterior q(z|x)to approximate the unknown true posterior
p(z|x).To obtain better disentanglement ability, researchers
design various extra regularizers based on the original V AE
loss function. A penalty coefficient is introduced to ELBO
loss by Œ≤-vae (Higgins et al., 2016) to strengthen the inde-
pendence constraint of the variational posterior distribution
q(z|x). FactorV AE (Kim & Mnih, 2018) imposes indepen-
dence constraint according to the definition of independence.
However, better disentanglement ability often leads to more
1The experiments code can be found in
https ://www.dropbox.com/sh/c 8nd1qbpb 20ling/AABhhj
rlRGOF 4X5h‚àíosw 0aza?dl= 0

--- PAGE 3 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
reconstruction errors. To balance the trade-off between
reconstruction and disentanglement, Burgess et al. (2018)
proposes a simple modification based on Œ≤-V AE, making
the quality of disentanglement can be improved as much as
possible without too much reconstruction error. However,
we believe that the conflict between reconstruction error and
disentanglement quality does not naturally exist but from the
improper disentanglement such as independent assumption
as follows. Higgins et al. (2018) assumed that the gener-
ating factors were natural and independent in disentangled
representation learning.
However, Suter et al. (2019) disagreed with the indepen-
dence assumption. They assumed that the generating factors
of the observable data are causally influenced by the group
of confounding factors, and first introduced SCM (Kra-
jewski & Matthews, 2010) to describe causal relationships
among generating factors. Tr ¬®auble et al. (2021) suggested
that, if some generating factors are correlated in the data
set, methods based on independent assumption might have
a bias against disentanglement. Other researchers (Yang
et al., 2021; Shen et al., 2022) have also taken experiments
of disentanglement learning in real-world data based on
the assumption that the real-world data is not generated by
independent factors. Here, we also follow the same assump-
tion that generates factors are not independent and even
believe there are underlining causal relationships among
these factors.
Causal Disentanglement. Over the past decades, many
researchers (Hoyer et al., 2008; Zhang & Hyvarinen, 2012;
Shimizu et al., 2006) have paid attention to the discovery
of causality from observational data. With the development
of disentanglement learning, the community has raised the
interest in combining causality and disentangled represen-
tation. Kocaoglu et al. (2017) proposed a method called
CausalGAN which supports ‚Äùdo-operation‚Äù on images but it
requires the causal graph given as a prior. Suter et al. (2019)
believed that the underlying causal generative process will
impact the level of disentanglement, and firstly proposed
the definition of the causal disentanglement process.
Yang et al. (2021) is the first to implement the causal disen-
tanglement process proposed by Suter et al. (2019), called
CausalV AE. However, their method is semi-supervised be-
cause they require labels of generative factors. But such
labels are uneasily acquired in the graph, so our work con-
centrates on an unsupervised method, which makes latent
variables learn underlying causal information from data.
3. Notations and Preliminaries
Notations. Formally, let G= (V,E)denotes an undirected
and unweighted graph, with its adjacency matrix and degree
matrices as Adj={0,1}n√ónandD, respectively. Here, VandEare the node and edge sets, where n=|V|. Nodes are
associated with pre-defined attributes, written as Attri ‚àà
Rn. We use N(¬∑)to represent Gaussian distribution.
Variational Graph Auto-Encoder (VGAE). In VGAE,
the high-dimensional observation Œµis projected into a
low-dimensional space for compact representation zus-
ing an encoder-decoder framework. Concretely, the en-
coder compresses the input data, and the decoder checks
the soundness of the compressed one by recovering
raw data. Mathematically, we often optimize V AE by
maximizing evidence lower bound (ELBO), denoting as
max p,qLV AE(Œµ),where LV AE(Œµ) =Eq(z|Œµ)[logp(Œµ|z)]‚àí
DKL(q(z|Œµ)‚à•p(z)), where DKLis the Kullback-Leibler
divergence (Joyce, 2011). VGAE is a special cases of
V AE , with graph-based functions for q(¬∑)andp(¬∑). Using
graph convolutional network (GCN) (Zhang et al., 2018),
we have them defined as follows: p(Adjij= 1|Œµi, Œµj) =
œÉ 
Œµ‚ä§
iŒµj
andq(Œµi|Adj, Attri ) =N(¬µi, diag (œÉi)), where
the mean is ¬µ=GCN ¬µ(Adj, Attri )and covariance is
œÉ=GCN œÉ(Adj, Attri ). Here, œÉ(¬∑)is an activation func-
tion and takes the logistic Sigmoid function by default.
Linear Structured Causal Model (SCM). Linear SCM
defines a causal system with linear equations representing
the semantics as follows (Shimizu et al., 2006; Yang et al.,
2021), with independent exogenous factors œµ‚ààRKand
endogenous variables z‚ààRK,
z= Œ¶Tz+œµ= 
I‚àíŒ¶T‚àí1œµ, œµ‚àº N(0, I) (1)
where Œ¶‚ààRK√óKis an adjacency matrix for a directed
acyclic graph (DAG) that captures the causal structure of
nconcepts. Note we use the same letters here to avoid the
abuse of notation.
Disentangled Causal Process (DCP). DCP studies disen-
tanglement in the latent space by considering confounding
variables, which results in theoretically sound properties as
opposed to heuristics in the prior (Suter et al., 2019). Its
detailed definition is as follows.
Definition 3.1. Given mcausal generative factors as G=
[G1, . . . , G m]andLconfounders as C={C1, ..., C L},
causal disentanglement for the observation Xis possible
if and only if Xcan be represented in a SCM context as
follows,
C‚ÜêŒ∂C (2)
Gi‚Üêqi(HC
i, Œ∂i), i= 1,2, ..., m (3)
X‚Üêg(G, Œ∂x) (4)
Here, HC
i‚àà {C1, ..., C L}is the father node of Gi, i.e.,
HC
i‚ÜíGiholds regarding causality. Œ∂c,{Œ∂i}i=1,2,...,m, Œ∂x
are independent noise variables. Note qiandgare prede-
fined functions.

--- PAGE 4 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
4. Theory
In this section, we first formulate the problem of causal dis-
entangled representation learning in VGAE. We next present
a theoretical analysis of causal disentanglement, where we
provide a tight upper bound to approximate the optimum
(see Section 4.1). After that, we introduce a practical so-
lution to accomplish this approximation together with its
properties (see Section 4.2).
Problem Formulation. Denote the input graph data as X
and its optimal latent factor as Z‚àó(Tr¬®auble et al., 2021),
the optimal data distribution is formulated as p‚àó(X) =R
Z‚àóp‚àó(X|Z‚àó)p‚àó(Z‚àó)dZ‚àó, along with an non-i.i.d. assump-
tion on Z‚àó, i.e., p‚àó(Z‚àó)Ã∏=Q
ip(Z‚àó
i). As a common so-
lution to disentanglement, VGAE disentangles input data
into latent representation Z, and the corresponding data dis-
tribution is pŒ∏(X) =R
ZpŒ∏(X|Z)p(Z)dZ, where Œ∏is the
learnable parameters. Given that all correlations can be
modeled as causal structures (Sch ¬®olkopf & von K ¬®ugelgen,
2022), we let Zpossess a causal structure and define this
structure with DCP. Having no labels from Z, the goal of
unsupervised causal disentangled representation learning
in VGAE is to achieve an optimal latent factor Z‚àówhile
making pŒ∏(X) =p‚àó(X)always hold.
4.1. A theoretical analysis of causal disentanglement
Definition 4.1. Given the the process (2) and (3) in the DCP ,
we obtain the generative factors as G=Q(Œ∂+), with Œ∂G=
{Œ∂1, Œ∂2, ..., Œ∂ m}andŒ∂+=Œ∂C‚à™Œ∂G. The distribution of data
can be attained as pŒ∏Œ∂x(X) =R
GpŒ∏Œ∂x(X|G)p(G)dG.
This definition suggests that a given data can be represented
with causal generative factors while having no assumption
of independence as in the previous V AE. Based on this, as
we will see in Theorem 4.1, the causal disentanglement
guarantees an optimal solution to attain the following: 1)
the distribution consistency between the input data and the
predicted one, i.e., pŒ∏(X) =p‚àó(X), and 2) the optimal
latent factor Z‚àó. In contrast, traditional V AE imposes an
independence assumption on Z and suffers a sub-optimality
solution (Tr ¬®auble et al., 2021) w.r.t the true data distribution,
leading to pŒ∏(X)Ã∏=p‚àó(X). With this difference, the above
causal disentanglement avoids such a assumption.
Theorem 4.1. Given independent Normal distributed vari-
ables N=N1, ..., N K, there exists an optimal causal-
ity modeling function Qthat represents the causal gen-
erative factor G={G1, ..., G K}=Q(N), equating to
the optimal disentangled latent factor Z‚àó, while holding
pŒ∏Œ∂x(X) =p‚àó(X).
We defer the proof of Theorem 4.1 to the Appendix A.1.
Theorem 4.1 proves that in a casual setting, there must exist
an optimal solution for the V AE. Next, we introduce a
generalized causal generative factor expression that unifiesthe base for causal disentanglement.
Definition 4.2. (general causal generative factor expres-
sion). Given independent Normal distributed variables
N={N1, ..., N K}and a matrix A‚ààRK√óK, any causal
generative factor can be formulated as Gi=Qi(Bi), where
B=A‚àódiag(N).
According to Theorem 4.1 and the above definition, we
attain a unified optimal expression of generative factor as
follows (detailed proof is at Appendix A.2):
Proposition 1. (unified optimal generative factor expres-
sion). Let Aas a lower triangular matrix, then the ex-
pression of optimal generative factors can be unified as
Gi=Qi(Bi), where B=ÀÜA‚àódiag(N)andÀÜAis permuted
fromA.
Having established the connections between optimal gen-
erative factors in Proposition 1, we arrive at a necessary
condition for optimal factors. Whereas in this paper we aim
to acquire both the necessary and sufficient conditions for
the optimal factors. Solving these two together yields an
analytical solution for the optimal factors (at Appendix A.1),
making the implementation difficult in modern deep archi-
tectures. Such a solution becomes infeasible alongside an
unknown distribution for the optimal latent representations.
A practical solution is to approximate the optimal fac-
tors, within acceptable confidence, while being practically
feasible.
Provided a representation base Bi, assume the existence
of an approximated generative factor to the optimal one,
Qi(Bi), over the same space, denoted as Q‚Ä≤
i(Bi). We derive
a tight upper bound on the approximation error by setting
Q‚Ä≤
ias a linear function, as shown in Theorem 4.2.
Theorem 4.2. Given Biin Proposition 1, and Niwith an
interval of Ni‚àà[¬µi‚àíŒ¥, ¬µi+Œ¥],i= 1,2, ..., K , for an op-
timal Qi(Bi), there exist a linear function Q‚Ä≤
imake Q‚Ä≤
i(Bi)
, the absolute error has such bond: |Qi(Bi)‚àíQ‚Ä≤
i(Bi)| ‚â§
Oi(Œ¥), where Oi(Œ¥) =ai+Œ¥Œõi,Œõi=bi+Pi
t=1citdŒ¥2
t,
ai, bi, cit, dtare constant unrelated to Œ¥,0< d t<1,
i= 1,2, ..., K andt= 1,2, ..., K ,Œ¥is a non-negative
real number unrelated to distribution of N.
Please see Appendix A.3 for more details. Theorem 4.2
suggests that over 95% probability, the range of Niis
within [¬µi‚àí2œÉi, ¬µi+ 2œÉi]and hence the error is bound
byOi(2œÉi), i.e., the bound is nearly constant with 95%
confidence. Note that we assume the optimal latent repre-
sentation (Z‚àó
1, Z‚àó
2, ..., Z‚àó
K)as a linear uniform distribution.
One could arrive at different bounds with distributions, and
we take the uniform distribution for simplicity.

--- PAGE 5 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
4.2. Concept-free Causal Disentanglement
Theorem 4.2 says we can obtain an approximated optimal
generative factor by appointing the projection function lin-
ear. This approximation enables a practical implementation
toward the optimal latent factor. Formally, we introduce the
linear projection-based generative factor:
Proposition 2. (Approximated generative factor expres-
sion). Given independent Normal distributed variables N=
{N1, ..., N K}, a lower triangular matrix ÀúAi, a causal gener-
ative factor can be formulated as G‚Ä≤
i=Q‚Ä≤
i(Bi) =ÀúAi‚àóN‚Ä≤,
where ÀúAis obtained by permuting a lower triangular matrix.
The proof is given in the Appendix A.4. We set our causal
disentanglement in the context of the Structural Causal
Model (SCM) and focus on a linear SCM because of its
simplicity. Following this, we formalize the causal structure
inGas follows,
G‚Ä≤= Œ¶G‚Ä≤+Œµ‚ÜíG‚Ä≤= (I‚àíŒ¶)‚àí1ŒµT(5)
where Œ¶‚ààRK√óKis a DAG adjacency matrix and Œµis a
independent variable. The resulting (I‚àíŒ¶)‚àí1is also a per-
muted low triangular matrix, see the proof in Appendix A.6.
Note that Proposition 2 is for a general causal setting. Let-
tingN=Œµ, the above linear representation based on SCM
shares the same expression as that in the proposition, and
thus inherits the ideal property of approximating the optimal
latent factor Z‚àó. Furthermore, the two variables in Eq. 5
are learned from data in a straightforward manner, without
any labels for supervision. Denoted each Gias a concept
(Kumar et al., 2017) we arrive at an unsupervised causal
disentanglement that does not require any concept labels,
called concept-free causal disentanglement.
In unsupervised disentanglement learning, along with the
linear Gaussian assumption, the identifiability problem (Lo-
catello et al., 2019) often arises due to the discrepancy be-
tween the pre-defined concepts and the learned ones. With-
out supervision, we cannot achieve these pre-defined con-
cepts, especially given limited data. However, as we will
see in the following theorem, these pre-defined concepts
are attainable when sufficient data is accessed. Since these
concepts hold in multiple samples, making them the ground
truth.
Theorem 4.3. Given n observations
{X(1), X(2), ..., X(n)}sampled from the same distri-
bution p‚àó(X), along with their corresponding optimal
generative factors {Z(1), Z(2), ..., Z(n)}, the function of
these generative factors will converge to the same ground
truth (GT) concept.
A formal version of Theorem 4.3 and its proof can be found
at Appendix A.5. More importantly, we believe concepts
obtained by the theorem are better than human-labeledconcepts because these are limited and may involve bias.
The above discrepancy does not always imply errors in the
learned concepts, and conversely, the latter can be a com-
pensation for human-defined ones.
Besides, Theorem 4.3 enables guaranteed learning toward
the ground truth (GT) concepts and leads to the following
property:
Property 1. (Consistency of generative factors). Given ob-
servations sampled from the same distribution, each
sample‚Äôs optimal generative factors, i.e., Z‚àó, capture a
portion of GT concepts, implying that one can approxi-
mate the GT concepts with a merging of Z‚àó, where we
call the merged one an approximated concept.
The consistency property implies that concepts learned from
individual samples capture the GT concepts shared by all
data from the same distribution, making these concepts
adaptable. Therefore, under the same distribution, transfer-
ring concepts from observed data to newly sampled data
benefits the learning of new data, thus significantly reducing
the data demand and avoiding training from scratch.
5. Method
In this section, we propose a novel VGAE with a causal dis-
entanglement model, namely Concept-free Causal VGAE
(CCVGAE), whose goal is to obtain optimal disentangled
latent representations. We also introduce a concept-free
causal disentanglement framework in a meta-learning set-
ting, called concept-free causal Meta-Graph (CC-Meta-
Graph), to harness the property of concept consistency. We
begin by introducing the definition of CCVGAE as follows,
Definition 5.1. (CCVGAE). Given an input graph‚Äôs adja-
cency matrix Adjand node attributes Attri , the proposed
CCVGAE is defined by:
‚Ä¢A prior data distribution p‚àó(Z‚àó)Ã∏=Q
ip(Z‚àó
i)roots
on a set of causal structured latent factors Z‚àó=
{Z‚àó
1, Z‚àó
2, ...Z‚àó
K}.
‚Ä¢An encoder is composed of a GNN-based compres-
sion component and a causal disentanglement com-
ponent. The former employs GNN to compress the
adjacency matrix Adjand node attributes Attri into
a low-dimensional latent space as œµ. The latter (pa-
rameterized by œï) performs our concept-free causal
disentanglement with œµas input, optimizes the under-
lying causal structure Œ¶in the learning procedure,
and outputs the posterior approximation parameters:
qœï(G‚Ä≤|œµ,Œ¶)( see Eq. 5).
‚Ä¢A decoder pœà(Adj|G‚Ä≤)that takes the obtained la-
tent factor Zto infer the adjacency matrix of the

--- PAGE 6 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
input graph data and is parameterized by œà, i.e.,
p(Adjij= 1|Gi, Gj) =œÉ 
G‚ä§
iGj
, where œÉ(¬∑)is
the logistic sigmoid function.
Optimization Objective. The optimization of CCVGAE
is to encourage an equivalence between the approximated
distribution pŒ∏G(X)and the optimal one p‚àó(X). In particu-
lar, the evidence lower bound (ELBO) is used to minimize
the divergence between the above two distributions, and to
enforce that the distribution of Œµis independent Gaussian,
as follows,
LG=Eq(ÀÜG|Adj,Attri )log(p(Adj|ÀÜG))+
KL(q(Œµ|(Adj, Attri ))|N(0, I)).(6)
Apart from minimizing distribution divergences, we also
want to shorten the distance between the observation and the
recovered one by measuring the mean squared error (MSE),
written as: LMSE =MSE (Attri, p (Attri|ÀÜG)).
Meanwhile, performing causal structure modeling demands
a DAG constraint on Œ¶. For the convenience of optimization,
we impose a differentiable constraint function (Yu et al.,
2019) as: LŒ¶=tr((I‚àír
KŒ¶‚ó¶Œ¶)K)‚àíK, where ris an
arbitrary positive number, tr(¬∑)denotes trace norm and K
denotes the number of concepts. Combining the above loss
functions, we derive the overall loss function as follows,
L=‚àíLG+Œ±LŒ¶+Œ≤LMSE, (7)
where Œ±andŒ≤are hyper-parameters. The overall algorithm
is in Appendix A.7.
5.1. Concept-free Causal disentanglement Meta-graph
Meta-Graph (Bose et al., 2019) deals with the few-shot link
prediction task: it aims to predict links on target graphs ( GT)
with a model trained on a few source graphs ( GS), where the
source and target graphs are drawn from the same domain.
Denoted the distribution over graphs in the same domain as
p(G), the distributions of the source and target graphs follow
the same, i.e., GS‚àºp(G)andGT‚àºp(G). To accomplish
this task, we demand high-quality adaptation that transfers
the information in the training data to newly arrived data.
According to Property 1, our concept-free causal disentan-
glement can provide fast adaptation and hence is well suited
for a meta-learning setting. Meta-Graph employs traditional
VGAE to capture information to supply an initialization for
training a subsequent link prediction model. Thanks to the
consistency property, our proposed disentanglement solu-
tion can capture information (i.e., concepts) that is adapt-
able to newly arrived data. To this end, we replace VGAE
with CCVGAE and let the other components remain in the
Meta-Graph, called CC-Meta-Graph. We present the corre-
sponding algorithm in Appendix A.7.6. Experiments
6.1. Task 1: Link Prediction
This experiment aims to study how the proposed method,
CCVGAE , performs on the link prediction task when com-
pared to state-of-the-art methods.
Datasets. We experiment on 6graph benchmark datasets
from various domains (Sen et al., 2008; Pei et al., 2020; Tang
et al., 2009), including Cora, dRisk, Actor, Corn, Texas, and
Wisconsin. Table 6.1 presents the statistics of these datasets,
including the numbers of nodes, edges, and node attributes.
Table 1. Statistics of datasets in our experiments. Note the initial
number of edges for the synthetic data is 4894 .#demotes number
of.
Dataset #Node #Edge #Attr
Cora 2708 5429 1433
Corn 183 295 1703
Texas 183 309 1703
Wisconsin 251 499 1703
dRisk 100 478 4
Actor 7600 33544 931
Synthetic 100 4984‚àó16
Note that dRisk is a data set transformed from dRiskKB (Xu
et al., 2014), constructed from the biological text. dRiskKB
contains 12981 nodes representing disease names, with
weighted edges indicating correlations between disease
pairs. To simplify the dataset, we randomly select 100
nodes from dRiskKB and transfer the weighted edges to the
non-weighted edges. The dRiskKB does not provide node
attributes, so we randomly generate 4dimensions of one-hot
features as node attributes.
Considering that real-world datasets often have unknown
causality, we thus construct synthetic data with controllable
causality.In particular, we produce attributes of nodes, X,
and the adjacency matrix, Adj‚ààR100√ó100, as follows:
Adj=œÉ(Z¬∑ZT)andAttri = 20Sin(Z). Here, we pro-
duceZusing linear SCM to ensure its causality, Mathemat-
ically, we derive Z=CTZ+Œµ‚ÜíZ= (I‚àíCT)‚àí1Œµ,
where C‚ààR16√ó16is a random lower triangular matrix
andŒµ‚ààR16is an independent random vector with same
variance normal distribution.
Baselines. We compare CCVGAE to three prior methods:
(1) VGAE (Kipf & Welling, 2016b), which is the first graph-
based V AEs; (2) SIG-V AE (Hasanzadeh et al., 2019), which
uses a hierarchical variational framework for encoder and a
Bernoulli-Poisson link decoder; (3) DGAE (Wu & Cheng,
2022) incorporates standard auto-encoders (AEs) into GAEs
to enhance the ability of modeling structured information.
Metrics. To evaluate our method, we perform the link
prediction task and thus take two commonly used metrics

--- PAGE 7 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
Table 2. AUC ( %) and AP ( %) scores for all baselines on real-world datasets. Note that X-DGAE shows the best results among all
variations of DGAE, including 6-DGAEŒ≤
Œ±, 36-DGAEŒ≤
Œ±, and 64-DGAEŒ≤
Œ±.‚àódenotes results from the original article.
GV AE SIG-V AE X-DGAE CCVGAE CCVGAE w/o CC
AUC AP AUC AP AUC AP AUC AP AUC AP
Cora 0.91¬±0.02 0.92¬±0.01 0.92¬±0.01 0.93¬±0.02 0.93¬±0.02 0.92¬±0.02 0.85¬±0.03 0.85¬±0.05 0.72¬±0.04 0.73¬±0.03
Corn 0.53¬±0.03 0.66¬±0.06 0.62¬±0.05 0.64¬±0.03 0.73¬±0.10 0.77¬±0.10 0.74¬±0.06 0.78¬±0.04 0.68¬±0.06 0.73¬±0.05
Texas 0.51¬±0.06 0.59¬±0.04 0.60¬±0.03 0.63¬±0.05 0.46‚àó
¬±0.09 0.61‚àó
¬±0.08 0.75¬±0.07 0.80¬±0.07 0.74¬±0.05 0.75¬±0.06
Wisconsin 0.57¬±0.04 0.68¬±0.04 0.68¬±0.05 0.69¬±0.06 0.54‚àó
¬±0.09 0.67‚àó
¬±0.09 0.75¬±0.04 0.79¬±0.05 0.68¬±0.04 0.69¬±0.04
dRisk 0.61¬±0.03 0.62¬±0.05 0.58¬±0.03 0.56¬±0.04 0.73¬±0.11 0.72¬±0.10 0.75¬±0.06 0.72¬±0.05 0.63¬±0.05 0.62¬±0.06
Actor 0.76¬±0.07 0.81¬±0.06 0.77¬±0.03 0.80¬±0.05 0.77¬±0.02 0.80¬±0.03 0.78¬±0.07 0.81¬±0.06 0.72¬±0.03 0.76¬±0.04
Table 3. The performance of Meta-Graph-based baselines under different settings: varying number of meta-training loops and the
requirement of meta-training data.
PPI FIRSTMM DB
loopsCC-Meta-Graph Meta-Graph Rand-Meta-Graph CC-Meta-Graph Meta-Graph Rand-Meta-Graph
5% 10% 5% 10% 5% 10% 5% 10% 5% 10% 5% 10%
10 0.70¬±0.01 0.76¬±0.01 0.59¬±0.02 0.70¬±0.01 0.50¬±0.01 0.50¬±0.00 0.59¬±0.02 0.61¬±0.01 0.57¬±0.01 0.59¬±0.01 0.50¬±0.01 0.50¬±0.01
30 0.70¬±0.01 0.77¬±0.01 0.66¬±0.01 0.75¬±0.02 0.51¬±0.00 0.52¬±0.01 0.59¬±0.00 0.61¬±0.00 0.58¬±0.01 0.60¬±0.00 0.52¬±0.01 0.51¬±0.00
50 0.72¬±0.02 0.77¬±0.00 0.70¬±0.01 0.77¬±0.01 0.51¬±0.01 0.52¬±0.01 0.60¬±0.00 0.62¬±0.02 0.59¬±0.01 0.61¬±0.01 0.51¬±0.02 0.51¬±0.00
70 0.73¬±0.01 0.77¬±0.00 0.72¬±0.01 0.77¬±0.01 0.51¬±0.00 0.51¬±0.00 0.61¬±0.01 0.62¬±0.01 0.59¬±0.01 0.62¬±0.00 0.51¬±0.00 0.52¬±0.01
in this area (Kipf & Welling, 2016b): Area Under ROC
Curve (AUC) and Average Precision (AP) scores. All the
experiment results are averaged over 3seeds.
Implementation details. We train the proposed model for
200iterations using Adam. As for the mean and variance,
we use 32-dimensional and 16-dimensional GCN layers to
implement, respectively.
Main results. We benchmark all the methods across 6real-
world datasets. In Table 2, we observe that CCVGAE (ours)
can reliably compete others with up to 29% improvement
regarding AUC and 19% improvement regarding AP. Recall
thatCCVGAE improves on VGAE by integrating a causal
layer to encourage disentangled representations, suggesting
that the significant improvement is due to the expressiveness
of those disentangled representations. SIG-V AE improve
the representation by imposing graph structure-aware dis-
tributions instead of independent Gaussian, which results
in better performance than VGAE. DGAE enhances VGAE
by deepening GCN layers resulting in a better result than
VGAE, especially for non-Euclidean data.
Additionally, we find that the performance on the Cora
dataset shows different trends than other datasets. We hy-
pothesize that such data could be generated under nearly
independent factors, thus countering the validity of our as-
sumption, i.e., p‚àó(X) =R
Z‚àóp‚àó(X|Z‚àó)p‚àó(Z‚àó)dZ‚àówith
p‚àó(Z‚àó)Ã∏=Q
ip(Z‚àó
i), and resulting in poor performance.
We also experiment on the synthetic data with a predefined
causal structure and achieve advantages as before. In Fig-
ure 2(a), we present performance for all methods by varying
the variance of Œµin a large range: from 10to300. Interest-
ingly, we find that the performance varies little as the noise
level increases, implying that these V AE-based methods arerobust to noise as they capture the variance of the distribu-
tion well. Together, the robustness of our model benefits
from the modeling of causality and variances.
6.2. Task 2: Few Shot Link Prediction
In this experiment, we aim to demonstrate the effectiveness
of the proposed CC-Meta-Graph. As this is a meta-learning
model, it consists of a meta-training phase followed by a
testing phase, and its goal is to transfer knowledge from
meta-training to the test phase. We will investigate the
performance of all methods regarding (1) the number of
meta-training loops and (2) the meta-training data require-
ment because these are the keys to a meta-learning model‚Äôs
performance.
Baselines. Our experiment consists of three baselines corre-
sponding to Meta-Graph (Bose et al., 2019) modifications,
which employ pre-trained VGAEs, pre-trained CC-VGAEs,
and randomness for initialization, called Meta-Graph, CC-
Meta-Graph and Rand-Meta-Graph, respectively. In particu-
lar, the first baselines two are pre-trained on training graphs
and fine-tuned on test graphs.
Datasets. We experiment on two benchmark datasets (Bose
et al., 2019; Zitnik & Leskovec, 2017), including protein-
protein interaction (PPI) and FirstMM DB. In this experi-
ment, for all datasets, we perform link prediction by meta-
training on a small subset of edges and then infer unseen
edges. Under all settings, we use 80% of these graphs to
pre-train weights and 10% as meta-validation, optimizing
the global model parameters, and the rest for meta-testing.
In terms of link prediction, we train all methods with two
different settings: 5%and10% edges of graphs, trying to
see the effectiveness of using the data. Apart from meta-

--- PAGE 8 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
0 100 200 300
Variance50607080AUC (%)
 CCVGAE
VGAESIG-VGAE DGAE
(a)
0 30 60 90
Meta-training Loops506070AUC (%)
CC-Meta-Graph
Meta-Graph
Rand-Meta-Graph (b)
4 8 12 16
Singular Value Index0.00.40.81.0Relative level of singular value
CCVGAE
 VGAESIG-VAE DGAE (c)
Figure 2. (a): The comparison of all baselines of the few shot link prediction task on the synthetic data set. The x-axis denotes the variance
ofŒµ, which is used to construct the synthetic dataset. The index of the maximum is 1, the smaller the value and the larger the index. (b):
The performance of three methods when varying the number of meta-training loops (the PPI dataset). (c): The redundancy reduction
analysis for causal disentangled representation. We take SVD of representations and normalize the eigenvalues to make the maximum as
1. The X-axis is the index of sorted normalized singular value, i.e., the first one denotes the largest value.
training, we always use 20% of edges for validation and the
rest for testing.
Main results. In Table 3, we present the performance of
all methods under different settings. Our method, CC-Meta-
Graph, outperforms others consistently, providing up to
a11% absolute improvement. Notably, we can see that
with5%of the data, the performance of CC-Meta-Graph is
competitive with the others given 10%, suggesting that our
model can produce better generalizable representations with
much less data and align with the consistency property.
We also evaluate how our model behaves under different
meta-training epochs, as shown in Figure 2(b). Our method
shows near-optimal performance even with only a few loops
as opposed to a few dozen loops for Meta-Graph. Since
Rand-Meta-Graph passes random values to the fine-tuning
stage and thus can not benefit from the meta-training mech-
anism, resulting in the worst performance consistently.
To summarize, the superiority of our model validates the
effectiveness of transferring global information to newly
arrived data, even with significantly small data and only a
few training loops, making our proposed method applicable
under a limited budget.
6.3. Ablation Study
Module Importance. Recall that, for representation learn-
ing, we employ a causal structure to enforce disentangle-
ment, which is DAG-structured Œ¶in Eq. 5. Thereby, we in-
vestigate how our method performs without such a causality
structure constraint, called CCVGAE w/o CC. In Table 2,
we find that the model without the DAG constraint , i.e.,
CCVGAE w/o CC, reduces the absolute performance by
12% and12% regarding AUC and AP, respectively. These
ablation results suggest the necessity of causal structure in
our model.The necessarily of LMSE .We investigate how our method
performs without LMSE , called CCVGAE w/o MSE. In
table A.8, we find CCVGAE andCCVGAE w/o MSE have
similar performance (within 2 %absolute gap) in Corn,
Texas, Actor. In Cora, dRisk, Wisconsin, CCVGAE w/o
MSE reduces the absolute performance by up to 5%and7%
regarding AUC and AP, respectively. These results suggest
thatLMSE may slightly improve performance in some data,
but not major.
6.4. Analysis on the redundancy reduction
We now present a redundancy reduction perspective to un-
derstand the effectiveness of our disentangled representa-
tions. In particular, we apply the singular value decompo-
sition (SVD) on the obtained representations from Texas
dataset and compare the magnitudes of their eigenvalues,
i.e., the importance of each eigenvector. In Figure 2(c), we
observe that the singular values of our method decrease
slower, demonstrating that the importance of these eigen-
vectors is less concentrated. This implies that our represen-
tations are less redundant, making them more expressive
under low-dimensional settings.
7. Conclusion
In this paper, we provide a tight upper bound for the approx-
imation of the optimal solution in the V AE framework, to-
gether with a practical solution, called Concept-free Causal
Disentanglement. We then propose an enhanced VGAE by
a new causal disentanglement layer with the above idea,
called CCVGAE . In addition, we discover the consistency
of our derived concepts, which motivates us to develop
a meta-learning model, called CC-Meta-Graph, aiming to
transfer global information from limited data to new ones.
Our experimental results show the effectiveness of both
models in the link prediction task and the few-shot one.

--- PAGE 9 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
References
Bengio, Y ., Courville, A., and Vincent, P. Representation
learning: A review and new perspectives. IEEE transac-
tions on pattern analysis and machine intelligence , 35(8):
1798‚Äì1828, 2013.
Bose, A. J., Jain, A., Molino, P., and Hamilton, W. L. Meta-
graph: Few shot link prediction via meta learning. arXiv
preprint arXiv:1912.09867 , 2019.
Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters,
N., Desjardins, G., and Lerchner, A. Understanding dis-
entangling in Œ≤-vae. arXiv preprint arXiv:1804.03599 ,
2018.
Eastwood, C. and Williams, C. K. A framework for the
quantitative evaluation of disentangled representations.
InInternational Conference on Learning Representations ,
2018.
Fan, S., Wang, X., Mo, Y ., Shi, C., and Tang, J. Debiasing
graph neural networks via learning disentangled causal
substructure. arXiv preprint arXiv:2209.14107 , 2022.
Hamilton, W. L. Graph representation learning. Synthesis
Lectures on Artifical Intelligence and Machine Learning ,
14(3):1‚Äì159, 2020.
Hasanzadeh, A., Hajiramezanali, E., Narayanan, K.,
Duffield, N., Zhou, M., and Qian, X. Semi-implicit graph
variational auto-encoders. Advances in neural informa-
tion processing systems , 32, 2019.
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,
Botvinick, M., Mohamed, S., and Lerchner, A. beta-
vae: Learning basic visual concepts with a constrained
variational framework. 2016.
Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey,
L., Rezende, D., and Lerchner, A. Towards a defi-
nition of disentangled representations. arXiv preprint
arXiv:1812.02230 , 2018.
Hoyer, P., Janzing, D., Mooij, J. M., Peters, J., and
Sch¬®olkopf, B. Nonlinear causal discovery with additive
noise models. Advances in neural information processing
systems , 21, 2008.
Jiang, W. and Luo, J. Graph neural network for traffic
forecasting: A survey. Expert Systems with Applications ,
207:117921, nov 2022.
Joyce, J. M. Kullback-leibler divergence. In International
encyclopedia of statistical science , pp. 720‚Äì722. Springer,
2011.
Kim, H. and Mnih, A. Disentangling by factorising. In
International Conference on Machine Learning , pp. 2649‚Äì
2658. PMLR, 2018.Kipf, T. N. and Welling, M. Variational graph auto-
encoders, 2016a. URL https://arxiv.org/abs/
1611.07308 .
Kipf, T. N. and Welling, M. Variational graph auto-encoders.
arXiv preprint arXiv:1611.07308 , 2016b.
Kocaoglu, M., Snyder, C., Dimakis, A. G., and Vish-
wanath, S. Causalgan: Learning causal implicit gen-
erative models with adversarial training. arXiv preprint
arXiv:1709.02023 , 2017.
Krajewski, G. and Matthews, D. Rh baayen, analyzing
linguistic data: A practical introduction to statistics using
r. cambridge: Cambridge university press, 2008. pp. 368.
isbn-13: 978-0-521-70918-7. Journal of Child Language ,
37(2):465‚Äì470, 2010.
Kumar, A., Sattigeri, P., and Balakrishnan, A. Variational
inference of disentangled latent concepts from unlabeled
observations. arXiv preprint arXiv:1711.00848 , 2017.
Li, Y . and Mandt, S. Disentangled sequential autoencoder.
InInternational Conference on Machine Learning , 2018.
Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S.,
Sch¬®olkopf, B., and Bachem, O. Challenging common
assumptions in the unsupervised learning of disentangled
representations. In international conference on machine
learning , pp. 4114‚Äì4124. PMLR, 2019.
Locatello, F., Bauer, S., Lucic, M., R ¬®atsch, G., Gelly, S.,
Sch¬®olkopf, B., and Bachem, O. A commentary on the
unsupervised learning of disentangled representations. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence , volume 34, pp. 13681‚Äì13684, 2020.
Ma, J., Cui, P., Kuang, K., Wang, X., and Zhu, W. Disen-
tangled graph convolutional networks. In International
conference on machine learning , pp. 4212‚Äì4221. PMLR,
2019.
Paige, B., van de Meent, J.-W., Desmaison, A., Goodman,
N., Kohli, P., Wood, F., Torr, P., et al. Learning disentan-
gled representations with semi-supervised deep genera-
tive models. Advances in neural information processing
systems , 30, 2017.
Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B.
Geom-gcn: Geometric graph convolutional networks.
arXiv preprint arXiv:2002.05287 , 2020.
Ping, H. Independence decomposition of multidimensional
random variables. In Proceedings of the 12th Annual
Academic Conference of China Field Statistics Research
Association , 2005.
Sch¬®olkopf, B. and von K ¬®ugelgen, J. From statistical to
causal learning. arXiv preprint arXiv:2204.00607 , 2022.

--- PAGE 10 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B.,
and Eliassi-Rad, T. Collective classification in network
data. AI magazine , 29(3):93‚Äì93, 2008.
Shen, X., Liu, F., Dong, H., Lian, Q., Chen, Z., and Zhang,
T. Weakly supervised disentangled generative causal
representation learning. Journal of Machine Learning
Research , 23:1‚Äì55, 2022.
Shimizu, S., Hoyer, P. O., Hyv ¬®arinen, A., Kerminen, A.,
and Jordan, M. A linear non-gaussian acyclic model for
causal discovery. Journal of Machine Learning Research ,
7(10), 2006.
Suter, R., Miladinovic, D., Sch ¬®olkopf, B., and Bauer, S.
Robustly disentangled causal mechanisms: Validating
deep representations for interventional robustness. In
International Conference on Machine Learning , pp. 6056‚Äì
6065. PMLR, 2019.
Tang, J., Sun, J., Wang, C., and Yang, Z. Social influence
analysis in large-scale networks. In Proceedings of the
15th ACM SIGKDD international conference on Knowl-
edge discovery and data mining , pp. 807‚Äì816, 2009.
Trauble, F., Creager, E., Kilbertus, N., Locatello, F., Dittadi,
A., Goyal, A., Scholkopf, B., and Bauer, S. On disen-
tangled representations learned from correlated data. In
International Conference on Machine Learning , 2020.
Tr¬®auble, F., Creager, E., Kilbertus, N., Locatello, F., Dit-
tadi, A., Goyal, A., Sch ¬®olkopf, B., and Bauer, S. On
disentangled representations learned from correlated data.
InInternational Conference on Machine Learning , pp.
10401‚Äì10412. PMLR, 2021.
Wu, S., Sun, F., Zhang, W., Xie, X., and Cui, B. Graph neu-
ral networks in recommender systems: A survey, 2020.
URL https://arxiv.org/abs/2011.02260 .
Wu, X. and Cheng, Q. Stabilizing and enhancing link pre-
diction through deepened graph auto-encoders. In IJCAI:
proceedings of the conference , volume 2022, pp. 3587‚Äì
3593. NIH Public Access, 2022.
Xu, R., Li, L., and Wang, Q. driskkb: a large-scale disease-
disease risk relationship knowledge base constructed
from biomedical text. BMC bioinformatics , 15(1):1‚Äì13,
2014.
Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., and Wang,
J. Causalvae: Disentangled representation learning via
neural structural causal models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 9593‚Äì9602, 2021.
Yang, Y ., Feng, Z., Song, M., and Wang, X. Factorizable
graph convolutional networks. Advances in Neural Infor-
mation Processing Systems , 33:20286‚Äì20296, 2020.You, J., Liu, B., Ying, R., Pande, V ., and Leskovec, J. Graph
convolutional policy network for goal-directed molecu-
lar graph generation. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing
Systems , NIPS‚Äô18, pp. 6412‚Äì6422, Red Hook, NY , USA,
2018. Curran Associates Inc.
Yu, Y ., Chen, J., Gao, T., and Yu, M. Dag-gnn: Dag struc-
ture learning with graph neural networks. In Interna-
tional Conference on Machine Learning , pp. 7154‚Äì7163.
PMLR, 2019.
Zhang, K. and Hyvarinen, A. On the identifiability
of the post-nonlinear causal model. arXiv preprint
arXiv:1205.2599 , 2012.
Zhang, S., Tong, H., Xu, J., and Maciejewski, R. Graph con-
volutional networks: Algorithms, applications and open
challenges. In Computational Data and Social Networks:
7th International Conference, CSoNet 2018, Shanghai,
China, December 18‚Äì20, 2018, Proceedings 7 , pp. 79‚Äì91.
Springer, 2018.
Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang,
L., Li, C., and Sun, M. Graph neural networks: A review
of methods and applications. AI Open , 1:57‚Äì81, 2020.
Zhu, X., Xu, C., and Tao, D. Commutative lie group vae for
disentanglement learning. In International Conference
on Machine Learning , pp. 12924‚Äì12934. PMLR, 2021.
Zitnik, M. and Leskovec, J. Predicting multicellular function
through multi-layer tissue networks. Bioinformatics , 33
(14):i190‚Äìi198, 2017.

--- PAGE 11 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
A. Appendix
A.1. Proof of Theorem 4.1
Before detailing the proof, we first introduce two necessary
lemmas.
Lemma 1. ‚àÄK dimension continuous variable X=
(X1, ..., X K), if the support set of its joint probability den-
sity is a convex set in RK, then ‚àÉK dimensional inde-
pendent uniform variables U= (U1, ..., U K)and a set
of function F={f1, f2, ..., f K}result in X=F(U)that
Xk=fk(U1, ..., U k).
The proof of Lemma 1 is provided by Ping (2005). The
above lemma indicates that it is possible to represent contin-
uous observations with convex joint density distributions by
projecting a list of independent variables onto some func-
tions. We now present the following lemma showing that
any given continuous variable can be associated with a uni-
form distribution.
Lemma 2. ‚àÄcontinuous variable X, set its distribution
function as f(X) =P(X‚â§x), then f(X)‚àºU(0,1).
Proof. P(f(X)‚â§a) = P(X‚â§f‚àí1(a)) =
f(f‚àí1(u)) =a, where ais a constant.
Then we prove Theorem 4.1:
Proof. Generally, proving Theorem 4.1 is equal to finding
functions qi,i= 1,2, ..., K that make Gi=qi(NS
i) =
Z‚àó
i, NS‚äÜNtrue. We now present the proof in four steps
as follows.
Step 1: Because of Lemma1, we get that there exists in-
dependent uniform variables (U1, ..., U K)and function F
make Z‚àó=F(U), in which:
U= (U1, ..., U K) (8)
Z‚àó
i=fi(U1, ..., U i) (9)
SetUi‚àºU(ai, bi),i= 1,2, ..., K .
Step 2: Set there are arbitrary K independent normal vari-
ables N1, ..., N K,Ni‚àºN(¬µi, œÉ2
i). Set distribution func-
tion of Niisgi, which means gi(x) =P(Ni‚â§x). Denote:
gi(Ni) =U‚Ä≤
i, i= 1,2, ..., K (10)
Step 3: Because of Lemma2, U‚Ä≤
1, ..., U‚Ä≤
Kare independent
and they all are variables of uniform distribution U(0,1).
Then Uiin Step 1 can be represents by U‚Ä≤
ias:
Ui= (bi‚àíai)U‚Ä≤
i+ai, i= 1,2, ..., K (11)Because gi(Ni) =U‚Ä≤
iin (10), which means U‚Ä≤
iis function
ofNi, so we can denote:
(bi‚àíai)U‚Ä≤
i+ai= (bi‚àíai)gi(Ni) +ai=hi(Ni)(12)
Step 4: Then we can find Z‚àó
i=qi(N1, ..., N i)as:
Z‚àó
i=fi(h1(N1), h2(N2), ..., h i(Ni)) =qi(N1, ..., N i)
(13)
In which set of fiis from equation 9 and hiis
from equation 12. Therefore, we can induce Theo-
rem 4.1 is right because ‚àÄZ‚àó(in equation p‚àó(X) =R
Z‚àóp‚àó(X|Z‚àó)p‚àó(Z‚àó)dZ‚àó),‚àÉa function Qmakes G=
Q(NS) ={qi(N1, ..., N i)}i=1,2,...,K (from equation (13))
is equal to Z‚àó.
A.2. Proof of Proposition 1
Proof.
LetAas a lower triangular matrix, ÀÜAis permuted from A.
We can induce that ÀÜA‚àódiag(N)can be acquired by such
matrix with finite row exchange :
Ô£´
Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠A11N1 0 0 ... 0
A21N1A22N2 0 ... 0
A31N1A32N2A33N3... 0
...
AK1N1AK2N2AK3N3... A KKNKÔ£∂
Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏
Where ÀÜA={Aij}. So we can induce that for each qiin
Equation 13, there must exist one raw of ÀÜA‚àódiag(N)equal
to(ai1N1, ai2N2, ..., a iiNi), which is exactly q‚àí1
i(Z‚àó
i). So
optimal generative factors G=Z‚àócan be expressed as
ÀÜA‚àódiag(N).
Now we provide a remark that implies any functions Q‚Ä≤
with the mentioned two conditions is guaranteed to meet
the same mapping relationships with the true function Q,
which makes G‚Ä≤acquired by such Qhas the same statistical
properties with the true G.
Remark 1. Consider a function set Q‚Ä≤={Q‚Ä≤
1, ...Q‚Ä≤
K}that
computes causal generative factors with a row-wise formu-
lation as Gi=Q‚Ä≤
i(Bi). Assuming B=A‚àódiag(N‚Ä≤)‚àà
RK√óK, along with the following conditions:
Condition 1: A‚ààRK√óKcan be obtained by finite row
exchanges of a lower triangular matrix;
Condition 2: N‚Ä≤consists of Kindependent normal vari-
ables as N‚Ä≤= (N‚Ä≤
1, N‚Ä≤
2, ..., N‚Ä≤
K)T;
then
(1) There must exist two real numbers denoted as aiandbi,
and exist Nk‚ààNas in Theorem 4.1, such that the follow-
ing equation holds: N‚Ä≤
i=ai√óNk+bi, i= 1,2, ..., K .

--- PAGE 12 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
(2) Denote Q={Q1, Q2, ..., Q K}(in Theorem 4.1) and
Q‚àí1
i(Gi) =NS
i‚äÜ {N1, N2, ..., N K}, then for each Qi
there must exist Q‚Ä≤
m‚ààQ‚Ä≤and a constant diagonal ma-
trixViand constant vector Misuch that: Q‚àí1
i(Gi) =
ViQ‚Ä≤
m‚àí1(Gi) +Mi=NS
iifQiandQ‚Ä≤
mare reversible.
Proof. Proof of Remark 1 is equal to proving such 2 state-
ments:
(1)‚àÄiin 1,2,..., K, there exist a constant diagonal matrix Vi
and constant vector MithatN‚Ä≤S
i= (N‚Ä≤
1, N‚Ä≤
2, ..., N‚Ä≤
i)can be
acquired by NS
i= (N1, N2, ...N i):N‚Ä≤S
i=ViNS
i+Mi.
(2)Q={q1, q2, ..., q K}in Equation 13 is unique.
Proof of statement(1):
Because ‚àÄtwo normal variables with the same dimensions,
denoting NiandN‚Ä≤
i, there exist two constant vi, mithat
N‚Ä≤
i=viNi+mi. Statement(2) just represents N‚Ä≤
i=viNi+
mias random vector.
Proof of statement(2):
Because fiandhiis unique. So qiis unique and Qis
unique.
The (1) in Theorem 1 can be acquired by statement(1). Be-
cause statement(2) tells us Qis unique, so we can induce
by Proposition 1 that Equation 13 can also be acquired by
Gi=Qi(Bi) =Q‚Ä≤
i(Bi)ifN=N‚Ä≤. However, we can not
ensure N=N‚Ä≤, so with statement(2), we can induce (2) in
Theorem 1.
A.3. Proof of Theorem 4.2
Before proof, we need a lemma as following:
Lemma 3. Denote the distribution function of normal vari-
ableN‚àº(¬µ, œÉ2)asFN(x). There exist a linear function
fL(x): (1) Make the absolute error |FN(x)‚àífL(x)| ‚â§
1‚àö
2œÄx(1‚àíe‚àíx2
2œÉ2)(2) If x‚àà[¬µ‚àíŒµ, ¬µ+Œµ], then the abso-
lute error |FN(x)‚àífL(x)| ‚â§1‚àö
2œÄŒµ(1‚àíe‚àíŒµ2
2œÉ2)
Proof. The proof of (1): Without losing generalization, we
consider ¬µ= 0and the linear function is fL(x) =1‚àö
2œÄx+
1
2. We only consider x > 0, because both of fL(x)and
distribution function are central symmetric about (0,1
2), sothe absolute error is same when x <0.
|FN(t)‚àífL(t)|
= (1‚àö
2œÄt+1
2)‚àíZt
‚àí‚àû1‚àö
2œÄe‚àíx2
2œÉ2dx
=1‚àö
2œÄt‚àíZt
01‚àö
2œÄe‚àíx2
2œÉ2dx
=Zt
01‚àö
2œÄdx‚àíZt
01‚àö
2œÄe‚àíx2
2œÉ2dx
=Zt
01‚àö
2œÄ(1‚àíe‚àíx2
2œÉ2)dx
‚â§Zt
01‚àö
2œÄ(1‚àíe‚àít2
2œÉ2)dx
=1‚àö
2œÄt(1‚àíe‚àít2
2œÉ2)(14)
The proof of (2): Without losing generalization, we consider
¬µ= 0. Because1‚àö
2œÄt(1‚àíe‚àít2
2œÉ2)is a increasing function.
Ift‚àà[0, Œµ],1‚àö
2œÄt(1‚àíe‚àít2
2œÉ2)‚â§1‚àö
2œÄŒµ(1‚àíe‚àíŒµ2
2œÉ2). The
same as t‚àà[‚àíŒµ,0].
Now we prove Theorem 4.2:
Proof. Assume Z‚àó
1, Z‚àó
2, ..., Z‚àó
Kis a linear uniform dis-
tribution means that: P(Z‚àó
1)‚àºU(Œ∑1+r10, Œ∑1+
r11)andP(Z‚àó
k|Z‚àó
1, ..., Z‚àó
k‚àí1)‚àºU(Œ∑k+rk0, Œ∑k+
rk1), k= 2 , ..., K with Œ∑k=Pk‚àí1
t=1œâktZ‚àó
t, and
œâk1, œâk2, ..., œâ k(k‚àí1), rk0, rk1is real number. Then we can
get that the conditional distribution when Z‚àó
kis in its non-
zero interval:
P(Z‚àó
k‚â§z‚àó
k|Z‚àó
1=z‚àó
1, Z‚àó
2=z‚àó
2, ..., Z‚àó
k‚àí1=z‚àó
k‚àí1)
=1
rk1‚àírk0(z‚àó
k‚àírk0) +œâk1z‚àó
1+...+œâk(k‚àí1)z‚àó
k‚àí1
(15)
We can find that this conditional distribution is a linear func-
tion of (z‚àó
1, ..., z‚àó
k). Moreover, we can induce by Mathemati-
cal Induction that the joint distribution P(Z‚àó
1, ..., Z‚àó
k) =
P(Z‚àó
k|Z‚àó
1, ..., Z‚àó
k‚àí1)P(Z‚àó
1, ..., Z‚àó
k‚àí1)is a constant when
(Z‚àó
1, ..., Z‚àó
k)is in their non-zero interval.
We can find the formulation of fiin Lemma 1 in Ping
(2005) as:
fi(x1, x2, ..., x k) =pX(Xi‚â§xi|X1=x1,X2=x2,...,X i‚àí1=xi‚àí1)
pX(x1,...,x k‚àí1)
pXmeans the probability dense function of Xin Lemma 1,
but in equation 9, pXmeans the dense function of Z‚àó=
{Z‚àó
1, Z‚àó
2, ..., Z‚àó
K}in true data set. So we can induce from
equation 15 that, the numerator of fi(U1, ..., U i)in equa-
tion 9 is linear function of U1, ..., U iand denominator is

--- PAGE 13 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
a constant. Therefore, equation 9 is a linear function of
U1, ..., U i, denoting as:
fi(U1, ..., U i)
=œÅi0+œÅi1U1+...+œÅi(i‚àí1)Ui‚àí1+œÅiiUi(16)
In equation 13, hiis(bi‚àíai)gi(Ni) +ai, where giis the
distribution of normal variable. Without losing generality,
we set bi= 1, ai= 0, then we have hi=gi, which has
the absolute error bound with a linear function as Lemma 3.
Based on equation 16 and Lemma 3, we can get there exist
a linear function fL(N1, N2, ..., N i) =Pi
k=1fLk(Nk)has
the bound with and the optimal solution G=Z‚àó
iwhen
Ni‚àà[¬µi‚àíŒ¥, ¬µi+Œ¥]because:
|fi(h1(N1), h2(N2), ..., h i(Ni))‚àífL(N1, N2, ..., N i)|
=|œÅi0+œÅi1h1(N1) +...+œÅi(i‚àí1)hi‚àí1(Ni‚àí1) +œÅiihi(Ni)
‚àíiX
k=1fLk(Nk)|
‚â§ |iX
k=1(œÅikh1(Nk)‚àífLk(Nk))|+|œÅi0|
‚â§ |œÅi0|+Œ¥‚àö
2œÄiX
k=1œÅik(1‚àíe‚àíŒ¥2
2œÉ2
k)
=|œÅi0|+Œ¥(iX
k=1œÅik‚àö
2œÄ‚àíiX
k=1œÅik‚àö
2œÄe‚àíŒ¥2
2œÉ2
k)
=|œÅi0|+Œ¥(iX
k=1œÅik‚àö
2œÄ‚àíiX
k=1œÅik‚àö
2œÄ(e‚àí1
2œÉ2
k)Œ¥2)
=ai+Œ¥(bi+iX
k=1cikdŒ¥2
k)
(17)
A.4. Proof of Proposition 2
Proof. Now we start from one of rows in ÀÜA‚àódiag(N‚Ä≤)
which is (ai1N1, ai2N2, ai3N3,0, ...,0)(for simplicity,
we denote this row is i-th row. Note that there must
exist such a row because ÀÜAis permuted from triangular
matrix). In Theorem 4.2, we proved that Q‚Ä≤can be
implemented as a linear function. So G‚Ä≤
i=Q‚Ä≤
i(Bi) =
Q‚Ä≤
i({ÀÜA‚àódiag(N‚Ä≤)}i) =Q‚Ä≤
i(ai1N1, ai2N2, ai3N3,0, ...,0).
So:Qi(ai1N1, ai2N2, ai3N3,0, ...,0)
=qi1ai1‚àóN1+qi2ai1‚àóN2+qi3ai1‚àóN3
= (qi1ai1, qi2ai2, qi3ai3,0...,0)‚àó(N1, N2, N3, ..., N K)T
‚àí‚Üí(ci1, ci2, ci3,0...,0)‚àó(N1, N2, N3, ..., N K)T
=ÀúAi‚àóN‚Ä≤
(18)
Here ÀúAi= ( ci1, ci2, ci3,0...,0) and N‚Ä≤=
(N1, N2, N3, ..., N K)T. We can adopt this expres-
sion method to other rows. Finally, we can make such
conclusion: ÀúAis a matrix with the same non-zero position
asÀÜA, means that ÀúAis also a matrix permuted from lower
triangular matrix.
A.5. Formal version of Theorem 4.3
Given nobservations {X(1), X(2), ..., X(n)}sampled from
the same distribution p‚àó(X), along with their corresponding
optimal generative factors {Z(1), Z(2), ..., Z(n)}, a function
¬ØZ(n)with these generative factors will converge to the same
ground truth (GT) concept ¬®Zaslimn‚Üí‚àû¬ØZ(n)=¬®Z, where
¬ØZ(n)= (Z(1)+Z(2)+...+Z(n))/n.
Proof. The conclusion can be easily deduced from the law
of large numbers.
A.6. Proof of (I‚àíŒ¶T)‚àí1
Before the proof, we need a lemma:
Lemma 4. IfDis the adjacency matrix of DAG with nodes
vector Z, which means that Z=DZ. Then there exist a
lower triangular matrix Tand a vector Z‚Ä≤acquired by finite
row exchange of Z, making Z‚Ä≤=TZ‚Ä≤.
Proof. In a DAG, there must exist at least one node with
0 in-degree. We can remove arbitrary one node with 0
in-degree, and make this node as the first node in Z‚Ä≤.
Because the graph without this node is also a DAG, so we
can also find at least one node with 0 in-degree, and make
the second node in Z‚Ä≤. Because the second node‚Äôs in-degree
is 0 in graph without the first node, so the first line of Thas
at most 1 non-zero element.
Repeat this process and we can find such TandZ‚Ä≤.
To prove the (I‚àíŒ¶T)‚àí1can be acquired by a lower trian-
gular matrix with finite row exchange, we need to prove:
There exist a lower triangular matrix Land a elementary
matrix PK√óKacquired by unit matrix with finite row ex-
change, making that GT= (I‚àíŒ¶T)‚àí1œµT=PLœµT,Œ¶is
DAG adjacency matrix with nodes vector GT.
Now we have such proof:

--- PAGE 14 ---
Concept-free Causal Disentanglement with Variational Graph Auto-Encoder
Proof. With Lemma 4, we can induce that there exist a
lower triangular matrix Tand a vector GT
Racquired by
finite row exchange of GT, making GT
R=TGT
R+œµ‚àí‚Üí
GT
R= (I‚àíT)‚àí1œµT.
Because GT
Ris acquired by finite row exchange of GT, we
can denote GT
R=P‚Ä≤GT,P‚Ä≤is acquired by unit matrix with
finite row exchange. So we have P‚Ä≤GT= (I‚àíT)‚àí1œµT‚àí‚Üí
GT=P‚Ä≤‚àí1(I‚àíT)‚àí1œµT. LetP=P‚Ä≤‚àí1,L= (I‚àíT)‚àí1,
then we have GT=PLœµT.
A.7. Algorithm of CCVGAE and Causal-Meta-Graph
for Few Shot Link Prediction
Algorithm 1: Concept-free Causal-Meta-Graph for Few
Shot Link Prediction
Result: GNN global parameters œï, Graph signature
function œà, Global causal layer parameters C
Initialize learning rates: Œ±,Œ≤,Œ≥;
Sample a mini-batch of graphs, Gbatch from p(G)
foreachG‚ààGbatch do
Œµ=Œµtrain‚à™Œµval‚à™Œµtest// Split edges into train,
val, and test;;
sG=œà(G, Œµtrain)// Compute graph signature;
Initialize: œï(0)‚Üêœï// Initialize local parameters via
global parameters
fork in[1 :K]do
sG=stopgrad (sG)// Stop Gradients to Graph
Signature;
Z= (I‚àíCT)‚àí1sG// Compote hidden
representation;
Ltrain =‚àíELBO train +Œ±H(C);
Update œï(k)‚Üêœï(k‚àí1)‚àíŒ≤‚ñΩœïLtrain
end
Initialize: œï‚ÜêœïK;
sG=œà(G, Œµval‚à™Œµtrain)// Compute graph
signature with validation edges;
Lval=‚àíELBO val+Œ±H(C);
Update œï‚Üêœï‚àíŒ≥‚ñΩœïLval;
Update œà‚Üêœà‚àíŒ≥‚ñΩœàLval;
Update C‚ÜêC‚àíŒ≥‚ñΩCLval
end
A.8. Ablation study resultAlgorithm 2: CCVGAE
Input: Graph edges E, node features X,Œ±,Œ≤
Initialize GCN parameters GCN ¬µ,GCN œÉ, causal
matrix Œ¶;
E=Etrain‚à™ Eval‚à™ Etest// Split edges into train, val,
and test;
A=Atrain +Aval+Atest//Generate train, valid, test
adjacency
forepoch in [1 :number of epoch ]do
¬µ=GCN ¬µ(Atrain, X)// Compute mean of Œµ;
œÉ=GCN œÉ(Atrain, X)// Compute variance of Œµ;
Œµ=N(¬µ, œÉ)//Generate Œµas independent normal
distribution;
G= (I‚àíCT)‚àí1Œµ// Compute generate factors;
ÀÜAtrain =œÉ1
GT
iÀÜGj
//Reconstruct adjacency
matrix;
ÀÜX=œÉ2(G)//Reconstruct node features;
Ltrain =‚àíLG+Œ±LŒ¶+Œ≤LMSE ;
Update GCN ¬µ,GCN œÉ,Œ¶
end
Compute ROC (Atest,ÀÜAtest)andAP(Atest,ÀÜAtest)
Table 4. AUC ( %) and AP ( %) scores for CCVGAE w/o MSE and
CCVGAE on real-world datasets.
CCVGAE w/o MSE CCVGAE
AUC AP AUC AP
Cora 0.80¬±0.02 0.82¬±0.03 0.85¬±0.03 0.85¬±0.05
Corn 0.73¬±0.03 0.79¬±0.06 0.74¬±0.06 0.78¬±0.04
Texas 0.76¬±0.04 0.78¬±0.03 0.75¬±0.07 0.80¬±0.07
Wisconsin 0.72¬±0.07 0.77¬±0.06 0.75¬±0.04 0.79¬±0.05
dRisk 0.71¬±0.05 0.65¬±0.07 0.75¬±0.06 0.72¬±0.05
Actor 0.79¬±0.04 0.81¬±0.03 0.78¬±0.07 0.81¬±0.06
