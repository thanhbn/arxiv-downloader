# 2208.11271.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/autoencoder/2208.11271.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 2936530 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
Giáº£i quyáº¿t tÃ¬m kiáº¿m mÃ£ dÃ i báº±ng phÆ°Æ¡ng phÃ¡p chia tÃ¡ch, mÃ£ hÃ³a vÃ  tá»•ng há»£p
Fan Hu1âˆ—, Yanlin Wang2â€ â€¡, Lun Du3,
Hongyu Zhang4, Shi Han3, Dongmei Zhang3, Xirong Li1
1Äáº¡i há»c NhÃ¢n dÃ¢n Trung Quá»‘c
2Khoa Ká»¹ thuáº­t Pháº§n má»m, Äáº¡i há»c Trung SÆ¡n
3Microsoft
4Äáº¡i há»c TrÃ¹ng KhÃ¡nh
TÃ³m táº¯t
TÃ¬m kiáº¿m mÃ£ vá»›i ngÃ´n ngá»¯ tá»± nhiÃªn giÃºp chÃºng ta tÃ¡i sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n mÃ£ hiá»‡n cÃ³. Nhá» cÃ¡c mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n dá»±a trÃªn Transformer, hiá»‡u suáº¥t tÃ¬m kiáº¿m mÃ£ Ä‘Ã£ Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ. Tuy nhiÃªn, do Ä‘á»™ phá»©c táº¡p báº­c hai cá»§a cÆ¡ cháº¿ tá»± chÃº Ã½ Ä‘a Ä‘áº§u, cÃ³ giá»›i háº¡n vá» Ä‘á»™ dÃ i token Ä‘áº§u vÃ o. Äá»ƒ huáº¥n luyá»‡n hiá»‡u quáº£ trÃªn GPU tiÃªu chuáº©n nhÆ° V100, cÃ¡c mÃ´ hÃ¬nh mÃ£ tiá»n huáº¥n luyá»‡n hiá»‡n cÃ³, bao gá»“m GraphCodeBERT, CodeBERT, RoBERTa (code), máº·c Ä‘á»‹nh láº¥y 256 token Ä‘áº§u tiÃªn, khiáº¿n chÃºng khÃ´ng thá»ƒ biá»ƒu diá»…n thÃ´ng tin Ä‘áº§y Ä‘á»§ cá»§a mÃ£ dÃ i lá»›n hÆ¡n 256 token. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» mÃ£ dÃ i, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t baseline má»›i SEA (Split, Encode and Aggregate), chia mÃ£ dÃ i thÃ nh cÃ¡c khá»‘i mÃ£, mÃ£ hÃ³a cÃ¡c khá»‘i nÃ y thÃ nh embedding, vÃ  tá»•ng há»£p chÃºng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c biá»ƒu diá»…n mÃ£ dÃ i toÃ n diá»‡n. Vá»›i SEA, chÃºng ta cÃ³ thá»ƒ trá»±c tiáº¿p sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n dá»±a trÃªn Transformer Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a mÃ£ dÃ i mÃ  khÃ´ng thay Ä‘á»•i cáº¥u trÃºc bÃªn trong vÃ  tÃ¡i huáº¥n luyá»‡n. ChÃºng tÃ´i cÅ©ng so sÃ¡nh SEA vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p Transformer thÆ°a thá»›t. Vá»›i GraphCodeBERT lÃ m bá»™ mÃ£ hÃ³a, SEA Ä‘áº¡t Ä‘iá»ƒm mean reciprocal ranking tá»•ng thá»ƒ lÃ  0.785, cao hÆ¡n 10.1% so vá»›i GraphCodeBERT trÃªn benchmark CodeSearchNet, chá»©ng minh SEA lÃ  má»™t baseline máº¡nh cho tÃ¬m kiáº¿m mÃ£ dÃ i.
Tá»« khÃ³a: tÃ¬m kiáº¿m mÃ£, hiá»ƒu mÃ£ dÃ i, biá»ƒu diá»…n mÃ£

1. Giá»›i thiá»‡u
Má»™t ká»¹ thuáº­t tÃ¬m kiáº¿m mÃ£ tá»‘t giÃºp cÃ¡c nhÃ  phÃ¡t triá»ƒn
thÃºc Ä‘áº©y phÃ¡t triá»ƒn pháº§n má»m báº±ng cÃ¡ch tÃ¬m kiáº¿m
cÃ¡c Ä‘oáº¡n mÃ£ sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn. Nhá»¯ng tiáº¿n
bá»™ gáº§n Ä‘Ã¢y Ä‘Ã£ chá»©ng minh hiá»‡u quáº£ cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p tiá»n huáº¥n luyá»‡n mÃ£ dá»±a trÃªn Transformer,
bao gá»“m CodeBERT (Feng et al., 2020), CoCLR
(Huang et al., 2021), vÃ  GraphCodeBERT (Guo
et al., 2021), Ä‘Ã£ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t tÃ¬m kiáº¿m mÃ£ thÃ´ng qua tiá»n huáº¥n luyá»‡n tá»± giÃ¡m sÃ¡t trÃªn
corpus mÃ£ quy mÃ´ lá»›n.

Tuy nhiÃªn, nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘á»‘i máº·t vá»›i má»™t háº¡n cháº¿ cá»‘ há»¯u. Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n vÃ  bá»™ nhá»›
cá»§a cÆ¡ cháº¿ tá»± chÃº Ã½ trong Transformer gá»‘c tÄƒng theo
báº­c hai vá»›i Ä‘á»™ dÃ i Ä‘áº§u vÃ o, táº¡o ra rÃ ng buá»™c vá» Ä‘á»™ dÃ i Ä‘áº§u vÃ o khoáº£ng 512 token. Äá»ƒ huáº¥n luyá»‡n hiá»‡u quáº£ trÃªn GPU tiÃªu chuáº©n nhÆ°
V100, GraphCodeBERT vÃ  CodeBERT chá»‰ xem xÃ©t 256 token Ä‘áº§u tiÃªn cá»§a cÃ¡c Ä‘oáº¡n mÃ£ vÃ  loáº¡i
bá» báº¥t ká»³ token nÃ o vÆ°á»£t quÃ¡ giá»›i háº¡n nÃ y. Tuy nhiÃªn, háº¡n cháº¿ Ä‘á»™ dÃ i nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n váº¥n Ä‘á» vá» Ä‘á»™ chÃ­nh xÃ¡c, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i cÃ¡c Ä‘oáº¡n mÃ£ dÃ i. VÃ­ dá»¥, khi
kiá»ƒm tra cÃ¡c trÆ°á»ng há»£p khÃ³ cá»§a GraphCode-
BERT, chÃºng tÃ´i tháº¥y ráº±ng GraphCodeBERT cÃ³ hiá»‡u suáº¥t tháº¥p cho má»™t sá»‘ Ä‘oáº¡n mÃ£ dÃ i mÃ 
thÃ´ng tin quan trá»ng náº±m á»Ÿ cuá»‘i. NhÆ°
Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1, cÃ¡c tá»« khÃ³a "Tensor" vÃ 
"patches" xuáº¥t hiá»‡n sau ngÆ°á»¡ng cáº¯t 256-token Ä‘Æ°á»£c Ä‘áº·t bá»Ÿi
GraphCodeBERT, dáº«n Ä‘áº¿n viá»‡c chÃºng bá»‹ loáº¡i trá»« khá»i
xem xÃ©t. Do Ä‘Ã³, Ä‘oáº¡n mÃ£
tÆ°Æ¡ng á»©ng Ä‘Æ°á»£c xáº¿p háº¡ng á»Ÿ vá»‹ trÃ­ 21,148.

ChÃºng tÃ´i tiáº¿p tá»¥c tiáº¿n hÃ nh cÃ¡c nghiÃªn cá»©u thá»±c nghiá»‡m vá»
GraphCodeBERT trÃªn táº­p dá»¯ liá»‡u CodeSearch-
Net Ä‘Æ°á»£c sá»­ dá»¥ng cÃ´ng khai (Husain et al., 2019), vÃ  quan sÃ¡t tháº¥y
sá»± giáº£m dáº§n hiá»‡u suáº¥t tÃ¬m kiáº¿m khi Ä‘á»™
dÃ i cá»§a mÃ£ ground-truth trong truy váº¥n tÄƒng lÃªn (tham kháº£o Báº£ng 1). Váº¥n Ä‘á» nÃ y tÆ°Æ¡ng tá»±
vá»›i váº¥n Ä‘á» vÄƒn báº£n dÃ i trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»±
nhiÃªn, mÃ  Ä‘Ã£ cÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t,
bao gá»“m xá»­ lÃ½ phÃ¢n cáº¥p (Zhang
et al., 2019b), attention thÆ°a thá»›t (Child et al., 2019;
Beltagy et al., 2020), vÃ  tÃ¡i hiá»‡n á»Ÿ má»©c Ä‘oáº¡n
(Dai et al., 2019). Tuy nhiÃªn, viá»‡c Ã¡p dá»¥ng trá»±c tiáº¿p cÃ¡c
phÆ°Æ¡ng phÃ¡p nÃ y cho mÃ£ dÃ i Ä‘áº·t ra hai thÃ¡ch thá»©c.
Thá»© nháº¥t, cÃ¡c ká»¹ thuáº­t nÃ y thay Ä‘á»•i cáº¥u trÃºc bÃªn
trong cá»§a mÃ´ hÃ¬nh Transformer, cÃ³ thá»ƒ lÃ m cho
cÃ¡c tham sá»‘ tiá»n huáº¥n luyá»‡n hiá»‡n cÃ³ trá»Ÿ nÃªn khÃ´ng há»£p lá»‡. Thá»©
hai, mÃ£ dÃ i khÃ¡c vá»›i vÄƒn báº£n dÃ i á»Ÿ chá»— nÃ³ lÃ  má»™t
ngÃ´n ngá»¯ cÃ³ cáº¥u trÃºc cao. KhÃ´ng giá»‘ng nhÆ° má»™t tÃ i liá»‡u vÄƒn báº£n dÃ i
cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t tá»•ng thá»ƒ gáº¯n káº¿t vá»›i
ngá»¯ nghÄ©a hoÃ n chá»‰nh, ngá»¯ nghÄ©a cá»§a mÃ£ lÃ  khÃ´ng
liÃªn tá»¥c, vÃ  cÃ¡c hÃ m khÃ¡c nhau Ä‘Æ°á»£c phÃ¢n bá»‘
trÃªn cÃ¡c vá»‹ trÃ­ khÃ¡c nhau. CÃ¡c thÃ­ nghiá»‡m so sÃ¡nh
Ä‘Æ°á»£c thá»±c hiá»‡n trong Pháº§n 6.2 cung cáº¥p báº±ng chá»©ng arXiv:2208.11271v3  [cs.SE]  26 Mar 2024

--- TRANG 2 ---
Xáº¿p háº¡ng 1 (káº¿t quáº£ sai):
defpatch(self, *args, **kwargs):
return super(Deposit, self).patch(*args, **
kwargs)
Xáº¿p háº¡ng 21148 (ground truth):
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
img = Image.open(fpath)
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches .append(
PIL2array(patch))
return torch.Byte Tensor(
np.array( patches [:n]))256 tokensTráº£ vá» má»™t tensor chá»©a cÃ¡c patches. 
HÃ¬nh 1: VÃ­ dá»¥ vá» GraphCodeBERT.
GraphCodeBERT cáº¯t bá» cÃ¡c token vÆ°á»£t quÃ¡ 256 token. CÃ¡c token chÃ­nh Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u mÃ u vÃ ng.
há»— trá»£ nhá»¯ng lo ngáº¡i nÃ y.

Do Ä‘Ã³, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  chia mÃ£ dÃ i trong khi
báº£o tá»“n thÃ´ng tin ngá»¯ nghÄ©a cá»§a nÃ³. ChÃºng tÃ´i mong muá»‘n
Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y mÃ  khÃ´ng thay Ä‘á»•i cáº¥u trÃºc bÃªn trong cá»§a
cÃ¡c mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n dá»±a trÃªn Transformer hoáº·c yÃªu cáº§u
tÃ¡i huáº¥n luyá»‡n. Äá»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t SEA
(Split, Encode, vÃ  Aggregate) Ä‘á»ƒ xá»­ lÃ½ mÃ£ dÃ i
vÃ  cÃ³ Ä‘Æ°á»£c biá»ƒu diá»…n mÃ£ Ä‘Æ°á»£c cáº£i thiá»‡n.

NhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 2, quÃ¡ trÃ¬nh bao gá»“m
viá»‡c chia mÃ£ dÃ i thÃ nh má»™t táº­p há»£p cÃ¡c máº£nh mÃ£,
sau Ä‘Ã³ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t Ä‘á»ƒ
táº¡o ra má»™t táº­p há»£p khá»‘i mÃ£ chá»“ng láº¥p má»™t pháº§n.
CÃ¡c bá»™ mÃ£ hÃ³a mÃ£ hiá»‡n cÃ³ sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c
embedding cho má»—i khá»‘i mÃ£. Cuá»‘i cÃ¹ng, nhá»¯ng
embedding nÃ y Ä‘Æ°á»£c tá»•ng há»£p Ä‘á»ƒ táº¡o ra biá»ƒu diá»…n cho toÃ n bá»™ mÃ£ dÃ i. ThÃ´ng qua cÃ¡c thÃ­ nghiá»‡m rá»™ng rÃ£i, chÃºng tÃ´i Ä‘Ã£ tháº¥y ráº±ng phÆ°Æ¡ng phÃ¡p chia tÃ¡ch dá»±a trÃªn AST
Ä‘á» xuáº¥t vÃ  phÆ°Æ¡ng phÃ¡p tá»•ng há»£p dá»±a trÃªn attention vÆ°á»£t trá»™i hÆ¡n cÃ¡c ká»¹ thuáº­t khÃ¡c cho viá»‡c chia tÃ¡ch vÃ  tá»•ng há»£p. Do sá»‘ lÆ°á»£ng khá»‘i mÃ£ khÃ¡c nhau
cÃ³ Ä‘Æ°á»£c tá»« cÃ¡c Ä‘oáº¡n mÃ£ khÃ¡c nhau, viá»‡c hoáº¡t Ä‘á»™ng song song trá»Ÿ nÃªn thÃ¡ch thá»©c. Äá»ƒ
giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i Ä‘Ã£ thiáº¿t káº¿ má»™t mÃ´-Ä‘un káº¿t há»£p-chia cho viá»‡c tÄƒng tá»‘c. Äiá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ  SEA khÃ´ng phá»¥ thuá»™c vÃ o bá»™ mÃ£ hÃ³a, nghÄ©a lÃ  nÃ³ cÃ³ thá»ƒ
Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i cÃ¡c bá»™ mÃ£ hÃ³a dá»±a trÃªn Transformer khÃ¡c nhau.
Khi so sÃ¡nh vá»›i cÃ¡c baseline bá»™ mÃ£ hÃ³a dá»±a trÃªn Transformer khÃ¡c nhau, SEA Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong hiá»‡u suáº¥t mean reciprocal ranking (MRR), tá»« 7% Ä‘áº¿n 10%.

Báº£ng 1: Hiá»‡u suáº¥t tÃ¬m kiáº¿m mÃ£ (MRR) cá»§a
cÃ¡c Ä‘á»™ dÃ i token mÃ£ ground-truth khÃ¡c nhau. ChÃºng tÃ´i Ä‘áº·t
Ä‘á»™ dÃ i cáº¯t mÃ£ tá»« 50 Ä‘áº¿n 512. Káº¿t quáº£ cao
nháº¥t trong má»—i cá»™t Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u. Táº­p dá»¯ liá»‡u:
CodeSearchNet python. MÃ´ hÃ¬nh: GraphCodeBERT.
Äá»™ dÃ i token Äá»™ dÃ i cáº¯t mÃ£
50 100 256 400 512
[0, 256) 0.6274 0.6856 0.6909 0.6897 0.6906
[256, 512) 0.6239 0.7027 0.7237 0.7258 0.7265
[512, 768) 0.6004 0.6467 0.7168 0.7180 0.7181
[768, 1024) 0.6038 0.6315 0.7111 0.7375 0.7276
[1024, 1943) 0.6202 0.6573 0.6589 0.6835 0.6825

CÃ¡c Ä‘Ã³ng gÃ³p cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:
â€¢PhÃ¡t hiá»‡n vÃ  xÃ¡c minh thá»±c nghiá»‡m vá» khÃ³
khÄƒn trong viá»‡c mÃ´ hÃ¬nh hÃ³a mÃ£ dÃ i trong cÃ¡c
mÃ´ hÃ¬nh tÃ¬m kiáº¿m mÃ£ dá»±a trÃªn Transformer hiá»‡n cÃ³.
â€¢ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t baseline má»›i SEA vÃ  khÃ¡m phÃ¡
cÃ i Ä‘áº·t chia tÃ¡ch vÃ  tá»•ng há»£p tá»‘i Æ°u.
ChÃºng tÃ´i cÅ©ng thiáº¿t káº¿ má»™t mÃ´-Ä‘un káº¿t há»£p-chia cho
viá»‡c tÄƒng tá»‘c.
â€¢ThÃ´ng qua cÃ¡c thÃ­ nghiá»‡m rá»™ng rÃ£i, chÃºng tÃ´i cho tháº¥y
hiá»‡u quáº£ cá»§a SEA Ä‘á» xuáº¥t vá»›i cÃ¡c
baseline bá»™ mÃ£ hÃ³a khÃ¡c nhau trong sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh, táº¡o ra má»™t baseline máº¡nh cho
tÃ¬m kiáº¿m mÃ£. MÃ£ nguá»“n vÃ  dá»¯ liá»‡u thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cÃ³ sáºµn táº¡i: https://github.
com/fly-dragon211/SEA .

2. CÃ´ng trÃ¬nh liÃªn quan
2.1. PhÆ°Æ¡ng phÃ¡p tÃ¬m kiáº¿m mÃ£
CÃ¡c nghiÃªn cá»©u Ä‘áº§u (Nie et al., 2016; Yang and Huang,
2017; Rosario, 2000; Hill et al., 2011; Satter and
Sakib, 2016; Lv et al., 2015; Van Nguyen et al.,
2017) trong tÃ¬m kiáº¿m mÃ£ chá»§ yáº¿u Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t truy xuáº¥t thÃ´ng tin (IR) trá»±c tiáº¿p, xem tÃ¬m kiáº¿m mÃ£
nhÆ° má»™t nhiá»‡m vá»¥ khá»›p vÄƒn báº£n. Cáº£ truy váº¥n vÃ  Ä‘oáº¡n mÃ£
Ä‘á»u Ä‘Æ°á»£c xem nhÆ° vÄƒn báº£n thuáº§n tÃºy, vÃ  cÃ¡c
thuáº­t toÃ¡n khá»›p vÄƒn báº£n truyá»n thá»‘ng nhÆ° tÃºi tá»«
(BOW) (SchÃ¼tze et al., 2008), Jaccard (Jac-
card, 1901), táº§n suáº¥t thuáº­t ngá»¯-nghá»‹ch Ä‘áº£o táº§n suáº¥t tÃ i liá»‡u
(TF-IDF) (Robertson and Jones, 1976),
BM25 (má»™t phiÃªn báº£n cáº£i tiáº¿n cá»§a TF-IDF) (Robertson
and Zaragoza, 2009), vÃ  mÃ´ hÃ¬nh boolean má»Ÿ rá»™ng (Lv et al., 2015) Ä‘Æ°á»£c sá»­ dá»¥ng. VÃ¬ Ä‘á»™ dÃ i mÃ£
cÃ³ tÃ¡c Ä‘á»™ng tá»‘i thiá»ƒu Ä‘áº¿n Ä‘á»™ phá»©c táº¡p mÃ´ hÃ¬nh hÃ³a, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ mÃ£ hÃ³a mÃ£ dÃ i mÃ  khÃ´ng
cáº¯t bá».

Sau khi giá»›i thiá»‡u mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n quy mÃ´ lá»›n BERT (Devlin et al., 2019), Code-
BERT Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Feng et al. (2020). Code-
BERT lÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n trÃªn mÃ£ nguá»“n
vÃ  bÃ¬nh luáº­n khÃ´ng nhÃ£n, Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t áº¥n tÆ°á»£ng

--- TRANG 3 ---
hiá»‡u suáº¥t trong tÃ¬m kiáº¿m mÃ£ dá»±a trÃªn vÄƒn báº£n thÃ´ng qua
tinh chá»‰nh trÃªn cÃ¡c táº­p dá»¯ liá»‡u cáº·p vÄƒn báº£n-mÃ£. Huang
et al. (2021) giá»›i thiá»‡u CoCLR, má»™t phÆ°Æ¡ng phÃ¡p há»c
tÆ°Æ¡ng pháº£n nÃ¢ng cao khá»›p truy váº¥n-mÃ£.
Sun et al. (2022) phÃ¡t triá»ƒn má»™t ká»¹ thuáº­t dá»‹ch mÃ£
nháº­n biáº¿t ngá»¯ cáº£nh dá»‹ch cÃ¡c Ä‘oáº¡n mÃ£ thÃ nh mÃ´ táº£
ngÃ´n ngá»¯ tá»± nhiÃªn. Gu et al. (2022)
sá»­ dá»¥ng bÄƒm sÃ¢u vÃ  phÃ¢n loáº¡i mÃ£ Ä‘á»ƒ
tÄƒng tá»‘c tÃ¬m kiáº¿m mÃ£, trong khi Chai et al. (2022)
Ä‘iá»u chá»‰nh há»c meta few-shot cho tÃ¬m kiáº¿m mÃ£.
Guo et al. (2021) Ä‘á» xuáº¥t GraphCodeBERT, káº¿t
há»£p cÃ¡c nhiá»‡m vá»¥ tiá»n huáº¥n luyá»‡n nháº­n biáº¿t cáº¥u trÃºc Ä‘á»ƒ
cáº£i thiá»‡n hiá»ƒu mÃ£ vÃ  hiá»‡u suáº¥t. Gáº§n
Ä‘Ã¢y, Hu et al. (2023) sá»­ dá»¥ng khung tÃ¬m kiáº¿m mÃ£
fusion hai giai Ä‘oáº¡n káº¿t há»£p bi-encoder
vÃ  cross-encoder Ä‘á»ƒ nÃ¢ng cao hiá»‡u suáº¥t. Tuy
nhiÃªn, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a Transformer
vÃ  bá»™ nhá»› GPU háº¡n cháº¿ thÆ°á»ng dáº«n Ä‘áº¿n viá»‡c cáº¯t bá»
cÃ¡c Ä‘oáº¡n mÃ£ dÃ i.

2.2. Biá»ƒu diá»…n mÃ£ tháº§n kinh vá»›i
cáº¥u trÃºc mÃ£
Gáº§n Ä‘Ã¢y, Ä‘Ã£ cÃ³ nhá»¯ng tiáº¿n bá»™ Ä‘Ã¡ng chÃº Ã½
trong cÃ¡c phÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n mÃ£ tháº§n kinh táº­n dá»¥ng
cáº¥u trÃºc mÃ£, Ä‘áº·c biá»‡t lÃ  CÃ¢y cÃº phÃ¡p trá»«u tÆ°á»£ng
(AST), mang láº¡i hiá»‡u suáº¥t áº¥n tÆ°á»£ng (Alon
et al., 2020; Sun et al., 2020; Bui et al., 2021;
Kim et al., 2021; Peng et al., 2021; Hellendoorn
et al., 2019; Allamanis et al., 2021; Georgiev et al.,
2022; Ma et al., 2023; Du and Yu, 2023). MMAN
(Wan et al., 2019) káº¿t há»£p má»™t lá»›p fusion attention
Ä‘a phÆ°Æ¡ng thá»©c Ä‘á»ƒ káº¿t há»£p biá»ƒu diá»…n AST vÃ  Control Flow
Graph (CFG). ASTNN (Zhang
et al., 2019a) vÃ  CAST (Shi et al., 2021) phÃ¢n
Ä‘oáº¡n AST lá»›n thÃ nh chuá»—i cÃ¡c cÃ¢y cÃ¢u lá»‡nh
nhá» hÆ¡n, mÃ£ hÃ³a chÃºng thÃ nh vector báº±ng cÃ¡ch báº¯t
giá»¯ thÃ´ng tin tá»« vá»±ng vÃ  cÃº phÃ¡p cá»§a má»—i
cÃ¢u lá»‡nh. TBCAA (Chen et al., 2019) sá»­ dá»¥ng máº¡ng
tÃ­ch cháº­p dá»±a trÃªn cÃ¢y trÃªn AST Ä‘Æ°á»£c tÄƒng cÆ°á»ng API. UniXcoder (Guo et al., 2022) táº­n dá»¥ng cáº£
AST vÃ  bÃ¬nh luáº­n mÃ£ Ä‘á»ƒ lÃ m phong phÃº biá»ƒu diá»…n mÃ£.
GraphCodeBERT (Guo et al., 2021) káº¿t
há»£p quan há»‡ biáº¿n Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« AST trong
cÃ¡c nhiá»‡m vá»¥ tiá»n huáº¥n luyá»‡n cá»§a nÃ³. Trong cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i, chÃºng tÃ´i Ä‘áº·c biá»‡t
muá»‘n báº¯t giá»¯ vÃ  mÃ´ hÃ¬nh hÃ³a thÃ´ng tin cáº¥u trÃºc
cÃ³ máº·t trong cÃ¡c Ä‘oáº¡n mÃ£ dÃ i.

2.3. Transformer cho vÄƒn báº£n dÃ i
Viá»‡c Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh Transformer cho vÄƒn báº£n dÃ i
cÃ³ thá»ƒ Ä‘Æ°á»£c chia rá»™ng thÃ nh hai loáº¡i: má»Ÿ rá»™ng
attention vÃ  nÃ¢ng cao mÃ´ hÃ¬nh Trans-
former gá»‘c, vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p tá»•ng há»£p. Loáº¡i
Ä‘áº§u tiÃªn bao gá»“m bá»‘n phÆ°Æ¡ng phÃ¡p chÃ­nh: attention
thÆ°a thá»›t (Child et al., 2019; Correia et al., 2019;
Beltagy et al., 2020; Kitaev et al., 2019; Roy et al.,
2021; Ainslie et al., 2020; Jiang et al., 2020; GÃ¼n-
ther et al., 2023), tÃ¡i hiá»‡n (Dai et al., 2019), cÆ¡

Báº£ng 2: Thá»‘ng kÃª Ä‘á»™ dÃ i token mÃ£ cá»§a táº­p
Ä‘Ã¡nh giÃ¡ CodeSearchNet.
Äá»™ dÃ i Ruby JS Go Py Java Php Tá»•ng thá»ƒ
[0, 256) 16% 10% 22% 14% 13% 13% 14%
[256, 512) 44% 29% 38% 30% 27% 26% 32%
[512, + âˆ) 41% 62% 40% 56% 60% 61% 54%

cháº¿ phÃ¢n cáº¥p (Zhang et al., 2019b; Gao
and Callan, 2022), vÃ  attention nÃ©n (Ye
et al., 2019; Guo et al., 2019). Attention thÆ°a thá»›t
háº¡n cháº¿ má»—i token chá»‰ chÃº Ã½ Ä‘áº¿n má»™t táº­p con cá»§a
cÃ¡c token khÃ¡c. TÃ¡i hiá»‡n tÃ­ch há»£p cÃ¡c yáº¿u tá»‘ máº¡ng
tháº§n kinh tÃ¡i hiá»‡n vÃ o cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘á»ƒ má»Ÿ
rá»™ng pháº¡m vi attention cá»§a chÃºng. CÆ¡ cháº¿ phÃ¢n cáº¥p
mÃ´ hÃ¬nh vÄƒn báº£n Ä‘áº§u vÃ o dÃ i má»™t cÃ¡ch phÃ¢n cáº¥p, tá»« cÃ¢u
Ä‘áº¿n Ä‘oáº¡n vÄƒn. Attention nÃ©n nÃ©n cÃ³ chá»n lá»c
cÃ¡c pháº§n cá»¥ thá»ƒ cá»§a Ä‘áº§u vÃ o.

Loáº¡i thá»© hai, phÆ°Æ¡ng phÃ¡p tá»•ng há»£p, bao
gá»“m viá»‡c tá»•ng há»£p nhiá»u Ä‘iá»ƒm sá»‘ Ä‘oáº¡n vÄƒn hoáº·c biá»ƒu
diá»…n cho má»™t tÃ i liá»‡u dÃ i. VÃ­ dá»¥,
Wang et al. (2019) Ä‘á» xuáº¥t má»™t mÃ´ hÃ¬nh BERT
Ä‘a Ä‘oáº¡n vÄƒn Ä‘á»ƒ chuáº©n hÃ³a toÃ n cá»¥c Ä‘iá»ƒm sá»‘ cÃ¢u tráº£ lá»i trÃªn
táº¥t cáº£ cÃ¡c Ä‘oáº¡n vÄƒn trong nhiá»‡m vá»¥ há»i Ä‘Ã¡p. Trong
bá»‘i cáº£nh xáº¿p háº¡ng tÃ i liá»‡u, SMITH (Yang et al.,
2020) há»c biá»ƒu diá»…n tÃ i liá»‡u thÃ´ng qua
tá»•ng há»£p biá»ƒu diá»…n cÃ¢u phÃ¢n cáº¥p.
PARADE (Li et al., 2020) sá»­ dá»¥ng Max, CNN, At-
tention, vÃ  Transformer Ä‘á»ƒ tá»•ng há»£p cÃ¡c biá»ƒu diá»…n Ä‘oáº¡n vÄƒn. Tsujimura et al. (2023) sá»­ dá»¥ng
phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t Ä‘á»ƒ quáº£n lÃ½ chuá»—i Ä‘áº§u vÃ o
dÃ i trong bá»‘i cáº£nh cÃ¡c nhiá»‡m vá»¥ Nháº­n dáº¡ng Thá»±c thá»ƒ CÃ³ tÃªn
y táº¿.

Tuy nhiÃªn, nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ khÃ´ng hoÃ n toÃ n phÃ¹
há»£p cho mÃ£ cÃ³ cáº¥u trÃºc cao. Trong cÃ¡c chÆ°Æ¡ng trÃ¬nh
Ä‘Æ°á»£c thiáº¿t káº¿ tá»‘t, mÃ£ trong cÃ¹ng má»™t module, nhÆ°
má»™t hÃ m, Ä‘Æ°á»£c káº¿t ná»‘i cháº·t cháº½ vá»›i nhau, trong khi tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c module khÃ¡c nhau Ä‘Æ°á»£c káº¿t ná»‘i lá»ng láº»o, tuÃ¢n theo nguyÃªn táº¯c gáº¯n káº¿t cao vÃ 
khá»›p ná»‘i tháº¥p. NgÆ°á»£c láº¡i, vÄƒn báº£n dÃ i trong ngÃ´n ngá»¯ tá»±
nhiÃªn cÃ³ xu hÆ°á»›ng thá»ƒ hiá»‡n tÃ­nh máº¡ch láº¡c. Trong bÃ i bÃ¡o nÃ y,
chÃºng tÃ´i Ä‘iá»u tra kháº£ nÄƒng Ã¡p dá»¥ng cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p vÄƒn báº£n dÃ i
trong lÄ©nh vá»±c tÃ¬m kiáº¿m mÃ£ vÃ  Ä‘á» xuáº¥t má»™t
baseline má»›i SEA cho tÃ¬m kiáº¿m mÃ£ dÃ i.

3. Äá»™ng lá»±c: Váº¥n Ä‘á» mÃ£ dÃ i
3.1. Kiáº¿n thá»©c cÆ¡ báº£n
TÃ¬m kiáº¿m mÃ£ nháº±m tÃ¬m Ä‘oáº¡n mÃ£ C phÃ¹ há»£p nháº¥t
tá»« má»™t codebase cho trÆ°á»›c khá»›p vá»›i má»™t
truy váº¥n Q. Äá»‘i vá»›i má»™t mÃ´ hÃ¬nh há»c sÃ¢u hiá»‡n táº¡i, chÃºng ta
Ä‘áº§u tiÃªn chuyá»ƒn Ä‘á»•i truy váº¥n Q vÃ  cÃ¡c Ä‘oáº¡n mÃ£ C thÃ nh
token truy váº¥n vÃ  mÃ£ vá»›i tokenizer nhÆ°
BPE (Sennrich et al., 2016). Sau Ä‘Ã³ chÃºng ta chuyá»ƒn Ä‘á»•i
ID token cá»§a truy váº¥n Q vÃ  cÃ¡c Ä‘oáº¡n mÃ£
C thÃ nh biá»ƒu diá»…n vector eq vÃ  ec báº±ng cÃ¡c
bá»™ mÃ£ hÃ³a máº¡ng tháº§n kinh, vÃ  tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± (hoáº·c

--- TRANG 4 ---
khoáº£ng cÃ¡ch) trong khÃ´ng gian Euclidean nhÆ°
Ä‘á»™ tÆ°Æ¡ng tá»± Cosine hoáº·c khoáº£ng cÃ¡ch Euclidean Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c
Ä‘iá»ƒm sá»‘ tÆ°Æ¡ng tá»± xuyÃªn phÆ°Æ¡ng thá»©c s. Viá»‡c tÃ­nh toÃ¡n
cÃ³ thá»ƒ Ä‘Æ°á»£c cÃ´ng thá»©c hÃ³a nhÆ° sau:

eq= Î“(tokenizer (Q))
ec= Î“â€²(tokenizer (C)), CâˆˆCodebase
s=sim(eq,ec)(1)
trong Ä‘Ã³ Î“ vÃ  Î“â€² lÃ  hai bá»™ mÃ£ hÃ³a máº¡ng tháº§n kinh
Ä‘Æ°á»£c huáº¥n luyá»‡n tá»‘t há»c tá»« dá»¯ liá»‡u cáº·p cÃ³ nhÃ£n.

3.2. Váº¥n Ä‘á» mÃ£ dÃ i
Äá»ƒ kiá»ƒm soÃ¡t chi phÃ­ bá»™ nhá»› vÃ  tÃ­nh toÃ¡n trong
giai Ä‘oáº¡n huáº¥n luyá»‡n, viá»‡c cáº¯t bá» mÃ£ dÃ i lÃ  thÃ´ng lá»‡
phá»• biáº¿n. VÃ­ dá»¥, GraphCodeBERT thÆ°á»ng
láº¥y 256 token mÃ£ Ä‘áº§u tiÃªn theo máº·c Ä‘á»‹nh. Äá»ƒ Ä‘iá»u
tra liá»‡u phÆ°Æ¡ng phÃ¡p cáº¯t bá» nÃ y cÃ³ dáº«n Ä‘áº¿n
máº¥t thÃ´ng tin hay khÃ´ng, chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh thá»‘ng kÃª Ä‘á»™ dÃ i token
trÃªn CodeSearchNet. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2, chÃºng tÃ´i
tháº¥y ráº±ng cÃ¡c Ä‘oáº¡n vá»›i Ä‘á»™ dÃ i token nhá» hÆ¡n
256 chá»‰ chiáº¿m 14.1%, trong khi 53.5% cÃ¡c Ä‘oáº¡n mÃ£
vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i mÃ£ hÃ³a tá»‘i Ä‘a
512 token cho Transformer. Äiá»u nÃ y cho tháº¥y ráº±ng
viá»‡c cáº¯t bá» dáº«n Ä‘áº¿n máº¥t thÃ´ng tin cho cÃ¡c Ä‘oáº¡n
cÃ³ Ä‘á»™ dÃ i token lá»›n hÆ¡n 256.

Äá»ƒ kiá»ƒm tra sá»± khÃ¡c biá»‡t hiá»‡u suáº¥t tÃ¬m kiáº¿m cá»§a
GraphCodeBERT trÃªn cÃ¡c táº­p con truy váº¥n vá»›i Ä‘á»™ dÃ i mÃ£ ground truth (GT) khÃ¡c nhau, chÃºng tÃ´i chia
táº­p con test python cá»§a CodeSearchNet (CSN) thÃ nh 5
táº­p truy váº¥n riÃªng biá»‡t dá»±a trÃªn Ä‘á»™ dÃ i token mÃ£ GT khÃ¡c nhau. ChÃºng tÃ´i tÃ­nh toÃ¡n Mean Reciprocal Rank
(MRR) cá»§a GraphCodeBERT cho cÃ¡c Ä‘á»™ dÃ i cáº¯t mÃ£ khÃ¡c nhau, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1. ÄÃ¡ng chÃº Ã½, chÃºng tÃ´i quan
sÃ¡t tháº¥y xu hÆ°á»›ng giáº£m trong hiá»‡u suáº¥t tÃ¬m kiáº¿m khi
Ä‘á»™ dÃ i token mÃ£ ground-truth tÄƒng (tá»«
trÃªn xuá»‘ng dÆ°á»›i) Ä‘á»‘i vá»›i Ä‘á»™ dÃ i token mÃ£ vÆ°á»£t quÃ¡
256 token, cho tháº¥y ráº±ng cÃ¡c Ä‘oáº¡n mÃ£ dÃ i Ä‘áº·t ra
thÃ¡ch thá»©c cho GraphCodeBERT. HÆ¡n ná»¯a, khi Ä‘á»™
dÃ i cáº¯t mÃ£ má»Ÿ rá»™ng tá»« trÃ¡i sang pháº£i,
chÃºng tÃ´i quan sÃ¡t tháº¥y hiá»‡u suáº¥t tÃ¬m kiáº¿m tÆ°Æ¡ng Ä‘á»‘i á»•n Ä‘á»‹nh khi Ä‘á»™ dÃ i cáº¯t vÆ°á»£t quÃ¡
Ä‘á»™ dÃ i token. VÃ  xuáº¥t hiá»‡n xu hÆ°á»›ng tÄƒng
trong hiá»‡u suáº¥t tÃ¬m kiáº¿m Ä‘á»‘i vá»›i cÃ¡c Ä‘oáº¡n mÃ£ cÃ³
Ä‘á»™ dÃ i token vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i cáº¯t.
Äiá»u nÃ y gá»£i Ã½ ráº±ng viá»‡c chá»‰ cáº¯t bá» mÃ£ dÃ i cÃ³ thá»ƒ
dáº«n Ä‘áº¿n máº¥t thÃ´ng tin cÃ³ giÃ¡ trá»‹.

4. SEA
Trong pháº§n nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y tá»•ng quan toÃ n diá»‡n
vá» SEA, bao gá»“m kiáº¿n trÃºc mÃ´ hÃ¬nh, phÆ°Æ¡ng phÃ¡p chia tÃ¡ch, ká»¹ thuáº­t tá»•ng há»£p,
vÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tÄƒng tá»‘c
suy luáº­n.

4.1. Kiáº¿n trÃºc mÃ´ hÃ¬nh
ChÃºng tÃ´i giá»›i thiá»‡u SEA cá»§a chÃºng tÃ´i trong pháº§n nÃ y. Pipeline
tá»•ng thá»ƒ Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2. Cho má»™t Ä‘oáº¡n mÃ£
C, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  cÃ³ Ä‘Æ°á»£c biá»ƒu diá»…n mÃ£
ec. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng má»™t
phÆ°Æ¡ng phÃ¡p Ä‘a bÆ°á»›c. Äáº§u tiÃªn chÃºng tÃ´i chia Ä‘oáº¡n mÃ£ thÃ nh
má»™t táº­p há»£p máº£nh mÃ£:
P=Split (C) ={p1, p2, . . . , p n}.(2)
Sau Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c
má»™t táº­p há»£p khá»‘i mÃ£ chá»“ng láº¥p má»™t pháº§n:
B=SlidingWindow (P) ={b1, b2, . . . , b k}.(3)
Giáº£ sá»­ kÃ­ch thÆ°á»›c cá»­a sá»• lÃ  w vÃ  bÆ°á»›c lÃ  s,
thÃ¬ sá»‘ khá»‘i mÃ£ lÃ  k=âŒŠnâˆ’w
s+ 1âŒ‹,
trong Ä‘Ã³ âŒŠÂ·âŒ‹ tham chiáº¿u Ä‘áº¿n lÃ m trÃ²n xuá»‘ng. Tiáº¿p theo, chÃºng tÃ´i sá»­ dá»¥ng má»™t
bá»™ mÃ£ hÃ³a mÃ£, nhÆ° GraphCodeBERT, Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c
embedding cho má»—i khá»‘i mÃ£ trong k khá»‘i:
eB={eb1, eb2, . . . , e bk}. (4)
Cuá»‘i cÃ¹ng, má»™t phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ káº¿t
há»£p k embedding thÃ nh biá»ƒu diá»…n mÃ£
ec:
ec=Aggregation (eB) (5)

4.2. PhÆ°Æ¡ng phÃ¡p chia tÃ¡ch
Äá»ƒ cÃ³ Ä‘Æ°á»£c táº­p há»£p máº£nh mÃ£, chÃºng tÃ´i khÃ¡m phÃ¡ bá»‘n
phÆ°Æ¡ng phÃ¡p chia tÃ¡ch, cá»¥ thá»ƒ lÃ  chia tÃ¡ch dá»±a trÃªn khoáº£ng tráº¯ng,
chia tÃ¡ch dá»±a trÃªn token, chia tÃ¡ch dá»±a trÃªn dÃ²ng, vÃ  chia tÃ¡ch dá»±a trÃªn AST. Chia tÃ¡ch dá»±a trÃªn khoáº£ng tráº¯ng Ä‘Æ¡n giáº£n lÃ 
chia theo khoáº£ng tráº¯ng, dáº«n Ä‘áº¿n viá»‡c chia má»™t chuá»—i
nhÆ° "def read_image_file" Ä‘Æ°á»£c chia thÃ nh {'def',
'read_image_file'}. TÆ°Æ¡ng tá»±, chia tÃ¡ch dá»±a trÃªn token
vÃ  chia tÃ¡ch dá»±a trÃªn dÃ²ng bao gá»“m viá»‡c chia dá»±a trÃªn
token vÃ  dÃ²ng, tÆ°Æ¡ng á»©ng.

Má»™t CÃ¢y CÃº phÃ¡p Trá»«u tÆ°á»£ng (AST) lÃ  má»™t biá»ƒu diá»…n cÃ¢y
cá»§a cáº¥u trÃºc cÃº phÃ¡p cá»§a mÃ£ nguá»“n
Ä‘Æ°á»£c viáº¿t báº±ng má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh. Má»—i nÃºt trong
AST tÆ°Æ¡ng á»©ng vá»›i má»™t cáº¥u trÃºc cá»¥ thá»ƒ trong
mÃ£, nhÆ° biá»ƒu thá»©c, cÃ¢u lá»‡nh, hoáº·c khai
bÃ¡o. Cáº¥u trÃºc phÃ¢n cáº¥p cá»§a AST pháº£n Ã¡nh
cÃº phÃ¡p cá»§a ngÃ´n ngá»¯ láº­p trÃ¬nh, trá»«u tÆ°á»£ng hÃ³a
má»™t sá»‘ chi tiáº¿t cÃº phÃ¡p Ä‘á»ƒ táº­p trung vÃ o cáº¥u trÃºc
cá»‘t lÃµi.

Äá»‘i vá»›i chia tÃ¡ch dá»±a trÃªn AST, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  táº¡o ra má»™t
phÆ°Æ¡ng phÃ¡p vá»«a Ä‘Æ¡n giáº£n vá»«a cÃ³ thá»ƒ Ã¡p
dá»¥ng cho nhiá»u ngÃ´n ngá»¯ láº­p trÃ¬nh khÃ¡c nhau. Láº¥y cáº£m há»©ng
tá»« CAST (Shi et al., 2021), chÃºng tÃ´i phÃ¢n tÃ­ch mÃ£ nguá»“n
thÃ nh CÃ¢y CÃº phÃ¡p Trá»«u tÆ°á»£ng vá»›i tree_sitter1,
vÃ  duyá»‡t AST nÃ y báº±ng duyá»‡t preorder. Trong
trÆ°á»ng há»£p cÃ¡c cáº¥u trÃºc há»—n há»£p (i.e.for, if, def, etc.),
nhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 2(a), chÃºng tÃ´i Ä‘á»‹nh nghÄ©a táº­p há»£p
cÃ¡c nÃºt AST {head_block, body}, trong Ä‘Ã³ head_block
1https://github.com/tree-sitter/
py-tree-sitter

--- TRANG 5 ---
def 
â€¦def 
PIL2array
â€¦ return 
np.array
â€¦for 
fpath in
â€¦img = 
Image. 
â€¦...def 
â€¦def 
PIL2array
â€¦ return 
np.array
â€¦for 
fpath in
â€¦img = 
Image. 
â€¦...
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches.append(
PIL2array(patch))
return torch.ByteTensor(
np.array(patches[:n]))Äá»‹nh nghÄ©a
hÃ mtÃªn read_image_file
thÃ¢ntham sá»‘
tÃªn
thÃ¢n
CÃ¢u lá»‡nh return
Äá»‹nh nghÄ©a
hÃ m
â€¦
IN
 CÃ¢u lá»‡nh For
â€¦
â€¦
â€¦â€¦
thÃ¢n
 â€¦
CÃ¢u lá»‡nh return
GraphCodeBERT
GraphCodeBERT
GraphCodeBERT ...MÃ´-Ä‘un
Fusion
Cá»­a sá»• trÆ°á»£t
 Bá»™ mÃ£ hÃ³a mÃ£
 Biá»ƒu diá»…n mÃ£(a) Chia tÃ¡ch mÃ£ dá»±a trÃªn AST.
def 
â€¦def 
PIL2array
â€¦ return 
np.array
â€¦for 
fpath in
â€¦img = 
Image. 
â€¦...def 
â€¦def 
PIL2array
â€¦ return 
np.array
â€¦for 
fpath in
â€¦img = 
Image. 
â€¦...
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches.append(
PIL2array(patch))
return torch.ByteTensor(
np.array(patches[:n]))Äá»‹nh nghÄ©a
hÃ mtÃªn read_image_file
thÃ¢ntham sá»‘
tÃªn
thÃ¢n
CÃ¢u lá»‡nh return
Äá»‹nh nghÄ©a
hÃ m
â€¦
IN
 CÃ¢u lá»‡nh For
â€¦
â€¦
â€¦â€¦
thÃ¢n
 â€¦
CÃ¢u lá»‡nh return
GraphCodeBERT
GraphCodeBERT
GraphCodeBERT ...MÃ´-Ä‘un
tá»•ng há»£p
Cá»­a sá»• trÆ°á»£t
 Bá»™ mÃ£ hÃ³a mÃ£
 Biá»ƒu diá»…n mÃ£
(b) Cá»­a sá»• trÆ°á»£t vÃ  tá»•ng há»£p.
HÃ¬nh 2: Pipeline cá»§a kiáº¿n trÃºc SEA (split, encode and aggregate) Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i.
chá»‹u trÃ¡ch nhiá»‡m chia Ä‘áº§u vÃ  thÃ¢n cá»§a
cÃ¡c cÃ¢u lá»‡nh lá»“ng nhau nhÆ° cÃ¢u lá»‡nh if vÃ  While, trong khi body tÆ°Æ¡ng á»©ng vá»›i khai bÃ¡o phÆ°Æ¡ng thá»©c. Khi gáº·p má»™t cáº¥u trÃºc
há»—n há»£p, chÃºng tÃ´i chÃ¨n má»™t dáº¥u chia trÆ°á»›c vÃ  sau
head_block, hiá»‡u quáº£ chia má»™t AST lá»›n thÃ nh
má»™t chuá»—i cÃ¡c cÃ¢y con khÃ´ng chá»“ng láº¥p. Sau
Ä‘Ã³, dá»±a trÃªn viá»‡c chia AST, chÃºng tÃ´i xÃ¢y dá»±ng
táº­p há»£p máº£nh mÃ£ P báº±ng cÃ¡ch chia mÃ£ gá»‘c
tÆ°Æ¡ng á»©ng.

4.3. PhÆ°Æ¡ng phÃ¡p tá»•ng há»£p
Meanpooling / Maxpooling. Má»™t phÆ°Æ¡ng phÃ¡p
Ä‘Æ¡n giáº£n Ä‘á»ƒ tá»•ng há»£p embedding cá»§a k khá»‘i
mÃ£ lÃ  tÃ­nh toÃ¡n trung bÃ¬nh hoáº·c tá»‘i Ä‘a cá»§a
embedding cá»§a chÃºng:
ec=Mean /Max ({eb1, eb2, . . . , e bk}).(6)
Tuy nhiÃªn, má»™t háº¡n cháº¿ cá»§a meanpooling lÃ  má»—i
khá»‘i mÃ£ Ä‘Ã³ng gÃ³p nhÆ° nhau vÃ o biá»ƒu diá»…n cuá»‘i cÃ¹ng, báº¥t ká»ƒ cháº¥t lÆ°á»£ng cÃ¡ nhÃ¢n cá»§a chÃºng. TÆ°Æ¡ng
tá»±, maxpooling lÃ m ná»•i báº­t khá»‘i
cÃ³ giÃ¡ trá»‹ cao nháº¥t. Äá»ƒ giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ nÃ y
vÃ  nÃ¢ng cao quÃ¡ trÃ¬nh tá»•ng há»£p, chÃºng tÃ´i Ä‘á» xuáº¥t
viá»‡c káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p embedding cÃ³ trá»ng sá»‘.

Tá»•ng há»£p dá»±a trÃªn attention. Nháº­n thá»©c
ráº±ng khÃ´ng pháº£i táº¥t cáº£ cÃ¡c khá»‘i mÃ£ Ä‘á»u cÃ³ táº§m quan trá»ng nhÆ° nhau trong
viá»‡c biá»ƒu diá»…n cÃ¡c Ä‘oáº¡n mÃ£ dÃ i, chÃºng tÃ´i giá»›i thiá»‡u trá»ng sá»‘ tá»± thÃ­ch á»©ng Î± cho má»—i embedding khá»‘i trong

Bá»™ mÃ£ hÃ³a
Linear
Softmaxâ€¦Bá»™ mÃ£ hÃ³a
1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
ğ‘˜Ã—1
ğ‘˜Ã—1
1Ã—ğ‘‘1Ã—ğ‘‘
1Ã—ğ‘‘ğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
Mean/MaxConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯
Linear
SoftmaxMean/Max tanh
Linear1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
1Ã—ğ‘‘Mean/Max1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
ğ‘˜Ã—128
ğ‘˜Ã—1
1Ã—ğ‘‘1Ã—ğ‘‘ğ‘˜Ã—128
ğ‘˜Ã—1Bá»™ mÃ£ hÃ³aâ€¦Bá»™ mÃ£ hÃ³ağ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
ConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯
Bá»™ mÃ£ hÃ³aâ€¦Bá»™ mÃ£ hÃ³ağ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
ConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯(a) Attention má»™t lá»›p vá»›i
mean / max.
Bá»™ mÃ£ hÃ³a
Linear
Softmaxâ€¦Bá»™ mÃ£ hÃ³a
1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
ğ‘˜Ã—1
ğ‘˜Ã—1
1Ã—ğ‘‘1Ã—ğ‘‘
1Ã—ğ‘‘ğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
Mean/MaxConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯
Linear
SoftmaxMean/Max tanh
Linear1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
1Ã—ğ‘‘Mean/Max1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
ğ‘˜Ã—128
ğ‘˜Ã—1
1Ã—ğ‘‘1Ã—ğ‘‘ğ‘˜Ã—128
ğ‘˜Ã—1Bá»™ mÃ£ hÃ³aâ€¦Bá»™ mÃ£ hÃ³ağ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
ConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯
Bá»™ mÃ£ hÃ³aâ€¦Bá»™ mÃ£ hÃ³ağ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
ConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯(b) Attention hai lá»›p vá»›i
mean / max.
HÃ¬nh 3: CÃ¡c phÆ°Æ¡ng phÃ¡p tá»•ng há»£p dá»±a trÃªn attention.
quÃ¡ trÃ¬nh tá»•ng há»£p:
ec=kX
iÎ±iebi. (7)
Láº¥y cáº£m há»©ng tá»« Multi-Instance Learning dá»±a trÃªn attention
(Li et al., 2021) vÃ  Lightweight Attentional Feature
Fusion (Hu et al., 2022), chÃºng tÃ´i tÃ­nh toÃ¡n trá»ng sá»‘
{Î±1, . . . , Î± k} nhÆ° sau:
{a1, . . . , a k}=softmax (Linear ({eb1, . . . , e bk})).
(8)

--- TRANG 6 ---
Báº£ng 3: PhÃ¢n tÃ­ch chi phÃ­ tÃ­nh toÃ¡n. n lÃ  Ä‘á»™ dÃ i
chuá»—i, d lÃ  chiá»u biá»ƒu diá»…n,
k lÃ  sá»‘ khá»‘i mÃ£, l lÃ  sá»‘ lá»›p.
LÆ°u Ã½ ráº±ng chÃºng tÃ´i sá»­ dá»¥ng attention má»™t lá»›p cho SEA.
PhÆ°Æ¡ng phÃ¡p Tham sá»‘ Äá»™ phá»©c táº¡p
GraphCodeBERT 5d2Â·l O (n2Â·dÂ·l)
SEA 5d2Â·l+d O (n2
kÂ·dÂ·l)

Äá»‘i vá»›i attention má»™t lá»›p, Linear tham chiáº¿u Ä‘áº¿n má»™t lá»›p
Ä‘Æ°á»£c káº¿t ná»‘i Ä‘áº§y Ä‘á»§ chuyá»ƒn Ä‘á»•i chiá»u thÃ nh 1.
Äá»‘i vá»›i attention hai lá»›p, Linear tham chiáº¿u Ä‘áº¿n hai lá»›p
Ä‘Æ°á»£c káº¿t ná»‘i Ä‘áº§y Ä‘á»§ Ä‘áº§u tiÃªn chuyá»ƒn Ä‘á»•i chiá»u
thÃ nh 128 vÃ  sau Ä‘Ã³ chuyá»ƒn Ä‘á»•i chiá»u thÃ nh 1. HÆ¡n
ná»¯a, nhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 3(a) vÃ  HÃ¬nh
3(b), chÃºng tÃ´i khÃ¡m phÃ¡ sá»± káº¿t há»£p cá»§a attention vá»›i
cÃ¡c phÆ°Æ¡ng phÃ¡p meanpooling / maxpooling:
ec=kX
i(Î±iebi) +Mean /Max ({eb1, eb2, . . . , e bk}).
(9)

Äá»‘i vá»›i phÃ¢n tÃ­ch chi phÃ­ tÃ­nh toÃ¡n, SEA sá»­ dá»¥ng
phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t Ä‘á»ƒ giáº£m Ä‘Ã¡ng ká»ƒ Ä‘á»™ phá»©c táº¡p xuá»‘ng 1/k. Äá»™ phá»©c táº¡p gá»‘c cá»§a Graph-
CodeBERT Ä‘Æ°á»£c cho bá»Ÿi O(n2Â·dÂ·l), trong Ä‘Ã³ n, d, l
biá»ƒu thá»‹ Ä‘á»™ dÃ i chuá»—i, chiá»u biá»ƒu diá»…n,
vÃ  sá»‘ lá»›p, tÆ°Æ¡ng á»©ng. Báº±ng cÃ¡ch sá»­ dá»¥ng
phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t, Ä‘á»™ phá»©c táº¡p cho má»—i
cá»­a sá»• trá»Ÿ thÃ nh O(w2Â·dÂ·l), trong Ä‘Ã³ w biá»ƒu thá»‹
kÃ­ch thÆ°á»›c cá»­a sá»•. Äáº·t bÆ°á»›c s=w, tá»•ng
sá»‘ khá»‘i mÃ£ trá»Ÿ thÃ nh k=n
w, dáº«n Ä‘áº¿n
kÃ­ch thÆ°á»›c cá»­a sá»• w=n
k. Do Ä‘Ã³, Ä‘á»™ phá»©c táº¡p
tá»•ng thá»ƒ Ä‘Æ°á»£c Ä‘Æ¡n giáº£n hÃ³a thÃ nh:
O(kÂ·w2Â·dÂ·l) =O(kÂ·(n
k)2
Â·dÂ·l) =O(n2
kÂ·dÂ·l).(10)
Viá»‡c giáº£m Ä‘á»™ phá»©c táº¡p Ä‘Ã¡ng ká»ƒ nÃ y xuá»‘ng 1
k cho phÃ©p
SEA mÃ£ hÃ³a mÃ£ dÃ i vá»›i Ã­t chi phÃ­ bá»™ nhá»› vÃ 
tÃ­nh toÃ¡n hÆ¡n.

HÆ¡n ná»¯a, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 3, chÃºng tÃ´i quan sÃ¡t
ráº±ng so vá»›i GraphCodeBERT, SEA káº¿t
há»£p Tá»•ng há»£p attention má»™t lá»›p chá»‰ giá»›i thiá»‡u
thÃªm d tham sá»‘ cÃ³ thá»ƒ há»c. Máº·c dÃ¹
viá»‡c tÄƒng sá»‘ lÆ°á»£ng tham sá»‘ khiÃªm tá»‘n nÃ y, nÃ³ Ä‘Ã³ng vai trÃ²
then chá»‘t trong viá»‡c nÃ¢ng cao hiá»‡u quáº£ cá»§a
giai Ä‘oáº¡n tá»•ng há»£p, nhÆ° cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i sáº½ cung cáº¥p
báº±ng chá»©ng trong Pháº§n 6.1.

4.4. Xá»­ lÃ½ lÃ´
Äá»ƒ nÃ¢ng cao hiá»‡u quáº£ suy luáº­n trÃªn cÃ¡c táº­p dá»¯ liá»‡u lá»›n,
cáº§n thiáº¿t pháº£i táº¡o ra má»™t phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ lÃ´
cÃ³ kháº£ nÄƒng mÃ£ hÃ³a nhiá»u Ä‘oáº¡n mÃ£ dÃ i
Ä‘á»“ng thá»i. NhÆ° Ä‘Æ°á»£c nÃªu trong Pháº§n 4.1, chÃºng tÃ´i
cÃ³ Ä‘Æ°á»£c nhiá»u khá»‘i mÃ£ tá»« má»—i Ä‘oáº¡n mÃ£ dÃ i.
Tuy nhiÃªn, do sá»‘ lÆ°á»£ng khÃ¡c nhau cá»§a

Bá»™ mÃ£ hÃ³a mÃ£Fusion
LÃ´ 
mÃ£
Fusion
FusionLÃ´
khá»‘iEmbedding khá»‘i
â‘¡
â‘¡
â‘¡â‘ Khá»‘i 
mÃ£20221014 ä¿®æ”¹
89%11%
Ä‘á»™ dÃ i token <= 128
Ä‘á»™ dÃ i token > 128
9%
91%Ä‘á»™ dÃ i token <= 256
Ä‘á»™ dÃ i token > 256

HÃ¬nh 4: PhÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia xá»­ lÃ½ lÃ´. â‘  vÃ  â‘¡ tham chiáº¿u Ä‘áº¿n phÆ°Æ¡ng phÃ¡p káº¿t há»£p vÃ  chia.
cÃ¡c khá»‘i mÃ£ tÆ°Æ¡ng á»©ng cho cÃ¡c Ä‘oáº¡n mÃ£ dÃ i khÃ¡c nhau, viá»‡c táº¡o ra má»™t phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ lÃ´ tá»•ng quÃ¡t Ä‘áº·t ra thÃ¡ch thá»©c.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i giá»›i thiá»‡u
phÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia. NhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 4, giáº£ sá»­
kÃ­ch thÆ°á»›c lÃ´ lÃ  3 (bao gá»“m ba Ä‘oáº¡n mÃ£),
sá»‘ lÆ°á»£ng khá»‘i mÃ£ tÆ°Æ¡ng á»©ng cho má»—i
Ä‘oáº¡n láº§n lÆ°á»£t lÃ  2, 3, vÃ  1. ChÃºng tÃ´i báº¯t Ä‘áº§u báº±ng cÃ¡ch
káº¿t há»£p sÃ¡u khá»‘i mÃ£ nÃ y thÃ nh má»™t lÃ´ khá»‘i
vÃ  thiáº¿t láº­p má»™t Ã¡nh xáº¡ M liÃªn káº¿t chá»‰ sá»‘ mÃ£
vá»›i chá»‰ sá»‘ khá»‘i. Sau Ä‘Ã³, chÃºng tÃ´i Ä‘Æ°a
lÃ´ khá»‘i nÃ y vÃ o bá»™ mÃ£ hÃ³a mÃ£ song song
Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c embedding khá»‘i. Cuá»‘i cÃ¹ng, táº­n dá»¥ng
thÃ´ng tin tá»« Ã¡nh xáº¡ M, chÃºng tÃ´i tÃ¡ch
embedding thÃ nh ba nhÃ³m vÃ  Ä‘Æ°a chÃºng
vÃ o mÃ´-Ä‘un tá»•ng há»£p Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c biá»ƒu diá»…n mÃ£ riÃªng biá»‡t.

5. Thiáº¿t káº¿ thÃ­ nghiá»‡m
5.1. Táº­p dá»¯ liá»‡u
ChÃºng tÃ´i tiáº¿n hÃ nh thÃ­ nghiá»‡m trÃªn táº­p dá»¯ liá»‡u Code-
SearchNet (Husain et al., 2019) Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i, bao gá»“m sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh, i.e., Ruby,
JavaScript, Go, Python, Java, vÃ  PHP. Theo
phÆ°Æ¡ng phÃ¡p trong (Guo et al., 2021), chÃºng tÃ´i Ã¡p dá»¥ng lá»c Ä‘á»ƒ loáº¡i bá» cÃ¡c truy váº¥n cháº¥t lÆ°á»£ng tháº¥p vÃ  má»Ÿ rá»™ng táº­p
truy xuáº¥t Ä‘á»ƒ bao gá»“m toÃ n bá»™ corpus mÃ£.

5.2. Metric Ä‘Ã¡nh giÃ¡
Trong Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i, chÃºng tÃ´i sá»­ dá»¥ng hai tiÃªu chÃ­ tá»± Ä‘á»™ng phá»• biáº¿n: MRR (Mean Reciprocal Ranking) vÃ  R@k
(Ä‘á»™ chÃ­nh xÃ¡c top-k, k=1, 5, 10, 100). ChÃºng Ä‘Æ°á»£c
sá»­ dá»¥ng phá»• biáº¿n trong cÃ¡c nghiÃªn cá»©u tÃ¬m kiáº¿m mÃ£ trÆ°á»›c Ä‘Ã¢y (Lv
et al., 2015; Gu et al., 2018; Sachdev et al., 2018;
Husain et al., 2019; Feng et al., 2020; Huang et al.,
2021; Guo et al., 2021). NgoÃ i ra, chÃºng tÃ´i bÃ¡o cÃ¡o
sá»‘ lÆ°á»£ng tham sá»‘ vÃ  thá»i gian suy luáº­n lÃ m
thÆ°á»›c Ä‘o hiá»‡u quáº£.

5.3. CÃ i Ä‘áº·t thÃ­ nghiá»‡m
Baseline cá»§a chÃºng tÃ´i lÃ  GraphCodeBERT. CÃ¡c tham
sá»‘ cá»§a bá»™ mÃ£ hÃ³a mÃ£ vÃ  ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘Æ°á»£c

--- TRANG 7 ---
Báº£ng 4: Hiá»‡u suáº¥t tÃ¬m kiáº¿m cá»§a cÃ¡c biáº¿n thá»ƒ SEA khÃ¡c nhau. Táº­p dá»¯ liá»‡u: CodeSearchNet Ruby.
Cá»­a sá»• BÆ°á»›c Chia tÃ¡ch Tá»•ng há»£p MRR R@1 R@5 R@10 R@100
GraphCodeBERT â€“ â€“ â€“ â€“ 0.6948 59.3 82.1 87.3 96.5
SEA-SpaceSplitting256 128 Space Maxpooling 0.6919 58.5 82.0 87.2 95.2
256 128 Space Meanpooling 0.6929 58.3 83.0 87.4 95.6
256 128 Space Attention (hai lá»›p) 0.6940 58.7 83.4 87.1 94.8
256 128 Space Attention (hai lá»›p) + Mean 0.7490 66.3 85.2 88.9 94.4
256 128 Space Attention (má»™t lá»›p) 0.6989 59.6 82.2 86.8 95.0
256 128 Space Attention (má»™t lá»›p) + Mean 0.7495 66.1 86.3 89.0 94.3
128 64 Space Attention (má»™t lá»›p) + Mean 0.7545 66.2 87.5 90.2 95.2
64 32 Space Attention (má»™t lá»›p) + Mean 0.7431 65.1 85.6 88.7 94.0
SEA-TokenSplitting256 128 Token Attention (má»™t lá»›p) + Mean 0.7752 68.4 89.1 91.9 96.0
128 64 Token Attention (má»™t lá»›p) + Mean 0.7606 67.2 87.5 91.3 95.6
64 32 Token Attention (má»™t lá»›p) + Mean 0.7352 62.8 87.2 90.6 95.0
SEA-LineSplitting64 32 Line Attention (má»™t lá»›p) + Mean 0.7635 67.3 88.2 91.3 95.6
32 16 Line Attention (má»™t lá»›p) + Mean 0.7537 66.1 87.2 90.3 95.2
16 8 Line Attention (má»™t lá»›p) + Mean 0.7498 65.5 86.9 90.3 95.0
SEA-ASTSplitting64 32 AST Attention (má»™t lá»›p) + Mean 0.7539 65.7 91.4 95.0 97.6
32 16 AST Attention (má»™t lá»›p) + Mean 0.7762 68.8 89.1 92.0 96.4
16 8 AST Attention (má»™t lá»›p) + Mean 0.7744 68.8 88.7 91.4 96.3

khá»Ÿi táº¡o bá»Ÿi GraphCodeBERT. Äá»ƒ huáº¥n luyá»‡n, chÃºng tÃ´i
chá»n ngáº«u nhiÃªn 6 khá»‘i mÃ£ tá»« cÃ¡c khá»‘i mÃ£ Ä‘Ã£ chia cá»§a má»™t mÃ£ dÃ i. KÃ­ch thÆ°á»›c lÃ´ huáº¥n luyá»‡n lÃ 
32. Äá»ƒ Ä‘Ã¡nh giÃ¡, chÃºng tÃ´i sá»­ dá»¥ng táº¥t cáº£ cÃ¡c khá»‘i mÃ£ Ä‘Ã£ chia
cá»§a má»™t mÃ£ dÃ i. KÃ­ch thÆ°á»›c lÃ´ Ä‘Ã¡nh giÃ¡ lÃ  256.
Táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m Ä‘Æ°á»£c tiáº¿n hÃ nh trÃªn má»™t mÃ¡y vá»›i
Intel Xeon E5-2698v4 2.2Ghz 20-Core CPU vÃ 
hai Tesla V100 32GB GPU.

6. Káº¿t quáº£ thÃ­ nghiá»‡m
6.1. Cáº¥u hÃ¬nh SEA tá»‘i Æ°u
Äá»ƒ xÃ¡c Ä‘á»‹nh cáº¥u hÃ¬nh tá»‘i Æ°u cho SEA, chÃºng tÃ´i
tiáº¿n hÃ nh thÃ­ nghiá»‡m báº±ng cÃ¡ch thay Ä‘á»•i kiáº¿n trÃºc
cá»§a chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p chia tÃ¡ch mÃ£ khÃ¡c nhau vÃ  phÆ°Æ¡ng phÃ¡p tá»•ng há»£p, trong khi Ä‘o lÆ°á»ng nhá»¯ng thay Ä‘á»•i
káº¿t quáº£ trong hiá»‡u suáº¥t tÃ¬m kiáº¿m. VÃ¬ táº­p dá»¯ liá»‡u CodeSearchNet Ruby tÆ°Æ¡ng Ä‘á»‘i nhá»,
chÃºng tÃ´i táº­p trung vÃ o viá»‡c tiáº¿n hÃ nh thÃ­ nghiá»‡m trÃªn táº­p con ruby, vÃ  chÃºng tÃ´i trÃ¬nh bÃ y káº¿t quáº£ trong Báº£ng 4.

Trong cÃ¡c dÃ²ng SpaceSplitting cá»§a Báº£ng 4, chÃºng tÃ´i thÃ­ nghiá»‡m
vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tá»•ng há»£p khÃ¡c nhau nhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong
Pháº§n 4.3. CÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho tháº¥y viá»‡c sá»­ dá»¥ng báº¥t ká»³
phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Ä‘Æ¡n láº» nÃ o má»™t cÃ¡ch cÃ´ láº­p khÃ´ng mang láº¡i
cáº£i thiá»‡n hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ so
vá»›i Baseline GraphCodeBERT. Tuy nhiÃªn, khi
fusion phÆ°Æ¡ng phÃ¡p attention vá»›i meanpooling, chÃºng tÃ´i
quan sÃ¡t tháº¥y nÃ¢ng cao hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ.
Cá»¥ thá»ƒ, phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Attention (má»™t lá»›p) + Mean cáº£i thiá»‡n MRR vÃ  R@1 láº§n lÆ°á»£t
7.9% vÃ  11.5%. Do Ä‘Ã³, cho cÃ¡c
thÃ­ nghiá»‡m tiáº¿p theo, chÃºng tÃ´i chá»n sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Attention
(má»™t lá»›p) + Mean.

Trong cÃ¡c dÃ²ng SpaceSplitting, TokenSplitting,
LineSplitting, ASTSplitting cá»§a Báº£ng 4, chÃºng tÃ´i khÃ¡m phÃ¡ cÃ¡c
phÆ°Æ¡ng phÃ¡p chia mÃ£ khÃ¡c nhau, nhÆ° Ä‘Æ°á»£c chi tiáº¿t trong Pháº§n 4.2. Äá»‘i vá»›i
phÆ°Æ¡ng phÃ¡p chia dá»±a trÃªn space vÃ  token, chÃºng tÃ´i Ä‘áº·t
kÃ­ch thÆ°á»›c cá»­a sá»• tá»« 64 Ä‘áº¿n 256 do Ä‘á»™ chi tiáº¿t
nhá» hÆ¡n cá»§a viá»‡c chia. NgÆ°á»£c láº¡i, Ä‘á»‘i vá»›i phÆ°Æ¡ng phÃ¡p chia dá»±a trÃªn dÃ²ng vÃ  AST, chÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c cá»­a sá»• tá»«
16 Ä‘áº¿n 64. ÄÃ¡ng chÃº Ã½, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng phÆ°Æ¡ng phÃ¡p chia dá»±a trÃªn AST thá»ƒ hiá»‡n hiá»‡u suáº¥t xuáº¥t sáº¯c,
Ä‘áº¡t Ä‘Æ°á»£c MRR vÃ  R@1 cao nháº¥t vá»›i kÃ­ch thÆ°á»›c cá»­a sá»•
32. Káº¿t quáº£ lÃ , trong cÃ¡c thÃ­ nghiá»‡m tiáº¿p theo,
SEA tham chiáº¿u Ä‘áº¿n SEA-ASTSplitting vá»›i kÃ­ch thÆ°á»›c cá»­a sá»•
32, kÃ­ch thÆ°á»›c bÆ°á»›c 16 vÃ  phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Attention (má»™t lá»›p)
+ Mean.

6.2. So sÃ¡nh vá»›i ba Transformer thÆ°a thá»›t
Trong pháº§n nÃ y, chÃºng tÃ´i tiáº¿n hÃ nh so sÃ¡nh giá»¯a
SEA vÃ  ba Transformer thÆ°a thá»›t, BIGBIRD (Za-
heer et al., 2020), Longformer (Beltagy et al., 2020),
vÃ  LongCoder (Guo et al., 2023). BIGBIRD vÃ 
Longformer lÃ  hai Transformer hÆ°á»›ng tÃ i liá»‡u dÃ i ná»•i tiáº¿ng. LongCoder sá»­ dá»¥ng cÆ¡ cháº¿ cá»­a sá»• trÆ°á»£t Ä‘á»ƒ xá»­ lÃ½ Ä‘áº§u vÃ o mÃ£ dÃ i cho hoÃ n thiá»‡n mÃ£. Cá»¥ thá»ƒ, chÃºng tÃ´i táº­n dá»¥ng cÃ¡c mÃ´ hÃ¬nh bigbird-roberta-base2, longformer-base-40963 vÃ 
longcoder-base4, vá»›i Ä‘á»™ dÃ i token lÃ 
1024. Do BIGBIRD vÃ  Longformer khÃ´ng Ä‘Æ°á»£c
tiá»n huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u mÃ£, chÃºng tÃ´i cÅ©ng tiáº¿n hÃ nh
thÃ­ nghiá»‡m Ä‘á»ƒ khá»Ÿi táº¡o BIGBIRD vÃ  Longformer
vá»›i cÃ¡c tham sá»‘ cá»§a GraphCodeBERT. Káº¿t
quáº£ Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 5. So sÃ¡nh káº¿t quáº£ trÆ°á»›c vÃ  sau khi khá»Ÿi táº¡o BIGBIRD vÃ 
Longformer vá»›i cÃ¡c tham sá»‘ cá»§a GraphCode-
BERT, chÃºng tÃ´i tháº¥y ráº±ng káº¿t quáº£ MRR cáº£i thiá»‡n tá»«
0.2952 vÃ  0.5016 lÃªn 0.6121 vÃ  0.6595, tÆ°Æ¡ng

2https://huggingface.co/google/bigbird-roberta-base
3https://huggingface.co/allenai/longformer-base-
4096
4https://huggingface.co/microsoft/longcoder-base

--- TRANG 8 ---
Báº£ng 5: So sÃ¡nh vá»›i Transformer thÆ°a thá»›t. KÃ½ hiá»‡u (G) chá»‰ ra ráº±ng mÃ´ hÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o
vá»›i cÃ¡c tham sá»‘ GraphCodeBERT. Thá»i gian suy luáº­n mÃ£ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng cÃ¡ch chá»n ngáº«u nhiÃªn 1000
mÃ£ vÃ  tÃ­nh toÃ¡n thá»i gian suy luáº­n trung bÃ¬nh. ChÃºng tÃ´i láº·p láº¡i má»—i láº§n tÃ­nh toÃ¡n thÃ­ nghiá»‡m ba
láº§n vÃ  bÃ¡o cÃ¡o trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n. Táº­p dá»¯ liá»‡u: CodeSearchNet Ruby. SEA vÆ°á»£t trá»™i hÆ¡n
cÃ¡c mÃ´ hÃ¬nh khÃ¡c má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ ( p <0.01).
MÃ´ hÃ¬nh #Param. Äá»™ dÃ i Token Thá»i gian suy luáº­n MRR R@1 R@5 R@10
GraphCodeBERT 124.6M 256 6.3 Â±0.3ms 0.6948 59.3 82.1 87.3
BIGBIRD 127.5M 1024 20.1 Â±0.2ms 0.2952 19.2 39.8 51.1
BIGBIRD (G) 127.5M 1024 19.8 Â±0.0ms 0.6121 50.8 74.2 80.7
Longformer 148.7M 1024 33.7 Â±0.2ms 0.5128 39.9 65.3 72.4
Longformer (G) 148.7M 1024 33.7 Â±0.1ms 0.6595 55.1 79.4 84.0
LongCoder 149.6M 1024 68.6 Â±0.2ms 0.4718 35.8 61.1 67.8
SEA 124.6M - 7.2 Â±0.5ms 0.7762 68.8 89.1 92.0
- w/o combine-divide 124.6M - 24.3 Â±2.4ms 0.7762 68.8 89.1 92.0

á»©ng. ChÃºng tÃ´i quy káº¿t khoáº£ng cÃ¡ch hiá»‡u suáº¥t nÃ y cho
nhu cáº§u tÃ¡i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ban Ä‘áº§u
Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u ngÃ´n ngá»¯ tá»± nhiÃªn. ChÃºng tÃ´i quan
sÃ¡t tháº¥y MRR cá»§a LongCoder lÃ  0.4718, biá»ƒu thá»‹
má»™t sá»± giáº£m Ä‘Ã¡ng ká»ƒ so vá»›i
GraphCodeBERT, gá»£i Ã½ ráº±ng LongCoder cÃ³ thá»ƒ
chá»§ yáº¿u phÃ¹ há»£p cho cÃ¡c nhiá»‡m vá»¥ HoÃ n thiá»‡n MÃ£. ChÃºng tÃ´i
cÅ©ng tiáº¿n hÃ nh t-test giá»¯a SEA cá»§a chÃºng tÃ´i vÃ  cÃ¡c
baseline khÃ¡c, vÃ  káº¿t quáº£ chá»©ng minh ráº±ng SEA
vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ hÆ¡n táº¥t cáº£ cÃ¡c baseline Transformer thÆ°a thá»›t ( p <0.01), lÃ m ná»•i báº­t hiá»‡u suáº¥t vÆ°á»£t trá»™i cá»§a nÃ³ trong lÄ©nh vá»±c tÃ¬m kiáº¿m mÃ£.

Vá» sá»‘ lÆ°á»£ng tham sá»‘ mÃ´ hÃ¬nh vÃ  hiá»‡u quáº£ tÃ¬m kiáº¿m, SEA ná»•i báº­t vÃ¬ nÃ³ cÃ³ sá»‘ lÆ°á»£ng tham
sá»‘ tháº¥p hÆ¡n vÃ  thá»i gian suy luáº­n ngáº¯n hÆ¡n so
vá»›i BIGBIRD, Longformer vÃ  LongCoder. ÄÃ¡ng chÃº Ã½ ráº±ng sá»‘ lÆ°á»£ng tham sá»‘ cá»§a SEA Ä‘Æ°á»£c cÄƒn chá»‰nh cháº·t cháº½ vá»›i GraphCodeBERT, chá»‰ khÃ¡c
bá»Ÿi viá»‡c thÃªm má»™t lá»›p attention duy nháº¥t. Tuy nhiÃªn,
thay Ä‘á»•i nhá» nÃ y dáº«n Ä‘áº¿n sá»± tÄƒng cÆ°á»ng Ä‘Ã¡ng ká»ƒ trong
hiá»‡u suáº¥t tÃ¬m kiáº¿m. ChÃºng tÃ´i cÅ©ng trÃ¬nh bÃ y káº¿t quáº£ thÃ­ nghiá»‡m mÃ  khÃ´ng sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia trong Báº£ng 5. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng trong khi hiá»‡u suáº¥t tÃ¬m kiáº¿m váº«n á»•n Ä‘á»‹nh, thá»i gian suy luáº­n tÄƒng hÆ¡n ba láº§n. Äiá»u nÃ y lÃ m ná»•i báº­t sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong thá»i gian suy luáº­n
do phÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia mang láº¡i,
tá»« Ä‘Ã³ xÃ¡c nháº­n hiá»‡u quáº£ cá»§a nÃ³ trong viá»‡c tÄƒng tá»‘c
quÃ¡ trÃ¬nh suy luáº­n cá»§a mÃ´ hÃ¬nh.

6.3. Hiá»‡u suáº¥t SEA trÃªn cÃ¡c Ä‘á»™ dÃ i mÃ£ khÃ¡c nhau
Äá»ƒ khÃ¡m phÃ¡ sá»± cáº£i thiá»‡n cá»§a SEA Ä‘á» xuáº¥t
cho cÃ¡c Ä‘oáº¡n mÃ£ cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau, chÃºng tÃ´i trÃ¬nh bÃ y
so sÃ¡nh hiá»‡u suáº¥t tÃ¬m kiáº¿m giá»¯a phÆ°Æ¡ng phÃ¡p baseline GraphCodeBERT vÃ  SEA dÆ°á»›i
cÃ¡c Ä‘á»™ dÃ i token mÃ£ ground-truth khÃ¡c nhau. Káº¿t
quáº£ Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 5.

ÄÃ¡ng chÃº Ã½, hiá»‡u suáº¥t truy xuáº¥t cá»§a má»—i táº­p con truy váº¥n thá»ƒ hiá»‡n nÃ¢ng cao rÃµ rá»‡t, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i káº¿t quáº£ truy xuáº¥t mÃ£ dÃ i. ChÃºng tÃ´i quy cÃ¡ch cáº£i thiá»‡n nÃ y cho hai yáº¿u tá»‘ quan trá»ng. Thá»© nháº¥t, mÃ´-Ä‘un tá»•ng há»£p cá»§a SEA báº¯t giá»¯ thÃ­ch á»©ng vÃ 
káº¿t há»£p thÃ´ng tin tá»« cÃ¡c phÃ¢n Ä‘oáº¡n Ä‘a dáº¡ng cá»§a
mÃ£ dÃ i, dáº«n Ä‘áº¿n biá»ƒu diá»…n mÃ£ toÃ n diá»‡n
vÃ  thÃ´ng tin phong phÃº hÆ¡n. Thá»© hai,
phÆ°Æ¡ng phÃ¡p chia mÃ£ Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi SEA cÃ³ thá»ƒ Ä‘Æ°á»£c
xem nhÆ° má»™t hÃ¬nh thá»©c data augmentation, cung cáº¥p
bá»‘i cáº£nh vÃ  biáº¿n thá»ƒ bá»• sung há»— trá»£ trong viá»‡c tÄƒng cÆ°á»ng
biá»ƒu diá»…n mÃ£. TÃ³m láº¡i, SEA
mang láº¡i biá»ƒu diá»…n mÃ£ máº¡nh máº½ hÆ¡n, nÃ¢ng cao Ä‘Ã¡ng ká»ƒ
hiá»‡u suáº¥t truy xuáº¥t tá»•ng thá»ƒ.

0.600.620.640.660.680.700.720.740.760.780.80
[0, 256) [256, 512) [512, 768) [768, 1024) [1024, 1943)MRR
Äá»™ dÃ i token mÃ£
GraphCodeBERT
 SEA

HÃ¬nh 5: So sÃ¡nh hiá»‡u suáº¥t giá»¯a
GraphCodeBERT vÃ  SEA trong cÃ¡c Ä‘á»™ dÃ i token mÃ£ ground-truth khÃ¡c nhau. So vá»›i GraphCodeBERT,
SEA Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ ( p <0.01) cho cÃ¡c Ä‘á»™ dÃ i token mÃ£ khÃ¡c nhau.

6.4. So sÃ¡nh baseline trÃªn
nhiá»u ngÃ´n ngá»¯ láº­p trÃ¬nh
Äá»ƒ Ä‘áº£m báº£o so sÃ¡nh cÃ´ng báº±ng vÃ  cÃ³ thá»ƒ tÃ¡i táº¡o, chÃºng tÃ´i
cáº©n tháº­n chá»n cÃ¡c baseline dá»±a trÃªn tiá»n huáº¥n luyá»‡n Ä‘Ã¡p á»©ng ba tiÃªu chÃ­ sau: 1) MÃ£ nguá»“n
cÃ³ sáºµn cÃ´ng khai; 2) MÃ´ hÃ¬nh tá»•ng thá»ƒ cÃ³ thá»ƒ
thÃ­ch á»©ng vá»›i táº¥t cáº£ sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh
trÃªn táº­p dá»¯ liá»‡u CodeSearchNet; 3) BÃ i bÃ¡o Ä‘Æ°á»£c
peer-review náº¿u nÃ³ Ä‘Æ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o nghiÃªn cá»©u

--- TRANG 9 ---
Báº£ng 6: MRR trÃªn sÃ¡u ngÃ´n ngá»¯ cá»§a táº­p dá»¯ liá»‡u CodeSearchNet. SEA á»Ÿ Ä‘Ã¢y tham chiáº¿u Ä‘áº¿n SEA-ASTSplitting
vá»›i kÃ­ch thÆ°á»›c cá»­a sá»• 32 vÃ  bÆ°á»›c 16. SEA +RoBERTa tham chiáº¿u Ä‘áº¿n SEA vá»›i RoBERTa lÃ m bá»™ mÃ£ hÃ³a mÃ£.
SEA vÆ°á»£t trá»™i hÆ¡n baseline má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ ( p <0.01).
MÃ´ hÃ¬nh / PhÆ°Æ¡ng phÃ¡p Ruby Javascript Go Python Java Php Tá»•ng thá»ƒ
RoBERTa 0.587 0.517 0.850 0.587 0.599 0.560 0.617
UniXcoder 0.586 0.603 0.881 0.695 0.687 0.644 0.683
CodeBERT 0.679 0.620 0.882 0.672 0.676 0.628 0.693
GraphCodeBERT 0.703 0.644 0.897 0.692 0.691 0.649 0.713
SEA +RoBERTa 0.651 (10.9% â†‘) 0.593 (14.6% â†‘) 0.879 (3.5% â†‘) 0.633 (7.9% â†‘) 0.666 (11.1% â†‘) 0.647 (15.6% â†‘) 0.678 (10.0% â†‘)
SEA +UniXcoder 0.648 (10.7% â†‘) 0.692 (14.8% â†‘) 0.896 (1.8% â†‘) 0.707 (1.7% â†‘) 0.739 (7.5% â†‘) 0.712 (10.5% â†‘) 0.732 (7.3% â†‘)
SEA +CodeBERT 0.742 (9.3% â†‘) 0.696 (12.3% â†‘) 0.905 (2.6% â†‘) 0.714 (6.2% â†‘) 0.732 (8.3% â†‘) 0.711 (13.2% â†‘) 0.750 (8.3% â†‘)
SEA +GraphCodeBERT 0.776 (10.4% â†‘) 0.742 (15.2% â†‘) 0.921 (2.7% â†‘) 0.754 (8.9% â†‘) 0.768 (11.1% â†‘) 0.748 (15.3% â†‘) 0.785 (10.1% â†‘)

per. Do Ä‘Ã³, chÃºng tÃ´i chá»n bá»‘n phÆ°Æ¡ng phÃ¡p sÃ¢u end-to-end: RoBERTa (Liu et al., 2019), UniX-
coder (Guo et al., 2022), CodeBERT (Feng et al.,
2020), vÃ  GraphCodeBERT (Guo et al., 2021).
Trong Báº£ng 6, chÃºng tÃ´i trÃ¬nh bÃ y káº¿t quáº£ MRR, chá»©ng minh ráº±ng SEA vÆ°á»£t trá»™i hÆ¡n táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p trÃªn
táº¥t cáº£ sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh. ÄÃ¡ng chÃº Ã½, káº¿t luáº­n nÃ y
váº«n nháº¥t quÃ¡n cho metric recall vÃ 
má»™t biáº¿n thá»ƒ khÃ¡c cá»§a SEA, káº¿t quáº£ cÃ³ thá»ƒ
Ä‘Æ°á»£c tÃ¬m tháº¥y trong gÃ³i sao chÃ©p cá»§a chÃºng tÃ´i. Nhá»¯ng phÃ¡t
hiá»‡n nÃ y cá»§ng cá»‘ tÃ­nh vÆ°á»£t trá»™i cá»§a SEA so vá»›i
cÃ¡c baseline dá»±a trÃªn tiá»n huáº¥n luyá»‡n trÃªn cÃ¡c
ngÃ´n ngá»¯ láº­p trÃ¬nh Ä‘a dáº¡ng.

7. Káº¿t luáº­n
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giáº£i quyáº¿t thÃ¡ch thá»©c mÃ´ hÃ¬nh hÃ³a hiá»‡u quáº£ mÃ£ dÃ i cho tÃ¬m kiáº¿m mÃ£. ChÃºng tÃ´i
giá»›i thiá»‡u SEA, má»™t phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ mang láº¡i
biá»ƒu diá»…n mÃ£ Ä‘Æ°á»£c cáº£i thiá»‡n cho cÃ¡c Ä‘oáº¡n mÃ£ dÃ i. Máº·c dÃ¹ Ä‘Æ¡n giáº£n, káº¿t quáº£ thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i
cho tháº¥y hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t Ä‘Ã¡ng chÃº Ã½
cá»§a SEA. ChÃºng tÃ´i tin ráº±ng cÃ´ng trÃ¬nh nÃ y má»Ÿ ra nhá»¯ng kháº£ nÄƒng má»›i cho tÃ¬m kiáº¿m mÃ£.

8. TuyÃªn bá»‘ Ä‘áº¡o Ä‘á»©c
CÃ¡c má»Ÿ rá»™ng vÃ  á»©ng dá»¥ng tÆ°Æ¡ng lai phÃ¡t sinh tá»« cÃ´ng
trÃ¬nh cá»§a chÃºng tÃ´i nÃªn chÃº Ã½ Ä‘áº¿n tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng
cá»§a viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh quy mÃ´ lá»›n. ChÃºng nÃªn
tÃ­ch cá»±c trÃ¡nh viá»‡c sá»­ dá»¥ng sai má»¥c Ä‘Ã­ch tiá»m nÄƒng báº±ng cÃ¡ch tÃ¬m kiáº¿m Ã½ Ä‘á»‹nh Ä‘á»™c háº¡i. Tuy nhiÃªn, khÃ´ng cÃ³ kháº£ nÄƒng mÃ´ hÃ¬nh trong hÃ¬nh thá»©c hiá»‡n táº¡i sáº½ dáº«n Ä‘áº¿n tÃ¡c Ä‘á»™ng nhÆ° váº­y
trong tÆ°Æ¡ng lai gáº§n. MÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cÅ©ng cÃ³ tiá»m nÄƒng táº¡o ra tÃ¡c Ä‘á»™ng tÃ­ch cá»±c trong cÃ¡c lÄ©nh vá»±c nhÆ°
tÃ¬m kiáº¿m mÃ£, hiá»ƒu mÃ£ dÃ i vÃ  biá»ƒu diá»…n mÃ£.

9. Lá»i cáº£m Æ¡n
CÃ´ng viá»‡c Ä‘Æ°á»£c mÃ´ táº£ trong bÃ i bÃ¡o nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi CCF-Huawei Populus Grove Fund CCF-
HuaweiSE202301.

10. TÃ i liá»‡u tham kháº£o thÆ° má»¥c
Joshua Ainslie, Santiago OntaÃ±Ã³n, Chris Alberti,
Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang,
vÃ  Li Yang. 2020. ETC: encoding long and
structured inputs in transformers. Trong EMNLP.

Miltiadis Allamanis, Henry Jackson-Flux, vÃ  Marc
Brockschmidt. 2021. Self-supervised bug detec-
tion and repair. Trong NeurIPS.

Uri Alon, Roy Sadaka, Omer Levy, vÃ  Eran Yahav.
2020. Structural language models of code. Trong
ICML.

Iz Beltagy, Matthew E Peters, vÃ  Arman Cohan.
2020. Longformer: The long-document trans-
former.arXiv.

Nghi DQ Bui, Yijun Yu, vÃ  Lingxiao Jiang. 2021.
Treecaps: Tree-based capsule networks for
source code processing. Trong AAAI.

Yitian Chai, Hongyu Zhang, Beijun Shen, vÃ  Xi-
aodong Gu. 2022. Cross-domain deep code
search with few-shot meta learning. arXiv.

Long Chen, Wei Ye, vÃ  Shikun Zhang. 2019. Cap-
turing source code semantics via tree-based con-
volution over api-enhanced ast. Trong CF.

Rewon Child, Scott Gray, Alec Radford, vÃ  Ilya
Sutskever. 2019. Generating long sequences
with sparse transformers. arXiv.

GonÃ§alo M Correia, Vlad Niculae, vÃ  AndrÃ© FT
Martins. 2019. Adaptively sparse transformers.
Trong EMNLP-IJCNLP.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G
Carbonell, Quoc Le, vÃ  Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language models
beyond a fixed-length context. Trong ACL.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, vÃ 
Kristina Toutanova. 2019. BERT: pre-training

--- TRANG 10 ---
of deep bidirectional transformers for language
understanding. Trong NAACL-HLT.

Yali Du vÃ  Zhongxing Yu. 2023. Pre-training code
representation with semantic flow graph for ef-
fective bug localization. Trong FSE/ESEC.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,
Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, vÃ  Ming Zhou. 2020.
Codebert: A pre-trained model for programming
and natural languages. Trong EMNLP.

Luyu Gao vÃ  Jamie Callan. 2022. Long document
re-ranking with modular re-ranker. Trong SIGIR.

Dobrik Georgiev, Marc Brockschmidt, vÃ  Miltiadis
Allamanis. 2022. Heat: Hyperedge attention net-
works.arXiv.

Wenchao Gu, Yanlin Wang, Lun Du, Hongyu
Zhang, Shi Han, Dongmei Zhang, vÃ  Michael
Lyu. 2022. Accelerating code search with deep
hashing and code classification. Trong ACL.

Xiaodong Gu, Hongyu Zhang, vÃ  Sunghun Kim.
2018. Deep code search. Trong ICSE.

Michael GÃ¼nther, Jackmin Ong, Isabelle Mohr,
Alaeddine Abdessalem, Tanguy Abel, Moham-
mad Kalim Akram, Susana Guzman, Georgios
Mastrapas, Saba Sturua, Bo Wang, et al. 2023.
Jina embeddings 2: 8192-token general-purpose
text embeddings for long documents. arXiv.

Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming
Zhou, vÃ  Jian Yin. 2022. Unixcoder: Unified
cross-modal pre-training for code representation.
Trong ACL.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-
fano, Shao Kun Deng, Colin B. Clement, Dawn
Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
vÃ  Ming Zhou. 2021. Graphcodebert: Pre-
training code representations with data flow. Trong
ICLR.

Daya Guo, Canwen Xu, Nan Duan, Jian Yin, vÃ  Ju-
lian J. McAuley. 2023. Longcoder: A long-range
pre-trained language model for code completion.
Trong ICML, Proceedings of Machine Learning Re-
search. PMLR.

Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,
Xiangyang Xue, vÃ  Zheng Zhang. 2019. Star-
transformer. Trong NAACL-HLT. Association for Com-
putational Linguistics.

Vincent J Hellendoorn, Charles Sutton, Rishabh
Singh, Petros Maniatis, vÃ  David Bieber. 2019.
Global relational models of source code. Trong ICLR.

Emily Hill, Lori Pollock, vÃ  K Vijay-Shanker. 2011.
Improving source code search with natural lan-
guage phrasal representations of method signa-
tures. Trong ASE. IEEE.

Fan Hu, Aozhu Chen, Ziyue Wang, Fangming Zhou,
Jianfeng Dong, vÃ  Xirong Li. 2022. Lightweight
attentional feature fusion: A new baseline for
text-to-video retrieval. Trong ECCV. Springer.

Fan Hu, Yanlin Wang, Lun Du, Xirong Li, Hongyu
Zhang, Shi Han, vÃ  Dongmei Zhang. 2023. Re-
visiting code search in a two-stage paradigm. Trong
WSDM.

Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong,
Ke Xu, Daxin Jiang, Ming Zhou, vÃ  Nan Duan.
2021. Cosqa: 20,000+ web queries for code
search and question answering. Trong ACL.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Mil-
tiadis Allamanis, vÃ  Marc Brockschmidt. 2019.
Codesearchnet challenge: Evaluating the state
of semantic code search. arXiv.

Paul Jaccard. 1901. Ã‰tude comparative de la distri-
bution florale dans une portion des alpes et des
jura. Bull Soc Vaudoise Sci Nat, pages 547â€“579.

Jyun-Yu Jiang, Chenyan Xiong, Chia-Jung Lee, vÃ 
Wei Wang. 2020. Long document ranking with
query-directed sparse transformer. Trong EMNLP
Findings.

Seohyun Kim, Jinman Zhao, Yuchi Tian, vÃ  Satish
Chandra. 2021. Code prediction by feeding trees
to transformers. Trong ICSE.

Nikita Kitaev, Lukasz Kaiser, vÃ  Anselm Levskaya.
2019. Reformer: The efficient transformer. Trong
ICLR.

Canjia Li, Andrew Yates, Sean MacAvaney, Ben He,
vÃ  Yingfei Sun. 2020. Parade: Passage rep-
resentation aggregation for document reranking.
ACM Transactions on Information Systems.

Xirong Li, Yang Zhou, Jie Wang, Hailan Lin,
Jianchun Zhao, Dayong Ding, Weihong Yu, vÃ 
Youxin Chen. 2021. Multi-modal multi-instance
learning for retinal disease recognition. Trong
ACM MM.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, vÃ  Veselin Stoyanov.
2019. Roberta: A robustly optimized bert pre-
training approach. arXiv.

Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei
Wang, Dongmei Zhang, vÃ  Jianjun Zhao. 2015.
Codehow: Effective code search based on api
understanding and extended boolean model (e).
Trong ASE.

--- TRANG 11 ---
Y Ma, Yali Du, vÃ  Ming Li. 2023. Capturing the
long-distance dependency in the control flow
graph via structural-guided attention for bug lo-
calization. Trong IJCAI.

Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, vÃ 
Xiaochen Li. 2016. Query expansion based
on crowd knowledge for code search. IEEE
Transactions on Services Computing, pages
771â€“783.

Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, vÃ 
Zhi Jin. 2021. Integrating tree path in transformer
for code representation. Trong NeurIPS.

Stephen Robertson vÃ  Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and
beyond. Now Publishers Inc.

Stephen E Robertson vÃ  K Sparck Jones. 1976.
Relevance weighting of search terms. Journal
of the American Society for Information science,
pages 129â€“146.

Barbara Rosario. 2000. Latent semantic indexing:
An overview. Techn. rep. INFOSYS, pages 1â€“16.

Aurko Roy, Mohammad Saffar, Ashish Vaswani,
vÃ  David Grangier. 2021. Efficient content-
based sparse attention with routing trans-
formers. Transactions of the Association for
Computational Linguistics, 9:53â€“68.

Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun
Kim, Koushik Sen, vÃ  Satish Chandra. 2018.
Retrieval on source code: a neural code search.
Trong MAPL.

Abdus Satter vÃ  Kazi Sakib. 2016. A search log
mining based query expansion technique to im-
prove effectiveness in code search. Trong ICCIT,
pages 586â€“591. IEEE.

Hinrich SchÃ¼tze, Christopher D Manning, vÃ 
Prabhakar Raghavan. 2008. Introduction to
information retrieval, volume 39. Cambridge Uni-
versity Press Cambridge.

Rico Sennrich, Barry Haddow, vÃ  Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. Trong ACL.

Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,
Shi Han, Dongmei Zhang, vÃ  Hongbin Sun.
2021. Cast: Enhancing code summarization
with hierarchical splitting and reconstruction of
abstract syntax trees. Trong EMNLP.

Weisong Sun, Chunrong Fang, Yuchen Chen,
Guanhong Tao, Tingxu Han, vÃ  Quanjun Zhang.
2022. Code search based on context-aware code
translation. arXiv.

Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun,
Lili Mou, vÃ  Lu Zhang. 2020. Treegen: A tree-
based transformer architecture for code genera-
tion. Trong AAAI.

Tomoki Tsujimura, Koshi Yamada, Ryuki Ida,
Makoto Miwa, vÃ  Yutaka Sasaki. 2023. Contex-
tualized medication event extraction with strid-
ing ner and multi-turn qa. Journal of Biomedical
Informatics, page 104416.

Thanh Van Nguyen, Anh Tuan Nguyen, Hung Dang
Phan, Trong Duc Nguyen, vÃ  Tien N Nguyen.
2017. Combining word2vec with revised vector
space model for better code retrieval. Trong ICSE-C.
IEEE.

Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu,
Zhou Zhao, Jian Wu, vÃ  Philip S. Yu. 2019.
Multi-modal attention network learning for seman-
tic source code retrieval. Trong ASE.

Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh
Nallapati, vÃ  Bing Xiang. 2019. Multi-passage
bert: A globally normalized bert model for open-
domain question answering. Trong EMNLP.

Liu Yang, Mingyang Zhang, Cheng Li, Michael
Bendersky, vÃ  Marc Najork. 2020. Beyond
512 tokens: Siamese multi-depth transformer-
based hierarchical encoder for long-form docu-
ment matching. Trong CIKM.

Yangrui Yang vÃ  Qing Huang. 2017. Iecs: Intent-
enforced code search via extended boolean
model. Journal of Intelligent & Fuzzy Systems,
pages 2565â€“2576.

Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, vÃ 
Zheng Zhang. 2019. Bp-transformer: Modelling
long-range context via binary partitioning. arXiv.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, vÃ  Amr Ahmed. 2020. Big bird:
Transformers for longer sequences. Trong NeurIPS.

Jian Zhang, Xu Wang, Hongyu Zhang, Hailong
Sun, Kaixuan Wang, vÃ  Xudong Liu. 2019a. A
novel neural source code representation based
on abstract syntax tree. Trong ICSE.

Xingxing Zhang, Furu Wei, vÃ  Ming Zhou. 2019b.
Hibert: Document level pre-training of hierarchi-
cal bidirectional transformers for document sum-
marization. Trong ACL.
