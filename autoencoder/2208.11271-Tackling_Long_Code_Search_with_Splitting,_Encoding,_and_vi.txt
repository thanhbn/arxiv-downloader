# 2208.11271.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/autoencoder/2208.11271.pdf
# Kích thước tệp: 2936530 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Giải quyết tìm kiếm mã dài bằng phương pháp chia tách, mã hóa và tổng hợp
Fan Hu1∗, Yanlin Wang2†‡, Lun Du3,
Hongyu Zhang4, Shi Han3, Dongmei Zhang3, Xirong Li1
1Đại học Nhân dân Trung Quốc
2Khoa Kỹ thuật Phần mềm, Đại học Trung Sơn
3Microsoft
4Đại học Trùng Khánh
Tóm tắt
Tìm kiếm mã với ngôn ngữ tự nhiên giúp chúng ta tái sử dụng các đoạn mã hiện có. Nhờ các mô hình tiền huấn luyện dựa trên Transformer, hiệu suất tìm kiếm mã đã được cải thiện đáng kể. Tuy nhiên, do độ phức tạp bậc hai của cơ chế tự chú ý đa đầu, có giới hạn về độ dài token đầu vào. Để huấn luyện hiệu quả trên GPU tiêu chuẩn như V100, các mô hình mã tiền huấn luyện hiện có, bao gồm GraphCodeBERT, CodeBERT, RoBERTa (code), mặc định lấy 256 token đầu tiên, khiến chúng không thể biểu diễn thông tin đầy đủ của mã dài lớn hơn 256 token. Để giải quyết vấn đề mã dài, chúng tôi đề xuất một baseline mới SEA (Split, Encode and Aggregate), chia mã dài thành các khối mã, mã hóa các khối này thành embedding, và tổng hợp chúng để có được biểu diễn mã dài toàn diện. Với SEA, chúng ta có thể trực tiếp sử dụng các mô hình tiền huấn luyện dựa trên Transformer để mô hình hóa mã dài mà không thay đổi cấu trúc bên trong và tái huấn luyện. Chúng tôi cũng so sánh SEA với các phương pháp Transformer thưa thớt. Với GraphCodeBERT làm bộ mã hóa, SEA đạt điểm mean reciprocal ranking tổng thể là 0.785, cao hơn 10.1% so với GraphCodeBERT trên benchmark CodeSearchNet, chứng minh SEA là một baseline mạnh cho tìm kiếm mã dài.
Từ khóa: tìm kiếm mã, hiểu mã dài, biểu diễn mã

1. Giới thiệu
Một kỹ thuật tìm kiếm mã tốt giúp các nhà phát triển
thúc đẩy phát triển phần mềm bằng cách tìm kiếm
các đoạn mã sử dụng ngôn ngữ tự nhiên. Những tiến
bộ gần đây đã chứng minh hiệu quả của các phương pháp tiền huấn luyện mã dựa trên Transformer,
bao gồm CodeBERT (Feng et al., 2020), CoCLR
(Huang et al., 2021), và GraphCodeBERT (Guo
et al., 2021), đã cải thiện đáng kể hiệu suất tìm kiếm mã thông qua tiền huấn luyện tự giám sát trên
corpus mã quy mô lớn.

Tuy nhiên, những phương pháp này đối mặt với một hạn chế cố hữu. Độ phức tạp tính toán và bộ nhớ
của cơ chế tự chú ý trong Transformer gốc tăng theo
bậc hai với độ dài đầu vào, tạo ra ràng buộc về độ dài đầu vào khoảng 512 token. Để huấn luyện hiệu quả trên GPU tiêu chuẩn như
V100, GraphCodeBERT và CodeBERT chỉ xem xét 256 token đầu tiên của các đoạn mã và loại
bỏ bất kỳ token nào vượt quá giới hạn này. Tuy nhiên, hạn chế độ dài này có thể dẫn đến vấn đề về độ chính xác, đặc biệt đối với các đoạn mã dài. Ví dụ, khi
kiểm tra các trường hợp khó của GraphCode-
BERT, chúng tôi thấy rằng GraphCodeBERT có hiệu suất thấp cho một số đoạn mã dài mà
thông tin quan trọng nằm ở cuối. Như
được minh họa trong Hình 1, các từ khóa "Tensor" và
"patches" xuất hiện sau ngưỡng cắt 256-token được đặt bởi
GraphCodeBERT, dẫn đến việc chúng bị loại trừ khỏi
xem xét. Do đó, đoạn mã
tương ứng được xếp hạng ở vị trí 21,148.

Chúng tôi tiếp tục tiến hành các nghiên cứu thực nghiệm về
GraphCodeBERT trên tập dữ liệu CodeSearch-
Net được sử dụng công khai (Husain et al., 2019), và quan sát thấy
sự giảm dần hiệu suất tìm kiếm khi độ
dài của mã ground-truth trong truy vấn tăng lên (tham khảo Bảng 1). Vấn đề này tương tự
với vấn đề văn bản dài trong xử lý ngôn ngữ tự
nhiên, mà đã có nhiều phương pháp được đề xuất,
bao gồm xử lý phân cấp (Zhang
et al., 2019b), attention thưa thớt (Child et al., 2019;
Beltagy et al., 2020), và tái hiện ở mức đoạn
(Dai et al., 2019). Tuy nhiên, việc áp dụng trực tiếp các
phương pháp này cho mã dài đặt ra hai thách thức.
Thứ nhất, các kỹ thuật này thay đổi cấu trúc bên
trong của mô hình Transformer, có thể làm cho
các tham số tiền huấn luyện hiện có trở nên không hợp lệ. Thứ
hai, mã dài khác với văn bản dài ở chỗ nó là một
ngôn ngữ có cấu trúc cao. Không giống như một tài liệu văn bản dài
có thể được xem như một tổng thể gắn kết với
ngữ nghĩa hoàn chỉnh, ngữ nghĩa của mã là không
liên tục, và các hàm khác nhau được phân bố
trên các vị trí khác nhau. Các thí nghiệm so sánh
được thực hiện trong Phần 6.2 cung cấp bằng chứng arXiv:2208.11271v3  [cs.SE]  26 Mar 2024

--- TRANG 2 ---
Xếp hạng 1 (kết quả sai):
defpatch(self, *args, **kwargs):
return super(Deposit, self).patch(*args, **
kwargs)
Xếp hạng 21148 (ground truth):
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
img = Image.open(fpath)
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches .append(
PIL2array(patch))
return torch.Byte Tensor(
np.array( patches [:n]))256 tokensTrả về một tensor chứa các patches. 
Hình 1: Ví dụ về GraphCodeBERT.
GraphCodeBERT cắt bỏ các token vượt quá 256 token. Các token chính được đánh dấu màu vàng.
hỗ trợ những lo ngại này.

Do đó, mục tiêu của chúng tôi là chia mã dài trong khi
bảo tồn thông tin ngữ nghĩa của nó. Chúng tôi mong muốn
đạt được điều này mà không thay đổi cấu trúc bên trong của
các mô hình tiền huấn luyện dựa trên Transformer hoặc yêu cầu
tái huấn luyện. Để giải quyết điều này, chúng tôi đề xuất SEA
(Split, Encode, và Aggregate) để xử lý mã dài
và có được biểu diễn mã được cải thiện.

Như được mô tả trong Hình 2, quá trình bao gồm
việc chia mã dài thành một tập hợp các mảnh mã,
sau đó sử dụng phương pháp cửa sổ trượt để
tạo ra một tập hợp khối mã chồng lấp một phần.
Các bộ mã hóa mã hiện có sau đó được sử dụng để có được
embedding cho mỗi khối mã. Cuối cùng, những
embedding này được tổng hợp để tạo ra biểu diễn cho toàn bộ mã dài. Thông qua các thí nghiệm rộng rãi, chúng tôi đã thấy rằng phương pháp chia tách dựa trên AST
đề xuất và phương pháp tổng hợp dựa trên attention vượt trội hơn các kỹ thuật khác cho việc chia tách và tổng hợp. Do số lượng khối mã khác nhau
có được từ các đoạn mã khác nhau, việc hoạt động song song trở nên thách thức. Để
giải quyết vấn đề này, chúng tôi đã thiết kế một mô-đun kết hợp-chia cho việc tăng tốc. Điều quan trọng cần lưu ý là SEA không phụ thuộc vào bộ mã hóa, nghĩa là nó có thể
được sử dụng với các bộ mã hóa dựa trên Transformer khác nhau.
Khi so sánh với các baseline bộ mã hóa dựa trên Transformer khác nhau, SEA đạt được cải thiện đáng kể trong hiệu suất mean reciprocal ranking (MRR), từ 7% đến 10%.

Bảng 1: Hiệu suất tìm kiếm mã (MRR) của
các độ dài token mã ground-truth khác nhau. Chúng tôi đặt
độ dài cắt mã từ 50 đến 512. Kết quả cao
nhất trong mỗi cột được đánh dấu. Tập dữ liệu:
CodeSearchNet python. Mô hình: GraphCodeBERT.
Độ dài token Độ dài cắt mã
50 100 256 400 512
[0, 256) 0.6274 0.6856 0.6909 0.6897 0.6906
[256, 512) 0.6239 0.7027 0.7237 0.7258 0.7265
[512, 768) 0.6004 0.6467 0.7168 0.7180 0.7181
[768, 1024) 0.6038 0.6315 0.7111 0.7375 0.7276
[1024, 1943) 0.6202 0.6573 0.6589 0.6835 0.6825

Các đóng góp có thể được tóm tắt như sau:
•Phát hiện và xác minh thực nghiệm về khó
khăn trong việc mô hình hóa mã dài trong các
mô hình tìm kiếm mã dựa trên Transformer hiện có.
•Chúng tôi đề xuất một baseline mới SEA và khám phá
cài đặt chia tách và tổng hợp tối ưu.
Chúng tôi cũng thiết kế một mô-đun kết hợp-chia cho
việc tăng tốc.
•Thông qua các thí nghiệm rộng rãi, chúng tôi cho thấy
hiệu quả của SEA đề xuất với các
baseline bộ mã hóa khác nhau trong sáu ngôn ngữ lập trình, tạo ra một baseline mạnh cho
tìm kiếm mã. Mã nguồn và dữ liệu thí nghiệm của chúng tôi có sẵn tại: https://github.
com/fly-dragon211/SEA .

2. Công trình liên quan
2.1. Phương pháp tìm kiếm mã
Các nghiên cứu đầu (Nie et al., 2016; Yang and Huang,
2017; Rosario, 2000; Hill et al., 2011; Satter and
Sakib, 2016; Lv et al., 2015; Van Nguyen et al.,
2017) trong tìm kiếm mã chủ yếu áp dụng các kỹ thuật truy xuất thông tin (IR) trực tiếp, xem tìm kiếm mã
như một nhiệm vụ khớp văn bản. Cả truy vấn và đoạn mã
đều được xem như văn bản thuần túy, và các
thuật toán khớp văn bản truyền thống như túi từ
(BOW) (Schütze et al., 2008), Jaccard (Jac-
card, 1901), tần suất thuật ngữ-nghịch đảo tần suất tài liệu
(TF-IDF) (Robertson and Jones, 1976),
BM25 (một phiên bản cải tiến của TF-IDF) (Robertson
and Zaragoza, 2009), và mô hình boolean mở rộng (Lv et al., 2015) được sử dụng. Vì độ dài mã
có tác động tối thiểu đến độ phức tạp mô hình hóa, các phương pháp này có thể mã hóa mã dài mà không
cắt bỏ.

Sau khi giới thiệu mô hình tiền huấn luyện quy mô lớn BERT (Devlin et al., 2019), Code-
BERT được đề xuất bởi Feng et al. (2020). Code-
BERT là một mô hình được tiền huấn luyện trên mã nguồn
và bình luận không nhãn, đã đạt được hiệu suất ấn tượng

--- TRANG 3 ---
hiệu suất trong tìm kiếm mã dựa trên văn bản thông qua
tinh chỉnh trên các tập dữ liệu cặp văn bản-mã. Huang
et al. (2021) giới thiệu CoCLR, một phương pháp học
tương phản nâng cao khớp truy vấn-mã.
Sun et al. (2022) phát triển một kỹ thuật dịch mã
nhận biết ngữ cảnh dịch các đoạn mã thành mô tả
ngôn ngữ tự nhiên. Gu et al. (2022)
sử dụng băm sâu và phân loại mã để
tăng tốc tìm kiếm mã, trong khi Chai et al. (2022)
điều chỉnh học meta few-shot cho tìm kiếm mã.
Guo et al. (2021) đề xuất GraphCodeBERT, kết
hợp các nhiệm vụ tiền huấn luyện nhận biết cấu trúc để
cải thiện hiểu mã và hiệu suất. Gần
đây, Hu et al. (2023) sử dụng khung tìm kiếm mã
fusion hai giai đoạn kết hợp bi-encoder
và cross-encoder để nâng cao hiệu suất. Tuy
nhiên, độ phức tạp tính toán của Transformer
và bộ nhớ GPU hạn chế thường dẫn đến việc cắt bỏ
các đoạn mã dài.

2.2. Biểu diễn mã thần kinh với
cấu trúc mã
Gần đây, đã có những tiến bộ đáng chú ý
trong các phương pháp biểu diễn mã thần kinh tận dụng
cấu trúc mã, đặc biệt là Cây cú pháp trừu tượng
(AST), mang lại hiệu suất ấn tượng (Alon
et al., 2020; Sun et al., 2020; Bui et al., 2021;
Kim et al., 2021; Peng et al., 2021; Hellendoorn
et al., 2019; Allamanis et al., 2021; Georgiev et al.,
2022; Ma et al., 2023; Du and Yu, 2023). MMAN
(Wan et al., 2019) kết hợp một lớp fusion attention
đa phương thức để kết hợp biểu diễn AST và Control Flow
Graph (CFG). ASTNN (Zhang
et al., 2019a) và CAST (Shi et al., 2021) phân
đoạn AST lớn thành chuỗi các cây câu lệnh
nhỏ hơn, mã hóa chúng thành vector bằng cách bắt
giữ thông tin từ vựng và cú pháp của mỗi
câu lệnh. TBCAA (Chen et al., 2019) sử dụng mạng
tích chập dựa trên cây trên AST được tăng cường API. UniXcoder (Guo et al., 2022) tận dụng cả
AST và bình luận mã để làm phong phú biểu diễn mã.
GraphCodeBERT (Guo et al., 2021) kết
hợp quan hệ biến được trích xuất từ AST trong
các nhiệm vụ tiền huấn luyện của nó. Trong công trình của chúng tôi, chúng tôi đặc biệt
muốn bắt giữ và mô hình hóa thông tin cấu trúc
có mặt trong các đoạn mã dài.

2.3. Transformer cho văn bản dài
Việc áp dụng các mô hình Transformer cho văn bản dài
có thể được chia rộng thành hai loại: mở rộng
attention và nâng cao mô hình Trans-
former gốc, và các phương pháp tổng hợp. Loại
đầu tiên bao gồm bốn phương pháp chính: attention
thưa thớt (Child et al., 2019; Correia et al., 2019;
Beltagy et al., 2020; Kitaev et al., 2019; Roy et al.,
2021; Ainslie et al., 2020; Jiang et al., 2020; Gün-
ther et al., 2023), tái hiện (Dai et al., 2019), cơ

Bảng 2: Thống kê độ dài token mã của tập
đánh giá CodeSearchNet.
Độ dài Ruby JS Go Py Java Php Tổng thể
[0, 256) 16% 10% 22% 14% 13% 13% 14%
[256, 512) 44% 29% 38% 30% 27% 26% 32%
[512, + ∞) 41% 62% 40% 56% 60% 61% 54%

chế phân cấp (Zhang et al., 2019b; Gao
and Callan, 2022), và attention nén (Ye
et al., 2019; Guo et al., 2019). Attention thưa thớt
hạn chế mỗi token chỉ chú ý đến một tập con của
các token khác. Tái hiện tích hợp các yếu tố mạng
thần kinh tái hiện vào các mô hình Transformer để mở
rộng phạm vi attention của chúng. Cơ chế phân cấp
mô hình văn bản đầu vào dài một cách phân cấp, từ câu
đến đoạn văn. Attention nén nén có chọn lọc
các phần cụ thể của đầu vào.

Loại thứ hai, phương pháp tổng hợp, bao
gồm việc tổng hợp nhiều điểm số đoạn văn hoặc biểu
diễn cho một tài liệu dài. Ví dụ,
Wang et al. (2019) đề xuất một mô hình BERT
đa đoạn văn để chuẩn hóa toàn cục điểm số câu trả lời trên
tất cả các đoạn văn trong nhiệm vụ hỏi đáp. Trong
bối cảnh xếp hạng tài liệu, SMITH (Yang et al.,
2020) học biểu diễn tài liệu thông qua
tổng hợp biểu diễn câu phân cấp.
PARADE (Li et al., 2020) sử dụng Max, CNN, At-
tention, và Transformer để tổng hợp các biểu diễn đoạn văn. Tsujimura et al. (2023) sử dụng
phương pháp cửa sổ trượt để quản lý chuỗi đầu vào
dài trong bối cảnh các nhiệm vụ Nhận dạng Thực thể Có tên
y tế.

Tuy nhiên, những phương pháp này có thể không hoàn toàn phù
hợp cho mã có cấu trúc cao. Trong các chương trình
được thiết kế tốt, mã trong cùng một module, như
một hàm, được kết nối chặt chẽ với nhau, trong khi tương tác giữa các module khác nhau được kết nối lỏng lẻo, tuân theo nguyên tắc gắn kết cao và
khớp nối thấp. Ngược lại, văn bản dài trong ngôn ngữ tự
nhiên có xu hướng thể hiện tính mạch lạc. Trong bài báo này,
chúng tôi điều tra khả năng áp dụng của các phương pháp văn bản dài
trong lĩnh vực tìm kiếm mã và đề xuất một
baseline mới SEA cho tìm kiếm mã dài.

3. Động lực: Vấn đề mã dài
3.1. Kiến thức cơ bản
Tìm kiếm mã nhằm tìm đoạn mã C phù hợp nhất
từ một codebase cho trước khớp với một
truy vấn Q. Đối với một mô hình học sâu hiện tại, chúng ta
đầu tiên chuyển đổi truy vấn Q và các đoạn mã C thành
token truy vấn và mã với tokenizer như
BPE (Sennrich et al., 2016). Sau đó chúng ta chuyển đổi
ID token của truy vấn Q và các đoạn mã
C thành biểu diễn vector eq và ec bằng các
bộ mã hóa mạng thần kinh, và tính toán độ tương tự (hoặc

--- TRANG 4 ---
khoảng cách) trong không gian Euclidean như
độ tương tự Cosine hoặc khoảng cách Euclidean để có được
điểm số tương tự xuyên phương thức s. Việc tính toán
có thể được công thức hóa như sau:

eq= Γ(tokenizer (Q))
ec= Γ′(tokenizer (C)), C∈Codebase
s=sim(eq,ec)(1)
trong đó Γ và Γ′ là hai bộ mã hóa mạng thần kinh
được huấn luyện tốt học từ dữ liệu cặp có nhãn.

3.2. Vấn đề mã dài
Để kiểm soát chi phí bộ nhớ và tính toán trong
giai đoạn huấn luyện, việc cắt bỏ mã dài là thông lệ
phổ biến. Ví dụ, GraphCodeBERT thường
lấy 256 token mã đầu tiên theo mặc định. Để điều
tra liệu phương pháp cắt bỏ này có dẫn đến
mất thông tin hay không, chúng tôi đã tiến hành thống kê độ dài token
trên CodeSearchNet. Như được hiển thị trong Bảng 2, chúng tôi
thấy rằng các đoạn với độ dài token nhỏ hơn
256 chỉ chiếm 14.1%, trong khi 53.5% các đoạn mã
vượt quá độ dài mã hóa tối đa
512 token cho Transformer. Điều này cho thấy rằng
việc cắt bỏ dẫn đến mất thông tin cho các đoạn
có độ dài token lớn hơn 256.

Để kiểm tra sự khác biệt hiệu suất tìm kiếm của
GraphCodeBERT trên các tập con truy vấn với độ dài mã ground truth (GT) khác nhau, chúng tôi chia
tập con test python của CodeSearchNet (CSN) thành 5
tập truy vấn riêng biệt dựa trên độ dài token mã GT khác nhau. Chúng tôi tính toán Mean Reciprocal Rank
(MRR) của GraphCodeBERT cho các độ dài cắt mã khác nhau, như được hiển thị trong Bảng 1. Đáng chú ý, chúng tôi quan
sát thấy xu hướng giảm trong hiệu suất tìm kiếm khi
độ dài token mã ground-truth tăng (từ
trên xuống dưới) đối với độ dài token mã vượt quá
256 token, cho thấy rằng các đoạn mã dài đặt ra
thách thức cho GraphCodeBERT. Hơn nữa, khi độ
dài cắt mã mở rộng từ trái sang phải,
chúng tôi quan sát thấy hiệu suất tìm kiếm tương đối ổn định khi độ dài cắt vượt quá
độ dài token. Và xuất hiện xu hướng tăng
trong hiệu suất tìm kiếm đối với các đoạn mã có
độ dài token vượt quá độ dài cắt.
Điều này gợi ý rằng việc chỉ cắt bỏ mã dài có thể
dẫn đến mất thông tin có giá trị.

4. SEA
Trong phần này, chúng tôi trình bày tổng quan toàn diện
về SEA, bao gồm kiến trúc mô hình, phương pháp chia tách, kỹ thuật tổng hợp,
và phương pháp kết hợp-chia được thiết kế để tăng tốc
suy luận.

4.1. Kiến trúc mô hình
Chúng tôi giới thiệu SEA của chúng tôi trong phần này. Pipeline
tổng thể được minh họa trong Hình 2. Cho một đoạn mã
C, mục tiêu của chúng tôi là có được biểu diễn mã
ec. Để đạt được điều này, chúng tôi sử dụng một
phương pháp đa bước. Đầu tiên chúng tôi chia đoạn mã thành
một tập hợp mảnh mã:
P=Split (C) ={p1, p2, . . . , p n}.(2)
Sau đó chúng tôi sử dụng phương pháp cửa sổ trượt để có được
một tập hợp khối mã chồng lấp một phần:
B=SlidingWindow (P) ={b1, b2, . . . , b k}.(3)
Giả sử kích thước cửa sổ là w và bước là s,
thì số khối mã là k=⌊n−w
s+ 1⌋,
trong đó ⌊·⌋ tham chiếu đến làm tròn xuống. Tiếp theo, chúng tôi sử dụng một
bộ mã hóa mã, như GraphCodeBERT, để có được
embedding cho mỗi khối mã trong k khối:
eB={eb1, eb2, . . . , e bk}. (4)
Cuối cùng, một phương pháp tổng hợp được áp dụng để kết
hợp k embedding thành biểu diễn mã
ec:
ec=Aggregation (eB) (5)

4.2. Phương pháp chia tách
Để có được tập hợp mảnh mã, chúng tôi khám phá bốn
phương pháp chia tách, cụ thể là chia tách dựa trên khoảng trắng,
chia tách dựa trên token, chia tách dựa trên dòng, và chia tách dựa trên AST. Chia tách dựa trên khoảng trắng đơn giản là
chia theo khoảng trắng, dẫn đến việc chia một chuỗi
như "def read_image_file" được chia thành {'def',
'read_image_file'}. Tương tự, chia tách dựa trên token
và chia tách dựa trên dòng bao gồm việc chia dựa trên
token và dòng, tương ứng.

Một Cây Cú pháp Trừu tượng (AST) là một biểu diễn cây
của cấu trúc cú pháp của mã nguồn
được viết bằng một ngôn ngữ lập trình. Mỗi nút trong
AST tương ứng với một cấu trúc cụ thể trong
mã, như biểu thức, câu lệnh, hoặc khai
báo. Cấu trúc phân cấp của AST phản ánh
cú pháp của ngôn ngữ lập trình, trừu tượng hóa
một số chi tiết cú pháp để tập trung vào cấu trúc
cốt lõi.

Đối với chia tách dựa trên AST, mục tiêu của chúng tôi là tạo ra một
phương pháp vừa đơn giản vừa có thể áp
dụng cho nhiều ngôn ngữ lập trình khác nhau. Lấy cảm hứng
từ CAST (Shi et al., 2021), chúng tôi phân tích mã nguồn
thành Cây Cú pháp Trừu tượng với tree_sitter1,
và duyệt AST này bằng duyệt preorder. Trong
trường hợp các cấu trúc hỗn hợp (i.e.for, if, def, etc.),
như được mô tả trong Hình 2(a), chúng tôi định nghĩa tập hợp
các nút AST {head_block, body}, trong đó head_block
1https://github.com/tree-sitter/
py-tree-sitter

--- TRANG 5 ---
def 
…def 
PIL2array
… return 
np.array
…for 
fpath in
…img = 
Image. 
…...def 
…def 
PIL2array
… return 
np.array
…for 
fpath in
…img = 
Image. 
…...
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches.append(
PIL2array(patch))
return torch.ByteTensor(
np.array(patches[:n]))Định nghĩa
hàmtên read_image_file
thântham số
tên
thân
Câu lệnh return
Định nghĩa
hàm
…
IN
 Câu lệnh For
…
…
……
thân
 …
Câu lệnh return
GraphCodeBERT
GraphCodeBERT
GraphCodeBERT ...Mô-đun
Fusion
Cửa sổ trượt
 Bộ mã hóa mã
 Biểu diễn mã(a) Chia tách mã dựa trên AST.
def 
…def 
PIL2array
… return 
np.array
…for 
fpath in
…img = 
Image. 
…...def 
…def 
PIL2array
… return 
np.array
…for 
fpath in
…img = 
Image. 
…...
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches.append(
PIL2array(patch))
return torch.ByteTensor(
np.array(patches[:n]))Định nghĩa
hàmtên read_image_file
thântham số
tên
thân
Câu lệnh return
Định nghĩa
hàm
…
IN
 Câu lệnh For
…
…
……
thân
 …
Câu lệnh return
GraphCodeBERT
GraphCodeBERT
GraphCodeBERT ...Mô-đun
tổng hợp
Cửa sổ trượt
 Bộ mã hóa mã
 Biểu diễn mã
(b) Cửa sổ trượt và tổng hợp.
Hình 2: Pipeline của kiến trúc SEA (split, encode and aggregate) đề xuất của chúng tôi.
chịu trách nhiệm chia đầu và thân của
các câu lệnh lồng nhau như câu lệnh if và While, trong khi body tương ứng với khai báo phương thức. Khi gặp một cấu trúc
hỗn hợp, chúng tôi chèn một dấu chia trước và sau
head_block, hiệu quả chia một AST lớn thành
một chuỗi các cây con không chồng lấp. Sau
đó, dựa trên việc chia AST, chúng tôi xây dựng
tập hợp mảnh mã P bằng cách chia mã gốc
tương ứng.

4.3. Phương pháp tổng hợp
Meanpooling / Maxpooling. Một phương pháp
đơn giản để tổng hợp embedding của k khối
mã là tính toán trung bình hoặc tối đa của
embedding của chúng:
ec=Mean /Max ({eb1, eb2, . . . , e bk}).(6)
Tuy nhiên, một hạn chế của meanpooling là mỗi
khối mã đóng góp như nhau vào biểu diễn cuối cùng, bất kể chất lượng cá nhân của chúng. Tương
tự, maxpooling làm nổi bật khối
có giá trị cao nhất. Để giải quyết những hạn chế này
và nâng cao quá trình tổng hợp, chúng tôi đề xuất
việc kết hợp các phương pháp embedding có trọng số.

Tổng hợp dựa trên attention. Nhận thức
rằng không phải tất cả các khối mã đều có tầm quan trọng như nhau trong
việc biểu diễn các đoạn mã dài, chúng tôi giới thiệu trọng số tự thích ứng α cho mỗi embedding khối trong

Bộ mã hóa
Linear
Softmax…Bộ mã hóa
1×𝑑 1×𝑑
𝑘×𝑑
𝑘×1
𝑘×1
1×𝑑1×𝑑
1×𝑑𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
Mean/MaxConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞
Linear
SoftmaxMean/Max tanh
Linear1×𝑑 1×𝑑
𝑘×𝑑
1×𝑑Mean/Max1×𝑑 1×𝑑
𝑘×𝑑
𝑘×128
𝑘×1
1×𝑑1×𝑑𝑘×128
𝑘×1Bộ mã hóa…Bộ mã hóa𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞
Bộ mã hóa…Bộ mã hóa𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞(a) Attention một lớp với
mean / max.
Bộ mã hóa
Linear
Softmax…Bộ mã hóa
1×𝑑 1×𝑑
𝑘×𝑑
𝑘×1
𝑘×1
1×𝑑1×𝑑
1×𝑑𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
Mean/MaxConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞
Linear
SoftmaxMean/Max tanh
Linear1×𝑑 1×𝑑
𝑘×𝑑
1×𝑑Mean/Max1×𝑑 1×𝑑
𝑘×𝑑
𝑘×128
𝑘×1
1×𝑑1×𝑑𝑘×128
𝑘×1Bộ mã hóa…Bộ mã hóa𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞
Bộ mã hóa…Bộ mã hóa𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞(b) Attention hai lớp với
mean / max.
Hình 3: Các phương pháp tổng hợp dựa trên attention.
quá trình tổng hợp:
ec=kX
iαiebi. (7)
Lấy cảm hứng từ Multi-Instance Learning dựa trên attention
(Li et al., 2021) và Lightweight Attentional Feature
Fusion (Hu et al., 2022), chúng tôi tính toán trọng số
{α1, . . . , α k} như sau:
{a1, . . . , a k}=softmax (Linear ({eb1, . . . , e bk})).
(8)

--- TRANG 6 ---
Bảng 3: Phân tích chi phí tính toán. n là độ dài
chuỗi, d là chiều biểu diễn,
k là số khối mã, l là số lớp.
Lưu ý rằng chúng tôi sử dụng attention một lớp cho SEA.
Phương pháp Tham số Độ phức tạp
GraphCodeBERT 5d2·l O (n2·d·l)
SEA 5d2·l+d O (n2
k·d·l)

Đối với attention một lớp, Linear tham chiếu đến một lớp
được kết nối đầy đủ chuyển đổi chiều thành 1.
Đối với attention hai lớp, Linear tham chiếu đến hai lớp
được kết nối đầy đủ đầu tiên chuyển đổi chiều
thành 128 và sau đó chuyển đổi chiều thành 1. Hơn
nữa, như được minh họa trong Hình 3(a) và Hình
3(b), chúng tôi khám phá sự kết hợp của attention với
các phương pháp meanpooling / maxpooling:
ec=kX
i(αiebi) +Mean /Max ({eb1, eb2, . . . , e bk}).
(9)

Đối với phân tích chi phí tính toán, SEA sử dụng
phương pháp cửa sổ trượt để giảm đáng kể độ phức tạp xuống 1/k. Độ phức tạp gốc của Graph-
CodeBERT được cho bởi O(n2·d·l), trong đó n, d, l
biểu thị độ dài chuỗi, chiều biểu diễn,
và số lớp, tương ứng. Bằng cách sử dụng
phương pháp cửa sổ trượt, độ phức tạp cho mỗi
cửa sổ trở thành O(w2·d·l), trong đó w biểu thị
kích thước cửa sổ. Đặt bước s=w, tổng
số khối mã trở thành k=n
w, dẫn đến
kích thước cửa sổ w=n
k. Do đó, độ phức tạp
tổng thể được đơn giản hóa thành:
O(k·w2·d·l) =O(k·(n
k)2
·d·l) =O(n2
k·d·l).(10)
Việc giảm độ phức tạp đáng kể này xuống 1
k cho phép
SEA mã hóa mã dài với ít chi phí bộ nhớ và
tính toán hơn.

Hơn nữa, như được hiển thị trong Bảng 3, chúng tôi quan sát
rằng so với GraphCodeBERT, SEA kết
hợp Tổng hợp attention một lớp chỉ giới thiệu
thêm d tham số có thể học. Mặc dù
việc tăng số lượng tham số khiêm tốn này, nó đóng vai trò
then chốt trong việc nâng cao hiệu quả của
giai đoạn tổng hợp, như các thí nghiệm của chúng tôi sẽ cung cấp
bằng chứng trong Phần 6.1.

4.4. Xử lý lô
Để nâng cao hiệu quả suy luận trên các tập dữ liệu lớn,
cần thiết phải tạo ra một phương pháp xử lý lô
có khả năng mã hóa nhiều đoạn mã dài
đồng thời. Như được nêu trong Phần 4.1, chúng tôi
có được nhiều khối mã từ mỗi đoạn mã dài.
Tuy nhiên, do số lượng khác nhau của

Bộ mã hóa mãFusion
Lô 
mã
Fusion
FusionLô
khốiEmbedding khối
②
②
②①Khối 
mã20221014 修改
89%11%
độ dài token <= 128
độ dài token > 128
9%
91%độ dài token <= 256
độ dài token > 256

Hình 4: Phương pháp kết hợp-chia xử lý lô. ① và ② tham chiếu đến phương pháp kết hợp và chia.
các khối mã tương ứng cho các đoạn mã dài khác nhau, việc tạo ra một phương pháp xử lý lô tổng quát đặt ra thách thức.

Để giải quyết vấn đề này, chúng tôi giới thiệu
phương pháp kết hợp-chia. Như được minh họa trong Hình 4, giả sử
kích thước lô là 3 (bao gồm ba đoạn mã),
số lượng khối mã tương ứng cho mỗi
đoạn lần lượt là 2, 3, và 1. Chúng tôi bắt đầu bằng cách
kết hợp sáu khối mã này thành một lô khối
và thiết lập một ánh xạ M liên kết chỉ số mã
với chỉ số khối. Sau đó, chúng tôi đưa
lô khối này vào bộ mã hóa mã song song
để có được embedding khối. Cuối cùng, tận dụng
thông tin từ ánh xạ M, chúng tôi tách
embedding thành ba nhóm và đưa chúng
vào mô-đun tổng hợp để có được biểu diễn mã riêng biệt.

5. Thiết kế thí nghiệm
5.1. Tập dữ liệu
Chúng tôi tiến hành thí nghiệm trên tập dữ liệu Code-
SearchNet (Husain et al., 2019) được sử dụng rộng rãi, bao gồm sáu ngôn ngữ lập trình, i.e., Ruby,
JavaScript, Go, Python, Java, và PHP. Theo
phương pháp trong (Guo et al., 2021), chúng tôi áp dụng lọc để loại bỏ các truy vấn chất lượng thấp và mở rộng tập
truy xuất để bao gồm toàn bộ corpus mã.

5.2. Metric đánh giá
Trong đánh giá của chúng tôi, chúng tôi sử dụng hai tiêu chí tự động phổ biến: MRR (Mean Reciprocal Ranking) và R@k
(độ chính xác top-k, k=1, 5, 10, 100). Chúng được
sử dụng phổ biến trong các nghiên cứu tìm kiếm mã trước đây (Lv
et al., 2015; Gu et al., 2018; Sachdev et al., 2018;
Husain et al., 2019; Feng et al., 2020; Huang et al.,
2021; Guo et al., 2021). Ngoài ra, chúng tôi báo cáo
số lượng tham số và thời gian suy luận làm
thước đo hiệu quả.

5.3. Cài đặt thí nghiệm
Baseline của chúng tôi là GraphCodeBERT. Các tham
số của bộ mã hóa mã và ngôn ngữ tự nhiên được

--- TRANG 7 ---
Bảng 4: Hiệu suất tìm kiếm của các biến thể SEA khác nhau. Tập dữ liệu: CodeSearchNet Ruby.
Cửa sổ Bước Chia tách Tổng hợp MRR R@1 R@5 R@10 R@100
GraphCodeBERT – – – – 0.6948 59.3 82.1 87.3 96.5
SEA-SpaceSplitting256 128 Space Maxpooling 0.6919 58.5 82.0 87.2 95.2
256 128 Space Meanpooling 0.6929 58.3 83.0 87.4 95.6
256 128 Space Attention (hai lớp) 0.6940 58.7 83.4 87.1 94.8
256 128 Space Attention (hai lớp) + Mean 0.7490 66.3 85.2 88.9 94.4
256 128 Space Attention (một lớp) 0.6989 59.6 82.2 86.8 95.0
256 128 Space Attention (một lớp) + Mean 0.7495 66.1 86.3 89.0 94.3
128 64 Space Attention (một lớp) + Mean 0.7545 66.2 87.5 90.2 95.2
64 32 Space Attention (một lớp) + Mean 0.7431 65.1 85.6 88.7 94.0
SEA-TokenSplitting256 128 Token Attention (một lớp) + Mean 0.7752 68.4 89.1 91.9 96.0
128 64 Token Attention (một lớp) + Mean 0.7606 67.2 87.5 91.3 95.6
64 32 Token Attention (một lớp) + Mean 0.7352 62.8 87.2 90.6 95.0
SEA-LineSplitting64 32 Line Attention (một lớp) + Mean 0.7635 67.3 88.2 91.3 95.6
32 16 Line Attention (một lớp) + Mean 0.7537 66.1 87.2 90.3 95.2
16 8 Line Attention (một lớp) + Mean 0.7498 65.5 86.9 90.3 95.0
SEA-ASTSplitting64 32 AST Attention (một lớp) + Mean 0.7539 65.7 91.4 95.0 97.6
32 16 AST Attention (một lớp) + Mean 0.7762 68.8 89.1 92.0 96.4
16 8 AST Attention (một lớp) + Mean 0.7744 68.8 88.7 91.4 96.3

khởi tạo bởi GraphCodeBERT. Để huấn luyện, chúng tôi
chọn ngẫu nhiên 6 khối mã từ các khối mã đã chia của một mã dài. Kích thước lô huấn luyện là
32. Để đánh giá, chúng tôi sử dụng tất cả các khối mã đã chia
của một mã dài. Kích thước lô đánh giá là 256.
Tất cả các thí nghiệm được tiến hành trên một máy với
Intel Xeon E5-2698v4 2.2Ghz 20-Core CPU và
hai Tesla V100 32GB GPU.

6. Kết quả thí nghiệm
6.1. Cấu hình SEA tối ưu
Để xác định cấu hình tối ưu cho SEA, chúng tôi
tiến hành thí nghiệm bằng cách thay đổi kiến trúc
của chúng tôi sử dụng các phương pháp chia tách mã khác nhau và phương pháp tổng hợp, trong khi đo lường những thay đổi
kết quả trong hiệu suất tìm kiếm. Vì tập dữ liệu CodeSearchNet Ruby tương đối nhỏ,
chúng tôi tập trung vào việc tiến hành thí nghiệm trên tập con ruby, và chúng tôi trình bày kết quả trong Bảng 4.

Trong các dòng SpaceSplitting của Bảng 4, chúng tôi thí nghiệm
với các phương pháp tổng hợp khác nhau như được mô tả trong
Phần 4.3. Các phát hiện của chúng tôi cho thấy việc sử dụng bất kỳ
phương pháp tổng hợp đơn lẻ nào một cách cô lập không mang lại
cải thiện hiệu suất đáng kể so
với Baseline GraphCodeBERT. Tuy nhiên, khi
fusion phương pháp attention với meanpooling, chúng tôi
quan sát thấy nâng cao hiệu suất đáng kể.
Cụ thể, phương pháp tổng hợp Attention (một lớp) + Mean cải thiện MRR và R@1 lần lượt
7.9% và 11.5%. Do đó, cho các
thí nghiệm tiếp theo, chúng tôi chọn sử dụng phương pháp tổng hợp Attention
(một lớp) + Mean.

Trong các dòng SpaceSplitting, TokenSplitting,
LineSplitting, ASTSplitting của Bảng 4, chúng tôi khám phá các
phương pháp chia mã khác nhau, như được chi tiết trong Phần 4.2. Đối với
phương pháp chia dựa trên space và token, chúng tôi đặt
kích thước cửa sổ từ 64 đến 256 do độ chi tiết
nhỏ hơn của việc chia. Ngược lại, đối với phương pháp chia dựa trên dòng và AST, chúng tôi đặt kích thước cửa sổ từ
16 đến 64. Đáng chú ý, chúng tôi quan sát thấy rằng phương pháp chia dựa trên AST thể hiện hiệu suất xuất sắc,
đạt được MRR và R@1 cao nhất với kích thước cửa sổ
32. Kết quả là, trong các thí nghiệm tiếp theo,
SEA tham chiếu đến SEA-ASTSplitting với kích thước cửa sổ
32, kích thước bước 16 và phương pháp tổng hợp Attention (một lớp)
+ Mean.

6.2. So sánh với ba Transformer thưa thớt
Trong phần này, chúng tôi tiến hành so sánh giữa
SEA và ba Transformer thưa thớt, BIGBIRD (Za-
heer et al., 2020), Longformer (Beltagy et al., 2020),
và LongCoder (Guo et al., 2023). BIGBIRD và
Longformer là hai Transformer hướng tài liệu dài nổi tiếng. LongCoder sử dụng cơ chế cửa sổ trượt để xử lý đầu vào mã dài cho hoàn thiện mã. Cụ thể, chúng tôi tận dụng các mô hình bigbird-roberta-base2, longformer-base-40963 và
longcoder-base4, với độ dài token là
1024. Do BIGBIRD và Longformer không được
tiền huấn luyện trên tập dữ liệu mã, chúng tôi cũng tiến hành
thí nghiệm để khởi tạo BIGBIRD và Longformer
với các tham số của GraphCodeBERT. Kết
quả được trình bày trong Bảng 5. So sánh kết quả trước và sau khi khởi tạo BIGBIRD và
Longformer với các tham số của GraphCode-
BERT, chúng tôi thấy rằng kết quả MRR cải thiện từ
0.2952 và 0.5016 lên 0.6121 và 0.6595, tương

2https://huggingface.co/google/bigbird-roberta-base
3https://huggingface.co/allenai/longformer-base-
4096
4https://huggingface.co/microsoft/longcoder-base

--- TRANG 8 ---
Bảng 5: So sánh với Transformer thưa thớt. Ký hiệu (G) chỉ ra rằng mô hình được khởi tạo
với các tham số GraphCodeBERT. Thời gian suy luận mã được xác định bằng cách chọn ngẫu nhiên 1000
mã và tính toán thời gian suy luận trung bình. Chúng tôi lặp lại mỗi lần tính toán thí nghiệm ba
lần và báo cáo trung bình và độ lệch chuẩn. Tập dữ liệu: CodeSearchNet Ruby. SEA vượt trội hơn
các mô hình khác một cách đáng kể ( p <0.01).
Mô hình #Param. Độ dài Token Thời gian suy luận MRR R@1 R@5 R@10
GraphCodeBERT 124.6M 256 6.3 ±0.3ms 0.6948 59.3 82.1 87.3
BIGBIRD 127.5M 1024 20.1 ±0.2ms 0.2952 19.2 39.8 51.1
BIGBIRD (G) 127.5M 1024 19.8 ±0.0ms 0.6121 50.8 74.2 80.7
Longformer 148.7M 1024 33.7 ±0.2ms 0.5128 39.9 65.3 72.4
Longformer (G) 148.7M 1024 33.7 ±0.1ms 0.6595 55.1 79.4 84.0
LongCoder 149.6M 1024 68.6 ±0.2ms 0.4718 35.8 61.1 67.8
SEA 124.6M - 7.2 ±0.5ms 0.7762 68.8 89.1 92.0
- w/o combine-divide 124.6M - 24.3 ±2.4ms 0.7762 68.8 89.1 92.0

ứng. Chúng tôi quy kết khoảng cách hiệu suất này cho
nhu cầu tái huấn luyện các mô hình ban đầu
được tiền huấn luyện trên tập dữ liệu ngôn ngữ tự nhiên. Chúng tôi quan
sát thấy MRR của LongCoder là 0.4718, biểu thị
một sự giảm đáng kể so với
GraphCodeBERT, gợi ý rằng LongCoder có thể
chủ yếu phù hợp cho các nhiệm vụ Hoàn thiện Mã. Chúng tôi
cũng tiến hành t-test giữa SEA của chúng tôi và các
baseline khác, và kết quả chứng minh rằng SEA
vượt trội đáng kể hơn tất cả các baseline Transformer thưa thớt ( p <0.01), làm nổi bật hiệu suất vượt trội của nó trong lĩnh vực tìm kiếm mã.

Về số lượng tham số mô hình và hiệu quả tìm kiếm, SEA nổi bật vì nó có số lượng tham
số thấp hơn và thời gian suy luận ngắn hơn so
với BIGBIRD, Longformer và LongCoder. Đáng chú ý rằng số lượng tham số của SEA được căn chỉnh chặt chẽ với GraphCodeBERT, chỉ khác
bởi việc thêm một lớp attention duy nhất. Tuy nhiên,
thay đổi nhỏ này dẫn đến sự tăng cường đáng kể trong
hiệu suất tìm kiếm. Chúng tôi cũng trình bày kết quả thí nghiệm mà không sử dụng phương pháp kết hợp-chia trong Bảng 5. Chúng tôi quan sát thấy rằng trong khi hiệu suất tìm kiếm vẫn ổn định, thời gian suy luận tăng hơn ba lần. Điều này làm nổi bật sự cải thiện đáng kể trong thời gian suy luận
do phương pháp kết hợp-chia mang lại,
từ đó xác nhận hiệu quả của nó trong việc tăng tốc
quá trình suy luận của mô hình.

6.3. Hiệu suất SEA trên các độ dài mã khác nhau
Để khám phá sự cải thiện của SEA đề xuất
cho các đoạn mã có độ dài khác nhau, chúng tôi trình bày
so sánh hiệu suất tìm kiếm giữa phương pháp baseline GraphCodeBERT và SEA dưới
các độ dài token mã ground-truth khác nhau. Kết
quả được mô tả trong Hình 5.

Đáng chú ý, hiệu suất truy xuất của mỗi tập con truy vấn thể hiện nâng cao rõ rệt, đặc biệt đối với kết quả truy xuất mã dài. Chúng tôi quy cách cải thiện này cho hai yếu tố quan trọng. Thứ nhất, mô-đun tổng hợp của SEA bắt giữ thích ứng và
kết hợp thông tin từ các phân đoạn đa dạng của
mã dài, dẫn đến biểu diễn mã toàn diện
và thông tin phong phú hơn. Thứ hai,
phương pháp chia mã được sử dụng bởi SEA có thể được
xem như một hình thức data augmentation, cung cấp
bối cảnh và biến thể bổ sung hỗ trợ trong việc tăng cường
biểu diễn mã. Tóm lại, SEA
mang lại biểu diễn mã mạnh mẽ hơn, nâng cao đáng kể
hiệu suất truy xuất tổng thể.

0.600.620.640.660.680.700.720.740.760.780.80
[0, 256) [256, 512) [512, 768) [768, 1024) [1024, 1943)MRR
Độ dài token mã
GraphCodeBERT
 SEA

Hình 5: So sánh hiệu suất giữa
GraphCodeBERT và SEA trong các độ dài token mã ground-truth khác nhau. So với GraphCodeBERT,
SEA đạt hiệu suất tốt hơn đáng kể ( p <0.01) cho các độ dài token mã khác nhau.

6.4. So sánh baseline trên
nhiều ngôn ngữ lập trình
Để đảm bảo so sánh công bằng và có thể tái tạo, chúng tôi
cẩn thận chọn các baseline dựa trên tiền huấn luyện đáp ứng ba tiêu chí sau: 1) Mã nguồn
có sẵn công khai; 2) Mô hình tổng thể có thể
thích ứng với tất cả sáu ngôn ngữ lập trình
trên tập dữ liệu CodeSearchNet; 3) Bài báo được
peer-review nếu nó được xuất bản như một bài báo nghiên cứu

--- TRANG 9 ---
Bảng 6: MRR trên sáu ngôn ngữ của tập dữ liệu CodeSearchNet. SEA ở đây tham chiếu đến SEA-ASTSplitting
với kích thước cửa sổ 32 và bước 16. SEA +RoBERTa tham chiếu đến SEA với RoBERTa làm bộ mã hóa mã.
SEA vượt trội hơn baseline một cách đáng kể ( p <0.01).
Mô hình / Phương pháp Ruby Javascript Go Python Java Php Tổng thể
RoBERTa 0.587 0.517 0.850 0.587 0.599 0.560 0.617
UniXcoder 0.586 0.603 0.881 0.695 0.687 0.644 0.683
CodeBERT 0.679 0.620 0.882 0.672 0.676 0.628 0.693
GraphCodeBERT 0.703 0.644 0.897 0.692 0.691 0.649 0.713
SEA +RoBERTa 0.651 (10.9% ↑) 0.593 (14.6% ↑) 0.879 (3.5% ↑) 0.633 (7.9% ↑) 0.666 (11.1% ↑) 0.647 (15.6% ↑) 0.678 (10.0% ↑)
SEA +UniXcoder 0.648 (10.7% ↑) 0.692 (14.8% ↑) 0.896 (1.8% ↑) 0.707 (1.7% ↑) 0.739 (7.5% ↑) 0.712 (10.5% ↑) 0.732 (7.3% ↑)
SEA +CodeBERT 0.742 (9.3% ↑) 0.696 (12.3% ↑) 0.905 (2.6% ↑) 0.714 (6.2% ↑) 0.732 (8.3% ↑) 0.711 (13.2% ↑) 0.750 (8.3% ↑)
SEA +GraphCodeBERT 0.776 (10.4% ↑) 0.742 (15.2% ↑) 0.921 (2.7% ↑) 0.754 (8.9% ↑) 0.768 (11.1% ↑) 0.748 (15.3% ↑) 0.785 (10.1% ↑)

per. Do đó, chúng tôi chọn bốn phương pháp sâu end-to-end: RoBERTa (Liu et al., 2019), UniX-
coder (Guo et al., 2022), CodeBERT (Feng et al.,
2020), và GraphCodeBERT (Guo et al., 2021).
Trong Bảng 6, chúng tôi trình bày kết quả MRR, chứng minh rằng SEA vượt trội hơn tất cả các phương pháp trên
tất cả sáu ngôn ngữ lập trình. Đáng chú ý, kết luận này
vẫn nhất quán cho metric recall và
một biến thể khác của SEA, kết quả có thể
được tìm thấy trong gói sao chép của chúng tôi. Những phát
hiện này củng cố tính vượt trội của SEA so với
các baseline dựa trên tiền huấn luyện trên các
ngôn ngữ lập trình đa dạng.

7. Kết luận
Trong bài báo này, chúng tôi giải quyết thách thức mô hình hóa hiệu quả mã dài cho tìm kiếm mã. Chúng tôi
giới thiệu SEA, một phương pháp hiệu quả mang lại
biểu diễn mã được cải thiện cho các đoạn mã dài. Mặc dù đơn giản, kết quả thí nghiệm của chúng tôi
cho thấy hiệu quả và hiệu suất đáng chú ý
của SEA. Chúng tôi tin rằng công trình này mở ra những khả năng mới cho tìm kiếm mã.

8. Tuyên bố đạo đức
Các mở rộng và ứng dụng tương lai phát sinh từ công
trình của chúng tôi nên chú ý đến tác động môi trường
của việc huấn luyện các mô hình quy mô lớn. Chúng nên
tích cực tránh việc sử dụng sai mục đích tiềm năng bằng cách tìm kiếm ý định độc hại. Tuy nhiên, không có khả năng mô hình trong hình thức hiện tại sẽ dẫn đến tác động như vậy
trong tương lai gần. Mô hình của chúng tôi cũng có tiềm năng tạo ra tác động tích cực trong các lĩnh vực như
tìm kiếm mã, hiểu mã dài và biểu diễn mã.

9. Lời cảm ơn
Công việc được mô tả trong bài báo này được hỗ trợ một phần bởi CCF-Huawei Populus Grove Fund CCF-
HuaweiSE202301.

10. Tài liệu tham khảo thư mục
Joshua Ainslie, Santiago Ontañón, Chris Alberti,
Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang,
và Li Yang. 2020. ETC: encoding long and
structured inputs in transformers. Trong EMNLP.

Miltiadis Allamanis, Henry Jackson-Flux, và Marc
Brockschmidt. 2021. Self-supervised bug detec-
tion and repair. Trong NeurIPS.

Uri Alon, Roy Sadaka, Omer Levy, và Eran Yahav.
2020. Structural language models of code. Trong
ICML.

Iz Beltagy, Matthew E Peters, và Arman Cohan.
2020. Longformer: The long-document trans-
former.arXiv.

Nghi DQ Bui, Yijun Yu, và Lingxiao Jiang. 2021.
Treecaps: Tree-based capsule networks for
source code processing. Trong AAAI.

Yitian Chai, Hongyu Zhang, Beijun Shen, và Xi-
aodong Gu. 2022. Cross-domain deep code
search with few-shot meta learning. arXiv.

Long Chen, Wei Ye, và Shikun Zhang. 2019. Cap-
turing source code semantics via tree-based con-
volution over api-enhanced ast. Trong CF.

Rewon Child, Scott Gray, Alec Radford, và Ilya
Sutskever. 2019. Generating long sequences
with sparse transformers. arXiv.

Gonçalo M Correia, Vlad Niculae, và André FT
Martins. 2019. Adaptively sparse transformers.
Trong EMNLP-IJCNLP.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G
Carbonell, Quoc Le, và Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language models
beyond a fixed-length context. Trong ACL.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và
Kristina Toutanova. 2019. BERT: pre-training

--- TRANG 10 ---
of deep bidirectional transformers for language
understanding. Trong NAACL-HLT.

Yali Du và Zhongxing Yu. 2023. Pre-training code
representation with semantic flow graph for ef-
fective bug localization. Trong FSE/ESEC.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,
Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, và Ming Zhou. 2020.
Codebert: A pre-trained model for programming
and natural languages. Trong EMNLP.

Luyu Gao và Jamie Callan. 2022. Long document
re-ranking with modular re-ranker. Trong SIGIR.

Dobrik Georgiev, Marc Brockschmidt, và Miltiadis
Allamanis. 2022. Heat: Hyperedge attention net-
works.arXiv.

Wenchao Gu, Yanlin Wang, Lun Du, Hongyu
Zhang, Shi Han, Dongmei Zhang, và Michael
Lyu. 2022. Accelerating code search with deep
hashing and code classification. Trong ACL.

Xiaodong Gu, Hongyu Zhang, và Sunghun Kim.
2018. Deep code search. Trong ICSE.

Michael Günther, Jackmin Ong, Isabelle Mohr,
Alaeddine Abdessalem, Tanguy Abel, Moham-
mad Kalim Akram, Susana Guzman, Georgios
Mastrapas, Saba Sturua, Bo Wang, et al. 2023.
Jina embeddings 2: 8192-token general-purpose
text embeddings for long documents. arXiv.

Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming
Zhou, và Jian Yin. 2022. Unixcoder: Unified
cross-modal pre-training for code representation.
Trong ACL.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-
fano, Shao Kun Deng, Colin B. Clement, Dawn
Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
và Ming Zhou. 2021. Graphcodebert: Pre-
training code representations with data flow. Trong
ICLR.

Daya Guo, Canwen Xu, Nan Duan, Jian Yin, và Ju-
lian J. McAuley. 2023. Longcoder: A long-range
pre-trained language model for code completion.
Trong ICML, Proceedings of Machine Learning Re-
search. PMLR.

Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,
Xiangyang Xue, và Zheng Zhang. 2019. Star-
transformer. Trong NAACL-HLT. Association for Com-
putational Linguistics.

Vincent J Hellendoorn, Charles Sutton, Rishabh
Singh, Petros Maniatis, và David Bieber. 2019.
Global relational models of source code. Trong ICLR.

Emily Hill, Lori Pollock, và K Vijay-Shanker. 2011.
Improving source code search with natural lan-
guage phrasal representations of method signa-
tures. Trong ASE. IEEE.

Fan Hu, Aozhu Chen, Ziyue Wang, Fangming Zhou,
Jianfeng Dong, và Xirong Li. 2022. Lightweight
attentional feature fusion: A new baseline for
text-to-video retrieval. Trong ECCV. Springer.

Fan Hu, Yanlin Wang, Lun Du, Xirong Li, Hongyu
Zhang, Shi Han, và Dongmei Zhang. 2023. Re-
visiting code search in a two-stage paradigm. Trong
WSDM.

Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong,
Ke Xu, Daxin Jiang, Ming Zhou, và Nan Duan.
2021. Cosqa: 20,000+ web queries for code
search and question answering. Trong ACL.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Mil-
tiadis Allamanis, và Marc Brockschmidt. 2019.
Codesearchnet challenge: Evaluating the state
of semantic code search. arXiv.

Paul Jaccard. 1901. Étude comparative de la distri-
bution florale dans une portion des alpes et des
jura. Bull Soc Vaudoise Sci Nat, pages 547–579.

Jyun-Yu Jiang, Chenyan Xiong, Chia-Jung Lee, và
Wei Wang. 2020. Long document ranking with
query-directed sparse transformer. Trong EMNLP
Findings.

Seohyun Kim, Jinman Zhao, Yuchi Tian, và Satish
Chandra. 2021. Code prediction by feeding trees
to transformers. Trong ICSE.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya.
2019. Reformer: The efficient transformer. Trong
ICLR.

Canjia Li, Andrew Yates, Sean MacAvaney, Ben He,
và Yingfei Sun. 2020. Parade: Passage rep-
resentation aggregation for document reranking.
ACM Transactions on Information Systems.

Xirong Li, Yang Zhou, Jie Wang, Hailan Lin,
Jianchun Zhao, Dayong Ding, Weihong Yu, và
Youxin Chen. 2021. Multi-modal multi-instance
learning for retinal disease recognition. Trong
ACM MM.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, và Veselin Stoyanov.
2019. Roberta: A robustly optimized bert pre-
training approach. arXiv.

Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei
Wang, Dongmei Zhang, và Jianjun Zhao. 2015.
Codehow: Effective code search based on api
understanding and extended boolean model (e).
Trong ASE.

--- TRANG 11 ---
Y Ma, Yali Du, và Ming Li. 2023. Capturing the
long-distance dependency in the control flow
graph via structural-guided attention for bug lo-
calization. Trong IJCAI.

Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, và
Xiaochen Li. 2016. Query expansion based
on crowd knowledge for code search. IEEE
Transactions on Services Computing, pages
771–783.

Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, và
Zhi Jin. 2021. Integrating tree path in transformer
for code representation. Trong NeurIPS.

Stephen Robertson và Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and
beyond. Now Publishers Inc.

Stephen E Robertson và K Sparck Jones. 1976.
Relevance weighting of search terms. Journal
of the American Society for Information science,
pages 129–146.

Barbara Rosario. 2000. Latent semantic indexing:
An overview. Techn. rep. INFOSYS, pages 1–16.

Aurko Roy, Mohammad Saffar, Ashish Vaswani,
và David Grangier. 2021. Efficient content-
based sparse attention with routing trans-
formers. Transactions of the Association for
Computational Linguistics, 9:53–68.

Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun
Kim, Koushik Sen, và Satish Chandra. 2018.
Retrieval on source code: a neural code search.
Trong MAPL.

Abdus Satter và Kazi Sakib. 2016. A search log
mining based query expansion technique to im-
prove effectiveness in code search. Trong ICCIT,
pages 586–591. IEEE.

Hinrich Schütze, Christopher D Manning, và
Prabhakar Raghavan. 2008. Introduction to
information retrieval, volume 39. Cambridge Uni-
versity Press Cambridge.

Rico Sennrich, Barry Haddow, và Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. Trong ACL.

Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,
Shi Han, Dongmei Zhang, và Hongbin Sun.
2021. Cast: Enhancing code summarization
with hierarchical splitting and reconstruction of
abstract syntax trees. Trong EMNLP.

Weisong Sun, Chunrong Fang, Yuchen Chen,
Guanhong Tao, Tingxu Han, và Quanjun Zhang.
2022. Code search based on context-aware code
translation. arXiv.

Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun,
Lili Mou, và Lu Zhang. 2020. Treegen: A tree-
based transformer architecture for code genera-
tion. Trong AAAI.

Tomoki Tsujimura, Koshi Yamada, Ryuki Ida,
Makoto Miwa, và Yutaka Sasaki. 2023. Contex-
tualized medication event extraction with strid-
ing ner and multi-turn qa. Journal of Biomedical
Informatics, page 104416.

Thanh Van Nguyen, Anh Tuan Nguyen, Hung Dang
Phan, Trong Duc Nguyen, và Tien N Nguyen.
2017. Combining word2vec with revised vector
space model for better code retrieval. Trong ICSE-C.
IEEE.

Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu,
Zhou Zhao, Jian Wu, và Philip S. Yu. 2019.
Multi-modal attention network learning for seman-
tic source code retrieval. Trong ASE.

Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh
Nallapati, và Bing Xiang. 2019. Multi-passage
bert: A globally normalized bert model for open-
domain question answering. Trong EMNLP.

Liu Yang, Mingyang Zhang, Cheng Li, Michael
Bendersky, và Marc Najork. 2020. Beyond
512 tokens: Siamese multi-depth transformer-
based hierarchical encoder for long-form docu-
ment matching. Trong CIKM.

Yangrui Yang và Qing Huang. 2017. Iecs: Intent-
enforced code search via extended boolean
model. Journal of Intelligent & Fuzzy Systems,
pages 2565–2576.

Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, và
Zheng Zhang. 2019. Bp-transformer: Modelling
long-range context via binary partitioning. arXiv.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, và Amr Ahmed. 2020. Big bird:
Transformers for longer sequences. Trong NeurIPS.

Jian Zhang, Xu Wang, Hongyu Zhang, Hailong
Sun, Kaixuan Wang, và Xudong Liu. 2019a. A
novel neural source code representation based
on abstract syntax tree. Trong ICSE.

Xingxing Zhang, Furu Wei, và Ming Zhou. 2019b.
Hibert: Document level pre-training of hierarchi-
cal bidirectional transformers for document sum-
marization. Trong ACL.
