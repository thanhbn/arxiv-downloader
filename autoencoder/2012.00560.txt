# 2012.00560.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/autoencoder/2012.00560.pdf
# File size: 2322376 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Noname manuscript No.
(will be inserted by the editor)
Quick and Robust Feature Selection: the Strength of
Energy-eÔ¨Écient Sparse Training for Autoencoders
Zahra Atashgahi Ghada Sokar Tim
van der Lee Elena Mocanu Decebal
Constantin Mocanu Raymond Veldhuis 
Mykola Pechenizkiy
Received: date / Accepted: date
Abstract Majorcomplicationsarise fromtherecentincreaseintheamountofhigh-
dimensional data, including high computational costs and memory requirements.
Feature selection, which identiÔ¨Åes the most relevant and informative attributes of
a dataset, has been introduced as a solution to this problem. Most of the existing
feature selection methods are computationally ineÔ¨Écient; ineÔ¨Écient algorithms
lead to high energy consumption, which is not desirable for devices with limited
computational and energy resources. In this paper, a novel and Ô¨Çexible method for
unsupervised feature selection is proposed. This method, named QuickSelection1,
introduces the strength of the neuron in sparse neural networks as a criterion to
measure the feature importance. This criterion, blended with sparsely connected
denoising autoencoders trained with the sparse evolutionary training procedure,
derives the importance of all input features simultaneously. We implement Quick-
Selection in a purely sparse manner as opposed to the typical approach of using
a binary mask over connections to simulate sparsity. It results in a considerable
speed increase and memory reduction. When tested on several benchmark datasets,
including Ô¨Åve low-dimensional and three high-dimensional datasets, the proposed
method is able to achieve the best trade-oÔ¨Ä of classiÔ¨Åcation and clustering accuracy,
running time, and maximum memory usage, among widely used approaches for
feature selection. Besides, our proposed method requires the least amount of energy
among the state-of-the-art autoencoder-based feature selection methods.
This paper has been accepted for publication in the Machine Learning Journal (ECML-PKDD
2022 Journal Track)
Z. AtashgahiE. MocanuD.C. MocanuR.N.J. Veldhuis
Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente,
Enschede, 7500AE, the Netherland
E-mail: z.atashgahi@utwente.nl
G.A.Z.N. Sokar1T. Lee2D.C. Mocanu1M. Pechenizkiy1
Eindhoven University of Technology, 5600 MB Eindhoven, the Netherlands
1Department of Mathematics and Computer Science2Department of Electrical Engineering
M. Pechenizkiy
Faculty of Information Technology, University of Jyv√§skyl√§, 40014 Jyv√§skyl√§, Finland
1The code is available at: https://github.com/zahraatashgahi/QuickSelectionarXiv:2012.00560v2  [cs.LG]  13 Sep 2021

--- PAGE 2 ---
2 Zahra Atashgahi et al.
Keywords Feature Selection Deep LearningSparse Autoencoders Sparse
Training
1 Introduction
In the last few years, considerable attention has been paid to the problem of dimen-
sionality reduction and many approaches have been proposed [ 53]. There are two
main techniques for reducing the number of features of a high-dimensional dataset:
feature extraction and feature selection. Feature extraction focuses on transforming
the data into a lower-dimensional space. This transformation is done through a map-
ping which results in a new set of features [ 40]. Feature selection reduces the feature
space by selecting a subset of the original attributes without generating new features
[12]. Based on the availability of the labels, feature selection methods are divided
into three categories: supervised [ 2,12], semi-supervised [ 58,48], and unsupervised
[43,16]. Supervised feature selection algorithms try to maximize some function of
predictive accuracy given the class labels. In unsupervised learning, the search for
discriminative features is done blindly, without having the class labels. Therefore,
unsupervised feature selection is considered as a much harder problem [16].
Featureselectionmethodsimprovethescalabilityofmachinelearningalgorithms
sincetheyreducethedimensionalityofdata.Besides,theyreducetheever-increasing
demands for computational and memory resources that are introduced by the emer-
gence of big data. This can lead to a considerable decrease in energy consumption
in data centers. This can ease not only the problem of high energy costs in data
centers but also the critical challenges imposed on the environment [ 56]. As outlined
by the High-Level Expert Group on ArtiÔ¨Åcial Intelligence (AI) [ 22], environmental
well-being is one of the requirements of a trust-worthy AI system. The development,
deployment, and process of an AI-system should be assessed to ensure that they
would function in the most environmentally friendly way possible. For example,
resource usage and energy consumption through training can be evaluated.
However, a challenging problem that arises in the feature selection domain is
that selecting features from datasets that contain a huge number of features and
samples, may require a massive amount of memory, computational, and energy
resources. Since most of the existing feature selection techniques were designed to
process small-scale data, their eÔ¨Éciency can be downgraded with high-dimensional
data [9]. Only a few studies have focused on designing feature selection algorithms
that are eÔ¨Écient in terms of computation [ 52,1]. The main contributions of this
paper can be summarized as follows:
We propose a new fast and robust unsupervised feature selection method, named
QuickSelection. As brieÔ¨Çy sketched in Figure 1, It has two key components:
(1) Inspired by node strength in graph theory, the method proposes the neu-
ron strength of sparse neural networks as a criterion to measure the feature
importance; and (2) The method introduces sparsely connected Denoising Au-
toencoders (sparse DAEs) trained from scratch with the sparse evolutionary
training procedure to model the data distribution eÔ¨Éciently. The imposed
sparsity before training also reduces the amount of required memory and the
training running time.
We implement QuickSelection in a completely sparse manner in Python using
the SciPy library and Cython rather than using a binary mask over connections

--- PAGE 3 ---
Quick and Robust Feature Selection 3
Epoch 5
(c) Compute
Neurons Strength(d) Feature
Selections1 = 0.2
s2 = 0.5
s5 = 0.3s4 = 0s3 = 1.3f 1
f 2
f 3
f 4
f 5f 2
f 3
(b) T rain Sparse-DAE(a) Initialize
Sparse-DAEEpoch 10 Epoch 0
Fig. 1: A high-level overview of the proposed method, ‚ÄúQuickSelection‚Äù. (a) At
epoch 0, connections are randomly initialized. (b) After initializing the sparse
structure, we start the training procedure. After 5epochs, some connections are
changed during the training procedure, and as a result, the strength of some
neurons has increased or decreased. At epoch 10, the network has converged, and
we can observe which neurons are important (larger and darker blue circles) and
which are not. (c) When the network is converged, we compute the strength of all
input neurons. (d) Finally, we select Kfeatures corresponding to neurons with the
highest strength values.
to simulate sparsity. This ensures minimum resource requirements, i.e., just
Random-Access Memory (RAM) and Central Processing Unit (CPU), without
demanding Graphic Processing Unit (GPU).
The experiments performed on 8 benchmark datasets suggest that QuickSelec-
tion has several advantages over the state-of-the-art, as follows:
It is the Ô¨Årst or the second-best performer in terms of both classiÔ¨Åcation and
clustering accuracy in almost all scenarios considered.
It is the best performer in terms of the trade-oÔ¨Ä between classiÔ¨Åcation and
clustering accuracy, running time, and memory requirement.
The proposed sparse architecture for feature selection has at least one order
of magnitude fewer parameters than its dense equivalent. This leads to the
outstanding fact that the wall clock training time of QuickSelection running
on CPU is smaller than the wall clock training time of its autoencoder-based
competitors running on GPU in most of the cases.
Last but not least, QuickSelection computational eÔ¨Éciency makes it have the
minimum energy consumption among the autoencoder-based feature selection
methods considered.
2 Related Work
2.1 Feature Selection
The literature on feature selection shows a variety of approaches that can be
divided into three major categories, including Ô¨Ålter, wrapper, and embedded
methods. Filter methods use a ranking criterion to score the features and then
remove the features with scores below a threshold. These criteria can be Laplacian

--- PAGE 4 ---
4 Zahra Atashgahi et al.
score [27], Correlation, Mutual Information [ 12], and many other scoring methods
such as Bayesian scoring function, t-test scoring, and Information theory-based
criteria [33]. These methods are usually fast and computationally eÔ¨Écient. Wrapper
methods evaluate diÔ¨Äerent subsets of features to detect the best subset. Wrapper
methods usually give better performance than Ô¨Ålter methods; they use a predictive
model to score each subset of features. However, this results in high computation
complexity. Seminal contributions for this type of feature selection have been
made by Kohavi and John [ 30]. In [30], the authors used a tree structure to
evaluate the subsets of features. Embedded methods unify the learning process,
and the feature selection [ 31]. Multi-Cluster Feature Selection (MCFS) [ 11] is
an unsupervised method for embedded feature selection, which selects features
using spectral regression with L1-norm regularization. A key limitation of this
algorithm is that it is computationally intensive since it depends on computing
the eigenvectors of the data similarity matrix and then solving an L1-regularized
regression problem for each eigenvector [ 19]. Unsupervised Discriminative Feature
Selection(UDFS) [ 57]is anotherunsupervisedembedded featureselectionalgorithm
that simultaneously utilizes both feature and discriminative information to select
features [37].
2.2 Autoencoders for Feature Selection
Inthelastfewyears,manydeeplearning-basedmodelshavebeendevelopedtoselect
features from the input data using the learning procedure of deep neural networks
[38]. In [42], a Multi-Layer Perceptron (MLP) is augmented with a pairwise-coupling
layer to feed each input feature along with its knockoÔ¨Ä counterpart into the network.
After the training, the authors use the Ô¨Ålter weights of the pairwise-coupling layer
to rank input features. Autoencoders which are generally known as a strong tool for
feature extraction [ 8], are being explored to perform unsupervised feature selection.
In [24], authors combine autoencoder regression and group lasso task for unsuper-
vised feature selection named AutoEncoder Feature Selector (AEFS). In [ 15], an
autoencoder is combined with three variants of structural regularization to perform
unsupervised feature selection. These regularizations are based on slack variables,
weights, and gradients, respectively. Another recently proposed autoencoder-based
embedded method is feature selection with Concrete Autoencoder (CAE) [ 5]. This
method selects features by learning a concrete distribution over input features.
They proposed a concrete selector layer that selects a linear combination of input
features that converges to a discrete set of Kfeatures during training. In [ 49], the
authors showed that a large set of parameters in CAE might lead to over-Ô¨Åtting in
case of having a limited number of samples. In addition, CAE may select features
more than once since there is no interaction between the neurons of the selector
layer. To mitigate these problems, they proposed a concrete neural network feature
selection (FsNet) method, which includes a selector layer and a supervised deep
neural network. The training procedure of FsNet considers reducing the reconstruc-
tion loss and maximizing the classiÔ¨Åcation accuracy simultaneously. In our research,
we focus mostly on unsupervised feature selection methods.
Denoising Autoencoder (DAE) is introduced to solve the problem of learning
the identity function in the autoencoders. This problem is most likely to happen
when we have more hidden neurons than inputs [ 4]. As a result, the network output

--- PAGE 5 ---
Quick and Robust Feature Selection 5
may be equal to the inputs, which makes the autoencoder useless. DAEs solve
the aforementioned problem by introducing noise on the input data and trying to
reconstruct the original input from its noisy version [ 54]. As a result, DAEs learn a
representation of the input data that is robust to small irrelevant changes in the
input. In this research, we use the ability of this type of neural network to encode
the input data distribution and select the most important features. Moreover, we
demonstrate the eÔ¨Äect of noise addition on the feature selection results.
2.3 Sparse Training
Deep neural networks usually have at least some fully-connected layers, which
results in a large number of parameters. In a high-dimensional space, this is not
desirable since it may cause a signiÔ¨Åcant decrease in training speed and a rise
in memory requirement. To tackle this problem, sparse neural networks have
been proposed. Pruning the dense neural networks is one of the most well-known
methods to achieve a sparse neural network [ 35,26]. In [25], Han et al. start from
a pre-trained network, prune the unimportant weights, and retrain the network.
Although this method can output a network with the desired sparsity level, the
minimum computation cost is as much as the cost of training a dense network. To
reduce this cost, Lee et al. [ 36] start with a dense neural network, and prune it prior
to training based on connection sensitivity; then, the sparse network is trained in
the standard way. However, starting from a dense neural network requires at least
the memory size of the dense neural network and the computational resources for
one training iteration of a dense network. Therefore, this method might not be
suitable for low resource devices.
In 2016, Mocanu et al. [ 44] have introduced the idea of training sparse neural
networks from scratch, a concept which recently has started to be known as sparse
training. The sparse connectivity pattern was Ô¨Åxed before training using graph
theory, network science, and data statistics. While it showed promising results,
outperforming the dense counterpart, the static sparsity pattern did not always
model the data optimally. In order to address these issues, in 2018, Mocanu et al.
[45] have proposed the Sparse Evolutionary Training (SET) algorithm which makes
use of dynamic sparsity during training. The idea is to start with a sparse neural
network before training and dynamically change its connections during training in
order to automatically model the data distribution. This results in a signiÔ¨Åcant
decrease in the number of parameters and increased performance. SET evolves the
sparse connections at each training epoch by removing a fraction connections
with the smallest magnitude, and randomly adding new connections in each layer.
Bourgin et al. [ 10] have shown that a sparse MLP trained with SET achieves state-
of-the-art results on tabular data in predicting human decisions, outperforming
fully-connected neural networks and Random Forest, among others.
In this work, we introduce for the Ô¨Årst time sparse training in the world of
denoising autoencoders, and we named the newly introduced model sparse denoising
autoencoder (sparse DAE). We train the sparse DAE with the SET algorithm to
keep the number of parameters low, during the training. Then, we then exploit the
trained network to select the most important features.

--- PAGE 6 ---
6 Zahra Atashgahi et al.
3 Proposed Method
To address the problem of the high dimensionality of the data, we propose a novel
method, named ‚ÄúQuickSelection‚Äù, to select the most informative attributes from the
data, based on their strength (importance). In short, we train a sparse denoising
autoencoder network from scratch in an unsupervised adaptive manner. Then, we
use the trained network to derive the strength of each neuron in the input features.
The basic idea of our proposed approach is to impose sparse connections on
DAE, which proved its success in the related Ô¨Åeld of feature extraction, to eÔ¨Éciently
handle the computational complexity of high-dimensional data in terms of memory
resources. Sparse connections are evolved in an adaptive manner that helps in
identifying informative features.
A couple of methods have been proposed for training deep neural networks
from scratch using sparse connections and sparse training [ 14,45,7,46,17,59]. All
these methods are implemented using a binary mask over connections to simulate
sparsity since all standard deep learning libraries and hardware (e.g. GPUs) are not
optimized for sparse weight matrix operations. Unlike the aforementioned methods,
we implement our proposed method in a purely sparse manner to meet our goal of
actually using the advantages of a very small number of parameters during training.
We decided to use SET in training our sparse DAE.
The choice of SET is due to its desirable characteristic. SET is a simple method
yet achieves satisfactory performance. Unlike other methods that calculate and
store information for all the network weights, including the non-existing ones, SET
is memory eÔ¨Écient. It stores the weights for the existing sparse connections only.
It does not need any high computational complexity as the evolution procedure
depends on the magnitude of the existing connections only. This is a favourable
advantage to our proposed method to select informative features quickly. In the
following subsections, we Ô¨Årst present the structure of our proposed sparse denoising
autoencoder network and then explain the feature selection method. The pseudo-
code of our proposed method can be found in Algorithm 1.
3.1 Sparse DAE
Structure As the goal of our proposed method is to do fast feature selection in a
memory-eÔ¨Écient way, we consider here the model with the least possible number of
hidden layers, one hidden layer, as more layers mean more computation. Initially,
sparse connections between two consecutive layers of neurons are initialized with
an Erd≈ës‚ÄìR√©nyi random graph, in which the probability of the connection between
two neurons is given by
P(Wl
ij) =(nl 1+nl)
nl 1nl; (1)
wheredenotes the parameter that controls the sparsity level, nldenotes number
of neurons at layer l, andWl
ijis the connection between neuron iin layerl 1and
neuronjin layerl, stored in the sparse weight matrix Wl.
Input denoising We use the additive noise model to corrupt the original data:
ex=x+nfN(; 2); (2)

--- PAGE 7 ---
Quick and Robust Feature Selection 7
where xis the input data vector from dataset X,nf(noise factor) is a hyperpa-
rameter of the model which determines the level of corruption, and N(; 2)is
a Gaussian noise. After denoising the data, we derive the hidden representation
husing this corrupted input. Then, the output zis reconstructed from the hid-
den representation. Formally, the hidden representation hand the output zare
computed as follows:
h=a(W1ex+b1); (3)
z=a(W2h+b2); (4)
where W1andW2are the sparse weight matrices of hidden and output layers
respectively, b1andb2are the bias vectors of their corresponding layer, and ais
the activation function of each layer. The objective of our network is to reconstruct
the original features in the output. For this reason, we use mean squared error
(MSE) as the loss function to measure the diÔ¨Äerence between original features x
and the reconstructed output z:
LMSE =kz xk2
2: (5)
Finally, the weights can be optimized using the standard training algorithms
(e.g., Stochastic Gradient Descent (SGD), AdaGrad, and Adam) with the above
reconstruction error.
Training procedure We adapt the SET training procedure [ 45] in training our
proposed network for feature selection. SET works as follows. After each training
epoch, a fraction of the smallest positive weights and a fraction of the largest
negative weights at each layer is removed. The selection is based on the magnitude
of the weights. New connections in the same amount as the removed ones are
randomly added in each layer. Therefore the total number of connections in each
layer remains the same, while the number of connections per neuron varies, as
represented in Figure 1. The weights of these new connections are initialized from
a standard normal distribution.The random addition of new connections do not
have a high risk of not Ô¨Ånding a good sparse connectivity at the end of the training
process because it has been shown in [ 41] that sparse training can unveil a vast
number of very diÔ¨Äerent sparse connectivity local optima which achieve a very
similar performance.
3.2 Feature Selection
We select the most important features of the data based on the weights of their
corresponding input neurons of the trained sparse DAE. Inspired by node strength
in graph theory [ 6], we determine the importance of each neuron based on its
strength . We estimate the strength of each neuron ( si) by the summation of
absolute weights of its outgoing connections.
si=n1X
j=1jW1
ijj; (6)
wheren1is the number of neurons of the Ô¨Årst hidden layer, and W1
ijdenotes the
weight of connection linking input neuron ito hidden neuron j.

--- PAGE 8 ---
8 Zahra Atashgahi et al.
  Random Initialization  After 3 epochs  After 6 Epochs  After 10 epochs    height (pixel)  
     
Strength   width (pixel)  width (pixel)  width (pixel)  width (pixel)    
Fig. 2: Neuron‚Äôs strength on the MNIST dataset. The heat-maps above are a 2D
representation of the input neuron‚Äôs strength. It can be observed that the strength
of neurons is random at the beginning of training. After a few epochs, the pattern
changes, and neurons in the center become more important and similar to the
MNIST data pattern.
As represented in Figure 1, the strength of the input neurons changes during
training; we have depicted the strength of the neurons according to their size and
color. After convergence, we compute the strength for all of the input neurons; each
input neuron corresponds to a feature. Then, we select the features corresponding
to the neurons with Klargest strength values:
F
s= argmax
FsF;jFsj=kX
fi2Fssi; (7)
where FandF
saretheoriginalfeaturesetandtheÔ¨Ånalselectedfeaturesrespectively,
fiis theithfeature of F, andKis the number of features to be selected. In addition,
by sorting all the features based on their strength, we will derive the importance
of all features in the dataset. In short, we will be able to rank all input features by
training just once a single sparse DAE model.
Algorithm 1 QuickSelection
1:Input:DatasetX, noise factor nf, sparsity hyperparameter , number of hidden neurons
nh, number of selected features K
2: Input denoising: ex=x+nfN(; 2)
3:Structure initialization: Initialize sparse-DAE with nhhidden neurons and sparsity
level determined by 
4:procedure Training sparse-DAE
5: Let the loss be LMSE =kz xk2
2wherezis the output of sparse-DAE
6: fori2f1;:::;epochsgdo
7: Perform standard forward propagation and backpropagation
8: Perform weight removal and addition for topology optimization
9:procedure QuickSelection
10: Compute neurons strength:
11: fori2f1;:::; #inputfeaturesgdo
12:si=nhP
j=1jW1
ijj
13: Select Kfeatures: F
s= argmaxFsF;jFsj=KP
fi2Fssi;

--- PAGE 9 ---
Quick and Robust Feature Selection 9
For a deeper understanding of the above process, we analyze the strength of
each input neuron in a 2D map on the MNIST dataset. This is illustrated in Figure
2. At the beginning of training, all the neurons have small strength due to the
random initialization of each weight to a small value. During the network evolution,
stronger connections are linked to important features gradually. We can observe
that after ten epochs, the neurons in the center of the map become stronger. This
pattern is similar to the pattern of MNIST data in which most of the digits appear
in the middle of the picture.
We studied other metrics for estimating the neuron importance such as the
strength of output neurons, degree of input and output neurons, and strength and
degree of neurons simultaneously. However, in our experiments, all these methods
have been outperformed by the strength of the input neurons in terms of accuracy
and stability.
4 Experiments
In order to verify the validity of our proposed method, we carry out several exper-
iments. In this section, Ô¨Årst, we state the settings of the experiments, including
hyperparameters and datasets. Then, we perform feature selection with QuickSe-
lection and compare the results with other methods, including MCFS, Laplacian
Score, and three autoencoder-based feature selection methods. After that, we do
diÔ¨Äerent analyses on QuickSelection to understand its behavior. Finally, we discuss
the scalability of QuickSelection and compare it with the other methods considered.
4.1 Settings
The experiment settings, including the values of hyperparameters, implementation
details, the structure of the sparse DAE, datasets we use for evaluation, and the
evaluation metric, are as follows.
4.1.1 Hyperparameters and Implementation
For feature selection, we consider the case of the simplest sparse DAE with one
hidden layer consisting of 1000 neurons. This choice is made due to our main
objective to decrease the model complexity and the number of parameters. The
activation function used for the hidden and output layer neurons is ‚ÄúSigmoid‚Äù and
‚ÄúLinear‚Äù respectively, except for the Madelon dataset where we use ‚ÄúTanh‚Äù for the
output activation function. We train the network with SGD and a learning rate
of0:01. The hyperparameter , the fraction of weights to be removed in the SET
procedure, is 0:2. Also,, which determines the sparsity level, is set to 13. We set
the noise factor ( nf) to 0.2 in the experiments. To improve the learning process of
our network, we standardize the features of our dataset such that each attribute
has zero mean and unit variance . However, for SMK and PCMAC datasets, we
use Min-Max scaling. The preprocessing method for each dataset is determined
with a small experiment of the two preprocessing method.

--- PAGE 10 ---
10 Zahra Atashgahi et al.
We implement sparse DAE and QuickSelection2in a purely sparse manner in
Python, using the Scipy library [ 28] and Cython. We compare our proposed method
to MCFS, Laplacian score (LS), AEFS, and CAE, which have been mentioned
in Section 2. We also performed some experiments with UDFS; however, since
we were not able to obtain many of the results in the considered time limit (24
hours), we do not include the results in the paper. We have used the scikit-feature
repository for the implementation of MCFS, and Laplacian score [ 37]. Also, we
use the implementation of feature selection with CAE and AEFS from Github3.
In addition, to highlight the advantages of using sparse layers, we compare our
results with a fully-connected autoencoder (FCAE) using the neuron strength as a
measure of the importance of each feature. To have a fair comparison, the structure
of this network is considered similar to our DAE, one hidden layer containing 1000
neurons implemented using TensorFlow. Furthermore, we have studied the eÔ¨Äect of
other components of QuickSelection, including input denoising and SET training
algorithm, in Appendix B.1 and F, respectively.
For all the other methods (except FCAE for which all the hyperparameters
and preprocessing are similar to QuickSelection), we scaled the data between
zero and one, since it yields better performance than data standardization for
these methods. The hyperparameters of the aforementioned methods have been
set similar to the ones reported in the corresponding code or paper. For AEFS,
we tuned the regularization hyperparameter between 0:0001and1000, since this
method is sensitive to this value. We perform our experiments on a single CPU
core, Intel Xeon Processor E5 v4, and for the methods that require GPU, we use
NVIDIA TESLA P100.
4.1.2 Datasets
We evaluate the performance of our proposed method on eight datasets, including
Ô¨Åve low-dimensional datasets and three high-dimensional ones. Table 1 illustrates
the characteristics of these datasets.
‚Äì COIL-20 [47] consists of 1440images taken from 20objects ( 72poses for each
object).
‚Äì Madelon [23] is an artiÔ¨Åcial dataset with 5 informative features and 15 linear
combinations of them. The rest of the features are distractor features since they
have no predictive power.
‚Äì Human Activity Recognition (HAR) [3] is created by collecting the obser-
vations of 30subjects performing 6activities such as walking, standing, and
sitting. The data was recorded by a smart-phone connected to the subjects‚Äô
body.
‚Äì Isolet [18] has been created with the spoken name of each letter of the English
alphabet.
‚Äì MNIST [34] is a database of 28x28 images of handwritten digits.
‚Äì SMK-CAN-187 [50] is a gene expression dataset with 19993 features. This
dataset compares smokers with and without lung cancer.
2The implementation of QuickSelection is available at: https://github.com/
zahraatashgahi/QuickSelection
3The implementation of AEFS and CAE is available at: https://github.com/mfbalin/
Concrete-Autoencoders

--- PAGE 11 ---
Quick and Robust Feature Selection 11
Table 1: Datasets characteristics.
Dataset Dimensions Type Samples Train Test Classes
Coil20 1024 Image 1440 1152 288 20
Isolet 617 Speech 7737 6237 1560 26
HAR 561 Time Series 10299 7352 2947 6
Madelon 500 ArtiÔ¨Åcial 2600 2000 600 2
MNIST 784 Image 70000 60000 10000 10
SMK-CAN-187 19993 Microarray 187 149 38 2
GLA-BRA-180 49151 Microarray 180 144 36 4
PCMAC 3289 Text 1943 1554 389 2
‚Äì GLA-BRA-180 [51] consists of the expression proÔ¨Åle of Stem cell factor useful
to determine tumor angiogenesis.
‚Äì PCMAC [32] is a subset of the 20 Newsgroups data.
4.1.3 Evaluation Metrics
To evaluate our model, we compute two metrics: clustering accuracy and classiÔ¨Åca-
tion accuracy. To derive clustering accuracy [ 37], Ô¨Årst, we perform K-means using
the subset of the dataset corresponding to the selected features and get the cluster
labels. Then, we Ô¨Ånd the best match between the class labels and the cluster labels
and report the clustering accuracy. We repeat the K-means algorithm 10 times
and report the average clustering results since K-means may converge to a local
optimal.
To compute classiÔ¨Åcation accuracy, we use a supervised classiÔ¨Åcation model
named ‚ÄúExtremely randomized trees‚Äù (ExtraTrees), which is an ensemble learning
method that Ô¨Åts several randomized decision trees on diÔ¨Äerent parts of the data [ 21].
The choice of the classiÔ¨Åcation method is made due to the computational-eÔ¨Éciency
of the ExtraTrees classiÔ¨Åer. To compute classiÔ¨Åcation accuracy, Ô¨Årst, we derive the
Kselected features using each feature selection method considered. Then, we train
the ExtraTrees classiÔ¨Åer with 50trees as estimators on the Kselected features of
the training set. Finally, we compute the classiÔ¨Åcation accuracy on the unseen test
data. For the datasets that do not contain a test set, we split the data into training
and testing sets, including 80%of the total original samples for the training set
and the remaining 20%for the testing set. In addition, we have evaluated the
classiÔ¨Åcation accuracy of feature selection using the random forest classiÔ¨Åer [ 39] in
Appendix G.
4.2 Feature Selection
We select 50 features from each dataset except Madelon, for which we select just
20 features since most of its features are non-informative noise. Then, we compute
the clustering and classiÔ¨Åcation accuracy on the selected subset of features; the
more informative features selected, the higher accuracy will be achieved. The
clustering and classiÔ¨Åcation accuracy results of our model and the other methods
is summarized in Tables 2 and 3, respectively. These results are an average of 5
runs for each case. For the autoencoder-based feature selection methods, including
CAE, AEFS, and FCAE, we consider 100 training epochs. However, we present

--- PAGE 12 ---
12 Zahra Atashgahi et al.
Table 2: Clustering accuracy (%) using 50 selected features (except Madelon for
which we select 20 features). On each dataset, the bold entry is the best-performer,
and the italic one is the second-best performer.
Method COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS67.00.7 33.80.562.40.057.20.0 35.2 0 51.6 0.265.80.350.60.0
LS 55.5 0.4 33.2 0.2 61.20.0 58.1 0.014.90.1 51.6 0.4 55.5 0.4 50.6 0.0
CAE 60.0 1.1 31.6 1.3 51.4 0.4 56.9 3.649.21.5 60.7 0.455.41.3 52.01.2
AEFS 51.2 1.7 31.0 2.7 55.0 2.2 50.8 0.2 40.0 1.9 52.4 1.8 56.1 5.2 50.9 0.5
FCAE 60.21.728.72.5 49.5 8.7 50.9 0.4 28.2 8.5 51.5 0.8 53.5 3.0 50.9 0.1
QS1059.52.1 32.5 2.8 56.0 2.6 57.5 3.8 45.4 3.9 54.03.153.64.7 50.9 0.5
QS100 60.22.035.12.754.64.558.21.5 48.32.451.80.8 59.51.852.51.1
QSbest63.81.5 42.2 2.6 59.5 4.3 58.6 0.9 48.3 2.4 54.9 1.39 59.5 1.8 53.1 0
Table 3: ClassiÔ¨Åcation accuracy (%) using 50 selected features (except Madelon for
which we select 20 features). On each dataset, the bold entry is the best-performer,
and the italic one is the second-best performer.
Method COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS 99.2 0.3 79.5 0.4 88.9 0.3 81.7 0.8 88.7 0 75.8 1.5 70.6 3.8 55.5 0.0
LS 89.8 0.4 83.0 0.2 86.4 0.491.40.920.70.1 71.6 5.6 71.7 1.1 50.4 0.0
CAE 99.60.389.80.6 91.7 1.087.52.095.40.171.63.1 70.0 4.159.91.5
AEFS 93.0 2.7 85.1 2.4 87.7 1.4 52.1 2.8 86.1 2.0 76.34.468.93.7 57.1 3.6
FCAE99.70.281.65.9 87.4 2.4 53.5 8.1 68.8 28.7 71.6 3.5 72.84.858.11.9
QS1098.80.6 86.9 1.1 88.8 0.7 86.6 3.6 93.80.676.94.669.43.0 58.94.4
QS10099.70.3 89.01.3 90.2 1.2 90.3 0.793.50.5 75.7 3.973.33.358.02.9
QSbest99.70.3 89.0 1.3 90.5 1.6 90.9 0.5 94.2 0.5 81.6 2.9 73.3 3.3 61.3 6.1
the results of QuickSelection at epoch 10 and 100 named QuickSelection 10and
QuickSelection 100, respectively. This is mainly due to the fact that our proposed
method is able to achieve a reasonable accuracy after the Ô¨Årst few epochs. Moreover,
we perform hyperparameter tuning for andusing the grid search method over a
limited number of values for all datasets; the best result is presented in Table 2
and 3 as QuickSelection best. The results of hyperparameters selection can be found
in Appendix B.2. However, we do not perform hyperparameter optimization for
the other methods (except AEFS). Therefore, in order to have a fair comparison
between all methods, we do not compare the results of QuickSelection bestwith the
other methods.
From Table 2, it can be observed that QuickSelection outperforms all the other
methods on Isolet, Madelon, and PCMAC, in terms of clustering accuracy, while
being the second-best performer on Coil20, MNIST, SMK, and GLA. Furthermore,
On the HAR dataset, it is the best performer among all the autoencoder-based
feature selection methods considered. As shown in Table 3, QuickSelection outper-
forms all the other methods on Coil20, SMK, and GLA, in terms of classiÔ¨Åcation
accuracy, while being the second-best performer on the other datasets. From these
Tables, it is clear that QuickSelection can outperform its equivalent dense network
(FCAE) in terms of classiÔ¨Åcation and clustering accuracy on all datasets.

--- PAGE 13 ---
Quick and Robust Feature Selection 13
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000006/uni00000003/uni00000052/uni00000049/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni00000055/uni00000048/uni00000050/uni00000052/uni00000059/uni00000048/uni00000047/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000003A/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003 (/uni00000008)
/uni0000002f/uni00000048/uni00000044/uni00000056/uni00000057/uni00000003/uni00000057/uni00000052/uni00000003/uni00000050/uni00000052/uni00000056/uni00000057/uni00000003/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057
/uni00000013/uni00000003 /uni00000014/uni00000013/uni00000013/uni00000003 /uni00000015/uni00000013/uni00000013/uni00000003 /uni00000016/uni00000013/uni00000013/uni00000003 /uni00000017/uni00000013/uni00000013/uni00000003 /uni00000018/uni00000013/uni00000013
/uni00000006/uni00000003/uni00000052/uni00000049/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni00000055/uni00000048/uni00000050/uni00000052/uni00000059/uni00000048/uni00000047/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000003A/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003 (/uni00000008)
/uni00000030/uni00000052/uni00000056/uni00000057/uni00000003/uni00000057/uni00000052/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000056/uni00000057/uni00000003/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057
/uni00000030/uni00000052/uni00000056/uni00000057/uni00000003/uni00000057/uni00000052/uni00000003 l/uni00000048/uni00000044/uni00000056/uni00000057/uni00000003/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057
Fig. 3: InÔ¨Çuence of feature removal on Madelon dataset. After deriving the impor-
tance of the features with QuickSelection, we sort and then remove them based on
the above two methods.
It can be observed in Tables 2 and 3, that Lap_score has a poor performance
when the number of samples is large (e.g. MNIST). However, in the tasks with a
low number of samples and features, even on noisy environments such as Madelon,
Lap_score has a relatively good performance. In contrast, CAE has a poor perfor-
mance in noisy environments (e.g., Madelon), while it has a decent classiÔ¨Åcation
accuracy on the other datasets considered. It is the best or second-best performer
on Ô¨Åve datasets, in terms of classiÔ¨Åcation accuracy, when K= 50. AEFS and
FCAE cannot achieve a good performance on Madelon, either. We believe that
the dense layers are the main cause of this behaviour; the dense connections try
to learn all input features, even the noisy features. Therefore, they fail to detect
the most important attributes of the data. MCFS performs decently on most of
the datasets in terms of clustering accuracy. This is due to the main objective of
MCFS to preserve the multi-cluster structure of the data. However, this method
also has a poor performance on the datasets with a large number of samples (e.g.,
MNIST) and noisy features (e.g., Madelon).
However, since evaluating the methods using a single value of Kmight not be
enough for comparison, we performed another experiment using diÔ¨Äerent values of
K. In Appendix A.1, we test other values for Kon all datasets, and compare the
methods in terms of classiÔ¨Åcation accuracy, clustering accuracy, running time, and
maximum memory usage. The summary of the results of this Appendix has been
summarized in Section 5.1.
4.2.1 Relevancy of Selected Features
To illustrate the ability of QuickSelection in Ô¨Ånding informative features, we
analyze thoroughly the Madelon dataset results, which has the interesting property
of containing many noisy features. We perform the following experiments; Ô¨Årst,
we sort the features based on their strength. Then, we remove the features one
by one from the least important feature to the most important one. In each
step, we train an ExtraTrees classiÔ¨Åer with the remained features. We repeat this
experiment by removing the feature from the most important ones to the least
important ones. The result of classiÔ¨Åcation accuracy for both experiments can be
seen in Figure 3. On the left side of Figure 3, we can observe that removing the
least important features, which are noise, increases the accuracy. The maximum
accuracy occurs after we remove 480 noise features. This corresponds to the moment

--- PAGE 14 ---
14 Zahra Atashgahi et al.
      
      
 
Fig. 4: Average values of all data samples of each class corresponding to the 50
selected features on MNIST after 100 training epochs (bottom), along with the
average of the actual data samples of each class (top).
when all the noise features are supposed to be removed. In Figure 3 (right), it
can be seen that removing the features in a reverse order results in a sudden
decrease in the classiÔ¨Åcation accuracy. After removing 20 features (indicated by
the vertical blue line), the classiÔ¨Åer performs like a random classiÔ¨Åer. We conclude
that QuickSelection is able to Ô¨Ånd the most informative features in good order.
To better show the relevancy of the features found by QuickSelection, we
visualize the 50 features selected on the MNIST dataset per class, by averaging
their corresponding values from all data samples belonging to one class. As can
be observed in Figure 4, the resulting shape resembles the actual samples of the
corresponding digit. We discuss the results of all classes at diÔ¨Äerent training epochs
in more detail in Appendix C.
5 Discussion
5.1 Accuracy and Computational EÔ¨Éciency Trade-oÔ¨Ä
In this section, we perform a thorough comparison between the models in terms of
running time, energy consumption, memory requirement, clustering accuracy, and
classiÔ¨Åcation accuracy. In short, we change the number of features to be selected
(K) and measure the accuracy, running time, and maximum memory usage across
all methods. Then, we compute two scores to summarize the results and compare
methods.
WeanalysetheeÔ¨Äectofchanging KonQuickSelectionperformanceandcompare
with other methods; the results are presented in Figure 10 in Appendix A.1. Figure
10a compares the performance of all methods when Kis changing between 5 and
100 on low-dimensional datasets, including Coil20, Isolet, HAR, and Madelon.
Figure 10b illustrates performance comparison for Kbetween 5 and 300 on the
MNIST dataset, which is also a low-dimensional dataset. We discuss this dataset
separately since it has a large number of samples that makes it diÔ¨Äerent from other
low-dimensional datasets. Figure 10c represents a similar comparison on three
high-dimensional datasets, including SMK, GLA, and PCMAC. It should be noted
that to have a fair comparison, we use a single CPU core to run these methods;
however, since the implementations of CAE and AEFS are optimized for parallel

--- PAGE 15 ---
Quick and Robust Feature Selection 15
/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c
(a) Score 1. Scores are given
based on the ranking of the
methods.
/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000014/uni00000017/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c(b) Score 2. Scores are given
basedonthenormalizedvalue
of each objective.
/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c
/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013
/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni00000014
(c) Heat-map visualization of
Score 1.
Fig. 5: Feature selection comparison in terms of classiÔ¨Åcation accuracy, clustering
accuracy, speed, and memory requirement, on each dataset and for diÔ¨Äerent values
ofK, using two scoring variants.
computation, we use a GPU to run these methods. We also measure the running
time of feature selection with CAE on CPU.
To compare the memory requirement of each method, we proÔ¨Åle the maximum
memory usage during feature selection for diÔ¨Äerent values of K. The results are
presented in Figure 11 in Appendix A.1, derived using a Python library named
resource4. Besides, to compare memory occupied by the autoencoder-based models,
we count the number of parameters for each model. The results are shown in Figure
14 in the Appendix A.3.
However, comparing all of these methods only by looking into the graphs in
Figure 10 and Figure 11 is not easily possible, and the trade-oÔ¨Ä between the factors
is not clear. For this reason, we compute two scores to take all these metrics into
account simultaneously.
Score 1. To compute this score, on each dataset and for each value of K, we
rank the methods based on the running time, memory requirement, clustering
accuracy, and classiÔ¨Åcation accuracy. Then, we give a score of 1to the best and
second-best performer; this is mainly due to the fact that in most cases, the
diÔ¨Äerence between these two is negligible. After that, we compute the summation
of these scores for each method on all datasets. The results are presented in
Figure 5a; to ease the comparison of diÔ¨Äerent components in the score, a heat-map
visualization of the scores is presented in Figure 5c. The cumulative score for each
method consists of four parts that correspond to each metric considered. As it
is obvious in this Figure, QuickSelection (cumulative score of QuickSelection 10
and QuickSelection 100) outperforms all other methods by a signiÔ¨Åcant gap. Our
proposed method is able to achieve the best trade-oÔ¨Ä between accuracy, running
time, and memory usage, among all these methods. Laplacian score, the second-
best performer, has a decent performance in terms of running time and memory,
while it cannot perform well in terms of accuracy. On the other hand, CAE has a
satisfactory performance in terms of accuracy. However, it is not among the best
two performers in terms of computational resources for any values of K. Finally,
FCAE and AEFS cannot achieve a decent performance compared to the other
methods. A more detailed version of Figure 5a is available in Figure 12 in Appendix
A.1.
4https://docs.python.org/2/library/resource.html

--- PAGE 16 ---
16 Zahra Atashgahi et al.
Score 2. In addition to the raking-based score, we calculate another score to
consider all the methods, even the lower-ranking ones. With this aim, on each
dataset and value of K, we normalize each performance metric between 0and1,
using the values of the best performer and worst performer on each metric. The
value of 1in the accuracy score means the highest accuracy. However, for the
memory and running time, the value of 1means the least memory requirement and
the least running time, respectively. After normalizing the metrics, we accumulate
the normalized values for each method and on all datasets. The results are depicted
in Figure 5b. As can be seen in this diagram, QuickSelection (we consider the
results of QuickSelection 100) outperforms the other methods by a large margin.
CAE has a close performance to QuickSelection in terms of both accuracy metrics,
while it has a poor performance in terms of memory and running time. In contrast,
Lap_score is computationally eÔ¨Écient while having the lowest accuracy score. In
summary, it can be observed in Figure 5b, that QuickSelection achieves the best
trade-oÔ¨Ä of the four objectives among the considered methods.
Energy Consumption. The next analysis we perform concerns the energy
consumption of each method. We estimate the energy consumption of each method
using the running time of the corresponding algorithm for each dataset and value
ofK. We assume that each method uses the maximum power of the corresponding
computational resources during its running time. Therefore, we derive the power
consumption of each method, using the running time and maximum power con-
sumption of CPU and/or GPU, which can be found within the speciÔ¨Åcation of
the corresponding CPU or GPU model. As shown in Figure 13 in Appendix A.2,
the Laplacian score feature selection needs the least amount of energy among the
methods on all datasets except the MNIST dataset. QuickSelection 10is the best
performer on MNIST in terms of energy consumption. Laplacian score and MCFS
are sensitive to the number of samples. They cannot perform well on MNIST, either
in terms of accuracy or eÔ¨Éciency. The maximum memory usage during feature se-
lection for Laplacian score and MCFS on MNIST is 56 GB and 85 GB, respectively.
Therefore, they are not a good choice in case of having a large number of samples.
QuickSelection is the second-best performer in terms of energy consumption, and
also the best performer among the autoencoder-based methods. QuickSelection is
not sensitive to the number of samples or the number of dimensions.
EÔ¨Éciency vs Accuracy. In order to study the trade-oÔ¨Ä between accuracy
and resource eÔ¨Éciency, we perform another in-depth analysis. In this analysis,
we plot the trade-oÔ¨Ä between accuracy (including, classiÔ¨Åcation and clustering
accuracy) and resource requirement (including, memory and energy consumption).
The results are shown in Figures 6 and 7 that correspond to the energy-accuracy
and memory-accuracy trade-oÔ¨Ä, respectively. Each point in these plots refers to the
results of a particular combination between a speciÔ¨Åc method and dataset when
selecting 50 features (except Madelon, for which we select 20 features). As can
be observed in these plots, QuickSelection, MCFS, and Lap_score usually have a
good trade-oÔ¨Ä between the considered metrics. A good trade-oÔ¨Ä between a pair of
metrics is to maximize the accuracy (classiÔ¨Åcation or clustering accuracy) while
minimizing the computational cost (power consumption or memory requirement).
However, when the number of samples increases (on the MNIST dataset), both
MCFS and Lap_score fail to maintain a low computational cost and high accuracy.
Therefore, when the dataset size increases, these two methods are not an optimal
choice. Among the autoencoder-based methods, in most cases QuickSelection 10

--- PAGE 17 ---
Quick and Robust Feature Selection 17
20 30 40 50 60
Clustering Accuracy (%)104
103
102
101
100101102103Power Consumption (Kwh)
20 30 40 50 60 70 80 90 100
Classification Accuracy (%)104
103
102
101
100101102103Power Consumption (Kwh)Coil20
Isolet
HAR
Madelon
MNIST
SMK
GLA
PCMAC
FCAE
CAE(GPU)
AEFS(GPU)
MCFS
Lap_score
QS_10
QS_100
Fig. 6: Estimated power consumption (Kwh) vs. accuracy (%) when selecting 50
features (except Madelon for which we select 20 features). Each point refers to the
result of a single dataset (speciÔ¨Åed by colors) and method (speciÔ¨Åed by markers)
where the x and y-axis show the accuracy and the estimated power consumption,
respectively.
20 30 40 50 60
Clustering Accuracy (%)02468Maximum memory requirement (Kb)1e7
20 30 40 50 60 70 80 90 100
Classification Accuracy (%)02468Maximum memory requirement (Kb)1e7
20 30 40 50 60
Clustering Accuracy (%)0.00.51.01.52.02.53.0Maximum memory requirement (Kb)1e6
20 30 40 50 60 70 80 90 100
Classification Accuracy (%)0.00.51.01.52.02.53.0Maximum memory requirement (Kb)1e6
Coil20
Isolet
HAR
Madelon
MNIST
SMK
GLA
PCMAC
FCAE
CAE(GPU)
AEFS(GPU)
MCFS
Lap_score
QS
Fig. 7: Maximum memory requirement (Kb) vs. accuracy (%) when selecting 50
features (except Madelon for which we select 20 features). Each point refers to the
result of a single dataset (speciÔ¨Åed by colors) and method (speciÔ¨Åed by markers)
where the x and y-axis show the accuracy and the maximum memory requirement,
respectively. Due to the high memory requirement of MCFS and Lap_score on the
MNIST dataset which makes it diÔ¨Écult to compare the other results (upper plots),
we zoom in this section in the bottom plots.
and QuickSelection 100are among the Pareto optimal points. Another signiÔ¨Åcant
advantage of our proposed method is that it gives the ranking of the features as
the output. Therefore, unlike the MCFS or CAE that need the value of Kas their
input, QuickSelection is not dependent on Kand needs just a single training of
the sparse DAE model for any values of K. Therefore, the computational cost
of QuickSelection is the same for all values of K, and only a single run of this
algorithm is required to get the hierarchical importance of features.

--- PAGE 18 ---
18 Zahra Atashgahi et al.
0 5000 10000 15000 20000 25000 30000 35000 40000
# of features02500500075001000012500150001750020000running time (s)
QS_10 (K=All, n=1000, CPU)
QS_100 (K=All, n=1000, CPU)
QS_100 (K=All, n=10000, CPU)
CAE (K=100, n=150, GPU)
CAE (K=300, n=450, GPU)
CAE (K=100, n=1000, GPU)
CAE (K=100, n=10000, GPU)
CAE (K=100, n=150, CPU)
AEFS (K=All, n=300, GPU)
AEFS (K=All, n=1000, GPU)
AEFS (K=All, n=10000, GPU)
FCAE (K=All, n=1000, CPU)
FCAE (K=All, n=10000, CPU)
Fig. 8: Running time comparison on an artiÔ¨Åcially generated dataset. The features
are generated using a standard normal distribution and the number of samples for
each case is 5000.
5.2 Running Time Comparison on an ArtiÔ¨Åcially Generated Dataset
In this section, we perform a comparison of the running time of the autoencoder-
based feature selection methods on an artiÔ¨Åcially generated dataset. Since on the
benchmark datasets both the number of features and samples are diÔ¨Äerent, it is not
easily possible to compare clearly the eÔ¨Éciency of the methods. This experiment
aims at comparing the models real wall-clock training time in a controlled environ-
ment with respect to the number of input features and hidden neurons. In addition,
in Appendix E, we have conducted another experiment regarding evaluation of the
methods on a very large artiÔ¨Åcial dataset, in terms of both computational resources
and accuracy.
In this experiment, we aim to compare the speed of QuickSelection versus
other autoencoder-based feature selection methods for diÔ¨Äerent numbers of input
features. We run all of them on an artiÔ¨Åcially generated dataset with various
numbers of features and 5000 samples, for 100 training epochs (10 epochs for
QuickSelection 10). The features of this dataset are generated using a standard
normal distribution. In addition, we aim to compare the running time of diÔ¨Äerent
structures for these algorithms. The speciÔ¨Åcations of the network structure for
each method, the computational resources used for feature selection, and the
corresponding results can be seen in Figure 8.
For CAE, we consider two diÔ¨Äerent values of K. The structure of CAE depends
on this value. CAE has two hidden layers including a concrete selector and a
decoder that have Kand1:5Kneurons, respectively. Therefore, by increasing the
number of selected features, the running time of the model will also increase. In
addition, we consider the cases of CAE with 1000and10000hidden neurons in the
decoder layer (manually changed in the code) to be able to compare it with the
other models. We also measure the running time of performing feature selection
with CAE using only a single CPU core. It can be seen from Figure 8 that its
running time is considerably high. The general structures of AEFS, QuickSelection,
and FCAE are similar in terms of the number of hidden layers. They are basic

--- PAGE 19 ---
Quick and Robust Feature Selection 19
autoencoders with a single hidden layer. For AEFS, we considered three structures
with diÔ¨Äerent numbers of hidden neurons, including 300, 1000, and 10000. Finally,
for QuickSelection and FCAE, we consider two diÔ¨Äerent values for the number of
hidden neurons, including 1000 and 10000.
It can be observed that the running time of AEFS with 1000and10000hidden
neurons using a GPU, is much larger than the running time of QuickSelection 100
with similar numbers of hidden neurons using only a single CPU core, respectively.
The same pattern is also visible in the case of CAE with 1000and10000hidden
neurons. This pattern also repeats in the case of FCAE with 10000hidden neurons.
The running time of FCAE with 1000 hidden neurons is approximately similar
to QuickSelection 100. However, the diÔ¨Äerence between these two methods is more
signiÔ¨Åcant when we increase the number of hidden neurons to 10000. This is
mainly due to the fact that the diÔ¨Äerence between the number of parameters of
QuickSelection and the other methods become much higher for large values of K.
Besides, these observations depict that the running time of QuickSelection does
not change signiÔ¨Åcantly by increasing the number of hidden neurons.
As we have also mentioned before, QuickSelection gives the ranking of the
features as the output. Therefore, unlike CAE which should be run separately for
diÔ¨Äerent values of K, QuickSelection is not aÔ¨Äected by the choice of K because
it computes the importance of all features at the same time and after Ô¨Ånishing
the training. In short, QuickSelection 10has the least running time among other
autoencoder-based methods while being independent of the value of K. In addition,
unlike the other methods, the running time of QuickSelection is not sensitive to
the number of hidden neurons since the number of parameters is low even for a
very large hidden layer.
5.3 Neuron Strength Analysis
In this section, we discuss the validity of neurons strength as a measure of the
feature importance. We observe the evolution of the network during training to
analyze how the neuron strength of important and unimportant neurons changes
during training.
We argue that the most important features that lead to the highest accuracy of
feature selection are the features corresponding to neurons with the highest strength.
In a neural network, weight magnitude is a metric that shows the importance of
each connection [ 29]. This stems from the fact that weights with a small magnitude
have small eÔ¨Äect on the performance of the model. At the beginning of training, we
initialize all connections to a small random value. Therefore, all the neurons have
almost the same strength/importance. As the training proceeds, some connections
grow to a larger value while some others are pruned from the network during the
dynamic connections removal and regrowth of the SET training procedure. The
growth of the stable connection weights demonstrates their signiÔ¨Åcance in the
performance of the network. As a result, the neurons connected to these important
weights contain important information. In contrast, the magnitude of the weights
connected to unimportant neurons gradually decreases until they are removed
from the network. In short, important neurons receive connections with a larger
magnitude. As a result, neuron strength, which is the summation of the magnitude

--- PAGE 20 ---
20 Zahra Atashgahi et al.
of weights connected to a neuron, can be a measure of the importance of an input
neuron and its corresponding feature.
To support our claim, we observe the evolution of neurons‚Äô strength on the
Madelon dataset. This choice is made due to the distinction of informative and
non-informative features in the Madelon dataset. As described earlier, this dataset
has 20 informative features, and the rest of the features are non-informative noise.
We consider 20 most informative and non-informative features detected by QS10
andQS100, and monitor their strength during training (as observed in Figure 3,
the maximum accuracy is achieved using the 20 most informative features, while
the least accuracy is achieved using the least important features). The features
selected by QS10are also being monitored after the algorithm is Ô¨Ånished (epoch
10) until epoch 100, in order to compare the quality of the selected features by
QS10withQS100. In other words, we extract the index of important features using
QS10, and continue the training without making any changes in the network and
monitor how the strength of the neurons corresponding to the selected index would
evolve after epoch 10. The results are presented in Figure 9. At the initialization
(epoch 0), the strength of all these neurons is almost similar and below 5. As the
training starts, the strength of signiÔ¨Åcant neurons increases, while the strength of
unimportant neurons does not change signiÔ¨Åcantly. As can be seen in Figure 9,
some of the important features selected by QS10are not among those of QS100;
this can explain the diÔ¨Äerence in the performance of these two methods in Table
2 and 3. However, QS10is able to detect a large majority of the features found
byQS100; these features are among the most important ones among the Ô¨Ånal 20
selected features. Therefore, we can conclude that most of the important features
are detectable by QuickSelection, even at the Ô¨Årst few epochs of the algorithm.
6 Conclusion
In this paper, a novel method (QuickSelection) for energy-eÔ¨Écient unsupervised
feature selection has been proposed. It introduces neuron strength in sparse neural
networks as a measure of feature importance. Besides, it proposes sparse DAE
to accurately model the data distribution and to rank all features simultaneously
based on their importance. By using sparse layers instead of dense ones from the
beginning, the number of parameters drops signiÔ¨Åcantly. As a result, QuickSelection
requires much less memory and computational resources than its equivalent dense
model and its competitors. For example, on low-dimensional datasets, including
Coil20, Isolet, HAR, and Madelon, and for all values of K, QuickSelection 100which
runs on one CPU core is at least 4 times faster than its direct competitor, CAE,
which runs on a GPU, while having a close performance in terms of classiÔ¨Åcation
and clustering accuracy. We empirically demonstrate that QuickSelection achieves
the best trade-oÔ¨Ä between clustering accuracy, classiÔ¨Åcation accuracy, maximum
memory requirement, and running time, among other methods considered. Besides,
our proposed method requires the least amount of energy among autoencoder-based
methods considered.
The main drawback of the the proposed method is the lack of a parallel
implementation. The running time of QuickSelection can be further decreased by
an implementation that takes advantage of multi-core CPU or GPU. We believe
that interesting future research would be to study the eÔ¨Äects of sparse training

--- PAGE 21 ---
Quick and Robust Feature Selection 21
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000038/uni00000051/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni0000002c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000038/uni00000051/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni0000002c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
Fig. 9: Strength of the 20 most informative and non-informative features of Madelon
dataset, selected by QS10andQS100. Each line in the plots corresponds to the
strength values of a selected feature by QS10/QS100during training. The features
selected by QS10have been observed until epoch 100to compare the quality of
these features with QS100.
and neuron strength in other types of autoencoders for feature selection, e.g. CAE.
Nevertheless, this paper has just started to explore one of the most important
characteristics of QuickSelection, i.e. scalability, and we intend to explore further
its full potential on datasets with millions of features. Besides, this paper showed
that we can perform feature selection using neural networks eÔ¨Éciently in terms of
computational cost and memory requirement. This can pave the way for reducing
the ever-increasing computational costs of deep learning models imposed on data
centers. As a result, this will not only save the energy costs of processing high-
dimensional data but also will ease the challenges of high energy consumption
imposed on the environment.
Acknowledgements This research has been partly funded by the NWO EDIC project.
References
1.Amirali Aghazadeh, Ryan Spring, Daniel Lejeune, Gautam Dasarathy, An-
shumali Shrivastava, et al. Mission: Ultra large-scale feature selection using
count-sketches. In International Conference on Machine Learning , pages 80‚Äì88,
2018.
2.Jun Chin Ang, Andri Mirzal, Habibollah Haron, and Haza Nuzly Abdull
Hamed. Supervised, unsupervised, and semi-supervised feature selection: a

--- PAGE 22 ---
22 Zahra Atashgahi et al.
review on gene selection. IEEE/ACM transactions on computational biology
and bioinformatics , 13(5):971‚Äì989, 2015.
3.Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis
Reyes-Ortiz. A public domain dataset for human activity recognition using
smartphones. In Esann, 2013.
4.Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In
Proceedings of ICML workshop on unsupervised and transfer learning , pages
37‚Äì49, 2012.
5.Muhammed Fatih Balƒ±n, Abubakar Abid, and James Zou. Concrete autoen-
coders: DiÔ¨Äerentiable feature selection and reconstruction. In International
Conference on Machine Learning , pages 444‚Äì453, 2019.
6.Alain Barrat, Marc Barthelemy, Romualdo Pastor-Satorras, and Alessandro
Vespignani. The architecture of complex weighted networks. Proceedings of
the national academy of sciences , 101(11):3747‚Äì3752, 2004.
7.GuillaumeBellec,DavidKappel,WolfgangMaass,andRobertLegenstein. Deep
rewiring: Training very sparse deep networks. arXiv preprint arXiv:1711.05136 ,
2017.
8.Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning:
A review and new perspectives. IEEE transactions on pattern analysis and
machine intelligence , 35(8):1798‚Äì1828, 2013.
9.Ver√≥nica Bol√≥n-Canedo, Noelia S√°nchez-Maro√±o, and Amparo Alonso-Betanzos.
Feature selection for high-dimensional data . Springer, 2015.
10.David D. Bourgin, Joshua C. Peterson, Daniel Reichman, Stuart J. Rus-
sell, and Thomas L. GriÔ¨Éths. Cognitive model priors for predicting hu-
man decisions. In Kamalika Chaudhuri and Ruslan Salakhutdinov, edi-
tors,Proceedings of the 36th International Conference on Machine Learn-
ing, volume 97 of Proceedings of Machine Learning Research , pages 5133‚Äì
5141, Long Beach, California, USA, 09‚Äì15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/peterson19a.html .
11.Deng Cai, Chiyuan Zhang, and Xiaofei He. Unsupervised feature selection for
multi-cluster data. In Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining , pages 333‚Äì342. ACM,
2010.
12.Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods.
Computers & Electrical Engineering , 40(1):16‚Äì28, 2014.
13. Fran√ßois Chollet et al. Keras. https://keras.io , 2015.
14.Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster
training without losing performance. arXiv preprint arXiv:1907.04840 , 2019.
15.Guillaume Doquet and Mich√®le Sebag. Agnostic feature selection. In Joint Eu-
ropean Conference on Machine Learning and Knowledge Discovery in Databases ,
pages 343‚Äì358. Springer, 2019.
16.Jennifer G Dy and Carla E Brodley. Feature selection for unsupervised learning.
Journal of machine learning research , 5(Aug):845‚Äì889, 2004.
17.Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich
Elsen. Rigging the lottery: Making all tickets winners. arXiv preprint
arXiv:1911.11134 , 2019.
18.Mark Fanty and Ronald Cole. Spoken letter recognition. In Advances in Neural
Information Processing Systems , pages 220‚Äì226, 1991.

--- PAGE 23 ---
Quick and Robust Feature Selection 23
19.Ahmed K Farahat, Ali Ghodsi, and Mohamed S Kamel. EÔ¨Écient greedy feature
selection for unsupervised learning. Knowledge and information systems , 35
(2):285‚Äì310, 2013.
20.Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding
sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 , 2018.
21.Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized
trees.Machine learning , 63(1):3‚Äì42, 2006.
22.AI High-Level Expert Group. Assessment list for trustworthy artiÔ¨Åcial intelli-
gence (ALTAI) for self-assessment, 2020.
23.Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti A Zadeh. Feature
extraction: foundations and applications , volume 207. Springer, 2008.
24.Kai Han, Yunhe Wang, Chao Zhang, Chao Li, and Chao Xu. Autoencoder
inspired unsupervised feature selection. In 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , pages 2941‚Äì2945. IEEE,
2018.
25.Song Han, JeÔ¨Ä Pool, John Tran, and William Dally. Learning both weights and
connections for eÔ¨Écient neural network. In Advances in neural information
processing systems , pages 1135‚Äì1143, 2015.
26.Babak Hassibi and David G Stork. Second order derivatives for network
pruning: Optimal brain surgeon. In Advances in neural information processing
systems, pages 164‚Äì171, 1993.
27.Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection.
InAdvances in neural information processing systems , pages 507‚Äì514, 2006.
28.Eric Jones, Travis Oliphant, and Pearu Peterson. Scipy: Open source scientiÔ¨Åc
tools for python. 2001.
29.Taskin Kavzoglu and Paul M Mather. Assessing artiÔ¨Åcial neural network prun-
ing algorithms. In Proceedings of the 24th Annual Conference and Exhibition
of the Remote Sensing Society , pages 9‚Äì11, 1998.
30.RonKohaviandGeorgeHJohn. Wrappersforfeaturesubsetselection. ArtiÔ¨Åcial
intelligence , 97(1-2):273‚Äì324, 1997.
31.Thomas Navin Lal, Olivier Chapelle, Jason Weston, and Andr√© ElisseeÔ¨Ä.
Embedded methods. In Feature extraction , pages 137‚Äì165. Springer, 2006.
32.Ken Lang. Newsweeder: Learning to Ô¨Ålter netnews. In Machine Learning
Proceedings 1995 , pages 331‚Äì339. Elsevier, 1995.
33.Cosmin Lazar, Jonatan Taminau, Stijn Meganck, David SteenhoÔ¨Ä, Alain Co-
letta, Colin Molter, Virginie de Schaetzen, Robin Duque, Hugues Bersini, and
Ann Nowe. A survey on Ô¨Ålter techniques for feature selection in gene expression
microarray analysis. IEEE/ACM Transactions on Computational Biology and
Bioinformatics , 9(4):1106‚Äì1119, 2012.
34.Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
com/exdb/mnist/ , 1998.
35.Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In
Advances in neural information processing systems , pages 598‚Äì605, 1990.
36.Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-
shot network pruning based on connection sensitivity. arXiv preprint
arXiv:1810.02340 , 2018.
37.Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino,
Jiliang Tang, and Huan Liu. Feature selection: A data perspective. ACM
Computing Surveys (CSUR) , 50(6):94, 2018.

--- PAGE 24 ---
24 Zahra Atashgahi et al.
38.Yifeng Li, Chih-Yu Chen, and Wyeth W Wasserman. Deep feature selec-
tion: theory and application to identify enhancers and promoters. Journal of
Computational Biology , 23(5):322‚Äì336, 2016.
39.Andy Liaw, Matthew Wiener, et al. ClassiÔ¨Åcation and regression by random-
forest.R news, 2(3):18‚Äì22, 2002.
40.Huan Liu and Hiroshi Motoda. Feature extraction, construction and selection:
A data mining perspective , volume 453. Springer Science & Business Media,
1998.
41.Shiwei Liu, Tim van der Lee, Anil Yaman, Zahra Atashgahi, Davide Ferrar,
Ghada Sokar, Mykola Pechenizkiy, and Decebal C Mocanu. Topological insights
into sparse neural networks. In Proceedings of the European Conference on
Machine Learning and Principles and Practice of Knowledge Discovery in
Databases (ECML PKDD) 2020. , 2020.
42.Yang Lu, Yingying Fan, Jinchi Lv, and William StaÔ¨Äord Noble. Deeppink:
reproducible feature selection in deep neural networks. In Advances in Neural
Information Processing Systems , pages 8676‚Äì8686, 2018.
43.Jianyu Miao and Lingfeng Niu. A survey on feature selection. Procedia
Computer Science , 91:919‚Äì926, 2016.
44.Decebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine
Gibescu, and Antonio Liotta. A topological insight into restricted boltzmann
machines. Machine Learning , 104(2-3):243‚Äì270, 2016.
45. Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen,
Madeleine Gibescu, and Antonio Liotta. Scalable training of artiÔ¨Åcial neural
networks with adaptive sparse connectivity inspired by network science. Nature
communications , 9(1):2383, 2018.
46.Hesham Mostafa and Xin Wang. Parameter eÔ¨Écient training of deep convo-
lutional neural networks by dynamic sparse reparameterization. In Kamalika
Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th Interna-
tional Conference on Machine Learning , volume 97 of Proceedings of Machine
Learning Research , pages 4646‚Äì4655, Long Beach, California, USA, 09‚Äì15 Jun
2019. PMLR. URL http://proceedings.mlr.press/v97/mostafa19a.html .
47.Sameer A Nene, Shree K Nayar, Hiroshi Murase, et al. Columbia object image
library (coil-20). 1996.
48.Razieh Sheikhpour, Mehdi Agha Sarram, Sajjad Gharaghani, and Mohammad
Ali Zare Chahooki. A survey on semi-supervised feature selection methods.
Pattern Recognition , 64:141‚Äì158, 2017.
49.Dinesh Singh and Makoto Yamada. Fsnet: Feature selection network on high-
dimensional biological data. arXiv preprint arXiv:2001.08322 , 2020.
50.Avrum Spira, Jennifer E Beane, Vishal Shah, Katrina Steiling, Gang Liu, Frank
Schembri, Sean Gilman, Yves-Martine Dumas, Paul Calner, Paola Sebastiani,
et al. Airway epithelial gene expression in the diagnostic evaluation of smokers
with suspect lung cancer. Nature medicine , 13(3):361‚Äì366, 2007.
51.Lixin Sun, Ai-Min Hui, Qin Su, Alexander Vortmeyer, Yuri Kotliarov, Sandra
Pastorino, Antonino Passaniti, Jayant Menon, Jennifer Walling, Rolando Bailey,
et al. Neuronal and glioma-derived stem cell factor induces angiogenesis within
the brain. Cancer cell , 9(4):287‚Äì300, 2006.
52.Mingkui Tan, Ivor W Tsang, and Li Wang. Towards ultrahigh dimensional
feature selection for big data. Journal of Machine Learning Research , 2014.

--- PAGE 25 ---
Quick and Robust Feature Selection 25
53.LaurensVanDerMaaten,EricPostma,andJaapVandenHerik. Dimensionality
reduction: a comparative. J Mach Learn Res , 10(66-71):13, 2009.
54.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
Extracting and composing robust features with denoising autoencoders. In
Proceedings of the 25th international conference on Machine learning , pages
1096‚Äì1103. ACM, 2008.
55.Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis.
Chemometrics and intelligent laboratory systems , 2(1-3):37‚Äì52, 1987.
56.JunYang,WenjingXiao,ChunJiang,MShamimHossain,GhulamMuhammad,
and Syed Umar Amin. Ai-powered green cloud and data center. IEEE Access ,
7:4195‚Äì4203, 2018.
57.Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, and Xiaofang Zhou. L2,
1-norm regularized discriminative feature selection for unsupervised. In Twenty-
Second International Joint Conference on ArtiÔ¨Åcial Intelligence , 2011.
58.Zheng Zhao and Huan Liu. Semi-supervised feature selection via spectral
analysis. In Proceedings of the 2007 SIAM international conference on data
mining, pages 641‚Äì646. SIAM, 2007.
59.Hangyu Zhu and Yaochu Jin. Multi-objective evolutionary federated learning.
IEEE transactions on neural networks and learning systems , 2019.

--- PAGE 26 ---
26 Zahra Atashgahi et al.
Appendix
A Performance Evaluation
In this appendix, we compare all methods from diÔ¨Äerent aspects including accuracy, memory
usage, running time, energy consumption, and the number of parameters. We perform diÔ¨Äerent
experiments to gain a deep insight into the performance of QuickSelection.
A.1 Discussion: Accuracy and Computational EÔ¨Éciency Trade-oÔ¨Ä
In this section, we compare the performance of all methods in more detail. We run feature
selection for diÔ¨Äerent values of K on each dataset and then measure the performance.
As shown in Figure 10, we compare clustering accuracy, classiÔ¨Åcation accuracy, and running
time among the methods for diÔ¨Äerent values of K. The comparison of maximum memory
(RAM) requirement is also depicted in Figure 11. For all methods except CAE and AEFS, we
run the experiments on a single CPU core. Since the implementations of CAE and AEFS are
optimized for GPU, we measure the running time of these methods using a GPU. However,
we also consider the running time of CAE using a single CPU core. It should be noticed that
since Laplacian score, AEFS, FCAE, and QuickSelection give the ranking of the features as
the output of the feature selection process, we need to run them just once for all values of K.
However, MCFS and CAE need the Kvalue as an input of their algorithm. So, the running
time depends on the value of K. In the implementation of AEFS, Kis used to set the number
of hidden values. However, it is not the requirement of the algorithm.
We summarize the results of the aforementioned plots in Figure 12; we compare the methods
using the score 1, which is introduced in Section 5.1. This score is computed based on the
methods‚Äô ranking in clustering accuracy, classiÔ¨Åcation accuracy, running time, and memory. As
explained in Section 5.1, we give a score of one to each method that is the Ô¨Årst or second-best
performer in each of the considered metrics. Then, we compute a sum over all of these scores
on all datasets and on all values of K; the Ô¨Ånal scores for each method can be seen in Figure
12. The Ô¨Årst column depicts the results on low-dimensional datasets with a low number of
samples, including Coil20, Isolet, HAR, and Madelon. The second column shows the results
corresponding to MNIST. Similarly, the third column corresponds to high-dimensional datasets,
including SMK, GLA, and PCMAC. The total score over all of these datasets is shown in
the4thcolumn. In Figure 12, there exist four rows; the Ô¨Årst row corresponds to considering
QuickSelection 10and QuickSelection 100simultaneously, and the sum of their scores are depicted
in the second row. The last two rows correspond to considering each of these two methods
separately.
However, since the performance of each method can be diÔ¨Äerent in each of the three groups
of datasets, we compute a normalized version of the score 1, based on the number of datasets
in each group. For example, the Laplacian score has a poor performance on MNIST, and this
pattern would be similar on other datasets with a large number of samples. However, there is
just one dataset with a large number of samples in this experiment. On the other hand, on
high-dimensional datasets with a low number of samples, this method has a good performance
in terms of running time, and we have three datasets with such characteristics. So, we normalize
the values of score 1, such that instead of giving a score of one to each method, we give a
score of one divided by the number of datasets in the corresponding group. The results of the
normalized score 1are shown in the last column of Figure 12.
A.2 Energy Consumption
We perform another experiment regarding the comparison of energy consumption among all
methods. The results are presented in Figure 13. More details regarding this plot are given in
the paper in Section 5.1.

--- PAGE 27 ---
Quick and Robust Feature Selection 27
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002b/uni00000024/uni00000035
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000017/uni0000001b/uni00000018/uni00000013/uni00000018/uni00000015/uni00000018/uni00000017/uni00000018/uni00000019/uni00000018/uni0000001b/uni00000019/uni00000013/uni00000019/uni00000015/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002b/uni00000024/uni00000035
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000026/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni0000002c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni0000002b/uni00000024/uni00000035
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
(a) Low-dimensional datasets
/uni00000018
/uni00000014/uni00000013
/uni00000015/uni00000013
/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000016/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048/uni00000018
/uni00000014/uni00000013
/uni00000015/uni00000013
/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000016/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000014/uni00000013/uni00000018/uni00000014/uni00000013/uni00000019/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
(b) MNIST
/uni00000018/uni00000014/uni00000013
/uni00000015/uni00000013
/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000016/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000018/uni00000013/uni00000018/uni00000015/uni00000018/uni00000017/uni00000018/uni00000019/uni00000018/uni0000001b/uni00000019/uni00000013/uni00000019/uni00000015/uni00000019/uni00000017/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000036/uni00000030/uni0000002e
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013
/uni00000015/uni00000013
/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000016/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002a/uni0000002f/uni00000024
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013
/uni00000015/uni00000013
/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000016/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000018/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000018/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000018/uni00000018/uni00000015/uni00000011/uni00000013/uni00000018/uni00000015/uni00000011/uni00000018/uni00000018/uni00000016/uni00000011/uni00000013/uni00000018/uni00000016/uni00000011/uni00000018/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048/uni00000018/uni00000014/uni00000013
/uni00000015/uni00000013
/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000016/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000036/uni00000030/uni0000002e
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013
/uni00000015/uni00000013
/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000016/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002a/uni0000002f/uni00000024
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000018/uni00000014/uni00000013
/uni00000015/uni00000013
/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000014/uni00000018/uni00000013
/uni00000016/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni0000000b/uni0000002e/uni0000000c/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000014/uni00000013/uni00000018/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000036/uni00000030/uni0000002e
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000014/uni00000013/uni00000018/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni0000002a/uni0000002f/uni00000024
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017/uni00000014/uni00000013/uni00000018/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e (c) High-dimensional datasets
Fig. 10: Comparison of clustering accuracy, classiÔ¨Åcation accuracy, and running
time for various values of Kamong all the methods considered on eight datasets,
including low-dimensional and high-dimensional datasets. The running time of
CAE and AEFS is measured using a GPU, while all the other methods use only a
single CPU core.

--- PAGE 28 ---
28 Zahra Atashgahi et al.
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni0000002e/uni00000045/uni0000000c/uni00000014/uni00000048/uni00000019 /uni00000026/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni0000002e/uni00000045/uni0000000c/uni00000014/uni00000048/uni00000019 /uni0000002c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni0000002e/uni00000045/uni0000000c/uni00000014/uni00000048/uni00000019 /uni0000002b/uni00000024/uni00000035
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni0000002e/uni00000045/uni0000000c/uni00000014/uni00000048/uni00000019 /uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni0000002e/uni00000045/uni0000000c/uni00000014/uni00000048/uni0000001b /uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni0000002e/uni00000003/uni00000020/uni00000003/uni00000016/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni0000002e/uni00000045/uni0000000c/uni00000014/uni00000048/uni0000001a /uni00000036/uni00000030/uni0000002e
/uni0000002e/uni00000003/uni00000020/uni00000003/uni00000016/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni0000002e/uni00000045/uni0000000c/uni00000014/uni00000048/uni0000001a /uni0000002a/uni0000002f/uni00000024
/uni0000002e/uni00000003/uni00000020/uni00000003/uni00000016/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000030/uni00000044/uni0000005b/uni0000004c/uni00000050/uni00000058/uni00000050/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni00000055/uni00000048/uni00000054/uni00000058/uni0000004c/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni0000000b/uni0000002e/uni00000045/uni0000000c/uni00000014/uni00000048/uni0000001a /uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni0000002e/uni00000003/uni00000020/uni00000003/uni00000016/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
Fig. 11: Maximum memory usage during feature selection for diÔ¨Äerent values of K.
 Low-dimensional datasets  MNIST dataset  High-dimensional datasets  All datasets  All datasets (Average)  
QS 10, QS 100 
     
SUM  
     
 
QS 10 
     
QS 100 
     
 
Fig. 12: Feature selection results comparison in terms of classiÔ¨Åcation accuracy,
clustering accuracy, speed, and memory. The Scores are based on the ranking of
the methods on each dataset and for diÔ¨Äerent values of K(Score 1).
A.3 Number of Parameters
In Figure 14, we compare the number of parameters of the autoencoder-based methods. FCAE,
a fully connected-autoencoder with 1000 hidden neurons, has the highest number of parameters
on all datasets. Our proposed network, sparse DAE, has the lowest number of parameters
in most cases. It has 1000 hidden neurons that are sparsely connected to input and output
neurons. The number of parameters of AEFS and CAE depends on the number of selected
features. As also mentioned earlier, the structure of AEFS is similar to FCAE with a diÔ¨Äerence
in the number of hidden neurons. The number of hidden neurons in the implementation of
AEFS is set to K.

--- PAGE 29 ---
Quick and Robust Feature Selection 29
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004e/uni0000005a/uni0000004b/uni0000000c/uni00000026/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004e/uni0000005a/uni0000004b/uni0000000c/uni0000002c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004e/uni0000005a/uni0000004b/uni0000000c/uni0000002b/uni00000024/uni00000035
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004e/uni0000005a/uni0000004b/uni0000000c/uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004e/uni0000005a/uni0000004b/uni0000000c/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004e/uni0000005a/uni0000004b/uni0000000c/uni00000036/uni00000030/uni0000002e
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004e/uni0000005a/uni0000004b/uni0000000c/uni0000002a/uni0000002f/uni00000024
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000030/uni00000026/uni00000029/uni00000036
/uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000026/uni00000024/uni00000028/uni0000000b/uni00000026/uni00000033/uni00000038/uni0000000c
/uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c
/uni00000029/uni00000026/uni00000024/uni00000028
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000014/uni00000013/uni00000017
/uni00000014/uni00000013/uni00000016
/uni00000014/uni00000013/uni00000015
/uni00000014/uni00000013/uni00000014
/uni00000014/uni00000013/uni00000013/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni00000058/uni00000050/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004e/uni0000005a/uni0000004b/uni0000000c/uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
Fig. 13: Energy consumption of all methods for diÔ¨Äerent values of K.
/uni00000026/uni00000024/uni00000028 /uni00000024/uni00000028/uni00000029/uni00000036 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000018/uni00000013/uni00000014/uni00000011/uni0000001a/uni00000018/uni00000015/uni00000011/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000014/uni00000048/uni00000019 /uni00000026/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028 /uni00000024/uni00000028/uni00000029/uni00000036 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000014/uni00000048/uni00000019 /uni0000002c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028 /uni00000024/uni00000028/uni00000029/uni00000036 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000014/uni00000048/uni00000019 /uni0000002b/uni00000024/uni00000035
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028 /uni00000024/uni00000028/uni00000029/uni00000036 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000014/uni00000048/uni00000019 /uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028 /uni00000024/uni00000028/uni00000029/uni00000036 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000017/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000014/uni00000048/uni00000019 /uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni0000002e/uni00000003/uni00000020/uni00000003/uni00000016/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028 /uni00000024/uni00000028/uni00000029/uni00000036 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000014/uni00000048/uni0000001a /uni00000036/uni00000030/uni0000002e
/uni0000002e/uni00000003/uni00000020/uni00000003/uni00000016/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028 /uni00000024/uni00000028/uni00000029/uni00000036 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000014/uni00000048/uni0000001a /uni0000002a/uni0000002f/uni00000024
/uni0000002e/uni00000003/uni00000020/uni00000003/uni00000016/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
/uni00000026/uni00000024/uni00000028 /uni00000024/uni00000028/uni00000029/uni00000036 /uni00000029/uni00000026/uni00000024/uni00000028 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000014/uni00000048/uni00000019 /uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni0000002e/uni00000003/uni00000020/uni00000003/uni00000016/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000014/uni00000013/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000018/uni00000013
/uni0000004e/uni00000003/uni00000020/uni00000003/uni00000015/uni00000013
/uni00000029/uni00000052/uni00000055/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni0000004e
Fig. 14: Number of parameters of autoencoder-based models for diÔ¨Äerent values of
K.
B Parameter Selection
In this appendix, we discuss the eÔ¨Äect of three hyperparameters of QuickSelection on feature
selection performance.
B.1 Noise Factor
To analyze the eÔ¨Äect of the noise level on QuickSelection behavior, we evaluate the sparse DAE
model with diÔ¨Äerent noise factors. To this end, we test diÔ¨Äerent noise factors between 0and0:8.
The results can be observed in Figure 15. These results are an average of 5 runs for each case.
We can observe that adding 20%to40%noise on the data seems to be optimal; it improves
the performance on most of the datasets for QuickSelection 10and QuickSelection 100compared
to the model without any noise. We choose the noise factor of 0:2for all the experiments.
It is clear in Figure 15, that setting the noise factor to a large value may corrupt the
input data in such a way that the network would not be able to model the data distribution
accurately. For example, on the Isolet dataset, the clustering accuracy degrades for 10%when
we add 80%noise on the input data compared to the model with the noise factor of 0:2. Also,
the result is less stable when we add a large amount of noise. In this example, we can observe

--- PAGE 30 ---
30 Zahra Atashgahi et al.
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001b
/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000049/uni00000044/uni00000046/uni00000057/uni00000052/uni00000055/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000034/uni00000058/uni0000004c/uni00000046/uni0000004e/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000014/uni00000013
/uni00000046/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni0000004c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni0000002b/uni00000024/uni00000035
/uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni00000036/uni00000030/uni0000002e
/uni0000002a/uni0000002f/uni00000024
/uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001b
/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000049/uni00000044/uni00000046/uni00000057/uni00000052/uni00000055/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000034/uni00000058/uni0000004c/uni00000046/uni0000004e/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000046/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni0000004c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni0000002b/uni00000024/uni00000035
/uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni00000036/uni00000030/uni0000002e
/uni0000002a/uni0000002f/uni00000024
/uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001b
/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000049/uni00000044/uni00000046/uni00000057/uni00000052/uni00000055/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni0000001c/uni00000018/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000034/uni00000058/uni0000004c/uni00000046/uni0000004e/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000014/uni00000013
/uni00000046/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni0000004c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni0000002b/uni00000024/uni00000035
/uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni00000036/uni00000030/uni0000002e
/uni0000002a/uni0000002f/uni00000024
/uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001b
/uni00000031/uni00000052/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000049/uni00000044/uni00000046/uni00000057/uni00000052/uni00000055/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni0000001c/uni00000018/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000034/uni00000058/uni0000004c/uni00000046/uni0000004e/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000046/uni00000052/uni0000004c/uni0000004f/uni00000015/uni00000013
/uni0000004c/uni00000056/uni00000052/uni0000004f/uni00000048/uni00000057
/uni0000002b/uni00000024/uni00000035
/uni00000030/uni00000044/uni00000047/uni00000048/uni0000004f/uni00000052/uni00000051
/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037
/uni00000036/uni00000030/uni0000002e
/uni0000002a/uni0000002f/uni00000024
/uni00000033/uni00000026/uni00000030/uni00000024/uni00000026
Fig. 15: Clustering and classiÔ¨Åcation accuracy for feature selection using
QuickSelection 10and QuickSelection 100with diÔ¨Äerent values of noise factor. We
select 50 features from all datasets except Madelon for which we select 20 features.
that adding 20%noise to the original data improves both classiÔ¨Åcation and clustering accuracy
of QuickSelection 100by approximately 3%.
From this Ô¨Ågure, it can be observed that the improvement of adding noise, is more obvious
in QuickSelection 100than QuickSelection 10. When we add noise to the data, it needs more
time to learn the original structure of the data. So, we need to run it for more epochs to get a
proper result.
B.2 SET Hyperparameters
As explained in the paper, andare the hyperparameters of the SET algorithm which control
the number of connections to remove/add for each topology change and the sparsity level,
respectively. The corresponding density level of each value for each dataset can be observed
in Table 4.
To illustrate the eÔ¨Äect of the hyperparameters and, we perform a grid search within a
small set of values on all of the datasets. The obtained results can be found in Tables 5 and 6.
As we increase the value, the number of connections in our model increases, and therefore, the
computation time will increase. So, we prefer using small values for this parameter. Additionally,
Table 4:values and their corresponding density level.
Density [%]
COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
2 0.39 0.52 0.39 0.59 0.45 0.20 0.2 0.26
5 0.98 1.30 0.98 1.48 1.13 0.53 0.51 0.65
10 1.95 2.58 1.95 2.95 2.25 1.04 1.02 1.13
13 2.53 3.35 2.53 3.82 2.91 1.35 1.32 1.69
20 3.87 5.10 3.87 5.82 4.45 2.07 2.04 2.6
25 4.87 6.45 4.87 7.37 5.63 2.65 2.55 3.26

--- PAGE 31 ---
Quick and Robust Feature Selection 31
for a large value of , in some cases the model is not able to converge in 100 epochs; for
example, on the MNIST dataset, we can observe that for an value of 25, the model has lower
performance in terms of clustering and classiÔ¨Åcation accuracy.
It can be observed that = 0:2and= 13(as chosen for the experiments performed in
the paper) lead to a decent performance on all datasets. For these values, QuickSelection is
able to achieve high clustering and classiÔ¨Åcation accuracy.
Overall, although searching for the best pair of andwill improve the performance,
QuickSelection is not extremely sensitive to these values. As can be seen in Tables 5 and 6, for
all values of these hyperparameters QuickSelection has a reasonable performance. Even with
= 2which leads to a very sparse model, QuickSelection has decent performance, and in some
cases better than a denser network.
C Visualization of Selected Features on MNIST
In Figure 16, we visualize the 50 best features found by QuickSelection on the MNIST dataset
at diÔ¨Äerent epochs. These features are mostly at the center of the image, similar to the pattern
of MNIST digits.
Then, we visualize the features selected for each class separately. In Figure 17, each picture
at diÔ¨Äerent epochs is the average of the 50 selected features of all the samples of each class along
with the average of the actual samples of the corresponding class. As we can see, during training,
these features become more similar to the pattern of digits of each class. Thus, QuickSelection
is able to Ô¨Ånd the most relevant features for all classes.
Epoch 1  Epoch 10  Epoch 100  
   
 
Fig. 16: 50 most informative features of MNIST dataset selected by QuickSelection
after 1, 10, and 100 epochs of training.
 Epoch 1  Epoch 10  Epoch 100  Class  Epoch 1  Epoch 10  Epoch 100  Class  
        
        
        
        
        
Fig. 17: Average of the data samples of each MNIST class corresponding to the 50
selected features after 1, 10, and 100 epochs of training along with the average of
the actual samples of each class.

--- PAGE 32 ---
32 Zahra Atashgahi et al.
Table 5: Hyper-parameter selection for QuickSelection 10. Each entry of each ta-
ble contains clustering accuracy and classiÔ¨Åcation accuracy in percentages (%),
respectively.
(a) Coil20

2 5 10 13 20 25
0.161.42.3, 98.6 1.5 59.7 1.8, 96.9 1.6 61.7 2.8, 98.3 0.8 60.3 2.0, 98.6 1.7 61.9 0.9, 99.5 0.3 60.8 2.4, 99.7 0.2
0.259.43.8, 97.9 1.4 59.3 0.9, 98.5 1.2 60.1 2.6, 98.9 0.6 59.5 2.1, 98.8 0.6 63.6 2.5, 99.7 0.3 61.0 3.0, 99.7 0.4
0.361.32.1, 98.7 1.6 59.5 1.9, 96.7 0.6 60.3 0.9, 99.1 0.6 60.1 1.9, 98.2 1.0 59.9 2.6, 98.7 0.9 60.8 1.9, 99.4 0.4
0.460.53.2, 97.5 1.3 59.0 2.4, 96.8 1.8 57.2 1.5, 97.7 1.7 62.0 2.5, 99.2 0.3 62.1 2.5, 99.7 0.3 63.8 1.5, 99.4 0.6
0.561.22.3, 98.0 0.9 58.4 2.6, 97.8 1.2 58.9 2.6, 97.3 1.3 60.6 1.6, 98.2 1.6 59.1 2.9, 99.0 0.6 62.8 1.6, 99.0 1.0
(b) Isolet

2 5 10 13 20 25
0.128.62.1, 82.5 3.7 31.7 2.4, 84.9 2.4 31.2 2.2, 87.4 1.7 29.4 1.5, 88.4 1.3 32.4 1.8, 86.9 1.5 31.7 1.0, 86.9 0.7
0.226.12.2, 81.5 1.8 30.2 2.4, 86.2 2.8 30.9 2.3, 88.2 0.6 32.5 2.8, 86.9 1.1 32.4 1.6, 87.8 1.6 33.4 2.1, 87.5 1.0
0.327.92.0, 83.0 3.6 30.6 3.3, 87.2 2.1 31.9 2.5, 87.3 1.1 32.9 1.1, 87.7 1.1 32.2 1.8, 87.3 0.9 32.3 2.4, 88.1 1.7
0.426.92.0, 82.5 2.5 30.0 0.8, 86.4 3.3 31.5 2.3, 86.2 2.6 29.0 1.4, 85.4 1.6 34.5 2.9, 86.5 2.3 31.9 3.4, 87.1 1.1
0.526.91.9, 81.8 1.7 30.7 1.9, 84.9 3.1 30.7 2.3, 86.7 2.3 31.2 3.5, 86.9 1.7 33.1 1.5, 87.4 0.8 33.8 2.1, 87.1 0.6
(c) HAR

2 5 10 13 20 25
0.143.02.7, 83.2 1.6 56.8 1.0, 88.6 1.5 55.8 3.8, 88.4 0.6 56.0 2.6, 87.7 0.8 56.6 4.9, 90.1 1.9 56.4 4.7, 88.4 1.3
0.243.82.1, 82.7 2.6 56.5 0.9, 89.2 2.2 58.8 1.1, 88.4 0.5 56.0 2.6, 88.8 0.7 54.5 4.1, 88.9 1.3 55.0 2.8, 89.7 0.8
0.343.02.1, 84.5 2.2 55.7 1.7, 89.6 1.4 59.2 1.0, 88.4 1.1 54.7 5.2, 88.4 1.5 56.3 2.4, 89.1 1.8 57.7 2.1, 89.3 1.7
0.442.43.7, 82.9 1.4 55.5 1.4, 89.8 1.5 56.9 1.8, 88.7 0.6 59.1 0.7, 89.4 0.6 56.7 3.2, 89.7 1.2 56.9 2.2, 89.1 1.8
0.545.97.0, 84.0 3.1 52.2 5.2, 89.9 0.7 57.9 1.9, 89.2 0.6 59.5 4.3, 89.7 1.1 55.3 2.7, 89.5 0.6 52.6 2.6, 89.9 0.8
(d) Madelon

2 5 10 13 20 25
0.153.51.3, 55.3 1.9 54.0 4.0, 62.1 7.9 56.7 3.8, 78.5 7.0 56.6 2.5, 86.2 2.1 58.1 2.5, 87.6 1.9 56.5 3.9, 89.4 1.1
0.252.92.8, 61.4 5.6 57.0 3.2, 68.6 4.2 57.6 2.9, 83.6 3.5 57.5 3.8, 86.6 3.6 55.5 3.7, 88.3 0.7 59.1 0.9, 86.0 1.5
0.353.42.5, 58.7 10.3 56.4 3.5, 67.1 9.1 53.9 3.1, 81.4 5.6 54.4 3.3, 86.4 2.0 58.2 2.7, 88.3 1.8 55.5 2.6, 88.5 0.8
0.453.84.1, 55.3 6.3 55.9 4.3, 62.6 6.4 54.4 2.4, 80.1 3.6 53.6 2.4, 84.9 3.2 56.8 3.0, 89.7 0.8 58.1 1.8, 86.4 2.8
0.555.23.8, 56.0 3.9 52.2 2.3, 61.3 9.0 56.0 2.4, 81.7 3.9 57.9 2.8, 84.2 4.6 57.8 2.3, 86.9 0.6 57.2 2.5, 89.0 2.1
(e) MNIST

2 5 10 13 20 25
0.142.63.5, 91.4 0.5 41.2 1.8, 93.2 0.4 46.8 4.4, 94.3 0.2 46.9 1.3, 93.9 0.3 43.3 1.7, 93.5 0.4 39.5 1.7, 92.4 1.2
0.243.82.3, 92.5 0.6 43.9 1.4, 93.4 0.6 43.0 3.2, 93.6 0.4 45.4 3.9, 93.8 0.6 38.5 2.9, 92.7 0.6 37.6 3.4, 91.2 1.1
0.342.11.8, 91.9 0.5 45.4 2.4, 94.2 0.4 47.4 1.2, 94.0 0.1 45.5 3.3, 94.0 0.4 39.7 1.9, 92.8 0.7 33.9 3.3, 88.3 2.1
0.441.92.0, 92.9 0.7 46.9 2.6, 93.7 0.4 46.2 0.9, 94.1 0.3 43.4 1.5, 93.7 0.3 37.0 3.6, 90.9 1.5 27.9 2.2, 81.7 3.1
0.543.33.6, 92.3 0.5 45.9 4.8, 93.8 0.6 45.2 2.9, 93.8 0.4 42.8 2.7, 93.8 0.7 39.7 2.8, 91.3 0.6 28.0 2.5, 77.7 6.1
(f) SMK

2 5 10 13 20 25
0.152.41.3, 72.1 7.0 53.8 3.0, 79.5 2.0 52.7 1.6, 76.8 4.5 56.0 1.7, 73.7 4.1 55.0 2.3, 74.2 7.3 53.7 2.4, 76.3 4.4
0.254.11.7, 73.7 4.7 53.5 2.7, 74.2 8.4 55.3 2.6, 75.3 4.9 54.0 3.1, 76.9 4.6 52.9 1.2, 81.6 5.0 54.5 3.0, 76.8 6.3
0.356.92.7, 76.8 6.1 54.7 1.3, 75.3 5.2 53.9 2.4, 74.7 4.9 53.9 2.3, 74.2 4.5 54.5 0.7, 76.3 3.7 54.8 2.9, 75.8 3.9
0.455.43.7, 74.7 2.1 55.5 1.7, 74.2 2.6 53.1 1.8, 72.6 4.9 52.8 1.6, 74.7 4.6 53.4 2.9, 72.6 4.3 53.1 2.5, 72.6 4.3
0.553.31.3, 77.4 5.4 55.2 3.0, 76.3 3.7 53.6 2.5, 76.3 6.0 52.5 1.5, 78.9 4.4 52.3 1.0, 77.4 7.7 51.9 1.5, 77.9 2.7
(g) GLA

2 5 10 13 20 25
0.154.12.8, 66.7 5.0 54.7 3.5, 67.2 5.9 55.0 4.6, 66.7 1.8 54.5 1.5, 67.8 5.7 56.6 4.0, 75.0 6.3 55.9 3.2, 68.9 5.4
0.250.23.5, 67.8 3.8 53.4 3.3, 67.2 6.2 56.6 2.5, 70.0 4.4 53.6 4.7, 69.4 3.0 56.7 2.2, 68.3 2.8 52.6 1.5, 68.9 1.1
0.356.23.5, 68.9 4.8 53.3 4.8, 68.3 3.8 54.4 2.4, 67.8 2.8 57.8 4.3, 70.0 3.2 56.1 1.9, 70.6 3.8 56.0 3.0, 71.1 4.5
0.455.63.5, 68.9 2.1 54.2 1.5, 68.3 4.5 57.5 3.1, 68.3 2.2 56.9 1.1, 70.6 2.8 55.7 3.6, 68.3 4.5 55.4 2.4, 68.9 6.9
0.554.92.6, 68.9 4.1 54.0 2.5, 66.1 3.7 54.8 2.4, 71.1 4.5 54.5 5.1, 67.2 6.4 56.5 5.6, 71.1 1.4 55.8 2.0, 65.6 3.8
(h) PCMAC

2 5 10 13 20 25
0.151.00.5, 61.1 4.2 51.0 0.6, 57.0 2.0 51.1 1.1, 59.3 3.4 51.4 0.5, 56.6 3.0 50.9 0.2, 55.5 3.5 51.3 0.5, 59.4 2.2
0.250.50.4, 61.3 6.1 50.8 0.5, 57.0 3.5 50.7 0.4, 55.8 2.1 50.9 0.5, 58.9 4.4 51.0 0.2, 59.2 4.0 51.0 0.6, 57.8 2.1
0.351.31.0, 58.7 2.9 50.9 0.3, 57.4 1.1 51.0 0.4, 57.0 2.0 51.2 0.5, 59.2 3.2 50.7 0.3, 58.2 1.9 51.1 0.6, 58.3 1.9
0.450.70.3, 58.1 2.4 51.3 0.4, 55.7 2.8 50.9 0.5, 55.2 1.0 51.1 0.3, 58.1 2.5 51.1 0.2, 57.9 3.7 51.6 0.9, 55.4 2.2
0.551.10.5, 57.4 2.4 51.1 0.4, 57.0 1.6 51.2 0.9, 58.1 3.0 51.0 0.6, 56.4 1.4 50.9 0.3, 55.8 1.9 51.6 0.9, 58.0 2.4

--- PAGE 33 ---
Quick and Robust Feature Selection 33
Table 6: Hyper-parameter selection for QuickSelection 100. Each entry of each
table contains clustering accuracy and classiÔ¨Åcation accuracy in percentages (%),
respectively.
(a) Coil20

2 5 10 13 20 25
0.163.20.7, 99.7 0.2 62.8 1.1, 99.4 0.7 60.2 3.5, 99.2 0.4 61.8 1.5, 99.7 0.5 56.0 2.3, 98.8 1.0 53.4 1.7, 98.8 0.5
0.261.30.9, 99.1 0.7 62.1 3.2, 99.7 0.1 61.7 2.3, 99.6 0.4 60.2 2.0, 99.7 0.3 56.9 1.8, 99.1 0.6 53.9 1.5, 98.9 0.7
0.362.11.5, 98.5 0.8 62.0 2.6, 99.4 0.7 60.0 1.8, 99.5 0.2 60.2 2.5, 99.3 0.2 55.0 1.6, 98.8 0.9 53.8 1.7, 98.3 0.8
0.458.91.3, 98.3 0.6 62.9 1.0, 99.7 0.3 62.0 3.0, 99.5 0.5 62.3 1.4, 99.7 0.4 57.8 2.5, 99.2 0.2 57.2 2.2, 99.0 0.7
0.558.11.9, 97.1 1.7 59.9 1.5, 99.4 0.4 63.2 2.6, 99.0 0.8 64.2 1.3, 99.6 0.3 59.2 2.9, 98.8 1.1 58.0 1.4, 99.1 1.0
(b) Isolet

2 5 10 13 20 25
0.129.42.2, 87.1 1.1 29.7 1.5, 84.8 3.2 28.3 2.7, 83.4 4.2 33.2 3.0, 89.3 1.8 37.7 1.9, 87.5 1.8 36.2 2.4, 88.3 1.2
0.229.42.2, 85.9 2.1 29.6 2.7, 86.0 1.8 31.5 2.0, 85.5 3.7 35.1 2.7, 89.0 1.3 35.5 2.5, 87.5 2.2 38.9 1.7, 87.5 0.4
0.330.32.2, 85.7 3.1 30.2 1.8, 84.2 3.8 30.0 2.6, 84.5 1.8 33.5 2.3, 87.6 1.8 35.7 3.0, 87.1 2.5 38.1 1.7, 87.4 1.7
0.431.13.3, 85.9 3.8 29.5 2.5, 86.1 3.2 30.4 3.4, 83.7 3.5 29.6 1.3, 85.4 0.8 33.1 3.6, 87.6 2.7 35.4 1.5, 87.9 1.2
0.530.42.5, 88.0 2.1 29.5 1.8, 86.2 2.7 31.5 2.7, 86.4 1.9 31.4 2.1, 86.2 1.9 33.3 2.0, 86.4 2.1 35.7 2.0, 89.5 0.6
(c) HAR

2 5 10 13 20 25
0.152.75.2, 88.4 2.3 57.0 1.4, 89.5 1.2 56.1 1.9, 88.8 1.2 54.2 4.1, 88.5 2.1 56.0 2.0, 89.2 2.5 55.2 3.5, 87.5 2.1
0.248.94.0, 85.8 1.7 57.4 0.4, 90.0 0.6 52.1 4.5, 88.6 2.7 54.6 4.5, 90.2 1.2 54.2 1.8, 89.4 0.7 53.9 3.0, 89.2 1.9
0.350.96.7, 88.5 2.8 56.3 6.4, 89.5 1.2 54.2 3.6, 90.8 1.5 53.6 4.4, 90.5 3.2 52.0 6.3, 88.0 1.4 51.1 3.1, 89.4 1.1
0.448.96.1, 88.7 2.6 55.7 6.6, 90.5 1.4 54.1 4.1, 91.1 0.3 50.8 4.1, 89.2 1.3 49.7 3.2, 90.2 1.1 54.1 4.4, 89.0 1.9
0.546.05.7, 87.3 3.2 55.5 8.1, 90.5 1.6 52.8 5.3, 90.2 1.3 55.4 0.7, 90.4 1.0 50.4 4.3, 89.4 0.4 49.6 5.3, 88.3 1.1
(d) Madelon

2 5 10 13 20 25
0.153.73.3, 75.1 6.8 58.5 2.8, 86.1 3.5 57.1 2.2, 90.3 1.0 56.7 2.2, 89.9 1.2 58.4 0.5, 90.3 0.8 58.3 0.5, 89.8 1.1
0.254.32.4, 81.2 4.7 55.6 2.5, 88.2 2.1 57.1 2.6, 89.6 0.9 58.2 1.5, 90.3 0.7 58.1 0.1, 90.3 1.3 58.1 0.0, 90.8 0.5
0.353.12.6, 82.0 4.5 60.1 0.8, 87.5 1.3 57.6 2.0, 89.6 1.2 57.6 1.5, 89.4 1.3 58.1 0.0, 90.9 0.4 58.3 0.5, 89.5 1.5
0.455.02.8, 78.6 8.3 58.4 2.9, 87.0 4.6 55.8 2.2, 90.6 0.6 58.4 0.7, 90.1 0.9 57.4 1.6, 90.3 0.7 58.1 0.0, 90.9 1.2
0.555.63.2, 74.3 3.3 57.1 3.2, 87.1 2.4 57.1 3.5, 90.0 0.6 58.9 0.4, 90.3 0.3 58.5 0.6, 89.3 0.7 58.5 0.4, 89.4 1.3
(e) MNIST

2 5 10 13 20 25
0.144.12.2, 92.8 1.0 46.3 3.4, 94.0 0.4 48.3 1.7, 94.0 0.6 44.3 2.7, 93.8 0.5 43.5 3.4, 92.8 0.5 31.3 4.3, 85.2 4.9
0.243.35.1, 93.5 1.2 44.3 1.4, 93.7 0.3 47.1 3.1, 93.7 0.6 48.3 2.4, 93.5 0.5 37.7 1.3, 91.3 0.4 33.7 3.5, 87.8 1.5
0.345.12.8, 93.2 0.3 48.4 4.5, 93.7 0.8 46.0 4.6, 93.4 0.3 44.9 4.4, 93.8 0.4 35.8 3.0, 91.6 0.9 38.1 1.3, 90.8 0.6
0.445.12.4, 93.4 0.3 45.4 2.3, 94.2 0.3 45.0 2.1, 93.4 0.5 40.1 4.0, 92.4 0.7 41.5 4.9, 91.8 1.3 32.5 2.7, 87.7 3.7
0.545.72.6, 93.8 0.7 44.1 2.6, 93.7 0.6 43.8 2.6, 93.8 0.5 43.2 1.8, 92.6 0.7 36.6 4.5, 91.5 1.0 36.0 2.7, 88.5 1.4
(f) SMK

2 5 10 13 20 25
0.153.11.3, 72.6 5.9 52.6 1.6, 76.3 2.4 51.6 0.9, 76.8 4.8 53.5 1.6, 75.8 2.6 54.6 3.2, 72.6 2.1 51.4 0.9, 76.8 5.4
0.253.32.3, 74.2 5.4 53.0 1.5, 76.8 3.1 51.1 0.6, 78.4 5.9 51.8 0.8, 75.7 3.9 51.3 0.6, 78.4 3.5 51.9 1.6, 78.4 6.5
0.353.31.7, 74.2 4.5 50.7 0.3, 76.3 6.5 51.6 0.9, 76.8 6.1 50.9 0.5, 74.7 3.6 51.2 0.6, 77.4 2.7 51.4 0.7, 77.4 5.4
0.452.42.6, 78.4 4.5 51.6 0.7, 78.4 2.6 50.9 0.4, 75.8 5.4 51.9 0.8, 75.3 7.2 51.1 0.5, 76.8 4.5 50.8 0.3, 76.3 2.4
0.553.01.3, 76.8 6.1 52.1 1.1, 74.7 2.7 51.9 0.8, 74.2 3.5 50.7 0.4, 75.8 3.9 50.3 0.0, 78.9 4.1 51.1 0.5, 81.0 4.2
(g) GLA

2 5 10 13 20 25
0.157.62.6, 68.9 5.9 57.1 1.9, 67.2 1.1 57.4 2.8, 72.2 3.9 57.7 2.9, 68.9 3.2 57.4 2.9, 73.3 4.8 59.2 2.7, 71.7 4.1
0.257.03.4, 64.4 3.2 60.8 3.8, 71.1 3.3 58.7 3.5, 67.8 6.0 59.5 1.8, 73.3 3.3 58.6 2.0, 72.8 2.1 55.6 1.3, 70.6 7.2
0.357.73.5, 73.9 3.3 58.3 4.1, 67.2 3.2 54.8 0.9, 72.2 3.5 58.0 4.3, 67.8 3.8 56.4 3.5, 68.3 4.2 57.3 2.8, 66.7 2.5
0.456.12.6, 71.1 3.3 57.9 2.9, 67.2 4.8 54.4 2.5, 67.2 3.2 59.0 4.0, 69.4 4.6 56.9 2.3, 69.4 2.5 59.9 3.6, 69.4 4.6
0.555.22.2, 67.2 6.4 56.0 1.7, 63.9 1.8 58.0 2.2, 68.3 6.0 59.0 3.1, 70.0 5.4 59.5 3.2, 71.1 6.2 53.6 1.7, 68.3 4.2
(h) PCMAC

2 5 10 13 20 25
0.150.60.3, 58.1 3.8 50.8 0.4, 57.4 3.1 51.4 1.2, 58.5 2.3 51.0 0.4, 59.2 3.2 50.8 0.2, 59.2 3.1 52.6 1.0, 58.8 3.4
0.250.70.4, 59.4 2.9 50.7 0.5, 60.6 3.4 52.1 1.7, 57.2 3.4 52.5 1.1, 58.0 2.9 53.1 0.0, 58.6 2.6 53.1 0.0, 60.1 2.0
0.351.50.9, 57.2 2.9 51.4 0.9, 56.0 2.2 51.7 1.2, 58.1 0.9 52.2 1.1, 56.5 1.7 53.1 0.0, 59.5 2.4 53.1 0.0, 57.3 4.1
0.450.90.4, 59.8 6.7 51.3 0.9, 56.3 4.1 52.0 1.3, 57.3 3.0 53.1 0.0, 56.7 2.2 53.1 0.0, 56.6 2.0 53.1 0.0, 57.6 2.0
0.550.70.2, 56.9 0.5 51.3 0.9, 57.1 2.1 52.6 0.9, 59.6 1.9 53.1 0.0, 57.7 1.8 53.1 0.0, 56.8 3.4 53.1 0.0, 59.8 1.6

--- PAGE 34 ---
34 Zahra Atashgahi et al.
D Feature Extraction
Although it is not the main focus of the paper, we perform a small analysis on the MNIST
dataset to study the performance of sparse DAE as a feature extractor. We train it to map the
high-dimensional features into a lower-dimensional space.
The structure we consider for feature extraction has three hidden layers with 1000, 50,
and 1000 neurons, respectively; the middle layer (50 neurons) is the extracted low-dimensional
representation. We compare the results with fully-connected DAE (FC-DAE - implemented
in Keras [ 13]). We also extract features using the Principal Component Analysis (PCA) [ 55]
technique as a baseline method. Then, we train an ExtraTrees classiÔ¨Åer on these extracted
features and compute the classiÔ¨Åcation accuracy. The results are presented in Figure 18.
To achieve the best density level that suits our network, we test diÔ¨Äerent values. As
shown in Figure 18, sparse DAE (density = 3.26%) has the best performance among them.
Sparse DAE (density = 3.26%), FC-DAE, and PCA achieve 95.2%, 96.2%, and 95.6% accuracy,
respectively. Although sparse DAE can not perform as well as the FC-DAE, it approximately
has 54 k parameters compared to 1.67 m parameters of FC-DAE. Such a small number of
parameters of this model results in a high rise in the running speed and a signiÔ¨Åcant drop in
the memory requirement. Furthermore, it is interesting to observe that a very sparse DAE
(below 1% density) can achieve more than 90.0% accuracy on MNIST while having about 150
times fewer parameters than FC-DAE.
1 11 21 31 41 51 61 71 81 91 101
# of epochs80.082.585.087.590.092.595.0Accuracy (%)
sparse DAE (density = 0.68%)
sparse DAE (density = 1.66%)
sparse DAE (density = 3.26%)
sparse DAE (density = 6.26%)
sparse DAE (density = 9.04%)
sparse DAE (density = 11.62%)
PCA
FC-DAE
Fig. 18: ClassiÔ¨Åcation accuracy for feature extraction using sparse DAE with
diÔ¨Äerent density level on the MNIST dataset (number of extracted features = 50)
compared with FC-DAE and PCA.
E Feature Selection on a Large Dataset
In this appendix, we evaluate the performance of the methods on a very large dataset, in terms
of both number of samples and dimensions.
In this experiment, Ô¨Årst, we generate two artiÔ¨Åcial datasets with high number of samples and
features. The choice of an artiÔ¨Åcial dataset was made to easily control the number of relevant
features of the dataset, as in most of the real-world datasets the number of informative features
are not clear. These datasets are generated using sklearn5library tools, make_classiÔ¨Åcation
function, which generates datasets with a desired number of features and samples. This function
allows us to adjust the number of informative, redundant, and non-informative features. Table
7 shows the characteristics of the two artiÔ¨Åcially generated datasets. We generated 2datasets
5https://scikit-learn.org/

--- PAGE 35 ---
Quick and Robust Feature Selection 35
Table 7: Characteristics of the two artiÔ¨Åcially generated datasets. The classiÔ¨Åcation
and clustering accuracy have been obtained using all the features.
Dataset Samples FeaturesInformative
FeaturesRedundant
Features ClassesClassiÔ¨Åcation
Accuracy ( %)Clustering
Accuracy ( %)
ArtiÔ¨Åcial1 40000 8000 500 3000 5 59.8 30.6
ArtiÔ¨Åcial2 40000 8000 1000 0 5 26.6 22.7
Table 8: Feature selection results on two artiÔ¨Åcially generated datasets ( K= 1000)
ArtiÔ¨Åcial1 Dataset ArtiÔ¨Åcial2 Dataset Number of Parameters
MethodClassiÔ¨Åcation
Accuracy ( %)Clustering
Accuracy ( %)ClassiÔ¨Åcation
Accuracy ( %)Clustering
Accuracy ( %)
Lap_score 49.4 24.4 22.0 21.3 -
MCFS 68.2 29.3 24.6 21.7 -
CAE 23.4 20.6 21.1 20.4 26106
AEFS 34.7 23.3 22.8 21.4 32106
FCAE 43.8 24.3 22.9 21.5 32106
QS10 68.1 25.7 24.8 21.21 0.8106
QS100 68.4 24.8 34.5 24.6 0.8106
QS200 - - 39.7 29.6 0.8106
with 40000samples and 8000features. However, the number of informative and redundant
features are diÔ¨Äerent in these datasets. ArtiÔ¨Åcial 2dataset is much noisier than ArtiÔ¨Åcial 1;
therefore, Ô¨Ånding relevant features of ArtiÔ¨Åcial 2is more diÔ¨Écult compared to Ô¨Ånding them on
the ArtiÔ¨Åcial 1dataset.
After generating the datasets, we evaluate feature selection performance of the methods
consideredinthemanuscript,andcomparetheresultswithQuickSelection.Thehyperparameters
used in this experiment are similar to the ones used in Section 4.1.1, except for hidden neurons
and the sparsity level. The number of hidden neurons for autoencoder-based methods has been
set to 2000, and the hyperparameter of QuickSelection, , has been adjusted to 40. The number
of selected features ( K) is1000. The number of training epochs for the autoencoder-based
methods is 100. However, since the QuickSelection did not converge in 100epochs on the
ArtiÔ¨Åcial 2dataset, we continued the training until epoch 200. The results of this experiment
are presented in Table 8.
As can be seen in Table 8, QuickSelection 100outperforms all the other methods in terms
of classiÔ¨Åcation accuracy on both datasets. It can also outperform the other methods in terms
of clustering accuracy on the ArtiÔ¨Åcial 2dataset. As mentioned earlier, QuickSelection achieves
a higher accuracy on the ArtiÔ¨Åcial 2dataset when it is trained for more than 100epochs.
However, since for all the other methods we use 100epochs, we only consider the results of
QuickSelection 100to have a fair comparison (it should be noted that increasing the number of
training epochs did not improve the results of the other methods). On noisy and very large
datasets, CAE, AEFS, and FCAE have a poor performance in feature selection. In addition,
they have around 30times more parameters than QuickSelection. CAE has the lowest accuracy
among these methods; this method is very sensitive to noise. Lap_score and MCFS have a
poor performance on the ArtiÔ¨Åcial 2dataset that is noisier than ArtiÔ¨Åcial 1. On the ArtiÔ¨Åcial 1
dataset, MCFS achieves the highest clustering accuracy. However, the memory requirement of
MCFS and Lap_score is noticeably large. On this dataset, they need about 26GBof RAM.
However, QuickSelection needs only about 8GBmemory. In summary, QuickSelection 100has a
decent performance on these large datasets, while having the lowest number of parameters.

--- PAGE 36 ---
36 Zahra Atashgahi et al.
F Sparse Training Algorithm Analysis
In this appendix, we aim to analyze the eÔ¨Äect of the SET training procedure on the performance
of QuickSelection.
We perform QuickSelection using another algorithm to obtain and train the sparse network,
and then, compare the result with the original QuickSelection. We derive the sparse denoising
autoencoder using the lottery ticket hypothesis algorithm [ 20], as follows. The lottery ticket
hypothesis (LTH), Ô¨Årst, starts with training a dense network. After that, it derives the topology
of the sparse network by pruning the unimportant weights of the trained dense network.
Then, using both the sparse topology and the initial weight values of the connections in the
dense training phase, the network is retrained. On the Ô¨Ånal obtained sparse model, we apply
QuickSelection principles to select the most informative features.
In this experiment, the structure, sparsity level, and other hyperparameters are similar
to the settings described in Section 4.1.1; we use a simple autoencoder with one hidden
layer containing 1000hidden neurons, trained for 100epochs. The results of feature selection
(K= 50) are available in Tables 9 and 10. We refer to the feature selection performed using
QuickSelection principles and the Sparse DAE obtained with LTH as QSLTH
100. We use QS 100
for the QuickSelection that is done using the Sparse DAE obtained with SET.
As can be observed in Tables 9 and 10, in most of the cases QS 100outperforms QSLTH
100. We
believe that optimizing the sparse topology and the weights, simultaneously, results in feature
strength that are more meaningful for the feature selection. We discussed neuron strength in
more detail in Section 5.3. In addition, due to having an extra phase of dense training, the
computational resource requirements of LTH are much higher than the ones of SET. To clarify
this aspect, we present a comparison for the number of parameters between these two methods.
The results can be found in Table 11. The much higher number of parameters in QSLTH
100in
comparison with the number of parameters in QS 100is given by the dense training phase of
LTH.
Table 9: Clustering accuracy (%) using 50selected features (except Madelon for
which we select 20features).
Method Coil20 Isolet HAR Madelon MNIST SMK GLA PCMAC
QS10060.2¬±2.0 35.1 ¬±2.7 54.6 ¬±4.5 58.2 ¬±1.5 48.3 ¬±2.451.8¬±0.859.5¬±1.8 52.5 ¬±1.1
QSLTH
10058.8¬±3.3 31.2 ¬±2.4 50.2 ¬±6.3 50.8 ¬±0.5 37.5 ¬±4.054.6¬±2.754.6¬±3.7 50.8 ¬±0.6
Table 10: ClassiÔ¨Åcation accuracy (%) using 50selected features (except Madelon
for which we select 20features).
Method Coil20 Isolet HAR Madelon MNIST SMK GLA PCMAC
QS10099.7¬±0.3 89.0 ¬±1.3 90.2 ¬±1.2 90.3 ¬±0.7 93.5 ¬±0.5 75.7 ¬±3.9 73.3 ¬±3.358.0¬±2.9
QSLTH
10099.6¬±0.6 84.5 ¬±3.9 86.3 ¬±6.3 53.0 ¬±7.2 82.6 ¬±2.4 74.2 ¬±2.7 71.3 ¬±4.259.5¬±5.9
Table 11: Number of parameters of QS 100and QSLTH
100(divided by 106).
Method Coil20 Isolet HAR Madelon MNIST SMK GLA PCMAC
QS1000.054 0.043 0.042 0.040 0.048 0.566 1.3 0.115
QSLTH
1002.054 1.243 1.142 1.040 1.548 40.566 99.3 6.715

--- PAGE 37 ---
Quick and Robust Feature Selection 37
G Performance Evaluation using Random Forest ClassiÔ¨Åer
In this appendix, we validate the classiÔ¨Åcation accuracy results using another classiÔ¨Åer. We
repeat the experiment from Section 4.2 in the manuscript; however, we measure the accuracy
of selecting 50 features (for Madelon, we select 20 features) using the RandomForest classiÔ¨Åer
[39] instead of the ExtraTrees classiÔ¨Åer. The results are presented in Table 12.
As can be seen in Table 12, QuickSelection 100is the best performer in 5 out of 8 cases. By
comparing the results with Table 3 which demonstrates the classiÔ¨Åcation accuracy measured
by the ExtraTrees classiÔ¨Åer, it is clear that there have been subtle changes in the accuracy
values. This has resulted in some changes in the ranking of the methods in terms of the
performance, as in several cases, the performance of the methods are very close. The reason
behind choosing ExtraTrees classiÔ¨Åer in the experiment was due to the low computational
cost. However, as discussed in the paper, to perform an extensive evaluation, we have also
measured the performance using clustering accuracy. Overall, by looking into the results of the
three approaches to compute accuracy, it is clear that QuickSelection is a performant feature
selection method in terms of the quality of the selected features.
Table 12: ClassiÔ¨Åcation accuracy (%) using 50 selected features (except Madelon for
which we select 20 features). On each dataset, the bold entry is the best-performer,
and the italic one is the second-best performer. The classiÔ¨Åer used for evaluation is
the random forest classiÔ¨Åer.
Method COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS99.50.379.90.4 88.5 0.4 81.9 0.7 89.2 0.0 76.3 3.7 69.4 3.9 56.5 0.16
LS 88.9 0.8 83.4 0.2 86.4 0.388.90.620.70.1 67.9 3.1 71.1 2.8 50.13 0
CAE 99.30.6 89.0 0.789.81.0 84.20.995.20.2 76.74.776.63.8 61.62.3
AEFS 92.4 2.3 84.9 1.7 87.8 1.1 59.6 4.0 87.6 0.8 71.1 6.2 67.2 4.8 57.7 2.2
FCAE 99.0 0.6 85.8 5.2 83.6 2.6 62.7 13.1 69.6 2.9 74.2 2.6 68.9 4.0 58.8 2.5
QS1098.50.9 87.0 0.7 87.6 0.5 81.5 3.8 93.60.675.12.3 68.1 4.6 60.0 3.7
QS10099.50.3 89.1 1.3 89.01.288.90.793.20.578.53.5 73.33.167.93.8
