# 2012.00560.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/autoencoder/2012.00560.pdf
# Kích thước tệp: 2322376 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo không tên số.
(sẽ được chèn bởi biên tập viên)
Lựa chọn đặc trưng nhanh và mạnh mẽ: Sức mạnh của
Huấn luyện thưa thớt tiết kiệm năng lượng cho Autoencoders
Zahra Atashgahi Ghada Sokar Tim
van der Lee Elena Mocanu Decebal
Constantin Mocanu Raymond Veldhuis 
Mykola Pechenizkiy
Nhận: ngày / Chấp nhận: ngày
Tóm tắt Những phức tạp lớn phát sinh từ sự gia tăng gần đây về lượng dữ liệu
có chiều cao, bao gồm chi phí tính toán cao và yêu cầu bộ nhớ.
Lựa chọn đặc trưng, xác định các thuộc tính liên quan nhất và mang thông tin của
một tập dữ liệu, đã được giới thiệu như một giải pháp cho vấn đề này. Hầu hết các
phương pháp lựa chọn đặc trưng hiện có đều không hiệu quả về mặt tính toán; các thuật toán
không hiệu quả dẫn đến tiêu thụ năng lượng cao, điều này không mong muốn cho các thiết bị có
tài nguyên tính toán và năng lượng hạn chế. Trong bài báo này, một phương pháp mới và
linh hoạt cho lựa chọn đặc trưng không giám sát được đề xuất. Phương pháp này, được đặt tên là QuickSelection1,
giới thiệu sức mạnh của neuron trong mạng thần kinh thưa như một tiêu chí để
đo tầm quan trọng của đặc trưng. Tiêu chí này, kết hợp với các denoising autoencoders
kết nối thưa được huấn luyện với quy trình huấn luyện tiến hóa thưa,
rút ra tầm quan trọng của tất cả các đặc trưng đầu vào cùng một lúc. Chúng tôi triển khai Quick-
Selection theo cách hoàn toàn thưa thớt thay vì cách tiếp cận điển hình sử dụng
mặt nạ nhị phân trên các kết nối để mô phỏng độ thưa. Điều này dẫn đến tăng tốc độ
đáng kể và giảm bộ nhớ. Khi thử nghiệm trên một số tập dữ liệu chuẩn,
bao gồm năm tập dữ liệu chiều thấp và ba tập dữ liệu chiều cao, phương pháp đề xuất
có thể đạt được sự cân bằng tốt nhất về độ chính xác phân loại và phân cụm,
thời gian chạy, và mức sử dụng bộ nhớ tối đa, trong số các phương pháp được sử dụng rộng rãi cho
lựa chọn đặc trưng. Bên cạnh đó, phương pháp đề xuất của chúng tôi yêu cầu lượng năng lượng
ít nhất trong số các phương pháp lựa chọn đặc trưng dựa trên autoencoder.

Bài báo này đã được chấp nhận xuất bản trên Tạp chí Học máy (ECML-PKDD
2022 Journal Track)
Z. AtashgahiE. MocanuD.C. MocanuR.N.J. Veldhuis
Khoa Kỹ thuật Điện, Toán học và Khoa học Máy tính, Đại học Twente,
Enschede, 7500AE, Hà Lan
E-mail: z.atashgahi@utwente.nl
G.A.Z.N. Sokar1T. Lee2D.C. Mocanu1M. Pechenizkiy1
Đại học Công nghệ Eindhoven, 5600 MB Eindhoven, Hà Lan
1Khoa Toán học và Khoa học Máy tính2Khoa Kỹ thuật Điện
M. Pechenizkiy
Khoa Công nghệ Thông tin, Đại học Jyväskylä, 40014 Jyväskylä, Phần Lan
1Mã nguồn có sẵn tại: https://github.com/zahraatashgahi/QuickSelectionarXiv:2012.00560v2  [cs.LG]  13 Sep 2021

--- TRANG 2 ---
2 Zahra Atashgahi và cộng sự.
Từ khóa Lựa chọn đặc trưng Học sâu Autoencoders thưa Huấn luyện
thưa

1 Giới thiệu
Trong những năm gần đây, sự chú ý đáng kể đã được dành cho vấn đề giảm chiều
và nhiều phương pháp đã được đề xuất [ 53]. Có hai
kỹ thuật chính để giảm số lượng đặc trưng của một tập dữ liệu chiều cao:
trích xuất đặc trưng và lựa chọn đặc trưng. Trích xuất đặc trưng tập trung vào việc biến đổi
dữ liệu thành không gian chiều thấp hơn. Việc biến đổi này được thực hiện thông qua một ánh xạ
dẫn đến một tập hợp đặc trưng mới [ 40]. Lựa chọn đặc trưng giảm không gian
đặc trưng bằng cách chọn một tập con của các thuộc tính gốc mà không tạo ra các đặc trưng mới
[12]. Dựa trên sự có sẵn của nhãn, các phương pháp lựa chọn đặc trưng được chia
thành ba loại: có giám sát [ 2,12], bán giám sát [ 58,48], và không giám sát
[43,16]. Các thuật toán lựa chọn đặc trưng có giám sát cố gắng tối đa hóa một số hàm
độ chính xác dự đoán được cho các nhãn lớp. Trong học không giám sát, việc tìm kiếm
các đặc trưng phân biệt được thực hiện một cách mù quáng, không có nhãn lớp. Do đó,
lựa chọn đặc trưng không giám sát được coi là một vấn đề khó hơn nhiều [16].

Các phương pháp lựa chọn đặc trưng cải thiện khả năng mở rộng của các thuật toán học máy
vì chúng giảm chiều của dữ liệu. Bên cạnh đó, chúng giảm nhu cầu
ngày càng tăng về tài nguyên tính toán và bộ nhớ được đưa ra bởi sự xuất hiện
của dữ liệu lớn. Điều này có thể dẫn đến giảm đáng kể trong tiêu thụ năng lượng
tại các trung tâm dữ liệu. Điều này có thể giảm bớt không chỉ vấn đề chi phí năng lượng cao trong
trung tâm dữ liệu mà còn các thách thức quan trọng áp đặt lên môi trường [ 56]. Như được nêu
bởi Nhóm Chuyên gia Cấp cao về Trí tuệ Nhân tạo (AI) [ 22], sức khỏe môi trường
là một trong những yêu cầu của hệ thống AI đáng tin cậy. Việc phát triển,
triển khai, và quy trình của một hệ thống AI nên được đánh giá để đảm bảo rằng chúng
sẽ hoạt động theo cách thân thiện với môi trường nhất có thể. Ví dụ,
việc sử dụng tài nguyên và tiêu thụ năng lượng thông qua huấn luyện có thể được đánh giá.

Tuy nhiên, một vấn đề thách thức phát sinh trong lĩnh vực lựa chọn đặc trưng là
việc chọn đặc trưng từ các tập dữ liệu chứa một số lượng lớn đặc trưng và
mẫu, có thể yêu cầu một lượng lớn bộ nhớ, tính toán, và tài nguyên
năng lượng. Vì hầu hết các kỹ thuật lựa chọn đặc trưng hiện có được thiết kế để
xử lý dữ liệu quy mô nhỏ, hiệu quả của chúng có thể bị giảm với dữ liệu
chiều cao [9]. Chỉ có một số ít nghiên cứu tập trung vào việc thiết kế các thuật toán lựa chọn đặc trưng
hiệu quả về mặt tính toán [ 52,1]. Các đóng góp chính của bài báo này
có thể được tóm tắt như sau:

Chúng tôi đề xuất một phương pháp lựa chọn đặc trưng không giám sát mới nhanh và mạnh mẽ, được đặt tên là
QuickSelection. Như được phác thảo ngắn gọn trong Hình 1, nó có hai thành phần chính:
(1) Lấy cảm hứng từ sức mạnh node trong lý thuyết đồ thị, phương pháp đề xuất sức mạnh
neuron của mạng thần kinh thưa như một tiêu chí để đo tầm quan trọng
đặc trưng; và (2) Phương pháp giới thiệu Denoising Autoencoders kết nối thưa
(sparse DAEs) được huấn luyện từ đầu với quy trình huấn luyện tiến hóa thưa
để mô hình hóa phân phối dữ liệu một cách hiệu quả. Độ thưa áp đặt trước
huấn luyện cũng giảm lượng bộ nhớ cần thiết và thời gian chạy huấn luyện.

Chúng tôi triển khai QuickSelection theo cách hoàn toàn thưa trong Python sử dụng
thư viện SciPy và Cython thay vì sử dụng mặt nạ nhị phân trên các kết nối

--- TRANG 3 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 3
Epoch 5
(c) Tính toán
Sức mạnh Neurons(d) Lựa chọn
Đặc trưngs1 = 0.2
s2 = 0.5
s5 = 0.3s4 = 0s3 = 1.3f 1
f 2
f 3
f 4
f 5f 2
f 3
(b) Huấn luyện Sparse-DAE(a) Khởi tạo
Sparse-DAEEpoch 10 Epoch 0
Hình 1: Tổng quan cấp cao về phương pháp đề xuất, "QuickSelection". (a) Tại
epoch 0, các kết nối được khởi tạo ngẫu nhiên. (b) Sau khi khởi tạo cấu trúc thưa,
chúng ta bắt đầu quy trình huấn luyện. Sau 5 epochs, một số kết nối được
thay đổi trong quá trình huấn luyện, và kết quả là sức mạnh của một số
neurons đã tăng hoặc giảm. Tại epoch 10, mạng đã hội tụ, và
chúng ta có thể quan sát những neurons nào quan trọng (các vòng tròn xanh lớn hơn và tối hơn) và
những neurons nào không quan trọng. (c) Khi mạng đã hội tụ, chúng ta tính toán sức mạnh của tất cả
neurons đầu vào. (d) Cuối cùng, chúng ta chọn K đặc trưng tương ứng với neurons có
giá trị sức mạnh cao nhất.

để mô phỏng độ thưa. Điều này đảm bảo yêu cầu tài nguyên tối thiểu, tức là chỉ
Bộ nhớ truy cập ngẫu nhiên (RAM) và Bộ xử lý trung tâm (CPU), mà không
yêu cầu Bộ xử lý đồ họa (GPU).

Các thí nghiệm thực hiện trên 8 tập dữ liệu chuẩn cho thấy QuickSelection
có một số ưu điểm so với phương pháp tiên tiến hiện tại, như sau:
Nó là người thực hiện tốt nhất hoặc tốt thứ hai về cả độ chính xác phân loại và
phân cụm trong hầu hết tất cả các kịch bản được xem xét.
Nó là người thực hiện tốt nhất về sự cân bằng giữa độ chính xác phân loại và
phân cụm, thời gian chạy, và yêu cầu bộ nhớ.
Kiến trúc thưa đề xuất cho lựa chọn đặc trưng có ít nhất một bậc
độ lớn ít tham số hơn so với đối tác dày đặc tương đương. Điều này dẫn đến
thực tế nổi bật là thời gian huấn luyện wall clock của QuickSelection chạy
trên CPU nhỏ hơn thời gian huấn luyện wall clock của các đối thủ cạnh tranh dựa trên autoencoder
chạy trên GPU trong hầu hết các trường hợp.
Cuối cùng nhưng không kém phần quan trọng, hiệu quả tính toán của QuickSelection làm cho nó có
mức tiêu thụ năng lượng tối thiểu trong số các phương pháp lựa chọn đặc trưng dựa trên autoencoder
được xem xét.

2 Nghiên cứu liên quan

2.1 Lựa chọn đặc trưng

Tài liệu về lựa chọn đặc trưng cho thấy nhiều phương pháp khác nhau có thể được
chia thành ba loại chính, bao gồm phương pháp lọc, wrapper, và embedded.
Phương pháp lọc sử dụng một tiêu chí xếp hạng để chấm điểm các đặc trưng và sau đó
loại bỏ các đặc trưng có điểm số dưới một ngưỡng. Các tiêu chí này có thể là điểm số
Laplacian

--- TRANG 4 ---
4 Zahra Atashgahi và cộng sự.

[27], Tương quan, Thông tin Tương hỗ [ 12], và nhiều phương pháp chấm điểm khác
như hàm chấm điểm Bayesian, chấm điểm t-test, và tiêu chí dựa trên Lý thuyết Thông tin
[33]. Các phương pháp này thường nhanh và hiệu quả về mặt tính toán. Phương pháp
Wrapper đánh giá các tập con khác nhau của đặc trưng để phát hiện tập con tốt nhất. 
Phương pháp Wrapper thường cho hiệu suất tốt hơn phương pháp lọc; chúng sử dụng một
mô hình dự đoán để chấm điểm từng tập con đặc trưng. Tuy nhiên, điều này dẫn đến
độ phức tạp tính toán cao. Các đóng góp tiên phong cho loại lựa chọn đặc trưng này đã được
thực hiện bởi Kohavi và John [ 30]. Trong [30], các tác giả đã sử dụng cấu trúc cây để
đánh giá các tập con đặc trưng. Phương pháp Embedded thống nhất quá trình học, 
và lựa chọn đặc trưng [ 31]. Multi-Cluster Feature Selection (MCFS) [ 11] là
một phương pháp không giám sát cho lựa chọn đặc trưng embedded, chọn đặc trưng
sử dụng hồi quy phổ với regularization L1-norm. Một hạn chế chính của thuật toán
này là nó tính toán chuyên sâu vì nó phụ thuộc vào việc tính toán
các eigenvector của ma trận tương tự dữ liệu và sau đó giải một bài toán hồi quy
được regularized L1 cho mỗi eigenvector [ 19]. Unsupervised Discriminative Feature
Selection(UDFS) [ 57] là một thuật toán lựa chọn đặc trưng embedded không giám sát khác
đồng thời sử dụng cả thông tin đặc trưng và phân biệt để chọn
đặc trưng [37].

2.2 Autoencoders cho Lựa chọn đặc trưng

Trong vài năm gần đây, nhiều mô hình dựa trên học sâu đã được phát triển để chọn
đặc trưng từ dữ liệu đầu vào sử dụng quy trình học của mạng thần kinh sâu
[38]. Trong [42], một Multi-Layer Perceptron (MLP) được bổ sung với một lớp ghép cặp
để đưa mỗi đặc trưng đầu vào cùng với đối tác knockoff của nó vào mạng.
Sau huấn luyện, các tác giả sử dụng trọng số bộ lọc của lớp ghép cặp
để xếp hạng các đặc trưng đầu vào. Autoencoders thường được biết đến như một công cụ mạnh mẽ cho
trích xuất đặc trưng [ 8], đang được khám phá để thực hiện lựa chọn đặc trưng không giám sát.
Trong [24], các tác giả kết hợp hồi quy autoencoder và nhiệm vụ group lasso cho lựa chọn
đặc trưng không giám sát được đặt tên là AutoEncoder Feature Selector (AEFS). Trong [ 15], một
autoencoder được kết hợp với ba biến thể của regularization cấu trúc để thực hiện
lựa chọn đặc trưng không giám sát. Các regularization này dựa trên biến slack,
trọng số, và gradient, tương ứng. Một phương pháp embedded dựa trên autoencoder được đề xuất gần đây khác
là lựa chọn đặc trưng với Concrete Autoencoder (CAE) [ 5]. Phương pháp này
chọn đặc trưng bằng cách học một phân phối concrete trên các đặc trưng đầu vào.
Họ đề xuất một lớp selector concrete chọn một tổ hợp tuyến tính của đặc trưng
đầu vào hội tụ về một tập hợp rời rạc của K đặc trưng trong quá trình huấn luyện. Trong [ 49], các
tác giả đã chỉ ra rằng một tập lớn tham số trong CAE có thể dẫn đến over-fitting trong
trường hợp có số lượng mẫu hạn chế. Ngoài ra, CAE có thể chọn đặc trưng
nhiều lần vì không có tương tác giữa các neurons của lớp selector.
Để giảm thiểu những vấn đề này, họ đã đề xuất một phương pháp lựa chọn đặc trưng mạng thần kinh
concrete (FsNet), bao gồm một lớp selector và một mạng thần kinh sâu có giám sát.
Quy trình huấn luyện của FsNet xem xét việc giảm loss tái tạo
và tối đa hóa độ chính xác phân loại đồng thời. Trong nghiên cứu của chúng tôi,
chúng tôi tập trung chủ yếu vào các phương pháp lựa chọn đặc trưng không giám sát.

Denoising Autoencoder (DAE) được giới thiệu để giải quyết vấn đề học
hàm đồng nhất trong autoencoders. Vấn đề này rất có thể xảy ra
khi chúng ta có nhiều neurons ẩn hơn đầu vào [ 4]. Kết quả là, đầu ra mạng

--- TRANG 5 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 5

có thể bằng với đầu vào, điều này làm cho autoencoder trở nên vô dụng. DAEs giải quyết
vấn đề nói trên bằng cách đưa nhiễu vào dữ liệu đầu vào và cố gắng
tái tạo đầu vào gốc từ phiên bản nhiễu của nó [ 54]. Kết quả là, DAEs học một
biểu diễn của dữ liệu đầu vào mạnh mẽ trước những thay đổi nhỏ không liên quan trong
đầu vào. Trong nghiên cứu này, chúng tôi sử dụng khả năng của loại mạng thần kinh này để mã hóa
phân phối dữ liệu đầu vào và chọn các đặc trưng quan trọng nhất. Hơn nữa, chúng tôi
chứng minh tác động của việc thêm nhiễu đối với kết quả lựa chọn đặc trưng.

2.3 Huấn luyện thưa

Mạng thần kinh sâu thường có ít nhất một số lớp fully-connected, dẫn đến
một số lượng lớn tham số. Trong không gian chiều cao, điều này không
mong muốn vì có thể gây ra sự giảm đáng kể trong tốc độ huấn luyện và tăng
yêu cầu bộ nhớ. Để giải quyết vấn đề này, mạng thần kinh thưa đã
được đề xuất. Cắt tỉa mạng thần kinh dày đặc là một trong những phương pháp nổi tiếng nhất
để đạt được mạng thần kinh thưa [ 35,26]. Trong [25], Han và cộng sự bắt đầu từ
một mạng được huấn luyện trước, cắt tỉa các trọng số không quan trọng, và huấn luyện lại mạng.
Mặc dù phương pháp này có thể đưa ra một mạng với mức độ thưa mong muốn, chi phí
tính toán tối thiểu bằng với chi phí huấn luyện một mạng dày đặc. Để
giảm chi phí này, Lee và cộng sự [ 36] bắt đầu với một mạng thần kinh dày đặc, và cắt tỉa nó trước
khi huấn luyện dựa trên độ nhạy kết nối; sau đó, mạng thưa được huấn luyện theo
cách tiêu chuẩn. Tuy nhiên, bắt đầu từ một mạng thần kinh dày đặc yêu cầu ít nhất
kích thước bộ nhớ của mạng thần kinh dày đặc và tài nguyên tính toán cho
một lần lặp huấn luyện của mạng dày đặc. Do đó, phương pháp này có thể không
phù hợp cho các thiết bị tài nguyên thấp.

Vào năm 2016, Mocanu và cộng sự [ 44] đã giới thiệu ý tưởng huấn luyện mạng thần kinh thưa
từ đầu, một khái niệm gần đây bắt đầu được biết đến như huấn luyện thưa. Mẫu
kết nối thưa được cố định trước khi huấn luyện sử dụng lý thuyết đồ thị, khoa học mạng,
và thống kê dữ liệu. Mặc dù nó cho thấy kết quả đầy hứa hẹn,
vượt trội hơn đối tác dày đặc, mẫu thưa tĩnh không phải lúc nào cũng
mô hình hóa dữ liệu một cách tối ưu. Để giải quyết những vấn đề này, vào năm 2018, Mocanu và cộng sự
[45] đã đề xuất thuật toán Sparse Evolutionary Training (SET) sử dụng
độ thưa động trong quá trình huấn luyện. Ý tưởng là bắt đầu với một mạng thần kinh thưa
trước khi huấn luyện và thay đổi động các kết nối của nó trong quá trình huấn luyện để
tự động mô hình hóa phân phối dữ liệu. Điều này dẫn đến một sự
giảm đáng kể số lượng tham số và tăng hiệu suất. SET tiến hóa các
kết nối thưa tại mỗi epoch huấn luyện bằng cách loại bỏ một phần kết nối
có độ lớn nhỏ nhất, và thêm ngẫu nhiên các kết nối mới trong mỗi lớp.
Bourgin và cộng sự [ 10] đã chỉ ra rằng một MLP thưa được huấn luyện với SET đạt được kết quả
tốt nhất trên dữ liệu dạng bảng trong việc dự đoán quyết định của con người, vượt trội hơn
mạng thần kinh fully-connected và Random Forest, trong số những phương pháp khác.

Trong công trình này, chúng tôi giới thiệu lần đầu tiên huấn luyện thưa trong thế giới
denoising autoencoders, và chúng tôi đặt tên cho mô hình mới được giới thiệu là sparse denoising
autoencoder (sparse DAE). Chúng tôi huấn luyện sparse DAE với thuật toán SET để
giữ số lượng tham số thấp, trong quá trình huấn luyện. Sau đó, chúng tôi khai thác
mạng đã huấn luyện để chọn các đặc trưng quan trọng nhất.

--- TRANG 6 ---
6 Zahra Atashgahi và cộng sự.

3 Phương pháp đề xuất

Để giải quyết vấn đề chiều cao của dữ liệu, chúng tôi đề xuất một phương pháp mới,
được đặt tên là "QuickSelection", để chọn các thuộc tính mang thông tin nhất từ
dữ liệu, dựa trên sức mạnh (tầm quan trọng) của chúng. Tóm lại, chúng tôi huấn luyện một mạng
sparse denoising autoencoder từ đầu theo cách không giám sát thích ứng. Sau đó, chúng tôi
sử dụng mạng đã huấn luyện để rút ra sức mạnh của mỗi neuron trong các đặc trưng đầu vào.

Ý tưởng cơ bản của phương pháp đề xuất của chúng tôi là áp đặt các kết nối thưa trên
DAE, đã chứng minh thành công trong lĩnh vực liên quan của trích xuất đặc trưng, để hiệu quả
xử lý độ phức tạp tính toán của dữ liệu chiều cao về mặt tài nguyên
bộ nhớ. Các kết nối thưa được phát triển theo cách thích ứng giúp trong
việc xác định các đặc trưng mang thông tin.

Một số phương pháp đã được đề xuất cho việc huấn luyện mạng thần kinh sâu
từ đầu sử dụng kết nối thưa và huấn luyện thưa [ 14,45,7,46,17,59]. Tất cả
các phương pháp này được triển khai sử dụng mặt nạ nhị phân trên các kết nối để mô phỏng
độ thưa vì tất cả các thư viện học sâu tiêu chuẩn và phần cứng (ví dụ: GPUs) không được
tối ưu hóa cho các phép toán ma trận trọng số thưa. Không giống như các phương pháp nói trên,
chúng tôi triển khai phương pháp đề xuất của mình theo cách hoàn toàn thưa để đáp ứng mục tiêu
thực sự sử dụng các ưu điểm của một số lượng rất nhỏ tham số trong quá trình huấn luyện.
Chúng tôi quyết định sử dụng SET trong việc huấn luyện sparse DAE của chúng tôi.

Việc lựa chọn SET là do đặc tính mong muốn của nó. SET là một phương pháp đơn giản
nhưng đạt được hiệu suất thỏa đáng. Không giống như các phương pháp khác tính toán và
lưu trữ thông tin cho tất cả trọng số mạng, bao gồm cả những trọng số không tồn tại, SET
hiệu quả về bộ nhớ. Nó chỉ lưu trữ trọng số cho các kết nối thưa hiện có.
Nó không cần độ phức tạp tính toán cao vì quy trình tiến hóa
chỉ phụ thuộc vào độ lớn của các kết nối hiện có. Đây là một ưu điểm
thuận lợi cho phương pháp đề xuất của chúng tôi để chọn các đặc trưng mang thông tin một cách nhanh chóng. Trong
các phần con sau, chúng tôi trước tiên trình bày cấu trúc của mạng sparse denoising
autoencoder đề xuất và sau đó giải thích phương pháp lựa chọn đặc trưng. Mã giả
của phương pháp đề xuất có thể được tìm thấy trong Thuật toán 1.

3.1 Sparse DAE

Cấu trúc Vì mục tiêu của phương pháp đề xuất của chúng tôi là thực hiện lựa chọn đặc trưng nhanh theo
cách hiệu quả bộ nhớ, chúng tôi xem xét ở đây mô hình với số lượng ít nhất có thể của
các lớp ẩn, một lớp ẩn, vì nhiều lớp có nghĩa là nhiều tính toán hơn. Ban đầu,
các kết nối thưa giữa hai lớp neurons liên tiếp được khởi tạo với
một đồ thị ngẫu nhiên Erdős–Rényi, trong đó xác suất kết nối giữa
hai neurons được cho bởi

P(Wl
ij) =(nl1+nl)
nl1nl; (1)

trong đó  biểu thị tham số kiểm soát mức độ thưa, nl biểu thị số
neurons tại lớp l, và Wl
ij là kết nối giữa neuron i trong lớp l1 và
neuron j trong lớp l, được lưu trữ trong ma trận trọng số thưa Wl.

Khử nhiễu đầu vào Chúng tôi sử dụng mô hình nhiễu cộng để làm hỏng dữ liệu gốc:
ex=x+nfN(; 2); (2)

--- TRANG 7 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 7

trong đó x là vector dữ liệu đầu vào từ tập dữ liệu X, nf (noise factor) là một siêu
tham số của mô hình xác định mức độ hỏng, và N(; 2) là
nhiễu Gaussian. Sau khi khử nhiễu dữ liệu, chúng tôi rút ra biểu diễn ẩn
h sử dụng đầu vào bị hỏng này. Sau đó, đầu ra z được tái tạo từ biểu
diễn ẩn. Chính thức, biểu diễn ẩn h và đầu ra z được
tính toán như sau:

h=a(W1ex+b1); (3)
z=a(W2h+b2); (4)

trong đó W1 và W2 là các ma trận trọng số thưa của lớp ẩn và đầu ra
tương ứng, b1 và b2 là các vector bias của lớp tương ứng, và a là
hàm kích hoạt của mỗi lớp. Mục tiêu của mạng của chúng tôi là tái tạo
các đặc trưng gốc trong đầu ra. Vì lý do này, chúng tôi sử dụng sai số bình phương trung bình
(MSE) làm hàm loss để đo sự khác biệt giữa đặc trưng gốc x
và đầu ra tái tạo z:

LMSE =kzxk2
2: (5)

Cuối cùng, các trọng số có thể được tối ưu hóa sử dụng các thuật toán huấn luyện tiêu chuẩn
(ví dụ: Stochastic Gradient Descent (SGD), AdaGrad, và Adam) với sai số
tái tạo ở trên.

Quy trình huấn luyện Chúng tôi điều chỉnh quy trình huấn luyện SET [ 45] trong việc huấn luyện
mạng đề xuất của chúng tôi cho lựa chọn đặc trưng. SET hoạt động như sau. Sau mỗi epoch
huấn luyện, một phần các trọng số dương nhỏ nhất và một phần các trọng số
âm lớn nhất tại mỗi lớp bị loại bỏ. Việc lựa chọn dựa trên độ lớn
của các trọng số. Các kết nối mới với số lượng tương tự như các kết nối bị loại bỏ được
thêm ngẫu nhiên trong mỗi lớp. Do đó tổng số kết nối trong mỗi
lớp vẫn giữ nguyên, trong khi số lượng kết nối trên mỗi neuron thay đổi, như
được biểu diễn trong Hình 1. Các trọng số của những kết nối mới này được khởi tạo từ
một phân phối chuẩn. Việc thêm ngẫu nhiên các kết nối mới không có
rủi ro cao không tìm thấy một kết nối thưa tốt ở cuối quá trình huấn luyện
vì đã được chỉ ra trong [ 41] rằng huấn luyện thưa có thể khám phá một số lượng lớn
các tối ưu cục bộ kết nối thưa rất khác nhau đạt được hiệu suất
rất tương tự.

3.2 Lựa chọn đặc trưng

Chúng tôi chọn các đặc trưng quan trọng nhất của dữ liệu dựa trên trọng số của
các neurons đầu vào tương ứng của sparse DAE đã huấn luyện. Lấy cảm hứng từ sức mạnh node
trong lý thuyết đồ thị [ 6], chúng tôi xác định tầm quan trọng của mỗi neuron dựa trên sức mạnh của nó.
Chúng tôi ước tính sức mạnh của mỗi neuron ( si) bằng tổng của
trọng số tuyệt đối của các kết nối đi ra của nó.

si=n1X
j=1jW1
ijj; (6)

trong đó n1 là số neurons của lớp ẩn đầu tiên, và W1
ij biểu thị
trọng số của kết nối liên kết neuron đầu vào i với neuron ẩn j.

--- TRANG 8 ---
8 Zahra Atashgahi và cộng sự.

  Khởi tạo ngẫu nhiên  Sau 3 epochs  Sau 6 Epochs  Sau 10 epochs    chiều cao (pixel)  
     
Sức mạnh   chiều rộng (pixel)  chiều rộng (pixel)  chiều rộng (pixel)  chiều rộng (pixel)    

Hình 2: Sức mạnh của neuron trên tập dữ liệu MNIST. Các heat-map ở trên là biểu diễn 2D
của sức mạnh neuron đầu vào. Có thể quan sát thấy rằng sức mạnh
của các neurons là ngẫu nhiên khi bắt đầu huấn luyện. Sau một vài epochs, mẫu
thay đổi, và các neurons ở trung tâm trở nên quan trọng hơn và tương tự như
mẫu dữ liệu MNIST.

Như được biểu diễn trong Hình 1, sức mạnh của các neurons đầu vào thay đổi trong
quá trình huấn luyện; chúng tôi đã mô tả sức mạnh của các neurons theo kích thước và
màu sắc của chúng. Sau khi hội tụ, chúng tôi tính toán sức mạnh cho tất cả các neurons đầu vào; mỗi
neuron đầu vào tương ứng với một đặc trưng. Sau đó, chúng tôi chọn các đặc trưng tương ứng
với các neurons có K giá trị sức mạnh lớn nhất:

F
s= argmax
FsF;jFsj=kX
fi2Fssi; (7)

trong đó F và F
s là tập đặc trưng gốc và tập đặc trưng được chọn cuối cùng tương ứng,
fi là đặc trưng thứ i của F, và K là số lượng đặc trưng cần được chọn. Ngoài ra,
bằng cách sắp xếp tất cả các đặc trưng dựa trên sức mạnh của chúng, chúng tôi sẽ rút ra tầm quan trọng
của tất cả đặc trưng trong tập dữ liệu. Tóm lại, chúng tôi sẽ có thể xếp hạng tất cả đặc trưng đầu vào bằng
cách huấn luyện chỉ một lần một mô hình sparse DAE duy nhất.

Thuật toán 1 QuickSelection
1:Đầu vào:Tập dữ liệu X, noise factor nf, siêu tham số thưa , số neurons ẩn
nh, số đặc trưng được chọn K
2: Khử nhiễu đầu vào: ex=x+nfN(; 2)
3:Khởi tạo cấu trúc: Khởi tạo sparse-DAE với nh neurons ẩn và mức độ thưa
được xác định bởi 
4:thủ tục Huấn luyện sparse-DAE
5: Đặt loss là LMSE =kzxk2
2 trong đó z là đầu ra của sparse-DAE
6: cho i2f1;:::;epochsg làm
7: Thực hiện forward propagation và backpropagation tiêu chuẩn
8: Thực hiện loại bỏ và thêm trọng số để tối ưu hóa topology
9:thủ tục QuickSelection
10: Tính toán sức mạnh neurons:
11: cho i2f1;:::; #inputfeaturesg làm
12:si=nhP
j=1jW1
ijj
13: Chọn K đặc trưng: F
s= argmaxFsF;jFsj=KP
fi2Fssi;

--- TRANG 9 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 9

Để hiểu sâu hơn về quá trình trên, chúng tôi phân tích sức mạnh của
mỗi neuron đầu vào trong bản đồ 2D trên tập dữ liệu MNIST. Điều này được minh họa trong Hình
2. Khi bắt đầu huấn luyện, tất cả các neurons có sức mạnh nhỏ do việc
khởi tạo ngẫu nhiên mỗi trọng số thành một giá trị nhỏ. Trong quá trình tiến hóa mạng,
các kết nối mạnh hơn được liên kết với các đặc trưng quan trọng dần dần. Chúng ta có thể quan sát
rằng sau mười epochs, các neurons ở trung tâm bản đồ trở nên mạnh hơn. Mẫu
này tương tự như mẫu của dữ liệu MNIST trong đó hầu hết các chữ số xuất hiện
ở giữa của hình ảnh.

Chúng tôi đã nghiên cứu các metric khác để ước tính tầm quan trọng neuron như
sức mạnh của neurons đầu ra, degree của neurons đầu vào và đầu ra, và sức mạnh và
degree của neurons đồng thời. Tuy nhiên, trong các thí nghiệm của chúng tôi, tất cả các phương pháp này
đã bị vượt trội bởi sức mạnh của neurons đầu vào về mặt độ chính xác
và tính ổn định.

4 Thí nghiệm

Để xác minh tính hợp lệ của phương pháp đề xuất của chúng tôi, chúng tôi thực hiện một số thí
nghiệm. Trong phần này, trước tiên, chúng tôi nêu các thiết lập của thí nghiệm, bao gồm
siêu tham số và tập dữ liệu. Sau đó, chúng tôi thực hiện lựa chọn đặc trưng với QuickSelection và so sánh kết quả với các phương pháp khác, bao gồm MCFS, Laplacian
Score, và ba phương pháp lựa chọn đặc trưng dựa trên autoencoder. Sau đó, chúng tôi thực hiện
các phân tích khác nhau trên QuickSelection để hiểu hành vi của nó. Cuối cùng, chúng tôi thảo luận
về khả năng mở rộng của QuickSelection và so sánh nó với các phương pháp khác được xem xét.

4.1 Thiết lập

Các thiết lập thí nghiệm, bao gồm giá trị của siêu tham số, chi tiết triển khai,
cấu trúc của sparse DAE, tập dữ liệu chúng tôi sử dụng để đánh giá, và
metric đánh giá, như sau.

4.1.1 Siêu tham số và Triển khai

Để lựa chọn đặc trưng, chúng tôi xem xét trường hợp của sparse DAE đơn giản nhất với một
lớp ẩn gồm 1000 neurons. Lựa chọn này được thực hiện do mục tiêu chính của chúng tôi là giảm
độ phức tạp mô hình và số lượng tham số. Hàm kích hoạt được sử dụng cho neurons lớp ẩn và đầu ra
là "Sigmoid" và "Linear" tương ứng, ngoại trừ đối với tập dữ liệu Madelon nơi chúng tôi sử dụng "Tanh" cho
hàm kích hoạt đầu ra. Chúng tôi huấn luyện mạng với SGD và tốc độ học
0:01. Siêu tham số , phần trọng số bị loại bỏ trong quy trình SET,
là 0:2. Ngoài ra,, xác định mức độ thưa, được đặt thành 13. Chúng tôi đặt
noise factor ( nf) thành 0.2 trong các thí nghiệm. Để cải thiện quá trình học của
mạng của chúng tôi, chúng tôi chuẩn hóa các đặc trưng của tập dữ liệu sao cho mỗi thuộc tính
có mean bằng không và variance đơn vị. Tuy nhiên, đối với tập dữ liệu SMK và PCMAC, chúng tôi
sử dụng Min-Max scaling. Phương pháp tiền xử lý cho mỗi tập dữ liệu được xác định
bằng một thí nghiệm nhỏ của hai phương pháp tiền xử lý.

--- TRANG 10 ---
10 Zahra Atashgahi và cộng sự.

Chúng tôi triển khai sparse DAE và QuickSelection2 theo cách hoàn toàn thưa trong
Python, sử dụng thư viện Scipy [ 28] và Cython. Chúng tôi so sánh phương pháp đề xuất của mình
với MCFS, Laplacian score (LS), AEFS, và CAE, đã được đề cập
trong Phần 2. Chúng tôi cũng thực hiện một số thí nghiệm với UDFS; tuy nhiên, vì
chúng tôi không thể có được nhiều kết quả trong giới hạn thời gian được xem xét (24
giờ), chúng tôi không bao gồm kết quả trong bài báo. Chúng tôi đã sử dụng kho lưu trữ scikit-feature
cho việc triển khai MCFS, và Laplacian score [ 37]. Ngoài ra, chúng tôi
sử dụng việc triển khai lựa chọn đặc trưng với CAE và AEFS từ Github3.
Thêm vào đó, để làm nổi bật các ưu điểm của việc sử dụng các lớp thưa, chúng tôi so sánh
kết quả của chúng tôi với một autoencoder fully-connected (FCAE) sử dụng sức mạnh neuron làm
thước đo tầm quan trọng của mỗi đặc trưng. Để có một so sánh công bằng, cấu trúc
của mạng này được xem xét tương tự như DAE của chúng tôi, một lớp ẩn chứa 1000
neurons được triển khai sử dụng TensorFlow. Hơn nữa, chúng tôi đã nghiên cứu tác động của
các thành phần khác của QuickSelection, bao gồm khử nhiễu đầu vào và thuật toán huấn luyện SET,
trong Phụ lục B.1 và F, tương ứng.

Đối với tất cả các phương pháp khác (ngoại trừ FCAE mà tất cả siêu tham số
và tiền xử lý tương tự như QuickSelection), chúng tôi chia tỷ lệ dữ liệu giữa
không và một, vì nó mang lại hiệu suất tốt hơn so với chuẩn hóa dữ liệu cho
các phương pháp này. Các siêu tham số của các phương pháp nói trên đã được
đặt tương tự như những cái được báo cáo trong mã hoặc bài báo tương ứng. Đối với AEFS,
chúng tôi điều chỉnh siêu tham số regularization giữa 0:0001 và 1000, vì phương pháp này
nhạy cảm với giá trị này. Chúng tôi thực hiện các thí nghiệm của mình trên một lõi CPU duy nhất,
Intel Xeon Processor E5 v4, và đối với các phương pháp yêu cầu GPU, chúng tôi sử dụng
NVIDIA TESLA P100.

4.1.2 Tập dữ liệu

Chúng tôi đánh giá hiệu suất của phương pháp đề xuất của mình trên tám tập dữ liệu, bao gồm
năm tập dữ liệu chiều thấp và ba tập dữ liệu chiều cao. Bảng 1 minh họa
các đặc tính của những tập dữ liệu này.

– COIL-20 [47] gồm 1440 hình ảnh được chụp từ 20 đối tượng ( 72 tư thế cho mỗi
đối tượng).
– Madelon [23] là một tập dữ liệu nhân tạo với 5 đặc trưng mang thông tin và 15 tổ hợp tuyến tính
của chúng. Phần còn lại của các đặc trưng là các đặc trưng gây nhiễu vì chúng
không có khả năng dự đoán.
– Human Activity Recognition (HAR) [3] được tạo bằng cách thu thập các quan
sát của 30 đối tượng thực hiện 6 hoạt động như đi bộ, đứng, và
ngồi. Dữ liệu được ghi bởi một điện thoại thông minh được kết nối với cơ thể của đối tượng.
– Isolet [18] đã được tạo với tên nói của mỗi chữ cái của bảng chữ cái
tiếng Anh.
– MNIST [34] là một cơ sở dữ liệu của hình ảnh 28x28 chứa các chữ số viết tay.
– SMK-CAN-187 [50] là một tập dữ liệu biểu hiện gen với 19993 đặc trưng. Tập dữ liệu này
so sánh những người hút thuốc có và không có ung thư phổi.

2Việc triển khai QuickSelection có sẵn tại: https://github.com/
zahraatashgahi/QuickSelection
3Việc triển khai AEFS và CAE có sẵn tại: https://github.com/mfbalin/
Concrete-Autoencoders

--- TRANG 11 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 11

Bảng 1: Đặc tính tập dữ liệu.
Tập dữ liệu Chiều Loại Mẫu Huấn luyện Kiểm tra Lớp
Coil20 1024 Hình ảnh 1440 1152 288 20
Isolet 617 Giọng nói 7737 6237 1560 26
HAR 561 Chuỗi thời gian 10299 7352 2947 6
Madelon 500 Nhân tạo 2600 2000 600 2
MNIST 784 Hình ảnh 70000 60000 10000 10
SMK-CAN-187 19993 Microarray 187 149 38 2
GLA-BRA-180 49151 Microarray 180 144 36 4
PCMAC 3289 Văn bản 1943 1554 389 2

– GLA-BRA-180 [51] gồm hồ sơ biểu hiện của Stem cell factor hữu ích
để xác định angiogenesis khối u.
– PCMAC [32] là một tập con của dữ liệu 20 Newsgroups.

4.1.3 Metric đánh giá

Để đánh giá mô hình của chúng tôi, chúng tôi tính toán hai metric: độ chính xác phân cụm và độ chính xác phân
loại. Để rút ra độ chính xác phân cụm [ 37], trước tiên, chúng tôi thực hiện K-means sử dụng
tập con của tập dữ liệu tương ứng với các đặc trưng được chọn và nhận các nhãn cụm.
Sau đó, chúng tôi tìm sự khớp tốt nhất giữa nhãn lớp và nhãn cụm
và báo cáo độ chính xác phân cụm. Chúng tôi lặp lại thuật toán K-means 10 lần
và báo cáo kết quả phân cụm trung bình vì K-means có thể hội tụ về một tối ưu
cục bộ.

Để tính độ chính xác phân loại, chúng tôi sử dụng một mô hình phân loại có giám sát
được đặt tên là "Extremely randomized trees" (ExtraTrees), là một phương pháp học ensemble
phù hợp với một số cây quyết định ngẫu nhiên hóa trên các phần khác nhau của dữ liệu [ 21].
Việc lựa chọn phương pháp phân loại được thực hiện do tính hiệu quả tính toán
của bộ phân loại ExtraTrees. Để tính độ chính xác phân loại, trước tiên, chúng tôi rút ra
K đặc trưng được chọn sử dụng mỗi phương pháp lựa chọn đặc trưng được xem xét. Sau đó, chúng tôi huấn luyện
bộ phân loại ExtraTrees với 50 cây làm estimators trên K đặc trưng được chọn của
tập huấn luyện. Cuối cùng, chúng tôi tính độ chính xác phân loại trên dữ liệu kiểm tra
chưa thấy. Đối với các tập dữ liệu không chứa tập kiểm tra, chúng tôi chia dữ liệu thành tập
huấn luyện và kiểm tra, bao gồm 80% tổng số mẫu gốc cho tập huấn luyện
và 20% còn lại cho tập kiểm tra. Ngoài ra, chúng tôi đã đánh giá
độ chính xác phân loại của lựa chọn đặc trưng sử dụng bộ phân loại random forest [ 39] trong
Phụ lục G.

4.2 Lựa chọn đặc trưng

Chúng tôi chọn 50 đặc trưng từ mỗi tập dữ liệu ngoại trừ Madelon, mà chúng tôi chọn chỉ
20 đặc trưng vì hầu hết các đặc trưng của nó là nhiễu không mang thông tin. Sau đó, chúng tôi tính toán
độ chính xác phân cụm và phân loại trên tập con đặc trưng được chọn; càng nhiều đặc trưng
mang thông tin được chọn, độ chính xác càng cao sẽ đạt được. Kết quả độ chính xác phân cụm và
phân loại của mô hình của chúng tôi và các phương pháp khác
được tóm tắt trong Bảng 2 và 3, tương ứng. Những kết quả này là trung bình của 5 lần chạy
cho mỗi trường hợp. Đối với các phương pháp lựa chọn đặc trưng dựa trên autoencoder, bao gồm
CAE, AEFS, và FCAE, chúng tôi xem xét 100 epochs huấn luyện. Tuy nhiên, chúng tôi trình bày

--- TRANG 12 ---
12 Zahra Atashgahi và cộng sự.

Bảng 2: Độ chính xác phân cụm (%) sử dụng 50 đặc trưng được chọn (ngoại trừ Madelon
mà chúng tôi chọn 20 đặc trưng). Trên mỗi tập dữ liệu, mục in đậm là người thực hiện tốt nhất,
và mục in nghiêng là người thực hiện tốt thứ hai.
Phương pháp COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS67.00.7 33.80.562.40.057.20.0 35.2 0 51.6 0.265.80.350.60.0
LS 55.5 0.4 33.2 0.2 61.20.0 58.1 0.014.90.1 51.6 0.4 55.5 0.4 50.6 0.0
CAE 60.0 1.1 31.6 1.3 51.4 0.4 56.9 3.649.21.5 60.7 0.455.41.3 52.01.2
AEFS 51.2 1.7 31.0 2.7 55.0 2.2 50.8 0.2 40.0 1.9 52.4 1.8 56.1 5.2 50.9 0.5
FCAE 60.21.728.72.5 49.5 8.7 50.9 0.4 28.2 8.5 51.5 0.8 53.5 3.0 50.9 0.1
QS1059.52.1 32.5 2.8 56.0 2.6 57.5 3.8 45.4 3.9 54.03.153.64.7 50.9 0.5
QS100 60.22.035.12.754.64.558.21.5 48.32.451.80.8 59.51.852.51.1
QSbest63.81.5 42.2 2.6 59.5 4.3 58.6 0.9 48.3 2.4 54.9 1.39 59.5 1.8 53.1 0

Bảng 3: Độ chính xác phân loại (%) sử dụng 50 đặc trưng được chọn (ngoại trừ Madelon
mà chúng tôi chọn 20 đặc trưng). Trên mỗi tập dữ liệu, mục in đậm là người thực hiện tốt nhất,
và mục in nghiêng là người thực hiện tốt thứ hai.
Phương pháp COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS 99.2 0.3 79.5 0.4 88.9 0.3 81.7 0.8 88.7 0 75.8 1.5 70.6 3.8 55.5 0.0
LS 89.8 0.4 83.0 0.2 86.4 0.491.40.920.70.1 71.6 5.6 71.7 1.1 50.4 0.0
CAE 99.60.389.80.6 91.7 1.087.52.095.40.171.63.1 70.0 4.159.91.5
AEFS 93.0 2.7 85.1 2.4 87.7 1.4 52.1 2.8 86.1 2.0 76.34.468.93.7 57.1 3.6
FCAE99.70.281.65.9 87.4 2.4 53.5 8.1 68.8 28.7 71.6 3.5 72.84.858.11.9
QS1098.80.6 86.9 1.1 88.8 0.7 86.6 3.6 93.80.676.94.669.43.0 58.94.4
QS10099.70.3 89.01.3 90.2 1.2 90.3 0.793.50.5 75.7 3.973.33.358.02.9
QSbest99.70.3 89.0 1.3 90.5 1.6 90.9 0.5 94.2 0.5 81.6 2.9 73.3 3.3 61.3 6.1

kết quả của QuickSelection tại epoch 10 và 100 được đặt tên là QuickSelection 10 và
QuickSelection 100, tương ứng. Điều này chủ yếu do thực tế rằng phương pháp đề xuất
của chúng tôi có thể đạt được độ chính xác hợp lý sau vài epochs đầu tiên. Hơn nữa,
chúng tôi thực hiện điều chỉnh siêu tham số cho  và  sử dụng phương pháp grid search trên
một số giá trị hạn chế cho tất cả tập dữ liệu; kết quả tốt nhất được trình bày trong Bảng 2
và 3 như QuickSelection best. Kết quả của việc chọn siêu tham số có thể được tìm thấy
trong Phụ lục B.2. Tuy nhiên, chúng tôi không thực hiện tối ưu hóa siêu tham số cho
các phương pháp khác (ngoại trừ AEFS). Do đó, để có so sánh công bằng
giữa tất cả các phương pháp, chúng tôi không so sánh kết quả của QuickSelection best với
các phương pháp khác.

Từ Bảng 2, có thể quan sát thấy rằng QuickSelection vượt trội hơn tất cả các phương pháp khác
trên Isolet, Madelon, và PCMAC, về độ chính xác phân cụm, trong khi là
người thực hiện tốt thứ hai trên Coil20, MNIST, SMK, và GLA. Hơn nữa,
Trên tập dữ liệu HAR, nó là người thực hiện tốt nhất trong tất cả các phương pháp lựa chọn đặc trưng dựa trên autoencoder được xem xét. Như được hiển thị trong Bảng 3, QuickSelection vượt trội hơn tất cả các phương pháp khác trên Coil20, SMK, và GLA, về độ chính xác phân
loại, trong khi là người thực hiện tốt thứ hai trên các tập dữ liệu khác. Từ những
Bảng này, rõ ràng là QuickSelection có thể vượt trội hơn mạng dày đặc tương đương của nó
(FCAE) về độ chính xác phân loại và phân cụm trên tất cả tập dữ liệu.

--- TRANG 13 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 13

/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000006/uni00000003/uni00000052/uni00000049/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni00000055/uni00000048/uni00000050/uni00000052/uni00000059/uni00000048/uni00000047/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000003A/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003 (/uni00000008)
/uni0000002f/uni00000048/uni00000044/uni00000056/uni00000057/uni00000003/uni00000057/uni00000052/uni00000003/uni00000050/uni00000052/uni00000056/uni00000057/uni00000003/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057
/uni00000013/uni00000003 /uni00000014/uni00000013/uni00000013/uni00000003 /uni00000015/uni00000013/uni00000013/uni00000003 /uni00000016/uni00000013/uni00000013/uni00000003 /uni00000017/uni00000013/uni00000013/uni00000003 /uni00000018/uni00000013/uni00000013
/uni00000006/uni00000003/uni00000052/uni00000049/uni00000003/uni00000049/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000003/uni00000055/uni00000048/uni00000050/uni00000052/uni00000059/uni00000048/uni00000047/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000003A/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003 (/uni00000008)
/uni00000030/uni00000052/uni00000056/uni00000057/uni00000003/uni00000057/uni00000052/uni00000003/uni0000002f/uni00000048/uni00000044/uni00000056/uni00000057/uni00000003/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057
/uni00000030/uni00000052/uni00000056/uni00000057/uni00000003/uni00000057/uni00000052/uni00000003 l/uni00000048/uni00000044/uni00000056/uni00000057/uni00000003/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057

Hình 3: Ảnh hưởng của việc loại bỏ đặc trưng trên tập dữ liệu Madelon. Sau khi rút ra tầm quan
trọng của các đặc trưng với QuickSelection, chúng tôi sắp xếp và sau đó loại bỏ chúng dựa trên
hai phương pháp ở trên.

Có thể quan sát trong Bảng 2 và 3, rằng Lap_score có hiệu suất kém
khi số lượng mẫu lớn (ví dụ: MNIST). Tuy nhiên, trong các nhiệm vụ có
số lượng mẫu và đặc trưng thấp, ngay cả trong môi trường nhiễu như Madelon,
Lap_score có hiệu suất tương đối tốt. Ngược lại, CAE có hiệu suất kém
trong môi trường nhiễu (ví dụ: Madelon), trong khi nó có độ chính xác phân loại tốt trên các tập dữ liệu khác được xem xét. Nó là người thực hiện tốt nhất hoặc tốt thứ hai
trên năm tập dữ liệu, về độ chính xác phân loại, khi K= 50. AEFS và
FCAE cũng không thể đạt được hiệu suất tốt trên Madelon. Chúng tôi tin rằng
các lớp dày đặc là nguyên nhân chính của hành vi này; các kết nối dày đặc cố gắng
học tất cả đặc trưng đầu vào, ngay cả các đặc trưng nhiễu. Do đó, chúng thất bại trong việc phát hiện
các thuộc tính quan trọng nhất của dữ liệu. MCFS hoạt động tốt trên hầu hết
các tập dữ liệu về độ chính xác phân cụm. Điều này do mục tiêu chính của
MCFS là bảo tồn cấu trúc đa cụm của dữ liệu. Tuy nhiên, phương pháp này
cũng có hiệu suất kém trên các tập dữ liệu có số lượng mẫu lớn (ví dụ:
MNIST) và đặc trưng nhiễu (ví dụ: Madelon).

Tuy nhiên, vì việc đánh giá các phương pháp chỉ bằng cách nhìn vào một giá trị duy nhất của K có thể không
đủ để so sánh, chúng tôi đã thực hiện một thí nghiệm khác sử dụng các giá trị khác nhau của
K. Trong Phụ lục A.1, chúng tôi kiểm tra các giá trị khác cho K trên tất cả tập dữ liệu, và so sánh các
phương pháp về độ chính xác phân loại, độ chính xác phân cụm, thời gian chạy, và
mức sử dụng bộ nhớ tối đa. Tóm tắt kết quả của Phụ lục này đã được
tóm tắt trong Phần 5.1.

4.2.1 Mức độ liên quan của Đặc trưng được chọn

Để minh họa khả năng của QuickSelection trong việc tìm các đặc trưng mang thông tin, chúng tôi
phân tích kỹ lưỡng kết quả tập dữ liệu Madelon, có đặc tính thú vị
chứa nhiều đặc trưng nhiễu. Chúng tôi thực hiện các thí nghiệm sau; trước tiên,
chúng tôi sắp xếp các đặc trưng dựa trên sức mạnh của chúng. Sau đó, chúng tôi loại bỏ các đặc trưng từng cái một
từ đặc trưng ít quan trọng nhất đến đặc trưng quan trọng nhất. Trong mỗi
bước, chúng tôi huấn luyện một bộ phân loại ExtraTrees với các đặc trưng còn lại. Chúng tôi lặp lại thí nghiệm này
bằng cách loại bỏ đặc trưng từ những đặc trưng quan trọng nhất đến ít quan trọng nhất.
Kết quả của độ chính xác phân loại cho cả hai thí nghiệm có thể
được thấy trong Hình 3. Ở phía bên trái của Hình 3, chúng ta có thể quan sát rằng việc loại bỏ các
đặc trưng ít quan trọng nhất, là nhiễu, làm tăng độ chính xác. Độ chính xác tối đa
xảy ra sau khi chúng ta loại bỏ 480 đặc trưng nhiễu. Điều này tương ứng với thời điểm

--- TRANG 14 ---
14 Zahra Atashgahi và cộng sự.

      
      
 

Hình 4: Giá trị trung bình của tất cả mẫu dữ liệu của mỗi lớp tương ứng với 50
đặc trưng được chọn trên MNIST sau 100 epochs huấn luyện (dưới), cùng với
trung bình của các mẫu dữ liệu thực tế của mỗi lớp (trên).

khi tất cả các đặc trưng nhiễu được cho là bị loại bỏ. Trong Hình 3 (phải), có thể
thấy rằng việc loại bỏ các đặc trưng theo thứ tự ngược lại dẫn đến một sự
giảm đột ngột trong độ chính xác phân loại. Sau khi loại bỏ 20 đặc trưng (được chỉ ra bởi
đường thẳng đứng màu xanh), bộ phân loại hoạt động như một bộ phân loại ngẫu nhiên. Chúng tôi kết luận
rằng QuickSelection có thể tìm ra các đặc trưng mang thông tin nhất theo thứ tự tốt.

Để thể hiện tốt hơn mức độ liên quan của các đặc trưng được tìm thấy bởi QuickSelection, chúng tôi
trực quan hóa 50 đặc trưng được chọn trên tập dữ liệu MNIST theo lớp, bằng cách tính trung bình
các giá trị tương ứng của chúng từ tất cả mẫu dữ liệu thuộc về một lớp. Như có thể
quan sát thấy trong Hình 4, hình dạng kết quả giống với các mẫu thực tế của
chữ số tương ứng. Chúng tôi thảo luận về kết quả của tất cả các lớp tại các epochs huấn luyện
khác nhau chi tiết hơn trong Phụ lục C.

5 Thảo luận

5.1 Sự cân bằng giữa Độ chính xác và Hiệu quả Tính toán

Trong phần này, chúng tôi thực hiện so sánh kỹ lưỡng giữa các mô hình về
thời gian chạy, tiêu thụ năng lượng, yêu cầu bộ nhớ, độ chính xác phân cụm, và
độ chính xác phân loại. Tóm lại, chúng tôi thay đổi số lượng đặc trưng cần được chọn
(K) và đo độ chính xác, thời gian chạy, và mức sử dụng bộ nhớ tối đa trên
tất cả các phương pháp. Sau đó, chúng tôi tính hai điểm số để tóm tắt kết quả và so sánh
các phương pháp.

Chúng tôi phân tích tác động của việc thay đổi K đối với hiệu suất QuickSelection và so sánh
với các phương pháp khác; kết quả được trình bày trong Hình 10 trong Phụ lục A.1. Hình
10a so sánh hiệu suất của tất cả các phương pháp khi K thay đổi giữa 5 và
100 trên các tập dữ liệu chiều thấp, bao gồm Coil20, Isolet, HAR, và Madelon.
Hình 10b minh họa so sánh hiệu suất cho K giữa 5 và 300 trên
tập dữ liệu MNIST, cũng là một tập dữ liệu chiều thấp. Chúng tôi thảo luận về tập dữ liệu này
riêng biệt vì nó có số lượng mẫu lớn làm cho nó khác với các
tập dữ liệu chiều thấp khác. Hình 10c đại diện cho một so sánh tương tự trên ba
tập dữ liệu chiều cao, bao gồm SMK, GLA, và PCMAC. Cần lưu ý rằng để có
so sánh công bằng, chúng tôi sử dụng một lõi CPU duy nhất để chạy các phương pháp này;
tuy nhiên, vì việc triển khai CAE và AEFS được tối ưu hóa cho việc chạy song song

--- TRANG 15 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 15

/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c
(a) Điểm số 1. Điểm số được đưa ra
dựa trên xếp hạng của các
phương pháp.

/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000014/uni00000017/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c(b) Điểm số 2. Điểm số được đưa ra
dựa trên giá trị chuẩn hóa
của mỗi mục tiêu.

/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004c/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c
/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013
/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni00000014
(c) Trực quan hóa heat-map của
Điểm số 1.

Hình 5: So sánh lựa chọn đặc trưng về độ chính xác phân loại, độ chính xác phân cụm,
tốc độ, và yêu cầu bộ nhớ, trên mỗi tập dữ liệu và cho các giá trị khác nhau
của K, sử dụng hai biến thể chấm điểm.

tính toán, chúng tôi sử dụng GPU để chạy các phương pháp này. Chúng tôi cũng đo thời gian chạy
của lựa chọn đặc trưng với CAE trên CPU.

Để so sánh yêu cầu bộ nhớ của mỗi phương pháp, chúng tôi profile mức sử dụng bộ nhớ tối đa
trong quá trình lựa chọn đặc trưng cho các giá trị khác nhau của K. Kết quả được
trình bày trong Hình 11 trong Phụ lục A.1, được rút ra sử dụng thư viện Python có tên
resource4. Bên cạnh đó, để so sánh bộ nhớ được chiếm bởi các mô hình dựa trên autoencoder,
chúng tôi đếm số lượng tham số cho mỗi mô hình. Kết quả được hiển thị trong Hình
14 trong Phụ lục A.3.

Tuy nhiên, việc so sánh tất cả các phương pháp này chỉ bằng cách nhìn vào các đồ thị trong
Hình 10 và Hình 11 là không dễ dàng, và sự cân bằng giữa các yếu tố
không rõ ràng. Vì lý do này, chúng tôi tính hai điểm số để tính đến tất cả các metrics này
đồng thời.

Điểm số 1. Để tính điểm số này, trên mỗi tập dữ liệu và cho mỗi giá trị của K, chúng tôi
xếp hạng các phương pháp dựa trên thời gian chạy, yêu cầu bộ nhớ, độ chính xác phân cụm,
và độ chính xác phân loại. Sau đó, chúng tôi đưa ra điểm số 1 cho người thực hiện tốt nhất và
tốt thứ hai; điều này chủ yếu do thực tế rằng trong hầu hết các trường hợp, sự
khác biệt giữa hai người này là không đáng kể. Sau đó, chúng tôi tính tổng
của những điểm số này cho mỗi phương pháp trên tất cả tập dữ liệu. Kết quả được trình bày trong
Hình 5a; để dễ dàng so sánh các thành phần khác nhau trong điểm số, một trực quan hóa heat-map
của các điểm số được trình bày trong Hình 5c. Điểm số tích lũy cho mỗi
phương pháp gồm bốn phần tương ứng với mỗi metric được xem xét. Như có thể
thấy rõ trong Hình này, QuickSelection (điểm số tích lũy của QuickSelection 10
và QuickSelection 100) vượt trội hơn tất cả các phương pháp khác với khoảng cách đáng kể. Phương pháp đề xuất
của chúng tôi có thể đạt được sự cân bằng tốt nhất giữa độ chính xác, thời gian chạy,
và mức sử dụng bộ nhớ, trong số tất cả các phương pháp này. Laplacian score, người thực hiện
tốt thứ hai, có hiệu suất tốt về thời gian chạy và bộ nhớ,
trong khi nó không thể hoạt động tốt về độ chính xác. Mặt khác, CAE có
hiệu suất thỏa đáng về độ chính xác. Tuy nhiên, nó không nằm trong số hai người thực hiện tốt nhất
về tài nguyên tính toán cho bất kỳ giá trị nào của K. Cuối cùng,
FCAE và AEFS không thể đạt được hiệu suất tốt so với các
phương pháp khác. Một phiên bản chi tiết hơn của Hình 5a có sẵn trong Hình 12 trong Phụ lục
A.1.

4https://docs.python.org/2/library/resource.html

--- TRANG 16 ---
16 Zahra Atashgahi và cộng sự.

Điểm số 2. Ngoài điểm số dựa trên xếp hạng, chúng tôi tính một điểm số khác để
xem xét tất cả các phương pháp, ngay cả những phương pháp xếp hạng thấp hơn. Với mục đích này, trên mỗi
tập dữ liệu và giá trị của K, chúng tôi chuẩn hóa mỗi metric hiệu suất giữa 0 và 1,
sử dụng các giá trị của người thực hiện tốt nhất và tệ nhất trên mỗi metric. Giá trị
1 trong điểm số độ chính xác có nghĩa là độ chính xác cao nhất. Tuy nhiên, đối với
bộ nhớ và thời gian chạy, giá trị 1 có nghĩa là yêu cầu bộ nhớ ít nhất và
thời gian chạy ít nhất, tương ứng. Sau khi chuẩn hóa các metrics, chúng tôi tích lũy
các giá trị chuẩn hóa cho mỗi phương pháp và trên tất cả tập dữ liệu. Kết quả được mô tả
trong Hình 5b. Như có thể thấy trong biểu đồ này, QuickSelection (chúng tôi xem xét
kết quả của QuickSelection 100) vượt trội hơn các phương pháp khác với khoảng cách lớn.
CAE có hiệu suất gần với QuickSelection về cả hai metrics độ chính xác,
trong khi nó có hiệu suất kém về bộ nhớ và thời gian chạy. Ngược lại,
Lap_score hiệu quả về mặt tính toán trong khi có điểm số độ chính xác thấp nhất. Tóm lại,
có thể quan sát thấy trong Hình 5b, rằng QuickSelection đạt được sự cân bằng tốt nhất
của bốn mục tiêu trong số các phương pháp được xem xét.

Tiêu thụ Năng lượng. Phân tích tiếp theo chúng tôi thực hiện liên quan đến tiêu thụ năng lượng
của mỗi phương pháp. Chúng tôi ước tính tiêu thụ năng lượng của mỗi phương pháp
sử dụng thời gian chạy của thuật toán tương ứng cho mỗi tập dữ liệu và giá trị
của K. Chúng tôi giả định rằng mỗi phương pháp sử dụng công suất tối đa của tài nguyên
tính toán tương ứng trong thời gian chạy của nó. Do đó, chúng tôi rút ra mức tiêu thụ điện
của mỗi phương pháp, sử dụng thời gian chạy và mức tiêu thụ điện tối đa
của CPU và/hoặc GPU, có thể được tìm thấy trong thông số kỹ thuật của
mô hình CPU hoặc GPU tương ứng. Như được hiển thị trong Hình 13 trong Phụ lục A.2,
lựa chọn đặc trưng Laplacian score cần ít năng lượng nhất trong số
các phương pháp trên tất cả tập dữ liệu ngoại trừ tập dữ liệu MNIST. QuickSelection 10 là người thực hiện tốt nhất
trên MNIST về tiêu thụ năng lượng. Laplacian score và MCFS
nhạy cảm với số lượng mẫu. Chúng không thể hoạt động tốt trên MNIST, dù về
độ chính xác hay hiệu quả. Mức sử dụng bộ nhớ tối đa trong quá trình lựa chọn đặc trưng cho Laplacian score và MCFS trên MNIST là 56 GB và 85 GB, tương ứng.
Do đó, chúng không phải là lựa chọn tốt trong trường hợp có số lượng mẫu lớn.
QuickSelection là người thực hiện tốt thứ hai về tiêu thụ năng lượng, và
cũng là người thực hiện tốt nhất trong số các phương pháp dựa trên autoencoder. QuickSelection không nhạy cảm với số lượng mẫu hoặc số lượng chiều.

Hiệu quả vs Độ chính xác. Để nghiên cứu sự cân bằng giữa độ chính xác
và hiệu quả tài nguyên, chúng tôi thực hiện một phân tích sâu khác. Trong phân tích này,
chúng tôi vẽ sự cân bằng giữa độ chính xác (bao gồm, độ chính xác phân loại và phân cụm)
và yêu cầu tài nguyên (bao gồm, bộ nhớ và tiêu thụ năng lượng).
Kết quả được hiển thị trong Hình 6 và 7 tương ứng với sự cân bằng năng lượng-độ chính xác
và bộ nhớ-độ chính xác, tương ứng. Mỗi điểm trong những biểu đồ này đề cập đến kết quả
của một tổ hợp cụ thể giữa một phương pháp cụ thể và tập dữ liệu khi
chọn 50 đặc trưng (ngoại trừ Madelon, mà chúng tôi chọn 20 đặc trưng). Như có thể
quan sát thấy trong những biểu đồ này, QuickSelection, MCFS, và Lap_score thường có
sự cân bằng tốt giữa các metrics được xem xét. Một sự cân bằng tốt giữa một cặp
metrics là tối đa hóa độ chính xác (độ chính xác phân loại hoặc phân cụm) trong khi
tối thiểu hóa chi phí tính toán (tiêu thụ điện hoặc yêu cầu bộ nhớ).
Tuy nhiên, khi số lượng mẫu tăng (trên tập dữ liệu MNIST), cả
MCFS và Lap_score đều thất bại trong việc duy trì chi phí tính toán thấp và độ chính xác cao.
Do đó, khi kích thước tập dữ liệu tăng, hai phương pháp này không phải là lựa chọn
tối ưu. Trong số các phương pháp dựa trên autoencoder, trong hầu hết các trường hợp QuickSelection 10

--- TRANG 17 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 17

20 30 40 50 60
Độ chính xác phân cụm (%)104
103
102
101
100101102103Tiêu thụ điện (Kwh)
20 30 40 50 60 70 80 90 100
Độ chính xác phân loại (%)104
103
102
101
100101102103Tiêu thụ điện (Kwh)Coil20
Isolet
HAR
Madelon
MNIST
SMK
GLA
PCMAC
FCAE
CAE(GPU)
AEFS(GPU)
MCFS
Lap_score
QS_10
QS_100

Hình 6: Ước tính tiêu thụ điện (Kwh) vs. độ chính xác (%) khi chọn 50
đặc trưng (ngoại trừ Madelon mà chúng tôi chọn 20 đặc trưng). Mỗi điểm đề cập đến
kết quả của một tập dữ liệu duy nhất (được chỉ định bởi màu sắc) và phương pháp (được chỉ định bởi markers)
nơi trục x và y hiển thị độ chính xác và ước tính tiêu thụ điện,
tương ứng.

20 30 40 50 60
Độ chính xác phân cụm (%)02468Yêu cầu bộ nhớ tối đa (Kb)1e7
20 30 40 50 60 70 80 90 100
Độ chính xác phân loại (%)02468Yêu cầu bộ nhớ tối đa (Kb)1e7
20 30 40 50 60
Độ chính xác phân cụm (%)0.00.51.01.52.02.53.0Yêu cầu bộ nhớ tối đa (Kb)1e6
20 30 40 50 60 70 80 90 100
Độ chính xác phân loại (%)0.00.51.01.52.02.53.0Yêu cầu bộ nhớ tối đa (Kb)1e6
Coil20
Isolet
HAR
Madelon
MNIST
SMK
GLA
PCMAC
FCAE
CAE(GPU)
AEFS(GPU)
MCFS
Lap_score
QS

Hình 7: Yêu cầu bộ nhớ tối đa (Kb) vs. độ chính xác (%) khi chọn 50
đặc trưng (ngoại trừ Madelon mà chúng tôi chọn 20 đặc trưng). Mỗi điểm đề cập đến
kết quả của một tập dữ liệu duy nhất (được chỉ định bởi màu sắc) và phương pháp (được chỉ định bởi markers)
nơi trục x và y hiển thị độ chính xác và yêu cầu bộ nhớ tối đa,
tương ứng. Do yêu cầu bộ nhớ cao của MCFS và Lap_score trên
tập dữ liệu MNIST làm cho việc so sánh các kết quả khác khó khăn (biểu đồ trên),
chúng tôi phóng to phần này trong biểu đồ dưới.

và QuickSelection 100 nằm trong số các điểm tối ưu Pareto. Một ưu điểm
đáng kể khác của phương pháp đề xuất của chúng tôi là nó đưa ra xếp hạng của các đặc trưng làm
đầu ra. Do đó, không giống như MCFS hoặc CAE cần giá trị của K làm
đầu vào của chúng, QuickSelection không phụ thuộc vào K và chỉ cần một lần huấn luyện duy nhất
của mô hình sparse DAE cho bất kỳ giá trị nào của K. Do đó, chi phí tính toán
của QuickSelection là như nhau cho tất cả giá trị của K, và chỉ cần một lần chạy duy nhất của
thuật toán này để có được tầm quan trọng phân cấp của các đặc trưng.

--- TRANG 18 ---
18 Zahra Atashgahi và cộng sự.

0 5000 10000 15000 20000 25000 30000 35000 40000
# of features02500500075001000012500150001750020000thời gian chạy (s)
QS_10 (K=All, n=1000, CPU)
QS_100 (K=All, n=1000, CPU)
QS_100 (K=All, n=10000, CPU)
CAE (K=100, n=150, GPU)
CAE (K=300, n=450, GPU)
CAE (K=100, n=1000, GPU)
CAE (K=100, n=10000, GPU)
CAE (K=100, n=150, CPU)
AEFS (K=All, n=300, GPU)
AEFS (K=All, n=1000, GPU)
AEFS (K=All, n=10000, GPU)
FCAE (K=All, n=1000, CPU)
FCAE (K=All, n=10000, CPU)

Hình 8: So sánh thời gian chạy trên một tập dữ liệu được tạo nhân tạo. Các đặc trưng
được tạo bằng phân phối chuẩn và số lượng mẫu cho
mỗi trường hợp là 5000.

5.2 So sánh Thời gian Chạy trên một Tập dữ liệu được tạo Nhân tạo

Trong phần này, chúng tôi thực hiện so sánh thời gian chạy của các phương pháp lựa chọn đặc trưng
dựa trên autoencoder trên một tập dữ liệu được tạo nhân tạo. Vì trên các tập dữ liệu chuẩn
cả số lượng đặc trưng và mẫu đều khác nhau, không dễ dàng
so sánh rõ ràng hiệu quả của các phương pháp. Thí nghiệm này
nhằm mục đích so sánh thời gian huấn luyện wall-clock thực tế của các mô hình trong một môi trường được kiểm soát
về số lượng đặc trưng đầu vào và neurons ẩn. Ngoài ra,
trong Phụ lục E, chúng tôi đã tiến hành một thí nghiệm khác liên quan đến đánh giá
các phương pháp trên một tập dữ liệu nhân tạo rất lớn, về cả tài nguyên tính toán
và độ chính xác.

Trong thí nghiệm này, chúng tôi nhằm mục đích so sánh tốc độ của QuickSelection so với
các phương pháp lựa chọn đặc trưng dựa trên autoencoder khác cho số lượng khác nhau của đặc trưng
đầu vào. Chúng tôi chạy tất cả chúng trên một tập dữ liệu được tạo nhân tạo với số lượng
đặc trưng khác nhau và 5000 mẫu, trong 100 epochs huấn luyện (10 epochs cho
QuickSelection 10). Các đặc trưng của tập dữ liệu này được tạo bằng phân phối chuẩn.
Ngoài ra, chúng tôi nhằm mục đích so sánh thời gian chạy của các cấu trúc khác nhau
cho các thuật toán này. Thông số kỹ thuật của cấu trúc mạng cho
mỗi phương pháp, tài nguyên tính toán được sử dụng để lựa chọn đặc trưng, và
kết quả tương ứng có thể được thấy trong Hình 8.

Đối với CAE, chúng tôi xem xét hai giá trị khác nhau của K. Cấu trúc của CAE phụ thuộc
vào giá trị này. CAE có hai lớp ẩn bao gồm một concrete selector và một
decoder có K và 1:5K neurons, tương ứng. Do đó, bằng cách tăng số lượng
đặc trưng được chọn, thời gian chạy của mô hình cũng sẽ tăng. Ngoài ra,
chúng tôi xem xét các trường hợp của CAE với 1000 và 10000 neurons ẩn trong lớp
decoder (được thay đổi thủ công trong mã) để có thể so sánh nó với
các mô hình khác. Chúng tôi cũng đo thời gian chạy của việc thực hiện lựa chọn đặc trưng
với CAE chỉ sử dụng một lõi CPU duy nhất. Có thể thấy từ Hình 8 rằng thời gian
chạy của nó là đáng kể cao. Cấu trúc chung của AEFS, QuickSelection,
và FCAE tương tự về số lượng lớp ẩn. Chúng là các autoencoder cơ bản

--- TRANG 19 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 19

với một lớp ẩn duy nhất. Đối với AEFS, chúng tôi xem xét ba cấu trúc
với số lượng neurons ẩn khác nhau, bao gồm 300, 1000, và 10000. Cuối cùng,
đối với QuickSelection và FCAE, chúng tôi xem xét hai giá trị khác nhau cho số lượng
neurons ẩn, bao gồm 1000 và 10000.

Có thể quan sát thấy rằng thời gian chạy của AEFS với 1000 và 10000 neurons ẩn
sử dụng GPU, lớn hơn nhiều so với thời gian chạy của QuickSelection 100
với số lượng neurons ẩn tương tự chỉ sử dụng một lõi CPU duy nhất, tương ứng.
Mô hình tương tự cũng có thể nhìn thấy trong trường hợp của CAE với 1000 và 10000 neurons ẩn.
Mô hình này cũng lặp lại trong trường hợp của FCAE với 10000 neurons ẩn.
Thời gian chạy của FCAE với 1000 neurons ẩn gần tương tự
với QuickSelection 100. Tuy nhiên, sự khác biệt giữa hai phương pháp này đáng kể hơn
khi chúng ta tăng số lượng neurons ẩn lên 10000. Điều này chủ yếu do
thực tế rằng sự khác biệt giữa số lượng tham số của
QuickSelection và các phương pháp khác trở nên cao hơn nhiều cho các giá trị lớn của K.
Bên cạnh đó, những quan sát này mô tả rằng thời gian chạy của QuickSelection
không thay đổi đáng kể khi tăng số lượng neurons ẩn.

Như chúng tôi cũng đã đề cập trước đó, QuickSelection đưa ra xếp hạng của các đặc trưng làm
đầu ra. Do đó, không giống như CAE phải được chạy riêng biệt cho
các giá trị khác nhau của K, QuickSelection không bị ảnh hưởng bởi việc lựa chọn K vì
nó tính toán tầm quan trọng của tất cả đặc trưng cùng một lúc và sau khi hoàn thành
huấn luyện. Tóm lại, QuickSelection 10 có thời gian chạy ít nhất trong số các
phương pháp dựa trên autoencoder khác trong khi độc lập với giá trị của K. Ngoài ra,
không giống như các phương pháp khác, thời gian chạy của QuickSelection không nhạy cảm với
số lượng neurons ẩn vì số lượng tham số thấp ngay cả đối với một
lớp ẩn rất lớn.

5.3 Phân tích Sức mạnh Neuron

Trong phần này, chúng tôi thảo luận về tính hợp lệ của sức mạnh neurons như một thước đo tầm quan trọng
đặc trưng. Chúng tôi quan sát sự tiến hóa của mạng trong quá trình huấn luyện để
phân tích cách sức mạnh neuron của các neurons quan trọng và không quan trọng thay đổi
trong quá trình huấn luyện.

Chúng tôi lập luận rằng các đặc trưng quan trọng nhất dẫn đến độ chính xác cao nhất của
lựa chọn đặc trưng là các đặc trưng tương ứng với neurons có sức mạnh cao nhất.
Trong mạng thần kinh, độ lớn trọng số là một metric cho thấy tầm quan trọng của
mỗi kết nối [ 29]. Điều này bắt nguồn từ thực tế rằng các trọng số có độ lớn nhỏ
có tác động nhỏ đến hiệu suất của mô hình. Khi bắt đầu huấn luyện, chúng ta
khởi tạo tất cả kết nối thành một giá trị ngẫu nhiên nhỏ. Do đó, tất cả các neurons có
gần như cùng sức mạnh/tầm quan trọng. Khi quá trình huấn luyện tiến triển, một số kết nối
phát triển thành giá trị lớn hơn trong khi một số khác bị cắt tỉa khỏi mạng trong
quá trình loại bỏ và tái sinh kết nối động của quy trình huấn luyện SET. Sự
tăng trưởng của các trọng số kết nối ổn định chứng tỏ tầm quan trọng của chúng trong
hiệu suất của mạng. Kết quả là, các neurons được kết nối với những
trọng số quan trọng này chứa thông tin quan trọng. Ngược lại, độ lớn của các trọng số
được kết nối với các neurons không quan trọng dần dần giảm
cho đến khi chúng bị loại bỏ khỏi mạng. Tóm lại, các neurons quan trọng nhận được các kết nối với
độ lớn lớn hơn. Kết quả là, sức mạnh neuron, là tổng của độ lớn
của các trọng số được kết nối với một neuron, có thể là thước đo tầm quan trọng của một neuron
đầu vào và đặc trưng tương ứng của nó.

Để hỗ trợ cho tuyên bố của chúng tôi, chúng tôi quan sát sự tiến hóa của sức mạnh neurons trên
tập dữ liệu Madelon. Lựa chọn này được thực hiện do sự phân biệt của các đặc trưng mang thông tin và
không mang thông tin trong tập dữ liệu Madelon. Như được mô tả trước đó, tập dữ liệu này
có 20 đặc trưng mang thông tin, và phần còn lại của các đặc trưng là nhiễu không mang thông tin.
Chúng tôi xem xét 20 đặc trưng mang thông tin nhất và không mang thông tin phát hiện bởi QS10
và QS100, và theo dõi sức mạnh của chúng trong quá trình huấn luyện (như quan sát trong Hình 3,
độ chính xác tối đa đạt được khi sử dụng 20 đặc trưng mang thông tin nhất, trong khi
độ chính xác ít nhất đạt được khi sử dụng các đặc trưng ít quan trọng nhất). Các đặc trưng
được chọn bởi QS10 cũng được theo dõi sau khi thuật toán kết thúc (epoch
10) cho đến epoch 100, để so sánh chất lượng của các đặc trưng được chọn bởi
QS10 với QS100. Nói cách khác, chúng tôi trích xuất chỉ số của các đặc trưng quan trọng sử dụng
QS10, và tiếp tục huấn luyện mà không thực hiện bất kỳ thay đổi nào trong mạng và
theo dõi cách sức mạnh của các neurons tương ứng với chỉ số được chọn sẽ
tiến hóa sau epoch 10. Kết quả được trình bày trong Hình 9. Tại khởi tạo
(epoch 0), sức mạnh của tất cả các neurons này gần như tương tự và dưới 5. Khi
huấn luyện bắt đầu, sức mạnh của các neurons quan trọng tăng, trong khi sức mạnh của
các neurons không quan trọng không thay đổi đáng kể. Như có thể thấy trong Hình 9,
một số đặc trưng quan trọng được chọn bởi QS10 không nằm trong số những đặc trưng của QS100;
điều này có thể giải thích sự khác biệt trong hiệu suất của hai phương pháp này trong Bảng
2 và 3. Tuy nhiên, QS10 có thể phát hiện một số lượng lớn các đặc trưng được tìm thấy
bởi QS100; những đặc trưng này nằm trong số những đặc trưng quan trọng nhất trong số
20 đặc trưng được chọn cuối cùng. Do đó, chúng ta có thể kết luận rằng hầu hết các đặc trưng quan trọng
có thể phát hiện được bởi QuickSelection, ngay cả trong vài epochs đầu tiên của thuật toán.

6 Kết luận

Trong bài báo này, một phương pháp mới (QuickSelection) cho lựa chọn đặc trưng không giám sát
tiết kiệm năng lượng đã được đề xuất. Nó giới thiệu sức mạnh neuron trong mạng thần kinh thưa
như một thước đo tầm quan trọng đặc trưng. Bên cạnh đó, nó đề xuất sparse DAE
để mô hình hóa chính xác phân phối dữ liệu và xếp hạng tất cả đặc trưng đồng thời
dựa trên tầm quan trọng của chúng. Bằng cách sử dụng các lớp thưa thay vì các lớp dày đặc từ
đầu, số lượng tham số giảm đáng kể. Kết quả là, QuickSelection
yêu cầu ít bộ nhớ và tài nguyên tính toán hơn nhiều so với mô hình dày đặc tương đương
của nó và các đối thủ cạnh tranh. Ví dụ, trên các tập dữ liệu chiều thấp, bao gồm
Coil20, Isolet, HAR, và Madelon, và cho tất cả giá trị của K, QuickSelection 100 chạy trên một lõi CPU ít nhất nhanh hơn 4 lần so với đối thủ cạnh tranh trực tiếp của nó, CAE,
chạy trên GPU, trong khi có hiệu suất gần trong về độ chính xác
phân loại và phân cụm. Chúng tôi chứng minh thực nghiệm rằng QuickSelection đạt được
sự cân bằng tốt nhất giữa độ chính xác phân cụm, độ chính xác phân loại, yêu cầu bộ nhớ tối đa,
và thời gian chạy, trong số các phương pháp khác được xem xét. Bên cạnh đó,
phương pháp đề xuất của chúng tôi yêu cầu ít năng lượng nhất trong số các phương pháp dựa trên autoencoder
được xem xét.

Nhược điểm chính của phương pháp đề xuất là thiếu một
triển khai song song. Thời gian chạy của QuickSelection có thể được giảm thêm bởi
một triển khai tận dụng CPU đa lõi hoặc GPU. Chúng tôi tin rằng
nghiên cứu tương lai thú vị sẽ là nghiên cứu tác động của huấn luyện thưa

--- TRANG 20 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 21

/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000038/uni00000051/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni0000002c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000038/uni00000051/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni0000002c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013

Hình 9: Sức mạnh của 20 đặc trưng mang thông tin nhất và không mang thông tin của tập dữ liệu Madelon,
được chọn bởi QS10 và QS100. Mỗi đường trong các biểu đồ tương ứng với
giá trị sức mạnh của một đặc trưng được chọn bởi QS10/QS100 trong quá trình huấn luyện. Các đặc trưng
được chọn bởi QS10 đã được quan sát cho đến epoch 100 để so sánh chất lượng
của những đặc trưng này với QS100.

và sức mạnh neuron trong các loại autoencoders khác cho lựa chọn đặc trưng, ví dụ: CAE.
Tuy nhiên, bài báo này chỉ mới bắt đầu khám phá một trong những đặc tính quan trọng nhất
của QuickSelection, tức là khả năng mở rộng, và chúng tôi có ý định khám phá thêm
tiềm năng đầy đủ của nó trên các tập dữ liệu có hàng triệu đặc trưng. Bên cạnh đó, bài báo này đã chỉ ra
rằng chúng ta có thể thực hiện lựa chọn đặc trưng sử dụng mạng thần kinh một cách hiệu quả về
chi phí tính toán và yêu cầu bộ nhớ. Điều này có thể mở đường cho việc giảm
chi phí tính toán ngày càng tăng của các mô hình học sâu áp đặt lên
trung tâm dữ liệu. Kết quả là, điều này sẽ không chỉ tiết kiệm chi phí năng lượng của việc xử lý dữ liệu
chiều cao mà còn sẽ giảm bớt những thách thức của tiêu thụ năng lượng cao
áp đặt lên môi trường.

Lời cảm ơn Nghiên cứu này đã được tài trợ một phần bởi dự án NWO EDIC.

Tài liệu tham khảo
1.Amirali Aghazadeh, Ryan Spring, Daniel Lejeune, Gautam Dasarathy, An-
shumali Shrivastava, và cộng sự. Mission: Ultra large-scale feature selection using
count-sketches. Trong International Conference on Machine Learning , trang 80–88,
2018.
2.Jun Chin Ang, Andri Mirzal, Habibollah Haron, và Haza Nuzly Abdull
Hamed. Supervised, unsupervised, and semi-supervised feature selection: a

--- TRANG 21 ---
22 Zahra Atashgahi và cộng sự.

review on gene selection. IEEE/ACM transactions on computational biology
and bioinformatics , 13(5):971–989, 2015.
3.Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, và Jorge Luis
Reyes-Ortiz. A public domain dataset for human activity recognition using
smartphones. Trong Esann, 2013.
4.Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. Trong
Proceedings of ICML workshop on unsupervised and transfer learning , trang
37–49, 2012.
5.Muhammed Fatih Balın, Abubakar Abid, và James Zou. Concrete autoen-
coders: Differentiable feature selection and reconstruction. Trong International
Conference on Machine Learning , trang 444–453, 2019.
6.Alain Barrat, Marc Barthelemy, Romualdo Pastor-Satorras, và Alessandro
Vespignani. The architecture of complex weighted networks. Proceedings of
the national academy of sciences , 101(11):3747–3752, 2004.
7.GuillaumeBellec,DavidKappel,WolfgangMaass,vàRobertLegenstein. Deep
rewiring: Training very sparse deep networks. arXiv preprint arXiv:1711.05136 ,
2017.
8.Yoshua Bengio, Aaron Courville, và Pascal Vincent. Representation learning:
A review and new perspectives. IEEE transactions on pattern analysis and
machine intelligence , 35(8):1798–1828, 2013.
9.Verónica Bolón-Canedo, Noelia Sánchez-Maroño, và Amparo Alonso-Betanzos.
Feature selection for high-dimensional data . Springer, 2015.
10.David D. Bourgin, Joshua C. Peterson, Daniel Reichman, Stuart J. Rus-
sell, và Thomas L. Griffiths. Cognitive model priors for predicting hu-
man decisions. Trong Kamalika Chaudhuri và Ruslan Salakhutdinov, edi-
tors,Proceedings of the 36th International Conference on Machine Learn-
ing, volume 97 of Proceedings of Machine Learning Research , trang 5133–
5141, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/peterson19a.html .
11.Deng Cai, Chiyuan Zhang, và Xiaofei He. Unsupervised feature selection for
multi-cluster data. Trong Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining , trang 333–342. ACM,
2010.
12.Girish Chandrashekar và Ferat Sahin. A survey on feature selection methods.
Computers & Electrical Engineering , 40(1):16–28, 2014.
13. François Chollet và cộng sự. Keras. https://keras.io , 2015.
14.Tim Dettmers và Luke Zettlemoyer. Sparse networks from scratch: Faster
training without losing performance. arXiv preprint arXiv:1907.04840 , 2019.
15.Guillaume Doquet và Michèle Sebag. Agnostic feature selection. Trong Joint Eu-
ropean Conference on Machine Learning and Knowledge Discovery in Databases ,
trang 343–358. Springer, 2019.
16.Jennifer G Dy và Carla E Brodley. Feature selection for unsupervised learning.
Journal of machine learning research , 5(Aug):845–889, 2004.
17.Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, và Erich
Elsen. Rigging the lottery: Making all tickets winners. arXiv preprint
arXiv:1911.11134 , 2019.
18.Mark Fanty và Ronald Cole. Spoken letter recognition. Trong Advances in Neural
Information Processing Systems , trang 220–226, 1991.

--- TRANG 22 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 23

19.Ahmed K Farahat, Ali Ghodsi, và Mohamed S Kamel. Efficient greedy feature
selection for unsupervised learning. Knowledge and information systems , 35
(2):285–310, 2013.
20.Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding
sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 , 2018.
21.Pierre Geurts, Damien Ernst, và Louis Wehenkel. Extremely randomized
trees.Machine learning , 63(1):3–42, 2006.
22.AI High-Level Expert Group. Assessment list for trustworthy artificial intelli-
gence (ALTAI) for self-assessment, 2020.
23.Isabelle Guyon, Steve Gunn, Masoud Nikravesh, và Lofti A Zadeh. Feature
extraction: foundations and applications , volume 207. Springer, 2008.
24.Kai Han, Yunhe Wang, Chao Zhang, Chao Li, và Chao Xu. Autoencoder
inspired unsupervised feature selection. Trong 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , trang 2941–2945. IEEE,
2018.
25.Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and
connections for efficient neural network. Trong Advances in neural information
processing systems , trang 1135–1143, 2015.
26.Babak Hassibi và David G Stork. Second order derivatives for network
pruning: Optimal brain surgeon. Trong Advances in neural information processing
systems, trang 164–171, 1993.
27.Xiaofei He, Deng Cai, và Partha Niyogi. Laplacian score for feature selection.
TrongAdvances in neural information processing systems , trang 507–514, 2006.
28.Eric Jones, Travis Oliphant, và Pearu Peterson. Scipy: Open source scientific
tools for python. 2001.
29.Taskin Kavzoglu và Paul M Mather. Assessing artificial neural network prun-
ing algorithms. Trong Proceedings of the 24th Annual Conference and Exhibition
of the Remote Sensing Society , trang 9–11, 1998.
30.RonKohaviváGeorgeHJohn. Wrappersforfeaturesubsetselection. Artificial
intelligence , 97(1-2):273–324, 1997.
31.Thomas Navin Lal, Olivier Chapelle, Jason Weston, và André Elisseeff.
Embedded methods. Trong Feature extraction , trang 137–165. Springer, 2006.
32.Ken Lang. Newsweeder: Learning to filter netnews. Trong Machine Learning
Proceedings 1995 , trang 331–339. Elsevier, 1995.
33.Cosmin Lazar, Jonatan Taminau, Stijn Meganck, David Steenhoff, Alain Co-
letta, Colin Molter, Virginie de Schaetzen, Robin Duque, Hugues Bersini, và
Ann Nowe. A survey on filter techniques for feature selection in gene expression
microarray analysis. IEEE/ACM Transactions on Computational Biology and
Bioinformatics , 9(4):1106–1119, 2012.
34.Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
com/exdb/mnist/ , 1998.
35.Yann LeCun, John S Denker, và Sara A Solla. Optimal brain damage. Trong
Advances in neural information processing systems , trang 598–605, 1990.
36.Namhoon Lee, Thalaiyasingam Ajanthan, và Philip HS Torr. Snip: Single-
shot network pruning based on connection sensitivity. arXiv preprint
arXiv:1810.02340 , 2018.
37.Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino,
Jiliang Tang, và Huan Liu. Feature selection: A data perspective. ACM
Computing Surveys (CSUR) , 50(6):94, 2018.

--- TRANG 23 ---
24 Zahra Atashgahi và cộng sự.

38.Yifeng Li, Chih-Yu Chen, và Wyeth W Wasserman. Deep feature selec-
tion: theory and application to identify enhancers and promoters. Journal of
Computational Biology , 23(5):322–336, 2016.
39.Andy Liaw, Matthew Wiener, và cộng sự. Classification and regression by random-
forest.R news, 2(3):18–22, 2002.
40.Huan Liu và Hiroshi Motoda. Feature extraction, construction and selection:
A data mining perspective , volume 453. Springer Science & Business Media,
1998.
41.Shiwei Liu, Tim van der Lee, Anil Yaman, Zahra Atashgahi, Davide Ferrar,
Ghada Sokar, Mykola Pechenizkiy, và Decebal C Mocanu. Topological insights
into sparse neural networks. Trong Proceedings of the European Conference on
Machine Learning and Principles and Practice of Knowledge Discovery in
Databases (ECML PKDD) 2020. , 2020.
42.Yang Lu, Yingying Fan, Jinchi Lv, và William Stafford Noble. Deeppink:
reproducible feature selection in deep neural networks. Trong Advances in Neural
Information Processing Systems , trang 8676–8686, 2018.
43.Jianyu Miao và Lingfeng Niu. A survey on feature selection. Procedia
Computer Science , 91:919–926, 2016.
44.Decebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine
Gibescu, và Antonio Liotta. A topological insight into restricted boltzmann
machines. Machine Learning , 104(2-3):243–270, 2016.
45. Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen,
Madeleine Gibescu, và Antonio Liotta. Scalable training of artificial neural
networks with adaptive sparse connectivity inspired by network science. Nature
communications , 9(1):2383, 2018.
46.Hesham Mostafa và Xin Wang. Parameter efficient training of deep convo-
lutional neural networks by dynamic sparse reparameterization. Trong Kamalika
Chaudhuri và Ruslan Salakhutdinov, editors, Proceedings of the 36th Interna-
tional Conference on Machine Learning , volume 97 of Proceedings of Machine
Learning Research , trang 4646–4655, Long Beach, California, USA, 09–15 Jun
2019. PMLR. URL http://proceedings.mlr.press/v97/mostafa19a.html .
47.Sameer A Nene, Shree K Nayar, Hiroshi Murase, và cộng sự. Columbia object image
library (coil-20). 1996.
48.Razieh Sheikhpour, Mehdi Agha Sarram, Sajjad Gharaghani, và Mohammad
Ali Zare Chahooki. A survey on semi-supervised feature selection methods.
Pattern Recognition , 64:141–158, 2017.
49.Dinesh Singh và Makoto Yamada. Fsnet: Feature selection network on high-
dimensional biological data. arXiv preprint arXiv:2001.08322 , 2020.
50.Avrum Spira, Jennifer E Beane, Vishal Shah, Katrina Steiling, Gang Liu, Frank
Schembri, Sean Gilman, Yves-Martine Dumas, Paul Calner, Paola Sebastiani,
và cộng sự. Airway epithelial gene expression in the diagnostic evaluation of smokers
with suspect lung cancer. Nature medicine , 13(3):361–366, 2007.
51.Lixin Sun, Ai-Min Hui, Qin Su, Alexander Vortmeyer, Yuri Kotliarov, Sandra
Pastorino, Antonino Passaniti, Jayant Menon, Jennifer Walling, Rolando Bailey,
và cộng sự. Neuronal and glioma-derived stem cell factor induces angiogenesis within
the brain. Cancer cell , 9(4):287–300, 2006.
52.Mingkui Tan, Ivor W Tsang, và Li Wang. Towards ultrahigh dimensional
feature selection for big data. Journal of Machine Learning Research , 2014.

--- TRANG 24 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 25

53.LaurensVanDerMaaten,EricPostma,vàJaapVandenHerik. Dimensionality
reduction: a comparative. J Mach Learn Res , 10(66-71):13, 2009.
54.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, và Pierre-Antoine Manzagol.
Extracting and composing robust features with denoising autoencoders. Trong
Proceedings of the 25th international conference on Machine learning , trang
1096–1103. ACM, 2008.
55.Svante Wold, Kim Esbensen, và Paul Geladi. Principal component analysis.
Chemometrics and intelligent laboratory systems , 2(1-3):37–52, 1987.
56.JunYang,WenjingXiao,ChunJiang,MShamimHossain,GhulamMuhammad,
và Syed Umar Amin. Ai-powered green cloud and data center. IEEE Access ,
7:4195–4203, 2018.
57.Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, và Xiaofang Zhou. L2,
1-norm regularized discriminative feature selection for unsupervised. Trong Twenty-
Second International Joint Conference on Artificial Intelligence , 2011.
58.Zheng Zhao và Huan Liu. Semi-supervised feature selection via spectral
analysis. Trong Proceedings of the 2007 SIAM international conference on data
mining, trang 641–646. SIAM, 2007.
59.Hangyu Zhu và Yaochu Jin. Multi-objective evolutionary federated learning.
IEEE transactions on neural networks and learning systems , 2019.

--- TRANG 25 ---
26 Zahra Atashgahi và cộng sự.

Phụ lục

A Đánh giá Hiệu suất
Trong phụ lục này, chúng tôi so sánh tất cả các phương pháp từ các khía cạnh khác nhau bao gồm độ chính xác, sử dụng bộ nhớ,
thời gian chạy, tiêu thụ năng lượng, và số lượng tham số. Chúng tôi thực hiện các
thí nghiệm khác nhau để có cái nhìn sâu sắc về hiệu suất của QuickSelection.

A.1 Thảo luận: Sự cân bằng giữa Độ chính xác và Hiệu quả Tính toán
Trong phần này, chúng tôi so sánh hiệu suất của tất cả các phương pháp chi tiết hơn. Chúng tôi chạy lựa chọn
đặc trưng cho các giá trị khác nhau của K trên mỗi tập dữ liệu và sau đó đo hiệu suất.

Như được hiển thị trong Hình 10, chúng tôi so sánh độ chính xác phân cụm, độ chính xác phân loại, và thời gian
chạy giữa các phương pháp cho các giá trị khác nhau của K. So sánh yêu cầu bộ nhớ tối đa
(RAM) cũng được mô tả trong Hình 11. Đối với tất cả các phương pháp ngoại trừ CAE và AEFS, chúng tôi
chạy các thí nghiệm trên một lõi CPU duy nhất. Vì việc triển khai CAE và AEFS được
tối ưu hóa cho GPU, chúng tôi đo thời gian chạy của các phương pháp này sử dụng GPU. Tuy nhiên,
chúng tôi cũng xem xét thời gian chạy của CAE sử dụng một lõi CPU duy nhất. Cần lưu ý rằng
vì Laplacian score, AEFS, FCAE, và QuickSelection đưa ra xếp hạng của các đặc trưng làm
đầu ra của quá trình lựa chọn đặc trưng, chúng tôi cần chạy chúng chỉ một lần cho tất cả giá trị của K.
Tuy nhiên, MCFS và CAE cần giá trị K làm đầu vào của thuật toán của chúng. Vì vậy, thời gian
chạy phụ thuộc vào giá trị của K. Trong việc triển khai AEFS, K được sử dụng để đặt số
lượng giá trị ẩn. Tuy nhiên, đó không phải là yêu cầu của thuật toán.

Chúng tôi tóm tắt kết quả của các biểu đồ nói trên trong Hình 12; chúng tôi so sánh các phương pháp
sử dụng điểm số 1, được giới thiệu trong Phần 5.1. Điểm số này được tính dựa trên
xếp hạng của các phương pháp trong độ chính xác phân cụm, độ chính xác phân loại, thời gian chạy, và bộ nhớ. Như
được giải thích trong Phần 5.1, chúng tôi đưa ra điểm số một cho mỗi phương pháp là người thực hiện tốt nhất hoặc tốt thứ hai trong mỗi metrics được xem xét. Sau đó, chúng tôi tính tổng của tất cả những điểm số này
trên tất cả tập dữ liệu và trên tất cả giá trị của K; điểm số cuối cùng cho mỗi phương pháp có thể được thấy trong Hình
12. Cột đầu tiên mô tả kết quả trên các tập dữ liệu chiều thấp với số lượng mẫu thấp, bao gồm Coil20, Isolet, HAR, và Madelon. Cột thứ hai hiển thị kết quả
tương ứng với MNIST. Tương tự, cột thứ ba tương ứng với các tập dữ liệu chiều cao, bao gồm SMK, GLA, và PCMAC. Tổng điểm số trên tất cả các tập dữ liệu này được hiển thị trong
cột thứ 4. Trong Hình 12, có bốn hàng; hàng đầu tiên tương ứng với việc xem xét
QuickSelection 10 và QuickSelection 100 đồng thời, và tổng điểm số của chúng được mô tả
trong hàng thứ hai. Hai hàng cuối tương ứng với việc xem xét mỗi phương pháp này
riêng biệt.

Tuy nhiên, vì hiệu suất của mỗi phương pháp có thể khác nhau trong mỗi ba nhóm
tập dữ liệu, chúng tôi tính một phiên bản chuẩn hóa của điểm số 1, dựa trên số lượng tập dữ liệu
trong mỗi nhóm. Ví dụ, Laplacian score có hiệu suất kém trên MNIST, và mô hình
này sẽ tương tự trên các tập dữ liệu khác có số lượng mẫu lớn. Tuy nhiên, chỉ có
một tập dữ liệu với số lượng mẫu lớn trong thí nghiệm này. Mặt khác, trên
các tập dữ liệu chiều cao với số lượng mẫu thấp, phương pháp này có hiệu suất tốt
về thời gian chạy, và chúng tôi có ba tập dữ liệu với những đặc tính như vậy. Vì vậy, chúng tôi chuẩn hóa
các giá trị của điểm số 1, sao cho thay vì đưa ra điểm số một cho mỗi phương pháp, chúng tôi đưa ra
điểm số một chia cho số lượng tập dữ liệu trong nhóm tương ứng. Kết quả của
điểm số 1 chuẩn hóa được hiển thị trong cột cuối của Hình 12.

A.2 Tiêu thụ Năng lượng
Chúng tôi thực hiện một thí nghiệm khác liên quan đến so sánh tiêu thụ năng lượng giữa tất cả
các phương pháp. Kết quả được trình bày trong Hình 13. Chi tiết hơn về biểu đồ này được đưa ra trong
bài báo trong Phần 5.1.

--- TRANG 26 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 27

[Nội dung của các hình và đồ thị được giữ nguyên do chúng là biểu diễn trực quan]

--- TRANG 27 ---

--- TRANG 28 ---
28 Zahra Atashgahi và cộng sự.

[Tiếp tục nội dung các hình và đồ thị]

--- TRANG 29 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 29

[Tiếp tục nội dung các hình và đồ thị]

A.3 Số lượng Tham số
Trong Hình 14, chúng tôi so sánh số lượng tham số của các phương pháp dựa trên autoencoder. FCAE,
một autoencoder fully connected với 1000 neurons ẩn, có số lượng tham số cao nhất
trên tất cả tập dữ liệu. Mạng đề xuất của chúng tôi, sparse DAE, có số lượng tham số thấp nhất
trong hầu hết các trường hợp. Nó có 1000 neurons ẩn được kết nối thưa với neurons đầu vào và đầu ra.
Số lượng tham số của AEFS và CAE phụ thuộc vào số lượng đặc trưng được chọn. Như cũng
đã đề cập trước đó, cấu trúc của AEFS tương tự như FCAE với sự khác biệt
trong số lượng neurons ẩn. Số lượng neurons ẩn trong việc triển khai
AEFS được đặt thành K.

--- TRANG 30 ---
30 Zahra Atashgahi và cộng sự.

[Tiếp tục nội dung hình]

B Lựa chọn Tham số
Trong phụ lục này, chúng tôi thảo luận về tác động của ba siêu tham số của QuickSelection đối với hiệu suất lựa chọn
đặc trưng.

B.1 Noise Factor
Để phân tích tác động của mức độ nhiễu đối với hành vi QuickSelection, chúng tôi đánh giá mô hình
sparse DAE với các noise factors khác nhau. Để làm điều này, chúng tôi kiểm tra các noise factors khác nhau giữa 0 và 0:8.
Kết quả có thể được quan sát trong Hình 15. Những kết quả này là trung bình của 5 lần chạy cho mỗi trường hợp.
Chúng ta có thể quan sát rằng việc thêm 20% đến 40% nhiễu vào dữ liệu có vẻ tối ưu; nó cải thiện
hiệu suất trên hầu hết các tập dữ liệu cho QuickSelection 10 và QuickSelection 100 so với
mô hình không có nhiễu nào. Chúng tôi chọn noise factor 0:2 cho tất cả các thí nghiệm.

Rõ ràng trong Hình 15, rằng việc đặt noise factor thành một giá trị lớn có thể làm hỏng
dữ liệu đầu vào theo cách mà mạng sẽ không thể mô hình hóa phân phối dữ liệu
một cách chính xác. Ví dụ, trên tập dữ liệu Isolet, độ chính xác phân cụm giảm 10% khi
chúng ta thêm 80% nhiễu vào dữ liệu đầu vào so với mô hình với noise factor 0:2. Ngoài ra,
kết quả kém ổn định hơn khi chúng ta thêm một lượng lớn nhiễu. Trong ví dụ này, chúng ta có thể quan sát

--- TRANG 31 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 31

rằng việc thêm 20% nhiễu vào dữ liệu gốc cải thiện cả độ chính xác phân loại và phân cụm
của QuickSelection 100 khoảng 3%.

Từ hình này, có thể quan sát thấy rằng việc cải thiện do thêm nhiễu, rõ ràng hơn
trong QuickSelection 100 so với QuickSelection 10. Khi chúng ta thêm nhiễu vào dữ liệu, nó cần nhiều thời gian hơn
để học cấu trúc gốc của dữ liệu. Vì vậy, chúng ta cần chạy nó trong nhiều epochs hơn để có được
kết quả phù hợp.

B.2 Siêu tham số SET
Như được giải thích trong bài báo,  và  là các siêu tham số của thuật toán SET kiểm soát
số lượng kết nối cần loại bỏ/thêm cho mỗi thay đổi topology và mức độ thưa,
tương ứng. Mức density tương ứng của mỗi giá trị cho mỗi tập dữ liệu có thể được quan sát
trong Bảng 4.

Để minh họa tác động của các siêu tham số  và , chúng tôi thực hiện grid search trong một
tập hợp giá trị nhỏ trên tất cả tập dữ liệu. Kết quả thu được có thể được tìm thấy trong Bảng 5 và 6.
Khi chúng ta tăng giá trị , số lượng kết nối trong mô hình của chúng tôi tăng, và do đó,
thời gian tính toán sẽ tăng. Vì vậy, chúng tôi thích sử dụng giá trị nhỏ cho tham số này. Ngoài ra,

Bảng 4: Giá trị  và mức density tương ứng của chúng.
Density [%]
COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
2 0.39 0.52 0.39 0.59 0.45 0.20 0.2 0.26
5 0.98 1.30 0.98 1.48 1.13 0.53 0.51 0.65
10 1.95 2.58 1.95 2.95 2.25 1.04 1.02 1.13
13 2.53 3.35 2.53 3.82 2.91 1.35 1.32 1.69
20 3.87 5.10 3.87 5.82 4.45 2.07 2.04 2.6
25 4.87 6.45 4.87 7.37 5.63 2.65 2.55 3.26

--- TRANG 32 ---
32 Zahra Atashgahi và cộng sự.

đối với giá trị lớn của , trong một số trường hợp mô hình không thể hội tụ trong 100 epochs; ví
dụ, trên tập dữ liệu MNIST, chúng ta có thể quan sát rằng với giá trị  là 25, mô hình có hiệu suất
thấp hơn về độ chính xác phân cụm và phân loại.

Có thể quan sát thấy rằng  = 0:2 và  = 13 (như được chọn cho các thí nghiệm thực hiện trong
bài báo) dẫn đến hiệu suất tốt trên tất cả tập dữ liệu. Đối với những giá trị này, QuickSelection
có thể đạt được độ chính xác phân cụm và phân loại cao.

Nhìn chung, mặc dù tìm kiếm cặp tốt nhất của  và  sẽ cải thiện hiệu suất,
QuickSelection không cực kỳ nhạy cảm với những giá trị này. Như có thể thấy trong Bảng 5 và 6, đối với
tất cả giá trị của những siêu tham số này QuickSelection có hiệu suất hợp lý. Ngay cả với
 = 2 dẫn đến một mô hình rất thưa, QuickSelection có hiệu suất tốt, và trong
một số trường hợp tốt hơn một mạng dày đặc hơn.

C Trực quan hóa Đặc trưng được Chọn trên MNIST
Trong Hình 16, chúng tôi trực quan hóa 50 đặc trưng tốt nhất được tìm thấy bởi QuickSelection trên tập dữ liệu MNIST
tại các epochs khác nhau. Những đặc trưng này chủ yếu ở trung tâm của hình ảnh, tương tự như mô hình
của các chữ số MNIST.

Sau đó, chúng tôi trực quan hóa các đặc trưng được chọn cho mỗi lớp riêng biệt. Trong Hình 17, mỗi hình ảnh
tại các epochs khác nhau là trung bình của 50 đặc trưng được chọn của tất cả mẫu của mỗi lớp cùng
với trung bình của các mẫu thực tế của lớp tương ứng. Như chúng ta có thể thấy, trong quá trình huấn luyện,
những đặc trưng này trở nên tương tự hơn với mô hình của chữ số của mỗi lớp. Vì vậy, QuickSelection
có thể tìm ra các đặc trưng liên quan nhất cho tất cả các lớp.

Epoch 1  Epoch 10  Epoch 100  
   
 

Hình 16: 50 đặc trưng mang thông tin nhất của tập dữ liệu MNIST được chọn bởi QuickSelection
sau 1, 10, và 100 epochs huấn luyện.

 Epoch 1  Epoch 10  Epoch 100  Class  Epoch 1  Epoch 10  Epoch 100  Class  
        
        
        
        
        

Hình 17: Trung bình của các mẫu dữ liệu của mỗi lớp MNIST tương ứng với 50
đặc trưng được chọn sau 1, 10, và 100 epochs huấn luyện cùng với trung bình của
các mẫu thực tế của mỗi lớp.

--- TRANG 33 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 33

Bảng 5: Lựa chọn siêu tham số cho QuickSelection 10. Mỗi mục của mỗi bảng
chứa độ chính xác phân cụm và độ chính xác phân loại theo phần trăm (%),
tương ứng.

[Nội dung các bảng được giữ nguyên cấu trúc]

--- TRANG 34 ---
34 Zahra Atashgahi và cộng sự.

Bảng 6: Lựa chọn siêu tham số cho QuickSelection 100. Mỗi mục của mỗi
bảng chứa độ chính xác phân cụm và độ chính xác phân loại theo phần trăm (%),
tương ứng.

[Nội dung các bảng được giữ nguyên cấu trúc]

--- TRANG 35 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 35

D Trích xuất Đặc trưng
Mặc dù không phải là trọng tâm chính của bài báo, chúng tôi thực hiện một phân tích nhỏ trên tập dữ liệu
MNIST để nghiên cứu hiệu suất của sparse DAE như một bộ trích xuất đặc trưng. Chúng tôi huấn luyện nó để ánh xạ
các đặc trưng chiều cao thành không gian chiều thấp hơn.

Cấu trúc chúng tôi xem xét cho trích xuất đặc trưng có ba lớp ẩn với 1000, 50,
và 1000 neurons, tương ứng; lớp giữa (50 neurons) là biểu diễn chiều thấp được trích xuất.
Chúng tôi so sánh kết quả với fully-connected DAE (FC-DAE - được triển khai
trong Keras [ 13]). Chúng tôi cũng trích xuất đặc trưng sử dụng kỹ thuật Principal Component Analysis (PCA) [ 55]
như một phương pháp baseline. Sau đó, chúng tôi huấn luyện một bộ phân loại ExtraTrees trên những đặc trưng được trích xuất này
và tính độ chính xác phân loại. Kết quả được trình bày trong Hình 18.

Để đạt được mức density tốt nhất phù hợp với mạng của chúng tôi, chúng tôi kiểm tra
các giá trị khác nhau. Như được hiển thị trong Hình 18, sparse DAE (density = 3.26%) có hiệu suất tốt nhất trong số chúng.
Sparse DAE (density = 3.26%), FC-DAE, và PCA đạt được 95.2%, 96.2%, và 95.6% độ chính xác,
tương ứng. Mặc dù sparse DAE không thể hoạt động tốt như FC-DAE, nó có khoảng
54 k tham số so với 1.67 m tham số của FC-DAE. Số lượng tham số nhỏ như vậy của mô hình này
dẫn đến tăng tốc độ chạy cao và giảm đáng kể
yêu cầu bộ nhớ. Hơn nữa, thú vị khi quan sát rằng một DAE rất thưa
(dưới 1% density) có thể đạt được hơn 90.0% độ chính xác trên MNIST trong khi có khoảng 150
lần ít tham số hơn FC-DAE.

1 11 21 31 41 51 61 71 81 91 101
# of epochs80.082.585.087.590.092.595.0Accuracy (%)
sparse DAE (density = 0.68%)
sparse DAE (density = 1.66%)
sparse DAE (density = 3.26%)
sparse DAE (density = 6.26%)
sparse DAE (density = 9.04%)
sparse DAE (density = 11.62%)
PCA
FC-DAE

Hình 18: Độ chính xác phân loại cho trích xuất đặc trưng sử dụng sparse DAE với
mức density khác nhau trên tập dữ liệu MNIST (số lượng đặc trưng được trích xuất = 50)
so với FC-DAE và PCA.

E Lựa chọn Đặc trưng trên một Tập dữ liệu Lớn
Trong phụ lục này, chúng tôi đánh giá hiệu suất của các phương pháp trên một tập dữ liệu rất lớn, về
cả số lượng mẫu và chiều.

Trong thí nghiệm này, trước tiên, chúng tôi tạo ra hai tập dữ liệu nhân tạo với số lượng mẫu và
đặc trưng cao. Việc lựa chọn tập dữ liệu nhân tạo được thực hiện để dễ dàng kiểm soát số lượng đặc trưng
liên quan của tập dữ liệu, vì trong hầu hết các tập dữ liệu thực tế số lượng đặc trưng mang thông tin
không rõ ràng. Những tập dữ liệu này được tạo sử dụng công cụ thư viện sklearn5, hàm make_classification,
tạo ra các tập dữ liệu với số lượng đặc trưng và mẫu mong muốn. Hàm này
cho phép chúng tôi điều chỉnh số lượng đặc trưng mang thông tin, dư thừa, và không mang thông tin. Bảng
7 hiển thị đặc tính của hai tập dữ liệu được tạo nhân tạo. Chúng tôi tạo ra 2 tập dữ liệu

5https://scikit-learn.org/

--- TRANG 36 ---
36 Zahra Atashgahi và cộng sự.

Bảng 7: Đặc tính của hai tập dữ liệu được tạo nhân tạo. Độ chính xác phân loại
và phân cụm đã được thu được sử dụng tất cả các đặc trưng.
Tập dữ liệu Mẫu Đặc trưng Đặc trưng
Mang thông tin Đặc trưng
Dư thừa Lớp Độ chính xác
Phân loại ( %) Độ chính xác
Phân cụm ( %)
Nhân tạo1 40000 8000 500 3000 5 59.8 30.6
Nhân tạo2 40000 8000 1000 0 5 26.6 22.7

Bảng 8: Kết quả lựa chọn đặc trưng trên hai tập dữ liệu được tạo nhân tạo ( K= 1000)
Tập dữ liệu Nhân tạo1 Tập dữ liệu Nhân tạo2 Số lượng Tham số
Phương pháp Độ chính xác
Phân loại ( %) Độ chính xác
Phân cụm ( %) Độ chính xác
Phân loại ( %) Độ chính xác
Phân cụm ( %)
Lap_score 49.4 24.4 22.0 21.3 -
MCFS 68.2 29.3 24.6 21.7 -
CAE 23.4 20.6 21.1 20.4 26106
AEFS 34.7 23.3 22.8 21.4 32106
FCAE 43.8 24.3 22.9 21.5 32106
QS10 68.1 25.7 24.8 21.21 0.8106
QS100 68.4 24.8 34.5 24.6 0.8106
QS200 - - 39.7 29.6 0.8106

với 40000 mẫu và 8000 đặc trưng. Tuy nhiên, số lượng đặc trưng mang thông tin và dư thừa
khác nhau trong những tập dữ liệu này. Tập dữ liệu Nhân tạo 2 nhiễu hơn nhiều so với Nhân tạo 1;
do đó, việc tìm các đặc trưng liên quan của Nhân tạo 2 khó khăn hơn so với việc tìm chúng trên
tập dữ liệu Nhân tạo 1.

Sau khi tạo ra các tập dữ liệu, chúng tôi đánh giá hiệu suất lựa chọn đặc trưng của các phương pháp
được xem xét trong bản thảo, và so sánh kết quả với QuickSelection. Các siêu tham số
được sử dụng trong thí nghiệm này tương tự như những cái được sử dụng trong Phần 4.1.1, ngoại trừ đối với neurons ẩn
và mức độ thưa. Số lượng neurons ẩn cho các phương pháp dựa trên autoencoder đã được
đặt thành 2000, và siêu tham số của QuickSelection, , đã được điều chỉnh thành 40. Số lượng
đặc trưng được chọn ( K) là 1000. Số lượng epochs huấn luyện cho các phương pháp dựa trên autoencoder
là 100. Tuy nhiên, vì QuickSelection không hội tụ trong 100 epochs trên
tập dữ liệu Nhân tạo 2, chúng tôi tiếp tục huấn luyện cho đến epoch 200. Kết quả của thí nghiệm này
được trình bày trong Bảng 8.

Như có thể thấy trong Bảng 8, QuickSelection 100 vượt trội hơn tất cả các phương pháp khác về
độ chính xác phân loại trên cả hai tập dữ liệu. Nó cũng có thể vượt trội hơn các phương pháp khác về
độ chính xác phân cụm trên tập dữ liệu Nhân tạo 2. Như đã đề cập trước đó, QuickSelection đạt được
độ chính xác cao hơn trên tập dữ liệu Nhân tạo 2 khi được huấn luyện trong hơn 100 epochs.
Tuy nhiên, vì đối với tất cả các phương pháp khác chúng tôi sử dụng 100 epochs, chúng tôi chỉ xem xét kết quả của
QuickSelection 100 để có so sánh công bằng (cần lưu ý rằng việc tăng số lượng
epochs huấn luyện không cải thiện kết quả của các phương pháp khác). Trên các tập dữ liệu nhiễu và rất lớn, CAE, AEFS, và FCAE có hiệu suất kém trong lựa chọn đặc trưng. Ngoài ra,
chúng có khoảng 30 lần nhiều tham số hơn QuickSelection. CAE có độ chính xác thấp nhất
trong số các phương pháp này; phương pháp này rất nhạy cảm với nhiễu. Lap_score và MCFS có
hiệu suất kém trên tập dữ liệu Nhân tạo 2 nhiễu hơn Nhân tạo 1. Trên tập dữ liệu Nhân tạo 1
, MCFS đạt được độ chính xác phân cụm cao nhất. Tuy nhiên, yêu cầu bộ nhớ của
MCFS và Lap_score đáng kể lớn. Trên tập dữ liệu này, chúng cần khoảng 26GB RAM.
Tuy nhiên, QuickSelection chỉ cần khoảng 8GB bộ nhớ. Tóm lại, QuickSelection 100 có
hiệu suất tốt trên những tập dữ liệu lớn này, trong khi có số lượng tham số thấp nhất.

--- TRANG 37 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 37

F Phân tích Thuật toán Huấn luyện Thưa
Trong phụ lục này, chúng tôi nhằm mục đích phân tích tác động của quy trình huấn luyện SET đối với hiệu suất
của QuickSelection.

Chúng tôi thực hiện QuickSelection sử dụng một thuật toán khác để có được và huấn luyện mạng thưa,
và sau đó, so sánh kết quả với QuickSelection gốc. Chúng tôi rút ra sparse denoising
autoencoder sử dụng thuật toán lottery ticket hypothesis [ 20], như sau. Lottery ticket
hypothesis (LTH), trước tiên, bắt đầu với việc huấn luyện một mạng dày đặc. Sau đó, nó rút ra topology
của mạng thưa bằng cách cắt tỉa các trọng số không quan trọng của mạng dày đặc đã huấn luyện.
Sau đó, sử dụng cả topology thưa và các giá trị trọng số ban đầu của các kết nối trong
giai đoạn huấn luyện dày đặc, mạng được huấn luyện lại. Trên mô hình thưa cuối cùng thu được, chúng tôi áp dụng
các nguyên tắc QuickSelection để chọn các đặc trưng mang thông tin nhất.

Trong thí nghiệm này, cấu trúc, mức độ thưa, và các siêu tham số khác tương tự
như các thiết lập được mô tả trong Phần 4.1.1; chúng tôi sử dụng một autoencoder đơn giản với một lớp ẩn
chứa 1000 neurons ẩn, được huấn luyện trong 100 epochs. Kết quả của lựa chọn đặc trưng
(K= 50) có sẵn trong Bảng 9 và 10. Chúng tôi đề cập đến việc lựa chọn đặc trưng được thực hiện sử dụng
các nguyên tắc QuickSelection và Sparse DAE thu được với LTH là QSLTH
100. Chúng tôi sử dụng QS 100
cho QuickSelection được thực hiện sử dụng Sparse DAE thu được với SET.

Như có thể quan sát trong Bảng 9 và 10, trong hầu hết các trường hợp QS 100 vượt trội hơn QSLTH
100. Chúng tôi
tin rằng việc tối ưu hóa topology thưa và trọng số, đồng thời, dẫn đến sức mạnh đặc trưng
có ý nghĩa hơn cho việc lựa chọn đặc trưng. Chúng tôi đã thảo luận về sức mạnh neuron chi tiết hơn trong Phần 5.3. Ngoài ra, do có một giai đoạn huấn luyện dày đặc bổ sung, yêu cầu tài nguyên tính toán
của LTH cao hơn nhiều so với của SET. Để làm rõ
khía cạnh này, chúng tôi trình bày một so sánh về số lượng tham số giữa hai phương pháp này.
Kết quả có thể được tìm thấy trong Bảng 11. Số lượng tham số cao hơn nhiều trong QSLTH
100 so
với số lượng tham số trong QS 100 được đưa ra bởi giai đoạn huấn luyện dày đặc của
LTH.

Bảng 9: Độ chính xác phân cụm (%) sử dụng 50 đặc trưng được chọn (ngoại trừ Madelon
mà chúng tôi chọn 20 đặc trưng).
Phương pháp Coil20 Isolet HAR Madelon MNIST SMK GLA PCMAC
QS10060.2±2.0 35.1 ±2.7 54.6 ±4.5 58.2 ±1.5 48.3 ±2.451.8±0.859.5±1.8 52.5 ±1.1
QSLTH
10058.8±3.3 31.2 ±2.4 50.2 ±6.3 50.8 ±0.5 37.5 ±4.054.6±2.754.6±3.7 50.8 ±0.6

Bảng 10: Độ chính xác phân loại (%) sử dụng 50 đặc trưng được chọn (ngoại trừ Madelon
mà chúng tôi chọn 20 đặc trưng).
Phương pháp Coil20 Isolet HAR Madelon MNIST SMK GLA PCMAC
QS10099.7±0.3 89.0 ±1.3 90.2 ±1.2 90.3 ±0.7 93.5 ±0.5 75.7 ±3.9 73.3 ±3.358.0±2.9
QSLTH
10099.6±0.6 84.5 ±3.9 86.3 ±6.3 53.0 ±7.2 82.6 ±2.4 74.2 ±2.7 71.3 ±4.259.5±5.9

Bảng 11: Số lượng tham số của QS 100 và QSLTH
100 (chia cho 106).
Phương pháp Coil20 Isolet HAR Madelon MNIST SMK GLA PCMAC
QS1000.054 0.043 0.042 0.040 0.048 0.566 1.3 0.115
QSLTH
1002.054 1.243 1.142 1.040 1.548 40.566 99.3 6.715

--- TRANG 38 ---
38 Zahra Atashgahi và cộng sự.

G Đánh giá Hiệu suất sử dụng Bộ phân loại Random Forest
Trong phụ lục này, chúng tôi xác thực kết quả độ chính xác phân loại sử dụng một bộ phân loại khác. Chúng tôi
lặp lại thí nghiệm từ Phần 4.2 trong bản thảo; tuy nhiên, chúng tôi đo độ chính xác của việc chọn 50
đặc trưng (đối với Madelon, chúng tôi chọn 20 đặc trưng) sử dụng bộ phân loại RandomForest
[39] thay vì bộ phân loại ExtraTrees. Kết quả được trình bày trong Bảng 12.

Như có thể thấy trong Bảng 12, QuickSelection 100 là người thực hiện tốt nhất trong 5 trên 8 trường hợp. Bằng cách
so sánh kết quả với Bảng 3 chứng minh độ chính xác phân loại được đo bởi
bộ phân loại ExtraTrees, rõ ràng là đã có những thay đổi tinh tế trong các giá trị độ chính xác. Điều này đã
dẫn đến một số thay đổi trong xếp hạng của các phương pháp về
hiệu suất, vì trong một số trường hợp, hiệu suất của các phương pháp rất gần nhau. Lý do
đằng sau việc chọn bộ phân loại ExtraTrees trong thí nghiệm là do chi phí tính toán thấp. Tuy nhiên, như đã thảo luận trong bài báo, để thực hiện đánh giá toàn diện, chúng tôi cũng đã
đo hiệu suất sử dụng độ chính xác phân cụm. Nhìn chung, bằng cách nhìn vào kết quả của
ba phương pháp để tính độ chính xác, rõ ràng rằng QuickSelection là một phương pháp lựa chọn đặc trưng
hiệu quả về chất lượng của các đặc trưng được chọn.

Bảng 12: Độ chính xác phân loại (%) sử dụng 50 đặc trưng được chọn (ngoại trừ Madelon
mà chúng tôi chọn 20 đặc trưng). Trên mỗi tập dữ liệu, mục in đậm là người thực hiện tốt nhất,
và mục in nghiêng là người thực hiện tốt thứ hai. Bộ phân loại được sử dụng để đánh giá là
bộ phân loại random forest.
Phương pháp COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS99.50.379.90.4 88.5 0.4 81.9 0.7 89.2 0.0 76.3 3.7 69.4 3.9 56.5 0.16
LS 88.9 0.8 83.4 0.2 86.4 0.388.90.620.70.1 67.9 3.1 71.1 2.8 50.13 0
CAE 99.30.6 89.0 0.789.81.0 84.20.995.20.2 76.74.776.63.8 61.62.3
AEFS 92.4 2.3 84.9 1.7 87.8 1.1 59.6 4.0 87.6 0.8 71.1 6.2 67.2 4.8 57.7 2.2
FCAE 99.0 0.6 85.8 5.2 83.6 2.6 62.7 13.1 69.6 2.9 74.2 2.6 68.9 4.0 58.8 2.5
QS1098.50.9 87.0 0.7 87.6 0.5 81.5 3.8 93.60.675.12.3 68.1 4.6 60.0 3.7
QS10099.50.3 89.1 1.3 89.01.288.90.793.20.578.53.5 73.33.167.93.8
