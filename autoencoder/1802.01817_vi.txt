# 1802.01817.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/autoencoder/1802.01817.pdf
# Kích thước tệp: 330592 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
AUTO-ENCODER TÍCH CHẬP ĐỆ QUY CẤP BYTE CHO VĂN BẢN
Xiang Zhang
Viện Khoa học Toán học Courant, Đại học New York
Facebook AI Research, Facebook Inc.
xiang@cs.nyu.edu
Yann LeCun
Viện Khoa học Toán học Courant, Đại học New York
Trung tâm Khoa học Dữ liệu, Đại học New York
Facebook AI Research, Facebook Inc.
yann@cs.nyu.edu
TÓM TẮT
Bài báo này đề xuất tự động mã hóa văn bản ở cấp byte sử dụng mạng tích chập với kiến trúc đệ quy. Động lực là khám phá liệu có thể có việc tạo văn bản có thể mở rộng và đồng nhất ở cấp byte theo cách không tuần tự thông qua nhiệm vụ đơn giản của tự động mã hóa hay không. Chúng tôi chỉ ra rằng việc tạo văn bản không tuần tự từ một biểu diễn có độ dài cố định không chỉ có thể, mà còn đạt được kết quả tự động mã hóa tốt hơn nhiều so với mạng hồi tiếp. Mô hình đề xuất là một khung mã hóa-giải mã tích chập sâu đa giai đoạn sử dụng kết nối phần dư (He et al., 2016), chứa lên đến 160 lớp có tham số. Mỗi bộ mã hóa hoặc giải mã chứa một nhóm mô-đun được chia sẻ bao gồm các lớp gộp hoặc lấy mẫu lên, làm cho mạng đệ quy về mặt mức độ trừu tượng trong biểu diễn. Kết quả cho 6 bộ dữ liệu đoạn văn quy mô lớn được báo cáo, bằng 3 ngôn ngữ bao gồm tiếng Ả Rập, Trung Quốc và Anh. Các phân tích được thực hiện để nghiên cứu một số tính chất của mô hình đề xuất.

1 GIỚI THIỆU
n x Tích chập
Gộpn x Tích chậpVăn bản
n x Tích chập
Gộp
n x Tích chập
Gộp
n x Tuyến tính...(n-1) x Tích chập
Mở rộng Tích chậpn x Tích chập
(n-1) x Tích chập
Mở rộng Tích chập
(n-1) x Tích chập
Mở rộng Tích chập
n x Tuyến tính...
Đặc trưngBộ mã hóa Bộ giải mãTiền tố
Đệ quy
(chia sẻ)
Hậu tốĐệ quy
(chia sẻ)
Tiền tốHậu tố

Hình 1: Mô hình autoencoderGần đây, việc tạo văn bản sử dụng
mạng tích chập (ConvNets)
bắt đầu trở thành một lựa chọn thay thế
cho mạng hồi tiếp cho việc học
chuỗi-đến-chuỗi (Gehring et al.,
2017). Giả định chủ đạo cho
cả hai phương pháp này là văn bản
được tạo ra từng từ một.
Quá trình tạo tuần tự như vậy
mang theo rủi ro của vấn đề
đầu ra hoặc gradient biến mất hoặc
bùng nổ (Bengio et al., 1994), điều này
giới hạn độ dài của kết quả
được tạo ra của nó.
Hạn chế về khả năng mở rộng như vậy
thúc đẩy chúng tôi khám phá liệu việc
tạo văn bản không tuần tự có
thể hay không.
Đồng thời, xử lý văn bản từ
các cấp độ thấp hơn từ - chẳng hạn như
ký tự (Zhang et al., 2015) (Kim et al., 2016) và byte (Gillick et al., 2016) (Zhang & Le-
Cun, 2017) - cũng đang được khám phá do lời hứa của nó trong việc xử lý các ngôn ngữ khác biệt theo cùng

Một phần của các thí nghiệm đã được hoàn thành khi tác giả đầu tiên đang thực tập tại Facebook AI Research.
1arXiv:1802.01817v1  [cs.CL]  6 Feb 2018

--- TRANG 2 ---
u
y1y2...ynh1h2...hn(a) Tuần tự
u
y1y2...yn (b) Không tuần tự
Hình 2: Bộ giải mã tuần tự và không tuần tự được minh họa trong các mô hình đồ họa. u là một vector
chứa biểu diễn được mã hóa. yi là các thực thể đầu ra. hi là các biểu diễn ẩn. Lưu ý
rằng cả hai đều hàm ý sự độc lập có điều kiện giữa các đầu ra được điều kiện bởi biểu diễn u.

cách. Cụ thể, công trình của Zhang & LeCun (2017) chỉ ra rằng việc mã hóa một-nóng đơn giản trên
byte có thể cho kết quả tốt nhất cho phân loại văn bản trong nhiều ngôn ngữ khác nhau. Lý do là nó
đạt được sự cân bằng tốt nhất giữa hiệu suất tính toán và độ chính xác phân loại. Được lấy cảm hứng
từ những kết quả này, bài báo này khám phá tự động mã hóa cho văn bản sử dụng mạng tích chập cấp byte
có cấu trúc đệ quy, như một bước đầu tiên hướng tới việc tạo văn bản cấp thấp và không tuần tự.

Đối với nhiệm vụ tự động mã hóa văn bản, chúng ta nên tránh sử dụng các cơ chế chú ý phổ biến như
những cơ chế được sử dụng trong dịch máy Bahdanau et al. (2015), bởi vì chúng luôn cung cấp một đường dẫn
thông tin trực tiếp cho phép auto-encoder sao chép trực tiếp từ đầu vào. Điều này làm giảm
mục đích nghiên cứu khả năng biểu diễn của các mô hình khác nhau. Do đó, tất cả các mô hình được
xem xét trong bài báo này sẽ mã hóa đến và giải mã từ một biểu diễn vector có độ dài cố định.

Bài báo của Zhang et al. (2017) là một kết quả trước đó về việc sử dụng mạng tích chập cấp từ
cho tự động mã hóa văn bản. Bài báo này khác với nó ở một số cách chính trong việc sử dụng mạng tích chập.
Trước hết, các mô hình của chúng tôi hoạt động từ cấp byte thay vì từ, điều này có thể nói là làm cho
vấn đề trở nên thách thức hơn. Thứ hai, mạng của chúng tôi là động với cấu trúc đệ quy mà
mở rộng theo độ dài của văn bản đầu vào, điều này theo thiết kế có thể tránh các giải pháp tầm thường cho tự động mã hóa
chẳng hạn như hàm đồng nhất. Thứ ba, bằng cách sử dụng các heuristic thiết kế mới nhất như kết nối phần dư
(He et al., 2016), mạng của chúng tôi có thể mở rộng lên đến vài trăm lớp sâu, so với một
mạng tĩnh chứa một vài lớp.

Trong bài báo này, một số tính chất của mô hình tự động mã hóa được nghiên cứu. Sau đây là danh sách.
1. Áp dụng mô hình cho 3 ngôn ngữ - tiếng Ả Rập, Trung Quốc và Anh - cho thấy mô hình
có thể xử lý tất cả các ngôn ngữ khác nhau theo cùng một cách với độ chính xác tốt như nhau.
2. So sánh với bộ nhớ ngắn hạn dài (LSTM) (Hochreiter & Schmidhuber, 1997)
cho thấy một lợi thế đáng kể của việc sử dụng mạng tích chập cho tự động mã hóa văn bản.
3. Chúng tôi xác định rằng một bộ giải mã tích chập đệ quy như của chúng tôi có thể tạo ra chính xác
byte kết thúc chuỗi, mặc dù quá trình giải mã là không tuần tự.
4. Bằng cách nghiên cứu lỗi tự động mã hóa khi các mẫu chứa byte nhiễu ngẫu nhiên, chúng tôi
chỉ ra rằng mô hình không thoái hóa thành hàm đồng nhất. Tuy nhiên, nó cũng không thể
khử nhiễu đầu vào rất tốt.
5. Cấu trúc đệ quy yêu cầu một lớp gộp. Chúng tôi so sánh giữa gộp trung bình,
gộp L2 và gộp tối đa, và xác định rằng gộp tối đa là lựa chọn tốt nhất.
6. Lợi thế của đệ quy được thiết lập bằng so sánh với một mô hình tĩnh không
có các nhóm mô-đun được chia sẻ. Điều này cho thấy các heuristic ngôn ngữ như đệ quy
hữu ích cho việc thiết kế mô hình cho xử lý ngôn ngữ.
7. Chúng tôi cũng khám phá các mô hình có kích thước khác nhau bằng cách thay đổi độ sâu mạng tối đa từ
40 đến 320. Kết quả cho thấy các mô hình sâu hơn cho kết quả tốt hơn.

2 AUTO-ENCODER TÍCH CHẬP ĐỆ QUY CẤP BYTE

Trong phần này, chúng tôi giới thiệu thiết kế của mô hình auto-encoder tích chập với cấu trúc đệ quy.
Mô hình bao gồm 6 nhóm mô-đun, với 3 cho bộ mã hóa và 3 cho bộ giải mã.
Mô hình đầu tiên mã hóa một đầu vào có độ dài biến đổi thành một vector có độ dài cố định kích thước 1024,
sau đó giải mã trở lại cùng độ dài đầu vào. Kiến trúc bộ giải mã là một gương ngược của bộ mã hóa. Tất cả
2

--- TRANG 3 ---
các lớp tích chập trong bài báo này có zero-padding được thêm vào để đảm bảo rằng mỗi lớp tích chập
xuất ra cùng độ dài với đầu vào. Chúng cũng đều có kích thước đặc trưng 256 và kích thước kernel 3. Tất cả
các lớp có tham số trong mô hình của chúng tôi sử dụng ReLU (Nair & Hinton, 2010) làm tính phi tuyến.

Trong bộ mã hóa, nhóm mô-đun đầu tiên bao gồm n lớp tích chập thời gian (1-D). Nó
nhận một chuỗi byte được mã hóa một-nóng làm đầu vào, trong đó mỗi byte được mã hóa như một vector 256 chiều.
Nhóm mô-đun đầu tiên này chuyển đổi đầu vào thành một biểu diễn nội bộ. Chúng tôi gọi
nhóm mô-đun này là nhóm tiền tố. Nhóm mô-đun thứ hai bao gồm n lớp tích chập thời gian
cộng với một lớp max-pooling kích thước 2. Nhóm này giảm độ dài đầu vào theo hệ số
2, và nó có thể được áp dụng lặp đi lặp lại để giảm độ dài biểu diễn một cách đệ quy. Do đó,
chúng tôi đặt tên nhóm thứ hai này là nhóm đệ quy. Nhóm đệ quy được áp dụng cho đến khi kích thước
của biểu diễn trở thành 1024, thực tế là một đặc trưng có chiều 256 và độ dài 4. Sau đó,
sau nhóm đệ quy cuối cùng là một nhóm hậu tố gồm n lớp tuyến tính cho chuyển đổi đặc trưng.

12345678 12345678
12345678 1234567812345678
12345678
12345678
12345678

Hình 3: Quá trình định hình lại. Điều này minh họa
quá trình định hình lại để chuyển đổi một
biểu diễn có kích thước đặc trưng 4 và độ dài 8 thành
kích thước đặc trưng 2 và độ dài 16. Các màu khác nhau
biểu thị các đặc trưng nguồn khác nhau, và các số
là chỉ số trong chiều độ dài.Bộ giải mã là một gương ngược đối xứng của
bộ mã hóa. Nhóm tiền tố bộ giải mã bao gồm
n lớp tuyến tính, theo sau là một nhóm đệ quy
bộ giải mã mà mở rộng độ dài biểu diễn
theo hệ số 2. Việc mở rộng này được thực hiện tại
lớp tích chập đầu tiên của nhóm này, nơi
nó xuất ra 512 đặc trưng sẽ được định hình lại
thành 256 đặc trưng. Quá trình định hình lại mà chúng tôi sử dụng
đảm bảo rằng các giá trị đặc trưng tương ứng với
trường nhìn gần nhau trong đầu vào, tương tự như
ý tưởng của tích chập sub-pixel (hoặc pixel shuf-
fling) (Shi et al., 2016). Hình 3 mô tả quá trình
định hình lại này để chuyển đổi biểu diễn
có kích thước đặc trưng 4 và độ dài 8 thành kích thước đặc trưng 2
và độ dài 16. Sau khi nhóm đệ quy này được
áp dụng vài lần (giống như của nhóm đệ quy
bộ mã hóa), một nhóm hậu tố bộ giải mã gồm n
lớp tích chập được áp dụng để giải mã các
đặc trưng đệ quy thành một chuỗi byte.

Đầu ra cuối cùng của bộ giải mã được hiểu như
xác suất của các byte sau khi đi qua một hàm softmax. Do đó, hàm mất mát mà chúng tôi sử dụng đơn giản là
negative-log likelihood trên các đầu ra softmax riêng lẻ. Điều đáng chú ý là điều này không
hàm ý rằng các byte đầu ra độc lập vô điều kiện với nhau. Đối với bộ giải mã văn bản không tuần tự của chúng tôi,
sự độc lập giữa các byte đầu ra được điều kiện bởi biểu diễn từ bộ mã hóa, có nghĩa là
sự phụ thuộc lẫn nhau của chúng được mô hình hóa bởi chính bộ giải mã. Hình 2 minh họa
sự khác biệt giữa việc tạo văn bản tuần tự và không tuần tự sử dụng các mô hình đồ họa.

Tùy thuộc vào độ dài đầu vào và kích thước của biểu diễn được mã hóa, mô hình của chúng tôi có thể cực kỳ
sâu. Ví dụ, với n = 8 và chiều mã hóa 1024 (giảm xuống độ dài 4 với 256
đặc trưng), cho một mẫu có độ dài 1024 byte, toàn bộ mô hình có 160 lớp có tham số. Huấn luyện
một mô hình động sâu như vậy có thể rất thách thức khi sử dụng gradient descent ngẫu nhiên (SGD) do
vấn đề gradient biến mất (Bengio et al., 1994). Do đó, chúng tôi sử dụng ý tưởng được đề xuất gần đây
về kết nối phần dư (He et al., 2016) để làm cho tối ưu hóa dễ dàng hơn. Đối với mọi cặp
lớp có tham số liền kề, biểu diễn đặc trưng đầu vào được truyền qua đến đầu ra bằng phép cộng.
Chúng tôi không thể huấn luyện một mô hình được thiết kế theo cách này mà không có các kết nối phần dư như vậy.

Đối với tất cả các mô hình của chúng tôi, chúng tôi sử dụng một biểu diễn được mã hóa có chiều 1024 (đệ quy đến độ dài
4 với 256 đặc trưng). Đối với một mẫu đầu vào có độ dài tùy ý l, chúng tôi đầu tiên thêm byte null kết thúc chuỗi
vào nó, và sau đó đệm nó đến độ dài 2⌈log₂(l+1)⌉ với tất cả các vector không. Điều này làm cho độ dài đầu vào
trở thành một lũy thừa cơ số 2 của một số nguyên nào đó, vì các nhóm đệ quy trong cả bộ mã hóa và bộ giải mã
giảm hoặc mở rộng độ dài biểu diễn theo hệ số 2. Nếu l < 4, nó được đệm đến kích thước 4 và
không đi qua các nhóm đệ quy. Dễ thấy rằng độ sâu của mạng động này
cho một mẫu có độ dài l là theo thứ tự log₂l, có thể làm cho các biểu diễn ẩn hiệu quả hơn
và dễ học hơn so với mạng hồi tiếp có thứ tự tuyến tính về độ sâu.

3

--- TRANG 4 ---
Bảng 1: Bộ dữ liệu
TÊN BÀI BÁO ĐOẠN VĂN NGÔN NGỮ HUẤN LUYỆN KIỂM TRA HUẤN LUYỆN KIỂM TRA
enwiki 7,634,438 850,457 41,256,261 4,583,893 Tiếng Anh
hudong 1,618,817 180,278 53,675,117 5,999,920 Tiếng Trung
argiga 3,011,403 334,764 27,989,646 3,116,719 Tiếng Ả Rập
engiga 8,887,583 988,513 116,456,520 12,969,170 Tiếng Anh
zhgiga 5,097,198 567,179 38,094,390 4,237,643 Tiếng Trung
allgiga 16,996,184 1,890,456 182,540,556 20,323,532 Đa ngôn ngữ

3 KẾT QUẢ CHO TỰ ĐỘNG MÃ HÓA ĐA NGÔN NGỮ

Trong phần này, chúng tôi trình bày kết quả của auto-encoder tích chập đệ quy cấp byte của chúng tôi.

3.1 BỘ DỮ LIỆU

Tất cả các bộ dữ liệu của chúng tôi đều ở cấp độ đoạn văn. Xử lý trước tối thiểu được áp dụng cho chúng vì
mô hình của chúng tôi có thể được áp dụng cho tất cả các ngôn ngữ theo cùng một cách. Chúng tôi cũng xây dựng một bộ dữ liệu với
các mẫu được trộn lẫn trong cả ba ngôn ngữ để kiểm tra khả năng xử lý dữ liệu đa ngôn ngữ của mô hình.

enwiki. Bộ dữ liệu này chứa các đoạn văn từ Wikipedia tiếng Anh¹, được xây dựng từ bản dump
vào ngày 1 tháng 6 năm 2016. Chúng tôi có thể thu được 8,484,895 bài báo, và sau đó chia ra 7,634,438 cho
huấn luyện và 850,457 cho kiểm tra. Số lượng đoạn văn cho huấn luyện và kiểm tra do đó là
41,256,261 và 4,583,893 tương ứng.

Bảng 2: Lỗi cấp byte huấn luyện và kiểm tra
BỘ DỮ LIỆU NGÔN NGỮ HUẤN LUYỆN KIỂM TRA
enwiki Tiếng Anh 3.34% 3.34%
hudong Tiếng Trung 3.21% 3.16%
argiga Tiếng Ả Rập 3.08% 3.09%
engiga Tiếng Anh 2.09% 2.08%
zhgiga Tiếng Trung 5.11% 5.24%
allgiga Đa ngôn ngữ 2.48% 2.50%hudong. Bộ dữ liệu này chứa các đoạn văn
từ trang web bách khoa toàn thư Trung Quốc
baike.com². Chúng tôi đã thu thập 1,799,095
bài viết từ nó và sử dụng 1,618,817 cho
huấn luyện và 180,278 cho kiểm tra. Số
lượng đoạn văn cho huấn luyện và kiểm tra
là 53,675,117 và 5,999,920.

argiga. Bộ dữ liệu này chứa các đoạn văn
từ bản phát hành Arabic Gigaword Fifth Edition
(Parker et al., 2011a), là một
bộ sưu tập các bài báo newswire tiếng Ả Rập. Tổng
cộng có 3,346,167 bài báo, và chúng tôi
sử dụng 3,011,403 cho huấn luyện và 334,764 cho
kiểm tra. Do đó, chúng tôi có 27,989,646
đoạn văn cho huấn luyện và 3,116,719 cho kiểm tra.

engiga. Bộ dữ liệu này chứa các đoạn văn từ bản phát hành English Gigaword Fifth Edition (Parker
et al., 2011c), là một bộ sưu tập các bài báo newswire tiếng Anh. Tổng cộng có 9,876,096 bài
báo, và chúng tôi sử dụng 8,887,583 cho huấn luyện và 988,513 cho kiểm tra. Do đó, chúng tôi có 116,456,520
đoạn văn cho huấn luyện và 12,969,170 cho kiểm tra.

zhgiga. Bộ dữ liệu này chứa các đoạn văn từ bản phát hành Chinese Gigaword Fifth Edition (Parker
et al., 2011b), là một bộ sưu tập các bài báo newswire tiếng Trung. Tổng cộng có 5,664,377
bài báo, và chúng tôi sử dụng 5,097,198 cho huấn luyện và 567,179 cho kiểm tra. Do đó, chúng tôi có 38,094,390
đoạn văn cho huấn luyện và 4,237,643 cho kiểm tra.

allgiga. Vì ba bộ dữ liệu Gigaword rất giống nhau, chúng tôi kết hợp chúng để tạo thành
một bộ dữ liệu đa ngôn ngữ của các đoạn văn bài báo newswire. Trong bộ dữ liệu này, có 18,886,640 bài báo

¹https://en.wikipedia.org
²http://www.baike.com/

4

--- TRANG 5 ---
với 16,996,184 cho huấn luyện và 1,890,456 cho kiểm tra. Số lượng đoạn văn cho huấn luyện và
kiểm tra là 182,540,556 và 20,323,532 tương ứng.

Bảng 1 là tóm tắt của những bộ dữ liệu này. Đối với các bộ dữ liệu lớn như vậy, thời gian kiểm tra có thể không thể
chấp nhận được. Do đó, chúng tôi báo cáo tất cả kết quả dựa trên 1,000,000 mẫu được lấy mẫu ngẫu nhiên từ
tập huấn luyện hoặc kiểm tra tùy thuộc vào tình huống. Rất ít quá khớp được quan sát ngay cả đối với
mô hình lớn nhất của chúng tôi.

3.2 KẾT QUẢ

Bảng 3: Lỗi cấp byte cho mạng hồi tiếp
bộ nhớ ngắn hạn dài (LSTM)
BỘ DỮ LIỆU NGÔN NGỮ HUẤN LUYỆN KIỂM TRA
enwiki Tiếng Anh 67.71% 67.80%
hudong Tiếng Trung 64.47% 64.56%
argiga Tiếng Ả Rập 61.23% 61.29%
engiga Tiếng Anh 70.47% 70.45%
zhgiga Tiếng Trung 75.91% 75.90%
allgiga Đa ngôn ngữ 72.39% 72.44%Bất kể bộ dữ liệu nào, tất cả các auto-encoder
văn bản của chúng tôi đều được huấn luyện với cùng
siêu tham số sử dụng gradient descent ngẫu nhiên
(SGD) với momentum (Polyak, 1964) (Sutskever et al., 2013).
Mô hình mà chúng tôi sử dụng có n = 8 - tức là,
có 8 lớp có tham số trong mỗi nhóm mô-đun
tiền tố, đệ quy và hậu tố, cho cả bộ mã hóa
và bộ giải mã. Mỗi epoch huấn luyện chứa
1,000,000 bước, và mỗi bước được huấn luyện trên
một mẫu được chọn ngẫu nhiên với độ dài lên đến
1024 byte. Do đó, độ sâu mô hình tối đa là 160.
Chúng tôi chỉ lan truyền ngược qua các byte hợp lệ
trong đầu ra. Lưu ý rằng mỗi mẫu chứa một byte
kết thúc chuỗi ("null" byte) theo thiết kế.

Chúng tôi đặt tốc độ học ban đầu là 0.001, và giảm một nửa mỗi 10 epoch. Momentum 0.9 được áp dụng
để tăng tốc huấn luyện. Một weight decay nhỏ 0.00001 được sử dụng để ổn định huấn luyện. Tùy thuộc vào
độ dài của mỗi mẫu, các nhóm đệ quy bộ mã hóa hoặc bộ giải mã được áp dụng một số lần nhất định.
Chúng tôi thấy rằng việc chia gradient của các nhóm đệ quy này cho số lượng bản sao được chia sẻ
có thể tăng tốc huấn luyện. Quá trình huấn luyện dừng ở epoch thứ 100.

-20-18-16-13-11-9-7-5-3-11357911131517190.00%20.00%40.00%60.00%80.00%100.00%120.00%
huấn luyệnkiểm tra

Hình 4: Biểu đồ của sự khác biệt độ dàiLưu ý rằng vì các bộ dữ liệu engiga và allgiga
có hơn 100,000,000 mẫu huấn luyện, khi huấn luyện
dừng, mô hình chưa thấy toàn bộ dữ liệu huấn luyện.
Tuy nhiên, huấn luyện thêm không đạt được bất kỳ
cải thiện quan sát được nào.

Bảng 2 chi tiết các lỗi cấp byte cho mô hình của chúng tôi
trên tất cả các bộ dữ liệu đã nêu. Những kết quả này
chỉ ra rằng các mô hình của chúng tôi có thể đạt được
tỷ lệ lỗi rất tốt cho tự động mã hóa trong các ngôn ngữ khác nhau.
Kết quả cho bộ dữ liệu allgiga cũng chỉ ra rằng
mô hình không gặp khó khăn trong việc học từ các bộ dữ liệu
đa ngôn ngữ chứa các mẫu của các ngôn ngữ rất khác nhau.

4 THẢO LUẬN

Phần này cung cấp so sánh với mạng hồi tiếp, và nghiên cứu về một tập hợp các tính chất khác nhau
của mô hình tự động mã hóa được đề xuất của chúng tôi. Hầu hết những kết quả này được thực hiện sử dụng bộ dữ liệu enwiki.

4.1 SO SÁNH VỚI MẠNG HỒI TIẾP

Chúng tôi xây dựng một mạng hồi tiếp baseline đơn giản sử dụng các đơn vị bộ nhớ ngắn hạn dài "vanilla"
(Hochreiter & Schmidhuber, 1997). Trong mô hình này, cả byte đầu vào và đầu ra đều được nhúng
vào các vector có chiều 1024 để chúng tôi có thể sử dụng một biểu diễn ẩn có chiều 1024. Bộ
5

--- TRANG 6 ---
00.10.20.30.40.50.60.70.80.9100.20.40.60.811.2
Lỗi huấn luyệnLỗi kiểm traTham chiếu(a) Groundtruth
00.10.20.30.40.50.60.70.80.9100.20.40.60.811.2
Lỗi huấn luyệnLỗi kiểm traTham chiếu (b) Đột biến

Hình 5: Lỗi cấp byte đối với các mẫu đột biến ngẫu nhiên

mã hóa đọc văn bản theo thứ tự ngược, điều này được quan sát bởi Sutskever et al. (2014) là việc đảo ngược
chuỗi đầu vào có thể cải thiện chất lượng đầu ra. Đầu ra ẩn 1024 chiều của tế bào cuối cùng được
sử dụng làm đầu vào cho bộ giải mã.

Bộ giải mã cũng nhúng byte đầu vào và đầu ra vào các vector có chiều 1024 và sử dụng một
biểu diễn ẩn có chiều 1024. Trong quá trình giải mã, byte được tạo gần nhất được
đưa vào bước thời gian tiếp theo. Điều này được gọi là "teacher forcing" được quan sát là cải thiện
kết quả tự động mã hóa trong trường hợp của chúng tôi. Quá trình giải mã sử dụng thuật toán tìm kiếm chùm
có kích thước 2. Trong quá trình học, chúng tôi chỉ lan truyền ngược qua chuỗi có khả năng nhất sau tìm kiếm chùm.

Bảng 3 chi tiết kết quả cho LSTM. Các lỗi cấp byte lớn đến mức kết quả của các mô hình của chúng tôi trong
bảng 2 ít nhất tốt hơn một bậc độ lớn. Hạn chế cơ bản của mạng hồi tiếp
là bất kể cấp độ thực thể (từ, ký tự hoặc byte), chúng có thể nhớ khoảng lên đến
50 trong số chúng một cách chính xác, và sau đó thất bại trong việc dự đoán chính xác chúng. Theo cấu trúc, quá trình
tạo văn bản đệ quy không tuần tự của chúng tôi có thể hy vọng là một giải pháp thay thế cho điều này,
như đã rõ ràng trong kết quả ở đây.

4.2 KẾT THÚC CHUỖI

6412819225632038444851257664070476883289696010240.00%5.00%10.00%15.00%20.00%25.00%
lỗi huấn luyệnlỗi kiểm tra

Hình 6: Lỗi cấp byte theo độ dàiMột điều tạo ra sự khác biệt giữa việc tạo văn bản
tuần tự và không tuần tự là cách quyết định khi nào
kết thúc chuỗi byte được tạo ra. Đối với quá trình
tạo tuần tự như bộ giải mã hồi tiếp, chúng ta có thể
dừng khi một số ký hiệu kết thúc chuỗi được tạo ra.
Đối với quá trình tạo không tuần tự, chúng ta có thể
coi ký hiệu kết thúc chuỗi đầu tiên gặp phải như
dấu hiệu kết thúc, mặc dù nó sẽ không tránh khỏi
việc tạo ra một số ký hiệu thêm sau nó. Sau đó, một
câu hỏi tự nhiên cần hỏi là, cách đơn giản này để
xác định kết thúc chuỗi có hiệu quả không?

Để trả lời câu hỏi này, chúng tôi tính toán sự khác biệt
của các ký hiệu kết thúc chuỗi giữa văn bản được tạo
và groundtruth của nó cho 1,000,000 mẫu, cho cả
tập huấn luyện và kiểm tra của bộ dữ liệu enwiki.
Điều chúng tôi phát hiện là phân phối của sự khác biệt độ dài
tập trung cao ở 0, ở 99.63% cho cả huấn luyện và kiểm tra. Hình 4 cho thấy biểu đồ đầy đủ,
trong đó các sự khác biệt độ dài khác 0 hầu như không nhìn thấy. Điều này cho thấy quá trình tạo văn bản
không tuần tự của chúng tôi có thể mô hình hóa vị trí kết thúc chuỗi khá chính xác. Một lý do cho điều này là
đối với mọi mẫu chúng tôi có một ký hiệu kết thúc chuỗi - byte "null" - sao cho mạng
đã học được mô hình hóa nó khá sớm trong quá trình huấn luyện.

6

--- TRANG 7 ---
1-6465-128129-192193-256257-320321-384385-448449-512513-576577-640641-704705-768767-832833-896897-960961-10241025-0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%
enwikihudongargigaengigazhgigaallgiga

Hình 7: Biểu đồ tần suất mẫu ở các độ dài khác nhau

4.3 HOÁN VỊ NGẪU NHIÊN CỦA CÁC MẪU

Một vấn đề tiềm ẩn cụ thể đối với nhiệm vụ tự động mã hóa là rủi ro học giải pháp thoái hóa - hàm đồng nhất.
Một cách để kiểm tra điều này là đột biến các byte đầu vào ngẫu nhiên và
xem liệu tỷ lệ lỗi có khớp với xác suất đột biến hay không. Chúng tôi thí nghiệm với xác suất đột biến
từ 0 đến 1 với khoảng cách 0.1, và cho mỗi trường hợp chúng tôi kiểm tra các lỗi cấp byte cho
100,000 mẫu trong cả tập huấn luyện và kiểm tra của bộ dữ liệu enwiki.

Lưu ý rằng chúng tôi có thể tính toán lỗi cấp byte theo 2 cách. Cách đầu tiên là tính toán lỗi đối với
các mẫu groundtruth. Nếu giải pháp thoái hóa thành hàm đồng nhất, thì lỗi cấp byte
nên tương quan với xác suất đột biến. Cách thứ hai là tính toán lỗi đối với các mẫu đột biến.
Nếu giải pháp thoái hóa thành hàm đồng nhất, thì lỗi cấp byte nên gần 0
bất kể xác suất đột biến. Hình 5 cho thấy kết quả theo 2 cách tính toán lỗi này, và kết quả
chỉ ra mạnh mẽ rằng mô hình không thoái hóa thành việc học hàm đồng nhất.

Bảng 4: Lỗi cấp byte cho các
lớp gộp khác nhau
GỘPING HUẤN LUYỆN KIỂM TRA
max 3.34% 3.34%
average 7.91% 7.98%
L2 6.85% 6.77%Điều đáng chú ý là các lỗi đối với các mẫu groundtruth
trong hình 5 cũng chứng minh rằng mô hình của chúng tôi thiếu
khả năng khử nhiễu các mẫu đột biến. Điều này có thể được
thấy từ hiện tượng rằng các lỗi cho mỗi xác suất đột biến
cao hơn giá trị đường chéo tham chiếu, thay vì thấp hơn.
Điều này là do thiếu tiêu chí khử nhiễu trong quá trình
huấn luyện của chúng tôi.

4.4 ĐỘ DÀI MẪU

Chúng tôi cũng thực hiện các thí nghiệm để chỉ ra lỗi cấp byte
thay đổi như thế nào đối với độ dài mẫu. Hình 7 cho thấy
biểu đồ độ dài mẫu cho tất cả các bộ dữ liệu. Nó chỉ ra rằng
phần lớn các mẫu đoạn văn có thể được mô hình hóa tốt
dưới 1024 byte. Hình 6 cho thấy lỗi cấp byte của các mô hình của chúng tôi đối với độ dài
của các mẫu. Hình này được tạo ra bằng cách kiểm tra 1,000,000 mẫu từ mỗi tập huấn luyện và kiểm tra
của bộ dữ liệu enwiki. Mỗi bin trong biểu đồ đại diện cho một phạm vi 64 với giới hạn trên được chỉ ra.
Ví dụ, lỗi ở 512 chỉ ra lỗi được tổng hợp cho các mẫu có độ dài 449 đến 512.

Bảng 5: Lỗi cấp byte cho các
mô hình đệ quy và tĩnh
MÔ HÌNH HUẤN LUYỆN KIỂM TRA
đệ quy 3.34% 3.34%
tĩnh 8.01% 8.05%Một hiện tượng thú vị là các lỗi có tương quan cao với
số lượng nhóm đệ quy được áp dụng cho cả bộ mã hóa
và bộ giải mã. Trong biểu đồ, các bin 64, 128, 192-256,
320-512, 576-1024 đại diện cho các cấp độ đệ quy
4, 5, 6, 7, 8 tương ứng. Các lỗi cho cùng cấp độ đệ quy
gần như giống nhau, mặc dù có sự khác biệt độ dài lớn
khi các cấp độ đệ quy trở nên sâu. Lý do cho điều này cũng
liên quan đến thực tế rằng có xu hướng có nhiều văn bản
ngắn hơn văn bản dài hơn trong bộ dữ liệu, như được chứng minh trong hình 7.

7

--- TRANG 8 ---
1357911131517192123252729313335373941434547495153555759616365676971737577798183858789919395979900.10.20.30.40.50.60.70.80.9
lỗi huấn luyện đệ quy lỗi kiểm tra đệ quy lỗi huấn luyện tĩnh lỗi kiểm tra tĩnh

Hình 8: Lỗi trong quá trình huấn luyện cho các mô hình đệ quy và tĩnh.

4.5 CÁC LỚP GỘPING

Phần này chi tiết một thí nghiệm trong việc nghiên cứu cách các lỗi huấn luyện và kiểm tra thay đổi với
việc lựa chọn các lớp gộping trong mạng bộ mã hóa. Các thí nghiệm được thực hiện trên mô hình đã nêu
với n = 8, và thay thế lớp max-pooling trong bộ mã hóa bằng các lớp average-pooling hoặc L2-pooling.
Bảng 4 chi tiết kết quả. Các số chỉ ra mạnh mẽ rằng max-pooling là lựa chọn tốt nhất. Max-pooling
chọn các giá trị lớn nhất trong trường nhìn của nó, giúp mạng đạt được tối ưu tốt hơn (Boureau et al., 2010).

4.6 ĐỆ QUY

Bảng 6: Lỗi cấp byte tùy thuộc
vào độ sâu mô hình
n ĐỘ SÂU HUẤN LUYỆN KIỂM TRA
2 40 9.05% 9.07%
4 80 5.07% 5.11%
8 160 3.34% 3.34%
16 320 2.91% 2.92%Việc sử dụng đệ quy trong mô hình đề xuất xuất phát từ
trực giác ngôn ngữ học rằng cấu trúc có thể giúp mô hình
học các biểu diễn tốt hơn. Tuy nhiên, không có gì đảm bảo
rằng trực giác như vậy có thể hữu ích cho mô hình trừ khi
so sánh được thực hiện với một mô hình tĩnh nhận đầu vào
có độ dài cố định và đi qua một mạng với cùng kiến trúc
của các nhóm đệ quy mà không chia sẻ trọng số.

Hình 8 cho thấy các lỗi huấn luyện và kiểm tra khi huấn luyện
một mô hình tĩnh với cùng siêu tham số. Mô hình tĩnh
nhận 1024 byte, và các vector không được đệm nếu độ dài mẫu
nhỏ hơn. Do đó nhóm đệ quy được áp dụng 8 lần trong cả
bộ mã hóa và bộ giải mã, mặc dù trọng số của chúng không được chia sẻ. Kết quả chỉ ra rằng một mô hình đệ quy
không chỉ học nhanh hơn, mà còn có thể đạt được kết quả tốt hơn. Bảng 5 liệt kê các lỗi cấp byte.

4.7 ĐỘ SÂU MÔ HÌNH

Phần này khám phá liệu việc thay đổi kích thước mô hình có thể tạo ra sự khác biệt về kết quả hay không.
Bảng 6 liệt kê các lỗi huấn luyện và kiểm tra của các độ sâu mô hình khác nhau với n ∈ {2, 4, 8, 16}.
Kết quả chỉ ra rằng tỷ lệ lỗi tốt nhất được đạt được với mô hình lớn nhất, với rất ít quá khớp.
Điều này một phần do thực tế rằng các bộ dữ liệu của chúng tôi khá lớn đối với các mô hình được đề cập.

5 KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất tự động mã hóa văn bản sử dụng một mạng tích chập đệ quy. Mô hình
chứa 6 phần - 3 cho bộ mã hóa và 3 cho bộ giải mã. Bộ mã hóa và bộ giải mã đều chứa
một nhóm mô-đun tiền tố và một nhóm mô-đun hậu tố cho chuyển đổi đặc trưng. Một nhóm mô-đun đệ quy
được bao gồm ở giữa tiền tố và hậu tố cho mỗi bộ mã hóa và bộ giải mã, mà
thu nhỏ hoặc mở rộng độ dài biểu diễn một cách đệ quy. Do đó, mô hình của chúng tôi về cơ bản tạo
văn bản theo cách không tuần tự.

Các thí nghiệm sử dụng mô hình này được thực hiện trên 6 bộ dữ liệu quy mô lớn bằng tiếng Ả Rập, Trung Quốc và Anh.
So sánh với mạng hồi tiếp được cung cấp để chỉ ra rằng mô hình của chúng tôi đạt được kết quả tuyệt vời trong
tự động mã hóa văn bản. Các tính chất của mô hình đề xuất được nghiên cứu, bao gồm khả năng tạo ra
ký hiệu kết thúc chuỗi, liệu mô hình có thoái hóa thành hàm đồng nhất hay không, và các biến thể của
các lớp gộping, đệ quy và độ sâu của các mô hình. Trong tương lai, chúng tôi hy vọng mở rộng các mô hình của mình thành
các mô hình tạo không tuần tự không có đầu vào, và sử dụng nó cho nhiều nhiệm vụ chuỗi-đến-chuỗi hơn như
dịch máy.

TÀI LIỆU THAM KHẢO

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.

Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is
difficult. Trans. Neur. Netw., 5(2):157–166, March 1994. ISSN 1045-9227. doi: 10.1109/72.
279181. URL http://dx.doi.org/10.1109/72.279181.

Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling in visual
recognition. In Proceedings of the 27th International Conference on Machine Learning (ICML-
10), pp. 111–118, 2010.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional
Sequence to Sequence Learning. In Proc. of ICML, 2017.

Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Subramanya. Multilingual language process-
ing from bytes. In Proceedings of NAA-HLT, pp. 1296–1306, 2016.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997.

Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language
models. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.

Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807–814,
2010.

Robert Parker, David Graff, Ke Chen, Junbo Kong, and Kazuaki Maeda. Arabic gigaword fifth
edition ldc2011t11, 2011a. Web Download.

Robert Parker, David Graff, Ke Chen, Junbo Kong, and Kazuaki Maeda. Chinese gigaword fifth
edition ldc2011t13, 2011b. Web Download.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth
edition ldc2011t07, 2011c. Web Download.

B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 4(5):1 – 17, 1964. ISSN 0041-5553.

Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel
Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient
sub-pixel convolutional neural network. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1874–1883, 2016.

Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139–1147, 2013.

9

--- TRANG 9 ---
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104–3112, 2014.

Xiang Zhang and Yann LeCun. Which encoding is the best for text classification in chinese, english,
japanese and korean? CoRR, abs/1708.02657, 2017.

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Processing Systems 28, pp. 649–657. 2015.

Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, and Lawrence Carin. Decon-
volutional paragraph representation learning. CoRR, abs/1708.04729, 2017.

10

--- TRANG 10 ---
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104–3112, 2014.

Xiang Zhang and Yann LeCun. Which encoding is the best for text classification in chinese, english,
japanese and korean? CoRR, abs/1708.02657, 2017.

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Processing Systems 28, pp. 649–657. 2015.

Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, and Lawrence Carin. Decon-
volutional paragraph representation learning. CoRR, abs/1708.04729, 2017.

10