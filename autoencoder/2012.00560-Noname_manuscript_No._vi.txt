# 2012.00560.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/autoencoder/2012.00560.pdf
# Kích thước tệp: 2322376 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo không tên số.
(sẽ được chèn bởi biên tập viên)
Lựa chọn đặc trưng nhanh và mạnh mẽ: Sức mạnh của
Huấn luyện thưa tiết kiệm năng lượng cho Autoencoders
Zahra Atashgahi Ghada Sokar Tim
van der Lee Elena Mocanu Decebal
Constantin Mocanu Raymond Veldhuis 
Mykola Pechenizkiy
Nhận: ngày / Chấp nhận: ngày
Tóm tắt Các biến chứng lớn phát sinh từ sự gia tăng gần đây về lượng dữ
liệu có chiều cao, bao gồm chi phí tính toán cao và yêu cầu bộ nhớ.
Lựa chọn đặc trưng, xác định các thuộc tính liên quan và thông tin nhất của
một tập dữ liệu, đã được giới thiệu như một giải pháp cho vấn đề này. Hầu hết các
phương pháp lựa chọn đặc trưng hiện tại đều không hiệu quả về mặt tính toán; các thuật toán
không hiệu quả dẫn đến tiêu thụ năng lượng cao, điều này không mong muốn cho các thiết bị có
tài nguyên tính toán và năng lượng hạn chế. Trong bài báo này, một phương pháp mới và
linh hoạt cho lựa chọn đặc trưng không giám sát được đề xuất. Phương pháp này, có tên QuickSelection1,
giới thiệu sức mạnh của neuron trong mạng nơ-ron thưa như một tiêu chí để
đo tầm quan trọng của đặc trưng. Tiêu chí này, được kết hợp với các denoising autoencoders
kết nối thưa được huấn luyện với quy trình huấn luyện tiến hóa thưa,
dẫn xuất tầm quan trọng của tất cả các đặc trưng đầu vào cùng lúc. Chúng tôi triển khai Quick-
Selection theo cách hoàn toàn thưa trái ngược với cách tiếp cận điển hình sử dụng
mặt nạ nhị phân trên các kết nối để mô phỏng độ thưa. Điều này dẫn đến sự gia tăng tốc độ
đáng kể và giảm bộ nhớ. Khi được thử nghiệm trên một số tập dữ liệu chuẩn,
bao gồm năm tập dữ liệu chiều thấp và ba tập dữ liệu chiều cao, phương pháp được đề xuất
có thể đạt được sự cân bằng tốt nhất về độ chính xác phân loại và phân cụm,
thời gian chạy, và sử dụng bộ nhớ tối đa, trong số các phương pháp được sử dụng rộng rãi cho
lựa chọn đặc trưng. Ngoài ra, phương pháp được đề xuất của chúng tôi yêu cầu lượng năng lượng
ít nhất trong số các phương pháp lựa chọn đặc trưng dựa trên autoencoder.
Bài báo này đã được chấp nhận để xuất bản trong Machine Learning Journal (ECML-PKDD
2022 Journal Track)
Z. AtashgahiE. MocanuD.C. MocanuR.N.J. Veldhuis
Khoa Kỹ thuật Điện, Toán học và Khoa học Máy tính, Đại học Twente,
Enschede, 7500AE, Hà Lan
E-mail: z.atashgahi@utwente.nl
G.A.Z.N. Sokar1T. Lee2D.C. Mocanu1M. Pechenizkiy1
Đại học Công nghệ Eindhoven, 5600 MB Eindhoven, Hà Lan
1Khoa Toán học và Khoa học Máy tính2Khoa Kỹ thuật Điện
M. Pechenizkiy
Khoa Công nghệ Thông tin, Đại học Jyväskylä, 40014 Jyväskylä, Phần Lan
1Mã nguồn có sẵn tại: https://github.com/zahraatashgahi/QuickSelectionarXiv:2012.00560v2  [cs.LG]  13 Sep 2021

--- TRANG 2 ---
2 Zahra Atashgahi et al.
Từ khóa Lựa chọn đặc trưng Deep LearningAutoencoders thưa Huấn
luyện thưa
1 Giới thiệu
Trong vài năm gần đây, sự chú ý đáng kể đã được dành cho vấn đề giảm
chiều và nhiều phương pháp đã được đề xuất [ 53]. Có hai
kỹ thuật chính để giảm số lượng đặc trưng của một tập dữ liệu chiều cao:
trích xuất đặc trưng và lựa chọn đặc trưng. Trích xuất đặc trưng tập trung vào việc biến đổi
dữ liệu thành không gian chiều thấp hơn. Việc biến đổi này được thực hiện thông qua một ánh
xạ dẫn đến một tập đặc trưng mới [ 40]. Lựa chọn đặc trưng giảm không gian
đặc trưng bằng cách chọn một tập con của các thuộc tính gốc mà không tạo ra đặc trưng mới
[12]. Dựa trên tính sẵn có của nhãn, các phương pháp lựa chọn đặc trưng được chia
thành ba loại: có giám sát [ 2,12], bán giám sát [ 58,48], và không giám sát
[43,16]. Các thuật toán lựa chọn đặc trưng có giám sát cố gắng tối đa hóa một số hàm
của độ chính xác dự đoán với các nhãn lớp. Trong học không giám sát, việc tìm kiếm
các đặc trưng phân biệt được thực hiện một cách mù quáng, không có nhãn lớp. Do đó,
lựa chọn đặc trưng không giám sát được coi là một vấn đề khó hơn nhiều [16].
Các phương pháp lựa chọn đặc trưng cải thiện khả năng mở rộng của các thuật toán học máy
vì chúng giảm chiều của dữ liệu. Ngoài ra, chúng giảm nhu cầu ngày càng tăng
về tài nguyên tính toán và bộ nhớ được đưa ra bởi sự xuất hiện của dữ liệu lớn.
Điều này có thể dẫn đến giảm đáng kể tiêu thụ năng lượng
trong các trung tâm dữ liệu. Điều này có thể giảm bớt không chỉ vấn đề chi phí năng lượng cao trong
trung tâm dữ liệu mà còn các thách thức quan trọng đặt ra cho môi trường [ 56]. Như được nêu
bởi Nhóm Chuyên gia Cấp cao về Trí tuệ Nhân tạo (AI) [ 22], phúc lợi môi trường
là một trong những yêu cầu của hệ thống AI đáng tin cậy. Việc phát triển,
triển khai và quá trình của hệ thống AI nên được đánh giá để đảm bảo rằng chúng
sẽ hoạt động theo cách thân thiện với môi trường nhất có thể. Ví dụ,
việc sử dụng tài nguyên và tiêu thụ năng lượng thông qua huấn luyện có thể được đánh giá.
Tuy nhiên, một vấn đề thách thức phát sinh trong lĩnh vực lựa chọn đặc trưng là
việc chọn đặc trưng từ các tập dữ liệu chứa số lượng lớn đặc trưng và
mẫu, có thể yêu cầu lượng lớn bộ nhớ, tính toán và tài nguyên
năng lượng. Vì hầu hết các kỹ thuật lựa chọn đặc trưng hiện tại được thiết kế để
xử lý dữ liệu quy mô nhỏ, hiệu quả của chúng có thể bị suy giảm với dữ liệu chiều cao
[9]. Chỉ một số nghiên cứu tập trung vào việc thiết kế các thuật toán lựa chọn đặc trưng
hiệu quả về mặt tính toán [ 52,1]. Các đóng góp chính của
bài báo này có thể được tóm tắt như sau:
Chúng tôi đề xuất một phương pháp lựa chọn đặc trưng không giám sát nhanh và mạnh mẽ mới, có tên
QuickSelection. Như được phác thảo ngắn gọn trong Hình 1, nó có hai thành phần chính:
(1) Lấy cảm hứng từ sức mạnh nút trong lý thuyết đồ thị, phương pháp đề xuất sức mạnh neu-
ron của mạng nơ-ron thưa như một tiêu chí để đo tầm quan trọng
đặc trưng; và (2) Phương pháp giới thiệu Denoising Autoencoders kết nối thưa
(sparse DAEs) được huấn luyện từ đầu với quy trình huấn luyện tiến hóa thưa
để mô hình hóa phân phối dữ liệu một cách hiệu quả. Độ thưa được áp đặt
trước huấn luyện cũng giảm lượng bộ nhớ cần thiết và
thời gian chạy huấn luyện.
Chúng tôi triển khai QuickSelection theo cách hoàn toàn thưa trong Python sử dụng
thư viện SciPy và Cython thay vì sử dụng mặt nạ nhị phân trên các kết nối

--- TRANG 3 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 3
Epoch 5
(c) Tính toán
Sức mạnh Neuron(d) Lựa chọn
Đặc trưngs1 = 0.2
s2 = 0.5
s5 = 0.3s4 = 0s3 = 1.3f 1
f 2
f 3
f 4
f 5f 2
f 3
(b) Huấn luyện Sparse-DAE(a) Khởi tạo
Sparse-DAEEpoch 10 Epoch 0
Hình 1: Tổng quan cấp cao về phương pháp được đề xuất, "QuickSelection". (a) Tại
epoch 0, các kết nối được khởi tạo ngẫu nhiên. (b) Sau khi khởi tạo cấu trúc thưa,
chúng ta bắt đầu quy trình huấn luyện. Sau 5 epoch, một số kết nối được
thay đổi trong quá trình huấn luyện, và kết quả là, sức mạnh của một số
neuron đã tăng hoặc giảm. Tại epoch 10, mạng đã hội tụ, và
chúng ta có thể quan sát những neuron nào quan trọng (các vòng tròn xanh lớn và đậm hơn) và
những neuron nào không. (c) Khi mạng đã hội tụ, chúng ta tính toán sức mạnh của tất cả
các neuron đầu vào. (d) Cuối cùng, chúng ta chọn K đặc trưng tương ứng với các neuron có
giá trị sức mạnh cao nhất.
để mô phỏng độ thưa. Điều này đảm bảo yêu cầu tài nguyên tối thiểu, tức là chỉ
Bộ nhớ truy cập ngẫu nhiên (RAM) và Bộ xử lý trung tâm (CPU), mà không
yêu cầu Bộ xử lý đồ họa (GPU).
Các thí nghiệm được thực hiện trên 8 tập dữ liệu chuẩn cho thấy QuickSelec-
tion có một số ưu điểm so với các phương pháp hiện đại, như sau:
Nó là người thực hiện tốt nhất hoặc thứ hai trong cả độ chính xác phân loại và
phân cụm trong hầu hết tất cả các tình huống được xem xét.
Nó là người thực hiện tốt nhất về sự cân bằng giữa độ chính xác phân loại và
phân cụm, thời gian chạy, và yêu cầu bộ nhớ.
Kiến trúc thưa được đề xuất cho lựa chọn đặc trưng có ít nhất một bậc
ít tham số hơn so với tương đương dày đặc. Điều này dẫn đến
thực tế nổi bật rằng thời gian huấn luyện đồng hồ treo tường của QuickSelection chạy
trên CPU nhỏ hơn thời gian huấn luyện đồng hồ treo tường của các đối thủ cạnh tranh dựa trên autoencoder
chạy trên GPU trong hầu hết các trường hợp.
Cuối cùng nhưng không kém phần quan trọng, hiệu quả tính toán của QuickSelection làm cho nó có
tiêu thụ năng lượng tối thiểu trong số các phương pháp lựa chọn đặc trưng dựa trên autoencoder
được xem xét.
2 Công trình liên quan
2.1 Lựa chọn đặc trưng
Tài liệu về lựa chọn đặc trưng cho thấy một loạt các phương pháp có thể được
chia thành ba loại chính, bao gồm các phương pháp filter, wrapper, và embedded.
Các phương pháp Filter sử dụng một tiêu chí xếp hạng để chấm điểm các đặc trưng và sau đó
loại bỏ các đặc trưng có điểm số dưới một ngưỡng. Những tiêu chí này có thể là điểm số Laplacian

--- TRANG 4 ---
4 Zahra Atashgahi et al.
[27], Correlation, Mutual Information [ 12], và nhiều phương pháp chấm điểm khác
như hàm chấm điểm Bayesian, chấm điểm t-test, và các tiêu chí dựa trên lý thuyết thông tin
[33]. Những phương pháp này thường nhanh và hiệu quả về mặt tính toán. Các phương pháp Wrapper
đánh giá các tập con khác nhau của đặc trưng để phát hiện tập con tốt nhất. Các phương pháp Wrapper
thường cho hiệu suất tốt hơn các phương pháp filter; chúng sử dụng một mô hình dự đoán
để chấm điểm từng tập con đặc trưng. Tuy nhiên, điều này dẫn đến độ phức tạp tính toán cao.
Những đóng góp quan trọng cho loại lựa chọn đặc trưng này đã được
thực hiện bởi Kohavi và John [ 30]. Trong [30], các tác giả đã sử dụng cấu trúc cây để
đánh giá các tập con đặc trưng. Các phương pháp Embedded thống nhất quá trình học,
và lựa chọn đặc trưng [ 31]. Multi-Cluster Feature Selection (MCFS) [ 11] là
một phương pháp không giám sát cho lựa chọn đặc trưng embedded, chọn đặc trưng
sử dụng hồi quy quang phổ với chuẩn hóa L1-norm. Một hạn chế chính của
thuật toán này là nó tốn nhiều tính toán vì nó phụ thuộc vào việc tính toán
các eigenvector của ma trận tương tự dữ liệu và sau đó giải một bài toán hồi quy
được chuẩn hóa L1 cho mỗi eigenvector [ 19]. Unsupervised Discriminative Feature
Selection (UDFS) [ 57] là một thuật toán lựa chọn đặc trưng embedded không giám sát khác
sử dụng đồng thời cả thông tin đặc trưng và phân biệt để chọn
đặc trưng [37].
2.2 Autoencoders cho Lựa chọn đặc trưng
Trong vài năm gần đây, nhiều mô hình dựa trên deep learning đã được phát triển để chọn
đặc trưng từ dữ liệu đầu vào sử dụng quy trình học của mạng nơ-ron sâu
[38]. Trong [42], một Multi-Layer Perceptron (MLP) được bổ sung với một lớp kết hợp cặp
để đưa mỗi đặc trưng đầu vào cùng với đối tác knockoff của nó vào mạng.
Sau khi huấn luyện, các tác giả sử dụng trọng số bộ lọc của lớp kết hợp cặp
để xếp hạng các đặc trưng đầu vào. Autoencoders thường được biết đến như một công cụ mạnh mẽ cho
trích xuất đặc trưng [ 8], đang được khám phá để thực hiện lựa chọn đặc trưng không giám sát.
Trong [24], các tác giả kết hợp hồi quy autoencoder và nhiệm vụ group lasso cho lựa chọn
đặc trưng không giám sát có tên AutoEncoder Feature Selector (AEFS). Trong [ 15], một
autoencoder được kết hợp với ba biến thể của chuẩn hóa cấu trúc để thực hiện
lựa chọn đặc trưng không giám sát. Những chuẩn hóa này dựa trên biến slack,
trọng số, và gradient tương ứng. Một phương pháp embedded dựa trên autoencoder được đề xuất gần đây khác
là lựa chọn đặc trưng với Concrete Autoencoder (CAE) [ 5]. Phương pháp này
chọn đặc trưng bằng cách học một phân phối concrete trên các đặc trưng đầu vào.
Họ đề xuất một lớp selector concrete chọn một tổ hợp tuyến tính của các đặc trưng đầu vào
hội tụ thành một tập rời rạc K đặc trưng trong quá trình huấn luyện. Trong [ 49], các
tác giả cho thấy rằng một tập lớn tham số trong CAE có thể dẫn đến over-fitting trong
trường hợp có số lượng mẫu hạn chế. Ngoài ra, CAE có thể chọn đặc trưng
nhiều hơn một lần vì không có tương tác giữa các neuron của lớp selector.
Để giảm thiểu những vấn đề này, họ đề xuất một phương pháp lựa chọn đặc trưng mạng nơ-ron concrete
(FsNet), bao gồm một lớp selector và một mạng nơ-ron sâu có giám sát. Quy trình huấn luyện
của FsNet xem xét việc giảm loss tái tạo và tối đa hóa độ chính xác
phân loại đồng thời. Trong nghiên cứu của chúng tôi, chúng tôi tập trung chủ yếu vào các phương pháp lựa chọn đặc trưng không giám sát.
Denoising Autoencoder (DAE) được giới thiệu để giải quyết vấn đề học
hàm đồng nhất trong autoencoders. Vấn đề này rất có khả năng xảy ra
khi chúng ta có nhiều neuron ẩn hơn đầu vào [ 4]. Kết quả là, đầu ra mạng

--- TRANG 5 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 5
có thể bằng với đầu vào, làm cho autoencoder trở nên vô dụng. DAEs giải quyết
vấn đề nói trên bằng cách đưa nhiễu vào dữ liệu đầu vào và cố gắng
tái tạo đầu vào gốc từ phiên bản nhiễu của nó [ 54]. Kết quả là, DAEs học một
biểu diễn của dữ liệu đầu vào mạnh mẽ với những thay đổi nhỏ không liên quan trong
đầu vào. Trong nghiên cứu này, chúng tôi sử dụng khả năng của loại mạng nơ-ron này để mã hóa
phân phối dữ liệu đầu vào và chọn các đặc trưng quan trọng nhất. Hơn nữa, chúng tôi
thể hiện tác động của việc thêm nhiễu đối với kết quả lựa chọn đặc trưng.
2.3 Huấn luyện thưa
Mạng nơ-ron sâu thường có ít nhất một số lớp kết nối đầy đủ, dẫn đến
một số lượng lớn tham số. Trong không gian chiều cao, điều này không
mong muốn vì có thể gây ra sự giảm đáng kể tốc độ huấn luyện và tăng
yêu cầu bộ nhớ. Để giải quyết vấn đề này, mạng nơ-ron thưa đã
được đề xuất. Cắt tỉa mạng nơ-ron dày đặc là một trong những phương pháp nổi tiếng nhất
để đạt được mạng nơ-ron thưa [ 35,26]. Trong [25], Han et al. bắt đầu từ
một mạng đã được huấn luyện trước, cắt tỉa các trọng số không quan trọng, và huấn luyện lại mạng.
Mặc dù phương pháp này có thể đưa ra một mạng với mức độ thưa mong muốn, chi phí tính toán
tối thiểu vẫn bằng chi phí huấn luyện một mạng dày đặc. Để
giảm chi phí này, Lee et al. [ 36] bắt đầu với một mạng nơ-ron dày đặc, và cắt tỉa nó trước
khi huấn luyện dựa trên độ nhạy kết nối; sau đó, mạng thưa được huấn luyện theo
cách tiêu chuẩn. Tuy nhiên, bắt đầu từ một mạng nơ-ron dày đặc yêu cầu ít nhất
kích thước bộ nhớ của mạng nơ-ron dày đặc và tài nguyên tính toán cho
một lần lặp huấn luyện của mạng dày đặc. Do đó, phương pháp này có thể không
phù hợp cho các thiết bị tài nguyên thấp.
Vào năm 2016, Mocanu et al. [ 44] đã giới thiệu ý tưởng huấn luyện mạng nơ-ron thưa
từ đầu, một khái niệm gần đây bắt đầu được biết đến như huấn luyện thưa. Mẫu
kết nối thưa đã được cố định trước khi huấn luyện sử dụng lý thuyết đồ thị, khoa học mạng,
và thống kê dữ liệu. Mặc dù nó cho thấy kết quả đầy hứa hẹn,
vượt trội hơn đối tác dày đặc, mẫu độ thưa tĩnh không phải lúc nào cũng
mô hình hóa dữ liệu một cách tối ưu. Để giải quyết những vấn đề này, vào năm 2018, Mocanu et al.
[45] đã đề xuất thuật toán Sparse Evolutionary Training (SET) sử dụng
độ thưa động trong quá trình huấn luyện. Ý tưởng là bắt đầu với một mạng nơ-ron thưa
trước khi huấn luyện và thay đổi động các kết nối của nó trong quá trình huấn luyện để
tự động mô hình hóa phân phối dữ liệu. Điều này dẫn đến sự
giảm đáng kể số lượng tham số và tăng hiệu suất. SET phát triển các
kết nối thưa tại mỗi epoch huấn luyện bằng cách loại bỏ một phần kết nối
có độ lớn nhỏ nhất, và thêm ngẫu nhiên các kết nối mới trong mỗi lớp.
Bourgin et al. [ 10] đã chỉ ra rằng một MLP thưa được huấn luyện với SET đạt được kết quả
tốt nhất trên dữ liệu dạng bảng trong việc dự đoán quyết định của con người, vượt trội hơn
mạng nơ-ron kết nối đầy đủ và Random Forest, trong số những cái khác.
Trong công trình này, chúng tôi giới thiệu lần đầu tiên huấn luyện thưa trong thế giới
denoising autoencoders, và chúng tôi đặt tên cho mô hình mới được giới thiệu là sparse denoising
autoencoder (sparse DAE). Chúng tôi huấn luyện sparse DAE với thuật toán SET để
giữ số lượng tham số thấp, trong quá trình huấn luyện. Sau đó, chúng tôi khai thác
mạng đã huấn luyện để chọn các đặc trưng quan trọng nhất.

--- TRANG 6 ---
6 Zahra Atashgahi et al.
3 Phương pháp được đề xuất
Để giải quyết vấn đề về chiều cao của dữ liệu, chúng tôi đề xuất một
phương pháp mới, có tên "QuickSelection", để chọn các thuộc tính thông tin nhất từ
dữ liệu, dựa trên sức mạnh (tầm quan trọng) của chúng. Tóm lại, chúng tôi huấn luyện một mạng sparse denoising
autoencoder từ đầu theo cách thích ứng không giám sát. Sau đó, chúng tôi
sử dụng mạng đã huấn luyện để dẫn xuất sức mạnh của mỗi neuron trong các đặc trưng đầu vào.
Ý tưởng cơ bản của phương pháp được đề xuất của chúng tôi là áp đặt các kết nối thưa trên
DAE, đã chứng minh thành công trong lĩnh vực liên quan của trích xuất đặc trưng, để
xử lý hiệu quả độ phức tạp tính toán của dữ liệu chiều cao về mặt tài nguyên
bộ nhớ. Các kết nối thưa được phát triển theo cách thích ứng giúp
xác định các đặc trưng thông tin.
Một số phương pháp đã được đề xuất để huấn luyện mạng nơ-ron sâu
từ đầu sử dụng các kết nối thưa và huấn luyện thưa [ 14,45,7,46,17,59]. Tất cả
những phương pháp này được triển khai sử dụng mặt nạ nhị phân trên các kết nối để mô phỏng
độ thưa vì tất cả các thư viện deep learning tiêu chuẩn và phần cứng (ví dụ: GPU) không
được tối ưu hóa cho các phép toán ma trận trọng số thưa. Không giống như các phương pháp đã nêu,
chúng tôi triển khai phương pháp được đề xuất của chúng tôi theo cách hoàn toàn thưa để đạt mục tiêu
thực sự sử dụng các ưu điểm của một số lượng rất nhỏ tham số trong quá trình huấn luyện.
Chúng tôi quyết định sử dụng SET trong việc huấn luyện sparse DAE của chúng tôi.
Lựa chọn SET là do đặc điểm mong muốn của nó. SET là một phương pháp đơn giản
nhưng đạt hiệu suất thỏa đáng. Không giống như các phương pháp khác tính toán và
lưu trữ thông tin cho tất cả trọng số mạng, bao gồm cả những cái không tồn tại, SET
hiệu quả về bộ nhớ. Nó chỉ lưu trữ trọng số cho các kết nối thưa hiện có.
Nó không cần bất kỳ độ phức tạp tính toán cao nào vì quy trình tiến hóa
chỉ phụ thuộc vào độ lớn của các kết nối hiện có. Đây là một lợi thế
thuận lợi cho phương pháp được đề xuất của chúng tôi để chọn các đặc trưng thông tin một cách nhanh chóng. Trong
các phần tiếp theo, chúng tôi trước tiên trình bày cấu trúc của mạng sparse denoising
autoencoder được đề xuất và sau đó giải thích phương pháp lựa chọn đặc trưng. Mã giả
của phương pháp được đề xuất của chúng tôi có thể được tìm thấy trong Thuật toán 1.
3.1 Sparse DAE
Cấu trúc Vì mục tiêu của phương pháp được đề xuất của chúng tôi là thực hiện lựa chọn đặc trưng nhanh theo cách
hiệu quả về bộ nhớ, chúng tôi xem xét ở đây mô hình với số lượng ít nhất có thể
các lớp ẩn, một lớp ẩn, vì nhiều lớp có nghĩa là nhiều tính toán hơn. Ban đầu,
các kết nối thưa giữa hai lớp neuron liên tiếp được khởi tạo với
một đồ thị ngẫu nhiên Erdős–Rényi, trong đó xác suất kết nối giữa
hai neuron được cho bởi
P(Wl
ij) =(nl1+nl)
nl1nl; (1)
trong đó ε biểu thị tham số điều khiển mức độ thưa, nl biểu thị số
neuron tại lớp l, và Wl
ij là kết nối giữa neuron i trong lớp l1 và
neuron j trong lớp l, được lưu trữ trong ma trận trọng số thưa Wl.
Khử nhiễu đầu vào Chúng tôi sử dụng mô hình nhiễu cộng để làm hỏng dữ liệu gốc:
ex=x+nfN(; 2); (2)

--- TRANG 7 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 7
trong đó x là vector dữ liệu đầu vào từ tập dữ liệu X, nf (hệ số nhiễu) là một siêu tham số
của mô hình xác định mức độ hỏng, và N(; 2) là
nhiễu Gaussian. Sau khi khử nhiễu dữ liệu, chúng tôi dẫn xuất biểu diễn ẩn
h sử dụng đầu vào bị hỏng này. Sau đó, đầu ra z được tái tạo từ biểu
diễn ẩn. Chính thức, biểu diễn ẩn h và đầu ra z được
tính toán như sau:
h=a(W1ex+b1); (3)
z=a(W2h+b2); (4)
trong đó W1 và W2 là các ma trận trọng số thưa của lớp ẩn và đầu ra
tương ứng, b1 và b2 là các vector bias của lớp tương ứng của chúng, và a là
hàm kích hoạt của mỗi lớp. Mục tiêu của mạng của chúng tôi là tái tạo
các đặc trưng gốc trong đầu ra. Vì lý do này, chúng tôi sử dụng mean squared error
(MSE) làm hàm loss để đo sự khác biệt giữa các đặc trưng gốc x
và đầu ra tái tạo z:
LMSE =kzxk2
2: (5)
Cuối cùng, các trọng số có thể được tối ưu hóa sử dụng các thuật toán huấn luyện tiêu chuẩn
(ví dụ: Stochastic Gradient Descent (SGD), AdaGrad, và Adam) với sai số tái tạo
ở trên.
Quy trình huấn luyện Chúng tôi thích ứng quy trình huấn luyện SET [ 45] trong việc huấn luyện
mạng được đề xuất của chúng tôi cho lựa chọn đặc trưng. SET hoạt động như sau. Sau mỗi epoch huấn luyện,
một phần của các trọng số dương nhỏ nhất và một phần của các trọng số âm lớn nhất
tại mỗi lớp được loại bỏ. Việc lựa chọn dựa trên độ lớn
của các trọng số. Các kết nối mới với cùng số lượng như các kết nối đã loại bỏ được
thêm ngẫu nhiên trong mỗi lớp. Do đó tổng số kết nối trong mỗi
lớp vẫn giữ nguyên, trong khi số kết nối trên mỗi neuron thay đổi, như
được biểu diễn trong Hình 1. Các trọng số của những kết nối mới này được khởi tạo từ
một phân phối chuẩn tiêu chuẩn. Việc thêm ngẫu nhiên các kết nối mới không
có rủi ro cao về việc không tìm thấy một kết nối thưa tốt ở cuối quá trình huấn luyện
vì đã được chứng minh trong [ 41] rằng huấn luyện thưa có thể tiết lộ một số lượng lớn
các cực tiểu địa phương kết nối thưa rất khác nhau đạt được hiệu suất rất
tương tự.
3.2 Lựa chọn đặc trưng
Chúng tôi chọn các đặc trưng quan trọng nhất của dữ liệu dựa trên trọng số của
các neuron đầu vào tương ứng của sparse DAE đã huấn luyện. Lấy cảm hứng từ sức mạnh nút
trong lý thuyết đồ thị [ 6], chúng tôi xác định tầm quan trọng của mỗi neuron dựa trên sức mạnh
của nó. Chúng tôi ước tính sức mạnh của mỗi neuron ( si) bằng tổng
các trọng số tuyệt đối của các kết nối đi ra của nó.
si=n1X
j=1jW1
ijj; (6)
trong đó n1 là số neuron của lớp ẩn đầu tiên, và W1
ij biểu thị
trọng số của kết nối liên kết neuron đầu vào i với neuron ẩn j.

--- TRANG 8 ---
8 Zahra Atashgahi et al.
  Khởi tạo ngẫu nhiên  Sau 3 epoch  Sau 6 Epoch  Sau 10 epochs    chiều cao (pixel)  
     
Sức mạnh   chiều rộng (pixel)  chiều rộng (pixel)  chiều rộng (pixel)  chiều rộng (pixel)    
Hình 2: Sức mạnh neuron trên tập dữ liệu MNIST. Các bản đồ nhiệt ở trên là biểu diễn 2D
của sức mạnh neuron đầu vào. Có thể quan sát thấy rằng sức mạnh
của các neuron là ngẫu nhiên khi bắt đầu huấn luyện. Sau một vài epoch, mẫu
thay đổi, và các neuron ở trung tâm trở nên quan trọng hơn và tương tự như
mẫu dữ liệu MNIST.
Như được biểu diễn trong Hình 1, sức mạnh của các neuron đầu vào thay đổi trong
quá trình huấn luyện; chúng tôi đã mô tả sức mạnh của các neuron theo kích thước và
màu sắc của chúng. Sau khi hội tụ, chúng tôi tính toán sức mạnh cho tất cả các neuron đầu vào; mỗi
neuron đầu vào tương ứng với một đặc trưng. Sau đó, chúng tôi chọn các đặc trưng tương ứng
với các neuron có K giá trị sức mạnh lớn nhất:
F
s= argmax
FsF;jFsj=kX
fi2Fssi; (7)
trong đó F và F
s là tập đặc trưng gốc và tập đặc trưng được chọn cuối cùng tương ứng,
fi là đặc trưng thứ i của F, và K là số đặc trưng cần được chọn. Ngoài ra,
bằng cách sắp xếp tất cả các đặc trưng dựa trên sức mạnh của chúng, chúng tôi sẽ dẫn xuất tầm quan trọng
của tất cả các đặc trưng trong tập dữ liệu. Tóm lại, chúng tôi sẽ có thể xếp hạng tất cả các đặc trưng đầu vào bằng
cách huấn luyện chỉ một lần một mô hình sparse DAE duy nhất.
Thuật toán 1 QuickSelection
1:Đầu vào:Tập dữ liệu X, hệ số nhiễu nf, siêu tham số độ thưa ε, số neuron ẩn
nh, số đặc trưng được chọn K
2: Khử nhiễu đầu vào: ex=x+nfN(; 2)
3:Khởi tạo cấu trúc: Khởi tạo sparse-DAE với nh neuron ẩn và mức độ thưa
được xác định bởi ε
4:quy trình Huấn luyện sparse-DAE
5: Đặt loss là LMSE =kzxk2
2 trong đó z là đầu ra của sparse-DAE
6: cho i∈{1;:::;epochs} do
7: Thực hiện lan truyền tiến và lan truyền ngược tiêu chuẩn
8: Thực hiện loại bỏ và thêm trọng số để tối ưu hóa cấu trúc
9:quy trình QuickSelection
10: Tính toán sức mạnh neuron:
11: cho i∈{1;:::; #inputfeatures} do
12:si=nhP
j=1jW1
ijj
13: Chọn K đặc trưng: F
s= argmaxFsF;jFsj=KP
fi2Fssi;

--- TRANG 9 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 9
Để hiểu sâu hơn về quy trình trên, chúng tôi phân tích sức mạnh của
mỗi neuron đầu vào trong bản đồ 2D trên tập dữ liệu MNIST. Điều này được minh họa trong Hình
2. Khi bắt đầu huấn luyện, tất cả các neuron có sức mạnh nhỏ do việc khởi tạo ngẫu nhiên
của mỗi trọng số thành một giá trị nhỏ. Trong quá trình phát triển mạng,
các kết nối mạnh hơn được liên kết với các đặc trưng quan trọng dần dần. Chúng ta có thể quan sát
rằng sau mười epoch, các neuron ở trung tâm bản đồ trở nên mạnh hơn. Mẫu
này tương tự như mẫu của dữ liệu MNIST trong đó hầu hết các chữ số xuất hiện
ở giữa bức tranh.
Chúng tôi đã nghiên cứu các metric khác để ước tính tầm quan trọng neuron như
sức mạnh của các neuron đầu ra, bậc của các neuron đầu vào và đầu ra, và sức mạnh và
bậc của các neuron đồng thời. Tuy nhiên, trong các thí nghiệm của chúng tôi, tất cả những phương pháp này
đã bị vượt trội bởi sức mạnh của các neuron đầu vào về mặt độ chính xác
và tính ổn định.
4 Thí nghiệm
Để xác minh tính hợp lệ của phương pháp được đề xuất của chúng tôi, chúng tôi thực hiện một số thí
nghiệm. Trong phần này, trước tiên, chúng tôi nêu các cài đặt của thí nghiệm, bao gồm
siêu tham số và tập dữ liệu. Sau đó, chúng tôi thực hiện lựa chọn đặc trưng với QuickSe-
lection và so sánh kết quả với các phương pháp khác, bao gồm MCFS, Laplacian
Score, và ba phương pháp lựa chọn đặc trưng dựa trên autoencoder. Sau đó, chúng tôi thực hiện
các phân tích khác nhau về QuickSelection để hiểu hành vi của nó. Cuối cùng, chúng tôi thảo luận
về khả năng mở rộng của QuickSelection và so sánh nó với các phương pháp khác được xem xét.
4.1 Cài đặt
Cài đặt thí nghiệm, bao gồm các giá trị của siêu tham số, chi tiết triển khai,
cấu trúc của sparse DAE, các tập dữ liệu chúng tôi sử dụng để đánh giá, và
metric đánh giá, như sau.
4.1.1 Siêu tham số và Triển khai
Đối với lựa chọn đặc trưng, chúng tôi xem xét trường hợp của sparse DAE đơn giản nhất với một
lớp ẩn gồm 1000 neuron. Lựa chọn này được thực hiện do mục tiêu chính của chúng tôi
để giảm độ phức tạp mô hình và số lượng tham số. Hàm kích hoạt
được sử dụng cho các neuron lớp ẩn và đầu ra là "Sigmoid" và
"Linear" tương ứng, ngoại trừ tập dữ liệu Madelon nơi chúng tôi sử dụng "Tanh" cho hàm kích hoạt
đầu ra. Chúng tôi huấn luyện mạng với SGD và tốc độ học
0:01. Siêu tham số ζ, phần trọng số được loại bỏ trong quy trình SET,
là 0:2. Ngoài ra, ε, xác định mức độ thưa, được đặt là 13. Chúng tôi đặt
hệ số nhiễu ( nf) là 0.2 trong các thí nghiệm. Để cải thiện quá trình học của
mạng của chúng tôi, chúng tôi chuẩn hóa các đặc trưng của tập dữ liệu sao cho mỗi thuộc tính
có mean bằng không và variance đơn vị. Tuy nhiên, đối với các tập dữ liệu SMK và PCMAC, chúng tôi
sử dụng Min-Max scaling. Phương pháp tiền xử lý cho mỗi tập dữ liệu được xác định
bằng một thí nghiệm nhỏ của hai phương pháp tiền xử lý.

--- TRANG 10 ---
10 Zahra Atashgahi et al.
Chúng tôi triển khai sparse DAE và QuickSelection2 theo cách hoàn toàn thưa trong
Python, sử dụng thư viện Scipy [ 28] và Cython. Chúng tôi so sánh phương pháp được đề xuất của chúng tôi
với MCFS, Laplacian score (LS), AEFS, và CAE, đã được đề cập
trong Phần 2. Chúng tôi cũng thực hiện một số thí nghiệm với UDFS; tuy nhiên, vì
chúng tôi không thể thu được nhiều kết quả trong giới hạn thời gian được xem xét (24
giờ), chúng tôi không bao gồm kết quả trong bài báo. Chúng tôi đã sử dụng kho scikit-feature
cho việc triển khai MCFS, và Laplacian score [ 37]. Ngoài ra, chúng tôi
sử dụng việc triển khai lựa chọn đặc trưng với CAE và AEFS từ Github3.
Ngoài ra, để làm nổi bật các ưu điểm của việc sử dụng các lớp thưa, chúng tôi so sánh
kết quả của chúng tôi với một autoencoder kết nối đầy đủ (FCAE) sử dụng sức mạnh neuron như một
thước đo tầm quan trọng của mỗi đặc trưng. Để có so sánh công bằng, cấu trúc
của mạng này được xem xét tương tự như DAE của chúng tôi, một lớp ẩn chứa 1000
neuron được triển khai sử dụng TensorFlow. Hơn nữa, chúng tôi đã nghiên cứu tác động
của các thành phần khác của QuickSelection, bao gồm khử nhiễu đầu vào và thuật toán huấn luyện SET,
trong Phụ lục B.1 và F, tương ứng.
Đối với tất cả các phương pháp khác (ngoại trừ FCAE mà tất cả các siêu tham số
và tiền xử lý đều tương tự như QuickSelection), chúng tôi chia tỷ lệ dữ liệu giữa
không và một, vì nó mang lại hiệu suất tốt hơn so với chuẩn hóa dữ liệu cho
những phương pháp này. Các siêu tham số của các phương pháp đã nêu đã được
đặt tương tự như các giá trị được báo cáo trong mã hoặc bài báo tương ứng. Đối với AEFS,
chúng tôi điều chỉnh siêu tham số chuẩn hóa giữa 0:0001 và 1000, vì phương pháp này
nhạy cảm với giá trị này. Chúng tôi thực hiện thí nghiệm của chúng tôi trên một lõi CPU duy nhất,
Intel Xeon Processor E5 v4, và đối với các phương pháp yêu cầu GPU, chúng tôi sử dụng
NVIDIA TESLA P100.
4.1.2 Tập dữ liệu
Chúng tôi đánh giá hiệu suất của phương pháp được đề xuất của chúng tôi trên tám tập dữ liệu, bao gồm
năm tập dữ liệu chiều thấp và ba tập chiều cao. Bảng 1 minh họa
các đặc tính của những tập dữ liệu này.
– COIL-20 [47] bao gồm 1440 hình ảnh được chụp từ 20 đối tượng ( 72 tư thế cho mỗi
đối tượng).
– Madelon [23] là một tập dữ liệu nhân tạo với 5 đặc trưng thông tin và 15 tổ hợp tuyến tính
của chúng. Phần còn lại của các đặc trưng là các đặc trưng làm mất tập trung vì chúng
không có khả năng dự đoán.
– Human Activity Recognition (HAR) [3] được tạo bằng cách thu thập các quan
sát của 30 đối tượng thực hiện 6 hoạt động như đi bộ, đứng, và
ngồi. Dữ liệu được ghi lại bởi một điện thoại thông minh được kết nối với cơ thể của đối tượng.
– Isolet [18] đã được tạo với tên được nói của mỗi chữ cái của bảng chữ cái
tiếng Anh.
– MNIST [34] là cơ sở dữ liệu của các hình ảnh 28x28 của các chữ số viết tay.
– SMK-CAN-187 [50] là một tập dữ liệu biểu hiện gen với 19993 đặc trưng. Tập dữ liệu này
so sánh người hút thuốc có và không có ung thư phổi.
2Việc triển khai QuickSelection có sẵn tại: https://github.com/
zahraatashgahi/QuickSelection
3Việc triển khai AEFS và CAE có sẵn tại: https://github.com/mfbalin/
Concrete-Autoencoders

--- TRANG 11 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 11
Bảng 1: Đặc tính tập dữ liệu.
Tập dữ liệu Chiều Loại Mẫu Huấn luyện Kiểm tra Lớp
Coil20 1024 Hình ảnh 1440 1152 288 20
Isolet 617 Giọng nói 7737 6237 1560 26
HAR 561 Chuỗi thời gian 10299 7352 2947 6
Madelon 500 Nhân tạo 2600 2000 600 2
MNIST 784 Hình ảnh 70000 60000 10000 10
SMK-CAN-187 19993 Microarray 187 149 38 2
GLA-BRA-180 49151 Microarray 180 144 36 4
PCMAC 3289 Văn bản 1943 1554 389 2
– GLA-BRA-180 [51] bao gồm hồ sơ biểu hiện của Stem cell factor hữu ích
để xác định angiogenesis khối u.
– PCMAC [32] là một tập con của dữ liệu 20 Newsgroups.
4.1.3 Metric đánh giá
Để đánh giá mô hình của chúng tôi, chúng tôi tính toán hai metric: độ chính xác phân cụm và độ chính xác phân
loại. Để dẫn xuất độ chính xác phân cụm [ 37], trước tiên, chúng tôi thực hiện K-means sử dụng
tập con của tập dữ liệu tương ứng với các đặc trưng được chọn và nhận các nhãn cụm.
Sau đó, chúng tôi tìm sự khớp tốt nhất giữa nhãn lớp và nhãn cụm
và báo cáo độ chính xác phân cụm. Chúng tôi lặp lại thuật toán K-means 10 lần
và báo cáo kết quả phân cụm trung bình vì K-means có thể hội tụ đến một cực trị
địa phương.
Để tính toán độ chính xác phân loại, chúng tôi sử dụng một mô hình phân loại có giám sát
có tên "Extremely randomized trees" (ExtraTrees), đây là một phương pháp học ensemble
phù hợp với một số cây quyết định ngẫu nhiên trên các phần khác nhau của dữ liệu [ 21].
Lựa chọn phương pháp phân loại được thực hiện do tính hiệu quả tính toán
của bộ phân loại ExtraTrees. Để tính toán độ chính xác phân loại, trước tiên, chúng tôi dẫn xuất
K đặc trưng được chọn sử dụng mỗi phương pháp lựa chọn đặc trưng được xem xét. Sau đó, chúng tôi huấn luyện
bộ phân loại ExtraTrees với 50 cây như các estimator trên K đặc trưng được chọn của
tập huấn luyện. Cuối cùng, chúng tôi tính toán độ chính xác phân loại trên dữ liệu kiểm tra
chưa thấy. Đối với các tập dữ liệu không chứa tập kiểm tra, chúng tôi chia dữ liệu thành
tập huấn luyện và kiểm tra, bao gồm 80% tổng mẫu gốc cho tập huấn luyện
và 20% còn lại cho tập kiểm tra. Ngoài ra, chúng tôi đã đánh giá độ chính xác
phân loại của lựa chọn đặc trưng sử dụng bộ phân loại random forest [ 39] trong
Phụ lục G.
4.2 Lựa chọn đặc trưng
Chúng tôi chọn 50 đặc trưng từ mỗi tập dữ liệu trừ Madelon, cho tập này chúng tôi chọn chỉ
20 đặc trưng vì hầu hết các đặc trưng của nó là nhiễu không thông tin. Sau đó, chúng tôi tính toán
độ chính xác phân cụm và phân loại trên tập con đặc trưng được chọn; càng nhiều đặc trưng thông tin được chọn,
độ chính xác càng cao sẽ đạt được. Kết quả độ chính xác phân cụm và
phân loại của mô hình của chúng tôi và các phương pháp khác
được tóm tắt trong Bảng 2 và 3, tương ứng. Những kết quả này là trung bình của 5 lần chạy
cho mỗi trường hợp. Đối với các phương pháp lựa chọn đặc trưng dựa trên autoencoder, bao gồm
CAE, AEFS, và FCAE, chúng tôi xem xét 100 epoch huấn luyện. Tuy nhiên, chúng tôi trình bày

--- TRANG 12 ---
12 Zahra Atashgahi et al.
Bảng 2: Độ chính xác phân cụm (%) sử dụng 50 đặc trưng được chọn (ngoại trừ Madelon
cho tập này chúng tôi chọn 20 đặc trưng). Trên mỗi tập dữ liệu, mục in đậm là người thực hiện tốt nhất,
và mục in nghiêng là người thực hiện tốt thứ hai.
Phương pháp COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS67.00.7 33.80.562.40.057.20.0 35.2 0 51.6 0.265.80.350.60.0
LS 55.5 0.4 33.2 0.2 61.20.0 58.1 0.014.90.1 51.6 0.4 55.5 0.4 50.6 0.0
CAE 60.0 1.1 31.6 1.3 51.4 0.4 56.9 3.649.21.5 60.7 0.455.41.3 52.01.2
AEFS 51.2 1.7 31.0 2.7 55.0 2.2 50.8 0.2 40.0 1.9 52.4 1.8 56.1 5.2 50.9 0.5
FCAE 60.21.728.72.5 49.5 8.7 50.9 0.4 28.2 8.5 51.5 0.8 53.5 3.0 50.9 0.1
QS1059.52.1 32.5 2.8 56.0 2.6 57.5 3.8 45.4 3.9 54.03.153.64.7 50.9 0.5
QS100 60.22.035.12.754.64.558.21.5 48.32.451.80.8 59.51.852.51.1
QSbest63.81.5 42.2 2.6 59.5 4.3 58.6 0.9 48.3 2.4 54.9 1.39 59.5 1.8 53.1 0
Bảng 3: Độ chính xác phân loại (%) sử dụng 50 đặc trưng được chọn (ngoại trừ Madelon
cho tập này chúng tôi chọn 20 đặc trưng). Trên mỗi tập dữ liệu, mục in đậm là người thực hiện tốt nhất,
và mục in nghiêng là người thực hiện tốt thứ hai.
Phương pháp COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS 99.2 0.3 79.5 0.4 88.9 0.3 81.7 0.8 88.7 0 75.8 1.5 70.6 3.8 55.5 0.0
LS 89.8 0.4 83.0 0.2 86.4 0.491.40.920.70.1 71.6 5.6 71.7 1.1 50.4 0.0
CAE 99.60.389.80.6 91.7 1.087.52.095.40.171.63.1 70.0 4.159.91.5
AEFS 93.0 2.7 85.1 2.4 87.7 1.4 52.1 2.8 86.1 2.0 76.34.468.93.7 57.1 3.6
FCAE99.70.281.65.9 87.4 2.4 53.5 8.1 68.8 28.7 71.6 3.5 72.84.858.11.9
QS1098.80.6 86.9 1.1 88.8 0.7 86.6 3.6 93.80.676.94.669.43.0 58.94.4
QS10099.70.3 89.01.3 90.2 1.2 90.3 0.793.50.5 75.7 3.973.33.358.02.9
QSbest99.70.3 89.0 1.3 90.5 1.6 90.9 0.5 94.2 0.5 81.6 2.9 73.3 3.3 61.3 6.1
kết quả của QuickSelection tại epoch 10 và 100 có tên QuickSelection 10 và
QuickSelection 100, tương ứng. Điều này chủ yếu do thực tế là phương pháp được đề xuất của chúng tôi
có thể đạt được độ chính xác hợp lý sau vài epoch đầu tiên. Hơn nữa,
chúng tôi thực hiện điều chỉnh siêu tham số cho ζ và ε sử dụng phương pháp grid search trên
một số lượng hạn chế các giá trị cho tất cả các tập dữ liệu; kết quả tốt nhất được trình bày trong Bảng 2
và 3 như QuickSelection best. Kết quả của việc chọn siêu tham số có thể được tìm thấy
trong Phụ lục B.2. Tuy nhiên, chúng tôi không thực hiện tối ưu hóa siêu tham số cho
các phương pháp khác (ngoại trừ AEFS). Do đó, để có so sánh công bằng
giữa tất cả các phương pháp, chúng tôi không so sánh kết quả của QuickSelection best với
các phương pháp khác.
Từ Bảng 2, có thể quan sát thấy rằng QuickSelection vượt trội hơn tất cả các phương pháp khác
trên Isolet, Madelon, và PCMAC, về độ chính xác phân cụm, trong khi
là người thực hiện tốt thứ hai trên Coil20, MNIST, SMK, và GLA. Hơn nữa,
Trên tập dữ liệu HAR, nó là người thực hiện tốt nhất trong tất cả các phương pháp lựa chọn đặc trưng dựa trên autoencoder
được xem xét. Như được hiển thị trong Bảng 3, QuickSelection vượt trội hơn
tất cả các phương pháp khác trên Coil20, SMK, và GLA, về độ chính xác phân
loại, trong khi là người thực hiện tốt thứ hai trên các tập dữ liệu khác. Từ những
Bảng này, rõ ràng rằng QuickSelection có thể vượt trội hơn mạng dày đặc tương đương của nó
(FCAE) về độ chính xác phân loại và phân cụm trên tất cả các tập dữ liệu.

--- TRANG 13 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 13
0 100 200 300 400 500
# of features removed80859095100Accuracy (%)
Least to most important
0 100 200 300 400 500
# of features removed020406080100Accuracy (%)
Most to least important
Hình 3: Ảnh hưởng của việc loại bỏ đặc trưng trên tập dữ liệu Madelon. Sau khi dẫn xuất tầm quan
trọng của các đặc trưng với QuickSelection, chúng tôi sắp xếp và sau đó loại bỏ chúng dựa trên
hai phương pháp trên.
Có thể quan sát trong Bảng 2 và 3, rằng Lap_score có hiệu suất kém
khi số lượng mẫu lớn (ví dụ: MNIST). Tuy nhiên, trong các nhiệm vụ có
số lượng mẫu và đặc trưng thấp, ngay cả trong môi trường nhiễu như Madelon,
Lap_score có hiệu suất tương đối tốt. Ngược lại, CAE có hiệu suất kém
trong môi trường nhiễu (ví dụ: Madelon), trong khi nó có độ chính xác phân loại khá tốt
trên các tập dữ liệu khác được xem xét. Nó là người thực hiện tốt nhất hoặc thứ hai
trên năm tập dữ liệu, về độ chính xác phân loại, khi K= 50. AEFS và
FCAE cũng không thể đạt hiệu suất tốt trên Madelon. Chúng tôi tin rằng
các lớp dày đặc là nguyên nhân chính của hành vi này; các kết nối dày đặc cố gắng
học tất cả các đặc trưng đầu vào, ngay cả các đặc trưng nhiễu. Do đó, chúng không thể phát hiện
các thuộc tính quan trọng nhất của dữ liệu. MCFS hoạt động khá tốt trên hầu hết
các tập dữ liệu về độ chính xác phân cụm. Điều này do mục tiêu chính của
MCFS để bảo tồn cấu trúc đa cụm của dữ liệu. Tuy nhiên, phương pháp này
cũng có hiệu suất kém trên các tập dữ liệu với số lượng lớn mẫu (ví dụ:
MNIST) và đặc trưng nhiễu (ví dụ: Madelon).
Tuy nhiên, vì đánh giá các phương pháp chỉ sử dụng một giá trị duy nhất của K có thể không
đủ để so sánh, chúng tôi đã thực hiện một thí nghiệm khác sử dụng các giá trị khác nhau của
K. Trong Phụ lục A.1, chúng tôi kiểm tra các giá trị khác cho K trên tất cả các tập dữ liệu, và so sánh
các phương pháp về độ chính xác phân loại, độ chính xác phân cụm, thời gian chạy, và
sử dụng bộ nhớ tối đa. Tóm tắt kết quả của Phụ lục này đã được
tóm tắt trong Phần 5.1.
4.2.1 Tính liên quan của các đặc trưng được chọn
Để minh họa khả năng của QuickSelection trong việc tìm các đặc trưng thông tin, chúng tôi
phân tích kỹ lưỡng kết quả tập dữ liệu Madelon, có tính chất thú vị
chứa nhiều đặc trưng nhiễu. Chúng tôi thực hiện các thí nghiệm sau; trước tiên,
chúng tôi sắp xếp các đặc trưng dựa trên sức mạnh của chúng. Sau đó, chúng tôi loại bỏ các đặc trưng từng cái một
từ đặc trưng ít quan trọng nhất đến quan trọng nhất. Trong mỗi
bước, chúng tôi huấn luyện một bộ phân loại ExtraTrees với các đặc trưng còn lại. Chúng tôi lặp lại thí nghiệm này
bằng cách loại bỏ đặc trưng từ những cái quan trọng nhất đến ít quan trọng nhất.
Kết quả của độ chính xác phân loại cho cả hai thí nghiệm có thể
được thấy trong Hình 3. Ở phía bên trái của Hình 3, chúng ta có thể quan sát rằng việc loại bỏ
các đặc trưng ít quan trọng nhất, đó là nhiễu, làm tăng độ chính xác. Độ chính xác tối đa
xảy ra sau khi chúng ta loại bỏ 480 đặc trưng nhiễu. Điều này tương ứng với thời điểm

--- TRANG 14 ---
14 Zahra Atashgahi et al.
      
      
 
Hình 4: Giá trị trung bình của tất cả các mẫu dữ liệu của mỗi lớp tương ứng với 50
đặc trưng được chọn trên MNIST sau 100 epoch huấn luyện (dưới), cùng với
trung bình của các mẫu dữ liệu thực tế của mỗi lớp (trên).
khi tất cả các đặc trưng nhiễu được cho là đã được loại bỏ. Trong Hình 3 (bên phải), có thể
thấy rằng việc loại bỏ các đặc trưng theo thứ tự ngược lại dẫn đến sự giảm đột ngột
trong độ chính xác phân loại. Sau khi loại bỏ 20 đặc trưng (được chỉ ra bởi
đường xanh dọc), bộ phân loại hoạt động như một bộ phân loại ngẫu nhiên. Chúng tôi kết luận
rằng QuickSelection có thể tìm các đặc trưng thông tin nhất theo thứ tự tốt.
Để thể hiện tốt hơn tính liên quan của các đặc trưng được tìm thấy bởi QuickSelection, chúng tôi
hiển thị 50 đặc trưng được chọn trên tập dữ liệu MNIST cho mỗi lớp, bằng cách tính trung bình
các giá trị tương ứng của chúng từ tất cả các mẫu dữ liệu thuộc về một lớp. Như có thể
quan sát trong Hình 4, hình dạng kết quả giống với các mẫu thực tế của
chữ số tương ứng. Chúng tôi thảo luận kết quả của tất cả các lớp tại
các epoch huấn luyện khác nhau chi tiết hơn trong Phụ lục C.
5 Thảo luận
5.1 Cân bằng giữa Độ chính xác và Hiệu quả Tính toán
Trong phần này, chúng tôi thực hiện so sánh kỹ lưỡng giữa các mô hình về
thời gian chạy, tiêu thụ năng lượng, yêu cầu bộ nhớ, độ chính xác phân cụm, và
độ chính xác phân loại. Tóm lại, chúng tôi thay đổi số lượng đặc trưng cần được chọn
(K) và đo độ chính xác, thời gian chạy, và sử dụng bộ nhớ tối đa trên
tất cả các phương pháp. Sau đó, chúng tôi tính toán hai điểm số để tóm tắt kết quả và so sánh
các phương pháp.
Chúng tôi phân tích tác động của việc thay đổi K đối với hiệu suất QuickSelection và so sánh
với các phương pháp khác; kết quả được trình bày trong Hình 10 trong Phụ lục A.1. Hình
10a so sánh hiệu suất của tất cả các phương pháp khi K thay đổi giữa 5 và
100 trên các tập dữ liệu chiều thấp, bao gồm Coil20, Isolet, HAR, và Madelon.
Hình 10b minh họa so sánh hiệu suất cho K giữa 5 và 300 trên
tập dữ liệu MNIST, cũng là một tập dữ liệu chiều thấp. Chúng tôi thảo luận tập dữ liệu này
riêng biệt vì nó có số lượng lớn mẫu làm cho nó khác biệt so với các
tập dữ liệu chiều thấp khác. Hình 10c đại diện cho so sánh tương tự trên ba
tập dữ liệu chiều cao, bao gồm SMK, GLA, và PCMAC. Cần lưu ý rằng để có
so sánh công bằng, chúng tôi sử dụng một lõi CPU duy nhất để chạy các phương pháp này; tuy nhiên, vì
các triển khai của CAE và AEFS được tối ưu hóa cho song song

--- TRANG 15 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 15
FCAE CAE(GPU) AEFS(GPU) MCFS Lap_score QS_10 QS_100
Phương pháp13151720Score1Clustering accuracy
Classification accuracy
Running time
Memory
(a) Điểm số 1. Điểm số được đưa ra
dựa trên xếp hạng của các
phương pháp.
FCAE CAE(GPU) AEFS(GPU) MCFS Lap_score QS
Phương pháp1234567Score2Clustering accuracy
Classification accuracy
Running time
Memory(b) Điểm số 2. Điểm số được đưa ra
dựa trên giá trị chuẩn hóa
của mỗi mục tiêu.
FCAE CAE(GPU) AEFS(GPU) MCFS Lap_score QS
Phương pháp10152025Total score
Clustering
accuracy
Classification
accuracy
Running time
Memory
(c) Hiển thị bản đồ nhiệt của
Điểm số 1.
Hình 5: So sánh lựa chọn đặc trưng về độ chính xác phân loại, độ chính xác phân cụm,
tốc độ, và yêu cầu bộ nhớ, trên mỗi tập dữ liệu và cho các giá trị khác nhau
của K, sử dụng hai biến thể chấm điểm.
tính toán, chúng tôi sử dụng GPU để chạy các phương pháp này. Chúng tôi cũng đo thời gian chạy
của lựa chọn đặc trưng với CAE trên CPU.
Để so sánh yêu cầu bộ nhớ của mỗi phương pháp, chúng tôi hồ sơ sử dụng bộ nhớ tối đa
trong quá trình lựa chọn đặc trưng cho các giá trị khác nhau của K. Kết quả được
trình bày trong Hình 11 trong Phụ lục A.1, được dẫn xuất sử dụng thư viện Python có tên
resource4. Bên cạnh đó, để so sánh bộ nhớ được chiếm bởi các mô hình dựa trên autoencoder,
chúng tôi đếm số lượng tham số cho mỗi mô hình. Kết quả được hiển thị trong Hình
14 trong Phụ lục A.3.
Tuy nhiên, việc so sánh tất cả những phương pháp này chỉ bằng cách nhìn vào các đồ thị trong
Hình 10 và Hình 11 không dễ dàng, và sự cân bằng giữa các yếu tố
không rõ ràng. Vì lý do này, chúng tôi tính toán hai điểm số để xem xét tất cả những metric này
đồng thời.
Điểm số 1. Để tính toán điểm số này, trên mỗi tập dữ liệu và cho mỗi giá trị của K, chúng tôi
xếp hạng các phương pháp dựa trên thời gian chạy, yêu cầu bộ nhớ, độ chính xác phân cụm,
và độ chính xác phân loại. Sau đó, chúng tôi đưa ra điểm số 1 cho người thực hiện tốt nhất và
thứ hai; điều này chủ yếu do thực tế rằng trong hầu hết các trường hợp, sự
khác biệt giữa hai cái này là không đáng kể. Sau đó, chúng tôi tính toán tổng
của những điểm số này cho mỗi phương pháp trên tất cả các tập dữ liệu. Kết quả được trình bày trong
Hình 5a; để dễ dàng so sánh các thành phần khác nhau trong điểm số, một hiển thị bản đồ nhiệt
của các điểm số được trình bày trong Hình 5c. Điểm số tích lũy cho mỗi
phương pháp bao gồm bốn phần tương ứng với mỗi metric được xem xét. Như rõ ràng
trong Hình này, QuickSelection (điểm số tích lũy của QuickSelection 10
và QuickSelection 100) vượt trội hơn tất cả các phương pháp khác bởi một khoảng cách đáng kể. Phương pháp
được đề xuất của chúng tôi có thể đạt được sự cân bằng tốt nhất giữa độ chính xác, thời gian chạy,
và sử dụng bộ nhớ, trong số tất cả những phương pháp này. Laplacian score, người thực hiện tốt
thứ hai, có hiệu suất khá tốt về thời gian chạy và bộ nhớ,
trong khi nó không thể hoạt động tốt về độ chính xác. Mặt khác, CAE có
hiệu suất thỏa đáng về độ chính xác. Tuy nhiên, nó không nằm trong hai người thực hiện tốt nhất
về tài nguyên tính toán cho bất kỳ giá trị nào của K. Cuối cùng,
FCAE và AEFS không thể đạt hiệu suất khá tốt so với các
phương pháp khác. Một phiên bản chi tiết hơn của Hình 5a có sẵn trong Hình 12 trong Phụ lục
A.1.
4https://docs.python.org/2/library/resource.html

--- TRANG 16 ---
16 Zahra Atashgahi et al.
Điểm số 2. Ngoài điểm số dựa trên xếp hạng, chúng tôi tính toán một điểm số khác để
xem xét tất cả các phương pháp, ngay cả những cái xếp hạng thấp hơn. Với mục đích này, trên mỗi
tập dữ liệu và giá trị của K, chúng tôi chuẩn hóa mỗi metric hiệu suất giữa 0 và 1,
sử dụng các giá trị của người thực hiện tốt nhất và tệ nhất trên mỗi metric. Giá trị
1 trong điểm số độ chính xác có nghĩa là độ chính xác cao nhất. Tuy nhiên, đối với
bộ nhớ và thời gian chạy, giá trị 1 có nghĩa là yêu cầu bộ nhớ ít nhất và
thời gian chạy ít nhất, tương ứng. Sau khi chuẩn hóa các metric, chúng tôi tích lũy
các giá trị chuẩn hóa cho mỗi phương pháp và trên tất cả các tập dữ liệu. Kết quả được mô tả
trong Hình 5b. Như có thể thấy trong sơ đồ này, QuickSelection (chúng tôi xem xét
kết quả của QuickSelection 100) vượt trội hơn các phương pháp khác bởi một biên độ lớn.
CAE có hiệu suất gần với QuickSelection về cả hai metric độ chính xác,
trong khi nó có hiệu suất kém về bộ nhớ và thời gian chạy. Ngược lại,
Lap_score hiệu quả về mặt tính toán trong khi có điểm số độ chính xác thấp nhất. Tóm lại,
có thể quan sát trong Hình 5b, rằng QuickSelection đạt được sự cân bằng tốt nhất
của bốn mục tiêu trong số các phương pháp được xem xét.
Tiêu thụ năng lượng. Phân tích tiếp theo chúng tôi thực hiện liên quan đến tiêu thụ năng lượng
của mỗi phương pháp. Chúng tôi ước tính tiêu thụ năng lượng của mỗi phương pháp
sử dụng thời gian chạy của thuật toán tương ứng cho mỗi tập dữ liệu và giá trị
của K. Chúng tôi giả định rằng mỗi phương pháp sử dụng công suất tối đa của
tài nguyên tính toán tương ứng trong thời gian chạy của nó. Do đó, chúng tôi dẫn xuất tiêu thụ công suất
của mỗi phương pháp, sử dụng thời gian chạy và tiêu thụ công suất tối đa
của CPU và/hoặc GPU, có thể được tìm thấy trong thông số kỹ thuật của
CPU hoặc GPU tương ứng. Như được hiển thị trong Hình 13 trong Phụ lục A.2,
lựa chọn đặc trưng Laplacian score cần lượng năng lượng ít nhất trong số
các phương pháp trên tất cả các tập dữ liệu ngoại trừ tập dữ liệu MNIST. QuickSelection 10 là người thực hiện tốt nhất
trên MNIST về tiêu thụ năng lượng. Laplacian score và MCFS
nhạy cảm với số lượng mẫu. Chúng không thể hoạt động tốt trên MNIST, cả về
độ chính xác lẫn hiệu quả. Sử dụng bộ nhớ tối đa trong quá trình lựa chọn đặc trưng
cho Laplacian score và MCFS trên MNIST là 56 GB và 85 GB, tương ứng.
Do đó, chúng không phải là lựa chọn tốt trong trường hợp có số lượng lớn mẫu.
QuickSelection là người thực hiện tốt thứ hai về tiêu thụ năng lượng, và
cũng là người thực hiện tốt nhất trong số các phương pháp dựa trên autoencoder. QuickSelection không
nhạy cảm với số lượng mẫu hoặc số lượng chiều.
Hiệu quả vs Độ chính xác. Để nghiên cứu sự cân bằng giữa độ chính xác
và hiệu quả tài nguyên, chúng tôi thực hiện một phân tích sâu khác. Trong phân tích này,
chúng tôi vẽ sự cân bằng giữa độ chính xác (bao gồm, độ chính xác phân loại và phân cụm)
và yêu cầu tài nguyên (bao gồm, bộ nhớ và tiêu thụ năng lượng).
Kết quả được hiển thị trong Hình 6 và 7 tương ứng với sự cân bằng năng lượng-độ chính xác
và bộ nhớ-độ chính xác, tương ứng. Mỗi điểm trong những biểu đồ này đề cập đến
kết quả của một sự kết hợp cụ thể giữa một phương pháp và tập dữ liệu cụ thể khi
chọn 50 đặc trưng (ngoại trừ Madelon, cho tập này chúng tôi chọn 20 đặc trưng). Như có thể
quan sát trong những biểu đồ này, QuickSelection, MCFS, và Lap_score thường có
sự cân bằng tốt giữa các metric được xem xét. Sự cân bằng tốt giữa một cặp
metric là tối đa hóa độ chính xác (độ chính xác phân loại hoặc phân cụm) trong khi
tối thiểu hóa chi phí tính toán (tiêu thụ công suất hoặc yêu cầu bộ nhớ).
Tuy nhiên, khi số lượng mẫu tăng (trên tập dữ liệu MNIST), cả
MCFS và Lap_score đều không thể duy trì chi phí tính toán thấp và độ chính xác cao.
Do đó, khi kích thước tập dữ liệu tăng, hai phương pháp này không phải là lựa chọn tối ưu.
Trong số các phương pháp dựa trên autoencoder, trong hầu hết các trường hợp QuickSelection 10

--- TRANG 17 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 17
20 30 40 50 60
Clustering Accuracy (%)104
103
102
101
100101102103Power Consumption (Kwh)
20 30 40 50 60 70 80 90 100
Classification Accuracy (%)104
103
102
101
100101102103Power Consumption (Kwh)Coil20
Isolet
HAR
Madelon
MNIST
SMK
GLA
PCMAC
FCAE
CAE(GPU)
AEFS(GPU)
MCFS
Lap_score
QS_10
QS_100
Hình 6: Tiêu thụ công suất ước tính (Kwh) vs. độ chính xác (%) khi chọn 50
đặc trưng (ngoại trừ Madelon cho tập này chúng tôi chọn 20 đặc trưng). Mỗi điểm đề cập đến
kết quả của một tập dữ liệu duy nhất (được chỉ định bằng màu sắc) và phương pháp (được chỉ định bằng dấu hiệu)
trong đó trục x và y hiển thị độ chính xác và tiêu thụ công suất ước tính,
tương ứng.
20 30 40 50 60
Clustering Accuracy (%)02468Maximum memory requirement (Kb)1e7
20 30 40 50 60 70 80 90 100
Classification Accuracy (%)02468Maximum memory requirement (Kb)1e7
20 30 40 50 60
Clustering Accuracy (%)0.00.51.01.52.02.53.0Maximum memory requirement (Kb)1e6
20 30 40 50 60 70 80 90 100
Classification Accuracy (%)0.00.51.01.52.02.53.0Maximum memory requirement (Kb)1e6
Coil20
Isolet
HAR
Madelon
MNIST
SMK
GLA
PCMAC
FCAE
CAE(GPU)
AEFS(GPU)
MCFS
Lap_score
QS
Hình 7: Yêu cầu bộ nhớ tối đa (Kb) vs. độ chính xác (%) khi chọn 50
đặc trưng (ngoại trừ Madelon cho tập này chúng tôi chọn 20 đặc trưng). Mỗi điểm đề cập đến
kết quả của một tập dữ liệu duy nhất (được chỉ định bằng màu sắc) và phương pháp (được chỉ định bằng dấu hiệu)
trong đó trục x và y hiển thị độ chính xác và yêu cầu bộ nhớ tối đa,
tương ứng. Do yêu cầu bộ nhớ cao của MCFS và Lap_score trên
tập dữ liệu MNIST làm cho việc so sánh các kết quả khác khó khăn (biểu đồ trên),
chúng tôi phóng to phần này trong các biểu đồ dưới.
và QuickSelection 100 nằm trong các điểm Pareto tối ưu. Một ưu điểm đáng kể khác
của phương pháp được đề xuất của chúng tôi là nó đưa ra xếp hạng của các đặc trưng làm
đầu ra. Do đó, không giống như MCFS hoặc CAE cần giá trị của K như
đầu vào của chúng, QuickSelection không phụ thuộc vào K và cần chỉ một lần huấn luyện duy nhất
của mô hình sparse DAE cho bất kỳ giá trị nào của K. Do đó, chi phí tính toán
của QuickSelection giống nhau cho tất cả các giá trị của K, và chỉ cần một lần chạy duy nhất
của thuật toán này để có được tầm quan trọng phân cấp của các đặc trưng.

--- TRANG 18 ---
18 Zahra Atashgahi et al.
0 5000 10000 15000 20000 25000 30000 35000 40000
# of features02500500075001000012500150001750020000running time (s)
QS_10 (K=All, n=1000, CPU)
QS_100 (K=All, n=1000, CPU)
QS_100 (K=All, n=10000, CPU)
CAE (K=100, n=150, GPU)
CAE (K=300, n=450, GPU)
CAE (K=100, n=1000, GPU)
CAE (K=100, n=10000, GPU)
CAE (K=100, n=150, CPU)
AEFS (K=All, n=300, GPU)
AEFS (K=All, n=1000, GPU)
AEFS (K=All, n=10000, GPU)
FCAE (K=All, n=1000, CPU)
FCAE (K=All, n=10000, CPU)
Hình 8: So sánh thời gian chạy trên tập dữ liệu được tạo nhân tạo. Các đặc trưng
được tạo sử dụng phân phối chuẩn tiêu chuẩn và số lượng mẫu cho
mỗi trường hợp là 5000.
5.2 So sánh thời gian chạy trên tập dữ liệu được tạo nhân tạo
Trong phần này, chúng tôi thực hiện so sánh thời gian chạy của các phương pháp lựa chọn đặc trưng
dựa trên autoencoder trên một tập dữ liệu được tạo nhân tạo. Vì trên các tập dữ liệu chuẩn
cả số lượng đặc trưng và mẫu đều khác nhau, không dễ dàng có thể so sánh rõ ràng
hiệu quả của các phương pháp. Thí nghiệm này nhằm so sánh thời gian huấn luyện đồng hồ treo tường
thực của các mô hình trong một môi trường được kiểm soát với số lượng đặc trưng đầu vào và neuron ẩn.
Ngoài ra, trong Phụ lục E, chúng tôi đã tiến hành một thí nghiệm khác về đánh giá
các phương pháp trên một tập dữ liệu nhân tạo rất lớn, cả về tài nguyên tính toán
và độ chính xác.
Trong thí nghiệm này, chúng tôi nhằm so sánh tốc độ của QuickSelection với
các phương pháp lựa chọn đặc trưng dựa trên autoencoder khác cho số lượng đặc trưng đầu vào khác nhau.
Chúng tôi chạy tất cả chúng trên một tập dữ liệu được tạo nhân tạo với số lượng đặc trưng khác nhau
và 5000 mẫu, cho 100 epoch huấn luyện (10 epoch cho
QuickSelection 10). Các đặc trưng của tập dữ liệu này được tạo sử dụng phân phối chuẩn
tiêu chuẩn. Ngoài ra, chúng tôi nhằm so sánh thời gian chạy của các
cấu trúc khác nhau cho những thuật toán này. Thông số kỹ thuật của cấu trúc mạng
cho mỗi phương pháp, tài nguyên tính toán được sử dụng cho lựa chọn đặc trưng, và
kết quả tương ứng có thể được thấy trong Hình 8.
Đối với CAE, chúng tôi xem xét hai giá trị khác nhau của K. Cấu trúc của CAE phụ thuộc
vào giá trị này. CAE có hai lớp ẩn bao gồm một selector concrete và một
decoder có K và 1:5K neuron, tương ứng. Do đó, bằng cách tăng
số lượng đặc trưng được chọn, thời gian chạy của mô hình cũng sẽ tăng. Ngoài ra,
chúng tôi xem xét các trường hợp của CAE với 1000 và 10000 neuron ẩn trong
lớp decoder (được thay đổi thủ công trong mã) để có thể so sánh nó với
các mô hình khác. Chúng tôi cũng đo thời gian chạy của việc thực hiện lựa chọn đặc trưng
với CAE chỉ sử dụng một lõi CPU duy nhất. Có thể thấy từ Hình 8 rằng
thời gian chạy của nó khá cao. Cấu trúc chung của AEFS, QuickSelection,
và FCAE tương tự về số lượng lớp ẩn. Chúng là các autoencoder cơ bản

--- TRANG 19 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 19
với một lớp ẩn duy nhất. Đối với AEFS, chúng tôi xem xét ba cấu trúc
với số lượng neuron ẩn khác nhau, bao gồm 300, 1000, và 10000. Cuối cùng,
đối với QuickSelection và FCAE, chúng tôi xem xét hai giá trị khác nhau cho số lượng
neuron ẩn, bao gồm 1000 và 10000.
Có thể quan sát thấy rằng thời gian chạy của AEFS với 1000 và 10000 neuron ẩn
sử dụng GPU, lớn hơn nhiều so với thời gian chạy của QuickSelection 100
với số lượng neuron ẩn tương tự chỉ sử dụng một lõi CPU duy nhất, tương ứng.
Mẫu tương tự cũng có thể thấy trong trường hợp của CAE với 1000 và 10000 neuron ẩn.
Mẫu này cũng lặp lại trong trường hợp của FCAE với 10000 neuron ẩn.
Thời gian chạy của FCAE với 1000 neuron ẩn xấp xỉ tương tự như
QuickSelection 100. Tuy nhiên, sự khác biệt giữa hai phương pháp này đáng kể hơn
khi chúng ta tăng số lượng neuron ẩn lên 10000. Điều này chủ yếu do
thực tế rằng sự khác biệt giữa số lượng tham số của
QuickSelection và các phương pháp khác trở nên cao hơn nhiều cho các giá trị lớn của K.
Bên cạnh đó, những quan sát này mô tả rằng thời gian chạy của QuickSelection
không thay đổi đáng kể bằng cách tăng số lượng neuron ẩn.
Như chúng tôi cũng đã đề cập trước đây, QuickSelection đưa ra xếp hạng của các đặc trưng làm
đầu ra. Do đó, không giống như CAE phải được chạy riêng biệt cho
các giá trị khác nhau của K, QuickSelection không bị ảnh hưởng bởi lựa chọn của K vì
nó tính toán tầm quan trọng của tất cả các đặc trưng cùng một lúc và sau khi hoàn thành
huấn luyện. Tóm lại, QuickSelection 10 có thời gian chạy ít nhất trong số các
phương pháp dựa trên autoencoder khác trong khi độc lập với giá trị của K. Ngoài ra,
không giống như các phương pháp khác, thời gian chạy của QuickSelection không nhạy cảm với
số lượng neuron ẩn vì số lượng tham số thấp ngay cả cho một
lớp ẩn rất lớn.
5.3 Phân tích sức mạnh neuron
Trong phần này, chúng tôi thảo luận về tính hợp lệ của sức mạnh neuron như một thước đo
tầm quan trọng đặc trưng. Chúng tôi quan sát sự phát triển của mạng trong quá trình huấn luyện để
phân tích cách sức mạnh neuron của các neuron quan trọng và không quan trọng thay đổi
trong quá trình huấn luyện.
Chúng tôi lập luận rằng các đặc trưng quan trọng nhất dẫn đến độ chính xác cao nhất của
lựa chọn đặc trưng là các đặc trưng tương ứng với các neuron có sức mạnh cao nhất.
Trong mạng nơ-ron, độ lớn trọng số là một metric cho thấy tầm quan trọng của
mỗi kết nối [ 29]. Điều này xuất phát từ thực tế rằng các trọng số có độ lớn nhỏ
có tác động nhỏ đến hiệu suất của mô hình. Khi bắt đầu huấn luyện, chúng ta
khởi tạo tất cả các kết nối thành một giá trị ngẫu nhiên nhỏ. Do đó, tất cả các neuron có
gần như cùng sức mạnh/tầm quan trọng. Khi quá trình huấn luyện tiến triển, một số kết nối
phát triển thành giá trị lớn hơn trong khi một số khác bị cắt tỉa khỏi mạng trong
quá trình loại bỏ và tái sinh kết nối động của quy trình huấn luyện SET. Sự phát triển
của các trọng số kết nối ổn định thể hiện tầm quan trọng của chúng trong
hiệu suất của mạng. Kết quả là, các neuron được kết nối với những
trọng số quan trọng này chứa thông tin quan trọng. Ngược lại, độ lớn của các trọng số
được kết nối với các neuron không quan trọng giảm dần cho đến khi chúng bị loại bỏ
khỏi mạng. Tóm lại, các neuron quan trọng nhận các kết nối với độ lớn
lớn hơn. Kết quả là, sức mạnh neuron, là tổng độ lớn

--- TRANG 20 ---
20 Zahra Atashgahi et al.
0 10 20 30 40 50 60 70 80 90 100
Epochs (t)051015Strength
Important Neurons Strength - QS10
Important Neurons Strength - QS100
0 10 20 30 40 50 60 70 80 90 100
Epochs (t)051015Strength
Important Neurons Strength - QS10
Important Neurons Strength - QS100
0 10 20 30 40 50 60 70 80 90 100
Epochs (t)051015Strength
Important Neurons Strength - QS10
Important Neurons Strength - QS100
0 10 20 30 40 50 60 70 80 90 100
Epochs (t)051015Strength
Important Neurons Strength - QS100
Hình 9: Sức mạnh của 20 đặc trưng thông tin nhất và không thông tin nhất của tập dữ liệu Madelon,
được chọn bởi QS10 và QS100. Mỗi đường trong các biểu đồ tương ứng với
giá trị sức mạnh của một đặc trưng được chọn bởi QS10/QS100 trong quá trình huấn luyện. Các đặc trưng
được chọn bởi QS10 đã được quan sát cho đến epoch 100 để so sánh chất lượng
của những đặc trưng này với QS100.
của các trọng số được kết nối với một neuron, có thể là một thước đo tầm quan trọng của một neuron đầu vào
và đặc trưng tương ứng của nó.
Để hỗ trợ tuyên bố của chúng tôi, chúng tôi quan sát sự phát triển của sức mạnh neuron trên
tập dữ liệu Madelon. Lựa chọn này được thực hiện do sự phân biệt của các đặc trưng thông tin và
không thông tin trong tập dữ liệu Madelon. Như đã mô tả trước đây, tập dữ liệu này
có 20 đặc trưng thông tin, và phần còn lại của các đặc trưng là nhiễu không thông tin.
Chúng tôi xem xét 20 đặc trưng thông tin nhất và không thông tin nhất được phát hiện bởi QS10
và QS100, và theo dõi sức mạnh của chúng trong quá trình huấn luyện (như quan sát trong Hình 3,
độ chính xác tối đa đạt được sử dụng 20 đặc trưng thông tin nhất, trong khi
độ chính xác ít nhất đạt được sử dụng các đặc trưng ít quan trọng nhất). Các đặc trưng
được chọn bởi QS10 cũng được theo dõi sau khi thuật toán hoàn thành (epoch
10) cho đến epoch 100, để so sánh chất lượng của các đặc trưng được chọn bởi
QS10 với QS100. Nói cách khác, chúng tôi trích xuất chỉ số của các đặc trưng quan trọng sử dụng
QS10, và tiếp tục huấn luyện mà không thực hiện bất kỳ thay đổi nào trong mạng và
theo dõi cách sức mạnh của các neuron tương ứng với chỉ số được chọn sẽ
phát triển sau epoch 10. Kết quả được trình bày trong Hình 9. Tại khởi tạo
(epoch 0), sức mạnh của tất cả những neuron này gần như tương tự và dưới 5. Khi
quá trình huấn luyện bắt đầu, sức mạnh của các neuron quan trọng tăng, trong khi sức mạnh của
các neuron không quan trọng không thay đổi đáng kể. Như có thể thấy trong Hình 9,
một số đặc trưng quan trọng được chọn bởi QS10 không nằm trong những đặc trưng của QS100;
điều này có thể giải thích sự khác biệt trong hiệu suất của hai phương pháp này trong Bảng
2 và 3. Tuy nhiên, QS10 có thể phát hiện phần lớn các đặc trưng được tìm thấy
bởi QS100; những đặc trưng này nằm trong số những cái quan trọng nhất trong số
20 đặc trưng được chọn cuối cùng. Do đó, chúng ta có thể kết luận rằng hầu hết các đặc trưng quan trọng
có thể phát hiện được bởi QuickSelection, ngay cả trong vài epoch đầu tiên của thuật toán.
6 Kết luận
Trong bài báo này, một phương pháp mới (QuickSelection) cho lựa chọn đặc trưng không giám sát
tiết kiệm năng lượng đã được đề xuất. Nó giới thiệu sức mạnh neuron trong mạng nơ-ron thưa
như một thước đo tầm quan trọng đặc trưng. Bên cạnh đó, nó đề xuất sparse DAE
để mô hình hóa chính xác phân phối dữ liệu và xếp hạng tất cả các đặc trưng đồng thời
dựa trên tầm quan trọng của chúng. Bằng cách sử dụng các lớp thưa thay vì dày đặc từ
đầu, số lượng tham số giảm đáng kể. Kết quả là, QuickSelection
yêu cầu ít bộ nhớ và tài nguyên tính toán hơn nhiều so với mô hình dày đặc tương đương
và các đối thủ cạnh tranh của nó. Ví dụ, trên các tập dữ liệu chiều thấp, bao gồm
Coil20, Isolet, HAR, và Madelon, và cho tất cả các giá trị của K, QuickSelection 100 chạy trên một lõi CPU
nhanh hơn ít nhất 4 lần so với đối thủ cạnh tranh trực tiếp của nó, CAE,
chạy trên GPU, trong khi có hiệu suất gần như nhau về độ chính xác phân loại
và phân cụm. Chúng tôi chứng minh thực nghiệm rằng QuickSelection đạt được
sự cân bằng tốt nhất giữa độ chính xác phân cụm, độ chính xác phân loại, yêu cầu bộ nhớ tối đa,
và thời gian chạy, trong số các phương pháp khác được xem xét. Bên cạnh đó,
phương pháp được đề xuất của chúng tôi yêu cầu lượng năng lượng ít nhất trong số các phương pháp
dựa trên autoencoder được xem xét.
Nhược điểm chính của phương pháp được đề xuất là thiếu triển khai song song.
Thời gian chạy của QuickSelection có thể được giảm thêm bằng
một triển khai tận dụng CPU đa lõi hoặc GPU. Chúng tôi tin rằng
nghiên cứu tương lai thú vị sẽ là nghiên cứu các tác động của huấn luyện thưa

--- TRANG 21 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 21
và sức mạnh neuron trong các loại autoencoder khác cho lựa chọn đặc trưng, ví dụ: CAE.
Tuy nhiên, bài báo này mới chỉ bắt đầu khám phá một trong những đặc tính quan trọng nhất
của QuickSelection, tức là khả năng mở rộng, và chúng tôi dự định khám phá thêm
tiềm năng đầy đủ của nó trên các tập dữ liệu với hàng triệu đặc trưng. Bên cạnh đó, bài báo này đã cho thấy
rằng chúng ta có thể thực hiện lựa chọn đặc trưng sử dụng mạng nơ-ron một cách hiệu quả về
chi phí tính toán và yêu cầu bộ nhớ. Điều này có thể mở đường cho việc giảm
chi phí tính toán ngày càng tăng của các mô hình deep learning được áp đặt lên các
trung tâm dữ liệu. Kết quả là, điều này sẽ không chỉ tiết kiệm chi phí năng lượng của việc xử lý dữ liệu
chiều cao mà còn sẽ giảm bớt các thách thức của tiêu thụ năng lượng cao
được áp đặt lên môi trường.
Lời cảm ơn Nghiên cứu này đã được tài trợ một phần bởi dự án NWO EDIC.
Tài liệu tham khảo
1.Amirali Aghazadeh, Ryan Spring, Daniel Lejeune, Gautam Dasarathy, An-
shumali Shrivastava, et al. Mission: Ultra large-scale feature selection using
count-sketches. In International Conference on Machine Learning , pages 80–88,
2018.
2.Jun Chin Ang, Andri Mirzal, Habibollah Haron, and Haza Nuzly Abdull
Hamed. Supervised, unsupervised, and semi-supervised feature selection: a

--- TRANG 22 ---
22 Zahra Atashgahi et al.
review on gene selection. IEEE/ACM transactions on computational biology
and bioinformatics , 13(5):971–989, 2015.
3.Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis
Reyes-Ortiz. A public domain dataset for human activity recognition using
smartphones. In Esann, 2013.
4.Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In
Proceedings of ICML workshop on unsupervised and transfer learning , pages
37–49, 2012.
5.Muhammed Fatih Balın, Abubakar Abid, and James Zou. Concrete autoen-
coders: Diﬀerentiable feature selection and reconstruction. In International
Conference on Machine Learning , pages 444–453, 2019.
6.Alain Barrat, Marc Barthelemy, Romualdo Pastor-Satorras, and Alessandro
Vespignani. The architecture of complex weighted networks. Proceedings of
the national academy of sciences , 101(11):3747–3752, 2004.
7.GuillaumeBellec,DavidKappel,WolfgangMaass,andRobertLegenstein. Deep
rewiring: Training very sparse deep networks. arXiv preprint arXiv:1711.05136 ,
2017.
8.Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning:
A review and new perspectives. IEEE transactions on pattern analysis and
machine intelligence , 35(8):1798–1828, 2013.
9.Verónica Bolón-Canedo, Noelia Sánchez-Maroño, and Amparo Alonso-Betanzos.
Feature selection for high-dimensional data . Springer, 2015.
10.David D. Bourgin, Joshua C. Peterson, Daniel Reichman, Stuart J. Rus-
sell, and Thomas L. Griﬃths. Cognitive model priors for predicting hu-
man decisions. In Kamalika Chaudhuri and Ruslan Salakhutdinov, edi-
tors,Proceedings of the 36th International Conference on Machine Learn-
ing, volume 97 of Proceedings of Machine Learning Research , pages 5133–
5141, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/peterson19a.html .
11.Deng Cai, Chiyuan Zhang, and Xiaofei He. Unsupervised feature selection for
multi-cluster data. In Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining , pages 333–342. ACM,
2010.
12.Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods.
Computers & Electrical Engineering , 40(1):16–28, 2014.
13. François Chollet et al. Keras. https://keras.io , 2015.
14.Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster
training without losing performance. arXiv preprint arXiv:1907.04840 , 2019.
15.Guillaume Doquet and Michèle Sebag. Agnostic feature selection. In Joint Eu-
ropean Conference on Machine Learning and Knowledge Discovery in Databases ,
pages 343–358. Springer, 2019.
16.Jennifer G Dy and Carla E Brodley. Feature selection for unsupervised learning.
Journal of machine learning research , 5(Aug):845–889, 2004.
17.Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich
Elsen. Rigging the lottery: Making all tickets winners. arXiv preprint
arXiv:1911.11134 , 2019.
18.Mark Fanty and Ronald Cole. Spoken letter recognition. In Advances in Neural
Information Processing Systems , pages 220–226, 1991.

--- TRANG 23 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 23
19.Ahmed K Farahat, Ali Ghodsi, and Mohamed S Kamel. Eﬃcient greedy feature
selection for unsupervised learning. Knowledge and information systems , 35
(2):285–310, 2013.
20.Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding
sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 , 2018.
21.Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized
trees.Machine learning , 63(1):3–42, 2006.
22.AI High-Level Expert Group. Assessment list for trustworthy artiﬁcial intelli-
gence (ALTAI) for self-assessment, 2020.
23.Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti A Zadeh. Feature
extraction: foundations and applications , volume 207. Springer, 2008.
24.Kai Han, Yunhe Wang, Chao Zhang, Chao Li, and Chao Xu. Autoencoder
inspired unsupervised feature selection. In 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , pages 2941–2945. IEEE,
2018.
25.Song Han, Jeﬀ Pool, John Tran, and William Dally. Learning both weights and
connections for eﬃcient neural network. In Advances in neural information
processing systems , pages 1135–1143, 2015.
26.Babak Hassibi and David G Stork. Second order derivatives for network
pruning: Optimal brain surgeon. In Advances in neural information processing
systems, pages 164–171, 1993.
27.Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection.
InAdvances in neural information processing systems , pages 507–514, 2006.
28.Eric Jones, Travis Oliphant, and Pearu Peterson. Scipy: Open source scientiﬁc
tools for python. 2001.
29.Taskin Kavzoglu and Paul M Mather. Assessing artiﬁcial neural network prun-
ing algorithms. In Proceedings of the 24th Annual Conference and Exhibition
of the Remote Sensing Society , pages 9–11, 1998.
30.RonKohaviandGeorgeHJohn. Wrappersforfeaturesubsetselection. Artiﬁcial
intelligence , 97(1-2):273–324, 1997.
31.Thomas Navin Lal, Olivier Chapelle, Jason Weston, and André Elisseeﬀ.
Embedded methods. In Feature extraction , pages 137–165. Springer, 2006.
32.Ken Lang. Newsweeder: Learning to ﬁlter netnews. In Machine Learning
Proceedings 1995 , pages 331–339. Elsevier, 1995.
33.Cosmin Lazar, Jonatan Taminau, Stijn Meganck, David Steenhoﬀ, Alain Co-
letta, Colin Molter, Virginie de Schaetzen, Robin Duque, Hugues Bersini, and
Ann Nowe. A survey on ﬁlter techniques for feature selection in gene expression
microarray analysis. IEEE/ACM Transactions on Computational Biology and
Bioinformatics , 9(4):1106–1119, 2012.
34.Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
com/exdb/mnist/ , 1998.
35.Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In
Advances in neural information processing systems , pages 598–605, 1990.
36.Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-
shot network pruning based on connection sensitivity. arXiv preprint
arXiv:1810.02340 , 2018.
37.Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino,
Jiliang Tang, and Huan Liu. Feature selection: A data perspective. ACM
Computing Surveys (CSUR) , 50(6):94, 2018.

--- TRANG 24 ---
24 Zahra Atashgahi et al.
38.Yifeng Li, Chih-Yu Chen, and Wyeth W Wasserman. Deep feature selec-
tion: theory and application to identify enhancers and promoters. Journal of
Computational Biology , 23(5):322–336, 2016.
39.Andy Liaw, Matthew Wiener, et al. Classiﬁcation and regression by random-
forest.R news, 2(3):18–22, 2002.
40.Huan Liu and Hiroshi Motoda. Feature extraction, construction and selection:
A data mining perspective , volume 453. Springer Science & Business Media,
1998.
41.Shiwei Liu, Tim van der Lee, Anil Yaman, Zahra Atashgahi, Davide Ferrar,
Ghada Sokar, Mykola Pechenizkiy, and Decebal C Mocanu. Topological insights
into sparse neural networks. In Proceedings of the European Conference on
Machine Learning and Principles and Practice of Knowledge Discovery in
Databases (ECML PKDD) 2020. , 2020.
42.Yang Lu, Yingying Fan, Jinchi Lv, and William Staﬀord Noble. Deeppink:
reproducible feature selection in deep neural networks. In Advances in Neural
Information Processing Systems , pages 8676–8686, 2018.
43.Jianyu Miao and Lingfeng Niu. A survey on feature selection. Procedia
Computer Science , 91:919–926, 2016.
44.Decebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine
Gibescu, and Antonio Liotta. A topological insight into restricted boltzmann
machines. Machine Learning , 104(2-3):243–270, 2016.
45. Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen,
Madeleine Gibescu, and Antonio Liotta. Scalable training of artiﬁcial neural
networks with adaptive sparse connectivity inspired by network science. Nature
communications , 9(1):2383, 2018.
46.Hesham Mostafa and Xin Wang. Parameter eﬃcient training of deep convo-
lutional neural networks by dynamic sparse reparameterization. In Kamalika
Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th Interna-
tional Conference on Machine Learning , volume 97 of Proceedings of Machine
Learning Research , pages 4646–4655, Long Beach, California, USA, 09–15 Jun
2019. PMLR. URL http://proceedings.mlr.press/v97/mostafa19a.html .
47.Sameer A Nene, Shree K Nayar, Hiroshi Murase, et al. Columbia object image
library (coil-20). 1996.
48.Razieh Sheikhpour, Mehdi Agha Sarram, Sajjad Gharaghani, and Mohammad
Ali Zare Chahooki. A survey on semi-supervised feature selection methods.
Pattern Recognition , 64:141–158, 2017.
49.Dinesh Singh and Makoto Yamada. Fsnet: Feature selection network on high-
dimensional biological data. arXiv preprint arXiv:2001.08322 , 2020.
50.Avrum Spira, Jennifer E Beane, Vishal Shah, Katrina Steiling, Gang Liu, Frank
Schembri, Sean Gilman, Yves-Martine Dumas, Paul Calner, Paola Sebastiani,
et al. Airway epithelial gene expression in the diagnostic evaluation of smokers
with suspect lung cancer. Nature medicine , 13(3):361–366, 2007.
51.Lixin Sun, Ai-Min Hui, Qin Su, Alexander Vortmeyer, Yuri Kotliarov, Sandra
Pastorino, Antonino Passaniti, Jayant Menon, Jennifer Walling, Rolando Bailey,
et al. Neuronal and glioma-derived stem cell factor induces angiogenesis within
the brain. Cancer cell , 9(4):287–300, 2006.
52.Mingkui Tan, Ivor W Tsang, and Li Wang. Towards ultrahigh dimensional
feature selection for big data. Journal of Machine Learning Research , 2014.

--- TRANG 25 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 25
53.LaurensVanDerMaaten,EricPostma,andJaapVandenHerik. Dimensionality
reduction: a comparative. J Mach Learn Res , 10(66-71):13, 2009.
54.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
Extracting and composing robust features with denoising autoencoders. In
Proceedings of the 25th international conference on Machine learning , pages
1096–1103. ACM, 2008.
55.Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis.
Chemometrics and intelligent laboratory systems , 2(1-3):37–52, 1987.
56.JunYang,WenjingXiao,ChunJiang,MShamimHossain,GhulamMuhammad,
and Syed Umar Amin. Ai-powered green cloud and data center. IEEE Access ,
7:4195–4203, 2018.
57.Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, and Xiaofang Zhou. L2,
1-norm regularized discriminative feature selection for unsupervised. In Twenty-
Second International Joint Conference on Artiﬁcial Intelligence , 2011.
58.Zheng Zhao and Huan Liu. Semi-supervised feature selection via spectral
analysis. In Proceedings of the 2007 SIAM international conference on data
mining, pages 641–646. SIAM, 2007.
59.Hangyu Zhu and Yaochu Jin. Multi-objective evolutionary federated learning.
IEEE transactions on neural networks and learning systems , 2019.

--- TRANG 26 ---
26 Zahra Atashgahi et al.
Phụ lục
A Đánh giá hiệu suất
Trong phụ lục này, chúng tôi so sánh tất cả các phương pháp từ các khía cạnh khác nhau bao gồm độ chính xác, sử dụng bộ nhớ
, thời gian chạy, tiêu thụ năng lượng, và số lượng tham số. Chúng tôi thực hiện các
thí nghiệm khác nhau để có được cái nhìn sâu sắc về hiệu suất của QuickSelection.
A.1 Thảo luận: Cân bằng giữa Độ chính xác và Hiệu quả Tính toán
Trong phần này, chúng tôi so sánh hiệu suất của tất cả các phương pháp chi tiết hơn. Chúng tôi chạy lựa chọn đặc trưng
cho các giá trị khác nhau của K trên mỗi tập dữ liệu và sau đó đo hiệu suất.
Như được hiển thị trong Hình 10, chúng tôi so sánh độ chính xác phân cụm, độ chính xác phân loại, và thời gian chạy
trong số các phương pháp cho các giá trị khác nhau của K. So sánh yêu cầu bộ nhớ tối đa
(RAM) cũng được mô tả trong Hình 11. Đối với tất cả các phương pháp ngoại trừ CAE và AEFS, chúng tôi
chạy các thí nghiệm trên một lõi CPU duy nhất. Vì các triển khai của CAE và AEFS được
tối ưu hóa cho GPU, chúng tôi đo thời gian chạy của những phương pháp này sử dụng GPU. Tuy nhiên,
chúng tôi cũng xem xét thời gian chạy của CAE sử dụng một lõi CPU duy nhất. Cần lưu ý rằng
vì Laplacian score, AEFS, FCAE, và QuickSelection đưa ra xếp hạng của các đặc trưng làm
đầu ra của quá trình lựa chọn đặc trưng, chúng tôi cần chạy chúng chỉ một lần cho tất cả các giá trị của K.
Tuy nhiên, MCFS và CAE cần giá trị K làm đầu vào của thuật toán của chúng. Vậy, thời gian chạy
phụ thuộc vào giá trị của K. Trong triển khai của AEFS, K được sử dụng để đặt số
giá trị ẩn. Tuy nhiên, nó không phải là yêu cầu của thuật toán.
Chúng tôi tóm tắt kết quả của các biểu đồ đã nêu trong Hình 12; chúng tôi so sánh các phương pháp
sử dụng điểm số 1, được giới thiệu trong Phần 5.1. Điểm số này được tính toán dựa trên
xếp hạng của các phương pháp trong độ chính xác phân cụm, độ chính xác phân loại, thời gian chạy, và bộ nhớ. Như
giải thích trong Phần 5.1, chúng tôi đưa ra điểm số một cho mỗi phương pháp là người thực hiện tốt nhất hoặc thứ hai
trong mỗi metric được xem xét. Sau đó, chúng tôi tính toán tổng trên tất cả những điểm số này
trên tất cả các tập dữ liệu và trên tất cả các giá trị của K; điểm số cuối cùng cho mỗi phương pháp có thể được thấy trong Hình
12. Cột đầu tiên mô tả kết quả trên các tập dữ liệu chiều thấp với số lượng mẫu thấp,
bao gồm Coil20, Isolet, HAR, và Madelon. Cột thứ hai hiển thị kết quả
tương ứng với MNIST. Tương tự, cột thứ ba tương ứng với các tập dữ liệu chiều cao,
bao gồm SMK, GLA, và PCMAC. Tổng điểm số trên tất cả những tập dữ liệu này được hiển thị trong
cột thứ 4. Trong Hình 12, có bốn hàng; hàng đầu tiên tương ứng với việc xem xét
QuickSelection 10 và QuickSelection 100 đồng thời, và tổng điểm số của chúng được mô tả
trong hàng thứ hai. Hai hàng cuối tương ứng với việc xem xét từng phương pháp này
riêng biệt.
Tuy nhiên, vì hiệu suất của mỗi phương pháp có thể khác nhau trong mỗi nhóm ba
tập dữ liệu, chúng tôi tính toán một phiên bản chuẩn hóa của điểm số 1, dựa trên số lượng tập dữ liệu
trong mỗi nhóm. Ví dụ, Laplacian score có hiệu suất kém trên MNIST, và mẫu này
sẽ tương tự trên các tập dữ liệu khác với số lượng lớn mẫu. Tuy nhiên, chỉ có
một tập dữ liệu với số lượng lớn mẫu trong thí nghiệm này. Mặt khác, trên
các tập dữ liệu chiều cao với số lượng mẫu thấp, phương pháp này có hiệu suất tốt
về thời gian chạy, và chúng tôi có ba tập dữ liệu với những đặc tính như vậy. Vậy, chúng tôi chuẩn hóa
các giá trị của điểm số 1, sao cho thay vì đưa ra điểm số một cho mỗi phương pháp, chúng tôi đưa ra
điểm số một chia cho số lượng tập dữ liệu trong nhóm tương ứng. Kết quả của
điểm số 1 chuẩn hóa được hiển thị trong cột cuối của Hình 12.
A.2 Tiêu thụ năng lượng
Chúng tôi thực hiện một thí nghiệm khác về so sánh tiêu thụ năng lượng trong tất cả
các phương pháp. Kết quả được trình bày trong Hình 13. Chi tiết hơn về biểu đồ này được đưa ra trong
bài báo trong Phần 5.1.

--- TRANG 27 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 27
[Phần tiếp theo chứa nhiều biểu đồ và bảng so sánh hiệu suất của các phương pháp khác nhau trên nhiều tập dữ liệu, với các thông số như độ chính xác phân cụm, độ chính xác phân loại, và thời gian chạy được hiển thị. Các biểu đồ này cho thấy QuickSelection hoạt động tốt so với các phương pháp khác trong hầu hết các trường hợp.]

--- TRANG 28 ---
28 Zahra Atashgahi et al.
[Tiếp tục với các biểu đồ so sánh yêu cầu bộ nhớ tối đa giữa các phương pháp khác nhau cho các giá trị khác nhau của K]

--- TRANG 29 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 29
[Biểu đồ so sánh tiêu thụ năng lượng của tất cả các phương pháp cho các giá trị khác nhau của K]

--- TRANG 30 ---
30 Zahra Atashgahi et al.
[Biểu đồ so sánh số lượng tham số của các mô hình dựa trên autoencoder cho các giá trị khác nhau của K]

[Các biểu đồ hiển thị tác động của hệ số nhiễu đến độ chính xác phân cụm và phân loại khi sử dụng QuickSelection 10 và QuickSelection 100 với các giá trị hệ số nhiễu khác nhau]

--- TRANG 31 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 31
[Các bảng hiển thị kết quả lựa chọn siêu tham số cho QuickSelection 10, với mỗi mục chứa độ chính xác phân cụm và độ chính xác phân loại theo phần trăm cho các tập dữ liệu khác nhau]

--- TRANG 32 ---
32 Zahra Atashgahi et al.
[Hình ảnh hiển thị 50 đặc trưng thông tin nhất của tập dữ liệu MNIST được chọn bởi QuickSelection sau 1, 10, và 100 epoch huấn luyện]

--- TRANG 33 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 33
[Các bảng hiển thị kết quả lựa chọn siêu tham số cho QuickSelection 100, với mỗi mục chứa độ chính xác phân cụm và độ chính xác phân loại theo phần trăm cho các tập dữ liệu khác nhau]

--- TRANG 34 ---
34 Zahra Atashgahi et al.
[Biểu đồ hiển thị độ chính xác phân loại cho trích xuất đặc trưng sử dụng sparse DAE với mức độ dày đặc khác nhau trên tập dữ liệu MNIST so với FC-DAE và PCA]

--- TRANG 35 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 35
[Bảng mô tả đặc tính của hai tập dữ liệu được tạo nhân tạo và bảng kết quả lựa chọn đặc trưng trên những tập dữ liệu này]

--- TRANG 36 ---
36 Zahra Atashgahi et al.
[Các bảng so sánh độ chính xác phân cụm và phân loại sử dụng 50 đặc trưng được chọn giữa QS100 và QSLTH100, cùng với bảng so sánh số lượng tham số]

--- TRANG 37 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 37
[Bảng hiển thị độ chính xác phân loại sử dụng 50 đặc trưng được chọn với bộ phân loại random forest thay vì ExtraTrees, cho thấy QuickSelection vẫn hoạt động tốt với bộ phân loại khác]
