# 2208.11271.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/autoencoder/2208.11271.pdf
# Kích thước tệp: 2936530 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Giải quyết Tìm kiếm Mã dài với Chia, Mã hóa và Tổng hợp
Fan Hu1∗, Yanlin Wang2†‡, Lun Du3,
Hongyu Zhang4, Shi Han3, Dongmei Zhang3, Xirong Li1
1Đại học Nhân dân Trung Quốc
2Trường Kỹ thuật Phần mềm, Đại học Trung Sơn
3Microsoft
4Đại học Trùng Khánh
Tóm tắt
Tìm kiếm mã với ngôn ngữ tự nhiên giúp chúng ta tái sử dụng các đoạn mã hiện có. Nhờ vào các mô hình tiền huấn luyện dựa trên Transformer, hiệu suất tìm kiếm mã đã được cải thiện đáng kể. Tuy nhiên, do độ phức tạp bậc hai của cơ chế tự chú ý đa đầu, có một giới hạn về độ dài token đầu vào. Để huấn luyện hiệu quả trên các GPU tiêu chuẩn như V100, các mô hình mã tiền huấn luyện hiện có, bao gồm GraphCodeBERT, CodeBERT, RoBERTa (code), lấy 256 token đầu tiên theo mặc định, điều này khiến chúng không thể biểu diễn thông tin đầy đủ của mã dài hơn 256 token. Để giải quyết vấn đề mã dài, chúng tôi đề xuất một baseline mới SEA (Split, Encode and Aggregate), chia mã dài thành các khối mã, mã hóa các khối này thành embeddings, và tổng hợp chúng để có được biểu diễn mã dài toàn diện. Với SEA, chúng ta có thể trực tiếp sử dụng các mô hình tiền huấn luyện dựa trên Transformer để mô hình hóa mã dài mà không thay đổi cấu trúc bên trong và tiền huấn luyện lại. Chúng tôi cũng so sánh SEA với các phương pháp Transformer thưa thớt. Với GraphCodeBERT làm encoder, SEA đạt được điểm mean reciprocal ranking tổng thể là 0.785, cao hơn 10.1% so với GraphCodeBERT trên benchmark CodeSearchNet, chứng minh SEA là một baseline mạnh cho tìm kiếm mã dài.
Từ khóa: tìm kiếm mã, hiểu mã dài, biểu diễn mã

1. Giới thiệu
Một kỹ thuật tìm kiếm mã tốt giúp các nhà phát triển
thúc đẩy phát triển phần mềm bằng cách tìm kiếm
các đoạn mã sử dụng ngôn ngữ tự nhiên. Những tiến
bộ gần đây đã chứng minh hiệu quả
của các phương pháp tiền huấn luyện mã dựa trên Transformer,
bao gồm CodeBERT (Feng et al., 2020), CoCLR
(Huang et al., 2021), và GraphCodeBERT (Guo
et al., 2021), đã cải thiện đáng kể
hiệu suất tìm kiếm mã thông qua tiền huấn luyện tự giám sát
trên corpus mã quy mô lớn.

Tuy nhiên, các phương pháp này gặp phải một hạn chế
vốn có. Độ phức tạp tính toán và bộ nhớ
của cơ chế tự chú ý trong Transformer gốc tăng
theo bậc hai với độ dài đầu vào, tạo ra một ràng
buộc về độ dài đầu vào khoảng 512
token. Để huấn luyện hiệu quả trên các GPU tiêu chuẩn như
V100, GraphCodeBERT và CodeBERT chỉ xem xét
256 token đầu tiên của các đoạn mã và loại
bỏ bất kỳ token nào vượt quá giới hạn này. Tuy nhiên, việc
hạn chế độ dài này có thể dẫn đến vấn đề về độ chính xác, đặc
biệt đối với các đoạn mã dài. Ví dụ, khi
kiểm tra các trường hợp khó của GraphCode-
BERT, chúng tôi thấy rằng GraphCodeBERT có hiệu
suất thấp cho một số đoạn mã dài trong đó
thông tin quan trọng nằm về phía cuối. Như
minh họa trong Hình 1, các từ khóa "Tensor" và
"patches" xuất hiện sau điểm cắt 256 token được đặt bởi
GraphCodeBERT, dẫn đến việc chúng bị loại khỏi
xem xét. Do đó, đoạn mã
tương ứng được xếp hạng ở vị trí 21,148.

Chúng tôi tiếp tục tiến hành các nghiên cứu thực nghiệm trên
GraphCodeBERT trong bộ dữ liệu CodeSearch-
Net được sử dụng công khai (Husain et al., 2019), và quan sát
thấy một sự giảm dần trong hiệu suất tìm kiếm khi độ
dài của mã ground-truth trong truy vấn tăng
lên (tham khảo Bảng 1). Vấn đề này tương tự
với vấn đề văn bản dài trong xử lý ngôn ngữ tự nhiên, mà đã có nhiều phương pháp được đề xuất,
bao gồm xử lý phân cấp (Zhang
et al., 2019b), attention thưa thớt (Child et al., 2019;
Beltagy et al., 2020), và tái hiện ở mức segment
(Dai et al., 2019). Tuy nhiên, việc áp dụng trực tiếp các
phương pháp này cho mã dài gặp hai thử thách.
Thứ nhất, các kỹ thuật này sửa đổi cấu trúc bên
trong của mô hình Transformer, có thể khiến
các tham số tiền huấn luyện hiện có trở nên không hợp lệ. Thứ
hai, mã dài khác với văn bản dài ở chỗ nó là một
ngôn ngữ có cấu trúc cao. Không giống như một tài liệu văn bản dài
có thể được coi là một tổng thể gắn kết với
ngữ nghĩa hoàn chỉnh, ngữ nghĩa của mã là không
liên tục, và các hàm khác nhau được phân bố
qua các vị trí khác nhau. Các thí nghiệm so sánh
được tiến hành trong Phần 6.2 cung cấp bằng chứng arXiv:2208.11271v3 [cs.SE] 26 Mar 2024

--- TRANG 2 ---
Hạng 1 (kết quả sai):
defpatch(self, *args, **kwargs):
return super(Deposit, self).patch(*args, **
kwargs)
Hạng 21148 (ground truth):
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
img = Image.open(fpath)
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches .append(
PIL2array(patch))
return torch.Byte Tensor(
np.array( patches [:n]))256 tokensTraả về một tensor chứa các patches.
Hình 1: Ví dụ trường hợp của GraphCodeBERT.
GraphCodeBERT cắt bỏ các token vượt quá 256 to-
ken. Các token quan trọng được làm nổi bật bằng màu vàng.

ủng hộ những mối quan tâm này.

Do đó, mục tiêu của chúng tôi là chia mã dài trong khi
bảo tồn thông tin ngữ nghĩa của nó. Chúng tôi nhằm
đạt được điều này mà không thay đổi cấu trúc bên trong của
các mô hình tiền huấn luyện dựa trên Transformer hoặc yêu cầu
tiền huấn luyện lại. Để giải quyết vấn đề này, chúng tôi đề xuất SEA
(Split, Encode, and Aggregate) để xử lý mã dài
và có được biểu diễn mã được cải thiện.

Như mô tả trong Hình 2, quá trình bao gồm
chia mã dài thành một tập hợp các đoạn mã,
tiếp theo là sử dụng phương pháp cửa sổ trượt để
tạo ra một tập hợp khối mã chồng lấp một phần.
Các encoder mã hiện có sau đó được sử dụng để có được
embeddings cho mỗi khối mã. Cuối cùng, các
embeddings này được tổng hợp để tạo ra biểu diễn
cho toàn bộ mã dài. Thông qua các thí nghiệm rộng rãi,
chúng tôi đã thấy rằng phương pháp chia dựa trên AST
được đề xuất và phương pháp tổng hợp dựa trên attention vượt trội so với các kỹ thuật khác cho việc chia
và tổng hợp. Do số lượng khối mã khác nhau
được thu thập từ các đoạn mã khác nhau, việc vận hành song song trở nên thách thức. Để
giải quyết vấn đề này, chúng tôi đã thiết kế một module kết hợp-
chia để tăng tốc. Điều quan trọng cần lưu ý là SEA là encoder-agnostic, có nghĩa là nó có thể
được sử dụng với các encoder dựa trên Transformer khác nhau.
Khi so sánh với các baseline encoder dựa trên Transformer khác nhau, SEA đạt được một cải thiện đáng kể trong hiệu suất mean reciprocal ranking (MRR), từ 7% đến 10%.

Bảng 1: Hiệu suất tìm kiếm mã (MRR) của
các độ dài token mã ground-truth khác nhau. Chúng tôi đặt
độ dài cắt mã từ 50 đến 512. Kết quả cao
nhất trong mỗi cột được làm nổi bật. Bộ dữ liệu:
CodeSearchNet python. Mô hình: GraphCodeBERT.

Độ dài token | Độ dài cắt mã
             | 50    | 100   | 256   | 400   | 512
[0, 256)     | 0.6274| 0.6856| 0.6909| 0.6897| 0.6906
[256, 512)   | 0.6239| 0.7027| 0.7237| 0.7258| 0.7265
[512, 768)   | 0.6004| 0.6467| 0.7168| 0.7180| 0.7181
[768, 1024)  | 0.6038| 0.6315| 0.7111| 0.7375| 0.7276
[1024, 1943) | 0.6202| 0.6573| 0.6589| 0.6835| 0.6825

Các đóng góp có thể được tóm tắt như sau:
•Phát hiện và xác minh thực nghiệm về khó
khăn trong việc mô hình hóa mã dài trong các
mô hình tìm kiếm mã dựa trên Transformer hiện có.
•Chúng tôi đề xuất một baseline mới SEA và khám phá
một cài đặt chia và tổng hợp tối ưu.
Chúng tôi cũng thiết kế một module kết hợp-chia để
tăng tốc.
•Thông qua các thí nghiệm rộng rãi, chúng tôi chỉ ra
hiệu quả của SEA được đề xuất với các
baseline encoder khác nhau trong sáu ngôn ngữ lập trình, tạo ra một baseline mạnh cho
tìm kiếm mã. Mã nguồn và dữ liệu thí nghiệm
của chúng tôi có sẵn tại: https://github.
com/fly-dragon211/SEA .

2. Công trình liên quan

2.1. Phương pháp Tìm kiếm Mã

Các nghiên cứu ban đầu (Nie et al., 2016; Yang and Huang,
2017; Rosario, 2000; Hill et al., 2011; Satter and
Sakib, 2016; Lv et al., 2015; Van Nguyen et al.,
2017) trong tìm kiếm mã chủ yếu áp dụng các kỹ thuật truy xuất thông tin (IR) một cách trực tiếp, coi tìm kiếm mã
như một nhiệm vụ khớp văn bản. Cả truy vấn và đoạn mã
đều được coi là văn bản thuần túy, và các
thuật toán khớp văn bản truyền thống như bag-of-
words (BOW) (Schütze et al., 2008), Jaccard (Jac-
card, 1901), term frequency-inverse document fre-
quency (TF-IDF) (Robertson and Jones, 1976),
BM25 (một phiên bản cải tiến của TF-IDF) (Robertson
and Zaragoza, 2009), và mô hình boolean mở rộng (Lv et al., 2015) đã được sử dụng. Vì độ dài mã
có tác động tối thiểu đến độ phức tạp mô hình,
các phương pháp này có thể mã hóa mã dài mà không
cắt bỏ.

Sau khi giới thiệu mô hình tiền huấn luyện quy mô lớn BERT (Devlin et al., 2019), Code-
BERT được đề xuất bởi Feng et al. (2020). Code-
BERT là một mô hình được tiền huấn luyện trên mã nguồn
và chú thích không được gán nhãn, đã đạt được hiệu suất ấn tượng

--- TRANG 3 ---
trong tìm kiếm mã dựa trên văn bản thông qua
fine-tuning trên các bộ dữ liệu văn bản-mã được ghép đôi. Huang
et al. (2021) giới thiệu CoCLR, một phương pháp học
tương phản tăng cường khớp truy vấn-mã.
Sun et al. (2022) phát triển một kỹ thuật dịch mã
nhận biết ngữ cảnh dịch các đoạn mã
thành mô tả ngôn ngữ tự nhiên. Gu et al. (2022)
sử dụng deep hashing và phân loại mã để
tăng tốc tìm kiếm mã, trong khi Chai et al. (2022)
thích ứng few-shot meta-learning với tìm kiếm mã.
Guo et al. (2021) đề xuất GraphCodeBERT, kết
hợp các nhiệm vụ tiền huấn luyện nhận biết cấu trúc để
cải thiện hiểu biết và hiệu suất mã. Gần
đây, Hu et al. (2023) sử dụng một framework tìm kiếm
mã fusion hai giai đoạn kết hợp bi-encoders
và cross-encoders để tăng cường hiệu suất. Tuy
nhiên, độ phức tạp tính toán của Transformers
và bộ nhớ GPU hạn chế thường dẫn đến việc cắt bỏ
các đoạn mã dài.

2.2. Biểu diễn Mã Neural với
Cấu trúc Mã

Gần đây, đã có những tiến bộ đáng chú ý
trong các phương pháp biểu diễn mã neural tận dụng
cấu trúc mã, đặc biệt là Abstract Syntax
Trees (AST), mang lại hiệu suất ấn tượng (Alon
et al., 2020; Sun et al., 2020; Bui et al., 2021;
Kim et al., 2021; Peng et al., 2021; Hellendoorn
et al., 2019; Allamanis et al., 2021; Georgiev et al.,
2022; Ma et al., 2023; Du and Yu, 2023). MMAN
(Wan et al., 2019) kết hợp một lớp fusion attention
đa modal để kết hợp biểu diễn AST và Control Flow
Graph (CFG). ASTNN (Zhang
et al., 2019a) và CAST (Shi et al., 2021) phân
đoạn AST lớn thành các chuỗi của các cây câu lệnh
nhỏ hơn, mã hóa chúng thành vectors bằng cách nắm bắt
thông tin từ vựng và cú pháp của mỗi
câu lệnh. TBCAA (Chen et al., 2019) sử dụng một
mạng convolutional dựa trên cây trên các
AST được tăng cường API. UniXcoder (Guo et al., 2022) tận dụng cả
AST và chú thích mã để làm phong phú biểu diễn mã. GraphCodeBERT (Guo et al., 2021) kết
hợp các quan hệ biến được trích xuất từ AST trong
các nhiệm vụ tiền huấn luyện của nó. Trong công trình của chúng tôi, chúng tôi đặc biệt
nhằm nắm bắt và mô hình hóa thông tin cấu trúc
có mặt trong các đoạn mã dài.

2.3. Transformer cho Văn bản Dài

Ứng dụng của các mô hình Transformer cho văn bản dài
có thể được chia rộng thành hai loại: mở rộng
attention và tăng cường mô hình Trans-
former gốc, và các phương pháp tổng hợp. Loại đầu tiên
bao gồm bốn phương pháp chính: attention
thưa thớt (Child et al., 2019; Correia et al., 2019;
Beltagy et al., 2020; Kitaev et al., 2019; Roy et al.,
2021; Ainslie et al., 2020; Jiang et al., 2020; Gün-
ther et al., 2023), tái hiện (Dai et al., 2019), cơ chế phân cấp (Zhang et al., 2019b; Gao
and Callan, 2022), và attention nén (Ye
et al., 2019; Guo et al., 2019). Attention thưa thớt
hạn chế mỗi token chỉ chú ý đến một tập con
các token khác. Tái hiện tích hợp các phần tử mạng
neural tái hiện vào các mô hình Transformer để mở
rộng span attention của chúng. Các cơ chế phân cấp
mô hình input văn bản dài một cách phân cấp, từ câu
đến đoạn văn. Attention nén có chọn lọc
nén các phần cụ thể của input.

Bảng 2: Thống kê độ dài token mã của bộ đánh giá Code-
SearchNet.

Độ dài | Ruby | JS | Go | Py | Java | Php | Tổng thể
[0, 256) | 16% | 10% | 22% | 14% | 13% | 13% | 14%
[256, 512) | 44% | 29% | 38% | 30% | 27% | 26% | 32%
[512, + ∞) | 41% | 62% | 40% | 56% | 60% | 61% | 54%

Loại thứ hai, các phương pháp tổng hợp, bao
gồm tổng hợp nhiều điểm passage hoặc biểu
diễn cho một tài liệu dài. Ví dụ,
Wang et al. (2019) đề xuất một mô hình BERT
đa passage để chuẩn hóa toàn cục các điểm trả lời qua
tất cả các passage trong nhiệm vụ hỏi đáp. Trong
ngữ cảnh xếp hạng tài liệu, SMITH (Yang et al.,
2020) học biểu diễn tài liệu thông qua
tổng hợp biểu diễn câu phân cấp.
PARADE (Li et al., 2020) sử dụng Max, CNN, At-
tention, và Transformer để tổng hợp các
biểu diễn passage. Tsujimura et al. (2023) sử dụng
phương pháp cửa sổ trượt để quản lý các chuỗi input dài
trong ngữ cảnh của các nhiệm vụ Named Entity
Recognition y tế.

Tuy nhiên, các phương pháp này có thể không hoàn toàn phù
hợp cho mã có cấu trúc cao. Trong các chương trình
được thiết kế tốt, mã trong cùng một module, như
một hàm, được kết nối chặt chẽ, trong khi các tương tác
giữa các module khác nhau được kết nối lỏng lẻo, tuân thủ
nguyên tắc gắn kết cao và
kết nối thấp. Ngược lại, văn bản dài trong ngôn ngữ tự
nhiên có xu hướng thể hiện tính mạch lạc. Trong bài báo này,
chúng tôi điều tra khả năng áp dụng của các phương pháp văn bản dài
trong lĩnh vực tìm kiếm mã và đề xuất một
baseline mới SEA cho tìm kiếm mã dài.

3. Động lực: Vấn đề Mã Dài

3.1. Kiến thức Cơ bản

Tìm kiếm mã nhằm tìm đoạn mã C phù hợp nhất
từ một codebase đã cho khớp với một
truy vấn Q. Đối với một mô hình deep-learning hiện tại, chúng ta
đầu tiên chuyển đổi truy vấn Q và các đoạn mã C thành
các token truy vấn và mã với tokenizer như
BPE (Sennrich et al., 2016). Sau đó chúng ta chuyển đổi
các id token của truy vấn Q và các đoạn mã
C thành biểu diễn vector eq và ec bằng
các encoder mạng neural, và tính toán các phép đo độ tương tự (hoặc

--- TRANG 4 ---
khoảng cách) trong không gian Euclidean như
độ tương tự Cosine hoặc khoảng cách Euclidean để có được
điểm tương tự cross-modal s. Phép tính
có thể được hình thức hóa như sau:

eq = Γ(tokenizer(Q))
ec = Γ′(tokenizer(C)), C ∈ Codebase
s = sim(eq, ec)                    (1)

trong đó Γ và Γ′ là hai encoder mạng neural
được huấn luyện tốt học từ dữ liệu được ghép đôi có nhãn.

3.2. Vấn đề Mã Dài

Để kiểm soát chi phí bộ nhớ và tính toán trong
giai đoạn huấn luyện, việc cắt bỏ mã dài là thông lệ
thông thường. Ví dụ, GraphCodeBERT thường
lấy 256 token mã đầu tiên theo mặc định. Để điều
tra liệu phương pháp cắt bỏ này có dẫn đến
mất thông tin hay không, chúng tôi đã tiến hành thống kê độ dài token
trên CodeSearchNet. Như thể hiện trong Bảng 2, chúng tôi
thấy rằng các đoạn với độ dài token nhỏ hơn
256 chỉ chiếm 14.1%, trong khi 53.5% đoạn mã
vượt quá độ dài mã hóa tối đa
512 token cho Transformers. Điều này cho thấy rằng
việc cắt bỏ dẫn đến mất thông tin cho các đoạn
có độ dài token lớn hơn 256.

Để kiểm tra sự khác biệt hiệu suất tìm kiếm của
GraphCodeBERT trên các tập con truy vấn với
độ dài mã ground truth (GT) khác nhau, chúng tôi chia
tập con test python của CodeSearchNet (CSN) thành 5
tập truy vấn riêng biệt dựa trên độ dài token mã GT
khác nhau. Chúng tôi tính toán Mean Reciprocal Rank
(MRR) của GraphCodeBERT cho các độ dài cắt mã
khác nhau, như thể hiện trong Bảng 1. Đáng chú ý, chúng tôi quan
sát thấy một xu hướng giảm trong hiệu suất tìm kiếm khi
độ dài token mã ground-truth tăng (từ
trên xuống dưới) cho độ dài token mã vượt quá
256 token, cho thấy rằng các đoạn mã dài gây ra
thách thức cho GraphCodeBERT. Hơn nữa, khi độ
dài cắt mã kéo dài từ trái sang phải,
chúng tôi quan sát thấy hiệu suất tìm kiếm tương đối nhất quán
khi độ dài cắt vượt quá
độ dài token. Và xuất hiện một xu hướng tăng
trong hiệu suất tìm kiếm cho các đoạn mã với
độ dài token vượt quá độ dài cắt.
Điều này gợi ý rằng việc đơn giản cắt bỏ mã dài có thể
dẫn đến mất thông tin có giá trị.

4. SEA

Trong phần này, chúng tôi trình bày một tổng quan toàn diện
về SEA, bao gồm kiến trúc mô hình, các phương pháp chia, kỹ thuật tổng hợp,
và phương pháp kết hợp-chia được thiết kế để tăng tốc
suy luận.

4.1. Kiến trúc Mô hình

Chúng tôi giới thiệu SEA của chúng tôi trong phần này. Pipeline
tổng thể được minh họa trong Hình 2. Cho một đoạn mã
C, mục tiêu của chúng tôi là tạo ra một biểu diễn mã
ec. Để đạt được điều này, chúng tôi sử dụng một
phương pháp đa bước. Đầu tiên chúng tôi chia đoạn mã thành
một tập hợp các đoạn mã:

P = Split(C) = {p1, p2, ..., pn}.     (2)

Sau đó chúng tôi sử dụng phương pháp cửa sổ trượt để có được
một tập hợp khối mã chồng lấp một phần:

B = SlidingWindow(P) = {b1, b2, ..., bk}.     (3)

Giả sử kích thước cửa sổ là w và bước là s,
thì số khối mã là k = ⌊(n-w)/s + 1⌋,
trong đó ⌊·⌋ đề cập đến làm tròn xuống. Tiếp theo, chúng tôi sử dụng một
encoder mã, như GraphCodeBERT, để có được
embeddings cho mỗi trong k khối mã:

eB = {eb1, eb2, ..., ebk}.     (4)

Cuối cùng, một phương pháp tổng hợp được áp dụng để kết
hợp k embeddings thành biểu diễn mã
ec:

ec = Aggregation(eB)     (5)

4.2. Phương pháp Chia

Để có được tập hợp đoạn mã, chúng tôi khám phá bốn
phương pháp chia, cụ thể là chia dựa trên khoảng trắng,
chia dựa trên token, chia dựa trên dòng, và chia
dựa trên AST. Chia dựa trên khoảng trắng đơn giản là
chia theo khoảng trắng, dẫn đến việc chia một chuỗi
như "def read_image_file" được chia thành {'def',
'read_image_file'}. Tương tự, chia dựa trên token
và chia dựa trên dòng bao gồm việc chia dựa trên
token và dòng, tương ứng.

Một Abstract Syntax Tree (AST) là một biểu diễn cây
của cấu trúc cú pháp của mã nguồn
được viết bằng một ngôn ngữ lập trình. Mỗi nút trong
AST tương ứng với một cấu trúc cụ thể trong
mã, như biểu thức, câu lệnh, hoặc khai
báo. Cấu trúc phân cấp của AST phản ánh
cú pháp của các ngôn ngữ lập trình, trừu tượng hóa
một số chi tiết cú pháp để tập trung vào cấu trúc
cốt lõi.

Đối với việc chia dựa trên AST, mục tiêu của chúng tôi là nghĩ ra một
phương pháp vừa đơn giản vừa có thể áp
dụng cho nhiều ngôn ngữ lập trình. Lấy cảm hứng
từ CAST (Shi et al., 2021), chúng tôi phân tích một mã nguồn
thành một Abstract Syntax Tree với tree_sitter¹,
và duyệt AST này bằng preorder traversal. Trong
trường hợp của các cấu trúc tổng hợp (tức là for, if, def, v.v.),
như mô tả trong Hình 2(a), chúng tôi định nghĩa tập hợp các
nút AST {head_block, body}, trong đó head_block

¹https://github.com/tree-sitter/
py-tree-sitter

--- TRANG 5 ---
def 
…def 
PIL2array
… return 
np.array
…for 
fpath in
…img = 
Image. 
…...def 
…def 
PIL2array
… return 
np.array
…for 
fpath in
…img = 
Image. 
…...
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches.append(
PIL2array(patch))
return torch.ByteTensor(
np.array(patches[:n]))Định nghĩa
hàmtên read_image_file
bodytham số
tên
body
Câu lệnh Return
Định nghĩa
hàm
…
IN
 Câu lệnh
For
…
…
……
body
 …
Câu lệnh Return
GraphCodeBERT
GraphCodeBERT
GraphCodeBERT ...Module
Fusion
Cửa sổ trượt
 Encoder mã
 Biểu diễn mã(a) Chia mã dựa trên AST.
def 
…def 
PIL2array
… return 
np.array
…for 
fpath in
…img = 
Image. 
…...def 
…def 
PIL2array
… return 
np.array
…for 
fpath in
…img = 
Image. 
…...
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches.append(
PIL2array(patch))
return torch.ByteTensor(
np.array(patches[:n]))Định nghĩa
hàmtên read_image_file
bodytham số
tên
body
Câu lệnh Return
Định nghĩa
hàm
…
IN
 Câu lệnh
For
…
…
……
body
 …
Câu lệnh Return
GraphCodeBERT
GraphCodeBERT
GraphCodeBERT ...Module
Aggregation
Cửa sổ trượt
 Encoder mã
 Biểu diễn mã
(b) Cửa sổ trượt và tổng hợp.

Hình 2: Pipeline của kiến trúc SEA (split, encode and aggregate) được đề xuất.

chịu trách nhiệm chia header và body
của các câu lệnh lồng nhau như if và While state-
ments, trong khi body tương ứng với các khai báo phương thức. Khi gặp một cấu trúc
tổng hợp, chúng tôi chèn một dấu chia trước và sau
head_block, hiệu quả chia một AST lớn thành
một chuỗi các subtree không chồng lấp. Tiếp
theo, dựa trên việc chia AST, chúng tôi xây dựng
tập hợp đoạn mã P bằng cách chia mã gốc
tương ứng.

4.3. Phương pháp Tổng hợp

Meanpooling / Maxpooling. Một phương pháp đơn giản
để tổng hợp các embeddings của k khối mã
là tính trung bình hoặc giá trị tối đa của
embeddings của chúng:

ec = Mean/Max({eb1, eb2, ..., ebk}).     (6)

Tuy nhiên, một hạn chế của meanpooling là mỗi
khối mã đóng góp như nhau vào biểu diễn
cuối cùng, bất kể chất lượng cá nhân của chúng. Tương
tự, maxpooling nổi bật khối
có giá trị cao nhất. Để giải quyết những hạn chế này
và tăng cường quá trình tổng hợp, chúng tôi đề xuất
việc kết hợp các phương pháp embedding có trọng số.

Tổng hợp dựa trên Attention. Nhận ra
rằng không phải tất cả các khối mã đều có tầm quan trọng bằng nhau trong
việc biểu diễn các đoạn mã dài, chúng tôi giới thiệu các trọng số
tự thích ứng α cho mỗi embedding khối trong

Encoder
Linear
Softmax…Encoder
1×𝑑 1×𝑑
𝑘×𝑑
𝑘×1
𝑘×1
1×𝑑1×𝑑
1×𝑑𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
Mean/MaxConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞
Linear
SoftmaxMean/Max tanh
Linear1×𝑑 1×𝑑
𝑘×𝑑
1×𝑑Mean/Max1×𝑑 1×𝑑
𝑘×𝑑
𝑘×128
𝑘×1
1×𝑑1×𝑑𝑘×128
𝑘×1Encoder…Encoder𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞
Encoder…Encoder𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞(a) Attention một lớp với
mean / max.

Encoder
Linear
Softmax…Encoder
1×𝑑 1×𝑑
𝑘×𝑑
𝑘×1
𝑘×1
1×𝑑1×𝑑
1×𝑑𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
Mean/MaxConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞
Linear
SoftmaxMean/Max tanh
Linear1×𝑑 1×𝑑
𝑘×𝑑
1×𝑑Mean/Max1×𝑑 1×𝑑
𝑘×𝑑
𝑘×128
𝑘×1
1×𝑑1×𝑑𝑘×128
𝑘×1Encoder…Encoder𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞
Encoder…Encoder𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘ଵ
ConCat𝐶𝑜𝑑𝑒 𝐵𝑙𝑜𝑐𝑘௞(b) Attention hai lớp với
mean / max.

Hình 3: Các phương pháp tổng hợp dựa trên attention.

quá trình tổng hợp:

ec = Σ(i=1 to k) αi ebi.     (7)

Lấy cảm hứng từ Multi-Instance Learning dựa trên attention
(Li et al., 2021) và Lightweight Attentional Feature
Fusion (Hu et al., 2022), chúng tôi tính toán các trọng số
{α1, ..., αk} như sau:

{a1, ..., ak} = softmax(Linear({eb1, ..., ebk})).     (8)

--- TRANG 6 ---
Bảng 3: Phân tích chi phí tính toán. n là độ dài chuỗi, d là chiều biểu diễn, k là số khối mã, l là số lớp. Lưu ý rằng chúng tôi sử dụng attention một lớp cho SEA.

Phương pháp | Tham số | Độ phức tạp
GraphCodeBERT | 5d²·l | O(n²·d·l)
SEA | 5d²·l+d | O(n²/k·d·l)

Đối với attention một lớp, Linear đề cập đến một lớp
kết nối đầy đủ chuyển đổi chiều thành 1.
Đối với attention hai lớp, Linear đề cập đến hai lớp
kết nối đầy đủ đầu tiên chuyển đổi chiều
thành 128 và sau đó chuyển đổi chiều thành 1. Hơn
nữa, như minh họa trong Hình 3(a) và Hình
3(b), chúng tôi khám phá sự kết hợp của attention với
các phương pháp meanpooling / maxpooling:

ec = Σ(i=1 to k)(αi ebi) + Mean/Max({eb1, eb2, ..., ebk}).     (9)

Để phân tích chi phí tính toán, SEA sử dụng
phương pháp cửa sổ trượt để giảm đáng kể độ phức tạp
xuống 1/k. Độ phức tạp gốc của Graph-
CodeBERT được cho bởi O(n²·d·l), trong đó n, d, l
biểu diễn độ dài chuỗi, chiều biểu diễn,
và số lớp, tương ứng. Bằng cách sử dụng
phương pháp cửa sổ trượt, độ phức tạp cho mỗi
cửa sổ trở thành O(w²·d·l), trong đó w biểu thị
kích thước cửa sổ. Đặt bước s = w, tổng
số khối mã trở thành k = n/w, dẫn đến
kích thước cửa sổ w = n/k. Do đó, độ phức tạp
tổng thể được đơn giản hóa thành:

O(k·w²·d·l) = O(k·(n/k)²·d·l) = O(n²/k·d·l).     (10)

Việc giảm đáng kể độ phức tạp này xuống 1/k cho phép
SEA mã hóa mã dài với ít bộ nhớ và
chi phí tính toán hơn.

Hơn nữa, như thể hiện trong Bảng 3, chúng tôi quan sát
rằng so với GraphCodeBERT, SEA kết
hợp Aggregation attention một lớp chỉ giới thiệu
thêm d tham số có thể học. Mặc dù
sự tăng khiêm tốn này trong số lượng tham số, nó đóng một
vai trò quan trọng trong việc tăng cường hiệu quả của
giai đoạn tổng hợp, như các thí nghiệm của chúng tôi sẽ cung cấp
bằng chứng trong Phần 6.1.

4.4. Xử lý Batch

Để tăng cường hiệu quả suy luận trên các bộ dữ liệu lớn,
cần thiết phải thiết kế một phương pháp xử lý batch
có khả năng mã hóa nhiều đoạn mã dài
đồng thời. Như đã nêu trong Phần 4.1, chúng tôi
có được nhiều khối mã từ mỗi đoạn mã dài.
Tuy nhiên, do số lượng khác nhau của

Encoder mãFusionBatch
mã
Fusion
FusionBatch
khốiEmbeddings khối
②
②
②①Khối
mã20221014 sửa đổi
89%11%
độ dài token <= 128
độ dài token > 128
9%
91%độ dài token <= 256
độ dài token > 256

Hình 4: Phương pháp kết hợp-chia cho xử lý batch. ① và ② đề cập đến các phương pháp kết hợp và chia.

các khối mã tương ứng cho các đoạn mã dài
khác nhau, việc thiết kế một phương pháp xử lý batch
chung gặp thách thức.

Để giải quyết vấn đề này, chúng tôi giới thiệu phương pháp kết hợp-
chia. Như minh họa trong Hình 4, giả sử
kích thước batch là 3 (bao gồm ba đoạn mã),
số lượng khối mã tương ứng cho mỗi
đoạn là 2, 3, và 1, tương ứng. Chúng tôi bắt đầu bằng
việc kết hợp sáu khối mã này thành một batch khối
và thiết lập một ánh xạ M liên kết
chỉ số mã với chỉ số khối. Tiếp theo, chúng tôi nhập
batch khối này vào encoder mã song song
để có được embeddings khối. Cuối cùng, tận dụng
thông tin từ ánh xạ M, chúng tôi tách
các embeddings thành ba nhóm và nhập chúng
vào module tổng hợp để có được các biểu diễn mã
riêng biệt.

5. Thiết kế Thí nghiệm

5.1. Bộ dữ liệu

Chúng tôi tiến hành các thí nghiệm trên bộ dữ liệu Code-
SearchNet (Husain et al., 2019) được sử dụng rộng rãi,
bao gồm sáu ngôn ngữ lập trình, tức là Ruby,
JavaScript, Go, Python, Java, và PHP. Theo
phương pháp trong (Guo et al., 2021), chúng tôi áp dụng
lọc để loại bỏ các truy vấn chất lượng thấp và mở rộng
tập hợp truy xuất để bao gồm toàn bộ corpus mã.

5.2. Các chỉ số Đánh giá

Trong đánh giá của chúng tôi, chúng tôi sử dụng hai tiêu chí tự động
phổ biến: MRR (Mean Reciprocal Ranking) và R@k
(độ chính xác top-k, k=1, 5, 10, 100). Chúng
thường được sử dụng trong các nghiên cứu tìm kiếm mã trước đây (Lv
et al., 2015; Gu et al., 2018; Sachdev et al., 2018;
Husain et al., 2019; Feng et al., 2020; Huang et al.,
2021; Guo et al., 2021). Ngoài ra, chúng tôi báo cáo
số lượng tham số và thời gian suy luận như
thước đo hiệu quả.

5.3. Cài đặt Thí nghiệm

Baseline của chúng tôi là GraphCodeBERT. Các tham số
của encoder mã và ngôn ngữ tự nhiên được

--- TRANG 7 ---
Bảng 4: Hiệu suất tìm kiếm của các biến thể SEA khác nhau. Bộ dữ liệu: CodeSearchNet Ruby.

Cửa sổ | Bước | Chia | Tổng hợp | MRR | R@1 | R@5 | R@10 | R@100
GraphCodeBERT | – | – | – | – | 0.6948 | 59.3 | 82.1 | 87.3 | 96.5
SEA-SpaceSplitting256 | 128 | Space | Maxpooling | 0.6919 | 58.5 | 82.0 | 87.2 | 95.2
256 | 128 | Space | Meanpooling | 0.6929 | 58.3 | 83.0 | 87.4 | 95.6
256 | 128 | Space | Attention (two layers) | 0.6940 | 58.7 | 83.4 | 87.1 | 94.8
256 | 128 | Space | Attention (two layers) + Mean | 0.7490 | 66.3 | 85.2 | 88.9 | 94.4
256 | 128 | Space | Attention (one layer) | 0.6989 | 59.6 | 82.2 | 86.8 | 95.0
256 | 128 | Space | Attention (one layer) + Mean | 0.7495 | 66.1 | 86.3 | 89.0 | 94.3
128 | 64 | Space | Attention (one layer) + Mean | 0.7545 | 66.2 | 87.5 | 90.2 | 95.2
64 | 32 | Space | Attention (one layer) + Mean | 0.7431 | 65.1 | 85.6 | 88.7 | 94.0
SEA-TokenSplitting256 | 128 | Token | Attention (one layer) + Mean | 0.7752 | 68.4 | 89.1 | 91.9 | 96.0
128 | 64 | Token | Attention (one layer) + Mean | 0.7606 | 67.2 | 87.5 | 91.3 | 95.6
64 | 32 | Token | Attention (one layer) + Mean | 0.7352 | 62.8 | 87.2 | 90.6 | 95.0
SEA-LineSplitting64 | 32 | Line | Attention (one layer) + Mean | 0.7635 | 67.3 | 88.2 | 91.3 | 95.6
32 | 16 | Line | Attention (one layer) + Mean | 0.7537 | 66.1 | 87.2 | 90.3 | 95.2
16 | 8 | Line | Attention (one layer) + Mean | 0.7498 | 65.5 | 86.9 | 90.3 | 95.0
SEA-ASTSplitting64 | 32 | AST | Attention (one layer) + Mean | 0.7539 | 65.7 | 91.4 | 95.0 | 97.6
32 | 16 | AST | Attention (one layer) + Mean | 0.7762 | 68.8 | 89.1 | 92.0 | 96.4
16 | 8 | AST | Attention (one layer) + Mean | 0.7744 | 68.8 | 88.7 | 91.4 | 96.3

khởi tạo bởi GraphCodeBERT. Để huấn luyện, chúng tôi ngẫu
nhiên chọn 6 khối mã từ các khối mã được chia
của một mã dài. Kích thước batch huấn luyện là
32. Để đánh giá, chúng tôi sử dụng tất cả các khối mã được chia
của một mã dài. Kích thước batch được đánh giá là 256.
Tất cả các thí nghiệm được tiến hành trên một máy với
Intel Xeon E5-2698v4 2.2Ghz 20-Core CPU và
hai Tesla V100 32GB GPUs.

6. Kết quả Thí nghiệm

6.1. Cấu hình SEA Tối ưu

Để xác định cấu hình tối ưu cho SEA, chúng tôi
tiến hành các thí nghiệm bằng cách thay đổi kiến trúc của chúng tôi
sử dụng các phương pháp chia mã khác nhau và các
phương pháp tổng hợp, trong khi đo lường những thay đổi
kết quả trong hiệu suất tìm kiếm. Cho rằng
bộ dữ liệu CodeSearchNet Ruby tương đối nhỏ,
chúng tôi tập trung vào việc tiến hành các thí nghiệm trên tập con ruby,
và chúng tôi trình bày kết quả trong Bảng 4.

Trong các hàng SpaceSplitting của Bảng 4, chúng tôi thử nghiệm
với các phương pháp tổng hợp khác nhau như mô tả trong
Phần 4.3. Phát hiện của chúng tôi cho thấy rằng sử dụng bất kỳ
phương pháp tổng hợp đơn lẻ nào một cách riêng lẻ không mang lại
cải thiện hiệu suất đáng kể so
với Baseline GraphCodeBERT. Tuy nhiên, khi
kết hợp phương pháp attention với meanpooling, chúng tôi
quan sát thấy cải thiện hiệu suất đáng kể.
Cụ thể, phương pháp tổng hợp Attention (one layer) + Mean
cải thiện MRR và R@1 lần lượt 7.9%
và 11.5%. Do đó, cho các
thí nghiệm tiếp theo, chúng tôi chọn sử dụng phương pháp tổng hợp Attention
(one layer) + Mean.

Trong các hàng SpaceSplitting, TokenSplitting,
LineSplitting, ASTSplitting của Bảng 4, chúng tôi khám phá các
phương pháp chia mã khác nhau, như chi tiết trong Phần 4.2. Đối với
các phương pháp chia dựa trên space và token, chúng tôi đặt

kích thước cửa sổ từ 64 đến 256 do tính hạt
nhỏ hơn của việc chia. Ngược lại, đối với các
phương pháp chia dựa trên line và AST, chúng tôi đặt kích thước cửa sổ từ
16 đến 64. Đáng chú ý, chúng tôi quan sát thấy rằng phương pháp chia
dựa trên AST thể hiện hiệu suất xuất sắc,
đạt được MRR và R@1 cao nhất với kích thước cửa sổ
32. Kết quả là, trong các thí nghiệm tiếp theo,
SEA đề cập đến SEA-ASTSplitting với kích thước cửa sổ
32, kích thước bước 16 và phương pháp tổng hợp Attention (one layer)
+ Mean.

6.2. So sánh với Ba Sparse
Transformers

Trong phần này, chúng tôi tiến hành so sánh giữa
SEA và ba sparse Transformers, BIGBIRD (Za-
heer et al., 2020), Longformer (Beltagy et al., 2020),
và LongCoder (Guo et al., 2023). BIGBIRD và
Longformer là hai Transformer hướng tài liệu dài
nổi tiếng. LongCoder sử dụng một cơ chế cửa sổ trượt
để xử lý input mã dài cho hoàn thành mã. Cụ thể, chúng tôi tận dụng các
mô hình bigbird-roberta-base², longformer-base-4096³ và
longcoder-base⁴, với độ dài token
1024. Do BIGBIRD và Longformer không được
tiền huấn luyện trên bộ dữ liệu mã, chúng tôi cũng tiến hành
các thí nghiệm để khởi tạo BIGBIRD và Longformer
với các tham số của GraphCodeBERT. Kết
quả được trình bày trong Bảng 5. So sánh các
kết quả trước và sau khi khởi tạo BIGBIRD và
Longformer với các tham số của GraphCode-
BERT, chúng tôi thấy rằng kết quả MRR cải thiện từ
0.2952 và 0.5016 lên 0.6121 và 0.6595, tương

²https://huggingface.co/google/bigbird-roberta-base
³https://huggingface.co/allenai/longformer-base-
4096
⁴https://huggingface.co/microsoft/longcoder-base

--- TRANG 8 ---
Bảng 5: So sánh với sparse Transformers. Ký hiệu (G) cho biết rằng mô hình được khởi tạo với các tham số GraphCodeBERT. Thời gian suy luận mã được xác định bằng cách chọn ngẫu nhiên 1000 mã và tính toán thời gian suy luận trung bình. Chúng tôi lặp lại mỗi lần tính toán thí nghiệm ba lần và báo cáo trung bình và độ lệch chuẩn. Bộ dữ liệu: CodeSearchNet Ruby. SEA vượt trội hơn các mô hình khác một cách đáng kể (p < 0.01).

Mô hình | #Param. | Độ dài Token | Thời gian Suy luận | MRR | R@1 | R@5 | R@10
GraphCodeBERT | 124.6M | 256 | 6.3 ±0.3ms | 0.6948 | 59.3 | 82.1 | 87.3
BIGBIRD | 127.5M | 1024 | 20.1 ±0.2ms | 0.2952 | 19.2 | 39.8 | 51.1
BIGBIRD (G) | 127.5M | 1024 | 19.8 ±0.0ms | 0.6121 | 50.8 | 74.2 | 80.7
Longformer | 148.7M | 1024 | 33.7 ±0.2ms | 0.5128 | 39.9 | 65.3 | 72.4
Longformer (G) | 148.7M | 1024 | 33.7 ±0.1ms | 0.6595 | 55.1 | 79.4 | 84.0
LongCoder | 149.6M | 1024 | 68.6 ±0.2ms | 0.4718 | 35.8 | 61.1 | 67.8
SEA | 124.6M | - | 7.2 ±0.5ms | 0.7762 | 68.8 | 89.1 | 92.0
- w/o combine-divide | 124.6M | - | 24.3 ±2.4ms | 0.7762 | 68.8 | 89.1 | 92.0

ứng. Chúng tôi cho rằng khoảng cách hiệu suất này do
nhu cầu tiền huấn luyện lại các mô hình ban đầu được
tiền huấn luyện trên các bộ dữ liệu ngôn ngữ tự nhiên. Chúng tôi quan
sát thấy rằng MRR của LongCoder là 0.4718, điều này
thể hiện một sự giảm đáng kể so với
GraphCodeBERT, gợi ý rằng LongCoder có thể
chủ yếu phù hợp cho các nhiệm vụ Hoàn thành Mã. Chúng tôi
cũng tiến hành t-tests giữa SEA của chúng tôi và các
baseline khác, và kết quả chứng minh rằng SEA
vượt trội đáng kể so với tất cả các baseline sparse Transformer
(p < 0.01), làm nổi bật hiệu suất vượt trội của nó
trong lĩnh vực tìm kiếm mã.

Về tham số mô hình và hiệu quả tìm kiếm,
SEA nổi bật vì nó có số lượng tham số
thấp hơn và thời gian suy luận ngắn hơn so
với BIGBIRD, Longformer và LongCoder. Đáng
chú ý là số lượng tham số của SEA gần như
tương đồng với GraphCodeBERT, chỉ khác nhau bằng
việc thêm một lớp attention duy nhất. Tuy nhiên,
thay đổi nhỏ này dẫn đến một cải thiện đáng kể trong
hiệu suất tìm kiếm. Chúng tôi cũng trình bày kết quả thí nghiệm
mà không sử dụng phương pháp kết hợp-chia
trong Bảng 5. Chúng tôi quan sát thấy rằng trong khi
hiệu suất tìm kiếm vẫn ổn định, thời gian suy luận
tăng hơn ba lần. Điều này làm nổi bật
cải thiện đáng kể trong thời gian suy luận do
phương pháp kết hợp-chia mang lại, từ đó xác nhận hiệu quả của nó trong việc tăng tốc
quá trình suy luận của mô hình.

6.3. Hiệu suất SEA trên Độ dài Mã
Khác nhau

Để khám phá cải thiện của SEA được đề xuất
cho các đoạn mã với độ dài khác nhau, chúng tôi trình bày
so sánh hiệu suất tìm kiếm giữa phương pháp
baseline GraphCodeBERT và SEA dưới
các độ dài token mã ground-truth khác nhau. Kết
quả được mô tả trong Hình 5.

Đáng chú ý, hiệu suất truy xuất của mỗi tập con truy vấn
thể hiện những cải thiện đáng chú ý, đặc
biệt đối với kết quả truy xuất mã dài. Chúng tôi cho rằng

0.600.620.640.660.680.700.720.740.760.780.80
[0, 256) [256, 512) [512, 768) [768, 1024) [1024, 1943)MRR
Độ dài token mã
GraphCodeBERT
 SEA

Hình 5: So sánh hiệu suất giữa
GraphCodeBERT và SEA trong các độ dài token mã ground-truth
khác nhau. So với GraphCodeBERT,
SEA đạt được hiệu suất tốt hơn đáng kể (p < 0.01)
cho các độ dài token mã khác nhau.

cải thiện này là do hai yếu tố quan trọng. Thứ nhất, module tổng hợp
của SEA thích ứng nắm bắt và
kết hợp thông tin từ các phân đoạn đa dạng của
mã dài, dẫn đến một biểu diễn mã toàn diện
và có thông tin hơn. Thứ hai,
phương pháp chia mã được sử dụng bởi SEA có thể được
xem như một hình thức tăng cường dữ liệu, cung cấp
ngữ cảnh và biến thể bổ sung giúp củng cố
biểu diễn mã. Tóm lại, SEA
mang lại một biểu diễn mã mạnh mẽ hơn, cải thiện đáng kể
hiệu suất truy xuất tổng thể.

6.4. So sánh Baseline trên
Nhiều Ngôn ngữ Lập trình

Để đảm bảo so sánh công bằng và có thể tái tạo, chúng tôi
cẩn thận chọn các baseline dựa trên tiền huấn luyện đáp ứng
ba tiêu chí sau: 1) Mã nguồn
có sẵn công khai; 2) Mô hình tổng thể có thể
thích ứng với tất cả sáu ngôn ngữ lập trình
trên bộ dữ liệu CodeSearchNet; 3) Bài báo được
đánh giá ngang hàng nếu nó được xuất bản như một bài báo nghiên cứu.

--- TRANG 9 ---
Bảng 6: MRR trên sáu ngôn ngữ của bộ dữ liệu CodeSearchNet. SEA ở đây đề cập đến SEA-ASTSplitting với kích thước cửa sổ 32 và bước 16. SEA +RoBERTa đề cập đến SEA với RoBERTa làm encoder mã. SEA vượt trội so với các baseline một cách đáng kể (p < 0.01).

Mô hình / Phương pháp | Ruby | Javascript | Go | Python | Java | Php | Tổng thể
RoBERTa | 0.587 | 0.517 | 0.850 | 0.587 | 0.599 | 0.560 | 0.617
UniXcoder | 0.586 | 0.603 | 0.881 | 0.695 | 0.687 | 0.644 | 0.683
CodeBERT | 0.679 | 0.620 | 0.882 | 0.672 | 0.676 | 0.628 | 0.693
GraphCodeBERT | 0.703 | 0.644 | 0.897 | 0.692 | 0.691 | 0.649 | 0.713
SEA +RoBERTa | 0.651 (10.9% ↑) | 0.593 (14.6% ↑) | 0.879 (3.5% ↑) | 0.633 (7.9% ↑) | 0.666 (11.1% ↑) | 0.647 (15.6% ↑) | 0.678 (10.0% ↑)
SEA +UniXcoder | 0.648 (10.7% ↑) | 0.692 (14.8% ↑) | 0.896 (1.8% ↑) | 0.707 (1.7% ↑) | 0.739 (7.5% ↑) | 0.712 (10.5% ↑) | 0.732 (7.3% ↑)
SEA +CodeBERT | 0.742 (9.3% ↑) | 0.696 (12.3% ↑) | 0.905 (2.6% ↑) | 0.714 (6.2% ↑) | 0.732 (8.3% ↑) | 0.711 (13.2% ↑) | 0.750 (8.3% ↑)
SEA +GraphCodeBERT | 0.776 (10.4% ↑) | 0.742 (15.2% ↑) | 0.921 (2.7% ↑) | 0.754 (8.9% ↑) | 0.768 (11.1% ↑) | 0.748 (15.3% ↑) | 0.785 (10.1% ↑)

per. Do đó, chúng tôi chọn bốn phương pháp deep end-to-end
approaches: RoBERTa (Liu et al., 2019), UniX-
coder (Guo et al., 2022), CodeBERT (Feng et al.,
2020), và GraphCodeBERT (Guo et al., 2021).

Trong Bảng 6, chúng tôi trình bày kết quả MRR, chứng minh
rằng SEA vượt trội so với tất cả các phương pháp trên
tất cả sáu ngôn ngữ lập trình. Đáng chú ý, kết luận này
vẫn nhất quán cho chỉ số recall và
một biến thể khác của SEA, kết quả có thể
được tìm thấy trong gói sao chép của chúng tôi. Những phát hiện này
củng cố tính ưu việt của SEA so
với các baseline dựa trên tiền huấn luyện trên nhiều
ngôn ngữ lập trình đa dạng.

7. Kết luận

Trong bài báo này, chúng tôi giải quyết thách thức của việc
mô hình hóa mã dài một cách hiệu quả cho tìm kiếm mã. Chúng tôi
giới thiệu SEA, một phương pháp hiệu quả mang lại
biểu diễn mã được cải thiện cho các đoạn mã dài.
Mặc dù tính đơn giản của nó, kết quả thí nghiệm của chúng tôi
cho thấy hiệu quả và hiệu suất đáng chú ý
của SEA. Chúng tôi tin rằng công trình này mở ra những khả năng mới
cho tìm kiếm mã.

8. Tuyên bố Đạo đức

Các mở rộng và ứng dụng tương lai phát sinh từ
công trình của chúng tôi nên chú ý đến tác động môi trường
của việc huấn luyện các mô hình quy mô lớn. Họ nên
tích cực tránh việc lạm dụng tiềm năng bằng cách tìm kiếm với
ý định độc hại. Tuy nhiên, không có khả năng mô hình
trong hình thức hiện tại sẽ dẫn đến tác động như vậy
trong tương lai gần. Mô hình của chúng tôi cũng có tiềm năng
tạo ra tác động tích cực trong các lĩnh vực như
tìm kiếm mã, hiểu mã dài và biểu diễn mã.

9. Lời cảm ơn

Công việc được mô tả trong bài báo này được hỗ trợ một phần bởi
CCF-Huawei Populus Grove Fund CCF-
HuaweiSE202301.

10. Tài liệu tham khảo

Joshua Ainslie, Santiago Ontañón, Chris Alberti,
Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang,
và Li Yang. 2020. ETC: encoding long and
structured inputs in transformers. Trong EMNLP.

Miltiadis Allamanis, Henry Jackson-Flux, và Marc
Brockschmidt. 2021. Self-supervised bug detec-
tion and repair. Trong NeurIPS.

Uri Alon, Roy Sadaka, Omer Levy, và Eran Yahav.
2020. Structural language models of code. Trong
ICML.

Iz Beltagy, Matthew E Peters, và Arman Cohan.
2020. Longformer: The long-document trans-
former. arXiv.

Nghi DQ Bui, Yijun Yu, và Lingxiao Jiang. 2021.
Treecaps: Tree-based capsule networks for
source code processing. Trong AAAI.

Yitian Chai, Hongyu Zhang, Beijun Shen, và Xi-
aodong Gu. 2022. Cross-domain deep code
search with few-shot meta learning. arXiv.

Long Chen, Wei Ye, và Shikun Zhang. 2019. Cap-
turing source code semantics via tree-based con-
volution over api-enhanced ast. Trong CF.

Rewon Child, Scott Gray, Alec Radford, và Ilya
Sutskever. 2019. Generating long sequences
with sparse transformers. arXiv.

Gonçalo M Correia, Vlad Niculae, và André FT
Martins. 2019. Adaptively sparse transformers.
Trong EMNLP-IJCNLP.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G
Carbonell, Quoc Le, và Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language models
beyond a fixed-length context. Trong ACL.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và
Kristina Toutanova. 2019. BERT: pre-training

--- TRANG 10 ---
of deep bidirectional transformers for language
understanding. Trong NAACL-HLT.

Yali Du và Zhongxing Yu. 2023. Pre-training code
representation with semantic flow graph for ef-
fective bug localization. Trong FSE/ESEC.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,
Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, và Ming Zhou. 2020.
Codebert: A pre-trained model for programming
and natural languages. Trong EMNLP.

Luyu Gao và Jamie Callan. 2022. Long document
re-ranking with modular re-ranker. Trong SIGIR.

Dobrik Georgiev, Marc Brockschmidt, và Miltiadis
Allamanis. 2022. Heat: Hyperedge attention net-
works. arXiv.

Wenchao Gu, Yanlin Wang, Lun Du, Hongyu
Zhang, Shi Han, Dongmei Zhang, và Michael
Lyu. 2022. Accelerating code search with deep
hashing and code classification. Trong ACL.

Xiaodong Gu, Hongyu Zhang, và Sunghun Kim.
2018. Deep code search. Trong ICSE.

Michael Günther, Jackmin Ong, Isabelle Mohr,
Alaeddine Abdessalem, Tanguy Abel, Moham-
mad Kalim Akram, Susana Guzman, Georgios
Mastrapas, Saba Sturua, Bo Wang, et al. 2023.
Jinaembeddings2: 8192-token general-purpose
text embeddings for long documents. arXiv.

Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming
Zhou, và Jian Yin. 2022. Unixcoder: Unified
cross-modal pre-training for code representation.
Trong ACL.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-
fano, Shao Kun Deng, Colin B. Clement, Dawn
Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
và Ming Zhou. 2021. Graphcodebert: Pre-
training code representations with data flow. Trong
ICLR.

Daya Guo, Canwen Xu, Nan Duan, Jian Yin, và Ju-
lian J. McAuley. 2023. Longcoder: A long-range
pre-trained language model for code completion.
Trong ICML, Proceedings of Machine Learning Re-
search. PMLR.

Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,
Xiangyang Xue, và Zheng Zhang. 2019. Star-
transformer. Trong NAACL-HLT. Association for Com-
putational Linguistics.

Vincent J Hellendoorn, Charles Sutton, Rishabh
Singh, Petros Maniatis, và David Bieber. 2019.
Global relational models of source code. Trong ICLR.

Emily Hill, Lori Pollock, và K Vijay-Shanker. 2011.
Improving source code search with natural lan-
guage phrasal representations of method signa-
tures. Trong ASE. IEEE.

Fan Hu, Aozhu Chen, Ziyue Wang, Fangming Zhou,
Jianfeng Dong, và Xirong Li. 2022. Lightweight
attentional feature fusion: A new baseline for
text-to-video retrieval. Trong ECCV. Springer.

Fan Hu, Yanlin Wang, Lun Du, Xirong Li, Hongyu
Zhang, Shi Han, và Dongmei Zhang. 2023. Re-
visiting code search in a two-stage paradigm. Trong
WSDM.

Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong,
Ke Xu, Daxin Jiang, Ming Zhou, và Nan Duan.
2021. Cosqa: 20,000+ web queries for code
search and question answering. Trong ACL.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Mil-
tiadis Allamanis, và Marc Brockschmidt. 2019.
Codesearchnet challenge: Evaluating the state
of semantic code search. arXiv.

Paul Jaccard. 1901. Étude comparative de la distri-
bution florale dans une portion des alpes et des
jura. Bull Soc Vaudoise Sci Nat, trang 547–579.

Jyun-Yu Jiang, Chenyan Xiong, Chia-Jung Lee, và
Wei Wang. 2020. Long document ranking with
query-directed sparse transformer. Trong EMNLP
Findings.

Seohyun Kim, Jinman Zhao, Yuchi Tian, và Satish
Chandra. 2021. Code prediction by feeding trees
to transformers. Trong ICSE.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya.
2019. Reformer: The efficient transformer. Trong
ICLR.

Canjia Li, Andrew Yates, Sean MacAvaney, Ben He,
và Yingfei Sun. 2020. Parade: Passage rep-
resentation aggregation for document reranking.
ACM Transactions on Information Systems.

Xirong Li, Yang Zhou, Jie Wang, Hailan Lin,
Jianchun Zhao, Dayong Ding, Weihong Yu, và
Youxin Chen. 2021. Multi-modal multi-instance
learning for retinal disease recognition. Trong
ACM MM.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, và Veselin Stoyanov.
2019. Roberta: A robustly optimized bert pre-
training approach. arXiv.

Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei
Wang, Dongmei Zhang, và Jianjun Zhao. 2015.
Codehow: Effective code search based on api
understanding and extended boolean model (e).
Trong ASE.

--- TRANG 11 ---
Y Ma, Yali Du, và Ming Li. 2023. Capturing the
long-distance dependency in the control flow
graph via structural-guided attention for bug lo-
calization. Trong IJCAI.

Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, và
Xiaochen Li. 2016. Query expansion based
on crowd knowledge for code search. IEEE
Transactions on Services Computing, trang
771–783.

Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, và
Zhi Jin. 2021. Integrating tree path in transformer
for code representation. Trong NeurIPS.

Stephen Robertson và Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and
beyond. Now Publishers Inc.

Stephen E Robertson và K Sparck Jones. 1976.
Relevance weighting of search terms. Journal
of the American Society for Information science,
trang 129–146.

Barbara Rosario. 2000. Latent semantic indexing:
An overview. Techn. rep. INFOSYS, trang 1–16.

Aurko Roy, Mohammad Saffar, Ashish Vaswani,
và David Grangier. 2021. Efficient content-
based sparse attention with routing trans-
formers. Transactions of the Association for
Computational Linguistics, 9:53–68.

Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun
Kim, Koushik Sen, và Satish Chandra. 2018.
Retrieval on source code: a neural code search.
Trong MAPL.

Abdus Satter và Kazi Sakib. 2016. A search log
mining based query expansion technique to im-
prove effectiveness in code search. Trong ICCIT,
trang 586–591. IEEE.

Hinrich Schütze, Christopher D Manning, và
Prabhakar Raghavan. 2008. Introduction to
information retrieval, volume 39. Cambridge Uni-
versity Press Cambridge.

Rico Sennrich, Barry Haddow, và Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. Trong ACL.

Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,
Shi Han, Dongmei Zhang, và Hongbin Sun.
2021. Cast: Enhancing code summarization
with hierarchical splitting and reconstruction of
abstract syntax trees. Trong EMNLP.

Weisong Sun, Chunrong Fang, Yuchen Chen,
Guanhong Tao, Tingxu Han, và Quanjun Zhang.
2022. Code search based on context-aware code
translation. arXiv.

Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun,
Lili Mou, và Lu Zhang. 2020. Treegen: A tree-
based transformer architecture for code genera-
tion. Trong AAAI.

Tomoki Tsujimura, Koshi Yamada, Ryuki Ida,
Makoto Miwa, và Yutaka Sasaki. 2023. Contex-
tualized medication event extraction with strid-
ing ner and multi-turn qa. Journal of Biomedical
Informatics, trang 104416.

Thanh Van Nguyen, Anh Tuan Nguyen, Hung Dang
Phan, Trong Duc Nguyen, và Tien N Nguyen.
2017. Combining word2vec with revised vector
space model for better code retrieval. Trong ICSE-C.
IEEE.

Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu,
Zhou Zhao, Jian Wu, và Philip S. Yu. 2019.
Multi-modal attention network learning for seman-
tic source code retrieval. Trong ASE.

Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh
Nallapati, và Bing Xiang. 2019. Multi-passage
bert: A globally normalized bert model for open-
domain question answering. Trong EMNLP.

Liu Yang, Mingyang Zhang, Cheng Li, Michael
Bendersky, và Marc Najork. 2020. Beyond
512 tokens: Siamese multi-depth transformer-
based hierarchical encoder for long-form docu-
ment matching. Trong CIKM.

Yangrui Yang và Qing Huang. 2017. Iecs: Intent-
enforced code search via extended boolean
model. Journal of Intelligent & Fuzzy Systems,
trang 2565–2576.

Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, và
Zheng Zhang. 2019. Bp-transformer: Modelling
long-range context via binary partitioning. arXiv.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, và Amr Ahmed. 2020. Big bird:
Transformers for longer sequences. Trong NeurIPS.

Jian Zhang, Xu Wang, Hongyu Zhang, Hailong
Sun, Kaixuan Wang, và Xudong Liu. 2019a. A
novel neural source code representation based
on abstract syntax tree. Trong ICSE.

Xingxing Zhang, Furu Wei, và Ming Zhou. 2019b.
Hibert: Document level pre-training of hierarchi-
cal bidirectional transformers for document sum-
marization. Trong ACL.
