# 2208.11271.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/autoencoder/2208.11271.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 2936530 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
Giáº£i quyáº¿t TÃ¬m kiáº¿m MÃ£ dÃ i vá»›i Chia, MÃ£ hÃ³a vÃ  Tá»•ng há»£p
Fan Hu1âˆ—, Yanlin Wang2â€ â€¡, Lun Du3,
Hongyu Zhang4, Shi Han3, Dongmei Zhang3, Xirong Li1
1Äáº¡i há»c NhÃ¢n dÃ¢n Trung Quá»‘c
2TrÆ°á»ng Ká»¹ thuáº­t Pháº§n má»m, Äáº¡i há»c Trung SÆ¡n
3Microsoft
4Äáº¡i há»c TrÃ¹ng KhÃ¡nh
TÃ³m táº¯t
TÃ¬m kiáº¿m mÃ£ vá»›i ngÃ´n ngá»¯ tá»± nhiÃªn giÃºp chÃºng ta tÃ¡i sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n mÃ£ hiá»‡n cÃ³. Nhá» vÃ o cÃ¡c mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n dá»±a trÃªn Transformer, hiá»‡u suáº¥t tÃ¬m kiáº¿m mÃ£ Ä‘Ã£ Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ. Tuy nhiÃªn, do Ä‘á»™ phá»©c táº¡p báº­c hai cá»§a cÆ¡ cháº¿ tá»± chÃº Ã½ Ä‘a Ä‘áº§u, cÃ³ má»™t giá»›i háº¡n vá» Ä‘á»™ dÃ i token Ä‘áº§u vÃ o. Äá»ƒ huáº¥n luyá»‡n hiá»‡u quáº£ trÃªn cÃ¡c GPU tiÃªu chuáº©n nhÆ° V100, cÃ¡c mÃ´ hÃ¬nh mÃ£ tiá»n huáº¥n luyá»‡n hiá»‡n cÃ³, bao gá»“m GraphCodeBERT, CodeBERT, RoBERTa (code), láº¥y 256 token Ä‘áº§u tiÃªn theo máº·c Ä‘á»‹nh, Ä‘iá»u nÃ y khiáº¿n chÃºng khÃ´ng thá»ƒ biá»ƒu diá»…n thÃ´ng tin Ä‘áº§y Ä‘á»§ cá»§a mÃ£ dÃ i hÆ¡n 256 token. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» mÃ£ dÃ i, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t baseline má»›i SEA (Split, Encode and Aggregate), chia mÃ£ dÃ i thÃ nh cÃ¡c khá»‘i mÃ£, mÃ£ hÃ³a cÃ¡c khá»‘i nÃ y thÃ nh embeddings, vÃ  tá»•ng há»£p chÃºng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c biá»ƒu diá»…n mÃ£ dÃ i toÃ n diá»‡n. Vá»›i SEA, chÃºng ta cÃ³ thá»ƒ trá»±c tiáº¿p sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n dá»±a trÃªn Transformer Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a mÃ£ dÃ i mÃ  khÃ´ng thay Ä‘á»•i cáº¥u trÃºc bÃªn trong vÃ  tiá»n huáº¥n luyá»‡n láº¡i. ChÃºng tÃ´i cÅ©ng so sÃ¡nh SEA vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p Transformer thÆ°a thá»›t. Vá»›i GraphCodeBERT lÃ m encoder, SEA Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm mean reciprocal ranking tá»•ng thá»ƒ lÃ  0.785, cao hÆ¡n 10.1% so vá»›i GraphCodeBERT trÃªn benchmark CodeSearchNet, chá»©ng minh SEA lÃ  má»™t baseline máº¡nh cho tÃ¬m kiáº¿m mÃ£ dÃ i.
Tá»« khÃ³a: tÃ¬m kiáº¿m mÃ£, hiá»ƒu mÃ£ dÃ i, biá»ƒu diá»…n mÃ£

1. Giá»›i thiá»‡u
Má»™t ká»¹ thuáº­t tÃ¬m kiáº¿m mÃ£ tá»‘t giÃºp cÃ¡c nhÃ  phÃ¡t triá»ƒn
thÃºc Ä‘áº©y phÃ¡t triá»ƒn pháº§n má»m báº±ng cÃ¡ch tÃ¬m kiáº¿m
cÃ¡c Ä‘oáº¡n mÃ£ sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn. Nhá»¯ng tiáº¿n
bá»™ gáº§n Ä‘Ã¢y Ä‘Ã£ chá»©ng minh hiá»‡u quáº£
cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p tiá»n huáº¥n luyá»‡n mÃ£ dá»±a trÃªn Transformer,
bao gá»“m CodeBERT (Feng et al., 2020), CoCLR
(Huang et al., 2021), vÃ  GraphCodeBERT (Guo
et al., 2021), Ä‘Ã£ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
hiá»‡u suáº¥t tÃ¬m kiáº¿m mÃ£ thÃ´ng qua tiá»n huáº¥n luyá»‡n tá»± giÃ¡m sÃ¡t
trÃªn corpus mÃ£ quy mÃ´ lá»›n.

Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y gáº·p pháº£i má»™t háº¡n cháº¿
vá»‘n cÃ³. Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n vÃ  bá»™ nhá»›
cá»§a cÆ¡ cháº¿ tá»± chÃº Ã½ trong Transformer gá»‘c tÄƒng
theo báº­c hai vá»›i Ä‘á»™ dÃ i Ä‘áº§u vÃ o, táº¡o ra má»™t rÃ ng
buá»™c vá» Ä‘á»™ dÃ i Ä‘áº§u vÃ o khoáº£ng 512
token. Äá»ƒ huáº¥n luyá»‡n hiá»‡u quáº£ trÃªn cÃ¡c GPU tiÃªu chuáº©n nhÆ°
V100, GraphCodeBERT vÃ  CodeBERT chá»‰ xem xÃ©t
256 token Ä‘áº§u tiÃªn cá»§a cÃ¡c Ä‘oáº¡n mÃ£ vÃ  loáº¡i
bá» báº¥t ká»³ token nÃ o vÆ°á»£t quÃ¡ giá»›i háº¡n nÃ y. Tuy nhiÃªn, viá»‡c
háº¡n cháº¿ Ä‘á»™ dÃ i nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n váº¥n Ä‘á» vá» Ä‘á»™ chÃ­nh xÃ¡c, Ä‘áº·c
biá»‡t Ä‘á»‘i vá»›i cÃ¡c Ä‘oáº¡n mÃ£ dÃ i. VÃ­ dá»¥, khi
kiá»ƒm tra cÃ¡c trÆ°á»ng há»£p khÃ³ cá»§a GraphCode-
BERT, chÃºng tÃ´i tháº¥y ráº±ng GraphCodeBERT cÃ³ hiá»‡u
suáº¥t tháº¥p cho má»™t sá»‘ Ä‘oáº¡n mÃ£ dÃ i trong Ä‘Ã³
thÃ´ng tin quan trá»ng náº±m vá» phÃ­a cuá»‘i. NhÆ°
minh há»a trong HÃ¬nh 1, cÃ¡c tá»« khÃ³a "Tensor" vÃ 
"patches" xuáº¥t hiá»‡n sau Ä‘iá»ƒm cáº¯t 256 token Ä‘Æ°á»£c Ä‘áº·t bá»Ÿi
GraphCodeBERT, dáº«n Ä‘áº¿n viá»‡c chÃºng bá»‹ loáº¡i khá»i
xem xÃ©t. Do Ä‘Ã³, Ä‘oáº¡n mÃ£
tÆ°Æ¡ng á»©ng Ä‘Æ°á»£c xáº¿p háº¡ng á»Ÿ vá»‹ trÃ­ 21,148.

ChÃºng tÃ´i tiáº¿p tá»¥c tiáº¿n hÃ nh cÃ¡c nghiÃªn cá»©u thá»±c nghiá»‡m trÃªn
GraphCodeBERT trong bá»™ dá»¯ liá»‡u CodeSearch-
Net Ä‘Æ°á»£c sá»­ dá»¥ng cÃ´ng khai (Husain et al., 2019), vÃ  quan sÃ¡t
tháº¥y má»™t sá»± giáº£m dáº§n trong hiá»‡u suáº¥t tÃ¬m kiáº¿m khi Ä‘á»™
dÃ i cá»§a mÃ£ ground-truth trong truy váº¥n tÄƒng
lÃªn (tham kháº£o Báº£ng 1). Váº¥n Ä‘á» nÃ y tÆ°Æ¡ng tá»±
vá»›i váº¥n Ä‘á» vÄƒn báº£n dÃ i trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, mÃ  Ä‘Ã£ cÃ³ nhiá»u phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t,
bao gá»“m xá»­ lÃ½ phÃ¢n cáº¥p (Zhang
et al., 2019b), attention thÆ°a thá»›t (Child et al., 2019;
Beltagy et al., 2020), vÃ  tÃ¡i hiá»‡n á»Ÿ má»©c segment
(Dai et al., 2019). Tuy nhiÃªn, viá»‡c Ã¡p dá»¥ng trá»±c tiáº¿p cÃ¡c
phÆ°Æ¡ng phÃ¡p nÃ y cho mÃ£ dÃ i gáº·p hai thá»­ thÃ¡ch.
Thá»© nháº¥t, cÃ¡c ká»¹ thuáº­t nÃ y sá»­a Ä‘á»•i cáº¥u trÃºc bÃªn
trong cá»§a mÃ´ hÃ¬nh Transformer, cÃ³ thá»ƒ khiáº¿n
cÃ¡c tham sá»‘ tiá»n huáº¥n luyá»‡n hiá»‡n cÃ³ trá»Ÿ nÃªn khÃ´ng há»£p lá»‡. Thá»©
hai, mÃ£ dÃ i khÃ¡c vá»›i vÄƒn báº£n dÃ i á»Ÿ chá»— nÃ³ lÃ  má»™t
ngÃ´n ngá»¯ cÃ³ cáº¥u trÃºc cao. KhÃ´ng giá»‘ng nhÆ° má»™t tÃ i liá»‡u vÄƒn báº£n dÃ i
cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t tá»•ng thá»ƒ gáº¯n káº¿t vá»›i
ngá»¯ nghÄ©a hoÃ n chá»‰nh, ngá»¯ nghÄ©a cá»§a mÃ£ lÃ  khÃ´ng
liÃªn tá»¥c, vÃ  cÃ¡c hÃ m khÃ¡c nhau Ä‘Æ°á»£c phÃ¢n bá»‘
qua cÃ¡c vá»‹ trÃ­ khÃ¡c nhau. CÃ¡c thÃ­ nghiá»‡m so sÃ¡nh
Ä‘Æ°á»£c tiáº¿n hÃ nh trong Pháº§n 6.2 cung cáº¥p báº±ng chá»©ng arXiv:2208.11271v3 [cs.SE] 26 Mar 2024

--- TRANG 2 ---
Háº¡ng 1 (káº¿t quáº£ sai):
defpatch(self, *args, **kwargs):
return super(Deposit, self).patch(*args, **
kwargs)
Háº¡ng 21148 (ground truth):
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
img = Image.open(fpath)
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches .append(
PIL2array(patch))
return torch.Byte Tensor(
np.array( patches [:n]))256 tokensTraáº£ vá» má»™t tensor chá»©a cÃ¡c patches.
HÃ¬nh 1: VÃ­ dá»¥ trÆ°á»ng há»£p cá»§a GraphCodeBERT.
GraphCodeBERT cáº¯t bá» cÃ¡c token vÆ°á»£t quÃ¡ 256 to-
ken. CÃ¡c token quan trá»ng Ä‘Æ°á»£c lÃ m ná»•i báº­t báº±ng mÃ u vÃ ng.

á»§ng há»™ nhá»¯ng má»‘i quan tÃ¢m nÃ y.

Do Ä‘Ã³, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  chia mÃ£ dÃ i trong khi
báº£o tá»“n thÃ´ng tin ngá»¯ nghÄ©a cá»§a nÃ³. ChÃºng tÃ´i nháº±m
Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y mÃ  khÃ´ng thay Ä‘á»•i cáº¥u trÃºc bÃªn trong cá»§a
cÃ¡c mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n dá»±a trÃªn Transformer hoáº·c yÃªu cáº§u
tiá»n huáº¥n luyá»‡n láº¡i. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t SEA
(Split, Encode, and Aggregate) Ä‘á»ƒ xá»­ lÃ½ mÃ£ dÃ i
vÃ  cÃ³ Ä‘Æ°á»£c biá»ƒu diá»…n mÃ£ Ä‘Æ°á»£c cáº£i thiá»‡n.

NhÆ° mÃ´ táº£ trong HÃ¬nh 2, quÃ¡ trÃ¬nh bao gá»“m
chia mÃ£ dÃ i thÃ nh má»™t táº­p há»£p cÃ¡c Ä‘oáº¡n mÃ£,
tiáº¿p theo lÃ  sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t Ä‘á»ƒ
táº¡o ra má»™t táº­p há»£p khá»‘i mÃ£ chá»“ng láº¥p má»™t pháº§n.
CÃ¡c encoder mÃ£ hiá»‡n cÃ³ sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c
embeddings cho má»—i khá»‘i mÃ£. Cuá»‘i cÃ¹ng, cÃ¡c
embeddings nÃ y Ä‘Æ°á»£c tá»•ng há»£p Ä‘á»ƒ táº¡o ra biá»ƒu diá»…n
cho toÃ n bá»™ mÃ£ dÃ i. ThÃ´ng qua cÃ¡c thÃ­ nghiá»‡m rá»™ng rÃ£i,
chÃºng tÃ´i Ä‘Ã£ tháº¥y ráº±ng phÆ°Æ¡ng phÃ¡p chia dá»±a trÃªn AST
Ä‘Æ°á»£c Ä‘á» xuáº¥t vÃ  phÆ°Æ¡ng phÃ¡p tá»•ng há»£p dá»±a trÃªn attention vÆ°á»£t trá»™i so vá»›i cÃ¡c ká»¹ thuáº­t khÃ¡c cho viá»‡c chia
vÃ  tá»•ng há»£p. Do sá»‘ lÆ°á»£ng khá»‘i mÃ£ khÃ¡c nhau
Ä‘Æ°á»£c thu tháº­p tá»« cÃ¡c Ä‘oáº¡n mÃ£ khÃ¡c nhau, viá»‡c váº­n hÃ nh song song trá»Ÿ nÃªn thÃ¡ch thá»©c. Äá»ƒ
giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i Ä‘Ã£ thiáº¿t káº¿ má»™t module káº¿t há»£p-
chia Ä‘á»ƒ tÄƒng tá»‘c. Äiá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ  SEA lÃ  encoder-agnostic, cÃ³ nghÄ©a lÃ  nÃ³ cÃ³ thá»ƒ
Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i cÃ¡c encoder dá»±a trÃªn Transformer khÃ¡c nhau.
Khi so sÃ¡nh vá»›i cÃ¡c baseline encoder dá»±a trÃªn Transformer khÃ¡c nhau, SEA Ä‘áº¡t Ä‘Æ°á»£c má»™t cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong hiá»‡u suáº¥t mean reciprocal ranking (MRR), tá»« 7% Ä‘áº¿n 10%.

Báº£ng 1: Hiá»‡u suáº¥t tÃ¬m kiáº¿m mÃ£ (MRR) cá»§a
cÃ¡c Ä‘á»™ dÃ i token mÃ£ ground-truth khÃ¡c nhau. ChÃºng tÃ´i Ä‘áº·t
Ä‘á»™ dÃ i cáº¯t mÃ£ tá»« 50 Ä‘áº¿n 512. Káº¿t quáº£ cao
nháº¥t trong má»—i cá»™t Ä‘Æ°á»£c lÃ m ná»•i báº­t. Bá»™ dá»¯ liá»‡u:
CodeSearchNet python. MÃ´ hÃ¬nh: GraphCodeBERT.

Äá»™ dÃ i token | Äá»™ dÃ i cáº¯t mÃ£
             | 50    | 100   | 256   | 400   | 512
[0, 256)     | 0.6274| 0.6856| 0.6909| 0.6897| 0.6906
[256, 512)   | 0.6239| 0.7027| 0.7237| 0.7258| 0.7265
[512, 768)   | 0.6004| 0.6467| 0.7168| 0.7180| 0.7181
[768, 1024)  | 0.6038| 0.6315| 0.7111| 0.7375| 0.7276
[1024, 1943) | 0.6202| 0.6573| 0.6589| 0.6835| 0.6825

CÃ¡c Ä‘Ã³ng gÃ³p cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:
â€¢PhÃ¡t hiá»‡n vÃ  xÃ¡c minh thá»±c nghiá»‡m vá» khÃ³
khÄƒn trong viá»‡c mÃ´ hÃ¬nh hÃ³a mÃ£ dÃ i trong cÃ¡c
mÃ´ hÃ¬nh tÃ¬m kiáº¿m mÃ£ dá»±a trÃªn Transformer hiá»‡n cÃ³.
â€¢ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t baseline má»›i SEA vÃ  khÃ¡m phÃ¡
má»™t cÃ i Ä‘áº·t chia vÃ  tá»•ng há»£p tá»‘i Æ°u.
ChÃºng tÃ´i cÅ©ng thiáº¿t káº¿ má»™t module káº¿t há»£p-chia Ä‘á»ƒ
tÄƒng tá»‘c.
â€¢ThÃ´ng qua cÃ¡c thÃ­ nghiá»‡m rá»™ng rÃ£i, chÃºng tÃ´i chá»‰ ra
hiá»‡u quáº£ cá»§a SEA Ä‘Æ°á»£c Ä‘á» xuáº¥t vá»›i cÃ¡c
baseline encoder khÃ¡c nhau trong sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh, táº¡o ra má»™t baseline máº¡nh cho
tÃ¬m kiáº¿m mÃ£. MÃ£ nguá»“n vÃ  dá»¯ liá»‡u thÃ­ nghiá»‡m
cá»§a chÃºng tÃ´i cÃ³ sáºµn táº¡i: https://github.
com/fly-dragon211/SEA .

2. CÃ´ng trÃ¬nh liÃªn quan

2.1. PhÆ°Æ¡ng phÃ¡p TÃ¬m kiáº¿m MÃ£

CÃ¡c nghiÃªn cá»©u ban Ä‘áº§u (Nie et al., 2016; Yang and Huang,
2017; Rosario, 2000; Hill et al., 2011; Satter and
Sakib, 2016; Lv et al., 2015; Van Nguyen et al.,
2017) trong tÃ¬m kiáº¿m mÃ£ chá»§ yáº¿u Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t truy xuáº¥t thÃ´ng tin (IR) má»™t cÃ¡ch trá»±c tiáº¿p, coi tÃ¬m kiáº¿m mÃ£
nhÆ° má»™t nhiá»‡m vá»¥ khá»›p vÄƒn báº£n. Cáº£ truy váº¥n vÃ  Ä‘oáº¡n mÃ£
Ä‘á»u Ä‘Æ°á»£c coi lÃ  vÄƒn báº£n thuáº§n tÃºy, vÃ  cÃ¡c
thuáº­t toÃ¡n khá»›p vÄƒn báº£n truyá»n thá»‘ng nhÆ° bag-of-
words (BOW) (SchÃ¼tze et al., 2008), Jaccard (Jac-
card, 1901), term frequency-inverse document fre-
quency (TF-IDF) (Robertson and Jones, 1976),
BM25 (má»™t phiÃªn báº£n cáº£i tiáº¿n cá»§a TF-IDF) (Robertson
and Zaragoza, 2009), vÃ  mÃ´ hÃ¬nh boolean má»Ÿ rá»™ng (Lv et al., 2015) Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng. VÃ¬ Ä‘á»™ dÃ i mÃ£
cÃ³ tÃ¡c Ä‘á»™ng tá»‘i thiá»ƒu Ä‘áº¿n Ä‘á»™ phá»©c táº¡p mÃ´ hÃ¬nh,
cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ mÃ£ hÃ³a mÃ£ dÃ i mÃ  khÃ´ng
cáº¯t bá».

Sau khi giá»›i thiá»‡u mÃ´ hÃ¬nh tiá»n huáº¥n luyá»‡n quy mÃ´ lá»›n BERT (Devlin et al., 2019), Code-
BERT Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Feng et al. (2020). Code-
BERT lÃ  má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n trÃªn mÃ£ nguá»“n
vÃ  chÃº thÃ­ch khÃ´ng Ä‘Æ°á»£c gÃ¡n nhÃ£n, Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t áº¥n tÆ°á»£ng

--- TRANG 3 ---
trong tÃ¬m kiáº¿m mÃ£ dá»±a trÃªn vÄƒn báº£n thÃ´ng qua
fine-tuning trÃªn cÃ¡c bá»™ dá»¯ liá»‡u vÄƒn báº£n-mÃ£ Ä‘Æ°á»£c ghÃ©p Ä‘Ã´i. Huang
et al. (2021) giá»›i thiá»‡u CoCLR, má»™t phÆ°Æ¡ng phÃ¡p há»c
tÆ°Æ¡ng pháº£n tÄƒng cÆ°á»ng khá»›p truy váº¥n-mÃ£.
Sun et al. (2022) phÃ¡t triá»ƒn má»™t ká»¹ thuáº­t dá»‹ch mÃ£
nháº­n biáº¿t ngá»¯ cáº£nh dá»‹ch cÃ¡c Ä‘oáº¡n mÃ£
thÃ nh mÃ´ táº£ ngÃ´n ngá»¯ tá»± nhiÃªn. Gu et al. (2022)
sá»­ dá»¥ng deep hashing vÃ  phÃ¢n loáº¡i mÃ£ Ä‘á»ƒ
tÄƒng tá»‘c tÃ¬m kiáº¿m mÃ£, trong khi Chai et al. (2022)
thÃ­ch á»©ng few-shot meta-learning vá»›i tÃ¬m kiáº¿m mÃ£.
Guo et al. (2021) Ä‘á» xuáº¥t GraphCodeBERT, káº¿t
há»£p cÃ¡c nhiá»‡m vá»¥ tiá»n huáº¥n luyá»‡n nháº­n biáº¿t cáº¥u trÃºc Ä‘á»ƒ
cáº£i thiá»‡n hiá»ƒu biáº¿t vÃ  hiá»‡u suáº¥t mÃ£. Gáº§n
Ä‘Ã¢y, Hu et al. (2023) sá»­ dá»¥ng má»™t framework tÃ¬m kiáº¿m
mÃ£ fusion hai giai Ä‘oáº¡n káº¿t há»£p bi-encoders
vÃ  cross-encoders Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»‡u suáº¥t. Tuy
nhiÃªn, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a Transformers
vÃ  bá»™ nhá»› GPU háº¡n cháº¿ thÆ°á»ng dáº«n Ä‘áº¿n viá»‡c cáº¯t bá»
cÃ¡c Ä‘oáº¡n mÃ£ dÃ i.

2.2. Biá»ƒu diá»…n MÃ£ Neural vá»›i
Cáº¥u trÃºc MÃ£

Gáº§n Ä‘Ã¢y, Ä‘Ã£ cÃ³ nhá»¯ng tiáº¿n bá»™ Ä‘Ã¡ng chÃº Ã½
trong cÃ¡c phÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n mÃ£ neural táº­n dá»¥ng
cáº¥u trÃºc mÃ£, Ä‘áº·c biá»‡t lÃ  Abstract Syntax
Trees (AST), mang láº¡i hiá»‡u suáº¥t áº¥n tÆ°á»£ng (Alon
et al., 2020; Sun et al., 2020; Bui et al., 2021;
Kim et al., 2021; Peng et al., 2021; Hellendoorn
et al., 2019; Allamanis et al., 2021; Georgiev et al.,
2022; Ma et al., 2023; Du and Yu, 2023). MMAN
(Wan et al., 2019) káº¿t há»£p má»™t lá»›p fusion attention
Ä‘a modal Ä‘á»ƒ káº¿t há»£p biá»ƒu diá»…n AST vÃ  Control Flow
Graph (CFG). ASTNN (Zhang
et al., 2019a) vÃ  CAST (Shi et al., 2021) phÃ¢n
Ä‘oáº¡n AST lá»›n thÃ nh cÃ¡c chuá»—i cá»§a cÃ¡c cÃ¢y cÃ¢u lá»‡nh
nhá» hÆ¡n, mÃ£ hÃ³a chÃºng thÃ nh vectors báº±ng cÃ¡ch náº¯m báº¯t
thÃ´ng tin tá»« vá»±ng vÃ  cÃº phÃ¡p cá»§a má»—i
cÃ¢u lá»‡nh. TBCAA (Chen et al., 2019) sá»­ dá»¥ng má»™t
máº¡ng convolutional dá»±a trÃªn cÃ¢y trÃªn cÃ¡c
AST Ä‘Æ°á»£c tÄƒng cÆ°á»ng API. UniXcoder (Guo et al., 2022) táº­n dá»¥ng cáº£
AST vÃ  chÃº thÃ­ch mÃ£ Ä‘á»ƒ lÃ m phong phÃº biá»ƒu diá»…n mÃ£. GraphCodeBERT (Guo et al., 2021) káº¿t
há»£p cÃ¡c quan há»‡ biáº¿n Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« AST trong
cÃ¡c nhiá»‡m vá»¥ tiá»n huáº¥n luyá»‡n cá»§a nÃ³. Trong cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i, chÃºng tÃ´i Ä‘áº·c biá»‡t
nháº±m náº¯m báº¯t vÃ  mÃ´ hÃ¬nh hÃ³a thÃ´ng tin cáº¥u trÃºc
cÃ³ máº·t trong cÃ¡c Ä‘oáº¡n mÃ£ dÃ i.

2.3. Transformer cho VÄƒn báº£n DÃ i

á»¨ng dá»¥ng cá»§a cÃ¡c mÃ´ hÃ¬nh Transformer cho vÄƒn báº£n dÃ i
cÃ³ thá»ƒ Ä‘Æ°á»£c chia rá»™ng thÃ nh hai loáº¡i: má»Ÿ rá»™ng
attention vÃ  tÄƒng cÆ°á»ng mÃ´ hÃ¬nh Trans-
former gá»‘c, vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p tá»•ng há»£p. Loáº¡i Ä‘áº§u tiÃªn
bao gá»“m bá»‘n phÆ°Æ¡ng phÃ¡p chÃ­nh: attention
thÆ°a thá»›t (Child et al., 2019; Correia et al., 2019;
Beltagy et al., 2020; Kitaev et al., 2019; Roy et al.,
2021; Ainslie et al., 2020; Jiang et al., 2020; GÃ¼n-
ther et al., 2023), tÃ¡i hiá»‡n (Dai et al., 2019), cÆ¡ cháº¿ phÃ¢n cáº¥p (Zhang et al., 2019b; Gao
and Callan, 2022), vÃ  attention nÃ©n (Ye
et al., 2019; Guo et al., 2019). Attention thÆ°a thá»›t
háº¡n cháº¿ má»—i token chá»‰ chÃº Ã½ Ä‘áº¿n má»™t táº­p con
cÃ¡c token khÃ¡c. TÃ¡i hiá»‡n tÃ­ch há»£p cÃ¡c pháº§n tá»­ máº¡ng
neural tÃ¡i hiá»‡n vÃ o cÃ¡c mÃ´ hÃ¬nh Transformer Ä‘á»ƒ má»Ÿ
rá»™ng span attention cá»§a chÃºng. CÃ¡c cÆ¡ cháº¿ phÃ¢n cáº¥p
mÃ´ hÃ¬nh input vÄƒn báº£n dÃ i má»™t cÃ¡ch phÃ¢n cáº¥p, tá»« cÃ¢u
Ä‘áº¿n Ä‘oáº¡n vÄƒn. Attention nÃ©n cÃ³ chá»n lá»c
nÃ©n cÃ¡c pháº§n cá»¥ thá»ƒ cá»§a input.

Báº£ng 2: Thá»‘ng kÃª Ä‘á»™ dÃ i token mÃ£ cá»§a bá»™ Ä‘Ã¡nh giÃ¡ Code-
SearchNet.

Äá»™ dÃ i | Ruby | JS | Go | Py | Java | Php | Tá»•ng thá»ƒ
[0, 256) | 16% | 10% | 22% | 14% | 13% | 13% | 14%
[256, 512) | 44% | 29% | 38% | 30% | 27% | 26% | 32%
[512, + âˆ) | 41% | 62% | 40% | 56% | 60% | 61% | 54%

Loáº¡i thá»© hai, cÃ¡c phÆ°Æ¡ng phÃ¡p tá»•ng há»£p, bao
gá»“m tá»•ng há»£p nhiá»u Ä‘iá»ƒm passage hoáº·c biá»ƒu
diá»…n cho má»™t tÃ i liá»‡u dÃ i. VÃ­ dá»¥,
Wang et al. (2019) Ä‘á» xuáº¥t má»™t mÃ´ hÃ¬nh BERT
Ä‘a passage Ä‘á»ƒ chuáº©n hÃ³a toÃ n cá»¥c cÃ¡c Ä‘iá»ƒm tráº£ lá»i qua
táº¥t cáº£ cÃ¡c passage trong nhiá»‡m vá»¥ há»i Ä‘Ã¡p. Trong
ngá»¯ cáº£nh xáº¿p háº¡ng tÃ i liá»‡u, SMITH (Yang et al.,
2020) há»c biá»ƒu diá»…n tÃ i liá»‡u thÃ´ng qua
tá»•ng há»£p biá»ƒu diá»…n cÃ¢u phÃ¢n cáº¥p.
PARADE (Li et al., 2020) sá»­ dá»¥ng Max, CNN, At-
tention, vÃ  Transformer Ä‘á»ƒ tá»•ng há»£p cÃ¡c
biá»ƒu diá»…n passage. Tsujimura et al. (2023) sá»­ dá»¥ng
phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t Ä‘á»ƒ quáº£n lÃ½ cÃ¡c chuá»—i input dÃ i
trong ngá»¯ cáº£nh cá»§a cÃ¡c nhiá»‡m vá»¥ Named Entity
Recognition y táº¿.

Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ khÃ´ng hoÃ n toÃ n phÃ¹
há»£p cho mÃ£ cÃ³ cáº¥u trÃºc cao. Trong cÃ¡c chÆ°Æ¡ng trÃ¬nh
Ä‘Æ°á»£c thiáº¿t káº¿ tá»‘t, mÃ£ trong cÃ¹ng má»™t module, nhÆ°
má»™t hÃ m, Ä‘Æ°á»£c káº¿t ná»‘i cháº·t cháº½, trong khi cÃ¡c tÆ°Æ¡ng tÃ¡c
giá»¯a cÃ¡c module khÃ¡c nhau Ä‘Æ°á»£c káº¿t ná»‘i lá»ng láº»o, tuÃ¢n thá»§
nguyÃªn táº¯c gáº¯n káº¿t cao vÃ 
káº¿t ná»‘i tháº¥p. NgÆ°á»£c láº¡i, vÄƒn báº£n dÃ i trong ngÃ´n ngá»¯ tá»±
nhiÃªn cÃ³ xu hÆ°á»›ng thá»ƒ hiá»‡n tÃ­nh máº¡ch láº¡c. Trong bÃ i bÃ¡o nÃ y,
chÃºng tÃ´i Ä‘iá»u tra kháº£ nÄƒng Ã¡p dá»¥ng cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p vÄƒn báº£n dÃ i
trong lÄ©nh vá»±c tÃ¬m kiáº¿m mÃ£ vÃ  Ä‘á» xuáº¥t má»™t
baseline má»›i SEA cho tÃ¬m kiáº¿m mÃ£ dÃ i.

3. Äá»™ng lá»±c: Váº¥n Ä‘á» MÃ£ DÃ i

3.1. Kiáº¿n thá»©c CÆ¡ báº£n

TÃ¬m kiáº¿m mÃ£ nháº±m tÃ¬m Ä‘oáº¡n mÃ£ C phÃ¹ há»£p nháº¥t
tá»« má»™t codebase Ä‘Ã£ cho khá»›p vá»›i má»™t
truy váº¥n Q. Äá»‘i vá»›i má»™t mÃ´ hÃ¬nh deep-learning hiá»‡n táº¡i, chÃºng ta
Ä‘áº§u tiÃªn chuyá»ƒn Ä‘á»•i truy váº¥n Q vÃ  cÃ¡c Ä‘oáº¡n mÃ£ C thÃ nh
cÃ¡c token truy váº¥n vÃ  mÃ£ vá»›i tokenizer nhÆ°
BPE (Sennrich et al., 2016). Sau Ä‘Ã³ chÃºng ta chuyá»ƒn Ä‘á»•i
cÃ¡c id token cá»§a truy váº¥n Q vÃ  cÃ¡c Ä‘oáº¡n mÃ£
C thÃ nh biá»ƒu diá»…n vector eq vÃ  ec báº±ng
cÃ¡c encoder máº¡ng neural, vÃ  tÃ­nh toÃ¡n cÃ¡c phÃ©p Ä‘o Ä‘á»™ tÆ°Æ¡ng tá»± (hoáº·c

--- TRANG 4 ---
khoáº£ng cÃ¡ch) trong khÃ´ng gian Euclidean nhÆ°
Ä‘á»™ tÆ°Æ¡ng tá»± Cosine hoáº·c khoáº£ng cÃ¡ch Euclidean Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c
Ä‘iá»ƒm tÆ°Æ¡ng tá»± cross-modal s. PhÃ©p tÃ­nh
cÃ³ thá»ƒ Ä‘Æ°á»£c hÃ¬nh thá»©c hÃ³a nhÆ° sau:

eq = Î“(tokenizer(Q))
ec = Î“â€²(tokenizer(C)), C âˆˆ Codebase
s = sim(eq, ec)                    (1)

trong Ä‘Ã³ Î“ vÃ  Î“â€² lÃ  hai encoder máº¡ng neural
Ä‘Æ°á»£c huáº¥n luyá»‡n tá»‘t há»c tá»« dá»¯ liá»‡u Ä‘Æ°á»£c ghÃ©p Ä‘Ã´i cÃ³ nhÃ£n.

3.2. Váº¥n Ä‘á» MÃ£ DÃ i

Äá»ƒ kiá»ƒm soÃ¡t chi phÃ­ bá»™ nhá»› vÃ  tÃ­nh toÃ¡n trong
giai Ä‘oáº¡n huáº¥n luyá»‡n, viá»‡c cáº¯t bá» mÃ£ dÃ i lÃ  thÃ´ng lá»‡
thÃ´ng thÆ°á»ng. VÃ­ dá»¥, GraphCodeBERT thÆ°á»ng
láº¥y 256 token mÃ£ Ä‘áº§u tiÃªn theo máº·c Ä‘á»‹nh. Äá»ƒ Ä‘iá»u
tra liá»‡u phÆ°Æ¡ng phÃ¡p cáº¯t bá» nÃ y cÃ³ dáº«n Ä‘áº¿n
máº¥t thÃ´ng tin hay khÃ´ng, chÃºng tÃ´i Ä‘Ã£ tiáº¿n hÃ nh thá»‘ng kÃª Ä‘á»™ dÃ i token
trÃªn CodeSearchNet. NhÆ° thá»ƒ hiá»‡n trong Báº£ng 2, chÃºng tÃ´i
tháº¥y ráº±ng cÃ¡c Ä‘oáº¡n vá»›i Ä‘á»™ dÃ i token nhá» hÆ¡n
256 chá»‰ chiáº¿m 14.1%, trong khi 53.5% Ä‘oáº¡n mÃ£
vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i mÃ£ hÃ³a tá»‘i Ä‘a
512 token cho Transformers. Äiá»u nÃ y cho tháº¥y ráº±ng
viá»‡c cáº¯t bá» dáº«n Ä‘áº¿n máº¥t thÃ´ng tin cho cÃ¡c Ä‘oáº¡n
cÃ³ Ä‘á»™ dÃ i token lá»›n hÆ¡n 256.

Äá»ƒ kiá»ƒm tra sá»± khÃ¡c biá»‡t hiá»‡u suáº¥t tÃ¬m kiáº¿m cá»§a
GraphCodeBERT trÃªn cÃ¡c táº­p con truy váº¥n vá»›i
Ä‘á»™ dÃ i mÃ£ ground truth (GT) khÃ¡c nhau, chÃºng tÃ´i chia
táº­p con test python cá»§a CodeSearchNet (CSN) thÃ nh 5
táº­p truy váº¥n riÃªng biá»‡t dá»±a trÃªn Ä‘á»™ dÃ i token mÃ£ GT
khÃ¡c nhau. ChÃºng tÃ´i tÃ­nh toÃ¡n Mean Reciprocal Rank
(MRR) cá»§a GraphCodeBERT cho cÃ¡c Ä‘á»™ dÃ i cáº¯t mÃ£
khÃ¡c nhau, nhÆ° thá»ƒ hiá»‡n trong Báº£ng 1. ÄÃ¡ng chÃº Ã½, chÃºng tÃ´i quan
sÃ¡t tháº¥y má»™t xu hÆ°á»›ng giáº£m trong hiá»‡u suáº¥t tÃ¬m kiáº¿m khi
Ä‘á»™ dÃ i token mÃ£ ground-truth tÄƒng (tá»«
trÃªn xuá»‘ng dÆ°á»›i) cho Ä‘á»™ dÃ i token mÃ£ vÆ°á»£t quÃ¡
256 token, cho tháº¥y ráº±ng cÃ¡c Ä‘oáº¡n mÃ£ dÃ i gÃ¢y ra
thÃ¡ch thá»©c cho GraphCodeBERT. HÆ¡n ná»¯a, khi Ä‘á»™
dÃ i cáº¯t mÃ£ kÃ©o dÃ i tá»« trÃ¡i sang pháº£i,
chÃºng tÃ´i quan sÃ¡t tháº¥y hiá»‡u suáº¥t tÃ¬m kiáº¿m tÆ°Æ¡ng Ä‘á»‘i nháº¥t quÃ¡n
khi Ä‘á»™ dÃ i cáº¯t vÆ°á»£t quÃ¡
Ä‘á»™ dÃ i token. VÃ  xuáº¥t hiá»‡n má»™t xu hÆ°á»›ng tÄƒng
trong hiá»‡u suáº¥t tÃ¬m kiáº¿m cho cÃ¡c Ä‘oáº¡n mÃ£ vá»›i
Ä‘á»™ dÃ i token vÆ°á»£t quÃ¡ Ä‘á»™ dÃ i cáº¯t.
Äiá»u nÃ y gá»£i Ã½ ráº±ng viá»‡c Ä‘Æ¡n giáº£n cáº¯t bá» mÃ£ dÃ i cÃ³ thá»ƒ
dáº«n Ä‘áº¿n máº¥t thÃ´ng tin cÃ³ giÃ¡ trá»‹.

4. SEA

Trong pháº§n nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y má»™t tá»•ng quan toÃ n diá»‡n
vá» SEA, bao gá»“m kiáº¿n trÃºc mÃ´ hÃ¬nh, cÃ¡c phÆ°Æ¡ng phÃ¡p chia, ká»¹ thuáº­t tá»•ng há»£p,
vÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tÄƒng tá»‘c
suy luáº­n.

4.1. Kiáº¿n trÃºc MÃ´ hÃ¬nh

ChÃºng tÃ´i giá»›i thiá»‡u SEA cá»§a chÃºng tÃ´i trong pháº§n nÃ y. Pipeline
tá»•ng thá»ƒ Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2. Cho má»™t Ä‘oáº¡n mÃ£
C, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  táº¡o ra má»™t biá»ƒu diá»…n mÃ£
ec. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng má»™t
phÆ°Æ¡ng phÃ¡p Ä‘a bÆ°á»›c. Äáº§u tiÃªn chÃºng tÃ´i chia Ä‘oáº¡n mÃ£ thÃ nh
má»™t táº­p há»£p cÃ¡c Ä‘oáº¡n mÃ£:

P = Split(C) = {p1, p2, ..., pn}.     (2)

Sau Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c
má»™t táº­p há»£p khá»‘i mÃ£ chá»“ng láº¥p má»™t pháº§n:

B = SlidingWindow(P) = {b1, b2, ..., bk}.     (3)

Giáº£ sá»­ kÃ­ch thÆ°á»›c cá»­a sá»• lÃ  w vÃ  bÆ°á»›c lÃ  s,
thÃ¬ sá»‘ khá»‘i mÃ£ lÃ  k = âŒŠ(n-w)/s + 1âŒ‹,
trong Ä‘Ã³ âŒŠÂ·âŒ‹ Ä‘á» cáº­p Ä‘áº¿n lÃ m trÃ²n xuá»‘ng. Tiáº¿p theo, chÃºng tÃ´i sá»­ dá»¥ng má»™t
encoder mÃ£, nhÆ° GraphCodeBERT, Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c
embeddings cho má»—i trong k khá»‘i mÃ£:

eB = {eb1, eb2, ..., ebk}.     (4)

Cuá»‘i cÃ¹ng, má»™t phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ káº¿t
há»£p k embeddings thÃ nh biá»ƒu diá»…n mÃ£
ec:

ec = Aggregation(eB)     (5)

4.2. PhÆ°Æ¡ng phÃ¡p Chia

Äá»ƒ cÃ³ Ä‘Æ°á»£c táº­p há»£p Ä‘oáº¡n mÃ£, chÃºng tÃ´i khÃ¡m phÃ¡ bá»‘n
phÆ°Æ¡ng phÃ¡p chia, cá»¥ thá»ƒ lÃ  chia dá»±a trÃªn khoáº£ng tráº¯ng,
chia dá»±a trÃªn token, chia dá»±a trÃªn dÃ²ng, vÃ  chia
dá»±a trÃªn AST. Chia dá»±a trÃªn khoáº£ng tráº¯ng Ä‘Æ¡n giáº£n lÃ 
chia theo khoáº£ng tráº¯ng, dáº«n Ä‘áº¿n viá»‡c chia má»™t chuá»—i
nhÆ° "def read_image_file" Ä‘Æ°á»£c chia thÃ nh {'def',
'read_image_file'}. TÆ°Æ¡ng tá»±, chia dá»±a trÃªn token
vÃ  chia dá»±a trÃªn dÃ²ng bao gá»“m viá»‡c chia dá»±a trÃªn
token vÃ  dÃ²ng, tÆ°Æ¡ng á»©ng.

Má»™t Abstract Syntax Tree (AST) lÃ  má»™t biá»ƒu diá»…n cÃ¢y
cá»§a cáº¥u trÃºc cÃº phÃ¡p cá»§a mÃ£ nguá»“n
Ä‘Æ°á»£c viáº¿t báº±ng má»™t ngÃ´n ngá»¯ láº­p trÃ¬nh. Má»—i nÃºt trong
AST tÆ°Æ¡ng á»©ng vá»›i má»™t cáº¥u trÃºc cá»¥ thá»ƒ trong
mÃ£, nhÆ° biá»ƒu thá»©c, cÃ¢u lá»‡nh, hoáº·c khai
bÃ¡o. Cáº¥u trÃºc phÃ¢n cáº¥p cá»§a AST pháº£n Ã¡nh
cÃº phÃ¡p cá»§a cÃ¡c ngÃ´n ngá»¯ láº­p trÃ¬nh, trá»«u tÆ°á»£ng hÃ³a
má»™t sá»‘ chi tiáº¿t cÃº phÃ¡p Ä‘á»ƒ táº­p trung vÃ o cáº¥u trÃºc
cá»‘t lÃµi.

Äá»‘i vá»›i viá»‡c chia dá»±a trÃªn AST, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  nghÄ© ra má»™t
phÆ°Æ¡ng phÃ¡p vá»«a Ä‘Æ¡n giáº£n vá»«a cÃ³ thá»ƒ Ã¡p
dá»¥ng cho nhiá»u ngÃ´n ngá»¯ láº­p trÃ¬nh. Láº¥y cáº£m há»©ng
tá»« CAST (Shi et al., 2021), chÃºng tÃ´i phÃ¢n tÃ­ch má»™t mÃ£ nguá»“n
thÃ nh má»™t Abstract Syntax Tree vá»›i tree_sitterÂ¹,
vÃ  duyá»‡t AST nÃ y báº±ng preorder traversal. Trong
trÆ°á»ng há»£p cá»§a cÃ¡c cáº¥u trÃºc tá»•ng há»£p (tá»©c lÃ  for, if, def, v.v.),
nhÆ° mÃ´ táº£ trong HÃ¬nh 2(a), chÃºng tÃ´i Ä‘á»‹nh nghÄ©a táº­p há»£p cÃ¡c
nÃºt AST {head_block, body}, trong Ä‘Ã³ head_block

Â¹https://github.com/tree-sitter/
py-tree-sitter

--- TRANG 5 ---
def 
â€¦def 
PIL2array
â€¦ return 
np.array
â€¦for 
fpath in
â€¦img = 
Image. 
â€¦...def 
â€¦def 
PIL2array
â€¦ return 
np.array
â€¦for 
fpath in
â€¦img = 
Image. 
â€¦...
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches.append(
PIL2array(patch))
return torch.ByteTensor(
np.array(patches[:n]))Äá»‹nh nghÄ©a
hÃ mtÃªn read_image_file
bodytham sá»‘
tÃªn
body
CÃ¢u lá»‡nh Return
Äá»‹nh nghÄ©a
hÃ m
â€¦
IN
 CÃ¢u lá»‡nh
For
â€¦
â€¦
â€¦â€¦
body
 â€¦
CÃ¢u lá»‡nh Return
GraphCodeBERT
GraphCodeBERT
GraphCodeBERT ...Module
Fusion
Cá»­a sá»• trÆ°á»£t
 Encoder mÃ£
 Biá»ƒu diá»…n mÃ£(a) Chia mÃ£ dá»±a trÃªn AST.
def 
â€¦def 
PIL2array
â€¦ return 
np.array
â€¦for 
fpath in
â€¦img = 
Image. 
â€¦...def 
â€¦def 
PIL2array
â€¦ return 
np.array
â€¦for 
fpath in
â€¦img = 
Image. 
â€¦...
defread_image_file(data_dir, image_ext, n):
defPIL2array(img):
return np.array(
img.getdata(), dtype=np.uint8
).reshape(64, 64)
...
forfpathinlist_files:
foryinrange(0, 1024, 64):
forxinrange(0, 1024, 64):
patch = img.crop(
(x, y, x + 64, y + 64))
patches.append(
PIL2array(patch))
return torch.ByteTensor(
np.array(patches[:n]))Äá»‹nh nghÄ©a
hÃ mtÃªn read_image_file
bodytham sá»‘
tÃªn
body
CÃ¢u lá»‡nh Return
Äá»‹nh nghÄ©a
hÃ m
â€¦
IN
 CÃ¢u lá»‡nh
For
â€¦
â€¦
â€¦â€¦
body
 â€¦
CÃ¢u lá»‡nh Return
GraphCodeBERT
GraphCodeBERT
GraphCodeBERT ...Module
Aggregation
Cá»­a sá»• trÆ°á»£t
 Encoder mÃ£
 Biá»ƒu diá»…n mÃ£
(b) Cá»­a sá»• trÆ°á»£t vÃ  tá»•ng há»£p.

HÃ¬nh 2: Pipeline cá»§a kiáº¿n trÃºc SEA (split, encode and aggregate) Ä‘Æ°á»£c Ä‘á» xuáº¥t.

chá»‹u trÃ¡ch nhiá»‡m chia header vÃ  body
cá»§a cÃ¡c cÃ¢u lá»‡nh lá»“ng nhau nhÆ° if vÃ  While state-
ments, trong khi body tÆ°Æ¡ng á»©ng vá»›i cÃ¡c khai bÃ¡o phÆ°Æ¡ng thá»©c. Khi gáº·p má»™t cáº¥u trÃºc
tá»•ng há»£p, chÃºng tÃ´i chÃ¨n má»™t dáº¥u chia trÆ°á»›c vÃ  sau
head_block, hiá»‡u quáº£ chia má»™t AST lá»›n thÃ nh
má»™t chuá»—i cÃ¡c subtree khÃ´ng chá»“ng láº¥p. Tiáº¿p
theo, dá»±a trÃªn viá»‡c chia AST, chÃºng tÃ´i xÃ¢y dá»±ng
táº­p há»£p Ä‘oáº¡n mÃ£ P báº±ng cÃ¡ch chia mÃ£ gá»‘c
tÆ°Æ¡ng á»©ng.

4.3. PhÆ°Æ¡ng phÃ¡p Tá»•ng há»£p

Meanpooling / Maxpooling. Má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n
Ä‘á»ƒ tá»•ng há»£p cÃ¡c embeddings cá»§a k khá»‘i mÃ£
lÃ  tÃ­nh trung bÃ¬nh hoáº·c giÃ¡ trá»‹ tá»‘i Ä‘a cá»§a
embeddings cá»§a chÃºng:

ec = Mean/Max({eb1, eb2, ..., ebk}).     (6)

Tuy nhiÃªn, má»™t háº¡n cháº¿ cá»§a meanpooling lÃ  má»—i
khá»‘i mÃ£ Ä‘Ã³ng gÃ³p nhÆ° nhau vÃ o biá»ƒu diá»…n
cuá»‘i cÃ¹ng, báº¥t ká»ƒ cháº¥t lÆ°á»£ng cÃ¡ nhÃ¢n cá»§a chÃºng. TÆ°Æ¡ng
tá»±, maxpooling ná»•i báº­t khá»‘i
cÃ³ giÃ¡ trá»‹ cao nháº¥t. Äá»ƒ giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ nÃ y
vÃ  tÄƒng cÆ°á»ng quÃ¡ trÃ¬nh tá»•ng há»£p, chÃºng tÃ´i Ä‘á» xuáº¥t
viá»‡c káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p embedding cÃ³ trá»ng sá»‘.

Tá»•ng há»£p dá»±a trÃªn Attention. Nháº­n ra
ráº±ng khÃ´ng pháº£i táº¥t cáº£ cÃ¡c khá»‘i mÃ£ Ä‘á»u cÃ³ táº§m quan trá»ng báº±ng nhau trong
viá»‡c biá»ƒu diá»…n cÃ¡c Ä‘oáº¡n mÃ£ dÃ i, chÃºng tÃ´i giá»›i thiá»‡u cÃ¡c trá»ng sá»‘
tá»± thÃ­ch á»©ng Î± cho má»—i embedding khá»‘i trong

Encoder
Linear
Softmaxâ€¦Encoder
1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
ğ‘˜Ã—1
ğ‘˜Ã—1
1Ã—ğ‘‘1Ã—ğ‘‘
1Ã—ğ‘‘ğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
Mean/MaxConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯
Linear
SoftmaxMean/Max tanh
Linear1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
1Ã—ğ‘‘Mean/Max1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
ğ‘˜Ã—128
ğ‘˜Ã—1
1Ã—ğ‘‘1Ã—ğ‘‘ğ‘˜Ã—128
ğ‘˜Ã—1Encoderâ€¦Encoderğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
ConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯
Encoderâ€¦Encoderğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
ConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯(a) Attention má»™t lá»›p vá»›i
mean / max.

Encoder
Linear
Softmaxâ€¦Encoder
1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
ğ‘˜Ã—1
ğ‘˜Ã—1
1Ã—ğ‘‘1Ã—ğ‘‘
1Ã—ğ‘‘ğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
Mean/MaxConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯
Linear
SoftmaxMean/Max tanh
Linear1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
1Ã—ğ‘‘Mean/Max1Ã—ğ‘‘ 1Ã—ğ‘‘
ğ‘˜Ã—ğ‘‘
ğ‘˜Ã—128
ğ‘˜Ã—1
1Ã—ğ‘‘1Ã—ğ‘‘ğ‘˜Ã—128
ğ‘˜Ã—1Encoderâ€¦Encoderğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
ConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯
Encoderâ€¦Encoderğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¬µ
ConCatğ¶ğ‘œğ‘‘ğ‘’ ğµğ‘™ğ‘œğ‘ğ‘˜à¯(b) Attention hai lá»›p vá»›i
mean / max.

HÃ¬nh 3: CÃ¡c phÆ°Æ¡ng phÃ¡p tá»•ng há»£p dá»±a trÃªn attention.

quÃ¡ trÃ¬nh tá»•ng há»£p:

ec = Î£(i=1 to k) Î±i ebi.     (7)

Láº¥y cáº£m há»©ng tá»« Multi-Instance Learning dá»±a trÃªn attention
(Li et al., 2021) vÃ  Lightweight Attentional Feature
Fusion (Hu et al., 2022), chÃºng tÃ´i tÃ­nh toÃ¡n cÃ¡c trá»ng sá»‘
{Î±1, ..., Î±k} nhÆ° sau:

{a1, ..., ak} = softmax(Linear({eb1, ..., ebk})).     (8)

--- TRANG 6 ---
Báº£ng 3: PhÃ¢n tÃ­ch chi phÃ­ tÃ­nh toÃ¡n. n lÃ  Ä‘á»™ dÃ i chuá»—i, d lÃ  chiá»u biá»ƒu diá»…n, k lÃ  sá»‘ khá»‘i mÃ£, l lÃ  sá»‘ lá»›p. LÆ°u Ã½ ráº±ng chÃºng tÃ´i sá»­ dá»¥ng attention má»™t lá»›p cho SEA.

PhÆ°Æ¡ng phÃ¡p | Tham sá»‘ | Äá»™ phá»©c táº¡p
GraphCodeBERT | 5dÂ²Â·l | O(nÂ²Â·dÂ·l)
SEA | 5dÂ²Â·l+d | O(nÂ²/kÂ·dÂ·l)

Äá»‘i vá»›i attention má»™t lá»›p, Linear Ä‘á» cáº­p Ä‘áº¿n má»™t lá»›p
káº¿t ná»‘i Ä‘áº§y Ä‘á»§ chuyá»ƒn Ä‘á»•i chiá»u thÃ nh 1.
Äá»‘i vá»›i attention hai lá»›p, Linear Ä‘á» cáº­p Ä‘áº¿n hai lá»›p
káº¿t ná»‘i Ä‘áº§y Ä‘á»§ Ä‘áº§u tiÃªn chuyá»ƒn Ä‘á»•i chiá»u
thÃ nh 128 vÃ  sau Ä‘Ã³ chuyá»ƒn Ä‘á»•i chiá»u thÃ nh 1. HÆ¡n
ná»¯a, nhÆ° minh há»a trong HÃ¬nh 3(a) vÃ  HÃ¬nh
3(b), chÃºng tÃ´i khÃ¡m phÃ¡ sá»± káº¿t há»£p cá»§a attention vá»›i
cÃ¡c phÆ°Æ¡ng phÃ¡p meanpooling / maxpooling:

ec = Î£(i=1 to k)(Î±i ebi) + Mean/Max({eb1, eb2, ..., ebk}).     (9)

Äá»ƒ phÃ¢n tÃ­ch chi phÃ­ tÃ­nh toÃ¡n, SEA sá»­ dá»¥ng
phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t Ä‘á»ƒ giáº£m Ä‘Ã¡ng ká»ƒ Ä‘á»™ phá»©c táº¡p
xuá»‘ng 1/k. Äá»™ phá»©c táº¡p gá»‘c cá»§a Graph-
CodeBERT Ä‘Æ°á»£c cho bá»Ÿi O(nÂ²Â·dÂ·l), trong Ä‘Ã³ n, d, l
biá»ƒu diá»…n Ä‘á»™ dÃ i chuá»—i, chiá»u biá»ƒu diá»…n,
vÃ  sá»‘ lá»›p, tÆ°Æ¡ng á»©ng. Báº±ng cÃ¡ch sá»­ dá»¥ng
phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t, Ä‘á»™ phá»©c táº¡p cho má»—i
cá»­a sá»• trá»Ÿ thÃ nh O(wÂ²Â·dÂ·l), trong Ä‘Ã³ w biá»ƒu thá»‹
kÃ­ch thÆ°á»›c cá»­a sá»•. Äáº·t bÆ°á»›c s = w, tá»•ng
sá»‘ khá»‘i mÃ£ trá»Ÿ thÃ nh k = n/w, dáº«n Ä‘áº¿n
kÃ­ch thÆ°á»›c cá»­a sá»• w = n/k. Do Ä‘Ã³, Ä‘á»™ phá»©c táº¡p
tá»•ng thá»ƒ Ä‘Æ°á»£c Ä‘Æ¡n giáº£n hÃ³a thÃ nh:

O(kÂ·wÂ²Â·dÂ·l) = O(kÂ·(n/k)Â²Â·dÂ·l) = O(nÂ²/kÂ·dÂ·l).     (10)

Viá»‡c giáº£m Ä‘Ã¡ng ká»ƒ Ä‘á»™ phá»©c táº¡p nÃ y xuá»‘ng 1/k cho phÃ©p
SEA mÃ£ hÃ³a mÃ£ dÃ i vá»›i Ã­t bá»™ nhá»› vÃ 
chi phÃ­ tÃ­nh toÃ¡n hÆ¡n.

HÆ¡n ná»¯a, nhÆ° thá»ƒ hiá»‡n trong Báº£ng 3, chÃºng tÃ´i quan sÃ¡t
ráº±ng so vá»›i GraphCodeBERT, SEA káº¿t
há»£p Aggregation attention má»™t lá»›p chá»‰ giá»›i thiá»‡u
thÃªm d tham sá»‘ cÃ³ thá»ƒ há»c. Máº·c dÃ¹
sá»± tÄƒng khiÃªm tá»‘n nÃ y trong sá»‘ lÆ°á»£ng tham sá»‘, nÃ³ Ä‘Ã³ng má»™t
vai trÃ² quan trá»ng trong viá»‡c tÄƒng cÆ°á»ng hiá»‡u quáº£ cá»§a
giai Ä‘oáº¡n tá»•ng há»£p, nhÆ° cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i sáº½ cung cáº¥p
báº±ng chá»©ng trong Pháº§n 6.1.

4.4. Xá»­ lÃ½ Batch

Äá»ƒ tÄƒng cÆ°á»ng hiá»‡u quáº£ suy luáº­n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u lá»›n,
cáº§n thiáº¿t pháº£i thiáº¿t káº¿ má»™t phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ batch
cÃ³ kháº£ nÄƒng mÃ£ hÃ³a nhiá»u Ä‘oáº¡n mÃ£ dÃ i
Ä‘á»“ng thá»i. NhÆ° Ä‘Ã£ nÃªu trong Pháº§n 4.1, chÃºng tÃ´i
cÃ³ Ä‘Æ°á»£c nhiá»u khá»‘i mÃ£ tá»« má»—i Ä‘oáº¡n mÃ£ dÃ i.
Tuy nhiÃªn, do sá»‘ lÆ°á»£ng khÃ¡c nhau cá»§a

Encoder mÃ£FusionBatch
mÃ£
Fusion
FusionBatch
khá»‘iEmbeddings khá»‘i
â‘¡
â‘¡
â‘¡â‘ Khá»‘i
mÃ£20221014 sá»­a Ä‘á»•i
89%11%
Ä‘á»™ dÃ i token <= 128
Ä‘á»™ dÃ i token > 128
9%
91%Ä‘á»™ dÃ i token <= 256
Ä‘á»™ dÃ i token > 256

HÃ¬nh 4: PhÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia cho xá»­ lÃ½ batch. â‘  vÃ  â‘¡ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c phÆ°Æ¡ng phÃ¡p káº¿t há»£p vÃ  chia.

cÃ¡c khá»‘i mÃ£ tÆ°Æ¡ng á»©ng cho cÃ¡c Ä‘oáº¡n mÃ£ dÃ i
khÃ¡c nhau, viá»‡c thiáº¿t káº¿ má»™t phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ batch
chung gáº·p thÃ¡ch thá»©c.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i giá»›i thiá»‡u phÆ°Æ¡ng phÃ¡p káº¿t há»£p-
chia. NhÆ° minh há»a trong HÃ¬nh 4, giáº£ sá»­
kÃ­ch thÆ°á»›c batch lÃ  3 (bao gá»“m ba Ä‘oáº¡n mÃ£),
sá»‘ lÆ°á»£ng khá»‘i mÃ£ tÆ°Æ¡ng á»©ng cho má»—i
Ä‘oáº¡n lÃ  2, 3, vÃ  1, tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i báº¯t Ä‘áº§u báº±ng
viá»‡c káº¿t há»£p sÃ¡u khá»‘i mÃ£ nÃ y thÃ nh má»™t batch khá»‘i
vÃ  thiáº¿t láº­p má»™t Ã¡nh xáº¡ M liÃªn káº¿t
chá»‰ sá»‘ mÃ£ vá»›i chá»‰ sá»‘ khá»‘i. Tiáº¿p theo, chÃºng tÃ´i nháº­p
batch khá»‘i nÃ y vÃ o encoder mÃ£ song song
Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c embeddings khá»‘i. Cuá»‘i cÃ¹ng, táº­n dá»¥ng
thÃ´ng tin tá»« Ã¡nh xáº¡ M, chÃºng tÃ´i tÃ¡ch
cÃ¡c embeddings thÃ nh ba nhÃ³m vÃ  nháº­p chÃºng
vÃ o module tá»•ng há»£p Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c biá»ƒu diá»…n mÃ£
riÃªng biá»‡t.

5. Thiáº¿t káº¿ ThÃ­ nghiá»‡m

5.1. Bá»™ dá»¯ liá»‡u

ChÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m trÃªn bá»™ dá»¯ liá»‡u Code-
SearchNet (Husain et al., 2019) Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i,
bao gá»“m sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh, tá»©c lÃ  Ruby,
JavaScript, Go, Python, Java, vÃ  PHP. Theo
phÆ°Æ¡ng phÃ¡p trong (Guo et al., 2021), chÃºng tÃ´i Ã¡p dá»¥ng
lá»c Ä‘á»ƒ loáº¡i bá» cÃ¡c truy váº¥n cháº¥t lÆ°á»£ng tháº¥p vÃ  má»Ÿ rá»™ng
táº­p há»£p truy xuáº¥t Ä‘á»ƒ bao gá»“m toÃ n bá»™ corpus mÃ£.

5.2. CÃ¡c chá»‰ sá»‘ ÄÃ¡nh giÃ¡

Trong Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i, chÃºng tÃ´i sá»­ dá»¥ng hai tiÃªu chÃ­ tá»± Ä‘á»™ng
phá»• biáº¿n: MRR (Mean Reciprocal Ranking) vÃ  R@k
(Ä‘á»™ chÃ­nh xÃ¡c top-k, k=1, 5, 10, 100). ChÃºng
thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c nghiÃªn cá»©u tÃ¬m kiáº¿m mÃ£ trÆ°á»›c Ä‘Ã¢y (Lv
et al., 2015; Gu et al., 2018; Sachdev et al., 2018;
Husain et al., 2019; Feng et al., 2020; Huang et al.,
2021; Guo et al., 2021). NgoÃ i ra, chÃºng tÃ´i bÃ¡o cÃ¡o
sá»‘ lÆ°á»£ng tham sá»‘ vÃ  thá»i gian suy luáº­n nhÆ°
thÆ°á»›c Ä‘o hiá»‡u quáº£.

5.3. CÃ i Ä‘áº·t ThÃ­ nghiá»‡m

Baseline cá»§a chÃºng tÃ´i lÃ  GraphCodeBERT. CÃ¡c tham sá»‘
cá»§a encoder mÃ£ vÃ  ngÃ´n ngá»¯ tá»± nhiÃªn Ä‘Æ°á»£c

--- TRANG 7 ---
Báº£ng 4: Hiá»‡u suáº¥t tÃ¬m kiáº¿m cá»§a cÃ¡c biáº¿n thá»ƒ SEA khÃ¡c nhau. Bá»™ dá»¯ liá»‡u: CodeSearchNet Ruby.

Cá»­a sá»• | BÆ°á»›c | Chia | Tá»•ng há»£p | MRR | R@1 | R@5 | R@10 | R@100
GraphCodeBERT | â€“ | â€“ | â€“ | â€“ | 0.6948 | 59.3 | 82.1 | 87.3 | 96.5
SEA-SpaceSplitting256 | 128 | Space | Maxpooling | 0.6919 | 58.5 | 82.0 | 87.2 | 95.2
256 | 128 | Space | Meanpooling | 0.6929 | 58.3 | 83.0 | 87.4 | 95.6
256 | 128 | Space | Attention (two layers) | 0.6940 | 58.7 | 83.4 | 87.1 | 94.8
256 | 128 | Space | Attention (two layers) + Mean | 0.7490 | 66.3 | 85.2 | 88.9 | 94.4
256 | 128 | Space | Attention (one layer) | 0.6989 | 59.6 | 82.2 | 86.8 | 95.0
256 | 128 | Space | Attention (one layer) + Mean | 0.7495 | 66.1 | 86.3 | 89.0 | 94.3
128 | 64 | Space | Attention (one layer) + Mean | 0.7545 | 66.2 | 87.5 | 90.2 | 95.2
64 | 32 | Space | Attention (one layer) + Mean | 0.7431 | 65.1 | 85.6 | 88.7 | 94.0
SEA-TokenSplitting256 | 128 | Token | Attention (one layer) + Mean | 0.7752 | 68.4 | 89.1 | 91.9 | 96.0
128 | 64 | Token | Attention (one layer) + Mean | 0.7606 | 67.2 | 87.5 | 91.3 | 95.6
64 | 32 | Token | Attention (one layer) + Mean | 0.7352 | 62.8 | 87.2 | 90.6 | 95.0
SEA-LineSplitting64 | 32 | Line | Attention (one layer) + Mean | 0.7635 | 67.3 | 88.2 | 91.3 | 95.6
32 | 16 | Line | Attention (one layer) + Mean | 0.7537 | 66.1 | 87.2 | 90.3 | 95.2
16 | 8 | Line | Attention (one layer) + Mean | 0.7498 | 65.5 | 86.9 | 90.3 | 95.0
SEA-ASTSplitting64 | 32 | AST | Attention (one layer) + Mean | 0.7539 | 65.7 | 91.4 | 95.0 | 97.6
32 | 16 | AST | Attention (one layer) + Mean | 0.7762 | 68.8 | 89.1 | 92.0 | 96.4
16 | 8 | AST | Attention (one layer) + Mean | 0.7744 | 68.8 | 88.7 | 91.4 | 96.3

khá»Ÿi táº¡o bá»Ÿi GraphCodeBERT. Äá»ƒ huáº¥n luyá»‡n, chÃºng tÃ´i ngáº«u
nhiÃªn chá»n 6 khá»‘i mÃ£ tá»« cÃ¡c khá»‘i mÃ£ Ä‘Æ°á»£c chia
cá»§a má»™t mÃ£ dÃ i. KÃ­ch thÆ°á»›c batch huáº¥n luyá»‡n lÃ 
32. Äá»ƒ Ä‘Ã¡nh giÃ¡, chÃºng tÃ´i sá»­ dá»¥ng táº¥t cáº£ cÃ¡c khá»‘i mÃ£ Ä‘Æ°á»£c chia
cá»§a má»™t mÃ£ dÃ i. KÃ­ch thÆ°á»›c batch Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ lÃ  256.
Táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m Ä‘Æ°á»£c tiáº¿n hÃ nh trÃªn má»™t mÃ¡y vá»›i
Intel Xeon E5-2698v4 2.2Ghz 20-Core CPU vÃ 
hai Tesla V100 32GB GPUs.

6. Káº¿t quáº£ ThÃ­ nghiá»‡m

6.1. Cáº¥u hÃ¬nh SEA Tá»‘i Æ°u

Äá»ƒ xÃ¡c Ä‘á»‹nh cáº¥u hÃ¬nh tá»‘i Æ°u cho SEA, chÃºng tÃ´i
tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m báº±ng cÃ¡ch thay Ä‘á»•i kiáº¿n trÃºc cá»§a chÃºng tÃ´i
sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p chia mÃ£ khÃ¡c nhau vÃ  cÃ¡c
phÆ°Æ¡ng phÃ¡p tá»•ng há»£p, trong khi Ä‘o lÆ°á»ng nhá»¯ng thay Ä‘á»•i
káº¿t quáº£ trong hiá»‡u suáº¥t tÃ¬m kiáº¿m. Cho ráº±ng
bá»™ dá»¯ liá»‡u CodeSearchNet Ruby tÆ°Æ¡ng Ä‘á»‘i nhá»,
chÃºng tÃ´i táº­p trung vÃ o viá»‡c tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m trÃªn táº­p con ruby,
vÃ  chÃºng tÃ´i trÃ¬nh bÃ y káº¿t quáº£ trong Báº£ng 4.

Trong cÃ¡c hÃ ng SpaceSplitting cá»§a Báº£ng 4, chÃºng tÃ´i thá»­ nghiá»‡m
vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tá»•ng há»£p khÃ¡c nhau nhÆ° mÃ´ táº£ trong
Pháº§n 4.3. PhÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng sá»­ dá»¥ng báº¥t ká»³
phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Ä‘Æ¡n láº» nÃ o má»™t cÃ¡ch riÃªng láº» khÃ´ng mang láº¡i
cáº£i thiá»‡n hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ so
vá»›i Baseline GraphCodeBERT. Tuy nhiÃªn, khi
káº¿t há»£p phÆ°Æ¡ng phÃ¡p attention vá»›i meanpooling, chÃºng tÃ´i
quan sÃ¡t tháº¥y cáº£i thiá»‡n hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ.
Cá»¥ thá»ƒ, phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Attention (one layer) + Mean
cáº£i thiá»‡n MRR vÃ  R@1 láº§n lÆ°á»£t 7.9%
vÃ  11.5%. Do Ä‘Ã³, cho cÃ¡c
thÃ­ nghiá»‡m tiáº¿p theo, chÃºng tÃ´i chá»n sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Attention
(one layer) + Mean.

Trong cÃ¡c hÃ ng SpaceSplitting, TokenSplitting,
LineSplitting, ASTSplitting cá»§a Báº£ng 4, chÃºng tÃ´i khÃ¡m phÃ¡ cÃ¡c
phÆ°Æ¡ng phÃ¡p chia mÃ£ khÃ¡c nhau, nhÆ° chi tiáº¿t trong Pháº§n 4.2. Äá»‘i vá»›i
cÃ¡c phÆ°Æ¡ng phÃ¡p chia dá»±a trÃªn space vÃ  token, chÃºng tÃ´i Ä‘áº·t

kÃ­ch thÆ°á»›c cá»­a sá»• tá»« 64 Ä‘áº¿n 256 do tÃ­nh háº¡t
nhá» hÆ¡n cá»§a viá»‡c chia. NgÆ°á»£c láº¡i, Ä‘á»‘i vá»›i cÃ¡c
phÆ°Æ¡ng phÃ¡p chia dá»±a trÃªn line vÃ  AST, chÃºng tÃ´i Ä‘áº·t kÃ­ch thÆ°á»›c cá»­a sá»• tá»«
16 Ä‘áº¿n 64. ÄÃ¡ng chÃº Ã½, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng phÆ°Æ¡ng phÃ¡p chia
dá»±a trÃªn AST thá»ƒ hiá»‡n hiá»‡u suáº¥t xuáº¥t sáº¯c,
Ä‘áº¡t Ä‘Æ°á»£c MRR vÃ  R@1 cao nháº¥t vá»›i kÃ­ch thÆ°á»›c cá»­a sá»•
32. Káº¿t quáº£ lÃ , trong cÃ¡c thÃ­ nghiá»‡m tiáº¿p theo,
SEA Ä‘á» cáº­p Ä‘áº¿n SEA-ASTSplitting vá»›i kÃ­ch thÆ°á»›c cá»­a sá»•
32, kÃ­ch thÆ°á»›c bÆ°á»›c 16 vÃ  phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Attention (one layer)
+ Mean.

6.2. So sÃ¡nh vá»›i Ba Sparse
Transformers

Trong pháº§n nÃ y, chÃºng tÃ´i tiáº¿n hÃ nh so sÃ¡nh giá»¯a
SEA vÃ  ba sparse Transformers, BIGBIRD (Za-
heer et al., 2020), Longformer (Beltagy et al., 2020),
vÃ  LongCoder (Guo et al., 2023). BIGBIRD vÃ 
Longformer lÃ  hai Transformer hÆ°á»›ng tÃ i liá»‡u dÃ i
ná»•i tiáº¿ng. LongCoder sá»­ dá»¥ng má»™t cÆ¡ cháº¿ cá»­a sá»• trÆ°á»£t
Ä‘á»ƒ xá»­ lÃ½ input mÃ£ dÃ i cho hoÃ n thÃ nh mÃ£. Cá»¥ thá»ƒ, chÃºng tÃ´i táº­n dá»¥ng cÃ¡c
mÃ´ hÃ¬nh bigbird-roberta-baseÂ², longformer-base-4096Â³ vÃ 
longcoder-baseâ´, vá»›i Ä‘á»™ dÃ i token
1024. Do BIGBIRD vÃ  Longformer khÃ´ng Ä‘Æ°á»£c
tiá»n huáº¥n luyá»‡n trÃªn bá»™ dá»¯ liá»‡u mÃ£, chÃºng tÃ´i cÅ©ng tiáº¿n hÃ nh
cÃ¡c thÃ­ nghiá»‡m Ä‘á»ƒ khá»Ÿi táº¡o BIGBIRD vÃ  Longformer
vá»›i cÃ¡c tham sá»‘ cá»§a GraphCodeBERT. Káº¿t
quáº£ Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 5. So sÃ¡nh cÃ¡c
káº¿t quáº£ trÆ°á»›c vÃ  sau khi khá»Ÿi táº¡o BIGBIRD vÃ 
Longformer vá»›i cÃ¡c tham sá»‘ cá»§a GraphCode-
BERT, chÃºng tÃ´i tháº¥y ráº±ng káº¿t quáº£ MRR cáº£i thiá»‡n tá»«
0.2952 vÃ  0.5016 lÃªn 0.6121 vÃ  0.6595, tÆ°Æ¡ng

Â²https://huggingface.co/google/bigbird-roberta-base
Â³https://huggingface.co/allenai/longformer-base-
4096
â´https://huggingface.co/microsoft/longcoder-base

--- TRANG 8 ---
Báº£ng 5: So sÃ¡nh vá»›i sparse Transformers. KÃ½ hiá»‡u (G) cho biáº¿t ráº±ng mÃ´ hÃ¬nh Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i cÃ¡c tham sá»‘ GraphCodeBERT. Thá»i gian suy luáº­n mÃ£ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng cÃ¡ch chá»n ngáº«u nhiÃªn 1000 mÃ£ vÃ  tÃ­nh toÃ¡n thá»i gian suy luáº­n trung bÃ¬nh. ChÃºng tÃ´i láº·p láº¡i má»—i láº§n tÃ­nh toÃ¡n thÃ­ nghiá»‡m ba láº§n vÃ  bÃ¡o cÃ¡o trung bÃ¬nh vÃ  Ä‘á»™ lá»‡ch chuáº©n. Bá»™ dá»¯ liá»‡u: CodeSearchNet Ruby. SEA vÆ°á»£t trá»™i hÆ¡n cÃ¡c mÃ´ hÃ¬nh khÃ¡c má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ (p < 0.01).

MÃ´ hÃ¬nh | #Param. | Äá»™ dÃ i Token | Thá»i gian Suy luáº­n | MRR | R@1 | R@5 | R@10
GraphCodeBERT | 124.6M | 256 | 6.3 Â±0.3ms | 0.6948 | 59.3 | 82.1 | 87.3
BIGBIRD | 127.5M | 1024 | 20.1 Â±0.2ms | 0.2952 | 19.2 | 39.8 | 51.1
BIGBIRD (G) | 127.5M | 1024 | 19.8 Â±0.0ms | 0.6121 | 50.8 | 74.2 | 80.7
Longformer | 148.7M | 1024 | 33.7 Â±0.2ms | 0.5128 | 39.9 | 65.3 | 72.4
Longformer (G) | 148.7M | 1024 | 33.7 Â±0.1ms | 0.6595 | 55.1 | 79.4 | 84.0
LongCoder | 149.6M | 1024 | 68.6 Â±0.2ms | 0.4718 | 35.8 | 61.1 | 67.8
SEA | 124.6M | - | 7.2 Â±0.5ms | 0.7762 | 68.8 | 89.1 | 92.0
- w/o combine-divide | 124.6M | - | 24.3 Â±2.4ms | 0.7762 | 68.8 | 89.1 | 92.0

á»©ng. ChÃºng tÃ´i cho ráº±ng khoáº£ng cÃ¡ch hiá»‡u suáº¥t nÃ y do
nhu cáº§u tiá»n huáº¥n luyá»‡n láº¡i cÃ¡c mÃ´ hÃ¬nh ban Ä‘áº§u Ä‘Æ°á»£c
tiá»n huáº¥n luyá»‡n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u ngÃ´n ngá»¯ tá»± nhiÃªn. ChÃºng tÃ´i quan
sÃ¡t tháº¥y ráº±ng MRR cá»§a LongCoder lÃ  0.4718, Ä‘iá»u nÃ y
thá»ƒ hiá»‡n má»™t sá»± giáº£m Ä‘Ã¡ng ká»ƒ so vá»›i
GraphCodeBERT, gá»£i Ã½ ráº±ng LongCoder cÃ³ thá»ƒ
chá»§ yáº¿u phÃ¹ há»£p cho cÃ¡c nhiá»‡m vá»¥ HoÃ n thÃ nh MÃ£. ChÃºng tÃ´i
cÅ©ng tiáº¿n hÃ nh t-tests giá»¯a SEA cá»§a chÃºng tÃ´i vÃ  cÃ¡c
baseline khÃ¡c, vÃ  káº¿t quáº£ chá»©ng minh ráº±ng SEA
vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i táº¥t cáº£ cÃ¡c baseline sparse Transformer
(p < 0.01), lÃ m ná»•i báº­t hiá»‡u suáº¥t vÆ°á»£t trá»™i cá»§a nÃ³
trong lÄ©nh vá»±c tÃ¬m kiáº¿m mÃ£.

Vá» tham sá»‘ mÃ´ hÃ¬nh vÃ  hiá»‡u quáº£ tÃ¬m kiáº¿m,
SEA ná»•i báº­t vÃ¬ nÃ³ cÃ³ sá»‘ lÆ°á»£ng tham sá»‘
tháº¥p hÆ¡n vÃ  thá»i gian suy luáº­n ngáº¯n hÆ¡n so
vá»›i BIGBIRD, Longformer vÃ  LongCoder. ÄÃ¡ng
chÃº Ã½ lÃ  sá»‘ lÆ°á»£ng tham sá»‘ cá»§a SEA gáº§n nhÆ°
tÆ°Æ¡ng Ä‘á»“ng vá»›i GraphCodeBERT, chá»‰ khÃ¡c nhau báº±ng
viá»‡c thÃªm má»™t lá»›p attention duy nháº¥t. Tuy nhiÃªn,
thay Ä‘á»•i nhá» nÃ y dáº«n Ä‘áº¿n má»™t cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong
hiá»‡u suáº¥t tÃ¬m kiáº¿m. ChÃºng tÃ´i cÅ©ng trÃ¬nh bÃ y káº¿t quáº£ thÃ­ nghiá»‡m
mÃ  khÃ´ng sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia
trong Báº£ng 5. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng trong khi
hiá»‡u suáº¥t tÃ¬m kiáº¿m váº«n á»•n Ä‘á»‹nh, thá»i gian suy luáº­n
tÄƒng hÆ¡n ba láº§n. Äiá»u nÃ y lÃ m ná»•i báº­t
cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong thá»i gian suy luáº­n do
phÆ°Æ¡ng phÃ¡p káº¿t há»£p-chia mang láº¡i, tá»« Ä‘Ã³ xÃ¡c nháº­n hiá»‡u quáº£ cá»§a nÃ³ trong viá»‡c tÄƒng tá»‘c
quÃ¡ trÃ¬nh suy luáº­n cá»§a mÃ´ hÃ¬nh.

6.3. Hiá»‡u suáº¥t SEA trÃªn Äá»™ dÃ i MÃ£
KhÃ¡c nhau

Äá»ƒ khÃ¡m phÃ¡ cáº£i thiá»‡n cá»§a SEA Ä‘Æ°á»£c Ä‘á» xuáº¥t
cho cÃ¡c Ä‘oáº¡n mÃ£ vá»›i Ä‘á»™ dÃ i khÃ¡c nhau, chÃºng tÃ´i trÃ¬nh bÃ y
so sÃ¡nh hiá»‡u suáº¥t tÃ¬m kiáº¿m giá»¯a phÆ°Æ¡ng phÃ¡p
baseline GraphCodeBERT vÃ  SEA dÆ°á»›i
cÃ¡c Ä‘á»™ dÃ i token mÃ£ ground-truth khÃ¡c nhau. Káº¿t
quáº£ Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 5.

ÄÃ¡ng chÃº Ã½, hiá»‡u suáº¥t truy xuáº¥t cá»§a má»—i táº­p con truy váº¥n
thá»ƒ hiá»‡n nhá»¯ng cáº£i thiá»‡n Ä‘Ã¡ng chÃº Ã½, Ä‘áº·c
biá»‡t Ä‘á»‘i vá»›i káº¿t quáº£ truy xuáº¥t mÃ£ dÃ i. ChÃºng tÃ´i cho ráº±ng

0.600.620.640.660.680.700.720.740.760.780.80
[0, 256) [256, 512) [512, 768) [768, 1024) [1024, 1943)MRR
Äá»™ dÃ i token mÃ£
GraphCodeBERT
 SEA

HÃ¬nh 5: So sÃ¡nh hiá»‡u suáº¥t giá»¯a
GraphCodeBERT vÃ  SEA trong cÃ¡c Ä‘á»™ dÃ i token mÃ£ ground-truth
khÃ¡c nhau. So vá»›i GraphCodeBERT,
SEA Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ (p < 0.01)
cho cÃ¡c Ä‘á»™ dÃ i token mÃ£ khÃ¡c nhau.

cáº£i thiá»‡n nÃ y lÃ  do hai yáº¿u tá»‘ quan trá»ng. Thá»© nháº¥t, module tá»•ng há»£p
cá»§a SEA thÃ­ch á»©ng náº¯m báº¯t vÃ 
káº¿t há»£p thÃ´ng tin tá»« cÃ¡c phÃ¢n Ä‘oáº¡n Ä‘a dáº¡ng cá»§a
mÃ£ dÃ i, dáº«n Ä‘áº¿n má»™t biá»ƒu diá»…n mÃ£ toÃ n diá»‡n
vÃ  cÃ³ thÃ´ng tin hÆ¡n. Thá»© hai,
phÆ°Æ¡ng phÃ¡p chia mÃ£ Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi SEA cÃ³ thá»ƒ Ä‘Æ°á»£c
xem nhÆ° má»™t hÃ¬nh thá»©c tÄƒng cÆ°á»ng dá»¯ liá»‡u, cung cáº¥p
ngá»¯ cáº£nh vÃ  biáº¿n thá»ƒ bá»• sung giÃºp cá»§ng cá»‘
biá»ƒu diá»…n mÃ£. TÃ³m láº¡i, SEA
mang láº¡i má»™t biá»ƒu diá»…n mÃ£ máº¡nh máº½ hÆ¡n, cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
hiá»‡u suáº¥t truy xuáº¥t tá»•ng thá»ƒ.

6.4. So sÃ¡nh Baseline trÃªn
Nhiá»u NgÃ´n ngá»¯ Láº­p trÃ¬nh

Äá»ƒ Ä‘áº£m báº£o so sÃ¡nh cÃ´ng báº±ng vÃ  cÃ³ thá»ƒ tÃ¡i táº¡o, chÃºng tÃ´i
cáº©n tháº­n chá»n cÃ¡c baseline dá»±a trÃªn tiá»n huáº¥n luyá»‡n Ä‘Ã¡p á»©ng
ba tiÃªu chÃ­ sau: 1) MÃ£ nguá»“n
cÃ³ sáºµn cÃ´ng khai; 2) MÃ´ hÃ¬nh tá»•ng thá»ƒ cÃ³ thá»ƒ
thÃ­ch á»©ng vá»›i táº¥t cáº£ sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh
trÃªn bá»™ dá»¯ liá»‡u CodeSearchNet; 3) BÃ i bÃ¡o Ä‘Æ°á»£c
Ä‘Ã¡nh giÃ¡ ngang hÃ ng náº¿u nÃ³ Ä‘Æ°á»£c xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o nghiÃªn cá»©u.

--- TRANG 9 ---
Báº£ng 6: MRR trÃªn sÃ¡u ngÃ´n ngá»¯ cá»§a bá»™ dá»¯ liá»‡u CodeSearchNet. SEA á»Ÿ Ä‘Ã¢y Ä‘á» cáº­p Ä‘áº¿n SEA-ASTSplitting vá»›i kÃ­ch thÆ°á»›c cá»­a sá»• 32 vÃ  bÆ°á»›c 16. SEA +RoBERTa Ä‘á» cáº­p Ä‘áº¿n SEA vá»›i RoBERTa lÃ m encoder mÃ£. SEA vÆ°á»£t trá»™i so vá»›i cÃ¡c baseline má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ (p < 0.01).

MÃ´ hÃ¬nh / PhÆ°Æ¡ng phÃ¡p | Ruby | Javascript | Go | Python | Java | Php | Tá»•ng thá»ƒ
RoBERTa | 0.587 | 0.517 | 0.850 | 0.587 | 0.599 | 0.560 | 0.617
UniXcoder | 0.586 | 0.603 | 0.881 | 0.695 | 0.687 | 0.644 | 0.683
CodeBERT | 0.679 | 0.620 | 0.882 | 0.672 | 0.676 | 0.628 | 0.693
GraphCodeBERT | 0.703 | 0.644 | 0.897 | 0.692 | 0.691 | 0.649 | 0.713
SEA +RoBERTa | 0.651 (10.9% â†‘) | 0.593 (14.6% â†‘) | 0.879 (3.5% â†‘) | 0.633 (7.9% â†‘) | 0.666 (11.1% â†‘) | 0.647 (15.6% â†‘) | 0.678 (10.0% â†‘)
SEA +UniXcoder | 0.648 (10.7% â†‘) | 0.692 (14.8% â†‘) | 0.896 (1.8% â†‘) | 0.707 (1.7% â†‘) | 0.739 (7.5% â†‘) | 0.712 (10.5% â†‘) | 0.732 (7.3% â†‘)
SEA +CodeBERT | 0.742 (9.3% â†‘) | 0.696 (12.3% â†‘) | 0.905 (2.6% â†‘) | 0.714 (6.2% â†‘) | 0.732 (8.3% â†‘) | 0.711 (13.2% â†‘) | 0.750 (8.3% â†‘)
SEA +GraphCodeBERT | 0.776 (10.4% â†‘) | 0.742 (15.2% â†‘) | 0.921 (2.7% â†‘) | 0.754 (8.9% â†‘) | 0.768 (11.1% â†‘) | 0.748 (15.3% â†‘) | 0.785 (10.1% â†‘)

per. Do Ä‘Ã³, chÃºng tÃ´i chá»n bá»‘n phÆ°Æ¡ng phÃ¡p deep end-to-end
approaches: RoBERTa (Liu et al., 2019), UniX-
coder (Guo et al., 2022), CodeBERT (Feng et al.,
2020), vÃ  GraphCodeBERT (Guo et al., 2021).

Trong Báº£ng 6, chÃºng tÃ´i trÃ¬nh bÃ y káº¿t quáº£ MRR, chá»©ng minh
ráº±ng SEA vÆ°á»£t trá»™i so vá»›i táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p trÃªn
táº¥t cáº£ sÃ¡u ngÃ´n ngá»¯ láº­p trÃ¬nh. ÄÃ¡ng chÃº Ã½, káº¿t luáº­n nÃ y
váº«n nháº¥t quÃ¡n cho chá»‰ sá»‘ recall vÃ 
má»™t biáº¿n thá»ƒ khÃ¡c cá»§a SEA, káº¿t quáº£ cÃ³ thá»ƒ
Ä‘Æ°á»£c tÃ¬m tháº¥y trong gÃ³i sao chÃ©p cá»§a chÃºng tÃ´i. Nhá»¯ng phÃ¡t hiá»‡n nÃ y
cá»§ng cá»‘ tÃ­nh Æ°u viá»‡t cá»§a SEA so
vá»›i cÃ¡c baseline dá»±a trÃªn tiá»n huáº¥n luyá»‡n trÃªn nhiá»u
ngÃ´n ngá»¯ láº­p trÃ¬nh Ä‘a dáº¡ng.

7. Káº¿t luáº­n

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giáº£i quyáº¿t thÃ¡ch thá»©c cá»§a viá»‡c
mÃ´ hÃ¬nh hÃ³a mÃ£ dÃ i má»™t cÃ¡ch hiá»‡u quáº£ cho tÃ¬m kiáº¿m mÃ£. ChÃºng tÃ´i
giá»›i thiá»‡u SEA, má»™t phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ mang láº¡i
biá»ƒu diá»…n mÃ£ Ä‘Æ°á»£c cáº£i thiá»‡n cho cÃ¡c Ä‘oáº¡n mÃ£ dÃ i.
Máº·c dÃ¹ tÃ­nh Ä‘Æ¡n giáº£n cá»§a nÃ³, káº¿t quáº£ thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i
cho tháº¥y hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t Ä‘Ã¡ng chÃº Ã½
cá»§a SEA. ChÃºng tÃ´i tin ráº±ng cÃ´ng trÃ¬nh nÃ y má»Ÿ ra nhá»¯ng kháº£ nÄƒng má»›i
cho tÃ¬m kiáº¿m mÃ£.

8. TuyÃªn bá»‘ Äáº¡o Ä‘á»©c

CÃ¡c má»Ÿ rá»™ng vÃ  á»©ng dá»¥ng tÆ°Æ¡ng lai phÃ¡t sinh tá»«
cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i nÃªn chÃº Ã½ Ä‘áº¿n tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng
cá»§a viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh quy mÃ´ lá»›n. Há» nÃªn
tÃ­ch cá»±c trÃ¡nh viá»‡c láº¡m dá»¥ng tiá»m nÄƒng báº±ng cÃ¡ch tÃ¬m kiáº¿m vá»›i
Ã½ Ä‘á»‹nh Ä‘á»™c háº¡i. Tuy nhiÃªn, khÃ´ng cÃ³ kháº£ nÄƒng mÃ´ hÃ¬nh
trong hÃ¬nh thá»©c hiá»‡n táº¡i sáº½ dáº«n Ä‘áº¿n tÃ¡c Ä‘á»™ng nhÆ° váº­y
trong tÆ°Æ¡ng lai gáº§n. MÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cÅ©ng cÃ³ tiá»m nÄƒng
táº¡o ra tÃ¡c Ä‘á»™ng tÃ­ch cá»±c trong cÃ¡c lÄ©nh vá»±c nhÆ°
tÃ¬m kiáº¿m mÃ£, hiá»ƒu mÃ£ dÃ i vÃ  biá»ƒu diá»…n mÃ£.

9. Lá»i cáº£m Æ¡n

CÃ´ng viá»‡c Ä‘Æ°á»£c mÃ´ táº£ trong bÃ i bÃ¡o nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi
CCF-Huawei Populus Grove Fund CCF-
HuaweiSE202301.

10. TÃ i liá»‡u tham kháº£o

Joshua Ainslie, Santiago OntaÃ±Ã³n, Chris Alberti,
Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang,
vÃ  Li Yang. 2020. ETC: encoding long and
structured inputs in transformers. Trong EMNLP.

Miltiadis Allamanis, Henry Jackson-Flux, vÃ  Marc
Brockschmidt. 2021. Self-supervised bug detec-
tion and repair. Trong NeurIPS.

Uri Alon, Roy Sadaka, Omer Levy, vÃ  Eran Yahav.
2020. Structural language models of code. Trong
ICML.

Iz Beltagy, Matthew E Peters, vÃ  Arman Cohan.
2020. Longformer: The long-document trans-
former. arXiv.

Nghi DQ Bui, Yijun Yu, vÃ  Lingxiao Jiang. 2021.
Treecaps: Tree-based capsule networks for
source code processing. Trong AAAI.

Yitian Chai, Hongyu Zhang, Beijun Shen, vÃ  Xi-
aodong Gu. 2022. Cross-domain deep code
search with few-shot meta learning. arXiv.

Long Chen, Wei Ye, vÃ  Shikun Zhang. 2019. Cap-
turing source code semantics via tree-based con-
volution over api-enhanced ast. Trong CF.

Rewon Child, Scott Gray, Alec Radford, vÃ  Ilya
Sutskever. 2019. Generating long sequences
with sparse transformers. arXiv.

GonÃ§alo M Correia, Vlad Niculae, vÃ  AndrÃ© FT
Martins. 2019. Adaptively sparse transformers.
Trong EMNLP-IJCNLP.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G
Carbonell, Quoc Le, vÃ  Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language models
beyond a fixed-length context. Trong ACL.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, vÃ 
Kristina Toutanova. 2019. BERT: pre-training

--- TRANG 10 ---
of deep bidirectional transformers for language
understanding. Trong NAACL-HLT.

Yali Du vÃ  Zhongxing Yu. 2023. Pre-training code
representation with semantic flow graph for ef-
fective bug localization. Trong FSE/ESEC.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,
Xiaocheng Feng, Ming Gong, Linjun Shou, Bing
Qin, Ting Liu, Daxin Jiang, vÃ  Ming Zhou. 2020.
Codebert: A pre-trained model for programming
and natural languages. Trong EMNLP.

Luyu Gao vÃ  Jamie Callan. 2022. Long document
re-ranking with modular re-ranker. Trong SIGIR.

Dobrik Georgiev, Marc Brockschmidt, vÃ  Miltiadis
Allamanis. 2022. Heat: Hyperedge attention net-
works. arXiv.

Wenchao Gu, Yanlin Wang, Lun Du, Hongyu
Zhang, Shi Han, Dongmei Zhang, vÃ  Michael
Lyu. 2022. Accelerating code search with deep
hashing and code classification. Trong ACL.

Xiaodong Gu, Hongyu Zhang, vÃ  Sunghun Kim.
2018. Deep code search. Trong ICSE.

Michael GÃ¼nther, Jackmin Ong, Isabelle Mohr,
Alaeddine Abdessalem, Tanguy Abel, Moham-
mad Kalim Akram, Susana Guzman, Georgios
Mastrapas, Saba Sturua, Bo Wang, et al. 2023.
Jinaembeddings2: 8192-token general-purpose
text embeddings for long documents. arXiv.

Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming
Zhou, vÃ  Jian Yin. 2022. Unixcoder: Unified
cross-modal pre-training for code representation.
Trong ACL.

Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,
Duyu Tang, Shujie Liu, Long Zhou, Nan Duan,
Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-
fano, Shao Kun Deng, Colin B. Clement, Dawn
Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,
vÃ  Ming Zhou. 2021. Graphcodebert: Pre-
training code representations with data flow. Trong
ICLR.

Daya Guo, Canwen Xu, Nan Duan, Jian Yin, vÃ  Ju-
lian J. McAuley. 2023. Longcoder: A long-range
pre-trained language model for code completion.
Trong ICML, Proceedings of Machine Learning Re-
search. PMLR.

Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,
Xiangyang Xue, vÃ  Zheng Zhang. 2019. Star-
transformer. Trong NAACL-HLT. Association for Com-
putational Linguistics.

Vincent J Hellendoorn, Charles Sutton, Rishabh
Singh, Petros Maniatis, vÃ  David Bieber. 2019.
Global relational models of source code. Trong ICLR.

Emily Hill, Lori Pollock, vÃ  K Vijay-Shanker. 2011.
Improving source code search with natural lan-
guage phrasal representations of method signa-
tures. Trong ASE. IEEE.

Fan Hu, Aozhu Chen, Ziyue Wang, Fangming Zhou,
Jianfeng Dong, vÃ  Xirong Li. 2022. Lightweight
attentional feature fusion: A new baseline for
text-to-video retrieval. Trong ECCV. Springer.

Fan Hu, Yanlin Wang, Lun Du, Xirong Li, Hongyu
Zhang, Shi Han, vÃ  Dongmei Zhang. 2023. Re-
visiting code search in a two-stage paradigm. Trong
WSDM.

Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong,
Ke Xu, Daxin Jiang, Ming Zhou, vÃ  Nan Duan.
2021. Cosqa: 20,000+ web queries for code
search and question answering. Trong ACL.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Mil-
tiadis Allamanis, vÃ  Marc Brockschmidt. 2019.
Codesearchnet challenge: Evaluating the state
of semantic code search. arXiv.

Paul Jaccard. 1901. Ã‰tude comparative de la distri-
bution florale dans une portion des alpes et des
jura. Bull Soc Vaudoise Sci Nat, trang 547â€“579.

Jyun-Yu Jiang, Chenyan Xiong, Chia-Jung Lee, vÃ 
Wei Wang. 2020. Long document ranking with
query-directed sparse transformer. Trong EMNLP
Findings.

Seohyun Kim, Jinman Zhao, Yuchi Tian, vÃ  Satish
Chandra. 2021. Code prediction by feeding trees
to transformers. Trong ICSE.

Nikita Kitaev, Lukasz Kaiser, vÃ  Anselm Levskaya.
2019. Reformer: The efficient transformer. Trong
ICLR.

Canjia Li, Andrew Yates, Sean MacAvaney, Ben He,
vÃ  Yingfei Sun. 2020. Parade: Passage rep-
resentation aggregation for document reranking.
ACM Transactions on Information Systems.

Xirong Li, Yang Zhou, Jie Wang, Hailan Lin,
Jianchun Zhao, Dayong Ding, Weihong Yu, vÃ 
Youxin Chen. 2021. Multi-modal multi-instance
learning for retinal disease recognition. Trong
ACM MM.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, vÃ  Veselin Stoyanov.
2019. Roberta: A robustly optimized bert pre-
training approach. arXiv.

Fei Lv, Hongyu Zhang, Jian-guang Lou, Shaowei
Wang, Dongmei Zhang, vÃ  Jianjun Zhao. 2015.
Codehow: Effective code search based on api
understanding and extended boolean model (e).
Trong ASE.

--- TRANG 11 ---
Y Ma, Yali Du, vÃ  Ming Li. 2023. Capturing the
long-distance dependency in the control flow
graph via structural-guided attention for bug lo-
calization. Trong IJCAI.

Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, vÃ 
Xiaochen Li. 2016. Query expansion based
on crowd knowledge for code search. IEEE
Transactions on Services Computing, trang
771â€“783.

Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, vÃ 
Zhi Jin. 2021. Integrating tree path in transformer
for code representation. Trong NeurIPS.

Stephen Robertson vÃ  Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and
beyond. Now Publishers Inc.

Stephen E Robertson vÃ  K Sparck Jones. 1976.
Relevance weighting of search terms. Journal
of the American Society for Information science,
trang 129â€“146.

Barbara Rosario. 2000. Latent semantic indexing:
An overview. Techn. rep. INFOSYS, trang 1â€“16.

Aurko Roy, Mohammad Saffar, Ashish Vaswani,
vÃ  David Grangier. 2021. Efficient content-
based sparse attention with routing trans-
formers. Transactions of the Association for
Computational Linguistics, 9:53â€“68.

Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun
Kim, Koushik Sen, vÃ  Satish Chandra. 2018.
Retrieval on source code: a neural code search.
Trong MAPL.

Abdus Satter vÃ  Kazi Sakib. 2016. A search log
mining based query expansion technique to im-
prove effectiveness in code search. Trong ICCIT,
trang 586â€“591. IEEE.

Hinrich SchÃ¼tze, Christopher D Manning, vÃ 
Prabhakar Raghavan. 2008. Introduction to
information retrieval, volume 39. Cambridge Uni-
versity Press Cambridge.

Rico Sennrich, Barry Haddow, vÃ  Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. Trong ACL.

Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,
Shi Han, Dongmei Zhang, vÃ  Hongbin Sun.
2021. Cast: Enhancing code summarization
with hierarchical splitting and reconstruction of
abstract syntax trees. Trong EMNLP.

Weisong Sun, Chunrong Fang, Yuchen Chen,
Guanhong Tao, Tingxu Han, vÃ  Quanjun Zhang.
2022. Code search based on context-aware code
translation. arXiv.

Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun,
Lili Mou, vÃ  Lu Zhang. 2020. Treegen: A tree-
based transformer architecture for code genera-
tion. Trong AAAI.

Tomoki Tsujimura, Koshi Yamada, Ryuki Ida,
Makoto Miwa, vÃ  Yutaka Sasaki. 2023. Contex-
tualized medication event extraction with strid-
ing ner and multi-turn qa. Journal of Biomedical
Informatics, trang 104416.

Thanh Van Nguyen, Anh Tuan Nguyen, Hung Dang
Phan, Trong Duc Nguyen, vÃ  Tien N Nguyen.
2017. Combining word2vec with revised vector
space model for better code retrieval. Trong ICSE-C.
IEEE.

Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu,
Zhou Zhao, Jian Wu, vÃ  Philip S. Yu. 2019.
Multi-modal attention network learning for seman-
tic source code retrieval. Trong ASE.

Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh
Nallapati, vÃ  Bing Xiang. 2019. Multi-passage
bert: A globally normalized bert model for open-
domain question answering. Trong EMNLP.

Liu Yang, Mingyang Zhang, Cheng Li, Michael
Bendersky, vÃ  Marc Najork. 2020. Beyond
512 tokens: Siamese multi-depth transformer-
based hierarchical encoder for long-form docu-
ment matching. Trong CIKM.

Yangrui Yang vÃ  Qing Huang. 2017. Iecs: Intent-
enforced code search via extended boolean
model. Journal of Intelligent & Fuzzy Systems,
trang 2565â€“2576.

Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, vÃ 
Zheng Zhang. 2019. Bp-transformer: Modelling
long-range context via binary partitioning. arXiv.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, vÃ  Amr Ahmed. 2020. Big bird:
Transformers for longer sequences. Trong NeurIPS.

Jian Zhang, Xu Wang, Hongyu Zhang, Hailong
Sun, Kaixuan Wang, vÃ  Xudong Liu. 2019a. A
novel neural source code representation based
on abstract syntax tree. Trong ICSE.

Xingxing Zhang, Furu Wei, vÃ  Ming Zhou. 2019b.
Hibert: Document level pre-training of hierarchi-
cal bidirectional transformers for document sum-
marization. Trong ACL.
