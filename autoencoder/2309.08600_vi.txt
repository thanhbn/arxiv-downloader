# SPARSE AUTOENCODERS FIND HIGHLY INTER -
PRETABLE FEATURES IN LANGUAGE MODELS
Hoagy Cunningham∗12, Aidan Ewart∗13, Logan Riggs∗1, Robert Huben, Lee Sharkey4
1EleutherAI,2MATS,3Bristol AI Safety Centre,4Apollo Research
{hoagycunningham, aidanprattewart, logansmith5 }@gmail.com

# AUTOENCODERS THƯA THỚT TÌM RA CÁC ĐẶC TRƯNG CÓ TÍNH GIẢI THÍCH CAO TRONG CÁC MÔ HÌNH NGÔN NGỮ

## TÓM TẮT
Một trong những rào cản đối với việc hiểu biết tốt hơn về bên trong của mạng neural là tính đa nghĩa, nơi các neuron dường như kích hoạt trong nhiều bối cảnh khác nhau về mặt ngữ nghĩa. Tính đa nghĩa ngăn cản chúng ta xác định những giải thích ngắn gọn, dễ hiểu đối với con người về những gì mạng neural đang thực hiện bên trong. Một nguyên nhân được giả định của tính đa nghĩa là hiện tượng chồng chất, nơi mạng neural biểu diễn nhiều đặc trưng hơn số neuron mà chúng có bằng cách gán các đặc trưng cho một tập hợp các hướng hoàn chỉnh quá mức trong không gian kích hoạt, thay vì cho từng neuron riêng lẻ. Ở đây, chúng tôi cố gắng xác định những hướng đó, sử dụng autoencoders thưa thớt để tái tạo lại các kích hoạt bên trong của một mô hình ngôn ngữ. Những autoencoders này học các tập hợp đặc trưng kích hoạt thưa thớt có tính giải thích cao và đơn nghĩa hơn so với các hướng được xác định bởi các phương pháp thay thế, nơi tính giải thích được đo lường bằng các phương pháp tự động. Hơn nữa, chúng tôi chỉ ra rằng với tập hợp đặc trưng đã học của chúng tôi, chúng tôi có thể xác định chính xác các đặc trưng chịu trách nhiệm nhân quả cho hành vi phản thực tế trong nhiệm vụ nhận dạng đối tượng gián tiếp (Wang et al., 2022) ở mức độ tinh vi hơn so với các phân tách trước đây. Công trình này chỉ ra rằng có thể giải quyết hiện tượng chồng chất trong các mô hình ngôn ngữ bằng một phương pháp có thể mở rộng, không giám sát. Phương pháp của chúng tôi có thể phục vụ như một nền tảng cho công việc giải thích cơ học trong tương lai, mà chúng tôi hy vọng sẽ cho phép tính minh bạch và khả năng điều khiển mô hình tốt hơn.

## 1 GIỚI THIỆU
Những tiến bộ trong trí tuệ nhân tạo (AI) đã dẫn đến việc phát triển các hệ thống AI có khả năng cao đưa ra quyết định vì những lý do mà chúng ta không hiểu. Điều này đã gây ra lo ngại rằng các hệ thống AI mà chúng ta không thể tin tưởng đang được triển khai rộng rãi trong nền kinh tế và cuộc sống của chúng ta, mang lại một số rủi ro mới (Hendrycks et al., 2023), bao gồm các rủi ro tương lai tiềm ẩn rằng AI có thể lừa dối con người để đạt được các mục tiêu không mong muốn (Ngo et al., 2022). Khả năng giải thích cơ học tìm cách giảm thiểu những rủi ro như vậy thông qua việc hiểu cách mạng neural tính toán đầu ra của chúng, cho phép chúng ta thiết kế ngược các phần của quá trình nội bộ của chúng và thực hiện các thay đổi có mục tiêu (Cammarata et al., 2021; Wang et al., 2022; Elhage et al., 2021).

Để thiết kế ngược một mạng neural, cần phải chia nhỏ nó thành các đơn vị nhỏ hơn (đặc trưng) có thể được phân tích riêng biệt. Việc sử dụng từng neuron riêng lẻ làm các đơn vị này đã có một số thành công (Olah et al., 2020; Bills et al., 2023), nhưng một thách thức chính là các neuron thường có tính đa nghĩa, kích hoạt cho một số loại đặc trưng không liên quan (Olah et al., 2020). Ngoài ra, đối với một số loại kích hoạt mạng, chẳng hạn như luồng dư của transformer, có ít lý do để mong đợi các đặc trưng sẽ phù hợp với cơ sở neuron (Elhage et al., 2023).

Elhage et al. (2022b) điều tra tại sao tính đa nghĩa có thể phát sinh và đưa ra giả thuyết rằng nó có thể là kết quả của việc các mô hình học nhiều đặc trưng riêng biệt hơn số chiều trong lớp. Họ gọi hiện tượng này là chồng chất. Vì một không gian vector chỉ có thể có nhiều vector trực giao bằng số chiều của nó, điều này có nghĩa là mạng sẽ học một cơ sở hoàn chỉnh quá mức của các đặc trưng không trực giao. Các đặc trưng phải kích hoạt đủ thưa thớt để hiện tượng chồng chất phát sinh vì, không có độ thưa thớt cao, sự can thiệp giữa các đặc trưng không trực giao ngăn cản bất kỳ lợi ích hiệu suất nào từ hiện tượng chồng chất. Điều này gợi ý rằng chúng ta có thể khôi phục các đặc trưng của mạng bằng cách tìm một tập hợp các hướng trong không gian kích hoạt sao cho mỗi vector kích hoạt có thể được tái tạo từ một kết hợp tuyến tính thưa thớt của các hướng này. Điều này tương đương với bài toán học từ điển thưa thớt đã được biết đến (Olshausen & Field, 1997).

Dựa trên Sharkey et al. (2023), chúng tôi huấn luyện các autoencoders thưa thớt để học các tập hợp hướng này. Phương pháp của chúng tôi cũng tương tự như Yun et al. (2021), những người áp dụng học từ điển thưa thớt cho tất cả các lớp luồng dư trong một mô hình ngôn ngữ đồng thời. Phương pháp của chúng tôi được tóm tắt trong Hình 1 và được mô tả trong Phần 2.

Sau đó, chúng tôi sử dụng một số kỹ thuật để xác minh rằng các đặc trưng đã học của chúng tôi đại diện cho một phân tách có ý nghĩa về mặt ngữ nghĩa của không gian kích hoạt. Đầu tiên, chúng tôi chỉ ra rằng các đặc trưng của chúng tôi trung bình có tính giải thích cao hơn so với neuron và các kỹ thuật phân tách ma trận khác, được đo lường bằng điểm số giải thích tự động (Phần 3) (Bills et al., 2023). Tiếp theo, chúng tôi chỉ ra rằng chúng tôi có thể xác định chính xác các đặc trưng được sử dụng cho một nhiệm vụ cụ thể chính xác hơn so với các phương pháp khác (Phần 4). Cuối cùng, chúng tôi thực hiện các nghiên cứu trường hợp trên một số lượng nhỏ đặc trưng, cho thấy chúng không chỉ có tính đơn nghĩa mà còn có tác động có thể dự đoán được lên đầu ra của mô hình, và có thể được sử dụng để phát hiện mạch tinh vi. (Phần 5).

## 2 LẤY CÁC ĐẶC TRƯNG RA KHỎI HIỆN TƯỢNG CHỒNG CHẤT VỚI HỌC TỪ ĐIỂN THƯA THỚT

Để lấy các đặc trưng mạng ra khỏi hiện tượng chồng chất, chúng tôi sử dụng các kỹ thuật từ học từ điển thưa thớt (Olshausen & Field, 1997; Lee et al., 2006). Giả sử rằng mỗi vector trong một tập hợp các vector đã cho {xi}nvec i=1⊂Rd được tạo thành từ một kết hợp tuyến tính thưa thớt của các vector chưa biết {gj}ngt j=1⊂Rd, tức là xi=∑j ai,jgj trong đó ai là một vector thưa thớt. Trong trường hợp của chúng tôi, các vector dữ liệu {xi}nvec i=1 là các kích hoạt bên trong của một mô hình ngôn ngữ, chẳng hạn như Pythia-70M (Biderman et al., 2023), và {gj}ngt j=1 là các đặc trưng mạng thật chưa biết. Chúng tôi muốn học một từ điển các vector, được gọi là đặc trưng từ điển, {fk}nfeat k=1⊂Rd trong đó với bất kỳ đặc trưng mạng gj nào, tồn tại một đặc trưng từ điển fk sao cho gj≈fk.

Để học từ điển, chúng tôi huấn luyện một autoencoder với một thuật ngữ phạt độ thưa thớt trên các kích hoạt ẩn của nó. Autoencoder là một mạng neural với một lớp ẩn duy nhất có kích thước dhid=R·din, trong đó din là chiều của các vector kích hoạt bên trong mô hình ngôn ngữ¹, và R là một siêu tham số kiểm soát tỷ lệ của kích thước từ điển đặc trưng so với chiều mô hình. Chúng tôi sử dụng hàm kích hoạt ReLU trong lớp ẩn (Fukushima, 1975). Chúng tôi cũng sử dụng trọng số liên kết cho mạng neural của chúng tôi, có nghĩa là các ma trận trọng số của encoder và decoder là chuyển vị của nhau². Do đó, trên vector đầu vào x∈ {xi}, mạng của chúng tôi tạo ra đầu ra x̂, được đưa ra bởi

c = ReLU(Mx + b) (1)
x̂ = MTc (2)
= ∑(i=0 to dhid-1) cifi (3)

trong đó M∈Rdhid×din và b∈Rdhid là các tham số đã học của chúng tôi, và M được chuẩn hóa theo hàng³. Ma trận tham số M của chúng tôi là từ điển đặc trưng của chúng tôi, bao gồm dhid hàng của các đặc trưng từ điển fi. Đầu ra x̂ được dự định là một tái tạo của vector gốc x, và lớp ẩn c bao gồm các hệ số chúng tôi sử dụng trong việc tái tạo x của chúng tôi.

Autoencoder của chúng tôi được huấn luyện để tối thiểu hóa hàm mất mát

L(x) = ||x - x̂||²₂ + α||c||₁ (4)
        ⌊────────────⌋   ⌊──────⌋
        Mất mát tái tạo   Mất mát thưa thớt

trong đó α là một siêu tham số kiểm soát độ thưa thớt của việc tái tạo. Thuật ngữ mất mát ℓ1 trên c khuyến khích việc tái tạo của chúng tôi là một kết hợp tuyến tính thưa thớt của các đặc trưng từ điển. Có thể chỉ ra bằng thực nghiệm (Sharkey et al., 2023) và lý thuyết (Wright & Ma, 2022) rằng việc tái tạo với phạt ℓ1 có thể khôi phục các đặc trưng thật đã tạo ra dữ liệu. Để biết thêm chi tiết về quá trình huấn luyện của chúng tôi, xem Phụ lục B.

## 3 GIẢI THÍCH CÁC ĐẶC TRƯNG TỪ ĐIỂN

### 3.1 TÍNH GIẢI THÍCH Ở QUY MÔ LỚN

Sau khi học được một tập hợp các đặc trưng từ điển, chúng tôi muốn hiểu liệu các đặc trưng đã học của chúng tôi có hiển thị tính đa nghĩa giảm và do đó có tính giải thích cao hơn hay không. Để làm điều này một cách có thể mở rộng, chúng tôi cần một thước đo để đo lường mức độ giải thích của một đặc trưng từ điển. Chúng tôi sử dụng phương pháp tự động được giới thiệu trong Bills et al. (2023) vì nó mở rộng tốt để đo lường tính giải thích trên hàng nghìn đặc trưng từ điển mà autoencoders của chúng tôi học. Tóm lại, quy trình giải thích tự động lấy các mẫu văn bản nơi đặc trưng từ điển kích hoạt, yêu cầu một mô hình ngôn ngữ viết một diễn giải có thể đọc được cho con người về đặc trưng từ điển, và sau đó nhắc mô hình ngôn ngữ sử dụng mô tả này để dự đoán kích hoạt của đặc trưng từ điển trên các mẫu văn bản khác. Mối tương quan giữa các kích hoạt dự đoán của mô hình và các kích hoạt thực tế là điểm số giải thích của đặc trưng đó. Xem Phụ lục A và Bills et al. (2023) để biết thêm chi tiết.

Chúng tôi hiển thị các mô tả và điểm số hàng đầu và ngẫu nhiên cho năm đặc trưng từ điển từ luồng dư lớp 1 trong Bảng 1. Các đặc trưng được hiển thị là năm đặc trưng đầu tiên theo thứ tự (tùy ý) trong từ điển.

Đặc trưng | Mô tả (Được tạo bởi GPT-4) | Điểm số giải thích
---------|-------------------------|------------------
1-0000 | các phần của tên riêng, đặc biệt là họ. | 0.33
1-0001 | các hành động được thực hiện bởi chủ thể hoặc đối tượng. | -0.11
1-0002 | các trường hợp của chữ 'W' và các từ bắt đầu bằng 'w'. | 0.55
1-0003 | số '5' và cũng ghi nhận kích hoạt vừa phải đến thấp cho tên riêng và một số danh từ. | 0.57
1-0004 | thuật ngữ pháp lý và tham chiếu vụ án tòa án. | 0.19

Bảng 1: Kết quả giải thích tự động trên năm đặc trưng đầu tiên được tìm thấy trong luồng dư lớp 1. Giải thích tự động tạo ra một mô tả về ý nghĩa của đặc trưng và một điểm số cho việc mô tả đó dự đoán các kích hoạt khác tốt như thế nào.

### 3.2 CÁC ĐẶC TRƯNG TỪ ĐIỂN THƯA THỚT CÓ TÍNH GIẢI THÍCH CAO HỠN SO VỚI CÁC ĐƯỜNG CƠ SỞ

Chúng tôi đánh giá điểm số giải thích của chúng tôi so với nhiều phương pháp thay thế khác để tìm từ điển đặc trưng trong các mô hình ngôn ngữ. Cụ thể, chúng tôi so sánh điểm số giải thích trên các đặc trưng từ điển của chúng tôi với những điểm số được tạo ra bởi a) cơ sở mặc định, b) hướng ngẫu nhiên, c) Phân tích Thành phần Chính (PCA), và d) Phân tích Thành phần Độc lập (ICA). Đối với các hướng ngẫu nhiên và cho cơ sở mặc định trong luồng dư, chúng tôi thay thế các kích hoạt âm bằng số không để tất cả các kích hoạt đặc trưng đều không âm⁴.

Hình 2 cho thấy rằng các đặc trưng từ điển của chúng tôi có tính giải thích cao hơn nhiều theo thước đo này so với các đặc trưng từ điển được tìm thấy bằng các kỹ thuật tương tự. Chúng tôi thấy rằng sức mạnh của hiệu ứng này giảm khi chúng tôi di chuyển qua mô hình, có thể so sánh với ICA ở lớp 4 và cho thấy cải thiện tối thiểu ở lớp cuối cùng.

Điều này có thể chỉ ra rằng autoencoders thưa thớt hoạt động kém hơn ở các lớp sau nhưng cũng có thể liên quan đến những khó khăn của giải thích tự động, cả vì bằng cách xây dựng trên các lớp trước đó, các đặc trưng sau có thể phức tạp hơn, và vì chúng thường được giải thích tốt nhất bằng hiệu ứng của chúng lên đầu ra. Bills et al. (2023) đã chỉ ra rằng GPT-4 có thể tạo ra các giải thích rất gần với chất lượng trung bình của các giải thích do con người tạo ra khi được cung cấp dữ liệu tương tự. Tuy nhiên, họ cũng chỉ ra rằng các LLM hiện tại bị hạn chế trong các loại mẫu mà chúng có thể tìm thấy, đôi khi gặp khó khăn trong việc tìm ra các mẫu tập trung xung quanh token tiếp theo hoặc token trước đó thay vì token hiện tại, và trong giao thức hiện tại không thể xác minh đầu ra bằng cách nhìn vào các thay đổi trong đầu ra hoặc dữ liệu khác.

Chúng tôi có hiển thị, trong Phần 5, một phương pháp để xem hiệu ứng nhân quả của một đặc trưng lên logits đầu ra bằng tay, nhưng hiện tại chúng tôi không gửi thông tin này cho mô hình ngôn ngữ để tạo giả thuyết. Phần nghiên cứu trường hợp cũng trình bày một đặc trưng từ điển dấu ngoặc đóng, cho thấy các đặc trưng lớp cuối này có thể cung cấp cái nhìn sâu sắc về hoạt động của mô hình.

Xem Phụ lục C để khám phá đầy đủ hơn các từ điển đã học khác nhau qua lăng kính giải thích tự động, xem xét cả MLPs và luồng dư.

Hình 2: Điểm số giải thích tự động hàng đầu và ngẫu nhiên trung bình của các hướng đã học của chúng tôi trong luồng dư, so sánh với một số đường cơ sở, sử dụng 150 đặc trưng mỗi loại. Các thanh lỗi hiển thị khoảng tin cậy 95% xung quanh giá trị trung bình. Các từ điển đặc trưng được sử dụng ở đây được huấn luyện trong 10 epochs sử dụng α=.00086 và R= 2.

## 4 XÁC ĐỊNH CÁC ĐẶC TRƯNG TỪ ĐIỂN QUAN TRỌNG VỀ MẶT NHÂN QUẢ CHO NHẬN DẠNG ĐỐI TƯỢNG GIÁN TIẾP

Trong phần này, chúng tôi định lượng liệu các đặc trưng từ điển đã học của chúng tôi có định vị một hành vi mô hình cụ thể chặt chẽ hơn so với phân tách PCA của các kích hoạt của mô hình hay không. Chúng tôi thực hiện điều này thông qua việc vá kích hoạt, một hình thức phân tích trung gian nhân quả (Vig et al., 2020), qua đó chúng tôi chỉnh sửa các kích hoạt bên trong của mô hình theo các hướng được chỉ ra bởi các đặc trưng từ điển của chúng tôi và đo lường các thay đổi đối với đầu ra của mô hình. Chúng tôi thấy rằng các đặc trưng từ điển của chúng tôi yêu cầu ít vá hơn để đạt được một mức độ phân kỳ KL nhất định trên nhiệm vụ được nghiên cứu so với các phân tách tương tự (Hình 3).

Cụ thể, chúng tôi nghiên cứu hành vi mô hình trên nhiệm vụ Nhận dạng Đối tượng Gián tiếp (IOI) (Wang et al., 2022), trong đó mô hình hoàn thành các câu như "Sau đó, Alice và Bob đi đến cửa hàng. Alice đưa một món ăn nhẹ cho ". Nhiệm vụ này được chọn vì nó nắm bắt một hành vi mô hình đơn giản, đã được nghiên cứu trước đây. Nhớ lại rằng việc huấn luyện từ điển đặc trưng của chúng tôi không nhấn mạnh bất kỳ nhiệm vụ cụ thể nào.

### 4.1 ĐIỀU CHỈNH VÁ KÍCH HOẠT CHO CÁC ĐẶC TRƯNG TỪ ĐIỂN

Trong thí nghiệm của chúng tôi, chúng tôi chạy mô hình trên một câu mục tiêu phản thực tế, đó là một biến thể của câu IOI cơ sở với đối tượng gián tiếp được thay đổi (ví dụ, với "Bob" được thay thế bằng "Vanessa"); lưu các kích hoạt được mã hóa của các đặc trưng từ điển của chúng tôi; và sử dụng các kích hoạt đã lưu để chỉnh sửa luồng dư của mô hình khi chạy trên câu cơ sở.

Cụ thể, chúng tôi thực hiện quy trình sau. Cố định một lớp của mô hình để can thiệp. Chạy mô hình trên câu mục tiêu, lưu logits đầu ra mô hình y và các đặc trưng được mã hóa c̄1, ..., c̄k của lớp đó tại mỗi token trong k token. Sau đó, chạy mô hình trên câu cơ sở cho đến lớp can thiệp, tính toán các đặc trưng được mã hóa c1, ..., ck tại mỗi token, và tại mỗi vị trí thay thế vector luồng dư xi bằng vector được vá

x'i = xi + ∑(j∈F) (c̄i,j - ci,j)fj

trong đó F là tập con của các đặc trưng mà chúng tôi can thiệp vào (chúng tôi mô tả quá trình lựa chọn cho F sau trong phần này). Gọi z là logits đầu ra của mô hình khi bạn hoàn thành việc áp dụng nó cho luồng dư được vá x'1, ..., x'k. Cuối cùng, tính toán phân kỳ KL DKL(z||y), đo lường mức độ gần gũi của các dự đoán mô hình được vá so với mục tiêu. Chúng tôi so sánh các can thiệp này với các can thiệp tương đương sử dụng các thành phần chính được tìm thấy như trong Phần 3.2.

Để chọn tập con đặc trưng F, chúng tôi sử dụng thuật toán Khám phá Mạch Tự động (ACDC) của Conmy et al. (2023). Cụ thể, chúng tôi sử dụng Thuật toán 4.1 của họ trên các đặc trưng của chúng tôi, coi chúng như một đồ thị tính toán phẳng trong đó mỗi đặc trưng đóng góp một thay đổi độc lập cho thước đo đầu ra DKL, như được mô tả ở trên và được tính trung bình trên một tập kiểm tra gồm 50 điểm dữ liệu IOI. Kết quả là một thứ tự trên các đặc trưng sao cho việc vá đặc trưng tiếp theo thường dẫn đến mất mát DKL nhỏ hơn so với mỗi đặc trưng trước đó. Sau đó, các tập con đặc trưng F của chúng tôi là k đặc trưng đầu tiên theo thứ tự này. Chúng tôi áp dụng ACDC riêng biệt trên mỗi phân tách.

### 4.2 ĐỊNH VỊ CHÍNH XÁC CÁC ĐẶC TRƯNG TỪ ĐIỂN IOI

Chúng tôi hiển thị trong Hình 3 rằng các từ điển đặc trưng thưa thớt của chúng tôi cho phép cùng một lượng chỉnh sửa mô hình, được đo lường bằng phân kỳ KL từ mục tiêu, trong ít vá hơn (Trái) và với độ lớn chỉnh sửa nhỏ hơn (Phải) so với phân tách PCA. Chúng tôi cũng chỉ ra rằng điều này không xảy ra nếu chúng tôi huấn luyện một từ điển không thưa (α= 0). Tuy nhiên, các từ điển với hệ số thưa thớt α lớn hơn có độ chính xác tái tạo tổng thể thấp hơn, điều này xuất hiện trong Hình 3 như một phân kỳ KL tối thiểu lớn hơn. Trong Hình 3, chúng tôi xem xét các can thiệp trên lớp 11 của luồng dư, và chúng tôi vẽ các can thiệp trên các lớp khác trong Phụ lục F.

Hình 3: (Trái) Số lượng đặc trưng được vá so với phân kỳ KL từ mục tiêu, sử dụng các phân tách luồng dư khác nhau. Chúng tôi thấy rằng việc vá một số lượng tương đối nhỏ các đặc trưng từ điển hiệu quả hơn so với việc vá các thành phần PCA và các đặc trưng từ từ điển không thưa α= 0. (Phải) Độ lớn chỉnh sửa trung bình so với phân kỳ KL từ mục tiêu khi chúng tôi tăng số lượng đặc trưng được vá. Chúng tôi thấy rằng các từ điển thưa của chúng tôi cải thiện biên Pareto của độ lớn chỉnh sửa so với tính toàn diện của việc chỉnh sửa. Trong cả hai hình, các từ điển đặc trưng được huấn luyện trên 10.000 phần tử đầu tiên của Pile (Gao et al., 2020) (khoảng 7 triệu kích hoạt) sử dụng các giá trị α và R được chỉ ra, trên lớp 11 của Pythia-410M (xem Phụ lục F cho kết quả trên các lớp khác).

## 5 NGHIÊN CỨU TRƯỜNG HỢP

Trong phần này, chúng tôi điều tra các đặc trưng từ điển riêng lẻ, làm nổi bật một số đặc trưng dường như tương ứng với một giải thích duy nhất có thể hiểu được đối với con người (tức là có tính đơn nghĩa). Chúng tôi thực hiện ba phân tích về các đặc trưng từ điển của chúng tôi để xác định ý nghĩa ngữ nghĩa của chúng: (1) Đầu vào: Chúng tôi xác định những token nào kích hoạt đặc trưng từ điển và trong những bối cảnh nào, (2) Đầu ra: Chúng tôi xác định việc loại bỏ đặc trưng thay đổi logits đầu ra của mô hình như thế nào, và (3) Đặc trưng trung gian: Chúng tôi xác định các đặc trưng từ điển trong các lớp trước đó khiến đặc trưng được phân tích kích hoạt.

### 5.1 ĐẦU VÀO: CÁC ĐẶC TRƯNG TỪ ĐIỂN CÓ TÍNH ĐƠN NGHĨA CAO

Chúng tôi đầu tiên phân tích các hướng từ điển của chúng tôi bằng cách kiểm tra văn bản nào khiến chúng kích hoạt. Một đặc trưng từ điển đơn nghĩa lý tưởng sẽ chỉ kích hoạt trên văn bản tương ứng với một đặc trưng thế giới thực duy nhất, trong khi một đặc trưng từ điển đa nghĩa có thể kích hoạt trong các bối cảnh không liên quan.

Để minh họa tốt hơn tính đơn nghĩa của một số đặc trưng từ điển, chúng tôi vẽ biểu đồ tần suất của các kích hoạt trên các kích hoạt token. Kỹ thuật này chỉ hoạt động đối với các đặc trưng từ điển kích hoạt cho một tập hợp nhỏ các token. Chúng tôi tìm thấy các đặc trưng từ điển chỉ kích hoạt trên dấu nháy đơn (Hình 4); dấu chấm; token " the"; và ký tự xuống dòng. Đặc trưng dấu nháy đơn trong Hình 4 tương phản với cơ sở mặc định cho luồng dư, nơi chiều đại diện nhiều nhất cho dấu nháy đơn được hiển thị trong Hình 11 ở Phụ lục D.1; chiều này có tính đa nghĩa vì nó đại diện cho thông tin khác nhau ở các phạm vi kích hoạt khác nhau.

Mặc dù đặc trưng từ điển được thảo luận trong phần trước chỉ kích hoạt cho dấu nháy đơn, nó không kích hoạt trên tất cả các dấu nháy đơn. Điều này có thể được thấy trong Hình 14 và 15 ở Phụ lục D.2, hiển thị hai đặc trưng từ điển kích hoạt dấu nháy đơn khác, nhưng cho các bối cảnh khác nhau (như "[I/We/They]'ll" và "[don/won/wouldn]'t"). Chi tiết về cách chúng tôi tìm kiếm và lựa chọn các đặc trưng từ điển có thể được tìm thấy trong Phụ lục D.3.

### 5.2 ĐẦU RA: CÁC ĐẶC TRƯNG TỪ ĐIỂN CÓ TÁC ĐỘNG TRỰC QUAN LÊN LOGITS

Ngoài việc xem xét những token nào kích hoạt đặc trưng từ điển, chúng tôi điều tra cách các đặc trưng từ điển ảnh hưởng đến dự đoán đầu ra của mô hình cho token tiếp theo bằng cách loại bỏ đặc trưng khỏi luồng dư⁵. Nếu đặc trưng từ điển của chúng tôi có thể giải thích được, việc trừ giá trị của nó khỏi luồng dư sẽ có tác động logic lên dự đoán của token tiếp theo. Chúng tôi thấy trong Hình 4 (Phải) rằng hiệu ứng của việc loại bỏ đặc trưng dấu nháy đơn chủ yếu làm giảm logit cho "s" tiếp theo. Điều này phù hợp với những gì người ta mong đợi từ một đặc trưng từ điển phát hiện dấu nháy đơn và được mô hình sử dụng để dự đoán token "s" sẽ xuất hiện ngay sau dấu nháy đơn trong sở hữu cách và rút gọn như "let's".

### 5.3 ĐẶC TRƯNG TRUNG GIAN: CÁC ĐẶC TRƯNG TỪ ĐIỂN CHO PHÉP PHÁT HIỆN MẠCH TỰ ĐỘNG

Chúng tôi cũng có thể hiểu các đặc trưng từ điển liên quan đến các đặc trưng từ điển upstream và downstream: cho một đặc trưng từ điển, những đặc trưng từ điển nào trong các lớp trước đó khiến nó kích hoạt, và những đặc trưng từ điển nào trong các lớp sau đó mà nó khiến kích hoạt?

Để tự động phát hiện các đặc trưng từ điển liên quan, chúng tôi chọn một đặc trưng từ điển mục tiêu như đặc trưng của lớp 5 cho các token trong ngoặc đơn dự đoán một ngoặc đóng (Hình 5). Đối với đặc trưng từ điển mục tiêu này, chúng tôi tìm kích hoạt tối đa M của nó trên tập dữ liệu của chúng tôi, sau đó lấy mẫu 20 bối cảnh khiến đặc trưng mục tiêu kích hoạt trong phạm vi [M/2, M]. Đối với mỗi đặc trưng từ điển trong lớp trước, chúng tôi chạy lại mô hình trong khi loại bỏ đặc trưng này và sắp xếp các đặc trưng lớp trước theo mức độ mà việc loại bỏ của chúng làm giảm đặc trưng mục tiêu. Nếu muốn, chúng tôi sau đó có thể áp dụng đệ quy kỹ thuật này cho các đặc trưng từ điển trong lớp trước có tác động lớn. Kết quả của quá trình này tạo thành một cây nhân quả, như Hình 5.

Là lớp cuối cùng, vai trò của lớp 5 là đầu ra các hướng tương ứng trực tiếp với các token trong ma trận unembed. Thực tế, khi chúng tôi unembed đặc trưng 52027, các token hàng đầu đều là các biến thể ngoặc đóng. Một cách trực quan, các lớp trước sẽ phát hiện tất cả các tình huống đi trước ngoặc đóng, như ngày tháng, từ viết tắt và cụm từ.

Hình 4: Biểu đồ tần suất của số lượng token cho đặc trưng từ điển 556. (Trái) Đối với tất cả các điểm dữ liệu kích hoạt đặc trưng từ điển 556, chúng tôi hiển thị số lượng của mỗi token trong mỗi phạm vi kích hoạt. Phần lớn các kích hoạt là dấu nháy đơn, đặc biệt là đối với các kích hoạt cao hơn. Đáng chú ý, các token kích hoạt thấp hơn có khái niệm tương tự như dấu nháy đơn, chẳng hạn như dấu chấm câu khác. (Phải) Chúng tôi hiển thị những dự đoán token nào bị ức chế bằng cách loại bỏ đặc trưng, được đo lường bằng sự khác biệt trong logits giữa mô hình đã loại bỏ và không loại bỏ. Chúng tôi thấy rằng token có dự đoán giảm nhiều nhất là token "s". Lưu ý rằng có 12k logits bị ảnh hưởng tiêu cực, nhưng chúng tôi đặt ngưỡng 0.1 để rõ ràng về mặt hình ảnh.

Hình 5: Mạch cho đặc trưng từ điển ngoặc đóng, với các diễn giải của con người về mỗi đặc trưng được hiển thị. Độ dày cạnh chỉ ra sức mạnh của hiệu ứng nhân quả giữa các đặc trưng từ điển trong các lớp luồng dư liên tiếp, được đo lường bằng việc loại bỏ. Nhiều đặc trưng từ điển trên các lớp tương ứng với các đặc trưng thế giới thực tương tự và thường chỉ đến các hướng tương tự trong không gian kích hoạt, được đo lường bằng độ tương tự cosine.

## 6 THẢO LUẬN

### 6.1 CÔNG TRÌNH LIÊN QUAN

Một số lượng hạn chế các công trình trước đây đã học từ điển của các đặc trưng kích hoạt thưa thớt trong các mô hình được huấn luyện trước, bao gồm Yun et al. (2021) và Sharkey et al. (2023), công trình sau đã thúc đẩy công việc này. Tuy nhiên, các phương pháp tương tự đã được áp dụng trong các lĩnh vực khác, đặc biệt là trong việc hiểu các neuron trong vỏ não thị giác (Olshausen & Field, 2004; Wright & Ma, 2022).

Trái ngược với phương pháp của chúng tôi, nơi chúng tôi cố gắng áp đặt độ thưa thớt sau khi huấn luyện, nhiều công trình trước đây đã khuyến khích độ thưa thớt trong mạng neural thông qua các thay đổi trong kiến trúc hoặc quá trình huấn luyện. Những phương pháp này bao gồm thay đổi cơ chế attention (Correia et al., 2019), thêm phạt ℓ1 vào kích hoạt neuron (Kasioumis et al., 2021; Georgiadis, 2019), cắt tỉa neuron (Frankle & Carbin, 2018), và sử dụng hàm softmax làm phi tuyến trong các lớp MLP (Elhage et al., 2022a). Tuy nhiên, việc huấn luyện một mô hình nền tảng tiên tiến với những ràng buộc bổ sung này là khó khăn (Elhage et al., 2022a), và các cải thiện về tính giải thích không phải lúc nào cũng được thực hiện (Meister et al., 2021).

### 6.2 HẠN CHẾ VÀ CÔNG VIỆC TƯƠNG LAI

Mặc dù chúng tôi đã trình bày bằng chứng rằng các đặc trưng từ điển của chúng tôi có thể giải thích được và quan trọng về mặt nhân quả, chúng tôi không đạt được mất mát tái tạo bằng 0 (Phương trình 4), cho thấy rằng các từ điển của chúng tôi không thể nắm bắt tất cả thông tin trong kích hoạt của một lớp. Chúng tôi cũng đã xác nhận điều này bằng cách đo lường perplexity của các dự đoán của mô hình khi một lớp được thay thế bằng việc tái tạo của nó. Ví dụ, việc thay thế kích hoạt luồng dư trong lớp 2 của Pythia-70M bằng việc tái tạo các kích hoạt đó của chúng tôi làm tăng perplexity trên Pile (Gao et al., 2020) từ 25 lên 40. Để giảm thiểu mất mát thông tin này, chúng tôi muốn khám phá các kiến trúc autoencoder thưa khác và thử tối thiểu hóa thay đổi trong đầu ra mô hình khi thay thế các kích hoạt bằng các vector được tái tạo của chúng tôi, thay vì mất mát tái tạo. Các nỗ lực trong tương lai cũng có thể cố gắng cải thiện việc khám phá từ điển đặc trưng bằng cách kết hợp thông tin về trọng số của mô hình hoặc các đặc trưng từ điển được tìm thấy trong các lớp liền kề vào quá trình huấn luyện.

Các phương pháp hiện tại của chúng tôi để huấn luyện autoencoders thưa phù hợp nhất với luồng dư. Có bằng chứng rằng chúng có thể áp dụng được cho MLPs (xem Phụ lục C), nhưng pipeline huấn luyện được sử dụng để huấn luyện các từ điển trong bài báo này không thể học một cách mạnh mẽ các cơ sở hoàn chỉnh quá mức trong các lớp trung gian của MLP. Chúng tôi hào hứng với công việc tương lai điều tra những thay đổi nào có thể được thực hiện để hiểu rõ hơn các tính toán được thực hiện bởi các attention heads và lớp MLP, mỗi cái đặt ra những thách thức khác nhau.

Trong Phần 4, chúng tôi chỉ ra rằng đối với nhiệm vụ IOI, hành vi phụ thuộc vào một số lượng tương đối nhỏ các đặc trưng. Vì từ điển của chúng tôi được huấn luyện theo cách không phụ thuộc nhiệm vụ, chúng tôi mong đợi kết quả này sẽ tổng quát hóa cho các nhiệm vụ và hành vi tương tự, nhưng cần nhiều công việc hơn để xác nhận nghi ngờ này. Nếu thuộc tính này tổng quát hóa, chúng tôi sẽ có một tập hợp các đặc trưng cho phép hiểu nhiều hành vi mô hình chỉ sử dụng vài đặc trưng mỗi hành vi. Chúng tôi cũng muốn theo dõi các phụ thuộc nhân quả giữa các đặc trưng trong các lớp khác nhau, với mục tiêu tổng thể là cung cấp một lăng kính để xem các mô hình ngôn ngữ dưới đó các phụ thuộc nhân quả là thưa thớt. Điều này hy vọng sẽ là một bước hướng tới mục tiêu cuối cùng là xây dựng sự hiểu biết từ đầu đến cuối về cách một mô hình tính toán đầu ra của nó.

### 6.3 KẾT LUẬN

Autoencoders thưa là một phương pháp có thể mở rộng, không giám sát để tách các đặc trưng mạng mô hình ngôn ngữ khỏi hiện tượng chồng chất. Phương pháp của chúng tôi chỉ yêu cầu các kích hoạt mô hình không nhãn và sử dụng ít hơn các bậc độ lớn về mặt tính toán so với việc huấn luyện các mô hình gốc. Chúng tôi đã chứng minh rằng các đặc trưng từ điển mà chúng tôi học được có tính giải thích cao hơn bằng giải thích tự động, cho phép chúng tôi xác định chính xác các đặc trưng chịu trách nhiệm cho một hành vi nhất định một cách tinh vi hơn, và có tính đơn nghĩa hơn so với các phương pháp tương tự. Phương pháp này có thể tạo điều kiện cho việc lập bản đồ các mạch mô hình, chỉnh sửa mô hình có mục tiêu, và hiểu biết tốt hơn về các biểu diễn mô hình.

Một giấc mơ đầy tham vọng trong lĩnh vực giải thích là an toàn liệt kê (Elhage et al., 2022b): tạo ra một giải thích có thể hiểu được đối với con người về các tính toán của mô hình dưới dạng một danh sách đầy đủ các đặc trưng của mô hình và do đó cung cấp một đảm bảo rằng mô hình sẽ không thực hiện các hành vi nguy hiểm như lừa dối. Chúng tôi hy vọng rằng các kỹ thuật mà chúng tôi trình bày trong bài báo này cũng cung cấp một bước hướng tới việc đạt được tham vọng này.

## LỜI CẢM ƠN

Chúng tôi muốn cảm ơn Chương trình Truy cập Nhà nghiên cứu OpenAI vì khoản tài trợ tín dụng mô hình của họ cho việc giải thích tự động và CoreWeave vì đã cung cấp cho EleutherAI các tài nguyên máy tính cho dự án này. Chúng tôi cũng cảm ơn Nora Belrose, Arthur Conmy, Jake Mendel, và Nhóm Giải thích Tự động OpenAI (Jeff Wu, William Saunders, Steven Bills, Henk Tillman, và Daniel Mossing) vì những thảo luận có giá trị về thiết kế của các thí nghiệm khác nhau. Chúng tôi cảm ơn Wes Gurnee, Adam Jermyn, Stella Biderman, Leo Gao, Curtis Huebner, Scott Emmons, và William Saunders vì phản hồi của họ về các phiên bản trước của bài báo này. Cảm ơn Delta Hessler vì đã đọc lại. LR được hỗ trợ bởi Quỹ Tương lai Dài hạn. RH được hỗ trợ bởi một khoản tài trợ Open Philanthropy. HC được giúp đỡ rất nhiều bởi chương trình MATS, được tài trợ bởi AI Safety Support.

## TÀI LIỆU THAM KHẢO

Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397–2430. PMLR, 2023.

Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. URL https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html.(Date accessed: 14.05.2023), 2023.

Nick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, and Chris Olah. Curve circuits. Distill, 2021. doi: 10.23915/distill.00024.006. https://distill.pub/2020/circuits/curve-circuits.

Arthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. arXiv preprint arXiv:2304.14997, 2023.

Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2174–2184, 2019.

Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021.

Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah. Softmax linear units. Transformer Circuits Thread, 2022a. https://transformer-circuits.pub/2022/solu/index.html.

Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022b.

Nelson Elhage, Robert Lasenby, and Chris Olah. Privileged bases in the transformer residual stream, 2023. URL https://transformer-circuits.pub/2023/privileged-basis/index.html. Accessed: 2023-08-07.

Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.

Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biol. Cybern., 20(3–4):121–136, sep 1975. ISSN 0340-1200. doi: 10.1007/BF00342633. URL https://doi.org/10.1007/BF00342633.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

Georgios Georgiadis. Accelerating convolutional neural networks via activation map compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7085–7095, 2019.

Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An overview of catastrophic ai risks. arXiv preprint arXiv:2306.12001, 2023.

Theodoros Kasioumis, Joe Townsend, and Hiroya Inakoshi. Elite backprop: Training sparse interpretable neurons. In NeSy, pp. 82–93, 2021.

Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efficient sparse coding algorithms. Advances in neural information processing systems, 19, 2006.

Clara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell. Is sparse attention more interpretable? In Annual Meeting of the Association for Computational Linguistics, 2021. URL https://api.semanticscholar.org/CorpusID:235293798.

Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626, 2022.

Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 5(3):e00024–001, 2020.

Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):3311–3325, 1997.

Bruno A Olshausen and David J Field. Sparse coding of sensory inputs. Current opinion in neurobiology, 14(4):481–487, 2004.

Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu. Analysis of the optimization landscapes for overcomplete representation learning. arXiv preprint arXiv:1912.02427, 2019.

Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders, 2023. URL https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition. Accessed: 2023-05-10.

Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33:12388–12401, 2020.

Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022.

John Wright and Yi Ma. High-dimensional data analysis with low-dimensional models: Principles, computation, and applications. Cambridge University Press, 2022.

Zeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann LeCun. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. arXiv preprint arXiv:2103.15949, 2021.

## A QUY TRÌNH GIẢI THÍCH TỰ ĐỘNG

Quá trình giải thích tự động bao gồm năm bước và tạo ra cả một diễn giải và một điểm số giải thích tự động:

1. Trên mỗi dòng trong 50.000 dòng đầu tiên của OpenWebText, lấy một đoạn câu 64-token, và đo lường kích hoạt của đặc trưng trên mỗi token của đoạn này. Các kích hoạt đặc trưng được quy mô lại thành các giá trị số nguyên từ 0 đến 10.

2. Lấy 20 đoạn có điểm số kích hoạt cao nhất và chuyển 5 trong số này cho GPT-4, cùng với các kích hoạt mỗi token đã được quy mô lại. Hướng dẫn GPT-4 đề xuất một giải thích cho khi nào đặc trưng (hoặc neuron) kích hoạt, dẫn đến một diễn giải.

3. Sử dụng GPT-3.5⁶ để mô phỏng đặc trưng trên 5 đoạn kích hoạt cao khác và 5 đoạn được chọn ngẫu nhiên (với biến thiên khác không) bằng cách yêu cầu nó cung cấp các kích hoạt mỗi token.

4. Tính toán tương quan của các kích hoạt được mô phỏng và các kích hoạt thực tế. Tương quan này là điểm số giải thích tự động của đặc trưng. Các văn bản được chọn để chấm điểm một đặc trưng có thể là các đoạn văn bản ngẫu nhiên, các đoạn được chọn vì chứa kích hoạt đặc biệt cao của đặc trưng đó, hoặc một hỗn hợp đều của cả hai. Chúng tôi sử dụng một hỗn hợp của cả hai trừ khi có ghi chú khác, cũng được gọi là chấm điểm 'hàng đầu-ngẫu nhiên'.

5. Nếu, trong số 50.000 đoạn, có ít hơn 20 đoạn chứa biến thiên khác không trong kích hoạt, thì đặc trưng đó bị bỏ qua hoàn toàn.

Mặc dù việc sử dụng các đoạn ngẫu nhiên trong Bước 4 cuối cùng là tốt hơn với kích thước mẫu đủ lớn, các kích thước mẫu nhỏ tổng cộng 640 token được sử dụng để phân tích có nghĩa là một mẫu ngẫu nhiên có thể sẽ không chứa bất kỳ ví dụ kích hoạt cao nào cho tất cả trừ các đặc trưng phổ biến nhất, làm cho chấm điểm hàng đầu-ngẫu nhiên trở thành một lựa chọn thay thế mong muốn.

## B HUẤN LUYỆN AUTOENCODER THƯA VÀ LỰA CHỌN SIÊU THAM SỐ

Để huấn luyện autoencoder thưa được mô tả trong Phần 2, chúng tôi sử dụng dữ liệu từ Pile (Gao et al., 2020), một corpus webtext công cộng lớn. Chúng tôi chạy mô hình mà chúng tôi muốn giải thích trên văn bản này trong khi lưu cache và lưu các kích hoạt tại một lớp cụ thể. Những kích hoạt này sau đó tạo thành một tập dữ liệu, mà chúng tôi sử dụng để huấn luyện các autoencoders. Các autoencoders được huấn luyện với trình tối ưu Adam với tốc độ học 1e-3 và được huấn luyện trên 5-50M vector kích hoạt trong 1-3 epochs, với các từ điển lớn hơn mất nhiều thời gian hơn để hội tụ. Một lần chạy huấn luyện đơn sử dụng số lượng dữ liệu này hoàn thành trong vòng một giờ trên một GPU A40 duy nhất.

Khi thay đổi siêu tham số α kiểm soát tầm quan trọng của thuật ngữ mất mát thưa thớt, chúng tôi liên tục tìm thấy một sự đánh đổi mượt mà giữa độ thưa thớt và độ chính xác của autoencoder của chúng tôi, như được hiển thị trong Hình 6. Việc thiếu 'nhô lên' hoặc 'đầu gối' trong các biểu đồ này cung cấp một số bằng chứng rằng không có một cách duy nhất đúng đắn để phân tách không gian kích hoạt thành một cơ sở thưa, mặc dù để xác nhận điều này sẽ cần nhiều thí nghiệm bổ sung. Hình 7 hiển thị hành vi hội tụ của một tập hợp các mô hình với α khác nhau qua nhiều epochs.

Hình 6: Sự đánh đổi giữa số lượng đặc trưng kích hoạt trung bình và tỷ lệ phương sai không được giải thích cho MLP tại lớp 0.

Hình 7: Sự đánh đổi giữa độ thưa thớt và phương sai không được giải thích trong việc tái tạo của chúng tôi. Mỗi chuỗi điểm là một quét của siêu tham số α, được huấn luyện cho số epochs được đưa ra trong chú thích.

## C KẾT QUẢ GIẢI THÍCH TỰ ĐỘNG THÊM

### C.1 TÍNH GIẢI THÍCH NHẤT QUÁN TRÊN CÁC KÍCH THƯỚC TỪ ĐIỂN

Chúng tôi thấy rằng điểm số giải thích cao hơn của các từ điển đặc trưng của chúng tôi không chỉ giới hạn ở các từ điển hoàn chỉnh quá mức (nơi tỷ lệ, R, của các đặc trưng từ điển so với chiều mô hình là >1), mà còn xảy ra ngay cả trong các từ điển nhỏ hơn so với cơ sở cơ bản, như được hiển thị trong Hình 8. Những từ điển nhỏ này có thể tái tạo các vector kích hoạt kém chính xác hơn, vì vậy với mỗi đặc trưng có thể giải thích tương tự, các từ điển lớn hơn sẽ có thể giải thích nhiều phương sai tổng thể hơn.

### C.2 ĐIỂM SỐ GIẢI THÍCH CAO KHÔNG PHẢI LÀ SẢN PHẨM CỦA CHẤM ĐIỂM HÀNG ĐẦU

Một mối quan tâm có thể có là phương pháp giải thích tự động được mô tả trong Phần 3 kết hợp các đoạn kích hoạt hàng đầu (thường lớn) với các kích hoạt ngẫu nhiên (thường nhỏ), làm cho việc xác định các kích hoạt tương đối dễ dàng. Theo dẫn dắt của Bills et al. (2023), chúng tôi kiểm soát điều này bằng cách tính toán lại điểm số giải thích tự động bằng cách sửa đổi Bước 3 chỉ sử dụng các đoạn được chọn ngẫu nhiên. Với kích thước mẫu lớn, việc sử dụng các đoạn ngẫu nhiên nên là thử nghiệm thực sự về khả năng giải thích một đặc trưng tiềm năng của chúng ta. Tuy nhiên, các đặc trưng mà chúng ta đang xem xét có đuôi nặng, vì vậy với kích thước mẫu hạn chế, chúng ta nên mong đợi các mẫu ngẫu nhiên sẽ đánh giá thấp tương quan thực.

Trong Hình 9, chúng tôi hiển thị điểm số giải thích tự động cho các đoạn chỉ sử dụng các đoạn ngẫu nhiên. Phù hợp với Bills et al. (2023), chúng tôi thấy rằng điểm số chỉ ngẫu nhiên nhỏ hơn đáng kể so với điểm số hàng đầu và ngẫu nhiên, nhưng cũng rằng các đặc trưng đã học của chúng tôi vẫn liên tục vượt trội so với các đường cơ sở, đặc biệt là ở các lớp đầu. Vì các đặc trưng đã học của chúng tôi thưa hơn so với các đường cơ sở và do đó, kích hoạt ít hơn trên một đoạn nhất định, điều này có thể sẽ đánh giá thấp hiệu suất của mã hóa thưa so với các đường cơ sở.

Một mối quan tâm tiềm ẩn bổ sung là cấu trúc của các autoencoders cho phép chúng nhạy cảm với ít hơn một hướng đầy đủ trong không gian kích hoạt, dẫn đến một so sánh không công bằng. Chúng tôi hiển thị trong Phụ lục G rằng đây không phải là nguồn gốc của hiệu suất cải thiện của mã hóa thưa.

Hình 8: So sánh điểm số giải thích trung bình trên các kích thước từ điển. Tất cả các từ điển được huấn luyện trên 20M vector kích hoạt thu được bằng cách chạy Pythia-70M trên Pile với α=.00086.

Hình 9: Điểm số giải thích chỉ ngẫu nhiên trên mỗi lớp, một thước đo về mức độ tốt của việc diễn giải cụm kích hoạt hàng đầu có thể giải thích toàn bộ phạm vi kích hoạt.

Mặc dù luồng dư thường có thể được coi như một không gian vector không có cơ sở ưu tiên (một cơ sở trong đó chúng ta mong đợi các thay đổi sẽ có ý nghĩa đặc biệt bất thường, chẳng hạn như cơ sở tiêu chuẩn sau một phi tuyến trong MLP), đã được lưu ý rằng có xu hướng cho các transformer lưu trữ thông tin trong cơ sở luồng dư (Dettmers et al., 2022), được tin là do trình tối ưu Adam lưu gradient với độ chính xác hữu hạn trong cơ sở dư (Elhage et al., 2023). Chúng tôi không thấy các hướng cơ sở luồng dư có thể giải thích được hơn so với các hướng ngẫu nhiên.

### C.3 GIẢI THÍCH SUBLAYER MLP

Phương pháp của chúng tôi để học một từ điển đặc trưng và giải thích các đặc trưng kết quả, về nguyên tắc, có thể được áp dụng cho bất kỳ tập hợp kích hoạt bên trong nào của một mô hình ngôn ngữ, không chỉ luồng dư. Việc áp dụng phương pháp của chúng tôi cho sublayer MLP của transformer đã dẫn đến thành công hỗn hợp. Phương pháp của chúng tôi vẫn tìm thấy nhiều đặc trưng có thể giải thích được hơn so với các neuron. Tuy nhiên, kiến trúc của chúng tôi cũng học nhiều đặc trưng chết, không bao giờ kích hoạt trên toàn bộ corpus. Trong một số trường hợp, có quá nhiều đặc trưng chết đến mức tập hợp các đặc trưng sống không tạo thành một cơ sở hoàn chỉnh quá mức. Ví dụ, trong một từ điển có gấp đôi số đặc trưng so với neuron, có thể ít hơn một nửa đủ kích hoạt để thực hiện giải thích tự động. Các ngoại lệ cho điều này là các lớp đầu, nơi một phần lớn trong số chúng kích hoạt.

Để học các đặc trưng trong các lớp MLP, chúng tôi thấy rằng chúng tôi giữ lại một số lượng đặc trưng lớn hơn nếu chúng tôi sử dụng một ma trận khác cho encoder và decoder, để Phương trình 1 và 2 trở thành

c = ReLU(Mex + b) (5)
x̂ = MT_dc (6)

Chúng tôi hiện đang làm việc trên các phương pháp để vượt qua điều này và tìm các cơ sở thực sự hoàn chỉnh quá mức trong các lớp MLP giữa và sau.

### C.4 ĐIỂM SỐ GIẢI THÍCH TƯƠNG QUAN VỚI KURTOSIS VÀ SKEW CỦA KÍCH HOẠT

Đã được chỉ ra rằng việc tìm kiếm các từ điển thưa, hoàn chỉnh quá mức có thể được tái tạo lại dưới dạng tìm kiếm các hướng tối đa hóa chuẩn ℓ4 (Qu et al., 2019).

Chúng tôi cung cấp một thử nghiệm về tính hữu ích của điều này bằng cách phân tích tương quan giữa tính giải thích và một số thuộc tính của các hướng đã học. Chúng tôi thấy rằng có tương quan 0.19 và 0.24 giữa mức độ skew dương và kurtosis tương ứng mà các kích hoạt đặc trưng có và điểm số giải thích hàng đầu và ngẫu nhiên của chúng, như được hiển thị trong Bảng 2.

Điều này cũng phù hợp với giải thích trực quan rằng mức độ can thiệp do các đặc trưng kích hoạt khác sẽ được phân phối roughly normal theo định lý giới hạn trung tâm. Nếu đây là trường hợp, thì các đặc trưng sẽ đáng chú ý vì tính đuôi nặng của chúng.

Moment | Tương quan với điểm số giải thích hàng đầu-ngẫu nhiên
-------|------------------------------------------------
Mean | -0.09
Variance | 0.02
Skew | 0.20
Kurtosis | 0.15

Bảng 2: Tương quan của điểm số giải thích với các moment đặc trưng trên kết quả luồng dư, tất cả các lớp, với tỷ lệ kích thước từ điển R∈ {0.5,1,2,4,8}.

Điều này cũng giải thích tại sao Phân tích Thành phần Độc lập (ICA), tối đa hóa tính phi-Gaussian của các thành phần được tìm thấy, là phương pháp có hiệu suất tốt nhất trong số các lựa chọn thay thế mà chúng tôi đã xem xét.

## D PHÂN TÍCH ĐẶC TRƯNG ĐỊNH TÍNH

### D.1 CƠ SỞ LUỒNG DƯ

Hình 11 đưa ra biểu đồ tần suất kích hoạt token của cơ sở luồng dư. Kết nối chiều luồng dư này với đặc trưng dấu nháy đơn từ Hình 4, chiều dư này là chiều cao thứ 10 được đọc từ luồng dư bởi đặc trưng của chúng tôi⁷.

Hình 11: Biểu đồ tần suất của số lượng token trong cơ sở neuron. Mặc dù có một phần lớn dấu nháy đơn trong phạm vi kích hoạt trên, điều này chỉ giải thích một phần rất nhỏ của phương sai cho các phạm vi kích hoạt trung bình đến thấp hơn.

### D.2 VÍ DỤ VỀ CÁC ĐẶC TRƯNG ĐÃ HỌC

Các đặc trưng khác được hiển thị trong Hình 12, 13, 14 và 15.

### D.3 CHI TIẾT TÌM KIẾM ĐẶC TRƯNG

Chúng tôi tìm kiếm đặc trưng dấu nháy đơn bằng cách sử dụng câu " I don't know about that. It is now up to Dave"', và xem đặc trưng nào (hoặc chiều luồng dư nào) kích hoạt nhiều nhất cho token dấu nháy đơn cuối cùng. Đặc trưng kích hoạt hàng đầu trong từ điển của chúng tôi là một đặc trưng chiều ngoại lai (tức là, một hướng đặc trưng chủ yếu đọc từ một chiều ngoại lai của luồng dư), các dấu nháy đơn sau O (và dự đoán O'Brien, O'Donnell, O'Connor, O'clock, v.v.), sau đó là đặc trưng dấu nháy đơn-đi trước-s.

Đối với chiều cơ sở dư, chúng tôi tìm kiếm các chiều kích hoạt tối đa và tối thiểu (vì luồng dư có thể có cả giá trị dương và âm), nơi hai chiều dương hàng đầu là các chiều ngoại lai, hai chiều âm hàng đầu là chiều được hiển thị của chúng tôi và một chiều ngoại lai khác, tương ứng.

### D.4 CÁC PHƯƠNG PHÁP GIẢI THÍCH THẤT BẠI

Chúng tôi đã thử một phương pháp dựa trên trọng số đi từ từ điển trong lớp 4 đến từ điển trong lớp 5 bằng cách nhân một đặc trưng với MLP và kiểm tra độ tương tự cosine với các đặc trưng trong lớp 5. Không có kết nối có ý nghĩa. Ngoài ra, không rõ cách áp dụng điều này cho sublayer Attention vì chúng ta cần xem đặc trưng ở chiều vị trí nào. Chúng tôi mong đợi điều này thất bại do đi ra ngoài phân phối.

Hình 12: Đặc trưng 'If' trong bối cảnh mã hóa

Hình 13: Đặc trưng cấp token 'Dis' hiển thị bigrams, chẳng hạn như 'disCLAIM', 'disclosed', 'disordered', v.v.

Hình 14: Đặc trưng dấu nháy đơn trong bối cảnh giống "I'll".

Hình 15: Đặc trưng dấu nháy đơn trong bối cảnh giống "don't".

## E SỐ LƯỢNG ĐẶC TRƯNG KÍCH HOẠT

Trong Hình 16, chúng tôi thấy rằng, đối với luồng dư, chúng tôi liên tục học các từ điển ít nhất là 4x hoàn chỉnh quá mức trước khi một số đặc trưng bắt đầu rụng hoàn toàn, với các siêu tham số đúng. Đối với các lớp MLP, bạn thấy số lượng lớn các đặc trưng chết ngay cả với siêu tham số α= 0. Những hình này đã thông báo cho việc lựa chọn α= 8.6e−4 và α= 3.2e−4 đi vào các biểu đồ trong Phần 3 cho luồng dư và MLP tương ứng. Do phần lớn không gian đầu vào không bao giờ được sử dụng do phi tuyến, các đặc trưng từ điển MLP dễ dàng bị kẹt ở vị trí mà chúng hầu như không bao giờ kích hoạt. Trong tương lai, chúng tôi dự định khởi tạo lại những 'đặc trưng chết' như vậy để đảm bảo rằng chúng tôi học được nhiều đặc trưng từ điển hữu ích nhất có thể.

Hình 16: Số lượng đặc trưng kích hoạt, được định nghĩa là kích hoạt hơn 10 lần trên 10M điểm dữ liệu, thay đổi với siêu tham số thưa thớt α và tỷ lệ kích thước từ điển R.

## F CHỈNH SỬA HÀNH VI IOI TRÊN CÁC LỚP KHÁC

Trong Hình 17, chúng tôi hiển thị kết quả của quy trình trong Phần 4 trên một loạt các lớp trong Pythia-410M.

Hình 17: Phân kỳ từ đầu ra mục tiêu so với số lượng đặc trưng được vá và độ lớn của các chỉnh sửa cho các lớp 3, 7, 11, 15, 19 và 23 của luồng dư của Pythia-410M. Pythia-410M có 24 lớp, mà chúng tôi đánh chỉ số 0, 1, ..., 23.

## G SO SÁNH TOPK

Như đã đề cập trong Phần 3, các hướng so sánh được học bởi mã hóa thưa và những hướng trong các đường cơ sở không hoàn toàn đồng đều. Điều này là do, ví dụ, một hướng PCA kích hoạt đến toàn bộ nửa không gian ở một bên của siêu phẳng qua gốc tọa độ, trong khi một đặc trưng mã hóa thưa kích hoạt trên ít hơn một hướng đầy đủ, chỉ ở phía xa của một siêu phẳng không cắt gốc tọa độ. Điều này là do bias được áp dụng trước kích hoạt, trong thực tế, luôn luôn âm.

Để kiểm tra liệu sự khác biệt này có chịu trách nhiệm cho điểm số cao hơn hay không, chúng tôi chạy một biến thể của PCA và ICA trong đó chúng tôi có một số lượng cố định các hướng, K, có thể kích hoạt cho bất kỳ điểm dữ liệu đơn nào. Chúng tôi đặt K này bằng số lượng đặc trưng kích hoạt trung bình cho một từ điển mã hóa thưa với tỷ lệ R= 1 và α= 8.6e−4 được huấn luyện trên lớp được đề cập. Chúng tôi so sánh kết quả trong Hình 18, cho thấy rằng thay đổi này không giải thích được nhiều hơn một phần nhỏ của sự cải thiện trong điểm số.

Hình 18: Điểm số giải thích tự động trên các lớp cho luồng dư, bao gồm các đường cơ sở top-K cho ICA và PCA.

⁶Trong khi quá trình được mô tả trong Bills et al. (2023) sử dụng GPT-4 cho bước mô phỏng, chúng tôi sử dụng GPT-3.5. Điều này là do giao thức mô phỏng yêu cầu logprobs của mô hình để chấm điểm, và API công cộng của OpenAI cho GPT-3.5 (nhưng không phải GPT-4) hỗ trợ trả về logprobs.

¹Chúng tôi chủ yếu nghiên cứu luồng dư trong Pythia-70M và Pythia 410-M, với luồng dư có kích thước din= 512 và din= 1024, tương ứng (Biderman et al., 2023)

²Chúng tôi sử dụng trọng số liên kết vì (a) chúng mã hóa kỳ vọng của chúng tôi rằng các hướng phát hiện và định nghĩa đặc trưng nên giống nhau hoặc rất tương tự, (b) chúng giảm một nửa chi phí bộ nhớ của mô hình, và (c) chúng loại bỏ sự mơ hồ về việc liệu hướng đã học nên được giải thích như hướng encoder hay decoder. Chúng không làm giảm hiệu suất khi huấn luyện trên dữ liệu luồng dư nhưng chúng tôi đã quan sát một số giảm hiệu suất khi sử dụng dữ liệu MLP.

³Việc chuẩn hóa các hàng (đặc trưng từ điển) ngăn mô hình giảm thuật ngữ mất mát thưa thớt ||c||1 bằng cách tăng kích thước của các vector đặc trưng trong M.

⁴Đối với PCA, chúng tôi sử dụng phương pháp ước tính trực tuyến và chạy phân tách trên cùng một lượng dữ liệu mà chúng tôi đã sử dụng để huấn luyện các autoencoders. Đối với ICA, do thời gian hội tụ chậm hơn, chúng tôi chạy chỉ trên 2GB dữ liệu, khoảng 4 triệu kích hoạt cho luồng dư và 1m kích hoạt cho MLPs.

⁵Cụ thể, chúng tôi sử dụng ablation ít-hơn-hạng-một, nơi chúng tôi hạ thấp vector kích hoạt theo hướng của đặc trưng chỉ đến điểm mà đặc trưng không còn kích hoạt.

⁷Chín đặc trưng đầu tiên không có dấu nháy đơn trong các kích hoạt hàng đầu của chúng như chiều 21.
