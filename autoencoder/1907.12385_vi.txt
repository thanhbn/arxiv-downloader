# 1907.12385.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/autoencoder/1907.12385.pdf
# Kích thước tệp: 3847400 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Phân Tích Nhân Tố Không Gian Tiềm Ẩn và Thao Tác thông qua Phép Chiếu Ma Trận Con
Xiao Li1 2Chenghua Lin2Ruizhe Li2Chaozheng Wang1Frank Guerin3

Tóm tắt
Chúng tôi giải quyết vấn đề tách biệt không gian tiềm ẩn của một bộ tự mã hóa để tách thông tin thuộc tính được gán nhãn khỏi thông tin đặc trưng khác. Điều này sau đó cho phép chúng ta thay đổi các thuộc tính được chọn trong khi bảo toàn thông tin khác. Phương pháp của chúng tôi, phép chiếu ma trận con, đơn giản hơn nhiều so với các cách tiếp cận trước đây để phân tích nhân tố không gian tiềm ẩn, ví dụ không yêu cầu nhiều bộ phân biệt hoặc cân nhắc cẩn thận giữa các hàm mất mát của chúng. Hơn nữa, mô hình mới của chúng tôi có thể được áp dụng cho các bộ tự mã hóa như một plugin, và hoạt động trên các lĩnh vực đa dạng như hình ảnh hoặc văn bản. Chúng tôi chứng minh tính hữu ích của phương pháp cho việc thao tác thuộc tính trong các bộ tự mã hóa được huấn luyện trên các lĩnh vực khác nhau, sử dụng cả đánh giá của con người và các phương pháp tự động. Chất lượng tạo sinh của mô hình mới (ví dụ: tái tạo, tạo sinh có điều kiện) có tính cạnh tranh cao với một số đường cơ sở mạnh.

1. Giới thiệu
Chúng tôi nghiên cứu vấn đề thao tác nhiều thuộc tính của các mẫu dữ liệu. Điều này có thể được áp dụng cho dữ liệu hình ảnh, ví dụ để thao tác một bức ảnh khuôn mặt để thêm râu, thay đổi giới tính, hoặc tuổi. Nó cũng có thể được áp dụng cho văn bản, ví dụ để thay đổi phong cách hoặc cảm xúc của một văn bản. Chúng tôi giả định rằng chúng ta có một tập dữ liệu huấn luyện trong đó các thuộc tính được gán nhãn. Tuy nhiên có một khía cạnh không có giám sát vì chúng ta không có các mẫu của cùng một cá thể với các kết hợp thuộc tính khác nhau, ví dụ, cùng một người có và không có râu. Hơn nữa, các mẫu huấn luyện có một số kết hợp thuộc tính có tương quan cao, trong khi các kết hợp khác hoàn toàn vắng mặt; ví dụ, trong tập dữ liệu CelebA tóc vàng và hoa tai có tương quan cao với nữ giới (Torfason et al., 2016), trong khi một phụ nữ có râu thì vắng mặt. Tuy nhiên, chúng tôi muốn hệ thống của mình có thể tách biệt các yếu tố giải thích trong không gian pixel, để hiểu, ví dụ, tóc vàng chỉ tương ứng với những thay đổi màu sắc đối với các pixel tóc, và không có thay đổi nào khác ở nơi khác trên khuôn mặt.

Thách thức tách biệt nhiều yếu tố giải thích này đặt ra những vấn đề thú vị cho các mô hình tạo sinh. Trong hình ảnh khuôn mặt chẳng hạn, ngay cả khi dữ liệu huấn luyện không có phụ nữ có râu, một mô hình tạo sinh tốt nên có thể 'tưởng tượng' các ví dụ mới kết hợp các thuộc tính theo cách không có trong dữ liệu huấn luyện. Như được ghi nhận bởi Higgins et al. (2016) "Các mô hình không thể tổng quát hóa cho dữ liệu bên ngoài vỏ lồi của phân phối huấn luyện ... trừ khi chúng học về các yếu tố tạo sinh dữ liệu và tái kết hợp chúng theo những cách mới." Lý tưởng nhất, chúng ta nên tách biệt và cô lập hoàn toàn các yếu tố tạo sinh dữ liệu, để chúng ta có thể biểu diễn các yếu tố tạo sinh của một mẫu bằng một vector có một phần thông tin thuộc tính được gán nhãn, và một phần khác với thông tin đặc trưng khác của mẫu. Điều này theo một cách nhỏ là một phần của xu hướng chung cố gắng đưa nghiên cứu mạng nơ-ron sâu hướng tới các mô hình giải thích về thế giới (LeCun, 2013; Lake et al., 2016; Yuille & Liu, 2018), điều này đòi hỏi sự tách biệt. Vấn đề này quan trọng vì việc cô lập các yếu tố giải thích là một cách để vượt qua sự bùng nổ tổ hợp của các ví dụ huấn luyện cần thiết nếu các yếu tố như vậy không được cô lập (Yuille & Liu, 2018).

Một cách tiếp cận điển hình cho vấn đề sử dụng một bộ tự mã hóa (AE) mã hóa một đầu vào cho trước (ví dụ: hình ảnh, văn bản, v.v.) thành một vector tiềm ẩn, và sau đó khôi phục (giải mã) vector tiềm ẩn về đầu vào cho trước (Lample et al., 2017; Hu et al., 2017; Xiao et al., 2018; Li et al., 2019). Vector tiềm ẩn chứa thông tin thuộc tính cũng như thông tin đặc trưng khác của đầu vào. Nếu có thể thay đổi thông tin thuộc tính trong không gian tiềm ẩn, thì có thể tạo ra các ví dụ với các thuộc tính đã thay đổi. Khó khăn ở đây là hai mặt: (1) học một biểu diễn không gian tiềm ẩn tách các thuộc tính khỏi tất cả thông tin đặc trưng khác, và (2) tách biệt hoàn toàn các thuộc tính. Nếu chúng ta thất bại trong phần tách biệt, thì nỗ lực tạo sinh với các thuộc tính cụ thể có thể xung đột với thông tin khác trong không gian tiềm ẩn (như trong Kingma et al. (2014) v.v., xem x2). Nếu chúng ta thất bại trong phần thứ hai thì các ví dụ được tạo sinh với các thuộc tính được chỉ định cũng sẽ bị nhiễm với các thuộc tính giả (xem Hình 1 Trái).

Nhiều cách tiếp cận gần đây sử dụng các cấu trúc mạng nơ-ron phụ với huấn luyện đối kháng theo phong cách của Mạng Đối Kháng Tạo Sinh (GANs). Các mạng mới này có thể được sử dụng để loại bỏ thông tin thuộc tính khỏi không gian tiềm ẩn (Lample et al., 2017), hoặc để phản hồi một thuật ngữ mất mát để áp đặt các thuộc tính họ muốn xuất hiện trong đầu ra (He et al., 2019). Các cách tiếp cận đối kháng này có các thuật ngữ mất mát cạnh tranh (ví dụ: mất mát tái tạo, mất mát phân loại thuộc tính, mất mát đầu ra thực tế), và yêu cầu lựa chọn cẩn thận các siêu tham số để cân bằng các hàm mất mát này. Trong trường hợp của Lample et al. (2017), một mất mát tăng dần chậm là rất quan trọng. Các siêu tham số và lịch trình huấn luyện này phải được xác định bằng thử và sai, để tránh bất ổn định trong huấn luyện. Ngay cả sau khi huấn luyện thành công, chúng tôi đã phát hiện rằng một số mô hình bỏ qua các thuộc tính mong muốn và đặt quá nhiều trọng lượng vào tái tạo và đầu ra thực tế (xem x4). Điều này một phần vì chúng ta đẩy các hệ thống vào cài đặt rất khó khăn của việc huấn luyện cho nhiều thuộc tính cùng nhau (ví dụ: 40 thuộc tính cho CelebA). Đây là một cài đặt rất khắt khe cho việc tách biệt, ví dụ: để tách biệt son môi, trang điểm, và tóc vàng khỏi nữ giới, và để tách biệt râu, lông mày rậm, và bóng râu 5 giờ khỏi nam giới.

Chúng tôi đề xuất một phương pháp đơn giản và tổng quát, Phép Chiếu Ma Trận Con (MSP), trực tiếp tách thông tin thuộc tính khỏi tất cả thông tin không thuộc tính khác, mà không dựa vào việc cân bằng các thuật ngữ mất mát từ các mạng nơ-ron phụ. Các biến của chúng tôi đại diện cho các thuộc tính được tách biệt hoàn toàn, với một biến cô lập cho mỗi thuộc tính của tập huấn luyện. Do đó, khi chúng ta thực hiện tạo sinh có điều kiện, chúng ta có thể gán các thuộc tính thuần túy kết hợp với dữ liệu tiềm ẩn khác không xung đột, để các hình ảnh được tạo sinh có chất lượng cao và không bị nhiễm với các thuộc tính giả. Trong khi đó, mô hình của chúng tôi là một plugin phổ quát. Về lý thuyết, nó có thể được áp dụng cho bất kỳ AE hiện có nào (nếu và chỉ nếu các AE sử dụng một vector tiềm ẩn). Nếu AE là một mô hình tạo sinh (như VAE), với cách tiếp cận của chúng tôi, nó trở thành một mô hình tạo sinh có điều kiện tạo nội dung dựa trên các ràng buộc điều kiện cho trước. Trong trường hợp hình ảnh, chúng tôi thêm một PatchGAN ở cuối bộ tạo sinh của chúng ta để làm sắc nét hình ảnh, nhưng điều này không kết nối với nhiệm vụ thao tác thuộc tính và không phải là cốt lõi của mô hình; nó có thể được thay thế bằng bất kỳ phương pháp siêu phân giải và làm sắc nét nào.

Plugin của chúng tôi có hai cách sử dụng: (1) các mẫu có thể được tạo sinh từ một hạt giống ngẫu nhiên, nhưng với các thuộc tính cho trước; (2) một mẫu cho trước có thể được sửa đổi để có các thuộc tính mong muốn được chỉ định. Các đóng góp chính của chúng tôi là: (1) Một plugin đơn giản và phổ quát cho tạo sinh có điều kiện và thay thế nội dung, có thể áp dụng trực tiếp cho bất kỳ kiến trúc AE nào (ví dụ: hình ảnh hoặc văn bản). (2) Hiệu suất mạnh trong việc học các biểu diễn tiềm ẩn tách biệt của nhiều (ví dụ: 40) thuộc tính. (3) Một chiến lược cân bằng có nguyên tắc để kết hợp các thuật ngữ mất mát cho huấn luyện. Mã cho mô hình của chúng tôi có sẵn trực tuyến1.

2. Công trình liên quan
Các cách tiếp cận đầu tiên để kiểm soát việc tạo sinh theo thuộc tính (VAE có điều kiện (Kingma et al., 2014; Sohn et al., 2015; Yan et al., 2016)) đơn giản thêm thông tin thuộc tính như một đầu vào bổ sung cho bộ mã hóa hoặc bộ giải mã. Các cách tiếp cận này tạo sinh sử dụng một vector tiềm ẩn z và cũng một vector thuộc tính y, trong đó z thường xung đột với y, vì thông tin thuộc tính chưa được loại bỏ khỏi z. Với các đầu vào xung đột, điều tốt nhất mà VAE có thể làm là tạo ra một hình ảnh mờ.

Mạng Đối Kháng Tạo Sinh (GANs) có thể được tăng cường với các bộ mã hóa. IcGAN huấn luyện các bộ mã hóa riêng biệt cho các vector y và z, nhưng không cố gắng loại bỏ thông tin có thể xung đột (Perarnau et al., 2016). Các tác giả IcGAN cũng lưu ý rằng nó có thể thất bại trong việc tạo ra các kết hợp thuộc tính bất thường như một phụ nữ có ria mép, vì bộ phân biệt GAN ngăn cản bộ tạo sinh tạo ra các mẫu bên ngoài phân phối huấn luyện.

Công trình gần đây hơn đã giải quyết vấn đề tách thông tin thuộc tính khỏi vector tiềm ẩn, sử dụng một mạng phụ mới (như một bộ phân biệt GAN) (Lample et al., 2017; Creswell et al., 2017; Klys et al., 2018), cố gắng đoán thuộc tính của vector tiềm ẩn z, và phạt bộ tạo sinh nếu thông tin thuộc tính vẫn còn. Một nhược điểm đáng kể của các cách tiếp cận đối kháng này là phải rất cẩn thận trong huấn luyện để mất mát từ bộ phân biệt (đang cố gắng loại bỏ thông tin thuộc tính) không làm xáo trộn việc huấn luyện để tạo ra một tái tạo tốt. Trong trường hợp của mạng Fader (Lample et al., 2017), cần thiết phải bắt đầu với trọng số mất mát phân biệt là không, và tăng tuyến tính lên 0.0001 trong 500.000 lần lặp đầu tiên; các tác giả nói "Lịch trình này hóa ra rất quan trọng trong các thí nghiệm của chúng tôi. Không có nó, chúng tôi quan sát thấy rằng bộ mã hóa bị ảnh hưởng quá nhiều bởi mất mát từ bộ phân biệt, ngay cả với các giá trị thấp của [hệ số mất mát]."

Mặc dù cách tiếp cận đối kháng này có thể loại bỏ thành công thông tin thuộc tính khỏi z, không có gì ngăn cản bộ giải mã (bộ tạo sinh) liên kết thông tin giả khác với thuộc tính. Ví dụ, bộ giải mã có thể liên kết thuộc tính dự định cho 'kính' với một khuôn mặt già hơn hoặc nam tính hơn. Đây là điều chúng ta thấy trong kết quả của hai trong số các cách tiếp cận đối kháng (xem Hình 2).

Hầu hết các kết quả trong Creswell et al. (2017) tập trung vào thuộc tính 'mỉm cười' (không được tái tạo ở đây), và điều này được tách biệt rất tốt. Chỉ khi tập dữ liệu huấn luyện liên kết các thuộc tính khác với thuộc tính được huấn luyện thì sự vướng víu mới phát sinh. Trong Creswell et al. (2017), vector thuộc tính là một biến nhị phân duy nhất để hệ thống chỉ có thể được huấn luyện để kiểm soát (hoặc phân loại) một thuộc tính. Không bất ngờ khi một bộ tạo sinh sẽ liên kết thông tin giả với một thuộc tính nếu sự liên kết có mặt trong dữ liệu huấn luyện và hệ thống đã được huấn luyện chỉ trên các ví dụ gán nhãn một thuộc tính duy nhất, ví dụ: kính. Hệ thống không thể biết rằng nó nên cô lập 'đeo kính', chứ không phải 'đeo kính và già hơn'. Mạng Fader (Lample et al., 2017) có thể huấn luyện cho nhiều thuộc tính cùng nhau, tuy nhiên He et al. (2019) tuyên bố rằng "Mặc dù Mạng Fader có khả năng chỉnh sửa nhiều thuộc tính với một mô hình, trong thực tế, cài đặt nhiều thuộc tính làm cho kết quả bị mờ."

Các công trình gần đây nhất (2018-19) dựa trên GAN. Chúng không cố gắng loại bỏ thông tin thuộc tính khỏi không gian tiềm ẩn, mà thay vào đó thêm một bộ phân loại thuộc tính bổ sung sau khi tạo sinh, và áp đặt một mất mát phân loại thuộc tính. Điều này là ngoài một bộ phân biệt GAN điển hình cho hình ảnh thực tế. AttGAN (He et al., 2019) sử dụng một bộ mã hóa, bộ giải mã (bộ tạo sinh), và bộ phân loại thuộc tính và bộ phân biệt được áp dụng cho đầu ra của bộ tạo sinh. StarGAN (Choi et al., 2018) và RelGAN (Wu et al., 2019) không sử dụng bộ mã hóa, nhưng sử dụng một bộ tạo sinh duy nhất hai lần, trong một chu kỳ; hướng đầu tiên thay đổi thuộc tính như một GAN có điều kiện, hướng thứ hai cố gắng tái tạo hình ảnh (sử dụng thuộc tính gốc), và do đó yêu cầu thông tin không thuộc tính được bảo toàn. StarGAN sử dụng một bộ phân biệt và bộ phân loại thuộc tính, như AttGAN, trong khi RelGAN thêm một mạng thứ ba cho nội suy.

Tất cả các công trình được trích dẫn từ 2017 đến 2019 đều có một thành phần đối kháng (theo phong cách của GAN); chúng huấn luyện các bộ phân loại phụ để phản hồi các thuật ngữ mất mát, để đảm bảo chúng loại bỏ các thuộc tính không mong muốn, hoặc thực thi các thuộc tính mong muốn. Chúng cần một cân bằng cẩn thận giữa các thuật ngữ mất mát, nhưng không có phương pháp có nguyên tắc nào để xác định các siêu tham số cân bằng này. Công trình của chúng tôi không dựa vào một thành phần đối kháng để thao tác thuộc tính; chúng tôi sử dụng một phương pháp trực tiếp hơn về phép chiếu ma trận lên các không gian con, để phân tích nhân tố biểu diễn tiềm ẩn và tách các thuộc tính khỏi thông tin khác. Hơn nữa, không giống như các công trình trên2, chúng tôi không sử dụng bất kỳ kết nối bỏ qua nào. Kết nối bỏ qua có thể gây ra lỗi khi một vùng của hình ảnh nguồn và đích khá khác nhau, chúng tôi minh họa điều này thêm trong Hình 1 Phải.

Ngoài các công trình trên sử dụng thuộc tính được gán nhãn, cũng có công trình về vấn đề khó khăn hơn là học không giám sát các yếu tố tạo sinh tách biệt của dữ liệu (Chen et al., 2016; Higgins et al., 2017; Kumar et al., 2018). Tuy nhiên, các cách tiếp cận có giám sát (được gán nhãn) tạo ra các mẫu rõ ràng hơn nhiều của các thuộc tính được chọn, và sự tách biệt vượt trội. Một cách tiếp cận thay thế cho việc tạo sinh có kiểm soát là đơn giản huấn luyện một mạng nơ-ron tích chập sâu và thực hiện nội suy tuyến tính trong không gian đặc trưng sâu (Upchurch et al., 2017). Điều này cho thấy kết quả tốt một cách đáng ngạc nhiên, nhưng trong việc thay đổi một thuộc tính mà chỉ nên ảnh hưởng đến một vùng cục bộ, nó có thể ảnh hưởng đến nhiều vùng hình ảnh hơn, và có thể tạo ra kết quả không thực tế cho các tư thế khuôn mặt hiếm hơn.

3. Phương pháp
3.1. Công thức hóa vấn đề
Chúng tôi quan tâm đến việc phân tích nhân tố và thao tác nhiều thuộc tính từ một biểu diễn tiềm ẩn được học bởi một Bộ tự mã hóa (AE) tùy ý. Giả sử chúng ta được cho một tập dữ liệu D của các phần tử (x;y) với x ∈ Rn và y ∈ Y = {0;1}k đại diện cho k thuộc tính của x.

Gọi một AE tùy ý được biểu diễn bởi z = F(x) và x' = G(z), trong đó F() là bộ mã hóa, G() là bộ giải mã, z là vector tiềm ẩn mã hóa x, và x' là việc tái tạo của x (xem Hình 3). Lưu ý rằng khi x' là một xấp xỉ tốt của x (tức là, x' ≈ x), thông tin thuộc tính của x được biểu diễn trong y cũng sẽ được thu nhận trong mã hóa tiềm ẩn z. Thao tác thuộc tính có nghĩa là chúng ta thay thế các thuộc tính y được thu nhận bởi z bằng các thuộc tính mới yn. Gọi K() là một hàm thay thế, thì chúng ta có không gian tiềm ẩn mới zn = K(z;yn) và xn = G(K(z;yn)), trong đó thông tin thuộc tính được mã hóa trong yn có thể được dự đoán từ xn và thông tin không thuộc tính của x sẽ được bảo toàn.

Để đưa ra một ví dụ cụ thể, cho một hình ảnh khuôn mặt, x, chúng ta muốn thao tác x đối với sự có mặt hoặc vắng mặt của một tập các thuộc tính mong muốn được mã hóa trong yn (ví dụ: một khuôn mặt có hoặc không có nụ cười, đeo hoặc không đeo kính), tạo ra hình ảnh được thao tác xn, mà không thay đổi danh tính của khuôn mặt (tức là, bảo toàn thông tin không thuộc tính của x).

3.2. Học Biểu diễn Tiềm ẩn Tách biệt thông qua Phép Chiếu Ma Trận Con
Để giải quyết vấn đề được công thức hóa trong x3:1, chúng tôi đề xuất một phương pháp tổng quát để tách thông tin về thuộc tính y từ z dựa trên ý tưởng thực hiện phép chiếu ma trận trực giao lên các không gian con. Mô hình của chúng tôi hoạt động như một plugin phổ quát và về lý thuyết, nó có thể được áp dụng cho bất kỳ AE hiện có nào.

Kiến trúc tổng quát của mô hình MSP được đề xuất được mô tả trong Hình 3 (a). Cho một vector tiềm ẩn z mã hóa x và một hàm khả nghịch phức tạp tùy ý H(), H() biến đổi z thành một không gian tuyến tính mới (ẑ = H(z)) sao cho có thể tìm thấy một ma trận M trong đó (a) phép chiếu của ẑ lên M (ký hiệu là ŷ) tiến gần đến y (tức là, ŷ thu nhận thông tin thuộc tính),

Mẑ = ŷ; ŷ → y (1)

và (b) có một ma trận trực giao U[M;N], trong đó N là không gian null của M (tức là, M ⊥ N) và phép chiếu của ẑ lên N (ký hiệu là ŝ) thu nhận thông tin không thuộc tính. Vì U là trực giao, chúng ta cũng có UT ≡ U-1.

Hình 3 (b) trình bày một kiến trúc đơn giản hóa của mô hình MSP, tương đương với kiến trúc tổng quát. Sự đơn giản hóa này tồn tại vì như đã giải thích trước đó H() là khả nghịch. Vì vậy, khi bộ mã hóa và bộ giải mã của một AE có đủ dung lượng, chúng về cơ bản có thể hấp thụ H và H-1. Nói cách khác, thay vì khớp F và G, bộ mã hóa và bộ giải mã sẽ khớp H◦F() và G◦H-1() thay thế. Vì M bản thân nó là một ma trận trực giao không đầy đủ, các phép toán tương tự không thể được áp dụng cho M.

Mục tiêu học chính của chúng tôi, ngoài mục tiêu AE ban đầu (tức là mất mát tái tạo LAE; xem x3.3 và Eq. 8), sau đó là ước lượng M điều này không tầm thường. Chúng tôi biến vấn đề tìm một giải pháp tối ưu cho M thành một vấn đề tối ưu hóa, trong đó chúng ta cần (1) ép buộc ŷ càng gần y (tức là, vector mã hóa các thuộc tính thực tế) càng tốt; và (2) tối thiểu hóa ||ŝ||2 để ŝ chứa càng ít thông tin từ ẑ càng tốt. Điều này có thể được công thức hóa thành hàm mất mát sau

LMSP = L1 + L2 (2)
L1 = ||ŷ - y||2 = ||Mẑ - y||2 (3)
L2 = ||ŝ||2 (4)

Ở đây L1 và L2 mã hóa hai ràng buộc trên, tương ứng, và ŷ là các thuộc tính được dự đoán. Cho rằng AE dựa vào thông tin của ẑ để tái tạo x, các ràng buộc tối ưu hóa của LAE và L2 về cơ bản giới thiệu một quá trình đối kháng: một mặt, nó ngăn cản bất kỳ thông tin nào của ẑ được lưu trữ trong ŝ do hình phạt từ L2; mặt khác, AE yêu cầu thông tin từ ẑ để tái tạo x. Vì vậy, giải pháp tốt nhất là chỉ khôi phục thông tin cần thiết cho việc tái tạo (ngoại trừ thông tin thuộc tính) trong ŝ. Bằng cách tối ưu hóa LMSP, chúng ta khiến ẑ được phân tích nhân tố, với thông tin thuộc tính được lưu trữ trong ŷ, trong khi ŝ chỉ giữ lại thông tin không thuộc tính.

Phần đầu tiên của hàm mất mát L1 tương đối đơn giản. Trở ngại chính ở đây là tính toán L2 vì ŝ không được biết. Chúng tôi phát triển một chiến lược để tính toán ||ŝ||2 gián tiếp. Theo định nghĩa của ŷ và ŝ, chúng ta có thể suy ra:

L2 = ||ŝ||2 = ||ŝ - 0||2
= ||[ŷ; ŝ] - [ŷ; 0]||2  Biến đổi đồng nhất
= ||Uẑ - [ŷ; 0]||2 (5)

trong đó dấu ngoặc vuông biểu diễn phép nối vector. Vì U là trực giao, chúng ta có

L2 = ||Uẑ - [ŷ; 0]||2
= ||U-1(Uẑ - [ŷ; 0])||2
= ||ẑ - U-1[ŷ; 0]||2 = ||ẑ - UT[ŷ; 0]||2
= ||ẑ - [M; N]T[ŷ; 0]||2
= ||ẑ - MTŷ||2 ≈ ||ẑ - MTy||2 (6)

Với Eq. 6 (sử dụng các tính chất của ma trận trực giao), chúng tôi tránh tính toán ŝ và N trực tiếp khi tối thiểu hóa ||ŝ||2, và biến vấn đề tối thiểu hóa thành tối ưu hóa M thay thế. Cuối cùng, chúng ta có:

LMSP = L1 + L2
= ||Mẑ - y||2 + ||ẑ - MTŷ||2
≈ ||Mẑ - y||2 + ||ẑ - MTy||2 (7)

Hàm mất mát trong Eq. 7 cũng đảm bảo rằng sau khi huấn luyện, giải pháp cho M sẽ là một phần của ma trận trực giao U (xem x4.5). Khi LMSP nhỏ, chuyển vị của M trở thành nghịch đảo của M.

3.3. Áp dụng Khung Phép Chiếu Ma Trận Con cho một AE
Để áp dụng khung phép chiếu ma trận con (MSP) của chúng tôi cho một AE hiện có, chỉ cần suy ra một hàm mất mát cuối cùng bằng cách kết hợp mất mát của AE và mất mát của khung MSP.

L = LAE + λLMSP (8)

trong đó λ là trọng số cho LMSP. Như được minh họa trong Hình 3 (a) và (b), cần lưu ý rằng việc áp dụng khung của chúng tôi sẽ không thay đổi cấu trúc của AE, trong đó thành phần MSP của chúng tôi chỉ đơn giản lấy vector tiềm ẩn ẑ của AE làm đầu vào. LAE hy vọng rằng ŝ có thể lưu trữ nhiều thông tin hơn để tái tạo x, nhưng LMSP muốn ŝ chứa ít thông tin hơn. Khi λ nhỏ, mô hình trở thành một AE tiêu chuẩn. Khi λ quá lớn, thông tin không thuộc tính trong ẑ giảm quá mức, dẫn đến các sản phẩm được tạo sinh có xu hướng trung bình của các mẫu huấn luyện. Do đó, một thách thức khác mà chúng ta phải đối mặt là cách thiết lập λ một cách thích hợp.

Chúng tôi đề xuất một chiến lược có nguyên tắc để xác định hiệu quả giá trị của λ (chiến lược này được sử dụng trong tất cả các thí nghiệm trong bài báo này). Vì LAE và LMSP về cơ bản đại diện cho một mối quan hệ cạnh tranh cho tài nguyên ẑ, chúng tôi chỉ định rằng LAE và LMSP có cùng ảnh hưởng đến việc cập nhật ŝ. Chúng tôi sử dụng α để biểu diễn "cường độ" mà AE cập nhật ẑ trong mỗi quá trình lan truyền ngược. Cường độ này phụ thuộc vào cấu trúc của mô hình và hàm mất mát được sử dụng bởi mô hình. Ví dụ, giả sử một AE (để tạo sinh hình ảnh) sử dụng một bộ giải mã CNN và L2-loss. Trong quá trình huấn luyện, lỗi của mỗi pixel giữa hình ảnh được tạo sinh và hình ảnh thực được lan truyền ngược đến ẑ như gradient của ẑ. Tổng của các gradient này là gradient cuối cùng của ẑ (tức là, tương ứng với LAE). Đối với một hình ảnh có h×w pixel và c kênh màu, có h×w×c phần của gradient được tích lũy, vì vậy cường độ là h×w×c.

Cường độ của ŷ để cập nhật ẑ là tổng số lượng thuộc tính (tức là kích thước của ŷ), vì lỗi cho mỗi thuộc tính được lan truyền ngược đến ẑ và tích lũy (tức là, tương ứng với LMSP). Do đó, để cân bằng ảnh hưởng của LAE và LMSP đối với việc cập nhật ẑ trong quá trình huấn luyện, chúng ta có:

λ = h×w×c / (size(attribute) + size(ẑ)) (9)

Khi sử dụng cross-entropy-loss (hoặc NLL loss v.v.), thường dành cho các mô hình tạo sinh văn bản (ví dụ: mô hình seq2seq), mỗi từ được tạo sinh chỉ tạo ra một cường độ, bất kể kích thước nhúng từ. Trong khi đó, các giá trị mất mát được trả về bởi cross-entropy-loss tỷ lệ thuận với lỗi, nhưng các giá trị mất mát được trả về bởi mất mát MSP (là một MSE loss) tỷ lệ thuận với bình phương của lỗi. Do đó, đối với một câu có độ dài k, cường độ của toàn bộ câu đến ẑ là k2, để cho cross-entropy-loss,

λ = k2 / (size(attribute) + size(ẑ)) (10)

3.4. Thay thế Nội dung và Tạo sinh Có điều kiện
Sau khi MSP được huấn luyện (tức là, M được ước lượng), có hai cách để thực hiện thay thế nội dung hoặc thay đổi thuộc tính. Một cách là suy ra ma trận trực giao U = [M;N] bằng cách giải không gian null N của M (tức là, không gian null được cấu thành từ tất cả các nghiệm cụ thể cho n đối với phương trình Mn = 0, trong đó n là một biến độc lập). Cho một đầu vào x, chúng ta trước tiên mã hóa nó thành ẑ. Sau đó chúng ta sử dụng U để có được vector thuộc tính ŷ của x và vector thông tin không thuộc tính ŝ như sau.

[ŷ;ŝ] = [M;N]ẑ = Uẑ = U◦encoder(x) (11)

Tại thời điểm này, chúng ta có thể trực tiếp thay thế [ŷ;ŝ] bằng [yn;ŝ], trong đó yn là vector thuộc tính mới. Với [yn;ŝ] và UT (lưu ý rằng UT tiến gần đến U-1 trong quá trình huấn luyện), chúng ta có thể suy ra mã tiềm ẩn mới zn và sau đó giải mã nó thành xn, cuối cùng thu nhận các thuộc tính mới mong muốn.

xn = decoder(zn) = decoder(UT[yn;ŝ]) (12)

Thay vào đó, chúng ta có thể tránh tính toán tường minh ma trận N (tức là tránh tính toán ŝ), để thay thế nội dung. Theo Eqs.11 và 12, chúng ta định nghĩa d là khoảng cách giữa ẑ và zn.

d = ẑ - zn = UT[ŷ;ŝ] - UT[yn;ŝ]
= UT([ŷ;ŝ] - [yn;ŝ]) = UT[ŷ - yn;0]
= [M;N]T[ŷ - yn;0]
= MT(ŷ - yn) = MT(Mẑ - yn) (13)

Cần lưu ý rằng ở đây ẑ ≠ MTMẑ vì mất mát tái tạo không cho phép ŝ bằng không. Do đó, chúng ta có:

zn = ẑ - d = ẑ - MT(Mẑ - yn) (14)
xn = decoder(ẑ - MT(Mẑ - yn)) (15)

Nếu AE bản thân nó là một mô hình tạo sinh (như VAE), thì cấu trúc AE+MSP trở thành một mô hình tạo sinh có điều kiện. Cho một s được lấy mẫu ngẫu nhiên và một vector thuộc tính yr, mô hình có thể tạo ra mẫu mới xr với các thuộc tính mong muốn với Eq.12.

--- TRANG 6 ---
♂+râu♂+trang điểm miệng mở +cười♂+hói tóc -kính♀+râu♂-trang điểm miệng mở -cười♂+tóc mái tóc-kính♂-râu♀+trang điểm miệng đóng +cười♀+hói tóc +kính♀-râu♀-trang điểm miệng đóng -cười♀+tóc mái tóc+kính

hình ảnh đầu vào MSP Mạng Fader AttGan ♂+râu♂+trang điểm miệng mở +cười♂+hói tóc -kính♀+râu♂-trang điểm miệng mở -cười♂+tóc mái tóc-kính♂-râu♀+trang điểm miệng đóng +cười♀+hói tóc +kính♀-râu♀-trang điểm miệng đóng -cười♀+tóc mái tóc+kính♂+râu♂+trang điểm miệng mở +cười♂+hói tóc -kính♀+râu♂-trang điểm miệng mở -cười♂+tóc mái tóc-kính♂-râu♀+trang điểm miệng đóng +cười♀+hói tóc +kính♀-râu♀-trang điểm miệng đóng -cười♀+tóc mái tóc+kính

hình ảnh đầu vào MSP Mạng Fader AttGan ♂+râu♂+trang điểm miệng mở +cười♂+hói tóc -kính♀+râu♂-trang điểm miệng mở -cười♂+tóc mái tóc-kính♂-râu♀+trang điểm miệng đóng +cười♀+hói tóc +kính♀-râu♀-trang điểm miệng đóng -cười♀+tóc mái tóc+kính♂+râu♂+trang điểm miệng mở +cười♂+hói tóc -kính♀+râu♂-trang điểm miệng mở -cười♂+tóc mái tóc-kính♂-râu♀+trang điểm miệng đóng +cười♀+hói tóc +kính♀-râu♀-trang điểm miệng đóng -cười♀+tóc mái tóc+kính

Mô hình của chúng tôi Đường cơ sở Đường cơ sở

Mô hình của chúng tôi Đường cơ sở Đường cơ sở

Mô hình của chúng tôi Đường cơ sở Đường cơ sở

Hình 4. Ví dụ về biến đổi thuộc tính hình ảnh sử dụng MSP (mô hình của chúng tôi), Mạng Fader và AttGAN.

4. Đánh giá
Ở đây chúng tôi đánh giá khả năng tách biệt. Chúng tôi cũng đánh giá tính trực giao của M vì nó là một chỉ số quan trọng về mức độ thuật toán của chúng tôi có thể xấp xỉ M.

4.1. Phép Chiếu Ma Trận Con trong VAE
Chúng tôi áp dụng mô hình của chúng tôi trên một VAE vanilla (Kingma & Welling, 2013) với bộ mã hóa và bộ giải mã CNN tiêu chuẩn (các kiến trúc giống như Lample et al. (2017)). Chúng tôi sử dụng

--- TRANG 7 ---
Phân Tích Nhân Tố Không Gian Tiềm Ẩn và Thao Tác thông qua Phép Chiếu Ma Trận Con

Bảng 1. Kết quả đánh giá chất lượng tạo sinh. Các số trong bảng biểu thị phần trăm người tham gia dưới tiêu đề cột cảm thấy hình ảnh tốt hơn có hoặc không có MSP.

Seq2seq VAE VAE+GAN
Chất lượng tốt hơn 34.5% 12.6% 17.9%
chỉ với AE
Chất lượng tốt hơn 37.6% 12.0% 15.2%
với AE+MSP
cả hai chất lượng tương tự 27.9% 75.4% 66.9%

Bảng 2. Độ chính xác phân loại (bộ phân loại ResNet-CNN) của hình ảnh được tạo sinh sử dụng MSP, Mạng Fader và AttGAN.

MSP(của chúng tôi) Fader AttGAN
nam × râu 0.78 0.42 0.45
nữ × râu 0.52 0.03 0.41
nam × không râu 0.86 0.40 0.42
nữ × không râu 0.90 0.61 0.63
nam × trang điểm 0.52 0.02 0.35
nam × không trang điểm 0.89 0.50 0.47
nữ × trang điểm 0.87 0.63 0.52
nữ × không trang điểm 0.67 0.42 0.47
cười × miệng mở 0.89 0.59 0.63
không cười × miệng mở 0.66 0.11 0.29
cười × miệng bình thường 0.95 0.34 0.33
không cười × miệng bình thường 0.76 0.43 0.38
nam × hói 0.78 0.10 0.29
nam × tóc mái 0.56 0.05 0.19
nữ × hói 0.29 0.01 0.17
nữ × tóc mái 0.68 0.21 0.20
không kính × tóc đen 0.74 0.38 0.53
không kính × tóc vàng 0.86 0.36 0.79
có kính × tóc đen 0.82 0.21 0.32
có kính × tóc vàng 0.77 0.19 0.33

bộ tối ưu ADAM với tốc độ học = 0.0002, kích thước mini-batch là 256, và hình ảnh được nâng cấp lên 256×256. Chúng tôi thêm một PatchGAN bổ sung (Li & Wand, 2016) để làm cho hình ảnh được sản xuất sắc nét. Kiến trúc của bộ phân biệt PatchGAN cũng áp dụng phiên bản của Lample et al. (2017).

Các đường cơ sở của chúng tôi là mạng Fader (Lample et al., 2017) và AttGAN (He et al., 2019), dựa trên mã và cài đặt đã công bố của họ. Chúng tôi không so sánh StarGAN (Choi et al., 2018) vì chúng tôi cảm thấy nó được thay thế bởi AttGAN, đã chứng minh hiệu suất tốt hơn. Chúng tôi không so sánh RelGAN (Wu et al., 2019) vì nó không tách biệt thuộc tính (xem Hình 1 (trái), cũng RelGAN không thể thêm ria mép hoặc râu vào khuôn mặt nữ, thay vào đó nó sẽ biến đổi thành khuôn mặt nam có râu).

Chúng tôi đánh giá trên tập dữ liệu CelebA (Liu et al., 2015) (202.600 hình ảnh) và huấn luyện một mô hình trên tất cả 40 thuộc tính được gán nhãn. Các ví dụ được tạo sinh được hiển thị trong Hình 4. Về mặt định tính, chúng ta thấy các ví dụ rõ ràng về những gì mạng Fader và AttGan không thể làm: Đối với người phụ nữ đeo kính (giữa) FaderNetwork và AttGan cho thấy hoàn toàn không có khả năng loại bỏ kính; FaderNetwork hoàn toàn thất bại trong việc thêm kính vào hai khuôn mặt khác, và AttGan chỉ có thể quản lý viền yếu trên người phụ nữ cuối cùng (dưới). FaderNetwork nói chung gặp khó khăn trong việc thay đổi thuộc tính, đặc biệt đối với hai phụ nữ, trong khi AttGan làm tốt hơn, nhưng gặp khó khăn với một số thuộc tính nhất định, ví dụ: chủ yếu nó thất bại trong việc thay đổi người phụ nữ cuối cùng thành nam, và gặp khó khăn trong việc loại bỏ trang điểm.

Để đánh giá định lượng, chúng tôi huấn luyện một bộ phân loại (ResNet-CNN) để đo độ chính xác mà các thuộc tính được thay đổi. Bảng 2 cho thấy rằng cách tiếp cận MSP của chúng tôi vượt trội so với các đối thủ cạnh tranh. Cuối cùng chúng tôi tính toán Khoảng cách Khởi tạo Fréchet (FID) trung bình (Heusel et al., 2017) cho mỗi phương pháp: MSP=35.0, Fader=26.3, AttGAN=7.3 (thấp hơn là tốt hơn, 0 là tốt nhất). Điểm FID cố gắng tính toán sự tương tự của hình ảnh gốc và hình ảnh được tạo sinh. Rõ ràng AttGAN là người chiến thắng về chất lượng trong khi MSP của chúng tôi là người chiến thắng về độ chính xác của việc sửa đổi thuộc tính. Khi AttGAN không thể xử lý việc sửa đổi thuộc tính, nó tạo ra hình ảnh không thay đổi và có thể nhận được điểm FID thấp hơn.

Kết quả của thao tác thuộc tính (cả định tính và định lượng) đáng ngạc nhiên là tệ đối với mạng Fader và AttGAN, đặc biệt so với các ví dụ được hiển thị trong các bài báo gốc của họ. Lý do chính cho điều này là chúng tôi đã huấn luyện những mô hình đó trên tất cả 40 thuộc tính cùng nhau. Mạng Fader hoạt động tốt nhất khi được huấn luyện trên một thuộc tính duy nhất, như đã lưu ý trong Phần 2. Bài báo AttGAN gốc huấn luyện trên 13 thuộc tính, và thực sự nó hoạt động tốt hơn trong thao tác thuộc tính so với Fader trong hình ảnh của chúng tôi. Đối với phiên bản 40-thuộc tính-cùng nhau, khi bất kỳ thuộc tính nào được thay đổi, tất cả những thuộc tính khác phải không thay đổi. Ví dụ, khi chúng ta chuyển từ nam sang nữ (Hình 4 trái), ngầm hiểu rằng nữ giới nên giữ không trang điểm hoặc son môi, v.v. Theo hướng từ nữ sang nam, nam giới nên giữ không có lông mày rậm hoặc bóng râu 5 giờ. Bài báo mạng Fader gốc hiển thị một ví dụ đẹp về nội suy giữa nam và nữ, nhưng nữ giới có trang điểm và son môi và nam giới có lông mày rậm và bóng râu 5 giờ. Cài đặt 40-thuộc tính khó khăn của chúng tôi là trung tâm của mục tiêu, như đã nêu trong phần giới thiệu: chúng tôi muốn tách biệt hoàn toàn nhiều thuộc tính, vì điều này mang lại cho mô hình tạo sinh khả năng 'tưởng tượng' các ví dụ mới kết hợp các thuộc tính theo cách không có trong dữ liệu huấn luyện.

4.2. Đánh giá của Con người về Chất lượng Ví dụ Được tạo sinh
Chúng tôi đánh giá liệu mô hình MSP của chúng tôi có giảm chất lượng của các ví dụ được tạo sinh hay không, sử dụng đánh giá của con người qua Amazon Mechanical Turk (thuê 150 người tham gia tổng cộng).

Đối với mỗi mô hình, chúng tôi tạo ngẫu nhiên 1.000 cặp ví dụ. Mỗi cặp chứa một ví dụ được tái tạo (từ AE) và một ví dụ được tạo sinh bởi AE+MSP với một hoặc hai sửa đổi thuộc tính ngẫu nhiên (các thuộc tính được thay đổi thành 1 nếu chúng ban đầu >0, hoặc thay đổi thành 1 nếu chúng <0)3. Những người tham gia được hiển thị các cặp ví dụ từng cặp một trong bài kiểm tra mù, và họ được yêu cầu vui lòng chọn cái có chất lượng văn bản/hình ảnh tốt hơn, hoặc chọn cả hai nếu bạn nghĩ chúng hoạt động tương tự. Những người tham gia được cho biết rằng chất lượng văn bản có nghĩa là tính trôi chảy, độ chính xác ngữ nghĩa, và độ chính xác cú pháp, và chất lượng hình ảnh có nghĩa là độ rõ ràng và khả năng nhận dạng (khuôn mặt). Kết quả được hiển thị trong Bảng 1. Chúng tôi coi các điểm số (tức là lựa chọn của người tham gia) của kết quả như một thang điểm Likert, và chúng tôi đặt giả thuyết null của chúng tôi là H0: chất lượng tạo sinh của AE+MSP tệ hơn so với chỉ sử dụng AE, và, H1: chất lượng tạo sinh của AE+MSP bằng hoặc cao hơn so với chỉ sử dụng AE. Các giả thuyết được kiểm tra bằng kiểm định Mann-Whitney U rời rạc, bác bỏ H0 với p < 0:03.

4.3. Đánh giá Tách biệt
Tách biệt cũng là một đặc trưng quan trọng của mô hình chúng tôi. Nó có nghĩa là khi một thuộc tính được sửa đổi, các thuộc tính khác vẫn không thay đổi. Chúng tôi làm cho ba mô hình (VAE+GAN+MSP, AttGAN, và Mạng Fader) tạo ra hình ảnh bằng cách thao tác hai nhóm thuộc tính có tương quan cao, độ mở của miệng / cười, và nam / râu. Đối với hai nhóm, ba mô hình tương ứng nên tạo ra hình ảnh với miệng đóng không cười, miệng đóng cười, miệng mở không cười, miệng mở cười, nữ không râu, nữ có râu, nam không râu, và nam có râu. Chúng tôi thuê 50 người tham gia trong Amazon Mechanical Turk; mỗi người trong số họ được giao 40 khối hình ảnh. Một khối chứa bốn hình ảnh, được từ nhóm miệng / cười hoặc nhóm nam / râu, và được tạo sinh bởi một trong ba mô hình. Những người tham gia được cho biết hình ảnh nào nên đại diện cho thuộc tính nào, và những người tham gia đánh giá liệu nó có làm như vậy cho mỗi hình ảnh trong khối bằng cách sử dụng thang điểm Likert 3 cấp (hoàn hảo, có thể nhận ra, và không thể nhận ra/không thay đổi). Kết quả được hiển thị trong Bảng 4. Nó cho thấy rằng mô hình của chúng tôi hoạt động tốt hơn đáng kể so với đường cơ sở. (p < 0:0001).

Bảng 3. Đánh giá định lượng về tách biệt (sử dụng bộ phân loại ResNet-CNN).

Thuộc tính Ảnh hưởng đến MSP Fader AttGAN
đích các thuộc tính khác (của chúng tôi)
được thay đổi
giới tính râu 0.01 0.28 0.09
râu giới tính 0.07 0.11 0.02
giới tính trang điểm 0.02 0.07 0.05
trang điểm giới tính 0.05 0.09 0.14
cười miệng mở 0.01 0.20 0.07
miệng mở cười 0.02 0.07 0.09

thao tác thuộc tính miệng mở / cười
Mạng Fader AttGAN VAE+GAN
MSP (của chúng tôi)
hoàn hảo 36.7% 47.5% 68.3%
có thể nhận ra 20.8% 15.3% 4.9%
không thể nhận ra/không thay đổi 42.5% 37.2% 26.8%

thao tác thuộc tính nam / râu
Mạng Fader AttGAN VAE+GAN
MSP (của chúng tôi)
hoàn hảo 38.3% 55.9% 74.4%
có thể nhận ra 8.3% 11.2% 11.6%
không thể nhận ra/không thay đổi 53.3% 32.9% 14.0%

Bảng 4. Kết quả đánh giá thủ công về tách biệt. Các số trong bảng biểu thị phần trăm người tham gia dưới tiêu đề cột cảm thấy hình ảnh đại diện cho thuộc tính được chỉ định (ví dụ: cười) theo cách hoàn hảo, có thể nhận ra, hoặc không thể nhận ra/không thay đổi.

Ngoài đánh giá của con người, chúng tôi cũng tiến hành đánh giá định lượng để kiểm tra mức độ một mô hình có thể thay đổi một thuộc tính một cách cô lập. Đối với một số thuộc tính có tương quan cao được chọn, chúng tôi thay đổi một thuộc tính đích, và đo lường sự thay đổi trong một thuộc tính không đích khác. Chẳng hạn (hàng giới tính/râu trong Bảng 3), khi thuộc tính giới tính được thay đổi thủ công, chúng tôi đo lường mức độ mà thuộc tính râu bị thay đổi do đó. Kết quả được hiển thị trong Bảng 3. Lưu ý rằng, các điểm số cho thấy mức độ các thuộc tính không đích bị ảnh hưởng, nhưng không phải liệu các thuộc tính đích có được thay đổi đúng cách trong hình ảnh được tạo sinh hay không. Do đó các điểm số cần được đọc kết hợp với Bảng 2. Theo cả Bảng 2 và Bảng 3, chúng ta có thể kết luận rằng trong cả hai khía cạnh của việc thao tác thuộc tính và tránh ảnh hưởng đến các thuộc tính không đích, hiệu suất của mô hình chúng tôi vượt trội so với các đường cơ sở.

4.4. Phép Chiếu Ma Trận Con trong Seq2seq
Chúng tôi áp dụng mô hình của chúng tôi vào một mô hình seq2seq cổ điển để thay thế nội dung văn bản, nhằm xác định liệu chúng ta có thể thay thế từ theo các thuộc tính cho trước và giữ các từ khác không thay đổi hay không. Trong nhiệm vụ này, chúng tôi áp dụng kho dữ liệu E2E (Dušek et al., 2019), chứa hơn 50k đánh giá về nhà hàng (tập dữ liệu E2E được phát triển cho Tạo sinh Ngôn ngữ Tự nhiên, nhưng ở đây chúng tôi sử dụng nó để thay thế nội dung). Mỗi đánh giá là một câu duy nhất được gán nhãn bởi các cặp thuộc tính-giá trị, ví dụ: "name=[The Eagle]", "food=[French]", và "customerRating=[3/5]". Chúng tôi coi mỗi cặp thuộc tính-giá trị như một nhãn duy nhất. Tất cả các thuộc tính tạo thành y có các mục là 1 hoặc 0 để đại diện cho mỗi giá trị (văn bản chính xác của tên thuộc tính hoặc giá trị KHÔNG được sử dụng).

Cả bộ mã hóa và bộ giải mã của mô hình seq2seq đều được tạo thành bởi các LSTM hai lớp. Mô hình được huấn luyện trong 1000

--- TRANG 9 ---
Phân Tích Nhân Tố Không Gian Tiềm Ẩn và Thao Tác thông qua Phép Chiếu Ma Trận Con

Ví dụ 1 Ví dụ 2
Thuộc tính gốc eatType[pub], customer-rating[5-out-of-5], name[Blue-Spice], near[Crowne-Plaza-Hotel] familyFriendly[yes], area[city-centre], eatType[pub], food[Japanese], near[Express-by-Holiday-Inn], name[Green-Man]
Văn bản gốc the blue spice pub , near crowne plaza hotel , has a customer rating of 5 out of 5 . near the express by holiday inn in the city centre is green man . it is a japanese pub that is family-friendly .
Thuộc tính mới eatType[coffee-shop], customer-rating[5-out-of-5], name[Blue-Spice], near[Avalon] familyFriendly[no], area[riverside], eatType[coffee-shop], food[French], near[The-Six-Bells], name[Green-Man]
Văn bản mới the blue spice coffee shop , near avalon has a customer rating of 5 out of 5 . near the six bells in the riverside area is a green man . it is a french coffee shop that is not family-friendly .

Bảng 5. Kết quả thay đổi thuộc tính trong kho dữ liệu E2E.

(a)(b)(c)(d)
01530456075015304560750200400600800100002004006008001000
051015202530350510152025303502004006008001000020040060080010001.00.80.60.40.20.0-0.2-0.4

Hình 5. Đo lường tính trực giao: Bản đồ nhiệt của MTM và UTU cho Seq2seq+MSP (a,b), và VAE+GAN+MSP (c,d)

epoch (trên Tesla T4 khoảng 12 giờ). Sau khi huấn luyện, chúng tôi tái tạo các câu đánh giá với các thuộc tính được thay thế ngẫu nhiên, ví dụ thay thế "name=[The Eagle]" bằng "name=[Burger King]", "customerRating=[3/5]" bằng "customerRating=[1/5]". 50% thuộc tính được thay đổi trong mỗi câu. Kết quả được hiển thị trong Bảng 5.

4.5. Đánh giá Trực giao
Khả năng tách biệt thuộc tính được đảm bảo bởi tính trực giao của M trong mô hình của chúng tôi. Thay vì trực tiếp sử dụng một ma trận trực giao, chúng tôi huấn luyện M để trở nên trực giao. Do đó, chúng tôi đánh giá M gần với ma trận trực giao như thế nào. Hình 5 hiển thị bản đồ nhiệt của MTM và UTU, cho thấy rằng sản phẩm khá gần với ma trận đơn vị. Nó hình dung MTM trong phiên bản seq2seq của mô hình (Hình 5 (a)) và trong phiên bản VAE (Hình 5 (c)). Ma trận UTU (U được tạo thành bởi M nối với không gian null của nó) cũng được hình dung (trong Hình 5 (b) và (d)). Rõ ràng là khi các ma trận được nhân với chuyển vị của chúng, các sản phẩm xấp xỉ ma trận đơn vị. Mặc dù Hình 5 (c) cho thấy rằng một số ít thuộc tính vẫn hơi vướng víu (bởi các pixel xanh lá cây và tím đậm), điều này chủ yếu gây ra bởi một số thuộc tính xung đột trong CelebA, ví dụ: receding-hairline ↔ bald ↔ bangs, và straight-hair ↔ wavy-hair. Do đó, M thực sự được huấn luyện để trở thành một ma trận trực giao (một phần).

5. Kết luận
Chúng tôi đề xuất một plugin phép chiếu ma trận có thể được gắn vào các bộ tự mã hóa khác nhau (ví dụ: Seq2seq, VAE) để làm cho không gian tiềm ẩn được phân tích nhân tố và tách biệt, dựa trên thông tin thuộc tính được gán nhãn, đảm bảo rằng thao tác trong không gian tiềm ẩn dễ dàng hơn nhiều. Chúng tôi kiểm tra khả năng thao tác thuộc tính của mô hình trên một tập dữ liệu hình ảnh và kho dữ liệu văn bản, thu được kết quả cho thấy sự tách biệt rõ ràng. Ngoài ra, mô hình của chúng tôi bao gồm một quy trình huấn luyện đơn giản hơn so với các cách tiếp cận đối kháng cần huấn luyện dài với trọng số rất thấp trên mất mát từ bộ phân biệt loại bỏ thông tin thuộc tính, để tránh bộ mã hóa bị ảnh hưởng quá nhiều bởi thuật ngữ mất mát này (Lample et al., 2017).

Lời cảm ơn
Chúng tôi xin cảm ơn tất cả các nhà đánh giá ẩn danh vì những nhận xét sâu sắc của họ. Công trình này được hỗ trợ bởi giải thưởng được trao bởi Hội đồng Nghiên cứu Khoa học Kỹ thuật và Vật lý Vương quốc Anh (Số hiệu tài trợ: EP/P011829/1).

--- TRANG 10 ---
Phân Tích Nhân Tố Không Gian Tiềm Ẩn và Thao Tác thông qua Phép Chiếu Ma Trận Con

Tài liệu tham khảo

Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., và Abbeel, P. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Trong Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., và Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 2172–2180. Curran Associates, Inc., 2016.

Choi, Y., Choi, M., Kim, M., Ha, J., Kim, S., và Choo, J. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. Trong 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 8789–8797. IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00916.

Creswell, A., Bharath, A. A., và Sengupta, B. Conditional autoencoders with adversarial information factorization. CoRR, abs/1711.05175, 2017. URL http://arxiv.org/abs/1711.05175.

Dušek, O., Novikova, J., và Rieser, V. Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG Challenge. arXiv preprint arXiv:1901.11528, January 2019. URL https://arxiv.org/abs/1901.11528.

He, Z., Zuo, W., Kan, M., Shan, S., và Chen, X. Attgan: Facial attribute editing by only changing what you want. IEEE Transactions on Image Processing, 28(11):5464–5478, Nov 2019. ISSN 1941-0042. doi: 10.1109/TIP.2019.2916751.

Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., và Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Trong Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, pp. 66296640, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.

Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., và Lerchner, A. Early visual concept learning with unsupervised deep learning. CoRR, abs/1606.05579, 2016. URL http://arxiv.org/abs/1606.05579.

Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., và Lerchner, A. beta-vae: Learning basic visual concepts with a constrained variational framework. Trong 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?id=Sy2fzU9gl.

Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., và Xing, E. P. Toward controlled generation of text. Trong Precup, D. và Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 của Proceedings of Machine Learning Research, pp. 1587–1596, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/hu17e.html.

Kingma, D. P. và Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

Kingma, D. P., Mohamed, S., Jimenez Rezende, D., và Welling, M. Semi-supervised learning with deep generative models. Trong Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., và Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 27, pp. 3581–3589. Curran Associates, Inc., 2014.

Klys, J., Snell, J., và Zemel, R. Learning latent subspaces in variational autoencoders. Trong Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., và Garnett, R. (eds.), Advances in Neural Information Processing Systems 31, pp. 6444–6454. Curran Associates, Inc., 2018.

Kumar, A., Sattigeri, P., và Balakrishnan, A. Variational inference of disentangled latent concepts from unlabeled observations. Trong 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum?id=H1kG7GZAW.

Lake, B. M., Ullman, T. D., Tenenbaum, J. B., và Gershman, S. J. Building machines that learn and think like people. CoRR, abs/1604.00289, 2016.

Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., et al. Fader networks: Manipulating images by sliding attributes. Trong Advances in Neural Information Processing Systems, pp. 5967–5976, 2017.

LeCun, Y. The power and limits of deep learning. Research Technology Management, 61(6):22–27, 2013. doi: 10.1080/08956308.2018.1516928.

Li, C. và Wand, M. Precomputed real-time texture synthesis with markovian generative adversarial networks. Trong European Conference on Computer Vision, pp. 702–716. Springer, 2016.

Li, R., Li, X., Lin, C., Collinson, M., và Mao, R. A stable variational autoencoder for text modelling. Trong Proceedings of the 12th International Conference on Natural Language Generation, pp. 594–599, 2019.

--- TRANG 11 ---
Phân Tích Nhân Tố Không Gian Tiềm Ẩn và Thao Tác thông qua Phép Chiếu Ma Trận Con

Liu, Z., Luo, P., Wang, X., và Tang, X. Deep learning face attributes in the wild. Trong Proceedings of the IEEE international conference on computer vision, pp. 3730–3738, 2015.

Perarnau, G., van de Weijer, J., Raducanu, B., và Álvarez, J. M. Invertible conditional gans for image editing. CoRR, abs/1611.06355, 2016. URL http://arxiv.org/abs/1611.06355.

Sohn, K., Lee, H., và Yan, X. Learning structured output representation using deep conditional generative models. Trong Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., và Garnett, R. (eds.), Advances in Neural Information Processing Systems 28, pp. 3483–3491. Curran Associates, Inc., 2015.

Torfason, R., Agustsson, E., Rothe, R., và Timofte, R. From face images and attributes to attributes. Trong 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 20-24, 11 2016. doi: 10.1007/978-3-319-54187-7_21.

Upchurch, P., Gardner, J. R., Pleiss, G., Pless, R., Snavely, N., Bala, K., và Weinberger, K. Q. Deep feature interpolation for image content changes. Trong 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 6090–6099, 2017. doi: 10.1109/CVPR.2017.645. URL https://doi.org/10.1109/CVPR.2017.645.

Wu, P.-W., Lin, Y.-J., Chang, C.-H., Chang, E. Y., và Liao, S.-W. Relgan: Multi-domain image-to-image translation via relative attributes. Trong The IEEE International Conference on Computer Vision (ICCV), October 2019.

Xiao, Y., Zhao, T., và Wang, W. Y. Dirichlet variational autoencoder for text modeling. CoRR, abs/1811.00135, 2018. URL http://arxiv.org/abs/1811.00135.

Yan, X., Yang, J., Sohn, K., và Lee, H. Attribute2image: Conditional image generation from visual attributes. Trong Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, pp. 776–791, 2016. doi: 10.1007/978-3-319-46493-0_47. URL https://doi.org/10.1007/978-3-319-46493-0_47.

Yuille, A. L. và Liu, C. Deep nets: What have they ever done for vision? CoRR, abs/1805.04025, 2018.