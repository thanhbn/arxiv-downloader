# SPARSE AUTOENCODERS FIND HIGHLY INTER -
PRETABLE FEATURES IN LANGUAGE MODELS
Hoagy Cunningham∗12, Aidan Ewart∗13, Logan Riggs∗1, Robert Huben, Lee Sharkey4
1EleutherAI,2MATS,3Bristol AI Safety Centre,4Apollo Research
{hoagycunningham, aidanprattewart, logansmith5 }@gmail.com

# TÓM TẮT
Một trong những rào cản đối với việc hiểu rõ hơn về bên trong của mạng neural là tính đa nghĩa, trong đó các nơ-ron có vẻ kích hoạt trong nhiều ngữ cảnh khác nhau về mặt ngữ nghĩa. Tính đa nghĩa ngăn cản chúng ta xác định những giải thích ngắn gọn, dễ hiểu đối với con người về những gì mạng neural đang thực hiện bên trong. Một nguyên nhân được giả thuyết của tính đa nghĩa là hiện tượng chồng chất, trong đó mạng neural biểu diễn nhiều đặc trưng hơn số nơ-ron chúng có bằng cách gán các đặc trưng cho một tập hợp hướng quá đầy đủ trong không gian kích hoạt, thay vì cho các nơ-ron riêng lẻ. Ở đây, chúng tôi cố gắng xác định những hướng đó, sử dụng các autoencoder thưa để tái tạo các kích hoạt bên trong của một mô hình ngôn ngữ. Những autoencoder này học các tập hợp đặc trưng kích hoạt thưa thớt có tính diễn giải và tính đơn nghĩa cao hơn so với các hướng được xác định bởi các phương pháp thay thế, trong đó tính diễn giải được đo bằng các phương pháp tự động. Hơn nữa, chúng tôi cho thấy với tập hợp đặc trưng đã học của mình, chúng tôi có thể xác định chính xác các đặc trưng chịu trách nhiệm nhân quả cho hành vi phản thực tế trong tác vụ nhận dạng đối tượng gián tiếp (Wang et al., 2022) với độ tinh tế cao hơn so với các phân rã trước đó. Công trình này chỉ ra rằng có thể giải quyết hiện tượng chồng chất trong các mô hình ngôn ngữ bằng một phương pháp có thể mở rộng, không giám sát. Phương pháp của chúng tôi có thể phục vụ như một nền tảng cho công việc diễn giải cơ học trong tương lai, mà chúng tôi hy vọng sẽ cho phép tính minh bạch và khả năng điều khiển mô hình tốt hơn.

# 1 GIỚI THIỆU
Những tiến bộ trong trí tuệ nhân tạo (AI) đã dẫn đến việc phát triển các hệ thống AI có khả năng cao đưa ra quyết định vì những lý do mà chúng ta không hiểu. Điều này đã gây ra lo ngại rằng các hệ thống AI mà chúng ta không thể tin tưởng đang được triển khai rộng rãi trong nền kinh tế và cuộc sống của chúng ta, giới thiệu một số rủi ro mới (Hendrycks et al., 2023), bao gồm các rủi ro tiềm tàng trong tương lai rằng AI có thể lừa dối con người để hoàn thành các mục tiêu không mong muốn (Ngo et al., 2022). Diễn giải cơ học tìm cách giảm thiểu những rủi ro như vậy thông qua việc hiểu cách mạng neural tính toán đầu ra của chúng, cho phép chúng ta tái tạo ngược một phần quy trình bên trong của chúng và thực hiện những thay đổi có mục tiêu đối với chúng (Cammarata et al., 2021; Wang et al., 2022; Elhage et al., 2021).

Để tái tạo ngược một mạng neural, cần thiết phải chia nhỏ nó thành các đơn vị nhỏ hơn (đặc trưng) có thể được phân tích riêng biệt. Việc sử dụng các nơ-ron riêng lẻ như những đơn vị này đã có một số thành công (Olah et al., 2020; Bills et al., 2023), nhưng một thách thức chính là các nơ-ron thường có tính đa nghĩa, kích hoạt cho nhiều loại đặc trưng không liên quan (Olah et al., 2020). Ngoài ra, đối với một số loại kích hoạt mạng, chẳng hạn như luồng dư của một transformer, có ít lý do để mong đợi các đặc trưng sẽ phù hợp với cơ sở nơ-ron (Elhage et al., 2023).

Elhage et al. (2022b) nghiên cứu tại sao tính đa nghĩa có thể phát sinh và đưa ra giả thuyết rằng nó có thể là kết quả của việc các mô hình học nhiều đặc trưng riêng biệt hơn số chiều trong lớp. Họ gọi hiện tượng này là chồng chất. Vì một không gian vector chỉ có thể có nhiều vector trực giao bằng số chiều của nó, điều này có nghĩa là mạng sẽ học một cơ sở quá đầy đủ gồm các đặc trưng không trực giao. Các đặc trưng phải kích hoạt đủ thưa thớt để hiện tượng chồng chất phát sinh bởi vì, không có độ thưa cao, sự can thiệp giữa các đặc trưng không trực giao ngăn cản bất kỳ lợi ích hiệu suất nào từ chồng chất. Điều này gợi ý rằng chúng ta có thể khôi phục các đặc trưng của mạng bằng cách tìm một tập hợp các hướng trong không gian kích hoạt sao cho mỗi vector kích hoạt có thể được tái tạo từ các kết hợp tuyến tính thưa của những hướng này. Điều này tương đương với bài toán học từ điển thưa được biết đến (Olshausen & Field, 1997).

Dựa trên Sharkey et al. (2023), chúng tôi huấn luyện các autoencoder thưa để học những tập hợp hướng này. Phương pháp của chúng tôi cũng tương tự như Yun et al. (2021), những người áp dụng học từ điển thưa cho tất cả các lớp luồng dư trong một mô hình ngôn ngữ cùng lúc. Phương pháp của chúng tôi được tóm tắt trong Hình 1 và mô tả trong Phần 2.

Sau đó chúng tôi sử dụng một số kỹ thuật để xác minh rằng các đặc trưng đã học của chúng tôi biểu diễn một phân rã có ý nghĩa về mặt ngữ nghĩa của không gian kích hoạt. Đầu tiên, chúng tôi cho thấy các đặc trưng của chúng tôi trung bình có tính diễn giải cao hơn so với các nơ-ron và các kỹ thuật phân rã ma trận khác, được đo bằng điểm số tự diễn giải (Phần 3) (Bills et al., 2023). Tiếp theo, chúng tôi cho thấy rằng chúng tôi có thể xác định chính xác các đặc trưng được sử dụng cho một tác vụ nhất định một cách chính xác hơn so với các phương pháp khác (Phần 4). Cuối cùng, chúng tôi chạy các nghiên cứu trường hợp trên một số lượng nhỏ đặc trưng, cho thấy chúng không chỉ có tính đơn nghĩa mà còn có tác động có thể dự đoán đối với đầu ra mô hình, và có thể được sử dụng để phát hiện mạch tinh tế. (Phần 5).

# 2 ĐƯららራ CÁC ĐẶC TRƯNG RA KHỎI CHỒNG CHẤT VỚI HỌC TỪ ĐIỂN THƯA

Để đưa các đặc trưng mạng ra khỏi chồng chất, chúng tôi sử dụng các kỹ thuật từ học từ điển thưa (Olshausen & Field, 1997; Lee et al., 2006). Giả sử rằng mỗi vector trong một tập hợp các vector {xi}nvec i=1⊂Rd được tạo thành từ một kết hợp tuyến tính thưa của các vector chưa biết {gj}ngt j=1⊂Rd, tức là xi=∑j ai,jgj trong đó ai là một vector thưa. Trong trường hợp của chúng tôi, các vector dữ liệu {xi}nvec i=1 là các kích hoạt bên trong của một mô hình ngôn ngữ, chẳng hạn như Pythia-70M (Biderman et al., 2023), và {gj}ngt j=1 là các đặc trưng mạng thực tế chưa biết. Chúng tôi muốn học một từ điển các vector, được gọi là các đặc trưng từ điển, {fk}nfeat k=1⊂Rd trong đó đối với bất kỳ đặc trưng mạng gj nào, tồn tại một đặc trưng từ điển fk sao cho gj≈fk.

Để học từ điển, chúng tôi huấn luyện một autoencoder với một số hạng phạt về độ thưa trên các kích hoạt ẩn của nó. Autoencoder là một mạng neural với một lớp ẩn duy nhất có kích thước dhid=R×din, trong đó din là chiều của các vector kích hoạt bên trong của mô hình ngôn ngữ¹, và R là một siêu tham số kiểm soát tỷ lệ của kích thước từ điển đặc trưng với chiều mô hình. Chúng tôi sử dụng hàm kích hoạt ReLU trong lớp ẩn (Fukushima, 1975). Chúng tôi cũng sử dụng trọng số liên kết cho mạng neural của mình, có nghĩa là các ma trận trọng số của bộ mã hóa và bộ giải mã là chuyển vị của nhau.² Do đó, trên vector đầu vào x∈ {xi}, mạng của chúng tôi tạo ra đầu ra x̂, được cho bởi

c = ReLU(Mx + b)                    (1)
x̂ = M^T c                          (2)
  = ∑(i=0 to dhid-1) ci fi          (3)

trong đó M∈Rdhid×din và b∈Rdhid là các tham số đã học của chúng tôi, và M được chuẩn hóa theo hàng³. Ma trận tham số M của chúng tôi là từ điển đặc trưng của chúng tôi, bao gồm dhid hàng của các đặc trưng từ điển fi. Đầu ra x̂ được dự định là một tái tạo của vector gốc x, và lớp ẩn c bao gồm các hệ số chúng tôi sử dụng trong việc tái tạo x.

Autoencoder của chúng tôi được huấn luyện để tối thiểu hóa hàm mất mát

L(x) = ||x - x̂||²₂ + α||c||₁        (4)
       ⎯⎯⎯⎯⎯⎯⎯⎯⎯   ⎯⎯⎯⎯⎯
       Mất mát tái tạo  Mất mát độ thưa

trong đó α là một siêu tham số kiểm soát độ thưa của việc tái tạo. Số hạng mất mát ℓ1 trên c khuyến khích việc tái tạo của chúng tôi là một kết hợp tuyến tính thưa của các đặc trưng từ điển. Có thể chứng minh bằng thực nghiệm (Sharkey et al., 2023) và lý thuyết (Wright & Ma, 2022) rằng việc tái tạo với phạt ℓ1 có thể khôi phục các đặc trưng thực tế đã tạo ra dữ liệu. Để biết thêm chi tiết về quy trình huấn luyện của chúng tôi, xem Phụ lục B.

¹Chúng tôi chủ yếu nghiên cứu luồng dư trong Pythia-70M và Pythia 410-M, trong đó luồng dư có kích thước din = 512 và din = 1024, tương ứng (Biderman et al., 2023)

²Chúng tôi sử dụng trọng số liên kết vì (a) chúng mã hóa kỳ vọng của chúng tôi rằng các hướng phát hiện và định nghĩa đặc trưng nên giống hoặc rất tương tự nhau, (b) chúng giảm một nửa chi phí bộ nhớ của mô hình, và (c) chúng loại bỏ sự mơ hồ về việc hướng đã học nên được diễn giải là hướng bộ mã hóa hay bộ giải mã. Chúng không làm giảm hiệu suất khi huấn luyện trên dữ liệu luồng dư nhưng chúng tôi đã quan sát một số giảm hiệu suất khi sử dụng dữ liệu MLP.

³Việc chuẩn hóa các hàng (đặc trưng từ điển) ngăn mô hình giảm số hạng mất mát độ thưa ||c||₁ bằng cách tăng kích thước của các vector đặc trưng trong M.

# 3 DIỄN GIẢI CÁC ĐẶC TRƯNG TỪ ĐIỂN

## 3.1 TÍNH DIỄN GIẢI Ở QUY MÔ LỚN

Sau khi học được một tập hợp các đặc trưng từ điển, chúng tôi muốn hiểu liệu các đặc trưng đã học của chúng tôi có thể hiện tính đa nghĩa giảm và do đó có tính diễn giải cao hơn hay không. Để làm điều này một cách có thể mở rộng, chúng tôi cần một chỉ số để đo mức độ diễn giải của một đặc trưng từ điển. Chúng tôi sử dụng phương pháp tự động được giới thiệu trong Bills et al. (2023) vì nó mở rộng tốt để đo tính diễn giải trên hàng nghìn đặc trưng từ điển mà các autoencoder của chúng tôi học được. Tóm lại, quy trình tự diễn giải lấy các mẫu văn bản mà đặc trưng từ điển kích hoạt, yêu cầu một mô hình ngôn ngữ viết một cách diễn giải có thể đọc được của con người về đặc trưng từ điển, và sau đó nhắc mô hình ngôn ngữ sử dụng mô tả này để dự đoán việc kích hoạt đặc trưng từ điển trên các mẫu văn bản khác. Mối tương quan giữa các kích hoạt dự đoán của mô hình và các kích hoạt thực tế là điểm số diễn giải của đặc trưng đó. Xem Phụ lục A và Bills et al. (2023) để biết thêm chi tiết.

Chúng tôi hiển thị các mô tả và điểm số đầu-và-ngẫu nhiên cho năm đặc trưng từ điển từ luồng dư lớp 1 trong Bảng 1. Các đặc trưng được hiển thị là năm đặc trưng đầu tiên theo thứ tự (tùy ý) trong từ điển.

| Đặc trưng | Mô tả (Được tạo bởi GPT-4) | Điểm số Diễn giải |
|-----------|---------------------------|-------------------|
| 1-0000 | các phần của tên riêng, đặc biệt là họ. | 0.33 |
| 1-0001 | các hành động được thực hiện bởi một chủ thể hoặc đối tượng. | -0.11 |
| 1-0002 | các trường hợp của chữ cái 'W' và các từ bắt đầu bằng 'w'. | 0.55 |
| 1-0003 | số '5' và cũng ghi nhận mức kích hoạt trung bình đến thấp cho tên riêng và một số danh từ. | 0.57 |
| 1-0004 | thuật ngữ pháp lý và tham chiếu đến các vụ án của tòa án. | 0.19 |

Bảng 1: Kết quả tự diễn giải trên năm đặc trưng đầu tiên được tìm thấy trong luồng dư lớp 1. Tự diễn giải tạo ra một mô tả về ý nghĩa của đặc trưng và một điểm số về mức độ mô tả đó dự đoán các kích hoạt khác.

## 3.2 CÁC ĐẶC TRƯNG TỪ ĐIỂN THƯA CÓ TÍNH DIỄN GIẢI CAO HƠN CÁC ĐƯỜNG CƠ SỞ

Chúng tôi đánh giá điểm số diễn giải của chúng tôi so với nhiều phương pháp thay thế khác nhau để tìm từ điển đặc trưng trong các mô hình ngôn ngữ. Cụ thể, chúng tôi so sánh điểm số diễn giải trên các đặc trưng từ điển của chúng tôi với những điểm được tạo ra bởi a) cơ sở mặc định, b) các hướng ngẫu nhiên, c) Phân tích Thành phần Chính (PCA), và d) Phân tích Thành phần Độc lập (ICA). Đối với các hướng ngẫu nhiên và đối với cơ sở mặc định trong luồng dư, chúng tôi thay thế các kích hoạt âm bằng số không để tất cả các kích hoạt đặc trưng đều không âm⁴.

Hình 2 cho thấy các đặc trưng từ điển của chúng tôi có tính diễn giải cao hơn nhiều theo tiêu chí này so với các đặc trưng từ điển được tìm thấy bởi các kỹ thuật tương đương. Chúng tôi thấy rằng sức mạnh của hiệu ứng này giảm khi chúng tôi di chuyển qua mô hình, có thể so sánh với ICA ở lớp 4 và cho thấy cải thiện tối thiểu ở lớp cuối cùng.

Điều này có thể chỉ ra rằng các autoencoder thưa hoạt động kém hơn ở các lớp sau nhưng cũng có thể liên quan đến những khó khăn của diễn giải tự động, cả vì bằng cách xây dựng trên các lớp trước đó, các đặc trưng sau có thể phức tạp hơn, và vì chúng thường được giải thích tốt nhất bởi tác động của chúng đối với đầu ra. Bills et al. (2023) đã chỉ ra rằng GPT-4 có thể tạo ra các giải thích rất gần với chất lượng trung bình của các giải thích được tạo bởi con người khi được cung cấp dữ liệu tương tự. Tuy nhiên, họ cũng cho thấy rằng các LLM hiện tại bị hạn chế trong các loại mẫu mà chúng có thể tìm thấy, đôi khi gặp khó khăn trong việc tìm các mẫu tập trung xung quanh các token tiếp theo hoặc trước đó thay vì token hiện tại, và trong giao thức hiện tại không thể xác minh đầu ra bằng cách nhìn vào những thay đổi trong đầu ra hoặc dữ liệu khác.

Chúng tôi thực sự cho thấy, trong Phần 5, một phương pháp để xem tác động nhân quả của một đặc trưng đối với logits đầu ra bằng tay, nhưng hiện tại chúng tôi không gửi thông tin này cho mô hình ngôn ngữ để tạo giả thuyết. Phần nghiên cứu trường hợp cũng chứng minh một đặc trưng từ điển dấu ngoặc đóng, cho thấy những đặc trưng lớp cuối này có thể cung cấp cái nhìn sâu sắc vào hoạt động của mô hình.

Xem Phụ lục C để khám phá đầy đủ hơn các từ điển đã học khác nhau thông qua góc nhìn diễn giải tự động, xem xét cả MLPs và luồng dư.

⁴Đối với PCA chúng tôi sử dụng phương pháp ước lượng trực tuyến và chạy phân rã trên cùng một lượng dữ liệu mà chúng tôi đã sử dụng để huấn luyện các autoencoder. Đối với ICA, do thời gian hội tụ chậm hơn, chúng tôi chỉ chạy trên 2GB dữ liệu, khoảng 4 triệu kích hoạt cho luồng dư và 1 triệu kích hoạt cho MLPs.

![Hình 2: Điểm số tự diễn giải đầu-và-ngẫu nhiên trung bình của các hướng đã học trong luồng dư, so với một số đường cơ sở, sử dụng 150 đặc trưng mỗi cái. Các thanh lỗi hiển thị khoảng tin cậy 95% xung quanh giá trị trung bình. Các từ điển đặc trưng được sử dụng ở đây được huấn luyện trong 10 epochs sử dụng α=.00086 và R= 2.]

# 4 XÁC ĐỊNH CÁC ĐẶC TRƯNG TỪ ĐIỂN QUAN TRỌNG VỀ MẶT NHÂN QUẢ CHO NHẬN DẠNG ĐỐI TƯỢNG GIÁN TIẾP

Trong phần này, chúng tôi định lượng liệu các đặc trưng từ điển đã học của chúng tôi có bản địa hóa một hành vi mô hình cụ thể chặt chẽ hơn so với phân rã PCA của các kích hoạt mô hình hay không. Chúng tôi thực hiện điều này thông qua việc vá kích hoạt, một dạng phân tích trung gian nhân quả (Vig et al., 2020), thông qua đó chúng tôi chỉnh sửa các kích hoạt bên trong của mô hình theo các hướng được chỉ ra bởi các đặc trưng từ điển của chúng tôi và đo các thay đổi đối với đầu ra của mô hình. Chúng tôi thấy rằng các đặc trưng từ điển của chúng tôi yêu cầu ít vá hơn để đạt được một mức độ phân kỳ KL nhất định trong tác vụ được nghiên cứu so với các phân rã tương đương (Hình 3).

![Hình 3: (Trái) Số lượng đặc trưng được vá so với phân kỳ KL từ mục tiêu, sử dụng các phân rã luồng dư khác nhau. Chúng tôi thấy rằng việc vá một số lượng tương đối nhỏ các đặc trưng từ điển hiệu quả hơn so với việc vá các thành phần PCA và đặc trưng từ từ điển không thưa α= 0. (Phải) Độ lớn chỉnh sửa trung bình so với phân kỳ KL từ mục tiêu khi chúng tôi tăng số lượng đặc trưng được vá. Chúng tôi thấy rằng các từ điển thưa của chúng tôi cải thiện biên Pareto của độ lớn chỉnh sửa so với tính triệt để của việc chỉnh sửa. Trong cả hai hình, các từ điển đặc trưng được huấn luyện trên 10.000 phần tử đầu tiên của Pile (Gao et al., 2020) (khoảng 7 triệu kích hoạt) sử dụng các giá trị α và R được chỉ ra, trên lớp 11 của Pythia-410M (xem Phụ lục F cho kết quả trên các lớp khác).]

Cụ thể, chúng tôi nghiên cứu hành vi mô hình trên tác vụ Nhận dạng Đối tượng Gián tiếp (IOI) (Wang et al., 2022), trong đó mô hình hoàn thành các câu như "Then, Alice and Bob went to the store. Alice gave a snack to ". Tác vụ này được chọn vì nó nắm bắt một hành vi mô hình đơn giản, đã được nghiên cứu trước đó. Hãy nhớ rằng việc huấn luyện các từ điển đặc trưng của chúng tôi không nhấn mạnh bất kỳ tác vụ cụ thể nào.

## 4.1 ĐIỀU CHỈNH VÁ KÍCH HOẠT CHO CÁC ĐẶC TRƯNG TỪ ĐIỂN

Trong thí nghiệm của chúng tôi, chúng tôi chạy mô hình trên một câu mục tiêu phản thực tế, là một biến thể của câu IOI cơ sở với đối tượng gián tiếp được thay đổi (ví dụ, với "Bob" được thay thế bằng "Vanessa"); lưu các kích hoạt được mã hóa của các đặc trưng từ điển của chúng tôi; và sử dụng các kích hoạt đã lưu để chỉnh sửa luồng dư của mô hình khi chạy trên câu cơ sở.

Cụ thể, chúng tôi thực hiện quy trình sau. Cố định một lớp của mô hình để can thiệp. Chạy mô hình trên câu mục tiêu, lưu logits đầu ra mô hình y và các đặc trưng được mã hóa c̄₁, ..., c̄ₖ của lớp đó tại mỗi trong k token. Sau đó, chạy mô hình trên câu cơ sở thông qua lớp can thiệp, tính toán các đặc trưng được mã hóa c₁, ..., cₖ tại mỗi token, và tại mỗi vị trí thay thế vector luồng dư xᵢ bằng vector được vá

x'ᵢ = xᵢ + ∑(j∈F) (c̄ᵢ,ⱼ - cᵢ,ⱼ)fⱼ

trong đó F là tập con của các đặc trưng mà chúng tôi can thiệp (chúng tôi mô tả quy trình lựa chọn cho F sau trong phần này). Gọi z là logits đầu ra của mô hình khi bạn hoàn thành việc áp dụng nó cho luồng dư được vá x'₁, ..., x'ₖ. Cuối cùng, tính toán phân kỳ KL D_KL(z||y), đo mức độ gần của dự đoán mô hình được vá với mục tiêu. Chúng tôi so sánh những can thiệp này với các can thiệp tương đương sử dụng các thành phần chính được tìm thấy như trong Phần 3.2.

Để chọn tập con đặc trưng F, chúng tôi sử dụng thuật toán Khám phá Mạch Tự động (ACDC) của Conmy et al. (2023). Cụ thể, chúng tôi sử dụng Thuật toán 4.1 của họ trên các đặc trưng của chúng tôi, coi chúng như một đồ thị tính toán phẳng trong đó mỗi đặc trưng đóng góp một thay đổi độc lập cho chỉ số đầu ra D_KL, như được mô tả ở trên và tính trung bình trên một tập thử nghiệm gồm 50 điểm dữ liệu IOI. Kết quả là một thứ tự trên các đặc trưng sao cho việc vá đặc trưng tiếp theo thường dẫn đến mất mát D_KL nhỏ hơn so với mỗi đặc trưng trước đó. Sau đó các tập con đặc trưng F của chúng tôi là k đặc trưng đầu tiên theo thứ tự này. Chúng tôi áp dụng ACDC riêng biệt trên mỗi phân rã.

## 4.2 BẢN ĐỊA HÓA CHÍNH XÁC CÁC ĐẶC TRƯNG TỪ ĐIỂN IOI

Chúng tôi cho thấy trong Hình 3 rằng các từ điển đặc trưng thưa của chúng tôi cho phép cùng một lượng chỉnh sửa mô hình, được đo bằng phân kỳ KL từ mục tiêu, trong ít vá hơn (Trái) và với độ lớn chỉnh sửa nhỏ hơn (Phải) so với phân rã PCA. Chúng tôi cũng cho thấy rằng điều này không xảy ra nếu chúng tôi huấn luyện một từ điển không thưa (α= 0). Tuy nhiên, các từ điển với hệ số độ thưa α lớn hơn có độ chính xác tái tạo tổng thể thấp hơn mà xuất hiện trong Hình 3 như một phân kỳ KL tối thiểu lớn hơn. Trong Hình 3 chúng tôi xem xét các can thiệp trên lớp 11 của luồng dư, và chúng tôi vẽ các can thiệp trên các lớp khác trong Phụ lục F.

# 5 NGHIÊN CỨU TRƯỜNG HỢP

Trong phần này, chúng tôi điều tra các đặc trưng từ điển riêng lẻ, làm nổi bật một số đặc trưng có vẻ tương ứng với một giải thích duy nhất có thể hiểu được của con người (tức là có tính đơn nghĩa). Chúng tôi thực hiện ba phân tích về các đặc trưng từ điển của chúng tôi để xác định ý nghĩa ngữ nghĩa của chúng: (1) Đầu vào: Chúng tôi xác định những token nào kích hoạt đặc trưng từ điển và trong những ngữ cảnh nào, (2) Đầu ra: Chúng tôi xác định việc loại bỏ đặc trưng thay đổi logits đầu ra của mô hình như thế nào, và (3) Đặc trưng trung gian: Chúng tôi xác định các đặc trưng từ điển trong các lớp trước đó gây ra việc kích hoạt đặc trưng được phân tích.

## 5.1 ĐẦU VÀO: CÁC ĐẶC TRƯNG TỪ ĐIỂN CÓ TÍNH ĐƠN NGHĨA CAO

Trước tiên chúng tôi phân tích các hướng từ điển của chúng tôi bằng cách kiểm tra văn bản nào khiến chúng kích hoạt. Một đặc trưng từ điển đơn nghĩa lý tưởng sẽ chỉ kích hoạt trên văn bản tương ứng với một đặc trưng thế giới thực duy nhất, trong khi một đặc trưng từ điển đa nghĩa có thể kích hoạt trong các ngữ cảnh không liên quan.

Để minh họa tốt hơn tính đơn nghĩa của một số đặc trưng từ điển, chúng tôi vẽ biểu đồ của các kích hoạt qua các kích hoạt token. Kỹ thuật này chỉ hoạt động đối với các đặc trưng từ điển kích hoạt cho một tập hợp nhỏ các token. Chúng tôi tìm thấy các đặc trưng từ điển chỉ kích hoạt trên dấu nháy đơn (Hình 4); dấu chấm; token "the"; và ký tự xuống dòng. Đặc trưng dấu nháy đơn trong Hình 4 tương phản với cơ sở mặc định cho luồng dư, trong đó chiều biểu diễn dấu nháy đơn nhiều nhất được hiển thị trong Hình 11 trong Phụ lục D.1; chiều này có tính đa nghĩa vì nó biểu diễn thông tin khác nhau ở các phạm vi kích hoạt khác nhau.

Mặc dù đặc trưng từ điển được thảo luận trong phần trước chỉ kích hoạt đối với dấu nháy đơn, nó không kích hoạt trên tất cả dấu nháy đơn. Điều này có thể được thấy trong Hình 14 và 15 trong Phụ lục D.2, cho thấy hai đặc trưng từ điển kích hoạt dấu nháy đơn khác, nhưng cho các ngữ cảnh khác nhau (chẳng hạn như "[I/We/They]'ll" và "[don/won/wouldn]'t"). Chi tiết về cách chúng tôi tìm kiếm và lựa chọn các đặc trưng từ điển có thể được tìm thấy trong Phụ lục D.3.

![Hình 4: Biểu đồ số lượng token cho đặc trưng từ điển 556. (Trái) Đối với tất cả các điểm dữ liệu kích hoạt đặc trưng từ điển 556, chúng tôi hiển thị số lượng của mỗi token trong mỗi phạm vi kích hoạt. Phần lớn các kích hoạt là dấu nháy đơn, đặc biệt là đối với các kích hoạt cao hơn. Đáng chú ý là các token kích hoạt thấp hơn về mặt khái niệm tương tự với dấu nháy đơn, chẳng hạn như dấu câu khác. (Phải) Chúng tôi hiển thị những dự đoán token nào bị triệt tiêu bởi việc loại bỏ đặc trưng, được đo bằng sự khác biệt trong logits giữa mô hình bị loại bỏ và không bị loại bỏ. Chúng tôi thấy rằng token có dự đoán giảm nhiều nhất là token "s". Lưu ý rằng có 12k logits bị ảnh hưởng tiêu cực, nhưng chúng tôi đặt ngưỡng 0.1 để rõ ràng về mặt thị giác.]

## 5.2 ĐẦU RA: CÁC ĐẶC TRƯNG TỪ ĐIỂN CÓ TÁC ĐỘNG TRỰC QUAN ĐỐI VỚI LOGITS

Ngoài việc xem các token nào kích hoạt đặc trưng từ điển, chúng tôi điều tra cách các đặc trưng từ điển ảnh hưởng đến dự đoán đầu ra của mô hình cho token tiếp theo bằng cách loại bỏ đặc trưng khỏi luồng dư⁵. Nếu đặc trưng từ điển của chúng tôi có thể diễn giải, việc trừ giá trị của nó khỏi luồng dư sẽ có tác động hợp lý đối với dự đoán của token tiếp theo. Chúng tôi thấy trong Hình 4 (Phải) rằng tác động của việc loại bỏ đặc trưng dấu nháy đơn chủ yếu làm giảm logit cho "s" tiếp theo. Điều này phù hợp với những gì người ta mong đợi từ một đặc trưng từ điển phát hiện dấu nháy đơn và được sử dụng bởi mô hình để dự đoán token "s" sẽ xuất hiện ngay sau dấu nháy đơn trong sở hữu cách và từ rút gọn như "let's".

## 5.3 ĐẶC TRƯNG TRUNG GIAN: CÁC ĐẶC TRƯNG TỪ ĐIỂN CHO PHÉP PHÁT HIỆN MẠCH TỰ ĐỘNG

Chúng tôi cũng có thể hiểu các đặc trưng từ điển liên quan đến các đặc trưng từ điển ngược dòng và xuôi dòng: cho một đặc trưng từ điển, các đặc trưng từ điển nào trong các lớp trước đó khiến nó kích hoạt, và các đặc trưng từ điển nào trong các lớp sau đó nó khiến kích hoạt?

Để tự động phát hiện các đặc trưng từ điển liên quan, chúng tôi chọn một đặc trưng từ điển mục tiêu chẳng hạn như đặc trưng của lớp 5 cho các token trong ngoặc đơn dự đoán dấu ngoặc đóng (Hình 5). Đối với đặc trưng từ điển mục tiêu này, chúng tôi tìm kích hoạt tối đa M của nó trên tập dữ liệu của chúng tôi, sau đó lấy mẫu 20 ngữ cảnh khiến đặc trưng mục tiêu kích hoạt trong phạm vi [M/2, M]. Đối với mỗi đặc trưng từ điển trong lớp trước, chúng tôi chạy lại mô hình trong khi loại bỏ đặc trưng này và sắp xếp các đặc trưng lớp trước theo mức độ loại bỏ của chúng làm giảm đặc trưng mục tiêu. Nếu muốn, chúng tôi sau đó có thể áp dụng đệ quy kỹ thuật này cho các đặc trưng từ điển trong lớp trước có tác động lớn. Kết quả của quá trình này tạo thành một cây nhân quả, chẳng hạn như Hình 5.

Là lớp cuối cùng, vai trò của lớp 5 là xuất ra các hướng tương ứng trực tiếp với các token trong ma trận không nhúng. Trên thực tế, khi chúng tôi không nhúng đặc trưng 52027, các token hàng đầu đều là các biến thể dấu ngoặc đóng. Trực quan, các lớp trước sẽ phát hiện tất cả các tình huống đi trước dấu ngoặc đóng, chẳng hạn như ngày tháng, từ viết tắt và cụm từ.

![Hình 5: Mạch cho đặc trưng từ điển dấu ngoặc đóng, với các cách diễn giải của con người về mỗi đặc trưng được hiển thị. Độ dày cạnh chỉ ra sức mạnh của tác động nhân quả giữa các đặc trưng từ điển trong các lớp luồng dư liên tiếp, được đo bằng các loại bỏ. Nhiều đặc trưng từ điển qua các lớp tương ứng với các đặc trưng thế giới thực tương tự và thường chỉ theo các hướng tương tự trong không gian kích hoạt, được đo bằng độ tương tự cosine.]

⁵Cụ thể chúng tôi sử dụng loại bỏ dưới-hạng-một, trong đó chúng tôi hạ thấp vector kích hoạt theo hướng của đặc trưng chỉ đến điểm mà đặc trưng không còn hoạt động.

# 6 THẢO LUẬN

## 6.1 CÔNG TRÌNH LIÊN QUAN

Một số lượng hạn chế các công trình trước đó đã học từ điển của các đặc trưng kích hoạt thưa trong các mô hình đã được huấn luyện trước, bao gồm Yun et al. (2021) và Sharkey et al. (2023), công trình thứ hai đã thúc đẩy công việc này. Tuy nhiên, các phương pháp tương tự đã được áp dụng trong các lĩnh vực khác, đặc biệt trong việc hiểu các nơ-ron trong vỏ não thị giác (Olshausen & Field, 2004; Wright & Ma, 2022).

Ngược lại với phương pháp của chúng tôi, trong đó chúng tôi cố gắng áp đặt độ thưa sau khi huấn luyện, nhiều công trình trước đó đã khuyến khích độ thưa trong mạng neural thông qua các thay đổi đối với kiến trúc hoặc quy trình huấn luyện. Những phương pháp này bao gồm thay đổi cơ chế chú ý (Correia et al., 2019), thêm phạt ℓ1 vào kích hoạt nơ-ron (Kasioumis et al., 2021; Georgiadis, 2019), cắt tỉa nơ-ron (Frankle & Carbin, 2018), và sử dụng hàm softmax làm phi tuyến trong các lớp MLP (Elhage et al., 2022a). Tuy nhiên, việc huấn luyện một mô hình nền tảng tiên tiến với những ràng buộc bổ sung này là khó khăn (Elhage et al., 2022a), và những cải thiện về tính diễn giải không phải lúc nào cũng được thực hiện (Meister et al., 2021).

## 6.2 HẠN CHẾ VÀ CÔNG VIỆC TƯƠNG LAI

Mặc dù chúng tôi đã trình bày bằng chứng rằng các đặc trưng từ điển của chúng tôi có thể diễn giải và quan trọng về mặt nhân quả, chúng tôi không đạt được mất mát tái tạo bằng 0 (Phương trình 4), cho thấy các từ điển của chúng tôi không thể nắm bắt tất cả thông tin trong các kích hoạt của một lớp. Chúng tôi cũng đã xác nhận điều này bằng cách đo độ phức tạp của dự đoán mô hình khi một lớp được thay thế bằng việc tái tạo của nó. Ví dụ, việc thay thế các kích hoạt luồng dư trong lớp 2 của Pythia-70M bằng việc tái tạo các kích hoạt đó của chúng tôi làm tăng độ phức tạp trên Pile (Gao et al., 2020) từ 25 lên 40. Để giảm mất mát thông tin này, chúng tôi muốn khám phá các kiến trúc autoencoder thưa khác và thử tối thiểu hóa thay đổi trong đầu ra mô hình khi thay thế các kích hoạt bằng các vector tái tạo của chúng tôi, thay vì mất mát tái tạo. Những nỗ lực trong tương lai cũng có thể cố gắng cải thiện việc khám phá từ điển đặc trưng bằng cách kết hợp thông tin về trọng số của mô hình hoặc các đặc trưng từ điển được tìm thấy trong các lớp liền kề vào quy trình huấn luyện.

Các phương pháp hiện tại của chúng tôi để huấn luyện autoencoder thưa phù hợp nhất với luồng dư. Có bằng chứng rằng chúng có thể áp dụng được cho MLPs (xem Phụ lục C), nhưng quy trình huấn luyện được sử dụng để huấn luyện các từ điển trong bài báo này không thể học một cách mạnh mẽ các cơ sở quá đầy đủ trong các lớp trung gian của MLP. Chúng tôi hào hứng với công việc tương lai điều tra những thay đổi nào có thể được thực hiện để hiểu rõ hơn các tính toán được thực hiện bởi các đầu chú ý và lớp MLP, mỗi cái đặt ra những thách thức khác nhau.

Trong Phần 4, chúng tôi cho thấy rằng đối với tác vụ IOI, hành vi phụ thuộc vào một số lượng tương đối nhỏ các đặc trưng. Vì từ điển của chúng tôi được huấn luyện theo cách không theo tác vụ, chúng tôi mong đợi kết quả này khái quát hóa cho các tác vụ và hành vi tương tự, nhưng cần nhiều công việc hơn để xác nhận nghi ngờ này. Nếu thuộc tính này khái quát hóa, chúng tôi sẽ có một tập hợp các đặc trưng cho phép hiểu nhiều hành vi mô hình chỉ sử dụng một vài đặc trưng mỗi hành vi. Chúng tôi cũng muốn theo dấu các phụ thuộc nhân quả giữa các đặc trưng trong các lớp khác nhau, với mục tiêu tổng thể là cung cấp một lăng kính để xem các mô hình ngôn ngữ trong đó các phụ thuộc nhân quả là thưa. Điều này hy vọng sẽ là một bước hướng tới mục tiêu cuối cùng của việc xây dựng sự hiểu biết từ đầu đến cuối về cách một mô hình tính toán đầu ra của nó.

## 6.3 KẾT LUẬN

Autoencoder thưa là một phương pháp có thể mở rộng, không giám sát để tháo gỡ các đặc trưng mạng mô hình ngôn ngữ khỏi chồng chất. Phương pháp của chúng tôi chỉ yêu cầu các kích hoạt mô hình không được gán nhãn và sử dụng ít hơn nhiều bậc độ lớn tính toán so với việc huấn luyện các mô hình gốc. Chúng tôi đã chứng minh rằng các đặc trưng từ điển mà chúng tôi học được có tính diễn giải cao hơn bằng tự diễn giải, cho phép chúng tôi xác định chính xác các đặc trưng chịu trách nhiệm cho một hành vi nhất định một cách tinh tế hơn, và có tính đơn nghĩa hơn so với các phương pháp tương đương. Phương pháp này có thể tạo điều kiện cho việc lập bản đồ các mạch mô hình, chỉnh sửa mô hình có mục tiêu, và hiểu rõ hơn về các biểu diễn mô hình.

Một giấc mơ đầy tham vọng trong lĩnh vực diễn giải là an toàn liệt kê (Elhage et al., 2022b): tạo ra một giải thích có thể hiểu được của con người về các tính toán của mô hình dưới dạng một danh sách đầy đủ các đặc trưng của mô hình và do đó cung cấp đảm bảo rằng mô hình sẽ không thực hiện các hành vi nguy hiểm như lừa dối. Chúng tôi hy vọng rằng các kỹ thuật mà chúng tôi trình bày trong bài báo này cũng cung cấp một bước hướng tới việc đạt được tham vọng này.

# LỜI CẢM ơN

Chúng tôi muốn cảm ơn Chương trình Truy cập Nhà nghiên cứu OpenAI cho khoản tài trợ tín dụng mô hình của họ cho tự diễn giải và CoreWeave vì đã cung cấp cho EleutherAI các tài nguyên tính toán cho dự án này. Chúng tôi cũng cảm ở Nora Belrose, Arthur Conmy, Jake Mendel, và Nhóm Diễn giải Tự động OpenAI (Jeff Wu, William Saunders, Steven Bills, Henk Tillman, và Daniel Mossing) cho các cuộc thảo luận có giá trị về thiết kế của các thí nghiệm khác nhau. Chúng tôi cảm ơn Wes Gurnee, Adam Jermyn, Stella Biderman, Leo Gao, Curtis Huebner, Scott Emmons, và William Saunders cho phản hồi của họ về các phiên bản trước của bài báo này. Cảm ơn Delta Hessler vì đã đọc lại. LR được hỗ trợ bởi Quỹ Tương lai Dài hạn. RH được hỗ trợ bởi một khoản tài trợ từ Open Philanthropy. HC đã được hỗ trợ rất nhiều bởi chương trình MATS, được tài trợ bởi AI Safety Support.

# TÀI LIỆU THAM KHẢO

Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397–2430. PMLR, 2023.

Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023), 2023.

Nick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, and Chris Olah. Curve circuits. Distill, 2021. doi: 10.23915/distill.00024.006. https://distill.pub/2020/circuits/curve- circuits.

Arthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adrià Garriga- Alonso. Towards automated circuit discovery for mechanistic interpretability. arXiv preprint arXiv:2304.14997, 2023.

Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2174–2184, 2019.

Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.

Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021.

Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah. Softmax linear units. Transformer Circuits Thread, 2022a. https://transformer-circuits.pub/2022/solu/index.html.

Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022b.

Nelson Elhage, Robert Lasenby, and Chris Olah. Privileged bases in the transformer residual stream, 2023. URL https://transformer-circuits.pub/2023/privileged- basis/index.html. Accessed: 2023-08-07.

Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.

Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biol. Cybern., 20 (3–4):121–136, sep 1975. ISSN 0340-1200. doi: 10.1007/BF00342633. URL https://doi. org/10.1007/BF00342633.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

Georgios Georgiadis. Accelerating convolutional neural networks via activation map compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7085–7095, 2019.

Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An overview of catastrophic ai risks. arXiv preprint arXiv:2306.12001, 2023.

Theodoros Kasioumis, Joe Townsend, and Hiroya Inakoshi. Elite backprop: Training sparse interpretable neurons. In NeSy, pp. 82–93, 2021.

Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efficient sparse coding algorithms. Advances in neural information processing systems, 19, 2006.

Clara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell. Is sparse attention more interpretable? In Annual Meeting of the Association for Computational Linguistics, 2021. URL https://api.semanticscholar.org/CorpusID:235293798.

Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626, 2022.

Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 5(3):e00024–001, 2020.

Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):3311–3325, 1997.

Bruno A Olshausen and David J Field. Sparse coding of sensory inputs. Current opinion in neurobiology, 14(4):481–487, 2004.

Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu. Analysis of the optimization landscapes for overcomplete representation learning. arXiv preprint arXiv:1912.02427, 2019.

Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders, 2023. URL https://www.alignmentforum.org/posts/ z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out- of-superposition. Accessed: 2023-05-10.

Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33:12388–12401, 2020.

Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022.

John Wright and Yi Ma. High-dimensional data analysis with low-dimensional models: Principles, computation, and applications. Cambridge University Press, 2022.

Zeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann LeCun. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. arXiv preprint arXiv:2103.15949, 2021.

# A QUY TRÌNH TỰ DIỄN GIẢI

Quy trình tự diễn giải bao gồm năm bước và tạo ra cả một cách diễn giải và một điểm số tự diễn giải:

1. Trên mỗi trong 50.000 dòng đầu tiên của OpenWebText, lấy một đoạn câu 64-token, và đo kích hoạt của đặc trưng trên mỗi token của đoạn này. Các kích hoạt đặc trưng được chia lại thành các giá trị số nguyên từ 0 đến 10.

2. Lấy 20 đoạn với điểm số kích hoạt cao nhất và chuyển 5 trong số này cho GPT-4, cùng với các kích hoạt trên mỗi token đã được chia lại. Hướng dẫn GPT-4 đưa ra giải thích cho khi nào đặc trưng (hoặc nơ-ron) kích hoạt, dẫn đến một cách diễn giải.

3. Sử dụng GPT-3.5⁶ để mô phỏng đặc trưng trên 5 đoạn kích hoạt cao khác và 5 đoạn được chọn ngẫu nhiên (với biến thể khác không) bằng cách yêu cầu nó cung cấp các kích hoạt trên mỗi token.

4. Tính toán mối tương quan của các kích hoạt được mô phỏng và các kích hoạt thực tế. Mối tương quan này là điểm số tự diễn giải của đặc trưng. Các văn bản được chọn để ghi điểm một đặc trưng có thể là các đoạn văn bản ngẫu nhiên, các đoạn được chọn vì chứa kích hoạt đặc biệt cao của đặc trưng đó, hoặc một hỗn hợp đều của cả hai. Chúng tôi sử dụng một hỗn hợp của cả hai trừ khi có ghi chú khác, cũng được gọi là ghi điểm 'đầu-ngẫu nhiên'.

5. Nếu, trong số 50.000 đoạn, có ít hơn 20 đoạn chứa biến thể khác không trong kích hoạt, thì đặc trưng sẽ được bỏ qua hoàn toàn.

Mặc dù việc sử dụng các đoạn ngẫu nhiên trong Bước 4 cuối cùng là thích hợp hơn với kích thước mẫu đủ lớn, kích thước mẫu nhỏ tổng cộng 640 token được sử dụng để phân tích có nghĩa là một mẫu ngẫu nhiên có thể sẽ không chứa bất kỳ ví dụ kích hoạt cao nào cho tất cả trừ các đặc trưng phổ biến nhất, làm cho việc ghi điểm đầu-ngẫu nhiên trở thành một giải pháp thay thế mong muốn.

⁶Trong khi quy trình được mô tả trong Bills et al. (2023) sử dụng GPT-4 cho bước mô phỏng, chúng tôi sử dụng GPT-3.5. Điều này là do giao thức mô phỏng yêu cầu logprobs của mô hình để ghi điểm, và API công khai của OpenAI cho GPT-3.5 (nhưng không phải GPT-4) hỗ trợ việc trả về logprobs.

# B HUẤN LUYỆN AUTOENCODER THƯA VÀ LỰA CHỌN SIÊU THAM SỐ

Để huấn luyện autoencoder thưa được mô tả trong Phần 2, chúng tôi sử dụng dữ liệu từ Pile (Gao et al., 2020), một kho văn bản web lớn, công khai. Chúng tôi chạy mô hình mà chúng tôi muốn diễn giải trên văn bản này trong khi cache và lưu các kích hoạt tại một lớp cụ thể. Những kích hoạt này sau đó tạo thành một tập dữ liệu, mà chúng tôi sử dụng để huấn luyện các autoencoder. Các autoencoder được huấn luyện với bộ tối ưu hóa Adam với tốc độ học 1e-3 và được huấn luyện trên 5-50M vector kích hoạt trong 1-3 epoch, với các từ điển lớn hơn mất nhiều thời gian hơn để hội tụ. Một lần chạy huấn luyện duy nhất sử dụng lượng dữ liệu này hoàn thành trong dưới một giờ trên một GPU A40 duy nhất.

Khi thay đổi siêu tham số α kiểm soát tầm quan trọng của số hạng mất mát độ thưa, chúng tôi liên tục tìm thấy một sự đánh đổi mượt mà giữa độ thưa và độ chính xác của autoencoder của chúng tôi, như được hiển thị trong Hình 6. Việc thiếu một 'gờ' hoặc 'đầu gối' trong những biểu đồ này cung cấp một số bằng chứng rằng không có một cách duy nhất đúng để phân rã các không gian kích hoạt thành một cơ sở thưa, mặc dù để xác nhận điều này sẽ yêu cầu nhiều thí nghiệm bổ sung. Hình 7 cho thấy hành vi hội tụ của một tập hợp mô hình với α khác nhau qua nhiều epoch.

![Hình 6: Sự đánh đổi giữa số lượng đặc trưng hoạt động trung bình và tỷ lệ phương sai không được giải thích cho MLP tại lớp 0.]

![Hình 7: Sự đánh đổi giữa độ thưa và phương sai không được giải thích trong việc tái tạo của chúng tôi. Mỗi chuỗi điểm là một sweep của siêu tham số α, được huấn luyện cho số epoch được đưa ra trong chú thích.]

# C KẾT QUẢ TỰ DIỄN GIẢI THÊM

## C.1 TÍNH DIỄN GIẢI NHẤT QUÁN QUA CÁC KÍCH THƯỚC TỪ ĐIỂN

Chúng tôi thấy rằng điểm số diễn giải cao hơn của các từ điển đặc trưng của chúng tôi không giới hạn ở các từ điển quá đầy đủ (trong đó tỷ lệ, R, của các đặc trưng từ điển với các chiều mô hình là >1), mà xảy ra ngay cả trong các từ điển nhỏ hơn cơ sở cơ bản, như được hiển thị trong Hình 8. Những từ điển nhỏ này có thể tái tạo các vector kích hoạt ít chính xác hơn, vì vậy với mỗi đặc trưng có tính diễn giải tương tự, các từ điển lớn hơn sẽ có thể giải thích nhiều phương sai tổng thể hơn.

## C.2 ĐIỂM SỐ DIỄN GIẢI CAO KHÔNG PHẢI LÀ SẢN PHẨM CỦA VIỆC GHI ĐIỂM HÀNG ĐẦU

Một mối quan tâm có thể có là phương pháp tự diễn giải được mô tả trong Phần 3 kết hợp các đoạn kích hoạt hàng đầu (thường lớn) với các kích hoạt ngẫu nhiên (thường nhỏ), làm cho việc xác định các kích hoạt tương đối dễ dàng. Theo dẫn dắt của Bills et al. (2023), chúng tôi kiểm soát điều này bằng cách tính toán lại điểm số tự diễn giải bằng cách sửa đổi Bước 3 chỉ sử dụng các đoạn được chọn ngẫu nhiên. Với kích thước mẫu lớn, việc sử dụng các đoạn ngẫu nhiên nên là thử nghiệm thực sự về khả năng diễn giải một đặc trưng tiềm năng. Tuy nhiên, các đặc trưng mà chúng tôi đang xem xét có đuôi nặng, vì vậy với kích thước mẫu hạn chế, chúng tôi nên mong đợi các mẫu ngẫu nhiên sẽ đánh giá thấp mối tương quan thực sự.

Trong Hình 9 chúng tôi hiển thị điểm số tự diễn giải cho các đoạn chỉ sử dụng các đoạn ngẫu nhiên. Phù hợp với Bills et al. (2023), chúng tôi thấy rằng điểm số chỉ-ngẫu nhiên nhỏ hơn đáng kể so với điểm số đầu-và-ngẫu nhiên, nhưng cũng rằng các đặc trưng đã học của chúng tôi vẫn vượt trội so với các đường cơ sở một cách nhất quán, đặc biệt là trong các lớp đầu. Vì các đặc trưng đã học của chúng tôi thưa hơn so với các đường cơ sở và do đó, kích hoạt ít hơn trên một đoạn nhất định, điều này có khả năng đánh giá thấp hiệu suất của mã hóa thưa so với các đường cơ sở.

Một mối quan tâm tiềm năng bổ sung là cấu trúc của các autoencoder cho phép chúng nhạy cảm với ít hơn một hướng đầy đủ trong không gian kích hoạt, dẫn đến so sánh không công bằng. Chúng tôi cho thấy trong Phụ lục G rằng đây không phải là nguồn gốc của hiệu suất được cải thiện của mã hóa thưa.

![Hình 8: So sánh điểm số diễn giải trung bình qua các kích thước từ điển. Tất cả các từ điển được huấn luyện trên 20M vector kích hoạt thu được bằng cách chạy Pythia-70M trên Pile với α=.00086.]

![Hình 9: Điểm số diễn giải chỉ-ngẫu nhiên qua mỗi lớp, một thước đo về mức độ diễn giải của cụm kích hoạt hàng đầu có thể giải thích toàn bộ phạm vi kích hoạt.]

![Hình 10: Điểm số diễn giải đầu-và-ngẫu nhiên và chỉ-ngẫu nhiên qua mỗi lớp MLP, sử dụng hệ số ℓ1 α= 3.2e−4 và tỷ lệ kích thước từ điển R= 1.]

Mặc dù luồng dư thường có thể được coi là một không gian vector không có cơ sở đặc quyền (một cơ sở trong đó chúng ta mong đợi các thay đổi sẽ có ý nghĩa đặc biệt, chẳng hạn như cơ sở tiêu chuẩn sau một phi tuyến trong MLP), đã được lưu ý rằng có xu hướng cho transformer lưu trữ thông tin trong cơ sở luồng dư (Dettmers et al., 2022), được cho là do bộ tối ưu hóa Adam lưu gradient với độ chính xác hữu hạn trong cơ sở dư (Elhage et al., 2023). Chúng tôi không thấy các hướng cơ sở luồng dư có tính diễn giải cao hơn so với các hướng ngẫu nhiên.

## C.3 DIỄN GIẢI LỚP CON MLP

Phương pháp của chúng tôi về việc học một từ điển đặc trưng và diễn giải các đặc trưng kết quả có thể, về nguyên tắc, được áp dụng cho bất kỳ tập hợp các kích hoạt bên trong nào của một mô hình ngôn ngữ, không chỉ luồng dư. Việc áp dụng phương pháp của chúng tôi cho lớp con MLP của một transformer đã có kết quả hỗn hợp. Phương pháp của chúng tôi vẫn tìm thấy nhiều đặc trưng có tính diễn giải cao hơn so với các nơ-ron. Tuy nhiên, kiến trúc của chúng tôi cũng học nhiều đặc trưng chết, không bao giờ kích hoạt trên toàn bộ kho dữ liệu. Trong một số trường hợp, có quá nhiều đặc trưng chết đến mức tập hợp các đặc trưng sống không tạo thành một cơ sở quá đầy đủ. Ví dụ, trong một từ điển với số đặc trưng gấp đôi số nơ-ron, ít hơn một nửa có thể hoạt động đủ để thực hiện diễn giải tự động. Ngoại lệ đối với điều này là các lớp đầu, trong đó một phần lớn trong số chúng hoạt động.

Để học các đặc trưng trong các lớp MLP, chúng tôi thấy rằng chúng tôi giữ lại một số lượng đặc trưng lớn hơn nếu chúng tôi sử dụng một ma trận khác cho bộ mã hóa và bộ giải mã, để Phương trình 1 và 2 trở thành

c = ReLU(Me x + b)     (5)
x̂ = MT d c            (6)

Chúng tôi hiện đang làm việc trên các phương pháp để vượt qua điều này và tìm các cơ sở thực sự quá đầy đủ trong các lớp MLP giữa và sau.

## C.4 ĐIỂM SỐ DIỄN GIẢI TƯƠNG QUAN VỚI KURTOSIS VÀ ĐỘ LỆCH CỦA KÍCH HOẠT

Đã được chỉ ra rằng việc tìm kiếm các từ điển thưa, quá đầy đủ có thể được tái tạo lại dưới dạng tìm kiếm các hướng tối đa hóa chuẩn ℓ4 (Qu et al., 2019).

Chúng tôi cung cấp một thử nghiệm về tính hữu ích của điều này bằng cách phân tích mối tương quan giữa tính diễn giải và một số thuộc tính của các hướng đã học. Chúng tôi thấy rằng có mối tương quan 0.19 và 0.24 giữa mức độ lệch dương và kurtosis tương ứng mà các kích hoạt đặc trưng có và điểm số diễn giải đầu-và-ngẫu nhiên của chúng, như được hiển thị trong Bảng 2.

Điều này cũng phù hợp với giải thích trực quan rằng mức độ can thiệp do các đặc trưng hoạt động khác sẽ được phân phối gần như bình thường theo định lý giới hạn trung tâm. Nếu đây là trường hợp, thì các đặc trưng sẽ đáng chú ý vì tính đuôi nặng của chúng.

| Moment | Mối tương quan với điểm số diễn giải đầu-ngẫu nhiên |
|---------|---------------------------------------------------|
| Trung bình | -0.09 |
| Phương sai | 0.02 |
| Độ lệch | 0.20 |
| Kurtosis | 0.15 |

Bảng 2: Mối tương quan của điểm số diễn giải với các moment đặc trưng qua kết quả luồng dư, tất cả các lớp, với tỷ lệ kích thước từ điển R∈ {0.5,1,2,4,8}.

Điều này cũng giải thích tại sao Phân tích Thành phần Độc lập (ICA), tối đa hóa tính không-Gaussian của các thành phần được tìm thấy, là giải pháp thực hiện tốt nhất trong số các lựa chọn thay thế mà chúng tôi đã xem xét.

# D PHÂN TÍCH ĐẶC TRƯNG ĐỊNH TÍNH

## D.1 CƠ SỞ LUỒNG DƯ

Hình 11 đưa ra biểu đồ kích hoạt token của cơ sở luồng dư. Kết nối chiều luồng dư này với đặc trưng dấu nháy đơn từ Hình 4, chiều dư này là chiều cao thứ 10 được đọc từ luồng dư bởi đặc trưng của chúng tôi⁷.

![Hình 11: Biểu đồ số lượng token trong cơ sở nơ-ron. Mặc dù có một phần lớn dấu nháy đơn trong phạm vi kích hoạt trên, điều này chỉ giải thích một phần rất nhỏ phương sai cho các phạm vi kích hoạt trung bình đến thấp.]

## D.2 VÍ DỤ VỀ CÁC ĐẶC TRƯNG ĐÃ HỌC

Các đặc trưng khác được hiển thị trong Hình 12, 13, 14, và 15.

![Hình 12: Đặc trưng 'If' trong các ngữ cảnh mã hóa]

![Hình 13: Đặc trưng cấp token 'Dis' hiển thị các bigram, chẳng hạn như 'disCLAIM', 'disclosed', 'disordered', v.v.]

![Hình 14: Đặc trưng dấu nháy đơn trong các ngữ cảnh "[I/We/They]'ll".]

![Hình 15: Đặc trưng dấu nháy đơn trong các ngữ cảnh "[don/won/wouldn]'t".]

## D.3 CHI TIẾT TÌM KIẾM ĐẶC TRƯNG

Chúng tôi tìm kiếm đặc trưng dấu nháy đơn bằng cách sử dụng câu "I don't know about that. It is now up to Dave"', và xem đặc trưng (hoặc chiều luồng dư) nào kích hoạt nhiều nhất cho token dấu nháy đơn cuối cùng. Đặc trưng kích hoạt hàng đầu trong từ điển của chúng tôi là một đặc trưng chiều ngoại lai (tức là, một hướng đặc trưng chủ yếu đọc từ một chiều ngoại lai của luồng dư), các dấu nháy đơn sau O (và dự đoán O'Brien, O'Donnell, O'Connor, O'clock, v.v.), sau đó là đặc trưng dấu-nháy-đơn-đi-trước-s.

Đối với chiều cơ sở dư, chúng tôi tìm kiếm các chiều kích hoạt tối đa và tối thiểu (vì luồng dư có thể vừa dương vừa âm), trong đó hai chiều dương hàng đầu là các chiều ngoại lai, hai chiều âm hàng đầu là chiều hiển thị của chúng tôi và một chiều ngoại lai khác, tương ứng.

## D.4 CÁC PHƯƠNG PHÁP DIỄN GIẢI THẤT BẠI

Chúng tôi đã thử một phương pháp dựa trên trọng số đi từ từ điển trong lớp 4 đến từ điển trong lớp 5 bằng cách nhân một đặc trưng với MLP và kiểm tra độ tương tự cosine với các đặc trưng trong lớp 5. Không có kết nối có ý nghĩa nào. Ngoài ra, không rõ cách áp dụng điều này cho lớp con Attention vì chúng tôi cần xem đặc trưng ở chiều vị trí nào. Chúng tôi mong đợi điều này thất bại bằng cách đi ra ngoài phân phối.

⁷Chín đặc trưng đầu tiên không có dấu nháy đơn trong các kích hoạt hàng đầu của chúng như chiều 21.

# E SỐ LƯỢNG ĐẶC TRƯNG HOẠT ĐỘNG

Trong Hình 16 chúng tôi thấy rằng, đối với luồng dư, chúng tôi học một cách nhất quán các từ điển ít nhất là 4x quá đầy đủ trước khi một số đặc trưng bắt đầu bỏ ra hoàn toàn, với các siêu tham số đúng. Đối với các lớp MLP bạn thấy số lượng lớn các đặc trưng chết ngay cả với siêu tham số α= 0. Những hình này đã thông báo việc lựa chọn α= 8.6e−4 và α= 3.2e−4 đã được đưa vào các biểu đồ trong Phần 3 cho luồng dư và MLP tương ứng. Do phần lớn của không gian đầu vào không bao giờ được sử dụng do phi tuyến, việc các đặc trưng từ điển MLP bị kẹt ở vị trí mà chúng hầu như không bao giờ kích hoạt dễ dàng hơn nhiều. Trong tương lai chúng tôi dự định khởi tạo lại những 'đặc trưng chết' như vậy để đảm bảo rằng chúng tôi học được nhiều đặc trưng từ điển hữu ích nhất có thể.

![Hình 16: Số lượng đặc trưng hoạt động, được định nghĩa là kích hoạt hơn 10 lần trên 10M điểm dữ liệu, thay đổi với siêu tham số độ thưa α và tỷ lệ kích thước từ điển R.]

# F CHỈNH SỬA HÀNH VI IOI TRÊN CÁC LỚP KHÁC

Trong Hình 17 chúng tôi hiển thị kết quả của quy trình trong Phần 4 qua một phạm vi các lớp trong Pythia-410M.

![Hình 17: Phân kỳ từ đầu ra mục tiêu so với số lượng đặc trưng được vá và độ lớn của các chỉnh sửa cho các lớp 3, 7, 11, 15, 19 và 23 của luồng dư của Pythia-410M. Pythia-410M có 24 lớp, mà chúng tôi đánh chỉ mục 0, 1, ..., 23.]

# G SO SÁNH TOPK

Như đã đề cập trong Phần 3, các hướng so sánh được học bởi mã hóa thưa và những hướng trong các đường cơ sở không hoàn toàn đồng đều. Điều này là do, ví dụ, một hướng PCA hoạt động đối với toàn bộ nửa không gian ở một bên của một siêu phẳng qua gốc tọa độ, trong khi một đặc trưng mã hóa thưa kích hoạt trên ít hơn một hướng đầy đủ, chỉ ở phía xa của một siêu phẳng không cắt gốc tọa độ. Điều này là do độ lệch được áp dụng trước khi kích hoạt, mà trong thực tế, luôn là âm. Để kiểm tra liệu sự khác biệt này có chịu trách nhiệm cho điểm số cao hơn hay không, chúng tôi chạy một biến thể của PCA và ICA trong đó chúng tôi có một số cố định các hướng, K, có thể hoạt động cho bất kỳ điểm dữ liệu đơn lẻ nào. Chúng tôi đặt K này bằng số lượng đặc trưng hoạt động trung bình cho một từ điển mã hóa thưa với tỷ lệ R= 1 và α= 8.6e−4 được huấn luyện trên lớp được đề cập. Chúng tôi so sánh kết quả trong Hình 18, cho thấy thay đổi này không giải thích hơn một phần nhỏ của sự cải thiện trong điểm số.

![Hình 18: Điểm số tự diễn giải qua các lớp cho luồng dư, bao gồm các đường cơ sở top-K cho ICA và PCA.]
