# 2404.06910.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2404.06910.pdf
# Kích thước tệp: 1171335 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất
Thomas Merth1Qichen Fu1Mohammad Rastegari*2Mahyar Najibi1

Tóm tắt
Mặc dù có những thành công của các mô hình ngôn ngữ lớn (LLM), chúng thể hiện những nhược điểm đáng kể, đặc biệt khi xử lý ngữ cảnh dài. Chi phí suy luận của chúng tăng theo bậc hai so với độ dài chuỗi, khiến chúng trở nên đắt đỏ để triển khai trong một số ứng dụng xử lý văn bản thực tế, chẳng hạn như sinh trả lời có tăng cường truy xuất (RAG). Ngoài ra, các LLM cũng thể hiện "hiện tượng phân tán", trong đó ngữ cảnh không liên quan trong prompt làm giảm chất lượng đầu ra. Để giải quyết những nhược điểm này, chúng tôi đề xuất một phương pháp prompting RAG mới, superposition prompting, có thể được áp dụng trực tiếp cho các LLM dựa trên transformer đã được huấn luyện trước mà không cần fine-tuning. Ở mức độ cao, superposition prompting cho phép LLM xử lý các tài liệu đầu vào trong các đường dẫn prompt song song, loại bỏ các đường dẫn khi chúng được coi là không liên quan. Chúng tôi chứng minh khả năng của phương pháp để đồng thời tăng cường hiệu quả thời gian trên nhiều benchmark trả lời câu hỏi sử dụng nhiều LLM được huấn luyện trước. Hơn nữa, kỹ thuật của chúng tôi cải thiện đáng kể độ chính xác khi ngữ cảnh được truy xuất lớn so với ngữ cảnh mà mô hình được huấn luyện. Ví dụ, cách tiếp cận của chúng tôi tạo ra sự giảm 93× về thời gian tính toán trong khi cải thiện độ chính xác 43% trên tập dữ liệu NaturalQuestions-Open với mô hình MPT-7B instruction-tuned so với RAG ngây thơ.

1. Giới thiệu
Các mô hình ngôn ngữ lớn tự hồi quy dựa trên Transformer (LLM) đã dẫn đến những bước tiến vượt bậc trong hiệu suất mô hình hóa văn bản so với các phương pháp trước đây (Zhao et al., 2023). Tuy nhiên, chúng có yêu cầu tính toán khổng lồ, đặc biệt khi độ dài ngữ cảnh tăng do chi phí tính toán bậc hai của self-attention. Nhiều nghiên cứu trước đây đã khám phá cách tăng tốc suy luận LLM (Huang et al., 2023; Miao et al., 2023). Tuy nhiên, những tối ưu hóa như vậy thường yêu cầu thay đổi kiến trúc hoặc tham số đáng kể cho mô hình được huấn luyện trước, do đó đòi hỏi các quy trình huấn luyện lại hoặc fine-tuning tốn kém. Ngoài việc gây ra yêu cầu tính toán không mong muốn, ngữ cảnh đầu vào dài cũng có thể dẫn đến ảo giác và/hoặc phản hồi phân kỳ trong đầu ra mô hình (Liu et al., 2023a; Shi et al., 2023).

Sinh trả lời có tăng cường truy xuất (RAG) là một ứng dụng hấp dẫn của các LLM dựa trên transformer. Trong bối cảnh này, LLM có thể căn cứ phản hồi của nó vào ngữ cảnh phụ trợ, có liên quan. Thông thường, các tài liệu được truy xuất chứa văn bản dạng dài, dẫn đến những nhược điểm đã nêu ở trên (Gao et al., 2023). Để cải thiện và tăng tốc RAG, chúng tôi đề xuất superposition prompting1. Superposition prompting được chứng minh là

1Chúng tôi lấy cảm hứng từ công thức "tích phân đường dẫn" của cơ học lượng tử (Feynman, 1965), trong đó động lực học của một hạt

--- TRANG 2 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất

Câu hỏi: khi nào as you like it được biểu diễn lần đầu?
(Tiêu đề: As You Like It) ban nhạc Johnny Flynn and The Sussex Wit. Việc sản xuất bao gồm Pippa Nixon trong vai Rosalind, Luke Norris trong vai Orlando, Adrian Scarborough trong vai Touchstone…
(Tiêu đề: Live Not as You Would Like To) A.N.Ostrovsky" trong hai tập được xuất bản bởi Count Grigory Kushelev-Bezborodko. "Live Not As You Would Like To" ra mắt tại…
(Tiêu đề: Exactly Like You (musical)) Ward trong vai Arlene Murphy, Susan Mansur trong vai Pricilla Vanerhosen và Michael McGrath trong vai Martin Murphy. Exactly Like You (musical) Exactly Like You…
(Tiêu đề: As You Like It) As You Like It là một hài kịch điền viên của William Shakespeare được tin là đã được viết vào năm 1599. Buổi biểu diễn đầu tiên của vở kịch không chắc chắn... 1603 đã được đề xuất như một khả năng.
### Phản hồi:
As You Like It có thể đã được biểu diễn lần đầu vào năm 1603.

[Hình ảnh minh họa so sánh superposition prompting với paradigm "cổ điển" (Naive LLM-RAG)]

Hình 2. So sánh superposition prompting với paradigm prompting "cổ điển" (Naive LLM-RAG). Hình vuông đại diện cho một token, và mũi tên mô tả các phụ thuộc attention. Trong khi cách tiếp cận cổ điển là DAG kiểu "danh sách liên kết", superposition prompting sắp xếp các phụ thuộc token sao cho tất cả tài liệu được xử lý độc lập. Do cấu trúc phụ thuộc này, chúng ta có thể dễ dàng tận dụng các logit LLM để cắt tỉa ngữ cảnh không liên quan, cải thiện lý luận ngữ cảnh dài. Cấu trúc phụ thuộc cũng cho phép xử lý prompt nhanh hơn, do các cơ hội mới cho caching và song song hóa của KV cache và tính toán logit (mỗi hộp xám đại diện, về mặt logic, một "batch" được xử lý bởi LLM, tái sử dụng KV cache thượng nguồn).

đồng thời cải thiện độ chính xác mô hình và hiệu quả thời gian tính toán cho các tác vụ trả lời câu hỏi dựa trên RAG mà không cần huấn luyện hoặc fine-tuning bổ sung. Để làm nổi bật kết quả của chúng tôi, chúng tôi xin tham khảo Hình 1.

Trong nghiên cứu này, các đóng góp của chúng tôi như sau; (1) chúng tôi đề xuất một khung tổng quát mới để prompting LLM trong các tình huống RAG, (2) chúng tôi chứng minh lợi ích của phương pháp trên các tập dữ liệu trả lời câu hỏi, và (3) chúng tôi cung cấp bằng chứng thực nghiệm và nghiên cứu ablation rộng rãi để tăng thêm niềm tin vào các quyết định thiết kế của chúng tôi. Chúng tôi cũng đề xuất các tối ưu hóa thực tế bổ sung để tăng tốc suy luận bằng cách cắt tỉa, caching và song song hóa tính toán của các đường dẫn prompt. Những tối ưu hóa này có thể thực hiện được do cấu trúc tôpô của các superposition prompt của chúng tôi.

Để tái tạo, triển khai của chúng tôi có thể được tìm thấy tại https://github.com/apple/ml-superposition-prompting.

2. Nghiên cứu liên quan

Sinh Trả Lời có Tăng Cường Truy Xuất. Sinh trả lời có tăng cường truy xuất (RAG) là một ứng dụng phổ biến của LLM để tạo ra câu trả lời cho câu hỏi dựa trên một tập hợp các tài liệu được truy xuất (Lewis et al., 2020). Thay vì chỉ đơn giản prompting mô hình ngôn ngữ với một truy vấn, RAG tăng cường prompt bằng cách chèn một tập hợp các tài liệu được truy xuất vào prompt. Nếu được thực hiện đúng, những tài liệu này chứa kiến thức hữu ích liên quan đến truy vấn, điều này sẽ gợi ra đầu ra chính xác và đáng tin cậy hơn từ mô hình. Nghiên cứu rộng rãi (Lewis et al., 2020; Guu et al., 2020; Borgeaud et al., 2021b; Gao et al., 2023; Asai et al., 2023) đã cho thấy RAG hiệu quả cho nhiều tác vụ chuyên sâu về kiến thức (Petroni et al., 2020). Tuy nhiên, việc kết hợp các tài liệu được truy xuất làm tăng đáng kể độ dài chuỗi đầu vào và gây ra overhead tính toán bổ sung, làm tăng mối quan tâm về hiệu quả. Giải quyết các thách thức của xử lý ngữ cảnh dài và hiệu quả cho RAG đã trở thành trọng tâm chính trong nghiên cứu gần đây (Guu et al., 2020; Beltagy et al., 2020; Ratner et al., 2022).

Xử Lý Ngữ Cảnh Dài Hiệu Quả. Đã có những nỗ lực đáng kể để giảm dung lượng bộ nhớ và chi phí tính toán của transformer sử dụng các kỹ thuật như nén và KV-caching (Sheng et al., 2023; Lin et al., 2023; Xiao et al., 2022). Các phiên bản hiệu quả hơn của kiến trúc transformer cũng đã được khám phá. Ví dụ, Longformer (Beltagy et al., 2020) đã giới thiệu một sự thay thế drop-in cho self-attention tiêu chuẩn, khiến nó mở rộng tuyến tính với độ dài chuỗi. Tương tự, Reformer (Kitaev et al., 2020) sử dụng locality-sensitive hashing để giảm độ phức tạp của attention và cải thiện hiệu quả cho các chuỗi dài. Song song, SparseTransformer (Child et al., 2019) tập trung vào tính thưa thớt của các lớp attention.

--- TRANG 3 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất

Mặc dù những đổi mới trên giải quyết hiệu quả của xử lý ngữ cảnh dài, chúng thường yêu cầu thay đổi kiến trúc không tầm thường và/hoặc huấn luyện lại. Điều này khiến chúng không thực tế để sử dụng với các LLM đã được huấn luyện trước hiện có (Touvron et al., 2023; Zhang et al., 2022). Gần với công việc của chúng tôi hơn là các phương pháp hiệu quả tối ưu hóa KV caching và xem xét tầm quan trọng của token (Zhang et al., 2023; Liu et al., 2023c). Các nghiên cứu khác (trực giao với của chúng tôi) điều tra cách cải thiện hiệu quả của sinh đầu ra LLM (Ning et al., 2023). Các phương pháp trên khác với của chúng tôi vì chúng điều tra tăng tốc cho LLM nói chung, trong khi chúng tôi nhằm tận dụng đặc điểm cụ thể của bối cảnh RAG để đạt được những cải thiện xa hơn.

Gần nhất với công việc của chúng tôi là Prompt Cache được đề xuất gần đây (Gim et al., 2023). Phương pháp này cũng tận dụng cấu trúc modular của bối cảnh RAG để thực hiện local attention trên preamble và tài liệu độc lập và cache kết quả. Ngược lại, phương pháp của chúng tôi giữ lại các phụ thuộc attention giữa các phân đoạn dưới dạng đồ thị phụ thuộc. Cũng khác biệt, chúng tôi đề xuất các cơ chế cắt tỉa và song song hóa không được khám phá bởi Gim et al., 2023.

Prompt Engineering. Prompt engineering là quá trình thiết kế và điều chỉnh prompt một cách có chủ ý trước khi đưa chúng vào các mô hình ngôn ngữ để tạo văn bản (Liu et al., 2023b). Khám phá trước đây (Bubeck et al., 2023) cho thấy cách xây dựng prompt cẩn thận có thể cải thiện đáng kể phản hồi của mô hình. Thú vị là nghiên cứu gần đây "Lost in the Middle" (Liu et al., 2023a) đã cho thấy chỉ riêng vị trí của "tài liệu vàng" (tài liệu chứa câu trả lời) trong ngữ cảnh dài đã ảnh hưởng đáng kể đến hiệu suất của các mô hình ngôn ngữ. Một chủ đề khác của các nghiên cứu prompt engineering đã khám phá cách sử dụng các cấu trúc giống đồ thị khi prompting LLM. Phương pháp đề xuất của chúng tôi có thể thoạt nhìn giống hệt với các phương pháp prompting "dựa trên cây" và "dựa trên đồ thị" khác, chẳng hạn như Tree of Thoughts (Yao et al., 2023) và Graph of Thoughts (Besta et al., 2023). Tuy nhiên, những phương pháp này được đề xuất trong bối cảnh lý luận nhiều bước, không phải RAG. Khác với những phương pháp trên, Parallel Context Windows (Ratner et al., 2022)—cùng với các nghiên cứu "structured attention" khác (Cai et al., 2023; Ye et al., 2023)—nhằm xây dựng các phụ thuộc giữa các phân đoạn văn bản prompt. Tuy nhiên, những nghiên cứu này thường được áp dụng cho các ứng dụng few-shot learning, không phải sinh trả lời có tăng cường truy xuất. Cách tiếp cận của chúng tôi cũng khác với những bài báo structured attention này ở chỗ chúng tôi hoạt động trên đồ thị acyclic có hướng tổng quát, trái ngược với chỉ trường hợp đặc biệt của cây.

3. Phương pháp đề xuất

Tác vụ sinh trả lời có tăng cường truy xuất bao gồm các phân đoạn văn bản riêng biệt—preamble (hay còn gọi là system prompt), một corpus tài liệu (tĩnh), và một truy vấn do người dùng cung cấp (động). Thay vì nối các phân đoạn văn bản này trong không gian văn bản, chúng tôi nhóm chúng thành các "batch" riêng biệt (các hộp xám trong Hình 2), được truyền dưới dạng các lời gọi đến LLM (tái sử dụng KV cache từ các phân đoạn token thượng nguồn). Với một truy vấn làm đầu vào, superposition prompting xử lý tất cả các lựa chọn tài liệu được ghép nối với truy vấn độc lập (có điều kiện trên preamble)—trong Hình 2, điều này có thể được thấy như cấu trúc phân nhánh. Một khi batch truy vấn được xử lý, chúng tôi sau đó sử dụng path pruning (Phần 3.2.3) để loại bỏ toàn bộ các phụ thuộc attention dựa trên một chỉ số quan trọng (chiếc kéo trong Hình 2). Cả hai tối ưu hóa này đều cải thiện hiệu quả suy luận và cho phép mô hình loại bỏ các tài liệu gây phân tâm không liên quan đến truy vấn.

Được hỗ trợ bởi cấu trúc bổ sung của cách tiếp cận superposition prompting của chúng tôi, chúng tôi sau đó đề xuất các kỹ thuật để tăng tốc hơn nữa suy luận. Đầu tiên, khái niệm cấp cao về chia sẻ token trên các prompt cho phép chúng tôi sử dụng prompt path caching (Phần 3.3.1). Cuối cùng, chúng tôi mô tả một chiến lược prompt path parallelization (Phần 3.3.2) tận dụng tính độc lập giữa các phân đoạn.

3.1. Sinh Trả Lời có Tăng Cường Truy Xuất

Chúng tôi cách điệu hóa chuỗi token dưới dạng vector in đậm và sử dụng ⊕ để biểu thị phép nối theo chiều chuỗi. Giả sử có nd tài liệu offline (đã được token hóa trước) sẵn có để truy xuất, chúng tôi định nghĩa tập hợp các chuỗi token tài liệu {d1, . . . ,dnd}. Chúng tôi ký hiệu truy vấn người dùng là q, và chuỗi preamble tùy chỉnh của chúng tôi là p. Mục tiêu là trả về một phản hồi r nào đó trả lời truy vấn, tất cả trong khi tối thiểu hóa độ trễ giữa việc nhận truy vấn và phục vụ phản hồi như quan sát bởi client.

Giải pháp baseline rõ ràng (mà chúng tôi gọi là "Naive LLM-RAG") là nơi một người chỉ đơn giản nối các chuỗi đầu vào như x=p⊕d1⊕ ··· ⊕ dnd⊕q, sau đó tự động sinh r sử dụng x làm prompt. Tuy nhiên, như được hiển thị trong Phần 4, cách tiếp cận của chúng tôi vượt trội massively so với baseline như vậy cả về chất lượng và hiệu suất.

3.2. Superposition Prompting

Bây giờ chúng tôi mô tả chi tiết superposition prompting, một paradigm mới để prompting các mô hình ngôn ngữ. Trong superposition prompting, các prompt không được biểu diễn dưới dạng một chuỗi token đơn giản như chúng với các phương pháp prompting "cổ điển" (ví dụ Naive LLM-RAG). Thay vào đó, các superpositioned prompt là đồ thị acyclic có hướng (DAG) trong đó các nút là chuỗi token, và các cạnh mã hóa các phụ thuộc attention. Nói một cách rõ ràng, một chuỗi token cụ thể, v, attend to các token trong một chuỗi token khác, u, khi và chỉ khi có một đường dẫn từ u đến v trong DAG. Theo nghĩa này, superposition prompting là một tổng quát hóa của prompting "cổ điển" (vì một prompt "cổ điển" là trường hợp đặc biệt của danh sách liên kết). Vui lòng tham khảo Thuật toán 3 để có một công thức hóa thuật toán.

--- TRANG 4 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất

[Hình 3 mô tả các phụ thuộc attention ngầm phải được tính toán trong "online serving"]

Hình 3. Các phụ thuộc attention ngầm phải được tính toán trong "online serving" (các màu trong (b)-(f) tương ứng với màu phân đoạn token trong Hình 2). Lưu ý cách các tối ưu hóa khác nhau giảm gánh nặng tính toán cần thiết tại thời điểm online serving bằng cách cắt tỉa, precomputing và song song hóa công việc. Đáng nhấn mạnh lại là trong thực tế, suy luận không phải là sparse attention trên một chuỗi lớn, mà là dense attention với nhiều phân đoạn token ngắn hơn khác nhau.

3.2.1. TÔPÔ ĐƯỜNG DẪN PROMPT FORKJOIN

Để tận dụng superposition prompting cho RAG, chúng ta phải xây dựng một tôpô đồ thị từ các phân đoạn văn bản của chúng ta. Chúng tôi đề xuất cấu trúc đồ thị ForkJoin, được mô tả trong Hình 2. Lưu ý rằng mỗi chuỗi qi là một bản sao của q gốc (Phần 3.2.3 sẽ biện minh quyết định này). Mặc dù việc nhân bản này cuối cùng làm tăng số lượng token được xử lý, phân tích ablation của chúng tôi trong Phụ lục B.1 chứng minh tính ưu việt của cách tiếp cận này về độ chính xác. Hơn nữa, Phần 3.3.2 mô tả cách chi phí của việc nhân bản này có thể được ẩn hoàn toàn khỏi người dùng. Tôpô ForkJoin (ngầm) dẫn đến cấu trúc "local attention" giả (Hình 3). Chúng tôi nhấn mạnh rằng mẫu attention kết quả này là một cấu trúc chỉ để hình dung—trong thực tế, tất cả các lời gọi đến LLM sử dụng dense attention hoàn toàn, mặc dù trên độ dài ngữ cảnh tương đối nhỏ hơn. Cụ thể, mỗi hộp đứt nét trong Hình 2 là một lời gọi LLM riêng biệt.

3.2.2. PHÂN CÔNG VỊ TRÍ TOKEN

Với prompting cổ điển, các token được (theo mặc định) cách đều nhau, với khoảng cách là 1. Tuy nhiên, với superposition prompting, việc định vị token không tầm thường, vì các đường dẫn (có độ dài có thể khác nhau) chạy song song với nhau. Do đó, chúng ta tìm cách trả lời câu hỏi, "làm thế nào để chúng ta gán vị trí có ý nghĩa cho các token trong superposition prompt?"

Một cách tiếp cận đơn giản có thể là cắt ngắn chuỗi token đến độ dài chung để thực thi một vị trí chia sẻ. Tuy nhiên, việc cắt ngắn có thể dẫn đến mất tín hiệu đầu vào nếu các token bị cắt chứa thông tin có giá trị cho truy vấn. Một cách tiếp cận khác có thể là căn trái (hoặc đệm phải) chuỗi đến độ dài chung (Gim et al., 2023). Mặc dù cách tiếp cận đệm căn trái này đơn giản, nó tạo ra các gián đoạn trong việc gán vị trí chuỗi prompt (xem Phụ lục E để định lượng). Với lược đồ mã hóa ALiBi (Press et al., 2021), có thể dễ dàng chỉ ra rằng các gián đoạn gán một cách không công bằng các hình phạt attention cho các token trong tài liệu ngắn hơn, vì các token sẽ tập trung ở các vị trí token sớm hơn (và do đó khoảng cách lớn hơn từ token hiện tại).²Do đó, chúng tôi được thúc đẩy đề xuất một chiến lược gán vị trí không dẫn đến gián đoạn.

[Hình 4 mô tả trực quan về phân công vị trí equilibrium đề xuất so với căn trái]

Hình 4. Trực quan trực quan cho phép gán vị trí equilibrium đề xuất của chúng tôi so với căn trái (xem Phần 3.2.2).

Chúng tôi đề xuất path equilibrium positioning như một chiến lược đơn giản, hợp lý. Với path equilibrium positioning, chúng tôi căn cách tuyến tính các đường dẫn chồng chéo để phù hợp với trung bình điều hòa, S(D), của độ dài tập thể của chúng (cho một tập hợp các đường dẫn chồng chéo D)

S(D) = nd/∑d∈D(1/‖d‖)                  (1)

Một cách trực quan, các vị trí token kết quả khớp với trạng thái cân bằng của các khối lượng kết nối được nối bởi lò xo (Hình 4).

Lưu ý rằng chiến lược path equilibrium positioning dẫn đến các vị trí có giá trị thực. Đây là một sự khởi hành từ việc sử dụng phổ biến các phép gán vị trí token, trong đó các vị trí có giá trị số nguyên là chủ đạo.³Chúng tôi lưu ý rằng việc lựa chọn lược đồ gán vị trí không ảnh hưởng đến hiệu suất suy luận

²Một thiên vị tương tự sẽ tồn tại nếu sử dụng chiến lược căn phải, ngoại trừ tài liệu ngắn hơn sẽ được gán không công bằng các boosts attention so với tài liệu dài hơn.

³Mặc dù điều này tầm thường đối với mã hóa vị trí ALiBi, nó không tầm thường đối với lược đồ Rotary Position Embedding (Su et al., 2021). Để xử lý trường hợp này, chúng tôi nội suy tuyến tính đối số của các hàm sinusoidal.

--- TRANG 5 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất

ciency, nhưng có thể ảnh hưởng đến chất lượng đầu ra mô hình. Trong Phần 5.1, chúng tôi xác thực tính ưu việt của path equilibrium positioning.

3.2.3. CẮT TỈA ĐƯỜNG DẪN

Khai thác thêm cấu trúc tôpô mà chúng tôi đã áp đặt lên prompt, chúng tôi đề xuất path pruning như một cơ chế để loại bỏ các tài liệu mà nó thấy không liên quan đến truy vấn. Như được chứng minh trong các thí nghiệm của chúng tôi, điều này có thể có lợi cho cả hiệu quả và độ chính xác đối với RAG dựa trên LLM.

Để cắt tỉa đường dẫn, chúng ta phải tính điểm saliency cho mỗi đường dẫn. Lấy cảm hứng từ SGPT (Muennighoff, 2022), chúng tôi áp dụng quy tắc Bayes cho đầu ra của đầu language modeling để tính điểm saliency hoặc entailment. Ở mức độ cao, chúng tôi tận dụng định lý Bayes để tính phân phối posterior

P(di|qi,p) ∝ P(q|di,p)P(di|p)

như một chỉ số saliency về mức độ liên quan của tài liệu di đối với truy vấn.⁴Trong các thí nghiệm của chúng tôi, chúng tôi quyết định chỉ số đường dẫn nào để cắt tỉa bằng cách chọn tham lam top-k của phân phối phân loại này (chúng tôi thực hiện ablations đối với việc lựa chọn k trong Phần 4.1.2 và Phụ lục A.1).

Để "vật lý" áp dụng việc cắt tỉa, chúng ta có thể đơn giản loại bỏ KV cache tương ứng với các tài liệu và truy vấn dọc theo những đường dẫn đó. Thuận tiện, tất cả KV cache còn lại có thể được nối đơn giản với nhau để sử dụng trong sinh tự hồi quy của phản hồi.

Chúng tôi cung cấp ablations chống lại các chỉ số saliency hợp lý khác trong Phần 5.2. Một biểu diễn trực quan về tác động của path pruning đối với các mẫu attention (ngầm) cũng có thể được thấy trong Hình 3c.

3.3. Tối ưu hóa Runtime Lossless

3.3.1. PATH CACHING

Giả sử bộ nhớ phụ trợ có sẵn, chúng ta có thể tăng tốc suy luận của superposition prompts bằng cách thực hiện công việc trước khi bất kỳ truy vấn nào đến. Kỹ thuật path caching này tổng quát hóa các ý tưởng được đưa ra trong PagedAttention (Kwon et al., 2023), trong đó chúng ta cache các embeddings KV dọc theo tất cả các tiền tố đường dẫn (không chỉ "nút gốc", như PagedAttention làm). Quan trọng là cách tiếp cận của chúng tôi cũng khác với PromptCache (Gim et al., 2023). Trong khi các "prompt modules" được cache của họ chỉ attend locally với chính chúng, các path prefix KV cache của chúng tôi attend locally với chính chúng cũng như với tất cả tổ tiên của chúng trong đồ thị. Vui lòng tham khảo Thuật toán 2 trong phụ lục để có công thức hóa.

Bây giờ chúng tôi mô tả cơ chế path caching của chúng tôi. Cụ thể,

⁴Điều này giống với "nguyên lý tác động ít nhất", xác định trọng số đường dẫn tối ưu trong công thức tích phân đường dẫn của cơ học lượng tử.

preamble KV cache và document KVs không được điều kiện hóa trên truy vấn, và do đó có thể được precomputed trong giai đoạn "preprocessing". Sau đó, trong giai đoạn "online serving", chúng ta truy xuất preamble và document KVs từ storage thay vì các chuỗi token đầu vào gốc. Hình 3d cho thấy cách, trong giai đoạn online serving, phần lớn các phụ thuộc attention đã được tính toán.

Lưu ý rằng yêu cầu bộ nhớ để sử dụng path caching là một bội số vô hướng, cmodel, của độ dài chuỗi được token hóa thô. Ở đây, cmodel là một vô hướng cố định phụ thuộc vào các khía cạnh khác nhau của mô hình, chẳng hạn như số lớp và chiều embedding (ví dụ cbloom-7b1 = 492 KB).

3.3.2. SONG SONG HÓA ĐƯỜNG DẪN

Vì các đường dẫn superpositioned của ForkJoin độc lập với nhau (theo thiết kế), các KV cache và logits tương ứng của các phân đoạn truy vấn có thể được tính song song. Mặc dù điều này không giảm "CPU time", nó quan trọng là giảm wall-clock time mà người dùng trải nghiệm. Việc song song hóa trên các truy vấn nhân bản có thể được thực hiện bằng (1) nối chuỗi dọc theo chiều batch trước suy luận⁵ (2) ủy thác các lời gọi mô hình trên một cluster phân tán của các nút tính toán (ví dụ GPU), hoặc (3) kết hợp batching và suy luận phân tán. Chiến lược hiệu quả nhất sẽ phụ thuộc vào đặc điểm cụ thể của cấu hình cluster (ví dụ băng thông mạng tương đối so với tính toán có sẵn trên mỗi nút).

4. Kết quả thực nghiệm

Chúng tôi thực hiện thí nghiệm trên ba họ mô hình ngôn ngữ lớn, cụ thể là OpenELM (Mehta et al., 2024), BLOOMZ (Muennighoff et al., 2023), và MPT (MosaicML NLP Team, 2023). Để định lượng hiệu quả của superposition prompting khi kết hợp với các mô hình có quy mô khác nhau, chúng tôi sử dụng các kích thước mô hình khác nhau từ những họ này. Đối với OpenELM, chúng tôi sử dụng cấu hình 3B-Instruct. Đối với BLOOMZ, chúng tôi khởi tạo các mô hình 3B tham số (bloomz-3b) và 7.1B tham số (bloomz-7b1). Cuối cùng, đối với MPT, chúng tôi sử dụng mô hình 7B tham số được fine-tuned instruction có sẵn (mpt-7b-instruct). Tập hợp mô hình này bao gồm các kiến trúc, lược đồ mã hóa vị trí, kích thước và quy trình pretraining khác nhau. Chúng tôi nhắc nhở người đọc rằng chúng tôi sử dụng các checkpoint pretrained được phát hành công khai, mà không sử dụng bất kỳ huấn luyện, fine-tuning hoặc thích ứng tác vụ bổ sung nào.

Đối với các thí nghiệm của chúng tôi, chúng tôi chủ yếu quan tâm đến sự đánh đổi thời gian tính toán so với độ chính xác.⁶Chúng tôi sử dụng gói fvcore

⁵Nói chung, độ dài đường dẫn prompt sẽ khác nhau, do đó yêu cầu đệm đến độ dài chung. Tuy nhiên, một chiến lược binning độ dài có thể giảm thiểu hầu hết overhead trong thực tế.

⁶Trong phân tích timing và speedup của chúng tôi, chúng tôi tuân theo các nghiên cứu trước đây (Gim et al., 2023) và không xem xét phần truy xuất dữ liệu của pipeline RAG, điều này sẽ yêu cầu quá nhiều giả định.

--- TRANG 6 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất

(facebookresearch, 2024) để tính số lượng floating point operation (FLOP) lý thuyết cho các cài đặt suy luận khác nhau. Chúng tôi đánh giá chi phí tính toán của mỗi phương pháp theo đơn vị compute cycles—tương tự như FLOPs, nhưng tính đến tính song song. Trong thực tế, để đạt được các speedup, tài nguyên bổ sung (bộ nhớ phụ trợ và/hoặc tính toán phụ trợ cho song song hóa) sẽ được yêu cầu. Tuy nhiên, như đã nêu, mục tiêu của việc khám phá này là acceleration, không nhất thiết là giảm FLOPs. Chúng tôi xin tham khảo Phụ lục A.2 để có phân tích chi tiết về các lợi ích speedup lý thuyết được bật bởi mỗi tối ưu hóa được đề xuất của chúng tôi.

4.1. Kết quả

Chúng tôi tận dụng các tập dữ liệu NaturalQuestions-Open (Liu et al., 2023a) và MuSiQue (Trivedi et al., 2022) có sẵn công khai. Chúng tôi không thực hiện bất kỳ điều chỉnh prompt thủ công hoặc prompt engineering nào cho bất kỳ phương pháp hoặc baseline nào, và sử dụng cùng một prompt trên tất cả các thí nghiệm (mỗi tập dữ liệu) để kiểm soát sự khác biệt có thể phát sinh với việc thay đổi từ ngữ prompt. Để tái tạo, chúng tôi trình bày từ ngữ prompt chính xác được sử dụng cho mỗi tập dữ liệu trong Phụ lục F. Chúng tôi sử dụng giải mã tự hồi quy tham lam trong tất cả các thí nghiệm, và ngẫu nhiên hóa thứ tự tài liệu để ngăn chặn bất kỳ thiên vị hệ thống nào có thể do vị trí của "gold documents" (à la Liu et al., 2023a).

4.1.1. NATURAL QUESTIONS-OPEN

NaturalQuestions-Open (Liu et al., 2023a) là một benchmark trả lời câu hỏi miền mở được phái sinh từ Natural Questions (Kwiatkowski et al., 2019). Nó chứa các truy vấn lịch sử được phát hành cho công cụ tìm kiếm Google, được kết hợp với các câu trả lời sử dụng nội dung của Wikipedia tiếng Anh. Chúng tôi tuân theo cùng một thiết lập thí nghiệm như Liu et al., 2023a, bao gồm cùng phương pháp tiền xử lý và đánh giá cho cài đặt 20 tài liệu (báo cáo Best EM Subspan, hoặc "Accuracy" cho ngắn gọn).

Chúng tôi trình bày so sánh speedup so với độ chính xác trong Bảng 1. Đối với baseline TF-IDF, chúng tôi sử dụng TF-IDF (từ gói SciPy Virtanen et al., 2020) để chọn top-k tài liệu có điều kiện trên truy vấn, sau đó thực hiện "naive LLM-RAG" (như được mô tả trong Phần 3.1). Baseline BM-25 của chúng tôi tương đương, ngoại trừ chúng tôi sử dụng Brown, 2020 cho việc chọn top-k tài liệu. Chúng tôi cũng có một baseline tương đương trong đó chúng tôi sử dụng Contriever (Izacard et al., 2021) để chọn top-k tài liệu.⁷Chúng tôi so sánh với phương pháp Attention Sort được đề xuất gần đây, sử dụng phương pháp của họ chính xác như được mô tả trong

⁷Để biểu diễn hào phóng hơn của các baseline BM-25, TF-IDF và Contriever, chúng tôi tính các chỉ số speedup giả sử document KV caching (mặc dù theo hiểu biết của chúng tôi, điều này chưa được đề xuất trước đây trong literature). Lưu ý rằng caching không thể với Naive LLM-RAG hoặc Attention Sort vì thứ tự tài liệu thay đổi, và nói chung, tài liệu attend to tài liệu khác (do đó song song hóa cũng không thể).

Peysakhovich & Lerer, 2023. Cuối cùng, chúng tôi so sánh với Prompt Cache (Gim et al., 2023). Lưu ý rằng Naive LLM-RAG, Prompt Cache và Attention Sort luôn attend to tất cả tài liệu.

Ngoài Bảng 1, chúng tôi trình bày các nghiên cứu ablation kiến trúc khác nhau trong Phần 5 và Phụ lục A để biện minh các quyết định thiết kế của chúng tôi.

4.1.2. MUSIQUE

MuSiQue (Trivedi et al., 2022) là một tập dữ liệu lý luận multi-hop bao gồm các cặp câu hỏi trả lời được thu thập với mục tiêu làm cho lý luận ngắt kết nối khó hơn và do đó thêm vào độ khó của các tập dữ liệu trả lời câu hỏi multi-hop được giới thiệu trước đây. Chúng tôi xác thực cách tiếp cận của mình trên split dev của MuSiQue-Ans (báo cáo Answer EM và F1).

Một sửa đổi nhỏ được thực hiện cho superposition prompting để xử lý thiết lập lý luận multi-hop của MuSiQue. Cụ thể, chúng tôi áp dụng iteratively superposition pruning để xây dựng một chuỗi t×k tài liệu⁸, trong đó t và k là các siêu tham số. Tại mỗi bước thời gian {1, . . . t}, chúng tôi tạo một superposition với tất cả tài liệu còn lại, cắt tỉa để giữ top k, prepend những tài liệu (được cache) đó vào prefix đang chạy, sau đó lặp lại. Một mô tả trực quan của iterative superposition này được trình bày trong Hình 6. Chúng tôi giả thuyết rằng iterative superposition có thể cải thiện hiệu suất vì chúng tôi trang bị LLM để giải quyết iteratively thách thức lý luận multi-hop.

Đối với các baseline của chúng tôi, chúng tôi so sánh với Attention Sort, Prompt Cache và Naive LLM-RAG (tất cả đều luôn attend to tất cả tài liệu). Kết quả của chúng tôi được tóm tắt trong Bảng 2.

4.2. Phân tích

4.2.1. SUPERPOSITION PROMPTING CÓ THỂ CẢI THIỆN HIỆU QUẢ THỜI GIAN

Kết quả trên tập dữ liệu NaturalQuestions-Open Bảng 1 cho thấy superposition prompting là phương pháp hiệu quả hàng đầu theo thứ tự độ lớn. Những lợi ích này chủ yếu do các cơ chế path parallelism và path pruning. Bảng 6 trình bày một phân tích về đóng góp của mỗi cơ chế này đối với speedup. Ví dụ, đối với mpt-7b-instruct (trên NaturalQuestions-Open), chỉ riêng caching tạo ra speedup 10.2×, trong khi chỉ riêng parallelism tạo ra speedup 14.8×. Những tối ưu hóa này kết hợp với pruning tạo ra speedup 93.7× tổng thể.

Với MuSiQue, chúng ta thấy speedup tổng thể thấp hơn cho

⁸Lưu ý rằng chỉ k tài liệu đầu tiên được chọn là có thể cache. Các tài liệu tiếp theo không thể cache vì KV cache của chúng phụ thuộc vào các tài liệu trước đó (được chọn động trong quá trình phục vụ truy vấn).

--- TRANG 7 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất

Bảng 1. Độ chính xác sinh trả lời có tăng cường truy xuất cho các mô hình và phương pháp khác nhau trên tập dữ liệu NaturalQuestions-Open. Đối với các baseline có siêu tham số—cụ thể là tham số top-k cho BM-25, TF-IDF và Contriever—chúng tôi trình bày cấu hình độ chính xác cao nhất của chúng (xem Phụ lục A.1 cho tất cả cấu hình). Chúng tôi nhấn mạnh tính ưu việt của superposition prompting so với các baseline được xem xét dọc theo các trục cả độ chính xác và speedup.

[Bảng dữ liệu với các cột: MODEL, APPROACH, COMPUTE CYCLES, THEOR. SPEEDUP, ACCURACY]

các cài đặt superposition prompting hiệu suất cao nhất (Bảng 2). Điều này là do việc sử dụng iterative superposition (Phần 4.1.2), hạn chế các cơ hội caching đối với việc chọn k tài liệu đầu tiên.

4.2.2. SUPERPOSITION PROMPTING CÓ THỂ CẢI THIỆN ĐỘ CHÍNH XÁC

Trong Bảng 1 chúng ta thấy rõ ràng rằng superposition prompting là phương pháp thống trị về độ chính xác trên NaturalQuestions-Open, thấy những cải thiện 12–43% so với giải pháp ngây thơ, và lên đến 15% cải thiện so với đối thủ cạnh tranh tốt nhất tiếp theo. Với MuSiQue (Bảng 2), chúng tôi lưu ý rằng superposition prompting tạo ra độ chính xác cao nhất cho mỗi mô hình.

Một giải thích cho việc cải thiện độ chính xác là cách superposition prompting giảm độ dài chuỗi như được nhận thức bởi transformer. Các nghiên cứu gần đây đã điều tra sự thiếu rõ ràng về khả năng "ngoại suy độ dài" của các LLM dựa trên transformer (Press et al., 2021; Ruoss et al., 2023; Kazemnejad et al., 2023). Một thuộc tính thuận tiện của superposition prompting là—từ quan điểm của transformer—độ dài chuỗi tối đa quan sát được là đường dẫn dài nhất qua đồ thị.⁹Ví dụ, với NaturalQuestions-Open, superposition prompting giảm đường dẫn tối đa (và do đó độ dài chuỗi) từ trung bình 2923 token xuống 206 token. Theo nghĩa này, superposition prompting cho RAG có thể cho phép các transformer không có ngữ cảnh dài hoạt động tốt trên các chuỗi dài. Thuộc tính này có thể cho phép các nhà phát triển mô hình giảm đáng kể chi phí pretraining (vì huấn luyện LLM "ngữ cảnh dài" có mục đích đặc biệt dẫn đến chi phí tăng (Press et al., 2021)).

Một giải thích khác cho việc cải thiện độ chính xác là hiện tượng "phân tâm" LLM. Các nghiên cứu trước đây của Liu et al., 2023a; Borgeaud et al., 2021a; Shi et al., 2023 trình bày các lập luận về cách LLM có thể nhạy cảm với ngữ cảnh nhiễu hoặc không liên quan. Với việc bao gồm cơ chế path pruning, chúng tôi trang bị mô hình với một cách có cấu trúc để lọc ra "tiếng ồn" (tức là tài liệu không liên quan).

⁹Điều này có nghĩa là độ dài chuỗi hiệu quả (được nhận thức) là O(1) thay vì O(nd), trong đó nd là số lượng tài liệu offline.

--- TRANG 8 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất

Bảng 2. Độ chính xác sinh trả lời có tăng cường truy xuất cho các mô hình khác nhau trên tập dữ liệu MuSiQue. Đối với superposition prompting, t biểu thị số lần lặp của iterative superposition (được mô tả trong Phần 4.1.2), và k biểu thị top-k được chọn (tức là không bị cắt tỉa) tại mỗi bước (xem Phần 3.2.3).

[Bảng dữ liệu với các cột: MODEL, APPROACH, COMPUTE CYCLES, THEOR. SPEEDUP, F1, EM]

4.2.3. ĐỘ NHẠY CẢM VỚI ALIBI VS. ROPE

Vì các lý do được nêu trong Phần 3.2.2, superposition prompting rất tự nhiên phù hợp cho các transformer chấp nhận các phép gán vị trí token có giá trị liên tục (tức là chúng thể hiện thuộc tính nội suy vị trí như được định nghĩa bởi Chen et al., 2023). Mặc dù lược đồ mã hóa vị trí ALiBi đã được chỉ ra là sở hữu thuộc tính này, đã được đề xuất rằng fine-tuning có thể được yêu cầu để trang bị các mô hình dựa trên Rotary Position Embedding (RoPE) với thuộc tính này.

Các thí nghiệm của chúng tôi xác thực rằng cơ chế gán vị trí equilibrium được đề xuất của chúng tôi tương thích ngay cả với một mô hình dựa trên RoPE không được fine-tuned (tức là họ OpenELM). Chúng tôi để lại cho các nghiên cứu tương lai để đo lường mức độ mà fine-tuning có thể cải thiện độ chính xác (nếu có).

Chúng tôi lưu ý rằng OpenELM-3B-Instruct có độ chính xác thấp hơn đáng kể cho nhiều baseline, chẳng hạn như AttentionSort, Naive LLM-RAG và thậm chí Prompt Cache. Chúng tôi giả thuyết rằng điều này là do thiếu khả năng ngoại suy độ dài của RoPE, điều này sẽ trở nên rõ ràng hơn đối với những baseline đó.

Bảng 3. Thay đổi hàm gán vị trí được sử dụng với superposition prompting trên tập dữ liệu NaturalQuestions-Open.

[Bảng dữ liệu với các cột: MODEL, APPROACH, ACCURACY]

5. Ablations

5.1. Ablation Gán Vị Trí

Trong Bảng 3, chúng tôi điều tra tác động của chiến lược gán vị trí trong quá trình superposition prompting. Chúng tôi so sánh path positioning equilibrium được đề xuất của chúng tôi với chiến lược left aligned được mô tả trong Phần 3.2.2. Các phát hiện của chúng tôi xác thận giả thuyết của chúng tôi được nêu trong Thuật toán 1, trong đó chúng tôi suy đoán rằng left alignment sẽ dẫn đến hiệu suất tệ hơn (do thiên vị attention chuỗi dài).

5.2. Ablation Chỉ Số Saliency Đường Dẫn

Trong Bảng 4, chúng tôi ablate việc lựa chọn chỉ số "path saliency" của chúng tôi. Chúng tôi so sánh với hai baseline khác—attention và none. Với none, chúng tôi đơn giản không cắt tỉa. Baseline attention bao gồm việc sử dụng điểm attention cho mỗi tài liệu (trung bình trên các token, lớp và attention head) làm điểm đường dẫn. Chúng tôi nhấn mạnh rằng path saliency Bayesian của chúng tôi vượt trội đáng kể so với attention-based scoring, cũng như baseline control.

Bảng 4. Độ chính xác sinh trả lời có tăng cường truy xuất cho các chỉ số path saliency khác nhau trên tập dữ liệu NaturalQuestions-Open.

[Bảng dữ liệu với các cột: MODEL, SELECTION METRIC, ACCURACY]

5.3. Ablation Hệ Số Superposition

Chúng tôi giới thiệu siêu tham số superposition factor như một tham số để nội suy giữa một prompt hoàn toàn superimposed và hoàn toàn "cổ điển". Các hệ số superposition lớn hơn tương ứng với các prompt "superimposed hơn", trong khi các hệ số superposition nhỏ hơn tương ứng với các prompt "ít superimposed hơn" (đạt được bằng cách kết hợp các tài liệu liền kề trước khi tạo các đường dẫn prompt).

Chính thức, chúng tôi định nghĩa m là số lượng tài liệu được xem xét cho một truy vấn sinh trả lời có tăng cường truy xuất (ví dụ, đây là m = 20 cho các cài đặt phổ biến của NaturalQuestions-Open (Liu et al., 2023a) và MuSiQue (Trivedi et al., 2022)). Bằng cách đặt hệ số superposition γ ∈ [1, m], chúng tôi tính "tài liệu hiệu quả trên mỗi đường dẫn" là ⌊m/γ⌉. Quan trọng, lưu ý rằng khi γ = 1, chúng ta đã giảm xuống trường hợp "cổ điển" (Naive LLM-RAG). Chúng tôi thực hiện ablation bằng cách quét tham số hệ số superposition này và trình bày kết quả trong Hình 5. Một biểu diễn trực quan được trình bày trong Hình 7.

Các đường cong thường cho thấy cải thiện dọc theo cả hai trục khi chúng ta tăng thương số superposition. Thú vị là độ chính xác tối đa có thể không hoàn toàn superimposed, cho thấy rằng giá trị này nên được điều chỉnh cho ứng dụng cụ thể.

[Hình 5 mô tả việc quét các giá trị của superposition factor (SF) trên tập dữ liệu NaturalQuestions-Open với nhiều mô hình khác nhau]

6. Kết luận và Thảo luận

Trong nghiên cứu này, chúng tôi đã giới thiệu một khung mới để tăng tốc và cải thiện sinh trả lời có tăng cường truy xuất với LLM. Chúng tôi đã xác minh sự tổng quát hóa của phương pháp trên các mô hình và tập dữ liệu khác nhau và thực hiện các ablation rộng rãi.

Phương pháp của chúng tôi, superposition prompting, đã được chỉ ra là cải thiện độ chính xác mô hình hóa chuỗi dài trong các tác vụ trả lời câu hỏi đơn và đa hop, tất cả trong khi giảm độ trễ phản hồi mà người dùng quan sát được. Thuận tiện, tối ưu hóa này đã được chỉ ra là hiệu quả mà không cần bất kỳ fine-tuning hoặc huấn luyện bổ sung nào cho mô hình cơ sở. Chúng tôi hoãn cho nghiên cứu tương lai để khám phá cách (nếu có) fine-tuning có thể cải thiện thêm superposition prompting. Chúng tôi cũng nhấn mạnh rằng nghiên cứu tương lai nên điều tra cách tổng quát hóa những ý tưởng này ngoài bối cảnh RAG.

Tuyên bố Tác động

Bài báo này trình bày nghiên cứu có mục tiêu thúc đẩy lĩnh vực Học máy. Cụ thể, chúng tôi đang đề xuất một kỹ thuật học máy có thể được sử dụng để cải thiện chất lượng mô hình hóa ngôn ngữ sinh tạo trong khi giảm chi phí tính toán thường liên quan đến việc triển khai chúng. Có nhiều hậu quả tiềm năng của nghiên cứu này, đối với lĩnh vực nói chung, không có hậu quả nào mà chúng tôi cảm thấy phải được làm nổi bật cụ thể ở đây.

--- TRANG 9 ---
Superposition Prompting: Cải Thiện và Tăng Tốc Sinh Trả Lời có Tăng Cường Truy Xuất

Lời cảm ơn
Chúng tôi muốn gửi lời cảm ơn đến Sachin Mehta, Maxwell Horton, Enrico Fini và Arsalan Farooq vì đã thảo luận và phản hồi về bài báo.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo với các citation và URLs]

--- TRANG 10-21 ---
[Phần còn lại của tài liệu bao gồm các phụ lục, thuật toán, bảng biểu bổ sung, và các ví dụ prompt chi tiết]
