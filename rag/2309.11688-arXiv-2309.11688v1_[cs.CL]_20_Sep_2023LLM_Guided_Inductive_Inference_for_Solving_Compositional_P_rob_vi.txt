# 2309.11688.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2309.11688.pdf
# Kích thước tệp: 133157 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2309.11688v1  [cs.CL]  20 Sep 2023Suy luận quy nạp được hướng dẫn bởi LLM để giải quyết các vấn đề tổ hợp
Abhigya Sodani1Lauren Moos2Matthew Mirman1

Tóm tắt
Trong khi các mô hình ngôn ngữ lớn (LLM) đã
thể hiện hiệu suất ấn tượng trong các nhiệm vụ
hỏi đáp, hiệu suất của chúng bị hạn chế khi
các câu hỏi yêu cầu kiến thức không được
bao gồm trong dữ liệu huấn luyện của mô hình
và chỉ có thể thu được thông qua quan sát trực tiếp
hoặc tương tác với thế giới thực. Các phương pháp
hiện tại phân tách các nhiệm vụ lý luận thông qua
việc sử dụng các module được gọi tuần tự, hạn chế
khả năng trả lời các nhiệm vụ lý luận sâu. Chúng tôi
giới thiệu một phương pháp, LLM có thể mở rộng
dựa trên đệ quy (REBEL), xử lý các nhiệm vụ lý luận
sâu trong thế giới mở bằng cách sử dụng các kỹ thuật
lý luận tự động như lập kế hoạch động và chiến lược
chuỗi tiến. REBEL cho phép LLM lý luận thông qua
phân tách vấn đề đệ quy và sử dụng các công cụ bên ngoài.
Các công cụ mà REBEL sử dụng chỉ được chỉ định bằng
mô tả ngôn ngữ tự nhiên. Chúng tôi tiếp tục chứng minh
khả năng REBEL trên một tập hợp các vấn đề yêu cầu
việc sử dụng lồng nhau sâu các công cụ bên ngoài trong
một bối cảnh tổ hợp và đối thoại.

1. Giới thiệu
Gần đây, các mô hình neural cho việc tạo ngôn ngữ tự nhiên
đã thể hiện kết quả ấn tượng (Koroteev, 2021;
Devlin et al., 2018; Brown et al., 2020), mở ra những
con đường mới quan trọng để giải quyết các nhiệm vụ lý luận
ngôn ngữ tự nhiên một cách chính xác (Huang & Chang, 2022; Qiao et al., 2022).
Trong khi LLM đã cho thấy khả năng độc đáo để mở rộng theo
những cách có thể dự đoán và hiệu quả, vẫn chưa rõ liệu chúng
có thể hiện hành vi mở rộng này trên các nhiệm vụ lý luận phức tạp
(Huang & Chang, 2022). Hơn nữa, những hạn chế của các
mô hình ngôn ngữ lớn trong việc truy cập các nguồn kiến thức bên ngoài
động hạn chế đáng kể tính hữu ích của chúng. Lý luận của con người
bao gồm một sự kết hợp của quan sát và tương tác

1Anarchy, Extensional, Inc, California, USA2Yoyodyne Inc,
California, USA. Liên hệ: Abhigya Sodani <abhigya@anarchy.ai>, Matthew Mirman <matt@anarchy.ai>.

Kỷ yếu Hội nghị Quốc tế lần thứ 40 về Học máy, Honolulu, Hawaii, USA. PMLR 202, 2023. Bản quyền
2023 thuộc về (các) tác giả.tác với thế giới, làm nổi bật bản chất hướng hành động của
lý luận.

Trong bài báo này, chúng tôi giải quyết vấn đề này bằng cách giới thiệu
khung LLM có thể mở rộng dựa trên đệ quy (REBEL). REBEL cho
phép LLM lý luận thông qua các vấn đề cực kỳ phức tạp
yêu cầu kiến thức từ các nguồn bên ngoài khác nhau.
Điều này được thực hiện bằng cách sử dụng một engine suy luận,
sử dụng các công cụ được cung cấp, thu thập các sự kiện cần thiết
để suy ra câu trả lời đúng. Cụ thể, chúng tôi trình bày ba đóng góp:

1. Thiết kế một hệ thống có khả năng trả lời câu hỏi
sử dụng bất kỳ công cụ bên ngoài tùy ý nào.

2. Một đánh giá cho thấy REBEL cải thiện hiệu suất
hiện đại trong việc truy xuất sự kiện nhiều bước và các vấn đề
hỏi đáp tổ hợp.

3. Phát hành mã và bộ đánh giá của chúng tôi để sử dụng
mã nguồn mở tại rebel.anarchy.ai.

2. Các công trình liên quan
Ở mức độ cao, các phương pháp tiếp cận các nhiệm vụ lý luận
sử dụng LLM có thể được chia thành các kỹ thuật kỹ thuật prompt
(Liu et al., 2023; Schlag et al., 2023) và fine-tuning
(Micheli & Fleuret, 2021; Schick et al., 2023), hoặc
kết hợp các phương pháp trên. Ở đây chúng tôi chỉ tập trung vào
các kỹ thuật prompt.

Forward chaining (Liebowitz, 1988) là một chiến lược lý luận
được sử dụng trong lịch sử bởi các hệ thống chuyên gia. Nó hoạt động bằng
cách áp dụng lặp đi lặp lại các quy tắc suy luận logic từ một
kho lưu trữ ban đầu của các tiên đề đã biết để cuối cùng lý tưởng là tạo ra
mục tiêu. Chiến lược này gần đây đã được sử dụng
để giải quyết các vấn đề ngôn ngữ tự nhiên với sự hỗ trợ của
LLM trong Chain of Thought (CoT) (Wei et al., 2022). ReAct
(Yao et al., 2023) xây dựng dựa trên CoT bằng cách tạo ra
các hành động cụ thể cho nhiệm vụ để đáp lại lý luận. Chameleon
(Lu et al., 2023) đi xa hơn, sử dụng LLM để tổng hợp
các pipeline công cụ bao gồm các mô hình thị giác máy tính
có sẵn, các engine tìm kiếm web, và các cuộc gọi đến các mô hình
tạo sinh. Trái ngược với forward-chaining, kỹ thuật
backward-chaining (Russell, 2010) cố gắng hạn chế không gian
tìm kiếm của các suy luận có thể bằng cách xác định những gì
phải đúng để một mục tiêu được thể hiện (Picco et al., 2021).

1

--- TRANG 2 ---
Suy luận quy nạp được hướng dẫn bởi LLM để giải quyết các vấn đề tổ hợp

Press et al. (2022) trình bày một phương pháp để đánh giá
khả năng giải quyết vấn đề trên một danh mục các nhiệm vụ lý luận
không tầm thường với cấu trúc tổ hợp (Lake & Baroni,
2018; Keysers et al., 2019) được giải quyết kém bởi các
phương pháp trước đây. Họ biểu thị lỗi tổ hợp như số
câu hỏi trong đó hai câu hỏi con được trả lời
đúng nhưng câu hỏi cấp cao nhất thì không. Công trình trước đây đã
cho thấy cách điều này có thể được giải quyết thông qua phân tách vấn đề
(Yang et al., 2022; Zhou et al., 2022; Drozdov et al.,
2022; Khot et al., 2022). Trong công trình này, chúng tôi cho thấy cách
phân tách vấn đề có thể được tăng cường với việc sử dụng công cụ.

3. Phương pháp
Trong phần này chúng tôi giới thiệu thuật toán REBEL như được thể hiện
trong Hình 1, và tất cả ký hiệu và kiến thức nền cần thiết. Ở mức độ cao,
nó hoạt động đệ quy để giải quyết câu hỏi, chia nhỏ
câu hỏi thành các câu hỏi con cho đến khi không thể tạo ra
câu hỏi con nào nữa. Hãy gọi câu hỏi/câu hỏi con thứ n
là Question n và câu trả lời của nó là Answer n. Ví dụ,
câu hỏi do người dùng cung cấp sẽ là Question 0. Hãy gọi
các câu hỏi con được tạo ra để trả lời Question n
là Subquestions n. Trong mỗi bước đệ quy, chúng ta chia
Question n thành Subquestions n. Hãy gọi câu trả lời
cho thành viên thứ i của Subquestions n là subanswers n[i].
Chúng ta gọi đệ quy mỗi thành viên của Subquestions n, và
mỗi subanswers n[i] được trả về như một sự kiện là tuple
(Subquestions n[i], subanswers n[i]). Sự kiện này được
thêm vào một danh sách các sự kiện toàn cục cho mỗi Question n.
Danh sách sự kiện này trở thành Memory n được sử dụng để thông báo
Answer n.

Để dừng đệ quy không giới hạn, chúng ta xóa các thành viên
của Subquestions n có các featurization có độ tương tự cosine
trên 0.98 với featurization của Question n.

Hệ thống REBEL chứa một ToolList, là một danh sách
được đánh số của các công cụ chúng ta có sẵn và mô tả của chúng.
Nếu cần, chúng ta xác định một Tool n cho mỗi
Question n, là số của công cụ cần thiết để
trả lời Question n cho Memory n.

Dưới đây chúng ta định nghĩa các bước cơ bản của thuật toán này:
chia tách câu hỏi, kiểm tra bộ nhớ, chọn công cụ, và sử dụng công cụ.
Hình 1 mô tả pipeline này.

3.1. Chia tách câu hỏi
Subroutine split chia Question n thành
Subquestions n với kích thước của Subquestions n
là số câu hỏi con mà LLM tạo ra.
LLM được prompt với ToolList, và 4 ví dụ
chia tách câu hỏi. Bước này được thể hiện trong
bước 1 của Hình 1. Để xem một ví dụ duy nhất về ngữ cảnh cho
chia tách câu hỏi, xem Phụ lục A.

Thuật toán 1 REBEL
hàm promptf(Question n, facts, allowsplit =
True)
if allowsplit then
Subquestions n = split(Question n, facts) {chia
câu hỏi thành các câu hỏi con để trả lời}
for subquestion từ 1 đến s trong Subquestions n do
if cossimilarity(Question n, subquestion) > 0.98
then
Xóa subquestion
allowsplit = False
end if
end for
for subquestion từ 1 đến s trong Subquestions n do
, newfacts = PROMPTF
(subquestion, facts, allowsplit)
facts += newfact
end for
end if
if MEMORYCHECK(Question n, facts) then
Answer n = CALL GPT(Question n, facts)
return Answer n, (Question n, Answer n)
else
tool = PICK TOOL(Question n, facts)
toolinput = CALL GPT(tool, Question n, facts)
{để xác định đầu vào công cụ}
Answer n = USETOOL(toolinput, facts)
end if
return Answer n, (Question n, Answer n)
end function

Chúng ta trả lời mỗi câu hỏi con và kết quả của nó được trả về
như một fact (xem Thuật toán 1). Những sự kiện này được tích lũy
và chuyển cho tất cả các câu hỏi con tiếp theo. Danh sách
Subquestions n được sắp xếp sao cho sự kiện thu được từ
việc trả lời câu hỏi con có chỉ số thấp hơn sẽ hỗ trợ trong việc
trả lời câu hỏi con có chỉ số cao hơn.

3.2. Kiểm tra bộ nhớ
Chúng ta kiểm tra xem một câu hỏi có thể được trả lời mà không cần
sử dụng công cụ nào không. Điều này có thể có nghĩa là câu hỏi có thể được
trả lời bằng Memory n hoặc câu hỏi có thể được trả lời
bởi một LLM mà không cần sử dụng công cụ nào (xem bước 2 Hình
1). Nếu đây là trường hợp, chúng ta trực tiếp cung cấp cho LLM cơ sở
Memory n và Question n để tìm Answer n. Để xem
prompt kiểm tra bộ nhớ hoàn chỉnh, xem Phụ lục B.

3.3. Bộ chọn công cụ
Ở đây chúng ta kích hoạt LLM để quyết định thành viên nào của
ToolList (được mô tả bằng số nguyên Tool n) sẽ tốt nhất để
quyết định câu trả lời cho một câu hỏi. Đây là một hệ thống
được prompt 0-shot có thể được thấy trong bước 3 của Hình 1.

2

--- TRANG 3 ---
Suy luận quy nạp được hướng dẫn bởi LLM để giải quyết các vấn đề tổ hợp

Hình 1. Mô tả trực quan của pipeline của Thuật toán REBEL từ Thuật toán 1 để trả lời một Question n nào đó. Các hộp màu xanh chứa
mô tả của mỗi bước của pipeline, và các hộp màu đỏ chứa biến đầu ra cho mỗi bước sẽ được sử dụng trong các bước tiếp theo.

Chia tách câu hỏi
1) Tạo ra
Subquestions n và
đệ quy lấy câu trả lời của chúng
và thêm chúng vào Memory n.
Memory n

Kiểm tra bộ nhớ
2) Tạo ra một boolean
MemoryCheck n cho biết
liệu câu trả lời cho
Questions n có thể
được tìm thấy trong Memory n.
MemoryCheck n

Chọn công cụ
3) Nếu MemoryCheck n
là false, chúng ta tạo ra
Tool n, công cụ tốt nhất
để trả lời Question n
Tool n

Tạo đầu vào công cụ
4) Lấy Memory n
và Question n và
tạo ra đầu vào cho Tool n
... "toolnparam k":
"toolnvalue k", ...

3.4. Tạo đầu vào công cụ
Chúng ta sử dụng GPT-3 để tạo đầu vào chuẩn hóa cho các công cụ của chúng ta.
Chúng ta cung cấp các công cụ cho LLM với 2 trường. Mô tả
của công cụ và các tham số động của công cụ. Chúng ta
lưu trữ 3 trường khác về mỗi công cụ được ẩn khỏi
LLM, đó là: nếu công cụ là một yêu cầu GET/POST, URL
endpoint cho công cụ, và các tham số tĩnh của
công cụ. Các tham số động là các tham số sẽ được
điều chỉnh dựa trên mỗi cuộc gọi (ví dụ, và trường truy vấn).
Các tham số tĩnh là các tham số giữ nguyên trong
mỗi cuộc gọi API (ví dụ, một khóa xác thực).

REBEL sử dụng 3 công cụ mặc định: search, weather, và Google
Maps. Chúng ta cấu hình đầu vào cho mọi công cụ như một JSON. Một
JSON đầu vào công cụ ánh xạ các tham số động của một công cụ
cụ thể đến các giá trị mà những tham số đó nên có để thu được
thông tin để trả lời một câu hỏi cụ thể: {"toolnparam 1":
"toolnvalue1",...,"toolnparam k": "toolnvaluek"}. Một
định dạng JSON chuẩn hóa giảm tải cho LLM để
định dạng toàn bộ cuộc gọi API bởi chính nó.

REBEL cho phép thêm các công cụ tùy ý vào nó, tuy nhiên,
các ví dụ k-shot được cung cấp cho LLM
để tạo đầu vào cho Tool n được thiết kế xung quanh
3 công cụ cơ sở. Chúng ta đã phát hiện ra rằng việc prompting này
ngoại suy đến việc sử dụng 0-shot của các công cụ chưa thấy và tùy ý.
Xem Phụ lục C để biết ngữ cảnh tạo đầu vào công cụ hoàn chỉnh
cho một shot duy nhất.

3.5. Sử dụng công cụ
Hàm UseTool lấy các tham số động (từ
đầu vào công cụ được tạo bởi LLM), các tham số tĩnh mà
chúng ta đã lưu trữ cho mỗi công cụ, và endpoint API và
tạo một URL yêu cầu duy nhất. URL này được yêu cầu, và
đầu ra trả về được lưu trữ như một chuỗi. Nếu đầu ra trả về
dài hơn 15.000 ký tự, nó sẽ bị cắt bớt đến mức đó.
Sau đó, chúng ta sử dụng một LLM, được cung cấp với Memory n,
Question n, và đầu ra yêu cầu API, để tạo ra một câu
trả lời cho Question n. Câu trả lời này được trả về từ hàm
UseTool như Answer n. Cách tiếp cận của chúng ta có một số
hệ quả. Về mặt tích cực, người dùng không cần
chỉ ra cách phân tích đầu ra của các công cụ mà họ cung cấp cho chúng ta,
điều này làm cho REBEL cực kỳ có thể mở rộng và linh hoạt để
diễn giải nhiều loại và định dạng trả về công cụ. Về mặt tiêu cực,
do bản chất cực kỳ không có cấu trúc của các giá trị trả về
công cụ, lỗi được gây ra bởi UseTool không thể
trả lời một câu hỏi dựa trên giá trị trả về công cụ.

4. Đánh giá
Trong phần này, đầu tiên chúng tôi giới thiệu thiết lập thí nghiệm,
bao gồm các benchmark được sử dụng để đánh giá, và sau đó
trình bày kết quả.

4.1. Thiết lập thí nghiệm
Chúng tôi đã thử nghiệm REBEL trên 3 bộ dữ liệu: Compositional Celebrities
(Press et al., 2022), FEVER (Thorne et al., 2018), và
HotPotQA (Yang et al., 2018).

Trên các bộ dữ liệu này, tính đúng đắn được xác định bởi một
người thử nghiệm con người dựa trên đầu ra của mỗi hệ thống. Đầu ra
ReAct chỉ đơn giản là câu trả lời cho câu hỏi, trong khi
REBEL thường xuất ra câu trả lời được bao bọc trong lý luận
đằng sau suy nghĩ của hệ thống. Đối với những thí nghiệm này, hai
bộ quy tắc riêng biệt phải được xác định cho việc xác minh sự kiện
và các câu hỏi truy xuất sự kiện. Đối với các câu hỏi truy xuất sự kiện,
một câu trả lời được coi là đúng nếu câu trả lời mong muốn được
chứa trong đầu ra hệ thống. Đối với xác minh sự kiện, nếu
đầu ra mô hình xác định tính đúng đắn của một tuyên bố
giống với tính đúng đắn mong muốn, thì câu trả lời được tạo
được coi là đúng.

Trên Compositional Celebrities, do hạn chế tính toán,
chúng tôi đã thử nghiệm sử dụng 5 trong số 17 danh mục có sẵn,
sử dụng 100 câu hỏi mỗi danh mục, được chọn ngẫu nhiên. Những

3

--- TRANG 4 ---
Suy luận quy nạp được hướng dẫn bởi LLM để giải quyết các vấn đề tổ hợp

danh mục có thể được tìm thấy trong Bảng 1.

Chúng tôi đã thử nghiệm trên FEVER và HotPotQA với 100 câu hỏi
ngẫu nhiên giống nhau từ mỗi bộ dữ liệu trên cả ReAct và
REBEL. Kết quả độ chính xác cho thí nghiệm này có thể
được tìm thấy tại Bảng 2. FEVER có 3 loại nhãn đầu ra
tiềm năng (SUPPORTS, REFUTES, NOT ENOUGH INFO). Để
ngăn chặn các câu trả lời đúng tình cờ từ hệ thống
REBEL, chỉ các câu hỏi có nhãn SUPPORTS và
REFUTES được xem xét.

Đối với thí nghiệm này, REBEL chỉ được phép sử dụng
một công cụ tìm kiếm để truy vấn internet, vì đó là công cụ duy nhất
mà hệ thống ReAct có quyền truy cập.

Mã của chúng tôi, có thể được tìm thấy tại rebel.anarchy.ai,
được triển khai bằng Python sử dụng OpenAI Completion
API để truy cập GPT-3 (text-davinci-003).

4.2. Kết quả
Chúng tôi phát hiện ra rằng REBEL vượt trội hơn ReAct trong việc trả lời
các câu hỏi yêu cầu i) thu thập nhiều sự kiện để xác định
một câu trả lời ii) các truy vấn tìm kiếm rất cụ thể trả về
lượng lớn dữ liệu không có cấu trúc. Với kết quả thí nghiệm
của chúng tôi, chúng tôi đã có thể cho thấy rằng REBEL là một hệ thống
hiện đại về khả năng trả lời nhất quán
các câu hỏi từ các cơ sở kiến thức khác biệt.

4.2.1. TRUY XUẤT SỰ KIỆN NHIỀU BƯỚC
Chúng tôi đã sử dụng 2 bộ dữ liệu để thử nghiệm truy xuất sự kiện nhiều bước:
Compositional Celebrities và HotPotQA.

Compositional Celebrities là một bộ dữ liệu bao gồm 8.6k
câu hỏi về Celebrities trong các danh mục khác nhau. Tất cả
các câu hỏi yêu cầu truy xuất hai sự kiện và lý luận cơ bản.
Hai sự kiện này chưa bao giờ xuất hiện cùng nhau trong bất kỳ văn bản nào
có thể tưởng tượng là một phần của việc huấn luyện LLM và
cách duy nhất để đạt được kết luận là cả hai
phải được đánh giá đúng và kết hợp với
nhau. Chúng tôi phát hiện ra rằng hệ thống REBEL phần lớn
vượt trội hơn hệ thống ReAct ở tất cả 5 danh mục
được thử nghiệm trên Compositional Celebrities.
Trung bình, trên 5 danh mục được thử nghiệm, REBEL đánh bại
ReAct 27.6 phần trăm. Lý do cho điều này có thể là
khả năng của hệ thống REBEL để làm việc với dữ liệu trả về
công cụ không có cấu trúc. Điều này cho phép hệ thống REBEL tạo
và diễn giải các truy vấn công cụ rất cụ thể, trong khi các
hệ thống khác yêu cầu đầu ra chuẩn hóa có thể trở nên
bị ràng buộc bởi một tập hợp các truy vấn công cụ có thể nhỏ hơn.
Kết quả của thí nghiệm này có thể được tìm thấy trong Bảng 1.

HotpotQA là một bộ dữ liệu hỏi đáp thách thức
chứa 113.000 cặp câu hỏi và câu trả lời có nguồn gốc
từ các bài viết Wikipedia. Các câu hỏi trong HotpotQA
cần thiết tổng hợp thông tin từ các nguồn đa dạng

Bảng 1. Độ chính xác (phần trăm câu hỏi được trả lời đúng) của
các thuật toán khác nhau trên các danh mục của Compositional Celebrities.

DANH MỤC | REACT | REBEL
BIRTHPLACE ROUNDED LAT | 28 | 59
BIRTHPLACE CURRENCY | 85 | 94
BIRTHPLACE CURRENCY SYMBOL | 35 | 47
BIRTHYEAR NOBEL LITERATURE | 33 | 82
BIRTHDATE USPRESIDENT | 53 | 90

Bảng 2. Độ chính xác (phần trăm câu hỏi được trả lời đúng) của
các thuật toán khác nhau trên HotPotQA và FEVER.

BỘ DỮ LIỆU | REACT | REBEL
FEVER | 72 | 78
HOTPOTQA | 63 | 50

và không thể được tìm thấy trong các cơ sở kiến thức
huấn luyện có sẵn. ReAct vượt trội hơn REBEL trên HotPotQA
13 phần trăm (Bảng 2).

HotPotQA có các câu hỏi nhiều hơn đáng kể
2-hops, và trên những câu hỏi này REBEL có xu hướng tạo ra
một cây đệ quy lớn của các câu hỏi con. Điều này giới thiệu
vấn đề tạo ra các câu hỏi con mất ngữ cảnh của
câu hỏi gốc. Nhiều lần điều này có thể dẫn đến LLM
không thể lý luận thông qua cửa sổ ngữ cảnh lớn
được tạo ra khi xử lý các lớp câu hỏi con đệ quy này,
dẫn đến LLM không tìm thấy giải pháp.

4.2.2. XÁC MINH SỰ KIỆN
Để thử nghiệm khả năng xác minh sự kiện, chúng tôi đã sử dụng
bộ dữ liệu FEVER. Benchmark này được thiết kế để đánh giá khả năng
của các mô hình trích xuất thông tin thực tế từ các nguồn
văn bản và xác minh các tuyên bố. Nhiệm vụ xác minh sự kiện
bao gồm việc xác định độ chính xác của các tuyên bố được đưa ra
trong một đoạn văn bản.

Trên FEVER, hệ thống REBEL (78 phần trăm độ chính xác)
hoạt động tốt hơn một chút (Bảng 2) so với hệ thống ReAct
(72 phần trăm). Lý do cho việc vượt trội này của hệ thống
REBEL là do lượng lớn
"sự kiện" mà nó thu thập trong quá trình giải quyết đệ quy
một vấn đề xác minh sự kiện. Trong một số trường hợp, hệ thống ReAct
không thể tìm thấy thông tin mà nó đang tìm kiếm để trả lời
một câu hỏi, và do đó báo cáo rằng nó không thể đưa ra quyết định
nếu một sự kiện nhất định là đúng hay không.

4.3. Nghiên cứu loại bỏ
Để xác định hiệu quả của REBEL, chúng tôi đã
tiến hành một số thử nghiệm loại bỏ. Trong những thử nghiệm này, mục đích là

4

--- TRANG 5 ---
Suy luận quy nạp được hướng dẫn bởi LLM để giải quyết các vấn đề tổ hợp

Bảng 3. Độ chính xác (phần trăm câu hỏi được trả lời đúng) của các thuật toán khác nhau trên các danh mục của Compositional Celebrities.

DANH MỤC | GPT3 | REBEL KHÔNG CÔNG CỤ | REBEL
BIRTHPLACE ROUNDED LAT | 16 | 39 | 59
BIRTHPLACE CURRENCY | 95 | 94 | 94
BIRTHPLACE CURRENCY SYMBOL | 28 | 45 | 47
BIRTHYEAR NOBEL LITERATURE | 95 | 90 | 82
BIRTHDATE USPRESIDENT | 44 | 91 | 90

Bảng 4. Độ chính xác (phần trăm câu hỏi được trả lời đúng) của các thuật toán khác nhau trên HotPotQA và FEVER.

BỘ DỮ LIỆU | GPT3 | REBEL KHÔNG CÔNG CỤ | REBEL
FEVER | 77 | 73 | 78
HOTPOTQA | 43 | 46 | 50

cô lập ảnh hưởng của hệ thống REBEL đối với việc giải quyết vấn đề
tổ hợp. Chúng tôi đã sử dụng GPT3 thuần túy (text-davinci-003)
làm baseline. Kết quả của những thử nghiệm này nằm trong (Bảng 3
và Bảng 4).

Những bảng này cho thấy rằng GPT3 vượt trội hơn REBEL (có
hoặc không có công cụ tìm kiếm bên ngoài) khi một
câu hỏi có thể dễ dàng được trả lời với dữ liệu mà
tập huấn luyện của GPT3 có. Điều này được thấy trong Bảng 3
trong các hàng liên quan đến Birthyear NobelLiterature và
Birthplace Currency.

Thuật toán REBEL mà không có công cụ tìm kiếm bên ngoài vượt
trội hơn baseline khi xử lý thông tin là cần
thiết để xác định câu trả lời cuối cùng. Ví dụ về điều này
bao gồm các câu hỏi yêu cầu trả về một ký hiệu tiền tệ
hoặc một vĩ độ được làm tròn. GPT3 thành công trong việc
lấy tên tiền tệ hoặc vĩ độ một cách chính xác, nhưng thất bại trong việc
làm tròn vĩ độ hoặc trả về ký hiệu liên kết với
tên tiền tệ. Việc thêm tìm kiếm bên ngoài đã tăng cường
khả năng của thuật toán REBEL để lý luận với các sự kiện hiện tại,
và do đó cải thiện hiệu suất thuật toán REBEL
trên hầu hết các danh mục của Compositional Celebrities. Thỉnh thoảng,
việc bao gồm công cụ tìm kiếm bên ngoài đã giảm
hiệu suất do bản chất không có cấu trúc của dữ liệu trả về
mà công cụ bên ngoài cung cấp. Một ví dụ về điều này là trên
danh mục Birthyear NobelLiterature của Compositional
Celebrities.

Trên hầu hết các danh mục của Compositional Celebrities và trên
HotPotQA, REBEL mà không sử dụng công cụ tìm kiếm bên ngoài
đã cải thiện hiệu suất so với baseline GPT3. Điều này chỉ ra
rằng cách tiếp cận đệ quy của chúng tôi thêm khả năng lý luận
vào GPT3 độc lập với việc sử dụng công cụ bên ngoài.

5. Phân tích chi phí
Bản chất tìm kiếm đệ quy của thuật toán REBEL
có nghĩa là nó sử dụng nhiều cuộc gọi đến một LLM trước khi

Bảng 5. Thời gian trung bình để trả lời một câu hỏi từ Compositional
Celebrities

THUẬT TOÁN | THỜI GIAN(S)
GPT3 | 0.94
REBEL KHÔNG CÔNG CỤ | 5.358
REBEL VỚI CÔNG CỤ | 9.76

xác định câu trả lời cho một câu hỏi. Nhược điểm của
cách tiếp cận này thể hiện trong độ trễ (Bảng 5)
và chi phí tiền tệ của các truy vấn LLM. Bất kỳ công cụ bên ngoài nào
được cung cấp cho hệ thống REBEL cũng sẽ được gọi
rất thường xuyên, có thể dẫn đến REBEL là một
hệ thống tốn kém về mặt tiền tệ ở phía đó.

Nếu người dùng muốn sử dụng REBEL mà không có công cụ nào, một
chi phí về mặt ảo giác có khả năng phát sinh.
Do thiếu bất kỳ cơ sở kiến thức bên ngoài nào, một ảo giác
trên một câu hỏi con có khả năng làm ô nhiễm
toàn bộ cây lý luận.

6. Kết luận
Chúng tôi đã giới thiệu REBEL, một thuật toán lý luận đệ quy
được thiết kế để sử dụng bất kỳ API tùy ý nào như một công cụ bên ngoài.
REBEL vượt trội hơn hiện đại trên các câu hỏi yêu cầu
thu thập nhiều sự kiện và những câu hỏi được hưởng lợi
từ khả năng thực hiện các truy vấn rất cụ thể đến các
nguồn dữ liệu bên ngoài, có thể không có cấu trúc. REBEL
cũng có cải thiện có thể chứng minh so với LLM GPT3
khi trả lời các câu hỏi yêu cầu xử lý thông tin
nhiều bước. Tuy nhiên, thuật toán REBEL có xu hướng
làm phức tạp quá mức các vấn đề đơn giản, dẫn đến giảm
độ chính xác khi so sánh với baseline GPT3 trên các câu hỏi
yêu cầu tính tổ hợp tối thiểu.

Công việc tương lai lý tưởng sẽ giải quyết việc fine-tuning LLM cho

5

--- TRANG 6 ---
Suy luận quy nạp được hướng dẫn bởi LLM để giải quyết các vấn đề tổ hợp

mỗi bước trong pipeline REBEL và thử nghiệm với
việc hạn chế độ sâu đệ quy của việc tạo câu hỏi con.

Tài liệu tham khảo
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems,
33:1877–1901, 2020.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

Drozdov, A., Schärli, N., Akyürek, E., Scales, N., Song, X.,
Chen, X., Bousquet, O., and Zhou, D. Compositional
semantic parsing with large language models. arXiv
preprint arXiv:2209.15003, 2022.

Huang, J. and Chang, K. C.-C. Towards reasoning in
large language models: A survey. arXiv preprint
arXiv:2212.10403, 2022.

Keysers, D., Schärli, N., Scales, N., Buisman, H., Furrer, D., Kashubin, S., Momchev, N., Sinopalnikov, D.,
Stafiniak, L., Tihon, T., et al. Measuring compositional
generalization: A comprehensive method on realistic
data. arXiv preprint arXiv:1912.09713, 2019.

Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K.,
Clark, P., and Sabharwal, A. Decomposed prompting:
A modular approach for solving complex tasks. arXiv
preprint arXiv:2210.02406, 2022.

Koroteev, M. Bert: a review of applications in natural
language processing and understanding. arXiv preprint
arXiv:2103.11943, 2021.

Lake, B. and Baroni, M. Generalization without systematicity: On the compositional skills of sequence-to-sequence
recurrent networks. In International conference on machine learning, pp. 2873–2882. PMLR, 2018.

Liebowitz, J. Introduction to expert systems. Mitchell Publishing, Inc., 1988.

Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1–35, 2023.

Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu,
Ying Nian Zhu, S.-C., and Gao, J. Chameleon: Plug-andplay compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023.

Micheli, V. and Fleuret, F. Language models are few-shot
butlers. arXiv preprint arXiv:2104.07972, 2021.

Picco, G., Lam, H. T., Sbodio, M. L., and Garcia, V. L. Neural unification for logic reasoning over natural language.
arXiv preprint arXiv:2109.08460, 2021.

Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,
and Lewis, M. Measuring and narrowing the compositionality gap in language models. arXiv:2210.03350v1,
2022.

Qiao, S., Ou, Y., Zhang, N., Chen, X., Yao, Y., Deng,
S., Tan, C., Huang, F., and Chen, H. Reasoning with
language model prompting: A survey. arXiv preprint
arXiv:2212.09597, 2022.

Russell, S. J. Artificial intelligence a modern approach.
Pearson Education, Inc., 2010.

Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use
tools. arXiv preprint arXiv:2302.04761, 2023.

Schlag, I., Sukhbaatar, S., Celikyilmaz, A., tau Yih, W.,
Weston, J., Schmidhuber, J., and Li, X. Large language
model programs. arXiv:2305.05364, 2023.

Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. Fever: a large-scale dataset for fact extraction and
verification. arXiv preprint arXiv:1803.05355, 2018.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chainof-thought prompting elicits reasoning in large language
models. NeurIPS, 2022.

Yang, J., Jiang, H., Yin, Q., Zhang, D., Yin, B., and Yang,
D. Seqzero: Few-shot compositional semantic parsing
with sequential prompts and zero-shot models. arXiv
preprint arXiv:2205.07381, 2022.

Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W.,
Salakhutdinov, R., and Manning, C. D. Hotpotqa: A
dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.

Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
K., and Cao, Y. React: Synergizing reasoning and acting
in language models. ICLR, 2023.

Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N.,
Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and
Chi, E. Least-to-most prompting enables complex
reasoning in large language models. arXiv preprint
arXiv:2205.10625, 2022.

6

--- TRANG 7 ---
Suy luận quy nạp được hướng dẫn bởi LLM để giải quyết các vấn đề tổ hợp

A. Phụ lục A
Prompt Chia tách Câu hỏi
Công cụ chúng ta có quyền truy cập =
công cụ 1: Công cụ trả về kết quả của các truy vấn dạng tự do tương tự như những truy vấn được sử dụng cho wolfram alpha. Điều này hữu ích cho
toán học phức tạp hoặc truy xuất dữ liệu trực tiếp. Có thể được sử dụng để lấy ngày hiện tại.
công cụ 2: Tìm khoảng cách lái xe và thời gian di chuyển giữa hai thành phố.
công cụ 3: Tìm thời tiết tại một địa điểm và trả về bằng độ C.

Q=Question n
Nhìn vào các công cụ chúng ta có quyền truy cập. Chia Q thành các câu hỏi con để trả lời Q mà mỗi câu có thể được giải quyết với một
lần sử dụng một công cụ. Tạo ít câu hỏi con nhất có thể. Chia mỗi câu hỏi con bằng dấu phẩy và không có thông tin bổ sung
nào khác ngoài các câu hỏi con.

B. Phụ lục B
Prompt Kiểm tra Bộ nhớ
Q: "Mấy giờ rồi?" Liệu câu trả lời cho Q có được tìm thấy trong bộ nhớ hoặc trong cơ sở kiến thức của bạn chưa? Trả lời bằng
có hoặc không. không
Q: "Bạn cảm thấy thế nào?" Liệu câu trả lời cho Q có được tìm thấy trong bộ nhớ hoặc trong cơ sở kiến thức của bạn chưa? Trả lời bằng
có hoặc không. có
Q: "Bầu trời màu gì" Liệu câu trả lời cho Q có được tìm thấy trong bộ nhớ hoặc trong cơ sở kiến thức của bạn chưa? Trả lời bằng
có hoặc không. có
Q: "Nhiệt độ ở Portland là bao nhiêu?" Liệu câu trả lời cho Q có được tìm thấy trong bộ nhớ hoặc trong cơ sở kiến thức của bạn
chưa? Trả lời bằng có hoặc không. không
Bộ nhớ: Memory n
Q:Question n Liệu câu trả lời cho Q có được tìm thấy trong bộ nhớ hoặc trong cơ sở kiến thức của bạn chưa? Trả lời bằng có hoặc
không.

C. Phụ lục C
Prompt Đầu vào Công cụ

7

--- TRANG 8 ---
Suy luận quy nạp được hướng dẫn bởi LLM để giải quyết các vấn đề tổ hợp

<TOOL>
<ID>1</ID>
<DESC>Tìm khoảng cách lái xe
và thời gian di chuyển
giữa hai thành phố.</DESC>
<PARAMS>{"origins": thành phố xuất phát,
"destinations": thành phố đích}</PARAMS>
</TOOL>

<CASE>
<Q>Sẽ mất bao lâu
để di chuyển giữa
Nam Phi và Kenya.
</Q>
<THOUGHT>
<P>Đầu vào cho
công cụ 1 nên là gì để
trả lời Q?</P>
<A ty=JSON>
{"origins": "Nam Phi",
"destinations": "Kenya"}
</A>
</THOUGHT>
</CASE>

8

--- TRANG 9 ---
Hình này "flowchart.png" có sẵn ở định dạng "png"
từ:
http://arxiv.org/ps/2309.11688v1
