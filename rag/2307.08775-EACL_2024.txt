# 2307.08775.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2307.08775.pdf
# File size: 2575179 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
EACL 2024
GEAR : Augmenting Language Models with
Generalizable and Efficient Tool Resolution
Yining Lu‚ô°and Haoping Yu‚ô°and Daniel Khashabi
Johns Hopkins University, Baltimore, MD
{ylu130, hyu90, danielk}@jhu.edu
Abstract
Augmenting large language models (LLM) to
use external tools enhances their performance
across a variety of tasks. However, prior works
over-rely on task-specific demonstration of tool
use that limits their generalizability and com-
putational cost due to making many calls to
large-scale LLMs. We introduce GEAR , a com-
putationally efficient query-tool grounding al-
gorithm that is generalizable to various tasks
that require tool use while not relying on task-
specific demonstrations. GEAR achieves better
efficiency by delegating tool grounding and ex-
ecution to small language models (SLM) and
LLM, respectively; while leveraging semantic
and pattern-based evaluation at both question
and answer levels for generalizable tool ground-
ing. We evaluate GEAR on 14 datasets across
6 downstream tasks, demonstrating its strong
generalizability to novel tasks, tools and dif-
ferent SLMs. Despite offering more efficiency,
GEAR achieves higher precision in tool ground-
ing compared to prior strategies using LLM
prompting, thus improving downstream accu-
racy at a reduced computational cost. For ex-
ample, we demonstrate that GEAR -augmented
GPT-J and GPT-3 outperform counterpart tool-
augmented baselines because of better tool use.
1 Introduction
Recently there has been a surge in research on Aug-
mented Language Model (Mialon et al., 2023),
which aims to enable models interface existing
‚Äútools‚Äù for various purposes, such as accessing the
latest information (Izacard et al., 2022), interacting
with third-party services (Liang et al., 2023), per-
forming precise calculations (Schick et al., 2023),
or reasoning via code (Cheng et al., 2022; Gao
et al., 2022). The paradigmatic framework of these
tool-augmented LM studies generally comprises
two steps: selecting a tool and executing it via a
generated API call. Consequently, choosing suit-
able tools is essential for task success.
‚ô°Equal contribution
Final decision 
by LLMTool 
Response: 18 ‚Ä¶ Toolbox üíº
Tool Usage Example Query-Tool Grounding Tool Execution Q: A restaurant charges 4% service charge. If your 
order amounted to 450, how much did you pay? 
Pre-processing SLM 
Post-processing SLM Calc(‚Äú75 + 25‚Äù) Tool Description 
Tool Usage Example ‚ÄúArithmetic 
calculator for ‚Ä¶‚Äù
Wiki(‚ÄúGhana 
flag red meaning‚Äù) Tool Description 
Tool Usage Example ‚ÄúWikiAPI lookup ‚Ä¶‚Äù
MT(‚ÄúHello‚Äù, ‚Äúde‚Äù) Tool Description 
Tool Usage Example ‚ÄúMT tool 
translate ‚Ä¶‚Äù
Q: A restaurant charges 4% service 
charge. If your order amounted to 450, 
how much did you pay? Selected tool ‚úì Calculator Wiki MT QA ‚Ä¶Grounding Score 
The answer is 18 
Figure 1: GEAR leverages small language models
(SLM) to facilitate the process of tool grounding for
a given query and has the ability to add and utilize new
tools for novel tasks without the need for fine-tuning or
extra demonstrations. GEAR utilizes a large language
model (LLM) in the tool execution module to ensure the
accuracy of the final answer.
The existing works teach language models
to select tools using either fine-tuning or in-
context learning approaches. For example, Tool-
former (Schick et al., 2023) is tailored and limited
to a predetermined set of tools observed during
pre-training. On the other hand, approaches based
on in-context learning (Li et al., 2023; Paranjape
et al., 2023; Chen et al., 2023; Sun et al., 2023;
Yao et al., 2022) rely on many calls to LLM and
task-specific demonstrations which diminish their
cost efficiency and limits their scalability to a large
tool library. To address these limitations, we focus
on making the query-tool grounding process more
efficient ,scalable andgeneralizable .arXiv:2307.08775v2  [cs.AI]  31 Jan 2024

--- PAGE 2 ---
Feature CoT Zero-shot CoT Toolformer ToolkenGPT ART GEAR
Tool Use ‚úó ‚úó ‚úî ‚úî ‚úî ‚úî
Novel Task Generalization ‚úó ‚úî ‚úî ‚úó ‚úó ‚úî
Extensibility to New Tools at Inference N/A N/A ‚úó ‚úó ‚úî ‚úî
Grounding Algorithm N/A N/A FinetuneLLM
GenerationLLM-Based or
Cosine SimilarityGEAR
# of LLM Calls at Inference 1 1 1 1 N 1
Input DataTask-Specific
DemonstrationsSingle
QueryAugmented
DatasetSupervised
DataTask-Specific
DemonstrationsSingle
Query
Table 1: Comparing GEAR with the recent related works for generalization, computation efficiency, and key
grounding algorithms. N is the task library size.
In this work, we present GEAR ,Augment lan-
guage models with Generalizable and Efficient tool
Resolution, a query-tool grounding algorithm that
enables efficient use of tools while also allowing
for generalization to both new tasks and large tool
libraries. The GEAR framework (Figure 1) is com-
prised of two key modules: (i) Query-Tool Ground-
ing and (ii) Execution. In the query-tool grounding
module, we compute a grounding score comprised
of semantic and pattern based evaluations (intro-
duced in ¬ß3). The intuition behind the grounding
score is to enable comprehensive query-to-query
and answer-to-answer comparisons by leveraging
tool description and usage examples, respectively.
By considering both question and answer perspec-
tives, the final grounding score provides a compre-
hensive evaluation of the suitability and compati-
bility between the given queries and the available
tools. Then GEAR passes the selected tool and
the given query to the execution module where a
LLM is prompted to generate the appropriate API
call to obtain the ultimate response from the tool.
In general, given ntools in a tool library, GEAR
makes (n+ 1) calls to SLMs and only 1call to
LLM (Algorithm 1).
Compared to all other in-context learning ap-
proaches (Li et al., 2023; Paranjape et al., 2023),
GEAR significantly reduces the workload on the
LLM to do tool grounding, subtask decomposition
and API call generation across all tools by assign-
ing query-tool grounding to SLM. For instance,
compared to ART (Paranjape et al., 2023), GEAR
reduces the calls to LLM by directing its intermedi-
ate calls to an SLM (e.g., GPT-Neo ) leading to 4√ó
reduction in computational cost (FLOPS), while
providing higher accuracy (details in ¬ß5.2; Table 5).
To the best of our knowledge, there is currently
no fine-grained algorithm for query-tool grounding,
nor have there been comprehensive empirical ex-
periments to assess tool grounding accuracy acrossvarious tool library sizes. Thus, we conduct exper-
iments1forGEAR on a variety of different down-
stream tasks and tool libraries. Our experiments
demonstrate that, GEAR improves grounding ques-
tions to tools, which leads to stronger downstream
performance compared to other few-shot or tool-
augmented baselines. For example, GEAR lever-
aging SLMs (e.g., GPT-Neo with 1.3B parameters)
consistently achieves high grounding performance
on 12 datasets from 6 NLP tasks, resulting in bet-
ter downstream accuracy than few-shot prompting
and ART (Paranjape et al., 2023). We also provide
evidence of the strong generalizability of GEAR
to novel tasks, large tool libraries, and different
SLMs.
2 Related Work
We divide the notable prior works on tool-
augmented models into two groups based on how
they modify language models: one uses fine-tuning,
while the other uses in-context prompting. We also
touch upon works in embodied LM applications.
Tool Use via Fine-tuning. There have been some
research efforts focusing on training models to
use various language tools (Thoppilan et al., 2022;
Komeili et al., 2022; Shuster et al., 2022; Khot
et al., 2021, 2022).
More recently, Schick et al. (2023) proposes
Toolformer which uses a self-supervision manner
to train LLMs to use Wikipedia, QA, Calculator,
Machine Translation, and Calendar tools. Parisi
et al. (2022) uses a similar self-supervised approach
for teaching models to use tools. Hao et al. (2023)
treats tools as special tokens of LLM and learns
embeddings for them. Qiao et al. (2023) proposes
a two-stage framework that enables the model to
learn through feedback derived from tool execu-
tion. Yang et al. (2023) employs instruction tuning
1Code to reproduce our results is available.

--- PAGE 3 ---
to enable LLMs to use multimodal tools. Although
fine-tuning allows somewhat accurate tool ground-
ing among those observed during training, a key
issue with the resulting models is that they cannot
utilize new tools without retraining, thus hindering
models‚Äô generalizability to new tools and tasks.
Tool Use via In-Context Learning. Prior work
has used in-context prompting of LLMs utilizes
prompts to guide language models generating con-
textually relevant responses, which is generally
more generalizable than fine-tuning. Some notable
works here include Chain-of-thought (Wei et al.,
2022), Zero-shot CoT (Kojima et al., 2022), among
others. These, however, have no access or use ex-
ternal tools.
ART (Paranjape et al., 2023), and other concur-
rent studies (Lu et al., 2023; Qian et al., 2023)
support accessing new tools through code or assem-
bling tool sequences to generate the final response.
Nonetheless, their way of accessing tools relies on
extra task-specific information like demonstrations
of how a task needs to be divided or conveyed to ex-
isting tools. This restricts their generalizability to
new tasks that may necessitate new tools or a differ-
ent combination of tools. Concurrent work (Hsieh
et al., 2023) addresses this issue via documental
tool descriptions. However, GEAR complements
this work in that, our approach also uses tool out-
puts for more accurate tool grounding.
Another core issue in all these works is the tool
grounding mechanism. Lu et al. (2023); Qian
et al. (2023) rely solely on LLM prompting for
tool grounding while ART applies cosine similar-
ity query/tool representations for task grounding.
However, little is understood about tradeoffs or lim-
its of these approaches, which we explore in our
experiments. To address these, our method extends
these works and captures both semantic and pattern
relationships (introduced in ¬ß3.1 and ¬ß3.2) between
query and tools. This allows GEAR to successfully
identify and utilize unseen tools for low-resource
tasks (novel tasks) without the need for additional
task information. Table 1 compares GEAR , CoT,
Zero-shot CoT, Toolformer, and ART.
Embodied Language Model in Robotics. Re-
cent research has focused on employing language
models for robotic agents planning and their com-
munication with the world (Driess et al., 2023;
Zhao et al., 2023; Song et al., 2022; Huang et al.,
2023; Vemprala et al., 2023). This is similar to thesetup here involving a language model‚Äôs interaction
with external tools. Huang et al. (2022) and Lynch
et al. (2022) leverage various sources of human lan-
guage and textual feedback to guide robots while
solving complex tasks. GEAR shares the same un-
derlying idea with SayCan (Ahn et al., 2022) which
utilizes binary scores for robotic affordance, while
GEAR employs a distinct method that is designed
for more general tool and task settings.
3GEAR : Generalizable and Efficient
Augmented Tool Resolution
We start with the formal problem statement. We
are given an input query Qthat we aim to solve. In
addition, we are provided with a tool library T‚âú
{(T1, d1, œÄ1),(T2, d2, œÄ2),¬∑¬∑¬∑,(Tn, dn, œÄn)}with
ntools. Each tool Tican receive an API call (e.g., a
question or a formula) and respond accordingly, of-
ten in the form of natural language. If the provided
input is unparsable to the tool, it would return an
empty response. Each tool is also supplied with its
natural language description ( di) and demonstra-
tions ( œÄi) showing examples of natural language
questions parsed by each tool.
GEAR aims to find the most appropriate tool
for solving Q. As it can be observed in the Al-
gorithm 1, GEAR iterates over the tools (line 2)
and scores each tool iwith respect to the given
question Q(line 5). This score is a linear combi-
nation of two scores, a semantic similarity score
S(., .)and a pattern similarity score P(., .). Se-
mantic score (defined in ¬ß3.1) provides a measure
of semantic alignment between the tool descrip-
tiondiand the given query Q. Pattern similarity
score (defined in ¬ß3.2) scores the alignment be-
tween the responses obtained from SLM and each
tool, which provides an indication of how closely
the tool‚Äôs output aligns with a preliminary answer.
The algorithm ultimately picks the most appropri-
ate tool based on their scores (line 7) and obtains
the final tool response via an API call generated by
a LLM (line8, line9).
3.1 Semantic Similarity Score
Semantic similarity measures the alignment be-
tween the provided question to the language de-
scription of a tool. For instance, in Figure 2, the de-
scription of Calculator is semantically closer to
a query that contains numbers, leading to a higher
semantic score. Formally, this score is defined as:
S(Q, d i) =fSLM(Q, d i),

--- PAGE 4 ---
Semantic 
Pattern Final Answer f(Q,d  ) i+2
Question 
Calc Usage Example 18Q: A restaurant 
charges 4% service 
charge. If your 
order amounted to 
450, how much did 
you pay? ‚Äú450‚Äùi q ÃÇa i 
You will pay 4% service charge ÃÇa 
Calculator WikiSearch 
0.1713 
0.5447 0.0414 
0.1898 MT
0.1166 
0.1172 ‚Ä¶ Calc(‚Äú450 * 0.04‚Äù) Calculator Call ‚ÄúWikiAPI lookup ‚Ä¶‚ÄùWiki(‚ÄúGhana flag red 
meaning‚Äù) 
‚ÄúAccording to the 
Restaurant Association ‚Ä¶‚Äù MT(‚ÄúRestaurant‚Äù
, ‚Äúde‚Äù)‚ÄúGastst√§tte‚Äù‚ÄúArithmetic calculator 
for ‚Ä¶‚ÄùTool Description: d i Tool Usage Example: i œÄ 
Calc(‚Äú75 + 25‚Äù) 
Calc(‚Äú450‚Äù) if(Q,d ) Tool Description: d i+1 Tool Usage Example: i+1 œÄ 
f(Q,d  ) i+1
Wiki(‚ÄúRestaurant 
service charge‚Äù) q i+1 ÃÇa i+1 
ÃÇa Tool Description: d i+2 Tool Usage Example: i+2 œÄ 
q i+2 ÃÇa i+2 
ÃÇa MT(‚ÄúHello‚Äù, ‚Äúde‚Äù) ‚ÄúMT translates ‚Ä¶‚Äù
‚Ä¶‚Ä¶
Grounding Score 0.7160 0.2312 0.2338 ‚Ä¶Calculator WikiSearch Machine Translation 
Figure 2: GEAR framework. It computes the pattern score by comparing the preliminary answer (in gray line) to
tool responses (in green box) and the semantic score by comparing the query to tool descriptions (in blue box).
Grounding tool with the highest weighted average score and executing it via a LLM to obtain the final answer.
Algorithm 1 GEAR Algorithm
Input: Query Q, Tool library T, Small Language Model
(SLM), Large Language Models (LLM)
Output: Grounded tool, and answer to the input question
1:ÀÜasample‚Üê ‚àí ‚àí ‚àí SLM(Q)
2:for(Ti, di, œÄi)inTdo
3: qisample‚Üê ‚àí ‚àí ‚àí SLM(œÄi+Q) ‚ñ∑Generate API call
4: ÀÜai‚ÜêTi(qi) ‚ñ∑Get the tool‚Äôs response
5: fi(Q)‚ÜêŒ≥S(Q, di) + (1‚àíŒ≥)P(ÀÜa,ÀÜai)‚ñ∑Score it
6:end for
7:Œπ‚Üêarg maxifi(Q) ‚ñ∑Select the best tool
8:qŒπsample‚Üê ‚àí ‚àí ‚àí LLM(œÄŒπ+Q) ‚ñ∑Generate API call
9:aŒπ‚ÜêTŒπ(qŒπ) ‚ñ∑API call to the selected tool
10:Return grounded tool TŒπand the final answer aŒπ.
where fis a similarity function utilizing the repre-
sentation of SLM, quantifying the degree to which
the query Qis semantically close to the tool de-
scription di. A popular choice to implement this
similarity function (used in our experiments) is co-
sine distance between the representations query Q
and tool description di:
S(Q, d i) =cos(enc SLM(Q),enc SLM(di)),
where enc SLM(.)is the representation of SLM.
3.2 Pattern Similarity Score
Pattern similarity provides an answer-level align-
ment score. This score computes an alignment
between a preliminary guess ÀÜaand the response
generated by each tool ÀÜai. For instance, in Fig-
ure 2, the preliminary answer is ‚Äú4‚Äù, which has a
higher pattern similarity score with Calculator ‚Äôs
response (‚Äú450‚Äù, denoted in red), as both are num-
bers. Whereas, the responses from Wiki andMT
are descriptive responses with a large proportion of
English tokens (in black) and a non-ASCII token(in orange) that is not exhibited in the preliminary
answer. Pattern similarity is computed based on
the following steps.
Preliminary guess. First, SLM generates a zero-
shot preliminary answer ÀÜafor the given query using
greedy decoding (line 1).2
Tool-based response. Then SLM is prompted by
the given query and few shot usage examples to
obtain API call qi:
qisample‚Üê ‚àí ‚àí ‚àí ‚àí SLM(œÄi+Q).
We then obtain the tool response ÀÜai‚ÜêTi(qi)ifqi
is parsable by the tool Ti, otherwise empty.
Scoring the alignment. The scoring is based on
a predefined pattern set Sconsisting of distinct
elements that correspond to output patterns of var-
ious tools. These pattern elements, for example,
can represent numbers, English words, symbols,
URLs, or certain robotic movements.3We encode
raw tool response ÀÜaito its corresponding pattern
set{ej(t)| ‚àÄj‚àà {1,2,¬∑¬∑¬∑,|S|},‚àÄt‚ààÀÜai}, where
tis the word token of ÀÜaiand the encoding func-
tionej:t‚Üí S encodes word token to the jth
pattern of Sif token exhibits that pattern, other-
wise empty.4Formally, the output of ejfortis ei-
2We recommend greedy decoding for this zero-shot SLM-
based step to reduce the risk of significantly poor responses
which may occur in stochastic decoding.
3While our evaluation is focused on language tools, the
idea discussed here should in principle generalize to other
modalities such as physical tools.
4For instance, if S={e,f,n} consisting of English,
non-ASCII and number patterns respectively, the sentence
‚ÄúHello World 2023‚Äù would be encoded to {e,e,n} . If mul-
tiple patterns are exhibited in one word token, each pattern
would be encoded separately: the German word ‚Äú l√§cheln ‚Äù
=‚áí{e,f,e} .

--- PAGE 5 ---
ther a multiset of jthpattern ( {S1
j,¬∑¬∑¬∑,Sn
j}where
n‚â•1) or an empty set œï. Thus, the final encoded
pattern set of ÀÜaiis the multisubset of S. The en-
coding of ÀÜafollows the same procedure. Let CÀÜa
j
andCÀÜai
jdenote the number of jthpattern encoded
byejin the pattern set of ÀÜaandÀÜai. Namely, for
ÀÜai,CÀÜai
j=|{ej(t)| ‚àÄt‚ààÀÜai}|. Let|ÀÜa|and|ÀÜai|be
the length of final encoded pattern sets of ÀÜaandÀÜai.
The pattern similarity score between tool response
ÀÜaiand preliminary answer ÀÜais computed as:
P(ÀÜa,ÀÜai) =X
j‚àà{1,¬∑¬∑¬∑,|S|}(CÀÜa
j+Œª)CÀÜai
j
(|ÀÜa|+Œª|S|)|ÀÜai|log1
Pj,
where Pjis the prior probability of the jthpattern
from a prior pattern distribution P.P,Sandej
can be shared across different task and tool library
settings. Add- Œªsmoothing is applied to solve the
pattern zero-frequency issue. However, if ÀÜaiis
empty, P(ÀÜa,ÀÜai)will be assigned its lower bound
value 0. In our experiment, we use regular expres-
sions as encoding functions ej.
Intuitively, the pattern similarity score P(ÀÜa,ÀÜai)
is the cross entropy between the prior pattern dis-
tribution Pand the smoothed joint pattern distri-
bution from true tool response ÀÜaiand preliminary
answer ÀÜa. It is proved to have strict lower and upper
bounds in Appendix A.1 and holds the following
five essential properties: (i) Order Insensitive (ii)
Length Insensitive (iii)Pattern Sensitive (iv)Pat-
tern Set Size Insensitive (v)Commutative . Expla-
nations and proofs of these properties are provided
in Appendix A.2.
We hypothesize that tools could easily elicit their
latent pattern distribution through parsable API
calls, irrespective of its correctness. Therefore,
despite their less reliable performance, SLMs are
sufficient for query-tool grounding, because their
key task is to generate appropriate response pat-
terns in ÀÜafor the given query and parsable API
callqifor the target tool, which is much simpler
than reasoning to make ÀÜa(zero-shot result with-
out tool use) or qi(API call for result with tool
use) correct. In Appendix A.3, we discuss mock
responses which can further enhance the efficiency
and generalizability of the grounding process.
4 Experiment Setup
4.1GEAR Implementation.
We implement GEAR according to the construction
described in ¬ß3. Throughout the experiments theAlgorithm ‚Üí
Grounding Model ‚Üí
Execution Model ‚Üí
Datasets ‚ÜìZero-shot
GPT-JFew-shot
GPT-JART‚àó
llm
GPT-Neo
GPT-JGEAR
GPT-Neo
GPT-J
ASDiv 7.5 21.4 16.7 23.3
GSM8K 0.4 5.6 9.8 3.8
SV AMP 2.0 13.1 11.2 18.6‚Ü±Average (Arithm) 3.3 13.4 12.6 15.2
IWSLT (cn) 10.5 16.9 4.1 21.1
IWSLT (ar) 8.5 18.7 4.8 17.6
IWSLT (de) 7.7 19.3 5.4 32.9
IWSLT (fr) 7.9 22.7 6.7 38.4
IWSLT (ja) 5.5 14.4 3.4 12.9
IWSLT(ko) 8.9 15.2 3.6 14.9‚Ü±Average (MT) 8.2 17.9 4.7 23.0
NQ-Open 10.2 31.1 21.2 43.4
WebQS 5.3 18.2 11.2 22.1
TriviaQA 27.3 46.5 29.3 50.3‚Ü±Average (ODQA) 14.3 31.9 20.6 38.6
CSQA 10.9 37.1 6.3 60.7
COPA 6.5 27.0 1.0 13.6
SocialIQA 8.4 26.0 5.5 41.5‚Ü±Average (CSQA) 8.6 30.0 4.3 38.6
Table 2: Downstream task performance results (¬ß5.1).
Evidently, GEAR -augmented GPT-J outperforms our
baselines when using a consistent set of grounding and
execution models.
LLMs in our study are GPT-J andGPT3 davinci-003
(in short, GPT-3 ), and our SLMs are GPT-Neo ,
GPT2 medium ,GPT2 large ,MiniLM andMPNet .5
Specifically for our implementation of GEAR ,
we use MPNet to calculate semantic similarity
scores and GPT-Neo for generating preliminary an-
swers and API calls to calculate pattern similarity
scores. For LLMs, we use either GPT-J orGPT-3
for final tool execution.
Tools. To evaluate the performance for a variety
of purposes, we create a total of 10 different tools,
including 4 basic tools: Calculator ,MT,Wiki ,
andQA; and 6 novel tools: Timezone Converter ,
Multilingual QA ,Sleep , Exponential
Calculator ,Logarithmic Calculator , and
Movement Controller . All of them are accessi-
ble via specific API calls and have corresponding
returns. Examples of API calls are shown in
Table 13 and more information about tools can be
found in Appendix C.
Datasets. We conduct our experiment on 14
datasets across 6 downstream tasks. The dataset
5We accessed the OpenAI models on April through June,
2023.

--- PAGE 6 ---
Models ASDiv SV AMP SQuAD T-REX TriviaQA MLQA(es)
Toolformer ( GPT-J ) 40.4 29.4 33.8 53.5 48.8 20.6
ART‚àó
llm(GPT-Neo /GPT-3 ) 37.0 21.3 17.7 20.6 24.3 14.0
ART cs(MiniLM /GPT3 davinci-003 ) 86.7 77.3 39.3 50.4 61.0
GEAR (GPT-Neo /GPT3 davinci-003 )74.9 (-11.8) 79.9 (+2.6) 61.1 (+21.8) 83.1 (+32.7) 62.5 (+1.5) 58.3 (+37.7)
Table 3: Comparing GEAR with Toolformer (Schick et al., 2023) and ART (Paranjape et al., 2023) (¬ß5.1). The
original ART work, ART cs, employs MiniLM for cosine similarity strategy and does not have QAorMTfor the
MLQA task.
ModelsEvaluate on ‚Üí
Demonstration ‚ÜìASDiv GSM8K SV AMP TriviaQA NQ-Open WebQS
ART cs(MiniLM /GPT3 davinci-003 )ASDiv 97.9 88.5 87.2 2.1 1.4 0.0
GSM8K 93.8 88.4 81.9 0.3 1.1 0.0
SV AMP 98.3 74.5 75.7 0.0 1.1 0.0
TriviaQA 25.8 32.2 22.5 98.1 96.2 0.4
NQ-Open 25.3 25.2 22.4 97.4 98.2 0.4
WebQS 28.6 39.9 28.3 94.8 96.8 1.1
GEAR (GPT-Neo /GPT3 davinci-003 ) 83.1 83.0 89.0 63.0 65.6 54.3
Table 4: Cross-dataset generalization evaluation of tool grounding accuracy (¬ß5.2). Evidently, GEAR can identify
the appropriate tool for a given task without requiring in-domain demonstrations while ART has a significant
grounding performance decline on out-domain demonstrations, with each score representing grounding accura-
cy/affordance ratio in percentage.
details and evaluation metrics can be found in Ap-
pendix B.4.
4.2 Baseline Systems
We organize our baselines as follows:
‚Ä¢Zero-shot : This baseline directly asks questions
to LLM without any instruction.
‚Ä¢Few-shot : This baseline involves prompting
LLM with natural language instructions that ar-
ticulate the requirements of the given task.
‚Ä¢ART : This approach uses prompting LLM for
multi-step reasoning and tools execution (Paran-
jape et al., 2023). Besides the results in the orig-
inal paper, we experiment with a reimplementa-
tion of ART (referred to as ART‚àó) adapted to our
tools and tasks. Specifically, following the orig-
inal work, we report two variants of this model
with different tool-grounding strategies proposed
in its paper: (1) LLM-based prompting similarity
(ART‚àó
llm) and (2) cosine similarity (ART‚àó
cs).
To ensure a fair comparison between baselines,
we let few-shot, ART‚àó, and GEAR use the same
prompt examples (Appendix H).
5 Experimental Findings
We compare the downstream performances of mod-
els (¬ß5.1), and compare their generalizability tonew tools or tasks (¬ß5.2).
5.1 Results on Downstream Tasks
We first evaluate all our models on the downstream
task performance with a tool library containing
4 basic tools (Table 2). For consistency of com-
parisons, all the baselines use GPT-J for the final
answer execution. GEAR outperforms all the base-
lines across four basic tasks. For example, the ac-
curacy of GEAR -augmented GPT-J is24.3%and
6.7%higher than zero-shot and few-shot baselines
on the ODQA (Open-domain QA) task. Compared
to the ART‚àó
llm,GEAR consistently has superior per-
formance because of better tool use. Later in ¬ß5.2
we show that this performance gap is due to the
difference in tool grounding accuracy. Additional
results using GPT-3 as execution model (in place
ofGPT-J ) are provided in Appendix D.
Table 3 puts Toolformer (Schick et al., 2023),
ART (Paranjape et al., 2023) and GEAR together,
evaluating on their shared datasets. All datasets
are evaluated under a 4 basic tools library except
for MLQA which uses a 5-tools library with an
extra Multilingual QA tool. Since Toolformer
code and model are not available online, we are
not able to reproduce their results and therefore,
copy the numbers from its paper. The compari-
son is unfair to Toolformer as it uses a finetuned
GPT-J model. But it is informative that GEAR -

--- PAGE 7 ---
Algorithm ‚Üí GEAR ART‚àó
llm ART‚àó
llm ART‚àó
cs
Grounding Model ‚Üí
Dataset (w/ 4 Tools) ‚Üì Target Tool ‚ÜìGPT-Neo
(1.3B)GPT2 large
(774M)GPT2 medium
(355M)GPT-Neo
(1.3B)GPT3 davinci-003
(175B)MPNet
(110M)
ASDiv Cal 83.1 77.7 58.7 25.6 46.5 98.8
GSM8K Cal 83.0 65.3 55.6 38.0 45.5 99.5
SV AMP Cal 89.0 76.5 65.1 21.0 50.0 100.0‚Ü±Average (Arithm) 85.0 73.2 59.8 28.2 47.3 99.4
IWSLT (cn) MT 84.1 95.5 98.2 30.0 63.2 99.9
IWSLT (ar) MT 66.6 27.8 61.6 98.6
IWSLT (de) MT 96.9 94.4 95.2 31.6 66.0 94.0
IWSLT (fr) MT 96.6 94.0 96.0 33.8 64.4 92.2
IWSLT (ja) MT 72.4 89.3 91.1 30.8 62.8 97.8
IWSLT (ko) MT 82.2 66.7 91.7 25.9 72.7 99.4‚Ü±Average (MT) 83.1 88.0 94.4 30.0 65.1 97.0
NQ-Open Wiki 63.0 61.3 59.1 10.9 44.0 39.4
WebQS Wiki 65.6 83.1 81.4 13.6 56.8 60.5
TriviaQA Wiki 54.3 77.2 71.7 13.2 58.1 41.5‚Ü±Average (ODQA) 61.0 73.9 70.7 12.6 53.0 47.1
CommonsenseQA QA 77.1 84.0 84.9 10.1 34.9 69.7
COPA QA 41.3 77.2 61.2 7.2 24.4 29.7
SocialIQA QA 75.7 87.6 59.5 16.4 42.4 14.1‚Ü±Average (CSQA) 64.7 82.9 68.5 11.2 33.9 37.8
# of Operation in GFLOPS61573 937 430 5455 7284206160
Table 5: Tool grounding accuracy for 4 downstream tasks with a 4-tools library (¬ß5.2). Bold denotes the highest
value within its grounding strategy and underline represents the highest among all baselines. We find that GEAR
yields better performance compared to the LLM-based strategy on all datasets. GEAR is generalizable to
smaller SLMs and even achieve better grounding results on certain tasks .
augmented GPT-3 outperforms the original work
ART cs, which employs the same-sized model with
task-specific demonstrations, on 4 out of 5 tasks.
This performance gain also emphasizes the strong
generalization capability of GEAR .
5.2 Results on Tool Grounding
We systematically examine the tool grounding ac-
curacy (the percentage of correctly selected tools)
across a variety of tool library sizes and model
sizes. We first calculate the grounding accuracy for
a tool library comprising 4 basic tools. Then we
expand the tool library to a total of 10, as described
in Appendix C.2, by introducing competitor and
distractor tools. We re-evaluate the grounding accu-
racy for the four basic tasks, along with two novel
tasks requiring Multilingual QA andTimezone
Converter tools. The main results are shown in
Table 5 and Figure 3.
GEAR is more generalizable than other query-
tool grounding algorithms. According to Ta-
6Since OpenAI has not open sourced their GPT3 davinci-003 ,
we approximate the operations as # tokens √ó# params , which
is the lower bound of operations. The real amount of opera-
tions should exceed this estimation.ble 5, GEAR utilizing GPT-Neo with 1.3B parame-
ters significantly outperforms the LLM-based strat-
egy proposed by ART (Paranjape et al., 2023), even
when the latter uses GPT-3 which is 134 √ólarger.
The best-reported similarity strategy in ART, which
calculates the cosine similarity between the given
demonstration and textual description of tasks, per-
forms outstandingly well on Arithmetic and MT
tasks. We hypothesize this is because of the pres-
ence of distinct and unique keywords in Arithmetic
and MT queries, which are easily distinguishable
by word embeddings. However, for more open-
ended NLP tasks like Open-domain and Common-
sense QA, word embeddings are less generaliz-
able in selecting the correct tools, resulting in low
grounding accuracy of 47.1%and37.8%. In con-
trast,GEAR ‚Äôs grounding strategy is shown to be
more strong with grounding accuracy of 61.0%and
64.7%on the aforementioned tasks.
Table 4 displays a substantial decline in ground-
ing accuracy of ART (Paranjape et al., 2023)
when using out-domain demonstrations. In con-
trast,GEAR consistently maintains its high perfor-
mance without requiring in-domain demonstrations.
We also demonstrate GEAR outperforms retrieval-

--- PAGE 8 ---
Figure 3: Grounding accuracy of GEAR when the tool
library is expanded from 4 to 10 tools (¬ß5.2). We in-
crementally incorporate these tools: Multilingual
QA,Timezone Converter ,Sleep ,Logarithmic
Calculator , and Movement Controller .
based baselines on query-tool grounding, as shown
in Table 11 in Appendix E.
GEAR is generalizable to smaller language mod-
els. We evaluate the grounding performance of
GEAR on two smaller GPT-2 models. As reported
in Table 5, GEAR consistently exhibits high-level
grounding accuracy on both SLMs and even out-
performs GPT-Neo on certain tasks. For example,
GEAR -augmented GPT2 large achieves 73.9%and
82.9%grounding accuracy for the Open-domain
QA and Commensense QA tasks, greatly higher
than those of ART‚àóbaselines. Moreover, as the
model size increases, the marginal grounding ac-
curacy gain diminishes. This is because as long as
the SLM produces expected patterns for the given
query, the correctness of the preliminary answer
has no bearing on the pattern similarity score (see
case study in ¬ß6.2). Which, in turn, experimen-
tally proves the feasibility of employing SLMs for
query-tool grounding.
GEAR is generalizable to larger tool libraries.
Because of a more comprehensive grounding pro-
cess,GEAR enables certain tasks to generalize bet-
ter for larger sets of tools. Figure 3 displays the
grounding accuracy changing from 4 to 10 tools.
The general low decreasing rates for Arithmetic,
MT and Open-domain QA demonstrate the ability
ofGEAR in handling tool libraries of varying sizes.
We hypothesize the drops between the fourth
and fifth tools of CommonsenseQA and SoicalIQA
datasets are likely due to the introduction of the
Multilingual QA tool which has functional over-Task GEARPerformance change ‚àÜ
¬¨Pattern Sim ¬¨Semantic Sim
Arithm 74.0 -2.3 -11.5
MT 80.5 +10.9 -69.9
ODQA 40.7 -15.4 -21.1
CSQA 33.4 -21.6 -18.9
MLQA 54.4 -10.6 -31.5
TZ Conversion 96.4 +3.6 -94.9
Table 6: The result of leave-one-out ablation study
for 10-tools library (¬ß6.1). The decrease in ground-
ing accuracy on both columns demonstrates the im-
portance of considering both semantic and pattern
scores for query-tool grounding.
lap with the basic QAtool. Specifically, the
Multilingual QA tool can also solve reasoning
tasks by translating contexts from English to En-
glish; therefore, if we consider Multilingual QA
as the correct tool for the Commonsense QA task
as well, the averaged final grounding accuracy of
Commonsense QA task will increase to 49.1%,
with a 15.6%decrease compared to Table 5.
We also compare GEAR and the best variant
ART‚àó
csunder a 10-tools library on 6 downstream
tasks with two extra novel tasks. In short, GEAR
outperforms ART‚àó
cson 5 out of 6 tasks. See Ap-
pendix E for detailed results.
6 Analysis
6.1 Ablation Study
We now perform a leave-one-out experiment to bet-
ter understand the contribution of each score (¬ß3.1
and ¬ß3.2) to the final grounding accuracy. We con-
duct experiments for a 10-tools library with only
either semantic similarity score or pattern similar-
ity score. The results are shown in Table 6. For the
10-tools library, there are 4 out of 6 tasks displaying
grounding accuracy decline in both semantic and
pattern columns, suggesting that it is crucial to con-
sider both semantic and pattern similarity scores
for query-tool grounding. Tasks such as MT and
Timezone Conversion show increased grounding
accuracy in the semantic column, which is likely
due to the same reason discussed in ¬ß5.2: these two
tasks contain unique keywords so that single se-
mantic similarity score suffices to distinguish them
from other tasks (more results in Appendix F.)
6.2 Case Study on SLM‚Äôs Size
It is natural to question whether GEAR will have
much better performance if we replace SLM with

--- PAGE 9 ---
[Question ] Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her
friends every day with four. She sells the remainder at the farmers‚Äô market daily for $2 per fresh duck egg. How
much in dollars does she make every day at the farmers‚Äô market?
[Preliminary answer from GPT2 medium ] i make $2
[Preliminary answer from GPT2 large] the average american spends $1000 a year on food
[Preliminary answer from GPT-Neo ] janet‚Äôs ducks lay 16 eggs per day
[Preliminary answer from GPT-3 ] Janet eats 3of the 16eggs, so she has 16 - 3 = ¬´16-3=13¬ª13 eggs left. She
bakes muffins with 4of these eggs, so she has 13 - 4 = ¬´13-4=9¬ª9 eggs left to sell. Thus, she makes $9 * 2 =
$¬´9*2=18¬ª18 every day at the farmers‚Äô market. Answer: $18.
[Question ] In which decade did Billboard magazine first publish and American hit chart?
[Preliminary answer from GPT2 medium ] the first billboard magazine was published in the fall of 1892.
[Preliminary answer from GPT2 large] the first billboard magazine was published in the united states in the year 900.
[Preliminary answer from GPT-Neo ] the billboard hot 100 is a weekly chart that ranks the most popular songs in the
United States
[Preliminary answer from GPT-3 ] The 1930 s. Specifically, Billboard magazine first published its "Music Popularity
Chart" in 1936.
Figure 4: A comparison of output patterns between SLMs and LLM. The lines subsequent to [ Question ] represents
the output generated by the corresponding model, with patterns ( number ,symbol andEnglish alphabet ) labeled in
different colors. While SLMs tend to be less accurate than LLM, their responses provide sufficient clues (pattern
distribution) about the form of the expected answer .
355M(GPT2) 774M(GPT2)1.3B(GPT Neo) 175B(GPT3)606570758085GEAR Grounding Accuracy (%)
Arithmetic
Commonsense QA
Figure 5: Averaged GEAR grounding performance over
SLM sizes (number of parameters, in log scale) on Arith-
metic and Commonsense QA tasks. Each task is evalu-
ated by three datasets. GEAR with SLM has a similar
grounding accuracy as with LLM .
LLM, namely, ÀÜasample‚Üê ‚àí ‚àí ‚àí ‚àí LLM(Q)in Algorithm 1.
We provide a case study (Figure 4) showing the im-
pact of various SLM choices, including the setting
where replacing SLM with LLM, to further illus-
trate our observation in ¬ß5.2 that as the model size
increases, the marginal grounding accuracy gain
diminishes (Figure 5). In the first example from
GSM8K (Cobbe et al., 2021), we can see that SLM
offers the similar indicative signal as LLM that
the potential answer should contain number and
symbol patterns, despite their responses being in-
correct. We also observe that this phenomenon not
only happens in pattern-specific tasks (e.g. Arith-
metic) but also occurs in more general open-ended
tasks like Commonsense QA. The second Trivi-
aQA (Joshi et al., 2017) example shows that the
pattern distributions generated by the SLMs closelyresemble the LLM‚Äôs distribution: a single number
amid English text.
Thus as long as API calls are properly generated,
it is highly likely that GEAR with SLM will select
the same tool as with LLM. In other words, gener-
ating executable API calls from SLM now becomes
the only empirical limitation of the upper bound
of the pattern similarity score. As the model size
increases, this limitation will become less strict,
resulting in a diminished rate of improvement in
grounding performance.
To validate the above observations, we visualize
the grounding performance of GEAR across dif-
ferent SLM sizes on these two tasks in Figure 5.
Evidently, as the increasing of SLM sizes, the
grounding performance margin tends to decrease.
Note that because of different model families, SLM
grounding performance may not necessarily be
monotonically increasing (orange line).
7 Conclusion
In this paper, we introduce GEAR : a generalizable
query-tool grounding algorithm that enables effi-
cient tool groundings without extra fine-tuning or
task-specific demonstrations. This is accomplished
by introducing a fine-grained scoring mechanism
that leverages both semantic and pattern similar-
ities and leveraging smaller language models for
query-tool grounding. To validate the generalizabil-
ity of GEAR , we conduct extensive experiments
that demonstrate its capacity to deal with large tool
libraries and novel tasks.

--- PAGE 10 ---
Limitations
While GEAR aims to improve the query-tool
grounding and exhibits strong generalization and
robustness for large tool libraries, including user-
provided pipelines, it has a potential limitation in
lacking support for automatic tool pipeline con-
struction. Future works could focus on how to
combine GEAR with automatic reasoning and task
decomposition works, such as ART (Paranjape
et al., 2023), Chameleon (Lu et al., 2023), and
CREATOR (Qian et al., 2023). We believe that
the combination of generalizable and efficient tool
grounding with multi-hop reasoning would further
boost the performance of the current SOTA LLMs.
Theoretically, GEAR supports tools that have
non-textual returns via mock responses. How-
ever, we only test the Sleep and Movement
Controller tools in the main experiment and the
Image Generation tool in the GEAR -augmented
chatbot. Though achieving promising results on
these three tools, future works, especially in the em-
bodied LM area, could further explore how mock
responses can be used in grounding human lan-
guage with physical world tools.
Acknowledgements
The authors would like to thank Adam Byerly, Tian-
jian Li, and Zhengping Jiang for their helpful in-
put on earlier versions of this work. The authors
would like to acknowledge the support of ONR
grant N00014-24-1-2089 and the gifts from Ama-
zon and the Allen Institute for AI. Moreover, the
authors would like to thank the Center for Lan-
guage and Speech Processing members at Johns
Hopkins University for their valuable feedback and
comments. GPU machines for conducting exper-
iments were provided by ARCH Rockfish cluster
(https://www.arch.jhu.edu ).
References
Michael Ahn, Anthony Brohan, Noah Brown, Yev-
gen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu,
Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,
Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-
month, Nikhil Joshi, Ryan Julian, Dmitry Kalash-
nikov, Yuheng Kuang, Kuang-Huei Lee, Sergey
Levine, Yao Lu, Linda Luu, Carolina Parada, Pe-
ter Pastor, Jornell Quiambao, Kanishka Rao, Jarek
Rettinghouse, Diego Reyes, Pierre Sermanet, Nico-
las Sievers, Clayton Tan, Alexander Toshev, VincentVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,
Mengyuan Yan, and Andy Zeng. 2022. Do as i can
and not as i say: Grounding language in robotic af-
fordances. In arXiv preprint arXiv:2204.01691 .
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Mauro Cettolo, Marcello Federico, Luisa Bentivogli,
Jan Niehues, Sebastian St√ºker, Katsuhito Sudoh,
Koichiro Yoshino, and Christian Federmann. 2017.
Overview of the IWSLT 2017 evaluation campaign.
InProceedings of the 14th International Conference
on Spoken Language Translation , pages 2‚Äì14, Tokyo,
Japan. International Workshop on Spoken Language
Translation.
Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong,
Wayne Xin Zhao, and Ji-Rong Wen. 2023. Chat-
cot: Tool-augmented chain-of-thought reasoning on
chat-based large language models. arXiv preprint
arXiv:2305.14323 .
Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu
Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong,
Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
et al. 2022. Binding language models in symbolic
languages. arXiv preprint arXiv:2210.02875 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.
2023. Palm-e: An embodied multimodal language
model. arXiv preprint arXiv:2303.03378 .
Hady Elsahar, Pavlos V ougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon Hare, Frederique Lafor-
est, and Elena Simperl. 2018. T-REx: A large scale
alignment of natural language with knowledge base
triples. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation
(LREC 2018) , Miyazaki, Japan. European Language
Resources Association (ELRA).
Evelyn Fix and Joseph Lawson Hodges. 1989. Dis-
criminatory analysis. nonparametric discrimination:
Consistency properties. International Statistical Re-
view/Revue Internationale de Statistique , 57(3):238‚Äì
247.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language
models. arXiv preprint arXiv:2211.10435 .

--- PAGE 11 ---
Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting
Hu. 2023. Toolkengpt: Augmenting frozen lan-
guage models with massive tools via tool embeddings.
arXiv preprint arXiv:2305.11554 .
Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa
Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Kr-
ishna, and Tomas Pfister. 2023. Tool documenta-
tion enables zero-shot tool-usage with large language
models. arXiv preprint arXiv:2308.00675 .
Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess,
Andy Zeng, Yao Lu, Pete Florence, Igor Mor-
datch, Sergey Levine, Karol Hausman, et al. 2023.
Grounded decoding: Guiding text generation with
grounded models for robot control. arXiv preprint
arXiv:2303.00855 .
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Liang, Pete Florence, Andy Zeng, Jonathan Tomp-
son, Igor Mordatch, Yevgen Chebotar, et al. 2022.
Inner monologue: Embodied reasoning through
planning with language models. arXiv preprint
arXiv:2207.05608 .
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2022. Few-shot learning with re-
trieval augmented language models. arXiv preprint
arXiv:2208.03299 .
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1601‚Äì1611, Vancouver,
Canada. Association for Computational Linguistics.
Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter
Clark, and Ashish Sabharwal. 2021. Text modular
networks: Learning to decompose tasks in the lan-
guage of existing models. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Tushar Khot, Kyle Richardson, Daniel Khashabi, and
Ashish Sabharwal. 2022. Hey ai, can you solve com-
plex tasks by talking to agents? In Annual Meeting of
the Association for Computational Linguistics (ACL)
- Findings .
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In Advances
in Neural Information Processing Systems (NeurIPS).
Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.
Internet-augmented dialogue generation. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 8460‚Äì8478, Dublin, Ireland. Association
for Computational Linguistics.Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 6086‚Äì6096.
Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian
Riedel, and Holger Schwenk. 2020. MLQA: Evalu-
ating cross-lingual extractive question answering. In
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 7315‚Äì7330.
Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu,
Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-
bank: A benchmark for tool-augmented llms. arXiv
preprint arXiv:2304.08244 .
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,
Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,
Shaoguang Mao, et al. 2023. Taskmatrix. ai: Com-
pleting tasks by connecting foundation models with
millions of apis. arXiv preprint arXiv:2303.16434 .
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-
Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-
feng Gao. 2023. Chameleon: Plug-and-play compo-
sitional reasoning with large language models. arXiv
preprint arXiv:2304.09842 .
Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli
Ding, James Betker, Robert Baruch, Travis Arm-
strong, and Pete Florence. 2022. Interactive lan-
guage: Talking to robots in real time. arXiv preprint
arXiv:2210.06407 .
Gr√©goire Mialon, Roberto Dess√¨, Maria Lomeli, Christo-
foros Nalmpantis, Ram Pasunuru, Roberta Raileanu,
Baptiste Rozi√®re, Timo Schick, Jane Dwivedi-Yu,
Asli Celikyilmaz, et al. 2023. Augmented language
models: a survey. arXiv preprint arXiv:2302.07842 .
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and develop-
ing English math word problem solvers. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 975‚Äì984, Online. Association
for Computational Linguistics.
Bhargavi Paranjape, Scott Lundberg, Sameer Singh,
Hannaneh Hajishirzi, Luke Zettlemoyer, and
Marco Tulio Ribeiro. 2023. Art: Automatic multi-
step reasoning and tool-use for large language mod-
els.arXiv preprint arXiv:2303.09014 .
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:
Tool augmented language models. arXiv preprint
arXiv:2205.12255 .
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 2080‚Äì2094, On-
line. Association for Computational Linguistics.

--- PAGE 12 ---
Cheng Qian, Chi Han, Yi R Fung, Yujia Qin, Zhiyuan
Liu, and Heng Ji. 2023. Creator: Disentan-
gling abstract and concrete reasonings of large lan-
guage models through tool creation. arXiv preprint
arXiv:2305.14318 .
Shuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu
Zhang. 2023. Making language models better tool
learners with execution feedback. arXiv preprint
arXiv:2305.13068 .
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Stephen E Robertson, Steve Walker, Susan Jones,
Micheline M Hancock-Beaulieu, Mike Gatford, et al.
1995. Okapi at trec-3. Nist Special Publication Sp ,
109:109.
Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S Gordon. 2011. Choice of plausible alter-
natives: An evaluation of commonsense causal rea-
soning. In AAAI spring symposium: logical formal-
izations of commonsense reasoning .
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,
Christopher Potts, and Matei Zaharia. 2022. Col-
bertv2: Effective and efficient retrieval via
lightweight late interaction.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019. Social IQa: Com-
monsense reasoning about social interactions. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 4463‚Äì
4473, Hong Kong, China. Association for Computa-
tional Linguistics.
Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
arXiv preprint arXiv:2302.04761 .
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,
Eric Michael Smith, Stephen Roller, Megan Ung,
Moya Chen, Kushal Arora, Joshua Lane, et al. 2022.
Blenderbot 3: a deployed conversational agent that
continually learns to responsibly engage. arXiv
preprint arXiv:2208.03188 .
Chan Hee Song, Jiaman Wu, Clayton Washington,
Brian M Sadler, Wei-Lun Chao, and Yu Su. 2022.
Llm-planner: Few-shot grounded planning for em-
bodied agents with large language models. arXiv
preprint arXiv:2212.04088 .
Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai,
and Chao Zhang. 2023. Adaplanner: Adaptive plan-
ning from feedback with language models. arXiv
preprint arXiv:2305.16653 .Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL), pages 4149‚Äì4158, Minneapolis,
Minnesota. Association for Computational Linguis-
tics.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. LaMDA: Language Models for Dialog Appli-
cations. arXiv preprint arXiv:2201.08239 .
Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and
Ashish Kapoor. 2023. Chatgpt for robotics: De-
sign principles and model abilities. Technical Report
MSR-TR-2023-8, Microsoft.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .
Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge,
Xiu Li, and Ying Shan. 2023. Gpt4tools: Teaching
large language model to use tools via self-instruction.
arXiv preprint arXiv:2305.18752 .
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. In International Conference on Learning
Representations (ICLR).
Xufeng Zhao, Mengdi Li, Cornelius Weber, Muham-
mad Burhan Hafez, and Stefan Wermter. 2023. Chat
with the environment: Interactive multimodal per-
ception using large language models. arXiv preprint
arXiv:2303.08268 .

--- PAGE 13 ---
Supplemental Material
Appendix Contents
Appendix A Further illustrations for pattern similarity score: examples and proofs
Appendix B Extra experiment details: hyperparameters, patterns, models and datasets
Appendix C Tool introduction
Appendix D Extra results on downstream experiment
Appendix E Extra results on tool grounding experiment
Appendix F Extra results on ablation study
Appendix G GEAR -augmented chatbot: implementation details and survey results
Appendix H Prompts used for GEAR and few-shot baseline
A Pattern Similarity Score
A.1 Pattern Similarity Score Bounds
Because the count CandŒªare nonnegative, Pj‚àà[0,1],|ÀÜai|and|ÀÜa|indicate the total number of encoded
patterns from tool response and preliminary answer, we always have P(ÀÜa,ÀÜai)‚â•0. In this proof, for
better understanding, we assume a most common case that each word token is encoded to only one
pattern, namely no word token exhibits multiple patterns. Thus, |ÀÜai|and|ÀÜa|are equal to the length of
unencoded sequences of tool response and preliminary answer. pÀÜai(¬∑) =CÀÜai
j
|ÀÜai|andpÀÜa(¬∑) =CÀÜa
j
|ÀÜa|represent
the probability of jthpattern in raw ÀÜaiandÀÜa.
P(ÀÜa,ÀÜai) =X
j‚àà{1,¬∑¬∑¬∑,|S|}(CÀÜa
j+Œª)CÀÜai
j
(|ÀÜa|+Œª|S|)|ÀÜai|log1
Pj
=X
j‚àà{1,¬∑¬∑¬∑,|S|}CÀÜa
jCÀÜai
j+ŒªCÀÜai
j
|ÀÜa||ÀÜai|+Œª|S||ÀÜai|log1
Pj
IfŒª= 0:
P(ÀÜa,ÀÜai)
=X
j‚àà{1,¬∑¬∑¬∑,|S|}CÀÜa
jCÀÜai
j
|ÀÜa||ÀÜai|log1
Pj
=X
x‚àà{E(ÀÜai)}X
y‚àà{E(ÀÜa)}pÀÜai,ÀÜa(x, y)Ix=ylog1
P(x)
=X
x‚àà{E(ÀÜai)}X
y‚àà{E(ÀÜa)}pÀÜai(x)pÀÜa(y)Ix=ylog1
P(x)
=X
x‚àà{E(ÀÜai)}pÀÜai(x) log1
P(x)X
y‚àà{E(ÀÜa)}pÀÜa(y)Ix=y
‚â§X
x‚àà{E(ÀÜai)}pÀÜai(x) log1
P(x)¬∑pÀÜa(x)
(Because it is possible {E(ÀÜai)} ‚à© {E(ÀÜa)}=œï)
=X
x‚àà{E(ÀÜai)}pÀÜai(x)pÀÜa(x) log1
P(x)
‚â§X
x‚àà{E(ÀÜai)}pÀÜai(x) log1
P(x)
=CE(pÀÜai,P)

--- PAGE 14 ---
Properties ÀÜa(Encoded) ÀÜa1(Encoded) ÀÜa2(Encoded) Result
Order Insensitive ene ene een P(ÀÜa,ÀÜa1) =P(ÀÜa,ÀÜa2)
Length Insensitive eee en enenen P(ÀÜa,ÀÜa1) =P(ÀÜa,ÀÜa2)
Pattern Sensitive ene ene enn P(ÀÜa,ÀÜa1)< P(ÀÜa,ÀÜa2)
Commutative ene eee nnn P(ÀÜa,ÀÜa1) =P(ÀÜa1,ÀÜa)
Table 7: Examples illustrating the four essential properties of pattern similarity scores
CE(pÀÜai,P)is the cross-entropy between the pattern distribution of raw tool response and the prior pattern
distribution. {E(ÀÜai)}and{E(ÀÜa)}are two sets of patterns derived from encoding tool response ÀÜaiand
preliminary answer ÀÜa, respectively. pÀÜai,ÀÜa(x, y)is the joint probability of pattern xandyinÀÜaiandÀÜa.
Because ÀÜaiandÀÜaare obtained independently, we can simply write the joint probability as the product of
pÀÜai(x)andpÀÜa(y).Ix=yis the indicator function. P(x)is the prior probaility of the pattern x. Note that
unlike jwhich is an index variable, xandyhere are real pattern variables.
IfŒª >0: letŒ¥‚äÜ {1,2,¬∑¬∑¬∑,|S|} such that CÀÜa
Œ±>0forŒ±‚ààŒ¥andCÀÜa
Œ≤= 0forŒ≤‚àà {1,2,¬∑¬∑¬∑,|S|} \ Œ¥.
P(ÀÜa,ÀÜai)
=X
Œ±‚ààŒ¥CÀÜa
Œ±CÀÜaiŒ±+ŒªCÀÜaiŒ±
|ÀÜa||ÀÜai|+Œª|S||ÀÜai|log1
PŒ±
+X
Œ≤‚àà{1,2,¬∑¬∑¬∑,|S|}\ Œ¥ŒªCÀÜai
Œ≤
|ÀÜa||ÀÜai|+Œª|S||ÀÜai|log1
PŒ≤
=X
Œ±‚ààŒ¥CÀÜaiŒ±
|ÀÜai|¬∑CÀÜa
Œ±+Œª
|ÀÜa|+Œª|S|log1
PŒ±
+X
Œ≤‚àà{1,2,¬∑¬∑¬∑,|S|}\ Œ¥CÀÜai
Œ≤
|ÀÜai|¬∑Œª
|ÀÜa|+Œª|S|log1
PŒ≤
‚â§X
Œ±‚ààŒ¥CÀÜa
Œ±+Œª
|ÀÜa|+Œª|S|log1
PŒ±
+X
Œ≤‚àà{1,2,¬∑¬∑¬∑,|S|}\ Œ¥Œª
|ÀÜa|+Œª|S|log1
PŒ≤
=CEŒ±(ÀúpÀÜa,P) +ŒªCEŒ≤(U(0,|ÀÜa|+Œª|S|),P)
where Uis the uniform distribution and ÀúpÀÜais the smoothed pattern distribution of ÀÜa.
A.2 Pattern Similarity Score Properties
‚Ä¢Order Insensitive: The position of a pattern should not influence the score, as the preliminary answer
generated by the SLM tends to be disorganized.
‚Ä¢Length Insensitive: The score should not be biased toward the length of tools‚Äô responses, as certain tools
are inclined to generate longer responses.
‚Ä¢Pattern Sensitive: Given the prior distribution P, tools that exhibit rare patterns are more likely to be
chosen when the preliminary answer ÀÜaalso exhibits those patterns.
‚Ä¢Pattern Set Size Insensitive: The average pattern similarity score should remain consistent for various
tool library and pattern set sizes. This property ensures a consistent hyperparameter Œ≥(the weight for
semantic and pattern scores).
‚Ä¢Commutative: P(ÀÜa,ÀÜai) =P(ÀÜai,ÀÜa)should be hold for any preliminary answer ÀÜaand tool responses ÀÜai.
Table 7 gives illustrative examples for the pattern similarity score. eandndenote English token pattern

--- PAGE 15 ---
and number pattern. The less frequency of numbers ‚Äú n‚Äù in real corpus compared to English tokens ‚Äú e‚Äù
results in a smaller prior probability P(n)<P(e), leading to the result in the Pattern Sensitive row. In
other words, with the same length, tool response ÀÜa2containing more rare patterns which also exhibit in
the preliminary answer ÀÜawould have higher pattern similarity score.
The Pattern Set Size Insensitive property also holds because the denominator (|ÀÜa|+Œª|S|)|ÀÜai|is
insensitive to the ‚àÜ|S|, given that |ÀÜa| ‚â´ |S| andŒªis typically small. Therefore, as long as the tool
response or preliminary answer does not exhibit the given patterns, namely CÀÜa
j= 0orCÀÜai
j= 0,P(ÀÜa,ÀÜai)
would not significantly change regardless of the size of |S|.
To prove the length-insensitive property, we have to first assume tool responses ÀÜa1andÀÜa2share the
same pattern probability distribution. Namely, we have
CÀÜa1
j
|ÀÜa1|=CÀÜa2
j
|ÀÜa2|,‚àÄj‚àà {1,2,¬∑¬∑¬∑,|S|}
Then the comparison of pattern similarity scores for these two tools is only determined by the preliminary
answer ÀÜa, pattern set size |S|andŒª, with no sensitivity to the length of tool responses.
A.3 Mock Pattern
When dealing with a large tool library, iterating through all tools for true responses is inefficient and some
tools may not have textual responses to encode. Conversely, through the utilization of pattern scores,
we can set certain tools to generate mock responses with corresponding mock patterns during the tool
grounding process, eliminating the requirement for actual execution, thereby reducing the GEAR ‚Äôs time
complexity and generalizing it to various types of tools. In the experiment section ¬ß5, we test the efficiency
and generalizability of mock patterns for tool grounding by adding Sleep andMovement Controller
to the tool library.

--- PAGE 16 ---
B Implementation Details
B.1 Hyperparameters
To avoid bias toward to pattern similarity score, we use add-one smoothing and set Œª= 1. Additionally,
based on our experiment, we observed that the mean of pattern similarity score is consistently three times
greater than the mean of the semantic score. In order to achieve a proper balance between these two
scores, we set Œ≥= 0.75throughout the entire experiment.
B.2 Patterns
For 4 tools experiments, we use the following four patterns: S= {English token pattern: e, non-ASCII
token pattern: f, number pattern: n, symbol pattern: s}. Because we believe these four basic patterns
could cover a lot of language tools. Based on their frequency in the real corpus, we set their prior
probabilities as follows: P= {e: 0.78, f: 0.18, n: 0.05, s: 0.02}.
For generalization experiments where the tool library size varies between 4 to 10, we consistently use
the following prior pattern distribution: P= {e: 0.75, f: 0.15, n: 0.02, s: 0.02, Sleep Pattern: 0.02, Move
pattern: 0.02, Time pattern: 0.02}.
B.3 Models
‚Ä¢GPT-J is from https://huggingface.co/EleutherAI/gpt-j-6b
‚Ä¢GPT-Neo is from https://huggingface.co/EleutherAI/gpt-neo-1.3B
‚Ä¢MiniLM is from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
‚Ä¢MPNet is from https://huggingface.co/sentence-transformers/all-mpnet-base-v2
B.4 Tasks and Datasets
We use the following 12 datasets from 6 downstream tasks in our main experiment, plus 2 extra datasets
(SQuAD (Rajpurkar et al., 2016) and Trex (Elsahar et al., 2018)) in Table 3. To keep the evaluation costs
manageable, we use 1K instances per dataset.
‚Ä¢Arithmetic (Arithm) : We evaluate on ASDiv (Miao et al., 2020), GSM8K (Cobbe et al., 2021) and
SV AMP (Patel et al., 2021) datastes. Given the arithmetic nature of these datasets, we expect successful
grounding in Calculator tool should improve their performance.
‚Ä¢Machine Translation (MT) : We use IWSLT-2017 (Cettolo et al., 2017) dataset to evaluate the utility of
successful grounding to the MTtool. The input data consists of an English prompt and a non-English
context in Simplified Chinese, Arabic, German, French, Japanese, or Korean. We utilize diverse English
prompts for English translation requests (e.g., ‚ÄúHow do you say ... in English‚Äù, ‚ÄúSpeak ... to English‚Äù,
etc.). We sample 1K instances for each source language.
‚Ä¢Open-domain QA (ODQA) : We experiment with NQ-Open (Lee et al., 2019), WebQS (Berant et al.,
2013), and TriviaQA (Joshi et al., 2017), since open-domain questions require external knowledge,
successful grounding of these tasks to Wiki tool improve their performance.
‚Ä¢Commensense QA (CSQA) : To investigate the benefit of utilizing the QAtool, we evaluate all baselines
on CommonsenseQA (Talmor et al., 2019), COPA (Roemmele et al., 2011), and SocialIQA (Sap et al.,
2019). Those datasets require the model to perform commonsense reasoning for a given context and
select the answer from a variety of choices.
‚Ä¢Multilingual QA (MLQA) : MLQA (Lewis et al., 2020) is a hard multilingual question-answering
benchmark, expecting Multilingual QA to tackle such problem. Each instance includes an English
context and a query presented in Arabic, German, Spanish, Hindi, Vietnamese, or Chinese. We randomly
sample 1K instances for each language.

--- PAGE 17 ---
‚Ä¢Timezone Conversion : we create this dataset programmatically by iterating over all combinations
of time zones, randomly-generated numbers which are verbalized into the natural language via real
querying scenarios. Specifically, we set 5 querying templates and 3 time formats, combining them with
randomly selected timezones to construct the dataset. Here are two examples:
My friend is in Cordoba, and I am in Madeira. If it is 2016-07-14 08:24:07 here, what time is it there?
I want to make a call to someone. He is in Johannesburg, and I am in Pitcairn. If it is May 16 2023
10:31:14AM here, what time is it there?
Successfully grounding to the Timezone Converter should improve the performance of this task.
Evaluation Metrics. For the Arithmetic task, we convert all English numerals to their numerical
equivalents and then pick the last number as the answer.7These are not needed when using Calculator
tool, as it always outputs a single number. Ultimately, we compute an exact match accuracy between the
resulting numbers and gold answers. For ODQA and MLQA tasks, following (Schick et al., 2023), we
verify if the generated output contains the gold answer. For the CSQA task, we compute the accuracy
as the ratio of accurately selected outputs. For the MT task, the translation quality is evaluated using a
BLEU (as percentage).
7For zero-shot or few-shot baseline the overall answer typically appears after the rationales.

--- PAGE 18 ---
C Tools
We prioritize two factors for choosing tools: 1) whether they could compete with others 2) whether their
function is naturally beyond the capability of any LLM.
C.1 Basic Tools
Description and usage prompts for each basic tool are provided in Table 15
‚Ä¢QA: Our question-answering system is based on an external language model specially trained for answer-
ing questions. We utilize ChatGPT in our experiment, renowned for its performance in comprehending
and reasoning with human language.
‚Ä¢Calculator : The Calculator is built from the Python built-in function eval, which supports four
fundamental arithmetic operations with priorities that can be specified using brackets. The output is
rounded to three decimal places.
‚Ä¢MT: The core of our machine translation tool is the Google Translate API. It accepts two input arguments:
the text to be translated and the target language.
‚Ä¢Wiki : The last basic tool employed in our experiment is the Wikipedia Search (Wikisearch) engine. It
returns wiki paragraphs in response to queries. This tool advances models by supplying external factual
knowledge and its returned output is more formal and informative than that of QA. In our experiment, we
use ColBERTv2 (Santhanam et al., 2022) as the search retriever to index relevant information.
C.2 Novel Tools
For the selection of novel tools, we follow these two factors: whether they could compete with existing
tools or whether their function is naturally beyond the capability of any LLM. Consequently, we add the
following six tools:
‚Ä¢Logarithmic Calculator andExponential Calculator : These two tools aim to solve logarithm
and exponential problems and serve as competitors to the Calculator tool.
‚Ä¢Multilingual QA : We compose MTandQAtools to form the Multilingual QA pipeline. It involves
two steps: translating the query to the target language using MT, and passing the context and translated
query to the QAto find the final answer.
‚Ä¢Timezone Converter : This tool is implemented by the Python pytz library. It converts a time from
one time zone to another. Such a task is also solvable by the QA tool but not accurately. Therefore, we
want to assess the success rate of grounding the most appropriate tools for such endeavors.
‚Ä¢Sleep : This tool suspends the entire program for a specified duration. This tool is intended to test the
mock response functionality for our system. We do not expect the program to sleep during the tool
grounding procedure; a mocked response is sufficient. However, once selected, this tool should perform
its intended function.
‚Ä¢Movement Controller : This tool instructs a robot to move a specified distance in a chosen direction.
Similarly to Sleep , this tool is used for testing the mock response for grounding tools with non-textual
outputs. During the grounding process, its returned response is a mock text: ‚Äú Robot is moving
forward for {} meters ‚Äù.

--- PAGE 19 ---
Models ASDiv GSM8K SV AMP NQ-Open WebQA TriviaQA
ART cs(MiniLM /GPT3 davinci-003 ) 86.7 69.7 77.3 56.7 17.7 61.0
GEAR (GPT-Neo /GPT3 davinci-003 )74.9 (-11.8) 71.1 (+1.4) 79.9 (+2.6) 53.8 (-2.9) 23.6 (+5.9) 62.5 (+1.5)
Table 8: Comparing GEAR with ART (Paranjape et al., 2023) on Arithmetic and Open-domain QA tasks
Algorithm ‚Üí
Grounding Model ‚Üí
Execution Model ‚Üí
Datasets ‚ÜìZero-shot
GPT-3Few-shot
GPT-3ART‚àó
llm
GPT-Neo
GPT-3GEAR
GPT-Neo
GPT-3
ASDiv 78.7 75.3 37.0 74.9
GSM8K 62.4 69.9 14.7 71.1
SV AMP 75.4 73.7 21.3 79.9‚Ü±Average (Arithm) 72.2 73.0 24.3 75.3
IWSLT(cn) 43.1 30.1 19.2 39.2
IWSLT(ar) 47.2 41.1 16.1 41.8
IWSLT(de) 51.6 40.8 25.0 51.0
IWSLT(fr) 55.8 42.7 25.9 55.0
IWSLT(ja) 31.4 28.6 13.2 28.8
IWSLT(ko) 37.9 31.3 16.5 36.5‚Ü±Average (MT) 44.5 35.8 19.3 42.0
NQ-Open 58.0 66.1 24.0 53.8
WebQS 24.9 28.1 11.2 23.6
TriviaQA 54.9 70.4 24.3 62.5‚Ü±Average (ODQA) 45.9 54.9 19.8 46.6
CSQA 74.7 75.6 5.0 70.1
COPA 45.5 33.7 0.3 36.7
SocialIQA 56.8 64.8 1.2 59.5‚Ü±Average (CSQA) 59.0 58.0 2.2 55.4
Table 9: Downstream task performance result. Evidently, GEAR -augmented GPT-3 achieves competitive results
with GPT-3 few-shot and ART , both of which provided with task-specific demonstrations for solutions.
D Downstream Performance
Results for GPT-3 baselines can be seen in Table 9. For MT and Commensense QA tasks, even the
few-shot performance is lower than zero-shot, we hypothesize that this is because the GPT-3 model has
seen those datasets during the pretraining and memorized them.
A comparison of GEAR with ART (Paranjape et al., 2023) on Arithmetic and Open-domain QA tasks is
provided in Table 8. The downstream accuracy of GEAR -augmented GPT-3 is only slightly higher than
those of ART-augmented GPT-3 , because according to Table 4, ART achieves at least 90% grounding
accuracy on most Arithmetic and Open-domain QA datasets. However, it is worth noting that ART
requires in-domain demonstration for each task/dataset while GEAR does not.

--- PAGE 20 ---
Figure 6: Confusion matrix of grounding results of four basic tools. Each number represents the number of examples
being grounded to the tool.
Dataset (w/10 Tools) Target Tool GEAR ART‚àó
cs
Average (Arithm) Cal 74.0 97.2
Average (MT) MT 80.5 78.5
Average (ODQA) Wiki 40.7 21.1
Average (CSQA) QA 33.4 22.8
Average (MLQA) MLQA 54.4 17.6
Timezone Conversion TZ Conveter 96.4 95.0
Table 10: Tool grounding accuracy for 6 downstream tasks with a 10-tools library (¬ß5.2). GEAR using GPT-Neo
outperforms ART‚àó
csusing MPNet with the cosine similarity strategy on 5 out of 6 tasks.
E Grounding Performance
According to Figure 6, it is clear that Calculator andMTtools have no strong competitors on Arithmetic
and MT tasks, while QAandWiki tools are more likely to compete with each other on CommonsenseQA
and Open-domain QA tasks. This is due to the functional overlap of these two tools on open-ended NLP
tasks.
GEAR is more generalizable than retrieval-based baselines We compare GEAR with two retrieval-
based baselines: Okapi BM25 (Robertson et al., 1995) and KNN (Fix and Hodges, 1989) with 50 training
examples for each tool under the 4-tools library. Like GEAR , BM25 is a general-purpose approach that
does not need supervision. However, from Table 11, the grounding accuracy of BM25 is smaller than
GEAR ‚Äôs (GPT-Neo version) on 13/15 datasets. All MT tasks get a 0%accuracy from BM25 since their
inputs contain non-ASCII tokens, which are not accounted for in the description of the MT tool. Although
the performance of KNN is generally higher than GEAR on MT and Open-domain tasks, it requires
training and is easily overfitting, which hinders its generalizability to low-resource tasks that utilize novel
tools without sufficient labeled data.
GEAR is generalizable to novel tasks We further evaluate GEAR ‚Äôs generalizability to novel tasks using
MLQA (Lewis et al., 2020) and Timezone Conversion datasets. From Table 10, GEAR achieves 54.4%
and96.4%grounding accuracy on these two novel tasks with a 10-tools library. It outperforms ART‚àó
cson
5 out of 6 tasks, revealing its strong generalizability to both large tool libraries and novel tasks.

--- PAGE 21 ---
Algorithm ‚Üí BM25 KNN GEAR (GPT-Neo )
Datasets ‚Üì
ASDiv 0.5 66.5 83.1
GSM8K 0.2 58.7 83.0
SV AMP 1.2 75.1 89.0‚Ü±Average (Arithm) 0.6 66.8 85.0
IWSLT(cn) 100 84.1
IWSLT(ar) 99.7 66.6
IWSLT(de) 80.0 96.9
IWSLT(fr) 84.6 96.6
IWSLT(ja) 100 72.4
IWSLT(ko) 100 82.2‚Ü±Average (MT) 94.1 83.1
NQ-Open 76.2 73.4 63.0
WebQS 45.4 55.9 65.6
TriviaQA 62.5 83.3 54.3‚Ü±Average (ODQA) 61.4 70.9 61.0
CSQA 24.6 75.8 77.1
COPA 0.6 32.9 41.3
SocialIQA 14.5 57.2 75.7‚Ü±Average (CSQA) 13.2 55.3 64.7
Table 11: Tool grounding accuracy for 4 downstream tasks with a 4-tools library (" " denotes 0).GEAR with
GPT-Neo consistently achieves high grounding performance compared to BM25 and KNN.
Figure 7: The average similarity scores for different tasks and tools. Clearly, the semantic and pattern scores
(already weighted by Œ≥) collaboratively and accurately identify tools for the four basic tasks.
F Ablation Study
For the 4-tools library, we plot the average final grounding score for each task and tool in Figure 7.
Notably, neither the semantic nor the pattern similarity score dominates the query-tool grounding on most
tasks, but they collaborate with each other to correctly identify the tools.

--- PAGE 22 ---
GGEAR Augmented Chatbot
Because GEAR does not require extra task-specific demonstrations, one of its practical applications is that
it can be integrated into any chatbot. To validate it, we create a GEAR augmented chatbot using ChatGPT
as the execution LLM and conduct a survey experiment.
Figure 8 illustrates the differences between GEAR -augmented chatbot and a normal chatbot and how
GEAR interacts with a LM in a dialogue setting. For each user query, we first prompt the LM to determine
if a tool usage is necessary. If true, the original query will be sent directly to GEAR , andGEAR will return
the response from the selected tool as well as the tool name and confidence score for selecting that tool.
This information is then processed by the LM to generate a more natural, user-friendly response. Figure 9
provides examples of how our GEAR augmented chatbot works. We equip it with the following six
tools: Weather Search, Location Search, Image Generation, Current Time-Timezone Converter Pipeline,
Wikipedia Search and Machine Translation.
We surveyed 50 individuals about the use of our GEAR -augmented chatbot. The evaluators first use
ChatGPT-based chatbot for two weeks, then switch to a GEAR -augmented chatbot for the next two
weeks. After fully experiencing these two chatbots, they are asked to complete the survey (Table 12)
which contains four types of questions regarding tool grounding performance and final answer quality.
Participants are unpaid and their feedback is unmodified.
The survey reveals that 76% of users agree that integrating tool usages makes the chatbot more useful
and fascinating, and more than 90% of queries grounded correct tools. Image generation and weather
search tools are the most popular tools among the 6 tools, with more than 50% of users employing them
to solve problems. Regarding response quality, our survey indicates that an average of 78.4%of questions
are answered to the user‚Äôs satisfaction, a 16.9%increase in satisfaction compared to the previous chatbot
that lacks the tool utilization functionality. The Current Time-Time Zone Convertor Pipeline has the
highest accuracy, at 100% , while the Machine Translation tool has the lowest quality, with a satisfaction
ratio of only 50.5%. We infer that the performance of the Google Translate API may not be adequate to
satisfy the needs of our evaluators, given that most of them are translating extremely complex sentences
between English, Japanese, and Chinese.
In summary, GEAR substantially improves users‚Äô experience on ChatGPT, and it also has excellent
generalization capabilities to novel tools. Note that these novel tools lack training data, but with GEAR
and just a few words of tool description and usage examples, they can be easily integrated into a chatbot
to provide precise and reliable answers to users.

--- PAGE 23 ---
More Human SupervisionNormal Chatbot Response [w/o GEAR]
What time will be tomorrow this time?
Less Human SupervisionTool-Augmented Chatbot Response [w/ GEAR]
I'm sorry, but as an AI language model, I don't have real-time 
data or the ability to predict the future.
Try again
I apologize for any confusion. If you provide me with your current 
time zone, I can give you the time difference between now and 
tomorrow at the same time.
The current time is 2023-06-10 23:39:28, EDT. 
Tomorrow's time will be 2023-06-11 23:39:28 
EDT. I used the Time API and the confidence 
score is 0.4953EDT
The Eastern Daylight Time (EDT) is observed in certain regions of 
North America during the daylight saving time period. However, 
as I mentioned earlier, I don't have real-time data. 
As of now, EDT is usually UTC-4:00 during daylight saving time. 
So, if you are currently in EDT and it is, for example, 3:00 PM, 
then tomorrow at the same time would be around 3:00 PM as well. 
Try again
Please wait while I am selecting and calling the API...
GEARLM
[Yes] Original Question
[Result] 2023-06-10 23:39:28, EDT 
[API] Time [Confidence] 0.4953What time will be tomorrow this time?Hello
[No] LM ResponseHello! How can I assist you today?
Figure 8: A comparison between the performance of ChatGPT and GEAR augmented chatbot. GEAR requires
minimal human supervision, excels in numerous tool-solvable tasks, and offers interpretable confidence scores for
users.
(a)GEAR augmented chatbot screenshot illustrating its tool library.
(b)GEAR augmented chatbot screenshot of using the Time tool.
(c)GEAR augmented chatbot screenshot of using the Location Search tool.
Figure 9: Screenshots of GEAR augmented chatbot using various tools. Using the command /GEAR to ask GEAR
chatbot to output tool response directly without going through the ChatGPT. While the command @TallChatBot
enables a normal conversation where GEAR interacts with ChatGPT to provide more human-readable answers.

--- PAGE 24 ---
Survery Question Question Type Answer
How would you rate your overall experience with GEAR -augmented chatbot? rating scale 0-10
Do you think GEAR -augmented chatbot has become smarter compared to the previous
version?rating scale 0-10
Do you think GEAR -augmented chatbot has become more helpful than the previous one? rating scale 0-10
How accurate do you think the answers of the older bot are? rating scale 0-10
How accurate do you think the answers of the new version bot are? rating scale 0-10
Have you noticed that the chatbot is using external tools to help you? Likert scales yes or no
How would you rate the chatbot‚Äôs accuracy in choosing the right tool to answer your
query?rating scale 0-10
Can you recall a situation where the chatbot chose the wrong tool for your query? If so,
please describe it briefly.open-ended open-ended
Have you ever instructed the chatbot to use a different tool for your query, or did the
chatbot automatically choose a different tool because you weren‚Äôt satisfied with the results?Likert scales yes or no
Will the chatbot be able to switch to the right tool based on your instructions? Likert scales yes or no
When a chatbot uses an external tool, how would you rate its response accuracy? rating scale 0-10
Can you recall any instances where the chatbot used external tools to produce output errors
or didn‚Äôt meet your expectations? If so, please describe it brieflyopen-ended open-ended
What tools of chatbots have you used? multiple-choice multiple-choice
How would you rate the accuracy of the output generated by the chatbot using the Time
tool?rating scale 0-10
How would you rate the accuracy of the output generated by the chatbot using the
Wikisearch tool?rating scale 0-10
How would you rate the accuracy of the output generated by the chatbot using the Weather
Lookup tool?rating scale 0-10
How would you rate the accuracy of the output generated by the chatbot using the Location
Search tool?rating scale 0-10
How would you rate the accuracy of the output generated by the chatbot using the Image
Generation tool?rating scale 0-10
How would you rate the accuracy of the output generated by the chatbot using the Machine
Translation tool?rating scale 0-10
Please provide any additional feedback or suggestions you have for improving GEAR -
augmented chatbot performance.open-ended open-ended
Overall Score you want give to the GEAR -augmented chatbot rating scale 0-100
Table 12: Survey Questions

--- PAGE 25 ---
H Prompts
Table 13 provides examples of API calls and outputs for each tool
Table 14 shows task-specific demonstrations used for the few-shot baseline in the experiment
Table 15 presents the description and usage example of each basic tool.
Tool Example API Call Example Output
Question
AnsweringQA("What century did the Normans first gain their sepa-
rate identity?")The Normans first gained their
separate identity in the 11th cen-
tury.
Calculator Calculator(2 + 4) 6
Machine Trans-
lationMT("Â§™Â§ö‰∏úË•øË¶ÅÂú®Ëøô18ÂàÜÈíüÂÜÖËÆ≤Ëø∞‰∫Ü„ÄÇ", "en") There are too many things to be
described in this 18 minutes.
Wikipedia
SearchWikiSearch("Lord Of The Flies") Lord of the Flies (song) "Lord
of the Flies" is an Iron Maiden
single and second track on their
1995 album "The X Factor".
Multilingual
QAMultilingualQA("question: „ÄäË°óÊú∫Ê∏∏ÊàèË°óÂ§¥Èú∏ÁéãII„ÄãÁöÑ
Ê∏∏ÊàèÊú∫‰∏äÊúâÂ§öÂ∞ëÁî®‰∫éÊéßÂà∂ËßíËâ≤ÁöÑÂúÜÂΩ¢Áâ©‰ΩìÔºücontext:
For example, the six button layout of the arcade games
Street Fighter II or Mortal Kombat cannot be comfortably
emulated on a console joypad, so licensed home arcade
sticks for these games have been manufactured for home
consoles and PCs.")Six
Exponential Pow(2, 3) 8
Logarithm Log(2, 8) 3
Timezone Con-
verterTimezoneConverter("2022-01-02 22:00:00", "Asia/Shang-
hai", "America/New_York")2022-01-02 09:00:00
Sleep Sleep(20) Sleep for 20 seconds ( Mock Re-
sponse )
Movement Con-
trollerRobotMove(0.3) Robot is moving forward for 0.3
meters ( Mock Response )
Table 13: Examples of API Calls and Outputs for Each Tool

--- PAGE 26 ---
Task Demonstration
MathYou are the Calculator tool. Your task is to answer the
questions that contain numbers and require arithmetic
operations, including addition, subtraction, multiplication,
division. Here are some examples:
Input: There were 86 pineapples in a store. The owner sold
48 pineapples. 9 of the remaining pineapples were rotten and
thrown away. How many fresh pineapples are left?
Output: There are total 86 pineapples. 48 pineapples are sold
out, so there are 86 - 48 pineapples now. 9 of the remaining
are thrown away, so there are 86 - 48 - 9 pineapples. That is
29 pineapples.
Commonsense Rea-
soningYou are the Question Answering tool that answers questions by
reasoning and commonsense knowledge. Here are some examples:
Input: The women met for coffee. What was the cause of this?
A: The cafe reopened in a new location. B: They wanted to catch
up with each other.
Output: Considering the options, the more likely cause for
the women meeting for coffee would be B: They wanted to catch
up with each other. Meeting for coffee is often chosen as a
way to have a relaxed and informal conversation, providing an
opportunity for friends or acquaintances to reconnect and share
updates about their lives.
Open-domain Ques-
tion AnsweringYou are the Wikipedia Search tool that is to look up information
from Wikipedia that is necessary to answer the question. Here
are some examples:
Input: The colors on the flag of Ghana have the following
meanings: green for forests, and gold for mineral wealth. What
is the meaning of red?
Output: The color Red commemorates those who died or worked for
the country‚Äôs independence.
Machine TranslationYou are the Machine Translation tool that is used for
translating text from one language to another. Here are some
examples:
Input: How do I ask Japanese students if they had their dinner
yet?
Output: „Åî„Çí„ÇÇ„ÅÜ È£ü„Åπ„Åæ„Åó„Åü„Åã„ÄÇ
Table 14: Example of Various Task Demonstrations for Few-Shot Baselines

--- PAGE 27 ---
Tool Description Few-Shot Prompt
CalculatorCalculator API is
used for answering
questions that
contain numbers and
require arithmetic
operations,
including addition,
subtraction,
multiplication,
division.Calculator API is used for solving questions that require
arithmetic operations, including addition, subtraction,
multiplication, division. You task is to rephrase
the question prepended by the special token <Q>and
generate Calculator API call prepended by <API>for
solving that question. You can call the API by writing
"[Calculator(formula)]" where "formula" is the arithmetical
formula you want to solve. Here are some examples of
Calculator API calls:
Input: There were 86 pineapples in a store. The owner sold
48 pineapples. 9 of the remaining pineapples were rotten and
thrown away. How many fresh pineapples are left?
Output: <Q>There are total 86 pineapples. 48 pineapples
are sold out, so there are 86 - 48 pineapples now. 9 of the
remaining are thrown away, so there are 86 - 48 - 9 pineapples.
<API>[Calculator(86 - 48 - 9)].
Question
AnsweringQuestion Answering
API answers
questions by
reasoning and
commonsense
knowledge.Question Answering API answers questions by reasoning and
commonsense knowledge. You task is to rephrase the question
prepended by the special token <Q>and generate QA API call
prepended by <API>for solving that question. Here are some
examples of API calls: You can call the API by writing
"[QA(question)]" where "question" is the question you want
to ask. Here are some examples of QA API calls:
Input: What do people want to acquire from opening business?
A: home B: wealth C: bankruptcy D: get rich
Output: <Q>What do people want to acquire from opening
business? A: home B: wealth C: bankruptcy D: get rich
<API>[QA("What do people want to acquire from opening
business? A: home B: wealth C: bankruptcy D: get rich")].
Wiki SearchWikipedia Search
API is to look up
information from
Wikipedia that is
necessary to answer
the question.Wikipedia Search API is to look up information from Wikipedia
that is necessary to answer the question. You task is
to rephrase the question prepended by the special token
<Q>and generate Wikipedia Search API call prepended by
<API>for solving that question. You can do so by writing
"[WikiSearch(term)]" where "term" is the search term you want
to look up. Here are some examples of WikiSearch API calls:
Input: The colors on the flag of Ghana have the following
meanings: green for forests, and gold for mineral wealth.
What is the meaning of red?
Output: <Q>Ghana flag green means forests, Ghana flag gold
means mineral wealth, what is the the meaning of Ghana flag
red? <API>[WikiSearch("Ghana flag red meaning")].
Machine
TranslationMachine Translation
API is used for
translating text
from one language
to another.Machine Translation API is used for translating text from one
language to another. You task is to rephrase the question
prepended by the special token <Q>and generate MT API call
prepended by <API>for solving that question. You can do so
by writing "[MT(text, target_language)]" where "text" is the
text to be translated and "target_language" is the language to
translate to. Here are some examples of MT API calls:
Input: How do I ask Japanese students if they had their
dinner yet?
Output: <Q>Translate "Did you have dinner yet" in Japanese
<API>[MT("Did you have dinner yet?", "ja")].
Table 15: Descriptions and Usage Prompts of Four Basic Tools
