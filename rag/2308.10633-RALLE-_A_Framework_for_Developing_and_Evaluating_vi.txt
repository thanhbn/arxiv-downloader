# 2308.10633.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2308.10633.pdf
# Kích thước file: 1528854 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
RALLE: Một Framework để Phát triển và Đánh giá
Mô hình Ngôn ngữ Lớn tăng cường Truy xuất
Yasuto Hoshi∗, Daisuke Miyashita∗, Youyang Ng, Kento Tatsuno,
Yasuhiro Morioka, Osamu Torii, Jun Deguchi
Kioxia Corporation, Nhật Bản
yasuto1.hoshi@kioxia.com
Tóm tắt
Các mô hình ngôn ngữ lớn tăng cường truy xuất
(R-LLMs) kết hợp các mô hình ngôn ngữ lớn
(LLMs) được huấn luyện trước với hệ thống
truy xuất thông tin để cải thiện độ chính xác
của việc trả lời câu hỏi thực tế. Tuy nhiên,
các thư viện hiện tại để xây dựng R-LLMs
cung cấp các trừu tượng cấp cao mà không
có đủ tính minh bạch để đánh giá và tối ưu
hóa prompts trong các quy trình suy luận cụ
thể như truy xuất và sinh. Để giải quyết
khoảng trống này, chúng tôi trình bày RALLE,
một framework mã nguồn mở được thiết kế
để tạo thuận lợi cho việc phát triển, đánh giá
và tối ưu hóa R-LLMs cho các nhiệm vụ chuyên
sâu về kiến thức. Với RALLE, các nhà phát
triển có thể dễ dàng phát triển và đánh giá
R-LLMs, cải thiện prompts thủ công, đánh giá
các quy trình suy luận riêng lẻ, và đo lường
hiệu suất tổng thể của hệ thống một cách
khách quan và định lượng. Bằng cách tận
dụng các tính năng này, các nhà phát triển
có thể nâng cao hiệu suất và độ chính xác
của R-LLMs trong các nhiệm vụ sinh chuyên
sâu về kiến thức. Chúng tôi mở mã nguồn
tại https://github.com/yhoshi3/RaLLe .

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) đã cho thấy tiềm
năng to lớn cho các nhiệm vụ hiểu và sinh ngôn ngữ
tự nhiên (Brown et al., 2020; Chowdhery et al., 2022;
OpenAI, 2023). Tuy nhiên, chúng gặp thách thức khi
trả lời các câu hỏi thực tế do ảo giác (hoặc bịa đặt)
(Bang et al., 2023; Borji, 2023), kiến thức tham số
lỗi thời (Liska et al., 2022), và hiệu quả bộ nhớ của
kiến thức tham số (ví dụ, Heinzerling và Inui, 2021).
Để giải quyết những hạn chế này, các nhà nghiên cứu
đã chuyển sang phương pháp tăng cường truy xuất
được sử dụng trong trả lời câu hỏi miền mở (QA)
(Chen et al., 2017), sau này được gọi là mô hình
ngôn ngữ lớn tăng cường truy xuất hay R-LLMs.

So với các thiết lập sách đóng nơi các mô hình ngôn
ngữ sinh câu trả lời mà không truy xuất, R-LLMs
(thiết lập sách mở) cho phép truy xuất thông tin liên
quan từ cơ sở dữ liệu hoặc kho văn bản bên ngoài
(Mialon et al., 2023; Ng et al., 2023), điều này đã
dẫn đến cải thiện độ chính xác trong QA miền mở
(Shi et al., 2023). Ngoài ra, R-LLMs có thể có được
các tính năng mở rộng ngay cả khi không cần huấn
luyện bổ sung, như tham chiếu rõ ràng, giảm ảo giác
thực tế (Nakano et al., 2021), và cập nhật dễ dàng
nguồn kiến thức (ví dụ, Guu et al., 2020; Ng et al.,
2023).

Sinh tăng cường truy xuất cần nghiên cứu và phát
triển thêm để đạt được tiềm năng đầy đủ. Ví dụ,
mặc dù hệ thống retriever-reader đã được huấn
luyện trên tập dữ liệu Natural Questions (NQ)
(Kwiatkowski et al., 2019), điểm F1 của nó trên
nhiệm vụ câu trả lời ngắn là 68.3 và vẫn thua kém
điểm F1 oracle là 75.7 (Asai và Choi, 2021). Điều
này ngụ ý rằng có thể cải thiện thêm phương pháp
sinh tăng cường truy xuất. Ngoài ra, người dùng có
thể nhận thức rằng các kết quả được sinh bởi R-LLMs
có thể chứa lỗi thực tế, đặc biệt khi áp dụng cho các
nhiệm vụ chuyên sâu về kiến thức. Tuy nhiên, hiện
tại thiếu framework đánh giá dễ tiếp cận để đánh giá
chất lượng đầu ra của chúng. Điều này khiến việc
xác định các lĩnh vực cần cải thiện trở nên khó khăn.

Hơn nữa, việc có các công cụ hiệu quả để phát triển
R-LLMs là rất quan trọng. Những công cụ này nên
cho phép thiết kế các bước suy luận như retrieve-then-
generate, lựa chọn sự kết hợp của retrievers và LLMs,
đánh giá hiệu suất của toàn bộ hệ thống, và kiểm tra
các prompts được sử dụng trong mỗi bước suy luận.
Các công cụ hiện có như ChatGPT Retrieval Plugin¹,
Guidance², và LangChain³(Chase, 2023), cung cấp
mức độ trừu tượng cao, khiến việc xác minh chức
năng của các bước suy luận riêng lẻ hoặc tối ưu

¹https://github.com/openai/
chatgpt-retrieval-plugin
²https://github.com/microsoft/guidance
³Lưu ý: Mã của chúng tôi không sử dụng cả hai này.

--- TRANG 2 ---
Embedding và 
Indexing
Nguồn kiến thức
Tài liệu
Document
Encoder
Embedding
Vectors
IndexThiết kế một
Chuỗi suy luậnĐánh giá R-LLM đã phát triển Giao diện Chat đơn giản
Dễ dàng kiểm tra các thực hành tốt nhất. Để theo dõi kết quả, configs, và prompts
trên MLflow.
Output (Câu trả lời)Action 1
ví dụ: Viết lại câu hỏi
với LLMRALLE: Framework phát triển và đánh giá Retrieval-Augmented LLM 
Input (Câu hỏi)
Đường cao tốc Mỹ số 1 bắt đầu và kết thúc ở đâu
Đường số 1 của Mỹ bắt đầu ở Key West, 
Florida và kết thúc ở Fort Kent, Maine.
Action 2
ví dụ: Truy xuất sử dụng
câu hỏi đã viết lại
Action N
ví dụ: Xuất ra câu trả lời
với LLMPrompt Engineering cho mỗi Action
Sinh câu trả lời
Prompt template Prompt LLM output
Làm nổi bật các
câu trả lời chuẩnHình 1: Tổng quan về RALLE, framework phát triển và đánh giá được đề xuất cho R-LLMs. Có thể định nghĩa bất kỳ số lượng actions nào cho một R-LLM. Mỗi action có thể được thực thi riêng lẻ để kiểm tra các prompts tương ứng. Thiết lập thí nghiệm và kết quả đánh giá có thể được theo dõi bằng MLflow. Ngoài ra, có thể xây dựng giao diện chat đơn giản để kiểm tra các thực hành tốt nhất từ giai đoạn phát triển và đánh giá trong môi trường thực tế.

hóa prompts trong mỗi bước trở nên khó khăn. Việc
thiếu tính minh bạch này có thể cản trở việc tối ưu
hóa R-LLMs.

Trong bài báo này, chúng tôi đề xuất RALLE, một
framework dễ tiếp cận cho việc phát triển và Đánh
giá mô hình Ngôn ngữ Lớn tăng cường Truy xuất.
Chúng tôi cũng trình bày kết quả đánh giá của một
số R-LLMs mà chúng tôi đã xây dựng bằng cách sử
dụng các retrievers và LLMs mã nguồn mở. Theo hiểu
biết của chúng tôi, RALLE là framework đầu tiên
trao quyền cho các nhà phát triển R-LLM và nhà
nghiên cứu QA miền mở để phát triển, đánh giá và
cải thiện R-LLMs một cách hiệu quả bằng các chỉ số
khách quan.

RALLE cung cấp một số lợi ích chính:

1. Phát triển và kiểm tra dễ dàng: người dùng có thể
dễ dàng lựa chọn, kết hợp và kiểm tra các retrievers
và LLMs khác nhau, đặc biệt là các mô hình mã
nguồn mở, trong giao diện đồ họa.

2. Đánh giá khách quan các R-LLMs: RALLE cung
cấp các thí nghiệm có thể tái tạo với các benchmark/
chỉ số khách quan, cho phép đánh giá hiệu suất
R-LLM một cách khách quan.

3. Prompt engineering minh bạch: tất cả đầu vào
(prompts) và đầu ra của mỗi action đều hiển thị
cho nhà phát triển, cho phép khám phá và tối ưu
hóa prompts dễ dàng.

2 Sử dụng RALLE

Hình 1 trình bày tổng quan về các tính năng chính
của framework được đề xuất⁴. Quy trình phát triển
chính bao gồm ba giai đoạn: (1) embedding và
indexing các tài liệu nguồn kiến thức, (2) thiết kế
chuỗi suy luận bao gồm một R-LLM với các mẫu
prompt tùy chỉnh cho mỗi action, và (3) benchmark
R-LLM đã phát triển.

2.1 Document Embedding và Indexing

Để bắt đầu, các tài liệu nguồn kiến thức có thể được
mã hóa bằng mô hình encoder tùy ý, chẳng hạn như
sparse hoặc dense retriever. Để indexing hiệu quả
các dense embeddings, một số phương pháp có sẵn
mặc định, bao gồm Faiss (Johnson et al., 2019),
HNSW (Malkov và Yashunin, 2020), và DiskANN
(Jayaram Subramanya et al., 2019). Mặc định, một
chỉ mục HNSW được xây dựng với ef_construction
= 128 (kích thước danh sách động cho các láng giềng
gần nhất) và m = 32 (số lượng liên kết được tạo cho
mỗi phần tử mới trong quá trình xây dựng đồ thị).

2.2 Xây dựng chuỗi

Sau khi hoàn thành document embedding và indexing,
các retrievers (và các chỉ mục tương ứng) và LLMs
có thể được tải thông qua GUI dựa trên Gradio⁵

⁴Vui lòng xem thêm video minh họa.
⁵https://www.gradio.app/

--- TRANG 3 ---
(Abid et al., 2019) để thiết lập chuỗi suy luận bao
gồm một R-LLM. Chuỗi actions này cho phép người
dùng thiết kế pipeline cho suy luận đa bước, như
[retrieve]-[generate], hoặc quy trình phức tạp hơn như
[rewrite query]-[retrieve]-[generate] được đề xuất
trong Ma et al. (2023). Tính linh hoạt của tính năng
này đặc biệt có lợi trong việc tạo các chuỗi phù hợp
với các trường hợp sử dụng cụ thể.

Một chuỗi single-action có thể hoạt động như một
retriever đơn giản trả về các tài liệu được truy xuất,
hoặc một QA sách đóng tận dụng kiến thức tham số
của LLM để cung cấp câu trả lời mà không cần truy
xuất. Ngược lại, một chuỗi với nhiều actions bao
gồm truy xuất cho phép sinh tăng cường truy xuất
hoặc QA sách mở, cho phép LLM truy cập các tài
liệu bên ngoài liên quan đến câu hỏi. Thiết lập mặc
định của chúng tôi cho R-LLMs bao gồm hai actions:
retrieve và generate.

2.3 Prompt Engineering

Framework RALLE cho phép các nhà phát triển tạo
ra các mẫu prompt tùy chỉnh cho LLMs và thậm chí
cho các truy vấn tìm kiếm trên cơ sở từng chuỗi một
cách tương tác. Mỗi action có thể được thực thi độc
lập, cho phép kiểm soát chính xác các phản hồi của
LLM, chẳng hạn như chỉ định định dạng đầu ra mong
muốn hoặc ngăn chặn các ảo giác không mong muốn.
Để tăng tính linh hoạt của việc phát triển prompt,
RALLE tích hợp hỗ trợ cho f-strings và hàm eval()
trong Python.

2.4 Theo dõi thí nghiệm

Chúng tôi sử dụng MLflow (LF Projects, 2023) để
theo dõi các thí nghiệm, cùng với các file cấu hình
và mẫu prompt liên quan. Điều này cho phép chúng
tôi so sánh hiệu suất của các lần chạy thí nghiệm
khác nhau một cách khách quan, giúp chúng tôi phát
triển R-LLMs tốt hơn.

2.5 Chat AI

RALLE cũng cung cấp hỗ trợ để xây dựng giao diện
chat đơn giản. Điều này cho phép người dùng kiểm
tra các thực hành tốt nhất từ giai đoạn phát triển và
đánh giá trong môi trường thực tế.

3 Thiết lập thí nghiệm

Trong phần này, chúng tôi đánh giá hiệu suất của
R-LLMs được xây dựng với một số kết hợp của
retrievers và LLMs mã nguồn mở trên các nhiệm vụ
chuyên sâu về kiến thức.

3.1 Nhiệm vụ và Tập dữ liệu

Chúng tôi sử dụng benchmark KILT (Knowledge
Intensive Language Tasks) (Petroni et al., 2021),
một benchmark mở rộng bao gồm 11 tập dữ liệu
trên 5 nhiệm vụ xử lý ngôn ngữ tự nhiên chuyên sâu
về kiến thức: fact checking, entity linking, slot
filling, open-domain question answering, và dialogue
(để biết thêm chi tiết về KILT, xem Petroni et al.
(2021)). Chúng tôi sử dụng tập huấn luyện để phát
triển prompts và tập phát triển để đánh giá.

Làm nguồn kiến thức, chúng tôi sử dụng các đoạn
văn Wikipedia đã được tiền xử lý do KILT cung cấp.
Các đoạn văn được tạo từ các bài báo Wikipedia
tiếng Anh dựa trên dữ liệu dump Wikipedia ngày
2019/08/01, bao gồm tổng cộng 5.9 triệu bài báo
và 22.2 triệu đoạn văn 100 từ. Đối với cả dense và
sparse retrievers, chúng tôi sử dụng tập hợp các đoạn
văn 100 từ sau khi tiền xử lý bổ sung thêm tiêu đề
của bài báo vào đầu mỗi đoạn văn.

Lưu ý rằng RALLE không phụ thuộc vào tập dữ
liệu, cho phép các nhà phát triển sử dụng tập dữ liệu
QA và kho văn bản riêng để phát triển và đánh giá.
Xem Phụ lục A.10 để biết thêm thông tin.

3.2 Mô hình

Phần phụ này chi tiết các retrievers và LLMs được
sử dụng để xây dựng R-LLMs trong thí nghiệm của
chúng tôi. RALLE cho phép các nhà thực hành và
nhà nghiên cứu dễ dàng thí nghiệm với các mô hình
mới nhất có sẵn trong các kho mã nguồn mở. Ngoại
trừ BM25, tất cả các mô hình đều có sẵn từ Hugging
Face (Wolf et al., 2020) (xem Phụ lục A.9 để biết
tóm tắt).

3.2.1 LLMs

LLM được sử dụng trong R-LLM phải hiểu các hướng
dẫn được cung cấp trong prompt và sinh ra phản hồi
phù hợp dựa trên thông tin đã cho. Để đạt được điều
này, chúng tôi sử dụng các LLMs được tinh chỉnh
hướng dẫn với tham số nhiệt độ được đặt ở mức 0
để có hiệu suất tối ưu và khả năng tái tạo.

Llama-2-chat được tinh chỉnh với supervised fine-
tuning và reinforcement learning with human
feedback (RLHF) (Christiano et al., 2017; Stiennon
et al., 2020) để phù hợp với sở thích con người về
tính hữu ích và an toàn (Touvron et al., 2023b).
Trong thí nghiệm của chúng tôi, chúng tôi sử dụng
cả mô hình 13 tỷ (Llama2-13B) và 70 tỷ (Llama2-
70B) tham số.

WizardVicunaLM-13B⁶(W-Vicuna-13B) (Lee,
2023) được hình thành bằng cách kết hợp các khái
niệm

⁶https://huggingface.co/junelee/
wizard-vicuna-13b

--- TRANG 4 ---
Model dim. max len. MTEB Retrieval
BM25 - - 42.3♠
m-e5 1,024 514 51.43
e5 1,024 512 50.56

Bảng 1: Tóm tắt các retrievers được sử dụng trong đánh giá của chúng tôi. Chiều của vector dense embedding được hiển thị trong dim., trong khi độ dài token tối đa của chuỗi đầu vào là max len. Chỉ số đánh giá cho MTEB Retrieval là nDCG@10.♠: Kết quả từ Ram et al. (2022). Kết quả trên MTEB Retrieval ngoại trừ BM25 được sao chép từ leaderboard MTEB⁷.

của WizardLM (Xu et al., 2023) (tinh chỉnh các hướng dẫn ban đầu với phương pháp Evol-Instruct (Xu et al., 2023)) và Vicuna (Chiang et al., 2023) (một mô hình LLaMA được tinh chỉnh (Touvron et al., 2023a) với dữ liệu hội thoại đa vòng từ chatbots).

3.2.2 Retrievers

Chúng tôi thí nghiệm với cả sparse và dense retrievers cho việc truy xuất tài liệu. Cụ thể, chúng tôi chọn các dense retrievers đã đạt độ chính xác cao trên nhiệm vụ retrieval của leaderboard Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2023)⁷ tính đến tháng 7 năm 2023. Danh sách các retrievers được sử dụng trong nghiên cứu của chúng tôi có thể được tìm thấy trong Bảng 1. Trong các thí nghiệm open-book, 5 tài liệu liên quan nhất được truy xuất.

Làm chỉ số của hiệu suất retrieval, chúng tôi tuân theo Petroni et al. (2021) và sử dụng page-level R-precision (Craswell, 2016) và recall@5. Page-level R-precision là tỷ lệ phần trăm của R_gold pages bên trong mỗi tập provenance trong số top-R pages được truy xuất. Thông thường, R-Precision tương đương với Precision@1 ngoại trừ FEVER và HotPotQA (tập dữ liệu multi-hop).

BM25 (Robertson và Zaragoza, 2009) là hàm truy xuất bag-of-words dựa trên term-matching. Chúng tôi sử dụng implementation Pyserini (Lin et al., 2021) của unigram BM25 với các tham số mặc định k1 = 0.9 (term frequency scaling) và b = 0.4 (document length normalization). Các tài liệu cho BM25 retrieval là cùng các đoạn văn 100 từ như các dense retrievers.

e5-large-v2⁸(e5) (Wang et al., 2022) là mô hình bi-encoder có giám sát với một query encoder và một

⁷https://huggingface.co/spaces/mteb/
leaderboard
⁸https://huggingface.co/intfloat/
e5-large-v2

document encoder. multilingual-e5-large⁹(m-e5) là mô hình e5 được tinh chỉnh đa ngôn ngữ.

3.3 Prompts

Chúng tôi sử dụng các mẫu prompt tùy chỉnh được thiết kế đặc biệt cho từng tập dữ liệu trong KILT. RALLE chấp nhận các mẫu với định dạng ngôn ngữ không tự nhiên, chẳng hạn như f-strings và hàm eval() trong Python. Điều này cho phép các nhà phát triển cẩn thận tạo ra các mẫu prompt của họ để có hiệu suất tối ưu. Các mẫu prompt được sử dụng trong thí nghiệm của chúng tôi được hiển thị trong Phụ lục A.11.

Đối với nhiệm vụ entity linking của KILT (AY2, WnWi, và WnCw), chúng tôi sử dụng mẫu REWRITE-EL theo mặc định cho các truy vấn tìm kiếm. Mẫu này trích xuất các mention thực thể cụ thể được hỏi làm truy vấn, vì việc sử dụng toàn bộ span của câu hỏi khó có thể tìm thấy tài liệu liên quan (chúng tôi sẽ thảo luận trong Phần 4.3). Sau khi truy xuất các tài liệu liên quan, tiêu đề Wikipedia top-1 được xuất ra làm câu trả lời. Kết quả là, độ chính xác downstream trong nhiệm vụ entity linking không bị ảnh hưởng bởi số lượng tài liệu được truy xuất (nếu có một hoặc nhiều hơn).

4 Kết quả Benchmark KILT

Phần này cung cấp hiệu suất downstream và retrieval của các R-LLMs được phát triển và đánh giá bằng RALLE.

4.1 Baseline

Chúng tôi so sánh kết quả của mình với mô hình BART-large (Lewis et al., 2020a) cho thiết lập closed-book và mô hình RAG (Lewis et al., 2020b) cho thiết lập open-book, được trình bày trong Petroni et al. (2021). Đáng chú ý, các mô hình baseline này được tinh chỉnh đặc biệt trên benchmark KILT, trong khi các LLMs được chọn và R-LLMs được xây dựng của chúng tôi thì không. Xem thêm Phụ lục A.5 để biết thông tin bổ sung về baselines.

4.2 Hiệu suất Downstream

Chúng tôi tóm tắt hiệu suất downstream¹⁰ trong Bảng 2. RALLE cũng bao gồm tỷ lệ phần trăm has_answer cho câu trả lời ngắn, một chỉ số proxy để đo tỷ lệ các câu hỏi chứa câu trả lời chuẩn trong đầu ra cuối cùng được sinh bởi R-LLM (xem Phụ lục A.3 để biết thêm chi tiết).

⁹https://huggingface.co/intfloat/
multilingual-e5-large
¹⁰Xem thêm Bảng 6 trong Phụ lục A.6 để biết kết quả bổ sung trong thiết lập closed-book.

--- TRANG 5 ---
Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Dataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW
Model / Metric Accuracy Exact Match RL F1
BART-large♢(closed-book ) 80.7 86.6 47.9 48.0 43.8 3.0 26.2 16.9 32.5 22.7 13.8
Llama2-70B ( closed-book ) 33.6 (74.9) 39.8 (54.5) 42.8 (53.8)39.2 (55.7)28.5 (40.5) 11.3 (13.6) 19.6 (37.4) 13.9 (25.1) 67.4 (80.8) 23.0 13.3
RAG♢87.7 77.4 49.0 46.7 61.5 47.4 48.8 27.7 61.7 16.1 13.3
e5 + W-Vicuna-13B 10.6 (42.4) 51.2 (57.9)48.6 (51.4) 45.6 (51.4) 31.6 (46.1) 23.0 (29.3) 18.7 (38.0) 19.7 (28.3) 43.1 (67.7) 21.4 12.3
e5 + Llama2-13B 66.3 (73.5) 51.2 (57.9) 48.6 (51.4) 45.6 (51.4) 17.2 (42.3) 31.7 (41.1) 36.1 (43.3) 14.3 (25.5) 56.3 (76.2) 20.9 12.3
BM25 + Llama2-70B 46.2 (86.3) 18.0 (35.9) 19.1 (32.2) 14.2 (30.9) 25.9 (43.0) 31.4 (37.8) 25.3 (34.3) 25.9 (33.4) 65.8 (80.0) 21.3 12.2
e5 + Llama2-70B 49.9 (88.6) 51.2 (57.9) 48.6 (51.4) 45.6 (51.4) 28.9 (49.2) 35.0 (43.2)36.4 (48.8)28.1 (35.8)71.1 (83.9) 21.5 13.2
e5 (DiskANN) 49.9 (87.9) 44.3 (50.5) 45.3 (48.1) 43.0 (48.8) 25.3 (43.9) 32.1 (37.9) 36.1 (48.4) 26.7 (34.3) 70.4 (83.2) 21.5 13.1
top-2 49.3 (88.1) 51.2 (57.9) 48.6 (51.4) 45.6 (51.4) 23.5 (44.9) 34.7 (43.0) 33.7 (46.2) 23.8 (34.2) 71.3 (82.9) 21.6 13.3
top-10 50.2 (88.0) 51.2 (57.9) 48.6 (51.4) 45.6 (51.4) 31.1 (49.3)35.4 (42.5) 35.2 (48.1) 24.9 (35.7) 59.3 (82.8) 21.5 13.2
Model / Metric KILT-Accuracy KILT-EM KILT-RL KILT-F1
RAG♢55.5 77.4 49.0 46.7 25.4 42.6 36.3 3.1 36.1 2.7 7.5
e5 + W-Vicuna-13B 8.4 (33.5) 51.2 (51.2)48.6 (48.6)45.5 (45.5)19.0 (28.0) 22.2 (28.1) 14.4 (27.8) 8.6 (11.9) 26.6 (40.3) 2.7 7.3
e5 + Llama2-13B 53.1 (58.7) 51.2 (51.2) 48.6 (48.6) 45.5 (45.5) 11.5 (25.7) 29.8 (38.5) 27.5 (32.5) 5.6 (10.6) 34.7 (46.1) 2.7 7.4
BM25 + Llama2-70B 21.9 (44.4) 17.6 (17.6) 18.9 (18.9) 13.9 (13.9) 14.5 (22.5) 24.9 (29.6) 9.3 (12.4) 4.5 (5.9) 23.6 (27.9) 1.5 4.0
e5 + Llama2-70B 40.2 (71.2) 51.2 (51.2) 48.6 (48.6) 45.5 (45.5) 19.2 (29.7) 32.8 (40.4) 27.7 (36.3)11.3 (14.5)42.8 (49.7) 2.7 8.1
e5 (DiskANN) 38.3 (68.5) 44.3 (44.3) 45.3 (45.3) 42.8 (42.8) 19.3 (24.2) 30.2 (35.5) 27.3 (35.9) 9.3 (12.1) 42.1 (49.0) 2.7 8.0
top-2 39.6 (70.7) 51.2 (51.2) 48.6 (48.6) 45.5 (45.5) 15.6 (28.0) 32.9 (40.6)25.7 (35.2) 7.6 (13.1) 43.1 (49.3) 2.7 8.3
top-10 40.4 (70.7) 51.2 (51.2) 48.6 (48.6) 45.5 (45.5) 20.5 (29.9)33.2 (39.8) 27.1 (36.1) 9.9 (14.3) 36.1 (48.9) 2.7 8.1

Bảng 2: Hiệu suất downstream trên tập dev KILT. Theo Petroni et al. (2021), chúng tôi báo cáo kết quả của các chỉ số điển hình cho mỗi tập dữ liệu, với in đậm chỉ kết quả tốt nhất và gạch chân chỉ kết quả thứ hai. Các chỉ số có tiền tố KILT- chỉ trao điểm hiệu suất đầu ra khi R-Prec = 1 (thành công retrieval). Các số trong ngoặc đại diện cho tỷ lệ phần trăm has_answer, tương ứng với tỷ lệ câu hỏi có câu trả lời chuẩn được bao gồm trong đầu ra cuối cùng. Các số hiển thị màu xám được sao chép từ cột ở trên vì chúng không thay đổi dựa trên thiết lập đã cho (chúng tôi sử dụng hàm Identity của RALLE cho các nhiệm vụ, thay vì LLM).♢: Kết quả từ Petroni et al. (2021).

R-LLM được xây dựng của chúng tôi (e5 + Llama2-70B) vượt trội hơn mô hình RAG trên cả HoPo và TQA, mặc dù không được tinh chỉnh với KILT như RAG. Hơn nữa, các R-LLMs được xây dựng của chúng tôi thể hiện mức độ chính xác chấp nhận được trên các tập dữ liệu khác, mà không có bất kỳ nhược điểm đáng kể nào. Kết quả cho thấy các LLMs được sử dụng trong nghiên cứu này thể hiện khả năng nhất định để hiểu các tài liệu được truy xuất.

Hơn nữa, phân tích của chúng tôi tiết lộ một số yếu tố có thể đóng góp vào việc cải thiện hiệu suất downstream, bao gồm tăng cường retrieval (ngoại trừ ELI5), tăng quy mô mô hình (ngoại trừ FEV và T-REx), và tham chiếu nhiều tài liệu hơn trong quá trình sinh (ngoại trừ NQ, HoPo, TQA và WoW). Tuy nhiên, một số tập dữ liệu thể hiện ngoại lệ đối với những xu hướng này hoặc có hiệu suất thấp hơn so với tỷ lệ phần trăm has_answer tương ứng (như FEV, T-REx, NQ, và TQA). Để giải quyết vấn đề này, các nhà phát triển có thể cải thiện R-LLM với RALLE bằng cách tinh chỉnh chuỗi suy luận và các mẫu prompt. Trong Phần A.4, chúng tôi cung cấp những nỗ lực ban đầu để phát triển chuỗi suy luận với ba actions trên một số tập dữ liệu.

Nhìn chung, kết quả đánh giá downstream cung cấp cái nhìn sâu sắc có giá trị về mức độ hoạt động tốt của các R-LLMs được xây dựng trên các nhiệm vụ chuyên sâu về kiến thức, cho phép các nhà phát triển xác định các lĩnh vực cần cải thiện.

4.3 Hiệu suất Retrieval

Bảng 3 hiển thị hiệu suất retrieval của các retrievers được chọn trên tập phát triển KILT (xem thêm Bảng 8 trong Phụ lục để biết kết quả của recall@5). Theo Bảng 3, e5 (với chỉ mục Faiss Flat) đạt hiệu suất retrieval cao nhất trung bình, mặc dù m-e5 tốt hơn trên nhiệm vụ MTEB Retrieval (Bảng 1). Mặc dù độ chính xác retrieval của e5 vượt trội so với RAG trên KILT, hiệu suất downstream của R-LLM sử dụng e5 vẫn kém hơn so với RAG (Bảng 2). Điều này cho thấy có chỗ cải thiện tiềm năng thông qua các prompts được tối ưu hóa thêm để nâng cao hiệu suất trên tập dữ liệu mục tiêu.

Như được mô tả trong Phần 3.3, REWRITE-EL đóng vai trò là mẫu mặc định cho các truy vấn tìm kiếm

--- TRANG 6 ---
Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Dataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW Avg.
Model R-Precision
RAG♢63.5 77.4 49.0 46.7 29.3 65.4 60.3 30.8 49.3 16.4 46.7 48.6
BM25 52.1 17.7 20.6 15.3 34.0 57.7 26.3 41.3 31.7 6.8 28.8 30.2
−REWRITE-EL 52.1 3.0 (−14.7) 0.1 (−20.5) 2.8* (−12.5) 34.0 57.7 26.3 41.3 31.7 6.8 28.8 25.9 (−4.3)
m-e5 (Flat) 81.7 41.8 45.8 41.6 47.1 81.4 63.0 54.0 56.1 11.9 57.9 52.9
−REWRITE-EL 81.7 3.2 (−38.6) 0.1 (−45.7) 3.1 (−38.5) 47.1 81.4 63.0 54.0 56.1 11.9 57.9 41.8 (−11.1)
e5 (Flat) 82.0 51.6 51.6 49.2 45.3 81.9 65.2 54.3 56.1 12.9 56.8 55.2
−REWRITE-EL 82.0 3.4 (−48.2) 0.0 (−51.6) 2.6 (−46.6) 45.3 81.9 65.2 54.3 56.1 12.9 56.8 41.9 (−13.3)
e5 (HNSW) 67.9 38.9 42.3 40.5 23.1 53.0 60.3 34.9 50.4 10.2 54.5 43.3
−REWRITE-EL 67.9 2.9 (−36.0) 0.0 (−42.3) 1.6 (−38.9) 23.1 53.0 60.3 34.9 50.4 10.2 54.5 32.6 (−10.7)
e5 (DiskANN) 78.8 44.7 47.8 46.0 37.1 74.5 64.9 49.1 55.4 12.9 56.6 51.6
−REWRITE-EL 78.8 3.2 (−41.5) 0.1 (−47.7) 1.8 (−44.2) 37.1 74.5 64.9 49.1 55.4 12.9 56.6 39.4 (−12.2)

Bảng 3: Hiệu suất retrieval trên tập dev KILT. Chúng tôi báo cáo page-level R-Precision trên tập phát triển KILT. Avg. đề cập đến macro-average của điểm retrieval trong mỗi tập dữ liệu. In đậm chỉ kết quả tốt nhất.♢: Kết quả từ Petroni et al. (2021). *: BM25 (không có REWRITE-EL) thất bại với các truy vấn dài (45 trong số 5,599 câu hỏi) trong WnCw.

Retrieval
Model Avg. R-Prec Memory sec/Q
BM25 30.2 - 0.121
e5 (Flat) 55.2 84.8 GB 0.169
e5 (HNSW) 43.3 90.4 GB 0.008
e5 (DiskANN) 51.6 10.9 GB 0.022
Completion in the Closed-Book Setting sec/Q
Llama-70B 6.727
Retrieval + Generation sec/Q
BM25 + Llama2-70B 3.637
e5 + Llama2-70B 3.793
e5 (DiskANN) + Llama2-70B 3.628

Bảng 4: Độ trễ thực thi tính bằng giây mỗi câu hỏi (sec/Q). Memory trong Retrieval chỉ ra dung lượng bộ nhớ (DRAM) tối đa.

liên quan đến nhiệm vụ entity linking (AY2, WnWi, và WnCw). Như được hiển thị trong Bảng 3, việc sử dụng mẫu REWRITE-EL dẫn đến độ chính xác retrieval cao hơn khi so sánh với việc sử dụng toàn bộ văn bản câu hỏi làm truy vấn tìm kiếm (thiết lập −REWRITE-EL). Điều này cho thấy việc bỏ qua thông tin không cần thiết từ các truy vấn tìm kiếm rất hữu ích đặc biệt cho nhiệm vụ entity linking.

4.4 Phân tích tốc độ

RALLE cho phép người dùng tối ưu hóa sự đánh đổi giữa độ trễ (tính bằng giây mỗi câu hỏi) và độ chính xác bằng cách so sánh các cấu hình khác nhau. Như được thể hiện trong Bảng 4, việc sử dụng các thuật toán tìm kiếm láng giềng gần nhất xấp xỉ (ANNS) như HNSW và DiskANN có thể giảm đáng kể độ trễ retrieval với chi phí là giảm độ chính xác. Lưu ý rằng, sự cân bằng tối ưu giữa tốc độ và độ chính xác phụ thuộc vào yêu cầu cụ thể của ứng dụng, và RALLE cho phép người dùng dễ dàng thí nghiệm với các thiết lập ANNS đa dạng để xác định tác động của chúng đối với cả hai yếu tố.

Đáng chú ý, DiskANN đạt được độ chính xác chỉ thấp hơn một chút so với chỉ mục flat Faiss trong khi cải thiện đáng kể tốc độ tìm kiếm, mặc dù yêu cầu ít dung lượng bộ nhớ hơn so với cả chỉ mục flat và HNSW. Mặc dù việc giảm thời gian thực thi R-LLM đạt được thông qua ANNS có thể xuất hiện tương đối nhỏ, yêu cầu DRAM thấp hơn đáng kể của DiskANN có thể làm cho nó trở thành giải pháp thực tế hơn cho các tình huống mà dung lượng DRAM bị hạn chế và chỉ mục flat vượt quá dung lượng DRAM có sẵn. Để biết thêm chi tiết về độ trễ, tham khảo Bảng 9 trong Phụ lục A.8.

5 Kết luận

Bài báo này giới thiệu RALLE, một framework dễ tiếp cận để phát triển và đánh giá R-LLMs. Chúng tôi cũng báo cáo kết quả đánh giá của một số R-LLMs được xây dựng bằng các retrievers và LLMs mã nguồn mở trên các nhiệm vụ chuyên sâu về kiến thức. Nhìn chung, RALLE mang lại một bước tiến đáng kể trong nghiên cứu sinh tăng cường truy xuất, cho phép phát triển, đánh giá và cải thiện R-LLMs một cách hiệu quả. Chúng tôi hy vọng rằng RALLE sẽ đóng góp vào việc phát triển các thực hành tốt nhất cho R-LLMs.

Hạn chế

Tất cả các đánh giá KILT được trình bày trong bài báo này đều được thực hiện bằng tập phát triển để duy trì tính công bằng

--- TRANG 7 ---
và nhất quán trong các đánh giá, vì các câu trả lời của tập kiểm tra vẫn được giữ bí mật¹¹.

Mặc dù R-LLMs thể hiện tính hợp lệ cao, nó vẫn thua kém so với mô hình nhỏ hơn nhưng chuyên biệt, RAG, trên nhiệm vụ downstream KILT (tham khảo Bảng 2). Sự khác biệt này có thể được quy cho nhiều yếu tố, bao gồm độ trưởng thành của prompt và khả năng sinh phản hồi của LLMs. Mặc dù các prompts được sử dụng đã được phát triển cẩn thận, có khả năng tồn tại các prompts tối ưu hơn (được thảo luận trong Phần 4.3). Hơn nữa, việc tinh chỉnh LLMs với các nhiệm vụ sinh tăng cường truy xuất có thể nâng cao hiệu suất của chúng trên các nhiệm vụ downstream. Do đó, độ chính xác đánh giá được báo cáo ở đây sẽ đại diện cho một ước tính bảo thủ.

Prompt engineering là một khía cạnh quan trọng của quy trình sinh tăng cường truy xuất, vì các đầu ra được sinh có thể khác nhau đáng kể giữa các mô hình, ngay cả khi được cung cấp cùng một prompt. RALLE mang lại lợi thế trong khía cạnh này, cho phép người dùng dễ dàng thí nghiệm với các prompts đa dạng cho các hành vi, tập dữ liệu và chuỗi actions phức tạp khác nhau.

Trong lĩnh vực phát triển prompt, các kỹ thuật như Automatic Prompt Engineer (APE) (Zhou et al., 2023) tự động hóa việc tạo prompts từ các cặp input-output và sampling để xác định các prompts hiệu quả nhất. Tuy nhiên, các cặp input-output trong sinh tăng cường truy xuất khác biệt rõ rệt so với các nhiệm vụ induction hướng dẫn đơn giản. Bởi vì văn bản đầu vào cho sinh tăng cường truy xuất thường có thể dài và phức tạp, việc tự động induce các prompts hiệu quả từ các cặp input-output là khó khăn.

Công cụ này cho phép các nhà phát triển xây dựng chuỗi suy luận với các actions được định nghĩa trước, trong khi những tiến bộ gần đây cũng đã giới thiệu các phương pháp cho phép LLMs xác định các actions (Yao et al., 2023). Một phương pháp bao gồm việc truy xuất tài liệu bằng truy vấn được viết lại bởi LLM và sau đó tóm tắt chúng cho đến khi thu được thông tin mong muốn. Tuy nhiên, trong các thí nghiệm ban đầu của chúng tôi (không được mô tả trong bài báo này), chúng tôi quan sát các trường hợp mà các LLMs tương đối nhỏ (thường ít hơn 100 tỷ tham số) bị mắc kẹt trong các chu kỳ truy xuất và tóm tắt lặp đi lặp lại, cản trở khả năng đạt đến việc sinh câu trả lời cuối cùng. Công cụ của chúng tôi giải quyết vấn đề này bằng cách cố ý xây dựng các chuỗi suy luận rõ ràng để tránh các hoạt động không mong muốn.

¹¹https://eval.ai/web/challenges/
challenge-page/689/overview

Tài liệu tham khảo

Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan,
Abdulrahman Alfozan, và James Zou. 2019. Gradio:
Chia sẻ và kiểm tra không rắc rối các mô hình ml trong
tự nhiên. arXiv preprint arXiv:1906.02569.

Akari Asai và Eunsol Choi. 2021. Thách thức trong
QA tìm kiếm thông tin: Câu hỏi không thể trả lời
và truy xuất đoạn văn. Trong Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:
Long Papers), trang 1492–1504, Online. Association
for Computational Linguistics.

Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, et al. 2023. Đánh giá
đa nhiệm vụ, đa ngôn ngữ, đa phương thức của chatgpt
về lý luận, ảo giác và tương tác. arXiv preprint
arXiv:2302.04023.

Ali Borji. 2023. Kho lưu trữ phân loại các lỗi ChatGPT.
arXiv preprint arXiv:2302.03494.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, và Dario Amodei. 2020.
Các mô hình ngôn ngữ là học viên few-shot. Trong Advances in Neural Information Processing Systems,
tập 33, trang 1877–1901. Curran Associates,
Inc.

Harrison Chase. 2023. LangChain. https://
langchain.com/.

Danqi Chen, Adam Fisch, Jason Weston, và Antoine
Bordes. 2017. Đọc Wikipedia để trả lời câu hỏi miền
mở. Trong Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics
(Volume 1: Long Papers), trang 1870–1879, Vancouver, Canada. Association for Computational Linguistics.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, và Eric P. Xing. 2023. Vicuna: Một chatbot
mã nguồn mở gây ấn tượng GPT-4 với chất lượng
90%* ChatGPT.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben

--- TRANG 8 ---
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
và Noah Fiedel. 2022. PaLM: Mở rộng mô hình hóa
ngôn ngữ với pathways. arxiv:2204.02311.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, và Dario Amodei. 2017. Học tăng
cường sâu từ sở thích con người. Trong Advances
in Neural Information Processing Systems, tập 30.
Curran Associates, Inc.

Nick Craswell. 2016. R-Precision, trang 1–1. Springer
New York, New York, NY.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
và Mingwei Chang. 2020. Tiền huấn luyện mô hình
ngôn ngữ tăng cường truy xuất. Trong Proceedings
of the 37th International Conference on Machine
Learning, tập 119 của Proceedings of Machine Learning Research, trang 3929–3938. PMLR.

Benjamin Heinzerling và Kentaro Inui. 2021. Các mô
hình ngôn ngữ như cơ sở kiến thức: Về biểu diễn thực
thể, dung lượng lưu trữ và truy vấn diễn giải. Trong
Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Linguistics: Main Volume, trang 1772–1791, Online.
Association for Computational Linguistics.

Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnawamy, và Rohan Kadekodi. 2019. Diskann: Tìm kiếm láng giềng
gần nhất tỷ điểm nhanh chính xác trên một node đơn.
Trong Advances in Neural Information Processing
Systems, tập 32. Curran Associates, Inc.

Jeff Johnson, Matthijs Douze, và Hervé Jégou. 2019.
Tìm kiếm tương tự quy mô tỷ với GPUs. IEEE
Transactions on Big Data, 7(3):535–547.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và
Wen-tau Yih. 2020. Truy xuất đoạn văn dày đặc cho
trả lời câu hỏi miền mở. Trong Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), trang 6769–6781,
Online. Association for Computational Linguistics.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, và Slav Petrov. 2019. Natural
questions: Một benchmark cho nghiên cứu trả lời
câu hỏi. Transactions of the Association for Computational Linguistics, 7:452–466.

June Lee. 2023. WizardVicunaLM. https://
github.com/melodysdreamj/
WizardVicunaLM.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, và Luke Zettlemoyer. 2020a.
BART: Tiền huấn luyện chuỗi-tới-chuỗi khử nhiễu
cho sinh ngôn ngữ tự nhiên, dịch thuật và hiểu.
Trong Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, trang
7871–7880, Online. Association for Computational
Linguistics.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, và Douwe Kiela. 2020b.
Sinh tăng cường truy xuất cho các nhiệm vụ nlp chuyên
sâu kiến thức. Trong Advances in Neural Information Processing Systems, tập 33, trang 9459–9474.
Curran Associates, Inc.

LF Projects. 2023. MLflow – một nền tảng cho vòng
đời học máy. https://mlflow.org/.

Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, và Rodrigo Nogueira.
2021. Pyserini: Một bộ công cụ Python cho nghiên
cứu truy xuất thông tin có thể tái tạo với biểu diễn
thưa và dày đặc. Trong Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR 2021), trang 2356–2362.

Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, Cyprien
De Masson D'Autume, Tim Scholtes, Manzil Zaheer,
Susannah Young, Ellen Gilsenan-Mcmahon, Sophia
Austin, Phil Blunsom, và Angeliki Lazaridou. 2022.
StreamingQA: Một benchmark cho thích ứng với
kiến thức mới theo thời gian trong các mô hình trả
lời câu hỏi. Trong Proceedings of the 39th International Conference on Machine Learning, tập 162
của Proceedings of Machine Learning Research,
trang 13604–13622. PMLR.

Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,
và Nan Duan. 2023. Viết lại truy vấn cho các mô
hình ngôn ngữ lớn tăng cường truy xuất. arXiv
preprint arXiv:2305.14283.

Yu A. Malkov và D. A. Yashunin. 2020. Tìm kiếm
láng giềng gần nhất xấp xỉ hiệu quả và mạnh mẽ
bằng đồ thị thế giới nhỏ có thể điều hướng phân cấp.
IEEE Trans. Pattern Anal. Mach. Intell., 42(4):824–
836.

Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,
Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,

--- TRANG 9 ---
Asli Celikyilmaz, Edouard Grave, Yann LeCun, và
Thomas Scialom. 2023. Các mô hình ngôn ngữ tăng
cường: một khảo sát. arXiv preprint arXiv:2302.07842.

Niklas Muennighoff, Nouamane Tazi, Loic Magne, và
Nils Reimers. 2023. MTEB: Benchmark embedding
văn bản khổng lồ. Trong Proceedings of the 17th
Conference of the European Chapter of the Association for Computational Linguistics, trang 2014–
2037, Dubrovnik, Croatia. Association for Computational Linguistics.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
et al. 2021. Webgpt: Trả lời câu hỏi có hỗ trợ trình
duyệt với phản hồi của con người. arXiv preprint
arXiv:2112.09332.

Youyang Ng, Daisuke Miyashita, Yasuto Hoshi, Yasuhiro Morioka, Osamu Torii, Tomoya Kodama, và
Jun Deguchi. 2023. SimplyRetrieve: Một công cụ
ai tạo sinh tập trung truy xuất riêng tư và nhẹ. arXiv
preprint arXiv:2308.03983.

OpenAI. 2023. Báo cáo kỹ thuật GPT-4. arXiv preprint
arXiv:2303.08774, abs/2303.08774.

Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Maillard,
Vassilis Plachouras, Tim Rocktäschel, và Sebastian
Riedel. 2021. KILT: một benchmark cho các nhiệm
vụ ngôn ngữ chuyên sâu kiến thức. Trong Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 2523–
2544, Online. Association for Computational Linguistics.

Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant, và Amir Globerson. 2022.
Bạn đang token về điều gì? truy xuất dày đặc như
phân phối trên từ vựng. arXiv preprint arXiv:2212.10380.

Stephen Robertson và Hugo Zaragoza. 2009. Framework liên quan xác suất: BM25 và hơn thế nữa.
Found. Trends Inf. Retr., 3(4):333–389.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, và Wen-tau Yih. 2023. RePlug: Các mô hình
ngôn ngữ hộp đen tăng cường truy xuất. arXiv
preprint arXiv:2301.12652.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, và Paul F Christiano. 2020. Học
tóm tắt với phản hồi của con người. Trong Advances
in Neural Information Processing Systems, tập 33,
trang 3008–3021. Curran Associates, Inc.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, và Guillaume Lample. 2023a. LLaMA:
Các mô hình ngôn ngữ nền tảng mở và hiệu quả.
arXiv preprint arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Các mô hình chat
nền tảng mở và tinh chỉnh. arXiv preprint arXiv:2307.09288.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
và Furu Wei. 2022. Text embeddings bằng tiền
huấn luyện contrastive giám sát yếu. arXiv preprint
arXiv:2212.03533.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, và Alexander Rush. 2020. Transformers: Xử lý ngôn ngữ tự nhiên hiện đại. Trong
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, trang 38–45, Online. Association
for Computational Linguistics.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin
Jiang. 2023. WizardLM: Trao quyền cho các mô
hình ngôn ngữ lớn để tuân theo hướng dẫn phức tạp.
arXiv preprint arXiv:2304.12244.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, và Yuan Cao. 2023.
ReAct: Đồng bộ hóa lý luận và hành động trong các
mô hình ngôn ngữ. Trong The Eleventh International
Conference on Learning Representations.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,
Keiran Paster, Silviu Pitis, Harris Chan, và Jimmy
Ba. 2023. Các mô hình ngôn ngữ lớn là kỹ sư prompt
ở mức con người. Trong The Eleventh International
Conference on Learning Representations.

A Phụ lục

A.1 Tài nguyên tính toán

Các thí nghiệm đánh giá được thực hiện trên máy
chủ Ubuntu 20.04.6 được trang bị Intel(R) Xeon(R)
Gold 6326 CPU ở 2.90 GHz CPU cores, và một
node với 4 × NVIDIA A100 Tensor Core GPU với
bộ nhớ 40 GB, và một mảng RAID-5 với bộ điều
khiển Dell(R) PERC H745 Front và KIOXIA(R)
PM6-R SAS SSDs để lưu trữ. Phiên bản CUDA là
12.2, phiên bản Python là 3.9.16, phiên bản PyTorch
là 2.0.1, và phiên bản Transformers là 4.29.2.

--- TRANG 10 ---
Hình 2: Ảnh chụp màn hình của tab Development chain của RALLE. Các nhà phát triển có thể tạo chuỗi action tùy chỉnh bao gồm nhiều actions suy luận. Đối với mỗi action, các nhà phát triển có thể chỉ định mẫu prompt, xác nhận kết quả áp dụng mẫu, và thực thi action bằng prompt mới được định nghĩa, riêng lẻ. Hơn nữa, RALLE có thể làm nổi bật các câu trả lời chuẩn trong các tài liệu được truy xuất hoặc đầu ra của LLM, cũng như làm nổi bật các Wikipedia IDs của provenance được truy xuất thành công.

--- TRANG 11 ---
A.2 Màn hình phát triển của RALLE

Hình 2 hiển thị màn hình phát triển chuỗi¹². Các nhà phát triển có thể tạo chuỗi suy luận cho R-LLM trên tab Develop chain này. Người ta có thể chọn tập dữ liệu và chỉ định độ dài chuỗi mong muốn, đại diện cho tổng số actions. Mặc định, có hai actions: truy xuất với retriever và sinh với LLM.

Các mẫu prompt cho mỗi action có thể được định nghĩa bằng f-strings hoặc hàm eval trong Python. Kết quả áp dụng mẫu có thể được xác nhận mà không cần thực thi retrieval và generation. Kết quả thực thi có thể được xem bằng cách nhấp vào nút Interpret prompt and execute this action.

Các toán tử action có sẵn là LLM, Retriever, và Identity. LLM sinh văn bản dựa trên prompt đã cho. Retriever truy xuất top k tài liệu liên quan nhất liên quan đến truy vấn đầu vào. Và Identity đơn giản xuất ra prompt gốc mà không sử dụng retriever hoặc LLM.

Để thực thi toàn bộ chuỗi, nhấp vào nút Execute entire chain. Ở cuối tab này, câu hỏi được chọn và câu trả lời tương ứng có thể được xem lại. Ngoài ra, RALLE cho phép làm nổi bật các câu trả lời chuẩn trong các tài liệu được truy xuất hoặc đầu ra của LLM, cũng như làm nổi bật Wikipedia ID của provenance được truy xuất thành công.

A.3 Chỉ số bổ sung: has_answer

RALLE cũng bao gồm tỷ lệ phần trăm has_answer (ví dụ, Karpukhin et al., 2020) cho câu trả lời ngắn, một chỉ số proxy để đo tỷ lệ câu hỏi chứa câu trả lời chuẩn trong đầu ra cuối cùng được sinh bởi R-LLM. Bằng cách theo dõi chỉ số này, các nhà phát triển có thể xác định các tình huống mà mô hình sinh phản hồi bao gồm câu trả lời chuẩn nhưng có thể bị bỏ qua do thiên lệch đánh giá như exact matching. Thông tin này có thể giúp tinh chỉnh prompts để cải thiện hiệu suất tổng thể.

A.4 Nỗ lực xây dựng chuỗi 3-action

Theo Phần 4.2, tăng cường retrieval có tác động đáng kể đến hiệu suất trong fact checking, open-domain QA cho câu trả lời ngắn, và slot filling tasks khi so sánh các thiết lập closed-book và open-book của Llama2-70B. Trong nhiệm vụ entity linking (AY2, WnWi, và WnCw), tuy nhiên, phương pháp của chúng tôi được mô tả trong Phần 3.3 (retrieve, sau đó xuất ra tiêu đề Wikipedia top-1 được truy xuất) có thể không hiệu quả.

¹²Vui lòng xem thêm video minh họa.

Để cải thiện hiệu suất, chúng tôi xây dựng chuỗi 3-action cho tập dữ liệu AY2: (1) truy xuất top-5 tài liệu liên quan, (2) giải thích mention thực thể được hỏi, và (3) dự đoán tiêu đề Wikipedia dựa trên giải thích và top-5 tiêu đề được truy xuất. Ngoài ra, chúng tôi khám phá việc phát triển chuỗi 3-action cho tập dữ liệu T-REx và NQ, bao gồm (1) retrieval, (2) viết lại câu hỏi, và (3) sinh câu trả lời. Bảng 12 hiển thị các prompts được sử dụng trong chuỗi 3-action.

Bảng 5 hiển thị hiệu suất downstream với chuỗi 3-action trên tập dữ liệu AY2, NQ, và T-REx. Trong khi chuỗi 3-action vượt trội hơn chuỗi 2-action (retrieve-then-generate) trên tập dữ liệu NQ, nó kém hơn độ chính xác 2-action trên tập dữ liệu AY2 và T-REx. Điều này cho thấy chuỗi 3-action được xây dựng đặc biệt cho hai tập dữ liệu này cần tối ưu hóa thêm. Tuy nhiên, giá trị has_answer cho AY2 (70.0%) cao hơn so với chuỗi 2-action (47.8%), cho thấy việc tích hợp các bước xử lý hậu kỳ vào chuỗi 3-action (do đó trở thành chuỗi 4-action) có thể tăng độ chính xác, đặc biệt cho AY2.

Một trong những lợi ích của công cụ chúng tôi là nó cho phép định nghĩa dễ dàng các actions suy luận bổ sung như vậy. Điều này có nghĩa là các nhà phát triển có thể tùy chỉnh chuỗi để thực hiện các nhiệm vụ cụ thể ngoài thiết lập mặc định, mang lại cho họ tính linh hoạt và kiểm soát lớn hơn trong quá trình phát triển.

A.5 Chi tiết về mô hình Baseline trong thiết lập Open-Book

Làm baseline trong thiết lập open-book, chúng tôi trình bày kết quả của mô hình Retrieval-Augmented Generation (RAG) (Lewis et al., 2020b) được hiển thị trong Petroni et al. (2021), đã đạt hiệu suất mạnh trong benchmark KILT. Mô hình RAG bao gồm bi-encoder retriever và sequence-to-sequence generator (mô hình BART (Lewis et al., 2020a)), cả hai đều được huấn luyện end-to-end. Tổng số tham số có thể huấn luyện trong mô hình RAG là khoảng 626 triệu. Điều quan trọng cần lưu ý là mô hình RAG được huấn luyện đặc biệt cho benchmark KILT, trong khi các LLMs được chọn và R-LLMs được xây dựng của chúng tôi thì không.

A.6 Hiệu suất Downstream KILT trong thiết lập Closed-Book

Bảng 6 tóm tắt kết quả downstream KILT trong thiết lập closed-book. Mô hình baseline (BART-large) đã được tinh chỉnh trên KILT

--- TRANG 12 ---
Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Dataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW
Model / Metric Accuracy Exact Match RL F1
Llama2-70B ( closed-book ) 33.6 (74.9) 39.8 (54.5) 42.8 (53.8)39.2 (55.7)28.5 (40.5) 11.3 (13.6) 19.6 (37.4) 13.9 (25.1) 67.4 (80.8) 23.0 13.3
RAG♢87.7 77.4 49.0 46.7 61.5 47.4 48.8 27.7 61.7 16.1 13.3
e5 + Llama2-70B 49.9 (88.6) 51.2 (57.9) 48.6 (51.4) 45.6 (51.4) 28.9 (49.2)35.0 (43.2)36.4 (48.8) 28.1 (35.8)71.1 (83.9) 21.5 13.2
3-action - 24.4 (70.0) - - 16.3 (46.8) - 36.9 (49.3) - - - -
Model / Metric KILT-Accuracy KILT-EM KILT-RL KILT-F1
RAG♢55.5 77.4 49.0 46.7 25.4 42.6 36.3 3.1 36.1 2.7 7.5
e5 + Llama2-70B 40.2 (71.2) 51.2 (51.2)48.6 (48.6)45.5 (45.5)19.2 (29.7)32.8 (40.4)27.7 (36.3) 11.3 (14.5)42.8 (49.7) 2.7 8.1
3-action - 9.5 (27.7) - - 10.4 (27.9) - 28.0 (36.6) - - - -

Bảng 5: Hiệu suất downstream của chuỗi 3-action trên tập dev KILT cùng với baselines. Các số trong ngoặc đại diện cho tỷ lệ phần trăm has_answer, tương ứng với tỷ lệ câu hỏi có câu trả lời chuẩn được bao gồm trong đầu ra cuối cùng của LLM.♢: Kết quả từ Petroni et al. (2021).

Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Dataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW
Model / Metric Accuracy Exact Match RL F1
BART-large♢80.7 86.6 47.9 48.0 43.8 3.0 26.2 16.9 32.5 22.7 13.8
W-Vicuna-13B 0.0 (58.4) 0.1 (52.2) 2.0 (44.9) 0.0 (48.1) 17.9 (33.0) 5.9 (8.5) 6.2 (27.4) 1.7 (17.1) 20.0 (64.5) 22.7 12.7
Llama2-13B 26.3 (50.7) 34.6 (47.5) 35.0 (42.8) 28.5 (41.3) 26.9 (36.7) 7.8 (9.9) 11.5 (29.1) 8.3 (20.3) 43.0 (70.2) 27.6 13.0
Llama2-70B 33.6 (74.9) 39.8 (54.5) 42.8 (53.8) 39.2 (55.7) 28.5 (40.5) 11.3 (13.6) 19.6 (37.4) 13.9 (25.1) 67.4 (80.8) 23.0 13.3

Bảng 6: Hiệu suất downstream trên tập phát triển KILT trong thiết lập closed-book (generation mà không có retrieval). Theo Petroni et al. (2021), chúng tôi báo cáo kết quả của các chỉ số điển hình cho mỗi tập dữ liệu, với in đậm chỉ kết quả tốt nhất. Các số trong ngoặc đại diện cho tỷ lệ phần trăm has_answer, tương ứng với tỷ lệ câu hỏi có câu trả lời chuẩn được bao gồm trong đầu ra cuối cùng của LLM.♢: Kết quả từ Petroni et al. (2021).

tập dữ liệu, trong khi các LLMs được chọn của chúng tôi thì không. Mặc dù vậy, các LLMs thể hiện hiệu suất vượt trội so với baseline trên một số tập dữ liệu. Cụ thể, mô hình Llama2-70B vượt trội hơn baseline BART trên tập dữ liệu zsRE và TQA, và mô hình Llama2-13B vượt trội hơn baseline trên tập dữ liệu ELI5. Điều này cho thấy kiến thức tham số được nhúng trong các LLMs và khả năng sinh văn bản của chúng có thể được tận dụng hiệu quả cho các nhiệm vụ chuyên sâu kiến thức, ngay cả trong thiết lập zero-shot. Tuy nhiên, như được mô tả trong Phần 4.2, tăng cường retrieval có thể nâng cao hiệu suất trên các nhiệm vụ downstream, ngoại trừ tập dữ liệu ELI5. Chúng tôi cũng trình bày hiệu suất closed-book của một số LLMs trên tập phát triển của tập dữ liệu NQ trong Bảng 7.

A.7 Kết quả bổ sung cho hiệu suất Retrieval

Bảng 8 trình bày recall@5 của các retrievers được sử dụng trong thí nghiệm của chúng tôi. Lưu ý rằng mặc dù m-e5 vượt trội hơn e5 trên nhiệm vụ MTEB Retrieval (được hiển thị trong Bảng 1), e5 vẫn thể hiện hiệu suất vượt trội so với m-e5 về cả R-precision (được hiển thị trong Bảng 3) và recall@5.

A.8 Chi tiết phân tích tốc độ

Bảng 9 trình bày chi tiết phân tích tốc độ trên tập phát triển KILT. Tốc độ tìm kiếm của BM25 (không có REWRITE-EL) giảm khi tổng số từ trong truy vấn tăng. Ngược lại, đối với tìm kiếm vector dày đặc, tốc độ tìm kiếm vẫn tương đối không đổi bất kể kích thước truy vấn do chiều cố định của các vector embedding.

Theo Bảng 9, thời gian thực thi cần thiết cho generation với LLM dài hơn thời gian cần thiết cho retrieval, đặc biệt khi sinh các phản hồi dài như ELI5 và WoW. Do đó, có vẻ phản trực giác rằng lợi thế của ANNS được sử dụng trong tìm kiếm vector không được thực hiện đầy đủ về mặt thời gian thực thi của R-LLMs. Tuy nhiên, như đã thảo luận trước đó trong Phần 4.4, DiskANN yêu cầu ít bộ nhớ hơn so với các thuật toán tìm kiếm vector khác, có nghĩa là việc sử dụng thuật toán như vậy thực sự có thể giúp tiết kiệm tài nguyên tính toán cho R-LLM.

Chúng tôi quan sát rằng Llama2-13B yêu cầu nhiều thời gian hơn

--- TRANG 13 ---
NQ
Model Name EM has_answer f1 sec/Q
Llama-2-70b-chat 19.6 37.4 36.8 2.254
Llama-2-13b-chat 11.5 29.1 28.1 1.179
StableBeluga2 16.2 40.9 35.5 2.858
gpt-3.5-turbo 25.4 38.9 41.1 -

Bảng 7: Độ chính xác trên tập dev NQ trong thiết lập closed-book. Đối với gpt-3.5-turbo (phiên bản 0613), độ chính xác được tính toán loại trừ năm câu hỏi trong số 2,837 câu hỏi trong tập phát triển NQ được coi là prompts không phù hợp bởi OpenAI và không được xử lý.

Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Dataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW Avg.
Model Recall@5
RAG♢76.1 77.5 49.0 46.7 33.7 73.1 65.5 12.3 56.9 27.3 66.6 53.1
BM25 74.2 28.8 34.7 30.6 42.7 74.7 42.5 22.8 48.7 12.3 45.1 41.6
−REWRITE-EL 74.2 7.6 (−21.2) 3.1 (−31.6) 5.9* (−24.7) 42.7 74.7 42.5 22.8 48.7 12.3 45.1 34.5 (−7.1)
m-e5 (Flat) 91.0 58.5 60.6 62.2 53.1 87.0 69.5 40.4 65.4 19.1 75.0 62.0
−REWRITE-EL 91.0 7.8 (−50.7) 3.8 (−56.8) 5.5 (−56.7) 53.1 87.0 69.5 40.4 65.4 19.1 75.0 47.1 (−14.9)
m-e5 (HNSW) −REWRITE-EL 63.2 4.9 3.5 2.6 26.0 48.2 55.6 14.1 48.7 14.6 66.8 31.7
e5 (Flat) 90.6 66.1 63.3 66.7 52.1 87.2 71.6 40.9 65.4 21.3 75.3 63.7
−REWRITE-EL 90.6 7.6 (−58.5) 3.4 (−59.9) 4.8 (−61.9) 52.1 87.2 71.6 40.9 65.4 21.3 75.3 47.3 (−16.4)
e5 (HNSW) 74.7 49.6 50.7 50.8 26.7 55.9 65.2 19.3 58.6 16.0 70.9 48.9
−REWRITE-EL 74.7 6.0 (−43.6) 3.3 (−47.4) 3.3 (−47.5) 26.7 55.9 65.2 19.3 58.6 16.0 70.9 36.4 (−12.5)
e5 (DiskANN) 86.6 57.1 58.3 60.9 42.1 78.7 70.7 34.7 64.6 20.8 75.0 59.0
−REWRITE-EL 86.6 7.4 (−49.7) 3.3 (−55.0) 3.6 (−57.3) 42.1 78.7 70.7 34.7 64.6 20.8 75.0 44.3 (−14.7)

Bảng 8: Hiệu suất retrieval (recall@5) trên tập dev KILT. Avg. đề cập đến macro-average của điểm trong mỗi tập dữ liệu. In đậm chỉ kết quả tốt nhất. Các số hiển thị màu xám được sao chép từ cột ở trên vì chúng không thay đổi dựa trên thiết lập đã cho.♢: Kết quả từ Petroni et al. (2021). *: BM25 (không có REWRITE-EL) thất bại với các truy vấn dài (45 trong số 5,599 câu hỏi) trong WnCw.

để xử lý mỗi câu hỏi so với Llama2-70B. Sau khi phân tích thêm, chúng tôi phát hiện rằng mô hình Llama2-13B thỉnh thoảng tạo ra các phản hồi vô nghĩa như nhiều ký tự xuống dòng ("\n"), một phần do hạn chế của prompts của chúng tôi.

A.9 Thông tin mô hình

Như được hiển thị trong Bảng 10, chúng tôi sử dụng một số mô hình mã nguồn mở từ Hugging Face, cụ thể là các phiên bản được phát hành chính thức. Chúng tôi tải các mô hình phân tán ở độ chính xác 8-bit theo mặc định ngoại trừ mô hình Llama2-70B (ở 4-bit) bằng thư viện Hugging Face Accelerate¹³.

A.10 Sử dụng tập dữ liệu tùy chỉnh

Ngoài việc sử dụng các tập dữ liệu KILT, RALLE cho phép các nhà phát triển phát triển và đánh giá R-LLMs trên các tập dữ liệu QA và kho văn bản riêng của họ. Để sử dụng

¹³https://huggingface.co/docs/
accelerate/index

các tập dữ liệu tùy chỉnh với RALLE, bạn sẽ cần thực hiện tiền xử lý sau:

• Chuẩn bị kho văn bản của bạn dưới dạng file TSV chứa các document IDs, texts, và titles.

• Tạo file JSONL cho tập dữ liệu QA của bạn. Định dạng sẽ trông như thế này: { "id": "", "input": "", "output": [{"answer": "", "provenance": [{"wikipedia_id": "", "title": ""}]}]}, trong đó "input" đại diện cho một câu hỏi.

Xem repo của chúng tôi để biết hướng dẫn chi tiết hơn:
https://github.com/yhoshi3/RaLLe.

A.11 Các Prompts được sử dụng trong đánh giá

Bảng 11 tóm tắt các prompts được sử dụng trong thí nghiệm của chúng tôi. Open-book chỉ thiết lập retrieve-then-generate. Các truy vấn được sử dụng cho retrieval là các câu hỏi thô mà không có bất kỳ viết lại nào, ngoại trừ các thiết lập REWRITE-EL của AY2, WnWi, và WnCw.

--- TRANG 14 ---
Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Tasks FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW Avg.
Models Completion in Closed-Book Setting (in seconds per question)
W-Vicuna-13B 1.565 13.040 10.870 9.793 0.983 1.142 2.165 1.969 1.414 22.820 7.122 6.626
Llama2-13B 0.625 1.077 1.036 1.201 0.940 0.913 1.270 1.185 1.014 40.100 9.522 5.353
Llama2-70B 1.765 2.936 2.745 2.618 1.953 2.031 2.285 2.188 1.877 42.500 11.100 6.727
Retrieval + Generation (in seconds per question)
e5 + W-Vicuna-13B 1.529 1.310 1.368 1.158 1.192 1.453 2.595 1.945 1.734 15.480 10.850 3.692
e5 + Llama2-13B 1.084 1.165 1.209 1.046 1.300 1.407 1.284 1.975 9.830 32.48 16.76 6.322
BM25 + Llama2-70B 1.841 0.008 0.009 0.008 2.015 2.296 2.206 2.344 2.249 15.020 12.010 3.637
e5 + Llama2-70B 1.926 0.133 0.131 0.135 2.135 2.424 2.419 2.346 2.238 16.030 11.810 3.793
e5 (top-2) + Llama2-70B 1.544 0.133 0.131 0.135 1.661 1.908 1.994 1.833 1.759 15.120 10.820 3.367
e5 (top-10) + Llama2-70B 2.811 0.133 0.131 0.135 2.951 3.276 - 13.900 14.400 35.070 24.100 -
e5 (DiskANN) + Llama2-70B 1.803 0.044 0.044 0.043 2.009 2.281 2.166 2.247 2.116 15.780 11.370 3.628
e5 + Llama2-70B ( 3-action ) - 25.41 - - 4.993 - 16.320 - - - -
Retrieval (in seconds per question)
BM25 0.038 0.008 0.009 0.008 0.018 0.013 0.052 0.105 0.086 0.136 0.857 0.121
BM25 (without REWRITE-EL ) 0.038 5.700 4.531 5.440 0.018 0.013 0.052 0.105 0.086 0.136 0.857 1.543
m-e5 (Flat) 0.174 0.164 0.166 0.176 0.187 0.165 0.194 0.156 0.176 0.177 0.165 0.173
m-e5 (HNSW) 0.008 0.013 0.013 0.015 0.008 0.009 0.009 0.009 0.009 0.011 0.010 0.010
e5 (Flat) 0.177 0.168 0.172 0.159 0.201 0.170 0.171 0.146 0.155 0.174 0.165 0.169
e5 (HNSW) 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.009 0.008
e5 (DiskANN) 0.018 0.020 0.038 0.020 0.020 0.030 0.020 0.021 0.019 0.019 0.021 0.022
Mean Query Length (tokens)
11.1±4.0 357.9 ±149.0 331.5 ±113.5 505.2 ±31.1 7.5±2.5 7.6±2.3 9.9±2.1 19.5±6.6 17.9±8.9 21.0±10.7 86.3±58.0

Bảng 9: Thời gian thực thi (tính bằng giây mỗi câu hỏi) trong RALLE. Avg. đề cập đến macro-average của thời gian trong mỗi nhiệm vụ. Độ dài truy vấn trung bình và độ lệch chuẩn của nó (được hiển thị dưới dạng ± sau giá trị) cũng được hiển thị, được tính toán bằng tokenizer e5.

Language Model
Model Name Size max len. emb dim URL
wizard-vicuna-13b (Lee, 2023) 13,015,864,320 2,048 - https://huggingface.co/junelee/wizard-vicuna-13b
Llama-2-13b-chat (Touvron et al., 2023b) 13,015,864,320 4,096 - https://huggingface.co/meta-llama/Llama-2-13b-chat
Llama-2-70b-chat (Touvron et al., 2023b) 68,976,653,312 4,096 - https://huggingface.co/meta-llama/Llama-2-70b-chat
StableBeluga2 70B 4,096 - https://huggingface.co/stabilityai/StableBeluga2
Retriever
multilingual-e5-large 559,890,946 514 1,024 https://huggingface.co/intfloat/multilingual-e5-large
e5-large-v2 (Wang et al., 2022) 335,142,400 512 1,024 https://huggingface.co/intfloat/e5-large-v2

Bảng 10: Liên kết Hugging Face của các mô hình được sử dụng trong đánh giá của chúng tôi. Size đề cập đến tổng số tham số hiệu quả của mỗi mô hình. max len. đề cập đến độ dài token tối đa của đầu vào mô hình.

Closed-book chỉ ra rằng LLM trả lời câu hỏi đã cho mà không có retrieval. Mặc dù những prompts này đã là thực hành tốt nhất đã được thiết lập của chúng tôi, chúng tôi nhận ra rằng có thể có cơ hội cải thiện (xem thêm Phần 5).

--- TRANG 15 ---
Open-book Closed-book
FEVER
Action 1: Retriever
{question}
Action 2: LLM
{response[0]} ←-
←-
Trả lời bằng MỘT TỪ nếu tài liệu HỖ TRỢ hoặc BÁC BỎ
"{question}". ←-
←-
Trả lời:Action 1: LLM
Trả lời bằng MỘT TỪ nếu kiến thức của bạn HỖ TRỢ hoặc BÁC BỎ
"{question}". ←-
←-
Trả lời:
AY2
Action 1: Retriever
'What is " '+'{}'.format(question).split(
'[START_ENT] ')[1].split( '[END_ENT] ')[0][1:-1] + '" ?'
Action 2: Identity
'{}'.format(wiki_id_title[0]).split( ';')[0].split( ',
')[0]Action 1: LLM
'Tiêu đề Wikipedia liên quan nhất đến thực thể " '+'{}'.format(question).split( '[START_ENT]
')[1].split( '[END_ENT] ')[0] + '" trong ngữ cảnh của
"'+'{}'.format(question).split( '[START_ENT] ')[0][-100:]
+'{}'.format(question).split( '[START_ENT] ')[1].split(
'[END_ENT] ')[0] + '{}'.format(question).split(
'[END_ENT] ')[1][:100] + '''..." là gì?\n\nVui lòng chỉ trả lời
tiêu đề Wikipedia.\n\nTrả lời: '''
WnWi
Action 1: Retriever
'What is " '+'{}'.format(question).split(
'[START_ENT] ')[1].split( '[END_ENT] ')[0][1:-1] + '" ?'
Action 2: Identity
'{}'.format(wiki_id_title[0]).split( ';')[0].split( ',
')[0]Action 1: LLM
'Tiêu đề Wikipedia liên quan nhất đến thực thể " '+'{}'.format(question).split( '[START_ENT]
')[1].split( '[END_ENT] ')[0] + '" trong ngữ cảnh của
"'+'{}'.format(question).split( '[START_ENT] ')[0][-100:]
+'{}'.format(question).split( '[START_ENT] ')[1].split(
'[END_ENT] ')[0] + '{}'.format(question).split(
'[END_ENT] ')[1][:100] + '''..." là gì?\n\nVui lòng chỉ trả lời
tiêu đề Wikipedia.\n\nTrả lời: '''
WnCw
Action 1: Retriever
'What is " '+'{}'.format(question).split(
'[START_ENT] ')[1].split( '[END_ENT] ')[0][1:-1] + '" ?'
Action 2: Identity
'{}'.format(wiki_id_title[0]).split( ';')[0].split( ',
')[0]Action 1: LLM
'Tiêu đề Wikipedia liên quan nhất đến thực thể " '+'{}'.format(question).split( '[START_ENT]
')[1].split( '[END_ENT] ')[0] + '" trong ngữ cảnh của
"'+'{}'.format(question).split( '[START_ENT] ')[0][-100:]
+'{}'.format(question).split( '[START_ENT] ')[1].split(
'[END_ENT] ')[0] + '{}'.format(question).split(
'[END_ENT] ')[1][:100] + '''..." là gì?\n\nVui lòng chỉ trả lời
tiêu đề Wikipedia.\n\nTrả lời: '''
Tiếp tục trang sau...
Bảng 11: Mẫu prompt được sử dụng trong thí nghiệm của chúng tôi. Mũi tên móc trái ←- đề cập đến dòng mới. Lưu ý rằng RALLE hỗ trợ f-strings và hàm eval() trong Python.

--- TRANG 16 ---
Bảng 11 – tiếp tục từ trang trước.
Open-book Closed-book
T-REx
Action 1: Retriever ( f-strings )
{question}
Action 2: LLM ( eval() )
'''Tham khảo tài liệu sau, trả lời "cái gì là '''+'{}'.format(question).split( '[SEP] ')[1] +
'của'+'{}'.format(question).split( '[SEP] ')[0] + '''?"
trong 5 từ hoặc ít hơn.\n\n '''+'{}'.format(response[0]) +
'''\n\n '''+'{}'.format(question).split( '[SEP] ')[1] + ':
'Action 1: LLM ( eval() )
'Cái gì là '+'"'+'{}'.for-
mat(question).split( '[SEP] ')[1] + '" của " '+'{}'.for-
mat(question).split( '[SEP] ')[0] + '"'+'''trong 5 từ hoặc
ít hơn?\n\n '''+'{}'.format(question).split( '[SEP] ')[1] +
':'
zsRE
Action 1: Retriever ( f-strings )
{question}
Action 2: LLM ( eval() )
Tham khảo tài liệu sau, trả lời "{ques-
tion}?" trong 5 từ hoặc ít hơn. ←-
←-
{response[0]} ←-
←-
Trả lời:Action 1: LLM ( eval() )
'Cho tôi biết '+'"'+'{}'.for-
mat(question).split( '[SEP] ')[1] + '" của " '+'{}'.for-
mat(question).split( '[SEP] ')[0] + '"'+'''trong 5 từ hoặc
ít hơn.\n\n '''+'{}'.format(question).split( '[SEP] ')[1] +
':'
NQ
Action 1: Retriever
{question}
Action 2: LLM
Tham khảo tài liệu sau, trả lời "{ques-
tion}?" trong 5 từ hoặc ít hơn. ←-
←-
{response[0]} ←-
←-
Trả lời:Action 1: LLM
Trả lời '{question}? 'trong 5 từ hoặc ít hơn. ←-
←-
Trả lời:
HoPo
Action 1: Retriever
{question}
Action 2: LLM
Tham khảo tài liệu sau, trả lời "{ques-
tion}?" trong 5 từ hoặc ít hơn. ←-
←-
{response[0]} ←-
←-
Trả lời:Action 1: LLM
Trả lời '{question}? 'trong 5 từ hoặc ít hơn. ←-
←-
Trả lời:
Tiếp tục trang sau...

--- TRANG 17 ---
Bảng 11 – tiếp tục từ trang trước.
Open-book Closed-book
TQA
Action 1: Retriever
{question}
Action 2: LLM
Tham khảo tài liệu sau, trả lời "{ques-
tion}?" trong 5 từ hoặc ít hơn. ←-
←-
{response[0]} ←-
←-
Trả lời:Action 1: LLM
Trả lời '{question} 'trong 5 từ hoặc ít hơn. ←-
←-
Trả lời:
ELI5
Action 1: Retriever
{question}
Action 2: LLM
Tham khảo tài liệu sau, trả lời "{question}".
←-
←-
{response[0]} ←-
←-
Giải thích những câu hỏi sau như thể tôi 5 tuổi. ←-
{question} ←-
←-
Trả lời:Action 1: LLM
Giải thích '{question} 'như thể tôi 5 tuổi. ←-
←-
Trả lời:
WoW
Action 1: Retriever
{question}
Action 2: LLM
Tham khảo tài liệu sau, xuất ra phản hồi ngắn gọn và có thông tin cho cuộc trò chuyện. ←-
←-
{response[0]} ←-
←-
Tham khảo tài liệu trên, xuất ra phản hồi ngắn gọn và có thông tin cho cuộc trò chuyện sau. ←-
←-
Cuộc trò chuyện này kết thúc ở lượt của bạn. ←-
←-
{question} ←-
←-
Câu trả lời có thông tin và ngắn gọn: ←-
←-Action 1: LLM
Xuất ra phản hồi ngắn gọn và có thông tin cho cuộc trò chuyện.
Cuộc trò chuyện này kết thúc ở lượt của bạn. ←-
←-
{question} ←-
←-
Câu trả lời có thông tin và ngắn gọn:

--- TRANG 18 ---
AY2
Action 1: Retriever
'What is " '+'{}'.format(question).split( '[START_ENT] ')[1].split( '[END_ENT] ')[0] + '" in the context of " '+
'{}'.format(question).split( '[START_ENT] ')[0][-100:] + '{}'.format(question).split( '[START_ENT] ')[1].split( '[END_ENT] ')[0]
+'{}'.format(question).split( '[END_ENT] ')[1][:100] + '..."? '
Action 2: LLM
'What is " '+'{}'.format(question).split( '[START_ENT] ')[1].split( '[END_ENT] ')[0] + '" in the context of " '+
'{}'.format(question).split( '[START_ENT] ')[0][-100:] + '{}'.format(question).split( '[START_ENT] ')[1].split( '[END_ENT] ')[0]
+'{}'.format(question).split( '[END_ENT] ')[1][:100] + '..."?\nTrả lời bằng một câu ngắn gọn và súc tích. '+'''\n\nTrả lời:\n '''
Action 3: LLM
'Vui lòng chọn tiêu đề phù hợp nhất cho từ " '+'{}'.format(question).split( '[START_ENT]
')[1].split( '[END_ENT] ')[0] + '" dựa trên Mô tả đã cho. '+'''\nNếu không có tiêu đề nào trong số này phù hợp với nhu cầu của bạn, vui lòng đề xuất một tiêu đề thay thế có thể. '''+'''\Tiêu đề: \n '''+'/'.join([titleid.split( ',')[0] for titleid in '{}'.for-
mat(wiki_id_title[0]).split( ';')]) + '''\n\nMô tả:\n '''+'{}'.format(response[1]) + '''\n\nTiêu đề Wikipedia:\n '''
T-REx
Action 1: Retriever
{question}
Action 2: LLM
Đặt câu hỏi yêu cầu [SEP] trong câu sau: ←-
'{question} '←-
←-
Câu hỏi được tạo:
Action 3: LLM
{response[0]} ←-
←-
Tham khảo tài liệu trên, trả lời "{response[1]}" trong 5 từ hoặc ít hơn. ←-
←-
Trả lời:
NQ
Action 1: Retriever
{question}
Action 2: LLM
Vui lòng viết lại câu hỏi sau một cách rõ ràng. ←-
←-
{question}? ←-
←-
Câu hỏi được viết lại:
Action 3: LLM
Tham khảo tài liệu sau, trả lời "{response[1]}" trong 5 từ hoặc ít hơn. ←-
←-
{response[0]} ←-
←-
Trả lời:
Bảng 12: Mẫu prompt được sử dụng trong chuỗi 3-action.
