# 2204.04581.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2204.04581.pdf
# File size: 2324706 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Augmenting Pre-trained Language Models with
QA-Memory for Open-Domain Question Answering
Wenhu Chen, Pat Verga, Michiel de Jongy, John Wieting, William W. Cohen
Google Research, University of Southern Californiay
{wenhuchen,patverga,jwieting,wcohen}@google.com, msdejong@usc.edu
Abstract
Existing state-of-the-art methods for open-
domain question-answering (ODQA) use an
open book approach in which information is
ﬁrst retrieved from a large text corpus or
knowledge base (KB) and then reasoned over
to produce an answer. A recent alternative
is to retrieve from a collection of previously-
generated question-answer pairs; this has sev-
eral practical advantages including being more
memory and compute-efﬁcient. Question-
answer pairs are also appealing in that they
can be viewed as an intermediate between text
and KB triples: like KB triples, they often con-
cisely express a single relationship, but like
text, have much higher coverage than tradi-
tional KBs. In this work, we describe a new
QA system that augments a text-to-text model
with a large memory of question-answer pairs,
and a new pre-training task for the latent step
of question retrieval. The pre-training task
substantially simpliﬁes training and greatly
improves performance on smaller QA bench-
marks. Unlike prior systems of this sort, our
QA system can also answer multi-hop ques-
tions that do not explicitly appear in the col-
lection of stored question-answer pairs.
1 Introduction
Open-domain question answering (ODQA) is a
well-studied knowledge-intensive task. State-of-
the-art methods require retrieving relevant knowl-
edge from a large corpus or datastore before rea-
soning over this retrieved evidence. Most existing
methods retrieve documents (Chen et al., 2017a;
Lee et al., 2019; Karpukhin et al., 2020) or struc-
tured KB triples (Verga et al., 2021). Recently, a
few works have proposed retrieving from a collec-
tion of question-answer (QA) pairs—an approach
made feasible by advances in scalable automatic
question generation. In this setting, a new ques-
tion is answered by retrieving paraphrases from a
question index, and returning the associated answer(Xiao et al., 2021; Lewis et al., 2021). Notably, the
RePAQ system from (Lewis et al., 2021) won the
2020 EfﬁcientQA competition (Min et al., 2021),
outperforming closed-book QA (CBQA) models by
a signiﬁcant margin and matching the prior SoTA
performance on NQ (Kwiatkowski et al., 2019).
A collection of QA pairs is appealing for several
reasons. As opposed to text passages and much like
a KB triple, QA pairs are often concise, tending
to express a single relationship. However, unlike
KB triples, QA collections have good coverage of
actually asked questions like those in standard open
QA datasets. RePAQ demonstrated several advan-
tageous properties such as memory and computa-
tional efﬁciency, strong selective QA performance
(i.e. selectively abstaining from answering), and ef-
fective ensembling with text-retrieval QA systems.
However, question-retrieval QA systems have
several limitations as well. First, there is no large-
scale supervised data for question-question re-
trieval . This contrasts with the step of retriev-
ing text given a question, where supervised data
is used to build retrievers like DPR (Karpukhin
et al., 2020). To address this, RePAQ uses a latent-
retrieval training process (similar to REALM (Lee
et al., 2019)), in which the retriever is trained us-
ing the downstream end loss from the QA task.
This requires asynchronously updating the index
as training proceeds, a process that is complex and
computationally expensive. This is also a problem
for domains with limited QA data: as we will show,
RePAQ’s performance is disappointing on smaller
datasets like WebQuestions (Berant et al., 2013),
containing only 3K training instances. To address
this problem, we introduce a novel pre-training
task for question retrieval , which can be applied to
any text-QA dataset, which great improves perfor-
mance on smaller datasets.
A second problem is that RePAQ is limited to
answering questions explicitly stored in the index,
or paraphrases of such questions. This contrastsarXiv:2204.04581v3  [cs.CL]  23 Jan 2023

--- PAGE 2 ---
Figure 1: During pre-training, the encoder ﬁrst encodes textual input and use special token representation to query
the QA-memory. The retrieved QA-pairs are integrated to the decoder to generate outputs.
with QA systems that retrieve from KBs, which can
typically generate complex queries that combine
the atomic triples in the KB.To address this, we
present an extended model that answers multi-hop
questions by iteratively retrieving from a question-
answer corpus, the ﬁrst question-retrieval-based
QA system that addresses this task.
In more detail, we propose a new QA-Memory-
Augmented Transformer (QAMAT) with better
compositionality paired with a lower complex-
ity training strategy. QAMAT is based on a
T5 encoder-decoder (Raffel et al., 2020) paired
with an integrated key-value memory (Khandelwal
et al., 2019; Borgeaud et al., 2021) populated with
question-answer pairs (See Figure 1). Given an
input, the encoder generates a query representation
scored against the QA memory and retrieves the
top-K relevant QA pairs. The encoder then repro-
cesses the input along with the retrievals forming a
QA-injected representation which is passed to the
decoder to attend to and generate.
To reduce the training (ﬁne-tuning) sample com-
plexity, we propose to ﬁrst pre-train QAMAT on a
large-scale corpus to teach the model to retrieve and
interpret QA pairs. We construct the pre-training
corpus by leveraging existing methods for ques-
tion generation, producing a very large set of po-
tentially interesting questions from text passages
(Zhou et al., 2017; Alberti et al., 2019; Lewis et al.,
2021). For each QA pair and the passage it was
generated from, we mask the answer and train the
model to ﬁll the mask by retrieving and using an
appropriate QA pair. We show that pre-training
greatly boosts the model’s performance and helps
the model generalize to different domains. For
example, the pre-trained model can achieve a zero-
shot performance of 40% EM on NQ and TriviaQA
without anyﬁne-tuning.
The effectiveness of this pre-training task meansthat we can avoid the expensive latent training pro-
cedure used by RePAQ, and instead use an efﬁcient
two-stage training pipeline. In the ﬁrst stage, we
use a small local in-batch memory of QA pairs
to optimize the QA pair encoder. We then freeze
the encoder and construct the index for the global
memory. In the second stage, we retrieve from this
ﬁxed global memory and continue to optimize the
remaining parameters—including the parameters
used to construct queries to the global memory—
for better performance.
Lastly, we extend QAMAT to build QAMAT+,
which iteratively retrieves from the memory to gen-
erate outputs. We demonstrate that QAMAT+ ef-
fectively chains multiple QA-pairs together to an-
swer multi-hop questions in HotpotQA (Yang et al.,
2018) and Musique (Trivedi et al., 2021). Such
compositional reasoning capability is nonexistent
in RePAQ (Lewis et al., 2021).
In summary, we develop a new QA augmented
architecture which extends the lines of research
considering QA pairs as a representation of knowl-
edge as well as those on memory-augmented lan-
guage models. When paired with our proposed pre-
training strategy (section 4), we address many of
the shortcomings of previous QA-indexing-based
approaches leading to lower sample complexity
training and the ability to perform compositional
reasoning (subsection 3.5).
2 Related Work
2.1 Retriever-Reader Models
Retrieve-and-read models have been widely studied
to address knowledge-intensive tasks and achieve
state-of-the-art performance on most QA tasks.
These methods use two models, one to retrieve
from a passage index based on BM25 (Robertson
and Zaragoza, 2009), and one to perform reading
comprehension on the returned passages (Chen

--- PAGE 3 ---
et al., 2017b). More recently, deep retrieval models
have gained more popularity to replace traditional
string-similarity retriever.
DPR (Karpukhin et al., 2020) is a widely
used supervised approach to achieve better results
than BM25 on a large collection of text retrieval
tasks (Thakur et al., 2021). Contrastive learning
is used to train the deep retriever model to distin-
guish between annotated positive and mined nega-
tive candidates. More recently, ColBERT (Khattab
and Zaharia, 2020) has been proposed to integrate
more ﬁne-grained late fusion between query and
context to improve DPR.
Retrieval Augmented Generation (RAG) (Lewis
et al., 2020), Fusion-in-Decoder (FiD) (Izacard and
Grave, 2021) and End-to-end training of Multi-
Document Reader and Retriever (EmDR) (Singh
et al., 2021) are proposed to read retrievals to ex-
tract or generate answers. These models require a
trained retriever/reranker to obtain top-K results,
which are fed to the reader to generate the answer.
As discussed in section 1, our model provides better
interpretability due to atomic knowledge represen-
tation. In subsection 5.4, we also demonstrate that
our model’s inference speed is 5x faster.
2.2 Question Generation
The problem of question generation (Zhou et al.,
2017) has attracted attention from the community
in recent years. It has been used for data augmen-
tation (Alberti et al., 2019) to improve current QA
systems or to improve retrieval systems (Nogueira
et al., 2019). Pan et al. (2021) also demonstrated
that by connecting generated single-hop questions,
we can train zero-shot multi-hop question answer-
ing systems. Besides QA, it has also been widely
used in other domains like evaluating factual con-
sistency of summarization (Eyal et al., 2019; Wang
et al., 2020) or enhancing contextualized represen-
tation (Jia et al., 2021). Most related to our work
is PAQ (Lewis et al., 2021), which aims to gener-
ate and use QA pairs as retrieval units for question
answering. The efﬁcacy of this data was further
veriﬁed when it was used to train DPR, yielding
better domain generalization (O ˘guz et al., 2021).
2.3 Memory-Augmented Language Models
End-to-end memory-augmented language models
aim to train a model to explicitly access external
memory. The current work is focused on storing
entities (Févry et al., 2020), entity mentions (Dhin-
gra et al., 2019; Sun et al., 2021; de Jong et al.,2022) or knowledge triples (Verga et al., 2021).
Memory attention layers are then used to inﬂuence
the computation of transformer layers. These enti-
ties and fact-centric memories are naturally atomic
and interpretable, and models employing them
have shown competitive performance on entity-
focused QA datasets like Web-Question-SP (Yih
et al., 2016) and ComplexWebQuestions (Talmor
and Berant, 2018). However, these models are lim-
ited to integrating entity-centric knowledge and
classifying the answer w.r.t a pre-deﬁned entity
list. For example, these models cannot handle
questions with non-entity answers, e.g. number,
date, noun phrases, etc, which are ubiquitous in
various QA datasets like NQ (Kwiatkowski et al.,
2019), SQuAD (Rajpurkar et al., 2016), or Hot-
potQA (Yang et al., 2018).
3 Our Model: QAMAT
3.1 Problem Deﬁnition
The input to our model is a piece of text X=
x1;;xn, whereXis either a question dur-
ing ﬁne-tuning or a paragraph in pre-training.
Pre-training is formulated as a span corruption
task (Raffel et al., 2019): given an example in the
pre-training corpus as (X;fQk;Akgm
k=1), where
A1;;Amcorrespond to spans in the input
X. We sample kspans from Xas a cloze an-
swer and replace all tokens within a span with
a [MASK] token, and the model needs to re-
cover all the answers. During ﬁne-tuning, we add
an artiﬁcial [MASK] in the question front, and
let the model recover this as the answer. The
pre-training/ﬁne-tuning objective function is to
maximize the masked language model objectives
p(YjX) =P
mi2Mp(YjX;mi)p(mijX), which
marginalizes over the entire memory M. However,
due to its intractability in a large-scale memory, we
adopt an approximation to only sum over the top-K
memory entries TopK(M).
We deﬁne the encoder function as f, which
takes an input sequence Xas input to generate a
sequence of vector F(X)2Rnd, wherenis the
input length and dis the hidden size. The desig-
nated position ofF(X)will be used as the query
and memory representation, which are denoted
asf(X;[MASK] )2Rd(at [MASK] position)
and memory key/value as f(mk
i;[CLS] )2Rd(at
[CLS] position). For brevity, we leave out [MASK]
and [CLS] and simply use f().
We also deﬁne a broadcast operator Bn
k(x)to

--- PAGE 4 ---
Figure 2: Architecture: upper ﬁgure shows the retrieval process with shared encoder, the lower ﬁgure shows the
decoder process to leverage neural and discrete representation of memory retrieval.
broadcast a vector into a matrix by assigning the
vectorxtok-th row while ﬁlling the rest with zero,
i.e.Bn
k(x) = [0;:::xT;:::;0].
3.2 Dense Retriever
The memory Mcontains separate key and value
components, where the key mk
icontains a ques-
tion, and the corresponding value mv
icontains the
question-answer concatenation. To retrieve the top-
k QA-pairs from the memory, we use our encoder
fto encodeXandmiseparately and select the
top-K entries TopK(M)based on their inner prod-
uct, i.e.TopKmi2Mf(X)f(mk
i).
3.3 Neural Memory Integration
After the model retrieves the Top-K candidates,
their corresponding memory values mv
ineeds be
leveraged into the encoder to inﬂuence the decoder
outputs in a differentiable fashion. We write our
objectivep(YjX)as:
X
mi2TopK(M)p(YjX;mi)p(mijX)
=X
mi2TopK(M)p(mijX)g(YjF(X) +Bn
k[f(mv
i)])
g(YjX
mi2TopK(M)p(mijX)(F(X) +Bn
k[f(mv
i)]))
=g(YjF(X) +Bn
k[X
mi2TopK(M)p(mijX)f(mv
i)])
p(mijX) =ef(X)f(mk
i)
P
mi2TopKMef(X)f(mk
i)
The probability p(YjX;mi)is parmeterized by a
decoder function g, which takes a memory-infused
encoder representation F(X) +Bn
k[f(mv
i)]as
input. We approximate this marginal probabil-
ity by pulling weighted summation inside the de-
coder function gto derive an aggregated memory-
infused encoder representations F(X) +Bn
k[].
The retrieval weight p(mjX)is calculated as the
softmax over the retrieval score over top-K items.
For simplicity, H(X;TopK(M);p(mjX))is used
to denote this encoder representation, thus the ob-jective can be written as follows:
p(YjX) =g(YjH(X;TopK(M);p(mjX))) (1)
As shown in the upper part of Figure 2, we ﬁrst
use weighted-sum over the neural representation of
retrieved memory entries f(mv
i)and then simply
add it to the encoder representation to infuse the re-
trieved QA-pair information. These two operations
are both differentiable, which makes the it possible
to train retriever latently. In essence, the retriever
will increasing weights p(mijX)on more relevant
memory items instead of irrelevant ones.
3.4 Neural + Discrete Memory Integration
A disadvantage of adopting weighted-sumP
ip(mijX)f(mv
i)2Rdis that all the infor-
mation from all of the top-K documents are
overly compressed into a d-dimension vector,
whereas the token retrieval representation contains
more information. Therefore, we propose to
add a ﬁne-grained token-wise representation
^H(X;TopK(M))to help the model access the
retrieved discrete values midirectly. The represen-
tation is obtained by encoding the concatenation
of the input Xand retrieved discrete tokens
^X=Concat [mk;;m1;X]2R(n+kjmj)d.
Such discrete memory integration greatly en-
riches the representation for miand enables cross-
attention between the query and retrieval, address-
ing the bottleneck problem. However, such dis-
crete representation cannot propagate gradients
back to the retriever. Finally, we propose to com-
bine the neural memory H(X;)and discrete mem-
ory^H(X;)integration to combine their merits.
p(YjX)
=g(YjH0(X;TopK(M);p(mjX)) +^H(X;TopK(M)))
(2)
where theis the balancing factor to weight the
two representations. We use H0(:::) = [0;H(:::)]
to represent the concatenation of zero-matrix 02
Rkjmjd, which has consistent dimension with ^H.

--- PAGE 5 ---
After leveraging ^H, our model demonstrates signif-
icant improvements on the downstream tasks with
14% on TriviaQA and 10% on HotpotQA.
H(X;TopK(M);p(mjX))is only used to la-
tently train the retriever, after training, we can drop
it and only use the concatenated representation
^H(X;TopK(M))as the encoder representation.
The decoder gwill attend to ^Hand perform a
greedy search over vocabulary to generate output.
3.5 Multi-hop Extension
To further extend QAMAT’s capability to per-
form compositional reasoning, we propose a cas-
caded architecture (depicted in Figure 3) known
as QAMAT+, where the model learns to perform
multiple rounds of retrieval before feeding the
augmented inputs to the decoder. Speciﬁcally
for two-hop reasoning, we use Xas the query
to retrieve a ﬁrst-round of top-K memory val-
uesTopK(M; 1)with our learned retriever fde-
scribed in subsection 3.2. Next, we augment the
query by concatenating the retrieved values as
X1= [TopK(M; 1);X]. This new query X1is
used to perform a second round of retrieval to ob-
tain additional top-K memory values, TopK(M; 2).
Based onTopK(M; 2), we compute the hybrid
encoder representation H(X1;TopK(M; 2)) and
^H(X1;TopK(M; 2)) to computep(YjX1;).
Figure 3: QAMAT+ architecture: Multi-Hop frame-
work for question-answer memory integration.
4 Training
4.1 Pre-training Corpus
Our QA-pairs are constructed by combining 30M
deduplicated QA-pairs from PAQ (Lewis et al.,
2021)(originally 65M, we delete paraphrases to
keep a subset) and 30M additional QA-pairs gen-
erated from our own pipeline. The additional QA-
pairs are populated from non-overlapping passage
blocks to increase the knowledge coverage over
Wikipedia. Our QA generation pipeline is simi-
lar to (Lewis et al., 2021) but trained solely on
SQuAD 2.0 (Rajpurkar et al., 2018) and ﬁltered
with a cheap reading comprehension model rather
than FiD (Izacard and Grave, 2021), the details are
Figure 4: Two stage training procedure: in-batch train-
ing with a batch-speciﬁc memory and end-to-end gradi-
ent updates, global training with a ﬁxed global memory
and partial gradient updates.
described in the Appendix. The ﬁnal statistics of
our QA-memory is described in Table 1, where the
total size is comparable to RePAQ.
Memory Size #Passages Training Data
Dedup-PAQ 30M 10M NaturalQuestions
Additional 30M 10M SQuAD 2.0
Combined 60M 20M -
Table 1: The breakdown statistics of our QA corpus.
We denote the entire memory as Mand formu-
late the pre-training corpus as fX;fQk;Akgm
k=1g,
whereXis the passage aligned with multiple QA-
pairsfQk;Akgm
k=1generated from it.
4.2 End-to-End Training
During training, the retrieval process is integrated
into the model’s training loop. The most widely
adopted approach to accomplish this is approxi-
mate nearest neighbor search (ANNS) efﬁciently
implemented by several libraries like ScaNN (Guo
et al., 2020), FAISS (Johnson et al., 2019), etc.
These libraries require a ﬁxed set of dense vec-
tors to construct the index and perform a Nearest-
Neighbor search using approximate algorithms.
However, our memory encoder fis continuously
updated, which poses great challenges for ANNS
index building. REALM (Guu et al., 2020) and
RePAQ (Lewis et al., 2021) use an asynchronous in-
dex building sub-process to refresh the index every
K steps, which is known to be extremely computa-
tionally expensive, especially with a large memory.
To avoid such expensive computation overhead, we
are inspired by TOME (de Jong et al., 2022) to
adopt a two-stage training as shown in Figure 4.
In-Batch Pre-training In the ﬁrst stage, in-
stead of using the whole memory, we propose
a batch-speciﬁc memory that concatenates the
positive, random negative, and hard negative en-

--- PAGE 6 ---
tries from each instance in the batch. Assuming
we have a batch size of Bcontaining examples
fXi;fQk
i;Ak
igK
k=1gB
i=1. For each example there
existKpositive QA-pairs generated from the given
contextXi. Additionally, we mine Khard nega-
tive QA-pairsfQk
i;Ak
igK
k=1for each input Xito
increase retrieval difﬁculty. This hard negative min-
ing is done with BM25 (Robertson and Zaragoza,
2009) similar to DPR (Karpukhin et al., 2020). We
construct the in-batch memory by aggregating the
KBpositive QA-pairs and KBhard negative
memory entries, so the in-batch memory ^Mcon-
tains a total of 2KBQA-pairs (roughly a few
thousand). Due to the small size of the memory,
we can construct the memory index very efﬁciently.
Thus, it enables us to continuously update the mem-
ory encoder parameters fto achieve strong QA-
pair retrieval performance.
Global Pre-training and Fine-Tuning In this
stage, we ﬁrst freeze the memory encoder fto
generate memory-key embedding for the entire
memory to build its index. We then incorporate the
on-device approximate search algorithm1to per-
form the nearest-neighbor search over the memory
index to retrieve the top-K QA-pairs. Formally, we
propose to maximize the same objective as Equa-
tion 2 but with stop-gradient applied to p(mjX)
term. In this step, the model will only update the
query model fand the decoder model g. During
ﬁne-tuning, we follow the same recipe as the global
pre-training. Instead of feeding masked passages
as inputs, we use questions with pseudo [MASK]
token in the front as the input.
4.3 Multihop Extension
For our extension model QAMAT+, since the re-
trieval augmentation process cannot be learned la-
tently, i.e. the gradient propagation is blocked in
the concatenation step, we add additional supervi-
sion to maximize the groundtruth retrieval proba-
bilityp(m1jX)for the ﬁrst-round retrieval m1. We
add such retrieval supervision objective to the orig-
inal objective p(YjX1), whereX1is the retrieval-
augmented inputs as described in subsection 3.5.
1https://github.com/google-research/language/
tree/master/language/mentionmemory5 QA Experiments
5.1 Implementation Details
Our model is based on the T5-base or large archi-
tecture implemented in JAX2and pre-trained on
32 TPUs on Google Cloud3. During in-batch train-
ing, our query and index encoder fare shared
and initialized from the T5 encoder (during global
training the index encoder is ﬁxed and the query
encoder continues to be updated). Our decoder g
is similarly initialized from the T5 decoder. In to-
tal, we construct60M question-answer pairs as
the global memory. The memory key is the ques-
tion tokenized by T5 sentencepiece model into 32
tokens, and the memory value is the answer con-
catenated with its question tokenized into 40 tokens.
The memory is indexed by a pre-computed matrix
Mk2RjMjdcomputed based on its keys (ques-
tions). The corresponding top-K memory values
(question+answer) will be fetched.
During in-batch pre-training, we use a large
batch size of 512 and a learning rate of 1e-3,
where each example contains a positive Q-A pair
and 7 hard negative QA-pairs mined through
BM25 (Robertson and Zaragoza, 2009). The in-
batch memory contains a total of 4096 entries, we
set Top-k of 4 and update over all the modules. Af-
ter 100K steps of in-batch pre-training, we switch
to global pre-training with global memory retrieval.
We decrease the batch size to 32 and enlarge Top-K
to 16 for larger memory. We update only the query
encoder and decoder for another 100K steps. Fi-
nally, we set K to 32 to ﬁne-tune on downstream
datasets with a decreased learning rate of 5e-4.
5.2 Datasets
We evaluate our framework on the three most
widely used single-hop open-domain question-
answering datasets and two multi-hop open-domain
question-answering datasets
NQ-Open The NaturalQuestions (Kwiatkowski
et al., 2019) dataset consists of naturally occurring
Google queries and their answers. We follow Lee
et al. (2019) to keep questions that have a "short
answer type". It consists of 79168 training exam-
ples, 8757 dev examples, and 3610 test examples.
TriviaQA The TriviaQA dataset is a collection of
trivia question-answer pairs that were scraped from
the web (Joshi et al., 2017). We use their unﬁltered
2https://github.com/google-research/t5x
3https://cloud.google.com/tpu/

--- PAGE 7 ---
version to evaluate our model consisting of 78785
training, 8837 dev, and 113313 test examples.
WebQuestions The WebQuestion dataset contains
questions that were sampled from Google Suggest
API (Berant et al., 2013). The answers are anno-
tated from FreeBase, the training set contains 3417
examples, the dev set contains 361 examples, and
the test set contains 2032 examples.
HotpotQA The HotpotQA dataset contains ques-
tions generated by human workers by reading two
passages (Yang et al., 2018). The questions are
designed to require multiple hops and include both
bridge questions and comparison questions. The
training set contains a total of 90564 examples, the
dev-set contains 7405 examples for evaluation.
Musique The Musique dataset contains questions
created by composing multiple questions from ex-
isting single-hop questions and was constructed to
contain less bias and artifacts (Trivedi et al., 2021).
In our experiments, we consider only the subset
of 2-hop questions, resulting in a training set of
14376 examples and a dev set of 1252 examples
for evaluation. While the dataset was originally
designed as a distractor setting (given a question
and a small number of passages, return the answer),
we instead consider an open-domain setting.
5.3 Baselines
We compare our model with baselines from the fol-
lowing categories. 1) CBQA large language models
(T5 XXL), which directly outputs an answer with-
out retrieval. 2) Entity/KG memory-augmented
models that use memory attention to incorporate
entity-level features into language models (Entities-
as-Experts (EaE) (Févry et al., 2020), Fact-Injected
Language Model (FilM) (Verga et al., 2021), Men-
tionMemory (TOME) (de Jong et al., 2022)). 3)
Retrieve-and-read model, which retrieves passages
to pass to a reader model which predicts the answer.
4) QA-retrieval models, which train a retriever to
collect QA-pairs from a large datastore, and then
rerank these QA-pairs (top 50-100) with original
query with cross-attention. The highest-ranked an-
swer is returned as the ﬁnal answer.
5.4 Single-Hop Results
Our results are summarized in Table 2 which re-
ports exact-match (EM) score.
Comparison with RePAQ Our main comparison
is with the previous best QA-retrieval-based ap-proach "RePAQ w/ rerank (XXL ALBERT)". This
model has a similar number of parameters to QA-
MAT (Large). Without using an explicit re-ranking
procedure, our model performs slightly worse on
NQ but obtains signiﬁcant gains on TriviaQA and
WebQuestion. Especially on WebQuestion, which
only contains 3K training examples, RePAQ per-
forms signiﬁcantly worse than the other datasets be-
cause it requires a high volume of examples to up-
date the retriever from scratch. With our proposed
pre-training strategy, QAMAT can initialize from a
much better checkpoint to decrease the sample com-
plexity, yielding an absolute 6% EM improvement.
Additionally, without any ﬁne-tuning, we demon-
strate that our model already achieves promising
results across these datasets, nearly matching the
performance of "RePAQ w/o rerank"4.
Comparison with retrieve-and-read models In
comparison to this class of model, QAMAT roughly
matches the performance of RAG, though it still
lags behind the SoTA model FiD. However, FiD
requires reading 100 passages, i.e. 20K tokens
while our best model works more efﬁciently by
only reading top-32 QA-pairs, i.e. 1.2K tokens.
To investigate the speed difference between these
approaches, we compared their inference speeds
using the same hardware (32 Google Cloud v3
TPUs). We found that QAMAT can answer 240
Qs/sec, while FiD only answers 50 Qs/sec, a 5x
inference time speedup over FiD.
5.5 Multi-hop Results
Since the document corpora source of HotpotQA
and Musique are different from single-hop QA
datasets, we adopt question generation model
trained on SQuAD 2.0 (Rajpurkar et al., 2018) to
generate questions for these two datasets. To create
the document corpora, we gather all of the pro-
vided positive and negative documents, obtaining
500K passages for HotpotQA and 85K passages
for Musique. We then use the trained generation
models to populate 3M QA pairs for HotpotQA
and 500K QA pairs for Musique. These QA pairs
are then used as the memory source for QAMAT+,
simulating a (slightly smaller) open-domain setup.
When training QAMAT+ on Musique, we initialize
from HotpotQA’s in-Batch pre-trained checkpoint,
which can bring 5-7% F1 improvement.
4It’s worth noting that the question generation models are
trained using some of these datasets’ training data so this is
not truly “zero-shot” performance.

--- PAGE 8 ---
Model (Test Set) NQ TQA WQ
T5-3B (Roberts et al., 2020) 30.4 35.1 33.6
T5-11B (Roberts et al., 2020) 32.6 42.3 37.2
EaE (Févry et al., 2020) - 43.2 -
FILM (Verga et al., 2021) - 29.1 -
TOME-2 (de Jong et al., 2022) - 53.4 -
DensePhrases (Lee et al., 2021) 40.9 50.7 -
REALM (Guu et al., 2020) 40.4 55.8 40.7
DPR (Karpukhin et al., 2020) 41.5 57.9 42.4
RAG-Seq (Lewis et al., 2020) 44.5 56.8 45.2
FiD (Izacard and Grave, 2021) 48.2 65.0 -
RePAQ (Lewis et al., 2021) 41.2 38.8 29.4 y
RePAQ+Rerank (Lewis et al., 2021) 47.6 50.7 37.6 y
QAMAT Zero-Shot (Base) 37.9 34.1 25.9
QAMAT Zero-Shot (Large) 39.8 40.0 25.1
QAMAT Fine-tuned (Base) 44.5 53.2 43.0
QAMAT Fine-tuned (Large) 45.5 54.8 43.6
Table 2: The main experimental results on single-hop
question answering datasets (NQ=NaturalQuestions,
TQA=TriviaQA, WQ=WebQuestions), ymeans Best-
effort replication using our own implementation.
Model (Dev Set F1 Score) HPQ MusQ
T5-3B (Roberts et al., 2020) 27.8 7.5
T5-11B (Roberts et al., 2020) 30.2 9.0
MDR+T5-Decoder (Xiong et al., 2020) 62.6 26.8
RePAQ (Lewis et al., 2021) y 47.8 18.6
QAMAT 42.0 16.7
QAMAT+ 57.6 29.8
Table 3: The main experimental results on MultiHop
QA datasets with QAMAT and QAMAT+, ymeans
Best-effort replication using our own implementation.
In Table 3, we show that QAMAT+ achieves
promising results on both multi-hop datasets, out-
performing T5-CBQA and RePAQ by a large mar-
gin. Additionally, QAMAT+ performs consider-
ably better than the single-hop QAMAT, demon-
strating the effectiveness of performing multi-
round retrieval. Though QAMAT+ still lags behind
the document-based model (MDR+T5 Decoder) on
HotpotQA, it surpasses it on the more challenging
Musique dataset. These encouraging results sug-
gest the potential for QAMAT+ to perform compo-
sitional reasoning over multiple QA-pairs, which
greatly increases the coverage of QA datastore to
cover more composite factual information.
5.6 Ablation Studies
Number of Retrievals To understand the proper-
ties of our model better, we ﬁrst investigate the im-
pact of the number of retrievals, K, on the model’s
performance. We gradually increase the Kto col-Top-K 1 10 20 30
NQ-Recall@K 0.41 0.58 0.62 0.64
TriviaQA-Recll@K 0.46 0.66 0.70 0.72
NQ-EM@K 0.39 0.42 0.44 0.44
TriviaQA-EM@K 0.45 0.51 0.53 0.53
Table 4: The retrieval recall and EM score of different
retrieval numbers on test sets.
Pre-training Stages NQ TQA WQ
Only In-Batch 42.1 48.2 39.7
Only Global 26.0 28.9 26.1
In-Batch!Global 44.5 53.2 43.0
Table 5: Downstream EM performance of models when
pre-trained using in-batch, global, or both stages.
lect the recall and ﬁnal QA performance. The re-
sults are shown in Table 4. We observe that even
though retrieval recall continues to increase beyond
K > 20, the EM score saturates much earlier. Fu-
ture research could improve performance further
by developing decoders to more accurately exploit
these larger retrievals sets.
Importance of Two-Stage Pre-training We next
analyze the importance of the two-stage pre-
training from section 4 by removing either the in-
batch or global stage. From our results shown in Ta-
ble 5, we can see that using in-batch pre-training
alone leads to a degradation in performance when
compared to the two-stage approach. This is likely
because the model is never exposed to the full set
of hard negatives which will be encountered when
performing retrieval over the global memory. On
the other hand, if we directly pre-train the global-
memory model without any in-batch initialization,
the retriever performance is nearly random and the
decoder consequently learns to ignore the retrieval
and simply memorize question-answer pairs.
6 Conclusion
In this paper, we propose a more accurate and efﬁ-
cient architecture to utilize QA-pairs as represen-
tation units of knowledge. Our proposed model
QAMAT outperforms RePAQ signiﬁcantly, while
leveraging our less expensive training procedure.
Furthermore, we show how a QA-backed model
can perform compositional reasoning and address
more complex queries. In the future, we hope to fur-
ther close the gap with state-of-the-art document-
based retrieve-and-read models and extend this ap-
proach to a broader set of tasks.

--- PAGE 9 ---
Limitations
Our approach has several limitations: 1) we use
generated question-answer pairs as a knowledge
base, which are extracted from web documents.
In order to maintain high quality and faithfulness,
the question generation pipeline needs to be well
trained with a sufﬁcient amount of clean data. Such
conditions might not hold for other domains outside
of Wikipedia like biomedical text, thus the general
QA-as-Knowledge-Base concept could require ad-
ditional innovations to extend to other areas. 2) Our
latent retrieval learning requires quasi paired data
to learn the alignment between the query and mem-
ory. This is hard to satisfy in some domains with
noisier data or only a very weak alignment between
a query and the memory. 3) Our model requires
mined intermediate retrieval signals to train QA-
MAT+, which currently relies on lexical-overlap-
based heuristics. In other cases, this may not be suf-
ﬁcient and instead might require a more principled
design to mine better intermediate supervision.
Ethical Statement
Our work encourages the model to ground on the
existing knowledge populated from large textual
collections. We believe it is a reasonable towards
building more trustworthy and more robust ma-
chine learning models. Having better attributions
to knowledge source could help humans better un-
derstand the model’s rationale for decision making.
However, we do admit that the question genera-
tion models used to populate the QA knowledge
base could potentially exacerbate the biases already
present in the original Wikipedia data. We will
keep working on this direction to minimize its po-
tential negative impacts.
References
Chris Alberti, Daniel Andor, Emily Pitler, Jacob De-
vlin, and Michael Collins. 2019. Synthetic qa cor-
pora generation with roundtrip consistency. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 6168–
6173.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
conference on empirical methods in natural lan-
guage processing , pages 1533–1544.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Millican,George van den Driessche, Jean-Baptiste Lespiau,
Bogdan Damoc, Aidan Clark, et al. 2021. Improv-
ing language models by retrieving from trillions of
tokens. arXiv preprint arXiv:2112.04426 .
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017a. Reading wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1870–
1879.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017b. Reading Wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1870–
1879, Vancouver, Canada. Association for Computa-
tional Linguistics.
Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-
ald, Fei Sha, and William Cohen. 2022. Mention
memory: incorporating textual knowledge into trans-
formers through entity mention attention. Interna-
tional Conference on Learning Representations .
Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachan-
dran, Graham Neubig, Ruslan Salakhutdinov, and
William W Cohen. 2019. Differentiable reasoning
over a virtual knowledge base. In International Con-
ference on Learning Representations .
Matan Eyal, Tal Baumel, and Michael Elhadad. 2019.
Question answering as an automatic evaluation met-
ric for news article summarization. In Proceedings
of NAACL-HLT , pages 3938–3948.
Thibault Févry, Livio Baldini Soares, Nicholas Fitzger-
ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-
tities as experts: Sparse memory access with entity
supervision. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 4937–4951.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,
David Simcha, Felix Chern, and Sanjiv Kumar. 2020.
Accelerating large-scale inference with anisotropic
vector quantization. In International Conference on
Machine Learning , pages 3887–3896. PMLR.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-
supat, and Mingwei Chang. 2020. Retrieval aug-
mented language model pre-training. In Proceed-
ings of the 37th International Conference on Ma-
chine Learning , volume 119 of Proceedings of Ma-
chine Learning Research , pages 3929–3938. PMLR.
Gautier Izacard and Édouard Grave. 2021. Leveraging
passage retrieval with generative models for open
domain question answering. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 874–880.

--- PAGE 10 ---
Robin Jia, Mike Lewis, and Luke Zettlemoyer. 2021.
Question answering infused pre-training of general-
purpose contextualized representations. arXiv
preprint arXiv:2106.08190 .
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.
Billion-scale similarity search with GPUs. IEEE
Transactions on Big Data , 7(3):535–547.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1601–1611.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 6769–
6781.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through memorization: Nearest neighbor language
models. In International Conference on Learning
Representations .
Omar Khattab and Matei Zaharia. 2020. Colbert: Ef-
ﬁcient and effective passage search via contextual-
ized late interaction over bert. In Proceedings of
the 43rd International ACM SIGIR conference on
research and development in Information Retrieval ,
pages 39–48.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, et al. 2019. Natural questions: a bench-
mark for question answering research. Transactions
of the Association for Computational Linguistics ,
7:453–466.
Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi
Chen. 2021. Learning dense representations of
phrases at scale. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 6634–6647.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics , pages 6086–6096.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale
Minervini, Heinrich Küttler, Aleksandra Piktus, Pon-
tus Stenetorp, and Sebastian Riedel. 2021. Paq: 65
million probably-asked questions and what you can
do with them. Transactions of the Association for
Computational Linguistics , 9:1098–1115.
Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi
Chen, Eunsol Choi, Michael Collins, Kelvin Guu,
Hannaneh Hajishirzi, Kenton Lee, Jennimaria Palo-
maki, et al. 2021. Neurips 2020 efﬁcientqa com-
petition: Systems, analyses and lessons learned.
InNeurIPS 2020 Competition and Demonstration
Track , pages 86–111. PMLR.
Rodrigo Nogueira, Wei Yang, Jimmy Lin, and
Kyunghyun Cho. 2019. Document expansion by
query prediction. arXiv preprint arXiv:1904.08375 .
Barlas O ˘guz, Kushal Lakhotia, Anchit Gupta, Patrick
Lewis, Vladimir Karpukhin, Aleksandra Piktus,
Xilun Chen, Sebastian Riedel, Wen-tau Yih, Sonal
Gupta, et al. 2021. Domain-matched pre-
training tasks for dense retrieval. arXiv preprint
arXiv:2107.13602 .
Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-
Yen Kan, and William Yang Wang. 2021. Unsuper-
vised multi-hop question answering by question gen-
eration. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 5866–5880.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. arXiv preprint arXiv:1910.10683 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a uniﬁed text-to-text
transformer. Journal of Machine Learning Research ,
21:1–67.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 784–789.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 5418–5426.

--- PAGE 11 ---
Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond . Now Publishers Inc.
Devendra Singh, Siva Reddy, Will Hamilton, Chris
Dyer, and Dani Yogatama. 2021. End-to-end train-
ing of multi-document reader and retriever for open-
domain question answering. Advances in Neural In-
formation Processing Systems , 34.
Haitian Sun, Pat Verga, Bhuwan Dhingra, Ruslan
Salakhutdinov, and William W Cohen. 2021. Rea-
soning over virtual knowledge bases with open pred-
icate relations. In International Conference on Ma-
chine Learning , pages 9966–9977. PMLR.
Alon Talmor and Jonathan Berant. 2018. The web as
a knowledge-base for answering complex questions.
InProceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers) , pages 641–651.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. Beir:
A heterogeneous benchmark for zero-shot evalua-
tion of information retrieval models. In Thirty-ﬁfth
Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 2) .
Harsh Trivedi, Niranjan Balasubramanian, Tushar
Khot, and Ashish Sabharwal. 2021. Musique: Multi-
hop questions via single-hop question composition.
arXiv preprint arXiv:2108.00573 .
Pat Verga, Haitian Sun, Livio Baldini Soares, and
William Weston Cohen. 2021. Adaptable and inter-
pretable neural memory over symbolic knowledge.
InProceedings of NAACL-HLT , pages 3678–3691.
Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.
Asking and answering questions to evaluate the fac-
tual consistency of summaries. In Proceedings of
the 58th Annual Meeting of the Association for Com-
putational Linguistics , pages 5008–5020.
Jinfeng Xiao, Lidan Wang, Franck Dernoncourt, Trung
Bui, Tong Sun, and Jiawei Han. 2021. Open-domain
question answering with pre-constructed question
spaces. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Student Research Work-
shop , pages 61–67.
Wenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick
Lewis, William Yang Wang, Yashar Mehdad, Scott
Yih, Sebastian Riedel, Douwe Kiela, et al. 2020. An-
swering complex open-domain questions with multi-
hop dense retrieval. In International Conference on
Learning Representations .
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D Manning. 2018. Hotpotqa: A dataset for
diverse, explainable multi-hop question answering.InProceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing , pages
2369–2380.
Wen-tau Yih, Matthew Richardson, Christopher Meek,
Ming-Wei Chang, and Jina Suh. 2016. The value of
semantic parse labeling for knowledge base question
answering. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 201–206.
Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,
Hangbo Bao, and Ming Zhou. 2017. Neural ques-
tion generation from text: A preliminary study. In
National CCF Conference on Natural Language
Processing and Chinese Computing , pages 662–671.
Springer.

--- PAGE 12 ---
A Question Answer Pairs as Knowledge
Base
We can see QA-pairs as a virtual knowledge graph,
where the question template deﬁnes the relation,
the topic entity in the question deﬁnes the head
entity node, and the answer denotes the tail entity.
A typical example is given in Figure 5, such com-
positionality makes the QA-pair more controllable
and easy to reason over than documents.
Figure 5: QA pairs can be seen as virtual knowledge
base, where the question can represent complex rela-
tions connecting subject and answer.
B Question Generation
Here, we use existing SQuAD datasets’ <Q, A,
Document> triples (Rajpurkar et al., 2016) to train
answer extraction, question generation model.
Answer Extraction Speciﬁcally, our answer ex-
traction model takes a document as the input and
trains an encoder-decoder model to generate a po-
tential answer. We use beam search over the trained
model to ﬁnd the highest-likely answers in the
given document. In our experiment, the answer ex-
traction model is trained with the SQuAD dataset,
where the document is given as the input, and the
answer spans are the prediction targets.
Question Generation For the question genera-
tion model, we take the SQuAD dataset and use
document + extracted answer as the input to gen-
erate questions as the outputs. This step is also ac-
complished by an encoder-decoder model. which is
mainly purposed for reading comprehension prob-
lems, where the annotated questions are highly cor-
related with the document containing very few hal-
lucinations. However, the questions in SQuAD (Ra-jpurkar et al., 2016) could be contextualized or am-
biguous, which could lead to ambiguity problems
to hurt the retrieval performance. Therefore, we
add question ﬁltering to select the most accurate
QA pairs.
Question Filtering For the question ﬁltering
model, we take the document + generated question
to generate an answer. We compare the predicted
answer vs. the original answer to see if they match
each other. If not, the QA-pair will be ﬁltered based
on such inconsistency. We use a reading compre-
hension model trained with SQuAD to predict the
answer. The predicted answer based on the docu-
ment will match with the original QA-pair to decide
its consistency. Such an option runs much faster,
providing much higher recall but lower precision
compared to the open-domain FiD ﬁltering used
in (Lewis et al., 2021).
We visualize our question generation pipeline
mentioned above in Figure 6.
Figure 6: Question generation pipeline: Answers are
extracted from passages and then questions are gener-
ated conditioned on that contextualized answer. This
procedure is used to generate both our model’s QA
memory and our pre-training data.
C Ablation Study
We experiment with two variants of memory to see
their performance difference.
C.1 PAQ memory
The ﬁrst version is the standard PAQ corpus (Lewis
et al., 2021) containing 65M QA pairs, where
these QA-pairs are generated by models trained on
NQ (Kwiatkowski et al., 2019) and ﬁltered through
FiD model (Izacard and Grave, 2021) also trained
on NQ (Kwiatkowski et al., 2019). This memory
is highly precise due to ODQA-ﬁltering process,
however, it only covers information from 9M out of
the 20M passage blocks used in DPR (Karpukhin
et al., 2020).

--- PAGE 13 ---
Our memory contains 30M PAQ corpus being
de-duplicated, i.e. only one question corresponds
to an answer span. We generate 30M additional
QA-pairs based on the left-out 10M documents
from PAQ (Lewis et al., 2021) and add these com-
plementary QA-pairs to form our 60M memory to
increase the coverage. However, since our ﬁltering
procedure is based on reading comprehension, the
precision of QA-pairs is lower than the original
PAQ memory.
Memory NQ TriviaQA WebQuestions
PAQ 65M 44.7 48.0 39.4
Ours 60M 44.5 53.2 43.0
Table 6: Impact of different memory over the down-
stream QA dataset performance.
As can be seen, from Table 6, using the
most precise but low-coverage PAQ memory from
PAQ (Lewis et al., 2021) yields the worse results
on TriviaQA and WebQuestions. After adding an
additional 30M PAQs to the memory generated by
our pipeline, we are able to achieve 4-5% improve-
ments on these two datasets while still maintaining
NQ’s performance.
C.2 Size of Pre-training Corpus
Next, we investigate the impact of the size of the
pre-training corpus. As a baseline, we repurpose
the aligned query-passage corpus used to train
DPR (Karpukhin et al., 2020) which we adapt to
our setting by simply reversing the pairs (120K pas-
sage -> question retrieval). Additionally, we vary
the size of generated pre-training corpus (from 1M
to 20M instances) to see its impact on the model’s
ﬁnal downstream performance. From Table 7, we
can see that the smaller-sized pre-training corpus
can drastically reduce the model’s performance,
with up to a 5% drop seen on TriviaQA.
Pre-train Examples NQ TQA WQ
120K 42.5 48.2 39.7
1M 42.8 48.8 40.2
5M 43.8 51.5 41.7
10M 44.3 52.1 42.5
20M 44.5 53.2 43.0
Table 7: Impact of pre-training corpus size on ﬁnal
downstream EM performance. The upper portion is
pre-trained using the DPR-reverse corpus described in
subsection 5.6 and the lower portion uses subsets of our
generated pre-training corpus (subsection 4.1)
Figure 7: The impact of memory size on downstream
QA EM performance.
Size of Memory Finally, we look at how big of
memory we need to reach optimal downstream ac-
curacy and how the model behaves with a smaller
memory. As is shown in Figure 7, having a small
memory of less than 5M entries does not improve
over a model with no memory at all. Due to the
lack of coverage, the model does not receive a use-
ful signal from the retrieval and is subsequently
not incentivized to utilize those retrievals when
making a prediction. However, once the size of
the memory increases beyond 15M we observe a
steep increase in the ﬁnal performance, indicating
that the model is gradually learning to incorporate
retrieved information retrievals to assist prediction.
D MultiHop QA Training
In order to train the multi-hop QA model, we need
to have intermediate supervision for the query aug-
mentation process. Here we use a string-based
match to derive what are the most possible interme-
diate questions from a collection of pre-generated
QA pairs. We depict the mining process as Fig-
ure 8.

--- PAGE 14 ---
Figure 8: We ﬁrst ﬁnd the ﬁnal question based on answer string matching with the pre-generated question, and
then base on that to trace back the intermediate question.
