# 2304.06762.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2304.06762.pdf
# Kích thước tệp: 1096196 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Liệu Chúng Ta Có Nên Tiền Huấn Luyện Các Mô Hình Ngôn Ngữ Tự Hồi Quy với Truy Xuất Không?
Một Nghiên Cứu Toàn Diện
Boxin Wang∗ ‡1Wei Ping∗†2Peng Xu∗2Lawrence McAfee2
Zihan Liu2Mohammad Shoeybi2Yi Dong2Oleksii Kuchaiev2
Bo Li1Chaowei Xiao2,3Anima Anandkumar2Bryan Catanzaro2
Tóm tắt
Các mô hình ngôn ngữ (LMs) chỉ có bộ giải mã lớn có thể được cải thiện đáng kể về mặt độ phức tạp nhờ truy xuất (ví dụ, RETRO), nhưng tác động của nó đến chất lượng tạo văn bản và độ chính xác tác vụ hạ lưu vẫn chưa rõ ràng. Do đó, vẫn còn là một câu hỏi mở: liệu chúng ta có nên tiền huấn luyện các LMs tự hồi quy lớn với truy xuất không? Để trả lời câu hỏi này, chúng tôi thực hiện một nghiên cứu toàn diện về một LM tăng cường truy xuất có thể mở rộng được tiền huấn luyện (tức là RETRO) so với GPT tiêu chuẩn và GPT tăng cường truy xuất được tích hợp ở giai đoạn tinh chỉnh hoặc suy luận. Đầu tiên, chúng tôi cung cấp công thức để tái tạo RETRO lên đến 9.5B tham số trong khi truy xuất một tập văn bản với 330B token. Dựa trên đó, chúng tôi có những phát hiện mới sau: i) RETRO vượt trội GPT về tạo văn bản với ít thoái hóa hơn nhiều (tức là lặp lại), độ chính xác thực tế cao hơn một cách vừa phải, và độc tính thấp hơn một chút với cơ sở dữ liệu truy xuất không độc hại. ii) Trên benchmark LM Evaluation Harness, RETRO vượt trội GPT rõ rệt trong các tác vụ tri thức chuyên sâu, nhưng ngang bằng với GPT trong các tác vụ khác. Hơn nữa, chúng tôi giới thiệu một biến thể đơn giản của mô hình, RETRO++, cải thiện đáng kể kết quả QA miền mở của RETRO gốc (ví dụ, điểm EM +8.6 trên Natural Question) và vượt trội đáng kể GPT tăng cường truy xuất trong cả thiết lập đánh giá tinh chỉnh và zero-shot. Những phát hiện của chúng tôi làm nổi bật hướng đầy hứa hẹn của việc tiền huấn luyện các LMs tự hồi quy với truy xuất như các mô hình nền tảng tương lai. Chúng tôi phát hành mã và mô hình tại: https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LMs), bao gồm các LMs có mặt nạ (ví dụ, BERT (Devlin et al., 2018)), LMs tự hồi quy (ví dụ, GPT (Brown et al., 2020)), và LMs mã hóa-giải mã (ví dụ, T5 (Raffel et al., 2020), BART (Lewis et al., 2020a)), đã đạt được kết quả tốt nhất cho nhiều tác vụ NLP khác nhau. Trong số đó, các LMs tự hồi quy như GPT-3 (Brown et al., 2020) và GPT-4 (OpenAI, 2023) thể hiện khả năng học trong ngữ cảnh đáng chú ý và kết quả tạo văn bản dài xuất sắc. Do tầm quan trọng của nó, cộng đồng đã dành nhiều nỗ lực đáng kể để mở rộng quy mô các LMs tự hồi quy tạo sinh này với nhiều dữ liệu và tham số hơn và quan sát được những đột phá đáng kể trong nhiều ứng dụng thực tế (ví dụ, Brown et al., 2020), bao gồm tạo văn bản mở và các tác vụ hạ lưu khác nhau (ví dụ, trả lời câu hỏi). Các ví dụ công khai thành công bao gồm GPT-3 (với 170B tham số) (Brown et al., 2020), Gopher (280B) (Rae et al., 2021), Megatron-Turing (530B) (Smith et al., 2022), và PaLM (540B) (Chowdhery et al., 2022).

Mặc dù các LMs tự hồi quy quy mô lớn đã đạt được thành công to lớn, chúng cũng gặp phải một số điểm yếu. Thứ nhất, nó đòi hỏi một số lượng lớn tham số mô hình để ghi nhớ kiến thức thế giới, điều này làm cho việc triển khai tốn kém. Thứ hai, khó bảo vệ độ chính xác thực tế, có thể cung cấp cho người dùng thông tin không chính xác (Lee et al., 2022). Thứ ba, việc cập nhật kiến thức mô hình đã học trong quá trình tiền huấn luyện với các sự kiện cập nhật tốn kém (Meng et al., 2022), dẫn đến các câu trả lời lỗi thời (Lewis et al., 2020b).

Để giảm thiểu các vấn đề trên, một hướng nghiên cứu đề xuất cải thiện các mô hình ngôn ngữ với truy xuất. Quá trình truy xuất có thể được tích hợp vào LMs ở: i) giai đoạn tinh chỉnh (Karpukhin et al., 2020; Lewis et al., 2020b; Guu et al., 2020), hoặc ii) giai đoạn tiền huấn luyện (Borgeaud et al., 2022; Izacard et al., 2022). Hầu hết các công trình trước đây tăng cường BERT hoặc LMs mã hóa-giải mã với truy xuất ở giai đoạn tinh chỉnh, thể hiện thành công cho các tác vụ NLP tri thức chuyên sâu (Guu et al., 2020; Karpukhin et al., 2020; Lewis et al., 2020b; Khandelwal et al., 2020). Tuy nhiên, việc tiền huấn luyện các LMs tự hồi quy (chỉ có bộ giải mã) với truy xuất vẫn còn tương đối ít được khám phá, đặc biệt xem xét thành công đáng chú ý của ChatGPT (OpenAI, 2022) làm nổi bật tầm quan trọng cực kỳ lớn của các LMs tự hồi quy.

Gần đây nhất, RETRO (Borgeaud et al., 2022) đề xuất tiền huấn luyện các LMs tự hồi quy với một mô-đun truy xuất, có thể mở rộng thực tế đến tiền huấn luyện quy mô lớn từ đầu bằng cách truy xuất hàng tỷ token và giảm đáng kể tham số mô hình trong khi đạt được độ phức tạp thấp hơn so với GPT tiêu chuẩn. Nó cũng cung cấp tính linh hoạt để cập nhật kiến thức được lưu trữ trong LMs (Petroni et al., 2019) bằng cách cập nhật cơ sở dữ liệu truy xuất mà không cần huấn luyện lại LMs. Thành công của việc tiền huấn luyện LMs với truy xuất đặt ra một câu hỏi quan trọng cho cộng đồng nếu chúng ta muốn tiền huấn luyện các LMs tự hồi quy trong tương lai: Liệu chúng ta có nên tiền huấn luyện các LMs tự hồi quy (chỉ giải mã) với truy xuất theo mặc định hay không? Tuy nhiên, các công trình trước đây (Borgeaud et al., 2022) bỏ lỡ việc đánh giá quan trọng về việc liệu mô hình như RETRO có thể đạt được kết quả tương đương hoặc thậm chí tốt hơn về tạo văn bản mở và các tác vụ NLP hạ lưu khác nhau, ngoài độ phức tạp thấp hơn trên tập dữ liệu held-out so với GPT tiêu chuẩn.

Để trả lời câu hỏi trên và lấp đầy khoảng trống còn thiếu, chúng tôi thực hiện một nghiên cứu mở rộng về RETRO, vì theo hiểu biết của chúng tôi, RETRO là LM tự hồi quy tăng cường truy xuất duy nhất hỗ trợ tiền huấn luyện quy mô lớn với truy xuất trên tập văn bản tiền huấn luyện khổng lồ với hàng trăm tỷ hoặc nghìn tỷ token. Nghiên cứu toàn diện của chúng tôi làm sáng tỏ hướng đầy hứa hẹn của việc tiền huấn luyện các LMs tự hồi quy với truy xuất để phục vụ như các mô hình nền tảng tương lai, vì chúng tổng thể vượt trội các mô hình GPT tiêu chuẩn về độ phức tạp, chất lượng tạo văn bản, và hiệu suất tác vụ hạ lưu, đặc biệt cho các tác vụ tri thức chuyên sâu, bao gồm QA miền mở.

2 Phát hiện chính
Chúng tôi tái tạo thành công và tiền huấn luyện RETRO (Borgeaud et al., 2022) từ đầu, với kích thước tham số từ 148M đến 9.5B bằng cách truy xuất từ tập văn bản với hơn 330B token. Ngoài ra, chúng tôi thảo luận về chiến lược suy luận của RETRO để tạo văn bản không được đề cập trong Borgeaud et al. (2022), và thực hiện đánh giá quy mô lớn trong các tình huống khác nhau.

Để giảm thiểu các biến số khác biệt giữa RETRO và GPT, chúng tôi sử dụng cùng kiến trúc bộ giải mã, cùng siêu tham số, và cùng tập văn bản tiền huấn luyện để tiền huấn luyện RETRO và GPT với cùng số bước tiền huấn luyện. Chúng tôi nổi bật các phát hiện mới của chúng tôi cho RETRO và GPT như sau:

2.1 Tạo văn bản
Chúng tôi tiến hành một nghiên cứu có hệ thống (xem §5) để hiểu và phân tích RETRO bằng cách đánh giá chất lượng tạo văn bản mở của nó thông qua đánh giá con người và tự động. RETRO thể hiện hiệu suất tốt hơn GPT với ít lặp lại hơn đáng kể, độ chính xác thực tế cao hơn vừa phải, và mức độ độc tính thấp hơn một chút. RETRO ngang bằng với GPT về mặt tính trôi chảy, mạch lạc.

2.2 Benchmark LM Evaluation Harness
Về đánh giá zero-shot trên benchmark tiêu chuẩn, RETRO có thể cải thiện tổng thể so với GPT qua các tác vụ khác nhau, vượt trội đáng kể GPT trong các tác vụ tri thức chuyên sâu như Hellaswag và BoolQ trong khi đạt hiệu suất tương tự ở các tác vụ khác. Cụ thể, chúng tôi đánh giá khả năng zero-shot của RETRO và GPT trên chín tác vụ phân loại NLP hạ lưu đại diện (xem §6). Ngoài ra, các phát hiện của chúng tôi chứng minh rằng RETRO có thể tận dụng các láng giềng được truy xuất và cải thiện đáng kể độ chính xác cho các tác vụ tri thức chuyên sâu trong đánh giá zero-shot. Ngược lại, việc tích hợp các láng giềng được truy xuất này trực tiếp trong giai đoạn suy luận có thể làm hại hiệu suất của GPT. Những kết quả này củng cố thêm tiềm năng của RETRO, được tiền huấn luyện với khả năng truy xuất, như một phương pháp đầy hứa hẹn.

2.3 QA miền mở
Đối với các tác vụ QA miền mở, RETRO đạt hiệu suất vượt trội đáng kể so với GPT tăng cường truy xuất tích hợp truy xuất trong quá trình tinh chỉnh qua các kích thước mô hình và tập dữ liệu khác nhau. Cụ thể, chúng tôi đề xuất một biến thể của mô hình, RETRO++, cho QA miền mở đưa bằng chứng có liên quan nhất vào bộ giải mã và nhiều bằng chứng hơn vào bộ mã hóa của nó, khác với phiên bản gốc (Borgeaud et al., 2022). RETRO++ có thể cải thiện đáng kể điểm khớp chính xác (EM) trên Natrual Question từ 40.9% lên 54.1%, cao hơn đáng kể so với 45.5% được báo cáo bởi RETRO gốc.

--- TRANG 3 ---
Tên mô hình #/ Token Khi tích hợp truy xuất Kiến trúc Khởi tạo Tái lập chỉ mục
RETRO (Borgeaud et al.) O(10¹²) Tiền huấn luyện chỉ có bộ giải mã Từ đầu / GPT tiền huấn luyện Không
Atlas (Izacard et al.) O(10⁹) Tiền huấn luyện mã hóa-giải mã T5 tiền huấn luyện Có
REALM (Guu et al.) O(10⁹) Tiền huấn luyện chỉ có bộ mã hóa BERT tiền huấn luyện Có
RAG (Lewis et al.) O(10⁹) Tinh chỉnh mã hóa-giải mã BART tiền huấn luyện Không
DPR (Karpukhin et al.) O(10⁹) Tinh chỉnh chỉ có bộ mã hóa BERT tiền huấn luyện Không
FiD (Izacard and Grave) O(10⁹) Tinh chỉnh mã hóa-giải mã T5 tiền huấn luyện Không
KNN-LM (Khandelwal et al.) O(10⁹) Suy luận chỉ có bộ giải mã GPT tiền huấn luyện Không

Bảng 1: So sánh các mô hình tăng cường truy xuất khác nhau về #/ token truy xuất, giai đoạn nào tích hợp truy xuất vào LMs, kiến trúc của LM nền tảng, có cần khởi tạo từ checkpoint LM hiện có hay không, và có cần tái lập chỉ mục tốn kém hay không. RETRO là LM tăng cường truy xuất có thể mở rộng nhất do truy xuất theo khối và LM tự hồi quy chỉ có bộ giải mã có thể mở rộng (Thoppilan et al., 2022; Brown et al., 2020; Smith et al., 2022; Chowdhery et al., 2022) mà không cần làm mới chỉ mục truy xuất tốn kém.

3 Công trình liên quan
Truy xuất đã được áp dụng trong các tác vụ NLP khác nhau trong nhiều năm, bao gồm trả lời câu hỏi (QA) (ví dụ, Bilotti et al., 2007), dịch máy (ví dụ, Zhang et al., 2018), và hội thoại (Shuster et al., 2021; Thoppilan et al., 2022; Komeili et al., 2021). Đặc biệt, các mô hình ngôn ngữ đã được tăng cường với truy xuất ở các giai đoạn khác nhau, bao gồm thời gian suy luận (Khandelwal et al., 2020; Yogatama et al., 2021), giai đoạn tinh chỉnh (Karpukhin et al., 2020; Lewis et al., 2020b; Guu et al., 2020), và giai đoạn tiền huấn luyện (Borgeaud et al., 2022; Izacard et al., 2022).

LMs đã được tăng cường với truy xuất ở giai đoạn tinh chỉnh cho các tác vụ hạ lưu, chủ yếu cho QA miền mở. DPR (Karpukhin et al., 2020) tinh chỉnh một BERT để mã hóa câu hỏi và BERT khác để mã hóa câu trả lời trong khung mã hóa kép, sử dụng mất mát đối chiếu để căn chỉnh biểu diễn ẩn của câu hỏi và câu trả lời tương ứng. RAG (Lewis et al., 2020b) nghiên cứu công thức tinh chỉnh cho các mô hình tạo sinh tăng cường truy xuất, đặc biệt trên các tác vụ QA miền mở. FiD (Izacard and Grave, 2021) cải thiện RAG với LM nền tảng T5 tốt hơn, và kết hợp nhiều đoạn văn được truy xuất vào bộ giải mã trong quá trình tinh chỉnh để cải thiện thêm độ chính xác QA. WebGPT (Nakano et al., 2021) tận dụng công cụ tìm kiếm web và tinh chỉnh GPT sử dụng học tăng cường với phản hồi con người (RLHF) để tạo tài liệu tham khảo và cải thiện tính thực tế, điều này trực giao với công trình của chúng tôi tập trung vào tiền huấn luyện với truy xuất. RLHF được đề xuất cũng có thể được áp dụng cho RETRO.

REALM (Guu et al., 2020) thực hiện cả chiến lược tiền huấn luyện không giám sát và tinh chỉnh có giám sát cho mô hình BERT tăng cường truy xuất trong QA miền mở. Tiền huấn luyện của họ bao gồm việc nhúng lại và tái lập chỉ mục tất cả tài liệu một cách bất đồng bộ sau mỗi vài trăm bước huấn luyện, điều này nhanh chóng trở nên không thực tế cho tập văn bản huấn luyện với nghìn tỷ token. Atlas (Izacard et al., 2022) sử dụng phương pháp tương tự nhưng tăng cường kiến trúc T5 (Raffel et al., 2020) với truy xuất ở cả tiền huấn luyện và tinh chỉnh. Trước khi tiền huấn luyện, nó đầu tiên khởi tạo LM nền tảng mã hóa-giải mã với T5 tiền huấn luyện, và bộ truy xuất dày đặc với Contriever tiền huấn luyện (Izacard et al.). Trong quá trình tiền huấn luyện, nó cũng áp dụng làm mới chỉ mục bất đồng bộ sau mỗi 1000 bước.

Ngược lại, RETRO (Borgeaud et al., 2022) nhúng và lập chỉ mục toàn bộ tập văn bản huấn luyện ở mức khối (ví dụ, kích thước khối = 64) với BERT đóng băng trước khi tiền huấn luyện. Trong quá trình tiền huấn luyện, mô hình dựa vào bộ mã hóa hai chiều có thể huấn luyện để nhúng các khối văn bản thô được truy xuất. Bộ giải mã GPT tiếp tục "chọn" phần bằng chứng có liên quan từ phía bộ mã hóa bằng attention chéo theo khối. Thiết kế kiến trúc này cho phép tiền huấn luyện LM trên hàng trăm tỷ token bằng cách truy xuất từ nghìn tỷ token. Xem Bảng 1 để so sánh hoàn chỉnh các LMs tăng cường truy xuất.

4 Mô hình và Triển khai
Trong phần này, chúng tôi đầu tiên giới thiệu những kiến thức cơ bản về RETRO, sau đó cung cấp công thức chi tiết về triển khai của chúng tôi, bao gồm cơ sở dữ liệu truy xuất, tiền huấn luyện, và tinh chỉnh và tạo sinh tăng cường truy xuất.

4.1 Kiến thức cơ bản về RETRO
RETRO là một mô hình ngôn ngữ tự hồi quy được tăng cường với mô-đun truy xuất sử dụng truy xuất theo khối, cho phép nó mở rộng quy mô lên nghìn tỷ token. Mô hình chia cả chuỗi đầu vào và kho dữ liệu truy xuất thành các chuỗi khối. RETRO truy xuất các khối láng giềng gần nhất từ cơ sở dữ liệu truy xuất bằng khối trước đó và kết hợp thông tin này với ngữ cảnh từ các khối trước đó để hướng dẫn việc tạo khối tiếp theo. Để duy trì tính nhân quả, mô hình chỉ có thể sử dụng các láng giềng gần nhất của khối trước đó cho việc tạo sinh tự hồi quy.

4.2 Triển khai
Vì RETRO không có triển khai mã nguồn mở chính thức và các checkpoint tiền huấn luyện, chúng tôi tái tạo và tiền huấn luyện RETRO từ đầu.

4.2.1 Cơ sở dữ liệu truy xuất
Chúng tôi xây dựng cơ sở dữ liệu truy xuất với toàn bộ tập dữ liệu tiền huấn luyện được đề cập trong §B. Bằng cách này, RETRO và GPT tiêu chuẩn có kích thước tương tự là so sánh công bằng, vì chúng được tiền huấn luyện sử dụng cùng thông tin từ tập văn bản tiền huấn luyện. Cơ sở dữ liệu truy xuất là cơ sở dữ liệu khóa-giá trị, trong đó giá trị là các khối được chia từ tập văn bản tiền huấn luyện, và khóa là các nhúng BERT tương ứng. Tập dữ liệu tiền huấn luyện 330B token của chúng tôi tạo ra cơ sở dữ liệu truy xuất gồm tổng cộng 5.3B khối với kích thước khối m = 64.

Chỉ mục truy xuất. Chúng tôi sử dụng chỉ mục Faiss (Johnson et al., 2019) như triển khai cho bộ truy xuất dày đặc để tìm kiếm các láng giềng gần nhất xấp xỉ trong không gian nhúng BERT. Chúng tôi cấu hình chỉ mục Faiss để nhóm các nhúng dày đặc thành 2²² centroid được tăng tốc với đồ thị Hierarchical Navigable Small World (Malkov và Yashunin, 2018) để tăng tốc truy vấn. Chúng tôi cũng mã hóa các nhúng bằng lượng tử hóa tích tối ưu (Gray và Neuhoff, 1998; Ge et al., 2014) để nén tải bộ nhớ và cải thiện thêm thông lượng truy vấn. Kết quả là, chúng tôi có thể đạt 4 ms mỗi truy vấn trên toàn bộ tập văn bản tiền huấn luyện trung bình cho mỗi khối trên node DGX-2H. Có thể tìm thêm chi tiết trong Phụ lục §A.

4.2.2 Tiền huấn luyện các mô hình RETRO
Chúng tôi sử dụng cùng cấu hình transformer (#/ tầng, kích thước ẩn, đầu attention) và tiền huấn luyện cả RETRO và GPT tiêu chuẩn từ đầu. Cụ thể, chúng tôi tiền huấn luyện RETRO qua các kích thước tham số khác nhau, từ 148M (Small), 410M (Medium), 1.5B (XL), và 9.5B (XXL). Chúng tôi cũng sử dụng cùng lịch trình tiền huấn luyện để tiền huấn luyện RETRO và GPT với cùng số bước. Chúng tôi liệt kê độ phức tạp xác thực của GPT và RETRO sau tiền huấn luyện trong Bảng 2. Chúng tôi trình bày thêm chi tiết trong Phụ lục §B, bao gồm lịch trình tiền huấn luyện, chi phí tính toán (giờ GPU), và kiến trúc mô hình.

[THIS IS TABLE: Bảng 2 showing validation perplexity for GPT and RETRO models of different sizes]
Nhỏ Trung bình XL XXL
GPT 17.76 13.18 10.18 7.86
RETRO (k= 2) 12.99 10.06 8.10 6.72

4.2.3 Tạo sinh tăng cường truy xuất
Chúng tôi thảo luận về công thức tạo sinh và suy luận trong chế độ xử lý theo lô cho RETRO, điều này bị thiếu trong tài liệu trước đó.

Quy tắc "Left Padding". Truy xuất theo khối của RETRO cải thiện khả năng mở rộng nhưng tạo ra các ràng buộc căn chỉnh theo khối, dẫn đến vấn đề trong tạo sinh có điều kiện với ngữ cảnh ngắn. Khi độ dài chuỗi nhỏ hơn kích thước khối, RETRO không thể sử dụng khả năng truy xuất của nó vì không có khối trước đó để truy xuất. Thay vào đó, RETRO thêm các token đệm vào bên trái ngữ cảnh, cho phép RETRO tận dụng các láng giềng được truy xuất từ ngữ cảnh trước đó để hướng dẫn việc tạo token tiếp theo (Hình 1a). Chúng tôi tóm tắt nguyên tắc chung này trong RETRO như quy tắc "left padding", vì nó có thể tận dụng thông tin ngữ cảnh để truy xuất nhiều nhất. Quy tắc này vẫn được ưa thích cho các chuỗi đầu vào lớn hơn kích thước khối, vì nó đảm bảo ngữ cảnh gần nhất và bên phải nhất được sử dụng để truy xuất, làm cho nó phù hợp hơn cho dự đoán token tiếp theo (xem Hình 1b).

Tần suất truy xuất. Để tạo sinh hiệu quả các chuỗi dài với RETRO, chúng tôi lưu ý sự đánh đổi linh hoạt giữa tạo sinh tăng cường truy xuất và tải tính toán. Phương pháp trực tiếp bao gồm truy xuất ở mỗi bước giải mã, tối đa hóa việc sử dụng mô-đun truy xuất nhưng tăng tải tính toán (Hình 1b, bước truy xuất = 1). Phương pháp khác truy xuất láng giềng ở tần suất của kích thước khối, giảm tải nhưng hy sinh độ chính xác (Phụ lục Hình 3b, bước truy xuất = 64). Để cân bằng các yếu tố này, chúng tôi giới thiệu bước truy xuất linh hoạt, cho phép các nhà thực hành mô hình chọn bao nhiêu token để tạo với các láng giềng được truy xuất hiện tại trước khi cập nhật ngữ cảnh. Các bước truy xuất nhỏ hơn được ưa thích cho các tác vụ hạ lưu với câu trả lời ngắn để đảm bảo láng giềng chính xác, trong khi các bước lớn hơn được sử dụng để tạo sinh hiệu quả các đoạn văn dài. Chúng tôi cung cấp thêm chi tiết trong Phụ lục §C.

4.2.4 Huấn luyện theo lô cho các tác vụ hạ lưu
Khi tinh chỉnh RETRO cho các tác vụ hạ lưu (ví dụ, QA), việc tách ngữ cảnh hoặc câu hỏi khỏi khối câu trả lời ứng viên để duy trì tính nhân quả trong mô hình tự hồi quy là rất quan trọng. Điều này dẫn đến quy tắc "left padding" được sửa đổi: đệm các khối ngữ cảnh từ bên trái và các khối câu trả lời từ bên phải (Hình 1c). Đệm căn chỉnh các chuỗi đầu vào với kích thước khối, cho phép huấn luyện và suy luận chế độ lô để đánh giá nhanh hơn. Bằng cách thêm các khối đệm vào bên phải, các chuỗi có số lượng khối khác nhau có thể được xử lý cùng nhau, cải thiện thêm hiệu quả.

5 Tạo văn bản mở
Trong phần này, chúng tôi đi sâu vào bài toán tạo văn bản mở, đề cập đến các tác vụ tạo phần tiếp theo mạch lạc cho trước gợi ý trước đó. Vì bài toán này cho RETRO chưa bao giờ được nghiên cứu trước đây, chúng tôi cố gắng lấp đầy khoảng trống và đánh giá việc tạo văn bản mở của RETRO so với GPT từ ba khía cạnh: a) chất lượng văn bản, b) tính thực tế, và c) độc tính.

5.1 Chất lượng văn bản
Chúng tôi thực hiện cả đánh giá tự động và con người.

5.1.1 Đánh giá tự động
Các chỉ số đánh giá. Chúng tôi theo các công trình trước (Holtzman et al., 2019; Zhu et al., 2018) và xem xét các chỉ số sau: Repetition % đo phần trăm các văn bản tạo ra chứa cụm từ lặp lại, SELF-BLUE đánh giá tính đa dạng của các văn bản tạo ra, và Zipf Coefficient đo việc sử dụng từ vựng. Xem định nghĩa chi tiết và thiết lập đánh giá trong Phụ lục §D.1.

Kết quả thực nghiệm. Kết quả của chúng tôi được hiển thị trong Bảng 3. Chúng tôi lưu ý rằng RETRO có thể giảm phần trăm lặp lại so với GPT với biên độ lớn qua các kích thước khác nhau. Cụ thể, RETRO trung bình giảm 21% lặp lại so với GPT qua các kích thước khác nhau. Điều này cho thấy mô-đun truy xuất có thể giúp giảm thoái hóa văn bản bằng cách tham khảo văn bản con người được truy xuất. Về việc sử dụng từ vựng và tính đa dạng tạo sinh, chúng tôi không quan sát thấy sự khác biệt lớn giữa GPT và RETRO, điều này ngụ ý rằng những thuộc tính này chủ yếu phụ thuộc vào thành phần bộ giải mã của LMs.

[THIS IS TABLE: Bảng 3 showing automatic evaluation metrics for text generation quality across different model sizes]

5.1.2 Đánh giá con người
Chúng tôi cũng tiến hành đánh giá con người để xác minh thêm chất lượng của văn bản được tạo ra.

Các chỉ số đánh giá. Chúng tôi yêu cầu các chuyên gia chú thích đánh giá mỗi văn bản tạo ra với điểm tính trôi chảy, đo khả năng đọc hiểu của con người và lỗi ngữ pháp từ 1 (Không thể đọc hiểu) đến 5 (Rất trôi chảy), và điểm mạch lạc, đo sự liên quan giữa gợi ý và các phần tiếp theo tương ứng từ 1 (Không liên quan) đến 5 (Rất liên quan). Thêm chi tiết có thể được tìm thấy trong §D.2.

Kết quả thực nghiệm. Chúng tôi trình bày biểu đồ histogram phiếu bầu con người trong Phụ lục Hình 4. Chúng tôi quan sát thấy hầu hết phiếu bầu tập trung ở khu vực điểm >= 3 cho cả sự liên quan và tính trôi chảy, điều này cho thấy văn bản được tạo ra từ cả hai mô hình đều có chất lượng cao và liên quan chặt chẽ đến các gợi ý. Sự khác biệt giữa GPT và RETRO rất tinh tế, với điểm sự liên quan trung bình (3.726) và tính trôi chảy (3.826) của RETRO vượt trội nhẹ so với điểm sự liên quan trung bình (3.715) và tính trôi chảy (3.818) của GPT.

Từ cả đánh giá tự động và con người, chúng tôi có thể kết luận rằng mặc dù việc tạo sinh của RETRO thêm một số phức tạp, chúng tôi không thấy dấu hiệu nào của sự thoái hóa của RETRO so với GPT. Hơn nữa, RETRO được chứng minh là có thể giảm lặp lại và cải thiện nhẹ chất lượng tạo văn bản.

5.2 Tính thực tế
Tính thực tế đề cập đến việc mạch lạc để cung cấp các nguồn kiến thức sự thật cơ bản trong NLP. Chúng tôi tận dụng hai benchmark được thiết lập tốt (Lee et al., 2022; Lin et al., 2021) để đánh giá độ chính xác thực tế của RETRO và GPT. Chúng tôi đặt thiết lập thực nghiệm chi tiết và các chỉ số trong §E.

Factuality Prompts. Chúng tôi trình bày kết quả trên benchmark FACTUALITY PROMPTS trong Bảng 4a, nơi chúng tôi theo giao thức (Lee et al., 2022) và sử dụng các chiến lược giải mã khác nhau để đánh giá. Chúng tôi quan sát thấy tổng thể RETRO thể hiện độ chính xác thực tế tốt hơn và ít ảo giác hơn. Quan sát của chúng tôi cũng phù hợp với (Lee et al., 2022) rằng khác với lấy mẫu top-p với tính ngẫu nhiên, giải mã tham lam cho độ chính xác thực tế tốt hơn nhiều bằng cách hy sinh tính đa dạng mẫu. RETRO cho thấy cải thiện đáng kể so với GPT sử dụng cả thuật toán giải mã top-p và tham lam.

TruthfulQA. Chúng tôi cũng đánh giá tính trung thực trong Bảng 4b, nơi chúng tôi quan sát kết luận nhất quán rằng RETRO có xu hướng bền vững hơn với các quan niệm sai lầm của con người so với GPT qua các gợi ý khác nhau (được thể hiện trong Phụ lục Bảng 12 và 13). Vì việc sử dụng định dạng null mang lại hiệu suất tốt hơn một cách nhất quán, chúng tôi do đó cố định gợi ý và tiến hành nghiên cứu loại bỏ với các cơ sở dữ liệu truy xuất khác nhau. Chúng tôi quan sát thấy cơ sở dữ liệu tiền huấn luyện và cơ sở dữ liệu DPR mang lại độ chính xác thực tế tốt nhất. Điều này cho thấy rằng (i) bộ truy xuất được tinh chỉnh hoặc (ii) truy xuất từ tập văn bản lớn hơn có hữu ích để cải thiện độ chính xác thực tế. Ngoài ra, biên độ cải thiện của RETRO trên TruthfulQA không đáng kể như trên FACTUALITY PROMPTS. Điều này có khả năng là do TruthfulQA là tập dữ liệu đối nghịch chứa các loại câu hỏi khó, chẳng hạn như lỗi logic và lỗi chỉ số (ví dụ, "Bạn có phải là trí tuệ nhân tạo không?", thêm ví dụ có thể được tìm thấy trong Phụ lục Bảng 14). Trong những trường hợp này, truy xuất từ cơ sở dữ liệu không hiệu quả trong việc hỗ trợ trả lời những câu hỏi như vậy.

[THIS IS TABLE: Bảng 4 showing factuality and truthfulness evaluation results for RETRO (XL) and GPT (XL)]

5.3 Độc tính
Độc tính của LMs đề cập đến khả năng LMs đưa ra các văn bản tạo ra độc hại. Trong nghiên cứu này, chúng tôi theo benchmark REALTOXICTY PROMPTS (Gehman et al., 2020) để đánh giá độc tính tiềm tàng của RETRO và GPT.

Các chỉ số đánh giá. Theo Gehman et al. (2020), chúng tôi báo cáo Expected Maximum Toxicity, đánh giá độc tính của văn bản tạo ra trong trường hợp xấu nhất, cũng như Toxicity Probability ước tính tần suất thực nghiệm của việc tạo ra ngôn ngữ độc hại. Xem thêm chi tiết và thiết lập trong §F.

Kết quả thực nghiệm. Độc tính của LMs được hiển thị trong Bảng 5. So với GPT, chúng tôi lưu ý rằng RETRO với tập văn bản tiền huấn luyện thậm chí còn tăng độc tính của các văn bản tạo ra. Hơn nữa, chúng tôi quan sát thấy sự gia tăng độc tính nhiều hơn trong các gợi ý độc hại so với các gợi ý không độc hại. Điều này cho thấy rằng khi gợi ý RETRO với ngữ cảnh độc hại, nó có nhiều khả năng truy xuất bằng chứng độc hại và do đó khuếch đại các vấn đề. Để xác nhận vấn đề khuếch đại độc tính, chúng tôi tiếp tục tiến hành hai bộ nghiên cứu loại bỏ: (i) Chúng tôi lưu bằng chứng truy xuất và tính toán Expected Mean Toxicity của cả văn bản tạo ra và bằng chứng truy xuất. Chúng tôi quan sát thấy rằng độc tính của bằng chứng truy xuất là 0.177, cao hơn độc tính của các văn bản tạo ra (0.146). (ii) Chúng tôi thay đổi cơ sở dữ liệu truy xuất sang cơ sở dữ liệu Wikipedia, cho thấy độc tính thấp hơn cho bằng chứng truy xuất (0.132). Kết quả là, chúng tôi quan sát thấy RETRO với cơ sở dữ liệu truy xuất Wikipedia có thể giúp giảm thiểu độc tính của GPT như được hiển thị trong Bảng 5, với xác suất độc tính giảm từ 37% xuống 35%.

Chúng tôi cũng lưu ý rằng việc sử dụng N lớn hơn như láng giềng gần nhất và lọc bằng chứng truy xuất theo độc tính không hữu ích lắm. Chúng tôi giả thuyết lý do là sự tương tự giữa đầu vào và bằng chứng truy xuất bị hạn chế với N lớn hơn, do đó mang lại cross-attention thấp trên bằng chứng truy xuất.

[THIS IS TABLE: Bảng 5 showing toxicity evaluation results for GPT (XL) and RETRO (XL) with different configurations]

6 Benchmark LM Evaluation Harness
Bên cạnh việc tạo văn bản mở, việc kiểm tra tính tổng quát của RETRO trên các tác vụ hạ lưu khác nhau cũng quan trọng, điều này cũng bị thiếu trong tài liệu. Do đó, chúng tôi sử dụng LM Evaluation Harness Benchmark (Gao et al., 2021) và xem xét chín tác vụ NLP hạ lưu đại diện sau. Xem thêm chi tiết trong §G.

Đánh giá zero-shot. Chúng tôi trình bày kết quả đánh giá zero-shot trong Bảng 6. Chúng tôi thấy rằng trung bình RETRO có thể cải thiện độ chính xác tác vụ hạ lưu qua các tác vụ khác nhau. Hơn nữa, chúng tôi quan sát thấy các cải thiện lớn hơn trong các tác vụ tri thức chuyên sâu như Hellaswag và BoolQ (6 trong 8 trường hợp), yêu cầu kiến thức thực tế để hướng dẫn lý luận. Lưu ý rằng kết quả đánh giá zero-shot dễ bị ảnh hưởng bởi định dạng gợi ý, vì vậy kết quả có một số biến thiên nhất định.

GPT tăng cường truy xuất tại thời gian suy luận. Chúng tôi đã thấy rằng truy xuất cải thiện đáng kể RETRO qua các tác vụ hạ lưu khác nhau trong thiết lập zero-shot. Trong nghiên cứu loại bỏ này, chúng tôi thêm bằng chứng truy xuất của RETRO vào đầu ngữ cảnh để xem liệu truy xuất cũng có thể hữu ích cho GPT tại thời gian suy luận hay không. Chúng tôi đánh giá độ chính xác zero-shot sau khi thêm vào đầu bằng chứng truy xuất top-1. Kết quả được hiển thị trong Phụ lục Bảng 16. Chúng tôi quan sát thấy việc thêm trực tiếp bằng chứng từ cơ sở dữ liệu truy xuất làm rối ngữ cảnh GPT trong thiết lập zero-shot, mang lại độ chính xác thấp khoảng 24.5%. Chúng tôi giả thuyết lý do là bằng chứng truy xuất có thể ồn ào. Không có tiền huấn luyện hoặc tinh chỉnh phù hợp, GPT trong thiết lập học zero-shot quá chú ý đến bằng chứng ồn ào, do đó cho độ chính xác hạ lưu thấp.

[THIS IS TABLE: Bảng 6 showing accuracy results on nine downstream tasks in zero-shot setting for different model sizes]

7 Trả lời câu hỏi miền mở
Trong phần này, chúng tôi nghiên cứu hai tập dữ liệu QA miền mở được sử dụng rộng rãi, Natural Question (NQ) và TriviaQA.

7.1 Thiết lập thực nghiệm
Bằng chứng được truy xuất như ngữ cảnh. Công trình RETRO ban đầu tận dụng bằng chứng được truy xuất (tức là các đoạn văn) bằng cách đưa tất cả vào bộ mã hóa. Chúng tôi lập luận rằng bằng chứng có liên quan hàng đầu quan trọng hơn những cái khác và nên được sử dụng làm ngữ cảnh cho câu hỏi. Do đó, bằng chứng liên quan hàng đầu nên được đưa vào bộ giải mã, và phần còn lại của bằng chứng có thể được tích hợp bởi bộ mã hóa. Để triển khai trong các thí nghiệm của chúng tôi, chúng tôi thêm đoạn văn liên quan top-1 vào đầu đầu vào bộ giải mã, và định dạng lại đầu vào với Template A: "title: {title}, source: {source} \n question: {question} \n answer: {answer}". Đối với các mô hình không có bằng chứng được truy xuất trong ngữ cảnh, chúng tôi theo Borgeaud et al. (2022) để định dạng đầu vào với Template B: "question: {question} \n answer: {answer}".

Ngoài một số phương pháp cơ sở trong Bảng 7, chúng tôi so sánh các mô hình sau: 1) GPT (close-book) đơn giản tinh chỉnh mô hình GPT tiền huấn luyện với Template B đầu vào mà không sử dụng bất kỳ tài liệu được truy xuất nào. 2) RAG GPT áp dụng tinh chỉnh RAG (Lewis et al., 2020b) cho GPT, đặt bằng chứng được truy xuất làm ngữ cảnh của nó. Nó sử dụng các tài liệu được truy xuất hàng đầu bởi DPR với Template A đầu vào và tinh chỉnh mô hình GPT tiền huấn luyện, đại diện cho việc tích hợp truy xuất vào GPT ở giai đoạn tinh chỉnh. 3) RETRO mã hóa bằng chứng được truy xuất bằng bộ mã hóa và tinh chỉnh mô hình RETRO tiền huấn luyện với Template B đầu vào. 4) RETRO++ tinh chỉnh mô hình RETRO tiền huấn luyện với bằng chứng được truy xuất hàng đầu được bao gồm Template A đầu vào trong khi để phần còn lại của bằng chứng cho bộ mã hóa. Thêm chi tiết có thể được tìm thấy trong §H.

[THIS IS TABLE: Bảng 7 showing comparison of RETRO and existing QA models with NQ and TriviaQA results]

7.2 Kết quả và phân tích
Bảng 7 hiển thị kết quả trên NQ và TriviaQA. RETRO++ của chúng tôi đạt điểm Exact Match (EM) 54.1, cao hơn 8.6 so với bài báo RETRO gốc. Chúng tôi thấy chìa khóa thành công của RETRO là tích hợp tài liệu được truy xuất hàng đầu từ DPR vào bộ giải mã làm ngữ cảnh, mang lại cho chúng tôi 13.2 cải thiện tuyệt đối bằng cách so sánh RETRO và RETRO++ của chúng tôi. Lưu ý rằng RETRO của chúng tôi có điểm EM thấp hơn (40.91) so với bài báo gốc (45.5), vì mô hình của họ được huấn luyện trên 600B token, trong khi của chúng tôi được huấn luyện trên 330B token. Bằng cách so sánh RAG GPT với RETRO++, chúng tôi cho thấy rằng tiền huấn luyện LM tự hồi quy với truy xuất (tức là RETRO++) mang lại độ chính xác QA tốt hơn so với chỉ tinh chỉnh LM tự hồi quy với truy xuất (tức là RAG GPT). Phụ lục §H.3 đưa ra các nghiên cứu định tính về NQ.

Mở rộng quy mô kích thước mô hình. Hình 2 hiển thị điểm EM khi mở rộng quy mô kích thước mô hình cho RAG GPT và RETRO++ trên NQ và TriviaQA. Khi kích thước mô hình tăng, hiệu suất của tất cả các mô hình tăng đơn điệu. RETRO++ đạt hiệu suất tốt nhất qua tất cả các tác vụ và kích thước mô hình. Lưu ý rằng, Wang et al. (2023) tiếp tục mở rộng quy mô RETRO lên 48B và thảo luận về cách điều chỉnh hướng dẫn có thể giúp cải thiện LLMs tăng cường truy xuất cho trả lời câu hỏi miền mở zero-shot.

[THIS IS FIGURE: Hình 2 showing performance comparisons between RAG GPT and RETRO++ on NQ and TriviaQA across different model sizes]

7.3 Đánh giá zero-shot với và không có điều chỉnh hướng dẫn
Điều chỉnh hướng dẫn (Wei et al., 2022a; Chung et al., 2022) tinh chỉnh LLMs trên tập hợp các tập dữ liệu được mô tả qua hướng dẫn ngôn ngữ tự nhiên, cải thiện đáng kể độ chính xác zero-shot cho các tác vụ hạ lưu chưa thấy. Trong tiểu mục này, chúng tôi nghiên cứu cách điều chỉnh hướng dẫn có thể giúp với QA miền mở cho LLMs tăng cường truy xuất.

Dữ liệu điều chỉnh hướng dẫn. Chúng tôi sử dụng hỗn hợp các tập dữ liệu điều chỉnh hướng dẫn chất lượng cao gồm 128K mẫu để huấn luyện LLMs tuân theo hướng dẫn, bao gồm: tập dữ liệu đối thoại xã hội chất lượng cao SODA (Kim et al., 2022), tập dữ liệu QA dạng dài ELI5 yêu cầu câu trả lời chi tiết (Fan et al., 2019), hướng dẫn được tạo bởi LLM: Self-Instruct (Wang et al., 2022) và Unnatural Instructions (Honovich et al., 2022), tập dữ liệu FLAN và Chain-of-thought (Chung et al., 2022; Wei et al., 2022b; Longpre et al., 2023), tập dữ liệu hội thoại được viết bởi con người công khai OpenAssistant (Köpf et al., 2023) và Dolly (Conover et al., 2023).

Chi tiết triển khai. Chúng tôi tiến hành điều chỉnh hướng dẫn cho cả GPT (XXL) và RETRO (XXL). Chúng tôi tinh chỉnh LLMs bằng cách chỉ tính mất mát trên phản hồi cuối cùng từ trợ lý với kích thước lô 128 và tốc độ học 5e-6 trong 1000 bước với weight decay 0.01. Chúng tôi sử dụng trình tối ưu Adam (Kingma và Ba, 2014) với β1 = 0.9 và β2 = 0.98. Sau tinh chỉnh, chúng tôi theo cùng định dạng gợi ý như RAG GPT cho GPT được điều chỉnh hướng dẫn (XXL) và RETRO++ cho RETRO được điều chỉnh hướng dẫn (XXL) và đánh giá độ chính xác zero-shot trên tập dữ liệu Natural Question (NQ).

Kết quả. Kết quả của GPT tăng cường truy xuất (RAG GPT) và RETRO++ trước và sau điều chỉnh hướng dẫn được hiển thị trong Bảng 8. Chúng tôi quan sát thấy rằng việc áp dụng điều chỉnh hướng dẫn với RETRO và GPT tăng cường truy xuất (RAG GPT) thực sự mang lại cải thiện độ chính xác đáng kể. Hơn nữa, RETRO++ thể hiện độ chính xác tốt hơn một cách nhất quán so với RAG GPT. Kết quả này càng xác nhận tiềm năng và khả năng của RETRO khi sử dụng các kỹ thuật tiên tiến như điều chỉnh hướng dẫn. Lưu ý rằng, Wang et al. (2023) tiếp tục mở rộng quy mô RETRO lên 48B tham số để tiết lộ sức mạnh của điều chỉnh hướng dẫn.

[THIS IS TABLE: Bảng 8 showing Exact Match scores for zero-shot evaluation before and after instruction tuning]

8 Kết luận
Trong công trình này, chúng tôi thực hiện một nghiên cứu toàn diện về LLM tăng cường truy xuất tiền huấn luyện để trả lời câu hỏi: Liệu chúng ta có nên tiền huấn luyện LMs chỉ có bộ giải mã với truy xuất không? Chúng tôi quan sát thấy các cải thiện nhất quán trong chất lượng tạo văn bản, độ chính xác thực tế, độc tính thấp hơn, và độ chính xác tác vụ hạ lưu, đặc biệt cho các tác vụ tri thức chuyên sâu, bao gồm QA miền mở. Với ~25% phần trăm giờ GPU bổ sung cho tiền huấn luyện (xem Bảng 11 Phụ lục B), chúng tôi lập luận rằng tiền huấn luyện các mô hình ngôn ngữ tạo sinh với truy xuất là một hướng đầy hứa hẹn.

Hạn chế
Mặc dù hiệu suất ấn tượng của RETRO và RETRO++, các phát hiện của chúng tôi tiết lộ một số hạn chế mở đường cho nghiên cứu tương lai để giải quyết:

• Chất lượng của cơ sở dữ liệu truy xuất. Độ chính xác thực tế và giảm độc tính trong văn bản được tạo ra phụ thuộc vào chất lượng và phạm vi của cơ sở dữ liệu truy xuất. Điều này có nghĩa là hiệu suất và đầu ra của mô hình có thể thay đổi dựa trên cơ sở dữ liệu truy xuất. Hiệu suất của RETRO có thể bị ảnh hưởng nếu cơ sở dữ liệu chứa thông tin không chính xác, thiên vị, hoặc lỗi thời.

• Khả năng mở rộng. Việc tiền huấn luyện GPT và LLM tăng cường truy xuất từ đầu đòi hỏi tài nguyên tính toán đáng kể. Công trình của chúng tôi theo Borgeaud et al. (2022) và tiền huấn luyện GPT và RETRO lên kích thước 9B. Chúng tôi để lại việc tiếp tục mở rộng quy mô các LLMs tăng cường truy xuất như một công trình tương lai quan trọng.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên như trong bản gốc]

Phụ lục
[Nội dung phụ lục được giữ nguyên như trong bản gốc với các chi tiết kỹ thuật, bảng và hình ảnh]
