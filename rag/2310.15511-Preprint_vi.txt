# 2310.15511.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2310.15511.pdf
# Kích thước file: 1028566 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Bản thảo
KITAB: ĐÁNH GIÁ CÁC LLM VỀ THỎA MÃN RÀNG BUỘC
CHO TRUY XUẤT THÔNG TIN
Marah I Abdin¹ Suriya Gunasekar¹ Varun Chandrasekaran² Jerry Li¹
Mert Yuksekgonul³ Rahee Ghosh Peshawaria¹ Ranjita Naik¹ Besmira Nushi¹
¹Microsoft Research, ²University of Illinois Urbana-Champaign, ³Stanford University

TÓM TẮT
Chúng tôi nghiên cứu khả năng của các mô hình tiên tiến trong việc trả lời các truy vấn thỏa mãn ràng buộc cho truy xuất thông tin (ví dụ: "danh sách các cửa hàng kem ở San Diego"). Trong quá khứ, những truy vấn như vậy được coi là những nhiệm vụ chỉ có thể được giải quyết thông qua tìm kiếm web hoặc cơ sở tri thức. Gần đây hơn, các mô hình ngôn ngữ lớn (LLM) đã thể hiện những khả năng nổi lên ban đầu trong nhiệm vụ này. Tuy nhiên, nhiều tiêu chuẩn đánh giá truy xuất hiện tại hoặc đã bão hòa hoặc không đo lường việc thỏa mãn ràng buộc. Được thúc đẩy bởi những lo ngại gia tăng xung quanh tính không chính xác về mặt sự thật và ảo giác của các LLM, chúng tôi trình bày KITAB, một bộ dữ liệu mới để đo lường khả năng thỏa mãn ràng buộc của các mô hình ngôn ngữ. KITAB bao gồm dữ liệu liên quan đến sách qua hơn 600 tác giả và 13.000 truy vấn, và cũng cung cấp một phương pháp thu thập dữ liệu động và kiểm chứng ràng buộc liên quan để thu được dữ liệu kiểm tra tương tự cho các tác giả khác. Các thí nghiệm mở rộng của chúng tôi trên GPT4 và GPT3.5 đặc trưng hóa và tách biệt các chế độ thất bại phổ biến qua các chiều như độ phổ biến thông tin, loại ràng buộc và khả năng có sẵn ngữ cảnh. Kết quả cho thấy rằng trong trường hợp thiếu ngữ cảnh, các mô hình thể hiện những hạn chế nghiêm trọng được đo bằng thông tin không liên quan, lỗi thực tế và sự không đầy đủ, nhiều trong số đó trở nên tồi tệ hơn khi độ phổ biến thông tin giảm. Trong khi khả năng có sẵn ngữ cảnh giảm thiểu thông tin không liên quan, nó không hữu ích cho việc thỏa mãn ràng buộc, xác định những rào cản cơ bản đối với việc thỏa mãn ràng buộc. Chúng tôi mở nguồn các đóng góp của mình để thúc đẩy nghiên cứu sâu hơn về việc cải thiện khả năng thỏa mãn ràng buộc của các mô hình tương lai.¹

1 GIỚI THIỆU
Trả lời các truy vấn thực tế là một trong nhiều khả năng nổi lên của các mô hình ngôn ngữ lớn (LLM) (OpenAI, 2023; Touvron et al., 2023; Chowdhery et al., 2022). Điều này đã tái tạo lại cách thức hoạt động của các công cụ tìm kiếm bằng cách kết hợp trực tiếp các LLM đối thoại như một phần của trải nghiệm người dùng (ví dụ: BingChat). Như với nhiều khả năng nổi lên, đánh giá nghiêm ngặt vẫn là một thách thức do việc thiếu liên tục các tiêu chuẩn phù hợp, bão hòa tiêu chuẩn, ô nhiễm dữ liệu đào tạo và khó khăn trong việc đánh giá đầu ra được tạo ra mở từ các LLM (Chang et al., 2023; Liang et al., 2022). Đồng thời, một số lo ngại đã nảy sinh về việc các LLM lặp đi lặp lại việc bịa đặt thông tin sai hoặc cung cấp nội dung không liên quan (được gọi một cách không chính thức là ảo giác) (Bender et al., 2021; Bommasani et al., 2021; Zhang et al., 2023; Sun et al., 2023a).

Công trình này nghiên cứu và đánh giá khả năng thỏa mãn ràng buộc của các LLM trong bối cảnh truy xuất thông tin (IR). Tương tự như các bài toán tìm kiếm có ràng buộc truyền thống (Meseguer, 1989), các truy vấn thỏa mãn ràng buộc trong IR là các truy vấn bao gồm một tập hợp các ràng buộc cần được thỏa mãn bởi đầu ra được tạo. Khung này đã được đề xuất gần đây để nghiên cứu và phát hiện lỗi thực tế của các LLM bởi Yuksekgonul et al. (2023) như một góc nhìn hữu ích cũng kết nối độ phổ biến thông tin và tính khả thi của ràng buộc với khả năng thỏa mãn các ràng buộc đó của LLM. Ở đây, chúng tôi sử dụng cùng một khung để hướng dẫn đánh giá LLM và thiết kế thí nghiệm. Các truy vấn có ràng buộc cũng có thể được coi là dạng tổng quát hơn của các truy vấn từ khóa, boolean hoặc khớp mẫu (Baeza-Yates et al., 1999) và tìm kiếm web theo khía cạnh (Tunkelang, 2009; Hahn et al., 2010),

¹https://huggingface.co/datasets/microsoft/kitab
†Liên hệ với marah.abdin@microsoft.com và besmira.nushi@microsoft.com.

--- TRANG 2 ---
Bản thảo
Thông tin không liên quan ↓ Thông tin liên quan (Sách từ tác giả) Đầy đủ ↑ Tất cả Đúng ↑
Thỏa mãn ↑ Không thỏa mãn ↓

GPT4 0.26|0.33|0.00 0.51|0.49|0.78 0.24|0.19|0.21 0.24|0.26|0.70 0.08|0.08|0.31
GPT3.5 0.20|0.44|0.00 0.44|0.26|0.68 0.36|0.30|0.32 0.16|0.16|0.47 0.07|0.02|0.15

Bảng 1: Hiệu suất mô hình tổng hợp trên KITAB cho 3 lời nhắc KHÔNG-NGỮ-CẢNH|TỰ-NGỮ-CẢNH|VỚI-NGỮ-CẢNH (xem định nghĩa trong § 3.2) cho các truy vấn yêu cầu danh sách sách từ một tác giả nhất định thỏa mãn một ràng buộc sách bổ sung. Cả hai mô hình đều có tỷ lệ cao thông tin không liên quan và việc thỏa mãn ràng buộc kém trên toàn bộ. Khả năng có sẵn ngữ cảnh giảm thiểu tỷ lệ thông tin không liên quan, nhưng việc thỏa mãn ràng buộc vẫn còn thấp. Tính đúng đắn hoàn toàn (tức là khớp hoàn hảo của đầu ra mô hình sau xử lý và sự thật cơ sở) rất thấp trên tất cả các điều kiện và mô hình nhưng có sự cải thiện rõ ràng cho VỚI-NGỮ-CẢNH. Kết quả tương tự cho các truy vấn có hai ràng buộc sách được hiển thị trong Phụ lục, Bảng 5.

trong đó các ràng buộc được biểu đạt bằng ngôn ngữ tự nhiên. Ví dụ, truy vấn "Danh sách các bài báo nghiên cứu được viết bởi {tác giả} xuất bản sau {năm}", tự nhiên chỉ định ít nhất hai ràng buộc về đầu ra yêu cầu. Trong khi sự đa dạng của các loại ràng buộc qua các yêu cầu người dùng trong một công cụ tìm kiếm được hỗ trợ bởi LLM có thể lớn và một số ràng buộc có thể khó phân tích và xác minh hơn, về cơ bản, nhiều tương tác người dùng thuộc định nghĩa này, đặc biệt trong các tình huống mà người dùng tìm kiếm thông tin cụ thể và chính xác thay vì văn bản sáng tạo mở.

Trong khi có một số tiêu chuẩn và báo cáo để đánh giá tính đúng đắn thực tế trên các truy vấn đơn giản với các ràng buộc đơn và mong đợi một mục đầu ra duy nhất (ví dụ: "Thành phố nào là thủ đô của Ukraine") (Lin et al., 2021; Elazar et al., 2021; Kwiatkowski et al., 2019; Zellers et al., 2019), nhiều trong số các tiêu chuẩn này đã bão hòa và ít hiểu biết về hiệu suất của các LLM trên các truy vấn phức tạp hơn với một số loại ràng buộc và tạo ra đầu ra dài hơn. Việc duy trì tính nhất quán với các ràng buộc trên một văn bản được tạo dài hơn là quan trọng để nghiên cứu vì đây là một yếu tố phân biệt chính giữa các kiến trúc trước đây và mới hơn (Chang et al., 2023), thể hiện tính tự nhất quán tốt hơn.

Đáng ngạc nhiên, như chúng tôi sẽ thể hiện trong phân tích này, việc duy trì tính nhất quán với các ràng buộc bên ngoài vẫn là thách thức ngay cả đối với các LLM tiên tiến (GPT4 và GPT3.5) được đào tạo trên dữ liệu quy mô internet (xem Bảng 1). Để hiểu rõ hơn về cách thức và thời điểm các thất bại này xảy ra, chúng tôi đóng góp KITAB, một bộ dữ liệu và phương pháp thu thập dữ liệu động tập trung vào các truy vấn văn học, như một ví dụ cổ điển của một lĩnh vực có thể hưởng lợi từ việc truy xuất hiệu quả và có đủ thông tin công khai có thể cũng được sử dụng trong quá trình đào tạo (ví dụ: trên Wikipedia). Các truy vấn KITAB có dạng: "Danh sách tất cả sách từ Toni Morrison xuất bản giữa 1970-1980?", trong đó ràng buộc đầu tiên được cố định cho một tác giả và những ràng buộc tiếp theo có thể thay đổi giữa các ràng buộc từ vựng, thời gian và thực thể có tên.

Chúng tôi sử dụng KITAB để kiểm tra các LLM qua các điều kiện được kiểm soát khác nhau: i) khả năng cơ bản của chúng trong việc truy xuất tất cả sách từ một tác giả (TẤT-CẢ-SÁCH), ii) hiệu suất trên các truy vấn có cả ràng buộc tác giả và ràng buộc sách chỉ sử dụng kiến thức của LLM (KHÔNG-NGỮ-CẢNH), iii) hiệu suất khi LLM có quyền truy cập vào một ngữ cảnh hoàn chỉnh với tất cả sách từ tác giả, để phân biệt giữa các cài đặt tham số và truy xuất tăng cường (VỚI-NGỮ-CẢNH), và cuối cùng iv) hiệu suất cho các lời nhắc chuỗi suy nghĩ tiêu chuẩn và các lời nhắc yêu cầu LLM trước tiên xây dựng ngữ cảnh riêng của nó với tất cả sách từ tác giả, như một phương pháp truy xuất tự cung cấp không sử dụng các hệ thống khác (TỰ-NGỮ-CẢNH). Những điều kiện này cho phép chúng tôi đặc trưng hóa và tách biệt cẩn thận các chế độ thất bại cho nhiệm vụ, và rút ra những hiểu biết như sau:

• Chỉ sử dụng kiến thức tham số của chúng, các LLM tiên tiến có tỷ lệ cao thể hiện sách không liên quan (có thể ảo giác), không được viết từ tác giả nhất định, thay đổi giữa 12% và 41%. Thông tin không liên quan tăng đột ngột đối với các tác giả có độ phổ biến thấp hơn.

• Khả năng có sẵn ngữ cảnh hoàn chỉnh giải quyết sự không liên quan, nhưng việc thất bại trong thỏa mãn ràng buộc vẫn là một trở ngại chính qua cả hai LLM và các loại ràng buộc khác nhau, ngay cả với ngữ cảnh hoàn chỉnh.

• Các phương pháp tự truy xuất tăng đáng kể tỷ lệ thông tin không liên quan (có thể ảo giác) và các tiêu đề được bịa đặt không từ tác giả, vì mục đích thỏa mãn ràng buộc.

• Trong khi GPT4 cải thiện tất cả điểm số khi so sánh với GPT3.5, sự khác biệt giữa hai LLM không quá ấn tượng, cho thấy rằng quy mô một mình có thể không giải quyết các vấn đề lọc với ràng buộc. Tất cả tính đúng đắn (tức là khớp hoàn hảo với sự thật cơ sở) vẫn thấp đáng kể dưới 35%.

Bên cạnh bộ dữ liệu và một báo cáo chi tiết về GPT4 và GPT3.5, công trình này cũng đóng góp một phương pháp để thu thập và làm sạch các phiên bản khác của KITAB sử dụng cùng quy trình nhưng trên một danh sách tác giả rời rạc. Quy trình này có thể có tầm quan trọng đáng kể để đối đầu với bão hòa hoặc rò rỉ tiêu chuẩn, và để hỗ trợ kiểm tra độc lập trong các tình huống khi bộ dữ liệu ban đầu có thể được sử dụng trong đào tạo.

--- TRANG 3 ---
Bản thảo

2 BỐI CẢNH & CÔNG TRÌNH LIÊN QUAN

Truy vấn Thực tế: Hầu hết công trình trước đây tập trung vào việc xác định các sự thật cụ thể trong các tham số của LLM (Meng et al., 2022; Geva et al., 2023; Mallen et al., 2022), hoặc hiểu cách hiệu suất của LLM trong những nhiệm vụ này có thể được cải thiện (Chuang et al., 2023). Trong khi những công trình này gián tiếp đánh giá khả năng phản hồi chính xác của LLM đối với các truy vấn thực tế, chúng chủ yếu tập trung vào các phản hồi ngắn, sử dụng các bộ dữ liệu đã bão hòa (tức là với hiệu suất SOTA khá cao), hoặc tệ hơn - bị ô nhiễm. Ví dụ, Nori et al. (2023) lưu ý rằng GPT4 có thể tái tạo các câu hỏi từ SQuAD 2.0 (Rajpurkar et al., 2018) từng chữ, trong khi OpenAI (2023) lưu ý sự ô nhiễm cho MMLU (Hendrycks et al., 2020), và Sun et al. (2023b) nhấn mạnh cách GPT4 đạt được kết quả tiên tiến cho BEIR (Thakur et al., 2021).

Một giải pháp đầy hứa hẹn cho việc thất bại trong tìm kiếm sự thật và ảo giác là kết hợp việc tạo với các cơ chế truy xuất như được thực hiện trong tạo tăng cường truy xuất (RAG) (Nakano et al., 2021; Lewis et al., 2020)). Như chúng tôi thảo luận trong § 3.2, chúng tôi mô phỏng cài đặt này bằng cách cung cấp thông tin hoàn chỉnh mong muốn trong ngữ cảnh và sau đó đánh giá LLM trong khả năng phản hồi các truy vấn thực tế. Trong thực tế, việc truy xuất trước trong RAG có thể gây ra những thách thức mới qua nhiều lĩnh vực, đặc biệt khi công cụ truy xuất không đáng tin cậy hoặc đắt đỏ.

Thỏa mãn Ràng buộc: Như đã thảo luận bởi Yuksekgonul et al. (2023), nhiều truy vấn (và nhiệm vụ) có thể được xem qua lăng kính thỏa mãn ràng buộc. Sử dụng cùng lăng kính này cung cấp cho chúng ta một khung tự nhiên để tạo ra các truy vấn với các khái niệm phức tạp khác nhau, tức là bằng cách thay đổi các ràng buộc. Sự khác biệt chính giữa nghiên cứu này và công trình của Yuksekgonul et al. (2023), là ở đây chúng tôi đóng góp một bộ dữ liệu (và đánh giá chức năng) thách thức ngay cả đối với các mô hình độc quyền lớn như GPT4, trong khi Yuksekgonul et al. (2023) đề xuất một phương pháp dựa trên chú ý để hiểu cơ chế và phát hiện thất bại của các mô hình mã nguồn mở sử dụng nội bộ mô hình. Rộng hơn, các nhiệm vụ khác có thể được xem như các bài toán thỏa mãn ràng buộc bao gồm lập kế hoạch (Valmeekam et al., 2022), điều chỉnh hướng dẫn (Zhou et al., 2023), và tạo được kiểm soát (Zheng et al., 2023).

Phức tạp Ràng buộc và Truy vấn: Một cách đo lường độ phức tạp truy vấn là sử dụng khái niệm ràng buộc (Meseguer, 1989; Gent et al., 1996), xem điều này như một hàm của số lượng giải pháp cho một ràng buộc nhất định. Theo tinh thần tương tự, chúng tôi đo lường phần bù của tỷ lệ giữa số lượng giải pháp S thỏa mãn ràng buộc và tổng số mục trong miền N (ràng buộc cao hơn, phức tạp hơn), tức là κ = 1 - S/N. Ràng buộc cũng có thể được xem như đối lập của tính chọn lọc truy vấn trong các hệ thống cơ sở dữ liệu (Getoor et al., 2001), tức là tỷ lệ phần trăm bản ghi thỏa mãn truy vấn. Độ phổ biến ràng buộc đo lường độ phổ biến của các thực thể trong các ràng buộc cụ thể (phổ biến hơn, ít phức tạp hơn). Lý tưởng nhất, độ phổ biến sẽ đo lường trực tiếp tần suất thông tin trong dữ liệu đào tạo. Trong trường hợp thiếu thông tin như vậy, chúng tôi sử dụng số lượng liên kết trang web trong trang WikiData của tác giả. Trong nhiều bài toán thế giới mở, không thể tính toán trực tiếp độ phổ biến hoặc ràng buộc, đó là lý do tại sao chúng tôi cung cấp thông tin này trong KITAB.

3 PHƯƠNG PHÁP

Các Câu hỏi Nghiên cứu. Dù người dùng có thể tìm kiếm các sự thật kiến thức tổng quát (ví dụ: "Vaccine nào cần tiêm ở bốn tuổi?") hoặc sử dụng các LLM để nghiên cứu và thu thập thông tin về một chủ đề (ví dụ: "Danh sách tất cả tác giả từ châu Phi đã giành giải Nobel?"), việc thất bại trong thỏa mãn các ràng buộc nhất định và lỗi thực tế có thể dẫn đến thiếu tin tưởng, thất vọng và lo ngại an toàn (ví dụ: lời khuyên y tế). Mục tiêu của chúng tôi là phân tích hiệu suất mô hình và tạo ra sự minh bạch xung quanh thời điểm và cách thức các LLM hiện tại thất bại trên các truy vấn có ràng buộc. Để hướng dẫn thiết kế bộ dữ liệu và thí nghiệm, chúng tôi tập trung vào các câu hỏi nghiên cứu sau:

RQ1: Hiệu suất mô hình thay đổi như thế nào tùy thuộc vào loại ràng buộc?
RQ2: Hiệu suất mô hình thay đổi như thế nào nếu thông tin hoàn chỉnh được cung cấp trong ngữ cảnh?
RQ3: Hiệu suất mô hình thay đổi như thế nào tùy thuộc vào độ phổ biến nội dung và ràng buộc?
RQ4: Những nút thắt chính trong các truy vấn thỏa mãn ràng buộc trong IR đối với các LLM hiện tại là gì?

Để trả lời những câu hỏi này, chúng tôi đã thiết kế bộ dữ liệu KITAB. KITAB chứa các truy vấn với số lượng ràng buộc khác nhau từ các tác giả có độ phổ biến khác nhau. Có sự đa dạng cao trong (i) loại ràng buộc, (ii) số lượng giải pháp ứng cử (tức là ràng buộc), và (iii) độ phổ biến tác giả

--- TRANG 4 ---
Bản thảo

[Biểu đồ 1: Độ phổ biến tác giả cho một ràng buộc sách. Một ràng buộc sách Hai ràng buộc sách]

[Bảng 2: Thống kê KITAB về tần suất ràng buộc và ràng buộc trung bình. Các truy vấn ràng buộc sách hai có hơn một loại ràng buộc.]

(tức là một đại diện cho tần suất trong bộ dữ liệu). Hình 1 và Bảng 2 tóm tắt các thống kê dữ liệu chính. Thông tin chi tiết hơn cũng có sẵn trong Phụ lục, Hình 5 và 6.

3.1 THU THẬP DỮ LIỆU KITAB

Lấy mẫu tác giả. Để khởi tạo việc thu thập dữ liệu, trước tiên chúng tôi lấy mẫu 20.000 tác giả (tức là các thực thể được đánh dấu là nhà văn) ngẫu nhiên từ WikiData, như một nguồn dữ liệu công khai có thể đã được sử dụng trong thời gian đào tạo cho một số mô hình (Gao et al., 2020). Để tránh dữ liệu có thể không chính xác và các ngoại lệ cực đoan, chúng tôi lọc ra các tác giả sinh trước năm 1850 và những người có ít hơn 10 hoặc nhiều hơn 300 tác phẩm được liên kết với hồ sơ của họ, dẫn đến 1505 tác giả. Tiếp theo, chúng tôi đối chiếu những tác giả này với kho lưu trữ Open Library sử dụng tên tác giả và năm sinh, và chỉ giữ lại những người có ít nhất năm tác phẩm trong Open Library (sau khi làm sạch và loại bỏ trùng lặp sách), dẫn đến 599 tác giả. Những lựa chọn lọc này đảm bảo rằng mẫu cuối cùng chứa một phân phối tự nhiên nhưng hữu ích về độ phổ biến tác giả để có thể xây dựng các truy vấn có thể thỏa mãn, vì công trình trước đây (Carlini et al., 2022; Biderman et al., 2023; Yuksekgonul et al., 2023; Mallen et al., 2022) đã xác định độ phổ biến như một yếu tố chính đối với lỗi thực tế. Trong khi Mallen et al. (2022) đo lường độ phổ biến thông qua số lượt truy cập trang, Shokouhi (2011) đã chứng minh rằng lượt truy cập trang có tính thời vụ và có thể vẽ ra một bức tranh sai về độ phổ biến. Do đó, tương tự như Yuksekgonul et al. (2023), chúng tôi sẽ sử dụng số lượng liên kết trang web trong WikiData như một đại diện cho độ phổ biến thông tin.

Hình 1 hiển thị phân phối số lượng sitelink trong WikiData (như một đại diện cho độ phổ biến) qua toàn bộ mẫu, bao gồm một nhóm kiểm soát bổ sung gồm 12 tác giả nổi tiếng được chọn thủ công từ năm châu lục. Nhóm kiểm soát được sử dụng cho các kiểm tra chất lượng lặp đi lặp lại trên quy trình làm sạch dữ liệu được mô tả tiếp theo. Mẫu cuối cùng chứa 611 tác giả.

Thu thập sách. Sử dụng tên tác giả và năm sinh của họ, chúng tôi đối chiếu kho văn thư Open Library và thu thập tất cả sách từ tác giả được gắn thẻ là tiếng Anh bởi API, hoặc nơi trường ngôn ngữ trống. Sau đó, chúng tôi thực hiện kiểm tra bổ sung sử dụng API Phát hiện Ngôn ngữ của Azure Cognitive Services để chúng tôi chỉ giữ lại các tiêu đề phiên bản tiếng Anh sớm nhất, vì các lời nhắc của chúng tôi cũng bằng tiếng Anh. Hơn nữa, quy trình làm sạch dữ liệu bao gồm một số kiểm tra chất lượng và tính nhất quán, cụ thể là về loại bỏ trùng lặp và kiểm tra chéo quyền tác giả và năm xuất bản của cuốn sách trên cả Open Library và WikiData. Chúng tôi cũng giữ các biến thể của cùng một tiêu đề để tạo điều kiện đánh giá mô hình khi cùng một cuốn sách có thể được biết đến với các tiêu đề và dòng tác giả hơi khác nhau (ví dụ: "Gödel, Escher, Bach" vs. "Gödel, Escher, Bach: An Eternal Golden Braid").

Mặc dù nỗ lực tốt nhất của chúng tôi trong việc thu thập một tập hợp sách hoàn chỉnh và chính xác, chúng tôi cũng đối mặt với nhiều thách thức trong truy xuất và làm sạch, mà chúng tôi mô tả thêm trong Phụ lục C.1. Để ước tính mức độ mà các vấn đề làm sạch dữ liệu có thể có thể ảnh hưởng đến chất lượng dữ liệu của KITAB và đánh giá thêm, chúng tôi cũng đã thực hiện một bài tập chú thích dữ liệu thủ công trong đó chúng tôi tìm kiếm trên web các tiêu đề được cung cấp bởi GPT4 và GPT3.5 nhưng được đánh dấu là không từ tác giả trong bộ dữ liệu của chúng tôi. Tóm lại, chúng tôi thấy rằng dựa trên chú thích thủ công của một mẫu phụ các truy vấn, ít hơn 5% các truy vấn đến GPT4 và ít hơn 6% các truy vấn đến GPT3.5 có thể bị ảnh hưởng bởi các trường hợp mà mô hình tìm thấy một tiêu đề sách không có trong KITAB và do đó sẽ được đánh dấu là không từ tác giả trong quá trình đánh giá của chúng tôi. Trong khi điều này có thể được khắc phục bằng cách sử dụng các nguồn dữ liệu khác, tác động của thông tin bị thiếu đối với so sánh mô hình là nhỏ.

Cùng với sách, KITAB cũng cung cấp nhiều siêu dữ liệu sách để kích hoạt các hàm xác minh cho việc thỏa mãn ràng buộc, bao gồm: năm xuất bản, danh sách tên người hoặc tên thành phố trong tiêu đề (nếu có). Nhận dạng thực thể cho tên người được thực hiện bằng cách sử dụng cả Azure Cognitive Services và GPT4 (Mẫu 4 trong Phụ lục D), vì chúng tôi thấy hai phương pháp bổ sung cho việc phát hiện tên từ các nền văn hóa khác nhau. Đối với tên thành phố, chúng tôi sử dụng Azure Cognitive Services cùng với Geonames, một cơ sở dữ liệu các thành phố có hơn 1000 cư dân (Opendatasoft, 2023).

Ràng buộc và truy vấn. Tất cả các truy vấn trong KITAB có dạng sau:
Liệt kê tất cả sách được viết bởi Toni Morrison (sinh năm 1931) [ràng buộc tác giả] đã được xuất bản lần đầu giữa 1970-1980 [ràng buộc sách].

Trong mỗi truy vấn, ràng buộc đầu tiên luôn được cố định cho một tác giả và những ràng buộc tiếp theo có thể thay đổi giữa các ràng buộc từ vựng (tiêu đề bắt đầu hoặc kết thúc bằng một chữ cái, số từ trong tiêu đề), thời gian (xuất bản giữa năm bắt đầu và năm kết thúc), và thực thể có tên (tên thành phố hoặc tên người có mặt hoặc không có mặt trong tiêu đề) để kiểm tra các khả năng thỏa mãn ràng buộc khác nhau. Vì có một số lượng lớn các trường hợp ràng buộc tùy thuộc vào tính bản chất của chúng, chúng tôi lấy mẫu phụ từ tập hợp các truy vấn có thể lớn theo cách đảm bảo i) đại diện cân bằng qua các loại ràng buộc, và ii) một loạt các ràng buộc có ràng buộc khác nhau. Chúng tôi cũng thêm các ràng buộc "không thỏa mãn được", không khớp với bất kỳ tiêu đề sách nào trong dữ liệu của chúng tôi, chiếm 7,99% số truy vấn.

Bộ dữ liệu cuối cùng chứa 8239 truy vấn với một ràng buộc sách và 4750 truy vấn với hai ràng buộc sách. Bảng 2 hiển thị cách những truy vấn này được phân phối qua các loại ràng buộc khác nhau. Đối với tất cả các truy vấn ràng buộc kép, cả hai ràng buộc đều có thể thỏa mãn riêng lẻ và được tạo ra bằng cách kết hợp dữ liệu ràng buộc đơn của chúng tôi. Chỉ 0,76% các truy vấn không thể thỏa mãn cùng nhau qua cả hai ràng buộc.

Chi tiết thêm về quy trình lấy mẫu ràng buộc được trình bày trong Phụ lục § C.2.

Để kích hoạt đánh giá mô hình ngoại tuyến, KITAB không chỉ cung cấp siêu dữ liệu sách và các hàm xác minh ràng buộc, mà còn bao gồm một ánh xạ của tất cả sách thỏa mãn mỗi trong số 12.989 truy vấn. Tất cả điều này cung cấp một công cụ thuận tiện cũng cho việc đánh giá đầu ra được tạo bởi LLM, mà chúng tôi chi tiết trong § 4.1. Trong khi cho công trình này chúng tôi tập trung vào lĩnh vực văn học, thiết kế quy trình có thể chứng minh hữu ích cho các lĩnh vực khác (ví dụ: phim, nhà hàng, bài báo nghiên cứu, v.v.).

3.2 CÁC ĐIỀU KIỆN THÍ NGHIỆM

Để trả lời các câu hỏi nghiên cứu được trình bày, chúng tôi đặt ra các điều kiện thí nghiệm sau đây ánh xạ đến các mẫu lời nhắc cụ thể, được chi tiết trong Phụ lục D. Tất cả các mẫu trong danh sách này ngoại trừ Mẫu 1, yêu cầu mô hình cung cấp một lý do ngắn gọn trước đây về lý do tại sao một cuốn sách trong danh sách đầu ra thỏa mãn một ràng buộc nhất định, như một phương pháp chuỗi suy nghĩ tiêu chuẩn.

TẤT-CẢ-SÁCH (Mẫu 1): Liệt kê tất cả sách từ tác giả. Điều kiện này cho phép chúng tôi ước tính một giới hạn trên của hiệu suất mô hình trong việc truy xuất thông tin liên quan cho tất cả các truy vấn, bất kể các ràng buộc khác. Trong kết quả thí nghiệm, chúng tôi sẽ sử dụng khái niệm tỷ lệ sách không từ tác giả như tỷ lệ thông tin không liên quan vì những mục này không liên quan đến truy vấn, bất kể các ràng buộc khác có được thỏa mãn hay không. Điều kiện này sau đó giúp tách biệt cách sự không liên quan thông tin thay đổi giữa các truy vấn không có, một hoặc hai ràng buộc sách bổ sung, cho các cài đặt chỉ sử dụng kiến thức tham số của mô hình.

KHÔNG-NGỮ-CẢNH (Mẫu 2a): Liệt kê tất cả sách từ tác giả cũng thỏa mãn các ràng buộc sách khác. Cùng một mẫu được sử dụng để kiểm tra hai ràng buộc sách. Điều kiện này sẽ đo lường hiệu suất mô hình trong việc thỏa mãn các loại ràng buộc khác nhau, chỉ sử dụng kiến thức tham số của mô hình.

VỚI-NGỮ-CẢNH (Mẫu 2b): Đầu tiên, cung cấp một danh sách đầy đủ các sách từ tác giả như ngữ cảnh đầu vào cho mô hình. Sau đó, yêu cầu mô hình liệt kê tất cả sách từ tác giả cũng thỏa mãn ràng buộc sách khác. Cùng một mẫu được sử dụng để kiểm tra hai ràng buộc sách. Điều kiện này có ý định mô phỏng các cài đặt tăng cường truy xuất Nakano et al. (2021); Lewis et al. (2020) nơi phần truy xuất của hệ thống có thể cung cấp một ngữ cảnh hoàn chỉnh cho mô hình và nhiệm vụ của mô hình sau đó chỉ là chạy và xác minh các ràng buộc. Trong khi ngữ cảnh được truy xuất có thể thường cũng không đầy đủ trong thực tế, ở đây chúng tôi cung cấp danh sách tất cả sách từ tác giả được biết đến KITAB để cô lập các thất bại có thể chỉ đối với thiếu sót của mô hình trong việc xác minh ràng buộc. Lưu ý rằng một số ràng buộc (nhưng không phải tất cả) cũng có thể được giải quyết thông qua các ngôn ngữ khai báo (tức là SQL) nếu ngữ cảnh đầu vào được cấu trúc hoặc thậm chí có thể yêu cầu mô hình viết mã để xác minh ràng buộc. Tuy nhiên, với bản chất rộng hơn của các truy vấn của chúng tôi và thực tế là ngữ cảnh đầu vào liên quan thường không được cấu trúc, ở đây chúng tôi quan tâm đến việc kiểm tra khả năng bản địa của mô hình để xác minh các ràng buộc cơ bản.

--- TRANG 5 ---
Bản thảo

TỰ-NGỮ-CẢNH (Mẫu 3): Yêu cầu mô hình trước tiên tự truy xuất tất cả sách từ tác giả, và sau đó sử dụng danh sách đó để tìm những sách cũng thỏa mãn ràng buộc sách. Điều này kiểm tra liệu mô hình có thể mô phỏng một cài đặt truy xuất tự cung cấp, như một phương pháp chuỗi suy nghĩ tiên tiến hơn.

MỤC-ĐƠN (Mẫu 4): Yêu cầu mô hình áp dụng một ràng buộc trên một tiêu đề sách duy nhất để tách biệt hiệu suất của mô hình trong việc áp dụng ràng buộc trên một mục duy nhất khỏi việc áp dụng ràng buộc trên toàn bộ danh sách. Ở đây, chúng tôi lấy mẫu 400 truy vấn sử dụng một cuốn sách duy nhất như được mô tả trong Phụ lục § C.2.

4 THÍ NGHIỆM

Chúng tôi đánh giá hiệu suất của GPT4 và GPT3.5 trên bộ dữ liệu của chúng tôi, với các mẫu lời nhắc và độ dài token tối đa như được định nghĩa trong Phần 3.2. Tất cả thí nghiệm được thực hiện với nhiệt độ 0.

4.1 CÁC CHỈ SỐ VÀ ĐÁNH GIÁ

Nguyên tắc hướng dẫn cho việc thiết kế các chỉ số được sử dụng trong đánh giá này là phải khoan dung nhất có thể với mô hình trong khi vẫn có thể đo lường các xu hướng tích cực và tiêu cực quan trọng. Trong các đánh giá sớm, chúng tôi thấy rằng các câu trả lời của mô hình có thể khác nhau một chút so với câu trả lời sự thật cơ sở, ví dụ: bằng cách bỏ qua một dòng tác giả trong tiêu đề, xuất ra các biến thể của tiêu đề, hoặc lặp lại một tiêu đề. Để đảm bảo những yếu tố này không làm giảm hiệu suất mô hình một cách giả tạo, chúng tôi thiết kế các chỉ số của mình để phù hợp với những khớp một phần và/hoặc mờ như vậy. Đối với các ràng buộc đếm, chúng tôi cũng coi các tiêu đề có nhiều hơn hoặc ít hơn một từ so với ràng buộc được chỉ định là thỏa mãn, để thêm khoan dung hơn cho đánh giá.

Đáng ngạc nhiên, ngay cả với tất cả sự khoan dung này, các mô hình SOTA vẫn hoạt động kém trên KITAB.

Tính toán sự không liên quan thông tin và thỏa mãn một phần. Đối với mỗi truy vấn và câu trả lời mà mô hình cung cấp, chúng tôi tính toán phần sách không liên quan, cũng như phần các câu trả lời thỏa mãn và không thỏa mãn, theo cách chứa các tiêu đề lặp lại, tiêu đề một phần và khớp mờ. Chúng tôi thực hiện như sau. Đầu tiên, chúng tôi xử lý danh sách câu trả lời cuối cùng từ mô hình thành một tập hợp n chuỗi K={k₁, ..., kₙ}. Đối với mỗi kᵢ, chúng tôi kiểm tra xem có tồn tại một cuốn sách trong tập hợp sự thật cơ sở của các sách bởi tác giả đó là một khớp tập con chuỗi cho kᵢ (theo cả hai hướng), hoặc nếu bất kỳ cuốn sách nào trong sự thật cơ sở có khớp 80% trong khoảng cách Levenshtein. Nếu nó vượt qua một trong những kiểm tra này, chúng tôi liên kết nó với giải pháp sự thật cơ sở đó. Nếu không, chúng tôi đánh dấu cuốn sách là không liên quan (tức là không từ tác giả). Sau đó chúng tôi nhóm tất cả các chuỗi khớp với cùng một sự thật cơ sở thành một cụm duy nhất.

Quá trình này tạo ra một phân vùng của K thành m cụm C₁, ..., Cₘ trong đó mỗi cụm hoặc là kích thước 1, chứa một cuốn sách không liên quan duy nhất (tức là một cuốn sách không được viết bởi tác giả), hoặc một cụm nơi tất cả sách được ánh xạ đến cùng một cuốn sách sự thật cơ sở. Chúng tôi gọi cụm trước là tập hợp các cụm không liên quan, và cụm sau là các cụm liên quan. Sau đó chúng tôi chia nhỏ hơn nữa các cụm liên quan thành hai loại. Chúng tôi nói rằng một cụm liên quan là một cụm thỏa mãn nếu bất kỳ chuỗi nào trong cụm thỏa mãn ràng buộc, và nếu không chúng tôi nói đó là một cụm không thỏa mãn. Lưu ý rằng có chủ ý, chúng tôi không đặt tên cho các cụm không liên quan như ảo giác vì có thể là trường hợp một cuốn sách được truy xuất bởi LLM tồn tại nhưng không từ tác giả. Điều này khó kiểm tra hơn vì nó yêu cầu quyền truy cập vào toàn bộ tập hợp sách từng được viết, mặc dù về chất lượng chúng tôi thấy một số trường hợp với nhiều tiêu đề thậm chí không xuất hiện trong tìm kiếm web và có thể không tồn tại.

Với những định nghĩa này, bây giờ chúng tôi có thể định nghĩa các chỉ số của mình. Đối với mỗi truy vấn, chúng tôi báo cáo phần của các cụm không liên quan, thỏa mãn và không thỏa mãn. Chúng tôi ký hiệu ba đại lượng này bằng pᵢᵣᵣ, pₛₐₜ và pᵤₙₛₐₜ, tương ứng. Theo định nghĩa, pᵢᵣᵣ + pₛₐₜ + pᵤₙₛₐₜ = 1. Chúng tôi nhấn mạnh rằng đây là những thuật ngữ rất khoan dung cho mô hình, và kết quả là, rất có thể chúng tôi đang đánh giá quá cao hiệu suất mô hình thực. Tuy nhiên, chúng tôi tin rằng điều này làm cho phát hiện định tính của chúng tôi rằng các mô hình SOTA vẫn gặp khó khăn trong nhiệm vụ này trở nên thú vị hơn.

Tính toán sự đầy đủ và tất cả-đúng. Chúng tôi cũng muốn đánh giá phần câu trả lời đúng mà mô hình trả về, tức là sự đầy đủ của nó. Đối với mỗi truy vấn, chúng tôi định nghĩa sự đầy đủ của câu trả lời của mô hình như sau. Đối với mỗi cuốn sách trong sự thật cơ sở, chúng tôi kiểm tra xem nó có khớp gần đúng với một cuốn sách bởi mô hình, sử dụng cùng phương pháp như trên (tức là khớp tập con và khớp mờ). Sau đó chúng tôi định nghĩa sự đầy đủ của câu trả lời của mô hình, ký hiệu pₒₘₚ, là phần các câu trả lời sự thật cơ sở có khớp gần đúng như vậy. Cuối cùng, chúng tôi nói rằng câu trả lời của mô hình là tất cả đúng nếu pₛₐₜ = 1 và pₒₘₚ = 1. Đây là chỉ số đánh giá nghiêm ngặt nhất đo lường liệu mô hình có mắc lỗi thực tế nào cho truy vấn và tìm thấy tất cả thông tin liên quan.

--- TRANG 6 ---
Bản thảo

[Bảng 3: Hiệu suất GPT4 trên KITAB cho KHÔNG-NGỮ-CẢNH|TỰ-NGỮ-CẢNH|NGỮ-CẢNH qua các loại ràng buộc khác nhau cho các truy vấn có một ràng buộc sách. Kết quả cho GPT3.5 được hiển thị trong Phụ lục, Bảng 4. Các đánh giá tương tự cho các truy vấn có hai ràng buộc sách được trình bày trong Phụ lục, Bảng 6 và 7, tương ứng.]

4.2 KẾT QUẢ

Kết quả tổng thể. Chúng tôi trình bày các thống kê tổng thể được tính trung bình trên toàn bộ bộ dữ liệu trong Bảng 1. Đối với mỗi chỉ số, kết quả được hiển thị cho các điều kiện KHÔNG-NGỮ-CẢNH|TỰ-NGỮ-CẢNH|VỚI-NGỮ-CẢNH theo thứ tự. Nhìn chung, GPT4 hoạt động khá kém trên bộ dữ liệu này, và mặc dù nó hoạt động tốt hơn GPT3.5, sự khác biệt không quá ấn tượng, cho thấy rằng việc cải thiện các nhiệm vụ thỏa mãn ràng buộc có thể không chỉ đến từ việc mở rộng quy mô. Trong khi chuỗi suy nghĩ giúp cải thiện độ chính xác, nó dường như không đủ tự nó, xem Phụ lục F (Ví dụ 1), và thực tế, chuỗi suy nghĩ tiên tiến (được đo bằng TỰ-NGỮ-CẢNH) tăng tỷ lệ xuất hiện sách không liên quan. Chúng tôi cũng quan sát thấy rằng trong khi tỷ lệ xuất hiện sách không liên quan trở nên không đáng kể khi ngữ cảnh được cung cấp (VỚI-NGỮ-CẢNH), điều này không giải quyết các vấn đề với thỏa mãn ràng buộc, sự đầy đủ và tất cả tính đúng đắn, xem Phụ lục F (Ví dụ 2). Hiệu suất mô hình vẫn không đáng tin cậy ngay cả với ngữ cảnh hoàn chỉnh được cung cấp từ KITAB, mô phỏng các cài đặt hỗ trợ tìm kiếm.

Chúng tôi cũng chia nhỏ hiệu suất theo loại truy vấn trong Bảng 3 cho GPT4 và Phụ lục, Bảng 4 cho GPT3.5. Chúng tôi tìm thấy những biến thể thú vị giữa các loại truy vấn. GPT4 gặp khó khăn nhiều hơn với các truy vấn kết thúc-bằng so với các truy vấn bắt đầu-bằng. Khác với ràng buộc bắt đầu-bằng, để mô hình thỏa mãn những ràng buộc kết thúc-bằng, nó phải lập kế hoạch trước và nhìn vào tương lai của một số thế hệ token có thể dẫn đến một chuỗi kết thúc bằng một chữ cái. Đối với các truy vấn dựa trên thực thể, chúng tôi thấy rằng các truy vấn phủ định (ví dụ: không chứa) dễ thỏa mãn hơn và điều đó được phản ánh trong hiệu suất mô hình. Tuy nhiên, ngay cả trong các loại hoạt động tốt nhất, GPT4 vẫn mắc một phần lỗi không thể bỏ qua.

Độ phổ biến. Tiếp theo chúng tôi xem xét mối tương quan giữa độ phổ biến (được đo bằng sitelink WikiData) và hiệu suất mô hình, trong Hình 2 cho GPT4. Xem Phụ lục, Hình 7b cho GPT3.5. Đáng ngạc nhiên, trong khi thông tin không liên quan giảm với độ phổ biến cao hơn, chúng tôi không thấy mối tương quan tích cực rõ ràng giữa độ phổ biến và các kết quả mong muốn như thỏa mãn, sự đầy đủ và tất cả-đúng. Một lần nữa, kết quả này cho thấy rằng thỏa mãn ràng buộc vẫn là một nhiệm vụ khó giải quyết chỉ với dữ liệu lớn hơn (tức là độ phổ biến cao hơn). Một quan sát thú vị và, theo hiểu biết của chúng tôi, mới lạ là dường như có một "chuyển pha" tương đối sắc nét trong tỷ lệ xuất hiện sách không liên quan liên quan đến độ phổ biến. Khi số lượng sitelink cho tác giả rất nhỏ, tức là giữa 0-10, sự không liên quan khá cao. Sau đó, tỷ lệ sách không liên quan giảm, nhưng nhanh chóng làm phẳng, và không cải thiện với nhiều sitelink hơn, với bất kỳ ý nghĩa thống kê nào. Chúng tôi đoán rằng điều này là do các "quyết định thực dụng" cần được thực hiện trong thời gian đào tạo; với các mô hình dành tài nguyên ghi nhớ chỉ sau khi thấy tác giả một số lần. Tất nhiên, đây là một cái nhìn đơn giản về quá trình chuyển đổi nhanh quan sát được trong độ phổ biến, và hiện tượng này đáng được nghiên cứu trong tương lai. Quan trọng là, tất cả tính đúng đắn vẫn thấp đáng kinh ngạc qua tất cả điều kiện và các nhóm độ phổ biến (<35%). Phát hiện này có những hàm ý quan trọng đối với độ tin cậy và tính đầy đủ của thông tin, nếu các mô hình được đánh giá trong công trình này được sử dụng như một phần của các hệ thống tự động lớn hơn.

Ràng buộc. Hình 3 hiển thị mối quan hệ giữa ràng buộc (như được định nghĩa trong Phần 2) và hiệu suất mô hình GPT4. Kết quả tương tự được hiển thị cho GPT3.5 trong Phụ lục, Hình 8b. Ở đây, chúng tôi thấy một hiện tượng tinh tế hơn khi kết quả được tổng hợp qua các loại ràng buộc khác nhau, với hiệu suất mô hình giống như một phân phối hình S, gần như song phân, nhất quán cho cả hai mô hình. Điều này dễ quan sát hơn trong Hình 3 cho điều kiện VỚI-NGỮ-CẢNH, đặc biệt cho sự đầy đủ và tất cả-đúng. Để hiểu rõ hơn về động lực, sau đó chúng tôi phân tách cùng các hình nhưng cho mỗi loại ràng buộc trong Phụ lục, Hình 9 và 10. Đầu tiên, chúng tôi thấy rằng trong khi đối với hầu hết các loại ràng buộc, ràng buộc cao hơn liên quan đến hiệu suất mô hình thấp hơn (nhất quán với các phát hiện của Yuksekgonul et al. (2023)), đối với các ràng buộc cụ thể như kết thúc-bằng và tên-thành-phố, điều ngược lại là đúng. Ngoài ra, đối với các ràng buộc thực thể (tên người và tên thành phố), hai dạng (thực thể tồn tại hoặc không tồn tại trong tiêu đề) được đặt ở hai đầu khác nhau của ràng buộc. Điều này cũng có thể được thấy trong Bảng 2 và Hình 6 nơi các truy vấn phủ định được đặt ở đầu thấp hơn của biểu đồ. Do đó, khi tổng hợp, động lực tổng thể có thể giống như một hiệu ứng gần như song phân của ràng buộc đối với hiệu suất. Trong khi chúng tôi không có lời giải thích đầy đủ về lý do tại sao các ràng buộc kết thúc-bằng và tên-thành-phố hoạt động khác nhau, sự biến thể làm nổi bật tầm quan trọng của các bộ dữ liệu được kiểm soát, quy mô lớn như KITAB trong việc đo lường hành vi nổi lên của các LLM ở quy mô.

--- TRANG 7 ---
Bản thảo

[Hình 2: Hiệu suất GPT-4 trên KITAB so sánh KHÔNG-NGỮ-CẢNH (trái), TỰ-NGỮ-CẢNH (giữa) và VỚI-NGỮ-CẢNH (phải) các truy vấn qua các nhóm độ phổ biến khác nhau. Chúng tôi hiển thị xu hướng cho thông tin không liên quan và tỷ lệ không thỏa mãn trong biểu đồ trên; và cho thỏa mãn, hoàn thành và tỷ lệ đúng trong biểu đồ dưới.]

[Hình 3: Hiệu suất GPT-4 trên KITAB cho các truy vấn qua các nhóm ràng buộc khác nhau. Tương tự như Hình 2, chúng tôi so sánh KHÔNG-NGỮ-CẢNH (trái), TỰ-NGỮ-CẢNH (giữa) và VỚI-NGỮ-CẢNH (phải) với thông tin không liên quan và tỷ lệ không thỏa mãn ở trên; và thỏa mãn, sự đầy đủ và tỷ lệ tất cả đúng ở dưới.]

Nhiều ràng buộc. Hình 4 hiển thị hiệu suất mô hình trên các truy vấn chỉ có ràng buộc tác giả so với có thêm một và hai ràng buộc sách. Không ngạc nhiên, hiệu suất mô hình liên tục giảm đối với các truy vấn phức tạp hơn và có ràng buộc hơn với hai ràng buộc sách. Như một cơ sở ngây thơ, chúng tôi cũng so sánh với hiệu suất trên các truy vấn chỉ có ràng buộc tác giả. Trong khi sự đầy đủ và thỏa mãn ràng buộc giảm theo hướng không có ràng buộc sách đến hai ràng buộc sách, thông tin không liên quan theo động lực khác. Đặc biệt, các mô hình dường như bịa đặt nhiều thông tin không liên quan hơn đáng kể khi chúng được yêu cầu liệt kê tất cả sách từ một tác giả. Thực tế, nếu người ta coi toàn bộ tập hợp sách của tất cả tác giả có sẵn trong dữ liệu đào tạo như miền cho các truy vấn TẤT-CẢ-SÁCH, ràng buộc của truy vấn như vậy khi không có ràng buộc khác nào hiện diện là khá cao. Điều này có thể chứng minh rằng việc ước tính cardinality miền để tính toán ràng buộc không đơn giản và một số ràng buộc hàng đầu (tức là tác giả trong trường hợp của chúng tôi) có thể phục vụ như tay cầm điều kiện đối với kích thước miền được sử dụng bởi mô hình. Tuy nhiên, phát hiện này đáng được thí nghiệm trong tương lai để nghiên cứu xem liệu và cách thức điều kiện như vậy xảy ra. Chi tiết thêm về hiệu suất mô hình theo loại ràng buộc cho các truy vấn có hai ràng buộc sách có thể được tìm thấy trong Bảng 6 và 7 cho GPT4 và 3.5.

Phân tích tách biệt thêm. Để hiểu rõ hơn về cách thông tin không liên quan lan truyền ở các giai đoạn khác nhau của các truy vấn của chúng tôi, chúng tôi nghiên cứu điều kiện TỰ-NGỮ-CẢNH chi tiết hơn. Chúng tôi quan sát thấy rằng sự không liên quan cho phần đầu tiên của quá trình chuỗi suy nghĩ khi mô hình xuất ra tất cả sách từ tác giả cao đáng kể, 0,42 cho GPT4 và 0,47 cho GPT3.5. Mặc dù sau khi áp dụng ràng buộc, sự không liên quan giảm xuống 0,33 và 0,44, điều này vẫn còn cao hơn các điều kiện khác vì mô hình không thể phục hồi từ các tiêu đề bịa đặt ban đầu. Về chất lượng, chúng tôi quan sát thấy rằng đôi khi các mô hình thu thập sách không liên quan trong điều kiện TỰ-NGỮ-CẢNH sao cho chúng có thể thỏa mãn ràng buộc sau này (xem Ví dụ 3 và 4 trong Phụ lục F).

Cuối cùng, chúng tôi xem xét hiệu suất mô hình trong việc thỏa mãn ràng buộc cho danh sách MỤC-ĐƠN của sách. Ở đây, chúng tôi đo lường độ chính xác của mô hình trong việc phát hiện liệu một ràng buộc có được thỏa mãn cho một tiêu đề sử dụng cùng lời nhắc như cho VỚI-NGỮ-CẢNH. Độ chính xác mô hình cho MỤC-ĐƠN được hiển thị trong các cột đầu tiên của Bảng 3 và 4. Khi so sánh những chỉ số này với tỷ lệ thỏa mãn từ VỚI-NGỮ-CẢNH, chúng tôi thấy rằng các loại ràng buộc có hai hành vi rất khác nhau nhất quán qua cả hai mô hình. Các ràng buộc như bắt đầu-bằng, kết thúc-bằng và năm xuất bản dễ kiểm tra hơn cho các tiêu đề riêng lẻ so với cho danh sách. Thay vào đó, các ràng buộc thực thể trở nên dễ dàng hơn cho danh sách tiêu đề sách, điều này cộng hưởng với thực tế rằng nhận dạng thực thể được coi là một khả năng cốt lõi của các LLM trên văn bản dài hơn².

5 KẾT LUẬN

Chúng tôi đã trình bày KITAB, một bộ dữ liệu và phương pháp thu thập dữ liệu động để đánh giá khả năng của các mô hình ngôn ngữ lớn trong việc lọc thông tin sử dụng ràng buộc. Bộ dữ liệu cung cấp tính linh hoạt thuận tiện để kiểm soát loại và độ phức tạp của ràng buộc trong các truy vấn mong đợi danh sách đầu ra dài hơn, vượt ra ngoài các sự thật đơn giản. Một phân tích sâu về GPT4 và GPT3.5, hai mô hình tiên tiến được triển khai trong thế giới thực như một phần của các hệ thống tìm kiếm đối thoại, cho thấy rằng mặc dù có những khả năng nổi lên thú vị của các mô hình như vậy trong việc tìm thông tin, những hạn chế quan trọng vẫn còn khi các mô hình bịa đặt thông tin không liên quan khi chỉ sử dụng kiến thức tham số hoặc khi chúng thất bại trong việc thỏa mãn các ràng buộc được chỉ định ngay cả khi được cung cấp ngữ cảnh hoàn chỉnh và liên quan nhất để lọc. Chúng tôi hy vọng rằng bộ dữ liệu và phương pháp mở đường cho các đánh giá nghiêm ngặt và quy mô lớn trong tương lai về các khả năng nổi lên trong các bài toán truy xuất thông tin.

²Chúng tôi loại trừ ràng buộc số-từ khỏi cuộc thảo luận này vì đánh giá VỚI-NGỮ-CẢNH của chúng tôi khoan dung với các câu trả lời dài hơn hoặc ngắn hơn một từ so với ràng buộc nhất định.

--- TRANG 8 ---
Bản thảo

[Hình 4: Hiệu suất mô hình trên các truy vấn chỉ có ràng buộc tác giả so với cộng thêm một ràng buộc sách và cộng thêm hai ràng buộc sách. Kết quả cho các truy vấn có ràng buộc sách dựa trên KHÔNG-NGỮ-CẢNH (Mẫu 2a).]

--- TRANG 9 ---
Bản thảo

TÀI LIỆU THAM KHẢO

Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. Modern information retrieval. ACM press New York, 1999.

Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In FAccT, pp. 610–623, 2021.

Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large language models. arXiv preprint arXiv:2304.11158, 2023.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883, 2023.

Xin Luna Dong and Theodoros Rekatsinas. Data integration and machine learning: A natural synergy. In SIGMOD, pp. 1645–1650, 2018.

Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012–1031, 2021.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

Ian P Gent, Ewan MacIntyre, Patrick Prosser, Toby Walsh, et al. The constrainedness of search. In AAAI/IAAI, Vol. 1, pp. 246–252, 1996.

Lise Getoor, Benjamin Taskar, and Daphne Koller. Selectivity estimation using probabilistic models. In SIGMOD, 2001.

Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. arXiv preprint arXiv:2304.14767, 2023.

Rasmus Hahn, Christian Bizer, Christopher Sahnwaldt, Christian Herta, Scott Robinson, Michaela Bürgle, Holger Düwiger, and Ulrich Scheel. Faceted wikipedia search. In Business Information Systems: 13th International Conference, BIS 2010, Berlin, Germany, May 3-5, 2010. Proceedings 13, pp. 1–11. Springer, 2010.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.

--- TRANG 10 ---
Bản thảo

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459–9474, 2020.

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.

Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 2022.

Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.

Pedro Meseguer. Constraint satisfaction problems: An overview. AI communications, 1989.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.

OpenAI. Gpt-4 technical report. ArXiv, 2023.

Opendatasoft. Geonames, 2023. URL https://public.opendatasoft.com/explore/dataset/geonames-all-cities-with-a-population-1000.

Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.

Milad Shokouhi. Detecting seasonal queries by time-series analysis. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pp. 1171–1172, 2011.

Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs? arXiv preprint arXiv:2308.10168, 2023a.

Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. Is chatgpt good at search? investigating large language models as re-ranking agent. arXiv preprint arXiv:2304.09542, 2023b.

Nandan Thakur, Nils Reimers, Andreas Rückle, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Daniel Tunkelang. Faceted search, volume 5. Morgan & Claypool Publishers, 2009.

Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498, 2022.

Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. Attention satisfies: A constraint-satisfaction lens on factual errors of language models. arXiv preprint, 2023.

--- TRANG 11 ---
Bản thảo

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.

Xin Zheng, Hongyu Lin, Xianpei Han, and Le Sun. Toward unified controllable text generation via regular expression instruction. arXiv preprint arXiv:2309.10447, 2023.

Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. Controlled text generation with natural language instructions. ICML, 2023.

--- TRANG 12 ---
Bản thảo

A THỐNG KÊ KITAB

[Hình 5: Phân phối các truy vấn qua độ phổ biến tác giả được đo bằng số lượng sitelink trên Wikidata, cho các truy vấn có một ràng buộc sách (trái) và hai ràng buộc sách (phải).]

[Hình 6: Phân phối các truy vấn qua ràng buộc tác giả được đo bằng phần bù của tỷ lệ giữa số lượng sách thỏa mãn ràng buộc sách và tổng số sách từ tác giả. Phân phối được hiển thị cho các truy vấn có một ràng buộc sách (trái) và hai ràng buộc sách (phải). Lưu ý rằng hầu hết phân phối trong phạm vi thấp hơn của ràng buộc bị chi phối bởi các ràng buộc yêu cầu không có tên người hoặc không có tên thành phố trong tiêu đề, vốn tự nhiên dễ thỏa mãn hơn.]

B ĐÁNH GIÁ TRÊN CÁC TRUY VẤN CÓ MỘT VÀ HAI RÀNG BUỘC SÁCH

[Bảng 4: Hiệu suất GPT3.5 trên KITAB cho KHÔNG-NGỮ-CẢNH|TỰ-NGỮ-CẢNH|NGỮ-CẢNH qua các loại ràng buộc khác nhau cho các truy vấn có một ràng buộc sách.]

--- TRANG 13 ---
Bản thảo

[Bảng 5: Hiệu suất mô hình tổng hợp trên KITAB cho KHÔNG-NGỮ-CẢNH|NGỮ-CẢNH cho các truy vấn có hai ràng buộc sách.]

[Bảng 6: Hiệu suất GPT4 trên KITAB cho KHÔNG-NGỮ-CẢNH|NGỮ-CẢNH qua các loại ràng buộc khác nhau cho các truy vấn có hai ràng buộc sách. Loại cần xuất hiện ít nhất một lần trong truy vấn để được nhóm dưới một loại ràng buộc. Tỷ lệ thỏa mãn được báo cáo chung cho cả hai ràng buộc (tức là cả hai cần được thỏa mãn).]

[Bảng 7: Hiệu suất GPT3.5 trên KITAB cho KHÔNG-NGỮ-CẢNH|NGỮ-CẢNH qua các loại ràng buộc khác nhau cho các truy vấn có hai ràng buộc sách. Loại cần xuất hiện ít nhất một lần trong truy vấn để được nhóm dưới một loại ràng buộc. Tỷ lệ thỏa mãn được báo cáo chung cho cả hai ràng buộc (tức là cả hai cần được thỏa mãn).]

C CHI TIẾT THÊM VỀ THU THẬP DỮ LIỆU KITAB

C.1 LÀM SẠCH DỮ LIỆU

Ở đây, chúng tôi chi tiết tất cả các bước liên quan đến việc làm sạch dữ liệu của KITAB. Những bước này cũng có sẵn trong thu thập dữ liệu động của chúng tôi cho người dùng tương lai của KITAB, những người có thể tái tạo cùng quy trình cho các mẫu tác giả khác.

Lọc sách. Chúng tôi chỉ truy xuất các sách được gắn thẻ là tiếng Anh bởi Open Library và những sách không có ngôn ngữ được chỉ định, vì tất cả lời nhắc của chúng tôi đều bằng tiếng Anh và yêu cầu mô hình chỉ trả về tiêu đề tiếng Anh. Chúng tôi giữ những sách không có ngôn ngữ được chỉ định để cải thiện tính đầy đủ thu thập sách. Tuy nhiên, chúng tôi vẫn kiểm tra qua API Azure Cognitive Services xem những tiêu đề này có bằng tiếng Anh không trước khi thêm chúng vào danh sách. Tiếp theo, chúng tôi cũng loại bỏ những sách dường như có hơn hai tác giả vì nhiều sách như vậy sẽ không nhất thiết xuất hiện như sách từ một tác giả cụ thể. Trong hầu hết trường hợp, đây là những bộ sưu tập tác phẩm giữa nhiều tác giả (ví dụ: "The Best Short Stories 2022: The O. Henry Prize Winners"). Cuối cùng, chúng tôi cũng tìm thấy một danh sách tiêu đề trong Open Library là một phần của bộ sưu tập tác giả nhưng không được viết bởi tác giả. Để giảm thiểu điều này, chúng tôi đối chiếu với Wikidata cho cùng tiêu đề và đảm bảo rằng tác giả trên Wikidata khớp với tác giả trên OpenLibrary. Bước này thường được sử dụng trong tích hợp dữ liệu cho mục đích làm sạch (Dong & Rekatsinas, 2018) và cải thiện đáng kể chất lượng dữ liệu tổng thể.

Loại bỏ trùng lặp. Để loại bỏ trùng lặp các tiêu đề sách có thể dư thừa, trước tiên chúng tôi chuyển thành chữ thường và loại bỏ tất cả dấu câu khỏi chuỗi tiêu đề và sau đó nhóm các sách được truy xuất qua Open Library bằng cách sử dụng i) khớp mờ³ với ngưỡng 80%, và ii) kiểm tra tập con cho các trường hợp khi tiêu đề sách có thể xuất hiện dưới dạng dài hơn hoặc viết tắt (ví dụ: "Gödel, Escher, Bach" vs. "Gödel, Escher, Bach: An Eternal Golden Braid"). Chúng tôi đặc biệt loại bỏ từ đầu tiên của tiêu đề nếu tiêu đề bắt đầu bằng "The" hoặc "A/An" cho mục đích loại bỏ trùng lặp, và áp dụng cùng chiến lược trong quá trình đánh giá mô hình để chúng tôi không phạt mô hình nếu nó có hoặc không bỏ lỡ một tiêu đề chỉ vì những chi tiết nhỏ này. Trong quá trình loại bỏ trùng lặp, chúng tôi giữ như một năm xuất bản giá trị tối thiểu của các năm xuất bản qua tất cả tiêu đề trong cụm. Lưu ý rằng quá trình loại bỏ trùng lặp cũng ảnh hưởng đến danh sách sách sự thật cơ sở trong KITAB, vì cùng một cuốn sách có thể xuất hiện nhiều lần trên Open Library. Để giảm thiểu điều này, chúng tôi theo dõi các tiêu đề dư thừa của cùng một cuốn sách trong giai đoạn thu thập sách và sau đó sử dụng các tiêu đề dư thừa cũng trong khi tính toán ràng buộc truy vấn và sự đầy đủ. Ví dụ, trong ví dụ ở trên, nếu cả hai biến thể của tiêu đề sách đều có mặt trong sự thật cơ sở Open Library (ví dụ: "Gödel, Escher, Bach" vs. "Gödel, Escher, Bach: An Eternal Golden Braid"), cuốn sách sẽ được hợp nhất thành một và được đánh dấu là thỏa mãn cả hai ràng buộc: "tiêu đề kết thúc bằng chữ h" và "tiêu đề kết thúc bằng chữ d".

Kiểm tra thủ công về chất lượng. Các kiểm tra thủ công bao gồm việc xem xét hai mẫu 200 truy vấn cho mỗi mô hình và đầu ra tương ứng của chúng và kiểm tra các tiêu đề mà mô hình đã đề cập nhưng chúng tôi đã đánh dấu chúng là thông tin không liên quan (tức là không từ tác giả). Đối với những tiêu đề này, chúng tôi tìm kiếm trên Web để xem liệu có tồn tại một cuốn sách từ tác giả đó với cùng tiêu đề. Quá trình này xác định rằng 5% và 6% các truy vấn trong mẫu GPT4 và GPT3.5 tương ứng có ít nhất một tiêu đề trong đầu ra mô hình thực sự thuộc về tác giả theo tìm kiếm web nhưng tiêu đề đó không có mặt trên OpenLibrary. Lưu ý rằng tác động của sự biến thể này đối với tỷ lệ thông tin không liên quan thực tế thấp hơn nhiều so với 5% vì nó được đo qua các truy vấn chứ không phải sách riêng lẻ, như một ước tính giới hạn trên.

C.2 LẤY MẪU RÀNG BUỘC VÀ TRUY VẤN

Hơn nữa, chúng tôi mô tả việc xây dựng và lựa chọn các truy vấn thỏa mãn ràng buộc của KITAB và sự thật cơ sở.

Các truy vấn có một ràng buộc sách. Đối với mỗi tác giả, chúng tôi tạo ra các ràng buộc bằng cách liên kết mỗi tiêu đề với các chữ cái bắt đầu và kết thúc, số từ, phạm vi năm xuất bản 5 năm, và liệu tiêu đề có tên người hoặc tên thành phố. Từ việc tích lũy lớn này, sau đó chúng tôi lấy mẫu ngẫu nhiên 15% các ràng buộc cho tiêu đề bắt đầu hoặc kết thúc và 50% cho số từ tiêu đề. Chúng tôi cũng thêm các ràng buộc không thể thỏa mãn của mỗi loại, ví dụ: chúng tôi chọn ngẫu nhiên các chữ cái mà không có tiêu đề sách nào trong tập hợp của chúng tôi bắt đầu. Tập hợp các truy vấn không thể thỏa mãn chiếm 7,99% số truy vấn. Tập hợp ràng buộc sách đơn cuối cùng là 8239 truy vấn.

Các truy vấn có hai ràng buộc sách. Đối với tất cả các truy vấn ràng buộc kép, cả hai ràng buộc đều có thể thỏa mãn riêng lẻ và được tạo ra bằng cách kết hợp dữ liệu ràng buộc đơn của chúng tôi. Chỉ 0,76% các truy vấn không thể thỏa mãn cùng nhau qua cả hai ràng buộc. Để có sự đa dạng về độ khó, chúng tôi tách riêng các ràng buộc dễ thỏa mãn hơn kết hợp tiêu đề bắt đầu bằng và xuất bản trong phạm vi năm ánh xạ đến hơn một cuốn sách và lấy mẫu 25% số sau; trong các ràng buộc kép còn lại, chúng tôi lấy mẫu 5% ánh xạ đến một cuốn sách duy nhất và 10% ánh xạ đến hơn 2 cuốn sách. Tập hợp cuối cùng có 4750 truy vấn.

Các truy vấn MỤC-ĐƠN. Từ dữ liệu ràng buộc sách một của KITAB, chúng tôi chọn ngẫu nhiên 200 truy vấn, lấy mẫu 50 truy vấn mỗi loại cho các ràng buộc tiêu đề bắt đầu và kết thúc bằng, 30 mỗi loại cho xuất bản trong phạm vi và số từ trong tiêu đề, và 20 mỗi loại cho sự hiện diện của tên người hoặc tên thành phố trong tiêu đề. Đối với mỗi truy vấn, chúng tôi chọn ngẫu nhiên một mục sách duy nhất thỏa mãn ràng buộc từ sự thật cơ sở và cung cấp nó như ngữ cảnh. Ngoài ra, đối với mỗi truy vấn ràng buộc có thể thỏa mãn, chúng tôi cũng lấy mẫu một cuốn sách bởi tác giả không có mặt trong sự thật cơ sở như một ngữ cảnh đối lập, dẫn đến tổng cộng 400 truy vấn mục đơn.

³https://github.com/seatgeek/fuzzywuzzy

--- TRANG 14 ---
Bản thảo

D CÁC MẪU LỜI NHẮC

[MẪU 1 TẤT-CẢ-SÁCH]: Liệt kê tất cả sách từ tác giả.
Độ dài token tối đa = 1000
Danh sách tất cả sách được viết bởi {$author} (sinh năm {$birth year}) [ràng buộc tác giả]. Tất cả tiêu đề sách cần bằng tiếng Anh. Luôn kết thúc phản hồi của bạn với định dạng sau, không thêm bất kỳ văn bản hoặc nhận xét bổ sung nào:
Đầu ra:
1. Tiêu đề: <tiêu đề>
2. Tiêu đề: <tiêu đề>
...
N. Tiêu đề: <tiêu đề>

[MẪU 2A KHÔNG-NGỮ-CẢNH]: Liệt kê tất cả sách từ tác giả thỏa mãn các ràng buộc khác, không có ngữ cảnh.
Độ dài token tối đa = 400
Danh sách tất cả sách được viết bởi {$author} (sinh năm {$birth year}) [ràng buộc $author] thỏa mãn tất cả các tiêu chí sau. Tất cả tiêu đề sách cần bằng tiếng Anh. Hãy suy nghĩ từng bước. Đưa ra lý do 1-2 câu cho việc tại sao các sách thỏa mãn tiêu chí. Tiêu chí: {$constraints} [ràng buộc sách] Nhớ rằng mỗi cuốn sách trong danh sách đầu ra cần thỏa mãn tất cả tiêu chí. Luôn kết thúc phản hồi của bạn với định dạng sau. Không thêm bất kỳ văn bản hoặc nhận xét bổ sung nào sau danh sách đầu ra.
Đầu ra:
1. Lý do: <lý do>. Tiêu đề: <tiêu đề>
2. Lý do: <lý do>. Tiêu đề: <tiêu đề>
...
N. Lý do: <lý do>. Tiêu đề: <tiêu đề>

[MẪU 2B VỚI-NGỮ-CẢNH]: Liệt kê tất cả sách từ tác giả thỏa mãn các ràng buộc khác, với ngữ cảnh.
Độ dài token tối đa = 1000 (200 cho điều kiện MỤC-ĐƠN)
Sau đây là danh sách các sách bởi {$author} (sinh năm {$birth year}) [ràng buộc $author] với ngày xuất bản trong ngoặc đơn. Danh sách:
{$all books} [tất cả sách]
Tìm tất cả sách trong danh sách này thỏa mãn tất cả tiêu chí sau. Hãy suy nghĩ từng bước. Đưa ra lý do 1-2 câu cho việc tại sao các sách thỏa mãn tiêu chí. Tiêu chí: {$constraints} [ràng buộc sách] Nhớ rằng mỗi cuốn sách trong danh sách đầu ra cần thỏa mãn tất cả tiêu chí. Luôn kết thúc phản hồi của bạn với định dạng sau. Không thêm bất kỳ văn bản hoặc nhận xét bổ sung nào sau danh sách đầu ra.
Đầu ra:
1. Lý do: <lý do>. Tiêu đề: <tiêu đề>
2. Lý do: <lý do>. Tiêu đề: <tiêu đề>
...
N. Lý do: <lý do>. Tiêu đề: <tiêu đề>

--- TRANG 15 ---
Bản thảo

[MẪU 3 TỰ-NGỮ-CẢNH]: Liệt kê tất cả sách từ tác giả thỏa mãn một ràng buộc khác, tự truy xuất ngữ cảnh.
Độ dài token tối đa = 3000
Danh sách tất cả sách được viết bởi {$author} (sinh năm {$birth year}) [ràng buộc $author] thỏa mãn tất cả tiêu chí sau. Tất cả tiêu đề sách cần bằng tiếng Anh.
Tiêu chí: {$constraints} [ràng buộc sách] Đầu tiên, truy xuất tất cả sách bởi {$author} (sinh năm {$birth year}) và liệt kê chúng trong danh sách "Tất cả Sách". Sau đó, chọn tập con các sách thỏa mãn Ràng buộc 1 và liệt kê chúng dưới danh sách "Đầu ra Cuối cùng". Hãy suy nghĩ từng bước. Đưa ra lý do 1-2 câu cho việc tại sao các sách thỏa mãn tiêu chí. Nhớ rằng mỗi cuốn sách trong danh sách đầu ra cuối cùng cần thỏa mãn tất cả tiêu chí. Luôn kết thúc phản hồi của bạn với định dạng sau. Không thêm bất kỳ văn bản hoặc nhận xét bổ sung nào sau danh sách đầu ra.

Tất cả Sách:
1. Tiêu đề: <tiêu đề>
2. Tiêu đề: <tiêu đề>
...
N. Tiêu đề: <tiêu đề>

Đầu ra Cuối cùng:
1. Lý do: <lý do>. Tiêu đề: <tiêu đề>
2. Lý do: <lý do>. Tiêu đề: <tiêu đề>
...
N. Lý do: <lý do>. Tiêu đề: <tiêu đề>

[MẪU 4 KIỂM-TRA-TÊN]: Tìm tất cả sách chứa tên người trong tiêu đề.
Sau đây là danh sách các sách. Danh sách:
{$all books}
Tìm tất cả sách chứa tên người trong tiêu đề. Luôn kết thúc phản hồi của bạn với định dạng sau. Không thêm bất kỳ văn bản hoặc nhận xét bổ sung nào sau danh sách đầu ra.
Đầu ra:
1. Lý do: <lý do>. Tiêu đề: <tiêu đề>
2. Lý do: <lý do>. Tiêu đề: <tiêu đề>
...
N. Lý do: <lý do>. Tiêu đề: <tiêu đề>

--- TRANG 16 ---
Bản thảo

E KẾT QUẢ BỔ SUNG VỀ MỐI QUAN HỆ HIỆU SUẤT VỚI ĐỘ PHỔ BIẾN VÀ RÀNG BUỘC

E.1 ĐỘ PHỔ BIẾN

[Hình 7: Hiệu suất GPT-4 và GPT-3.5 so với độ phổ biến.]

--- TRANG 17 ---
Bản thảo

E.2 RÀNG BUỘC

[Hình 8: Hiệu suất GPT-4 và GPT-3.5 so với ràng buộc.]

--- TRANG 18 ---
Bản thảo

E.3 PHÂN TÍCH HIỆU SUẤT GPT-4 THEO LOẠI RÀNG BUỘC

[Hình 9: Ràng buộc so với sự không liên quan và không thỏa mãn cho GPT-4 qua các loại ràng buộc.]

--- TRANG 19 ---
Bản thảo

[Hình 10: Ràng buộc so với thỏa mãn, sự đầy đủ và tính đúng đắn cho GPT-4 qua các loại ràng buộc.]

--- TRANG 20 ---
Bản thảo

F VÍ DỤ

[VÍ DỤ 1]: Tỷ lệ không thỏa mãn cao với KHÔNG-NGỮ-CẢNH.
Tác giả: Michael Scott
Ràng buộc: Tiêu đề sách chứa chỉ 4 từ.
Sự thật cơ sở KITAB: ''[A Celtic odyssey', 'Celtic Wisdom for Business', 'Green and golden tales', 'Irish animal tales.', 'Irish folk and fairy tales', 'Irish ghosts and hauntings', 'Irish hero tales', 'Judith and spider', 'Magical Irish folk tales', 'The Children of Lir', 'The Culai heritage', 'The book of Celtic wisdom', 'The last of the fianna', 'The quest of the sons', 'The river gods', 'The seven treasures', 'The thirteen hallows]''

Tất cả sách KITAB ''['A Celtic odyssey (1985)', 'Billy the Kid and the vampyres of Vegas (2011)', 'Celtic Wisdom for Business (2001)', 'Earthlord (1992)', 'Firelord (1994)', 'Gemini game (1993)', 'Green and golden tales (1988)', 'Irish Fairytales (Green and Golden Tales) (1989)', 'Irish animal tales. (1989)', 'Irish folk and fairy tales (1983)', 'Irish ghosts and hauntings (1994)', 'Irish hero tales (1989)', 'Judith and spider (1992)', 'Lottery (1993)', 'Magical Irish folk tales (1995)', 'Mirror image (2016)', 'October moon (1992)', 'Reflection. (1992)', 'Tales from the land of Erin (1985)', 'The Children of Lir (1986)', 'The Culai heritage (2001)', 'The Magician (2008)', 'The alchemyst (2007)', 'The book of Celtic wisdom (2002)', 'The enchantress (2012)', 'The last of the fianna (1987)', 'The necromancer (2010)', 'The quest of the sons (1988)', 'The river gods (1991)', 'The seven treasures (1992)', 'The thirteen hallows (2011)', 'The warlock (2011)', 'Windlord (1991)']''

Sách GPT4: ''['alchemyst the secrets of the immortal nicholas flamel', 'magician the secrets of the immortal nicholas flamel', 'necromancer the secrets of the immortal nicholas flamel', 'warlock the secrets of the immortal nicholas flamel', 'enchantress the secrets of the immortal nicholas flamel']''

Quan sát: Mặc dù mô hình đã thành công tạo ra 5 cuốn sách ánh xạ đến danh sách sách tác giả KITAB, không có tiêu đề nào thỏa mãn ràng buộc số từ.

[VÍ DỤ 2]: Tỷ lệ không thỏa mãn cao còn lại với NGỮ-CẢNH.
Tác giả: Gabriel García Márquez
Ràng buộc: Tiêu đề sách kết thúc bằng chữ a.
Sự thật cơ sở KITAB: ''['Love in the time of cholera', 'The fragrance of guava']''

Tất cả sách KITAB (ngữ cảnh): ''['100 Years of Solitude (1967)', 'Armando Morales (2004)', 'Autumn of the Patriarch (2008)', 'Chronicle of a Death Foretold (1981)', 'Clandestine in Chile (1985)', 'Collected Novellas (1990)', 'Conversations with Gabriel Garcia Marquez (2005)', 'For the sake of a country within reach of the children (1996)', 'Gabriel Garcia Marquez (2015)', 'Granta 31 (1990)', 'Harvard Diss Romance Set (1942)', "I'm Not Here to Give a Speech (2014)", 'In evil hour (1962)', 'Innocent Erendira and Other Stories (1979)', 'Leaf storm (1955)', 'Living to tell the tale (2002)', 'Love in the time of cholera (1985)', 'Memories of My Melancholy Whores (2004)', 'News of a kidnapping (1996)', 'No One Writes to the Colonel (1961)', 'Of love and other demons (1994)', 'One hundred years of solitude (1967)', 'Scandal of the Century (2020)', 'Strange Pilgrims (1992)', 'The Handsomest Drowned Man in the World (1968)', 'The Short story (1987)', 'The fragrance of guava (1982)', 'The general in his labyrinth (1989)', 'The story of a shipwrecked sailor (1970)', 'Three Novellas (1991)', 'Tomas Sanchez (2003)']''

Sách GPT4: ''['in evil hour', 'of love and other demons']''

Quan sát: Mặc dù mô hình thành công theo dõi ngữ cảnh được cung cấp và tất cả sách thực sự từ tác giả, nó thất bại trong việc trích xuất các sách thỏa mãn ràng buộc.

--- TRANG 21 ---
Bản thảo

[VÍ DỤ 3]: TỰ-NGỮ-CẢNH dẫn đến sự không liên quan thông tin cao hơn.
Tác giả: Russell Humke Fitzgibbon
Ràng buộc: Tiêu đề sách bắt đầu bằng chữ v.
Tất cả sách KITAB cho tác giả: ''['A directory of Latin American political parties (1970)', 'Assessing democracy in Latin America (1998)', 'Brazil: a chronology and fact book, 1488-1973 (1974)', 'Cuba and the United States, 1900-1935 (1935)', 'Global politics (1944)', 'Latin America (1946)', 'The constitutions of the Americas, as of January 1, 1948. (1948)', 'Uruguay (1954)', 'Visual outline of Latin American history (1938)']''

Tự ngữ cảnh GPT4: ''['vatican and the kremlin', 'vatican in world politics', 'catholic church and the authoritarian regime in austria', 'catholic church and the totalitarian regime in italy', 'catholic church and the dictatorship in portugal', 'catholic church and the fascist regime in spain']''

Quan sát: Không có cuốn sách nào trong đầu ra mô hình từ tác giả. Mô hình bắt đầu bằng việc tạo ra hai cuốn sách không liên quan thỏa mãn ràng buộc nhưng không từ tác giả. Với ngữ cảnh đó, sau đó nó tiếp tục tự động tạo ra thêm các sách tương tự về các chủ đề tương tự (tôn giáo) nhưng vẫn không liên quan và cũng không thỏa mãn ràng buộc nữa.

[VÍ DỤ 4]: TỰ-NGỮ-CẢNH dẫn đến sự không liên quan thông tin cao hơn.
Tác giả: Johnston McCulley
Ràng buộc: Sách chứa tên người trong tiêu đề.
Tất cả sách KITAB cho tác giả: ''['Alias the thunderbolt (1927)', 'Avenging Twins Collect (2007)', 'Black Grandee (2019)', 'Brand of Silence (2020)', 'Broadway Bab (1919)', 'Bullet law (1960)', 'Canyon of peril (1935)', 'Captain Fly-By-Night (2012)', 'Companions of Jehu (2008)', 'Demon (2007)', "Devil's Doubloons (2019)", 'Don Renegade (2017)', 'Ghost Phone (2014)', 'Iron Horse Town. (1952)', "Johnston McCulley's Zorro (2000)", 'King of Chaos and Other Adventures (2019)', 'Land of Lost Hope (2016)', 'Man Who Changed Rooms (2008)', 'Scarlet Scourge (2007)', 'Slave of Mystery and Other Tales of Suspense from the Pulps (2006)', 'South of the pass (1944)', 'Spider Strain (2017)', 'Tales of Thubway Tham (2005)', 'Texas showdown (1953)', "A white man's chance (1927)", 'The Black Star (1921)', 'The Cougar Kid (1945)', 'The Rollicking rogue (1941)', 'The blocked trail (1932)', 'The caballero. (1947)', 'The mark of Zorro (1924)', 'The masked woman (1920)', 'The range cavalier (1933)', "The rangers' code (1924)", 'Three Mysteries by Johnston Mcculley (2009)', 'Tragedy Trail (2020)', 'Western Megapack (2011)', 'Wild Noreen (2007)', "Women's War (2008)"]''

Tự ngữ cảnh GPT4: ''['mark of zorro', 'bold caballero', 'crimson clown', 'crimson clown again', 'crimson clowns apprehension', 'crimson clowns return', 'crimson clowns success', 'crimson clowns triumph', 'crimson clowns victory', 'crimson clowns willingness', 'crimson clowns yielding', 'crimson clowns zeal', 'crimson clowns zest', 'crimson clowns zestfulness', 'crimson clowns zestiness', 'crimson clowns zestlessness', 'crimson clowns zestlessnesslessness', 'crimson clowns zestlessnesslessnesslessness', 'crimson clowns zestlessnesslessnesslessnesslessness', ....]''

Quan sát: 3 cuốn sách đầu tiên được mô hình trả về cho ngữ cảnh thực tế từ tác giả (và ít nhiều thỏa mãn ràng buộc), tuy nhiên, mô hình tiếp tục tự động tạo ra các biến thể tiêu đề không liên quan và ảo giác từ 'Crimson Clown' trong một mẫu liên tục có vấn đề.
