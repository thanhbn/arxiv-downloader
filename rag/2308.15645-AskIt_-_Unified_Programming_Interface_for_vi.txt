# 2308.15645.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2308.15645.pdf
# Kích thước tệp: 627998 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
AskIt : Giao diện Lập trình Thống nhất cho
Lập trình với Mô hình Ngôn ngữ Lớn
Katsumi Okuda
CSAIL, MIT
Cambridge, USA
okuda@csail.mit.edu
Mitsubishi Electric Corporation
Amagasaki, Nhật BảnSaman Amarasinghe
CSAIL, MIT
Cambridge, USA
saman@csail.mit.edu
Tóm tắt —Các Mô hình Ngôn ngữ Lớn (LLM) thể hiện một hiện tượng độc đáo được gọi là khả năng nổi sinh, chứng minh sự thành thạo qua nhiều nhiệm vụ, từ tóm tắt văn bản đến sinh mã. Trong khi những khả năng này mở ra những con đường mới trong thiết kế phần mềm và chế tác, việc kết hợp chúng đưa ra những thách thức đáng kể. Các nhà phát triển phải đối mặt với các quyết định về việc sử dụng LLM để thực hiện trực tiếp các tác vụ trong ứng dụng cũng như để sinh và thực thi mã để hoàn thành những tác vụ này. Hơn nữa, thiết kế prompt hiệu quả trở thành mối quan tâm quan trọng, do nhu cầu trích xuất dữ liệu từ các đầu ra ngôn ngữ tự nhiên. Để giải quyết những phức tạp này, bài báo này giới thiệu AskIt, một ngôn ngữ đặc thù miền (DSL) được thiết kế đặc biệt cho LLM. AskIt đơn giản hóa việc tích hợp LLM bằng cách cung cấp một giao diện thống nhất không chỉ cho phép thực thi tác vụ trực tiếp sử dụng LLM mà còn hỗ trợ toàn bộ chu kỳ sinh mã và thực thi. Khả năng kép này được đạt được thông qua (1) điều khiển đầu ra có hướng dẫn kiểu, (2) định nghĩa hàm dựa trên mẫu, và (3) sinh prompt cho cả hai chế độ sử dụng. Các đánh giá của chúng tôi nhấn mạnh hiệu quả của AskIt. Qua 50 tác vụ, AskIt sinh ra các prompt ngắn gọn, đạt được giảm 16,14% độ dài prompt so với các điểm chuẩn. Ngoài ra, bằng cách cho phép chuyển đổi liền mạch giữa việc sử dụng LLM trực tiếp trong ứng dụng và để sinh mã, AskIt đạt được những cải thiện hiệu quả đáng kể, như quan sát được trong các thí nghiệm điểm chuẩn GSM8K của chúng tôi. Các triển khai của AskIt trong TypeScript và Python có sẵn tại https://github.com/katsumiok/ts-askit và https://github.com/katsumiok/pyaskit, tương ứng.
Từ khóa chỉ mục —ngôn ngữ đặc thù miền, sinh mã, mô hình ngôn ngữ lớn, kỹ thuật phần mềm, trí tuệ nhân tạo
I. GIỚI THIỆU
Các nghiên cứu gần đây [1] đã tiết lộ những khả năng đáng chú ý của Các Mô hình Ngôn ngữ Lớn (LLM), trở nên ngày càng rõ rệt với việc mở rộng quy mô mô hình. Những khả năng này bao gồm một loạt rộng các tác vụ, bao gồm các phép toán số học [2], trả lời câu hỏi [3], tóm tắt văn bản [4], dịch ngôn ngữ [5], diễn đạt lại [6], dự đoán văn bản [7], và sinh mã [8]–[12]. Điều thú vị là, những khả năng này không được truyền đạt một cách rõ ràng mà được nuôi dưỡng một cách tự nhiên thông qua việc tiếp xúc rộng rãi với dữ liệu ngôn ngữ tự nhiên trong quá trình đào tạo. Hiện tượng này, được gọi là khả năng nổi sinh, phân biệt LLM. Khái niệm về khả năng nổi sinh rất hấp dẫn, gợi ý rằng với những tiến bộ hơn nữa trong các mô hình ngôn ngữ, thậm chí những khả năng tinh vi hơn có thể xuất hiện.

Sự gia tăng của những khả năng nổi sinh này có những tác động đáng kể đối với phát triển phần mềm, có khả năng thay đổi chính những phương pháp mà phần mềm được tạo ra. Các nhà phát triển có thể kết hợp LLM trong các ứng dụng để xử lý các tác vụ như trả lời câu hỏi, tóm tắt văn bản, hoặc dịch ngôn ngữ. Một ứng dụng khác của LLM là trong sinh mã. Các công cụ như Jigsaw [13] và Codex/Copilot [8] khai thác LLM để chuyển đổi các mô tả ngôn ngữ tự nhiên thành mã. Ngay cả không có những công cụ cụ thể này, các nhà phát triển có thể tận dụng các chatbot dựa trên LLM, như ChatGPT dựa trên GPT-4 [14], BingAI, và Bard, cho cùng mục đích.

Tuy nhiên, việc tích hợp LLM vào phát triển phần mềm không thiếu những thách thức. Một quyết định chính mà các nhà phát triển phải đối mặt là liệu nên nhúng LLM trực tiếp vào ứng dụng hay sử dụng nó để sinh mã. Sự khác biệt giữa hai ứng dụng này rất rõ rệt, khiến việc chuyển đổi giữa chúng sau này trở nên cồng kềnh. Ví dụ, trong khi người ta có thể kết hợp một LLM trực tiếp vào một ứng dụng để sắp xếp một danh sách số, một cách tiếp cận khác sẽ là sử dụng LLM để sinh mã để sắp xếp. Việc chọn lựa giữa những phương pháp luận này sau khi quyết định có thể rất cồng kềnh. Những phương pháp luận này khác nhau đáng kể, và việc thay đổi cách tiếp cận được chọn sau đó đòi hỏi nỗ lực đáng kể.

Hơn nữa, bất kể cách tiếp cận nào, các nhà phát triển phải nghĩ ra các prompt hiệu quả, trích xuất dữ liệu phù hợp từ đầu ra của LLM, và sau đó xử lý nó. Nếu ứng dụng tích hợp một LLM cho chức năng của nó, mã phải được viết để phân tích phản hồi của LLM — một nhiệm vụ không tầm thường do định dạng ngôn ngữ tự nhiên. Do đó, việc chỉ định định dạng dữ liệu mong muốn trong prompt thường được áp dụng để dễ dàng phân tích phản hồi. Tuy nhiên, điều này đòi hỏi thiết kế prompt chính xác, cụ thể cho tác vụ. Khi LLM được sử dụng để sinh mã, mã kết quả phải được tích hợp thủ công vào ứng dụng.

Để đáp ứng những thách thức này, chúng tôi trình bày AskIt: một ngôn ngữ đặc thù miền (DSL) được điều chỉnh cho LLM. AskIt cung cấp một giao diện lập trình hài hòa qua các tác vụ đa dạng, với (1) điều khiển đầu ra có hướng dẫn kiểu, (2) định nghĩa hàm dựa trên mẫu, (3) khả năng sinh mã. Điều khiển đầu ra có hướng dẫn kiểu loại bỏ nhu cầu chỉ định định dạng dữ liệu trong các prompt ngôn ngữ tự nhiên, loại trừ việc thiết kế prompt phức tạp trước đây cần thiết cho việc trích xuất phản hồi.

--- TRANG 2 ---
Định nghĩa hàm dựa trên mẫu cho phép các nhà phát triển tạo ra các hàm tận dụng một LLM, sử dụng các prompt được điều chỉnh cho các tác vụ cụ thể. Những mẫu như vậy có thể chấp nhận các tham số đầu vào một cách liền mạch ánh xạ đến các tham số của hàm được định nghĩa. Với sinh mã, không có ranh giới giữa việc tích hợp một LLM vào một ứng dụng và sử dụng nó để sinh mã, cho phép chuyển đổi dễ dàng giữa hai mà không cần điều chỉnh mẫu prompt.

Chúng tôi chứng minh khả năng áp dụng của AskIt qua một loạt rộng các tác vụ LLM. Bằng cách sử dụng AskIt để triển khai 50 tác vụ phổ biến, chúng tôi cho thấy AskIt có thể sinh 7,56 dòng mã TypeScript và 6,52 dòng mã Python trung bình. Chúng tôi cũng xác nhận rằng AskIt có thể giảm độ dài prompt trung bình 16,14% so với các prompt gốc được sử dụng trong điểm chuẩn OpenAI Evals¹.

Ngoài ra, chúng tôi đo lường tốc độ tăng của các hàm được định nghĩa với AskIt khi chúng tôi chuyển từ việc sử dụng một LLM như một phần của ứng dụng sang thực thi các hàm tương đương được sinh bởi LLM. Một thí nghiệm với điểm chuẩn GSM8K [15] tiết lộ rằng các hàm được sinh bởi AskIt với GPT-4 đạt được tốc độ tăng 275.092,55x trong TypeScript và 6.969.904,73x trong Python, tương ứng, so với các hàm tương tự sử dụng GPT-4 như một phần của ứng dụng.

Những đóng góp của bài báo này được tóm tắt như sau:
1) Chúng tôi phân loại các tác vụ theo việc chúng có thể được trả lời trực tiếp bởi LLM hay không và liệu chúng có thể được mã hóa bởi LLM hay không và xác định những thách thức phải đối mặt khi tích hợp LLM vào phát triển phần mềm.
2) Chúng tôi giới thiệu một giao diện lập trình thống nhất được điều chỉnh cho LLM để phù hợp với các tác vụ có thể mã hóa, các tác vụ có thể trả lời trực tiếp, và các tác vụ thuộc cả hai loại. Giao diện này có điều khiển đầu ra có hướng dẫn kiểu, định nghĩa hàm dựa trên mẫu, và khả năng sinh mã. Điều này loại bỏ ranh giới giữa việc tích hợp ứng dụng trực tiếp của một LLM và việc sử dụng nó để sinh mã.
3) Chúng tôi triển khai AskIt trong cả ngôn ngữ kiểu tĩnh (TypeScript) và ngôn ngữ kiểu động (Python) và đánh giá chúng qua một tập hợp đa dạng các tác vụ, thể hiện sức mạnh của nó trong sinh mã và hiệu quả trong việc giảm prompt.

II. CÁC VÍ DỤ ĐỘNG LỰC
Để nhấn mạnh nhu cầu về một cách tiếp cận thống nhất và hợp lý để kết hợp LLM vào các tác vụ lập trình, phần này khám phá hai ứng dụng riêng biệt có thể hưởng lợi từ LLM. Cái đầu tiên chứng minh tiềm năng của LLM trong phân tích cảm xúc của các đánh giá sản phẩm, trong khi cái thứ hai thảo luận về một tác vụ truy cập tệp lưu trữ kết quả của phân tích cảm xúc trong hệ thống tệp cục bộ. Trong cả hai trường hợp, nhà phát triển phần mềm phải tạo ra một prompt và hoặc là diễn giải phản hồi từ LLM hoặc tích hợp mã được sinh vào mã nguồn của họ.

¹https://github.com/openai/evals

A. Ví dụ
1) Sử dụng một LLM như một Phần của Ứng dụng: Xem xét một kịch bản mà một nhà phát triển đang viết một chương trình để phân tích cảm xúc của các đánh giá sản phẩm. Mặc dù tác vụ này truyền thống dựa vào các pipeline xử lý ngôn ngữ tự nhiên phức tạp hoặc các mô hình học máy, một LLM như GPT-4 có thể đơn giản hóa đáng kể quá trình. Với một prompt được tạo ra một cách phù hợp, LLM có thể diễn giải và suy luận cảm xúc đằng sau một đánh giá cho trước.

Dưới đây là một biểu diễn mã giả đơn giản hóa:
```
1review = "Sản phẩm tuyệt vời. Nó vượt quá tất cả mong đợi của tôi."
2prompt = "Xác định cảm xúc của đánh giá này: '" + review + "'. Cảm xúc cuối cùng nên được đặt trong [ và ] như [tiêu cực]."
3response = LLM.predict(prompt) # phản hồi: "Cảm xúc của đánh giá là [tích cực]."
4sentiment = parse_sentiment(response) # cảm xúc: "tích cực"
```
trong đó # biểu thị một bình luận. Dòng 1 khởi tạo đánh giá. Trong thực tế, điều này thường sẽ được lấy từ cơ sở dữ liệu hoặc một nguồn dữ liệu khác, nhưng nó được mã hóa cứng ở đây cho mục đích minh họa. Dòng 2 tạo ra prompt bằng cách tích hợp đánh giá với một cấu trúc mẫu. Dòng 3 gắn kết một LLM, xử lý prompt để tạo ra một phản hồi. Cuối cùng, Dòng 4 trích xuất cảm xúc từ phản hồi.

Kịch bản này giới thiệu hai thách thức chính:
• Phân tích phản hồi của LLM: Các nhà phát triển phải viết mã để trích xuất cảm xúc từ đầu ra ngôn ngữ tự nhiên của LLM. Do tính biến đổi tiềm ẩn trong các phản hồi của LLM, dựa trên prompt và hành vi vốn có của nó, việc trích xuất này xa không tầm thường.
• Tạo ra prompt: Tác vụ này đòi hỏi hiểu biết sâu về xử lý ngôn ngữ tự nhiên và quen thuộc với các phản hồi LLM tiềm năng. Bằng cách chỉ định định dạng phản hồi mong muốn với [ và ], nhà phát triển có thể làm cho việc phân tích tiếp theo dễ dàng.

Trong khi ví dụ được cung cấp là đơn giản, với việc xây dựng prompt và phân tích phản hồi tương đối đơn giản, các vấn đề phức tạp hơn có thể giới thiệu những thách thức. Trong những kịch bản này, các kỹ thuật như Chuỗi Suy nghĩ (CoT) [16] trở nên cần thiết, hướng dẫn LLM tạo ra các phản hồi tốt hơn. Nếu đầu ra mong đợi liên quan đến nhiều khía cạnh, chẳng hạn như một danh sách hoặc một số giá trị, việc tạo ra prompt và phân tích phản hồi có thể trở nên phức tạp hơn, đòi hỏi nỗ lực thêm từ nhà phát triển. Do đó, trong khi các LLM như GPT-4 cung cấp những khả năng mạnh mẽ, các nhà phát triển phải khéo léo tạo ra các prompt và phương pháp phân tích của họ để đảm bảo tính chính xác và độ tin cậy.

2) Sử dụng một LLM để Sinh Mã: Mở rộng tác vụ phân tích cảm xúc, hãy đi sâu vào một kịch bản mà kết quả của phân tích cảm xúc cần được lưu vào một tệp CSV (giá trị tách bằng dấu phẩy) cục bộ. Mặc dù đây không phải là một chức năng vốn có của LLM, khả năng của chúng trong việc sinh mã phù hợp minh họa tính thích ứng của chúng.

Thông thường, LLM sinh các đoạn mã để đáp ứng một mô tả cấp cao được cung cấp bởi người dùng. Ví dụ, một nhà phát triển có thể sử dụng một LLM, tìm cách sinh mã Python

--- TRANG 3 ---
Hình 1: Sinh mã bởi LLM (ChatGPT)

Các Tác vụKhông thể mã hóa nhưng Có thể Trả lời Trực tiếpCác Tác vụ Giao nhauCó thể Mã hóa và Có thể Trả lời Trực tiếpCó thể Mã hóa nhưng Không thể Trả lời Trực tiếpCác Tác vụ

Hình 2: Phân loại Tác vụ

lưu kết quả phân tích cảm xúc vào một tệp cục bộ. Sự tương tác này thường diễn ra trong các nền tảng như ChatGPT, Bard, hoặc BingAI. Ở đây, nhà phát triển sẽ đặt ra một mô tả tác vụ, chẳng hạn như "Sinh một hàm Python để ghi lại một đánh giá sản phẩm và cảm xúc liên quan của nó vào một tệp CSV cụ thể."

Hình 1 minh họa một sự tương tác với ChatGPT. Sau khi nhà phát triển nhập mô tả tác vụ, ChatGPT đáp ứng với một đoạn mã phù hợp. Nhà phát triển sau đó có thể sao chép thủ công và kết hợp đoạn mã này vào codebase hiện có của họ. Hàm được sinh này mở tệp CSV được chỉ định ở chế độ thêm và lưu 'review' và 'sentiment' như một hàng mới.

Tuy nhiên, điều quan trọng cần lưu ý là ChatGPT không thể thực thi mã được sinh này trực tiếp vì nó không có quyền truy cập vào hệ thống tệp cục bộ. Do đó, các nhà phát triển cần tích hợp thủ công đoạn mã này vào môi trường phần mềm của họ. Đôi khi, việc kết hợp này đòi hỏi các điều chỉnh để làm cho mã mới được thêm phù hợp với codebase hiện có. Trong khi cách tiếp cận sinh mã này hữu ích, nó vẫn đòi hỏi sự can thiệp thủ công mà có thể được làm hiệu quả hơn.

B. Phân loại và Ví dụ về Các Loại Vấn đề cho LLM
Trong các ví dụ trước, chúng tôi đã trình bày hai loại tác vụ riêng biệt có thể được giải quyết hoặc hỗ trợ bởi LLM. Một liên quan đến việc sử dụng LLM như một phần của ứng dụng, trong khi cái khác đòi hỏi việc sử dụng LLM để sinh mã. Để tối ưu hóa việc sử dụng LLM, chúng tôi phân loại các tác vụ dựa trên hai chiều sau:
• Có thể Trả lời Trực tiếp hay Không: Xác định liệu LLM có thể trả lời trực tiếp tác vụ hay không.
• Có thể Mã hóa hay Không thể Mã hóa: Cho biết liệu LLM có thể sinh mã để thực hiện tác vụ hay không.

Những chiều này trực giao với nhau. Chúng ta có thể tận dụng LLM nếu một trong hai chiều đúng. Nói cách khác, chúng ta có thể tích hợp LLM vào một ứng dụng nếu tác vụ có thể trả lời trực tiếp, và chúng ta có thể sử dụng LLM để triển khai mã nếu tác vụ có thể mã hóa. Với những chiều này, chúng ta có thể nhóm các tác vụ thành ba loại: các tác vụ không thể mã hóa nhưng có thể trả lời trực tiếp, các tác vụ giao nhau, và các tác vụ có thể mã hóa nhưng không thể trả lời trực tiếp, như minh họa trong Hình 2. Hai ví dụ trước thuộc các loại tác vụ không thể mã hóa nhưng có thể trả lời trực tiếp và tác vụ có thể mã hóa nhưng không thể trả lời trực tiếp, tương ứng. Tác vụ phân tích cảm xúc đại diện cho một tác vụ không thể mã hóa nhưng có thể trả lời trực tiếp vì LLM có thể giải quyết nó ngay lập tức, trong khi các phương pháp lập trình truyền thống có thể không đạt được độ chính xác tương đương. Ngược lại, tác vụ truy cập tệp có thể mã hóa nhưng không thể trả lời trực tiếp vì các kỹ thuật lập trình truyền thống có thể xử lý nó, nhưng nó không được trả lời một cách đơn giản bởi LLM.

Đáng chú ý là một số tác vụ có thể được giải quyết cả trực tiếp bởi LLM hoặc thông qua mã được sinh bởi LLM. Đối với những tác vụ như vậy, có thể chọn bất kỳ giải pháp nào. Lấy ví dụ, câu hỏi toán học "7 nhân 8 bằng bao nhiêu?". Điều này có thể được giải quyết bằng cách sử dụng LLM trong một ứng dụng hoặc bằng cách sử dụng LLM để sinh mã. Nói chung, các tác vụ giao nhau thể hiện hiệu suất vượt trội khi được giải quyết bởi mã được sinh hơn khi được giải quyết trực tiếp bởi LLM. Tuy nhiên, việc phân định các tác vụ không phải lúc nào cũng rõ ràng và có thể mơ hồ. Các ranh giới tách ba loại thường mờ nhạt. Như chúng tôi sẽ minh họa với kết quả thí nghiệm của chúng tôi, một số vấn đề toán học có thể trả lời được bởi LLM nhưng kháng cự mã hóa bởi LLM.

Thách thức phát sinh từ nhu cầu triển khai riêng biệt của hai cách tiếp cận. Khi LLM được kết hợp vào một ứng dụng, chúng ta phải tạo ra mã để phân tích các phản hồi LLM. Ngược lại, khi LLM được sử dụng để sinh mã, mã kết quả phải được tích hợp thủ công vào mã nguồn của chúng ta. Một giao diện thống nhất cho những chiến lược này vắng mặt, làm phức tạp việc chuyển đổi giữa hai phương pháp. Nếu một giao diện thống nhất như vậy tồn tại, việc chuyển đổi giữa hai kỹ thuật sẽ đơn giản hơn. Khả năng thích ứng này là cần thiết, đặc biệt là do tính mơ hồ vốn có trong phân loại tác vụ. Hơn nữa, khi LLM tiếp tục phát triển, các ranh giới xác định ba loại chắc chắn sẽ thay đổi.

III. THIẾT KẾ VÀ TRIỂN KHAI
A. Tổng quan
Ngôn ngữ Đặc thù Miền (DSL) của chúng tôi, AskIt, cung cấp hai API: ask và define. Chúng phục vụ như một giao diện thống nhất bằng cách mượn cú pháp của các lời gọi hàm từ ngôn ngữ lập trình chủ. Do đó, chúng có thể được sử dụng bất cứ nơi nào các lời gọi hàm được cho phép. Những API này giải quyết một loạt rộng các tác vụ, chẳng hạn như các tác vụ không thể mã hóa nhưng có thể trả lời trực tiếp, các tác vụ giao nhau, và các tác vụ có thể mã hóa nhưng không thể trả lời trực tiếp, như chi tiết trong phần trước. Các tính năng của API như sau: (1) Điều khiển đầu ra có hướng dẫn kiểu, (2) định nghĩa hàm dựa trên mẫu, và (3) sinh mã từ giao diện thống nhất.

Như một bằng chứng về khái niệm, chúng tôi triển khai các phiên bản TypeScript và Python của AskIt. Trình biên dịch DSL được tạo ra như một plugin trình biên dịch TypeScript cho TypeScript và như một thư viện Python cho Python. Trình biên dịch và runtime AskIt tổng hợp prompt cho LLM và parser cho phản hồi dựa trên thông tin kiểu của hàm và biến được nhúng trong biểu thức mẫu. Nó cũng sinh hàm thực hiện các tác vụ có thể mã hóa. Trong phần sau, chúng tôi minh họa những tính năng này sử dụng cùng các ví dụ được cung cấp trong phần trước. Mặc dù chúng tôi sử dụng TypeScript cho các ví dụ, một cú pháp tương tự có thể được áp dụng cho Python. Việc triển khai trong Python được thảo luận sau trong III-F.

Điều khiển Đầu ra Có hướng dẫn Kiểu: Một ví dụ điển hình của một tác vụ không thể mã hóa nhưng có thể trả lời trực tiếp là xác định cảm xúc của một đánh giá. Chúng tôi giả định cảm xúc có thể là tích cực hoặc tiêu cực. Thay vì chi tiết định dạng đầu ra mong đợi trong prompt, chúng ta có thể chỉ định kiểu đầu ra mong đợi trong DSL. Ví dụ, mã sau là mã AskIt hợp lệ cho tác vụ phân tích cảm xúc:
```
1let sentiment = await ask<'positive' | 'negative'>('What is the sentiment of the following review: "The product is fantastic. It exceeds all my expectations."');
```
Ở đây, ask là một API chấp nhận một prompt và trả về một phản hồi. Kiểu của phản hồi được chỉ ra trong tham số kiểu của ask. Trong trường hợp này, 'positive'|'negative' là một kiểu hợp, bao gồm hai kiểu chuỗi theo nghĩa đen và có nghĩa là phản hồi là 'positive' hoặc 'negative'. Thông tin kiểu này hỗ trợ trong việc sinh prompt cho LLM. Sau khi thực thi mã, biến sentiment sẽ được gán giá trị 'positive'. await là một từ khóa chỉ ra việc thực thi bất đồng bộ của API ask. API ask trả về một promise, và từ khóa await được sử dụng để chờ promise được giải quyết.

Hơn nữa, một prompt có thể được tham số hóa bằng cách sử dụng một mẫu prompt như một đối số cho ask. Sử dụng một mẫu prompt, ví dụ trên có thể được viết lại như:
```
1let sentiment = await ask<'positive' | 'negative'>('What is the sentiment of {{review}}?');
```
Ở đây, review là một biến kiểu chuỗi. {{ và }} đánh dấu đầu và cuối của một biến trong mẫu prompt, tương ứng. Biến review bắt biểu tượng được khai báo trong cùng phạm vi.

Định nghĩa Hàm Dựa trên Mẫu: Trong phát triển phần mềm thực tế, cùng một tác vụ thường cần nhân bản. AskIt giới thiệu một cơ chế để xây dựng một hàm để thực hiện lặp lại cùng một tác vụ. Ví dụ, một hàm có thể được thiết kế để trả về cảm xúc của một đánh giá:
```
1let getSentiment = define<'positive' | 'negative'>('What is the sentiment of {{review}}?');
```
Ở đây, define là một API chấp nhận một mẫu prompt và trả về một hàm. Tham số kiểu của define xác định giá trị trả về của hàm. Tham số của hàm được định nghĩa trong mẫu prompt. Trong ví dụ này, hàm nhận một biến có tên review. Tham số trong mẫu prompt tương ứng với tham số của hàm được định nghĩa với cùng tên. Bằng cách đưa ra một đối số thực tế, hàm được định nghĩa có thể được gọi như sau:
```
1let sentiment = await getSentiment({review: 'The product is fantastic. It exceeds all my expectations.'});
```
Khi thực thi, sentiment sẽ giữ giá trị 'positive'.

Sinh Mã: Như thảo luận trước đây, LLM có thể được sử dụng để sinh mã. Ví dụ, LLM có thể được sử dụng để triển khai một hàm thêm một đánh giá và cảm xúc của nó vào một tệp CSV. Không cần sử dụng các API khác nhau để sinh mã. Giao diện gắn kết của chúng tôi cho phép sinh hàm:
```
1let appendReviewToCsv = define<void>('Append {{review}} and {{sentiment}} as a new row in the CSV file named {{filename}}');
```
Ở đây, filename, review, và sentiment là các biến. Mã trên có thể được gọi ở bất cứ đâu trong mã nguồn:
```
1appendReviewToCsv({
2 filename: 'reviews.csv',
3 review: 'The product is fantastic. It exceeds all my expectations.',
4 sentiment: 'positive'
5});
```
Trước khi thực thi mã, trình biên dịch DSL, với sự hỗ trợ của một LLM, sẽ sinh một hàm thêm một đánh giá và cảm xúc của nó vào một tệp CSV.

B. Cú pháp
Cú pháp AskIt xây dựng trên cấu trúc lời gọi hàm của ngôn ngữ lập trình chủ. Listing 1 trình bày cú pháp AskIt được điều chỉnh cho TypeScript. Đối với ngữ pháp này, chúng tôi giả định rằng TYPE, STRING_LITERAL, IDENTIFIER, và CONSTANT_EXPRESSION là các biểu tượng không đầu cuối. Chúng biểu thị kiểu, chuỗi theo nghĩa đen, định danh, và biểu thức hằng số trong ngôn ngữ lập trình chủ, tương ứng.

Các API chính được cung cấp bởi AskIt là ask và define (Dòng 1). API ask lấy kiểu phản hồi của một LLM như một tham số kiểu và lấy một mẫu prompt như một tham số hàm. Tùy chọn, nó cũng có thể lấy các ví dụ về các cặp đầu vào-đầu ra của tác vụ (Dòng 2). Những ví dụ này tạo điều kiện cho việc học few-shot [5] và cung cấp một cách Lập trình bằng Ví dụ (PBE) [17], [18]. Trái lại, API define lấy kiểu phản hồi của LLM và các kiểu tham số tùy chọn như các tham số kiểu (Dòng 3). Giống như ask, define có thể kết hợp một mẫu prompt và các ví dụ. Hơn nữa, define có thể chấp nhận hai tập hợp các ví dụ đầu vào-đầu ra. Trong khi tập hợp đầu tiên được sử dụng cho việc học few-shot, tập hợp thứ hai được sử dụng để xác thực mã được sinh.

Mẫu prompt về cơ bản là một chuỗi theo nghĩa đen (Dòng 4), nhưng nó có thể có các placeholder cho các biến. Những placeholder này là các định danh được bao quanh bởi {{ và }}.

--- TRANG 4 ---
```
1askit_api ::= ask | define
2ask ::= "ask" "<" TYPE ">" "(" prompt_template examples? ")"
3define ::= "define" "<" TYPE param_types? ">" "(" prompt_template examples? examples? ")"
4prompt_template ::= STRING_LITERAL
5param_types ::= "," "{" IDENTIFIER ":" TYPE ("," IDENTIFIER ":" TYPE) *"}"
6examples ::= "," "[" example ("," example) *"]"
7example ::= "{" "input" ":" input "," "output" ":" CONSTANT_EXPRESSION "}"
8input ::= "{" IDENTIFIER ":" CONSTANT_EXPRESSION ("," IDENTIFIER ":" CONSTANT_EXPRESSION) *"}"
```
Listing 1: Cú pháp của AskIt cho TypeScript

Chương trình Người dùngTrình biên dịch AskItLLM
Các Hàm Tác vụCác Hàm Tác vụCác Hàm Được sinhChương trình Người dùng Cập nhậtPromptPromptPrompt1PromptPromptPhản hồiPrompt LLMPromptPromptPromptPromptPromptPhản hồiCho Các Tác vụ Có thể Trả lời Trực tiếpCho Các Tác vụ Có thể Mã hóa
Runtime AskIt234
5789
106

Hình 3: Luồng tính toán của AskIt DSL cho ngôn ngữ kiểu tĩnh

Tên biến trong placeholder này nên là một định danh hợp lệ của ngôn ngữ lập trình chủ.

Các kiểu tham số là các cặp khóa-giá trị được liệt kê trong { và }, được phân tách bằng dấu phẩy (Dòng 5). Ở đây, khóa biểu thị tên biến, và giá trị đại diện cho kiểu của nó trong ngôn ngữ chủ.

Các ví dụ bao gồm các cặp đầu vào-đầu ra. Chúng được bao quanh trong [ và ] và được phân tách bằng dấu phẩy (Dòng 6). Mỗi ví dụ, được giới hạn bởi { và }, có một khóa đầu vào, liên kết đến một đầu vào tác vụ, và một khóa đầu ra, trỏ đến đầu ra tác vụ (Dòng 7). Một đầu vào là một tập hợp các cặp khóa-giá trị, nơi khóa là tên biến và giá trị là một biểu thức hằng số được định nghĩa bởi ngôn ngữ chủ. Đầu ra là một biểu thức hằng số độc lập.

C. Luồng Tính toán
Trong một bằng chứng về khái niệm, chúng tôi triển khai AskIt cho TypeScript và một trình biên dịch DSL như một plugin trình biên dịch TypeScript. Trình biên dịch DSL được kích hoạt khi trình biên dịch TypeScript biên dịch mã nguồn. Mã nguồn được viết trong TypeScript mở rộng với DSL của chúng tôi, và đầu ra của trình biên dịch DSL là mã TypeScript.

Luồng tính toán của trình biên dịch DSL được minh họa trong Hình 3. Phía trái của hình cho thấy luồng tính toán tại thời điểm biên dịch, và phía phải cho thấy luồng tính toán tại runtime. Khi trình biên dịch DSL được kích hoạt, nó duyệt qua Cây Cú pháp Trừu tượng (AST) của mã nguồn và chuyển đổi các API AskIt thành các hàm cụ thể được viết trong TypeScript (1). Nếu lời gọi đến define được phát hiện và nó là một tác vụ có thể mã hóa, trình biên dịch DSL sinh một hàm (2–6). Đầu tiên, trình biên dịch DSL sinh một prompt để yêu cầu một LLM mã hóa tác vụ (2). Sau đó, prompt được chuyển đến LLM (3). LLM sinh một phản hồi với một mã cho tác vụ (4), và trình biên dịch DSL nhận phản hồi và phân tích phản hồi để trích xuất mã được sinh (5). Cuối cùng, trình biên dịch DSL xác thực mã và lưu trữ nó. Đồng thời, một lời gọi đến define được thay thế bằng một tham chiếu đến hàm được sinh (6).

Trình biên dịch DSL cũng cập nhật các lời gọi đến ask và define ngay cả khi chúng không phải là các tác vụ có thể mã hóa. Trình biên dịch DSL trích xuất thông tin kiểu từ tham số kiểu của ask và define và mã hóa chúng thành dữ liệu để được sử dụng để sinh một prompt tại runtime (6).

Chương trình người dùng tại runtime bao gồm các hàm được sinh, chương trình người dùng cập nhật và runtime DSL. Khi chương trình người dùng được thực thi, chương trình người dùng cập nhật gọi các hàm được sinh nếu tác vụ có thể mã hóa. Nếu lời gọi đến ask hoặc các hàm được định nghĩa bởi define được phát hiện, runtime DSL sinh một prompt dựa trên thông tin kiểu được trích xuất tại thời điểm biên dịch (7). Sau đó, prompt được chuyển đến LLM (8). LLM sinh một phản hồi chứa dữ liệu trong kiểu được chỉ định (9), và runtime DSL nhận phản hồi và phân tích nó để trích xuất câu trả lời (10).

D. Sinh Mã cho Các Tác vụ Có thể Mã hóa
Trình biên dịch DSL của chúng tôi sinh một hàm thực hiện tác vụ được chỉ định bởi mẫu prompt được chuyển đến lời gọi define. Tất cả các lời gọi đến define được kiểm tra để xác định xem tác vụ có thể mã hóa hay không. Nếu có, trình biên dịch DSL sinh một hàm và thay thế lời gọi đến define bằng hàm được sinh. Kết quả là, các lời gọi đến các hàm được sinh được thực thi mà không gọi LLM tại runtime.

Chúng tôi cung cấp hai cách để chỉ định tác vụ để có thể mã hóa bởi LLM. Phương pháp đầu tiên cho phép người dùng chỉ định tên của một tệp nguồn chứa lời gọi đến define. Trong trường hợp này, trình biên dịch DSL sinh các hàm cho tất cả các lời gọi đến define trong tệp nguồn được chỉ định. Phương pháp thứ hai, chi tiết hơn, cho phép người dùng chỉ định tên của hàm sẽ được sinh. Tên hàm này tương ứng với tên biến mà kết quả của lời gọi define được gán.

Đối với tất cả các lời gọi đến define được chỉ định là có thể mã hóa, trình biên dịch DSL tuân theo những bước này để sinh một hàm với một LLM:

Bước 1: Trình biên dịch DSL tạo một prompt cho LLM dựa trên mẫu prompt được đưa ra cho lời gọi define.
Bước 2: Trình biên dịch DSL gửi prompt này đến LLM và nhận phản hồi từ LLM.

--- TRANG 5 ---
```
1Q: Implement the following function:
2'''typescript
3export function func({x, y}: {x: number, y: number}): number {
4 // add 'x' and 'y'
5}
6'''
```

```
1A:
2'''typescript
3export function func({x, y}: {x: number, y: number}): number {
4 // add 'x' and 'y'
5 return x + y;
6}
7'''
```

```
1Q: Implement the following function:
2'''typescript
3export function calculateFactorial({n}: {n: number}): number {
4 // Calculate the factorial of 'n'
5}
6'''
```

Hình 4: Prompt để yêu cầu LLM mã hóa tác vụ

Bước 3: Trình biên dịch DSL phân tích phản hồi này để trích xuất mã của tác vụ và xác thực nó.

Bước 2 và Bước 3 được thực thi nhiều lần cho đến khi một mã được sinh vượt qua việc xác thực trong Bước 3. Việc xác thực bao gồm một kiểm tra cú pháp và một kiểm tra ngữ nghĩa bằng cách sử dụng thực thi với các ví dụ kiểm tra.

Trong Bước 1, trình biên dịch DSL xây dựng một prompt để yêu cầu LLM triển khai tác vụ. Prompt này hướng dẫn LLM hoàn thành thân của hàm. Chữ ký hàm được dẫn xuất từ thông tin kiểu của tham số kiểu từ lời gọi define. Cả kiểu trả về và các kiểu tham số đều được lấy từ tham số kiểu của lời gọi define. Trình biên dịch DSL gán một tên duy nhất cho hàm và phác thảo thân hàm rỗng cho LLM điền vào. Chúng tôi áp dụng một cách tiếp cận học một lần cho sinh hàm. Trong prompt được sinh, chúng tôi đầu tiên cung cấp một sinh mã mẫu để làm sáng tỏ quá trình sinh mã. Sau đó, chúng tôi hướng dẫn LLM sinh một hàm triển khai tác vụ được chỉ định.

Ví dụ, xem xét kịch bản mà trình biên dịch DSL tạo một hàm với API define:
```
1let calculateFactorial = define<number, {n: number}>("Calculate the factorial of {{n}}")
```
Tham số kiểu thứ nhất và thứ hai chỉ định kiểu trả về và kiểu tham số của hàm được định nghĩa. Từ lời gọi này, trình biên dịch DSL sinh một hàm có chữ ký như sau:
```
1function calculateFactorial({n}: {n: number}): number
```
Chúng ta có thể gọi hàm này với một đối số có tên, như calculateFactorial({n: 10}). Chúng tôi áp dụng các tham số có tên thay vì các tham số vị trí vì chúng mạnh mẽ hơn cho việc sửa đổi prompt. Các tham số có tên không bị ảnh hưởng bởi thứ tự xuất hiện trong một mẫu prompt.

Kiểu trả về và các kiểu tham số bắt nguồn từ tham số kiểu của lời gọi define. Trình biên dịch DSL gán một tên duy nhất cho hàm và vạch ra hàm rỗng cho LLM hoàn thành. Prompt hướng dẫn LLM triển khai thân hàm được hiển thị trong Hình 4. Prompt này bao gồm ba phần. Hai phần đầu luôn giống nhau bất kể tác vụ. Chúng cung cấp cho LLM một ví dụ về đầu vào và đầu ra. Ví dụ này đòi hỏi việc xây dựng một hàm chấp nhận hai số và xuất tổng của chúng. Phần đầu tiên yêu cầu LLM triển khai hàm. Trong khi thân hàm rỗng, prompt chi tiết tác vụ sẽ được thực hiện như một bình luận bên trong thân. Phần thứ hai đưa ra một ví dụ về phản hồi mong đợi từ LLM. Phản hồi mong đợi là mã triển khai hàm. Phần thứ ba là phần cụ thể cho tác vụ và hướng dẫn LLM triển khai tác vụ đã cho. Phản hồi mong đợi này là mã triển khai hàm. Cấu trúc của hướng dẫn cho LLM giống như hướng dẫn trong phần đầu tiên.

Trong Bước 2, trình biên dịch DSL gửi prompt được tạo đến LLM và nhận phản hồi từ LLM. Bước này được thực thi sử dụng API cấp thấp được cung cấp bởi LLM. Trong triển khai của chúng tôi, chúng tôi sử dụng API OpenAI cho bước này. API có một tham số có tên temperature, điều khiển tính ngẫu nhiên của phản hồi và có thể có giá trị từ 0.0 đến 2.0. Chúng tôi sử dụng giá trị mặc định 1.0 cho tham số này, vì chúng tôi tìm kiếm một mức độ ngẫu nhiên nhất định trong các phản hồi để đảm bảo một phản hồi duy nhất cho mỗi lần thử lại.

Trong Bước 3, trình biên dịch DSL trích xuất mã từ phản hồi. Phản hồi của LLM dự kiến chứa hàm được sinh trong định dạng khối mã của markdown: '''typescript ... '''. Như vậy, trình biên dịch DSL của chúng tôi có thể trích xuất hàm bằng cách tìm khối mã. Trình biên dịch DSL kiểm tra mã về mặt cú pháp và, tùy chọn, kiểm tra nó về mặt ngữ nghĩa bằng cách sử dụng các ví dụ kiểm tra được cung cấp trong lời gọi define.

Người dùng có thể chỉ định các ví dụ kiểm tra cho tác vụ trong lời gọi define. Những ví dụ này được cung cấp như các cặp đầu vào-đầu ra. Đầu vào là một tập hợp các cặp khóa-giá trị, nơi khóa là tên tham số và giá trị là một biểu thức hằng số. Đầu ra là một biểu thức hằng số độc lập. Trình biên dịch DSL thực thi hàm được sinh với đầu vào và so sánh đầu ra với đầu ra mong đợi.

Nếu mã vượt qua việc xác thực, trình biên dịch DSL lưu trữ nó trong một tệp trong thư mục có tên askit, nằm trong cùng thư mục với tệp nguồn. Trình biên dịch DSL cũng thay thế lời gọi đến define bằng một lời gọi đến hàm được sinh. Tệp chứa mã được sinh được đặt tên theo mẫu prompt. Người dùng có thể xem lại mã được sinh nếu cần thiết.

E. Tương tác với LLM cho Các Tác vụ Có thể Trả lời Trực tiếp
Đối với mỗi lời gọi define và ask không thể mã hóa, trình biên dịch DSL chỉ trích xuất thông tin kiểu từ tham số kiểu và mã hóa chúng thành dữ liệu để được sử dụng để sinh một prompt tại runtime. Các lời gọi đến các hàm được định nghĩa bởi define và các lời gọi đến ask được thay thế bằng một lời gọi đến runtime DSL của chúng tôi

--- TRANG 6 ---
```
1You are a helpful assistant that generates responses in JSON format enclosed with '''json and ''' like:
2'''json
3{ "reason": "Step-by-step reason for the answer", "answer": "Final answer or result" }
4'''
5The response in the JSON code block should match the type defined as follows:
6'''ts
7{ reason: string; answer: { title: string; author: string; year: number }[] }
8'''
9Explain your answer step-by-step in the 'reason' field.
10
11List 'n' classic books on 'subject'.
12where 'n' = 5, 'subject' = "computer science"
```
Listing 2: Prompt để yêu cầu LLM thực hiện tác vụ trực tiếp

lấy thông tin kiểu như một tham số ngoài các tham số gốc. Runtime DSL của chúng tôi tương tác với một LLM để thực thi tác vụ được chỉ định bởi mẫu prompt được chuyển đến lời gọi define hoặc ask. Các bước tương tác giữa runtime DSL và LLM như sau:

Bước 1: Runtime DSL tạo một prompt cho LLM dựa trên mẫu prompt được đưa ra cho lời gọi define.
Bước 2: Runtime DSL gửi prompt này đến LLM và nhận phản hồi từ LLM.
Bước 3: Runtime DSL phân tích phản hồi này để trích xuất câu trả lời và xác thực nó sử dụng thông tin kiểu.

Bước 2 và Bước 3 được lặp lại cho đến khi một câu trả lời trong kiểu hợp lệ có sẵn.

Trong Bước 1, runtime AskIt sinh một prompt để yêu cầu LLM thực hiện tác vụ được chỉ định bởi mẫu prompt được chuyển đến lời gọi define hoặc ask. Nó cũng sử dụng kiểu trả về được chỉ định trong tham số kiểu của lời gọi define hoặc ask để sinh một prompt hạn chế phản hồi của LLM.

Khó khăn của việc tương tác với LLM nằm ở việc trích xuất câu trả lời từ phản hồi của LLM. Các phản hồi LLM thường ở định dạng ngôn ngữ tự nhiên, khiến việc trích xuất câu trả lời trở nên thách thức. Để giải quyết vấn đề này, chúng tôi hạn chế phản hồi của LLM ở định dạng JSON (JavaScript Object Notation). Ý tưởng cốt lõi của việc sinh prompt của chúng tôi là tận dụng hiểu biết của LLM về ngữ pháp và ngữ nghĩa của các ngôn ngữ lập trình. Ví dụ, một LLM, như GPT, có thể hiểu ngữ pháp của JSON. Bằng cách yêu cầu LLM trả lời ở định dạng JSON, chúng tôi đơn giản hóa nhiệm vụ trích xuất câu trả lời từ phản hồi của nó.

Tuy nhiên, việc chỉ định định dạng JSON đơn thuần không đảm bảo sự dễ dàng của việc trích xuất câu trả lời vì cấu trúc JSON có thể thay đổi. Vấn đề này có thể được giải quyết bằng cách hạn chế phản hồi của LLM thành một định dạng JSON cụ thể. May mắn thay, LLM có thể nắm bắt ngữ nghĩa của các kiểu trong các ngôn ngữ lập trình. Ví dụ, GPT có thể hiểu ngữ nghĩa của các kiểu TypeScript. Hơn nữa, các kiểu TypeScript là lý tưởng để hạn chế cấu trúc JSON vì chúng có thể được xem như một lược đồ JSON. Ví dụ, kiểu {x: number; y: number} có thể được nhận thức như một lược đồ JSON. Ví dụ nó chấp nhận đối tượng JSON {"x": 1, "y": -1} nhưng từ chối đối tượng JSON [1, -1]. Cách tiếp cận này được giữ lại ngay cả khi ngôn ngữ chủ không phải là TypeScript. Triển khai AskIt của chúng tôi cho Python sử dụng các kiểu TypeScript để hạn chế phản hồi JSON của LLM, mặc dù Python là ngôn ngữ chủ.

Chúng tôi mong đợi tất cả các đối tượng JSON có hai trường: reason và answer bất kể tác vụ. Một tùy chọn khác có thể là không sử dụng những trường này và sử dụng toàn bộ đối tượng JSON như câu trả lời. Tuy nhiên, nếu trường answer không được chỉ định, LLM đôi khi tự động sinh trường answer, điều này không được mong đợi. Hành vi này làm phức tạp quá trình trích xuất câu trả lời. Để tránh vấn đề này, chúng tôi luôn mong đợi LLM cung cấp trường answer.

Như một ví dụ, xem xét một kịch bản mà phản hồi của LLM dự kiến là một danh sách các từ điển. Một hàm có thể được định nghĩa như sau:
```
1type Book = { title: string; author: string; year: number }
2let getBooks = define<Book[]>("List {{n}} classic books on {{subject}}.")
```
Ở đây, Dòng 1 định nghĩa một kiểu Book và Dòng 2 định nghĩa một hàm getBooks trả về một danh sách Book. Hàm này có thể được gọi như:
```
1let csBooks = getBooks({n: 5, subject: "computer science"})
```
Khi hàm này được gọi trong runtime, runtime DSL tạo một prompt như được hiển thị trong Listing 2.

Trong Listing 2, dòng đầu tiên cho biết rằng phản hồi nên ở định dạng JSON được bao quanh với '''json và '''. Dòng 2–4 cung cấp một ví dụ về định dạng JSON mong đợi. Chúng tôi minh họa rằng phản hồi nên chứa cả một câu trả lời và một lý do, được minh họa bởi phản hồi được cung cấp. Dòng 1–4 là một tuyên bố cố định, luôn được sinh bất kể các tham số của hàm. Dòng 5–8 được tạo ra dựa trên thông tin kiểu của hàm. 'reason' luôn được chỉ định là string, bất kể thông tin kiểu của hàm. Ngược lại, 'answer' là cụ thể cho tác vụ. Trong trường hợp này, kiểu của 'answer' được chỉ định là { title: string; author: string; year: number }[] vì thông tin kiểu của hàm là Book[].

Dòng 9 là một tuyên bố cố định khác, luôn được sinh bất kể mô tả tác vụ. Chúng tôi hướng dẫn LLM làm sáng tỏ câu trả lời của nó trong trường 'reason'. Điều này thúc đẩy Chuỗi Suy nghĩ (CoT) [19].

Dòng 11-12 được xây dựng dựa trên mẫu prompt được chuyển đến lời gọi define và các đối số được chuyển đến hàm. {{ và }} trong mẫu prompt được thay thế bằng dấu ngoặc đơn (Dòng 11), và các giá trị của mỗi tham số được nối vào mẫu prompt (Dòng 12).

Trong Bước 2, runtime DSL gửi prompt đến LLM và nhận phản hồi từ LLM. Bước này sử dụng API cấp thấp được cung cấp bởi LLM. Chúng tôi sử dụng API OpenAI trong triển khai của chúng tôi.

Trong Bước 3, runtime DSL phân tích phản hồi và trích xuất câu trả lời từ phản hồi. Phản hồi dự kiến chứa đối tượng JSON. Tuy nhiên, LLM không phải lúc nào cũng cung cấp câu trả lời trong định dạng mong đợi. Trong những trường hợp như vậy, runtime DSL yêu cầu LLM thử lại tác vụ. Runtime DSL sử dụng một cơ chế phản hồi để gợi ra phản hồi mong muốn trong lần thử thứ hai hoặc các lần tiếp theo. Qua nhiều lần lặp, runtime DSL tinh chỉnh prompt để hướng dẫn LLM cung cấp phản hồi trong định dạng mong đợi. Đối với mỗi lần lặp, runtime DSL đánh giá phản hồi dựa trên các tiêu chí sau:
1) Phản hồi chứa đối tượng JSON.
2) Đối tượng JSON bao gồm trường answer.
3) Trường answer khớp với kiểu mong đợi.

Nếu bất kỳ tiêu chí nào trong số này không được đáp ứng, runtime DSL tinh chỉnh prompt bằng cách thêm phản hồi của LLM và một hướng dẫn mới vào prompt gốc. Hướng dẫn mới chỉ ra phần của phản hồi không đáp ứng tiêu chí và hướng dẫn LLM sửa đổi phản hồi.

F. Triển khai cho Ngôn ngữ Kiểu Động
Trình biên dịch DSL của chúng tôi có thể được triển khai trong một ngôn ngữ kiểu động. Trong một ngôn ngữ kiểu động, thông tin kiểu được cung cấp tại runtime. Do đó, việc sinh mã cho các tác vụ có thể mã hóa nên được thực hiện tại runtime thay vì tại thời điểm biên dịch.

Triển khai AskIt của chúng tôi cho Python được thực hiện đầy đủ như một thư viện. API của AskIt cho Python gần như giống hệt với API của AskIt cho TypeScript, ngoại trừ hai điểm sau:
1) Kiểu trả về của ask và define được chỉ định như một tham số của hàm thay vì như một tham số kiểu.
2) Biên dịch được gọi một cách rõ ràng bằng cách gọi phương thức compile trên hàm được trả về bởi define.

Điểm đầu tiên liên quan đến cách thông tin kiểu được cung cấp cho define. Trong trường hợp triển khai Python, kiểu được chỉ định bởi một đối tượng kiểu được cung cấp như đối số đầu tiên của hàm thay vì một tham số kiểu. AskIt cho Python cung cấp các API để tạo một đối tượng kiểu cho kiểu trả về của ask và define. Các API được cung cấp được liệt kê trong Bảng I. Cột đầu tiên là tên của API, và cột thứ hai mô tả kiểu được tạo bởi API. Cột thứ ba cung cấp các ví dụ sử dụng, và cột thứ tư chỉ ra kiểu tương đương trong TypeScript.

Ví dụ, cùng tác vụ được giới thiệu trong III-E có thể được triển khai trong Python như sau:
```
1Book = dict({ "title": str, "author": str, "year": int })
2getBooks = define(List(Book), "List {{n}} classic books on {{subject}}.")
```
Dòng đầu tiên định nghĩa một đối tượng kiểu sử dụng các API được cung cấp. Dòng thứ hai định nghĩa một hàm trả về một danh sách Book.

Điểm khác biệt thứ hai liên quan đến cách sinh mã được thực hiện. Trong trường hợp của Python, người dùng phải chỉ định một cách rõ ràng khi sinh mã xảy ra. Cho mục đích này, các hàm được định nghĩa bởi define trả về một đối tượng hàm thực hiện phương thức compile. Khi phương thức compile được gọi, sinh mã tiến hành theo cách tương tự như thời điểm biên dịch của AskIt cho TypeScript.

Ví dụ, tác vụ được mô tả trong Phần III-D có thể được triển khai trong Python như sau:
```
1calculateFactorial = define(int, "Calculate the factorial of {{n}}").compile()
```
Khi phương thức compile được gọi, sinh mã diễn ra, dẫn đến việc trả về một đối tượng hàm thực hiện tác vụ. Mã được sinh được lưu trữ trong một tệp khi tạo ra lần đầu tiên, đảm bảo rằng sinh mã chỉ xảy ra một lần, bất kể phương thức compile được gọi bao nhiêu lần.

IV. ĐÁNH GIÁ THỰC NGHIỆM
Để đánh giá hiệu quả của AskIt, chúng tôi đã tiến hành một loạt thí nghiệm. Mỗi thí nghiệm được thiết kế để trả lời những câu hỏi riêng biệt về DSL của chúng tôi, đặc biệt nhắm mục tiêu các loại tác vụ khác nhau:
• Các tác vụ có thể mã hóa:
    – RQ1: AskIt giảm LOC cần thiết để triển khai các tác vụ có thể mã hóa như thế nào?
    – RQ2: Các ví dụ về tác vụ có hiệu quả trong việc cải thiện độ chính xác của mã được sinh không?
• Các tác vụ có thể trả lời trực tiếp:
    – RQ3: AskIt giảm LOC của việc sinh prompt cho các tác vụ có thể trả lời trực tiếp như thế nào?
• Các tác vụ giao nhau:
    – RQ4: Tốc độ và hiệu suất của các hàm được sinh bởi AskIt so sánh với cùng hàm trước khi sinh mã như thế nào?

Để giải quyết những câu hỏi này, chúng tôi thực hiện ba thí nghiệm khác nhau cho mỗi loại tác vụ.

A. Các Tác vụ Có thể Mã hóa
1) Các Tác vụ Mã hóa Phổ biến: Để giải quyết RQ1 và RQ2, chúng tôi thiết kế một thí nghiệm liên quan đến việc triển khai một tập hợp 50 tác vụ sử dụng AskIt. Để đảm bảo những tác vụ này vừa phù hợp vừa thực tế, chúng tôi nhờ sự giúp đỡ của ChatGPT. Cụ thể, chúng tôi hỏi về 50 tác vụ mã hóa TypeScript được yêu cầu phổ biến nhất. 50 tác vụ này sau đó phục vụ như nền tảng cho việc triển khai của chúng tôi trong TypeScript và Python sử dụng AskIt. Để xác minh tính đúng đắn của mã được sinh, chúng tôi cung cấp cho AskIt các kiểm tra ví dụ cho mỗi tác vụ. Nếu một kiểm tra thất bại, AskIt sẽ cố gắng tái sinh mã lên đến một giới hạn thử lại tối đa được định trước, được đặt là 9. Trong thí nghiệm này, chúng tôi chỉ định "gpt-3.5-turbo-16k" làm LLM backend cho AskIt.

Kết quả của chúng tôi cho mười tác vụ đầu tiên và các tác vụ đáng chú ý bổ sung được trình bày trong bảng II. Cột đầu tiên liệt kê 50 tác vụ. Cột thứ hai của bảng hiển thị mẫu prompt được sử dụng trong cả triển khai TypeScript và Python. Cột thứ ba chỉ ra kiểu trả về được sử dụng trong lời gọi define cho mỗi tác vụ. Cột thứ tư vạch ra các kiểu tham số được sử dụng trong lời gọi define. Chúng tôi chỉ sử dụng các kiểu tham số cho TypeScript vì triển khai Python không sử dụng các kiểu tham số. Cột năm và sáu liệt kê số dòng mã (LOC) trong mã TypeScript được sinh và các lần thử lại liên quan. LOC chỉ đếm

--- TRANG 7 ---
BẢNG I: Các kiểu và ví dụ của chúng
API Mô tả Ví dụ Sử dụng Kiểu Tương đương trong TypeScript
int Số nguyên int number
float Số thực float number
bool Boolean bool boolean
str Chuỗi str string
literal Literal literal(123) 123
list Danh sách list(int) number[]
dict Từ điển dict({ 'x':int, 'y':int }) {x: number, y: number}
union Hợp union(literal('yes'),literal('no')) 'yes' | 'no'

các dòng nội dung, bỏ qua các dòng trống hoặc chỉ có bình luận. Hai cột tiếp theo trình bày các chi tiết tương tự cho Python.

Trung bình, AskIt sản xuất 7,56 dòng cho TypeScript và 6,52 dòng cho Python. Xem xét rằng mỗi định nghĩa hàm AskIt dẫn đến một dòng duy nhất, một giảm hiệu quả 6,56 và 5,52 dòng đã được đạt được cho TypeScript và Python, tương ứng. Mặc dù tất cả các tác vụ đã được hiển thị thành công trong TypeScript, các tác vụ #11 và #21-24 gặp phải vấn đề trong Python. Điều này bắt nguồn từ biến thể Python của AskIt không tận dụng các kiểu tham số để sinh prompt trong LLM. Ví dụ, trong Tác vụ #11 cho Python, chúng tôi giả định kiểu tham số cho xs là Array. Trái lại, mã được sinh giả định nó là set.

Số lần thử lại cho việc sinh mã thành công dao động từ 0 đến 7 cho Python. Trong một vài trường hợp, mã được sinh ban đầu không vượt qua kiểm tra ví dụ. Mặc dù số lần thử lại có vẻ không đáng kể, điều cần thiết là nhận ra rằng nó không luôn luôn bằng không. Điều này cho thấy rằng LLM đôi khi có thể tạo ra mã sai. Như một ví dụ, mã cho Tác vụ #14 trong Python thất bại trong lần chạy đầu tiên, tính toán các số Fibonacci lên đến n + 1 thay vì n, đòi hỏi bảy lần thử lại. Do đó, việc cung cấp cho AskIt các ví dụ tác vụ là quan trọng để đảm bảo tính đúng đắn của mã được sinh.

2) HumanEval: Để so sánh mã được sinh bởi AskIt với mã viết tay, chúng tôi sử dụng điểm chuẩn HumanEval [8]. Điểm chuẩn HumanEval là một tập dữ liệu gồm 164 tác vụ mã hóa. Mỗi mô tả tác vụ bao gồm một prompt, các trường hợp kiểm tra, và giải pháp viết tay tương ứng. Các trường hợp kiểm tra là các cặp đầu vào-đầu ra, và mã viết tay là một hàm Python vượt qua tất cả các trường hợp kiểm tra. Điểm chuẩn HumanEval được thiết kế để đánh giá hiệu suất của LLM trên các tác vụ mã hóa. Chúng tôi sử dụng điểm chuẩn HumanEval để so sánh mã được sinh bởi AskIt với mã viết tay.

Chúng tôi chỉ định các prompt của điểm chuẩn HumanEval làm mẫu prompt cho AskIt. Chúng tôi chuyển đổi các ví dụ học few-shot được mô tả trong các prompt thành ví dụ đào tạo cho lời gọi define. Chúng tôi sử dụng các trường hợp kiểm tra của điểm chuẩn HumanEval làm ví dụ kiểm tra để kiểm tra tính đúng đắn của mã được sinh.

Kết quả là, chúng tôi đã thành công sinh mã hợp lệ cho 139 trong số 164 tác vụ, có nghĩa là tỷ lệ thành công là 84,8%. Độ chính xác này có thể so sánh với độ chính xác của các LLM hiện đại [20]–[24] trên điểm chuẩn HumanEval. Trung bình, mã được sinh cho 139 tác vụ là 8,05 dòng, trong khi mã nguồn trong AskIt là 23,74 dòng. Mã nguồn dài hơn mã được sinh vì nó bao gồm các dòng bổ sung cho các ví dụ đào tạo và kiểm tra. So với mã viết tay, mã được sinh trung bình dài hơn 1,27 lần, vì độ dài trung bình của mã viết tay là 7,57 dòng.

Hình 5 cho thấy biểu đồ phân tán của LOC (Số dòng Mã) của mã được sinh và mã viết tay. Mỗi điểm trên biểu đồ tương ứng với một tác vụ mã hóa cụ thể, với trục x biểu thị LOC trong phiên bản viết tay và trục y đại diện cho LOC trong phiên bản được sinh. Kích thước của mỗi điểm tỷ lệ thuận với tỷ lệ LOC được sinh so với LOC viết tay. Biểu đồ phân tán này tiết lộ một phân bố rộng của các điểm dữ liệu, cho thấy sự biến đổi trong mối quan hệ giữa độ dài của mã viết tay và mã được sinh. Mặc dù AskIt nói chung tạo ra mã dài hơn mã viết tay, trong 49 trong số 139 tác vụ (35,3%) mã được sinh ngắn hơn. Kết quả này cho thấy rằng AskIt có thể sinh mã ngắn hơn mã viết tay trong một số trường hợp.

B. Các Tác vụ Có thể Trả lời Trực tiếp
Để trả lời RQ3, chúng tôi biến đổi các prompt hiện có cho LLM thành các prompt AskIt được điều chỉnh cho các tác vụ có thể trả lời trực tiếp. Sau đó chúng tôi so sánh độ dài của các prompt gốc với các prompt AskIt. Nguồn của những prompt này là OpenAI Evals². Repository OpenAI Evals chứa hơn 300 điểm chuẩn, đại diện cho các trường hợp sử dụng thực tế của LLM. Đặc biệt, phần lớn những điểm chuẩn này bắt nguồn từ người dùng LLM thực tế.

Mỗi điểm chuẩn trong repository bao gồm nhiều trường hợp kiểm tra. Đến lượt mình, mỗi trường hợp kiểm tra bao gồm một prompt và phản hồi LLM dự kiến. Cho thí nghiệm này, chúng tôi hạn chế tập trung vào 50 điểm chuẩn đầu tiên từ OpenAI Evals. Ngoài ra, chúng tôi chỉ chọn trường hợp kiểm tra đầu tiên từ mỗi

²https://github.com/openai/evals

điểm chuẩn, vì tất cả các trường hợp kiểm tra trong một điểm chuẩn cụ thể chia sẻ một kiểu tương tự nhưng với các đầu vào khác nhau.

Quá trình sửa đổi của chúng tôi cho các prompt AskIt liên quan đến việc loại bỏ thông tin thừa. Điều này bao gồm các cụm từ chỉ định định dạng phản hồi của LLM hoặc nhắc LLM làm sáng tỏ lý do của câu trả lời. Prompt AskIt vốn dĩ kết hợp thông tin như vậy. Ví dụ, xem xét đoạn trích này từ prompt gốc cho điểm chuẩn 2dmovement.dev.v0: "Please note: In the following EXERCISE, it is essential that you only respond with a single line in the format (x, y)." Những chỉ thị như vậy có thể được bỏ qua từ prompt AskIt. Thay vào đó, chúng tôi chi tiết kiểu phản hồi LLM mong đợi trong prompt AskIt. Trong trường hợp nói trên, kiểu phản hồi được chỉ định là { x: number, y: number }.

Cho rằng hầu hết các điểm chuẩn không thể giải quyết được bởi GPT-3.5 và GPT-4, chúng tôi chỉ đảm bảo rằng prompt sửa đổi của chúng tôi mang lại một định dạng đầu ra phù hợp với phản hồi mong đợi của LLM, như được chỉ định trong trường hợp kiểm tra.

Hình 6 trình bày một biểu đồ tóm tắt các giảm độ dài prompt. Trục x đại diện cho phạm vi số ký tự, và trục y hiển thị tần suất của các điểm chuẩn trong mỗi phạm vi. Biểu đồ này minh họa trực quan phân bố của các giảm độ dài prompt đạt được bằng cách biến đổi các prompt gốc thành các prompt AskIt. Trung bình, chúng tôi quan sát giảm 16,14% số ký tự từ các prompt gốc.

Tất cả các kiểu được sử dụng trong các điểm chuẩn được trình bày trong Hình 7. Trục x đại diện cho các kiểu, và trục y hiển thị số lần sử dụng cho mỗi kiểu. Chúng tôi đếm số lần sử dụng theo hai cách: (1) đếm số lần sử dụng của kiểu cấp cao nhất, và (2) đếm số lần sử dụng của tất cả các kiểu. Kiểu cấp cao nhất được sử dụng thường xuyên nhất là string, theo sau là number và boolean. Mặc dù kiểu literal không phải là kiểu cấp cao nhất, nó thường được kết hợp với các kiểu khác.

C. Các Tác vụ Giao nhau
Một trong những lợi ích của giao diện thống nhất được cung cấp bởi AskIt là nó cải thiện hiệu suất của các tác vụ giao nhau bằng cách sử dụng LLM để sinh mã cho tác vụ mà không cần viết lại mẫu prompt. Để trả lời RQ4, chúng tôi so sánh hiệu suất của các hàm trong hai kịch bản: khi sử dụng LLM để trả lời tác vụ trực tiếp, và khi hàm được sinh bởi LLM để thực hiện tác vụ.

Chúng tôi sử dụng điểm chuẩn GSM8K [25], một tập dữ liệu về các bài toán văn chất lượng cao cấp tiểu học. Chúng tôi chuyển đổi các giá trị số được bao quanh bởi khoảng trắng trong mô tả bài toán thành các biến vì các chương trình được sinh thường được tái sử dụng với các giá trị khác nhau trong một thiết lập thực tế.

Chúng tôi sử dụng các giá trị gốc làm ví dụ kiểm tra để kiểm tra tính đúng đắn của chương trình được sinh. Chúng tôi thực thi chương trình theo hai cách sau: (1) thực thi trực tiếp chương trình được sinh bởi AskIt, và (2) biên dịch chương trình được sinh bởi AskIt và thực thi chương trình đã biên dịch. Nếu chương trình được sinh thất bại trong việc vượt qua ví dụ kiểm tra, chúng tôi thử lại lên đến 9 lần để sinh chương trình đúng.

Điểm chuẩn GSM8K bao gồm dữ liệu đào tạo và dữ liệu kiểm tra. Chúng tôi chỉ sử dụng dữ liệu kiểm tra cho đánh giá của chúng tôi vì chúng tôi sử dụng GPT mà không tinh chỉnh với dữ liệu đào tạo. Dữ liệu kiểm tra

--- TRANG 8 ---
BẢNG II: Tóm tắt 50 tác vụ có thể mã hóa được triển khai sử dụng AskIt

# | Mẫu Prompt | Kiểu Trả về trong TypeScript | Kiểu Tham số (Chỉ TypeScript) | TypeScript | Python
--|------------|------------------------------|------------------------------|------------|--------
  |           |                              |                              | LOC | Thử lại | LOC | Thử lại
1 | Đảo ngược chuỗi {{s}}. | string | { s: string } | 5 | 0 | 4 | 0
2 | Tính giai thừa của {{n}}. | number | { n: number } | 9 | 0 | 7 | 0
3 | Nối các chuỗi {{ss}}. | string | { ss: string[] } | 5 | 0 | 5 | 0
4 | Sắp xếp các số {{ns}} theo thứ tự tăng dần. | number[] | { ns: number[] } | 5 | 0 | 5 | 0
5 | Tìm số lớn nhất trong {{ns}}. | number | { ns: number[] } | 3 | 0 | 5 | 0
6 | Kiểm tra xem {{n}} có phải là palindrome không. | boolean | { n: number } | 7 | 0 | 6 | 0
7 | Tính tổng tất cả các số trong {{ns}}. | number | { ns: number[] } | 5 | 0 | 5 | 0
8 | Tính trung bình của tất cả các số trong {{ns}}. | number | { ns: number[] } | 8 | 0 | 5 | 0
9 | Đếm số lần xuất hiện của {{x}} trong {{xs}}. | number | { xs: number[]; x: number } | 6 | 0 | 8 | 0
10 | Loại bỏ tất cả các trường hợp của {{x}} từ {{xs}}. | number[] | { xs: number[]; x: number } | 3 | 0 | 6 | 0
11 | Trả về các phần tử duy nhất trong {{xs}}. | number[] | { xs: number[] } | 5 | 0 | 0 | 0
12 | Tìm giai thừa của {{n}}. | number | { n: number } | 9 | 0 | 7 | 0
14 | Sinh dãy Fibonacci lên đến {{n}}. | number[] | { n: number } | 10 | 0 | 19 | 0
21 | Chuyển đổi đối tượng JSON {{o}} thành chuỗi. | string | { o: any } | 3 | 0 | 0 | 0
24 | Tìm sự khác biệt giữa các ngày {{d1}} và {{d2}}. | number | { d1: Date; d2: Date } | 6 | 0 | 0 | 0

chứa 1.319 bài toán. Chúng tôi sử dụng "gpt-4" làm LLM backend cho AskIt trong thí nghiệm này. Tất cả các đo lường thời gian được thực hiện trên một máy với CPU Apple M1 và 16GB RAM.

Trong TypeScript, 1.138 bài toán đã được giải quyết bởi GPT-4. Trong Python, GPT-4 trực tiếp giải quyết 1.159 bài toán. Sự khác biệt không đáng kể. Vì cả hai triển khai đều sử dụng cùng mô hình GPT và cùng prompt, sự khác biệt có vẻ đến từ tính ngẫu nhiên của phản hồi của GPT-4. Chúng tôi sử dụng 1.138 và 1.159 bài toán này để sinh chương trình. Chúng tôi đã thành công sinh chương trình cho 1.114 và 1.134 bài toán trong TypeScript và Python, tương ứng.

Kết quả được hiển thị trong Bảng III cho TypeScript và Python, tương ứng. Độ trễ là thời gian để có được câu trả lời từ LLM. Thời gian thực thi là thời gian để thực thi hàm được sinh. Thời gian biên dịch là thời gian để sinh hàm cho bài toán. Trung bình, các mã được sinh trả lời bài toán nhanh hơn 275.092,55x và 6.969.904,73x lần trong TypeScript và Python, tương ứng, so với việc sử dụng LLM trực tiếp để trả lời bài toán. Trong khi tỷ lệ tăng tốc khác nhau giữa TypeScript và Python, mã được sinh nhanh hơn đáng kể so với LLM trong cả hai trường hợp.

V. CÔNG TRÌNH LIÊN QUAN
LMQL [26] là một ngôn ngữ truy vấn được thiết kế đặc biệt cho các mô hình ngôn ngữ lớn (LLM), kết hợp các prompt ngôn ngữ tự nhiên với tính biểu đạt của Python. Nó cung cấp các tính năng như ràng buộc, gỡ lỗi, truy xuất, và luồng điều khiển để tạo điều kiện tương tác với LLM. LMQL cung cấp hỗ trợ Python đầy đủ, cho phép luồng điều khiển và logic mạnh mẽ trong logic prompting. LMQL cho phép các nhà phát triển mô hình khai báo các ràng buộc logic chi phối đầu ra mô hình. Những điều này được chuyển thành "mặt nạ dự đoán cấp token" - token là những gì LLM xử lý. Trong khi nó hỗ trợ ràng buộc kiểu, các kiểu được hỗ trợ hạn chế và không được tích hợp với hệ thống kiểu của ngôn ngữ lập trình cơ bản. Ví dụ, LMQL không hỗ trợ khả năng định nghĩa các kiểu tùy chỉnh.

LLMChain³ là một thư viện cung cấp một mẫu prompt hỗ trợ các tham số như AskIt. Nó sinh các prompt bằng cách điền vào mẫu với các tham số. Trong khi LLMChain cung cấp hàm apply_and_parse để phân tích phản hồi của LLM, người dùng cần chỉ định parser để trích xuất câu trả lời từ phản hồi. Mặt khác, AskIt tự động phân tích phản hồi và trích xuất câu trả lời từ phản hồi dựa trên thông tin kiểu. API OpenAI⁴ là một API cấp thấp cho LLM. Phiên bản mới nhất của API OpenAI hỗ trợ

³https://docs.langchain.com/docs/components/chains/llm-chain
⁴https://beta.openai.com/docs/api-reference

gọi hàm. Người dùng có thể đăng ký một hàm do người dùng định nghĩa với LLM và gọi nó từ prompt. Đối số cho hàm được truyền như một đối tượng JSON. Bằng cách khai thác tính năng này, người dùng có thể có được câu trả lời trong một định dạng cụ thể. Tuy nhiên, mã phức tạp hơn việc sử dụng AskIt, vì người dùng cần viết mã để đăng ký hàm, và làm cho nó có thể gọi từ prompt. Trong khi AskIt không sử dụng tính năng này, nó có thể được sử dụng để triển khai AskIt.

Khác với AskIt, LMQL, LLMChain, và API OpenAI không hỗ trợ sinh mã. Trong khi mã có thể được sinh sử dụng chúng nếu người dùng viết prompt tương ứng, mã được sinh không thể được tích hợp liền mạch vào phần còn lại của chương trình. Với AskIt, việc chuyển đổi giữa việc sử dụng LLM trực tiếp và sử dụng mã được sinh có thể được thực hiện mà không cần thay đổi mẫu prompt.

Một cách tiếp cận khác để tích hợp LLM vào lập trình là cho phép LLM sử dụng API để chúng có thể truy cập các cơ sở kiến thức rộng hơn và năng động hơn, cũng như thực hiện các tác vụ tính toán phức tạp. Thách thức là sự phức tạp của việc tích hợp hàng triệu API thay đổi, có thể có các chức năng chồng chéo và những hạn chế tinh vi. Gorilla [27] đề xuất sử dụng tinh chỉnh tự hướng dẫn và truy xuất để cho phép LLM chọn chính xác từ các tập hợp công cụ lớn, chồng chéo, và thay đổi được thể hiện qua API và tài liệu API của chúng.

VI. CÔNG VIỆC TƯƠNG LAI
Một hạn chế của AskIt là nó không đảm bảo an toàn của mã được sinh. Nếu hàm được sinh chứa mã có hại, nó có thể gây ra rủi ro bảo mật. Ví dụ, hàm được sinh có thể chứa mã bất ngờ xóa tất cả các tệp trong một thư mục. Triển khai hiện tại dựa vào việc xem xét của người dùng đối với mã được sinh. Để giảm thiểu rủi ro này, cần phát triển một cơ chế đảm bảo an toàn của mã được sinh. Các cách tiếp cận có thể bao gồm sử dụng sandbox hoặc công cụ phân tích tĩnh.

Một cải tiến khác sẽ là sinh mã hiệu quả hơn. Mặc dù chúng tôi hiện tại sinh mã trong TypeScript và Python, mã này không được tối ưu hóa. Một cách tiếp cận tiềm năng liên quan đến việc sử dụng LLM để sinh mã trong các ngôn ngữ cấp thấp, chẳng hạn như C hoặc LLVM IR, với giao diện hàm ngoại.

VII. KẾT LUẬN
Trong bài báo này, chúng tôi đã giới thiệu một ngôn ngữ đặc thù miền (DSL), AskIt. AskIt cung cấp một giao diện thống nhất để tương tác với các mô hình ngôn ngữ lớn (LLM) cho các tác vụ khác nhau. Giao diện thống nhất hỗ trợ (1) Điều khiển đầu ra có hướng dẫn kiểu của LLM, (2) Định nghĩa hàm dựa trên mẫu, và (3) Khả năng sinh mã cho các tác vụ có thể mã hóa. Kết quả thí nghiệm cho thấy AskIt giảm số dòng mã cần thiết để triển khai các tác vụ có thể mã hóa 6,56 và 5,52 dòng cho TypeScript và Python, tương ứng. AskIt cũng giảm số dòng sinh prompt cho các tác vụ có thể trả lời trực tiếp 16,14%. Mã được sinh cho các tác vụ giao nhau nhanh hơn trung bình 275.092,55x và 6.969.904,73x lần so với việc sử dụng LLM trực tiếp để trả lời bài toán trong TypeScript và Python, tương ứng.

--- TRANG 9 ---
BẢNG III: Kết quả thí nghiệm sử dụng GSM8K

Các Chỉ số Trung bình | TypeScript | Python
---------------------|------------|--------
Độ trễ (s) | 13,28 | 22,97
Thời gian Thực thi (µs) | 49,11 | 5,09
Thời gian Biên dịch (s) | 14,19 | 20,38
Tỷ lệ Tăng tốc | 275092,55 | 6969904,73

PHỤ LỤC
A. Tóm tắt
Phụ lục được thiết kế để tạo điều kiện tái tạo các thí nghiệm được thực hiện trong bài báo này. Nó bao gồm một container Docker, được bổ sung bởi một tập hợp các script, để tái tạo liền mạch các kết quả được hiển thị trong Hình 5, Hình 6, và Hình 7, cũng như dữ liệu được trình bày trong Bảng II và Bảng III.

B. Danh sách kiểm tra artifact (meta-information)
• Tập dữ liệu: HumanEval, OpenAI Evals, GSM8K
• Môi trường runtime: Bất kỳ hệ điều hành nào hỗ trợ Docker
• Phần cứng: Bất kỳ máy nào có Docker được cài đặt
• Chỉ số: Số dòng mã, giảm độ dài prompt, tốc độ tăng
• Đầu ra: Hình 5, 6, 7, Bảng II, Bảng III
• Thí nghiệm:
• Cần bao nhiêu không gian đĩa (khoảng)?: 5GB
• Cần bao nhiêu thời gian để chuẩn bị luồng công việc (khoảng)?: 15 phút
• Cần bao nhiêu thời gian để hoàn thành thí nghiệm (khoảng)?: 1 ngày
• Có sẵn công khai?: Có
• Giấy phép mã (nếu có sẵn công khai): Giấy phép MIT
• Framework luồng công việc được sử dụng?: Không, nhưng chúng tôi cung cấp các script để tái tạo thí nghiệm
• Được lưu trữ (cung cấp DOI)?: Có

C. Mô tả
1) Cách phân phối: Artifact này có sẵn tại https://github.com/katsumiok/askit-artifact.git và được lưu trữ tại https://doi.org/10.5281/zenodo.10327179 [28].
2) Phụ thuộc phần cứng: Không có phụ thuộc phần cứng.
3) Phụ thuộc phần mềm: Artifact được phân phối dưới dạng container Docker. Phụ thuộc phần mềm duy nhất là Docker.
4) Tập dữ liệu: Artifact bao gồm các tập dữ liệu sau:
• HumanEval
• OpenAI Evals
• GSM8K
Những tập dữ liệu này được sửa đổi để tương thích với AskIt.

D. Cài đặt
1) Clone artifact từ repository GitHub.
```
$ git clone https://github.com/katsumiok/askit-artifact.git
```
2) Cài đặt Docker.
3) Chỉnh sửa askit-artifact/Dockerfile để chỉ định khóa API cho OpenAI API. Cập nhật dòng sau trong askit-artifact/Dockerfile:
```
ENV OPENAI_API_KEY sk-xxxxxxxxxxxxxxxxxxxxxxxx
```
với khóa API của bạn.
4) Xây dựng image Docker từ Dockerfile trong thư mục artifact.
```
$ cd askit-artifact
$ docker build -t askit .
```

BẢNG IV: Các tệp được sinh bởi Luồng công việc Thí nghiệm

Tệp | Hình/Bảng
-----|----------
fig/loc.pdf | Hình 5
fig/prompt_reduction.pdf | Hình 6
fig/type_count.pdf | Hình 7
tab/common_tasks.tex | Bảng II
tab/gsm8k.tex | Bảng III

E. Luồng công việc thí nghiệm
Trước khi chạy thí nghiệm, bạn cần khởi động container Docker bằng cách chạy lệnh sau trong thư mục artifact:
```
$ docker run -it -v $PWD:/root/docker-artifact askit
```

Để tái tạo kết quả được hiển thị trong Hình 5, Hình 6, và Hình 7, cũng như dữ liệu được trình bày trong Bảng II và Bảng III, chạy các lệnh sau:
```
$ make
```
Lệnh trên chạy các script để sinh các hình và bảng từ kết quả thí nghiệm đã được bao gồm trong artifact.

Để tái tạo kết quả thí nghiệm từ đầu, loại bỏ các tệp trung gian bằng cách chạy lệnh sau:
```
$ make clean_all
```
và sau đó chạy lệnh sau:
```
$ ./run_all.sh
```
Lệnh trên chạy tất cả các thí nghiệm và sinh các tệp trung gian được sử dụng để sinh các hình và bảng. Gõ make một lần nữa sẽ sinh các hình và bảng từ các tệp trung gian. Lưu ý rằng make không chạy thí nghiệm và chỉ sinh các hình và bảng từ các tệp trung gian.

F. Đánh giá và kết quả mong đợi
Luồng công việc sinh các tệp được liệt kê trong Bảng IV. Những hình và bảng này có thể hơi khác so với những cái được hiển thị trong bài báo do tính ngẫu nhiên của các mô hình ngôn ngữ và sự khác biệt trong môi trường. Tuy nhiên, các hình và bảng nên tương tự như những cái được hiển thị trong bài báo.

G. Tùy chỉnh thí nghiệm
Bảng V hiển thị các script để tùy chỉnh thí nghiệm. Những script này được thực thi bởi run_all.sh. Mỗi script đặt biến môi trường ASKIT_MODEL để chỉ định mô hình ngôn ngữ được sử dụng trong thí nghiệm. Bằng cách đặt biến môi trường ASKIT_MODEL trong những script này, bạn có thể tùy chỉnh thí nghiệm để sử dụng một mô hình khác. Ví dụ, chúng tôi chỉ định "gpt-4" trong GSM8K/ts/run.sh để sử dụng GPT-4 cho thí nghiệm trên GSM8K trong TypeScript. Bạn có thể thay đổi nó thành "gpt-3.5-turbo-16k" để sử dụng GPT-3.5 Turbo 16K thay thế.

Phần còn lại của phần này mô tả chi tiết các thí nghiệm trong mỗi phần.

1) Thí nghiệm trong IV-A1: 50 Tác vụ Mã hóa Phổ biến: Trong thí nghiệm này, chúng tôi chạy AskIt trên 50 tác vụ mã hóa phổ biến trong TypeScript và Python. Mã nguồn của các tác vụ được bao gồm trong artifact trong các tệp sau:
• coding/ts-askit/examples/src/top50def.ts
• coding/pyaskit/examples/top50.py

coding/ts-askit/examples/run.sh và coding/pyaskit/examples/run.sh chạy AskIt trên các tác vụ trong TypeScript và Python, tương ứng. Sau khi chạy những script này, mã được sinh được lưu trong coding/ts-askit/examples/src/askit và coding/pyaskit/examples/askit, tương ứng. coding/make_table.py sinh Bảng II từ mã được sinh.

--- TRANG 10 ---
0 50 100 150 200 250 300 350 400
Giảm độ dài prompt (ký tự)01020Số lượng

Hình 6: Biểu đồ tần suất giảm số ký tự trong AskIt so với prompt gốc

booleanobjectArrayliteralnumber stringunion01020Số lượngTất cả các kiểu
Các kiểu cấp cao nhất

Hình 7: Số lần sử dụng cho mỗi kiểu

BẢNG V: Các Script để Tùy chỉnh Thí nghiệm

Script | Mô tả | Phần
-------|--------|-----
coding/ts-askit/examples/run.sh | Chạy AskIt trên 50 tác vụ mã hóa phổ biến trong TypeScript | Phần IV-A1
coding/pyaskit/examples/run.sh | Chạy AskIt trên 50 tác vụ mã hóa phổ biến trong Python | Phần IV-A1
HumanEval/run.sh | Chạy AskIt trên HumanEval trong Python | Phần IV-A2
openai_evals/run.sh | Chạy AskIt trên OpenAI Evals trong Python | Phần IV-B
GSM8K/ts/run.sh | Chạy AskIt trên GSM8K trong TypeScript | Phần IV-C
GSM8K/python/run.sh | Chạy AskIt trên GSM8K trong Python | Phần IV-C

2) Thí nghiệm trong IV-A2: HumanEval: Trong thí nghiệm này, chúng tôi chạy AskIt trên HumanEval trong Python và so sánh mã được sinh với mã viết tay. Có 164 tác vụ trong HumanEval. HumanEval/HumanEval.jsonl là dữ liệu HumanEval gốc, chứa mã viết tay và prompt. 164 tác vụ được viết trong AskIt được đại diện bởi các tệp có tên từ 0.py đến 163.py trong thư mục HumanEval/HumanEval. Sau khi chạy HumanEval/run.sh, mã được sinh được lưu trong HumanEval/askit. HumanEval/make_table.py sinh Hình 5 từ mã được sinh và mã viết tay.

3) Thí nghiệm trong IV-B: OpenAI Evals: Trong thí nghiệm này, chúng tôi so sánh độ dài của các prompt được sinh bởi AskIt với các prompt gốc trong OpenAI Evals. Các prompt cho AskIt được lưu trữ trong openai_evals/data, và các prompt gốc được lưu trữ trong openai_evals/odata ở định dạng JSON. openai_evals/run.sh chạy AskIt trên các prompt trong openai_evals/data để kiểm tra xem các phản hồi của LLM có tương ứng với các kiểu mong đợi không. openai_evals/make_table.py sinh Hình 6 và Hình 7 bằng cách so sánh các prompt được sinh với các prompt gốc.

4) Thí nghiệm trong IV-C: GSM8K: Trong thí nghiệm này, chúng tôi chạy GSM8K trong TypeScript và Python trong hai thiết lập: có và không có sinh mã. Có 1319 tác vụ trong GSM8K. Các tệp có tên từ 0.ts đến 1318.ts trong thư mục GSM8K/ts/src chứa 1319 tác vụ được viết trong TypeScript và được sử dụng để chạy GSM8K mà không sinh mã. Tương tự, các tệp có tên từ 0.ts đến 1318.ts trong thư mục GSM8K/ts/src2 là các tác vụ được viết trong TypeScript, được sử dụng để chạy GSM8K với sinh mã. GSM8K/ts/run.sh chuyển đổi các tệp TypeScript trong cả GSM8K/ts/src và GSM8K/ts/src2 và chạy các tệp JavaScript được sinh. Tuy nhiên, GSM8K/ts/run.sh chỉ sinh các hàm cho các tác vụ trong GSM8K/ts/src2 và không sinh các hàm cho các tác vụ trong GSM8K/ts/src. Sau khi chạy GSM8K/ts/run.sh, mã được sinh được lưu trong GSM8K/ts/src2/askit. Các chỉ số, bao gồm thời gian thực thi, được lưu trong GSM8K/ts/json và GSM8K/ts/json2 dưới dạng các tệp JSON có tên từ 0.json đến 1318.json, tương ứng.

Một tệp có tên GSM8K/test.jsonl chứa mô tả tác vụ ở định dạng JSON. GSM8K/python/run.py, được thực thi bởi GSM8K/python/run.sh, chạy GSM8K trong Python sử dụng GSM8K/test.jsonl. Sau khi chạy GSM8K/python/run.py, mã được sinh được lưu trong GSM8K/python/askit. Các chỉ số, bao gồm thời gian thực thi, được lưu trong GSM8K/python/json dưới dạng các tệp JSON có tên từ 0.json đến 1318.json. Những tệp JSON này chứa các chỉ số cho cả các tác vụ có và không có sinh mã.

GSM8K/make_table.py sinh Bảng III từ các chỉ số được tạo ra bởi GSM8K/ts/run.sh và GSM8K/python/run.sh.

TÀI LIỆU THAM KHẢO
[1] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, và W. Fedus, "Emergent abilities of large language models," 2022.
[2] D. Saxton, E. Grefenstette, F. Hill, và P. Kohli, "Analysing mathematical reasoning abilities of neural models," 2019.
[3] P. Lewis, L. Denoyer, và S. Riedel, "Unsupervised question answering by cloze translation," trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2019. [Trực tuyến]. Có sẵn: https://doi.org/10.18653%2Fv1%2Fp19-1484
[4] A. Narayan, P. Talukdar, và C. Cardie, "Abstractive text summarization using pointer-generator networks," arXiv preprint arXiv:1801.07038, 2018.
[5] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, D. Amodei, và T. Mordatch, "Language models are few-shot learners," arXiv preprint arXiv:2005.14165, 2020.
[6] A. Prakash, S. A. Hasan, K. Lee, V. Datla, A. Qadir, J. Liu, và O. Farri, "Neural paraphrase generation with stacked residual lstm networks," 2016.
[7] A. Srivastava, E. Grefenstette, D. Das, I. Sutskever, và R. S. Zemel, "Improving language understanding by generating text," arXiv preprint arXiv:1801.06146, 2018.
[8] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, và W. Zaremba, "Evaluating large language models trained on code," 2021.
[9] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al., "Competition-level code generation with alphacode," Science, vol. 378, no. 6624, pp. 1092–1097, 2022.
[10] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W. tau Yih, L. Zettlemoyer, và M. Lewis, "Incoder: A generative model for code infilling and synthesis," 2023.
[11] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. Murthy, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, và H. de Vries, "Starcoder: may the source be with you!" 2023.
[12] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, và G. Synnaeve, "Code llama: Open foundation models for code," 2023.
[13] N. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Rajamani, và R. Sharma, "Jigsaw: Large language models meet program synthesis," trong Proceedings of the 44th International Conference on Software Engineering, ser. ICSE '22. New York, NY, USA: Association for Computing Machinery, 2022, p. 1219–1231. [Trực tuyến]. Có sẵn: https://doi.org/10.1145/3510003.3510203
[14] OpenAI, "Gpt-4 technical report," 2023.
[15] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, và J. Schulman, "Training verifiers to solve math word problems," 2021.
[16] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, và Y. Iwasawa, "Large language models are zero-shot reasoners," trong Advances in Neural Information Processing Systems, vol. 35, 2022, pp. 22 199–22 213.

--- TRANG 11 ---
5 10 15 20 25 30
Số dòng mã viết tay5101520253035 Số dòng mã được sinh

Hình 5: Biểu đồ phân tán của LOC của mã được sinh và mã viết tay

[17] S. Gulwani, "Automating string processing in spreadsheets using input-output examples," SIGPLAN Not., vol. 46, no. 1, p. 317–330, jan 2011. [Trực tuyến]. Có sẵn: https://doi.org/10.1145/1925844.1926423
[18] D. E. Shaw, W. R. Swartout, và C. C. Green, "Inferring lisp programs from examples." trong IJCAI, vol. 75, 1975, pp. 260–267.
[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, và D. Zhou, "Chain-of-thought prompting elicits reasoning in large language models," trong Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, và A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022, pp. 24 824–24 837. [Trực tuyến]. Có sẵn: https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf
[20] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang, J. Wang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu, và J. Schmidhuber, "Metagpt: Meta programming for a multi-agent collaborative framework," 2023.
[21] E. Zelikman, Q. Huang, G. Poesia, N. D. Goodman, và N. Haber, "Parsel: Algorithmic reasoning with language models by composing decompositions," 2023.
[22] D. Huang, Z. Nan, X. Hu, P. Jin, S. Peng, Y. Wen, R. Zhang, Z. Du, Q. Guo, Y. Pu, và Y. Chen, "Anpl: Compiling natural programs with interactive decomposition," 2023.
[23] N. Muennighoff, Q. Liu, A. Zebaze, Q. Zheng, B. Hui, T. Y. Zhuo, S. Singh, X. Tang, L. von Werra, và S. Longpre, "Octopack: Instruction tuning code large language models," 2023.
[24] A. Zhou, K. Yan, M. Shlapentokh-Rothman, H. Wang, và Y.-X. Wang, "Language agent tree search unifies reasoning acting and planning in language models," 2023.
[25] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, và J. Schulman, "Training verifiers to solve math word problems," CoRR, vol. abs/2110.14168, 2021. [Trực tuyến]. Có sẵn: https://arxiv.org/abs/2110.14168
[26] L. Beurer-Kellner, M. Fischer, và M. Vechev, "Prompting is programming: A query language for large language models," Proc. ACM Program. Lang., vol. 7, no. PLDI, jun 2023. [Trực tuyến]. Có sẵn: https://doi.org/10.1145/3591300
[27] S. G. Patil, T. Zhang, X. Wang, và J. E. Gonzalez, "Gorilla: Large language model connected with massive apis," arXiv preprint arXiv:2305.15334, 2023.
[28] K. Okuda, "Artifact for cgo'24 paper: 'askit: Unified programming interface for programming with large language models'," https://doi.org/10.5281/zenodo.10327179, 2023.
