# 2207.05987.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2207.05987.pdf
# Kích thước tệp: 1645179 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản tại hội nghị ICLR 2023
DocPrompting : TẠO MÃ BẰNG CÁCH TRUY XUẤT
TÀI LIỆU HƯỚNG DẪN
Shuyan Zhou†, Uri Alon†
Frank F. Xu†,Zhiruo Wang†,Zhengbao Jiang†,Graham Neubig†‡
†Viện Công nghệ Ngôn ngữ, Đại học Carnegie Mellon,
‡Inspired Cognition
fshuyanzh,ualon,fangzhex,zhiruow,zhengbaj,gneubig g@cs.cmu.edu
TÓM TẮT
Các thư viện mã nguồn công khai đang liên tục phát triển và thay đổi.
Điều này làm cho việc các mô hình mã theo kịp tất cả các API có sẵn trở nên bất khả thi
chỉ bằng cách huấn luyện những mô hình này trên các kho lưu trữ mã hiện có. Do đó, các
mô hình hiện tại về bản chất không thể tổng quát hóa để sử dụng các hàm và thư viện chưa
thấy, vì chúng sẽ không bao giờ xuất hiện trong dữ liệu huấn luyện của chúng. Ngược lại, khi các
lập trình viên con người sử dụng các hàm và thư viện lần đầu tiên, họ thường xuyên tham khảo
các tài nguyên văn bản như hướng dẫn mã và tài liệu, để khám phá và
hiểu chức năng có sẵn. Được truyền cảm hứng từ quan sát này, chúng tôi giới thiệu
DocPrompting : một phương pháp tạo mã từ ngôn ngữ tự nhiên mà một cách rõ ràng
tận dụng tài liệu mã bằng cách (1) truy xuất các phần tài liệu liên quan
với một ý định ngôn ngữ tự nhiên (NL), và (2) tạo mã dựa trên ý định NL
và tài liệu đã truy xuất. DocPrompting là tổng quát: nó có thể được áp
dụng cho bất kỳ ngôn ngữ lập trình nào, và không phụ thuộc vào mô hình neural cơ bản.
Chúng tôi chứng minh rằng DocPrompting liên tục cải thiện các mô hình NL-to-code:
DocPrompting cải thiện các mô hình cơ sở mạnh như CodeT5 đến 2.85% trong pass@ 1
(tăng tương đối 52%) và 4.39% trong pass@ 10(tăng tương đối 30%) trong đánh giá
dựa trên thực thi trên benchmark Python CoNaLa phổ biến; trên bộ dữ liệu Bash mới
tldr ,DocPrompting cải thiện CodeT5 và GPT-Neo-1.3B lên đến 6.9% khớp chính xác tuyệt đối.1
1 GIỚI THIỆU
Chúng tôi giải quyết nhiệm vụ tạo mã từ ngôn ngữ tự nhiên (NL →code): tạo một đoạn mã,
được viết bằng ngôn ngữ lập trình đa năng như Python hoặc Bash, cho trước một ý định
ngôn ngữ tự nhiên. Nhiệm vụ này đã thấy sự tăng trưởng phổ biến mạnh mẽ gần đây do sự xuất hiện của các mô hình ngôn ngữ lớn được huấn luyện trên lượng lớn ngôn ngữ tự nhiên và mã (Chen et al., 2021; Xu et al., 2022;
Fried et al., 2022). Các mô hình NL →code tạo điều kiện lập trình cho cả lập trình viên chuyên nghiệp và thiếu kinh nghiệm, bằng cách cho phép các lập trình viên viết mã chỉ bằng cách thể hiện ý định cấp cao hơn của họ.
Nhiều mô hình tạo mã hiện tại hoặc học trực tiếp từ các cặp đầu vào-đầu ra được cung cấp như
dữ liệu huấn luyện (Allamanis et al., 2015; Yin and Neubig, 2017; Iyer et al., 2018; Brockschmidt et al.,
2019; Xu et al., 2020; Alon et al., 2020; Wang et al., 2021), hoặc học ánh xạ giữa đầu vào
và đầu ra một cách ngầm định từ các kho ngữ liệu tự nhiên xảy ra của ngôn ngữ tự nhiên và mã đan xen
(Austin et al., 2021; Nijkamp et al., 2022). Tuy nhiên, tất cả các công trình này giả định rằng tất cả các thư viện
và lời gọi hàm đã được thấy trong dữ liệu huấn luyện ; và ở thời điểm kiểm tra, mô hình đã huấn luyện sẽ chỉ cần
tạo ra các thư viện và lời gọi hàm đã thấy. Tuy nhiên, các hàm và thư viện mới được giới thiệu
liên tục, và ngay cả một lời gọi hàm đã thấy cũng có thể có các đối số chưa thấy. Do đó, những mô hình hiện tại này
về bản chất không thể tổng quát hóa để tạo ra những cách sử dụng chưa thấy như vậy.
Trái ngược với những mô hình hiện tại này, các lập trình viên con người thường xuyên tham khảo hướng dẫn và tài
liệu khi viết mã (Nykaza et al., 2002; Lethbridge et al., 2003). Điều này cho phép con người
dễ dàng sử dụng các hàm và thư viện mà họ chưa bao giờ thấy hoặc sử dụng trước đây. Được truyền cảm hứng từ khả năng này,
1Dữ liệu và mã có sẵn tại https://github.com/shuyanzhou/docprompting .
1arXiv:2207.05987v3  [cs.CL]  18 Feb 2023

--- TRANG 2 ---
Được xuất bản tại hội nghị ICLR 2023
Tạo HTML với syntax highlighting Python cho "print('reading docs')"Bộ truy xuấtBộ tạo
/u1D49Fncd1d2d3Pygment là một syntax highlighter tổng quátMột lexer chia nguồn thành tokens, fragments …  class PythonLexer Cho mã nguồn PythonMột formatter lấy luồng token và ghi nó vào tệp đầu ra …  class HtmlFormatter Định dạng tokens như HTML 4 <span> tags với …from pygments import *code = 'print("reading docs")'s = highlight(code, PythonLexer(),              HtmlFormatter())
Hình 1: DocPrompting : cho trước một ý định NL n
, bộ truy xuất truy xuất một tập các tài liệu liên quan
{d1
;d2
;d3
}từ một nhóm tài liệu D
. Sau đó, bộ tạo tạo ra mã c
dựa trên
NL và docs đã truy xuất. DocPrompting cho phép mô hình tổng quát hóa đến các cách sử dụng chưa thấy trước đây
bằng cách đọc những docs đó. Chữ nghiêng màu xanh làm nổi bật các token được chia sẻ giữa NL và docs; Chữ đậm
hiển thị các token được chia sẻ giữa docs và đoạn mã.
chúng tôi đề xuất DocPrompting : một phương pháp tạo mã học cách truy xuất tài liệu mã
trước khi tạo mã. Một tổng quan về phương pháp của chúng tôi được minh họa trong Hình 1: Đầu tiên, một bộ truy xuất tài liệu
sử dụng ý định NL n
để truy xuất tài liệu mã liên quan {d1
;d2
;d3
}từ một nhóm tài
liệu D
. Sau đó, một bộ tạo mã sử dụng những docs này trong prompt của nó để tạo ra mã
tương ứng c
. Nhóm tài liệu phục vụ như một kho dữ liệu bên ngoài có thể được cập nhật thường xuyên
với nội dung mới (ví dụ, tài liệu của các thư viện mới phát hành), mà không cần huấn luyện lại bất kỳ thành phần
mô hình nào. Theo cách này, DocPrompting có thể tận dụng tài liệu mới được thêm vào, và nó có thể tạo
mã chứa các hàm và thư viện chưa thấy và chưa sử dụng. DocPrompting là tổng quát và có thể áp dụng
cho bất kỳ ngôn ngữ lập trình và kiến trúc cơ sở nào. Theo hiểu biết tốt nhất của chúng tôi, đây là
minh chứng đầu tiên về việc tận dụng tài liệu trong các mô hình mã một cách rõ ràng và hiệu quả.
Chúng tôi chứng minh hiệu quả của DocPrompting trên hai benchmark và nhiệm vụ NL→code, trên
hai ngôn ngữ lập trình, và sử dụng một số mô hình cơ sở: GPT-Neo (Black et al., 2021), T5 (Raffel
et al., 2020), CodeT5 (Wang et al., 2021), Fusion-in-Decoder (Izacard and Grave, 2021)), và Codex
(Chen et al., 2021). Hơn nữa, chúng tôi thử nghiệm với cả bộ truy xuất thưa thớt như BM25 (Robertson and
Jones, 1976) và các mô hình truy xuất dày đặc như SimCSE (Gao et al., 2021). Cuối cùng, chúng tôi giới thiệu
hai benchmark mới cho việc tạo mã dựa trên truy xuất: (a) trong Bash, chúng tôi tuyển chọn một benchmark mới
bằng cách thu thập kho lưu trữ tldr, và xây dựng các phần huấn luyện/phát triển/kiểm tra mà không có
các lệnh chồng chéo; (b) trong Python, chúng tôi chia lại benchmark CoNaLa phổ biến (Yin et al., 2018)
bằng cách làm cho mọi ví dụ kiểm tra chứa ít nhất một hàm Python không được thấy trong dữ liệu huấn luyện.
Các mô hình sử dụng DocPrompting liên tục vượt trội hơn các mô hình cơ sở của chúng mà tạo mã hoàn toàn
dựa trên các ý định NL. Sử dụng DocPrompting cải thiện các mô hình cơ sở mạnh như CodeT5 đến 2.85% trong pass@ 1(tăng tương đối 52%) và 4.39% trong pass@ 10(tăng tương đối 30%) trong đánh giá
dựa trên thực thi trong CoNaLa ; trên bộ dữ liệu tldr mới, DocPrompting cải thiện CodeT5 và GPT-Neo-
1.3B lên đến 6.9% khớp chính xác tuyệt đối. Chúng tôi phát hành các benchmark mới của mình, bao gồm chú thích
của các tài liệu oracle cho mỗi ví dụ và các nhóm tài liệu, để phục vụ như một bệ thử nghiệm cho
các mô hình tạo mã dựa trên truy xuất trong tương lai.
2 TẠO MÃ BẰNG CÁCH ĐỌC TÀI LIỆU
Giả định cơ bản của chúng tôi là tài liệu mã là tài nguyên đầy đủ nhất nhưng súc tích nhất
cho hầu hết các thư viện và ngôn ngữ lập trình (Roehm et al., 2012), và tài liệu cho phép
tổng quát hóa hiệu quả đến các thư viện và hàm chưa thấy (Forward and Lethbridge, 2002). Chúng tôi theo
mô hình truy xuất-rồi-tạo (Lewis et al., 2020; Guu et al., 2020), tập trung vào việc truy xuất
tài liệu . Trong phần này, chúng tôi mô tả phương pháp tổng quát của DocPrompting ; trong§3 và §6.2,
chúng tôi trình bày chi tiết và thử nghiệm với các triển khai thực tế của DocPrompting .
Công thức Cho trước ý định NL n, mục tiêu của chúng tôi là tạo ra một đoạn mã tương ứng cđược viết bằng
một ngôn ngữ lập trình (PL) nào đó như Python. Chúng tôi giả định rằng một mô hình có quyền truy cập vào một bộ sưu tập
tài liệu mã D. Mỗi tài liệu di∈Dmô tả việc sử dụng của một thư viện, một hàm, hoặc một
2

--- TRANG 3 ---
Được xuất bản tại hội nghị ICLR 2023
đối số trong PL đó. Việc xây dựng Dlà linh hoạt: nó có thể là một tập hợp toàn diện của tất cả
các thư viện và hàm có sẵn trong một PL, hoặc một tập con tùy chỉnh cho phạm vi của một dự án cụ thể.
2.1 BỐI CẢNH : TẠO ĐƯỢC ĐIỀU KIỆN BỞITUY XUẤT
Mặc dù một mô hình có thể sử dụng toàn bộ bộ sưu tập tài liệu D, chỉ một số ít tài liệu trong D
liên quan đến bất kỳ ý định cụ thể nào. Hơn nữa, thường không khả thi về mặt tính toán để trực tiếp điều kiện
trên toàn bộ, không giới hạn, bộ sưu tập tài liệu trong khi đưa ra dự đoán. Do đó, chúng tôi đầu tiên để
mô hình chọn một tập con tài liệu Dn={d1;d2;::;d k}⊆Dpotentially có khả năng liên quan cho trước n,
và tham khảo tập con này trong khi tạo c.
Tổng thể, chúng tôi phân tách xác suất tạo cthành xác suất chọn một
tập con tài liệu cụ thể P(Dn/divides.alt0D;n), và xác suất tạo mã điều kiện trên
ý định và các tài liệu đã chọn P(c/divides.alt0Dn;n); cuối cùng, chúng tôi marginalize trên tất cả Dn⊆D:
P(c/divides.alt0D;n)=/summation.dispDn⊆DP(c/divides.alt0Dn;n)⋅P(Dn/divides.alt0D;n) (1)
giả định rằng cindependent của DchoĐn(có nghĩa là, (c/upmodelsD/divides.alt0Dn)). Vì việc liệt kê tất cả các tập con
có thể Dnlà không khả thi về mặt tính toán, chúng tôi theo thực hành phổ biến và xấp xỉ
marginalization trên Dntrong Phương trình (1) bằng cách lấy tập con tài liệu được truy xuất có xác suất cao nhất
^Dn, và sau đó điều kiện dự đoán của ctrên những tài liệu có khả năng nhất này:
^Dn∶=argmaxDn⊆DP(Dn/divides.alt0D;n) P(c/divides.alt0D;n)≈P(c/divides.alt0^Dn;n)⋅P(^Dn/divides.alt0D;n) (2)
2.2 DocPrompting : TẠO MÃ BẰNG CÁCH TRUY XUẤT TÀI LIỆU
Phương trình 2 ngụ ý rằng DocPrompting dựa vào hai thành phần chính: Một bộ truy xuất Rtruy xuất
các tài liệu liên quan ^Dncho trước ý định n; và một bộ tạo Gtạo đoạn mã cđiều kiện
trên các tài liệu đã truy xuất ^Dnvà ý định n, tạo thành một prompt mới. Cụ thể, R
tính toán điểm tương tự s(di;n)giữa một ý định nvà mọi tài liệu di∈D. Do đó, tập con
^Dn⊆Dlà top- ktài liệu với điểm tương tự cao nhất: ^Dn=top-kdi∈D(s(di;n)).
Một tổng quan về phương pháp của chúng tôi được minh họa trong Hình 1: cho trước ý định Tạo HTML với python
syntax highlighting cho "print('reading docs')" , bộ truy xuất Rtruy xuất ba tài liệu liên quan:
d1mô tả thư viện syntax highlighting pygments ,d2mô tả class PythonLexer , và
d3mô tả class HtmlFormatter. Cho trước những docs này và ý định, bộ tạo Gtạo ra
đoạn mã c, sử dụng PythonLexer vàHtmlFormatter từ thư viện pygment.
3 CÁC THỰC HIỆN THỰC TẾ CỦA DocPrompting
DocPrompting là một phương pháp tổng quát không bị ràng buộc với bất kỳ lựa chọn mô hình cụ thể nào, và nó có thể được
thực hiện với bất kỳ bộ truy xuất và bộ tạo cơ sở nào. Phần này trình bày các thực hiện cụ thể của
RvàGmà chúng tôi thấy cung cấp hiệu suất tốt nhất trong các thử nghiệm của chúng tôi.
3.1 THỰC HIỆN BỘ TRUY XUẤT
Chúng tôi thử nghiệm với hai loại bộ truy xuất chính: bộ truy xuất thưa thớt và bộ truy xuất dày đặc. Là bộ truy xuất thưa thớt của chúng tôi, chúng tôi sử dụng Elasticsearch2với BM25 tiêu chuẩn (Robertson and Jones, 1976). Bộ truy xuất này
đại diện tài liệu sử dụng các đặc trưng thưa thớt dựa vào tần số từ, như BM25 và TF-IDF.
Là bộ truy xuất dày đặc của chúng tôi, chúng tôi theo công trình trước (Chen et al., 2020; Karpukhin et al., 2020; Gao et al.,
2021): cho trước một bộ ba (n;c;D∗
n), trong đó D∗
nlà các docs oracle cho n, mỗi d+
i∈D∗
nvànhình thành một
cặp dương(n;d+
i), trong khi mỗi d−
j∉D∗
nvànhình thành một cặp âm/parenleft.alt1ni;d−
j/parenright.alt1. Chúng tôi huấn luyện bộ truy xuất
theo cách tương phản nơi điểm tương tự của một cặp dương được tối đa hóa trong khi điểm của
các cặp âm trong batch được tối thiểu hóa. Cho một cặp (ni;d+
i), hàm mất mát được định nghĩa là:
Lr=−logexp/parenleft.alt2sim(hn;hd+
i)/parenright.alt2
exp/parenleft.alt2sim(hn;hd+
i)/parenright.alt2+∑d−
j∈B/slash.leftD∗nexp/parenleft.alt2sim(hn;hd−
j)/parenright.alt2(3)
2https://github.com/elastic/elasticsearch
3

--- TRANG 4 ---
Được xuất bản tại hội nghị ICLR 2023
trong đó hxlà biểu diễn của xđược tính toán bởi một bộ mã hóa neural, và Blà docs dương cho các
ví dụ khác trong batch. Chúng tôi định nghĩa sim (hx;hy)là độ tương tự cosine giữa hxvàhy.
Chúng tôi sử dụng tất cả (ni;d+
i)trong tập huấn luyện như tập dữ liệu huấn luyện có giám sát của chúng tôi. Thêm vào đó, chúng tôi sử dụng tất cả
các câu trong nhóm tài liệu cho giám sát yếu: Theo Chen et al. (2020) và Gao et al.
(2021), các biểu diễn của cùng một câu với các mặt nạ dropout khác nhau được coi như một ví dụ dương. Thay vì sử dụng giám sát có giám sát hoặc giám sát yếu như trong Gao et al. (2021), chúng tôi
đơn giản trộn hai tín hiệu giám sát này, và các ví dụ được phân phối ngẫu nhiên vào các batch.
Sự trộn lẫn các nhiệm vụ này không chỉ tạo điều kiện cho quá trình học ( §6.2), mà còn giảm nỗ lực kỹ thuật
cần thiết để lưu trữ và tải lại các mô hình cho các giai đoạn huấn luyện có giám sát và không giám sát riêng biệt.
Chúng tôi khởi tạo bộ mã hóa truy xuất với mô hình tốt nhất của Gao et al. (2021) hoặc bộ mã hóa của
CodeT5-base (Wang et al., 2021). Chi tiết huấn luyện bổ sung được cung cấp trong Phụ lục C
3.2 THỰC HIỆN BỘ TẠO
Chúng tôi thử nghiệm với nhiều mô hình tạo khác nhau. Chúng tôi đã sử dụng GPT-Neo-125M, GPT-Neo-1.3B (Black
et al., 2021) và Codex (Chen et al., 2021), nơi chúng tôi nối các tài liệu đã truy xuất và
ý định NL thành một prompt dài, đơn lẻ. T5-base (Raffel et al., 2019) và CodeT5-base (Wang et al.,
2021) có kích thước đầu vào ngắn hơn là 512 token, đôi khi quá ngắn cho việc nối
nhiều docs. Do đó, cho T5 và CodeT5 chúng tôi áp dụng phương pháp fusion-in-decoder (FiD; Izacard and
Grave, 2021): chúng tôi đầu tiên nối ý định nvới mỗi di∈^Dnđã truy xuất và mã hóa mỗi cặp (n;di)
độc lập. Sau đó, bộ giải mã chú ý đến tất cảcác cặp NL-document đã mã hóa. Chúng tôi tinh chỉnh
bộ tạo để tối đa hóa log-likelihood của mã tham chiếu ccho trước nvà^Dn.
Với Codex (Chen et al., 2021), chúng tôi đã thực hiện few-shot learning thay vì tinh chỉnh vì các
tham số mô hình không được công khai. Chúng tôi xây dựng prompt với ba ví dụ tĩnh,
mỗi cái là một sự nối của tài liệu đã truy xuất, một ý định NL và đoạn mã tham chiếu
. Sau đó chúng tôi thêm ví dụ kiểm tra và tài liệu đã truy xuất của nó vào các ví dụ few-shot.
Chúng tôi sử dụng phiên bản code-davinci-001 vì chúng tôi nghi ngờ rò rỉ dữ liệu tiềm tàng của tập kiểm tra vào
tập huấn luyện của code-davinci-002 . Xem thêm chi tiết trong Phụ lục H. Chi tiết huấn luyện, cài đặt siêu tham số và ví dụ prompt có thể được tìm thấy trong Phụ lục E và D.
4 THIẾT LẬP THỬ NGHIỆM
Chúng tôi đánh giá DocPrompting trên hai nhiệm vụ NL→code: shell scripting ( §4.1), trong đó chúng tôi tạo
các lệnh shell phức tạp cho trước một ý định, và lập trình Python ( §4.2), nơi chúng tôi tạo
câu trả lời bằng Python cho các câu hỏi NL. Trong phần này, chúng tôi đầu tiên giới thiệu một benchmark mới được tuyển chọn
tldr ; sau đó chúng tôi mô tả việc chia lại benchmark CoNaLa phổ biến (Yin et al., 2018). Cho mỗi
benchmark, chúng tôi cung cấp một nhóm tài liệu toàn cầu Dđược chia sẻ cho tất cả các ví dụ và các tài liệu oracle
D∗
nmà chúng tôi sử dụng để huấn luyện bộ truy xuất. Chúng tôi phát hành các benchmark mới được tuyển chọn của chúng tôi để phục vụ
như bệ thử nghiệm cho các mô hình tạo mã dựa trên truy xuất trong tương lai.
4.1 SHELL SCRIPTING
Hình 2: Một ví dụ cặp NL-code
từtldr , cùng với ba mục tài liệu
oracle.tldr là một dự án được điều khiển bởi cộng đồng duy trì các trang trợ giúp dễ
đọc với các ví dụ cho hơn 2.5 k lệnh Bash trong hơn 25 ngôn ngữ tự nhiên3. Chúng tôi thu thập các cặp
ý định tiếng Anh và dòng lệnh Bash. Các ý định NL được
viết bởi người dùng, và các lệnh Bash từ những cái
phổ biến như cat vàtar, đến các lệnh không phổ biến như
toilet vàfaketime . Benchmark tldr của chúng tôi
chứa 1,879 lệnh Bash duy nhất và 9,187 cặp NL →Bash
. Chúng tôi xây dựng tập huấn luyện, phát triển và tập kiểm tra
với các lệnh hoàn toàn tách biệt để kiểm tra khả năng tổng quát hóa
của một mô hình tạo mã. Nhóm tài liệu được chia sẻ
Dđược tạo nên từ 400 k đoạn văn từ 1,879 hướng dẫn
Bash. Mỗi đoạn văn mô tả một khái niệm đơn lẻ như một
3https://github.com/tldr-pages/tldr
4

--- TRANG 5 ---
Được xuất bản tại hội nghị ICLR 2023
cờ đối số. Chúng tôi tiếp tục tuyển chọn các tài liệu oracle D∗
ncho mỗi ví dụ sử dụng khớp chuỗi đơn giản. Một ví dụ từ tldr được hiển thị trong Hình 2. Theo hiểu biết tốt nhất của chúng tôi, đây là
công trình đầu tiên tận dụng tldr như một benchmark NL→code. Thống kê chi tiết và các chi tiết bổ sung
được cung cấp trong Phụ lục A. Trong tldr , mỗi ý định NL dẫn đến một lệnh Bash đơn lẻ với một
tổ hợp của các cờ đối số. Do đó chúng tôi đầu tiên truy xuất toàn bộ hướng dẫn Bash; sau đó, chúng tôi lấy
hướng dẫn hàng đầu và truy xuất top-10 đoạn văn từ hướng dẫn đó.
Các chỉ số đánh giá Chúng tôi đo: (a) độ chính xác tên lệnh (CMD Acc) – liệu tên lệnh
( ví dụ,cat) có khớp chính xác hay không; (b) khớp chính xác (EM) – khớp chính xác giữa tham chiếu và
thế hệ; (c) F1 cấp token; và (d) BLEU cấp ký tự (charBLEU; Lin et al., 2018; Shi
et al., 2022). Trong tất cả các chỉ số, chúng tôi bỏ qua tên biến cụ thể của người dùng trong tham chiếu và đầu ra của mô hình. Ví dụ, " mycli -u [user] -h [host] [database] " được đánh giá
như "mycli -u $1 -h $2 $3 ".
4.2 LẬP TRÌNH PYTHON
CoNaLa (Yin et al., 2018) là một benchmark phổ biến cho việc tạo NL →Python. Các ý định NL là
câu hỏi StackOverﬂow, và các đoạn mã là câu trả lời của chúng. Cả ý định và đoạn mã đều được
viết lại bởi các người chú thích con người. Chúng tôi chia lại tập dữ liệu để kiểm tra khả năng tổng quát hóa của mô hình đến các hàm
Python chưa thấy. Trong việc chia lại của chúng tôi, chúng tôi xác minh rằng mọi ví dụ trong tập phát triển hoặc tập kiểm tra sử dụng ít nhất
một hàm Python ( ví dụ,plt.plot ) không được thấy trong dữ liệu huấn luyện. Thêm vào đó, chúng tôi đảm bảo
rằng các ví dụ từ cùng một bài đăng StackOverﬂow nằm trong cùng một tập để ngăn ngừa rò rỉ.
Việc chia lại này dẫn đến 2,135/201/543 ví dụ trong tập huấn luyện/phát triển/kiểm tra, tương ứng.
Nhóm tài liệu CoNaLa Dchứa 35,763 tài liệu, mỗi cái mô tả một hàm đơn lẻ,
từ tất cả các thư viện Python có sẵn trên DevDocs (https://devdocs.io ). Chúng bao gồm các thư viện tích hợp sẵn
và các thư viện phổ biến khác như numpy . Chúng tôi xây dựng các docs oracle D∗
ncho mỗi
ví dụ bằng cách khớp tất cả tên hàm trong mã đích cvới docs. Thêm chi tiết trong Phụ lục B.
Các chỉ số đánh giá Chúng tôi theo Yin et al. (2018) và đo BLEU-4. Vì chúng tôi tập trung vào việc tổng
quát hóa đến các hàm chưa thấy, chúng tôi báo cáo thêm recall tên hàm ( recall ) và recall hàm chưa thấy
( recall unseen ), đo recall trong số các lời gọi hàm không xuất hiện trong tập
huấn luyện. Cuối cùng, theo Chen et al. (2021); Austin et al. (2021), chúng tôi sử dụng các unit test được viết thủ công từ Wang et al. (2022) cho 100 ví dụ từ tập kiểm tra của CoNaLa và đo pass@ k. Chúng tôi
theo Chen et al. (2021) và thực hiện nucleus sampling (Holtzman et al., 2019) với p=0:95.
Cho mỗi k, chúng tôi tìm kiếm nhiệt độ tốt nhất cho mỗi mô hình từ {0:2;0:4;0:6;0:8;1:0}. Trung bình
, mỗi ví dụ có 2.03 test. Việc nối nhiều docs Python thường vượt quá
giới hạn độ dài của GPT-Neo, do đó chúng tôi thử nghiệm trong tập dữ liệu này với FiD, cho phép đầu vào dài hơn.
Chi tiết bổ sung được cung cấp trong Phụ lục B.
5 KẾT QUẢ
Trong tất cả các kết quả sau, tất cả các mô hình với DocPrompting sử dụng top- 10docs được truy xuất từ bộ truy xuất tốt nhất
trên tập dữ liệu đó (Bảng 4). Mọi baseline sử dụng thiết lập chính xác như phiên bản "+ DocPrompting "
của nó, ngoại trừ việc không sử dụng tài liệu.
5.1 KẾT QUẢ SHELL SCRIPTING
Kết quả cho tldr được hiển thị trong Bảng 1. DocPrompting liên tục cải thiện các mô hình cơ sở. Ví
dụ, T5+ DocPrompting đạt được độ chính xác hơn gấp đôi trong việc dự đoán tên lệnh
, hơn 16 điểm charBLEU trên toàn bộ dự đoán, và gần 9% khớp chính xác
tuyệt đối, so với T5 vanilla. Trong cài đặt few-shot learning với Codex, DocPrompting
mang lại lợi ích 6.7 điểm charBLEU, và cải thiện liên tục trên tất cả các chỉ số so với baseline
chỉ quan sát các cặp NL-code trong prompt của nó. Những kết quả này cho thấy rằng việc truy xuất tài liệu
cũng mang lại lợi ích cho các mô hình mạnh như Codex, và chỉ với vài ví dụ trong ngữ cảnh.
Tạo mã với tên lệnh oracle Trong cài đặt thực tế, một lập trình viên con người có thể
biết tên lệnh họ cần sử dụng ( ví dụ,awk), nhưng không biết cách sử dụng chính xác và các cờ. Thực tế
, hiểu biết tốt hơn về cách sử dụng các lệnh đã biết là mục đích của các trang man Unix và
5

--- TRANG 6 ---
Được xuất bản tại hội nghị ICLR 2023
Bảng 1: Kết quả trên shell scripting, sử dụng bộ truy xuất BM25 với top-10 docs được truy xuất, trên tập kiểm tra
oftldr . Cho các thử nghiệm "oracle command name", chúng tôi đã chọn mô hình tốt nhất của mỗi loại.
Mô hình CMD Acc (%) EM (%) Token F1 charBLEU
GPT-Neo-125M- 11.96 1.94 28.75 19.99
+DocPrompting 25.32 3.56 31.23 24.43
GPT-Neo-1.3B- 14.55 3.12 32.46 24.70
+DocPrompting 27.59 9.05 37.24 30.57
T5- 10.02 0.76 19.90 25.48
+DocPrompting 30.28 9.16 37.58 31.97
CodeT5- 14.60 2.18 30.00 21.50
+DocPrompting 30.72 9.15 36.71 33.83
Codex 3-shots- 27.48 8.94 36.04 16.94
+DocPrompting 31.21 9.29 36.77 23.72
Với tên lệnh oracle
T5- - 12.96 59.36 45.05
+DocPrompting - 22.55 64.84 54.28
Codex 3-shots- - 22.44 62.26 50.29
+DocPrompting - 32.43 69.73 55.21
Bảng 2: So sánh với các phương pháp truy xuất ví dụ (Parvez et al., 2021; Pasupat et al., 2021)
.
Mô hình CMD Acc (%) EM (%) Token F1 charBLEU
GPT-Neo-125M+ExPrompting 6.68 0.32 20.49 11.15
+DocPrompting 25.32 3.56 31.23 24.43
GPT-Neo-1.3B+ExPrompting 14.01 2.8 30.07 22.11
+DocPrompting 27.59 9.05 37.24 30.57
dự án tldr. Chúng tôi đã tiến hành một thử nghiệm oracle nơi chúng tôi cung cấp cho T5 (là mô hình mạnh nhất
sử dụng DocPrompting ) và Codex với tên lệnh oracle ( ví dụ,awk). Thông tin oracle này
được cung cấp cho cả baseline và mô hình sử dụng DocPrompting . Kết quả được
hiển thị ở phần dưới của Bảng 1. Khi tên lệnh oracle được đưa ra, DocPrompting tiếp tục
cải thiện so với các mô hình cơ sở. Ví dụ, khi cung cấp cho Codex tên lệnh ground truth
, DocPrompting cải thiện khớp chính xác của nó từ 22.44% lên 32.43%.
Chúng ta có nên truy xuất tài liệu hay ví dụ? Tất cả các mô hình dựa trên truy xuất hiện tại của mã
truy xuất các cặp NL-code hoặc đoạn mã, thay vì tài liệu. Để mô phỏng kịch bản này, chúng tôi
theo Parvez et al. (2021) và Pasupat et al. (2021) để truy xuất các cặp NL-code từ tập huấn luyện
của tldr , và gọi baseline này là ExPrompting . Chúng tôi tinh chỉnh bộ truy xuất RoBERTa tốt nhất
và hai bộ tạo, và truy xuất top- 30cặp NL-code cho mỗi ví dụ. Như hiển thị trong Bảng 2,
việc truy xuất tài liệu (DocPrompting ) cung cấp lợi ích cao hơn nhiều so với việc truy xuất ví dụ
(ExPrompting ). Về mặt lý thuyết, việc thêm ví dụ của các lệnh chưa thấy cũng có thể giúp ExPrompting
tổng quát hóa đến chúng. Tuy nhiên, các thư viện và hàm mới có thể chưa có ví dụ có sẵn trên
web, trong khi tài liệu thường có sẵn khi thư viện được phát hành.
5.2 KẾT QUẢ LẬP TRÌNH PYTHON
Bảng 3 hiển thị kết quả trên CoNaLa . CodeT5+ DocPrompting mang lại 1.65 cải thiện BLEU
so với baseline state-of-the-art được khởi tạo với CodeT5.4Khi đo recall của các
tên hàm được tạo, lợi ích của DocPrompting đặc biệt cao hơn cho các hàm chưa thấy
(recall unseen ). Ví dụ, DocPrompting đạt 18.30 so với chỉ 9.03 của base CodeT5
trong các hàm chưa thấy. Thêm vào đó, DocPrompting cải thiện cài đặt in-context learning với Codex.
4Trong một thử nghiệm riêng trên phân chia gốc của CoNaLa , baseline này đạt điểm BLEU 39.12,
vượt trội hơn state-of-the-art trước đó (Beau and Crabb ´e, 2022) 4.92 điểm BLEU.
6

--- TRANG 7 ---
Được xuất bản tại hội nghị ICLR 2023
Bảng 3: Kết quả trên CoNaLa , sử dụng bộ truy xuất CodeT5 với top-10 docs được truy xuất. Function recall
(Recall) đo có bao nhiêu hàm trong mã tham chiếu được dự đoán chính xác, và unseen
function recall (Recall unseen ) chỉ xem xét tập con được loại bỏ khỏi dữ liệu huấn luyện.
Mô hình BLEU Recall Recall unseen
Codex 3-shots- 43.16 39.52 -
+DocPrompting 43.47 39.87 -
+DocPrompting oracle docs 50.59 57.84 -
T5- 28.07 14.36 2.57
+DocPrompting 30.04 21.34 8.24
CodeT5- 34.57 24.24 9.03
+DocPrompting 36.22 27.80 18.30
+DocPrompting oracle docs 49.04 72.20 63.91
110 50 100 2000510152025303540
8.2618.7027.5431.8735.46
5.4114.3123.3825.5427.08
kpass@ k
+DocPrompting
CodeT5
Hình 3: Pass@ kcủa CodeT5 với và không có
DocPrompting trên 100 ví dụ CoNaLa.1 2 30%20%40%60%80%100%
12%
0% 0%24%
2% 0%
n-gramRecalltldr
1 2 3 4 530%
14%11%9% 7%91%
52%
28%
16%11%
n-gramCoNaLa
NL← →Code
(NL+Docs) ← →Code
Hình 4: Sử dụng tài liệu tăng đáng kể
recall chồng chéo n-gram giữa
đầu vào và đầu ra, trong tldr vàCoNaLa .
Chúng tôi giả thuyết rằng lợi ích nhỏ chủ yếu do rò rỉ dữ liệu tiềm tàng của Codex, vi phạm
việc chia các hàm đã thấy và chưa thấy. Lý do khác là một bộ tạo mạnh như Codex
có thể yêu cầu một bộ truy xuất mạnh tương đương. Chúng tôi thấy rằng Codex có thể đạt kết quả cao hơn nữa
với bộ truy xuất oracle, cho thấy tiềm năng cải thiện thêm bằng cách cải thiện các bộ truy xuất.
Cuối cùng, CodeT5 hoạt động tốt hơn T5, với và không sử dụng DocPrompting . Điều này nhấn mạnh
tầm quan trọng của việc sử dụng các mô hình được pretrain cụ thể cho mã.
Đánh giá dựa trên thực thi Kết quả được hiển thị trong Hình 3. Sử dụng DocPrompting liên tục
vượt trội hơn baseline CodeT5 cho tất cả các giá trị của pass@ k. Ví dụ, DocPrompting mang lại
2.85% cải thiện trên pass@1 và 4.45% cải thiện trên pass@5, là những con số thực tế
của các completion có thể được đề xuất trong một IDE. Khi k=200,DocPrompting mở rộng khoảng cách
đến 8.38%. Những kết quả này chứng minh rằng DocPrompting không chỉ cải thiện chất lượng của
mã được tạo về mặt hình thức bề ngoài, mà còn tăng tính đúng đắn chức năng của nó. Chi tiết bổ sung và
kết quả được cung cấp trong Phụ lục G.
6 PHÂN TÍCH
6.1 TẠI SAO ĐỌC TÀI LIỆU GIÚP TẠO MÃ CHÍNH XÁC HỚN ?
Chúng tôi tin rằng một trong những lý do chính là tài liệu làm dễ dàng hóa ánh xạ giữa các ý định NL
và mã , vì tài liệu chứa cả mô tả NL vàchữ ký hàm.
Chúng tôi tính toán sự chồng chéo n-gram giữa các ý định NL và các đoạn mã tương ứng của chúng
(NL← →code), và sự chồng chéo giữa các ý định NL với top- 10tài liệu được truy xuất của chúng và
các đoạn mã của chúng ((NL+docs) ← →code). Như hiển thị trong Hình 4, việc thêm tài liệu tăng đáng kể
sự chồng chéo trên các n-gram, và tăng, ví dụ, sự chồng chéo unigram từ 12% lên
7

--- TRANG 8 ---
Được xuất bản tại hội nghị ICLR 2023
Bảng 4: Hiệu suất truy xuất của nhiều mô hình trên tập dev của tldr (trên) và CoNaLa (dưới).
RoBERTa là mô hình tốt nhất được lấy từ từ Gao et al. (2021), và CodeT5 là bộ mã hóa của CodeT5-
base (Wang et al., 2021). Các mô hình với chỉ số dưới "off-shelf" là các mô hình off-the-shelf, và
các mô hình khác được tinh chỉnh với mục tiêu trong Phương trình 3. Cột cuối cùng là mô hình tốt nhất
(RoBERTa cho tldr và CodeT5 cho CoNaLa ) được huấn luyện mà không có corpus giám sát yếu.
n BM25 RoBERTa off-shelf RoBERTa CodeT5 off-shelf CodeT5 Best w/o weak sup.
tldr1 32.81 17.53 30.03 10.45 18.10 28.30
5 51.73 37.89 52.50 20.26 38.52 50.50
10 59.86 46.80 60.33 25.73 51.03 59.84
20 62.01 56.11 64.30 33.65 57.26 62.30
CoNaLa1 3.01 4.46 13.49 4.60 16.54 10.51
5 7.16 7.58 26.38 8.63 42.35 21.15
10 9.73 10.93 34.86 12.25 55.81 29.34
20 11.46 13.89 45.46 18.46 66.79 42.21
24% trong tldr . Có nghĩa là, một trong những lý do mà việc truy xuất tài liệu giúp tạo mã chính xác
là tài liệu bắc cầu khoảng cách giữa "thuật ngữ ý định" và "thuật ngữ mã".
6.2 NGHIÊN CỨU LOẠI BỎ
Chúng tôi so sánh các cấu hình khác nhau của bộ truy xuất, để thu thập thêm hiểu biết cho
DocPrompting hiệu quả . Bảng 4 hiển thị so sánh giữa các bộ truy xuất khác nhau và thiết lập của chúng. Đầu tiên, hiệu suất của BM25 khác nhau giữa các tập dữ liệu: Trong tldr , BM25 khớp với recall của các bộ truy xuất
dày đặc được huấn luyện; tuy nhiên trong CoNaLa , BM25 chỉ đạt recall@ 10của 9.73%, và các bộ truy xuất dày đặc mạnh
như bộ mã hóa của CodeT5 đạt recall@10 của 55.81. Chúng tôi giả thuyết rằng sự khác biệt này
giữa các tập dữ liệu xuất phát từ cách những tập dữ liệu này được tạo: các ý định tldr được viết dựa
trên các lệnh và hướng dẫn Bash hiện có; trong khi các ví dụ CoNaLa được khai thác từ các bài đăng StackOverﬂow
, nơi người dùng đặt câu hỏi với ngữ cảnh hạn chế hoặc không có ngữ cảnh. Do đó, các ý định NL trong CoNaLa yêu cầu
sự căn chỉnh ngữ nghĩa tốt hơn với các tài liệu, và do đó hưởng lợi từ các bộ truy xuất dày đặc. Khoảng cách
do các quy trình tuyển chọn dữ liệu khác nhau cũng được quan sát bởi Rodriguez và Boyd-Graber
(2021) trong việc trả lời câu hỏi miền mở (QA).
Thứ hai, các bộ truy xuất được pretrain trên ngôn ngữ lập trình đích thường mạnh hơn.
Ví dụ trong CoNaLa , CodeT5 được pretrain trên Python, vừa là bộ truy xuất off-the-shelf tốt hơn
vừa là bộ truy xuất được tinh chỉnh tốt hơn RoBERTa, được pretrain chủ yếu trên văn bản. Ngược lại
, tldr dựa trên Bash, mà cả CodeT5 lẫn RoBERTa đều không được pretrain rõ ràng.
Do đó, tldr hưởng lợi chủ yếu từ BM25 và RoBERTa thay vì CodeT5 như bộ truy xuất.
Cuối cùng, việc huấn luyện bộ truy xuất sử dụng giám sát yếu trên nhóm tài liệu (Phần 3.1)
cải thiện đáng kể bộ truy xuất. Recall của các bộ truy xuất tốt nhất của mỗi tập dữ liệu mà không có corpus này được hiển thị trong cột cuối cùng của Bảng 4 ("Best w/o weak sup."). Trên CoNaLa , việc loại bỏ
corpus này dẫn đến sự suy giảm hiệu suất nghiêm trọng. Một giải thích có thể là giám sát yếu này
giúp bộ truy xuất thực hiện thích ứng miền hiệu quả hơn.
6.3 NGHIÊN CỨU TRƯỜNG HỢP
Chúng tôi kiểm tra các đầu ra của mô hình và hiển thị hai ví dụ đại diện trong Bảng 5. Trong ví dụ đầu tiên
, Image.open không được thấy trong tập huấn luyện, và baseline CodeT5
dự đoán sai os.open . Ngược lại, việc sử dụng DocPrompting cho phép truy xuất các docs và
dự đoán đúng Image.open . Trong ví dụ thứ hai, df.to csv không được thấy trong huấn luyện, và baseline
CodeT5 không dự đoán đúng nó. Ngược lại, DocPrompting dự đoán hầu hết lời gọi df.to csv
một cách chính xác, nhờ vào các docs được truy xuất. Tuy nhiên, DocPrompting tạo ra đối số
không chính xác skiprows=1 , thay vì header=False . Lý do là cùng với tài liệu được truy xuất
của df.tocsv, bộ truy xuất cũng truy xuất tài liệu của df.read csv,
có đối số skiprows. Có nghĩa là, bộ tạo sử dụng đối số của df.read csv với
hàm df.tocsv. Cải thiện thêm các bộ truy xuất và bộ tạo, và lọc sau dựa trên tính hợp lệ của tên đối số, có thể giảm thiểu những sai lầm như vậy.
8

--- TRANG 9 ---
Được xuất bản tại hội nghị ICLR 2023
Bảng 5: Ví dụ về dự đoán từ CoNaLa , của base CodeT5 so với
CodeT5+ DocPrompting . Các hàm chưa thấy được::::::::::gạch dưới.
Ý định NL :Mở hình ảnh "picture.jpg"
Ground truth : img =::::::::::Image.open('picture.jpg') nn Img.show
CodeT5 : os.open('picture.jpg', 'r')
CodeT5+ DocPrompting :image =:::::::::::Image.open('picture.jpg', 'rb')
Ý định NL :Loại trừ tên cột khi ghi dataframe 'df' vào tệp csv 'ﬁlename.csv'
Ground truth ::::::::::df.to csv ('filename.csv', header=False)
CodeT5 : df.drop(['col1', 'col2'], axis=1, inplace=True)
CodeT5+ DocPrompting ::::::::::df.to csv('filename.csv', skiprows=1)
7 CÔNG TRÌNH LIÊN QUAN
Tạo mã Thực hành phổ biến nhất trong việc tạo mã NL →code là huấn luyện một mô hình trên tập dữ liệu
các cặp NL-code (Allamanis et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al.,
2018). Tuy nhiên, tất cả các công trình này giả định rằng corpus huấn luyện của chúng bao gồm tất cảcác thư viện và
hàm cần thiết, và các mô hình của chúng về bản chất không có khả năng tạo ra các thư viện và hàm không
được thấy trong dữ liệu huấn luyện. Ngược lại, DocPrompting cho phép các mô hình tạo ra các lời gọi đến hàm chưa thấy
, bằng cách truy xuất tài liệu của những hàm này và đọc chúng tại thời điểm kiểm tra. Hayati et al.
(2018); Parvez et al. (2021); Hashimoto et al. (2018) và Lu et al. (2017) học cách truy xuất ví dụ tại
thời điểm kiểm tra; Pasupat et al. (2021) cũng xem xét các cài đặt nơi dữ liệu kiểm tra có sự dịch chuyển phân phối khỏi
dữ liệu huấn luyện. Tuy nhiên, khi các thư viện mới được phát hành, chúng thường đi kèm với tài liệu,
và do đó chúng tôi giả định rằng tài liệu cho các thư viện mới có khả năng có sẵn cao hơn nhiều so với
các cặp ý định ngôn ngữ tự nhiên và đoạn mã cụ thể (n;c)đã sử dụng những thư viện này. Các
mô hình của Shrivastava et al. và Wu et al. (2021) truy xuất các đoạn mã từ các tệp liên quan trong cùng
dự án; ngược lại, khi dự đoán các thư viện và hàm mới bên ngoài dự án của người dùng,
tài liệu là nguồn có khả năng có sẵn nhất.
Tạo được tăng cường bởi truy xuất Mô hình truy xuất-rồi-tạo đã trở nên phổ biến trong
lĩnh vực trả lời câu hỏi miền mở (Guu et al., 2020; Lewis et al., 2020; Karpukhin et al.,
2020), nơi câu trả lời cho một câu hỏi miền mở tồn tại chỉ trong vài tài liệu trong một
nhóm lớn hơn nhiều. Mặc dù DocPrompting có cách tiếp cận tương tự, việc truy xuất tài liệu trong tạo mã
thậm chí còn có giá trị hơn, vì các thư viện mã được cập nhật liên tục, và các thư viện mới được
giới thiệu hàng ngày. Do đó, DocPrompting cho phép cập nhật nhóm tài liệu thường xuyên với nội dung mới
, mà không cần huấn luyện lại bất kỳ thành phần mô hình nào.
Tạo được điều kiện bởi tài liệu Mô hình của Zhong et al. (2019) đọc tài liệu để
hiểu động học môi trường trong một trò chơi lưới, và Branavan et al. (2011) điều khiển các agent được đặt
trong một trò chơi (Civilization II) bằng cách đọc hướng dẫn của trò chơi. Tuy nhiên, tất cả các mô hình của họ được
điều chỉnh cho các trò chơi cụ thể; ngược lại, DocPrompting là tổng quát và có thể áp dụng cho nhiều
ngôn ngữ lập trình và tập dữ liệu.
8 KẾT LUẬN
Chúng tôi đề xuất DocPrompting , một phương pháp đơn giản và hiệu quả cho việc tạo mã bằng cách truy xuất
tài liệu liên quan. DocPrompting liên tục cải thiện các mô hình NL →code trong hai nhiệm vụ, trong
hai PL, và trên nhiều mô hình cơ sở mạnh. DocPrompting cải thiện các mô hình cơ sở mạnh như
CodeT5 đến 2.85% trong pass@ 1(tăng tương đối 52%) trong đánh giá dựa trên thực thi trên benchmark
Python CoNaLa phổ biến; trên bộ dữ liệu Bash mới tldr ,DocPrompting cải thiện CodeT5 và
GPT-Neo-1.3B lên đến 6.9% khớp chính xác, và Codex 6.78 điểm charBLEU.
Những kết quả này mở ra một hướng đầy hứa hẹn cho việc tạo mã NL →code. Chúng tôi tin rằng kết quả của chúng tôi có thể được
cải thiện thêm bằng cách sử dụng mã hóa thông minh hơn về bản chất có cấu trúc của các tài liệu dài, và sử dụng
huấn luyện chung của bộ truy xuất và bộ tạo, hy vọng sẽ tránh được các lỗi cascading. Hơn nữa,
chúng tôi tin rằng các nguyên tắc và phương pháp được trình bày trong bài báo này có thể áp dụng cho các
nhiệm vụ liên quan đến mã bổ sung, và các tài nguyên giống tài liệu khác như hướng dẫn và bài đăng blog. Để đạt được những
mục đích này, chúng tôi công khai tất cả mã, dữ liệu và mô hình của chúng tôi.
9

--- TRANG 10 ---
Được xuất bản tại hội nghị ICLR 2023
9 LỜI CẢM ơN
Chúng tôi cảm ơn các reviewer ẩn danh vì những nhận xét và đề xuất hữu ích của họ. Công trình này được
hỗ trợ bởi một món quà từ Amazon AI và hợp đồng từ Phòng thí nghiệm Nghiên cứu Không quân
dưới số thỏa thuận FA8750-19-2-0200. Chính phủ Hoa Kỳ được ủy quyền sao chép và
phân phối bản in lại cho mục đích Chính phủ bất chấp bất kỳ ghi chú bản quyền nào trên đó. Các
quan điểm và kết luận được chứa ở đây là của các tác giả và không nên được hiểu là
nhất thiết đại diện cho các chính sách hoặc sự ủng hộ chính thức, được thể hiện rõ ràng hoặc ngầm ý, của
Phòng thí nghiệm Nghiên cứu Không quân hoặc Chính phủ Hoa Kỳ.
TÀI LIỆU THAM KHẢO
Miltiadis Allamanis, Daniel Tarlow, Andrew D. Gordon, và Yi Wei. Bimodal modelling of source code and
natural language. Trong Francis R. Bach và David M. Blei, editors, Proceedings of the 32nd International
Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , volume 37 of JMLR Workshop
and Conference Proceedings , pages 2123–2132. JMLR.org, 2015. URL http://proceedings.mlr.
press/v37/allamanis15.html .
Uri Alon, Roy Sadaka, Omer Levy, và Eran Yahav. Structural language models of code. Trong International
conference on machine learning , pages 245–256. PMLR, 2020.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,
Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint ,
abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732 .
Nathana ¨el Beau và Beno ˆıt Crabb ´e. The impact of lexical and grammatical processing on generating code
from natural language. ArXiv preprint , abs/2202.13972, 2022. URL https://arxiv.org/abs/2202.
13972 .
Sid Black, Leo Gao, Phil Wang, Connor Leahy, và Stella Biderman. GPT-Neo: Large Scale Autoregres-
sive Language Modeling with Mesh-Tensorﬂow, 2021. URL https://doi.org/10.5281/zenodo.
5297715 . If you use this software, please cite it using these metadata.
S.R.K. Branavan, David Silver, và Regina Barzilay. Learning to win by reading manuals in a Monte-Carlo
framework. Trong Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies , pages 268–277, Portland, Oregon, USA, 2011. Association for Computational
Linguistics. URL https://aclanthology.org/P11-1028 .
Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, và Oleksandr Polozov. Generative code
modeling with graphs. Trong International Conference on Learning Representations , 2019. URL https:
//openreview.net/forum?id=Bke4KsA5FX .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura
Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. ArXiv
preprint , abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 .
Ting Chen, Simon Kornblith, Mohammad Norouzi, và Geoffrey E. Hinton. A simple framework for contrastive
learning of visual representations. Trong Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of Machine Learning Research ,
pages 1597–1607. PMLR, 2020. URL http://proceedings.mlr.press/v119/chen20j.html .
Andrew Forward và Timothy C Lethbridge. The relevance of software documentation, tools and technologies:
a survey. Trong Proceedings of the 2002 ACM symposium on Document engineering , pages 26–33, 2002.
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,
Luke Zettlemoyer, và Mike Lewis. Incoder: A generative model for code inﬁlling and synthesis. ArXiv
preprint , abs/2204.05999, 2022. URL https://arxiv.org/abs/2204.05999 .
Tianyu Gao, Xingcheng Yao, và Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings.
InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages
6894–6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.emnlp-main.552. URL https://aclanthology.org/2021.emnlp-main.
552.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, và Sanjiv Kumar. Accelerating
large-scale inference with anisotropic vector quantization. Trong International Conference on Machine Learning ,
2020. URL https://arxiv.org/abs/1908.10396 .
10

--- TRANG 11 ---
Được xuất bản tại hội nghị ICLR 2023
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Ming-Wei Chang. Realm: Retrieval-augmented
language model pre-training. ArXiv preprint , abs/2002.08909, 2020. URL https://arxiv.org/abs/
2002.08909 .
Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, và Percy S Liang. A retrieve-and-edit framework for
predicting structured outputs. Advances in Neural Information Processing Systems , 31, 2018.
Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin, Anthony Tomasic, và Graham
Neubig. Retrieval-based neural code generation. Trong Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing , pages 925–930, Brussels, Belgium, 2018. Association for Computational
Linguistics. doi: 10.18653/v1/D18-1111. URL https://aclanthology.org/D18-1111 .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và Yejin Choi. The curious case of neural text degeneration.
arXiv preprint arXiv:1904.09751 , 2019.
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, và Luke Zettlemoyer. Mapping language to code in pro-
grammatic context. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing , pages 1643–1652, Brussels, Belgium, 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1192. URL https://aclanthology.org/D18-1192 .
Gautier Izacard và Edouard Grave. Leveraging passage retrieval with generative models for open domain ques-
tion answering. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Compu-
tational Linguistics: Main Volume , pages 874–880, Online, 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.eacl-main.74. URL https://aclanthology.org/2021.eacl-main.74 .
Jeff Johnson, Matthijs Douze, và Herv ´e J´egou. Billion-scale similarity search with GPUs. IEEE Transactions
on Big Data , 7(3):535–547, 2019.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
và Wen-tau Yih. Dense passage retrieval for open-domain question answering. Trong Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769–6781,
Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL
https://aclanthology.org/2020.emnlp-main.550 .
Timothy C Lethbridge, Janice Singer, và Andrew Forward. How software engineers use documentation: The
state of the practice. IEEE software , 20(6):35–39, 2003.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, Sebastian Riedel, và Douwe Kiela. Retrieval-
augmented generation for knowledge-intensive NLP tasks. Trong Hugo Larochelle, Marc'Aurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, và Hsuan-Tien Lin, editors, Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, Decem-
ber 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
6b493230205f780e1bc26945df7481e5-Abstract.html .
Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, và Michael D. Ernst. NL2Bash: A corpus and semantic
parser for natural language interface to the linux operating system. Trong Proceedings of the Eleventh International
Conference on Language Resources and Evaluation (LREC 2018) , Miyazaki, Japan, 2018. European Language
Resources Association (ELRA). URL https://aclanthology.org/L18-1491 .
Yanxin Lu, Swarat Chaudhuri, Chris Jermaine, và David Melski. Data-driven program completion. arXiv
preprint arXiv:1705.09042 , 2017.
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming
Xiong. A conversational paradigm for program synthesis. arXiv preprint , 2022.
Janet Nykaza, Rhonda Messinger, Fran Boehme, Cherie L Norman, Matthew Mace, và Manuel Gordon. What
programmers really want: results of a needs assessment for sdk documentation. Trong Proceedings of the 20th
annual international conference on Computer documentation , pages 133–141, 2002.
Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. Retrieval aug-
mented code generation and summarization. Trong Findings of the Association for Computational Linguistics:
EMNLP 2021 , pages 2719–2734, Punta Cana, Dominican Republic, 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.ﬁndings-emnlp.232. URL https://aclanthology.org/2021.
findings-emnlp.232 .
11

--- TRANG 12 ---
Được xuất bản tại hội nghị ICLR 2023
Panupong Pasupat, Yuan Zhang, và Kelvin Guu. Controllable semantic parsing via retrieval augmentation.
InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages
7683–7698, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.
doi: 10.18653/v1/2021.emnlp-main.607. URL https://aclanthology.org/2021.emnlp-main.
607.
Maxim Rabinovich, Mitchell Stern, và Dan Klein. Abstract syntax networks for code generation and semantic
parsing. Trong Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) , pages 1139–1149, Vancouver, Canada, 2017. Association for Computational Linguistics.
doi: 10.18653/v1/P17-1105. URL https://aclanthology.org/P17-1105 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, và Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. ArXiv
preprint , abs/1910.10683, 2019. URL https://arxiv.org/abs/1910.10683 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, và Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of
Machine Learning Research , 21:1–67, 2020.
Stephen E Robertson và K Sparck Jones. Relevance weighting of search terms. Journal of the American Society
for Information science , 27(3):129–146, 1976.
Pedro Rodriguez và Jordan Boyd-Graber. Evaluation paradigms in question answering. Trong Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing , pages 9630–9642, Online and
Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
emnlp-main.758. URL https://aclanthology.org/2021.emnlp-main.758 .
Tobias Roehm, Rebecca Tiarks, Rainer Koschke, và Walid Maalej. How do professional developers comprehend
software? Trong 2012 34th International Conference on Software Engineering (ICSE) , pages 255–265. IEEE,
2012.
Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, và Sida I. Wang. Natural language to code
translation with execution, 2022. URL https://arxiv.org/abs/2204.11454 .
Disha Shrivastava, Hugo Larochelle, và Daniel Tarlow. Repository-level prompt generation for large language
models of code. Trong ICML 2022 Workshop on Knowledge Retrieval and Language Models .
Yue Wang, Weishi Wang, Shaﬁq Joty, và Steven C.H. Hoi. CodeT5: Identiﬁer-aware uniﬁed pre-trained
encoder-decoder models for code understanding and generation. Trong Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing , pages 8696–8708, Online and Punta Cana, Dominican
Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.685. URL
https://aclanthology.org/2021.emnlp-main.685 .
Zhiruo Wang, Shuyan Zhou, Daniel Fried, và Graham Neubig. Execution-based evaluation for open-domain
code generation. arXiv preprint arXiv:2212.10481 , 2022.
Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, và Christian Szegedy. Memorizing transformers. Trong
International Conference on Learning Representations , 2021.
Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, và Graham Neubig. Incorporating external
knowledge through pre-training for natural language to code generation. Trong Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics , pages 6045–6052, Online, 2020. Association for
Computational Linguistics. doi: 10.18653/v1/2020.acl-main.538. URL https://aclanthology.org/
2020.acl-main.538 .
Frank F Xu, Uri Alon, Graham Neubig, và Vincent J Hellendoorn. A systematic evaluation of large lan-
guage models of code. ArXiv preprint , abs/2202.13169, 2022. URL https://arxiv.org/abs/2202.
13169 .
Pengcheng Yin và Graham Neubig. A syntactic neural model for general-purpose code generation. Trong
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pages 440–450, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.
18653/v1/P17-1041. URL https://aclanthology.org/P17-1041 .
Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, và Graham Neubig. Learning to mine aligned
code and natural language pairs from stack overﬂow. Trong 2018 IEEE/ACM 15th international conference on
mining software repositories (MSR) , pages 476–486. IEEE, 2018.
Victor Zhong, Tim Rockt ¨aschel, và Edward Grefenstette. Rtfm: Generalising to novel environment dynamics
via reading. ArXiv preprint , abs/1910.08210, 2019. URL https://arxiv.org/abs/1910.08210 .
12

--- TRANG 13 ---
Được xuất bản tại hội nghị ICLR 2023
A T L D R : MỘT BENCHMARK SHELL SCRIPTING MỚI ĐƯỢC TUYỂN CHỌN
Các cặp NL→Bash Cho mỗi lệnh ( ví dụ,cat), người dùng đóng góp các ví dụ về cặp mô tả NL và
mã bash (chủ yếu là một dòng), bao gồm các cờ và đối số khác nhau, bao phủ các cách sử dụng phổ biến của
lệnh đó. Một ví dụ được hiển thị trong Hình 2.
Chúng tôi thu thập các cặp NL-code từ các tệp markdown5trong thư mục linux vàcommon. Chúng tôi loại bỏ các
lệnh Bash mà hướng dẫn của chúng không có sẵn (thảo luận dưới đây). Thống kê chi tiết được hiển thị trong Bảng 6. Trung bình
, mỗi lệnh có 4.84 cặp NL →Bash và có tổng cộng 9187 cặp NL-code. Để kiểm tra
khả năng tổng quát hóa của một mô hình, chúng tôi xây dựng tập huấn luyện, phát triển và tập kiểm tra với các lệnh hoàn toàn khác nhau
.
Bảng 6: Thống kê của benchmark shell scripting tldr
# Lệnh Cặp NL →Bash
train 1315 6414
dev 376 1845
test 188 928
total 1879 9187
Nhóm tài liệu DChúng tôi lấy hướng dẫn bash của 1897 lệnh bash trong tldr để xây dựng một
nhóm tài liệu. Chúng tôi tìm kiếm mỗi tên lệnh tại manned.org6, một trang web lưu trữ các trang hướng dẫn Unix
(giống như lệnh Unix ' man <command> ), và sau đó trích xuất nội dung văn bản từ trang hướng dẫn
trả về. Chúng tôi tiếp tục chia mỗi hướng dẫn thành nhiều đoạn văn bằng cách ngắt dòng để mỗi đoạn văn
mô tả một cách tinh tế một khái niệm đơn lẻ như chức năng lệnh hoặc cách sử dụng cờ. Chúng tôi đưa ra quyết định này
do khối lượng lớn nội dung mà mỗi hướng dẫn có, quá dài để vừa với giới hạn độ dài của một mô hình neural
, và quá nhiễu và làm mô hình mất tập trung với thông tin không liên quan. Điều này dẫn đến 400 k mục
riêng lẻ trong nhóm tổng cộng.
Hướng dẫn oracle D∗
iChúng tôi tìm tài liệu ground truth cho mỗi cặp (n;c)thông qua tên lệnh
và heuristic khớp cờ. Ví dụ, cho trước một đoạn mã toilet 'input text' -f
'font filename' , chúng tôi giới hạn tìm kiếm của chúng tôi đến tài liệu từ trang hướng dẫn toilet và chọn
tài liệu bắt đầu với cờ -flàm một đoạn văn oracle. Cùng với đoạn văn đầu tiên thường
tóm tắt một lệnh, những đoạn văn này tạo thành D∗
n.
Các chỉ số đánh giá Chúng tôi sử dụng bốn chỉ số đánh giá để đo chất lượng của mã được tạo: (a) độ
chính xác tên lệnh ( CMD Acc ) – đo liệu tên lệnh ( ví dụ,cat) có được dự đoán đúng hay không;
(b) F1 cấp token – chuyển đổi mã tham chiếu và mã được tạo thành túi từ và đo precision, recall,
và F1 overlap cấp token; (c) khớp chính xác (EM) – đo khớp chính xác giữa tham chiếu
và thế hệ; và (d) BLEU cấp ký tự (charBLEU; Lin et al., 2018; Shi et al., 2022).
Cho token level F1, exact match, và charBLEU, chúng tôi bỏ qua tất cả các biến cụ thể của người dùng trong tham chiếu và
đầu ra hệ thống. Ví dụ, " mycli -u [user] -h [host] [database] " được chuyển đổi thành
"mycli -u $1 -h $2 $3 ". Điều này chủ yếu vì các biến không được khởi tạo trong tldr và
kiểu của placeholder khác nhau giữa các contributor. Ví dụ, một số contributor có thể viết [user] như
[username] hoặc[your name] . Do đó, việc đo hình thức bề ngoài của tên biến cụ thể của người dùng ít
có ý nghĩa hơn.
B CHIA LẠI CONALA
Các cặp NL→Python Chúng tôi điều chỉnh benchmark CoNaLa phổ biến và chia lại tập dữ liệu để kiểm tra kịch bản
tổng quát hóa. Việc chia lại này làm cho mọi ví dụ trong tập phát triển và tập kiểm tra có ít nhất một hàm
Python ( ví dụ,plt.plot ) không được thấy trong dữ liệu huấn luyện. Có 2135, 201, và 543 ví dụ trong
tập huấn luyện, phát triển và kiểm tra, tương ứng. Chúng tôi theo công trình gốc Yin et al. (2018) để đánh giá
đầu ra hệ thống với BLEU-4. Vì chúng tôi tập trung vào cài đặt tổng quát hóa, chúng tôi báo cáo thêm độ chính xác hàm chưa thấy
, đo phần trăm các hàm được giữ lại được dự đoán đúng mà không xuất hiện trong
tập huấn luyện.
Unit test được chú thích bởi con người Theo Chen et al. (2021) và Austin et al. (2021), chúng tôi tiến hành đánh giá
dựa trên thực thi trên CoNaLa để đo tính đúng đắn chức năng của mã được tạo. Chúng tôi chọn ngẫu nhiên
100 ví dụ từ tập kiểm tra và chú thích thủ công unit test cho mỗi ví dụ. Ví dụ, chúng tôi viết các test
như assert gen code("abcds", 2) == 4 vàassert gen code("abde", 2) == -1 để
xác minh liệu hàm gencode có thể thực hiện " tìm chỉ số của chuỗi con 's' trong chuỗi 'str' bắt đầu
từ chỉ số 2 ". Mỗi ví dụ được chú thích bởi một người chú thích đơn lẻ. Việc chú thích được thực hiện bởi hai tác giả của
bài báo lập trình Python hàng ngày. Trung bình, chúng tôi chú thích 2.03 unit test cho mỗi ví dụ.
Nhóm tài liệu DNhóm tài liệu của chúng tôi chứa 35763 hướng dẫn. Những hàm này từ
tất cả các thư viện Python có sẵn trên DevDocs7. Những thư viện này chứa thư viện tích hợp sẵn Python
, và các thư viện phổ biến như numpy vàpandas . Tài liệu trên DevDocs được tuyển chọn và tiếp tục
biến đổi và lập chỉ mục để cho phép tìm kiếm nhanh các API. Sau đó chúng tôi trích xuất mỗi chữ ký API và
tài liệu tương ứng trong mọi thư viện, loại bỏ bất kỳ nội dung nào trong tài liệu không phải là văn bản, và
phân đoạn tài liệu thành nhiều đoạn văn dựa trên các thẻ HTML <p>. Nhóm tài liệu
sau đó chứa các cặp chữ ký API và một đoạn văn đơn lẻ trong tài liệu tương ứng. Mặc dù
nhóm tài liệu không toàn diện để bao phủ tất cả các thư viện và hàm Python, chúng tôi thấy nó có tỷ lệ bao phủ cao
trên tập dữ liệu CoNaLa. Lựa chọn này phản ánh tính linh hoạt của phương pháp của chúng tôi dựa trên đặc điểm
của một kịch bản đích.
Hướng dẫn oracle D∗
iĐể tìm các tài liệu oracle cho một ý định NL đã cho D∗
itừ
ví dụ (n;c)gốc, chúng tôi đầu tiên lập chỉ mục các tên hàm với đường dẫn tuyệt đối ( ví dụ,plot được lập chỉ mục với
matplotlib.pyplot.plot ) với Elasticsearch. Sau đó chúng tôi truy vấn công cụ tìm kiếm với phiên bản sạch
của cnơi tên biến được loại bỏ. Top- 5hàm sau khi khử trùng được coi như hướng dẫn oracle
D∗
i.
Liên kết ngôn ngữ tự nhiên và mã trong quá trình pretraining Mặc dù nỗ lực của chúng tôi, có thể
một số hàm được giữ lại trong tập kiểm tra đã được thấy để liên kết với ngữ cảnh NL ( ví dụ, comment) trong
quá trình pretraining của một bộ truy xuất và bộ tạo. Vì các bộ tạo được khởi tạo từ cùng một checkpoint trong
cả mô hình baseline và DocPrompting, liên kết có thể như vậy được mong đợi giúp đỡ cả hai mô hình như nhau. Trong bộ truy xuất, liên kết có thể như vậy không làm cho bộ truy xuất thấy các ý định NL chính xác cùng
với tài liệu tương ứng, và do đó việc khớp giữa NL ← →doc không bị rò rỉ. Tuy nhiên, có thể
có những ý định tương tự về mặt ngữ nghĩa được thấy cùng với các đoạn mã của các hàm được giữ lại
. Tuy nhiên, sự xuất hiện cùng lúc như vậy là "gián tiếp" và "không giám sát".
C HUẤN LUYỆN BỘ TRUY XUẤT DÀY ĐẶC
Chúng tôi tinh chỉnh mô hình trong 10 epoch với batch size 512 và tỷ lệ học 1e−5. Vì CodeT5 không
sử dụng token[CLS], chúng tôi thay thế lấy trung bình của trạng thái ẩn của lớp cuối cùng làm biểu diễn văn bản.
ChoCoNaLa , chúng tôi cũng sử dụng 100k ví dụ"mined" đầu tiên được cung cấp như một phần của CoNaLa làm corpus có giám sát.
ChoCoNaLa , chúng tôi chỉ áp dụng một bước tìm kiếm đơn lẻ vì mỗi đoạn mã thường chứa nhiều hơn một
hàm. Chúng tôi cũng quan sát thấy rằng việc sử dụng câu đầu tiên thường tóm tắt cách sử dụng của một hàm đạt được
hiệu suất truy xuất tốt nhất so với các lựa chọn thay thế khác như sử dụng đoạn văn đầu tiên, hoặc đơn giản cắt bớt đến
độ dài token tối đa. Việc huấn luyện mất đến 15 giờ trên một GPU A6000 đơn lẻ.
D HUẤN LUYỆN BỘ TẠO
Chúng tôi huấn luyện các bộ tạo nguồn đơn của chúng tôi trong 20 epoch với tỷ lệ học 4e−5. Chúng tôi huấn luyện các bộ tạo dựa trên FiD của chúng tôi
trong 10000 bước. Độ dài doc được đặt là 200, bất kỳ nội dung nào thêm sẽ bị cắt bớt. Chúng tôi theo (Izacard and
Grave, 2021) để đặt tỷ lệ học là 5e−5với 2000 bước warmup và suy giảm tỷ lệ học tuyến tính. Batch size
được đặt là 8. Mô hình tốt nhất được chọn dựa trên điểm F1 cấp token trên tập phát triển cho tldr và
điểm BLEU cho CoNaLa . Việc huấn luyện mất 8 giờ trên một GPU A6000 đơn lẻ.
E PROMPTS CODEX
Cho baseline, chúng tôi prompt Codex với ba cặp NL-code và thêm truy vấn kiểm tra vào cuối. Một ví dụ trên
tldr được hiển thị ở trên Bảng 7. Ở dưới, chúng tôi liệt kê prompt với DocPrompting nơi tài liệu
được cung cấp cùng. Trong cài đặt oracle command name, chúng tôi thêm tên lệnh trước mỗi ý định NL
7https://devdocs.io
14

--- TRANG 15 ---
Được xuất bản tại hội nghị ICLR 2023
1 3 5 10 15 20 25 30203040506070
16.533.142.355.860.466.770.1
73.7Recall Truy xuất@ k
1 3 5 10 15 20 3033.735.936.2 36.2
33.435.6
34.936.3
Docs Đã truy xuất
BLEU TạoCoNaLa
Recall
BLEU
Hình 5: Recall@ k(%) và điểm BLEU tương ứng bằng cách sử dụng top- kdocs này trên
tập dữ liệu CoNaLa (sử dụng CodeT5).
cho prompt baseline. Cho prompt DocPrompting, chúng tôi thay thế các docs tiềm năng với các docs được truy xuất
từ hướng dẫn oracle.
F PHÂN TÍCH BỔ SUNG
Hiệu quả tham số Như hiển thị trong Bảng 1, dưới ngân sách tham số đã cho, chúng tôi thấy rằng DocPrompting
chủ yếu hưởng lợi từ mã hóa song song (FiD). Ví dụ, mã hóa song song T5+ DocPrompting (220M
tham số) vượt trội đáng kể so với 125M tham số mã hóa chung Neo-125M+ DocPrompting . Chỉ
việc mở rộng Neo+ DocPrompting lên 1.3B tham số mới có thể khớp với 220M tham số T5+ DocPrompting .
Một giải thích có thể là mặc dù base Neo-1.3B (không có DocPrompting ) thường hoạt động tốt hơn
base T5 (không có DocPrompting ), mã hóa song song cho phép tận dụng các tài liệu được truy xuất tốt hơn,
vì các tài liệu được mã hóa độc lập ở phía encoder.
Tác động của số lượng tài liệu Hình 5 hiển thị recall@ kvà điểm BLEU so với k,
số lượng tài liệu được truy xuất. Việc tăng kliên tục mang lại recall cao hơn; tuy nhiên, khi nhiều tài liệu không liên quan hơn
được truy xuất, bộ tạo không thể phân biệt hiệu quả chúng khỏi những cái liên quan và hiệu suất tổng thể
vẫn tương tự. Ví dụ, CodeT5 đạt điểm BLEU cao nhất sử dụng 5≤k≤10. Ngược lại, khi
bộ tạo được cung cấp chỉ với các docs oracle, điểm BLEU của nó đạt 49.04 (Bảng 3). Điều này
cho thấy rằng cả precision và recall của docs đều quan trọng, và lợi ích của việc sử dụng các giá trị k lớn hơn trong open
domain QA (Izacard and Grave, 2021) không nhất thiết đúng trong việc tạo mã.
Chồng chéo n-gram đầy đủ Bảng 8 hiển thị rằng việc sử dụng tài liệu tăng đáng kể recall chồng chéo n-gram
giữa đầu vào và đầu ra, trong tldr vàCoNaLa . Vì chúng tôi sử dụng BM25 để truy xuất docs trong tldr ,
sự chồng chéo NL ← →Retrieved docs cao theo cấu trúc. Trong CoNaLa , sự chồng chéo unigram NL ← →Retrieved docs
cũng cao, nhưng vì chúng tôi sử dụng bộ truy xuất dày đặc, sự chồng chéo n-gram tổng quát không cần phải cao để
DocPrompting hoạt động tốt.
Độ trễ truy xuất Mặc dù việc truy xuất docs dẫn đến tính toán bổ sung tại thời điểm kiểm tra, việc tăng
độ trễ không quá mức. Đầu tiên, việc mã hóa đầu vào cho bước truy xuất "chi phí" một lần forward pass đơn lẻ qua
encoder của bộ truy xuất, ít tốn kém hơn đáng kể so với việc tạo (yêu cầu nhiều bước thời gian
của decoder). Tất cả tài liệu trong nhóm truy xuất có thể được mã hóa trước, và việc tìm top- k
kết quả có thể được thực hiện nhanh chóng sử dụng các thư viện như FAISS Johnson et al. (2019) trên GPU hoặc ScaNN Guo
et al. (2020) trên CPU. Chi phí của tìm kiếm top- knày là sub-linear theo kích thước của nhóm tài liệu. Thứ hai,
đầu vào bổ sung cho bộ tạo dẫn đến tăng tiêu thụ bộ nhớ, nhưng chỉ tăng nhỏ
độ trễ vì các token của một đầu vào cho trước có thể được mã hóa song song. Nếu sự khác biệt này quan trọng trong các
cài đặt thực tế, chúng ta có thể giảm số lượng tài liệu được truy xuất. Hình 5 hiển thị rằng việc truy xuất ít nhất năm docs
có thể đủ trong nhiều trường hợp.
G BIỂU ĐỒ PASS@kĐẦY ĐỦ
Trong đánh giá chính dựa trên thực thi, kết quả pass@ ktrongPhần 5.2 và Hình 3, chúng tôi lấy nhiệt độ tốt nhất
cho mọi mô hình và giá trị của k. Ở đây, chúng tôi hiển thị tất cả các biểu đồ pass@ kvới các nhiệt độ khác nhau trong Hình 6.
15

--- TRANG 16 ---
Được xuất bản tại hội nghị ICLR 2023
# lấy nhãn của một phân vùng fat32
fatlabel /dev/sda1
# END
# hiển thị thông tin mà không bao gồm các cột login, jcpu và pcpu
w --short
# END
# sắp xếp tệp csv theo cột 9
csvsort -c 9 data.csv
# END
# tìm kiếm một gói trong các nguồn hiện tại của bạn
Tài liệu tiềm năng 0: fatlabel sẽ hiển thị hoặc thay đổi nhãn âm lượng hoặc ID âm lượng trên hệ thống tệp MS- DOS
được đặt trên DEVICE ...
# lấy nhãn của một phân vùng fat32
fatlabel /dev/sda1
# END
Tài liệu tiềm năng 0: w hiển thị thông tin về các người dùng hiện tại trên máy, và các
tiến trình của họ. Header hiển thị, theo thứ tự này ...
Tài liệu tiềm năng 1: -s, –short Sử dụng định dạng ngắn. Không in thời gian đăng nhập, JCPU hoặc PCPU
times.
# hiển thị thông tin mà không bao gồm các cột login, jcpu và pcpu
w --short
# END
Tài liệu tiềm năng 0: Sắp xếp các tệp CSV. Giống như lệnh Unix "sort", nhưng cho dữ liệu bảng
Tài liệu tiềm năng 1: usage: csvsort [-h] [-d DELIMITER] [-t] [-q QUOTECHAR] [-u 0,1,2,3] [-b]
[-p ESCAPECHAR] ...
Tài liệu tiềm năng 2: optional arguments: -h, –hel show this help message and exit -n, –names
Display column names and indices from the input CSV and exit. -c COLUMNS ...
Tài liệu tiềm năng 3: csvsort -c 9 examples/realdata/FY09 EDU Recipients byState.csv
Tài liệu tiềm năng 4: csvcut -c 1,9 examples/realdata/FY09 EDU Recipients byState.csv — csvsort
-r -c 2 — head -n 5
# sắp xếp tệp csv theo cột 9
csvsort -c 9 data.csv
# END
Tài liệu tiềm năng 1: ...
Tài liệu tiềm năng 2: ...
...
# tìm kiếm một gói trong các nguồn hiện tại của bạn
Bảng 7: Trên: prompt Codex baseline với ba cặp NL-code và một ý định kiểm tra. Dưới: DocPrompt-
ingprompt cho Codex. Trong mỗi ví dụ in-context learning, các docs oracle, ý định NL và
lệnh bash tương ứng được cung cấp. Chúng tôi sử dụng tối đa năm docs oracle cho những ví dụ này. Cho
một ví dụ kiểm tra, top- 5đoạn văn từ bộ truy xuất được biểu diễn với ý định NL. Nội dung của tài liệu
đã được bỏ qua ("...") để tiết kiệm không gian.
16

--- TRANG 17 ---
Được xuất bản tại hội nghị ICLR 2023
Bảng 8: Chồng chéo n-gram giữa các nội dung khác nhau (%). Việc sử dụng tài liệu tăng đáng kể
chồng chéo n-gram recall giữa đầu vào và đầu ra, trong tldr vàCoNaLa .
tldr 1 2 3
NL← →Code 12 0 0
(NL+retrieved docs) ← →Code 24 2 0
NL← →Retrieved docs 39 8 3CoNaLa 1 2 3 4 5
NL← →Code 30 14 11 9 7
(NL+retrieved docs) ← →Code 91 52 28 16 11
NL← →Retrieved docs 72 14 3 1 1
0 25 50 75 100 125 150 175 200
k0.060.080.100.120.140.160.18pass@k
temperature=0.2
CodeT5
+DocPrompting
0 25 50 75 100 125 150 175 200
k0.0500.0750.1000.1250.1500.1750.2000.225pass@k
temperature=0.4
CodeT5
+DocPrompting
0 25 50 75 100 125 150 175 200
k0.050.100.150.200.250.30pass@k
temperature=0.6
CodeT5
+DocPrompting
0 25 50 75 100 125 150 175 200
k0.050.100.150.200.250.300.35pass@k
temperature=0.8
CodeT5
+DocPrompting
0 25 50 75 100 125 150 175 200
k0.050.100.150.200.250.300.35pass@k
temperature=1.0
CodeT5
+DocPrompting
Hình 6: Pass@ ktrên 100 ví dụ trên tập kiểm tra với các nhiệt độ khác nhau.
17

--- TRANG 18 ---
Được xuất bản tại hội nghị ICLR 2023
Bảng 9: Kết quả trên tldr vàCoNaLa với code-davinci-002 .
tldr
Mô hình CMD Acc (%) EM (%) Token F1 charBLEU
Codex - 39.01 14.55 44.89 33.93
3-shots +DocPrompting 36.10 13.97 42.55 32.93
Với tên lệnh oracle
- - 20.22 59.22 38.14
+DocPrompting - 33.15 68.59 44.76
CoNaLa
BLEU Recall
- 48.39 43.35
+DocPrompting 47.21 44.70
+DocPrompting oracle docs 54.67 59.68
H THỬ NGHIỆM VỚI code-davinci-002
Kết quả với code-davinci-002 dưới cài đặt few-shot learning được hiển thị trong Bảng 9. Trong các cài đặt không oracle,
Codex+ DocPrompting không cải thiện so với base Codex; một giải thích có thể là các tập dữ liệu bị
rò rỉ vào corpus huấn luyện của Codex. Ví dụ, CoNaLa được trích xuất từ StackOverﬂow, được
bao gồm trong corpus CommonCrawl lớn8được sử dụng để huấn luyện GPT-3, và có thể cả Codex. Do đó, Codex
có thể đã ghi nhớ mã đích, và do đó không cần tài liệu bổ sung. Mặc dù vấn đề rò rỉ dữ liệu có thể
đã xảy ra trong code-davinci-001, chúng tôi nghi ngờ rằng vấn đề này đã trở nên tồi tệ hơn trong phiên bản 002mạnh hơn. Dù sao, chúng tôi tin rằng khả năng lớn của Codex yêu cầu một bộ truy xuất mạnh tương đương
để cải thiện so với mô hình cơ sở. Với bộ truy xuất oracle, DocPrompting mang lại cải thiện đáng kể trên
cả hai tập dữ liệu. Do đó, kết quả không oracle có thể được cải thiện thêm bằng cách sử dụng bộ truy xuất không oracle mạnh hơn.
I VÍ DỤ
I.1 T L D R
Các ví dụ trên tldr có trong Bảng 10 . Trong ba trường hợp đầu, baseline T5 không thể tạo ra lệnh bash đúng
trong khi T5+ DocPrompting truy xuất tài liệu bash đúng và sử dụng các đối số đúng. Trong hai trường hợp dưới
, T5 cũng dự đoán lệnh bash đúng. Tuy nhiên, trong trường hợp puppet, nó thiếu
các cách sử dụng cụ thể của việc thực thi mã. Trong trường hợp elixir, nó tạo ra các cờ đối số --example và
--arguments mà elixir hoàn toàn không hỗ trợ. Mặt khác, T5+ DocPrompting thực sự dựa
việc tạo của nó vào các docs được truy xuất và do đó nó tạo ra mã chính xác hơn.
I.2CONALA
Thêm ví dụ trên CoNaLa được hiển thị trong Bảng 11.
8https://commoncrawl.org/the-data/
18

--- TRANG 19 ---
Được xuất bản tại hội nghị ICLR 2023
Bảng 10: Ví dụ về dự đoán từ tldr , của base T5 so với T5+ DocPrompting . Trong
tldr , các lệnh bash trong tập kiểm tra hoàn toàn khác với những cái trong tập huấn luyện.
chơi một tệp midi trong một key cụ thể (0 = c major/a minor, 1 = f major/d minor, +1 = g major/e minor, etc.)
Ground truth : timidity --force-keysig='-flats|+sharps' 'path/to/file.mid'
T5: midiplay 'key'
T5+DocPrompting :timidity --force-keysig 'f|f|f' 'path/to/file.midi'
dọn dẹp tất cả các tệp tex tạm thời trong thư mục hiện tại
Ground truth : latexmk -c
T5: tex clean
T5+DocPrompting :latexmk -c
chỉ định thuật toán hash cho tệp chỉ mục (thử nghiệm)
Ground truth : git show-index --object-format='sha1|sha256' 'path/to/file'
T5: hash --experimental 'path/to/index.hashes'
T5+DocPrompting :git show-index --object-format= 'path/to/hash-algorithm'
'path/to/experimental'
thực thi mã puppet
Ground truth : puppet apply --execute 'code'
T5: puppet
T5+DocPrompting :puppet apply --execute 'path/to/file.puppet'
đánh giá mã elixir bằng cách truyền nó như một đối số
Ground truth : elixir -e 'code'
T5: elixir --example --arguments 'path/to/file.elixir'
T5+DocPrompting :elixir -e 'path/to/file.elixir'
Bảng 11: Ví dụ về dự đoán từ CoNaLa , của base CodeT5 so với
CodeT5+ DocPrompting . Các hàm chưa thấy được::::::::::gạch dưới.
đặt thư mục làm việc hiện tại thành 'c:nUsers nuname ndesktop npython'
Ground truth ::::::::::os.chdir('c: nUsers nuname ndesktop npython')
CodeT5 : os.system('c: nUsers nuname ndesktop npython')
CodeT5+ DocPrompting ::::::::::os.chdir('c: nUsers nuname ndesktop npython')
chuyển đổi dataframe 'df' thành đối tượng thưa thớt kiểu integer
Ground truth ::::::::::::::df.to sparse(0)
CodeT5 : np.isinstance(df, np.integer)
CodeT5+ DocPrompting ::::::::::::::df.to sparse('i')
19
