# 2403.15268.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2403.15268.pdf
# Kích thước tệp: 1011049 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Đánh thức Tăng cường Sinh thành: Học cách Đánh thức Kiến thức Nội tại của Mô hình Ngôn ngữ Lớn để Trả lời Câu hỏi
Huanxuan Liao1,2, Shizhu He1,2*, Yao Xu1,2, Yuanzhe Zhang1,
Shengping Liu3,Kang Liu1,2,Jun Zhao1,2
1Phòng thí nghiệm Nhận thức và Trí tuệ Quyết định cho Hệ thống Phức tạp,
Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
2Trường Trí tuệ Nhân tạo, Đại học Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
3Unisound, Bắc Kinh, Trung Quốc
liaohuanxuan2023@ia.ac.cn {yao.xu, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn

Tóm tắt
Tăng cường Sinh thành bằng Truy xuất và Tăng cường Sinh thành bằng Sinh thành đã được đề xuất để nâng cao kiến thức cần thiết cho việc trả lời câu hỏi với Mô hình Ngôn ngữ Lớn (LLM) bằng cách tận dụng ngữ cảnh phong phú hơn. Tuy nhiên, phương pháp trước phụ thuộc vào tài nguyên bên ngoài, và cả hai đều yêu cầu kết hợp các tài liệu rõ ràng vào ngữ cảnh, điều này làm tăng chi phí thực thi và tính dễ bị ảnh hưởng bởi dữ liệu nhiễu trong quá trình suy luận. Các nghiên cứu gần đây chỉ ra rằng LLM mô hình hóa kiến thức phong phú, nhưng nó thường không được kích hoạt và đánh thức một cách hiệu quả. Được truyền cảm hứng từ điều này, chúng tôi đề xuất một khung tăng cường kiến thức mới, Tăng cường Sinh thành bằng Đánh thức (AAG), mô phỏng khả năng của con người trả lời câu hỏi chỉ bằng cách suy nghĩ và gợi nhớ để bù đắp cho những khoảng trống kiến thức, từ đó đánh thức kiến thức liên quan trong LLM mà không cần dựa vào tài nguyên bên ngoài. AAG bao gồm hai thành phần chính để đánh thức ngữ cảnh phong phú hơn. Đánh thức rõ ràng tinh chỉnh một bộ tạo ngữ cảnh để tạo ra một tài liệu tổng hợp, nén hoạt động như ngữ cảnh biểu tượng. Đánh thức ngầm sử dụng một siêu mạng để tạo ra các bộ chuyển đổi dựa trên câu hỏi và tài liệu tổng hợp, được chèn vào LLM để phục vụ như ngữ cảnh tham số. Kết quả thực nghiệm trên ba bộ dữ liệu cho thấy AAG có những ưu thế đáng kể trong cả cài đặt miền mở và sách đóng, cũng như trong khái quát hóa ngoài phân phối. Mã của chúng tôi sẽ có sẵn tại https://github.com/Xnhyacinth/IAG.

1 Giới thiệu
Chúng ta có thể biết nhiều hơn những gì chúng ta có thể nói. — Michael Polanyi

Các nhiệm vụ đòi hỏi kiến thức chuyên sâu như trả lời câu hỏi (QA) đòi hỏi việc sử dụng kiến thức thế giới và miền rộng rãi (Berant et al., 2013; Joshi et al., 2017; Kwiatkowski et al., 2019). Ngày nay, Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện năng lực đáng chú ý trong hầu hết mọi nhiệm vụ và ngành công nghiệp (Liu et al., 2023b). Tuy nhiên, LLM thiếu khả năng đầy đủ để xử lý độc lập các nhiệm vụ đòi hỏi kiến thức chuyên sâu (Frisoni et al., 2024) và thường tạo ra ảo giác (Zhao et al., 2023).

Trong những năm gần đây, để giải quyết ảo giác trong LLM và nâng cao hiệu suất trả lời câu hỏi, các nhà nghiên cứu đã phát triển một số phương pháp tăng cường kiến thức cho LLM. Các phương pháp này chủ yếu chia thành hai loại: Tăng cường Sinh thành bằng Truy xuất (RAG) (Guu et al., 2020) truy xuất tài liệu từ tài nguyên bên ngoài (ví dụ: Wikipedia) và kết hợp cả tài liệu được truy xuất và câu hỏi vào LLM (Izacard and Grave, 2021) (phần trên của Hình 1). Tăng cường Sinh thành bằng Sinh thành (GAG) (Kim et al., 2024) sử dụng LLM như ChatGPT (Ouyang et al., 2022) để tạo ra các tài liệu liên quan hơn, sau đó được sử dụng để nâng cao việc tạo ra câu trả lời (phần giữa của Hình 1).

Tuy nhiên, các phương pháp này có những nhược điểm sau¹: 1) Phụ thuộc vào tài nguyên bên ngoài, RAG dựa vào tài nguyên kiến thức miền bên ngoài (Ke et al., 2024), trong khi GAG phụ thuộc vào một LLM bên ngoài mạnh hơn như một bộ tạo kiến thức. Sự phụ thuộc này hạn chế ứng dụng rộng rãi hơn của chúng. 2) Chi phí thực thi tăng, tài nguyên tính toán và thời gian suy luận cần thiết tăng đáng kể theo số lượng tài liệu. Ví dụ, phương pháp RAG điển hình FiD (Izacard and Grave, 2021) phải xử lý hơn 12.000 token để truy xuất 100 tài liệu, dẫn đến tăng độ dài prompt hơn 100 lần và tăng thời gian suy luận hơn 100² lần (Liu et al., 2023a). Tương tự, phương pháp GAG (Yu et al., 2023) phát sinh thêm chi phí tài chính, như lời gọi API. 3) Đào tạo lại cụ thể, các phương pháp này thường yêu cầu đào tạo lại cho các miền, nhiệm vụ và bộ dữ liệu khác nhau (Li et al., 2024). Điều này làm tăng thách thức trong việc tái sử dụng mô hình qua các tình huống khác nhau, dẫn đến kém hiệu quả tài nguyên do tính hiệu quả tham số thấp và nhu cầu dữ liệu rộng lớn.

Thực tế, LLM vốn có kiến thức phong phú và tiềm năng đáng kể cho việc giải quyết các nhiệm vụ đòi hỏi kiến thức chuyên sâu (Bhagavatula et al., 2020). Hiệu suất trên các nhiệm vụ cụ thể có thể được cải thiện bằng cách kích hoạt và đánh thức kiến thức liên quan một cách hiệu quả hơn mà không cần tài nguyên bên ngoài. Ví dụ, các chiến lược như lặp lại câu hỏi hai lần (Xu et al., 2023), củng cố kiến thức với prompt như "Theo tôi biết" (Yao et al., 2023), và sử dụng mô hình thị giác-ngôn ngữ để tưởng tượng hình ảnh (Tang et al., 2023) đều có thể nâng cao hiệu suất của LLM trên các nhiệm vụ downstream. Có nghĩa là, LLM mô hình hóa kiến thức phong phú, nhưng nó thường không được kích hoạt và đánh thức một cách hiệu quả.

Được truyền cảm hứng từ các phát hiện trên và để giảm thiểu các thách thức trong RAG và GAG, chúng tôi đề xuất một khung tăng cường kiến thức mới được gọi là Tăng cường Sinh thành bằng Đánh thức (AAG) mô phỏng khả năng của con người bù đắp cho thiếu hụt kiến thức thông qua suy nghĩ và gợi nhớ trong QA. AAG sử dụng bộ tạo ngữ cảnh để tạo ra một tài liệu giả nén như ngữ cảnh biểu tượng đồng thời giảm nhu cầu tính toán. Ví dụ, AAG sử dụng "ngôn ngữ chính thức ... Jamaica" (chỉ 20 token) như kiến thức thay vì "Jamaica được coi là... ngôn ngữ chính thức là tiếng Anh..." (>200 token) trong RAG hoặc GAG cho câu hỏi "người Jamaica nói gì?" trong WebQ (Berant et al., 2013). Ngoài ra, AAG

¹Một so sánh trực quan hơn có thể thấy trong A.1.

sử dụng siêu mạng để tạo ra các bộ chuyển đổi như ngữ cảnh tham số cho mỗi câu hỏi, tích hợp ưu điểm của học tập dựa trên hướng dẫn với các mô-đun hiệu quả tham số để đánh thức ngữ cảnh phong phú hơn trong LLM (phần dưới của Hình 1).

Cụ thể, để đánh thức đầy đủ kiến thức vốn có của LLM, chúng tôi thiết kế hai mô-đun chính để thu được các loại ngữ cảnh khác nhau và cải thiện việc sử dụng kiến thức liên quan trong LLM. Mô-đun đánh thức rõ ràng đầu tiên sử dụng chưng cất biểu tượng để nén ngữ cảnh, sau đó tinh chỉnh bộ tạo ngữ cảnh để tạo ra một tài liệu giả súc tích, giảm hiệu quả độ dài xử lý văn bản. Tiếp theo, trong khung chưng cất kiến thức, mô-đun đánh thức ngầm sử dụng siêu mạng để chuyển đổi câu hỏi và dữ liệu nhiệm vụ khác (ví dụ: tài liệu) thành các bộ chuyển đổi được chèn vào LLM. Việc tạo ra động này cho phép tạo ra mô-đun thích ứng và liên quan ngữ cảnh hơn, nâng cao khả năng của mô hình xử lý các nhiệm vụ đa dạng và phức tạp một cách hiệu quả.

Ý tưởng cốt lõi của AAG là cho phép các mô hình học sinh thiếu thông tin ngữ cảnh phong phú mô phỏng các mô hình giáo viên có thông tin như vậy.

Chúng tôi đánh giá AAG được đề xuất trên các LLM khác nhau, bao gồm T5 (Roberts et al., 2020a) và Llama2 (Touvron et al., 2023). Kết quả thực nghiệm trên các bộ dữ liệu NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) và WebQ cho thấy AAG được đề xuất mang lại lợi ích hiệu suất đồng thời giảm chi phí và thời gian tính toán trong suy luận. Đáng chú ý, nó vượt trội hơn các baseline truy xuất và tạo ra kiến thức 2% trong cùng cài đặt tài liệu và có thể đạt được hiệu suất tương tự đồng thời giảm chi phí suy luận (token được xử lý) lên đến 4×. Tóm lại, các đóng góp của bài báo này được tóm tắt như sau:

•Chúng tôi đề xuất một khung tăng cường kiến thức mới AAG để đánh thức ngữ cảnh phong phú hơn (ngữ cảnh biểu tượng và tham số) hiệu quả hơn mà không cần dựa vào tài nguyên bên ngoài.

•Chúng tôi sử dụng siêu mạng có điều kiện văn bản để tạo ra các mô-đun hiệu quả tham số như ngữ cảnh tham số dựa trên câu hỏi và một tài liệu nén giả.

•Kết quả thực nghiệm cho thấy AAG đánh thức hiệu quả kiến thức liên quan của LLM, thể hiện ưu thế đáng kể trong cả cài đặt miền mở và sách đóng đồng thời giảm chi phí suy luận.

--- TRANG 2 ---
2 Nghiên cứu Liên quan
Bài báo này chủ yếu sử dụng nén ngữ cảnh, siêu mạng và chưng cất kiến thức để đạt được tăng cường kiến thức. Sau đây sẽ làm rõ nghiên cứu liên quan qua bốn khía cạnh.

Tăng cường Kiến thức thường được áp dụng để giảm thiểu vấn đề thiếu kiến thức trong LLM. Có hai phương pháp chính: RAG (Sun et al., 2019; Wang et al., 2024) và GAG (Abdallah and Jatowt, 2023). Phương pháp RAG điển hình FiD (Izacard and Grave, 2021) truy xuất tài liệu từ Wikipedia để trả lời câu hỏi. LLM phục vụ như cơ sở kiến thức đã là trọng tâm của nhiều nghiên cứu ủng hộ việc trích xuất kiến thức từ các mô hình như vậy (ví dụ: GPT-3). Chẳng hạn, Yu et al. (2023) tạo ra 10 tài liệu cho mỗi câu hỏi. Tuy nhiên, RAG yêu cầu tài nguyên bên ngoài, và cả RAG và GAG đều cần ngữ cảnh dài dòng. Gần đây, các phương pháp đã được phát triển để nâng cao khả năng LLM bằng cách mô phỏng trí tưởng tượng của con người về thông tin thị giác bằng cách sử dụng các mô hình thị giác-ngôn ngữ hiện có (Tang et al., 2023; Akter et al., 2024). Phương pháp đề xuất của chúng tôi không chỉ loại bỏ nhu cầu tài nguyên bên ngoài mà còn cải thiện hiệu quả kích hoạt kiến thức nội tại trong LLM.

Nén Ngữ cảnh thường được sử dụng để cải thiện hiệu quả của LLM trong việc xử lý ngữ cảnh dài. Các nghiên cứu gần đây (Mu et al., 2023) đề xuất rằng ngữ cảnh dài được cô đọng thành các vector tóm tắt (prompt mềm) để đảm bảo việc sử dụng hiệu quả của chúng bởi LLM. Đồng thời, một số nghiên cứu (Jiang et al., 2023; Pan et al., 2024) gợi ý sử dụng tính dư thừa thông tin và entropy trong văn bản dài để nén ngữ cảnh (Li et al., 2023). Khác với các phương pháp này, bài báo này nhằm nâng cao khả năng mô hình hóa ngữ cảnh dài của LLM. Bằng cách phát triển một bộ tạo ngữ cảnh tạo ra ngữ cảnh nén, mô hình QA hoạt động trên ngữ cảnh ngắn có thể đạt được hiểu biết ngữ cảnh phong phú tương tự như các mô hình được thiết kế cho ngữ cảnh dài hơn.

Chưng cất Kiến thức là một kỹ thuật trong đó một mô hình nhỏ hơn học cách mô phỏng dự đoán của một mô hình lớn hơn, nhằm duy trì hiệu suất đồng thời giảm tài nguyên tính toán (Hinton et al., 2015). Các nghiên cứu gần đây (West et al., 2022) trình bày chưng cất kiến thức biểu tượng, một quá trình tạo điều kiện chuyển giao kiến thức từ mô hình giáo viên thông qua trích xuất dữ liệu huấn luyện để sau đó huấn luyện mô hình học sinh (Wang et al., 2023b; Ranaldi and Freitas, 2024). Trong bài báo này, quá trình thu được ngữ cảnh nén trong quá trình tinh chỉnh bộ tạo ngữ cảnh tương tự như một dạng chưng cất biểu tượng. Về huấn luyện, chúng tôi nhấn mạnh việc chưng cất khả năng mô hình hóa ngữ cảnh dài của LLM.

Siêu mạng được thiết kế để giảm số lượng tham số (Ha et al., 2016), tức là một mạng neural nhỏ tạo ra tham số cho một mạng neural lớn khác. Nó cung cấp một giải pháp giảm sự phụ thuộc vào gradient descent cho các miền cụ thể. Các nghiên cứu gần đây (Phang et al., 2022; Ivison et al., 2023) đã khám phá việc nâng cao hiệu suất mô hình trong cài đặt zero/few-shot thông qua meta-learning liên quan đến siêu mạng. Chúng tôi sử dụng siêu mạng để có được ngữ cảnh tham số bằng cách chuyển đổi động câu hỏi và dữ liệu khác thành các bộ chuyển đổi được chèn vào LLM để đạt hiệu quả và khái quát hóa.

3 Phương pháp
Trong phần này, chúng tôi giới thiệu chi tiết AAG để kích hoạt kiến thức nội tại của LLM và thu được ngữ cảnh phong phú hơn cho QA. Tiền đề cơ bản của phương pháp này là QA với ngữ cảnh phong phú hơn (mô hình giáo viên) mang lại biểu diễn nội tại tốt hơn và hiệu suất lớn hơn (ví dụ: RAG với tài liệu được truy xuất). Do đó, để cho phép mô hình học sinh không có tài liệu bên ngoài như ngữ cảnh cũng sở hữu ngữ cảnh phong phú, cần thiết phải học cách tự tạo ra ngữ cảnh (mặc dù không quá dài) và cho phép mô hình học sinh mô phỏng và có được biểu diễn nội tại phong phú.

Cụ thể, như được thể hiện trong Hình 2, AAG bao gồm hai mô-đun chính. Đánh thức rõ ràng với nén ngữ cảnh dài học cách tạo ra một tài liệu giả nén (§ 3.2). Đánh thức ngầm với siêu mạng tận dụng kiến thức ẩn học một phép chiếu đặc trưng kiến thức chung qua các câu hỏi (§ 3.3). Siêu mạng được huấn luyện để tạo ra các mô-đun LoRA nhẹ để căn chỉnh câu hỏi và kiến thức nội tại. Bên cạnh đó, có chưng cất ngữ cảnh dài trong huấn luyện, học biểu diễn phong phú của giáo viên để bù đắp cho kiến thức bị thiếu trong học nhãn (§ 3.4).

3.1 Công thức hóa
Công thức hóa nhiệm vụ của chúng tôi tuân theo RAG cho QA (Guu et al., 2020). Gọi V* biểu thị tập hợp vô hạn, bao gồm tất cả các chuỗi tiềm năng trên các token trong từ vựng V, và bao gồm cả chuỗi rỗng. Một thể hiện trong bộ dữ liệu QA được định nghĩa là một

--- TRANG 3 ---
ba phần tử (q,a,c) bao gồm câu hỏi q, câu trả lời a, và ngữ cảnh c, trong đó q,a,c∈ V*. Theo quy ước, ngữ cảnh c được rút ra từ kho kiến thức Z, như Wikipedia, trong đó Z ⊂ V*. Chi tiết nền tảng bổ sung có sẵn trong B.1.

3.2 Đánh thức Rõ ràng với Bộ tạo Ngữ cảnh

Để thu được tài liệu giả ngắn d, chúng tôi tinh chỉnh một bộ tạo ngữ cảnh² để sử dụng kiến thức của nó trong việc tạo ra một tài liệu giả nén như ngữ cảnh biểu tượng, từ đó giảm độ dài đầu vào. Đồng thời, chúng tôi tránh phụ thuộc vào cơ sở kiến thức cố định và giảm thiểu lỗi kho kiến thức bằng cách kết hợp ngữ cảnh có thể hữu ích (Lee et al., 2023). Sử dụng khung chưng cất kiến thức, mô hình học sinh học cách tạo ra văn bản nén mà mô hình giáo viên tạo ra dựa trên ngữ cảnh rộng lớn.

Cụ thể, cho mỗi điểm dữ liệu Dtrain = {(qi,ai,ci)}ⁿᵢ₌₁, chúng tôi áp dụng phương pháp nén ngữ cảnh dài LongLLMLingua (Jiang et al., 2023) cho văn bản được truy xuất ci, dẫn đến văn bản nén c'i. Như được thể hiện trong phần bên trái của Hình 2, sau đó, chúng tôi tinh chỉnh bộ tạo ngữ cảnh pθ với các tham số có thể huấn luyện θ để tận dụng đầy đủ kiến thức vốn có của nó để tạo ra c'i, hướng dẫn mô hình suy nghĩ về kiến thức của nó và tạo ra một tài liệu giả ngắn. Mục tiêu của chúng tôi là giảm thiểu log-likelihood âm của chuỗi văn bản nén c'i cho một prompt cụ thể p(B.2) và câu hỏi qi.

Lce=−1/n ∑ⁿᵢ₌₁ logpθ(c'i|p,qi) (1)

Quá trình này cho phép LLM hình dung tài liệu nén một cách vững chắc song song với yêu cầu kiến thức của câu hỏi.

3.3 Đánh thức Ngầm với Siêu mạng

Nói chung, ngữ cảnh phong phú hơn có thể giúp LLM trả lời câu hỏi tốt hơn. Có nghĩa là, biểu diễn của câu hỏi và trạng thái nội tại của LLM khi sử dụng ngữ cảnh phong phú là những trạng thái tốt hơn. Do đó, trong trường hợp không có ngữ cảnh, chúng ta nên tập trung vào

²Chúng tôi thảo luận về vai trò của bộ tạo ngữ cảnh trong A.2.

việc xây dựng mô hình để đánh thức LLM đạt được trạng thái tốt hơn này và như một mô hình QA tốt hơn.

Chúng tôi sử dụng siêu mạng³ để chuyển đổi câu hỏi q và tài liệu giả ngắn d thành một mô-đun LoRA hiệu quả tham số cụ thể được chèn vào LLM, phục vụ như ngữ cảnh tham số cho câu hỏi. Điều này tương tự như lặp lại câu hỏi trong prompt (Xu et al., 2023) và kết hợp các gợi ý chủ đề nhất định để kích thích việc gọi lại của mô hình về các câu hỏi liên quan (Wang et al., 2023c). Tuy nhiên, sự khác biệt nằm ở chỗ chúng hoạt động như các đặc trưng đánh thức, trong khi chúng tôi đang tạo ra các tham số mô hình như đánh thức kiến thức.

Kiến trúc siêu mạng để tạo ra trọng số LoRA được chi tiết trong Hình 3. Cụ thể, Dqk và Uqk biểu thị các phép chiếu xuống và lên rank thấp của lớp k liên kết với ma trận Query WQ trong mô-đun attention, trong khi Dvk và Uvk tương ứng với những phép chiếu liên kết với ma trận Value WV. Siêu mạng, được ký hiệu là gD và gU, nhận concat (f, i{q,v}k) làm đầu vào, trong đó f là vector đặc trưng thu được bằng cách sử dụng encoder của mô hình và giảm chiều thông qua thuật toán whitening (Su, 2021). Để đạt được biến đổi whitening này, trước tiên chúng tôi tính toán mean của vector μ=1/N ∑Nᵢ₌₁xi và căn giữ dữ liệu bằng cách trừ μ từ mỗi vector xi. Tiếp theo, chúng tôi tính toán ma trận hiệp phương sai C của các vector đã được căn giữ x̃i=xi−μ, được cho bởi C=1/N ∑Nᵢ₌₁x̃ix̃iTᵢ. Sau đó chúng tôi thực hiện Phân tích Giá trị Đơn (SVD) trên ma trận hiệp phương sai: C=UΛUT, trong đó U chứa các eigenvector và Λ là ma trận đường chéo của eigenvalue. Ma trận biến đổi W được dẫn xuất từ phân tích eigenvalue như W=UΛ⁻¹/², trong đó Λ⁻¹/² chia tỷ lệ các eigenvector bằng căn bậc hai nghịch đảo của eigenvalue tương ứng của chúng. Do đó, áp dụng biến đổi x̃i = (x̃i)W không chỉ căn giữ dữ liệu quanh không mà còn dẫn đến ma trận hiệp phương sai tương đương với ma trận đơn vị, đảm bảo rằng các vector được biến đổi không tương quan và có phương sai đơn vị. Thuật ngữ idx{q,v}k ∈ {0, . . . , 2×#blocks} biểu thị embedding vị trí, phân biệt giữa các lớp và QV. Mỗi siêu mạng được đặc trưng bởi trọng số Wd và Wu, biểu thị các phép chiếu xuống và lên tương ứng. Phương trình siêu mạng cho D{q,v} được biểu diễn như sau:

fi=whitening (Encoder (qi;di)) (2)

³Chúng tôi tiến hành phân tích chi tiết về lý do đằng sau siêu mạng trong A.3.

g(x) = Wu·ReLU (Wd·x) (3)
D{q,v}=gD((fi;idx{q,v}k)) (4)

trong đó Encoder biểu thị encoder của mô hình, whitening là thuật toán giảm chiều, ReLU là hàm kích hoạt, và idxqk= 2k, idxvk= 2k+1. gD và gU biểu thị các hàm giảm chiều và tăng chiều của siêu mạng tương ứng.

3.4 Huấn luyện với Chưng cất Ngữ cảnh Dài

Trong khung chưng cất kiến thức, các yếu tố như biểu diễn ẩn (Jiao et al., 2020), phụ thuộc attention (Wang et al., 2020), và mối quan hệ giữa các biểu diễn (Park et al., 2021) được coi là cần thiết cho việc chuyển giao kiến thức hiệu quả. Trong bài báo này, chúng tôi giới thiệu chưng cất ngữ cảnh dài (LCD) như kiến thức ngữ cảnh hóa chủ yếu hướng dẫn mô hình học sinh.

Cụ thể, mô hình giáo viên, FiD (Izacard and Grave, 2021), xử lý đầu vào ngữ cảnh dài hơn, về mặt lý thuyết chứa nhiều thông tin hơn do ngữ cảnh phong phú hơn. Điều này cho phép nó kích hoạt kiến thức nội tại cụ thể hơn, phục vụ như một mô hình giám sát. Mô hình giáo viên hỗ trợ mô hình học sinh, T5 (Roberts et al., 2020a), có cùng kích thước nhưng sử dụng đầu vào ngữ cảnh ngắn hơn, trong việc kích hoạt biểu diễn đặc trưng và kiến thức phong phú hơn. Mục tiêu tối ưu hóa cho mô hình học sinh tại mỗi mini-batch zr= (xr, yr) là:

Ls(θs, θt, zr) =αLce(yr, S(xr;θs)) +(1−α)Lce(T(xr;θt), S(xr;θs)) (5)

trong đó chúng ta có mô hình giáo viên được ký hiệu là T(·;θt) và mô hình học sinh được ký hiệu là S(·;θs). Các tham số mô hình tương ứng là θt và θs.

Như được minh họa ở bên phải của Hình 2, chúng tôi thực hiện căn chỉnh biểu diễn bổ sung để tạo điều kiện chuyển giao kiến thức tốt hơn. Trong quá trình chưng cất của chúng tôi, cả mô hình giáo viên và học sinh đều có L lớp. Văn bản đầu vào được xử lý qua các lớp này, tạo ra các trạng thái ẩn đầu ra tương ứng {Htl}Ll=0 và {Hsl}Ll=0, cùng với các ma trận attention {Atl}Ll=1 và {Asl}Ll=1. Để căn chỉnh trạng thái ẩn, chúng tôi tính toán độ gần gũi giữa trạng thái ẩn của giáo viên và học sinh bằng cách sử dụng khoảng cách cosine (COS) (Park et al., 2021).

Lhid=−COS(Hsl, Htl) (6)

Trong khi để căn chỉnh phụ thuộc attention, chúng tôi tuân theo (Jiao et al., 2020) để tối ưu hóa sai số bình phương trung bình (MSE) giữa các ma trận attention của giáo viên và học sinh:

Lattn=−MSE(Asl, Atl) (7)

Mục tiêu tổng thể cho việc chuyển giao kiến thức là:

Lalign(Hsl, Htl, Asl, Atl) =Lattn+Lhid (8)

Mục tiêu tổng thể cho việc huấn luyện AAG là tổng có trọng số của hai mục tiêu:

L=Ls+λLalign (9)

4 Thí nghiệm
Trong phần này, chúng tôi tiến hành thí nghiệm để chứng minh tính hiệu quả và hiệu suất của AAG trên QA. Thí nghiệm chủ yếu trả lời bốn câu hỏi nghiên cứu (RQ):

RQ1: AAG có thể đạt được tăng cường kiến thức cho QA trên LLM không? (§ 4.4)
RQ2: AAG có khả năng khái quát hóa ngoài phân phối tốt không? (§ 4.5)
RQ3: AAG có ưu thế về hiệu quả và hiệu suất so với RAG và GAG không? (§ 4.6)
RQ4: Vai trò của các mô-đun đánh thức rõ ràng và ngầm trong AAG là gì? (§ 4.7)

4.1 Bộ dữ liệu
Chúng tôi đánh giá phương pháp đề xuất trên ba bộ dữ liệu trả lời câu hỏi công khai: NaturalQuestions (NQ) (Kwiatkowski et al., 2019), WebQuestions (WQ) (Berant et al., 2013) và TriviaQA (TQA) (Joshi et al., 2017). Để đánh giá hiệu suất mô hình, chúng tôi sử dụng điểm exact match (EM) để đánh giá câu trả lời dự đoán (Rajpurkar et al., 2016). Chúng tôi cung cấp chi tiết bộ dữ liệu trong B.4.

4.2 Baseline
Cả mô hình ngôn ngữ cỡ vừa (<1B) và mô hình ngôn ngữ lớn (≥3B) đều được xem xét. T5 (Roberts et al., 2020a) được chọn làm backbone cho các mô hình ngôn ngữ cỡ vừa của chúng tôi. Chúng tôi đánh giá AAG được đề xuất so với một số phương pháp tăng cường kiến thức, bao gồm các mô hình RAG như DPR (Karpukhin et al., 2020), RAG (Lewis et al., 2020), EAR (Chuang et al., 2023), RFiD (Wang et al., 2023a), FILCO (Wang et al., 2023d) và FiD (Izacard and Grave, 2021), cũng như mô hình GAG GENREAD (Yu et al., 2023), và phương pháp tinh chỉnh hiệu quả tham số LoRA (Hu et al., 2021).

Để chứng minh khả năng plug-and-play của AAG trên cài đặt zero-shot của LLM (≥3B), chúng tôi sử dụng Llama2-7B và -13B (Touvron et al., 2023) làm mô hình cơ bản. Chúng tôi đánh giá với 6 cài đặt đa dạng: không truy xuất, có truy xuất, với LoRA, RECITE (Sun et al., 2023), HICL (Wang et al., 2024) và sử dụng AAG được đề xuất.

4.3 Triển khai
Trong giai đoạn pretraining, bộ tạo ngữ cảnh được khởi tạo với T5-large sử dụng các cặp câu hỏi-nén được tạo ra. Trong giai đoạn thứ hai, mô hình giáo viên sử dụng một FiD reader với các kích thước khác nhau (FiD-l và FiD-xl) được tinh chỉnh trên phần huấn luyện của bộ dữ liệu đích. Mô hình học sinh đóng băng backbone và chỉ cập nhật siêu mạng, các lớp FFN và norm. B.3 chứa thêm chi tiết triển khai và baseline.

4.4 Kết quả Chính

4.4.1 Cài đặt Có giám sát
Bảng 1 trình bày kết quả hiệu suất, với kết quả đầy đủ bao gồm T5-Base được chi tiết trong C.1. So với các mô hình sách đóng, cũng như các phương pháp RAG và GAG, phương pháp AAG được đề xuất của chúng tôi đạt được hiệu suất state-of-the-art (SOTA) sử dụng số lượng tài liệu tương đương.

Trong cài đặt sách đóng (phần trên của bảng), phương pháp của chúng tôi vượt trội hơn baseline trátâm bằng +2% điểm EM, chứng minh khả năng vượt trội trong việc tận dụng kiến thức nội tại thông qua đánh thức. Đáng chú ý, khi kích thước mô hình tăng, lợi ích hiệu suất từ phương pháp đánh thức trở nên rõ rệt hơn.

Các phần sau trình bày kết quả thực nghiệm trong cài đặt miền mở⁴. Đáng chú ý, AAG được đề xuất chỉ sử dụng một tài liệu giả ngắn, đạt được hoặc vượt quá hiệu suất của các phương pháp RAG và GAG, xử lý 10 tài liệu. Những kết quả này chứng minh rằng AAG cân bằng hiệu quả giữa hiệu suất và overhead bằng cách tận dụng văn bản nén được tưởng tượng.

AAG vượt trội hơn baseline khi số tài liệu khớp. Khi AAG sử dụng 10 tài liệu được truy xuất trong cài đặt RAG, nó vượt trội hơn hiệu suất RFiD 1.6% trong NQ, 4.4% trong TQA, và 2.7% trong WQ. Khi AAG sử dụng 10 tài liệu được tạo ra trong cài đặt GAG, nó vượt trội hơn baseline mạnh GENREAD (clustering) 4.5% trong NQ, 0.7% trong TQA, và 1.1% trong WQ.

⁴Do hạn chế bộ nhớ, AAG trong cài đặt RAG sử dụng 30 tài liệu.

4.4.2 Cài đặt Zero-shot
Hình 4 minh họa kết quả zero-shot cho LLM triển khai AAG với Llama2-7B và -13B đã đóng băng. Nghiên cứu này tìm cách khám phá khả năng nâng cao LLM thông qua AAG. Do nhu cầu tính toán cao của việc huấn luyện, chúng tôi chỉ tinh chỉnh siêu mạng trên bộ dữ liệu hỗn hợp mà không có LCD trong thí nghiệm này và đánh giá hiệu suất trong cài đặt zero-shot. Thông tin prompt chi tiết có thể tìm thấy trong B.2.

Chúng tôi nhận thấy rằng hiệu suất của Llama2 có thể được nâng cao bằng cách tưởng tượng kiến thức một cách tự chủ. Trong khi tận dụng ngữ cảnh tưởng tượng rõ ràng có thể khuếch đại EM trung bình +1%, điều này không đáng kể bằng sự cải thiện đạt được bởi việc truy xuất 10 tài liệu, cho thấy hạn chế của việc chỉ dựa vào gợi ý prompt để kích hoạt kiến thức tương ứng. AAG có thể nâng cao kiến thức thông qua hai quá trình đánh thức chính, gia tăng EM +15.33% cho NQ, +11.97% cho TQA, và +16.38% cho WQ. So với hai phương pháp RAG tiên tiến khác, AAG sử dụng một tài liệu duy nhất có hiệu suất chỉ thấp hơn 1 EM so với phương pháp HICL (Wang et al., 2024) trên TQA nhưng đạt được +10% EM trên NQ và +5% EM trên WQ. Với AAG, Llama2-7B thể hiện sự cải thiện trung bình +14% trên ba bộ dữ liệu. Xu hướng này cũng được quan sát trong kết quả của Llama2-13B (Hình 5). Điều này ngụ ý rằng ngay cả trong cài đặt zero-shot, phương pháp của chúng tôi vẫn có thể mang lại lợi ích đáng kể cho LLM.

4.5 Hiệu suất Ngoài Phân phối (OOD)
Để chứng minh thêm khả năng khái quát hóa của phương pháp AAG và tầm quan trọng của siêu mạng, chúng tôi cũng đánh giá hiệu suất của nó trong khái quát hóa OOD. Bảng 2 cho thấy hiệu suất IID và OOD của FiD, và các phương pháp AAG với các cài đặt tài liệu khác nhau khi huấn luyện trên NQ (Từ khái quát hóa NQ sang hai bộ dữ liệu khác).

Rõ ràng là việc tăng cung cấp tài liệu dẫn đến hiệu suất OOD tốt hơn, có thể do sự hiện diện của nội dung hướng đến câu trả lời trong các tài liệu này. Đáng chú ý, AAG có thể đạt được trong khoảng cách tương đối hẹp 5% so với FiD, ngay cả khi sử dụng một tài liệu tưởng tượng duy nhất so với 10 tài liệu được truy xuất.

Đồng thời, AAG thường thể hiện hiệu suất vượt trội trong OOD khi được cung cấp 10 tài liệu được truy xuất. Sự vượt trội này có thể được truy nguyên về vai trò then chốt của siêu mạng trong việc tạo ra trọng số bộ chuyển đổi LoRA dựa trên câu hỏi. Điều này trang bị cho mô hình khả năng gọi và truy cập kiến thức nội tại dựa trên diễn ngôn cụ thể ngữ cảnh thay vì giới hạn trong việc giải quyết các câu hỏi riêng biệt.

4.6 Chi phí Huấn luyện và Tăng tốc Suy luận
Chúng tôi tiến hành đo lường tốc độ suy luận được ghi lại theo thời gian GPU và thời gian huấn luyện cho 5000 bước trên bộ dữ liệu NQ sử dụng T5-Base. Các thí nghiệm được tiến hành trên một GPU RTX 3090 duy nhất, duy trì kích thước batch tiêu chuẩn là 8 trong huấn luyện và 1 trong suy luận. Một trường hợp suy luận chi tiết được thể hiện trong Phụ lục D.

Như rõ ràng từ Bảng 3, ưu thế của phương pháp đề xuất nằm ở yêu cầu giảm thiểu cho việc cập nhật tham số, có thể được quy cho việc sử dụng siêu mạng chung tạo ra các bộ chuyển đổi LoRA, từ đó loại bỏ nhu cầu thiết lập các bộ chuyển đổi LoRA cá nhân. Mặc dù thiếu ưu thế huấn luyện do hạn chế chưng cất, AAG đạt được lý luận hiệu quả thông qua thiết kế cực kỳ nhẹ, tiết kiệm hơn một nửa thời gian huấn luyện so với các phương pháp sử dụng số lượng lớn tài liệu (0.3×). So với hai phương pháp khác, số token được xử lý giảm đáng kể, trong khi vượt trội hơn chúng hoặc thể hiện sự khác biệt không đáng kể về hiệu suất. Điều này thể hiện sự cân bằng tối ưu giữa hiệu quả và nhu cầu tính toán. Hơn nữa, không như GAG, phương pháp của chúng tôi không phát sinh chi phí tài chính liên quan đến lời gọi API, và kích thước mô hình giảm tạo điều kiện cho việc tạo ra nhanh hơn.

4.7 Nghiên cứu Loại bỏ
Nghiên cứu này giới thiệu hai quá trình đánh thức chính để kích thích kiến thức nội tại của LLM: đánh thức rõ ràng (EA) và đánh thức ngầm (IA). Chúng tôi đặc biệt kiểm tra ảnh hưởng của các loại đánh thức khác nhau đối với hiệu suất.

Bảng 4 chứng minh rằng cả EA và IA đều quan trọng đối với AAG. Bỏ qua một trong hai dẫn đến giảm hiệu suất đáng kể, với sự sụt giảm vượt quá 30% được quan sát khi EA bị bỏ qua. Điều này hài hòa với quan sát ban đầu rằng sự cải thiện hiệu suất trở nên rõ rệt hơn khi có tài liệu liên quan, do đó nhấn mạnh sự vượt trội của EA.

Kết quả của Chưng cất Ngữ cảnh Dài (LCD) bao gồm Ls và Lalign cũng đóng góp nhỏ cho kết quả tổng thể. Điều này xác nhận khẳng định trước đây rằng ngữ cảnh rộng lớn hơn có xu hướng tối ưu hóa hiệu suất, mặc dù với lợi ích hạn chế. Tác động của EA đối với việc áp dụng siêu mạng là tối thiểu (<3%), cho thấy siêu mạng trong IA chủ yếu phục vụ để đánh thức kiến thức tham số thay vì sử dụng ngữ cảnh được tạo ra. Các thí nghiệm và phân tích trên chứng minh tầm quan trọng của từng thành phần và tính hiệu quả của phương pháp AAG của chúng tôi.

--- TRANG 9 ---
5 Kết luận
Nghiên cứu này đề xuất một chiến lược tăng cường kiến thức mới cho Mô hình Ngôn ngữ Lớn (LLM), cụ thể là Tăng cường Sinh thành bằng Đánh thức (AAG) cho trả lời câu hỏi miền mở. AAG khai thác hiệu quả kiến thức vốn có của LLM thông qua phương pháp đánh thức kép để đánh thức ngữ cảnh phong phú hơn. Đánh thức rõ ràng với bộ tạo ngữ cảnh tạo ra một tài liệu giả ngắn như ngữ cảnh biểu tượng, trong khi đánh thức ngầm sử dụng siêu mạng để chuyển đổi câu hỏi và tài liệu thành các bộ chuyển đổi được chèn vào LLM như ngữ cảnh tham số. Kết quả thực nghiệm chứng minh sự cải thiện hiệu suất đáng kể đồng thời vẫn tương đối nhẹ. Mặc dù trọng tâm chính của phương pháp này là một nhiệm vụ cụ thể, chúng tôi tin rằng những phát hiện này có thể mang lại góc nhìn mới về cách khai thác tốt hơn tiềm năng của LLM.

Hạn chế
Trong khi nghiên cứu này đã chứng minh những thành tích đáng kể trong các nhiệm vụ QA, có những hạn chế đáng chú ý:

Nhiệm vụ. Các phương pháp được đề xuất trong nghiên cứu được chuyên biệt cụ thể cho QA. Vẫn chưa biết chúng sẽ hiệu quả như thế nào trong các loại nhiệm vụ đòi hỏi kiến thức chuyên sâu khác, như kiểm tra sự thật hoặc hệ thống đối thoại. Cần xác nhận thêm để đánh giá khả năng khái quát hóa và ứng dụng của phương pháp này.

Đa phương thức. Chúng tôi chỉ xem xét văn bản tưởng tượng và biểu diễn ẩn. Trong công việc tương lai, việc khám phá thông tin đa phương thức bao gồm tác động của việc tưởng tượng hình ảnh đối với hiệu suất là điều bắt buộc.

Phương pháp. Phương pháp của chúng tôi dựa vào kiến thức được học bởi LLM trong giai đoạn pre-training, có thể hạn chế khả năng của mô hình thích ứng nhanh chóng với thông tin mới. Sự phụ thuộc vào kích hoạt kiến thức nội tại trong AAG có thể dẫn đến quá trình ra quyết định ít minh bạch hơn trong mô hình, khiến việc giải thích logic đằng sau các câu trả lời được tạo ra trở nên thách thức. Trong tương lai, cần tiếp tục khám phá các phương pháp tăng cường kiến thức thích ứng để tối ưu hóa kết quả hơn nữa.

Siêu mạng. Đối với cài đặt nhẹ và hiệu quả, siêu mạng của chúng tôi sử dụng MLP hai lớp. Tuy nhiên, một số nghiên cứu sử dụng các mô hình lớn hơn, như GPT-2 hoặc T5, làm siêu mạng. Do hạn chế tài nguyên tính toán, chúng tôi không khám phá hoặc so sánh tác động của các mô hình siêu mạng khác nhau đối với kết quả. Tuy nhiên, phương pháp của chúng tôi chủ yếu tập trung vào việc tạo ra các mô-đun hiệu quả tham số để nâng cao kích hoạt kiến thức và khái quát hóa.

Cân nhắc Đạo đức
Trong bài báo này, chúng tôi đề xuất một phương pháp tăng cường kiến thức mới nhằm tận dụng kiến thức của LLM. Tuy nhiên, LLM có thể tạo ra kiến thức không phù hợp hoặc phân biệt đối xử. Phương pháp của chúng tôi không gây ra mối quan tâm đạo đức. Các bộ dữ liệu chúng tôi sử dụng là công khai, và không có vấn đề riêng tư.

Lời cảm ơn
Công việc này được hỗ trợ bởi Chương trình R&D Trọng điểm Quốc gia của Trung Quốc (Số 2022YFF0711900) và Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62376270, Số 62276264). Công việc này được hỗ trợ bởi Hiệp hội Thúc đẩy Đổi mới Thanh niên CAS.

Tài liệu tham khảo
[Các tài liệu tham khảo được duy trì nguyên văn tiếng Anh do tính chất học thuật]

--- TRANG 10-20 ---
[Phần còn lại của tài liệu bao gồm các phụ lục A, B, C, D với các bảng kết quả thí nghiệm chi tiết, thiết lập thực nghiệm, và nghiên cứu trường hợp. Do độ dài, tôi sẽ dịch các phần quan trọng nhất:]

Phụ lục A - Phương pháp
A.1 So sánh Ba Mô hình
So với RAG và GAG, phương pháp của chúng tôi có những hạn chế nhất định, như yêu cầu quá trình huấn luyện phức tạp hơn và sự cần thiết phải huấn luyện một mô hình...

[Tiếp tục với các phần phụ lục khác mô tả chi tiết về thiết lập thực nghiệm, kết quả đầy đủ, và các nghiên cứu trường hợp]
