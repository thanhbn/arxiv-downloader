# 2307.12798.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2307.12798.pdf
# Kích thước tệp: 282727 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
RRAML: Học Máy Tăng Cường Truy Xuất với Tăng Cường
(Reinforced Retrieval Augmented Machine Learning)

Andrea Bacciu1, Florin Cuconasu1, Federico Siciliano1, Fabrizio Silvestri1,
Nicola Tonellotto2, và Giovanni Trappolini1
{surname }@diag.uniroma1.it1, nicola.tonellotto@unipi.it2
1Đại học Sapienza Rome
2Đại học Pisa

Tóm tắt. Sự xuất hiện của các mô hình ngôn ngữ lớn (LLMs) đã cách mạng hóa học máy và các lĩnh vực liên quan, thể hiện khả năng đáng chú ý trong việc hiểu, tạo sinh và thao tác ngôn ngữ con người. Tuy nhiên, việc sử dụng thông thường thông qua các lời nhắc văn bản dựa trên API áp đặt một số hạn chế về ràng buộc ngữ cảnh và tính khả dụng của nguồn bên ngoài. LLMs gặp phải vấn đề ảo giác văn bản, và trong năm qua, một số phương pháp đã được đưa ra để khắc phục vấn đề này: thêm một Cơ sở Tri thức bên ngoài hoặc một bộ nhớ bên ngoài bao gồm các embedding được lưu trữ và truy xuất bởi cơ sở dữ liệu vector. Tuy nhiên, trong tất cả các phương pháp hiện tại, các vấn đề chính là: (i) chúng cần truy cập vào mô hình embedding và sau đó điều chỉnh nó cho nhiệm vụ mà chúng phải giải quyết; (ii) trong trường hợp chúng phải tối ưu hóa mô hình embedding, chúng cần có quyền truy cập vào các tham số của LLM, mà trong nhiều trường hợp là "hộp đen". Để giải quyết những thách thức này, chúng tôi đề xuất một khung công việc mới gọi là Học Máy Tăng Cường Truy Xuất với Tăng Cường (RRAML). RRAML tích hợp khả năng suy luận của LLMs với thông tin hỗ trợ được truy xuất bởi một bộ truy xuất chuyên dụng từ một cơ sở dữ liệu rộng lớn do người dùng cung cấp. Bằng cách tận dụng những tiến bộ gần đây trong học tăng cường, phương pháp của chúng tôi giải quyết hiệu quả một số thách thức quan trọng. Thứ nhất, nó tránh được nhu cầu truy cập gradient của LLM. Thứ hai, phương pháp của chúng tôi giảm bớt gánh nặng huấn luyện lại LLMs cho các nhiệm vụ cụ thể, vì điều này thường không thực tế hoặc không thể thực hiện được do hạn chế truy cập vào mô hình và cường độ tính toán liên quan. Ngoài ra, chúng tôi liên kết liền mạch nhiệm vụ của bộ truy xuất với bộ suy luận, giảm thiểu ảo giác và giảm các tài liệu được truy xuất không liên quan và có thể gây hại. Chúng tôi tin rằng chương trình nghiên cứu được nêu trong bài báo này có tiềm năng tác động sâu sắc đến lĩnh vực AI, dân chủ hóa quyền truy cập và sử dụng LLMs cho một loạt các thực thể.

Từ khóa: Học Sâu • Truy Xuất Thông Tin • Mô Hình Ngôn Ngữ Lớn.

1 Giới thiệu

Sự ra đời của Mô hình Ngôn ngữ Lớn (LLMs) đã mang lại sự thay đổi mô hình trong học máy và các ngành liên quan. LLMs [2,20,13,24,1] đã thể hiện khả năng chưa từng có trong việc hiểu, tạo sinh và thao tác ngôn ngữ con người. Nổi tiếng, ChatGPT [13] đã bước vào không gian công cộng bằng cách đạt được một triệu người dùng chỉ trong vài ngày. Cách thức những mô hình này thường được sử dụng là thông qua API chỉ cho phép gửi một lời nhắc văn bản và nhận lại từ máy chủ văn bản được tạo sinh. Tuy nhiên, điều này gây ra một hạn chế ngay lập tức: tất cả thông tin phải được truyền qua ngữ cảnh này, và chúng ta biết các mô hình dựa trên transformer không mở rộng một cách tốt đẹp. Ngay cả khi chúng có thể, chi phí API được tính dựa trên việc sử dụng. Do đó, việc sử dụng ngữ cảnh dài sẽ tốn kém. Ngay cả khi ai đó có tài nguyên để chạy LLM của riêng họ, chi phí huấn luyện và cơ sở hạ tầng phần cứng, và tác động môi trường cần được xem xét. Tuy nhiên, có một nhu cầu cấp thiết để điều chỉnh sức mạnh to lớn của những mô hình đó theo nhu cầu cụ thể của người dùng bằng cách đảm bảo rằng họ có thể sử dụng khả năng suy luận của LLMs, thông qua học trong ngữ cảnh [2] trên dữ liệu của họ.

arXiv:2307.12798v3 [cs.CL] 27 Jul 2023

--- TRANG 2 ---
2 Bacciu, Cuconasu, Siciliano, Silvestri, Tonellotto, Trappolini, 2023

Một giải pháp là áp dụng phương pháp tăng cường truy xuất [8,26]. Trong bối cảnh này, một bộ truy xuất được sử dụng để lọc ra thông tin liên quan để truyền làm ngữ cảnh cho bộ suy luận. Tuy nhiên, điều này tạo ra một vấn đề mới, cụ thể là bộ truy xuất và bộ suy luận không được căn chỉnh [25,22,23]. Đặc biệt, bộ truy xuất có thể không được huấn luyện trên nhiệm vụ mà người dùng quan tâm. Hơn nữa, bộ truy xuất thực sự có thể cung cấp những phần thông tin "nguy hiểm" cho bộ suy luận, như đã được chứng minh trong [19], dẫn đến kết quả kém và quan trọng hơn là dẫn đến ảo giác.

Lý tưởng nhất, người ta sẽ phải tinh chỉnh những mô hình này để giải quyết các vấn đề này. Trong bối cảnh này, việc tinh chỉnh mô hình cho một nhiệm vụ nhất định về mặt kỹ thuật là không thể. Chúng tôi tự hỏi: "Liệu có thể sử dụng API bảo vệ những LLMs mạnh mẽ đó trên dữ liệu của chúng ta mà không cần tinh chỉnh?" Chúng tôi chỉ ra rằng câu hỏi này có câu trả lời tích cực và trong bài báo này, chúng tôi đề xuất một khung công việc mới, Học Máy Tăng Cường Truy Xuất với Tăng Cường (RRAML), trong đó chúng tôi kết hợp khả năng suy luận của các mô hình nền tảng lớn được tăng cường bởi việc cung cấp thông tin liên quan hỗ trợ được cung cấp bởi một bộ truy xuất tìm kiếm chúng trong một cơ sở dữ liệu lớn. Trong bối cảnh này, một mô hình truy xuất hiệu quả được giao nhiệm vụ tìm kiếm thông tin liên quan trong một cơ sở dữ liệu dữ liệu tùy ý lớn do người dùng cung cấp. Khi tập hợp dữ liệu liên quan này đã được truy xuất, nó được chuyển tiếp đến bộ suy luận (một mô hình nền tảng lớn như ChatGPT, chẳng hạn) thông qua API của nó để "suy luận" trên đầu vào và tạo ra kết quả phù hợp. Đặc biệt, chúng tôi dự định vượt qua các hạn chế hiện tại, cụ thể là nhiệm vụ của bộ truy xuất bị tách rời khỏi nhiệm vụ của bộ suy luận, do đó giảm xu hướng ảo giác của LLM và giảm số lượng tài liệu gây hại (như được định nghĩa trong [3,18,25]) được trả về bởi bộ truy xuất. Phương pháp mà chúng tôi nghĩ ra trong công việc nghiên cứu này khai thác những tiến bộ gần đây trong học tăng cường. Gần đây, thực tế, các kỹ thuật học tăng cường như PPO [21] đã được sử dụng để cải thiện các mô hình nền tảng lớn với phản hồi của con người nơi mà hàm mất mát không khả vi. Chúng tôi đề xuất liên kết giai đoạn huấn luyện của bộ truy xuất với kết quả nhiệm vụ cuối cùng bằng cách sử dụng một mô hình phần thưởng được chế tạo có mục đích phụ thuộc vào phản hồi của con người hoặc vào các đặc điểm cụ thể của dữ liệu nhiệm vụ. Kỹ thuật RL cũng mang lại lợi thế không yêu cầu tinh chỉnh

--- TRANG 3 ---
RRAML: Học Máy Tăng Cường Truy Xuất với Tăng Cường 3

một LLM làm bộ suy luận, có thể được coi là một hộp đen trong bối cảnh này, và có thể được thay thế tự do.

Cuối cùng, chúng tôi lập luận rằng chương trình nghiên cứu mà chúng tôi đặt ra trong bài báo này có tiềm năng tác động lớn đến lĩnh vực AI và dân chủ hóa quyền truy cập và sử dụng các mô hình nền tảng lớn này đối với một tập hợp lớn các thực thể.

2 Phương pháp

Hệ thống nhận đầu vào là mô tả nhiệm vụ, một truy vấn và một cơ sở dữ liệu và đưa ra đầu ra là phản hồi được tạo bởi một bộ suy luận. Kiến trúc hệ thống tổng thể, được hiển thị trong Hình 1, bao gồm ba thành phần chính: một Mô hình Ngôn ngữ Tạo sinh, một Bộ Truy xuất và một Bộ Suy luận (thường là một LLM).

[Cơ sở dữ liệu] [Truy vấn] [Mô tả Nhiệm vụ]
Mô hình Tạo sinh
Bộ Truy xuất
Lời nhắc Đầu vào
Tập Hỗ trợ
Bộ Suy luận (LLM)
Đầu ra
Đầu ra Mong đợi
Học Tăng cường
Phần thưởng / Mất mát
Học Tăng cường
Phản hồi Con người
Không muốn / không thể tinh chỉnh
Đóng băng
Được huấn luyện

Hình 1. Thiết kế cấp cao của khung công việc RRAML. Bên trái, có ba đầu vào: Mô tả Nhiệm vụ, truy vấn của người dùng, và một cơ sở dữ liệu đại diện cho tri thức bên ngoài được sử dụng để tăng cường/cập nhật bộ suy luận. Sau đó, chúng tôi trình bày luồng kiến trúc tổng thể với Bộ Truy xuất, Mô hình Ngôn ngữ Tạo sinh và Bộ Suy luận. Cuối cùng, cách phần thưởng được tính toán và lan truyền trong Mô hình Ngôn ngữ Tạo sinh và Bộ Truy xuất.

Chi tiết hơn, Mô hình Ngôn ngữ Tạo sinh nhận mô tả nhiệm vụ và truy vấn làm đầu vào và tạo ra một lời nhắc. Bộ Truy xuất nhận truy vấn và cơ sở dữ liệu làm đầu vào và đưa ra một tập hỗ trợ, sau đó được nối với truy vấn và truyền đến Bộ Suy luận.

2.1 Dữ liệu

Dữ liệu là một thành phần quan trọng của khung công việc: mô tả nhiệm vụ hướng dẫn việc tạo ra một lời nhắc phù hợp, truy vấn đại diện cho yêu cầu của người dùng, và cơ sở dữ liệu cung cấp dữ liệu cần thiết cho bộ suy luận để thực hiện nhiệm vụ.

--- TRANG 4 ---
4 Bacciu, Cuconasu, Siciliano, Silvestri, Tonellotto, Trappolini, 2023

Mô tả Nhiệm vụ Mô tả nhiệm vụ là một chuỗi định nghĩa bản chất của nhiệm vụ, có thể với các kết quả mong đợi, mà người dùng muốn thực hiện. Ví dụ, nếu người dùng muốn tạo một bản tóm tắt của nhiều bài báo tin tức, một mô tả nhiệm vụ có thể là "Tóm tắt Tin tức". Nếu người dùng muốn thực hiện trả lời câu hỏi trên một bộ sưu tập tài liệu rộng lớn, mô tả nhiệm vụ có thể là "Trả lời Câu hỏi".

Truy vấn Truy vấn đại diện cho nhu cầu của người dùng. Bộ Truy xuất sẽ hoạt động trên cơ sở dữ liệu theo truy vấn của người dùng, và dữ liệu kết quả là đầu vào cho nhiệm vụ. Ví dụ, nếu người dùng muốn tóm tắt một bộ sưu tập bài báo tin tức, truy vấn có thể là chủ đề mà người dùng quan tâm. Nếu người dùng muốn trả lời một truy vấn cụ thể, điều này trở thành câu hỏi thực tế.

Cơ sở Dữ liệu Cơ sở dữ liệu là một bộ sưu tập dữ liệu công cộng hoặc riêng tư (hoặc tài liệu) có thể được truy vấn để cung cấp thông tin liên quan nhằm thỏa mãn nhu cầu thông tin của người dùng. Cơ sở dữ liệu đại diện cho tri thức cần thiết cho Bộ Suy luận để thực hiện nhiệm vụ. Dữ liệu được lưu trữ trong cơ sở dữ liệu sẽ phụ thuộc vào nhiệm vụ cụ thể và có thể bao gồm văn bản, hình ảnh, âm thanh và các loại dữ liệu khác (như trong [25]). Ví dụ, nếu người dùng muốn tóm tắt nhiều bài báo tin tức, cơ sở dữ liệu có thể là một bộ sưu tập bài báo được lập chỉ mục. Nếu người dùng muốn thực hiện Trả lời Câu hỏi, cơ sở dữ liệu có thể bao gồm các sự kiện liên quan đến một chủ đề cụ thể (như trong [22,23]).

2.2 Mô hình

Mô hình Ngôn ngữ Tạo sinh Thành phần Mô hình Ngôn ngữ Tạo sinh của khung công việc chịu trách nhiệm tạo ra các hướng dẫn văn bản dựa trên Mô tả Nhiệm vụ và Truy vấn đầu vào nhằm tối đa hóa phần thưởng đối với Bộ Suy luận. Cụ thể, nó nhận một chuỗi đại diện cho nhiệm vụ cần thực hiện (Mô tả Nhiệm vụ) và một truy vấn (Truy vấn) đại diện cho yêu cầu của người dùng. Mô hình Ngôn ngữ Tạo sinh sau đó tạo ra một lời nhắc văn bản liên quan đến truy vấn và nhiệm vụ bằng cách thực hiện kỹ thuật lời nhắc tự động.

Bộ Truy xuất Thành phần Bộ Truy xuất của khung công việc chịu trách nhiệm truy xuất dữ liệu liên quan từ Cơ sở Dữ liệu dựa trên truy vấn của người dùng. Chúng tôi gọi đầu ra của Bộ Truy xuất là tập hỗ trợ (như trong [22,23]). Một tập hỗ trợ là một tập con của dữ liệu từ Cơ sở Dữ liệu mà hoặc trực tiếp trả lời truy vấn đã cho hoặc đóng góp vào câu trả lời cuối cùng.

Bộ Tổng hợp Lời nhắc Thành phần này chịu trách nhiệm xử lý đầu vào cần thiết cho Bộ Suy luận. Ở dạng đơn giản nhất, nó chỉ cần nối lời nhắc được tạo bởi Mô hình Ngôn ngữ Tạo sinh với Tập Hỗ trợ được cung cấp bởi Bộ Truy xuất. Tuy nhiên, trong phiên bản phức tạp hơn, nó có thể cần phải chỉnh sửa lại lời nhắc dựa trên số lượng tập hỗ trợ nhận được để đảm bảo rằng LLM có thể cung cấp phản hồi mạch lạc. Ví dụ, nếu Bộ Truy xuất cung cấp hai tập hỗ trợ, Bộ Tổng hợp Lời nhắc có thể cần chia lời nhắc thành hai phần và nối mỗi phần với một trong các tập hỗ trợ.

--- TRANG 5 ---
RRAML: Học Máy Tăng Cường Truy Xuất với Tăng Cường 5

Bộ Suy luận Bộ Suy luận chịu trách nhiệm tạo ra câu trả lời cho truy vấn của người dùng dựa trên lời nhắc cuối cùng được tạo bởi Bộ Tổng hợp Lời nhắc. Bộ Suy luận có thể là một mô hình được huấn luyện trước như GPT hoặc một mô hình được huấn luyện tùy chỉnh cụ thể cho nhiệm vụ hiện tại. Đầu ra của LLM là một phản hồi văn bản, có thể được phân tích thêm để tuân thủ đầu ra dự định.

2.3 Học Tăng cường

Phần Học Tăng cường (RL) của khung công việc chịu trách nhiệm tinh chỉnh Mô hình Ngôn ngữ Tạo sinh (GLM) và Bộ Truy xuất dựa trên phần thưởng được tính toán. RL là một phần quan trọng của RRAML, nó sẽ được sử dụng để liên tục cải thiện GLM và Bộ Truy xuất. Như đã đề cập trước đó, bộ truy xuất sẽ bị phạt nếu một số khuyến nghị của nó dẫn đến việc Bộ Suy luận ảo giác, ví dụ bằng cách thêm các tài liệu gây hại. RL cho phép chúng ta tích hợp và tăng cường các tín hiệu trong việc huấn luyện các mô hình này, vượt ra ngoài dữ liệu có trong tập huấn luyện của chúng, đảm bảo rằng chúng được căn chỉnh với môi trường (tức là, bộ suy luận và nhiệm vụ cuối cùng).

Phần thưởng Hàm phần thưởng có thể được định nghĩa dựa trên sự tương đồng giữa đầu ra được tạo và đầu ra mong đợi và nó có thể được ước tính bằng cách huấn luyện một Mô hình Phần thưởng [21].

Thuật toán RL Phương pháp RL cụ thể có thể được sử dụng là Deep Q-Networks (DQN) [11], đây là một thuật toán RL không có mô hình học để tối đa hóa phần thưởng tích lũy theo thời gian. DQN kết hợp Q-Learning, đây là một thuật toán RL học hàm giá trị hành động tối ưu, với một Mạng Nơ-ron Sâu để xấp xỉ hàm giá trị hành động. Trong khung công việc được đề xuất, DQN được sử dụng để huấn luyện Mô hình Ngôn ngữ Tạo sinh và Bộ Truy xuất để tối đa hóa phần thưởng thu được từ phản hồi của người dùng. Quá trình cập nhật được thực hiện bằng cách lan truyền ngược tín hiệu phần thưởng qua các mạng nơ-ron bằng Gradient Descent Ngẫu nhiên (SGD). Trọng số của các mạng nơ-ron được cập nhật theo hướng tối đa hóa phần thưởng mong đợi, sử dụng quy tắc cập nhật Q-Learning. Việc cập nhật được thực hiện lặp đi lặp lại cho đến khi hội tụ, đạt được khi phần thưởng mong đợi ngừng cải thiện.

Con người trong vòng lặp Sở thích của con người có thể được tích hợp vào hệ thống ML của chúng ta bằng cách cho phép người dùng cung cấp phản hồi về đầu ra của hệ thống. Phản hồi này sẽ được sử dụng để tính toán phần thưởng cho thuật toán RL và sẽ giúp cải thiện hiệu suất của hệ thống tổng thể theo thời gian. Chúng tôi thừa nhận rằng một số nhiệm vụ có thể không có đầu ra mong đợi rõ ràng hoặc có thể yêu cầu ngữ cảnh bổ sung không có sẵn trong dữ liệu đầu vào. Trong những trường hợp này, chúng tôi sẽ tận dụng các phương pháp tiếp cận con người trong vòng lặp để cung cấp ngữ cảnh và hướng dẫn bổ sung cho hệ thống. Ví dụ, các nền tảng cộng đồng hoặc các chuyên gia chủ đề nội bộ có thể được sử dụng để cung cấp phản hồi về đầu ra của hệ thống và giúp huấn luyện mô hình trên các nhiệm vụ phức tạp hơn.

--- TRANG 6 ---
6 Bacciu, Cuconasu, Siciliano, Silvestri, Tonellotto, Trappolini, 2023

3 Ví dụ Trường hợp Sử dụng

RRAML chứng minh hiệu quả trong nhiều ứng dụng. Xem xét một tình huống mà một công ty sở hữu một cơ sở dữ liệu riêng, bao gồm thông tin thực tế được thể hiện bằng ngôn ngữ tự nhiên, và họ cần áp dụng suy luận cho dữ liệu này. Khối lượng dữ liệu của họ có thể vượt quá khả năng ngữ cảnh của LLM, và tinh chỉnh không phải là một lựa chọn, vì giá cả/tác động môi trường hoặc vì LLM được phục vụ bởi các API của công ty khác. Để giải quyết thách thức này, RRAML sử dụng bộ truy xuất của mình để chỉ lấy các sự kiện liên quan trong ngữ cảnh, cho phép LLM suy luận về chúng.

Ví dụ, giả sử một công ty có danh sách nhân viên, các dự án mà nhân viên hiện đang hoặc đã được phân công trước đó, và lưới đánh giá hiệu suất với phản hồi dựa trên văn bản từ cấp trên. Công ty có thể muốn phân công nhân viên cho một dự án mới về một chủ đề cụ thể. Để làm như vậy, cần phải nhập thông tin chứa trong những dữ liệu này vào LLM. Tuy nhiên, do hạn chế về khả năng, toàn bộ dữ liệu không thể vừa với ngữ cảnh. Do đó, bộ truy xuất phải trả về một tập con của thông tin này, có thể loại trừ dữ liệu về các dự án từ quá khứ xa, nhân viên đã quá tải với nhiều dự án, hoặc nhân viên chưa bao giờ làm việc trên một dự án liên quan đến cùng chủ đề.

4 Công trình Liên quan

Những năm gần đây đã chứng kiến sự xuất hiện của các mô hình ngôn ngữ lớn. Bắt đầu từ Mô hình Huấn luyện/Tạo sinh đầu tiên, được biết đến nhiều hơn với tên GPT [17], những loại mô hình ngôn ngữ lớn này đã cải thiện nhanh chóng. GPT-4 [14] là phiên bản mới nhất, nhưng trong khi đó, nhiều người đã vội vã đề xuất phiên bản riêng của họ. Google gần đây đã phát hành BARD3, trong khi Meta đã đề xuất quan điểm riêng của họ về LLM với LLaMA [24]. Cộng đồng nghiên cứu cũng đã tận dụng nỗ lực của mình bằng cách phát hành một số LLM mã nguồn mở với các kích thước khác nhau, như Bloom [20], Dolly4, và RWKV [16]. Tuy nhiên, tất cả những mô hình này đều không thể mở rộng đến kích thước ngữ cảnh lớn hơn, hoặc do chi phí tính toán quá mức hoặc do "mất nó ở giữa", như được hiển thị trong [9].

Để giải quyết hạn chế về độ dài ngữ cảnh này, một số người đã cố gắng tích hợp tri thức bên ngoài vào LLMs [6,4,15]. Đặc biệt, trong "Học máy tăng cường truy xuất" [27], các tác giả đã hình dung ra một khung công việc trong đó các hệ thống truy xuất có thể tăng cường hiệu suất của một mô hình học máy. Gần đây hơn, đã có những nỗ lực huấn luyện chung các mô hình truy xuất với LLMs [8,28], đáng chú ý là dòng nghiên cứu về cơ sở dữ liệu nơ-ron, trong đó các tác giả đã cố gắng thay thế một cơ sở dữ liệu truyền thống bằng một khung nơ-ron loại bỏ nhu cầu về một lược đồ [23,22,25]. Tuy nhiên, tất cả những công trình này đều giả định quyền truy cập đầy đủ vào mô-đun suy luận, điều này không phải là trường hợp đối với hầu hết người dùng trong thực tế.

Để vượt qua hạn chế này, nhiều người đã cố gắng chế tạo các hệ thống có thể

3https://bard.google.com/
4https://github.com/databrickslabs/dolly

--- TRANG 7 ---
RRAML: Học Máy Tăng Cường Truy Xuất với Tăng Cường 7

cung cấp một lời nhắc được tối ưu hóa làm đầu vào cho LLM. Ví dụ, nghiên cứu được thực hiện bởi [10] đã chứng minh ảnh hưởng đáng kể của trình tự mà các lời nhắc được trình bày đến hiệu suất cuối cùng của nhiệm vụ. Trong khi đó, một nghiên cứu của Nie et al. [12] nổi bật rằng hiệu suất nhạy cảm với sự sắp xếp của các ví dụ trong lời nhắc, các mẫu lời nhắc, và các trường hợp trong ngữ cảnh trong lời nhắc. Lester et al. [7] đề xuất một phương pháp để tăng cường hiệu suất nhiệm vụ bằng cách thêm các token có thể điều chỉnh trong quá trình tinh chỉnh. LLM-AUGMENTER lặp đi lặp lại sửa đổi [15] để cải thiện phản hồi của mô hình.

Tất cả các công trình được giới thiệu ở trên không cải thiện bộ truy xuất, được giả định là cố định. Trong công trình của chúng tôi, chúng tôi đề xuất tinh chỉnh bộ truy xuất kết hợp với bộ suy luận để cải thiện kết quả. Vì phản hồi không khả vi, chúng tôi sử dụng học tăng cường. Đặc biệt, công thức gần đây như Proximal Policy Optimization (PPO) [5] sử dụng một mô-đun phần thưởng nơ-ron khả vi để bao gồm và giải quyết phản hồi thường không khả vi, như trong trường hợp học tăng cường với phản hồi của con người (RLHF).

5 Kết luận

Tóm lại, RRAML cung cấp một khung công việc đầy hứa hẹn để xây dựng các giao diện thông minh tương tác với các mô hình ngôn ngữ lớn như GPT. Bằng cách kết hợp một mô hình ngôn ngữ tạo sinh với một bộ truy xuất, phương pháp này có thể cải thiện hiệu quả hiệu suất của các mô hình ngôn ngữ và giúp chúng hiểu ý định của người dùng tốt hơn.

Tuy nhiên, phương pháp này cũng đi kèm với một số thách thức và không chắc chắn, chẳng hạn như nhu cầu về một lượng lớn dữ liệu huấn luyện, khả năng thiên vị trong dữ liệu và mô hình, và khó khăn trong việc cân bằng sự đánh đổi giữa các phương pháp tạo sinh và dựa trên truy xuất.

Bất chấp những thách thức này, RRAML mang lại hứa hẹn lớn để tạo ra các giao diện thông minh, tự nhiên và hiệu quả hơn để tương tác với các mô hình ngôn ngữ. Chúng tôi hy vọng rằng bài báo này đã cung cấp một cái nhìn tổng quan hữu ích về phương pháp này và các ứng dụng tiềm năng của nó, và chúng tôi mong chờ những nghiên cứu và phát triển hơn nữa trong lĩnh vực thú vị này.

--- TRANG 8 ---
8 Bacciu, Cuconasu, Siciliano, Silvestri, Tonellotto, Trappolini, 2023

Tài liệu tham khảo

1. Bacciu, A., Trappolini, G., Santilli, A., Rodolà, E., Silvestri, F.: Fauno: The italian large language model that will leave you senza parole! arXiv preprint arXiv:2306.14457 (2023)

2. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877–1901 (2020)

3. Carmel, D., Cohen, N., Ingber, A., Kravi, E.: Ir evaluation and learning in the presence of forbidden documents. In: Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 556–566 (2022)

4. Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., Weston, J.: Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241 (2018)

5. Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L., Madry, A.: Implementation matters in deep policy gradients: A case study on ppo and trpo. arXiv preprint arXiv:2005.12729 (2020)

6. Ghazvininejad, M., Brockett, C., Chang, M.W., Dolan, B., Gao, J., Yih, W.t., Galley, M.: A knowledge-grounded neural conversation model. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 32 (2018)

7. Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021)

8. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., et al.: Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33, 9459–9474 (2020)

9. Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., Liang, P.: Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172 (2023)

10. Lu, Y., Bartolo, M., Moore, A., Riedel, S., Stenetorp, P.: Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786 (2021)

11. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level control through deep reinforcement learning. nature 518(7540), 529–533 (2015)

12. Nie, F., Chen, M., Zhang, Z., Cheng, X.: Improving few-shot performance of language models via nearest neighbor calibration. arXiv preprint arXiv:2212.02216 (2022)

13. OpenAI: Chatgpt: A large-scale language model for conversational ai. OpenAI Blog (November 2022)

14. OpenAI: Gpt-4 technical report (2023)

15. Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., et al.: Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813 (2023)

16. PENG, B.: RWKV-LM (Aug 2021). https://doi.org/10.5281/zenodo.5196577, https://github.com/BlinkDL/RWKV-LM

17. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language understanding by generative pre-training. OpenAI Blog (2018)

--- TRANG 9 ---
RRAML: Học Máy Tăng Cường Truy Xuất với Tăng Cường 9

18. Sauchuk, A., Thorne, J., Halevy, A., Tonellotto, N., Silvestri, F.: On the role of relevance in natural language processing tasks. In: Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 1785–1789 (2022)

19. Sauchuk, A., Thorne, J., Halevy, A.Y., Tonellotto, N., Silvestri, F.: On the role of relevance in natural language processing tasks. In: Amigó, E., Castells, P., Gonzalo, J., Carterette, B., Culpepper, J.S., Kazai, G. (eds.) SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. pp. 1785–1789. ACM (2022). https://doi.org/10.1145/3477495.3532034, https://doi.org/10.1145/3477495.3532034

20. Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A.S., Yvon, F., Gallé, M., et al.: Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022)

21. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017)

22. Thorne, J., Yazdani, M., Saeidi, M., Silvestri, F., Riedel, S., Halevy, A.: Database reasoning over text. arXiv preprint arXiv:2106.01074 (2021)

23. Thorne, J., Yazdani, M., Saeidi, M., Silvestri, F., Riedel, S., Halevy, A.: From natural language processing to neural databases. In: Proceedings of the VLDB Endowment. vol. 14, pp. 1033–1039. VLDB Endowment (2021)

24. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: Llama: Open and efficient foundation language models (2023)

25. Trappolini, G., Santilli, A., Rodolà, E., Halevy, A., Silvestri, F.: Multimodal neural databases. arXiv preprint arXiv:2305.01447 (2023)

26. Xie, Z., Singh, S., McAuley, J., Majumder, B.P.: Factual and informative review generation for explainable recommendation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13816–13824 (2023)

27. Zamani, H., Diaz, F., Dehghani, M., Metzler, D., Bendersky, M.: Retrieval-enhanced machine learning. In: Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 2875–2886 (2022)

28. Zhang, Y., Sun, S., Gao, X., Fang, Y., Brockett, C., Galley, M., Gao, J., Dolan, B.: Retgen: A joint framework for retrieval and grounded text generation modeling. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 11739–11747 (2022)
