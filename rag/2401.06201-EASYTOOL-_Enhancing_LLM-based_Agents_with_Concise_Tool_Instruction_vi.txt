(Touvron et al., 2023a) trên ToolBench. Chúng tôi không áp dụng ToolLLaMA-7B trên EASYTOOL do khả năng tuân theo hướng dẫn kém của nó. Hơn nữa, chúng tôi cũng áp dụng Mistral-Instruct-7B (Jiang et al., 2023a) để so sánh, được tinh chỉnh từ Mistral-7B và thể hiện khả năng tuân theo hướng dẫn tuyệt vời. Đối với các baselines, chúng tôi tuân theo các thiết lập trong Qin et al. (2023), cung cấp tài liệu công cụ cho chúng để sử dụng công cụ.

Kết quả Chính Chúng tôi đơn giản hóa tài liệu công cụ từ ToolBench thành hướng dẫn công cụ ngắn gọn với EASYTOOL. Mỗi hướng dẫn công cụ bao gồm một mô tả công cụ và hướng dẫn chức năng. Như được hiển thị trong Bảng 3, với EASYTOOL, việc thay thế tài liệu công cụ bằng hướng dẫn công cụ của chúng tôi có thể giảm đáng kể chi phí token của mỗi công cụ. Đặc biệt trong ToolBench, chi phí token đã được giảm 70,43%.

Hơn nữa, kết quả trong Bảng 4 cho thấy rằng:
1) Với hướng dẫn công cụ được tạo bởi EASYTOOL, LLM có thể đạt được hiệu suất tốt nhất. Đáng chú ý, ChatGPT + DFSDT-EASYTOOL thậm chí còn vượt qua GPT-4 + DFSDT về tỷ lệ thành công, cho thấy sự vượt trội của hướng dẫn công cụ so với tài liệu công cụ trong việc hỗ trợ sử dụng công cụ cho LLM; 2) Vicuna và Mistral-Instruct-7B dẫn đến thất bại khi trực tiếp sử dụng công cụ. Dựa trên kinh nghiệm trước đây (Shen et al., 2023b), chúng tôi quy hiện tượng này cho việc thiếu huấn luyện trong dữ liệu có định dạng (ví dụ: gọi hàm). Tuy nhiên, hướng dẫn công cụ được tạo bởi EASYTOOL có thể giúp những mô hình này hiểu rõ hơn việc sử dụng công cụ, thậm chí khiến chúng vượt trội hơn phương pháp tinh chỉnh, tức là ToolLLaMA; 3) Mistral-Instruct-7B vượt trội hơn Vicuna-7B với EASYTOOL, cho thấy rằng các mô hình có khả năng tuân theo hướng dẫn tốt hơn có thể đạt được cải thiện lớn hơn với hướng dẫn công cụ chất lượng cao.

EASYTOOL giúp truy xuất công cụ chất lượng cao. Trong các tình huống thế giới thực, việc yêu cầu người dùng đề xuất thủ công các công cụ từ một nhóm lớn cho LLM để lựa chọn có thể không thực tế. Do đó, ToolBench cũng cung cấp một retriever dày đặc dựa trên BERT-base (Devlin et al., 2019) để truy xuất các công cụ liên quan để giải quyết yêu cầu người dùng, và tuyên bố rằng nó vượt trội hơn text-embedding-ada-002, tức là GPT Ada (Ouyang et al., 2022), truy xuất công cụ dựa trên độ tương đồng cosine embedding giữa các nhiệm vụ con được phân tách bởi yêu cầu người dùng và mô tả công cụ từ tài liệu công cụ gốc trong ToolBench. Chúng tôi cho rằng hiệu suất kém của Ada có thể do mô tả công cụ chất lượng thấp, thường chứa các chi tiết không liên quan và thiếu hướng dẫn chức năng rõ ràng. Do đó, chúng tôi áp dụng mô tả công cụ được tạo bởi EASYTOOL để thay thế mô tả công cụ gốc. Theo Qin et al. (2023), chúng tôi so sánh hiệu suất của các phương pháp truy xuất này bằng NDCG (Järvelin and Kekäläinen, 2002). Kết quả trong Bảng 5 cho thấy rằng việc cung cấp mô tả công cụ được tạo bởi EASYTOOL có thể cải thiện đáng kể hiệu suất truy xuất.

Bảng 5: Hiệu suất của các retriever khác nhau cho hai tập con trong ToolBench.

EASYTOOL giúp LLM lựa chọn và thực thi công cụ. Để trả lời câu hỏi này, chúng tôi sử dụng I1-Instruction của ToolBench, bao gồm 100 yêu cầu người dùng có thể giải quyết bằng một công cụ duy nhất. Chúng tôi đầu tiên lấy công cụ vàng từ I1-Instruction và sau đó truy xuất các công cụ khác khác nhau dựa trên độ tương đồng cosine embedding giữa yêu cầu người dùng và mô tả công cụ làm công cụ ứng viên. Sau đó, chúng tôi đánh giá độ chính xác lựa chọn của LLM với số lượng công cụ ứng viên khác nhau, sử dụng mô tả ToolBench gốc hoặc những mô tả được tạo bởi EASYTOOL. Hình 3 minh họa rằng các mô tả được nâng cao bởi EASYTOOL cho phép LLM chọn công cụ chính xác hiệu quả hơn từ một nhóm lớn hơn.

Hình 3: Độ chính xác lựa chọn của LLM trên I1-instruction của ToolBench.

Đối với mỗi nhiệm vụ con trong I2-Category và I3-Instruction, chúng tôi truy xuất top 10 công cụ tương tự nhất bằng cách sử dụng mô tả công cụ của chúng tôi và yêu cầu các mô hình chọn và thực thi chúng. Như được hiển thị trong Bảng 4, việc sử dụng những công cụ được truy xuất này tỏ ra có thể so sánh, và đôi khi thậm chí vượt trội hơn ground truth tool set. Lý do là EASYTOOL-Retriever có thể truy xuất các công cụ tương tự với chức năng tốt hơn để thay thế một số công cụ trong ground truth tool set.

Phân tích Lỗi Chúng tôi tuân theo Zhang et al. (2023) và định nghĩa hai loại lỗi, tức là lỗi tên công cụ và lỗi tham số. Lỗi tên công cụ có nghĩa là các mô hình gọi các hàm công cụ không tồn tại không có trong kho công cụ, và lỗi tham số có nghĩa là các mô hình truyền các tham số không hợp lệ. Cả hai lỗi đều dẫn đến thực thi công cụ không thành công. Chúng tôi lấy mẫu 100 dữ liệu từ I2-Category và I3-Instruction và sử dụng ba người đánh giá để kiểm tra thủ công đầu ra của LLM với tài liệu công cụ và hướng dẫn công cụ được tạo bởi EASYTOOL. Chúng tôi trình bày tỷ lệ lỗi của mỗi loại lỗi trên ToolBench trong Hình 4. Kết quả cho thấy rằng LLM có thể tạo ra tên công cụ không tồn tại và truyền các tham số không hợp lệ cho các hàm công cụ đúng. Tuy nhiên, EASYTOOL của chúng tôi, với hướng dẫn công cụ ngắn gọn và hiệu quả, có thể giảm đáng kể những hành vi không chính xác này, dẫn đến thực thi công cụ thành công.

Hình 4: Tỷ lệ lỗi của các lệnh gọi công cụ trong các LLM khác nhau.

5.2 Dịch vụ Web Thế giới Thực
Các dịch vụ web thế giới thực thường cần thực thi các công cụ theo một thứ tự cụ thể. Ví dụ, một Dịch vụ Web giỏ hàng mua sắm yêu cầu người dùng thêm các mặt hàng vào giỏ hàng trước khi thanh toán. Chúng tôi nhằm khám phá khả năng của LLM để tìm đường dẫn giải pháp công cụ chính xác.

Benchmark Chúng tôi chọn RestBench (Song et al., 2023), bao gồm các nhiệm vụ trong các tình huống dịch vụ web thế giới thực. Chúng tôi đánh giá phương pháp của chúng tôi trên một tập con của RestBench, tức là TMDB. TMDB là một trang web thông tin phim cung cấp 55 API RESTful chính thức làm công cụ, bao gồm thông tin về phim,

--- TRANG 8 ---
TV, diễn viên và hình ảnh. Theo metric đánh giá trong RestBench, chúng tôi sử dụng tỷ lệ đường dẫn chính xác (CP%) để đo độ chính xác, là tỷ lệ đường dẫn công cụ được tạo bởi mô hình chứa đường dẫn công cụ vàng như một dãy con.

Baselines Chúng tôi chọn RestGPT (Song et al., 2023) làm mô hình cơ sở của chúng tôi. RestGPT có hai phiên bản, tức là RestGPT dựa trên Vicuna-13B và RestGPT dựa trên ChatGPT. Đối với RestGPT dựa trên Vicuna-13B, chúng tôi so sánh phương pháp của chúng tôi với ToolDec. Đối với RestGPT dựa trên ChatGPT, chúng tôi so sánh phương pháp của chúng tôi với ReAct vì ToolDec không thể áp dụng trong các mô hình closed-source.

Kết quả Chúng tôi đơn giản hóa tài liệu công cụ dài từ RestBench thành hướng dẫn công cụ ngắn gọn với EASYTOOL cho LLM sử dụng. Để so sánh, chúng tôi sử dụng prompt từ Song et al. (2023) chứa mô tả công cụ gốc và bốn ví dụ. Bảng 3 chứng minh rằng EASYTOOL giảm đáng kể chi phí token. Ngoài ra, Hình 5 làm nổi bật sự cải thiện đáng kể trong tỷ lệ đường dẫn chính xác, thể hiện hiệu quả của EASYTOOL trong việc hỗ trợ LLM tìm đường dẫn giải pháp công cụ chính xác.

Hình 5: Tỷ lệ đường dẫn chính xác (CP%) trên hai phiên bản của RestBench với các phương pháp khác nhau.

5.3 Lý luận Số học
Chúng tôi cũng khám phá liệu EASYTOOL có thể trao cho các agent dựa trên LLM khả năng sử dụng công cụ tốt hơn trong các bài toán toán học phức tạp với tài liệu công cụ không đầy đủ.

Benchmark Chúng tôi áp dụng FuncQA (Hao et al., 2023), kiểm tra khả năng lý luận số học của LLM trên các bài toán toán học phức tạp liên quan đến 13 công cụ phép toán số học (ví dụ: nhân, lũy thừa và lcm). Chúng tôi sử dụng hai tập con của FuncQA, tức là câu hỏi one-hop và multi-hop, để đánh giá phương pháp của chúng tôi. Các câu hỏi one-hop bao gồm 68 bài toán toán học có thể giải quyết bằng một công cụ. 60 câu hỏi multi-hop yêu cầu một số bước lý luận, trung bình 2,78 lần sử dụng công cụ mỗi câu hỏi. Chúng tôi đo độ chính xác bằng cách tính toán tỷ lệ phần trăm các bài toán được trả lời chính xác, với dung sai lỗi 0,1%. Chúng tôi cũng đo tỷ lệ lỗi công cụ (tức là Error), tỷ lệ các nhiệm vụ có ít nhất một lỗi liên quan đến công cụ.

Baselines Theo Hao et al. (2023), chúng tôi chọn Vicuna-30B và ChatGPT và so sánh phương pháp của chúng tôi với học 0-shot, prompting Chain-of-thought (CoT) và ReAct. Đối với ReAct, chúng tôi tuân theo các thiết lập trong (Hao et al., 2023), cung cấp bốn ví dụ, bao gồm năm ví dụ công cụ.

Kết quả Không giống như các bộ dữ liệu khác, FuncQA chỉ cung cấp tên và hàm gọi của một công cụ làm tài liệu, mà không có bất kỳ mô tả công cụ nào khác để minh họa sử dụng thêm. Do đó, chỉ bằng cách tận dụng tên công cụ và hàm gọi được cung cấp, chúng tôi cũng có thể áp dụng EASYTOOL để tạo mô tả công cụ với các tình huống sử dụng để xây dựng hướng dẫn công cụ cho FuncQA. Kết quả trong Bảng 6 cho thấy rằng: 1) Các hướng dẫn công cụ được tạo dựa trên phương pháp của chúng tôi (+ EASYTOOL) cải thiện đáng kể khả năng sử dụng công cụ của LLM trong các bài toán toán học phức tạp; 2) Hơn nữa, tỷ lệ lỗi công cụ thấp hơn của các mô hình với EASYTOOL cho thấy rằng so với học few-shot với minh họa, các hướng dẫn công cụ ngắn gọn và hiệu quả có thể hướng dẫn các mô hình chọn công cụ chính xác và truyền tham số hợp lệ tốt hơn.

Bảng 6: Độ chính xác của Vicuna-30B và ChatGPT trên FuncQA.

6 Kết luận
Trong bài báo này, chúng tôi giới thiệu EASYTOOL, một phương pháp dễ dàng và hiệu quả để nâng cao khả năng sử dụng công cụ của các agent dựa trên LLM thông qua việc đơn giản hóa và tinh chỉnh tài liệu công cụ thành hướng dẫn công cụ rõ ràng, có cấu trúc và thực tế. Các thí nghiệm toàn diện của chúng tôi chứng minh rằng EASYTOOL có thể nâng cao hiệu quả hiệu suất trong các ứng dụng thế giới thực khác nhau. Chúng tôi hy vọng EASYTOOL có thể là một phát triển đáng kể trong lĩnh vực các agent dựa trên LLM.

Hạn chế
Thứ nhất, bài báo này chỉ tập trung vào tài liệu công cụ có độ dài token không vượt quá giới hạn đầu vào ChatGPT. Tài liệu có số lượng token vượt quá giới hạn này không thể được xử lý bởi EASYTOOL mà không có tiền xử lý bổ sung. Thứ hai, phương pháp của chúng tôi bị hạn chế cho tài liệu đơn lẻ, bỏ qua các phụ thuộc giữa các công cụ. Việc xem xét những phụ thuộc này trong mô tả công cụ có thể nâng cao đáng kể hiệu quả của mô hình trong một số tình huống nhất định. Cuối cùng, EASYTOOL chỉ hoạt động trên các mô hình có khả năng tuân theo hướng dẫn. Công việc tương lai có thể tập trung vào huấn luyện các mô hình chuyên biệt bằng cách sử dụng hướng dẫn công cụ được tạo bởi EASYTOOL, do đó cải thiện khả năng của chúng trong việc sử dụng công cụ.

Tuyên bố Đạo đức
Chúng tôi thừa nhận rằng tất cả các tác giả đều được thông báo về và tuân thủ Quy tắc Đạo đức ACL và Quy tắc Ứng xử.

Sử dụng Chú thích Con người Tổ chức của chúng tôi đã tuyển dụng các người chú thích để thực hiện các chú thích của mô tả công cụ và hướng dẫn chức năng. Chúng tôi đảm bảo quyền riêng tư của các người chú thích được tôn trọng trong quá trình chú thích. Các người chú thích nhận được bồi thường vượt quá mức lương tối thiểu địa phương và đã đồng ý với việc sử dụng hướng dẫn công cụ được tạo bởi EASYTOOL cho mục đích nghiên cứu. Phụ lục A cung cấp thêm chi tiết về các chú thích.

Rủi ro Các benchmark công cụ trong thí nghiệm của chúng tôi được lấy từ các nguồn có sẵn công khai. Tuy nhiên, chúng tôi không thể đảm bảo rằng chúng không có ngôn ngữ có hại xã hội hoặc độc hại. Hơn nữa, việc đánh giá chất lượng dữ liệu của hướng dẫn công cụ dựa trên lẽ thường, có thể khác nhau giữa các cá nhân từ nền tảng đa dạng. Chúng tôi sử dụng ChatGPT để sửa lỗi ngữ pháp trong bài báo này.

Tài liệu tham khảo
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. 2023. Vicuna: Một chatbot mã nguồn mở gây ấn tượng gpt-4 với 90%* chất lượng chatgpt.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Significant Gravitas. 2023. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/Significant-Gravitas/Auto-GPT.

Shibo Hao, Tianyang Liu, Zhen Wang, và Zhiting Hu. 2023. ToolkenGPT: Augmenting frozen language models with massive tools via tool embeddings. Trong Thirty-seventh Conference on Neural Information Processing Systems.

Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, và Tomas Pfister. 2023. Tool documentation enables zero-shot tool-usage with large language models.

Kalervo Järvelin và Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422–446.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, và William El Sayed. 2023a. Mistral 7b.

Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, và Lili Qiu. 2023b. LLMLingua: Compressing prompts for accelerated inference of large language models. Trong Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, trang 13358–13376, Singapore. Association for Computational Linguistics.

Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, và Yongbin Li. 2023. Api-bank: A comprehensive benchmark for tool-augmented llms. Trong Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, trang 3102–3116.

Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, và Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. volume abs/2304.09842.

Jesse Mu, Xiang Lisa Li, và Noah D. Goodman. 2023. Learning to compress prompts with gist tokens. CoRR, abs/2304.08467.

--- TRANG 10 ---
OpenAI. 2022. Chatgpt.

OpenAI. 2023. GPT-4 technical report.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, và Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Trong Advances in Neural Information Processing Systems.

Aaron Parisi, Yao Zhao, và Noah Fiedel. 2022. TALM: tool augmented language models. CoRR, abs/2205.12255.

Shishir G. Patil, Tianjun Zhang, Xin Wang, và Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. CoRR, abs/2305.15334.

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, và Maosong Sun. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. CoRR, abs/2307.16789.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, và Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, và Yueting Zhuang. 2023a. HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face. Trong Thirty-seventh Conference on Neural Information Processing Systems.

Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, và Yueting Zhuang. 2023b. Taskbench: Benchmarking large language models for task automation. arXiv preprint arXiv:2311.18760.

Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, và Sujian Li. 2023. Restgpt: Connecting large language models with real-world restful apis.

Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, và Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases.

Gemini Team và Google. 2023. Gemini: A family of highly capable multimodal models.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288.

Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, và Jian Zhang. 2023. On the tool manipulation capability of open-source large language models.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, và Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Trong The Eleventh International Conference on Learning Representations.

Kexun Zhang, Hongqiao Chen, Lei Li, và William Wang. 2023. Syntax error-free and generalizable tool use for llms via finite-state decoding.

Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, và Chao Zhang. 2024. Toolchain*: Efficient action space navigation in large language models with a* search. Trong The Twelfth International Conference on Learning Representations.

--- TRANG 11 ---
A Chi tiết Crowdsourcing cho Đánh giá Hướng dẫn Công cụ
Chúng tôi đã tuyển dụng một nhóm ba sinh viên đại học. Chúng tôi trả cho mỗi người chú thích 8$/giờ, vượt quá mức lương tối thiểu địa phương. Ảnh chụp màn hình của hướng dẫn và giao diện cho chú thích mô tả công cụ và hướng dẫn chức năng được hiển thị trong Hình 7 và Hình 8.

B Chi tiết của ToolBench

B.1 Đánh giá Tỷ lệ Thành công
Prompt của đánh giá tỷ lệ thành công được đưa ra trong Danh sách 1.

Danh sách 1: Mẫu hướng dẫn cho GPT-4 để đánh giá tỷ lệ thành công của kết quả trên ToolBench

Vui lòng kiểm tra xem phản hồi có thể trả lời một cách hợp lý và chính xác câu hỏi hay không. Nếu có thể, vui lòng xuất 'YES'; Nếu không, vui lòng xuất 'NO'

Bạn cần đưa ra lý do trước rồi mới quyết định xem phản hồi có thể trả lời một cách hợp lý và chính xác câu hỏi hay không. Bạn chỉ được xuất dưới định dạng JSON có thể phân tích. Hai ví dụ đầu ra trông như:

Ví dụ 1: {"Reason": "Lý do tại sao bạn nghĩ phản hồi có thể trả lời một cách hợp lý và chính xác câu hỏi", "Choice": "Yes"}

Ví dụ 2: {"Reason": "Lý do tại sao bạn nghĩ phản hồi không thể trả lời một cách hợp lý và chính xác câu hỏi", "Choice": "No"}

Đây là câu hỏi của người dùng: {question}
Đây là phản hồi: {answer}
Đầu ra:

B.2 Chi tiết của Baselines trên ToolBench
Vicuna-7B (Chiang et al., 2023) là biến thể LLaMA được tinh chỉnh trên hướng dẫn và cuộc trò chuyện do người dùng chia sẻ. Mistral-Instruct-7B (Jiang et al., 2023a) là biến thể Mistral-7B được tinh chỉnh trên hướng dẫn, thể hiện khả năng tuân theo hướng dẫn và lý luận tuyệt vời. ReAct trong Qin et al. (2023) đầu tiên phân tách yêu cầu sử dụng thành các nhiệm vụ con và sau đó lập kế hoạch các lệnh gọi công cụ để hoàn thành các nhiệm vụ con. DFSDT trong Qin et al. (2023) áp dụng cây quyết định dựa trên tìm kiếm theo chiều sâu để cho phép LLM đưa ra quyết định có cân nhắc bằng cách đánh giá các đường dẫn lý luận khác nhau.

Theo thiết lập trong Qin et al. (2023), đối với ChatGPT và GPT-4, chúng tôi trực tiếp tận dụng lệnh gọi hàm để sử dụng công cụ. Đối với các mô hình khác, chúng tôi tổng hợp đầu vào ở định dạng lệnh gọi hàm cho những mô hình này.

C Đánh giá Tính mạnh mẽ
Trong phần này, chúng tôi nhằm đánh giá tính mạnh mẽ của các prompt nhiệm vụ trong Bảng 2. Chúng tôi yêu cầu ChatGPT viết lại những prompt nhiệm vụ này ba lần, và các prompt nhiệm vụ mới được hiển thị trong Danh sách 2. Chúng tôi lấy mẫu 100 tài liệu công cụ từ ToolBench và yêu cầu ChatGPT tạo mô tả công cụ và hướng dẫn chức năng công cụ dựa trên các prompt nhiệm vụ mới. Sau đó, chúng tôi yêu cầu hai người chú thích đánh giá chất lượng của bốn kết quả (một từ prompt nhiệm vụ của chúng ta và ba từ prompt nhiệm vụ do ChatGPT tạo). Kết quả trong Hình 6 cho thấy rằng những thay đổi đối với prompt nhiệm vụ, mà không làm thay đổi ý nghĩa thực tế, không ảnh hưởng đến chất lượng của mô tả công cụ và hướng dẫn chức năng công cụ, do đó chứng minh tính mạnh mẽ của các prompt của chúng tôi.

Danh sách 2: Các prompt nhiệm vụ do ChatGPT tạo cho việc tạo mô tả công cụ và xây dựng hướng dẫn chức năng công cụ.

Prompt-1:
- Mô tả Công cụ:
Nhiệm vụ của bạn liên quan đến việc phát triển một mô tả ngắn gọn và thực tế về việc sử dụng một công cụ cụ thể, như được nêu trong tài liệu của nó. Mô tả này chỉ nên tập trung vào các chức năng của công cụ, loại trừ bất kỳ chi tiết không liên quan nào.

- Hướng dẫn Chức năng Công cụ:
Tạo một tình huống kết hợp việc sử dụng một công cụ được chỉ định, đảm bảo nó sử dụng các tham số được cung cấp. Nhận mô tả của một công cụ, bao gồm mục đích và danh sách các tham số của nó. Thiết kế một tình huống sử dụng những tham số này một cách hiệu quả. Nếu cả "required_parameters" và "optional_parameters" đều vắng mặt, định dạng phản hồi của bạn như: {"Scenario": XX, "Parameters": {}}.

Prompt-2:
- Mô tả Công cụ:
Nhiệm vụ của bạn liên quan đến việc tạo ra một mô tả ngắn gọn và thực tế của một công cụ, sử dụng tài liệu của nó làm tham khảo. Tập trung vào việc nêu bật các chức năng của công cụ, loại trừ bất kỳ chi tiết không liên quan nào.

- Hướng dẫn Chức năng Công cụ:
Nhiệm vụ của bạn liên quan đến việc phát triển một tình huống sử dụng một công cụ được chỉ định. Đây là các hướng dẫn: Bạn sẽ nhận thông tin về một công cụ, bao gồm mục đích sử dụng dự định và một

--- TRANG 12 ---
danh sách các tham số. Tình huống của bạn nên kết hợp những tham số này. Trong trường hợp cả "required_parameters" và "optional_parameters" đều vắng mặt, định dạng phản hồi của bạn như sau: {"Scenario": XX, "Parameters": {}}.

Prompt-3:
- Mô tả Công cụ:
Nhiệm vụ của bạn liên quan đến việc viết một mô tả ngắn gọn và rõ ràng về việc sử dụng của một công cụ, được hướng dẫn bởi tài liệu của nó. Mô tả này chỉ nên tập trung vào các chức năng của công cụ, bỏ qua bất kỳ chi tiết không liên quan nào.

- Hướng dẫn Chức năng Công cụ:
Nhiệm vụ của bạn liên quan đến việc tạo ra một tình huống sử dụng một công cụ cụ thể. Đây là cách tiến hành: Đầu tiên, làm quen với mục đích sử dụng dự định của công cụ và các tham số có sẵn của nó. Sau đó, thiết kế một tình huống kết hợp những tham số này một cách hiệu quả. Trong trường hợp cả "required_parameters" và "optional_parameters" đều vắng mặt, định dạng phản hồi của bạn như sau: {"Scenario": XX, "Parameters": {}}.

Hình 6: So sánh các prompt nhiệm vụ của chúng tôi với các prompt nhiệm vụ do ChatGPT tạo.

D Phương pháp Nén Prompt
Chúng tôi cũng áp dụng LLMLingua (Jiang et al., 2023b), một phương pháp nén prompt, để xác định và loại bỏ các token không cần thiết trong tài liệu công cụ. Như được hiển thị trong Bảng 7, phương pháp này không thể được áp dụng cho nhiệm vụ của chúng tôi vì nó có thể nén một số token trong tham số và hàm, là những thứ cần thiết cho việc thực thi công cụ thành công.

E Ví dụ của Hướng dẫn Công cụ

E.1 Ví dụ Dữ liệu của ToolBench
Bảng 8 trình bày một số ví dụ về hướng dẫn công cụ được tạo bởi EASYTOOL trong ToolBench để hiểu rõ hơn.

E.2 Ví dụ Dữ liệu của RestBench
Bảng 9 trình bày một số ví dụ về hướng dẫn công cụ được tạo bởi EASYTOOL trong RestBench để hiểu rõ hơn.

E.3 Ví dụ Dữ liệu của FuncQA
Bảng 10 trình bày một số ví dụ về hướng dẫn công cụ được tạo bởi EASYTOOL trong FuncQA để hiểu rõ hơn.

--- TRANG 13 ---
Tài liệu Công cụ:
{
"product_id": "api_b04d269d-c7dd-4b84-8e17-6fba24d64d3d",
"tool_description": "Get Products from Ebay (Unofficial)",
"home_url": "https://rapidapi.com/felixeschmittfes/api/ebay32/",
"name": "Ebay",
"title": "Ebay",
"pricing": "FREEMIUM",
"tool_name": "Ebay",
"host": "ebay32.p.rapidapi.com",
"api_list": [
{
"name": "Product Details",
"url": "https://ebay32.p.rapidapi.com/product/195499451557",
"description": "Get the product details for a given product id and a specific country.\nDefault country is `United States`.\nSpecify country with country name or country code.\n\nAllowed countries:\nDefault: `us`\n- Germany (de)\n- France (fr)\n- Australia (au)\n- Austria (at)\n- Canada (ca)\n- Hong Kong (hk)\n- Ireland (ie)\n- Italy (it)\n- Malaysia (my)\n- Netherlands (nl)\n- Singapore (sg)\n- Switzerland (ch)\n- United Kingdom (uk)",
"method": "GET",
"required_parameters": [
{
"name": "product_id",
"type": "NUMBER",
"description": "ID of the product. Can be obtained from the url of the product or by using the `/search` endpoint.",
"default": "195499451557"
}
],
"optional_parameters": [
{
"name": "country",
"type": "STRING",
"description": "Valid country to return offers for.\nValid values are in description of this endpoint.\nDefault: `united states`.",
"default": "germany"
},
{
"name": "country_code",
"type": "STRING",
"description": "Country code of the valid country to return offers for.\nValid values are in description of this endpoint.\nDefault: `us`.",
"default": "de"
}
]
}
]
}

Hướng dẫn Công cụ được Nén bởi LLMLingua:
{
"product"_b04d269d-c7be-fba24d64d",
"_": "Get fromay (Unofficial":// id ./fixeschmittfes/ay/"
""
""
"FREEM"
""
".p."
"_": [
"": "Product Details",
"url": "https://ebay32.p.rapidapi.com/product/195499451557",
"description": "Get the product details for a given product id and a specific country.
Default country is `United States`.
Specify country with country name or country code.

Bảng 7: Tài liệu công cụ gốc và hướng dẫn công cụ được nén bởi LLMLingua.

--- TRANG 14 ---
Hình 7: Ảnh chụp màn hình của hướng dẫn và giao diện cho chú thích mô tả công cụ.

--- TRANG 15 ---
Hình 8: Ảnh chụp màn hình của hướng dẫn và giao diện cho chú thích hướng dẫn chức năng công cụ.

--- TRANG 16 ---
Mô tả Công cụ:
/* Ví dụ 1 */
'TokopediaApi' có thể tìm kiếm và truy xuất chi tiết sản phẩm từ Tokopedia. Công cụ này có 2 API: 1. 'Search Product' có thể tìm kiếm sản phẩm trên Tokopedia dựa trên chuỗi truy vấn và loại hành động. 2. 'Get Product Detail' có thể truy xuất thông tin chi tiết về một sản phẩm trên Tokopedia dựa trên slug của nó.

/* Ví dụ 2 */
'Tokopedia Super API' có thể dễ dàng truy xuất thông tin shop và sản phẩm. Công cụ này có 1 API: 1. 'sortProductsMaster' có thể cung cấp danh sách các phương pháp sắp xếp có sẵn.

Hướng dẫn Chức năng Công cụ:
/* Ví dụ 1 */
{
"name": "Search Product",
"description": "Search The Product",
"required_parameters": [
{
"name": "query",
"type": "STRING",
"description": "",
"default": "Celana Jeans"
},
{
"name": "act",
"type": "STRING",
"description": "",
"default": "search"
}
],
"optional_parameters": [],
"Example": {
"Scenario": "nếu bạn muốn tìm kiếm một sản phẩm với truy vấn 'Celana Jeans' sử dụng hành động 'search'",
"Parameters": {
"query": "Celana Jeans",
"act": "search"
}
}
}

/* Ví dụ 2 */
{
"name": "sortProductsMaster",
"description": "danh sách các phương pháp sắp xếp có sẵn",
"required_parameters": [],
"optional_parameters": [],
"Example": {
"Scenario": "nếu bạn muốn truy xuất danh sách các phương pháp sắp xếp có sẵn cho sản phẩm sử dụng Tokopedia Super API",
"Parameters": {}
}
}

Bảng 8: Hướng dẫn công cụ của ToolBench được tạo bởi EASYTOOL.

--- TRANG 17 ---
Mô tả Công cụ:
/* Ví dụ 1 */
'/tv/latest' có thể lấy chương trình TV được tạo mới nhất.

/* Ví dụ 2 */
'/search/collection' có thể tìm kiếm các bộ sưu tập, có thể lấy collection_id.

Hướng dẫn Chức năng Công cụ:
/* Ví dụ 1 */
{
"tool_usage": "GET /person/{person_id}/tv_credits",
"Example": {
"Scenario": "Nếu bạn muốn lấy credits chương trình TV của một người với person_id 456.",
"Parameters": {
"input": "GET /person/456/tv_credits"
}
}
}

/* Ví dụ 2 */
{
"tool_usage": "GET /tv/latest",
"Example": {
"Scenario": "Nếu bạn muốn lấy chương trình TV được tạo mới nhất.",
"Parameters": {
"input": "GET /tv/latest"
}
}
}

Bảng 9: Hướng dẫn công cụ của RestBench được tạo bởi EASYTOOL.

--- TRANG 18 ---
Mô tả Công cụ:
/* Ví dụ 1 */
'add_' trả về tổng của tất cả các đối số được truyền cho nó, được chuẩn hóa đến 2 chữ số thập phân.

/* Ví dụ 2 */
'subtract_' trả về hiệu của các đối số được truyền cho nó, bắt đầu với đối số đầu tiên và trừ tất cả các đối số tiếp theo, được chuẩn hóa đến 2 chữ số thập phân.

Hướng dẫn Chức năng Công cụ:
/* Ví dụ 1 */
{
"required_parameters":[
{
"name":"input",
"type":"List"
}
],
"Example":{
"Scenario":"nếu bạn muốn cộng 2 với 1.",
"Parameters":{
"input":[2,1]
}
}
}

/* Ví dụ 2 */
{
"required_parameters": [
{
"name": "input",
"type": "List"
}
],
"Example": {
"Scenario": "nếu bạn muốn trừ 2 từ 1.",
"Parameters": {
"input": [1,2]
}
}
}

Bảng 10: Hướng dẫn công cụ của FuncQA được tạo bởi EASYTOOL.
