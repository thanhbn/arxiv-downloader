[Tiếp tục với các tài liệu tham khảo còn lại từ [21] đến [101] và tất cả các bảng phụ lục...]

[21] DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, và tomtung. 2017. Quora Question Pairs. https://kaggle.com/competitions/quora-question-pairs
[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding.
[23] William B. Dolan và Chris Brockett. 2005. Automatically Constructing a Corpus of Sentential Paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005. Asian Federation of Natural Language Processing. https://aclanthology.org/I05-5002/
[24] Ondrej Dusek, David M. Howcroft, và Verena Rieser. 2019. Semantic Noise Matters for Neural Natural Language Generation. In Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, Tokyo, Japan, October 29 - November 1, 2019, Kees van Deemter, Chenghua Lin, và Hiroya Takamura (Eds.). Association for Computational Linguistics, 421–426. https://doi.org/10.18653/v1/W19-8652
[25] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling.
[26] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, và Connor Leahy. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling.
[27] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Mingwei Chang. 2020. Retrieval augmented language model pre-training. , 3929–3938 trang.
[28] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. 2020. Measuring massive multitask language understanding.
[29] Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, và Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. , 113–122 trang.
[30] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, và Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning.
[31] Gautier Izacard và Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering.
[32] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, và Edouard Grave. 2022. Few-shot learning with retrieval augmented language models.
[33] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, và Pascale Fung. 2023. Survey of hallucination in natural language generation. , 38 trang.
[34] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, và Graham Neubig. 2023. Active retrieval augmented generation.
[35] Jeff Johnson, Matthijs Douze, và Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 7, 3 (2019), 535–547.
[36] Constantinos Karouzos, Georgios Paraskevopoulos, và Alexandros Potamianos. 2021. UDALM: Unsupervised domain adaptation through language modeling.
[37] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering.
[38] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, và Dan Roth. 2018. Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), Marilyn A. Walker, Heng Ji, và Amanda Stent (Eds.). Association for Computational Linguistics, 252–262. https://doi.org/10.18653/v1/n18-1023
[39] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. , 453–466 trang.
[40] Hector J. Levesque. 2011. The Winograd Schema Challenge. In Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI. http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2502
[41] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. , 9459–9474 trang.
[42] Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, và Sebastian Riedel. 2021. Paq: 65 million probably-asked questions and what you can do with them. , 1098–1115 trang.
[43] Bill Yuchen Lin, Ming Shen, Wangchunshu Zhou, Pei Zhou, Chandra Bhagavatula, Yejin Choi, và Xiang Ren. 2020. CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning. In Conference on Automated Knowledge Base Construction, AKBC 2020, Virtual, June 22-24, 2020, Dipanjan Das, Hannaneh Hajishirzi, Andrew McCallum, và Sameer Singh (Eds.). https://www.akbc.ws/2020/papers/yuD2q50HWv
[44] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, và Jie Tang. 2023. WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences.
[45] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.
[46] Zheng Liu và Yingxia Shao. 2022. Retromae: Pre-training retrieval-oriented transformers via masked auto-encoder.
[47] Aman Madaan, Niket Tandon, Peter Clark, và Yiming Yang. 2022. Memory-assisted prompt editing to improve gpt-3 after deployment.
[48] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, và Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories.
[49] Kelong Mao, Hongjin Qian, Fengran Mo, Zhicheng Dou, Bang Liu, Xiaohua Cheng, và Zhao Cao. 2023. Learning Denoised and Interpretable Session Representation for Conversational Search. , 3193–3202 trang.
[50] Kelong Mao, Hongjin Qian, Fengran Mo, Zhicheng Dou, Bang Liu, Xiaohua Cheng, và Zhao Cao. 2023. Learning Denoised and Interpretable Session Representation for Conversational Search. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW '23). Association for Computing Machinery, New York, NY, USA, 3193–3202. https://doi.org/10.1145/3543507.3583265
[51] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: a survey.
[52] Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. 2018. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, Ellen Riloff, David Chiang, Julia Hockenmaier, và Jun'ichi Tsujii (Eds.). Association for Computational Linguistics, 2381–2391. https://doi.org/10.18653/v1/d18-1260
[53] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, và Nils Reimers. 2022. MTEB: Massive text embedding benchmark.
[54] Linyong Nan, Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, và Nazneen Fatema Rajani. 2021. DART: Open-Domain Structured Data Record to Text Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, và Yichao Zhou (Eds.). Association for Computational Linguistics, 432–447. https://doi.org/10.18653/v1/2021.naacl-main.37
[55] Courtney Napoles, Matthew R. Gormley, và Benjamin Van Durme. 2012. Annotated Gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX@NAACL-HLT 2012, Montrèal, Canada, June 7-8, 2012, James Fan, Raphael Hoffman, Aditya Kalyanpur, Sebastian Riedel, Fabian M. Suchanek, và Partha Pratim Talukdar (Eds.). Association for Computational Linguistics, 95–100. https://aclanthology.org/W12-3018/
[56] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pre-training.
[57] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, và Li Deng. 2016. Ms marco: A human-generated machine reading comprehension dataset.
[58] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. , 27730–27744 trang.
[59] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2020. KILT: a benchmark for knowledge intensive language tasks.
[60] Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Oğuz, Edouard Grave, Wen-tau Yih, et al. 2021. The web is your oyster-knowledge-intensive NLP against a very large web corpus.
[61] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023. Tool learning with foundation models.
[62] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis.
[63] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, và Haifeng Wang. 2020. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering.
[64] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, và Timothy P Lillicrap. 2019. Compressive Transformers for Long-Range Sequence Modelling. https://arxiv.org/abs/1911.05507
[65] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. , 5485–5551 trang.
[66] Pranav Rajpurkar, Robin Jia, và Percy Liang. 2018. Know What You Don't Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, Iryna Gurevych và Yusuke Miyao (Eds.). Association for Computational Linguistics, 784–789. https://doi.org/10.18653/v1/P18-2124
[67] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. SQuAD: 100, 000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, Jian Su, Xavier Carreras, và Kevin Duh (Eds.). The Association for Computational Linguistics, 2383–2392. https://doi.org/10.18653/v1/d16-1264
[68] Nils Reimers và Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks.
[69] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. , 333–389 trang.
[70] Melissa Roemmele, Cosmin Adrian Bejan, và Andrew S. Gordon. 2011. Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. In Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI. http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418
[71] Ohad Rubin và Jonathan Berant. 2023. Long-range Language Modeling with Self-retrieval.
[72] Tapan Sahni, Chinmay Chandak, Naveen Reddy Chedeti, và Manish Singh. 2017. Efficient Twitter sentiment classification using subjective distant supervision. In 9th International Conference on Communication Systems and Networks, COMSNETS 2017, Bengaluru, India, January 4-8, 2017. IEEE, 548–553. https://doi.org/10.1109/COMSNETS.2017.7945451
[73] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. 2021. WinoGrande: an adversarial winograd schema challenge at scale. Commun. ACM 64, 9 (2021), 99–106. https://doi.org/10.1145/3474381
[74] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, và Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools.
[75] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, và Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL. ACL, 1631–1642. https://aclanthology.org/D13-1170/
[76] Hongjin Su, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, Tao Yu, et al. 2022. One embedder, any task: Instruction-finetuned text embeddings.
[77] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, và Iryna Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models.
[78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.
[79] Lewis Tunstall, Leandro Von Werra, và Thomas Wolf. 2022. Natural language processing with transformers.
[80] Kexin Wang, Nandan Thakur, Nils Reimers, và Iryna Gurevych. 2021. Gpl: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval.
[81] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, và Furu Wei. 2022. Simlm: Pre-training with representation bottleneck for dense passage retrieval.
[82] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, và Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training.
[83] Liang Wang, Nan Yang, và Furu Wei. 2023. Learning to Retrieve In-Context Examples for Large Language Models.
[84] Tianshi Wang, Li Liu, Huaxiang Zhang, Long Zhang, và Xiuxiu Chen. 2020. Joint Character-Level Convolutional and Generative Adversarial Networks for Text Classification. Complex. 2020 (2020), 8516216:1–8516216:11. https://doi.org/10.1155/2020/8516216
[85] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, và Furu Wei. 2023. Augmenting Language Models with Long-Term Memory.
[86] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, và Quoc V Le. 2021. Finetuned language models are zero-shot learners.
[87] Adina Williams, Nikita Nangia, và Samuel R. Bowman. 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), Marilyn A. Walker, Heng Ji, và Amanda Stent (Eds.). Association for Computational Linguistics, 1112–1122. https://doi.org/10.18653/v1/n18-1101
[88] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, và Christian Szegedy. 2022. Memorizing transformers.
[89] Shitao Xiao và Zheng Liu. 2023. BAAI General Embedding. https://github.com/FlagOpen/FlagEmbedding
[90] Shitao Xiao, Zheng Liu, Weihao Han, Jianjin Zhang, Defu Lian, Yeyun Gong, Qi Chen, Fan Yang, Hao Sun, Yingxia Shao, et al. 2022. Distill-vq: Learning retrieval oriented vector quantization by distilling knowledge from dense embeddings. , 1513–1523 trang.
[91] Shitao Xiao, Zheng Liu, Yingxia Shao, Defu Lian, và Xing Xie. 2021. Matching-oriented product quantization for ad-hoc retrieval.
[92] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, và Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval.
[93] Jing Xu, Arthur Szlam, và Jason Weston. 2021. Beyond goldfish memory: Long-term open-domain conversation.
[94] Jing Xu, Arthur Szlam, và Jason Weston. 2022. Beyond Goldfish Memory: Long-Term Open-Domain Conversation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, và Aline Villavicencio (Eds.). Association for Computational Linguistics, 5180–5197. https://doi.org/10.18653/v1/2022.acl-long.356
[95] Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, và Arnold Overwijk. 2022. Cocodr: Combating distribution shifts in zero-shot dense retrieval with contrastive and distributionally robust learning.
[96] Zichun Yu, Chenyan Xiong, Shi Yu, và Zhiyuan Liu. 2023. Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In.
[97] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. 2019. HellaSwag: Can a Machine Really Finish Your Sentence?. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, và Lluís Màrquez (Eds.). Association for Computational Linguistics, 4791–4800. https://doi.org/10.18653/v1/p19-1472
[98] Rui Zhang và Joel R. Tetreault. 2019. This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, và Lluís Màrquez (Eds.). Association for Computational Linguistics, 446–456. https://doi.org/10.18653/v1/p19-1043
[99] Xiang Zhang, Junbo Jake Zhao, và Yann LeCun. 2015. Character-level Convolutional Networks for Text Classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, và Roman Garnett (Eds.). 649–657. https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html
[100] Yuan Zhang, Jason Baldridge, và Luheng He. 2019. PAWS: Paraphrase Adversaries from Word Scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, và Thamar Solorio (Eds.). Association for Computational Linguistics, 1298–1308. https://doi.org/10.18653/v1/n19-1131
[101] Bartosz Piotrowski Zhangir Azerbayev, Edward Ayers. 2008. Proof-Pile. https://huggingface.co/datasets/hoskinson-center/proof-pile/. [Online; truy cập ngày 19-July-2008].

--- TRANG 13 ---
Retrieve Anything To Augment Large Language Models Conference'17, July 2017, Washington, DC, USA

Bảng 5: Thống kê của Multi-Session Chat: số lượt hội thoại trung bình và số token trung bình mỗi phát ngôn trong huấn luyện/kiểm tra.

| Phân chia | #Mẫu | # Lượt Lịch sử | Độ dài Phát ngôn |
|-----------|------|----------------|------------------|
| Huấn luyện | 48925 | 14 | 33 |
| Kiểm tra | 2763 | 27 | 44 |

A CHI TIẾT BỘ DỮ LIỆU

A.1 Tăng cường Kiến thức

A.1.1 MMLU. MMLU là một bộ dữ liệu hiểu ngôn ngữ đa nhiệm với 14042 câu hỏi trắc nghiệm, bao trùm 57 nhiệm vụ phụ đa dạng, như đại số tuyến tính, khoa học máy tính, v.v. Đối với tăng cường truy xuất, chúng tôi truy xuất 3 đoạn văn từ bộ sưu tập MSMARCO Passage [57], bao gồm 8841823 đoạn văn từ web. Chúng tôi sử dụng template prompt chính thức cho đánh giá MMLU, được thể hiện trong A.1, và thêm các đoạn văn được truy xuất vào đầu câu hỏi. Vì các thí nghiệm của chúng tôi dựa trên mô hình Chat được tinh chỉnh, chúng tôi bỏ qua các ví dụ few-shot trên MMLU. Chúng tôi chọn tùy chọn có khả năng cao nhất (từ A, B, C, D) làm câu trả lời từ LLM.

Prompt A.1: MMLU (Zero-Shot)
Kiến thức:
<Đoạn văn 1>
<Đoạn văn 2>
<Đoạn văn 3>
Sau đây là các câu hỏi trắc nghiệm (có đáp án) về <chủ đề>.
<Câu hỏi>
A. <Tùy chọn 1>
B. <Tùy chọn 2>
C. <Tùy chọn 3>
D. <Tùy chọn 4>
Đáp án:

A.1.2 PopQA. PopQA là một bộ dữ liệu trả lời câu hỏi tập trung vào thực thể Wikipedia bao gồm 14267 câu hỏi về các thực thể phổ biến và đuôi dài. Đối với tăng cường truy xuất, chúng tôi truy xuất 3 đoạn văn từ dump Wikipedia 2019 được tiền xử lý bởi [37], chứa 21051324 đoạn văn với 100 token mỗi đoạn. Chúng tôi sử dụng template prompt chính thức và chiến lược đánh giá few-shot trên PopQA, được thể hiện trong A.2. Có 15 minh họa few-shot cho mỗi câu hỏi, mỗi minh họa đến từ một mối quan hệ riêng biệt. Mô hình tiến hành sinh greedy cho đến token \n. Câu trả lời được tạo ra được coi là đúng nếu nó chứa bất kỳ câu trả lời nào được định nghĩa trước trong bộ dữ liệu.

Prompt A.2: PopQA
Kiến thức:
<Đoạn văn 1>
<Đoạn văn 2>
<Đoạn văn 3>
Q: <Câu hỏi 1> A: <Đáp án 1>
Q: <Câu hỏi 2> A: <Đáp án 2>
...
Q: <Câu hỏi 15> A: <Đáp án 15>
Q: <Câu hỏi> A:

A.1.3 Huấn luyện. Cả MMLU và PopQA chỉ được sử dụng để đánh giá. Để huấn luyện retriever hướng tới việc thu thập kiến thức hữu ích cho LLM, chúng tôi sử dụng hai bộ dữ liệu truy xuất phổ biến: MSMARCO [57] và NQ [39]. Thống kê của chúng được thể hiện trong Bảng 9.

A.2 Mô hình hóa Ngữ cảnh Dài

A.2.1 Hội thoại Dài. Chúng tôi tận dụng bộ dữ liệu Multi-Session Chat [94] để đánh giá hiệu suất của LLM trên hội thoại dài. MSC là một bộ dữ liệu hội thoại chứa nhiều phiên giữa hai người nói. Thống kê của bộ dữ liệu Multi-Session Chat được báo cáo trong Bảng 5. Mỗi lượt hội thoại có định dạng này: "Người nói 1: xxx\nNgười nói 2: xxx" và các lượt liên tiếp được phân tách bằng \n. Perplexity cấp token được đánh giá bằng phản hồi từ Người nói 2. Đối với baseline None, chúng tôi chỉ nhập vào LLM lượt hội thoại cuối cùng. Đối với baseline Recent, 2 lượt hội thoại gần nhất được nhập vào LLM. Đối với tăng cường truy xuất, một lượt hội thoại được truy xuất từ toàn bộ lịch sử và được nối phía trước lượt cuối cùng.

Bảng 6: Thống kê của các bộ dữ liệu mô hình ngôn ngữ tầm xa.

| Bộ dữ liệu | #Mẫu Huấn luyện | #Mẫu Kiểm tra | Độ dài |
|------------|------------------|----------------|---------|
| Books3 | 10000 | 1000 | 101010 |
| Arxiv | 10000 | 757 | 26735 |
| CodeParrot | 10000 | 1000 | 217364 |
| PG19 | n.a. | 1000 | 90447 |

A.2.2 Mô hình Ngôn ngữ Tầm xa. Chúng tôi sử dụng bốn bộ dữ liệu mô hình ngôn ngữ tầm xa phổ biến để đánh giá hiệu suất của LLM trên các chuỗi dài: Books3 bao gồm các tác phẩm văn học đa dạng từ các lĩnh vực khác nhau, và PG19 chứa sách từ Project Gutenberg. Cả hai bộ dữ liệu đều được trích xuất từ the Pile [26]. Arxiv, a.k.a. Proof-Pile [101], là một bộ sưu tập các bản thảo toán học trên arxiv. CodeParrot là một corpus lớn mã dự án được làm sạch từ Github. Chúng tôi nối mã của cùng một repository để có được văn bản đủ dài, dẫn đến 437079 mẫu tổng cộng. Đối với tất cả bốn bộ dữ liệu, chúng tôi lọc ra văn bản ngắn hơn 160k ký tự, sau đó lấy mẫu ngẫu nhiên 10000 để huấn luyện và 1000 để kiểm tra. Bộ dữ liệu PG19 được giữ lại và chỉ được sử dụng trong kiểm tra. Chúng tôi tóm tắt thống kê của bốn bộ dữ liệu mô hình ngôn ngữ tầm xa trong Bảng 6.

Trong thực tế, chúng tôi cắt ngắn tất cả các mẫu kiểm tra xuống 32768 token. Perplexity cấp token được đánh giá với batch size 1 trên 1024 token cuối cùng (được gọi là target tokens). Đối với baseline None và Recent, 2048 và 4096 token cuối cùng được đưa vào mô hình, tương ứng. Đối với tăng cường truy xuất, văn bản được chia thành các chunks với kích thước chunk 128, và 2048 token cuối cùng luôn được cố định trong quá trình đánh giá. Đối với mỗi chunk trong target tokens, chúng tôi truy xuất 8 chunks và chunk tiếp theo của chúng (kích thước chunk 128) từ 30720 token trước đó. Các chunks được truy xuất được nối trực tiếp phía trước 2048 token cố định mà không có dấu phân cách.

A.3 Học Trong ngữ cảnh
Thông tin chi tiết về các bộ dữ liệu học trong ngữ cảnh được báo cáo trong Bảng 7. Cụ thể, có hai chiến lược đánh giá: Likelihood và Generation. Chiến lược đầu tiên có nghĩa là chúng tôi chấm điểm mỗi tùy chọn ứng viên với khả năng của LLM khi có các tùy chọn có sẵn (ví dụ Có và Không trên bộ dữ liệu WSC), và chọn tùy chọn có điểm cao nhất; Chiến lược thứ hai có nghĩa là chúng tôi để LLM thực hiện sinh greedy mà không lấy mẫu. Theo [83], chúng tôi lấy mẫu ngẫu nhiên tối đa 30000 instance từ mỗi bộ dữ liệu để huấn luyện, và giữ lại 4 bộ dữ liệu từ huấn luyện. Tuy nhiên, khác với đánh giá của họ, chúng tôi giữ nguyên top-8 ví dụ được truy xuất bất kể chúng có thuộc cùng nhiệm vụ với hướng dẫn đầu vào hay không.

A.4 Học Công cụ và Tìm kiếm Hội thoại

A.4.1 Học Công cụ. Chúng tôi sử dụng bộ dữ liệu ToolBench [62] để đánh giá hiệu suất của truy xuất công cụ, nơi retriever nhận vào một yêu cầu người dùng và tìm kiếm một công cụ hữu ích theo mô tả của nó. Thống kê của ToolBench được báo cáo trong Bảng 8.

A.4.2 Tìm kiếm Hội thoại. Chúng tôi sử dụng bộ dữ liệu QReCC phổ biến [3] để đánh giá hiệu suất của tìm kiếm hội thoại. Cụ thể, có một cuộc hội thoại ngắn theo sau bởi một truy vấn "theo ngữ cảnh" trong mỗi mẫu. Retriever nhận vào sự nối của toàn bộ cuộc hội thoại và truy vấn để tìm đoạn văn liên quan trong corpus đã cho. Thống kê của QReCC được báo cáo trong Bảng 8.

Bảng 8: Thống kê của các bộ dữ liệu học công cụ và tìm kiếm hội thoại.

| Bộ dữ liệu | #Mẫu Huấn luyện | #Mẫu Kiểm tra | #Corpus |
|------------|------------------|----------------|----------|
| ToolBench | 87322 | 100 | 10439 |
| QReCC | 29596 | 8209 | 54573064 |

A.5 Tóm tắt Dữ liệu Huấn luyện Đa nhiệm
Chúng tôi tóm tắt chi tiết bộ dữ liệu để huấn luyện trong Bảng 9. Đáng chú ý, chúng tôi lặp lại dữ liệu ToolBench trong mỗi epoch vì chúng tôi thấy retriever yêu cầu nhiều epoch hơn để hội tụ trên nhiệm vụ đơn lẻ này.

[Tiếp tục với các bảng và phần phụ lục còn lại...]

Bảng 7: Thông tin chi tiết về các bộ dữ liệu học trong ngữ cảnh.

[Đây là một bảng lớn với nhiều hàng dữ liệu về các bộ dữ liệu khác nhau, bao gồm tên bộ dữ liệu, danh mục, số mẫu huấn luyện/kiểm tra, metric và chiến lược đánh giá]

Bảng 9: Chi tiết bộ dữ liệu để huấn luyện.

| Nhiệm vụ | Bộ dữ liệu | #Mẫu Huấn luyện | Lặp lại | Chưng cất Ổn định | Nhiệt độ Phần thưởng |
|----------|------------|------------------|---------|-------------------|---------------------|
| Trả lời Câu hỏi | MSMARCO | 400870 | 1 | ✓ | 1 |
| | NQ | 58622 | 1 | | 1 |
| Học Trong ngữ cảnh | – | 591359 | 1 | ✓ | 1 |
| Hội thoại Dài | MSC | 48925 | 1 | ✓ | 0.1 |
| Mô hình Ngôn ngữ Tầm xa | Books3 | 10000 | 1 | ✓ | 0.1 |
| | Arxiv | 10000 | 1 | | 0.1 |
| | CodeParrot | 10000 | 1 | | 0.1 |
| Học Công cụ | ToolBench | 87322 | 2 | ✗ | n.a. |
| Tìm kiếm Hội thoại | QReCC | 29596 | 1 | ✗ | n.a. |
| Tổng cộng | n.a. | 1333911 | n.a. | n.a. | n.a. |

Bảng 10: Hướng dẫn cho mỗi nhiệm vụ.

| Nhiệm vụ | Hướng dẫn Đầu vào |
|----------|-------------------|
| Trả lời Câu hỏi | Truy vấn: Biểu diễn truy vấn này để truy xuất tài liệu liên quan: |
| | Khóa: Biểu diễn tài liệu này để truy xuất: |
| Học Trong ngữ cảnh | Truy vấn: Chuyển đổi ví dụ này thành vector để tìm ví dụ hữu ích: |
| | Khóa: Chuyển đổi ví dụ này thành vector để truy xuất: |
| Hội thoại Dài | Truy vấn: Nhúng cuộc hội thoại này để tìm các cuộc hội thoại lịch sử hữu ích: |
| | Khóa: Nhúng cuộc hội thoại lịch sử này để truy xuất: |
| Mô hình Ngôn ngữ Tầm xa | Truy vấn: Nhúng chunk văn bản này để tìm các chunk lịch sử hữu ích: |
| | Khóa: Nhúng chunk văn bản lịch sử này để truy xuất: |
| Học Công cụ | Truy vấn: Biến đổi yêu cầu người dùng này để lấy mô tả công cụ hữu ích: |
| | Khóa: Biến đổi mô tả công cụ này để truy xuất: |
| Tìm kiếm Hội thoại | Truy vấn: Mã hóa truy vấn và ngữ cảnh này để tìm kiếm đoạn văn liên quan: |
| | Khóa: Mã hóa đoạn văn này để truy xuất: |

Bảng 11: Cài đặt siêu tham số để huấn luyện.

| Tham số | Giá trị |
|---------|---------|
| #GPU | 8×A100 (40G) |
| #Mẫu Âm Cứng | 7 |
| Kích thước Lô Mỗi GPU | 100 |
| Optimizer | AdamW |
| Tỷ lệ Học | 5e-6 |
| Weight Decay | 0.01 |
| Scheduler | Linear với Warm Up 0.2 |
| Bước Tối đa | 10000 |
| Gradient Checkpointing | ✓ |

B CHI TIẾT TRIỂN KHAI

B.1 Hướng dẫn
Chúng tôi sử dụng các hướng dẫn đa dạng để phân biệt các nhiệm vụ khác nhau cho retriever. Các hướng dẫn được sử dụng cho mỗi nhiệm vụ được thể hiện trong Bảng 10.

B.2 Cài đặt Huấn luyện
Các cài đặt siêu tham số để huấn luyện LLM-Embedder được báo cáo trong Bảng 11. Để đánh giá, chúng tôi sử dụng Flat index từ Faiss [35] khi cần truy xuất từ một corpus bên ngoài. Chúng tôi sẽ phát hành mã của chúng tôi khi bài báo được chấp nhận.

C TÁC ĐỘNG CỦA LLM-EMBEDDER ĐỐI VỚI CÁC LLM KHÁC NHAU
Chúng tôi đánh giá tác động của LLM-Embedder đối với các LLM khác nhau để xác thực khả năng tổng quát hóa của nó. Cụ thể, chúng tôi sử dụng Aquila-7B-Chat [1], Qwen-7B-Chat [6], Baichuan2-7B-Chat [9], và Llama-2-13B-Chat [78]. Kết quả được thể hiện trong Bảng 12. Cụ thể, chúng tôi so sánh hai baseline: None, nơi LLM được sử dụng riêng lẻ mà không có tăng cường truy xuất; BGE, nơi LLM được tăng cường với kiến thức, ví dụ và bộ nhớ được truy xuất (được giới thiệu trong Phụ lục A). Chúng tôi báo cáo độ chính xác trung bình cho MMLU, độ chính xác cho PopQA, điểm trung bình cho học trong ngữ cảnh, và perplexity cho cả Multi-Session Chat và Arxiv. Lưu ý rằng chúng tôi không nhân bản đánh giá của học công cụ và tìm kiếm hội thoại vì hiệu suất của chúng được đo trực tiếp bằng các metric truy xuất.

Bảng 12: Tác động của LLM-Embedder đối với các LLM khác nhau.

| LLM | Retriever | MMLU | PopQA | ICL | MSC | Arxiv |
|-----|-----------|------|-------|-----|-----|-------|
| Llama-2-7B-Chat | None | 0.4599 | 0.2061 | 0.4645 | 19.3501 | 3.7647 |
| | BGE | 0.4896 | 0.4491 | 0.5974 | 14.2943 | 3.2912 |
| | LLM-Embedder | 0.4903 | 0.5052 | 0.6268 | 13.4832 | 3.2322 |
| Aquila-7B-Chat | None | 0.4499 | 0.2028 | 0.5145 | 16.0108 | 3.1204 |
| | BGE | 0.4832 | 0.3982 | 0.5732 | 14.1843 | 2.7914 |
| | LLM-Embedder | 0.4847 | 0.4405 | 0.5903 | 14.1836 | 2.7351 |
| Qwen-7B-Chat | None | 0.5561 | 0.2393 | 0.5346 | 21.0466 | 2.7888 |
| | BGE | 0.5787 | 0.4447 | 0.6329 | 16.2064 | 2.5165 |
| | LLM-Embedder | 0.5762 | 0.4782 | 0.6457 | 15.4524 | 2.4824 |
| Baichuan2-7B-Chat | None | 0.5226 | 0.2356 | 0.4907 | 18.9711 | 2.7510 |
| | BGE | 0.5534 | 0.4407 | 0.5960 | 16.0759 | 2.4440 |
| | LLM-Embedder | 0.5511 | 0.4848 | 0.6179 | 15.5890 | 2.4131 |
| Llama-2-13B-Chat | None | 0.5386 | 0.2886 | 0.4607 | 14.7334 | 3.2357 |
| | BGE | 0.5603 | 0.4595 | 0.6196 | 11.6875 | 2.9036 |
| | LLM-Embedder | 0.5580 | 0.5026 | 0.6439 | 11.5384 | 2.8540 |

Chúng ta có thể quan sát thấy rằng các kết luận của chúng tôi trong Phần 3.2.2 vẫn đúng. Trước hết, truy xuất từ thế giới bên ngoài có lợi cho hiệu suất của LLM trong tất cả bốn kịch bản, vì hiệu suất của LLM thuần túy (tức là None) kém hơn so với LLM được tăng cường truy xuất (BGE và LLM-Embedder). Bên cạnh đó, LLM-Embedder được đề xuất của chúng tôi có thể tổng quát hóa tốt và duy trì sự vượt trội so với BGE trên hầu hết các bộ dữ liệu (đặc biệt là PopQA và ICL). Một ngoại lệ là MMLU, nơi LLM-Embedder bị BGE vượt trội một chút khi sử dụng Qwen, Baichuan và Llama-2-13B. Có vẻ như các LLM khác nhau sử dụng cùng một kiến thức theo những cách khác nhau, từ đó thu được kết quả hơi khác nhau.
