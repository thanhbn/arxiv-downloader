# 2308.13259.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2308.13259.pdf
# File size: 869908 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for
Knowledge-intensive Question Answering
Keheng Wang*1, Feiyu Duan*1, Sirui Wang3, Peiguang Li3, Yunsen Xian3, Chuantao Yin1,
Wenge Rong2, Zhang Xiong2
1Sino-French Engineering School, Beihang University
2School of Computer Science and Engineering, Beihang University
3Meituan Inc., Beijing, China
wkh9575638@buaa.edu.cn, duanfeiyu@buaa.edu.cn, wangsirui@meituan.com, lipeiguang@meituan.com,
xianyunsen@meituan.com, chuantao.yin@buaa.edu.cn, w.rong@buaa.edu.cn, xiongz@buaa.edu.cn
Abstract
Equipped with Chain-of-Thought (CoT), Large language
models (LLMs) have shown impressive reasoning ability in
various downstream tasks. Even so, suffering from hallucina-
tions and the inability to access external knowledge, LLMs
often come with incorrect or unfaithful intermediate reason-
ing steps, especially in the context of answering knowledge-
intensive tasks such as KBQA. To alleviate this issue, we
propose a framework called Knowledge-Driven Chain-of-
Thought (KD-CoT) to verify and modify reasoning traces in
CoT via interaction with external knowledge, and thus over-
come the hallucinations and error propagation. Concretely,
we formulate the CoT rationale process of LLMs into a struc-
tured multi-round QA format. In each round, LLMs inter-
act with a QA system that retrieves external knowledge and
produce faithful reasoning traces based on retrieved precise
answers. The structured CoT reasoning of LLMs is facili-
tated by our developed KBQA CoT collection, which serves
as in-context learning demonstrations and can also be uti-
lized as feedback augmentation to train a robust retriever.
Extensive experiments on WebQSP and ComplexWebQues-
tion datasets demonstrate the effectiveness of proposed KD-
CoT in task-solving reasoning generation, which outperforms
the vanilla CoT ICL with an absolute success rate of 8.0%
and 5.1%. Furthermore, our proposed feedback-augmented
retriever outperforms the state-of-the-art baselines for re-
trieving knowledge, achieving significant improvement in Hit
and recall performance. Our code and data are released on
https://github.com/AdelWang/KD-CoT/tree/main.
Introduction
Large language models (LLMs) pre-trained on massive lan-
guage corpora have shown impressive performance in vari-
ous NLP tasks (Brown et al. 2020; Du et al. 2022; Touvron
et al. 2023a). The ability of LLMs can be further unleashed
through in-context learning conditioning on a few concate-
nated demonstrations without task-specific training or fine-
tuning. Recent works have explored LLMs’ reasoning abil-
ity to tackle complex reasoning problems through prompting
(Wei et al. 2023; Zhou et al. 2023) and decoding (Wang et al.
2023b).
Despite advancements, LLMs still encounter halluci-
nations or lack of knowledge while solving knowledge-
*These authors contributed equally.
 
Question:  What is the daughter of the actor that played the character Xania?
 
Thought 1: I need to ﬁnd out who played Xania and then determine the name of
their daughter. 
Action 1: Question[Who played Xania?] 
Answer 1: I'm sorry, I could not ﬁnd any information on a character named
Xania.
 
......
 
Action 5: Finish[I'm sorry, I could not ﬁnd an answer to your question.]
 
Question:  Who is the head coach of the team that won 2009 AFC Championship
Game?
 
Thought 1:  I need to ﬁnd out which team won the 2009 AFC Championship
Game and then determine their head coach.
Action 1:  Question[Which team won the 2009 AFC Championship Game?]
Answer 1:  Indianapolis Colts
 
Thought 2: The Indianapolis Colts won the 2009 AFC Championship Game. I
now need to ﬁnd out who their head coach is.
......
 
Action 4:  Finish[ Jim Caldwell ]Failures of LLMFigure 1: LLMs suffer from hallucination or inability to an-
swer sub-questions while solving question answering tasks
that require encyclopedic knowledge, resulting in erroneous
subsequent reasoning and final answer. We highlight the er-
rors with red blocks.
intensive tasks. As shown in Figure 1, both these failures
will lead to erroneous subsequent reasoning steps and in-
correct final answers. Previous work prompts LLMs to gen-
erate structured Chain-of-Thought (CoT) by searching rele-
vant information from the Web (Yao et al. 2023), or verifies
the intermediate reasoning through an additional verification
system and returns the input to LLMs to re-generate the ra-
tionale (Wang et al. 2023a). However, the problem of hal-
lucinations in complex multi-hop problem scenarios is still
understudied.
A naive method is to directly input numerous contextual
knowledge into LLMs, converting the question answering
task into reading comprehension. However, ensuring com-
prehensive knowledge coverage necessitates a large amountarXiv:2308.13259v2  [cs.CL]  28 Oct 2023

--- PAGE 2 ---
of context, making it difficult for LLMs to fully understand
(Liu et al. 2023a). Considering that current state-of-the-
art methods leverage a retrieve-then-read pipeline that re-
trieves external knowledge and returns a knowledge-related
answer for solving knowledge-intensive Question Answer-
ing tasks, we can address the aforementioned issue by apply-
ing such a paradigm. Recent work has either enhanced the
retriever’s capability to retrieve external knowledge (Izacard
et al. 2021; Chuang et al. 2023) or improved the reader’s
ability to extract answers from the retrieved knowledge (Yu
et al. 2023, 2022). Other works focus on multi-hop Knowl-
edge Base Question Answering (KBQA) by leveraging in-
termediate supervision signals (He et al. 2021) or decom-
posing the question into several sub-questions (Sun et al.
2020; Khot et al. 2021). Nevertheless, few works leverage
the understanding and reasoning capabilities of LLMs to ad-
dress complex multi-hop KBQA tasks, as well as investigate
the problem of model hallucination for encyclopedic knowl-
edge. Moreover, there is a dearth study on how to boost faith-
ful reasoning by leveraging external knowledge to improve
the intermediate reasoning traces.
To address the above issue, we propose a Knowledge-
Driven Chain-of-Thought (KD-CoT), an interactive frame-
work that utilizes a QA system to access external knowl-
edge and provide high-quality answers to LLMs for solving
knowledge-intensive KBQA tasks. KD-CoT is designed to
encourage LLMs to generate verbal reasoning traces, facili-
tating dynamic reasoning that can verify and adjust interme-
diate reasoning steps by accessing external knowledge. We
also create a KBQA CoT collection that can be applied to
perform ICL, and train a robust retriever utilizing the con-
structed collection.
Our main contributions can be summarized as follows:
• We present a KBQA CoT collection by prompting LLMs,
which could be used for fine-tuning smaller LMs to ac-
quire CoT reasoning ability and be applied to perform
ICL.
• We propose a retriever-reader-verifier QA system to ac-
cess external knowledge and interact with LLM. We
leverage the constructed CoT collection as feedback aug-
mentation to train a robust retriever, achieving significant
improvement in Hit scores on WebQSP and CWQ.
• We introduce KD-CoT, a Knowledge-Driven Chain-of-
Thought framework to improve the reasoning perfor-
mance of large language models. Experimental results
demonstrate the effectiveness of our proposed frame-
work, achieving 8.0 and 5.1 Hit@1 improvement on We-
bQSP and CWQ compared to the vanilla ICL method.
Methodology
In this section, we first present the procedure for construct-
ing the CoT Collection. Then we introduce the Knowledge-
Driven CoT framework, which encompasses the implemen-
tation of the interaction and the training of the external QA
system.
 
Question:  Who are the children of Ike and the woman who composed Nutbush
City Limits?
Hint:  answer: ["Ronnie T urner", "Raymond Craig T urner"], composition_answer:
["tina turner"]
 
Thought 1:  I need to ﬁnd out who Ike is and who composed Nutbush City Limits,
and then determine their children.
Action 1:  Question[ Who is Ike?]
Answer 1:  Ike T urner
 
Thought 2:  Ike T urner is the person being referred to. I now need to ﬁnd out who
composed Nutbush City Limits.
Action 2:  Question[ Who composed Nutbush City Limits? ]
Answer 2:  Tina T urner
 
Thought 3:  Tina T urner composed Nutbush City Limits. I now need to ﬁnd out
who the children of Ike and T ina T urner are.
Action 3:  Multi_Answer_Question[ Who are the children of Ike and T ina T urner? ]
Answer 3:  Ronnie T urner and Raymond Craig T urner
 
Thought 4:  I found the children of Ike and T ina T urner: Ronnie T urner and
Raymond Craig T urner .
Action 4:  Finish[ Ronnie T urner and Raymond Craig T urner .] CoT of CWQFigure 2: Illustration of manually written demonstrations.
Blue and Green parts denote the sub-question and sub-
answer generated by LLM, respectively.
CoT Collection
The rationale data for CoT fine-tuning has shown great
value, while constructing such high-quality rationales is
quite challenging due to the difficulty in gathering human-
authored rationales (Kim et al. 2023) and the hallucinations
of large models (Bang et al. 2023). Here we provide a de-
tailed description of how we construct knowledge-intensive
rationales with the help of LLMs.
It has been proven that LLMs perform ICL conditioning
on demonstrations have a better understanding of the task
and generate more accurate responses (Brown et al. 2020).
Inspired by this, we assign the demonstrations every time
request LLMs. To obtain the demand demonstrations, we
first manually write several accurate CoT demonstrations as
the anchor set, and then we employ an iterative algorithm
to construct our full collection. In each iteration, we choose
the candidate in the current collection that holds the high-
est cosine similarity with the question in the training set to
serve as the demonstration. We utilize RoBERTa (Reimers
and Gurevych 2019)1to embed questions. Next, we request
ChatGPT2to generate the structured CoT, and append gen-
erated results ”Finish” with the correct answer to the collec-
tion. The construction details are referred to Algorithm 1.
Notably, we observe that concatenating the ground truth an-
swer and the composition answer (if have) as ”Hint” before
the rationale can greatly improve the efficiency of collection
construction, so the final demonstration is presented in the
format of <Question, Hint, CoT >as illustrated in Figure 2.
1Downloaded from Sentence Transformers; Roberta-large-nli-
stsb
2OpenAI gpt-3.5-turbo

--- PAGE 3 ---
 
Question:  What was the name of music producer of The Don
Killuminati: The 7 Day Theory in juice?
 
Thought 1:  I need to ﬁnd out which founder of New Y ork University held
a position in the 4th United States Congress. 
 
Action 1:  Multi_Answer_Question[music producer + The Don
Killuminati: The 7 Day Theory + Juice] 
Answer 1:  Tupac
 
Thought 2:  Tupac was the music producer of The Don Killuminati: The
7 Day Theory in Juice. Now I need to ﬁnd out his name in the movie. 
 
Action 2:  Question[What was T upac's character name in Juice?] 
Answer 2:  Bishop 
 
Thought 3:  Tupac's character name in Juice was Bishop. 
Action 3:  Finish[Bishop.] 
Question:  What was the name of music producer of The Don Killuminati:
The 7 Day Theory in juice?
 
Thought 1:  I need to ﬁnd out which founder of New York University held
a position in the 4th United States Congress. 
 
Action 1:  Multi_Answer_Question [music produ cer + The Don Killuminati:
The 7 Day Theory + Juice ] 
Answer 1:  There are two possible producers - QD III and Living Legend  
 
Thought 2: I have multiple answers. I need to ﬁnd out the name of the
producer who was also credited in Juice.  
Action 2:  Ques tion[Who was the music producer of The Don Killuminati:
The 7 Day Theory and credited in Juice?] 
Answer 2:  QD III 
 
Thought 3:  QD III was the music producer of The Don Killuminati: The 7
Day Theory and was also credited in Juice. 
Action 3:  Finish[QD III.]CoT
KB
Retriever
Reader
Veriﬁer
TupacCoT
Figure 3: The overall framework of Knowledge-Driven CoT, including a prompted large model and a QA system that accesses
external knowledge. By modifying sub-answers of intermediate questions, LLM can generate more faithful subsequent infer-
ence steps, which lead to correct final answers. Blue, Green, and Red blocks represent the sub-question fed to QA system,
correct/incorrect reasoning and answers, respectively
Algorithm 1: Construct CoT Collection
Require: Human-annotated demonstrations, Dh
Require: A fixed human-annotated instruction, I
Require: Question-Answer training set, Q, A
Require: Large language model, LLM
Require: Demonstration selection pool, P
P←Dh
iteration ←0
while Qis not empty and iteration < 5do
Demons ←SimilaritySelection (Q, P )
Inputs ←Concat (I, Demons, Q )
Outputs ←LLM (inputs )
Constructed ←AnswerMatch (outputs, A )
P←Extend (P, Constructed )
Q←Q\A
iteration ←iteration + 1
end while
return P
Knowledge-Driven CoT
Due to hallucinations and the inability to access external
knowledge, LLMs struggle to generate faithful reasoning
steps for knowledge-intensive QA tasks. To address this is-
sue, we propose Knowledge-Driven Chain-of-Thought Rea-
soning (KD-CoT), which incorporates a QA system to in-
teract with LLMs. The overall framework of our proposed
KD-CoT is shown in Figure 3.
For each question in the test set, we select the instance
with the highest cosine similarity from the collection, and
utilize its rationale as the demonstration to perform one-
shot ICL3. Then the extracted intermediate sub-question is
taken as the input of the QA system to perform interaction,
3Increasing the number of ICL demonstrations will improve the
performance of LLMs, but also much more costly. We only con-which is comprised of a retrieve-then-read pipeline and an
answer verifier. The former module retrieves external knowl-
edge and proposes a candidate answer based on the retrieved
information, while the latter chooses between the original
sub-answers generated by LLM and the proposed candidate
answer. We repeat the above interaction until the CoT is fin-
ished. Note that our motivation is to supervise the interme-
diate reasoning of LLM and not to alter the ultimate answer.
Therefore, we restrict our interaction with the external QA
system to sub-questions leading up to the final Action . For
example, Action 4 ”Finish” the entire CoT in Figure 2, we
solely verify the sub-answers of Action 1 ,2and3iteratively.
To ensure the accuracy of the intermediate reasoning
steps, it is crucial to have a strong QA system that can effec-
tively access external knowledge and generate highly precise
answers. We then introduce how to train our retrieve-then-
read pipeline and the verifier.
KB Linearization We aim to interact with both structural
(KBs) and unstructured (Wikipedia) external knowledge.
However, directly retrieving information from KBs is non-
trivial due to its large scale and complication with semantics
and structure. To address this issue, we simply follow the
linearization method proposed in (Yu et al. 2023) to pro-
cess Freebase KB data (Bollacker et al. 2008) into unstruc-
tured text. Given a head entity, we extract its 1-hop subgraph
and concatenate the KB triplets with spaces, then group the
entire subgraph into a single passage. For example (music
recording, releases, Palavras de Guerra Ao Vivo) and (music
recording, artist, Ol ´ıvia Hime) will be processed into ”mu-
sic recording releases Palavras de Guerra Ao Vivo. music
recording artist Ol ´ıvia Hime”.
After pre-processing, we concatenate Wikipedia passages
with KB passages to perform knowledge retrieval.
Feedback-Augmented Retriever To align with previous
catenate a single demonstration for CoT-ICL if not specified.

--- PAGE 4 ---
 
Question:  Which is the correct answer for the question
Which state includes a university that sometimes goes by
the name USC and also includes a city named
Columbia?  Ukrainian hryvnia or South Carolina
 
Veriﬁer output:  South Carolina
 
Target output:  South Carolina
 
Question:  Which is the correct answer for the question What
country in Mediterranean that has Zonguldak
Province?  Maldives or South African
 
Veriﬁer output:  Neither, the correct answer should be Turkey
 
Target output:  TurkeyVeriﬁerFigure 4: Illustrations of input-output pairs of the verifier.
We highlight the correct answers with green blocks.
work (Oguz et al. 2022; Yu et al. 2023), we apply Dense Pas-
sage Retrieval (DPR) (Karpukhin et al. 2020) as the model
architecture of the retrieval system. To obtain a robust re-
triever, we propose to utilize the constructed CoT as feed-
back to identify relevant passages. Specifically, we extract
the last reasoning sub-question from the CoT rationale as
the query augmentation and concatenate it with the origi-
nal question. To identify positive and negative passages rel-
evant to the question, we further concatenate the augmented
query with the answers4, and use the BM25 algorithm to
extract the top 100 related passages. We identify passages
that contain entities present in both the query and answer
as positive examples, while passages that only contain the
answer or query entities are considered hard negatives. If
no co-occurrence passage is found, we follow the primary
settings in the original paper and use the passage contain-
ing only the answer as positive to ensure the recall rate of
the multi-answer question. We utilize Spacy5to recognize
named entities in the query. Note that the feedback of LLM
is only used for identifying positive/negative passages, we
use the original questions to train our DPR model.
Fuse-in-Decoder Reader For our reader, we use the main-
stream Fuse-in-Decoder architecture (Izacard and Grave
2021) to train a Transformer (Vaswani et al. 2017) model.
Specifically, given a question q and its top-N relevant pas-
sages P, the FiD reader first separately encodes each passage
pqiconcatenated with q:
Pi=Encoder (Concat [q, pqi])∈RL×H(1)
Where L, H represent sequence length and hidden size, re-
spectively. Then the token embeddings of all passages output
from the encoder are concatenated and fed to the decoder to
generate the final answer. Different from previous work, we
employ all answers as training targets instead of selecting
4Query mentioned below stands for <question, rationale ques-
tion>, and BM25 searches the relevant passages on <question, ra-
tionale question, answers >
5https://spacy.io/one randomly.
A=Decoder (Concat [P1, P2, ..., P N]) (2)
Verifier We train a Llama2-7b (Touvron et al. 2023b)
with Parameter-Efficient-Fine-Tuning (PEFT) on the origi-
nal KBQA training set as our verifier. During inference, the
model takes the original sub-answers generated by LLM and
the candidate answers generated by the retrieve-then-read
pipeline as input, and outputs its preferred one. If neither
answer is selected, the verifier will generate a new answer.
We illustrate several examples in Figure 4.
If not specified, greedy decoding is used for Reader and
Verifier during inference.
Experiment
Experiment Settings
For our main experiment, we use ChatGPT as our backbone
”thinker” to interact with an external QA system, which is
denoted as ”LLM” in the subsequent sections of this pa-
per. The QA system includes a BERT-base (Devlin et al.
2019) retriever, a T5-large (Raffel et al. 2020) reader, and a
Llama2-7b verifier fine-tuned with LoRA (Hu et al. 2021)6.
We prompt LLM to perform structured multi-round QA rea-
soning with demonstrations selected from our constructed
CoT collection.
We train our retriever on a merged dataset of WebQSP
and CWQ for saving the cost of embedding massive knowl-
edge and use the same DPR architecture in the original paper
(Karpukhin et al. 2020). The number of retrieved passages is
100 if not specified. The reader is also trained on the merged
dataset to effectively tackle both single-hop and multi-hop
question scenarios.
To show the effectiveness and correctness of our CoT col-
lection, we also conduct experiments that involve fine-tuning
smaller models on the constructed rationale data. We use
Flan-T5-3B (Chung et al. 2022) and T5-3B (Raffel et al.
2020) as the base models and compare the results with text-
to-text QA fine-tuning. To indicate the CoT paradigm that
generates both rationale and answers, we incorporate a trig-
ger phrase ”Let’s think step by step” into the sequence dur-
ing training and evaluation.
Dataset We evaluate KD-CoT on two KBQA datasets:
WebQSP (Yih et al. 2016) and ComplexWebQuestions
(CWQ) (Talmor and Berant 2018). We use the original
datasets to train our external QA system, and use the con-
structed CoT collection to apply ICL on ChatGPT. The data
statistics are shown in Table 1.
Evaluation metric Following previous work, we evaluate
our model based on metrics Hits@1 and F1, where Hits@1
focuses on the single top-ranked answer while F1 considers
coverage of all the answers. To account for the fact that it’s
difficult to extract the desired answers from LLM’s output,
we adjust our evaluation criteria. We deem the generated re-
sults to be correct if they contain the ground truth answer.
6We use the checkpoints downloaded from Huggingface.

--- PAGE 5 ---
# DataWebQSP CWQ
train test train test
original 3098 1639 27625 3519
CoT collection 2888 1639 26695 3519
Table 1: Data statistics of original datasets and our CoT
collections. After collection construction, we obtained 2888
and 26695 rationale data for WebQSP and CWQ, respec-
tively.
Method \DatasetWebQSP CWQ
Hit@1 F1 Hit@1
UnikQA (Oguz et al. 2022) 79.1 - -
DeCAF (Yu et al. 2023) 80.7 77.1 67.0
DeCAF w/o LF (Yu et al. 2023) 74.2 49.5 47.9
OurRetrieve −then−read 73.7 50.2 50.5
LLM Retrieval 4−passages 52.4 38.2 26.9
LLM QA pairs 4−shot 53.2 39.2 42.2
LLM CoT fixed 50.3 37.8 34.0
LLM QA−CoT fixed 56.6 42.5 42.4
LLM QA−CoT selected 60.6 47.8 50.6
KD-CoT 68.6 52.5 55.7
KD-CoT w/o Retrieve −then−read 66.8 49.4 49.2
KD-CoT w/o V erifier 59.9 47.6 49.2
Table 2: Experimental results on WebQSP and CWQ. KD-
CoT significantly outperforms the vanilla CoT ICL.
Konwledge-Driven CoT Results
Table 2 reports the performance of our proposed KD-CoT.
The results show that KD-CoT outperforms vanilla CoT
ICL by a significant margin of 8.0 and 5.1 points on We-
bQSP and CWQ, respectively. This highlights the effective-
ness of interacting with the external QA system, as it enables
the LLM to generate more accurate intermediate reasoning
steps, leading to more precise final answers.
To further demonstrate that our process of verifying and
correcting sub-answers can lead to faithful reasoning so as
to rectify previously incorrect responses, we tally the alter-
ations in the count of correct and incorrect answers before
and after undergoing our interactive framework. The results
are shown in Figure 5. On WebQSP and CWQ, we cor-
rect 13.5% and 10.6% of questions that were incorrectly an-
swered previously, while only 5.8% and 5.4% are modified
to be incorrect.
Despite its efficiency in extracting sub-questions to inter-
act with the external QA system, ReAct format CoT also
reduces its flexibility in formulating reasoning steps due to
the structural constraint (Yao et al. 2023). Once the output
generated is inadequately structured and unable to extract
sub-questions, we consider it a failure in answering. Con-
sequently, the capability of LLM might be underestimated.
Retrieval Results
We evaluate the effectiveness of our feedback-augmented
DPR in Table 3. It can be seen that our proposed FBA-DPR
significantly outperforms previous results in Yu et al. (2023)
Incorrect Correct
WebQSP0102030405060Quantity percentageincorrect correct
Incorrect Correct
CWQ0102030405060Quantity percentageincorrect correctFigure 5: Quantity percentage of questions answered cor-
rectly/incorrectly by the LLM. The horizontal axis repre-
sents the state before passing through the interaction frame-
work. Red and Green blocks represent the proportion of
questions answered correctly/incorrectly after the entire in-
teraction.
MethodWebQSP CWQ
H / R@20 H / R@100 H / R @20 H / R@100
BM25 66.8 / 49.8 83.8 / 69.8 47.8 / 42.7 65.4 / 59.3
DPR - / - 91.6 / 80.6 - / - 71.4 / 65.6
FBA-DPR 89.0 / 75.6 95.4 / 88.4 68.7 / 62.5 81.3 / 76.5
w/o wiki 88.9 / 74.2 94.8 / 86.3 65.0 / 58.5 77.8 / 72.6
Table 3: Retrieval results on WebQSP and CWQ. H@N and
R@N stand for the answer hits rate and recall rate of Top-
N retrieved passages, respectively. DPR results are copied
from Yu et al. (2023), BM25 and FBA-DPR results are ob-
tained in our setting.
on both WebQSP and CWQ, achieving 3.8 and 7.8 points of
improvement on Hit@100.
CoT Fine-tuning Results
We conduct experiments under two different settings: Direct
Fine-tuning andCoT Fine-tuning . For Direct Fine-tuning
we train the model on the original QA pairs, which takes
the questions as input and directly generates the answers.
ForCoT Fine-tuning we use the trigger ”Let’s think step by
step” concatenated with the questions, and train the model to
output rationales and final answers. The results are reported
in Table 4.
Model \DatasetWebQSP CWQ
Direct CoT Direct CoT
T5-3B 40.8 41.9 39.4 38.6
FlanT5-3B 46.1 47.0 50.5 43.8
Llama2-7B-LoRA 63.8 64.1 48.4 45.1
Table 4: Comparison of Direct Fine-tuning and CoT Fine-
tuning. Hits@1 score is reported.
We observe that fine-tuning LMs with CoT rationales
slightly outperforms Direct Fine-tuning for solving simpler
questions. In complex multi-hop question scenarios, CoT

--- PAGE 6 ---
0 1 2 3
Iteration6264666870Hit@1 score
Performance after each iteration on WebQSP
DPR+FiD
BM25+FiD
0 1 2 3
Iteration5152535455Hit@1 score
Performance after each iteration on CWQ
DPR+FiD
BM25+FiD(a) LLM performance after each iteration of interaction. The highest performance is achieved in the first iteration for WebQSP and the last
iteration for CWQ, as WebQSP is primarily comprised of single-hop questions, whereas CWQ contains more complex multi-hop questions.
1 2 3
WebQSP Iteration0.00.10.20.30.40.50.60.7Proportion of sources of answers
200400600800100012001400
Number of processed instance
FiD-reader Llama-verifer ChatGPT T otal Nums
1 2 3
CWQ Iteration0.00.10.20.30.40.50.60.7Proportion of sources of answers
500100015002000250030003500
Number of processed instance
FiD-reader Llama-verifer ChatGPT T otal Nums
(b) Answer source during each iteration. In most cases, the verifier prefers sub-answers output by ChatGPT, about half of the sub-answers are
modified by the external QA system in each iteration.
Figure 6: a) LLM performance after each iteration of interaction. b) Answer source during each iteration.
fine-tuning brings negative gains. This might be because 1)
The reasoning procedure of the original LM differs from that
of the CoT collection. Fine-tuning the LM may potentially
disrupt the original knowledge, resulting in a degradation
in performance; 2) LMs still struggle to generate faithful
multi-step reasoning even fine-tuned with CoT when solv-
ing knowledge-intensive tasks. However, the models still
achieve competitive results with Direct Fine-tuning, demon-
strating the correctness of our constructed collection.
Analysis & Ablation Study
This section aims to address the following question through
analysis and ablation experiments.
Benefits of structured CoT and CoT collection?
To discuss the benefits of rationale in the form of structured
multi-round QA, and to highlight the significance of the CoT
collection we construct in performing ICL, we evaluate the
following model settings, with the name corresponding to
the rows in Table 2:
• LLM Retrieval 4−passages We roughly concatenate thetop-ranked retrieved passages with the question as input
and instruct LLM to answer the question.
• LLM QA pairs 4−shotWe utilize other QA pairs with the
highest cosine similarity to the target question as the
demonstrations to perform ICL.
• LLM CoT fixed We manually design unstructured ratio-
nales aligned with the content of our structured CoT,
and utilize them as demonstrations to prompt LLM.
The in-context demonstration is selected within human-
annotated unstructured rationales.
• LLM QA−CoT fixed The in-context demonstration is se-
lected within human-annotated structured rationales.
• LLM QA−CoT selected The in-context demonstration is
selected within our constructed CoT collection.
To align with one-shot CoT ICL, we restrict the input length
and concatenate only 4 passages/QA pairs as the context that
fed into LLM. Experimental results are shown in Table 2.
We observe that LLM achieves superior performance when
utilizing structured rationale as the demonstration, outper-

--- PAGE 7 ---
forming other ICL methods. This suggests that our proposed
multi-round QA format rationale is more effective in un-
leashing LLM’s reasoning capability. Directly concatenating
the retrieved knowledge does not have a positive contribu-
tion to the model’s ability, especially in complex multi-hop
question scenarios, and it performs the worst among all ICL
methods. By employing our constructed CoT collection, we
further improve the LLM’s ability, highlighting the effec-
tiveness and necessity of the collection construction.
Benefits of QA system?
To assess the effectiveness of the QA system, we con-
duct two supplementary experiments where we remove the
retrieve-then-read pipeline and the verifier separately. The
results are reported in Table 2. We observe that the per-
formance degrades when the retrieve-then-read pipeline is
removed, showing the importance of accessing external
knowledge for precise sub-answers generation. Moreover,
the performance without the verifier underperforms our KD-
CoT setting, showing that in certain cases the sub-answers
generated by the LLM are superior. Further combining the
output of the reader and LLM to generate better answers is
important for improving performance.
We further investigate the performance gain after each it-
eration and count the source of the modified answer. The
results are shown in Figure 6. As can be seen in Figure
6(a), the highest performance of LLM is achieved in the
first iteration for WebQSP and the last iteration for CWQ,
as WebQSP is primarily comprised of single-hop questions,
whereas CWQ contains more complex multi-hop questions.
We also observe that LLM tends to produce redundant infer-
ence steps despite being able to answer questions within two
hops of reasoning. This leads to the necessity of second and
third iterations to terminate the CoT while solving WebQSP
questions. An extra Halter (Creswell and Shanahan 2022) to
determine whether the current reasoning step can answer the
questions can be a possible method for solving this issue. As
the number of iterations increases, the performance on the
CWQ dataset also improves. This suggests that our interac-
tion framework can assist the model in better reasoning for
complex multi-hop questions.
Figure 6(b) shows the source of modified answers. In most
cases, the verifier will keep the original sub-answers gener-
ated by ChatGPT, about half of the sub-answers are modified
and fed to the next iteration. This implies that a robust reader
capable of producing varied and accurate answers is crucial
in fully unleashing the potential of LLM.
Related Work
Chain-of-thought Prompting
With the massive increase of model parameters and training
data, models begin to emerge powerful reasoning capabili-
ties (Wei et al. 2022). Inspired by this performance break-
through, (Wei et al. 2023) proposes a gradient-free method
of chain-of-thought (CoT) prompting, which allows mod-
els to give reliable answers after thinking and interpreting.
Based on this research, several studies have been conducted
to improve the effectiveness. For example, some studies fo-
cus on how to make LLM generate more accurate and re-liable chains of thought (He, Zhang, and Roth 2022; Wang
et al. 2023b; Lyu et al. 2023), while some works investi-
gate more efficient ways of generating chains of thought to
unleash the potential of LLM reasoning (Creswell, Shana-
han, and Higgins 2022; Zhou et al. 2023; Jin and Lu 2023).
As LLMs are confined to the knowledge learned from the
training corpus, extensive efforts have been made recently
to facilitate LLMs in dynamically interacting with the real
world to obtain the information required for model reason-
ing. (Yao et al. 2023; Press et al. 2023; Liu et al. 2023b; Peng
et al. 2023). For instance, ReAct (Yao et al. 2023) suggests
utilizing a blend of reasoning and action to allow LLMs to
acquire background knowledge from Wikipedia and solve
problems automatically, thus mitigating the hallucination is-
sue of LLMs. However, they simply concatenate the state-
ments found on the wiki page to the end of the sub-question
without performing more accurate retrieval. In contrast, we
make use of a more advanced retriever-reader pipeline to fur-
nish the model with more accurate and targeted knowledge.
Knowledge Base Question Answering
Knowledge base question answering (KBQA) has been a
popular research topic in recent years, and a series of ap-
proaches have been suggested to enhance the efficiency of
Knowledge base QA systems. The retriever-reader pipeline
is a commonly employed technique, where the retriever ex-
tracts the most pertinent corpus from a knowledge base
based on the question, and the reader produces the ulti-
mate answer by utilizing the question and the corpus re-
trieved. Hence, certain studies concentrate on enhancing the
retriever’s efficiency (Karpukhin et al. 2020; Izacard et al.
2021; Chuang et al. 2023), whereas others prioritize the
reader’s performance (Izacard and Grave 2021; Yu et al.
2022). Additionally, there are studies that delve into the in-
corporation of structured knowledge from the knowledge
base into the question answering system, such as (Oguz
et al. 2022; Yu et al. 2023), while the latter also utilizes
logical form to improve the accuracy of answer generation.
Although previous works are limited to small models with
weaker reasoning capabilities, they have formed an efficient
system of information retrieval, condensing the extracted
knowledge corpus into brief statements. Therefore, our re-
search integrates this system with LLMs to offer necessary
knowledge and aid the LLMs in producing more dependable
chains of thought.
Before LLMs, several works proposed methods for
decomposing multi-hop questions into single-hop sub-
questions (Min et al. 2019; Sun et al. 2020; Khot et al. 2021).
Some rule-based methods generate unnatural sub-questions,
while other methods are constrained by the model’s capacity.
In contrast, our approach leverages CoT to induce LLMs to
decompose complex multi-hop questions into sub-questions
that are more comprehensible and logically coherent.
Conclusion
In this paper, we investigate the faithful reasoning of
large language models on knowledge-intensive KBQA
tasks. We propose a Knowledge-Driven Chain-of-Thought

--- PAGE 8 ---
framework to improve the reasoning performance of large
language models. Through experiments on knowledge-
intensive KBQA tasks, we show that KD-CoT leads to supe-
rior performance with interpretable inference steps. We also
present a CoT collection on the KBQA datasets that can be
utilized for CoT fine-tuning and few-shots in-context learn-
ing. Additionally, we investigate a new training approach to
develop a robust retriever that can efficiently access exter-
nal knowledge, which results in a substantial improvement
in the Hit scores for retrieved knowledge.
Discussion & Future work
Despite the fact that our method can efficiently access exter-
nal knowledge and correct the sub-answers generated by the
large language model, the final performance of LLM still
leaves behind the current SOTA. This could be caused by:
1) The inability of the QA system to generate precise an-
swers for all sub-questions, as our simply designed reader
achieves only 73.7 Hit@1 on the WebQSP dataset. 2) The
reasoning hallucination. LLM can still hallucinate reasoning
despite our corrections to precise answers. Therefore, future
work can focus on training a more robust reader or super-
vising both the reasoning questions and answers. Addition-
ally, although we use greedy decoding for the QA system to
generate candidate answers, our method is still costly as the
entire framework contains an extra LLM and a verifier. In
future work, more efficient techniques such as searching on
the web or filtering retrieved knowledge can be utilized to
further decrease the cost of KD-CoT.
References
Bang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,
B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V .; Xu,
Y .; and Fung, P. 2023. A Multitask, Multilingual, Multi-
modal Evaluation of ChatGPT on Reasoning, Hallucination,
and Interactivity. arXiv:2302.04023.
Bollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Tay-
lor, J. 2008. Freebase: A Collaboratively Created Graph
Database for Structuring Human Knowledge. In Proceed-
ings of the 2008 ACM SIGMOD International Conference
on Management of Data , SIGMOD ’08, 1247–1250. New
York, NY , USA: Association for Computing Machinery.
ISBN 9781605581026.
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,
T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,
C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,
A.; Sutskever, I.; and Amodei, D. 2020. Language Models
are Few-Shot Learners. arXiv:2005.14165.
Chuang, Y .-S.; Fang, W.; Li, S.-W.; tau Yih, W.; and Glass,
J. 2023. Expand, Rerank, and Retrieve: Query Reranking for
Open-Domain Question Answering. arXiv:2305.17080.
Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-
dus, W.; Li, Y .; Wang, X.; Dehghani, M.; Brahma, S.; Web-
son, A.; Gu, S. S.; Dai, Z.; Suzgun, M.; Chen, X.; Chowd-hery, A.; Castro-Ros, A.; Pellat, M.; Robinson, K.; Val-
ter, D.; Narang, S.; Mishra, G.; Yu, A.; Zhao, V .; Huang,
Y .; Dai, A.; Yu, H.; Petrov, S.; Chi, E. H.; Dean, J.; De-
vlin, J.; Roberts, A.; Zhou, D.; Le, Q. V .; and Wei, J.
2022. Scaling Instruction-Finetuned Language Models.
arXiv:2210.11416.
Creswell, A.; and Shanahan, M. 2022. Faithful Reasoning
Using Large Language Models. arXiv:2208.14271.
Creswell, A.; Shanahan, M.; and Higgins, I. 2022. Selection-
Inference: Exploiting Large Language Models for Inter-
pretable Logical Reasoning. arXiv:2205.09712.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. arXiv:1810.04805.
Du, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
Tang, J. 2022. GLM: General Language Model Pretraining
with Autoregressive Blank Infilling. arXiv:2103.10360.
He, G.; Lan, Y .; Jiang, J.; Zhao, W. X.; and Wen, J.-R. 2021.
Improving Multi-hop Knowledge Base Question Answer-
ing by Learning Intermediate Supervision Signals. In Pro-
ceedings of the 14th ACM International Conference on Web
Search and Data Mining . ACM.
He, H.; Zhang, H.; and Roth, D. 2022. Rethinking
with Retrieval: Faithful Large Language Model Inference.
arXiv:2301.00303.
Hu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,
S.; Wang, L.; and Chen, W. 2021. LoRA: Low-Rank Adap-
tation of Large Language Models. arXiv:2106.09685.
Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski,
P.; Joulin, A.; and Grave, E. 2021. Unsupervised Dense In-
formation Retrieval with Contrastive Learning.
Izacard, G.; and Grave, E. 2021. Leveraging Passage Re-
trieval with Generative Models for Open Domain Question
Answering. arXiv:2007.01282.
Jin, Z.; and Lu, W. 2023. Tab-CoT: Zero-shot Tabular Chain
of Thought. arXiv:2305.17812.
Karpukhin, V .; O ˘guz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov,
S.; Chen, D.; and tau Yih, W. 2020. Dense Passage Retrieval
for Open-Domain Question Answering. arXiv:2004.04906.
Khot, T.; Khashabi, D.; Richardson, K.; Clark, P.; and Sab-
harwal, A. 2021. Text Modular Networks: Learning to
Decompose Tasks in the Language of Existing Models.
arXiv:2009.00751.
Kim, S.; Joo, S. J.; Jang, Y .; Chae, H.; and Yeo, J. 2023.
CoTEVer: Chain of Thought Prompting Annotation Toolkit
for Explanation Verification. arXiv:2303.03628.
Liu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua, M.;
Petroni, F.; and Liang, P. 2023a. Lost in the Middle: How
Language Models Use Long Contexts. arXiv:2307.03172.
Liu, X.; Lai, H.; Yu, H.; Xu, Y .; Zeng, A.; Du, Z.; Zhang,
P.; Dong, Y .; and Tang, J. 2023b. WebGLM: Towards An
Efficient Web-Enhanced Question Answering System with
Human Preferences. arXiv:2306.07906.
Lyu, Q.; Havaldar, S.; Stein, A.; Zhang, L.; Rao, D.; Wong,
E.; Apidianaki, M.; and Callison-Burch, C. 2023. Faithful
Chain-of-Thought Reasoning. arXiv:2301.13379.

--- PAGE 9 ---
Min, S.; Zhong, V .; Zettlemoyer, L.; and Hajishirzi, H. 2019.
Multi-hop Reading Comprehension through Question De-
composition and Rescoring. arXiv:1906.02916.
Oguz, B.; Chen, X.; Karpukhin, V .; Peshterliev, S.;
Okhonko, D.; Schlichtkrull, M.; Gupta, S.; Mehdad, Y .; and
Yih, S. 2022. UniK-QA: Unified Representations of Struc-
tured and Unstructured Knowledge for Open-Domain Ques-
tion Answering. arXiv:2012.14610.
Peng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y .; Hu, Y .;
Huang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.
Check Your Facts and Try Again: Improving Large Lan-
guage Models with External Knowledge and Automated
Feedback. arXiv:2302.12813.
Press, O.; Zhang, M.; Min, S.; Schmidt, L.; Smith, N. A.;
and Lewis, M. 2023. Measuring and Narrowing the Compo-
sitionality Gap in Language Models. arXiv:2210.03350.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring
the Limits of Transfer Learning with a Unified Text-to-Text
Transformer. arXiv:1910.10683.
Reimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-
tence Embeddings using Siamese BERT-Networks. In Pro-
ceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing . Association for Computa-
tional Linguistics.
Sun, Y .; Zhang, L.; Cheng, G.; and Qu, Y . 2020. SPARQA:
Skeleton-based Semantic Parsing for Complex Questions
over Knowledge Bases. arXiv:2003.13956.
Talmor, A.; and Berant, J. 2018. The Web as a Knowledge-
Base for Answering Complex Questions. In Proceedings
of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers) , 641–651.
New Orleans, Louisiana: Association for Computational
Linguistics.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;
Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lam-
ple, G. 2023a. LLaMA: Open and Efficient Foundation Lan-
guage Models. arXiv:2302.13971.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-
rull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;
Gao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,
S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;
Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;
Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,
X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-
ton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;
Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,
B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;
Zarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-
driguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b.
Llama 2: Open Foundation and Fine-Tuned Chat Models.
arXiv:2307.09288.Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-
tention Is All You Need. arXiv:1706.03762.
Wang, J.; Sun, Q.; Chen, N.; Li, X.; and Gao, M.
2023a. Boosting Language Models Reasoning with Chain-
of-Knowledge Prompting. arXiv:2306.06427.
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,
S.; Chowdhery, A.; and Zhou, D. 2023b. Self-Consistency
Improves Chain of Thought Reasoning in Language Models.
arXiv:2203.11171.
Wei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;
Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Met-
zler, D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.;
Dean, J.; and Fedus, W. 2022. Emergent Abilities of Large
Language Models. arXiv:2206.07682.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;
Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023. Chain-of-
Thought Prompting Elicits Reasoning in Large Language
Models. arXiv:2201.11903.
Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,
K. R.; and Cao, Y . 2023. ReAct: Synergizing Reasoning and
Acting in Language Models. In The Eleventh International
Conference on Learning Representations .
Yih, W.-t.; Richardson, M.; Meek, C.; Chang, M.-W.; and
Suh, J. 2016. The Value of Semantic Parse Labeling for
Knowledge Base Question Answering. In Proceedings of the
54th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , 201–206. Berlin, Ger-
many: Association for Computational Linguistics.
Yu, D.; Zhang, S.; Ng, P.; Zhu, H.; Li, A. H.; Wang, J.; Hu,
Y .; Wang, W.; Wang, Z.; and Xiang, B. 2023. DecAF: Joint
Decoding of Answers and Logical Forms for Question An-
swering over Knowledge Bases. arXiv:2210.00063.
Yu, D.; Zhu, C.; Fang, Y .; Yu, W.; Wang, S.; Xu, Y .; Ren,
X.; Yang, Y .; and Zeng, M. 2022. KG-FiD: Infusing Knowl-
edge Graph in Fusion-in-Decoder for Open-Domain Ques-
tion Answering. arXiv:2110.04330.
Zhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,
X.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; and Chi,
E. 2023. Least-to-Most Prompting Enables Complex Rea-
soning in Large Language Models. arXiv:2205.10625.

--- PAGE 10 ---
Appendix
Implementation details
We present here in detail the parameter settings we used to
train the QA system and to perform CoT fine-tuning.
Model # Params # Total Params
BERT-base-uncased 110M 110M
T5-large 770M 770M
Llama2-7b lora 12M 7B
T5-3B 3B 3B
flan-T5-3B 3B 3B
Llama2-7b lora 12M 7B
Table 5: Models utilized and their parameters. # Params rep-
resents trainable parameters.
BERT T5-large Llama-7B T5-3B Flan-T5-3B
Lr 2e-5 5e-5 1e-4 1e-4 1e-4
Batch Size 128 16 32 32 32
Epoch 40 - 5 5 5
Clip Norm 2.0 1.0 1.0 1.0 1.0
Table 6: Hyper-params settings for training QA system and
for CoT fine-tuning.
Specifically, we train our model using Deepspeed, with
pytorch==2.0.0, peft==0.2.0, and transformers==4.29.1. For
models larger than 1B, we train with precision bfloat16. We
adopt LoRA and insert low-rank adapters with dimensions
equal to 16 for fine-tuning Llama. For the optimizer and
learning scheduler, we apply AdamW with Beta=[0.9, 0.95]
and LinearDecay with a warmup ratio equal to 0.1. All ex-
periments are conducted on 8 ×40G Nvidia A100.
Demonstration illustration
We add final answers and composition answers as ”Hint”
before structured rationale to construct our CoT collection.
When conducting in-context learning during inference, we
eliminate the ”Hint” from the chosen demonstration. We
provide two examples as illustrated in Figure 7, one for con-
structing the CoT collection and the other for performing
ICL on large language models during inference.
Case Analysis
We present in Figure 8 and 9 several cases before and after
our Knowledge-Driven Chain-of-Thought. We observe in-
dicates that by correcting the sub-answers, the subsequent
reasoning steps become more dependable and accurate, ulti-
mately resulting in the correct final answers.
Instruction:  Solve a question answering task with interleaving Thought, Action,
Answer steps. Thought can reason about the current situation, and Action can be three
types:
(1) Question[question], which returns a sub-question with single answer that helps solve
the original problem.
(2) Multi_Answer_Question[keyword], which returns a sub-question with multiple
answers.
(3) Finish[answer], which returns the answer and ﬁnishes the task.
Question:  Who are the children of Ike and the woman who composed Nutbush City
Limits?
Hint:  answer: ["Ronnie T urner", "Raymond Craig T urner"], composition_answer: ["tina
turner"]
Thought 1:  I need to ﬁnd out who Ike is and who composed Nutbush City Limits, and
then determine their children.
Action 1:  Question[Who is Ike?]
Answer 1:  Ike T urner
Thought 2:  Ike T urner is the person being referred to. I now need to ﬁnd out who
composed Nutbush City Limits.
Action 2:  Question[ Who composed Nutbush City Limits? ]
Answer 2:  Tina T urner
Thought 3:  Tina T urner composed Nutbush City Limits. I now need to ﬁnd out who
the children of Ike and T ina T urner are.
Action 3:  Multi_Answer_Question[ Who are the children of Ike and T ina T urner? ]
Answer 3:  Ronnie T urner and Raymond Craig T urner
Thought 4:  I found the children of Ike and T ina T urner: Ronnie T urner and Raymond
Craig T urner .
Action 4:  Finish[ Ronnie T urner and Raymond Craig T urner .] 
Question:  What disease did the music producer of Sometimes Late at Night have?
Hint:  answer: ["Mysophobia "], composition_answer: ["michael jackson"]  CoT collection
Instruction:  Solve a question answering task with interleaving Thought, Action,
Answer steps. Thought can reason about the current situation, and Action can be
three types:
(1) Question[question], which returns a sub-question with single answer that helps
solve the original problem.
(2) Multi_Answer_Question[keyword], which returns a sub-question with multiple
answers.
(3) Finish[answer], which returns the answer and ﬁnishes the task.
Question  What movie did Ron Howard direct that was produced designed by
James L. Schoppe?  Thought 1 I need to ﬁnd out which movie Ron Howard directed
that was designed by James L. Schoppe.
Action 1  Question[Which movie did Ron Howard direct that was designed by
James L. Schoppe?] 
Answer 1  Gung Ho
Thought 2  Ron Howard directed the movie Gung Ho that was designed by James
L. Schoppe.
Action 2  Finish[Gung Ho]
Question  What movie involving T oonexplainers that Ron Howard worked on?LLM inference
Figure 7: Input for constructing CoT collection (up) and in-
put for LLM inference (down).

--- PAGE 11 ---
Question:  What founder of New Y ork University had a position in the 4th United
States Congress?
Thought 1  I need to ﬁnd out which founder of New Y ork University held a position
in the 4th United States Congress.
Action 1  Multi_Answer_Question[founder + New Y ork University + 4th United
States Congress]
Answer 1  1. Albert Gallatin 2. Samuel L. Mitchill 3. James Kent 4. Gouverneur
Morris  Albert Gallatin
Thought 2  There are four founders of New Y ork University who held a position in
the 4th United States Congress - Albert Gallatin, Samuel L. Mitchill, James Kent,
and Gouverneur Morris.  Albert Gallatin is the founder of New Y ork University who
held a position in the 4th United States Congress.
Action 2  Question[Which one of the four founders of New Y ork University held a
position in the 4th United States Congress?] Finish[Albert Gallatin]
Answer 2  Gouverneur Morris  
Thought 3  Gouverneur Morris is the founder of New Y ork University who held a
position in the 4th United States Congress.  
Action 3  Finish[Gouverneur Morris]
Question:  Who was the Arizona's Governor in 2009 who also suf fered with Breast
cancer?  
Thought 1  I need to ﬁnd out who was the Arizona's Governor in 2009 and also
suffered with Breast cancer .
Action 1  Question[Who was the Arizona's Governor in 2009?]
Answer 1  Jan Brewer  Janet Napolitano
Thought 2  Jan Brewer was the Arizona's Governor in 2009. I now need to ﬁnd out
if she suf fered with Breast cancer . Janet Napolitano was the Arizona's Governor in
2009. Now , I need to ﬁnd out if she suf fered from Breast cancer .
Action 2  Multi_Answer_Question[Jan Brewer Breast cancer]  Question[Did Janet
Napolitano suf fer from Breast cancer]
Answer 2  Jan Brewer was diagnosed with Breast cancer in 2006.  Breast cancer . 
Thought 3  Jan Brewer was diagnosed with Breast cancer in 2006. I now need to
ﬁnd out if she continued to suf fer from it in 2009.  Janet Napolitano suf fered from
Breast cancer . I need to ﬁnd out if she had it in 2009.
Action 3  Question[Did Jan Brewer continue to suf fer from Breast cancer in 2009?]
Question[Did Janet Napolitano have Breast cancer in 2009?]
Answer 3  No Yes 
Thought 4  Jan Brewer was the Arizona's Governor in 2009 and was diagnosed
with Breast cancer in 2006, but did not continue to suf fer from it in 2009.  Janet
Napolitano was the Governor of Arizona in 2009 and she suf fered from Breast
cancer during that time.
Action 4  Finish[Jan Brewer was the Arizona's Governor in 2009 and was
diagnosed with Breast cancer in 2006, but did not continue to suf fer from it in
2009.]  Finish[Janet Napolitano was the Arizona's Governor in 2009 and suf fered
from Breast cancer .]CWQ caseFigure 8: Cases of CWQ. Red and Green blocks represent
the original hallucinations of LLM and the faithful reason-
ing after sub-answer correction. Yellow block signifies that
the answer generated by the QA system is not entirely pre-
cise, but it does not impact the inference of the subsequent
models.
Question:  What episode of how i met your mother is carrie underwood on?
Thought 1  I need to ﬁnd out which episode of How I Met Y our Mother features
Carrie Underwood.
Action 1:  Multi_Answer_Question[Carrie Underwood How I Met Y our Mother
episode]
Answer 1  How I Met Y our Mother: Sandcastles in the Sand (Season 3, Episode
16) - How I Met Y our Mother: Ultimate Playbook (Season 9, Episode 2)  Hooked
Thought 2  Carrie Underwood appeared in two episodes of How I Met Y our
Mother: ""Sandcastles in the Sand"" in season 3 and ""Ultimate Playbook"" in
season 9. The episode of How I Met Y our Mother featuring Carrie Underwood is
called Hooked.
Action 2:  Finish[ ""Sandcastles in the Sand"" and ""Ultimate Playbook"" ]Hooked
Question:  What type of cancer did eva peron have?  
Thought 1  I need to ﬁnd out what type of cancer Eva Peron had.
Action 1  Multi_Answer_Question[cancer type in Eva Peron]
Answer 1  (1) Cervical Cancer , (2) Uterine Cancer , (3) Ovarian Cancer
Thought 2  Eva Peron had one of three types of cancer: cervical, uterine, or
ovarian.
Action 2  Question[Which type of cancer did Eva Peron have?]
Answer 2  Uterine Cancer Cervical Cancer  
Thought 3  Eva Peron had uterine cancer . Eva Peron had cervical cancer .
Action 2:  Finish[ Uterine Cancer ]Cervical CancerWebQSP caseFigure 9: Cases of WebQSP, by applying KD-CoT we rec-
tify the sub-answers and make the reasoning of LLM more
faithful. Red and Green blocks represent the original halluci-
nations of LLM and the faithful reasoning after sub-answer
correction.
