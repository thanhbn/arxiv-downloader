# 2405.04065.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2405.04065.pdf
# Kích thước tệp: 488469 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2405.04065v4  [cs.CL]  13 Jun 2025FLASH BACK :
Mô hình hóa Ngôn ngữ tăng cường Truy xuất hiệu quả cho Suy luận nhanh
Runheng Liu†, Xingchen Xiao†, Heyan Huang∗, Zewen Chi, Zhijing Wu
Trường Khoa học và Công nghệ Máy tính, Viện Công nghệ Bắc Kinh
{rhliu,xcxiao,hhy63,czw,zhijingwu}@bit.edu.cn
Tóm tắt
Mô hình hóa Ngôn ngữ tăng cường Truy xuất
(RALM) bằng cách tích hợp các mô hình ngôn
ngữ lớn (LLM) với các tài liệu liên quan từ một
kho dữ liệu bên ngoài là một phương pháp đã
được chứng minh để cho phép LLM tạo ra thông
tin vượt ra ngoài phạm vi của kho dữ liệu tiền
huấn luyện. Các nghiên cứu trước đây bằng cách
truy xuất một tập hợp token lặp đi lặp lại với nội
dung được truy xuất đặt trước đầu vào gây ra vấn
đề thời gian chạy cao, điều này làm giảm hiệu
quả suy luận của các LLM vì chúng không sử
dụng hiệu quả bộ nhớ đệm Key-Value (KV). Chúng
tôi đề xuất FLASH BACK, một RALM mô-đun
được thiết kế để cải thiện hiệu quả suy luận của
RALM với mẫu ngữ cảnh nối thêm trong khi duy
trì hiệu suất tốt sau khi tinh chỉnh bằng Low-Rank
Adaption. FLASH BACK nối thêm các tài liệu được
truy xuất vào cuối ngữ cảnh để sử dụng hiệu quả
bộ nhớ đệm KV. Chúng tôi cũng giới thiệu Marking
Token như hai token gợi ý đặc biệt để đánh dấu
ngữ cảnh nối thêm trong quá trình tinh chỉnh. Các
thí nghiệm của chúng tôi cho thấy FLASH BACK
có thể cải thiện hiệu suất mô hình hóa ngôn ngữ
trong metric perplexity. Chúng tôi đã chứng minh
rằng Marking Token là một phần bổ sung có thể
sử dụng được khi tinh chỉnh các mô hình trên các
mẫu ngữ cảnh cụ thể. Bằng cách bỏ qua việc tính
toán lại không cần thiết, FLASH BACK đạt được
tốc độ suy luận nhanh với đầu vào ngữ cảnh dài.
Tốc độ suy luận nhanh hơn tới 4× so với phương
pháp đặt trước tương ứng trên LLM 7B (Llama 2)
trong thử nghiệm thời gian chạy.
1
1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) dựa trên kiến
trúc Transformer (Vaswani et al., 2023) như GPT,
Llama, và OPT, v.v. (Brown et al., 2020; Touvron
et al., 2023; Zhang et al., 2022) cần tài nguyên
tính toán khổng lồ để duy trì
∗Tác giả liên hệ.
†Những tác giả này đóng góp ngang nhau cho công trình này.
1Mã nguồn của chúng tôi trên GitHub:
https://github.com/BIT-NLP-GROUP/FlashBack
26.8744.7949.2288.01130.13
63.93130.71139.74313.91564.02
0 100 200 300 400 500 600OPT-2.7B(2048)OPT-6.7B(2048)LLaMa2-7B(2048)LLaMa2-7B(3072)LLaMa2-7B(4096)
Thời gian (Giây)Mẫu Đặt trướcMẫu Nối thêm (Của chúng tôi)Hình 1: Một loạt thử nghiệm suy luận với đầu vào mô
phỏng và nội dung được truy xuất trên OPT và Llama 2
để so sánh Mẫu Nối thêm với Mẫu Đặt trước.
(Độ dài chuỗi tối đa được ký hiệu dưới tên mô hình)
kiến thức của chúng cập nhật (Meng et al., 2023).
Do đó, Mô hình hóa Ngôn ngữ tăng cường Truy
xuất (RALM) đã xuất hiện như một phương pháp
phổ biến, cho phép tạo nội dung tận dụng các kho
dữ liệu bên ngoài để mở rộng ra ngoài kiến thức
vốn có trong các tham số của mô hình, từ đó giảm
chi phí tính toán để nắm bắt kiến thức cập nhật.
Hiệu quả của việc sử dụng RALM để cải thiện hiệu
suất của LLM đã được chứng minh bởi các nghiên
cứu trước đây (Guu et al., 2020; Lewis et al., 2021;
Izacard et al., 2022; Wang et al., 2023b).
Các nghiên cứu gần đây sử dụng nội dung được
truy xuất đơn giản bằng cách đặt trước nội dung
được truy xuất vào đầu vào mà không cập nhật
các mô hình ngôn ngữ thể hiện tiềm năng ấn tượng
của việc thích ứng các LLM có sẵn với nội dung
được truy xuất (Shi et al., 2023; Ram et al., 2023).
Các nghiên cứu khác xây dựng RALM thông qua
việc tiền huấn luyện LLM (Borgeaud et al., 2022),
hoặc tham gia vào việc tiền huấn luyện liên tục của
các RALM hiện có (Izacard et al., 2022; Wang et
al., 2023a).
Tuy nhiên, những nghiên cứu này được giới thiệu
với các hạn chế. Thứ nhất, các LLM có sẵn không
được huấn luyện cơ bản để kết hợp nội dung được
truy xuất, và

--- TRANG 2 ---
Hình 2: Một minh họa về quy trình FLASH BACK. Phía bên trái của sơ đồ là giai đoạn tinh chỉnh mô hình của chúng tôi với Marking Token và mô-đun LoRA. Quy trình suy luận ở phía bên phải.

việc tiền huấn luyện rộng rãi các LLM để xây dựng
RALM phát sinh chi phí tính toán cao (Lin et al.,
2023). Thứ hai, mặc dù các phương pháp trong
ngữ cảnh đã được áp dụng hiệu quả trên các LLM
có sẵn (Ram et al., 2023), công trình của chúng tôi
đã xác minh rằng việc đặt trước đầu vào dài với
In-Context RALM gây ra chi phí tính toán cao, điều
này làm tăng thời gian suy luận. Cụ thể, nội dung
được truy xuất thay đổi với mỗi bước truy xuất
token, và bộ nhớ đệm key-value (KV) được tính
toán của toàn bộ ngữ cảnh bị loại bỏ, yêu cầu tính
toán lại cho mỗi thay đổi. Quá trình tạo văn bản
với ngữ cảnh đầu vào dài có thể tăng đáng kể thời
gian chạy suy luận, và bộ nhớ đệm attention KV
phát triển thậm chí lớn hơn trên LLM với hàng
trăm tỷ tham số (Pope et al., 2022). Trong những
hoàn cảnh này, việc tính toán lại bộ nhớ đệm KV
trở thành một nhiệm vụ tính toán chuyên sâu, đòi
hỏi chi phí đáng kể cho RALM với truy xuất liên
tục (Ram et al., 2023).

Trong bài báo này, chúng tôi đề xuất FLASH BACK
để khắc phục những vấn đề chúng tôi đã đề cập
ở trên, một phương pháp sử dụng các LLM có sẵn
một cách hiệu quả mà không cần tiền huấn luyện
thêm các LLM như được hiển thị trong Hình 2,
để giải quyết vấn đề không hiệu quả bằng cách
phá vỡ sự phụ thuộc của đầu vào vào nội dung
được truy xuất trong ngữ cảnh. Cụ thể, thay vì
đặt trước nội dung được truy xuất trực tiếp vào
đầu vào, chúng tôi sử dụng Mẫu Ngữ cảnh Nối
thêm, trong đó nội dung được truy xuất được nối
thêm vào cuối đầu vào (xem Hình 3). Trong phương
pháp của chúng tôi, bộ nhớ đệm KV của mỗi đầu
vào là tĩnh trong quá trình suy luận, và việc tính
toán lại bộ nhớ đệm KV chỉ diễn ra trong phần
nội dung được truy xuất. Mặc dù có những lợi
thế trong việc áp dụng mẫu ngữ cảnh này, nó
cũng đưa ra một vấn đề làm giảm hiệu suất của
RALM. Mẫu nối thêm của chúng tôi phá vỡ tính
coherence ngữ nghĩa của ngữ cảnh, dẫn đến mất
mát trong perplexity khi áp dụng nó cho RALM
hiện có (xem Bảng 1). Để giải quyết vấn đề này,
được truyền cảm hứng bởi Ren et al. (2023), FLASH
BACK giới thiệu một Marking Token có thể điều
chỉnh để thích ứng với mẫu ngữ cảnh nối thêm
của chúng tôi để khôi phục hiệu suất tương đương.

Đặc biệt, chúng tôi sử dụng LoRA (Hu et al., 2021)
để tinh chỉnh Marking Token trong khi giữ nguyên
cả retriever và LLM đông lạnh. Do đó, FLASH BACK
là một phương pháp có thể thích ứng có thể hợp
tác với các RALM khác thực hiện truy xuất cho
mỗi n token, từ đó nhắm đến tinh chỉnh hiệu quả,
suy luận và căn chỉnh ngữ cảnh-mô hình.

Các đóng góp chính của chúng tôi có thể được
liệt kê như sau:

• Công trình của chúng tôi đã phân tích ngữ cảnh
đặt trước trong phân tích FLOPs và các thí
nghiệm. Chúng tôi nhận thấy nó không hiệu
quả trong RALM với đầu vào dài do việc tính
toán lại dư thừa bộ nhớ đệm KV (xem Hình 3
và phần 3.3).

• Chúng tôi nhận thấy rằng Mẫu Ngữ cảnh Nối
thêm có thể cải thiện hiệu quả suy luận của
RALM. (Nhanh hơn tới 4× trong Llama 2 7B
như được hiển thị trong Hình 1)

• Chúng tôi khai thác Marking Token có thể được
sử dụng để thích ứng các LLM với các mẫu
ngữ cảnh mới bằng tinh chỉnh hiệu quả với
LoRA thay vì tiền huấn luyện.

2 Bối cảnh và Nghiên cứu liên quan

RALM Truy xuất-Đọc Các nghiên cứu trước đây
(Borgeaud et al., 2022; Lin et al., 2023; Ram et
al., 2023; Shi et al., 2023) đã tạo ra các mô-đun
riêng biệt cho việc lựa chọn tài liệu và đọc tài
liệu, và phương pháp này có thể được định nghĩa
là RALM Truy xuất-Đọc. Trong khung RALM

--- TRANG 3 ---
gần đây, đặc biệt đối với những khung sử dụng
LLM, yêu cầu bắt buộc là căn chỉnh các tài liệu
được truy xuất với các yêu cầu cụ thể của LLM
(Gao et al., 2024). AAR, REPLUG, và UPRISE
là các retriever tinh chỉnh với LLM đông lạnh để
đạt được căn chỉnh (Cheng et al., 2023; Shi et al.,
2023; Yu et al., 2023). RETRO sử dụng retriever
đông lạnh để xây dựng RALM riêng của họ
(Borgeaud et al., 2022). In-Context RALM sử dụng
retriever đông lạnh để lựa chọn tài liệu và LLM
đông lạnh để đọc tài liệu mà không trải qua huấn
luyện bổ sung cho LLM hoặc retriever (Ram et al.,
2023). RAVEN (Huang et al., 2023) tiếp tục tiền
huấn luyện ATLAS (Izacard et al., 2022) để cải
thiện khả năng trong ngữ cảnh của nó, điều này
căn chỉnh mô hình với chiến lược prompting cụ
thể, và họ cũng đề xuất Fusion-in-Context Learning
để kết hợp nhiều ví dụ trong ngữ cảnh hơn trong
quá trình suy luận của nó.

Do đó, mô hình hóa của chúng tôi gần với thể
loại RALM sử dụng LLM chỉ decoder (Mô hình
Tự hồi quy) và RALM Retrieval-Read sử dụng
các mô-đun riêng biệt cho việc lựa chọn tài liệu
và đọc tài liệu (Borgeaud et al., 2022; Ram et al.,
2023; Lin et al., 2023; Shi et al., 2023). Hơn nữa,
phương pháp được đề xuất của chúng tôi được
phân loại là Tăng cường đầu vào bởi nghiên cứu
gần đây (Asai et al., 2024). Sau tất cả, loại RALM
này gần đây đã được định nghĩa là Modular RAG
(Retrieval-Augmented Generation) và ngày càng
trở thành chuẩn mực thống trị trong lĩnh vực RAG
(Gao et al., 2024). Hơn nữa, đáng chú ý là
FinanceBench (Islam et al., 2023) cũng nghiên
cứu hiệu quả của mẫu ngữ cảnh. Tuy nhiên, nỗ
lực của họ trong việc sửa đổi vị trí ngữ cảnh chỉ
tập trung vào độ chính xác suy luận thay vì hiệu
quả. GRITLM (Muennighoff et al., 2024) cải thiện
hiệu quả suy luận mô hình bằng cách tái sử dụng
các biểu diễn dựa trên embedding chỉ áp dụng
cho mô hình embedding. Công trình FLASH BACK
của chúng tôi, là một phương pháp dựa trên văn
bản không bị giới hạn ở các mô hình embedding.

3 Phương pháp

3.1 RALM với In-Context-Learning

Trong khung In-Context RALM (Ram et al., 2023),
một kho dữ liệu bên ngoài C được cung cấp cho
retriever. Retriever có trách nhiệm xác định nội
dung cụ thể trong kho dữ liệu để được sử dụng
làm nội dung được truy xuất, và sau đó nội dung
được truy xuất có thể được nối với đầu vào để
tạo thành một ngữ cảnh điều kiện các dự đoán
của LLM. Cho một chuỗi token x1, ..., xn với độ
dài n, Xác suất của chuỗi token được biểu diễn
bởi

p(x1, ..., xn) = ∏(i=1 to n) pθ(xi|[RC(x<i);x<i]), (1)

trong đó θ là tham số của một mô hình tự hồi quy,
RC(x<i) là nội dung được truy xuất từ kho dữ liệu
C sử dụng tiền tố x<i và [RC(x<i);x<i] ký hiệu
sự nối chuỗi của RC(x<i) và x<i.

Để cân bằng giữa chi phí thời gian chạy và hiệu
suất mô hình, bước truy xuất s và độ dài truy vấn
ℓ được đề xuất để thực hiện cuộc gọi retriever
ít thường xuyên hơn (ít nhất, mỗi s > 1 token).
Do đó, việc truy xuất sẽ được thực hiện cho mỗi
s token, sử dụng ℓ token cuối cùng làm nội dung
được truy xuất được chỉ định cho đầu vào.

Xác suất của chuỗi token được biểu diễn bởi

p(x1, ..., xn) = ∏(j=0 to ns-1) ∏(i=1 to s) pθ(xs·j+i|[RC(q(s,ℓ)j);x<s·j+i]), (2)

trong đó ns = n/s là số bước truy xuất,
q(s,ℓ)j = xs·j-ℓ+1, ..., xs·j là đầu vào retriever.

Công trình của chúng tôi đã tuân theo cơ chế truy
xuất này cùng với việc sử dụng các mẫu ngữ cảnh
khác nhau vì cơ chế này sử dụng các mô-đun truy
xuất và reader (LLM) riêng biệt để chúng có thể
được tối ưu hóa độc lập.

3.2 Mẫu Ngữ cảnh

Trong các mô hình dựa trên transformer chỉ
decoder, việc tính toán các mô-đun attention liên
quan đến truy vấn của token hiện tại và các biểu
diễn key-value của các token đi trước (Vaswani
et al., 2023). Trong quá trình suy luận, key-value
của token hiện tại sẽ được lưu trong bộ nhớ đệm
KV, loại bỏ yêu cầu trong bước tiếp theo. Chúng
tôi nhận thấy rằng việc đặt trước nội dung được
truy xuất vào đầu vào đã phổ biến trong các phương
pháp trước đây (Ram et al., 2023; Shi et al., 2023).
Tuy nhiên, key-value được tính toán cho các token
trước đó trở nên lỗi thời, và việc tính toán lại
key-value mới được yêu cầu cho mỗi bước truy
xuất vì nội dung được truy xuất thay đổi với mỗi
lần truy xuất.

Để tránh chi phí lớn của việc tính toán lại, chúng
tôi loại bỏ sự phụ thuộc tự hồi quy giữa nội dung
được truy xuất và đầu vào trong ngữ cảnh bằng
cách sử dụng một

--- TRANG 4 ---
Hình 3: So sánh các mẫu ngữ cảnh đặt trước và nối thêm trong khoảng thời gian thay đổi nội dung được truy xuất,
mũi tên biểu thị rằng các token được tạo ra trong vòng trước sẽ trở thành một phần của đầu vào trong vòng tiếp theo.
Trong mẫu ngữ cảnh đặt trước, key-value của tất cả các token trong vòng trước nên được tính toán lại. Ngược lại,
trong mẫu ngữ cảnh nối thêm, key-value của đầu vào trong vòng trước có thể được tái sử dụng trong vòng tiếp theo.

mẫu ngữ cảnh nối thêm. Cụ thể, chúng tôi nối
thêm nội dung được truy xuất vào đầu vào (ở
cuối đầu vào) như được hiển thị trong Hình 3.

Sau đó, xác suất của chuỗi token được biểu diễn
bởi

p(x1, ..., xn) = ∏(j=0 to ns-1) ∏(i=1 to s) pθ(xs·j+i|[x<s·j+i;RC(q(s,ℓ)j)]). (3)

Trong trường hợp này, đối với mỗi đầu vào, sự
thay đổi của nội dung được truy xuất RC(q(s,ℓ)j)
sẽ không yêu cầu tính toán lại giá trị mới, do đó
bảo tồn giá trị được tính toán của các token trước
đó trong bộ nhớ đệm KV x<s·j+i. (Mã giả thuật
toán trong Phụ lục A.)

3.3 Phân tích FLOPs của mẫu ngữ cảnh

Chúng tôi đã phân tích các Phép toán Dấu phẩy
động (FLOPs) của việc tính toán lại. Trong phân
tích sau đây, kích thước batch đầu vào được ký
hiệu là b, kích thước ẩn của mô hình là h, và các
chiều của vector Key và Value cũng là h, với độ
dài chuỗi tối đa là T, bước truy xuất là s, và một
mô hình bao gồm l lớp, thì kích thước của vector
đầu vào được cho bởi [b, i·s, h]. Các chiều của
ma trận chiếu key và value là [h, h], và số lần
truy xuất được ký hiệu là i. Tổng thể, FLOPs của
việc tính toán lại được công thức hóa như:

C0 = 2l∑(i=1 to T/s) 2bish² = 2T(T+s)bh²l/s. (4)

Nó tăng bậc hai như một hàm của độ dài chuỗi
tối đa T, và do đó, nó sẽ tăng đáng kể khi độ dài
chuỗi tối đa T ở giá trị lớn.

3.4 Marking Token và Lựa chọn Tinh chỉnh

Vì các LLM không được căn chỉnh rõ ràng với
mẫu nối thêm của chúng tôi, chúng tôi sử dụng
Marking Token và Low-Rank Adaption (LoRA)
(Hu et al., 2021) để thích ứng chúng với mẫu nối
thêm trong khi giữ trọng số mô hình gốc đông
lạnh để căn chỉnh được đạt được mà không sửa
đổi khả năng vốn có của các LLM. Marking Token
được biểu diễn như hai token gợi ý đặc biệt ký
hiệu <MARK_L>, <MARK_R> được thêm vào từ
vựng của mô hình, và chúng được sử dụng để
đánh dấu ranh giới của nội dung được truy xuất
trong ngữ cảnh (xem Hình 2). Sau đó, chúng tôi
tinh chỉnh mô hình sử dụng LoRA với Marking
Token được áp dụng để thích ứng với mẫu nối
thêm. LoRA là một tinh chỉnh hiệu quả tham số
(PEFT) để tinh chỉnh một tập con tham số trong
mô hình được tiền huấn luyện trong khi nhắm
đến việc đạt được hiệu suất tương đương với tinh
chỉnh mô hình đầy đủ. Các nghiên cứu gần đây
cho thấy rằng việc quên là không thể tránh khỏi
trong tinh chỉnh, nhưng phương pháp PEFT có
thể tạo thuận lợi cho việc quên ít hơn trong quá
trình tinh chỉnh, điều này không làm tổn hại
nhiều đến hiệu suất của LLM được tiền huấn
luyện (Kalajdzievski, 2024). Trong FLASH BACK,
chúng tôi chọn LoRA để chứng minh cách tinh
chỉnh một tập hợp tương đối tối thiểu trọng số
mô hình cho phép mô hình của chúng tôi thích
ứng với mẫu ngữ cảnh nối thêm trong khi giữ
trọng số mô hình tiền huấn luyện đông lạnh. Các
thí nghiệm của chúng tôi cho thấy FLASH BACK
có tốc độ suy luận tăng đáng kể và vẫn có thể
duy trì perplexity (PPL) tương đương sau khi tinh
chỉnh. Trong quá trình tinh chỉnh, tất cả các tham
số được

--- TRANG 5 ---
đông lạnh ngoại trừ embedding của <MARK_L>,
<MARL_R>, và các mô-đun LoRA được áp dụng
cho các lớp attention.

3.5 Phân tích FLOPs của Marking Token và
LoRA

Nếu rank của ma trận trọng số LoRA là r, thì các
chiều của ma trận chiếu là [h, r]. Độ dài trung
bình của nội dung được truy xuất là d. Chúng tôi
chỉ trang bị các mô-đun LoRA, hai ma trận chiếu,
cho các ma trận Key và Value. Tổng thể, FLOPs
của các mô-đun LoRA trong mẫu ngữ cảnh nối
thêm có thể được công thức hóa như:²

C1 = 2l(4bThr + bTh)
    + 2l∑(i=1 to T/s)[4b(d+s)hr+b(d+s)h]
    = 2l(4r+1)bhT(d+2s)/s (5)

Kết hợp phương trình 4 và 5, việc giảm FLOPs
khi sử dụng mẫu ngữ cảnh nối thêm với LoRA là:

Cgiảm = C0 - C1
       = 2lTbh[(T+s)h-(4r+1)(d+2s)]/s (6)

3.6 Retriever

Thử nghiệm thời gian chạy của chúng tôi đã sử
dụng một mô hình thưa thớt, BM25 (Robertson
và Zaragoza, 2009), và nhiệm vụ QA được thử
nghiệm với retriever DPR (Karpukhin et al., 2020)
để chứng minh ý tưởng của chúng tôi. Trong thực
tế, FLASH BACK hỗ trợ cơ bản việc chuyển đổi
sang các retriever khác theo cách plug-and-play.

4 Thí nghiệm

4.1 Cải thiện Thời gian chạy

Thiết lập Chúng tôi thử nghiệm OPT, GPT-2, và
Llama2 trong các kích thước mô hình khác nhau
và sử dụng nội dung được truy xuất mô phỏng và
đầu vào trong các thử nghiệm tốc độ suy luận.
Thí nghiệm của chúng tôi bao gồm các thử nghiệm
với độ dài đầu vào khác nhau. Ngoài ra, nó bao
gồm việc kiểm tra thời gian suy luận dưới hai
mẫu ngữ cảnh riêng biệt.

Kết quả Trong Hình 4, chúng tôi so sánh mẫu
nối thêm (trong ngữ cảnh), mẫu đặt trước (trong
ngữ cảnh), và FlashBack trong các mô hình OPT.
Chúng tôi mở rộng kích thước mô hình từ 125M
đến 6.7B. Gia tốc

²Chúng tôi chỉ xem xét các phép toán nhân và bỏ qua một số
phép toán cộng.

thời gian suy luận hiệu quả hơn trong các mô hình
lớn có nhiều lớp hơn và kích thước ẩn lớn hơn.
Trong Hình 5, chúng tôi mở rộng độ dài chuỗi
tối đa từ 1088 đến 3968 và quan sát một cải thiện
đáng kể, vì Llama 2 có độ dài chuỗi tối đa lớn
hơn, là 4096 token. (Lưu ý: Kết quả thử nghiệm
tốc độ suy luận có thể thay đổi tùy thuộc vào
phần cứng được sử dụng và cài đặt; các thử
nghiệm thời gian chạy của chúng tôi được thực
hiện trên một GPU A100-80GB đơn được đặt ở
trạng thái P0)

4.2 Mô hình hóa Ngôn ngữ

Thiết lập Chúng tôi thử nghiệm FLASH BACK
trên tập dữ liệu Wikitext-2 (Merity et al., 2016)
và ba tập dữ liệu: Arxiv, Freelaw, và Stackexchange
từ the Pile (Gao et al., 2020). Chúng tôi báo cáo
perplexity trung bình ở cấp độ token làm metric
đánh giá (loại trừ <MARK_L>,<MARK_R>).

Các thí nghiệm của chúng tôi sử dụng các mô
hình OPT (1.3B-6.7B) và GPT-2 (124M-1.5B).

Chúng tôi so sánh FLASH BACK với mẫu ngữ
cảnh nối thêm mà không có tinh chỉnh và cũng
với mẫu ngữ cảnh đặt trước. (Đối với các mô hình
nhỏ hơn dưới 774M tham số, kết quả ở Phụ lục
C Bảng 8)

Kết quả Bảng 1 hiển thị kết quả của các cấu hình
khác nhau trong tập dev. Chúng ta có thể thấy
rằng mẫu ngữ cảnh nối thêm được áp dụng trực
tiếp trên LLM thông qua phương pháp trong ngữ
cảnh tạo ra perplexity cao hơn so với mô hình
sử dụng mẫu ngữ cảnh đặt trước. Kết quả của
chúng tôi về tinh chỉnh với các mẫu ngữ cảnh
nối thêm bằng cách sử dụng LoRA chứng minh
rằng mô hình có thể thích ứng với các mẫu như
vậy sau khi tinh chỉnh. Đáng chú ý, với cả Marking
Token và LoRA được áp dụng, sự khác biệt về
kết quả perplexity giữa hai mẫu ngữ cảnh riêng
biệt trở nên được giảm bớt dần khi kích thước
của mô hình thử nghiệm tăng lên. Chúng tôi cũng
thử nghiệm GPT-2 từ 124M đến 1.5B để xác thực
quan sát này. (Xem Bảng 8 trong D). Hơn nữa,
chúng tôi thử nghiệm Llama 3.2-3B để xác thực
mô hình hóa ngôn ngữ của chúng tôi trong một
trong những mô hình mã nguồn mở mới nhất
(Xem Bảng 9 trong E).

4.3 Mô hình hóa Ngôn ngữ với Truy xuất Liên
tục

Thiết lập Vì chi phí thời gian chạy bổ sung của
LoRA so với tham số đầy đủ là có thể chấp nhận
được, chúng tôi sử dụng nó trong các thí nghiệm
sau đây. Chúng tôi tiếp tục thử nghiệm FLASH
BACK trên tập dữ liệu Wikitext-2 (Merity et al.,
2016) và thiết kế một kịch bản truy xuất nhiều
lần. Cụ thể, cho một khối token

--- TRANG 6 ---
10.2220.09 23.131.550.13
13.5927.62 29.6340.9351.75
13.0727.1443.7270.56139.6
020406080100120140160
OPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7BThời gian (Giây)
Mẫu Nối thêmFlashbackMẫu Đặt trướcHình 4: Thời gian cho các mô hình OPT tạo ra một chuỗi có độ dài 2048 token (bao gồm 128 token cho nội dung được truy xuất), cho một đầu vào 512 token, với bước truy xuất là 16.

Mô hình A./P. LoRA M.T. Wikitext-2 Arxiv Freelaw Stackexchange
OPT-1.3BN. 16.76 9.64 8.58 7.78
P. 15.02 9.59 8.35 7.63
P. ✓ 10.23 8.33 7.47 6.87
P. ✓ ✓ 10.22 8.31 7.45 6.87
A. 81.57 51.94 53.73 42.99
A. ✓ 12.65 11.38 10.11 9.13
A. ✓ ✓ 10.49 8.72 7.85 7.17
OPT-2.7BN. 14.50 8.72 7.74 6.99
P. 13.14 8.68 7.56 6.88
P. ✓ 9.23 7.69 6.80 6.23
P. ✓ ✓ 9.23 7.68 6.78 6.22
A. 76.41 50.68 51.78 42.48
A. ✓ 11.58 10.88 9.31 8.51
A. ✓ ✓ 9.52 8.08 7.19 6.53
OPT-6.7BN. 12.30 7.74 6.94 6.22
P. 11.20 7.73 6.83 6.15
P. ✓ 8.23 6.98 6.19 5.58
P. ✓ ✓ 8.24 6.99 6.18 5.58
A. 68.31 46.53 48.33 40.25
A. ✓ 10.54 10.92 9.27 8.04
A. ✓ ✓ 8.59 7.43 6.64 5.94

Bảng 1: Perplexity (càng thấp càng tốt) của các mô hình OPT trên toàn bộ tập validation của tập dữ liệu WikiText-2 và tập test một phần của các tập dữ liệu Arxiv, Freelaw và Stackexchange. Bước truy xuất được đặt là 16. A. và P. chỉ mẫu ngữ cảnh nối thêm và đặt trước tương ứng. M.T. chỉ Marking Token. Và N. chỉ không có truy xuất.

x1,...,nc, chúng tôi chia nó theo bước truy xuất để
có được các tiền tố: x1,...,s, x1,...,2s, ..., x1,...,nc-s,
trong đó nc là độ dài của khối, và s là bước truy
xuất. Sau đó chúng tôi sử dụng mỗi tiền tố với
các đoạn văn được truy xuất tương ứng để dự
đoán s token tiếp theo. Chúng tôi báo cáo perplexity
trung bình ở cấp độ token làm metric đánh giá
(loại trừ <MARK_L>,<MARK_R>).

Kết quả Giống như kết quả trong nhiệm vụ mô
hình hóa ngôn ngữ, một xu hướng tương tự được
quan sát trong Bảng 2. Cả Marking Token và
LoRA đều cần thiết để cải thiện hiệu suất mô
hình. Khoảng cách hiệu suất giữa các mẫu ngữ
cảnh giảm khi kích thước mô hình tăng.

--- TRANG 7 ---
19.8539.7261.0182.97105.45130.13
42.79104.32184.73288.24413.93564.02
0100200300400500600
1088 1664 2240 2816 3392 3968Thời gian (Giây)
Độ dài Chuỗi được Tạo (Token)
Mẫu Nối thêm Mẫu Đặt trướcHình 5: Thời gian cho Llama2-7B tạo các chuỗi với
độ dài khác nhau, cho một đầu vào 512 token, với bước
truy xuất là 16.

Mô hình A./P. LoRA M.T. Perplexity ↓
OPT-1.3BP. 26.30
P. ✓ 14.82
P. ✓ ✓ 14.79
A. ✓ 15.41
A. ✓ ✓ 15.20
OPT-2.7BP. 22.73
P. ✓ 13.14
P. ✓ ✓ 13.14
A. ✓ 13.72
A. ✓ ✓ 13.49
OPT-6.7BP. 19.51
P. ✓ 11.65
P. ✓ ✓ 11.68
A. ✓ 12.18
A. ✓ ✓ 11.98

Bảng 2: Perplexity của các mô hình OPT trong nhiệm
vụ mô hình hóa ngôn ngữ với truy xuất liên tục, được
đánh giá trên tập validation của WikiText-2. Bước truy
xuất được đặt là 16.

4.4 Hỏi Đáp

Vì cải thiện perplexity của mô hình dẫn đến cải
thiện hiệu suất trong các nhiệm vụ downstream,
perplexity là một metric proxy cho khả năng cải
thiện mục đích chung. Trong In-Context RALM,
các đoạn văn được truy xuất tăng cường hiệu
suất mô hình cả trong các nhiệm vụ upstream và
downstream–mô hình hóa ngôn ngữ và hỏi đáp,
với perplexity và điểm EM làm metric, tương ứng.
Mặc dù FlashBack có thể cải thiện các mô hình
trong các nhiệm vụ mô hình hóa ngôn ngữ, không
rõ liệu nó có ảnh hưởng đến khả năng downstream
của các mô hình hay không.

Thiết lập Chúng tôi thử nghiệm FlashBack trong
các cài đặt khác nhau (xem Bảng 3) trong các tập
dữ liệu hỏi đáp miền mở Natural Questions (NQ)
(Kwiatkowski et al., 2019) và TriviaQA (Joshi et
al., 2017) để quan sát ảnh hưởng của mẫu ngữ
cảnh và marking token trong nhiệm vụ QA.

Kết quả Kết quả của chúng tôi cho thấy rằng
mẫu ngữ cảnh không ảnh hưởng rõ ràng đến hiệu
suất mô hình trong các nhiệm vụ QA sau khi tinh
chỉnh mô hình. Tuy nhiên, Marking Token có thể
cải thiện hiệu suất của mô hình trong các nhiệm
vụ QA, đặc biệt là cho thử nghiệm với mẫu ngữ
cảnh nối thêm.

A./P. LoRA M.T.EM↑
NQ TriviaQA
P. ✓ 17.0 43.3
P. ✓ ✓ 17.5 44.7
A. ✓ 18.7 42.5
A. ✓ ✓ 20.3 43.0

Bảng 3: Kết quả QA của tập test NQ, TriviaQA cho
OPT-6.7B. Mô hình được tinh chỉnh trên nhiệm vụ mô
hình hóa ngôn ngữ với truy xuất liên tục. Chúng tôi
thử nghiệm với retriever DPR và giữ 2 tài liệu được
truy xuất hàng đầu.

4.5 Chi tiết Triển khai

Cải thiện Thời gian chạy Chúng tôi lấy mẫu ngẫu
nhiên các chuỗi token mô phỏng từ phân phối
đều rời rạc U{500,1000} để trở thành pseudo-input
và pseudo-retrieved content. Chúng tôi duy trì
độ dài của nội dung được truy xuất mô phỏng ở
giá trị cố định 128, sau đó thử nghiệm các mô
hình với các độ dài chuỗi tối đa khác nhau. (Chi
tiết thiết lập được hiển thị trong Phụ lục Bảng 5).
Đối với bước truy xuất, tất cả các mô hình được
đặt thành 16. Tất cả các thí nghiệm thời gian chạy
được thực hiện sử dụng một GPU NVIDIA A100
đơn.

Mô hình hóa Ngôn ngữ Đối với mỗi s token trong
tập train (tham chiếu đến target), chúng tôi lấy
mẫu từ phân phối đều rời rạc U{T/2, T-s} để có
được vị trí bắt đầu của target, trong đó s là bước
truy xuất và T là độ dài chuỗi tối đa. Đối với
Wikitext-2, Wikitext-103, một siêu tập của
Wikitext-2, được sử dụng làm kho truy xuất. Để
tránh rò rỉ dữ liệu, đầu ra target được bao gồm
trong nội dung được truy xuất, và chúng tôi lọc
nội dung được truy xuất với các tiêu đề khớp với
Wikitext-2 trong quá trình xây dựng dữ liệu. Do
kích thước lớn hơn nhiều của tập dữ liệu Pile,
đối với Arxiv, Freelaw, và Stackexchange, phần
validation và 80% phần test là một hỗn hợp làm
kho truy xuất. Phần còn lại 20% của phần test
được chia

--- TRANG 8 ---
thành hai phần bằng nhau làm tập training và tập
test, tương ứng. Chúng tôi sử dụng truy xuất BM25
và cấu hình độ dài truy vấn là 16, chỉ ra việc sử
dụng 32 token đi trước của target làm đầu vào
cho retriever. Chúng tôi sử dụng mục tiêu dự đoán
token tiếp theo cho các thí nghiệm mô hình hóa
ngôn ngữ. Loss được tính toán trên các token
target trong ngữ cảnh.

Chi tiết Huấn luyện Các tham số có thể huấn
luyện cho tất cả các mô hình bao gồm hai token
embedding của <MARK_L> và <MARK_R>, và
các mô-đun LoRA được áp dụng cho tất cả các
lớp attention. Rank của ma trận trọng số LoRA
được đặt là 16. Kích thước batch được đặt là 16.
Tỷ lệ học cho các mô hình GPT-2 (124M-1.5B)
được chỉ định là [4e-4, 2e-4, 1e-4, 5e-5]. Tương
ứng, tỷ lệ học cho các mô hình OPT (1.3B-6.7B)
được chỉ định là [1e-4, 5e-5, 5e-5]. Chúng tôi
huấn luyện trong 5K bước bổ sung, với 10% bước
warm-up và lịch tỷ lệ học cosine. Để chạy trên
một GPU 24GB RAM đơn, có thể sử dụng định
dạng bf16 trên mô hình OPT-6.7B và đặt độ dài
chuỗi tối đa là 512.

Trong triển khai của chúng tôi, chúng tôi sử dụng
thư viện Transformers (Wolf et al., 2020) và truy
xuất BM25 từ thư viện Pyserini (Lin et al., 2021).
Tất cả các thí nghiệm tinh chỉnh được thực hiện
bằng cách sử dụng 4×24GB RAM GPU.

5 Nghiên cứu Ablation

Marking Token Chúng tôi đã thử nghiệm mô
hình sử dụng mẫu ngữ cảnh nối thêm chỉ được
tinh chỉnh với LoRA mà không có Marking Token
để so sánh với FLASH BACK. Trong Bảng 1, kết
quả thử nghiệm của các mô hình với Marking
Token và tinh chỉnh LoRA có PPL thấp hơn, có
nghĩa là tinh chỉnh Marking Token có thể tăng
cường thêm hiệu suất của RALM, và do đó đáp
ứng kỳ vọng của chúng tôi. Hơn nữa, chúng tôi
cũng thử nghiệm xem liệu Marking Token và các
mô-đun LoRA có thêm chi phí suy luận bổ sung
vào mô hình hóa của chúng tôi hay không. Kết
quả của chúng tôi trong Hình 4 cho thấy FLASH
BACK chậm hơi so với mô hình mà không có
những mô-đun này, nhưng sự khác biệt đủ nhỏ
để có thể bỏ qua so với việc tăng tổng thể trong
tăng tốc tốc độ suy luận. Trong thử nghiệm QA
(Bảng 3), kết quả cho thấy marking token có thể
hiệu quả trong việc dự đoán hiệu suất mô hình
trong cả hai mẫu ngữ cảnh.

Bước Truy xuất Việc phân bổ số bước truy xuất
cho mỗi lần lặp là một lựa chọn thiết kế

Perplexity
MôLhìnhBước Truy xuất
16 32 64
GPT2-124M 28.11 23.64 21.80
GPT2-355M 16.46 15.66 15.80
GPT2-774M 13.46 13.12 13.24
GPT2-1.5B 12.59 12.28 12.24
OPT-125M 24.73 22.42 21.26
OPT-350M 17.29 16.71 16.71
OPT-1.3B 11.51 11.29 11.31
OPT-2.7B 10.70 10.38 10.23
OPT-6.7B 9.40 9.10 9.09

Bảng 4: Perplexity (càng thấp càng tốt) của các mô
hình GPT và OPT trên tập validation của tập dữ liệu
WikiText-2, thay đổi bước truy xuất, bằng cách sử dụng
FLASH BACK.

liên quan đến hiệu suất của mô hình cả về perplexity
và thời gian suy luận. Như chúng tôi đã đề cập
ở trên trong phần 3.1, In-Context RALM (Ram et
al., 2023) chọn sử dụng bước truy xuất được chọn
kỹ để cân bằng sự đánh đổi hiệu suất. Họ chọn
sử dụng giá trị bước tương đối nhỏ để có tạo
sinh tốt hơn (perplexity thấp hơn) mà không thêm
chi phí suy luận quá mức. Tuy nhiên, các thí
nghiệm của chúng tôi (xem Bảng 4 và 6 trong
phụ lục) về việc thay đổi bước truy xuất trên FLASH
BACK ngụ ý một ý kiến khác. Chúng tôi nhận
thấy rằng việc sử dụng 32 hoặc 64 làm bước truy
xuất không làm giảm hiệu suất của mô hình. Khi
thử nghiệm với một mô hình lớn hơn như OPT-6.7B,
thử nghiệm với bước truy xuất cao hơn có perplexity
thấp nhất so với thử nghiệm sử dụng cùng mô
hình với bước truy xuất thấp hơn. Do đó, sự đánh
đổi giữa perplexity và thời gian suy luận có thể
không phải là một hiện tượng đặc biệt rõ ràng
đối với một số trường hợp. Như một hệ quả,
chúng tôi đưa ra suy đoán táo bạo rằng nó có thể
là một lợi thế cho FLASH BACK để tăng bước
truy xuất mà không làm giảm mạnh perplexity
của mô hình và vẫn duy trì suy luận tốc độ cao
trong các kịch bản đầu vào ngữ cảnh dài. Hiện
tượng này có thể cần chứng minh thêm trong tương
lai.

6 Kết luận

Việc đặt trước nội dung được truy xuất vào đầu
vào mà không sửa đổi kiến trúc mô hình đã là
một phương pháp khả thi cho LM để sử dụng
kiến thức bên ngoài. Tuy nhiên, mẫu ngữ cảnh
đặt trước cản trở RALM không thể tổng quát hóa
hiệu quả ở độ dài ngữ cảnh đáng kể với đầu vào
dài.

--- TRANG 9 ---
Bài báo này trình bày phương pháp FLASH BACK,
cho phép RALM với tốc độ suy luận nhanh hơn
trong khi duy trì hiệu suất mô hình cạnh tranh.
Chúng tôi đã chứng minh rằng mẫu ngữ cảnh nối
thêm hiệu quả hơn so với phương pháp đặt trước
tương ứng. Ngoài ra, các thí nghiệm của chúng
tôi chứng minh rằng các LLM có thể thích ứng
với các mẫu ngữ cảnh mới bằng cách sử dụng
Marking Token với tinh chỉnh thích ứng (LoRA).
Hiệu suất trong chất lượng tạo sinh của nó gần
như tương đương với LLM sử dụng phương pháp
đặt trước tương ứng khi kích thước của mô hình
tăng lên.

Hạn chế

FlashBack được thiết kế đặc biệt cho các phương
pháp RALM sử dụng nhiều lần truy xuất bởi mỗi
n token. Các nghiên cứu khác sử dụng các chiến
lược truy xuất như truy xuất một lần hoặc truy
xuất bởi mỗi n khối văn bản không được bao
phủ trong bài báo của chúng tôi và cần nghiên
cứu thêm. Trong các thí nghiệm của chúng tôi,
bước truy xuất là một giá trị cố định trong mỗi
thử nghiệm, nhưng nó có thể là một biến động,
vì vậy có thể có những cải thiện bổ sung. Trong
thí nghiệm so sánh thời gian chạy, do tài nguyên
tính toán hạn chế, chúng tôi đã sử dụng dữ liệu
mô phỏng (giả) để thử nghiệm. Điều này là bởi
vì chúng tôi không tìm thấy một nhiệm vụ downstream
công khai phù hợp đáp ứng các yêu cầu sau: 1)
đầu vào ngữ cảnh dài, 2) truy xuất nhiều lần, và
3) văn bản được tạo trở thành một phần mới của
ngữ cảnh. Nhiệm vụ QA thường đi kèm với một
câu hỏi ngắn, là một đầu vào ngắn, vì vậy gánh
nặng của RALM chủ yếu nằm ở các tài liệu được
truy xuất, thay vì đầu vào. Tuy nhiên, chúng tôi
tin rằng loại kịch bản này tồn tại trong một số
ứng dụng nơi đầu vào dài được yêu cầu với nhiều
lần truy xuất. Ngoài ra, chúng tôi không thử
nghiệm mô hình hóa của chúng tôi trên các LLM
có kích thước trên 7B tham số vì chúng thường
yêu cầu tài nguyên tính toán rộng rãi.

Hơn nữa, trong FLASH BACK, chúng tôi không
mở rộng số lượng nội dung được truy xuất lên
một giá trị lớn. Có thể tổng quát hóa các mẫu
ngữ cảnh nối thêm cho nhiều nội dung được truy
xuất hơn (ví dụ, sử dụng khung REPLUG (Shi et
al., 2023)).

Tuyên bố Đạo đức

Nghiên cứu này khám phá ứng dụng Retrieval-
Augmented Generation (RAG) tận dụng các Mô
hình Ngôn ngữ Lớn (LLM). Chúng tôi gọi phương
pháp này là Retrieval-Augmented Language
Modeling (RALM). Dữ liệu trong nghiên cứu này
được lấy từ các tập dữ liệu và nguồn có sẵn công
khai tuân thủ các hướng dẫn đạo đức, đảm bảo
tôn trọng quyền riêng tư và sự đồng ý của người
dùng. Các thiên lệch tiềm ẩn có thể tồn tại vốn
dĩ trong cả kho dữ liệu bên ngoài và các LLM.
Việc sử dụng FLASH BACK cho các ứng dụng
thực tế yêu cầu các biện pháp phòng ngừa bổ
sung để tránh củng cố khuôn mẫu hoặc truyền
bá thông tin sai lệch thông qua quá trình tạo sinh.

Tài liệu tham khảo

Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh,
Luke Zettlemoyer, Hannaneh Hajishirzi, và Wen
tau Yih. 2024. Các mô hình ngôn ngữ đáng tin cậy,
có thể thích ứng và có thể phân bổ với truy xuất.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,
Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, Diego de Las Casas, Aurelia
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey
Irving, Oriol Vinyals, Simon Osindero, Karen Si-
monyan, Jack W. Rae, Erich Elsen, và Laurent Sifre.
2022. Cải thiện các mô hình ngôn ngữ bằng cách
truy xuất từ hàng nghìn tỷ token.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, và Dario Amodei.
2020. Các mô hình ngôn ngữ là những người học
vài lần.

Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng
Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei,
Denvy Deng, và Qi Zhang. 2023. Uprise: Truy xuất
gợi ý phổ quát để cải thiện đánh giá zero-shot.

Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, và Connor Leahy. 2020. The pile: Tập dữ
liệu 800gb văn bản đa dạng cho mô hình hóa ngôn
ngữ.

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,
Meng Wang, và Haofen Wang. 2024. Tạo sinh tăng
cường truy xuất cho các mô hình ngôn ngữ lớn: Một
khảo sát.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, và Ming-Wei Chang. 2020. Realm: Tiền huấn
luyện mô hình ngôn ngữ tăng cường truy xuất.

--- TRANG 10 ---
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và
Weizhu Chen. 2021. Lora: Thích ứng hạng thấp của
các mô hình ngôn ngữ lớn.

Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi,
Kevin Chen-Chuan Chang, và Bryan Catanzaro.
2023. Raven: Học trong ngữ cảnh với các mô hình
ngôn ngữ encoder-decoder tăng cường truy xuất.

Pranab Islam, Anand Kannappan, Douwe Kiela, Re-
becca Qian, Nino Scherrer, và Bertie Vidgen. 2023.
Financebench: Một benchmark mới cho hỏi đáp tài
chính.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, và Edouard
Grave. 2022. Atlas: Học vài lần với các mô hình
ngôn ngữ tăng cường truy xuất.

Mandar Joshi, Eunsol Choi, Daniel Weld, và Luke
Zettlemoyer. 2017. TriviaQA: Một tập dữ liệu thách
thức giám sát từ xa quy mô lớn cho hiểu đọc. Trong
Kỷ yếu Hội nghị thường niên lần thứ 55 của Hiệp
hội Ngôn ngữ học Tính toán (Tập 1: Bài báo dài),
trang 1601–1611, Vancouver, Canada. Hiệp hội Ngôn
ngữ học Tính toán.

Damjan Kalajdzievski. 2024. Quy luật mở rộng cho
việc quên khi tinh chỉnh các mô hình ngôn ngữ lớn.

Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và
Wen tau Yih. 2020. Truy xuất đoạn văn dày đặc cho
hỏi đáp miền mở.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, và Slav Petrov. 2019. Câu hỏi
Tự nhiên: Một Benchmark cho Nghiên cứu Hỏi Đáp.
Giao dịch của Hiệp hội Ngôn ngữ học Tính toán,
7:453–466.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-
täschel, Sebastian Riedel, và Douwe Kiela. 2021.
Tạo sinh tăng cường truy xuất cho các nhiệm vụ
nlp chuyên sâu kiến thức.

Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-
Hong Yang, Ronak Pradeep, và Rodrigo Nogueira.
2021. Pyserini: Một bộ công cụ python cho nghiên
cứu truy xuất thông tin có thể tái tạo với các biểu
diễn thưa và dày đặc. Trong Kỷ yếu Hội nghị quốc
tế ACM SIGIR lần thứ 44 về Nghiên cứu và Phát
triển trong Truy xuất Thông tin, SIGIR '21, trang
2356–2362, New York, NY, USA. Hiệp hội Máy tính.

Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia
Shi, Maria Lomeli, Rich James, Pedro Rodriguez,
Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke
Zettlemoyer, và Scott Yih. 2023. Ra-dit: Điều chỉnh
hướng dẫn kép tăng cường truy xuất.

Kevin Meng, David Bau, Alex Andonian, và Yonatan
Belinkov. 2023. Định vị và chỉnh sửa các liên kết
thực tế trong gpt.

Stephen Merity, Caiming Xiong, James Bradbury, và
Richard Socher. 2016. Các mô hình hỗn hợp sentinel
con trỏ.

Niklas Muennighoff, Hongjin Su, Liang Wang, Nan
Yang, Furu Wei, Tao Yu, Amanpreet Singh, và
Douwe Kiela. 2024. Điều chỉnh hướng dẫn biểu
diễn tạo sinh.

Reiner Pope, Sholto Douglas, Aakanksha Chowdhery,
Jacob Devlin, James Bradbury, Anselm Levskaya,
Jonathan Heek, Kefan Xiao, Shivani Agrawal, và
Jeff Dean. 2022. Mở rộng suy luận transformer một
cách hiệu quả.

Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, và Yoav
Shoham. 2023. Các mô hình ngôn ngữ tăng cường
truy xuất trong ngữ cảnh.

Siyu Ren, Qi Jia, và Kenny Q. Zhu. 2023. Nén ngữ
cảnh cho transformer tự hồi quy với token sentinel.

Stephen Robertson và Hugo Zaragoza. 2009. Khung
liên quan xác suất: Bm25 và hơn thế. Nền tảng và
Xu hướng trong Truy xuất Thông tin, 3:333–389.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, và
Wen tau Yih. 2023. Replug: Các mô hình ngôn ngữ
hộp đen tăng cường truy xuất.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, và Guillaume Lample. 2023. Llama: Các mô
hình ngôn ngữ nền tảng mở và hiệu quả.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, và Illia Polosukhin. 2023. Attention là tất
cả những gì bạn cần.

Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu,
Bo Li, Mohammad Shoeybi, và Bryan Catanzaro.
2023a. Instructretro: Điều chỉnh hướng dẫn sau
tiền huấn luyện tăng cường truy xuất.

Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee,
Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii
Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandku-
mar, và Bryan Catanzaro. 2023b. Chúng ta có nên
tiền huấn luyện các mô hình ngôn ngữ tự hồi quy
với truy xuất không? một nghiên cứu toàn diện.

--- TRANG 11 ---
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, và Alexander Rush. 2020. Trans-
formers: Xử lý ngôn ngữ tự nhiên tiên tiến. Trong
Kỷ yếu Hội nghị 2020 về Phương pháp Thực nghiệm
trong Xử lý Ngôn ngữ Tự nhiên: Trình diễn Hệ
thống, trang 38–45, Trực tuyến. Hiệp hội Ngôn
ngữ học Tính toán.

Zichun Yu, Chenyan Xiong, Shi Yu, và Zhiyuan Liu.
2023. Retriever thích ứng tăng cường cải thiện khả
năng tổng quát của các mô hình ngôn ngữ như
plugin chung.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, và Luke Zettlemoyer. 2022. Opt: Các mô hình
ngôn ngữ transformer tiền huấn luyện mở.

--- TRANG 12 ---
Phụ lục

A Thuật toán FlashBack

Thuật toán 1 In-Context RALM với FLASH BACK
Cho bước truy xuất s, độ dài truy vấn được truy
xuất ℓ, và độ dài chuỗi tối đa T.
Cho mô hình tự hồi quy pθ, retriever q, và chuỗi
gợi ý ban đầu x0, ..., xt.
Khởi tạo: n←t, bằng chứng được truy xuất e←None,
Bộ nhớ đệm Key-Value K←None.
while n≤T do
    Retriever được gọi cho mỗi stride token.
    if(n−t mod s) = 0 then
        e←q(xn−ℓ,..,n) ▷Gọi retriever
        le←độ dài của e
        Kết hợp các token được tạo trong quá khứ
        vào ngữ cảnh, sử dụng bộ nhớ đệm KV được
        cắt ngắn:
        pn, K1,...,le+n−1 ←
        pθ(x|[x1,...,ñ−1;xñ,...,n−1;e];K1,...,ñ−1)
        ñ←n ▷Lưu vị trí cuộc gọi retriever cuối cùng
    else
        pn, K1,..,le+n−1 ←
        pθ(x|[x1,..,ñ−1;e;xñ,...,n−1];K1,..,le+n−2)
    end if
    xn∼pn(x)
    n←n+ 1
end while

B Thiết lập Thí nghiệm

Mô hình Độ dài đầu vào Giới hạn độ dài
GPT2 256 1024
OPT 512 2048
Llama2 512 4096

Bảng 5: Độ dài đầu vào và giới hạn độ dài chúng tôi
sử dụng trong các thí nghiệm cải thiện thời gian chạy.

C Mô hình hóa Ngôn ngữ Với Nhiều Nội dung
Được Truy xuất Hơn

Được truyền cảm hứng bởi REPLUG (Shi et al.,
2023), chúng tôi nối thêm mỗi nội dung được truy
xuất riêng biệt vào đầu vào và tổng hợp các xác
suất đầu ra tương ứng.

Thiết lập Chúng tôi sử dụng cùng thiết lập như
trong Phần 4.2, ngoại trừ việc thay đổi số lượng
nội dung được truy xuất.

Thời gian (Giây)
Mô hình Bước Truy xuất
16 32 64
GPT2-124M 6.16 5.67 5.89
GPT2-355M 10.68 10.37 10.35
GPT2-774M 16.08 15.97 15.77
GPT2-1.5B 21.71 21.23 20.26
OPT-125M 10.72 10.09 10.10
OPT-350M 20.01 19.9 19.69
OPT-1.3B 22.13 20.72 19.79
OPT-2.7B 30.71 29.10 28.17
OPT-6.7B 50.25 43.79 41.06

Bảng 6: Thời gian cho các mô hình GPT và OPT tạo
các chuỗi với độ dài ngữ cảnh tối đa tương ứng, thay
đổi bước truy xuất.

Perplexity
Mô hình # Nội dung Được Truy xuất
1 2 4 8
GPT2-124M 25.00 24.06 24.65 25.72
GPT2-355M 16.05 15.58 15.91 16.50
GPT2-774M 13.28 12.94 13.21 13.67
GPT2-1.5B 12.49 12.19 12.43 12.83
OPT-125M 23.44 22.53 23.09 23.97
OPT-350M 16.89 16.31 16.67 17.29
OPT-1.3B 11.45 11.10 11.32 11.67
OPT-2.7B 10.48 10.14 10.32 10.63
OPT-6.7B 9.34 9.05 9.20 9.34

Bảng 7: Perplexity (càng thấp càng tốt) của các mô
hình GPT và OPT trên tập validation của tập dữ liệu
WikiText-2, thay đổi số lượng nội dung được truy xuất,
bằng cách sử dụng FLASH BACK.

Kết quả Từ kết quả thử nghiệm về việc thay đổi
số lượng nội dung được truy xuất, perplexity của
tất cả các mô hình chúng tôi thử nghiệm tăng xấu
đi một chút khi số lượng nội dung được truy xuất
tăng lên. Chúng tôi nhận thấy rằng perplexity tốt
nhất đến với việc đặt số lượng nội dung được truy
xuất là 2. Chúng tôi suy đoán rằng lý do đằng sau
hiện tượng này là tài liệu được truy xuất thứ hai
có khả năng có liên quan cao đến tài liệu được
truy xuất đầu tiên. Tuy nhiên, thí nghiệm này chỉ
để chứng minh rằng FLASH BACK có thể được
sử dụng cho truy xuất nhiều tài liệu. Cải thiện
thêm về truy xuất nhiều tài liệu không phải là
trọng tâm của chúng tôi trong bài báo này, và nó
có thể là một hướng trong công việc sau này.

--- TRANG 13 ---
D Kết quả Perplexity của Mô hình GPT2

Chúng tôi đã thử nghiệm các mô hình GPT2 trên
các kích thước mô hình khác nhau trong các metric
perplexity. Kết quả được trình bày trong Bảng 8.

E Kết quả Perplexity của Llama 3.2

Chúng tôi đã thử nghiệm các mô hình Llama 3.2-3B
trong các metric perplexity để chứng minh FlashBack
trên một trong những mô hình mới nhất. Kết quả
thí nghiệm được trình bày trong bảng 9.

--- TRANG 14 ---
Mô hình A./P. LoRA M.T. Wikitext-2 Arxiv Freelaw Stackexchange
GPT2-124MN 30.69 15.19 16.65 14.76
P. 26.00 14.98 16.00 14.43
P. ✓ 17.46 11.46 11.55 9.02
P. ✓ ✓ 17.41 11.44 11.53 9.00
A. 72.29 46.59 56.32 50.71
A. ✓ 25.56 22.35 24.31 17.41
A. ✓ ✓ 19.14 14.21 15.01 10.25
GPT2-355MN 22.13 11.79 12.01 10.07
P. 19.26 11.68 11.63 9.91
P. ✓ 13.33 9.46 9.00 7.13
P. ✓ ✓ 13.33 9.44 8.97 7.13
A. 59.50 42.33 47.23 37.00
A. ✓ 17.05 18.60 18.54 13.18
A. ✓ ✓ 14.37 10.51 9.94 7.81
GPT2-774MN. 19.11 10.69 11.29 9.74
P. 16.71 10.58 11.08 9.59
P. ✓ 11.74 8.56 8.03 6.45
P. ✓ ✓ 11.69 8.52 8.03 6.43
A. 56.15 42.44 49.25 39.61
A. ✓ 14.40 16.83 16.35 11.99
A. ✓ ✓ 12.13 8.95 8.47 6.78
GPT2-1.5BN. 17.32 9.87 10.61 9.03
P. 15.36 9.81 10.49 9.01
P. ✓ 10.84 8.07 7.45 6.01
P. ✓ ✓ 10.83 8.04 7.43 5.98
A. 53.30 41.72 49.25 40.25
A. ✓ 13.75 16.48 15.20 11.50
A. ✓ ✓ 11.32 8.57 8.01 6.35

Bảng 8: Perplexity (càng thấp càng tốt) của các mô hình OPT trên toàn bộ tập validation của tập dữ liệu WikiText-2 và tập test một phần của các tập dữ liệu Arxiv, Freelaw và Stackexchange. Bước truy xuất được đặt là 16. A. và P. chỉ mẫu ngữ cảnh nối thêm và đặt trước tương ứng. M.T. chỉ Marking Token. Và N. chỉ không có truy xuất.

Mô hình A./P. LoRA M.T. Wikitext-2 Arxiv Freelaw Stackexchange
Llama-3.2-3BN 9.02 5.33 6.16 6.14
P. 8.20 5.38 6.27 6.12
P. ✓ 6.92 4.93 5.95 5.74
P. ✓ ✓ 6.93 4.92 5.94 5.74
A. 33.00 32.03 38.63 38.06
A. ✓ 8.21 6.62 8.42 7.61
A. ✓ ✓ 7.56 5.40 6.71 6.25

Bảng 9: Perplexity của mô hình Llama-3.2-3B trên toàn bộ tập validation của tập dữ liệu WikiText-2 và tập test một phần của các tập dữ liệu Arxiv, Freelaw và Stackexchange. Bước truy xuất được đặt là 16.
