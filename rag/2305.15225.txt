# 2305.15225.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2305.15225.pdf
# File size: 2006147 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SAIL: Search-Augmented Instruction Learning
Hongyin Luo1Yung-Sung Chuang1Yuan Gong1Tianhua Zhang2
Yoon Kim1Xixin Wu2Danny Fox3Helen Meng2James Glass1
1MIT Computer Science and Artificial Intelligence Lab, Cambridge MA, USA
2CUHK Centre for Perceptual and Interactive Intelligence, Hong Kong SAR, China
3MIT Linguistics, Cambridge MA, USA
hyluo@mit.edu https://openlsr.org/sail-7b
Abstract
Large language models (LLMs) have been sig-
nificantly improved by instruction fine-tuning,
but still lack transparency and the ability to
utilize up-to-date knowledge and information.
In this work, we propose search-augmented
instruction learning (SAIL), which grounds
the language generation and instruction follow-
ing abilities on complex search results gener-
ated by in-house and external search engines.
With an instruction tuning corpus, we collect
search results for each training case from dif-
ferent search APIs and domains, and construct
a new search-grounded training set containing
(instruction, grounding information, response)
triplets. We then fine-tune the LLaMA-7B
model on the constructed training set. Since
the collected results contain unrelated and dis-
puting languages, the model needs to learn to
ground on trustworthy search results, filter out
distracting passages, and generate the target re-
sponse. The search result-denoising process en-
tails explicit trustworthy information selection
and multi-hop reasoning, since the retrieved
passages might be informative but not contain
the instruction-following answer. Experiments
show that the fine-tuned SAIL-7B model has
a strong instruction-following ability, and it
performs significantly better on transparency-
sensitive tasks, including open-ended question
answering and fact checking.
1 Introduction
Large language models (LLMs) have demonstrated
many impressive capabilities, including zero-shot
inference and few-shot in-context learning (Wei
et al., 2022a). Recent research has shown that
LLMs benefit from instruction tuning (Ouyang
et al., 2022), and that such instruction-tuned LLMs
significantly outperform plain LLMs on zero-shot
language tasks (Peng et al., 2023). Instruction-
tuned LLMs have shown an ability to generate both
natural and programming languages following nat-
ural language guidance and requests. To achievethe same goal, a pretrained LLM needs a number of
annotated examples as in-context learning prompts.
Despite their impressive behavior, LLMs have
a number of issues, including obsolence and trans-
parency. Understandably, LLMs are trained with
corpora constructed up to a certain time point. With
this fixed, pretrained or fine-tuned model, subse-
quently occurring information cannot appear in any
informed generation by the LLM. One way to up-
date the knowledge in LLMs is to re-train the entire
model with an updated training corpus. However,
this would be costly and time-consuming.
In terms of transparency, the predictions of
LLMs are opaque because generations are not
grounded on trustworthy sources. It is possible
for an LLM to generate undesirable language that
looks like human-generated text, including misin-
formation, stereotypes, and toxic language (Zhang
et al., 2023; Hartvigsen et al., 2022). Without pro-
viding legitimate sources for LLM generated texts
it is difficult to catch and avoid these undesirable
LLM behaviors.
To overcome these difficulties, a straightfor-
ward solution is to connect LLMs to information
retrieval systems, especially commercial search
engines. By doing so, the LLM can ground its
predictions on information retrieved from an up-
to-date knowledge base, and the sources of the
generations would be transparent to users. Be-
fore LLMs became large enough to memorize a
significant amount of world knowledge, retrieval-
based grounding had been heavily studied for open-
domain question answering (Chen et al., 2017;
Kwiatkowski et al., 2019; Guu et al., 2020). Recent
LLMs have also shown the potential of using in-
formation retrieval tools, e.g., Toolformer (Schick
et al., 2023) and the ChatGPT (OpenAI, 2022) re-
trieval plugin. However, there remains a challenge:
is there a trustworthy retrieval model and knowl-
edge base that can be utilized by LLMs?
Existing studies on open-domain question an-arXiv:2305.15225v2  [cs.CL]  25 Jun 2023

--- PAGE 2 ---
(3) Constrained CMIP6 projections indicate less warming and a slower ...The slower warming implies a lower snow cover loss rate by 10.5-40.2%. ... future changes in the predicted variable y ... model intercomparison project phase 5 global climate models using ...(1) From climate change 'certainty' to rapid decline: a timeline of IPCC ...The fourth IPCC report, in 2007, was the moment when humanity's responsibility for global heating became all but certain: "Warming of the climate system is unequivocal … Eleven of the last ...(2) AI study ﬁnds planet could cross 2-degree warming threshold by mid ...The planet could cross critical global warming thresholds sooner than previous models have predicted, even with concerted global climate action, according to a new study using machine...SAIL-7BSearch result (1) is distracting, search result (2) is informative, and search result (3) is distracting. i will utilize the informative search result (2) and my knowledge. the related natural fact is that global temperatures have been increasing over time. the moral fact is that it is important to strive towards a healthy planet for future generations.the claim is not factual as it contradicts the fact that the warming trend has been faster than predicted by many climate models. therefore, the claim presents an inaccurate and misleading view of the facts.GPT-4The claim is potentially factual but needs context. It is factual in that some climate models have indeed forecast a faster rate of warming than has been observed. However, without the context that models are generally accurate in long-term projections and that their primary purpose is to predict trends rather than speciﬁc year-by-year temperatures, the claim could be misleading. The claim is fair, as it does not contain hate speech or stereotypes.GPT-3.5-TurboThe claim is factual and fair. There is scientiﬁc evidence suggesting that the actual rate of warming has been slower than what some climate models have forecasted. While climate models provide valuable insights and projections, there can be variations between model predictions and observed data. The search results indicate that the rate of warming has been a topic of study …Perplexity.AIThe claim that “…” is partially factual based on the search results. Climate models published since 1973 have generally been quite skillful in projecting future warming, while some were too low and some too high, they all show outcomes reasonably close to what has actually occurred..
                                  Fact check the following claim:“However the warming trend is slower than most climate models have forecast.” Label: UNFACTUAL������������������������������������������Figure 1: Fact checking grounding on complicated search results with SAIL-7B and strong commercial language
models. The first and third passages are distracting since they do not contain information that supports or refutes the
claim, while the second passage disagrees with the claim. SAIL-7b Successfully make the the correct prediction
while other commercial LLMs are distracted.
swering have chosen Wikipedia as the de facto
knowledge base that contains the answer to most
questions. However, Zhang et al. (2023) found that
the knowledge contained in Wikipedia is not suf-
ficiently up-to-date nor complete for many tasks
that require the latest knowledge, so grounding on
Wikipedia might lead to worse answers than fully
relying on LLMs. Another option is to leverage an
internet search engin such as, for example, Google,
Bing, and DuckDuckGo.com1.
Although widely used commercial search en-
gines can index and retrieve a vast range of up-
to-date information, their retrieval accuracy is ulti-
mately limited, and third-party users cannot control
the performance at the model level. As a result,
retrieval results can be noisy, and unrelated infor-
mation might be shown to users. This behavior
suggests that there is a trade-off between deploying
in-house retrieval systems and external search en-
gines. Although it is possible to prompt LLMs to
directly use the retrieval results, distracting search
results can mislead the model and negatively influ-
ence the model’s performance. As shown in Figure
1, ChatGPT is confused by a distracting passage
and generates an incorrect fact check.
The challenges mentioned above are contradic-
tory, and both have a negative impact on grounded
1A free, privacy-proof, zero-tracking search engine.language modeling with current LLMs - static
knowledge bases and in-house retrievers are not suf-
ficient or up-to-date for all tasks, while commercial
search engines often generate distracting results.
To address these challenges simultaneously, we
propose a search-augmented instruction learning
(SAIL) model. Given input instructions and con-
texts, the model is trained to generate high-quality
responses according to the instruction grounding
on the noisy research results. In other words, the
model learns to denoise the retrieval results to gen-
erate high-quality responses.
In summary, we make the following contribu-
tions in this work:
1. We show that instruction-tuned LLMs can be
heavily misled by distracting grounding infor-
mation and noisy search results.
2.We constructed a search-augmented instruc-
tion training corpus.
3.We fine-tune a 7B-parameter language model
(SAIL-7B ) with the constructed training set,
which outperforms strong baseline models in-
cluding GPT-3.5-Turbo andVicuna-13B on
several NLP tasks.
By comparing the SAIL-7B model with LLaMA-7B ,
Vicuna-7B ,GPT-3.5-turbo , and Vicuna-13B

--- PAGE 3 ---
models on instruction following, question answer-
ing, and language checking tasks, we find that the
SAIL-7B model has a strong instruction following
ability and is robust against distracting grounding
search results generated by different retrieval mod-
els. In addition, the SAIL model also achieves com-
parable performance to state-of-the-art instruction-
following LLMs.
2 Method
2.1 Search Result Collection
In this work, we use the 52k self-instruction corpus
created by the Alpaca team (Taori et al., 2023), and
the corresponding responses generated by GPT-4
(Peng et al., 2023). For each instruction, we con-
struct a search query by simply concatenating the
instruction and the input, if any, and truncating the
query to at most 60 words to fulfill the limitation
of the search engine.
The constructed queries are fed into the Duck-
DuckGo search engine and the BM25 Wikipedia
retriever, and the top three search results are re-
tained. Each result consists of three fields: the title,
a short piece of preview text, and the corresponding
URL of the webpage. For simplicity, we do not
further scrape the retrieved webpage, but just use
the title and preview texts for further processing.
Each training example is assigned a different
search result. We pool the top-three DuckDuckGO
and top-two BM25 search passages, a total of five
search results. Among this pool, we randomly sam-
ple zero, one, two, and three search results with
20%, 20%, 20%, and 40% probability. Given this
randomness, some training cases could be associ-
ated with search results from a single source.
2.2 In-context Retrieval Selection
To encourage the LLM to focus on trustworthy and
informative search results, we concatenate a search
filtering sequence before each annotated response.
For example, “Search result (1) is informative and
search result (2) is distracting, so I will use the
information from the search result (1). ”
However, the trustworthiness of each search re-
sult is not labeled, and the number of retrieval items
is large. To solve this problem, we employ an
entailment classification model proposed in (Luo
and Glass, 2023). We feed each retrieved passage
and the corresponding response into the entailment
model and compare the entailed and contradictory
scores. While most predictions are neutral againstthe response, the relation between entailed and con-
tradictory scores can roughly indicate if a retrieved
passage can provide useful information to generate
the target response. As a result, we label “search
result (i) is informative” if the entailed score is
higher than the contradiction score, otherwise the
search item is distracting. With the constructed
label responses, the SAIL-7b model can generate
in-context search selection sequences as shown in
Figure 1.
2.3 Fine-tuning
After collecting the search results and generating in-
context retrieval selection sequences, we construct
input prompts following Figure 2 (b) with GPT-4
generated responses (Peng et al., 2023). Note that
the most relevant retrieval result is located at the
closest position to the instruction for the model
to better use its information. We fine-tune both
LLaMA-7b models with the constructed prompts
to generate both in-context retrieval selection and
annotated responses.
In practice, the models are fine-tuned with aca-
demic devices. Specifically, we use 4 ×NVIDIA
RTX A6000 GPUs (48GB ×4) to train the models
for 3 epochs. We apply mixed-precision training
(fp16) with the standard AdamW optimizer. We
set the maximum sequence length as 1,600 and the
batch size as 32. Following Vicuna, we apply gradi-
ent checkpointing to reduce the memory cost. The
entire fine-tuning process takes 24 hours (24 ×4
GPU hours). To enable the fine-tuning, we applied
gradient offload with Deepspeed and full-sharded
data parallel (FSDP) (Paszke et al., 2019).
2.4 Evaluation
SAIL for instruction following. Following Peng
et al. (2023), we evaluate the instruction following
the quality of different models by comparing with
GPT-4 responses on the same set of instructions
and scoring with GPT-4.
For each case, we construct an evaluation
prompt by concatenating the instruction, the GPT-4
response, and the response of the target model. We
feed the evaluation prompt to GPT-4 and ask it to
score the two responses between 0 to 10. We use
the Vicuna-Instructions-802corpus (Chiang et al.,
2023), which contains 80 questions to evaluate all
models and we calculate the total score a model
2https://github.com/lm-sys/FastChat/blob/main/
fastchat/eval/table/question.jsonl

--- PAGE 4 ---
Below is an instruction that describes a task.Write a response that appropriately completes the request.### Related Information:[Title 3]\n [Preview 3][Title 2]\n [Preview 2][Title 1]\n [Preview 1]### Instruction: [Instruction]### Input: [Input or None]### Response:Below is an instruction that describes a task.Write a response that appropriately completes the request.### Instruction: [Instruction]### Input: [Input or None]### Response:(a) Standard  Prompt
(b) Search-Augmented  PromptFigure 2: Different prompting strategies used in this
work. (a) Standard prompt : the prompt template used
in Peng et al. (2023) to generate GPT-4 responses to the
52k instructions. (b) Search-augmented prompt : com-
bining the top three search results and the instruction.
receives on all questions. We use the evaluation
prompt authored by the Vicuna team3. The highest
possible score is 80×10 = 800 . It is worth noting
that GPT-4 responses can receive slightly different
scores against different counterparts. To normalize
the difference, we calculate the ratio of model
score / GPT-4 score for each test case as the final
assessment as implemented in Peng et al. (2023).
SAIL for Question Answering. Besides evaluat-
ing the quality of instruction-guided generations,
we also assess the model’s ability to answer
commonsense questions. We also test the models
on two different settings, including instructed
zero-shot prediction and the search-augmentation
mode. We evaluate the model performance on
CommonsenseQA (CSQA; Talmor et al. (2019)),
OpenbookQA (OBQA; Mihaylov et al. (2018)),
and ARC-Challenge (Clark et al., 2018) bench-
marks. Both tasks require answering open-ended
questions by selecting from a given set of candi-
date answers. Through the question-answering
experiments, we show that instruction-tuned
language models can be significantly biased by
noisy research results.
SAIL for Fact and Fairness Checking. With the
recent advances in LLMs that generate human-like
3https://github.com/lm-sys/FastChat/blob/main/
fastchat/eval/table/prompt.jsonllanguages without guaranteed alignment, human
and machine-generated misinformation, stereo-
types, and toxicity have become timely and signifi-
cant concerns. Recent studies have shown that with
appropriate instructions and prompts, LLMs can
perform unified fact and fairness checking (Zhang
et al., 2023). However, other attempts have relied
only on LLMs, without grounding on any exter-
nal sources, thus reducing the trustworthiness and
transparency of the checking results.
In this work, we evaluate instructed fact and
fairness checking, with the UniLC benchmark
proposed in (Zhang et al., 2023), including
Climate-Fever, PubHealth, Hate Speech Detec-
tion, and Social Biase Frame (SBIC) tasks with
two different settings - zero-shot and search-
augmented. While we are not aware of what
corpora are used to train GPT-4 and Chat-
GPT, we assess the language-checking perfor-
mance of Vicuna-7B-v1.1 ,Vicuna-13B-v1.1 ,
andSAIL-7B with and without search results.
3 Experiments
3.1 Instruction Following
Automatic Evaluation with GPT-4. We compare
the performance of different models under end-
to-end and search grounded settings against GPT-
4 and ChatGPT models. The scoring results are
shown in Figure 3.
By comparing to GPT-4, we find that the search-
augmented SAIL-7B model significantly outper-
forms all other models (90% vs <85%) using
fewer training instructions and parameters, includ-
ing strong baselines including Vicuna-13B and
GPT-3.5-turbo powered ChatGPT. This indicates
that when the grounding information is provided,
the model does not need as many parameters to
memorize knowledge. In addition, the SAIL-7B
model also achieves high performance even without
search results, showing that the model performance
is stable under different generation settings. Similar
conclusions can be found by comparing all models
against ChatGPT. While GPT-4 is still better, ex-
periment results show that the search-augmented
SAIL-7B model achieves 103% of ChatGPT per-
formance and the no-augmentation SAIL model
achieves 98%, outperforming several strong base-
lines, including LLaMA tuned on GPT4 instruc-
tions and Vicuna models with the same number
of parameters. Besides GPT-4, search-augmented
SAIL-7B is the only model that outperforms Chat-

--- PAGE 5 ---
(a) All Chatbots against GPT-4
(b) All Chatbots against ChatGPTFigure 3: Scoring results of all language models on the instruction-following benchmark against GPT-4 and
ChatGPT. Search indicating generating responses with language models grounding on search results retrieved by
DuckDuckGO , and SAIL (7B) stands for generating responses without search results, although the model is trained
for grounded generations. Both Vicuna-7&13B are version 1.1 models.
GPT on both experiments.
In addition, we found that the search augmenta-
tion makes a significantly higher positive contribu-
tion to the SAIL model than all other models. With
ChatGPT, the effect of feeding search-augmented
prompts with instructions leads to very slight im-
provements in both evaluations. However, ground-
ing on search results can hurt the performance
of Vicuna and LLaMA-GPT4 models of different
sizes. By comparing against GPT4, Vicuna-13B
is slightly improved by search results, but the
improvement is not present when compared to
ChatGPT. For the Vicuna-7B andLLaMA-7B-GPT4
baselines, augmenting input prompts with search
engine outputs makes a significant, negative impact
on both evaluations. On the other hand, applying
search augmentation to SAIL-7B significantly im-
proves model performance on both experiments
(84% to 90% and 98% to 103%). These results
inform our findings:
•The search results contain useful informa-tion that can improve the performance of
instruction-following language models.
•Without search-augmented fine-tuning, it is
difficult for a language model to utilize
valuable information among the complicated
search results, and distracting retrieval results
can mislead the generations.
•Search-augmented instruction learning can
help the model better utilize the valuable in-
formation among noisy search results and im-
prove instruction-following performance.
Data Statistics. We first show the word pref-
erence of different models on the 80 unseen in-
structions. The results are shown in Figure 4.
We compare the distributions of top-10 verbs
generated by GPT4 ,GPT-3.5-Turbo (ChatGPT),
Vicuna-7B-v1.1 , and SAIL-7B models. With
search augmentation, SAIL-7B generates signifi-
cantly more verbs that do not overlap with GPT’s
generations, as shown in Table 1. Only two top-
10 verbs generated by Vicuna are not covered by

--- PAGE 6 ---
GPT-4GPT-3.5-Turbo
Vicuna-7b-v1.1SAIL-LLaMA-7bhaveprovidecreateimpactusedetermineencouragegiveinfluencetake
impact
access
potential
prevalence
idea
opportunity
list
material
experience
alternative
misunderstanding
molecule
behavior
history
quality
trade
formula
indentation
language
tool
factor
force
goal
weight
diversity
habit
individual
user
access
answer
birth
information
balance
behavior
musician
type
responsibility
time
walk
whilehavegiveincludeplayprovidetakeusedeterminemakeattend
impact
pleasure
implication
potential
idea
response
total
exercise
message
quote
safety
role
benefit
clue
estimate
idea
break
input
snap
enzyme
formula
language
loop
age
number
type
contribution
schedule
stride
concert
luau
haveprovidetakeusecreateincludeconsidercontainmakeplay
impact
implication
opportunity
influence
estimate
clue
guidance
information
break
day
fact
moment
table
approach
bit
combination
array
barrier
challenge
schedule
perception
personality
policy
use
factor
step
tip
atom
molecule
catch
list
statement
rolehavetakecalculatematchrevolutionizecheckcreateincludeincreaseuse
impact
ability
background
implication
break
time
advantage
audience
number
length
weight
character
symbol
way
route
date
page
reputation
website
challenge
current
environment
form
study
tradition
trend
way
arousal
efficiency
level
time
ability
formula
gate
listeningFigure 4: Top-10 verbs and associated nouns generated by selective large language models.
Models Vicuna-7B-v1.1 SAIL-7B
Novel Include Calculate
Verbs Consider Match
Revolutionize
Check
Include
Increase
Count 2 6
Table 1: Top-10 verbs generated by LLaMA-based mod-
els that do not overlap with GPT-4 and ChatGPT.
GPT-4 and ChatGPT, while six out of ten verbs gen-
erated by SAIL-7b are not high-frequency verbs by
the GPT models. This indicates that the grounding
search results can shift the generation preference
of the language models.
The statistics of the generated responses is
shown in Table 2. GPT-4 generates the longest
and most diverse responses, while ChatGPT tends
to generate shorter and simpler answers. With-Models Avg. Std. Diversity
GPT-4 303.8 121.5 0.48
ChatGPT 135.1 63.6 0.56
Vicuna-13B 204.1 82.9 0.45
Vicuna-7B 196.5 90.3 0.45
SAIL-7B + Search 246.2 87.7 0.44
SAIL-7B 206.6 86.9 0.47
Table 2: Statistics about the length and diversity of
the generated responses of different language models.
Diversity stands for the total number of different words
divided by the total length.
out search augmentation, the lengths of SAIL-7B
generated sequences are similar to the Vicuna mod-
els. This indicates that search augmentation can
increase the length of the generated responses.
3.2 Question Answering
The experiment results of question answering
are shown in Table 3. CSQA, OBQA, and

--- PAGE 7 ---
Model LLaMA-7B Vicuna-7B Vicuna-13B SAIL-7B
Search None Wiki DDG None Wiki DDG None Wiki DDG None Wiki DDG
CSQA 48.4 47.7 49.6 44.9 45.6 47.6 50.6 51.1 50.9 51.5 51.0 51.8
OBQA 42.2 44.4 44.6 37.2 39.4 42.6 49.0 47.2 49.4 49.2 50.2 52.0
ARC-C 43.0 45.2 47.3 40.5 44.5 46.3 53.2 51.6 51.8 47.7 48.1 48.4
Avg. 44.5 45.8 47.2 40.9 43.3 45.5 51.0 50.0 50.7 49.5 49.8 50.7
Table 3: Question answering accuracy (%) by zero-shot models with simple instructions.
Model Metric Climate PubHealth Fact Avg. HSD SBIC Fairness Avg. All Avg.
Vicuna-7BAcc 57.9 60.6 59.2 55.9 74.5 65.2 62.2
F1 38.8 56.63 47.7 68.5 84.3 76.4 62.04
Vicuna-13BAcc 51.4 54.4 52.9 57.7 72.3 65.0 59.0
F1 42.5 57.7 50.1 69.6 82.9 76.3 63.2
LLaMA-7BAcc 58.8 59.9 59.3 62.3 74.8 68.6 63.9
F1 46.6 57.5 52.0 72.3 84.4 78.4 65.2
SAIL-7BAcc 63.5 69.2 66.4 70.1 76.4 73.2 69.8
F1 51.0 63.6 57.3 75.1 83.9 79.5 68.4
Table 4: Instructed zero-shot language checking performance on the UniLC benchmark.
ARC-Challenge are open-ended, selection-based
question-answering tasks. We compare instruction-
tuned Vicuna-7B ,Vicuna-13B ,LLaMA-7B-GPT4 ,
andSAIL-7B models under no-augmentation and
search-grounded settings with different sources.
All evaluations are zero-shot and instruction guided.
Traditionally, a knowledgeable LLM can answer
questions and select the most coherent and appro-
priate answers without external information. In
each task, we want to evaluate the performance of
different models and knowledge bases. We search
Wikipedia (Wiki) with the BM25 retriever, and the
web with DuckDuckGO (DDG), feeding the LLMs
with the top-3 search results, which could contain
unrelated and distracting information.
In general, we found that DuckDuckGo (DDG)
leads to better performance for all models on all
tasks because it is more flexible, covering a much
wider range of information. This suggests the effec-
tiveness of search engines over retrieving a static
knowledge base. We found that both LLaMA and
Vicuna-7B models can be slightly improved when
search results are provided on most tasks. How-
ever, the overall performance is limited. The av-
erage accuracy of searched-augmented LLaMA-7B
andVicuna-7B is below 50%.
With Vicuna-13B , which is a roughly two times
larger model, we get the best average performance(51.0%) on the three tasks without grounding in-
formation. However, adding search results hurts
its accuracy in most experiments. While augment-
ing the model with DDG search results slightly
improves the performance on CSQA and OBQA,
the accuracy on ARC-Challenge is decreased by
1.4%. With BM25-based Wikipedia search results,
the accuracy can decrease by as much as 1.8%.
While the Vicuna-13B model achieves strong non-
augmented performance, it is challenging to further
improve the accuracy by utilizing helpful informa-
tion in the search results.
In contrast, the SAIL-7B model improves on all
tasks when incorporating the search results, and
also achieves strong non-augmented performance.
Without retrieval results, SAIL-7B significantly out-
performs LLaMA andVicuna-7B on all tasks with
a large margin (49.5% vs 44.5% and 40.9% aver-
age accuracy). It also performs slightly better than
Vicuna-13B on CSQA and OBQA tasks, while
Vicuna-13B is still strongest on ARC-C. While
search augmentation leads to at most 0.5% im-
provement for Vicuna-13B, DDG search results
improve SAIL-7B by 2.8% on OBQA and 1.2%
on average, showing that the SAIL-7B model can
steadily utilize the helpful information among the
search results. As a result, the search-augmented
SAIL-7B model achieves the best performance on

--- PAGE 8 ---
Model Metric Climate PubHealth Avg.
Vicuna-7BAcc 57.7 60.1 58.9
Acc Diff -0.2 -0.5 -0.3
F1 49.5 57.6 53.6
F1 Diff +10.7 +1.0 +5.9
Vicuna-13BAcc 53.5 50.3 51.9
Acc Diff +2.1 -4.1 -1.0
F1 46.6 56.8 51.7
F1 Diff +4.1 -0.9 +1.6
LLaMA-7BAcc 55.8 62.8 59.3
Acc Diff -3.0 +2.9 -0.1
F1 50.2 59.7 54.9
F1 Diff +3.6 +2.2 +2.9
SAIL-7BAcc 65.8 70.7 68.3
Acc Diff +2.3 +1.5 +1.9
F1 55.2 64.5 59.9
F1 Diff +4.2 +0.9 +2.5
Table 5: Search augmented zero-shot language check-
ing performance on the Climate-fever and PubHealth
benchmarks.
both CSQA and OBQA.
3.3 Fact and Fairness Checking
The other task we evaluate model performance on
is unified fact and fairness checking (Zhang et al.,
2023), a combined benchmark with four sub-tasks
including fact-checking (Diggelmann et al., 2020;
Kotonya and Toni, 2020), hate speech detection
(de Gibert et al., 2018), and stereotype recognition
(Sap et al., 2020). We evaluate the zero-shot perfor-
mance on all four tasks, and the experiment results
are shown in Table 4. The SAIL-7B model achieves
the highest accuracy and F1 scores on all tasks, de-
spite no grounding information being provided for
the fact-checking tasks. We also found that the
Vicuna-7B and13Bmodels perform similarly on
fact and fairness checking.
For the fact-checking tasks, we further evaluate
the performance grounding on search results gener-
ated by DuckDuckGo. Grounding on an external
search engine has both advantages and disadvan-
tages. Many fact checking benchmarks provide
task-specific grounding corpora that limit the do-
main of information retrieval. However, internet
misinformation can be very arbitrary and related to
the latest facts. A commercial search engine is able
to catch a wide range of up-to-date information that
a retrieval model with a fixed knowledge base can-
not achieve. However, search engines are usually
less accurate than dense retrievers, and they mightretrieve disputed documents that influence the qual-
ity of fact checking. Our experiments show that the
search results are not helpful for all models. On
Clmate-Fever, augmenting the model with search
results decreases the overall accuracy of LLaMA by
3%. On the PubHealth task, both accuracy and F1
ofVicuna-13B model are decreased by the search
results, by 4% and 1% respectively. This shows
that the search results contain distracting informa-
tion, which prevents the models to utilize helpful
evidence among noises.
However, SAIL is more robust against distract-
ing languages and its fact-checking performance
is improved on the same set of search results, as
shown in Table 5. With search augmentation, the
fact-checking accuracy and F1 scores of SAIL
are improved on both tasks, as high as 4.2% on
Climate-Fever. The augmented SAIL model also
significantly outperforms all baselines, including
Vicuna-13B andLLaMA-7B tuned with GPT-4 re-
sponses by 9% accuracy and 5% F1, showing the
effectiveness of search augmented fine-tuning.
4 Related Work
4.1 Capabilities
Large language models. Beginning with GPT-3
(Brown et al., 2020a), LLMs have demonstrated
strong abilities in knowledge memorization and
text-based inference on a wide range of tasks.
Well-known LLMs include GPT3, LaMDA
(Thoppilan et al., 2022), FLAN (Wei et al., 2021),
OPT (Zhang et al., 2022), and LLaMA (Touvron
et al., 2023). Compared to smaller language
models, LLMs have several emergent abilities
(Wei et al., 2022a), including zero-shot multi-task
solving, and few-shot in-context learning with
chain-of-thought reasoning (Wei et al., 2022b;
Wang et al., 2022a).
Instruction following. Pretrained LLMs can
generate texts following certain formats and rules
by seeing a few examples in their prompts. To
make LLMs more scalable and improve zero-shot
performance, Ouyang et al. (2022) proposed
training GPT3 with instruction-response corpora.
As a result, InstructGPT, ChatGPT, and GPT4 can
handle a wide range of tasks without seeing any
examples. Recent research has also found that both
GPT-generated instructions and instruct-following
outputs (Peng et al., 2023) can improve the
instruction-following ability of LLMs. (Wang

--- PAGE 9 ---
et al., 2022a) proposed a semi-supervised method
to generate diverse instructions based on a seed
instruction base on NLP tasks (Mishra et al., 2022;
Wang et al., 2022b). A more recent study shows
that GPT-4 (OpenAI, 2023) can generate high-
quality instruction-following language. Recent
efforts on open-sourcing instruction-following
LLMs include Alpaca (Taori et al., 2023) and
Vicuna (Chiang et al., 2023).
Retrieval-augmented language models. Prior
to our work, several initiatives explored retrieval-
augmented language models (RALMs). The pio-
neering approaches – REALM (Guu et al., 2020)
and RAG (Lewis et al., 2020) – sought to train
language models with retrievers in an end-to-end
manner. RETRO (Borgeaud et al., 2022) intro-
duced the idea of training an LM on top of a frozen
retriever. Atlas (Izacard et al., 2022) further ex-
plored dedicated loss functions for the end-to-end
training of the retriever and the LM, achieving su-
perior performance on several few-shot learning
tasks. Recently, RePlug (Shi et al., 2023) and In-
context RALM (Ram et al., 2023) instead explore
an opposite direction: use a frozen black-box LM
while fine-tuning the retrieval modules. RePlug
shows its advantages of leveraging large LMs like
Codex (Chen et al., 2021) and GPT-3 (Brown et al.,
2020b), outperforming Altas on few-shot question-
answering tasks.
Despite the success of RALMs, most of these
models have limitations, including 1) constraining
the search space to a closed corpus like Wikipedia
2) lacking explicit mechanisms for disregarding dis-
tracting search results, and 3) applying a few-shot
in-context learning setting without considering in-
struction fine-tuning during RALM training. Con-
sequently, their applications remain relatively nar-
row, primarily focusing on tasks such as question-
answering and language modeling. SAIL addresses
these limitations by 1) employing real-world search
engines, 2) introducing a search result denoising
process capable of filtering out distracting informa-
tion, and 3) incorporating instruction fine-tuning.
Consequently, SAIL demonstrates its superiority
in broader applications, including instruction fol-
lowing for chatbots, fact and fairness checking, all
of which benefit from access to up-to-date informa-
tion retrieved from real-world search engines.4.2 Trustworthiness
Self-improving. Recent studies have found that
both pretrained and instruction fine-tuned LLMs
can improve themselves with appropriate prompt-
ing strategies. Compared to directly generating
the answers, the step-by-step, chain-of-thought
(Wei et al., 2022b) generation strategy significantly
improves the reasoning accuracy. Furthermore,
self-consistent predictions are usually more
trustworthy (Wang et al., 2022a). Huang et al.
(2022) showed that self-consistent predictions
generated by LLMs can be used as in-context
examples that significantly improve task and
domain adaptation. After instruction fine-tuning,
language models can generate suggestions to
improve their own outputs with self-reflection and
self-refinement prompting strategies (Shinn et al.,
2023; Madaan et al., 2023).
Fact and fairness checking. Aside from an abil-
ity to generate correct responses, we believe that
LLMs should take the responsibility of checking
undesirable and harmful language generated by
both machines and humans. Manakul et al. (2023)
found that the GPT-3 model can identify its own
hallucinations, and Zhang et al. (2023) proposed a
unified fact and fairness checking framework for
both human and machine-generated language.
5 Conclusion
In this work, we found that disputed and dis-
tracting search results can significantly mislead
the predictions of large language models. Sev-
eral transparency-sensitive tasks, including open-
domain question answering and language check-
ing can be negatively influenced by this phe-
nomenon. To solve this problem, we propose a
search-augmented instruction-following large lan-
guage model with 7B parameters. We construct
the first search-augmented instruction-tuning cor-
pus consisting of human-generated instructions,
GPT-4 generated responses, and search results gen-
erated by a BM25 retriever based on Wikipedia
and a commercial search engine. We then fine-
tuned the LLaMA-7B language model with the con-
structed training corpus on academic computational
resources. Experiments on instruction-following,
question answering, and fact/fairness checking
show that the search-augmented language model
can distill trustworthy and helpful information from
all search results and generate high-quality re-

--- PAGE 10 ---
sponses, improving both the performance and trans-
parency of instruction-following large language
models.
Acknowledgement
This research was supported by the Center for Per-
ceptual and Interactive Intelligence (CPII) Ltd un-
der the Innovation and Technology Commission’s
InnoHK Scheme.
Limitations
While the model we propose achieves high per-
formance with efficient model settings, the major
limitation of the model is that it does not explain
why a search result is trustworthy or informative or
not. In future work, we will fine-tune larger mod-
els and enable the models to recognize trustworthy
search results with explanations.
References
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning , pages 2206–2240. PMLR.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020a.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020b. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1870–1879.Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv ,
abs/1803.05457.
Ona de Gibert, Naiara Perez, Aitor Garcıa-Pablos, and
Montse Cuadros. 2018. Hate speech dataset from a
white supremacy forum. EMNLP 2018 , page 11.
Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-
lian, Massimiliano Ciaramita, and Markus Leip-
pold. 2020. Climate-fever: A dataset for verifica-
tion of real-world climate claims. arXiv preprint
arXiv:2012.00614 .
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In International confer-
ence on machine learning , pages 3929–3938. PMLR.
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.
Toxigen: A large-scale machine-generated dataset
for adversarial and implicit hate speech detection.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3309–3326.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,
Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.
Large language models can self-improve. arXiv
preprint arXiv:2210.11610 .
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2022. Few-shot learning with re-
trieval augmented language models. arXiv preprint
arXiv:2208.03299 .
Neema Kotonya and Francesca Toni. 2020. Explainable
automated fact-checking for public health claims. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7740–7754, Online. Association for Computa-
tional Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: A benchmark

--- PAGE 11 ---
for question answering research. Transactions of the
Association for Computational Linguistics , 7:452–
466.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Hongyin Luo and James Glass. 2023. Logic against
bias: Textual entailment mitigates stereotypical sen-
tence reasoning. In Proceedings of the 17th Confer-
ence of the European Chapter of the Association
for Computational Linguistics , pages 1243–1254,
Dubrovnik, Croatia. Association for Computational
Linguistics.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
et al. 2023. Self-refine: Iterative refinement with
self-feedback. arXiv preprint arXiv:2303.17651 .
Potsawee Manakul, Adian Liusie, and Mark JF Gales.
2023. Selfcheckgpt: Zero-resource black-box hal-
lucination detection for generative large language
models. arXiv preprint arXiv:2303.08896 .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. In Conference on Empirical Methods in Natural
Language Processing .
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
InACL.
OpenAI. 2022. Introducing chatgpt.
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019. Pytorch: An imperative style,
high-performance deep learning library. Advances in
neural information processing systems , 32.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277 .
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. arXiv preprint arXiv:2302.00083 .Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-
sky, Noah A Smith, and Yejin Choi. 2020. Social
bias frames: Reasoning about social and power im-
plications of language. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 5477–5490.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
arXiv preprint arXiv:2302.04761 .
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
joon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. 2023. Replug: Retrieval-
augmented black-box language models. arXiv
preprint arXiv:2301.12652 .
Noah Shinn, Beck Labash, and Ashwin Gopinath.
2023. Reflexion: an autonomous agent with dy-
namic memory and self-reflection. arXiv preprint
arXiv:2303.11366 .
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4149–4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. Lamda: Language models for dialog applica-
tions. arXiv preprint arXiv:2201.08239 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2022a. Self-consistency
improves chain of thought reasoning in language
models. arXiv preprint arXiv:2203.11171 .
Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
molabashi, Yeganeh Kordi, Amirreza Mirzaei,
Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, et al.
2022b. Super-naturalinstructions:generalization via
declarative instructions on 1600+ tasks. In EMNLP .

--- PAGE 12 ---
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022a. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei
Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,
Danny Fox, Helen Meng, and James Glass. 2023. In-
terpretable unified language checking. arXiv preprint
arXiv:2304.03728 .
