# 2310.05149.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2310.05149.pdf
# Kích thước tệp: 232796 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
CÁC MÔ HÌNH NGÔN NGỮ LỚN ĐƯỢC TĂNG CƯỜNG BẰNG TƯƠNG TÁC TRUY XUẤT-SINH TẠO
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin
Viện Công nghệ Harbin, Trung Quốc
TÓM TẮT
Các mô hình ngôn ngữ lớn được tăng cường với các tài liệu liên quan đến nhiệm vụ đã thể hiện hiệu suất ấn tượng trên các nhiệm vụ chuyên sâu về kiến thức. Tuy nhiên, về cách thức thu thập các tài liệu hiệu quả, các phương pháp hiện tại chủ yếu được chia thành hai loại. Một là truy xuất từ cơ sở kiến thức bên ngoài, và cái khác là sử dụng các mô hình ngôn ngữ lớn để sinh tạo tài liệu. Chúng tôi đề xuất một khung cộng tác truy xuất-sinh tạo lặp đi lặp lại. Nó không chỉ có thể tận dụng cả kiến thức tham số và phi tham số, mà còn giúp tìm ra đường dẫn lý luận chính xác thông qua các tương tác truy xuất-sinh tạo, điều này rất quan trọng đối với các nhiệm vụ yêu cầu lý luận nhiều bước. Chúng tôi tiến hành thí nghiệm trên bốn bộ dữ liệu hỏi đáp, bao gồm các nhiệm vụ QA một bước và nhiều bước. Kết quả thực nghiệm cho thấy phương pháp của chúng tôi cải thiện đáng kể khả năng lý luận của các mô hình ngôn ngữ lớn và vượt trội hơn các đường cơ sở trước đó.
Từ khóa —mô hình ngôn ngữ lớn, tăng cường truy xuất, hỏi đáp
1. GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện hiệu suất ấn tượng trên các nhiệm vụ ngôn ngữ đa dạng thông qua học trong ngữ cảnh [1, 2, 3, 4, 5, 6]. Tuy nhiên, chúng vẫn gặp khó khăn với các nhiệm vụ chuyên sâu về kiến thức yêu cầu truy cập vào một lượng lớn kiến thức, như hỏi đáp miền mở [7] và lý luận thông thường [8], vì kiến thức ẩn được bảo tồn trong các tham số có thể là một phần và không đủ. Như được thể hiện ở phía trên của Hình 1, một hướng đầy triển vọng là kết hợp kiến thức phi tham số để giúp giảm bớt vấn đề này với các mô hình ngôn ngữ lớn.
Nghiên cứu gần đây cho thấy việc truy xuất các tài liệu liên quan từ một kho dữ liệu bên ngoài [9, 10, 11] hoặc trực tiếp sinh tạo các tài liệu ngữ cảnh từ LLM [12, 13] đều có thể cải thiện hiệu suất của LLM trên các nhiệm vụ chuyên sâu về kiến thức. Cái trước, được gọi là truy xuất-rồi-đọc, yêu cầu một trình truy xuất để truy xuất các tài liệu liên quan. Cái sau, được biết đến như sinh tạo-rồi-đọc, tận dụng các mô hình ngôn ngữ lớn để sinh tạo các tài liệu liên quan trước khi trả lời câu hỏi. Tuy nhiên, như được thể hiện trong Hình 1, hai phương pháp trên bị cô lập và thiếu sự phối hợp với nhau. Để lấp đầy khoảng trống này, trong bài báo này, chúng tôi khám phá một khung cộng tác truy xuất-sinh tạo hiệu quả
Ai là ca sĩ chính của Depeche Mode? Depeche Mode hiện tại bao gồm Dave Gahan (giọng chính, đồng sáng tác) và Martin Gore (bàn phím, guitar, giọng phụ, sáng tác chính)… LLM Câu hỏi Tài liệu Dave Gahan Trả lời
Câu hỏi LLM Trình truy xuất LLM Trình truy xuất Tài liệu Câu hỏi
Tài liệu Câu hỏi
Tài liệu (1) Truy xuất (2) Sinh tạo (3) Tương tác Truy xuất-Sinh tạo Hình 1: Phía trên là phương pháp tiêu chuẩn sử dụng LLM để
hỏi đáp với các tài liệu liên quan. Phía dưới thể hiện ba phương pháp để sinh tạo các tài liệu liên quan.
work để cải thiện thêm khả năng của các mô hình ngôn ngữ lớn trong việc giải quyết các nhiệm vụ chuyên sâu về kiến thức.
Trong công trình này, chúng tôi trình bày ITRG, một khung tương tác Truy xuất-Sinh tạo Lặp lại để sinh tạo các tài liệu liên quan đồng thời khai thác kiến thức tham số và phi tham số. Trong mỗi vòng lặp, ITRG bao gồm hai bước quan trọng: truy xuất tăng cường sinh tạo (GAR) và sinh tạo tăng cường truy xuất (RAG). Trong bước GAR, chúng tôi đề xuất một phương pháp đơn giản và hiệu quả để mở rộng truy vấn bằng cách nối các tài liệu giả được sinh tạo từ các mô hình ngôn ngữ lớn và các câu hỏi gốc. Và các truy vấn được mở rộng cải thiện độ chính xác của việc truy xuất các tài liệu liên quan. Trong bước RAG, chúng tôi sử dụng các mô hình ngôn ngữ lớn để hiểu toàn diện các tài liệu được truy xuất để sinh tạo các tài liệu mới cho việc trả lời câu hỏi. Chúng tôi lặp lại các bước này cho đến khi đạt số lượng vòng lặp tối đa được cho phép. Thông qua nhiều lần cộng tác truy xuất sinh tạo, phương pháp của chúng tôi hỗ trợ khám phá đường dẫn lý luận phù hợp và cung cấp câu trả lời chính xác cho các câu hỏi.
Chúng tôi đánh giá hiệu quả của phương pháp trên 4 bộ dữ liệu hỏi đáp, bao gồm Natural Questions, TriviaQA, 2WikiMultiHopQA, và HotpotQA. Kết quả thực nghiệm cho thấy phương pháp của chúng tôi hoạt động tốt hơn các đường cơ sở trước đó trên tất cả các bộ dữ liệu. Tóm lại, các đóng góp chính của chúng tôi có thể được tóm tắt như sau: (1) Chúng tôi đề xuất ITRG, một khung tương tác truy xuất-sinh tạo lặp lại sử dụng cả kiến thức tham số và phi tham số. (2) Chúng tôi đề xuất một chiến lược truy xuất tăng cường sinh tạo đơn giản và hiệu quả và hai chiến lược sinh tạo tăng cường truy xuất. (3) Kết quả thực nghiệm cho thấy ITRG vượt trội hơn các phương pháp tăng cường truy xuất trước đó.arXiv:2310.05149v1 [cs.CL] 8 Oct 2023

--- TRANG 2 ---
2. TƯƠNG TÁC TRUY XUẤT-SINH TẠO LẶP LẠI
Trong phần này, chúng tôi trước tiên giới thiệu khung tổng thể, và sau đó giới thiệu chi tiết khung cộng tác truy xuất-sinh tạo, bao gồm truy xuất tăng cường sinh tạo và sinh tạo tăng cường truy xuất.
2.1. Tổng quan
Chúng tôi thể hiện khung của ITRG trong Hình 2. Cho một câu hỏi người dùng q và một kho tài liệu D={di}|D|i=1 (tức là, di là một đoạn văn Wikipedia.), ITRG lặp lại truy xuất tăng cường sinh tạo (GAR) và sinh tạo tăng cường truy xuất (RAG) trong T vòng lặp. Trong quá trình GAR của vòng lặp t, chúng tôi nối đầu ra yt−1 của vòng lặp trước và câu hỏi q để tạo thành một truy vấn mới, và sau đó sử dụng một trình truy xuất dày đặc để truy xuất top-k đoạn văn. Trong vòng lặp đầu tiên, chúng tôi chỉ sử dụng câu hỏi làm truy vấn. Trong quá trình RAG của vòng lặp t, dựa trên câu hỏi q và top-k đoạn văn được truy xuất, chúng tôi khai thác các mô hình ngôn ngữ lớn để sinh tạo các đoạn văn mới để trả lời câu hỏi. Cụ thể, chúng tôi đề xuất hai phương pháp để sinh tạo các đoạn văn mới, sẽ được giới thiệu chi tiết trong §2.3.
2.2. Truy xuất Tăng cường Sinh tạo
Các nhiệm vụ chuyên sâu về kiến thức (ví dụ, hỏi đáp miền mở) thường yêu cầu truy cập vào các tài liệu bổ sung. Một cách tiếp cận phổ biến là trực tiếp sử dụng câu hỏi làm truy vấn, và sau đó trang bị một trình truy xuất thưa thớt hoặc dày đặc để truy xuất các tài liệu liên quan. Trong thực tế, chúng tôi thấy rằng trong một số trường hợp việc sử dụng trực tiếp câu hỏi làm truy vấn không thể truy xuất các tài liệu liên quan vì có thể tồn tại khoảng cách ngữ nghĩa giữa chúng. Để giảm bớt vấn đề này, chúng tôi đề xuất một phương pháp mở rộng truy vấn đơn giản. Ở vòng lặp đầu tiên (t = 1), chúng tôi sử dụng câu hỏi gốc q làm truy vấn. Ở vòng lặp t (t > 1), chúng tôi nối câu hỏi gốc q và tài liệu được sinh tạo yt−1 trong vòng lặp trước làm truy vấn mới qt = [q; yt−1]. Sau đó, chúng tôi sử dụng một trình truy xuất dày đặc được đào tạo trước để truy xuất top-k tài liệu, được ký hiệu là Rt={d}.
Cho một câu hỏi đầu vào q, trình truy xuất nhằm truy xuất một tập hợp nhỏ các tài liệu từ kho D={di}|D|i=1 có liên quan đến q. Theo công trình trước đó [14], chúng tôi sử dụng một trình truy xuất dày đặc dựa trên kiến trúc mã hóa kép, trong đó một bộ mã hóa được sử dụng để mã hóa cả ngữ cảnh đầu vào q và tài liệu d. Cụ thể, bộ mã hóa ánh xạ mỗi tài liệu d ∈ D thành một embedding E(d) bằng cách lấy trung bình pooling của biểu diễn ẩn cuối cùng trên các token trong d. Tại thời điểm truy vấn, cùng một bộ mã hóa được áp dụng cho ngữ cảnh đầu vào q để thu được một embedding truy vấn E(q). Độ tương tự giữa embedding truy vấn và embedding tài liệu được tính bằng độ tương tự cosine của chúng: s(d, q) = cos(E(d), E(q)). Top-k tài liệu có điểm tương tự cao nhất được truy xuất.
Câu hỏi: Ngày sinh của mẹ Emilie HeghArntzen là gì? Truy xuất: Sinh tạo: Truy xuất: Sinh tạo: Truy xuất: Sinh tạo: Vòng lặp 1 Vòng lặp 2
Vòng lặp 3 infobox tên: Emilie HeghArntzen; chú thích: HeghArntzen năm 2018; ngày_sinh: 1 tháng 1, 1994; nơi_sinh: Skien, Na Uy; quốc tịch: Na Uy; Emilie HeghArntzen sinh ngày 1 tháng 1, 1994 tại Skien, Na Uy. Mẹ của cô không rõ. Camilla Marie Gjersem sinh cùng với một chị em sinh đôi, Anne Line, vào ngày 6 tháng 1, 1994 tại Hønefoss, Na Uy. Mẹ họ, PerlinaBangug, là người Philippines từ Ilagan, Isabela, và cha họ, PetterGjersem, là người Na Uy từ Raufoss. Camilla Gjersem là sinh viên luật tại Đại học Oslo. Hanne Hegh (sinh ngày 19 tháng 1, 1960) là vận động viên bóng ném Na Uy. Cô đã chơi 220 trận cho đội tuyển bóng ném quốc gia Na Uy từ 1978 đến 1992. Cô là mẹ của Emilie HeghArntzen. infobox tên: Hanne Hegh; chú thích: Hanne Hegh 2008; quốc tịch: Na Uy; ngày_sinh: 27 tháng 4, 1960; nơi_sinh: Oslo, Na Uy; Hanne Hegh sinh ngày 27 tháng 4, 1960 tại Oslo, Na Uy. Cô là mẹ của Emilie HeghArntzen, người sinh ngày 1 tháng 1, 1994 tại Skien, Na Uy. Hình 2: Khung tương tác truy xuất-sinh tạo lặp lại bao
gồm hai bước trong mỗi vòng lặp: (1) truy xuất tăng cường sinh tạo (GAR): sử dụng đầu ra của vòng lặp trước để mở rộng truy vấn nhằm giúp truy xuất các tài liệu liên quan hơn; (2) sinh tạo tăng cường truy xuất (RAG): sử dụng các tài liệu được truy xuất để sinh tạo các tài liệu mới để trả lời câu hỏi. Chúng tôi chỉ thể hiện ba vòng lặp trong hình này để ngắn gọn. Mũi tên liền thể hiện RAG trong một vòng lặp, và mũi tên đứt nét thể hiện GAR giữa các vòng lặp. Màu tím thể hiện thông tin chính xác và hữu ích, màu đỏ thể hiện thông tin sai hoặc không hợp lệ.
2.3. Sinh tạo Tăng cường Truy xuất
Theo công trình trước đó [13], đối với một câu hỏi cho trước q, chúng ta có thể trực tiếp nhắc các mô hình ngôn ngữ lớn để sinh tạo các tài liệu liên quan mà không cần truy xuất chúng từ một kho bên ngoài. Tuy nhiên, chúng tôi thấy rằng nếu chỉ sử dụng kiến thức tham số được học bởi mô hình lớn trong giai đoạn đào tạo trước, các tài liệu được sinh tạo có thể không đầy đủ. Sinh tạo tăng cường truy xuất (RAG) nhằm hiểu toàn diện kiến thức phi tham số được truy xuất và kiến thức tham số bên trong các mô hình ngôn ngữ lớn để sinh tạo kiến thức thực tế chính xác hơn. Cụ thể, chúng tôi đề xuất hai chiến lược, sẽ được mô tả chi tiết dưới đây.
2.3.1. Tinh chỉnh
Một ý tưởng trực quan là tinh chỉnh tài liệu được sinh tạo trước đó yt−1 dựa trên câu hỏi gốc q và top-k tài liệu được truy xuất ở bước vòng lặp hiện tại Rt để thu được một tài liệu mới yt. Chúng tôi gọi phương pháp này là tinh chỉnh. Xem xét rằng tài liệu được truy xuất trong vòng lặp trước Rt−1 đã được sử dụng để sinh tạo tài liệu cuối yt−1, chúng tôi tinh chỉnh đầu ra trước đó yt−1 với các tài liệu được cập nhật Rupdate.
Rupdate = Rt − Rt−1, (1)
yt = M(prompt(yt−1, q, Rupdate)), (2)

--- TRANG 3 ---
trong đó Rupdate có nghĩa là những tài liệu này chỉ được truy xuất trong vòng lặp hiện tại, không phải trong vòng lặp trước, M biểu thị một mô hình ngôn ngữ lớn được đào tạo trước tốt. Nếu Rupdate là một tập hợp rỗng, chúng tôi không sinh tạo lại tài liệu mới và đặt yt = yt−1.
2.3.2. Làm mới
Để tránh tác động tiêu cực của lỗi hoặc ảo giác trong tài liệu được sinh tạo trước đó yt−1, chúng tôi không sử dụng yt−1, được sử dụng trong tinh chỉnh. Chúng tôi làm mới bộ nhớ và để các mô hình ngôn ngữ lớn trực tiếp sinh tạo tài liệu yt dựa trên tài liệu được truy xuất Rt và câu hỏi gốc q. Phương pháp này được đặt tên là làm mới.
yt = M(prompt(q, Rt)) (3)
Cả tinh chỉnh và làm mới đều được thực hiện thông qua các prompt. Chúng tôi đưa ra prompt tương ứng với làm mới.
Prompt cho làm mới với tất cả tài liệu
Trong nhiệm vụ sau đây, bạn nên viết một tài liệu
chứa câu trả lời cho câu hỏi.
Đoạn văn: {Rt}
Câu hỏi: {q}
Tài liệu: {yt}
3. THIẾT LẬP THỰC NGHIỆM
3.1. Bộ dữ liệu
Chúng tôi đánh giá hiệu quả của ITRG trên bốn bộ dữ liệu hỏi đáp miền mở, bao gồm Natural Questions (NQ) [15], TriviaQA [16], 2WikiMultiHopQA [17] và HotpotQA [18]. Theo các công trình trước đó [19, 20], chúng tôi ngẫu nhiên lấy mẫu phụ 500 ví dụ từ mỗi bộ dữ liệu do chi phí chạy thực nghiệm. Chúng tôi đánh giá phương pháp của mình trong các thiết lập 0-shot, 1-shot và 5-shot. Các minh chứng few-shot được lấy mẫu ngẫu nhiên từ dữ liệu không tham gia vào quá trình đánh giá.
3.2. Đường cơ sở
GPT-3.5 [21] Chúng tôi sử dụng text-davinci-002 và text-davinci-003 làm đường cơ sở. Text-davinci-002 là một mô hình InstructGPT trong khi Text-davinci-003 được đào tạo với học tăng cường với các mô hình phần thưởng được đào tạo từ so sánh của con người.
Vanilla LM Các đường cơ sở Vanilla LM nhắc một LLM để trực tiếp sinh tạo câu trả lời theo mô hình học trong ngữ cảnh few-shot [1]. CoT Chúng tôi theo [22] để sinh tạo cả quá trình lý luận chuỗi suy nghĩ (CoT) và câu trả lời cuối cùng. Chúng tôi chỉ đánh giá phương pháp này trên các bộ dữ liệu lý luận nhiều bước trong thiết lập 5-shot1. Truy xuất-rồi-Đọc Đường cơ sở truy xuất-rồi-đọc bao gồm một trình truy xuất dày đặc được đào tạo trước tốt và một mô hình ngôn ngữ lớn. Trình truy xuất truy xuất các tài liệu liên quan cho câu hỏi, và sau đó LLM điều kiện hóa trên cả câu hỏi và các tài liệu được truy xuất để sinh tạo câu trả lời. Sinh tạo-rồi-Đọc Đường cơ sở sinh tạo-rồi-đọc trước tiên sử dụng các prompt few-shot để sinh tạo một tài liệu liên quan đến câu hỏi, và sau đó nối nó với câu hỏi để sinh tạo lại câu trả lời.
3.3. Chi tiết
LLaMA [6] là một mô hình ngôn ngữ lớn được đào tạo tốt mã nguồn mở. Xem xét hiệu suất và chi phí tính toán của mô hình, chúng tôi sử dụng LLaMA 33B làm LLM phụ trợ. Chúng tôi sử dụng giải mã tham lam cho cả sinh tạo tài liệu và sinh tạo câu trả lời, và thiết lập để sinh tạo tối đa 200 token và 15 token tương ứng. Chúng tôi truy xuất top-5 đoạn văn cho mỗi truy vấn và đặt số lượng vòng lặp tối đa T là 5. Chúng tôi trực tiếp sử dụng trình truy xuất dày đặc được đào tạo trước [23] và sử dụng bản dump Wikipedia tháng 12 năm 2018 làm kho truy xuất cho tất cả các bộ dữ liệu. Các câu trả lời được sinh tạo được đánh giá bằng thước đo khớp chính xác tiêu chuẩn (điểm EM): một câu trả lời được sinh tạo được coi là chính xác nếu nó khớp với bất kỳ câu trả lời nào trong danh sách câu trả lời sau khi chuẩn hóa. Đối với bước chuẩn hóa này, chúng tôi chuyển các câu trả lời được sinh tạo thành chữ thường và loại bỏ các mạo từ, dấu câu và khoảng trắng trùng lặp.
4. KẾT QUẢ
4.1. Kết quả chính
Bảng 1 báo cáo kết quả trên các bộ dữ liệu hỏi đáp một bước. Trong các thiết lập 1-shot và 5-shot, hiệu suất của Vanilla LM dựa trên LLaMA-33B rất gần với text-davinci-003. Điều này cho thấy LLaMA-33B là một mô hình ngôn ngữ mạnh, và việc chọn LLaMA-33B làm LLM phụ trợ là hợp lý. Truy xuất-rồi-đọc và sinh tạo-rồi-đọc đều vượt vanilla LM, xác minh rằng việc thêm kiến thức bên ngoài liên quan có thể cải thiện khả năng lý luận của các mô hình ngôn ngữ lớn. Ngoài ra, chúng tôi quan sát thấy phương pháp cộng tác truy xuất-sinh tạo lặp lại của chúng tôi ITRG đạt hiệu suất tiên tiến trên cả hai bộ dữ liệu. Cụ thể, ITRG (làm mới) hoạt động tốt hơn trên bộ dữ liệu NQ, và ITRG (tinh chỉnh) hoạt động tốt hơn trên bộ dữ liệu TriviaQA.
Bảng 2 trình bày kết quả trên các bộ dữ liệu hỏi đáp nhiều bước. Chúng tôi quan sát thấy LLaMA-33B vẫn có thể so sánh với text-davinci-003 trên các bộ dữ liệu hỏi đáp nhiều bước. Ngoài ra, CoT có thể trả lời câu hỏi chính xác hơn vanilla LM bằng cách sinh tạo quá trình lý luận. So với các mô hình đường cơ sở khác nhau, ITRG cải thiện đáng kể điểm khớp chính xác. Cụ thể, trên bộ dữ liệu 2WikiMultiHopQA, điểm khớp chính xác của ITRG (làm mới) trong thiết lập zero-shot là 32.2, vượt hiệu suất của vanilla LM trong thiết lập 5-shot với điểm 31.8. Trong thiết lập 5-shot, ITRG (làm mới) đạt điểm EM 38.6
1Chúng tôi cũng tiến hành đánh giá trong thiết lập 1-shot, nhưng câu trả lời cuối cùng không thể được sinh tạo theo hướng dẫn tương ứng

--- TRANG 4 ---
Bảng 1: Hiệu suất khớp chính xác trên hỏi đáp một bước. Tất cả kết quả ITRG đều từ vòng lặp cuối cùng (T = 5).
Phương pháp Natural Questions TriviaQA
0-shot 1-shot 5-shot 0-shot 1-shot 5-shot
GPT 3.5 Text-davinci-002 12.0 24.6 33.0 46.0 74.2 76.0
Text-davinci-003 29.4 33.0 33.8 75.8 78.6 77.8
LLaMA 33B Vanilla LM 27.0 29.4 32.4 74.8 70.8 75.8
Truy xuất-rồi-Đọc 27.8 30.6 29.8 74.6 76.0 76.0
Sinh tạo-rồi-Đọc 28.0 31.4 31.0 73.6 77.2 77.6
ITRG (tinh chỉnh) 34.4 34.6 34.8 79.0 79.4 80.6
ITRG (làm mới) 37.6 38.4 38.0 77.0 78.6 79.4
Bảng 2: Hiệu suất khớp chính xác trên hỏi đáp nhiều bước. Tất cả kết quả ITRG đều từ vòng lặp cuối cùng (T = 5).
Phương pháp 2WikiMultiHopQA HotpotQA
0-shot 1-shot 5-shot 0-shot 1-shot 5-shot
GPT 3.5 Text-davinci-002 16.4 27.6 30.8 12.2 20.2 22.2
Text-davinci-003 27.2 27.0 29.8 25.0 25.8 26.6
LLaMA 33B Vanilla LM 24.4 27.6 31.8 22.6 25.0 27.0
COT - - 32.2 - - 28.6
Truy xuất-rồi-Đọc 27.4 29.2 32.0 28.4 29.8 30.4
Sinh tạo-rồi-Đọc 30.0 30.4 31.6 25.0 27.0 27.0
ITRG (tinh chỉnh) 33.0 33.6 37.0 28.8 29.6 30.6
ITRG (làm mới) 32.2 36.2 38.6 31.0 32.6 33.4
Bảng 3: Hiệu suất khớp chính xác của ITRG (làm mới) ở các vòng lặp khác nhau trong thiết lập 5-shot.
Vòng lặp 1 2 3 4 5
Natural Questions 34.0 35.2 37.0 37.2 38.0
TriviaQA 79.8 79.2 79.8 79.8 79.4
2WikiMultiHopQA 34.8 37.4 37.2 38.6 38.6
HotpotQA 32.6 32.8 34.0 33.4 33.4
và cải thiện 6.8 điểm tuyệt đối. So với vanilla LM, ITRG (làm mới) có thể cải thiện điểm EM lần lượt 9.4, 7.6, và 6.4 điểm trong các thiết lập 0-shot, 1-shot, và 5-shot trên bộ dữ liệu Hotpotqa.
4.2. Hiệu suất ở Các Vòng lặp Khác nhau
Trong phần này, chúng tôi phân tích hiệu suất của mô hình và chất lượng của các tài liệu được sinh tạo trong quá trình lặp. Cụ thể, chúng tôi trình bày kết quả của ITRG (làm mới) ở các vòng lặp khác nhau trong thiết lập 5-shot trong Bảng 3. Chúng tôi đo recall câu trả lời của các tài liệu được sinh tạo ở các bước vòng lặp khác nhau và trình bày kết quả trong Bảng 4. Bảng 3 cho thấy hiệu suất của mô hình dần cải thiện theo vòng lặp. Và Bảng 4 cho thấy chất lượng của các tài liệu được sinh tạo cũng dần cải thiện theo vòng lặp. Những kết quả này xác minh rằng khung cộng tác truy xuất-sinh tạo lặp lại của chúng tôi
Bảng 4: Recall câu trả lời của các tài liệu được sinh tạo ở các vòng lặp khác nhau với ITRG (làm mới).
Vòng lặp 1 2 3 4 5
Natural Questions 44.0 46.4 48.4 48.8 48.0
TriviaQA 18.8 19.0 20.2 19.2 19.2
2WikiMultiHopQA 34.2 36.6 35.0 40.0 37.0
HotpotQA 34.2 34.8 35.6 33.8 33.6
hiệu quả và có thể tăng cường thêm khả năng lý luận của các mô hình ngôn ngữ lớn.
5. KẾT LUẬN
Trong bài báo này, chúng tôi trình bày ITRG, một khung tương tác truy xuất-sinh tạo lặp lại, bao gồm hai bước quan trọng: truy xuất tăng cường sinh tạo và sinh tạo tăng cường truy xuất. Chúng tạo thành một vòng lặp khép kín, và có thể cải thiện lẫn nhau qua nhiều vòng lặp. Chúng tôi đề xuất một chiến lược truy xuất tăng cường sinh tạo đơn giản và hiệu quả và hai chiến lược sinh tạo tăng cường truy xuất. Kết quả thực nghiệm cho thấy cách tiếp cận của chúng tôi vượt trội đáng kể so với một số đường cơ sở mạnh, bao gồm GPT 3.5, trên bốn bộ dữ liệu hỏi đáp miền mở, điều này chỉ ra rằng phương pháp của chúng tôi có thể cải thiện đáng kể khả năng lý luận của các mô hình ngôn ngữ lớn.

--- TRANG 5 ---
6. TÀI LIỆU THAM KHẢO
[1] T. Brown et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.
[2] J. Hoffmann et al., "Training compute-optimal large language models," 2022.
[3] A. Zeng et al., "Glm-130b: An open bilingual pre-trained model," arXiv preprint arXiv:2210.02414, 2022.
[4] A. Chowdhery et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[5] OpenAI, "Gpt-4 technical report," 2023.
[6] H. Touvron et al., "Llama: Open and efficient foundation language models," 2023.
[7] K. Lee, M.-W. Chang, and K. Toutanova, "Latent retrieval for weakly supervised open domain question answering," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 6086–6096. [Online]. Available: https://aclanthology.org/P19-1612
[8] R. Zellers, Y. Bisk, R. Schwartz, and Y. Choi, "SWAG: A large-scale adversarial dataset for grounded commonsense inference," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 93–104. [Online]. Available: https://www.aclweb.org/anthology/D18-1009
[9] O. Ram et al., "In-context retrieval-augmented language models," arXiv preprint arXiv:2302.00083, 2023.
[10] O. Khattab et al., "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp," 2023.
[11] W. Shi et al., "Replug: Retrieval-augmented black-box language models," arXiv preprint arXiv:2301.12652, 2023.
[12] W. Yu et al., "Generate rather than retrieve: Large language models are strong context generators," 2023.
[13] Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, "Recitation-augmented language models," 2023.
[14] G. Izacard and E. Grave, "Leveraging passage retrieval with generative models for open domain question answering," arXiv preprint arXiv:2007.01282, 2020.
[15] T. Kwiatkowski et al., "Natural questions: A benchmark for question answering research," Transactions of the Association for Computational Linguistics, vol. 7, pp. 452–466, 2019. [Online]. Available: https://aclanthology.org/Q19-1026
[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension," in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online]. Available: https://aclanthology.org/P17-1147
[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa, "Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps," in Proceedings of the 28th International Conference on Computational Linguistics. Barcelona, Spain (Online): International Committee on Computational Linguistics, Dec. 2020, pp. 6609–6625. [Online]. Available: https://aclanthology.org/2020.coling-main.580
[18] Z. Yang et al., "HotpotQA: A dataset for diverse, explainable multi-hop question answering," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2369–2380. [Online]. Available: https://aclanthology.org/D18-1259
[19] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions," arXiv preprint arXiv:2212.10509, 2022.
[20] Z. Jiang et al., "Active retrieval augmented generation," arXiv preprint arXiv:2305.06983, 2023.
[21] L. Ouyang et al., "Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27 730–27 744, 2022.
[22] J. Wei et al., "Chain of thought prompting elicits reasoning in large language models," arXiv preprint arXiv:2201.11903, 2022.
[23] G. Izacard et al., "Few-shot learning with retrieval augmented language models," arXiv preprint arXiv:2208.03299, 2022.
