# 2310.15594.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2310.15594.pdf
# Kích thước tệp: 900660 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Chuyển giao Kiến thức dựa trên Truy xuất: Một Phương pháp Hiệu quả cho Nén Mô hình Ngôn ngữ Lớn Cực đoan
Jiduan Liu1,2*, Jiahao Liu3*, Qifan Wang4, Jingang Wang3, Xunliang Cai3
Dongyan Zhao1,2,5,6 †, Ran Lucien Wang7, Rui Yan7†
1Viện Công nghệ Máy tính Wangxuan, Đại học Bắc Kinh
2Trung tâm Khoa học Dữ liệu, AAIS, Đại học Bắc Kinh;3Meituan;4Meta AI
5Phòng thí nghiệm Trọng điểm Quốc gia về Trí tuệ Nhân tạo Tổng quát;6BIGAI, Bắc Kinh, Trung Quốc
7Trường Trí tuệ Nhân tạo Gaoling, Đại học Nhân dân Trung Quốc
{liujiduan,zhaody}@pku.edu.cn ,ruiyan@ruc.edu.cn ,wqfcr@fb.com
{liujiahao12,wangjingang02,caixunliang}@meituan.com ,ran.wang.math@gmail.com

Tóm tắt
Các mô hình ngôn ngữ được huấn luyện trước quy mô lớn (LLMs) đã thể hiện hiệu suất xuất sắc trong nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) khác nhau. Tuy nhiên, kích thước khổng lồ của các mô hình này đặt ra những thách thức lớn cho việc triển khai chúng trong các ứng dụng thực tế. Mặc dù đã có nhiều kỹ thuật nén mô hình được đề xuất, hầu hết chúng không phù hợp để đạt được nén mô hình cực đoan khi có khoảng cách đáng kể về quy mô mô hình. Trong bài báo này, chúng tôi giới thiệu một mô hình nén mới được gọi là Chuyển giao Kiến thức dựa trên Truy xuất (RetriKT), có thể chuyển giao hiệu quả kiến thức của LLMs cho các mô hình quy mô cực nhỏ (ví dụ: 1%). Cụ thể, phương pháp của chúng tôi trích xuất kiến thức từ LLMs để xây dựng một kho kiến thức, từ đó mô hình quy mô nhỏ có thể truy xuất thông tin liên quan và tận dụng nó để suy luận hiệu quả. Để cải thiện chất lượng mô hình, các kỹ thuật điều chỉnh soft prompt và học tăng cường Proximal Policy Optimization (PPO) được sử dụng. Các thí nghiệm rộng rãi được thực hiện trên các tác vụ ít tài nguyên từ các benchmark SuperGLUE và GLUE. Kết quả cho thấy phương pháp đề xuất cải thiện đáng kể hiệu suất của các mô hình quy mô nhỏ bằng cách tận dụng kiến thức từ LLMs.

1 Giới thiệu
Các mô hình ngôn ngữ được huấn luyện trước (PLMs), như BERT/RoBERTa (Devlin et al., 2019; Liu et al., 2019), đã thể hiện hiệu suất xuất sắc trong nhiều ứng dụng xử lý ngôn ngữ tự nhiên (NLP). Tuy nhiên, các mô hình này thường bao gồm hàng trăm triệu tham số, tạo ra thách thức lớn cho các nhà nghiên cứu do quy mô khổng lồ của chúng. Kết quả là, tiềm năng đầy đủ của các mô hình ngôn ngữ được huấn luyện trước quy mô lớn (PLMs) vẫn chưa được khai thác. Để giải quyết thách thức này, nhiều kỹ thuật nén mô hình đã được đề xuất, bao gồm chưng cất kiến thức (Sanh et al., 2019; Jiao et al., 2020; Passban et al., 2021), cắt tỉa mạng (Liang et al., 2021a; Gordon et al., 2020), lượng tử hóa (Zhang et al., 2020; Tao et al., 2022) và chia sẻ trọng số (Lan et al., 2020).

Tuy nhiên, các phương pháp nén mô hình này không thể áp dụng trực tiếp cho các tình huống yêu cầu tỷ lệ nén cao, như chưng cất kiến thức. Trong những trường hợp như vậy, việc giới thiệu các mô hình hỗ trợ (Mirzadeh et al., 2020; Son et al., 2021) thường dẫn đến hiệu suất giảm và không ổn định. Gần đây, đã có sự quan tâm ngày càng tăng đối với việc tận dụng các mô hình ngôn ngữ lớn (LLMs) (Touvron et al., 2023; Zeng et al., 2022; Ouyang et al., 2022; Scao et al., 2022) có kiến thức ngôn ngữ rộng lớn và có thể được sử dụng hiệu quả trong nhiều tác vụ downstream khác nhau. Do đó, việc khám phá các phương pháp chuyển giao kiến thức này cho các mô hình quy mô nhỏ là điều cần thiết. Tuy nhiên, các phương pháp hiện tại không đủ để nén LLMs do tỷ lệ nén cực kỳ cao của chúng. Một số nghiên cứu trước đây (Wang et al., 2022; Dai et al., 2023; Ubani et al., 2023) đã đề xuất sử dụng LLMs để tăng cường dữ liệu và chuyển giao kiến thức cho các mô hình quy mô nhỏ, cho phép những mô hình sau thể hiện hiệu suất cải thiện trên các tập dữ liệu ít tài nguyên. Tuy nhiên, khi giải quyết các tác vụ thách thức hơn như benchmark SuperGLUE (Wang et al., 2019a), kích thước tham số hạn chế của các mô hình quy mô nhỏ trở thành trở ngại, ngăn chúng lưu giữ hiệu quả kiến thức được chuyển giao bởi LLMs. Do đó, việc cải thiện hiệu suất đạt được cho các mô hình quy mô nhỏ vẫn bị hạn chế.

--- TRANG 2 ---
Để chuyển giao hiệu quả kiến thức của các Mô hình Ngôn ngữ Lớn (LLMs) cho các mô hình quy mô nhỏ, cho phép chúng hoàn thành các tác vụ một cách hiệu quả và chính xác, chúng tôi đề xuất một mô hình nén mới được gọi là Chuyển giao Kiến thức dựa trên Truy xuất (RetriKT). Phương pháp của chúng tôi bao gồm hai bước chính: trích xuất kiến thức từ LLM để xây dựng kho kiến thức, và sau đó truy xuất thông tin liên quan từ kho kiến thức bởi mô hình quy mô nhỏ để hoàn thành tác vụ. Cụ thể hơn, chúng tôi sử dụng kỹ thuật điều chỉnh soft prompt để tinh chỉnh LLM, đảm bảo rằng nó tạo ra các mẫu trong miền. Thêm vào đó, chúng tôi giới thiệu thuật toán học tăng cường Proximal Policy Optimization (PPO) (Schulman et al., 2017) để nâng cao chất lượng sinh. Cuối cùng, mô hình quy mô nhỏ học cách truy xuất thông tin liên quan từ kho kiến thức.

Chúng tôi thực hiện một tập hợp thử nghiệm rộng rãi trên các tác vụ thực sự ít tài nguyên và thách thức từ các benchmark SuperGLUE (Wang et al., 2019a) và GLUE (Wang et al., 2019b). Kết quả thử nghiệm chứng minh rằng RetriKT cải thiện đáng kể hiệu suất của các mô hình quy mô nhỏ và vượt trội hơn các phương pháp chưng cất kiến thức SOTA trước đây bằng cách tận dụng kiến thức của LLMs. Điều này cho thấy hiệu quả và tính thực tiễn của mô hình chuyển giao kiến thức dựa trên truy xuất cho nén mô hình cực đoan.

Đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi đề xuất một mô hình nén mới được gọi là Chuyển giao Kiến thức dựa trên Truy xuất, nhằm chuyển giao kiến thức từ LLMs cho các mô hình quy mô cực nhỏ. Mô hình này giải quyết thách thức đạt được nén mô hình cực đoan khi có khoảng cách đáng kể về quy mô mô hình.

• Chúng tôi giới thiệu thuật toán học tăng cường PPO để nâng cao chất lượng sinh, và thiết kế cẩn thận hàm phần thưởng. Kỹ thuật này góp phần cải thiện tính đa dạng và độ chính xác của kiến thức được trích xuất từ LLMs sử dụng cho chuyển giao kiến thức.

• Chúng tôi thực hiện các thử nghiệm rộng rãi trên các tác vụ ít tài nguyên từ các benchmark SuperGLUE và GLUE. Kết quả cho thấy RetriKT cải thiện đáng kể hiệu suất của các mô hình quy mô nhỏ và vượt trội hơn các phương pháp chưng cất kiến thức SOTA trước đây bằng cách tận dụng kiến thức từ LLMs.

2 Công trình Liên quan
Bài báo này liên quan đến ba lĩnh vực nghiên cứu: chưng cất kiến thức, học tăng cường và tăng cường dữ liệu.

2.1 Chưng cất Kiến thức
Trước tiên chúng tôi giới thiệu các công trình liên quan về chưng cất kiến thức (KD), có thể được phân loại thành KD dựa trên phản hồi, dựa trên đặc trưng và dựa trên quan hệ. KD dựa trên phản hồi ban đầu được giới thiệu bởi Hinton et al. (2015), chuyển giao kiến thức nhãn bằng cách tối thiểu hóa KL-divergence giữa các phân phối dự đoán của giáo viên và học sinh. Dựa trên khái niệm này, Sanh et al. (2019); Liang et al. (2021b) đã áp dụng KD dựa trên phản hồi cho các tác vụ như mô hình hóa ngôn ngữ có mặt nạ và phân loại văn bản, tạo ra các mô hình nhỏ hơn với sự giảm hiệu suất nhẹ. Các phương pháp dựa trên đặc trưng ban đầu được giới thiệu bởi Romero et al. (2015), đòi hỏi việc căn chỉnh các kích hoạt đặc trưng giữa các mô hình giáo viên và học sinh. Dựa trên khái niệm này, các kỹ thuật gần đây hơn đã mở rộng phạm vi bằng cách kết hợp các biểu diễn ẩn của token [CLS] làm chỉ báo (Sun et al., 2019), khớp các biểu diễn ẩn của tất cả tokens (Jiao et al., 2020), và giới thiệu chưng cất dựa trên đặc trưng tùy chỉnh (Sun et al., 2020). Mặt khác, các phương pháp dựa trên quan hệ (Park et al., 2021; Liu et al., 2022a; Jiao et al., 2020; Wang et al., 2020, 2021a; Wu et al., 2023) nhằm nhấn mạnh tầm quan trọng của việc nắm bắt và sử dụng các quan hệ giữa các biểu diễn đa mức độ chi tiết theo cả hướng ngang và dọc.

2.2 Học Tăng cường
Gần đây, học tăng cường (RL) đã thu hút sự chú ý đáng kể trong lĩnh vực mô hình hóa ngôn ngữ. Phương pháp này đã được áp dụng thành công cho nhiều tác vụ ngôn ngữ khác nhau, bao gồm tóm tắt (Paulus et al., 2018; Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021), hệ thống đối thoại (Zhou et al., 2017; Jaques et al., 2020; Hancock et al., 2019), dịch máy (Bahdanau et al., 2017; Kreutzer et al., 2018; Kiegeland và Kreutzer, 2021), phân tích ngữ nghĩa (Lawrence và Riezler, 2018), tạo truyện (Zhou và Xu, 2020), và tạo câu hỏi (Pang và He, 2021). Đồng thời, đã có sự quan tâm ngày càng tăng trong việc sử dụng RL để căn chỉnh các mô hình ngôn ngữ với sở thích của con người trong nhiều tác vụ ngôn ngữ. Ví dụ, Ouyang et al. (2022) đã sử dụng

--- TRANG 3 ---
LLM
[CLS]...Mô hình Phần thưởng
...a. LLM với soft P-tuning
K Mẫu Tương tự Nhất
Khóa ([CLS])Giá trị ([True, False])
embedding câu[0.1, 0.9]
embedding câu[0.7, 0.3]
......
Logits Mềm
Kho kiến thức mở...
Mô hình Quy mô Nhỏ
[CLS]
b. Học tăng cường
c. Chuyển giao Kiến thức dựa trên Truy xuất
Khóa cho Kho Kiến thức
Độ Tương tự
Embedding Gần nhất
Giá trị
1.0
embedding câu[0.8, 0.2]
0.7
embedding câu[0.7, 0.3]
......
Nhãn
Giá trị
True[0.1, 0.9]
False[0.7, 0.3]
Tổng hợp
Phân kỳ KL
R=(Rđộ_chính_xác+alphaRđa_dạng)*BP
Phần thưởng
Phân phối Độ tương tự
Phân phối Độ tương tự
MLP
Soft Prompts

Hình 1: Khung làm việc của RetriKT bao gồm ba bước: (1) tinh chỉnh các soft prompts bổ sung cho LLM bằng học có giám sát; (2) tiếp tục tinh chỉnh các soft prompts bằng học tăng cường để nâng cao chất lượng sinh; (3) tạo kho kiến thức dựa trên kiến thức được trích xuất từ LLM, và dạy mô hình quy mô nhỏ cách truy xuất kiến thức liên quan từ đó. (Chuyển giao Kiến thức dựa trên Truy xuất)

các kỹ thuật RL, cụ thể là Proximal Policy Optimization (PPO) (Schulman et al., 2017), để tinh chỉnh một mô hình ngôn ngữ lớn và căn chỉnh nó với các mô hình sở thích của con người.

2.3 Tăng cường Dữ liệu
Wu et al. (2019) và Kumar et al. (2020) đã giới thiệu một phương pháp tạo dữ liệu tổng hợp bằng cách che ngẫu nhiên các từ trong các thể hiện huấn luyện gốc. Các công trình khác (Ding et al., 2020; Yang et al., 2020; Anaby-Tavor et al., 2020) liên quan đến việc sử dụng Mô hình Ngôn ngữ (LMs) và Mô hình Ngôn ngữ Được huấn luyện trước (PLMs) để tạo trực tiếp dữ liệu tổng hợp cho các tác vụ NLU. Wang et al. (2021b, 2022) đề xuất sử dụng hard prompts và soft prompts để tạo dữ liệu tổng hợp. Trong một nghiên cứu liên quan, Liu et al. (2021b); Zan et al. (2022) đã khám phá tính chất bổ sung của PLMs và các phương pháp cổ điển.

3 Phương pháp
Phương pháp của chúng tôi xoay quanh việc trích xuất kiến thức từ các tham số của Mô hình Ngôn ngữ Lớn (LLM) và tận dụng nó cho lợi ích của mô hình quy mô cực nhỏ thông qua truy xuất. Chúng tôi giới thiệu kỹ thuật trích xuất kiến thức trong miền từ LLM cho mỗi tác vụ bằng cách đóng băng các tham số của nó và tinh chỉnh các soft prompts bổ sung sử dụng tập dữ liệu tương ứng (phần 3.1). Để nâng cao hơn nữa chất lượng sinh, chúng tôi giới thiệu học tăng cường và thiết kế cẩn thận hàm phần thưởng để tinh chỉnh các soft prompts (phần 3.2). Sau đó, chúng tôi trích xuất kiến thức trong miền từ LLM và tạo kho kiến thức (phần 3.3). Cuối cùng, chúng tôi cho phép mô hình quy mô nhỏ truy xuất và sử dụng hiệu quả kiến thức được tạo bởi LLM để thực hiện thành công các tác vụ cụ thể (phần 3.4).

3.1 Điều chỉnh Soft Prompt
Để nâng cao việc trích xuất kiến thức trong miền từ LLM, việc giới thiệu các prompts phù hợp đi trước các đầu vào là rất quan trọng, hướng dẫn hiệu quả quá trình tạo văn bản (ví dụ: "Tạo ví dụ rte theo các từ khóa sau:"). Tuy nhiên, chỉ dựa vào thiết kế thủ công thường dẫn đến tính đa dạng và hiệu quả hạn chế trong kiến thức được tạo. Để vượt qua những hạn chế như vậy, chúng tôi sử dụng các tham số có thể huấn luyện được gọi là soft prompts, do đó thay thế các mẫu thủ công. Chúng tôi chỉ cần cập nhật các soft prompts này và giữ tất cả các tham số khác của LLM cố định trong quá trình huấn luyện cho mỗi tác vụ cụ thể.

Cụ thể hơn, chúng tôi sử dụng P-tuning v2 (Liu et al., 2022b), bao gồm việc kết hợp một soft prompt vào mỗi lớp Transformer (Vaswani et al.,

--- TRANG 4 ---
2017). Soft prompt được biểu diễn dưới dạng một chuỗi các vector có thể huấn luyện, ký hiệu là Pj = {pj1, . . . , pjk}, trong đó j là lớp thứ j và k là siêu tham số biểu thị độ dài prompt. Trạng thái ẩn thứ i tại lớp thứ j, ký hiệu là hji, có thể được biểu diễn như sau:

hji = {
    pji nếu i ≤ k
    wi nếu i > k ∧ j = 0
    Trans(Hj−1)i Trường hợp khác
}                                                    (1)

trong đó Trans() biểu diễn lớp Transformer, Hj biểu diễn tất cả trạng thái ẩn tại lớp thứ j và wi là embedding từ của văn bản đầu vào.

Để tạo ra các mẫu đa dạng, chúng tôi áp dụng phương pháp được trình bày trong PromDA (Wang et al., 2022), bao gồm việc tạo mẫu từ cả Input View và Output View. Input View được điều kiện hóa dựa trên các từ khóa có trong văn bản đầu vào, được trích xuất bằng thuật toán trích xuất từ khóa không giám sát Rake (Rose et al., 2010). Mặt khác, Output View được điều kiện hóa dựa trên các nhãn. Thêm vào đó, chúng tôi huấn luyện hai tập soft prompts, cụ thể là Pinput và Poutput, được thiết kế đặc biệt để tạo mẫu từ Input View và Output View tương ứng. Mỗi mẫu S = (Y, X) được tạo bởi LLM bao gồm hai phần, cụ thể là nhãn Y = {yi}lyi=1 và văn bản đầu vào X = {xi}lxi=1. Mục tiêu học có giám sát cho các soft prompts Pinput và Poutput có thể được hình thức hóa như sau:

Linput = −∑i=1^(ly+lx) log(p(si|s<i, Pinput, K)),                  (2)

Loutput = −∑i=1^(ly+lx) log(p(si|s<i, Poutput, Y)),               (3)

trong đó K và Y biểu diễn các từ khóa của văn bản đầu vào và nhãn, tương ứng.

3.2 Học Tăng cường
Việc xây dựng một kho kiến thức kết hợp độ chính xác và tính đa dạng là thiết yếu để cho phép mô hình quy mô nhỏ truy xuất hiệu quả thông tin liên quan. Tuy nhiên, các phương pháp học có giám sát truyền thống không thể tối ưu hóa các mục tiêu học này do thiếu khả năng phân biệt từng token. Do đó, chúng tôi sử dụng học tăng cường để cải thiện chất lượng sinh. Phương pháp này bao gồm việc đảm bảo rằng các văn bản được tạo ra phù hợp với các nhãn, do đó đảm bảo độ chính xác của kho kiến thức. Thêm vào đó, nó đảm bảo rằng các mẫu được tạo thể hiện tính đa dạng, do đó đáp ứng yêu cầu đa dạng của kho kiến thức.

Cụ thể hơn, chúng tôi sử dụng tập dữ liệu gốc để huấn luyện một mô hình phân loại, phục vụ như mô hình phần thưởng ký hiệu là RM(). Đối với mỗi mẫu được tạo, bao gồm cả văn bản đầu vào và nhãn, chúng tôi đánh giá điểm tin cậy của nhãn văn bản sử dụng mô hình phần thưởng, do đó thu được phần thưởng độ chính xác Raccuracy. Thêm vào đó, chúng tôi đánh giá tính đa dạng của các mẫu được tạo bằng cách sử dụng thước đo Self-Bleu (Zhu et al., 2018), tính toán điểm Bleu (Papineni et al., 2002) coi các mẫu khác là tham chiếu. Sau đó, chúng tôi tính toán phần thưởng đa dạng Rdiversity là 1 − b3, trong đó b3 biểu diễn thước đo Bleu-3.

Để tránh việc tạo ra các mẫu quá đơn giản bởi LLM, chúng tôi áp dụng phạt độ dài. Nếu độ dài của mẫu được tạo nằm dưới một ngưỡng nhất định lmin, phần thưởng được nhân với hệ số giảm:

BP = {
    1 nếu l ≥ lmin
    exp(1−lmin/l) nếu l < lmin
}                                                    (4)

trong đó l biểu diễn độ dài của mẫu được tạo. Phần thưởng cuối cùng cho mỗi mẫu được tạo là:

R = (Raccuracy + alpha × Rdiversity) × BP,                        (5)

trong đó alpha là siêu tham số để cân bằng trọng số của phần thưởng độ chính xác và phần thưởng đa dạng.

Theo các nghiên cứu trước đây (Ouyang et al., 2022; Bai et al., 2022), chúng tôi sử dụng Proximal Policy Optimization (PPO) (Schulman et al., 2017) như thuật toán học tăng cường để tinh chỉnh các soft prompts Pinput và Poutput. Để đảm bảo hiệu suất đáng tin cậy trong việc tạo mẫu trong miền, chúng tôi kết hợp các gradient có giám sát vào các gradient PPO. Mục tiêu của chúng tôi là tối thiểu hóa hàm mất mát sau:

LLLM = Lp + Lv + beta × Lsft,                                     (6)

trong đó Lp và Lv là mất mát actor và critic của thuật toán PPO tương ứng, Lsft biểu diễn Linput và Loutput cho Pinput và Poutput tương ứng, và beta là siêu tham số kiểm soát độ mạnh của các gradient có giám sát.

--- TRANG 5 ---
3.3 Kho Kiến thức
Chúng tôi sử dụng LLM đã được tinh chỉnh để tạo mẫu và xây dựng kho kiến thức cho mỗi tác vụ. Cụ thể, đối với mỗi mẫu trong tập dữ liệu gốc D, chúng tôi sử dụng các từ khóa được trích xuất bằng thuật toán Rake và nhãn làm đầu vào cho mô hình. Mỗi mẫu được sử dụng m lần, và trong mỗi lần lặp, n mẫu được tạo. Để đảm bảo sự khác biệt giữa các mẫu được tạo, chúng tôi áp dụng p-sampling (Holtzman et al., 2020), hạn chế các token vào tập nhỏ nhất có thể mà xác suất tích lũy vượt quá tham số xác suất p. Tập mẫu được tạo từ từ khóa được ký hiệu là DI, trong khi tập được tạo từ nhãn được ký hiệu là DO. Số lượng của cả DI và DO là mn|D|, trong đó |D| biểu diễn số lượng của tập dữ liệu gốc D.

Để nâng cao hơn nữa tính đa dạng của các mẫu được tạo, chúng tôi sử dụng nhãn của mỗi mẫu trong DI làm đầu vào mô hình để tạo một mẫu mỗi lần, tạo ra tập mẫu DIO. Tương tự, chúng tôi sử dụng từ khóa của mỗi ví dụ trong DO làm đầu vào mô hình để tạo một mẫu mỗi lần, tạo ra tập mẫu DOI. Kho kiến thức cuối cùng được cấu thành từ các tập mẫu này, cụ thể là DS ∪ DI ∪ DO ∪ DIO ∪ DOI, về lý thuyết tạo ra tổng cộng (1 + 4mn)|D| mẫu. Tuy nhiên, có khả năng trùng lặp trong các mẫu được tạo, vì vậy chúng tôi loại bỏ chúng.

Như được minh họa trong Hình 1, khóa cho mỗi mẫu trong kho kiến thức là embedding câu được tạo bởi mô hình quy mô nhỏ. Các embedding này cho phép mô hình truy xuất thông tin liên quan từ kho kiến thức. Giá trị của mỗi mẫu là phân phối xác suất được tạo bởi mô hình phần thưởng RM() cho mỗi tác vụ.

3.4 Chuyển giao Kiến thức dựa trên Truy xuất
Do số lượng tham số hạn chế trong mô hình quy mô nhỏ, việc ghi nhớ trực tiếp tất cả kiến thức được trích xuất từ LLM là không khả thi. Do đó, chúng tôi không huấn luyện mô hình quy mô nhỏ trực tiếp với các mẫu được tạo. Thay vào đó, chúng tôi tận dụng kiến thức theo cách truy xuất.

Cụ thể, chúng tôi sử dụng mô hình phần thưởng RM() để cung cấp biểu diễn câu fT(xi) cho mỗi mẫu được tạo xi, sau đó được sử dụng để tính điểm độ tương tự cosine với các mẫu được tạo khác trong mini-batch. Danh sách điểm độ tương tự thu được từ mô hình phần thưởng có thể được ký hiệu là STi = {sT(xi, xj)}j∈[1,N]∧j≠i = {φ(fT(xi), fT(xj))}j∈[1,N]∧j≠i, trong đó N biểu thị kích thước batch và φ ký hiệu độ tương tự cosine. Tương tự, chúng tôi có thể thu được danh sách điểm độ tương tự SSi từ mô hình quy mô nhỏ. Các điểm độ tương tự này sau đó được chuẩn hóa theo cách listwise để thu được các phân phối liên quan:

s̃T(xi, xj) = esT(xi,xj)/tau1 / ∑k∈[1,N]∧k≠i esT(xi,xk)/tau1,        (7)

s̃S(xi, xj) = esS(xi,xj)/tau2 / ∑k∈[1,N]∧k≠i esS(xi,xk)/tau2,        (8)

trong đó tau1 và tau2 là các siêu tham số nhiệt độ để làm mượt các phân phối.

Mục tiêu của chúng tôi là cho phép mô hình quy mô nhỏ học cách truy xuất thông tin liên quan từ kho kiến thức. Để đạt được điều này, chúng tôi tối thiểu hóa KL-divergence giữa hai phân phối liên quan S̃Ti = {s̃T(xi, xj)}j∈[1,N]∧j≠i và S̃Si = {s̃S(xi, xj)}j∈[1,N]∧j≠i làm mục tiêu học cho mô hình quy mô nhỏ:

Lsmall = ∑i=1^N S̃Si · log(S̃Si/S̃Ti).                              (9)

Trong quá trình suy luận của mô hình quy mô nhỏ, chúng tôi truy xuất k mẫu liên quan nhất từ kho kiến thức cho mỗi mẫu kiểm tra x, ký hiệu là {xi}ki=1. Để xác định trọng số của mỗi mẫu được truy xuất xi, chúng tôi tính điểm độ tương tự giữa mẫu kiểm tra x và mẫu được truy xuất xi. Điểm logit dự đoán cuối cùng cho mỗi lớp của mô hình quy mô nhỏ được thu được như sau:

pSc = φ(x, xi)/∑j=1^k φ(x, xj) × pTc,                              (10)

trong đó c là một lớp cụ thể và pTc là điểm tin cậy của nhãn c được dự đoán bởi mô hình phần thưởng RM(). Dự đoán cuối cùng của mô hình quy mô nhỏ là lớp có điểm logit lớn nhất.

--- TRANG 6 ---
[THIS IS TABLE: Main experimental results showing model performance across different tasks. The table includes model names, parameter counts, and performance scores for WiC, CB, COPA, RTE, BoolQ, CoLA tasks with averages]

Bảng 2: Kết quả thử nghiệm chính (%) trên sáu tác vụ ít tài nguyên. Chúng tôi cũng báo cáo số lượng tham số của mỗi PLM (không bao gồm embeddings). Chúng tôi tái hiện tất cả các mô hình baseline dựa trên mã được phát hành bởi AD-KD (Wu et al., 2023), và kết hợp BERT base làm mô hình TA (w. TA). Có hai phiên bản của RetriKT, một được huấn luyện bằng Vanilla KD với các mẫu được tạo (RetriKT-KD), và một được huấn luyện bằng mục tiêu học truy xuất (RetriKT-Retrieval). Kết quả tốt nhất của mỗi backbone được hiển thị in đậm. Kết quả có ý nghĩa thống kê so với tất cả baseline trên mỗi mô hình học sinh (tất cả p-value < 0.005).

4 Thiết lập Thử nghiệm

4.1 Tập dữ liệu
Chúng tôi đánh giá hiệu suất của phương pháp trên nhiều tập dữ liệu từ benchmark SuperGLUE (Wang et al., 2019a) và benchmark GLUE (Wang et al., 2019b), bao gồm BoolQ (Clark et al., 2019), CB (De Marneffe et al., 2019), COPA (Roemmele et al., 2011), RTE, WiC (Pilehvar và Camacho-Collados, 2019), CoLA (Warstadt et al., 2019). Những tập dữ liệu này đặt ra thách thức cho các mô hình quy mô nhỏ vì chúng được coi là khó và ít tài nguyên, với số mẫu huấn luyện ít hơn 10K (chi tiết trong Bảng 1).

4.2 Baseline
Chúng tôi so sánh các phương pháp của chúng tôi với Vanilla KD (Hinton et al., 2015) và các phương pháp KD mạnh gần đây bao gồm MSGKD (Liu et al., 2022a) và AD-KD (Wu et al., 2023), được tái hiện dựa trên mã được phát hành bởi AD-KD. Chúng tôi tinh chỉnh EncT5 xl (Liu et al., 2021a) làm mô hình giáo viên, và huấn luyện BERT base sử dụng phương pháp Vanilla KD làm mô hình hỗ trợ (TA) cho tất cả baseline. Chúng tôi thực hiện tìm kiếm lưới cho tốc độ học trong {1e-5, 2e-5, 3e-5, 4e-5}, kích thước batch trong {4,8,16} cho các tập dữ liệu COPA và CB, và kích thước batch trong {16,32,64} cho các tập dữ liệu khác. Epoch huấn luyện được đặt là 40 cho CB và COPA, 20 cho RTE và 10 cho các tập dữ liệu khác. Chúng tôi trình bày kết quả trên tập validation thu được từ checkpoint tốt nhất trong quá trình huấn luyện.

4.3 Triển khai
Chúng tôi triển khai tất cả thử nghiệm với framework deep learning PyTorch dựa trên PromDA (Wang et al., 2022) và thư viện trl (von Werra et al., 2020) trên tối đa tám GPU NVIDIA Tesla A100 (bộ nhớ 80GB). Chúng tôi xây dựng LLM dựa trên T5 xl với 3B tham số (Raffel et al., 2020), và dịch mỗi tập dữ liệu thành định dạng câu đơn theo mẫu được cung cấp bởi T5. Ví dụ được xử lý trước của mỗi tập dữ liệu được hiển thị trong Phụ lục B. Chúng tôi sử dụng hai mô hình BERT quy mô nhỏ được phát hành bởi Turc et al. (2019), một với 4 lớp Transformer, 512 neuron ẩn và 8 attention heads, và một với 2 lớp Transformer, 512 neuron ẩn và 8 attention heads.

Đầu tiên, chúng tôi điều chỉnh các soft prompts Pinput và Poutput bằng học có giám sát làm mô hình tham chiếu và khởi tạo cho thuật toán PPO. Độ dài prompt được đặt là 8 cho tất cả tập dữ liệu. Tốc độ học và kích thước batch được đặt lần lượt là 1e-3 và 64. Bước huấn luyện được đặt là 10K cho COPA và CB, 20K cho RTE, và 40K cho các tập dữ liệu khác. Chúng tôi tinh chỉnh EncT5 xl làm mô hình phần thưởng cho mỗi tập dữ liệu, cũng là mô hình giáo viên cho tất cả mô hình baseline để so sánh công bằng. Sau đó chúng tôi huấn luyện các soft prompts thông qua sự kết hợp của học có giám sát và học tăng cường bằng hàm mất mát được định nghĩa trong Eq.(6). Chúng tôi đặt alpha và beta lần lượt là 0.2 và 1. Số lượng tạo n và xác suất p cho top-p sampling luôn được đặt lần lượt là 5 và 0.9, trong khi thời gian mẫu m được đặt là 64 cho COPA và CB, 16 cho RTE, và 8 cho các tập dữ liệu khác. Cuối cùng, chúng tôi huấn luyện các mô hình BERT quy mô nhỏ với tìm kiếm lưới tốc độ học trong {2e-5, 3e-5}, kết hợp nhiệt độ (tau1, tau2) trong {(0.2, 0.1), (0.1, 0.05)}. Kích thước batch được đặt là 128. Có hai phiên bản của RetriKT, một được huấn luyện bằng Vanilla KD với các mẫu được tạo (RetriKT-KD), và một được huấn luyện bằng mục tiêu học truy xuất (RetriKT-Retrieval). Chi tiết huấn luyện thêm cho thuật toán PPO có thể được tìm thấy trong Phụ lục A.

5 Kết quả Thử nghiệm và Phân tích

5.1 Kết quả Chính
Như được trình bày trong Bảng 2, rõ ràng rằng cả RetriKT-KD và RetriKT-Retrieval đều vượt trội hơn các phương pháp KD trước đây trên tất cả backbone, điều này chứng minh hiệu quả của phương pháp chúng tôi. Ví dụ, so với phương pháp SOTA trước đây AD-KD, RetriKT-Retrieval cho thấy cải thiện đáng kể: 7.34% trên BERT 2 và 9.28% trên BERT 4. Đáng chú ý, RetriKT-Retrieval luôn vượt trội hơn RetriKT-KD, đặc biệt trên BERT 2, cho thấy rằng việc truy xuất thông tin liên quan từ kho kiến thức phù hợp hơn cho các mô hình quy mô nhỏ thay vì cố gắng ghi nhớ tất cả kiến thức. Hơn nữa, chúng tôi quan sát hiệu suất tương đương giữa Vanilla KD và Vanilla KD (w.TA), cho thấy những hạn chế trong việc sử dụng mô hình TA để chuyển giao kiến thức. Một phương pháp hiệu quả hơn để chuyển giao kiến thức từ LLM cho mô hình quy mô nhỏ là nhắc LLM tạo kiến thức trong miền, có thể được truy xuất bởi mô hình quy mô nhỏ làm thông tin liên quan cho suy luận hiệu quả.

5.2 Nghiên cứu Loại bỏ
Để điều tra tác động của các thành phần khác nhau của học tăng cường trong phương pháp chúng tôi, chúng tôi tiến hành một loạt nghiên cứu loại bỏ. Chúng tôi loại bỏ huấn luyện tăng cường (w/o RL), phần thưởng độ chính xác Raccuracy, phần thưởng đa dạng Rdiversity, và phạt độ dài BP, và đánh giá kết quả trên CB, COPA và CoLA, như được hiển thị trong Bảng 3. Một số quan sát chính có thể được rút ra từ kết quả thử nghiệm. Thứ nhất, rõ ràng rằng hiệu suất mô hình của cả RetriKT-KD và RetriKT-Retrieval đều giảm khi không có RL hoặc bất kỳ thành phần nào của RL. Điều này làm nổi bật hiệu quả của thiết kế hàm phần thưởng và RL của chúng tôi, nâng cao chất lượng sinh của LLM. Thứ hai, khi bất kỳ thành phần nào của RL được loại bỏ, hiệu suất mô hình trên CoLA cho thấy sự giảm tương đối nhỏ hơn so với CB và COPA. Điều này cho thấy rằng chất lượng sinh của LLM có tác động rõ rệt hơn trên các tập dữ liệu nhỏ hơn, trong khi các tập dữ liệu lớn hơn thể hiện một mức độ mạnh mẽ nhất định. Tuy nhiên, RL luôn cải thiện chất lượng sinh của LLM, do đó nâng cao hiệu suất mô hình tổng thể. Cuối cùng, đáng nhắc đến rằng RetriKT-Retrieval luôn vượt trội hơn RetriKT-KD trong hầu hết tất cả các thiết lập, xác nhận thêm hiệu quả của chuyển giao kiến thức dựa trên truy xuất cho các mô hình quy mô nhỏ.

--- TRANG 7 ---
[THIS IS TABLE: Ablation studies showing performance comparison across different components using BERT 2 and BERT 4 on datasets CB, COPA and CoLA]

5.3 Phân tích Chi tiết

Tác động của Số lần Lấy mẫu: Chúng tôi thực hiện thí nghiệm trên tập dữ liệu COPA để điều tra tác động của số lần lấy mẫu m đối với hiệu suất mô hình. Như được hiển thị trong Hình 2, với giá trị m nhỏ, hiệu suất của cả RetriKT-KD và RetriKT-Retrieval đều tương đương. Tuy nhiên, khi m tăng, chúng tôi quan sát sự cải thiện đáng kể trong hiệu suất của RetriKT-Retrieval so với RetriKT-KD. Phát hiện này cho thấy rằng các mô hình quy mô nhỏ, bị hạn chế bởi các tham số hạn chế của chúng, không thể ghi nhớ hiệu quả một lượng kiến thức lớn như vậy. Do đó, chuyển giao kiến thức dựa trên truy xuất tỏ ra là phương pháp thuận lợi hơn. Hơn nữa, khi m tiếp tục tăng, chúng tôi quan sát sự tăng tương ứng trong hiệu suất của cả RetriKT-KD và RetriKT-Retrieval, đạt đến một ngưỡng nhất định. Điều này cho thấy rằng việc tăng số lượng mẫu được tạo có thể nâng cao tính đa dạng của kiến thức. Tuy nhiên, do số lượng từ khóa được trích xuất từ tập dữ liệu gốc hạn chế, tính đa dạng của kiến thức đạt ổn định khi đạt ngưỡng tạo. Do đó, hiệu suất mô hình không cho thấy cải thiện thêm ngoài điểm này.

Tác động của Kích thước Mô hình: Chúng tôi tiến hành điều tra về tác động của kích thước mô hình đối với hiệu suất sử dụng hai tập dữ liệu, COPA và CoLA. Kết quả thí nghiệm được hiển thị trong Hình 3 cho thấy rằng khi sử dụng mô hình nhỏ hơn, RetriKT-Retrieval luôn vượt trội hơn RetriKT-KD, thể hiện sự ưu việt của chuyển giao kiến thức dựa trên truy xuất cho các mô hình quy mô nhỏ. Thêm vào đó, chúng tôi quan sát rằng cả RetriKT-Retrieval và RetriKT-KD đều luôn vượt trội hơn việc huấn luyện mô hình quy mô nhỏ chỉ tinh chỉnh bằng tập dữ liệu gốc. Quan sát này nhấn mạnh thêm hiệu quả của việc trích xuất kiến thức từ LLM và ứng dụng nó trong việc nâng cao hiệu suất mô hình.

Độ Chính xác và Tính Đa dạng: Trong phần này, chúng tôi xem xét liệu LLM được huấn luyện thông qua học tăng cường có thể tạo ra kiến thức chính xác và đa dạng hơn không. Để đánh giá tính đa dạng của kiến thức, chúng tôi sử dụng thước đo Self-Bleu trên các mẫu được tạo, trong khi độ chính xác của kiến thức được đo bằng cross-entropy giữa phân phối xác suất được dự đoán bởi mô hình phần thưởng và nhãn được tạo bởi LLM. Trong cả hai thước đo, giá trị nhỏ hơn cho thấy hiệu suất tốt hơn. Như được hiển thị trong Bảng 4, kết quả chứng minh rằng việc huấn luyện LLM thông qua học tăng cường dẫn đến việc tạo ra kiến thức chính xác và đa dạng hơn trên tất cả tập dữ liệu. Kết quả này làm nổi bật hiệu quả của thiết kế hàm phần thưởng của chúng tôi. Thêm vào đó, chúng tôi quan sát rằng các tập dữ liệu lớn hơn có xu hướng tạo ra các mẫu với thước đo Self-Bleu nhỏ hơn. Chúng tôi suy đoán rằng hiện tượng này là kết quả của khả năng trích xuất một loạt từ khóa đa dạng hơn của tập dữ liệu lớn hơn, do đó cho phép tạo ra kiến thức đa dạng hơn.

6 Kết luận
Nghiên cứu của chúng tôi giải quyết tác vụ nén LLMs và giới thiệu một mô hình nén tiên phong được gọi là Chuyển giao Kiến thức dựa trên Truy xuất. Phương pháp này chuyển giao hiệu quả kiến thức của LLMs cho các mô hình quy mô nhỏ bằng cách tạo kho kiến thức, cho phép mô hình nhỏ truy xuất thông tin liên quan trong quá trình suy luận. Thông qua các thí nghiệm rộng rãi được tiến hành trên các benchmark thường được sử dụng, chúng tôi chứng minh rằng framework của chúng tôi cải thiện đáng kể hiệu suất của các mô hình quy mô nhỏ bằng cách tận dụng kiến thức có trong LLMs. Trong nghiên cứu tương lai, chúng tôi có kế hoạch điều tra ứng dụng và hiệu suất của phương pháp đề xuất trên các mô hình ngôn ngữ thậm chí lớn hơn, như T5-11B.

Hạn chế
Trong phần này, chúng tôi thảo luận về các hạn chế của công trình như sau. Thứ nhất, do tài nguyên tính toán hạn chế, chúng tôi không thử các thí nghiệm với các mô hình lớn hơn như T5-11B. Hơn nữa, ràng buộc tài nguyên hạn chế việc tìm kiếm lưới siêu tham số của chúng tôi trong phạm vi hạn chế, có thể để lại chỗ cho việc nâng cao các thước đo được trình bày trong bài báo này. Thứ hai, phương pháp đề xuất của chúng tôi yêu cầu thiết lập kho kiến thức. So với các phương pháp nén mô hình khác, phương pháp của chúng tôi giới thiệu không gian lưu trữ bổ sung và phát sinh thời gian hơi thêm cho việc truy xuất.

Lời cảm ơn
Công trình này được hỗ trợ bởi Chương trình R&D Trọng điểm Quốc gia của Trung Quốc (Số 2021YFC3340303) và Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Tài trợ NSFC Số 62122089). Jingang Wang được tài trợ bởi Chương trình Nova Bắc Kinh (Tài trợ SỐ 20220484098). Chúng tôi chân thành cảm ơn tất cả các người đánh giá về những nhận xét và đề xuất có giá trị của họ, điều quan trọng để cải thiện công trình của chúng tôi. Chúng tôi cũng muốn ghi nhận Angela Li về những đóng góp của cô trong việc tạo ra các hình ảnh được sử dụng trong công trình này.

--- TRANG 8 ---
[Tiếp tục với phần References và các bảng/phụ lục khác...]

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài được dịch sang tiếng Việt, bao gồm các trích dẫn từ Anaby-Tavor et al. đến Zhu et al., theo định dạng học thuật tiêu chuẩn]

--- TRANG 9 ---
[Tiếp tục phần phân tích và kết luận]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 14 ---
A Chi tiết Huấn luyện cho PPO
Trong phần này, chúng tôi trình bày chi tiết huấn luyện của PPO trong Bảng. Ý nghĩa của các siêu tham số này được chi tiết trong thư viện trl (von Werra et al., 2020) và NLPO (Ramamurthy et al., 2022).

B Ví dụ Được Xử lý trước
Trong phần này, chúng tôi cung cấp ví dụ về việc xử lý trước của chúng tôi cho mỗi tập dữ liệu mà chúng tôi xem xét.

B.1 boolq
Đầu vào gốc:
Câu hỏi: Bạn có thể nuôi chồn hôi làm thú cưng ở Canada không
Đoạn văn: Chồn hôi làm thú cưng - Chồn hôi thú cưng Canada phải được mua từ người nhân giống được chứng nhận USDA tại Hoa Kỳ. Giấy phép nhập khẩu được yêu cầu từ Cơ quan Kiểm tra Thực phẩm Canada để đưa chồn hôi vào nước này. Chồn hôi phải được triệt sản và nhận cấy vi chip hoặc xăm. Phí kiểm tra thú y cũng phải được thanh toán. Việc nuôi chồn hôi sọc làm thú cưng ở Canada là bất hợp pháp.

Đầu vào được xử lý: question: Bạn có thể nuôi chồn hôi làm thú cưng ở Canada không passage: Chồn hôi làm thú cưng - Chồn hôi thú cưng Canada phải được mua từ người nhân giống được chứng nhận USDA tại Hoa Kỳ. Giấy phép nhập khẩu được yêu cầu từ Cơ quan Kiểm tra Thực phẩm Canada để đưa chồn hôi vào nước này. Chồn hôi phải được triệt sản và nhận cấy vi chip hoặc xăm. Phí kiểm tra thú y cũng phải được thanh toán. Việc nuôi chồn hôi sọc làm thú cưng ở Canada là bất hợp pháp.

Nhãn:{False: 0; True: 1}

[Tiếp tục với các ví dụ khác cho CB, CoLA, COPA, RTE, WiC]

--- TRANG 15 ---
[Bảng siêu tham số cho PPO và phần đánh giá độ trung thực của kiến thức được tạo]

C Độ Trung thực của Kiến thức Được tạo
Để xác nhận thêm độ trung thực của kiến thức được tạo, chúng tôi thu thập năm tình nguyện viên để đánh giá độ trung thực của các văn bản được tạo cho các tập dữ liệu WiC, COPA và CoLA. Mỗi văn bản sẽ được đánh giá từ 1 đến 5. Chúng tôi lấy mẫu ngẫu nhiên 100 văn bản từ mỗi tập dữ liệu và tính trung bình làm điểm độ trung thực cuối cùng. Kết quả đánh giá độ trung thực được hiển thị trong Bảng 6, cho thấy rằng độ trung thực của các văn bản được tạo cho mỗi tập dữ liệu khá cao.

[THIS IS TABLE: Bảng 6 showing faithfulness scores for WiC (4.61), COPA (4.36), and CoLA (4.49)]
