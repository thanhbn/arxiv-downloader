# 2309.11688.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2309.11688.pdf
# File size: 133157 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2309.11688v1  [cs.CL]  20 Sep 2023LLM Guided Inductive Inference for Solving Compositional P roblems
Abhigya Sodani1Lauren Moos2Matthew Mirman1
Abstract
While large language models (LLMs) have
demonstrated impressive performance in
question-answering tasks, their performance is
limited when the questions require knowledge
that is not included in the model’s training data
and can only be acquired through direct observa-
tion or interaction with the real world. Existing
methods decompose reasoning tasks through the
use of modules invoked sequentially, limiting
their ability to answer deep reasoning tasks. We
introduce a method, Recursion based extensible
LLM (REBEL), which handles open-world,
deep reasoning tasks by employing automated
reasoning techniques like dynamic planning
and forward-chaining strategies. REBEL allows
LLMs to reason via recursive problem decompo-
sition and utilization of external tools. The tools
that REBEL uses are speciﬁed only by natural
language description. We further demonstrate
REBEL capabilities on a set of problems that
require a deeply nested use of external tools in a
compositional and conversational setting.
1. Introduction
Recently, neural models for natural language generation
have demonstrated impressive results ( Koroteev ,2021 ;
Devlin et al. ,2018 ;Brown et al. ,2020 ), opening signiﬁ-
cant new avenues for solving natural language reasoning
tasks precisely ( Huang & Chang ,2022 ;Qiao et al. ,2022 ).
While LLMs have shown a unique ability to scale in
predictable and efﬁcient ways, is unclear whether they
show this scaling behavior on complex reasoning tasks
(Huang & Chang ,2022 ). Moreover, the limitations of large
language models in accessing dynamic external knowledge
sources signiﬁcantly restrict their usefulness. Human rea -
soning involves a combination of observations and interac-
1Anarchy, Extensional, Inc, California, USA2Yoyodyne Inc,
California, USA. Correspondence to: Abhigya Sodani <abhi-
gya@anarchy.ai >, Matthew Mirman <matt@anarchy.ai >.
Proceedings of the 40thInternational Conference on Machine
Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).tions with the world, highlighting the action-oriented na-
ture of reasoning.
In this paper, we address this by introducing the Recursion
Based Extensible LLM (REBEL) framework. REBEL al-
lows LLMs to reason through highly complex problems
that require knowledge from disparate external sources.
This is accomplished using an inference engine that, using
the provided tools, gathers the necessary facts to infer the
correct answer. Speciﬁcally we show three contributions:
1. Designing a system capable of answering questions
using any arbitrary external tool.
2. An evaluation showing that REBEL improves upon
the state-of-the-art performance on multi-Hop fact re-
trieval and compositional question answering prob-
lems.
3. Releasing our code and evaluation suite for open-
source usage at rebel.anarchy.ai .
2. Related Works
At a high-level, methods for approaching reasoning tasks
using LLMs can be broken down into prompt engineering
techniques ( Liu et al. ,2023 ;Schlag et al. ,2023 ) and ﬁne-
tuning ( Micheli & Fleuret ,2021 ;Schick et al. ,2023 ) tech-
niques, or combinations of the above. Here we focus only
on prompt techniques.
Forward chaining ( Liebowitz ,1988 ) is a reasoning strat-
egy historically used by expert systems. It operates by
repeatedly applying logical inference rules from an ini-
tial repository of known axioms to eventually ideally pro-
duce the goal. This strategy has recently been employed
to solve natural language problems with the assistance of
LLMs in Chain of Thought (CoT) ( Wei et al. ,2022 ). Re-
Act ( Yao et al. ,2023 ) builds off of CoT by generating
task-speciﬁc actions in response to reasoning. Chameleon
(Lu et al. ,2023 ) takes this further, using LLMs to syn-
thesize tool pipelines including off-the-shelf computer v i-
sion models, web-search engines, and calls to generative
models. In contrast to forward-chaining, the technique of
backward-chaining ( Russell ,2010 ) attempts to limit the
search-space of possible inferences by determining what
must be true for a goal to be shown ( Picco et al. ,2021 ).
1

--- PAGE 2 ---
LLM Guided Inductive Inference for Solving Compositional P roblems
Press et al. (2022 ) demonstrates a method to evaluate
problem-solving abilities on a category of non-trivial rea -
soning tasks with compositional structure ( Lake & Baroni ,
2018 ;Keysers et al. ,2019 ) that is poorly addressed by prior
methods. They express compositional error as the num-
ber of questions in which two subquestions are answered
correctly but the top-level question is not. Prior work has
shown how this can be addressed via problem decompo-
sition ( Yang et al. ,2022 ;Zhou et al. ,2022 ;Drozdov et al. ,
2022 ;Khot et al. ,2022 ). In this work, we show how prob-
lem decomposition can be augmented with tool usage.
3. Methods
In this section we introduce the REBEL algorithm as shown
in Fig. 1, and all necessary notation and background. At a
high level, it works recursively to solve questions, breaki ng
questions into subquestions until no further subquestions
can be generated. Let us call the nthquestion/subquestion
Question nand its answer Answer n. For example, the
user-provided question would be Question 0. Let us call
the subquestions that are generated to answer Question n
Subquestions n. In each recursive step, we break
Question nintoSubquestions n. Let us call the answer
to theithmember of Subquestions nsubanswers n[i].
We recursively call each member of Subquestions n, and
eachsubanswers n[i]is returned as a fact which is the tu-
ple(Subquestions n[i],subanswers n[i]). This fact is ap-
pended to a list of facts that is is global to each Question n.
This list of facts becomes Memory nwhich is used to in-
formAnswer n.
In order to stop unbounded recursion, we delete members
ofSubquestions nwhose featurizations have cosine simi-
larities above 0.98 to the featurization of Question n.
The REBEL system contains a ToolList, which is a num-
bered list of the tools we have available and their de-
scriptions. If required, we determine a Toolnfor each
Question n, which is the number of the tool required to
answerQuestion ngivenMemory n.
Below we deﬁne the basic steps of this algorithm: question
splitting, checking memory, picking tools, and using tools .
Figure 1 depicts this pipeline.
3.1. Question Splitting
The split subroutine divides Question n into
Subquestions nwith the size of Subquestions nbe-
ing the number of subquestions that the LLM generates.
The LLM is prompted with ToolList, and 4 shots of
question splitting examples. This step is representing in
step 1 of Figure 1. To see a single shot of context for
question splitting see Appendix A.Algorithm 1 REBEL
function promptf(Question n,facts ,allowsplit =
True )
ifallowsplit then
Subquestions n= split(Question n,facts ){split
the question into subquestions to answer }
forsubquestion from 1 to sinSubquestions ndo
ifcossimilarity( Question n,subquestion )>0.98
then
Deletesubquestion
allowsplit =False
end if
end for
forsubquestion from 1 to sinSubquestions ndo
,newfacts =PROMPTF
(subquestion,facts,allowsplit )
facts +=newfact
end for
end if
ifMEMORYCHECK (Question n,facts)then
Answer n=CALL GPT(Question n,facts)
returnAnswer n,(Question n,Answer n)
else
tool=PICK TOOL (Question n,facts)
toolinput =CALL GPT(tool,Question n,facts)
{to determine tool input }
Answer n=USETOOL(toolinput,facts )
end if
returnAnswer n,(Question n,Answer n)
end function
We answer each subquestion and its results are returned
as afact (see Algorithm 1). These facts are accumu-
lated and passed to all subsequent subquestions. The list
Subquestions nis ordered such that the fact gained from
answering a lower indexed subquestion will aid in the an-
swering of a higher indexed subquestion.
3.2. Memory Check
We check if a question can be answered without any tool
use. This can mean either that the question can be an-
swered using Memory nor the question can be answered
by an LLM without the use of any tools (see step 2 Figure
1). If this is the case, we directly provide our base LLM
Memory nandQuestion nto ﬁndAnswer n. To see the
complete memory check prompt see Appendix B.
3.3. Tool Picker
Here we evoke the LLM to decide what member of
ToolList (described by integer Tooln) would be best to
decide the answer to a question. This is a 0-shot prompted
system which can be seen in step 3 of Figure 1.
2

--- PAGE 3 ---
LLM Guided Inductive Inference for Solving Compositional P roblems
Figure 1. Visual depiction of the pipeline of the REBEL Algorithm from Algorithm 1to answer some Question n. Blue boxes contain
descriptions of each step of the pipeline, and the red boxes c ontains the output variable for each step that will be used in subsequent
steps.
Question Splitting
1) Generates
Subquestionsnand
recursively gets their
answers and appends
them to Memoryn.
MemorynMemory Checking
2) Generates a boolean
MemoryChecknwhich
indicates if the answer
toQuestionsncan be
found in Memoryn.
MemoryChecknTool Picking
3) If MemoryChecknis
false, we generate
Tooln, the best tool
to answer Questionn
ToolnTool Input Generation
4) Takes Memoryn
and Questionnand
generates input for Tooln
... ”toolnparamk”:
”toolnvaluek”, . . .
3.4. Tool Input Generation
We use GPT-3 to generate standardized input to our tools.
We provide the tools to the LLM with 2 ﬁelds. The descrip-
tion of the tool and the dynamic parameters of the tool. We
store the 3 more ﬁelds about each tool that are hidden from
the LLM, these are: if the tool is a GET/POST request, the
endpoint URL for the tool, and the static parameters of the
tool. The dynamic parameters are parameters that will be
adjusted based on each call (for example, and query ﬁeld).
The static parameters are parameters that stay the same on
each API call (for example, an authentication key).
REBEL uses 3 default tools: search, weather, and Google
Maps. We conﬁgure the inputs to every tool as a JSON. A
tool input JSON maps a given tool’s dynamic parameters
to the values those parameters should be in order to obtain
information to answer a given question: {”toolnparam 1” :
”toolnvalue1”,...”toolnparam k” : ”toolnvaluek”}. A
standardized JSON format reduces the load on the LLM to
format an entire API call by itself.
REBEL allows for an arbitrary tools to be added to it, how-
ever, the k-shot examples that are provided to the LLM
for generating input given Toolnare designed around the
3 base tools. We have found that this prompting does ex-
trapolate to 0-shot uses of unseen and arbitrary tools. See
Appendix Cfor a complete single shot of tool input gener-
ation context.
3.5. Tool Usage
The UseTool function takes the dynamic parameters (from
the LLM generated tool input), the static parameters that
we have stored for each tool, and the API endpoint and
makes a single request URL. This URL is requested, and
the return output is stored as a string. If the return out-
put is longer than 15,000 characters, it is truncated to that
amount. Then, we use an LLM, provided with Memory n,Question n, and the API request output, to generate an an-
swer to the Question n. This answer is returned from the
UseTool function as Answer n. Our approach has some
consequences. On the positive side, users do not have the
indicate how to parse the output of the tools they give us,
this makes REBEL extremely extendable and ﬂexible to in-
terpret many tool return types and formats. On the negative
side, because of the extremely unstructured nature of tool
returns, errors are caused by UseTool not being able to an-
swer a question based on a tool return.
4. Evaluation
In this section we ﬁrst introduce the experimental setup,
including the benchmarks used for evaluation, and then
present the results.
4.1. Experimental Setup
We tested REBEL on 3 datasets: Compositional Celebri-
ties ( Press et al. ,2022 ), FEVER ( Thorne et al. ,2018 ), and
HotPotQA ( Yang et al. ,2018 ).
On these datasets, correctness was determined by a hu-
man experimenter based on the output of each system. Re-
Act outputs with simply the answer to the question, while
REBEL often outputs the answer wrapped in reasoning be-
hind the system’s thoughts. For these experiments, two sep-
arate sets of rules had to be determined for fact veriﬁcation
and fact retrieving questions. For fact retrieving questio ns,
an answer was considered correct if the desired answer was
contained in the system output. For fact veriﬁcation, if the
model output determination of the truthfulness of a state-
ment was the same as the desired truthfulness, then the gen-
erated answer was considered correct.
On Compositional Celebrities, due to computational lim-
itations, we tested using 5 of the 17 categories available,
using 100 questions per category, randomly chosen. These
3

--- PAGE 4 ---
LLM Guided Inductive Inference for Solving Compositional P roblems
categories can be found in Table 1.
We tested on FEVER and HotPotQA with 100 of the same
random questions from each dataset on both ReAct and
REBEL. The accuracy results for this experiment can be
found at Table 2. FEVER has 3 types of potential output la-
bels (SUPPORTS, REFUTES, NOT ENOUGH INFO). In
order to make prevent accidental correct answers from the
REBEL system, only questions with the SUPPORTS and
REFUTES labels were considered.
For this experiment REBEL was only allowed to use a
search tool to query the internet, as that is the only tool
that the ReAct system has access to.
Our code, which can be found at rebel.anarchy.ai ,
was implemented in Python using the OpenAI Completion
API to access GPT-3 (text-davinci-003).
4.2. Results
We found that REBEL outperformed ReAct on answering
questions that require i) the gathering of many facts to dete r-
mine an answer ii) very speciﬁc search queries that return
large amounts of unstructured data. With our experimen-
tal results we were able to show that REBEL is a state-of-
the-art system in terms of its ability to consistently answe r
questions from disparate knowledge bases.
4.2.1. M ULTI -HOPFACT RETRIEVAL
We used 2 datasets to test multi-hop fact retrieval: Compo-
sitional Celebrities and HotPotQA.
Compositional Celebrities is a dataset consisting of 8.6k
questions about Celebrities in different categories. All
questions require retrieving two facts and basic reasoning .
These two facts have never co-occurred in any text that
would conceivably be part of the LLM training and the
only way that the conclusion could be reached is for both
of them to be evaluated correctly and composed with
one another. We found that the REBEL system largely
outperformed the ReAct system at all of the 5 categories
that were experimented on for Compositional Celebrities.
On average, over the 5 categories tested, REBEL beat
ReAct by 27.6 percent. The reason for this is likely the
ability of the REBEL system to work with unstructured
tool return data. This allows the REBEL system to make
and interpret very speciﬁc tool queries, whereas other
systems that require standardized output can become
constricted by the by a smaller possible set of tool queries.
The results of this experiment can be found in Table 1.
HotpotQA is a challenging question-answering dataset
containing 113,000 pairs of questions and answers derived
from Wikipedia articles. The questions in HotpotQA
necessitate synthesis of information from diverse sourcesTable 1. Accuracy (percent of questions answered correctly) of
different algorithms on the categories of Compositional Ce lebri-
ties.
CATEGORY REACT REBEL
BIRTHPLACE ROUNDED LAT 28 59
BIRTHPLACE CURRENCY 85 94
BIRTHPLACE CURRENCY SYMBOL 35 47
BIRTHYEAR NOBEL LITERATURE 33 82
BIRTHDATE USPRESIDENT 53 90
Table 2. Accuracy (percent of questions answered correctly) of
different algorithms on HotPotQA and FEVER.
DATASET REACT REBEL
FEVER 72 78
HOTPOTQA 63 50
and cannot be found pre-existing training knowledge
bases. ReAct outperformed REBEL on HotPotQA by
13 percent (Table 2).
HotPotQA has questions that are signiﬁcantly more than
2-hops, and on these questions REBEL tends to generate
a massive recursive tree of subquestions. This introduces
the issue of generating subquestions that lose context of
the original question. Many times this can lead to the LLM
not being able to reason through the large context window
generated when processing these layers of recursive sub-
questions, resulting in the LLM ﬁnding no solution.
4.2.2. F ACT VERIFICATION
To test fact veriﬁcation abilities, we employed the FEVER
dataset. This benchmark is designed to evaluate the abil-
ity of models to extract factual information from textual
sources and verify claims. The fact veriﬁcation task in-
volves determining the accuracy of claims made in a given
piece of text.
On FEVER, the REBEL system (78 percent accuracy)
performed slightly better (Table 2) than ReAct system
(72 percent). The reason for this out-performance by the
REBEL system is because of the signiﬁcant amount of
”facts” that it gathers during its recursive solving of a fac t
veriﬁcation problem. On several occasions, the ReAct sys-
tem cannot ﬁnd the information it is looking for to answer
a questions, and therefore reports that it cannot make a de-
termination if a certain fact is true or not.
4.3. Ablation Study
In order to determine the efﬁciency of REBEL, we con-
ducted several ablation tests. In these tests the aim was to
4

--- PAGE 5 ---
LLM Guided Inductive Inference for Solving Compositional P roblems
Table 3. Accuracy (percent of questions answered correctly) of diff erent algorithms on the categories of Compositional Celebr ities.
CATEGORY GPT3 REBEL W/O TOOLS REBEL
BIRTHPLACE ROUNDED LAT 16 39 59
BIRTHPLACE CURRENCY 95 94 94
BIRTHPLACE CURRENCY SYMBOL 28 45 47
BIRTHYEAR NOBEL LITERATURE 95 90 82
BIRTHDATE USPRESIDENT 44 91 90
Table 4. Accuracy (percent of questions answered correctly) of diff erent algorithms on HotPotQA and FEVER.
DATASET GPT3 REBEL W/O TOOLS REBEL
FEVER 77 73 78
HOTPOTQA 43 46 50
isolate the affect of the REBEL system upon compositional
problem solving. We used plain GPT3 (text-davinci-003)
as our baseline. The results of these tests are in (Table 3
and Table 4).
These tables show that GPT3 outperforms REBEL (with
or without an external search tool) when a ques-
tion can be easily answered with data that GPT3’s
training set. This is seen in Table 3in the
rows pertaining to Birthyear NobelLiterature and
Birthplace Currency .
The REBEL algorithm without the external search tool out-
performed the baseline when information processing is nec-
essary to determine a ﬁnal answer. Examples of this in-
clude questions that required the returning of a currency
symbol or a rounded latitude. GPT3 succeeded in fetch-
ing the currency name or latitude correctly, but failed to
round the latitude or return the symbol associated with
the currency name. Adding external search augmented
the REBEL algorithm’s ability to reason with current facts,
and therefore furthered the REBEL algorithm performance
on most categories of Compositional Celebrities. Occa-
sionally, the inclusion of an external search tool decrease d
performance due to the unstructured nature of return data
the external tool provided. An example of this is on the
Birthyear NobelLiterature category of Compositional
Celebrities.
On most categories of Compositional Celebrities and on
HotPotQA, REBEL without the use of an external search
tool improved performance over baseline GPT3. This indi-
cates that our recursive approach adds reasoning capabilit y
to GPT3 independently of external tool use.
5. Cost Analysis
The recursive search nature of the REBEL algorithm
means that it employs many calls to an LLM beforeTable 5. Average time taken to answer a question from Composi-
tional Celebrities
ALGORITHM TIME(S)
GPT3 0.94
REBEL W/O TOOLS 5.358
REBEL W/TOOLS 9.76
determining an answer to a question. The downsides of
this approach manifest themselves in latency (Table 5)
and monetary cost of LLM queries. Any external tools
that are provided to the REBEL system will also be called
very frequently, potentially leading to REBEL being a
monetarily expensive system on that front as well.
If a user desires to use REBEL without any tools, a
cost in terms of hallucination has a potential of arising.
Due to the lack of any external knowledge base, a halluci-
nation on one subquestion has the potential to pollute the
entire tree of reasoning.
6. Conclusion
We have introduced REBEL, a recursive reasoning algo-
rithm designed to use any arbitrary API as an external tool.
REBEL outperforms the state-of-the-art on questions that
require the collection of many facts and those that bene-
ﬁt from the ability to make highly speciﬁc queries to out-
side sources of data, which may be unstructured. REBEL
also has a demonstrable improvement over the GPT3 LLM
when answering questions that require multi-step informa-
tion processing. However, the REBEL algorithm tends to
over-complicate simple problems, leading to a reduction in
accuracy when compared to baseline GPT3 on questions
that require minimal compositionality.
Future work would ideally address ﬁne-tuning LLMs for
5

--- PAGE 6 ---
LLM Guided Inductive Inference for Solving Compositional P roblems
each step in the REBEL pipeline and experimenting with
limiting recursive depth of subquestion generation.
References
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learn-
ers.Advances in neural information processing systems ,
33:1877–1901, 2020.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 ,
2018.
Drozdov, A., Sch¨ arli, N., Aky¨ urek, E., Scales, N., Song, X .,
Chen, X., Bousquet, O., and Zhou, D. Compositional
semantic parsing with large language models. arXiv
preprint arXiv:2209.15003 , 2022.
Huang, J. and Chang, K. C.-C. Towards reasoning in
large language models: A survey. arXiv preprint
arXiv:2212.10403 , 2022.
Keysers, D., Sch¨ arli, N., Scales, N., Buisman, H., Fur-
rer, D., Kashubin, S., Momchev, N., Sinopalnikov, D.,
Staﬁniak, L., Tihon, T., et al. Measuring compositional
generalization: A comprehensive method on realistic
data. arXiv preprint arXiv:1912.09713 , 2019.
Khot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K. ,
Clark, P., and Sabharwal, A. Decomposed prompting:
A modular approach for solving complex tasks. arXiv
preprint arXiv:2210.02406 , 2022.
Koroteev, M. Bert: a review of applications in natural
language processing and understanding. arXiv preprint
arXiv:2103.11943 , 2021.
Lake, B. and Baroni, M. Generalization without systematic-
ity: On the compositional skills of sequence-to-sequence
recurrent networks. In International conference on ma-
chine learning , pp. 2873–2882. PMLR, 2018.
Liebowitz, J. Introduction to expert systems . Mitchell Pub-
lishing, Inc., 1988.
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neu-
big, G. Pre-train, prompt, and predict: A systematic sur-
vey of prompting methods in natural language process-
ing. ACM Computing Surveys , 55(9):1–35, 2023.
Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu,
Ying Nian Zhu, S.-C., and Gao, J. Chameleon: Plug-and-
play compositional reasoning with large language mod-
els.arXiv preprint arXiv:2304.09842 , 2023.Micheli, V . and Fleuret, F. Language models are few-shot
butlers. arXiv preprint arXiv:2104.07972 , 2021.
Picco, G., Lam, H. T., Sbodio, M. L., and Garcia, V . L. Neu-
ral uniﬁcation for logic reasoning over natural language.
arXiv preprint arXiv:2109.08460 , 2021.
Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,
and Lewis, M. Measuring and narrowing the composi-
tionality gap in language models. arXiv:2210.03350v1 ,
2022.
Qiao, S., Ou, Y ., Zhang, N., Chen, X., Yao, Y ., Deng,
S., Tan, C., Huang, F., and Chen, H. Reasoning with
language model prompting: A survey. arXiv preprint
arXiv:2212.09597 , 2022.
Russell, S. J. Artiﬁcial intelligence a modern approach .
Pearson Education, Inc., 2010.
Schick, T., Dwivedi-Yu, J., Dess` ı, R., Raileanu, R., Lomel i,
M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Tool-
former: Language models can teach themselves to use
tools. arXiv preprint arXiv:2302.04761 , 2023.
Schlag, I., Sukhbaatar, S., Celikyilmaz, A., tau Yih, W.,
Weston, J., Schmidhuber, J., and Li, X. Large language
model programs. arXiv:2305.05364 , 2023.
Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mit-
tal, A. Fever: a large-scale dataset for fact extraction and
veriﬁcation. arXiv preprint arXiv:1803.05355 , 2018.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
Xia, F., Chi, E. H., Le, Q. V ., and Zhou, D. Chain-
of-thought prompting elicits reasoning in large language
models. NeurIPS , 2022.
Yang, J., Jiang, H., Yin, Q., Zhang, D., Yin, B., and Yang,
D. Seqzero: Few-shot compositional semantic parsing
with sequential prompts and zero-shot models. arXiv
preprint arXiv:2205.07381 , 2022.
Yang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W.,
Salakhutdinov, R., and Manning, C. D. Hotpotqa: A
dataset for diverse, explainable multi-hop question an-
swering. arXiv preprint arXiv:1809.09600 , 2018.
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
K., and Cao, Y . React: Synergizing reasoning and acting
in language models. ICLR , 2023.
Zhou, D., Sch¨ arli, N., Hou, L., Wei, J., Scales, N.,
Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and
Chi, E. Least-to-most prompting enables complex
reasoning in large language models. arXiv preprint
arXiv:2205.10625 , 2022.
6

--- PAGE 7 ---
LLM Guided Inductive Inference for Solving Compositional P roblems
A. Appendix A
Question Splitting Prompt
Tools we have access to =
tool 1: The tool returns the results of free-form queries sim ilar to those used for wolfram alpha. This is useful for
complicated math or live data retrieval. Can be used to get th e current date.
tool 2: Find the driving distance and time to travel between t wo cities.
tool 3: Find the weather at a location and returns it in celciu s.
Q=Question n
Look at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one
use of one tool. Make as few subquestions as possible. Split e ach subquestion with a comma and have no extra
information other than the subquestions.
B. Appendix B
Memory Check Prompt
Q: ”What’s the time?” Is the answer to Q found in the memory or i n your knowledge base already? Answer with a
yes or no. no
Q: ”How you feeling?” Is the answer to Q found in the memory or i n your knowledge base already? Answer with
a yes or no. yes
Q: ”What color is the sky” Is the answer to Q found in the memory or in your knowledge base already? Answer
with a yes or no. yes
Q: ”What is the temperature in Portland?” Is the answer to Q fo und in the memory or in your knowledge base
already? Answer with a yes or no. no
Memory: Memory n
Q:Question nIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or
no.
C. Appendix C
Tool Input Prompt
7

--- PAGE 8 ---
LLM Guided Inductive Inference for Solving Compositional P roblems
<TOOL>
<ID>1</ID>
<DESC>Find the driving
distance and time to
travel between two
cities.</DESC>
<PARAMS>{"origins": the origin city,
"destinations": the destination
city}</PARAMS>
</TOOL>
<CASE>
<Q>How long would it take
to get between
South Africa and Kenya.
</Q>
<THOUGHT>
<P>What
should the input for
tool 1 be to
answer Q?</P>
<A ty=JSON>
{"origins": "South Africa",
"destinations": "Kenya"}
</A>
</THOUGHT>
</CASE>
8

--- PAGE 9 ---
This figure "flowchart.png" is available in "png"
 format from:
http://arxiv.org/ps/2309.11688v1
