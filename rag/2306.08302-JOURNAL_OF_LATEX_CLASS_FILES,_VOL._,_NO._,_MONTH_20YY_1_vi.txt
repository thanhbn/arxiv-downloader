# 2306.08302.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2306.08302.pdf
# Kích thước tệp: 3347785 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẠP CHÍ CÁC TỆP LỚP LATEX, TẬP ??, SỐ ??, THÁNG 20YY 1
Thống nhất Mô hình Ngôn ngữ Lớn và
Đồ thị Tri thức: Một Lộ trình
Shirui Pan, Thành viên cấp cao, IEEE , Linhao Luo,
Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Nghiên cứu viên, IEEE

Tóm tắt —Các mô hình ngôn ngữ lớn (LLMs), như ChatGPT và GPT4, đang tạo ra những làn sóng mới trong lĩnh vực xử lý ngôn ngữ tự nhiên và trí tuệ nhân tạo, do khả năng nổi bật và tính tổng quát hóa của chúng. Tuy nhiên, LLMs là các mô hình hộp đen, thường thiếu khả năng nắm bắt và truy cập kiến thức thực tế. Ngược lại, Đồ thị Tri thức (KGs), ví dụ như Wikipedia và Huapu, là các mô hình tri thức có cấu trúc lưu trữ rõ ràng kiến thức thực tế phong phú. KGs có thể tăng cường LLMs bằng cách cung cấp kiến thức bên ngoài cho suy luận và khả năng diễn giải. Trong khi đó, KGs khó xây dựng và phát triển bởi bản chất, điều này thách thức các phương pháp hiện có trong KGs để tạo ra các sự kiện mới và biểu diễn kiến thức chưa được thấy. Do đó, việc thống nhất LLMs và KGs với nhau và đồng thời tận dụng các ưu điểm của chúng là điều bổ sung. Trong bài báo này, chúng tôi trình bày một lộ trình có tầm nhìn xa cho việc thống nhất LLMs và KGs. Lộ trình của chúng tôi bao gồm ba khung chung, cụ thể là, 1) LLMs được tăng cường bởi KG, tích hợp KGs trong các giai đoạn tiền huấn luyện và suy luận của LLMs, hoặc nhằm mục đích tăng cường hiểu biết về kiến thức được học bởi LLMs; 2) KGs được bổ sung bởi LLM, tận dụng LLMs cho các nhiệm vụ KG khác nhau như nhúng, hoàn thành, xây dựng, tạo văn bản từ đồ thị, và trả lời câu hỏi; và 3) LLMs + KGs được đồng bộ hóa, trong đó LLMs và KGs đóng vai trò bình đẳng và làm việc theo cách có lợi lẫn nhau để tăng cường cả LLMs và KGs cho suy luận hai chiều được thúc đẩy bởi cả dữ liệu và kiến thức. Chúng tôi xem xét và tóm tắt các nỗ lực hiện có trong ba khung này trong lộ trình của chúng tôi và chỉ ra các hướng nghiên cứu tương lai của chúng.

Thuật ngữ chỉ mục —Xử lý Ngôn ngữ Tự nhiên, Mô hình Ngôn ngữ Lớn, Tiền huấn luyện Tạo sinh, Đồ thị Tri thức, Lộ trình, Suy luận Hai chiều.

✦

1 GIỚI THIỆU

Các mô hình ngôn ngữ lớn (LLMs)¹ (ví dụ: BERT [1], RoBERTA [2], và T5 [3]), được tiền huấn luyện trên kho văn bản quy mô lớn, đã cho thấy hiệu suất tuyệt vời trong các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) khác nhau, như trả lời câu hỏi [4], dịch máy [5], và tạo văn bản [6]. Gần đây, việc tăng kích thước mô hình đáng kể còn trang bị cho LLMs khả năng nổi bật [7], mở đường cho việc áp dụng LLMs như Trí tuệ Nhân tạo Tổng quát (AGI). Các LLMs tiên tiến như ChatGPT² và PaLM2³, với hàng tỷ tham số, thể hiện tiềm năng lớn trong nhiều nhiệm vụ thực tế phức tạp, như giáo dục [8], tạo mã [9] và đề xuất [10].

•Shirui Pan làm việc tại Trường Công nghệ Thông tin và Truyền thông và Viện Hệ thống Tích hợp và Thông minh (IIIS), Đại học Griffith, Queensland, Australia. Email: s.pan@griffith.edu.au;
•Linhao Luo và Yufei Wang làm việc tại Khoa Khoa học Dữ liệu và AI, Đại học Monash, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com.
•Chen Chen làm việc tại Đại học Công nghệ Nanyang, Singapore. E-mail: s190009@ntu.edu.sg.
•Jiapu Wang làm việc tại Khoa Công nghệ Thông tin, Đại học Công nghệ Bắc Kinh, Bắc Kinh, Trung Quốc. E-mail: jpwang@emails.bjut.edu.cn.
•Xindong Wu làm việc tại Phòng thí nghiệm Trọng điểm Kỹ thuật Tri thức với Dữ liệu Lớn (Bộ Giáo dục Trung Quốc), Đại học Công nghệ Hợp Phì, Hợp Phì, Trung Quốc, và cũng làm việc tại Trung tâm Nghiên cứu Kỹ thuật Tri thức, Phòng thí nghiệm Chiết Giang, Hàng Châu, Trung Quốc. Email: xwu@hfut.edu.cn.
•Shirui Pan và Linhao Luo đóng góp ngang nhau cho công trình này.
•Tác giả liên hệ: Xindong Wu.

1. LLMs cũng được biết đến như các mô hình ngôn ngữ tiền huấn luyện (PLMs).
2. https://openai.com/blog/chatgpt
3. https://ai.google/discover/palm2

Hình 1. Tóm tắt các ưu và nhược điểm của LLMs và KGs. Ưu điểm LLM: Kiến thức Tổng quát [11], Xử lý Ngôn ngữ [12], Khả năng Tổng quát hóa [13]; Nhược điểm LLM: Kiến thức Ngầm [14], Ảo giác [15], Thiếu quyết đoán [16], Hộp đen [17], Thiếu Kiến thức Chuyên ngành/Mới [18]. Ưu điểm KG: Kiến thức Có cấu trúc [19], Độ chính xác [20], Tính quyết đoán [21], Khả năng diễn giải [22], Kiến thức Chuyên ngành [23], Kiến thức Phát triển [24]; Nhược điểm KG: Không hoàn chỉnh [25], Thiếu Hiểu biết Ngôn ngữ [26], Sự kiện Chưa thấy [27]. Ưu điểm và Nhược điểm được chọn dựa trên tính đại diện của chúng. Thảo luận chi tiết có thể được tìm thấy trong Phụ lục A.

Mặc dù thành công trong nhiều ứng dụng, LLMs đã bị chỉ trích vì thiếu kiến thức thực tế. Cụ thể, LLMs ghi nhớ các sự kiện và kiến thức có trong kho dữ liệu huấn luyện [14]. Tuy nhiên, các nghiên cứu sâu hơn cho thấy rằng LLMs không thể nhớ lại các sự kiện và thường trải qua ảo giác bằng cách tạo ra các tuyên bố sai sự kiện [15], [28]. Ví dụ, LLMs có thể nói "Einstein đã khám phá ra trọng lực vào năm 1687" khi được hỏi, "Khi nào Einstein khám phá ra trọng lực?", điều này mâu thuẫn với sự thật rằng Isaac Newton đã công thức hóa lý thuyết hấp dẫn. Vấn đề này làm suy giảm nghiêm trọng độ tin cậy của LLMs.

Là các mô hình hộp đen, LLMs cũng bị chỉ trích vì thiếu khả năng diễn giải. LLMs biểu diễn kiến thức một cách ngầm trong các tham số của chúng. Rất khó để diễn giải hoặc xác thực kiến thức thu được bởi LLMs. Hơn nữa, LLMs thực hiện suy luận bằng một mô hình xác suất, đây là một quá trình thiếu quyết đoán [16]. Các mẫu và chức năng cụ thể mà LLMs sử dụng để đưa ra dự đoán hoặc quyết định không thể truy cập trực tiếp hoặc giải thích được cho con người [17]. Mặc dù một số LLMs được trang bị để giải thích dự đoán của chúng bằng cách áp dụng chuỗi suy nghĩ [29], các giải thích suy luận của chúng cũng gặp phải vấn đề ảo giác [30]. Điều này làm suy giảm nghiêm trọng việc áp dụng LLMs trong các tình huống có rủi ro cao, như chẩn đoán y tế và phán quyết pháp lý. Ví dụ, trong tình huống chẩn đoán y tế, LLMs có thể chẩn đoán sai bệnh và đưa ra giải thích mâu thuẫn với thường thức y tế. Điều này đặt ra một vấn đề khác là LLMs được huấn luyện trên kho dữ liệu tổng quát có thể không thể tổng quát hóa tốt cho các lĩnh vực cụ thể hoặc kiến thức mới do thiếu kiến thức chuyên ngành hoặc dữ liệu huấn luyện mới [18].

Để giải quyết các vấn đề trên, một giải pháp tiềm năng là tích hợp đồ thị tri thức (KGs) vào LLMs. Đồ thị tri thức (KGs), lưu trữ các sự kiện khổng lồ theo cách của các bộ ba, tức là, (thực thể đầu, quan hệ, thực thể đuôi), là một cách biểu diễn kiến thức có cấu trúc và quyết đoán (ví dụ: Wikidata [20], YAGO [31], và NELL [32]). KGs rất quan trọng cho các ứng dụng khác nhau vì chúng cung cấp kiến thức rõ ràng chính xác [19]. Bên cạnh đó, chúng được biết đến với khả năng suy luận tượng trưng [22], tạo ra kết quả có thể diễn giải. KGs cũng có thể phát triển tích cực với kiến thức mới được bổ sung liên tục [24]. Thêm vào đó, các chuyên gia có thể xây dựng KGs chuyên ngành để cung cấp kiến thức chuyên ngành chính xác và đáng tin cậy [23].

Tuy nhiên, KGs khó xây dựng [25], và các phương pháp hiện tại trong KGs [27], [33], [34] không đủ khả năng xử lý bản chất không hoàn chỉnh và thay đổi động của KGs thực tế. Các phương pháp này không thể mô hình hóa hiệu quả các thực thể chưa thấy và biểu diễn các sự kiện mới. Ngoài ra, chúng thường bỏ qua thông tin văn bản phong phú trong KGs. Hơn nữa, các phương pháp hiện có trong KGs thường được tùy chỉnh cho KGs hoặc nhiệm vụ cụ thể, không đủ tổng quát. Do đó, cũng cần thiết phải sử dụng LLMs để giải quyết các thách thức đối mặt trong KGs. Chúng tôi tóm tắt các ưu và nhược điểm của LLMs và KGs trong Hình 1, tương ứng.

Gần đây, khả năng thống nhất LLMs với KGs đã thu hút sự quan tâm ngày càng tăng từ các nhà nghiên cứu và thực hành. LLMs và KGs vốn dĩ có liên kết với nhau và có thể tăng cường lẫn nhau. Trong LLMs được tăng cường bởi KG, KGs không chỉ có thể được tích hợp vào các giai đoạn tiền huấn luyện và suy luận của LLMs để cung cấp kiến thức bên ngoài [35]–[37], mà còn được sử dụng để phân tích LLMs và cung cấp khả năng diễn giải [14], [38], [39]. Trong KGs được bổ sung bởi LLM, LLMs đã được sử dụng trong các nhiệm vụ liên quan đến KG khác nhau, ví dụ: nhúng KG [40], hoàn thành KG [26], xây dựng KG [41], tạo văn bản từ KG [42], và KGQA [43], để cải thiện hiệu suất và tạo thuận lợi cho việc ứng dụng KGs. Trong LLM + KG được đồng bộ hóa, các nhà nghiên cứu kết hợp các ưu điểm của LLMs và KGs để tăng cường lẫn nhau hiệu suất trong biểu diễn kiến thức [44] và suy luận [45], [46]. Mặc dù có một số khảo sát về LLMs được tăng cường kiến thức [47]–[49], chủ yếu tập trung vào việc sử dụng KGs như một kiến thức bên ngoài để tăng cường LLMs, chúng bỏ qua các khả năng khác của việc tích hợp KGs cho LLMs và vai trò tiềm năng của LLMs trong các ứng dụng KG.

Trong bài báo này, chúng tôi trình bày một lộ trình có tầm nhìn xa để thống nhất cả LLMs và KGs, nhằm tận dụng điểm mạnh tương ứng và vượt qua các hạn chế của từng phương pháp, cho các nhiệm vụ downstream khác nhau. Chúng tôi đề xuất phân loại chi tiết, tiến hành đánh giá toàn diện, và chỉ ra các hướng mới nổi trong những lĩnh vực phát triển nhanh này. Các đóng góp chính của chúng tôi được tóm tắt như sau:

1) Lộ trình. Chúng tôi trình bày một lộ trình có tầm nhìn xa để tích hợp LLMs và KGs. Lộ trình của chúng tôi, bao gồm ba khung chung để thống nhất LLMs và KGs, cụ thể là, LLMs được tăng cường bởi KG, KGs được bổ sung bởi LLM, và LLMs + KGs được đồng bộ hóa, cung cấp hướng dẫn cho việc thống nhất hai công nghệ khác biệt nhưng bổ sung này.

2) Phân loại và đánh giá. Đối với mỗi khung tích hợp trong lộ trình của chúng tôi, chúng tôi trình bày phân loại chi tiết và phân loại mới về nghiên cứu thống nhất LLMs và KGs. Trong mỗi danh mục, chúng tôi đánh giá nghiên cứu từ góc độ các chiến lược tích hợp và nhiệm vụ khác nhau, cung cấp nhiều hiểu biết hơn về mỗi khung.

3) Bao quát các tiến bộ mới nổi. Chúng tôi bao quát các kỹ thuật tiên tiến trong cả LLMs và KGs. Chúng tôi bao gồm thảo luận về các LLMs hiện đại như ChatGPT và GPT-4 cũng như các KGs mới như đồ thị tri thức đa phương thức.

4) Tóm tắt các thách thức và hướng tương lai. Chúng tôi nêu bật các thách thức trong nghiên cứu hiện tại và trình bày một số hướng nghiên cứu tương lai đầy hứa hẹn.

Phần còn lại của bài báo này được tổ chức như sau. Mục 2 trước tiên giải thích nền tảng của LLMs và KGs. Mục 3 giới thiệu lộ trình và phân loại tổng thể của bài báo này. Mục 4 trình bày các phương pháp LLM được tăng cường bởi KGs khác nhau. Mục 5 mô tả các phương pháp KG được bổ sung bởi LLM có thể. Mục 6 cho thấy các phương pháp đồng bộ hóa LLMs và KGs. Mục 7 thảo luận về các thách thức và hướng nghiên cứu tương lai. Cuối cùng, Mục 8 kết luận bài báo này.

2 NỀN TẢNG

Trong mục này, trước tiên chúng tôi sẽ giới thiệu ngắn gọn một số mô hình ngôn ngữ lớn (LLMs) đại diện và thảo luận về kỹ thuật prompt để sử dụng LLMs hiệu quả cho nhiều ứng dụng khác nhau. Sau đó, chúng tôi minh họa khái niệm về đồ thị tri thức (KGs) và trình bày các danh mục khác nhau của KGs.

2.1 Mô hình ngôn ngữ lớn (LLMs)

Các mô hình ngôn ngữ lớn (LLMs) được tiền huấn luyện trên kho dữ liệu quy mô lớn đã cho thấy tiềm năng lớn trong các nhiệm vụ NLP khác nhau [13]. Như được thể hiện trong Hình 3, hầu hết LLMs xuất phát từ thiết kế Transformer [50], chứa các mô-đun encoder và decoder được trang bị cơ chế self-attention. Dựa trên cấu trúc kiến trúc, LLMs có thể được phân loại thành ba nhóm: 1) LLMs chỉ có encoder, 2) LLMs encoder-decoder, và 3) LLMs chỉ có decoder. Như được thể hiện trong Hình 2, chúng tôi tóm tắt một số LLMs đại diện với các kiến trúc mô hình khác nhau, kích thước mô hình, và tính khả dụng mã nguồn mở.

2.1.1 LLMs chỉ có encoder.

Các mô hình ngôn ngữ lớn chỉ có encoder chỉ sử dụng encoder để mã hóa câu và hiểu mối quan hệ giữa các từ. Mô hình huấn luyện phổ biến cho các mô hình này là dự đoán các từ bị che trong một câu đầu vào. Phương pháp này không có giám sát và có thể được huấn luyện trên kho dữ liệu quy mô lớn. Các LLMs chỉ có encoder như BERT [1], ALBERT [51], RoBERTa [2], và ELECTRA [52] yêu cầu thêm một đầu dự đoán bổ sung để giải quyết các nhiệm vụ downstream. Các mô hình này hiệu quả nhất cho các nhiệm vụ yêu cầu hiểu toàn bộ câu, như phân loại văn bản [26] và nhận dạng thực thể có tên [53].

2.1.2 LLMs encoder-decoder.

Các mô hình ngôn ngữ lớn encoder-decoder áp dụng cả mô-đun encoder và decoder. Mô-đun encoder chịu trách nhiệm mã hóa câu đầu vào thành không gian ẩn, và decoder được sử dụng để tạo ra văn bản đầu ra mục tiêu. Các chiến lược huấn luyện trong LLMs encoder-decoder có thể linh hoạt hơn. Ví dụ, T5 [3] được tiền huấn luyện bằng cách che và dự đoán các đoạn từ bị che. UL2 [54] thống nhất một số mục tiêu huấn luyện như các đoạn che khác nhau và tần suất che. Các LLMs encoder-decoder (ví dụ: T0 [55], ST-MoE [56], và GLM-130B [57]) có thể trực tiếp giải quyết các nhiệm vụ tạo câu dựa trên một số ngữ cảnh, như tóm tắt, dịch, và trả lời câu hỏi [58].

2.1.3 LLMs chỉ có decoder.

Các mô hình ngôn ngữ lớn chỉ có decoder chỉ áp dụng mô-đun decoder để tạo ra văn bản đầu ra mục tiêu. Mô hình huấn luyện cho các mô hình này là dự đoán từ tiếp theo trong câu. Các LLMs chỉ có decoder quy mô lớn thường có thể thực hiện các nhiệm vụ downstream từ một vài ví dụ hoặc hướng dẫn đơn giản, mà không cần thêm đầu dự đoán hoặc tinh chỉnh [59]. Nhiều LLMs hiện đại (ví dụ: Chat-GPT [60] và GPT-4⁴) theo kiến trúc chỉ có decoder. Tuy nhiên, vì các mô hình này là mã nguồn đóng, việc cho các nhà nghiên cứu học thuật tiến hành nghiên cứu sâu hơn là thách thức. Gần đây, Alpaca⁵ và Vicuna⁶ được phát hành như các LLMs chỉ có decoder mã nguồn mở. Các mô hình này được tinh chỉnh dựa trên LLaMA [61] và đạt được hiệu suất tương đương với ChatGPT và GPT-4.

2.1.4 Kỹ thuật Prompt

Kỹ thuật prompt là một lĩnh vực mới tập trung vào việc tạo và tinh chỉnh các prompt để tối đa hóa hiệu quả của các mô hình ngôn ngữ lớn (LLMs) trong nhiều ứng dụng và lĩnh vực nghiên cứu khác nhau [62]. Như được thể hiện trong Hình 4, một prompt là một chuỗi đầu vào ngôn ngữ tự nhiên cho LLMs được chỉ định cho nhiệm vụ, như phân loại cảm xúc. Một prompt có thể chứa một số yếu tố, tức là, 1) Hướng dẫn, 2) Ngữ cảnh, và 3) Văn bản Đầu vào. Hướng dẫn là một câu ngắn hướng dẫn mô hình thực hiện một nhiệm vụ cụ thể. Ngữ cảnh cung cấp ngữ cảnh cho văn bản đầu vào hoặc các ví dụ few-shot. Văn bản Đầu vào là văn bản cần được xử lý bởi mô hình.

Kỹ thuật prompt tìm cách cải thiện khả năng của các mô hình ngôn ngữ lớn (ví dụ: ChatGPT) trong các nhiệm vụ phức tạp đa dạng như trả lời câu hỏi, phân loại cảm xúc, và suy luận thường thức. Prompt chuỗi suy nghĩ (CoT) [63] cho phép khả năng suy luận phức tạp thông qua các bước suy luận trung gian. Kỹ thuật prompt cũng cho phép tích hợp dữ liệu có cấu trúc như đồ thị tri thức (KGs) vào LLMs. Li et al. [64] đơn giản tuyến tính hóa KGs và sử dụng mẫu để chuyển đổi KGs thành các đoạn văn. Mindmap [65] thiết kế một prompt KG để chuyển đổi cấu trúc đồ thị thành một sơ đồ tư duy cho phép LLMs thực hiện suy luận trên đó. Prompt cung cấp một cách đơn giản để sử dụng tiềm năng của LLMs mà không cần tinh chỉnh. Thành thạo trong kỹ thuật prompt dẫn đến hiểu biết tốt hơn về điểm mạnh và điểm yếu của LLMs.

2.2 Đồ thị Tri thức (KGs)

Đồ thị tri thức (KGs) lưu trữ kiến thức có cấu trúc như một tập hợp các bộ ba KG={(h, r, t)⊆ E × R × E}, trong đó E và R tương ứng biểu thị tập hợp các thực thể và quan hệ. Các đồ thị tri thức hiện có (KGs) có thể được phân loại thành bốn nhóm dựa trên thông tin được lưu trữ: 1) KGs bách khoa toàn thư, 2) KGs thường thức, 3) KGs chuyên ngành, và 4) KGs đa phương thức. Chúng tôi minh họa các ví dụ về KGs của các danh mục khác nhau trong Hình 5.

2.2.1 Đồ thị Tri thức Bách khoa toàn thư.

Đồ thị tri thức bách khoa toàn thư là KGs phổ biến nhất, đại diện cho kiến thức tổng quát trong thế giới thực. Đồ thị tri thức bách khoa toàn thư thường được xây dựng bằng cách tích hợp thông tin từ các nguồn đa dạng và rộng lớn, bao gồm các chuyên gia con người, bách khoa toàn thư, và cơ sở dữ liệu. Wikidata [20] là một trong những đồ thị tri thức bách khoa toàn thư được sử dụng rộng rãi nhất, tích hợp nhiều loại kiến thức được trích xuất từ các bài viết trên Wikipedia. Các đồ thị tri thức bách khoa toàn thư điển hình khác, như Freebase [66], Dbpedia [67], và YAGO [31] cũng được bắt nguồn từ Wikipedia. Ngoài ra, NELL [32] là một đồ thị tri thức bách khoa toàn thư liên tục cải thiện, tự động trích xuất kiến thức từ web, và sử dụng kiến thức đó để cải thiện hiệu suất theo thời gian. Có một số đồ thị tri thức bách khoa toàn thư có sẵn bằng các ngôn ngữ khác ngoài tiếng Anh như CN-DBpedia [68] và Vikidia [69]. Đồ thị tri thức lớn nhất, có tên Knowledge Ocean (KO)⁷, hiện chứa 48.784.363 thực thể và 173.115.834 quan hệ bằng cả tiếng Anh và tiếng Trung.

2.2.2 Đồ thị Tri thức Thường thức.

Đồ thị tri thức thường thức hình thành kiến thức về các khái niệm hàng ngày, ví dụ: đối tượng, và sự kiện, cũng như mối quan hệ của chúng [70]. So với đồ thị tri thức bách khoa toàn thư, đồ thị tri thức thường thức thường mô hình hóa kiến thức ngầm được trích xuất từ văn bản như (Xe hơi, DùngĐể, Lái). ConceptNet [71] chứa một loạt các khái niệm và quan hệ thường thức, có thể giúp máy tính hiểu ý nghĩa của các từ mà con người sử dụng. ATOMIC [72], [73] và ASER [74] tập trung vào các hiệu ứng nhân quả giữa các sự kiện, có thể được sử dụng cho suy luận thường thức. Một số đồ thị tri thức thường thức khác, như TransOMCS [75] và CausalBanK [76] được xây dựng tự động để cung cấp kiến thức thường thức.

2.2.3 Đồ thị Tri thức Chuyên ngành

Đồ thị tri thức chuyên ngành thường được xây dựng để đại diện cho kiến thức trong một lĩnh vực cụ thể, ví dụ: y tế, sinh học, và tài chính [23]. So với đồ thị tri thức bách khoa toàn thư, đồ thị tri thức chuyên ngành thường có kích thước nhỏ hơn, nhưng chính xác và đáng tin cậy hơn. Ví dụ, UMLS [77] là một đồ thị tri thức chuyên ngành trong lĩnh vực y tế, chứa các khái niệm y sinh và mối quan hệ của chúng. Ngoài ra, có một số đồ thị tri thức chuyên ngành trong các lĩnh vực khác, như tài chính [78], địa chất [79], sinh học [80], hóa học [81] và phả hệ [82].

2.2.4 Đồ thị Tri thức Đa phương thức.

Không giống như các đồ thị tri thức thông thường chỉ chứa thông tin văn bản, đồ thị tri thức đa phương thức đại diện cho các sự kiện trong nhiều phương thức như hình ảnh, âm thanh, và video [83]. Ví dụ, IMGpedia [84], MMKG [85], và Richpedia [86] tích hợp cả thông tin văn bản và hình ảnh vào đồ thị tri thức. Các đồ thị tri thức này có thể được sử dụng cho các nhiệm vụ đa phương thức khác nhau như khớp hình ảnh-văn bản [87], trả lời câu hỏi thị giác [88], và đề xuất [89].

2.3 Ứng dụng

LLMs như KGs đã được áp dụng rộng rãi trong các ứng dụng thực tế khác nhau. Chúng tôi tóm tắt một số ứng dụng đại diện của việc sử dụng LLMs và KGs trong Bảng 1. ChatGPT/GPT-4 là các chatbot dựa trên LLM có thể giao tiếp với con người theo định dạng đối thoại tự nhiên. Để cải thiện nhận thức kiến thức của LLMs, ERNIE 3.0 và Bard tích hợp KGs vào các ứng dụng chatbot của chúng. Thay vì Chatbot. Firefly phát triển một ứng dụng chỉnh sửa ảnh cho phép người dùng chỉnh sửa ảnh bằng cách sử dụng mô tả ngôn ngữ tự nhiên. Copilot, New Bing, và Shop.ai áp dụng LLMs để trao quyền cho các ứng dụng của chúng trong các lĩnh vực trợ lý mã hóa, tìm kiếm web, và đề xuất, tương ứng. Wikidata và KO là hai ứng dụng đồ thị tri thức đại diện được sử dụng để cung cấp kiến thức bên ngoài. OpenBG [90] là một đồ thị tri thức được thiết kế cho đề xuất. Doctor.ai phát triển một trợ lý chăm sóc sức khỏe tích hợp LLMs và KGs để cung cấp lời khuyên y tế.

3 LỘ TRÌNH & PHÂN LOẠI

Trong mục này, trước tiên chúng tôi trình bày một lộ trình về các khung rõ ràng để thống nhất LLMs và KGs. Sau đó, chúng tôi trình bày phân loại nghiên cứu về việc thống nhất LLMs và KGs.

3.1 Lộ trình

Lộ trình thống nhất KGs và LLMs được minh họa trong Hình 6. Trong lộ trình, chúng tôi xác định ba khung cho việc thống nhất LLMs và KGs, bao gồm LLMs được tăng cường bởi KG, KGs được bổ sung bởi LLM, và LLMs + KGs được đồng bộ hóa. LLMs được tăng cường bởi KG và KGs được bổ sung bởi LLM là hai khung song song nhằm tăng cường khả năng của LLMs và KGs, tương ứng. Dựa trên các khung này, LLMs + KGs được đồng bộ hóa là một khung thống nhất nhằm đồng bộ hóa LLMs và KGs để tăng cường lẫn nhau.

3.1.1 LLMs được tăng cường bởi KG

LLMs được biết đến với khả năng học kiến thức từ kho dữ liệu quy mô lớn và đạt được hiệu suất hiện đại trong các nhiệm vụ NLP khác nhau. Tuy nhiên, LLMs thường bị chỉ trích vì các vấn đề ảo giác [15], và thiếu khả năng diễn giải. Để giải quyết các vấn đề này, các nhà nghiên cứu đã đề xuất tăng cường LLMs với đồ thị tri thức (KGs). KGs lưu trữ kiến thức khổng lồ theo cách rõ ràng và có cấu trúc, có thể được sử dụng để tăng cường nhận thức kiến thức của LLMs. Một số nhà nghiên cứu đã đề xuất tích hợp KGs vào LLMs trong giai đoạn tiền huấn luyện, có thể giúp LLMs học kiến thức từ KGs [35], [91]. Các nhà nghiên cứu khác đã đề xuất tích hợp KGs vào LLMs trong giai đoạn suy luận. Bằng cách truy xuất kiến thức từ KGs, nó có thể cải thiện đáng kể hiệu suất của LLMs trong việc truy cập kiến thức chuyên ngành [92]. Để cải thiện khả năng diễn giải của LLMs, các nhà nghiên cứu cũng sử dụng KGs để diễn giải các sự kiện [14] và quá trình suy luận của LLMs [38].

3.1.2 KGs được bổ sung bởi LLM

KGs lưu trữ kiến thức cấu trúc đóng vai trò thiết yếu trong nhiều ứng dụng thế giới thực [19]. Các phương pháp hiện có trong KGs thiếu khả năng xử lý KGs không hoàn chỉnh [33] và xử lý kho dữ liệu văn bản để xây dựng KGs [93]. Với tính tổng quát của LLMs, nhiều nhà nghiên cứu đang cố gắng khai thác sức mạnh của LLMs để giải quyết các nhiệm vụ liên quan đến KG.

Cách đơn giản nhất để áp dụng LLMs là làm bộ mã hóa văn bản cho các nhiệm vụ liên quan đến KG. Các nhà nghiên cứu tận dụng LLMs để xử lý kho dữ liệu văn bản trong KGs và sau đó sử dụng biểu diễn của văn bản để làm phong phú biểu diễn KGs [94]. Một số nghiên cứu cũng sử dụng LLMs để xử lý kho dữ liệu gốc và trích xuất quan hệ và thực thể để xây dựng KG [95]. Các nghiên cứu gần đây cố gắng thiết kế một prompt KG có thể chuyển đổi hiệu quả KGs có cấu trúc thành định dạng có thể được hiểu bởi LLMs. Theo cách này, LLMs có thể được áp dụng trực tiếp cho các nhiệm vụ liên quan đến KG, ví dụ: hoàn thành KG [96] và suy luận KG [97].

3.1.3 LLMs + KGs được đồng bộ hóa

Sự đồng bộ của LLMs và KGs đã thu hút sự quan tâm ngày càng tăng từ các nhà nghiên cứu những năm này [40], [42]. LLMs và KGs là hai kỹ thuật vốn dĩ bổ sung, nên được thống nhất thành một khung chung để tăng cường lẫn nhau.

Để khám phá sâu hơn về việc thống nhất, chúng tôi đề xuất một khung thống nhất của LLMs + KGs được đồng bộ hóa trong Hình 7. Khung thống nhất chứa bốn lớp: 1) Dữ liệu, 2) Mô hình Đồng bộ, 3) Kỹ thuật, và 4) Ứng dụng. Trong lớp Dữ liệu, LLMs và KGs được sử dụng để xử lý dữ liệu văn bản và cấu trúc, tương ứng. Với sự phát triển của LLMs đa phương thức [98] và KGs [99], khung này có thể được mở rộng để xử lý dữ liệu đa phương thức, như video, âm thanh, và hình ảnh. Trong lớp Mô hình Đồng bộ, LLMs và KGs có thể đồng bộ với nhau để cải thiện khả năng của chúng. Trong lớp Kỹ thuật, các kỹ thuật liên quan đã được sử dụng trong LLMs và KGs có thể được tích hợp vào khung này để tăng cường hiệu suất hơn nữa. Trong lớp Ứng dụng, LLMs và KGs có thể được tích hợp để giải quyết các ứng dụng thế giới thực khác nhau, như công cụ tìm kiếm [100], hệ thống đề xuất [10], và trợ lý AI [101].

3.2 Phân loại

Để hiểu rõ hơn về nghiên cứu thống nhất LLMs và KGs, chúng tôi cung cấp thêm phân loại chi tiết cho mỗi khung trong lộ trình. Cụ thể, chúng tôi tập trung vào các cách khác nhau để tích hợp KGs và LLMs, tức là LLMs được tăng cường bởi KG, LLMs được bổ sung bởi KG, và LLMs + KGs được đồng bộ hóa. Phân loại chi tiết của nghiên cứu được minh họa trong Hình 8.

LLMs được tăng cường bởi KG. Tích hợp KGs có thể tăng cường hiệu suất và khả năng diễn giải của LLMs trong các nhiệm vụ downstream khác nhau. Chúng tôi phân loại nghiên cứu về LLMs được tăng cường bởi KG thành ba nhóm:

1) Tiền huấn luyện LLM được tăng cường bởi KG bao gồm các công trình áp dụng KGs trong giai đoạn tiền huấn luyện và cải thiện biểu hiện kiến thức của LLMs.

2) Suy luận LLM được tăng cường bởi KG bao gồm nghiên cứu sử dụng KGs trong giai đoạn suy luận của LLMs, cho phép LLMs truy cập kiến thức mới nhất mà không cần huấn luyện lại.

3) Khả năng diễn giải LLM được tăng cường bởi KG bao gồm các công trình sử dụng KGs để hiểu kiến thức được học bởi LLMs và diễn giải quá trình suy luận của LLMs.

KGs được bổ sung bởi LLM. LLMs có thể được áp dụng để bổ sung các nhiệm vụ liên quan đến KG khác nhau. Chúng tôi phân loại nghiên cứu về KGs được bổ sung bởi LLM thành năm nhóm dựa trên các loại nhiệm vụ:

1) Nhúng KG được bổ sung bởi LLM bao gồm các nghiên cứu áp dụng LLMs để làm phong phú biểu diễn của KGs bằng cách mã hóa mô tả văn bản của thực thể và quan hệ.

2) Hoàn thành KG được bổ sung bởi LLM bao gồm các bài báo sử dụng LLMs để mã hóa văn bản hoặc tạo ra sự kiện để có hiệu suất KGC tốt hơn.

3) Xây dựng KG được bổ sung bởi LLM bao gồm các công trình áp dụng LLMs để giải quyết các nhiệm vụ khám phá thực thể, giải quyết coreference, và trích xuất quan hệ để xây dựng KG.

4) Tạo văn bản từ KG được bổ sung bởi LLM bao gồm nghiên cứu sử dụng LLMs để tạo ngôn ngữ tự nhiên mô tả các sự kiện từ KGs.

5) Trả lời câu hỏi KG được bổ sung bởi LLM bao gồm các nghiên cứu áp dụng LLMs để bắc cầu khoảng cách giữa các câu hỏi ngôn ngữ tự nhiên và truy xuất câu trả lời từ KGs.

LLMs + KGs được đồng bộ hóa. Sự đồng bộ của LLMs và KGs nhằm tích hợp LLMs và KGs thành một khung thống nhất để tăng cường lẫn nhau. Trong phân loại này, chúng tôi xem xét các nỗ lực gần đây của LLMs + KGs được đồng bộ hóa từ các góc độ biểu diễn kiến thức và suy luận.

Trong các phần tiếp theo (Mục 4, 5, và 6), chúng tôi sẽ cung cấp chi tiết về các phân loại này.

4 LLMs ĐƯỢC TĂNG CƯỜNG BỞI KG

Các mô hình ngôn ngữ lớn (LLMs) đạt được kết quả đầy hứa hẹn trong nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên. Tuy nhiên, LLMs đã bị chỉ trích vì thiếu kiến thức thực tế và có xu hướng tạo ra lỗi thực tế trong quá trình suy luận. Để giải quyết vấn đề này, các nhà nghiên cứu đã đề xuất tích hợp đồ thị tri thức (KGs) để tăng cường LLMs. Trong mục này, trước tiên chúng tôi giới thiệu tiền huấn luyện LLM được tăng cường bởi KG, nhằm tiêm kiến thức vào LLMs trong giai đoạn tiền huấn luyện. Sau đó, chúng tôi giới thiệu suy luận LLM được tăng cường bởi KG, cho phép LLMs xem xét kiến thức mới nhất khi tạo câu. Cuối cùng, chúng tôi giới thiệu khả năng diễn giải LLM được tăng cường bởi KG, nhằm cải thiện khả năng diễn giải của LLMs bằng cách sử dụng KGs. Bảng 2 tóm tắt các phương pháp điển hình tích hợp KGs cho LLMs.

4.1 Tiền huấn luyện LLM được tăng cường bởi KG

Các mô hình ngôn ngữ lớn hiện tại chủ yếu dựa vào huấn luyện không giám sát trên kho dữ liệu quy mô lớn. Mặc dù các mô hình này có thể thể hiện hiệu suất ấn tượng trên các nhiệm vụ downstream, chúng thường thiếu kiến thức thực tế liên quan đến thế giới thực. Các công trình trước đây tích hợp KGs vào các mô hình ngôn ngữ lớn có thể được phân loại thành ba phần: 1) Tích hợp KGs vào mục tiêu huấn luyện, 2) Tích hợp KGs vào đầu vào LLM, và 3) Tinh chỉnh hướng dẫn KGs.

4.1.1 Tích hợp KGs vào Mục tiêu Huấn luyện

Các nỗ lực nghiên cứu trong danh mục này tập trung vào việc thiết kế các mục tiêu huấn luyện có nhận thức kiến thức mới. Một ý tưởng trực quan là phơi bày nhiều thực thể kiến thức hơn trong mục tiêu tiền huấn luyện. GLM [102] tận dụng cấu trúc đồ thị tri thức để gán xác suất che. Cụ thể, các thực thể có thể được tiếp cận trong một số bước nhảy nhất định được coi là những thực thể quan trọng nhất cho việc học, và chúng được cho xác suất che cao hơn trong quá trình tiền huấn luyện. Hơn nữa, E-BERT [103] kiểm soát thêm sự cân bằng giữa tổn thất huấn luyện cấp token và cấp thực thể. Các giá trị tổn thất huấn luyện được sử dụng như dấu hiệu của quá trình học cho token và thực thể, xác định động tỷ lệ của chúng cho các epoch huấn luyện tiếp theo. SKEP [124] cũng theo một sự hợp nhất tương tự để tiêm kiến thức cảm xúc trong quá trình tiền huấn luyện LLMs. SKEP trước tiên xác định các từ có cảm xúc tích cực và tiêu cực bằng cách sử dụng PMI cùng với một tập hợp các từ hạt giống cảm xúc được định nghĩa trước. Sau đó, nó gán xác suất che cao hơn cho những từ cảm xúc đã xác định đó trong mục tiêu che từ.

Dòng công việc khác tận dụng rõ ràng các kết nối với kiến thức và văn bản đầu vào. Như được thể hiện trong Hình 9, ERNIE [35] đề xuất một mục tiêu huấn luyện căn chỉnh từ-thực thể mới như một mục tiêu tiền huấn luyện. Cụ thể, ERNIE đưa cả câu và các thực thể tương ứng được đề cập trong văn bản vào LLMs, và sau đó huấn luyện LLMs để dự đoán các liên kết căn chỉnh giữa các token văn bản và thực thể trong đồ thị tri thức. Tương tự, KALM [91] tăng cường các token đầu vào bằng cách tích hợp nhúng thực thể và bao gồm một nhiệm vụ tiền huấn luyện dự đoán thực thể ngoài mục tiêu tiền huấn luyện chỉ có token. Phương pháp này nhằm cải thiện khả năng của LLMs trong việc nắm bắt kiến thức liên quan đến thực thể. Cuối cùng, KEPLER [40] trực tiếp sử dụng cả mục tiêu huấn luyện nhúng đồ thị tri thức và mục tiêu tiền huấn luyện token bị che vào một bộ mã hóa chung dựa trên transformer. Deterministic LLM [104] tập trung vào tiền huấn luyện các mô hình ngôn ngữ để nắm bắt kiến thức thực tế quyết định. Nó chỉ che đoạn có thực thể quyết định như câu hỏi và giới thiệu học tương phản và mục tiêu phân loại manh mối bổ sung. WKLM [106] trước tiên thay thế các thực thể trong văn bản bằng các thực thể khác cùng loại và sau đó đưa chúng vào LLMs. Mô hình được tiền huấn luyện thêm để phân biệt liệu các thực thể đã được thay thế hay chưa.

4.1.2 Tích hợp KGs vào Đầu vào LLM

Như được thể hiện trong Hình 10, loại nghiên cứu này tập trung vào việc giới thiệu đồ thị con kiến thức liên quan vào đầu vào của LLMs. Cho một bộ ba đồ thị tri thức và các câu tương ứng, ERNIE 3.0 [101] biểu diễn bộ ba như một chuỗi token và nối trực tiếp chúng với các câu. Nó thêm che ngẫu nhiên token quan hệ trong bộ ba hoặc token trong câu để kết hợp tốt hơn kiến thức với biểu diễn văn bản. Tuy nhiên, phương pháp nối trực tiếp bộ ba kiến thức như vậy cho phép các token trong câu tương tác mạnh mẽ với các token trong đồ thị con kiến thức, có thể dẫn đến Tiếng ồn Kiến thức [36]. Để giải quyết vấn đề này, K-BERT [36] thực hiện bước đầu tiên để tiêm bộ ba kiến thức vào câu thông qua một ma trận hiển thị trong đó chỉ các thực thể kiến thức có quyền truy cập vào thông tin bộ ba kiến thức, trong khi các token trong câu chỉ có thể thấy nhau trong mô-đun self-attention. Để giảm thêm Tiếng ồn Kiến thức, Colake [107] đề xuất một đồ thị từ-kiến thức thống nhất (được thể hiện trong Hình 10) trong đó các token trong câu đầu vào tạo thành một đồ thị từ được kết nối đầy đủ trong đó các token được căn chỉnh với thực thể kiến thức được kết nối với các thực thể lân cận của chúng.

Các phương pháp trên thực sự có thể tiêm một lượng lớn kiến thức vào LLMs. Tuy nhiên, chúng chủ yếu tập trung vào các thực thể phổ biến và bỏ qua những thực thể ít thường xuyên và đuôi dài. DkLLM [108] nhằm cải thiện biểu diễn LLMs đối với những thực thể đó. DkLLM trước tiên đề xuất một phép đo mới để xác định thực thể đuôi dài và sau đó thay thế những thực thể được chọn này trong văn bản bằng nhúng token giả như đầu vào mới cho các mô hình ngôn ngữ lớn. Hơn nữa, Dict-BERT [125] đề xuất tận dụng từ điển bên ngoài để giải quyết vấn đề này. Cụ thể, Dict-BERT cải thiện chất lượng biểu diễn của các từ hiếm bằng cách thêm định nghĩa của chúng từ từ điển vào cuối văn bản đầu vào và huấn luyện mô hình ngôn ngữ để căn chỉnh cục bộ biểu diễn từ hiếm trong câu đầu vào và định nghĩa từ điển cũng như phân biệt liệu văn bản đầu vào và định nghĩa có được ánh xạ chính xác hay không.

4.1.3 Tinh chỉnh Hướng dẫn KGs

Thay vì tiêm kiến thức thực tế vào LLMs, việc Tinh chỉnh Hướng dẫn KGs nhằm tinh chỉnh LLMs để hiểu rõ hơn cấu trúc của KGs và tuân theo hiệu quả hướng dẫn của người dùng để thực hiện các nhiệm vụ phức tạp. Tinh chỉnh Hướng dẫn KGs sử dụng cả sự kiện và cấu trúc của KGs để tạo ra các tập dữ liệu tinh chỉnh hướng dẫn. LLMs được tinh chỉnh trên các tập dữ liệu này có thể trích xuất cả kiến thức thực tế và cấu trúc từ KGs, tăng cường khả năng suy luận của LLMs. KP-PLM [109] trước tiên thiết kế một số mẫu prompt để chuyển đổi đồ thị cấu trúc thành văn bản ngôn ngữ tự nhiên. Sau đó, hai nhiệm vụ tự giám sát được đề xuất để tinh chỉnh LLMs để tận dụng thêm kiến thức từ các prompt này. OntoPrompt [110] đề xuất tinh chỉnh prompt được tăng cường ontology có thể đặt kiến thức về thực thể vào ngữ cảnh của LLMs, được tinh chỉnh thêm trên một số nhiệm vụ downstream. ChatKBQA [111] tinh chỉnh LLMs trên cấu trúc KG để tạo ra các truy vấn logic, có thể được thực thi trên KGs để có được câu trả lời. Để suy luận tốt hơn trên đồ thị, RoG [112] trình bày một khung lập kế hoạch-truy xuất-suy luận. RoG được tinh chỉnh trên cấu trúc KG để tạo ra các đường dẫn quan hệ được củng cố bởi KGs như các kế hoạch trung thực. Các kế hoạch này sau đó được sử dụng để truy xuất các đường dẫn suy luận hợp lệ từ KGs để LLMs thực hiện suy luận trung thực và tạo ra kết quả có thể diễn giải.

Tinh chỉnh Hướng dẫn KGs có thể tận dụng tốt hơn kiến thức từ KGs cho các nhiệm vụ downstream. Tuy nhiên, nó yêu cầu huấn luyện lại các mô hình, điều này tốn thời gian và yêu cầu nhiều tài nguyên.

4.2 Suy luận LLM được tăng cường bởi KG

Các phương pháp trên có thể hợp nhất hiệu quả kiến thức vào LLMs. Tuy nhiên, kiến thức thế giới thực có thể thay đổi và hạn chế của các phương pháp này là chúng không cho phép cập nhật kiến thức được tích hợp mà không cần huấn luyện lại mô hình. Kết quả là, chúng có thể không tổng quát hóa tốt cho kiến thức chưa thấy trong quá trình suy luận [126]. Do đó, nghiên cứu đáng kể đã được dành cho việc giữ không gian kiến thức và không gian văn bản riêng biệt và tiêm kiến thức trong khi suy luận. Các phương pháp này chủ yếu tập trung vào các nhiệm vụ Trả lời Câu hỏi (QA), vì QA yêu cầu mô hình nắm bắt cả ý nghĩa ngữ nghĩa văn bản và kiến thức thế giới thực cập nhật.

4.2.1 Hợp nhất Kiến thức Tăng cường Truy xuất

Hợp nhất Kiến thức Tăng cường Truy xuất là một phương pháp phổ biến để tiêm kiến thức vào LLMs trong quá trình suy luận. Ý tưởng chính là truy xuất kiến thức liên quan từ một kho dữ liệu lớn và sau đó hợp nhất kiến thức được truy xuất vào LLMs. Như được thể hiện trong Hình 11, RAG [92] đề xuất kết hợp các mô-đun không tham số và tham số để xử lý kiến thức bên ngoài. Cho văn bản đầu vào, RAG trước tiên tìm kiếm KG liên quan trong mô-đun không tham số thông qua MIPS để có được một số tài liệu. RAG sau đó xử lý những tài liệu này như các biến ẩn z và đưa chúng vào bộ tạo đầu ra, được trao quyền bởi Seq2Seq LLMs, như thông tin ngữ cảnh bổ sung. Nghiên cứu cho thấy rằng việc sử dụng các tài liệu được truy xuất khác nhau như điều kiện tại các bước tạo khác nhau hoạt động tốt hơn so với chỉ sử dụng một tài liệu duy nhất để hướng dẫn toàn bộ quá trình tạo. Kết quả thực nghiệm cho thấy RAG vượt trội hơn các mô hình baseline chỉ có tham số và chỉ không tham số khác trong QA miền mở. RAG cũng có thể tạo ra văn bản cụ thể, đa dạng và thực tế hơn so với các baseline chỉ có tham số khác. Story-fragments [127] cải thiện thêm kiến trúc bằng cách thêm một mô-đun bổ sung để xác định các thực thể kiến thức nổi bật và hợp nhất chúng vào bộ tạo để cải thiện chất lượng của các câu chuyện dài được tạo ra. EMAT [115] cải thiện thêm hiệu quả của hệ thống như vậy bằng cách mã hóa kiến thức bên ngoài thành bộ nhớ key-value và khai thác tìm kiếm tích vô hướng tối đa nhanh để truy vấn bộ nhớ. REALM [114] đề xuất một bộ truy xuất kiến thức mới để giúp mô hình truy xuất và chú ý đến các tài liệu từ một kho dữ liệu lớn trong giai đoạn tiền huấn luyện và cải thiện thành công hiệu suất của trả lời câu hỏi miền mở. KGLM [113] chọn các sự kiện từ một đồ thị tri thức sử dụng ngữ cảnh hiện tại để tạo ra các câu thực tế. Với sự giúp đỡ của một đồ thị tri thức bên ngoài, KGLM có thể mô tả các sự kiện sử dụng các từ hoặc cụm từ ngoài miền.

4.2.2 Prompting KGs

Để đưa cấu trúc KG tốt hơn vào LLM trong quá trình suy luận, prompting KGs nhằm thiết kế một prompt được chế tác để chuyển đổi KGs có cấu trúc thành chuỗi văn bản, có thể được đưa vào như ngữ cảnh vào LLMs. Theo cách này, LLMs có thể tận dụng tốt hơn cấu trúc của KGs để thực hiện suy luận. Li et al. [64] áp dụng mẫu được định nghĩa trước để chuyển đổi mỗi bộ ba thành một câu ngắn, có thể được hiểu bởi LLMs để suy luận. Mindmap [65] thiết kế một prompt KG để chuyển đổi cấu trúc đồ thị thành một sơ đồ tư duy cho phép LLMs thực hiện suy luận bằng cách củng cố các sự kiện trong KGs và kiến thức ngầm từ LLMs. ChatRule [116] lấy mẫu một số đường dẫn quan hệ từ KGs, được phát âm và đưa vào LLMs. Sau đó, LLMs được nhắc tạo ra các quy tắc logic có ý nghĩa có thể được sử dụng để suy luận. CoK [117] đề xuất một prompting chuỗi kiến thức sử dụng một chuỗi bộ ba để kích thích khả năng suy luận của LLMs để đạt được câu trả lời cuối cùng.

Prompting KGs trình bày một cách đơn giản để đồng bộ hóa LLMs và KGs. Bằng cách sử dụng prompt, chúng ta có thể dễ dàng khai thác sức mạnh của LLMs để thực hiện suy luận dựa trên KGs mà không cần huấn luyện lại các mô hình. Tuy nhiên, prompt thường được thiết kế thủ công, yêu cầu nhiều nỗ lực con người.

4.3 So sánh giữa Tiền huấn luyện LLM được tăng cường bởi KG và Suy luận

Các phương pháp Tiền huấn luyện LLM được tăng cường bởi KG thường làm phong phú kho dữ liệu không nhãn quy mô lớn với kiến thức thế giới thực có liên quan ngữ nghĩa. Các phương pháp này cho phép biểu diễn kiến thức được căn chỉnh với ngữ cảnh ngôn ngữ phù hợp và huấn luyện rõ ràng LLMs để tận dụng những kiến thức đó từ đầu. Khi áp dụng các LLMs kết quả cho các nhiệm vụ downstream chuyên sâu về kiến thức, chúng nên đạt được hiệu suất tối ưu. Ngược lại, các phương pháp Suy luận LLM được tăng cường bởi KG chỉ trình bày kiến thức cho LLMs trong giai đoạn suy luận và LLMs cơ bản có thể không được huấn luyện để tận dụng đầy đủ những kiến thức này khi thực hiện các nhiệm vụ downstream, có thể dẫn đến hiệu suất mô hình không tối ưu.

Tuy nhiên, kiến thức thế giới thực là động và yêu cầu cập nhật thường xuyên. Mặc dù hiệu quả, các phương pháp Tiền huấn luyện LLM được tăng cường bởi KG không bao giờ cho phép cập nhật hoặc chỉnh sửa kiến thức mà không cần huấn luyện lại mô hình. Kết quả là, các phương pháp Tiền huấn luyện LLM được tăng cường bởi KG có thể tổng quát hóa kém đối với kiến thức gần đây hoặc chưa thấy. Các phương pháp Suy luận LLM được tăng cường bởi KG có thể dễ dàng duy trì cập nhật kiến thức bằng cách thay đổi đầu vào suy luận. Các phương pháp này giúp cải thiện hiệu suất LLMs trên kiến thức và miền mới.

Tóm lại, khi nào sử dụng các phương pháp này phụ thuộc vào các tình huống ứng dụng. Nếu ai đó muốn áp dụng LLMs để xử lý kiến thức không nhạy cảm với thời gian trong các lĩnh vực cụ thể (ví dụ: kiến thức thường thức và suy luận), các phương pháp Tiền huấn luyện LLM được tăng cường bởi KG nên được xem xét. Nếu không, các phương pháp Suy luận LLM được tăng cường bởi KG có thể được sử dụng để xử lý kiến thức miền mở với cập nhật thường xuyên.

4.4 Khả năng diễn giải LLM được tăng cường bởi KG

Mặc dù LLMs đã đạt được thành công đáng kể trong nhiều nhiệm vụ NLP, chúng vẫn bị chỉ trích vì thiếu khả năng diễn giải. Khả năng diễn giải mô hình ngôn ngữ lớn (LLM) đề cập đến việc hiểu và giải thích hoạt động bên trong và quá trình ra quyết định của mô hình ngôn ngữ lớn [17]. Điều này có thể cải thiện độ tin cậy của LLMs và tạo thuận lợi cho việc ứng dụng chúng trong các tình huống có rủi ro cao như chẩn đoán y tế và phán quyết pháp lý. Đồ thị tri thức (KGs) biểu diễn kiến thức một cách có cấu trúc và có thể cung cấp khả năng diễn giải tốt cho kết quả suy luận. Do đó, các nhà nghiên cứu cố gắng sử dụng KGs để cải thiện khả năng diễn giải của LLMs, có thể được nhóm thành hai danh mục: 1) KGs để thăm dò mô hình ngôn ngữ, và 2) KGs để phân tích mô hình ngôn ngữ.

4.4.1 KGs để Thăm dò LLM

Thăm dò mô hình ngôn ngữ lớn (LLM) nhằm hiểu kiến thức được lưu trữ trong LLMs. LLMs, được huấn luyện trên kho dữ liệu quy mô lớn, thường được biết đến là chứa kiến thức khổng lồ. Tuy nhiên, LLMs lưu trữ kiến thức theo cách ẩn, khiến việc tìm ra kiến thức được lưu trữ trở nên khó khăn. Hơn nữa, LLMs gặp phải vấn đề ảo giác [15], dẫn đến tạo ra các tuyên bố mâu thuẫn với sự thật. Vấn đề này ảnh hưởng đáng kể đến độ tin cậy của LLMs. Do đó, cần thiết phải thăm dò và xác minh kiến thức được lưu trữ trong LLMs.

LAMA [14] là công trình đầu tiên thăm dò kiến thức trong LLMs bằng cách sử dụng KGs. Như được thể hiện trong Hình 12, LAMA trước tiên chuyển đổi các sự kiện trong KGs thành các tuyên bố cloze bằng một mẫu prompt được định nghĩa trước và sau đó sử dụng LLMs để dự đoán thực thể bị thiếu. Kết quả dự đoán được sử dụng để đánh giá kiến thức được lưu trữ trong LLMs. Ví dụ, chúng ta cố gắng thăm dò liệu LLMs có biết sự kiện (Obama, nghề nghiệp, tổng thống) hay không. Chúng ta trước tiên chuyển đổi bộ ba sự kiện thành một câu hỏi cloze "Nghề nghiệp của Obama là ." với đối tượng bị che. Sau đó, chúng ta kiểm tra liệu LLMs có thể dự đoán đúng đối tượng "tổng thống" hay không.

Tuy nhiên, LAMA bỏ qua sự thật rằng các prompt là không phù hợp. Ví dụ, prompt "Obama đã làm việc như một" có thể thuận lợi hơn cho việc dự đoán chỗ trống bởi các mô hình ngôn ngữ so với "Obama là một theo nghề nghiệp". Vì vậy, LPAQA [118] đề xuất một phương pháp dựa trên khai thác và diễn giải để tự động tạo ra các prompt chất lượng cao và đa dạng để đánh giá chính xác hơn kiến thức có trong mô hình ngôn ngữ. Hơn nữa, Adolphs et al. [128] cố gắng sử dụng các ví dụ để làm cho mô hình ngôn ngữ hiểu truy vấn, và các thí nghiệm có được cải thiện đáng kể cho BERT-large trên dữ liệu T-REx. Không giống như sử dụng các mẫu prompt được định nghĩa thủ công, Autoprompt [119] đề xuất một phương pháp tự động, dựa trên tìm kiếm được hướng dẫn bởi gradient để tạo prompt. LLM-facteval [121] thiết kế một khung hệ thống tự động tạo ra các câu hỏi thăm dò từ KGs. Các câu hỏi được tạo ra sau đó được sử dụng để đánh giá kiến thức thực tế được lưu trữ trong LLMs.

Thay vì thăm dò kiến thức tổng quát bằng cách sử dụng đồ thị tri thức bách khoa toàn thư và thường thức, BioLAMA [129] và MedLAMA [120] thăm dò kiến thức y tế trong LLMs bằng cách sử dụng đồ thị tri thức y tế. Alex et al. [130] điều tra khả năng của LLMs trong việc giữ lại kiến thức thực tế ít phổ biến. Họ chọn các sự kiện không phổ biến từ đồ thị tri thức Wikidata có các thực thể được nhấp với tần suất thấp. Những sự kiện này sau đó được sử dụng để đánh giá, trong đó kết quả cho thấy LLMs gặp khó khăn với kiến thức như vậy, và việc mở rộng không thể cải thiện đáng kể việc ghi nhớ kiến thức thực tế ở phần đuôi.

4.4.2 KGs để Phân tích LLM

Đồ thị tri thức (KGs) để phân tích các mô hình ngôn ngữ tiền huấn luyện (LLMs) nhằm trả lời các câu hỏi sau như "LLMs tạo ra kết quả như thế nào?", và "chức năng và cấu trúc hoạt động như thế nào trong LLMs?". Để phân tích quá trình suy luận của LLMs, như được thể hiện trong Hình 13, KagNet [38] và QA-GNN [131] làm cho kết quả được tạo ra bởi LLMs tại mỗi bước suy luận được củng cố bởi đồ thị tri thức. Theo cách này, quá trình suy luận của LLMs có thể được giải thích bằng cách trích xuất cấu trúc đồ thị từ KGs. Shaobo et al. [123] điều tra cách LLMs tạo ra kết quả chính xác. Họ áp dụng phân tích lấy cảm hứng từ nhân quả từ các sự kiện được trích xuất từ KGs. Phân tích này đo lường định lượng các mẫu từ mà LLMs phụ thuộc vào để tạo ra kết quả. Kết quả cho thấy LLMs tạo ra các sự kiện bị thiếu hơn bằng các từ gần vị trí hơn là các từ phụ thuộc kiến thức. Vì vậy, họ tuyên bố rằng LLMs không đủ khả năng để ghi nhớ kiến thức thực tế vì sự phụ thuộc không chính xác. Để diễn giải việc huấn luyện LLMs, Swamy et al. [122] áp dụng mô hình ngôn ngữ trong quá trình tiền huấn luyện để tạo ra đồ thị tri thức. Kiến thức được thu thập bởi LLMs trong quá trình huấn luyện có thể được tiết lộ bằng các sự kiện trong KGs một cách rõ ràng. Để khám phá cách kiến thức ngầm được lưu trữ trong các tham số của LLMs, Dai et al. [39] đề xuất khái niệm neuron kiến thức. Cụ thể, việc kích hoạt các neuron kiến thức được xác định có tương quan cao với biểu hiện kiến thức. Vì vậy, họ khám phá kiến thức và sự kiện được đại diện bởi mỗi neuron bằng cách ức chế và khuếch đại các neuron kiến thức.

5 KGS ĐƯỢC BỔ SUNG BỞI LLM

Đồ thị tri thức nổi tiếng với việc biểu diễn kiến thức theo cách có cấu trúc. Chúng đã được áp dụng trong nhiều nhiệm vụ downstream như trả lời câu hỏi, đề xuất, và tìm kiếm web. Tuy nhiên, các KGs thông thường thường không hoàn chỉnh và các phương pháp hiện có thường thiếu xem xét thông tin văn bản. Để giải quyết các vấn đề này, nghiên cứu gần đây đã khám phá việc tích hợp LLMs để bổ sung KGs để xem xét thông tin văn bản và cải thiện hiệu suất trong các nhiệm vụ downstream. Trong mục này, chúng tôi sẽ giới thiệu nghiên cứu gần đây về KGs được bổ sung bởi LLM. Chúng tôi sẽ giới thiệu các phương pháp tích hợp LLMs cho nhúng KG, hoàn thành KG, xây dựng KG, tạo văn bản từ KG, và trả lời câu hỏi KG, tương ứng. Các công trình đại diện được tóm tắt trong Bảng 3.

5.1 Nhúng KG được bổ sung bởi LLM

Nhúng đồ thị tri thức (KGE) nhằm ánh xạ mỗi thực thể và quan hệ vào một không gian vector có chiều thấp (nhúng). Những nhúng này chứa cả thông tin ngữ nghĩa và cấu trúc của KGs, có thể được sử dụng cho các nhiệm vụ khác nhau như trả lời câu hỏi [180], suy luận [38], và đề xuất [181]. Các phương pháp nhúng đồ thị tri thức thông thường chủ yếu dựa vào thông tin cấu trúc của KGs để tối ưu hóa một hàm điểm số được định nghĩa trên nhúng (ví dụ: TransE [33], và DisMult [182]). Tuy nhiên, các phương pháp này thường thiếu khả năng biểu diễn các thực thể chưa thấy và quan hệ đuôi dài do kết nối cấu trúc hạn chế của chúng [183], [184]. Để giải quyết vấn đề này, như được thể hiện trong Hình 14, nghiên cứu gần đây áp dụng LLMs để làm phong phú biểu diễn của KGs bằng cách mã hóa mô tả văn bản của thực thể và quan hệ [40], [94].

5.1.1 LLMs như Bộ mã hóa Văn bản

Pretrain-KGE [94] là một phương pháp đại diện theo khung được thể hiện trong Hình 14. Cho một bộ ba (h, r, t) từ KGs, trước tiên nó sử dụng một bộ mã hóa LLM để mã hóa mô tả văn bản của thực thể h, t, và quan hệ r thành biểu diễn như

eh=LLM(Texth), et=LLM(Textt), er=LLM(Textr), (1)

trong đó eh, er, và et biểu thị nhúng ban đầu của thực thể h, t, và quan hệ r, tương ứng. Pretrain-KGE sử dụng BERT như bộ mã hóa LLM trong các thí nghiệm. Sau đó, các nhúng ban đầu được đưa vào một mô hình KGE để tạo ra các nhúng cuối cùng vh, vr, và vt. Trong giai đoạn huấn luyện KGE, họ tối ưu hóa mô hình KGE bằng cách tuân theo hàm tổn thất KGE tiêu chuẩn như

L= [γ+f(vh, vr, vt)−f(v′h, v′r, v′t)], (2)

trong đó f là hàm điểm số KGE, γ là một siêu tham số biên, và v′h, v′r, và v′t là các mẫu âm. Theo cách này, mô hình KGE có thể học thông tin cấu trúc đầy đủ, trong khi dự trữ kiến thức một phần từ LLM cho phép nhúng đồ thị tri thức tốt hơn. KEPLER [40] cung cấp một mô hình thống nhất cho nhúng kiến thức và biểu diễn ngôn ngữ tiền huấn luyện. Mô hình này không chỉ tạo ra nhúng kiến thức được tăng cường văn bản hiệu quả sử dụng LLMs mạnh mẽ mà còn tích hợp liền mạch kiến thức thực tế vào LLMs. Nayyeri et al. [132] sử dụng LLMs để tạo ra biểu diễn cấp từ, cấp câu, và cấp tài liệu. Chúng được tích hợp với nhúng cấu trúc đồ thị thành một vector thống nhất bằng biểu diễn Dihedron và Quaternion của số siêu phức 4D. Huang et al. [133] kết hợp LLMs với các bộ mã hóa thị giác và đồ thị khác để học nhúng đồ thị tri thức đa phương thức tăng cường hiệu suất của các nhiệm vụ downstream. CoDEx [134] trình bày một hàm tổn thất mới được trao quyền bởi LLMs hướng dẫn các mô hình KGE trong việc đo lường khả năng của các bộ ba bằng cách xem xét thông tin văn bản. Hàm tổn thất được đề xuất là bất khả tri với cấu trúc mô hình có thể được tích hợp với bất kỳ mô hình KGE nào.

5.1.2 LLMs để Nhúng Văn bản và KG Chung

Thay vì sử dụng mô hình KGE để xem xét cấu trúc đồ thị, một dòng phương pháp khác trực tiếp sử dụng LLMs để tích hợp cả cấu trúc đồ thị và thông tin văn bản vào không gian nhúng đồng thời. Như được thể hiện trong Hình 15, kNN-KGE [136] xử lý các thực thể và quan hệ như các token đặc biệt trong LLM. Trong quá trình huấn luyện, nó chuyển đổi mỗi bộ ba (h, r, t) và mô tả văn bản tương ứng thành một câu x như

x=[CLS] hTexth[SEP] r[SEP] [MASK] Textt[SEP] , (3)

trong đó các thực thể đuôi được thay thế bằng [MASK]. Câu được đưa vào một LLM, sau đó tinh chỉnh mô hình để dự đoán thực thể bị che, được hình thức hóa như

PLLM(t|h, r) =P([MASK]=t |x,Θ), (4)

trong đó Θ biểu thị các tham số của LLM. LLM được tối ưu hóa để tối đa hóa xác suất của thực thể chính xác t. Sau khi huấn luyện, các biểu diễn token tương ứng trong LLMs được sử dụng như nhúng cho thực thể và quan hệ. Tương tự, LMKE [135] đề xuất một phương pháp học tương phản để cải thiện việc học các nhúng được tạo ra bởi LLMs cho KGE. Trong khi đó, để nắm bắt tốt hơn cấu trúc đồ thị, LambdaKG [137] lấy mẫu các thực thể lân cận 1-hop và nối các token của chúng với bộ ba như một câu đưa vào LLMs.

5.2 Hoàn thành KG được bổ sung bởi LLM

Hoàn thành Đồ thị Tri thức (KGC) đề cập đến nhiệm vụ suy luận các sự kiện bị thiếu trong một đồ thị tri thức cho trước. Tương tự như KGE, các phương pháp KGC thông thường chủ yếu tập trung vào cấu trúc của KG, mà không xem xét thông tin văn bản rộng lớn. Tuy nhiên, việc tích hợp gần đây của LLMs cho phép các phương pháp KGC mã hóa văn bản hoặc tạo ra sự kiện để có hiệu suất KGC tốt hơn. Các phương pháp này rơi vào hai danh mục riêng biệt dựa trên phong cách sử dụng của chúng: 1) LLM như Bộ mã hóa (PaE), và 2) LLM như Bộ tạo (PaG).

5.2.1 LLM như Bộ mã hóa (PaE).

Như được thể hiện trong Hình 16 (a), (b), và (c), dòng công việc này trước tiên sử dụng LLMs chỉ có encoder để mã hóa thông tin văn bản cũng như sự kiện KG. Sau đó, chúng dự đoán tính hợp lý của các bộ ba hoặc thực thể bị che bằng cách đưa biểu diễn được mã hóa vào một đầu dự đoán, có thể là một MLP đơn giản hoặc hàm điểm số KG thông thường (ví dụ: TransE [33] và TransR [185]).

Mã hóa Chung. Vì các LLMs chỉ có encoder (ví dụ: Bert [1]) giỏi mã hóa chuỗi văn bản, KG-BERT [26] biểu diễn một bộ ba (h, r, t) như một chuỗi văn bản và mã hóa nó bằng LLM Hình 16(a).

x=[CLS] Texth[SEP] Textr[SEP] Textt[SEP] , (5)

Trạng thái ẩn cuối cùng của token [CLS] được đưa vào một bộ phân loại để dự đoán khả năng của bộ ba, được hình thức hóa như

s=σ(MLP(e[CLS])), (6)

trong đó σ(·) biểu thị hàm sigmoid và e[CLS] biểu thị biểu diễn được mã hóa bởi LLMs. Để cải thiện hiệu quả của KG-BERT, MTL-KGC [138] đề xuất một khung Học Đa nhiệm vụ cho KGC tích hợp các nhiệm vụ phụ bổ sung vào huấn luyện của mô hình, tức là dự đoán (RP) và xếp hạng liên quan (RR). PKGC [139] đánh giá tính hợp lệ của một bộ ba (h, r, t) bằng cách chuyển đổi bộ ba và thông tin hỗ trợ của nó thành các câu ngôn ngữ tự nhiên với các mẫu được định nghĩa trước. Các câu này sau đó được xử lý bởi LLMs để phân loại nhị phân. Thông tin hỗ trợ của bộ ba được bắt nguồn từ các thuộc tính của h và t với một hàm phát âm. Ví dụ, nếu bộ ba là (Lebron James, thành viên của đội thể thao, Lakers), thông tin về Lebron James được phát âm như "Lebron James: cầu thủ bóng rổ Mỹ". LASS [140] quan sát rằng ngữ nghĩa ngôn ngữ và cấu trúc đồ thị đều quan trọng cho KGC. Kết quả là, LASS được đề xuất để học chung hai loại nhúng: nhúng ngữ nghĩa và nhúng cấu trúc. Trong phương pháp này, toàn bộ văn bản của một bộ ba được chuyển tiếp đến LLM, và trung bình gộp các đầu ra LLM tương ứng cho h, r, và t được tính toán riêng biệt. Những nhúng này sau đó được chuyển đến một phương pháp dựa trên đồ thị, tức là TransE, để tái tạo cấu trúc KG.

Mã hóa MLM. Thay vì mã hóa toàn bộ văn bản của một bộ ba, nhiều công trình giới thiệu khái niệm Mô hình Ngôn ngữ Bị che (MLM) để mã hóa văn bản KG (Hình 16(b)). MEM-KGC [141] sử dụng cơ chế phân loại Mô hình Thực thể Bị che (MEM) để dự đoán các thực thể bị che của bộ ba. Văn bản đầu vào có dạng

x=[CLS] Texth[SEP] Textr[SEP] [MASK] [SEP] , (7)

Tương tự như Phương trình 4, nó cố gắng tối đa hóa xác suất rằng thực thể bị che là thực thể chính xác t. Thêm vào đó, để cho phép mô hình học các thực thể chưa thấy, MEM-KGC tích hợp học đa nhiệm vụ cho dự đoán thực thể và siêu lớp dựa trên mô tả văn bản của thực thể:

x=[CLS] [MASK] [SEP] Texth[SEP] . (8)

OpenWorld KGC [142] mở rộng mô hình MEM-KGC để giải quyết các thách thức của KGC thế giới mở với một khung pipeline, trong đó hai mô-đun tuần tự dựa trên MLM được định nghĩa: Dự đoán Mô tả Thực thể (EDP), một mô-đun phụ dự đoán một thực thể tương ứng với một mô tả văn bản cho trước; Dự đoán Bộ ba Không hoàn chỉnh (ITP), mô-đun mục tiêu dự đoán một thực thể hợp lý cho một bộ ba không hoàn chỉnh cho trước (h, r,?). EDP trước tiên mã hóa bộ ba bằng Phương trình 8 và tạo ra trạng thái ẩn cuối cùng, sau đó được chuyển tiếp vào ITP như một nhúng của thực thể đầu trong Phương trình 7 để dự đoán thực thể mục tiêu.

Mã hóa Riêng biệt. Như được thể hiện trong Hình 16(c), các phương pháp này bao gồm việc phân chia một bộ ba (h, r, t) thành hai phần riêng biệt, tức là (h, r) và t, có thể được biểu hiện như

x(h,r)=[CLS] Texth[SEP] Textr[SEP] , (9)
xt=[CLS] Textt[SEP] . (10)

Sau đó hai phần được mã hóa riêng biệt bởi LLMs, và các trạng thái ẩn cuối cùng của các token [CLS] được sử dụng như biểu diễn của (h, r) và t, tương ứng. Các biểu diễn sau đó được đưa vào một hàm điểm số để dự đoán khả năng của bộ ba, được hình thức hóa như

s=fscore(e(h,r), et), (11)

trong đó fscore biểu thị hàm điểm số như TransE.

StAR [143] áp dụng các bộ mã hóa văn bản kiểu Siamese trên văn bản của chúng, mã hóa chúng thành các biểu diễn có ngữ cảnh riêng biệt. Để tránh sự bùng nổ tổ hợp của các phương pháp mã hóa văn bản, ví dụ: KG-BERT, StAR sử dụng một mô-đun điểm số bao gồm cả bộ phân loại quyết định và đo lường không gian cho việc học biểu diễn và cấu trúc tương ứng, cũng tăng cường kiến thức có cấu trúc bằng cách khám phá các đặc tính không gian. SimKGC [144] là một ví dụ khác về việc tận dụng bộ mã hóa văn bản Siamese để mã hóa biểu diễn văn bản. Sau quá trình mã hóa, SimKGC áp dụng các kỹ thuật học tương phản cho những biểu diễn này. Quá trình này bao gồm việc tính toán sự tương tự giữa các biểu diễn được mã hóa của một bộ ba cho trước và các mẫu tích cực và tiêu cực của nó. Cụ thể, sự tương tự giữa biểu diễn được mã hóa của bộ ba và mẫu tích cực được tối đa hóa, trong khi sự tương tự giữa biểu diễn được mã hóa của bộ ba và mẫu tiêu cực được tối thiểu hóa. Điều này cho phép SimKGC học một không gian biểu diễn tách biệt các bộ ba hợp lý và không hợp lý. Để tránh việc overfitting thông tin văn bản, CSPromp-KG [186] sử dụng học prompt hiệu quả tham số cho KGC.

LP-BERT [145] là một phương pháp KGC lai kết hợp cả Mã hóa MLM và Mã hóa Riêng biệt. Phương pháp này bao gồm hai giai đoạn, cụ thể là tiền huấn luyện và tinh chỉnh. Trong quá trình tiền huấn luyện, phương pháp sử dụng cơ chế MLM tiêu chuẩn để tiền huấn luyện một LLM với dữ liệu KGC. Trong giai đoạn tinh chỉnh, LLM mã hóa cả hai phần và được tối ưu hóa sử dụng một chiến lược học tương phản (tương tự như SimKGC [144]).

5.2.2 LLM như Bộ tạo (PaG).

Các công trình gần đây sử dụng LLMs như các bộ tạo chuỗi đến chuỗi trong KGC. Như được trình bày trong Hình 17 (a) và (b), các phương pháp này bao gồm LLMs encoder-decoder hoặc chỉ có decoder. LLMs nhận đầu vào văn bản chuỗi của bộ ba truy vấn (h, r,?), và tạo ra văn bản của thực thể đuôi t trực tiếp.

GenKGC [96] sử dụng mô hình ngôn ngữ lớn BART [5] làm mô hình xương sống. Lấy cảm hứng từ phương pháp học trong ngữ cảnh được sử dụng trong GPT-3 [59], trong đó mô hình nối các mẫu liên quan để học câu trả lời đầu ra chính xác, GenKGC đề xuất một kỹ thuật trình diễn được hướng dẫn bởi quan hệ bao gồm các bộ ba có cùng quan hệ để tạo thuận lợi cho quá trình học của mô hình. Ngoài ra, trong quá trình tạo, một phương pháp giải mã phân cấp có nhận thức thực thể được đề xuất để giảm độ phức tạp thời gian. KGT5 [146] giới thiệu một mô hình KGC mới hoàn thành bốn yêu cầu chính của các mô hình như vậy: khả năng mở rộng, chất lượng, tính linh hoạt và đơn giản. Để giải quyết các mục tiêu này, mô hình được đề xuất sử dụng kiến trúc T5 small đơn giản. Mô hình khác biệt với các phương pháp KGC trước đây, trong đó nó được khởi tạo ngẫu nhiên thay vì sử dụng các mô hình tiền huấn luyện. KG-S2S [147] là một khung toàn diện có thể được áp dụng cho các loại nhiệm vụ KGC khác nhau, bao gồm KGC Tĩnh, KGC Thời gian, và KGC Few-shot. Để đạt được mục tiêu này, KG-S2S tái hình thức hóa sự kiện bộ ba KG tiêu chuẩn bằng cách giới thiệu một yếu tố bổ sung, tạo thành một bộ bốn (h, r, t, m), trong đó m đại diện cho yếu tố "điều kiện" bổ sung. Mặc dù các nhiệm vụ KGC khác nhau có thể đề cập đến các điều kiện khác nhau, chúng thường có định dạng văn bản tương tự, cho phép thống nhất trên các nhiệm vụ KGC khác nhau. Phương pháp KG-S2S tích hợp các kỹ thuật khác nhau như mô tả thực thể, prompt mềm, và Seq2Seq Dropout để cải thiện hiệu suất của mô hình. Ngoài ra, nó sử dụng giải mã bị ràng buộc để đảm bảo các thực thể được tạo ra là hợp lệ. Đối với các LLMs mã nguồn đóng (ví dụ: ChatGPT và GPT-4), AutoKG áp dụng kỹ thuật prompt để thiết kế các prompt tùy chỉnh [93]. Như được thể hiện trong Hình 18, các prompt này chứa mô tả nhiệm vụ, các ví dụ few-shot, và đầu vào kiểm tra, hướng dẫn LLMs dự đoán thực thể đuôi để hoàn thành KG.

So sánh giữa PaE và PaG. LLMs như Bộ mã hóa (PaE) áp dụng một đầu dự đoán bổ sung trên đỉnh biểu diễn được mã hóa bởi LLMs. Do đó, khung PaE dễ tinh chỉnh hơn nhiều vì chúng ta chỉ có thể tối ưu hóa các đầu dự đoán và đóng băng LLMs. Hơn nữa, đầu ra của dự đoán có thể được chỉ định dễ dàng và tích hợp với các hàm KGC hiện có cho các nhiệm vụ KGC khác nhau. Tuy nhiên, trong giai đoạn suy luận, PaE yêu cầu tính toán điểm số cho mọi ứng viên trong KGs, có thể tốn kém về mặt tính toán. Bên cạnh đó, chúng không thể tổng quát hóa cho các thực thể chưa thấy. Hơn nữa, PaE yêu cầu đầu ra biểu diễn của LLMs, trong khi một số LLMs hiện đại (ví dụ: GPT-4¹) là mã nguồn đóng và không cấp quyền truy cập vào đầu ra biểu diễn.

LLMs như Bộ tạo (PaG), mặt khác, không cần đầu dự đoán, có thể được sử dụng mà không cần tinh chỉnh hoặc truy cập vào biểu diễn. Do đó, khung PaG phù hợp với tất cả các loại LLMs. Ngoài ra, PaG trực tiếp tạo ra thực thể đuôi, làm cho nó hiệu quả trong suy luận mà không cần xếp hạng tất cả các ứng viên và dễ dàng tổng quát hóa cho các thực thể chưa thấy. Nhưng, thách thức của PaG là các thực thể được tạo ra có thể đa dạng và không nằm trong KGs. Hơn nữa, thời gian của một suy luận đơn lẻ dài hơn do tạo tự hồi quy. Cuối cùng, cách thiết kế một prompt mạnh mẽ đưa KGs vào LLMs vẫn là một câu hỏi mở. Do đó, mặc dù PaG đã chứng minh kết quả đầy hứa hẹn cho các nhiệm vụ KGC, sự đánh đổi giữa độ phức tạp mô hình và hiệu quả tính toán phải được xem xét cẩn thận khi chọn một khung KGC dựa trên LLM phù hợp.

5.2.3 Phân tích Mô hình

Justin et al. [187] cung cấp một phân tích toàn diện về các phương pháp KGC được tích hợp với LLMs. Nghiên cứu của họ điều tra chất lượng của các nhúng LLM và thấy rằng chúng không tối ưu cho việc xếp hạng thực thể hiệu quả. Đáp lại, họ đề xuất một số kỹ thuật xử lý nhúng để cải thiện tính phù hợp của chúng cho việc truy xuất ứng viên. Nghiên cứu cũng so sánh các chiều lựa chọn mô hình khác nhau, như Trích xuất Nhúng, Trích xuất Thực thể Truy vấn, và Lựa chọn Mô hình Ngôn ngữ. Cuối cùng, các tác giả đề xuất một khung thích ứng hiệu quả LLM cho hoàn thành đồ thị tri thức.

5.3 Xây dựng KG được bổ sung bởi LLM

Xây dựng đồ thị tri thức bao gồm việc tạo ra một biểu diễn có cấu trúc của kiến thức trong một lĩnh vực cụ thể. Điều này bao gồm việc xác định các thực thể và mối quan hệ của chúng với nhau. Quá trình xây dựng đồ thị tri thức thường bao gồm nhiều giai đoạn, bao gồm 1) khám phá thực thể, 2) giải quyết coreference, và 3) trích xuất quan hệ. Hình 19 trình bày khung chung của việc áp dụng LLMs cho mỗi giai đoạn trong xây dựng KG. Các phương pháp gần đây hơn đã khám phá 4) xây dựng đồ thị tri thức đầu cuối đến cuối, bao gồm việc xây dựng một đồ thị tri thức hoàn chỉnh trong một bước hoặc trực tiếp 5) chưng cất đồ thị tri thức từ LLMs.

5.3.1 Khám phá Thực thể

Khám phá thực thể trong xây dựng KG đề cập đến quá trình xác định và trích xuất các thực thể từ các nguồn dữ liệu không có cấu trúc, như tài liệu văn bản, trang web, hoặc bài đăng trên mạng xã hội, và tích hợp chúng để xây dựng đồ thị tri thức.

Nhận dạng Thực thể Có tên (NER) bao gồm việc xác định và gắn thẻ các thực thể có tên trong dữ liệu văn bản với vị trí và phân loại của chúng. Các thực thể có tên bao gồm con người, tổ chức, địa điểm, và các loại thực thể khác. Các phương pháp NER hiện đại thường sử dụng LLMs để tận dụng hiểu biết ngữ cảnh và kiến thức ngôn ngữ của chúng để nhận dạng và phân loại thực thể chính xác. Có ba nhiệm vụ con NER dựa trên các loại span NER được xác định, tức là NER phẳng, NER lồng nhau, và NER không liên tục. 1) NER phẳng là để xác định các thực thể có tên không chồng chéo từ văn bản đầu vào. Nó thường được khái niệm hóa như một vấn đề gắn nhãn chuỗi trong đó mỗi token trong văn bản được gán một nhãn duy nhất dựa trên vị trí của nó trong chuỗi [1], [148], [188], [189]. 2) NER lồng nhau xem xét các tình huống phức tạp cho phép một token thuộc về nhiều thực thể. Phương pháp dựa trên span [190]–[194] là một nhánh phổ biến của NER lồng nhau liệt kê tất cả các span ứng viên và phân loại chúng thành các loại thực thể (bao gồm một loại không phải thực thể). Các phương pháp dựa trên phân tích [195]–[197] tiết lộ sự tương tự giữa NER lồng nhau và các nhiệm vụ phân tích cấu trúc (dự đoán các span lồng nhau và không chồng chéo), và đề xuất tích hợp các hiểu biết của phân tích cấu trúc vào NER lồng nhau. 3) NER không liên tục xác định các thực thể có tên có thể không liền kề trong văn bản. Để giải quyết thách thức này, [198] sử dụng đầu ra LLM để xác định các mảnh thực thể và xác định liệu chúng có chồng chéo hay liên tiếp.

Không giống như các phương pháp cụ thể nhiệm vụ, GenerativeNER [149] sử dụng một LLM chuỗi đến chuỗi với cơ chế con trỏ để tạo ra một chuỗi thực thể, có khả năng giải quyết cả ba loại nhiệm vụ con NER.

Phân loại Thực thể (ET) nhằm cung cấp thông tin loại tinh tế và siêu tinh tế cho một thực thể được đề cập trong ngữ cảnh. Các phương pháp này thường sử dụng LLM để mã hóa các đề cập, ngữ cảnh và loại. LDET [150] áp dụng nhúng ELMo tiền huấn luyện [148] cho biểu diễn từ và áp dụng LSTM như các bộ mã hóa câu và đề cập. BOX4Types [151] nhận ra tầm quan trọng của sự phụ thuộc loại và sử dụng BERT để biểu diễn vector ẩn và mỗi loại trong không gian hình chữ nhật siêu (hộp). LRN [199] xem xét các phụ thuộc bên ngoài và bên trong giữa các nhãn. Nó mã hóa ngữ cảnh và thực thể bằng BERT và sử dụng những nhúng đầu ra này để thực hiện suy luận diễn dịch và quy nạp. MLMET [200] sử dụng các mẫu được định nghĩa trước để xây dựng các mẫu đầu vào cho BERT MLM và sử dụng [MASK] để dự đoán các từ ngữ phụ thuộc ngữ cảnh của đề cập, có thể được xem như các nhãn loại. PL [201] và DFET [202] sử dụng học prompt cho phân loại thực thể. LITE [203] hình thức hóa phân loại thực thể như suy luận văn bản và sử dụng RoBERTa-large-MNLI như mạng xương sống.

Liên kết Thực thể (EL), được biết đến như định danh thực thể, bao gồm việc liên kết các đề cập thực thể xuất hiện trong văn bản với các thực thể tương ứng của chúng trong một đồ thị tri thức. [204] đề xuất các hệ thống EL đầu cuối đến cuối dựa trên BERT khám phá và liên kết thực thể chung. ELQ [152] sử dụng kiến trúc bi-encoder nhanh để thực hiện chung phát hiện đề cập và liên kết trong một lần chạy cho các hệ thống trả lời câu hỏi downstream. Không giống như các mô hình trước đây khung EL như khớp trong không gian vector, GENRE [205] hình thức hóa nó như một vấn đề chuỗi đến chuỗi, tạo tự hồi quy một phiên bản của đầu vào được đánh dấu với các định danh duy nhất của một thực thể được biểu hiện bằng ngôn ngữ tự nhiên. GENRE được mở rộng thành phiên bản đa ngôn ngữ mGENRE [206]. Xem xét các thách thức hiệu quả của các phương pháp EL tạo sinh, [207] song song hóa liên kết tự hồi quy trên tất cả các đề cập tiềm năng và dựa vào một decoder nông và hiệu quả. ReFinED [153] đề xuất một phương pháp EL zero-shot hiệu quả bằng cách tận dụng các loại thực thể tinh tế và mô tả thực thể được xử lý bởi một bộ mã hóa dựa trên LLM.

5.3.2 Giải quyết Coreference (CR)

Giải quyết coreference là tìm tất cả các biểu hiện (tức là các đề cập) đề cập đến cùng một thực thể hoặc sự kiện trong một văn bản. CR trong tài liệu đề cập đến nhiệm vụ con CR trong đó tất cả các đề cập này nằm trong một tài liệu duy nhất. Mandar et al. [154] khởi tạo giải quyết coreference dựa trên LLM bằng cách thay thế bộ mã hóa LSTM trước đây [208] bằng BERT. Công trình này được theo sau bởi việc giới thiệu SpanBERT [155] được tiền huấn luyện trên kiến trúc BERT với một mô hình ngôn ngữ bị che dựa trên span (MLM). Lấy cảm hứng từ các công trình này, Tuan Manh et al. [209] trình bày một baseline mạnh bằng cách tích hợp bộ mã hóa SpanBERT vào một phương pháp không phải LLM e2e-coref [208]. CorefBERT tận dụng nhiệm vụ Dự đoán Tham chiếu Đề cập (MRP) che một hoặc một số đề cập và yêu cầu mô hình dự đoán các referent tương ứng của đề cập bị che. CorefQA [210] hình thức hóa giải quyết coreference như một nhiệm vụ trả lời câu hỏi, trong đó các truy vấn ngữ cảnh được tạo cho mỗi đề cập ứng viên và các span coreference được trích xuất từ tài liệu sử dụng các truy vấn. Tuan Manh et al. [211] giới thiệu một cơ chế gating và một phương pháp huấn luyện nhiễu để trích xuất thông tin từ các đề cập sự kiện sử dụng bộ mã hóa SpanBERT.

Để giảm dấu chân bộ nhớ lớn đối mặt bởi các mô hình NER dựa trên LLM lớn, Yuval et al. [212] và Raghuveer el al. [213] đề xuất các mô hình từ đầu đến cuối và xấp xỉ, tương ứng, cả hai sử dụng các hàm song tuyến để tính toán điểm số đề cập và tiền đề với sự phụ thuộc giảm vào biểu diễn cấp span.

CR chéo tài liệu đề cập đến nhiệm vụ con trong đó các đề cập đề cập đến cùng một thực thể hoặc sự kiện có thể nằm trên nhiều tài liệu. CDML [156] đề xuất một phương pháp mô hình hóa ngôn ngữ chéo tài liệu tiền huấn luyện một bộ mã hóa Longformer [214] trên các tài liệu liên quan được nối và sử dụng một MLP cho phân loại nhị phân để xác định liệu một cặp đề cập có coreference hay không. CrossCR [157] sử dụng một mô hình đầu cuối đến cuối cho giải quyết coreference chéo tài liệu tiền huấn luyện bộ chấm điểm đề cập trên các span đề cập vàng và sử dụng một bộ chấm điểm theo cặp để so sánh các đề cập với tất cả các span trên tất cả các tài liệu. CR-RL [158] đề xuất một bộ giải quyết coreference dựa trên học tăng cường sâu actor-critic cho CR chéo tài liệu.

5.3.3 Trích xuất Quan hệ (RE)

Trích xuất quan hệ bao gồm việc xác định các mối quan hệ ngữ nghĩa giữa các thực thể được đề cập trong văn bản ngôn ngữ tự nhiên. Có hai loại phương pháp trích xuất quan hệ, tức là RE cấp câu và RE cấp tài liệu, theo phạm vi của văn bản được phân tích.

RE cấp câu tập trung vào việc xác định quan hệ giữa các thực thể trong một câu duy nhất. Peng et al. [159] và TRE [215] giới thiệu LLM để cải thiện hiệu suất của các mô hình trích xuất quan hệ. BERT-MTB [216] học biểu diễn quan hệ dựa trên BERT bằng cách thực hiện nhiệm vụ khớp-the-blanks và tích hợp các mục tiêu được thiết kế cho trích xuất quan hệ. Curriculum-RE [160] sử dụng học chương trình để cải thiện các mô hình trích xuất quan hệ bằng cách tăng dần độ khó của dữ liệu trong quá trình huấn luyện. RECENT [217] giới thiệu SpanBERT và khai thác hạn chế loại thực thể để giảm các loại quan hệ ứng viên nhiễu. Jiewen [218] mở rộng RECENT bằng cách kết hợp cả thông tin thực thể và thông tin nhãn vào nhúng cấp câu, cho phép nhúng nhận biết thực thể-nhãn.

RE cấp tài liệu (DocRE) nhằm trích xuất quan hệ giữa các thực thể trên nhiều câu trong một tài liệu. Hong et al. [219] đề xuất một baseline mạnh cho DocRE bằng cách thay thế xương sống BiLSTM bằng LLMs. HIN [220] sử dụng LLM để mã hóa và tổng hợp biểu diễn thực thể ở các cấp độ khác nhau, bao gồm cấp thực thể, câu, và tài liệu. GLRE [221] là một mạng từ toàn cầu đến cục bộ, sử dụng LLM để mã hóa thông tin tài liệu theo biểu diễn toàn cầu và cục bộ của thực thể cũng như biểu diễn quan hệ ngữ cảnh. SIRE [222] sử dụng hai bộ mã hóa dựa trên LLM để trích xuất quan hệ trong câu và giữa các câu. LSR [223] và GAIN [224] đề xuất các phương pháp dựa trên đồ thị tạo ra cấu trúc đồ thị trên đỉnh LLM để trích xuất quan hệ tốt hơn. DocuNet [225] hình thức hóa DocRE như một nhiệm vụ phân đoạn ngữ nghĩa và giới thiệu một U-Net [226] trên bộ mã hóa LLM để nắm bắt các phụ thuộc cục bộ và toàn cầu giữa các thực thể. ATLOP [227] tập trung vào các vấn đề đa nhãn trong DocRE, có thể được xử lý bằng hai kỹ thuật, tức là ngưỡng thích ứng cho bộ phân loại và gộp ngữ cảnh địa phương hóa cho LLM. DREEAM [161] mở rộng và cải thiện thêm ATLOP bằng cách tích hợp thông tin bằng chứng.

Xây dựng KG Đầu cuối đến Cuối. Hiện tại, các nhà nghiên cứu đang khám phá việc sử dụng LLMs cho xây dựng KG đầu cuối đến cuối. Kumar et al. [95] đề xuất một phương pháp thống nhất để xây dựng KGs từ văn bản thô, chứa hai thành phần được trao quyền bởi LLMs. Họ trước tiên tinh chỉnh một LLM trên các nhiệm vụ nhận dạng thực thể có tên để làm cho nó có khả năng nhận dạng thực thể trong văn bản thô. Sau đó, họ đề xuất một "2-model BERT" khác để giải quyết nhiệm vụ trích xuất quan hệ, chứa hai bộ phân loại dựa trên BERT. Bộ phân loại đầu tiên học lớp quan hệ trong khi bộ phân loại nhị phân thứ hai học hướng của quan hệ giữa hai thực thể. Các bộ ba và quan hệ được dự đoán sau đó được sử dụng để xây dựng KG. Guo et al. [162] đề xuất một mô hình trích xuất kiến thức đầu cuối đến cuối dựa trên BERT, có thể được áp dụng để xây dựng KGs từ văn bản Trung Quốc Cổ điển. Grapher [41] trình bày một hệ thống đầu cuối đến cuối đa giai đoạn mới. Nó trước tiên sử dụng LLMs để tạo ra các thực thể KG, theo sau bởi một đầu xây dựng quan hệ đơn giản, cho phép xây dựng KG hiệu quả từ mô tả văn bản. PiVE [163] đề xuất một khung prompting với xác minh lặp sử dụng một LLM nhỏ hơn như T5 để sửa các lỗi trong KGs được tạo ra bởi một LLM lớn hơn (ví dụ: ChatGPT). Để khám phá thêm các LLMs tiên tiến, AutoKG thiết kế một số prompt cho các nhiệm vụ xây dựng KG khác nhau (ví dụ: phân loại thực thể, liên kết thực thể, và trích xuất quan hệ). Sau đó, nó áp dụng prompt để thực hiện xây dựng KG sử dụng ChatGPT và GPT-4.

5.3.4 Chưng cất Đồ thị Tri thức từ LLMs

LLMs đã được chứng minh là mã hóa ngầm kiến thức khổng lồ [14]. Như được thể hiện trong Hình 20, một số nghiên cứu nhằm chưng cất kiến thức từ LLMs để xây dựng KGs. COMET [164] đề xuất một mô hình transformer thường thức xây dựng KGs thường thức bằng cách sử dụng các bộ ba hiện có như một tập hạt giống kiến thức để huấn luyện. Sử dụng tập hạt giống này, một LLM học thích ứng các biểu diễn đã học của nó với việc tạo kiến thức, và tạo ra các bộ ba mới có chất lượng cao. Kết quả thực nghiệm tiết lộ rằng kiến thức ngầm từ LLMs được chuyển giao để tạo ra kiến thức rõ ràng trong KGs thường thức. BertNet [165] đề xuất một khung mới cho xây dựng KG tự động được trao quyền bởi LLMs. Nó chỉ yêu cầu định nghĩa tối thiểu của quan hệ như đầu vào và tự động tạo ra các prompt đa dạng, và thực hiện tìm kiếm kiến thức hiệu quả trong một LLM cho trước cho các đầu ra nhất quán. Các KGs được xây dựng cho thấy chất lượng, tính đa dạng, và tính mới cạnh tranh với một tập hợp quan hệ phong phú hơn và phức tạp, không thể được trích xuất bởi các phương pháp trước đây. West et al. [166] đề xuất một khung chưng cất kiến thức tượng trưng chưng cất kiến thức tượng trưng từ LLMs. Họ trước tiên tinh chỉnh một LLM học sinh nhỏ bằng cách chưng cất các sự kiện thường thức từ một LLM lớn như GPT-3. Sau đó, LLM học sinh được sử dụng để tạo ra KGs thường thức.

5.4 Tạo văn bản từ KG được bổ sung bởi LLM

Mục tiêu của việc tạo văn bản từ Đồ thị tri thức (KG-to-text) là tạo ra văn bản chất lượng cao mô tả chính xác và nhất quán thông tin đồ thị tri thức đầu vào [228]. Tạo văn bản từ KG kết nối đồ thị tri thức và văn bản, cải thiện đáng kể khả năng áp dụng của KG trong các tình huống NLG thực tế hơn, bao gồm kể chuyện [229] và đối thoại có nền tảng kiến thức [230]. Tuy nhiên, việc thu thập lượng lớn dữ liệu song song đồ thị-văn bản là thách thức và tốn kém, dẫn đến huấn luyện không đủ và chất lượng tạo kém. Vì vậy, nhiều nỗ lực nghiên cứu sử dụng: 1) tận dụng kiến thức từ LLMs hoặc 2) xây dựng kho dữ liệu KG-văn bản có giám sát yếu quy mô lớn để giải quyết vấn đề này.

5.4.1 Tận dụng Kiến thức từ LLMs

Như các nỗ lực nghiên cứu tiên phong trong việc sử dụng LLMs cho việc tạo văn bản từ KG, Ribeiro et al. [167] và Kale và Rastogi [231] trực tiếp tinh chỉnh các LLMs khác nhau, bao gồm BART và T5, với mục tiêu chuyển giao kiến thức LLMs cho nhiệm vụ này. Như được thể hiện trong Hình 21, cả hai công trình đơn giản biểu diễn đồ thị đầu vào như một duyệt tuyến tính và thấy rằng phương pháp ngây thơ như vậy thành công vượt trội nhiều hệ thống tạo văn bản từ KG hiện đại. Thú vị, Ribeiro et al. [167] cũng thấy rằng tiếp tục tiền huấn luyện có thể cải thiện thêm hiệu suất mô hình. Tuy nhiên, các phương pháp này không thể tích hợp rõ ràng ngữ nghĩa đồ thị phong phú trong KGs. Để tăng cường LLMs với thông tin cấu trúc KG, JointGT [42] đề xuất tiêm biểu diễn bảo tồn cấu trúc KG vào các mô hình ngôn ngữ lớn Seq2Seq. Cho các KG con đầu vào và văn bản tương ứng, JointGT trước tiên biểu diễn các thực thể và quan hệ KG như một chuỗi token, sau đó nối chúng với các token văn bản được đưa vào LLM. Sau mô-đun self-attention tiêu chuẩn, JointGT sau đó sử dụng một lớp gộp để có được biểu diễn ngữ nghĩa ngữ cảnh của các thực thể và quan hệ kiến thức. Cuối cùng, những biểu diễn KG được gộp này sau đó được tổng hợp trong một lớp self-attention có nhận thức cấu trúc khác. JointGT cũng triển khai các mục tiêu tiền huấn luyện bổ sung, bao gồm các nhiệm vụ tái tạo KG và văn bản cho các đầu vào bị che, để cải thiện việc căn chỉnh giữa thông tin văn bản và đồ thị. Li et al. [168] tập trung vào tình huống few-shot. Nó trước tiên sử dụng một chiến lược tìm kiếm chiều rộng đầu tiên (BFS) mới để duyệt tốt hơn cấu trúc KG đầu vào và đưa các biểu diễn đồ thị tuyến tính hóa được tăng cường vào LLMs cho các đầu ra được tạo ra chất lượng cao, sau đó căn chỉnh biểu diễn thực thể KG dựa trên GCN và dựa trên LLM. Colas et al. [169] trước tiên chuyển đổi đồ thị thành biểu diễn phù hợp của nó trước khi tuyến tính hóa đồ thị. Tiếp theo, mỗi nút KG được mã hóa thông qua một cơ chế attention toàn cầu, theo sau bởi một mô-đun attention có nhận thức đồ thị, cuối cùng được giải mã thành một chuỗi token. Khác với các công trình này, KG-BART [37] giữ cấu trúc của KGs và tận dụng attention đồ thị để tổng hợp ngữ nghĩa khái niệm phong phú trong KG con, tăng cường tính tổng quát của mô hình trên các tập khái niệm chưa thấy.

5.4.2 Xây dựng Kho dữ liệu KG-văn bản căn chỉnh yếu quy mô lớn

Mặc dù LLMs đã đạt được thành công thực nghiệm đáng kể, các mục tiêu tiền huấn luyện không giám sát của chúng không nhất thiết được căn chỉnh tốt với nhiệm vụ tạo văn bản từ KG, thúc đẩy các nhà nghiên cứu phát triển kho dữ liệu KG-văn bản căn chỉnh quy mô lớn. Jin et al. [170] đề xuất 1,3M dữ liệu huấn luyện KG-to-graph không giám sát từ Wikipedia. Cụ thể, họ trước tiên phát hiện các thực thể xuất hiện trong văn bản thông qua các hyperlink và bộ phát hiện thực thể có tên, và sau đó chỉ thêm văn bản chia sẻ một tập hợp thực thể chung với đồ thị tri thức tương ứng, tương tự như ý tưởng giám sát khoảng cách trong nhiệm vụ trích xuất quan hệ [232]. Họ cũng cung cấp 1.000+ dữ liệu kiểm tra KG-to-Text được chú thích bởi con người để xác minh hiệu quả của các mô hình KG-to-Text tiền huấn luyện. Tương tự, Chen et al. [171] cũng đề xuất một kho dữ liệu văn bản có nền tảng KG được thu thập từ English Wikidump. Để đảm bảo kết nối giữa KG và văn bản, họ chỉ trích xuất các câu có ít nhất hai liên kết anchor Wikipedia. Sau đó, họ sử dụng các thực thể từ những liên kết đó để truy vấn các lân cận xung quanh trong WikiData và tính toán sự chồng chéo từ vựng giữa những lân cận này và các câu gốc. Cuối cùng, chỉ các cặp chồng chéo cao được chọn. Các tác giả khám phá cả bộ mã hóa dựa trên đồ thị và dựa trên chuỗi và xác định ưu điểm của chúng trong các nhiệm vụ và cài đặt khác nhau.

5.5 Trả lời câu hỏi KG được bổ sung bởi LLM

Trả lời câu hỏi đồ thị tri thức (KGQA) nhằm tìm câu trả lời cho các câu hỏi ngôn ngữ tự nhiên dựa trên các sự kiện có cấu trúc được lưu trữ trong đồ thị tri thức [233], [234]. Thách thức không thể tránh khỏi trong KGQA là truy xuất các sự kiện liên quan và mở rộng ưu thế suy luận của KGs cho QA. Do đó, các nghiên cứu gần đây áp dụng LLMs để bắc cầu khoảng cách giữa các câu hỏi ngôn ngữ tự nhiên và đồ thị tri thức có cấu trúc [174], [175], [235]. Khung chung của việc áp dụng LLMs cho KGQA được minh họa trong Hình 22, trong đó LLMs có thể được sử dụng như 1) bộ trích xuất thực thể/quan hệ, và 2) bộ suy luận câu trả lời.

5.5.1 LLMs như Bộ trích xuất Thực thể/quan hệ

Bộ trích xuất thực thể/quan hệ được thiết kế để xác định các thực thể và mối quan hệ được đề cập trong các câu hỏi ngôn ngữ tự nhiên và truy xuất các sự kiện liên quan trong KGs. Cho khả năng thành thạo trong hiểu ngôn ngữ, LLMs có thể được sử dụng hiệu quả cho mục đích này. Lukovnikov et al. [172] là người đầu tiên sử dụng LLMs như bộ phân loại cho dự đoán quan hệ, dẫn đến cải thiện hiệu suất đáng chú ý so với các mạng nơ-ron nông. Nan et al. [174] giới thiệu hai khung KGQA dựa trên LLM áp dụng LLMs để phát hiện các thực thể và quan hệ được đề cập. Sau đó, họ truy vấn câu trả lời trong KGs sử dụng các cặp thực thể-quan hệ được trích xuất. QA-GNN [131] sử dụng LLMs để mã hóa các cặp câu hỏi và câu trả lời ứng viên, được áp dụng để ước lượng tầm quan trọng của các thực thể KG tương đối. Các thực thể được truy xuất để tạo thành một đồ thị con, trong đó một suy luận câu trả lời được thực hiện bởi một mạng nơ-ron đồ thị. Luo et al. [173] sử dụng LLMs để tính toán sự tương tự giữa quan hệ và câu hỏi để truy xuất các sự kiện liên quan, được hình thức hóa như

s(r, q) =LLM(r)⊤LLM(q), (12)

trong đó q biểu thị câu hỏi, r biểu thị quan hệ, và LLM(·) sẽ tạo ra biểu diễn cho q và r, tương ứng. Hơn nữa, Zhang et al. [236] đề xuất một bộ truy xuất đường dẫn dựa trên LLM để truy xuất các quan hệ liên quan đến câu hỏi từng bước và xây dựng một số đường dẫn. Xác suất của mỗi đường dẫn có thể được tính như

P(p|q) =|p|∏t=1s(rt, q), (13)

trong đó p biểu thị đường dẫn, và rt biểu thị quan hệ tại bước nhảy thứ t của p. Các quan hệ và đường dẫn được truy xuất có thể được sử dụng như kiến thức ngữ cảnh để cải thiện hiệu suất của bộ suy luận câu trả lời như

P(a|q) =∑p∈PP(a|p)P(p|q), (14)

trong đó P biểu thị các đường dẫn được truy xuất và a biểu thị câu trả lời.

5.5.2 LLMs như Bộ suy luận Câu trả lời

Bộ suy luận câu trả lời được thiết kế để suy luận trên các sự kiện được truy xuất và tạo ra câu trả lời. LLMs có thể được sử dụng như bộ suy luận câu trả lời để tạo ra câu trả lời trực tiếp. Ví dụ, như được thể hiện trong Hình 22, DEKCOR [175] nối các sự kiện được truy xuất với câu hỏi và câu trả lời ứng viên như

x=[CLS] q[SEP] Related Facts [SEP] a[SEP] , (15)

trong đó a biểu thị câu trả lời ứng viên. Sau đó, nó đưa chúng vào LLMs để dự đoán điểm số câu trả lời. Sau khi sử dụng LLMs để tạo ra biểu diễn của x như ngữ cảnh QA, DRLK [176] đề xuất một Bộ Suy luận Phân cấp Động để nắm bắt các tương tác giữa ngữ cảnh QA và câu trả lời để dự đoán câu trả lời. Yan et al. [235] đề xuất một khung KGQA dựa trên LLM gồm hai giai đoạn: (1) truy xuất các sự kiện liên quan từ KGs và (2) tạo ra câu trả lời dựa trên các sự kiện được truy xuất. Giai đoạn đầu tiên tương tự như bộ trích xuất thực thể/quan hệ. Cho một thực thể câu trả lời ứng viên a, nó trích xuất một loạt đường dẫn p1, . . . , pn từ KGs. Nhưng giai đoạn thứ hai là một bộ suy luận câu trả lời dựa trên LLM. Nó trước tiên phát âm các đường dẫn bằng cách sử dụng tên thực thể và tên quan hệ trong KGs. Sau đó, nó nối câu hỏi q và tất cả đường dẫn p1, . . . , pn để tạo một mẫu đầu vào như

x=[CLS] q[SEP] p1[SEP] ···[SEP] pn[SEP] . (16)

Những đường dẫn này được coi như các sự kiện liên quan cho câu trả lời ứng viên a. Cuối cùng, nó sử dụng LLMs để dự đoán liệu giả thuyết: "a là câu trả lời của q" có được hỗ trợ bởi những sự kiện đó hay không, được hình thức hóa như

e[CLS] =LLM(x), (17)
s=σ(MLP(e[CLS])), (18)

trong đó nó mã hóa x sử dụng một LLM và đưa biểu diễn tương ứng với token [CLS] để phân loại nhị phân, và σ(·) biểu thị hàm sigmoid.

Để hướng dẫn LLMs suy luận tốt hơn thông qua KGs, OreoLM [177] đề xuất một Lớp Tương tác Kiến thức (KIL) được chèn giữa các lớp LLM. KIL tương tác với một mô-đun suy luận KG, trong đó nó khám phá các đường dẫn suy luận khác nhau, và sau đó mô-đun suy luận có thể suy luận trên các đường dẫn để tạo ra câu trả lời. GreaseLM [178] hợp nhất các biểu diễn từ LLMs và mạng nơ-ron đồ thị để suy luận hiệu quả trên các sự kiện KG và ngữ cảnh ngôn ngữ. UniKGQA [43] thống nhất việc truy xuất sự kiện và suy luận thành một khung thống nhất. UniKGQA bao gồm hai mô-đun. Mô-đun đầu tiên là một mô-đun khớp ngữ nghĩa sử dụng một LLM để khớp câu hỏi với các quan hệ tương ứng của chúng một cách ngữ nghĩa. Mô-đun thứ hai là một mô-đun truyền thông tin khớp, truyền thông tin khớp dọc theo các cạnh có hướng trên KGs để suy luận câu trả lời. Tương tự, ReLMKG [179] thực hiện suy luận chung trên một mô hình ngôn ngữ lớn và đồ thị tri thức liên quan. Câu hỏi và đường dẫn được phát âm được mã hóa bởi mô hình ngôn ngữ, và các lớp khác nhau của mô hình ngôn ngữ tạo ra các đầu ra hướng dẫn một mạng nơ-ron đồ thị để thực hiện truyền thông điệp. Quá trình này sử dụng kiến thức rõ ràng có trong đồ thị tri thức có cấu trúc cho mục đích suy luận. StructGPT [237] áp dụng một giao diện tùy chỉnh để cho phép các mô hình ngôn ngữ lớn (ví dụ: ChatGPT) suy luận trực tiếp trên KGs để thực hiện trả lời câu hỏi đa bước.

6 LLMs + KGs ĐƯỢC ĐỒNG BỘ HÓA

Sự đồng bộ của LLMs và KGs đã thu hút sự quan tâm ngày càng tăng những năm này, kết hợp các ưu điểm của LLMs và KGs để tăng cường lẫn nhau hiệu suất trong các ứng dụng downstream khác nhau. Ví dụ, LLMs có thể được sử dụng để hiểu ngôn ngữ tự nhiên, trong khi KGs được xử lý như một cơ sở tri thức, cung cấp kiến thức thực tế. Việc thống nhất LLMs và KGs có thể dẫn đến một mô hình mạnh mẽ cho biểu diễn kiến thức và suy luận.

Trong mục này, chúng tôi sẽ thảo luận về Synergized LLMs + KGs hiện đại từ hai góc độ: 1) Biểu diễn Kiến thức Đồng bộ, và 2) Suy luận Đồng bộ. Các công trình đại diện được tóm tắt trong Bảng 4.

6.1 Biểu diễn Kiến thức Đồng bộ

Kho dữ liệu văn bản và đồ thị tri thức đều chứa kiến thức khổng lồ. Tuy nhiên, kiến thức trong kho dữ liệu văn bản thường ngầm và không có cấu trúc, trong khi kiến thức trong KGs rõ ràng và có cấu trúc. Biểu diễn Kiến thức Đồng bộ nhằm thiết kế một mô hình đồng bộ có thể biểu diễn hiệu quả kiến thức từ cả LLMs và KGs. Mô hình đồng bộ có thể cung cấp hiểu biết tốt hơn về kiến thức từ cả hai nguồn, làm cho nó có giá trị cho nhiều nhiệm vụ downstream.

Để biểu diễn chung kiến thức, các nhà nghiên cứu đề xuất các mô hình đồng bộ bằng cách giới thiệu các mô-đun hợp nhất KG bổ sung, được huấn luyện chung với LLMs. Như được thể hiện trong Hình 23, ERNIE [35] đề xuất một kiến trúc bộ mã hóa kép văn bản-kiến thức trong đó một T-encoder trước tiên mã hóa các câu đầu vào, sau đó một K-encoder xử lý đồ thị tri thức được hợp nhất với biểu diễn văn bản từ T-encoder. BERT-MK [241] sử dụng kiến trúc bộ mã hóa kép tương tự nhưng nó giới thiệu thông tin bổ sung của các thực thể lân cận trong thành phần bộ mã hóa kiến thức trong quá trình tiền huấn luyện của LLMs. Tuy nhiên, một số thực thể lân cận trong KGs có thể không liên quan đến văn bản đầu vào, dẫn đến dư thừa và tiếng ồn bổ sung. CokeBERT [242] tập trung vào vấn đề này và đề xuất một mô-đun dựa trên GNN để lọc ra các thực thể KG không liên quan sử dụng văn bản đầu vào. JAKET [243] đề xuất hợp nhất thông tin thực thể ở giữa mô hình ngôn ngữ lớn.

KEPLER [40] trình bày một mô hình thống nhất cho nhúng kiến thức và biểu diễn ngôn ngữ tiền huấn luyện. Trong KEPLER, họ mã hóa mô tả thực thể văn bản bằng một LLM như nhúng của chúng, và sau đó tối ưu hóa chung các mục tiêu nhúng kiến thức và mô hình hóa ngôn ngữ. JointGT [42] đề xuất một mô hình học biểu diễn chung đồ thị-văn bản, đề xuất ba nhiệm vụ tiền huấn luyện để căn chỉnh biểu diễn của đồ thị và văn bản. DRAGON [44] trình bày một phương pháp tự giám sát để tiền huấn luyện một mô hình nền tảng ngôn ngữ-kiến thức chung từ văn bản và KG. Nó nhận các đoạn văn bản và đồ thị con KG liên quan như đầu vào và hợp nhất thông tin từ cả hai phương thức theo hai chiều. Sau đó, DRAGON sử dụng hai nhiệm vụ suy luận tự giám sát, tức là mô hình hóa ngôn ngữ bị che và dự đoán liên kết KG để tối ưu hóa các tham số mô hình. HKLM [238] giới thiệu một LLM thống nhất tích hợp KGs để học biểu diễn của kiến thức chuyên ngành.

6.2 Suy luận Đồng bộ

Để sử dụng tốt hơn kiến thức từ kho dữ liệu văn bản và suy luận đồ thị tri thức, Suy luận Đồng bộ nhằm thiết kế một mô hình đồng bộ có thể thực hiện suy luận hiệu quả với cả LLMs và KGs.

Suy luận Hợp nhất LLM-KG. Suy luận Hợp nhất LLM-KG tận dụng hai bộ mã hóa LLM và KG riêng biệt để xử lý đầu vào văn bản và KG liên quan [244]. Hai bộ mã hóa này đều quan trọng và hợp nhất chung kiến thức từ hai nguồn để suy luận. Để cải thiện tương tác giữa văn bản và kiến thức, KagNet [38] đề xuất trước tiên mã hóa KG đầu vào, và sau đó tăng cường biểu diễn văn bản đầu vào. Ngược lại, MHGRN [234] sử dụng các đầu ra LLM cuối cùng của văn bản đầu vào để hướng dẫn quá trình suy luận trên KGs. Tuy nhiên, cả hai chỉ thiết kế tương tác một chiều giữa văn bản và KGs. Để giải quyết vấn đề này, QA-GNN [131] đề xuất sử dụng một mô hình dựa trên GNN để suy luận chung trên ngữ cảnh đầu vào và thông tin KG thông qua truyền thông điệp. Cụ thể, QA-GNN biểu diễn thông tin văn bản đầu vào như một nút đặc biệt thông qua một phép toán gộp và kết nối nút này với các thực thể khác trong KG. Tuy nhiên, các đầu vào văn bản chỉ được gộp thành một vector dày đơn, hạn chế hiệu suất hợp nhất thông tin. JointLK [245] sau đó đề xuất một khung với tương tác tinh tế giữa bất kỳ token nào trong đầu vào văn bản và bất kỳ thực thể KG nào thông qua cơ chế attention hai chiều LM-to-KG và KG-to-LM. Như được thể hiện trong Hình 24, điểm số tích vô hướng theo cặp được tính toán trên tất cả token văn bản và thực thể KG, điểm số attention hai chiều được tính toán riêng biệt. Ngoài ra, tại mỗi lớp jointLK, KGs cũng được cắt tỉa động dựa trên điểm số attention để cho phép các lớp sau tập trung vào các cấu trúc KG con quan trọng hơn. Mặc dù hiệu quả, trong JointLK, quá trình hợp nhất giữa văn bản đầu vào và KG vẫn sử dụng các đầu ra LLM cuối cùng như biểu diễn văn bản đầu vào. GreaseLM [178] thiết kế tương tác sâu và phong phú giữa các token văn bản đầu vào và thực thể KG tại mỗi lớp của LLMs. Kiến trúc và phương pháp hợp nhất chủ yếu tương tự như ERNIE [35] được thảo luận trong Mục 6.1, ngoại trừ GreaseLM không sử dụng T-encoder chỉ văn bản để xử lý văn bản đầu vào.

LLMs như Tác nhân Suy luận. Thay vì sử dụng hai bộ mã hóa để hợp nhất kiến thức, LLMs cũng có thể được xử lý như các tác nhân để tương tác với KGs để thực hiện suy luận [246], như được minh họa trong Hình 25. KD-CoT [247] lặp đi lặp lại truy xuất sự kiện từ KGs và tạo ra các dấu vết suy luận trung thực, hướng dẫn LLMs tạo ra câu trả lời. KSL [239] dạy LLMs tìm kiếm trên KGs để truy xuất các sự kiện liên quan và sau đó tạo ra câu trả lời. StructGPT [237] thiết kế một số giao diện API để cho phép LLMs truy cập dữ liệu có cấu trúc và thực hiện suy luận bằng cách duyệt trên KGs. Think-on-graph [240] cung cấp một khung plug-and-play linh hoạt trong đó các tác nhân LLM lặp đi lặp lại thực hiện tìm kiếm chùm trên KGs để khám phá các đường dẫn suy luận và tạo ra câu trả lời. Để tăng cường khả năng tác nhân, AgentTuning [248] trình bày một số tập dữ liệu tinh chỉnh hướng dẫn để hướng dẫn các tác nhân LLM thực hiện suy luận trên KGs.

So sánh và Thảo luận. Suy luận Hợp nhất LLM-KG kết hợp bộ mã hóa LLM và bộ mã hóa KG để biểu diễn kiến thức theo cách thống nhất. Sau đó nó sử dụng một mô-đun suy luận đồng bộ để suy luận chung kết quả. Khung này cho phép các bộ mã hóa và mô-đun suy luận khác nhau, được huấn luyện đầu cuối đến cuối để sử dụng hiệu quả kiến thức và khả năng suy luận của LLMs và KGs. Tuy nhiên, các mô-đun bổ sung này có thể giới thiệu tham số và chi phí tính toán bổ sung trong khi thiếu khả năng diễn giải. LLMs như Tác nhân để suy luận KG cung cấp một khung linh hoạt để suy luận trên KGs mà không có chi phí huấn luyện bổ sung, có thể được tổng quát hóa cho các LLMs và KGs khác nhau. Trong khi đó, quá trình suy luận có thể diễn giải, có thể được sử dụng để giải thích kết quả. Tuy nhiên, việc định nghĩa các hành động và chính sách cho các tác nhân LLM cũng thách thức. Sự đồng bộ của LLMs và KGs vẫn là một chủ đề nghiên cứu đang diễn ra, với tiềm năng có các khung mạnh mẽ hơn trong tương lai.

7 HƯỚNG TƯƠNG LAI VÀ CỘT MỐC

Trong mục này, chúng tôi thảo luận về các hướng tương lai và một số cột mốc trong lĩnh vực nghiên cứu thống nhất KGs và LLMs.

7.1 KGs để Phát hiện Ảo giác trong LLMs

Vấn đề ảo giác trong LLMs, tạo ra nội dung sai sự kiện, làm cản trở đáng kể độ tin cậy của LLMs. Như đã thảo luận trong Mục 4, các nghiên cứu hiện tại cố gắng sử dụng KGs để có được LLMs đáng tin cậy hơn thông qua tiền huấn luyện hoặc suy luận được tăng cường bởi KG. Mặc dù có những nỗ lực, vấn đề ảo giác có thể tiếp tục tồn tại trong lĩnh vực LLMs trong tương lai gần. Do đó, để giành được sự tin tưởng của công chúng và các ứng dụng rộng hơn, việc phát hiện và đánh giá các trường hợp ảo giác trong LLMs và các hình thức khác của nội dung được tạo ra bởi AI (AIGC) là bắt buộc. Các phương pháp hiện tại cố gắng phát hiện ảo giác bằng cách huấn luyện một bộ phân loại nơ-ron trên một tập nhỏ tài liệu [249], không mạnh mẽ cũng không đủ mạnh để xử lý các LLMs ngày càng phát triển. Gần đây, các nhà nghiên cứu cố gắng sử dụng KGs như một nguồn bên ngoài để xác thực LLMs [250]. Các nghiên cứu sâu hơn kết hợp LLMs và KGs để đạt được một mô hình kiểm tra sự thật tổng quát có thể phát hiện ảo giác trên các lĩnh vực [251]. Do đó, nó mở ra một cánh cửa mới để sử dụng KGs để phát hiện ảo giác.

7.2 KGs để Chỉnh sửa Kiến thức trong LLMs

Mặc dù LLMs có khả năng lưu trữ kiến thức thế giới thực khổng lồ, chúng không thể nhanh chóng cập nhật kiến thức nội bộ của mình khi tình huống thế giới thực thay đổi. Có một số nỗ lực nghiên cứu được đề xuất để chỉnh sửa kiến thức trong LLMs [252] mà không cần huấn luyện lại toàn bộ LLMs. Tuy nhiên, các giải pháp như vậy vẫn gặp phải hiệu suất kém hoặc chi phí tính toán [253]. Các nghiên cứu hiện tại [254] cũng tiết lộ rằng chỉnh sửa một sự kiện đơn lẻ sẽ gây ra hiệu ứng gợn sóng cho kiến thức liên quan khác. Do đó, cần thiết phải phát triển một phương pháp hiệu quả và hiệu quả hơn để chỉnh sửa kiến thức trong LLMs. Gần đây, các nhà nghiên cứu cố gắng tận dụng KGs để chỉnh sửa kiến thức trong LLMs một cách hiệu quả.

7.3 KGs để Tiêm Kiến thức LLMs Hộp đen

Mặc dù tiền huấn luyện và chỉnh sửa kiến thức có thể cập nhật LLMs để bắt kịp với kiến thức mới nhất, chúng vẫn cần truy cập vào cấu trúc và tham số nội bộ của LLMs. Tuy nhiên, nhiều LLMs lớn hiện đại (ví dụ: ChatGPT) chỉ cung cấp API cho người dùng và nhà phát triển truy cập, làm cho chúng trở thành hộp đen đối với công chúng. Do đó, không thể tuân theo các phương pháp tiêm KG thông thường được mô tả [38], [244] thay đổi cấu trúc LLM bằng cách thêm các mô-đun hợp nhất kiến thức bổ sung. Việc chuyển đổi các loại kiến thức khác nhau thành các prompt văn bản khác nhau dường như là một giải pháp khả thi. Tuy nhiên, không rõ liệu các prompt này có thể tổng quát hóa tốt cho các LLMs mới hay không. Hơn nữa, phương pháp dựa trên prompt bị hạn chế bởi độ dài của token đầu vào của LLMs. Do đó, cách thực hiện tiêm kiến thức hiệu quả cho LLMs hộp đen vẫn là một câu hỏi mở để chúng ta khám phá [255], [256].

7.4 LLMs Đa phương thức cho KGs

Các đồ thị tri thức hiện tại thường dựa vào văn bản và cấu trúc đồ thị để xử lý các ứng dụng liên quan đến KG. Tuy nhiên, các đồ thị tri thức thế giới thực thường được xây dựng bởi dữ liệu từ các phương thức đa dạng [99], [257], [258]. Do đó, việc tận dụng hiệu quả các biểu diễn từ nhiều phương thức sẽ là một thách thức đáng kể cho nghiên cứu tương lai trong KGs [259]. Một giải pháp tiềm năng là phát triển các phương pháp có thể mã hóa và căn chỉnh chính xác các thực thể trên các phương thức khác nhau. Gần đây, với sự phát triển của LLMs đa phương thức [98], [260], việc tận dụng LLMs để căn chỉnh phương thức có triển vọng trong việc này. Nhưng, bắc cầu khoảng cách giữa LLMs đa phương thức và cấu trúc KG vẫn là một thách thức quan trọng trong lĩnh vực này, đòi hỏi điều tra và tiến bộ sâu hơn.

7.5 LLMs để Hiểu Cấu trúc KG

Các LLMs thông thường được huấn luyện trên dữ liệu văn bản thuần túy không được thiết kế để hiểu dữ liệu có cấu trúc như đồ thị tri thức. Vì vậy, LLMs có thể không nắm bắt đầy đủ hoặc hiểu thông tin được truyền đạt bởi cấu trúc KG. Một cách đơn giản là tuyến tính hóa dữ liệu có cấu trúc thành một câu mà LLMs có thể hiểu. Tuy nhiên, quy mô của KGs làm cho việc tuyến tính hóa toàn bộ KGs như đầu vào trở nên không thể. Hơn nữa, quá trình tuyến tính hóa có thể mất một số thông tin cơ bản trong KGs. Do đó, cần thiết phải phát triển LLMs có thể hiểu trực tiếp cấu trúc KG và suy luận trên đó [237].

7.6 LLMs và KGs Đồng bộ cho Suy luận Hai chiều

KGs và LLMs là hai công nghệ bổ sung có thể đồng bộ hóa lẫn nhau. Tuy nhiên, sự đồng bộ của LLMs và KGs ít được khám phá bởi các nhà nghiên cứu hiện tại. Một sự đồng bộ mong muốn của LLMs và KGs sẽ bao gồm việc tận dụng điểm mạnh của cả hai công nghệ để vượt qua các hạn chế cá nhân của chúng. LLMs, như ChatGPT, xuất sắc trong việc tạo ra văn bản giống con người và hiểu ngôn ngữ tự nhiên, trong khi KGs là cơ sở dữ liệu có cấu trúc nắm bắt và biểu diễn kiến thức theo cách có cấu trúc. Bằng cách kết hợp khả năng của chúng, chúng ta có thể tạo ra một hệ thống mạnh mẽ hưởng lợi từ hiểu biết ngữ cảnh của LLMs và biểu diễn kiến thức có cấu trúc của KGs. Để thống nhất tốt hơn LLMs và KGs, nhiều kỹ thuật tiên tiến cần được tích hợp, như học đa phương thức [261], mạng nơ-ron đồ thị [262], và học liên tục [263]. Cuối cùng, sự đồng bộ của LLMs và KGs có thể được áp dụng cho nhiều ứng dụng thế giới thực, như công cụ tìm kiếm [100], hệ thống đề xuất [10], [89], và khám phá thuốc.

Với một vấn đề ứng dụng cho trước, chúng ta có thể áp dụng một KG để thực hiện tìm kiếm dựa trên kiến thức cho các mục tiêu tiềm năng và dữ liệu chưa thấy, và đồng thời bắt đầu với LLMs để thực hiện suy luận dựa trên dữ liệu/văn bản để xem những mục dữ liệu/mục tiêu mới nào có thể được dẫn xuất. Khi tìm kiếm dựa trên kiến thức được kết hợp với suy luận dựa trên dữ liệu/văn bản, chúng có thể xác thực lẫn nhau, dẫn đến các giải pháp hiệu quả và hiệu quả được thúc đẩy bởi bánh xe kép. Do đó, chúng ta có thể dự đoán sự quan tâm ngày càng tăng để mở khóa tiềm năng của việc tích hợp KGs và LLMs cho các ứng dụng downstream đa dạng với cả khả năng tạo sinh và suy luận trong tương lai gần.

8 KẾT LUẬN

Thống nhất các mô hình ngôn ngữ lớn (LLMs) và đồ thị tri thức (KGs) là một hướng nghiên cứu tích cực đã thu hút sự quan tâm ngày càng tăng từ cả học thuật và công nghiệp. Trong bài báo này, chúng tôi cung cấp một tổng quan kỹ lưỡng về nghiên cứu gần đây trong lĩnh vực này. Chúng tôi trước tiên giới thiệu các cách khác nhau để tích hợp KGs để tăng cường LLMs. Sau đó, chúng tôi giới thiệu các phương pháp hiện có áp dụng LLMs cho KGs và thiết lập phân loại dựa trên nhiều nhiệm vụ KG khác nhau. Cuối cùng, chúng tôi thảo luận về các thách thức và hướng tương lai trong lĩnh vực này. Chúng tôi dự đoán rằng sẽ có nhiều giai đoạn (cột mốc) trong lộ trình thống nhất KGs và LLMs, như được thể hiện trong Hình 26. Cụ thể, chúng tôi sẽ dự đoán nghiên cứu ngày càng tăng về ba giai đoạn: Giai đoạn 1: LLMs được tăng cường bởi KG, KGs được bổ sung bởi LLM, Giai đoạn 2: LLMs + KGs được đồng bộ hóa, và Giai đoạn 3: Hiểu Cấu trúc Đồ thị, Đa phương thức, Cập nhật Kiến thức. Chúng tôi hy vọng rằng bài báo này sẽ cung cấp một hướng dẫn để thúc đẩy nghiên cứu tương lai.

LỜI CẢM ƠN

Nghiên cứu này được hỗ trợ bởi Hội đồng Nghiên cứu Australia (ARC) theo các khoản tài trợ FT210100097 và DP240101547 và Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) theo khoản tài trợ 62120106008.

TÀI LIỆU THAM KHẢO

[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[2] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.
[3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485–5551, 2020.
[4] D. Su, Y. Xu, G. I. Winata, P. Xu, H. Kim, Z. Liu, and P. Fung, "Generalizing question answering system with pre-trained language model fine-tuning," in Proceedings of the 2nd Workshop on Machine Reading for Question Answering, 2019, pp. 203–211.
[5] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in ACL, 2020, pp. 7871–7880.
[6] J. Li, T. Tang, W. X. Zhao, and J.-R. Wen, "Pretrained language models for text generation: A survey," arXiv preprint arXiv:2105.10311, 2021.
[7] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., "Emergent abilities of large language models," Transactions on Machine Learning Research.
[8] K. Malinka, M. Perešíni, A. Firc, O. Hujňák, and F. Januš, "On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?" arXiv preprint arXiv:2303.11146, 2023.
[9] Z. Li, C. Wang, Z. Liu, H. Wang, S. Wang, and C. Gao, "Cctest: Testing and repairing code completion systems," ICSE, 2023.
[10] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, "Is chatgpt a good recommender? a preliminary study," arXiv preprint arXiv:2304.10149, 2023.

PHỤ LỤC A
ƯU VÀ NHƯỢC ĐIỂM CỦA LLMs VÀ KGS

Trong mục này, chúng tôi giới thiệu chi tiết các ưu và nhược điểm của LLMs và KGs. Chúng tôi tóm tắt các ưu và nhược điểm của LLMs và KGs trong Hình 1, tương ứng.

Ưu điểm LLM.
•Kiến thức Tổng quát [11]: LLMs được tiền huấn luyện trên kho dữ liệu quy mô lớn, chứa một lượng lớn kiến thức tổng quát, như kiến thức thường thức [264] và kiến thức thực tế [14]. Kiến thức như vậy có thể được chưng cất từ LLMs và sử dụng cho các nhiệm vụ downstream [265].
•Xử lý Ngôn ngữ [12]: LLMs đã cho thấy hiệu suất tuyệt vời trong việc hiểu ngôn ngữ tự nhiên [266]. Do đó, LLMs có thể được sử dụng trong nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên, như trả lời câu hỏi [4], dịch máy [5], và tạo văn bản [6].
•Khả năng Tổng quát hóa [13]: LLMs cho phép khả năng tổng quát hóa tuyệt vời, có thể được áp dụng cho các nhiệm vụ downstream khác nhau [267]. Bằng cách cung cấp các ví dụ few-shot [59] hoặc tinh chỉnh trên dữ liệu đa nhiệm vụ [3], LLMs đạt được hiệu suất tuyệt vời trên nhiều nhiệm vụ.

Nhược điểm LLM.
•Kiến thức Ngầm [14]: LLMs biểu diễn kiến thức một cách ngầm trong các tham số của chúng. Rất khó để diễn giải hoặc xác thực kiến thức thu được bởi LLMs.
•Ảo giác [15]: LLMs thường trải qua ảo giác bằng cách tạo ra nội dung có vẻ hợp lý nhưng sai sự kiện [268]. Vấn đề này làm giảm đáng kể độ tin cậy của LLMs trong các tình huống thế giới thực.
•Thiếu quyết đoán [16]: LLMs thực hiện suy luận bằng cách tạo từ một mô hình xác suất, đây là một quá trình thiếu quyết đoán. Kết quả được tạo ra được lấy mẫu từ phân phối xác suất, khó kiểm soát.
•Hộp đen [17]: LLMs bị chỉ trích vì thiếu khả năng diễn giải. Không rõ để biết các mẫu và chức năng cụ thể mà LLMs sử dụng để đưa ra dự đoán hoặc quyết định.
•Thiếu Kiến thức Chuyên ngành/Mới [18]: LLMs được huấn luyện trên kho dữ liệu tổng quát có thể không thể tổng quát hóa tốt cho các lĩnh vực cụ thể hoặc kiến thức mới do thiếu kiến thức chuyên ngành hoặc dữ liệu huấn luyện mới.

Ưu điểm KG.
•Kiến thức Có cấu trúc [19]: KGs lưu trữ sự kiện theo định dạng có cấu trúc (tức là các bộ ba), có thể được hiểu bởi cả con người và máy móc.
•Độ chính xác [20]: Sự kiện trong KGs thường được chọn lọc thủ công hoặc xác thực bởi các chuyên gia, chính xác và đáng tin cậy hơn so với những sự kiện trong LLMs.
•Tính quyết đoán [21]: Kiến thức thực tế trong KGs được lưu trữ theo cách quyết đoán. Thuật toán suy luận trong KGs cũng quyết định, có thể cung cấp kết quả quyết đoán.
•Khả năng diễn giải [22]: KGs được biết đến với khả năng suy luận tượng trưng, cung cấp một quá trình suy luận có thể diễn giải được hiểu bởi con người.
•Kiến thức Chuyên ngành [23]: Nhiều lĩnh vực có thể xây dựng KGs của họ bởi các chuyên gia để cung cấp kiến thức chuyên ngành chính xác và đáng tin cậy.
•Kiến thức Phát triển [24]: Sự kiện trong KGs liên tục phát triển. KGs có thể được cập nhật với sự kiện mới bằng cách chèn các bộ ba mới và xóa những bộ ba lỗi thời.

Nhược điểm KG.
•Không hoàn chỉnh [25]: KGs khó xây dựng và thường không hoàn chỉnh, hạn chế khả năng của KGs trong việc cung cấp kiến thức toàn diện.
•Thiếu Hiểu biết Ngôn ngữ [33]: Hầu hết các nghiên cứu về KGs mô hình hóa cấu trúc của kiến thức, nhưng bỏ qua thông tin văn bản trong KGs. Thông tin văn bản trong KGs thường bị bỏ qua trong các nhiệm vụ liên quan đến KG, như hoàn thành KG [26] và KGQA [43].
•Sự kiện Chưa thấy [27]: KGs thay đổi động, khiến việc mô hình hóa các thực thể chưa thấy và biểu diễn sự kiện mới trở nên khó khăn.
