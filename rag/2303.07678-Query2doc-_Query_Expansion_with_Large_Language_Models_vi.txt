# 2303.07678.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2303.07678.pdf
# Kích thước tệp: 403496 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Query2doc: Mở rộng truy vấn với các mô hình ngôn ngữ lớn
Liang Wang và Nan Yang và Furu Wei
Microsoft Research
{wangliang,nanya,fuwei}@microsoft.com
Tóm tắt
Bài báo này giới thiệu một phương pháp mở rộng truy vấn đơn giản nhưng hiệu quả, được ký hiệu là query2doc, để cải thiện cả hệ thống truy xuất thưa thớt và dày đặc. Phương pháp được đề xuất đầu tiên tạo ra các tài liệu giả bằng cách gợi ý mô hình ngôn ngữ lớn (LLM) với few-shot, sau đó mở rộng truy vấn với các tài liệu giả được tạo ra. LLM được huấn luyện trên kho dữ liệu văn bản quy mô web và rất giỏi trong việc ghi nhớ kiến thức. Các tài liệu giả từ LLM thường chứa thông tin có liên quan cao có thể hỗ trợ trong việc làm rõ nghĩa truy vấn và hướng dẫn các bộ truy xuất. Kết quả thực nghiệm cho thấy query2doc nâng cao hiệu suất của BM25 từ 3% đến 15% trên các bộ dữ liệu IR ad-hoc, như MS-MARCO và TREC DL, mà không cần tinh chỉnh mô hình nào. Hơn nữa, phương pháp của chúng tôi cũng có lợi cho các bộ truy xuất dày đặc tiên tiến về cả kết quả trong miền và ngoài miền.

1 Giới thiệu
Truy xuất thông tin (IR) nhằm mục đích định vị các tài liệu có liên quan từ một kho dữ liệu lớn được đưa ra một truy vấn của người dùng. Đây là một thành phần cốt lõi trong các công cụ tìm kiếm hiện đại và các nhà nghiên cứu đã đầu tư hàng thập kỷ trong lĩnh vực này. Có hai mô hình chính cho IR: truy xuất thưa thớt dựa trên từ vựng, như BM25, và truy xuất dày đặc dựa trên embedding (Xiong et al., 2021; Qu et al., 2021). Mặc dù các bộ truy xuất dày đặc hoạt động tốt hơn khi có sẵn lượng lớn dữ liệu được gán nhãn (Karpukhin et al., 2020), BM25 vẫn cạnh tranh trên các bộ dữ liệu ngoài miền (Thakur et al., 2021).

Mở rộng truy vấn (Rocchio, 1971; Lavrenko và Croft, 2001) là một kỹ thuật lâu đời viết lại truy vấn dựa trên phản hồi giả có liên quan hoặc các nguồn kiến thức bên ngoài như WordNet. Đối với truy xuất thưa thớt, nó có thể giúp thu hẹp khoảng cách từ vựng giữa truy vấn và các tài liệu. Tuy nhiên, các phương pháp mở rộng truy vấn như RM3 (Lavrenko và Croft, 2001; Lv và Zhai, 2009) chỉ cho thấy thành công hạn chế trên các bộ dữ liệu phổ biến (Campos et al., 2016), và hầu hết các bộ truy xuất dày đặc tiên tiến không áp dụng kỹ thuật này. Trong khi đó, các phương pháp mở rộng tài liệu như doc2query (Nogueira et al., 2019) đã được chứng minh là hiệu quả cho truy xuất thưa thớt.

Trong bài báo này, chúng tôi chứng minh hiệu quả của LLM (Brown et al., 2020) như các mô hình mở rộng truy vấn bằng cách tạo ra các tài liệu giả có điều kiện trên các gợi ý few-shot. Cho rằng các truy vấn tìm kiếm thường ngắn, mơ hồ, hoặc thiếu thông tin nền cần thiết, LLM có thể cung cấp thông tin có liên quan để hướng dẫn các hệ thống truy xuất, vì chúng ghi nhớ một lượng kiến thức và mẫu ngôn ngữ khổng lồ bằng cách huấn luyện trước trên hàng nghìn tỷ token.

Phương pháp được đề xuất của chúng tôi, được gọi là query2doc, tạo ra các tài liệu giả bằng cách gợi ý LLM với few-shot và nối chúng với truy vấn gốc để tạo thành một truy vấn mới. Phương pháp này đơn giản để triển khai và không yêu cầu thay đổi gì trong các pipeline huấn luyện hoặc kiến trúc mô hình, làm cho nó trực giao với tiến bộ trong lĩnh vực LLM và truy xuất thông tin. Các phương pháp tương lai có thể dễ dàng xây dựng dựa trên khung mở rộng truy vấn của chúng tôi.

Để đánh giá trong miền, chúng tôi áp dụng xếp hạng đoạn văn MS-MARCO (Campos et al., 2016), bộ dữ liệu TREC DL 2019 và 2020. Các tài liệu giả được tạo ra bằng cách gợi ý một phiên bản cải tiến của GPT-3 text-davinci-003 từ OpenAI (Brown et al., 2020). Kết quả cho thấy query2doc cải thiện đáng kể thuật toán BM25 có sẵn mà không cần tinh chỉnh mô hình nào, đặc biệt cho các truy vấn khó từ track TREC DL. Các bộ truy xuất dày đặc mạnh, bao gồm DPR (Karpukhin et al., 2020), SimLM (Wang et al., 2023), và E5 (Wang et al., 2022) cũng có lợi từ query2doc, mặc dù lợi ích có xu hướng giảm khi chưng cất từ một bộ xếp hạng lại mạnh dựa trên cross-encoder. Thực nghiệm trong cài đặt OOD zero-shot cho thấy phương pháp của chúng tôi vượt trội hơn các baseline mạnh trên hầu hết các bộ dữ liệu.

--- TRANG 2 ---
bộ dữ liệu. Phân tích sâu hơn cũng tiết lộ tầm quan trọng của quy mô mô hình: query2doc hoạt động tốt nhất khi kết hợp với các LLM có khả năng nhất trong khi các mô hình ngôn ngữ nhỏ chỉ cung cấp cải thiện nhỏ so với baseline. Để hỗ trợ tái tạo, chúng tôi phát hành tất cả các thế hệ từ text-davinci-003 tại https://huggingface.co/datasets/intfloat/query2doc_msmarco.

2 Phương pháp

Viết một đoạn văn trả lời truy vấn đã cho:
Truy vấn: what state is this zip code 85282
Đoạn văn: Chào mừng đến với TEMPE, AZ 85282. 
85282 là một mã zip nông thôn ở Tempe, Arizona. 
Dân số chủ yếu là người da trắng...
...
Truy vấn: when was pokemon green released
Đoạn văn: Gợi ý LLM
Pokemon Green được phát hành tại Nhật Bản vào ngày 27 tháng 2 năm 1996. Đây là trò chơi đầu tiên trong series Pokemon và đóng vai trò là cơ sở cho Pokemon Red và Blue, được phát hành tại Mỹ năm 1998. Pokemon Green gốc vẫn là một tác phẩm kinh điển được yêu thích trong số các fan của series. Đầu ra LLM

Hình 1: Minh họa gợi ý few-shot query2doc. Chúng tôi bỏ qua một số ví dụ trong ngữ cảnh vì lý do về không gian.

Cho một truy vấn q, chúng tôi sử dụng gợi ý few-shot để tạo ra một tài liệu giả d′ như được mô tả trong Hình 1. Gợi ý bao gồm một hướng dẫn ngắn "Viết một đoạn văn trả lời truy vấn đã cho:" và k cặp được gán nhãn được lấy mẫu ngẫu nhiên từ một tập huấn luyện. Chúng tôi sử dụng k = 4 xuyên suốt bài báo này. Sau đó, chúng tôi viết lại q thành một truy vấn mới q+ bằng cách nối với tài liệu giả d′. Có những khác biệt nhỏ trong thao tác nối cho các bộ truy xuất thưa thớt và dày đặc, mà chúng tôi trình bày chi tiết trong phần sau.

Truy xuất thưa thớt Vì truy vấn q thường ngắn hơn nhiều so với các tài liệu giả, để cân bằng trọng số tương đối của truy vấn và tài liệu giả, chúng tôi tăng cường trọng số thuật ngữ truy vấn bằng cách lặp lại truy vấn n lần trước khi nối với tài liệu giả d′:

q+ = concat({q} ×n, d′) (1)

Ở đây, "concat" biểu thị hàm nối chuỗi. q+ được sử dụng làm truy vấn mới cho truy xuất BM25. Chúng tôi thấy rằng n = 5 là một giá trị tốt chung và không điều chỉnh nó trên cơ sở bộ dữ liệu.

Truy xuất dày đặc Truy vấn mới q+ là một phép nối đơn giản của truy vấn gốc q và tài liệu giả d′ được phân tách bằng [SEP]:

q+ = concat(q, [SEP], d′) (2)

Để huấn luyện các bộ truy xuất dày đặc, một số yếu tố có thể ảnh hưởng đến hiệu suất cuối cùng, như khai thác negative khó (Xiong et al., 2021), huấn luyện trước trung gian (Gao và Callan, 2021), và chưng cất kiến thức từ một bộ xếp hạng lại dựa trên cross-encoder (Qu et al., 2021). Trong bài báo này, chúng tôi điều tra hai cài đặt để hiểu toàn diện hơn về phương pháp của chúng tôi. Cài đặt đầu tiên là huấn luyện các mô hình DPR (Karpukhin et al., 2020) được khởi tạo từ BERT base với chỉ các negative khó BM25. Mục tiêu tối ưu hóa là một loss đối chiếu tiêu chuẩn:

Lcont = −log(e^(hq·hd))/(e^(hq·hd) + Σ(di∈N) e^(hq·hdi)) (3)

trong đó hq và hd đại diện cho các embedding cho truy vấn và tài liệu, tương ứng. N biểu thị tập hợp các negative khó.

Cài đặt thứ hai là xây dựng dựa trên các bộ truy xuất dày đặc tiên tiến và sử dụng phân kỳ KL để chưng cất từ một mô hình teacher cross-encoder.

min DKL(pce, pstu) + αLcont (4)

pce và pstu là các xác suất từ cross-encoder và mô hình student của chúng tôi, tương ứng. α là một hệ số để cân bằng loss chưng cất và loss đối chiếu.

So sánh với Phản hồi giả có liên quan
Phương pháp được đề xuất của chúng tôi có liên quan đến phương pháp cổ điển của phản hồi giả có liên quan (PRF) (Lavrenko và Croft, 2001; Lv và Zhai, 2009). Trong PRF truyền thống, các tín hiệu phản hồi cho mở rộng truy vấn đến từ các tài liệu top-k thu được trong bước truy xuất ban đầu, trong khi phương pháp của chúng tôi gợi ý LLM để tạo ra các tài liệu giả. Phương pháp của chúng tôi không dựa vào chất lượng của kết quả truy xuất ban đầu, thường ồn ào hoặc không liên quan. Thay vào đó, nó khai thác các LLM tiên tiến để tạo ra các tài liệu có khả năng chứa các thuật ngữ có liên quan hơn.

--- TRANG 3 ---
[Bảng 1 với kết quả chính về xếp hạng đoạn văn MS-MARCO và bộ dữ liệu TREC được giữ nguyên vì đây là dữ liệu kỹ thuật]

3 Thực nghiệm

3.1 Thiết lập

Bộ dữ liệu đánh giá Để đánh giá trong miền, chúng tôi sử dụng xếp hạng đoạn văn MS-MARCO (Campos et al., 2016), bộ dữ liệu TREC DL 2019 (Craswell et al., 2020a) và 2020 (Craswell et al., 2020b). Để đánh giá zero-shot ngoài miền, chúng tôi chọn năm bộ dữ liệu tài nguyên thấp từ benchmark BEIR (Thakur et al., 2021). Các metric đánh giá bao gồm MRR@10, R@k (k ∈ {50,1k}), và nDCG@10.

Siêu tham số Đối với truy xuất thưa thớt bao gồm BM25 và RM3, chúng tôi áp dụng triển khai mặc định từ Pyserini (Lin et al., 2021). Khi huấn luyện các bộ truy xuất dày đặc, chúng tôi sử dụng hầu hết các siêu tham số giống như SimLM (Wang et al., 2023), ngoại trừ việc tăng độ dài truy vấn tối đa lên 144 để bao gồm các tài liệu giả. Khi gợi ý LLM, chúng tôi bao gồm 4 ví dụ trong ngữ cảnh và sử dụng nhiệt độ mặc định là 1 để lấy mẫu tối đa 128 token. Để biết thêm chi tiết, vui lòng tham khảo Phụ lục A.

3.2 Kết quả chính

Trong Bảng 1, chúng tôi liệt kê kết quả trên xếp hạng đoạn văn MS-MARCO và bộ dữ liệu TREC DL. Đối với truy xuất thưa thớt, "BM25 + query2doc" đánh bại baseline BM25 với cải thiện hơn 15% trên bộ dữ liệu TREC DL 2019 và 2020. Kiểm tra thủ công của chúng tôi tiết lộ rằng hầu hết các truy vấn từ track TREC DL là các truy vấn tập trung vào thực thể có đuôi dài, được hưởng lợi nhiều hơn từ khớp từ vựng chính xác. Phương pháp mở rộng truy vấn truyền thống RM3 chỉ cải thiện một cách biên tế metric R@1k. Mặc dù phương pháp mở rộng tài liệu docT5query đạt được con số tốt hơn trên tập dev MS-MARCO, nó yêu cầu huấn luyện một bộ tạo truy vấn dựa trên T5 với tất cả dữ liệu được gán nhãn có sẵn, trong khi "BM25 + query2doc" không yêu cầu tinh chỉnh mô hình nào.

Đối với truy xuất dày đặc, các biến thể mô hình kết hợp với query2doc cũng vượt trội hơn các baseline tương ứng trên tất cả các metric. Tuy nhiên, lợi ích mang lại bởi query2doc có xu hướng giảm khi sử dụng huấn luyện trước trung gian hoặc chưng cất kiến thức từ các bộ xếp hạng lại cross-encoder, như được thể hiện bởi kết quả "SimLM + query2doc" và "E5 + query2doc".

Đối với truy xuất zero-shot ngoài miền, kết quả là hỗn hợp như được thể hiện trong Bảng 2. Các bộ dữ liệu tập trung vào thực thể như DBpedia thấy cải thiện lớn nhất. Trên các bộ dữ liệu NFCorpus và Scifact, chúng tôi quan sát sự giảm nhỏ về chất lượng xếp hạng. Điều này có thể do sự không khớp phân phối giữa huấn luyện và đánh giá.

4 Phân tích

Mở rộng quy mô LLM là quan trọng Đối với phương pháp được đề xuất của chúng tôi, một câu hỏi tự nhiên phát sinh là: quy mô mô hình ảnh hưởng như thế nào đến chất lượng mở rộng truy vấn? Bảng 3 cho thấy hiệu suất liên tục cải thiện khi chúng ta chuyển từ mô hình 1.3B

--- TRANG 4 ---
[Bảng 2 và 3 với dữ liệu kỹ thuật được giữ nguyên]

đến các mô hình 175B. Theo kinh nghiệm, các văn bản được tạo bởi các mô hình ngôn ngữ nhỏ hơn có xu hướng ngắn hơn và chứa nhiều lỗi thực tế hơn. Ngoài ra, mô hình "davinci-003" vượt trội hơn phiên bản trước đó "davinci-001" bằng cách sử dụng dữ liệu huấn luyện tốt hơn và cải thiện instruction tuning. GPT-4 được phát hành gần đây (OpenAI, 2023) đạt kết quả tốt nhất.

Lợi ích hiệu suất nhất quán qua các quy mô dữ liệu Hình 2 trình bày so sánh giữa hai biến thể của các mô hình DPR, khác nhau về lượng dữ liệu được gán nhãn sử dụng. Kết quả cho thấy biến thể "DPR + query2doc" luôn vượt trội hơn baseline DPR khoảng 1%, bất kể lượng dữ liệu được sử dụng để tinh chỉnh. Quan sát này làm nổi bật rằng đóng góp của chúng tôi trực giao với việc mở rộng liên tục các tín hiệu giám sát.

Cách sử dụng tài liệu giả Trong bài báo này, chúng tôi nối truy vấn gốc và các tài liệu giả làm truy vấn mới. Thay vào đó, người ta có thể chỉ sử dụng các tài liệu giả, như được thực hiện trong phương pháp HyDE (Gao et al., 2022). Kết quả được trình bày trong Bảng 4 chứng minh rằng truy vấn gốc và các tài liệu giả bổ sung cho nhau, và sự kết hợp của chúng dẫn đến hiệu suất tốt hơn đáng kể trong truy xuất thưa thớt.

Phân tích trường hợp Trong Bảng 5, chúng tôi cho thấy hai truy vấn cùng với các tài liệu giả tương ứng và sự thật cơ bản. Các tài liệu giả, được tạo ra bởi LLM, cung cấp thông tin chi tiết và hầu hết chính xác, do đó giảm sự không khớp từ vựng giữa truy vấn và tài liệu. Trong một số trường hợp, các tài liệu giả đủ để đáp ứng nhu cầu thông tin của người dùng, làm cho bước truy xuất trở nên không cần thiết. Tuy nhiên, đáng chú ý là các thế hệ LLM có thể chứa lỗi thực tế. Ví dụ, trong truy vấn thứ hai, bài hát chủ đề "It's a Jungle Out There" được sử dụng từ mùa hai năm 2003, không phải 2002. Mặc dù những lỗi như vậy có thể xuất hiện tinh tế và khó xác minh, chúng đặt ra một thách thức đáng kể cho việc xây dựng các hệ thống đáng tin cậy sử dụng LLM.

--- TRANG 5 ---
[Bảng 5 với các ví dụ chi tiết được giữ nguyên vì chứa thông tin kỹ thuật cụ thể]

5 Công trình liên quan

Mở rộng truy vấn và Mở rộng tài liệu là hai kỹ thuật cổ điển để cải thiện chất lượng truy xuất, đặc biệt cho các hệ thống truy xuất thưa thớt. Cả hai kỹ thuật đều nhằm mục đích giảm thiểu khoảng cách từ vựng giữa truy vấn và các tài liệu. Mở rộng truy vấn thường liên quan đến việc viết lại truy vấn dựa trên phản hồi mức độ liên quan (Lavrenko và Croft, 2001; Rocchio, 1971) hoặc các tài nguyên từ vựng như WordNet (Miller, 1992). Trong các trường hợp không có nhãn, các tài liệu top-k được truy xuất có thể phục vụ như các tín hiệu phản hồi giả có liên quan (Lv và Zhai, 2009). Liu et al. tinh chỉnh một mô hình encoder-decoder để tạo ra các manh mối ngữ cảnh.

Ngược lại, mở rộng tài liệu làm phong phú biểu diễn tài liệu bằng cách thêm các thuật ngữ liên quan bổ sung. Doc2query (Nogueira et al., 2019) huấn luyện một mô hình seq2seq để dự đoán các truy vấn giả dựa trên tài liệu và sau đó thêm các truy vấn giả được tạo ra vào chỉ mục tài liệu. Các mô hình truy xuất thưa thớt được học như SPLADE (Formal et al., 2021) và uniCOIL (Lin và Ma, 2021) cũng học trọng số thuật ngữ tài liệu theo cách end-to-end. Tuy nhiên, hầu hết các bộ truy xuất dày đặc tiên tiến (Ren et al., 2021; Wang et al., 2023) không áp dụng bất kỳ kỹ thuật mở rộng nào. Bài báo của chúng tôi chứng minh rằng các bộ truy xuất dày đặc mạnh cũng có lợi từ mở rộng truy vấn sử dụng LLM.

Các mô hình ngôn ngữ lớn (LLM) như GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), và LLaMA (Touvron et al., 2023) được huấn luyện trên hàng nghìn tỷ token với hàng tỷ tham số, thể hiện khả năng tổng quát hóa chưa từng có qua các nhiệm vụ khác nhau. LLM có thể tuân theo hướng dẫn theo cách zero-shot hoặc tiến hành học trong ngữ cảnh thông qua gợi ý few-shot. Gán nhãn một số ví dụ chất lượng cao chỉ yêu cầu nỗ lực tối thiểu của con người. Trong bài báo này, chúng tôi sử dụng gợi ý few-shot để tạo ra các tài liệu giả từ một truy vấn đã cho. Một công trình gần đây có liên quan chặt chẽ HyDE (Gao et al., 2022) thay vào đó tập trung vào cài đặt zero-shot và sử dụng embedding của các tài liệu giả để tìm kiếm tương tự. HyDE ngầm giả định rằng tài liệu sự thật cơ bản và các tài liệu giả thể hiện cùng một ngữ nghĩa bằng các từ khác nhau, điều này có thể không đúng cho một số truy vấn. Trong lĩnh vực trả lời câu hỏi, RECITE (Sun et al., 2022) và GENREAD (Yu et al., 2022) chứng minh rằng LLM là các bộ tạo ngữ cảnh mạnh mẽ và có thể mã hóa kiến thức thực tế phong phú. Tuy nhiên, như phân tích của chúng tôi cho thấy, LLM đôi khi có thể tạo ra các tuyên bố sai, cản trở ứng dụng thực tế của chúng trong các lĩnh vực quan trọng.

6 Kết luận

Bài báo này trình bày một phương pháp đơn giản query2doc để tận dụng LLM cho mở rộng truy vấn. Nó đầu tiên gợi ý LLM với các ví dụ few-shot để tạo ra các tài liệu giả và sau đó tích hợp với các bộ truy xuất thưa thớt hoặc dày đặc hiện có bằng cách tăng cường truy vấn với các tài liệu giả được tạo ra. Động lực cơ bản là chưng cất các LLM thông qua gợi ý. Mặc dù đơn giản, các đánh giá thực nghiệm chứng minh cải thiện nhất quán qua các mô hình và bộ dữ liệu truy xuất khác nhau.

--- TRANG 6 ---
Hạn chế

[Bảng 6 với dữ liệu độ trễ được giữ nguyên]

Một hạn chế rõ ràng là hiệu quả của truy xuất. Phương pháp của chúng tôi yêu cầu chạy suy luận với LLM có thể chậm hơn đáng kể do việc giải mã tự hồi quy token-by-token. Hơn nữa, với query2doc, tìm kiếm chỉ mục đảo ngược cũng trở nên chậm hơn vì số lượng thuật ngữ truy vấn tăng sau khi mở rộng. Điều này được hỗ trợ bởi kết quả benchmarking trong Bảng 6. Triển khai thực tế phương pháp của chúng tôi nên xem xét các yếu tố này.

[Phần Tài liệu tham khảo và Phụ lục A, B, C được giữ nguyên vì chứa thông tin kỹ thuật chi tiết]
