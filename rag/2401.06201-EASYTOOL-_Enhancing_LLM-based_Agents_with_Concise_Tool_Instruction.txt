# 2401.06201.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2401.06201.pdf
# File size: 1328789 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction
Siyu Yuan1*, Kaitao Song2*†,
Jiangjie Chen1, Xu Tan2, Yongliang Shen3, Kan Ren2, Dongsheng Li2, Deqing Yang1†
1Fudan University,2Microsoft Research Asia,3Zhejiang University
syyuan21@m.fudan.edu.cn ,{kaitaosong, xuta, dongsli}@microsoft.com
syl@zju.edu.cn ,{jjchen19,yangdeqing}@fudan.edu.cn
Abstract
There has been a rising interest in utilizing
tools in applications of autonomous agents
based on large language models (LLMs) to ad-
dress intricate real-world tasks. To develop
LLM-based agents, it usually requires LLMs
to understand many tool functions from dif-
ferent tool documentations. However, these
documentations could be diverse, redundant,
or incomplete, which immensely affects the
capability of LLMs in using tools. To solve
this, we introduce EASYTOOL, a framework
transforming diverse and lengthy tool docu-
mentation into a unified and concise tool in-
struction for easier tool usage. EASYTOOL
purifies essential information from extensive
tool documentation of different sources, and
elaborates a unified interface ( i.e., tool instruc-
tion) to offer standardized tool descriptions and
functionalities for LLM-based agents. Exten-
sive experiments on multiple different tasks
demonstrate that EASYTOOL can significantly
reduce token consumption and improve the
performance of LLM-based agents on tool
utilization in real-world scenarios.Our code
will be available at https://github.com/
microsoft/JARVIS/tree/main/easytool .
1 Introduction
Large language models (LLMs) (OpenAI, 2023;
Touvron et al., 2023a,b; Team and Google, 2023)
have recently ignited the spark of LLM-based au-
tonomous agents (Shen et al., 2023a; Gravitas,
2023), which aim to interact with the real-world
scenarios and address complex user requests. A
rising trend in enhancing their effectiveness is to
endow them with the capability of using external
tools (Schick et al., 2023; Shen et al., 2023a; Qin
et al., 2023). To bridge the gap between LLMs
*The first two authors have equal contributions. This
work was done when the first author was an intern at Microsoft
Research Asia.
†Corresponding authors.
Tool Description Tool Functionality Guidelinewith Example
List Movies is a tool used to list and search through all the available movies. This tool has 9 APIs:1. 'With RT Ratings' returns the list with the Rotten Tomatoes rating included...
{    "name": "With RT Ratings",    "description": "Returns the list with the Rotten Tomatoes rating included",    "required_parameters": [        {            "name": "with_rt_ratings",            "type": "BOOLEAN",            "description": "",            "default": "false"        }    ],    "optional_parameters": [],    "Example": {        "Scenario": "if you want to get the list of movies with Rotten Tomatoes ratings included",        "Parameters": {            "with_rt_ratings": true        }    }}
Scenario: if you want to get the list of movies with Rotten Tomatoes ratings included.Parameters: with_rt_ratings: trueTool Instruction: Concise, Unified and Effective
ExampleRapidAPI
Tool Documentation
So much redundant information!  
Without examples, I do not know when I should use these tools...
Why the formats of these tool documents are so different! I can not deal with them consistently!
HFModel
{   "downloads":1677372,   "id":"ProsusAI/finbert",   "likes":186,   "pipeline_tag":"text-classification",   "task":"text-classification",   "meta":{      "language":"en",      "tags":[         "financial-sentiment-analysis",         "sentiment-analysis"      ],      "widget":[         {            "text":"Stocks rallied and the British pound gained."         }      ]   },   "description":"\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n"
RapidAPI
{    "product_id": "api_20295783-1e06-4cf8-98a2-4b09b829ae7c",    "home_url": "https://rapidapi.com/jpbermoy/api/list-movies/",    "pricing": "FREEMIUM",    "host": "list-movies.p.rapidapi.com",    "tool_description": "An API used to list and search through out all the available movies. Can sort, filter, search and order the results",    "name": "List Movies",    "title": "List Movies",    "tool_name": "List Movies",    "api_list": [        {            "name": "With RT Ratings",            "url": "https://list-movies.p.rapidapi.com/list_movies.json/false",            "description": "Returns the list with the Rotten Tomatoes rating included",            "method": "GET",            "required_parameters": [                {                    "name": "with_rt_ratings",                    "type": "BOOLEAN",                    "description": "",                    "default": "false"                }            ],            "optional_parameters": [],        }    ]}
pricing: FREEMIUMhost: list-movies.p.rapidapi.comhome_url:https://rapidapi.com/jpbermoy/api/list-movies/
Figure 1: An illustration of the proposed EASYTOOL,
and some issues in tool documentation, e.g., Inconsis-
tency, Redundancy, Incompleteness. The documenta-
tions can be polished and refined by EASYTOOL into
more concise and effective tool instructions for better
tool usage.
and tool usage, agents usually first analyze a user
request, conduct planning or reasoning to decom-
pose it into sub-tasks, and then select the most
suitable tools for execution to obtain the final an-
swer. Therefore, improving LLMs’ capability to
use tools precisely has been critical to developing
an autonomous agent.
Previously, some researchers (Schick et al.,
2023; Qin et al., 2023; Patil et al., 2023; Parisi
et al., 2022; Hao et al., 2023) fine-tune open-source
LLMs to generate calling functions to use tools.
However, these methods usually require additional
datasets with tool use for training, cannot be ex-
tended to widely deployed black-box LLMs ( e.g.,
ChatGPT (OpenAI, 2022, 2023) and Gemini (Team
and Google, 2023)), and lack flexibility in inte-arXiv:2401.06201v3  [cs.CL]  27 Mar 2024

--- PAGE 2 ---
grating external tools in a plug-and-play way. An-
other line of work (Shen et al., 2023a; Song et al.,
2023; Lu et al., 2023; Xu et al., 2023) retrieves and
calls external tools by providing tool documenta-
tion and few-shot demonstrations of tool function-
ality. However, these methods struggle with limited
context length and face difficulties when handling
unusual tools, and thus hinder the development
of an omnipotent LLM-based agent. Therefore,
extensive effort is still required to efficiently and
effectively improve the quality of tool utilization.
For tool utilization, tool documentation plays
an indispensable component, which could include
multiple meta information like tool descriptions,
tool parameters, demonstrations and so on. How-
ever, as shown in Figure 1, we summarize the issues
from existing documentation that could hinder the
tool utilization of LLM-based agents:
•Inconsistency : Massive tools from different
sources often have inconsistent and diverse doc-
umentation formats, posing new challenges for
LLMs to understand;
•Redundancy : Tool documentation could encom-
pass massive redundant and useless informa-
tion, making it harder to grasp tool functionality
and resulting in excessive token consumption in
prompts;
•Incompleteness : We expect the tool documen-
tation to provide useful information to describe
its functions, parameters and demonstrations for
instructions. However, the absence of critical in-
formation in some tool documentations impedes
effective tool utilization.
Overall, we regard the information provided by
tool documentation as a critical element in instruct-
ing LLMs to use tools. However, the above issues
in tool documentation bring some challenges to
LLM-based agents to understand, especially con-
sidering the increasing of massive and diverse tools
from different domains. Therefore, how to parse
the documentation, extract the most essential infor-
mation and provide a unified format has become a
necessary topic to effectively use tools.
In this paper, we introduce EASYTOOL, an easy
and effective method to create clear, structured, and
unified instructions from tool documentations for
improving LLM-based agents in using tools. High-
quality tool instructions should follow two criteria:
easy to 1) understand its functionality for selection
and 2) predict its parameters for usage. To this
end, we first collect massive tool documentationsfrom different sources ( e.g., RestBench (Song et al.,
2023) and ToolBench (Qin et al., 2023)). Instead
of directly using these various tool documentations
with different complicated structures, we transform
these documentations into a more concise and uni-
fied tool instruction, which includes standard tool
descriptions and guidelines for tool functionality .
The converted tool descriptions can eliminate irrele-
vant content and only keep the core functionality of
each tool for LLMs to attend to. Moreover, EASY-
TOOL provides detailed information for tool usage
(e.g., its parameters with demonstrations generated
by ChatGPT (OpenAI, 2022)) in tool functionality
guidelines to instruct LLMs with tool usage.
Extensive experiments on multiple datasets
demonstrate these concise tool instructions gen-
erated by EASYTOOL can significantly reduce in-
correct tool usage. Furthermore, we also prove
that the capability of EASYTOOL can be gener-
alized to open-source LLMs in a plug-and-play
way and greatly improve their performance on tool
utilization in different real-world scenarios. Our
contributions can be summarized as:
•We analyze and explore the limitations of cur-
rent tool utilization in LLM-based agents and
first point out the deficiencies of tool docu-
mentation that hinder LLMs in using tools.
•To address these issues, we propose EASY-
TOOL, which creates high-quality tool instruc-
tions from documentation to facilitate tool us-
age in LLM-based agents.
•Experimental results on three datasets from
distinct domains show that our EASYTOOL
effectively and efficiently improves the capa-
bility of LLMs in tool utilization.
2 Related Work
With the emergence of powerful LLMs (OpenAI,
2023; Touvron et al., 2023a,b), using tools has been
considered a new trend to enhance the capabilities
of LLMs A conventional strategy is to build syn-
thetic data (Schick et al., 2023; Qin et al., 2023; Li
et al., 2023; Patil et al., 2023; Shen et al., 2023b)
involved tool use and then fine-tune LLMs to gener-
ate text with tool invocation. However, these meth-
ods cannot be extended to some powerful closed
LLMs, and lack the capability to use new tools. Al-
though some methods (Hao et al., 2023) attempted
to fine-tune LLMs to obtain tool embeddings for
plug-and-play usage, they still require additional
data for training to get tool embeddings.

--- PAGE 3 ---
user requestSubtask 1Subtask 2Subtask nTool Retrieval 
Tool Selection Tool Execution ......
Based on Tool Description
Pass the required parametersBased on Tool Functionality
IncorrectOutputAnswerTask PlanningTool 1Tool 2Tool 3...
Based on Tool DescriptionFigure 2: The four-stage framework of LLM-based
agents in tool-usage applications.
Therefore, there has arisen another branch (Shen
et al., 2023a; Song et al., 2023; Gravitas, 2023;
Zhuang et al., 2024) that directly used LLMs as the
controller and feed tool descriptions into prompts to
instruct LLMs to understand and call tools. These
methods do not need extra training and can use ex-
ternal tools in a plug-and-play paradigm, but they
are limited to context sizes and the quality of tool
documentation. As a result, these methods will lead
to some failed or incorrect tool invocation (Zhang
et al., 2023). Some work (Hsieh et al., 2023; Xu
et al., 2023) attempts to revise tool documentation
to support a zero-shot tool utilization, but some
inherent issues of tool documentation in real-world
scenarios still hinder the effective and efficient us-
age of many tools. Besides, different from naive
prompt compression (Mu et al., 2023; Jiang et al.,
2023b), which is only suitable to compress plain
prompt, the streamlined information from tool doc-
umentation should satisfy specific format and need
to confirm the accuracy of tool invocation when
processing user requests.
3 Preliminary
In this section, we first present the formulation to
define the tool utilization in LLM-based agents and
the limitations.
3.1 Task Formulation
Motivated by previous works (Shen et al., 2023a;
Song et al., 2023), just as shown in Figure 2, the
pipeline of LLM-based agents for tool utilization
can be summarized as a four-stage framework as:
•Task Planning: Agents analyze a user re-
quest Tand decompose it into subtasks T=
{t1, t2, ..., t n}with specific dependencies and ex-
ecution orders, each optimized for execution with
a single tool.Dataset Token Desc. Token Doc.Exp.
RestBench (Song et al., 2023) 58 3,881 ✗
Gorilla (Patil et al., 2023) 88 284 ✗
ToolAlpaca (Tang et al., 2023) 567 7,661 ✗
ToolBench (Qin et al., 2023) 744 2,530 ✗
HFmodels (Shen et al., 2023a) 777 1,196 ✓
Table 1: The statistics of tool documentations in tool
benchmarks. We report the average length of the
tool description with parameters ( Token Desc.), the aver-
age length of the tool documentations ( Token Doc.) and
whether the benchmarks have tool usage scenarios and
example ( Exp. ).
•Tool Retrieval: Here, the focus is on matching
these subtasks with suitable tools from the tool
inventory based on the similarity between the
subtasks and tools. The aim is to select the top-K
tools{a1, a2, ..., a K}, that have the highest simi-
larity to each subtask, forming a set of candidate
tools for each.
•Tool Selection: In this stage, the most ap-
propriate tool for each subtask from the set
{a1, a2, ..., a K}is chosen based on its descrip-
tion. This stage also includes preparing the pa-
rameters for tool execution, as specified in its
document.
•Tool Execution: After tool selection and param-
eter setup, the tool is executed. If a tool fails
during execution, the process reverts to the tool
selection stage for an alternative choice. This
retry mechanism continues until successful exe-
cution or until the maximum trial Ris reached.
After these stages, agents can orchestrate different
tools and use their powers to generate the final
answer for each user request.
3.2 Analysis
Previous studies typically adhere to the established
paradigm, instructing LLMs with the tool docu-
mentation to use tools. However, relying on the
tool documentation can hinder the performance of
LLM-based agents due to its inherent limitations.
Inconsistency In the real world, a wide variety
of tools from different sources results in substantial
diversity in terms of format, style, and guidelines.
As a result, this diversity contributes to a mess of
tool documentations without a cohesive and stan-
dardized structure, posing a significant challenge
for LLMs to effectively use these tools.

--- PAGE 4 ---
Redundancy Generally, the tool documents from
different communities usually contain redundant
information ( e.g., URLs, IDs, etc.). In practical ap-
plication, we just require LLMs to understand the
core function of the tool and then decide whether
to use and how to use this tool. As shown in Ta-
ble 1, we analyze multiple tool-based benchmarks
and the results reveal a high proportion of redun-
dant information in many tool documentations. For
example, the average length of tool documenta-
tions used in ToolBench is approximately 2,530
tokens in Table 1.1This useless information can
severely hinder LLMs from retrieving and selecting
tools, leading to an incorrect tool invocation. More-
over, LLMs are constrained by a maximum context
length, yet tool documentation is typically lengthy.
This excessive length can limit the range of tool
options available for LLMs to consider, posing a
challenge for efficient tool selection.
Incompleteness Previous work has demonstrated
that LLMs may pass invalid parameters, leading
to tool execution failure (Song et al., 2023; Qin
et al., 2023; Zhang et al., 2023; Shen et al., 2023b).
As shown in Table 1, unlike human-oriented in-
struction manuals that provide usage scenarios and
examples, existing tool documentation typically
lacks such context, only offering example codes
for tool invocation or results. This leads to LLMs
struggling to know when and how to refer to the
examples to pass the correct parameters, resulting
in invalid parameters.
4 Method
As aforementioned, polishing, streamlining, and en-
hancing the tool documentation is important to im-
prove tool utilization in LLM-based agents. In this
paper, we introduce EASYTOOL, a simple method
to condense tool documentation into more concise
and effective tool instructions. The overall work-
flow is illustrated in Figure 1. Our framework com-
prises two stages: the first stage is to re-organize
the original tool documentation by eliminating the
irrelevant information and only keeping the func-
tion description of each tool ( §4.1). Afterwards, for
each tool, we further design a functional-guideline
instruction for LLMs and enable LLMs to further
refine the tool documentation by providing param-
eters of each tool and simultaneously the examples
to instruct LLMs for usage ( §4.2).
1We adopt cl100k_base encoding. The code is in https:
//github.com/openai/tiktoken .I: Tool Description Generation
/*I: Task prompt */
Your task is to create a concise and effective tool usage
description based on the tool documentation. You should
ensure the description only contains the purposes of the
tool without irrelevant information. Here is an example:
/*Examples */
{Tool Documentation}
Tool usage description:
{Tool_name} is a tool that can {General_Purposes} .
This tool has {Number} multiple built-in functions:
1.{Function_1} is to {Functionality_of_Function_1}
2.{Function_2} is to ...
/*Auto generation of tool description */
{Tool Documentation of ‘Aviation Weather Center’}
Tool usage description:
‘Aviation Weather Center’ is a tool which can provide official
aviation weather data...
II: Tool Function Guidelines Construction
/*Task prompt */
Your task is to create the scenario that will use the tool.
1. You are given a tool with its purpose and its parameters
list. The scenario should adopt the parameters in the list.
2. If the parameters are null, you
should set: {"Scenario": XX, "Parameters":{}} .
Here is an example:
/*Examples */
{Tool_name} is a tool that can {General_Purposes} .
{Function_i} is to {Functionality_of_Function_i}
{Parameter List of Function_i}
One scenario for {Function_i} of{Tool_name} is:
{"Scenario": XX, "Parameters":{XX:XX}}
/*Auto-construction for Tool Function Guidelines */
‘Ebay’ can get products from Ebay in a specific country.
‘Product Details’ in ‘Ebay’ can get the product details for a
given product id and a specific country.
{Parameter List of ‘Product Details’}
One scenario for ‘Product Details’ of ‘Ebay’ is:
{"Scenario": "if you want to know the details of the product
with product ID 1954 in Germany from Ebay",
"Parameters":{"product_id": 1954, "country": "Germany"}} .
Table 2: Examples of prompt for ChatGPT for tool
description generation and tool function guidelines con-
struction. Green texts are generated by ChatGPT.
4.1 Tool Description Generation
As described above, tool documentation usually in-
cludes plenty of irrelevant information that makes
it difficult to understand practical usage for LLMs.
Moreover, some tools that enable multiple built-
in functions for different scenarios are not always
comprehensively described. For instance, Google
Maps offers both distance calculations and coordi-
nate provision, but its description might not cover
all functionalities. To address this, we expect to
use LLMs to polish and streamline these tool docu-
mentations and decode them into more concise and
effective tool descriptions. Here, just as shown in
Table 2 (I), we design an instruction and require

--- PAGE 5 ---
LLMs ( i.e., ChatGPT) to convert tool documenta-
tion to summarize its general purpose by follow-
ing the designed instruction. We also add extra
demonstrations into the instruction to enhance the
instruction-following of LLMs in parsing tool doc-
umentation.
4.2 Tool Functionality Guidelines
Construction
The tool descriptions generated in the previous step
aid LLMs in tool retrieval and selection. How-
ever, we still need to predict the correct parameters
of each tool for a successful execution. Previous
work (Qin et al., 2023; Zhang et al., 2023; Xu et al.,
2023) also confirms that many open-source LLMs
are still inadequate in executing tools, resulting in
parameter errors. Therefore, we further polish our
tool descriptions in the first stage to supplement
the parameters in the tool instructions. Here, we
design another instruction that requires LLMs to
extract parameters from tool documentation and
then organize it into a structured output, thus facili-
tating LLMs to invoke tools. As shown in Table 2
(II), we use ChatGPT to create examples, includ-
ing scenarios and parameter names with values to
demonstrate how to input parameters for different
scenarios and enhance LLMs to precisely use tools.
To verify the quality of generated examples for the
tool functionality guidelines, we input the parame-
ters to execute the tools to confirm the correct input
of parameters and the accuracy of results.
4.3 Evaluation
To assess the quality of tool descriptions, we se-
lect 100 examples from ToolBench and employ
three annotators to evaluate their accuracy. The
results confirm the high accuracy of all generated
tool descriptions. To assess the plausibility of the
scenarios, we also sample 100 tool functionality
guidelines from ToolBench and employ three anno-
tators to evaluate the plausibility of the scenarios.
The results show that all of the scenarios are reason-
able. The annotation details for quality evaluation
of tool instruction are shown in Appendix A.2
5 Experiment
In this section, we adopt EASYTOOL to three dis-
tinct tool-use applications to show that EASYTOOL
2The evaluation on the robustness of the prompts in Ta-
ble 2 is shown in Appendix C. We also compare with prompt
compression methods in Appendix D.Dataset Token Doc.Token Ins.Reduce (%)
ToolBench 2,530 748 70.43%
RestBench 3,881 103 97.35%
Table 3: The average number of tokens in each tool doc-
umentation ( Token Doc.) and tool instruction generated
byEASYTOOL (Token Ins.). We also report the reduced
ratio ( i.e.Reduce (%) ) for reference.
can help LLM-based agents better answer real-
world user requests through tool usage ( §5.1), find
correct tool solution paths ( §5.2), and improve
their tool utilization capabilities on complex math
problems ( §5.3).
5.1 Real-World Question Answering
Since LLMs are still limited to their training data, it
is essential for LLMs to use external tools to access
up-to-date information in response to user requests.
Benchmark We choose ToolBench (Qin et al.,
2023), a dataset containing diverse user requests
with a massive set of publicly available REST APIs
spanning 49 categories from RapidAPI Hub. We
use the most difficult subsets of ToolBench to eval-
uate our method, i.e., I2-Category (200 test data)
and I3-Instruction (100 test data), which contain
complex user requests that need multiple tools
from different categories to solve. On average,
an I2-Category request needs 6.76 tools, and an
I3-Instruction request needs 8.24 tools. Each data
sample of ToolBench consists of a user request
with a ground truth toolset, and thus models only
need to select and execute the tools from the toolset
to complete the user request.
For evaluation, ToolBench designs two evalu-
ation metrics based on ChatGPT: (1) Pass Rate ,
calculated by the proportion of instructions success-
fully completed within a limited budget; (2) Win
Rate , measured by asking a ChatGPT evaluator to
select its preference for two solution paths. Further-
more, we also measure Success Rate , which asks
GPT-4 to check whether the responses can reason-
ably and accurately answer the user requests.3
Baselines Following Qin et al. (2023), we select
ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023),
Vicuna-7B (Chiang et al., 2023), and ToolLLaMA-
7B as baselines and apply both ReACT (Yao et al.,
2023) and DFSDT (Qin et al., 2023) to them.
ToolLLaMA-7B is fine-tuned from a 7B LLaMA
3The prompt template for evaluating success rate is shown
in Appendix B.1

--- PAGE 6 ---
Model MethodI2-Category I3-Instruction Average
Pass Win Success Pass Win Success Pass Win Success
ChatGPTReACT 39.0 - 18.0 23.0 - 1.0 31.0 - 9.5
DFSDT 64.5 63.0 24.0 60.0 70.0 6.0 62.3 66.5 15.0
DFSDT-E ASYTOOL 74.5 76.5 68.5 65.0 88.0 37.0 69.8 82.3 52.8
DFSDT-E ASYTOOL-Retriever 69.0 71.0 60.5 66.0 89.0 42.0 67.5 80.0 51.3
ToolLLaMA-7BReACT 30.0 45.5 9.5 22.0 49.0 3.0 26.0 47.3 6.3
DFSDT 66.0 55.0 24.0 56.0 56.0 6.0 61.0 55.5 15.0
DFSDT-Retriever 57.0 60.0 11.5 54.0 69.0 2.0 55.5 64.5 6.8
Vicuna-7BReACT 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
DFSDT 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
DFSDT-E ASYTOOL 72.5 77.0 40.5 68.0 81.0 34.0 70.3 79.0 37.3
DFSDT-E ASYTOOL-Retriever 75.0 68.0 46.5 67.0 85.0 36.0 71.0 76.5 41.3
Mistral-Instruct-7BReACT 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
DFSDT 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
DFSDT-E ASYTOOL 75.0 76.0 56.0 66.0 87.0 38.0 70.5 81.5 47.0
DFSDT-E ASYTOOL-Retriever 74.5 71.5 54.5 68.0 88.0 38.0 71.3 79.8 46.3
GPT-4ReACT 67.5 53.5 27.0 40.0 71.0 4.0 53.8 62.3 15.5
DFSDT 69.5 57.0 42.0 59.0 73.0 50.0 64.3 65.0 46.0
DFSDT-E ASYTOOL 76.5 78.5 76.0 69.0 89.0 64.0 72.8 83.8 70.0
DFSDT-E ASYTOOL-Retriever 72.5 72.0 73.5 69.0 90.0 53.0 70.8 81.0 63.3
Table 4: Results of LLMs on ToolBench. Win rate (denoted as Win) is calculated by comparing each model with
ChatGPT-ReACT. The win rate higher than 50% means the model performs better than ChatGPT-ReACT. Apart
from Retriever, all methods use the ground truth toolset to select tools. The best results are bolded , and the second
best ones are underlined.
model (Touvron et al., 2023a) on ToolBench. We
do not adopt ToolLLaMA-7B on EASYTOOL due
to its poor instruction-following capability. Fur-
thermore, we also adopt Mistral-Instruct-7B (Jiang
et al., 2023a) for comparison, which is fine-tuned
from Mistral-7B and exhibits great instruction-
following capability.4For the baselines, we follow
the settings in Qin et al. (2023), which provides
tool documentation for them to use tools.
Main Result We simplify the tool documentation
from ToolBench into concise tool instructions with
EASYTOOL.5Each tool instruction consists of a
tool description and functionality guidelines. As
shown in Table 3, with EASYTOOL, replacing tool
documentation with our tool instruction can greatly
reduce the token cost of each tool. Especially in
ToolBench, the token cost was reduced by 70.43%.
Furthermore, results in Table 4 show that:
1)With the EASYTOOL generated tool instruc-
tions, LLMs can achieve state-of-the-art perfor-
mance. Notably, ChatGPT + DFSDT- EASYTOOL
even surpasses GPT-4 + DFSDT in success rate,
indicating the superiority of tool Instructions over
4Detailed information of baselines is shown in Ap-
pendix B.2
5The data examples of ToolBench are provided in Ap-
pendix E.1.MethodI2-Category I3-Instruction Average
@1 @5 @1 @5 @1 @5
BERT Retriever 68.2 77.9 81.7 87.1 75.0 82.5
GPT Ada 36.8 30.7 54.6 46.8 45.7 38.8
+ EASYTOOL 73.4 82.7 80.1 88.5 76.7 85.6
Table 5: The performance of different retrievers for
two subsets in ToolBench. We report NDCG@1 and
NDCG@5 .
tool documentation in facilitating tool utilization
for LLMs; 2)Vicuna and Mistral-Instruct-7B re-
sult in a failure when directly using tools. Based
on previous experiences (Shen et al., 2023b), we
attribute this phenomenon to the lack of training
in formatted data ( e.g., function call). However,
tool instructions generated by EASYTOOL can help
these models to better understand the usage of
tools, even making them outperform the fine-tuned
method, i.e., ToolLLaMA; 3)Mistral-Instruct-7B
outperforms Vicuna-7B with EASYTOOL, indicat-
ing that models with better instruction-following
capabilities can achieve greater improvements with
high-quality tool instructions.
EASYTOOL helps retrieve high-quality tools.
In real-world scenarios, asking users to manually
recommend tools from a large pool for LLMs

--- PAGE 7 ---
/uni00000018/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000018/uni00000013
/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000026/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000037/uni00000052/uni00000052/uni0000004f/uni00000056/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037
/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000028/uni00000024/uni00000036/uni0000003c/uni00000037/uni00000032/uni00000032/uni0000002f
/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000028/uni00000024/uni00000036/uni0000003c/uni00000037/uni00000032/uni00000032/uni0000002fFigure 3: The selection accuracy of LLMs on I1-
instruciton of ToolBench.
to select may not be practical. Therefore, Tool-
Bench also provides a dense retriever based on
BERT-base (Devlin et al., 2019) to retrieve relevant
tools for solving user requests, and claims that it
outperforms text-embedding-ada-002 ,i.e.GPT
Ada (Ouyang et al., 2022), which retrieves tools
based on the cosine embedding similarity between
the subtasks decomposed by user requests and tool
descriptions from original tool documentation in
ToolBench. We argue that the poor performance of
Ada may be due to low-quality tool descriptions,
which often contain irrelevant details and lack clear
functionality guidelines. Thus, we adopt the tool
description generated by EASYTOOL to replace
the original tool descriptions. Following Qin et al.
(2023), we compare the performance of these re-
trieval methods using NDCG (Järvelin and Kekäläi-
nen, 2002). Results in Table 5 show that provid-
ing tool descriptions generated by EASYTOOL can
greatly improve the retrieval performance.
EASYTOOL helps LLMs with tool selection and
execution. To answer this question, we utilize
the I1-Instruciton of ToolBench, which comprises
100 user requests solvable by a single tool. We
first obtain the golden tool from I1-Instruction and
then retrieve other different tools based on cosine
embedding similarity between the user request and
tool descriptions as candidate tools. Then, we eval-
uate the selection accuracy of LLMs with vary-
ing numbers of candidate tools, using either origi-
nal ToolBench descriptions or those generated by
EASYTOOL. Figure 3 illustrates that EASYTOOL-
enhanced descriptions enable LLMs to select the
correct tool more effectively from a larger pool.
For each subtask in I2-Category and I3-
Instruction, we retrieve the top 10 most similar
tools using our tool descriptions and ask models
to select and execute them. As shown in Table 4,
/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037 /uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037
/uni0000000e/uni00000003/uni00000028/uni00000024/uni00000036/uni0000003c/uni00000037/uni00000032/uni00000032/uni0000002f/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017 /uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017
/uni0000000e/uni00000003/uni00000028/uni00000024/uni00000036/uni0000003c/uni00000037/uni00000032/uni00000032/uni0000002f/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000052/uni00000052/uni0000004f/uni00000003/uni00000026/uni00000044/uni0000004f/uni0000004f/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000001b/uni00000008
/uni00000013/uni00000008/uni00000018/uni00000008
/uni00000013/uni00000008/uni00000015/uni00000018/uni00000008
/uni00000019/uni00000008/uni00000014/uni0000001a/uni00000008
/uni00000014/uni00000008/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055 /uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055Figure 4: Error rates of tool calls in different LLMs.
The error rate is the number of two tool-related errors
relative to the total number of tool calls. The results are
evaluated by human annotators .
using these retrieved tools proves to be comparable,
and sometimes even superior, to the ground truth
tool set. The rationale is that EASYTOOL-Retriever
can retrieve similar tools with better functionalities
to replace some tools in the ground truth tool set.
Error Analysis We follow Zhang et al. (2023)
and define two types of error, i.e., Tool name er-
ror and parameter error. Tool name error means
models call non-existent tool functions that are not
in the tool inventory, and parameter error means
models pass invalid parameters. Both errors lead to
unsuccessful tool execution. We sample 100 data
from I2-Category and I3-Instruction and employ
three annotators to manually examine the output of
LLMs with tool documentation and tool instruction
generated by EASYTOOL. We present the error
rates of each error type on ToolBench in Figure 4.
The results show that LLMs may generate non-
existent tool names and pass invalid parameters to
the right tool functions. However, our EASYTOOL,
with concise and effective tool instruction, can sig-
nificantly reduce these incorrect behaviors, leading
to successful tool execution.
5.2 Real-World Web Services
Real-world web services often need to execute
tools following a specific order. For example, a
shopping cart Web Service requires users to add
items to the cart before checkout. We aim to ex-
plore the capability of LLMs to find correct tool
solution paths.
Benchmark We select RestBench (Song et al.,
2023), which consists of tasks in real-world web
service scenarios. We evaluate our method on a sub-
set of RestBench, i.e., TMDB. TMDB is a movie
information website that offers 55 official RESTful
APIs as tools, which cover information on movies,

--- PAGE 8 ---
/uni00000035/uni00000048/uni00000056/uni00000057/uni0000002a/uni00000033/uni00000037 /uni0000000e/uni00000037/uni00000052/uni00000052/uni0000004f/uni00000027/uni00000048/uni00000046 /uni0000000e/uni00000028/uni00000024/uni00000036/uni0000003c/uni00000037/uni00000032/uni00000032/uni0000002f/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015/uni00000015/uni00000017/uni00000015/uni00000019/uni00000026/uni00000033/uni00000008/uni00000039/uni0000004c/uni00000046/uni00000058/uni00000051/uni00000044/uni00000003/uni0000000b/uni00000014/uni00000016/uni00000025/uni0000000c/uni00000003/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000050/uni00000052/uni00000047/uni00000048/uni0000004f
/uni00000035/uni00000048/uni00000024/uni00000046/uni00000057 /uni00000035/uni00000048/uni00000056/uni00000057/uni0000002a/uni00000033/uni00000037 /uni0000000e/uni00000028/uni00000024/uni00000036/uni0000003c/uni00000037/uni00000032/uni00000032/uni0000002f/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni00000026/uni00000033/uni00000008/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000003/uni00000045/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000050/uni00000052/uni00000047/uni00000048/uni0000004fFigure 5: The correct path rate (CP%) on two versions
of RestBench with different methods.
TVs, actors, and images. Following the evaluation
metric in RestBench, we use the correct path rate
(CP%) to measure accuracy, which is the propor-
tion of the model-generated tool path containing
the gold tool path as a subsequence.
Baselines We choose RestGPT (Song et al.,
2023) as our base model. The RestGPT has two
versions, i.e., Vicuna-13B-based RestGPT and
ChatGPT-based RestGPT. For Vicuna-13B-based
RestGPT, we compare our method with ToolDec.
For ChatGPT-based RestGPT, we compare our
method with ReAct since ToolDec cannot apply
in close-sourced models.
Result We simplify the long tool documentation
from RestBench into concise tool instructions with
EASYTOOL for LLMs to use.6For comparison, we
use the prompt from Song et al. (2023) containing
original tool descriptions and four examples. Ta-
ble 3 demonstrates that EASYTOOL significantly
reduces the token cost. Additionally, Figure 5 high-
lights the considerable improvement in the correct
path rate, signifying EASYTOOL’s effectiveness in
aiding LLMs to find the correct tool solution paths.
5.3 Numerical Reasoning
We also explore whether EASYTOOL can endow
LLM-based agents with better tool-utilization abil-
ity in complex math problems with incomplete tool
documentation.
Benchmark We adopt FuncQA (Hao et al.,
2023), which tests the numerical reasoning abil-
ity of LLMs on complex math problems involving
13 arithmetic operations tools ( e.g., multiply, power
and lcm). We use the two subsets of FuncQA. i.e.
one-hop andmulti-hop questions, to evaluate our
method. The one-hop questions consist of 68 math
problems solvable with one tool. The 60 multi-hop
questions require some reasoning steps, averaging
2.78 tool uses per question. We measure accuracy
6The data examples of RestBench are provided in Ap-
pendix E.2.Model One-hop (↑)Multi-hop (↑)Error (↓)
Vicuna-30B 15.00 1.00 -
+ CoT 13.33 4.00 -
+ ReAct 45.00 7.35 20.31
+ EASYTOOL 65.00 11.76 10.15
ChatGPT 55.00 9.00 -
+ CoT 48.33 17.64 -
+ ReAct 85.00 41.17 9.38
+ EASYTOOL 91.66 48.53 2.34
Table 6: The accuracy of Vicuna-30B and ChatGPT on
the FuncQA.
by calculating the percentage of correctly answered
problems, with a 0.1% error tolerance. We also
measure tool error rate ( i.e.,Error ), the proportion
of tasks that have at least one tool-related error.
Baselines Following Hao et al. (2023), We se-
lect Vicuna-30B and ChatGPT and compare our
method with 0-shot learning, Chain-of-thought
(CoT) prompting and ReAct. For ReAct, we follow
the settings in (Hao et al., 2023), which provide
four examples, including five tool examples.
Result Unlike the other datasets, FuncQA only
provides the name and calling function of a tool as
documentation, without any other tool descriptions
for further usage demonstration. Therefore, by only
leveraging the provided tool name and calling func-
tion, we can also apply EASYTOOL to generate tool
descriptions with usage scenarios to construct tool
instruction for FuncQA.7Results in Table 6 show
that: 1)The tool instructions generated based on
our method (+ EASYTOOL) significantly improve
the tool utilization ability of LLMs on complex
math problems; 2)Furthermore, a lower tool error
rate of models with EASYTOOL indicates that com-
pared to few-shot learning with demonstrations,
concise and effective tool instructions can better
guide models to select correct tools and pass valid
parameters.
6 Conclusion
In this paper, we introduce EASYTOOL, an easy
and effective method to enhance the tool utilization
capabilities of LLM-based agents through the sim-
plification and refinement of tool documentation
into a clear, structured and practical tool instruc-
tion. Our comprehensive experiments demonstrate
that EASYTOOL can effectively enhance perfor-
7The data examples of FuncQA are provided in Ap-
pendix E.3.

--- PAGE 9 ---
mance in different real-world applications. We
hope EASYTOOL can be a significant development
in the field of LLM-based agents.
Limitations
First, this paper only focuses on tool documenta-
tion whose token length does not exceed the Chat-
GPT input limit. Documentation with token counts
surpassing this limit cannot be processed by EASY-
TOOL without additional preprocessing. Second,
our method is limited to single documentation, ne-
glecting the dependencies among tools. Consider-
ing these dependencies in tool descriptions could
significantly enhance the model’s effectiveness in
certain scenarios. Finally, EASYTOOL only works
on models with instruction-following ability. Fu-
ture work can focus on training specialized models
using tool instructions generated by EASYTOOL,
thereby improving their capability in tool utiliza-
tion.
Ethics Statement
We acknowledge that all authors are informed
about and adhere to the ACL Code of Ethics and
the Code of Conduct.
Use of Human Annotations Our institution re-
cruited annotators to implement the annotations
of tool descriptions and functionality guidelines.
We ensure the privacy rights of the annotators are
respected during the annotation process. The an-
notators receive compensation exceeding the lo-
cal minimum wage and have consented to the use
of tool instructions generated by EASYTOOL for
research purposes. Appendix A provides further
details on the annotations.
Risks The tool benchmarks in our experiment are
sourced from publicly available sources. However,
we cannot guarantee that they are devoid of socially
harmful or toxic language. Furthermore, evaluating
the data quality of tool instructions is based on
common sense, which can vary among individuals
from diverse backgrounds. We use ChatGPT to
correct grammatical errors in this paper.
References
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, IonStoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Significant Gravitas. 2023. Auto-gpt: An au-
tonomous gpt-4 experiment. https://github.com/
Significant-Gravitas/Auto-GPT .
Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu.
2023. ToolkenGPT: Augmenting frozen language
models with massive tools via tool embeddings. In
Thirty-seventh Conference on Neural Information
Processing Systems .
Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa
Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Kr-
ishna, and Tomas Pfister. 2023. Tool documenta-
tion enables zero-shot tool-usage with large language
models.
Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumu-
lated gain-based evaluation of ir techniques. ACM
Trans. Inf. Syst. , 20(4):422–446.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, Lélio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023a. Mistral 7b.
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing
Yang, and Lili Qiu. 2023b. LLMLingua: Compress-
ing prompts for accelerated inference of large lan-
guage models. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 13358–13376, Singapore. Association
for Computational Linguistics.
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song,
Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,
and Yongbin Li. 2023. Api-bank: A comprehensive
benchmark for tool-augmented llms. In Proceedings
of the 2023 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2023, Singapore,
December 6-10, 2023 , pages 3102–3116.
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-
Wei Chang, Ying Nian Wu, Song-Chun Zhu, and
Jianfeng Gao. 2023. Chameleon: Plug-and-play com-
positional reasoning with large language models. vol-
ume abs/2304.09842.
Jesse Mu, Xiang Lisa Li, and Noah D. Goodman.
2023. Learning to compress prompts with gist to-
kens. CoRR , abs/2304.08467.

--- PAGE 10 ---
OpenAI. 2022. Chatgpt.
OpenAI. 2023. GPT-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Gray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems .
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022.
TALM: tool augmented language models. CoRR ,
abs/2205.12255.
Shishir G. Patil, Tianjun Zhang, Xin Wang, and
Joseph E. Gonzalez. 2023. Gorilla: Large lan-
guage model connected with massive apis. CoRR ,
abs/2305.15334.
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan
Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie,
Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu,
and Maosong Sun. 2023. Toolllm: Facilitating large
language models to master 16000+ real-world apis.
CoRR , abs/2307.16789.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
CoRR , abs/2302.04761.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023a. Hugging-
GPT: Solving AI tasks with chatGPT and its friends
in hugging face. In Thirty-seventh Conference on
Neural Information Processing Systems .
Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang,
Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li,
and Yueting Zhuang. 2023b. Taskbench: Benchmark-
ing large language models for task automation. arXiv
preprint arXiv:2311.18760 .
Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu,
Han Qian, Mingbo Song, Hailiang Huang, Cheng
Li, Ke Wang, Rong Yao, Ye Tian, and Sujian Li.
2023. Restgpt: Connecting large language models
with real-world restful apis.
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,
Qiao Liang, and Le Sun. 2023. Toolalpaca: Gener-
alized tool learning for language models with 3000
simulated cases.
Gemini Team and Google. 2023. Gemini: A family of
highly capable multimodal models.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, FaisalAzhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models. CoRR , abs/2307.09288.
Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu,
Zhengyu Chen, and Jian Zhang. 2023. On the tool
manipulation capability of open-source large lan-
guage models.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.
React: Synergizing reasoning and acting in language
models. In The Eleventh International Conference
on Learning Representations .
Kexun Zhang, Hongqiao Chen, Lei Li, and William
Wang. 2023. Syntax error-free and generalizable tool
use for llms via finite-state decoding.
Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra,
Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel,
and Chao Zhang. 2024. Toolchain*: Efficient action
space navigation in large language models with a*
search. In The Twelfth International Conference on
Learning Representations .

--- PAGE 11 ---
A Crowd-sourcing Details for Tool
Instruction Evaluation
We have recruited a team of three undergraduates.
We pay each annotator $8/h, exceeding the local
minimum wage. The screenshots of the instructions
and interface for tool description and functionality
guideline annotation are shown in Figure 7 and
Figure 8.
B Details of ToolBench
B.1 Success Rate Evaluation
The prompt of the success rate evaluation is given
in List 1.
Listing 1: Instruction templates for GPT-4 to evaluate
the success rate of the results on ToolBench
Please check whether the response can
reasonably and accurately answer the
question . If it can , please output 'YES
'; If not , please output 'NO'
You need to give reasons first and then
decide whether the response can
reasonably and accurately answer the
question . You must only output in a
parsible JSON format . Two example
outputs look like :
Example 1: {" Reason ": " The reason why
you think the response can reasonably
and accurately answer the question ", "
Choice ": " Yes "}
" Example 2: {" Reason ": " The reason why
you think the response cannot reasonably
and accurately answer the question ", "
Choice ": "No "}
This is the user 's question : { question }
This is the response : { answer }
Output :
B.2 The Details of Baselines on ToolBench
Vicuna-7B (Chiang et al., 2023) is the LLaMA vari-
ant fine-tuned on instructions and user-shared con-
versations. Mistral-Instruct-7B (Jiang et al., 2023a)
is the Mistral-7B variant fine-tuned on instructions,
which exhibits great instruction-following and rea-
soning capability. The ReAct in Qin et al. (2023)
first decomposes the use request into subtasks and
then plans the tool calls to complete the subtasks.
The DFSDT in Qin et al. (2023) adopts a depth-
first search-based decision tree to enable LLMs to
make deliberate decisions by assessing different
reasoning paths.
Following the setting in Qin et al. (2023), for
ChatGPT and GPT-4, we directly leverage the func-tion call to use tools8. For other models, we synthe-
size input in function call format to these models.
C Robustness Evaluation
In this section, we aim to evaluate the robustness
of the task prompts in Table 2. We ask ChatGPT to
rewrite these task prompts three times, and the new
task prompts are shown in List 2. We sample 100
tool documentations from ToolBench and ask Chat-
GPT to generate tool descriptions and tool func-
tionality guidelines based on the new task prompts.
Then, we ask two annotators to evaluate the qual-
ity of four results (one from our task prompts and
three from ChatGPT-generated task prompts). The
results in Figure 6 show that the changes to the task
prompt, without altering the actual meaning, do not
affect the quality of the tool description and tool
functionality guidelines, thereby demonstrating the
robustness of our prompts.
Listing 2: The ChatGPT generated task prompts for
tool description generation and tool function guidelines
construction.
Prompt -1:
- Tool Description :
Your assignment involves developing a
succinct and practical description on
using a specific tool , as outlined in
its documentation . This description
should focus solely on the tool 's
functions , excluding any extraneous
details .
- Tool Function Guidelines :
Create a scenario that incorporates the
use of a specified tool , ensuring it
utilizes the provided parameters .
Receive a description of a tool ,
including its purpose and a list of
parameters . Design a scenario that
effectively employs these parameters .
If both " required_parameters " and "
optional_parameters " are absent , format
your response as:
{" Scenario ": XX , " Parameters ": {}}.
Prompt -2:
- Tool Description :
Your assignment involves crafting a
succinct and practical description of a
tool , using its documentation as a
reference . Focus on outlining the tool 's
functions , excluding any extraneous
details .
- Tool Function Guidelines :
Your assignment involves developing a
scenario that utilizes a specified tool .
Here are the guidelines :
You will receive information about a
tool , including its intended use and a
8https://openai.com/blog/
function-calling-and-other-api-updates

--- PAGE 12 ---
/uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000010/uni00000014/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000010/uni00000015/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000010/uni00000016
/uni00000014/uni00000011/uni00000013/uni00000008/uni00000015/uni00000011/uni00000013/uni00000008/uni00000013/uni00000011/uni00000013/uni00000008
/uni0000001c/uni0000001a/uni00000011/uni00000013/uni00000008/uni0000001c/uni00000018/uni00000011/uni00000013/uni00000008/uni0000001c/uni0000001b/uni00000011/uni00000013/uni00000008
/uni00000015/uni00000011/uni00000013/uni00000008/uni00000016/uni00000011/uni00000013/uni00000008/uni00000015/uni00000011/uni00000013/uni00000008
/uni0000003a/uni0000004c/uni00000051 /uni00000037/uni0000004c/uni00000048 /uni0000002f/uni00000052/uni00000056/uni00000048Figure 6: Comparison of our task prompts with Chat-
GPT generated task prompts. Percentage of wins, ties
and losses are calculated.
list of parameters . Your scenario should
incorporate these parameters .
In cases where both " required_parameters
" and " optional_parameters " are absent ,
format your response as follows :
{" Scenario ": XX , " Parameters ": {}}.
Prompt -3:
- Tool Description :
Your assignment involves writing a
succinct and clear description of a tool
's usage , guided by its documentation .
This description should exclusively
focus on the tool 's functions , omitting
any extraneous details .
- Tool Function Guidelines :
Your assignment involves crafting a
scenario that utilizes a specific tool .
Here 's how to proceed :
First , familiarize yourself with the
tool 's intended use and its available
parameters . Then , design a scenario that
effectively incorporates these
parameters .
In cases where both " required_parameters
" and " optional_parameters " are absent ,
format your response as follows :
{" Scenario ": XX , " Parameters ": {}}.
D Prompt Compression Method
We also adopt LLMLingua (Jiang et al., 2023b),
a prompt compression method, to identify and re-
move non-essential tokens in tool documentation.
As shown in Table 7, this method can not be applied
to our task since it may compress some tokens in
parameters and functions, which are essential for
successful tool execution.
E Examples of Tool Instruction
E.1 Data Examples of ToolBench
Table 8 presents some examples of tool instructions
generated by EASYTOOL in ToolBench for a better
understanding.E.2 Data Examples of RestBench
Table 9 presents some examples of tool instructions
generated by EASYTOOL in RestBench for a better
understanding.
E.3 Data Examples of FuncQA
Table 10 presents some examples of tool instruc-
tions generated by EASYTOOL in FuncQA for a
better understanding.

--- PAGE 13 ---
Tool Documentation :
{
" product_id ": " api_b04d269d -c7dd -4 b84 -8 e17 -6 fba24d64d3d ",
" tool_description ": " Get Products from Ebay ( Unofficial )",
" home_url ": " https :// rapidapi . com / felixeschmittfes / api / ebay32 /",
" name ": " Ebay ",
" title ": " Ebay ",
" pricing ": " FREEMIUM ",
" tool_name ": " Ebay ",
" host ": " ebay32 .p. rapidapi . com ",
" api_list ": [
{
" name ": " Product Details ",
" url ": " https :// ebay32 .p. rapidapi . com / product /195499451557" ,
" description ": " Get the product details for a given product id and a
specific country .\ nDefault country is `United States `.\ nSpecify country
with country name or country code .\n\ nAllowed countries :\ nDefault : `us`\
n- Germany (de)\n- France (fr)\n- Australia (au)\n- Austria (at)\n-
Canada (ca)\n- Hong Kong (hk)\n- Ireland (ie)\n- Italy (it)\n- Malaysia
(my)\n- Netherlands (nl)\n- Singapore (sg)\n- Switzerland (ch)\n- United
Kingdom (uk)",
" method ": " GET ",
" required_parameters ": [
{
" name ": " product_id ",
" type ": " NUMBER ",
" description ": "ID of the product . Can be obtained from the url
of the product or by using the `/search `endpoint .",
" default ": "195499451557"
}
],
" optional_parameters ": [
{
" name ": " country ",
" type ": " STRING ",
" description ": " Valid country to return offers for .\ nValid
values are in description of this endpoint .\ nDefault : `united
states `." ,
" default ": " germany "
},
{
" name ": " country_code ",
" type ": " STRING ",
" description ": " Country code of the valid country to return
offers for .\ nValid values are in description of this endpoint .\
nDefault : `us`." ,
" default ": "de"
}
]
}
]
}
Tool Instruction Compressed By LLMLingua :
{
" product " _b04d269d -c7be - fba24d64d ",
"_ " Get fromay ( Unofficial ":// id ./ fixeschmittfes /ay/ " " " " " FREEM " " ".p. "_ [
" " Product Details ",
" url ": " https :// ebay32 .p. rapidapi . com / product /195499451557" ,
" description ": " Get the product details for a given product id and a
specific country .
Default country is `United States `.
Specify country with country name or country code .
Table 7: The original tool documentation and tool instruction compressed by LLMLingua.

--- PAGE 14 ---
Figure 7: The screenshots of the instructions and interface for tool description annotation.

--- PAGE 15 ---
Figure 8: The screenshots of the instructions and interface for tool functionality guidelines annotation.

--- PAGE 16 ---
Tool Description :
/* Example 1 */
'TokopediaApi 'can search and retrieve product details from Tokopedia . This tool has
2 APIs : 1. 'Search Product 'can search for products on Tokopedia based on a query
string and action type . 2. 'Get Product Detail 'can retrieve detailed information
about a product on Tokopedia based on its slug .
/* Example 2 */
'Tokopedia Super API 'can effortlessly retrieve shop and product information . This
tool has 1 API : 1. 'sortProductsMaster 'can provide the list of available sorting
methods .
Tool Function Guidelines :
/* Example 1 */
{
" name ": " Search Product ",
" description ": " Search The Product ",
" required_parameters ": [
{
" name ": " query ",
" type ": " STRING ",
" description ": "",
" default ": " Celana Jeans "
},
{
" name ": " act ",
" type ": " STRING ",
" description ": "",
" default ": " search "
}
],
" optional_parameters ": [],
" Example ": {
" Scenario ": "if you want to search for a product with the query 'Celana
Jeans 'using the 'search 'action ",
" Parameters ": {
" query ": " Celana Jeans ",
" act ": " search "
}
}
}
/* Example 2 */
{
" name ": " sortProductsMaster ",
" description ": " the list of available sorting methods ",
" required_parameters ": [],
" optional_parameters ": [],
" Example ": {
" Scenario ": "if you want to retrieve the list of available sorting methods
for products using Tokopedia Super API ",
" Parameters ": {}
}
}
Table 8: The tool instruction of ToolBench generated by E ASYTOOL.

--- PAGE 17 ---
Tool Description :
/* Example 1 */
'/tv/ latest 'can get the most newly created TV show .
/* Example 2 */
'/ search / collection 'can search for collections , which can obtain collection_id .
Tool Function Guidelines :
/* Example 1 */
{
" tool_usage ": " GET / person /{ person_id }/ tv_credits ",
" Example ": {
" Scenario ": "If you want to get the TV show credits of a person with
person_id 456." ,
" Parameters ": {
" input ": " GET / person /456/ tv_credits "
}
}
}
/* Example 2 */
{
" tool_usage ": " GET /tv/ latest ",
" Example ": {
" Scenario ": "If you want to get the most newly created TV show .",
" Parameters ": {
" input ": " GET /tv/ latest "
}
}
}
Table 9: The tool instruction of RestBench generated by E ASYTOOL.

--- PAGE 18 ---
Tool Description :
/* Example 1 */
'add_ 'returns the sum of all the arguments passed to it , normalized to 2 decimal
places .
/* Example 2 */
'subtract_ 'returns the difference of the arguments passed to it , starting with the
first argument and subtracting all subsequent arguments , normalized to 2 decimal
places .
Tool Function Guidelines :
/* Example 1 */
{
" required_parameters ":[
{
" name ":" input ",
" type ":" List "
}
],
" Example ":{
" Scenario ":" if you want to add 2 to 1." ,
" Parameters ":{
" input ":[2 ,1]
}
}
}
/* Example 2 */
{
" required_parameters ": [
{
" name ": " input ",
" type ": " List "
}
],
" Example ": {
" Scenario ": "if you want to subtract 2 from 1." ,
" Parameters ": {
" input ": [1 ,2]
}
}
}
Table 10: The tool instruction of FuncQA generated by E ASYTOOL.
