# MEMORY-VQ: Nén cho Bộ nhớ Internet Quy mô Lớn Khả thi

Yury Zemlyanskiy∗ † †, Michiel de Jong∗ †, Luke Vilnis
Santiago Ontañón ,William W. Cohen ,Sumit Sanghai ,Joshua Ainslie
Google Research

Tóm tắt
Tăng cường truy xuất là một phương pháp mạnh mẽ nhưng tốn kém để làm cho các mô hình ngôn ngữ hiểu biết hơn về thế giới. Các phương pháp dựa trên bộ nhớ như LUMEN (de Jong et al., 2023a) tính toán trước các biểu diễn token cho các đoạn văn được truy xuất để tăng tốc đáng kể quá trình suy luận. Tuy nhiên, bộ nhớ cũng dẫn đến yêu cầu lưu trữ lớn hơn nhiều từ việc lưu trữ các biểu diễn đã tính toán trước.

Chúng tôi đề xuất MEMORY-VQ, một phương pháp mới để giảm yêu cầu lưu trữ của các mô hình tăng cường bộ nhớ mà không hy sinh hiệu suất. Phương pháp của chúng tôi sử dụng một bộ tự mã hóa biến phân lượng tử hóa vector (VQ-VAE) để nén các biểu diễn token. Chúng tôi áp dụng MEMORY-VQ cho mô hình LUMEN để có được LUMEN-VQ, một mô hình bộ nhớ đạt được tỷ lệ nén 16x với hiệu suất tương đương trên điểm chuẩn KILT. LUMEN-VQ cho phép tăng cường truy xuất thực tế ngay cả đối với các kho dữ liệu truy xuất cực kỳ lớn.

1 Giới thiệu

Tăng cường truy xuất là một phương pháp phổ biến để cải thiện kiến thức thực tế của các mô hình ngôn ngữ (Izacard và Grave, 2021; Borgeaud et al., 2022; Lewis et al., 2020; Khandelwal et al., 2020; Guu et al., 2020; Izacard et al., 2022). Truy xuất cung cấp cho mô hình bối cảnh bổ sung dưới dạng các đoạn văn bản liên quan đến truy vấn đầu vào. Tuy nhiên, tăng cường truy xuất đi kèm với chi phí tính toán tăng lên, vì mô hình phải xử lý các đoạn văn được truy xuất ngay lập tức.

Một hướng nghiên cứu gần đây (Zemlyanskiy et al., 2021; de Jong et al., 2022; Chen et al., 2022; Li et al., 2022; de Jong et al., 2023a) tăng tốc tăng cường truy xuất bằng cách mã hóa trước các đoạn văn từ kho dữ liệu. Theo cách này, mô hình có thể truy xuất các biểu diễn thay vì văn bản thô, điều này tránh được chi phí đọc các đoạn văn được truy xuất từ đầu. Một mô hình như vậy, LUMEN, nổi bật với hiệu suất mạnh mẽ, đạt được suy luận nhanh hơn 3 lần so với Fusion-in-Decoder tiêu chuẩn (Izacard và Grave, 2021) (FiD) với mất mát chất lượng tối thiểu.

Tuy nhiên, các mô hình bộ nhớ mã hóa trước này sử dụng nhiều không gian lưu trữ hơn các mô hình tăng cường truy xuất truyền thống - LUMEN lưu một embedding cho mỗi token trong kho dữ liệu, điều này chiếm nhiều không gian hơn ID token. Bảng 1 so sánh yêu cầu lưu trữ cho các mô hình có kích thước T5 XXL. FiD yêu cầu 2 byte để lưu trữ ID của mỗi token, trong khi LUMEN sử dụng một vector 4096 chiều với các giá trị bfloat16, tổng cộng 8KB mỗi token. Wikipedia chứa khoảng 4 tỷ token, có nghĩa là các biểu diễn token LUMEN chiếm 30TB. Đối với kho dữ liệu quy mô internet với 1 nghìn tỷ token, yêu cầu đĩa tăng lên mức không thực tế 7PB.

Công trình này kết hợp lượng tử hóa sản phẩm (Jégou et al., 2011) và phương pháp VQ-VAE (van den Oord et al., 2017) để giảm đáng kể yêu cầu lưu trữ cho các phương pháp dựa trên bộ nhớ với mất mát chất lượng hạn chế. Cụ thể, LUMEN-VQ đạt được tỷ lệ nén 16x, có nghĩa là chúng tôi chỉ cần 2TB để lưu trữ bộ nhớ cho toàn bộ Wikipedia và 500TB cho kho dữ liệu 1 nghìn tỷ token. Hơn nữa, LUMEN-VQ có mất mát hiệu suất tối thiểu trên điểm chuẩn KILT (Petroni et al., 2021) của các tác vụ chuyên sâu về kiến thức.

Đóng góp của chúng tôi là bài báo đầu tiên về nén các biểu diễn bộ nhớ token đã mã hóa trước. Việc nén này làm cho các phương pháp bộ nhớ như LUMEN trở nên thực tế ngay cả đối với các kho dữ liệu truy xuất cực kỳ lớn. Các công trình trước đây (ví dụ: (Santhanam et al., 2022; Yang et al., 2022b; Cohen et al., 2022; Yang et al., 2022a)) đã tập trung vào nén biểu diễn token cho các mô hình xếp hạng lại tương tác muộn. Ngược lại, cách tiếp cận của chúng tôi nén các biểu diễn trung gian của mô hình ngôn ngữ. Các biểu diễn nén này được sử dụng làm đầu vào cho LLM, và các tham số của lớp nén được huấn luyện cùng với phần còn lại của mô hình.

2 Kiến thức nền tảng

Chúng tôi nhằm mục tiêu đạt được hiệu suất FiD và LUMEN về chất lượng trong khi giảm yêu cầu lưu trữ LUMEN. Đầu tiên chúng tôi mô tả FiD và LUMEN, các phương pháp mà MEMORY-VQ được xây dựng dựa trên, và yêu cầu lưu trữ của chúng. Để phân tích chuyên sâu, vui lòng xem de Jong et al. (2023a). Chúng tôi tiếp theo với kiến thức nền tảng về lượng tử hóa vector, bao gồm lượng tử hóa sản phẩm và VQ-VAE được sử dụng cho MEMORY-VQ.

2.1 Các mô hình tăng cường truy xuất và bộ nhớ

2.1.1 Fusion-in-Decoder

Fusion-in-Decoder (FiD) (Izacard và Grave, 2021) được xây dựng dựa trên mô hình mã hóa-giải mã T5 (Raffel et al., 2020). Nó truy xuất các đoạn văn bản liên quan, nối chúng vào đầu vào Q, và xử lý mỗi cặp đầu vào-đoạn văn với bộ mã hóa. Các biểu diễn token kết quả được hợp nhất và được chú ý bởi bộ giải mã. Chúng tôi đánh dấu các thành phần trực tiếp bằng màu xanh và đã tính toán trước bằng màu cam. FiD không có bất kỳ thành phần nào đã tính toán trước.

G=Dec[Enc(Q;Passage1);...Enc(Q;Passagek)]

Nhu cầu lưu trữ FiD thấp vì chúng ta chỉ cần lưu trữ ID token. Mỗi ID có thể được mã hóa với 16 bit, vì vậy chi phí lưu trữ cho kho dữ liệu truy xuất với N token là

S_FiD = 16·N

2.1.2 LUMEN

LUMEN (de Jong et al., 2023a) giảm chi phí suy luận bằng cách tính toán trước một phần các biểu diễn bộ mã hóa cho các đoạn văn được truy xuất. Thay vì truy xuất văn bản thực tế, LUMEN truy xuất các biểu diễn lớp trung gian trong quá trình suy luận.

LUMEN được khởi tạo từ mô hình mã hóa-giải mã T5 đã được huấn luyện trước, với bộ mã hóa bộ nhớ chứa tỷ lệ 1−α đầu tiên của các lớp và bộ mã hóa trực tiếp với tỷ lệ α còn lại của các lớp. Bộ mã hóa bộ nhớ được áp dụng ngoại tuyến để tính toán trước các biểu diễn bộ nhớ cho các đoạn văn trong kho dữ liệu. Sau đó, các biểu diễn này được cập nhật động với bộ mã hóa trực tiếp đã tinh chỉnh dựa trên đầu vào và tác vụ. Để đảm bảo tương thích, MEMORY-VQ áp dụng bộ mã hóa bộ nhớ cho đầu vào trước khi nối biểu diễn câu hỏi với biểu diễn bộ nhớ.

H_i = [MemEnc(Q); MemEnc(Passage_i)]
G = Dec[Q; LiveEnc(H1); ... LiveEnc(Hk)]

Chọn α=1 tạo ra mô hình rất gần với FiD trong khi α=0 là mô hình bộ nhớ đầy đủ. Một trong những hiểu biết của bài báo LUMEN là người ta có thể đạt được hiệu suất FiD trong khi sử dụng α nhỏ, giảm chi phí suy luận xuống một phần α của FLOP bộ mã hóa FiD cho bất kỳ kích thước mô hình nào.

LUMEN giữ các biểu diễn đầu ra MemEnc d chiều cho mỗi token. Với định dạng bfloat16, tổng chi phí lưu trữ trở thành

S_LUMEN = 16d·N

2.2 Lượng tử hóa vector

Lượng tử hóa vector (VQ) là một kỹ thuật nén cổ điển cho dữ liệu vector. Ý tưởng chung là chuẩn bị một tập hợp các vector được gọi là "mã" và sau đó biểu diễn mỗi vector đầu vào bằng mã gần nhất. Cách tiếp cận này giảm đáng kể yêu cầu lưu trữ vì chúng ta chỉ cần lưu trữ ID số nguyên của mã thay vì toàn bộ vector đầu vào có chiều cao. VQ là phương pháp nén mất mát vì giải nén trả về giá trị của mã gần nhất (bằng cách tra cứu ID) thay vì vector gốc. Thông thường, các mã được tạo ra bằng cách phân cụm các vector đầu vào, ví dụ, sử dụng các phương pháp giống kmeans.

2.2.1 Lượng tử hóa sản phẩm

Một biến thể phổ biến của lượng tử hóa vector là lượng tử hóa sản phẩm (Jégou et al., 2011; Ge et al., 2013). Phương pháp này liên quan đến việc phân chia các vector chiều cao thành các không gian con và lượng tử hóa độc lập mỗi không gian con bằng cách sử dụng một chương trình con lượng tử hóa vector. Lượng tử hóa sản phẩm thường được sử dụng trong các công cụ tìm kiếm láng giềng gần đúng hiện đại (Guo et al., 2020; Johnson et al., 2021) để tăng tốc tra cứu.

2.2.2 VQ-VAE

Cách tiếp cận VQ-VAE (van den Oord et al., 2017) là một biến thể của bộ tự mã hóa biến phân sử dụng lượng tử hóa vector để có được biểu diễn tiềm ẩn rời rạc. Đáng chú ý, lớp nén VQ-VAE cho phép huấn luyện chung với phần còn lại của mô hình do có bộ ước lượng thẳng cho lan truyền ngược gradient. Phương pháp này thường được sử dụng trong việc tạo biểu diễn rời rạc của các đối tượng liên tục như hình ảnh hoặc âm thanh (van den Oord et al., 2017; Razavi et al., 2019).

3 MEMORY-VQ

Chúng tôi đề xuất MEMORY-VQ, một phương pháp hiệu quả để giảm yêu cầu lưu trữ cho các mô hình dựa trên bộ nhớ. Ý tưởng cấp cao là nén bộ nhớ bằng các kỹ thuật lượng tử hóa vector và lưu trữ các mã số nguyên thay vì các vector bộ nhớ gốc. Các mã được giải nén thành vector ngay lập tức. Áp dụng phương pháp này cho LUMEN tạo ra mô hình LUMEN-VQ sau đây.

codes_i = CompressVQ(MemEnc(Passage_i))
H_i = [MemEnc(Q_i); DecompressVQ(codes_i)]
G = Dec[Q; LiveEnc(H1); ... LiveEnc(Hk)]

Để thực hiện CompressVQ và DecompressVQ, chúng tôi áp dụng lượng tử hóa sản phẩm, chia mỗi vector thành các không gian con và lượng tử hóa độc lập mỗi không gian con bằng VQ-VAE. Các mã là trung bình di chuyển theo cấp số nhân của các vector bộ nhớ được gán cho mã trong mỗi batch. Phụ lục A trong van den Oord et al. (2017) chứa mô tả chi tiết.

Để huấn luyện lớp nén cùng với mô hình, chúng tôi tuân theo công thức VQ-VAE, nhưng chúng tôi tránh sử dụng mất mát cam kết trong các thí nghiệm của mình vì nó dẫn đến phân kỳ mô hình.

Để khởi tạo các sách mã, chúng tôi sử dụng quy trình tương tự như khởi tạo kmeans++ (Arthur và Vassilvitskii, 2007). Ngoài ra, chúng tôi thực hiện đặt lại sách mã (Williams et al., 2020) sử dụng cùng quy trình để khởi tạo lại các mã ít được sử dụng.

Chúng tôi chia bộ nhớ thành g không gian con, và nếu cần, đệm bộ nhớ bằng số không để đảm bảo chia hết. Mỗi không gian con có C mã. Do đó yêu cầu lưu trữ cho mỗi vector lượng tử hóa là số không gian con nhân với số bit cần thiết để biểu diễn mỗi ID, là logarit của số mã.

S_LUMEN-VQ = g · ⌈log₂C⌉ · N

4 Thí nghiệm

Hình 1: LUMEN-VQ đạt được sự cân bằng được cải thiện mạnh mẽ giữa hiệu suất và nén. Biểu đồ hiển thị trung bình khớp chính xác trên các tập dev của các tác vụ KILT như một hàm của tỷ lệ nén. Chúng tôi so sánh LUMEN-VQ với các baseline Scale down (LUMEN XL và LUMEN Large) và LUMEN-Light (FiD-Light từ Hofstätter et al. (2022a) được điều chỉnh cho LUMEN).

Bảng 2: So sánh hiệu suất của các cách tiếp cận khác nhau để khởi tạo và huấn luyện LUMEN-VQ.

4.1 Thiết lập thí nghiệm

Cấu hình mô hình LUMEN-VQ và LUMEN được xây dựng trên kiến trúc T5.1.1 (Raffel et al., 2020) và được triển khai trong JAX sử dụng Flax (Heek et al., 2020) và Flaxformer. Tất cả các mô hình tinh chỉnh các checkpoint T5.1.1 XXL công khai. Chúng tôi huấn luyện FiD sử dụng công thức từ Izacard và Grave (2021).

Thiết lập huấn luyện cho LUMEN và LUMEN-VQ dựa trên de Jong et al. (2023b). Chúng tôi khởi tạo bộ mã hóa bộ nhớ với tỷ lệ 1 - α đầu tiên của các lớp từ bộ mã hóa T5 và bộ mã hóa trực tiếp với tỷ lệ α còn lại của các lớp, trong đó α là tỷ lệ đã cho của các lớp trực tiếp. Chúng tôi đặt α=1/3 trong các thí nghiệm chính của mình.

Chúng tôi huấn luyện và đánh giá trên một tập con của các bộ dữ liệu tác vụ chuyên sâu về kiến thức từ điểm chuẩn KILT (Petroni et al., 2021). Chúng tôi áp dụng quy trình truy xuất từ Hofstätter et al. (2022b) và sử dụng mô hình GTR-Base (Ni et al., 2021) làm bộ truy xuất. Xem Phụ lục A và de Jong et al. (2023b) để biết chi tiết.

4.2 Kết quả chính

Trong các thí nghiệm chính của chúng tôi, chúng tôi nén bộ nhớ 4096 chiều của LUMEN-XXL sử dụng g = 256 không gian con và C = 65536 mã mỗi không gian con, cho phép chúng tôi lưu trữ ID mã ở định dạng int16. Chúng tôi cần 512 byte để lưu trữ mỗi vector token thay vì 8192 byte cho bộ nhớ gốc. Kết quả là, LUMEN-VQ đạt được tỷ lệ nén 16 với mất mát hiệu suất tối thiểu, như được hiển thị trong Bảng 1.

4.3 Cân bằng chất lượng-tỷ lệ nén

Chúng tôi nghiên cứu sự cân bằng chất lượng-nén cho LUMEN-VQ bằng cách thay đổi số không gian con. Chúng tôi so sánh với một số baseline đơn giản; đầu tiên liên quan đến việc thu nhỏ mô hình (ví dụ: LUMEN-XL hoặc LUMEN-Large). Điều này giảm d từ 4096 xuống 2048 hoặc 1024, tương ứng. Baseline thứ hai, được gọi là LUMEN-Light, được lấy cảm hứng từ cách tiếp cận FiD-Light (Hofstätter et al., 2022a). Trong LUMEN-Light, chúng tôi giữ lại bộ nhớ của K token đầu tiên, thay đổi K từ 1/2 đến 1/4 độ dài đoạn văn, đạt được tỷ lệ nén 2 và 4.

Hình 1 trình bày kết quả hiệu suất. Cả hai baseline đều thể hiện mất mát hiệu suất đáng kể khi tỷ lệ nén tăng. Ngược lại, thước đo LUMEN-VQ cho thấy sự suy giảm hiệu suất dần dần, với mất mát khoảng 0,2 điểm hiệu suất ở tỷ lệ nén 16.

4.4 Ablation

Chúng tôi nghiên cứu xem việc khởi tạo huấn luyện VQ-VAE từ mô hình LUMEN đã tinh chỉnh có cho kết quả tốt hơn không. Kết quả trong Bảng 2 cho thấy việc tinh chỉnh LUMEN-VQ từ đầu đạt được hiệu suất tương tự như việc khởi tạo từ mô hình LUMEN đã tinh chỉnh.

Chúng tôi cũng phân tích thành phần mô hình nào hưởng lợi nhiều nhất từ việc tinh chỉnh chung với VQ-VAE. Đóng băng bộ mã hóa bộ nhớ trong quá trình huấn luyện chung, bắt đầu với mô hình LUMEN đã tinh chỉnh, có ít tác động đến hiệu suất. Tuy nhiên, chỉ cập nhật mã VQ-VAE trong khi đóng băng toàn bộ mô hình dẫn đến giảm hiệu suất, cho thấy nhu cầu của mô hình phải thích ứng với lỗi lớp giải nén.

5 Công trình liên quan

Mô hình bộ nhớ Tăng cường truy xuất có thể tốn kém về mặt tính toán do bối cảnh bổ sung mà các mô hình ngôn ngữ cần xử lý. Để giảm thiểu điều này, các mô hình bộ nhớ như LUMEN (de Jong et al., 2023a), GLIMMER (de Jong et al., 2023b), và các mô hình khác (Zemlyanskiy et al., 2021; de Jong et al., 2022; Wu et al., 2022a; Li et al., 2022; Zhong et al., 2022; Chen et al., 2022; Wu et al., 2022b; Bertsch et al., 2023; Milbauer et al., 2023) lưu trữ các biểu diễn đã tính toán trước trong bộ nhớ. MEMORY-VQ tập trung vào việc cải thiện yêu cầu lưu trữ cho các mô hình dựa trên bộ nhớ. Trong khi các thí nghiệm của chúng tôi liên quan đến mô hình LUMEN (de Jong et al., 2023a) do hiệu suất mạnh mẽ của nó, phương pháp này áp dụng cho một loạt các mô hình rộng hơn.

Nén cho xếp hạng lại tương tác muộn MEMORY-VQ tập trung vào nén cho các mô hình bộ nhớ tương tác muộn, trong khi các công trình khác đã khám phá nén cho xếp hạng lại tương tác muộn. Ví dụ, SDR (Cohen et al., 2022) sử dụng bộ tự mã hóa để giảm chiều biểu diễn token, theo sau là lượng tử hóa sản phẩm. BECR (Yang et al., 2022a) sử dụng băm nhạy cảm với vị trí để nén biểu diễn token. CQ (Yang et al., 2022b) học các tham số lượng tử hóa vector bằng cách coi các mã như trọng số có thể học và sử dụng Gumbel-Softmax để xác định mã gần nhất có thể vi phân. Cuối cùng, ColBERTv2 (Santhanam et al., 2022) đề xuất một sơ đồ nén tùy chỉnh kết hợp PQ và lượng tử hóa số nguyên để xử lý phần dư tái tạo.

6 Kết luận

Chúng tôi giới thiệu MEMORY-VQ, một cách tiếp cận mới để giảm yêu cầu lưu trữ của các mô hình ngôn ngữ tăng cường bộ nhớ mà không làm giảm hiệu suất. Bằng cách sử dụng VQ-VAE để nén các biểu diễn token, chúng tôi có được mô hình LUMEN với nén 16x, được ký hiệu là LUMEN-VQ. Đáng chú ý, LUMEN-VQ duy trì hiệu suất gần với LUMEN và FiD và hưởng lợi từ việc tăng tốc suy luận LUMEN với chi phí lưu trữ giảm mạnh. Sử dụng MEMORY-VQ, tăng cường bộ nhớ là một giải pháp thực tế để tăng tốc suy luận mạnh mẽ với các kho dữ liệu truy xuất rộng lớn.
