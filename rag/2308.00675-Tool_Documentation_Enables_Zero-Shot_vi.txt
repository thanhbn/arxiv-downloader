# 2308.00675.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2308.00675.pdf
# Kích thước tệp: 4716678 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Tài liệu công cụ cho phép sử dụng công cụ Zero-Shot
với các mô hình ngôn ngữ lớn
Cheng-Yu Hsieh1†, Si-An Chen2†, Chun-Liang Li3, Yasuhisa Fujii4,
Alexander Ratner1, Chen-Yu Lee3, Ranjay Krishna1∗, Tomas Pfister3∗
1Đại học Washington,2Đại học Quốc gia Đài Loan,
3Google Cloud AI Research,4Google Research
cydhsieh@cs.washington.edu
Tóm tắt
Ngày nay, các mô hình ngôn ngữ lớn (LLM) được dạy sử dụng các công cụ mới bằng cách cung cấp
một số ít ví dụ minh họa về cách sử dụng công cụ. Thật không may, các ví dụ minh họa khó có được,
và có thể dẫn đến việc sử dụng thiên lệch không mong muốn nếu chọn sai ví dụ minh họa. Ngay cả
trong trường hợp hiếm khi có sẵn các ví dụ minh họa, cũng không có giao thức lựa chọn có nguyên tắc
để xác định nên cung cấp bao nhiêu và những ví dụ nào. Khi các nhiệm vụ trở nên phức tạp hơn, việc
tìm kiếm lựa chọn tăng theo cấp số nhân và luôn luôn trở nên khó giải quyết. Công trình của chúng tôi
cung cấp một giải pháp thay thế cho các ví dụ minh họa: tài liệu công cụ. Chúng tôi ủng hộ việc sử dụng
tài liệu công cụ—mô tả cho việc sử dụng từng công cụ cá nhân—thay vì các ví dụ minh họa. Chúng tôi
chứng minh nhận định của mình thông qua ba phát hiện thực nghiệm chính trên 6 nhiệm vụ qua cả hai
phương thức thị giác và ngôn ngữ. Thứ nhất, trên các điểm chuẩn hiện có, các prompt zero-shot chỉ với
tài liệu công cụ đã đủ để khơi gợi việc sử dụng công cụ phù hợp, đạt hiệu suất ngang bằng với các
prompt few-shot. Thứ hai, trên một bộ dữ liệu sử dụng công cụ thực tế mới được thu thập với hàng
trăm API công cụ có sẵn, chúng tôi cho thấy tài liệu công cụ có giá trị đáng kể hơn các ví dụ minh họa,
với tài liệu zero-shot vượt trội đáng kể so với few-shot không có tài liệu. Thứ ba, chúng tôi làm nổi bật
những lợi ích của tài liệu công cụ bằng cách giải quyết việc tạo ảnh và theo dõi video bằng cách sử dụng
các mô hình tiên tiến mới phát hành chưa từng thấy làm công cụ. Cuối cùng, chúng tôi làm nổi bật khả năng
sử dụng tài liệu công cụ để tự động kích hoạt các ứng dụng mới: bằng cách chỉ sử dụng tài liệu của
GroundingDino, Stable Diffusion, XMem và SAM, LLM có thể tái phát minh các chức năng của các mô hình
Grounded-SAM [23] và Track Anything [70] vừa được phát hành.

1 Giới thiệu
Ngày nay, các mô hình ngôn ngữ lớn (LLM) gợi lên hình ảnh của một thợ thủ công: khi được yêu cầu
giải quyết một nhiệm vụ phức tạp, chúng phân tích nhiệm vụ thành các nhiệm vụ con đơn giản hơn và
lắp ráp các công cụ tốt nhất có thể để giải quyết từng nhiệm vụ con [51,72]. Ví dụ, xem xét nhiệm vụ
phức tạp về trả lời câu hỏi cho trước hình ảnh trong Hình 1. Để trả lời "liệu hai nam châm sẽ hút hay
đẩy nhau", LLM cần những điều sau: nó cần xác định vị trí của các nam châm trong hình ảnh, trích xuất
kiến thức tổng quát giải thích rằng "các cực đối nghịch (giống nhau) hút (đẩy)". Giống như một thợ thủ
công có năng lực biết các công cụ của họ có khả năng gì, một LLM với kiến thức như vậy về các công cụ
của mình sẽ có thể gọi một công cụ (ví dụ: Trình phát hiện văn bản) để xác định các cực bắc và nam và
một công cụ thứ hai (ví dụ: Trình truy xuất kiến thức) để trích xuất kiến thức nền liên quan về lực từ.
Nhưng làm thế nào một LLM biết công cụ nào có khả năng gì?

†Công việc được thực hiện với tư cách là các nhà nghiên cứu sinh viên tại Google Cloud AI Research.
*Các tác giả đóng góp ngang bằng cho công trình này.
Preprint. Under review.arXiv:2308.00675v1 [cs.CL] 1 Aug 2023

--- TRANG 2 ---
Hình 1: Ví dụ quy trình sử dụng công cụ với LLM để giải quyết một nhiệm vụ trả lời câu hỏi đa phương thức.
Cho trước câu hỏi đầu vào với một hình ảnh, LLM chọn các công cụ phù hợp từ bộ công cụ và tạo ra một
kế hoạch thực thi để trả lời câu hỏi một cách chính xác. Ở đây, LLM phác thảo một kế hoạch để đầu tiên
sử dụng Trình phát hiện văn bản để hiểu vị trí của các nam châm trong hình ảnh, sau đó tận dụng Trình
truy xuất kiến thức để thu được kiến thức nền liên quan về nam châm, rồi cuối cùng tạo ra giải pháp dựa
trên các bước trước đó.

Hiện tại, việc sử dụng công cụ LLM cung cấp cho LLM các ví dụ minh họa few-shot (demo) về những gì
các công cụ của nó có thể làm, hy vọng rằng những demo này sẽ giúp tổng quát hóa hành vi của mô hình
cho các nhiệm vụ phức tạp mới hơn. Quá trình này đã khá thành công cho đến nay. Những demo few-shot
này chứa một hoặc nhiều mẫu về ánh xạ <đầu vào, đầu ra> [68] trên các hướng dẫn đã cho và các kế hoạch
sử dụng công cụ tương ứng của chúng (được minh họa trong Hình 2). LLM được kỳ vọng sẽ tìm ra các
mẫu trong những demo này và tổng quát hóa chúng cho các nhiệm vụ mới. Đối với các nhiệm vụ văn bản,
LLM đã được trình bày với các demo về máy tính [15,47,56], trình thông dịch Python [13,18] và công cụ
tìm kiếm [62,43,50,56,40] có thể thực hiện các phép toán logic và số học để có được kiến thức chính xác
và thực tế hơn. Đối với các nhiệm vụ thị giác, LLM với các demo của các mô hình thị giác được huấn luyện
trước có thể thực hiện lý luận thị giác phức tạp [37,40,57,16,73], có thể tạo ra và thậm chí chỉnh sửa hình ảnh
[19,9]. Đối với các nhiệm vụ robot được thể hiện, LLM tương tự có thể được sử dụng để lý luận và lập kế
hoạch [75,21,1,17].

Chúng tôi cho rằng sự phụ thuộc vào demo trong việc sử dụng công cụ là không cần thiết trong một số
trường hợp, và thậm chí có thể là hạn chế. Thực tế, nghiên cứu gần đây phát hiện rằng LLM có xu hướng
nhạy cảm với demo [81], và cần phải lựa chọn demo cẩn thận để tránh thiên lệch hoặc overfitting với một
cách sử dụng cụ thể [12]. Điều này dẫn đến câu hỏi tiếp theo: làm thế nào chúng ta chọn những demo
few-shot nào để sử dụng? Không có phương pháp có nguyên tắc nào được biết đến để lựa chọn demo mà
không cần can thiệp của con người hoặc thậm chí để con người có thể lựa chọn hoặc tạo ra chúng một
cách hiệu quả. Để làm cho vấn đề tồi tệ hơn, khi chúng ta mở rộng quy mô số lượng công cụ mà LLM có
quyền truy cập, quá trình lựa chọn few-shot này trở nên khó giải quyết theo cấp số nhân. Giống như một
thợ thủ công không cần phải thấy một công cụ mới được minh họa và thay vào đó có thể nhận biết khả năng
của chúng từ việc đọc hướng dẫn sử dụng cho công cụ, chúng tôi tìm cách cho phép LLM học cách sử dụng
công cụ mà không cần thấy bất kỳ demo nào.

Công trình của chúng tôi cung cấp một giải pháp thay thế cho các ví dụ minh họa: tài liệu công cụ (doc).
Tương tự như ẩn dụ về một hướng dẫn chỉ ra khả năng của một công cụ vật lý, tài liệu của một công cụ
phần mềm phác thảo những gì công cụ có thể và không thể được sử dụng cho và cách gọi nó. Tài liệu
cung cấp hướng dẫn tương đối trung tính về các chức năng của công cụ và cách các công cụ cá nhân nên
được sử dụng (được minh họa trong Hình 2), và chúng thường có sẵn một cách thuận tiện thông qua việc
tạo ra các công cụ một cách tự nhiên. Trực quan, giống như thợ thủ công dựa vào việc sử dụng một công
cụ mới bằng cách đọc hướng dẫn, chúng tôi cung cấp cho LLM các tệp README khi gặp một công cụ/kho
lưu trữ mới. Với tài liệu, một LLM có thể không nhất thiết cần demo để sử dụng một công cụ mới.

Khác biệt với công trình hiện có chủ yếu dựa vào demo few-shot cho việc học công cụ, trong công trình
này, chúng tôi nghiên cứu liệu LLM có thể thay vào đó chỉ dựa vào tài liệu để sử dụng công cụ. Chúng
tôi nghiên cứu hiệu suất học công cụ của LLM khi chúng tôi bao gồm hoặc loại trừ tài liệu, và thay đổi
số lượng demo từ few-shot xuống zero-shot. Chúng tôi tiến hành các thử nghiệm trên 6 nhiệm vụ qua
các phương thức thị giác và văn bản. Các thử nghiệm của chúng tôi cho thấy rằng:

•Đáng ngạc nhiên, khi được cung cấp tài liệu công cụ, hiệu suất sử dụng công cụ zero-shot của LLM
ngang bằng hoặc thậm chí tốt hơn so với các đối tác few-shot của chúng, cho thấy rằng việc bao gồm
tài liệu là một cách hiệu quả để tránh được các demo few-shot cần thiết.

--- TRANG 3 ---
Ví dụ minh họa: Dưới đây là các ví dụ ánh xạ một vấn đề tới một kế hoạch sử dụng công cụ.
●Câu hỏi: Ba vật thể này có tính chất chung nào?
Kế hoạch sử dụng công cụ:
Trình phát hiện văn bản → Trình truy xuất kiến thức → Trình tạo giải pháp
●Câu hỏi: Lọ này được làm từ chất liệu gì?
Kế hoạch sử dụng công cụ:
Trình tạo chú thích hình ảnh → Trình tạo giải pháp
[DEMO Chips Pretzel Fries] Ví dụ minh họa

Mô tả: ví dụ về câu hỏi và kế hoạch sử dụng công cụ.
●Câu hỏi:
Những vật thể này có tính chất chung nào?
Kế hoạch sử dụng công cụ:
Trình phát hiện văn bản → Trình truy xuất kiến thức → Trình tạo giải pháp
●Câu hỏi: [...]
Kế hoạch sử dụng công cụ: [...]
●Câu hỏi: [...]
Kế hoạch sử dụng công cụ: [...]

Tài liệu
Mô tả: các công cụ có sẵn và chức năng của chúng.
●Trình phát hiện văn bản:
Nó phát hiện văn bản trong một hình ảnh [...]
●Trình truy xuất kiến thức:
Nó truy xuất kiến thức liên quan [...]
●Công cụ tìm kiếm:
Nó tìm kiếm trên web thông tin liên quan [...]
●Trình tạo chú thích hình ảnh:
Nó tạo ra chú thích cho một hình ảnh [...]
●...

Hình 2: Hai loại kiến thức để prompt LLM cho việc sử dụng công cụ: Ví dụ minh họa (demo) và
Tài liệu (docs). Demo bao gồm các cặp <đầu vào, đầu ra> trên các hướng dẫn đầu vào và các kế hoạch
sử dụng công cụ đầu ra tương ứng. Chúng đòi hỏi nỗ lực thủ công để tuyển chọn cẩn thận cho mỗi
nhiệm vụ mới, và hiệu suất mô hình có thể nhạy cảm với việc sử dụng demo nào [81,12]. Nhiều demo
cũng có thể cần thiết để có phạm vi bao phủ tốt khi số lượng công cụ tăng lên. Mặt khác, tài liệu cung
cấp mô tả cho chức năng công cụ, và thường có sẵn một cách tự nhiên cho các công cụ.

•Dựa trên phát hiện trên, chúng tôi nới lỏng ràng buộc demo few-shot, và cho thấy rằng chúng ta có thể
mở rộng quy mô hiệu quả lên một bộ công cụ lớn hơn đáng kể, trên một bộ dữ liệu sử dụng API mới
được thu thập, bằng cách đơn giản cung cấp cho LLM tài liệu.

•Chúng tôi cho thấy cách thêm các công cụ mới một cách liền mạch cùng với tài liệu của chúng vào
một bộ công cụ cho LLM để giải quyết các nhiệm vụ chưa thấy về chỉnh sửa hình ảnh và theo dõi video,
tất cả mà không cần bất kỳ demo nào trong một cách plug-and-play.

•Cuối cùng, với các công cụ chưa thấy được phát triển gần đây làm khối xây dựng, chúng tôi trình diễn
LLM có khả năng tái phát minh các công trình phổ biến nhưng thậm chí còn gần đây hơn Grounded-SAM
[23] và Track Anything [70], điều này gợi ý một tiềm năng từ việc sử dụng công cụ zero-shot đến khám
phá kiến thức tự động.

2 Công trình liên quan

LLM với tăng cường truy xuất và công cụ. Bất chấp những thành tựu đáng chú ý được thể hiện bởi LLM,
hiệu suất có thể được cải thiện thêm với việc sử dụng công cụ bên ngoài để trở nên chính xác hơn, hiệu
quả hơn hoặc linh hoạt hơn cho các ứng dụng rộng hơn. Các tác giả trong [51] đã chi tiết về nguồn gốc
nhận thức, sự thay đổi mô hình của các mô hình nền tảng, và vai trò bổ sung của công cụ và mô hình đối
với LLM. Ví dụ về việc sử dụng công cụ bắt đầu từ truy xuất kiến thức [6,20,33,74,77] và mở rộng đến
công cụ tìm kiếm [43,31,32,62,58,46,40], hệ thống QA [56], máy tính [15,47,56], trình thông dịch Python
[18,13,65,24,46,16], công cụ mô phỏng [37], mô hình học máy [57,73,69,40,16], hoặc thậm chí các công cụ
được tạo bởi LLM [11]. Các công trình tiên phong của LLM với công cụ thường dựa vào sự giám sát của
con người [62,31] hoặc các kỹ thuật học tự giám sát bổ sung [56], điều này đặt ra thách thức cho việc sử
dụng plug-and-play thực tế. Những tiến bộ gần đây loại bỏ việc huấn luyện bổ sung bằng cách sử dụng
các demo ví dụ trong prompt [19,75,73,57,40,46]. Công trình của chúng tôi đơn giản hóa thêm thiết kế
prompt bằng cách chỉ tận dụng tài liệu cho các công cụ cá nhân, trong khi duy trì hiệu suất cạnh tranh.

Lập kế hoạch với LLM. Các mô hình ngôn ngữ được chứng minh là có tiềm năng tiến hành lập kế hoạch
để giải quyết các nhiệm vụ phức tạp hoặc phân tích các nhiệm vụ phức tạp thành các vấn đề con khi được
prompt đúng cách. [21,22] truy xuất demo tại thời điểm kiểm tra với phạm vi bao phủ không gian kiến
thức lớn để tạo ra các hành động có thể chấp nhận. [28] dựa vào các demo được thiết kế sẵn cho việc
phân tích nhiệm vụ. Tương tự, các công trình gần đây về sử dụng công cụ với LLM tận dụng các ví dụ
minh họa về giải quyết các nhiệm vụ ví dụ với một kế hoạch công cụ [13,19,75,73,57,40,46]. Tuy nhiên,
việc tạo ra demo về tương tác giữa các công cụ có thể thách thức trong thực tế khi số lượng công cụ tăng
vọt. Công trình đồng thời [48,52,71] giải quyết thách thức bằng cách sử dụng LLM mạnh như GPT-4 [45]
để tạo ra các bộ dữ liệu tuân theo hướng dẫn lớn bao gồm các hướng dẫn đa dạng và các kế hoạch sử dụng
công cụ tương ứng, thường thông qua các cơ chế như self-instruct [66]. Các bộ dữ liệu kết quả sau đó có
thể được sử dụng để tinh chỉnh và trang bị cho các LLM khác (ví dụ: LLaMA [63] và OPT [79]) khả năng
sử dụng một bộ sưu tập công cụ lớn cho các hướng dẫn chưa thấy. Mặt khác, công trình của chúng tôi
trình diễn tiềm năng cho LLM để sử dụng bất kỳ công cụ mới chưa thấy nào bằng cách đọc tài liệu công
cụ của chúng.

Ví dụ minh họa và Tài liệu. Học từ ví dụ minh họa phổ biến trong học tăng cường [49,4,44,55]. [8] đề
xuất thuật toán học trong ngữ cảnh để thích ứng nhiệm vụ downstream hiệu quả và hiệu lực thông qua
việc hiển thị các ví dụ minh họa. Lấy cảm hứng từ thành công, hầu hết các công trình sử dụng công cụ
LLM hiện có đều dựa vào ví dụ minh họa few-shot [13,19,75,73,57,40,46]. Tuy nhiên, [12] cho thấy rằng
có nhiều ví dụ minh họa hơn có thể phản trực giác làm giảm hiệu suất, và có thể cần một sự lựa chọn
cẩn thận. [35] đề xuất một phương pháp truy xuất để lựa chọn demo, ngầm đòi hỏi một tập hợp ví dụ
lớn hơn để được chọn. Sử dụng tài liệu để cải thiện thuật toán tương đối ít được khám phá. [7,82] đề xuất
các thuật toán đọc tài liệu cho các trò chơi cụ thể. [83] giới thiệu DocPrompting, sử dụng một trình truy
xuất được huấn luyện trên dữ liệu huấn luyện đã cho để tăng cường việc tạo mã bằng cách truy xuất các
tài liệu liên quan. Trong công trình này, chúng tôi tiến một bước về việc khám phá lập kế hoạch công cụ
zero-shot trong LLM chỉ với sự hỗ trợ của tài liệu, và điều tra một loạt các nhiệm vụ đa dạng từ ngôn ngữ
đến các lĩnh vực thị giác. Trong khi [64,42] trình diễn khả năng lập kế hoạch zero-shot thuần túy của LLM,
chúng không nghiên cứu việc sử dụng công cụ hoặc các tình huống chưa thấy đối với các mô hình ngôn
ngữ. ViperGPT [16] là một công trình đồng thời, tập trung vào lập trình thị giác trong Python và sử dụng
các triển khai hàm và đặc tả làm tài liệu. Cuối cùng, trong khi AutoGPT [3] cung cấp nhiều demo trình
diễn khả năng sử dụng công cụ của LLM thông qua việc đọc tài liệu, nghiên cứu của chúng tôi tập trung
vào một khám phá có hệ thống từ các trường hợp sử dụng thực tế đến các điểm chuẩn học thuật.

3 Thiết lập thử nghiệm

3.1 Quy trình chung

Chúng tôi tuân theo khung chung của việc sử dụng công cụ với LLM trong [51], bao gồm nhiều công trình
gần đây [75,27,19,57,73,69,40]. Cụ thể, cho một hướng dẫn ngôn ngữ tự nhiên, một bộ lập kế hoạch LLM
tạo ra một chương trình để được thực thi tuần tự trong đó mỗi bước của chương trình có thể dựa vào việc
sử dụng các công cụ được chọn từ một bộ công cụ. Sau khi chương trình được tạo ra, nó sau đó được thực
thi bởi một môi trường cuối cùng trả về kết quả thực thi. Ở đây, chương trình mở rộng vượt ra ngoài thực
hành mã hóa thông thường [76,53,25] và liên quan chặt chẽ hơn với lý thuyết automata [59]: một tập hợp
các hướng dẫn về tự động hóa (ví dụ: công cụ trong trường hợp của chúng tôi). Do đó, bộ công cụ có thể
là các thư viện với các ngôn ngữ lập trình cụ thể (ví dụ: Python), hoặc tính toán chung với đầu vào-đầu ra
được định nghĩa đúng cách, chẳng hạn như các mô hình được huấn luyện, lời gọi API, và hơn thế nữa.

3.2 Phương pháp prompting sử dụng công cụ

Như đã thảo luận trong Mục 1, hai loại thông tin chính được xem xét trong việc prompting LLM cho
các kế hoạch sử dụng công cụ: ví dụ minh họa (demo) và tài liệu (docs). Demo trình diễn cách tương tác
công cụ có thể hoàn thành các nhiệm vụ cụ thể, trong khi docs mô tả các chức năng công cụ cá nhân mà
không có mối liên kết cụ thể với nhiệm vụ như được hiển thị trong Hình 2. Trong thử nghiệm, chúng tôi
khám phá các kết hợp bao gồm/loại trừ docs và demo trong prompt, cũng như thay đổi số lượng demo.

3.3 Nhiệm vụ đánh giá

Chúng tôi tiến hành các thử nghiệm trên 6 nhiệm vụ qua nhiều phương thức với nhiều bộ công cụ khác
nhau. Chúng tôi mô tả thiết lập và các bộ công cụ cho mỗi nhiệm vụ dưới đây. Ngoại trừ các trường hợp
cụ thể được chỉ định rõ ràng, bộ lập kế hoạch LLM là ChatGPT (gpt-3.5-turbo).

Trả lời câu hỏi đa phương thức trên ScienceQA. ScienceQA [39] bao gồm các câu hỏi khoa học trắc nghiệm
đa phương thức đòi hỏi hiểu biết ngôn ngữ và thị giác cũng như kiến thức chuyên ngành để trả lời chính
xác. Trên ScienceQA, chúng tôi tuân theo thiết lập được sử dụng trong Chameleon [40] và sử dụng cùng
bộ công cụ với 7 công cụ, chẳng hạn như công cụ tìm kiếm và trình phát hiện văn bản hình ảnh.

Lý luận toán học dạng bảng trên TabMWP. TabMWP [41] là một bộ dữ liệu lý luận toán học với các hình
thức bảng khác nhau. Nó đòi hỏi một mô hình hiểu các bảng có cấu trúc hoặc chuyên ngành, và sử dụng

--- TRANG 4 ---
Zero Shot
●llmcloud firewall allow my_vm --port 
8000 --protocol tcp [Ảo giác]
●touch my_file
●scp -P 8000 /path/to/my_file 
user@server_ip:~ [Lệnh sai]
● [Thiếu tạo chủ đề]
●llmcloud publish-message --project 
PROJ --message "Hi" [Ảo giác]

Tài liệu
●llmcloud comp firewall-rules create 
NAME --allow tcp:8000
●touch my_file
●llmcloud comp scp --port 8000 
my_file my_vm:./
●llmcloud pubsub topics create TOPIC
●llmcloud pubsub topics publish PROJ 
--message "hi"

Few Shot
●llmcloud comp firewall-rules 
create NAME --allow tcp:8000
●touch my_file
●llmcloud comp scp --P 8000 
my_file my_vm:./ [Cờ sai]
● [Thiếu tạo chủ đề]
●llmcloud pubsub topics publish 
PROJ my-topic --message "Hi"

Câu hỏi:
● Đây là một dịch vụ đám mây mới gọi là LLM VM, cung cấp
công cụ CLI SDK riêng (llmcloud).
● Tạo một tường lửa cho phép cổng 8000. Vui lòng chạm vào một tệp
my_file rồi sao chép nó đến máy chủ với cổng 8000.
● Cuối cùng, vui lòng xuất bản một tin nhắn "Hi" đến dự án.

Đáp án (trong GCP)
●gcloud compute firewall-rules create NAME 
--allow tcp:8000
●touch my_file
●gcloud compute scp --port 8000 my_file my_vm:./
●gcloud pubsub topics create TOPIC
●gcloud pubsub topics publish PROJ --message "hi"

Hình 3: Bộ công cụ dòng lệnh LLM Cloud Platform mới, là một bộ công cụ chưa thấy đối với LLM hiện
có dựa trên các công cụ dòng lệnh Google Cloud thực tế thông qua việc đổi tên.

thông tin để trả lời các câu hỏi toán học tương ứng. Trên TabMWP, chúng tôi cũng tuân theo Chameleon
[40] với cùng bộ công cụ với 9 công cụ, chẳng hạn như trình tạo chương trình và tra cứu cột.

Lý luận đa phương thức trên NLVRv2. NLVRv2 [60] đòi hỏi mô hình xác minh liệu một tuyên bố có đúng
trên một cặp hình ảnh, đòi hỏi hiểu biết tổng hợp về cả văn bản và hình ảnh. Trên NLVRv2, chúng tôi
tuân theo thiết lập được sử dụng trong Visual Programming (VisProg) [19] với 20 mô-đun thị giác (công
cụ) để hiểu và thao tác hình ảnh. Vì VisProg chỉ dựa vào các ví dụ minh họa few-shot và không sử dụng
tài liệu cho các mô-đun. Chúng tôi tạo ra tài liệu cho mỗi mô-đun bằng cách bao gồm mô tả về chức năng
của mô-đun và chữ ký hàm. Chúng tôi cung cấp tài liệu đầy đủ mà chúng tôi sử dụng cho mỗi mô-đun
trong phụ lục.

Sử dụng API chưa thấy trên một bộ dữ liệu mới được thu thập. Các điểm chuẩn hiện có được sử dụng
trong tài liệu đi kèm với một tập hợp công cụ hạn chế. Để khám phá các trường hợp sử dụng thực tế liên
quan đến một số lượng lớn công cụ, chúng tôi thu thập một điểm chuẩn mới gọi là LLM Cloud CLI bao
gồm 200 lệnh đại diện cho các chức năng của giao diện dòng lệnh Google Cloud Platform (GCP). Mỗi
lệnh trong CLI của chúng tôi được đổi tên từ lệnh GCP tương ứng, bảo tồn ngữ nghĩa và logic của các
công cụ gốc, trong khi chưa thấy đối với các mô hình ngôn ngữ. Ví dụ, lệnh gcloud compute create NAME,
chịu trách nhiệm tạo một máy ảo, được đổi tên thành llmvm compute make NAME. Các quy ước đổi tên
cũng cho phép chúng tôi sử dụng các ví dụ GCP xác thực làm demo few-shot và tận dụng tài liệu GCP
tương ứng. Điểm chuẩn bao gồm 50 câu hỏi, mỗi câu tập trung vào việc tạo và cấu hình các dịch vụ đám
mây cụ thể bằng các công cụ dòng lệnh. Mỗi câu hỏi đòi hỏi ít nhất hai lệnh để hoàn thành nhiệm vụ.
Chúng tôi hiển thị một ví dụ trong Hình 3, và bao gồm thêm trong phụ lục.

Do các ràng buộc về độ dài của LLM mà chúng tôi sử dụng, chúng tôi không thể đưa tài liệu của 200
công cụ vào một prompt duy nhất. Do đó, chúng tôi sử dụng một tìm kiếm TF-IDF đơn giản sử dụng các
câu hỏi làm truy vấn để truy xuất các tài liệu liên quan nhất và cắt ngắn chúng để vừa với độ dài prompt.
Thêm chi tiết có thể được tìm thấy trong phụ lục.

Chỉnh sửa hình ảnh bằng ngôn ngữ tự nhiên. Chúng tôi xem xét chỉnh sửa hình ảnh như một hình thức
đánh giá định tính. Quá trình này yêu cầu mô hình lập kế hoạch và sử dụng các mô-đun thị giác khác
nhau để xử lý các hướng dẫn ngôn ngữ tự nhiên phức tạp. Ví dụ, để thực hiện một hướng dẫn như "thay
thế xe buýt màu đỏ bằng xe đạp màu xanh lá cây", mô hình phải định vị xe buýt màu đỏ, tạo ra mặt nạ
phân đoạn của nó, và sau đó inpaint khu vực được che. Chúng tôi sử dụng các bộ công cụ từ VisProg.
Không giống như VisProg, phụ thuộc vào các ví dụ minh họa few-shot, mô hình của chúng tôi chỉ xem
tài liệu mô-đun. Chúng tôi tiếp tục bao gồm các công trình hiểu hình ảnh được phát hành gần đây, Segment
Anything (SAM) [30] và Grouding DINO [38] để mở rộng bộ công cụ để kiểm tra khả năng zero-shot trên
các công cụ mới và chưa thấy theo cách plug-and-play.

Theo dõi video. Theo dõi video cũng được sử dụng trong nghiên cứu này như một đánh giá định tính.
Nhiệm vụ này nhằm thu được các mặt nạ của một đối tượng được theo dõi trong mỗi khung hình của
video, đòi hỏi triển khai các quá trình như định vị đối tượng, phân đoạn và theo dõi. Ngoài SAM và
Groudning DINO, chúng tôi kết hợp tài liệu của một mô-đun theo dõi đối tượng chưa thấy, Xmen [14]
vào khung VisProg với mục tiêu trình diễn khả năng của mô hình để thích ứng và sử dụng các công cụ
mới mà không cần các ví dụ minh họa rõ ràng một lần nữa trên một nhiệm vụ khác.

--- TRANG 5 ---
Hình 4: Hiệu suất sử dụng công cụ với gpt-3.5-turbo trên các điểm chuẩn khác nhau, bao gồm từ
phương thức ngôn ngữ đến thị giác. Chúng tôi báo cáo kết quả với và không có tài liệu (doc) và ví dụ
minh họa (demo), và các kết hợp của chúng. Rõ ràng, chỉ với tài liệu (điểm màu xanh phía trên-trái)
cho thấy hiệu suất cạnh tranh trên tất cả các bộ dữ liệu.

4 Các phát hiện thực nghiệm

Chúng tôi trình diễn tầm quan trọng của tài liệu công cụ theo ba khía cạnh: Đầu tiên, chúng tôi cho thấy
rằng tài liệu công cụ làm giảm nhu cầu về ví dụ minh họa (Mục 4.1). Thứ hai, dựa trên phát hiện này,
chúng tôi tiếp tục cho thấy rằng việc dựa vào tài liệu thay vì ví dụ minh họa cung cấp một giải pháp có
thể mở rộng hơn để trang bị LLM với một số lượng lớn công cụ có sẵn (Mục 4.2). Cuối cùng, chúng tôi
cho thấy rằng chỉ với tài liệu công cụ, LLM có thể hiểu và sử dụng các mô hình thị giác gần đây nhất để
đạt được kết quả ấn tượng trong các nhiệm vụ chỉnh sửa hình ảnh và theo dõi video, trên đó các kết quả
hiện có được đạt được bằng demo được tạo thủ công hoặc quy trình được định nghĩa trước (Mục 4.3).

4.1 Tài liệu làm giảm nhu cầu về ví dụ minh họa

Trong mục này, chúng tôi cho thấy cách tài liệu công cụ làm giảm nhu cầu về ví dụ minh họa. Chúng tôi
trình bày các phát hiện trên ba bộ dữ liệu: ScienceQA, TabMWP và NLVRv2. Chúng tôi đánh giá hiệu
suất mô hình, có và không có tài liệu công cụ, qua số lượng ví dụ minh họa (demo) khác nhau trên mỗi
bộ dữ liệu.

Trong Hình 4, chúng ta thấy rằng khi được cung cấp tài liệu công cụ, mô hình có thể duy trì hiệu suất
ổn định khi chúng ta bỏ đi số lượng demo được sử dụng. Thực tế, không sử dụng bất kỳ demo nào (tức
là 0-shot), mô hình có thể đạt được hiệu suất ngang bằng với việc sử dụng 16-shot trên TabMWP, và sử
dụng 12-shot trên NLVRv2. Trên ScienceQA, mô hình thậm chí có thể đạt được hiệu suất tốt hơn chỉ với
tài liệu so với việc sử dụng thêm demo 10-shot. Mặt khác, không có tài liệu công cụ, hiệu suất mô hình
rất nhạy cảm với số lượng demo được sử dụng. Khi chúng ta giảm số lượng demo, chúng ta thấy sự sụt
giảm hiệu suất đáng kể trên cả ba bộ dữ liệu. Điều này làm nổi bật tầm quan trọng của tài liệu công cụ
và cho thấy rằng nó cung cấp một cách hiệu quả để giảm sự phụ thuộc vào demo.

Bằng cách làm giảm nhu cầu về demo, chúng ta có thể giảm bớt những nỗ lực cần thiết để tuyển chọn
cẩn thận những demo này. Ví dụ, phù hợp với các nghiên cứu gần đây [81,12], chúng tôi quan sát trong
Hình 4 rằng hiệu suất mô hình nhạy cảm với việc sử dụng demo nào, được thể hiện bởi độ biến thiên hiệu
suất lớn dưới 5-shot trên ScienceQA và 2-shot trên NLVRv2.

4.2 Tài liệu cho phép mở rộng quy mô hiệu quả trong việc sử dụng công cụ

Các phát hiện trong Mục 4.1 cho thấy rằng người ta thực sự có thể giảm sự phụ thuộc vào demo few-shot
với tài liệu công cụ. Bằng cách nới lỏng ràng buộc này, chúng tôi nghiên cứu liệu tài liệu công cụ có cho
phép một cách có thể mở rộng hơn để trang bị LLM với một số lượng lớn công cụ, trong đó demo few-shot
có thể đặc biệt thiếu sót trong việc bao phủ các trường hợp sử dụng công cụ hạn chế. Chúng tôi trình bày
các phát hiện của mình trong mục này trên bộ dữ liệu LLM Cloud CLI mới được thu thập với 200 công
cụ có sẵn.

Kết quả hướng dẫn định tính. Hình 3 phục vụ như một ví dụ định tính minh họa các hạn chế của LLM
với thông tin khác nhau. Như mong đợi, LLM zero-shot thành công xác định và phản hồi lệnh touch, quen
thuộc và được biết đến rộng rãi. Tuy nhiên, khi đối mặt với

--- TRANG 6 ---
Bảng 1: So sánh với các phương pháp baseline hiện có trên các điểm chuẩn khác nhau. Chúng tôi tuân
theo [40,19] để chọn các phương pháp baseline cho mỗi nhiệm vụ điểm chuẩn. Chúng ta thấy rằng 0-shot
với doc hoạt động cạnh tranh, vượt trội so với CoT và PoT trên ScienceQA và TabMWP. Trên NLVRv2,
ViLT-NLVR được tinh chỉnh trên bộ dữ liệu, trong khi LLM hoạt động theo cách zero-shot.

Điểm chuẩn | Phương pháp
CoT [67] | không có doc (0-shot) | có doc (0-shot)
ScienceQA | 78.54 | 78.25 | 79.91
PoT [13] | không có doc (0-shot) | có doc (0-shot)
TabMWP | 89.28 | 84.13 | 92.69
ViLT-NLVR [29] | không có doc (0-shot) | có doc (0-shot)
NLVRv2 | 76.30 | 0.00 | 63.40

Hình 5: Lập kế hoạch lệnh của LLM Cloud Platform CLI với và không có tài liệu (doc) và ví dụ minh
họa (demo), và các kết hợp của chúng. Ví dụ minh họa few-shot không có tài liệu dẫn đến hiệu suất
không thỏa đáng do phạm vi bao phủ thấp của số lượng lớn công cụ, trong khi việc đọc tài liệu tăng
cường đáng kể hiệu suất.

các dòng lệnh LLM-Cloud chưa thấy, LLM zero-shot không thể tạo ra phản hồi chính xác liên quan đến
những công cụ không quen thuộc này do thiếu kiến thức về cú pháp và cách sử dụng của chúng.

Trong khi các ví dụ minh họa few-shot có tiềm năng tăng cường hiệu suất mô hình, điều quan trọng là
phải thừa nhận rằng phạm vi bao phủ của các ví dụ minh họa này bị hạn chế do số lượng lớn các công cụ
dòng lệnh. Do đó, một số lệnh hoặc cờ có thể không được bao phủ đầy đủ. Trong Hình 3, mặc dù chúng
ta quan sát thấy việc sao chép dữ liệu thường xuất hiện trong các ví dụ few-shot, tuy nhiên, mô hình gặp
khó khăn trong việc cấu hình chính xác cờ ít phổ biến hơn --port, thay vào đó ảo giác việc sử dụng -P
dựa trên sự quen thuộc với lệnh scp -P trong Linux.

Ngược lại, trong cùng ví dụ được minh họa trong Hình 3, bằng cách chỉ sử dụng tài liệu được cung cấp,
các mô hình ngôn ngữ không chỉ thành công nhận biết các bước cần thiết để sử dụng công cụ (chẳng hạn
như một bước ẩn tạo chủ đề trước khi gửi tin nhắn), mà còn có khả năng cấu hình chính xác các cờ (ví
dụ: --port) bằng cách tận dụng thông tin được trích xuất từ tài liệu.

So sánh định lượng. Chúng tôi tính điểm F1 cấp độ dòng lệnh của mỗi ví dụ và báo cáo F1 trung bình
trên 50 ví dụ. Hình 5 trình diễn hiệu suất của các LLM khác nhau trong thiết lập zero-shot, nơi chúng
không có tiếp xúc trước với các công cụ dòng lệnh LLM-Cloud mà chúng tôi tạo ra. Như dự đoán, tất cả
LLM zero-shot đều thể hiện điểm F1 thấp. Zero-shot text-davinci-002 đạt điểm F1 0.02, trong khi mô
hình gpt-3.5-turbo đạt điểm hơi cao hơn là 0.13. Hiệu suất cải thiện của mô hình gpt-3.5-turbo có thể
được quy cho việc xử lý tốt hơn các lệnh Linux phổ biến, chẳng hạn như touch. Như đã đề cập trong so
sánh định lượng, demo few-shot cải thiện so với zero-shot, nhưng vẫn thất bại trên các lệnh hoặc cờ
không được bao phủ trong demo. Do đó, demo few-shot tốt nhất trong text-davinci-002 và gpt-3.5-turbo
chỉ có điểm F1 lần lượt là 0.05 và 0.19. Mặt khác, LLM với tài liệu tăng cường hiệu suất bằng một biên
độ lớn lên 0.37 trong text-davinci-002 và 0.45 trong gpt-3.5-turbo.

--- TRANG 7 ---
Hình 6: Plug-and-play các công cụ thị giác mới không có ví dụ minh họa. Chúng tôi thêm GroundingDINO
[38], Segment Anything (SAM) [30], XMem [14] làm công cụ mới cho VisProg. Chỉ với tài liệu của các
công cụ mới, LLM có thể tự động "tái phát minh" Grounded-SAM [23] và Track Anything [70] gần đây
mà không biết các phái sinh này, tiến thêm một bước hướng tới khám phá kiến thức tự động.

Chúng tôi tiếp tục so sánh hiệu suất của việc đọc tài liệu với việc đọc tài liệu được bổ sung với các ví dụ
minh họa few-shot. Trong trường hợp text-davinci-002, chỉ với tài liệu, chúng tôi đạt được điểm F1 0.37.
Ngược lại, tài liệu được tăng cường với các shot khác nhau mang lại điểm F1 trung bình 0.35. Tương tự,
trong thử nghiệm gpt-3.5-turbo, hiệu suất với các ví dụ minh họa shot khác nhau (0.44, 0.44, 0.42) đều
thấp hơn so với hiệu suất chỉ có tài liệu (0.45).

Những kết quả này làm nổi bật hai quan sát. Đầu tiên, hiệu suất của mô hình rất nhạy cảm với việc lựa
chọn các ví dụ minh họa few-shot. Quan sát này phù hợp với phát hiện trong [12] rằng nhiều demo few-shot
hơn có thể dư thừa và thậm chí làm giảm hiệu suất do các tương quan giả. Nó nhấn mạnh tầm quan trọng
của việc lựa chọn và thiết kế cẩn thận, có thể liên quan đến nhiều nỗ lực của con người hơn. Thứ hai,
baseline đọc tài liệu zero-shot thể hiện tính bền vững đáng chú ý và mang lại hiệu suất cạnh tranh trên
cả hai ví dụ. Điều này làm nổi bật giá trị tiềm năng và độ tin cậy của việc chỉ dựa vào tài liệu, thường
dễ có được trong nhiều gói và công cụ.

4.3 Plug-and-play với các công cụ hình ảnh và video mới

Trong mục này, chúng tôi xác thận rằng người ta có thể trang bị LLM với các công cụ chưa thấy để giải
quyết các nhiệm vụ mới chỉ với tài liệu công cụ, và không cần bất kỳ demo nào khác. Chúng tôi trình bày
kết quả của chúng tôi về các nhiệm vụ chỉnh sửa hình ảnh và theo dõi video. Chúng tôi cho thấy rằng
LLM có thể hiệu quả tái phát minh các pipeline chỉnh sửa hình ảnh và theo dõi video được lập trình bởi
con người hiện có, được hỗ trợ bởi các mô hình thị giác tiên tiến để đạt được kết quả ấn tượng.

Những tiến bộ gần đây trong các mô hình thị giác, bao gồm GroundingDINO [38], một trình phát hiện
đối tượng mở tiên tiến; Segment Anything (SAM) [30], một công cụ phân đoạn hình ảnh tiên tiến; và
XMem [14], một công cụ phân đoạn đối tượng video tiên tiến, đi kèm với tiến bộ của các mô hình ngôn
ngữ. Những đột phá này, xuất hiện trong năm qua, phục vụ như các công cụ bổ sung chưa quen thuộc
với LLM của chúng tôi (gpt-3.5-turbo). Bằng cách mở rộng VisProg để bao gồm những công cụ mới này,
chúng tôi bắt đầu khám phá thú vị liệu LLM có thể dễ dàng hiểu tài liệu liên quan đến những mô hình
mới này, và kết hợp những công cụ này theo cách plug-and-play, cho phép một loạt ứng dụng rộng.

Trong Hình 6, khi thực hiện một yêu cầu chỉnh sửa hình ảnh "thay thế băng ghế bằng ghế sofa màu xanh",
LLM tạo ra một chương trình VisProg khai thác sức mạnh của GroundingDINO và SAM từ bộ công cụ
mở rộng để phân đoạn băng ghế, và áp dụng stable diffusion [54] để tổng hợp ghế sofa. Chương trình này
tái phát minh bánh xe bằng cách nhân bản hành vi của dự án phổ biến gần đây, Grounded-SAM [23] mà
không có kiến thức trước về kho lưu trữ này. Tương tự, khi được giao nhiệm vụ theo dõi video "theo dõi
con mèo trong video", chương trình VisProg được tạo ra bởi LLM kết hợp GroundingDINO cùng với SAM
cho phân đoạn khung hình đầu tiên làm khởi tạo cho XMem để thực hiện theo dõi video. Nó một lần nữa
tái phát minh kết quả thu được trong công trình đương đại, Track Anything [70]. Chúng tôi lưu ý rằng
TaskMatrix [69] cũng có một phương pháp cập nhật với Grounded-SAM. Tuy nhiên, họ lập trình sẵn toàn
bộ pipeline chỉnh sửa Grounded-SAM như một hàm chỉnh sửa hình ảnh, cho phép LLM kiểm soát nó thay
vì cho phép LLM tạo ra chương trình chỉnh sửa sử dụng các công cụ xây dựng một mình như chúng tôi
trình bày ở đây.

Bằng cách thành công tái phát minh các chức năng của Grounded-SAM và Track Anything mà không có
kiến thức trước, chỉ dựa vào các khối xây dựng có sẵn, LLM không chỉ thể hiện khả năng dễ dàng hiểu
và kết hợp các công cụ mới chỉ với tài liệu mà còn làm nổi bật tiềm năng của nó cho khám phá kiến thức
tự động. Nó khám phá những hiểu biết mới thông qua việc tận dụng kiến thức hiện có của mình mà không
cần thêm ví dụ minh họa.

4.4 Hiệu suất và chất lượng tài liệu

Chúng tôi điều tra tác động của chất lượng tài liệu đến hiệu suất. Để đánh giá khả năng của LLM trong
việc hiểu tài liệu thực tế, chúng tôi không kỹ thuật hóa hoặc tuyển chọn nội dung của tài liệu. Thay vào
đó, chúng tôi thay đổi độ dài tài liệu bằng cách cắt ngắn tài liệu và giữ lại n từ đầu tiên, sử dụng nó như
một proxy để đánh giá tính toàn diện và chất lượng. Trong nghiên cứu loại bỏ này, chúng tôi xem xét
điểm chuẩn LLM-Cloud, có tài liệu dài dựa trên hướng dẫn CLI GCP thực tế. Chúng tôi minh họa kết quả
trong Hình 7.

[THIS IS FIGURE/CHART: A line graph showing F1 Score (y-axis, ranging from 0.10 to 0.45) vs Documentation Length (x-axis, ranging from 200 to 800). The graph shows two lines for gpt-3.5-turbo and text-davinci-002, both with documentation, showing increasing performance up to around 600 words, then slight decline.]

Hình 7: Hiệu suất của LLM tài liệu zero-shot khi thay đổi độ dài tài liệu đầu vào.

Trong cả thử nghiệm text-davinci-002 và gpt-3.5-turbo, chúng tôi nhất quán quan sát một xu hướng
mà hiệu suất cải thiện khi độ dài tài liệu tăng, lên đến độ dài 600. Phát hiện này phù hợp với giả thuyết
của chúng tôi rằng các mô hình có khả năng hiểu và tận dụng tài liệu hiệu quả. Đáng chú ý, sự cải thiện
hiệu suất này được đạt được mà không cần bất kỳ huấn luyện bổ sung, tinh chỉnh hay tuyển chọn tài liệu
nào. Nó làm nổi bật giá trị to lớn của việc cung cấp tài liệu toàn diện, vì nó trao quyền cho các mô hình
tận dụng một loạt các công cụ dòng lệnh ở quy mô lớn, chỉ thông qua quá trình đọc và hiểu tài liệu.

Chúng tôi lưu ý rằng có sự suy giảm hiệu suất sau khi độ dài tài liệu vượt quá 600 từ. Chúng tôi quy sự
suy giảm này cho những thách thức vốn có liên quan đến việc hiểu tài liệu dài trong các mô hình ngôn
ngữ [61]. Tuy nhiên, chúng tôi dự thấy những tiến bộ đang diễn ra trong việc xử lý đầu vào dài trong
các mô hình ngôn ngữ sẽ dần dần giải quyết hạn chế này [10,5,2]. Chúng tôi để lại việc khám phá các
giải pháp để vượt qua hạn chế này cho nghiên cứu tương lai.

--- TRANG 9 ---
5 Kết luận

Trong bài báo này, chúng tôi đã kiểm tra hiệu quả của tài liệu công cụ trong việc cho phép sử dụng công
cụ zero-shot với LLM. Chúng tôi đầu tiên cho thấy rằng LLM có thể đạt được hiệu suất ngang bằng hoặc
tốt hơn so với các đối tác few-shot của chúng khi được cung cấp tài liệu công cụ. Sau đó chúng tôi mở
rộng quy mô lên một bộ công cụ lớn hơn đáng kể trên một API mới được thu thập chỉ thông qua tài liệu.
Bằng cách đơn giản cắm vào các công cụ mới cùng với tài liệu của chúng, LLM có thể giải quyết các
nhiệm vụ chưa thấy trong chỉnh sửa hình ảnh và theo dõi video mà không cần demo thêm và nhân bản
các chức năng của các dự án phổ biến gần đây, gợi ý một tiềm năng cho khám phá kiến thức tự động.
Nhìn chung, chúng tôi đã làm sáng tỏ một góc nhìn mới về việc sử dụng công cụ với LLM bằng cách tập
trung vào khả năng lập kế hoạch và lý luận nội bộ của chúng với tài liệu, thay vì hướng dẫn rõ ràng hành
vi của chúng với demo.

Tài liệu tham khảo

[1]Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,
Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel
Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano,
Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang,
Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell
Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,
Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,
Mengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic
affordances. In arXiv preprint arXiv:2204.01691 , 2022.

[2]Anthropic. 100k context windows. https://www.anthropic.com/index/
100k-context-windows , 2023. Accessed: 05/15/2023.

[3]AutoGPT. Auto gpt. https://autogpt.net/category/chatgpt-tools/autogpt/ , 2023.
Accessed: 05/15/2023.

[4]Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelli-
gence 15 , pages 103–129, 1995.

[5]Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. Unlimiformer: Long-
range transformers with unlimited length input. arXiv preprint arXiv:2305.01625 , 2023.

[6]Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie
Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,
et al. Improving language models by retrieving from trillions of tokens. In International
conference on machine learning , pages 2206–2240. PMLR, 2022.

[7]SRK Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a
monte-carlo framework. Journal of Artificial Intelligence Research , 43:661–704, 2012.

[8]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.

[9]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece
Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.

[10] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scaling transformer to 1m tokens and
beyond with rmt. arXiv preprint arXiv:2304.11062 , 2023.

[11] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models
as tool makers. arXiv preprint arXiv:2305.17126 , 2023.

[12] Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. How many demonstrations do you
need for in-context learning? 2023.

--- TRANG 10 ---
[13] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts
prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv
preprint arXiv:2211.12588 , 2022.

[14] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an
atkinson-shiffrin memory model. In Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVIII , pages 640–658. Springer,
2022.

[15] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.

[16] Surís Dídac, Sachit Menon, and Carl V ondrick. Vipergpt: Visual inference via python execution
for reasoning. arXiv preprint arXiv:2303.08128 , 2023.

[17] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,
Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc
Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied
multimodal language model. In arXiv preprint arXiv:2303.03378 , 2023.

[18] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,
and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435 ,
2022.

[19] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning
without training. arXiv preprint arXiv:2211.11559 , 2022.

[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval
augmented language model pre-training. In International conference on machine learning ,
pages 3929–3938. PMLR, 2020.

[21] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as
zero-shot planners: Extracting actionable knowledge for embodied agents. In International
Conference on Machine Learning , pages 9118–9147. PMLR, 2022.

[22] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,
Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied
reasoning through planning with language models. arXiv preprint arXiv:2207.05608 , 2022.

[23] IDEA-Research. Grounded-segment-anything. https://github.com/IDEA-Research/
Grounded-Segment-Anything , 2023. Accessed: 05/15/2023.

[24] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using
large language models. arXiv preprint arXiv:2303.05398 , 2023.

[25] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to
code in programmatic context. arXiv preprint arXiv:1808.09588 , 2018.

[26] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark,
and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.
InFindings of the Association for Computational Linguistics: EMNLP 2020 , pages 1896–1907,
Online, November 2020. Association for Computational Linguistics.

[27] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,
and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for
knowledge-intensive nlp. arXiv preprint arXiv:2212.14024 , 2022.

[28] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and
Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks.
arXiv preprint arXiv:2210.02406 , 2022.

--- TRANG 11 ---
[29] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without
convolution or region supervision. In International Conference on Machine Learning , pages
5583–5594. PMLR, 2021.

[30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv
preprint arXiv:2304.02643 , 2023.

[31] Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation.
arXiv preprint arXiv:2107.07566 , 2021.

[32] Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-
augmented language models through few-shot prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115 , 2022.

[33] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented
generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing
Systems , 33:9459–9474, 2020.

[34] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A
simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 ,
2019.

[35] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
What makes good in-context examples for gpt- 3?arXiv preprint arXiv:2101.06804 , 2021.

[36] Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou.
Tapex: Table pre-training via learning a neural sql executor. arXiv preprint arXiv:2107.07653 ,
2021.

[37] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush V osoughi, Claire Cui, Denny
Zhou, and Andrew M Dai. Mind's eye: Grounded language model reasoning through simulation.
arXiv preprint arXiv:2210.05359 , 2022.

[38] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei
Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for
open-set object detection. arXiv preprint arXiv:2303.05499 , 2023.

[39] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind
Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought
chains for science question answering. In The 36th Conference on Neural Information Process-
ing Systems (NeurIPS) , 2022.

[40] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun
Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language
models. arXiv preprint arXiv:2304.09842 , 2023.

[41] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter
Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured
mathematical reasoning. In International Conference on Learning Representations (ICLR) ,
2023.

[42] Yujie Lu, Pan Lu, Zhiyu Chen, Wanrong Zhu, Xin Eric Wang, and William Yang Wang.
Multimodal procedural planning via dual text-image prompting. 2023.

[43] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-
pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.

[44] Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml,
volume 1, page 2, 2000.

[45] OpenAI. Gpt-4 technical report. 2023.

--- TRANG 12 ---
[46] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer,
and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language
models. arXiv preprint arXiv:2303.09014 , 2023.

[47] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv
preprint arXiv:2205.12255 , 2022.

[48] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language
model connected with massive apis. arXiv preprint arXiv:2305.15334 , 2023.

[49] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in
neural information processing systems , 1, 1988.

[50] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.
Measuring and narrowing the compositionality gap in language models. arXiv preprint
arXiv:2210.03350 , 2022.

[51] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei
Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint
arXiv:2304.08354 , 2023.

[52] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,
Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,
Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to
master 16000+ real-world apis, 2023.

[53] Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code genera-
tion and semantic parsing. arXiv preprint arXiv:1704.07535 , 2017.

[54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 10684–10695, 2022.

[55] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proceedings of the fourteenth interna-
tional conference on artificial intelligence and statistics , pages 627–635. JMLR Workshop and
Conference Proceedings, 2011.

[56] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-
moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach
themselves to use tools. arXiv preprint arXiv:2302.04761 , 2023.

[57] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint
arXiv:2303.17580 , 2023.

[58] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan
Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. Blenderbot 3: a deployed conversational
agent that continually learns to responsibly engage. arXiv preprint arXiv:2208.03188 , 2022.

[59] Michael Sipser. Introduction to the theory of computation. ACM Sigact News , 27(1):27–29,
1996.

[60] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for
reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics , pages 6418–6428, Florence, Italy,
July 2019. Association for Computational Linguistics.

[61] Simeng Sun, Katherine Thai, and Mohit Iyyer. Chapterbreak: A challenge dataset for long-range
language models. arXiv preprint arXiv:2204.10878 , 2022.

[62] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for
dialog applications. arXiv preprint arXiv:2201.08239 , 2022.

--- TRANG 13 ---
[63] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.

[64] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng
Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large
language models. arXiv preprint arXiv:2305.04091 , 2023.

[65] Xingyao Wang, Sha Li, and Heng Ji. Code4struct: Code generation for few-shot structured
prediction from natural language. arXiv preprint arXiv:2210.12810 , 2022.

[66] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-
tions. arXiv preprint arXiv:2212.10560 , 2022.

[67] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903 , 2022.

[68] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao
Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently.
arXiv preprint arXiv:2303.03846 , 2023.

[69] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.
Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint
arXiv:2303.04671 , 2023.

[70] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything:
Segment anything meets videos, 2023.

[71] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teach-
ing large language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752 ,
2023.

[72] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foun-
dation models for decision making: Problems, methods, and opportunities. arXiv preprint
arXiv:2303.04129 , 2023.

[73] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,
Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for
multimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023.

[74] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206 ,
2022.

[75] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,
2022.

[76] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code
generation. arXiv preprint arXiv:1704.01696 , 2017.

[77] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chen-
guang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language
models are strong context generators. In The Eleventh International Conference on Learning
Representations , 2023.

[78] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,
Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init
attention. arXiv preprint arXiv:2303.16199 , 2023.

[79] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068 , 2022.

--- TRANG 14 ---
[80] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Mul-
timodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 ,
2023.

[81] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use:
Improving few-shot performance of language models. In International Conference on Machine
Learning , pages 12697–12706. PMLR, 2021.

[82] Victor Zhong, Tim Rocktäschel, and Edward Grefenstette. Rtfm: Generalising to novel
environment dynamics via reading. arXiv preprint arXiv:1910.08210 , 2019.

[83] Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. Docprompting:
Generating code by retrieving the docs. In The Eleventh International Conference on Learning
Representations , 2023.

--- TRANG 15 ---
A Tác động rộng hơn và hạn chế

Công trình này nghiên cứu tầm quan trọng của tài liệu công cụ trong việc trang bị LLM với khả năng
soạn thảo việc sử dụng nhiều công cụ khác nhau để hoàn thành các nhiệm vụ phức tạp. Tuy nhiên, như
đã thảo luận trong [51], điều bắt buộc là phải suy ngẫm về những công cụ nào nên được cung cấp cho
LLM cũng như cách người ta nên diễn giải và dựa vào kết quả thu được từ các mô hình. Chúng tôi hình
dung tài liệu công cụ như một kênh để hướng dẫn LLM sử dụng các công cụ một cách an toàn hơn, phù
hợp với mục đích sử dụng ban đầu của các công cụ.

B Chi tiết triển khai

Trong mục này, chúng tôi cung cấp thêm chi tiết triển khai cho mỗi nhiệm vụ. Chúng tôi tiến hành tất cả
các thử nghiệm trên máy Debian GNU/Linux 10 với GPU A100 40GB.

B.1 ScienceQA

Trên ScienceQA [39], chúng tôi tuân theo chặt chẽ thiết lập gốc¹ được sử dụng trong Chameleon [40],
bao gồm tài liệu công cụ và demo few-shot (khi được sử dụng). Tuy nhiên chúng tôi thấy rằng mô-đun
"Image Captioner" được sử dụng trong công trình gốc thường cung cấp chú thích ít chính xác hơn trên
các hình ảnh đã cho. Trong tài liệu, chúng tôi do đó thêm mô tả về quan sát này cho mô-đun "Image
Captioner" như được hiển thị trong Hình 8.

Hình 8: Tài liệu được sử dụng trong bộ dữ liệu ScienceQA. Chúng tôi sử dụng tài liệu công cụ gốc
trong Chameleon [40] và thêm mô tả cho "Image Captioner" rằng các chú thích được tạo ra có thể
không chính xác.

B.2 TabMWP

Trên TabMWP [41], chúng tôi tuân theo nghiêm ngặt thiết lập gốc được sử dụng trong Chameleon [40].
Chúng tôi giới thiệu độc giả đến [40] và các triển khai mã nguồn mở của họ để biết thêm chi tiết.

B.3 NLVRv2

Trên NLVRv2, chúng tôi tuân theo thiết lập được sử dụng trong [19]. Tuy nhiên, vì tài liệu công cụ
không được sử dụng trong [19], chúng tôi tạo ra tài liệu riêng cho các công cụ được sử dụng. Hình 9
hiển thị tài liệu công cụ mà chúng tôi sử dụng cho một số công cụ có sẵn được sử dụng trong VisProg [19].

--- TRANG 16 ---
Hình 9: Ví dụ tài liệu được sử dụng cho các công cụ trong VisProg [19].

--- TRANG 17 ---
Hình 10: Các ví dụ tài liệu từ GCP CLI. Chúng tôi thu thập trang web, loại bỏ các thẻ HTML và áp
dụng quy trình đổi tên làm tài liệu của LLM-Cloud CLI đã tạo.

B.4 LLM-Cloud CLI

Thêm ví dụ. Trong Bảng 2, chúng tôi hiển thị thêm ví dụ về bộ dữ liệu LLM-Cloud CLI đã tạo, dựa
trên GCP CLI.

Tạo tài liệu công cụ. Trên bộ dữ liệu LLM-Cloud CLI, chúng tôi tạo tài liệu công cụ bằng cách sử dụng
thư viện BeautifulSoup² được sử dụng rộng rãi để thu thập tài liệu GCP CLI. Chúng tôi loại bỏ các thẻ
HTML và triển khai các quy trình đổi tên cho tài liệu LLM-Cloud CLI. Chúng tôi lưu ý rằng chúng tôi
cố ý không loại bỏ nội dung không liên quan như thuật ngữ và siêu liên kết. Một ví dụ tài liệu từ GCP
trước quy trình đổi tên của chúng tôi được hiển thị trong Hình 10. Điều này là để ngăn chặn việc kỹ
thuật hóa quá mức tài liệu để đánh giá tốt hơn tính bền vững của khả năng đọc tài liệu LLM.

Chi tiết truy xuất tài liệu. Cho số lượng lớn các công cụ dòng lệnh trong thử nghiệm của chúng tôi (tổng
cộng 200), tài liệu hoàn chỉnh không thể vừa trong một prompt duy nhất. Do đó, cho mỗi truy vấn, chúng
tôi sử dụng một tìm kiếm TF-IDF đơn giản để truy xuất 10 tài liệu liên quan hàng đầu. Sau đó chúng
tôi cắt ngắn độ dài tối đa 600 từ. Chúng tôi lưu ý rằng số lượng token thực tế phụ thuộc vào tokenizer
được sử dụng bởi mỗi LLM và thường nhiều hơn 600.

²https://pypi.org/project/beautifulsoup4/

--- TRANG 18 ---
Bảng 2: Thêm ví dụ về bộ dữ liệu LLM-Cloud CLI đã tạo.

[Bảng được dịch với các cột: Câu hỏi | Lệnh trong GCP | Lệnh sau khi đổi tên (Đáp án cuối cùng)]

--- TRANG 19 ---
[Phần tiếp theo của bảng]

--- TRANG 20 ---
B.5 Chỉnh sửa hình ảnh và theo dõi video

Như đã thảo luận trong Mục 4.3, bằng cách cung cấp tài liệu công cụ, chúng ta có thể dễ dàng thêm
các công cụ mới để cho phép LLM giải quyết các nhiệm vụ mới như chỉnh sửa hình ảnh và theo dõi video.
Ở đây, chúng tôi tận dụng những tiến bộ gần đây trong các mô hình thị giác và mở rộng bộ công cụ được
sử dụng trong VisProg [19] với ba công cụ mới: GroundingDINO [38], Segment Anything (SAM) [30],
và XMem [14]. Chúng tôi cung cấp tài liệu tương ứng của chúng trong Hình 11.

Hình 11: Tài liệu của các công cụ mới được giới thiệu trong VisProg. BETTERLOC, BETTERSEG,
TRACK gọi GroundingDINO, Segment Anything, XMem, tương ứng.

--- TRANG 21 ---
C Kết quả thử nghiệm

Trong mục này, chúng tôi hiển thị kết quả thử nghiệm trên mỗi nhiệm vụ với so sánh với nhiều baseline
hơn.

ScienceQA. Trong Bảng 3, chúng tôi so sánh prompting zero-shot với tài liệu công cụ với các phương
pháp baseline khác. Chúng tôi bao gồm các phương pháp baseline sau được tinh chỉnh trên tập huấn
luyện ScienceQA để tham khảo hiệu suất: ViLT [29], VisualBERT [34], UnifiedQA CoT [39], MM-CoT [80],
và LLaMA-Adapter [78]. Chúng tôi báo cáo kết quả thu được từ [40] cho các phương pháp được tinh chỉnh.
Để so sánh công bằng, chúng ta nên tập trung vào các thiết lập zero/few-shot. Do đó, chúng tôi bao gồm
Chain-of-Thought (CoT) [67] và Chameleon [40] làm baseline few-shot để so sánh. Chúng ta thấy rằng
với tài liệu công cụ, chúng ta không chỉ có thể đạt được hiệu suất tốt hơn so với các phương pháp few-shot
mà không cần bất kỳ demo nào, mà còn có thể sánh ngang (vượt trội) một số mô hình được tinh chỉnh
đặc biệt trên bộ dữ liệu.

[Các bảng 3, 4, 5, 6 và thêm ví dụ hình ảnh được dịch tương ứng]

--- TRANG 22 ---
[Tiếp tục với kết quả thử nghiệm và ví dụ chỉnh sửa hình ảnh]

--- TRANG 23 ---
Hình 12: Ví dụ chỉnh sửa hình ảnh bằng prompting zero-shot gpt-3.5-turbo với tài liệu công cụ.
Prompting zero-shot với tài liệu có thể tái tạo kết quả đạt được bởi VisProg sử dụng demo few-shot [19].
