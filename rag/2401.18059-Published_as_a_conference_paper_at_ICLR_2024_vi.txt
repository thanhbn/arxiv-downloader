# 2401.18059.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2401.18059.pdf
# Kích thước tệp: 2547113 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
RAPTOR: XỬ LÝ TÓM TẮT ĐỆ QUY
CHO TRUY XUẤT TỔ CHỨC DẠNG CÂY
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning
Đại học Stanford
psarthi@cs.stanford.edu
TÓM TẮT
Các mô hình ngôn ngữ được tăng cường truy xuất có thể thích ứng tốt hơn với những thay đổi trong trạng thái thế giới và kết hợp kiến thức đuôi dài. Tuy nhiên, hầu hết các phương pháp hiện tại chỉ truy xuất những đoạn văn bản liền kề ngắn từ kho tài liệu truy xuất, hạn chế việc hiểu toàn diện bối cảnh tổng thể của tài liệu. Chúng tôi giới thiệu phương pháp mới là nhúng, phân cụm và tóm tắt đệ quy các đoạn văn bản, xây dựng một cấu trúc cây với các mức độ tóm tắt khác nhau từ dưới lên. Tại thời điểm suy luận, mô hình RAPTOR của chúng tôi truy xuất từ cây này, tích hợp thông tin qua các tài liệu dài ở các mức độ trừu tượng khác nhau. Các thí nghiệm có kiểm soát cho thấy truy xuất với tóm tắt đệ quy mang lại cải thiện đáng kể so với các mô hình ngôn ngữ được tăng cường truy xuất truyền thống trên nhiều tác vụ. Trên các tác vụ hỏi đáp liên quan đến lý luận phức tạp, nhiều bước, chúng tôi đạt được kết quả tối ưu; ví dụ, bằng cách kết hợp truy xuất RAPTOR với việc sử dụng GPT-4, chúng tôi có thể cải thiện hiệu suất tốt nhất trên benchmark QuALITY tới 20% về độ chính xác tuyệt đối.

1 GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) đã nổi lên như những công cụ đột phá cho thấy hiệu suất ấn tượng trên nhiều tác vụ. Với kích thước ngày càng tăng của LLM, chúng có thể hoạt động độc lập như những kho tri thức rất hiệu quả, với các sự kiện được mã hóa trong các tham số của chúng (Petroni et al., 2019; Jiang et al., 2020; Talmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et al., 2023; Kandpal et al., 2023) và các mô hình có thể được cải thiện thêm với việc tinh chỉnh trên các tác vụ hạ nguồn (Roberts et al., 2020). Tuy nhiên, ngay cả một mô hình lớn cũng không chứa đủ kiến thức chuyên biết cho các tác vụ cụ thể và thế giới tiếp tục thay đổi, làm mất hiệu lực các sự kiện trong LLM. Việc cập nhật kiến thức của các mô hình này thông qua tinh chỉnh bổ sung hoặc chỉnh sửa là khó khăn, đặc biệt khi xử lý các kho văn bản khổng lồ (Lewis et al., 2020; Mitchell et al., 2022). Một phương pháp thay thế, được tiên phong trong các hệ thống hỏi đáp miền mở (Chen et al., 2017; Yu et al., 2018), là lập chỉ mục lượng lớn văn bản, sau khi chia thành các đoạn (đoạn văn), trong một hệ thống truy xuất thông tin riêng biệt. Thông tin được truy xuất sau đó được trình bày cho LLM cùng với câu hỏi như bối cảnh ("tăng cường truy xuất", Lewis et al., 2020; Izacard et al., 2022; Min et al., 2023; Ram et al., 2023), giúp dễ dàng cung cấp cho hệ thống kiến thức hiện tại cụ thể cho một miền nào đó và cho phép khả năng diễn giải và theo dõi nguồn gốc dễ dàng, trong khi kiến thức tham số của LLM là mờ ám và khó truy vết ngược về nguồn (Akyurek et al., 2022).

Tuy nhiên, các phương pháp được tăng cường truy xuất hiện tại cũng có những khuyết điểm. Vấn đề chúng tôi giải quyết là hầu hết các phương pháp hiện tại chỉ truy xuất một vài đoạn văn bản ngắn, liền kề, điều này hạn chế khả năng biểu diễn và tận dụng cấu trúc diễn ngôn quy mô lớn. Điều này đặc biệt liên quan đến các câu hỏi chủ đề đòi hỏi tích hợp kiến thức từ nhiều phần của văn bản, chẳng hạn như hiểu toàn bộ cuốn sách, như trong bộ dữ liệu NarrativeQA (Kočiský et al., 2018). Xem xét câu chuyện cổ tích Cinderella và câu hỏi "Cinderella đã đạt được kết thúc hạnh phúc như thế nào?". Các văn bản liền kề ngắn được truy xuất hàng đầu k sẽ không chứa đủ bối cảnh để trả lời câu hỏi.

Để giải quyết vấn đề này, chúng tôi thiết kế một hệ thống lập chỉ mục và truy xuất sử dụng cấu trúc cây để nắm bắt cả chi tiết cấp cao và cấp thấp về văn bản. Như được thể hiện trong Hình 1, hệ thống của chúng tôi, RAPTOR, phân cụm các đoạn văn bản, tạo tóm tắt văn bản của những cụm đó, và sau đó lặp lại, tạo ra một cây từ dưới lên. Cấu trúc này cho phép RAPTOR tải vào bối cảnh LLM các đoạn đại diện cho văn bản ở các mức độ khác nhau để có thể trả lời các câu hỏi ở các mức độ khác nhau một cách hiệu quả và hiệu suất.

1arXiv:2401.18059v1 [cs.CL] 31 Jan 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
2
3
4
5
 1
1
 2
 3
 3
 4
5
 5
6
 8
 7
Chỉ mục #8 
Văn bản: tóm tắt của 
nút 2 và 3 
Nút con: 2, 3 
Nhúng văn bản 
Đoạn văn bản 3
.1
4
.1
52. Tóm tắt 
bằng LLM
1.Phân cụm 
10
7
1
2
8
4
3
 5
6
9
Hình thành một lớp cây 
Lớp gốc 
Lớp lá Nội dung của một nút Cây RAPTOR 

Hình 1: Quá trình xây dựng cây: RAPTOR phân cụm đệ quy các đoạn văn bản dựa trên vector nhúng của chúng và tạo tóm tắt văn bản của những cụm đó, xây dựng một cây từ dưới lên. Các nút được phân cụm cùng nhau là anh em; một nút cha chứa tóm tắt văn bản của cụm đó.

Đóng góp chính của chúng tôi là ý tưởng sử dụng tóm tắt văn bản để cho phép tăng cường truy xuất bối cảnh ở các quy mô khác nhau, và chứng minh tính hiệu quả của nó trong các thí nghiệm trên các bộ sưu tập tài liệu dài. Các thí nghiệm có kiểm soát với ba mô hình ngôn ngữ (UnifiedQA (Khashabi et al., 2020), GPT-3 (Brown et al., 2020) và GPT-4 (OpenAI, 2023)) cho thấy RAPTOR vượt trội hơn tăng cường truy xuất hiện tại. Hơn nữa, RAPTOR kết hợp với GPT-4, và đôi khi thậm chí với UnifiedQA, đưa ra kết quả tối ưu mới trên ba tác vụ QA: câu hỏi phản hồi văn bản tự do về sách và phim (NarrativeQA, Kočiský et al. 2018), các bài báo NLP toàn văn (QASPER, Dasigi et al. 2021), và câu hỏi trắc nghiệm dựa trên các đoạn văn có độ dài trung bình (QuALITY, Pang et al. 2022).1

2 NGHIÊN CỨU LIÊN QUAN

Tại sao Truy xuất? Những tiến bộ gần đây trong phần cứng và thuật toán thực sự đã mở rộng độ dài bối cảnh mà các mô hình có thể xử lý, dẫn đến câu hỏi về nhu cầu của các hệ thống truy xuất (Dai et al., 2019; Dao et al., 2022; Liu et al., 2023). Tuy nhiên, như Liu et al. (2023) và Sun et al. (2021) đã lưu ý, các mô hình có xu hướng không tận dụng đầy đủ bối cảnh tầm xa và thấy hiệu suất giảm khi độ dài bối cảnh tăng, đặc biệt khi thông tin liên quan được nhúng trong một bối cảnh dài. Hơn nữa, thực tế, việc sử dụng bối cảnh dài tốn kém và chậm. Điều này cho thấy việc lựa chọn thông tin liên quan nhất cho các tác vụ đòi hỏi nhiều kiến thức vẫn quan trọng.

Phương pháp Truy xuất Các mô hình ngôn ngữ được tăng cường truy xuất (RALM) đã thấy cải thiện trong các thành phần khác nhau: bộ truy xuất, bộ đọc và đào tạo hệ thống đầu cuối. Các phương pháp truy xuất đã chuyển từ các kỹ thuật dựa trên thuật ngữ truyền thống như TF-IDF (Spärck Jones, 1972) và BM25 (Robertson et al., 1995; Roberts et al., 2020) sang các chiến lược dựa trên học sâu (Karpukhin et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Một số nghiên cứu gần đây đề xuất sử dụng các mô hình ngôn ngữ lớn làm bộ truy xuất do khả năng ghi nhớ kiến thức rộng lớn của chúng (Yu et al., 2022; Sun et al., 2022). Nghiên cứu về thành phần đọc bao gồm Fusion-in-Decoder (FiD) (Izacard & Grave, 2022), sử dụng cả DPR và BM25 để truy xuất và xử lý các đoạn văn độc lập trong bộ mã hóa và RETRO (Borgeaud et al., 2022; Wang et al., 2023), sử dụng attention xuyên đoạn và truy xuất theo đoạn để tạo văn bản dựa trên bối cảnh đã truy xuất.

Công việc đào tạo hệ thống đầu cuối bao gồm Atlas (Izacard et al., 2022), tinh chỉnh mô hình mã hóa-giải mã kết hợp với bộ truy xuất; REALM (Guu et al., 2020), một LM hai chiều, có mặt nạ được tinh chỉnh cho hỏi đáp miền mở; và RAG (Retrieval-Augmented Generation) (Lewis et al., 2020), tích hợp các mô hình chuỗi-sang-chuỗi được đào tạo trước với bộ truy xuất neural. Min et al. (2021) giới thiệu mô hình Joint Passage Retrieval (JPR) sử dụng thuật toán giải mã cây để xử lý tính đa dạng và liên quan của đoạn văn trong truy xuất đa câu trả lời. Dense Hierarchical Retrieval (DHR) và Hybrid Hierarchical Retrieval (HHR) đại diện cho những tiến bộ trong độ chính xác truy xuất bằng cách kết hợp truy xuất cấp tài liệu và đoạn văn và tích hợp các phương pháp truy xuất thưa và dày tương ứng (Liu et al., 2021; Arivazhagan et al., 2023).

1Chúng tôi sẽ công bố mã nguồn của RAPTOR công khai tại đây.
2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Mặc dù có sự đa dạng trong các phương pháp, các thành phần truy xuất của các mô hình chủ yếu dựa vào các phương pháp tiêu chuẩn, tức là phân đoạn kho tài liệu và mã hóa với các bộ truy xuất dựa trên BERT. Mặc dù phương pháp này được áp dụng rộng rãi, Nair et al. (2023) nêu bật một khuyết điểm tiềm năng: việc phân đoạn liền kề có thể không nắm bắt được độ sâu ngữ nghĩa hoàn chỉnh của văn bản. Việc đọc các đoạn trích xuất từ tài liệu kỹ thuật hoặc khoa học có thể thiếu bối cảnh quan trọng khiến chúng khó đọc hoặc thậm chí gây hiểu lầm. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023).

Tóm tắt đệ quy như Bối cảnh Các kỹ thuật tóm tắt cung cấp cái nhìn cô đọng về tài liệu, cho phép tương tác tập trung hơn với nội dung (Angelidis & Lapata, 2018). Mô hình tóm tắt/đoạn trích của Gao et al. (2023) sử dụng tóm tắt và đoạn trích của các đoạn văn, điều này cải thiện tính chính xác trên hầu hết các bộ dữ liệu nhưng đôi khi có thể là phương tiện nén có mất mát. Mô hình tóm tắt-trừu tượng đệ quy của Wu et al. (2021) sử dụng phân rã tác vụ để tóm tắt các đoạn văn bản nhỏ hơn, sau đó được tích hợp để tạo thành tóm tắt của các phần lớn hơn. Mặc dù phương pháp này hiệu quả trong việc nắm bắt các chủ đề rộng hơn, nó có thể bỏ lỡ các chi tiết chi tiết. LlamaIndex (Liu, 2022) giảm thiểu vấn đề này bằng cách tóm tắt tương tự các đoạn văn bản lân cận nhưng cũng giữ lại các nút trung gian do đó lưu trữ các mức độ chi tiết khác nhau, giữ lại các chi tiết chi tiết. Tuy nhiên, cả hai phương pháp, do dựa vào tính lân cận để nhóm hoặc tóm tắt các nút lân cận, vẫn có thể bỏ qua các phụ thuộc xa trong văn bản, mà chúng ta có thể tìm thấy và nhóm với RAPTOR.

3 PHƯƠNG PHÁP

Tổng quan về RAPTOR Dựa trên ý tưởng rằng các văn bản dài thường trình bày các chủ đề phụ và cấu trúc phân cấp (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR giải quyết vấn đề độ sâu ngữ nghĩa và kết nối trong việc đọc bằng cách xây dựng cấu trúc cây đệ quy cân bằng việc hiểu chủ đề rộng hơn với các chi tiết chi tiết và cho phép các nút được nhóm dựa trên độ tương tự ngữ nghĩa chứ không chỉ thứ tự trong văn bản.

Việc xây dựng cây RAPTOR bắt đầu với việc phân đoạn kho tài liệu truy xuất thành các văn bản ngắn, liền kề có độ dài 100, tương tự như các kỹ thuật tăng cường truy xuất truyền thống. Nếu một câu vượt quá giới hạn 100 token, chúng tôi chuyển toàn bộ câu sang đoạn tiếp theo, thay vì cắt nó giữa câu. Điều này bảo tồn tính mạch lạc ngữ cảnh và ngữ nghĩa của văn bản trong mỗi đoạn. Những văn bản này sau đó được nhúng bằng SBERT, một bộ mã hóa dựa trên BERT (multi-qa-mpnet-base-cos-v1) (Reimers & Gurevych, 2019). Các đoạn và các nhúng SBERT tương ứng của chúng tạo thành các nút lá của cấu trúc cây của chúng ta.

Để nhóm các đoạn văn bản tương tự, chúng tôi sử dụng thuật toán phân cụm. Một khi được phân cụm, một Mô hình Ngôn ngữ được sử dụng để tóm tắt các văn bản được nhóm. Những văn bản tóm tắt này sau đó được nhúng lại, và chu kỳ nhúng, phân cụm và tóm tắt tiếp tục cho đến khi việc phân cụm thêm trở nên không khả thi, dẫn đến một biểu diễn cây có cấu trúc, đa lớp của các tài liệu gốc. Một khía cạnh quan trọng của RAPTOR là hiệu quả tính toán của nó. Hệ thống mở rộng tuyến tính về cả thời gian xây dựng và chi phí token, làm cho nó phù hợp để xử lý các kho tài liệu lớn và phức tạp. Để thảo luận toàn diện về khả năng mở rộng của RAPTOR, vui lòng tham khảo Phụ lục A.

Để truy vấn trong cây này, chúng tôi giới thiệu hai chiến lược riêng biệt: duyệt cây và cây thu gọn. Phương pháp duyệt cây duyệt cây từng lớp, cắt tỉa và chọn các nút liên quan nhất ở mỗi cấp. Phương pháp cây thu gọn đánh giá các nút một cách tập thể qua tất cả các lớp để tìm những nút liên quan nhất.

Thuật toán Phân cụm Phân cụm đóng vai trò quan trọng trong việc xây dựng cây RAPTOR, tổ chức các đoạn văn bản thành các nhóm gắn kết. Bước này nhóm nội dung liên quan lại với nhau, giúp quá trình truy xuất tiếp theo.

Một trong những khía cạnh độc đáo của phương pháp phân cụm của chúng tôi là việc sử dụng phân cụm mềm, nơi các nút có thể thuộc về nhiều cụm mà không yêu cầu số lượng cụm cố định. Tính linh hoạt này rất cần thiết vì các đoạn văn bản riêng lẻ thường chứa thông tin liên quan đến các chủ đề khác nhau, do đó đảm bảo việc đưa chúng vào nhiều tóm tắt.

Thuật toán phân cụm của chúng tôi dựa trên Mô hình Hỗn hợp Gaussian (GMM), một phương pháp cung cấp cả tính linh hoạt và khung xác suất. GMM giả định rằng các điểm dữ liệu được tạo ra từ hỗn hợp của nhiều phân phối Gaussian.

3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Cho một tập hợp N đoạn văn bản, mỗi đoạn được biểu diễn như một vector nhúng dày đặc d chiều, khả năng của một vector văn bản x, cho thành viên của nó trong phân phối Gaussian thứ k, được ký hiệu là P(x|k) = N(x;μk,Σk). Phân phối xác suất tổng thể là một kết hợp có trọng số P(x) = ∑K k=1 πkN(x;μk,Σk), trong đó πk biểu thị trọng số hỗn hợp cho phân phối Gaussian thứ k.

Tính chiều cao của vector nhúng đặt ra thách thức cho GMM truyền thống, vì các độ đo khoảng cách có thể hoạt động kém khi được sử dụng để đo độ tương tự trong không gian nhiều chiều (Aggarwal et al., 2001). Để giảm thiểu điều này, chúng tôi sử dụng Uniform Manifold Approximation and Projection (UMAP), một kỹ thuật học đa tạp để giảm chiều (McInnes et al., 2018). Tham số số lượng láng giềng gần nhất, nneighbors, trong UMAP xác định sự cân bằng giữa việc bảo tồn cấu trúc cục bộ và toàn cầu. Thuật toán của chúng tôi thay đổi nneighbors để tạo cấu trúc phân cụm phân cấp: đầu tiên nó xác định các cụm toàn cầu và sau đó thực hiện phân cụm cục bộ trong các cụm toàn cầu này. Quá trình phân cụm hai bước này nắm bắt một phổ rộng các mối quan hệ giữa dữ liệu văn bản, từ các chủ đề rộng đến các chi tiết cụ thể.

Nếu bối cảnh kết hợp của một cụm cục bộ vượt quá ngưỡng token của mô hình tóm tắt, thuật toán của chúng tôi áp dụng đệ quy phân cụm trong cụm, đảm bảo rằng bối cảnh vẫn nằm trong ngưỡng token.

Để xác định số lượng cụm tối ưu, chúng tôi sử dụng Tiêu chí Thông tin Bayesian (BIC) để lựa chọn mô hình. BIC không chỉ phạt độ phức tạp của mô hình mà còn thưởng cho độ phù hợp (Schwarz, 1978). BIC cho một GMM đã cho là BIC = ln(N)k - 2ln(L̂), trong đó N là số lượng đoạn văn bản (hoặc điểm dữ liệu), k là số lượng tham số mô hình, và L̂ là giá trị tối đa của hàm khả năng của mô hình. Trong bối cảnh GMM, số lượng tham số k là một hàm của chiều của vector đầu vào và số lượng cụm.

Với số lượng cụm tối ưu được xác định bởi BIC, thuật toán Expectation-Maximization sau đó được sử dụng để ước tính các tham số GMM, cụ thể là trung bình, hiệp phương sai và trọng số hỗn hợp. Mặc dù giả định Gaussian trong GMM có thể không hoàn toàn phù hợp với bản chất của dữ liệu văn bản, thường biểu hiện phân phối thưa và lệch, các quan sát thực nghiệm của chúng tôi cho thấy nó cung cấp một mô hình hiệu quả cho mục đích của chúng tôi. Chúng tôi chạy một ablation so sánh GMM Clustering với tóm tắt các đoạn liền kề và cung cấp chi tiết trong Phụ lục B.

Tóm tắt Dựa trên Mô hình Sau khi phân cụm các nút bằng Mô hình Hỗn hợp Gaussian, các nút trong mỗi cụm được gửi đến một mô hình ngôn ngữ để tóm tắt. Bước này cho phép mô hình chuyển đổi các đoạn văn bản lớn thành các tóm tắt ngắn gọn, mạch lạc của các nút đã chọn. Cho các thí nghiệm của chúng tôi, chúng tôi sử dụng gpt-3.5-turbo để tạo tóm tắt. Bước tóm tắt cô đọng khối lượng thông tin được truy xuất có khả năng lớn thành kích thước có thể quản lý. Chúng tôi cung cấp số liệu thống kê về việc nén do tóm tắt trong Phụ lục C và prompt được sử dụng để tóm tắt trong Phụ lục D.

Mặc dù mô hình tóm tắt thường tạo ra các tóm tắt đáng tin cậy, một nghiên cứu chú thích tập trung đã tiết lộ rằng khoảng 4% số tóm tắt chứa các ảo giác nhỏ. Những điều này không lan truyền đến các nút cha và không có tác động có thể nhận biết đến các tác vụ hỏi đáp. Để phân tích sâu về ảo giác, tham khảo phụ lục E.

Truy vấn Trong phần này, chúng tôi trình bày chi tiết về hai cơ chế truy vấn được sử dụng bởi RAPTOR: duyệt cây và cây thu gọn. Các phương pháp này cung cấp những cách độc đáo để duyệt cây RAPTOR đa lớp để truy xuất thông tin liên quan, mỗi phương pháp có những ưu điểm và đánh đổi riêng. Chúng tôi cung cấp mã giả của cả hai phương pháp trong Phụ lục F. Lưu ý rằng chúng tôi nhúng tất cả các nút bằng SBERT.

Phương pháp duyệt cây đầu tiên chọn k nút gốc liên quan nhất dựa trên độ tương tự cosine của chúng với nhúng truy vấn. Các con của các nút đã chọn này được xem xét ở lớp tiếp theo và k nút hàng đầu được chọn từ nhóm này một lần nữa dựa trên độ tương tự cosine của chúng với vector truy vấn. Quá trình này được lặp lại cho đến khi chúng ta đạt đến các nút lá. Cuối cùng, văn bản từ tất cả các nút đã chọn được nối lại để tạo thành bối cảnh được truy xuất. Các bước của thuật toán được nêu dưới đây:

1. Bắt đầu tại lớp gốc của cây RAPTOR. Tính độ tương tự cosine giữa nhúng truy vấn và nhúng của tất cả các nút có mặt tại lớp ban đầu này.
2. Chọn k nút hàng đầu dựa trên điểm độ tương tự cosine cao nhất, tạo thành tập S1.

4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 2: Minh họa các cơ chế truy xuất duyệt cây và cây thu gọn. Duyệt cây bắt đầu tại cấp gốc của cây và truy xuất k nút hàng đầu (ở đây, top-1) dựa trên độ tương tự cosine với vector truy vấn. Tại mỗi cấp, nó truy xuất k nút hàng đầu từ các nút con của lớp trước đó. Cây thu gọn thu gọn cây thành một lớp duy nhất và truy xuất các nút cho đến khi đạt đến ngưỡng số lượng token, dựa trên độ tương tự cosine với vector truy vấn. Các nút mà tìm kiếm độ tương tự cosine được thực hiện được làm nổi bật trong cả hai minh họa.

3. Tiến đến các nút con của các phần tử trong tập S1. Tính độ tương tự cosine giữa vector truy vấn và vector nhúng của các nút con này.
4. Chọn k nút con hàng đầu có điểm độ tương tự cosine cao nhất với truy vấn, tạo thành tập S2.
5. Tiếp tục quá trình này đệ quy cho d lớp, tạo ra các tập S1, S2, ..., Sd.
6. Nối các tập S1 đến Sd để lắp ráp bối cảnh liên quan đến truy vấn.

Bằng cách điều chỉnh độ sâu d và số lượng nút k được chọn tại mỗi lớp, phương pháp duyệt cây cung cấp kiểm soát về tính cụ thể và chiều rộng của thông tin được truy xuất. Thuật toán bắt đầu với cái nhìn rộng bằng cách xem xét các lớp trên của cây và dần dần tập trung vào các chi tiết tinh tế hơn khi nó đi xuống qua các lớp thấp hơn.

Phương pháp cây thu gọn cung cấp một cách đơn giản hơn để tìm kiếm thông tin liên quan bằng cách xem xét tất cả các nút trong cây đồng thời, như được mô tả trong Hình 2. Thay vì đi từng lớp một, phương pháp này làm phẳng cây đa lớp thành một lớp duy nhất, về cơ bản đưa tất cả các nút lên cùng một cấp để so sánh. Các bước cho phương pháp này được nêu dưới đây:

1. Đầu tiên, thu gọn toàn bộ cây RAPTOR thành một lớp duy nhất. Tập hợp nút mới này, được ký hiệu là C, chứa các nút từ mọi lớp của cây gốc.
2. Tiếp theo, tính độ tương tự cosine giữa nhúng truy vấn và nhúng của tất cả các nút có mặt trong tập thu gọn C.
3. Cuối cùng, chọn k nút hàng đầu có điểm độ tương tự cosine cao nhất với truy vấn. Tiếp tục thêm các nút vào tập kết quả cho đến khi bạn đạt đến số lượng token tối đa được xác định trước, đảm bảo bạn không vượt quá giới hạn đầu vào của mô hình.

Chúng tôi đã thử nghiệm cả hai phương pháp trên 20 câu chuyện từ bộ dữ liệu QASPER. Hình 3 cho thấy hiệu suất của duyệt cây với các kích thước top-k khác nhau và cây thu gọn với số lượng token tối đa khác nhau. Phương pháp cây thu gọn hoạt động tốt hơn một cách nhất quán. Chúng tôi tin rằng truy xuất cây thu gọn tốt hơn do cung cấp tính linh hoạt lớn hơn so với duyệt cây; tức là, bằng cách tìm kiếm qua tất cả các nút đồng thời, nó truy xuất thông tin ở mức độ chi tiết chính xác cho một câu hỏi đã cho. So sánh, khi sử dụng duyệt cây với cùng giá trị d và k, tỷ lệ các nút từ mỗi cấp của cây sẽ không đổi. Vì vậy, tỷ lệ thông tin chủ đề bậc cao so với chi tiết chi tiết sẽ giữ nguyên bất kể câu hỏi.

5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Tuy nhiên, một nhược điểm của phương pháp cây thu gọn là nó yêu cầu tìm kiếm độ tương tự cosine được thực hiện trên tất cả các nút trong cây. Tuy nhiên, điều này có thể được thực hiện hiệu quả hơn với các thư viện k-nearest neighbor nhanh như FAISS (Johnson et al., 2019).

Hình 3: So sánh các phương pháp truy vấn. Kết quả trên 20 câu chuyện từ bộ dữ liệu QASPER sử dụng duyệt cây với các giá trị top-k khác nhau, và cây thu gọn với độ dài bối cảnh khác nhau. Cây thu gọn với 2000 token tạo ra kết quả tốt nhất, vì vậy chúng tôi sử dụng chiến lược truy vấn này cho kết quả chính của chúng tôi.

Nhìn chung, do tính linh hoạt lớn hơn của phương pháp cây thu gọn và hiệu suất vượt trội của nó trên tập con của bộ dữ liệu QASPER, đây là phương pháp truy vấn mà chúng tôi tiếp tục. Cụ thể, chúng tôi sử dụng cây thu gọn với tối đa 2000 token, xấp xỉ bằng việc truy xuất 20 nút hàng đầu. Việc sử dụng phương pháp dựa trên token đảm bảo bối cảnh không vượt quá giới hạn bối cảnh của mô hình vì số lượng token có thể thay đổi giữa các nút. Đối với các thí nghiệm với mô hình UnifiedQA, chúng tôi cung cấp 400 token bối cảnh, vì UnifiedQA có độ dài bối cảnh tối đa là 512 token. Chúng tôi cung cấp cùng lượng token bối cảnh cho RAPTOR và cho các baseline.

Nghiên cứu Định tính Chúng tôi tiến hành phân tích định tính để hiểu lợi ích của quá trình truy xuất của RAPTOR so với các phương pháp Dense Passage Retrieval (DPR). Nghiên cứu của chúng tôi tập trung vào các câu hỏi chủ đề, đa bước sử dụng câu chuyện cổ tích Cinderella 1500 từ. Như được minh họa trong Hình 4, truy xuất dựa trên cây của RAPTOR cho phép nó chọn các nút từ các lớp cây khác nhau, phù hợp với mức độ chi tiết của câu hỏi. Phương pháp này thường mang lại thông tin liên quan và toàn diện hơn cho các tác vụ hạ nguồn so với DPR. Để thảo luận chi tiết và ví dụ, bao gồm văn bản được truy xuất bởi cả RAPTOR và DPR cho các câu hỏi cụ thể, vui lòng tham khảo phụ lục G.

4 THÍ NGHIỆM

Bộ dữ liệu Chúng tôi đo hiệu suất của RAPTOR trên ba bộ dữ liệu hỏi đáp: NarrativeQA, QASPER và QuALITY.

NarrativeQA là một bộ dữ liệu bao gồm các cặp câu hỏi-câu trả lời dựa trên toàn văn sách và bản ghi phim, tổng cộng 1.572 tài liệu (Kočiský et al., 2018; Wu et al., 2021). Tác vụ NarrativeQA-Story yêu cầu hiểu toàn diện toàn bộ câu chuyện để trả lời chính xác các câu hỏi của nó, do đó kiểm tra khả năng của mô hình trong việc hiểu các văn bản dài hơn trong lĩnh vực văn học. Chúng tôi đo hiệu suất trên bộ dữ liệu này bằng các chỉ số BLEU (B-1, B-4), ROUGE (R-L) và METEOR (M) tiêu chuẩn. Vui lòng xem phụ lục H để biết thêm chi tiết về script đánh giá NarrativeQA được sử dụng trong các thí nghiệm của chúng tôi.

Bộ dữ liệu QASPER bao gồm 5.049 câu hỏi trên 1.585 bài báo NLP, với mỗi câu hỏi tìm kiếm thông tin được nhúng trong toàn văn (Dasigi et al., 2021). Các loại câu trả lời trong QASPER được phân loại là Có thể trả lời/Không thể trả lời, Có/Không, Trừu tượng và Trích xuất. Độ chính xác được đo bằng F1 tiêu chuẩn.

Cuối cùng, bộ dữ liệu QuALITY bao gồm các câu hỏi trắc nghiệm, mỗi câu được đi kèm với các đoạn văn bối cảnh có độ dài trung bình khoảng 5.000 token (Pang et al., 2022). Bộ dữ liệu này yêu cầu lý luận trên toàn bộ tài liệu cho các tác vụ QA, cho phép chúng tôi đo hiệu suất của hệ thống truy xuất của chúng tôi trên các tài liệu có độ dài trung bình. Bộ dữ liệu bao gồm một tập con thách thức, QuALITY-HARD, chứa các câu hỏi mà phần lớn người chú thích con người đã trả lời sai trong cài đặt tốc độ. Chúng tôi báo cáo độ chính xác cho cả tập kiểm tra toàn bộ và tập con HARD.

So sánh Baseline có Kiểm soát Đầu tiên chúng tôi trình bày các so sánh có kiểm soát sử dụng UnifiedQA 3B làm bộ đọc, với SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009), và DPR (Karpukhin et al., 2020) làm các mô hình nhúng có và không có cấu trúc cây RAPTOR, trên ba bộ dữ liệu: QASPER, NarrativeQA và QuALITY. Như được thể hiện trong Bảng 1 và 2,

6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 4: Quá trình Truy vấn: Minh họa cách RAPTOR truy xuất thông tin cho hai câu hỏi về câu chuyện Cinderella: "Chủ đề trung tâm của câu chuyện là gì?" và "Cinderella đã tìm thấy kết thúc hạnh phúc như thế nào?". Các nút được làm nổi bật chỉ ra các lựa chọn của RAPTOR, trong khi các mũi tên chỉ đến các nút lá của DPR. Đáng chú ý, bối cảnh của RAPTOR thường bao gồm thông tin được truy xuất bởi DPR, hoặc trực tiếp hoặc trong các tóm tắt lớp cao hơn.

kết quả của chúng tôi chứng minh rằng RAPTOR, khi kết hợp với bất kỳ bộ truy xuất nào, luôn vượt trội hơn bộ truy xuất tương ứng trên tất cả các bộ dữ liệu.2

Vì RAPTOR với SBERT có hiệu suất tốt nhất, chúng tôi sử dụng nó trong tất cả các thí nghiệm tiếp theo. Bây giờ chúng tôi so sánh RAPTOR với BM25 và DPR, sử dụng ba LLM khác nhau: GPT-3, GPT-4 và UnifiedQA. Như được thể hiện trong Bảng 3, RAPTOR luôn vượt trội hơn BM25 và DPR trên cả ba Mô hình Ngôn ngữ trên bộ dữ liệu QASPER. Điểm F-1 Match của RAPTOR là 53.1%, 55.7% và 36.6% khi sử dụng GPT-3, GPT-4 và UnifiedQA tương ứng. Những điểm này vượt trội hơn DPR với khoảng cách 1.8, 2.7 và 4.5 điểm, và vượt trội hơn BM25 với 6.5, 5.5 và 10.2 điểm trên các LLM tương ứng. QASPER yêu cầu tổng hợp thông tin trong các bài báo NLP, vì vậy không có gì đáng ngạc nhiên khi các nút tóm tắt cấp cao hơn của RAPTOR cho phép nó vượt trội hơn các phương pháp chỉ có thể trích xuất k đoạn văn bản thô tương tự nhất, có thể không chứa phản hồi chính xác một cách riêng lẻ.

Bảng 1: Hiệu suất NarrativeQA Có + Không có RAPTOR: So sánh hiệu suất của các phương pháp truy xuất khác nhau (SBERT, BM25, DPR) có và không có RAPTOR trên bộ dữ liệu NarrativeQA, sử dụng UnifiedQA-3B làm mô hình ngôn ngữ. RAPTOR vượt trội hơn các baseline của mỗi phương pháp truy xuất tương ứng.

Model ROUGE BLEU-1 BLEU-4 METEOR
SBERT với RAPTOR 30.87% 23.50% 6.42% 19.20%
SBERT không có RAPTOR 29.26% 22.56% 5.95% 18.15%
BM25 với RAPTOR 27.93% 21.17% 5.70% 17.03%
BM25 không có RAPTOR 23.52% 17.73% 4.65% 13.98%
DPR với RAPTOR 30.94% 23.51% 6.45% 19.05%
DPR không có RAPTOR 29.56% 22.84% 6.12% 18.44%

Tương tự, trong bộ dữ liệu QuALITY như được thể hiện trong Bảng 4, RAPTOR đạt độ chính xác 62.4%, cải thiện 2% và 5.1% so với DPR và BM25. Các xu hướng tương tự được quan sát khi UnifiedQA được sử dụng, với RAPTOR vượt trội hơn DPR và BM25 lần lượt 2.7% và 6.7%.

Cuối cùng, trong bộ dữ liệu NarrativeQA, như được trình bày trong Bảng 6, RAPTOR xuất sắc trên nhiều chỉ số. Đối với ROUGE-L, nó vượt trội hơn BM25 và DPR lần lượt 7.3 và 2.7 điểm. Trong các chỉ số khác như BLEU-1, BLEU-4 và METEOR, RAPTOR vượt trội hơn BM25 và DPR với khoảng cách từ 1.7 đến 5.8 và 0.7 đến 2.1 điểm tương ứng.

2Đối với các thí nghiệm DPR trong Bảng 1 và 2, chúng tôi đã sử dụng mô hình dpr-multiset-base thay vì dpr-single-nq-base được sử dụng trong phần còn lại của các thí nghiệm được thực hiện trước đó. Quyết định này dựa trên hiệu suất được quan sát trong Karpukhin et al. (2020), nơi dpr-multiset-base cho thấy kết quả vượt trội.

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 2: Hiệu suất QuALITY và QASPER Có + Không có RAPTOR: So sánh hiệu suất trên các bộ dữ liệu QuALITY và QASPER của các phương pháp truy xuất khác nhau (SBERT, BM25, DPR) có và không có RAPTOR. UnifiedQA-3B được sử dụng làm mô hình ngôn ngữ. RAPTOR vượt trội hơn các baseline của mỗi phương pháp truy xuất tương ứng cho cả hai bộ dữ liệu.

Model Độ chính xác (QuALITY) Answer F1 (QASPER)
SBERT với RAPTOR 56.6% 36.70%
SBERT không có RAPTOR 54.9% 36.23%
BM25 với RAPTOR 52.1% 27.00%
BM25 không có RAPTOR 49.9% 26.47%
DPR với RAPTOR 54.7% 32.23%
DPR không có RAPTOR 53.1% 31.70%

Bảng 3: So sánh có kiểm soát điểm F-1 trên bộ dữ liệu QASPER, sử dụng ba mô hình ngôn ngữ khác nhau (GPT-3, GPT-4, UnifiedQA 3B) và các phương pháp truy xuất khác nhau. Cột "Title + Abstract" phản ánh hiệu suất khi chỉ tiêu đề và tóm tắt của các bài báo được sử dụng làm bối cảnh. RAPTOR vượt trội hơn các baseline đã được thiết lập BM25 và DPR trên tất cả các mô hình ngôn ngữ được thử nghiệm. Cụ thể, điểm F-1 của RAPTOR cao hơn ít nhất 1.8% điểm so với DPR và ít nhất 5.3% điểm so với BM25.

Retriever GPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match
Title + Abstract 25.2 22.2 17.5
BM25 46.6 50.2 26.4
DPR 51.3 53.0 32.1
RAPTOR 53.1 55.7 36.6

Bảng 4: So sánh độ chính xác trên bộ dữ liệu phát triển QuALITY cho hai mô hình ngôn ngữ khác nhau (GPT-3, UnifiedQA 3B) sử dụng các phương pháp truy xuất khác nhau. RAPTOR vượt trội hơn các baseline BM25 và DPR ít nhất 2.0% về độ chính xác.

Model GPT-3 Acc. UnifiedQA Acc.
BM25 57.3 49.9
DPR 60.4 53.9
RAPTOR 62.4 56.6

Bảng 5: Kết quả điểm F-1 Match của các mô hình khác nhau trên bộ dữ liệu QASPER.

Model F-1 Match
LongT5 XL (Guo et al., 2022) 53.1
CoLT5 XL (Ainslie et al., 2023) 53.9
RAPTOR + GPT-4 55.7

So sánh với Hệ thống Tối ưu Dựa trên các so sánh có kiểm soát của chúng tôi, chúng tôi kiểm tra hiệu suất của RAPTOR so với các mô hình tối ưu khác. Như được thể hiện trong Bảng 5, RAPTOR với GPT-4 thiết lập một benchmark mới trên QASPER, với điểm F-1 55.7%, vượt qua điểm số 53.9% của CoLT5 XL.

Trong bộ dữ liệu QuALITY, như được thể hiện trong Bảng 7, RAPTOR kết hợp với GPT-4 thiết lập một tối ưu mới với độ chính xác 82.6%, vượt qua kết quả tốt nhất trước đó là 62.3%. Đặc biệt, nó vượt trội hơn CoLISA 21.5% trên QuALITY-HARD, đại diện cho các câu hỏi mà con người mất thời gian bất thường dài để trả lời chính xác, yêu cầu đọc lại các phần của văn bản, lý luận khó khăn, hoặc cả hai.

Đối với bộ dữ liệu NarrativeQA, như được đại diện trong Bảng 6, RAPTOR kết hợp với UnifiedQA thiết lập điểm METEOR tối ưu mới. Khi so sánh với mô hình tóm tắt đệ quy của Wu et al. (2021), cũng sử dụng UnifiedQA, RAPTOR vượt trội hơn nó trên tất cả các chỉ số. Trong khi Wu et al. (2021) chỉ dựa vào tóm tắt trong nút gốc trên cùng của cấu trúc cây, RAPTOR hưởng lợi từ các lớp trung gian và phương pháp phân cụm của nó, cho phép nó nắm bắt một loạt thông tin, từ các chủ đề chung đến các chi tiết cụ thể, góp phần vào hiệu suất mạnh tổng thể của nó.

4.1 ĐÓNG GÓP CỦA CẤU TRÚC CÂY

Chúng tôi kiểm tra đóng góp của mỗi lớp nút đối với khả năng truy xuất của RAPTOR. Chúng tôi đã đặt giả thuyết rằng các nút trên đóng vai trò quan trọng trong việc xử lý các truy vấn chủ đề hoặc đa bước yêu cầu hiểu rộng hơn về văn bản.

8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 6: So sánh hiệu suất trên bộ dữ liệu NarrativeQA trên nhiều mô hình, tập trung vào bốn chỉ số: ROUGE-L, BLEU-1, BLEU-4 và METEOR. RAPTOR, khi kết hợp với UnifiedQA 3B, không chỉ vượt trội hơn các phương pháp truy xuất như BM25 và DPR mà còn thiết lập tối ưu mới trong chỉ số METEOR.

Model ROUGE-L BLEU-1 BLEU-4 METEOR
BiDAF (Kočiský et al., 2018) 6.2 5.7 0.3 3.7
BM25 + BERT (Mou et al., 2020) 15.5 14.5 1.4 5.0
Recursively Summarizing Books (Wu et al., 2021) 21.6 22.3 4.2 10.6
Retriever + Reader (Izacard & Grave, 2022) 32.0 35.3 7.5 11.1
RAPTOR + UnifiedQA 30.8 23.5 6.4 19.1

Bảng 7: Độ chính xác của bộ dữ liệu QuALITY trên cả tập kiểm tra tổng thể và tập con thách thức khó hơn. GPT-4 với RAPTOR thiết lập tối ưu mới.

Model Độ chính xác
Tập kiểm tra Tập con khó
Longformer-base (Beltagy et al., 2020) 39.5 35.3
DPR và DeBERTaV3-large (Pang et al., 2022) 55.4 46.1
CoLISA (DeBERTaV3-large) (Dong et al., 2023a) 62.3 54.7
RAPTOR + GPT-4 82.6 76.2

Bảng 8: Hiệu suất của RAPTOR khi truy vấn các lớp cây khác nhau cho Story 1 từ bộ dữ liệu QuALITY. Cột đại diện cho các điểm bắt đầu khác nhau (lớp cao nhất) và hàng đại diện cho số lượng lớp được truy vấn khác nhau.

Lớp được truy vấn / Lớp bắt đầu Lớp 0 (Nút lá) Lớp 1 Lớp 2
1 lớp 57.9 57.8 57.9
2 lớp - 52.6 63.15
3 lớp - - 73.68

Chúng tôi đã xác nhận giả thuyết này cả về mặt định lượng và định tính. Chúng tôi trình bày phân tích định tính trong phụ lục G. Để hiểu định lượng về đóng góp của các nút cấp cao, chúng tôi đã sử dụng các câu chuyện từ bộ dữ liệu QuALITY. Cây RAPTOR được xây dựng cho mỗi câu chuyện này, như được mô tả trong Phần 3. Tuy nhiên, trong quá trình truy xuất, chúng tôi giới hạn tìm kiếm đến các tập con lớp khác nhau. Ví dụ, chúng tôi chỉ truy xuất từ các nút lá và mỗi lớp trên, cũng như từ các tập con liền kề khác nhau của các lớp. Chúng tôi trình bày các phát hiện cụ thể cho một câu chuyện trong Bảng 8, tiết lộ rằng tìm kiếm cây đầy đủ, sử dụng tất cả các lớp, vượt trội hơn các chiến lược truy xuất chỉ tập trung vào các lớp cụ thể.

Những phát hiện này làm nổi bật tầm quan trọng của cấu trúc cây đầy đủ trong RAPTOR. Bằng cách cung cấp cả văn bản gốc và tóm tắt cấp cao hơn để truy xuất, RAPTOR có thể xử lý hiệu quả một loạt câu hỏi rộng hơn, từ các truy vấn chủ đề bậc cao đến các câu hỏi hướng chi tiết. Kết quả chi tiết cho các câu chuyện bổ sung và nghiên cứu ablation về đóng góp lớp có thể được tìm thấy trong Phụ lục I.

5 KẾT LUẬN

Trong bài báo này, chúng tôi đã trình bày RAPTOR, một hệ thống truy xuất dựa trên cây mới tăng cường kiến thức tham số của các mô hình ngôn ngữ lớn với thông tin ngữ cảnh ở các mức độ trừu tượng khác nhau. Bằng cách sử dụng các kỹ thuật phân cụm và tóm tắt đệ quy, RAPTOR tạo ra một cấu trúc cây phân cấp có khả năng tổng hợp thông tin qua các phần khác nhau của kho tài liệu truy xuất. Trong giai đoạn truy vấn, RAPTOR tận dụng cấu trúc cây này để truy xuất hiệu quả hơn. Các thí nghiệm có kiểm soát của chúng tôi đã chứng minh rằng RAPTOR không chỉ vượt trội hơn các phương pháp truy xuất truyền thống mà còn thiết lập các benchmark hiệu suất mới trên nhiều tác vụ hỏi đáp.

9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

6 TUYÊN BỐ VỀ KHẢ NĂNG TÁI SẢN XUẤT

Mô hình Ngôn ngữ cho QA và Tóm tắt Bốn mô hình ngôn ngữ được sử dụng trong các thí nghiệm RAPTOR của chúng tôi: GPT-3 và GPT-4 cho các tác vụ QA, và GPT-3.5-turbo cho tóm tắt. Các mô hình gpt-3, gpt-4 và gpt-3.5-turbo có thể được truy cập thông qua các cuộc gọi API (OpenAI API). UnifiedQA, được sử dụng cho các tác vụ QA, có sẵn công khai tại Hugging Face.

Bộ dữ liệu Đánh giá Ba bộ dữ liệu đánh giá được sử dụng trong các thí nghiệm của chúng tôi—QuALITY, QASPER và NarrativeQA—đều có thể truy cập công khai. Những bộ dữ liệu này đảm bảo rằng các kiểm tra truy xuất và QA được tiến hành trong nghiên cứu này có thể được lặp lại.

Mã nguồn Mã nguồn cho RAPTOR sẽ có sẵn công khai tại đây.

TÀI LIỆU THAM KHẢO

Charu C Aggarwal, Alexander Hinneburg, và Daniel A Keim. Về Hành vi Bất ngờ của Các Chỉ số Khoảng cách trong Không gian Nhiều chiều. Trong Database Theory—ICDT 2001: 8th International Conference London, UK, January 4–6, 2001 Proceedings 8, trang 420–434. Springer, 2001. URL https://link.springer.com/chapter/10.1007/3-540-44503-x_27.

Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. CoLT5: Faster long-range transformers với conditional computation. arXiv preprint arXiv:2303.09752, 2023. URL https://arxiv.org/abs/2303.09752.

Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, và Kelvin Guu. Hướng tới việc truy vết kiến thức trong các mô hình ngôn ngữ trở lại dữ liệu đào tạo. Trong Findings of the Association for Computational Linguistics: EMNLP 2022, trang 2429–2446, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.180. URL https://aclanthology.org/2022.findings-emnlp.180.

Stefanos Angelidis và Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment prediction và chúng đều được giám sát yếu. arXiv preprint arXiv:1808.08858, 2018. URL https://arxiv.org/abs/1808.08858.

Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, và Zhiheng Huang. Hybrid hierarchical retrieval cho open-domain question answering. Trong Anna Rogers, Jordan Boyd-Graber, và Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, trang 10680–10689, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL https://aclanthology.org/2023.findings-acl.679.

Iz Beltagy, Matthew E. Peters, và Arman Cohan. Longformer: The Long-document Transformer, 2020. URL https://arxiv.org/abs/2004.05150. arXiv preprint arXiv:2004.05150.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models bằng cách truy xuất từ hàng nghìn tỷ token. Trong International conference on machine learning, trang 2206–2240. PMLR, 2022. URL https://arxiv.org/abs/2112.04426.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language Models are Few-Shot Learners. Trong H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, và H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, trang 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General Intelligence: Early experiments với GPT-4. arXiv preprint arXiv:2303.12712, 2023. URL https://arxiv.org/abs/2303.12712.

Shuyang Cao và Lu Wang. HIBRIDS: Attention với hierarchical biases cho structure-aware long document summarization. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 786–807, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL https://aclanthology.org/2022.acl-long.58.

Danqi Chen, Adam Fisch, Jason Weston, và Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. Trong Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://aclanthology.org/P17-1171.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling Language Modeling với Pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311.

Arman Cohan và Nazli Goharian. Contextualizing citations cho scientific summarization sử dụng word embeddings và domain knowledge. Trong Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, trang 1133–1136, 2017. URL https://dl.acm.org/doi/abs/10.1145/3077136.3080740.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, và Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 2978–2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. FlashAttention: Fast và memory-efficient exact attention với IO-Awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022. URL https://arxiv.org/abs/2205.14135.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, và Matt Gardner. A Dataset of Information-Seeking Questions và Answers Anchored in Research Papers. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 4599–4610, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https://aclanthology.org/2021.naacl-main.365.

Mengxing Dong, Bowei Zou, Yanling Li, và Yu Hong. CoLISA: Inner Interaction via Contrastive Learning cho Multi-choice Reading Comprehension. Trong Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023, Proceedings, Part I, trang 264–278. Springer, 2023a. URL https://link.springer.com/chapter/10.1007/978-3-031-28244-7_17.

Zican Dong, Tianyi Tang, Lunyi Li, và Wayne Xin Zhao. A survey on long text modeling với transformers. arXiv preprint arXiv:2302.14502, 2023b. URL https://arxiv.org/abs/2302.14502.

Tianyu Gao, Howard Yen, Jiatong Yu, và Danqi Chen. Enabling large language models to generate text với citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/abs/2305.14627.

11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, và Yinfei Yang. LongT5: Efficient text-to-text transformer cho long sequences. Trong Findings of the Association for Computational Linguistics: NAACL 2022, trang 724–736, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55. URL https://aclanthology.org/2022.findings-naacl.55.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Mingwei Chang. Retrieval Augmented Language Model Pre-Training. Trong International conference on machine learning, trang 3929–3938. PMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL https://arxiv.org/abs/2203.15556.

Gautier Izacard và Edouard Grave. Distilling Knowledge từ Reader đến Retriever cho Question Answering, 2022. URL https://arxiv.org/abs/2012.04584. arXiv preprint arXiv:2012.04584.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, và Edouard Grave. Few-shot learning với retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022. URL https://arxiv.org/abs/2208.03299.

Zhengbao Jiang, Frank F Xu, Jun Araki, và Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020. URL https://arxiv.org/abs/1911.12543.

Jeff Johnson, Matthijs Douze, và Hervé Jégou. Billion-Scale Similarity Search với GPUs. IEEE Transactions on Big Data, 7(3):535–547, 2019. URL https://arxiv.org/abs/1702.08734.

Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, và Colin Raffel. Large Language Models struggle to learn Long-Tail Knowledge. Trong International Conference on Machine Learning, trang 15696–15707. PMLR, 2023. URL https://proceedings.mlr.press/v202/kandpal23a/kandpal23a.pdf.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và Wen-tau Yih. Dense Passage Retrieval cho Open-Domain Question Answering. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 6769–6781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550.

Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, và Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries với a single QA system. Trong Findings of the Association for Computational Linguistics: EMNLP 2020, trang 1896–1907, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.171.

Omar Khattab và Matei Zaharia. ColBERT: Efficient và effective passage search via contextualized late interaction over bert. Trong Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, trang 39–48, 2020. URL https://arxiv.org/abs/2004.12832.

Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, và Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions of the Association for Computational Linguistics, 6:317–328, 2018. URL https://arxiv.org/abs/1712.07040.

12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-Augmented Generation cho Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401.

Jerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index.

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, và Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. URL https://arxiv.org/abs/2307.03172.

Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, và Philip Yu. Dense hierarchical retrieval cho open-domain question answering. Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, trang 188–200, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19. URL https://aclanthology.org/2021.findings-emnlp.19.

Leland McInnes, John Healy, và James Melville. UMAP: Uniform Manifold Approximation và Projection cho Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.03426. arXiv preprint arXiv:1802.03426.

Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, và Hannaneh Hajishirzi. Joint passage ranking cho diverse multi-answer retrieval. Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 6997–7008, Online và Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560.

Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, và Luke Zettlemoyer. Nonparametric masked language modeling. Trong Findings of the Association for Computational Linguistics: ACL 2023, trang 2097–2118, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https://aclanthology.org/2023.findings-acl.132.

Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, và Chelsea Finn. Memory-based model editing at scale. Trong International Conference on Machine Learning, trang 15817–15831. PMLR, 2022. URL https://proceedings.mlr.press/v162/mitchell22a/mitchell22a.pdf.

Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, và Hui Su. Frustratingly hard evidence retrieval cho QA over books. Trong Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events, trang 108–113, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https://aclanthology.org/2020.nuse-1.13.

Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikrishna Karanam, và Sumit Shekhar. A neural CRF-based hierarchical approach cho linear text segmentation. Trong Findings of the Association for Computational Linguistics: EACL 2023, trang 883–893, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.findings-eacl.65.

Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, và Kyle Lo. A controllable qa-based framework cho decontextualization. arXiv preprint arXiv:2305.14772, 2023. URL https://arxiv.org/pdf/2305.14772.pdf.

OpenAI. GPT-4 Technical Report. ArXiv, abs/2303.08774, 2023. URL https://arxiv.org/abs/2303.08774.

Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, và Samuel Bowman. QuALITY: Question Answering với Long Input Texts, Yes! Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 5336–5358, Seattle, United States, July 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.naacl-main.391.

13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, và Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019. URL https://arxiv.org/abs/1909.01066.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, Analysis & Insights từ Training Gopher. arXiv preprint arXiv:2112.11446, 2021. URL https://arxiv.org/abs/2112.11446.

Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, và Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083, 2023. URL https://arxiv.org/abs/2302.00083.

Nils Reimers và Iryna Gurevych. Sentence-BERT: Sentence embeddings sử dụng Siamese BERT-networks. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing và the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), trang 3982–3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410.

Adam Roberts, Colin Raffel, và Noam Shazeer. How Much Knowledge Can You Pack Into the Parameters of a Language Model? Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 5418–5426, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020.emnlp-main.437.

Stephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 và Beyond. Foundations và Trends in Information Retrieval, 3(4):333–389, 2009. URL https://doi.org/10.1561/1500000019.

Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at TREC-3. Nist Special Publication Sp, 109:109, 1995. URL https://www.microsoft.com/en-us/research/publication/okapi-at-trec-3/.

Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, và Manzil Zaheer. Questions are all you need to train a dense passage retriever. Transactions of the Association for Computational Linguistics, 11:600–616, 2023. doi: 10.1162/tacl_a_00564. URL https://aclanthology.org/2023.tacl-1.35.

Gideon Schwarz. Estimating the Dimension of a Model. The annals of statistics, trang 461–464, 1978. URL https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full.

Karen Spärck Jones. A Statistical Interpretation of Term Specificity và its Application in Retrieval. Journal of documentation, 28(1):11–21, 1972. URL https://doi.org/10.1108/eb026526.

Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, và Mohit Iyyer. Do long-range language models actually use long-range context? Trong Marie-Francine Moens, Xuanjing Huang, Lucia Specia, và Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 807–822, Online và Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.62. URL https://aclanthology.org/2021.emnlp-main.62.

Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, và Denny Zhou. Recitation-augmented language models. arXiv preprint arXiv:2210.01296, 2022. URL https://arxiv.org/abs/2210.01296.

14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Alon Talmor, Yanai Elazar, Yoav Goldberg, và Jonathan Berant. oLMpics– on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8: 743–758, 2020. URL https://arxiv.org/abs/1912.13283.

Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models với retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023. URL https://arxiv.org/abs/2304.06762.

Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, và Paul Christiano. Recursively Summarizing Books với Human Feedback, 2021. URL https://arxiv.org/abs/2109.10862.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, và Quoc V. Le. QANet: Combining Local Convolution với Global Self-Attention cho Reading Comprehension, 2018. URL https://arxiv.org/abs/1804.09541. arXiv preprint arXiv:1804.09541.

Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, và Meng Jiang. Generate rather than retrieve: Large Language Models are strong context generators, 2022. URL https://arxiv.org/abs/2209.10063.

Shiyue Zhang, David Wan, và Mohit Bansal. Extractive is not faithful: An investigation of broad unfaithfulness problems trong extractive summarization. Trong Anna Rogers, Jordan Boyd-Graber, và Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 2153–2174, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.120. URL https://aclanthology.org/2023.acl-long.120.

A KHẢ NĂNG MỞ RỘNG VÀ HIỆU QUẢ TÍNH TOÁN CỦA QUÁ TRÌNH XÂY DỰNG CÂY

Để đánh giá hiệu quả tính toán và hiệu quả chi phí của quá trình xây dựng cây của RAPTOR, chúng tôi đã tiến hành các thí nghiệm trên laptop dành cho người tiêu dùng, cụ thể là Apple M1 Mac với 16GB RAM. Những thí nghiệm này nhằm chứng minh khả năng mở rộng và tính khả thi của RAPTOR trên phần cứng điển hình. Chúng tôi thay đổi độ dài bối cảnh từ 12.500 đến 78.000 token và đo cả chi phí token và thời gian cần thiết để hoàn thành quá trình xây dựng cây, từ phân chia và nhúng ban đầu đến xây dựng nút gốc cuối cùng.

Hình 5: Chi phí token như một hàm của độ dài tài liệu cho QASPER, NarrativeQA và QuALITY. Chi phí xây dựng cây RAPTOR mở rộng tuyến tính với độ dài tài liệu cho mỗi bộ dữ liệu.

Chi phí Token Chúng tôi nghiên cứu thực nghiệm mối quan hệ giữa độ dài tài liệu ban đầu và tổng số token được chi tiêu trong quá trình xây dựng cây, bao gồm cả token prompt và completion. Độ dài tài liệu thay đổi đáng kể qua ba bộ dữ liệu được kiểm tra: QuALITY, QASPER và NarrativeQA. Hình 5 minh họa mối tương quan tuyến tính rõ ràng giữa độ dài tài liệu ban đầu và tổng chi phí token, nhấn mạnh rằng RAPTOR duy trì mở rộng token tuyến tính bất kể độ phức tạp hoặc độ dài tài liệu.

15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 6: Thời gian xây dựng như một hàm của độ dài tài liệu cho các tài liệu lên đến 80.000 token. Thời gian xây dựng cây RAPTOR mở rộng tuyến tính với độ dài tài liệu cho mỗi bộ dữ liệu.

Thời gian Xây dựng Chúng tôi cũng quan sát thực nghiệm một xu hướng tuyến tính nhất quán giữa độ dài tài liệu và thời gian xây dựng, như được thể hiện trong Hình 6. Điều này cho thấy RAPTOR mở rộng tuyến tính về mặt thời gian, làm cho nó trở thành một giải pháp khả thi để xử lý hiệu quả các kho tài liệu lớn có độ dài khác nhau.

Kết luận Nhìn chung, kết quả thực nghiệm của chúng tôi chỉ ra rằng RAPTOR mở rộng cả về token được chi tiêu và thời gian xây dựng. Ngay cả khi độ phức tạp và khối lượng của văn bản đầu vào tăng, chi phí xây dựng cây mở rộng một cách có thể dự đoán và tuyến tính. Điều này chứng minh rằng RAPTOR có hiệu quả tính toán và phù hợp để xử lý các kho tài liệu lớn và đa dạng.

B NGHIÊN CỨU ABLATION VỀ CƠ CHẾ PHÂN CỤM TRONG RAPTOR

Để đánh giá hiệu quả của cơ chế phân cụm trong phương pháp RAPTOR của chúng tôi, chúng tôi đã tiến hành nghiên cứu ablation trên bộ dữ liệu QuALITY. Nghiên cứu này so sánh hiệu suất của RAPTOR với mã hóa và tóm tắt kiểu cây cân bằng của các đoạn liền kề, trái ngược với phương pháp phân cụm tiêu chuẩn của chúng tôi.

B.1 PHƯƠNG PHÁP LUẬN

Cả hai cấu hình trong nghiên cứu ablation này đều sử dụng nhúng SBERT và UnifiedQA để duy trì tính nhất quán trong truy xuất. Đối với RAPTOR, chúng tôi sử dụng quá trình phân cụm và tóm tắt điển hình của chúng tôi. Ngược lại, thiết lập thay thế liên quan đến việc tạo cây cân bằng bằng cách mã hóa và tóm tắt đệ quy các đoạn văn bản liền kề. Chúng tôi xác định kích thước cửa sổ cho thiết lập này dựa trên kích thước cụm trung bình được quan sát trong RAPTOR, khoảng 6.7 nút. Do đó, chúng tôi chọn kích thước cửa sổ là 7 nút. Phương pháp cây thu gọn được áp dụng để truy xuất trong cả hai mô hình.

B.2 KẾT QUẢ & THẢO LUẬN

Kết quả của nghiên cứu ablation được trình bày trong bảng 9. Kết quả từ nghiên cứu ablation này chỉ ra rõ ràng sự cải thiện về độ chính xác khi sử dụng cơ chế phân cụm của RAPTOR so với phương pháp cây dựa trên gần đây. Phát hiện này xác thực giả thuyết của chúng tôi rằng chiến lược phân cụm trong RAPTOR hiệu quả hơn trong việc nắm bắt nội dung đồng nhất để tóm tắt, từ đó nâng cao hiệu suất truy xuất tổng thể.

16

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 9: Kết quả nghiên cứu ablation so sánh RAPTOR với phương pháp cây dựa trên gần đây

Cấu hình Độ chính xác
RAPTOR + nhúng SBERT + UnifiedQA 56.6%
Cây dựa trên gần đây + nhúng SBERT + UnifiedQA 55.8%

C THỐNG KÊ BỘ DỮ LIỆU VÀ TỶ LỆ NÉN

Tỷ lệ trung bình của độ dài tóm tắt so với tổng độ dài nút con trên tất cả các bộ dữ liệu là 0.28, cho thấy tỷ lệ nén 72%. Trung bình, độ dài tóm tắt là 131 token, và độ dài nút con trung bình là 86 token. Dưới đây là số liệu thống kê chi tiết cho tất cả ba bộ dữ liệu:

Bảng 10: Thống kê về Độ dài Tóm tắt Trung bình và Độ dài Nút Con Qua Các Bộ dữ liệu

Bộ dữ liệu Độ dài Tóm tắt Trung bình (token) Độ dài Văn bản Nút Con Trung bình (token) Số Nút Con Trung bình Mỗi Cha Tỷ lệ Nén Trung bình (%)
Tất cả Bộ dữ liệu 131 85.6 6.7 .28
QuALITY 124.4 87.9 5.7 .28
NarrativeQA 129.7 85.5 6.8 .27
QASPER 145.9 86.2 5.7 .35

D PROMPT TÓM TẮT

Bảng 11 cho thấy prompt được sử dụng để tóm tắt.

Bảng 11: Prompt cho Tóm tắt

Vai trò Nội dung
system Bạn là một Cổng Tóm tắt Văn bản
user Viết một tóm tắt về nội dung sau, bao gồm càng nhiều chi tiết quan trọng càng tốt: {context}:

E PHÂN TÍCH ẢO GIÁC

Để đánh giá chất lượng và độ chính xác của các tóm tắt trong mô hình RAPTOR của chúng tôi, chúng tôi đã tiến hành phân tích tập trung vào ảo giác trong các tóm tắt được tạo ra. Các tóm tắt được tạo ra bởi gpt-3.5-turbo và sau đó được chú thích để định lượng tỷ lệ ảo giác, để kiểm tra xem những sai lệch như vậy có lan truyền đến các nút cha hay không, và để đánh giá tác động của chúng đến các tác vụ hỏi đáp (QA).

E.1 PHƯƠNG PHÁP LUẬN

Chúng tôi lấy mẫu ngẫu nhiên 150 nút qua 40 câu chuyện và đánh giá chúng về ảo giác. Chiến lược lấy mẫu này cung cấp cái nhìn rộng về hiệu suất của mô hình qua các bối cảnh khác nhau. Mỗi nút được chú thích bằng tay và xác định xem nó có chứa ảo giác hay không.

E.2 PHÁT HIỆN

Trong số 150 nút được lấy mẫu, 4% (6 nút) chứa một số dạng ảo giác. Thường xuyên nhất, những ảo giác này bắt nguồn từ việc mô hình thêm thông tin nhỏ có thể từ dữ liệu đào tạo của nó mà không có trong văn bản được tóm tắt, hoặc từ việc suy luận không chính xác một số thông tin khi tạo tóm tắt.

17

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Ví dụ:
Văn bản của các nút con:
"Và bạn sẽ đi cùng tôi đến với người dân của tôi? Chúng ta có thể sống ở đây giữa họ, và bạn sẽ là một chiến binh vĩ đại–ôi, khi Jor chết bạn thậm chí có thể là thủ lĩnh, vì không có ai mạnh mẽ như chiến binh của tôi..."Nhưng cha bạn sẽ không cho phép điều đó–Jor, cha tôi, Thủ lĩnh Tối cao của người Galus, sẽ không cho phép điều đó, vì giống như tôi bạn là cos-ata-lo. Ôi, Co-Tan, nếu chúng ta có thể!... Bradley nhận thấy rằng cô ấy nói bằng tiếng Anh–tiếng Anh hỏng như của Co-Tan nhưng cũng hấp dẫn.

Tóm tắt được tìm thấy trong cha của nút đó:
Nhân vật chính, Bradley, được Co-Tan yêu cầu ở lại với người dân của cô ấy và trở thành một chiến binh vĩ đại, nhưng anh ấy từ chối và phải trở về đất nước của mình. Tom Billings của Santa Monica đến và nói với họ rằng anh ấy đến để tìm kiếm một người tên Bowen J. Tyler, Jr. Ajor, em gái của Co-Tan, hào hứng về khả năng đi đến đất nước của Tom để xem những điều kỳ lạ và tuyệt vời...

Ảo giác ở đây là tóm tắt nói rằng Jr. Ajor và Co-Tan là chị em, nhưng không nói rõ hoặc ngụ ý điều này.

Khi xem xét tất cả các nút cha, chúng tôi thấy rằng ảo giác không lan truyền đến các lớp cao hơn. Nói chung, ảo giác là nhỏ và không thay đổi cách diễn giải chủ đề của văn bản.

E.3 TÁC ĐỘNG ĐẾN CÁC TÁC VỤ QA

Trong các phát hiện của chúng tôi, ảo giác không có tác động có thể nhận biết đến hiệu suất của các tác vụ QA. Điều này cho thấy ảo giác không phải là mối quan tâm lớn đối với thành phần tóm tắt trong kiến trúc RAPTOR của chúng tôi.

F MÃ GIẢ CHO CÁC PHƯƠNG PHÁP TRUY XUẤT

Thuật toán 1 Thuật toán Duyệt Cây
function TRAVERSE_TREE(tree, query, k)
    S_current ← tree.layer[0]
    for layer in range(tree.num_layers) do
        top_k ← []
        for node in S_current do
            score ← dot_product(query, node)
            top_k.append((node, score))
        end for
        S_layer ← sorted(top_k)[:k].nodes
        S_current ← S_layer
    end for
    return S_0 ∪ S_1 ∪ S_2 ∪ ... ∪ S_k
end function

G PHÂN TÍCH ĐỊNH TÍNH

Để kiểm tra định tính quá trình truy xuất của RAPTOR, chúng tôi thử nghiệm nó trên các câu hỏi chủ đề, đa bước về phiên bản 1500 từ của câu chuyện cổ tích Cinderella. Chúng tôi so sánh bối cảnh được truy xuất bởi RAPTOR với bối cảnh được truy xuất bởi Dense Passage Retrieval (DPR). Hình 4 trong bài báo chính chi tiết quá trình truy xuất trong cấu trúc cây của RAPTOR cho hai câu hỏi. Các nút mà RAPTOR chọn cho mỗi câu hỏi được làm nổi bật, trong khi các nút lá mà DPR chọn cho cùng câu hỏi được chỉ ra bằng mũi tên. So sánh này minh họa lợi thế của cấu trúc cây của RAPTOR. RAPTOR chọn các nút từ các lớp khác nhau tùy thuộc vào mức độ chi tiết được yêu cầu bởi

18

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Thuật toán 2 Thuật toán Cây Thu gọn
function COLLAPSED_TREE(tree, query, k, max_tokens)
    tree ← flatten(tree) ▷Làm phẳng cây thành 1D
    top_nodes ← []
    for node in tree do
        top_nodes.append((node, dot_product(query, node))
    end for
    top_nodes ← sorted(top_nodes)
    result ← []
    total_tokens ← 0
    for node in top_nodes do
        if total_tokens + node.token_size < max_tokens then
            result.append(node)
        end if
        total_tokens ← total_tokens + node.token_size
    end for
    return result
end function

Câu hỏi: Chủ đề trung tâm của câu chuyện là gì?
RAPTOR Bà tiên đỡ đầu giúp Cinderella tham dự buổi dạ hội bằng cách biến đổi những mảnh vải rách của cô ấy... Cinderella gây ấn tượng với Hoàng tử tại buổi dạ hội... cô ấy mất tích thời gian và phải chạy về nhà một mình trong bóng tối. Hoàng tử không thể tìm thấy Cinderella và đi tìm cô ấy... Cô ấy tha thứ cho các chị gái của mình, và luôn đối xử với họ rất tử tế, và Hoàng tử có lý do lớn để vui mừng vì anh ấy đã tìm thấy chiếc giày thủy tinh.
DPR Hai con chuột được biến thành người đánh xe; bốn con châu chấu thành những con ngựa trắng. Tiếp theo, Bà tiên chạm vào những mảnh vải rách của Cinderella, và chúng trở thành những chiếc áo satin giàu có, được trang trí bằng ren điểm... Chao ôi! đồng hồ đã điểm Mười một giờ. Cô ấy không tìm thấy xe ngựa, chỉ có một quả bí, và con chuột cùng những con chuột chạy nhanh chóng khi chúng nhìn thấy cô ấy; trong khi tất cả bộ váy đẹp của cô ấy biến thành đồ rách, và cô ấy phải chạy về nhà một mình... Họ nói với cô ấy một Công chúa xinh đẹp đã có mặt tại buổi dạ hội, người mà Hoàng tử rất thích thú. Họ không biết đó là Cinderella.

Câu hỏi: Cinderella tìm thấy kết thúc hạnh phúc như thế nào?
RAPTOR Câu chuyện về Cinderella liên quan đến bà đỡ đầu của cô ấy, một bà tiên, người biến một quả bí thành một chiếc xe ngựa lộng lẫy bằng cây đũa phép của mình và cho phép Cinderella tham dự buổi dạ hội. Tuy nhiên, Cinderella phải trở về nhà trước khi đồng hồ điểm mười một giờ hoặc chiếc váy của cô ấy sẽ biến trở lại thành đồ rách... Cinderella gây ấn tượng với Hoàng tử tại buổi dạ hội nhưng rời đi trước khi anh ấy có thể biết cô ấy là ai... Hoàng tử tìm kiếm chủ nhân của chiếc giày thủy tinh bị mất và phát hiện nó thuộc về Cinderella. Cô ấy tha thứ cho các chị gái của mình và Hoàng tử vui mừng vì đã tìm thấy cô ấy.
DPR đồng hồ đã điểm Mười một giờ... Hoàng tử rất ngạc nhiên khi anh ấy lại nhớ Cinderella, và rời buổi dạ hội, đi tìm cô ấy... Bà tiên chạm vào những mảnh vải rách của Cinderella, và chúng trở thành những chiếc áo satin giàu có, được trang trí bằng ren điểm... Đôi giày cũ của cô ấy trở thành một đôi giày thủy tinh quyến rũ, sáng như kim cương. "Bây giờ hãy đi dạ hội, yêu quý của ta," bà ấy nói, "và tận hưởng. Nhưng nhớ rằng, con phải rời khỏi phòng trước khi đồng hồ điểm mười một giờ. Nếu con không làm vậy chiếc váy của con sẽ trở lại thành đồ rách ban đầu."

Bảng 12: Đoạn trích liên quan từ văn bản được truy xuất bởi RAPTOR và DPR cho các câu hỏi về câu chuyện cổ tích Cinderella.

câu hỏi đang có. Hơn nữa, thông tin sẽ được truy xuất bởi DPR thường hơn không được bao gồm trong bối cảnh được truy xuất bởi RAPTOR, hoặc trực tiếp như một nút lá hoặc gián tiếp như một phần của tóm tắt từ lớp cao hơn.

"Câu hỏi đầu tiên chúng tôi kiểm tra là "Cinderella tìm thấy kết thúc hạnh phúc như thế nào?", một câu hỏi đa bước được trả lời tốt nhất bằng cách tổng hợp thông tin từ các đoạn văn bản khác nhau. Để kiểm soát khả năng quen thuộc tiềm năng của mô hình ngôn ngữ với câu chuyện Cinderella, chúng tôi hướng dẫn nó chỉ dựa vào thông tin được truy xuất cho câu trả lời của nó. Bảng 13 cho thấy văn bản được truy xuất bởi cả RAPTOR và DPR cho câu hỏi này. Bối cảnh của RAPTOR mô tả ngắn gọn hành trình của Cinderella đến hạnh phúc, trong khi các nút lá của DPR chủ yếu tập trung vào sự biến đổi ban đầu của cô ấy. Sự khác biệt trong thông tin được truy xuất tác động đáng kể đến các tác vụ hạ nguồn. Khi GPT-4 được cung cấp bối cảnh của RAPTOR, nó tạo ra một câu trả lời chi tiết: "Cinderella tìm thấy kết thúc hạnh phúc khi Hoàng tử tìm kiếm chủ nhân của chiếc giày thủy tinh bị mất và phát hiện nó thuộc về Cinderella. Cuối cùng họ kết hôn, biến đổi cuộc sống của Cinderella trở nên tốt đẹp hơn." Ngược lại, khi sử dụng bối cảnh của DPR, GPT-4 nói: "Dựa trên bối cảnh đã cho, không thể xác định Cinderella tìm thấy kết thúc hạnh phúc như thế nào, vì văn bản thiếu thông tin về kết luận của câu chuyện."

Câu hỏi thứ hai chúng tôi kiểm tra là "Chủ đề trung tâm của câu chuyện là gì?", một câu hỏi chủ đề đòi hỏi hiểu biết toàn diện về toàn bộ văn bản. Văn bản được truy xuất bởi RAPTOR và DPR cho câu hỏi này được thể hiện trong Bảng 13. Văn bản được truy xuất bởi RAPTOR chứa các mô tả ngắn về tất cả các phần chính của câu chuyện, trong khi văn bản được truy xuất bởi DPR chứa các mô tả chi tiết về một tập con hẹp của câu chuyện. Một lần nữa, sự khác biệt trong các cơ chế truy xuất ảnh hưởng đến hiệu suất của GPT-4 khi trả lời câu hỏi. Với bối cảnh của DPR, nó xuất ra "Chủ đề trung tâm của câu chuyện là sự biến đổi và sức mạnh của vẻ đẹp bên trong, khi Cinderella, một cô gái tốt bụng và khiêm tốn, được biến đổi kỳ diệu thành một công chúa xinh đẹp, thu hút sự chú ý và ngưỡng mộ của Hoàng tử và những người khác tại buổi dạ hội." Câu trả lời này chỉ tính đến phần đầu của câu chuyện, cho đến khi Cinderella lần đầu gặp hoàng tử. Ngược lại, với bối cảnh của RAPTOR, GPT-4 xuất ra "Chủ đề trung tâm của câu chuyện là sự biến đổi và vượt qua nghịch cảnh, khi Cinderella, với sự giúp đỡ của Bà tiên đỡ đầu, biến đổi từ một cô gái bị ngược đãi và chán nản thành một phụ nữ trẻ xinh đẹp và tự tin, cuối cùng tìm thấy hạnh phúc và tình yêu với Hoàng tử." Đây là một câu trả lời hoàn chỉnh hơn, thể hiện sự hiểu biết toàn diện về câu chuyện.

Phân tích định tính này chỉ ra rằng RAPTOR vượt trội hơn các cơ chế truy xuất trước đó vì thông tin mà nó truy xuất liên quan và đầy đủ hơn, cho phép hiệu suất tốt hơn trên các tác vụ hạ nguồn.

Chúng tôi cũng tạo ra một câu chuyện 2600 từ cùng với các câu hỏi về tường thuật và chủ đề của nó. Một đoạn trích từ câu chuyện có mặt dưới đây và PDF đầy đủ của câu chuyện này được liên kết tại đây. Đối với các câu hỏi như "Chủ đề trung tâm của câu chuyện là gì?", một nút cấp cao được truy xuất bao gồm câu: "Câu chuyện này nói về sức mạnh của kết nối con người... truyền cảm hứng và nâng đỡ lẫn nhau khi họ theo đuổi đam mê của mình." Tóm tắt này, không có rõ ràng trong văn bản gốc, gần như trả lời trực tiếp câu hỏi.

Đoạn trích từ "The Eager Writer":
"Đam mê viết lách của Ethan luôn là một phần của anh ấy. Khi còn nhỏ, anh ấy thường viết nguệch ngoạc những câu chuyện và bài thơ trong cuốn sổ tay của mình, và khi anh ấy lớn lên, tình yêu viết lách của anh ấy chỉ tăng thêm. Những buổi tối của anh ấy thường được dành trong ánh sáng mờ ảo của căn phòng, gõ phím trên laptop. Anh ấy gần đây đã nhận một công việc là người viết nội dung cho một công ty marketing trực tuyến để trả các hóa đơn, nhưng trái tim anh ấy vẫn khao khát thế giới kể chuyện. Tuy nhiên, giống như nhiều nhà văn đầy hoài bão, anh ấy đấu tranh để tìm chỗ đứng trong ngành. Anh ấy nhận một công việc là người viết nội dung cho một công ty marketing trực tuyến, nhưng anh ấy ngày càng rõ ràng rằng đây không phải là con đường anh ấy muốn theo đuổi. Chính trong thời gian này anh ấy tình cờ gặp ứng dụng Pathways. Ứng dụng này cung cấp một nền tảng cho những người trong nghề tương tự kết nối và chia sẻ kiến thức, và anh ấy coi đó là cơ hội cuối cùng để kết nối với những người khác có chung đam mê viết lách. Ethan thấy cơ hội gặp gỡ những người khác có chung đam mê và có thể cung cấp hướng dẫn và cố vấn. Anh ấy nhanh chóng đăng ký và ngạc nhiên với số lượng nhà văn anh ấy tìm thấy trên nền tảng, từ các chuyên gia được thiết lập tốt đến những người mới bắt đầu mới bắt đầu trong kinh doanh."

H SCRIPT ĐÁNH GIÁ NARRATIVE QA

Chúng tôi đã thực hiện một số thay đổi đối với script đánh giá của AllenNLP3 để phù hợp hơn với nhu cầu đánh giá của chúng tôi:
•Thêm Làm mịn: Làm mịn được tích hợp để xử lý các trường hợp điểm BLEU bằng không, do không có khớp n-gram nào xảy ra trong văn bản tham chiếu. Điểm BLEU bằng không làm lệch kết quả, dẫn đến đánh giá quá khắt khe cho các cụm từ hiếm hoặc mới. Bằng cách thêm hàm làm mịn, chúng tôi ngăn điểm BLEU giảm xuống không, cung cấp đánh giá công bằng hơn.

3docs.allennlp.org/models/main/models/rc/tools/narrativeqa/

20

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

•Thay đổi Trọng số BLEU-4: Script gốc áp dụng trọng số 1 cho n-gram bậc cao nhất (4-gram) và 0 cho phần còn lại trong tính toán BLEU-4 của nó (tức là, weights=(0, 0, 0, 1)). Phương pháp này có thể tập trung quá mức vào khớp 4-gram trong khi bỏ qua khớp bậc thấp hơn. Để cung cấp đánh giá cân bằng hơn, chúng tôi phân phối đều trọng số qua tất cả các cấp n-gram, thay đổi trọng số cho tính toán BLEU-4 thành (0.25, 0.25, 0.25, 0.25).

•Tokenization trước Mapping trong Tính toán METEOR: Script gốc sử dụng phương pháp chia tách và ánh xạ đơn giản cho tính toán METEOR. Chúng tôi đã sửa điều này bằng cách đầu tiên tokenize văn bản và sau đó ánh xạ các token. Sửa đổi này cải thiện độ chính xác của tính toán METEOR bằng cách tính đến các ranh giới ngôn ngữ chính xác của từ.

Câu hỏi: Chủ đề trung tâm của câu chuyện là gì?
RAPTOR Bà tiên đỡ đầu giúp Cinderella tham dự buổi dạ hội bằng cách biến đổi những mảnh vải rách của cô ấy... Cinderella gây ấn tượng với Hoàng tử tại buổi dạ hội... cô ấy mất tích thời gian và phải chạy về nhà một mình trong bóng tối. Hoàng tử không thể tìm thấy Cinderella và đi tìm cô ấy... Cô ấy tha thứ cho các chị gái của mình, và luôn đối xử với họ rất tử tế, và Hoàng tử có lý do lớn để vui mừng vì anh ấy đã tìm thấy chiếc giày thủy tinh.
DPR Hai con chuột được biến thành người đánh xe; bốn con châu chấu thành những con ngựa trắng. Tiếp theo, Bà tiên chạm vào những mảnh vải rách của Cinderella, và chúng trở thành những chiếc áo satin giàu có, được trang trí bằng ren điểm... Chao ôi! đồng hồ đã điểm Mười một giờ. Cô ấy không tìm thấy xe ngựa, chỉ có một quả bí, và con chuột cùng những con chuột chạy nhanh chóng khi chúng nhìn thấy cô ấy; trong khi tất cả bộ váy đẹp của cô ấy biến thành đồ rách, và cô ấy phải chạy về nhà một mình... Họ nói với cô ấy một Công chúa xinh đẹp đã có mặt tại buổi dạ hội, người mà Hoàng tử rất thích thú. Họ không biết đó là Cinderella.

Câu hỏi: Cinderella tìm thấy kết thúc hạnh phúc như thế nào?
RAPTOR Câu chuyện về Cinderella liên quan đến bà đỡ đầu của cô ấy, một bà tiên, người biến một quả bí thành một chiếc xe ngựa lộng lẫy bằng cây đũa phép của mình và cho phép Cinderella tham dự buổi dạ hội. Tuy nhiên, Cinderella phải trở về nhà trước khi đồng hồ điểm mười một giờ hoặc chiếc váy của cô ấy sẽ biến trở lại thành đồ rách... Cinderella gây ấn tượng với Hoàng tử tại buổi dạ hội nhưng rời đi trước khi anh ấy có thể biết cô ấy là ai... Hoàng tử tìm kiếm chủ nhân của chiếc giày thủy tinh bị mất và phát hiện nó thuộc về Cinderella. Cô ấy tha thứ cho các chị gái của mình và Hoàng tử vui mừng vì đã tìm thấy cô ấy.
DPR đồng hồ đã điểm Mười một giờ... Hoàng tử rất ngạc nhiên khi anh ấy lại nhớ Cinderella, và rời buổi dạ hội, đi tìm cô ấy... Bà tiên chạm vào những mảnh vải rách của Cinderella, và chúng trở thành những chiếc áo satin giàu có, được trang trí bằng ren điểm... Đôi giày cũ của cô ấy trở thành một đôi giày thủy tinh quyến rũ, sáng như kim cương. "Bây giờ hãy đi dạ hội, yêu quý của ta," bà ấy nói, "và tận hưởng. Nhưng nhớ rằng, con phải rời khỏi phòng trước khi đồng hồ điểm mười một giờ. Nếu con không làm vậy chiếc váy của con sẽ trở lại thành đồ rách ban đầu."

Bảng 13: Đoạn trích liên quan từ văn bản được truy xuất bởi RAPTOR và DPR cho các câu hỏi về câu chuyện cổ tích Cinderella.

I PHÂN TÍCH CÁC LỚP KHÁC NHAU VỀ HIỆU SUẤT CỦA RAPTOR

I.1 CÁC LỚP KHÁC NHAU TÁC ĐỘNG ĐẾN HIỆU SUẤT NHƯ THẾ NÀO?

Trong phần này, chúng tôi trình bày phân tích chi tiết về hiệu suất truy xuất của RAPTOR khi truy vấn các lớp khác nhau của cấu trúc cây phân cấp cho các câu chuyện khác nhau. Những bảng này xác nhận tính hữu ích của cấu trúc đa lớp của RAPTOR cho các yêu cầu truy vấn đa dạng.

Bảng 14: Hiệu suất của RAPTOR khi truy vấn các lớp khác nhau của cây cho Story 2.

Lớp được truy vấn / Lớp bắt đầu Lớp 0 (Nút lá) Lớp 1 Lớp 2
1 lớp 58.8 47.1 41.1
2 lớp - 64.7 52.9
3 lớp - - 47.1

21

--- TRANG 21 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 7: Biểu đồ tần số cho thấy tỷ lệ phần trăm các nút được truy xuất từ các lớp khác nhau của cây RAPTOR qua ba bộ dữ liệu (NarrativeQA, Quality, và Qasper) sử dụng ba bộ truy xuất (SBERT, BM25, và DPR). Dữ liệu chỉ ra rằng một phần đáng kể các nút đóng góp vào truy xuất cuối cùng đến từ các lớp không phải lá, với tỷ lệ đáng chú ý từ lớp thứ nhất và thứ hai, làm nổi bật tầm quan trọng của tóm tắt phân cấp của RAPTOR trong quá trình truy xuất.

Bảng 15: Hiệu suất của RAPTOR khi truy vấn các lớp khác nhau của cây cho Story 3.

Lớp được truy vấn / Lớp bắt đầu Lớp 0 (Nút lá) Lớp 1 Lớp 2
1 lớp 66.6 61.1 61.1
2 lớp - 66.6 66.6
3 lớp - - 83.3

Bảng 16: Hiệu suất của RAPTOR khi truy vấn các lớp khác nhau của cây cho Story 4.

Lớp được truy vấn / Lớp bắt đầu Lớp 0 (Nút lá) Lớp 1
1 lớp 94.7 84.2
2 lớp - 89.4

Bảng 17: Hiệu suất của RAPTOR khi truy vấn các lớp khác nhau của cây cho Story 5.

Lớp được truy vấn / Lớp bắt đầu Lớp 0 (Nút lá) Lớp 1
1 lớp 57.9 47.3
2 lớp - 68.4

I.2 CÁC NÚT ĐƯỢC TRUY XUẤT ĐẾN TỪ LỚP NÀO?

Chúng tôi tiến hành thêm nghiên cứu ablation qua tất cả ba bộ dữ liệu và qua ba bộ truy xuất khác nhau với RAPTOR với truy xuất cây thu gọn để kiểm tra các lớp mà các nút được truy xuất bắt nguồn. Chúng tôi quan sát rằng từ 18.5% đến 57% các nút được truy xuất đến từ các nút không phải lá. Như được minh họa trong Hình 7, mô hình truy xuất qua các lớp tiết lộ tầm quan trọng của cấu trúc cây đa lớp của RAPTOR. Đáng chú ý, một tỷ lệ đáng kể các nút được truy xuất bởi RAPTOR sử dụng bộ truy xuất DPR cho bộ dữ liệu NarrativeQA đến từ lớp thứ nhất và thứ hai của cây, trái ngược với các nút lá. Mô hình này nhất quán qua các bộ dữ liệu và bộ truy xuất khác, mặc dù với tỷ lệ phần trăm khác nhau.

Bảng 18: Tỷ lệ phần trăm các nút từ các nút không phải lá qua các bộ dữ liệu và bộ truy xuất khác nhau

Bộ dữ liệu DPR SBERT BM25
NarrativeQA 57.36% 36.78% 34.96%
Quality 32.28% 24.41% 32.36%
Qasper 22.93% 18.49% 22.76%

22

--- TRANG 22 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 19: Tỷ lệ phần trăm các nút từ các lớp khác nhau với DPR làm bộ truy xuất

Lớp NarrativeQA Quality Qasper
0 42.64% 67.71% 77.07%
1 45.00% 29.43% 21.88%
2 10.57% 2.85% 1.05%
3 1.78% - -
4 0.003% - -

Bảng 20: Tỷ lệ phần trăm các nút từ các lớp khác nhau với SBERT làm bộ truy xuất

Lớp NarrativeQA Quality Qasper
0 63.22% 75.59% 81.51%
1 31.51% 22.78% 17.84%
2 4.85% 1.63% 0.65%
3 0.42% - -

Bảng 21: Tỷ lệ phần trăm các nút từ các lớp khác nhau với BM25 làm bộ truy xuất

Lớp NarrativeQA Quality Qasper
0 65.04% 67.64% 77.24%
1 28.79% 28.85% 21.57%
2 5.36% 3.51% 1.19%
3 0.81% - -

23
