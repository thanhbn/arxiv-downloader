# GLIMMER: bộ sắp xếp lại bộ nhớ tương tác muộn tổng quát

Michiel de Jong∗ †, Yury Zemlyanskiy∗
Nicholas FitzGerald, Fei Sha, Sumit Sanghai, William W. Cohen, Joshua Ainslie
Google Research

Tóm tắt
Tăng cường bộ nhớ là một phương pháp mạnh mẽ để kết hợp hiệu quả thông tin bên ngoài vào các mô hình ngôn ngữ, nhưng dẫn đến hiệu suất giảm so với việc truy xuất văn bản. Công trình gần đây đã giới thiệu LUMEN, một mô hình lai bộ nhớ-truy xuất mà một phần tính toán trước bộ nhớ và cập nhật biểu diễn bộ nhớ tức thời với một bộ mã hóa trực tiếp nhỏ hơn.

Chúng tôi đề xuất GLIMMER, cải thiện phương pháp này thông qua 1) khai thác quyền truy cập miễn phí vào các biểu diễn bộ nhớ mạnh mẽ bằng cách áp dụng một bộ sắp xếp lại nông trên bộ nhớ để cải thiện mạnh mẽ chất lượng truy xuất với chi phí thấp, và 2) kết hợp huấn luyện đa nhiệm vụ để học một bộ nhớ tổng quát và chất lượng cao hơn cũng như bộ mã hóa trực tiếp. GLIMMER đạt được những cải thiện mạnh về hiệu suất với tốc độ nhanh hơn so với LUMEN và FiD trên điểm chuẩn KILT của các nhiệm vụ chuyên sâu về kiến thức.

1 Giới thiệu

Các mô hình ngôn ngữ tăng cường truy xuất đạt hiệu suất mạnh, nhưng tốn kém về mặt tính toán do cần phải xử lý các đoạn văn được truy xuất. Một khối lượng lớn các công trình cố gắng giảm chi phí đọc các đoạn văn được truy xuất thông qua tính toán có điều kiện (Ainslie et al., 2023b; Varshney et al., 2022; Schuster et al., 2022), sắp xếp lại (Wang et al., 2018; Yu et al., 2022; Wang et al., 2018), hoặc bộ nhớ (de Jong et al., 2022b; Wu et al., 2022a; Li et al., 2022).

Sắp xếp lại cải thiện chất lượng truy xuất và do đó giảm số lượng đoạn văn cần được xử lý bởi bộ đọc. Tuy nhiên, sắp xếp lại neural tốn kém, vì mỗi ứng viên được truy xuất được xử lý bởi một mạng neural. Các bộ sắp xếp lại tương tác muộn (Khattab và Zaharia, 2020; Cohen et al., 2022; MacAvaney et al., 2020) tính toán trước các biểu diễn token trung gian và áp dụng một mô hình neural nhỏ hơn tức thời để kết hợp biểu diễn truy vấn và tài liệu và tạo ra điểm xếp hạng. Tương tác muộn cải thiện mạnh mẽ tốc độ với chi phí lưu trữ và chi phí tính toán trước cũng như máy móc.

Gần đây ý tưởng tương tác muộn cũng đã được áp dụng cho sinh văn bản tăng cường truy xuất: LUMEN (de Jong et al., 2023) nội suy giữa tăng cường bộ nhớ và truy xuất để đạt được sự cân bằng chất lượng-tính toán tốt hơn.

Chúng tôi đề xuất GLIMMER (Bộ sắp xếp lại bộ nhớ tương tác muộn tổng quát), một phương pháp tương tác muộn kết hợp các hướng nghiên cứu này bằng cách thống nhất sắp xếp lại và bộ nhớ thành một mô hình đầu cuối đến đầu cuối duy nhất. Giống như LUMEN, GLIMMER bao gồm một bộ mã hóa bộ nhớ tạo ra các biểu diễn token được tính toán trước cho các tài liệu truy xuất, và một bộ mã hóa trực tiếp kết hợp các biểu diễn của các tài liệu được truy xuất với truy vấn. Sau các lớp đầu tiên của bộ mã hóa trực tiếp, một lớp xếp hạng chọn các đoạn văn liên quan nhất được giữ lại để xử lý thêm. Mô hình được huấn luyện để xếp hạng các đoạn văn theo tính hữu ích cho bộ đọc thông qua một hàm mất mát chưng cất độ phức tạp phụ trợ (Izacard et al., 2022).

GLIMMER cũng cải thiện so với LUMEN bằng cách sử dụng một bộ nhớ tổng quát duy nhất và bộ mã hóa trực tiếp cho tất cả các nhiệm vụ, được huấn luyện với tinh chỉnh đa nhiệm vụ trên các tập dữ liệu chuyên sâu về kiến thức.

Chúng tôi đánh giá trên điểm chuẩn KILT của các nhiệm vụ chuyên sâu về kiến thức (Petroni et al., 2020). Đầu tiên chúng tôi thấy rằng huấn luyện đa nhiệm vụ của bộ nhớ và các bộ mã hóa trực tiếp cải thiện mạnh chất lượng mô hình so với huấn luyện trên một nhiệm vụ duy nhất, đặc biệt khi dành ít khả năng hơn cho bộ mã hóa trực tiếp. Hơn nữa, GLIMMER cải thiện mạnh so với cả LUMEN được huấn luyện đa nhiệm vụ và FiD về cả chất lượng và tốc độ. Nói chung, GLIMMER thành công trong việc thống nhất sắp xếp lại và bộ nhớ thành một mô hình hiệu quả, chất lượng cao duy nhất.

2 Kiến thức nền

Chúng tôi quan tâm đến việc đạt được sự cân bằng tốt nhất có thể giữa chất lượng và tính toán suy luận. Phần sau mô tả FiD và LUMEN, các phương pháp cơ sở mà GLIMMER được xây dựng dựa trên, và các tính chất tính toán của chúng. Phân tích sâu hơn về các phương pháp này có thể được tìm thấy trong de Jong et al. (2023).

2.1 Fusion-in-Decoder

Fusion-in-Decoder (Izacard và Grave, 2021) dựa trên mô hình mã hóa-giải mã T5 (Raffel et al., 2020). Đối với mỗi đầu vào, một số đoạn văn bản liên quan được truy xuất, và đầu vào được thêm vào trước mỗi đoạn văn. Các cặp đầu vào-đoạn văn kết quả được mã hóa riêng biệt bởi bộ mã hóa, và các cặp được mã hóa sau đó được nối thành một chuỗi phẳng các biểu diễn token và được chú ý bởi bộ giải mã để tạo ra đầu ra mục tiêu. Đối với mỗi mô hình, các thành phần trực tiếp có màu xanh và các thành phần được tính toán trước trước khi suy luận có màu cam.

Đặt k là số lượng đoạn văn, np là số lượng token mỗi đoạn văn, nt là số lượng token mục tiêu, L là số lượng lớp, và d là chiều của mô hình. Theo phân tích từ de Jong et al. (2022a, 2023), FLOPs cho một mẫu suy luận đơn của FiD (bỏ qua tính toán điểm chú ý) được cho bởi:

FFiD = knp·L·14d² + nt·L·14d²

với các thừa số 8d² mỗi token từ các lớp feedforward, 4d² từ các lớp chiếu tự chú ý, và 2d² từ các lớp chiếu chéo chú ý. de Jong et al. (2023) chứa một dẫn xuất chi tiết hơn về độ phức tạp mô hình FiD.

2.2 LUMEN

Thông thường độ dài kết hợp của các đoạn văn được truy xuất lớn hơn nhiều so với độ dài mục tiêu, sao cho phần lớn FLOPs được tiêu thụ bởi bộ mã hóa xử lý các đoạn văn được truy xuất. LUMEN giảm chi phí suy luận bộ mã hóa bằng cách tính toán trước một phần biểu diễn bộ mã hóa cho các đoạn văn được truy xuất. Tại thời gian suy luận, LUMEN truy xuất các biểu diễn lớp trung gian thay vì văn bản.

Cụ thể hơn, LUMEN được khởi tạo từ một mô hình mã hóa-giải mã T5 được huấn luyện trước. Bộ giải mã hoạt động giống như bộ giải mã FiD tiêu chuẩn, nhưng bộ mã hóa T5 được chia thành một bộ mã hóa bộ nhớ lớn chứa tỷ lệ 1-α đầu tiên của các lớp, và một bộ mã hóa trực tiếp nhỏ hơn với tỷ lệ α còn lại của các lớp. Bộ mã hóa bộ nhớ được áp dụng ngoại tuyến cho các đoạn văn trong kho ngữ liệu để tính toán trước các biểu diễn bộ nhớ, sau đó được cập nhật có điều kiện trên đầu vào và nhiệm vụ tức thời bởi bộ mã hóa trực tiếp được tinh chỉnh. Để đảm bảo rằng các biểu diễn bộ nhớ và đầu vào tương thích, LUMEN áp dụng bộ mã hóa bộ nhớ cho đầu vào trước khi thêm biểu diễn câu hỏi vào biểu diễn bộ nhớ.

Hi = [MemEnc(Q); MemEnc(Passagei)]
G = Dec[Q; LiveEnc(H1); ...; LiveEnc(Hk)]

Chọn α = 1 cho ra một mô hình rất gần với FiD trong khi α = 0 là một mô hình bộ nhớ đầy đủ. Trong khi suy luận LUMEN chỉ áp dụng tỷ lệ α của các lớp, dẫn đến một phần α của FLOPs bộ đọc FiD cho bất kỳ kích thước mô hình nào.

FLUMEN = knp·αL·12d² + knp·L·2d² + nt·L·14d²

3 GLIMMER

GLIMMER xây dựng dựa trên LUMEN với hai khác biệt chính: GLIMMER kết hợp một bộ sắp xếp lại tích hợp, và chia sẻ bộ nhớ và bộ mã hóa trực tiếp qua nhiều nhiệm vụ. Các phương pháp sắp xếp lại tiêu chuẩn gặp khó khăn với một sự cân bằng: các mô hình nhỏ hơn có thể không đủ mạnh để đánh giá liệu một đoạn văn có liên quan đến đầu vào hay không, trong khi chi phí của các mô hình lớn hơn làm mất một phần lớn mục đích của việc sử dụng bộ sắp xếp lại ngay từ đầu. Kiến trúc LUMEN mang lại cơ hội để vượt qua sự cân bằng này, vì phần lớn các biểu diễn đoạn văn được tính toán trước. GLIMMER tái sử dụng các lớp đầu tiên của bộ mã hóa trực tiếp cho việc sắp xếp lại, tạo ra một mô hình sắp xếp lại mạnh mẽ với chi phí tính toán tương đối khiêm tốn.

Chia sẻ trọng số qua các nhiệm vụ, trong khi đó, cho phép huấn luyện bộ mã hóa bộ nhớ mà không lưu trữ các biểu diễn được tính toán trước trùng lặp, và tăng mạnh hiệu quả của bộ mã hóa trực tiếp. Hình 1 cho thấy tổng quan về kiến trúc GLIMMER.

3.1 Kiến trúc

So với LUMEN, GLIMMER chia bộ mã hóa trực tiếp thành hai thành phần, trong đó thành phần đầu tiên chịu tr책nhiệm tương tác ban đầu và sắp xếp lại và thành phần thứ hai thực hiện xử lý thêm trên các biểu diễn của các đoạn văn được chọn. Thành phần đầu tiên chứa tỷ lệ β của các lớp bộ mã hóa trực tiếp với phần còn lại của các lớp trong thành phần thứ hai. Sau bộ mã hóa trực tiếp đầu tiên, một lớp chiếu tuyến tính được áp dụng cho token đầu tiên của mỗi cặp đầu vào-đoạn văn để tạo ra điểm liên quan cho đoạn văn. Các đoạn văn top-m với điểm cao nhất trong số k ban đầu được xử lý bởi bộ mã hóa trực tiếp thứ hai, và các đoạn văn khác bị loại bỏ. Đầu ra của bộ mã hóa trực tiếp thứ hai được đưa vào bộ giải mã như trong FiD và LUMEN.

Hi = [MemEnc(Q); MemEnc(Passagei)]
H'i = LiveEncA(Hi)
Rj = H'i s.t. Rank[Score(H'i)] = j
G = Dec[Q; LiveEncB(R1); ...; LiveEncB(Rm)]

3.2 Huấn luyện

Bộ mã hóa bộ nhớ, cả hai thành phần bộ mã hóa trực tiếp, chiếu điểm và bộ giải mã đều được huấn luyện đầu cuối đến đầu cuối. Không giống như trong LUMEN, bộ mã hóa bộ nhớ không cần phải được đóng băng vì chúng tôi chia sẻ một bộ mã hóa bộ nhớ duy nhất giữa tất cả các nhiệm vụ. Để huấn luyện chiếu điểm và khuyến khích bộ nhớ và bộ mã hóa trực tiếp đầu tiên tạo ra các biểu diễn phù hợp cho việc sắp xếp lại, chúng tôi sử dụng một hàm mất mát chưng cất độ phức tạp phụ trợ (Izacard et al., 2022). Hàm mất mát này khuyến khích mô hình xếp hạng các đoạn văn theo mức độ chúng làm giảm độ phức tạp của việc sinh cuối cùng, nếu cặp đầu vào-đoạn văn đó được đưa vào bộ giải mã một mình. Cụ thể, chưng cất độ phức tạp tối thiểu hóa phân kỳ KL giữa phân phối được ngụ ý bởi các điểm sắp xếp lại (được tính từ đầu ra của thành phần bộ mã hóa trực tiếp đầu tiên được áp dụng cho việc nối các biểu diễn đầu vào và đoạn văn) và phân phối được ngụ ý bởi các độ phức tạp kết quả:

prank_k = exp(Score(Passagek, Q)/τ) / Σi exp(Score(Passagei, Q)/τ)
pLM_k = exp(log pLM(Answer | Passagek, Q)/τ) / Σi exp(log pLM(Answer | Passagei, Q)/τ)
Lpdist = KL(prank, pLM)

3.3 Phân tích tính toán

Sự khác biệt về độ phức tạp tính toán giữa GLIMMER và LUMEN nằm ở việc sắp xếp lại. Các đoạn văn m được chọn được xử lý bởi toàn bộ bộ mã hóa trực tiếp và sau đó được đưa qua bộ giải mã, tạo ra chi phí tính toán bằng với việc áp dụng LUMEN với m đoạn văn (ít hơn tổng số đoạn văn được truy xuất k). Tuy nhiên, đối với các đoạn văn không được chọn, GLIMMER vẫn áp dụng thành phần bộ mã hóa trực tiếp đầu tiên, dẫn đến chi phí sắp xếp lại:

FGLIMMER = Fm_LUMEN + (k-m)np·βαL·12d²

Nếu chúng ta sử dụng một số lượng nhỏ các đoạn văn được chọn m << k và phần nhỏ của các lớp sắp xếp lại β << 1, thì GLIMMER ít tốn kém về mặt tính toán hơn đáng kể so với LUMEN với k truy xuất.

Chúng tôi lưu ý rằng phân tích tính toán này giới hạn ở FLOPs, thay vì độ trễ thực tế. Đối với suy luận tự hồi quy, bộ giải mã thường bị nghẽn cổ chai bởi băng thông bộ nhớ thay vì FLOPs (Shazeer, 2019; de Jong et al., 2022a). Tuy nhiên, nhiều kỹ thuật gần đây làm giảm bớt ràng buộc này, chẳng hạn như các loại chú ý đa truy vấn (Shazeer, 2019; Ainslie et al., 2023a), thưa thớt lớp (de Jong et al., 2022a), giải mã suy đoán (Leviathan et al., 2022; Chen et al., 2023), và các kỹ thuật khác. Bất kỳ mô hình nào được triển khai trong môi trường mà tốc độ suy luận quan trọng có thể sẽ sử dụng một hoặc nhiều kỹ thuật như vậy, sao cho FLOPs là một ràng buộc ràng buộc. Đối với phần còn lại của bài báo này, chúng tôi sẽ đo chi phí tính toán bằng FLOPs; de Jong et al. (2023) chứa phân tích về cách FLOPs và độ trễ tương tác cho LUMEN.

Như chúng tôi sẽ cho thấy, GLIMMER đại diện cho sự cân bằng chất lượng-tính toán tốt hơn so với LUMEN và FiD.

4 Thí nghiệm

4.1 Thiết lập thí nghiệm

Cấu hình mô hình GLIMMER dựa trên kiến trúc T5.1.1 (Raffel et al., 2020) như LUMEN, được triển khai trong JAX (Heek et al., 2020), Flax (Heek et al., 2020) và Flaxformer. Tất cả các mô hình được khởi tạo từ các checkpoint T5.1.1 công khai. FiD được tinh chỉnh theo công thức từ bài báo gốc (Izacard và Grave, 2021). Đối với LUMEN và GLIMMER, cho tỷ lệ các lớp trực tiếp α, bộ mã hóa bộ nhớ được khởi tạo với tỷ lệ 1-α đầu tiên của các lớp của bộ mã hóa T5, và bộ mã hóa trực tiếp được khởi tạo với tỷ lệ α cuối cùng của các lớp của bộ mã hóa T5. Các thí nghiệm chính sử dụng α = 1/3.

Tinh chỉnh Đối với tinh chỉnh chúng tôi sử dụng bộ tối ưu hóa Adafactor (Shazeer và Stern, 2018) với tốc độ học không đổi 0.0001, kích thước batch 128, và tỷ lệ dropout 0.1 cho tất cả các nhiệm vụ. Đối với huấn luyện đa nhiệm vụ chúng tôi lấy mẫu đều từ các nhiệm vụ. Chúng tôi phân bổ 48 token cho câu hỏi và 304 token cho mỗi đoạn văn. Ngoài hàm mất mát mô hình ngôn ngữ tiêu chuẩn, các thí nghiệm sắp xếp lại sử dụng một hàm mất mát chưng cất độ phức tạp phụ trợ với trọng số và nhiệt độ 1.0. Chúng tôi huấn luyện cho đến khi hội tụ và chọn checkpoint với hiệu suất cao nhất trên tập dev. Chúng tôi sử dụng giải mã tham lam cho suy luận.

Dữ liệu Chúng tôi huấn luyện và đánh giá trên một tập con các tập dữ liệu từ điểm chuẩn KILT của các nhiệm vụ chuyên sâu về kiến thức (Petroni et al., 2020). Cụ thể, điều này bao gồm các tập dữ liệu trả lời câu hỏi Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), và HotPotQA (Yang et al., 2018), tập dữ liệu xác minh sự thật FEVER (Thorne et al., 2018), và các tập dữ liệu điền slot Zero Shot RE (Levy et al., 2017) và T-REx (ElSahar et al., 2018). Chúng tôi áp dụng quy trình lọc liên quan từ Hofstätter et al. (2022) để cải thiện các vấn đề từ các tập dữ liệu mất cân bằng.

Truy xuất Chúng tôi sử dụng quy trình truy xuất từ Hofstätter et al. (2022). Wikipedia được chia thành các đoạn lên đến 200 từ, và chúng tôi truy xuất các đoạn văn với điểm tương đồng cao nhất với truy vấn, được tính bởi một mô hình GTR-Base được huấn luyện trước (Ni et al., 2021).

4.2 Kết quả chính

Đối với kết quả chính của chúng tôi, chúng tôi so sánh FiD, LUMEN (với kiến trúc được cập nhật và huấn luyện đa nhiệm vụ) và GLIMMER. Do việc sắp xếp lại tích hợp, GLIMMER xử lý các đoạn văn hiệu quả hơn và do đó có thể truy xuất nhiều tài liệu hơn LUMEN, mà lần lượt có thể truy xuất nhiều tài liệu hơn FiD. Như Hình 2 cho thấy, hiệu quả này chuyển thành một mô hình chất lượng cao hơn và nhanh hơn, với GLIMMER vượt trội hơn LUMEN và FiD với tốc độ nhanh hơn.

4.3 Truy xuất và sắp xếp lại

Kết quả chính cho thấy rằng GLIMMER có thể đạt được chất lượng cao hơn với chi phí thấp hơn so với FiD và LUMEN bằng cách truy xuất nhiều đoạn văn hơn ban đầu và sắp xếp lại thành một số lượng đoạn văn nhỏ hơn nhiều. Ở đây chúng tôi điều tra cách các lựa chọn khác nhau về truy xuất và sắp xếp lại ảnh hưởng đến kết quả.

Số lượng đoạn văn được truy xuất và được chọn Hình 3 cho thấy hiệu suất thay đổi như thế nào với tổng số đoạn văn được truy xuất và số lượng đoạn văn được chọn sau khi sắp xếp lại. Hiệu suất tăng mạnh trong tổng số đoạn văn được truy xuất, với lợi ích giảm dần mạnh trong số lượng đoạn văn được chọn. Những kết quả này cho thấy rằng bộ sắp xếp lại hiệu quả chọn các đoạn văn hữu ích, sao cho nghẽn cổ chai là liệu thông tin liên quan có tồn tại trong các đoạn văn được truy xuất ban đầu hay không.

Trực giác trước được hỗ trợ thêm bởi Hình 4, khi việc áp dụng đủ các lớp sắp xếp lại gần như khôi phục hiệu suất của việc sử dụng tất cả 25 truy xuất. Mặt khác, một số sắp xếp lại neural với tương tác đầy đủ rõ ràng là hữu ích, vì việc sử dụng tỷ lệ sắp xếp lại ít hơn 0.25 (ít hơn 2 lớp sắp xếp lại) làm hại mạnh hiệu suất.

Thú vị là, như được hiển thị trong Hình 5, với một số lượng lớn truy xuất, việc lựa chọn đủ chính xác đến mức việc chọn nhiều đoạn văn hơn làm hại hiệu suất do sự xao nhãng từ ngữ cảnh không liên quan. Số lượng đoạn văn được chọn tối ưu thấp hơn với nhiều lớp sắp xếp lại hơn, vì các đoạn văn được xếp hạng cao nhất nắm bắt tốt hơn tất cả thông tin hữu ích.

Hiệu suất bộ sắp xếp lại

Bộ sắp xếp lại riêng biệt Cũng hữu ích khi xem xét tác động của việc sử dụng bộ mã hóa trực tiếp để thực hiện việc sắp xếp lại, trái ngược với một bộ sắp xếp lại riêng biệt. Bảng 1 so sánh hiệu suất của GLIMMER với việc sử dụng một bộ sắp xếp lại riêng biệt, được khởi tạo từ T5 hoặc được huấn luyện từ đầu. Chúng tôi lưu ý rằng việc sử dụng một bộ sắp xếp lại riêng biệt đạt được hiệu suất tương đương với chi phí của một mô hình phức tạp hơn, và chi phí bộ nhớ và tính toán bổ sung. Việc khởi tạo bộ sắp xếp lại từ các trọng số được huấn luyện trước là quan trọng - việc cố gắng học các lớp sắp xếp lại từ đầu làm giảm đáng kể hiệu suất.

4.4 Huấn luyện đa nhiệm vụ

Cải tiến chính thứ hai trong GLIMMER là chia sẻ bộ nhớ và bộ mã hóa trực tiếp giữa các nhiệm vụ, và do đó huấn luyện bộ mã hóa bộ nhớ. Chúng tôi trình bày các thí nghiệm cố gắng tách biệt tác động của những cải tiến này.

Hình 6 minh họa tác động của huấn luyện đa nhiệm vụ bằng cách so sánh hiệu suất trên NQ giữa các mô hình chỉ được huấn luyện trên NQ và các mô hình được huấn luyện trên KILT. Để cô lập tác động của huấn luyện đa nhiệm vụ, chúng tôi so sánh FiD và LUMEN, và huấn luyện bộ nhớ cho tất cả các mô hình trong so sánh này. Huấn luyện đa nhiệm vụ có lợi đáng kể cho tất cả các mô hình, nhưng có tác động không cân xứng đối với LUMEN, đặc biệt với các tỷ lệ trực tiếp thấp hơn. Hình 7 cho thấy sự khác biệt giữa huấn luyện đơn và đa nhiệm vụ như một hàm của tỷ lệ trực tiếp, với hiệu suất đa nhiệm vụ san bằng sớm hơn, cho thấy thêm tác động lớn hơn đối với tỷ lệ trực tiếp nhỏ hơn.

Tương tác muộn mà bộ mã hóa trực tiếp chịu trách nhiệm khá khác với nhiệm vụ huấn luyện trước của nó, vì vậy thật trực quan rằng bộ mã hóa trực tiếp sẽ có lợi không cân xứng từ kích thước và sự đa dạng của dữ liệu tăng lên.

Huấn luyện đa nhiệm vụ cũng cho phép học một bộ mã hóa bộ nhớ. Bảng 2 cho thấy rằng việc huấn luyện bộ mã hóa bộ nhớ quan trọng đối với hiệu suất, điều này được mong đợi vì bộ mã hóa được huấn luyện trước không được thiết kế để hoạt động như một bộ mã hóa bộ nhớ ngay từ đầu.

4.5 Các nghiên cứu loại bỏ khác

Có một số quyết định thú vị khác trong kiến trúc GLIMMER và quy trình huấn luyện. Bảng 3 trình bày các nghiên cứu loại bỏ của một số quyết định này.

Việc triển khai LUMEN ban đầu có một bộ mã hóa câu hỏi riêng biệt, điều này cần thiết vì bộ mã hóa bộ nhớ không được tinh chỉnh. Ở đây, chúng tôi cập nhật bộ mã hóa bộ nhớ với huấn luyện đa nhiệm vụ, vì vậy chúng tôi chọn tái sử dụng bộ mã hóa bộ nhớ để mã hóa câu hỏi, đơn giản hóa kiến trúc và giảm số lượng tham số. Chúng tôi thấy rằng sự đơn giản hóa này đi kèm với một chi phí nhỏ về hiệu suất.

Cũng có một số lựa chọn tham số về việc sắp xếp lại: trọng số của hàm mất mát chưng cất độ phức tạp, nhiệt độ của các phân phối điểm và độ phức tạp, và phương pháp tạo ra điểm sắp xếp lại. Việc đánh giá quá cao hoặc thấp hàm mất mát sắp xếp lại dẫn đến hiệu suất thấp hơn. Tuy nhiên, việc sử dụng nhiệt độ thấp hơn cho các phân phối điểm và độ phức tạp có giúp ích - Izacard et al. (2022) lập luận rằng tác động của hầu hết các đoạn văn riêng lẻ đối với độ phức tạp là nhỏ, và nhiệt độ thấp hơn giúp phân biệt những khác biệt đó. Cuối cùng, có vẻ như việc sử dụng token đầu tiên của mỗi đoạn văn hoạt động tương tự như việc tạo ra điểm từ các biểu diễn được gộp trung bình.

5 Công trình liên quan

Tăng cường truy xuất (Izacard và Grave, 2021; Borgeaud et al., 2022; Lewis et al., 2020; Khandelwal et al., 2020; Guu et al., 2020) là một kỹ thuật mạnh mẽ để cải thiện hiệu suất mô hình ngôn ngữ bằng cách tăng cường đầu vào với ngữ cảnh bổ sung. Công trình của chúng tôi tập trung vào việc cải thiện sự cân bằng chất lượng-tính toán cho các mô hình ngôn ngữ tăng cường truy xuất. Nó thực hiện điều này bằng cách thống nhất ba hướng nghiên cứu: bộ nhớ tương tác muộn, sắp xếp lại tương tác muộn, và học để truy xuất. Phương pháp của chúng tôi sử dụng khung kiến trúc từ Fusion-in-Decoder (Izacard và Grave, 2021), một trong những mô hình tăng cường truy xuất phổ biến nhất. Chúng tôi sử dụng huấn luyện đa nhiệm vụ trên KILT (Petroni et al., 2020) như trong Hofstätter et al. (2022).

Bộ nhớ Tăng cường truy xuất tốn kém do ngữ cảnh bổ sung cần được xử lý bởi mô hình ngôn ngữ. Các mô hình bộ nhớ như TOME (de Jong et al., 2022b), Memorizing Transformer (Wu et al., 2022a), và nhiều mô hình khác (Li et al., 2022; Zhong et al., 2022; Chen et al., 2022; Wu et al., 2022b; Yogatama et al., 2021; Bertsch et al., 2023) cố gắng tránh chi phí này bằng cách tính toán trước các biểu diễn và lưu trữ chúng vào bộ nhớ, sao cho các biểu diễn có thể được truy xuất trực tiếp thay vì được xử lý tức thời. Tuy nhiên, các phương pháp như vậy hy sinh chất lượng vì các biểu diễn bộ nhớ không được điều kiện hóa trên từng đầu vào riêng lẻ (Li et al., 2022; de Jong et al., 2023). Bộ nhớ tương tác muộn (de Jong et al., 2023; Milbauer et al., 2023) cải thiện chất lượng của các phương pháp bộ nhớ bằng cách chỉ tính toán trước một phần các biểu diễn truy xuất, và thực hiện một số tương tác giữa bộ nhớ và đầu vào tức thời. Cụ thể, công trình của chúng tôi dựa rất chặt chẽ trên LUMEN (de Jong et al., 2023).

Sắp xếp lại Giống như bản thân mô hình ngôn ngữ, các quy trình truy xuất đối mặt với sự cân bằng giữa xếp hạng trực tuyến tốn kém với tương tác đầy đủ (Chen et al., 2020) và các phương pháp bộ mã hóa kép phổ biến hơn như DPR (Karpukhin et al., 2020) và GTR (Ni et al., 2021) tính điểm dựa trên độ tương đồng tích vô hướng với kho ngữ liệu các biểu diễn đoạn văn được tính toán trước.

Thường các mô hình khác nhau cho truy xuất được áp dụng trong phương pháp pipeline, với một mô hình tính điểm ban đầu rẻ tiền theo sau bởi một bộ sắp xếp lại mạnh mẽ và đắt đỏ hơn (Mao et al., 2021; Wang et al., 2018; Yu et al., 2022). Nhiều bộ sắp xếp lại cũng sử dụng tương tác muộn để có được sự cân bằng tốt giữa chất lượng xếp hạng và tốc độ, như COLBERT (Khattab và Zaharia, 2020; Santhanam et al., 2022), PreTTR (MacAvaney et al., 2020), SDR (Cohen et al., 2022), và Poly-encoders (Humeau et al., 2020). GLIMMER kết hợp bộ nhớ tương tác muộn và sắp xếp lại thành một mô hình duy nhất, chia sẻ các biểu diễn được tính toán trước cho cả hai trường hợp sử dụng.

Học để truy xuất Các mô hình truy xuất thường được huấn luyện với dữ liệu có giám sát (Karpukhin et al., 2020; Ni et al., 2021), sử dụng các truy xuất vàng từ các tập dữ liệu như MS-MARCO (Nguyen et al., 2016) hoặc TREC CAR (Dietz et al., 2018). Khi chọn đoạn văn để sử dụng cho sinh tăng cường truy xuất, chúng ta có một tín hiệu bổ sung, cụ thể là những đoạn văn nào hữu ích nhất cho mô hình đọc. Một số công trình hiện có sử dụng tín hiệu này để cải thiện truy xuất (Guu et al., 2020; Sachan et al., 2021; Jiang et al., 2022; Sachan et al., 2021; Izacard et al., 2022). Chúng tôi theo ATLAS (Izacard et al., 2022) và sử dụng chưng cất độ phức tạp để huấn luyện bộ sắp xếp lại của chúng tôi chọn các đoạn văn giúp giảm độ phức tạp của mô hình đọc.

6 Kết luận

Các mô hình ngôn ngữ tăng cường truy xuất mạnh mẽ nhưng chậm trong suy luận, trong khi các mô hình tăng cường bộ nhớ được tính toán trước nhanh với chi phí của chất lượng. Các mô hình tương tác muộn lai như LUMEN trình bày một sự cân bằng chất lượng-tính toán tốt. Chúng tôi giới thiệu GLIMMER, một mô hình tương tác muộn được cải thiện cũng kết hợp việc sắp xếp lại đầu cuối đến đầu cuối được học và huấn luyện đa nhiệm vụ để đạt được sự cân bằng thậm chí tốt hơn. GLIMMER đạt được những cải thiện mạnh về chất lượng với tốc độ nhanh hơn so với LUMEN và FiD trên điểm chuẩn KILT của các nhiệm vụ chuyên sâu về kiến thức.

Lời cảm ơn
Chúng tôi cảm ơn Luke Vilnis, Tania Bedrax-Weiss và những người khác tại Google Research vì những bình luận và thảo luận sâu sắc.
