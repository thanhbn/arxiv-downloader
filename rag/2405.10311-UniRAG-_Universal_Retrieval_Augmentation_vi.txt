# UniRAG: Tăng cường Truy xuất Phổ quát
cho các Mô hình Ngôn ngữ Thị giác Lớn

Sahel Sharifymoghaddam*, Shivani Upadhyay∗, Wenhu Chen, Jimmy Lin
Khoa Khoa học Máy tính
Đại học Waterloo
{sahel.sharifymoghaddam, sjupadhyay, wenhu.chen, jimmylin}@uwaterloo.ca

Tóm tắt
Gần đây, các Mô hình Ngôn ngữ Thị giác Lớn (LVLMs) đã mở khóa nhiều trường hợp sử dụng phức tạp yêu cầu khả năng hiểu Đa phương thức (MM) (ví dụ: tạo chú thích hình ảnh hoặc trả lời câu hỏi thị giác) và khả năng tạo sinh MM (ví dụ: tạo hoặc chỉnh sửa hình ảnh theo văn bản hướng dẫn). Để cải thiện thêm độ chính xác đầu ra của các LVLM, chúng tôi giới thiệu UniRAG, một kỹ thuật cắm và chạy thêm thông tin được truy xuất có liên quan vào lời nhắc như các ví dụ few-shot trong quá trình suy luận. Không giống như niềm tin phổ biến rằng Tăng cường Truy xuất (RA) chủ yếu cải thiện việc tạo sinh hoặc hiểu các thực thể không phổ biến, kết quả đánh giá của chúng tôi trên tập dữ liệu MSCOCO với các thực thể phổ biến cho thấy cả các mô hình độc quyền như GPT-4o và Gemini-Pro cũng như các mô hình mã nguồn mở nhỏ hơn như LLaV A, LaVIT, và Emu2 đều cải thiện đáng kể chất lượng tạo sinh khi lời nhắc đầu vào của chúng được tăng cường với thông tin có liên quan được truy xuất bởi các bộ truy xuất Ngôn ngữ-Thị giác (VL) như các mô hình UniIR. Tất cả mã nguồn cần thiết để tái tạo kết quả của chúng tôi có sẵn tại https://github.com/castorini/UniRAG

1 Giới thiệu
Những tiến bộ gần đây trong các Mô hình Ngôn ngữ Thị giác Lớn (LVLMs), bao gồm cả các mô hình độc quyền như GPT-4o (OpenAI et al., 2024), Gemini-Pro (Team et al., 2024), DALL-E (Ramesh et al., 2021) và Parti (Yu et al., 2022) cũng như các mô hình mã nguồn mở như LLaV A (Liu et al., 2023c,b), LaVIT (Jin et al., 2023) và Emu2 (Sun et al., 2023), đóng vai trò quan trọng trong việc thu hẹp khoảng cách giữa các phương thức khác nhau. Những mô hình này đã thể hiện hiệu quả giống con người đáng chú ý và đạt được hiệu quả tiên tiến trong các đánh giá tiêu chuẩn khác nhau (Li và Lu, 2024).

Tuy nhiên, giống như các LLM, các VLM thường gặp khó khăn trong việc tạo ra kết quả chính xác về các chủ đề ít được biết đến hoặc gần đây (Huang et al., 2024; Bai et al., 2024). Hạn chế này xảy ra bởi vì các mô hình tạo phản hồi chỉ dựa trên dữ liệu huấn luyện của chúng, thiếu quyền truy cập vào thông tin bên ngoài trong quá trình suy luận. Với bản chất thay đổi nhanh chóng của thế giới, việc mong đợi các mô hình trả lời chính xác mọi truy vấn chỉ bằng kiến thức được huấn luyện trước là không thực tế.

Như một giải pháp thay thế, các kỹ thuật như Tạo sinh Tăng cường Truy xuất (RAG) đã thu hút sự chú ý đáng kể trong những năm gần đây (Gao et al., 2023). RAG trong ngữ cảnh (Ram et al., 2023) đặc biệt dựa vào khả năng học tập của các mô hình để kết hợp kiến thức mới nhất và hướng dẫn chúng tạo ra kết quả phù hợp hơn trong quá trình suy luận. Các nghiên cứu gần đây đã áp dụng RAG cho các nhiệm vụ VL, bao gồm Trả lời Câu hỏi Thị giác (VQA), tạo và chỉnh sửa hình ảnh theo văn bản hướng dẫn, và tạo chú thích hình ảnh (Yasunaga et al., 2022; Lin và Byrne, 2022; Chen et al., 2022a; Yu et al., 2023; Hu et al., 2023). Tuy nhiên, những phương pháp này chủ yếu tập trung vào việc huấn luyện các mô hình với các tập dữ liệu hình ảnh-văn bản lớn, để lại tiềm năng cho RAG trong ngữ cảnh cắm và chạy trong các ứng dụng VL chủ yếu chưa được khám phá. Khi các LVLM hộp đen trở nên phổ biến hơn, việc giải quyết khoảng trống này là cần thiết cho việc áp dụng đáng tin cậy của chúng trong các ứng dụng không thể chịu đựng được ảo giác.

Bài báo này giới thiệu kỹ thuật UniRAG cắm và chạy, được thiết kế để tăng cường độ trung thực của đầu ra mô hình. UniRAG tích hợp RA với các LVLM để hướng dẫn quá trình tạo sinh bằng thông tin trong ngữ cảnh có liên quan. Bằng cách kết hợp các cặp hình ảnh-văn bản xen kẽ như các ví dụ few-shot trong quá trình suy luận, UniRAG phục vụ như một phương pháp bất khả tri mô hình hiệu quả dạy các LVLM sẵn có về các ý định nhiệm vụ khác nhau và các thực thể không phổ biến. Đánh giá của chúng tôi trong công việc này chủ yếu tập trung vào các nhiệm vụ tạo chú thích hình ảnh (hình ảnh-sang-văn bản) và tạo hình ảnh (văn bản-sang-hình ảnh).

UniRAG áp dụng phương pháp truy xuất và tạo sinh hai giai đoạn. Để truy xuất, chúng tôi sử dụng bộ truy xuất UniIR (Wei et al., 2023), được huấn luyện với các tập dữ liệu VL đa dạng để truy xuất các đầu ra không đồng nhất trong cả phương thức văn bản và hình ảnh. Chúng tôi chủ yếu áp dụng các mô hình CLIP Score Fusion và BLIP Feature Fusion của UniIR được điều chỉnh hướng dẫn sử dụng CLIP (Radford et al., 2021) và BLIP-2 (Li et al., 2022), tương ứng. Trong giai đoạn tạo sinh, chúng tôi sử dụng nhiều mô hình LVLM độc quyền và mã nguồn mở sẵn có. Cụ thể, chúng tôi sử dụng LLaV A, GPT-4o và Gemini-Pro để tạo chú thích hình ảnh và LaVIT và Emu2 để tạo hình ảnh. Chúng tôi hướng dẫn các LVLM này sử dụng các kỹ thuật nhắc nhở zero-shot và few-shot để thể hiện khả năng tạo sinh được cải thiện của chúng do có quyền truy cập vào các ví dụ có liên quan trong quá trình suy luận.

Để đánh giá, chúng tôi chủ yếu sử dụng tập dữ liệu MSCOCO (Chen et al., 2015) từ tiêu chuẩn M-BEIR (Wei et al., 2023). Chúng tôi đánh giá việc tạo chú thích và hình ảnh sử dụng các nhiệm vụ hình ảnh-sang-văn bản và văn bản-sang-hình ảnh của M-BEIR, tương ứng. Bằng cách kết hợp RA với các LVLM, chúng tôi đạt được những cải thiện đáng kể so với hiệu quả cơ sở: tăng trung bình khoảng 9 điểm phần trăm trong SPICE (Anderson et al., 2016) cho việc tạo chú thích hình ảnh và giảm 25 đơn vị trong Khoảng cách Inception Fréchet (FID) (Heusel et al., 2017) cho việc tạo hình ảnh. Các đánh giá bổ sung sử dụng tập dữ liệu Fashion200k của M-BEIR chứng minh rằng UniRAG thậm chí còn hiệu quả hơn trong các ứng dụng cụ thể theo lĩnh vực.

Để tóm tắt, những đóng góp chính của chúng tôi trong bài báo này như sau:
• Giới thiệu kỹ thuật UniRAG cắm và chạy, kết hợp các bộ truy xuất VL với các LVLM sử dụng RAG trong ngữ cảnh.
• Đánh giá hiệu quả của UniRAG trong các nhiệm vụ tạo chú thích và tạo hình ảnh sử dụng năm LVLM, bao gồm cả các mô hình mã nguồn mở và độc quyền.
• Đánh giá UniRAG trên tập dữ liệu Fashion200k để thể hiện hiệu quả của nó trong các ứng dụng cụ thể theo lĩnh vực.

2 Công việc Liên quan
Tăng cường Truy xuất với Các Mô hình Tạo sinh: Các kỹ thuật Tăng cường Truy xuất (RAG) đã cải thiện đáng kể hiệu quả tạo sinh của LLM bằng cách kết hợp thông tin bên ngoài, dẫn đến kết quả chất lượng cao hơn (Gao et al., 2023). Trong khi một số nhà nghiên cứu chỉnh sửa kiến trúc LLM hoặc tinh chỉnh chúng để điều kiện hóa thông tin được truy xuất (Guu et al., 2020; Borgeaud et al., 2022), những người khác khám phá RAG trong ngữ cảnh (Ram et al., 2023; Dehghan et al., 2024), kết hợp dữ liệu có liên quan trực tiếp vào lời nhắc. Các nghiên cứu gần đây cũng đã xem xét việc tích hợp RAG với chuỗi suy nghĩ (Wei et al., 2022; Wang et al., 2023b) và các kỹ thuật nhắc nhở khác (Wang et al., 2023a; Li et al., 2024); tuy nhiên, hầu hết các công việc này chủ yếu tăng cường tạo sinh với văn bản.

Các Mô hình Ngôn ngữ-Thị giác và Tăng cường Truy xuất: Gần đây, một số mô hình đã được phát triển tích hợp hiểu biết về thị giác và ngôn ngữ (Ramesh et al., 2021; Radford et al., 2021; Yu et al., 2022; Li et al., 2022; Liu et al., 2023c,b; Jin et al., 2023; Sun et al., 2023; Zhai et al., 2023; OpenAI et al., 2024; Team et al., 2024). Những mô hình này thường sử dụng các bộ mã hóa cụ thể theo phương thức được huấn luyện trên các tập dữ liệu hình ảnh-văn bản lớn để tạo thành một không gian đặc trưng thống nhất. Thêm chi tiết về các LVLM được chọn để đánh giá có thể tìm thấy trong Phần 3.2.

Tương tự như các LLM truyền thống, các LVLM cũng gặp khó khăn trong việc tạo ra nội dung về các thực thể ít được biết đến hoặc các kết hợp không phổ biến của những thực thể phổ biến (Chen et al., 2022b; Blattmann et al., 2022). Bằng cách tận dụng truy xuất, các LVLM có thể được hướng dẫn để tạo ra các đầu ra chính xác và độ trung thực cao hơn. UniIR (Wei et al., 2023) giới thiệu các mô hình bộ truy xuất VL được điều chỉnh hướng dẫn với các tập dữ liệu VL đa dạng để thực hiện các nhiệm vụ truy xuất không đồng nhất liên quan đến cả văn bản và hình ảnh. RA-CM3 (Yasunaga et al., 2022), MuRAG (Chen et al., 2022a), CM3Leon (Yu et al., 2023) và REVEAL (Hu et al., 2023) sử dụng quy trình làm việc bộ truy xuất-bộ tạo sinh cho RAG trong các nhiệm vụ VL khác nhau. Tuy nhiên, tất cả chúng đều yêu cầu huấn luyện các mô hình tạo sinh và/hoặc truy xuất trên các tập dữ liệu VL rộng lớn.

Ngược lại, UniRAG kết hợp một bộ truy xuất VL trong quá trình suy luận, tạo điều kiện cho RAG trong ngữ cảnh cho các LLM hộp đen. Công việc của Liu et al. (2023a) gần nhất với chúng tôi, giới thiệu một phương pháp Chuỗi Suy nghĩ (CoT) RA. Tuy nhiên, không giống như UniRAG, nó tập trung vào việc cải thiện các LVLM cho VQA. Phương pháp của họ chọn nhiệm vụ phù hợp nhất từ tập huấn luyện/kiểm tra của mỗi tiêu chuẩn VQA như một minh chứng CoT few-shot. So sánh, UniRAG không dựa vào các ví dụ cụ thể theo nhiệm vụ để hướng dẫn bộ tạo sinh. Thay vào đó, nó truy xuất thông tin có liên quan từ một cơ sở dữ liệu bên ngoài để làm phong phú kiến thức của mô hình trong quá trình suy luận. Thiết kế bất khả tri nhiệm vụ này cho phép UniRAG hưởng lợi từ các nhiệm vụ hình ảnh-văn bản khác nhau, bao gồm VQA, tạo chú thích hình ảnh, tạo hình ảnh, và Nhận dạng Ký tự Quang học (OCR), bằng cách kết hợp các cặp hình ảnh-văn bản có liên quan về mặt ngữ nghĩa như các ví dụ few-shot.

3 Phương pháp
UniRAG áp dụng quy trình làm việc truy xuất và tạo sinh hai giai đoạn được giải thích dưới đây:

3.1 Truy xuất
Giai đoạn truy xuất là nơi đối với một truy vấn cho trước, k ứng viên có liên quan nhất được trích xuất từ một cơ sở dữ liệu VL. Trong giai đoạn này, phương thức truy vấn và ứng viên có thể là bất kỳ kết hợp nào của {văn bản, hình ảnh} → {văn bản, hình ảnh}. Cụ thể, đối với một nhiệm vụ hình ảnh-sang-văn bản như tạo chú thích, truy vấn là một hình ảnh, và các ứng viên được truy xuất là các chú thích; trong khi đối với một nhiệm vụ văn bản-sang-hình ảnh (ví dụ: tạo hình ảnh), truy vấn là một chú thích và các ứng viên được truy xuất là hình ảnh.

Để sử dụng các cặp hình ảnh-văn bản như các ví dụ trong ngữ cảnh trong giai đoạn tiếp theo, chúng ta cần chuyển đổi các ứng viên đơn phương thức được truy xuất thành các cặp. Điều này liên quan đến việc coi mỗi ứng viên được truy xuất như một truy vấn để tìm ứng viên bổ sung của nó dựa trên hai tiêu chí: a) nó phải có phương thức đối diện (văn bản cho một ứng viên hình ảnh và ngược lại) và b) nó phải khác với truy vấn gốc để tránh tiết lộ câu trả lời thực tế. Các mô hình bộ truy xuất được sử dụng cho các thí nghiệm của chúng tôi được giải thích thêm trong Phần 4.

3.2 Tạo sinh
Giai đoạn tạo sinh là nơi LVLM tạo ra đầu ra cần thiết trong cài đặt zero-shot (Kojima et al., 2023) hoặc few-shot (Brown et al., 2020). Tương tự như giai đoạn truy xuất, trong cài đặt zero-shot (cơ sở 1), truy vấn chỉ có mặt trong lời nhắc đầu vào; trong khi trong cài đặt few-shot, các ví dụ trong ngữ cảnh được bao gồm thêm trong lời nhắc. Trong cài đặt này, các ví dụ được thu thập bằng một trong các phương pháp sau: 1) lựa chọn ngẫu nhiên các truy vấn và các ứng viên thực tế tương ứng từ tập dữ liệu (cơ sở 2); 2) RAG trong ngữ cảnh với các cặp ứng viên được truy xuất từ giai đoạn trước. Chúng tôi tận dụng các LVLM khác nhau cho các nhiệm vụ tạo chú thích và hình ảnh, như được chi tiết trong Phần 4.

4 Thiết lập Thí nghiệm

4.1 Các Mô hình Được Chọn
Chúng tôi sử dụng các mô hình CLIP Score Fusion (CLIP-SF) và BLIP Feature Fusion (BLIP-FF) của UniIR làm bộ truy xuất VL. Kết quả thí nghiệm từ bài báo gốc cho thấy các mô hình UniIR được điều chỉnh hướng dẫn này vượt trội đáng kể so với các cơ sở CLIP và BLIP2 được huấn luyện trước của chúng trên các nhiệm vụ và cấu hình khác nhau, bao gồm các nhiệm vụ truy xuất hình ảnh và chú thích (Wei et al., 2023).

Để tạo ra các chú thích văn bản phù hợp cho hình ảnh, chúng tôi sử dụng các mô hình LLaV A (13B) (Liu et al., 2023b), Gemini-Pro (Team et al., 2024), và GPT-4o (OpenAI et al., 2024). Bảng 1 trình bày các chú thích mẫu được tạo bởi các mô hình này trong cài đặt zero-shot, few-shot ngẫu nhiên, và few-shot RAG (được truy xuất bởi CLIP-SF và BLIP-FF).

LLaVA là một LVLM mã nguồn mở dựa trên Vicuna (Chiang et al., 2023), sử dụng CLIP làm bộ mã hóa thị giác của nó. Nó được tinh chỉnh trên dữ liệu theo hướng dẫn VL được tạo bởi GPT-4 (Liu et al., 2023c). Khả năng hiểu và làm theo ý định của con người của LLaV A làm cho nó rất phù hợp cho việc tạo chú thích RAG. Hình 2 đến 5 trong Phụ lục A cho thấy các lời nhắc chi tiết được sử dụng cho việc tạo chú thích.

Không giống như việc tạo chú thích hình ảnh, chỉ yêu cầu hiểu biết MM, việc tạo hình ảnh đòi hỏi các mô hình có cả khả năng hiểu biết và tạo sinh MM. Để đáp ứng nhu cầu này, chúng tôi đã chọn LaVIT và Emu2-Gen từ một khảo sát về các LVLM (Zhang et al., 2024).

LaVIT là một mô hình mã nguồn mở dựa trên LLaMA-7B (Touvron et al., 2023) sử dụng một bộ token hóa thị giác để chuyển đổi hình ảnh thành các token thị giác rời rạc. Điều này cho phép nó xử lý cả đầu vào văn bản và hình ảnh sử dụng một mục tiêu thống nhất cho dự đoán token hình ảnh/văn bản tiếp theo. Các tác giả báo cáo Khoảng cách Inception Fréchet (FID) là 7.4 (Heusel et al., 2017) trên tập dữ liệu MSCOCO, cho thấy khả năng mạnh mẽ của nó trong các nhiệm vụ tạo hình ảnh.

Emu2-Gen là một mô hình mã nguồn mở khác, được tinh chỉnh từ Emu2 để tạo hình ảnh. Giống như LaVIT, nó sử dụng dự đoán token tiếp theo trên tất cả các phương thức và kết hợp một biến thể của CLIP làm bộ mã hóa thị giác cùng với LLaMA-33B làm xương sống. Nó có thể chấp nhận các đầu vào xen kẽ của hình ảnh, văn bản và video, và có một bộ giải mã thị giác để tạo hình ảnh. Trong khi hiệu quả zero-shot của nó trên các nhiệm vụ VL khác nhau thấp hơn so với Emu (Dai et al., 2023), Flamingo (Alayrac et al., 2022), và những cái khác, nó xuất sắc trong cài đặt few-shot, làm cho nó phù hợp cho việc tạo hình ảnh với RAG.

Bảng 2 hiển thị các ví dụ thị giác về việc tạo hình ảnh sử dụng LaVIT và Emu2-Gen trong cài đặt zero-shot, few-shot ngẫu nhiên, và few-shot RAG (sử dụng các bộ truy xuất UniIR).

4.2 Tập dữ liệu
Chúng tôi đánh giá UniRAG trên các nhiệm vụ hình ảnh-sang-văn bản và văn bản-sang-hình ảnh sử dụng các tập dữ liệu MSCOCO và Fashion200k từ tiêu chuẩn M-BEIR (Wei et al., 2023). Để đảm bảo truy xuất rộng rãi, chúng tôi sử dụng nhóm ứng viên toàn cầu của M-BEIR với hơn 5.5 triệu mục, bao gồm hình ảnh và văn bản từ tất cả 10 tập dữ liệu. Phương pháp này cho phép chúng tôi đánh giá trong một tình huống thực tế hơn với một cơ sở dữ liệu bên ngoài đa dạng và rộng lớn, thay vì giới hạn việc truy xuất vào kho văn liệu địa phương của mỗi tập dữ liệu.

Tập kiểm tra MSCOCO bao gồm khoảng 25k chú thích cho 5k hình ảnh duy nhất của các đối tượng phổ biến. Để tạo chú thích, chúng tôi sử dụng tất cả 5k hình ảnh làm truy vấn. Tuy nhiên, do thời gian tạo hình ảnh dài (hơn 12 giây mỗi hình ảnh), chúng tôi lấy mẫu ngẫu nhiên một truy vấn chú thích cho mỗi hình ảnh. Để duy trì tính nhất quán, tất cả các lần chạy tạo hình ảnh đều sử dụng cùng tập mẫu này. Chúng tôi phân tích ảnh hưởng của việc lấy mẫu này chi tiết trong Phần 5.3. Để chứng minh hiệu quả của UniRAG trong các nhiệm vụ cụ thể theo lĩnh vực, chúng tôi cũng đánh giá nó trên tập kiểm tra của tập dữ liệu Fashion200k từ M-BEIR. Tập này bao gồm khoảng 1.7k chú thích và 4.9k truy vấn hình ảnh, tất cả từ lĩnh vực thời trang.

4.3 Chi tiết Cấu hình
Chúng tôi sử dụng các mô hình LLaV A, Gemini-Pro, và GPT-4o để tạo chú thích, và LaVIT và Emu2-Gen để tạo hình ảnh. Đối với cả hai nhiệm vụ, chúng tôi thí nghiệm với việc thêm k ∈ {0,1,5} cặp hình ảnh-chú thích được truy xuất như các ví dụ trong ngữ cảnh, trong đó k = 0 thiết lập hiệu quả cơ sở của các mô hình tạo sinh mà không có các ví dụ trong ngữ cảnh ngẫu nhiên hoặc RAG. Vì LLaV A không cho phép nhiều hình ảnh làm đầu vào, chúng tôi ghép theo chiều dọc tất cả các hình ảnh ứng viên few-shot (và hình ảnh truy vấn để tạo chú thích) thành một hình ảnh duy nhất. Chiều rộng của hình ảnh được ghép cuối cùng khớp với hình ảnh rộng nhất, trong khi chiều cao là tổng của tất cả chiều cao riêng lẻ. Hình 6 cho thấy một hình ảnh được ghép mẫu với k = 1 ví dụ. Mỗi hình ảnh sau đó được gắn nhãn [n] trong lời nhắc, trong đó n cho biết vị trí của nó trong hình ảnh được ghép cuối cùng. Đối với tất cả các mô hình tạo sinh khác, các ví dụ trong ngữ cảnh được cung cấp theo định dạng xen kẽ sau: {hình ảnh1, văn bản1, ..., hình ảnhk, văn bảnk}.

Chúng tôi sử dụng cấu hình mặc định của LLaV A cho tất cả các suy luận của nó, với kích thước lô là bốn cho zero-shot và hai cho few-shot. Việc tạo chú thích zero-shot trên một GPU NVIDIA RTX A6000 duy nhất trung bình khoảng 5 giây, trong khi few-shot mất 2-3 lần lâu hơn. Đối với Gemini-Pro và GPT-4o, chúng tôi sử dụng API tạo nội dung của Vertex AI và hoàn thành trò chuyện của OpenAI, tương ứng, đặt tham số max_new_token của chúng thành 400. Thêm chi tiết về ước tính chi phí của chúng có thể tìm thấy trong Phụ lục B. Đối với LaVIT, chúng tôi tuân theo cấu hình mẫu được cung cấp, tạo ra hình ảnh 1024 × 1024 trong khoảng 12 giây trên một GPU NVIDIA RTX A6000 duy nhất. Tương tự, chúng tôi sử dụng cài đặt mặc định của Emu2-Gen. Vì nó có 37 tỷ tham số, chúng tôi chạy biến thể "bf16" của nó trên một NVIDIA H100 SXM từ nhà cung cấp Lambda GPU. Việc tạo hình ảnh với cài đặt này mất khoảng 8 giây cho zero-shot và one-shot, và 13 giây cho five-shot. Chúng tôi gặp lỗi CUDA hết bộ nhớ cho 36 trong số 5k chú thích trong quá trình tạo hình ảnh five-shot với Emu2-Gen, vì vậy chúng tôi giới hạn lời nhắc đến bốn cặp đầu tiên trong những trường hợp đó.

Đối với mỗi nhiệm vụ, chúng tôi báo cáo các số liệu được sử dụng phổ biến nhất: Để tạo chú thích, chúng tôi bao gồm các số liệu dựa trên n-gram, BLEU(1-4) (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), và ROUGE (Lin, 2004), cũng như số liệu dựa trên đồ thị cảnh, SPICE. Để tạo hình ảnh, chúng tôi báo cáo FID để đo độ trung thực hình ảnh, CLIP Score (Hessel et al., 2021) để căn chỉnh với lời nhắc chú thích, và Inspection Score (IS) (Salimans et al., 2016) để đánh giá chất lượng tổng thể. Phụ lục D giải thích các số liệu này chi tiết hơn.

5 Kết quả Đánh giá và Phân tích
Bảng 3 đến 6 trình bày kết quả cho các nhiệm vụ tạo chú thích và hình ảnh trên các tập dữ liệu MSCOCO và Fashion200k. Các hàng 1* từ mỗi bảng đo hiệu quả của các bộ truy xuất CLIP-SF và BLIP-FF. Chúng phản ánh mức độ hiệu quả của các mô hình tạo sinh nếu chúng trả về hình ảnh hoặc chú thích từ ví dụ được truy xuất đầu tiên như vậy. Để đảm bảo số lượng hình ảnh và chú thích bằng nhau để đánh giá, chúng tôi báo cáo hiệu quả của các bộ truy xuất chỉ sử dụng cặp hình ảnh-văn bản được truy xuất hàng đầu của chúng (k = 1) cho mỗi truy vấn.

CLIP-SF và BLIP-FF nhầm lẫn các phương thức ứng viên cho tới 7% và 25% các truy vấn MSCOCO, tương ứng, có nghĩa là chúng có thể truy xuất một hình ảnh cho một hình ảnh-sang-văn bản hoặc một văn bản cho một nhiệm vụ văn bản-sang-hình ảnh. Trong đánh giá của chúng tôi, khi một ứng viên được truy xuất có phương thức sai, chúng tôi sử dụng phần bổ sung của nó với phương thức đúng. Không giống như kết quả UniIR gốc, cho thấy CLIP-SF hiệu quả hơn cho việc truy xuất MSCOCO, điều chỉnh này cho phép cả hai bộ truy xuất hoạt động tương tự trên tập dữ liệu này. Đối với Fashion200k, phát hiện của chúng tôi xác nhận kết quả của UniIR, cho thấy BLIP-FF hiệu quả hơn.

Các hàng còn lại trong mỗi bảng so sánh kết quả zero-shot (k = 0) và few-shot (k > 0) của các mô hình tạo sinh sử dụng CLIP-SF (*a), BLIP-FF (*b), và lựa chọn ngẫu nhiên (*c) làm bộ truy xuất cho việc tạo sinh few-shot. Hành vi của các bộ truy xuất CLIP-SF và BLIP-FF trong các ứng dụng RAG khớp với hiệu quả độc lập của chúng (so sánh các hàng *a và *b trong mỗi bảng). Trên MSCOCO, cả hai bộ truy xuất tiếp tục hoạt động tương tự, trong khi BLIP-FF liên tục vượt trội so với CLIP-SF trên Fashion200k.

5.1 Tạo Chú thích
Bảng 3 cho thấy kết quả tạo chú thích cho các mô hình tạo sinh LLaV A, Gemini-Pro và GPT-4o trên tập dữ liệu MSCOCO. Trong khi một số số liệu được báo cáo trong bảng này làm tham khảo, chúng tôi tập trung vào SPICE vì tất cả các số liệu khác chủ yếu nhạy cảm với sự chồng chéo n-gram (Anderson et al., 2016).

Như được báo cáo trong bảng 3, việc tăng cường các cặp hình ảnh-chú thích có liên quan (k > 0) được truy xuất từ giai đoạn trước, tăng hiệu quả của các LVLM bất kể hiệu quả cơ sở, số lượng ví dụ trong ngữ cảnh và lựa chọn mô hình bộ truy xuất của chúng. Hiệu quả cơ sở (k = 0) của LLaV A vượt qua các mô hình Gemini-Pro và GPT-4o (so sánh các hàng 2-4). Mặc dù tất cả các mô hình đều hưởng lợi từ RA, khoảng cách này càng mở rộng khi chỉ một ví dụ được truy xuất duy nhất (k = 1) được thêm vào lời nhắc, làm cho one-shot BLIP-FF + LLaV A trở thành cài đặt hiệu quả nhất và one-shot CLIP-SF + LLaV A là người theo sau rất gần. Tăng thêm số lượng ví dụ được truy xuất từ một lên năm giúp các mô hình Gemini-P và GPT-4o. Tuy nhiên, LLaV A với năm ví dụ được truy xuất kém hiệu quả hơn LLaV A với một ví dụ duy nhất.

Đối với cả LLaV A và GPT-4, việc thêm một ví dụ được chọn ngẫu nhiên duy nhất vào lời nhắc cải thiện nhẹ chất lượng tạo sinh so với việc tạo sinh zero-shot. Tuy nhiên, tăng số lượng ví dụ được chọn ngẫu nhiên lên năm làm giảm hiệu quả của các bộ tạo sinh xuống dưới cơ sở zero-shot của chúng. Ngược lại, đối với Gemini-Pro, các ví dụ few-shot ngẫu nhiên cải thiện đáng kể chất lượng của các chú thích được tạo, mặc dù chúng vẫn kém hiệu quả hơn các ví dụ được truy xuất few-shot. Những xu hướng này cho thấy hiệu quả bất khả tri mô hình của UniRAG đến từ việc chỉ bao gồm các ví dụ có liên quan nhất trong ngữ cảnh.

Bảng 4 cho thấy kết quả tạo chú thích cho tập dữ liệu Fashion200k. Để giảm chi phí của các cuộc gọi API, chúng tôi chỉ báo cáo kết quả cho mô hình LLaV A trong thí nghiệm này. LLaV A hoạt động kém trong các tình huống zero-shot do kiến thức hạn chế của nó về tập dữ liệu Fashion200k cụ thể theo lĩnh vực. Trong khi các ví dụ few-shot được chọn ngẫu nhiên có thể giúp LLaV A nắm bắt ý định nhiệm vụ và mức độ chi tiết cần thiết để mô tả các sản phẩm thời trang, cải thiện thực sự xảy ra khi các ví dụ có liên quan được bao gồm trong lời nhắc. Bởi vì nhiều thực thể trong tập dữ liệu Fashion200k khá tương tự, bộ tạo sinh thường lặp lại chú thích trong ngữ cảnh của một hình ảnh tương tự cho hình ảnh truy vấn của nó, dẫn đến hiệu quả gần với các mô hình bộ truy xuất cơ sở. Vui lòng xem Phụ lục D để xem các hình ảnh mẫu.

5.2 Tạo Hình ảnh
Bảng 5 cho thấy kết quả tạo hình ảnh cho các mô hình tạo sinh LaVIT và Emu2-Gen (được ký hiệu là Emu-G trong bảng) trên tập dữ liệu MSCOCO. Như mong đợi, cả LaVIT với LLM xương sống 7b và Emu2-Gen với LLM xương sống 33b đều học từ các ví dụ trong ngữ cảnh và tạo ra hình ảnh tốt hơn khi các ví dụ cặp văn bản-hình ảnh được bao gồm trong lời nhắc. Tuy nhiên, mức độ cải thiện khác nhau đối với mỗi mô hình và nó thay đổi qua các cài đặt few-shot khác nhau. Trong khi Emu2-G hiệu quả hơn zero-shot, khả năng học trong ngữ cảnh vượt trội của LaVIT cải thiện đáng kể việc tạo sinh few-shot của nó và làm cho nó trở thành mô hình hiệu quả hơn trong tất cả cài đặt few-shot.

Cả hai mô hình đều tạo ra kết quả tốt nhất với một ví dụ few-shot RAG có liên quan nhất duy nhất (k = 1), bất kể mô hình bộ truy xuất. Tăng thêm số lượng ví dụ RAG lên k = 5 có tác động không đáng kể, làm giảm nhẹ độ trung thực hình ảnh để ủng hộ điểm CLIP và/hoặc Inception (IS) cao hơn một chút. Tuy nhiên, hai mô hình hành xử khác nhau đối với các ví dụ few-shot ngẫu nhiên; trong khi LaVIT vẫn chủ yếu hiệu quả với các ví dụ được chọn ngẫu nhiên, Emu2-G dường như bị nhầm lẫn bởi quá nhiều ví dụ ngẫu nhiên và kết quả ngẫu nhiên k = 5 của nó tệ hơn nhiều so với cơ sở zero-shot của nó.

Bảng 6 trình bày kết quả tạo hình ảnh cho LaVIT trên tập dữ liệu Fashion200k. Như mong đợi, FID và CLIP Score cho thấy xu hướng tương tự như trong Bảng 3, với các ví dụ ngẫu nhiên và RAG tăng cường dần độ trung thực của hình ảnh được tạo so với thực tế (FID thấp hơn) và cải thiện sự căn chỉnh của hình ảnh được tạo với các truy vấn chú thích (CLIP Score cao hơn). Tuy nhiên, Inception Score (IS) cao nhất trong cài đặt zero-shot. Điều này do tính đa dạng thấp của tập dữ liệu Fashion200k; việc tạo ra hình ảnh trung thực hơn trong ngữ cảnh này dẫn đến sự đa dạng thấp hơn giữa các hình ảnh được tạo, dẫn đến IS thấp hơn. Một quan sát đáng chú ý khác là, với các ví dụ RAG, LaVIT đạt được CLIP Score cao hơn những điểm của các bộ truy xuất. Điều này cho thấy rằng các hình ảnh được tạo căn chỉnh tốt hơn với các truy vấn chú thích so với các hình ảnh được truy xuất từ tập dữ liệu. Vui lòng xem Phụ lục D để xem các hình ảnh mẫu.

5.3 Ảnh hưởng của Lấy mẫu
Đối với mỗi hình ảnh trong tập kiểm tra MSCOCO của M-BEIR, chúng tôi đã lấy mẫu một chú thích duy nhất (Tập 1 Chú thích trong Bảng 7) làm truy vấn. Để nghiên cứu độ nhạy cảm với việc lấy mẫu, chúng tôi lặp lại quá trình lấy mẫu một lần nữa (Tập 2 Chú thích trong Bảng 7). Hai tập mẫu chỉ có 1030 chú thích chung nhưng cả hai đều bao phủ tất cả 5k hình ảnh khác biệt trong tập dữ liệu. Chúng tôi đã sử dụng CLIP-SF + LaVIT trong nghiên cứu loại bỏ này như đường ống bộ truy xuất-bộ tạo sinh.

Như được hiển thị trong Bảng 7, FID và CLIP Score cho tất cả các giá trị k rất tương tự giữa hai lần chạy với sự khác biệt ít hơn một phần trăm. Tuy nhiên, các Inception Score được báo cáo và độ lệch chuẩn của chúng thay đổi hơi nhiều hơn, nhưng vẫn trong khoảng tin cậy của nhau. Chúng tôi tin rằng sự khác biệt này liên quan đến sự không chắc chắn vốn có của số liệu khi áp dụng cho kích thước tập dữ liệu nhỏ.

5.4 So sánh Tập dữ liệu
Như được hiển thị trong Bảng 4 và 6, các mô hình nhỏ hơn hoạt động kém trên tập dữ liệu Fashion200k trong cả hai nhiệm vụ dưới cài đặt zero-shot, cho thấy kiến thức ngầm hạn chế của chúng về lĩnh vực thời trang. Tuy nhiên, sự phụ thuộc mạnh mẽ của chúng vào các ví dụ trong ngữ cảnh và cải thiện đáng kể dưới cài đặt few-shot RAG chứng minh hiệu quả của UniRAG trong việc thu hẹp khoảng cách kiến thức này.

Ngược lại, đối với tập dữ liệu MSCOCO (Bảng 3 và 5), nơi các mô hình nhỏ hơn có kiến thức vốn có nhiều hơn về các thực thể phổ biến, hiệu quả zero-shot lớn hơn và tác động của các ví dụ few-shot RAG ít rõ rệt hơn. Trong khi các ví dụ few-shot RAG mang lại lợi ích lớn hơn cho tập dữ liệu Fashion200k, điều này không có nghĩa là mô hình chỉ đơn giản sao chép các hình ảnh được truy xuất khi thiếu kiến thức được huấn luyện trước. Như được hiển thị trong Bảng 8, tập dữ liệu Fashion200k thể hiện FID cao hơn đáng kể khi so sánh các hình ảnh được tạo với hình ảnh được truy xuất hàng đầu-một, cho thấy rằng mô hình không chỉ đơn giản sao chép mà thay vào đó tạo ra các đầu ra khác biệt.

6 Kết luận
Chúng tôi đã giới thiệu UniRAG như một kỹ thuật tăng cường truy xuất bất khả tri mô hình cho các nhiệm vụ Ngôn ngữ-Thị giác (VL) và đánh giá nó trên các nhiệm vụ hình ảnh-sang-văn bản và văn bản-sang-hình ảnh. Chúng tôi đã sử dụng các mô hình LLaV A, Gemini-Pro, và GPT-4o để tạo chú thích cho hình ảnh đầu vào trong cài đặt zero-shot và few-shot (với RAG). Tương tự, chúng tôi đã sử dụng các mô hình LaVIT và Emu2-Gen để tạo hình ảnh từ chú thích đầu vào. Trong cài đặt few-shot RAG, chúng tôi đã tận dụng các mô hình CLIP-SF và BLIP-FF của UniIR để truy xuất các cặp hình ảnh-văn bản có liên quan và bao gồm chúng như các ví dụ trong ngữ cảnh. Để thể hiện thêm hiệu quả của các ví dụ few-shot RAG, chúng tôi cũng đã so sánh chúng với các ví dụ few-shot được chọn ngẫu nhiên.

Kết quả thí nghiệm của chúng tôi trên tập dữ liệu MSCOCO từ M-BEIR cho thấy tất cả các mô hình, bất kể hiệu quả cơ sở zero-shot của chúng, đều cải thiện sau khi được tiếp xúc với các ví dụ trong ngữ cảnh. Tuy nhiên, kết quả tốt nhất chỉ đạt được khi các ví dụ được truy xuất có liên quan được bao gồm trong lời nhắc, thay vì các cặp văn bản-hình ảnh ngẫu nhiên. Thực tế, đối với các mô hình LLaV A, GPT-4o, và Emu2-Gen, việc thêm năm ví dụ ngẫu nhiên làm giảm chất lượng tạo sinh xuống dưới cơ sở zero-shot của chúng. Ngược lại, trong cài đặt few-shot RAG, việc tăng số lượng ví dụ từ một lên năm hoặc cải thiện chất lượng tạo sinh (đối với Gemini-Pro và GPT-4o) hoặc không có ảnh hưởng (đối với LaVIT và Emu2-Gen). Đối với LLaV A, trong khi việc thêm nhiều ví dụ trong ngữ cảnh RAG làm giảm hiệu quả của nó, nó vẫn hoạt động đáng kể tốt hơn cả lời nhắc zero-shot và few-shot ngẫu nhiên. Các thí nghiệm của chúng tôi trên năm LVLM xác nhận rằng UniRAG với các bộ truy xuất UniIR là một cách hiệu quả bất khả tri mô hình để cải thiện chất lượng tạo sinh của các LVLM được huấn luyện trước bằng cách truy xuất các ví dụ few-shot có liên quan tại thời gian suy luận.

Để đánh giá hiệu quả của UniRAG với các thực thể cụ thể theo lĩnh vực không phổ biến, chúng tôi đã đánh giá nó trên tập dữ liệu Fashion200k. Kết quả của chúng tôi cho thấy UniRAG là cần thiết để cải thiện chất lượng tạo sinh trong các tình huống mà mô hình tạo sinh có kiến thức ngầm hạn chế về các thực thể.

7 Hạn chế
Trong khi chúng tôi đã mã nguồn mở UniRAG, việc sử dụng nó vẫn bị ràng buộc bởi các giấy phép và thỏa thuận sử dụng của các mô hình độc quyền GPT-4o và Gemini-Pro được sử dụng để suy luận. Hơn nữa, một trong các mô hình mã nguồn mở của chúng tôi LaVIT, được cấp phép dưới Giấy phép Cộng đồng LaVIT yêu cầu các yêu cầu giấy phép cụ thể cho việc sử dụng thương mại.

Mục tiêu chính của UniRAG là tăng cường kiến thức của mô hình về các thực thể chủ đề cụ thể, đảm bảo rằng thông tin được truy xuất ảnh hưởng trực tiếp đến đầu ra được tạo—một đặc điểm chung của tất cả các ứng dụng RAG. Trong khi việc truy xuất từ một cơ sở dữ liệu đa dạng và kết hợp nhiều ví dụ few-shot có thể mở rộng ảnh hưởng này, phương pháp này có thể ít phù hợp cho các ứng dụng ưu tiên sáng tạo hoặc tính độc đáo hơn độ chính xác tạo sinh. Ngoài ra, tính liên quan là tiêu chí duy nhất được sử dụng trong khi truy xuất các cặp ứng viên cho các ví dụ trong ngữ cảnh. Triển khai phương pháp này trong các ứng dụng thế giới thực yêu cầu xếp hạng các ứng viên dựa trên sự thật, tác hại tiềm ẩn, thành kiến và các yếu tố quan trọng khác cho AI có trách nhiệm, thay vì chỉ dựa vào tính liên quan. Độ trễ bổ sung và chi phí tính toán của việc truy xuất và suy luận với các lời nhắc lớn hơn cũng phải được xem xét cho các ứng dụng nhạy cảm thời gian hoặc chi phí.

Công việc này sử dụng các tập dữ liệu MSCOCO và Fashion200k chỉ tiếng Anh để đánh giá và thí nghiệm với các mô hình bộ truy xuất và tạo sinh được huấn luyện chủ yếu trên kho văn liệu tiếng Anh. Do đó, hiệu quả của phương pháp được đề xuất của chúng tôi đối với các ngôn ngữ ít tài nguyên không phải tiếng Anh vẫn chưa được khám phá. Hơn nữa, việc tạo ra hình ảnh của những người thật đòi hỏi sự thận trọng thêm và kiểm toán rộng rãi do các tác động bảo mật và riêng tư đáng kể. Cuối cùng, cần nghiên cứu thêm để đánh giá khả năng tổng quát hóa của việc tạo sinh được hướng dẫn bởi bộ truy xuất đối với việc truy xuất ngoài lĩnh vực. Ví dụ, việc sử dụng một tập dữ liệu đánh giá tập trung vào thực thể, mà bộ truy xuất chưa được huấn luyện trên đó, có thể chứng minh tốt hơn lợi thế của tăng cường truy xuất cho suy luận đa phương thức.

Lời cảm ơn
Nghiên cứu này được hỗ trợ một phần bởi Hội đồng Nghiên cứu Khoa học Tự nhiên và Kỹ thuật (NSERC) của Canada. Chúng tôi cũng cảm ơn Microsoft và Google đã cung cấp quyền truy cập vào OpenAI LLM qua Azure và Gemini qua VertexAI, tương ứng. Ngoài ra, chúng tôi đánh giá cao Cong Wei đã trả lời một số câu hỏi về UniIR và Xueguang Ma đã cung cấp phản hồi có giá trị.
