# 2310.15594.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2310.15594.pdf
# File size: 900660 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Retrieval-based Knowledge Transfer: An Effective Approach for Extreme
Large Language Model Compression
Jiduan Liu1,2∗, Jiahao Liu3∗, Qifan Wang4, Jingang Wang3, Xunliang Cai3
Dongyan Zhao1,2,5,6 †, Ran Lucien Wang7, Rui Yan7†
1Wangxuan Institute of Computer Technology, Peking University
2Center for Data Science, AAIS, Peking University;3Meituan;4Meta AI
5National Key Laboratory of General Artificial Intelligence;6BIGAI, Beijing, China
7Gaoling School of Artificial Intelligence, Renmin University of China
{liujiduan,zhaody}@pku.edu.cn ,ruiyan@ruc.edu.cn ,wqfcr@fb.com
{liujiahao12,wangjingang02,caixunliang}@meituan.com ,ran.wang.math@gmail.com
Abstract
Large-scale pre-trained language models
(LLMs) have demonstrated exceptional perfor-
mance in various natural language processing
(NLP) tasks. However, the massive size of
these models poses huge challenges for their
deployment in real-world applications. While
numerous model compression techniques have
been proposed, most of them are not well-
suited for achieving extreme model compres-
sion when there is a significant gap in model
scale. In this paper, we introduce a novel
compression paradigm called Retrieval-based
Knowledge Transfer (RetriKT), which effec-
tively transfers the knowledge of LLMs to ex-
tremely small-scale models (e.g., 1%). In par-
ticular, our approach extracts knowledge from
LLMs to construct a knowledge store, from
which the small-scale model can retrieve rele-
vant information and leverage it for effective
inference. To improve the quality of the model,
soft prompt tuning and Proximal Policy Opti-
mization (PPO) reinforcement learning tech-
niques are employed. Extensive experiments
are conducted on low-resource tasks from Su-
perGLUE and GLUE benchmarks. The results
demonstrate that the proposed approach sig-
nificantly enhances the performance of small-
scale models by leveraging the knowledge from
LLMs.
1 Introduction
Pre-trained language models (PLMs), such as
BERT/RoBERTa (Devlin et al., 2019; Liu et al.,
2019), have demonstrated exceptional performance
across various natural language processing (NLP)
applications. However, these models typically com-
prise hundreds of millions of parameters, present-
ing a substantial challenge for researchers due to
∗Equal contribution.
†Corresponding authors: Dongyan Zhao
(zhaody@pku.edu.cn) and Rui Yan (ruiyan@ruc.edu.cn).their massive scale. As a result, the full potential of
large-scale pre-trained language models (PLMs)
remains untapped. To tackle this challenge, a
multitude of model compression techniques have
been proposed, encompassing knowledge distilla-
tion (Sanh et al., 2019; Jiao et al., 2020; Passban
et al., 2021), network pruning (Liang et al., 2021a;
Gordon et al., 2020), quantization (Zhang et al.,
2020; Tao et al., 2022) and weight sharing (Lan
et al., 2020).
However, these model compression methods are
not directly applicable to scenarios requiring high
compression ratios, such as knowledge distillation.
In such cases, the introduction of assistant mod-
els (Mirzadeh et al., 2020; Son et al., 2021) often
leads to decreased and unstable performance. Re-
cently, there has been a growing interest in leverag-
ing large language models (LLMs) (Touvron et al.,
2023; Zeng et al., 2022; Ouyang et al., 2022; Scao
et al., 2022) that possess extensive language knowl-
edge and can be effectively employed in various
downstream tasks. Consequently, it is essential to
explore methods for transferring this knowledge
to small-scale models. Nonetheless, existing ap-
proaches are inadequate for compressing LLMs
due to their exceptionally high compression ratios.
Some prior research (Wang et al., 2022; Dai et al.,
2023; Ubani et al., 2023) has suggested utilizing
LLMs for data augmentation and knowledge trans-
fer to small-scale models, which allows the lat-
ter to demonstrate improved performance on low-
resource datasets. However, when tackling more
challenging tasks like the SuperGLUE benchmark
(Wang et al., 2019a), the limited parameter size of
small-scale models becomes a hindrance, prevent-
ing them from effectively retaining the knowledge
transferred by LLMs. Consequently, the perfor-
mance enhancement achieved for small-scale mod-
els remains constrained.arXiv:2310.15594v1  [cs.CL]  24 Oct 2023

--- PAGE 2 ---
To effectively transfer the knowledge of Large
Language Models (LLMs) to small-scale mod-
els, enabling them to efficiently and accurately
complete tasks, we propose a novel compression
paradigm called Retrieval-based Knowledge Trans-
fer (RetriKT). Our approach involves two main
steps: knowledge extraction from the LLM to con-
struct a knowledge store, and subsequent retrieval
of relevant information from the knowledge store
by the small-scale model to accomplish the task.
More specifically, we employ the technique of soft
prompt tuning to fine-tune an LLM, ensuring that
it generates in-domain samples. Additionally, we
introduce the Proximal Policy Optimization (PPO)
(Schulman et al., 2017) reinforcement learning al-
gorithm to enhance the generation quality. As a
final step, the small-scale model learns to retrieve
pertinent information from the knowledge store.
We perform an extensive set of experiments on
truly low-resource and challenging tasks sourced
from SuperGLUE (Wang et al., 2019a) and GLUE
(Wang et al., 2019b) benchmarks. The experimen-
tal results demonstrate that RetriKT significantly
enhances the performance of small-scale models
and outperforms previous SOTA knowledge dis-
tillation methods by leveraging the knowledge of
LLMs. This indicates the effectiveness and prac-
ticality of the retrieval-based knowledge transfer
paradigm for extreme model compression.
Our contributions can be summarized as follows:
•We propose a new compression paradigm
called Retrieval-based Knowledge Transfer,
which aims to transfer knowledge from
LLMs to extremely small-scale models. This
paradigm addresses the challenge of achiev-
ing extreme model compression when there is
a significant gap in model scale.
•We introduce the reinforcement learning algo-
rithm PPO to enhance the generation quality,
and carefully design the reward function. This
technique contributes to improving the diver-
sity and accuracy of the knowledge extracted
from LLMs used for knowledge transfer.
•We conduct extensive experiments on low-
resource tasks from the SuperGLUE and
GLUE benchmarks. The results demonstrate
that RetriKT significantly enhances the perfor-
mance of small-scale models and outperforms
previous SOTA knowledge distillation meth-
ods by leveraging the knowledge from LLMs.2 Related Work
This paper involves three areas of work: knowl-
edge distillation, reinforcement learning, and data
augmentation.
2.1 Knowledge Distillation
We first introduce related work of knowledge
distillation (KD), which can be categorized as
response-based, feature-based, and relation-based
KD. Response-based KD was initially introduced
by Hinton et al. (2015), which transfers label
knowledge by minimizing the KL-divergence be-
tween predicted distributions of the teacher and
the student. Building upon this concept, Sanh et al.
(2019); Liang et al. (2021b) applied response-based
KD to tasks such as masked language modeling
and text classification, resulting in smaller models
with slight performance drops. Feature-based ap-
proaches which are initially introduced by Romero
et al. (2015), entail aligning the feature activations
between the teacher and the student models. Build-
ing upon this concept, more recent techniques have
expanded the scope by incorporating hidden rep-
resentations of the [CLS] token as indicators (Sun
et al., 2019), matching hidden representations of
all tokens (Jiao et al., 2020), and introducing cus-
tomized feature-based distillation (Sun et al., 2020).
On the other hand, relation-based methods (Park
et al., 2021; Liu et al., 2022a; Jiao et al., 2020;
Wang et al., 2020, 2021a; Wu et al., 2023) aim to
emphasize the importance of capturing and utiliz-
ing relations among multi-granularity representa-
tions in both horizontal and vertical directions.
2.2 Reinforcement Learning
Recently, reinforcement learning (RL) has gained
significant attention in the field of language mod-
eling. This approach has been successfully ap-
plied to various language tasks, including summa-
rization (Paulus et al., 2018; Ziegler et al., 2019;
Stiennon et al., 2020; Wu et al., 2021), dialogue
systems (Zhou et al., 2017; Jaques et al., 2020;
Hancock et al., 2019), machine translation (Bah-
danau et al., 2017; Kreutzer et al., 2018; Kiegeland
and Kreutzer, 2021), semantic parsing (Lawrence
and Riezler, 2018), story generation (Zhou and
Xu, 2020), and question generation (Pang and He,
2021). Concurrently, there has been a growing in-
terest in using RL to align language models with
human preferences across a wide range of language
tasks. For example, Ouyang et al. (2022) employed

--- PAGE 3 ---
LLM
[CLS]...Reward Model
...a. LLM with soft P-tuning
K Most Similar SamplesKey ([CLS])Value ([True, False])sentence emb[0.1, 0.9]sentence emb[0.7, 0.3]……Soft Logits
Open-book knowledge-store...Small Scale Model
[CLS]b. Reinforcement learning
c. Retrieval-based Knowledge TransferKey for Knowledge StoreSimilarityNearest EmbeddingValue1.0sentence emb[0.8, 0.2]0.7sentence emb[0.7, 0.3]……LabelValueTrue[0.1, 0.9]False[0.7, 0.3]Aggregation
KLDivergenceR=(Raccuracy+αRdiversity)*BP  RewardSimilarity Distribution
Similarity DistributionMLPSoft PromptsFigure 1: The framework of RetriKT which consists of three steps: (1) fine-tune additional soft prompts for the LLM
by supervised learning; (2) further fine-tune the soft prompts by reinforcement learning to enhance the generation
quality; (3) create the knowledge store based on the knowledge extracted from the LLM, and teach the small-scale
model how to retrieve relevant knowledge from it. (Retrieval-based Knowledge Transfer)
RL techniques, specifically Proximal Policy Op-
timization (PPO) (Schulman et al., 2017), to fine-
tune a large language model and align it with mod-
els of human preference.
2.3 Data Augmentation
Wu et al. (2019) and Kumar et al. (2020) have
introduced a method to create synthetic data by
randomly masking words in the original training
instances. Another works (Ding et al., 2020; Yang
et al., 2020; Anaby-Tavor et al., 2020) involved
utilizing Language Models (LMs) and Pre-trained
Language Models (PLMs) to directly generate syn-
thetic data for NLU tasks. Wang et al. (2021b,
2022) proposed the use of hard prompts and soft
prompts to generate synthetic data. In a related
study, Liu et al. (2021b); Zan et al. (2022) have
uncovered the complementary nature of PLMs and
classical approaches.
3 Methodology
Our approach revolves around extracting knowl-
edge from the parameters of a Large Language
Model (LLM) and leveraging it for the benefit of
an extreme small-scale model through retrieval.
We introduce the technique to extract in-domain
knowledge from the LLM for each task by freez-
ing its parameters and fine-tuning additional softprompts using the corresponding dataset (section
3.1). To further enhance the generation quality,
we introduce reinforcement learning and carefully
design the reward function to fine-tune the soft
prompts (section 3.2). Subsequently, we extract the
in-domain knowledge from the LLM and create a
knowledge store (section 3.3). Finally, we enable
the small-scale model to retrieve and effectively
utilize the knowledge generated by the LLM to
successfully perform specific tasks (section 3.4).
3.1 Soft Prompt Tuning
To enhance the extraction of in-domain knowledge
from the LLM, it is crucial to introduce appropriate
prompts that precede the inputs, effectively guid-
ing the text generation process (e.g. "Generate rte
examples according to the following keywords:").
However, relying solely on manual design often
leads to limited diversity and effectiveness in the
generated knowledge. To overcome such limita-
tions, we employ trainable parameters known as
soft prompts, thereby replacing manual templates.
We only need to update these soft prompts and
keep all other parameters of the LLM fixed during
training for each specific task.
More specifically, we utilize P-tuning v2 (Liu
et al., 2022b), which involves incorporating a soft
prompt into each Transformer (Vaswani et al.,

--- PAGE 4 ---
2017) layer. The soft prompt is represented as
a sequence of trainable vectors, denoted as Pj=
{pj
1, . . . , pj
k}, where jis the j-th layer and kis a hy-
perparameter which represents the prompt length.
Thei-th hidden states at the j-th layer, denoted as
hj
i, can be expressed as follows:
hj
i=

pj
i i≤k
wi i > k∧j= 0
Trans (Hj−1)i Else(1)
where Trans ()represents the Transformer layer,
Hjrepresents all hidden states at j-th layer and
wiis the word embedding of the input text.
In order to generate diverse samples, we adopt
the approach presented in PromDA (Wang et al.,
2022), which involves generating samples from
both the Input View and the Output View . The Input
View is conditioned on the keywords present in the
input texts, which are extracted by the unsupervised
keyword extraction algorithm Rake (Rose et al.,
2010). On the other hand, the Output View is con-
ditioned on the labels. Additionally, we train two
sets of soft prompts, namely Pinput andPoutput ,
which are specifically designed to generate samples
from the Input View and the Output View respec-
tively. Each sample S= (Y, X)generated by
the LLM consists of two parts, namely the label
Y={yi}ly
i=1and the input text X={xi}lx
i=1. The
supervised learning object for soft prompts Pinput
andPoutput can be formalized as:
Linput =−ly+lxX
i=1log(p(si|s<i,Pinput, K)), (2)
Loutput =−ly+lxX
i=1log(p(si|s<i,Poutput , Y)),(3)
where KandYrepresent the keywords of the input
text and the label, respectively.
3.2 Reinforcement Learning
Constructing a knowledge store that combines ac-
curacy and diversity is essential for enabling the
small-scale model to effectively retrieve relevant
information. However, traditional supervised learn-
ing methods are unable to optimize these learn-
ing objectives due to the lack of per-token differ-
entiability. Therefore, we employ reinforcement
learning to improve the generation quality. This
approach involves guaranteeing that the generated
texts align with the labels, thereby ensuring theaccuracy of the knowledge store. Additionally, it
ensures that the generated samples exhibit diver-
sity, thus fulfilling the diversity requirement of the
knowledge store.
More specifically, we utilize the original dataset
to train a classification model, which serves as the
reward model denoted as RM(). For each gener-
ated sample, comprising both input text and label,
we assess the confidence score of the text’s label
using the reward model, thereby obtaining the ac-
curacy reward Raccuracy . Additionally, we evaluate
the diversity of the generated samples by employ-
ing the Self-Bleu metric (Zhu et al., 2018), which
calculates the Bleu score (Papineni et al., 2002)
considering other samples as references. Subse-
quently, we calculate the diversity reward Rdiversity
as1−b3, where b3represents the Bleu-3 metric.
To avoid the generation of overly simplistic pat-
terns by the LLM, we apply a length penalty. If
the length of a generated sample falls below a cer-
tain threshold lmin, the reward is multiplied by a
discount factor:
BP=1 l≥lmin
exp(1−lmin
l) l < l min(4)
where lrepresents the length of the generated sam-
ple. The final reward for each generated sample
is:
R= (Raccuracy +αRdiversity )×BP,(5)
where αis a hyperparameter to balance the weight
of accuracy reward and diversity reward.
Following previous studies (Ouyang et al., 2022;
Bai et al., 2022), we employ Proximal Policy Op-
timization (PPO) (Schulman et al., 2017) as the
reinforcement learning algorithm to fine-tune the
soft prompts Pinput andPoutput . To ensure reliable
performance in generating in-domain samples, we
incorporate supervised gradients into the PPO gra-
dients. Our objective is to minimize the following
loss function:
LLLM=Lp+Lv+β∗ Lsft, (6)
whereLpandLvare actor loss and critic loss of the
PPO algorithm respectively, Lsftrepresents Linput
andLoutput forPinput andPoutput respectively,
andβis the hyperparameter which controls the
strength of supervised gradients.

--- PAGE 5 ---
3.3 Knowledge Store
We employ the fine-tuned LLM to generate sam-
ples and construct a knowledge store for each task.
Specifically, for each sample in the original dataset
D, we utilize the keywords extracted by the Rake
algorithm and labels as inputs for the model. Each
sample is used mtimes, and in each iteration, n
samples are generated. To ensure distinctiveness
among the generated samples, we adopt p-sampling
(Holtzman et al., 2020), which constrains tokens to
the smallest possible set whose cumulative proba-
bility exceeds the probability parameter p. The set
of samples generated from keywords is denoted as
DI, while the set generated from labels is denoted
asDO. The quantity of both DIandDOismn|D|,
where |D|represents the quantity of the original
dataset D.
To further enhance the diversity of generated
samples, we utilize the label of each sample in
DIas the model input to generate a sample each
time, resulting in the sample set DIO. Similarly,
we use the keyword of each example in DOas the
model input to generate a sample each time, result-
ing in the sample set DOI. The final knowledge
store is composed of these sample sets, namely
DSDISDOSDIOSDOI, which theoretically
yields a total of (1 + 4 mn)|D|samples. However,
there is a possibility of duplicates in the generated
samples, so we remove them.
As illustrated in Figure 1, the key for each sam-
ple in the knowledge store is the sentence embed-
ding produced by the small-scale model. These
embeddings enable the model to retrieve relevant
information from the knowledge store. The value
of each sample is the probability distribution pro-
duced by the reward model RM()for each task.
3.4 Retrieval-based Knowledge Transfer
Due to the limited number of parameters in the
small-scale model, it is not feasible to directly
memorize all the knowledge extracted from the
LLM. Therefore, we do not train the small-scale
model directly with the generated samples. Instead,
we leverage the knowledge in a retrieval manner.
Specifically, we utilize the reward model RM()
to provide the sentence representation fT(xi)for
each generated sample xi, which is then used to
calculate cosine similarity scores with other gen-
erated samples in the mini-batch. The similar-
ity score list obtained from the reward model can
be denoted as ST
i={sT(xi, xj)}j∈[1,N]∧j̸=i=Dataset |Train| |Dev| |Test| Metrics
BoolQ 9427 3270 3245 acc.
CB 250 56 250 acc.
COPA 400 100 500 acc.
RTE 2490 277 3000 acc.
WiC 5428 638 1400 acc.
Cola 8551 1043 1063 mcc.
Table 1: The data statistics and metrics of all tasks.
{ϕ(fT(xi), fT(xj))}j∈[1,N]∧j̸=i, where Nrepre-
sents the batch size and ϕdenotes the cosine simi-
larity. Similarly, we can obtain the similarity score
listSS
ifrom the small-scale model. These similar-
ity scores are then normalized in a listwise way to
obtain the relevance distributions:
˜sT(xi, xj) =esT(xi,xj)/τ1
P
k∈[1,N]∧k̸=iesT(xi,xk)/τ1,(7)
˜sS(xi, xj) =esS(xi,xj)/τ2
P
k∈[1,N]∧k̸=iesS(xi,xk)/τ2,(8)
where τ1andτ2are temperature hyperparameters
to smooth the distributions.
Our objective is to enable the small-scale model
to learn how to retrieve relevant information from
the knowledge store. To achieve this, we mini-
mize the KL-divergence between the two relevance
distributions ˜ST
i={˜sT(xi, xj)}j∈[1,N]∧j̸=iand
˜SS
i={˜sS(xi, xj)}j∈[1,N]∧j̸=ias the learning ob-
ject for the small-scale model:
Lsmall =NX
i˜SS
i˙log˜SS
i
˜ST
i. (9)
During the inference of the small-scale model,
we retrieve the k most relevant samples from the
knowledge store for each test sample x, denoted
as{xi}k
i=1. To determine the weight of each re-
trieved sample xi, we calculate the similarity score
between the test sample xand the retrieved sample
xi. The final prediction logit score for each class
of the small-scale model is obtained as follows:
pS
c=ϕ(x, xi)Pk
j=1ϕ(x, xj)pT
c, (10)
where cis a specific class and pT
cis the confidence
score of the label cpredicted by the reward model
RM(). The final prediction of the small-scale
model is the class with the largest logit score.

--- PAGE 6 ---
Model #Params WiC CB COPA RTE BoolQ CoLA Avg.
EncT5 xl(Teacher) 3B 75.71 98.20 92.00 92.06 88.99 71.63 86.43
BERT 2(Student) 7.2M 59.72 78.57 55.00 61.37 68.81 23.35 57.80
+ Vanilla KD (Hinton et al., 2015) 7.2M 60.66 78.57 56.00 62.09 68.72 27.14 58.86
+ Vanilla KD (w. TA) (Hinton et al., 2015) 7.2M 61.13 82.14 58.00 61.37 69.33 26.12 59.68
+ MSGKD (w. TA) (Liu et al., 2022a) 7.2M 59.72 83.93 60.00 62.45 68.13 29.39 60.60
+ AD-KD (w. TA) (Wu et al., 2023) 7.2M 62.85 83.93 61.00 62.82 69.60 31.89 62.02
+ RetriKT-KD 7.2M 63.79 89.29 63.00 64.26 71.10 40.94 65.40
+ RetriKT-Retrieval 7.2M 66.61 94.64 66.00 66.43 74.62 47.87 69.36
BERT 4(Student) 13.5M 62.85 85.71 60.00 65.34 70.86 38.69 63.91
+ Vanilla KD (Hinton et al., 2015) 13.5M 62.70 85.71 62.00 66.43 70.73 39.16 64.46
+ Vanilla KD (w. TA) (Hinton et al., 2015) 13.5M 63.79 87.50 60.00 66.06 70.98 40.81 64.86
+ MSGKD (w. TA) (Liu et al., 2022a) 13.5M 62.38 87.50 63.00 65.34 71.13 40.78 65.02
+ AD-KD (w. TA) (Wu et al., 2023) 13.5M 64.89 87.50 64.00 66.43 72.81 41.77 66.23
+ RetriKT-KD 13.5M 68.65 92.86 72.00 74.01 77.22 58.16 73.82
+ RetriKT-Retrieval 13.5M 69.44 94.64 74.00 75.45 78.75 60.77 75.51
Table 2: Main experimental results (%) on six low-resource tasks. We also report the quantity of parameters of each
PLM (without embeddings). We re-implement all baseline models based on the released code provided by AD-KD
(Wu et al., 2023), and incorporate BERT baseas the TA model (w. TA). There are two versions of RetriKT, one of
which is trained by Vanilla KD by generated samples (RetriKT-KD), and the other is trained by retrieval learning
object (RetriKT-Retrieval). The best results of each backbone are shown in bold. Results are statistically significant
with respect to all baselines on each student model (all p-value < 0.005).
4 Experimental Setup
4.1 Datasets
We assess the performance of our method on mul-
tiple datasets from the SuperGLUE beachmark
(Wang et al., 2019a) and GLUE benchmark (Wang
et al., 2019b), including BoolQ (Clark et al., 2019),
CB (De Marneffe et al., 2019), COPA (Roemmele
et al., 2011), RTE, WiC (Pilehvar and Camacho-
Collados, 2019), CoLA (Warstadt et al., 2019).
These datasets pose a challenge for small-scale
models as they are considered difficult and low-
resource, with training samples numbering fewer
than 10K (details in Table 1).
4.2 Baselines
We compare our methods with Vanilla KD (Hinton
et al., 2015) and recent strong KD methods includ-
ing MSGKD (Liu et al., 2022a) and AD-KD (Wu
et al., 2023), which are re-implemented based on
the released code provided by AD-KD. We fine-
tune EncT5 xl(Liu et al., 2021a) as the teacher
model, and train BERT baseusing the Vanilla KD
method as the assistant model (TA) for all base-
lines. We carry out grid-search of learning rate
∈ {1e-5, 2e-5, 3e-5, 4e-5 }, batch size ∈ {4,8,16}for datasets COPA and CB, and batch size ∈
{16,32,64}for other datasets. The training epoch
is set to 40 for CB and COPA, 20 for RTE and 10
for other datasets. We present the results on the
validation set obtained from the best checkpoint
during training.
4.3 Implementation
We implement all experiments with the deep learn-
ing framework PyTorch based on PromDA (Wang
et al., 2022) and trl library (von Werra et al., 2020)
on up to eight NVIDIA Tesla A100 GPUs (80GB
memory). We build the LLM based on T5 xlwith
3B parameters (Raffel et al., 2020), and translate
each dataset into a single-sentence format accord-
ing to the template provided by T5. The pre-
processed example of each dataset is shown in Ap-
pendix B. We utilize two small-scale BERT models
released by Turc et al. (2019), one with 4 Trans-
former layers, 512 hidden neurons and 8 attention
heads, and the other with 2 Transformer layers, 512
hidden neurons and 8 attention heads.
First, we tune the soft prompts Pinput and
Poutput by supervised learning as the reference
and initial models for PPO algorithm. The prompt
length is set to 8 for all datasets. The learning

--- PAGE 7 ---
rate and batch size is set to 1e-3 and 64, respec-
tively. The training step is set to 10K for COPA
and CB, 20K for RTE, and 40K for other datasets.
We fine-tune EncT5 xlas the reward model for each
dataset, which is also the teacher model for all base-
line models for a fair comparison. Then we train
the soft prompts through a combination of super-
vised learning and reinforcement learning by the
loss function defined in Eq.(6). We set αandβ
to 0.2 and 1 respectively. Generate number nand
probability pfor top-p sampling are always set to
5 and 0.9 respectively, while sample time mis set
to 64 for COPA and CB, 16 for RTE, and 8 for
other datasets. Finally, we train the small-scale
BERT models with a grid search of learning rate
∈{2e-5, 3e-5}, temperate combinations (τ1, τ2)∈
{(0.2, 0.1), (0.1, 0.05)}. The batch size is set to
128. There are two versions of RetriKT, one of
which is trained by Vanilla KD by generated sam-
ples (RetriKT-KD), and the other is trained by re-
trieval learning object (RetriKT-Retrieval). More
training details for PPO algorithm can be found in
Appendix A
5 Experimental Results and Analysis
5.1 Main Results
As presented in Table 2, it is evident that both
RetriKT-KD and RetriKT-Retrieval outperform pre-
vious KD methods across all backbones, which
demonstrates the effectiveness of our approach. For
instance, compared to the previous state-of-the-art
method AD-KD, RetriKT-Retrieval demonstrates
significant improvements: 7.34% on BERT 2and
9.28% on BERT 4. Notably, RetriKT-Retrieval con-
sistently outperforms RetriKT-KD, especially on
BERT 2, indicating that retrieving relevant informa-
tion from the knowledge store is more suitable for
small-scale models rather than attempting to mem-
orize all the knowledge. Furthermore, we observe
comparable performance between Vanilla KD and
Vanilla KD (w.TA), suggesting limitations in using
the TA model for knowledge transfer. A more ef-
ficient approach for transferring knowledge from
the LLM to the small-scale model is to prompt the
LLM to generate in-domain knowledge, which can
be retrieved by the small-scale model as relevant
information for effective inference.
5.2 Ablation Study
To investigate the impact of different components
of reinforcement learning in our approach, we con-PLM Model CB COPA CoLA
BERT 2RetriKT-KD 89.29 63.00 40.94
w/o RL 87.50 61.00 36.38
w/o R accuracy 87.50 60.00 40.53
w/o R diversity 85.71 62.00 38.22
w/o BP 87.50 60.00 39.13
RetriKT-Retrieval 94.64 66.00 47.87
w/o RL 89.29 62.00 41.57
w/o R accuracy 91.07 62.00 45.32
w/o R diversity 87.50 61.00 43.24
w/o BP 89.29 62.00 44.98
BERT 4RetriKT-KD 92.86 72.00 58.16
w/o RL 89.29 67.00 53.18
w/o R accuracy 91.07 65.00 57.78
w/o R diversity 89.29 69.00 57.01
w/o BP 91.07 66.00 57.27
RetriKT-Retrieval 94.64 74.00 60.77
w/o RL 92.86 69.00 56.45
w/o R accuracy 92.86 69.00 59.94
w/o R diversity 91.07 70.00 58.25
w/o BP 92.86 69.00 57.82
Table 3: Ablation studies of different components uti-
lizing BERT 2and BERT 4as the small-scale models on
datasets CB, COPA and CoLA.
duct a series of ablation studies. We remove re-
inforcement training (w/o RL), accuracy reward
Raccuracy , diversity reward R diversity , and length
penalty BP, and evaluate the results on CB, COPA,
and CoLA, as shown in Table 3. Several key obser-
vations can be drawn from the experimental results.
Firstly, it is evident that the model performance of
both RetriKT-KD and RetriKT-Retrieval declines
without RL or any component of RL. This high-
lights the effectiveness of our reward function de-
sign and RL, which enhance the generation quality
of the LLM. Secondly, when any component of
RL is removed, the model performance on CoLA
shows relatively smaller decreases compared to
CB and COPA. This suggests that the generation
quality of the LLM has a more pronounced im-
pact on smaller datasets, while larger datasets ex-
hibit a certain level of robustness. However, RL
consistently improves the generation quality of
the LLM, thereby enhancing overall model perfor-
mance. Finally, it is worth mentioning that RetriKT-
Retrieval consistently outperforms RetriKT-KD
across almost all settings, further validating the
effectiveness of retrieval-based knowledge transfer
for small-scale models.

--- PAGE 8 ---
Metric Model WiC CB COPA RTE BoolQ CoLA
Self-BleuRetriKT 0.912 0.948 0.970 0.923 0.884 0.959
RetriKT w/o RL 0.945 0.968 0.983 0.937 0.939 0.973
Cross EntropyRetriKT 0.734 0.781 0.786 0.781 0.760 0.579
RetriKT w/o RL 0.758 0.896 0.796 0.794 0.779 0.618
Table 4: Diversity and accuracy analysis for the generated knowledge. Self-Bleu reflects the diversity of knowledge,
while cross entropy reflects the accuracy of knowledge. In both metrics, smaller values indicate better performance.
4 8 16 32 64 128
Sample Time m565860626466 Acc.
RetriKT-KD
RetriKT-Retrieval
Finetune
(a) BERT 2
4 8 16 32 64 128
Sample Time m6062646668707274 Acc.
RetriKT-KD
RetriKT-Retrieval
Finetune (b) BERT 4
Figure 2: Effect of the sample times m. Results are the
model performance on the COPA dataset using BERT 2
and BERT 4. The Finetune method is trained by the
original dataset so it is independent of m.
2-128 2-512 4-512 6-768 12-768
Model Size5560657075 Acc.
RetriKT-KD
RetriKT-Retrieval 
Finetune
(a) COPA
2-128 2-512 4-512 6-768 12-768
Model Size10203040506070 Mcc.
RetriKT-KD
RetriKT-Retrieval 
Finetune (b) CoLA
Figure 3: Effect of the model size. x-axis is the number
of layers and hidden size. Results are the model perfor-
mance on the COPA and CoLA datasets.
5.3 Detailed Analysis
Effect of the Sample Times We conduct experi-
ments on the COPA dataset to investigate the im-
pact of the sample time mon model performance.
As shown in Figure 2, with a small value of m,
the performance of both RetriKT-KD and RetriKT-
Retrieval is comparable. However, as mincreases,
we observe a significant improvement in the perfor-
mance of RetriKT-Retrieval compared to RetriKT-
KD. This finding suggests that small-scale models,
constrained by their limited parameters, are un-
able to effectively memorize such a large amount
of knowledge. Therefore, retrieval-based knowl-
edge transfer proves to be a more favorable ap-proach. Furthermore, as mcontinues to increase,
we observe a corresponding increase in the perfor-
mance of both RetriKT-KD and RetriKT-Retrieval,
reaching a certain threshold. This indicates that
increasing the number of generated samples can
enhance the diversity of knowledge. However, due
to the limited number of keywords extracted from
the original dataset, the diversity of knowledge
plateaus once the generation threshold is reached.
Consequently, the model performance does not
show further improvement beyond this point.
Effect of the Model Size We conduct an in-
vestigation into the impact of model size on per-
formance using two datasets, COPA and CoLA.
Our experimental results shown in Figure 3 reveal
that when employing a smaller model, RetriKT-
Retrieval consistently outperforms RetriKT-KD,
showcasing the superiority of retrieval-based
knowledge transfer for small-scale models. Ad-
ditionally, we observe that both RetriKT-Retrieval
and RetriKT-KD consistently outperform the train-
ing of the small-scale model solely fine-tuning by
the original dataset. This observation further em-
phasizes the effectiveness of knowledge extraction
from the LLM and its application in enhancing
model performance.
Accuracy and Diversity In this section, we ex-
amine whether the LLM trained through reinforce-
ment learning can generate more accurate and di-
verse knowledge. To evaluate the diversity of
knowledge, we employ the Self-Bleu metric on the
generated samples, while the accuracy of knowl-
edge is measured using the cross-entropy between
the probability distribution predicted by the reward
model and the label generated by the LLM. In both
metrics, smaller values indicate better performance.
As shown in Table 4, the results demonstrate that
training the LLM through reinforcement learning
leads to the generation of more accurate and diverse
knowledge across all datasets. This outcome high-
lights the effectiveness of our reward function de-

--- PAGE 9 ---
sign. Additionally, we observe that larger datasets
tend to yield samples with smaller Self-Bleu met-
rics. We conjecture that this phenomenon is a re-
sult of the larger dataset’s ability to extract a wider
range of diverse keywords, thereby enabling the
generation of more varied knowledge.
6 Conclusion
Our study tackles the task of compressing LLMs
and introduces a pioneering compression paradigm
called Retrieval-based Knowledge Transfer. This
approach efficiently transfers the knowledge of
LLMs to small-scale models by creating a knowl-
edge store, enabling the small model to retrieve per-
tinent information during inference. Through ex-
tensive experiments conducted on commonly used
benchmarks, we demonstrate that our framework
substantially improves the performance of small-
scale models by leveraging the knowledge con-
tained within LLMs. In future research, we plan to
investigate the application and performance of our
proposed method on even larger language models,
such as T5-11B.
Limitations
In this section, we discuss the limitations of our
work as follows. Firstly, due to limited computa-
tional resources, we did not attempt experiments
with larger models such as T5-11B. Furthermore,
resource constraints constrained our grid search
for hyperparameters within a limited range, which
potentially leaves room for enhancing the metrics
showcased in this paper. Secondly, our proposed
method requires the establishment of a knowledge
store. Compared to other model compression meth-
ods, our approach introduces additional storage
space and incurs slightly additional time for re-
trieval
Acknowledgements
This work is supported by National Key R&D Pro-
gram of China (No. 2021YFC3340303) and Na-
tional Natural Science Foundation of China (NSFC
Grant No. 62122089). Jingang Wang is funded by
Beijing Nova Program (Grant NO. 20220484098).
We sincerely thank all reviewers for their valuable
comments and suggestions, which are crucial for
improving our work. We would also like to ac-
knowledge Angela Li for her contributions in cre-
ating the figures used in this work.References
Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,
Amir Kantor, George Kour, Segev Shlomov, Naama
Tepper, and Naama Zwerdling. 2020. Do not have
enough data? deep learning to the rescue! In The
Thirty-Fourth AAAI Conference on Artificial Intelli-
gence, AAAI 2020, The Thirty-Second Innovative Ap-
plications of Artificial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational
Advances in Artificial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020 , pages 7383–
7390. AAAI Press.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. In 5th Inter-
national Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings . OpenReview.net.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan,
Nicholas Joseph, Saurav Kadavath, Jackson Kernion,
Tom Conerly, Sheer El Showk, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Tristan Hume,
Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
Nanda, Catherine Olsson, Dario Amodei, Tom B.
Brown, Jack Clark, Sam McCandlish, Chris Olah,
Benjamin Mann, and Jared Kaplan. 2022. Train-
ing a helpful and harmless assistant with rein-
forcement learning from human feedback. CoRR ,
abs/2204.05862.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. Boolq: Exploring the surprising
difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Volume 1
(Long and Short Papers) , pages 2924–2936. Associa-
tion for Computational Linguistics.
Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke
Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,
Sheng Li, Dajiang Zhu, Hongmin Cai, Quanzheng
Li, Dinggang Shen, Tianming Liu, and Xiang Li.
2023. Chataug: Leveraging chatgpt for text data
augmentation. CoRR , abs/2302.13007.
Marie-Catherine De Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
Inproceedings of Sinn und Bedeutung , volume 23,
pages 107–124.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for

--- PAGE 10 ---
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers) ,
pages 4171–4186. Association for Computational
Linguistics.
Bosheng Ding, Linlin Liu, Lidong Bing, Canasai Kru-
engkrai, Thien Hai Nguyen, Shafiq R. Joty, Luo Si,
and Chunyan Miao. 2020. DAGA: data augmentation
with a generation approach for low-resource tagging
tasks. CoRR , abs/2011.01549.
Mitchell A. Gordon, Kevin Duh, and Nicholas Andrews.
2020. Compressing BERT: studying the effects of
weight pruning on transfer learning. In Proceedings
of the 5th Workshop on Representation Learning for
NLP , RepL4NLP@ACL 2020, Online, July 9, 2020 ,
pages 143–155. Association for Computational Lin-
guistics.
Braden Hancock, Antoine Bordes, Pierre-Emmanuel
Mazaré, and Jason Weston. 2019. Learning from
dialogue after deployment: Feed yourself, chatbot!
InProceedings of the 57th Conference of the Associ-
ation for Computational Linguistics, ACL 2019, Flo-
rence, Italy, July 28- August 2, 2019, Volume 1: Long
Papers , pages 3667–3684. Association for Computa-
tional Linguistics.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
2015. Distilling the knowledge in a neural network.
CoRR , abs/1503.02531.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text
degeneration. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net.
Natasha Jaques, Judy Hanwen Shen, Asma Ghande-
harioun, Craig Ferguson, Àgata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind W. Picard. 2020.
Human-centric dialog training via offline reinforce-
ment learning. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP 2020, Online, November 16-20,
2020 , pages 3985–4003. Association for Computa-
tional Linguistics.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.
Tinybert: Distilling BERT for natural language un-
derstanding. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020, Online Event,
16-20 November 2020 , volume EMNLP 2020 of Find-
ings of ACL , pages 4163–4174. Association for Com-
putational Linguistics.
Samuel Kiegeland and Julia Kreutzer. 2021. Revisiting
the weaknesses of reinforcement learning for neu-
ral machine translation. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2021, Online,
June 6-11, 2021 , pages 1673–1681. Association for
Computational Linguistics.Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and
Stefan Riezler. 2018. Can neural machine translation
be improved with user feedback? In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2018,
New Orleans, Louisiana, USA, June 1-6, 2018, Vol-
ume 3 (Industry Papers) , pages 92–105. Association
for Computational Linguistics.
Varun Kumar, Ashutosh Choudhary, and Eunah Cho.
2020. Data augmentation using pre-trained trans-
former models. CoRR , abs/2003.02245.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2020. ALBERT: A lite BERT for self-supervised
learning of language representations. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020 . OpenReview.net.
Carolin Lawrence and Stefan Riezler. 2018. Improving
a neural semantic parser by counterfactual learning
from human bandit feedback. In Proceedings of the
56th Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2018, Melbourne, Australia,
July 15-20, 2018, Volume 1: Long Papers , pages
1820–1830. Association for Computational Linguis-
tics.
Chen Liang, Simiao Zuo, Minshuo Chen, Haoming
Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and
Weizhu Chen. 2021a. Super tickets in pre-trained
language models: From model compression to im-
proving generalization. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing, ACL/IJCNLP
2021, (Volume 1: Long Papers), Virtual Event, Au-
gust 1-6, 2021 , pages 6524–6538. Association for
Computational Linguistics.
Kevin J. Liang, Weituo Hao, Dinghan Shen, Yufan Zhou,
Weizhu Chen, Changyou Chen, and Lawrence Carin.
2021b. Mixkd: Towards efficient distillation of large-
scale language models. In 9th International Confer-
ence on Learning Representations, ICLR 2021, Vir-
tual Event, Austria, May 3-7, 2021 . OpenReview.net.
Chang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan
Zhao. 2022a. Multi-granularity structural knowl-
edge distillation for language model compression.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), ACL 2022, Dublin, Ireland, May
22-27, 2022 , pages 1001–1011. Association for Com-
putational Linguistics.
Frederick Liu, Siamak Shakeri, Hongkun Yu, and Jing
Li. 2021a. Enct5: Fine-tuning T5 encoder for non-
autoregressive tasks. CoRR , abs/2110.08426.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-
iao Du, Zhilin Yang, and Jie Tang. 2022b. P-tuning:

--- PAGE 11 ---
Prompt tuning can be comparable to fine-tuning
across scales and tasks. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), ACL 2022,
Dublin, Ireland, May 22-27, 2022 , pages 61–68. As-
sociation for Computational Linguistics.
Xuebo Liu, Longyue Wang, Derek F. Wong, Liang
Ding, Lidia S. Chao, Shuming Shi, and Zhaopeng Tu.
2021b. On the complementarity between pre-training
and back-translation for neural machine translation.
InFindings of the Association for Computational
Linguistics: EMNLP 2021 , pages 2900–2907, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining
approach. CoRR , abs/1907.11692.
Seyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang
Li, Nir Levine, Akihiro Matsukawa, and Hassan
Ghasemzadeh. 2020. Improved knowledge distilla-
tion via teacher assistant. In The Thirty-Fourth AAAI
Conference on Artificial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artificial
Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artificial In-
telligence, EAAI 2020, New York, NY, USA, February
7-12, 2020 , pages 5191–5198. AAAI Press.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In NeurIPS .
Richard Yuanzhe Pang and He He. 2021. Text genera-
tion by learning from demonstrations. In 9th Inter-
national Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA , pages 311–318. ACL.
Geondo Park, Gyeongman Kim, and Eunho Yang. 2021.
Distilling linguistic context for language model com-
pression. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Domini-
can Republic, 7-11 November, 2021 , pages 364–378.
Association for Computational Linguistics.
Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh,
and Qun Liu. 2021. ALP-KD: attention-based layerprojection for knowledge distillation. In Thirty-Fifth
AAAI Conference on Artificial Intelligence, AAAI
2021, Thirty-Third Conference on Innovative Ap-
plications of Artificial Intelligence, IAAI 2021, The
Eleventh Symposium on Educational Advances in Ar-
tificial Intelligence, EAAI 2021, Virtual Event, Febru-
ary 2-9, 2021 , pages 13657–13665. AAAI Press.
Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
marization. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings . OpenReview.net.
Mohammad Taher Pilehvar and José Camacho-Collados.
2019. Wic: the word-in-context dataset for evaluat-
ing context-sensitive meaning representations. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
2019, Volume 1 (Long and Short Papers) , pages 1267–
1273. Association for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res. , 21:140:1–140:67.
Rajkumar Ramamurthy, Prithviraj Ammanabrolu,
Kianté Brantley, Jack Hessel, Rafet Sifa, Christian
Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
2022. Is reinforcement learning (not) for natural
language processing?: Benchmarks, baselines, and
building blocks for natural language policy optimiza-
tion. CoRR , abs/2210.01241.
Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S. Gordon. 2011. Choice of plausible alter-
natives: An evaluation of commonsense causal rea-
soning. In Logical Formalizations of Commonsense
Reasoning, Papers from the 2011 AAAI Spring Sym-
posium, Technical Report SS-11-06, Stanford, Cali-
fornia, USA, March 21-23, 2011 . AAAI.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-
hou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. 2015. Fitnets: Hints for thin deep nets. In
3rd International Conference on Learning Represen-
tations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings .
Stuart Rose, Dave Engel, Nick Cramer, and Wendy
Cowley. 2010. Automatic keyword extraction from
individual documents. Text mining: applications and
theory , 1:1–20.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR ,
abs/1910.01108.

--- PAGE 12 ---
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ilic, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, Jonathan Tow, Alexander M. Rush,
Stella Biderman, Albert Webson, Pawan Sasanka Am-
manamanchi, Thomas Wang, Benoît Sagot, Niklas
Muennighoff, Albert Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina
McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile
Saulnier, Samson Tan, Pedro Ortiz Suarez, Vic-
tor Sanh, Hugo Laurençon, Yacine Jernite, Julien
Launay, Margaret Mitchell, Colin Raffel, Aaron
Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri
Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue,
Christopher Klamm, Colin Leong, Daniel van Strien,
David Ifeoluwa Adelani, and et al. 2022. BLOOM:
A 176b-parameter open-access multilingual language
model. CoRR , abs/2211.05100.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal policy
optimization algorithms. CoRR , abs/1707.06347.
Wonchul Son, Jaemin Na, Junyong Choi, and Wonjun
Hwang. 2021. Densely guided knowledge distillation
using multiple teacher assistants. In 2021 IEEE/CVF
International Conference on Computer Vision, ICCV
2021, Montreal, QC, Canada, October 10-17, 2021 ,
pages 9375–9384. IEEE.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F. Christiano. 2020. Learn-
ing to summarize from human feedback. CoRR ,
abs/2009.01325.
Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.
Patient knowledge distillation for BERT model com-
pression. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Nat-
ural Language Processing, EMNLP-IJCNLP 2019,
Hong Kong, China, November 3-7, 2019 , pages 4322–
4331. Association for Computational Linguistics.
Siqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang
Wang, and Jingjing Liu. 2020. Contrastive distil-
lation on intermediate representations for language
model compression. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2020, Online, November 16-20,
2020 , pages 498–508. Association for Computational
Linguistics.
Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin
Jiang, Qun Liu, Ping Luo, and Ngai Wong. 2022.
Compression of generative pre-trained language mod-
els via quantization. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022,
Dublin, Ireland, May 22-27, 2022 , pages 4821–4836.
Association for Computational Linguistics.Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. 2019. Well-read students learn better:
On the importance of pre-training compact models.
arXiv preprint arXiv:1908.08962 .
Solomon Ubani, Suleyman Olcay Polat, and Rod-
ney Nielsen. 2023. Zeroshotdataaug: Generating
and augmenting training data with chatgpt. CoRR ,
abs/2304.14334.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , pages 5998–6008.
Leandro von Werra, Younes Belkada, Lewis Tunstall,
Edward Beeching, Tristan Thrush, and Nathan Lam-
bert. 2020. Trl: Transformer reinforcement learning.
https://github.com/lvwerra/trl .
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. 2019a. Superglue: A stick-
ier benchmark for general-purpose language under-
standing systems. In Advances in Neural Information
Processing Systems 32: Annual Conference on Neu-
ral Information Processing Systems 2019, NeurIPS
2019, December 8-14, 2019, Vancouver, BC, Canada ,
pages 3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019b.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In 7th In-
ternational Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net.
Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,
and Furu Wei. 2021a. Minilmv2: Multi-head self-
attention relation distillation for compressing pre-
trained transformers. In Findings of the Associa-
tion for Computational Linguistics: ACL/IJCNLP
2021, Online Event, August 1-6, 2021 , volume
ACL/IJCNLP 2021 of Findings of ACL , pages 2140–
2151. Association for Computational Linguistics.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. In Advances in Neural
Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual .

--- PAGE 13 ---
Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu,
Chongyang Tao, Xiubo Geng, and Daxin Jiang. 2022.
Promda: Prompt-based data augmentation for low-
resource NLU tasks. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022,
Dublin, Ireland, May 22-27, 2022 , pages 4242–4255.
Association for Computational Linguistics.
Zirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.
2021b. Towards zero-label language learning. CoRR ,
abs/2109.09193.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Trans. Assoc. Comput. Linguistics , 7:625–641.
Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stien-
non, Ryan Lowe, Jan Leike, and Paul F. Christiano.
2021. Recursively summarizing books with human
feedback. CoRR , abs/2109.10862.
Siyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang,
and Rui Wang. 2023. AD-KD: attribution-driven
knowledge distillation for language model compres-
sion. CoRR , abs/2305.10010.
Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,
and Songlin Hu. 2019. Conditional BERT contex-
tual augmentation. In Computational Science - ICCS
2019 - 19th International Conference, Faro, Portu-
gal, June 12-14, 2019, Proceedings, Part IV , volume
11539 of Lecture Notes in Computer Science , pages
84–95. Springer.
Yiben Yang, Chaitanya Malaviya, Jared Fernandez,
Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,
Chandra Bhagavatula, Yejin Choi, and Doug Downey.
2020. G-daug: Generative data augmentation for
commonsense reasoning. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2020, Online Event, 16-20 November 2020 , volume
EMNLP 2020 of Findings of ACL , pages 1008–1025.
Association for Computational Linguistics.
Changtong Zan, Liang Ding, Li Shen, Yu Cao, Weifeng
Liu, and Dacheng Tao. 2022. On the complementar-
ity between pre-training and random-initialization for
resource-rich machine translation. In Proceedings of
the 29th International Conference on Computational
Linguistics, COLING 2022, Gyeongju, Republic of
Korea, October 12-17, 2022 , pages 5029–5034. Inter-
national Committee on Computational Linguistics.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan
Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng
Zhang, Yuxiao Dong, and Jie Tang. 2022. GLM-
130B: an open bilingual pre-trained model. CoRR ,
abs/2210.02414.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert:
Distillation-aware ultra-low bit BERT. In Proceed-
ings of the 2020 Conference on Empirical Methods inNatural Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 509–521. Association
for Computational Linguistics.
Li Zhou, Kevin Small, Oleg Rokhlenko, and Charles
Elkan. 2017. End-to-end offline goal-oriented di-
alog policy learning via policy gradient. CoRR ,
abs/1712.02838.
Wangchunshu Zhou and Ke Xu. 2020. Learning to com-
pare for better training and evaluation of open domain
natural language generation models. In The Thirty-
Fourth AAAI Conference on Artificial Intelligence,
AAAI 2020, The Thirty-Second Innovative Applica-
tions of Artificial Intelligence Conference, IAAI 2020,
The Tenth AAAI Symposium on Educational Advances
in Artificial Intelligence, EAAI 2020, New York, NY,
USA, February 7-12, 2020 , pages 9717–9724. AAAI
Press.
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan
Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A
benchmarking platform for text generation models.
InThe 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval,
SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 ,
pages 1097–1100. ACM.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul F. Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. CoRR ,
abs/1909.08593.

--- PAGE 14 ---
A Training Details for PPO
In this section, we present the training details of
PPO in Table. The meaning of these hyperparame-
ters is detailed in trl library (von Werra et al., 2020)
and NLPO (Ramamurthy et al., 2022).
B Preprocessed Examples
In this section, we provide examples of our prepro-
cessing for each of the data sets we consider.
B.1 boolq
Original input:
Question: Can you have a skunk as a pet
in canada
Passage: Skunks as pets – Canadian pet
skunks must be purchased from a
USDA-certified breeder in the United
States. An import permit is required
from the Canadian Food Inspection
Agency to bring the skunk into the
country. The skunk must be spayed
or neutered, and receive a microchip
implant or tattoo. A vet check fee
must also be paid. It is illegal to
keep striped skunks as pets in Canada.
Processed input: question: Can you have
a skunk as a pet in canada passage:
Skunks as pets – Canadian pet skunks
must be purchased from a USDA-certified
breeder in the United States. An import
permit is required from the Canadian
Food Inspection Agency to bring the
skunk into the country. The skunk must
be spayed or neutered, and receive a
microchip implant or tattoo. A vet check
fee must also be paid. It is illegal to
keep striped skunks as pets in Canada.
Label:{False: 0; True: 1}
B.2 CB
Original input:
Hypothesis: Valence was helping
Premise: Valence the void-brain, Valence
the virtuous valet. Why couldn’t
the figger choose his own portion of
titanic anatomy to shaft? Did he think
he was helping?Processed input: hypothesis: Valence was
helping premise: Valence the void-brain,
Valence the virtuous valet. Why couldn’t
the figger choose his own portion of
titanic anatomy to shaft? Did he think
he was helping?
Label:{entailment: 0; contradiction: 1;
neutral: 2}
B.3 CoLA
Original input:
Sentence: John made Bill master of
himself.
Processed input: sentence: John made Bill
master of himself.
Label:{unacceptable: 0; acceptable: 1}
B.4 COPA
Original input:
Question: effect
Premise: Political violence broke out in
the nation.
Choice 1: Many citizens relocated to the
capitol.
Choice 2: Many citizens took refuge in
other territories.
Processed input: premise: Political
violence broke out in the nation.
choice1: Many citizens relocated to the
capitol. choice2: Many citizens took
refuge in other territories. question:
effect
Label:{choice1: 0; choice2: 1}
B.5 RTE
Original input:
Sentence 1: A smaller proportion of
Yugoslavia’s Italians were settled
in Slovenia (at the 1991 national
census, some 3000 inhabitants of
Slovenia declared themselves as ethnic
Italians).
Sentence 2: Slovenia has 3,000
inhabitants.

--- PAGE 15 ---
Hyperparameters WiC CB COPA RTE BoolQ CoLA
learning rate 2e-3 2e-3 2e-3 2e-3 2e-3 2e-3
batch size 128 64 64 128 128 128
min batch size 32 16 16 32 32 32
epoch 20 100 100 50 10 20
ppo epoch 4 4 4 4 4 4
sample number 4 4 4 4 4 4
initial kl coeff 0.001 0.001 0.001 0.001 0.001 0.001
target kl 6 6 6 6 6 6
value function coeff 0.5 0.5 0.5 0.5 0.5 0.5
clip ratio 0.2 0.2 0.2 0.2 0.2 0.2
discount factor 0.99 0.99 0.99 0.99 0.99 0.99
gae lambda 0.95 0.95 0.95 0.95 0.95 0.95
Table 5: Hyperparameters for PPO
Processed input: sentence1: A smaller
proportion of Yugoslavia’s Italians
were settled in Slovenia (at the 1991
national census, some 3000 inhabitants
of Slovenia declared themselves as
ethnic Italians). sentence2: Slovenia
has 3,000 inhabitants.
Label:{entailment: 0; not_entailment: 1}
B.6 WiC
Original input:
POS:N
Sentence 1: It was the deliberation of
his act that was insulting .
Sentence 2: The deliberations of the jury
.
Word:deliberation
Processed input: sentence1: It was
the *deliberation* of his act that
was insulting. sentence2: The
*deliberations* of the jury. word:
deliberation
Label:{False: 0; True: 1}
C Faithfulness of the Generated
Knowledge
To further validate the faithfulness of the generated
knowledge, we gathered five volunteers to assess
the faithfulness of the generated texts for datasets
WiC, COPA, and CoLA. Each text will be given a
rating from 1 to 5. We randomly sampled 100 texts
from each dataset and calculate the average as theWiC COPA CoLA
4.61 4.36 4.49
Table 6: The faithfulness score of the generated texts
for datasets WiC, COPA and CoLA.
final faithfulness score. The results of the faithful-
ness evaluation are shown in Table 6, showing that
the faithfulness of generated texts for each dataset
is quite high.
