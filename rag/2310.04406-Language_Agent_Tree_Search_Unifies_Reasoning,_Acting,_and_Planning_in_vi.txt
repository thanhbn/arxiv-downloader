# 2310.04406.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2310.04406.pdf
# Kích thước tệp: 1054882 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tìm Kiếm Cây Tác Tử Ngôn Ngữ Thống Nhất Lý Luận, Hành Động và Lập Kế Hoạch trong
Mô Hình Ngôn Ngữ
Andy Zhou1 2Kai Yan1Michal Shlapentokh-Rothman1Haohan Wang1Yu-Xiong Wang1

Tóm tắt
Trong khi các mô hình ngôn ngữ (LM) đã cho thấy tiềm năng trên một loạt các nhiệm vụ ra quyết định, việc phụ thuộc vào các quy trình hành động đơn giản hạn chế việc triển khai rộng rãi của chúng như các tác tử tự động. Trong bài báo này, chúng tôi giới thiệu Tìm Kiếm Cây Tác Tử Ngôn Ngữ (LATS) – khung công tác chung đầu tiên kết hợp các khả năng của LM trong lý luận, hành động và lập kế hoạch. Bằng cách tận dụng khả năng học trong ngữ cảnh của LM, chúng tôi tích hợp Tìm Kiếm Cây Monte Carlo vào LATS để cho phép LM hoạt động như tác tử, cùng với các hàm giá trị được hỗ trợ bởi LM và tự phản chiếu để khám phá thành thạo và nâng cao việc ra quyết định. Một đặc điểm chính của phương pháp chúng tôi là việc kết hợp môi trường cho phản hồi bên ngoài, điều này cung cấp cơ chế giải quyết vấn đề có chủ ý và thích ứng hơn vượt qua các hạn chế của các kỹ thuật hiện có. Đánh giá thực nghiệm của chúng tôi trên các lĩnh vực đa dạng, bao gồm lập trình, hỏi-đáp tương tác (QA), điều hướng web và toán học, xác nhận hiệu quả và tính tổng quát của LATS trong việc ra quyết định trong khi duy trì hiệu suất lý luận cạnh tranh hoặc được cải thiện. Đáng chú ý, LATS đạt được độ chính xác pass@1 tiên tiến (92.7%) cho lập trình trên HumanEval với GPT-4 và thể hiện hiệu suất không gradient (điểm trung bình 75.9) có thể so sánh với tinh chỉnh dựa trên gradient cho điều hướng web trên WebShop với GPT-3.5. Mã có thể được tìm thấy tại https://github.com/lapisrocks/LanguageAgentTreeSearch.

1. Giới thiệu
Các tác tử tự động tổng quát có khả năng lý luận và ra quyết định trong nhiều môi trường khác nhau (Wooldridge và Jennings, 1995) đã là mối quan tâm lâu dài trong lĩnh vực trí tuệ nhân tạo. Trong khi điều này truyền thống được nghiên cứu trong học tăng cường, sự nổi lên gần đây của các mô hình ngôn ngữ (LM) (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; OpenAI, 2023) với khả năng lý luận mạnh mẽ và tính thích ứng tổng quát cung cấp một mô hình thay thế. Không chỉ LM xuất sắc trong các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) tiêu chuẩn như tóm tắt (Nallapati et al., 2016) và suy luận ngôn ngữ (Bowman et al., 2015), mà chúng cũng đã được thích ứng cho một tập hợp ngày càng đa dạng các nhiệm vụ thường đòi hỏi lý luận thông thường tiên tiến hoặc kỹ năng định lượng (Cobbe et al., 2021; Saparov và He, 2023). Ngoài ra, LM có khả năng thực hiện trong các môi trường phức tạp liên quan đến kiến thức và lý luận, như điều hướng web (Yao et al., 2022; Deng et al., 2023), sử dụng công cụ (Schick et al., 2023) và trò chơi mở (Fan et al., 2022).

Khả năng lý luận và hành động đã được cải thiện thêm bởi các kỹ thuật khuyến khích tăng cường LM với phản hồi hoặc quan sát từ môi trường bên ngoài, như được minh họa bởi ReAct (Yao et al., 2023b) và các công trình khác (Gao et al., 2023; Shinn et al., 2023). Điều này loại bỏ nhu cầu phụ thuộc hoàn toàn vào khả năng cơ bản của LM, nâng cao chúng thông qua các công cụ bên ngoài hoặc phản hồi ngữ nghĩa. Mặc dù có những điểm mạnh như vậy, các phương pháp này mang tính phản xạ và không đạt được đặc điểm ra quyết định có chủ ý và suy nghĩ của con người để giải quyết vấn đề (Sloman, 1996; Evans, 2010). Cụ thể, chúng không xem xét nhiều đường lý luận hoặc lập kế hoạch trước.

Công trình gần đây về LM được hướng dẫn bởi tìm kiếm (Xie et al., 2023; Yao et al., 2023a; Hao et al., 2023) giải quyết vấn đề này bằng cách tìm kiếm trên nhiều chuỗi lý luận. Trong khi cho phép lập kế hoạch, các phương pháp như vậy hoạt động một cách cô lập, thiếu việc kết hợp phản hồi bên ngoài có thể cải thiện lý luận.

Để vượt qua những thách thức này, chúng tôi đề xuất Tìm Kiếm Cây Tác Tử Ngôn Ngữ (LATS) – một khung thống nhất cho việc ra quyết định và lý luận với các mô hình ngôn ngữ. Như được minh họa trong Hình 1, LATS kết hợp các chiến lược lý luận, hành động và lập kế hoạch của LM bằng cách mở rộng ReAct (Yao et al., 2023b) thành một tìm kiếm trên không gian kết hợp của các bước lý luận và hành động có thể. Nỗ lực này không hề đơn giản – việc thích ứng các thuật toán tìm kiếm cho các tác tử ngôn ngữ và chuyển từ các nhiệm vụ không tương tác sang tương tác đòi hỏi thiết kế mới đáng kể về các nút, lời nhắc và thuật toán tìm kiếm. Cụ thể, các nút và lời nhắc phải lưu trữ và truy xuất phản hồi bên ngoài một cách hiệu quả, với thuật toán tìm kiếm kết hợp thông tin này vào các heuristics hữu ích để gán giá trị.

Thực tế, đánh giá thực nghiệm của chúng tôi, như được thể hiện trên HotPotQA (Yang et al., 2018) trong Phần 5.1, cho thấy rằng việc kết hợp đơn giản các phương pháp hiện có là không đủ, thậm chí không vượt qua hiệu suất lý luận nội bộ, mặc dù có quyền truy cập vào câu trả lời đúng từ môi trường.

Hiểu biết chính của chúng tôi làm nền tảng cho LATS là thích ứng Tìm Kiếm Cây Monte Carlo (MCTS), được truyền cảm hứng từ thành công của nó trong học tăng cường dựa trên mô hình (Silver et al., 2017) và quan sát rằng nhiều nhiệm vụ LM cho phép quay lại các bước trước đó, cho các tác tử ngôn ngữ, tái sử dụng LM đã được đào tạo trước như các tác tử với các hàm giá trị được hỗ trợ bởi LM và tự phản chiếu để khám phá thông minh hơn. Tận dụng các khả năng tổng quát và khả năng học trong ngữ cảnh của LM hiện đại, chúng tôi sử dụng ngôn ngữ như một giao diện giữa mỗi thành phần, cho phép LATS thích ứng lập kế hoạch với điều kiện môi trường mà không cần đào tạo thêm. Theo hiểu biết của chúng tôi, LATS là khung đầu tiên kết hợp lý luận, hành động và lập kế hoạch để nâng cao hiệu suất LM. Đáng chú ý, LATS tăng gấp đôi hiệu suất của ReAct (Yao et al., 2023b) trên HotPotQA (Yang et al., 2018) và tăng điểm trung bình 22.1 trên WebShop (Yao et al., 2022) với GPT-3.5. Khi được sử dụng với GPT-4, LATS đạt tỷ lệ Pass@1 92.7 trên HumanEval (Chen et al., 2021), thiết lập tiêu chuẩn hiện đại.

Đóng góp của chúng tôi như sau: 1) Chúng tôi giới thiệu LATS, một khung dựa trên Tìm Kiếm Cây Monte Carlo để xây dựng quỹ đạo tốt nhất từ các hành động được lấy mẫu, cho phép giải quyết vấn đề linh hoạt và thích ứng hơn so với các phương pháp nhắc nhở phản xạ. 2) Chúng tôi đề xuất một hàm giá trị mới hướng dẫn quá trình tìm kiếm và kết hợp các heuristics thành công như tự tinh chỉnh và tự nhất quán. 3) Bằng cách tích hợp phản hồi bên ngoài và tự phản chiếu, LATS nâng cao tính hợp lý của mô hình và cho phép các tác tử học từ kinh nghiệm, vượt qua các phương pháp tìm kiếm dựa trên lý luận. Thông qua các thí nghiệm trên các lĩnh vực đa dạng, bao gồm lập trình, hỏi-đáp tương tác (QA), điều hướng web và toán học, chúng tôi chứng minh tính linh hoạt của LATS để nâng cao lý luận và ra quyết định tự động.

--- TRANG 2 ---
Tìm Kiếm Cây Tác Tử Ngôn Ngữ Thống Nhất Lý Luận, Hành Động và Lập Kế Hoạch trong Mô Hình Ngôn Ngữ

2. Công trình liên quan
LM cho lý luận. Đối với LM, lý luận bao gồm việc phân tách các đầu vào phức tạp thành các bước trung gian tuần tự hướng tới câu trả lời cuối cùng (Cobbe et al., 2021), được thể hiện với việc nhắc nhở chuỗi suy nghĩ (CoT) (Wei et al., 2022) và các biến thể của nó (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022). Tuy nhiên, các phương pháp này, tạo ra các chuỗi tự hồi quy trong một bước duy nhất, thường gặp phải vấn đề lan truyền lỗi khi số bước tăng lên (Guo et al., 2018; Chen et al., 2023b), do các lỗi phức hợp. Các tiến bộ khác nhau nhằm giảm thiểu vấn đề này; một số phương pháp, như tự nhất quán (Wang et al., 2022), sử dụng bỏ phiếu đa số trên các chuỗi được lấy mẫu, trong khi những phương pháp khác tập trung vào phân tách nhiều bước, như nhắc nhở từ ít nhất đến nhiều nhất (Zhou et al., 2022). Gần đây, CoT đã được cải thiện với các thuật toán tìm kiếm (Yao et al., 2023a; Hao et al., 2023; Besta et al., 2023) có thể lấy mẫu quỹ đạo hiệu quả hơn. Nhắc nhở cây suy nghĩ (ToT) (Yao et al., 2023a) sử dụng tìm kiếm dựa trên DFS hoặc BFS (độ sâu/độ rộng đầu tiên) được hướng dẫn bởi heuristic được tạo bởi LM, trong khi lý luận qua lập kế hoạch (RAP) (Hao et al., 2023) sử dụng MCTS với các rollouts được mô phỏng bởi LM. Tuy nhiên, chúng chỉ dựa vào kiến thức nội bộ của LM và không thể thích ứng với phản hồi bên ngoài hữu ích.

LM cho hành động. Khả năng lý luận mạnh mẽ và thường thức của LM đã được thích ứng thêm cho các nhiệm vụ ra quyết định hoặc hành động như một mô hình chính sách trong các môi trường tương tác. Trong robot học, LM đã được sử dụng như các bộ điều khiển cấp cao của các chính sách điều khiển (Ahn et al., 2022; Huang et al., 2022; Driess et al., 2023). Công trình tương tự (Baker et al., 2022; Wang et al., 2023) cũng đã thích ứng các tác tử LM cho các trò chơi đa phương thức phức tạp như Minecraft (Guss et al., 2019; Fan et al., 2022). LM đặc biệt hữu ích trong các môi trường dựa trên văn bản (Liu et al., 2018; Shridhar et al., 2020; Liu et al., 2024), nơi các kỹ thuật nhắc nhở dựa trên hành động như ReAct (Yao et al., 2023b) đã thành công. Tương tự như CoT, ReAct bị hạn chế bởi tính đơn giản của nó và không thể thích ứng hiệu quả với điều kiện môi trường. Nhiều phần mở rộng đã được đề xuất để giải quyết vấn đề này, bao gồm tự tinh chỉnh (Madaan et al., 2023) và Reflexion (Shinn et al., 2023), sử dụng tự cải thiện để nâng cao lý luận và ra quyết định, và AdaPlanner (Sun et al., 2023), kết hợp cả phản hồi tích cực và tiêu cực. Tuy nhiên, các phương pháp này tập trung vào việc tinh chỉnh một quỹ đạo riêng lẻ và không xem xét các lựa chọn thay thế tại mỗi bước. Ngoài ra, công trình gần đây (Huang et al., 2024) đã gợi ý rằng LM không thể tự sửa lỗi lý luận nội bộ của chúng, làm cho việc sử dụng phản hồi bên ngoài trở nên quan trọng. Thay vì các môi trường ra quyết định thuần túy, khả năng lý luận và thực tiễn của LM đã được nâng cao bằng cách cung cấp quyền truy cập vào các công cụ bên ngoài, như API, công cụ tìm kiếm, máy tính và các mô hình khác (Schick et al., 2023; Shen et al., 2023; Surís et al., 2023). Chúng tôi tóm tắt công trình trước đây trong Bảng 1.

Tìm kiếm dựa trên cây. Tìm kiếm dựa trên cây, nơi nhiều nhánh kết quả được khám phá trong quá trình tìm kiếm, được sử dụng rộng rãi trong nhiều thuật toán lập kế hoạch (Swiechowski et al., 2021; LaValle, 1998) và học tăng cường (RL) (Hafner et al., 2019; Du et al., 2023; Wu et al., 2023) cho sự cân bằng tốt giữa khám phá và khai thác. Lưu ý rằng mặc dù tìm kiếm dựa trên cây cần một mô hình môi trường có thể mở rộng từ một trạng thái tùy ý (Vodopivec et al., 2017), thường đòi hỏi đào tạo thêm trong RL (Hafner et al., 2023), vấn đề như vậy không tồn tại đối với hầu hết các nhiệm vụ LM. Điều này là do chúng ta có thể dễ dàng quay lại bất kỳ trạng thái nào bằng cách đặt đầu vào là ngữ cảnh và đầu ra trước đó tương ứng từ LM cho nhiều nhiệm vụ. Do đó, chúng tôi hoạt động trên khung dựa trên cây và sử dụng MCTS (Swiechowski et al., 2021) để mở khóa hoàn toàn tiềm năng của LM. Ngoài ra, chúng tôi tránh chi phí đào tạo một hàm giá trị trên các mô tả ngôn ngữ bằng cách tận dụng khả năng học trong ngữ cảnh (Brown et al., 2020) của LM. Công trình đồng thời (Liu et al., 2023) cũng khám phá việc kết hợp các thuật toán tìm kiếm với các tác tử LM nhưng sử dụng một thuật toán tìm kiếm có sẵn, có thể không tối ưu cho LM. Cuối cùng, theo Yao et al. (2023a) và Hao et al. (2023), chúng tôi lưu ý rằng chúng tôi sử dụng lập kế hoạch và thuật toán tìm kiếm một cách thay thế cho nhau trong bài báo này.

3. Kiến thức nền tảng
3.1. Thiết lập vấn đề và Nhắc nhở
Trước tiên chúng tôi định nghĩa vấn đề của mình và phác thảo một số phương pháp đã được thiết lập tận dụng các mô hình ngôn ngữ cho lý luận hoặc ra quyết định. Trong lý luận hoặc ra quyết định của LM, chúng ta được cho một đầu vào x bằng ngôn ngữ tự nhiên và một mô hình ngôn ngữ đã được đào tạo trước pθ(x) được tham số hóa bởi θ; mục tiêu của chúng ta là tạo ra một đầu ra cuối cùng y∼pθ(x) tương ứng với câu trả lời (lý luận) hoặc hoàn thành nhiệm vụ (ra quyết định). Cả x và y đều là các chuỗi ngôn ngữ, được cấu thành từ một danh sách các token (các yếu tố cơ bản của ngôn ngữ tự nhiên, thường là từ), được ký hiệu là x = (x[1], . . . , x[lx]) và y = (y[1], . . . , y[ly]) trong đó lx và ly là độ dài.

LM giải mã văn bản tự hồi quy, tức là, không có đầu vào khác, xác suất để LM tạo ra một chuỗi y được cho bởi pθ(x) = ∏lx i=1 pθ(x[i]|x[1...i−1]). Thông thường, để cải thiện lý luận, các lời nhắc được cung cấp cùng với đầu vào x, đó là các hướng dẫn cụ thể hoặc ví dụ đầu vào-đầu ra few-shot. Chúng tôi ký hiệu quá trình chung trong đó một lời nhắc đầu vào IO(x) được chuyển đổi thành đầu ra y bởi LM: y∼pθ(prompt IO(x)).

Nhắc nhở chuỗi suy nghĩ (CoT) (Wei et al., 2022) phục vụ cho các tình huống mà việc ánh xạ trực tiếp từ x đến y phức tạp, ví dụ, khi x từ một truy vấn toán học hoặc câu hỏi thách thức. Nó dựa vào việc tạo các suy nghĩ z1, . . . , zl hoạt động như các bước đệm giữa x và y; mỗi suy nghĩ zi là một chuỗi ngôn ngữ. Để sử dụng nhắc nhở CoT, các suy nghĩ được trích xuất tuần tự như zi∼pCoT θ(x, z1···i−1), với đầu ra cuối cùng là y∼pCoT θ(x, z1···l).

Nhắc nhở cây suy nghĩ (ToT) (Yao et al., 2023a) mở rộng nhắc nhở CoT bằng cách khám phá nhiều đường lý luận trên các suy nghĩ. Nó định khung các vấn đề như một tìm kiếm trên một cây, trong đó mỗi nút s = [x, z1·i] đại diện cho một trạng thái giải pháp một phần bao gồm đầu vào ban đầu x và chuỗi suy nghĩ z1···i. Các suy nghĩ zi được tạo ra bằng đề xuất hoặc lấy mẫu với CoT zi∼pCoT θ(x, z1···i−1). Các thuật toán tìm kiếm như tìm kiếm theo độ sâu (DFS) hoặc tìm kiếm theo độ rộng (BFS) được sử dụng để khám phá cây một cách có hệ thống, được hướng dẫn bởi heuristics dựa trên đánh giá LM V(s) của mỗi trạng thái.

ReAct (Yao et al., 2023b) mở rộng các mô hình ngôn ngữ cho các nhiệm vụ mà việc ánh xạ từ x đến y được nâng cao bởi hoặc đòi hỏi tương tác với môi trường bên ngoài, như một trò chơi hoặc API. Kỹ thuật này xây dựng một không gian hành động Â = A ∪ Z thêm các hành động được phép a ∈ A vào các dấu vết lý luận z ∈ Z từ CoT. Các quan sát o từ môi trường được sử dụng để cải thiện cả lý luận và hành động. Để giải quyết vấn đề với ReAct, sau mỗi quan sát, các hành động được tạo ra từ pθ tuần tự như ai∼pReAct θ(x, o1···i−1, a1···i−1), với đầu ra cuối cùng là y∼pReAct θ(x, o1···l, a1···l). Trong bài báo này, phù hợp với các phương pháp tác tử LM khác như ReAct và Reflexion (Shinn et al., 2023), chúng tôi tập trung vào các nhiệm vụ ra quyết định mà việc quay lại giữa các lần lặp là khả thi.

Trong khi các kỹ thuật nhắc nhở được mô tả trước đây cải thiện hiệu suất LM trên các nhiệm vụ lý luận, chúng gặp khó khăn trên các nhiệm vụ khó khăn liên quan đến ra quyết định đa diện do một số hạn chế: 1) Tính linh hoạt: Các thiết kế nhắc nhở cơ bản (CoT hoặc ReAct) lấy mẫu tự hồi quy từ LM, bỏ qua các tiếp tục thay thế tiềm năng từ các trạng thái cụ thể. 2) Tính hợp lý: Các phương pháp dựa trên lý luận (CoT, RAP (Hao et al., 2023), hoặc ToT) chỉ dựa vào các biểu diễn nội bộ của LM và không thể xem xét các quan sát bên ngoài. Sự phụ thuộc này có nguy cơ ảo giác thực tế và lan truyền lỗi trong khi đặt ra một trần hiệu suất. 3) Tính thích ứng: Các chiến lược lập kế hoạch hiện tại (RAP hoặc ToT) sử dụng các thuật toán tìm kiếm đơn giản như BFS hoặc không thể tận dụng phản hồi môi trường để cải thiện lập kế hoạch. Ngoài ra, tác tử là tĩnh và không thể tái sử dụng kinh nghiệm trước đó hoặc học từ thử và sai. Trong khi RAP cũng áp dụng MCTS, nó bị hạn chế trong các nhiệm vụ mà LM có thể trở thành mô hình thế giới và dự đoán trạng thái chính xác. Những hạn chế này giới hạn khả năng của LM được triển khai như các tác tử giải quyết vấn đề tổng quát và tạo thành động lực cho LATS.

3.2. Tìm Kiếm Cây Monte Carlo (MCTS)
Tìm Kiếm Cây Monte Carlo (MCTS) là một thuật toán tìm kiếm heuristic được chứng minh thành công trên nhiều môi trường ra quyết định, như Atari (Ye et al., 2021) và Go (Silver et al., 2016). MCTS xây dựng một cây quyết định trong đó mỗi nút trong cây là một trạng thái và cạnh là một hành động. MCTS chạy trong k tập; cho mỗi tập, nó bắt đầu từ gốc (tức là, trạng thái ban đầu) và thực hiện lặp đi lặp lại hai bước để mở rộng cây: 1) Mở rộng, trong đó nhiều trạng thái con s được khám phá từ trạng thái cha hiện tại p bằng cách lấy mẫu n hành động, và 2) Lựa chọn, trong đó con có giá trị UCT (Upper Confidence bounds applied to Trees) (Kocsis và Szepesvári, 2006) cao nhất được chọn để mở rộng bởi lần lặp tiếp theo. UCT của một trạng thái con s được tính như sau:

UCT(s) = V(s) + ws√(ln N(p)/N(s)), (1)

trong đó N(s) là số lần thăm một nút s, V(s) là hàm giá trị (kỳ vọng trả về) từ cây con của s, w là trọng số khám phá, và p là nút cha của s. Khi kết thúc một tập được đạt tới, một lan truyền ngược được thực hiện: lợi nhuận r được sử dụng để cập nhật mỗi V(s) dọc theo đường đi với công thức V(s) = (Vold(s)(N(s)−1)+r)/N(s), trong đó Vold(s) là hàm giá trị cũ. Thông thường, hạn chế chính của MCTS là nó đòi hỏi một mô hình môi trường để hoàn tác các bước trước đó và tạo thành một cây tìm kiếm, có thể là một giả định mạnh. Tuy nhiên, hạn chế này không tồn tại đối với nhiều nhiệm vụ LM, vì chúng ta có thể dễ dàng đặt lại về bất kỳ bước nào bằng cách đơn giản copy-paste đầu vào văn bản lịch sử. Tính chất đặc biệt như vậy là động lực chính của công trình chúng tôi.

4. Thống nhất Lý luận, Hành động và Lập kế hoạch
4.1. Tác tử LM
Tùy thuộc vào thiết kế khung nhắc nhở cơ bản, LATS hỗ trợ các nhiệm vụ lý luận tuần tự hoặc ra quyết định. Tại bước thời gian t, một tác tử nhận được một quan sát ot ∈ O từ môi trường và thực hiện một hành động at ∈ A theo một chính sách nào đó π(at|x, o1···t−1, a1···t−1). Chúng tôi khởi tạo tác tử với pθ để tận dụng các biểu diễn ngôn ngữ hữu ích của LM như một người ra quyết định cơ bản. Chúng tôi theo việc thực thể hóa ReAct, trong đó không gian hành động Â = A ∪ Z bao gồm cả không gian của các hành động được phép A và không gian ngôn ngữ của các dấu vết lý luận Z. Các hành động trực tiếp ảnh hưởng đến môi trường và dẫn đến quan sát, trong khi các suy nghĩ được sử dụng để chính thức hóa các quyết định bằng cách tổ chức thông tin, lập kế hoạch các hành động tương lai, hoặc tiêm kiến thức nội bộ. Việc thực thể hóa chính xác của không gian hành động phụ thuộc vào môi trường cụ thể – đối với các nhiệm vụ ra quyết định, các hành động có thể bao gồm các lệnh trên một trang web, trong khi đối với các nhiệm vụ lý luận, không gian hành động có thể bị giới hạn trong một vài công cụ bên ngoài hoặc API. Trong các môi trường không có phản hồi, như các nhiệm vụ lý luận, chúng tôi sử dụng CoT như khung nhắc nhở cơ bản.

Thay vì giải mã tham lam một quỹ đạo hoặc giải pháp, chúng tôi lấy mẫu n hành động từ pθ sử dụng trạng thái hiện tại. Điều này dựa trên trực giác rằng đối với các nhiệm vụ ra quyết định phức tạp, có khả năng có một loạt các quỹ đạo hoặc đường lý luận tiềm năng đúng (Evans, 2010). Lấy mẫu một tập hợp đa dạng các ứng viên tại mỗi bước giảm thiểu tính chất ngẫu nhiên của việc tạo văn bản LM và cho phép khám phá lớn hơn trong cả không gian ra quyết định và lý luận. Chúng tôi bao bọc pθ trong thuật toán tìm kiếm được đề xuất của chúng tôi để có chủ ý xây dựng quỹ đạo tốt nhất từ các hành động được lấy mẫu.

4.2. LATS
Thành phần chính của LATS là một thuật toán tìm kiếm điều khiển quá trình giải quyết vấn đề với lập kế hoạch. Để tìm quỹ đạo hứa hẹn nhất và cân bằng có hệ thống giữa khám phá và khai thác, chúng tôi áp dụng một biến thể của MCTS định khung ra quyết định như một tìm kiếm cây, trong đó mỗi nút s = [x, a1···i, o1···i] đại diện cho một trạng thái bao gồm đầu vào ban đầu x, chuỗi hành động a1·i, và chuỗi quan sát o1·i, trong đó i là một token trong chuỗi văn bản.

Đóng góp kỹ thuật chính của chúng tôi là thích ứng MCTS cho các tác tử ngôn ngữ. LATS tái sử dụng pθ như một tác tử, người đánh giá trạng thái và người tạo phản hồi, tận dụng các biểu diễn ngôn ngữ hữu ích của LM hiện đại để tạo điều kiện cho lập kế hoạch. Trong khi MCTS tiêu chuẩn và RAP (Hao et al., 2023) dựa vào các mô hình động lực nội bộ để tạo điều kiện mô phỏng, LATS sử dụng tương tác môi trường và không đòi hỏi mô hình thế giới. Như được mô tả trong Hình 2, LATS bao gồm một loạt các hoạt động – lựa chọn, mở rộng, đánh giá, mô phỏng, lan truyền ngược và phản chiếu – được thực hiện liên tiếp cho đến khi nhiệm vụ được hoàn thành thành công hoặc đạt đến giới hạn tính toán sau khi lấy mẫu k quỹ đạo. Mã giả đầy đủ của LATS có thể được tìm thấy trong Phần A trong Phụ lục.

Lựa chọn. Trong hoạt động đầu tiên, thuật toán xác định một đoạn của cây hiện tại phù hợp nhất để mở rộng tiếp theo. Bắt đầu từ nút gốc, được ký hiệu là trạng thái ban đầu s0, một nút con được chọn tại mỗi cấp độ cây cho đến khi đạt đến một nút lá. Để cân bằng khám phá và khai thác, chúng tôi sử dụng thuật toán UCT như được hiển thị trong Phương trình 1.

Mở rộng. Sau khi chọn một nút, hoạt động thứ hai mở rộng cây bằng cách lấy mẫu n hành động từ pθ, như được mô tả trong phần trước. Môi trường nhận mỗi hành động và trả về phản hồi tương ứng như một quan sát. Điều này dẫn đến n nút con mới được thêm vào cây. Cây này được lưu trữ trong một cấu trúc bộ nhớ dài hạn bên ngoài.

Đánh giá. Hoạt động thứ ba gán một giá trị vô hướng cho mỗi nút con mới để lựa chọn và lan truyền ngược. Giá trị này hiệu quả định lượng tiến trình của tác tử trong việc hoàn thành nhiệm vụ, phục vụ như một heuristic để hướng dẫn thuật toán tìm kiếm hướng tới các vùng hứa hẹn nhất của cây. Vì LATS không liên quan đến đào tạo, chúng tôi đề xuất một hàm giá trị mới cho thiết lập này dựa trên hai thành phần: (1) điểm LM tự tạo và (2) điểm tự nhất quán.

Được truyền cảm hứng từ ToT, chúng tôi tái sử dụng pθ thành một hàm giá trị bằng cách nhắc nhở nó lý luận về một trạng thái cho trước. Để có được một giá trị vô hướng, chúng tôi hướng dẫn pθ kết thúc dấu vết lý luận của nó với một điểm chỉ ra tính đúng đắn của quỹ đạo. Sự khác biệt chính của chúng tôi so với ToT là chúng tôi có được giá trị này sau khi có được phản hồi từ môi trường, cải thiện việc gán giá trị. Điều này cũng cho phép mở rộng quy mô cho các môi trường thách thức hơn, vì LM khó cải thiện phản hồi của chúng mà không có phản hồi bên ngoài (Huang et al., 2024). Ngoài ra, để cải thiện thêm việc gán giá trị, chúng tôi giới thiệu một heuristic bổ sung dựa trên tự nhất quán (Wang et al., 2022), trong đó các hành động được lấy mẫu nhiều lần tại cùng một trạng thái có xu hướng chính xác hơn. Điều này dẫn đến hàm giá trị tổng thể:

V(s) = λ*LM(s) + (1−λ)*SC(s), (2)

trong đó λ là một siêu tham số. Đáng chú ý, phương pháp của chúng tôi cung cấp tính linh hoạt nâng cao so với heuristics được lập trình (Campbell et al., 2002) và hiệu quả lớn hơn so với heuristics được học (Silver et al., 2017).

Mô phỏng. Hoạt động thứ tư mở rộng nút được chọn hiện tại cho đến khi đạt đến trạng thái cuối cùng. Tại mỗi cấp độ sâu, chúng tôi lấy mẫu và đánh giá các nút với cùng các hoạt động nhưng ưu tiên các nút có giá trị cao nhất. Đạt đến trạng thái cuối cùng cung cấp phản hồi khách quan về tính đúng đắn của một quỹ đạo. Nếu nhiệm vụ được hoàn thành thành công, thì LATS kết thúc tìm kiếm. Nếu giải pháp thành công một phần hoặc không thành công, thì chúng tôi thực hiện hai hoạt động bổ sung như được mô tả dưới đây. Sự thành công của một quỹ đạo được xác định bởi thiết kế của môi trường cụ thể, như hoàn tất mua hàng trong môi trường điều hướng web.

Lan truyền ngược. Hoạt động này cập nhật các giá trị của cây dựa trên kết quả của một quỹ đạo. Đối với mỗi nút s0, s1, . . . , sl trong quỹ đạo từ gốc (trạng thái ban đầu s0) của cây tìm kiếm đến lá (trạng thái cuối cùng sl), giá trị của nó được cập nhật để phản ánh kết quả của mô phỏng bằng N(si) = N(si−1)+1 và V(si) = (V(si−1)N(si−1)+r)/N(si), trong đó r là phần thưởng. Những giá trị được cập nhật này được sử dụng trong công thức UCT (Phương trình 1) để hướng dẫn việc lựa chọn nút tiếp theo.

Phản chiếu. Ngoài phản hồi môi trường, chúng tôi tận dụng tự phản chiếu để tinh chỉnh thêm quá trình ra quyết định (Shinn et al., 2023; Madaan et al., 2023). Khi gặp phải một nút cuối cùng không thành công, pθ được nhắc nhở với quỹ đạo và phần thưởng cuối cùng để cung cấp một tự phản chiếu bằng lời mô tả các lỗi trong quá trình lý luận hoặc hành động và đề xuất các lựa chọn thay thế tốt hơn. Chúng tôi lưu trữ cả quỹ đạo thất bại và các phản chiếu tương ứng trong bộ nhớ. Trong các lần lặp tiếp theo, những điều này được tích hợp như ngữ cảnh bổ sung cho tác tử và hàm giá trị, tinh chỉnh cả hai thông qua học trong ngữ cảnh. Điều này cung cấp một tín hiệu gradient ngữ nghĩa hữu ích hơn một giá trị vô hướng, cho phép tác tử học từ thử và sai mà không có chi phí tối ưu hóa đắt đỏ như học tăng cường.

Thảo luận. Về mặt khái niệm, LATS có một số lợi thế đáng chú ý như một khung tổng quát cho lý luận và ra quyết định với các tác tử LM. (1) Tính tổng quát: LATS hỗ trợ cả các nhiệm vụ lý luận và ra quyết định bằng cách định nghĩa một không gian chung của suy nghĩ và hành động. (2) Tính thận trọng: Tận dụng MCTS và hàm giá trị LM trong LATS đảm bảo một tìm kiếm có nguyên tắc chọn các lựa chọn có giá trị cao trong khi khám phá các lựa chọn thay thế hứa hẹn. (3) Tính thích ứng: Kết hợp phản hồi bên ngoài thông qua quan sát và tự phản chiếu trong LATS cho phép thích ứng lớn hơn trong quá trình giải quyết vấn đề. (4) Tính linh hoạt: LATS có thể phù hợp với các tình huống, môi trường và quy định tài nguyên khác nhau bằng cách sửa đổi thiết kế trạng thái và kích thước cây. (5) Tính mô-đun: Tác tử LM cơ bản, người tạo phản chiếu và hàm giá trị có thể được thay đổi và thích ứng độc lập với các thuộc tính LM riêng lẻ.

5. Thí nghiệm
Để chứng minh khả năng áp dụng tổng quát của LATS, chúng tôi đánh giá phương pháp của mình trên nhiều lĩnh vực đòi hỏi lý luận và hành động: lập trình (Chen et al., 2021; Austin et al., 2022), HotPotQA (Yang et al., 2018), WebShop (Yao et al., 2022), và Game of 24 (Yao et al., 2023a).

5.1. HotPotQA
Đối với một nhiệm vụ có thể được tiếp cận với cả chiến lược dựa trên lý luận và dựa trên hành động, chúng tôi xem xét HotPotQA (Yang et al., 2018), một benchmark hỏi-đáp đa bước đòi hỏi truy xuất trên hai hoặc nhiều đoạn Wikipedia. Đối với không gian hành động, ngoài các suy nghĩ LM, chúng tôi tuân theo thiết lập từ Yao et al. (2023b), cung cấp cho tác tử các cuộc gọi API để tìm kiếm và truy xuất thông tin. Đầu ra của những cuộc gọi API này và các phản chiếu tự tạo tạo thành không gian quan sát. Lưu ý rằng phù hợp với công trình trước đây (Yao et al., 2023b; Shinn et al., 2023), chúng tôi sử dụng thiết lập oracle cho HotPotQA, trong đó môi trường cung cấp phản hồi về tính đúng đắn của câu trả lời khi nhận được một câu trả lời. Điều này cho phép so sánh công bằng giữa phương pháp của chúng tôi và các baseline trong các tình huống mà chất lượng phản hồi cao, cho phép chúng tôi tập trung đánh giá vào việc tác tử kết hợp phản hồi bên ngoài tốt như thế nào. Chúng tôi sử dụng một tập con của 100 câu hỏi và ba ví dụ few-shot cho mỗi phương pháp. Đối với ToT, chúng tôi sử dụng DFS như thuật toán tìm kiếm cơ bản. Đối với tất cả các phương pháp liên quan đến lấy mẫu, bao gồm LATS, chúng tôi lấy mẫu k = 50 quỹ đạo. Chi tiết thêm trong Phần D của Phụ lục.

Chúng tôi đánh giá các chiến lược lý luận nội bộ bằng cách loại bỏ các hành động và quan sát khỏi ngữ cảnh, tương ứng với CoT (Wei et al., 2022) và các biến thể của nó, CoT-SC (Wang et al., 2022), ToT (Yao et al., 2023a), và RAP (Hao et al., 2023). Những phương pháp này chỉ dựa vào kiến thức hiện có của tác tử để trả lời câu hỏi. Chúng tôi xem xét thêm các phương pháp dựa trên hành động ReAct, Reflexion và LATS, nâng cao tác tử với môi trường API tương tác và chủ yếu đánh giá khả năng truy xuất thông tin của nó. Chúng tôi cũng thiết kế một tích hợp đơn giản của các thuật toán tìm kiếm với các tác tử LM, mở rộng ToT và RAP với nhắc nhở ReAct để xử lý các quan sát bên ngoài. Ngoài ra, trong khi LATS được thiết kế cho các tình huống mà phản hồi bên ngoài có thể nâng cao lý luận, chúng tôi cũng triển khai một phiên bản chỉ lý luận với CoT như khung nhắc nhở cơ bản. Hơn nữa, chúng tôi kết hợp lý luận nội bộ và bên ngoài trong LATS bằng cách đầu tiên nhắc nhở với lời nhắc dựa trên CoT và sau đó chuyển sang lời nhắc dựa trên ReAct khi thất bại. Điều này gần gũi hơn với cách con người có thể tiếp cận nhiệm vụ này bằng cách sử dụng công cụ để truy xuất thông tin bổ sung chỉ khi câu trả lời chưa được biết.

Kết quả. Chúng tôi quan sát trong Bảng 2 và Bảng 3 rằng cả chiến lược lý luận nội bộ và truy xuất bên ngoài đều hoạt động tốt trên HotPotQA. Do bộ dữ liệu đào tạo quy mô lớn của chúng, LM hiện đại đã mã hóa kiến thức thực tế và thường có thể trả lời câu hỏi trực tiếp một cách chính xác. Trong khi CoT có thể cải thiện hiệu suất một chút trên các câu hỏi đòi hỏi lý luận, các lợi ích lớn hơn được quan sát với các phương pháp tìm kiếm ToT và RAP (Bảng 2, Hàng 4, 5), có thể lấy mẫu và khám phá nhiều đầu ra hơn. Chúng tôi quan sát kết quả tương tự cho các phương pháp dựa trên hành động. LATS vượt qua ReAct, ngay cả khi lấy mẫu cùng số lượng quỹ đạo, bằng cách mở rộng nhiều nút hơn với tìm kiếm có nguyên tắc. Điều này được chứng minh khi sửa đổi n, số lượng nút được mở rộng trong mỗi lần lặp. Tăng n có thể cải thiện hiệu suất một cách nhất quán, mặc dù với chi phí tính toán và suy luận lớn hơn. LATS cũng vượt trội RAP trong lý luận nội bộ, nhưng có hiệu suất cao hơn trong thiết lập ra quyết định của HotPotQA so với thiết lập lý luận. Trái với LATS, các phiên bản ReAct của ToT và RAP (Bảng 3, Hàng 4, 5) thậm chí còn hoạt động tệ hơn thiết lập chỉ lý luận của HotPotQA, điều này chỉ ra rằng thiết lập dựa trên hành động thách thức hơn và việc thích ứng thuật toán tìm kiếm cho các tình huống ra quyết định không hề đơn giản. Kết hợp lý luận nội bộ và bên ngoài trong LATS dẫn đến hiệu suất cao nhất, chỉ ra tầm quan trọng của phản hồi bên ngoài trong việc tăng cường lý luận ngay cả trong các nhiệm vụ mà LM cơ bản đã có thể thực hiện.

5.2. Lập trình
Để chứng minh tầm quan trọng của các quan sát bên ngoài cho các nhiệm vụ lý luận phức tạp, chúng tôi đánh giá các baseline và LATS trên lập trình với HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2022). Cả hai bộ dữ liệu đo tính đúng đắn của các chương trình được tổng hợp trong Python từ các mô tả ngôn ngữ tự nhiên. Chúng tôi sử dụng các giải pháp riêng lẻ như không gian hành động và bộ thử nghiệm và phản hồi trình biên dịch như quan sát bên ngoài. Chúng tôi tuân theo Chen et al. (2023a) và sử dụng LM để tạo ra một bộ thử nghiệm tổng hợp của các câu lệnh "assert" hợp lệ về mặt cú pháp cho mỗi câu hỏi. Đối với mỗi bước, giải pháp được đánh giá trên bộ thử nghiệm này, và kết quả, bao gồm các thử nghiệm thành công và thất bại cũng như đầu ra trình biên dịch, được thêm vào ngữ cảnh như một quan sát.

Đối với nhiệm vụ này, các baseline lý luận và hành động chia sẻ một không gian hành động, nhưng các phương pháp hành động có thể kết hợp quan sát như ngữ cảnh bổ sung. Đối với LATS, vì mỗi hành động tương ứng với một giải pháp hoàn chỉnh, chúng tôi bỏ qua bước mô phỏng của LATS và trực tiếp sử dụng tỷ lệ phần trăm của các thử nghiệm đã qua như phần thưởng được lan truyền ngược. Chúng tôi sử dụng k = 8 lần lặp, đặt số lượng thử nghiệm được tạo ra là 4, và lấy mẫu n = 5 giải pháp trong quá trình mở rộng. Sau khi tìm kiếm hoàn tất, chúng tôi chọn giải pháp có giá trị cao nhất và đánh giá nó trên bộ thử nghiệm thực cho đánh giá độ chính xác pass@1. Chi tiết thêm có thể được tìm thấy trong Phần D của Phụ lục.

Kết quả. Bảng 4 và Bảng 5 cho thấy rằng cả tìm kiếm và phản hồi ngữ nghĩa đều quan trọng cho hiệu suất tốt hơn. Mặc dù không sử dụng quan sát, ToT và RAP có thể cạnh tranh với Reflexion. LATS có hiệu suất cao nhất trên cả hai bộ dữ liệu. RAP sử dụng thuật toán tìm kiếm tương tự như LATS, điều này cho thấy tầm quan trọng của phản hồi bên ngoài cho các nhiệm vụ lý luận khó khăn như lập trình. Với GPT-4, việc sử dụng LATS thiết lập tiêu chuẩn hiện đại cho HumanEval, xác nhận rằng LATS có thể được sử dụng với LM tiên tiến hơn để có hiệu suất cao hơn.

5.3. WebShop
Đối với một môi trường ra quyết định phức tạp với các ứng dụng thực tiễn, chúng tôi xem xét WebShop (Yao et al., 2022), một môi trường mua sắm trực tuyến tương tác bao gồm một trang web với 1.18M sản phẩm thực tế và 12k hướng dẫn của con người. Các tác tử phải điều hướng một trang web thông qua nhiều lệnh khác nhau để mua một mặt hàng phù hợp với đặc tả của người dùng. Chúng tôi sử dụng không gian hành động được xây dựng trước của các lệnh tìm kiếm và nhấp chuột và phản hồi trình duyệt và phản chiếu cho quan sát. Hiệu suất được đo bằng hai chỉ số: điểm trung bình, phản ánh tỷ lệ phần trăm của các thuộc tính do người dùng chỉ định được đáp ứng bởi sản phẩm đã chọn, và tỷ lệ thành công, chỉ ra tần suất mà sản phẩm được chọn đáp ứng tất cả các điều kiện đã cho. Chúng tôi so sánh với các phương pháp nhắc nhở dựa trên hành động và các phương pháp dựa trên RL. Chúng tôi đánh giá trên 50 hướng dẫn, mở rộng n = 5 con cho LATS, và đặt k = 30 cho LATS, ReAct (best of k), và Reflexion. Chi tiết thêm và lời nhắc trong Phần D và Phần G của Phụ lục.

Kết quả. Chúng tôi thấy trong Bảng 6 rằng GPT-3.5 với ReAct có khả năng cạnh tranh với học bắt chước (IL) và có thể vượt qua các kỹ thuật học tăng cường với các chiến lược nhắc nhở mạnh hơn. Lấy mẫu k = 30 quỹ đạo với ReAct và Reflexion dẫn đến hiệu suất tương tự, gợi ý rằng phản hồi ngữ nghĩa không hữu ích như vậy trong các môi trường phức tạp như WebShop. Tương tự như Shinn et al. (2023), chúng tôi thấy rằng các phản chiếu được tạo ra thường chung chung và không cung cấp phản hồi hữu ích, dẫn đến xu hướng tác tử bị mắc kẹt trong các cực tiểu địa phương. Tuy nhiên, việc sử dụng LATS thực sự dẫn đến cải thiện đáng chú ý, chỉ ra một khám phá hiệu quả hơn cho cùng số lần lặp.

5.4. Nghiên cứu Loại bỏ và Phân tích Bổ sung
Chúng tôi tiếp tục kiểm tra khả năng lý luận của LATS trên Game of 24, và cũng tiến hành các thí nghiệm bổ sung trên HotPotQA để chứng minh hiệu ứng của mỗi thành phần của LATS (kết quả được hiển thị trong Bảng 8). Các loại bỏ thêm cho việc tiêu thụ token trên HotPotQA trong Bảng 9 của Phần C của Phụ lục.

Lý luận trên Game of 24. Để cho thấy cách LATS có thể được áp dụng cho các nhiệm vụ lý luận nội bộ thuần túy, chúng tôi đánh giá bổ sung trên Game of 24 (Yao et al., 2023a), một nhiệm vụ lý luận toán học trong đó tác tử phải xây dựng 24 từ một tập hợp các số và các phép toán cơ bản. Chúng tôi sử dụng CoT như thiết kế nhắc nhở cơ bản và sử dụng cùng các hoạt động như trong các thiết lập khác. Chúng tôi thấy trong Bảng 7 rằng LATS vượt trội so với các phương pháp trước đây được đề xuất cụ thể cho lý luận. Điều này là do hàm giá trị được đề xuất của chúng tôi, kết hợp tự nhất quán như một heuristic bổ sung.

Tự phản chiếu. LATS sử dụng tự phản chiếu để cung cấp các tín hiệu ngữ nghĩa bổ sung cho tác tử. Trong Bảng 8 (Hàng 5, 6), chúng tôi quan sát sự giảm hiệu suất 0.05 khi tự phản chiếu được loại bỏ khỏi LATS, xác nhận tính hữu ích của nó. Đây là một lợi ích nhỏ hơn so với lợi ích 0.19 mà Reflexion có so với ReAct như được hiển thị trong Bảng 3, gợi ý sự chồng chéo giữa các câu hỏi mà câu trả lời có thể được cải thiện bằng tự phản chiếu và tìm kiếm. Biến thể này vượt trội so với RAP (ReAct), phản ánh các cải tiến của chúng tôi đối với MCTS.

Thuật toán tìm kiếm. MCTS là một thuật toán tìm kiếm có nguyên tắc hơn so với các biến thể như A* (Zhuang et al., 2023) hoặc DFS và là cơ sở cho các lợi ích hiệu suất được quan sát. Chúng tôi quan sát các hiệu ứng của việc sử dụng DFS, và kết hợp heuristic dựa trên LM được sử dụng trong ToT trong đó các nhánh có giá trị thấp được cắt tỉa. Điều này loại bỏ các hoạt động lựa chọn và lan truyền ngược, và chúng tôi quan sát sự giảm hiệu suất 0.21 trong Bảng 8 (Hàng 4) khi lấy mẫu cùng số lượng nút nhưng vượt trội so với ToT (ReAct). Mặc dù cũng hưởng lợi từ phản hồi chân lý, LATS sử dụng nó tốt hơn ToT và RAP và có thể vượt trội so với các phương pháp này. Chúng tôi cũng thấy trong Bảng 8 (Hàng 3) rằng điểm LM, thành phần chính của hàm giá trị của chúng tôi, rất quan trọng để tận dụng phản hồi bên ngoài và hiệu suất mạnh.

Độ phức tạp mẫu và tiêu thụ token. Một mối quan tâm có thể của LATS là tìm kiếm có cấu trúc cây có thể tiêu thụ nhiều token hơn so với các phương pháp hiện có. Để nghiên cứu thêm chi phí tính toán của LATS so với các phương pháp trước đây, chúng tôi kiểm tra độ phức tạp mẫu (tức là, chi phí token tiệm cận) của tất cả các phương pháp được xem xét trong bài báo này và đếm số lượng nút trung bình được mở rộng bởi phương pháp của chúng tôi và các phương pháp có cấu trúc cây khác (ToT và RAP) khi tìm kiếm thành công trên HotPotQA. Chúng tôi trình bày kết quả trong Bảng 9 và Bảng 10, cho thấy phương pháp của chúng tôi có cùng độ phức tạp mẫu như các phương pháp tìm kiếm dựa trên cây khác và yêu cầu ít token tổng thể và trạng thái hơn. Khoảng cách chi phí token sẽ còn lớn hơn khi tính đến các quỹ đạo thất bại, vì phương pháp của chúng tôi có tỷ lệ thành công cao hơn và đạt đến giới hạn ngân sách tính toán ít thường xuyên hơn. Điều này cũng đúng khi lấy mẫu một số lượng quỹ đạo nhỏ hơn; trung bình, LATS yêu cầu ít hơn 3.55 nút so với RAP và 12.12 nút ít hơn so với ToT. Những phát hiện này nhấn mạnh các cải tiến của chúng tôi đối với MCTS và thích ứng với các tác tử LM, dẫn đến một cơ chế tìm kiếm có nguyên tắc và hiệu quả hơn.

6. Kết luận
Công trình này giới thiệu Tìm Kiếm Cây Tác Tử Ngôn Ngữ (LATS), khung đầu tiên thống nhất lý luận, hành động và lập kế hoạch để nâng cao khả năng giải quyết vấn đề của LM. LATS giải quyết các hạn chế chính của các kỹ thuật nhắc nhở trước đây bằng cách có chủ ý xây dựng quỹ đạo với các thuật toán tìm kiếm, kết hợp phản hồi bên ngoài và cho phép các tác tử học từ kinh nghiệm. Đánh giá của chúng tôi chứng minh khả năng của LATS khai thác các khả năng LM cho các nhiệm vụ ra quyết định khác nhau trong khi duy trì khả năng lý luận của nó mà không cần đào tạo bổ sung. Các sự kết hợp được đề xuất giữa tìm kiếm, tương tác và phản chiếu cung cấp một phương pháp đa năng cho việc ra quyết định tự động, làm nổi bật tiềm năng của LM như các tác tử tổng quát.

Hạn chế và hướng tương lai. LATS có hai hạn chế chính cần được xem xét trước khi áp dụng. Thứ nhất, nó có chi phí tính toán cao hơn so với các phương pháp nhắc nhở đơn giản hơn như ReAct hoặc Reflexion, có thể hạn chế tính thực tiễn của nó trong một số tình huống nhất định. Thứ hai, LATS giả định khả năng quay lại các trạng thái trước đó trong các môi trường ra quyết định, có thể không áp dụng được phổ biến trong tất cả các môi trường có thể. Mặc dù có những hạn chế này, đáng chú ý là LATS vẫn đạt được hiệu suất và hiệu quả tốt hơn so với các phương pháp tương tự, và số lượng nút được mở rộng tại mỗi bước cung cấp sự cân bằng giữa hiệu suất và hiệu quả. Ngoài ra, chúng tôi kỳ vọng chi phí tính toán thời gian suy luận sẽ giảm theo thời gian, từ đó tăng tính hữu ích của LATS và các phương pháp LM "System-2" khác. Cuối cùng, tính chất quay lại là khả thi trong nhiều ứng dụng thực tế, mở ra cơ hội mới trong cộng đồng ra quyết định LM. Các hướng tương lai bao gồm mở rộng quy mô LATS cho các môi trường phức tạp hơn hoặc khung đa tác tử và cải thiện hiệu quả để giảm chi phí. Một thảo luận chi tiết hơn về các hạn chế của LATS có thể được tìm thấy trong Phần B của Phụ lục.

--- TRANG 10 ---
Tìm Kiếm Cây Tác Tử Ngôn Ngữ Thống Nhất Lý Luận, Hành Động và Lập Kế Hoạch trong Mô Hình Ngôn Ngữ

Tuyên bố Tác động
LATS là một khung nâng cao hiệu suất LM thông qua tương tác với môi trường. Sự cải thiện này trong việc ra quyết định tự động có thể tạo điều kiện cho việc sử dụng có hại của LM. Mặt khác, LATS nâng cao khả năng diễn giải và tiềm năng cho sự liên kết lớn hơn, vì nó liên quan đến lý luận và hành động ngôn ngữ cấp cao thông qua nhiều vòng ra quyết định và phản chiếu thay vì dựa vào việc tạo tự hồi quy. Cuối cùng, việc nâng cao khả năng của các tác tử LM có thể gây ra rủi ro bảo mật, như thực thi phần mềm độc hại. Chúng tôi khuyến khích nghiên cứu thêm để hiểu đầy đủ và giảm thiểu các rủi ro của LM.

Lời cảm ơn
Chúng tôi cảm ơn Daniel Campos về phản hồi hữu ích cho các phiên bản trước của bài báo này. Công trình này được hỗ trợ một phần bởi NSF Grant 2106825, NIFA Award 2020-67021-32799, quỹ bầu Jump ARCHES thông qua Trung tâm Hệ thống Kỹ thuật Chăm sóc Sức khỏe tại Illinois và Quỹ OSF, và Viện Gia tốc Khám phá IBM-Illinois. Công trình này sử dụng GPU NVIDIA tại NCSA Delta thông qua các phân bổ CIS220014, CIS230012, và CIS230218 từ chương trình ACCESS.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài được dịch tiếp tục...]
