# 2210.01296.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2210.01296.pdf
# Kích thước file: 1021793 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
CÁC MÔ HÌNH NGÔN NGỮ TĂNG CƯỜNG BẰNG NGÂM THƠ
Zhiqing Sun1;2, Xuezhi Wang1, Yi Tay1, Yiming Yang2, Denny Zhou1
1Google Research, Brain Team
2Language Technologies Institute, Carnegie Mellon University
TÓM TẮT
Chúng tôi đề xuất một mô hình mới để giúp các Mô hình Ngôn ngữ Lớn (LLMs) tạo ra kiến thức thực tế chính xác hơn mà không cần truy xuất từ một kho dữ liệu bên ngoài, được gọi là tạo sinh tăng cường bằng NGÂM THƠ (RECITE). Khác với các mô hình ngôn ngữ tăng cường truy xuất mà truy xuất các tài liệu liên quan trước khi tạo ra đầu ra, với một đầu vào, RECITE đầu tiên ngâm thơ một hoặc vài đoạn văn liên quan từ bộ nhớ của chính LLMs thông qua lấy mẫu, và sau đó tạo ra các câu trả lời cuối cùng. Chúng tôi chỉ ra rằng RECITE là một mô hình mạnh mẽ cho các nhiệm vụ NLP chuyên sâu về kiến thức. Cụ thể, chúng tôi chỉ ra rằng bằng cách sử dụng ngâm thơ như bước trung gian, một sơ đồ ngâm-và-trả-lời có thể đạt được hiệu suất tiên tiến mới trong các nhiệm vụ trả lời câu hỏi sách đóng (CBQA) khác nhau. Trong các thí nghiệm, chúng tôi xác minh hiệu quả của RECITE trên bốn mô hình được huấn luyện trước (PaLM, UL2, OPT, và Codex) và ba nhiệm vụ CBQA (Natural Questions, TriviaQA, và HotpotQA). Mã nguồn của chúng tôi có sẵn tại https://github.com/Edward-Sun/RECITE .

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLMs) đã đạt được hiệu suất few-shot trong ngữ cảnh ấn tượng trên các nhiệm vụ NLP chuyên sâu về kiến thức (Brown và cộng sự, 2020; Rae và cộng sự, 2021; Hoffmann và cộng sự, 2022; Chowdhery và cộng sự, 2022). Ví dụ, trong trả lời câu hỏi miền mở (Chen và cộng sự, 2017), được minh họa chỉ bằng vài ví dụ về các cặp câu hỏi-câu trả lời, LLMs có thể trả lời các câu hỏi thực tế tùy ý (Joshi và cộng sự, 2017; Yang và cộng sự, 2018; Kwiatkowski và cộng sự, 2019). Nghiên cứu gần đây (Guu và cộng sự, 2020; Lewis và cộng sự, 2020; Izacard và cộng sự, 2022) cho thấy rằng tăng cường truy xuất có thể cải thiện thêm hiệu suất của LLMs trên các nhiệm vụ chuyên sâu về kiến thức bằng cách điều kiện hóa LLMs trên các đoạn văn liên quan được truy xuất từ một kho dữ liệu bên ngoài.

Câu hỏi: ai đã viết bài hát 
i hate you i love you 
Trả lời: Gnash 
…
Câu hỏi: ai đã viết 
school for good and evil Tạo sinh Trực tiếp 
(ví dụ, PaLM) 
Trả lời: Soman Chainani Đầu vào LM 
Đầu ra LM Câu hỏi: ai đã viết 
school for good and evil Tạo sinh Tăng cường Truy xuất 
(ví dụ, Atlas) 
Trả lời: Soman Chainani Đầu vào LM 
Đầu ra LM The School for Good and Evil 
là một bộ sáu cuốn sách thần thoại 
của Soman 
Chainani… Kết quả Truy xuất Câu hỏi: ai đã viết bài hát i hate you i 
love you 
Ngâm thơ: "I Hate U, I Love U" (được cách điệu 
bằng chữ thường) là một bài hát của ca sĩ và rapper 
người Mỹ Gnash có sự tham gia của ca sĩ người Mỹ 
Olivia O'Brien. 
Trả lời: Gnash 
…
Câu hỏi: ai đã viết school for good 
and evil Tạo sinh Tăng cường Ngâm thơ 
(của chúng tôi) 
Ngâm thơ: The School for Good and Evil 
được xuất bản lần đầu vào ngày 14 tháng 5 năm 2013 bởi 
Soman Chainani… 
Trả lời: Soman Chainani Đầu vào LM 
Đầu ra LM 

Hình 1: Minh họa việc đánh giá trả lời câu hỏi miền mở (few-shot) với tạo sinh trực tiếp (sách đóng) (Chowdhery và cộng sự, 2022), tạo sinh tăng cường truy xuất (sách mở) (Izacard và cộng sự, 2022), và tạo sinh tăng cường ngâm thơ (sách đóng) (của chúng tôi).

* Công việc được thực hiện trong thời gian thực tập tại Google.
1arXiv:2210.01296v2  [cs.CL]  16 Feb 2023

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bài báo này đề xuất một mô hình mới để giúp LLMs tạo ra kiến thức thực tế chính xác hơn mà không cần truy xuất từ một kho dữ liệu bên ngoài, được gọi là tạo sinh tăng cường bằng NGÂM THƠ (RECITE), trong đó chúng tôi giải quyết các nhiệm vụ NLP chuyên sâu về kiến thức bằng cách đầu tiên ngâm thơ thông tin liên quan và sau đó tạo ra các đầu ra. Mô hình hai bước như vậy phân tách nhiệm vụ chuyên sâu về kiến thức ban đầu thành hai nhiệm vụ phụ: ngâm thơ kiến thức và thực thi nhiệm vụ, trong đó cái trước có thể được coi như một dạng bước truy xuất kiến thức trung gian (từ các trọng số mô hình), trong khi cái sau là bước thực thi tạo ra các đầu ra cuối cùng.

Động lực để giới thiệu một bước ngâm thơ kiến thức bổ sung đến từ quan sát của chúng tôi rằng trong khi prompting few-shot có thể giúp LLMs thực thi các nhiệm vụ NLP cụ thể, những nhiệm vụ này thường không ở dạng tương tự như mục tiêu mô hình hóa ngôn ngữ nhân quả trong huấn luyện trước ban đầu. Điều này cản trở LLMs khỏi việc ngâm thơ kiến thức một cách hiệu quả từ bộ nhớ của chúng (Carlini và cộng sự, 2021). Hãy xem xét một học sinh làm bài kiểm tra sách đóng chứa các câu hỏi chuyên sâu về kiến thức, ví dụ, "chữ số thập phân thứ mười của π là gì?". Họ thường không thể trả lời trực tiếp câu hỏi này vì trong giai đoạn học tập (tương tự với giai đoạn huấn luyện trước mô hình hóa ngôn ngữ cho LLMs), rất khó có khả năng họ sẽ đọc "chữ số thập phân thứ mười của π là 5". Tuy nhiên, có thể có một số câu như "N chữ số đầu tiên của π là 3.14159 26535..." tồn tại trong sách giáo khoa mà học sinh có thể ngâm thơ. Do đó, một học sinh có thể trả lời được câu hỏi này trong sơ đồ ngâm-và-trả-lời: "10 chữ số đầu tiên của π là 3.14159 26535. Vậy câu trả lời là 5". Ở đây, bước ngâm thơ kiến thức có thể phục vụ như một bước trung gian mô phỏng nhiệm vụ huấn luyện trước mô hình hóa ngôn ngữ, và do đó giúp LLM tạo ra kiến thức thực tế tốt hơn.

Chúng tôi xác minh hiệu quả của tạo sinh tăng cường ngâm thơ trên các nhiệm vụ Trả lời Câu hỏi Sách Đóng (CBQA) few-shot (được gọi là ngâm-và-trả-lời trong ngữ cảnh CBQA), như được minh họa trong Hình 1. CBQA là một nhiệm vụ QA miền mở hấp dẫn ở chỗ một LM được tham số hóa hoàn toàn có thể tạo ra câu trả lời trực tiếp mà không cần một kho dữ liệu bên ngoài hoặc các mô hình truy xuất riêng biệt (Roberts và cộng sự, 2020). Chúng tôi chỉ ra rằng sơ đồ ngâm-và-trả-lời được đề xuất là một phương pháp hiệu quả cho CBQA và tương thích với các kỹ thuật khác để tăng hiệu suất few-shot của LLMs. Chúng tôi cũng chỉ ra rằng, ngoài việc cải thiện hiệu suất học trong ngữ cảnh few-shot của LLM được tăng cường RECITE, việc tinh chỉnh các LLMs được huấn luyện trước trên các cặp câu hỏi-đoạn văn được tạo tổng hợp có thể cải thiện thêm hiệu suất ngâm thơ và dẫn đến độ chính xác QA downstream tốt hơn.

Các thí nghiệm trên bốn mô hình ngôn ngữ lớn (PaLM (Chowdhery và cộng sự, 2022), UL2 (Tay và cộng sự, 2022a), OPT (Zhang và cộng sự, 2022)), và Codex (Chen và cộng sự, 2021) cho thấy rằng một sơ đồ ngâm-và-trả-lời có thể cải thiện hiệu suất trên các loại nhiệm vụ CBQA khác nhau, bao gồm QA một bước dựa trên Wikipedia (Natural Questions, Kwiatkowski và cộng sự 2019), câu hỏi trivia (TriviaQA, Joshi và cộng sự 2017), và QA nhiều bước dựa trên Wikipedia (HotpotQA, Yang và cộng sự 2018).

2 CÔNG TRÌNH LIÊN QUAN

2.1 TRẢ LỜI CÂU HỎI MIỀN MỞ

Trả lời câu hỏi miền mở (Prager và cộng sự, 2007) đề cập đến nhiệm vụ tạo ra câu trả lời cho các câu hỏi tự do ngữ cảnh tùy ý. Trong thiết lập sách mở, nó thường được giả định rằng mô hình QA có thể tìm câu trả lời trong một kho dữ liệu bên ngoài, ví dụ, Wikipedia (Chen và cộng sự, 2017; Izacard & Grave, 2021) hoặc các trang web (Lazaridou và cộng sự, 2022). Điều này tương tự như làm bài kiểm tra sách mở nơi học sinh có thể tìm kiếm trên một kho kiến thức bên ngoài. Đường ống tiêu chuẩn (Chen và cộng sự, 2017; Izacard & Grave, 2021; 2020) thường bao gồm một mô-đun truy xuất tài liệu có thể học hoặc không thể học và một mô-đun đọc dựa trên mạng thần kinh có thể học.

Trong thiết lập sách đóng, mô hình QA không được phép truy cập bất kỳ kiến thức bên ngoài nào, và cần lưu trữ tất cả kiến thức trong các tham số của nó. Gần đây người ta quan sát thấy rằng các mô hình ngôn ngữ được huấn luyện trước quy mô lớn (Devlin và cộng sự, 2019; Radford và cộng sự, a; Yang và cộng sự, 2019b) có thể nội hóa một loại "cơ sở dữ liệu kiến thức" ngầm sau khi huấn luyện trước (Petroni và cộng sự, 2019; Jiang và cộng sự, 2020; Talmor và cộng sự, 2020). Roberts và cộng sự (2020) chỉ ra rằng sau khi tinh chỉnh trên các cặp câu hỏi-câu trả lời sách mở, T5 (Raffel và cộng sự, 2020) có thể trả lời một phần lớn các câu hỏi chuyên sâu về kiến thức. Điều này tương tự như làm bài kiểm tra sách đóng. Tuy nhiên, Lewis và cộng sự (2021) phát hiện rằng hiệu suất cao chủ yếu do việc ghi nhớ câu hỏi tập huấn luyện. Wang và cộng sự (2021) cũng phát hiện rằng vẫn còn thách thức đối với các mô hình ngôn ngữ được huấn luyện trước quy mô tương đối nhỏ như RoBERTa (Liu và cộng sự, 2019) hoặc GPT-2 (Radford và cộng sự, b) để trả lời các câu hỏi sách đóng.

2

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Trong công trình này, chúng tôi tập trung vào việc đánh giá hiệu suất CBQA của các mô hình ngôn ngữ lớn (LLMs) trong thiết lập few-shot, điều này lý tưởng là giảm thiểu thiên lệch của sự chồng chéo train-test (Liu và cộng sự, 2021). Chúng tôi đề xuất một sơ đồ ngâm-và-trả-lời, tương tự như một học sinh đầu tiên ngâm thơ kiến thức thực tế về câu hỏi, và sau đó trả lời câu hỏi.

2.2 HỌC FEW-SHOT TRONG NGỮ CẢNH

Các mô hình ngôn ngữ lớn (LLMs) như GPT-3 (Brown và cộng sự, 2020) có khả năng đáng ngạc nhiên trong việc học trong ngữ cảnh, nơi mô hình học làm các nhiệm vụ mới đơn giản bằng cách được prompt một vài exemplar. LLMs học từ các exemplar này mà không được huấn luyện trước một cách rõ ràng cho việc học trong ngữ cảnh và không có bất kỳ cập nhật gradient hoặc tinh chỉnh nào. Nghiên cứu gần đây cho thấy rằng khả năng như vậy cải thiện với việc mở rộng cả kích thước mô hình (Brown và cộng sự, 2020; Rae và cộng sự, 2021; Chowdhery và cộng sự, 2022) và số lượng token để huấn luyện (Hoffmann và cộng sự, 2022). Khi được đánh giá trên các nhiệm vụ trả lời câu hỏi chuyên sâu về kiến thức, những mô hình này thường được đánh giá trong thiết lập sách đóng, nơi kiến thức thực tế được lưu trữ hoàn toàn trong các tham số mô hình của LLMs dày đặc.

Gần đây, Atlas (Izacard và cộng sự, 2022) cho thấy rằng đối với các nhiệm vụ NLP chuyên sâu về kiến thức, một mô hình tương đối nhẹ với việc tăng cường truy xuất có thể đạt được hiệu suất tương tự hoặc thậm chí tốt hơn thông qua tinh chỉnh few-shot, điều này chứng minh rằng việc ghi nhớ có thể được tách rời khỏi khái quát hóa trong LLMs. Ngược lại, chúng tôi chỉ ra rằng vẫn còn một lượng kiến thức bị đánh giá thấp lớn có thể được truy xuất từ các trọng số mô hình của LLMs thông qua prompting được thiết kế tốt hơn.

2.3 LẬP LUẬN TĂNG CƯỜNG RATIONALE

Ling và cộng sự (2017) tiên phong trong công việc giải quyết các bài toán từ toán học bằng cách tạo ra các giải pháp từng bước có thể đọc được bằng con người được mô tả bằng ngôn ngữ tự nhiên và các phương trình toán học trước câu trả lời cuối cùng. Điều đó về cơ bản khác với các công trình khác mà trực tiếp tạo ra câu trả lời cuối cùng hoặc sử dụng ngôn ngữ chính thức. ví dụ chỉ phương trình, để minh họa các bước giải trung gian (Roy và cộng sự, 2016; Amini và cộng sự, 2019; Chen và cộng sự, 2019). Cobbe và cộng sự (2021) mở rộng (Ling và cộng sự, 2017) bằng cách xây dựng một tập dữ liệu lớn hơn nhiều để tinh chỉnh một mô hình ngôn ngữ lớn được huấn luyện trước để giải quyết các bài toán từ toán học và một ranker được tham số hóa được huấn luyện để xếp hạng các giải pháp ứng viên để cải thiện tỷ lệ giải quyết. Wei và cộng sự (2022) đề xuất prompting chuỗi suy nghĩ kết hợp ý tưởng của rationale ngôn ngữ tự nhiên (Ling và cộng sự, 2017; Cobbe và cộng sự, 2021) với prompting few-shot (Brown và cộng sự, 2020).

Trong công trình này, thay vì tạo ra một chuỗi suy nghĩ cho các câu hỏi lập luận nhiều bước, chúng tôi phân tách quá trình trả lời một câu hỏi chuyên sâu về kiến thức thành hai bước: ngâm thơ kiến thức liên quan được lưu trữ trong các tham số mô hình, và sau đó trả lời câu hỏi.

2.4 GHI NHỚ TRONG CÁC MÔ HÌNH NGÔN NGỮ LỚN

Nghiên cứu gần đây cho thấy rằng các mô hình ngôn ngữ lớn có thể ghi nhớ dữ liệu huấn luyện của nó, và tạo ra các văn bản từ dữ liệu huấn luyện với các prompt nhất định (Carlini và cộng sự, 2021; 2022; Zhang và cộng sự, 2021; Kharitonov và cộng sự, 2021; Thakkar và cộng sự, 2020; Carlini và cộng sự, 2019; Tirumala và cộng sự, 2022). Liên quan nhất đến công trình của chúng tôi, Carlini và cộng sự (2022) phát hiện rằng khả năng ghi nhớ của LLMs tăng đáng kể khi dung lượng mô hình tăng, số lần một ví dụ đã được nhân đôi, và số lượng token ngữ cảnh được sử dụng để prompt mô hình. Trong khi những công trình này chủ yếu phân tích các tính chất cơ bản của việc ghi nhớ trong thiết lập chính xác, nơi chính xác N token được sử dụng làm prompt để tái tạo hậu tố của prompt, công trình của chúng tôi dựa trên "ghi nhớ mờ", nơi các prompt có xu hướng không hoàn toàn giống với dữ liệu huấn luyện, nhưng vẫn cải thiện độ chính xác ghi nhớ.

Ý tưởng tạo sinh tăng cường ngâm thơ được đề xuất cũng liên quan đến dòng công trình về việc sử dụng bộ nhớ Transformer như một mô hình truy xuất thông tin (Tay và cộng sự, 2022b) và các mô hình self-talk cho lập luận thông thường (Shwartz và cộng sự, 2020; Liu và cộng sự, 2022). Zhuang và cộng sự (2022); Wang và cộng sự (2022c); Zhou và cộng sự (2022) đề xuất tăng cường tài liệu tại thời điểm lập chỉ mục với một số truy vấn được tạo ra. Bevilacqua và cộng sự (2022) đề xuất trực tiếp tạo ra các n-gram được căn cứ trong một hoặc nhiều tài liệu với giải mã bị ràng buộc.

3

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
<Câu hỏi N> 1) học few-shot trong ngữ cảnh 
cho ngâm thơ <Câu hỏi 0> 
<Ngâm thơ 0> 
…
<Câu hỏi N> Đầu vào LM 
<Ngâm thơ N> Đầu ra LM 
2) học few-shot trong ngữ cảnh 
cho QA tăng cường ngâm thơ <Ngâm thơ N> 
<Câu hỏi 0> 
<Trả lời 0> 
…
<Câu hỏi N> Đầu vào LM 
<Trả lời N> Đầu ra LM 
<Trả lời N-0> 
3) tính nhất quán bản thân Bỏ phiếu đa số <Ngâm thơ N-0> 
<Ngâm thơ N-1> 
<Ngâm thơ N-2> <Trả lời N-0> 
<Trả lời N-1> 
<Trả lời N-0> 
Gia đình Oberoi là một phần của 
công ty khách sạn có trụ sở chính 
ở thành phố nào? Ngâm thơ 1 : Hai công ty holding chính của 
The Oberoi Group là EIH Ltd và EIH 
Associated Hotels (trước đây là East India Hotels). 
Gia đình Oberoi là cổ đông lớn trong 
EIH Ltd với 32.11% cổ phần. 
Ngâm thơ 2 : The Oberoi Group là một 
tập đoàn khách sạn sang trọng đoạt giải với trụ sở chính 
ở New Delhi, Ấn Độ. 
<Kết thúc Ngâm thơ> Câu trả lời là Delhi. 
4) ngâm-và-trả-lời nhiều lần 

Hình 2: Minh họa học trong ngữ cảnh dựa trên prompt cho việc tạo ra ngâm thơ, trả lời câu hỏi tăng cường ngâm thơ, tổng hợp tính nhất quán bản thân, và ngâm-và-trả-lời nhiều lần cho các câu hỏi nhiều bước (Mục 3.1). Trong sơ đồ ngâm-và-trả-lời nhiều lần, ngâm thơ sau có thể sử dụng thông tin từ những cái trước, chẳng hạn như "Oberoi Group" trong trường hợp này. Các prompt cho tính nhất quán bản thân và ngâm-và-trả-lời nhiều bước được bỏ qua vì ngắn gọn.

3 HỌC NGÂM THƠ CHO TRẢ LỜI CÂU HỎI SÁCH ĐÓNG

Mục tiêu của bài báo này là mô phỏng khả năng của con người trong việc ngâm thơ kiến thức thực tế liên quan (McDaniel và cộng sự, 2009) trước khi trả lời các câu hỏi chuyên sâu về kiến thức, sao cho những câu hỏi này có thể được trả lời chính xác hơn. Trong phần sau, chúng tôi mô tả sơ đồ ngâm-và-trả-lời cho trả lời câu hỏi sách đóng (CBQA) few-shot, bao gồm hai thành phần: (1) một mô-đun ngâm thơ bằng chứng để ngâm thơ các đoạn văn liên quan, và (2) một mô-đun trả lời câu hỏi để tạo ra câu trả lời dựa trên bằng chứng được ngâm thơ. Lưu ý rằng trong bài báo này, chúng tôi tập trung vào thiết lập few-shot, trong đó chúng tôi giả định chỉ có một vài demonstration câu hỏi-trả lời được cung cấp. Trong benchmark Natural Questions (Kwiatkowski và cộng sự, 2019), vì các câu hỏi là từ các truy vấn được gửi đến công cụ tìm kiếm Google bởi nhiều người dùng, và do đó có thể được coi là dữ liệu không được chú thích, chúng tôi giả định thêm rằng chúng tôi có các trang Wikipedia được truy xuất hàng đầu cho những câu hỏi này. Các đoạn văn trong những trang Wikipedia được truy xuất hàng đầu này sẽ được sử dụng để tạo ra dữ liệu câu hỏi-ngâm thơ được ghép đôi tổng hợp để tinh chỉnh LM (được mô tả trong Mục 3.2).

3.1 NGÂM-VÀ-TRẢ-LỜI DỰA TRÊN PROMPT CHO TRẢ LỜI CÂU HỎI

Trả lời câu hỏi tăng cường ngâm thơ Chúng tôi bắt đầu với trả lời câu hỏi một bước (Kwiatkowski và cộng sự, 2019; Joshi và cộng sự, 2017), nơi các câu trả lời thường được hỗ trợ bởi một tài liệu cụ thể trong kho dữ liệu, điều này đôi khi được gọi là bằng chứng (Joshi và cộng sự, 2017). Khác với prompting chuỗi suy nghĩ (Wei và cộng sự, 2022) nơi một rationale được tạo ra trực tiếp để giải thích câu trả lời được tạo ra (Joshi và cộng sự, 2017; Narang và cộng sự, 2020; Lampinen và cộng sự, 2022), chúng tôi đề xuất đầu tiên ngâm thơ một đoạn văn về câu hỏi, và sau đó trả lời câu hỏi dựa trên việc ngâm thơ.

Chúng tôi đề xuất một sơ đồ học-ngâm thơ dựa trên prompt bằng cách tận dụng khả năng học trong ngữ cảnh của LLM (Brown và cộng sự, 2020). Chúng tôi prompt LLM với các exemplar được ghép đôi của câu hỏi và bằng chứng được ngâm thơ, và LLM có thể học theo cách trong ngữ cảnh để tạo ra một việc ngâm thơ cho một câu hỏi tùy ý. Để thực hiện trả lời câu hỏi few-shot có điều kiện ngâm thơ, chúng tôi nối các đoạn văn được ngâm thơ ở đầu các exemplar câu hỏi-trả lời ban đầu như một prompt duy nhất, và sau đó tạo ra câu trả lời cuối cùng (Bước 1 & 2 trong Hình 2).

4

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
khi nào trợ cấp trẻ em được 
trả cho đứa trẻ đầu tiên? 
LM Ngâm thơ 
Đã tinh chỉnh Gợi ý đoạn văn: Child support --- 
Mô tả ngắn --- Đoạn văn #1 
Gợi ý đoạn văn: Child benefit --- 
Mô tả ngắn --- Đoạn văn #1 
Gợi ý đoạn văn: Child benefit --- 
Lịch sử --- Đoạn văn #2 Lấy mẫu các gợi ý đoạn văn đa dạng Giải mã tham lam các việc ngâm thơ 
1911 Trả lời câu hỏi với 
các việc ngâm thơ 
được tổng hợp 
Mô hình Ngôn ngữ 
Đông lạnh <Gợi ý đoạn văn K> 
<Đoạn văn K> <Câu hỏi được 
Tạo K> Tinh chỉnh gợi ý đoạn văn 
cho việc ngâm thơ đa dạng <Đoạn văn 0> 
<Câu hỏi 0> 
…
<Đoạn văn K> Đầu vào LM 
<Câu hỏi được 
Tạo K> Đầu ra LM 
tạo câu hỏi few-shot 
tinh chỉnh LM Đầu vào LM Đầu ra LM 
dự đoán <Gợi ý đoạn văn N> 
<Ngâm thơ N> <Câu hỏi N> Đầu vào LM Đầu ra LM 
Ngâm thơ: Luật hỗ trợ trẻ em đầu tiên 
được thông qua vào năm 1911 tại 
Vương quốc Anh… 
Ngâm thơ: Trợ cấp trẻ em đầu tiên 
được giới thiệu tại Vương quốc Anh 
vào năm 1911, và được trả cho… 
Ngâm thơ: Trợ cấp trẻ em là 
khoản thanh toán được thực hiện cho các bậc cha mẹ ở 
Vương quốc Anh, Ireland, 
Australia… QA với tăng cường 
ngâm thơ đa dạng 

Hình 3: Minh họa trả lời câu hỏi với ngâm thơ đa dạng và các quá trình tạo câu hỏi few-shot và tinh chỉnh tương ứng.

Tổng hợp tính nhất quán bản thân Kiến thức thực tế về một câu hỏi có thể xuất hiện ở nhiều nơi trong các kho dữ liệu huấn luyện của mô hình ngôn ngữ. Ví dụ, sự thật "Nữ hoàng Elizabeth II khai trương Cầu London vào ngày 17 tháng 3 năm 1973" có thể xuất hiện trong cả trang Wikipedia "London Bridge" và trang "March 1973", vì vậy rất có khả năng tồn tại kiến thức từ các bài viết khác nhau có thể dẫn đến cùng một câu trả lời đúng. Với động lực này, chúng tôi lập luận rằng tương tự như lập luận nhiều bước trong chuỗi suy nghĩ, trả lời câu hỏi tăng cường ngâm thơ cũng có thể hưởng lợi từ kỹ thuật tính nhất quán bản thân với giải mã nhiều đường (Wang và cộng sự, 2022b). Cụ thể, với một câu hỏi tùy ý, chúng tôi đầu tiên sử dụng lấy mẫu top-k để tạo ra một vài việc ngâm thơ độc lập, và sau đó giải mã tham lam câu trả lời của câu hỏi dựa trên các việc ngâm thơ được lấy mẫu. Cuối cùng, chúng tôi xác định câu trả lời tối ưu bằng cách bỏ phiếu đa số/đa nguyên (Bước 3 trong Hình 2).

Ngâm-và-trả-lời nhiều lần cho trả lời câu hỏi nhiều bước Trả lời câu hỏi nhiều bước yêu cầu hệ thống QA tìm và lập luận trên nhiều tài liệu hỗ trợ. Tuy nhiên, bản chất của việc ngâm thơ hạn chế chúng tôi ngâm thơ các đoạn văn từ một bài viết tại một thời điểm. Để áp dụng sơ đồ ngâm-và-trả-lời để giải quyết các câu hỏi nhiều bước, chúng tôi giới thiệu sơ đồ ngâm-và-trả-lời nhiều lần (Bước 4 trong Hình 2), tức là, với câu hỏi nhiều bước, chúng tôi sử dụng các từ prompt như "Ngâm thơ 1" và "Ngâm thơ 2" để khơi gợi LLM tạo ra các đoạn văn ngâm thơ về các chủ đề khác nhau. Vì các đoạn văn ngâm thơ nhiều lần được tạo ra trong một lần thông qua việc giải mã LLM tuần tự, việc tạo ra các đoạn văn sau có thể sử dụng hiệu quả thông tin cả trong câu hỏi ban đầu và các đoạn văn được ngâm thơ trước đó. Sơ đồ ngâm-và-trả-lời nhiều lần của chúng tôi cho trả lời câu hỏi nhiều bước cũng tương thích với kỹ thuật tính nhất quán bản thân, bằng cách áp dụng lấy mẫu top-k khi tạo ra nhiều việc ngâm thơ và thực hiện bỏ phiếu đa số cho các câu trả lời cuối cùng.

3.2 NGÂM THƠ ĐA DẠNG HÓA DỰA TRÊN GỢI Ý ĐOẠN VĂN VỚI TINH CHỈNH

Ngâm thơ đa dạng hóa dựa trên gợi ý đoạn văn Trong khi việc ngâm thơ dựa trên lấy mẫu và tính nhất quán bản thân cải thiện tính mạnh mẽ của phương pháp ngâm-và-trả-lời, một lập luận cho sự không hiệu quả của nó là nếu mô-đun ngâm thơ bằng chứng lấy mẫu sai sự thật về câu hỏi, mô-đun trả lời câu hỏi sẽ không thể tìm ra nó và có xu hướng tạo ra câu trả lời sai. Do đó, một mặt, chúng tôi cần sử dụng nhiệt độ lấy mẫu thấp để tránh tạo ra các việc ngâm thơ với sự thật sai, mặt khác, chúng tôi muốn đảm bảo các việc ngâm thơ được lấy mẫu có đủ đa dạng.

Để giải quyết tình huống khó xử như vậy, chúng tôi đề xuất ngâm thơ đa dạng hóa dựa trên gợi ý đoạn văn. Chúng tôi quan sát thấy rằng trong các cơ sở dữ liệu kiến thức văn bản được hình thành tốt, chẳng hạn như Wikipedia, chúng tôi thường có thể tìm thấy một gợi ý đoạn văn duy nhất cho mỗi đoạn văn, bằng cách nối các tiêu đề mục và thứ tự trong mục của mỗi đoạn văn. Ví dụ, gợi ý đoạn văn của đoạn văn thứ hai trong Mục 5.2 "Enforcement" của trang Wikipedia "Child support" sẽ là "Child support — Compliance and enforcement issues — Enforcement — Paragraph #2". Trong ngâm thơ đa dạng hóa dựa trên gợi ý đoạn văn, chúng tôi đầu tiên sử dụng lấy mẫu để tạo ra một tập hợp đa dạng các gợi ý đoạn văn, và sau đó sử dụng giải mã tham lam để đảm bảo độ chính xác thực tế của nội dung trong mỗi đoạn văn.

5

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Vì mỗi gợi ý đoạn văn tương ứng với một đoạn văn duy nhất, chúng tôi có thể đầu tiên khử trùng lặp các gợi ý đoạn văn và sau đó tạo ra các đoạn văn đầy đủ để có được các đoạn văn ngâm thơ đa dạng hơn. Hơn nữa, vì các đoạn văn được ngâm thơ ít có khả năng tương tự do các gợi ý đoạn văn duy nhất, được lấy cảm hứng từ tiến bộ gần đây về trả lời câu hỏi với nhiều đoạn văn được truy xuất (Izacard & Grave, 2021), chúng tôi sử dụng các việc ngâm thơ đa dạng được tổng hợp như một ngữ cảnh duy nhất, và tạo ra câu trả lời với thêm một vài demonstration cặp câu hỏi-trả lời. Hình 3 (dưới) minh họa sơ đồ ngâm-và-trả-lời với ngâm thơ đa dạng hóa dựa trên gợi ý đoạn văn.

Tinh chỉnh trên các câu hỏi được tạo few-shot Chúng tôi phát hiện rằng mặc dù dữ liệu huấn luyện của nhiều LLMs hiện có (Devlin và cộng sự, 2019; Chowdhery và cộng sự, 2022) chứa các kho dữ liệu Wikipedia, thường được coi là các tài liệu thực tế cho các nhiệm vụ trả lời câu hỏi chuyên sâu về kiến thức (Joshi và cộng sự, 2017; Kwiatkowski và cộng sự, 2019), các tiêu đề mục thường không được bao gồm một cách rõ ràng trong huấn luyện. Điều này khiến LLM được huấn luyện trước khó khám phá ánh xạ từ câu hỏi đến gợi ý đoạn văn, và đến đoạn văn đầy đủ chỉ bằng prompting few-shot.

Để giải quyết vấn đề này, chúng tôi đề xuất một giai đoạn tinh chỉnh bổ sung để thích ứng LLMs học các ánh xạ như vậy. Giả định chúng tôi có quyền truy cập không chỉ vào một vài cặp câu hỏi-trả lời, mà còn các trang Wikipedia được truy xuất hàng đầu cho các truy vấn được gửi đến công cụ tìm kiếm Google bởi nhiều người dùng (Kwiatkowski và cộng sự, 2019), chúng tôi có thể sử dụng prompting few-shot để tạo ra các cặp câu hỏi-gợi ý-đoạn văn tổng hợp và sau đó tinh chỉnh LLMs trên dữ liệu được tạo ra.

Cụ thể, chúng tôi sử dụng các cặp bằng chứng và câu hỏi thực tế làm prompt, và tạo ra các câu hỏi mới với học trong ngữ cảnh cho các đoạn văn được lấy mẫu ngẫu nhiên từ các trang Wikipedia. Tiếp theo, dựa trên các câu hỏi được tạo few-shot, chúng tôi huấn luyện LM để dự đoán gợi ý đoạn văn ban đầu, cũng như nội dung đoạn văn. Hình 3 (trên) minh họa toàn bộ quá trình tinh chỉnh gợi ý đoạn văn.

4 THÍ NGHIỆM

Trong phần này, chúng tôi báo cáo các đánh giá thực nghiệm của RECITE được đề xuất với các sơ đồ ngâm-và-trả-lời trên một tập hợp đa dạng các nhiệm vụ trả lời câu hỏi sách đóng few-shot và các mô hình ngôn ngữ khác nhau với các quy mô khác nhau.

4.1 THIẾT LẬP THÍ NGHIỆM

4.1.1 CÁC TẬP DỮ LIỆU ĐÁNH GIÁ

Natural Questions Natural Questions (Kwiatkowski và cộng sự, 2019) bao gồm các câu hỏi được tổng hợp từ công cụ tìm kiếm Google và các câu trả lời từ trang Wikipedia trong 5 kết quả tìm kiếm hàng đầu. Chúng tôi coi nó như một nhiệm vụ trả lời câu hỏi một bước. Vì Natural Questions chứa các chú thích "long answer" được gọi là, đó là toàn bộ hộp giới hạn HTML chứa đủ thông tin để suy ra câu trả lời, chúng tôi trực tiếp sử dụng "long answer" làm các exemplar ngâm thơ thực tế trong prompt của chúng tôi (Mục 3.1). Để so sánh trực tiếp với các LLMs gần đây (Chowdhery và cộng sự, 2022; Izacard và cộng sự, 2022), chúng tôi đánh giá các phương pháp của mình trong thiết lập 5-shot và 64-shot.

TriviaQA Tập dữ liệu TriviaQA (Joshi và cộng sự, 2017) được xây dựng bằng cách thu thập các cặp câu hỏi-trả lời được tác giả bởi những người đam mê Trivia và bằng chứng được thu thập hồi cứu của chúng. Vì không có cách rõ ràng để thu thập "long answer" trong các tài liệu bằng chứng hồi cứu (sự xuất hiện chính xác của câu trả lời có thể chứa đủ thông tin để suy ra câu trả lời), chúng tôi đánh giá TriviaQA trong thiết lập một bước 5-shot, và soạn thảo thủ công đoạn văn ngâm thơ từ Wikipedia cho 5 câu hỏi huấn luyện được lấy mẫu ngẫu nhiên. Prompt cụ thể có thể được tìm thấy trong phụ lục.

HotpotQA HotpotQA (Yang và cộng sự, 2018) được thiết kế để kiểm tra một cách rõ ràng khả năng thực hiện lập luận nhiều bước của các hệ thống QA. Nó được thu thập bằng cách soạn thảo một cách rõ ràng các câu hỏi yêu cầu lập luận

6

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bảng 1: So sánh hiệu suất trên Natural Questions (NQ), TriviaQA, và HotpotQA. Số lượng shot cho mỗi nhiệm vụ được đề cập trong ngoặc đơn.
PaLM-62B UL2-20B OPT-30B Codex-002
EM / F1 EM / F1 EM / F1 EM / F1
NQPrompting tiêu chuẩn (trực tiếp)25.76 / 36.47 (5)10.16 / 20.17 (5)14.97 / 22.93 (5)31.45 / 44.75 (5)28.98 / 40.13 (64)12.70 / 21.97 (16)
Ngâm-và-trả-lời (20-đường)28.70 /39.76 (5)14.16 /23.13 (5)17.84 /26.74 (5)35.84 /49.12 (5)31.34 /42.48 (64)14.94 /24.29 (16)
TriviaQAPrompting tiêu chuẩn (trực tiếp) 65.38 / 71.85 (5)48.73 / 54.32 (5)45.90 / 50.68 (5)81.84 / 86.09 (5)
Ngâm-và-trả-lời (20-đường) 65.84 /72.10 (5)53.42 /58.69 (5)49.02 /54.22 (5)83.50 /88.03 (5)
HotpotQAPrompting tiêu chuẩn (trực tiếp) 20.51 / 28.90 (4)16.99 / 24.99 (4)16.70 / 25.21 (4)28.32 / 39.03 (4)
Chuỗi suy nghĩ (20-đường) 23.73 / 32.80 (4)17.68 / 24.87 (4)16.89 / 24.03 (4)34.38 / 45.50 (4)
Ngâm-và-trả-lời (20-đường) 26.46 / 35.67 (4)19.04 /27.32 (4)17.77 /26.58 (4)37.11 /48.37 (4)
trên nhiều tài liệu ngữ cảnh hỗ trợ. Theo Wang và cộng sự (2022a), chúng tôi đánh giá HotpotQA như một nhiệm vụ trả lời câu hỏi nhiều bước trong thiết lập 4-shot. Nhưng thay vì prompting chuỗi suy nghĩ như trong (Wang và cộng sự, 2022a), chúng tôi sử dụng ngâm-và-trả-lời nhiều lần (Mục 3.1) để đạt được lập luận nhiều bước. Chúng tôi cũng cung cấp prompt cụ thể trong phụ lục.

Thước đo Chúng tôi tính toán điểm Exact Matching (EM) và F1 cho các câu trả lời được chuẩn hóa, trong khi việc chuẩn hóa văn bản cụ thể được áp dụng trên mỗi tập dữ liệu có thể hơi khác nhau.

4.1.2 CÁC MÔ HÌNH NGÔN NGỮ ĐƯỢC HUẤN LUYỆN TRƯỚC

Chúng tôi đánh giá hiệu quả của RECITE trên bốn mô hình ngôn ngữ: PaLM, UL2 (Tay và cộng sự, 2022a), OPT (Zhang và cộng sự, 2022), và Codex (Brown và cộng sự, 2020; Ouyang và cộng sự, 2022; Chen và cộng sự, 2021). Do hạn chế về không gian, các mô tả chi tiết của chúng được cung cấp trong Phụ lục D.

4.2 THÍ NGHIỆM

Chúng tôi sử dụng split test cho tất cả các nhiệm vụ nếu split test có sẵn và có nhãn để đánh giá, nếu không chúng tôi sử dụng split dev. Ngoài ra, TriviaQA và HotpotQA quá lớn để chạy các mô hình ngôn ngữ lớn, vì vậy chúng tôi đã sử dụng 1,024 điểm dữ liệu đầu tiên để đánh giá.

4.2.1 KẾT QUẢ DỰA TRÊN PROMPT

Chúng tôi báo cáo kết quả đánh giá trả lời câu hỏi sách đóng (CBQA) một bước trên Natural Questions (NQ) và TriviaQA và kết quả đánh giá CBQA nhiều bước trên HotpotQA. Trong Bảng 1, chúng tôi báo cáo kết quả với học trong ngữ cảnh dựa trên prompt và tính nhất quán bản thân.

Từ các bảng, chúng tôi có thể thấy rằng sơ đồ ngâm-và-trả-lời được đề xuất có thể cải thiện đáng kể hiệu suất CBQA trên cả hai tập dữ liệu với các mô hình ngôn ngữ được huấn luyện trước khác nhau. Trong khi các cải thiện hiệu suất trên NQ nhất quán hơn trên các mô hình ngôn ngữ khác nhau, chúng tôi thấy rằng các cải thiện từ ngâm-và-trả-lời đáng kể hơn trên các mô hình ngôn ngữ nhỏ hơn trên TriviaQA. Giả thuyết của chúng tôi là câu hỏi kiểu Trivia thường chứa nhiều thông tin ngữ cảnh hơn trong câu hỏi, do đó làm giảm hiệu quả của việc ngâm thơ đối với các LLMs mạnh như PaLM.

Bên cạnh đó, chúng tôi có thể thấy rằng sơ đồ ngâm-và-trả-lời có thể vượt trội hơn hiệu suất prompting chuỗi suy nghĩ trên nhiệm vụ lập luận nhiều bước. Thú vị là, chúng tôi cũng thấy rằng đối với các LLMs có lợi ích lớn từ chuỗi suy nghĩ (tức là, PaLM), chúng cũng có những cải thiện lớn từ ngâm-và-trả-lời.

4.2.2 KẾT QUẢ CỦA NGÂM THƠ ĐA DẠNG HÓA DỰA TRÊN GỢI Ý ĐOẠN VĂN

Đối với tập dữ liệu Natural Questions, vì nó có bộ sưu tập các trang Wikipedia được truy xuất hàng đầu tương ứng với các truy vấn không được chú thích được gửi đến công cụ tìm kiếm Google, chúng tôi báo cáo thêm kết quả ngâm thơ đa dạng hóa của mô hình PaLM được tinh chỉnh trong Bảng 2. Từ bảng, chúng tôi thấy rằng ngâm thơ đa dạng hóa có thể cải thiện thêm hiệu suất của PaLM trên tập dữ liệu NQ.

7

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bảng 2: So sánh hiệu suất của PaLM-62B trên tập dữ liệu Natural Questions (NQ) với prompting tiêu chuẩn, ngâm-và-trả-lời với lấy mẫu tính nhất quán bản thân, và ngâm-và-trả-lời với ngâm thơ đa dạng hóa. Số lượng shot cho mỗi nhiệm vụ được đề cập trong ngoặc đơn.
EM / F1 (5) EM / F1 (64)
Prompting tiêu chuẩn (trực tiếp) 25.76 / 36.47 28.98 / 40.13
Ngâm-và-trả-lời (20-đường) 28.70 / 39.76 31.34 / 42.48
Ngâm-và-trả-lời với ngâm thơ đa dạng hóa (20-đường) 32.20 /44.02 33.23 /45.29

4.3 PHÂN TÍCH

Hình 4: TriviaQA EM/F1 trên OPT-30B và UL2-20B với số lượng đường tính nhất quán bản thân khác nhau.

4.3.1 VỀ SỐ LƯỢNG ĐƯỜNG TÍNH NHẤT QUÁN BẢN THÂN

Chúng tôi phân tích cách số lượng đoạn văn được ngâm thơ sẽ ảnh hưởng đến hiệu suất của ngâm-và-trả-lời trong thiết lập tính nhất quán bản thân. Do chi phí suy luận đắt đỏ của LLMs, chúng tôi đầu tiên lấy mẫu lên đến k= 20 đoạn văn ngâm thơ, và sau đó áp dụng tính nhất quán bản thân cho một tập hợp con được chọn ngẫu nhiên của các việc ngâm thơ để mô phỏng ít đường hơn. Đối với mỗi số lượng đường tính nhất quán bản thân, chúng tôi đánh giá các tập hợp con được chọn ngẫu nhiên năm lần và báo cáo trung bình và độ lệch chuẩn. Chúng tôi tiến hành phân tích trên OPT-30B và UL2-20B trên tập dữ liệu TriviaQA và báo cáo kết quả trong Hình 4. Chúng tôi có thể thấy rằng lấy mẫu nhiều đoạn văn ngâm thơ hơn có xu hướng cải thiện hiệu suất ngâm-và-trả-lời, trong khi ít tính ngẫu nhiên được quan sát với nhiều đường tính nhất quán bản thân hơn.

4.3.2 VỀ TÍNH MẠNH MẼ CỦA CÁC EXEMPLAR FEW-SHOT

Một vấn đề nổi tiếng của học trong ngữ cảnh few-shot là tính không ổn định của nó đối với các lựa chọn exemplar và thứ tự của chúng (Zhao và cộng sự, 2021). Chúng tôi đánh giá tính mạnh mẽ của prompting tiêu chuẩn và phương pháp ngâm-và-trả-lời của chúng tôi với 5 seed ngẫu nhiên và báo cáo trung bình và độ lệch chuẩn của mô hình UL2 chạy trên tập dữ liệu TriviaQA trong Bảng 6. Các exemplar 5-shot được lấy mẫu ngẫu nhiên và xáo trộn cho mỗi seed. Từ bảng, chúng tôi có thể thấy rằng với lấy mẫu ngâm thơ, ngâm-và-trả-lời thể hiện tính mạnh mẽ tương tự (về độ lệch hiệu suất nhỏ) như prompting tiêu chuẩn dưới các seed ngẫu nhiên và số lượng đường tính nhất quán bản thân khác nhau. Các lợi ích tổng thể bằng ngâm-và-trả-lời là đáng kể so với prompting tiêu chuẩn bất kể sự lựa chọn các exemplar few-shot.

4.3.3 NGÂM THƠ V.S. TRUY XUẤT V.S. THỰC TẾ

Người ta có thể hỏi mà không có kho dữ liệu bên ngoài, liệu chất lượng của các đoạn văn được ngâm thơ với LLMs có tốt hơn các mô hình truy xuất đơn giản, ví dụ, BM25 (Robertson và cộng sự, 2009)1. Để trả lời câu hỏi này, chúng tôi đánh giá hiệu suất trả lời câu hỏi few-shot của UL2 và Codex trên ba loại đoạn văn ngữ cảnh: truy xuất, ngâm thơ, và thực tế. Chúng tôi báo cáo kết quả trên 1024 ví dụ validation đầu tiên trong tập dữ liệu Natural Questions (NQ), vì đây là tập dữ liệu duy nhất chứa chú thích "long answer" có thể được coi là đoạn văn ngữ cảnh thực tế. Từ Bảng 3, chúng tôi có thể thấy rằng mô hình truy xuất cổ điển, tức là, BM25, vẫn là một baseline rất mạnh để thu thập thông tin từ kho dữ liệu. Tuy nhiên, so với BM25, ngâm-và-trả-lời của chúng tôi vẫn đạt được hiệu suất khá cạnh tranh chỉ thông qua tạo sinh và mà không sử dụng bất kỳ kho dữ liệu bên ngoài nào. Bên cạnh đó, chúng tôi thấy rằng

1Chúng tôi sử dụng kho dữ liệu "enwiki-paragraphs" được lập chỉ mục trước trong gói pyserini ( https://github.com/castorini/pyserini ), ban đầu được thiết kế cho BERTserini (Yang và cộng sự, 2019a).

8

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bảng 3: Kết quả Natural Questions (NQ) với các đoạn văn ngữ cảnh khác nhau.
UL2-20B (5) Codex-002 (5)
EM / F1 EM / F1
Không có đoạn văn 10.16 / 20.17 31.45 / 44.75
Đoạn văn thực tế 41.02 / 55.73 49.32 / 64.32
Truy xuất BM25 (Top-1) 16.31 / 27.66 33.20 / 47.45
Ngâm thơ LM (5)(20-đường) 14.16 / 23.13 35.84 / 49.12

Bảng 4: Phân tích lỗi từng câu hỏi trên TriviaQA.
UL2-20B (5)OPT-30B (5)
Hits@Majority 53.42% 49.02%
Not Recit. 21.09% 22.27%
Hits@20-Recit. 5.66% 8.01%
Hits@20-Path 19.82% 20.07%

Bảng 5: Phân tích lỗi từng đường trên TriviaQA.
Recit. Ans. UL2-20B (5)OPT-30B (5)
3 3 33.60% 30.06%
3 7 7.87% 9.79%
7 3 12.10% 12.57%
7 7 46.44% 47.58%

các mô hình mạnh hơn (tức là, Codex) có xu hướng hưởng lợi nhiều hơn từ việc ngâm thơ của chính mô hình hơn là ngữ cảnh được truy xuất BM25.

4.3.4 PHÂN TÍCH LỖI

Chúng tôi thực hiện phân tích lỗi trên 1024 ví dụ đánh giá trong tập dữ liệu TriviaQA. Chúng tôi phân loại các lỗi thành ba danh mục: 1) Not Recit., tức là, câu trả lời đúng không được ngâm thơ trong bất kỳ đoạn văn ngâm thơ nào trong 20 đoạn văn trong tính nhất quán bản thân. 2) Hits@20-Recit., tức là, câu trả lời đúng có thể được tìm thấy trong một trong các đoạn văn ngâm thơ, nhưng không xuất hiện trong các đầu ra của mô-đun QA. 3) Hits@20-Path, tức là, câu trả lời đúng là một trong các đầu ra cuối cùng của 20 đường tính nhất quán bản thân, nhưng nó không có phiếu bầu đa số. Câu trả lời cuối cùng đúng được đánh dấu là Hits@Majority (tức là, Exact Matching). Một mô tả thuật toán được đưa ra trong Thuật toán 1. Chúng tôi báo cáo kết quả của UL2-20B và OPT-30B trong Bảng 4.

Chúng tôi có thể thấy rằng "No Recit" và "Hits@20-Path" chiếm phần lớn các lỗi, có nghĩa là mô-đun QA hoạt động khá tốt (nếu câu trả lời đúng xuất hiện trong một trong các đoạn văn ngâm thơ, nó sẽ được trích xuất bởi mô-đun QA trong hầu hết các trường hợp), và nút thắt cổ chai chính vẫn nằm ở chất lượng ngâm thơ và các chiến lược tổng hợp câu trả lời.

Chúng tôi cũng thực hiện phân tích lỗi từng đường, tức là, có bao nhiêu câu hỏi có thể được trả lời đúng (hoặc không) khi việc ngâm thơ chính xác chứa (hoặc không) các token câu trả lời. Kết quả được hiển thị trong Bảng 5. Chúng tôi có thể thấy rằng khoảng 7%10% câu hỏi có việc ngâm thơ đúng nhưng không thể tạo ra câu trả lời đúng, trong khi khoảng 12% câu hỏi không có việc ngâm thơ đúng nhưng vẫn có thể được trả lời đúng.

5 KẾT LUẬN & THẢO LUẬN

Trong bài báo này, chúng tôi đề xuất một khung tạo sinh tăng cường ngâm thơ mới để cải thiện hiệu suất của các mô hình ngôn ngữ trong thiết lập trả lời câu hỏi sách đóng. Chúng tôi giả thuyết rằng đối với các nhiệm vụ NLP chuyên sâu về kiến thức, khuyến khích mô hình ngâm thơ một cách rõ ràng một nguồn kiến thức cụ thể sẽ hữu ích trong việc tăng cường bộ nhớ của nó. Ngoài ra, chúng tôi thấy rằng đa dạng hóa quá trình ngâm thơ cũng có thể có lợi vì thường tồn tại nhiều nguồn kiến thức có thể được sử dụng để trả lời cùng một câu hỏi. Chúng tôi cho thấy kết quả đầy hứa hẹn trên ba mô hình ngôn ngữ lớn và trên ba tập dữ liệu QA sách đóng khác nhau, chứng minh hiệu quả của phương pháp ngâm-và-trả-lời được đề xuất.

Một hạn chế của phương pháp chúng tôi là việc cập nhật kiến thức nhạy cảm với thời gian cho phương pháp dựa trên LLM thuần túy đòi hỏi huấn luyện hoặc tinh chỉnh LLMs trên kho dữ liệu mới, điều này có thể tốn kém. Đối với công việc tương lai, chúng tôi dự định xác thực thêm hiệu quả của tạo sinh tăng cường ngâm thơ cho các nhiệm vụ NLP chuyên sâu về kiến thức khác trong thiết lập sách đóng, chẳng hạn như kiểm tra sự thật.

9

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
LỜI CẢM ƠN
Chúng tôi cảm ơn sự hỗ trợ và phản hồi của nhiều người từ nhóm Google Brain và những gợi ý xây dựng từ các nhà đánh giá ẩn danh.

TUYÊN BỐ ĐẠO ĐỨC
Mục tiêu của bài báo này là sử dụng việc ngâm thơ như một bước trung gian để tạo ra kiến thức thực tế chính xác hơn trong các đầu ra của mô hình. Do đó, phương pháp của chúng tôi về nguyên tắc nên cải thiện tính trung thực của các hệ thống LLM. Tuy nhiên, khác với các mô hình tạo sinh tăng cường truy xuất (RAG) có thể sử dụng kho dữ liệu bên ngoài đáng tin cậy, tất cả các bước trung gian trong RECITE được tạo ra bởi chính LLM, RECITE có thể tăng cường thêm các thiên lệch hiện có trong các trọng số mô hình của LLMs so với RAG.

TUYÊN BỐ TÁI TẠO
Trọng số mô hình Trọng số mô hình của hai LLMs được sử dụng trong các thí nghiệm của chúng tôi, tức là, UL2-20B (Tay và cộng sự, 2022a) và OPT-30B (Zhang và cộng sự, 2022), được phát hành công khai thông qua GCP bucket ( gs://scenic-bucket/ul2 ) và Github ( https://github.com/facebookresearch/metaseq ), tương ứng. Mô hình Codex ( code-davinci-002 ) có sẵn công khai thông qua các cuộc gọi API ( https://beta.openai.com/examples/ ).

Tập dữ liệu đánh giá Ba tập dữ liệu đánh giá được sử dụng trong các thí nghiệm của chúng tôi (Natural Questions2, TriviaQA3, và HotpotQA4) đều có thể truy cập công khai.

Prompts Chúng tôi cung cấp tất cả các prompt được sử dụng trong phụ lục.

Mã nguồn Mặc dù các ví dụ prompt trong phụ lục nên đủ để tái tạo tất cả kết quả trong bài báo của chúng tôi, chúng tôi mở mã nguồn tất cả mã đánh giá tại https://github.com/Edward-Sun/RECITE .

TÀI LIỆU THAM KHẢO

Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, và Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , trang 2357–2367, 2019.

Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault F ´evry, và cộng sự. Promptsource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations , trang 93–104, 2022.

Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, và Fabio Petroni. Autoregressive search engines: Generating substrings as document identifiers. arXiv preprint arXiv:2204.10628 , 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.

2https://github.com/google-research-datasets/natural-questions
3https://nlp.cs.washington.edu/triviaqa/
4https://github.com/hotpotqa/hotpot

10

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Nicholas Carlini, Chang Liu, ´Ulfar Erlingsson, Jernej Kos, và Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19) , trang 267–284, 2019.

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-V oss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, và cộng sự. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) , trang 2633–2650, 2021.

Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, và Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646 , 2022.

Danqi Chen, Adam Fisch, Jason Weston, và Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 1870–1879, 2017.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, và cộng sự. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.

Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, và Quoc V Le. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations , 2019.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, và cộng sự. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, và cộng sự. Scaling instruction-finetuned language models.arXiv preprint arXiv:2210.11416 , 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, và John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , trang 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Mingwei Chang. Retrieval augmented language model pre-training. In International Conference on Machine Learning , trang 3929–3938. PMLR, 2020.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, và cộng sự. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.

Gautier Izacard và Edouard Grave. Distilling knowledge from reader to retriever for question answering. In International Conference on Learning Representations , 2020.

Gautier Izacard và ´Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , trang 874–880, 2021.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, và Edouard Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022.

11

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Zhengbao Jiang, Frank F Xu, Jun Araki, và Graham Neubig. How can we know what language models know. Transactions of the Association for Computational Linguistics , 8:423–438, 2020.

Mandar Joshi, Eunsol Choi, Daniel S Weld, và Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 1601–1611, 2017.

Eugene Kharitonov, Marco Baroni, và Dieuwke Hupkes. How bpe affects memorization in transformers. arXiv preprint arXiv:2110.02782 , 2021.

Taku Kudo và John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 , 2018.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, và cộng sự. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:453–466, 2019.

Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, và Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329 , 2022.

Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, và Nikolai Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115 , 2022.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt ¨aschel, và cộng sự. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33: 9459–9474, 2020.

Patrick Lewis, Pontus Stenetorp, và Sebastian Riedel. Question and answer test-train overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , trang 1000–1008, 2021.

Wang Ling, Dani Yogatama, Chris Dyer, và Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 158–167, 2017.

Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, và Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 3154–3169, 2022.

Linqing Liu, Patrick Lewis, Sebastian Riedel, và Pontus Stenetorp. Challenges in generalization in open domain question answering. arXiv preprint arXiv:2109.01156 , 2021.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.

Mark A McDaniel, Daniel C Howard, và Gilles O Einstein. The read-recite-review study strategy: Effective and portable. Psychological Science , 20(4):516–522, 2009.

Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, và Karishma Malkan. Wt5?! training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546 , 2020.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, và cộng sự. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 , 2022.

12

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Fabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, và Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , trang 2463–2473, 2019.

John Prager và cộng sự. Open-domain question–answering. Foundations and Trends® in Information Retrieval , 1(2):91–231, 2007.

Alec Radford, Karthik Narasimhan, Tim Salimans, và Ilya Sutskever. Improving language understanding by generative pre-training. a.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. b.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, và cộng sự. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, và cộng sự. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. , 21(140):1–67, 2020.

Adam Roberts, Colin Raffel, và Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , trang 5418–5426, 2020.

Stephen Robertson, Hugo Zaragoza, và cộng sự. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval , 3(4):333–389, 2009.

Subhro Roy, Shyam Upadhyay, và Dan Roth. Equation parsing: Mapping sentences to grounded equations. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , trang 1088–1097, 2016.

Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, và cộng sự. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations , 2021.

Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , trang 4615–4629, 2020.

Alon Talmor, Yanai Elazar, Yoav Goldberg, và Jonathan Berant. olmpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics , 8:743–758, 2020.

Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, và Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131 , 2022a.

Yi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, và cộng sự. Transformer memory as a differentiable search index. arXiv preprint arXiv:2202.06991 , 2022b.

Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, và Franc ¸oise Beaufays. Understanding unintended memorization in federated learning. arXiv preprint arXiv:2006.07490 , 2020.

Kushal Tirumala, Aram H Markosyan, Luke Zettlemoyer, và Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. arXiv preprint arXiv:2205.10770 , 2022.

Cunxiang Wang, Pai Liu, và Yue Zhang. Can generative pre-trained language models serve as knowledge bases for closed-book qa? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , trang 3241–3251, 2021.

13

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, và Denny Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022a.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, và Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022b.

Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, và cộng sự. A neural corpus indexer for document retrieval. arXiv preprint arXiv:2206.02743 , 2022c.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, và Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations , 2021.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Brian Ichter, Fei Xia, Quoc Le, và Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems , 35, 2022.

Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, và Jimmy Lin. End-to-end open-domain question answering with bertserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) , trang 72–77, 2019a.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, và Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , trang 2369–2380, 2018.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems , 32, 2019b.

Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tram `er, và Nicholas Carlini. Counterfactual memorization in neural language models. arXiv preprint arXiv:2112.12938 , 2021.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, và cộng sự. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.

Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, và Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning , trang 12697–12706. PMLR, 2021.

Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Wu, Peitian Zhang, và Ji-Rong Wen. Ultron: An ultimate retriever on corpus with a model-based indexer. arXiv preprint arXiv:2208.09257 , 2022.

Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, và Daxin Jiang. Bridging the gap between indexing and retrieval for differentiable search index with query generation. arXiv preprint arXiv:2206.10128 , 2022.

14

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
A MINH HỌA CÁC PROMPT VÀ ĐẦU RA CỦA MÔ HÌNH NGÔN NGỮ
Hình 7-15 minh họa prompt ngâm thơ bằng chứng, trả lời câu hỏi mà chúng tôi đã sử dụng cho tập dữ liệu Natural Questions, TriviaQA, và HotpotQA. Chúng tôi cũng cung cấp các ví dụ về việc ngâm thơ được lấy mẫu cho những tập dữ liệu này. Lưu ý rằng đối với Natural Questions, chúng tôi sử dụng chú thích "long answer" làm việc ngâm thơ trong prompt, trong khi đối với hai tập dữ liệu khác, chúng tôi soạn thảo thủ công một vài đoạn văn ngâm thơ dựa trên tìm kiếm web.

B NGUYÊN TẮC THIẾT KẾ PROMPT
Chúng tôi chủ yếu theo Chowdhery và cộng sự (2022) và sử dụng hai ký hiệu dòng mới " nnnn" làm dấu phân cách giữa các thành phần khác nhau trong exemplar, và sử dụng ba ký hiệu dòng mới " nnnnnn" làm dấu phân cách giữa các exemplar khác nhau.

Đối với mô hình UL2 (Tay và cộng sự, 2022a), vì từ vựng SentencePiece (Kudo & Richardson, 2018) ban đầu của nó không mã hóa ký hiệu dòng mới " nn", chúng tôi thay vào đó sử dụng " ;" để thay thế "nn" làm dấu phân cách trong tất cả các prompt.

C CHI TIẾT TINH CHỈNH DỰA TRÊN GỢI Ý ĐOẠN VĂN
Đối với tập dữ liệu Natural Questions (Kwiatkowski và cộng sự, 2019), chúng tôi giả định rằng chúng tôi có các trang Wikipedia được truy xuất hàng đầu cho các truy vấn không được chú thích được gửi đến công cụ tìm kiếm Google bởi nhiều người dùng. Chúng tôi thu thập các đoạn văn trong những trang này làm kho dữ liệu, và sử dụng quy tắc để chú thích các gợi ý của những đoạn văn này.

Để so sánh công bằng với các mô hình dựa trên prompting trong cả 5-shot và 64-shot, chúng tôi chỉ sử dụng 5 exemplar "long answer"-câu hỏi được ghép đôi làm prompt để tạo ra câu hỏi tổng hợp cho các đoạn văn được lấy mẫu từ kho dữ liệu gợi ý-đoạn văn Wikipedia, và do đó xây dựng dữ liệu tinh chỉnh được ghép đôi câu hỏi-gợi ý-đoạn văn tổng hợp.

Chúng tôi huấn luyện PaLM trong kho dữ liệu được xây dựng trong 10,000 bước với kích thước batch là 64, mất khoảng 1 ngày trong 64 chip TPUv45. Mô hình được tinh chỉnh có thể được sử dụng cho ngâm thơ đa dạng hóa gợi ý đoạn văn mà không cần bất kỳ prompt nào khác.

D CÁC MÔ HÌNH NGÔN NGỮ ĐƯỢC HUẤN LUYỆN TRƯỚC
PaLM PaLM là một họ decoder được kích hoạt dày đặc được huấn luyện trên mục tiêu mô hình hóa ngôn ngữ. Nó có khả năng mạnh mẽ trong học trong ngữ cảnh few-shot, đa ngôn ngữ, cũng như các nhiệm vụ lập luận. Trong bài báo này, chúng tôi sử dụng mô hình PaLM với 62B tham số.

UL2 UL2 (Unifying Language Learning, Tay và cộng sự 2022a) là một mô hình encoder-decoder được huấn luyện trên một hỗn hợp các nhiệm vụ denoising trong một khung thống nhất. Trong bài báo này, vì chúng tôi chủ yếu tập trung vào khả năng học trong ngữ cảnh của các mô hình ngôn ngữ, chúng tôi sử dụng UL2-20B trong chế độ S-Denoiser (tức là, được huấn luyện trước với mô hình hóa ngôn ngữ tiền tố)6.

OPT OPT (Open Pre-trained Transformer language model, Zhang và cộng sự 2022) là một họ mô hình ngôn ngữ được kích hoạt dày đặc mã nguồn mở được phát hành gần đây nhằm tái tạo các kết quả tương đương như GPT-3 (Brown và cộng sự, 2020). Chúng tôi sử dụng 30B7trong bài báo này.

Codex Codex (Ouyang và cộng sự, 2022; Chen và cộng sự, 2021) là một biến thể của mô hình GPT-3 (Brown và cộng sự, 2020) có thể hiểu mã. Chúng tôi sử dụng OpenAI API8công khai để truy cập các đầu ra tạo sinh có điều kiện của mô hình code-davinvi-002 trong bài báo này.

5https://cloud.google.com/tpu/docs/v4-users-guide
6Điều này có thể đạt được bằng cách nối token "[NLG]" và "[extra id0]" vào đầu và cuối prompt.
7https://github.com/facebookresearch/metaseq
8https://openai.com/api/

15

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Hình 5: Nature Questions EM/F1 trên UL2-20B với số lượng shot khác nhau.

Hình 6: TriviaQA EM/F1 trên FLAN-T5 5-shot với các kích thước mô hình khác nhau.

Thuật toán 1 Phân tích Lỗi Từng Câu hỏi
Yêu cầu:N nhãn trả lời thực tế fAigN
i=1cho một câu hỏi duy nhất
Yêu cầu: các việc ngâm thơ fRigK
i=1và dự đoán trả lời fPigK
i=1từK đường
câu trả lời được chuẩn hóa fnormalize(Ai)gN
i=1
nếunormalize(majority vote(fPigK
i=1))2câu trả lời được chuẩn hóa thì
trả về "Hits@Majority "
nếu không nếu Any(fnormalize(Pi)2câu trả lời được chuẩn hóa gK
i=1)thì
trả về "Hits@20Path:"
nếu không nếu Any(fAny(fcâu trả lời được chuẩn hóa j2normalize(Ri)gK
i=1)gN
j=1)thì
trả về "Hits@20Recit:"
nếu không
trả về "NoRecit:"
kết thúc nếu

E PHÂN TÍCH VỀ SỐ LƯỢNG VÍ DỤ TRONG HỌC FEW-SHOT
Chúng tôi phân tích ảnh hưởng của số lượng shot trên tập dữ liệu Natural Questions cho mô hình prompting tiêu chuẩn và mô hình ngâm-và-trả-lời của chúng tôi trong Hình 5. Chúng tôi có thể thấy rằng prompting ngâm-và-trả-lời đạt được cải thiện nhất quán so với prompting tiêu chuẩn, trong khi cải thiện lớn nhất được đạt được trong thiết lập 1-shot.

F PHÂN TÍCH VỀ KÍCH THƯỚC MÔ HÌNH TRÊN CÁC MÔ HÌNH NGÔN NGỮ ĐƯỢC TINH CHỈNH THEO HƯỚNG DẪN
Các mô hình ngôn ngữ được tinh chỉnh theo hướng dẫn (Sanh và cộng sự, 2021; Bach và cộng sự, 2022; Wei và cộng sự, 2021; Chung và cộng sự, 2022) được huấn luyện trên một bộ sưu tập các tập dữ liệu được phát biểu như các hướng dẫn đã được chỉ ra là cải thiện

16

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Bảng 6: Đánh giá tính mạnh mẽ của UL2 trên TriviaQA với các exemplar few-shot khác nhau trên 5 seed ngẫu nhiên.
EM (5) F1(5)
Tiêu chuẩn (trực tiếp) 48.42 ( 0.71) 53.85 ( 0.57)
RECITE (5-đường) 49.75 ( 0.50) 54.78 ( 0.46)
RECITE (20-đường) 52.68 (0.62) 58.05 (0.58)

hiệu suất mô hình và khái quát hóa cho các nhiệm vụ chưa thấy. Chúng có xu hướng cho thấy hiệu suất zero-shot hoặc few-shot tốt hơn dưới cùng kích thước mô hình so với các đối tác được huấn luyện trước LM vanilla của chúng.

Chúng tôi phân tích hiệu suất trên một mô hình ngôn ngữ được tinh chỉnh theo hướng dẫn đại diện, FLAN-T5 (Chung và cộng sự, 2022), trên tập dữ liệu TriviaQA cho mô hình prompting tiêu chuẩn và mô hình ngâm-và-trả-lời của chúng tôi trong Hình 6. Chúng tôi có thể thấy rằng prompting ngâm-và-trả-lời đạt được cải thiện nhất quán so với prompting tiêu chuẩn, trong khi cải thiện lớn nhất được đạt được trong thiết lập "xxl (11B)" lớn nhất.

G SIÊU THAM SỐ MÔ HÌNH NGÔN NGỮ
Chúng tôi báo cáo các siêu tham số của LLMs mà chúng tôi đã sử dụng trong Bảng 7.

Bảng 7: Các siêu tham số mô hình của các mô hình ngôn ngữ lớn được sử dụng trong các thí nghiệm của chúng tôi. Lưu ý rằng đối với Codex (tức là, code-davinci-002 ), các chi tiết (bao gồm kích thước) không được biết, vì vậy chúng tôi báo cáo các siêu tham số của GPT-3 như một phỏng đoán giáo dục phổ biến.
Mô hình Loại # Lớp # Đầu dmodel # Tham số (B)
PaLM-62B decoder-only 64 32 8192 62.50
UL2-20B encoder-dencoder 32 / 32 16 4096 19.46
OPT-30B decoder-only 48 56 7168 29.97
Codexdecoder-only 96 96 12288 175.0

Câu hỏi: hoa kỳ đã thả bom nguyên tử ở đâu                                                                                                                                     
                                                                                                                                                                                           
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong trang Wikipedia, mục, 
và đoạn văn hoặc bảng sau:                                                                            
                                                                                                                                                                                           
Trả lời: Trong giai đoạn cuối của Thế chiến thứ hai, Hoa Kỳ đã kích nổ hai vũ khí hạt nhân 
trên các thành phố Hiroshima và Nagasaki của Nhật Bản vào ngày 6 và 9 tháng 8 năm 1945, 
tương ứng. Hoa Kỳ đã thả bom sau khi có được sự đồng ý của Vương quốc Anh, theo yêu cầu 
của Thỏa thuận Quebec. Hai vụ ném bom đã giết chết ít nhất 129,000 người, hầu hết trong số 
họ là dân thường. Chúng vẫn là việc sử dụng vũ khí hạt nhân duy nhất trong lịch sử chiến tranh.                                                                                                       
                                                                                                                                                                                       
Câu hỏi: ai bán nhiều dầu nhất trên thế giới                                                                                                                                              
                                                                                                                                                                                           
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong trang Wikipedia, mục, 
và đoạn văn hoặc bảng sau:                                                                            
                                                                                                                                                                                           
Trả lời: Ba nhà sản xuất hàng đầu trong lịch sử gần đây là (theo thứ tự bảng chữ cái) Nga, 
Saudi Arabia, và Hoa Kỳ. Mỗi quốc gia này đã trải qua sự suy giảm sản xuất lớn tại các thời điểm 
khác nhau trong quá khứ, nhưng kể từ năm 2014 cả ba đều đang sản xuất gần mức đỉnh của họ 
từ 9 đến 11 triệu thùng mỗi ngày. Saudi Arabia và Nga cũng đứng đầu danh sách các quốc gia 
xuất khẩu dầu, trong khi Nga không phải là thành viên của OPEC. Sản lượng dầu hàng tháng của 
Hoa Kỳ đã đạt 10.07 triệu thùng/ngày vào tháng 11 năm 2017, mức sản lượng dầu thô hàng tháng 
cao nhất trong lịch sử Hoa Kỳ. Vào đầu năm 2018, chính phủ Hoa Kỳ dự báo đất nước sẽ trở thành 
nhà xuất khẩu ròng dầu và khí đốt vào năm 2022, lần đầu tiên kể từ năm 1953. 

Hình 7: Hai exemplar ngâm thơ bằng chứng được lấy mẫu trong tập dữ liệu Natural Questions (NQ). Lưu ý rằng trong NQ, chúng tôi có thể trực tiếp sử dụng chú thích "long answer" làm các demonstration ngâm thơ.

17

--- TRANG 18 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Câu hỏi: khi nào tiết kiệm ánh sáng ban ngày bắt đầu ở mỹ 
Ngâm thơ: Ở Anh, Thời gian Tiết kiệm Ánh sáng Ban ngày đầu tiên được William Willett đề xuất lần đầu, một  
nhà xây dựng người Anh và chính trị gia Đảng Tự do, vào năm 1907, và sau một cuộc tham vấn công khai đề xuất của ông  
được chấp nhận vào năm 1908. Tiết kiệm Ánh sáng Ban ngày đầu tiên ở Anh được giới thiệu vào ngày 25  
tháng 3 năm 1916, và lần thứ hai vào ngày 27 tháng 3 năm sau đó. Nó bị bãi bỏ vào tháng 5 năm 1919,  
nhưng được giới thiệu lại vào tháng 5 năm 1919.                                                                                                                                                                      
                                                                                                                                                                                           
Ngâm thơ: Thời gian tiết kiệm ánh sáng ban ngày, còn được biết đến như tiết kiệm ánh sáng ban ngày, và một số biến thể khác,  
là thực hành điều chỉnh đồng hồ tiến lên một giờ trong những tháng mùa hè để ánh sáng buổi tối  
kéo dài thêm một giờ nữa. Thông thường, đồng hồ được điều chỉnh tiến lên một giờ gần đầu  
mùa xuân và được điều chỉnh lùi lại vào mùa thu. Về mặt lịch sử, việc thay đổi thời gian nhằm  
tiết kiệm năng lượng bằng cách tận dụng tốt hơn ánh sáng ban ngày có sẵn tại điểm xuân phân.                                                                                                                                     
                                                                                                                                                                                           
Ngâm thơ: Thời gian tiết kiệm ánh sáng ban ngày ở Hoa Kỳ là thực hành điều chỉnh đồng hồ tiến lên  
trong những tháng mùa hè để ánh sáng buổi tối kéo dài lâu hơn, trong khi hy sinh thời gian bình minh bình thường.  
Thực hành này cho phép mọi người có thêm ánh sáng buổi tối và ít ánh sáng buổi sáng hơn,  
điều này cho phép tăng năng suất và giải trí.                                                                 
                                                                                                                                                                                           
Ngâm thơ: Hoa Kỳ bắt đầu Thời gian Tiết kiệm Ánh sáng Ban ngày vào năm 1918, thực hiện  
một sự sắp xếp thời gian mùa hè toàn quốc, được đưa ra bởi  
Đạo luật Quốc phòng năm 1916. Ban đầu, Thời gian Tiết kiệm Ánh sáng Ban ngày được bắt đầu ở Hoa Kỳ để  
tiết kiệm năng lượng và giảm chi phí sinh hoạt. Hiện tại, nhiều quốc gia đã ngừng thực hành này,  
nhưng một số, như Hoa Kỳ, vẫn thực hiện nó trên cơ sở khu vực.. 

Hình 8: Bốn đoạn văn ngâm thơ được lấy mẫu từ UL2 cho cùng một câu hỏi ví dụ từ tập dữ liệu Natural Questions.

Câu hỏi : Do việc Rosa Parks từ chối nhường chỗ, hệ thống giao thông công cộng ở thành phố nào của Hoa Kỳ 
đã bị tàn phá bởi một cuộc tẩy chay xe buýt kéo dài một năm? 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong trang Wikipedia, mục, và đoạn văn sau: 
Trả lời : Vào ngày 1 tháng 12 năm 1955, tại Montgomery, Alabama, Parks đã từ chối lệnh của tài xế xe buýt James F. Blake về việc 
nhường một hàng bốn chỗ ngồi trong khu vực "người da màu" cho một hành khách da trắng, khi khu vực "da trắng" đã đầy. 

Câu hỏi : Giao hưởng cuối cùng của Beethoven là gì? 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong trang Wikipedia, mục, và đoạn văn sau: 
Trả lời : Năm 1823 chứng kiến sự hoàn thành của ba tác phẩm đáng chú ý, tất cả đều đã chiếm Beethoven trong 
một số năm, cụ thể là Missa solemnis, Giao hưởng số 9 và Diabelli Variations. 

Câu hỏi : Trong bộ phim John Boorman năm 1972 nào có một nhân vật chính, do Ned Beatty thủ vai, bị 'Hillbilly' hiếp dâm? 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong trang Wikipedia, mục, và đoạn văn sau: 
Trả lời : Năm 1972, Beatty ra mắt điện ảnh với vai Bobby Trippe trong Deliverance, có Jon Voight và Burt 
Reynolds, và được quay tại miền bắc Georgia. Nhân vật của Beatty bị buộc phải cởi trần trước họng súng bởi hai người đàn ông miền núi 
là những người làm nhục và hiếp dâm anh ta, một cảnh quá gây sốc đến nỗi nó vẫn được gọi là một cột mốc màn ảnh. 

Câu hỏi : Cây cầu nào bắc qua Sông Thames mà Nữ hoàng Elizabeth II đã khai trương vào ngày 17 tháng 3 năm 1973? 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong trang Wikipedia, mục, và đoạn văn sau: 
Trả lời : Nữ hoàng Elizabeth II của Vương quốc Anh khai trương cây cầu London mới. 

Câu hỏi : "Bài hát ""My Kind Of Town"", được viết bởi Sammy Cahn và Jimmy Van Heusen năm 1964, nói về 
thành phố nào?" 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong trang Wikipedia, mục, và đoạn văn sau: 
Trả lời : "My Kind of Town" xuất hiện nhỏ trên bảng xếp hạng pop Hoa Kỳ, đạt #110 năm 1964. Đây là 
bài hát thứ hai trong số hai bài hát trên bảng xếp hạng về Chicago được ghi âm bởi Sinatra. Bài kia là "Chicago (That Toddlin' 
Town)" từ năm 1957, đạt Hoa Kỳ #84. 

Hình 9: Prompt 5-shot mà chúng tôi đã sử dụng để thực hiện ngâm thơ bằng chứng trên tập dữ liệu TriviaQA.

18

--- TRANG 19 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Câu hỏi: Nguyên lý của nhà vật lý nào khẳng định rằng động lượng và vị trí của một hạt 
không thể được xác định chính xác cùng một lúc? 
Ngâm thơ: Năm 1935, Heisenberg (cùng với các trợ lý Walther Gerlach và Walther von  
Braun) đã xuất bản một bài báo trong đó họ mô tả một "nguyên lý không chắc chắn" mà sau này  
được gọi là nguyên lý bất định Heisenberg. Nguyên lý khẳng định rằng động lượng  
và vị trí của một hạt không thể được xác định chính xác cùng lúc. 
Ngâm thơ: Một trong những hệ quả của nguyên lý tương đối hẹp là vị trí  
và động lượng của một vật thể chuyển động không thể được đo đồng thời với độ chính xác cao. 
Ngâm thơ: Nguyên lý bất định phát biểu rằng không thể biết cả động lượng chính xác  
và vị trí chính xác của một vật thể (hạt) cùng một lúc, tức là, động lượng và vị trí  
không tương thích và, do đó, bổ sung. Nguyên lý bất định là một trong những kết quả cơ bản  
của cơ học lượng tử, và thường được coi là một trong những hệ quả đáng ngạc nhiên nhất của nó. 
Ngâm thơ: Nó phát biểu rằng hai phát biểu sau không thể cùng đúng: vị trí của một  
hạt không thể được xác định chính xác và biết chính xác, và động lượng của một  
hạt không thể được đo chính xác và biết chính xác. 

Hình 10: Bốn đoạn văn ngâm thơ được lấy mẫu từ UL2 cho cùng một câu hỏi ví dụ từ tập dữ liệu TriviaQA.

C: Do việc Rosa Parks từ chối nhường chỗ, hệ thống giao thông công cộng ở 
thành phố nào của Hoa Kỳ đã bị tàn phá bởi một cuộc tẩy chay xe buýt kéo dài một năm? 
T: Montgomery, Al 
C: Giao hưởng cuối cùng của Beethoven là gì? 
T: Thứ 9
C: Trong bộ phim John Boorman năm 1972 nào có một nhân vật chính, do Ned Beatty thủ vai, 
bị 'Hillbilly' hiếp dâm? 
T: Deliverance 
C: Cây cầu nào bắc qua Sông Thames mà Nữ hoàng Elizabeth II đã khai trương vào ngày 17 tháng 3 
năm 1973? 
T: Cầu London 
C: "Bài hát ""My Kind Of Town"", được viết bởi Sammy Cahn và Jimmy Van Heusen năm 
1964, nói về thành phố nào?" 
T: Chicago 

Hình 11: Prompt 5-shot mà chúng tôi đã sử dụng để thực hiện trả lời câu hỏi trên tập dữ liệu TriviaQA.

19

--- TRANG 20 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Câu hỏi:  Tạp chí nào được khởi động trước Arthur's Magazine hay First for Women? 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong hai trang Wikipedia, mục, và đoạn văn sau: 
Trả lời 1:  Tạp chí Arthur là một ấn phẩm hai tháng một lần được thành lập vào tháng 10 năm 2002, bởi nhà xuất bản Laris Kreslins và biên tập viên Jay 
Babcock. 
Trả lời 2: First for Women là một tạp chí phụ nữ được xuất bản bởi Bauer Media Group tại Hoa Kỳ. Tạp chí được khởi động vào năm 1989. 
Câu hỏi:  Gia đình Oberoi là một phần của công ty khách sạn có trụ sở chính ở thành phố nào? 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong hai trang Wikipedia, mục, và đoạn văn sau: 
Trả lời 1: P.R.S. Oberoi là chủ tịch hiện tại của The Oberoi Group. 
Trả lời 2:  The Oberoi Group là một tập đoàn khách sạn sang trọng đoạt giải với trụ sở chính ở New Delhi, Ấn Độ. 
Câu hỏi:  Quốc tịch của vợ James Henry Miller là gì? 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong hai trang Wikipedia, mục, và đoạn văn sau: 
Trả lời 1: Năm 1967, Miller kết hôn với người vợ thứ năm, ca sĩ gốc Nhật Hoki Tokuda. 
Trả lời 2: Hoki Tokuda là một nữ diễn viên Nhật Bản, được biết đến với Blind Woman's Curse (1970), The Abalone Girls (1965) và Nippon Paradise 
(1964). 
Câu hỏi:  Loạt phim truyền hình Hà Lan-Bỉ mà "House of Anubis" được dựa trên lên sóng lần đầu vào năm nào? 
Câu trả lời cho câu hỏi trên có thể được tìm thấy trong hai trang Wikipedia, mục, và đoạn văn sau: 
Trả lời 1: House of Anubis là một loạt phim truyền hình bí ẩn được phát triển cho Nickelodeon dựa trên loạt phim truyền hình Hà Lan–Bỉ Het 
Huis Anubis. 
Trả lời 2: Het Huis Anubis (tiếng Anh: The House of Anubis) là một bộ phim truyền hình bí ẩn cho trẻ em Hà Lan-Bỉ. Nó lên sóng lần đầu vào 
tháng 9 năm 2006. 

Hình 12: Prompt 4-shot mà chúng tôi đã sử dụng để thực hiện ngâm thơ bằng chứng nhiều lần trên tập dữ liệu HotpotQA.

C: Tạp chí nào được khởi động trước Arthur's Magazine hay First for Women? 
T: Câu trả lời là Arthur's Magazine. 
C: Gia đình Oberoi là một phần của công ty khách sạn có trụ sở chính ở thành phố nào? 
T: Câu trả lời là Delhi. 
C: Quốc tịch của vợ James Henry Miller là gì? 
T: Câu trả lời là Mỹ. 
C: Loạt phim truyền hình Hà Lan-Bỉ mà "House of Anubis" được dựa trên lên sóng lần đầu 
vào năm nào? 
T: Câu trả lời là  2006. 

Hình 13: Prompt 4-shot mà chúng tôi đã sử dụng để thực hiện trả lời câu hỏi trên tập dữ liệu HotpotQA.

20

--- TRANG 21 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Câu hỏi: Local H và For Against có cùng đến từ Hoa Kỳ không? 
Trả lời 1: Local H là một ban nhạc Mỹ từ San Diego, California.                                                                                                                                  
                                                                                                                                                                                           
Trả lời 2: For Against là một ban nhạc punk Mỹ từ Los Angeles, California.                                                                                                                       
                                                                                                                                                                                           
                                                                                                                                                                                           
Trả lời 1: Local H là một ban nhạc rock từ Los Angeles, California.                                                                                                                                     
                                                                                                                                                                                           
Trả lời 2: For Against là một ban nhạc rock Mỹ từ Phoenix, Arizona.                                                                                                                              
                                                                                                                                                                                           
                                                                                                                                                                                           
Trả lời 1: Local H là một ban nhạc rock Mỹ từ Austin, Texas.                                                                                                                                     
                                                                                                                                                                                           
Trả lời 2: For Against là một ban nhạc rock từ Nashville, Tennessee.                                                                                                                                    
                                                                                                                                                                                           
                                                                                                                                                                                           
Trả lời 1: Local H là một ban nhạc rock Mỹ từ Chicago, Illinois.                                                                                                                                 
                                                                                                                                                                                           
Trả lời 2: For Against là một ban nhạc indie rock từ New York City, New York.                                                                                                                          

Hình 14: Bốn việc ngâm thơ được lấy mẫu từ UL2 cho cùng một câu hỏi ví dụ từ tập dữ liệu HotpotQA.

C: Tạp chí nào được khởi động trước Arthur's Magazine hay First for Women? 
T: Arthur's Magazine bắt đầu năm 1844. First for Women bắt đầu năm 1989. Vậy Arthur's 
Magazine được khởi động trước. Câu trả lời là Arthur's Magazine. 
C: Gia đình Oberoi là một phần của công ty khách sạn có trụ sở chính ở thành phố nào? 
T: Gia đình Oberoi là một phần của công ty khách sạn gọi là The Oberoi Group. The Oberoi 
Group có trụ sở chính ở Delhi. Câu trả lời là Delhi. 
C: Quốc tịch của vợ James Henry Miller là gì? 
T: Vợ của James Henry Miller là June Miller. June Miller là người Mỹ. Câu trả lời là 
Mỹ. 
C: Loạt phim truyền hình Hà Lan-Bỉ mà "House of Anubis" được dựa trên lên sóng lần đầu 
vào năm nào? 
T: "House of Anubis" được dựa trên loạt phim truyền hình Hà Lan–Bỉ Het Huis Anubis. Het 
Huis Anubis lên sóng lần đầu vào tháng 9 năm 2006. Câu trả lời là 2006. 

Hình 15: Prompt 4-shot mà chúng tôi đã sử dụng cho baseline Chuỗi Suy nghĩ (Wei và cộng sự, 2022) trên tập dữ liệu HotpotQA. Prompt được lấy từ (Wang và cộng sự, 2022a).

21

--- TRANG 22 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Câu hỏi: Scott Derrickson và Ed Wood có cùng quốc tịch không? 
Trả lời Thực tế: Có
------------------------------------------------------------------ 
Trả lời (prompting tiêu chuẩn): Mỹ 
------------------------------------------------------------------ 
Trả lời (chuỗi suy nghĩ đường 1): Scott Derrickson là người Mỹ. Ed Wood cũng là người Mỹ. Câu trả lời là cả hai đều là 
người Mỹ .
Trả lời (chuỗi suy nghĩ đường 2): Scott Derrickson là người Mỹ. Ed Wood là người Mỹ. Vậy Scott Derrickson và Ed 
Wood có cùng quốc tịch. Câu trả lời là có.
Trả lời (chuỗi suy nghĩ đường 3): Scott Derrickson và Ed Wood có cùng quốc tịch. Cả Scott Derrickson và 
Ed Wood đều là người Mỹ. Câu trả lời là Có.
------------------------------------------------------------------ 
Trả lời (ngâm-và-trả-lời đường 1): 
Trả lời 1: Scott Derrickson là một nhà biên kịch, đạo diễn và nhà sản xuất người Mỹ. 
Trả lời 2: Edward D. Wood Jr. (10 tháng 10, 1924 – 10 tháng 12, 1978) là một nhà làm phim, diễn viên và tác giả người Mỹ. 
Câu trả lời là Mỹ .
Trả lời (ngâm-và-trả-lời đường 2): 
Trả lời 1: Scott Derrickson là một đạo diễn phim, nhà biên kịch và nhà sản xuất người Mỹ. 
Trả lời 2: Edward Davis Wood, Jr. (10 tháng 10, 1924 – 10 tháng 12, 1978) là một nhà làm phim, diễn viên, nhà văn, 
nhà sản xuất và đạo diễn người Mỹ. 
Câu trả lời là có.
Trả lời (ngâm-và-trả-lời đường 3): 
Trả lời 1: Scott Derrickson (sinh ngày 16 tháng 7, 1966) là một đạo diễn phim, nhà biên kịch và nhà sản xuất người Mỹ. 
Trả lời 2: Edward Davis Wood Jr. (10 tháng 10, 1924 – 10 tháng 12, 1978) là một nhà làm phim, diễn viên, nhà văn và 
nhà sản xuất người Mỹ. 
Câu trả lời là có.

Hình 16: So sánh định tính giữa prompting tiêu chuẩn, chuỗi suy nghĩ, và ngâm-và-trả-lời trên ví dụ đánh giá HotpotQA (I).

22

--- TRANG 23 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
Câu hỏi: Đạo diễn của bộ phim hài lãng mạn "Big Stone Gap" có trụ sở ở thành phố New York nào? 
Trả lời Thực tế: Greenwich Village, New York City 
------------------------------------------------------------------ 
Trả lời (prompting tiêu chuẩn): New York City 
------------------------------------------------------------------ 
Trả lời (chuỗi suy nghĩ đường 1): Adriana Trigiani là đạo diễn của bộ phim hài lãng mạn Big Stone Gap. 
Adriana Trigiani có trụ sở ở New York City. Câu trả lời là New York City .
Trả lời (chuỗi suy nghĩ đường 2): Đạo diễn của bộ phim hài lãng mạn "Big Stone Gap" là Adriana 
Trigiani. Adriana Trigiani có trụ sở ở New York City. Câu trả lời là New York City .
Trả lời (chuỗi suy nghĩ đường 3):  Đạo diễn của "Big Stone Gap" là Adriana Trigiani. Adriana Trigiani 
sống ở Manhattan. Câu trả lời là Manhattan .
------------------------------------------------------------------ 
Trả lời (ngâm-và-trả-lời đường 1): 
Trả lời 1: Big Stone Gap là một bộ phim hài lãng mạn Mỹ năm 2014 do Adriana Trigiani đạo diễn. 
Trả lời 2: Adriana Trigiani là một tác giả, nhà viết kịch, nhà làm phim và doanh nhân người Mỹ. Cô sống ở 
Greenwich Village, New York City. 
Câu trả lời là Greenwich Village .
Trả lời (ngâm-và-trả-lời đường 2): 
Trả lời 1: Big Stone Gap là một bộ phim hài lãng mạn Mỹ do Adriana Trigiani đạo diễn và được viết bởi 
Trigiani và Andrea Berloff. 
Trả lời 2: Adriana Trigiani là một tác giả bán chạy nhất, nhà làm phim và nhà viết kịch người Mỹ. Cô sống ở 
Greenwich Village, New York. 
Câu trả lời là Greenwich Village .
Trả lời (ngâm-và-trả-lời đường 3): 
Trả lời 1: Adriana Trigiani là một nhà văn và nhà làm phim người Mỹ. 
Trả lời 2: Big Stone Gap là một bộ phim hài lãng mạn Mỹ năm 2014 được viết và đạo diễn bởi Adriana 
Trigiani, dựa trên tiểu thuyết đầu tay năm 2000 cùng tên của cô. 
Câu trả lời là New York City .

Hình 17: So sánh định tính giữa prompting tiêu chuẩn, chuỗi suy nghĩ, và ngâm-và-trả-lời trên ví dụ đánh giá HotpotQA (II).

23
