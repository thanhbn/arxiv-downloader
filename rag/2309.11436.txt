# 2309.11436.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2309.11436.pdf
# File size: 6114916 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
You Only Look at Screens: Multimodal Chain-of-Action Agents
Zhuosheng Zhang1*, Aston Zhang2‚àó
1School of Electronic Information and Electrical Engineering,
Shanghai Jiao Tong University
2GenAI, Meta
zhangzs@sjtu.edu.cn, az@astonzhang.com
Abstract
Autonomous graphical user interface (GUI)
agents aim to facilitate task automation by in-
teracting with the user interface without man-
ual intervention. Recent studies have inves-
tigated eliciting the capabilities of large lan-
guage models (LLMs) for effective engage-
ment in diverse environments. To align with
the input-output requirement of LLMs, most
existing approaches are developed under a sand-
box setting where they rely on external tools
and application-specific APIs to parse the en-
vironment into textual elements and interpret
the predicted actions. Consequently, those ap-
proaches often grapple with inference ineffi-
ciency and error propagation risks. To miti-
gate the challenges, we introduce Auto-GUI,
a multimodal solution that directly interacts
with the interface, bypassing the need for en-
vironment parsing or reliance on application-
dependent APIs. Moreover, we propose a chain-
of-action technique‚Äîleveraging a series of in-
termediate previous action histories and future
action plans‚Äîto help the agent decide what ac-
tion to execute. We evaluate our approach on
a new device-control benchmark AITW with
30Kunique instructions, spanning multi-step
tasks such as application operation, web search-
ing, and web shopping. Experimental results
show that Auto-GUI achieves state-of-the-art
performance with an action type prediction ac-
curacy of 90% and an overall action success
rate of 74%. Code is publicly available at
https://github.com/cooelf/Auto-GUI .
1 Introduction
Building intelligent autonomous agents that are ca-
pable of task planning, decision making, and action
execution in a particular environment is a long-
standing goal of artificial intelligence (AI) (Searle,
1969; Wooldridge and Jennings, 1995; Maes, 1995;
Hendler, 1999). The advent of large language mod-
els (LLMs) (Brown et al., 2020; Chowdhery et al.,
*Work done at Amazon Web Services.2022; OpenAI, 2023) has flourished promising op-
portunities for developing autonomous agents to
assist users in completing tasks in distinct environ-
ments such as operation systems, specific appli-
cations, and web browsers (Adept, 2022; Rawles
et al., 2023; Liu et al., 2023; Zhou et al., 2023;
Wang et al., 2023c; Koh et al., 2024; Gao et al.,
2023; Yang et al., 2023).
Recent studies have explored prompt engineer-
ing (Richards, 2023; Nakajima, 2023; Reworkd,
2023; Sumers et al., 2023; Liu et al., 2023) and
fine-tuning techniques (Rawles et al., 2023; Wen
et al., 2023; Sun et al., 2022) to elicit the capability
of language models to execute actions in interac-
tive environments. However, there are at least two
major challenges that have limited real-world ap-
plications of autonomous agents.
First, existing approaches commonly rely on
external tools such as optical character recogni-
tion (OCR) and icon detectors (Zhang et al., 2021;
Sunkara et al., 2022) to parse the environment into
textual elements (e.g., HTML layouts) as inputs
to a language model (Figure 1(a)) (Rawles et al.,
2023). On the one hand, the parsed elements gen-
erate lengthy inputs, thus leading to inference in-
efficiency. Since computational latency is a key
measure in deployment, using lengthy inputs would
increase inference cost and may even exceed the
input length limit of the language model. On the
other hand, parsing the visual environment into
textual elements may also be prone to error propa-
gation or information loss because parsing mistakes
are inevitable using external tools.
Second, most existing approaches are under the
sand-box setting that requires accessing internal
APIs to interact with the environment (Zhou et al.,
2023; Gur et al., 2023), e.g., using a JavaScript
element selection on a webpage or a Python inter-
preter to execute actions. However in practice, the
API interface is often inaccessible in third-party
applications (Apps).arXiv:2309.11436v4  [cs.CL]  7 Jun 2024

--- PAGE 2 ---
Screen Parsing:OCR,Icon Detection,HTML ConversionLanguage ModelApplication-specificAPI Calls<imgid=0class=ICON_HOMEalt="HomeIcon"></img><imgid=1class=ICON_ARROW_UPWARDalt="Arrow_UpwardIcon"></img><pid=2class="text"alt="lowes.com/search?searchT">lowes.com/search?searchT</p>‚Ä¶ ‚Ä¶<imgid=48class=ICON_NAV_BAR_CIRCLEalt="Nav_BarCircle"></img><imgid=49class=ICON_NAV_BAR_RECTalt="NAV_BarRect"></img>click [29](JavaScript)
PlanningActionMemoryAction PredictionGoal: Look up the best rated coffee maker on Lowe‚ÄôsMultimodalAgentaction_type: [DUAL_POINT],touch_point: [0.5595, 0.6261], lift_point:[0.5595, 0.6261]typed_text: ‚Äù‚ÄùGoal: Look up the best rated coffee maker on Lowe‚Äôs
(a) Sandbox Paradigm(b) First Principles Thinking ParadigmFigure 1: Comparison of GUI agent paradigms. The sandbox paradigm depends on the intermediate transformation
between environments and agents, i.e., needing access to intermediate environment parsing or interval application-
dependent APIs. In contrast, our first principles thinking paradigm allows direct interactions on the screen without
intermediate transformation. Details of the action types and action points are presented in Section 3.3.
These challenges have motivated more advanced
techniques that are capable of first principles think-
ing(Aristotle; Irwin, 1989)‚Äîallowing direct in-
teractions on the screen without needing access
to intermediate environment parsing or interval
application-dependent APIs (Figure 1(b)). To ad-
dress the challenges, we introduce Auto-GUI , a
multimodal approach that directly interacts with
the graphical user interfaces (GUIs). To further
strengthen the agent‚Äôs action prediction capabil-
ity, we propose a novel chain-of-action technique,
where a chain of action is a series of intermediate
previous action histories and future action plans.
We evaluate Auto-GUI on a new device-control
benchmark AITW (Rawles et al., 2023) with 30 K
unique instructions, spanning multi-step tasks of
application operation, web searching, and web
shopping. Experimental results show that Auto-
GUI achieves state-of-the-art performance with an
action type prediction accuracy of 90% and an ac-
tion success rate of 74%.
In summary, our work makes the following tech-
nical contributions:
(i) We introduce Auto-GUI, a multimodal agent
for autonomous GUI control that can directly inter-
act with the screens, thus circumventing the con-
straints of environment parsing and application-
specific API access.
(ii) We propose a chain-of-action technique that
leverages the previously executed actions and fu-
ture action plans to help the agent decide what
action to execute at each step.
(iii) Auto-GUI achieves state-of-the-art perfor-mance with an action type prediction accuracy of
90% and an action success rate of 74%. Notably,
Auto-GUI is able to infer an action in less than one
second.
2 Related Work
Our work falls into the field of language agents.
This section will first review the recent progress
in building language agents and then discuss the
approaches to conduct user interface control with
language agents.
2.1 Language Agents
Language agents refer to those agents that can
follow user instructions and interact with environ-
ments to complete tasks. Such agents expand the
landscape of language models to compete in spe-
cific fields, including application operation, web
searching, and web shopping. There are two pop-
ular types of language agents, autonomous agents
and communicative agents. Autonomous agents
aim to assist humans to achieve specific goals in
the real world. Typical examples of autonomous
agents are AutoGPT (Richards, 2023), BabyAGI
(Nakajima, 2023), and AgentGPT (Reworkd, 2023).
In contrast, communicative agents are personalized
and socialized agents (Park et al., 2023; Wang et al.,
2023b; Zhu et al., 2023; Hong et al., 2023a) with
human behaviors that can communicate and collab-
orate with each other. They are often deployed in
immersive environments.
Inspired by the potential in real-world applica-
tions, this work focuses on autonomous agents, es-

--- PAGE 3 ---
Chain of Previous Action Histories: action_type: type, touch_point: [-1.0, -1.0], lift_point: [-1.0, -1.0], typed_text: ‚Äùbest rated coffee maker‚Äùaction_type: dual_point, touch_point: [0.2, 0.5],  lift_point: [0.8, 0.5], typed_text: ‚Äù‚ÄùAction Plan:[DUAL_POINT, STATUS_TASK_COMPLETE]Language EncoderImage EncoderSelf AttentionDecoder
ScreenProjectionFeedforward
Chain of Future Action PlansCurrent Action PredictionAction Decision:action_type: [DUAL_POINT],touch_point: [0.5595, 0.6261], lift_point: [0.5595, 0.6261],typed_text: ‚Äú‚ÄùGoal: Look up the best rated coffee maker on Lowe‚ÄôsChain of Actions
ùëå!"#$%&ùëã'%!(
ùëå)(!&ùëã*$+#%,-ùëã(!&'.!'/ùëã+",//&
ActionFigure 2: Model architecture of Auto-GUI. A chain of action consists of a chain of previous action histories Xhistory
and a chain of future action plans Yplanin the illustration.
pecially those working in mobile devices. We aim
to assist users by completing multi-step tasks (e.g.,
manipulating Apps, web shopping, and question an-
swering) without any manual intervention. Given
a user instruction in natural language, the agent is
required to interpret the instruction and execute ac-
tions by directly controlling its user interface. Due
to the requirement in real-world applications, the
agent is expected to be both effective and efficient.
2.2 GUI Control with Natural Language
Recently, LLMs have shown promise in build-
ing autonomous GUI agents with abilities of in-
struction following (Sanh et al., 2022; Taori et al.,
2023b; Chiang et al., 2023) and chain-of-thought
(CoT) prompting (Nye et al., 2022; Wei et al.,
2022). Especially, CoT prompting (Wei et al.,
2022; Kojima et al., 2022; Zhang et al., 2023a)
elicit LLMs‚Äô capacities of step-by-step planning,
decision making, and action execution. Those ca-
pacities have been shown to be effective in GUI
control tasks (Rawles et al., 2023).
However, the task environments are GUIs in-
stead of natural language that LLMs can process
directly. Therefore, the GUI states and actions are
required to be converted to textual formats to be
applicable to LLMs. For example, it is feasible
to parse the GUI screens by icon recognition and
OCR (Zhang et al., 2021; Sunkara et al., 2022;
Song et al., 2023) and organize the parsed elements
into HTML layouts. As a compromise, existing ap-
proaches are restricted in a sandbox setting where
they rely on external tools (Rawles et al., 2023;
Wen et al., 2023) and application-specific APIs(Zhou et al., 2023; Gur et al., 2023) for environ-
ment parsing and action interpretation; thus, com-
monly suffer from inference inefficiency and error
propagation. Although there are studies that have
considered multimodal architecture for processing
inputs in different modalities (Sun et al., 2022; Yan
et al., 2023), those studies still rely on fine-grained
environment parsing to ensure competitive perfor-
mance. As GUI tasks have shown prerequisites to
fine-grained grounding to the GUI contents, more
recent concurrent studies (Cheng et al., 2024; Hong
et al., 2023b) have explored GUI grounding pre-
training to improve the agent‚Äôs performance.
In contrast to the studies above, this work is
established upon first principles thinking, which
directly reads GUI without additional environment
parsing and provides the action (e.g., action type,
gesture coordinate, and typed text) that can be ef-
ficiently executed without needing any extra APIs.
Concretely, our approach diverges from existing
studies in several key aspects.
(i) Conceptually, Auto-GUI represents a novel
approach as a multimodal agent designed for au-
tonomous GUI control, enabling direct interaction
with screens. This approach effectively bypasses
the need for environment parsing and access to
application-specific APIs, thus enhancing adapt-
ability and versatility.
(ii) Methodologically, our proposal introduces a
chain-of-action technique. This method capitalizes
on previously executed actions and future action
plans to inform the decision-making process of the
agent at each step.
(iii) As a scientific contribution, our paper ar-

--- PAGE 4 ---
ticulates that a unified multimodal model out of
first principles thinking can serve as a strong au-
tonomous agent evidenced by state-of-the-art per-
formance. In addition, we facilitate a series of stud-
ies to understand the contributing factors regarding
the vision features and model scale.
3 Methodology
In this section, we will first introduce the basic
concepts for the GUI control task and then describe
the design of our proposed Auto-GUI framework.
3.1 Problem Formalization
Given a user instruction (also known as a goal),
the agent needs to complete the task with multiple
steps of interactions. The entire process is called an
episode , which is composed of a series of screens .
For each step in the episode, the agent will be pro-
vided with a screenshot, and the agent is required
to predict the action until the task is complete. De-
tailed examples can be found in Appendix A.1.
3.2 Framework Overview
Auto-GUI is a multimodal agent that decides what
action to take given the input screenshot and a user
instruction. To empower the agent‚Äôs decision mak-
ing capability, we introduce a chain-of-action ap-
proach by leveraging a series of intermediate pre-
vious action histories and future action plans to
predict actions.
The model architecture of Auto-GUI is illus-
trated in Figure 2. On a high level, Auto-GUI con-
sists of three stages. First, we acquire encoded fea-
tures from both vision and language inputs. Specifi-
cally, the vision input, i.e., a screenshot, is encoded
by a frozen vision encoder. Meanwhile, the lan-
guage input, consisting of the goal and a chain of
previous action histories‚Äîeach history contains a
tuple {action type, touch point, lift point, and typed
text}, is encoded by a language encoder. Second,
the encoded vision and language representations
are integrated by a self-attention module. Third,
the fused representation is fed to the decoder to
generate a chain of future action plans (i.e., action
types to execute in future steps) followed by action
prediction. A chain of action consists of two parts
in the procedure above: a chain of previous action
histories on the input side and a chain of future
action plans on the output side. In the following,
we describe the entire procedure in detail.Encoding Suppose that an episode consists of k
steps of interactions. Given a screenshot Xscreen‚àà
Rh√ów√ó3with height hand width wat step t‚àà
[1, k], we first feed it to a frozen image encoder
(e.g., BLIP-2 (Li et al., 2023)) and extract vision
features Hscreen‚ààR1√ódswhere dsis the dimension
of the vision features. Additionally, we leverage a
language encoder to extract the language features
Hlanguage ‚ààRn√ódlof the input goal Xgoalwhere
nis the number of tokens and dlis the dimension
of the language features. If t >1, there will be
a chain-of-action history already executed before
stept. We denote the chain of action histories as
Xhistory = [m1, . . . , m t]where micontains a tuple
of action type, touch point, lift point, and typed
text.1Otherwise, if t= 1,Xhistory will be set
empty:
Xhistory =(
[m1, . . . , m t],ift >1
<empty> , otherwise(1)
We concatenate XgoalandXhistory as the in-
put to the language encoder: Xlanguage =
{Xgoal, Xhistory}.
Then, we obtain the encoded representations of
the vision and language inputs as follows:
Hscreen =VisionExtractor (Xscreen), (2)
H‚Ä≤
screen =WH screen, (3)
Hlanguage =LanguageEncoder (Xlanguage ),(4)
where Wis a trainable projection matrix to convert
Hscreen into the same dimensionality as Hlanguage .
Interaction We correlate H‚Ä≤
screen andHlanguage
with a single-head self-attention network (Vaswani
et al., 2017),2where the query ( Q), key ( K), and
value ( V) areHlanguage ,H‚Ä≤
screen , andH‚Ä≤
screen , respec-
tively. The attention output Hattn
screen‚ààRn√ódis de-
fined as: Hattn
screen =Softmax (QK‚ä§
‚àödk)V, where dkis
the same as the dimension of Hlanguage because a
single head is used.
Then, a gated fusion (Zhang et al., 2020; Wu
et al., 2021; Zhang et al., 2023b) is adopted to fuse
1Following the design philosophy of AITW (Rawles et al.,
2023), the agent is presumed to execute a sequence of actions
until the task is completed. For practical use, it is feasible to
halt the agent using straightforward rules when required.
2It is possible to use a unified multimodal Transformer
architecture (Bavishi et al., 2023). Here we opt for a single-
layer interaction to maintain the lightweight nature of our
method, with parameters less than 1 billion. This design
choice also ensures flexibility to accommodate new modalities
in the future, such as integrating additional feature extractors.

--- PAGE 5 ---
Hlanguage andHattn
screen . We have the fused output
Hfuse‚ààRn√ódby:
Œª=Sigmoid (WlHlanguage +WvHattn
vision),(5)
Hfuse= (1‚àíŒª)¬∑Hlanguage +Œª¬∑Hattn
vision,(6)
where WlandWvare learnable parameters.
Decoding The fused representation Hfuse
is fed to a Transformer decoder to generate
the target predictions in a string format. The
target predictions consist of a chain of future
action plans Yplanand the current action pre-
diction Yaction separated by specific prompts:
{Action Plan: Yplan, Action Decision: Yaction}.
Concretely, Yplanis a chain of action types to
execute in future steps: Yplan= [action_type t,
. . ., action_type k].Yaction contains four compo-
nents: Yaction = {‚Äúaction_type‚Äù: <action_type>,
‚Äútouch_point‚Äù: <touch_point>, ‚Äúlift_point‚Äù:
<lift_point>, ‚Äútyped_text‚Äù: <typed_text>}. These
four components will be explained as follows.
3.3 Coordinate Normalization
Recall that a target action consists of four com-
ponents: action type, touch point, lift point, and
typed text. We consider six action types: dual-
point gesture ,type,go_back ,go_home ,enter ,
andstatus_complete . A dual-point gesture com-
prises a touch point and a lift point with [y, x]
coordinates.3The gesture actions ensure a flex-
ible action space and can represent clicks and
scrolls at arbitrary locations. For example, a
gesture action {‚Äútouch_point‚Äù: [0.7761, 0.7089],
‚Äúlift_point‚Äù: [0.7761, 0.7089]} means clicking at
the coordinate [0.7761, 0.7089], while a gesture ac-
tion {‚Äútouch_point‚Äù: [0.1898, 0.4477], ‚Äúlift_point‚Äù:
[0.8242, 0.4077]} means scrolling down. A
type action means typing a text and the text is
placed in the <typed_text> field. The other ac-
tion types, i.e., go_back, go_home, enter, and
status_complete are system actions, whose corre-
sponding <touch_point>, <lift_point> fields are
filled with -1, and the <typed_text> is empty.
We observe that high-precision coordinates are
not necessary for representing a click or scroll ac-
tion. Therefore, we apply normalized values of the
coordinates, which helps accelerate convergence
and mitigate the ambiguity of coordinates. The
normalization is applied to click and scroll actions.
For click actions, we keep four decimal places. For
3We follow the [y, x]coordinate format in AITW.scroll actions, we first determine the scroll direction
with the touch and lift points. Then, we transform
the touch and lift points into fixed directional co-
ordinates as follows: ‚Äúup‚Äù: {[0.8, 0.5], [0.2, 0.5]},
‚Äúdown‚Äù: {[0.2, 0.5], [0.8, 0.5]}, ‚Äúleft‚Äù: {[0.5, 0.8],
[0.5, 0.2]}, ‚Äúright‚Äù: {[0.5, 0.2], [0.5, 0.8]}, where
{[¬∑], [¬∑]} consists of the touch point and lift point in
the first [ ¬∑] and second [ ¬∑].4We provide examples
of target actions in Appendix B.1.
4 Experiments
4.1 Dataset
We use the AITW benchmark dataset (Rawles et al.,
2023). AITW is a large-scale GUI control bench-
mark dataset containing natural language instruc-
tions, screenshots, and actions. There are 715 K
episodes spanning 30 Kunique instructions, cov-
ering diverse multi-step tasks such as application
operation, web searching, and web shopping, on
over 350 Apps and websites. This dataset covers
various device types and operation systems in vary-
ing screen resolutions to ensure generality. There
are five subsets in the benchmark dataset, namely,
General, Install, GoogleApps, Single, and Web-
Shopping.
Dataset Episodes Screens Instructions
General 9,476 85,413 545
Install 25,760 250,058 688
GoogleApps 625,542 4,903,601 306
Single 26,303 85,668 15,366
WebShopping 28,061 365,253 13,473
Table 1: Dataset statistics.
Table 1 presents the data statistics of the AITW
dataset. Each subset is split episode-wise into a
training, validation, and test set (80/10/10%). More
details of the subsets can be found in Appendix A.2.
4.2 Baselines
We adopt three types of baselines, allowing for
a comprehensive comparison with our approach.
The baselines encompass the in-context earning
(ICL) and fine-tuning paradigms. They are based
on various backbone models of different sizes.
(i) In-context Learning LLMs. Few-shot PaLM
2, ChatGPT (turbo-3.5) are adopted. Following
prior studies (Rawles et al., 2023; Wang et al.,
2023a), we feed the LLM a textual description
4The numbers are relative scales and they generalize to
different screen sizes.

--- PAGE 6 ---
of the screen and a user instruction. The screen
is formatted as an HTML syntax, providing the
information of GUI elements derived from OCR
detection and icon detection from external tools
(Rawles et al., 2023). The model is required to
predict an action among pre-defined actions. In
addition, we report the results of the multimodal
GPT-4V by taking the vision image and action his-
tory as the input based on Yan et al. (2023).
(ii) Fine-tuned LLMs. We adopt Llama-2-7B
(Touvron et al., 2023) as the baseline and fine-tune
it with LoRA. We feed the model with the user
instruction and the screen descriptions in HTML
syntax (the same as in-context learning LLMs).
The model is expected to predict the action in the
same output format as in-context learning LLMs.
(iii) Specialized GUI Agent. We adopted the
Behavioural Cloning (BC) agent, which reported
the state-of-the-art performance in Rawles et al.
(2023). BC is a Transformer-based architecture
that takes a task instruction, the current screen,
and a stacked history of screen observations and
actions as input. All the embedded representations
are fused to predict the action by a decoder. There
are two BC variants, BC-single and BC-history,
depending on whether the model takes the screen-
action history as input.
More detailed implementation of the baselines
can be found in Appendix B.2.
4.3 Evaluation Measures
We compute the screen-wise action matching score
as the main evaluation measure, defined as the num-
ber of correct actions divided by the episode length
(i.e., the number of screenshots). A predicted ac-
tion is considered correct if the action type and
dual-point gesture match the gold ones. As we
described in Section 3.3, the gesture actions can
represent the click actions and scroll actions at ar-
bitrary locations. Following Rawles et al. (2023), a
click action is considered correct if its touch point
and lift point fall within a 14% screen distance
from the gold gestures or occur within the same
detected bounding box with the gold gestures. A
scroll action is considered correct if it has the same
scroll axis as the gold gesture.
The screen-wise action matching score has been
shown to correlate with the task complete score es-
timated by human evaluations (Rawles et al., 2023)
and is appropriate to measure the action success
rate for user instructions. Besides the overall match-
ing score, we will also compare the click regionaccuracy, scroll direction accuracy, action type ac-
curacy (e.g., clicking, scrolling, typing, etc), and
typed text accuracy for a more comprehensive ref-
erence (Section 5.1).
The evaluation criteria above apply to the BC
baselines and our Auto-GUI. For the LLMs, they
can only click on detected GUI elements, rather
than clicking at arbitrary locations. Therefore, we
consider if the clicked GUI element is matched
for click actions instead of comparing dual-point
gestures for LLMs.
4.4 Implementation Details
We adopt the encoder-decoder architecture (Raf-
fel et al., 2020) under small (60M), base (200M)
and large (700M) settings in our framework.
We apply FLAN-Alpaca to initialize our model
weights.5The vision features are obtained by the
frozen BLIP-2 encoder (Li et al., 2023) (version:
blip2_t5_instruct). We fine-tune the models up to
10 epochs, with a learning rate of 1e-4. The max-
imum input sequence length is 512. The batch
size is 4. Our experiments are run on 8 NVIDIA
Tesla V100 32G GPUs. Training the large and base
models takes 75 and 25 hours, respectively.
We develop two kinds of approaches to ana-
lyze their generalization abilities, namely Auto-
GUI separate , and Auto-GUI unified . Specifically, Auto-
GUI separate is trained and evaluated independently
on each subset. Auto-GUI unified is a unified model
trained on the training sets of each subset and eval-
uated on each test set. As the GoogleApps subset
is 10-100 times larger than the other subsets, using
all the training data to train a unified model would
suffer from the data imbalance issue (Zhang et al.,
2022). Therefore, we only use 10% training data
of GoogleApps. At the same time, the overall com-
putation cost can also be saved by 80%. We use
Auto-GUI unified as the default model for analysis
unless otherwise stated.
4.5 Main Results
Table 2 shows the main results. Based on the re-
sults, we have the following observations.
(i) Auto-GUI unified achieves the best overall per-
formance compared with all the baselines. Com-
pared with separate (not unified) models, Auto-
GUI unified shows general effectiveness across var-
ious tasks. The results show that a unified multi-
modal model out of first principles thinking can
5https://github.com/declare-lab/flan-alpaca .

--- PAGE 7 ---
Model Unified w/o Anno. Overall General Install GoogleApps Single WebShopping
PaLM 2-CoT ‚úì ‚úó 39.6 - - - -
ChatGPT-CoT ‚úì ‚úó 7.72 5.93 4.38 10.47 9.39 8.42
GPT-4V ‚úì ‚úó 52.96 43.01 46.14 49.18 78.29 48.18
Fine-tuned Llama 2 ‚úó ‚úó 28.40 28.56 35.18 30.99 27.35 19.92
BC-single ‚úó ‚úó 68.7 - - - -
BC-history ‚úó ‚úó 73.1 63.7 77.5 75.7 80.3 68.5
Auto-GUI separate ‚úó ‚úì 74.07 65.94 77.62 76.45 81.39 69.72
Auto-GUI unified ‚úì ‚úì 74.27 68.24 76.89 71.37 84.58 70.26
Table 2: Main results (%). Segment 1: in-context learning LLM baselines; Segment 2: fine-tuned Llama 2 baseline;
Segment 3: specialized agent baselines; Segment 4: our Auto-GUI results. Prior published best results are marked
with an underline . ‚ÄúUnified‚Äù means a general model that can work across subsets. ‚Äúw/o Anno.‚Äù means no screen
description is needed. The PaLM-CoT and BC results are from Rawles et al. (2023). The GPT-4V result is from
Yan et al. (2023). The other results are based on our own implementations. The overall score is computed as the
average accuracy on all the subsets. The best average result is in bold face.
0246810125055606570
(a) Previous ActionsAccuracy
0246810125055606570
(b) Previous Actions w/ Screens2 4 6 81052545658
(c) Future Plans
Figure 3: Performance of Auto-GUI with respect to varying length of chain of actions.
Model Accuracy
Auto-GUI 74.27
w/o future action plan 73.78
w/o previous action history 68.81
w/o chain of actions 68.53
w/o coordinate normalization 70.23
Table 3: Ablation study of Auto-GUI.
serve as a strong autonomous agent. Compared
with previous BC models, Auto-GUI unified has two
major advantages. First, Auto-GUI unified is a uni-
fied model that can be adapted to different scenar-
ios without the need to train specific models for
each task. Second, Auto-GUI unified does not need
additional annotations and is more practical in real-
world applications. Furthermore, Auto-GUI yields
divergent performance across subsets. We provide
the explanation in Appendix C.1 to save space.
(ii) Both the chain of actions and coordinate nor-
malization contribute to the overall performance
(+5.74% and 4.04%, respectively), as evidenced
by the ablation study in Table 3. Additionally, we
set the maximum numbers of the previous actions
and future actions to 8 and 4, respectively. The
choice is made according to our analysis of theGeneral subset with Auto-GUI separate (Figure 3).
The model under those setups achieves the opti-
mal performance, and neither the input nor output
sequence lengths exceed the model limit.
(iii) For the LLMs, using either prompting or
fine-tuning techniques does not achieve competi-
tive performance compared with multimodal ap-
proaches. The most plausible reason is that they
learn from the parsed HTML elements of the screen
so that they may suffer from information loss com-
pared with more informative vision features of the
screens. Specifically, we find that ChatGPT is quite
accurate at predicting the action type but fails at
lower-level executions (Appendix 5.2).
5 Analysis
5.1 Analysis of Auto-GUI by Category
To dive into the capability of Auto-GUI, we cal-
culate the click region accuracy, scroll direction
accuracy, action type accuracy, and typed text ac-
curacy. Figure 4 presents the results. We see that
Auto-GUI achieves over 90% action type accuracy
on average. In contrast, the major challenges lie
within the click region and scroll direction predic-
tions. Although the model is able to predict the
right action most of the time, it tends to click a

--- PAGE 8 ---
General Install GoogleApps Single WebShopping5060708090100Accuracy (%)Click (67.4%) Scroll (82.0%) Action Type (90.1%) Typed Text (93.1%)
Figure 4: Category accuracy of Auto-GUI. Values in parentheses represent the average accuracy on the subsets.
General Install GoogleApps Single WebShoppingGeneral
Install
GoogleApps
Single
WebShopping
Unified66 52 39 17 38
45 78 42 16 29
52 48 76 21 34
25 17 18 81 34
52 35 32 30 70
68 77 71 85 7020304050607080
Figure 5: Transfer results of Auto-GUI. The numbers
in the cells mean test accuracy. For example, we train
an Auto-GUI model on the training set of General and
then test its performance on the tests of each subset.
wrong place or scroll in a wrong direction. The
result reveals a future direction of improving the
model‚Äôs ability to understand the screen layouts,
e.g., using more advanced vision features.
5.2 Comparison with ICL by Category
To understand how the ICL baseline performs on
our task and assess the advantage of Auto-GUI, we
conduct a category comparison with ChatGPT.
In Table 6, we see that the ICL method (Chat-
GPT) is quite accurate at predicting the action type
(41.72%) but fails at lower-level executions, e.g.,
clicking positions (8.5%) and scrolling directions
(4.0%). The results show that using HTML-based
layout information is not enough to accurately exe-
cute actions. In contrast, Auto-GUI has the advan-
tage of predicting both action types and performing
low-level executions by leveraging multimodal per-
ception and the chain-of-action technique.
5.3 Generalization Ability
As our approach is designed under first principles
thinking and does not rely on pre-defined internal
APIs, it could be easily generalized to new taskdomains. To verify the generality, we evaluate the
performance of Auto-GUI separate on each subset in
Figure 5. For example, we train an Auto-GUI separate
model on the training set of General and then test
its performance on the tests of each subset.
We see that our approach is able to achieve a de-
cent performance, though the domains vary. This
result reveals that the model could capture gen-
eral knowledge for the GUI control task; thus is
applicable to different domains. In addition, the
unified model Auto-GUI unified can serve as a po-
tential choice in real-world applications owing to
more coverage of training data.
5.4 Comprehensive Analysis
Here, we present a comprehensive analysis of the
choice of pre-trained features and model scale. The
results are summarized in Table 4.
(i) Pre-trained Features. There are two kinds
of pre-trained features used in this work, the vi-
sion features and language model weights. For
vision features, we compare two popular types,
CLIP (Radford et al., 2021) and BLIP-2 (Li et al.,
2023). We observe that BLIP-2 achieves relatively
better performance. Therefore, we use BLIP-2
by default in Auto-GUI. For pre-trained language
model weights, we compare initializing the model
with the vanilla T5 (Raffel et al., 2020), FLAN-
T5 (Chung et al., 2022), and FLAN-Alpaca (Taori
et al., 2023a) weights under the large size. We see
that FLAN-Alpaca achieves the best performance
as it has been optimized with Stanford Alpaca syn-
thetic instruction tuning data.
(ii) Model Scale. Compared with the perfor-
mance gains from our technique components (chain
of actions and coordinate normalization) in Table
3, the benefit of scaling parameter size becomes
relatively marginal. As we observe that a larger
model size does not lead to dramatic improvement
in performance, we do not scale the model scale but

--- PAGE 9 ---
Model Overall General Install GoogleApps Single WebShopping
Auto-GUI on CLIP 71.84 66.28 74.40 69.71 81.60 67.23
Auto-GUI on BLIP-2 74.27 68.24 76.89 71.37 84.58 70.26
Auto-GUI on Vanilla-T5 large 72.98 66.61 75.40 70.86 83.47 68.54
Auto-GUI on FLAN-T5 large 73.36 67.59 76.35 70.71 83.01 69.12
Auto-GUI on FLAN-Alpaca large 74.27 68.24 76.89 71.37 84.58 70.26
Auto-GUI on FLAN-Alpaca small 71.38 65.26 74.90 68.70 81.20 66.83
Auto-GUI on FLAN-Alpaca base 72.84 66.97 75.93 70.29 82.56 68.46
Auto-GUI on FLAN-Alpaca large 74.27 68.24 76.89 71.37 84.58 70.26
Table 4: Results varying vision features and pre-trained language model weights.
Model Feature Extraction (s/n) Model Inference (s/n) Peak GPU Memory (GB)
Auto-GUI base 0.06 0.19 (45x) 4.6 (10x)
Auto-GUI large 0.06 0.59 (15x) 8.2 (6x)
Llama 2 - 8.5 49.7
Table 5: Computation cost of Auto-GUI and Llama. ‚Äús/n‚Äù is computed by time (s) divided by the number of
inferences (n). Llama 2 is hosted with 8-bit quantization and float16 precision to improve the inference speed.
Model Overall Action Type Click Scroll
ChatGPT 5.93 41.72 8.50 4.00
Auto-GUI 68.24 87.03 58.34 82.74
Table 6: Category comparison with the ICL baseline on
the General test set.
focus on the base (220M) and large (770M) models
in this work. In addition, our choice is also based
on other considerations, including the constriction
of GPU memory and computation budget.
According to the analysis above, our work offers
insights into the determinants of model efficacy:
‚Ä¢Vision features are critical, underscoring the
significance of effective perception.
‚Ä¢Model scale exhibits diminished importance,
indicating that larger model sizes do not necessarily
yield dramatic performance enhancements.
5.5 Computation Cost
Table 5 compares the inference speed and GPU
memory cost for Auto-GUI and Llama 2. Auto-
GUI is able to achieve nearly real-time inference
(within less than one second for an action predic-
tion) with less than 10GB GPU memory. The in-
ference speed is over 10 times faster than Llama 2.
Our work shows the strength of the medium-sized
language model in building autonomous agents,
which is able to achieve competitive performance
with fast inference speed and modest resource cost.
6 Conclusion
This work presents an autonomous GUI agent
called Auto-GUI that can interact in a multimodalGUI environment without environment parsing or
application-dependent API access. In addition,
we propose a chain-of-action technique that lever-
ages the previously executed actions and future
action plans to help the agent decide what action
to execute. Experimental results show that Auto-
GUI achieves superior performance to previous
prompting-based and fine-tuning baselines. We
show that it is possible to achieve state-of-the-art
performance by an end-to-end model without rely-
ing on external tools and application-specific APIs
to parse the environment and interpret the predicted
actions. Besides the strong performance and gener-
ality across domains, Auto-GUI is able to infer an
action in less than one second.
Limitations
We acknowledge two primary limitations in our
study. First, we opted not to extend the approach
to extremely large models because our work aims
to provide a simple yet effective solution for GUI
agents. Our findings suggest that scaling may not
be fundamentally advantageous in GUI problems.
The significance of model scale tends to diminish‚Äî
increasing the model size does not necessarily re-
sult in a substantial performance enhancement. Sec-
ond, our experiments and analysis were exclusively
conducted on AITW, which is the largest-scale and
widely recognized benchmark dataset in the re-
search line of autonomous GUI agents, to provide
timely and pertinent insights. Given the rapid de-
velopment of the field, we anticipate future studies
to explore the application of our approach on other
benchmark datasets as they become available.

--- PAGE 10 ---
References
Adept. 2022. Act-1: Transformer for actions.
https://www.adept.ai/act.
Aristotle. Physics 184a10‚Äì21.
Rohan Bavishi, Erich Elsen, Curtis Hawthorne,
Maxwell Nye, Augustus Odena, Arushi Somani, and
SaÀògnak Ta¬∏ sƒ±rlar. 2023. Introducing our multimodal
models: Fuyu-8b.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu,
Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024.
Seeclick: Harnessing gui grounding for advanced
visual gui agents. ArXiv preprint , abs/2401.10935.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
et al. 2023. Vicuna: An open-source chat-
bot impressing gpt-4 with 90%* chatgpt quality.
https://vicuna.lmsys.org.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. ArXiv preprint ,
abs/2204.02311.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
ArXiv preprint , abs/2210.11416.
Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran
Li, Dongxing Mao, Qinchen Wu, Weichen Zhang,
Peiyi Wang, Xiangwu Guo, et al. 2023. Assistgui:
Task-oriented desktop graphical user interface au-
tomation. ArXiv preprint , abs/2312.13108.
Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa
Safdari, Yutaka Matsuo, Douglas Eck, and Aleksan-
dra Faust. 2023. A real-world webagent with plan-
ning, long context understanding, and program syn-
thesis. ArXiv preprint , abs/2307.12856.
James Hendler. 1999. Is there an intelligent agent in
your future? Nature , 11.Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng
Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven
Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran,
Lingfeng Xiao, and Chenglin Wu. 2023a. Metagpt:
Meta programming for multi-agent collaborative
framework.
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng
Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang,
Yuxiao Dong, Ming Ding, et al. 2023b. Cogagent: A
visual language model for gui agents. ArXiv preprint ,
abs/2312.08914.
Terence Irwin. 1989. Aristotle‚Äôs first principles . Claren-
don Press.
Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram
Duvvur, Ming Chong Lim, Po-Yu Huang, Graham
Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and
Daniel Fried. 2024. Visualwebarena: Evaluating mul-
timodal agents on realistic visual web tasks. ArXiv
preprint , abs/2401.13649.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large
language models are zero-shot reasoners. ArXiv
preprint , abs/2205.11916.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. ArXiv preprint , abs/2301.12597.
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu
Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen
Men, Kejuan Yang, et al. 2023. Agentbench: Evalu-
ating llms as agents. ArXiv preprint , abs/2308.03688.
Pattie Maes. 1995. Agents that reduce work and infor-
mation overload. In Readings in human‚Äìcomputer
interaction , pages 811‚Äì821. Elsevier.
Yohei Nakajima. 2023. Babyagi.
https://github.com/yoheinakajima/babyagi.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, et al. 2022. Show your work: Scratch-
pads for intermediate computation with language
models. In Deep Learning for Code Workshop .
OpenAI. 2023. Gpt-4 technical report.
Joon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S Bern-
stein. 2023. Generative agents: Interactive simulacra
of human behavior. ArXiv preprint , abs/2304.03442.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021. Learn-
ing transferable visual models from natural language
supervision. In Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24

--- PAGE 11 ---
July 2021, Virtual Event , volume 139 of Proceedings
of Machine Learning Research , pages 8748‚Äì8763.
PMLR.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res. , 21:140:1‚Äì140:67.
Christopher Rawles, Alice Li, Daniel Rodriguez, Ori-
ana Riva, and Timothy P Lillicrap. 2023. An-
droidinthewild: A large-scale dataset for android de-
vice control. In Thirty-seventh Conference on Neural
Information Processing Systems Datasets and Bench-
marks Track .
Reworkd. 2023. Agentgpt.
https://github.com/reworkd/AgentGPT.
Toran Bruce Richards. 2023. Auto-gpt: An autonomous
gpt-4 experiment. https://github.com/Significant-
Gravitas/Auto-GPT.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-
ishala Neeraj, Jos Rozen, Abheesht Sharma, An-
drea Santilli, Thibault F√©vry, Jason Alan Fries, Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao,
Thomas Wolf, and Alexander M. Rush. 2022. Multi-
task prompted training enables zero-shot task gener-
alization. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 . OpenReview.net.
John R Searle. 1969. Speech acts: An essay in the
philosophy of language , volume 626. Cambridge
university press.
Yunpeng Song, Yiheng Bian, Yongtao Tang, and
Zhongmin Cai. 2023. Navigating interfaces with
ai for enhanced user interaction. ArXiv preprint ,
abs/2312.11190.
Theodore Sumers, Shunyu Yao, Karthik Narasimhan,
and Thomas L Griffiths. 2023. Cognitive ar-
chitectures for language agents. ArXiv preprint ,
abs/2309.02427.
Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai,
Zichen Zhu, and Kai Yu. 2022. META-GUI: To-
wards multi-modal conversational agents on mobile
GUI. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 6699‚Äì6712, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.Srinivas Sunkara, Maria Wang, Lijuan Liu, Gilles
Baechler, Yu-Chung Hsiao, Jindong Chen, Abhan-
shu Sharma, and James W. W. Stout. 2022. To-
wards better semantic understanding of mobile inter-
faces. In Proceedings of the 29th International Con-
ference on Computational Linguistics , pages 5636‚Äì
5650, Gyeongju, Republic of Korea. International
Committee on Computational Linguistics.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B Hashimoto. 2023a. Alpaca: A
strong, replicable instruction-following model. Stan-
ford Center for Research on Foundation Models.
https://crfm. stanford. edu/2023/03/13/alpaca. html .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B Hashimoto. 2023b. Stanford
alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. ArXiv preprint ,
abs/2307.09288.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , pages 5998‚Äì6008.
Bryan Wang, Gang Li, and Yang Li. 2023a. Enabling
conversational interaction with mobile ui using large
language models. In Proceedings of the 2023 CHI
Conference on Human Factors in Computing Systems ,
pages 1‚Äì17.
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-
dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-
ima Anandkumar. 2023b. V oyager: An open-ended
embodied agent with large language models. ArXiv
preprint , abs/2305.16291.
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao
Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,
Xu Chen, Yankai Lin, et al. 2023c. A survey on large
language model based autonomous agents. ArXiv
preprint , abs/2308.11432.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. ArXiv preprint , abs/2201.11903.
Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao,
Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu,
Yaqin Zhang, and Yunxin Liu. 2023. Empowering
llm to use smartphone for intelligent task automation.
ArXiv preprint , abs/2308.15272.

--- PAGE 12 ---
Michael Wooldridge and Nicholas R Jennings. 1995. In-
telligent agents: Theory and practice. The knowledge
engineering review , 10(2):115‚Äì152.
Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, and
Ben Kao. 2021. Good for misconceived reasons: An
empirical revisiting on the need for visual context
in multimodal machine translation. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 6153‚Äì6166, Online.
Association for Computational Linguistics.
An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin,
Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong,
Julian McAuley, Jianfeng Gao, et al. 2023. Gpt-
4v in wonderland: Large multimodal models for
zero-shot smartphone gui navigation. ArXiv preprint ,
abs/2311.07562.
Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Ze-
biao Huang, Bin Fu, and Gang Yu. 2023. Appa-
gent: Multimodal agents as smartphone users. ArXiv
preprint , abs/2312.13771.
Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin,
Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jef-
frey Nichols, Jason Wu, Chris Fleizach, et al. 2021.
Screen recognition: Creating accessibility metadata
for mobile applications from pixels. In Proceedings
of the 2021 CHI Conference on Human Factors in
Computing Systems , pages 1‚Äì15.
Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao
Utiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.
2020. Neural machine translation with universal
visual representation. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.
Zhuosheng Zhang, Shuohang Wang, Yichong Xu,
Yuwei Fang, Wenhao Yu, Yang Liu, Hai Zhao, Chen-
guang Zhu, and Michael Zeng. 2022. Task com-
pass: Scaling multi-task pre-training with task prefix.
InFindings of the Association for Computational
Linguistics: EMNLP 2022 , pages 5671‚Äì5685, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2023a. Automatic chain of thought prompt-
ing in large language models. In The Eleventh Inter-
national Conference on Learning Representations .
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,
George Karypis, and Alex Smola. 2023b. Multi-
modal chain-of-thought reasoning in language mod-
els.ArXiv preprint , abs/2302.00923.
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,
Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan
Bisk, Daniel Fried, Uri Alon, et al. 2023. Webarena:
A realistic web environment for building autonomous
agents. ArXiv preprint , abs/2307.13854.Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-
jie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu,
Xiaogang Wang, et al. 2023. Ghost in the minecraft:
Generally capable agents for open-world enviroments
via large language models with text-based knowledge
and memory. ArXiv preprint , abs/2305.17144.

--- PAGE 13 ---
A Data Details
A.1 Data Examples
We show the data examples from the AITW bench-
mark dataset (Rawles et al., 2023). Figures 6-9
show the examples in each subset, i.e., General, In-
stall, GoogleApps, Single, and WebShopping. The
gold actions for each screen are depicted in the
illustrations for reference.
A.2 Data Statistics
We use the AITW benchmark dataset (Rawles et al.,
2023). AITW is a large-scale benchmark dataset
for GUI control, which contains natural language
instructions, screenshots, and actions. There are
715Kepisodes spanning 30 Kunique instructions,
covering diverse multi-step tasks such as applica-
tion operation, web searching, and web shopping,
on over 350 Apps and websites. This dataset covers
various device types and operation systems in vary-
ing screen resolutions to ensure generality. There
are five subsets in the benchmark dataset: General,
Install, GoogleApps, Single, and WebShopping.
(i) General contains miscellaneous tasks that
need interaction with third-party Apps and web-
sites, as well as question answering.
(ii) Install contains tasks related to installing,
uninstalling, logging Apps, and App login support.
(iii) GoogleApps contains tasks about manipu-
lating various Google applications such as Gmail,
Calendar, Photos, and Settings.
(iv) Single contains atomic tasks (e.g., ‚Äúupvote
the post‚Äù) whose preceding actions have been al-
ready completed (e.g., opening Instagram, going to
home feed, looking at a post).
(v) WebShopping contains tasks related to online
shopping on E-commerce websites, e.g., searching
for an item, adding an item to the cart, and viewing
the shopping cart.
Table 7 presents the data statistics of the AITW
dataset. Each subset is split episode-wise into a
training, validation, and test set (80/10/10%).
Dataset Episodes Screens Instructions
General 9,476 85,413 545
Install 25,760 250,058 688
GoogleApps 625,542 4,903,601 306
Single 26,303 85,668 15,366
WebShopping 28,061 365,253 13,473
Table 7: Dataset statistics.
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
Set episode status as COMPLETEGoal: Open a new Chrome private windowFigure 6: An example episode from General.
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
Input text "microsoft authenticator"
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
0 100 200 300 400 5000
200
400
600
800
1000
Set episode status as COMPLETEGoal: uninstall "Microsoft Authenticator"
Figure 7: An example episode from Install.
B Implementation Details
B.1 Coordinate Normalization
Recall that a target action consists of four com-
ponents: action type, touch point, lift point, and
typed text. We consider six action types: dual-
point gesture ,type,go_back ,go_home ,enter ,
andstatus_complete . A dual-point gesture com-
prises a touch point and a lift point with [y, x]
coordinates. The gesture actions ensure a flex-
ible action space and can represent clicks and
scrolls at arbitrary locations. For example, a
gesture action {‚Äútouch_point‚Äù: [0.7761, 0.7089],
‚Äúlift_point‚Äù: [0.7761, 0.7089]} means clicking at
the coordinate [0.7761, 0.7089], while a gesture ac-
tion {‚Äútouch_point‚Äù: [0.1898, 0.4477], ‚Äúlift_point‚Äù:
[0.8242, 0.4077]} means scrolling down. A
type action means typing a text and the text is
placed in the <typed_text> field. The other ac-

--- PAGE 14 ---
Action Type Target Output
dual-point gesture (click) ‚Äúaction_type‚Äù: 4, ‚Äútouch_point‚Äù: [0.8497, 0.5964], ‚Äúlift_point‚Äù: [0.8497, 0.5964],
‚Äútyped_text‚Äù: ‚Äú‚Äù
dual-point gesture (scroll) ‚Äúaction_type‚Äù: 4, ‚Äútouch_point‚Äù: [0.2, 0.5], ‚Äúlift_point‚Äù: [0.8, 0.5], ‚Äútyped_text‚Äù:
‚Äú‚Äù
type ‚Äúaction_type‚Äù: 3, ‚Äútouch_point‚Äù: [-1.0, -1.0], ‚Äúlift_point‚Äù: [-1.0, -1.0],
‚Äútyped_text‚Äù: ‚Äúwhat‚Äôs the news in chile?‚Äù
go_back ‚Äúaction_type‚Äù: 5, ‚Äútouch_point‚Äù: [-1.0, -1.0], ‚Äúlift_point‚Äù: [-1.0, -1.0],
‚Äútyped_text‚Äù: ‚Äú‚Äù
go_home ‚Äúaction_type‚Äù: 6, ‚Äútouch_point‚Äù: [-1.0, -1.0], ‚Äúlift_point‚Äù: [-1.0, -1.0],
‚Äútyped_text‚Äù: ‚Äú‚Äù
enter ‚Äúaction_type‚Äù: 7, ‚Äútouch_point‚Äù: [-1.0, -1.0], ‚Äúlift_point‚Äù: [-1.0, -1.0],
‚Äútyped_text‚Äù: ‚Äú‚Äù
status_complete ‚Äúaction_type‚Äù: 10, ‚Äútouch_point‚Äù: [-1.0, -1.0], ‚Äúlift_point‚Äù: [-1.0, -1.0],
‚Äútyped_text‚Äù: ‚Äú‚Äù
Table 8: Target output examples after the coordinate normalization.
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700
0 50 100 150 200 250 300 350 4000
100
200
300
400
500
600
700 Set episode status as COMPLETEGoal: turn off javascript in the chrome app
Figure 8: An example episode from GoogleApps.
tion types, i.e., go_back, go_home, enter, and
status_complete are system actions, whose corre-
sponding <touch_point>, <lift_point> fields are
filled with -1, and the <typed_text> is empty.
We observe that high-precision coordinates are
not necessary for representing a click or scroll ac-
tion. Therefore, we apply normalized values of the
coordinates, which helps accelerate convergence
and mitigate the ambiguity of coordinates. The
normalization is applied to click and scroll actions.
For click actions, we keep four decimal places. For
scroll actions, we first determine the scroll direction
with the touch and lift points. Then, we transform
the touch and lift points into fixed directional co-
ordinates as follows: ‚Äúup‚Äù: {[0.8, 0.5], [0.2, 0.5]},
‚Äúdown‚Äù: {[0.2, 0.5], [0.8, 0.5]}, ‚Äúleft‚Äù: {[0.5, 0.8],
[0.5, 0.2]}, ‚Äúright‚Äù: {[0.5, 0.2], [0.5, 0.8]}, where
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400Input text "lowe's"
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400Input text "best rated coffee maker"
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400
0 100 200 300 400 500 600 7000
200
400
600
800
1000
1200
1400Set episode status as COMPLETEGoal: Look up the best rated coffee maker on Lowe's.Figure 9: An example episode from WebShopping.
{[¬∑], [¬∑]} consists of the touch point and lift point in
the first [ ¬∑] and second [ ¬∑]. We provide examples of
target actions in Table 8.
B.2 Baselines
We adopt three types of baselines for comparisons.
The baselines encompass the in-context earning
(ICL) and fine-tuning paradigms, along with vari-
ous backbone models of different sizes. This choice
of baselines allows for a comprehensive compari-
son with our proposed approach.
(i) In-context Learning LLMs. Few-shot PaLM
2, ChatGPT (turbo-3.5) are adopted. Following
previous studies (Rawles et al., 2023; Wang et al.,
2023a), we feed the LLM a textual description of
the screen and a user instruction. The textual de-
scription of the screen is formatted as an HTML

--- PAGE 15 ---
Model Overall General Install GoogleApps Single WebShopping
Auto-GUI 74.27 68.24 76.89 71.37 84.58 70.26
w/o previous action history 73.78 67.97 76.66 71.00 83.64 69.62
w/o future action plan 68.81 59.01 72.34 67.95 81.53 63.24
w/o chain of actions 68.53 58.99 72.06 67.50 81.25 62.86
w/o coordinate normalization 70.23 63.79 73.28 66.63 82.11 65.33
Table 9: Ablation study of Auto-GUI design components. We adopt Auto-GUI unified for analysis.
Model Overall General Install GoogleApps Single WebShopping
Auto-GUI base 72.84 66.97 75.93 70.29 82.56 68.46
w/ Screen Descriptions 75.54 70.30 78.05 73.04 85.31 71.00
Table 10: Results of Auto-GUI when using annotated screen descriptions.
syntax, providing the information of GUI elements
derived from OCR detection and icon detection
from external tools (Rawles et al., 2023). The
model is required to predict an action among pre-
defined actions. If the action is clicking, the model
will be required to provide the index of the clicked
GUI element. Alternatively, the model needs to pro-
vide the scroll direction if the action is scrolling. In
addition, 5-shot CoT prompting is leveraged to im-
prove the performance (Appendix B.3). In addition,
we report the results of the multimodal GPT-4V by
taking the vision image and action history as the
input based on Yan et al. (2023).
(ii) Fine-tuned LLMs. We adopt Llama 2 (Tou-
vron et al., 2023) as the baseline and fine-tune it
with LoRA. We feed the model with the user in-
struction and the screen descriptions in HTML syn-
tax (the same as adopted for in-context learning
LLMs). The model is expected to predict the action
in the same output format as in-context learning
LLMs. As fine-tuning an LLM is expensive, we
randomly sample 1% training data to help the LLM
adapt to our tasks.
(iii) Specialized GUI Agent. We adopted the
Behavioural Cloning (BC) agent, which reported
the state-of-the-art performance in Rawles et al.
(2023). BC is a Transformer-based architecture
that takes a task instruction, the current screen,
and a stacked history of screen observations and
actions as input. The task instruction and OCR-
detected texts are encoded by a pre-trained BERT.
The icons are represented by the embeddings for
each of the bounding box points. The screen history
is modeled by the {x, y}positions of the touch and
lift actions. All the embedded representations are
fused to predict the action by a decoder. There
are two BC variants, BC-single and BC-history,depending on whether the model takes as input the
screen-action history.
B.3 LLM Prompt
We use the prompt in Figures 10-11 for PaLM 2-
CoT and ChatGPT-CoT owing to its optimal per-
formance reported in Rawles et al. (2023).
C Further Analysis
C.1 Subset Analysis
We notice that Auto-GUI unified performs relatively
inferior to BC-history on the two App-centered
subsets, Install and GoogleApps. It is reason-
able because we only use 10% training data of
GoogleApps considering the data balance and com-
putation overhead. We observe that the perfor-
mance does not improve when we use all the train-
ing data of GoogleApps, possibly due to the data
imbalance issue (Zhang et al., 2022). In contrast,
our separate model Auto-GUI separate can achieve
better performance than BC-history, showing that
our approach is better than BC-history under the
same training setting. As we aim to study a simple
and unified approach that achieves generally strong
performance, we leave the treatment of the data
imbalance issue in future work.
C.2 Ablation Study
Table 9 shows the detailed results of the ablation
study. We see that both the chain of actions and
coordinate normalization contribute to the overall
performance (+5.74% and 4.04%, respectively).
C.3 Using Screen Descriptions
We are interested in whether Auto-GUI can be fur-
ther improved when screen annotations are avail-
able. Therefore, we incorporate screen descriptions

--- PAGE 16 ---
containing icon and text information, organized in
HTML syntax, into our language input Xlanguage .
Detailed examples of screen descriptions can be
found in the ‚ÄúScreen‚Äù block in Appendix B.3.
In Table 10, we see that Auto-GUI can perform
better when the annotated screen descriptions are
available. The results show that there is still room
for performance gains for Auto-GUI. However, as
the annotations are not always available in real-
world applications, we do not include them by de-
fault in our framework.

--- PAGE 17 ---
Given a mobile screen and a question, provide the action based on the screen information.
Available Actions:
{"action_type": "click", "idx": <element_idx>}
{"action_type": "type", "text": <text>}
{"action_type": "navigate_home"}
{"action_type": "navigate_back"}
{"action_type": "scroll", "direction": "up"}
{"action_type": "scroll", "direction": "down"}
{"action_type": "scroll", "direction": "left"}
{"action_type": "scroll", "direction": "right"}
Previous Actions:
{"step_idx": 0, "action_description": "press [HOME key]"}
{"step_idx": 2, "action_description": "click [Google Icon]"}
{"step_idx": 3, "action_description": "click [search for hotels]"}
Screen:
<img id=0 class="IconGoogle" alt="Google Icon">  </img>
<img id=1 class="IconX" alt="Close Icon">  </img>
<p id=2 class="text" alt="search for hotels"> search for hotels </p>
<p id=3 class="text" alt="in"> in </p>
<p id=4 class="text" alt="mexico city mexico"> mexico city mexico </p>
<img id=5 class="IconMagnifyingGlass" alt="Search Icon">  </img>
<p id=6 class="text" alt="Share"> Share </p>
<p id=7 class="text" alt="Select alI"> Select alI </p>
<p id=8 class="text" alt="Cut"> Cut </p>
<p id=9 class="text" alt="Copy"> Copy </p>
<p id=10 class="text" alt="hotel in mex"> hotel in mex </p>
<img id=11 class="IconMagnifyingGlass" alt="Search Icon">  </img>
<p id=12 class="text" alt="best hotel"> best hotel </p>
<p id=13 class="text" alt="mexico city"> mexico city </p>
<p id=14 class="text" alt="in"> in </p>
<img id=15 class="IconMagnifyingGlass" alt="Search Icon">  </img>
<p id=16 class="text" alt="K"> K </p>
<p id=17 class="text" alt="hotel ciudad"> hotel ciudad </p>
<p id=18 class="text" alt="de mexico"> de mexico </p>
<p id=19 class="text" alt="gran"> gran </p>
<img id=20 class="IconVBackward" alt="Left Icon">  </img>
<img id=21 class="IconNavBarCircle" alt="Home Icon">  </img>
<img id=22 class="IconNavBarRect" alt="Overview Icon">  </img>
Instruction: What time is it in Berlin?
Answer: Let's think step by step. I see unrelated search results in the Google app,
I must clear the search bar, so the action is {"action_type": "click", "idx": 1}
Previous Actions:
{"step_idx": 0, "action_description": "click [DISMISS]"}
Screen:
<p id=0 class="text" alt="Update your"> Update your </p>
<p id=1 class="text" alt="Gmail app"> Gmail app </p>
<p id=2 class="text" alt="attach files from"> attach files from </p>
<p id=3 class="text" alt="To"> To </p>
<p id=4 class="text" alt="download the"> download the </p>
<p id=5 class="text" alt="Drive,"> Drive, </p>
<p id=6 class="text" alt="latest"> latest </p>
<p id=7 class="text" alt="version"> version </p>
<p id=8 class="text" alt="of"> of </p>
<p id=9 class="text" alt="Gmail"> Gmail </p>
<p id=10 class="text" alt="UPDATE"> UPDATE </p>
<p id=11 class="text" alt="DISMISS"> DISMISS </p>
<p id=12 class="text" alt="Got"> Got </p>
<p id=13 class="text" alt="it"> it </p>
<img id=14 class="IconVBackward" alt="Left Icon">  </img>
Instruction: see creations saved in the google photos
Answer: Let's think step by step. I see a popup, I need to open Google Photos, so
the action is {"action_type": "click", "idx": 11}
Previous Actions:
Screen:
<p id=0 class="text" alt="M"> M </p>
<p id=1 class="text" alt="New in Gmail"> New in Gmail </p>Figure 10: LLM Prompt (Part-I).

--- PAGE 18 ---
<p id=2 class="text" alt="All the features you"> All the features you </p>
<p id=3 class="text" alt="love with"> love with </p>
<p id=4 class="text" alt="a fresh"> a fresh </p>
<p id=5 class="text" alt="look"> look </p>
<p id=6 class="text" alt="new"> new </p>
<p id=7 class="text" alt="GOT IT"> GOT IT </p>
Instruction: open app "Google Play services"
Answer: Let's think step by step. I see the GMail app, I need to open the app
drawer, so the action is {"action_type": "navigate_home"}
Previous Actions:
Screen:
<p id=0 class="text" alt="Tuesday, Aug"> Tuesday, Aug </p>
<p id=1 class="text" alt="9"> 9 </p>
<img id=2 class="IconChat" alt="Chat Icon">  </img>
<img id=3 class="IconGoogle" alt="Google Icon">  </img>
Instruction: open app "Messenger Lite" (install if not already installed)
Answer: Let's think step by step. I see the home screen, I need to open the app 
drawer, I should swipe up, so the action is {"action_type": "scroll", "direction":
"down"}
Previous Actions:
{"step_idx": 0, "action_description": "scroll down"}
Screen:
<img id=0 class="IconThreeDots" alt="More Icon">  </img>
<p id=1 class="text" alt="Search your phone and more"> Search your phone and more </p>
<p id=2 class="text" alt="M"> M </p>
<p id=3 class="text" alt="O"> O </p>
<img id=4 class="IconPlay" alt="Play Icon">  </img>
<p id=5 class="text" alt="Clock"> Clock </p>
<p id=6 class="text" alt="YouTube"> YouTube </p>
<p id=7 class="text" alt="Photos"> Photos </p>
<p id=8 class="text" alt="Gmail"> Gmail </p>
<p id=9 class="text" alt="All apps"> All apps </p>
<p id=10 class="text" alt="g"> g </p>
<p id=11 class="text" alt="O"> O </p>
<img id=12 class="IconTakePhoto" alt="Camera Icon">  </img>
<p id=13 class="text" alt="10"> 10 </p>
<p id=14 class="text" alt="Calendar"> Calendar </p>
<p id=15 class="text" alt="Camera"> Camera </p>
<p id=16 class="text" alt="Chrome"> Chrome </p>
<p id=17 class="text" alt="Clock"> Clock </p>
<p id=18 class="text" alt="0"> 0 </p>
<p id=19 class="text" alt="M"> M </p>
<p id=20 class="text" alt="B"> B </p>
<img id=21 class="IconPerson" alt="Person Icon">  </img>
<p id=22 class="text" alt="Gmail"> Gmail </p>
<p id=23 class="text" alt="Drive"> Drive </p>
<p id=24 class="text" alt="Files"> Files </p>
<p id=25 class="text" alt="Contacts"> Contacts </p>
<p id=26 class="text" alt="G OO"> G OO </p>
<img id=27 class="IconGoogle" alt="Google Icon">  </img>
<img id=28 class="IconLocation" alt="Location Icon">  </img>
<img id=29 class="IconCall" alt="Phone Icon">  </img>
<img id=30 class="IconChat" alt="Chat Icon">  </img>
<p id=31 class="text" alt="Google"> Google </p>
<p id=32 class="text" alt="Maps"> Maps </p>
Instruction: Search for hotels in Chicago.
Answer: Let's think step by step. I see the app drawer, I need to search, so the 
action is {"action_type": "click", "idx": 27}
Previous Actions:
<HISTORY>
Screen:
<SCREEN_REPRESENTATION>
Instruction: <GROUNDING_GOAL>
Answer: Let's think step by step. I seeFigure 11: LLM Prompt (Part-II).
