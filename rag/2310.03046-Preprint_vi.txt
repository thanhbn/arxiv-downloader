# 2310.03046.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2310.03046.pdf
# Kích thước tệp: 4799035 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo
ECOASSISTANT: SỬ DỤNG TRỢ LÝ LLM VỚI CHI PHÍ HỢP LÝ HỚN VÀ CHÍNH XÁC HỚN

Jieyu Zhang∗*,1, Ranjay Krishna1, Ahmed H. Awadallah2, Chi Wang2
1University of Washington
2Microsoft Research
1{jieyuz2,ranjay}@cs.washington.edu
2{hassanam,wang.chi}@microsoft.com

TÓM TẮT
Ngày nay, người dùng hỏi các mô hình ngôn ngữ lớn (LLM) như trợ lý để trả lời các truy vấn đòi hỏi kiến thức bên ngoài; họ hỏi về thời tiết ở một thành phố cụ thể, về giá cổ phiếu, và thậm chí về vị trí các địa điểm cụ thể trong khu phố của họ. Những truy vấn này yêu cầu LLM tạo ra mã để gọi các API bên ngoài nhằm trả lời câu hỏi của người dùng, tuy nhiên LLM hiếm khi tạo ra mã chính xác ngay lần đầu tiên, đòi hỏi việc tinh chỉnh mã lặp đi lặp lại dựa trên kết quả thực thi. Ngoài ra, việc sử dụng trợ lý LLM để hỗ trợ khối lượng truy vấn lớn có thể tốn kém. Trong nghiên cứu này, chúng tôi đóng góp một framework, EcoAssistant1, cho phép LLM trả lời các truy vấn dựa trên mã một cách hợp lý về chi phí và chính xác hơn. EcoAssistant chứa ba thành phần. Đầu tiên, nó cho phép các trợ lý LLM trò chuyện với một trình thực thi mã tự động để tinh chỉnh mã lặp đi lặp lại hoặc tạo ra câu trả lời dựa trên kết quả thực thi. Thứ hai, chúng tôi sử dụng một hệ thống phân cấp của các trợ lý LLM, cố gắng trả lời truy vấn với các LLM yếu hơn, rẻ hơn trước khi chuyển sang các LLM mạnh hơn, đắt hơn. Thứ ba, chúng tôi truy xuất các giải pháp từ các truy vấn thành công trong quá khứ làm minh họa ngữ cảnh để hỗ trợ các truy vấn tiếp theo. Thực nghiệm cho thấy EcoAssistant mang lại những lợi thế rõ rệt về khả năng chi trả và độ chính xác, vượt trội hơn GPT-4 10 điểm tỷ lệ thành công với ít hơn 50% chi phí của GPT-4.

1 GIỚI THIỆU
Gần đây, người dùng đã sử dụng các LLM đối thoại như ChatGPT (OpenAI, 2023) cho nhiều truy vấn khác nhau. Các báo cáo cho thấy 23% truy vấn người dùng ChatGPT là để trích xuất kiến thức (Fishkin, 2023). Nhiều truy vấn này đòi hỏi kiến thức bên ngoài thông tin được lưu trữ trong bất kỳ mô hình ngôn ngữ lớn (LLM) được đào tạo trước nào. Ví dụ, người dùng hỏi về thời tiết ở thành phố của họ: "Độ che phủ mây hiện tại ở Mumbai, Ấn Độ là bao nhiêu?"; họ hỏi về giá cổ phiếu: "Bạn có thể cho tôi biết giá mở cửa của Microsoft trong tháng 1 năm 2023 không?"; một số thậm chí hỏi về gợi ý địa điểm: "Tôi đang tìm một hiệu thuốc mở 24 giờ ở Montreal, bạn có thể tìm cho tôi không?". Những nhiệm vụ này chỉ có thể hoàn thành bằng cách gọi các API bên ngoài chứa thông tin được yêu cầu. Do đó, những loại nhiệm vụ này—những gì chúng tôi gọi là trả lời câu hỏi dựa trên mã—yêu cầu LLM tạo mã để lấy thông tin cần thiết thông qua API.

Giống như các lập trình viên con người hiếm khi tạo ra mã chính xác ngay lần thử đầu tiên, LLM cũng gặp khó khăn (Yang et al., 2023). Điều này đặc biệt nghiêm trọng vì các LLM hiện tại thiếu khả năng thực thi mã được tạo ra và gỡ lỗi lặp đi lặp lại như hầu hết các lập trình viên con người làm. Ngoài ra, khi ChatGPT nhận được khoảng 80 triệu truy vấn vào tháng 7 năm 2023, 23% trong số đó tạo nên 4 triệu truy vấn kiến thức chỉ riêng trong tháng đó (Fishkin, 2023; Chen et al., 2023; Wang et al., 2023a). Khối lượng truy vấn người dùng cao như vậy có thể tốn kém cho những ai muốn phát triển hệ thống sử dụng dịch vụ LLM trực tuyến có phí để xử lý các truy vấn này.

Để vượt qua những thách thức này, trong nghiên cứu này, chúng tôi trình bày EcoAssistant, hệ thống đầu tiên được thiết kế riêng để tận dụng các LLM đối thoại nhằm giải quyết việc trả lời câu hỏi dựa trên mã một cách hợp lý về chi phí và chính xác hơn.

∗Công việc trong thời gian thực tập tại Microsoft Research.
1https://github.com/JieyuZ2/EcoAssistant
1arXiv:2310.03046v1 [cs.SE] 3 Oct 2023

--- TRANG 2 ---
Bản thảo
EcoAssistant không cần bất kỳ chuẩn bị ngoại tuyến hoặc đào tạo nào; nó là một dịch vụ hoàn toàn trực tuyến cải thiện theo việc sử dụng. Nó chứa ba thành phần cơ bản. Đầu tiên, để hỗ trợ lập trình lặp, nó cho phép LLM đối thoại như một tác nhân trợ lý trò chuyện với một trình thực thi mã tự động và tinh chỉnh mã lặp đi lặp lại để thực hiện các lời gọi API chính xác. Chúng tôi xây dựng EcoAssistant sử dụng AutoGen (Wu et al., 2023), một framework gần đây cho phép xây dựng các ứng dụng LLM thông qua cuộc trò chuyện đa tác nhân. Không giống như các thực hành hiện tại sử dụng LLM để tạo mã hoặc trả lời truy vấn người dùng trong một lần tạo duy nhất, thiết kế hệ thống của chúng tôi khai thác tiến bộ gần đây của các LLM đối thoại có thể tinh chỉnh đầu ra của chúng lặp đi lặp lại (OpenAI, 2023; Touvron et al., 2023).

Thứ hai, chúng tôi sử dụng một hệ thống phân cấp của các trợ lý LLM, được gọi là phân cấp trợ lý, cố gắng trả lời truy vấn với các LLM yếu hơn, rẻ hơn trước khi chuyển sang các LLM mạnh hơn, đắt hơn. Đối với mỗi truy vấn, chúng tôi bắt đầu cuộc trò chuyện với trợ lý LLM hiệu quả nhất về chi phí, và dần dần chuyển sang những cái đắt hơn chỉ khi cái hiện tại thất bại. Vì các LLM thường có cấu trúc giá không đồng nhất, chiến lược đơn giản như vậy có thể giảm chi phí tổng thể của hệ thống bằng cách giảm việc sử dụng các LLM đắt tiền.

Thứ ba, chúng tôi đề xuất minh họa giải pháp, truy xuất các giải pháp từ các truy vấn thành công trong quá khứ làm minh họa ngữ cảnh để hỗ trợ các truy vấn tiếp theo. Để đạt được điều này, chúng tôi lưu trữ các cặp truy vấn-mã đúng trong cơ sở dữ liệu một khi truy vấn thành công; sau đó khi có truy vấn mới, chúng tôi truy xuất truy vấn tương tự nhất cũng như mã liên quan từ cơ sở dữ liệu làm minh họa ngữ cảnh trong prompt của LLM. Với các giải pháp đã được chứng minh được minh họa, trợ lý có nhiều khả năng tạo ra phản hồi chính xác và hiệu quả mà không cần lặp lại dư thừa, từ đó tăng khả năng thành công.

Mặc dù phân cấp trợ lý và minh họa giải pháp mang lại những lợi thế riêng biệt khi được sử dụng riêng lẻ, chúng tôi thấy rằng sự tương tác của chúng dẫn đến hiệu ứng hiệp đồng khuếch đại các lợi ích riêng lẻ của chúng. Điều này là do các trợ lý trong hệ thống phân cấp chia sẻ cơ sở dữ liệu lưu trữ các cặp truy vấn-mã, những giải pháp này từ các LLM mạnh hơn, đắt hơn phục vụ như hướng dẫn hữu ích cho các mô hình yếu hơn trong các truy vấn tiếp theo. Kết quả là, trợ lý yếu hơn có khả năng giải quyết nhiều truy vấn hơn trong tương lai, điều này càng làm giảm sự phụ thuộc của hệ thống vào các LLM đắt tiền.

Chúng tôi tiến hành các thí nghiệm có hệ thống trên nhiều loại truy vấn khác nhau để điều tra cả hiệu suất và chi phí đô la của hệ thống được đề xuất. Kết quả của chúng tôi làm nổi bật rằng phân cấp trợ lý có thể giảm đáng kể chi phí, trong khi minh họa giải pháp phần lớn tăng hiệu suất của hệ thống. Ngoài ra, EcoAssistant, kết hợp cả hai chiến lược này, đạt được hiệu suất vượt trội với việc giảm chi phí hơn nữa. Ngoài ra, chúng tôi cho thấy EcoAssistant vượt trội hơn một trợ lý GPT-4 riêng lẻ với biên độ 10% tỷ lệ thành công với ít hơn một nửa chi phí.

2 NHIỆM VỤ TRẢ LỜI CÂU HỎI DỰA TRÊN MÃ

Trong nghiên cứu này, chúng tôi tập trung vào một nhiệm vụ thực tế nhưng đầy thách thức gọi là trả lời câu hỏi dựa trên mã, trong đó LLM phải trả lời các truy vấn kiến thức; LLM phải tạo mã để gọi API để thu thập thông tin cần thiết để trả lời câu hỏi của người dùng. Ví dụ, một truy vấn người dùng có thể hỏi về thông tin động hoặc thời gian thực như thời tiết của một địa điểm cụ thể vào một ngày nhất định. Vì thông tin này không được lưu trữ trong kiến thức nội bộ của mô hình hoặc cơ sở kiến thức chung, mô hình sẽ dựa vào API thời tiết để thu thập thông tin. Để đạt được điều này, LLM cần không chỉ hiểu đúng truy vấn của người dùng mà còn viết mã Python tốt. Do đó, nhiệm vụ mới này đưa ra một thách thức đa mặt: nó đòi hỏi thành thạo trong hiểu ngôn ngữ và tạo cả ngôn ngữ tự nhiên và ngôn ngữ lập trình. Đặc điểm này phân biệt trả lời câu hỏi dựa trên mã với các mô hình trả lời câu hỏi hiện có như trả lời câu hỏi miền mở (Lee et al., 2019; Chen et al., 2017) hoặc trả lời câu hỏi trợ lý trình duyệt (Nakano et al., 2021), vì chúng thường không thách thức khả năng tạo và tinh chỉnh mã của LLM. Nó cũng khác với nhiệm vụ tạo mã chung (Chen et al., 2021; Hendrycks et al., 2021; Austin et al., 2021; Lu et al., 2021; Yang et al., 2023) bằng việc yêu cầu LLM khai thác API dành riêng cho miền dựa trên truy vấn người dùng.

Lập trình lặp. Trả lời câu hỏi dựa trên mã tự nhiên đòi hỏi lập trình lặp (Yang et al., 2023). Chúng tôi kết nối LLM cơ bản đang cố gắng tạo mã với một trình thực thi mã. Trực quan, trình thực thi mã thực thi mã được tạo và chuyển tiếp kết quả thực thi hoặc vết thực thi thất bại trở lại LLM. Tương tác này có thể xảy ra nhiều lần, khi LLM sử dụng vết thực thi trước đó để tinh chỉnh quá trình tạo của nó. Có thể xem quá trình này như một cuộc trò chuyện đa lượt tự động giữa LLM và trình thực thi mã, hoàn toàn diễn ra trong nền, không có sự tham gia của người dùng. Chúng tôi áp dụng các LLM trò chuyện như GPT-3.5-turbo, cho phép chúng tôi tận dụng tất cả các tiến bộ gần đây của LLM cho mục đích trò chuyện.

Các truy vấn đến liên tục. Chúng tôi cũng xem xét tình huống thực tế nơi các truy vấn đến liên tục theo thời gian. Do đó, mỗi truy vấn không phải là một nhiệm vụ độc lập mà có thể tận dụng các truy vấn trong quá khứ làm hướng dẫn. Trong bối cảnh như vậy, có thể tưởng tượng việc theo dõi các truy vấn thành công để cải thiện những truy vấn tương lai. Hệ thống của chúng tôi, được mô tả dưới đây, điều tra cách sử dụng các truy vấn trong quá khứ để phục vụ tốt hơn những truy vấn tương lai.

3 ECOASSISTANT: SỬ DỤNG TRỢ LÝ LLM VỚI CHI PHÍ HỢP LÝ HƠN VÀ CHÍNH XÁC HƠN

Để nhắc lại, nhiệm vụ trả lời câu hỏi dựa trên mã vừa đầy thách thức vừa tốn kém. LLM khó tạo ra mã chính xác ngay lần đầu tiên để sử dụng API, và việc xử lý khối lượng lớn truy vấn người dùng bằng dịch vụ LLM có phí có thể tốn kém. Để giải quyết nhiệm vụ này một cách hợp lý và chính xác, chúng tôi phát triển EcoAssistant, một hệ thống sử dụng LLM để trả lời các truy vấn kiến thức một cách chính xác trong khi giảm chi phí đô la.

EcoAssistant chứa ba thành phần (xem Hình 1). Đầu tiên, nó đặt LLM như một tác nhân trợ lý trong cuộc trò chuyện với một trình thực thi mã. LLM lặp đi lặp lại gỡ lỗi mã của nó bằng cách đọc đầu ra của trình thực thi mã hoặc vết thực thi thất bại, và cuối cùng tạo ra câu trả lời dựa trên thông tin thu được. Thứ hai, để giảm chi phí, chúng tôi sử dụng một hệ thống phân cấp của các trợ lý LLM, thử các truy vấn với các trợ lý LLM rẻ hơn trước khi chuyển sang các lựa chọn thay thế đắt hơn. Thứ ba, chúng tôi theo dõi các truy vấn thành công và mã liên quan và sử dụng chúng làm minh họa ngữ cảnh cho những truy vấn tiếp theo. Điều này cho phép LLM trong tương lai sử dụng các thành công trong quá khứ làm hướng dẫn. Hệ thống của chúng tôi không yêu cầu chuẩn bị ngoại tuyến, không cần tuyển chọn tập dữ liệu, và không cần đào tạo.

Hình 1: EcoAssistant: hệ thống bao gồm hai tác nhân, một tác nhân thực thi để thực thi mã và một tác nhân trợ lý khác được hỗ trợ bởi LLM để đề xuất mã nhằm thu thập thông tin và giải quyết các truy vấn người dùng. Cơ sở dữ liệu truy vấn-mã lưu trữ cặp truy vấn và mã thành công trước đó. Khi có truy vấn mới, truy vấn tương tự nhất trong cơ sở dữ liệu được truy xuất và sau đó được minh họa trong prompt ban đầu với mã liên quan. Cuộc trò chuyện gọi trợ lý hiệu quả nhất về chi phí trước và chỉ thử cái đắt hơn trong hệ thống phân cấp trợ lý khi cái hiện tại thất bại.

Cuộc trò chuyện tự động giữa các trợ lý LLM và trình thực thi mã. EcoAssistant đặt LLM như một tác nhân trợ lý trong cuộc trò chuyện với một trình thực thi mã. Trình thực thi trích xuất

--- TRANG 3 ---
Bản thảo
mã được tạo và thực thi nó, chuyển tiếp đầu ra trở lại LLM; sau đó nó chờ đợi lượt trò chuyện tiếp theo, nơi có thể LLM sẽ tinh chỉnh quá trình tạo của nó, học từ lỗi lầm trong quá khứ, hoặc tạo ra câu trả lời cuối cùng theo kết quả thực thi.

Để đạt được luồng trò chuyện này, chúng tôi phát triển hệ thống của mình dựa trên AutoGen (Wu et al., 2023), một cơ sở hạ tầng gần đây hỗ trợ cuộc trò chuyện đa tác nhân tự động. Cụ thể, chúng tôi tận dụng AssistantAgent và UserProxyAgent tích hợp sẵn của AutoGen làm trợ lý LLM và trình thực thi mã, tương ứng. Cái trước được cấu hình với các prompt hệ thống chuyên dụng được đề xuất trong AutoGen, hướng dẫn LLM 1) đề xuất mã trong một khối mã khi cần thiết, 2) tinh chỉnh mã theo kết quả thực thi, và 3) thêm một mã đặc biệt "TERMINATE" ở cuối phản hồi khi nó muốn kết thúc cuộc trò chuyện.

Cái sau, hoạt động như proxy của người dùng, tự động trích xuất mã từ tin nhắn của LLM, và thực thi nó trong môi trường cục bộ. Sau đó nó gửi kết quả thực thi trở lại LLM. Khi không phát hiện mã nào, nó sẽ gửi lại một tin nhắn mặc định. Do đó, cuộc trò chuyện được tự động hóa và người dùng chỉ cần nhập truy vấn ban đầu để kích hoạt cuộc trò chuyện mà không cần can thiệp thủ công như sao chép, dán và thực thi mã.

Cuối cùng, cuộc trò chuyện kết thúc khi gặp một trong các trường hợp sau: 1) cửa sổ ngữ cảnh của LLM bị vượt quá; 2) số lượt trao đổi qua lại trong cuộc trò chuyện vượt quá ngưỡng đã đặt2; và 3) LLM thêm "TERMINATE" ở cuối phản hồi của nó.

Phân cấp trợ lý. Chúng tôi sử dụng một hệ thống phân cấp của các trợ lý LLM. Cụ thể, với nhiều LLM, chúng tôi khởi tạo một AssistantAgent cho mỗi cái và bắt đầu cuộc trò chuyện tự động với trợ lý LLM hiệu quả nhất về chi phí. Nếu cuộc trò chuyện giữa trợ lý LLM hiện tại và trình thực thi mã kết thúc mà không giải quyết thành công truy vấn, hệ thống sau đó sẽ khởi động lại cuộc trò chuyện với trợ lý LLM đắt hơn tiếp theo trong hệ thống phân cấp. Xem xét rằng các LLM thường có cấu trúc giá khác nhau (ví dụ, GPT-3.5-turbo rẻ hơn GPT-4 một bậc độ lớn), chiến lược này có tiềm năng giảm đáng kể chi phí bằng cách giảm thiểu việc sử dụng các LLM đắt tiền, trong khi vẫn giải quyết hiệu quả các truy vấn.

Minh họa giải pháp. Trong hầu hết các tình huống thực tế, các truy vấn từ người dùng sẽ xuất hiện tuần tự theo thời gian. Hệ thống của chúng tôi tận dụng thành công trong quá khứ để giúp các trợ lý LLM giải quyết các truy vấn tương lai. Cụ thể, bất cứ khi nào một truy vấn được coi là được giải quyết thành công bởi phản hồi của người dùng, chúng tôi thu thập và lưu trữ truy vấn và đoạn mã được tạo cuối cùng. Những cặp truy vấn-mã này được lưu trong một cơ sở dữ liệu vector chuyên biệt. Khi có truy vấn mới xuất hiện, EcoAssistant truy xuất truy vấn tương tự nhất từ cơ sở dữ liệu, sau đó được thêm cùng với mã liên quan vào prompt ban đầu cho truy vấn mới, phục vụ như một minh họa. Chúng tôi cho thấy rằng việc sử dụng các cặp truy vấn-mã thành công trong quá khứ này cải thiện quá trình giải quyết truy vấn với ít lặp lại hơn và tăng cường hiệu suất của hệ thống.

Phân cấp trợ lý và minh họa giải pháp như các thiết kế độc lập mang lại những lợi thế riêng biệt: phân cấp trợ lý có tiềm năng giảm chi phí bằng cách giảm thiểu sự phụ thuộc vào các LLM đắt tiền, và minh họa giải pháp có thể tăng cường hiệu suất của LLM bằng cách tận dụng thành công trong quá khứ. Cùng nhau, chúng khuếch đại các lợi ích riêng lẻ. Ngay cả khi không có thiết kế chuyên biệt, các trợ lý LLM mạnh hơn ngầm tư vấn cho những cái yếu hơn trong hệ thống phân cấp bằng cách chia sẻ giải pháp của họ thông qua cơ sở dữ liệu truy vấn-mã.

4 THÍ NGHIỆM

Trong phần này, chúng tôi tiến hành các thí nghiệm để điều tra hiệu suất và chi phí đô la của EcoAssistant trên nhiều loại truy vấn khác nhau, với cả đánh giá mô hình và đánh giá con người. Chúng tôi thực nghiệm cho thấy các lợi ích riêng lẻ được giới thiệu bởi phân cấp trợ lý và minh họa giải pháp, và EcoAssistant có thể vượt trội hơn một trợ lý GPT-4 riêng lẻ 10% tỷ lệ thành công với ít hơn một nửa chi phí của trợ lý GPT-4.

2Chúng tôi đặt số lượt tối đa của một cuộc trò chuyện là 5 trong bài báo này.

--- TRANG 4 ---
Bản thảo

4.1 THIẾT LẬP

Tập dữ liệu Chúng tôi xem xét ba tập dữ liệu từ ToolBench (Qin et al., 2023) có các truy vấn tương ứng với các miền Places, Weather, và Stock tương ứng. Chúng tôi lấy mẫu ngẫu nhiên 100 truy vấn từ mỗi tập dữ liệu. Mỗi tập dữ liệu đi kèm với một API được khuyến nghị sử dụng. Chúng tôi liệt kê một truy vấn ví dụ và API cho mỗi tập dữ liệu trong Bảng 1. Ngoài việc đánh giá các phương pháp trên từng tập dữ liệu riêng biệt, chúng tôi cũng xem xét một thiết lập nơi cả ba tập dữ liệu được kết hợp, tạo ra 300 truy vấn. Sau đó chúng tôi xáo trộn ngẫu nhiên 300 truy vấn để tạo ba tập dữ liệu với thứ tự truy vấn khác nhau và gọi chúng là Mixed-1, Mixed-2, và Mixed-3 tương ứng.

Bảng 1: API mặc định và truy vấn ví dụ cho mỗi tập dữ liệu.

Tập dữ liệu | API | Truy vấn ví dụ
Places | Google Places1 | Tôi đang tìm một hiệu thuốc mở 24 giờ ở Montreal, bạn có thể tìm cho tôi không?
Weather | Weather API2 | Độ che phủ mây hiện tại ở Mumbai, Ấn Độ là bao nhiêu?
Stock | Alpha Vantage Stock API3 | Bạn có thể cho tôi biết giá mở cửa của Microsoft trong tháng 1 năm 2023 không?

1https://developers.google.com/maps/documentation/places/web-service/overview
2https://www.weatherapi.com
3https://www.alphavantage.co/documentation/

Prompt và LLM Prompt ban đầu chứa truy vấn, tên/khóa API, và cặp truy vấn-mã được truy xuất khi minh họa giải pháp được sử dụng, và LLM phải dựa vào kiến thức nội bộ của chúng về API và kỹ năng lập trình để trả lời truy vấn. Để tránh rò rỉ khóa API bí mật cho các dịch vụ LLM, chúng tôi tạo ngẫu nhiên một chuỗi văn bản bốn byte duy nhất dưới dạng thập lục phân cho mỗi API làm khóa giả, sau đó bất cứ khi nào mã được thực thi, chúng tôi thay thế khóa giả trong mã bằng khóa API thực tế để mã có thể hoạt động như mong đợi. Chúng tôi trình bày prompt mà chúng tôi đã sử dụng trong các thí nghiệm ở Phụ lục D. Vì các kỹ thuật chúng tôi đề xuất là trực giao với các phương pháp prompting hiện có và có thể được áp dụng một cách hiệp đồng, chúng tôi cũng tiến hành các thí nghiệm sử dụng prompting Chain-of-Thought (CoT) (Wei et al., 2022). Trong nghiên cứu này, chúng tôi tập trung vào các LLM đối thoại bao gồm hai mô hình hộp đen với chi phí khác nhau (GPT-3.5-turbo và GPT-4) và một mô hình mã nguồn mở (LLAMA-2-13B-chat (Touvron et al., 2023)). Chúng tôi giả định chi phí đô la của LLAMA-2-13B-chat là không vì nó có thể được lưu trữ với một lượng tài nguyên tính toán hợp lý, trong khi ghi lại chi phí sử dụng dịch vụ LLM hộp đen.

Các phương pháp so sánh và triển khai Chúng tôi điều tra hiệu suất của ba trợ lý được hỗ trợ bởi các LLM đối thoại khác nhau (LLAMA-2-13B-chat, GPT-3.5-turbo, và GPT-4) cũng như hai loại phân cấp trợ lý: AssistantHier-G (GPT-3.5-turbo + GPT-4) và AssistantHier-L (LLAMA-2-13B-chat + GPT-3.5-turbo + GPT-4). Đối với mỗi trợ lý hoặc phân cấp trợ lý, chúng tôi bao gồm phiên bản vanilla và các biến thể sau: + CoT (với prompting Chain-of-Thought), + SolDemo (với minh họa giải pháp), và + CoT + SolDemo (với cả prompting Chain-of-thought và minh họa giải pháp). Chúng tôi kết hợp mỗi trợ lý hoặc phân cấp trợ lý với một tác nhân thực thi mã để giải quyết nhiệm vụ trả lời câu hỏi dựa trên mã. Lưu ý rằng hệ thống EcoAssistant được đề xuất bao gồm cả phân cấp trợ lý và minh họa giải pháp tức là AssistantHier-G/L (+ CoT) + SolDemo. Chúng tôi triển khai tất cả các hệ thống dựa trên AutoGen (Wu et al., 2023), một thư viện Python3 cho framework cuộc trò chuyện đa tác nhân. Đối với minh họa giải pháp, chúng tôi sử dụng Chroma (Chroma, 2023), một cơ sở dữ liệu embedding mã nguồn mở để lưu trữ các cặp truy vấn-mã; chúng tôi sử dụng mô hình multi-qa-mpnet-base-dot-v1 để embedding truy vấn người dùng và độ tương tự cosine cho tìm kiếm tương tự4.

Giao thức đánh giá Chúng tôi tập trung vào cả chi phí đô la và hiệu suất của các phương pháp so sánh. Đối với hiệu suất mô hình, chúng tôi báo cáo tỷ lệ thành công, tức là phần trăm các truy vấn được xử lý thành công bởi mô hình. Vì các truy vấn thường không có câu trả lời chuẩn (Qin et al., 2023), ví dụ, hỏi thời tiết vào một ngày nhất định, chúng tôi áp dụng cả đánh giá mô hình và đánh giá con người. Đối với đánh giá mô hình, chúng tôi tận dụng GPT-4 như một proxy của người dùng để đánh giá xem hệ thống có xử lý thành công truy vấn hay không (Zheng et al., 2023; Fu et al., 2023; Wang et al., 2023b). Cụ thể, sau khi cuộc trò chuyện kết thúc, chúng tôi prompt GPT-4 với toàn bộ lịch sử cuộc trò chuyện và hỏi nó xem hệ thống được kiểm tra có xử lý thành công truy vấn hay không. Chi tiết có thể được tìm thấy trong Phụ lục D. Chúng tôi lặp lại mỗi đánh giá ba lần với các hạt giống ngẫu nhiên khác nhau nếu không được chỉ định khác.

3https://github.com/microsoft/autogen
4https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1

--- TRANG 5 ---
Bản thảo

4.2 ĐÁNH GIÁ MÔ HÌNH: TẬP DỮ LIỆU RIÊNG LẺ

Đầu tiên, chúng tôi tiến hành các thí nghiệm trên từng tập dữ liệu riêng lẻ trong ba tập để điều tra hiệu suất của các hệ thống so sánh. Kết quả được trình bày trong Bảng 2. Chúng tôi tóm tắt các phát hiện của mình như dưới đây.

Bảng 2: Tỷ lệ thành công (%) và chi phí đô la trên tập dữ liệu Places, Weather, và Stock.

Phương pháp | Places | | Weather | | Stock | |
| | Tỷ lệ thành công (%) | Chi phí | Tỷ lệ thành công (%) | Chi phí | Tỷ lệ thành công (%) | Chi phí |
LLAMA-2-13B-chat | 27.00 | 0.00 | 6.33 | 0.00 | 6.67 | 0.00
+ CoT | 25.00 | 0.00 | 7.67 | 0.00 | 6.33 | 0.00
+ SolDemo | 56.00 | 0.00 | 6.00 | 0.00 | 31.33 | 0.00
+ CoT + SolDemo | 52.00 | 0.00 | 4.67 | 0.00 | 14.00 | 0.00
GPT-3.5-turbo | 39.33 | 0.47 | 46.00 | 0.41 | 17.00 | 0.36
+ CoT | 61.33 | 0.67 | 69.00 | 0.67 | 50.00 | 0.84
+ SolDemo | 77.33 | 0.49 | 79.67 | 0.54 | 68.00 | 0.50
+ CoT + SolDemo | 70.33 | 0.73 | 78.33 | 0.69 | 64.67 | 0.80
GPT-4 | 85.00 | 12.58 | 87.33 | 10.73 | 59.33 | 18.49
+ CoT | 78.67 | 15.16 | 75.67 | 11.67 | 57.67 | 19.01
+ SolDemo | 88.00 | 11.76 | 87.33 | 10.81 | 75.00 | 14.33
+ CoT + SolDemo | 87.33 | 13.75 | 84.67 | 11.30 | 77.67 | 15.52
AssistantHier-G | 89.33 | 8.61 | 90.67 | 6.21 | 64.33 | 15.99
+ CoT | 89.33 | 7.28 | 88.33 | 4.87 | 73.33 | 11.85
+ SolDemo | 96.67 | 3.73 | 95.00 | 3.04 | 81.67 | 8.10
+ CoT + SolDemo | 96.33 | 5.52 | 93.00 | 3.49 | 86.00 | 8.04
AssistantHier-L | 91.67 | 5.97 | 91.67 | 5.89 | 66.33 | 15.10
+ CoT | 91.33 | 5.89 | 89.00 | 4.36 | 75.33 | 11.01
+ SolDemo | 97.00 | 3.33 | 98.00 | 2.24 | 85.00 | 6.70
+ CoT + SolDemo | 95.33 | 3.52 | 96.33 | 2.82 | 84.33 | 6.86

Phát hiện 1: các LLM so sánh có hiệu suất khác biệt - mô hình càng đắt thì hiệu suất càng tốt. Từ kết quả, chúng ta có thể thấy rằng các LLM riêng lẻ (GPT-3.5-turbo, GPT-4, LLAMA-2-13B-chat) có hiệu suất không đồng nhất. Cụ thể, GPT-4 đạt tỷ lệ thành công cao nhất tiếp theo là GPT-3.5-turbo, trong khi LLAMA-2-13B-chat hoạt động kém hơn những cái khác. Mặt khác, GPT-4 có chi phí cao nhất, GPT-3.5-turbo tương đối hiệu quả về chi phí, và LLAMA-2-13B-chat có chi phí đô la bằng không vì nó có thể được lưu trữ trên máy cục bộ.

Phát hiện 2: prompting Chain-of-Thought (CoT) có thể tăng cường đáng kể LLM với hiệu suất vừa phải, tức là GPT-3.5-turbo. Prompting Chain-of-Thought (CoT) liên tục tăng cường hiệu suất của GPT-3.5-turbo trên các tập dữ liệu. Tuy nhiên, đối với GPT-4 và LLAMA-2-13B-chat, tỷ lệ thành công không nhất thiết được hưởng lợi từ CoT. Chúng tôi giả thuyết điều này có thể là do GPT-4 đã rất có năng lực, để lại ít chỗ cho CoT cải thiện thêm hiệu suất của nó, trong khi hiệu suất của LLAMA-2-13B-chat không được hưởng lợi từ CoT có thể do sự thiếu sót cố hữu trong việc giải quyết các nhiệm vụ trả lời câu hỏi dựa trên mã. Ngoài ra, chúng tôi thấy rằng CoT có xu hướng tăng chi phí đô la, vì nó khuyến khích mô hình suy nghĩ từng bước và do đó sẽ tăng số lượng token mà LLM nhập và xuất. Cuối cùng, khi so sánh AssistantHier-G với AssistantHier-G + CoT, cái sau dẫn đến việc giảm chi phí. Điều này được quy cho CoT tăng cường tỷ lệ thành công của GPT-3.5-turbo, do đó giảm sự phụ thuộc vào GPT-4 đắt hơn.

Phát hiện 3: minh họa giải pháp có thể tăng tỷ lệ thành công, đặc biệt khi phương pháp không hoạt động tốt. Trong hầu hết tất cả các trường hợp, minh họa giải pháp có thể cải thiện đáng kể tỷ lệ thành công mà không gây ra sự gia tăng đáng chú ý trong chi phí, đặc biệt đối với các LLM kém cạnh tranh hơn. Cụ thể, minh họa giải pháp gần như tăng gấp đôi tỷ lệ thành công của GPT-3.5-turbo vanilla trên tập dữ liệu Places và Weather, trong khi giới thiệu sự tăng 3 lần tỷ lệ thành công trên tập dữ liệu Stock. Đối với LLAMA-2-13B-chat, minh họa giải pháp cũng tăng tỷ lệ thành công trên tập dữ liệu Places và Stock với biên độ lớn. Tuy nhiên đối với mô hình hiệu suất cao GPT-4, minh họa giải pháp không dẫn đến sự tăng hiệu suất đáng kể ngoại trừ tập dữ liệu Stock nơi GPT-4 chỉ thể hiện tỷ lệ thành công khoảng 60%.

Phát hiện 4: So với GPT-4, phân cấp trợ lý có thể giảm đáng kể chi phí trong khi cải thiện nhẹ tỷ lệ thành công. Bằng cách so sánh phân cấp trợ lý (AssistantHier-G và AssistantHier-L) với GPT-4 vanilla, chúng ta có thể thấy rằng chi phí được giảm đáng kể; cụ thể, AssistantHier-G đạt được tiết kiệm chi phí khoảng 10%-30%, trong khi AssistantHier-L thực hiện giảm trong khoảng 15%-50%. Ngoài ra, phân cấp trợ lý cũng dẫn đến sự tăng nhẹ trong tỷ lệ thành công vì nó cho phép thử nhiều trợ lý cho một truy vấn duy nhất.

Phát hiện 5: EcoAssistant (phân cấp trợ lý + minh họa giải pháp) đạt được hiệu suất vượt trội với chi phí vừa phải. Cuối cùng, EcoAssistant (AssistantHier-G/L (+ CoT) + SolDemo) mang lại tỷ lệ thành công cao nhất trên tất cả các tập dữ liệu, vượt trội hơn các biến thể GPT-4 hiệu suất cao nhất khoảng 10 điểm phần trăm biên độ trong tỷ lệ thành công. Ngoài ra, sự kết hợp này dẫn đến việc giảm chi phí thêm khoảng 30%-50% khi so sánh với việc chỉ sử dụng phương pháp phân cấp trợ lý (AssistantHier-G/L (+ CoT)). Những phát hiện này đáng ngạc nhiên, với việc minh họa giải pháp chỉ có thể tăng cường tỷ lệ thành công khi được sử dụng riêng lẻ. Chúng tôi quy sự hiệp đồng này cho thực tế rằng các giải pháp được tạo bởi GPT-4 hiệu suất cao sau đó hướng dẫn các LLM rẻ hơn, mặc dù yếu hơn. Kết quả là, tiết kiệm chi phí xuất hiện, vì những LLM tiết kiệm hơn này có thể giải quyết một số lượng lớn hơn các nhiệm vụ sử dụng giải pháp GPT-4 làm minh họa, từ đó giảm thiểu sự phụ thuộc vào trợ lý GPT-4 đắt hơn.

--- TRANG 6 ---
Bản thảo

4.3 ĐÁNH GIÁ MÔ HÌNH: TẬP DỮ LIỆU HỖN HỢP

Chúng tôi cũng đánh giá các phương pháp trên các tập dữ liệu hỗn hợp: Mixed-1, Mixed-2, và Mixed-3. Mỗi cái trong số chúng bao gồm các truy vấn từ tất cả các tập dữ liệu riêng lẻ, được phân biệt bởi các thứ tự truy vấn khác nhau. Những thí nghiệm này điều tra cách các phương pháp hoạt động khi các truy vấn trải dài nhiều miền và thứ tự. Cụ thể, chúng tôi đánh giá sáu phương pháp trong thí nghiệm này: GPT-3.5-turbo, GPT-3.5-turbo + SolDemo, GPT-4, GPT-4 + SolDemo, AssistantHier-G, AssistantHier-G + SolDemo (EcoAssistant). Chúng tôi trực quan hóa kết quả trong Hình 2 để minh họa xu hướng quy mô của chi phí và số truy vấn thành công liên quan đến số truy vấn được xử lý trong thiết lập streaming.

Hình 2: Các đường cong của số truy vấn thành công / chi phí liên quan đến số truy vấn được xử lý trên các tập dữ liệu hỗn hợp. Ba tập dữ liệu bao gồm các truy vấn từ tất cả các tập dữ liệu riêng lẻ, được phân biệt bởi các thứ tự truy vấn khác nhau. Chúng ta có thể thấy rằng EcoAssistant (AssistantHier-G + SolDemo) dẫn đến hiệu suất tốt nhất trong khi duy trì chi phí tương đối thấp.

Từ kết quả, chúng tôi rút ra các kết luận sau. Đầu tiên, vì minh họa giải pháp dựa vào các giải pháp trong quá khứ và do đó có thể bị ảnh hưởng bởi thứ tự của các truy vấn, sau đó việc xác định tính mạnh mẽ của nó đối với các chuỗi truy vấn khác nhau là quan trọng; kết quả cho thấy rằng phương pháp phần lớn không phụ thuộc vào thứ tự, như được chứng minh bởi tính nhất quán trong các đường cong hiệu suất (X + SolDemo) trên các tập dữ liệu. Thứ hai, hai biến thể của GPT-3.5-turbo là hiệu quả nhất về chi phí vì các đường cong chi phí gần như phẳng so với các phương pháp khác, tuy nhiên GPT-3.5-turbo một mình hoạt động kém hơn tất cả các phương pháp so sánh về số lượng thành công; tuy nhiên, việc tích hợp minh họa giải pháp (GPT-3.5-turbo + SolDemo) nâng cao đáng kể hiệu quả của nó. Thứ ba, mặc dù hiệu suất tốt mà GPT-4 mang lại, nó là phương pháp tốn kém nhất như được chỉ ra bởi đường cong chi phí dốc hơn của nó; may mắn thay, phân cấp trợ lý (AssistantHier-G) sẽ giảm chi phí vì các đường cong chi phí của nó có độ dốc nhỏ hơn, mà không hy sinh hiệu suất. Cuối cùng, EcoAssistant (AssistantHier-G + SolDemo) thể hiện hiệu suất tốt nhất và đồng thời có đường cong chi phí phẳng hơn nhiều so với các phương pháp khác, ngoại trừ các biến thể GPT-3.5-turbo.

--- TRANG 7 ---
Bản thảo

4.4 ĐÁNH GIÁ CON NGƯỜI: TẬP DỮ LIỆU HỖN HỢP

Đối với đánh giá con người, chúng tôi lấy mẫu 100 truy vấn từ tất cả 300 truy vấn của các tập dữ liệu khác nhau để tạo thành một tập dữ liệu được gọi là Mixed-100. Chúng tôi thu thập một đoạn mã cho mỗi trong số 100 truy vấn, có thể tạo ra thông tin cần thiết để giải quyết truy vấn. Cụ thể, đối với mỗi truy vấn, chúng tôi thu thập tất cả các đoạn mã được tạo bởi LLM trong các thí nghiệm trước đó. Từ bộ sưu tập này, chúng tôi chọn và xác thực thủ công một đoạn (với các sửa đổi thủ công cần thiết) để đảm bảo rằng đoạn mã có thể có hiệu quả thu thập thông tin cần thiết. Do đó, có thể tham khảo đầu ra của đoạn mã vàng khi đánh giá xem mô hình có giải quyết thành công truy vấn hay không. Đối với tất cả các thí nghiệm trong phần này, chúng tôi áp dụng chiến lược này để thực hiện đánh giá con người.

Bảng 3: Kết quả đánh giá con người trên tập dữ liệu Mixed-100.

| | GPT-3.5-turbo | GPT-3.5-turbo + SolDemo | GPT-4 | GPT-4 + SolDemo | AssistantHier-G | AssistantHier-G + SolDemo |
|---|---|---|---|---|---|---|
**Kết quả chính** | | | | | |
Tỷ lệ thành công (%) | 25 | 45 | 59 | 78 | 63 | 80
Chi phí | 0.36 | 0.48 | 13.77 | 10.27 | 11.84 | 5.90
**Trung bình lời gọi mô hình mỗi truy vấn** | | | | | |
GPT-3.5-turbo | 2.42 | 2.91 | - | - | 2.42 | 2.92
GPT-4 | - | - | 3.12 | 2.57 | 2.51 | 1.25

Từ phần kết quả chính của Bảng 3, chúng ta có thể thấy rằng kết luận chính của chúng tôi vẫn đúng cho trường hợp đánh giá con người. Cụ thể, minh họa giải pháp có thể cải thiện đáng kể tỷ lệ thành công, và phân cấp trợ lý đóng góp vào việc tiết kiệm chi phí khi so sánh với GPT-4. Quan trọng hơn, EcoAssistant (AssistantHier-G + SolDemo) mang lại hiệu suất hàng đầu với chi phí vừa phải; khi so sánh nó với GPT-4, chúng ta có thể thấy rằng tỷ lệ thành công của nó cao hơn 10 điểm trong khi gây ra ít hơn một nửa chi phí của GPT-4. Để giải thích tốt hơn hiệu ứng của các kỹ thuật được đề xuất, chúng tôi cũng trình bày số lời gọi mô hình trung bình mỗi truy vấn cho mỗi phương pháp trong Bảng 3. Đầu tiên, minh họa giải pháp tăng lời gọi mô hình của GPT-3.5-turbo, vì GPT-3.5-turbo vanilla gặp khó khăn trong việc tạo ra mã được định dạng có thể được trích xuất và thực thi bởi trình thực thi mã, và do đó cuộc trò chuyện kết thúc sớm mà không có truy vấn được giải quyết thích hợp; với minh họa giải pháp, GPT-3.5-turbo có nhiều khả năng tạo mã được định dạng cho trình thực thi mã thực thi, do đó cuộc trò chuyện sẽ tiếp tục. Thứ hai, đối với GPT-4, minh họa giải pháp giảm số lời gọi mô hình vì nó hướng dẫn mô hình viết mã tốt ngay từ đầu, yêu cầu ít lượt hơn để tinh chỉnh mã để xuất thông tin cần thiết. Cuối cùng, so sánh AssistantHier-G với/không có SolDemo, chúng ta có thể thấy rằng số lời gọi mô hình trung bình của GPT-3.5-turbo tăng trong khi của GPT-4 giảm. Điều này chỉ ra rằng EcoAssistant (AssistantHier-G + SolDemo), ngay cả với tỷ lệ thành công cao hơn, phụ thuộc ít hơn vào GPT-4 đắt tiền vì GPT-3.5-turbo có thể giải quyết nhiều truy vấn hơn nhờ minh họa giải pháp, dẫn đến việc tiết kiệm chi phí.

4.5 ĐÁNH GIÁ CON NGƯỜI: HỆ THỐNG TỰ ĐỘNG KHÔNG CÓ PHẢN HỒI CON NGƯỜI

Trong các thí nghiệm trên, chúng tôi kết hợp một người dùng trong vòng lặp—hoặc là con người thực tế hoặc mô hình GPT-4—để xác định việc hoàn thành thành công của một truy vấn. Phản hồi này phục vụ ba chức năng chính: 1) tính toán tỷ lệ thành công của phương pháp được đánh giá; 2) để minh họa giải pháp quyết định có lưu trữ cặp truy vấn-mã hay không; và 3) để phân cấp trợ lý quyết định có gọi trợ lý tiếp theo trong hệ thống phân cấp hay không. Tuy nhiên, trong thực tế, người dùng có thể thích một hệ thống hoạt động tự động, không cần phản hồi của người dùng. Về điều này, chúng tôi xây dựng một hệ thống tự động cho mỗi phương pháp so sánh, không yêu cầu phản hồi con người. Cụ thể, chúng tôi thêm một người đánh giá GPT-4 để phục vụ các chức năng 2) và 3) nói trên, và sau khi tất cả các truy vấn được xử lý, chúng tôi đánh giá thủ công sự thành công của mỗi truy vấn để tính toán tỷ lệ thành công. Ngoài ra, vì bây giờ chúng tôi coi người đánh giá GPT-4 như một phần của hệ thống, chúng tôi cũng bao gồm chi phí của nó như chi phí hệ thống. Lưu ý rằng phương pháp không có minh họa giải pháp hoặc phân cấp trợ lý (ví dụ, GPT-3.5-turbo một mình) sẽ giữ nguyên như trước.

Chúng tôi đánh giá các hệ thống tự động này trên tập dữ liệu Mixed-100, áp dụng cùng chiến lược đánh giá con người như trong phần trước. Kết quả được chi tiết trong Bảng 4. Khi so sánh EcoAssistant (AssistantHier-G + SolDemo) với GPT-4, chúng tôi vẫn quan sát thấy sự tăng tỷ lệ thành công vượt quá 10 điểm và giảm chi phí hơn 50%. Tuy nhiên, trong khi EcoAssistant có chi phí tương đương với phần đối tác không tự động của nó (như được hiển thị trong Bảng 3), tỷ lệ thành công của nó giảm 8 điểm. Điều này là do người đánh giá GPT-4 thỉnh thoảng sẽ không tin tưởng GPT-3.5-turbo và sẽ không chuyển sang trợ lý GPT-4, dẫn đến sự sụt giảm hiệu suất; đồng thời, vì nó gọi trợ lý GPT-4 ít thường xuyên hơn phiên bản không tự động của nó, chi phí không tăng ngay cả khi tính đến chi phí của người đánh giá GPT-4.

Chúng tôi cũng trình bày thời gian chạy của các hệ thống tự động này trong Bảng 4. Chúng ta có thể thấy rằng minh họa giải pháp có thể giảm đáng kể thời gian chạy, chỉ ra rằng nó hợp lý hóa quá trình giải quyết truy vấn với ít lặp lại hơn. Ngoài ra, phân cấp trợ lý (AssistantHier-G) thể hiện thời gian chạy dài nhất, vì nó thường thử cả trợ lý GPT-3.5-turbo và GPT-4. Đáng chú ý, EcoAssistant cần ít hơn một nửa thời gian chạy của AssistantHier-G và thậm chí vượt trội hơn một trợ lý GPT-4 độc lập. Điều này nhấn mạnh hiệu ứng hiệp đồng của việc tích hợp phân cấp trợ lý và minh họa giải pháp, càng làm giảm sự phụ thuộc vào trợ lý GPT-4 có độ trễ cao hơn.

Bảng 4: Kết quả đánh giá con người trên Mixed-100 cho các hệ thống tự động.

| | GPT-3.5-turbo | GPT-3.5-turbo + SolDemo | GPT-4 | GPT-4 + SolDemo | AssistantHier-G | AssistantHier-G + SolDemo |
|---|---|---|---|---|---|---|
**Kết quả chính** | | | | | |
Tỷ lệ thành công (%) | 25 | 47 | 59 | 77 | 54 | 72
Chi phí | 0.36 | 2.46 | 13.77 | 12.07 | 12.99 | 5.78
**Trung bình lời gọi mô hình mỗi truy vấn** | | | | | |
GPT-3.5-turbo | 2.42 | 2.84 | - | - | 2.45 | 2.90
GPT-4 | - | - | 3.12 | 2.49 | 2.29 | 0.59
Thời gian chạy (s) | 2414 | 2073 | 5272 | 3873 | 8993 | 4033

5 NGHIÊN CỨU LIÊN QUAN

Ở đây, chúng tôi thảo luận ngắn gọn về công trình liên quan của các nỗ lực hiện tại xây dựng hệ thống/ứng dụng LLM dựa trên cuộc trò chuyện đa tác nhân và các công trình trước đây về triển khai LLM hiệu quả về chi phí.

Cuộc trò chuyện đa tác nhân dựa trên LLM Các tác nhân dựa trên LLM đã thu hút sự chú ý lớn từ cả người thực hành và nhà nghiên cứu (Xi et al., 2023; Wang et al., 2023c; Liu et al., 2023). Gần đây, đã có những nỗ lực hướng tới việc khai thác các cuộc trò chuyện đa tác nhân trong các ứng dụng dựa trên LLM để giải phóng tiềm năng của sự hợp tác giữa các tác nhân (Wu et al., 2023). Các ứng dụng ví dụ bao gồm hoàn thành nhiệm vụ hợp tác với nhiều tác nhân (Li et al., 2023; Hong et al., 2023; Qian et al., 2023; Talebirad & Nadiri, 2023) và tận dụng cuộc tranh luận đa tác nhân để khuyến khích tư duy phân kỳ (Liang et al., 2023) hoặc để cải thiện tính factual và lý luận (Du et al., 2023). Trong nghiên cứu này, chúng tôi tập trung vào việc khai thác framework cuộc trò chuyện đa tác nhân để giải quyết trả lời câu hỏi dựa trên mã với trọng tâm vào cả hiệu quả chi phí và hiệu suất.

Triển khai LLM hiệu quả về chi phí Vô số nỗ lực đã được dành cho việc triển khai LLM hiệu quả về chi phí. Hầu hết các nỗ lực hiện tại nhằm cải thiện hiệu quả thời gian/tính toán thông qua các kỹ thuật như lượng tử hóa mô hình (Yao et al., 2023) và tóm tắt prompt (Arefeen et al., 2023), nén (Mu et al., 2023), và batching (Lin et al., 2023), v.v. Ngược lại, chúng tôi tìm cách giảm chi phí đô la của việc sử dụng dịch vụ API LLM. Với mục tiêu tương tự, EcoOptiGen (Wang et al., 2023a) cố gắng giảm chi phí đô la cho tối ưu hóa siêu tham số của suy luận LLM và FrugalGPT (Chen et al., 2023) khám phá một số kỹ thuật để giảm chi phí đô la cho tạo văn bản một lượt, trong khi chúng tôi tập trung vào việc tận dụng LLM như tác nhân trong cuộc trò chuyện đa tác nhân một cách hiệu quả về chi phí.

--- TRANG 9 ---
Bản thảo

6 KẾT LUẬN

Trong nghiên cứu này, chúng tôi khám phá các ứng dụng LLM hợp lý và chính xác cho trả lời câu hỏi dựa trên mã. Chúng tôi giới thiệu EcoAssistant, một hệ thống dựa trên LLM được xây dựng trên framework cuộc trò chuyện hai tác nhân. Nó bao gồm một tác nhân trợ lý được hỗ trợ bởi LLM và một tác nhân thực thi mã, và dựa vào tương tác của chúng để giải quyết các truy vấn người dùng. EcoAssistant cũng bao gồm hai kỹ thuật đơn giản nhưng hiệu quả: phân cấp trợ lý, ưu tiên các LLM hiệu quả về chi phí, và minh họa giải pháp, tận dụng các giải pháp thành công trong quá khứ cho các truy vấn mới. Các đánh giá thực nghiệm của chúng tôi chứng minh rằng EcoAssistant có thể đồng thời giảm chi phí và tăng cường hiệu suất.

TÀI LIỆU THAM KHẢO

[Phần tài liệu tham khảo được giữ nguyên với định dạng gốc do chứa nhiều thông tin kỹ thuật cụ thể]

--- TRANG 10 ---
Bản thảo

A HẠN CHẾ VÀ CÔNG VIỆC TƯƠNG LAI

Hạn chế Đầu tiên, hệ thống của chúng tôi dựa vào một hệ thống phân cấp được xác định trước của các trợ lý LLM. Hệ thống phân cấp tĩnh như vậy có thể không phải lúc nào cũng tối ưu cho tất cả các truy vấn, và hệ thống có thể được hưởng lợi từ một cơ chế lựa chọn thích ứng hơn. Thứ hai, hệ thống của chúng tôi phụ thuộc vào cơ sở dữ liệu để lưu trữ các cặp truy vấn-mã thành công trong quá khứ, nó có thể trở thành nút thắt cổ chai khi xử lý hàng triệu truy vấn. Do đó, một số cơ chế cắt tỉa xóa các mục ít hữu ích hơn trong cơ sở dữ liệu có thể hữu ích khi số lượng truy vấn bùng nổ. Thứ ba, trong khi EcoAssistant cố gắng xử lý một loạt rộng các truy vấn, nó có thể không thành thạo với các truy vấn chuyên sâu hoặc thích hợp đòi hỏi kiến thức miền cấp độ chuyên gia. Thứ tư, trong khi EcoAssistant cố gắng xử lý một loạt rộng các truy vấn, nó có thể không thành thạo với các truy vấn chuyên sâu hoặc thích hợp đòi hỏi kiến thức miền cấp độ chuyên gia. Ngoài ra, bản chất trò chuyện qua lại, đặc biệt với nhiều tinh chỉnh lặp, có thể gây ra độ trễ, dẫn đến thời gian phản hồi dài hơn cho người dùng cuối. Cuối cùng, hệ thống có thể gặp khó khăn với các ngữ cảnh trò chuyện dài, đặc biệt với các giới hạn token của các LLM hiện tại. Điều này có thể ảnh hưởng đến chất lượng phản hồi trong các cuộc trò chuyện mở rộng.

Công việc tương lai Ở đây, chúng tôi liệt kê một số hướng cho công việc tương lai. 1) Phản hồi người dùng có thông tin: Trong nghiên cứu này, chúng tôi tận dụng phản hồi người dùng nhị phân chỉ ra xem truy vấn người dùng có được giải quyết thành công hay không. Việc kết hợp phản hồi người dùng có thông tin hơn để hướng dẫn cuộc trò chuyện trong EcoAssistant có thể cho phép hoàn thành nhiệm vụ có mục tiêu và hiệu quả hơn. 2) Nhiều tác nhân hơn trong hệ thống: Trong nghiên cứu này, chúng tôi bao gồm hai loại tác nhân trong hệ thống. Một hướng khám phá có thể là thêm nhiều tác nhân vào hệ thống để hoàn thành nhiệm vụ hợp tác tốt hơn. 3) Cơ chế truy xuất nâng cao: Phương pháp hiện tại truy xuất các giải pháp trong quá khứ dựa trên độ tương tự truy vấn. Khám phá các cơ chế truy xuất nâng cao hơn có thể tăng cường hiệu quả của minh họa giải pháp. 4) Tương tác đa phương thức: Mở rộng EcoAssistant để hỗ trợ tương tác đa phương thức, như giọng nói hoặc hình ảnh, có thể mở rộng khả năng áp dụng của nó và phục vụ cơ sở người dùng rộng hơn.

B ĐÁNH GIÁ MÔ HÌNH V.S. ĐÁNH GIÁ CON NGƯỜI

Ở đây, chúng tôi điều tra độ tin cậy của đánh giá mô hình (GPT-4). Cụ thể, chúng tôi coi việc truy vấn có được giải quyết thành công hay không như một nhiệm vụ phân loại nhị phân, và sử dụng kết quả đánh giá con người làm chuẩn để đánh giá hiệu quả của kết quả đánh giá mô hình. Chúng tôi đánh giá độ chính xác, precision, và recall của đánh giá mô hình sử dụng các truy vấn và kết quả đánh giá con người trong Phần 4.4. Kết quả có thể được tìm thấy trong Bảng 5; chúng tôi tách riêng kết quả dựa trên các phương pháp được sử dụng để xử lý các truy vấn. Từ kết quả, chúng ta có thể thấy rằng tất cả recall đều là 100%, có nghĩa là khi người đánh giá GPT-4 kết luận rằng trợ lý thất bại, nó thực sự thất bại, trong khi precision dao động từ 66% đến 84%, chỉ ra rằng vẫn còn chỗ để cải thiện. Vì các hệ thống tự động được mô tả trong Phần 4.5 dựa vào người đánh giá GPT-4 để đánh giá xem truy vấn người dùng có được giải quyết thành công hay không, một người đánh giá GPT-4 tốt hơn có khả năng đóng góp vào các hệ thống tự động tốt hơn, mà chúng tôi để dành cho công việc tương lai.

Bảng 5: Chúng tôi đánh giá hiệu quả của đánh giá mô hình sử dụng kết quả đánh giá con người làm chuẩn cho các thí nghiệm trong Phần 4.4. Chúng tôi thấy rằng đánh giá mô hình luôn đúng khi nó kết luận rằng trợ lý thất bại trong việc giải quyết truy vấn người dùng, vì tất cả recall đều là 100%, trong khi precision dao động từ 66% đến 84%, chỉ ra rằng đánh giá mô hình có thể cung cấp một mức độ tín hiệu nhất định về hiệu suất hệ thống.

| Metric | GPT-3.5-turbo | GPT-3.5-turbo + SolDemo | GPT-4 | GPT-4 + SolDemo | AssistantHier-G | AssistantHier-G + SolDemo |
|---|---|---|---|---|---|---|
Accuracy | 91.75 | 82.63 | 80.41 | 85.86 | 85.88 | 71.72
Precision | 75.76 | 72.58 | 75.32 | 84.78 | 72.09 | 66.12
Recall | 100 | 100 | 100 | 100 | 100 | 100

C TRỰC QUAN HÓA CUỘC TRÒ CHUYỆN TRỢ LÝ-TRÌNH THỰC THI MÃ

Ở đây, chúng tôi minh họa cuộc trò chuyện trợ lý-trình thực thi mã trong Hình 3. Cụ thể, chúng tôi kích hoạt cuộc trò chuyện với một truy vấn người dùng. Sau đó cuộc trò chuyện giữa trợ lý LLM và trình thực thi mã sẽ tiến hành tự động cho đến khi một điều kiện kết thúc được thỏa mãn.

--- TRANG 13 ---
Bản thảo

Hình 3: Trực quan hóa cuộc trò chuyện trợ lý-trình thực thi mã.

D CHI TIẾT TRIỂN KHAI

Phần cứng Tất cả các thí nghiệm chạy trên máy với CPU Intel(R) Xeon(R) E5-2678 v3 với bộ nhớ 512G và hai GPU NVIDIA RTX A6000 48G. Lưu ý rằng các GPU chỉ để host mô hình LLAMA-2-13b-chat, trong khi các mô hình họ GPT như dịch vụ API LLM không yêu cầu GPU.

Prompt và tin nhắn mặc định Đầu tiên, chúng tôi trình bày prompt mà chúng tôi sử dụng cho mỗi truy vấn người dùng trong Hình 4, nơi phần màu đỏ chỉ ra API được khuyến nghị cho LLM sử dụng, phần màu xanh là template để minh họa cặp truy vấn-mã được truy xuất khi minh họa giải pháp được áp dụng, và phần màu đen cuối cùng là truy vấn người dùng. Đối với prompting Chain-of-Thought, chúng tôi thêm một câu "Hãy suy nghĩ từng bước." ở cuối prompt. Sau đó, chúng tôi trình bày prompt hệ thống mà chúng tôi sử dụng cho đánh giá mô hình trong Hình 5. Cụ thể, chúng tôi tận dụng GPT-4 cho đánh giá mô hình và đặt prompt hệ thống như trong Hình 5. Sau đó chúng tôi prompt mô hình GPT-4 với toàn bộ lịch sử cuộc trò chuyện, theo template được nêu trong điểm đầu tiên của prompt hệ thống. Mô hình GPT-4 sẽ tạo ra một token chỉ số duy nhất (yes/no) như đánh giá xem truy vấn người dùng có được giải quyết thành công hay không. Cuối cùng, trong tác nhân thực thi mã, nếu có một khối mã trong phản hồi được tạo bởi trợ lý, trình thực thi mã sẽ tự động trích xuất và thực thi mã, và sau đó gửi lại kết quả thực thi; nếu không phát hiện khối mã nào, chúng tôi đặt tin nhắn mặc định "Reply TERMINATE if everything is done." làm phản hồi của trình thực thi mã để cho cuộc trò chuyện tiếp tục.

E NGHIÊN CỨU TRƯỜNG HỢP

Ở đây, chúng tôi trình bày một số nghiên cứu trường hợp về cách minh họa giải pháp dẫn đến việc hoàn thành truy vấn thành công. Cụ thể, chúng tôi tập trung vào trợ lý GPT-3.5-turbo, và so sánh cuộc trò chuyện có và không có minh họa giải pháp. Chúng tôi trình bày một truy vấn từ mỗi trong ba tập dữ liệu. Ánh xạ giữa các hình của cuộc trò chuyện và phương pháp/tập dữ liệu tương ứng trong Bảng 6.

--- TRANG 14 ---
Bản thảo

Hình 4: Prompt chúng tôi sử dụng cho mỗi truy vấn người dùng.

Hình 5: Prompt hệ thống cho đánh giá mô hình.

Bảng 6: Bảng tra cứu cho các nghiên cứu trường hợp.

| Tập dữ liệu | Phương pháp | Hình |
|---|---|---|
| Places | GPT-3.5-turbo | Hình 6 |
| | GPT-3.5-turbo + SolDemo | Hình 7 |
| Weather | GPT-3.5-turbo | Hình 8 |
| | GPT-3.5-turbo + SolDemo | Hình 9 & 10 |
| Stock | GPT-3.5-turbo | Hình 11 |
| | GPT-3.5-turbo + SolDemo | Hình 12 |

--- TRANG 15 đến 22 ---
[Các hình minh họa cuộc trò chuyện được mô tả trong tiêu đề tiếng Việt]

Hình 6: Trợ lý GPT-3.5-turbo thất bại trong việc sử dụng API do tham số sai.

Hình 7: Với minh họa giải pháp, trợ lý GPT-3.5-turbo thành công thu thập thông tin nhà hàng và trả lời truy vấn người dùng.

Hình 8: GPT-3.5-turbo thất bại trong việc làm cho mã hoạt động vì nó nhầm lẫn sử dụng khóa API của Google Places cho WeatherAPI.

Hình 9: Nửa đầu của GPT-3.5-turbo với minh họa giải pháp cho truy vấn "Bạn có thể cung cấp cho tôi thời gian lặn và mọc mặt trời ở Paris vào thứ Tư tới không".

Hình 10: Nửa sau của GPT-3.5-turbo với minh họa giải pháp cho truy vấn "Bạn có thể cung cấp cho tôi thời gian lặn và mọc mặt trời ở Paris vào thứ Tư tới không". Với minh họa giải pháp, GPT-3.5-turbo thành công tìm ra ngày chính xác và thu thập thông tin được truy vấn để trả lời truy vấn người dùng.

Hình 11: Trợ lý GPT-3.5-turbo thất bại trong việc tính toán sự tăng giá cổ phiếu do logic mã không chính xác: biến price_on_date không được sử dụng đúng cách.

Hình 12: Với minh họa giải pháp, trợ lý GPT-3.5-turbo thu thập thông tin cổ phiếu chính xác và trả lời truy vấn người dùng thành công.
