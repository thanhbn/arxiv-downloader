# 2112.09118.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2112.09118.pdf
# File size: 686136 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published in Transactions on Machine Learning Research (08/2022)
Unsupervised Dense Information Retrieval with
Contrastive Learning
Gautier Izacard♦,♣,♥gizacard@fb.com
Mathilde Caron♦,♥,♠mathilde@fb.com
Lucas Hosseini♦hoss@fb.com
Sebastian Riedel♦,/trianglesriedel@fb.com
Piotr Bojanowski♦bojanowski@fb.com
Armand Joulin♦ajoulin@fb.com
Edouard Grave♦egrave@fb.com
♦Meta AI Research,♣Ecole normale supérieure, PSL University,♥Inria,
♠Université Grenoble Alpes,/triangleUniversity College London
Reviewed on OpenReview: https: // openreview. net/ forum? id= jKN1pXi7b0
Abstract
Recently, information retrieval has seen the emergence of dense retrievers, using neural
networks, as an alternative to classical sparse methods based on term-frequency. These
models have obtained state-of-the-art results on datasets and tasks where large training
sets are available. However, they do not transfer well to new applications with no training
data, and are outperformed by unsupervised term-frequency methods such as BM25. In
this work, we explore the limits of contrastive learning as a way to train unsupervised dense
retrievers and show that it leads to strong performance in various retrieval settings. On the
BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for
the Recall@100. When used as pre-training before ﬁne-tuning, either on a few thousands
in-domain examples or on the large MS MARCO dataset, our contrastive model leads to
improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual
retrieval, where training data is even scarcer than for English, and show that our approach
leads to strong unsupervised performance. Our model also exhibits strong cross-lingual
transfer when ﬁne-tuned on supervised English data only and evaluated on low resources
language such as Swahili. We show that our unsupervised models can perform cross-lingual
retrieval between diﬀerent scripts, such as retrieving English documents from Arabic queries,
which would not be possible with term matching methods.
1 Introduction
Document retrieval is the task of ﬁnding relevant documents in a large collection to answer speciﬁc queries.
This is an important task by itself and a core component to solve many natural language processing (NLP)
problems, such as open domain question answering (Chen et al., 2017a) or fact checking (Thorne et al., 2018).
Traditionally, retrieval systems, or retrievers, leverage lexical similarities to match queries and documents,
using, for instance, TF-IDF or BM25 weighting (Robertson & Zaragoza, 2009). These approaches, based
on near-exact matches between tokens of the queries and documents, suﬀer from the lexical gap and do
not generalize well (Berger et al., 2000). By contrast, approaches based on neural networks allow learning
beyond lexical similarities, resulting in state-of-the-art performance on question answering benchmarks, such
as MS MARCO (Nguyen et al., 2016) or NaturalQuestions (Kwiatkowski et al., 2019).
The strong retrieval results of neural networks have been possible for domains and applications where
large training datasets are available. In the case of retrieval, creating these datasets requires manually
matching queries to the relevant documents in the collection. This is hardly possible when the collection
1arXiv:2112.09118v4  [cs.IR]  29 Aug 2022

--- PAGE 2 ---
Published in Transactions on Machine Learning Research (08/2022)
contains millions or billions of elements, resulting in many scenarios where only a few in-domain examples,
if any, are available. A potential solution is to train a dense retriever on a large retrieval dataset such as
MS MARCO, and then apply it to new domains, a setting referred to as zero-shot . Unfortunately, in this
setting, dense retrievers are often outperformed by classical methods based on term-frequency, which do not
require supervision (Thakur et al., 2021). Moreover large annotated datasets are generally not available
in languages other than English. Thus, using large collections of supervised data is not suitable to train
multilingual retrieval systems.
A natural alternative to transfer learning is unsupervised learning, which raises the following question: is it
possible to train dense retrievers without supervision, and match the performance of BM25? Training dense
retrievers without supervision can be achieved by using an auxiliary task that approximates retrieval. Given
a document, one can generate a synthetic query and then train the network to retrieve the original document,
among many others, given the query. The inverse Cloze task (ICT), proposed by Lee et al. (2019) to pre-train
retrievers, uses a given sentence as a query and predicts the context surrounding it. While showing promising
results as pre-training (Chang et al., 2020; Sachan et al., 2021), this approach still lags behind BM25 when
used as a zero-shot retriever. ICT is strongly related to contrastive learning (Wu et al., 2018), which has
been widely applied in computer vision (Chen et al., 2020; He et al., 2020). In particular, computer vision
models trained with the latest contrastive learning methods led to features well suited to retrieval (Caron
et al., 2021). We thus propose to revisit how well contrastive learning performs to train dense retrievers
without supervision.
In this paper, we make the following contributions. First, we show that contrastive learning can lead to
strong unsupervised retrievers: our model achieves Recall@100 results competitive with BM25 on most of the
BEIR benchmark. Second, in a few-shot setting, we show that our model beneﬁts from few training examples,
and obtains better results than transferring models from large datasets such as MS MARCO. Third, when
used as a pre-training method before ﬁne-tuning on MS MARCO, our technique leads to strong performance
on the BEIR benchmark. We perform ablations to motivate our design choices, and show that cropping
works better than the inverse Cloze task. Finally we train a multilingual dense retriever with contrastive
learning and show that it achieves state-of-the-art performance.
Code and pre-trained models are available here: https://github.com/facebookresearch/contriever .
2 Related work
In this section, we brieﬂy review relevant work in information retrieval, and application of machine learning
to this problem. This is not an exhaustive review, and we refer the reader to Manning et al. (2008), Mitra
et al. (2018) and Lin et al. (2020) for a more complete introduction to the ﬁeld.
Term-frequency based information retrieval. Historically, in information retrieval, documents and
queries are represented as sparse vectors where each element of the vectors corresponds to a term of the
vocabulary. Diﬀerent weighing schemes have been proposed, to determine how important a particular term is
to a document in a large dataset. One of the most used weighing scheme is known as TF-IDF, and is based
on inverse document frequency, or term speciﬁcity (Jones, 1972). BM25, which is still widely used today,
extends TF-IDF (Robertson et al., 1995). A well known limitation of these approaches is that they rely on
near-exact match to retrieve documents. This led to the introduction of latent semantic analysis (Deerwester
et al., 1990), in which documents are represented as low dimensional dense vectors.
Neural network based information retrieval. Following the successful application of deep learning
methods to natural language processing, neural networks techniques were introduced for information retrieval.
Huang et al. (2013) proposed a deep bag-of-words model, in which representations of queries and documents are
computed independently. A relevance score is then obtained by taking the dot product between representations,
and the model is trained end-to-end on click data from a search engine. This method was later reﬁned by
replacing the bag-of-words model by convolutional neural networks (Shen et al., 2014) or recurrent neural
network (Palangi et al., 2016). A limitation of bi-encoders is that queries and documents are represented
by a single vector, preventing the model to capture ﬁne-grained interactions between terms. Nogueira & Cho
2

--- PAGE 3 ---
Published in Transactions on Machine Learning Research (08/2022)
(2019) introduced a cross-encoder model, based on the BERT model (Devlin et al., 2019), which jointly
encodes queries and documents. The application of a strong pre-trained model, as well as the cross-encoder
architecture, lead to important improvement on the MS MARCO benchmark (Bajaj et al., 2016).
The methods described in the previous paragraph were applied to re-rank documents, which were retrieved
with a traditional IR system such as BM25. Gillick et al. (2018) ﬁrst studied whether continuous retrievers,
based on bi-encoder neural models, could be viable alternative to re-ranking. In the context of question
answering, Karpukhin et al. (2020) introduced a dense passage retriever (DPR) based on the bi-encoder
architecture. This model is initialized with a BERT network, and trained discriminatively using pairs of
queries and relevant documents, with hard negatives from BM25. Xiong et al. (2020) further extended this
work by mining hard negatives with the model itself during optimization, and trained on the MS MARCO
dataset. Once a collection of documents, such as Wikipedia articles, is encoded, retrieval is performed
with a fast k-nearest neighbors library such as FAISS Johnson et al. (2019). To alleviate the limitations of
bi-encoders, Humeau et al. (2019) introduces the poly-encoder architecture, where documents are encoded
by multiple vectors. Similarly, Khattab et al. (2020) proposes the ColBERT model, which keeps a vector
representation for each term of the queries and documents. To make the retrieval tractable, the term-level
function is approximated to ﬁrst retrieve an initial set of candidates, which are then re-ranked with the true
score. In the context of question answering, knowledge distillation has been used to train retrievers, either
using the attention scores of the reader of the downstream task as synthetic labels (Izacard & Grave, 2020a),
or the relevance score from a cross encoder (Yang & Seo, 2020). Luan et al. (2020) compares, theoretically and
empirically, the performance of sparse and dense retrievers, including bi-, cross- and poly-encoders. Dense
retrievers, such as DPR, can lead to indices weighing nearly 100GB when encoding document collections such
as Wikipedia. Izacard et al. (2020) shows how to compress such indices, with limited impact on performance,
making them more practical to use.
Self-supervised learning for NLP. Following the success of word2vec (Mikolov et al., 2013), many
self-supervised techniques have been proposed to learn representation of text. Here, we brieﬂy review the ones
that are most related to our approach: sentence level models and contrastive techniques. Jernite et al. (2017)
introduced diﬀerent objective functions to learn sentence representations, including next sentence prediction
and sentence order prediction. These objectives were later used in pre-trained models based on transformers,
such as BERT (Devlin et al., 2019) and AlBERT (Lan et al., 2019). In the context of retrieval, Lee et al.
(2019) introduced the inverse cloze task (ICT), whose purpose is to predict the context surrounding a span of
text. Guu et al. (2020) integrated a bi-encoder retriever model in a BERT pre-training scheme. The retrieved
documents are used as additional context in the BERT task, and the whole system is trained end-to-end in
an unsupervised way. Similarly, Lewis et al. (2020) proposed to jointly learn a retriever and a generative
seq2seq model, using self-supervised training. Chang et al. (2020) compares diﬀerent pre-training tasks for
retrieval, including the inverse cloze task. In the context of natural language processing, Fang et al. (2020)
proposed to apply MoCo where positive pairs of sentences are obtained using back-translation. Diﬀerent
works augmented the masked language modeling objective with a contrastive loss (Giorgi et al., 2020; Wu
et al., 2020; Meng et al., 2021). SBERT (Reimers & Gurevych, 2019) uses a Siamese network similar to
contrastive learning to learn a BERT-like model that is adapted to matching sentence embeddings. Their
formulation is similar to our work but requires aligned pairs of sentences to form positive pairs while we
propose to use data augmentation to leverage large unaligned textual corpora. Concurrent to this work, Gao
& Callan (2021) have also shown the potential of contrastive learning for information retrieval; building on the
same observation that both tasks share a similar structure. Spider (Ram et al., 2021), a contemporary work,
uses spans appearing multiple times in a document to create pseudo examples for contrastive learning in order
to train unsupervised retrievers. Finally, Chen et al. (2021) train a dense retriever to imitate unsupervised
lexical-based methods. This improves performance on a range of tasks and achieves state-of-the-art results
when combining the resulting dense retriever with Contriever, our model pre-trained with contrastive learning.
3 Method
In this section, we describe how to train a dense retriever with no supervision. We review the model
architecture and then describe contrastive learning — a key component of its training.
3

--- PAGE 4 ---
Published in Transactions on Machine Learning Research (08/2022)
The objective of a retriever is to ﬁnd relevant documents in a large collection for a given query. Thus, the
retriever takes as input the set of documents and the query and outputs a relevance score for each document.
A standard approach is to encode each query-document pair with a neural network (Nogueira & Cho, 2019).
This procedure requires re-encoding every document for any new query and hence does not scale to large
collections of documents. Instead, we follow standard approaches (Huang et al., 2013; Karpukhin et al.,
2020) in information retrieval and use a bi-encoder architecture where documents and queries are encoded
independently. The relevance score between a query and a document is given by the dot product between their
representations after applying the encoder. More precisely, given a query qand document d, we encode each
of them independently using the same model, fθ, parameterized by θ. The relevance score s(q,d)between a
queryqand a document dis then the dot product of the resulting representations:
s(q,d) =/angbracketleftfθ(q),fθ(d)/angbracketright.
In practice, we use a transformer network for fθto embed both queries and documents. Alternatively, two
diﬀerent encoders can be used to encode queries and documents respectively as in DPR (Karpukhin et al.,
2020). Empirically, we observed that using the same encoder, such as in Xiong et al. (2020) and Reimers
& Gurevych (2019), generally improves robustness in the context of zero-shot transfer or few-shot learning,
while having no impact on other settings. Finally, the representation fθ(q)(resp.fθ(d)) for a query (resp.
document) is obtained by averaging the hidden representations of the last layer. Following previous work on
dense retrieval with neural networks, we use the BERT base uncased architecture and refer the reader to
Devlin et al. (2019) for more details.
3.1 Unsupervised training on unaligned documents
In this section, we describe our unsupervised training method. We brieﬂy review the loss function traditionally
used in contrastive learning and also used in ICT (Lee et al., 2019). We then discuss obtaining positive pairs
from a single text document, a critical ingredient for this training paradigm.
3.1.1 Contrastive learning
Contrastive learning is an approach that relies on the fact that every document is, in some way, unique. This
signal is the only information available in the absence of manual supervision. A contrastive loss is used to
learn by discriminating between documents. This loss compares either positive (from the same document) or
negative (from diﬀerent documents) pairs of document representations. Formally, given a query qwith an
associated positive document k+, and a pool of negative documents (ki)i=1..K, the contrastive InfoNCE loss
is deﬁned as:
L(q,k +) =−exp(s(q,k +)/τ)
exp(s(q,k +)/τ) +/summationtextK
i=1exp(s(q,ki)/τ), (1)
whereτis a temperature parameter. This loss encourages positive pairs to have high scores and negative pairs
to have low scores. Another interpretation of this loss function is the following: given the query representation
q, the goal is to recover, or retrieve, the representation k+corresponding to the positive document, among all
the negatives ki. In the following, we refer to the left-hand side representations in the score sas queries and
the right-hand side representations as keys.
3.1.2 Building positive pairs from a single document
A crucial element of contrastive learning is how to build positive pairs from a single input. In computer vision,
this step relies on applying two independent data augmentations to the same image, resulting in two “views”
that form a positive pair (Wu et al., 2018; Chen et al., 2020). While we consider similar independent text
transformation, we also explore dependent transformations designed to reduce the correlation between views.
Inverse Cloze Task is a data augmentation that generates two mutually exclusive views of a document,
introduced in the context of retrieval by Lee et al. (2019). The ﬁrst view is obtained by randomly sampling a
span of tokens from a segment of text, while the complement of the span forms the second view. Speciﬁcally,
given a sequence of text (w1,...,wn), ICT samples a span (wa,...,wb), where 1≤a≤b≤n, uses the
4

--- PAGE 5 ---
Published in Transactions on Machine Learning Research (08/2022)
Trec-COVID Tóuche-2020NFCorpusScidocsDBPedia
Climate-feverFiQA
CQADupStackMSMARCOHotpotQANQ
ArguAnaScifactFever QuoraAvg.050100Recall@100REALM SimCSE BM25 Contriever
Figure 1: Unsupervised retrieval. We compare our pre-training without using anyannotated data to
REALM (Guu et al., 2020), SimCSE (Gao et al., 2021) and BM25. For SimCSE we report results of the
model using RoBERTa large. REALM uses annotated entity recognition data for training. We highlight that
our unsupervised pre-training is on par with BM25 but on 2 datasets.
tokens of the span as the query and the complement (w1,...,wa−1,wb+1,...,wn)as the key. In the original
implementation by Lee et al. (2019) the span corresponds to a sentence, and is kept in the document 10% of
the time to encourage lexical matching. The Inverse Cloze Task is closely related to the Cloze task which
uses the span complement (w1,...,wa−1,wb+1,...,wn)as the query.
Independent cropping is a common independent data augmentation used for images where views are
generated independently by cropping the input. In the context of text, cropping is equivalent to sampling a
span of tokens. This strategy thus samples independently two spans from a document to form a positive
pair. As opposed to the inverse Cloze task, in cropping both views of the example correspond to contiguous
subsequence of the original data. A second diﬀerence between cropping and ICT is the fact that independent
random cropping is symmetric: both the queries and documents follow the same distribution. Independent
cropping also lead to overlap between the two views of the data, hence encouraging the network to learn
exact matches between the query and document, in a way that is similar to lexical matching methods like
BM25. In practice, we can either ﬁx the length of the span for the query and the key, or sample them.
Additional data augmentation. Finally, we also consider additional data augmentations such as random
word deletion, replacement or masking. We use these perturbations in addition to random cropping.
3.1.3 Building large set of negative pairs
An important aspect of contrastive learning is to sample a large set of negatives. Most standard frameworks
diﬀer from each other in terms of how the negatives are handled, and we brieﬂy describe two of them, in-batch
negative sampling and MoCo, that we use in this work.
Negatives within a batch. A ﬁrst solution is to generate the negatives by using the other examples from
the same batch: each example in a batch is transformed twice to generate positive pairs, and we generate
negatives by using the views from the other examples in the batch. We will refer to this technique as “in-batch
negatives”. In that case, the gradient is back-propagated through the representations of both the queries
and the keys. A downside of this approach is that it requires extremely large batch sizes to work well Chen
et al. (2020), with Qu et al. (2021) reporting improvement in the context of information retrieval up to 8192
negatives. This method has been widely used to train information retrieval models with supervised data Chen
et al. (2017b); Karpukhin et al. (2020) and was also considered when using ICT to pre-train retrievers by Lee
et al. (2019).
5

--- PAGE 6 ---
Published in Transactions on Machine Learning Research (08/2022)
Negative pairs across batches. An alternative approach is to store representations from previous batches
in a queue and use them as negative examples in the loss (Wu et al., 2018). This allows for smaller batch
size but slightly changes the loss by making it asymmetric between “queries” (one of the view generated
from the elements of the current batch), and “keys” (the elements stored in the queue). Gradient is only
backpropagated through the “queries”, and the representation of the “keys” are considered as ﬁxed. In
practice, the features stored in the queue from previous batches comes form previous iterations of the network.
This leads to a drop of performance when the network rapidly changes during training. Instead, He et al.
(2020) proposed to generate representations of keys from a second network that is updated more slowly. This
approach, called MoCo, considers two networks: one for the keys, parametrized by θk, and one of the query,
parametrized by θq. The parameters of the query network are updated with backpropagation and stochastic
gradient descent, similarly to when using in-batch negatives, while the parameters of the key network, or
Momentum encoder, is updated from the parameters of the query network by using a exponential moving
average:
θk←mθk+ (1−m)θq, (2)
wheremis the momentum parameter that takes its value in [0,1].
4 Experiments
In this section, we empirically evaluate our best retriever trained with contrastive learning, called Contriever
(contrastive retriever), which uses MoCo with random cropping. We use a contrastive learning procedure that
diﬀers from ICT (Lee et al., 2019) mainly in three aspects. First, positive pairs are sampled using random
cropping and tokens from each element of the pair are deleted with a probability of 10%. Second we use
MoCo where negatives consists of elements from previous batches stored in a queue. This allows to scale to a
large number of negatives. Third we use data from Wikipedia and CCNet (Wenzek et al., 2020) for training.
Ablation studies motivating these technical choices are performed in Section 6. More technical details about
our model are given in Appendix A.1.
4.1 Datasets
Contriever is trained with contrastive learning on documents sampled from a mix between Wikipedia data
and CCNet data (Wenzek et al., 2020), where half the batches are sampled from each source.
First, we evaluate our model on two question answering datasets: NaturalQuestions (Kwiatkowski et al.,
2019) and TriviaQA (Joshi et al., 2017). For both datasets, we use the open domain versions as introduced
by Lee et al. (2019), and the English Wikipedia dump from Dec. 20, 2018 as the collection of documents to
retrieve from. We report the top-k retrieval accuracy, i.e.the number of questions for which at least one of
the top-k passages contain the answer.
Second, we use the BEIR benchmark, introduced by Thakur et al. (2021), which contains 18 retrieval datasets,
corresponding to nine tasks, such as fact checking or citation prediction, and covering diﬀerent domains, such
as Wikipedia or scientiﬁc publications. Most datasets from BEIR do not contain a training set, and the focus
of the benchmark is zero-shot retrieval . However, most machine learning based retrievers are still trained
on supervised data, such as the large scale retrieval dataset MS MARCO (Bajaj et al., 2016). Following
standard practice, we report two metrics on this benchmark: nDCG@10 and Recall@100. The nDCG@10
focuses on the ranking of the top 10 retrieved documents, and is good at evaluating rankings returned to
humans, for example in a search engine. On the other hand, Recall@100 is relevant to evaluate retrievers that
are used in machine learning systems, such as question answering. Indeed, such models can process hundreds
of documents, and ignore their ranking (Izacard & Grave, 2020b). While nDCG@10 is the main metric of
BEIR, we are more interested in the Recall@100 to evaluate bi-encoders, as our goal is to develop retrievers
that can be used in ML systems. Moreover, in many settings, retrieved documents can be re-ranked with a
more powerful model such as a cross-encoder, thus improving the nDCG@10.
6

--- PAGE 7 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 1:Unsupervised recall@k on the test sets of NaturalQuestions and TriviaQA. For Inverse Cloze
Task and Masked Salient Spans we report the results of Sachan et al. (2021). The Masked Salient Spans
model uses annotated named entity recognition data. For BM25 we report the results of Ma et al. (2021)
NaturalQuestions TriviaQA
R@5 R@20 R@100 R@5 R@20 R@100
Inverse Cloze Task (Sachan et al., 2021) 32.3 50.9 66.8 40.2 57.5 73.6
Masked salient spans (Sachan et al., 2021) 41.7 59.8 74.9 53.3 68.2 79.4
BM25 (Ma et al., 2021) - 62.9 78.3 - 76.4 83.2
Contriever 47.867.882.1 59.474.283.2
supervised model: DPR (Karpukhin et al., 2020) - 78.4 85.4 - 79.4 85.0
supervised model: FiD-KD (Izacard & Grave, 2020a) 73.8 84.3 89.3 77.0 83.6 87.7
4.2 Baselines
First, we compare Contriever to BM25, which does not require supervision. On QA datasets, we compare to
dense retrievers trained with ICT and the Masked Salient Spans from Sachan et al. (2021). On BEIR, we
consider the retriever from REALM (Guu et al., 2020), and RoBERTa large ﬁne-tuned with SimCSE (Gao
etal.,2021), asunsuperviseddenseretrievers. WealsocomparetoML-basedretrieverstrainedonMSMARCO,
classiﬁed in three categories: sparse, dense and late-interaction. For sparse methods, we compare to Splade
v2(Formal et al., 2021), which computes sparse representations of documents with BERT pre-trained
model. For dense methods, we use DPR(Karpukhin et al., 2020) and ANCE(Xiong et al., 2020), which
are bi-encoders trained on supervised data such as NaturalQuestions or MS MARCO. We also compare
toTAS-B(Hofstätter et al., 2021), which performs distillation from a cross-encoder to a bi-encoder, and
GenQ, which creates synthetic query-document pairs with a generative model.1For late-interaction, we use
ColBERT (Khattab et al., 2020), which computes pairwise scores between contextualized representations of
queries and documents, as well as a cross-encoder used to re-rank documents retrieved with BM25.
4.3 Results
First, we compare the performance of fully unsupervised models, i.e., without ﬁne-tuning on MS MARCO or
other annotated data. In Table 1, we report the retrieval performance on two question answering datasets:
NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Here, our model is competitive
with a strong BM25 baseline (Ma et al., 2021), for example leading to 3 points improvement for the recall@100
on NaturalQuestions. It also outperforms previously proposed dense retrievers which were trained with ICT
or salient span masking. In Figure 1 we report the recall@100 performance of unsupervised models on the
BEIR benchmark. Interestingly, we observe that in this setting, Contriever is competitive compared to BM25
on all datasets, but TREC-COVID and Tóuche-2020. In particular, it obtains better performance than BM25
on 11 out of 15 datasets from the benchmark for the recall@100. Contriever also outperforms previously
proposed unsupervised dense retrievers, which obtains lower performance than BM25 in general. For the
nDCG@10, which puts more emphasis on the very ﬁrst retrieved documents, while Contriever largely closes
the gap between unsupervised retrievers and BM25, it is still outperformed by BM25 as reported in Table 11.
The diﬀerence is mainly due to the fact that BM25 largely outperforms Contriever on two datasets with
speciﬁc features: Trec-COVID and Tóuche-2020. Trec-COVID is an information retrieval dataset related to
COVID. However data used to train Contriever were collected before the COVID outbreak, thus they may
not be adapted. Tóuche-2020 contains long documents, which does not seem to be very well supported by
dense neural retrievers: even after supervised training, models are still lagging behind BM25. Overall, these
results show the potential of contrastive learning to train fully unsupervised dense retrievers.
1GenQ thus leads to one diﬀerent model for each dataset.
7

--- PAGE 8 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 2:BEIR Benchmark. We report nDCG@10 on the test sets from the BEIR benchmark for bi-encoder
methods without re-ranker. We also report the average and number of datasets where a method is the best
(“Best on”) over the entire BEIR benchmark (excluding three datasets because of their licence). Bold is the
best overall. MS MARCO is excluded from the average. “CE” refers to cross-encoder.
BM25 BM25+CE DPR ANCE TAS-B Gen-Q ColBERT Splade v2 OursOurs+CE
MS MARCO 22.8 41.3 17.7 38.8 40.8 40.8 40.1 43.3 40.7 47.0
Trec-COVID 65.6 75.7 33.2 65.4 48.1 61.9 67.7 71.0 59.6 70.1
NFCorpus 32.5 35.0 18.9 23.7 31.9 31.9 30.5 33.4 32.8 34.4
NQ 32.9 53.3 47.4 44.6 46.3 35.8 52.4 52.1 49.8 57.7
HotpotQA 60.3 70.7 39.1 45.6 58.4 53.4 59.3 68.4 63.8 71.5
FiQA 23.6 34.7 11.2 29.5 30.0 30.8 31.7 33.6 32.9 36.7
ArguAna 31.5 31.1 17.5 41.5 42.9 49.3 23.3 47.9 44.6 41.3
Touche-2020 36.7 27.1 13.1 24.0 16.2 18.2 20.2 36.4 23.0 29.8
CQADupStack 29.9 37.0. 15.3 29.6 31.4 34.7 35.0 - 34.5 37.7
Quora 78.9 82.5 24.8 85.2 83.5 83.0 85.4 83.8 86.5 82.4
DBPedia 31.3 40.9 26.3 28.1 38.4 32.8 39.2 43.5 41.3 47.1
Scidocs 15.8 16.6 7.7 12.2 14.9 14.3 14.5 15.8 16.5 17.1
FEVER 75.3 81.9 56.2 66.9 70.0 66.9 77.1 78.6 75.8 81.9
Climate-FEVER 21.3 25.3 14.8 19.8 22.8 17.5 18.4 23.5 23.7 25.8
Scifact 66.5 68.8 31.8 50.7 64.3 64.4 67.1 69.3 67.7 69.2
Avg. w/o CQA 44.0 49.5 26.3 41.3 43.7 43.1 45.1 50.6 47.5 51.2
Avg. 43.0 48.6 25.5 40.5 42.8 42.5 44.4 - 46.6 50.2
Best on 1 3 0 0 0 1 0 1 1 9
Table 3:Few-shot retrieval. Test nDCG@10 after training on a small in-domain training set. We compare
BERT and our model, with and without an intermediate ﬁne-tuning step on MS MARCO. Note that our
unsupervised pre-training alone outperforms BERT with intermediate MS MARCO ﬁne-tuning.
Additional data SciFact NFCorpus FiQA
# queries 729 2,590 5,500
BM25 - 66.5 32.5 23.6
BERT - 75.2 29.9 26.1
Contriever - 84.0 33.6 36.4
BERT ms marco 80.9 33.2 30.9
Contriever ms marco 84.8 35.8 38.1
Next, we report nDCG@10 on the BEIR benchmark for diﬀerent retrievers trained on MS MARCO in Table 2
(recall@100 can be found in Table 10 of appendix). We individually report results on each dataset as well as
the average over 14 datasets of the BEIR Benchmark (excluding 3 for license reasons). We observe that when
used as pre-training, contrastive learning leads to strong performance: contriever obtains the best results
among dense bi-encoder methods for the nDCG@10, and is state-of-the-art for the recall@100 (improving the
average recall@100 from 65.0 to 67.1). This strong recall@100 performance can be further exploited by using
a cross-encoder2to re-rank the retrieved documents: this leads to the state-of-the-art on 8 datasets of the
BEIR benchmark for the nDCG@10, as well as on average. It should be noted that our ﬁne-tuning procedure
on MS MARCO is simpler than for other retrievers, as we use a simple strategy for negative mining and do
not use distillation. Our model would probably also beneﬁts from improvements proposed by these retrievers,
but this is beyond the scope of this paper.
Finally, we illustrate the beneﬁt of our retriever compared to BM25 in a few-shot setting, where we have
access to a small number of in-domain retrieval examples. This setting is common in practice, and lexical
based methods, like BM25, cannot leverage the small training sets to adapt its weights. In Table 3, we report
nDCG@10 on three datasets from BEIR associated with the smallest training sets, ranging from 729 to
2We use the existing ms-marco-MiniLM-L-6-v2 cross-encoder model to perform the re-ranking.
8

--- PAGE 9 ---
Published in Transactions on Machine Learning Research (08/2022)
5,500 queries. We observe that on these small datasets, our pre-training leads to better results than BERT
pre-training, even when BERT is ﬁne-tuned on MS MARCO as an intermediate step. Our pre-trained model
also outperforms BM25, showing the advantage of dense retriever over lexical methods in the few-shot setting.
More details are given in Appendix A.3.
5 Multilingual retrieval
In this section, we illustrate another advantage of learning unsupervised dense retrievers, when performing
multi-lingual retrieval. While large labeled datasets are available in English, allowing to train strong dense
retrievers (as shown in previous sections), this is unfortunately not the case for lower resources languages.
Here, we show how unsupervised training is a promising direction. First, we show that our approach leads to
strong performance, either in a full unsupervised setting, or by ﬁne-tuning a multi-lingual model on English
data such as MS MARCO. Second, we demonstrate that our model can also perform cross-lingual retrieval,
by retrieving English documents from other languages queries. Unsupervised retrievers based on lexical
matching, such as BM25, would obtain poor performance, especially for pairs of languages with diﬀerent
scripts such as English and Arabic.
5.1 Multilingual pre-training
Our multilingual model, called mContriever , is jointly pre-trained on 29 languages. The multilingual pre-
training largely follows the method discussed in previous sections, but it diﬀers by few particularities related
to the pre-training data and the hyperparameters used. The model is initialized with the multilingual version
of BERT, mBERT, trained on 104 languages. For the pre-training data, we consider the languages contained
in CCNet (Wenzek et al., 2020) that are also part of our evaluation datasets. This results in a training set
containing the CCNet data for 29 languages detailed in Table 12. During pre-training, examples are uniformly
sampled over languages, i.e. the probability that a training sample comes from a speciﬁc language is the same
for all languages. We observed that increasing the number of languages considered for pre-training generally
deteriorates performance as reported in Appendix B.3 similarly to what has been observed for multilingual
masked language models (Conneau et al., 2019). We pre-trained our multilingual mContriever with a queue
size of 32768. This generally improves stability, and is able to compensate for the additional instabilities
observed in the multilingual setting. More detailed hyperparameters are given in Appendix B.1.
5.2 Fine-tuning
Large labeled datasets for information retrieval are generally available only in English. It is therefore natural
to investigate whether large monolingual datasets can be leveraged for multilingual retrieval. We consider
ﬁne-tuning our pre-trained mContriever model on MS MARCO. This generally improves performance in all
languages. The model trained on MS MARCO can be further ﬁne-tuned on Mr. TyDi achieving state-of-the-art
performance on this dataset. Further details regarding ﬁne-tuning are given in Appendix B.2.
5.3 Evaluation
We evaluate the performance of our pre-trained model with and without ﬁne-tuning on English data on
two diﬀerent benchmarks. First, we consider Mr. TyDi (Zhang et al., 2021), a multilingual information
retrieval benchmark derived from TyDi QA (Clark et al., 2020). Given a question, the goal is to ﬁnd relevant
documents in a pool of Wikipedia documents in the same language.
In Mr. TyDi the pool of documents is restricted to the language of the query. Being able to retrieve relevant
documents from another language is desirable to leverage large source of information that may no be available
in all languages. In order to evaluate cross-lingual retrieval performance we derive an evaluation setting
from MKQA (Longpre et al., 2020). Given a question in a speciﬁc language, we perform retrieval in English
Wikipedia, and evaluate if the English answer is in the retrieved documents. The MKQA dataset makes this
possible by providing the same questions and answers in 26 languages. We remove unanswerable questions,
questions accepting a binary yes/no answer and questions with long answers from the original MKQA dataset.
9

--- PAGE 10 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 4:Multilingual retrieval on Mr. TyDi. We report MRR@100 and Recall@100 on the test sets of
Mr. TyDi. mContriever ﬁne-tuned on MS MARCO is compared against its counterparts without contrastive
pre-training using a similar ﬁne-tuning recipe, referred to as mBERT + MS MARCO , as well as a model
initialized with XLM-R referred to as XLM-R + MS MARCO . We also report the results after ﬁne-tuning on
Mr. TyDi for the model trained on MS MARCO.
ar bn en ﬁ id ja ko ru sw te th avg
MRR@100
BM25 (Zhang et al., 2021) 36.7 41.3 15.1 28.8 38.2 21.7 28.1 32.9 39.6 42.4 41.7 33.3
mDPR (Zhang et al., 2021) 26.0 25.8 16.2 11.3 14.6 18.1 21.9 18.5 7.3 10.6 13.5 16.7
Hybrid (Zhang et al., 2021) 49.1 53.5 28.4 36.5 45.5 35.5 36.2 42.7 40.5 42.0 49.2 41.7
mBERT + MS MARCO 34.8 35.1 25.7 29.6 36.3 27.1 28.1 30.0 37.4 39.6 20.3 31.3
XLM-R + MS MARCO 36.5 41.7 23.0 32.7 39.2 24.8 32.2 29.3 35.1 54.7 38.5 35.2
mContriever 27.336.39.221.123.519.522.317.538.322.537.225.0
+ MS MARCO 43.442.327.135.142.632.434.236.151.237.440.238.4
+ Mr. Tydi 72.4 67.2 56.6 60.2 63.0 54.9 55.3 59.7 70.7 90.3 67.3 65.2
Recall@100
BM25 (Zhang et al., 2021) 80.0 87.4 55.1 72.5 84.6 65.6 79.7 66.0 76.4 81.3 85.3 74.3
mDPR (Zhang et al., 2021) 62.0 67.1 47.5 37.5 46.6 53.5 49.0 49.8 26.4 35.2 45.5 47.3
Hybrid (Zhang et al., 2021) 86.3 93.7 69.6 78.8 88.7 77.8 70.6 76.0 78.6 82.7 87.5 80.9
mBERT + MS MARCO 81.1 88.7 77.8 74.2 81.0 76.1 66.7 77.6 74.1 89.5 57.8 76.8
XLM-R + MS MARCO 79.9 84.2 73.1 81.6 87.4 70.9 71.1 74.1 73.9 91.2 89.5 79.7
mContriever 82.089.648.879.681.472.866.268.588.780.890.377.2
+ MS MARCO 88.791.477.288.189.881.778.283.891.496.690.587.0
+ Mr. Tydi 94.0 98.6 92.2 92.7 94.5 88.8 88.9 92.4 93.7 98.9 95.2 93.6
This results in an evaluation set of 6619 queries. It should be noted that methods based on term matching
such as BM25 are intrinsically limited in this cross-lingual retrieval setting because similar terms in diﬀerent
languages may not match.
5.4 Baselines
On Mr. TyDi (Zhang et al., 2021) we report results from the original paper. This includes a BM25 baseline,
a model ﬁne-tuned on NaturalQuestions using the DPR pipeline, and an hybrid model combining the two.
On our cross-lingual evaluation benchmark derived from MKQA, we consider the retriever of the CORA
question answering pipeline (Asai et al., 2021), trained on a combination of datasets containing the English
NaturalQuestions and the cross-lingual XOR-TyDi QA, with data augmentation based on translation.
Additionally, to isolate the eﬀect of contrastive pre-training, we also compare mContriever ﬁne-tuned on
MS MARCO to its counterparts without contrastive pre-training, initialized from mBERT. This model is
referred as mBERT + MSMARCO in tables. We also report results obtained by ﬁne-tuning XLM-R (Conneau
et al., 2019) on MS-MARCO. For both models we use the same hyper-parameters used to ﬁne-tune mContriever
on MS MARCO except for the temperature, additional details are reported in Appendix B.2.
5.5 Results
We report results on Mr. TyDi in Table 4. The eﬀectiveness of the multilingual pre-training appears clearly as
the pre-trained model ﬁne-tuned on MS MARCO achieve signiﬁcantly better performance than its counterparts
without pre-training when ﬁne-tuned using the same pipeline. Interestingly ﬁne-tuning on these English-only
datasets improves performance on all languages. Moreover our unsupervised mContriever outperforms BM25
for the Recall@100, and after ﬁne-tuning on MS MARCO it achieves state-of-the-art performance for this
metric. Performance can be further improved by ﬁne-tuning on the train set of Mr. TyDi. This appears to be
10

--- PAGE 11 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 5:Cross-lingual retrieval on MKQA. We report the average on all languages included in MKQA
for the Recall@100 and the Recall@20, and report the Recall@100 for a subset of languages. Complete results
are reported in Table 13 and Table 14 of appendix.
Avg. R@20 Avg. R@100 en ar ja ko es he de
CORA (Asai et al., 2021) 49.0 59.8 75.644.5 47.0 45.5 69.2 48.3 68.1
mBERT + MS MARCO 45.3 57.9 74.2 44.0 51.7 48.2 63.9 46.8 59.6
XLM-R + MS MARCO 46.9 59.6 73.4 42.5 53.2 49.6 63.4 46.9 61.1
mContriever 31.4 49.2 65.343.047.144.837.244.749.0
+ MS MARCO 53.9 65.6 75.6 53.3 60.4 55.4 70.0 59.666.6
Table 6:MoCo vs. in-batch negatives. In this table, we report nDCG@10 on the BEIR benchmark for
in-batch negatives and MoCo, without ﬁne-tuning on the MS MARCO dataset.
NFCorpus NQ FiQA ArguAna Quora DBPedia SciDocs FEVER AVG
MoCo 26.2 13.1 13.7 33.0 69.5 20.0 11.9 57.6 30.1
In-batch negatives 24.2 21.6 13.0 33.7 74.9 17.9 13.6 56.1 31.9
particularly important for the MRR@100 metric which put emphasis on the quality of the ﬁrst documents
retrieved. For this metric our unsupervised model is still lagging behind BM25.
Results on the cross-lingual retrieval benchmark derived from MKQA are reported in Table 5, with per
languagedetailsfortheRecall@100andRecall@20reportedinTable13andTable14ofappendix. Interestingly
using only supervised training data in English, our mContriever ﬁne-tuned on MS MARCO outperforms
the CORA retriever. Also, similarly to the results reported on Mr. TyDi, adding multilingual contrastive
pre-training before ﬁne-tuning on MS MARCO improves performance over its counterpart without pre-training.
On MKQA, evaluation is performed by lowercasing both queries and documents, we observed that this
improves performance. This does not impact the CORA retriever which is based on an uncased version of
mBERT.
6 Ablation studies
In this section, we investigate the inﬂuence of diﬀerent design choices on our method. In these ablations, all
the models are pre-trained on English Wikipedia for 200k gradient steps, with a batch size of 2,048 (on 32
GPUs). Each ﬁne-tuning on MS MARCO takes 20k gradient steps with a batch size of 512(on 8 GPUs),
using AdamW and no hard negatives.
MoCo vs. in-batch negatives. First, we compare the two contrastive pre-training methods: MoCo and
in-batch negatives. As in in-batch negatives, the number of negative examples is equal to the batch size, we
train models with a batch size of 4,096 and restrict the queue in MoCo to the same number of elements. This
experiment measures the eﬀect of using of momentum encoder for the keys instead of the same network as
for the queries. Using a momentum also prevents from backpropagating the gradient through the keys. We
report results, without ﬁne-tuning on MS MARCO in Table 6. We observe that the diﬀerence of performance
between the two methods is small, especially after ﬁne-tuning on MS MARCO. We thus propose to use MoCo
as our contrastive learning framework, since it scales to a larger number of negative examples without the
need to increase the batch size.
Number of negative examples. Next, we study the inﬂuence of the number of negatives used in the
contrastive loss, by varying the queue size of the MoCo algorithm. We consider values ranging from 2,048 to
131,072, and report results in Figure 2. We see that on average over the BEIR benchmark, increasing the
number of negatives leads to better retrieval performance, especially in the unsupervised setting. However,
we note that this eﬀect is not equally strong for all datasets.
11

--- PAGE 12 ---
Published in Transactions on Machine Learning Research (08/2022)
21121221321421521621710203040MS MARCO
21121221321421521621720304050TREC COVID
211212213214215216217222426283032NFCorpus
2112122132142152162171020304050NaturalQuestions
2112122132142152162173040506070HotpotQA
2112122132142152162171015202530FiQA
21121221321421521621730354045ArguAna
2112122132142152162171214161820Touche-2020
2112122132142152162176570758085Quora
211212213214215216217203040DBPedia
2112122132142152162171012141618SCIDOCS
21121221321421521621750607080FEVER
21121221321421521621710152025CLIMATE-FEVER
21121221321421521621755606570SciFacts
21121221321421521621730354045Average
Without MSMARCO With MSMARCO
Figure 2:Impact of the number of negatives. We report nDCG@10 as a function of the queue size, with
and without ﬁne-tuning on MS MARCO. We report numbers using the MoCo framework where the keys for
the negatives are computed with the momentum encoder and stored in a queue.
Data augmentations. Third, we compare diﬀerent ways to generate pairs of positive examples from a
single document or chunk of text. In particular, we compare random cropping, which leads to pairs with
overlap, and the inverse cloze task, which was previously considered to pre-train retrievers. Interestingly,
as shown in Table 7, the random cropping strategy outperforms the inverse cloze task in our setting. We
believe that random cropping, leading to the identical distributions of keys and queries, leads to more stable
training with MoCo compared to ICT. This might explains part of the diﬀerence of performance between the
two methods. We also investigate whether additional data perturbations, such as random word deletion or
replacement, are beneﬁcial for retrieval.
Training data. Finally, we study the impact of the pre-training data on the performance of our retriever,
by training on Wikipedia, CCNet or a mix of both sources of data. We report results in Table 8, and observe
that there is no clear winner between the two data sources. Unsurprisingly, training on the more diverse
CCNet data leads to strong improvements on datasets from diﬀerent domains than Wikipedia, such as FiQA
or Quora. On the other hand, on a dataset like FEVER, training on Wikipedia leads to better results. To get
the best of both worlds, we consider two strategies to mix the two data sources. In the “50/50%” strategy,
examples are sampled uniformly across domain, meaning that half the batches are from Wikipedia and the
other half from CCNet. In the “uniform” strategy, examples are sampled uniformly over the union of the
dataset. Since CCNet is signiﬁcantly larger than Wikipedia, this means that most of the batches are from
CCNet.
Impact of ﬁne-tuning on MS MARCO. To isolate the impact of pre-training from the impact of
ﬁne-tuning on MS MARCO, we apply the same ﬁne-tuning to the BERT base uncased model. We report
results in Table 9, and observe that when applied to BERT, our ﬁne-tuning leads to results that are lower
than the state-of-the-art. Hence, we believe that most of the improvement compared to the state-of-the-art
retrievers can be attributed to our contrastive pre-training strategy.
12

--- PAGE 13 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 7:Impact of data augmentions. We report nDCG@10 without ﬁne-tuning on MS MARCO.
NFCorpus NQ ArguAna Quora DBPedia SciDocs FEVER Overall
ICT 23.2 19.4 31.6 27.6 21.3 10.6 55.6 25.9
Crop 27.6 17.7 35.6 75.4 21.0 13.3 64.5 32.2
Crop + delete 26.8 20.8 35.8 77.3 21.5 14.0 67.9 33.8
Crop + replace 27.7 18.7 36.2 75.6 22.0 13.0 66.8 32.9
Table 8:Training data. We report nDCG@10 without ﬁne-tuning on MS MARCO.
NFCorpus NQ FiQA ArguAna Quora DBPedia SciDocs FEVER Overall
Wiki 27.6 17.7 16.3 35.6 75.4 21.0 13.3 64.5 33.0
CCNet 29.5 25.8 26.2 35.2 80.6 20.5 14.9 60.9 34.9
Uniform 31.0 19.4 25.1 37.8 80.4 21.5 14.7 59.8 33.9
50/50% 31.5 18.6 23.3 36.2 79.1 22.1 13.7 64.1 34.7
Table 9:Fine-tuning. We report nDCG@10 after ﬁne-tuning BERT and our model on MS MARCO.
NFCorpus NQ FiQA ArguAna Quora DBPedia SciDocs FEVER Overall
BERT 28.2 44.6 25.9 35.0 84.0 34.4 13.0 69.8 42.0
Contriever 33.2 50.2 28.8 46.0 85.4 38.8 16.0 77.7 46.5
7 Discussion
In this work, we propose to explore the limits of contrastive pre-training to learn dense text retrievers. We use
the MoCo technique, which allows to train models with a large number of negative examples. We make several
interesting observations: ﬁrst, we show that neural networks trained without supervision using contrastive
learning exhibit good retrieval performance, which are competitive with BM25 (albeit not state-of-the-art).
These results can be further improved by ﬁne-tuning on the supervised MS MARCO dataset, leading to
strong results, in particular for recall@100. Based on that observation, we use a cross-encoder to re-rank
documents retrieved with our model, leading to new state-of-the-art on the competitive BEIR benchmark.
We also performed extensive ablation experiments, and observed that independent random cropping seems to
be a strong alternative to the inverse Cloze task for training retrievers.
References
Akari Asai, Xinyan Yu, Jungo Kasai, and Hannaneh Hajishirzi. One question answering model for many
languages with cross-lingual dense passage retrieval. CoRR, abs/2107.11976, 2021. URL https://arxiv.
org/abs/2107.11976 . 10, 11
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder,
Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading
comprehension dataset. arXiv preprint arXiv:1611.09268 , 2016. 3, 6
Adam Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu Mittal. Bridging the lexical chasm:
statistical approaches to answer-ﬁnding. In Proceedings of the 23rd annual international ACM SIGIR
conference on Research and development in information retrieval , pp. 192–199, 2000. 1
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.
2
Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for
embedding-based large-scale retrieval. arXiv preprint arXiv:2002.03932 , 2020. 2, 3
13

--- PAGE 14 ---
Published in Transactions on Machine Learning Research (08/2022)
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain
questions. In Proc. ACL , 2017a. 1
Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong. On sampling strategies for neural network-based
collaborative ﬁltering. arXiv preprint arXiv:1706.07881 , 2017b. 5
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning , pp. 1597–1607. PMLR,
2020. 2, 4, 5
Xilun Chen, Kushal Lakhotia, Barlas Oğuz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad,
Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a
sparse one?, 2021. URL https://arxiv.org/abs/2110.06918 . 3
Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and
Jennimaria Palomaki. Tydi QA: A benchmark for information-seeking question answering in typologically
diverse languages. CoRR, abs/2003.05002, 2020. URL https://arxiv.org/abs/2003.05002 . 9
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual
representation learning at scale. CoRR, abs/1911.02116, 2019. URL http://arxiv.org/abs/1911.02116 .
9, 10, 19
Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing
by latent semantic analysis. Journal of the American society for information science , 41(6):391–407, 1990.
2
JacobDevlin, Ming-WeiChang, KentonLee, andKristinaToutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. In Proc. NAACL , 2019. 3, 4
Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. Cert: Contrastive self-supervised
learning for language understanding. arXiv preprint arXiv:2005.12766 , 2020. 3
Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. Splade v2: Sparse lexical
and expansion model for information retrieval. arXiv preprint arXiv:2109.10086 , 2021. 7
Luyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense passage
retrieval. arXiv preprint arXiv:2108.05540 , 2021. 3
Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings.
arXiv preprint arXiv:2104.08821 , 2021. 5, 7
Daniel Gillick, Alessandro Presta, and Gaurav Singh Tomar. End-to-end retrieval in continuous space. arXiv
preprint arXiv:1811.08008 , 2018. 3
John M Giorgi, Osvald Nitski, Gary D Bader, and Bo Wang. Declutr: Deep contrastive learning for
unsupervised textual representations. arXiv preprint arXiv:2006.03659 , 2020. 3
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented
language model pre-training. arXiv preprint arXiv:2002.08909 , 2020. 3, 5, 7
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 9729–9738, 2020. 2, 6, 17
Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. Eﬃciently teaching
an eﬀective dense retriever with balanced topic aware sampling. arXiv preprint arXiv:2104.06967 , 2021. 7
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured
semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international
conference on Information & Knowledge Management , pp. 2333–2338, 2013. 2, 4
14

--- PAGE 15 ---
Published in Transactions on Machine Learning Research (08/2022)
Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Transformer
architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint
arXiv:1905.01969 , 2019. 3
Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering,
2020a. URL https://arxiv.org/abs/2012.04584 . 3, 7
Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain
question answering. arXiv preprint arXiv:2007.01282 , 2020b. 6
Gautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola De Cao, Sebastian Riedel, and Edouard Grave. A
memory eﬃcient baseline for open domain question answering. arXiv preprint arXiv:2012.15156 , 2020. 3
Yacine Jernite, Samuel R Bowman, and David Sontag. Discourse-based objectives for fast unsupervised
sentence representation learning. arXiv preprint arXiv:1705.00557 , 2017. 3
Jeﬀ Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions
on Big Data , 2019. 3
Karen Sparck Jones. A statistical interpretation of term speciﬁcity and its application in retrieval. Journal
of documentation , 1972. 2
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehension. In Proc. ACL , 2017. 6, 7
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 , 2020. 3, 4,
5, 7
Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with colbert,
2020. 3, 7
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for
question answering research. Transactions of the Association for Computational Linguistics , 7:453–466,
2019. 1, 6, 7
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: adaptive sharpness-aware
minimization for scale-invariant learning of deep neural networks. arXiv preprint arXiv:2102.11600 , 2021.
17
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 ,
2019. 3
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain
question answering. In Proc. ACL , 2019. 2, 3, 4, 5, 6
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer.
Pre-training via paraphrasing. arXiv preprint arXiv:2006.15020 , 2020. 3
Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. Pretrained transformers for text ranking: Bert and beyond.
arXiv preprint arXiv:2010.06467 , 2020. 2
Shayne Longpre, Yi Lu, and Joachim Daiber. MKQA: A linguistically diverse benchmark for multilingual open
domain question answering. CoRR, abs/2007.15207, 2020. URL https://arxiv.org/abs/2007.15207 . 9
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. 17, 18
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, dense, and attentional
representations for text retrieval. arXiv preprint arXiv:2005.00181 , 2020. 3
15

--- PAGE 16 ---
Published in Transactions on Machine Learning Research (08/2022)
Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin. A replication study of dense passage retriever,
2021. 7
Christopher D Manning, Hinrich Schütze, and Prabhakar Raghavan. Introduction to information retrieval .
Cambridge university press, 2008. 2
Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, and Xia Song. Coco-lm:
Correcting and contrasting text sequences for language model pretraining. arXiv preprint arXiv:2102.08473 ,
2021. 3
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. Distributed representations of
words and phrases and their compositionality. In Advances in neural information processing systems , pp.
3111–3119, 2013. 3
Bhaskar Mitra, Nick Craswell, et al. An introduction to neural information retrieval. Foundations and
Trends ®in Information Retrieval , 13(1):1–126, 2018. 2
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms
marco: A human generated machine reading comprehension dataset. In CoCo@ NIPS , 2016. 1
Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085 ,
2019. 2, 4
Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab
Ward. Deep sentence embedding using long short-term memory networks: Analysis and application to
information retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 24(4):694–707,
2016. 2
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu,
and Haifeng Wang. RocketQA: An optimized training approach to dense passage retrieval for open-
domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies , pp. 5835–5847, Online,
June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.466. URL
https://aclanthology.org/2021.naacl-main.466 . 5
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. Learning to retrieve passages
without supervision, 2021. URL https://arxiv.org/abs/2112.07708 . 3
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv
preprint arXiv:1908.10084 , 2019. 3, 4
Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond . Now
Publishers Inc, 2009. 1
Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi
at TREC-3. NIST Special Publication Sp , 1995. 2
Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training
of multi-document reader and retriever for open-domain question answering, 2021. 2, 7
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. Learning semantic representations
using convolutional neural networks for web search. In Proceedings of the 23rd international conference on
world wide web , pp. 373–374, 2014. 2
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heteroge-
nous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663 ,
2021. 2, 6, 18, 19
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset
for fact extraction and veriﬁcation. arXiv preprint arXiv:1803.05355 , 2018. 1
16

--- PAGE 17 ---
Published in Transactions on Machine Learning Research (08/2022)
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand
Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data.
InProceedings of the 12th Language Resources and Evaluation Conference , 2020. 6, 9, 17
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric
instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 3733–3742, 2018. 2, 4, 6
Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive learning
for sentence representation. arXiv preprint arXiv:2012.15466 , 2020. 3
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold
Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv
preprint arXiv:2007.00808 , 2020. 3, 4, 7
Sohee Yang and Minjoon Seo. Is retriever merely an approximator of reader? arXiv preprint arXiv:2010.10999 ,
2020. 3
Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. tydi: A multi-lingual benchmark for dense
retrieval. CoRR, abs/2108.08787, 2021. URL https://arxiv.org/abs/2108.08787 . 9, 10
A Technical details for Contriever
A.1 Contrastive pre-training
For the model with ﬁne-tuning on MS MARCO, we use the MoCo algorithm He et al. (2020) with a queue of
size 131,072, a momentum value of 0.9995 and a temperature of 0.05. We use the random cropping data
augmentation, with documents of 256 tokens and span sizes sampled between 5% and 50% of the document
length. Documents are simply random piece of text sampled from a mix between Wikipedia and CCNet
data (Wenzek et al., 2020), where half the batches are sampled from each source. We also apply token
deletion with a probability of 10%. We optimize the model with the AdamW (Loshchilov & Hutter, 2019)
optimizer, with learning rate of 5·10−5, batch size of 2,048 and 500,000 steps. We initialize the network with
the publicly available BERT base uncased model.
A.2 Fine-tuning on MS MARCO
For the ﬁne-tuning on MS MARCO we do not use the MoCo algorithm and simply use in-batch negatives.
We use the ASAM optimizer (Kwon et al., 2021), with a learning rate of 10−5and a batch size of 1024 with a
temperature of 0.05, also used during pre-training. We train an initial model with random negative examples
for 20000 steps, mine hard negatives with this ﬁrst model, and re-train a second model with those. Each
query is associated with a gold document and a negative document, which is a random document in the ﬁrst
phase and a hard negative 10% of the time in the second phase. For each query, all documents from the
current batch aside of the gold document are used as negatives.
A.3 Few-shot training
For the few-shot evaluation presented in Table 3, we train for 500 epochs on each dataset with a batch size of
256 with in-batch random negatives. We evaluate performance performance on the development set every 100
gradient updates and perform early stopping based on this metric. For SciFact, we hold out randomly 10% of
the training data and use them as development set, leading to a train set containing 729 samples.
17

--- PAGE 18 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 10: BEIR Benchmark. We report the recall@100 on the test sets from the BEIR benchmark for
bi-encoder methods. We report the capped recall@100 on Trec-COVID following the original BEIR setup.
Note that using a cross-encoder to re-rank the top-100 documents do not change the recall@100, hence, we do
not include these methods in this table. We also report the average and number of datasets where a method
is the best (“Best on”) over the entire BEIR benchmark (excluding three datasets because of their licence).
Bold is the best overall. On Trec-COVID we report the capped Recall@100, see Thakur et al. (2021) for
more details. MS MARCO is excluded from the average.
BM25 DPR ANCE TAS-B Gen-Q ColBERT Splade v2 Ours
MS MARCO 65.8 55.2 85.2 88.4 88.4 86.5 - 89.1
Trec-COVID 49.8 21.2 45.7 38.7 45.6 46.4 12.3 40.7
NFCorpus 25.0 20.8 23.2 28.0 28.0 25.4 27.7 30.0
NQ 76.0 88.0 83.6 90.3 86.2 91.2 93.0 92.5
HotpotQA 74.0 59.1 57.8 72.8 67.3 74.8 82.0 77.7
FiQA 53.9 34.2 58.1 59.3 61.8 60.3 62.1 65.6
ArguAna 94.2 75.1 93.7 94.2 97.8 91.4 97.2 97.7
Touche-2020 53.8 30.1 45.8 43.1 45.1 43.9 35.4 29.4
CQADupStack 60.6 40.3 57.9 62.2 65.4 62.4 - 66.3
Quora 97.3 47.0 98.7 98.6 98.8 98.9 98.7 99.3
DBPedia 39.8 34.9 31.9 49.9 43.3 46.1 57.5 54.1
Scidocs 35.6 21.9 26.9 33.5 33.2 34.4 36.4 37.8
Fever 93.1 84.0 90.0 93.7 92.8 93.4 95.1 94.9
Climate-fever 43.6 39.0 44.5 53.4 45.0 44.4 52.4 57.4
Scifact 90.8 72.7 81.6 89.1 89.3 87.8 92.0 94.7
Avg. w/o CQA 63.6 48.3 60.1 65.0 64.2 64.5 64.8 67.1
Avg. 63.4 47.7 60.0 64.8 64.2 64.3 - 67.0
Best on 2 0 0 0 1 0 4 7
B Multilingual retrieval with mContriever
B.1 Hyperparameters for multilingual contrastive pre-training
The pre-trained mContriever model is pre-trained for 500,000 steps with a queue of size 32768, and temperature
of 0.05 and a momentum value of 0.999. We optimize the model with the AdamW (Loshchilov & Hutter,
2019) optimizer, with learning rate of 5·10−5. The learning rate follows a linear warmup for 20,000 steps
followed by linear decay until the end of training. Languages used for pre-training are detailed in Table 12.
B.2 Hyperparameters for multilingual ﬁne-tuning
We ﬁne-tune mContriever using in-batch negatives, AdamW optimizer (Loshchilov & Hutter, 2019), a learning
rate of 10−5, and a batch size of 1024 samples with a temperature τof0.05. On MS MARCO and Mr. TyDi
the model is trained for 20k gradient steps. We notice overﬁtting on NaturalQuestions, and thus reduced the
training to 1k gradient steps. We use a warmup of 1000 gradient steps with linear decay afterwards in all
cases. Hard negatives are mined on Mr. TyDi with the model trained on MS MARCO. We did not observe
signiﬁcant improvements using hard negatives on MS MARCO and NaturalQuestions.
For the ﬁne-tuning on MS MARCO of the models initiliazed from mBERT (resp. XLM-R) without contrastive
pre-training, we use a temperature τof1(resp. 5). We tried temperatures in {10,5,2,1,0.1,0.05}and chose
the one leading to the best performance. We observed a decrease in performance for lower temperatures.
We ﬁne-tuned mContriever with τ= 0.05following the temperature used during pre-training. We followed
the temperature τ= 0.05used for the training of Contriever, and did not test other temperatures for the
contrastive pre-training of the multilingual model, mContriever.
18

--- PAGE 19 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 11: Unsupervised retrieval. Performance of unsupervised methods on the BEIR datasets. We
report the capped recall@100 on Trec-COVID following the original BEIR setup. For SimCSE we report
results of the model using RoBERTa large. REALM uses annotated entity recognition data for training. On
Trec-COVID we report the capped Recall@100, see Thakur et al. (2021) for more details.
Model ( →) BM25 BERT SimCSE REALM Contriever
Dataset ( ↓) Recall@100
MS MARCO 65.8 3.5 33.6 52.6 67.2
Trec-COVID 49.8 10.6 26.8 8.1 17.2
NFCorpus 25.0 6.7 18.2 23.0 29.4
NQ 76.0 14.3 42.9 58.1 77.1
HotpotQA 74.0 15.8 42.7 56.1 70.4
FiQA-2018 53.9 6.9 41.0 28.0 56.2
ArguAna 94.2 59.1 95.2 73.1 90.1
Tóuche-2020 53.8 3.0 18.6 11.5 22.5
CQADupStack 60.6 11.0 48.9 35.5 61.4
Quora 97.3 74.6 97.9 92.7 98.7
DBPedia 39.8 7.1 21.5 33.0 45.3
SCIDOCS 35.6 11.3 23.0 23.1 36.0
Fever 93.1 13.6 50.8 82.6 93.6
Climate-fever 43.6 12.8 44.8 42.3 44.1
SciFact 90.8 35.2 75.3 83.8 92.6
Avg. 63.6 19.0 45.4 46.9 60.1
Best on 3 0 2 0 10
NDCG@10
MS MARCO 22.8 0.6 8.8 15.2 20.6
Trec-COVID 65.6 16.6 38.6 20.1 27.4
NFCorpus 32.5 2.5 14.0 24.1 31.7
NQ 32.9 2.7 12.6 15.2 25.4
HotpotQA 60.3 4.9 23.3 40.5 48.1
FiQA-2018 23.6 1.4 14.8 9.7 24.5
ArguAna 31.5 23.1 45.6 22.8 37.9
Tóuche-2020 36.7 3.4 11.6 7.3 19.3
CQADupStack 29.9 2.5 20.2 13.5 28.4
Quora 78.9 3.9 81.5 71.6 83.5
DBPedia 31.3 3.9 13.7 22.7 29.2
SCIDOCS 15.8 2.7 7.4 9.0 14.9
FEVER 75.3 4.9 20.1 42.9 68.2
Climate-fever 21.3 4.1 17.6 14.3 15.5
SciFact 66.5 9.8 38.5 47.1 64.9
Avg. 41.7 8.7 24.6 25.1 36.0
Best on 12 0 1 0 2
B.3 Curse of multilinguality
We tried to pre-train models on diﬀerent sets of languages. We generally observed performance deterioration
when scaling to more languages similarly to what has been observed for general multilingual masked language
models Conneau et al. (2019). In Table 15 we report results on Mr. TyDi with a model pre-trained on the
11 languages of Mr. TyDi versus the model used in the rest of the paper which has been pre-trained on 29
languages including the 11 languages of Mr. TyDi as detailed in Table 12. We also report performance of these
models after training on MS MARCO, eventually followed by further ﬁne-tuning on Mr. TyDi. It appears that
the performance of the unsupervised model and the performance after ﬁne-tuning on MS MARCO are better
for the model pre-trained only on 11 languages. The diﬀerence is mitigated after ﬁne-tuning on Mr. TyDi.
19

--- PAGE 20 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 12: List of languages used for multilingual retrieval.
ar bn da de en es
Language Arabic Bengali Danish German English Spanish
Pre-training 3 3 3 3 3 3
Mr. TyDi 3 3 7 7 7 3
MKQA 3 7 3 3 3 3
ﬁ fr he hu it id
Language Finnish French Hebrew Hungarian Italian Indonesian
Pre-training 3 3 3 3 3 3
Mr. TyDi 3 7 7 7 7 3
MKQA 3 3 3 3 3 7
ja km ko ms nl no
Language Japanese Khmer Korean Malay Dutch Norwegian
Pre-training 3 3 3 3 3 3
Mr. TyDi 3 7 3 7 7 7
MKQA 3 3 3 3 3 3
pl pt ru sv sw te
Language Polish Portugese Russian Swedish Swahili Telugu
Pre-training 3 3 3 3 3 3
Mr. TyDi 7 7 3 7 3 3
MKQA 3 3 3 3 7 7
th tr vi zh-cn zh-hk zh-tw
Language Thai Turkish Vietnamese Chinese (Simpliﬁed) Chinese (Hong Kong) Chinese (Traditional)
Pre-training 3 3 3 3 7 3
Mr. TyDi 3 7 7 7 7 7
MKQA 3 3 3 3 3 3
Table 13: Recall@100 on MKQA for cross-lingual retrieval in the setting described in Section 5.3.
avg en ar ﬁ ja ko ru es sv he th da de fr
CORA 59.8 75.644.5 61.3 47.0 45.5 58.6 69.2 68.0 48.3 44.4 68.9 68.1 70.2
mBERT + MS MARCO 57.9 74.2 44.0 51.7 55.7 48.2 57.4 63.9 62.7 46.8 51.7 63.7 59.6 65.2
XLM-R + MS MARCO 59.2 73.4 42.4 57.7 53.1 48.6 58.5 62.9 67.5 46.9 61.5 66.9 60.9 62.4
Contriever 49.265.343.043.147.144.851.837.254.544.751.449.349.050.2
+ MS MARCO 65.6 75.6 53.3 66.6 60.4 55.4 64.7 70.0 70.8 59.6 63.5 72.0 66.670.1
it nl pl pt hu vi ms km no tr zh-cn zh-hk zh-tw
CORA 68.3 72.065.6 67.9 59.5 61.2 67.9 35.6 68.3 61.5 52.0 52.8 52.8
mBERT + MS MARCO 64.1 66.7 59.0 61.9 57.5 58.6 62.8 32.9 63.2 56.0 58.4 59.3 59.3
XLM-R + MS MARCO 58.1 66.4 61.0 62.0 60.1 62.4 66.1 46.665.9 60.6 55.8 55.5 55.7
Contriever 56.761.744.454.547.745.156.727.850.244.354.351.952.5
+ MS MARCO 70.371.4 68.8 68.5 66.7 67.8 71.637.8 71.5 68.7 64.1 64.5 64.3
Table 14: Recall@20 on MKQA for cross-lingual retrieval in the setting described in Section 5.3.
avg en ar ﬁ ja ko ru es sv he th da de fr
CORA 49.0 68.531.7 49.7 34.1 33.1 46.5 60.358.1 36.8 33.6 59.4 58.5 61.6
mBERT + MS MARCO 45.3 65.5 30.2 38.9 41.7 34.5 44.3 52.4 50.5 32.6 38.5 52.5 46.6 53.8
XLM-R + MS MARCO 46.7 64.5 29.0 45.1 39.7 34.9 45.9 51.4 56.1 32.5 49.4 55.8 48.3 50.5
Contriever 31.450.226.626.729.427.932.720.737.622.231.131.231.230.7
+ MS MARCO 53.967.2 40.1 55.1 46.2 41.7 52.359.3 60.0 45.6 52.062.054.859.3
it nl pl pt hu vi ms km no tr zh-cn zh-hk zh-tw
CORA 58.2 63.554.3 58.447.6 49.8 57.6 24.8 58.8 49.1 38.6 40.5 39.6
mBERT + MS MARCO 52.1 55.3 45.6 49.5 44.6 46.9 49.9 21.5 51.3 42.7 44.6 45.3 45.5
XLM-R + MS MARCO 45.4 54.5 48.5 49.6 47.3 49.7 54.0 33.453.7 48.7 42.4 42.4 42.0
Contriever 38.645.125.137.628.327.339.615.733.226.535.032.732.5
+ MS MARCO 59.460.9 58.156.9 55.2 55.9 60.926.2 61.0 56.7 50.9 51.9 51.2
20

--- PAGE 21 ---
Published in Transactions on Machine Learning Research (08/2022)
Table 15: Performance dilution for multilingual retrievers. We report MRR@100 and R@100 on the
test set of Mr. TyDi after pre-training on two diﬀerent sets of languages, one containing the 11 languages
of Mr. TyDi which is included in the set of 29 languages used to train mContriever used in the rest of the
paper. We also report results of these models after ﬁne-tuning on MS MARCO, potentially followed by a
ﬁnal ﬁne-tuning stage on Mr. TyDi.
ar bn en ﬁ id ja ko ru sw te th avg
MRR@100
11 languages 28.9 38.6 10.2 22.9 26.2 21.7 26.3 20.5 39.0 20.2 40.6 26.8
29 languages 27.3 36.3 9.2 21.1 23.5 19.5 22.3 17.5 38.3 22.5 37.2 25.0
+ MS MARCO
11 languages 44.0 41.1 26.8 38.3 43.4 34.7 37.1 37.6 55.3 32.1 45.8 39.7
29 languages 43.4 42.3 27.1 35.1 42.6 32.4 34.2 36.1 51.2 37.4 40.2 38.4
+ Mr. TyDi
11 languages 73.5 66.8 55.9 60.4 62.7 53.8 55.8 60.6 69.5 89.8 68.7 65.2
29 languages 72.4 67.2 56.6 60.2 63.0 54.9 55.3 59.7 70.7 90.3 67.3 65.2
R@100
11 languages 83.4 89.2 55.2 81.5 83.6 75.6 72.4 75.7 88.9 70.0 91.9 78.9
29 languages 82.0 89.6 48.8 79.6 81.4 72.8 66.2 68.5 88.7 80.8 90.3 77.2
+ MS MARCO
11 languages 89.6 91.9 78.7 89.0 91.2 83.4 80.2 86.0 92.6 95.9 92.8 88.3
29 languages 88.7 91.4 77.2 88.1 89.8 81.7 78.2 83.8 91.4 96.6 90.5 87.0
+ Mr. TyDi
11 languages 94.2 98.2 93.3 93.6 94.7 89.1 87.1 92.3 94.5 98.9 96.9 93.9
29 languages 94.0 98.6 92.2 92.7 94.5 88.8 88.9 92.4 93.7 98.9 95.2 93.6
21
