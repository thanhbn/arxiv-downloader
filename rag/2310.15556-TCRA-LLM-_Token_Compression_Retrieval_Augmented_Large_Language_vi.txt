# TCRA-LLM: Nén Token Truy xuất Tăng cường Mô hình Ngôn ngữ Lớn để Giảm Chi phí Suy luận

Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming Qian
Meetyou AI Lab, Phòng thí nghiệm Trọng điểm Xiamen về Quản lý Sức khỏe Internet Phụ nữ, Đại học Osaka, Cơ quan Khoa học, Công nghệ và Nghiên cứu (A*STAR)
{liujunyi, liliangzhi, xiangtong}@xiaoyouzi.com, bowen.wang@is.ids.osaka-u.ac.jp, qiany@ihpc.a-star.edu.sg

## Tóm tắt

Kể từ khi ChatGPT phát hành API để sử dụng công khai, số lượng ứng dụng được xây dựng trên các mô hình ngôn ngữ lớn (LLM) thương mại tăng theo cấp số nhân. Một cách sử dụng phổ biến của các mô hình này là tận dụng khả năng học trong ngữ cảnh và tạo ra phản hồi cho các truy vấn của người dùng bằng cách sử dụng kiến thức thu được thông qua tăng cường truy xuất. Một vấn đề khi triển khai LLM tăng cường truy xuất thương mại là chi phí do ngữ cảnh được truy xuất bổ sung làm tăng đáng kể kích thước token đầu vào của LLM. Để giảm thiểu điều này, chúng tôi đề xuất một sơ đồ nén token bao gồm hai phương pháp: nén tóm tắt và nén ngữ nghĩa. Phương pháp đầu tiên áp dụng mô hình dựa trên T5 được tinh chỉnh bởi các tập dữ liệu được tạo bằng cách sử dụng tự hướng dẫn chứa các mẫu có độ dài khác nhau và giảm kích thước token bằng cách tóm tắt. Phương pháp thứ hai nén thêm kích thước token bằng cách loại bỏ các từ có tác động thấp hơn đến ngữ nghĩa. Để đánh giá đầy đủ hiệu quả của các phương pháp được đề xuất, chúng tôi đề xuất và sử dụng một tập dữ liệu gọi là Cơ sở dữ liệu Gợi ý Thực phẩm (FRDB) tập trung vào gợi ý thực phẩm cho phụ nữ trong thời kỳ mang thai hoặc trẻ sơ sinh. Nén tóm tắt của chúng tôi có thể giảm 65% kích thước token truy xuất với cải thiện thêm 0,3% về độ chính xác; nén ngữ nghĩa cung cấp cách linh hoạt hơn để đánh đổi kích thước token với hiệu suất, trong đó chúng tôi có thể giảm kích thước token 20% chỉ với 1,6% giảm độ chính xác.

## 1 Giới thiệu

Với sự gia tăng sức mạnh tính toán và tích lũy dữ liệu văn bản khổng lồ, các mô hình ngôn ngữ lớn (LLM) như ChatGPT (OpenAI, 2023b) và GPT-4 (OpenAI, 2023a) đã thể hiện hiệu suất ấn tượng trong hỏi đáp (QA) dựa trên đối thoại, cho phép chúng tương tác với người dùng một cách trôi chảy. Trong QA miền mở nơi các mô hình tham gia vào các cuộc trò chuyện thông thường với người dùng, LLM thể hiện hiệu suất đáng kinh ngạc bằng cách tận dụng khả năng học trong ngữ cảnh mạnh mẽ. Tuy nhiên, LLM có thể tạo ra các phản hồi mơ hồ hoặc câu trả lời không chính xác trong một số lĩnh vực chuyên môn nhất định, do thiếu kiến thức liên quan hoặc phạm vi thông tin hạn chế được thu thập trong giai đoạn đào tạo, điều này có thể dẫn đến các câu trả lời không trung thực và thậm chí gây tổn hại vật lý cho người dùng (Xiang et al., 2023). Đối với QA trong các lĩnh vực như vậy, sinh tăng cường truy xuất (RAG) (Lewis et al., 2020), nơi hệ thống truy xuất kiến thức bên ngoài trước đó và sau đó sử dụng LLM để tạo ra câu trả lời tận dụng kiến thức được truy xuất, có thể giảm đáng kể các ảo giác được tạo ra (Shi et al., 2023; Shuster et al., 2021).

Nhiều LLM thương mại hiện tại là các mô hình hộp đen, nơi kiến trúc mô hình và thông tin trọng số không được tiết lộ. Những LLM này sở hữu khả năng hiểu văn bản vượt trội, nhưng trong nhiều trường hợp chúng chỉ có thể đưa ra câu trả lời mong muốn thông qua kỹ thuật prompt phức tạp. Mặt khác, triển khai LLM mã nguồn mở lên máy chủ địa phương tốn nhiều tài nguyên, trái ngược với việc triển khai các mô hình nhỏ hơn như T5 (Raffel et al., 2020). Một số LLM thương mại như GPT-3.5-turbo (OpenAI, 2023c) và GPT-4 cung cấp quyền truy cập thông qua các cuộc gọi API; tuy nhiên, các mô hình này tính phí người dùng dựa trên kích thước đầu vào và đầu ra. Đối với các cá nhân hoặc công ty muốn tạo ra các dịch vụ riêng của họ bằng cách sử dụng LLM thông qua các cuộc gọi API, việc sử dụng các LLM thương mại có thể tốn nhiều tài nguyên nếu các yêu cầu được thực hiện thường xuyên. Do đó, cần thiết phải giảm thiểu số lượng token đầu vào trong khi duy trì hiệu suất tối ưu trong các cuộc gọi API.

Trong công trình này, chúng tôi đề xuất một sơ đồ nén token được thiết kế đặc biệt cho LLM tăng cường truy xuất (được hiển thị trong Hình 1), cụ thể là Mô hình Ngôn ngữ Lớn Tăng cường Truy xuất Nén Token (TCRA-LLM). Sơ đồ được đề xuất của chúng tôi có thể giảm tới 65% kích thước token với cải thiện thêm 0,3% về độ chính xác khi thực hiện QA trên tập dữ liệu được đề xuất của chúng tôi gọi là Cơ sở dữ liệu Gợi ý Thực phẩm (FRDB). Chúng tôi đề xuất hai cách tiếp cận để giảm kích thước token của đầu vào LLM: nén tóm tắt và nén ngữ nghĩa. Đối với nén tóm tắt, chúng tôi tận dụng sơ đồ tự hướng dẫn (Wang et al., 2022) để xây dựng nhiều tập dữ liệu tóm tắt với độ dài khác nhau để tinh chỉnh mô hình mT5 (Xue et al., 2020). Các mẫu từ các tập dữ liệu tóm tắt được tạo bởi GPT-3.5-turbo (OpenAI, 2023c), được hướng dẫn rút ngắn tóm tắt của các câu đầu vào theo cách lặp lại. Cách tiếp cận nén ngữ nghĩa dựa trên trực giác đơn giản nhưng hiệu quả, rằng việc loại bỏ các từ quan trọng về mặt ngữ nghĩa trong một câu sẽ không thay đổi đáng kể ngữ nghĩa của nó. Ở đây, chúng tôi triển khai một sentence-transformer đa ngôn ngữ (Reimers và Gurevych, 2020) để mã hóa các câu thành embedding nơi khoảng cách giữa embedding gốc và embedding bị nhiễu được sử dụng để đo độ lệch ngữ nghĩa từ ý nghĩa gốc. Độ lệch ngữ nghĩa lớn hơn cho thấy từ tương ứng sở hữu ngữ nghĩa quan trọng hơn trong câu. Chúng tôi tiến hành một quá trình lặp lại đo lường tầm quan trọng ngữ nghĩa của mỗi từ trong câu và loại bỏ các từ ít quan trọng hơn.

Tóm lại, công trình của chúng tôi có ba đóng góp sau:

1. Chúng tôi xây dựng một tập dữ liệu QA gợi ý thực phẩm (Phần 3) chứa kiến thức chuyên môn mà các LLM chung có thể không có. Tập dữ liệu này phục vụ mục đích đánh giá LLM tăng cường truy xuất.

2. Chúng tôi đề xuất một sơ đồ tự hướng dẫn đa cấp (Phần 4.2) để xây dựng các tập dữ liệu tóm tắt có độ dài khác nhau.

3. Chúng tôi đề xuất hai phương pháp nén token (Phần 4.2), cả hai đều có thể giảm số lượng token đầu vào trong các cuộc gọi API của LLM thương mại tăng cường truy xuất trong khi duy trì hiệu suất tối ưu.

## 2 Công trình Liên quan

LLM như GPT-3 (Brown et al., 2020), PALM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), Bloom (Scao et al., 2022), và LLaMA (Touvron et al., 2023) được đào tạo trên lượng dữ liệu khổng lồ và đã chứng minh khả năng hiểu mạnh mẽ. Các mô hình này đã được triển khai trong nhiều nhiệm vụ và đạt được kết quả đầy hứa hẹn (Zhang et al., 2023; Ashok và Lipton, 2023; Lu et al., 2023; Wang et al., 2023; Xiang et al., 2023). Một rào cản chính ngăn cản nhiều người tham gia vào triển khai thương mại của LLM là chi phí đào tạo và lưu trữ của chúng. Một cách để giảm chi phí như vậy là thông qua đào tạo các mô hình chuyên biệt theo lĩnh vực nhỏ hơn như BioMedLM (Bolton et al., 2022), BloombergGPT (Wu et al., 2023), và LawGPT (Nguyen, 2023). Việc đào tạo chuyên biệt theo lĩnh vực như vậy cho phép các LLM nhỏ hơn được áp dụng cho các lĩnh vực nhất định nhưng vẫn yêu cầu đầu tư khổng lồ. Ví dụ, BloombergGPT được đào tạo trên 512 GPU A100 40GB với tổng ngân sách khoảng 2,7 triệu đô la (Sheikh, 2023).

Thay vào đó, LLM có thể được sử dụng mà không cần tinh chỉnh thông qua tăng cường truy xuất tận dụng các nguồn dữ liệu bên ngoài, nơi dữ liệu được truy xuất được sử dụng làm thông tin bổ sung để giúp LLM cải thiện lý luận logic và sinh ngôn ngữ (Thorne et al., 2021; Izacard et al., 2022). Các thí nghiệm trước đây (Ram et al., 2023) cho thấy thông tin bổ sung có thể có lợi cho LLM trên các kích thước mô hình khác nhau. Tăng cường truy xuất loại bỏ chi phí tinh chỉnh LLM nội bộ trên dữ liệu mới, và có thể dễ dàng tích hợp với các dịch vụ LLM thương mại như ChatGPT (OpenAI, 2023b) từ OpenAI hoặc Bard (Pichai, 2023) từ Google. Nhiều nghiên cứu đã chỉ ra, việc áp dụng tăng cường truy xuất cho các LLM thương mại như ChatGPT cho phép các mô hình có được kiến thức trong các lĩnh vực cụ thể như khoa học tự nhiên và y học (Soong et al., 2023; Inaba et al., 2023) không được tiết lộ trong quá trình đào tạo của chúng và tăng cường truy xuất có thể được cải thiện thêm bằng cách áp dụng các bộ truy xuất tinh vi hơn (Shi et al., 2023). Tuy nhiên, các LLM thương mại đều có giới hạn về độ dài đầu vào đặt ra giới hạn trên về lượng thông tin có thể được đưa vào LLM. Các mô hình sau này như GPT-4 có hạn chế lỏng lẻo hơn nhưng chi phí suy luận tăng mạnh so với các mô hình khác. Một số công trình trước đây áp dụng tối ưu hóa prompt dựa trên template (Santra et al., 2023), chọn ngữ cảnh được truy xuất (Mallen et al., 2022) theo cách thích ứng, hoặc sử dụng cascade của LLM với các kích thước khác nhau (Chen et al., 2023) để giảm chi phí suy luận. Phương pháp được đề xuất của chúng tôi không xung đột với các phương pháp này và có thể được sử dụng cùng với chúng đồng thời.

## 3 FRDB

Chúng tôi xây dựng một Tập dữ liệu Gợi ý Thực phẩm bằng tiếng Trung gọi là FRDB, để gợi ý thực phẩm an toàn để tiêu thụ cho phụ nữ trước/trong/sau thai kỳ cũng như trẻ sơ sinh. Nó chứa hai phần: các cặp QA đa lựa chọn (MC) và một cơ sở dữ liệu kiến thức. Các cặp QA chứa 1.000 mẫu bao gồm 200 loại thực phẩm. Các danh mục thực phẩm được hiển thị trong Bảng 1.

| Loại thực phẩm | Số lượng ↓ |
|---|---|
| Món chính | 31 |
| Rau | 31 |
| Hải sản | 22 |
| Đồ ngọt | 22 |
| Thuốc/Thực phẩm bổ sung sức khỏe | 20 |
| Trái cây | 17 |
| Ngũ cốc | 16 |
| Nước ngọt | 13 |
| Gia vị | 10 |
| Thịt/Trứng | 10 |
| Đậu nành/Trái cây khô | 6 |
| Sản phẩm từ sữa | 2 |
| Tổng cộng | 200 |

Các câu trả lời có thể cho câu hỏi rơi vào ba lựa chọn dựa trên mức độ gợi ý tăng dần từ 1 (tránh) đến 3 (rất khuyến khích). Mỗi loại thực phẩm có năm đánh giá gợi ý tương ứng với năm nhóm: trước mang thai, mang thai, sau sinh, cho con bú, và trẻ sơ sinh. Ngoài ra, chúng tôi xây dựng một cơ sở dữ liệu kiến thức chứa 7.588 mục; chi tiết của các mục được hiển thị trong Bảng 2. Phân phối độ dài câu trong cơ sở dữ liệu kiến thức được hiển thị trong Hình 2.

| | Trung bình | Tối đa | Tối thiểu | Độ lệch chuẩn |
|---|---|---|---|---|
| Số từ | 88 | 248 | 12 | 27 |

Tất cả thông tin đã được xác minh bởi các chuyên gia lĩnh vực sức khỏe. Trong quá trình xác minh, chúng tôi loại bỏ văn bản mơ hồ đối với các người chú thích con người. Hai mẫu kiến thức được hiển thị trong Bảng 3. Các câu hỏi mẫu có sẵn trong Phụ lục A.

| Kiến thức chất lượng cao | Kiến thức mơ hồ |
|---|---|
| Tiêu thụ nấm sau sinh có lợi cho việc phục hồi sau sinh, giảm táo bón và thúc đẩy tiết sữa do chúng giàu vitamin B, protein và axit amin. Khuyến nghị lượng vừa phải dựa trên tình trạng phục hồi. | Các bà mẹ sau sinh an toàn khi tiêu thụ lượng bánh vừa phải. Bánh dễ tiêu hóa và hấp thụ đối với các bà mẹ sau sinh có hệ tiêu hóa yếu hơn. Tuy nhiên, bánh có sự đa dạng dinh dưỡng tương đối nhỏ và nên được tiêu thụ cùng với rau, trái cây và thịt để làm cho dinh dưỡng cân bằng hơn. |

## 4 Phương pháp

Thông thường, một LLM tăng cường truy xuất bao gồm ba thành phần (được hiển thị trong Hình 1), một cơ sở dữ liệu kiến thức, một bộ truy xuất và LLM. Cơ sở dữ liệu kiến thức chứa tất cả kiến thức chuyên biệt theo lĩnh vực có sẵn. Bộ truy xuất áp dụng câu hỏi làm truy vấn để tìm kiếm thông tin liên quan từ cơ sở dữ liệu kiến thức. Thông tin được truy xuất sau đó được công thức hóa thành ngữ cảnh được đóng gói cùng với câu hỏi như một prompt cho LLM để tạo ra câu trả lời. Các phương pháp được đề xuất của chúng tôi có thể nén thông tin được truy xuất và công thức hóa ngữ cảnh ngắn hơn nhưng duy trì hiệu quả của tăng cường truy xuất. Trong phần này, chúng tôi đi qua pipeline của hệ thống LLM tăng cường truy xuất cho QA và giới thiệu các phương pháp nén token được đề xuất của chúng tôi.

### 4.1 Truy xuất Thông tin

Nhìn chung, bước đầu tiên cho tăng cường truy xuất của LLM là truy xuất kiến thức. Cho một truy vấn người dùng x, bộ truy xuất trích xuất k mảnh thông tin từ cơ sở dữ liệu kiến thức D = {d1, d2, ···, dm} có khả năng liên quan nhất đến x. Có hai phương pháp truy xuất chính: truy xuất dày đặc (Karpukhin et al., 2020; Ni et al., 2021) và truy xuất thưa thớt (Robertson et al., 2009). Truy xuất dày đặc đầu tiên mã hóa các truy vấn và tài liệu thành embedding dày đặc (Huang et al., 2013; Yi et al., 2019) sử dụng các bộ mã hóa neural được đào tạo trước và sau đó tìm các láng giềng gần nhất của truy vấn trong không gian embedding sử dụng một thước đo liên quan như độ tương tự cosine (Yu et al., 2021). Truy xuất thưa thớt, mặt khác, ánh xạ các truy vấn và tài liệu vào một không gian chiều cao với các phương pháp như TF-IDF (Sparck Jones, 1972; Jones, 1973) và các tài liệu liên quan nhất được trả về cho người dùng làm câu trả lời. Ví dụ điển hình của truy xuất thưa thớt là BM25 (Robertson et al., 1995).

Ở đây chúng tôi đánh giá cả phương pháp truy xuất dày đặc và thưa thớt. Đối với truy xuất dày đặc, chúng tôi tuân theo một quy trình tương tự từ Huang et al. (2013): chúng tôi đầu tiên mã hóa văn bản sử dụng GPT-embedding (OpenAI, 2022) được cung cấp bởi OpenAI, sau đó triển khai cơ sở dữ liệu vector FAISS Index (Johnson et al., 2019) để lưu trữ các embedding, cho phép thao tác nhanh hơn trên chúng. Đối với truy xuất thưa thớt, chúng tôi triển khai BM25 (Robertson et al., 1995) được coi là cách tiêu chuẩn.

#### 4.1.1 Dự đoán Câu Tiếp theo

Các kết quả top-k được truy xuất Dk = {dk1, dk2, ···, dkk} (dki là các phần tử được truy xuất thứ i từ tập gốc D nơi 1 ≤ i ≤ k) được coi là liên quan nhất được xếp hạng bởi phương pháp truy xuất. Chỉ sử dụng kết quả top-1 làm ngữ cảnh không phải lúc nào cũng đáng tin cậy, nhưng sử dụng nhiều kết quả được truy xuất hơn sẽ tiêu thụ nhiều không gian hơn trong các token đầu vào, dẫn đến chi phí cao hơn; do đó, để cải thiện độ tin cậy mà không phát sinh chi phí bổ sung, chúng tôi đề xuất sử dụng dự đoán câu tiếp theo (NSP) như một phương pháp xếp hạng giai đoạn thứ hai. Nó dựa trên một giả định trực quan rằng thông tin được truy xuất có nhiều khả năng được dự đoán là câu tiếp theo của câu hỏi nếu nó liên quan nhiều hơn đến câu hỏi. Việc triển khai cách tiếp cận này dựa trên mô-đun NSP được đào tạo trước từ BERT (Devlin et al., 2018); câu được chọn s với xác suất tối đa từ NSP được chọn làm kết quả tốt nhất (Xem Phương trình 1).

s = dki
i = argmax{p(x, dk1), ···, p(x, dkk)}     (1)

Chúng tôi tiến hành các thí nghiệm để đánh giá tác động của việc bao gồm NSP vào LLM tăng cường truy xuất. Ở đây chúng tôi sử dụng GPT-3.5-turbo của OpenAI làm LLM cơ sở và đánh giá nó trên tập dữ liệu FRDB sử dụng các kết quả truy xuất top-1 làm ngữ cảnh. Kết quả được hiển thị trong Bảng 4. Có một cải thiện hiệu suất nhỏ sử dụng NSP với cả GPT-embedding và BM25 và do đó chúng tôi giữ mô-đun NSP này trong tất cả các thí nghiệm sau này của chúng tôi.

| Phương pháp | Độ chính xác (%) |
|---|---|
| Embedding | 89.1 |
| Embedding + NSP | 90.2 |
| BM25 | 83.4 |
| BM25 + NSP | 84.9 |

Từ thí nghiệm, chúng tôi cũng thấy rằng sự kết hợp của truy xuất dày đặc với cách tiếp cận NSP đạt được độ chính xác cao nhất. Chúng tôi điều chỉnh giá trị của k bằng cách tìm kiếm nó từ 1 đến 10 và thực hiện đánh giá tương ứng trên tập dữ liệu FRDB. Kết quả thí nghiệm được hiển thị trong Hình 3. Chúng tôi thấy rằng k = 3 là lựa chọn tối ưu và chúng tôi sẽ tuân thủ giá trị này trong tất cả các thí nghiệm tiếp theo của chúng tôi.

### 4.2 Nén Token

Văn bản truy xuất thường dài và dễ dàng tiêu thụ một lượng lớn không gian từ các token đầu vào trong các cuộc gọi API khi sử dụng LLM thương mại. Để giảm thiểu điều này, chúng tôi đề xuất hai phương pháp để nén văn bản được truy xuất. Phương pháp đầu tiên là nén tóm tắt tạo ra việc rút ngắn văn bản gốc bằng cách diễn đạt lại. Phương pháp thứ hai là nén ngữ nghĩa nơi chúng tôi nhiễu câu gốc và xếp hạng tác động của thay đổi ngữ nghĩa từ mỗi từ trong câu. Các từ có tác động ngữ nghĩa thấp hơn đến câu được loại bỏ.

#### 4.2.1 Nén Tóm tắt

Các mô hình tóm tắt như mô hình mT5 (Xue et al., 2020) đã được sử dụng rộng rãi trong nhiều ứng dụng để rút ngắn văn bản đầu vào, nhưng chúng không thể xuất ra tóm tắt với độ dài tùy ý do hạn chế của dữ liệu đào tạo. Để giải quyết điều này, chúng tôi đề xuất xây dựng một mô hình tóm tắt có thể xuất ra tóm tắt với các độ dài khác nhau.

Để xây dựng mô hình như vậy, chúng tôi tận dụng sức mạnh của tự hướng dẫn (Wang et al., 2022) nơi chúng tôi sử dụng GPT-3.5-turbo để tạo ra các tập dữ liệu đào tạo. Quy trình tạo dữ liệu được hiển thị trong Hình 4. Đầu tiên, chúng tôi bắt đầu với một văn bản x từ tập dữ liệu, sau đó đóng gói nó với hướng dẫn prompt bổ sung như chúng tôi minh họa trong Hình 4 và gửi nó đến GPT-3.5-turbo để tạo ra một tóm tắt. Nếu độ dài của tóm tắt đáp ứng yêu cầu, quy trình được kết thúc; nếu không, một prompt tiếp theo sẽ hướng dẫn GPT-3.5-turbo rút ngắn thêm tóm tắt đến độ dài mong muốn. Bằng cách làm này, chúng tôi xây dựng một bộ sưu tập các tập dữ liệu đào tạo với độ dài tóm tắt khác nhau. Chúng tôi xây dựng ba tập dữ liệu là 30%, 50%, và 70% độ dài gốc của chúng. Mỗi tập dữ liệu được sử dụng để tinh chỉnh một mô hình tóm tắt độc lập. Chúng tôi trích xuất ngẫu nhiên từ FRDB và tạo ra 400, 50, và 50 mẫu cho đào tạo, xác thực, và thử nghiệm tương ứng. Đào tạo trên các tập dữ liệu được tạo ra không chỉ cho phép mô hình tạo ra các tóm tắt có độ dài mong muốn, mà còn làm quen mô hình với thông tin chuyên biệt theo lĩnh vực bằng cách thực hiện thích ứng lĩnh vực thêm (Gururangan et al., 2020).

#### 4.2.2 Nén Ngữ nghĩa

Chúng tôi đề xuất một phương pháp nén khác dựa trên nhiễu của câu gốc và xếp hạng tác động của tầm quan trọng ngữ nghĩa cho mỗi từ trong câu nơi các từ có tầm quan trọng ít hơn sẽ được loại bỏ. Chúng tôi triển khai một sentence-transformer đa ngôn ngữ (Reimers và Gurevych, 2020) để mã hóa một câu thành embedding χ0. Sau đó chúng tôi lặp lại loại bỏ một từ trong câu và có được một embedding cập nhật χi nơi i là chỉ số của từ trong câu và n là số từ trong câu. Chúng tôi có một tập mới L theo dõi khoảng cách Euclidean giữa embedding gốc và bị nhiễu L = {L2(χ0, χ1), . . . , L2(χ0, χn)}. Chúng tôi ký hiệu pj là giá trị của phần trăm thứ j trong L. Lj là tập con mới đã loại bỏ các phần tử phần trăm dưới thứ j:

Lj = {ω ∈ L, ω > pj}     (2)

Các từ tương ứng với các phần tử trong tập Lj được trích xuất làm ngữ cảnh cho LLM.

## 5 Đánh giá

### 5.1 Thiết lập Thí nghiệm

Chúng tôi tiến hành nghiên cứu trên tập dữ liệu FRDB. Nén tóm tắt dựa trên mT5-multilingual-XLSum được đào tạo trước (Xue et al., 2020). Độ dài đầu vào và đầu ra tối đa được đặt thành 512, tỷ lệ học là 2e-5, số epoch được đặt thành 10, kích thước batch là 2, và phần còn lại của các thiết lập tuân theo các thiết lập mặc định từ các mô hình. Việc đào tạo các mô hình mT5 được tiến hành trên máy chủ chứa CPU AMD EPYC 7763, RAM 256GB, và GPU NVIDIA 4090 với bộ nhớ 24GB. Theo phương pháp được mô tả trong phần 4.2.1, ba phiên bản rút ngắn của tập dữ liệu tóm tắt với 30%, 50%, và 70% độ dài gốc của nó được tạo ra. Mỗi phiên bản được sử dụng để tinh chỉnh một mô hình tóm tắt mT5. Sentence-transformer đa ngôn ngữ được đào tạo trước (Reimers và Gurevych, 2020) được sử dụng làm bộ mã hóa embedding cho nén ngữ nghĩa. Ba câu nén trong 30%, 50%, và 70% độ dài gốc của chúng được tạo ra. GPT-3.5-turbo (OpenAI, 2023c) được sử dụng để xử lý các prompt được tạo bởi các phương pháp của chúng tôi.

### 5.2 Kết quả Nén Token

Chúng tôi tiến hành các thí nghiệm trên FRDB chứa 1.000 câu hỏi đa lựa chọn chuyên biệt về thực phẩm cho mẹ và bé, mỗi câu chỉ chứa một câu trả lời đúng. Ba phương pháp nén câu được đánh giá trong các thí nghiệm của chúng tôi: 1) xóa ngẫu nhiên xóa ngẫu nhiên các từ từ một câu, 2) nén tóm tắt, và 3) nén ngữ nghĩa.

Kết quả thí nghiệm được hiển thị trong Hình 5. Để xây dựng đường cơ sở, chúng tôi đánh giá hiệu suất GPT-3.5-turbo trong hai cấu hình: không có truy xuất và có truy xuất nhưng không có nén token. Chúng tôi quan sát từ kết quả rằng, một khi thông tin bổ sung được đưa vào mô hình làm ngữ cảnh, độ chính xác ngay lập tức cải thiện, từ 51% lên 90.2%. Điều này cho thấy lợi ích mà việc truy xuất thông tin lĩnh vực mang lại cho LLM.

Tiếp theo, chúng tôi so sánh việc sử dụng mô hình dựa trên mT5 được đào tạo trước gốc với việc sử dụng phiên bản nơi mô hình được tinh chỉnh trên các tập dữ liệu được tạo bởi tự hướng dẫn. Với tinh chỉnh, độ chính xác cải thiện đáng kể từ 60% lên 90.6%. Mô hình tóm tắt không có tinh chỉnh có độ dài đầu ra trung bình là 15 từ, so với 88, 46, và 21 từ cho mô hình được tinh chỉnh tập dữ liệu độ dài 70%, 50%, và 30% của chúng tôi tương ứng. Nó cho thấy rằng mô hình tóm tắt có thể loại bỏ thông tin quan trọng do nhầm lẫn nếu văn bản đầu vào là về một chủ đề mà mô hình tóm tắt không quen thuộc. Một tập tinh chỉnh nhỏ (400 mẫu) là đủ để giúp mô hình tóm tắt thích ứng với lĩnh vực mới.

Phương pháp nén thứ hai chúng tôi đề xuất là nén ngữ nghĩa. Nó có tính linh hoạt tốt hơn để tạo ra các độ dài token khác nhau nhưng mang lại hiệu suất thấp hơn so với phương pháp nén tóm tắt. Đường cơ sở cho thí nghiệm của chúng tôi là xóa ngẫu nhiên nơi chúng tôi xóa ngẫu nhiên một tỷ lệ phần trăm nhất định các từ. Việc xóa ngẫu nhiên này liên tục ghi điểm hiệu suất thấp hơn cả hai thuật toán được đề xuất của chúng tôi.

### 5.3 So sánh Giảm Chi phí

Phương pháp nén tóm tắt được đề xuất của chúng tôi được thử nghiệm trên một máy chủ với một GPU NVIDIA 4090 (24GB), RAM 32GB và CPU Intel i7. Thời gian chạy cho hệ thống như vậy là trung bình 0.383s mỗi mẫu. Một máy chủ tương tự trên AWS, tức là g5.2xlarge, có GPU A10G, RAM 32GB, và vCPU 8 lõi và giá theo giờ cho hệ thống như vậy là $0.485. Trong một giờ, hệ thống như vậy có thể xử lý khoảng 9.400 tóm tắt, vì vậy chi phí mỗi tóm tắt là $5.16e-5. Giả sử tỷ lệ sử dụng hệ thống chỉ là 50%, điều đó có nghĩa là chi phí mỗi tóm tắt là $1.0319e-04.

Bản thân GPT-3.5-turbo không có đủ kiến thức để trả lời chính xác câu hỏi từ tập dữ liệu của chúng tôi (độ chính xác 51% nếu không có ngữ cảnh bổ sung trong prompt). Do đó, cả GPT-3.5-turbo tăng cường truy xuất thông thường và hệ thống của chúng tôi (GPT-3.5-turbo tăng cường truy xuất với nén token bổ sung) đều yêu cầu truy xuất dày đặc để đạt hiệu suất chấp nhận được, và chi phí truy xuất, là $0.0001 cho 1.000 token truy vấn, là như nhau cho cả hai hệ thống. Vì các câu hỏi gốc thường ngắn, chúng tôi có thể giả định rằng độ dài trung bình của chúng là khoảng 128 token, chuyển thành $1.2500e-05 cho mỗi câu hỏi cho truy xuất. Giả sử ở kích thước đầy đủ đầu vào có 512 token và đầu ra có 64 token, tổng chi phí cho GPT-3.5-turbo tăng cường truy xuất thông thường (cho cả truy xuất và QA sử dụng các cuộc gọi API) là khoảng $8.9050e-04 cho mỗi câu hỏi. So với đó, thuật toán của chúng tôi có thể nén ngữ cảnh được truy xuất của GPT-3.5-turbo xuống 35% độ dài gốc của nó, chuyển thành trung bình 50% giảm trong các cuộc gọi API. Do đó, Chi phí sử dụng hệ thống nén token của chúng tôi là khoảng $6.1869e-04 cho mỗi câu hỏi. Nó giảm chi phí tổng thể 30%.

### 5.4 Entropy Thông tin vs Độ chính xác

Trong thí nghiệm tiếp theo, chúng tôi điều tra tác động của entropy thông tin đến độ chính xác của các phương pháp nén token khác nhau. Chúng tôi đo tần suất từ từ tập dữ liệu FRDB và sử dụng nó như một mô hình xác suất để tính entropy thông tin của mỗi từ. Entropy thông tin từ trung bình được tính trên mỗi câu để chuẩn hóa entropy. Kết quả cho ba phương pháp nén token được hiển thị trong Hình 6. Phương pháp xóa ngẫu nhiên loại bỏ các từ ngẫu nhiên dẫn đến entropy thông tin trung bình cho các độ dài câu khác nhau gần như giống nhau. Mặt khác, thuật toán nén ngữ nghĩa loại bỏ các từ có ý nghĩa ngữ nghĩa ít hơn. Thí nghiệm của chúng tôi cho thấy rằng, entropy thông tin trung bình giảm khi các câu trở nên ngắn hơn, cho thấy rằng câu trở nên ít nén được hơn. Ngoài ra, entropy thông tin từ trung bình có tương quan tích cực với độ chính xác khi nén ngữ nghĩa được sử dụng, cho thấy thông tin cao hơn sẽ có lợi cho hiệu suất mô hình. Ngược lại, nén tóm tắt cho thấy hiện tượng khác biệt. Thay vì loại bỏ các từ một cách ngây thơ, nén tóm tắt nén các câu gốc thành các độ dài khác nhau bằng cách diễn đạt lại các câu. Bằng cách làm này, các câu được rút ngắn có được entropy thông tin trung bình thấp hơn nhưng độ chính xác vẫn ở mức tương tự so với các câu gốc. Entropy thông tin trung bình thấp hơn cho thấy các câu trở nên cô đọng hơn nhưng ngữ nghĩa của các câu vẫn gần như giống nhau.

Tiếp theo, chúng tôi điều tra tác động của độ tương tự cosine giữa các câu gốc và được nén đến độ chính xác và chúng tôi tìm thấy một tương quan tích cực giữa độ chính xác và giá trị độ tương tự cosine. Nó cho thấy gần với ý nghĩa ngữ nghĩa gốc sẽ tạo ra độ chính xác tốt hơn.

### 5.5 Thống kê Tóm tắt

Mô hình tóm tắt của chúng tôi được xây dựng dựa trên mô hình mT5 được đào tạo trước và được tinh chỉnh trên tập dữ liệu được tạo tự hướng dẫn của chúng tôi. Chúng tôi tạo ra ba tập dữ liệu là 30%, 50%, và 70% độ dài so với văn bản gốc của chúng. Ba mô hình khác nhau được tinh chỉnh độc lập. Hình 8 cho thấy phân phối độ dài câu từ ba mô hình được tinh chỉnh của chúng tôi. Ở nén 70%, văn bản tóm tắt chuyển từ trung bình 88 từ xuống 46 từ cho độ dài 50% và 21 từ cho độ dài 30%.

### 5.6 Tỷ lệ Nén vs Entropy

Chúng tôi tiến hành một nghiên cứu về tỷ lệ nén của mô hình mT5 được tinh chỉnh với tập dữ liệu độ dài 30%. Đầu vào của chúng tôi bao gồm 1) độ dài gốc của câu, 2) entropy từ trung bình, và 3) entropy từ tích lũy. Chúng tôi triển khai một thuật toán hồi quy tuyến tính đơn giản để dự đoán tỷ lệ nén. Kết quả được hiển thị trong Hình 9. Chúng tôi thấy rằng, có một tương quan tích cực (0.31) giữa đầu vào được chọn của chúng tôi và tỷ lệ nén với RMSE là 11.4% và R-squared là 9.6%. Điều này cho thấy tỷ lệ nén của mỗi câu có thể được ước tính trước quá trình tóm tắt.

### 5.7 Nghiên cứu Ablation

Từ các thí nghiệm của chúng tôi, chúng tôi thấy phương pháp nén tóm tắt mang lại hiệu suất tốt nhất. Ở đây chúng tôi so sánh các phương pháp truy xuất khác nhau và điều tra các thiết lập tối ưu là gì. Bốn cấu hình được đánh giá: chỉ embedding, chỉ BM25, embedding trước sau đó BM25, và BM25 trước sau đó embedding. Hai cấu hình đầu tiên rất đơn giản; trong cấu hình thứ ba, chúng tôi áp dụng phương pháp dựa trên embedding để trích xuất các kết quả top-q, sau đó áp dụng BM25 để trích xuất các kết quả top-k từ các kết quả q nơi q ≥ k. Trong cấu hình thứ tư, chúng tôi đảo ngược thứ tự nơi chúng tôi đầu tiên trích xuất các kết quả top-q sử dụng BM25 và sau đó trích xuất các kết quả top-k từ các kết quả q sử dụng các phương pháp dựa trên embedding. Chúng tôi đặt k = 3 dựa trên các thí nghiệm trước đây. Kết quả đánh giá được hiển thị trong Bảng 5. Chúng tôi thấy cách tiếp cận truy xuất dày đặc đơn giản đạt được hiệu suất tốt nhất.

| Phương pháp | Top-q | Độ chính xác (%) |
|---|---|---|
| Embedding | n/a | 90.9 |
| Embedding+BM25 | 10 | 89.7 |
| Embedding+BM25 | 100 | 88.0 |
| BM25 | n/a | 74.7 |
| BM25+Embedding | 10 | 89.3 |
| BM25+Embedding | 100 | 89.8 |

## 6 Kết luận

Trong bài báo này, chúng tôi đề xuất hai phương pháp giảm kích thước token cho LLM tăng cường truy xuất. Ngoài ra, chúng tôi đề xuất một tập dữ liệu gợi ý thực phẩm chứa kiến thức chuyên biệt theo lĩnh vực để đánh giá hiệu suất LLM tăng cường truy xuất trên GPT-3.5-turbo. Chúng tôi cẩn thận chọn một tập con tập trung vào 200 loại gợi ý thực phẩm cho người mẹ và trẻ sơ sinh. Không có tăng cường truy xuất, mô hình GPT-3.5-turbo thương mại chỉ có thể trả lời đúng 51% câu hỏi, so với 90.2% với ngữ cảnh được truy xuất. Chúng tôi sử dụng 90.2% này làm mục tiêu và so sánh hiệu suất của các thuật toán nén token khác nhau. Nén tóm tắt được đề xuất của chúng tôi đạt được hiệu suất tốt nhất, đạt 90.5% độ chính xác với giảm 65% token trên ngữ cảnh được truy xuất. Nó cho thấy rằng văn bản được tóm tắt duy trì mức độ thông tin quan trọng tương tự nhưng với độ dài ngắn hơn đáng kể, cho thấy khả năng đầy hứa hẹn của phương pháp được đề xuất của chúng tôi. Phương pháp nén ngữ nghĩa có thể loại bỏ thêm các từ có ý nghĩa ngữ nghĩa thấp hơn và cung cấp cách linh hoạt hơn để đánh đổi độ dài của các câu với độ chính xác.

## Hạn chế

Mục tiêu của mô hình chúng tôi là giảm kích thước token cho LLM tăng cường truy xuất. Tỷ lệ nén được xác định dựa trên khả năng của các phương pháp để cô đọng các câu trong khi bảo tồn càng nhiều thông tin thiết yếu của chúng càng tốt. Nếu các câu đã ngắn và cô đọng về ý nghĩa, tỷ lệ nén sẽ không thể thấp nếu chúng ta muốn duy trì hầu hết thông tin quan trọng trong câu. Thuật toán của chúng tôi được thiết kế cho các LLM thương mại lớn và các LLM mã nguồn mở nhỏ hơn có thể không trải qua mức độ cải thiện hiệu suất tương tự.

## Tuyên bố Đạo đức

Các phương pháp được đề xuất của chúng tôi được thiết kế hoàn toàn cho mục tiêu giảm chi phí trong khi duy trì hiệu suất sử dụng LLM thương mại thông qua các cuộc gọi API. Các thuật toán không được thiết kế cho các hoạt động vi phạm quyền con người hoặc với mục đích quân sự. Dữ liệu chúng tôi thu thập cho tập dữ liệu được đề xuất không chứa bất kỳ thông tin riêng tư nào.

## Tài liệu Tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên như trong bản gốc tiếng Anh]

## Phụ lục

### A Câu hỏi ví dụ cho tập dữ liệu FRDB

Có phù hợp để trẻ sơ sinh tiêu thụ/uống sôcôla nóng không? (1) Khuyến khích, (2) Trung tính, (3) Tránh

1. Thực phẩm có lợi cho giai đoạn hiện tại, và tiêu thụ quá mức sẽ không gây ra bất thường về thể chất.
2. Ủng hộ lối sống lành mạnh và khuyên người dùng nên tiêu thụ lượng tương đối hạn chế các thực phẩm, bao gồm thực phẩm có quá nhiều muối, dầu, đường, v.v.
3. Có bằng chứng rằng nó sẽ gây hại sau khi ăn; Tài liệu có thẩm quyền chỉ ra rằng việc ăn bị cấm ở một giai đoạn nhất định.

### B Ví dụ nén câu

Các ví dụ được hiển thị trong Bảng 6.

### C Ví dụ QA

Các ví dụ QA được hiển thị trong Bảng 7.

### D Ví dụ kiến thức

Các ví dụ kiến thức được sử dụng để trả lời các câu hỏi được hiển thị trong Bảng 8.
