# 2304.09842.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2304.09842.pdf
# Kích thước file: 2226980 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Chameleon: Lý luận tổng hợp linh hoạt 
với các mô hình ngôn ngữ lớn
Pan Lu1, Baolin Peng2, Hao Cheng2, Michel Galley2
Kai-Wei Chang1, Ying Nian Wu1, Song-Chun Zhu1, Jianfeng Gao2
1Đại học California, Los Angeles 2Microsoft Research, Redmond
https://chameleon-llm.github.io

Bộ tạo câu trả lời
-Câu hỏi là về việc xác định sức hấp dẫn thuyết phục chính được sử dụng trong một quảng cáo. -Có ba loại sức hấp dẫn thuyết phục chính: ethos, pathos, và logos. -Ethos là một sức hấp dẫn đến uy tín, tính cách, hoặc thẩm quyền của người nói hoặc người viết. - Pathos là một sức hấp dẫn đến cảm xúc, tình cảm, hoặc sự đồng cảm của khán giả. - Logos là một sức hấp dẫn đến logic, lý luận, hoặc tính hợp lý, sử dụng sự kiện, thống kê, hoặc luận điểm. [194, 21]: Đĩa giấy Earthzy [244, 70]: bây giờ [172, 112]: mang dấu ấn [231, 159]: Sierra Club [275, 198]: phê duyệt

Phân tích quảng cáo để xác định loại sức hấp dẫn thuyết phục nào được sử dụng. Quảng cáo đề cập rằng đĩa giấy Earthzy mang dấu ấn phê duyệt của Sierra Club. Điều này ngụ ý rằng sản phẩm thân thiện với môi trường và đã được một tổ chức có uy tín chứng thực. Sức hấp dẫn này chủ yếu dựa trên uy tín và thẩm quyền của Sierra Club, đây là một ví dụ về ethos (tính cách).

Tìm kiếm Bing
Bộ tạo giải pháp
Bộ tạo truy vấn

Sức hấp dẫn thuyết phục chính nào được sử dụng trong quảng cáo này?
(A) pathos (cảm xúc) (B) ethos (tính cách) (C) logos (lý luận)
ethos (tính cách)

Hướng của lực đẩy này là gì?
(A) tránh xa gậy bóng chày (B) hướng về gậy bóng chày

Bộ mô tả hình ảnh
Truy xuất kiến thức
Bộ tạo câu trả lời
Bộ tạo giải pháp
Bộ mô tả hình ảnh
Bộ tạo câu trả lời
Bộ tạo giải pháp
Truy xuất kiến thức
Bộ phát hiện văn bản

Da của loài động vật nào được thích nghi để sinh tồn ở những nơi lạnh?
(A) Eurasian lynx (B) Thorny Devil

11
22
33

Hình 1: Các ví dụ từ phương pháp Chameleon với GPT-4 trên ScienceQA [32], một benchmark trả lời câu hỏi đa phương thức trong các lĩnh vực khoa học. Chameleon thích nghi với các truy vấn khác nhau bằng cách tổng hợp các chương trình để kết hợp nhiều công cụ khác nhau và thực thi chúng theo thứ tự để có được câu trả lời cuối cùng.

Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) đã đạt được tiến bộ đáng kể trong việc giải quyết các nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau nhờ khả năng lý luận nổi bật. Tuy nhiên, LLMs có những hạn chế cố hữu vì chúng không thể truy cập thông tin cập nhật (được lưu trữ trên Web hoặc trong các cơ sở tri thức đặc thù của nhiệm vụ), sử dụng các công cụ bên ngoài, và thực hiện lý luận toán học và logic chính xác. Trong bài báo này, chúng tôi trình bày Chameleon, một hệ thống AI giảm thiểu những hạn chế này bằng cách tăng cường LLMs với các module linh hoạt cho lý luận tổng hợp. Chameleon tổng hợp các chương trình bằng cách kết hợp nhiều công cụ khác nhau (ví dụ: LLMs, các mô hình thị giác có sẵn, các công cụ tìm kiếm web, các hàm Python, và các module dựa trên heuristic) để thực hiện các nhiệm vụ lý luận phức tạp. Cốt lõi của Chameleon là một bộ lập kế hoạch dựa trên LLM tập hợp một chuỗi các công cụ để thực thi nhằm tạo ra phản hồi cuối cùng. Chúng tôi thể hiện hiệu quả của Chameleon trên hai nhiệm vụ lý luận đa phương thức chuyên sâu về kiến thức: ScienceQA và TabMWP. Chameleon, được hỗ trợ bởi GPT-4, đạt độ chính xác tổng thể 86.54% trên ScienceQA, cải thiện kết quả few-shot tốt nhất đã công bố 11.37%. Trên TabMWP, Chameleon được hỗ trợ bởi GPT-4 cải thiện độ chính xác 17.0%, nâng trạng thái nghệ thuật lên 98.78%. Phân tích của chúng tôi cũng cho thấy bộ lập kế hoạch được hỗ trợ bởi GPT-4 thể hiện việc lựa chọn công cụ nhất quán và hợp lý hơn thông qua việc suy luận các ràng buộc tiềm năng từ hướng dẫn, so với bộ lập kế hoạch được hỗ trợ bởi ChatGPT.

Tiêu đề này lấy cảm hứng từ khả năng thích nghi và hòa hợp với môi trường xung quanh của tắc kè hoa, điều này tương đồng với khả năng thích nghi và tính linh hoạt của các mô hình ngôn ngữ lớn trong các nhiệm vụ lý luận tổng hợp với các công cụ bên ngoài.

Hội nghị lần thứ 37 về Hệ thống xử lý thông tin thần kinh (NeurIPS 2023). arXiv:2304.09842v3 [cs.CL] 31 Oct 2023

--- TRANG 2 ---
1 Giới thiệu

Tiến bộ đáng kể đã được quan sát thấy trong các mô hình ngôn ngữ lớn (LLMs) gần đây cho nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau, với các ví dụ nổi bật như GPT-3 [4], PaLM [8], LLaMA [64], ChatGPT [41], và GPT-4 được phát triển gần đây [42]. LLMs đã thể hiện những khả năng nổi bật, bao gồm học trong ngữ cảnh và lý luận chuỗi tư duy (CoT) [56]. Những mô hình này có khả năng giải quyết các nhiệm vụ đa dạng theo cách zero-shot [25] hoặc với sự hỗ trợ của một vài ví dụ [57], và chúng cho thấy tiềm năng lớn trong lập kế hoạch và ra quyết định tương tự như con người [17,16]. Mặc dù có những khả năng này, LLMs gặp phải những hạn chế cố hữu, như không thể truy cập thông tin cập nhật [26], thực hiện lý luận toán học chính xác [44,35], hoặc sử dụng các mô hình chuyên biệt [49]. Do đó, việc tăng cường LLMs hiện tại với khả năng tự động kết hợp các công cụ bên ngoài để giải quyết các nhiệm vụ thực tế là rất quan trọng để giải quyết những nhược điểm này.

Xem xét ví dụ ② trong Hình 1: Sức hấp dẫn thuyết phục chính nào được sử dụng trong quảng cáo này?. Để trả lời câu hỏi này, người ta cần: 1) suy luận rằng có một hình ảnh quảng cáo chứa ngữ cảnh văn bản và gọi một bộ giải mã văn bản để hiểu ngữ nghĩa; 2) truy xuất kiến thức nền về các sức hấp dẫn thuyết phục và sự khác biệt giữa ba sức hấp dẫn thuyết phục; 3) tạo ra một giải pháp dựa trên truy vấn đầu vào và kết quả trung gian từ các bước trước; và 4) cuối cùng tạo ra câu trả lời theo định dạng đặc thù của nhiệm vụ. Mặt khác, khi trả lời Da của loài động vật nào được thích nghi để sinh tồn ở những nơi lạnh (③), người ta có thể cần gọi các module như bộ mô tả hình ảnh để giải mã thông tin hình ảnh và công cụ tìm kiếm web để truy xuất kiến thức lĩnh vực nhằm hiểu các thuật ngữ khoa học. Tuy nhiên, các LLMs được tăng cường công cụ hiện tại vẫn gặp thách thức khi giải quyết những truy vấn thực tế này trong các kịch bản khác nhau. Hầu hết các phương pháp hiện có hoặc bị giới hạn ở một số lượng nhỏ công cụ [39,6,55,18,43,49] hoặc dựa vào các công cụ đặc thù lĩnh vực [40,60,13,59,52], và do đó không dễ tổng quát hóa cho các truy vấn của lĩnh vực mới (xem phần 2 và A.1 để thảo luận thêm). Trong công trình này, chúng tôi nghiên cứu cách cho phép LLMs tổng hợp các chương trình để nắm bắt logic của việc kết hợp các công cụ không đồng nhất.

Để giải quyết những thách thức của công việc hiện có, chúng tôi giới thiệu Chameleon, một framework lý luận tổng hợp linh hoạt tận dụng LLMs để tổng hợp các chương trình và kết hợp nhiều công cụ khác nhau cho một loạt các nhiệm vụ. Không giống như các LLMs được tăng cường công cụ hiện có [49,40,60,13,59,52], Chameleon sử dụng một tập hợp công cụ phong phú hơn, bao gồm LLMs, các mô hình thị giác có sẵn, các công cụ tìm kiếm web, các hàm Python, và các module dựa trên heuristic. Hơn nữa, Chameleon tận dụng khả năng học trong ngữ cảnh của LLMs và xây dựng trên một LLM như một bộ lập kế hoạch ngôn ngữ tự nhiên, mà không yêu cầu bất kỳ việc huấn luyện nào hoặc các quy tắc được thiết kế cẩn thận. Được hướng dẫn bởi các mô tả công cụ và ví dụ sử dụng, bộ lập kế hoạch suy luận một chương trình bao gồm một chuỗi các công cụ để thực thi nhằm tạo ra phản hồi cuối cùng cho một truy vấn của người dùng. Thay vì tạo ra các chương trình trong các ngôn ngữ đặc thù lĩnh vực [40,52,13], Chameleon tạo ra các chương trình giống ngôn ngữ tự nhiên (NL) (ví dụ: [Text_Detector, Knowledge_Retrieval, Solution_Generator, Answer_Generator] cho truy vấn thứ hai trong Hình 1). Các chương trình giống NL dễ hiểu và gỡ lỗi cho người dùng có kinh nghiệm lập trình hạn chế, và dễ dàng mở rộng cho các module mới. Trong quá trình thực thi mỗi module, module xử lý truy vấn và ngữ cảnh được lưu trong bộ nhớ đệm, trả về một kết quả được xác định bởi chính module đó, và cập nhật truy vấn và ngữ cảnh cho việc thực thi tiếp theo. Việc kết hợp các module như một chương trình tuần tự cho phép các module tiếp theo tận dụng ngữ cảnh được lưu trong bộ nhớ đệm trước đó và các truy vấn được cập nhật.

Chúng tôi thể hiện khả năng thích nghi và hiệu quả của Chameleon trên hai nhiệm vụ: ScienceQA [32] và TabMWP [33]. ScienceQA là một benchmark trả lời câu hỏi đa phương thức trải rộng trên nhiều định dạng ngữ cảnh và các chủ đề khoa học khác nhau, trong khi TabMWP là một benchmark toán học liên quan đến các ngữ cảnh bảng đa dạng. Hai benchmark này phục vụ như một testbed tốt để đánh giá khả năng của Chameleon trong việc phối hợp các công cụ đa dạng trên các loại và lĩnh vực khác nhau. Đáng chú ý, Chameleon với GPT-4 đạt độ chính xác 86.54% trên ScienceQA, cải thiện đáng kể so với mô hình few-shot tốt nhất đã công bố 11.37%. Trên TabMWP, sử dụng GPT-4 làm LLM cơ bản, Chameleon đạt được cải thiện 7.97% so với GPT-4 được gợi ý chuỗi tư duy (CoT) [57] và tăng 17.0% so với mô hình tốt nhất đã công bố [6], nâng trạng thái nghệ thuật lên 98.78%. Các nghiên cứu thêm cho thấy việc sử dụng GPT-4 như một bộ lập kế hoạch thể hiện việc lựa chọn công cụ nhất quán và hợp lý hơn và có thể suy luận các ràng buộc tiềm năng đã cho trong hướng dẫn, so với các LLMs khác như ChatGPT.

Những đóng góp của chúng tôi như sau: (1) Chúng tôi phát triển một framework lý luận tổng hợp linh hoạt, Chameleon, có hiệu quả kết hợp các công cụ bên ngoài để giải quyết những hạn chế cố hữu của LLMs và xử lý một loạt rộng các nhiệm vụ lý luận. (2) Dựa vào một LLM như một bộ lập kế hoạch ngôn ngữ tự nhiên để tạo ra các chương trình, Chameleon thành công tích hợp nhiều công cụ khác nhau, bao gồm LLMs, các mô hình thị giác có sẵn, các công cụ tìm kiếm web, các hàm Python, và các module dựa trên quy tắc, để xây dựng một hệ thống AI linh hoạt và

--- TRANG 3 ---
9 Bảng này cho thấy số dặm Wanda đã đi bộ đường dài mỗi ngày trong chuyến cắm trại của cô ấy. Trung vị là giá trị ở giữa trong một tập dữ liệu khi dữ liệu được sắp xếp theo thứ tự. Để tìm trung vị, dữ liệu phải được sắp xếp theo thứ tự từ nhỏ nhất đến lớn nhất (hoặc lớn nhất đến nhỏ nhất), và sau đó (các) giá trị ở giữa được xác định.

Bộ tạo câu trả lời
Truy xuất kiến thức
Bộ tạo chương trình
Bộ xác minh chương trình Bộ thực thi chương trình

Wanda đã đi cắm trại và ghi lại số dặm cô ấy đi bộ đường dài mỗi ngày. Trung vị của các số là bao nhiêu?
miles_hiked = [10, 9, 10, 5, 9]
miles_hiked = sorted(miles_hiked)
middle1 = (len(miles_hiked) - 1) // 2
middle2 = len(miles_hiked) // 2
ans = (miles_hiked[middle1] + miles_hiked[middle2]) / 2
ans=9.0

Dặm đi bộ đường dài
Ngày Dặm
Chủ nhật 10
Thứ hai 9
Thứ ba 10
Thứ tư 5
Thứ năm 9

Nhìn vào lịch trình sau. Xe buýt khởi hành từ ga tàu lúc nào?
(A)12:35 P.M. (B)1:10 P.M. (C)1:10 P.M. (D)10:45 A.M.

Địa điểm Đến Khởi hành
sân vận động 10:20 A.M. 10:25 A.M.
công viên 10:35 A.M. 10:45 A.M.
khách sạn 11:10 A.M. 11:15 A.M.
sân bay 12:05 P.M. 12:10 P.M.
ga tàu 12:25 P.M. 12:35 P.M.
bến xe buýt 1:10 P.M. 1:10 P.M.

12:35 P.M.

Bộ tạo câu trả lời
Tra cứu hàng
Bộ tạo giải pháp

Địa điểm Đến Khởi hành
ga tàu 12:25 P.M. 12:35 P.M.

(Bước 1) Tìm ga tàu trên lịch trình. Tìm thời gian khởi hành cho ga tàu.
(Bước 2) Ga tàu: 12:35 P.M. Xe buýt khởi hành từ ga tàu lúc 12:35 P.M.
(Bước 3) Câu trả lời là 12:35 P.M.

11
22

Hình 2: Hai ví dụ từ phương pháp Chameleon với GPT-4 trên TabMWP [33], một benchmark lý luận toán học với các ngữ cảnh bảng. Chameleon thể hiện tính linh hoạt và hiệu quả trong việc thích nghi với các truy vấn khác nhau đòi hỏi nhiều khả năng lý luận khác nhau.

hệ thống AI thích nghi có khả năng trả lời các truy vấn thực tế. (3) Chúng tôi chứng minh hiệu quả của Chameleon trên hai benchmark thách thức, vượt qua đáng kể trạng thái nghệ thuật.

2 Công việc liên quan

Lý luận tổng hợp Các phương pháp modular thần kinh và tổng hợp đã được khám phá để tự động thực hiện việc phân tách nhiệm vụ phụ mong muốn, tăng cường khả năng diễn giải và thích nghi trên các nhiệm vụ lý luận khác nhau. Công việc đầu tiên [2,3] cho rằng các nhiệm vụ lý luận phức tạp về cơ bản là tổng hợp và đề xuất các mạng module thần kinh (NMN) để phân tách chúng thành các nhiệm vụ phụ. Tuy nhiên, những phương pháp này dựa vào các parser có sẵn dễ vỡ và bị giới hạn bởi các cấu hình module. Một số công việc sau này [19,15,14,21], tiến thêm một bước bằng cách dự đoán các bố cục mạng đặc thù cho từng instance theo cách end-to-end, mà không dựa vào parser, sử dụng học tăng cường [58] và học có giám sát yếu. Trong lý luận thị giác, các mô hình bao gồm một bộ tạo chương trình và một engine thực thi đã được đề xuất để kết hợp học biểu diễn sâu và thực thi chương trình ký hiệu [19,61]. Trong lĩnh vực lý luận toán học, một solver có thể diễn giải đã được phát triển để kết hợp kiến thức định lý như các quy tắc có điều kiện và thực hiện lý luận ký hiệu từng bước [31]. Công việc của chúng tôi lấy cảm hứng từ các mạng module thần kinh, nhưng nó cung cấp một số lợi thế riêng biệt. Đầu tiên, Chameleon không yêu cầu giám sát đắt đỏ về các chương trình đặc thù nhiệm vụ cho việc huấn luyện mô hình. Thay vào đó, nó tạo ra các chương trình tuần tự, bao gồm các module, dễ tổng quát hóa cho các lĩnh vực và nhiệm vụ khác nhau, cho phép mở rộng đến các module mới theo cách linh hoạt. Thứ hai, Chameleon không yêu cầu bất kỳ việc huấn luyện nào, mà sử dụng khả năng học trong ngữ cảnh của LLMs để tạo ra các chương trình được hướng dẫn bởi hướng dẫn ngôn ngữ tự nhiên và các demonstration.

Các mô hình ngôn ngữ được tăng cường công cụ Trong những năm gần đây, sự phát triển của các mô hình ngôn ngữ lớn (LLMs) [48,8,9,53,4,41,42] đã có tiến bộ to lớn và đã kích thích nghiên cứu trong học prompt [57,33,22] và học hướng dẫn [53,64,46,11]. Mặc dù hiệu suất ấn tượng của LLMs, chúng gặp phải những hạn chế cố hữu, như không thể truy cập thông tin cập nhật [26], sử dụng các công cụ bên ngoài [49], hoặc thực hiện lý luận toán học chính xác [44,35]. Các benchmark gần đây, như ScienceQA và TabMWP [32,33,7,54,51,30], đã xuất hiện để đánh giá khả năng của LLMs trong việc xử lý các thách thức lý luận phức tạp, đặc biệt là những thách thức nhấn mạnh việc sử dụng các công cụ bên ngoài. Đồng thời, đã có sự quan tâm ngày càng tăng trong việc khai thác các công cụ bên ngoài và các phương pháp modular để tăng cường LLMs. Những LLMs được tăng cường này có thể truy cập thông tin thời gian thực được hỗ trợ bởi các công cụ tìm kiếm web [40] và tận dụng kiến thức đặc thù lĩnh vực từ các nguồn tài nguyên bên ngoài [62]. Một số công việc tận dụng trình thông dịch Python để tạo ra các chương trình phức tạp nhằm sử dụng các tài nguyên tính toán mạnh mẽ, và thực thi các nhiệm vụ lý luận logic hiệu quả hơn [55,10,6,39,18,43,36]. Ví dụ, Toolformer [49] xây dựng dữ liệu tăng cường sử dụng công cụ để huấn luyện các mô hình ngôn ngữ lựa chọn năm

--- TRANG 4 ---
[Tiếp tục bảng so sánh các mô hình và khả năng của chúng, với Chameleon ở dòng cuối cùng có đầy đủ các tính năng được đánh dấu ✓]

Bảng 1: So sánh các công việc tăng cường mô hình ngôn ngữ lớn với việc sử dụng công cụ. Chúng tôi báo cáo kích thước công cụ và các loại công cụ, bao gồm OpenAI (⚪), Hugging Face (🤗), Github (📂), Tìm kiếm Web (🔍), và code (💻). Chúng tôi so sánh các kỹ năng mà mỗi phương pháp sở hữu, như hiểu hình ảnh, tìm kiếm trình duyệt, truy xuất kiến thức, lý luận toán học, và hiểu bảng. Một số mô hình có thể kết hợp nhiều công cụ khác nhau, đề xuất một bộ lập kế hoạch để suy luận các công cụ liên quan cho việc thực thi, hoặc có thể mở rộng tự nhiên đến các công cụ mới. Nhãn "-" đề cập đến thông tin không chắc chắn trong tài liệu.

công cụ. Trong lĩnh vực các công cụ thị giác, nhiều phương pháp khác nhau đã được đề xuất để tăng cường khả năng của các mô hình ngôn ngữ lớn trong việc xử lý các nhiệm vụ thị giác [60,59,52,13,50], được tăng cường với các mô hình Hugging Face [50], các mô hình Azure [60], các mô hình nền tảng thị giác [59].

Chúng tôi so sánh Chameleon với các mô hình ngôn ngữ được tăng cường công cụ khác trong Bảng 1. Nhiều phương pháp trong số này hoặc bị hạn chế ở một tập hợp nhỏ các công cụ hoặc bị giới hạn ở các công cụ đặc thù nhiệm vụ, điều này làm giảm khả năng của chúng trên các khía cạnh kỹ năng khác nhau và cản trở khả năng tổng quát hóa của chúng đến các nhiệm vụ mới. Một dòng công việc gần đây dựa vào một lượng lớn giám sát [49,26] và tập trung vào việc tạo ra các lệnh [40] và chương trình [52,13] để suy luận việc lựa chọn công cụ. Tuy nhiên, phương pháp này cần các prompt được thiết kế cẩn thận cho các nhiệm vụ cụ thể và các công cụ đặc biệt, và không linh hoạt cũng không thích nghi. Ngược lại, Chameleon hướng dẫn LLMs với các hướng dẫn ngôn ngữ tự nhiên mô tả đơn giản vai trò của mỗi module và cung cấp một vài ví dụ gọi, loại bỏ nhu cầu huấn luyện bổ sung hoặc các prompt đặc thù công cụ khi học kết hợp các công cụ khác nhau. Quan trọng hơn, Chameleon cung cấp cho người dùng tính linh hoạt về các loại và nguồn công cụ, cập nhật các LLMs cơ bản, thêm các công cụ mới, và thích nghi với các nhiệm vụ mới. Công việc của chúng tôi có cùng tinh thần với AutoGPT [47], một agent GPT-4 tự trị với tham vọng trí tuệ nhân tạo tổng quát (AGI) để kết hợp nhiều công cụ nhằm đạt được các mục tiêu do người dùng xác định. Trong khi AutoGPT vẫn đang được phát triển, công việc của chúng tôi là công việc đầu tiên thực thi ý tưởng và xác minh hiệu quả của nó trên các benchmark được nghiên cứu kỹ.

3 Framework tổng quát: Chameleon

Để giải quyết những hạn chế của LLMs hiện tại trong việc sử dụng các công cụ đa dạng, chúng tôi đề xuất Chameleon, một framework lý luận tổng hợp linh hoạt mới, tổng hợp việc kết hợp các công cụ khác nhau để đáp ứng một loạt rộng các vấn đề. Chameleon bao gồm một kho module xác định các loại công cụ khác nhau và một bộ lập kế hoạch dựa trên LLM, có mục đích phân tách vấn đề ban đầu thành các nhiệm vụ phụ có thể được giải quyết hiệu quả bởi các công cụ đặc thù nhiệm vụ. Không giống như các phương pháp LLM được tăng cường công cụ hiện có [49,13,59,50], kho module của chúng tôi có nhiều loại công cụ như được minh họa trong Bảng 2, cho phép Chameleon thể hiện nhiều khả năng lý luận khác nhau, bao gồm hiểu hình ảnh, truy xuất kiến thức, tìm kiếm web, lý luận toán học phức tạp, và hiểu bảng. Thay vì tạo ra các chương trình đặc thù lĩnh vực [40,13,52], Chameleon sử dụng một bộ lập kế hoạch dựa trên LLM để tạo ra các chương trình giống ngôn ngữ tự nhiên tuân theo hướng dẫn ngôn ngữ tự nhiên, điều này ít dễ lỗi hơn, dễ mở rộng đến các module mới, và thân thiện với người dùng.

Chúng tôi hình thức hóa bộ lập kế hoạch như sau: cho truy vấn đầu vào x0, kho module M, và các ràng buộc G, bộ lập kế hoạch ngôn ngữ tự nhiên P lựa chọn một tập hợp các module có thể được thực thi tuần tự để trả lời truy vấn thông qua việc tạo ra một chương trình theo định dạng giống ngôn ngữ tự nhiên. Kho module M bao gồm một tập hợp các module được xây dựng sẵn: {Mi}, mỗi module tương ứng với một công cụ của các loại khác nhau (Bảng 2). G là các ràng buộc cho việc tạo ra kế hoạch, ví dụ, các mối quan hệ đồng thời và thứ tự

--- TRANG 5 ---
chuỗi của các module. Trong công việc của chúng tôi, bộ lập kế hoạch P là một LLM được hướng dẫn để tạo ra một chuỗi tên module trong thiết lập few-shot. Bộ lập kế hoạch được hướng dẫn bằng ngôn ngữ tự nhiên với một hướng dẫn nhiệm vụ lập kế hoạch I, các mô tả của các module trong M với các ràng buộc tương ứng G, cũng như một vài ví dụ demonstration D. Một kế hoạch độ dài T được lấy mẫu từ P có thể được ký hiệu là p = M1, . . . , MT, trong đó Mt đại diện cho phần tử thứ t trong kế hoạch được tạo ra và Mt ∈ M. Một cách chính thức, cho một truy vấn đầu vào (tuyên bố vấn đề) x0, một kế hoạch p được tạo ra như sau:

p ← P(x0; I, M, G, D). (1)

Cho kế hoạch được tạo ra, các module tương ứng cho mỗi bước sau đó được thực thi tuần tự. Kế hoạch là một chương trình ngôn ngữ tự nhiên trong đó mỗi module được liên kết đơn giản thông qua khớp chuỗi. Khi đánh giá module Mt tại thời điểm t, đầu ra của việc thực thi yt được tính bằng:

yt ← Mt(xt-1; ct-1), (2)

trong đó xt-1 là đầu vào cho module hiện tại Mt, và ct-1 là thông tin được lưu trong bộ nhớ đệm (ví dụ: ngữ nghĩa hình ảnh, kiến thức được truy xuất, các chương trình được tạo ra) từ lịch sử thực thi của các module. Cả đầu vào vấn đề xt và bộ nhớ đệm ct cho module tiếp theo Mt+1 đều được cập nhật, tương ứng, bởi:

xt ← update_input(xt-1, yt), (3)
ct ← update_cache(ct-1, yt). (4)

Các hàm update_input và update_cache được thiết kế thủ công cho mỗi Mi. Cụ thể, update_input được áp dụng cho các phần tử trong truy vấn đầu vào, bao gồm câu hỏi, ngữ cảnh bảng, và hình ảnh. Những phần tử này được cập nhật sau khi thực thi module. update_cache tương ứng với việc tạo ra thông tin mới, như mô tả cho hình ảnh đầu vào hoặc kiến thức được truy xuất từ các tài nguyên bên ngoài. Cuối cùng, phản hồi r cho truy vấn được tạo ra bởi module cuối cùng MT:

r = yT ← MT(xT-1; cT-1). (5)

4 Ứng dụng của Chameleon

Chúng tôi thể hiện các ứng dụng của Chameleon trên hai nhiệm vụ thách thức: ScienceQA [32] (phần 4.2) và TabMWP [33] (phần 4.3), sử dụng kho module được giới thiệu trong phần 4.1. Chi tiết thí nghiệm thêm có thể được tìm thấy trong phụ lục A.2.

4.1 Kho Module

Loại công cụ | Công cụ
---|---
OpenAI | Truy xuất kiến thức, Bộ tạo truy vấn, Tra cứu hàng, Tra cứu cột, Bộ diễn giải bảng, Bộ tạo chương trình, Bộ tạo giải pháp
Hugging Face | Bộ mô tả hình ảnh
Github | Bộ phát hiện văn bản 📂
Tìm kiếm Web | Tìm kiếm Bing
Python | Bộ xác minh chương trình, Bộ thực thi chương trình
Dựa trên quy tắc | Bộ tạo câu trả lời

Bảng 2: Các công cụ khác nhau trong kho module của chúng tôi.

Để đáp ứng các khả năng lý luận khác nhau trên một loạt truy vấn đa dạng, hệ thống của chúng tôi sử dụng một kho module phong phú gồm nhiều công cụ bên ngoài khác nhau. Chúng tôi cung cấp một cái nhìn tổng quan cấp cao về kho này ở đây, với các triển khai chi tiết trong các thí nghiệm cụ thể. Kho module hoàn chỉnh, M, được trình bày trong Bảng 2. Mỗi công cụ trong kho được định nghĩa như sau:

Truy xuất kiến thức: Module này truy xuất kiến thức nền bổ sung quan trọng để xử lý các vấn đề phức tạp. Nó đặc biệt có lợi cho các lĩnh vực chuyên biệt như khoa học và toán học, cung cấp ngữ cảnh cho nhiệm vụ. Ví dụ, nếu một truy vấn về bảng biểu mẫu thuế, module này có thể tạo ra kiến thức về các thủ tục thuế, cung cấp ngữ cảnh có giá trị.

Tìm kiếm Bing: Giống như "Truy xuất kiến thức", module "Tìm kiếm Bing" nhằm cung cấp kiến thức liên quan đến nhiệm vụ rộng rãi. Ngược lại, nó xuất sắc khi cần thông tin rộng hơn hoặc cập nhật từ nhiều nguồn. Sử dụng API công cụ tìm kiếm, module này trả về các kết quả tìm kiếm liên quan dựa trên truy vấn đầu vào, sau đó được phân tích và sử dụng bởi các module tiếp theo để thu thập thông tin ngữ cảnh phong phú hơn từ các nguồn đa dạng, tăng cường hiệu quả giải quyết vấn đề.

Bộ tạo truy vấn: Vì vấn đề ban đầu thường thiếu một truy vấn được thiết kế riêng để truy xuất thông tin liên quan đến nhiệm vụ, module này tạo ra các truy vấn công cụ tìm kiếm dựa trên vấn đề, sau đó được

--- TRANG 6 ---
sử dụng bởi module "Tìm kiếm Bing". Hầu hết, việc sử dụng module "Bộ tạo truy vấn" trước "Tìm kiếm Bing" là một chiến lược tốt. Kết hợp với công cụ tìm kiếm, việc tạo ra các truy vấn có mục tiêu hơn thường tạo điều kiện cho cả việc thu hồi và độ chính xác của thông tin được truy xuất.

Bộ mô tả hình ảnh: Được thiết kế để tạo ra các chú thích cho hình ảnh, module này cung cấp ngữ cảnh bổ sung quan trọng cho các truy vấn. Nó đặc biệt có giá trị khi hiểu một hình ảnh về mặt ngữ nghĩa, như xác định các đối tượng và tương tác trong một cảnh. Sử dụng các mô hình được huấn luyện trước, nó chuyển đổi dữ liệu thị giác thành ngôn ngữ, tạo điều kiện cho việc hiểu và lý luận hiệu quả về nội dung hình ảnh.

Bộ phát hiện văn bản: Module này được thiết kế để xác định văn bản trong một hình ảnh cho trước. Thông thường, "Bộ phát hiện văn bản" được sử dụng khi một câu hỏi yêu cầu trích xuất thông tin văn bản từ hình ảnh chứa biểu đồ, đồ thị, bảng, bản đồ, hoặc các yếu tố thị giác khác. Bằng cách phát hiện văn bản hiệu quả trong các định dạng khác nhau, module này hỗ trợ việc phân tích và hiểu nội dung dựa trên hình ảnh.

Tra cứu hàng: Module này rất quan trọng khi các truy vấn liên quan đến ngữ cảnh bảng, vì việc định vị các ô liên quan thường được yêu cầu. Các bảng lớn có thể làm hệ thống mất tập trung, vì vậy "Tra cứu hàng" đơn giản hóa bảng bằng cách chỉ giữ lại các hàng liên quan đến truy vấn. Nếu tất cả các hàng đều liên quan, nó trả về bảng gốc.

Tra cứu cột: Giống như module "Tra cứu hàng", "Tra cứu cột" giải quyết các câu hỏi liên quan đến ngữ cảnh bảng bằng cách tập trung vào các cột liên quan. Nó đơn giản hóa bảng bằng cách chỉ giữ lại các cột liên quan, hoặc trả về bảng gốc nếu tất cả các cột đều liên quan.

Bộ diễn giải bảng: Việc chuyển đổi các bảng có cấu trúc thành văn bản có khả năng tăng cường khả năng hiểu thông tin bảng của các module downstream khác nhau như được thể hiện bởi [37] cho việc trả lời câu hỏi miền mở, làm cho module này trở thành một phần quan trọng của hệ thống. Nó dịch các bảng thành các mô tả dễ hiểu cho các module như "Bộ tạo chương trình" và "Bộ tạo giải pháp", đặc biệt hữu ích cho các bảng nhỏ, đặc thù lĩnh vực như biểu đồ thân-lá hoặc bảng hàm.

Bộ tạo chương trình: Các phương pháp hỗ trợ chương trình được chứng minh là tăng cường khả năng lý luận logic và toán học của LLMs [55,10,6,39,18,43]. "Bộ tạo chương trình" tạo ra các chương trình Python để giải quyết các truy vấn hiệu quả, điều này đặc biệt có lợi cho các truy vấn yêu cầu tính toán phức tạp hoặc các hoạt động logic phức tạp, như câu lệnh "if-else".

Bộ xác minh chương trình: Các nghiên cứu gần đây nhấn mạnh tầm quan trọng của việc xác minh để giảm ảo giác [45,38]. Do đó, "Bộ xác minh chương trình" đảm bảo tính hợp lệ và không có lỗi của các chương trình được tạo ra bởi "Bộ tạo chương trình". Nó kiểm tra lỗi cú pháp và logic, và các vấn đề thực thi tiềm năng, tăng cường độ tin cậy và chính xác của các giải pháp.

Bộ thực thi chương trình: Module này thực thi chương trình được tạo ra bởi "Bộ tạo chương trình" và tạo ra kết quả, kết nối khoảng cách giữa việc tạo ra chương trình và dẫn xuất giải pháp cuối cùng.

Bộ tạo giải pháp: Module này tạo ra một giải pháp chi tiết cho truy vấn đầu vào sử dụng tất cả thông tin được lưu trong bộ nhớ đệm. Sử dụng phương pháp gợi ý chuỗi tư duy [57], nó đảm bảo các phản hồi mạch lạc và có cấu trúc tốt. Bộ lập kế hoạch có thể trực tiếp sử dụng module này thay vì các module chức năng khác nếu nó có thể giải quyết truy vấn một cách độc lập, đặc biệt là cho những truy vấn đơn giản hơn.

Bộ tạo câu trả lời: Module đặc thù nhiệm vụ này sử dụng phương pháp dựa trên quy tắc để trích xuất và chuẩn hóa câu trả lời từ kết quả của "Bộ thực thi chương trình" hoặc "Bộ tạo giải pháp". Không giống như "Bộ tạo giải pháp" cung cấp các giải pháp chi tiết nhiều bước, "Bộ tạo câu trả lời" phục vụ như module cuối cùng trong pipeline, cung cấp các câu trả lời ngắn gọn và đặc thù nhiệm vụ.

4.2 Trả lời câu hỏi khoa học

Trả lời câu hỏi khoa học (ScienceQA [32]) là một benchmark đa dạng cho việc trả lời câu hỏi đa phương thức trên một loạt các chủ đề và ngữ cảnh khoa học. Như các ví dụ được minh họa trong Hình 1, việc trả lời những câu hỏi này yêu cầu nhiều công cụ và kỹ năng khác nhau như mô tả hình ảnh, phát hiện văn bản, truy xuất kiến thức, tìm kiếm tài nguyên trực tuyến, và lý luận thị giác nhiều manh mối. Khi tạo ra các chương trình để sử dụng công cụ, chúng tôi giới hạn không gian tìm kiếm đến tập con kho liên quan (Bảng 6 trong phụ lục). Các chương trình được coi là không hợp lệ và mặc định về chuỗi "Bộ tạo giải pháp" và "Bộ tạo câu trả lời" nếu những phần tử này không phải là hai phần tử cuối cùng, theo baseline gợi ý chuỗi tư duy [57]. Xem Bảng 8 trong phụ lục cho prompt bộ lập kế hoạch ngôn ngữ tự nhiên được xây dựng. Các prompt cho các module dựa trên LLM như "Truy xuất kiến thức", "Bộ tạo truy vấn", và "Bộ tạo giải pháp" được thể hiện trong Bảng 10, 11, và 12, tương ứng, trong phụ lục.

--- TRANG 7 ---
[Bảng 3 tiếp tục với các kết quả đánh giá trên ScienceQA, với Chameleon đạt kết quả tốt nhất]

Bảng 3: Độ chính xác QA (%) trên tập kiểm thử của ScienceQA [32]. Chúng tôi báo cáo số tham số được điều chỉnh cho nhiệm vụ này và độ chính xác tổng thể, cùng với điểm số chính xác cho các loại câu hỏi khác nhau, bao gồm khoa học tự nhiên, xã hội, và ngôn ngữ, văn bản, hình ảnh, và không có ngữ cảnh, cũng như lớp 1-6 và 7-12. Điểm số cao nhất trong số các mô hình trong mỗi phần và tổng thể được tô sáng màu xanh và đỏ, tương ứng, và kết quả của mô hình tốt nhất của chúng tôi được đánh dấu đậm.

4.3 Lý luận toán học bảng

TabMWP [33] là một nhiệm vụ lý luận toán học liên quan đến các ngữ cảnh bảng đa dạng như lịch trình, giá cả, biểu mẫu thuế, biểu đồ, và mối quan hệ hàm (Hình 2). Nó yêu cầu các hệ thống AI hiểu nhiều định dạng bảng khác nhau và thực hiện các phép tính số hoặc ký hiệu chính xác. Giống như ScienceQA, chúng tôi hạn chế không gian tìm kiếm chương trình để tập trung vào hai loại công cụ: 1) những công cụ giúp LLMs tiêu hóa thông tin bảng tốt hơn (ví dụ: "Tra cứu hàng", "Tra cứu cột", và "Bộ diễn giải bảng") và 2) những công cụ thực hiện các phép tính ký hiệu chung thực (ví dụ: "Bộ tạo chương trình", "Bộ xác minh chương trình", và "Bộ thực thi chương trình") như được liệt kê trong Bảng 6. Các chương trình được tạo ra phải đáp ứng các ràng buộc nhất định, như bao gồm "Bộ tạo câu trả lời" và đặt "Bộ tạo chương trình" trước cả "Bộ xác minh chương trình" và "Bộ thực thi chương trình". Các chương trình không tuân thủ mặc định về chuỗi "Bộ tạo chương trình", "Bộ xác minh chương trình", "Bộ thực thi chương trình", và "Bộ tạo câu trả lời", phù hợp với baseline gợi ý chương trình tư duy [6] với việc xác minh được thêm vào.

5 Thí nghiệm

Chúng tôi đánh giá hiệu quả và khả năng thích nghi của Chameleon trên hai nhiệm vụ lý luận phức tạp, ScienceQA [32] và TabMWP [33]. Xem chi tiết thí nghiệm trong phụ lục A.2.

5.1 Kết quả thí nghiệm

ScienceQA. Bảng 3 trình bày kết quả của các baseline hiện có và phương pháp Chameleon của chúng tôi, với các kết quả chính được tô sáng trong Hình 3 (a). Sử dụng ChatGPT [41] làm LLM cơ bản, Chameleon

--- TRANG 8 ---
[Bảng 4 tiếp tục với các kết quả đánh giá trên TabMWP, với Chameleon đạt kết quả xuất sắc]

Bảng 4: Độ chính xác QA (%) trên tập kiểm thử của TabMWP [33]. Chúng tôi báo cáo số tham số được điều chỉnh cho nhiệm vụ này và độ chính xác tổng thể, và độ chính xác của các loại câu hỏi khác nhau, bao gồm câu hỏi văn bản tự do, câu hỏi nhiều lựa chọn, câu trả lời số nguyên, câu trả lời số thập phân, câu trả lời trích xuất, câu trả lời Boolean, câu trả lời văn bản khác, lớp 1-6, và lớp 7-8. * đề cập đến một tập con kết quả.

[Biểu đồ 3 cho thấy so sánh hiệu suất của các phương pháp khác nhau trên ScienceQA và TabMWP]

Hình 3: Kết quả của các baseline chính và Chameleon. Đường gạch ngang biểu thị hiệu suất của con người.

đạt độ chính xác 79.93%, cải thiện 1.62% so với ChatGPT được gợi ý Chuỗi tư duy (CoT) [57]. Đáng chú ý, Chameleon là một dạng tổng quát của CoT, trong đó chương trình được tạo ra là một chuỗi "Bộ tạo giải pháp" và "Bộ tạo câu trả lời". Chameleon có lợi từ việc sử dụng công cụ bổ sung, như "Truy xuất kiến thức", "Tìm kiếm Bing", "Bộ mô tả hình ảnh", và "Bộ phát hiện văn bản". Khi được xây dựng trên GPT-4 [42], mô hình của chúng tôi đạt độ chính xác 86.54%, vượt trội so với GPT-4 CoT [32] 2.55% và GPT-3 CoT 11.37%, tạo ra trạng thái nghệ thuật mới trong thiết lập few-shot.

TabMWP. Bảng 4 trình bày kết quả với các mô hình chính trong Hình 3 (b). Tương tự, các cải thiện đáng kể được quan sát thấy cho Chameleon so với cả các mô hình được tinh chỉnh và few-shot. Đáng chú ý là cả CoT và Chương trình tư duy (PoT) [6] đều có thể được xem như các trường hợp đặc biệt của Chameleon. Ngoài "Bộ tạo giải pháp" và "Bộ tạo câu trả lời", CoT không sử dụng bất kỳ công cụ nào, trong khi PoT

--- TRANG 9 ---
Không được gọi | Được gọi
---|---
Chameleon (ChatGPT) |
Chameleon (GPT-4) |

Truy xuất kiến thức | Bộ phát hiện văn bản | Bộ mô tả hình ảnh | Bộ tạo truy vấn | Tìm kiếm Bing

1972 17 | 30 8110 | 62 113 | 11 7083 28 | 9490 98 | 8997 89

Hình 4: Các công cụ được gọi trong các chương trình được tạo ra từ Chameleon trên ScienceQA.

Không được gọi | Được gọi
---|---
Chameleon (ChatGPT) |
Chameleon (GPT-4) |

Tra cứu hàng | Tra cứu cột | Bộ diễn giải bảng | Truy xuất kiến thức | Bộ tạo chương trình

7447 4 | 6 263 | 716 2571 | 72 9496 53 | 9397 84 | 7529 28

Hình 5: Các công cụ được gọi trong các chương trình được tạo ra từ Chameleon trên TabMWP.

chỉ dựa vào các công cụ lập trình ký hiệu như "Bộ tạo chương trình" và "Bộ thực thi chương trình". Chameleon (ChatGPT) vượt trội so với ChatGPT CoT và ChatGPT PoT lần lượt 11.25% và 3.79%, nhấn mạnh lợi thế của bộ công cụ phong phú của chúng tôi. Với GPT-4, Chameleon có thêm 5.50%, đạt độ chính xác 98.78%. Đáng chú ý, Chameleon (GPT-4) vượt qua Codex PoT-SC [6], mô hình tốt nhất đã công bố, 17.0% và hiệu suất con người 8.56%.

5.2 Phân tích định tính

Lập kế hoạch sử dụng công cụ. Tỷ lệ các công cụ chính được gọi trong các chương trình từ Chameleon trên ScienceQA và TabMWP được trực quan hóa trong Hình 4 và Hình 5, tương ứng. Thú vị là, ChatGPT và GPT-4 thể hiện các hành vi lập kế hoạch khác nhau. Nói chung, ChatGPT có xu hướng mạnh mẽ về việc sử dụng hoặc không sử dụng các công cụ nhất định, bị ảnh hưởng nhiều bởi các ví dụ trong ngữ cảnh. Ví dụ, ChatGPT gọi "Truy xuất kiến thức" trong 72% truy vấn nhưng chỉ gọi "Tìm kiếm Bing" trong 3% trường hợp trên ScienceQA; trên TabMWP, ChatGPT dựa rất nhiều vào "Tra cứu hàng" (47%) nhưng gọi "Tra cứu cột" ít thường xuyên hơn (4%). Tuy nhiên, GPT-4 hành động khách quan và hợp lý hơn trong việc lựa chọn công cụ. Ví dụ, GPT-4 gọi "Truy xuất kiến thức" thường xuyên hơn (81% so với 72%) và gọi "Tìm kiếm Bing" nhiều hơn ChatGPT (11% so với 3%) khi trả lời các câu hỏi khoa học trên ScienceQA. Ấn tượng là, GPT-4 luôn gọi "Bộ tạo truy vấn" và "Tìm kiếm Bing" đồng thời bằng cách quan sát các mô tả sử dụng công cụ, trong khi ChatGPT thiếu khả năng lý luận như vậy.

Module | ∆(ScienceQA) | ∆(TabMWP)
---|---|---
Truy xuất kiến thức | -7.8% | -2.2%
Tìm kiếm Bing | -7.4% | -
Bộ phát hiện văn bản | -8.4% | -
Bộ mô tả hình ảnh | -6.0% | -
Bộ tạo chương trình | - | -7.4%
Bộ diễn giải bảng | - | -0.2%

Bảng 5: Giảm điểm với các module bị vô hiệu hóa.

Nghiên cứu loại bỏ với các module bị vô hiệu hóa. Chúng tôi nghiên cứu sự suy giảm độ chính xác của Chameleon khi các module chính trong các chương trình được tạo ra bị vô hiệu hóa (Bảng 5), sử dụng ChatGPT làm LLMs cơ bản và 500 ví dụ kiểm thử. Kết quả cho thấy "Truy xuất kiến thức" đóng vai trò quan trọng trong cả hai nhiệm vụ. Các công cụ đặc thù lĩnh vực, như công cụ tìm kiếm và các mô hình thị giác cho ScienceQA, và các công cụ chương trình cho TabMWP, cũng được chứng minh là quan trọng.

Chuyển đổi module. Chúng tôi trực quan hóa các biểu đồ chuyển đổi của các module cho các chương trình được tạo ra bởi Chameleon (GPT-4) trên ScienceQA và TabMWP trong Hình 7 và 8, tương ứng. Các xác suất chuyển đổi trong những biểu đồ này được tính từ các chuyển đổi công cụ được quan sát trên các tập kiểm thử. Những biểu đồ này cho thấy bộ lập kế hoạch GPT-4 có thể đưa ra quyết định tốt về cách sắp xếp trình tự các công cụ trong thiết lập few-shot. Ví dụ, trên ScienceQA, Chameleon thường quyết định dựa vào "Bộ truy xuất kiến thức" hoặc "Tìm kiếm Bing", nhưng hiếm khi cả hai. Trên TabMWP, chúng tôi quan sát hai chế độ chính: hoặc đi qua module bộ tạo giải pháp hoặc thông qua bộ tạo chương trình, bộ xác minh, và bộ thực thi.

5.3 Nghiên cứu trường hợp

Ví dụ trực quan hóa của ScienceQA. Các ví dụ từ Chameleon (GPT-4) trên ScienceQA được trực quan hóa trong Hình 1. Chameleon (GPT-4) có thể thích nghi với các truy vấn đầu vào khác nhau bằng cách tạo ra các chương trình kết hợp nhiều công cụ khác nhau và thực thi chúng tuần tự để có được các phản hồi chính xác. Ví dụ, để trả lời câu hỏi đầu tiên (①), Hướng của lực đẩy này là gì?, hệ thống gọi mô hình mô tả hình ảnh để trích xuất thông tin ngữ nghĩa từ hình ảnh và sử dụng mô hình truy xuất kiến thức để thu thập kiến thức nền cho lý luận đa phương thức. Trong ví dụ thứ hai (②), bộ lập kế hoạch ngôn ngữ tự nhiên suy luận rằng cần một công cụ phát hiện văn bản để hiểu ngữ cảnh

--- TRANG 10 ---
của quảng cáo. Truy vấn thứ ba (③; chi tiết thêm được cung cấp trong Hình 9 trong phụ lục), Da của loài động vật nào được thích nghi để sinh tồn ở những nơi lạnh?, liên quan đến thuật ngữ khoa học liên quan đến sự sinh tồn của động vật. Bộ lập kế hoạch quyết định gọi công cụ tìm kiếm Bing để truy cập kiến thức đặc thù lĩnh vực, có lợi từ nhiều tài nguyên trực tuyến.

Ví dụ trực quan hóa của TabMWP. Khả năng thích nghi và tính linh hoạt của Chameleon cho nhiều truy vấn khác nhau cũng được quan sát thấy trên TabMWP, như được minh họa trong các ví dụ trong Hình 2. Ví dụ đầu tiên (①) liên quan đến lý luận toán học trên một biểu mẫu thuế. Chameleon (1) gọi mô hình truy xuất kiến thức để nhớ lại kiến thức cơ bản hỗ trợ hiểu bảng đặc thù lĩnh vực này, (2) mô tả bảng theo định dạng ngôn ngữ tự nhiên dễ đọc hơn, và (3) cuối cùng dựa vào các công cụ hỗ trợ chương trình để thực hiện các phép tính chính xác. Trong ví dụ thứ hai (②), hệ thống tạo ra mã Python phù hợp chặt chẽ với kiến thức nền được cung cấp bởi mô hình truy xuất kiến thức. Ví dụ thứ ba (③) yêu cầu hệ thống định vị ô trong một ngữ cảnh bảng lớn cho truy vấn đầu vào. Chameleon gọi mô hình tra cứu hàng để giúp định vị chính xác các hàng liên quan và tạo ra giải pháp ngôn ngữ thông qua một mô hình LLM, thay vì dựa vào các công cụ dựa trên chương trình.

Các trường hợp thất bại và hạn chế. Các ví dụ thất bại từ Chameleon (GPT-4) được minh họa trong Bảng 19 đến 24 trong phụ lục. Các phản hồi không chính xác có thể phát sinh từ những hạn chế của các module hiện tại hoặc từ các chương trình không tối ưu được tạo ra bởi bộ lập kế hoạch. Ngoài ra, kho module có thể thiếu các công cụ có khả năng giải quyết các khả năng cụ thể. Các hướng tương lai có thể liên quan đến việc nâng cấp các module và bộ lập kế hoạch, hoặc mở rộng kho module để hỗ trợ một loạt rộng hơn các khả năng. Các hạn chế và tác động rộng hơn được thảo luận tương ứng trong phần B và C của phụ lục.

5.4 Phân tích lỗi

Để kiểm tra các nguồn lỗi của các mô hình ngôn ngữ lớn cơ bản và hiểu cách mô hình của chúng tôi giảm thiểu các sai lầm từ các khía cạnh khác nhau, chúng tôi tiến hành một phân tích lỗi, như được thể hiện trong Hình 6. Chúng tôi chọn 50 ví dụ sai lầm từ baseline ChatGPT trên ScienceQA làm tập đánh giá. Chúng tôi đếm số ví dụ sai lầm và phân tích các loại sai lầm tương ứng cho ChatGPT, phương pháp Chameleon (ChatGPT) của chúng tôi, và Chameleon (GPT-4).

[Biểu đồ 6 cho thấy số lượng ví dụ sai lầm trong các danh mục khác nhau]

Hình 6: Số ví dụ sai lầm trong các danh mục khác nhau trên ScienceQA. Hình ảnh: mô tả hình ảnh, Kiến thức: hiểu kiến thức, Giải pháp: tạo ra giải pháp.

Kết quả cho thấy phương pháp Chameleon của chúng tôi có thể giảm đáng kể số lượng sai lầm so với ChatGPT. Mô hình của chúng tôi có các công cụ cho mô tả hình ảnh và truy xuất kiến thức, do đó các sai lầm được ChatGPT tạo ra trong danh mục hiểu hình ảnh được giảm xuống 10 và 19 từ 32 bởi Chameleon (ChatGPT) và Chameleon (GPT-4); trong khi các sai lầm được ChatGPT tạo ra trong danh mục hiểu kiến thức được giảm xuống 6 và 3 từ 37 bởi Chameleon (ChatGPT) và Chameleon (GPT-4). Có lợi từ việc thực thi tuần tự các công cụ, các sai lầm gây ra bởi việc tạo ra giải pháp cũng được giảm đáng kể. Ngoài ra, chúng tôi thấy rằng việc lập kế hoạch nhiệm vụ của GPT-4 vượt trội so với ChatGPT với một khoảng cách lớn.

6 Kết luận

Tóm lại, chúng tôi giới thiệu một framework lý luận tổng hợp linh hoạt mới, Chameleon, giải quyết những hạn chế của các mô hình ngôn ngữ lớn hiện tại bằng cách tăng cường chúng với các công cụ bên ngoài theo cách linh hoạt. Phương pháp của chúng tôi sử dụng một tập hợp công cụ đa dạng và thể hiện khả năng thích nghi và hiệu quả ấn tượng trên hai benchmark thách thức, ScienceQA và TabMWP. Bằng cách đạt được các cải thiện đáng kể về độ chính xác so với các mô hình trạng thái nghệ thuật hiện có, Chameleon thể hiện tiềm năng của nó trong việc giải quyết các truy vấn thực tế trên nhiều lĩnh vực khác nhau.

--- TRANG 11 ---
Lời cảm ơn

Chúng tôi muốn cảm ơn Chunyuan Li, Qiuyuan Huang, và các thành viên khác của nhóm Deep Learning tại Microsoft Research cho các cuộc thảo luận có giá trị của họ. Chúng tôi cũng cảm ơn Fan Yin từ Đại học California, Los Angeles, và Mingyang Sun từ Đại học Khoa học và Công nghệ Điện tử Trung Quốc cho việc xem xét kỹ lưỡng bài báo của chúng tôi và phản hồi xây dựng. Nghiên cứu của Pan Lu cho công việc này được hỗ trợ tài chính bởi Microsoft trong thời gian anh ấy thăm Microsoft Research, và cũng được hỗ trợ một phần bởi Amazon PhD Fellowship, Bloomberg PhD Fellowship, Qualcomm Innovation Fellowship, và UCLA Dissertation Year Fellowship. Kai-Wei được hỗ trợ bởi một khoản tài trợ ONR N00014-23-1-2780 và với tư cách là một Sloan Fellow.

Tài liệu tham khảo

[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, và Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[2] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, và Dan Klein. Learning to compose neural networks for question answering. Trong Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 1545–1554, 2016.

[3] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, và Dan Klein. Neural module networks. Trong Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), trang 39–48, 2016.

[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, và cộng sự. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[6] Wenhu Chen, Xueguang Ma, Xinyi Wang, và William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023.

[7] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, và Xinyi Wang. TheoremQA: A theorem-driven question answering dataset. Trong The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023.

[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, và cộng sự. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, và cộng sự. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

[10] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, và Graham Neubig. Pal: Program-aided language models. Trong International Conference on Machine Learning, trang 10764–10799. PMLR, 2023.

[11] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, và Yu Qiao. LLaMA-Adapter V2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.

[12] Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven CH Hoi, Xiaogang Wang, và Hongsheng Li. Dynamic fusion with intra-and inter-modality attention flow for visual question answering. Trong The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 6639–6648, 2019.

[13] Tanmay Gupta và Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 14953–14962, 2023.

[14] Ronghang Hu, Jacob Andreas, Trevor Darrell, và Kate Saenko. Explainable neural computation via stack neural module networks. Trong Proceedings of the European conference on computer vision (ECCV), trang 53–69, 2018.

--- TRANG 12 ---
[15] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, và Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. Trong Proceedings of the IEEE international conference on computer vision, trang 804–813, 2017.

[16] Wenlong Huang, Pieter Abbeel, Deepak Pathak, và Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Trong International Conference on Machine Learning, trang 9118–9147. PMLR, 2022.

[17] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, và cộng sự. Inner monologue: Embodied reasoning through planning with language models. Trong 6th Annual Conference on Robot Learning, 2022.

[18] Shima Imani, Liang Du, và Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. Trong ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models, 2023.

[19] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, và Ross Girshick. Inferring and executing programs for visual reasoning. Trong Proceedings of the IEEE international conference on computer vision (ICCV), trang 2989–2998, 2017.

[20] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, và Hannaneh Hajishirzi. UnifiedQA: Crossing format boundaries with a single QA system. Trong Findings of the Association for Computational Linguistics (EMNLP), trang 1896–1907, 2020.

[21] Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, và Ashish Sabharwal. Text modular networks: Learning to decompose tasks in the language of existing models. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 1264–1279, 2021.

[22] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, và Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. Trong International Conference on Learning Representations (ICLR), 2023.

[23] Jin-Hwa Kim, Jaehyun Jun, và Byoung-Tak Zhang. Bilinear attention networks. Trong Advances in Neural Information Processing Systems (NeurIPS), trang 1571–1581, 2018.

[24] Wonjae Kim, Bokyung Son, và Ildoo Kim. ViLT: Vision-and-language transformer without convolution or region supervision. Trong Proceedings of the 38th International Conference on Machine Learning (ICML), trang 5583–5594, 2021.

[25] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.

[26] Mojtaba Komeili, Kurt Shuster, và Jason Weston. Internet-augmented dialogue generation. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 8460–8478, 2022.

[27] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, và Kai-Wei Chang. VisualBERT: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.

[28] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, và Kai-Wei Chang. What does BERT with vision look at? Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), trang 5265–5275, 2020.

[29] Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, và Jian-Guang Lou. TAPEX: Table pre-training via learning a neural sql executor. Trong International Conference on Learning Representations (ICLR), 2022.

[30] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, và Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. arXiv preprint arXiv:2310.02255, 2023.

[31] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, và Song-Chun Zhu. InterGPS: Interpretable geometry problem solving with formal language and symbolic reasoning. Trong The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.

--- TRANG 13 ---
[32] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, và Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Trong The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.

[33] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, và Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Trong International Conference on Learning Representations (ICLR), 2023.

[34] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, và Song-Chun Zhu. IconQA: A new benchmark for abstract diagram understanding and visual language reasoning. Trong The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021.

[35] Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, và Kai-Wei Chang. A survey of deep learning for mathematical reasoning. Trong The 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023.

[36] Yujie Lu, Pan Lu, Zhiyu Chen, Wanrong Zhu, Xin Eric Wang, và William Yang Wang. Multimodal procedural planning via dual text-image prompting. arXiv preprint arXiv:2305.01795, 2023.

[37] Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, và Jianfeng Gao. Open domain question answering with a unified knowledge interface. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 1605–1620, Dublin, Ireland, Tháng 5 2022. Association for Computational Linguistics.

[38] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, và cộng sự. Self-Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.

[39] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, và Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. Trong The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.

[40] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, và cộng sự. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

[41] OpenAI. Chatgpt, 2022.

[42] OpenAI. GPT-4 technical report. ArXiv, abs/2303.08774, 2023.

[43] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, và Marco Tulio Ribeiro. ART: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023.

[44] Arkil Patel, Satwik Bhattamishra, và Navin Goyal. Are NLP models really able to solve simple math word problems? Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 2080–2094, 2021.

[45] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, và Jianfeng Gao. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.

[46] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277, 2023.

[47] Toran Bruce Richards. Auto-GPT: An experimental open-source attempt to make GPT-4 fully autonomous. https://github.com/Significant-Gravitas/Auto-GPT, 2023.

[48] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, và cộng sự. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

[49] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, và Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.

--- TRANG 14 ---
[50] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, và Yueting Zhuang. HuggingGPT: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.

[51] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, và Kai Yu. SciEval: A multi-level large language model evaluation benchmark for scientific research. arXiv preprint arXiv:2308.13149, 2023.

[52] Dídac Surís, Sachit Menon, và Carl Vondrick. ViperGPT: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.

[53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, và cộng sự. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[54] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, và Wei Wang. SciBench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023.

[55] Xingyao Wang, Sha Li, và Heng Ji. Code4Struct: Code generation for few-shot structured prediction from natural language. arXiv preprint arXiv:2210.12810, 2022.

[56] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, và cộng sự. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022.

[57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, và cộng sự. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

[58] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Reinforcement learning, trang 5–32, 1992.

[59] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, và Nan Duan. Visual ChatGPT: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.

[60] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, và Lijuan Wang. MM-REACT: Prompting ChatGPT for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.

[61] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, và Josh Tenenbaum. Neural-symbolic VQA: Disentangling reasoning from vision and language understanding. Advances in neural information processing systems (NeurIPS), 31, 2018.

[62] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, và Meng Jiang. Generate rather than retrieve: Large language models are strong context generators. Trong International Conference on Learning Representations (ICLR), 2023.

[63] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, và Qi Tian. Deep modular co-attention networks for visual question answering. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 6281–6290, 2019.

[64] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, và Qiao Yu. LLaMA-Adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.

[65] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, và Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023.

--- TRANG 15 ---
Tài liệu bổ sung cho
Chameleon: Lý luận tổng hợp linh hoạt 
với các mô hình ngôn ngữ lớn

A Phụ lục

A.1 Các LLMs được tăng cường công cụ hiện tại

Để giải quyết những hạn chế của LLMs, một hướng nghiên cứu tích cực liên quan đến việc tăng cường các mô hình ngôn ngữ với quyền truy cập vào các công cụ và tài nguyên bên ngoài, cũng như khám phá việc tích hợp các công cụ bên ngoài và các phương pháp modular linh hoạt. Ví dụ, được hỗ trợ bởi các công cụ tìm kiếm web và các tài nguyên kiến thức bên ngoài, LLMs có thể truy cập thông tin thời gian thực và tận dụng kiến thức đặc thù lĩnh vực [40]. Để tăng cường khả năng lý luận toán học, công việc gần đây sử dụng LLMs [5] để tạo ra các chương trình phức tạp nhằm khai thác các tài nguyên tính toán mạnh mẽ, và thực thi các nhiệm vụ lý luận logic hiệu quả hơn [55,10,6,39,18,43]. Một dòng công việc gần đây khác, như ViperGPT [52], Visual ChatGPT [59], VisProg [13], và HuggingGPT [50] kết hợp một tập hợp các mô hình thị giác nền tảng để trang bị cho LLMs khả năng thực hiện các nhiệm vụ lý luận thị giác.

A.2 Chi tiết thí nghiệm

Không gian tìm kiếm module. Các tập con kho cho ScienceQA và TabMWP được thể hiện trong Bảng 6.

Loại công cụ | Công cụ được sử dụng trên ScienceQA | Công cụ được sử dụng trên TabMWP
---|---|---
OpenAI | Truy xuất kiến thức, Bộ tạo truy vấn, Bộ tạo giải pháp | Truy xuất kiến thức, Tra cứu hàng, Tra cứu cột, Bộ diễn giải bảng, Bộ tạo chương trình, Bộ tạo giải pháp
Hugging Face | Bộ mô tả hình ảnh |
Github | Bộ phát hiện văn bản |
Tìm kiếm Web | Tìm kiếm Bing |
Python | | Bộ xác minh chương trình, Bộ thực thi chương trình
Dựa trên quy tắc | Bộ tạo câu trả lời | Bộ tạo câu trả lời

Bảng 6: Các công cụ được sử dụng trên ScienceQA và TabMWP, tương ứng. Các công cụ có thể tái sử dụng được đánh dấu màu xanh lá cây.

Triển khai bộ lập kế hoạch. Chúng tôi chọn engine gpt-3.5-turbo cho ChatGPT và engine gpt-4 cho GPT-4 khi xây dựng bộ lập kế hoạch dựa trên LLM. Độ dài tối đa cho các chương trình được tạo ra được đặt là 128, và nhiệt độ được đặt là 0 cho việc tạo ra mang tính xác định nhất. Các prompt bộ lập kế hoạch cho ScienceQA và TabMWP được minh họa trong Bảng 8 và Bảng 9, tương ứng.

Triển khai module cho ScienceQA. Theo mặc định, các mô hình dựa trên LLM sử dụng bốn ví dụ trong ngữ cảnh làm demonstration, có cài đặt nhiệt độ là 0, và cho phép tối đa 512 token để hoàn thành. Chi tiết triển khai cụ thể bổ sung được cung cấp như sau:

• Truy xuất kiến thức: Prompt bao gồm 3 ví dụ demonstration và template được thể hiện trong Bảng 10.

• Bộ tạo truy vấn: Template prompt được thể hiện trong Bảng 11. Số lượng token tối đa để hoàn thành được đặt là 64.

• Bộ tạo giải pháp: Prompt bao gồm 2 ví dụ demonstration và template được thể hiện trong Bảng 12.

--- TRANG 16 ---
• Bộ mô tả hình ảnh: Chúng tôi sử dụng mô hình mô tả¹ để tạo ra các mô tả văn bản cho hình ảnh đầu vào. Độ dài tối đa của các chú thích được tạo ra được đặt là 16, số lượng beam là 4, và số lượng token đầu ra tối đa là 512.

• Bộ phát hiện văn bản: Module này dựa trên mô hình github² để trích xuất nội dung văn bản với tọa độ trong hình ảnh.

• Tìm kiếm Bing: Module này gọi API Tìm kiếm Bing³ và trả về ba phản hồi hàng đầu cho truy vấn văn bản.

• Bộ tạo câu trả lời: Module này trích xuất đoạn câu trả lời từ kết quả được cung cấp bởi "Bộ tạo giải pháp" và chọn tùy chọn tương tự nhất từ các lựa chọn cho trước.

Triển khai module cho TabMWP. Tương tự như ScienceQA, các module dựa trên LLM theo mặc định sử dụng bốn ví dụ trong ngữ cảnh làm demonstration, có cài đặt nhiệt độ là 0, và cho phép tối đa 512 token để hoàn thành. Chi tiết triển khai bổ sung được cung cấp như sau:

• Truy xuất kiến thức: Prompt bao gồm 5 ví dụ demonstration và template được thể hiện trong Bảng 13.

• Tra cứu hàng: Nó chỉ được kích hoạt khi có nhiều hơn ba hàng và 18 ô bảng, để tăng tốc suy luận. Prompt bao gồm 7 ví dụ demonstration và template được thể hiện trong Bảng 14. Số lượng token tối đa để hoàn thành được đặt là 256.

• Tra cứu cột: Tương tự, module này được kích hoạt với hai cột trở lên và 18 ô trở lên. Prompt bao gồm 6 ví dụ demonstration và template được thể hiện trong Bảng 15. Số lượng token tối đa để hoàn thành được đặt là 256.

• Bộ diễn giải bảng: Prompt bao gồm 7 ví dụ demonstration và template được thể hiện trong Bảng 16.

• Bộ tạo chương trình: Template prompt được thể hiện trong Bảng 17. Số lượng token tối đa để hoàn thành được đặt là 256.

• Bộ tạo giải pháp: Prompt bao gồm 16 ví dụ demonstration và template được thể hiện trong Bảng 18.

• Bộ tạo câu trả lời: Nó được sử dụng để chuẩn hóa câu trả lời với độ chính xác hai chữ số thập phân cho các câu hỏi có câu trả lời số và chọn tùy chọn tương tự nhất cho các câu hỏi nhiều lựa chọn.

Triển khai update_input và update_cache. update_input được kích hoạt bởi việc thực thi các công cụ cụ thể, như 'Row_Lookup', thay đổi hoặc thay thế các phần tử trong đầu vào để phản ánh trạng thái được cập nhật. Các công cụ như 'Image_Captioner', 'Text_Detector', 'Knowledge_Retrieval', 'Web_Search', và 'Program_Generation' tạo ra các phần tử mới. update_cache lưu trữ những phần tử mới này trong bộ nhớ đệm, làm cho chúng có thể truy cập được cho việc thực thi các công cụ sau này.

A.3 Kết quả thí nghiệm

Thống kê chương trình được tạo ra. Chameleon sử dụng bộ lập kế hoạch ngôn ngữ tự nhiên dựa trên LLM để tạo ra các chương trình, tức là các chuỗi của các module (công cụ) được sử dụng. Chúng tôi báo cáo thống kê về số lượng chương trình được tạo ra duy nhất và độ dài trung bình của các chuỗi công cụ tương ứng bởi Chameleon trong Bảng 7. Trên cả ScienceQA và TabMWP, việc sử dụng GPT-4 làm LLM cơ bản tạo ra ít chương trình riêng biệt hơn, tức là các chương trình nhất quán hơn, so với việc sử dụng ChatGPT, ngay cả khi được đưa ra cùng một prompt trong mô hình lập kế hoạch. Kết quả của chúng tôi phù hợp với các phát hiện trong [42], quan sát thấy GPT-4 có khả năng hiểu ngữ cảnh dài, phù hợp với hướng dẫn của con người, và thực hiện lý luận cấp cao vượt trội so với các LLMs khác như ChatGPT.

B Hạn chế

Trong khi Chameleon đại diện cho một bước tiến đáng kể trong việc khai thác các mô hình ngôn ngữ lớn (LLMs) cho lý luận tổng hợp theo cách linh hoạt, có một vài lĩnh vực có thể được hưởng lợi từ

¹https://huggingface.co/nlpconnect/vit-gpt2-image-captioning
²https://github.com/JaidedAI/EasyOCR
³https://www.microsoft.com/bing

--- TRANG 17 ---
Nhiệm vụ | Mô hình | Số chương trình khác nhau | Độ dài chương trình trung bình
---|---|---|---
ScienceQA | Chuỗi tư duy (CoT) | 1 | 2
| Chameleon (ChatGPT) | 14 | 3.03
| Chameleon (GPT-4) | 11 | 3.40
TabMWP | Chuỗi tư duy (CoT) | 1 | 2
| Chương trình tư duy (PoT) | 1 | 3
| Chameleon (ChatGPT) | 28 | 4.17
| Chameleon (GPT-4) | 19 | 4.09

Bảng 7: Thống kê về số lượng chương trình được tạo ra khác nhau và độ dài trung bình của các chương trình được tạo ra bởi Chameleon, tương ứng. Gợi ý chuỗi tư duy (CoT) và gợi ý chương trình tư duy (PoT) cũng được so sánh vì chúng là các trường hợp đặc biệt của Chameleon.

việc tinh chỉnh thêm. Một lĩnh vực như vậy là việc mở rộng khả năng thích nghi của nó đến một loạt rộng hơn các nhiệm vụ và lĩnh vực, ngoài các benchmark được trình bày. Bộ lập kế hoạch dựa trên LLM, chịu trách nhiệm tổng hợp các chương trình và xác định chuỗi các công cụ, giới thiệu một phương pháp đổi mới, nhưng nó cũng đặt ra những câu hỏi nghiên cứu hấp dẫn về việc tối ưu hóa quy trình cho việc lựa chọn công cụ và chuỗi. Có thể trong thiết kế hệ thống hiện tại, chất lượng của bộ lập kế hoạch dựa trên LLM có thể ảnh hưởng đến hiệu suất tổng thể. Hơn nữa, Chameleon tạo ra chương trình tại một bước, mà không kết hợp cơ chế lập kế hoạch lại khi các module trong chương trình được xử lý. Ngoài ra, chúng tôi đưa ra giả định rằng danh sách các module và mô tả của chúng sẽ phù hợp trong cửa sổ ngữ cảnh của LLMs, điều này có thể không phải lúc nào cũng đúng. Khi độ phức tạp của nhiệm vụ tăng lên và kho module mở rộng, có thể có sự gia tăng tương ứng trong nhu cầu tính toán hoặc hạn chế do giới hạn ngữ cảnh, cho thấy các lĩnh vực tiềm năng cho việc tối ưu hóa trong tương lai. Tuy nhiên, những lĩnh vực tiềm năng để cải thiện này không làm giảm đi những thành tựu trung tâm của bài báo, mà thay vào đó cung cấp các hướng có giá trị cho công việc và nghiên cứu trong tương lai.

C Tác động rộng hơn

Công việc được trình bày trong bài báo này, Chameleon, có tiềm năng đáng kể cho tác động tích cực đến xã hội. Bằng cách tăng cường các mô hình ngôn ngữ lớn (LLMs) với các module linh hoạt cho lý luận tổng hợp, Chameleon có thể cung cấp các phản hồi chính xác hơn cho các nhiệm vụ phức tạp, đa phương thức, làm cho nó trở thành một framework có tiềm năng có giá trị cho nhiều ứng dụng khác nhau, bao gồm nhưng không giới hạn ở giáo dục, tài chính, và các hệ thống hỗ trợ quyết định. Ngoài ra, khả năng của hệ thống trong việc tổng hợp các chương trình mà không yêu cầu bất kỳ việc huấn luyện nào có thể dân chủ hóa quyền truy cập vào công nghệ AI, cho phép những người không chuyên tận dụng sức mạnh của AI trong các lĩnh vực đa dạng. Khi nghiên cứu tiếp tục tiến bộ trong các mô hình ngôn ngữ lớn và tích hợp công cụ, chúng tôi dự đoán rằng framework của chúng tôi sẽ phục vụ như một nền tảng cho những đổi mới thêm trong việc theo đuổi các giải pháp tổng quát hóa và hiệu quả hơn cho các nhiệm vụ lý luận phức tạp.

Trong khi có thể có những tác động tiêu cực đến xã hội liên quan đến Chameleon, như thông tin sai lệch và lo ngại về quyền riêng tư nếu các nguồn dữ liệu và công cụ bên ngoài mà nó sử dụng không được tuyển chọn cẩn thận, chúng tôi tin rằng những rủi ro này có thể được quản lý và giảm thiểu cẩn thận. Cũng có rủi ro rằng việc phụ thuộc quá mức vào tính tự trị gia tăng của Chameleon có thể làm suy yếu các kỹ năng tư duy phản biện hoặc các chức năng công việc. Để giảm thiểu hiệu quả những vấn đề này, việc tuyển chọn cẩn thận các nguồn dữ liệu và công cụ bên ngoài, cùng với cam kết mạnh mẽ về bảo vệ dữ liệu người dùng, là cần thiết. Ngoài ra, tính tự trị của Chameleon nên được xem như một phương tiện để tăng cường, chứ không phải thay thế, khả năng của con người. Do đó, việc phát triển các hướng dẫn đạo đức mạnh mẽ, các cơ chế minh bạch, và các biện pháp bảo vệ là quan trọng, làm nền tảng cho cam kết của chúng tôi đối với việc triển khai AI có trách nhiệm xã hội.

⁴https://www.usgs.gov/geology-and-ecology-of-national-parks/ecology-death-valley-national-park-0

--- TRANG 18 ---
▷ Hướng dẫn cho mô hình lập kế hoạch
Bạn cần hành động như một mô hình chính sách, khi được đưa ra một câu hỏi và một tập hợp modular, xác định chuỗi các module có thể được thực thi tuần tự có thể giải quyết câu hỏi.

Các module được định nghĩa như sau:
Query_Generator: Module này tạo ra một truy vấn công cụ tìm kiếm cho câu hỏi đã cho. Thông thường, chúng ta cân nhắc sử dụng "Query_Generator" khi câu hỏi liên quan đến kiến thức đặc thù lĩnh vực.

Bing_Search: Module này tìm kiếm trên web thông tin liên quan đến câu hỏi. Thông thường, chúng ta cân nhắc sử dụng "Bing_Search" khi câu hỏi liên quan đến kiến thức đặc thù lĩnh vực.

Image_Captioner: Module này tạo ra chú thích cho hình ảnh đã cho. Thông thường, chúng ta cân nhắc sử dụng "Image_Captioner" khi câu hỏi liên quan đến việc hiểu ngữ nghĩa của hình ảnh, và trường "has_image" trong metadata là True.

Text_Detector: Module này phát hiện văn bản trong hình ảnh đã cho. Thông thường, chúng ta cân nhắc sử dụng "Text_Detector" khi câu hỏi liên quan đến việc mở ra văn bản trong hình ảnh, ví dụ: biểu đồ, đồ thị, bảng, bản đồ, v.v., và trường "has_image" trong metadata là True.

Knowledge_Retrieval: Module này truy xuất kiến thức nền như gợi ý cho câu hỏi đã cho. Thông thường, chúng ta cân nhắc sử dụng "Knowledge_Retrieval" khi kiến thức nền hữu ích để hướng dẫn giải pháp.

Solution_Generator: Module này tạo ra một giải pháp chi tiết cho câu hỏi dựa trên thông tin được cung cấp. Thông thường, "Solution_Generator" sẽ kết hợp thông tin từ "Query_Generator", "Bing_Search", "Image_Captioner", "Text_Detector", và "Knowledge_Retrieval".

Answer_Generator: Module này trích xuất câu trả lời cuối cùng dưới dạng ngắn gọn từ giải pháp hoặc kết quả thực thi. Module này thường là module cuối cùng trong pipeline dự đoán.

Dưới đây là một số ví dụ ánh xạ vấn đề tới các module.

▷ Ví dụ trong ngữ cảnh
Câu hỏi: So sánh năng lượng động học trung bình của các hạt trong mỗi mẫu. Mẫu nào có nhiệt độ cao hơn?
Ngữ cảnh: Các biểu đồ dưới đây cho thấy hai mẫu khí tinh khiết trong các container đóng kín, cứng nhắc giống hệt nhau. Mỗi quả bóng màu đại diện cho một hạt khí. Cả hai mẫu đều có cùng số lượng hạt.
Tùy chọn: (A) không có; các mẫu có cùng nhiệt độ (B) mẫu A (C) mẫu B
Metadata: 'pid': 19, 'has_image': True, 'grade': 8, 'subject': 'natural science', 'topic': 'physics', 'category': 'Particle motion and energy', 'skill': 'Identify how particle motion affects temperature and pressure'
Modules: ["Text_Detector","Knowledge_Retrieval","Solution_Generator","Answer_Generator"]

Bảng 8: Prompt được xây dựng cho mô hình lập kế hoạch trên nhiệm vụ ScienceQA. Prompt bao gồm hướng dẫn mô tả vai trò của mô hình lập kế hoạch, các ví dụ trong ngữ cảnh ánh xạ vấn đề tới chuỗi module, và ví dụ kiểm thử.

--- TRANG 19 ---
▷ Hướng dẫn cho mô hình lập kế hoạch
Bạn cần hành động như một mô hình chính sách, khi được đưa ra một câu hỏi và một tập hợp modular, xác định chuỗi các module có thể được thực thi tuần tự có thể giải quyết câu hỏi.

Các module được định nghĩa như sau:
Program_Generator: Module này tạo ra một chương trình Python có thể giải quyết câu hỏi đã cho. Nó nhận câu hỏi và ngữ cảnh có thể và tạo ra một chương trình có thể được thực thi bởi module "Program_Executor". Thông thường, chúng ta cân nhắc sử dụng "Program_Generator" khi các câu hỏi và ngữ cảnh liên quan đến tính toán phức tạp, như các phép toán số học trên nhiều số, hoặc khi các câu hỏi liên quan đến các phép toán logic phức tạp, như câu lệnh "if-else".

Program_Verifier: Module này xác minh liệu chương trình được tạo ra từ "Program_Generator" có hợp lệ và không có lỗi hay không. Nó kiểm tra lỗi cú pháp, lỗi logic, và các vấn đề tiềm năng khác có thể phát sinh trong quá trình thực thi chương trình.

Program_Executor: Module này thực thi chương trình được tạo ra từ "Program_Generator" và tạo ra một đầu ra có thể được xử lý thêm bởi các module khác, như "Question_Answering".

Row_Lookup: Module này trả về bảng được đơn giản hóa chỉ còn lại các hàng liên quan đến câu hỏi. Nó nhận câu hỏi và một bảng và trả về bảng được đơn giản hóa. Nếu tất cả các hàng đều liên quan hoặc chỉ có ba hàng hoặc ít hơn, trả về bảng gốc. Thông thường, chúng ta chỉ cân nhắc sử dụng "Row_Lookup" khi bảng có hơn ba hàng và câu hỏi chỉ yêu cầu một số lượng nhỏ các hàng để trả lời câu hỏi.

Column_Lookup: Module này trả về bảng được đơn giản hóa chỉ còn lại các cột liên quan đến câu hỏi. Nó nhận câu hỏi và một bảng và trả về bảng được đơn giản hóa. Nếu tất cả các cột đều liên quan hoặc chỉ có hai cột, trả về bảng gốc. Thông thường, chúng ta cân nhắc sử dụng "Column_Lookup" khi bảng có hơn hai cột và câu hỏi chỉ yêu cầu một số lượng nhỏ các cột để trả lời câu hỏi.

Table_Verbalizer: Module này chuyển đổi bảng thành một mô tả có thể được hiểu dễ dàng bởi các module downstream, như "Program_Generator", "Solution_Generator", "Question_Answering". Thông thường, chúng ta cân nhắc sử dụng "Table_Verbalizer" khi bảng có một số lượng nhỏ các hàng và cột và bảng đặc thù lĩnh vực, như biểu đồ thân-lá, bảng hàm, v.v.

Knowledge_Retrieval: Module này truy xuất kiến thức đặc thù lĩnh vực cho câu hỏi và bảng đã cho. Thông thường, chúng ta cân nhắc sử dụng "Knowledge_Retrieval" khi câu hỏi và bảng liên quan đến kiến thức đặc thù lĩnh vực, như "biểu đồ thân-lá", "bảng hàm", "biểu mẫu thuế", v.v.

Solution_Generator: Module này tạo ra một giải pháp chi tiết cho câu hỏi dựa trên thông tin được cung cấp. Thông thường, chúng ta sử dụng "Solution_Generator" khi câu hỏi và bảng liên quan đến tính toán đơn giản, như các phép toán số học trên một số duy nhất.

Answer_Generator: Module này trích xuất câu trả lời cuối cùng dưới dạng ngắn gọn từ giải pháp hoặc kết quả thực thi. Module này thường theo sau module "Solution_Generator" hoặc "Problem_Executor".

Dưới đây là một số ví dụ ánh xạ vấn đề tới các module.

▷ Ví dụ trong ngữ cảnh
Bảng:
đồng hồ thiết kế | $8,141
áo khoác thiết kế | $6,391
Câu hỏi: Một chiếc đồng hồ thiết kế đắt hơn một chiếc áo khoác thiết kế bao nhiêu? (đơn vị: $)
Modules: ["Program_Generator","Program_Verifier","Program_Executor","Answer_Generator"]

Bảng 9: Prompt được xây dựng cho mô hình lập kế hoạch trên nhiệm vụ TabMWP. Tương tự, prompt bao gồm hướng dẫn, các ví dụ trong ngữ cảnh, và ví dụ kiểm thử.

--- TRANG 20 ---
▷ Hướng dẫn
Đọc câu hỏi sau đây, và tạo ra kiến thức nền như thông tin ngữ cảnh có thể hữu ích để trả lời câu hỏi.

▷ Ví dụ trong ngữ cảnh
Câu hỏi: Ba vật thể này có tính chất chung nào?
Tùy chọn: (A) cứng (B) mềm (C) màu vàng
Metadata: 'pid': 43, 'has_image': True, 'grade': 4, 'subject': 'natural science', 'topic': 'physics', 'category': 'Materials', 'skill': 'Compare properties of objects'
Văn bản được phát hiện trong hình ảnh: ['handkerchief', 'slippers', 'leisure suit']
Kiến thức:
- Câu hỏi này là về việc so sánh tính chất của ba vật thể: một chiếc khăn tay, dép và một bộ đồ thường.
- Các vật thể liên quan đến chủ đề vật lý và kỹ năng so sánh tính chất của các vật thể.
- Tính chất của các vật thể có thể bao gồm các đặc điểm vật lý như màu sắc, kết cấu, hình dạng, kích thước, trọng lượng và vật liệu.

Bảng 10: Prompt được xây dựng cho module "Knowledge Retrieval" trên nhiệm vụ ScienceQA.

▷ Hướng dẫn
Đọc câu hỏi và metadata sau đây, và tạo ra truy vấn để tìm kiếm trình duyệt như thông tin ngữ cảnh có thể hữu ích để trả lời câu hỏi.

▷ Ví dụ trong ngữ cảnh
Câu hỏi: Hai vật thể này có tính chất chung nào?
Tùy chọn: (A) cứng (B) có thể uốn cong
Metadata: 'pid': 329, 'has_image': True, 'grade': 2, 'subject': 'natural science', 'topic': 'physics', 'category': 'Materials', 'skill': 'Compare properties of objects'
Văn bản được phát hiện trong hình ảnh: [([[41, 183], [131, 183], [131, 199], [41, 199]], 'rubber gloves'), ([[245, 183], [313, 183], [313, 197], [245, 197]], 'rain boots')]
Truy vấn tìm kiếm: Tính chất vật liệu chung của dây nhảy và găng tay cao su

Bảng 11: Prompt được xây dựng cho module "Query Generator" trên nhiệm vụ ScienceQA.

--- TRANG 21 ---
▷ Hướng dẫn
Cho câu hỏi (và ngữ cảnh), chọn câu trả lời từ các tùy chọn ["A", "B", "C", "D", "E"]. Bạn nên đưa ra các giải pháp ngắn gọn và từng bước. Cuối cùng, kết luận câu trả lời theo định dạng "the answer is [ANSWER]", trong đó [ANSWER] là một trong các tùy chọn ["A", "B", "C", "D", "E"]. Ví dụ, "the answer is A", "the answer is B", "the answer is C", "the answer is D", hoặc "the answer is E". Nếu câu trả lời không có trong các tùy chọn, chọn tùy chọn có khả năng nhất.

▷ Ví dụ trong ngữ cảnh
Câu hỏi: Hai vật thể này có tính chất chung nào?
Ngữ cảnh: Chọn câu trả lời tốt hơn.
Tùy chọn: (A) cứng (B) có thể uốn cong
Metadata: 'pid': 6493, 'has_image': True, 'grade': 2, 'subject': 'natural science', 'topic': 'physics', 'category': 'Materials', 'skill': 'Compare properties of objects'
Chú thích hình ảnh: Một chiếc kéo bên cạnh một chiếc kéo khác.
Văn bản được phát hiện với tọa độ trong hình ảnh: [([[53, 185], [121, 185], [121, 199], [53, 199]], 'jump rope'), ([[233, 183], [323, 183], [323, 201], [233, 201]], 'rubber gloves')]
Kiến thức được truy xuất:
- Câu hỏi này là về việc so sánh tính chất của hai vật thể: găng tay cao su và ủng mưa.
- Các vật thể liên quan đến chủ đề vật lý và kỹ năng so sánh tính chất của các vật thể.
- Tính chất của các vật thể có thể bao gồm các đặc điểm vật lý như màu sắc, kết cấu, hình dạng, kích thước, trọng lượng và vật liệu. Trong trường hợp này, hai vật thể có tính chất có thể uốn cong chung.
Phản hồi tìm kiếm Bing: Các vật liệu phổ biến nhất được sử dụng cho găng tay dùng một lần là Latex, Vinyl và Nitrile. Mỗi vật liệu có lợi ích và nhược điểm riêng. Găng tay Latex được làm từ Latex cao su tự nhiên và là loại găng tay dùng một lần phổ biến nhất.
Giải pháp: Một vật thể có các tính chất khác nhau. Một tính chất của vật thể có thể cho bạn biết nó trông như thế nào, cảm giác như thế nào, vị như thế nào, hoặc mùi như thế nào. Các vật thể khác nhau có thể có cùng tính chất. Bạn có thể sử dụng những tính chất này để nhóm các vật thể lại. Nhìn vào mỗi vật thể. Đối với mỗi vật thể, quyết định xem nó có tính chất đó hay không. Một vật thể có thể uốn cong có thể được uốn mà không bị vỡ. Cả hai vật thể đều có thể uốn cong. Một vật thể cứng giữ nguyên hình dạng khi bạn bóp nó. Găng tay cao su không cứng. Tính chất mà cả hai vật thể có chung là có thể uốn cong. Do đó, câu trả lời là B.

Bảng 12: Prompt được xây dựng cho module "Solution Generator" trên nhiệm vụ ScienceQA.

▷ Hướng dẫn
Đọc bảng và câu hỏi sau đây, và tạo ra kiến thức đặc thù lĩnh vực như thông tin ngữ cảnh có thể hữu ích để trả lời câu hỏi.

▷ Ví dụ trong ngữ cảnh
Bảng:
x | y
10 | 15
11 | 9
12 | 2
Câu hỏi: Bảng cho thấy một hàm. Hàm này tuyến tính hay phi tuyến?
Kiến thức:
- Một hàm tuyến tính là một hàm có đồ thị là một đường thẳng.
- Một hàm phi tuyến là một hàm có đồ thị không phải là một đường thẳng.
- Phương trình của một hàm tuyến tính là y=mx+b, trong đó m là độ dốc và b là giao điểm trục y.
- Phương trình của một hàm phi tuyến không phải là y=mx+b.

Bảng 13: Prompt được xây dựng cho module "Knowledge Retrieval" trên nhiệm vụ TabMWP.

--- TRANG 22 ---
▷ Hướng dẫn
Đọc câu hỏi và bảng sau đây. Mỗi hàng được phân tách bởi một dòng mới ('\n') và mỗi cột được phân tách bởi một thanh dọc ('|'). Trả về bảng được đơn giản hóa chỉ còn lại các hàng liên quan đến câu hỏi. Nếu tất cả các hàng đều liên quan, hoặc số lượng hàng ít hơn ba, trả về bảng gốc.

▷ Ví dụ trong ngữ cảnh
Câu hỏi: Trong việc chuẩn bị cho lễ tốt nghiệp, một số giáo viên và học sinh đã tình nguyện cho các ủy ban tốt nghiệp khác nhau. Có bao nhiêu người trong ủy ban âm nhạc?
Bảng:
Ủy ban | Học sinh | Giáo viên
Chương trình | 5 | 17
Vé | 20 | 5
Âm nhạc | 20 | 15
Lịch trình | 15 | 20
Thực phẩm | 18 | 2
Bảng được đơn giản hóa:
Ủy ban | Học sinh | Giáo viên
Âm nhạc | 20 | 15

Bảng 14: Prompt được xây dựng cho module "Row Lookup" trên nhiệm vụ TabMWP.

▷ Hướng dẫn
Đọc câu hỏi và bảng sau đây. Mỗi hàng được phân tách bởi một dòng mới ('\n') và mỗi cột được phân tách bởi một thanh dọc ('|'). Trả về bảng được đơn giản hóa chỉ còn lại các cột liên quan đến câu hỏi. Nếu tất cả các cột đều liên quan, trả về bảng gốc.

▷ Ví dụ trong ngữ cảnh
Câu hỏi: Nhìn vào lịch trình sau đây. Giờ nghỉ kết thúc lúc nào?
Bảng:
Môn học | Bắt đầu | Kết thúc
Giờ nghỉ | 6:15 A.M. | 7:20 A.M.
Dàn nhạc | 7:30 A.M. | 8:40 A.M.
Nghệ thuật | 8:45 A.M. | 9:35 A.M.
Viết chữ | 9:45 A.M. | 10:20 A.M.
Thể dục | 10:30 A.M. | 11:15 A.M.
Ca đoàn | 11:20 A.M. | 12:25 P.M.
Khoa học | 12:35 P.M. | 1:35 P.M.
Đọc | 1:40 P.M. | 2:50 P.M.
Bảng được đơn giản hóa:
Môn học | Kết thúc
Giờ nghỉ | 7:20 A.M.
Dàn nhạc | 8:40 A.M.
Nghệ thuật | 9:35 A.M.
Viết chữ | 10:20 A.M.
Thể dục | 11:15 A.M.
Ca đoàn | 12:25 P.M.
Khoa học | 1:35 P.M.
Đọc | 2:50 P.M.

Bảng 15: Prompt được xây dựng cho module "Column Lookup" trên nhiệm vụ TabMWP.

--- TRANG 23 ---
▷ Hướng dẫn
Đọc câu hỏi và bảng sau đây. Viết một mô tả văn bản về bảng. Mô tả nên giữ lại thông tin quan trọng trong bảng để trả lời câu hỏi. Mô tả không nên trả lời câu hỏi.

▷ Ví dụ trong ngữ cảnh
Bảng:
Ủy ban | Học sinh | Giáo viên
Chương trình | 5 | 17
Vé | 20 | 5
Âm nhạc | 20 | 15
Lịch trình | 15 | 20
Thực phẩm | 18 | 2
Mô tả bảng: Bảng cho thấy số lượng học sinh và giáo viên trong mỗi ủy ban tốt nghiệp: Chương trình, Vé, Âm nhạc và Lịch trình. Ủy ban Âm nhạc có 20 học sinh và 15 giáo viên.

Bảng 16: Prompt được xây dựng cho module "Table Verbalizer" trên nhiệm vụ TabMWP.

▷ Hướng dẫn
Đọc bảng sau đây và sau đó viết mã Python để trả lời một câu hỏi.

▷ Ví dụ trong ngữ cảnh
Bảng:
Giá | Lượng cầu | Lượng cung
$895 | 21,000 | 3,400
$945 | 17,200 | 7,400
$995 | 13,400 | 11,400
$1,045 | 9,600 | 15,400
$1,095 | 5,800 | 19,400
Câu hỏi: Nhìn vào bảng. Sau đó trả lời câu hỏi. Ở mức giá $995, có thiếu hụt hay thừa? Vui lòng chọn từ các tùy chọn sau: ['shortage', 'surplus'].
Mã:
# Mã Python, trả về 'ans'. Đảm bảo rằng 'ans' là một chuỗi được chọn
từ các tùy chọn trong câu hỏi
quantity_demanded_at_price_955 = 13400
quantity_supplied_at_price_955 = 11400
if quantity_demanded_at_price_955 > quantity_supplied_at_price_955:
    ans = 'shortage'
else:
    ans = 'surplus'

Bảng 17: Prompt được xây dựng cho module "Program Generator" trên nhiệm vụ TabMWP.

--- TRANG 24 ---
▷ Hướng dẫn
Đọc bảng sau đây và sau đó trả lời một câu hỏi.

▷ Ví dụ trong ngữ cảnh
Bảng:
Giá | Lượng cầu | Lượng cung
$895 | 21,000 | 3,400
$945 | 17,200 | 7,400
$995 | 13,400 | 11,400
$1,045 | 9,600 | 15,400
$1,095 | 5,800 | 19,400
Câu hỏi: Nhìn vào bảng. Sau đó trả lời câu hỏi. Ở mức giá $995, có thiếu hụt hay thừa? Vui lòng chọn từ các tùy chọn sau: ['shortage', 'surplus'].
Giải pháp: Ở mức giá $995, lượng cầu lớn hơn lượng cung. Không đủ hàng hóa hoặc dịch vụ để bán ở mức giá đó. Vậy, có sự thiếu hụt. Câu trả lời là shortage.

Bảng 18: Prompt được xây dựng cho module "Solution Generator" trên nhiệm vụ TabMWP.

BẮT ĐẦU
knowledge_retrieval 0.53 
text_detector 0.3 query_generator 0.11 
image_captioner 0.06 
solution_generator 1.0  0.78 
 0.22 bing_search 1.0 
 0.79  0.01 
 0.21 
answer_generator 1.0  0.02  0.01 
 0.97 
KẾT THÚC 1.0 

Hình 7: Chuyển đổi giữa các module trong các chương trình được tạo ra bởi Chameleon (GPT-4) trên ScienceQA. BẮT ĐẦU là ký hiệu bắt đầu, KẾT THÚC là ký hiệu kết thúc và các ký hiệu khác là ký hiệu không kết thúc.

--- TRANG 25 ---
BẮT ĐẦU
program_generator 0.39 row_lookup 0.26 
knowledge_retrieval 0.19 
solution_generator 0.11 column_lookup 0.05 
program_verifier 1.0  0.2  0.24 
 0.26  0.03 
table_verbalizer 0.27 
 0.63  0.36  0.02 
answer_generator 1.0  0.74  0.02 
 0.02 
 0.25 
program_executor 1.0 
 1.0 
KẾT THÚC 1.0  1.0 

Hình 8: Chuyển đổi giữa các module trong các chương trình được tạo ra bởi Chameleon (GPT-4) trên TabMWP. BẮT ĐẦU là ký hiệu bắt đầu, KẾT THÚC là ký hiệu kết thúc và các ký hiệu khác là ký hiệu không kết thúc.

--- TRANG 26 ---
Một con gấu Bắc Cực đang đứng trong tuyết.
Thích nghi da động vật cho môi trường lạnh
Eurasian lynx Bộ tạo câu trả lời
Bộ tạo giải pháp
Bộ mô tả hình ảnh
(Phản hồi 1) Chim sử dụng lông, và hầu hết động vật có vú sử dụng lông hoặc lông thú, để bẫy một lớp không khí gần da và giảm truyền nhiệt ra môi trường. Động vật có vú biển như cá voi sử dụng mỡ, một lớp mỡ dày, như một dạng cách nhiệt hạng nặng. Trong thời tiết lạnh, chim làm bông lông và động vật dựng lông để làm dày lớp cách nhiệt.
(Phản hồi 2) Thích nghi: Có một lớp lông dày để cách nhiệt chống lạnh. Vào mùa đông, lông có màu trắng để ngụy trang trên băng và tuyết. Vào mùa hè, khi tuyết đã tan, chúng có lông mỏng, màu xám đen hoặc nâu cung cấp khả năng ngụy trang trên đá và thực vật.
(Phản hồi 3) Gấu Bắc Cực thích nghi với cuộc sống trong khí hậu lạnh. Gấu Bắc Cực thích nghi tốt để sinh tồn ở Bắc Cực. Sự thích nghi của chúng bao gồm: vẻ ngoài màu trắng - như ngụy trang khỏi con mồi trên tuyết...
Ngữ cảnh đề cập rằng gấu Bắc Cực sống trong và xung quanh Đại Tây Dương Bắc Cực lạnh, và da của chúng được thích nghi để giúp chúng sinh tồn ở những nơi lạnh. Trong số các tùy chọn đã cho, không Eurasian lynx cũng không thorny devil được đề cập trong ngữ cảnh. Tuy nhiên, gấu Bắc Cực được đề cập như một động vật có da thích nghi để sinh tồn ở những nơi lạnh. Vì gấu Bắc Cực không được liệt kê là một tùy chọn, chúng ta phải chọn tùy chọn có thể nhất từ các lựa chọn đã cho. Eurasian lynx sống ở các vùng lạnh hơn so với thorny devil, là một loài bò sát sống sa mạc. Do đó, câu trả lời là A.
Bộ tạo truy vấn
Tìm kiếm Bing

Da của loài động vật nào được thích nghi để sinh tồn ở những nơi lạnh?
(A) Eurasian lynx 
(B) Thorny Devil

Hình 9: Thêm một ví dụ từ phương pháp Chameleon (GPT-4) trên ScienceQA.

[Tiếp tục với ví dụ TabMWP về bảng lương của Bronson và các tính toán thuế]

Hình 10: Thêm một ví dụ từ phương pháp Chameleon (GPT-4) trên TabMWP.

--- TRANG 27 ---
▷ Truy vấn
Câu hỏi: Mô tả nào tốt hơn về hệ sinh thái Death Valley?
Ngữ cảnh: Hình: Death Valley. Death Valley là một hệ sinh thái sa mạc ở phía đông California.
Tùy chọn: (A) Nó có đất khô, mỏng. Nó cũng có nhiều loại sinh vật khác nhau. (B) Nó có một lượng mưa nhỏ. Nó cũng chỉ có một vài loại sinh vật.
Metadata: 'pid': '18973', 'has_image': True, 'grade': 3, 'subject': 'natural science', 'topic': 'biology', 'category': 'Ecosystems', 'skill': 'Describe ecosystems'

▷ Phản hồi
Chương trình được tạo ra: ["Query_Generator","Bing_Search","Solution_Generator","Answer_Generator"]
Bộ tạo truy vấn:
Đặc điểm của hệ sinh thái Death Valley
Tìm kiếm Bing: (cái đầu tiên từ các ứng viên phản hồi)
Du khách đến Death Valley để trải nghiệm sự rộng lớn khắc nghiệt và cô đơn của thung lũng, toàn cảnh của những hẻm núi và dãy núi gồ ghề; niềm vui của khí hậu mùa đông khô ráo, ôn hòa, thử thách của mùa hè nóng bức, khô cằn; sự nhẹ nhõm của những ngọn núi mát mẻ hơn; và những lời nhắc nhở về cuộc sống biên giới và lối sống của người Mỹ bản địa.
Bộ tạo giải pháp:
Death Valley là một hệ sinh thái sa mạc, có nghĩa là nó có những đặc điểm cụ thể liên quan đến khí hậu và các sinh vật sống ở đó. Một hệ sinh thái sa mạc thường có đất khô, mỏng và một lượng mưa nhỏ. Môi trường này chỉ hỗ trợ một vài loại sinh vật được thích nghi để sinh tồn trong những điều kiện khắc nghiệt như vậy. Do đó, câu trả lời là B.
Bộ tạo câu trả lời (Dự đoán sai):
(B) Nó có một lượng mưa nhỏ. Nó cũng chỉ có một vài loại sinh vật.
Câu trả lời đúng (Sự thật cơ bản):
(A) Nó có đất khô, mỏng. Nó cũng có nhiều loại sinh vật khác nhau.

Bảng 19: Một ví dụ về trường hợp thất bại trong ScienceQA từ Chameleon (GPT-4) nơi một số module hoạt động không chính xác. Truy vấn được tạo ra từ module "Query Generator" không đủ cụ thể, dẫn đến phản hồi từ "Bing Search" không mô tả đầy đủ các đặc điểm của hệ sinh thái Death Valley, đặc biệt là về các loại sinh vật có mặt. (Từ U.S. Geological Survey: Trái với tên gọi của nó, Vườn quốc gia Death Valley rất sống động... Mặc dù lượng mưa thấp và nhiệt độ mùa hè cực đoan, Death Valley là nơi sinh sống của nhiều loài đa dạng, một số trong đó là đặc hữu (chỉ được tìm thấy ở đây).⁴

--- TRANG 28 ---
▷ Truy vấn
Câu hỏi: Tính chất nào phù hợp với vật thể này?
Ngữ cảnh: Chọn câu trả lời tốt hơn.
Tùy chọn: (A) co giãn (B) màu vàng
Metadata: 'pid': '10338', 'has_image': True, 'grade': 2, 'subject': 'natural science', 'topic': 'physics', 'category': 'Materials', 'skill': 'Identify properties of an object'

▷ Phản hồi
Chương trình được tạo ra: ["Image_Captioner","Solution_Generator","Answer_Generator"]
Bộ mô tả hình ảnh:
Một chiếc diều đầy màu sắc đang được thả trên bầu trời.
Bộ tạo giải pháp:
Nhìn vào hình ảnh của vật thể, đó là một chiếc diều đầy màu sắc. Một vật thể co giãn có thể được kéo dài mà không bị vỡ. Diều thường được làm từ những vật liệu không co giãn, như nylon hoặc polyester. Một vật thể màu vàng có màu vàng. Chiếc diều trong hình ảnh đầy màu sắc và bao gồm màu vàng. Do đó, câu trả lời là B.
Bộ tạo câu trả lời (Dự đoán sai): (B) màu vàng
Câu trả lời đúng (Sự thật cơ bản): (A) co giãn

Bảng 20: Một ví dụ về trường hợp thất bại trong ScienceQA từ Chameleon (GPT-4) nơi các chương trình được tạo ra không tối ưu. Bộ lập kế hoạch LLM không gọi các công cụ "Text Detector" và "Knowledge Retrieval", dẫn đến việc thiếu thông tin ngữ nghĩa từ hình ảnh và thiếu kiến thức lĩnh vực liên quan. Ngoài ra, mô hình mô tả hình ảnh được sử dụng không cung cấp một mô tả văn bản chính xác về ngữ cảnh thị giác đã cho.

--- TRANG 29 ---
▷ Truy vấn
Câu hỏi: So sánh năng lượng động học trung bình của các hạt trong mỗi mẫu. Mẫu nào có nhiệt độ cao hơn?
Ngữ cảnh: Dưới đây là một mạng lưới thức ăn từ một hệ sinh thái đại dương ở Vịnh Monterey, ngoài khơi bờ biển California. Một mạng lưới thức ăn mô hình hóa cách vật chất được ăn bởi các sinh vật di chuyển qua một hệ sinh thái. Các mũi tên trong mạng lưới thức ăn biểu thị cách vật chất di chuyển giữa các sinh vật trong một hệ sinh thái.
Tùy chọn: (A) bat star (B) phytoplankton (C) kelp bass
Metadata: {'pid': '15175', 'has_image': True, 'grade': 6, 'subject': 'natural science', 'topic': 'biology', 'category': 'Ecological interactions', 'skill': 'Interpret food webs II'}

▷ Phản hồi
Chương trình được tạo ra: ["Text_Detector","Knowledge_Retrieval","Solution_Generator","Answer_Generator"]
Bộ phát hiện văn bản:
[([297, 91], 'orca'), ([653, 85], 'sea otter'),
([487, 191], 'kelp bass'), ([73, 221], 'phytoplankton'),
([625, 269], 'black rockfish'), ([323, 327], 'zooplankton'),
([149, 363], 'plainfin'), ([593, 361], 'kelp'),
([132, 380], 'midshipman'), ([401, 391], 'bat star'),
([271, 433], 'sea cucumber'), ([409, 471], 'sea urchin')]
Truy xuất kiến thức:
- Câu hỏi là về một mạng lưới thức ăn trong một hệ sinh thái đại dương ở Vịnh Monterey, California.
- Một mạng lưới thức ăn mô hình hóa cách vật chất được ăn bởi các sinh vật di chuyển qua một hệ sinh thái.
- Các mũi tên trong mạng lưới thức ăn biểu thị cách vật chất di chuyển giữa các sinh vật trong một hệ sinh thái.
- Các sinh vật được đề cập trong câu hỏi là bat star, phytoplankton, và kelp bass.
- Sinh vật mục tiêu là sea cucumber.
- Trong một mạng lưới thức ăn, vật chất di chuyển từ sinh vật này sang sinh vật khác thông qua tiêu thụ, săn mồi, hoặc phân hủy.
- Hiểu các mối quan hệ giữa các sinh vật trong mạng lưới thức ăn có thể giúp xác định dòng chảy vật chất giữa chúng.
Bộ tạo giải pháp:
Phân tích mạng lưới thức ăn để xác định sinh vật nào cuối cùng chuyển vật chất đến sea cucumber.
- Sea cucumber tiêu thụ bat star.
- Bat star tiêu thụ sea urchin.
- Sea urchin tiêu thụ kelp.
- Kelp nhận chất dinh dưỡng từ nước, có chứa vật chất từ phytoplankton và các nguồn khác.
Trong mạng lưới thức ăn này, vật chất từ bat star cuối cùng chuyển đến sea cucumber thông qua tiêu thụ. Do đó, câu trả lời là A.
Bộ tạo câu trả lời (Dự đoán sai): (A) bat star
Câu trả lời đúng (Sự thật cơ bản): (B) phytoplankton

Bảng 21: Một ví dụ về trường hợp thất bại trong ScienceQA từ Chameleon (GPT-4) nơi kho module hiện tại không đủ để giải quyết các khả năng cụ thể. Trong trường hợp này, kho module không có các công cụ cần thiết có khả năng phân tích các mũi tên trong minh họa mạng lưới thức ăn.

--- TRANG 30 ---
▷ Truy vấn
Bảng:
Bảng: (định dạng bán cấu trúc)
x|y
4|15
19|18
11|10
11|1
Câu hỏi: Nhìn vào bảng này. Quan hệ này có phải là một hàm không?
Tùy chọn: (A) có (B) không

▷ Phản hồi
Chương trình được tạo ra: ["Knowledge_Retrieval","Program_Generator","Program_Verifier","Program_Executor","Answer_Generator"]
Truy xuất kiến thức:
- Một quan hệ là một tập hợp các cặp có thứ tự (x,y).
- Một hàm là một quan hệ trong đó mỗi đầu vào (giá trị x) được ghép nối với chính xác một đầu ra (giá trị y).
- Để xác định nếu một quan hệ là một hàm, kiểm tra xem mỗi giá trị x có được ghép nối chỉ với một giá trị y hay không.
- Nếu một giá trị x được ghép nối với nhiều hơn một giá trị y, quan hệ đó không phải là một hàm.
Bộ tạo chương trình:
relation = {
    4: 15,
    19: 18,
    11: 10,
    11: 1
}
x_values = list(relation.keys())
unique_x_values = set(x_values)
if len(x_values) == len(unique_x_values):
    ans = 'yes'
else:
    ans = 'no'
Bộ xác minh chương trình: Đúng
Bộ thực thi chương trình:
ans = 'yes'
Bộ tạo câu trả lời (Dự đoán sai): (A) có
Câu trả lời đúng (Sự thật cơ bản): (B) không

Bảng 22: Một ví dụ về trường hợp thất bại trong TabMWP từ Chameleon (GPT-4) nơi một số module hoạt động không chính xác. Trong trường hợp này, bộ tạo chương trình dựa trên LLM không thành công trong việc tạo ra một chương trình phản ánh chính xác mối quan hệ toán học được biểu đạt trong truy vấn được cung cấp và kiến thức lĩnh vực được truy xuất.

--- TRANG 31 ---
▷ Truy vấn
Câu hỏi: Đây là hồ sơ tài chính hoàn chỉnh của Oliver cho tháng 9. Oliver có bao nhiêu tiền sau khi anh ấy mua quả bóng rổ?
Bảng:
Bảng: (định dạng bán cấu trúc)
Ngày|Mô tả |Nhận được |Chi phí |Quỹ có sẵn
|Số dư: cuối tháng 8 | | |$154.90
9/1|sổ ghi chú | |$6.95|$147.95
9/15|dắt chó |$15.00 | |$162.95
9/21|bóng rổ | |$11.35 |$151.60

▷ Phản hồi
Chương trình được tạo ra: ["Row_Lookup","Solution_Generator","Answer_Generator"]
Tra cứu hàng:
Ngày|Mô tả |Nhận được |Chi phí |Quỹ có sẵn
9/21|bóng rổ | |$11.35 |$151.60
Bộ tạo giải pháp:
(Bước 1) Trừ chi phí của quả bóng rổ từ quỹ có sẵn trước khi mua.
(Bước 2) $151.60 - $11.35 = $140.25
(Bước 3) Oliver có $140.25 sau khi anh ấy mua quả bóng rổ.
(Bước 4) Câu trả lời là $140.25.
Bộ tạo câu trả lời (Dự đoán sai): 140.25
Câu trả lời đúng (Sự thật cơ bản): 151.60

Bảng 23: Một ví dụ về trường hợp thất bại trong TabMWP từ Chameleon (GPT-4) nơi các chương trình được tạo ra không tối ưu. Trong ví dụ đã cho, bảng được đơn giản hóa được tạo ra bởi "Row Lookup" thiếu các mối quan hệ cấu trúc có trong ngữ cảnh bảng gốc. Những mối quan hệ này rất quan trọng để bộ tạo giải pháp dựa trên LLM hiểu chính xác bảng đặc thù lĩnh vực.

--- TRANG 32 ---
▷ Truy vấn
Bảng:
Tiêu đề bảng: Lịch trình xe buýt du lịch
Bảng: (định dạng bán cấu trúc)
ngôi nhà lịch sử |10:15 A.M. |10:30 A.M. |10:45 A.M. |11:00 A.M.
trung tâm thành phố |11:00 A.M. |11:15 A.M. |11:30 A.M. |11:45 A.M.
tòa nhà chọc trời |11:30 A.M. |11:45 A.M. |12:00 P.M. |12:15 P.M.
tòa nhà cũ |12:30 P.M. |12:45 P.M. |1:00 P.M. |1:15 P.M.
dinh thự thống đốc |1:00 P.M. |1:15 P.M. |1:30 P.M. |1:45 P.M.
Câu hỏi: Đây là hồ sơ tài chính hoàn chỉnh của Oliver cho tháng 9. Oliver có bao nhiêu tiền sau khi anh ấy mua quả bóng rổ?
Tùy chọn: (A) 2 giờ 15 phút (B) 2 giờ 45 phút (C) 15 phút (D) 30 phút

▷ Phản hồi
Chương trình được tạo ra: ["Row_Lookup","Solution_Generator","Answer_Generator"]
Tra cứu hàng:
ngôi nhà lịch sử |10:15 A.M. |10:30 A.M. |10:45 A.M. |11:00 A.M.
dinh thự thống đốc |1:00 P.M. |1:15 P.M. |1:30 P.M. |1:45 P.M.
Bộ tạo giải pháp:
(Bước 1) Tìm thời gian khởi hành cuối cùng từ ngôi nhà lịch sử và thời gian đến đầu tiên tại dinh thự thống đốc:
Khởi hành cuối cùng từ ngôi nhà lịch sử: 11:00 A.M.
Đến đầu tiên tại dinh thự thống đốc: 1:00 P.M.
(Bước 2) Trừ thời gian khởi hành từ thời gian đến:
1:00 P.M. - 11:00 A.M. = 2 giờ
(Bước 3) Mất 2 giờ để đi từ ngôi nhà lịch sử đến dinh thự thống đốc.
(Bước 4) Câu trả lời là 2 giờ 15 phút.
Bộ tạo câu trả lời (Dự đoán sai): (A) 2 giờ 15 phút
Câu trả lời đúng (Sự thật cơ bản): (B) 2 giờ 45 phút

Bảng 24: Một ví dụ về trường hợp thất bại trong TabMWP từ Chameleon (GPT-4) nơi một số module hoạt động không chính xác và các chương trình được tạo ra không tối ưu. Trong trường hợp này, bộ tạo giải pháp dựa trên LLM gặp khó khăn trong việc hiểu lịch trình xe buýt, bao gồm kiến thức đặc thù lĩnh vực. Hơn nữa, bộ lập kế hoạch LLM không sử dụng các công cụ như "Table Verbalizer" và "Column Lookup", có thể tăng cường khả năng hiểu ngữ cảnh bảng của LLM.
