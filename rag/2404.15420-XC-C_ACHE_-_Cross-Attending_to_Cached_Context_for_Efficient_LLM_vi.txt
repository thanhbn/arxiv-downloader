# 2404.15420.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2404.15420.pdf
# Kích thước tệp: 676420 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
XC-CACHE: Cross-Attending to Cached Context for Efficient LLM
Inference
João Monteiro†,3Étienne Marcotte†,1, Pierre-André Noël†,1, Valentina Zantedeschi†,1,
David Vázquez1, Nicolas Chapados1, Christopher Pal1,2, Perouz Taslakian†,1
1-ServiceNow Research
2-Quebec Artificial Intelligence Institute  
3-Autodesk – Công việc được thực hiện tại ServiceNow
†Những người đóng góp chính

Tóm tắt
Các prompt thường được sử dụng để điều kiện hóa việc sinh văn bản của các mô hình ngôn ngữ chỉ giải mã dựa trên thông tin tham khảo. Việc xử lý ngữ cảnh ngay tại thời điểm đó không hiệu quả do chi phí bậc hai của các phép toán self-attention, và việc lưu trữ cache là mong muốn. Tuy nhiên, việc lưu trữ cache các trạng thái transformer có thể dễ dàng yêu cầu gần như bằng không gian với các tham số mô hình. Khi ngữ cảnh phù hợp không được biết trước, việc lưu trữ cache prompt có thể thách thức. Công trình này giải quyết những hạn chế này bằng cách giới thiệu các mô hình được lấy cảm hứng từ kiến trúc encoder-decoder, sử dụng cross-attention để điều kiện hóa việc sinh văn bản dựa trên văn bản tham khảo mà không cần prompt. Cụ thể hơn, chúng tôi tận dụng các mô hình decoder-only đã được huấn luyện trước và chỉ huấn luyện một số lượng nhỏ các lớp được thêm vào. Chúng tôi sử dụng Question-Answering (QA) như một bệ thử để đánh giá khả năng thực hiện sinh có điều kiện của các mô hình và quan sát thấy chúng vượt trội hơn các phương pháp suy luận dựa trên prompt, có thể so sánh với các LLM được tinh chỉnh với prompt, và giảm đáng kể dấu chân không gian so với KV caching tiêu chuẩn hai bậc độ lớn. Cụ thể, chúng tôi giới thiệu XC-LLAMA chuyển đổi LLAMA 2 đã được huấn luyện trước thành kiến trúc encoder-decoder bằng cách tích hợp các lớp cross-attention xen kẽ giữa các lớp self-attention hiện có.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) đã thúc đẩy những tiến bộ trong mô hình hóa ngôn ngữ và cho phép sản xuất tự động văn bản gần như giống con người. Mặc dù có tiến bộ ấn tượng, những thách thức vẫn tồn tại trong việc áp dụng LLM trong các thiết lập thực tế như nguy cơ ảo giác (hay đúng hơn là confabulation (Bottou và Schölkopf, 2023; Millidge, 2023)) và nội dung không thực tế (Li et al., 2023a; Xu et al., 2024) hoặc độc hại (Zou et al., 2023; Xhonneux et al., 2024) trong văn bản được sinh ra. Hơn nữa, không có fine-tuning, việc điều chỉnh các mô hình này để kết hợp thông tin mới không có trong dữ liệu huấn luyện của chúng là điều đáng ngạc nhiên khó khăn (Luo et al., 2023; Kalajdzievski, 2024). Thật vậy, trong khi LLM có thể trả lời các truy vấn về dữ liệu huấn luyện của chúng một cách chính xác ở chế độ zero-shot (Petroni et al., 2019), các truy vấn liên quan đến thông tin không có sẵn tại thời điểm huấn luyện thường dẫn đến không chính xác (Maynez et al., 2020; Ji et al., 2023) như người ta mong đợi.

Công trình này tập trung vào việc định hướng sinh văn bản LLM dựa trên thông tin ngữ cảnh được cung cấp tại thời điểm suy luận (Hình 2(a)). Cách tiếp cận phổ biến nhất để điều kiện hóa sinh văn bản mô hình có lẽ là In-Context Learning¹ (ICL) (Radford et al., 2019; Brown et al., 2020): người ta thêm ngữ cảnh liên quan vào đầu prompt để sinh ra câu trả lời có điều kiện dựa trên truy vấn và ngữ cảnh kết hợp. Kỹ thuật này là thành phần cốt lõi của các framework phổ biến, chẳng hạn như Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), với đặc điểm là ngữ cảnh liên quan không được biết trước, mà phải được truy xuất từ một corpus (Ram et al., 2023).

¹Trong suốt bài báo này, chúng tôi sử dụng in-context learning để đề cập đến thiết lập suy luận dựa trên prompt 0-shot trong đó thông tin cần thiết để giải quyết nhiệm vụ được bao gồm trong prompt.

--- TRANG 2 ---
Chương 2: Quả trứng Phục sinh
biến mất...
Soạn một bài thơ ngắn về
chủ đề đã cho. Tất cả vần điệu...
(b)(a)
(c)
(d)
ngữ cảnh ngữ cảnh
truy vấn câu trả lời
câu trả lời
câu trả lời
câu trả lời truy vấn
ngữ cảnh
truy vấn truy vấn cache
cache Giấc mơ nào mà Harry đang có
khi con Thỏ ăn thịt anh? Sự lên và xuống của
Đế chế Ví dụ.
(công trình này)

Hình 2: Suy luận nhanh hơn trong mô hình hóa ngôn ngữ có điều kiện ngữ cảnh. (a) Một trường hợp sử dụng trong đó truy vấn của người dùng phải được diễn giải trong một ngữ cảnh nào đó để sinh ra câu trả lời. Công trình này tập trung vào các trường hợp trong đó truy vấn và câu trả lời đều nhỏ (nhẹ 🔸), nhưng ngữ cảnh lại lớn (nặng 🔺). Độ phức tạp thời gian LLM kết quả do đó là O(|ngữ cảnh|²) (chậm 🔴). (b) In-context learning (ICL) và retrieval augmented generation (RAG) là hai ví dụ trong đó truy vấn được sử dụng để tra cứu ngữ cảnh từ một corpus hữu hạn. (c) Trong nhiều trường hợp, ngữ cảnh có thể được xử lý trước thành một cache cho phép suy luận O(|ngữ cảnh|(|truy vấn|+|câu trả lời|)) (nhanh 🟢) trên một truy vấn cho trước. (d) Corpus ngữ cảnh hữu hạn có thể được xử lý offline, cho phép thực thi nhanh tại thời điểm suy luận. Vì kích thước cache ảnh hưởng đến chi phí lưu trữ và truyền thông, chúng tôi tìm kiếm các mô hình yêu cầu cache nhỏ hơn.

Mặc dù phần nào hiệu quả và đơn giản, ICL, như thường được thực hiện với các kiến trúc chỉ giải mã, có những khuyết điểm. Một mặt, việc sinh dựa trên ICL được biết là có phương sai cao liên quan đến template prompt sao cho các định dạng prompt hợp lệ tương đương tạo ra kết quả khác biệt đáng kể (Chen et al., 2023). Mặt khác, ICL tốn kém về thời gian và không gian. Việc xử lý ngữ cảnh ngay tại thời điểm đó chịu độ phức tạp bậc hai về độ dài do các phép toán self-attention (Vaswani et al., 2017). Thay thế là tiền xử lý và lưu trữ cache các trạng thái nội bộ ngữ cảnh (gọi là các trạng thái key-value hoặc KV) để tăng tốc suy luận (Hình 2(c)). Tuy nhiên, điều này có thể yêu cầu cùng bậc không gian như chính các tham số mô hình (chúng tôi đưa ra chi tiết trong Phần 2). Công trình gần đây đã giảm yêu cầu không gian của KV caching bằng cách lấy mẫu phụ các trạng thái (Xiao et al., 2023; Adnan et al., 2024), mặc dù với chi phí bỏ qua nội dung liên quan.

Để vượt qua những hạn chế này, chúng tôi đề xuất các thay thế cho ICL thực hiện sinh có điều kiện mà không tiêm thông tin liên quan vào prompt (Hình 2(a)), và tìm cách triển khai các phương pháp cache nhẹ như minh họa trong Hình 2(d). Cách tiếp cận của chúng tôi gợi nhớ đến các kiến trúc encoder-decoder, hiện có thể được coi là legacy, vì nó dựa vào các lớp cross-attention để điều kiện hóa việc sinh dựa trên các mã hóa ngữ cảnh được tính toán trước. Cụ thể hơn, chúng tôi đề xuất cross-context-cache (XC-CACHE), lưu trữ chỉ các đầu ra của các trạng thái ẩn encoder và dựa vào cross-attention để tiêu thụ cache tại thời điểm suy luận. Chúng tôi cụ thể hóa XC-CACHE thông qua hai cách tiếp cận hiệu quả tham số tận dụng các mô hình decoder-only đã được huấn luyện trước và mở rộng chúng với một encoder riêng để xử lý ngữ cảnh: một cách tiếp cận sử dụng decoder đông lạnh làm encoder (gọi là XC-LLAMA), và cách khác sử dụng một encoder hai chiều nhỏ (gọi là XC-LLAMA ENC). Quan trọng là, các kiến trúc encoder-decoder của chúng tôi phù hợp hơn để lưu trữ cache các trạng thái ngữ cảnh, yêu cầu ít không gian hơn các đối tác ICL hàng bậc độ lớn. Khi cache ngữ cảnh được kích hoạt, các mô hình được tinh chỉnh dẫn đến hiệu suất độ chính xác cao hơn, nhưng đòi hỏi dấu chân bộ nhớ lớn (và do đó, độ trễ và chi phí cao hơn). Ngược lại, cách tiếp cận XC-CACHE của chúng tôi giảm đáng kể yêu cầu bộ nhớ cache gần 98%; và như các kỹ thuật cải thiện hiệu quả khác như quantization (Frantar et al., 2023), việc giảm này đi kèm với chi phí nhỏ về độ chính xác, như minh họa trong Hình 1. Tuy nhiên, phương pháp của chúng tôi liên tục vượt trội hơn các thay thế ICL dựa trên LLAMA 2 hoặc GPT-3.5, như chi tiết trong Phần 5.

Nhìn chung, chúng tôi ủng hộ một sự thay đổi khái niệm trong thiết kế kiến trúc cho sinh có điều kiện, việc này nên tái trung tâm hóa việc lưu trữ cache và làm cho nó trở thành phần tích hợp vào hoạt động của mô hình thay vì một suy nghĩ sau. Những đóng góp của chúng tôi được tóm tắt như sau:

1. Khả năng lưu cache: Chúng tôi cung cấp bằng chứng rằng các kiến trúc encoder-decoder là ứng cử viên tốt cho sinh có điều kiện vì các mô hình thân thiện với cache của chúng tôi tăng cường hiệu suất mô hình so với ICL, trong khi giảm dấu chân bộ nhớ cache hơn 98%.

2. Hiệu quả tham số: Chúng tôi cho thấy rằng huấn luyện một vài lớp cross-attention (và tùy chọn, một encoder nhỏ) đủ để chuyển đổi decoder thành cặp encoder-decoder. Chúng tôi đóng góp một hỗn hợp các nhiệm vụ huấn luyện cho phép sinh có điều kiện ngữ cảnh mà không cần ICL tốn kém.

3. Decoder-as-encoder: Chúng tôi cho thấy rằng các biểu diễn được trích xuất từ các decoder nhân quả đã được huấn luyện trước có thể được sử dụng nguyên trạng để thay thế một encoder.

--- TRANG 3 ---
2 Lưu trữ Cache Biểu diễn

Gọi ngữ cảnh, truy vấn và câu trả lời là các chuỗi token từ một từ vựng chung V. Chúng ta viết |ngữ cảnh|, |truy vấn| và |câu trả lời| là độ dài tương ứng của các chuỗi này. Hình 2(a) minh họa một LLM mà, có điều kiện trên ngữ cảnh, tạo ra câu trả lời cho truy vấn do người dùng chỉ định.

Giả định. Trong những gì theo sau, chúng tôi đưa ra ba giả định chính: (A1) ngữ cảnh không duy nhất và phụ thuộc vào truy vấn; (A2) tồn tại một lượng ngữ cảnh có thể quản lý để điều kiện hóa; và (A3) ngữ cảnh lớn, tức là |truy vấn| + |câu trả lời| ≪ |ngữ cảnh|. Bằng cách làm như vậy, chúng tôi hạn chế bản thân vào các chế độ mà việc xử lý ngữ cảnh offline vừa hấp dẫn (A1 và A3) vừa khả thi (A2).

Nhiều trường hợp sử dụng ICL thỏa mãn những giả định này. Ví dụ, khi đối mặt với một số nhiệm vụ khác nhau, chúng ta có thể tạo ra các hướng dẫn cụ thể cho nhiệm vụ, mỗi cái chi tiết cách thu được câu trả lời cho truy vấn (Hình 2(b)). Điều tương tự áp dụng cho question-answering dựa trên RAG khi các tài liệu được truy xuất được mã hóa độc lập: retriever chọn từ corpus các tài liệu liên quan nhất (ngữ cảnh) để trả lời câu hỏi của người dùng (truy vấn).

2.1 Độ phức tạp thời gian suy luận

Nói chung, độ phức tạp thời gian của suy luận LLM bị chi phối bởi cơ chế attention của nó, trong thiết lập ICL là O((|ngữ cảnh| + |truy vấn| + |câu trả lời|)²). Trong chế độ ngữ cảnh lớn (A3), điều này đơn giản hóa thành O(|ngữ cảnh|²): do đó chúng ta có thể mong đợi tăng tốc suy luận rõ rệt bằng cách cải thiện cách chúng ta xử lý ngữ cảnh. Một cách để đạt được tăng tốc như vậy là tiền xử lý các ngữ cảnh offline thành một số trạng thái trung gian (cache) và cung cấp cho mô hình tại thời điểm suy luận (Hình 2(c)). Bằng cách này, chi phí bậc hai của việc xử lý ngữ cảnh được trả một lần, cho phép suy luận chỉ đơn giản tra cứu cache đã sẵn sàng (Hình 2(d)). Lưu ý rằng chi phí lưu trữ và truyền thông phát sinh là tuyến tính theo kích thước cache, cũng tuyến tính theo độ dài ngữ cảnh. Trong thiết lập này, độ phức tạp thời gian suy luận trở thành O(|ngữ cảnh|(|truy vấn| + |câu trả lời|)), tức là tuyến tính theo độ dài ngữ cảnh (Hình 3), một tăng tốc đáng kể.

2.2 Chi phí thực tế của caching

Các cân nhắc thực tế có thể ảnh hưởng tiêu cực đến thiết lập có cache. Tải và chi phí truyền thông đều tuyến tính theo kích thước cache, thúc đẩy cache nhỏ hơn. Bất kỳ phép toán bổ sung nào cần thiết tại thời điểm suy luận đều giảm bớt lợi ích caching.

[Hình 3: Biểu diễn cách điệu về thời gian thực thi attention (diện tích) cho mô hình hóa ngôn ngữ có điều kiện ngữ cảnh. Các đường đứt nét trong (b) hiển thị khoản tiết kiệm khi cache key và value (quá khứ) của ngữ cảnh được cung cấp. Đối với các mô hình nhân quả, diện tích dưới đường chéo thể hiện thời gian thực thi có thể được tiết kiệm bằng các phương tiện khác. Xem Phụ lục A để biết chi tiết.]

Các phương pháp caching có thể phát sinh chi phí (ẩn hoặc rõ ràng) về chất lượng của câu trả lời được sinh ra. Xem Phụ lục A để biết chi tiết.

2.3 Các cách tiếp cận Caching

KV Caching. Cách tiếp cận mặc định là lưu trữ các trạng thái key và value (quá khứ) được sinh ra trong khi xử lý ngữ cảnh, sau này gọi là KV caching. KV caching thường được liên kết với thiết lập hội thoại, trong trường hợp này cache thường vẫn ở trên thiết bị GPU giữa các vòng hội thoại (Hình 2(d)). Các thiết lập khác, chẳng hạn như phục vụ nhiều cuộc hội thoại, tuy nhiên đòi hỏi chúng ta phải di chuyển cache từ lưu trữ (và/hoặc CPU RAM) đến GPU, phát sinh thêm độ trễ. Như một ví dụ, đối với LLAMA 2-7B (Touvron et al., 2023) sử dụng độ chính xác 16 bit, chúng ta phải di chuyển khoảng 512 kB trên mỗi token². Kích thước cache nhỏ hơn trên mỗi token do đó là mong muốn.

JIT-KV Caching. Một cách tiếp cận thay thế là lưu trữ các trạng thái ẩn (quá khứ) của mô hình trong cache (trong trường hợp LLAMA 2-7B, điều này sẽ nhỏ hơn một nửa, tức là 256 kB trên mỗi token). Tại thời điểm suy luận, một khi các trạng thái ẩn này được tải lên GPU, chúng ta có thể khôi phục key và value đầy đủ trong các phép toán song song tốt O(|ngữ cảnh|) (nhớ rằng key và value của transformer được thu được bằng phép toán tuyến tính trên các trạng thái ẩn). Chúng tôi gọi cách tiếp cận này là "just-in-time" hoặc JIT-KV caching.

XC Caching (Của chúng tôi). Cả KV và JIT-KV caching đều thực hiện chính xác những tính toán giống như mô hình gốc mà không có cache. Cả hai đều chịu hai loại chi phí trong khi tạo ra cùng câu trả lời: kích thước cache và các phép toán cần thực hiện tại thời điểm suy luận (ví dụ: chuyển đổi trạng thái ẩn thành key và value). Trong công trình này, chúng tôi đề xuất XC caching (đọc là cross-context caching), xem xét mất mát chất lượng của câu trả lời được sinh ra như chi phí thứ ba và trình bày hai mô hình cân bằng ba chi phí này. Cả hai mô hình đều sử dụng các lớp cross-attention được tích hợp vào kiến trúc decoder, cho phép mô hình attend vào ngữ cảnh được cache với dấu chân bộ nhớ nhỏ gọn tại thời điểm suy luận. Chúng tôi mô tả cả hai mô hình trong phần tiếp theo.

²32 lớp, 32 head mỗi lớp, 128 floating point mỗi key và value (do đó hệ số 2), 2 byte mỗi floating point 16 bit

--- TRANG 4 ---
[Hình 4: Kiến trúc của XC-LLAMA. Một mô hình chỉ decoder triển khai kiến trúc encoder-decoder. Fine-tuning được thực hiện một cách hiệu quả tham số thông qua việc chỉ huấn luyện một số lượng nhỏ các lớp cross-attention.]

3 XC: Cross Attending to Efficiently Cached Context

Để giảm dấu chân bộ nhớ của caching, cách tiếp cận của chúng tôi lấy cảm hứng từ kiến trúc encoder-decoder, cho đến gần đây là thiết kế được ưa chuộng cho sinh có điều kiện. Gần đây, các mô hình chỉ decoder đã trở nên phổ biến hơn một phần do hiệu quả dữ liệu của chúng: toàn bộ ngân sách tham số được phân bổ cho decoder và không bị "lãng phí" cho encoder, với lợi thế bổ sung là tất cả các tham số được huấn luyện với tất cả các token dữ liệu. Mặc dù việc duy trì một encoder bên ngoài có thể có vẻ lãng phí, chúng tôi ủng hộ kiến trúc encoder-decoder: nó phù hợp hơn để tính toán trước và cache các biểu diễn ngữ cảnh. Thật vậy, chỉ các vector đầu ra encoder cần được lưu trữ, trái ngược với các trạng thái trung gian qua tất cả các lớp self-attention của decoder (KV và JIT-KV cache).

Trong những gì theo sau, chúng tôi đề cập đến một mô hình như được cấu thành từ một encoder E: V^|ngữ cảnh| → ℝ^{d|ngữ cảnh|}, nhận đầu vào ngữ cảnh và xuất ra các biểu diễn cấp token có kích thước d, và một decoder D: V^m × ℝ^{d|ngữ cảnh|} → Δ^|V|, nhận đầu vào truy vấn và các mã hóa ngữ cảnh và xuất ra câu trả lời trong simplex có kích thước |V|. Cụ thể hơn, decoder D tự động hồi quy xuất ra các tham số của phân phối categorical trên từ vựng.

Vì mục đích hiệu quả tham số và để tận dụng các LLM state-of-the-art đã được huấn luyện trước, chúng tôi bắt đầu từ một mô hình chỉ decoder hiện có và tăng cường nó với các lớp cross-attention mới xen kẽ giữa các lớp self-attention hiện có, như minh họa trong Hình 4. Chúng tôi xem xét hai chiến lược để triển khai encoder E. Chiến lược đầu tiên huấn luyện một encoder nhỏ, tức là E nhỏ hơn D hàng bậc độ lớn. Chiến lược thứ hai sử dụng decoder như encoder, tức là mô hình chỉ decoder đông lạnh được sử dụng nguyên trạng làm encoder (E := D). Cụ thể hơn, chúng tôi sử dụng làm mã hóa các biểu diễn được trích xuất từ D đã được huấn luyện trước tại lớp cuối cùng trước head mô hình hóa ngôn ngữ.

Việc chọn một cách tiếp cận này hay cách khác phụ thuộc vào các cân nhắc thực tế. Nếu caching có thể và các biểu diễn ngữ cảnh có thể được tính toán offline, thì việc sử dụng decoder làm encoder là ưu tiên hơn vì tính đơn giản tổng thể và hiệu quả tham số. Ngược lại, một encoder nhỏ sẽ làm cho việc xử lý ngữ cảnh just-in-time ít tốn kém hơn đáng kể.

Cả hai cách tiếp cận đều kế thừa lợi thế của các mô hình chỉ decoder đã được huấn luyện trong khi hưởng lợi từ việc sử dụng encoder trong quá trình suy luận. Đặc biệt, thông tin ngữ cảnh có thể được cache hiệu quả vì chỉ đầu ra tại lớp trên cùng của E cần được lưu trữ thay vì toàn bộ tập hợp các trạng thái trung gian của D.

Cuối cùng, để cho phép điều kiện hóa ngữ cảnh, chúng tôi huấn luyện độc quyền các module mới được thêm vào: các lớp cross-attention trong cả hai thiết lập và encoder nhỏ trong thiết lập yêu cầu nó. Decoder cơ sở được giữ đông lạnh trong cả hai thiết lập (ngay cả khi nó hoạt động như một encoder). Do đó, quy trình huấn luyện của chúng tôi không ảnh hưởng đến các tham số gốc của D, vẫn có thể được sử dụng như một mô hình ngôn ngữ đa mục đích khi các lớp bổ sung được loại bỏ.

4 Thiết lập Thí nghiệm

Chúng tôi tập trung vào nhiệm vụ question-answering và huấn luyện trên sự kết hợp của các dataset trong đó có sẵn bộ ba ngữ cảnh, truy vấn và câu trả lời. QA là một bệ thử lý tưởng cho các cách tiếp cận của chúng tôi vì nó yêu cầu tra cứu hiệu quả thông tin bên ngoài và tính toán nó tại thời điểm sinh. Nói cách khác, QA gần như cô lập khả năng "đọc" ngữ cảnh của mô hình, đó là điều chúng tôi cần kiểm tra.

4.1 Dataset Huấn luyện

Chúng tôi xây dựng dataset huấn luyện bằng cách chuẩn hóa và gộp chung các phân vùng huấn luyện của năm dataset có sẵn công khai và đa dạng (open-domain, multi-hop, conversational): NATURAL QUESTIONS (NQ) (Kwiatkowski et al., 2019), HOTPOTQA (Yang et al., 2018), TOPIOCQA (Adlakha et al., 2022), MS MARCO (Bajaj et al., 2016), và SQUAD-V2 (Rajpurkar et al., 2018). Các ví dụ từ dataset huấn luyện kết quả chứa truy vấn (câu hỏi ngôn ngữ tự nhiên), câu trả lời (đầu ra mong đợi), và một hoặc nhiều ngữ cảnh (ví dụ: bài viết knowledge base), trong đó ít nhất một trong số chúng chứa câu trả lời cho truy vấn. Chúng tôi đề cập đến ngữ cảnh này như ngữ cảnh tham khảo. Chúng tôi sử dụng tập validation để lựa chọn mô hình và điều chỉnh siêu tham số. Đối với các dataset có tập validation nhưng không có phân vùng test, chúng tôi sử dụng tập validation để test và tạo tập validation bằng cách giữ lại 10% các mẫu huấn luyện được chọn ngẫu nhiên. Chúng tôi áp dụng bước lọc thêm trong dữ liệu huấn luyện và loại bỏ các ví dụ có ngữ cảnh dài hơn 6.000 token, tương ứng với ít hơn 5% các mẫu.

Đánh giá được thực hiện trên các phân vùng test kết quả của NATURAL QUESTIONS, HOTPOTQA và TOPIOCQA. Chi tiết thêm và thống kê của các dataset chúng tôi xem xét có thể được tìm thấy trong Phụ lục C.

4.2 Nhiệm vụ Phụ trợ

Ngoài việc huấn luyện trên các nhiệm vụ QA chính, chúng tôi tối ưu hóa các mô hình trên các nhiệm vụ lặp lại ngữ cảnh, như mô tả dưới đây. Lợi thế của việc định nghĩa các nhiệm vụ phụ trợ như vậy có hai mặt. Một mặt, chúng cho phép chúng tôi tối ưu hóa likelihood của tất cả các token có sẵn, ngay cả những token được sử dụng làm đầu vào cho encoder. Mặt khác, chúng giúp tránh các giải pháp không tối ưu trong đó các lớp cross-attention hoạt động như các toán tử đồng nhất đơn giản. Thật vậy, các mô hình của chúng tôi có thể học cách bỏ qua ngữ cảnh nhưng không thể làm như vậy khi được giao nhiệm vụ lặp lại ngữ cảnh.

Trong thực tế, chúng tôi sử dụng các token dành riêng để hướng dẫn mô hình lặp lại ngữ cảnh, hoặc nguyên trạng hoặc bằng cách infilling (Bavarian et al., 2022), tức là trả về nó theo thứ tự prefix-suffix-middle hoặc suffix-prefix-middle, như được thực hiện bởi Li et al. (2023b); Lozhkov et al. (2024). Các nhiệm vụ như vậy giới thiệu tính biến thiên mới, khi mô hình học cách copy-paste ngữ cảnh và tìm và thay thế các đoạn bị thiếu, dẫn đến cải thiện hiệu suất trong thiết lập huấn luyện multi-epoch. Lưu ý rằng chúng tôi huấn luyện các mô hình trên question-answering và lặp lại ngữ cảnh trên mọi mẫu huấn luyện. Chúng tôi đặt một epoch huấn luyện tương ứng với hai lần đi qua dataset huấn luyện, trong đó chúng tôi thực hiện nhiệm vụ chính hoặc phụ trong mỗi lần đi qua, như minh họa trong Hình 5.

[Hình 5: Chiến lược huấn luyện đa nhiệm vụ. Trong một epoch, mỗi ví dụ được trình bày cho mô hình hai lần. Trong vòng đầu tiên, mô hình dự đoán câu trả lời có điều kiện dựa trên ngữ cảnh và truy vấn. Trong lần xuất hiện thứ hai của một ví dụ trong epoch, chúng tôi huấn luyện mô hình để lặp lại hoặc infill ngữ cảnh.]

4.3 Chi tiết Triển khai

Chúng tôi dựa vào LLAMA 2 (Touvron et al., 2023) đã được huấn luyện trước có sẵn công khai để định nghĩa các biến thể của XC-LLAMA. Cụ thể, tất cả các đánh giá thực nghiệm của chúng tôi sử dụng phiên bản 7 tỷ tham số của LLAMA 2. Đối với biến thể của XC-LLAMA trong đó một encoder chuyên dụng được huấn luyện (được gọi là XC-LLAMA ENC từ giờ), chúng tôi fine-tune làm encoder một LONGFORMER (Beltagy et al., 2020), đây là một BERT (Devlin et al., 2018) khoảng 125M tham số, được huấn luyện trước trên các tài liệu tương đối dài sử dụng self-attention hiệu quả. Tuy nhiên, chúng tôi lưu ý rằng LONGFORMER có sẵn công khai dài nhất chấp nhận đầu vào tối đa 4.096 token, ngắn hơn độ dài tối đa 6.000 của chúng tôi. Sau đó chúng tôi tăng độ dài đầu vào tối đa của mô hình bằng cách lặp lại các embedding vị trí bắt đầu. Sau khi fine-tuning, mô hình xử lý các ngữ cảnh dài hơn 4.096 token mà không có vấn đề đáng chú ý.

Về việc thêm các lớp cross-attention vào XC-LLAMA, chúng tôi giới thiệu một lớp cross-attention cứ mỗi vài lớp self-attention của transformer. Đặc biệt, chúng tôi thấy cấu hình 5-6 (tức là chèn năm lớp cross-attention trong khi bỏ qua sáu lớp self-attention) hoạt động nhất quán tốt. Do đó chúng tôi sử dụng cấu hình này trong suốt các đánh giá. Chúng tôi nhấn mạnh rằng chúng tôi cố gắng hiệu quả tham số và giữ số lượng tham số của các module được thêm dưới 10% số lượng tham số của D. Huấn luyện được thực hiện với optimizer ADAMW (Loshchilov và Hutter, 2017) với batch size 256 và trong 40.000 bước (tương đương 4 epoch huấn luyện). Chúng tôi sử dụng bộ lập lịch learning rate tuyến tính với giai đoạn warm-up và sao cho learning rate đạt 0 ở bước huấn luyện cuối cùng. Danh sách đầy đủ các giá trị siêu tham số được hiển thị trong Bảng 9 trong Phụ lục D.

4.4 Metric và Đánh giá

Chúng tôi so sánh các mô hình của mình với các phương pháp ICL để sinh câu trả lời có điều kiện trên ngữ cảnh và truy vấn. Chúng tôi lưu ý rằng các ngữ cảnh trong các trường hợp chúng tôi xem xét có tỷ lệ tín hiệu trên nhiễu tương đối thấp, vì hầu hết các token liên quan đến câu trả lời nhưng hoàn toàn không có liên quan. Trong một số tình huống cực đoan hơn, câu hỏi được đặt ra không thể được trả lời từ ngữ cảnh, và các mô hình được mong đợi (và được huấn luyện hoặc prompted để) chỉ ra rằng không thể trả lời dựa trên ngữ cảnh được cung cấp.

Chúng tôi sử dụng cùng metric và thiết lập đánh giá được mô tả bởi Adlakha et al. (2023) - chẳng hạn như F1 SCORE, RECALL, METEOR và ROUGE L - nhưng giữ F1 làm metric trọng tâm. Ngoài ra, chúng tôi đánh giá BERTSCORE (Zhang et al., 2019) được đo giữa dự đoán và câu trả lời ground-truth.

5 Kết quả

5.1 So sánh với các phương pháp hiện có

Đầu tiên chúng tôi so sánh phương pháp của mình với các cách tiếp cận hiện có cho sinh có điều kiện. Baseline chính của chúng tôi là ICL, tức là cung cấp ngữ cảnh như một phần của prompt. Cụ thể hơn, chúng tôi báo cáo kết quả baseline cho LLAMA 2-CHAT, mà chúng tôi thấy hoạt động tốt hơn LLAMA 2 cơ sở đã được huấn luyện trước. Chúng tôi cũng báo cáo kết quả cho GPT-3.5-TURBO. Đối với các baseline ICL này, chúng tôi chọn các template prompt dựa trên chất lượng câu trả lời được sinh trên dữ liệu validation mẫu (xem Phụ lục D để biết chi tiết). Cuối cùng, chúng tôi báo cáo kết quả của FUSION-IN-DECODER (FiD) (Izacard và Grave, 2021), một mô hình sinh có điều kiện dựa trên T5 (Raffel et al., 2020), liên tục chứng minh là state-of-the-art trên các nhiệm vụ QA (Borgeaud et al., 2022; Wang et al., 2023a; Adlakha et al., 2023). Không giống như backbone chỉ decoder của các mô hình chúng tôi, FiD có lẽ không còn là mô hình đa mục đích, vì tất cả các tham số của nó được fine-tune để thực hiện QA. Quan trọng hơn, như thảo luận sâu hơn trong Phần 5.2, việc tiền xử lý và cache các biểu diễn ngữ cảnh không phải là tùy chọn cho FiD vì nó yêu cầu biết câu hỏi tại thời điểm mã hóa. Tuy nhiên, chúng tôi giới thiệu FiD như một tham chiếu, để kiểm tra vị trí của các mô hình chúng tôi so với các mô hình chuyên biệt QA đã được thiết lập.

Kết quả được trình bày trong Bảng 1. Trên các dataset được xem xét, cross-attending vào các ngữ cảnh (XC-LLAMA hoặc XC-LLAMA ENC) cải thiện đáng kể hiệu suất so với prompting (LLAMA 2-CHAT). Khoảng cách thay đổi tùy thuộc vào dataset. Chúng tôi đoán rằng điều này do phương sai cao được gây ra bởi prompting, mặc dù có thể tồn tại prompt tối ưu cho mỗi dataset để giúp thu hẹp khoảng cách này. Do đó, các cách tiếp cận không dựa vào prompt cung cấp lợi thế là có thể áp dụng rộng rãi hơn và do đó thực tế hơn. Chúng tôi cũng lưu ý rằng ngay cả trong thiết lập mà decoder được sử dụng làm encoder, cross-attending vào ngữ cảnh vẫn mang lại hiệu suất tốt hơn ICL, bất kể decoder cơ sở chúng tôi so sánh. Điều này cho thấy rằng các lớp cross-attention được huấn luyện bù đắp cho tính không tối ưu tiềm ẩn vì các biểu diễn encoder không được huấn luyện rõ ràng cho nhiệm vụ chúng tôi đánh giá. Tập hợp kết quả rộng hơn bao gồm nhiều metric và mô hình hơn có thể được tìm thấy trong Bảng 6.

5.2 Cân bằng độ chính xác với hiệu quả

Kết quả trong phần trước cho thấy việc thêm và fine-tune các tham số chuyên dụng cho điều kiện hóa ngữ cảnh cải thiện hiệu suất so với prompting. Dựa trên quan sát này, trong phần này, chúng tôi mở rộng đánh giá để xem xét các cách tiếp cận thay thế tận dụng một số lượng nhỏ tham số bổ sung cho phép các mô hình điều kiện hóa sinh dựa trên ngữ cảnh tham khảo. Ngoài hiệu suất dự đoán, giờ chúng tôi cũng tập trung vào hiệu quả tính toán; cụ thể, chúng tôi đánh giá mức độ phù hợp của các mô hình khác nhau để tiền xử lý và cache các biểu diễn ngữ cảnh.

Do đó chúng tôi fine-tune các adapter LORA (Hu et al., 2021) áp dụng cho cùng decoder LLAMA 2 chúng tôi sử dụng cho XC-LLAMA. Nói cách khác, chúng tôi fine-tune baseline ICL từ Phần 5 để kiểm soát ảnh hưởng của prompting trong hiệu suất mô hình. Như quan sát trong Bảng 2, fine-tuning như vậy cải thiện đáng kể độ chính xác QA so với các baseline ICL được báo cáo trong Bảng 1. Tuy nhiên, mặc dù cho phép cải thiện hiệu suất dự đoán, việc dựa vào điều chỉnh mô hình kiểu LORA vẫn cần thiết lưu trữ tất cả các trạng thái KV trong mọi lớp để cache thông tin ngữ cảnh, phát sinh chi phí đáng kể. Ngược lại, các mô hình có encoder yêu cầu cache các trạng thái ẩn chỉ của lớp cuối cùng của chúng. Nói cách khác, các biến thể XC-LLAMA giảm đáng kể dấu chân caching đơn giản vì chúng yêu cầu lưu trữ chỉ các trạng thái ẩn cuối cùng của encoder. XC-LLAMA ENC giảm yêu cầu không gian hơn nữa, vì các biểu diễn của LONGFORMER có chiều thấp hơn so với LLAMA 2.

Giảm kích thước cache mang ý nghĩa thực tế đáng kể, đặc biệt trong việc giảm dấu chân bộ nhớ của corpus được tiền xử lý theo hàng bậc độ lớn. Điều này đặc biệt quan trọng khi lưu trữ các biểu diễn được tiền xử lý của các dataset khổng lồ như toàn bộ Wikipedia. Ngoài ra, việc giảm kích thước cache dẫn đến tiết kiệm runtime bằng cách giảm chi phí truyền thông, vì lượng thông tin được chuyển từ đĩa sang thiết bị được giảm đáng kể. Cuối cùng, việc thu nhỏ kích thước cache giải phóng bộ nhớ thiết bị tại thời điểm suy luận, cho phép sinh dài hơn hoặc batch size lớn hơn cho suy luận theo batch. Xem Phụ lục A để biết chi tiết.

Chúng tôi cung cấp góc nhìn kết hợp của hai thành phần hiệu suất đó - dự đoán và tính toán - trong Hình 1, trong đó chúng tôi vẽ biểu đồ F1 và BERTSCORE tổng hợp qua các dataset so với lượng cache trên mỗi token ngữ cảnh được yêu cầu bởi các mô hình. Lưu ý rằng các mô hình gần góc trên-phải hơn được ưu tiên vì chúng có độ chính xác cao với chi phí caching thấp. Trong khi không có phương pháp nào tối ưu cho cả hai tiêu chí, tập Pareto bao gồm: ở một đầu, các mô hình ICL được fine-tune với LORA cung cấp BERTSCORE tổng hợp cao hơn một chút nhưng yêu cầu không gian caching đáng kể; ở đầu kia, các mô hình có encoder hy sinh nhỏ về độ chính xác dự đoán trong khi giảm đáng kể dấu chân bộ nhớ. Chúng tôi cũng lưu ý rằng có khoảng cách giữa XC-LLAMA và XC-LLAMA ENC, và các tham số bổ sung được giới thiệu bởi XC-LLAMA ENC mang lại tăng cường độ chính xác và cải thiện hiệu quả không gian. Lưu ý rằng chúng tôi xem xét biến thể ICL bổ sung là LLAMA 2-ICL-JIT-KV, vì nó thực hiện phép chiếu KV just-in-time của các trạng thái ẩn được cache, đánh đổi thời gian lấy không gian.

Kết quả QA chi tiết được báo cáo trong Bảng 2. Như đã đề cập trước đó, các mô hình của chúng tôi phát sinh giảm nhẹ độ chính xác dự đoán nhưng đạt được tiết kiệm không gian đáng kể, điều này tỏ ra có lợi trong nhiều tình huống thực tế khác nhau. Để có cái nhìn tổng quan toàn diện, vui lòng tham khảo kết quả đầy đủ trong Bảng 6.

5.3 Mã hóa ngữ cảnh phụ thuộc câu hỏi

Trong số các baseline chúng tôi xem xét trong công trình này, những baseline dựa vào kiến trúc chỉ decoder cho mô hình hóa ngôn ngữ giữ lợi thế cơ bản so với các mô hình encoder-decoder: chúng có quyền truy cập vào câu hỏi trong khi tính toán các biểu diễn ngữ cảnh, cho phép các biểu diễn được sinh ra được điều chỉnh cho câu hỏi cụ thể. Thật vậy, trong các thiết lập encoder-decoder phổ biến không cache như FiD (Izacard và Grave, 2021), các ngữ cảnh được đưa vào encoder cùng với các câu hỏi.

Để kiểm tra tác động của việc có quyền truy cập vào câu hỏi trong khi mã hóa ngữ cảnh đối với hiệu suất của mô hình chúng tôi, chúng tôi thực hiện các thí nghiệm với một biến thể của XC-LLAMA ENC trong đó câu hỏi được thêm vào đầu đầu vào của encoder. Bảng 3 tóm tắt kết quả của các thí nghiệm này được thực hiện trên NQ trong đó cột có tên Question-in-context chỉ ra liệu encoder có quyền truy cập vào câu hỏi trong khi sinh các biểu diễn ngữ cảnh hay không. Chúng tôi quan sát cải thiện hiệu suất, gần như thu hẹp khoảng cách với baseline LoRA cache lớn. Quan sát này cho thấy rằng các biểu diễn ngữ cảnh phụ thuộc câu hỏi có thể được tích hợp vào các kiến trúc encoder-decoder có thể cache, cung cấp điều tốt nhất của cả hai thế giới: cache ngữ cảnh nhẹ và độ chính xác dự đoán cao.

6 Công trình liên quan

Decoder làm encoder. Việc tái sử dụng các decoder đã được huấn luyện trước đang trở thành cách tiếp cận phổ biến để tận dụng các mô hình ngôn ngữ mạnh mẽ cho các ứng dụng khác ngoài mô hình hóa sinh. Ví dụ, GRIT (Muennighoff et al., 2024) chuyển đổi decoder nhân quả đã được huấn luyện trước thành encoder hai chiều, tạo ra các embedding cấp câu trong khi duy trì khả năng thực hiện sinh tự động hồi quy văn bản. Tuy nhiên, không giống như các mô hình chúng tôi xem xét, việc chuyển đổi này yêu cầu fine-tune tất cả các tham số mô hình thay vì các tham số bổ sung. Các cách tiếp cận hiệu quả tham số để biến decoder thành encoder cũng được đề xuất, chẳng hạn như trong (Meng et al., 2024) và (BehnamGhader et al., 2024), trong đó decoder MISTRAL đã được huấn luyện trước (Jiang et al., 2023) được fine-tune trong thiết lập contrastive sử dụng adapter LORA để tạo ra các biểu diễn cấp câu cho retrieval. Gần với phương pháp của chúng tôi hơn là CODET5+ (Wang et al., 2023d), cũng định nghĩa các kiến trúc encoder-decoder thay vì biến decoder thành encoder câu. Tương tự như một trong các biến thể của chúng tôi (XC-LLAMA), nó được lắp ráp từ hai decoder đã được huấn luyện trước, một được sử dụng làm encoder và cái khác làm decoder và được liên kết với một vài phép toán cross-attention. Tuy nhiên, CODET5+ yêu cầu fine-tune toàn bộ encoder tương đối lớn. Chúng tôi cho thấy rằng một decoder đã được huấn luyện trước tốt có các biểu diễn đủ tốt, nhưng người ta có thể cải thiện nó một cách hiệu quả chỉ bằng cách sử dụng một encoder có thể huấn luyện rất nhỏ.

Điều kiện hóa mà không cần prompt. Một hướng nghiên cứu gần đây đã tập trung vào việc kiểm soát sinh của mô hình bằng cách can thiệp trực tiếp vào các tham số của nó (Wang et al., 2023c; Zhang et al., 2023; Wang et al., 2023b), đặc biệt, để giới thiệu hoặc xóa kiến thức sau huấn luyện. Các cách tiếp cận như vậy thường yêu cầu truy cập và viết lại các tham số nội bộ của mô hình ngôn ngữ đã được huấn luyện trước và không phù hợp với các ngữ cảnh thay đổi thường xuyên như trong thiết lập Question-Answer.

Suy luận hiệu quả. Tồn tại một số phương pháp để cải thiện tốc độ suy luận và dấu chân bộ nhớ. Một cách tiếp cận là giảm độ chính xác số hoặc quantize trọng số mô hình và/hoặc dữ liệu, điều này đã được chứng minh là bảo tồn độ chính xác mô hình chỉ với 8 bit trên mỗi trọng số (Dettmers et al., 2022), hoặc thậm chí độ chính xác thấp hơn với 4, 3 hoặc thậm chí 2 bit trên mỗi trọng số (Frantar et al., 2023). Ngoài ra, cache key-query có thể được nén (Ainslie et al., 2023; Nawrot et al., 2024), mặc dù làm như vậy yêu cầu fine-tuning. Hơn nữa, các phương pháp nén cache dọc theo trục thời gian đã được đề xuất (Bulatov et al., 2022; Ge et al., 2024; Chevalier et al., 2023). Tuy nhiên, tỷ lệ nén cho những trường hợp đó không thực sự có thể so sánh với việc có một encoder riêng biệt và cross-attending vào các đầu ra của nó. Ví dụ, Ge et al. (2024) báo cáo tỷ lệ nén tối đa 4 (trái ngược với ít nhất 32 như chúng tôi tuyên bố), và điều đó đi kèm với chi phí vì việc nén dọc theo chiều thời gian có khả năng loại bỏ nội dung liên quan, như thảo luận bởi Chevalier et al. (2023). Với kiến trúc tương tự như của chúng tôi, (Yen et al., 2024) báo cáo lợi ích của việc có encoder riêng biệt. Mặc dù chúng tôi cho thấy rằng việc áp dụng các toán tử cross-attention sau chỉ một vài lớp self-attention là đủ, và các biểu diễn của decoder đã được huấn luyện trước hoạt động khá tốt mà không cần huấn luyện thêm. Cuối cùng, việc sử dụng Flash Attention (Dao et al., 2022) dẫn đến tiết kiệm đáng kể cho việc xử lý ngữ cảnh just-in-time. Các phương pháp này bổ sung trực giao với những gì chúng tôi trình bày trong bài báo này và có thể được kết hợp với XC-LLAMA.

7 Kết luận

Chúng tôi giới thiệu XC-LLAMA như một cách tiếp cận để chuyển đổi mô hình ngôn ngữ chỉ decoder đã được huấn luyện trước thành kiến trúc encoder-decoder có thể điều kiện hóa sinh dựa trên cả đầu vào encoder và truy vấn decoder. Điều này được thực hiện bằng cách tích hợp các lớp cross-attention xen kẽ giữa các lớp self-attention hiện có của decoder đã được huấn luyện trước. Chúng tôi mô tả hai cách tiếp cận để định nghĩa encoder: sử dụng bản sao của decoder hoặc giới thiệu encoder hai chiều có thể huấn luyện nhưng nhỏ.

Kiến trúc được đề xuất cho phép giảm không gian caching với hệ số vượt quá 300. Khi được đánh giá trong thiết lập QA, chúng tôi quan sát độ chính xác dự đoán cao hơn những gì các cách tiếp cận ICL tiêu chuẩn đạt được, hoặc với LLAMA 2 hoặc GPT-3.5 TURBO. Ngoài ra, chúng tôi quan sát mức độ chính xác gần như ngang bằng với các mô hình prompted được fine-tune tốn kém cache, cung cấp thay thế thân thiện với cache hơn cho các mô hình ngôn ngữ prompted có tính thực tế cao. Chúng tôi lưu ý rằng chiến lược encoder-decoder để xử lý ngữ cảnh riêng biệt cung cấp lợi thế bổ sung là giải phóng không gian prompt, hiện được sử dụng độc quyền cho tương tác người dùng. Các hạn chế được xác định được thảo luận trong Phần 8.

8 Hạn chế

Hầu hết các mô hình được thảo luận trong bài báo này có kết quả ấn tượng trong thiết lập QA. Tuy nhiên, kinh nghiệm làm việc với các mô hình này của chúng tôi tiết lộ những hạn chế chủ yếu xuất phát từ sự phụ thuộc của chúng vào mô hình ngôn ngữ cơ bản. Do đó, các phương pháp này kế thừa những khuyết điểm tiềm ẩn của mô hình ngôn ngữ mà chúng xây dựng dựa trên.

Ví dụ, các mô hình ngôn ngữ lớn điển hình được huấn luyện trên lượng lớn văn bản, có khả năng bao gồm thông tin liên quan đến câu hỏi trong các benchmark QA có sẵn công khai. Trong khi việc huấn luyện như vậy trên dữ liệu liên quan có thể cung cấp lối tắt cho các mô hình để trả lời đúng câu hỏi trong ngữ cảnh mà chúng đã "ghi nhớ" trong quá trình huấn luyện, nó cũng có thể gây ra lỗi: mô hình có thể "nhớ" thông tin liên quan nhưng không chính xác liên quan đến truy vấn cụ thể. Nói đơn giản hơn, các cách tiếp cận ICL dựa trên prompting và có khả năng các mô hình của chúng tôi có thể bỏ qua ngữ cảnh và chỉ dựa vào bộ nhớ để sinh tiếp. Điều này là không mong muốn vì chúng tôi nhằm đảm bảo rằng các mô hình tính toán chính xác ngữ cảnh tham khảo thay vì của quá trình huấn luyện trước. Các mô hình fine-tuning giải quyết vấn đề đã đề cập ở trên bằng cách khuyến khích phụ thuộc vào ngữ cảnh được cung cấp, như chúng tôi làm rõ ràng với các nhiệm vụ phụ trợ lặp lại và điền ngữ cảnh. Tuy nhiên, nó cũng chuyên biệt hóa trong dữ liệu fine-tuning đến mức có thể làm tổn hại khả năng tổng quát hóa của các dataset khác biệt so với những gì được quan sát trong quá trình fine-tuning. Chúng tôi ước tính mức độ các mô hình biết câu trả lời sau huấn luyện trước trong Bảng 7.

Dataset Chưa thấy. Để kiểm tra khả năng tổng quát hóa của các mô hình trên dữ liệu out-of-distribution, không phân phối đồng nhất liên quan đến fine-tuning, chúng tôi sử dụng phiên bản pre-release của RepLiQA (Monteiro et al., 2024). Đây là dataset test được sắp xếp chứa 16.290 tài liệu tham khảo, với khoảng 5 cặp question-answer trên mỗi tài liệu. Các tài liệu được viết bởi các annotator con người, những người viết về các tình huống tưởng tượng, mỗi cái có một số tiểu mục với tiêu đề và phụ đề. Quan trọng là, cho rằng các tài liệu tham khảo trong dataset này chứa thông tin hư cấu hoặc không đúng, chúng ta có thể hợp lý giả định rằng không mô hình nào có thể dựa vào hoặc bị đánh lừa bởi bộ nhớ của dữ liệu huấn luyện. Ngoài ra, các mẫu trong dataset này thể hiện phong cách độc đáo có thể khác biệt so với dữ liệu fine-tuning.

Khi được đánh giá trong tình huống out-of-distribution thách thức hơn này, độ chính xác của tất cả các mô hình trải qua sự suy giảm đáng kể, như báo cáo trong Hình 6, trong đó chúng tôi so sánh điểm số QA cho tất cả các mô hình qua tất cả các dataset, bao gồm phiên bản pre-release của RepLiQA được gắn nhãn là Unseen. Chúng tôi đưa ra giả thuyết rằng, ngoài việc các mô hình không thể dựa vào bộ nhớ để trả lời truy vấn, yếu tố chính dẫn đến suy giảm độ chính xác là sự khác biệt giữa dataset test và dữ liệu fine-tuning. Sự khác biệt này có thể phát sinh từ các biến thể trong phong cách viết, độ dài tài liệu, tỷ lệ tín hiệu trên nhiễu, và sự hiện diện của nội dung gây mất tập trung liên quan đến câu hỏi nhưng không hữu ích cho câu trả lời của nó, cùng những yếu tố khác. Việc xác định các nguồn lỗi chính xác và tăng cường tính robust của các mô hình có điều kiện ngữ cảnh là những hướng nghiên cứu hứa hẹn cho tương lai.

Tuyên bố Đạo đức

Chúng tôi thừa nhận tác động môi trường của tài nguyên tính toán, bao gồm dấu chân carbon liên quan đến lưu trữ và xử lý dữ liệu. Bằng cách tập trung vào sinh có điều kiện thông qua caching với dấu chân bộ nhớ nhỏ, chúng tôi nhằm giảm thiểu tiêu thụ năng lượng và thúc đẩy tính bền vững trong các thực hành tính toán.

Trong khi giảm cache có thể dẫn đến cải thiện tốc độ, chúng tôi nhận ra tầm quan trọng của việc bổ sung triển khai các biện pháp bảo vệ để đảm bảo sử dụng có trách nhiệm các mô hình tính toán. Điều này đòi hỏi giảm thiểu bias, thúc đẩy công bằng và bảo tồn quyền riêng tư, do đó thúc đẩy triển khai có đạo đức của công nghệ.

Lời cảm ơn

Chúng tôi muốn cảm ơn Christian Hudon vì hỗ trợ kỹ thuật và cho phép mở rộng quy mô các thí nghiệm của chúng tôi. Chúng tôi cũng cảm ơn Siva Reddy vì những thảo luận sâu sắc giúp định hình công trình này.

Tài liệu tham khảo

[Phần tài liệu tham khảo được dịch tương tự với format và nội dung như bản gốc, bao gồm tất cả các trích dẫn từ Vaibhav Adlakha đến Andy Zou et al.]

--- TRANG 14 ---
A Chi phí thực tế của caching

Mặc dù độ phức tạp tuyến tính theo độ dài ngữ cảnh có thể đạt được trong khi duy trì chất lượng ngữ cảnh, các cân nhắc thực tế có thể ảnh hưởng tiêu cực đến thiết lập có cache.

Chi phí tải và truyền thông. Mặc dù chi phí lưu trữ dài hạn tương đối thấp, độ trễ tải và truyền thông tỷ lệ thuận với dấu chân không gian của nội dung được cache. Hai nút cổ chai chính ở đây là tải cache từ đĩa (nếu sử dụng lưu trữ đĩa), và di chuyển cache đến bộ nhớ GPU. Nếu tổng lượng thông tin cần cache vừa với CPU RAM, có thể không cần lưu trữ chúng trên đĩa trong quá trình suy luận, cho phép lợi ích tốt hơn tuyến tính trên kích thước caching nhỏ hơn.

Phép toán bổ sung. Trong trường hợp LLAMA 2, JIT-KV caching yêu cầu cache nhỏ hơn một nửa so với KV - 254 kB thay vì 512 kB trên mỗi token - và do đó rõ ràng có lợi thế hơn về mặt tải và truyền thông. Tuy nhiên, các phép toán bổ sung được yêu cầu tại thời điểm suy luận để chuyển đổi các trạng thái ẩn thành key và value mà mô hình yêu cầu. Ở đây các phép toán bổ sung này hóa ra là các phép toán tuyến tính rẻ. Tương tự, các cách tiếp cận XC caching của chúng tôi yêu cầu rất ít lưu trữ (tức là 8 kB và 1.5 kB trên mỗi token), đây là chiến thắng rõ ràng về mặt tải và truyền thông. Tuy nhiên, các lớp bổ sung được thêm vào mô hình gốc trong Phần 3), điều này gây ra chi phí thời gian phải được tính đến trong tăng tốc tổng thể. Một lần nữa, các phép toán bổ sung này hóa ra là không đáng kể so với lợi ích của caching.

Chi phí chất lượng rõ ràng. KV và JIT-KV caching đều trả về chính xác cùng câu trả lời như mô hình không cache tương ứng: các chiến lược caching này không phát sinh chi phí (rõ ràng) về chất lượng câu trả lời so với mô hình không cache đó. Ngược lại, chiến lược XC caching của chúng tôi không đảm bảo điều này: phải xem xét sự cân bằng giữa chi phí và lợi ích.

Chi phí chất lượng ẩn. Tất cả các phương pháp caching được xem xét trong công trình này đều có cùng "chi phí cơ hội" ẩn: việc xử lý ngữ cảnh offline có nghĩa là cache kết quả độc lập với đặc điểm cụ thể của truy vấn. Tùy thuộc vào trường hợp sử dụng, điều này có thể dẫn đến mất mát chất lượng câu trả lời. Ví dụ, trong thiết lập QA, việc biết câu hỏi khi xử lý ngữ cảnh cho phép mô hình tập trung vào thông tin hữu ích để sinh câu trả lời. Có thể nói, điều này dẫn đến các biểu diễn ngữ cảnh tốt hơn vì nó giảm nguy cơ làm nhầm LLM với thông tin không liên quan.

B Timing cache loading

[Bảng 4 trình bày so sánh thời gian tải cache ngữ cảnh từ đĩa đến bộ nhớ thiết bị cho mỗi cách tiếp cận với các độ dài chuỗi khác nhau và kết quả cho thấy kích thước cache giảm do XC-cache làm giảm thời gian tải hai bậc độ lớn.]

C Dataset

[Phần này mô tả chi tiết về các dataset huấn luyện bao gồm Natural Questions, HotpotQA, TopiOCQA, MS MARCO và Squad V2 với thống kê về số lượng mẫu train/valid/test.]

D Tập hợp kết quả đầy đủ

[Bảng 6 chứa kết quả cho tất cả các phương pháp, metric và tất cả các dataset được tính đến trong đánh giá thực nghiệm.]

E Kết quả suy luận không có ngữ cảnh

[Chúng tôi đánh giá các LLM đã được huấn luyện trước trên tập test của các dataset mà không truyền ngữ cảnh để có cảm nhận tốt hơn về số lượng câu trả lời chúng đã biết mà không cần ICL.]

F Chi tiết thêm về baseline

[Phần này cung cấp chi tiết về cài đặt GPT, fine-tuning LoRA và chi tiết LLAMA 2-Chat bao gồm các template prompt được sử dụng.]

G Phân tích độ nhạy: vị trí của ngữ cảnh tham khảo

[Chúng tôi khám phá độ nhạy của các mô hình đối với vị trí của ngữ cảnh tham khảo trong ngữ cảnh đầy đủ và thấy rằng các mô hình được fine-tune ít bị bias đối với các phần nhất định của ngữ cảnh hơn.]

H Siêu tham số huấn luyện

[Bảng 9 báo cáo các siêu tham số huấn luyện chính được sử dụng cho XC-LLAMA bao gồm optimizer, learning rate, batch size và các cài đặt kiến trúc.]
