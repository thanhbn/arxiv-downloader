# 2404.15420.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/rag/2404.15420.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 676420 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================

--- TRANG 1 ---
XC-CACHE: Cross-Attending to Cached Context for Efficient LLM
Inference
JoÃ£o Monteiroâ€ ,3Ã‰tienne Marcotteâ€ ,1, Pierre-AndrÃ© NoÃ«lâ€ ,1, Valentina Zantedeschiâ€ ,1,
David VÃ¡zquez1, Nicolas Chapados1, Christopher Pal1,2, Perouz Taslakianâ€ ,1
1-ServiceNow Research
2-Quebec Artificial Intelligence Institute  
3-Autodesk â€“ CÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n táº¡i ServiceNow
â€ Nhá»¯ng ngÆ°á»i Ä‘Ã³ng gÃ³p chÃ­nh

TÃ³m táº¯t
CÃ¡c prompt thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘iá»u kiá»‡n hÃ³a viá»‡c sinh vÄƒn báº£n cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ giáº£i mÃ£ dá»±a trÃªn thÃ´ng tin tham kháº£o. Viá»‡c xá»­ lÃ½ ngá»¯ cáº£nh ngay táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³ khÃ´ng hiá»‡u quáº£ do chi phÃ­ báº­c hai cá»§a cÃ¡c phÃ©p toÃ¡n self-attention, vÃ  viá»‡c lÆ°u trá»¯ cache lÃ  mong muá»‘n. Tuy nhiÃªn, viá»‡c lÆ°u trá»¯ cache cÃ¡c tráº¡ng thÃ¡i transformer cÃ³ thá»ƒ dá»… dÃ ng yÃªu cáº§u gáº§n nhÆ° báº±ng khÃ´ng gian vá»›i cÃ¡c tham sá»‘ mÃ´ hÃ¬nh. Khi ngá»¯ cáº£nh phÃ¹ há»£p khÃ´ng Ä‘Æ°á»£c biáº¿t trÆ°á»›c, viá»‡c lÆ°u trá»¯ cache prompt cÃ³ thá»ƒ thÃ¡ch thá»©c. CÃ´ng trÃ¬nh nÃ y giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ nÃ y báº±ng cÃ¡ch giá»›i thiá»‡u cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« kiáº¿n trÃºc encoder-decoder, sá»­ dá»¥ng cross-attention Ä‘á»ƒ Ä‘iá»u kiá»‡n hÃ³a viá»‡c sinh vÄƒn báº£n dá»±a trÃªn vÄƒn báº£n tham kháº£o mÃ  khÃ´ng cáº§n prompt. Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i táº­n dá»¥ng cÃ¡c mÃ´ hÃ¬nh decoder-only Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  chá»‰ huáº¥n luyá»‡n má»™t sá»‘ lÆ°á»£ng nhá» cÃ¡c lá»›p Ä‘Æ°á»£c thÃªm vÃ o. ChÃºng tÃ´i sá»­ dá»¥ng Question-Answering (QA) nhÆ° má»™t bá»‡ thá»­ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng thá»±c hiá»‡n sinh cÃ³ Ä‘iá»u kiá»‡n cá»§a cÃ¡c mÃ´ hÃ¬nh vÃ  quan sÃ¡t tháº¥y chÃºng vÆ°á»£t trá»™i hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p suy luáº­n dá»±a trÃªn prompt, cÃ³ thá»ƒ so sÃ¡nh vá»›i cÃ¡c LLM Ä‘Æ°á»£c tinh chá»‰nh vá»›i prompt, vÃ  giáº£m Ä‘Ã¡ng ká»ƒ dáº¥u chÃ¢n khÃ´ng gian so vá»›i KV caching tiÃªu chuáº©n hai báº­c Ä‘á»™ lá»›n. Cá»¥ thá»ƒ, chÃºng tÃ´i giá»›i thiá»‡u XC-LLAMA chuyá»ƒn Ä‘á»•i LLAMA 2 Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c thÃ nh kiáº¿n trÃºc encoder-decoder báº±ng cÃ¡ch tÃ­ch há»£p cÃ¡c lá»›p cross-attention xen káº½ giá»¯a cÃ¡c lá»›p self-attention hiá»‡n cÃ³.

1 Giá»›i thiá»‡u
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) Ä‘Ã£ thÃºc Ä‘áº©y nhá»¯ng tiáº¿n bá»™ trong mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ vÃ  cho phÃ©p sáº£n xuáº¥t tá»± Ä‘á»™ng vÄƒn báº£n gáº§n nhÆ° giá»‘ng con ngÆ°á»i. Máº·c dÃ¹ cÃ³ tiáº¿n bá»™ áº¥n tÆ°á»£ng, nhá»¯ng thÃ¡ch thá»©c váº«n tá»“n táº¡i trong viá»‡c Ã¡p dá»¥ng LLM trong cÃ¡c thiáº¿t láº­p thá»±c táº¿ nhÆ° nguy cÆ¡ áº£o giÃ¡c (hay Ä‘Ãºng hÆ¡n lÃ  confabulation (Bottou vÃ  SchÃ¶lkopf, 2023; Millidge, 2023)) vÃ  ná»™i dung khÃ´ng thá»±c táº¿ (Li et al., 2023a; Xu et al., 2024) hoáº·c Ä‘á»™c háº¡i (Zou et al., 2023; Xhonneux et al., 2024) trong vÄƒn báº£n Ä‘Æ°á»£c sinh ra. HÆ¡n ná»¯a, khÃ´ng cÃ³ fine-tuning, viá»‡c Ä‘iá»u chá»‰nh cÃ¡c mÃ´ hÃ¬nh nÃ y Ä‘á»ƒ káº¿t há»£p thÃ´ng tin má»›i khÃ´ng cÃ³ trong dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a chÃºng lÃ  Ä‘iá»u Ä‘Ã¡ng ngáº¡c nhiÃªn khÃ³ khÄƒn (Luo et al., 2023; Kalajdzievski, 2024). Tháº­t váº­y, trong khi LLM cÃ³ thá»ƒ tráº£ lá»i cÃ¡c truy váº¥n vá» dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a chÃºng má»™t cÃ¡ch chÃ­nh xÃ¡c á»Ÿ cháº¿ Ä‘á»™ zero-shot (Petroni et al., 2019), cÃ¡c truy váº¥n liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng cÃ³ sáºµn táº¡i thá»i Ä‘iá»ƒm huáº¥n luyá»‡n thÆ°á»ng dáº«n Ä‘áº¿n khÃ´ng chÃ­nh xÃ¡c (Maynez et al., 2020; Ji et al., 2023) nhÆ° ngÆ°á»i ta mong Ä‘á»£i.

CÃ´ng trÃ¬nh nÃ y táº­p trung vÃ o viá»‡c Ä‘á»‹nh hÆ°á»›ng sinh vÄƒn báº£n LLM dá»±a trÃªn thÃ´ng tin ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p táº¡i thá»i Ä‘iá»ƒm suy luáº­n (HÃ¬nh 2(a)). CÃ¡ch tiáº¿p cáº­n phá»• biáº¿n nháº¥t Ä‘á»ƒ Ä‘iá»u kiá»‡n hÃ³a sinh vÄƒn báº£n mÃ´ hÃ¬nh cÃ³ láº½ lÃ  In-Context LearningÂ¹ (ICL) (Radford et al., 2019; Brown et al., 2020): ngÆ°á»i ta thÃªm ngá»¯ cáº£nh liÃªn quan vÃ o Ä‘áº§u prompt Ä‘á»ƒ sinh ra cÃ¢u tráº£ lá»i cÃ³ Ä‘iá»u kiá»‡n dá»±a trÃªn truy váº¥n vÃ  ngá»¯ cáº£nh káº¿t há»£p. Ká»¹ thuáº­t nÃ y lÃ  thÃ nh pháº§n cá»‘t lÃµi cá»§a cÃ¡c framework phá»• biáº¿n, cháº³ng háº¡n nhÆ° Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), vá»›i Ä‘áº·c Ä‘iá»ƒm lÃ  ngá»¯ cáº£nh liÃªn quan khÃ´ng Ä‘Æ°á»£c biáº¿t trÆ°á»›c, mÃ  pháº£i Ä‘Æ°á»£c truy xuáº¥t tá»« má»™t corpus (Ram et al., 2023).

Â¹Trong suá»‘t bÃ i bÃ¡o nÃ y, chÃºng tÃ´i sá»­ dá»¥ng in-context learning Ä‘á»ƒ Ä‘á» cáº­p Ä‘áº¿n thiáº¿t láº­p suy luáº­n dá»±a trÃªn prompt 0-shot trong Ä‘Ã³ thÃ´ng tin cáº§n thiáº¿t Ä‘á»ƒ giáº£i quyáº¿t nhiá»‡m vá»¥ Ä‘Æ°á»£c bao gá»“m trong prompt.

--- TRANG 2 ---
ChÆ°Æ¡ng 2: Quáº£ trá»©ng Phá»¥c sinh
biáº¿n máº¥t...
Soáº¡n má»™t bÃ i thÆ¡ ngáº¯n vá»
chá»§ Ä‘á» Ä‘Ã£ cho. Táº¥t cáº£ váº§n Ä‘iá»‡u...
(b)(a)
(c)
(d)
ngá»¯ cáº£nh ngá»¯ cáº£nh
truy váº¥n cÃ¢u tráº£ lá»i
cÃ¢u tráº£ lá»i
cÃ¢u tráº£ lá»i
cÃ¢u tráº£ lá»i truy váº¥n
ngá»¯ cáº£nh
truy váº¥n truy váº¥n cache
cache Giáº¥c mÆ¡ nÃ o mÃ  Harry Ä‘ang cÃ³
khi con Thá» Äƒn thá»‹t anh? Sá»± lÃªn vÃ  xuá»‘ng cá»§a
Äáº¿ cháº¿ VÃ­ dá»¥.
(cÃ´ng trÃ¬nh nÃ y)

HÃ¬nh 2: Suy luáº­n nhanh hÆ¡n trong mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ cÃ³ Ä‘iá»u kiá»‡n ngá»¯ cáº£nh. (a) Má»™t trÆ°á»ng há»£p sá»­ dá»¥ng trong Ä‘Ã³ truy váº¥n cá»§a ngÆ°á»i dÃ¹ng pháº£i Ä‘Æ°á»£c diá»…n giáº£i trong má»™t ngá»¯ cáº£nh nÃ o Ä‘Ã³ Ä‘á»ƒ sinh ra cÃ¢u tráº£ lá»i. CÃ´ng trÃ¬nh nÃ y táº­p trung vÃ o cÃ¡c trÆ°á»ng há»£p trong Ä‘Ã³ truy váº¥n vÃ  cÃ¢u tráº£ lá»i Ä‘á»u nhá» (nháº¹ ğŸ”¸), nhÆ°ng ngá»¯ cáº£nh láº¡i lá»›n (náº·ng ğŸ”º). Äá»™ phá»©c táº¡p thá»i gian LLM káº¿t quáº£ do Ä‘Ã³ lÃ  O(|ngá»¯ cáº£nh|Â²) (cháº­m ğŸ”´). (b) In-context learning (ICL) vÃ  retrieval augmented generation (RAG) lÃ  hai vÃ­ dá»¥ trong Ä‘Ã³ truy váº¥n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tra cá»©u ngá»¯ cáº£nh tá»« má»™t corpus há»¯u háº¡n. (c) Trong nhiá»u trÆ°á»ng há»£p, ngá»¯ cáº£nh cÃ³ thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ trÆ°á»›c thÃ nh má»™t cache cho phÃ©p suy luáº­n O(|ngá»¯ cáº£nh|(|truy váº¥n|+|cÃ¢u tráº£ lá»i|)) (nhanh ğŸŸ¢) trÃªn má»™t truy váº¥n cho trÆ°á»›c. (d) Corpus ngá»¯ cáº£nh há»¯u háº¡n cÃ³ thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ offline, cho phÃ©p thá»±c thi nhanh táº¡i thá»i Ä‘iá»ƒm suy luáº­n. VÃ¬ kÃ­ch thÆ°á»›c cache áº£nh hÆ°á»Ÿng Ä‘áº¿n chi phÃ­ lÆ°u trá»¯ vÃ  truyá»n thÃ´ng, chÃºng tÃ´i tÃ¬m kiáº¿m cÃ¡c mÃ´ hÃ¬nh yÃªu cáº§u cache nhá» hÆ¡n.

Máº·c dÃ¹ pháº§n nÃ o hiá»‡u quáº£ vÃ  Ä‘Æ¡n giáº£n, ICL, nhÆ° thÆ°á»ng Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i cÃ¡c kiáº¿n trÃºc chá»‰ giáº£i mÃ£, cÃ³ nhá»¯ng khuyáº¿t Ä‘iá»ƒm. Má»™t máº·t, viá»‡c sinh dá»±a trÃªn ICL Ä‘Æ°á»£c biáº¿t lÃ  cÃ³ phÆ°Æ¡ng sai cao liÃªn quan Ä‘áº¿n template prompt sao cho cÃ¡c Ä‘á»‹nh dáº¡ng prompt há»£p lá»‡ tÆ°Æ¡ng Ä‘Æ°Æ¡ng táº¡o ra káº¿t quáº£ khÃ¡c biá»‡t Ä‘Ã¡ng ká»ƒ (Chen et al., 2023). Máº·t khÃ¡c, ICL tá»‘n kÃ©m vá» thá»i gian vÃ  khÃ´ng gian. Viá»‡c xá»­ lÃ½ ngá»¯ cáº£nh ngay táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³ chá»‹u Ä‘á»™ phá»©c táº¡p báº­c hai vá» Ä‘á»™ dÃ i do cÃ¡c phÃ©p toÃ¡n self-attention (Vaswani et al., 2017). Thay tháº¿ lÃ  tiá»n xá»­ lÃ½ vÃ  lÆ°u trá»¯ cache cÃ¡c tráº¡ng thÃ¡i ná»™i bá»™ ngá»¯ cáº£nh (gá»i lÃ  cÃ¡c tráº¡ng thÃ¡i key-value hoáº·c KV) Ä‘á»ƒ tÄƒng tá»‘c suy luáº­n (HÃ¬nh 2(c)). Tuy nhiÃªn, Ä‘iá»u nÃ y cÃ³ thá»ƒ yÃªu cáº§u cÃ¹ng báº­c khÃ´ng gian nhÆ° chÃ­nh cÃ¡c tham sá»‘ mÃ´ hÃ¬nh (chÃºng tÃ´i Ä‘Æ°a ra chi tiáº¿t trong Pháº§n 2). CÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y Ä‘Ã£ giáº£m yÃªu cáº§u khÃ´ng gian cá»§a KV caching báº±ng cÃ¡ch láº¥y máº«u phá»¥ cÃ¡c tráº¡ng thÃ¡i (Xiao et al., 2023; Adnan et al., 2024), máº·c dÃ¹ vá»›i chi phÃ­ bá» qua ná»™i dung liÃªn quan.

Äá»ƒ vÆ°á»£t qua nhá»¯ng háº¡n cháº¿ nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t cÃ¡c thay tháº¿ cho ICL thá»±c hiá»‡n sinh cÃ³ Ä‘iá»u kiá»‡n mÃ  khÃ´ng tiÃªm thÃ´ng tin liÃªn quan vÃ o prompt (HÃ¬nh 2(a)), vÃ  tÃ¬m cÃ¡ch triá»ƒn khai cÃ¡c phÆ°Æ¡ng phÃ¡p cache nháº¹ nhÆ° minh há»a trong HÃ¬nh 2(d). CÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i gá»£i nhá»› Ä‘áº¿n cÃ¡c kiáº¿n trÃºc encoder-decoder, hiá»‡n cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  legacy, vÃ¬ nÃ³ dá»±a vÃ o cÃ¡c lá»›p cross-attention Ä‘á»ƒ Ä‘iá»u kiá»‡n hÃ³a viá»‡c sinh dá»±a trÃªn cÃ¡c mÃ£ hÃ³a ngá»¯ cáº£nh Ä‘Æ°á»£c tÃ­nh toÃ¡n trÆ°á»›c. Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i Ä‘á» xuáº¥t cross-context-cache (XC-CACHE), lÆ°u trá»¯ chá»‰ cÃ¡c Ä‘áº§u ra cá»§a cÃ¡c tráº¡ng thÃ¡i áº©n encoder vÃ  dá»±a vÃ o cross-attention Ä‘á»ƒ tiÃªu thá»¥ cache táº¡i thá»i Ä‘iá»ƒm suy luáº­n. ChÃºng tÃ´i cá»¥ thá»ƒ hÃ³a XC-CACHE thÃ´ng qua hai cÃ¡ch tiáº¿p cáº­n hiá»‡u quáº£ tham sá»‘ táº­n dá»¥ng cÃ¡c mÃ´ hÃ¬nh decoder-only Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  má»Ÿ rá»™ng chÃºng vá»›i má»™t encoder riÃªng Ä‘á»ƒ xá»­ lÃ½ ngá»¯ cáº£nh: má»™t cÃ¡ch tiáº¿p cáº­n sá»­ dá»¥ng decoder Ä‘Ã´ng láº¡nh lÃ m encoder (gá»i lÃ  XC-LLAMA), vÃ  cÃ¡ch khÃ¡c sá»­ dá»¥ng má»™t encoder hai chiá»u nhá» (gá»i lÃ  XC-LLAMA ENC). Quan trá»ng lÃ , cÃ¡c kiáº¿n trÃºc encoder-decoder cá»§a chÃºng tÃ´i phÃ¹ há»£p hÆ¡n Ä‘á»ƒ lÆ°u trá»¯ cache cÃ¡c tráº¡ng thÃ¡i ngá»¯ cáº£nh, yÃªu cáº§u Ã­t khÃ´ng gian hÆ¡n cÃ¡c Ä‘á»‘i tÃ¡c ICL hÃ ng báº­c Ä‘á»™ lá»›n. Khi cache ngá»¯ cáº£nh Ä‘Æ°á»£c kÃ­ch hoáº¡t, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh dáº«n Ä‘áº¿n hiá»‡u suáº¥t Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n, nhÆ°ng Ä‘Ã²i há»i dáº¥u chÃ¢n bá»™ nhá»› lá»›n (vÃ  do Ä‘Ã³, Ä‘á»™ trá»… vÃ  chi phÃ­ cao hÆ¡n). NgÆ°á»£c láº¡i, cÃ¡ch tiáº¿p cáº­n XC-CACHE cá»§a chÃºng tÃ´i giáº£m Ä‘Ã¡ng ká»ƒ yÃªu cáº§u bá»™ nhá»› cache gáº§n 98%; vÃ  nhÆ° cÃ¡c ká»¹ thuáº­t cáº£i thiá»‡n hiá»‡u quáº£ khÃ¡c nhÆ° quantization (Frantar et al., 2023), viá»‡c giáº£m nÃ y Ä‘i kÃ¨m vá»›i chi phÃ­ nhá» vá» Ä‘á»™ chÃ­nh xÃ¡c, nhÆ° minh há»a trong HÃ¬nh 1. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i liÃªn tá»¥c vÆ°á»£t trá»™i hÆ¡n cÃ¡c thay tháº¿ ICL dá»±a trÃªn LLAMA 2 hoáº·c GPT-3.5, nhÆ° chi tiáº¿t trong Pháº§n 5.

NhÃ¬n chung, chÃºng tÃ´i á»§ng há»™ má»™t sá»± thay Ä‘á»•i khÃ¡i niá»‡m trong thiáº¿t káº¿ kiáº¿n trÃºc cho sinh cÃ³ Ä‘iá»u kiá»‡n, viá»‡c nÃ y nÃªn tÃ¡i trung tÃ¢m hÃ³a viá»‡c lÆ°u trá»¯ cache vÃ  lÃ m cho nÃ³ trá»Ÿ thÃ nh pháº§n tÃ­ch há»£p vÃ o hoáº¡t Ä‘á»™ng cá»§a mÃ´ hÃ¬nh thay vÃ¬ má»™t suy nghÄ© sau. Nhá»¯ng Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:

1. Kháº£ nÄƒng lÆ°u cache: ChÃºng tÃ´i cung cáº¥p báº±ng chá»©ng ráº±ng cÃ¡c kiáº¿n trÃºc encoder-decoder lÃ  á»©ng cá»­ viÃªn tá»‘t cho sinh cÃ³ Ä‘iá»u kiá»‡n vÃ¬ cÃ¡c mÃ´ hÃ¬nh thÃ¢n thiá»‡n vá»›i cache cá»§a chÃºng tÃ´i tÄƒng cÆ°á»ng hiá»‡u suáº¥t mÃ´ hÃ¬nh so vá»›i ICL, trong khi giáº£m dáº¥u chÃ¢n bá»™ nhá»› cache hÆ¡n 98%.

2. Hiá»‡u quáº£ tham sá»‘: ChÃºng tÃ´i cho tháº¥y ráº±ng huáº¥n luyá»‡n má»™t vÃ i lá»›p cross-attention (vÃ  tÃ¹y chá»n, má»™t encoder nhá») Ä‘á»§ Ä‘á»ƒ chuyá»ƒn Ä‘á»•i decoder thÃ nh cáº·p encoder-decoder. ChÃºng tÃ´i Ä‘Ã³ng gÃ³p má»™t há»—n há»£p cÃ¡c nhiá»‡m vá»¥ huáº¥n luyá»‡n cho phÃ©p sinh cÃ³ Ä‘iá»u kiá»‡n ngá»¯ cáº£nh mÃ  khÃ´ng cáº§n ICL tá»‘n kÃ©m.

3. Decoder-as-encoder: ChÃºng tÃ´i cho tháº¥y ráº±ng cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« cÃ¡c decoder nhÃ¢n quáº£ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng nguyÃªn tráº¡ng Ä‘á»ƒ thay tháº¿ má»™t encoder.

--- TRANG 3 ---
2 LÆ°u trá»¯ Cache Biá»ƒu diá»…n

Gá»i ngá»¯ cáº£nh, truy váº¥n vÃ  cÃ¢u tráº£ lá»i lÃ  cÃ¡c chuá»—i token tá»« má»™t tá»« vá»±ng chung V. ChÃºng ta viáº¿t |ngá»¯ cáº£nh|, |truy váº¥n| vÃ  |cÃ¢u tráº£ lá»i| lÃ  Ä‘á»™ dÃ i tÆ°Æ¡ng á»©ng cá»§a cÃ¡c chuá»—i nÃ y. HÃ¬nh 2(a) minh há»a má»™t LLM mÃ , cÃ³ Ä‘iá»u kiá»‡n trÃªn ngá»¯ cáº£nh, táº¡o ra cÃ¢u tráº£ lá»i cho truy váº¥n do ngÆ°á»i dÃ¹ng chá»‰ Ä‘á»‹nh.

Giáº£ Ä‘á»‹nh. Trong nhá»¯ng gÃ¬ theo sau, chÃºng tÃ´i Ä‘Æ°a ra ba giáº£ Ä‘á»‹nh chÃ­nh: (A1) ngá»¯ cáº£nh khÃ´ng duy nháº¥t vÃ  phá»¥ thuá»™c vÃ o truy váº¥n; (A2) tá»“n táº¡i má»™t lÆ°á»£ng ngá»¯ cáº£nh cÃ³ thá»ƒ quáº£n lÃ½ Ä‘á»ƒ Ä‘iá»u kiá»‡n hÃ³a; vÃ  (A3) ngá»¯ cáº£nh lá»›n, tá»©c lÃ  |truy váº¥n| + |cÃ¢u tráº£ lá»i| â‰ª |ngá»¯ cáº£nh|. Báº±ng cÃ¡ch lÃ m nhÆ° váº­y, chÃºng tÃ´i háº¡n cháº¿ báº£n thÃ¢n vÃ o cÃ¡c cháº¿ Ä‘á»™ mÃ  viá»‡c xá»­ lÃ½ ngá»¯ cáº£nh offline vá»«a háº¥p dáº«n (A1 vÃ  A3) vá»«a kháº£ thi (A2).

Nhiá»u trÆ°á»ng há»£p sá»­ dá»¥ng ICL thá»a mÃ£n nhá»¯ng giáº£ Ä‘á»‹nh nÃ y. VÃ­ dá»¥, khi Ä‘á»‘i máº·t vá»›i má»™t sá»‘ nhiá»‡m vá»¥ khÃ¡c nhau, chÃºng ta cÃ³ thá»ƒ táº¡o ra cÃ¡c hÆ°á»›ng dáº«n cá»¥ thá»ƒ cho nhiá»‡m vá»¥, má»—i cÃ¡i chi tiáº¿t cÃ¡ch thu Ä‘Æ°á»£c cÃ¢u tráº£ lá»i cho truy váº¥n (HÃ¬nh 2(b)). Äiá»u tÆ°Æ¡ng tá»± Ã¡p dá»¥ng cho question-answering dá»±a trÃªn RAG khi cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c truy xuáº¥t Ä‘Æ°á»£c mÃ£ hÃ³a Ä‘á»™c láº­p: retriever chá»n tá»« corpus cÃ¡c tÃ i liá»‡u liÃªn quan nháº¥t (ngá»¯ cáº£nh) Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng (truy váº¥n).

2.1 Äá»™ phá»©c táº¡p thá»i gian suy luáº­n

NÃ³i chung, Ä‘á»™ phá»©c táº¡p thá»i gian cá»§a suy luáº­n LLM bá»‹ chi phá»‘i bá»Ÿi cÆ¡ cháº¿ attention cá»§a nÃ³, trong thiáº¿t láº­p ICL lÃ  O((|ngá»¯ cáº£nh| + |truy váº¥n| + |cÃ¢u tráº£ lá»i|)Â²). Trong cháº¿ Ä‘á»™ ngá»¯ cáº£nh lá»›n (A3), Ä‘iá»u nÃ y Ä‘Æ¡n giáº£n hÃ³a thÃ nh O(|ngá»¯ cáº£nh|Â²): do Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ mong Ä‘á»£i tÄƒng tá»‘c suy luáº­n rÃµ rá»‡t báº±ng cÃ¡ch cáº£i thiá»‡n cÃ¡ch chÃºng ta xá»­ lÃ½ ngá»¯ cáº£nh. Má»™t cÃ¡ch Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c nhÆ° váº­y lÃ  tiá»n xá»­ lÃ½ cÃ¡c ngá»¯ cáº£nh offline thÃ nh má»™t sá»‘ tráº¡ng thÃ¡i trung gian (cache) vÃ  cung cáº¥p cho mÃ´ hÃ¬nh táº¡i thá»i Ä‘iá»ƒm suy luáº­n (HÃ¬nh 2(c)). Báº±ng cÃ¡ch nÃ y, chi phÃ­ báº­c hai cá»§a viá»‡c xá»­ lÃ½ ngá»¯ cáº£nh Ä‘Æ°á»£c tráº£ má»™t láº§n, cho phÃ©p suy luáº­n chá»‰ Ä‘Æ¡n giáº£n tra cá»©u cache Ä‘Ã£ sáºµn sÃ ng (HÃ¬nh 2(d)). LÆ°u Ã½ ráº±ng chi phÃ­ lÆ°u trá»¯ vÃ  truyá»n thÃ´ng phÃ¡t sinh lÃ  tuyáº¿n tÃ­nh theo kÃ­ch thÆ°á»›c cache, cÅ©ng tuyáº¿n tÃ­nh theo Ä‘á»™ dÃ i ngá»¯ cáº£nh. Trong thiáº¿t láº­p nÃ y, Ä‘á»™ phá»©c táº¡p thá»i gian suy luáº­n trá»Ÿ thÃ nh O(|ngá»¯ cáº£nh|(|truy váº¥n| + |cÃ¢u tráº£ lá»i|)), tá»©c lÃ  tuyáº¿n tÃ­nh theo Ä‘á»™ dÃ i ngá»¯ cáº£nh (HÃ¬nh 3), má»™t tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ.

2.2 Chi phÃ­ thá»±c táº¿ cá»§a caching

CÃ¡c cÃ¢n nháº¯c thá»±c táº¿ cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng tiÃªu cá»±c Ä‘áº¿n thiáº¿t láº­p cÃ³ cache. Táº£i vÃ  chi phÃ­ truyá»n thÃ´ng Ä‘á»u tuyáº¿n tÃ­nh theo kÃ­ch thÆ°á»›c cache, thÃºc Ä‘áº©y cache nhá» hÆ¡n. Báº¥t ká»³ phÃ©p toÃ¡n bá»• sung nÃ o cáº§n thiáº¿t táº¡i thá»i Ä‘iá»ƒm suy luáº­n Ä‘á»u giáº£m bá»›t lá»£i Ã­ch caching.

[HÃ¬nh 3: Biá»ƒu diá»…n cÃ¡ch Ä‘iá»‡u vá» thá»i gian thá»±c thi attention (diá»‡n tÃ­ch) cho mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ cÃ³ Ä‘iá»u kiá»‡n ngá»¯ cáº£nh. CÃ¡c Ä‘Æ°á»ng Ä‘á»©t nÃ©t trong (b) hiá»ƒn thá»‹ khoáº£n tiáº¿t kiá»‡m khi cache key vÃ  value (quÃ¡ khá»©) cá»§a ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p. Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh nhÃ¢n quáº£, diá»‡n tÃ­ch dÆ°á»›i Ä‘Æ°á»ng chÃ©o thá»ƒ hiá»‡n thá»i gian thá»±c thi cÃ³ thá»ƒ Ä‘Æ°á»£c tiáº¿t kiá»‡m báº±ng cÃ¡c phÆ°Æ¡ng tiá»‡n khÃ¡c. Xem Phá»¥ lá»¥c A Ä‘á»ƒ biáº¿t chi tiáº¿t.]

CÃ¡c phÆ°Æ¡ng phÃ¡p caching cÃ³ thá»ƒ phÃ¡t sinh chi phÃ­ (áº©n hoáº·c rÃµ rÃ ng) vá» cháº¥t lÆ°á»£ng cá»§a cÃ¢u tráº£ lá»i Ä‘Æ°á»£c sinh ra. Xem Phá»¥ lá»¥c A Ä‘á»ƒ biáº¿t chi tiáº¿t.

2.3 CÃ¡c cÃ¡ch tiáº¿p cáº­n Caching

KV Caching. CÃ¡ch tiáº¿p cáº­n máº·c Ä‘á»‹nh lÃ  lÆ°u trá»¯ cÃ¡c tráº¡ng thÃ¡i key vÃ  value (quÃ¡ khá»©) Ä‘Æ°á»£c sinh ra trong khi xá»­ lÃ½ ngá»¯ cáº£nh, sau nÃ y gá»i lÃ  KV caching. KV caching thÆ°á»ng Ä‘Æ°á»£c liÃªn káº¿t vá»›i thiáº¿t láº­p há»™i thoáº¡i, trong trÆ°á»ng há»£p nÃ y cache thÆ°á»ng váº«n á»Ÿ trÃªn thiáº¿t bá»‹ GPU giá»¯a cÃ¡c vÃ²ng há»™i thoáº¡i (HÃ¬nh 2(d)). CÃ¡c thiáº¿t láº­p khÃ¡c, cháº³ng háº¡n nhÆ° phá»¥c vá»¥ nhiá»u cuá»™c há»™i thoáº¡i, tuy nhiÃªn Ä‘Ã²i há»i chÃºng ta pháº£i di chuyá»ƒn cache tá»« lÆ°u trá»¯ (vÃ /hoáº·c CPU RAM) Ä‘áº¿n GPU, phÃ¡t sinh thÃªm Ä‘á»™ trá»…. NhÆ° má»™t vÃ­ dá»¥, Ä‘á»‘i vá»›i LLAMA 2-7B (Touvron et al., 2023) sá»­ dá»¥ng Ä‘á»™ chÃ­nh xÃ¡c 16 bit, chÃºng ta pháº£i di chuyá»ƒn khoáº£ng 512 kB trÃªn má»—i tokenÂ². KÃ­ch thÆ°á»›c cache nhá» hÆ¡n trÃªn má»—i token do Ä‘Ã³ lÃ  mong muá»‘n.

JIT-KV Caching. Má»™t cÃ¡ch tiáº¿p cáº­n thay tháº¿ lÃ  lÆ°u trá»¯ cÃ¡c tráº¡ng thÃ¡i áº©n (quÃ¡ khá»©) cá»§a mÃ´ hÃ¬nh trong cache (trong trÆ°á»ng há»£p LLAMA 2-7B, Ä‘iá»u nÃ y sáº½ nhá» hÆ¡n má»™t ná»­a, tá»©c lÃ  256 kB trÃªn má»—i token). Táº¡i thá»i Ä‘iá»ƒm suy luáº­n, má»™t khi cÃ¡c tráº¡ng thÃ¡i áº©n nÃ y Ä‘Æ°á»£c táº£i lÃªn GPU, chÃºng ta cÃ³ thá»ƒ khÃ´i phá»¥c key vÃ  value Ä‘áº§y Ä‘á»§ trong cÃ¡c phÃ©p toÃ¡n song song tá»‘t O(|ngá»¯ cáº£nh|) (nhá»› ráº±ng key vÃ  value cá»§a transformer Ä‘Æ°á»£c thu Ä‘Æ°á»£c báº±ng phÃ©p toÃ¡n tuyáº¿n tÃ­nh trÃªn cÃ¡c tráº¡ng thÃ¡i áº©n). ChÃºng tÃ´i gá»i cÃ¡ch tiáº¿p cáº­n nÃ y lÃ  "just-in-time" hoáº·c JIT-KV caching.

XC Caching (Cá»§a chÃºng tÃ´i). Cáº£ KV vÃ  JIT-KV caching Ä‘á»u thá»±c hiá»‡n chÃ­nh xÃ¡c nhá»¯ng tÃ­nh toÃ¡n giá»‘ng nhÆ° mÃ´ hÃ¬nh gá»‘c mÃ  khÃ´ng cÃ³ cache. Cáº£ hai Ä‘á»u chá»‹u hai loáº¡i chi phÃ­ trong khi táº¡o ra cÃ¹ng cÃ¢u tráº£ lá»i: kÃ­ch thÆ°á»›c cache vÃ  cÃ¡c phÃ©p toÃ¡n cáº§n thá»±c hiá»‡n táº¡i thá»i Ä‘iá»ƒm suy luáº­n (vÃ­ dá»¥: chuyá»ƒn Ä‘á»•i tráº¡ng thÃ¡i áº©n thÃ nh key vÃ  value). Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t XC caching (Ä‘á»c lÃ  cross-context caching), xem xÃ©t máº¥t mÃ¡t cháº¥t lÆ°á»£ng cá»§a cÃ¢u tráº£ lá»i Ä‘Æ°á»£c sinh ra nhÆ° chi phÃ­ thá»© ba vÃ  trÃ¬nh bÃ y hai mÃ´ hÃ¬nh cÃ¢n báº±ng ba chi phÃ­ nÃ y. Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u sá»­ dá»¥ng cÃ¡c lá»›p cross-attention Ä‘Æ°á»£c tÃ­ch há»£p vÃ o kiáº¿n trÃºc decoder, cho phÃ©p mÃ´ hÃ¬nh attend vÃ o ngá»¯ cáº£nh Ä‘Æ°á»£c cache vá»›i dáº¥u chÃ¢n bá»™ nhá»› nhá» gá»n táº¡i thá»i Ä‘iá»ƒm suy luáº­n. ChÃºng tÃ´i mÃ´ táº£ cáº£ hai mÃ´ hÃ¬nh trong pháº§n tiáº¿p theo.

Â²32 lá»›p, 32 head má»—i lá»›p, 128 floating point má»—i key vÃ  value (do Ä‘Ã³ há»‡ sá»‘ 2), 2 byte má»—i floating point 16 bit

--- TRANG 4 ---
[HÃ¬nh 4: Kiáº¿n trÃºc cá»§a XC-LLAMA. Má»™t mÃ´ hÃ¬nh chá»‰ decoder triá»ƒn khai kiáº¿n trÃºc encoder-decoder. Fine-tuning Ä‘Æ°á»£c thá»±c hiá»‡n má»™t cÃ¡ch hiá»‡u quáº£ tham sá»‘ thÃ´ng qua viá»‡c chá»‰ huáº¥n luyá»‡n má»™t sá»‘ lÆ°á»£ng nhá» cÃ¡c lá»›p cross-attention.]

3 XC: Cross Attending to Efficiently Cached Context

Äá»ƒ giáº£m dáº¥u chÃ¢n bá»™ nhá»› cá»§a caching, cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i láº¥y cáº£m há»©ng tá»« kiáº¿n trÃºc encoder-decoder, cho Ä‘áº¿n gáº§n Ä‘Ã¢y lÃ  thiáº¿t káº¿ Ä‘Æ°á»£c Æ°a chuá»™ng cho sinh cÃ³ Ä‘iá»u kiá»‡n. Gáº§n Ä‘Ã¢y, cÃ¡c mÃ´ hÃ¬nh chá»‰ decoder Ä‘Ã£ trá»Ÿ nÃªn phá»• biáº¿n hÆ¡n má»™t pháº§n do hiá»‡u quáº£ dá»¯ liá»‡u cá»§a chÃºng: toÃ n bá»™ ngÃ¢n sÃ¡ch tham sá»‘ Ä‘Æ°á»£c phÃ¢n bá»• cho decoder vÃ  khÃ´ng bá»‹ "lÃ£ng phÃ­" cho encoder, vá»›i lá»£i tháº¿ bá»• sung lÃ  táº¥t cáº£ cÃ¡c tham sá»‘ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i táº¥t cáº£ cÃ¡c token dá»¯ liá»‡u. Máº·c dÃ¹ viá»‡c duy trÃ¬ má»™t encoder bÃªn ngoÃ i cÃ³ thá»ƒ cÃ³ váº» lÃ£ng phÃ­, chÃºng tÃ´i á»§ng há»™ kiáº¿n trÃºc encoder-decoder: nÃ³ phÃ¹ há»£p hÆ¡n Ä‘á»ƒ tÃ­nh toÃ¡n trÆ°á»›c vÃ  cache cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh. Tháº­t váº­y, chá»‰ cÃ¡c vector Ä‘áº§u ra encoder cáº§n Ä‘Æ°á»£c lÆ°u trá»¯, trÃ¡i ngÆ°á»£c vá»›i cÃ¡c tráº¡ng thÃ¡i trung gian qua táº¥t cáº£ cÃ¡c lá»›p self-attention cá»§a decoder (KV vÃ  JIT-KV cache).

Trong nhá»¯ng gÃ¬ theo sau, chÃºng tÃ´i Ä‘á» cáº­p Ä‘áº¿n má»™t mÃ´ hÃ¬nh nhÆ° Ä‘Æ°á»£c cáº¥u thÃ nh tá»« má»™t encoder E: V^|ngá»¯ cáº£nh| â†’ â„^{d|ngá»¯ cáº£nh|}, nháº­n Ä‘áº§u vÃ o ngá»¯ cáº£nh vÃ  xuáº¥t ra cÃ¡c biá»ƒu diá»…n cáº¥p token cÃ³ kÃ­ch thÆ°á»›c d, vÃ  má»™t decoder D: V^m Ã— â„^{d|ngá»¯ cáº£nh|} â†’ Î”^|V|, nháº­n Ä‘áº§u vÃ o truy váº¥n vÃ  cÃ¡c mÃ£ hÃ³a ngá»¯ cáº£nh vÃ  xuáº¥t ra cÃ¢u tráº£ lá»i trong simplex cÃ³ kÃ­ch thÆ°á»›c |V|. Cá»¥ thá»ƒ hÆ¡n, decoder D tá»± Ä‘á»™ng há»“i quy xuáº¥t ra cÃ¡c tham sá»‘ cá»§a phÃ¢n phá»‘i categorical trÃªn tá»« vá»±ng.

VÃ¬ má»¥c Ä‘Ã­ch hiá»‡u quáº£ tham sá»‘ vÃ  Ä‘á»ƒ táº­n dá»¥ng cÃ¡c LLM state-of-the-art Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, chÃºng tÃ´i báº¯t Ä‘áº§u tá»« má»™t mÃ´ hÃ¬nh chá»‰ decoder hiá»‡n cÃ³ vÃ  tÄƒng cÆ°á»ng nÃ³ vá»›i cÃ¡c lá»›p cross-attention má»›i xen káº½ giá»¯a cÃ¡c lá»›p self-attention hiá»‡n cÃ³, nhÆ° minh há»a trong HÃ¬nh 4. ChÃºng tÃ´i xem xÃ©t hai chiáº¿n lÆ°á»£c Ä‘á»ƒ triá»ƒn khai encoder E. Chiáº¿n lÆ°á»£c Ä‘áº§u tiÃªn huáº¥n luyá»‡n má»™t encoder nhá», tá»©c lÃ  E nhá» hÆ¡n D hÃ ng báº­c Ä‘á»™ lá»›n. Chiáº¿n lÆ°á»£c thá»© hai sá»­ dá»¥ng decoder nhÆ° encoder, tá»©c lÃ  mÃ´ hÃ¬nh chá»‰ decoder Ä‘Ã´ng láº¡nh Ä‘Æ°á»£c sá»­ dá»¥ng nguyÃªn tráº¡ng lÃ m encoder (E := D). Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i sá»­ dá»¥ng lÃ m mÃ£ hÃ³a cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« D Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c táº¡i lá»›p cuá»‘i cÃ¹ng trÆ°á»›c head mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯.

Viá»‡c chá»n má»™t cÃ¡ch tiáº¿p cáº­n nÃ y hay cÃ¡ch khÃ¡c phá»¥ thuá»™c vÃ o cÃ¡c cÃ¢n nháº¯c thá»±c táº¿. Náº¿u caching cÃ³ thá»ƒ vÃ  cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n offline, thÃ¬ viá»‡c sá»­ dá»¥ng decoder lÃ m encoder lÃ  Æ°u tiÃªn hÆ¡n vÃ¬ tÃ­nh Ä‘Æ¡n giáº£n tá»•ng thá»ƒ vÃ  hiá»‡u quáº£ tham sá»‘. NgÆ°á»£c láº¡i, má»™t encoder nhá» sáº½ lÃ m cho viá»‡c xá»­ lÃ½ ngá»¯ cáº£nh just-in-time Ã­t tá»‘n kÃ©m hÆ¡n Ä‘Ã¡ng ká»ƒ.

Cáº£ hai cÃ¡ch tiáº¿p cáº­n Ä‘á»u káº¿ thá»«a lá»£i tháº¿ cá»§a cÃ¡c mÃ´ hÃ¬nh chá»‰ decoder Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trong khi hÆ°á»Ÿng lá»£i tá»« viá»‡c sá»­ dá»¥ng encoder trong quÃ¡ trÃ¬nh suy luáº­n. Äáº·c biá»‡t, thÃ´ng tin ngá»¯ cáº£nh cÃ³ thá»ƒ Ä‘Æ°á»£c cache hiá»‡u quáº£ vÃ¬ chá»‰ Ä‘áº§u ra táº¡i lá»›p trÃªn cÃ¹ng cá»§a E cáº§n Ä‘Æ°á»£c lÆ°u trá»¯ thay vÃ¬ toÃ n bá»™ táº­p há»£p cÃ¡c tráº¡ng thÃ¡i trung gian cá»§a D.

Cuá»‘i cÃ¹ng, Ä‘á»ƒ cho phÃ©p Ä‘iá»u kiá»‡n hÃ³a ngá»¯ cáº£nh, chÃºng tÃ´i huáº¥n luyá»‡n Ä‘á»™c quyá»n cÃ¡c module má»›i Ä‘Æ°á»£c thÃªm vÃ o: cÃ¡c lá»›p cross-attention trong cáº£ hai thiáº¿t láº­p vÃ  encoder nhá» trong thiáº¿t láº­p yÃªu cáº§u nÃ³. Decoder cÆ¡ sá»Ÿ Ä‘Æ°á»£c giá»¯ Ä‘Ã´ng láº¡nh trong cáº£ hai thiáº¿t láº­p (ngay cáº£ khi nÃ³ hoáº¡t Ä‘á»™ng nhÆ° má»™t encoder). Do Ä‘Ã³, quy trÃ¬nh huáº¥n luyá»‡n cá»§a chÃºng tÃ´i khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡c tham sá»‘ gá»‘c cá»§a D, váº«n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘a má»¥c Ä‘Ã­ch khi cÃ¡c lá»›p bá»• sung Ä‘Æ°á»£c loáº¡i bá».

4 Thiáº¿t láº­p ThÃ­ nghiá»‡m

ChÃºng tÃ´i táº­p trung vÃ o nhiá»‡m vá»¥ question-answering vÃ  huáº¥n luyá»‡n trÃªn sá»± káº¿t há»£p cá»§a cÃ¡c dataset trong Ä‘Ã³ cÃ³ sáºµn bá»™ ba ngá»¯ cáº£nh, truy váº¥n vÃ  cÃ¢u tráº£ lá»i. QA lÃ  má»™t bá»‡ thá»­ lÃ½ tÆ°á»Ÿng cho cÃ¡c cÃ¡ch tiáº¿p cáº­n cá»§a chÃºng tÃ´i vÃ¬ nÃ³ yÃªu cáº§u tra cá»©u hiá»‡u quáº£ thÃ´ng tin bÃªn ngoÃ i vÃ  tÃ­nh toÃ¡n nÃ³ táº¡i thá»i Ä‘iá»ƒm sinh. NÃ³i cÃ¡ch khÃ¡c, QA gáº§n nhÆ° cÃ´ láº­p kháº£ nÄƒng "Ä‘á»c" ngá»¯ cáº£nh cá»§a mÃ´ hÃ¬nh, Ä‘Ã³ lÃ  Ä‘iá»u chÃºng tÃ´i cáº§n kiá»ƒm tra.

4.1 Dataset Huáº¥n luyá»‡n

ChÃºng tÃ´i xÃ¢y dá»±ng dataset huáº¥n luyá»‡n báº±ng cÃ¡ch chuáº©n hÃ³a vÃ  gá»™p chung cÃ¡c phÃ¢n vÃ¹ng huáº¥n luyá»‡n cá»§a nÄƒm dataset cÃ³ sáºµn cÃ´ng khai vÃ  Ä‘a dáº¡ng (open-domain, multi-hop, conversational): NATURAL QUESTIONS (NQ) (Kwiatkowski et al., 2019), HOTPOTQA (Yang et al., 2018), TOPIOCQA (Adlakha et al., 2022), MS MARCO (Bajaj et al., 2016), vÃ  SQUAD-V2 (Rajpurkar et al., 2018). CÃ¡c vÃ­ dá»¥ tá»« dataset huáº¥n luyá»‡n káº¿t quáº£ chá»©a truy váº¥n (cÃ¢u há»i ngÃ´n ngá»¯ tá»± nhiÃªn), cÃ¢u tráº£ lá»i (Ä‘áº§u ra mong Ä‘á»£i), vÃ  má»™t hoáº·c nhiá»u ngá»¯ cáº£nh (vÃ­ dá»¥: bÃ i viáº¿t knowledge base), trong Ä‘Ã³ Ã­t nháº¥t má»™t trong sá»‘ chÃºng chá»©a cÃ¢u tráº£ lá»i cho truy váº¥n. ChÃºng tÃ´i Ä‘á» cáº­p Ä‘áº¿n ngá»¯ cáº£nh nÃ y nhÆ° ngá»¯ cáº£nh tham kháº£o. ChÃºng tÃ´i sá»­ dá»¥ng táº­p validation Ä‘á»ƒ lá»±a chá»n mÃ´ hÃ¬nh vÃ  Ä‘iá»u chá»‰nh siÃªu tham sá»‘. Äá»‘i vá»›i cÃ¡c dataset cÃ³ táº­p validation nhÆ°ng khÃ´ng cÃ³ phÃ¢n vÃ¹ng test, chÃºng tÃ´i sá»­ dá»¥ng táº­p validation Ä‘á»ƒ test vÃ  táº¡o táº­p validation báº±ng cÃ¡ch giá»¯ láº¡i 10% cÃ¡c máº«u huáº¥n luyá»‡n Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn. ChÃºng tÃ´i Ã¡p dá»¥ng bÆ°á»›c lá»c thÃªm trong dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  loáº¡i bá» cÃ¡c vÃ­ dá»¥ cÃ³ ngá»¯ cáº£nh dÃ i hÆ¡n 6.000 token, tÆ°Æ¡ng á»©ng vá»›i Ã­t hÆ¡n 5% cÃ¡c máº«u.

ÄÃ¡nh giÃ¡ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c phÃ¢n vÃ¹ng test káº¿t quáº£ cá»§a NATURAL QUESTIONS, HOTPOTQA vÃ  TOPIOCQA. Chi tiáº¿t thÃªm vÃ  thá»‘ng kÃª cá»§a cÃ¡c dataset chÃºng tÃ´i xem xÃ©t cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong Phá»¥ lá»¥c C.

4.2 Nhiá»‡m vá»¥ Phá»¥ trá»£

NgoÃ i viá»‡c huáº¥n luyá»‡n trÃªn cÃ¡c nhiá»‡m vá»¥ QA chÃ­nh, chÃºng tÃ´i tá»‘i Æ°u hÃ³a cÃ¡c mÃ´ hÃ¬nh trÃªn cÃ¡c nhiá»‡m vá»¥ láº·p láº¡i ngá»¯ cáº£nh, nhÆ° mÃ´ táº£ dÆ°á»›i Ä‘Ã¢y. Lá»£i tháº¿ cá»§a viá»‡c Ä‘á»‹nh nghÄ©a cÃ¡c nhiá»‡m vá»¥ phá»¥ trá»£ nhÆ° váº­y cÃ³ hai máº·t. Má»™t máº·t, chÃºng cho phÃ©p chÃºng tÃ´i tá»‘i Æ°u hÃ³a likelihood cá»§a táº¥t cáº£ cÃ¡c token cÃ³ sáºµn, ngay cáº£ nhá»¯ng token Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Ä‘áº§u vÃ o cho encoder. Máº·t khÃ¡c, chÃºng giÃºp trÃ¡nh cÃ¡c giáº£i phÃ¡p khÃ´ng tá»‘i Æ°u trong Ä‘Ã³ cÃ¡c lá»›p cross-attention hoáº¡t Ä‘á»™ng nhÆ° cÃ¡c toÃ¡n tá»­ Ä‘á»“ng nháº¥t Ä‘Æ¡n giáº£n. Tháº­t váº­y, cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cÃ³ thá»ƒ há»c cÃ¡ch bá» qua ngá»¯ cáº£nh nhÆ°ng khÃ´ng thá»ƒ lÃ m nhÆ° váº­y khi Ä‘Æ°á»£c giao nhiá»‡m vá»¥ láº·p láº¡i ngá»¯ cáº£nh.

Trong thá»±c táº¿, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c token dÃ nh riÃªng Ä‘á»ƒ hÆ°á»›ng dáº«n mÃ´ hÃ¬nh láº·p láº¡i ngá»¯ cáº£nh, hoáº·c nguyÃªn tráº¡ng hoáº·c báº±ng cÃ¡ch infilling (Bavarian et al., 2022), tá»©c lÃ  tráº£ vá» nÃ³ theo thá»© tá»± prefix-suffix-middle hoáº·c suffix-prefix-middle, nhÆ° Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Li et al. (2023b); Lozhkov et al. (2024). CÃ¡c nhiá»‡m vá»¥ nhÆ° váº­y giá»›i thiá»‡u tÃ­nh biáº¿n thiÃªn má»›i, khi mÃ´ hÃ¬nh há»c cÃ¡ch copy-paste ngá»¯ cáº£nh vÃ  tÃ¬m vÃ  thay tháº¿ cÃ¡c Ä‘oáº¡n bá»‹ thiáº¿u, dáº«n Ä‘áº¿n cáº£i thiá»‡n hiá»‡u suáº¥t trong thiáº¿t láº­p huáº¥n luyá»‡n multi-epoch. LÆ°u Ã½ ráº±ng chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh trÃªn question-answering vÃ  láº·p láº¡i ngá»¯ cáº£nh trÃªn má»i máº«u huáº¥n luyá»‡n. ChÃºng tÃ´i Ä‘áº·t má»™t epoch huáº¥n luyá»‡n tÆ°Æ¡ng á»©ng vá»›i hai láº§n Ä‘i qua dataset huáº¥n luyá»‡n, trong Ä‘Ã³ chÃºng tÃ´i thá»±c hiá»‡n nhiá»‡m vá»¥ chÃ­nh hoáº·c phá»¥ trong má»—i láº§n Ä‘i qua, nhÆ° minh há»a trong HÃ¬nh 5.

[HÃ¬nh 5: Chiáº¿n lÆ°á»£c huáº¥n luyá»‡n Ä‘a nhiá»‡m vá»¥. Trong má»™t epoch, má»—i vÃ­ dá»¥ Ä‘Æ°á»£c trÃ¬nh bÃ y cho mÃ´ hÃ¬nh hai láº§n. Trong vÃ²ng Ä‘áº§u tiÃªn, mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n cÃ¢u tráº£ lá»i cÃ³ Ä‘iá»u kiá»‡n dá»±a trÃªn ngá»¯ cáº£nh vÃ  truy váº¥n. Trong láº§n xuáº¥t hiá»‡n thá»© hai cá»§a má»™t vÃ­ dá»¥ trong epoch, chÃºng tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘á»ƒ láº·p láº¡i hoáº·c infill ngá»¯ cáº£nh.]

4.3 Chi tiáº¿t Triá»ƒn khai

ChÃºng tÃ´i dá»±a vÃ o LLAMA 2 (Touvron et al., 2023) Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cÃ³ sáºµn cÃ´ng khai Ä‘á»ƒ Ä‘á»‹nh nghÄ©a cÃ¡c biáº¿n thá»ƒ cá»§a XC-LLAMA. Cá»¥ thá»ƒ, táº¥t cáº£ cÃ¡c Ä‘Ã¡nh giÃ¡ thá»±c nghiá»‡m cá»§a chÃºng tÃ´i sá»­ dá»¥ng phiÃªn báº£n 7 tá»· tham sá»‘ cá»§a LLAMA 2. Äá»‘i vá»›i biáº¿n thá»ƒ cá»§a XC-LLAMA trong Ä‘Ã³ má»™t encoder chuyÃªn dá»¥ng Ä‘Æ°á»£c huáº¥n luyá»‡n (Ä‘Æ°á»£c gá»i lÃ  XC-LLAMA ENC tá»« giá»), chÃºng tÃ´i fine-tune lÃ m encoder má»™t LONGFORMER (Beltagy et al., 2020), Ä‘Ã¢y lÃ  má»™t BERT (Devlin et al., 2018) khoáº£ng 125M tham sá»‘, Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn cÃ¡c tÃ i liá»‡u tÆ°Æ¡ng Ä‘á»‘i dÃ i sá»­ dá»¥ng self-attention hiá»‡u quáº£. Tuy nhiÃªn, chÃºng tÃ´i lÆ°u Ã½ ráº±ng LONGFORMER cÃ³ sáºµn cÃ´ng khai dÃ i nháº¥t cháº¥p nháº­n Ä‘áº§u vÃ o tá»‘i Ä‘a 4.096 token, ngáº¯n hÆ¡n Ä‘á»™ dÃ i tá»‘i Ä‘a 6.000 cá»§a chÃºng tÃ´i. Sau Ä‘Ã³ chÃºng tÃ´i tÄƒng Ä‘á»™ dÃ i Ä‘áº§u vÃ o tá»‘i Ä‘a cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡ch láº·p láº¡i cÃ¡c embedding vá»‹ trÃ­ báº¯t Ä‘áº§u. Sau khi fine-tuning, mÃ´ hÃ¬nh xá»­ lÃ½ cÃ¡c ngá»¯ cáº£nh dÃ i hÆ¡n 4.096 token mÃ  khÃ´ng cÃ³ váº¥n Ä‘á» Ä‘Ã¡ng chÃº Ã½.

Vá» viá»‡c thÃªm cÃ¡c lá»›p cross-attention vÃ o XC-LLAMA, chÃºng tÃ´i giá»›i thiá»‡u má»™t lá»›p cross-attention cá»© má»—i vÃ i lá»›p self-attention cá»§a transformer. Äáº·c biá»‡t, chÃºng tÃ´i tháº¥y cáº¥u hÃ¬nh 5-6 (tá»©c lÃ  chÃ¨n nÄƒm lá»›p cross-attention trong khi bá» qua sÃ¡u lá»›p self-attention) hoáº¡t Ä‘á»™ng nháº¥t quÃ¡n tá»‘t. Do Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng cáº¥u hÃ¬nh nÃ y trong suá»‘t cÃ¡c Ä‘Ã¡nh giÃ¡. ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng chÃºng tÃ´i cá»‘ gáº¯ng hiá»‡u quáº£ tham sá»‘ vÃ  giá»¯ sá»‘ lÆ°á»£ng tham sá»‘ cá»§a cÃ¡c module Ä‘Æ°á»£c thÃªm dÆ°á»›i 10% sá»‘ lÆ°á»£ng tham sá»‘ cá»§a D. Huáº¥n luyá»‡n Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i optimizer ADAMW (Loshchilov vÃ  Hutter, 2017) vá»›i batch size 256 vÃ  trong 40.000 bÆ°á»›c (tÆ°Æ¡ng Ä‘Æ°Æ¡ng 4 epoch huáº¥n luyá»‡n). ChÃºng tÃ´i sá»­ dá»¥ng bá»™ láº­p lá»‹ch learning rate tuyáº¿n tÃ­nh vá»›i giai Ä‘oáº¡n warm-up vÃ  sao cho learning rate Ä‘áº¡t 0 á»Ÿ bÆ°á»›c huáº¥n luyá»‡n cuá»‘i cÃ¹ng. Danh sÃ¡ch Ä‘áº§y Ä‘á»§ cÃ¡c giÃ¡ trá»‹ siÃªu tham sá»‘ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 9 trong Phá»¥ lá»¥c D.

4.4 Metric vÃ  ÄÃ¡nh giÃ¡

ChÃºng tÃ´i so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh cá»§a mÃ¬nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p ICL Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i cÃ³ Ä‘iá»u kiá»‡n trÃªn ngá»¯ cáº£nh vÃ  truy váº¥n. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng cÃ¡c ngá»¯ cáº£nh trong cÃ¡c trÆ°á»ng há»£p chÃºng tÃ´i xem xÃ©t cÃ³ tá»· lá»‡ tÃ­n hiá»‡u trÃªn nhiá»…u tÆ°Æ¡ng Ä‘á»‘i tháº¥p, vÃ¬ háº§u háº¿t cÃ¡c token liÃªn quan Ä‘áº¿n cÃ¢u tráº£ lá»i nhÆ°ng hoÃ n toÃ n khÃ´ng cÃ³ liÃªn quan. Trong má»™t sá»‘ tÃ¬nh huá»‘ng cá»±c Ä‘oan hÆ¡n, cÃ¢u há»i Ä‘Æ°á»£c Ä‘áº·t ra khÃ´ng thá»ƒ Ä‘Æ°á»£c tráº£ lá»i tá»« ngá»¯ cáº£nh, vÃ  cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c mong Ä‘á»£i (vÃ  Ä‘Æ°á»£c huáº¥n luyá»‡n hoáº·c prompted Ä‘á»ƒ) chá»‰ ra ráº±ng khÃ´ng thá»ƒ tráº£ lá»i dá»±a trÃªn ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p.

ChÃºng tÃ´i sá»­ dá»¥ng cÃ¹ng metric vÃ  thiáº¿t láº­p Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c mÃ´ táº£ bá»Ÿi Adlakha et al. (2023) - cháº³ng háº¡n nhÆ° F1 SCORE, RECALL, METEOR vÃ  ROUGE L - nhÆ°ng giá»¯ F1 lÃ m metric trá»ng tÃ¢m. NgoÃ i ra, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ BERTSCORE (Zhang et al., 2019) Ä‘Æ°á»£c Ä‘o giá»¯a dá»± Ä‘oÃ¡n vÃ  cÃ¢u tráº£ lá»i ground-truth.

5 Káº¿t quáº£

5.1 So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n cÃ³

Äáº§u tiÃªn chÃºng tÃ´i so sÃ¡nh phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh vá»›i cÃ¡c cÃ¡ch tiáº¿p cáº­n hiá»‡n cÃ³ cho sinh cÃ³ Ä‘iá»u kiá»‡n. Baseline chÃ­nh cá»§a chÃºng tÃ´i lÃ  ICL, tá»©c lÃ  cung cáº¥p ngá»¯ cáº£nh nhÆ° má»™t pháº§n cá»§a prompt. Cá»¥ thá»ƒ hÆ¡n, chÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ baseline cho LLAMA 2-CHAT, mÃ  chÃºng tÃ´i tháº¥y hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n LLAMA 2 cÆ¡ sá»Ÿ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. ChÃºng tÃ´i cÅ©ng bÃ¡o cÃ¡o káº¿t quáº£ cho GPT-3.5-TURBO. Äá»‘i vá»›i cÃ¡c baseline ICL nÃ y, chÃºng tÃ´i chá»n cÃ¡c template prompt dá»±a trÃªn cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i Ä‘Æ°á»£c sinh trÃªn dá»¯ liá»‡u validation máº«u (xem Phá»¥ lá»¥c D Ä‘á»ƒ biáº¿t chi tiáº¿t). Cuá»‘i cÃ¹ng, chÃºng tÃ´i bÃ¡o cÃ¡o káº¿t quáº£ cá»§a FUSION-IN-DECODER (FiD) (Izacard vÃ  Grave, 2021), má»™t mÃ´ hÃ¬nh sinh cÃ³ Ä‘iá»u kiá»‡n dá»±a trÃªn T5 (Raffel et al., 2020), liÃªn tá»¥c chá»©ng minh lÃ  state-of-the-art trÃªn cÃ¡c nhiá»‡m vá»¥ QA (Borgeaud et al., 2022; Wang et al., 2023a; Adlakha et al., 2023). KhÃ´ng giá»‘ng nhÆ° backbone chá»‰ decoder cá»§a cÃ¡c mÃ´ hÃ¬nh chÃºng tÃ´i, FiD cÃ³ láº½ khÃ´ng cÃ²n lÃ  mÃ´ hÃ¬nh Ä‘a má»¥c Ä‘Ã­ch, vÃ¬ táº¥t cáº£ cÃ¡c tham sá»‘ cá»§a nÃ³ Ä‘Æ°á»£c fine-tune Ä‘á»ƒ thá»±c hiá»‡n QA. Quan trá»ng hÆ¡n, nhÆ° tháº£o luáº­n sÃ¢u hÆ¡n trong Pháº§n 5.2, viá»‡c tiá»n xá»­ lÃ½ vÃ  cache cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh khÃ´ng pháº£i lÃ  tÃ¹y chá»n cho FiD vÃ¬ nÃ³ yÃªu cáº§u biáº¿t cÃ¢u há»i táº¡i thá»i Ä‘iá»ƒm mÃ£ hÃ³a. Tuy nhiÃªn, chÃºng tÃ´i giá»›i thiá»‡u FiD nhÆ° má»™t tham chiáº¿u, Ä‘á»ƒ kiá»ƒm tra vá»‹ trÃ­ cá»§a cÃ¡c mÃ´ hÃ¬nh chÃºng tÃ´i so vá»›i cÃ¡c mÃ´ hÃ¬nh chuyÃªn biá»‡t QA Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p.

Káº¿t quáº£ Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 1. TrÃªn cÃ¡c dataset Ä‘Æ°á»£c xem xÃ©t, cross-attending vÃ o cÃ¡c ngá»¯ cáº£nh (XC-LLAMA hoáº·c XC-LLAMA ENC) cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t so vá»›i prompting (LLAMA 2-CHAT). Khoáº£ng cÃ¡ch thay Ä‘á»•i tÃ¹y thuá»™c vÃ o dataset. ChÃºng tÃ´i Ä‘oÃ¡n ráº±ng Ä‘iá»u nÃ y do phÆ°Æ¡ng sai cao Ä‘Æ°á»£c gÃ¢y ra bá»Ÿi prompting, máº·c dÃ¹ cÃ³ thá»ƒ tá»“n táº¡i prompt tá»‘i Æ°u cho má»—i dataset Ä‘á»ƒ giÃºp thu háº¹p khoáº£ng cÃ¡ch nÃ y. Do Ä‘Ã³, cÃ¡c cÃ¡ch tiáº¿p cáº­n khÃ´ng dá»±a vÃ o prompt cung cáº¥p lá»£i tháº¿ lÃ  cÃ³ thá»ƒ Ã¡p dá»¥ng rá»™ng rÃ£i hÆ¡n vÃ  do Ä‘Ã³ thá»±c táº¿ hÆ¡n. ChÃºng tÃ´i cÅ©ng lÆ°u Ã½ ráº±ng ngay cáº£ trong thiáº¿t láº­p mÃ  decoder Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m encoder, cross-attending vÃ o ngá»¯ cáº£nh váº«n mang láº¡i hiá»‡u suáº¥t tá»‘t hÆ¡n ICL, báº¥t ká»ƒ decoder cÆ¡ sá»Ÿ chÃºng tÃ´i so sÃ¡nh. Äiá»u nÃ y cho tháº¥y ráº±ng cÃ¡c lá»›p cross-attention Ä‘Æ°á»£c huáº¥n luyá»‡n bÃ¹ Ä‘áº¯p cho tÃ­nh khÃ´ng tá»‘i Æ°u tiá»m áº©n vÃ¬ cÃ¡c biá»ƒu diá»…n encoder khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n rÃµ rÃ ng cho nhiá»‡m vá»¥ chÃºng tÃ´i Ä‘Ã¡nh giÃ¡. Táº­p há»£p káº¿t quáº£ rá»™ng hÆ¡n bao gá»“m nhiá»u metric vÃ  mÃ´ hÃ¬nh hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong Báº£ng 6.

5.2 CÃ¢n báº±ng Ä‘á»™ chÃ­nh xÃ¡c vá»›i hiá»‡u quáº£

Káº¿t quáº£ trong pháº§n trÆ°á»›c cho tháº¥y viá»‡c thÃªm vÃ  fine-tune cÃ¡c tham sá»‘ chuyÃªn dá»¥ng cho Ä‘iá»u kiá»‡n hÃ³a ngá»¯ cáº£nh cáº£i thiá»‡n hiá»‡u suáº¥t so vá»›i prompting. Dá»±a trÃªn quan sÃ¡t nÃ y, trong pháº§n nÃ y, chÃºng tÃ´i má»Ÿ rá»™ng Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ xem xÃ©t cÃ¡c cÃ¡ch tiáº¿p cáº­n thay tháº¿ táº­n dá»¥ng má»™t sá»‘ lÆ°á»£ng nhá» tham sá»‘ bá»• sung cho phÃ©p cÃ¡c mÃ´ hÃ¬nh Ä‘iá»u kiá»‡n hÃ³a sinh dá»±a trÃªn ngá»¯ cáº£nh tham kháº£o. NgoÃ i hiá»‡u suáº¥t dá»± Ä‘oÃ¡n, giá» chÃºng tÃ´i cÅ©ng táº­p trung vÃ o hiá»‡u quáº£ tÃ­nh toÃ¡n; cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ má»©c Ä‘á»™ phÃ¹ há»£p cá»§a cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau Ä‘á»ƒ tiá»n xá»­ lÃ½ vÃ  cache cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh.

Do Ä‘Ã³ chÃºng tÃ´i fine-tune cÃ¡c adapter LORA (Hu et al., 2021) Ã¡p dá»¥ng cho cÃ¹ng decoder LLAMA 2 chÃºng tÃ´i sá»­ dá»¥ng cho XC-LLAMA. NÃ³i cÃ¡ch khÃ¡c, chÃºng tÃ´i fine-tune baseline ICL tá»« Pháº§n 5 Ä‘á»ƒ kiá»ƒm soÃ¡t áº£nh hÆ°á»Ÿng cá»§a prompting trong hiá»‡u suáº¥t mÃ´ hÃ¬nh. NhÆ° quan sÃ¡t trong Báº£ng 2, fine-tuning nhÆ° váº­y cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ chÃ­nh xÃ¡c QA so vá»›i cÃ¡c baseline ICL Ä‘Æ°á»£c bÃ¡o cÃ¡o trong Báº£ng 1. Tuy nhiÃªn, máº·c dÃ¹ cho phÃ©p cáº£i thiá»‡n hiá»‡u suáº¥t dá»± Ä‘oÃ¡n, viá»‡c dá»±a vÃ o Ä‘iá»u chá»‰nh mÃ´ hÃ¬nh kiá»ƒu LORA váº«n cáº§n thiáº¿t lÆ°u trá»¯ táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i KV trong má»i lá»›p Ä‘á»ƒ cache thÃ´ng tin ngá»¯ cáº£nh, phÃ¡t sinh chi phÃ­ Ä‘Ã¡ng ká»ƒ. NgÆ°á»£c láº¡i, cÃ¡c mÃ´ hÃ¬nh cÃ³ encoder yÃªu cáº§u cache cÃ¡c tráº¡ng thÃ¡i áº©n chá»‰ cá»§a lá»›p cuá»‘i cÃ¹ng cá»§a chÃºng. NÃ³i cÃ¡ch khÃ¡c, cÃ¡c biáº¿n thá»ƒ XC-LLAMA giáº£m Ä‘Ã¡ng ká»ƒ dáº¥u chÃ¢n caching Ä‘Æ¡n giáº£n vÃ¬ chÃºng yÃªu cáº§u lÆ°u trá»¯ chá»‰ cÃ¡c tráº¡ng thÃ¡i áº©n cuá»‘i cÃ¹ng cá»§a encoder. XC-LLAMA ENC giáº£m yÃªu cáº§u khÃ´ng gian hÆ¡n ná»¯a, vÃ¬ cÃ¡c biá»ƒu diá»…n cá»§a LONGFORMER cÃ³ chiá»u tháº¥p hÆ¡n so vá»›i LLAMA 2.

Giáº£m kÃ­ch thÆ°á»›c cache mang Ã½ nghÄ©a thá»±c táº¿ Ä‘Ã¡ng ká»ƒ, Ä‘áº·c biá»‡t trong viá»‡c giáº£m dáº¥u chÃ¢n bá»™ nhá»› cá»§a corpus Ä‘Æ°á»£c tiá»n xá»­ lÃ½ theo hÃ ng báº­c Ä‘á»™ lá»›n. Äiá»u nÃ y Ä‘áº·c biá»‡t quan trá»ng khi lÆ°u trá»¯ cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c tiá»n xá»­ lÃ½ cá»§a cÃ¡c dataset khá»•ng lá»“ nhÆ° toÃ n bá»™ Wikipedia. NgoÃ i ra, viá»‡c giáº£m kÃ­ch thÆ°á»›c cache dáº«n Ä‘áº¿n tiáº¿t kiá»‡m runtime báº±ng cÃ¡ch giáº£m chi phÃ­ truyá»n thÃ´ng, vÃ¬ lÆ°á»£ng thÃ´ng tin Ä‘Æ°á»£c chuyá»ƒn tá»« Ä‘Ä©a sang thiáº¿t bá»‹ Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ. Cuá»‘i cÃ¹ng, viá»‡c thu nhá» kÃ­ch thÆ°á»›c cache giáº£i phÃ³ng bá»™ nhá»› thiáº¿t bá»‹ táº¡i thá»i Ä‘iá»ƒm suy luáº­n, cho phÃ©p sinh dÃ i hÆ¡n hoáº·c batch size lá»›n hÆ¡n cho suy luáº­n theo batch. Xem Phá»¥ lá»¥c A Ä‘á»ƒ biáº¿t chi tiáº¿t.

ChÃºng tÃ´i cung cáº¥p gÃ³c nhÃ¬n káº¿t há»£p cá»§a hai thÃ nh pháº§n hiá»‡u suáº¥t Ä‘Ã³ - dá»± Ä‘oÃ¡n vÃ  tÃ­nh toÃ¡n - trong HÃ¬nh 1, trong Ä‘Ã³ chÃºng tÃ´i váº½ biá»ƒu Ä‘á»“ F1 vÃ  BERTSCORE tá»•ng há»£p qua cÃ¡c dataset so vá»›i lÆ°á»£ng cache trÃªn má»—i token ngá»¯ cáº£nh Ä‘Æ°á»£c yÃªu cáº§u bá»Ÿi cÃ¡c mÃ´ hÃ¬nh. LÆ°u Ã½ ráº±ng cÃ¡c mÃ´ hÃ¬nh gáº§n gÃ³c trÃªn-pháº£i hÆ¡n Ä‘Æ°á»£c Æ°u tiÃªn vÃ¬ chÃºng cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao vá»›i chi phÃ­ caching tháº¥p. Trong khi khÃ´ng cÃ³ phÆ°Æ¡ng phÃ¡p nÃ o tá»‘i Æ°u cho cáº£ hai tiÃªu chÃ­, táº­p Pareto bao gá»“m: á»Ÿ má»™t Ä‘áº§u, cÃ¡c mÃ´ hÃ¬nh ICL Ä‘Æ°á»£c fine-tune vá»›i LORA cung cáº¥p BERTSCORE tá»•ng há»£p cao hÆ¡n má»™t chÃºt nhÆ°ng yÃªu cáº§u khÃ´ng gian caching Ä‘Ã¡ng ká»ƒ; á»Ÿ Ä‘áº§u kia, cÃ¡c mÃ´ hÃ¬nh cÃ³ encoder hy sinh nhá» vá» Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n trong khi giáº£m Ä‘Ã¡ng ká»ƒ dáº¥u chÃ¢n bá»™ nhá»›. ChÃºng tÃ´i cÅ©ng lÆ°u Ã½ ráº±ng cÃ³ khoáº£ng cÃ¡ch giá»¯a XC-LLAMA vÃ  XC-LLAMA ENC, vÃ  cÃ¡c tham sá»‘ bá»• sung Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi XC-LLAMA ENC mang láº¡i tÄƒng cÆ°á»ng Ä‘á»™ chÃ­nh xÃ¡c vÃ  cáº£i thiá»‡n hiá»‡u quáº£ khÃ´ng gian. LÆ°u Ã½ ráº±ng chÃºng tÃ´i xem xÃ©t biáº¿n thá»ƒ ICL bá»• sung lÃ  LLAMA 2-ICL-JIT-KV, vÃ¬ nÃ³ thá»±c hiá»‡n phÃ©p chiáº¿u KV just-in-time cá»§a cÃ¡c tráº¡ng thÃ¡i áº©n Ä‘Æ°á»£c cache, Ä‘Ã¡nh Ä‘á»•i thá»i gian láº¥y khÃ´ng gian.

Káº¿t quáº£ QA chi tiáº¿t Ä‘Æ°á»£c bÃ¡o cÃ¡o trong Báº£ng 2. NhÆ° Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³, cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i phÃ¡t sinh giáº£m nháº¹ Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n nhÆ°ng Ä‘áº¡t Ä‘Æ°á»£c tiáº¿t kiá»‡m khÃ´ng gian Ä‘Ã¡ng ká»ƒ, Ä‘iá»u nÃ y tá» ra cÃ³ lá»£i trong nhiá»u tÃ¬nh huá»‘ng thá»±c táº¿ khÃ¡c nhau. Äá»ƒ cÃ³ cÃ¡i nhÃ¬n tá»•ng quan toÃ n diá»‡n, vui lÃ²ng tham kháº£o káº¿t quáº£ Ä‘áº§y Ä‘á»§ trong Báº£ng 6.

5.3 MÃ£ hÃ³a ngá»¯ cáº£nh phá»¥ thuá»™c cÃ¢u há»i

Trong sá»‘ cÃ¡c baseline chÃºng tÃ´i xem xÃ©t trong cÃ´ng trÃ¬nh nÃ y, nhá»¯ng baseline dá»±a vÃ o kiáº¿n trÃºc chá»‰ decoder cho mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ giá»¯ lá»£i tháº¿ cÆ¡ báº£n so vá»›i cÃ¡c mÃ´ hÃ¬nh encoder-decoder: chÃºng cÃ³ quyá»n truy cáº­p vÃ o cÃ¢u há»i trong khi tÃ­nh toÃ¡n cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh, cho phÃ©p cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c sinh ra Ä‘Æ°á»£c Ä‘iá»u chá»‰nh cho cÃ¢u há»i cá»¥ thá»ƒ. Tháº­t váº­y, trong cÃ¡c thiáº¿t láº­p encoder-decoder phá»• biáº¿n khÃ´ng cache nhÆ° FiD (Izacard vÃ  Grave, 2021), cÃ¡c ngá»¯ cáº£nh Ä‘Æ°á»£c Ä‘Æ°a vÃ o encoder cÃ¹ng vá»›i cÃ¡c cÃ¢u há»i.

Äá»ƒ kiá»ƒm tra tÃ¡c Ä‘á»™ng cá»§a viá»‡c cÃ³ quyá»n truy cáº­p vÃ o cÃ¢u há»i trong khi mÃ£ hÃ³a ngá»¯ cáº£nh Ä‘á»‘i vá»›i hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh chÃºng tÃ´i, chÃºng tÃ´i thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m vá»›i má»™t biáº¿n thá»ƒ cá»§a XC-LLAMA ENC trong Ä‘Ã³ cÃ¢u há»i Ä‘Æ°á»£c thÃªm vÃ o Ä‘áº§u Ä‘áº§u vÃ o cá»§a encoder. Báº£ng 3 tÃ³m táº¯t káº¿t quáº£ cá»§a cÃ¡c thÃ­ nghiá»‡m nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn NQ trong Ä‘Ã³ cá»™t cÃ³ tÃªn Question-in-context chá»‰ ra liá»‡u encoder cÃ³ quyá»n truy cáº­p vÃ o cÃ¢u há»i trong khi sinh cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh hay khÃ´ng. ChÃºng tÃ´i quan sÃ¡t cáº£i thiá»‡n hiá»‡u suáº¥t, gáº§n nhÆ° thu háº¹p khoáº£ng cÃ¡ch vá»›i baseline LoRA cache lá»›n. Quan sÃ¡t nÃ y cho tháº¥y ráº±ng cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh phá»¥ thuá»™c cÃ¢u há»i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­ch há»£p vÃ o cÃ¡c kiáº¿n trÃºc encoder-decoder cÃ³ thá»ƒ cache, cung cáº¥p Ä‘iá»u tá»‘t nháº¥t cá»§a cáº£ hai tháº¿ giá»›i: cache ngá»¯ cáº£nh nháº¹ vÃ  Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n cao.

6 CÃ´ng trÃ¬nh liÃªn quan

Decoder lÃ m encoder. Viá»‡c tÃ¡i sá»­ dá»¥ng cÃ¡c decoder Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘ang trá»Ÿ thÃ nh cÃ¡ch tiáº¿p cáº­n phá»• biáº¿n Ä‘á»ƒ táº­n dá»¥ng cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ máº¡nh máº½ cho cÃ¡c á»©ng dá»¥ng khÃ¡c ngoÃ i mÃ´ hÃ¬nh hÃ³a sinh. VÃ­ dá»¥, GRIT (Muennighoff et al., 2024) chuyá»ƒn Ä‘á»•i decoder nhÃ¢n quáº£ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c thÃ nh encoder hai chiá»u, táº¡o ra cÃ¡c embedding cáº¥p cÃ¢u trong khi duy trÃ¬ kháº£ nÄƒng thá»±c hiá»‡n sinh tá»± Ä‘á»™ng há»“i quy vÄƒn báº£n. Tuy nhiÃªn, khÃ´ng giá»‘ng nhÆ° cÃ¡c mÃ´ hÃ¬nh chÃºng tÃ´i xem xÃ©t, viá»‡c chuyá»ƒn Ä‘á»•i nÃ y yÃªu cáº§u fine-tune táº¥t cáº£ cÃ¡c tham sá»‘ mÃ´ hÃ¬nh thay vÃ¬ cÃ¡c tham sá»‘ bá»• sung. CÃ¡c cÃ¡ch tiáº¿p cáº­n hiá»‡u quáº£ tham sá»‘ Ä‘á»ƒ biáº¿n decoder thÃ nh encoder cÅ©ng Ä‘Æ°á»£c Ä‘á» xuáº¥t, cháº³ng háº¡n nhÆ° trong (Meng et al., 2024) vÃ  (BehnamGhader et al., 2024), trong Ä‘Ã³ decoder MISTRAL Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c (Jiang et al., 2023) Ä‘Æ°á»£c fine-tune trong thiáº¿t láº­p contrastive sá»­ dá»¥ng adapter LORA Ä‘á»ƒ táº¡o ra cÃ¡c biá»ƒu diá»…n cáº¥p cÃ¢u cho retrieval. Gáº§n vá»›i phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i hÆ¡n lÃ  CODET5+ (Wang et al., 2023d), cÅ©ng Ä‘á»‹nh nghÄ©a cÃ¡c kiáº¿n trÃºc encoder-decoder thay vÃ¬ biáº¿n decoder thÃ nh encoder cÃ¢u. TÆ°Æ¡ng tá»± nhÆ° má»™t trong cÃ¡c biáº¿n thá»ƒ cá»§a chÃºng tÃ´i (XC-LLAMA), nÃ³ Ä‘Æ°á»£c láº¯p rÃ¡p tá»« hai decoder Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, má»™t Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m encoder vÃ  cÃ¡i khÃ¡c lÃ m decoder vÃ  Ä‘Æ°á»£c liÃªn káº¿t vá»›i má»™t vÃ i phÃ©p toÃ¡n cross-attention. Tuy nhiÃªn, CODET5+ yÃªu cáº§u fine-tune toÃ n bá»™ encoder tÆ°Æ¡ng Ä‘á»‘i lá»›n. ChÃºng tÃ´i cho tháº¥y ráº±ng má»™t decoder Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c tá»‘t cÃ³ cÃ¡c biá»ƒu diá»…n Ä‘á»§ tá»‘t, nhÆ°ng ngÆ°á»i ta cÃ³ thá»ƒ cáº£i thiá»‡n nÃ³ má»™t cÃ¡ch hiá»‡u quáº£ chá»‰ báº±ng cÃ¡ch sá»­ dá»¥ng má»™t encoder cÃ³ thá»ƒ huáº¥n luyá»‡n ráº¥t nhá».

Äiá»u kiá»‡n hÃ³a mÃ  khÃ´ng cáº§n prompt. Má»™t hÆ°á»›ng nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Ã£ táº­p trung vÃ o viá»‡c kiá»ƒm soÃ¡t sinh cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡ch can thiá»‡p trá»±c tiáº¿p vÃ o cÃ¡c tham sá»‘ cá»§a nÃ³ (Wang et al., 2023c; Zhang et al., 2023; Wang et al., 2023b), Ä‘áº·c biá»‡t, Ä‘á»ƒ giá»›i thiá»‡u hoáº·c xÃ³a kiáº¿n thá»©c sau huáº¥n luyá»‡n. CÃ¡c cÃ¡ch tiáº¿p cáº­n nhÆ° váº­y thÆ°á»ng yÃªu cáº§u truy cáº­p vÃ  viáº¿t láº¡i cÃ¡c tham sá»‘ ná»™i bá»™ cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  khÃ´ng phÃ¹ há»£p vá»›i cÃ¡c ngá»¯ cáº£nh thay Ä‘á»•i thÆ°á»ng xuyÃªn nhÆ° trong thiáº¿t láº­p Question-Answer.

Suy luáº­n hiá»‡u quáº£. Tá»“n táº¡i má»™t sá»‘ phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ cáº£i thiá»‡n tá»‘c Ä‘á»™ suy luáº­n vÃ  dáº¥u chÃ¢n bá»™ nhá»›. Má»™t cÃ¡ch tiáº¿p cáº­n lÃ  giáº£m Ä‘á»™ chÃ­nh xÃ¡c sá»‘ hoáº·c quantize trá»ng sá»‘ mÃ´ hÃ¬nh vÃ /hoáº·c dá»¯ liá»‡u, Ä‘iá»u nÃ y Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  báº£o tá»“n Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh chá»‰ vá»›i 8 bit trÃªn má»—i trá»ng sá»‘ (Dettmers et al., 2022), hoáº·c tháº­m chÃ­ Ä‘á»™ chÃ­nh xÃ¡c tháº¥p hÆ¡n vá»›i 4, 3 hoáº·c tháº­m chÃ­ 2 bit trÃªn má»—i trá»ng sá»‘ (Frantar et al., 2023). NgoÃ i ra, cache key-query cÃ³ thá»ƒ Ä‘Æ°á»£c nÃ©n (Ainslie et al., 2023; Nawrot et al., 2024), máº·c dÃ¹ lÃ m nhÆ° váº­y yÃªu cáº§u fine-tuning. HÆ¡n ná»¯a, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ©n cache dá»c theo trá»¥c thá»i gian Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t (Bulatov et al., 2022; Ge et al., 2024; Chevalier et al., 2023). Tuy nhiÃªn, tá»· lá»‡ nÃ©n cho nhá»¯ng trÆ°á»ng há»£p Ä‘Ã³ khÃ´ng thá»±c sá»± cÃ³ thá»ƒ so sÃ¡nh vá»›i viá»‡c cÃ³ má»™t encoder riÃªng biá»‡t vÃ  cross-attending vÃ o cÃ¡c Ä‘áº§u ra cá»§a nÃ³. VÃ­ dá»¥, Ge et al. (2024) bÃ¡o cÃ¡o tá»· lá»‡ nÃ©n tá»‘i Ä‘a 4 (trÃ¡i ngÆ°á»£c vá»›i Ã­t nháº¥t 32 nhÆ° chÃºng tÃ´i tuyÃªn bá»‘), vÃ  Ä‘iá»u Ä‘Ã³ Ä‘i kÃ¨m vá»›i chi phÃ­ vÃ¬ viá»‡c nÃ©n dá»c theo chiá»u thá»i gian cÃ³ kháº£ nÄƒng loáº¡i bá» ná»™i dung liÃªn quan, nhÆ° tháº£o luáº­n bá»Ÿi Chevalier et al. (2023). Vá»›i kiáº¿n trÃºc tÆ°Æ¡ng tá»± nhÆ° cá»§a chÃºng tÃ´i, (Yen et al., 2024) bÃ¡o cÃ¡o lá»£i Ã­ch cá»§a viá»‡c cÃ³ encoder riÃªng biá»‡t. Máº·c dÃ¹ chÃºng tÃ´i cho tháº¥y ráº±ng viá»‡c Ã¡p dá»¥ng cÃ¡c toÃ¡n tá»­ cross-attention sau chá»‰ má»™t vÃ i lá»›p self-attention lÃ  Ä‘á»§, vÃ  cÃ¡c biá»ƒu diá»…n cá»§a decoder Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c hoáº¡t Ä‘á»™ng khÃ¡ tá»‘t mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n thÃªm. Cuá»‘i cÃ¹ng, viá»‡c sá»­ dá»¥ng Flash Attention (Dao et al., 2022) dáº«n Ä‘áº¿n tiáº¿t kiá»‡m Ä‘Ã¡ng ká»ƒ cho viá»‡c xá»­ lÃ½ ngá»¯ cáº£nh just-in-time. CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y bá»• sung trá»±c giao vá»›i nhá»¯ng gÃ¬ chÃºng tÃ´i trÃ¬nh bÃ y trong bÃ i bÃ¡o nÃ y vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c káº¿t há»£p vá»›i XC-LLAMA.

7 Káº¿t luáº­n

ChÃºng tÃ´i giá»›i thiá»‡u XC-LLAMA nhÆ° má»™t cÃ¡ch tiáº¿p cáº­n Ä‘á»ƒ chuyá»ƒn Ä‘á»•i mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ decoder Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c thÃ nh kiáº¿n trÃºc encoder-decoder cÃ³ thá»ƒ Ä‘iá»u kiá»‡n hÃ³a sinh dá»±a trÃªn cáº£ Ä‘áº§u vÃ o encoder vÃ  truy váº¥n decoder. Äiá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch tÃ­ch há»£p cÃ¡c lá»›p cross-attention xen káº½ giá»¯a cÃ¡c lá»›p self-attention hiá»‡n cÃ³ cá»§a decoder Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c. ChÃºng tÃ´i mÃ´ táº£ hai cÃ¡ch tiáº¿p cáº­n Ä‘á»ƒ Ä‘á»‹nh nghÄ©a encoder: sá»­ dá»¥ng báº£n sao cá»§a decoder hoáº·c giá»›i thiá»‡u encoder hai chiá»u cÃ³ thá»ƒ huáº¥n luyá»‡n nhÆ°ng nhá».

Kiáº¿n trÃºc Ä‘Æ°á»£c Ä‘á» xuáº¥t cho phÃ©p giáº£m khÃ´ng gian caching vá»›i há»‡ sá»‘ vÆ°á»£t quÃ¡ 300. Khi Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trong thiáº¿t láº­p QA, chÃºng tÃ´i quan sÃ¡t Ä‘á»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n cao hÆ¡n nhá»¯ng gÃ¬ cÃ¡c cÃ¡ch tiáº¿p cáº­n ICL tiÃªu chuáº©n Ä‘áº¡t Ä‘Æ°á»£c, hoáº·c vá»›i LLAMA 2 hoáº·c GPT-3.5 TURBO. NgoÃ i ra, chÃºng tÃ´i quan sÃ¡t má»©c Ä‘á»™ chÃ­nh xÃ¡c gáº§n nhÆ° ngang báº±ng vá»›i cÃ¡c mÃ´ hÃ¬nh prompted Ä‘Æ°á»£c fine-tune tá»‘n kÃ©m cache, cung cáº¥p thay tháº¿ thÃ¢n thiá»‡n vá»›i cache hÆ¡n cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ prompted cÃ³ tÃ­nh thá»±c táº¿ cao. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng chiáº¿n lÆ°á»£c encoder-decoder Ä‘á»ƒ xá»­ lÃ½ ngá»¯ cáº£nh riÃªng biá»‡t cung cáº¥p lá»£i tháº¿ bá»• sung lÃ  giáº£i phÃ³ng khÃ´ng gian prompt, hiá»‡n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»™c quyá»n cho tÆ°Æ¡ng tÃ¡c ngÆ°á»i dÃ¹ng. CÃ¡c háº¡n cháº¿ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c tháº£o luáº­n trong Pháº§n 8.

8 Háº¡n cháº¿

Háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tháº£o luáº­n trong bÃ i bÃ¡o nÃ y cÃ³ káº¿t quáº£ áº¥n tÆ°á»£ng trong thiáº¿t láº­p QA. Tuy nhiÃªn, kinh nghiá»‡m lÃ m viá»‡c vá»›i cÃ¡c mÃ´ hÃ¬nh nÃ y cá»§a chÃºng tÃ´i tiáº¿t lá»™ nhá»¯ng háº¡n cháº¿ chá»§ yáº¿u xuáº¥t phÃ¡t tá»« sá»± phá»¥ thuá»™c cá»§a chÃºng vÃ o mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÆ¡ báº£n. Do Ä‘Ã³, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y káº¿ thá»«a nhá»¯ng khuyáº¿t Ä‘iá»ƒm tiá»m áº©n cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ mÃ  chÃºng xÃ¢y dá»±ng dá»±a trÃªn.

VÃ­ dá»¥, cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘iá»ƒn hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn lÆ°á»£ng lá»›n vÄƒn báº£n, cÃ³ kháº£ nÄƒng bao gá»“m thÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i trong cÃ¡c benchmark QA cÃ³ sáºµn cÃ´ng khai. Trong khi viá»‡c huáº¥n luyá»‡n nhÆ° váº­y trÃªn dá»¯ liá»‡u liÃªn quan cÃ³ thá»ƒ cung cáº¥p lá»‘i táº¯t cho cÃ¡c mÃ´ hÃ¬nh Ä‘á»ƒ tráº£ lá»i Ä‘Ãºng cÃ¢u há»i trong ngá»¯ cáº£nh mÃ  chÃºng Ä‘Ã£ "ghi nhá»›" trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, nÃ³ cÅ©ng cÃ³ thá»ƒ gÃ¢y ra lá»—i: mÃ´ hÃ¬nh cÃ³ thá»ƒ "nhá»›" thÃ´ng tin liÃªn quan nhÆ°ng khÃ´ng chÃ­nh xÃ¡c liÃªn quan Ä‘áº¿n truy váº¥n cá»¥ thá»ƒ. NÃ³i Ä‘Æ¡n giáº£n hÆ¡n, cÃ¡c cÃ¡ch tiáº¿p cáº­n ICL dá»±a trÃªn prompting vÃ  cÃ³ kháº£ nÄƒng cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cÃ³ thá»ƒ bá» qua ngá»¯ cáº£nh vÃ  chá»‰ dá»±a vÃ o bá»™ nhá»› Ä‘á»ƒ sinh tiáº¿p. Äiá»u nÃ y lÃ  khÃ´ng mong muá»‘n vÃ¬ chÃºng tÃ´i nháº±m Ä‘áº£m báº£o ráº±ng cÃ¡c mÃ´ hÃ¬nh tÃ­nh toÃ¡n chÃ­nh xÃ¡c ngá»¯ cáº£nh tham kháº£o thay vÃ¬ cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c. CÃ¡c mÃ´ hÃ¬nh fine-tuning giáº£i quyáº¿t váº¥n Ä‘á» Ä‘Ã£ Ä‘á» cáº­p á»Ÿ trÃªn báº±ng cÃ¡ch khuyáº¿n khÃ­ch phá»¥ thuá»™c vÃ o ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p, nhÆ° chÃºng tÃ´i lÃ m rÃµ rÃ ng vá»›i cÃ¡c nhiá»‡m vá»¥ phá»¥ trá»£ láº·p láº¡i vÃ  Ä‘iá»n ngá»¯ cáº£nh. Tuy nhiÃªn, nÃ³ cÅ©ng chuyÃªn biá»‡t hÃ³a trong dá»¯ liá»‡u fine-tuning Ä‘áº¿n má»©c cÃ³ thá»ƒ lÃ m tá»•n háº¡i kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a cÃ¡c dataset khÃ¡c biá»‡t so vá»›i nhá»¯ng gÃ¬ Ä‘Æ°á»£c quan sÃ¡t trong quÃ¡ trÃ¬nh fine-tuning. ChÃºng tÃ´i Æ°á»›c tÃ­nh má»©c Ä‘á»™ cÃ¡c mÃ´ hÃ¬nh biáº¿t cÃ¢u tráº£ lá»i sau huáº¥n luyá»‡n trÆ°á»›c trong Báº£ng 7.

Dataset ChÆ°a tháº¥y. Äá»ƒ kiá»ƒm tra kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a cÃ¡c mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u out-of-distribution, khÃ´ng phÃ¢n phá»‘i Ä‘á»“ng nháº¥t liÃªn quan Ä‘áº¿n fine-tuning, chÃºng tÃ´i sá»­ dá»¥ng phiÃªn báº£n pre-release cá»§a RepLiQA (Monteiro et al., 2024). ÄÃ¢y lÃ  dataset test Ä‘Æ°á»£c sáº¯p xáº¿p chá»©a 16.290 tÃ i liá»‡u tham kháº£o, vá»›i khoáº£ng 5 cáº·p question-answer trÃªn má»—i tÃ i liá»‡u. CÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c viáº¿t bá»Ÿi cÃ¡c annotator con ngÆ°á»i, nhá»¯ng ngÆ°á»i viáº¿t vá» cÃ¡c tÃ¬nh huá»‘ng tÆ°á»Ÿng tÆ°á»£ng, má»—i cÃ¡i cÃ³ má»™t sá»‘ tiá»ƒu má»¥c vá»›i tiÃªu Ä‘á» vÃ  phá»¥ Ä‘á». Quan trá»ng lÃ , cho ráº±ng cÃ¡c tÃ i liá»‡u tham kháº£o trong dataset nÃ y chá»©a thÃ´ng tin hÆ° cáº¥u hoáº·c khÃ´ng Ä‘Ãºng, chÃºng ta cÃ³ thá»ƒ há»£p lÃ½ giáº£ Ä‘á»‹nh ráº±ng khÃ´ng mÃ´ hÃ¬nh nÃ o cÃ³ thá»ƒ dá»±a vÃ o hoáº·c bá»‹ Ä‘Ã¡nh lá»«a bá»Ÿi bá»™ nhá»› cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n. NgoÃ i ra, cÃ¡c máº«u trong dataset nÃ y thá»ƒ hiá»‡n phong cÃ¡ch Ä‘á»™c Ä‘Ã¡o cÃ³ thá»ƒ khÃ¡c biá»‡t so vá»›i dá»¯ liá»‡u fine-tuning.

Khi Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trong tÃ¬nh huá»‘ng out-of-distribution thÃ¡ch thá»©c hÆ¡n nÃ y, Ä‘á»™ chÃ­nh xÃ¡c cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh tráº£i qua sá»± suy giáº£m Ä‘Ã¡ng ká»ƒ, nhÆ° bÃ¡o cÃ¡o trong HÃ¬nh 6, trong Ä‘Ã³ chÃºng tÃ´i so sÃ¡nh Ä‘iá»ƒm sá»‘ QA cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh qua táº¥t cáº£ cÃ¡c dataset, bao gá»“m phiÃªn báº£n pre-release cá»§a RepLiQA Ä‘Æ°á»£c gáº¯n nhÃ£n lÃ  Unseen. ChÃºng tÃ´i Ä‘Æ°a ra giáº£ thuyáº¿t ráº±ng, ngoÃ i viá»‡c cÃ¡c mÃ´ hÃ¬nh khÃ´ng thá»ƒ dá»±a vÃ o bá»™ nhá»› Ä‘á»ƒ tráº£ lá»i truy váº¥n, yáº¿u tá»‘ chÃ­nh dáº«n Ä‘áº¿n suy giáº£m Ä‘á»™ chÃ­nh xÃ¡c lÃ  sá»± khÃ¡c biá»‡t giá»¯a dataset test vÃ  dá»¯ liá»‡u fine-tuning. Sá»± khÃ¡c biá»‡t nÃ y cÃ³ thá»ƒ phÃ¡t sinh tá»« cÃ¡c biáº¿n thá»ƒ trong phong cÃ¡ch viáº¿t, Ä‘á»™ dÃ i tÃ i liá»‡u, tá»· lá»‡ tÃ­n hiá»‡u trÃªn nhiá»…u, vÃ  sá»± hiá»‡n diá»‡n cá»§a ná»™i dung gÃ¢y máº¥t táº­p trung liÃªn quan Ä‘áº¿n cÃ¢u há»i nhÆ°ng khÃ´ng há»¯u Ã­ch cho cÃ¢u tráº£ lá»i cá»§a nÃ³, cÃ¹ng nhá»¯ng yáº¿u tá»‘ khÃ¡c. Viá»‡c xÃ¡c Ä‘á»‹nh cÃ¡c nguá»“n lá»—i chÃ­nh xÃ¡c vÃ  tÄƒng cÆ°á»ng tÃ­nh robust cá»§a cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘iá»u kiá»‡n ngá»¯ cáº£nh lÃ  nhá»¯ng hÆ°á»›ng nghiÃªn cá»©u há»©a háº¹n cho tÆ°Æ¡ng lai.

TuyÃªn bá»‘ Äáº¡o Ä‘á»©c

ChÃºng tÃ´i thá»«a nháº­n tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng cá»§a tÃ i nguyÃªn tÃ­nh toÃ¡n, bao gá»“m dáº¥u chÃ¢n carbon liÃªn quan Ä‘áº¿n lÆ°u trá»¯ vÃ  xá»­ lÃ½ dá»¯ liá»‡u. Báº±ng cÃ¡ch táº­p trung vÃ o sinh cÃ³ Ä‘iá»u kiá»‡n thÃ´ng qua caching vá»›i dáº¥u chÃ¢n bá»™ nhá»› nhá», chÃºng tÃ´i nháº±m giáº£m thiá»ƒu tiÃªu thá»¥ nÄƒng lÆ°á»£ng vÃ  thÃºc Ä‘áº©y tÃ­nh bá»n vá»¯ng trong cÃ¡c thá»±c hÃ nh tÃ­nh toÃ¡n.

Trong khi giáº£m cache cÃ³ thá»ƒ dáº«n Ä‘áº¿n cáº£i thiá»‡n tá»‘c Ä‘á»™, chÃºng tÃ´i nháº­n ra táº§m quan trá»ng cá»§a viá»‡c bá»• sung triá»ƒn khai cÃ¡c biá»‡n phÃ¡p báº£o vá»‡ Ä‘á»ƒ Ä‘áº£m báº£o sá»­ dá»¥ng cÃ³ trÃ¡ch nhiá»‡m cÃ¡c mÃ´ hÃ¬nh tÃ­nh toÃ¡n. Äiá»u nÃ y Ä‘Ã²i há»i giáº£m thiá»ƒu bias, thÃºc Ä‘áº©y cÃ´ng báº±ng vÃ  báº£o tá»“n quyá»n riÃªng tÆ°, do Ä‘Ã³ thÃºc Ä‘áº©y triá»ƒn khai cÃ³ Ä‘áº¡o Ä‘á»©c cá»§a cÃ´ng nghá»‡.

Lá»i cáº£m Æ¡n

ChÃºng tÃ´i muá»‘n cáº£m Æ¡n Christian Hudon vÃ¬ há»— trá»£ ká»¹ thuáº­t vÃ  cho phÃ©p má»Ÿ rá»™ng quy mÃ´ cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i. ChÃºng tÃ´i cÅ©ng cáº£m Æ¡n Siva Reddy vÃ¬ nhá»¯ng tháº£o luáº­n sÃ¢u sáº¯c giÃºp Ä‘á»‹nh hÃ¬nh cÃ´ng trÃ¬nh nÃ y.

TÃ i liá»‡u tham kháº£o

[Pháº§n tÃ i liá»‡u tham kháº£o Ä‘Æ°á»£c dá»‹ch tÆ°Æ¡ng tá»± vá»›i format vÃ  ná»™i dung nhÆ° báº£n gá»‘c, bao gá»“m táº¥t cáº£ cÃ¡c trÃ­ch dáº«n tá»« Vaibhav Adlakha Ä‘áº¿n Andy Zou et al.]

--- TRANG 14 ---
A Chi phÃ­ thá»±c táº¿ cá»§a caching

Máº·c dÃ¹ Ä‘á»™ phá»©c táº¡p tuyáº¿n tÃ­nh theo Ä‘á»™ dÃ i ngá»¯ cáº£nh cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c trong khi duy trÃ¬ cháº¥t lÆ°á»£ng ngá»¯ cáº£nh, cÃ¡c cÃ¢n nháº¯c thá»±c táº¿ cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng tiÃªu cá»±c Ä‘áº¿n thiáº¿t láº­p cÃ³ cache.

Chi phÃ­ táº£i vÃ  truyá»n thÃ´ng. Máº·c dÃ¹ chi phÃ­ lÆ°u trá»¯ dÃ i háº¡n tÆ°Æ¡ng Ä‘á»‘i tháº¥p, Ä‘á»™ trá»… táº£i vÃ  truyá»n thÃ´ng tá»· lá»‡ thuáº­n vá»›i dáº¥u chÃ¢n khÃ´ng gian cá»§a ná»™i dung Ä‘Æ°á»£c cache. Hai nÃºt cá»• chai chÃ­nh á»Ÿ Ä‘Ã¢y lÃ  táº£i cache tá»« Ä‘Ä©a (náº¿u sá»­ dá»¥ng lÆ°u trá»¯ Ä‘Ä©a), vÃ  di chuyá»ƒn cache Ä‘áº¿n bá»™ nhá»› GPU. Náº¿u tá»•ng lÆ°á»£ng thÃ´ng tin cáº§n cache vá»«a vá»›i CPU RAM, cÃ³ thá»ƒ khÃ´ng cáº§n lÆ°u trá»¯ chÃºng trÃªn Ä‘Ä©a trong quÃ¡ trÃ¬nh suy luáº­n, cho phÃ©p lá»£i Ã­ch tá»‘t hÆ¡n tuyáº¿n tÃ­nh trÃªn kÃ­ch thÆ°á»›c caching nhá» hÆ¡n.

PhÃ©p toÃ¡n bá»• sung. Trong trÆ°á»ng há»£p LLAMA 2, JIT-KV caching yÃªu cáº§u cache nhá» hÆ¡n má»™t ná»­a so vá»›i KV - 254 kB thay vÃ¬ 512 kB trÃªn má»—i token - vÃ  do Ä‘Ã³ rÃµ rÃ ng cÃ³ lá»£i tháº¿ hÆ¡n vá» máº·t táº£i vÃ  truyá»n thÃ´ng. Tuy nhiÃªn, cÃ¡c phÃ©p toÃ¡n bá»• sung Ä‘Æ°á»£c yÃªu cáº§u táº¡i thá»i Ä‘iá»ƒm suy luáº­n Ä‘á»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c tráº¡ng thÃ¡i áº©n thÃ nh key vÃ  value mÃ  mÃ´ hÃ¬nh yÃªu cáº§u. á» Ä‘Ã¢y cÃ¡c phÃ©p toÃ¡n bá»• sung nÃ y hÃ³a ra lÃ  cÃ¡c phÃ©p toÃ¡n tuyáº¿n tÃ­nh ráº». TÆ°Æ¡ng tá»±, cÃ¡c cÃ¡ch tiáº¿p cáº­n XC caching cá»§a chÃºng tÃ´i yÃªu cáº§u ráº¥t Ã­t lÆ°u trá»¯ (tá»©c lÃ  8 kB vÃ  1.5 kB trÃªn má»—i token), Ä‘Ã¢y lÃ  chiáº¿n tháº¯ng rÃµ rÃ ng vá» máº·t táº£i vÃ  truyá»n thÃ´ng. Tuy nhiÃªn, cÃ¡c lá»›p bá»• sung Ä‘Æ°á»£c thÃªm vÃ o mÃ´ hÃ¬nh gá»‘c trong Pháº§n 3), Ä‘iá»u nÃ y gÃ¢y ra chi phÃ­ thá»i gian pháº£i Ä‘Æ°á»£c tÃ­nh Ä‘áº¿n trong tÄƒng tá»‘c tá»•ng thá»ƒ. Má»™t láº§n ná»¯a, cÃ¡c phÃ©p toÃ¡n bá»• sung nÃ y hÃ³a ra lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ so vá»›i lá»£i Ã­ch cá»§a caching.

Chi phÃ­ cháº¥t lÆ°á»£ng rÃµ rÃ ng. KV vÃ  JIT-KV caching Ä‘á»u tráº£ vá» chÃ­nh xÃ¡c cÃ¹ng cÃ¢u tráº£ lá»i nhÆ° mÃ´ hÃ¬nh khÃ´ng cache tÆ°Æ¡ng á»©ng: cÃ¡c chiáº¿n lÆ°á»£c caching nÃ y khÃ´ng phÃ¡t sinh chi phÃ­ (rÃµ rÃ ng) vá» cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i so vá»›i mÃ´ hÃ¬nh khÃ´ng cache Ä‘Ã³. NgÆ°á»£c láº¡i, chiáº¿n lÆ°á»£c XC caching cá»§a chÃºng tÃ´i khÃ´ng Ä‘áº£m báº£o Ä‘iá»u nÃ y: pháº£i xem xÃ©t sá»± cÃ¢n báº±ng giá»¯a chi phÃ­ vÃ  lá»£i Ã­ch.

Chi phÃ­ cháº¥t lÆ°á»£ng áº©n. Táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p caching Ä‘Æ°á»£c xem xÃ©t trong cÃ´ng trÃ¬nh nÃ y Ä‘á»u cÃ³ cÃ¹ng "chi phÃ­ cÆ¡ há»™i" áº©n: viá»‡c xá»­ lÃ½ ngá»¯ cáº£nh offline cÃ³ nghÄ©a lÃ  cache káº¿t quáº£ Ä‘á»™c láº­p vá»›i Ä‘áº·c Ä‘iá»ƒm cá»¥ thá»ƒ cá»§a truy váº¥n. TÃ¹y thuá»™c vÃ o trÆ°á»ng há»£p sá»­ dá»¥ng, Ä‘iá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n máº¥t mÃ¡t cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i. VÃ­ dá»¥, trong thiáº¿t láº­p QA, viá»‡c biáº¿t cÃ¢u há»i khi xá»­ lÃ½ ngá»¯ cáº£nh cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o thÃ´ng tin há»¯u Ã­ch Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i. CÃ³ thá»ƒ nÃ³i, Ä‘iá»u nÃ y dáº«n Ä‘áº¿n cÃ¡c biá»ƒu diá»…n ngá»¯ cáº£nh tá»‘t hÆ¡n vÃ¬ nÃ³ giáº£m nguy cÆ¡ lÃ m nháº§m LLM vá»›i thÃ´ng tin khÃ´ng liÃªn quan.

B Timing cache loading

[Báº£ng 4 trÃ¬nh bÃ y so sÃ¡nh thá»i gian táº£i cache ngá»¯ cáº£nh tá»« Ä‘Ä©a Ä‘áº¿n bá»™ nhá»› thiáº¿t bá»‹ cho má»—i cÃ¡ch tiáº¿p cáº­n vá»›i cÃ¡c Ä‘á»™ dÃ i chuá»—i khÃ¡c nhau vÃ  káº¿t quáº£ cho tháº¥y kÃ­ch thÆ°á»›c cache giáº£m do XC-cache lÃ m giáº£m thá»i gian táº£i hai báº­c Ä‘á»™ lá»›n.]

C Dataset

[Pháº§n nÃ y mÃ´ táº£ chi tiáº¿t vá» cÃ¡c dataset huáº¥n luyá»‡n bao gá»“m Natural Questions, HotpotQA, TopiOCQA, MS MARCO vÃ  Squad V2 vá»›i thá»‘ng kÃª vá» sá»‘ lÆ°á»£ng máº«u train/valid/test.]

D Táº­p há»£p káº¿t quáº£ Ä‘áº§y Ä‘á»§

[Báº£ng 6 chá»©a káº¿t quáº£ cho táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p, metric vÃ  táº¥t cáº£ cÃ¡c dataset Ä‘Æ°á»£c tÃ­nh Ä‘áº¿n trong Ä‘Ã¡nh giÃ¡ thá»±c nghiá»‡m.]

E Káº¿t quáº£ suy luáº­n khÃ´ng cÃ³ ngá»¯ cáº£nh

[ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c LLM Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c trÃªn táº­p test cá»§a cÃ¡c dataset mÃ  khÃ´ng truyá»n ngá»¯ cáº£nh Ä‘á»ƒ cÃ³ cáº£m nháº­n tá»‘t hÆ¡n vá» sá»‘ lÆ°á»£ng cÃ¢u tráº£ lá»i chÃºng Ä‘Ã£ biáº¿t mÃ  khÃ´ng cáº§n ICL.]

F Chi tiáº¿t thÃªm vá» baseline

[Pháº§n nÃ y cung cáº¥p chi tiáº¿t vá» cÃ i Ä‘áº·t GPT, fine-tuning LoRA vÃ  chi tiáº¿t LLAMA 2-Chat bao gá»“m cÃ¡c template prompt Ä‘Æ°á»£c sá»­ dá»¥ng.]

G PhÃ¢n tÃ­ch Ä‘á»™ nháº¡y: vá»‹ trÃ­ cá»§a ngá»¯ cáº£nh tham kháº£o

[ChÃºng tÃ´i khÃ¡m phÃ¡ Ä‘á»™ nháº¡y cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘á»‘i vá»›i vá»‹ trÃ­ cá»§a ngá»¯ cáº£nh tham kháº£o trong ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ vÃ  tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c fine-tune Ã­t bá»‹ bias Ä‘á»‘i vá»›i cÃ¡c pháº§n nháº¥t Ä‘á»‹nh cá»§a ngá»¯ cáº£nh hÆ¡n.]

H SiÃªu tham sá»‘ huáº¥n luyá»‡n

[Báº£ng 9 bÃ¡o cÃ¡o cÃ¡c siÃªu tham sá»‘ huáº¥n luyá»‡n chÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng cho XC-LLAMA bao gá»“m optimizer, learning rate, batch size vÃ  cÃ¡c cÃ i Ä‘áº·t kiáº¿n trÃºc.]
