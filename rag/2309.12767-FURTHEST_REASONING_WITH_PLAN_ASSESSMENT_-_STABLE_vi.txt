# 2309.12767.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2309.12767.pdf
# Kích thước tệp: 1323851 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LẬP LUẬN XA NHẤT VỚI ĐÁNH GIÁ KẾ HOẠCH: CON ĐƯỜNG LẬP LUẬN ỔN ĐỊNH VỚI CÁC MÔ HÌNH NGÔN NGỮ LỚN TĂNG CƯỜNG TRUY VẤN

Yin Zhu∗
Đại học Nam Kinh
yinzhu@smail.nju.edu.cn

Zhiling Luo†
Học viện DAMO, Tập đoàn Alibaba
godot.lzl@alibaba-inc.com

Gong Cheng
Đại học Nam Kinh
gcheng@nju.edu.cn

TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLM), hoạt động như một người lập luận và trình tạo mạnh mẽ, thể hiện hiệu suất xuất sắc trên nhiều tác vụ ngôn ngữ tự nhiên khác nhau, chẳng hạn như trả lời câu hỏi (QA). Trong số các tác vụ này, Trả lời Câu hỏi Đa Bước (MHQA) đứng như một danh mục được thảo luận rộng rãi, đòi hỏi sự tích hợp liền mạch giữa LLM và việc truy xuất kiến thức bên ngoài. Các phương pháp hiện có sử dụng LLM để tạo ra các đường dẫn lập luận và kế hoạch, và sử dụng IR để truy xuất kiến thức liên quan một cách lặp đi lặp lại, nhưng các cách tiếp cận này có những khuyết điểm cố hữu. Một mặt, Bộ Truy xuất Thông tin (IR) bị cản trở bởi chất lượng thấp của các truy vấn được tạo ra bởi LLM. Mặt khác, LLM dễ dàng bị đánh lạc hướng bởi kiến thức không liên quan từ IR. Những sai sót này, tích tụ bởi tương tác lặp đi lặp lại giữa IR và LLM, dẫn đến thảm họa về hiệu quả ở cuối. Để vượt qua các rào cản trên, trong bài báo này, chúng tôi đề xuất một quy trình mới cho MHQA gọi là Lập-luận-xa-nhất-với-Đánh-giá-Kế-hoạch (FuRePA), bao gồm một framework cải tiến (Lập luận Xa nhất) và một mô-đun đính kèm (Bộ Đánh giá Kế hoạch). 1) Lập luận xa nhất hoạt động bằng cách che giấu đường dẫn lập luận trước đó và các truy vấn được tạo ra cho LLM, khuyến khích LLM tạo ra chuỗi suy nghĩ từ đầu trong mỗi lần lặp. Cách tiếp cận này cho phép LLM phá vỡ xiềng xích được xây dựng bởi những suy nghĩ và truy vấn sai lệch trước đó (nếu có). 2) Bộ Đánh giá Kế hoạch là một trình đánh giá được huấn luyện để chọn một kế hoạch phù hợp từ một nhóm các kế hoạch ứng viên được đề xuất bởi LLM. Các phương pháp của chúng tôi được đánh giá trên ba tập dữ liệu trả lời câu hỏi đa bước được công nhận cao và vượt trội hơn state-of-the-art trên hầu hết các chỉ số (đạt được 10%-12% về độ chính xác câu trả lời).

Từ khóa Trả lời Câu hỏi Đa bước · Đánh giá Kế hoạch · Lập luận Xa nhất

1 Giới thiệu

Vấn đề Trả lời Câu hỏi (QA) từ lâu đã thu hút sự chú ý nghiên cứu đáng kể. Khi các mô hình được cải tiến dần dần cung cấp hiệu suất xuất sắc trên các tác vụ QA đơn giản, các nhà nghiên cứu gần đây đã chuyển sự chú ý của họ sang các câu hỏi phức tạp hơn. Trong số này, Trả lời Câu hỏi Đa Bước (MHQA) đã cho thấy tiềm năng và thách thức to lớn qua các năm. ([1], [2], [3], [4], [5], [6] v.v.) Để giải quyết các câu hỏi đa bước phức tạp, khả năng lập luận và truy xuất đa bước là thiết yếu và được yêu cầu, do đó thúc đẩy sự cải tiến toàn diện của các mô hình xử lý ngôn ngữ tự nhiên. Trong tác vụ phức tạp này, một đường dẫn lập luận liên tục và có thể truy cập được là rất quan trọng cho độ chính xác và khả năng diễn giải, vì câu trả lời được thu được sau khi kết hợp nhiều bước lập luận.

Với sự gia tăng nhanh chóng của các Mô hình Ngôn ngữ Lớn (LLM) mạnh mẽ và khả năng CoT [7] của chúng, một phương pháp cơ bản trực quan và được triển khai rộng rãi để giải quyết MHQA là sử dụng LLM và IR xen kẽ. Thông thường, ở mỗi bước, LLM lấy các bước lập luận trước đó, thực hiện các truy vấn và truy xuất bằng chứng, sau đó đưa ra truy vấn tiếp theo. IR sau đó sử dụng truy vấn này để truy xuất thông tin liên quan. Vòng lặp này tiếp tục cho đến khi LLM có được tất cả bằng chứng cần thiết và

1Công việc được thực hiện trong thời gian thực tập tại Học viện DAMO, Tập đoàn Alibaba
2Tác giả liên lạc

--- TRANG 2 ---
Tiêu đề Chạy cho Header

Hình 1: Ví dụ cho thấy tầm quan trọng của việc chọn truy vấn chất lượng cao. Bộ Truy xuất Thông tin phát hiện các tài liệu liên quan từ kho dữ liệu với truy vấn. Truy vấn đầu tiên gọi lại sự thật cơ bản ở vị trí thứ 10 vì nó không đặc biệt và nhiều tài liệu có liên quan cao gây nhầm lẫn cho IR. Truy vấn thứ hai có chất lượng cao gọi lại sự thật cơ bản ở top-1 với ít tài liệu có liên quan cao hơn do sự phân biệt của nó trong kho dữ liệu.

trả lời câu hỏi. [8], [9], [10], [11] áp dụng framework cơ bản này. Mặc dù hiệu suất tuyệt vời của LLM trên các tác vụ ngôn ngữ tự nhiên, có một số khó khăn trong việc giải quyết MHQA với các phương pháp tăng cường bộ truy xuất.

Thách thức lớn đầu tiên nằm ở việc đánh giá và chọn lựa truy vấn chất lượng cao được phân tách bởi LLM. Cụ thể hơn, với khả năng đa dạng hóa tạo ra, LLM tạo ra một nhóm các kế hoạch ứng viên có sẵn. Các kế hoạch khác nhau nâng cao trần của các truy vấn chất lượng cao, nhưng các truy vấn chất lượng thấp cũng xuất hiện trong nhóm, cần được loại bỏ. Nếu IR lấy truy vấn chất lượng thấp, bằng chứng được truy xuất rất có thể sẽ không liên quan hoặc không hữu ích. Trong lần lặp tiếp theo, LLM tin tưởng bằng chứng sai này và đưa ra những kế hoạch thậm chí tệ hơn, lệch hướng đáng kể khỏi đường dẫn lập luận đúng. Ví dụ, như được hiển thị trong Hình 1, LLM tạo ra một số truy vấn khác nhau cho câu hỏi gốc. Truy vấn đầu tiên tập trung vào "NCAA Division", trong khi truy vấn thứ hai nhấn mạnh thông tin về "a record of 2-6". Trong quá trình truy xuất, hóa ra các tài liệu khổng lồ mô tả các đội Colorado Buffaloes của các môn thể thao và mùa giải khác nhau trong NCAA division, vì vậy truy vấn đầu tiên thiếu sự phân biệt và dẫn đến bằng chứng được truy xuất sai. Sự thật cơ bản tương ứng xếp hạng thứ 10, thấp hơn nhiều tài liệu khác chứa "NCAA Division". Ngược lại, truy vấn thứ hai có nội dung đặc biệt, giúp IR dễ dàng chọn tài liệu mục tiêu. Tuy nhiên, các phương pháp hiện có không tận dụng sự đa dạng tạo ra và phụ thuộc nhiều vào truy vấn đơn lẻ được tạo ra bởi LLM mà không có bất kỳ đánh giá nào, điều này gây ra thiệt hại không thể đảo ngược. Như một cơ chế bổ sung, self-consistency [12] chủ yếu nhắm vào các tác vụ mà một tập hợp đa dạng các đường dẫn lập luận có thể trực tiếp dẫn đến cùng kết quả mà không có bất kỳ truy vấn nào ở giữa các bước lập luận.

Tương tác lặp đi lặp lại giữa LLM và IR làm gián đoạn đường dẫn lập luận CoT, điều quan trọng trong MHQA, vì vậy duy trì tính liên tục của đường dẫn lập luận là một thách thức quan trọng khác. Đường dẫn lập luận mong đợi lệch hướng ở hai giai đoạn, kế hoạch LLM và truy xuất IR, vì LLM và IR đều là hộp đen trong quá trình giải pháp. Đầu tiên, khi lấy nội dung lập luận trước đó, truy vấn và bằng chứng làm đầu vào, LLM đưa ra kế hoạch không chính xác do ảo giác và quên lãng. Thứ hai, mặc dù truy vấn đủ tốt, IR vẫn không thể có được bằng chứng đúng. Khi cuộc trò chuyện và lập luận tiến triển, sự lệch hướng kế hoạch và truy xuất xảy ra thỉnh thoảng. Các nhà nghiên cứu đã nỗ lực đáng kể để giải quyết thách thức này với nhiều cách khác nhau để tránh phá vỡ chuỗi lập luận. ChatCot [11] kết hợp LLM và IR với các cuộc trò chuyện đa lượt để ngăn chặn sự gián đoạn công cụ ảnh hưởng đến tính liên tục của lập luận trong chuỗi suy nghĩ. Đáng chú ý rằng ChatCot đánh giá giá trị của các tài liệu được truy xuất để giúp làm rõ quá trình lập luận cho LLM. Tuy nhiên, trong thiết lập few-shot, quá nhiều vòng trò chuyện không tránh khỏi dẫn đến việc bỏ qua lập luận hoặc hướng dẫn trước đó, do đó gây ra đường dẫn lập luận phân kỳ. Trong [9], LLM tạo ra chuỗi lập luận toàn cục

--- TRANG 3 ---
Tiêu đề Chạy cho Header

ở mỗi lần lặp để tránh sự không liên tục, nhưng gặp phải vấn đề tương tự như ChatCot. Iter-RetGen [10] coi mỗi lần lặp như một quá trình QA hoàn chỉnh. Phương pháp này xây dựng một đường dẫn lập luận nguyên vẹn ở mỗi lần lặp với ít nguy cơ bỏ qua, nhưng không có bất kỳ cơ chế thích ứng nào như dừng sớm hoặc ngưỡng tin cậy, các lần lặp quá mức và tài liệu hỗ trợ có thể dẫn đến đường dẫn lập luận không chắc chắn và kết quả tệ hơn, như đã được hiển thị.

Trong bài báo này, chúng tôi đề xuất một framework gọi là Lập luận Xa nhất với Đánh giá Kế hoạch (FuRePA) để giải quyết các thách thức trên (Hình 2). Ở mỗi lần lặp t, LLM chỉ có quyền truy cập vào bằng chứng được truy xuất và câu hỏi gốc, sau đó dự đoán các kế hoạch tiếp theo thông qua học tập trong ngữ cảnh. Không giống như các phương pháp tương tác thông thường, các chuỗi lập luận và câu hỏi trước đó luôn được che giấu, buộc LLM phải lập luận từ đầu đến cuối. Thông qua cơ chế này, LLM phân tích ngữ cảnh và lập luận mà không bị ảnh hưởng bởi đường dẫn trước đó. Quá trình lập luận cũng phục hồi từ bằng chứng sai do IR gây ra.

Các kế hoạch bao gồm xác định câu trả lời khi tất cả tài liệu cần thiết được trình bày, và thực thi truy vấn khi cần thêm thông tin liên quan. Một bộ đánh giá kế hoạch được huấn luyện (phần bên phải của Hình 2) sau đó phân tích các kế hoạch ứng viên và chọn kế hoạch tốt nhất. Đối với các truy vấn khác nhau, bộ chấm điểm trong bộ đánh giá ước tính MRR của chúng liên quan đến ngữ cảnh miền đóng được lấy mẫu. Truy vấn có điểm số cao nhất sau đó được thực thi để truy xuất một tài liệu mới. Như ví dụ trong hình cho thấy, ở lần lặp 0 một số truy vấn được tạo ra và bộ đánh giá kế hoạch chọn cái thứ hai. Bằng chứng được truy xuất e0 được thêm vào tập hợp. Sau đó, ở lần lặp 1, LLM chỉ được cung cấp bằng chứng và câu hỏi gốc và đưa ra một số kế hoạch. Câu hỏi đầu tiên "what year was the 14th season ..." được sử dụng để truy xuất bằng chứng e1. Ở lần lặp 2, bộ đánh giá kế hoạch chọn trả lời câu hỏi và kết thúc quá trình.

Chúng tôi đánh giá framework của mình trên ba tác vụ đa bước. Phương pháp của chúng tôi đạt được khoảng 10% trên hầu hết các chỉ số so với state of the art trong thiết lập few-shot. Với nghiên cứu loại bỏ và nghiên cứu trường hợp, quy trình và chiến lược tối ưu hóa của chúng tôi đưa ra kết luận như mong đợi. Cơ chế che giấu và lập luận từ đầu trong framework của chúng tôi hiệu quả xử lý thách thức về tính liên tục đường dẫn lập luận đã đề cập, cũng như tránh quên lãng LLM và cải thiện quá trình lập luận. Quy trình đánh giá kế hoạch tạo ra và chọn lựa kế hoạch một cách sáng tạo hơn, bao gồm viết lại tốt hơn và truy vấn chính xác hơn.

Chúng tôi tóm tắt các đóng góp như sau:
• Một bộ đánh giá kế hoạch được huấn luyện là một mô-đun ước tính điểm MRR và chọn truy vấn chất lượng cao, gọi lại các văn bản tương tự. Tìm kiếm chất lượng cao cải thiện truy xuất và xác định tài liệu chứa bằng chứng tiềm năng.
• Một framework có tên Lập luận Xa nhất duy trì tính liên tục đường dẫn lập luận và hiệu quả giảm thiểu tối đa tác hại từ chuỗi suy nghĩ hoặc truy vấn trước đó bằng cách che giấu chúng.

2 Công trình Liên quan

Prompting Chuỗi Suy nghĩ Kể từ khi các mô hình ngôn ngữ lớn mang lại triển vọng thú vị về học tập few-shot trong ngữ cảnh thông qua các prompting khác nhau, Chuỗi Suy nghĩ [7] được đề xuất để cải thiện khả năng lập luận bằng cách làm rõ các bước lập luận trung gian. Theo công trình tiên phong, nhiều công trình về gợi ra lập luận trong LLM đã được nghiên cứu. Prompting "Step by Step" [13] được chứng minh là hiệu quả trong thiết lập zero-shot. Auto-CoT [14] tự động hóa việc xây dựng các bài trình bày, xây dựng các minh họa bằng cách lấy mẫu các câu hỏi phong phú và tạo ra các chuỗi lập luận tương ứng thay vì thiết kế thủ công. Self-consistency [12] là một chiến lược giải mã sử dụng ensemble để thay thế tìm kiếm tham lam trên cơ sở few-shot-cot để cải thiện độ chính xác giải mã. Least-to-Most [15] hướng dẫn mô hình phân tách vấn đề thành các câu hỏi con và trả lời các câu hỏi con một cách lặp đi lặp lại ở mỗi bước. Các phương pháp tương tự bao gồm Self-Ask [16], ReAct [8] v.v., tất cả đều áp dụng cho các câu hỏi đa bước. Để kết luận, chuỗi suy nghĩ, như một phương pháp để giới thiệu tính mạch lạc cấu trúc và logic trong các mô hình tạo sinh, mở rộng ứng dụng của LLM cho nhiều tác vụ khác nhau. Bằng cách giới thiệu các gợi ý có mục tiêu, nó được kỳ vọng sẽ cải thiện chất lượng tạo ra và đóng vai trò quan trọng trong nhiều tác vụ xử lý ngôn ngữ tự nhiên phức tạp, hầu hết đều yêu cầu một loạt các bước lập luận.

Phân tách Truy vấn Phân tách và tối ưu hóa truy vấn là rất quan trọng trong MHQA và các tác vụ lập luận đa bước khác. [17] phân tách các vấn đề khó thành các nhiệm vụ con đơn giản hơn. LLM trong bộ phân tách và bộ xử lý nhiệm vụ con lấy các prompting few-shot tương ứng. Plan-and-Solve [18] cũng thực hiện phân tách ngay từ đầu và thực hiện nó. HPE [19] phân tích câu hỏi đa bước thành một biểu diễn trung gian có cấu trúc bao gồm các câu hỏi đơn giản, biểu diễn các mối quan hệ. [20] cố gắng khám phá các phương pháp hiệu quả hơn CoT để gợi ra lập luận trung thực thông qua phân tách và tái tổ hợp nhân tố. Tuy nhiên, hầu hết các phương pháp trên chỉ làm cho LLM tạo ra các truy vấn và tin tưởng chúng mà không có bất kỳ tối ưu hóa hoặc đánh giá nào. Thực tế, LLM đôi khi đưa ra các kế hoạch không thỏa mãn, không có lợi cho các bước tiếp theo.

--- TRANG 4 ---
Tiêu đề Chạy cho Header

Hình 2: Chi tiết lặp trong framework của chúng tôi được hiển thị ở phần bên phải. Trong mỗi lần lặp t, LLM tạo ra chuỗi lập luận và kế hoạch (ví dụ: truy vấn hoặc câu trả lời) với ngữ cảnh được truy xuất và câu hỏi gốc làm đầu vào. Mô-đun bộ đánh giá kế hoạch xử lý các kế hoạch và chọn kế hoạch tốt nhất. Phần bên trái là quá trình giải quyết ví dụ của Hình 1

Tăng cường Truy xuất Ý tưởng tăng cường truy xuất được sử dụng rộng rãi trong tất cả các loại mô hình. Ví dụ, Realm [21] khám phá trả lời câu hỏi trích xuất miền mở, và các mô hình như RAG [22] đưa truy xuất vào các mô hình tạo sinh. Nhờ LLM phát triển nhanh chóng, các phương pháp khác nhau của các mô hình ngôn ngữ lớn tăng cường truy xuất cung cấp các giải pháp đầy hứa hẹn cho các tác vụ QA chuyên sâu về kiến thức phức tạp. In-context RALM [23] và REPLUG [24] để nguyên kiến trúc LM và thêm vào đầu các tài liệu được truy xuất vào đầu vào.

Chỉ việc truy xuất các tài liệu liên quan mới có tác dụng tích cực, trong khi những tài liệu không liên quan cũng có thể làm tổn hại chất lượng tạo ra và lập luận. [25] đánh giá tầm quan trọng dữ liệu và chất lượng của các điểm dữ liệu được truy xuất. [26] huấn luyện bộ viết lại truy vấn trong truy xuất với học tăng cường policy gradient. Ngoài ra, mô-đun truy xuất được thêm mới trong quy trình mang lại áp lực tính toán tài nguyên (không chỉ truy xuất yêu cầu tài nguyên, mà mô hình được tạo ra cũng tiêu thụ nhiều tài nguyên hơn do cần xem xét kết quả truy xuất), điều này cũng yêu cầu đánh giá thêm.

Khi giới thiệu thông tin bên ngoài vào các mô hình ngôn ngữ, các công trình gần đây không cố gắng giảm thiểu tác động của các tài liệu không liên quan khi chúng được truy xuất và tin cậy. Để tận dụng sự cải thiện về chất lượng truy xuất và chi phí tính toán lớn trong LLM, công trình của chúng tôi nỗ lực làm suy yếu tác động không thể tránh khỏi từ IR, tránh tích tụ lỗi nhiều hơn trong quy trình tiếp theo.

3 Phương pháp

Câu hỏi gốc Q và một kho truy xuất D={d} phục vụ như đầu vào của thể hiện MHQA.

3.1 Lập luận Xa nhất

Trong mỗi lần lặp t với giới hạn trên T, LLM M chấp nhận các bằng chứng được truy xuất Et và câu hỏi gốc Q được tích hợp làm đầu vào. Liên quan đến nhiệt độ tp và n lựa chọn trong các tham số, LLM tạo ra một đầu ra Pt={pi|1≤i≤n}, bao gồm n kế hoạch ứng viên. Vòng tuần hoàn được hiển thị trong Hình 2. Đầu vào và đầu ra của LLM được biểu diễn như:

Pt=M(prompt(Et,Q,sgn(t)), tp0+ Δtp, n),∀0≤t < T (1)

--- TRANG 5 ---
Tiêu đề Chạy cho Header

sgn(t) là một hàm dấu, cho biết liệu bước có phải là bước đầu tiên hay không. Các prompts ở đầu khác với những cái sau đó vì chúng ta chắc chắn không thể có được câu trả lời mà không có bất kỳ tài liệu được truy xuất nào. Do đó, các prompts trong lần lặp đầu tiên được thiết kế để lấy câu hỏi gốc và chỉ đưa ra các truy vấn.

3.1.1 Prompting

Vì vòng lập luận sgn(t) đã biết, chiến lược prompting chi tiết được triển khai trên các bằng chứng được truy xuất Et và câu hỏi Q. Ví dụ, mẫu hướng dẫn hệ thống ở các bước sau là:

System: "Bạn được đưa ra các vấn đề yêu cầu tìm kiếm và lập luận đa bước với các ngữ cảnh liên quan. Bạn nên cho tôi biết câu trả lời nếu tất cả thông tin cần thiết được cung cấp trong ngữ cảnh, nếu không hãy cho tôi biết thông tin cụ thể nào cần tìm kiếm tiếp theo. Đừng trả lời với sự không chắc chắn. Các dòng chỉ bắt đầu bằng '[Analysis]' hoặc '[Answer]' hoặc '[Search]'."

Hướng dẫn này yêu cầu LLM hiểu ngữ cảnh (các bằng chứng được truy xuất ở đây) và câu hỏi, và tạo ra chuỗi lập luận liên tục từ đầu đến trạng thái xa nhất có thể, tiếp theo là kế hoạch như câu trả lời hoặc truy vấn. Kế hoạch được tạo ra thông qua chuỗi suy nghĩ, bao gồm một chuỗi lập luận r và kế hoạch tiếp theo q hoặc a. Các ví dụ mẫu và các mẫu phản hồi mong đợi như sau:

User: "Context: Et Question: Q"
Assistant: "[Analysis] r [Search] q" hoặc "[Analysis] r [Answer] a"

Ở đây chúng tôi prompt LLM để lên kế hoạch cho bước tiếp theo chỉ với các bằng chứng được truy xuất và câu hỏi gốc. Đầu ra của LLM là một nhóm các kế hoạch Pt được định nghĩa bởi:

Pt:={pt1, pt2,···, ptn}
={(rt1, qt1),(rt2, qt2),···,(rtk, qtk)} ∪ {(rtk+1, at1),(rtk+2, at2),···,(rtn, atl)}
=Qt∪At (2)

Không giống như nhiều phương pháp tổng quát khác, chúng tôi che giấu nội dung lập luận trước đó và truy vấn đã thực thi vì lập luận và kế hoạch trước đó có tác động tiêu cực đến quá trình lập luận đa bước, điều được chứng minh trong mục 4. Do đó, LLM sẽ phân tích câu hỏi đa bước từ đầu theo các bằng chứng được truy xuất, bất kể chúng có giúp ích hay không, ngăn chặn các kế hoạch mới bị ảnh hưởng bởi lập luận và bằng chứng sai lệch trước đó. Đáng chú ý rằng trong các ví dụ few-shot, chúng tôi dạy LLM phân biệt các bằng chứng tương tự nhưng không liên quan khỏi những cái thực sự cần thiết, điều này tiếp tục giảm thiểu thông tin dư thừa do IR gây ra. Các kế hoạch đầu ra trên bao gồm k lần thực thi truy vấn {qti|1≤i≤k} và l lần xác định câu trả lời {atj|1≤j≤l} trong lần lặp t. Mỗi kế hoạch được tạo ra thông qua CoT rti.

3.1.2 Lặp cho Lập luận Xa nhất

Chúng tôi lặp lại bước lập luận xa nhất tối đa T lần. Sau khi chúng tôi có Pt ở bước t, bộ đánh giá kế hoạch (mục 3.2) được sử dụng để chọn kế hoạch tốt nhất pbest trong số chúng. Nếu pbest tuyên bố câu trả lời cuối cùng a, thì toàn bộ lặp kết thúc với đường dẫn lập luận r và câu trả lời a. Ngược lại pbest là một lần thực thi truy vấn q. Mô-đun truy xuất sắp xếp các tài liệu trong D và thêm tài liệu mới hàng đầu d vào tập bằng chứng được truy xuất Et, được định nghĩa bởi:

Et=∅ nếu t= 0
Et−1∪ {d}, trong đó d∉ Et−1 và d∈ D, ngược lại. (3)

Sau đó lần lặp tiếp theo chạy với ngữ cảnh được truy xuất mới Et+1. Câu trả lời sẽ được tạo ra bắt buộc nếu không được xác định sau vòng cuối.

3.2 Bộ Đánh giá Kế hoạch

Bộ đánh giá kế hoạch A là một mô-đun lấy các kế hoạch ứng viên khác nhau Pt được tạo ra bởi LLM và đưa ra kế hoạch tốt nhất pbest:

pbest=A(t,Pt, Et,D) (4)

Trong mỗi lần lặp trong Hình 2, LLM đưa ra các chuỗi suy nghĩ khác nhau và các kế hoạch tiếp theo, bao gồm thực thi truy vấn và xác định câu trả lời. Điều mà bộ đánh giá kế hoạch làm là bầu chọn cái phù hợp nhất, để xác suất của truy vấn chất lượng thấp và câu trả lời được suy đoán ngẫu nhiên được giảm và toàn bộ quá trình lập luận trở nên thuyết phục hơn. Et là tập hợp các truy vấn đã thực thi.

--- TRANG 6 ---
Tiêu đề Chạy cho Header

3.2.1 Quyết định Kế hoạch

Ngoài bước đầu tiên không thể tạo ra câu trả lời, các kế hoạch ứng viên có thể bao gồm xác định câu trả lời và thực thi truy vấn như được hiển thị trong mục 3.1. Quyết định kế hoạch được thực hiện bằng thuật toán bỏ phiếu. Khi tỷ lệ các kế hoạch trả lời trong Pt vượt quá ngưỡng nhất định θ, quá trình lập luận kết thúc với một cặp đường dẫn lập luận hoàn chỉnh và câu trả lời. Nếu không, các kế hoạch còn lại của thực thi truy vấn được sàng lọc và xếp hạng bởi bộ lọc truy vấn (mục 3.2.2) và bộ chấm điểm (mục 3.2.3). Ở lần lặp 2 trong Hình 2, khi hầu hết các kế hoạch cung cấp câu trả lời cuối cùng là "2009, Big 12 Conference", bất kỳ kế hoạch trả lời nào và CoT của nó được lấy làm kết quả. Hàm được biểu diễn như:

PlanDecision(t,Pt) = (
At0, nếu t >0 và |At|/|Pt|≥θ
Qt, ngược lại (5)

3.2.2 Bộ Lọc Truy vấn

Chúng tôi lọc các kế hoạch truy vấn Qt và nhận Q̂t với phương pháp phân cụm dựa trên mật độ để loại bỏ các truy vấn giống như trước đó. Q̂t bao gồm các ví dụ trung tâm từ mỗi cụm. Trong trường hợp Q̂t trống, có nghĩa là các truy vấn tương tự đã được thực thi trước đó, lần lặp này kết thúc với việc tăng nhiệt độ Δtp. Khi các truy vấn giống hệt nhau được tạo ra lặp đi lặp lại, có nghĩa là quy trình gặp phải các truy vấn chất lượng thấp hoặc thiếu bằng chứng. Tăng nhiệt độ (thay đổi ngưỡng xác suất giải mã) thay thế việc giải mã tham lam được sử dụng trong prompting chuỗi suy nghĩ bằng các đường dẫn lập luận và kế hoạch đa dạng hơn. Với việc phát hiện trùng lặp và đa dạng lập luận này, LLM có thể tạo ra nội dung thay thế như viết lại hoặc thay thế thực thể.

3.2.3 Bộ Chấm điểm Truy vấn

Bộ chấm điểm truy vấn tính toán điểm số đại diện cho chất lượng của một truy vấn trong một ngữ cảnh nhất định:

s= QueryScorer(q,D) (6)

Lấy mẫu và Gán nhãn Chúng tôi lấy mẫu tất cả dữ liệu huấn luyện theo cách tương tự như lập luận xa nhất trong mục 3.1. Đối với mỗi truy vấn được tạo ra ở các bước khác nhau, IR chấm điểm và xếp hạng độ tương tự giữa truy vấn và tài liệu trực tiếp. Tập dữ liệu không cung cấp tài liệu khớp chính xác của truy vấn con, vì vậy sự thật cơ bản được lấy làm tài liệu vàng đầu tiên dg trong các tài liệu được sắp xếp. Chúng tôi gán nhãn các tài liệu từ đầu đến sự thật cơ bản là những cái tích cực (hoặc những cái có liên quan cao trong Hình 1) (ŷj,1≤j≤g), và các tài liệu từ sự thật cơ bản đến cuối là tiêu cực (ŷj, g < j ≤ |D|) vì chúng tôi sẽ sử dụng Mean Reciprocal Rank (MRR)[27] để tham khảo trong hệ thống chấm điểm. Điểm vàng của một truy vấn ŝ là nghịch đảo của vị trí sự thật cơ bản 1/g.

Trong ví dụ được hiển thị trong mục 1, nếu truy vấn đầu tiên "the NCAA Division" được thực thi, các tài liệu hàng đầu sau khi xếp hạng IR sẽ là "The 2014 Colorado Buffaloes ..." và "The 2012 Colorado Buffaloes football ...", trong khi sự thật cơ bản liên quan nhất "The 2009 Colorado Buffaloes football team represented the University of Colorado in the 2009 NCAA Division I FBS college football season." được xếp hạng thứ 10. Trong trường hợp này, điểm chất lượng của truy vấn này là 1/10 và các tài liệu được xếp hạng trước sự thật cơ bản đều được gán nhãn là đúng. Mạng nơ-ron của chúng tôi sẽ được huấn luyện để phù hợp với các nhãn. Khi mô hình dự đoán chất lượng của một truy vấn, các tài liệu đúng ước tính sẽ được phát hiện là những cái có liên quan cao và gây nhầm lẫn và làm giảm điểm số của truy vấn đó. Chúng tôi đảm bảo truy vấn chính xác và đặc biệt nhất có thể với ước tính này.

Chấm điểm Chúng tôi huấn luyện và tối ưu hóa bộ chấm điểm với quy trình sau.

Hj=Encoder(q;dj), sj=σ(Hj[CLS]ws) (7)
strain=1/∑Nj=1sj (8)

Truy vấn đầu vào q được mã hóa với mỗi tài liệu dj và được chiếu thành một vô hướng sj, trong đó Hj là các trạng thái ẩn cuối cùng cho "[CLS] q[SEP] dj" và Hj[CLS] là các trạng thái ẩn của token [CLS]. Nghịch đảo của tổng mỗi vô hướng sj là điểm MRR của truy vấn q trong quá trình huấn luyện. Mất mát được biểu diễn như sau. BCELoss được sử dụng cho các nhãn tài liệu và MSELoss cho hồi quy phù hợp của điểm MRR. Việc giới thiệu MSELoss là để giúp giám sát và dự đoán các nhãn tài liệu tốt hơn.

L= BCELoss(sj,ŷj) +α·MSELoss(strain,ŝ) (9)

Trong giai đoạn dự đoán, điểm số được tính toán khác với Công thức 8. Chỉ các tài liệu có điểm số vượt quá 0.5 sẽ được dự đoán là đúng. Ngoài ra, điểm truy vấn sẽ là 0 nếu tất cả tài liệu đều tiêu cực vì đôi khi khi một

--- TRANG 7 ---
Tiêu đề Chạy cho Header

truy vấn không hợp lệ được thực thi, tổng sj sẽ gần bằng 0, điều này làm tăng điểm nghịch đảo spred của nó. Ví dụ, trong lần lặp 1 trong Hình 2, truy vấn thứ hai "What specific year and conference is being referred to in the context" là một mô tả trừu tượng không chỉ ra thông tin cụ thể nào. Trong trường hợp này sj của các tài liệu trong D gần bằng 0, strain sẽ là một số rất lớn đại diện kém chất lượng thực sự của truy vấn.

spred=(
0 ∑Nj=1sgn(2·sj−1) = 0
1/∑Nj=1sgn(2·sj−1) ∑Nj=1sgn(2·sj−1)>0 (10)

Cuối cùng, các truy vấn trong Q̂t được sắp xếp theo điểm số của chúng. Cái có điểm cao nhất được coi là truy vấn tốt nhất qbest và được thêm vào Et.

4 Thí nghiệm

4.1 Thiết lập Thí nghiệm

Tập dữ liệu và Chỉ số Đánh giá Chúng tôi thử nghiệm trên ba tập dữ liệu MHQA được công nhận yêu cầu quá trình truy xuất thông tin: HotPotQA[1], 2WikiMultiHopQA[28], và MuSiQue[29]. Ví dụ được phân tích trong mục 1 là từ HotPotQA. Về các chỉ số đánh giá, chúng tôi áp dụng các chỉ số và mã chính thức trong mỗi tập dữ liệu. Ngoài ra, Acc† từ Iter-RetGen[10] và cover-EM[30] cũng được thêm vào để so sánh.

Baseline Các baseline của chúng tôi là một số phương pháp gần đây giới thiệu IR cho LLM và khám phá tương tác của chúng. Ngoài baseline lập luận LLM cơ bản (Direct Retrieval và CoT), những cái khác được thiết kế cho các tác vụ liên quan đến tương tác đa bước giữa LLM và IR (ChatCot[11], Iter-RetGen[10], SearChain[9]). Tất cả các phương pháp trong triển khai của chúng tôi sử dụng cùng một bộ truy xuất. Chúng tôi không thử nghiệm với ReAct[8], Self-Ask[16] và các phương pháp tương tự vì hai lý do. Đầu tiên, kết quả của các phương pháp prompting phổ quát này được hiển thị trong các bài báo như Iter-RetGen. Ngoài ra, chúng về mặt khái niệm tương tự như ChatCot và thiết lập No mask (mục 4.3) trên các tác vụ của chúng tôi, được thử nghiệm và so sánh. Ngoài ra, Iter-RetGen không cung cấp mã và prompt liên quan, do đó kết quả của nó với huấn luyện được lấy trực tiếp từ bài báo của nó, trong khi những cái khác được tái tạo dưới thiết lập của chúng tôi.

Chi tiết Triển khai Mô hình ngôn ngữ lớn được sử dụng trong thí nghiệm của chúng tôi, bao gồm Acc† trong chỉ số đánh giá là gpt-3.5-turbo được cung cấp bởi OpenAI. Mô-đun bộ truy xuất thông tin là TART [31] (theo Iter-RetGen). Đối với cả ba tập dữ liệu, các thí nghiệm được thực hiện trên top 1/10 của tập dev dưới thiết lập distractor. (1/10 do chi phí lớn cho việc gọi API, và được chứng minh là phân phối tương tự với toàn bộ tập dev)

Trong framework lập luận xa nhất, nhiệt độ ban đầu tp0 của LLM là 0.2/1.0 và mức tăng Δtp là 0.8/0.5. Số lựa chọn n được đặt là 5. Bộ mã hóa của bộ phân loại kiểu là roberta-large [32]. Dữ liệu huấn luyện gốc trong ba tập dữ liệu được chia thành tập train và dev dưới 9:1 cho quá trình huấn luyện bộ phân loại. Lần lặp tối đa T trong quá trình lập luận là 6.

Đối với mô-đun bộ đánh giá kế hoạch, ngưỡng θ là 0.6. Trong bộ lọc truy vấn, thuật toán phân cụm dựa trên mật độ là DB-SCAN [33] với eps là 2. Các đặc trưng của truy vấn là bag of words [34]. Trong bộ chấm điểm truy vấn, bộ mã hóa cũng là roberta-large. Dữ liệu huấn luyện được lấy mẫu cho bộ chấm điểm chỉ đến từ tập dữ liệu hotpotQA. Trọng số của MSELoss α là 0.1.

4.2 Kết quả Chính

Kết quả của các baseline và công trình của chúng tôi được trình bày trong Bảng 1. (1) Framework lập luận xa nhất hiệu quả Chúng tôi so sánh mô hình của mình với các baseline cạnh tranh gần đây trên các tập dữ liệu này. Trên ba tập dữ liệu, hầu hết các chỉ số vượt trội hơn state of the art. Trong việc trả lời, tất cả các chỉ số vượt quá những phương pháp hiện có, đặc biệt là trên cover-EM và Acc†. Trong các sự kiện hỗ trợ, mặc dù SearChain1 không đạt được kết quả xuất sắc trên các chỉ số trả lời so với các phương pháp khác, exact match và precision của nó thể hiện hiệu suất tốt hơn. SearChain tập trung vào độ tin cậy của kiến thức bên ngoài, dẫn đến chiến lược truy xuất thận trọng hơn và precision cao hơn của các sự kiện hỗ trợ. Ngược lại, phương pháp truy xuất trực tiếp chỉ đơn giản gọi lại càng nhiều tài liệu liên quan đến câu hỏi gốc càng tốt, đạt được điểm recall tốt hơn. Mô hình của chúng tôi chủ yếu quan tâm đến việc trả lời câu hỏi với kiến thức bên ngoài và chú ý đến cả precision và recall. Về chi phí token, mô hình của chúng tôi tiết kiệm một lượng token nhất định so với các phương pháp độ chính xác cao khác, gần 1/10 token được sử dụng trong ChatCoT. (2) Cải thiện với bộ đánh giá kế hoạch Với mô-đun bộ đánh giá kế hoạch, một số truy vấn gây nhầm lẫn được sàng lọc và do đó tăng cường và ổn định quá trình lập luận ở một mức độ nhất định. Tuy nhiên, mô-đun dường như không hữu ích trên tập dữ liệu 2WikiMultiHop. Lý do chính là các câu hỏi đa bước trong 2WikiMultiHopQA không trực quan cho

1Thực sự việc thử nghiệm SearChain trên MuSiQue không phù hợp. Câu hỏi đa bước trong MuSiQue được cấu thành từ các câu hỏi đơn bước như Natural Questions [35] và Squad [36]. Tuy nhiên, dpr-reader-multiset-base, bộ đọc được sử dụng trong SearChain được huấn luyện trước trên các tập dữ liệu này.

--- TRANG 8 ---
Tiêu đề Chạy cho Header

Bảng 1: Tất cả các chỉ số chi tiết trong kết quả

[Bảng chi tiết với các chỉ số cho Answer, Supporting Facts, Joint và Cost trên các dataset HotPotQA, 2WikiMultiHop, và MuSiQue]

LLM phân tách. Ví dụ, "father in law" yêu cầu một bước truy xuất "husband/wife", điều không rõ ràng và khó cho LLM tạo ra các truy vấn phù hợp. Chúng tôi có thể phải áp dụng các prompt có mục tiêu cho loại câu hỏi này. (3) Phát hiện Xem xét kết quả của Direct Answer và Cot, các tài liệu được truy xuất của chúng hoàn toàn giống nhau từ chiến lược truy xuất giống hệt nhau. Lý do CoT hoạt động tệ hơn trả lời trực tiếp là CoT đòi hỏi độ chính xác của các sự kiện hỗ trợ để lập luận từ bằng chứng như các ví dụ few-shot, điều không được thỏa mãn dưới truy xuất trực tiếp của câu hỏi gốc. Trả lời trực tiếp không liên quan đến những vấn đề như vậy.

4.3 Nghiên cứu Loại bỏ

Che giấu Lập luận và Truy vấn Framework Lập luận Xa nhất có ý nghĩa trong việc che giấu quá trình lập luận và truy vấn trước đó. Các thí nghiệm loại bỏ liên quan được hiển thị trong Bảng 2. Các ví dụ mẫu trong few shot và các thiết lập khác hoàn toàn giống nhau. Thí nghiệm được thực hiện trong một phần nhỏ của tập dev trong HotPotQA và 2WikiMultiHopQA.

Kết quả chính chúng tôi tập trung vào là No mask và Double mask. Thiết lập No mask lấy nội dung lập luận và truy vấn lịch sử làm đầu vào cho LLM ở mỗi lần lặp, trong khi thiết lập Double mask che giấu cả hai và chỉ để lại câu hỏi gốc và các bằng chứng được truy xuất. So sánh này ngụ ý tác động tiêu cực mà nội dung lập luận và truy vấn lịch sử có đối với quá trình đa bước. Chi phí Token được tiết kiệm trong Double mask ngoài các chỉ số trả lời tốt hơn. Reasoning mask và Query mask là các thiết lập che giấu từng cái một, dẫn đến kết luận tương tự. Để nghiêm ngặt hơn, No mask (reasoning from scratch) bắt chước lập luận xa nhất trong việc tạo ra chuỗi suy nghĩ từ đầu thay vì tiếp tục lập luận, và kết quả tương tự như No mask. Giải pháp thực sự bao gồm hai phần: quá trình lập luận, và quá trình trả lời bắt buộc. Để loại bỏ ảnh hưởng của các lối tắt có thể có trong việc trả lời bắt buộc, No mask (mask at answering) thay thế việc trả lời bắt buộc bằng thiết lập double mask. Các khoảng cách minh họa rằng chuỗi lập luận lịch sử dư thừa ảnh hưởng bất lợi đến toàn bộ quy trình trong giải pháp đa bước. Ngoài ra, CoT trong lập luận và đưa ra các kế hoạch tiếp theo vẫn quan trọng như Double mask (no CoT) trình bày.

--- TRANG 9 ---
Tiêu đề Chạy cho Header

Bảng 2: Kết quả loại bỏ về che giấu Lập luận và Truy vấn.

[Bảng cho thấy các kết quả với different masking settings]

Bảng 3: α trong huấn luyện bộ chấm điểm, F1 là dự đoán nhãn tài liệu

[Bảng nhỏ với Model và Doc F1]

4.4 Phân tích

Mất mát Nhãn Hỗn hợp Đối với những ai có thể quan tâm, đây là lý do cho một MSELoss bổ sung trong hàm mất mát được đề cập trong mục 3.2.3. Hàm mất mát của huấn luyện trên các nhãn tài liệu có thể bao gồm hai phần: BCELoss và MSELoss. Mặc dù chỉ các nhãn tài liệu được sử dụng cho điểm truy vấn trong giai đoạn dự đoán, hồi quy trên MRR có lợi cho dự đoán nhãn. Hồi quy trên MRR kết nối các tài liệu ứng viên của một truy vấn và mở rộng sự khác biệt giữa chúng, điều hóa ra hiệu quả. Trọng số α của MSELoss không nên quá lớn ngay từ đầu vì mô hình phải học phân loại cơ bản trước.

Hạn chế Đầu tiên, do chi phí cao trên gpt-3.5, các thí nghiệm của chúng tôi được thực hiện với một tỷ lệ nhỏ của các tập dữ liệu, khoảng hàng nghìn ví dụ. Chúng tôi sẽ nghiên cứu hiệu suất của mô hình trên các tập dữ liệu phức tạp khác và các LLM mã nguồn mở khác. Thứ hai, mặc dù các thế hệ của giải mã LLM được đánh giá bởi bộ đánh giá kế hoạch, các kế hoạch vẫn phụ thuộc phần lớn vào khả năng của LLM. Các mô hình ngôn ngữ thể hiện tính ngẫu nhiên nhiều hơn khi ngưỡng xác suất giải mã được thay đổi, do đó ảnh hưởng đến các kế hoạch được tạo ra. Thứ ba, trong thực tế, IR có thể không phải là một bộ truy xuất truyền thống dựa trên độ tương tự. Nếu IR đại diện cho một cơ sở dữ liệu chẳng hạn, phương pháp của chúng tôi sẽ cần một số sửa đổi.

5 Kết luận

Dựa trên các thí nghiệm và kết quả được trình bày ở trên, rõ ràng rằng Lập luận Xa nhất với Đánh giá Kế hoạch thể hiện hiệu suất đặc biệt trong việc giải quyết các câu hỏi đa bước. Đầu tiên, framework Lập luận Xa nhất đóng góp đáng kể không chỉ cho quá trình lập luận mà còn cho quá trình trả lời trực tiếp. Trong các tình huống giải quyết vấn đề phức tạp và dài hạn như các câu hỏi đa bước, FuRePA được thiết kế để tạo ra các kế hoạch hiệu quả hơn, tránh ảnh hưởng của các quá trình lập luận phức tạp trước đó. Hơn nữa, thành phần Đánh giá Kế hoạch đóng vai trò quan trọng trong việc chọn lựa truy vấn phù hợp nhất từ các ứng viên được tạo ra bởi LLM, dẫn đến việc truy xuất bằng chứng liên quan hơn bởi Bộ Truy xuất Thông tin (IR). Sự tăng cường này trong việc chọn lựa truy vấn đóng góp vào một quá trình giải quyết câu hỏi đa bước hiệu quả và chính xác hơn.

Tài liệu tham khảo

[1]Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, và Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.

[2]Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, và William Wang. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. arXiv preprint arXiv:2004.07347, 2020.

[3]Johannes Welbl, Pontus Stenetorp, và Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287–302, 2018.

[4] Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, và Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328, 2018.

[5]Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.

[6]Vaibhav Mavi, Anubhav Jangra, và Adam Jatowt. A survey on multi-hop question answering and generation. arXiv preprint arXiv:2204.09140, 2022.

[7]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

--- TRANG 10 ---
Tiêu đề Chạy cho Header

[8]Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, và Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.

[9]Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-seng Chua. Search-in-the-chain: Towards the accurate, credible and traceable content generation for complex knowledge-intensive tasks. arXiv preprint arXiv:2304.14732, 2023.

[10] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, và Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023.

[11] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, và Ji-Rong Wen. Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models. arXiv preprint arXiv:2305.14323, 2023.

[12] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, và Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

[13] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.

[14] Zhuosheng Zhang, Aston Zhang, Mu Li, và Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.

[15] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.

[16] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, và Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.

[17] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, và Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.

[18] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, và Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023.

[19] Ye Liu, Semih Yavuz, Rui Meng, Dragomir Radev, Caiming Xiong, và Yingbo Zhou. Answering complex questions over text by hybrid question parsing and execution. arXiv preprint arXiv:2305.07789, 2023.

[20] Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, et al. Question decomposition improves the faithfulness of model-generated reasoning. arXiv preprint arXiv:2307.11768, 2023.

[21] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929–3938. PMLR, 2020.

[22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.

[23] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, và Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083, 2023.

[24] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, và Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.

[25] Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, và Ce Zhang. Improving retrieval-augmented large language models via data importance learning. arXiv preprint arXiv:2307.03027, 2023.

[26] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, và Nan Duan. Query rewriting for retrieval-augmented large language models. arXiv preprint arXiv:2305.14283, 2023.

[27] Ellen M Voorhees et al. The trec-8 question answering track report. In Trec, volume 99, pages 77–82, 1999.

[28] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, và Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020.

--- TRANG 11 ---
Tiêu đề Chạy cho Header

[29] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, và Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554, 2022.

[30] Corby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, và Saurabh Tiwary. Knowledge-aware language model pretraining. arXiv preprint arXiv:2007.00655, 2020.

[31] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, và Wen-tau Yih. Task-aware retrieval with instructions. arXiv preprint arXiv:2211.09260, 2022.

[32] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[33] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226–231, 1996.

[34] Zellig S Harris. Distributional structure. Word, 10(2-3):146–162, 1954.

[35] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.

[36] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

A Prompts

Các prompts chúng tôi sử dụng trong công trình của mình như sau, bao gồm prompts cho lần lặp đầu tiên và cho những lần sau. Nhiều thiết lập prompting hơn có thể được tìm thấy trong mã.

A.1 Lần Lặp Đầu tiên

System Bạn được đưa ra các vấn đề yêu cầu tìm kiếm và lập luận đa bước. Bạn nên theo các ví dụ và cho tôi biết thông tin nào cần tìm kiếm đầu tiên một cách chính xác.

User Để giải quyết vấn đề đa bước này: What was the name of the Infantry that was commanded by a Senator who was know as Jim Lane? Tôi nên tìm kiếm thông tin cụ thể nào từ wiki ở bước đầu tiên? Tìm kiếm một mục tiêu ở bước đầu.

Assistant [Search] the Senator who was known as Jim Lane

User Để giải quyết vấn đề đa bước này: Which person born on March 23, 1902 did George Roby Dempster have political battles with? Tôi nên tìm kiếm thông tin cụ thể nào từ wiki ở bước đầu tiên? Tìm kiếm một mục tiêu ở bước đầu.

Assistant [Search] the person George Roby Dempster have political battles with

User Để giải quyết vấn đề đa bước này: Do director of film Swami Dada and director of film Common Clay (1930 Film) have the same nationality? Tôi nên tìm kiếm thông tin cụ thể nào từ wiki ở bước đầu tiên? Tìm kiếm một mục tiêu ở bước đầu.

Assistant [Search] the nationality of director of film Swami Dada

User Để giải quyết vấn đề đa bước này: How many square miles is the state that houses the university that Joseph L. Badaracco attended? Tôi nên tìm kiếm thông tin cụ thể nào từ wiki ở bước đầu tiên? Tìm kiếm một mục tiêu ở bước đầu.

Assistant [Search] the university Joseph L. Badaracco attended

User Để giải quyết vấn đề đa bước này: The Tudor-style Norfolk Hotel, now run by the Fairmont group, is a landmark in the capital city of which west African country? Tôi nên tìm kiếm thông tin cụ thể nào từ wiki ở bước đầu tiên? Tìm kiếm một mục tiêu ở bước đầu.

Assistant [Search] the Tudor-style Norfolk Hotel, now run by the Fairmont group, is a landmark in what city

A.2 Các Lần Lặp Sau

System Bạn được đưa ra các vấn đề yêu cầu tìm kiếm và lập luận đa bước với các ngữ cảnh liên quan. Bạn nên cho tôi biết câu trả lời nếu tất cả thông tin cần thiết được cung cấp trong ngữ cảnh, nếu không hãy cho tôi biết thông tin cụ thể nào cần tìm kiếm tiếp theo. Đừng trả lời với sự không chắc chắn. Các dòng chỉ bắt đầu bằng '[Analysis]' hoặc '[Answer]' hoặc '[Search]'.

--- TRANG 12 ---
Tiêu đề Chạy cho Header

User
Context:
Saint Motel: The band consists of A/J Jackson (lead vocals, guitar, piano), Aaron Sharp (lead guitar), Dak Lerdamornpong (bass), and Greg Erwin (drums). Curve Lake First Nation: The Curve Lake First Nation registered a population on these three reserves of 741, with an additional 1,050 registered band members living off-reserve.
Question:
Which band has more members, Saint Motel or Curve?

Assistant
[Analysis] Theo ngữ cảnh, chúng ta bây giờ biết Saint Motel có 3 thành viên. Ngữ cảnh cung cấp thông tin về Curve Lake First Nation, không phải ban nhạc. Chúng ta vẫn cần thông tin về ban nhạc Curve.
[Search] the number of members in the band Curve

User
Context:
Dhyanchand Stadium is a cricket ground in Jhansi, India. The stadium is named after Major Dhyan Chand who lived in Jhansi. Dhyan Chand: He is known for his extraordinary goal-scoring feats, in addition to earning three Olympic gold medals (1928, 1932, and 1936) in field hockey, during an era where India was the most dominant team in Hockey.
Question:
Where is the stadium located which is named after the man who won three Olympic gold medals in 1928, 1932, and 1936?

Assistant
[Analysis] Người đàn ông giành ba huy chương vàng Olympic năm 1928, 1932 và 1936 là Dhyan Chand. Sân vận động được đặt tên theo ông là Dhyanchand Stadium. Dhyanchand Stadium nằm ở Jhansi, Ấn Độ. Sau đó chúng ta có câu trả lời cho câu hỏi.
[Answer] Jhansi, India

User
Context:
Matthias W. Day (August 8, 1853 – September 12, 1927) was a career American army officer who received the Medal of Honor, the United States' highest military decoration, for his actions during the American Indian Wars in the latter half of the 19th century. Day was a longtime officer with the African-American 9th Cavalry Regiment, seeing action during the Apache Wars against the Apache leaders Victorio and Geronimo.
Question:
In 2010, who controlled the Senate and the body who presides over any impeachment trial of the president of the country Matthias W. Day holds citizenship in

Assistant
[Analysis] Matthias W. Day có quốc tịch Hoa Kỳ. Sau đó chúng ta cần biết cơ quan nào chủ trì bất kỳ phiên tòa luận tội nào của tổng thống Hoa Kỳ?
[Search] Who presides over any impeachment trial of the president of the United States

User
Context:
The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.
Question:
The Oberoi family is part of a hotel company that has a head office in what city?

Assistant
[Analysis] Gia đình Oberoi là một phần của công ty khách sạn gọi là The Oberoi Group. Sau đó chúng ta cần biết The Oberoi Group có trụ sở chính ở thành phố nào.
[Search] the Oberoi Group has a head office in what city

User
Context:
"Human Error" is the twenty-fourth episode and season finale of the third season of "House" and the seventieth episode overall. House (also called House, M.D.) is an American television medical drama that originally ran on the Fox network for eight seasons, from November 16, 2004 to May 21, 2012.
Question:
"Human Error" is the season finale of the third season of a tv show that aired on what network?

--- TRANG 13 ---
Tiêu đề Chạy cho Header

Assistant
[Analysis] "Human Error" là tập cuối mùa thứ ba của "House". Chương trình truyền hình "House" được phát sóng trên mạng Fox. Sau đó chúng ta có câu trả lời.
[Answer] Fox network

B Nghiên cứu Trường hợp

Xem các bảng dưới đây.

--- TRANG 14 ---
Tiêu đề Chạy cho Header

Câu hỏi: Onika Tanya Maraj is a judge on a television show hosted by whom?
Câu trả lời: Ryan Seacrest

FuRePA
Lần lặp 0
Kế hoạch
the television show Onika Tanya Maraj is a judge on
Onika Tanya Maraj to find out on which television show she is a judge
Onika Tanya Maraj's television show
Onika Tanya Maraj television judge
Bằng chứng
Onika Tanya Maraj (born December 8, 1982), known professionally as Nicki Minaj, is a rapper, singer, songwriter and model.

Lần lặp 1
Phân tích
Onika Tanya Maraj, cũng được biết đến với tên Nicki Minaj, là một thẩm phán trong một chương trình truyền hình. Chúng ta cần tìm ra tên của người dẫn chương trình truyền hình đó.
Kế hoạch
Name of the host of the television show where Nicki Minaj is a judge
Nicki Minaj is a judge on a television show hosted by whom?
Who hosts the television show that Onika Tanya Maraj is a judge on?
Nicki Minaj recent television appearances or judge roles
Bằng chứng
American Idol (season 12): Randy Jackson returned for his twelfth season as judge and was joined by new judges Mariah Carey, Nicki Minaj and Keith Urban, who replaced Jennifer Lopez and Steven Tyler.

Lần lặp 2
Phân tích
Onika Tanya Maraj, được biết đến chuyên nghiệp với tên Nicki Minaj, là một thẩm phán trong một chương trình truyền hình. Chương trình truyền hình là 'American Idol' và người dẫn chương trình của 'American Idol' không được đề cập trong ngữ cảnh đã cho.
Kế hoạch
What television show is Onika Tanya Maraj a judge on?
Who is the host of "American Idol"
Host of the television show where Onika Tanya Maraj is a judge.
Who hosts the television show that Onika Tanya Maraj is a judge on?
Host of the television show with Onika Tanya Maraj as a judge
Bằng chứng
American Idol (season 12): Ryan Seacrest returned to host.

Lần lặp 3
Phân tích
Onika Tanya Maraj là một thẩm phán trong một chương trình truyền hình. Tên của chương trình truyền hình không được cung cấp trong ngữ cảnh. Tuy nhiên, chúng ta biết rằng Ryan Seacrest trở lại dẫn chương trình American Idol (season 12) và Onika Tanya Maraj là một trong những thẩm phán. Sau đó chúng ta có câu trả lời.
Câu trả lời
Ryan Seacrest.

FuRePA without PA
Lần lặp 0
Kế hoạch
the television show Onika Tanya Maraj is a judge on
Bằng chứng Onika Tanya Maraj (born December 8, 1982), known professionally as Nicki Minaj, is a rapper, singer, songwriter and model.

Lần lặp 1
Phân tích
Ngữ cảnh đề cập rằng Onika Tanya Maraj, cũng được biết đến với tên Nicki Minaj, là một thẩm phán trong một chương trình truyền hình. Tuy nhiên, nó không cung cấp thông tin về người dẫn chương trình. Chúng ta cần tìm ra ai dẫn chương trình truyền hình mà Onika Tanya Maraj là thẩm phán.
Kế hoạch
the host of the television show that Onika Tanya Maraj is a judge on.
Bằng chứng sai
Rewrapped: The show is a loose spinoff of the show Ünwrapped¨, whose host Marc Summers acts as ¨Head Judge ¨for each episode, alongside a representative of the company that produces the food of the day, and a third impartial judge involved in the food industry in some way (chef, critic, blogger, etc.).

Lần lặp 2
......
Lần lặp 5
Câu trả lời
Judge Andrew Napolitano

Bảng 4: Nghiên cứu Trường hợp của Bộ Đánh giá Kế hoạch về các kế hoạch đa dạng

--- TRANG 15 ---
Tiêu đề Chạy cho Header

Câu hỏi: North American Aviation was the company that created what single-engined training aircraft?
Câu trả lời: North American Aviation T-6 Texan

FuRePA
Lần lặp 0
Kế hoạch
the single-engined training aircraft created by North American Aviation
North American Aviation to find information about the company
the single-engined training aircraft created by North American Aviation"
Bằng chứng
North American Aviation (NAA) was a major American aerospace manufacturer, responsible for a number of historic aircraft, including the T-6 Texan trainer, the P-51 Mustang fighter, the B-25 Mitchell bomber, the F-86 Sabre jet fighter, the X-15 rocket plane, and the XB-70, as well as Apollo Command and Service Module, the second stage of the Saturn V rocket, the Space Shuttle orbiter and the B-1 Lancer.

Lần lặp 1
Phân tích
Theo ngữ cảnh, North American Aviation chịu trách nhiệm tạo ra nhiều máy bay lịch sử. Chúng ta cần biết máy bay huấn luyện đơn động cơ nào họ đã tạo ra.
Kế hoạch
North American Aviation single-engined training aircraft
Bằng chứng
The North American Aviation T-6 Texan is an American single-engined advanced trainer aircraft used to train pilots of the United States Army Air Forces (USAAF), United States Navy, Royal Air Force, and other air forces of the British Commonwealth during World War II and into the 1970s.

Lần lặp 2
Phân tích
North American Aviation là công ty đã tạo ra máy bay huấn luyện đơn động cơ T-6 Texan.
Câu trả lời
T-6 Texan

ChatCot
Kế hoạch
Information about North American Aviation
Bằng chứng
The North American Aviation NA-16 was the first trainer aircraft built by North American Aviation, and was the beginning of a line of closely related North American trainer aircraft that would eventually number more than 17,000 examples. Is this evidence useful?
Phân tích
Có, bằng chứng này hữu ích. Chúng ta có thể thấy rằng North American Aviation đã tạo ra một dòng máy bay huấn luyện, bắt đầu với NA-16.
Kế hoạch
the single-engined training aircraft created by North American Aviation
Bằng chứng
The North American Aviation NA-16 was the first trainer aircraft built by North American Aviation, and was the beginning of a line of closely related North American trainer aircraft that would eventually number more than 17,000 examples. Is this evidence useful?
Phân tích Sai
Có, bằng chứng này hữu ích. Chúng ta có thể thấy rằng North American Aviation NA-16 là máy bay huấn luyện đầu tiên được xây dựng bởi North American Aviation.
Câu trả lời
Chúng ta có câu trả lời. Câu trả lời là North American Aviation NA-16.

Bảng 5: Nghiên cứu Trường hợp của FuRePA và ChatCot

--- TRANG 16 ---
Tiêu đề Chạy cho Header

Câu hỏi: Where did Charles Stewart, 3Rd Duke Of Richmond's father graduate from?
Câu trả lời: University of Paris

FuRePA
Lần lặp 0
Kế hoạch
the educational institution where Charles Stewart, 3rd Duke of Richmond's father graduated from.
the father of Charles Stewart, 3rd Duke of Richmond's alma mater or graduation institution
the name of Charles Stewart, 3rd Duke of Richmond's father
the educational institution from which Charles Stewart, 3rd Duke of Richmond's father graduated
Bằng chứng
Charles Stewart, 3rd Duke of Richmond: The titles Duke of Richmond, Duke of Lennox and Earl of March, were resurrected for Charles II's illegitimate son by Louise de Kérouaille, Charles Lennox, 1st Duke of Richmond and Lennox, in 1675.

Lần lặp 1
Phân tích
Cha của Charles Stewart, 3rd Duke of Richmond không được đề cập trong ngữ cảnh đã cho. Chúng ta cần thông tin về cha của Charles Stewart, 3rd Duke of Richmond để tìm ra nơi ông tốt nghiệp.
Kế hoạch
Educational background of Charles Stewart, 3rd Duke of Richmond's father.
the educational background or university of Charles Stewart's father
Charles Lennox, 1st Duke of Richmond and Lennox's education
the educational background or where the father of Charles Stewart, 3rd Duke of Richmond graduated from.
Where did Charles Lennox, 1st Duke of Richmond and Lennox's father graduate from?
Bằng chứng
Charles Stewart, 3rd Duke of Richmond, 6th Duke of Lennox KG (7 March 1639December 1672) was the only son of George Stewart, 9th Seigneur d'Aubigny and Katherine Howard, daughter of Theophilus Howard, 2nd Earl of Suffolk.

Lần lặp 2
Phân tích
Theo ngữ cảnh, cha của Charles Stewart, 3rd Duke of Richmond đã tốt nghiệp từ một nơi nào đó. Tuy nhiên, ngữ cảnh không cung cấp thông tin về nơi đó. Chúng ta cần thông tin cụ thể hơn để trả lời câu hỏi.
Kế hoạch
Where did George Stewart, 9th Seigneur d'Aubigny graduate from?
George Stewart's graduation information
George Stewart's alma mater or where George Stewart graduated from.
the graduation details of George Stewart, 9th Seigneur d'Aubigny.
Where did Charles Stewart, 3rd Duke of Richmond's father graduate from?
Bằng chứng
Charles Stewart, 3rd Duke of Richmond: On 10 December 1645 he was created Baron Stuart of Newbury, Berkshire, and Earl of Lichfield, titles conferred on him ¨to perpetuate the titles which were intended to have been conferred on his uncle ¨Lord Bernard Stewart, youngest son of the Duke of Lennox, who had been killed in the Battle of Rowton Heath in the English Civil War in September of that year.

Lần lặp 3
Phân tích
Ngữ cảnh đề cập rằng cha của Charles Stewart, 3rd Duke of Richmond là George Stewart, 9th Seigneur d'Aubigny. Chúng ta cần biết George Stewart, 9th Seigneur d'Aubigny tốt nghiệp từ đâu.
Kế hoạch
Where did Charles Stewart, 3rd Duke of Richmond's father graduate from?
Where did George Stewart, 9th Seigneur d'Aubigny graduate from?
The educational background or alma mater of George Stewart, 9th Seigneur d'Aubigny.
Bằng chứng
George Stewart, 9th Seigneur d'Aubigny: By 1633, he was a student at the Collège de Navarre, part of the University of Paris, and he did homage to Louis XIII of France for the lordship of Aubigny on 5 August 1636, shortly after his eighteenth birthday.

Lần lặp 4
Phân tích
Cha của Charles Stewart, 3rd Duke of Richmond là George Stewart, 9th Seigneur d'Aubigny. Thông tin được cung cấp nói rằng George Stewart là sinh viên tại Collège de Navarre, một phần của Đại học Paris. Vậy, George Stewart tốt nghiệp từ Collège de Navarre, là một phần của Đại học Paris.
Câu trả lời
George Stewart, 9th Seigneur d'Aubigny tốt nghiệp từ Collège de Navarre, Đại học Paris.

Bảng 6: Nghiên cứu Trường hợp của FuRePA phục hồi từ lỗi
