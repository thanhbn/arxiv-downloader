# 2407.12529.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2407.12529.pdf
# File size: 468139 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Crafting the Path: Robust Query Rewriting for Information Retrieval
Ingeol Baek, Jimin Lee, Joonho Yang, Hwanhee Lee†
Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea
baekingeol98@gmail.com ,{ljm1690, plm3332, hwanheelee}@cau.ac.kr
Abstract
Query rewriting aims to generate a new query
that can complement the original query to im-
prove the information retrieval system. Recent
studies on query rewriting, such as query2doc ,
query2expand andquerey2cot , rely on the in-
ternal knowledge of Large Language Models
(LLMs) to generate a relevant passage to add
information to the query. Nevertheless, the ef-
ficacy of these methodologies may markedly
decline in instances where the requisite knowl-
edge is not encapsulated within the model’s in-
trinsic parameters. In this paper, we propose a
novel structured query rewriting method called
CRAFTING THEPATH tailored for retrieval
systems. CRAFTING THEPATH involves a
three-step process that crafts query-related in-
formation necessary for finding the passages
to be searched in each step. Specifically, the
CRAFTING THEPATH begins with Query Con-
cept Comprehension , proceeds to Query Type
Identification , and finally conducts Expected
Answer Extraction . Experimental results show
that our method outperforms previous rewriting
methods, especially in less familiar domains for
LLMs. We demonstrate that our method is less
dependent on the internal parameter knowledge
of the model and generates queries with fewer
factual inaccuracies. Furthermore, we observe
that CRAFTING THEPATH demonstrates su-
perior performance in the retrieval-augmented
generation scenarios.
1 Introduction
In an open-domain QA system (Lewis et al., 2020b;
Zhu et al., 2021; Li et al., 2022; Zhang et al., 2023;
Kamalloo et al., 2023), document retrievers are
utilized to retrieve the necessary information to an-
swer the given query. Query rewriting reformulates
original queries to help the retrieval system find rel-
evant passages. Recent works on query rewriting
focus on Large Language Models (LLMs) (Brown
†Corresponding author.et al., 2020; OpenAI, 2022, 2023; Touvron et al.,
2023) to generate additional information. Specif-
ically, these studies aim to generate a relevant
passage for a given query by leveraging the pre-
trained knowledge of LLMs. Utilizing these new
queries generated from LLMs has shown a signif-
icant increase in the performance of retrieval sys-
tems. Recently, various LLM based rewriting meth-
ods such as query2doc (Q2D) (Wang et al., 2023c),
query2expand (Q2E), and querey2cot (Q2C) (Jager-
man et al., 2023) have been introduced. Q2D gener-
ates a pseudo-document based on the original query,
which is then used as input for the retriever. Simi-
larly, Q2C employs a Chain-of-Thought (Wei et al.,
2022) approach, and Q2E generates semantically
equivalent queries. These approaches leverage the
rewriting of the original query into a form similar
to passages in the corpus. These techniques result
in superior performance improvements compared
to the base query alone.
The fundamental reason for utilizing retrieval
systems in open-domain QA is to use external
knowledge when QA systems do not know to gen-
erate correct answer (Gao et al., 2023). And this is
the original purpose of using Retrieval-Augmented
Generation (RAG) systems (Lewis et al., 2020a;
Wang et al., 2023a). However, relying heavily on
inherent knowledge of LLMs for query rewriting of-
ten leads to the generation of irrelevant information
and causes numerous factual errors (Chern et al.,
2023; Min et al., 2023; Luo et al., 2023) in the re-
formulated queries. As in the right examples of Fig-
ure 1, Q2C asserts that coffee originated in ancient
Egypt or Yemen, whereas its origin is Ethiopia.
Meanwhile, Q2D discusses a legendary story about
"goats" without providing any information about
the origin of the word "coffee". These types of
misinformation and unrelated contents can lead to
significant performance degradation because of the
included incorrect information in the reformulated
queries.
1arXiv:2407.12529v2  [cs.CL]  26 Aug 2024

--- PAGE 2 ---
Query Concept Comprehension :
Coffee is a popular beverage with 
a rich cultural history and complex 
production process.Step1
Query Type Identification:
To understand the origin of coffee, 
one must delve into its historical 
context and etymology .Step2
Expected Answer Extraction:
Coffee originated in Ethiopia aroun
d the 9th century, with the word 
"coffee" derived from the Arabic term 
"qahwa ," meaning wine of the 
roasted beans .Step3Query2CoT (Q2C):
The origin of the word "coffee" is debated, with the
ories tracing it to Ethiopia (from the Oromo langua
ge "buna dabo naw"), Yemen (from the Arabic " qa
hwa"), or even ancient Egypt .Regardless of its ety
mology, coffee's rich history as a beverage and cul
tural symbol is undeniable Many Believe it spread t
hrough trade routes, gaining popularity across vari
ous cultures and regions.
MRR: 20.00
Crafting the Path:
Coffee is a popular beverage with a rich cultural 
history and complex production process. To 
understand the origin of coffee, one must delve 
into its historical context and etymology .Coffee 
originated in Ethiopia around the 9th century, 
with the word "coffee" derived from the Arabic 
term " qahwa ," meaning wine of the roasted beans .
MRR: 33.33
Query2Doc (Q2D):
Coffee, this invigorating beverage, has a rich and 
complex history. Its origins can be traced back to 
Ethiopia, where legend has it that goats herding i
n the forest noticed their animals acting strangely 
after eating the berries of a certain plant . Intrigued 
by this observation, the goat herder stride the berri
es themselves and experienced a sense of alertn
ess and energy. The plant, Coffea arabica was lat
er cultivated for its seeds, which were roasted an
d ground to create the first coffee beverages. 
MRR: 16.66
Original Query:
what is the origin of the 
word coffee.Figure 1: Overview of our proposed query rewriting method CRAFTING THEPATH, along with the rewritten query
examples of query2doc (Q2D) and query2cot (Q2C) methodologies. We represent the factual error in red and the
accurate information in blue.
In this paper, we propose a novel query rewriting
method, CRAFTING THEPATH, which is a fine-
grained query reformulation technique through the
structured reasoning process. Instead of simply
generating the passage similar to the candidate doc-
uments, CRAFTING THEPATH focuses on identify-
ing what information needs to be found to solve the
given query. The CRAFTING THEPATH method
comprises three steps. The first step, Query Con-
cept Comprehension , provides fundamental back-
ground knowledge. Offering basic factual informa-
tion reduces the likelihood of including incorrect
information and helps the retrieval system clearly
understand the main topic. The second step, Query
Type Identification , specifies the required informa-
tion to filter out irrelevant information. Finally,
through Expected Answer Extraction process, the
retriever model identifies the essential information
it needs to find, facilitating the extraction of accu-
rate passages. This structured, process minimizes
unnecessary inferences by the model, thereby re-
ducing the possibility of factual errors.
CRAFTING THEPATH outperforms all of the
baseline methods and exhibits a reduced degree
of factual errors demonstrated by the 10% higher
FActScore (Min et al., 2023) compared to base-
lines. Additionally, our approach demonstrates en-
hanced performance without prior knowledge of
the model’s internal parameters. This is evidenced
by experiments in closed-book QA settings, where
rewriting models fail to provide correct answers, re-
sulting in a 3.57% increase in retrieval performance.
Furthermore, our method shows 7.3% less latency
compared to the baselines and also demonstrates
superior performance in adopting query rewritingto open-domain QA scenarios.
2 C RAFTING THEPATH
Our proposed rewriting method, CRAFTING THE
PATH, is composed of the following three steps.
2.1 Query Rewriting via C RAFTING THE
PATH
2.1.1 Step 1: Query Concept Comprehension
We begin with Query Concept Comprehension step,
which generates additional information that serves
as the contextual background for the existing query.
This step enriches the direct information about the
question object within the query. This step per-
forms a higher level of abstraction for the original
question. As shown in Figure 1, it elaborates on
the high-level concept and provides detailed expla-
nations about “coffee,” which is part of the original
query. Query Concept Comprehension step plays
a crucial role in generating high-level information
while aiding in identifying the information to be
searched for in the next step.
2.1.2 Step 2: Query Type Identification
Based on the specific information obtained through
the original query and Step 1, we proceed to Query
Type Identification step. In this step, we generate
the necessary information to retrieve relevant pas-
sages. Specifically, we create categories for the
query that help filter out irrelevant information. To
retrieve information to answer the origin of coffee
as in Figure 1, we can think that one must search
for the historical context and etymology of coffee.
This step can filter out irrelevant passages because
it specifies the information to be found through
2

--- PAGE 3 ---
rewriting, which serves as input for the retriever
model. Inspired by this point, Query Type Identifi-
cation aims to find the type of necessary informa-
tion that the ground truth passage might include
as in Figure 1, and it helps to identify passages
containing information on the “historical context
and etymology” of coffee.
2.1.3 Step 3: Expected Answer Extraction
The final step involves extracting expected answers
for the query based on the information generated
from the previous step. As in Figure 1, the details
regarding the origin of coffee being Ethiopia and
its etymology enable the retriever model to identify
the required information, facilitating the extraction
of accurate passage.
Input Prompt for C RAFTING THEPATH
Instruction: By following the requirements, write 3 steps
related to the Query and answer in the same format as the
example.
Requirements:
1. In step1, generate the contextual background from the
existing query is extracted.
2. In step2, generate what information is needed to solve
the question.
3. In step3, generate expected answer based on query,
step1, and step2.
4. If you think there is no more suitable answer, end up
with ’None’.
Query 1: what is the number one formula one car?
Step 1: Formula One (F1) is the highest class of interna-
tional automobile racing competition held by the FIA.
Step 2: To know the best car, you have to look at the race
records.
Step 3: Red Bull Racing’s RB20 is the best car.
(4-shot examples) ...
Query 5:
Table 1: Prompt used for C RAFTING THEPATH.
These three distinct steps offer a form of query
rewriting that enhances the retrieval of more ac-
curate information and minimizes the inclusion
of incorrect information, resulting in better Open-
domain QA performance in Figure 2. We imple-
ment all of these steps in CRAFTING THEPATH
with a single LLM call using the prompt in Table
1. Specifically, we provide the role of each step
with the examples in the prompts. Additionally,
to avoid producing inaccurate information, we in-
struct the model to generate “None” when it lacks
certain knowledge, providing clear guidance for
the retriever system’s input.2.2 Passage Retriever
Constructing Inputs of Retriever To construct
the final query q+, we expand the original query q
three times and concatenate qwith rewritten query
QRin sparse retrieval as shown in Eq. 1. In the
case of dense retrieval, a [SEP] token is inserted
between the query and the QR to differentiate them.
Sparse: q+=concat( {q} ×3, QR ). (1)
Dense: q+=concat( q,[SEP] , QR ). (2)
Training Dense Retriever To train a dense re-
triever, we utilize the Binary Passage Retrieval
(BPR) loss (Yamada et al., 2021) as follows to
reduce the memory usage:
Lcand=Pn
j=1max(0 ,−(⟨˜hqi,˜hp+
i⟩+⟨˜hqi,˜hp−
i,j⟩) +α).(3)
Lrerank =−logexp(⟨eqi,˜hp+
i⟩)
exp(⟨eqi,˜hp+
i⟩)+Pn
j=1exp(⟨eqi,˜hp−
i,j⟩),
(4)
where D={⟨qi, p+
i, p−
i,1,···, p−
i,n⟩}m
i=1denote a
set where mrepresents training instances, p+
ide-
notes a positive passage, and p−
i,jdenotes a negative
passage. We compute embedding e∈Rdusing an
encoder, each ˜hqand˜hprepresent the hash code
for a query and a passage, respectively. Lcand is
to identify positive passages based on ranking loss,
andαis the margin that is enforced between the
positive and negative scores. Lrerank is used to
minimize the negative log-likelihood for a posi-
tive passage. Finally, we employ the BPR loss as
follows:
Lbpr=Lcand+Lrerank . (5)
3 Experiments
3.1 Experimental Setup
Datasets We use the MS-MARCO passage
dataset (Campos et al., 2016) for training retriever.
Additionally, To demonstrate the robustness of our
model on unseen data, we utilize nine retrieval
datasets from BEIR (Thakur et al., 2021) for our
main experiment. In our experiment, we utilize the
nDCG@10 metric to evaluate the quality of the top
10 search results based on their relevance and order.
Additionally, we use the HotpotQA (Yang et al.,
2018) and NaturalQA (Kwiatkowski et al., 2019)
datasets for the open-domain QA experiment and
measure the accuracy.
3

--- PAGE 4 ---
scifact trec-covid nfcorpus quora scidocs hotpotqa dbpedia fiqa fever Avg
Ours 58.41(±0.10)64.59(±0.86)32.28(±0.03)75.21(±2.27)18.39(±0.10)50.66(±0.36)41.78(±0.18)40.74(±0.09)63.21(±0.26)49.47
Mistral-7b Q2D 59.40(±0.07)61.92(±1.07)32.03(±0.07)74.79(±0.41)18.40(±0.03)52.05(±0.59)41.46(±0.48)40.37(±0.41)64.26(±0.60)49.41
DenseFTQ2E 57.06(±2.11)55.89(±2.38)31.98(±0.23)71.98(±0.98)18.29(±0.01)48.57(±0.50)39.13(±0.23)40.90(±0.71)60.85(±1.20)47.19
Q2C 59.37(±1.83)63.14(±6.22)32.15(±0.16)74.33(±2.43)18.31(±0.05)52.27(±1.18)41.75(±0.14)40.60(±0.31)62.60(±0.48)49.39
Ours 58.40(±0.09)63.24(±1.70)32.60(±0.02)75.44(±1.96)17.99(±0.06)49.84(±0.23)40.32(±0.20)40.31(±0.09)62.12(±0.39)48.92
Phi-2 Q2D 58.23(±0.29)59.50(±0.68)31.52(±0.13)75.55(±0.43)18.05(±0.01)49.28(±0.80)39.68(±0.71)39.81(±0.04)62.73(±1.01)48.26
DenseFTQ2E 57.07(±3.23)56.58(±2.82)31.47(±0.16)72.46(±1.45)18.26(±0.03)46.94(±0.43)38.44(±0.08)40.61(±0.53)59.84(±1.26)46.85
Q2C 57.84(±1.36)62.23(±4.59)32.20(±0.17)73.56(±4.99)18.00(±0.02)49.39(±0.89)40.20(±0.16)39.61(±0.45)60.95(±0.27)48.22
Ours 70.78 74.79 35.51 75.82 16.10 57.41 46.68 29.12 63.66 52.21
Mistral-7b Q2D 71.14 67.73 35.01 70.37 15.68 58.82 41.36 28.67 67.72 50.72
Sparse (BM25) Q2E 68.61 66.29 35.14 76.66 16.13 54.58 42.69 28.15 52.98 49.03
Q2C 71.63 74.45 35.29 74.86 16.10 58.71 43.16 30.91 64.36 52.16
Ours 68.48 69.71 34.38 74.47 15.47 55.20 42.93 28.91 59.20 49.86
Phi-2 Q2D 67.15 59.42 32.83 71.45 14.95 52.61 37.50 25.10 57.67 46.52
Sparse (BM25) Q2E 68.71 66.94 33.51 77.35 16.21 53.35 39.18 26.72 52.53 48.28
Q2C 69.14 69.57 35.21 75.33 15.70 55.39 39.64 28.37 59.57 49.77
Table 2: Experimental results on BEIR dataset. Highest performance is highlighted in bold , and the second highest
isunderlined.
Baselines To analyze query rewriting methods
based on LLMs, we use three baselines: query2doc
(Q2D), query2cot (Q2C), and query2expand
(Q2E). All rewriting methods use 4-shot prompts.
For Q2D, Q2E and Q2C, we reference the prompt
from Wang et al. (2023c), and Jagerman et al.
(2023). In our experiments, we employ Mistral-
7b (Jiang et al., 2023) and Phi-2 (Microsoft, 2023)
as query rewriting models including CRAFTING
THEPATH. We conduct both dense and sparse
retrieval experiments with all rewriting methods
on the 9 BEIR datasets.
Implementation Details For reliable experi-
ments, we train five different retriever models us-
ing a different seed for each method. To evalu-
ate the query rewriting methods using dense re-
trieval models, we use a total of 20 models. For
each dataset, we construct 5 ( fine-tuned retrieval
models )×4 (rewriting methods ) = 20 embedding
vectors, and compute the mean and variance. In
experiments with the query rewriting method us-
ing Phi-2, we obtain results using models trained
with the new queries written by Mistral-7b. We em-
ploy all-mpnet-base-v21as our pre-trained dense
retrieval model. For the sparse retriever system,
We use the default BM25 parameters provided by
pyterrier (Macdonald and Tonellotto, 2020)2.
1https://huggingface.co/sentence-transformers/all-mpnet-
base-v2
2https://pyterrier.readthedocs.io/3.2 Results
Main Results As shown in Table 2, CRAFT -
ING THEPATH outperforms all approaches, in-
cluding query2doc (Q2D), query2cot (Q2C), and
query2expand (Q2E), for the average score across
9 BEIR datasets. Our approach consistently demon-
strates superior performance compared to existing
methods. CRAFTING THEPATH provides a robust
application across various domains and rewriting
model sizes in both sparse and dense retrievers.
Moreover, we observe significant performance im-
provements on the trec-covid (Wang et al., 2020;
V oorhees et al., 2020) and nfcorpus (Boteva et al.,
2016) datasets for both retriever types compared
to previous methods. Especially, trec-covid dataset
requires searching for the latest information on
queries about COVID-19. CRAFTING THEPATH
proves to be more effective in finding such recent
information. However, previous methods outper-
form our approach on datasets like FEVER (Thorne
et al., 2018). The FEVER dataset is based on
Wikipedia, which is frequently used for LLMs pre-
training data. Since the training data for the Mistral-
7b and Phi-2 models are not disclosed, and verify-
ing the presence of internal parameter knowledge
directly remains a challenge, it is difficult to con-
firm the internal knowledge of the models (Wang
et al., 2023a). However, we conduct measurements
of the impact of model’s internal parameter knowl-
edge through experiments in Table 4. As results
in Table 2, we observe the significant performance
improvements in these BEIR datasets from CRAFT -
4

--- PAGE 5 ---
1 3 5 7
K464850525456Accuacy
HotpotQA
1 3 5 7
K464850525456
NaturalQA
Rewriting Method
CTP
Q2D
Q2CFigure 2: The Retrieval-Augmented Generation performance of HotpotQA (left) and NaturalQA (right), when
performing CRAFTING THEPATH (CTP), query2doc (Q2D), and query2cot (Q2C). K means the number of retrieved
passages.
MS-MARCO Passage dev
nDCG MRR Recall@1K
CRAFTING THEPATH 45.42 33.11 97.05
w/o step3 44.99 32.91 96.67
w/o step2, 3 44.59 32.42 96.17
Table 3: Ablation study on CRAFTING THEPATH
method.
INGTHEPATH rewriting methods through the use
of a structured rewriting method.
Ablation Study We conduct an ablation study
on the CRAFTING THEPATH method to observe
the influence of each step. We measure the perfor-
mance on the MS-MARCO dataset by excluding
each step as shown in Table 3. Also, We utilize
nDCG, MRR, and Recall@1K to measure perfor-
mance. Our experiment with omitting Step 3 and
also conduct experiments excluding both Steps 2
and 3. Performance declines with the removal of
each step, demonstrating that each step is essential
for performance improvement.
3.3 Analysis
Measuring the Reliance on Internal Knowledge
To evaluate the reliance of the internal model pa-
rameter knowledge in query reformulation of each
LLM, we divide the dataset into problems where
each LLM can generate the correct answer and
those where it cannot in a closed-book setting.
Based on this division, we apply three rewrit-
ing methods and measure MRR and nDCG@10
scores as shown in Table 4. Both the Mistral-
7b and Phi-2 models demonstrate superior perfor-
mance in the INCORRECT ANSWER cases when
using our rewriting method, compared to previ-HotpotQA Correct Answer Incorrect Answer
Mistral-7b Ours Q2D Q2C Ours Q2D Q2C
nDCG@10 77.80 76.00 79.27 60.05 55.57 58.77
MRR 90.04 87.73 90.90 78.76 71.71 75.35
Phi-2 Ours Q2D Q2C Ours Q2D Q2C
nDCG@10 74.72 71.03 75.43 59.42 46.93 57.64
MRR 87.95 86.17 87.75 79.16 64.63 75.59
Table 4: The impact of reliance on rewriting model
internal knowledge.
ous rewriting approaches. This demonstrates that
our method achieves more effective information
retrieval when the model needs to search unknown
information, aligning with the original purpose of
using RAG (Lewis et al., 2020a) which is to use
external knowledge when QA systems do not have
knowledge to generate correct answer. In this ex-
periment, we use the Contriever (Izacard et al.,
2022) model for the dense retriever.
Evaluating on Open-Domain QA To evaluate
the answer generation results based on retrieved
passages, we conducted performance measure-
ments using HotpotQA (Yang et al., 2018) and Nat-
uralQA (Kwiatkowski et al., 2019) datasets. We
performed retrieval using rewritten queries, alter-
ing the number of passages retrieved with three
different rewriting methods to measure accuracy as
follows:
q′=Mrewriting (prompt (q)) (6)
p1, . . . , p k=MRetriever (q′) (7)
output =MGenerator (prompt (q, p1, ..., p k))(8)
Both the generation and rewriting models utilized
the Mistral-7b (Jiang et al., 2023) model, while
5

--- PAGE 6 ---
HotpotQAMistral-7bnDCG@10 MRR
New Query
CRAFTING THEPATH 65.49 82.21
query2doc (Q2D) 61.83 76.62
query2cot (Q2C) 65.04 80.11
Replace Answer to [MASK]
CRAFTING THEPATH 64.09 81.45
query2doc (Q2D) 61.08 76.36
query2cot (Q2C) 63.92 79.59
Delete New Queries with Answer
CRAFTING THEPATH 58.29 77.46
query2doc (Q2D) 53.39 69.91
query2cot (Q2C) 56.54 73.68
Table 5: The results of an answer modification experi-
ment.
the Contriever (Izacard et al., 2022) model served
as the retrieval model. In Eq 6 and 7, qis the
original query, and q′is the rewritten query, while
p1, ..., p krepresents the retrieved passages, and the
retriever searches the top k most relevant passages.
We generate the final answer based on the retrieved
passages and the query as shown in Eq 8. For more
details on our prompts refer to the Appendix A.2.
Figure 2 illustrates that our rewriting approach
yields the best performance across both HotpotQA
and NaturalQA. Additionally, we observed that the
Q2D method’s performance decreases when the
number of passages ( k) reaches seven. However,
theCRAFTING THEPATH method demonstrates
superior accuracy compared to the Q2D and Q2C
methods. Our method performs better in situations
where the rewriting model generates incorrect an-
swers, making it effective for open-domain QA.
We conduct an analysis of this in Tables 4 and 5.
Experiment on the Impact of Answer Pres-
ence We examine the changes in retrieval per-
formance based on the presence or absence of an-
swers. In Table 5, rewriting the HotpotQA dev
dataset (Yang et al., 2018), comprising 5,447 en-
tries using Mistral-7b, results in new queries that in-
clude 1,746, 1,623, and 1,752 answers for CRAFT -
INGTHEPATH, Q2D, and Q2C, respectively. To
evaluate the impact of our model’s reliance on in-
ternal knowledge for generating answers, we con-
ducted three experiments. First, in the New Query
setting, we directly utilize the new queries gener-
ated by the LLM. Second, in the Replace Answer
to [MASK] setting, if the new query contains an
answer, we mask the answer portion. Finally, in the
Delete New Queries with Answer experiment, we
exclude any new queries from the evaluation if at
least one of the three rewriting methods generated
a new query containing an answer. The evaluationBEIRFActScore
Crafting the Path Q2D Q2C
Mistral-7b 0.718 0.506 0.711
Phi-2 0.765 0.460 0.675
Table 6: Average FActScore (Min et al., 2023) on each
method.
dataset reduces to 3,212 in Delete New Queries
with Answer experiment. Our approach achieves
the best performance across all three experiments.
We observe that replacing answers with [MASK]
results in a decrease in MRR and nDCG@10 per-
formance across all rewriting methods. Notably,
the difference in nDCG@10 scores between our
method and Q2C was 0.45 in the first experiment
and 1.75 in the third experiment. This score gap
suggests that our approach is relatively less affected
by the presence or absence of answers in the queries
compared to existing methods.
Evaluating the Factuality of Queries To deter-
mine the impact of factual errors occurring during
query rewriting on retrieval performance, we use
FActScore to measure the accuracy of Mistral-7b
and Phi-2 across three rewriting methods. Unlike
Min et al. (2023), which uses atomic facts, we
simply divide the content by sentence. For each
separated sentence, we use a gold label passage as
evidence to output as True orFalse . If the fac-
tuality evaluation results for three sentences of a
rewritten query are True ,True ,False , we assign
a score of 2/3. We calculate the average for the
measured queries. In Table 6, CRAFTING THE
PATH exhibits the highest FActScore (the smallest
factual error), demonstrating the best performance
with both the Mistral-7b and Phi-2 models. We
utilize GPT-3.5 Turbo (OpenAI, 2022, 2023) as the
factuality evaluation model.
Query Length and Latency In Figure 3, we
compare the query length for each rewriting
method. Additionally, we include the prompts uti-
lized for each rewriting method in Appendix A.2.
We observe that the CRAFTING THEPATH ap-
proach has a shorter context length than all other
methods except for Q2E. Our approach maintains
a structured format, which allows us to minimize
unnecessary word generation while achieving su-
perior performance. In Table 8, since all three
methods use the same dense retriever architecture,
the search time is equal. Therefore, we measure
the search time using the sparse retriever (BM25)
6

--- PAGE 7 ---
Case Question Crafting the Path query2cot (Q2C)
Case 1:
Effectiveness
of generating
None.Question:
What is an rs in
money?New Query: The abbreviation "RS" does not have
a standard meaning in the context of money or fi-
nance.To understand the meaning of "RS" in a given
financial context, additional context or clarifica-
tion from the source is required .None.
-(MRR: 33.3)New Query: In finance, an RS (Relative Strength)
ratio is a technical indicator that compares the
stock’s performance against a benchmark index or
another stock. It measures the strength of a secu-
rity’s price action in relation to the market, helping
investors identify potential outperformance or under
performance. - (MRR: 12.5)
Case 2:
Errors in a pre-
vious step lead
to issues in sub-
sequent genera-
tion.Question:
What is sim-
plicheck?New Query: Simplicheck is a term or concept that
may represent a specific tool, process, or concept,
but without further context, its meaning remains
unclear . To understand what Simplicheck is, more
information or context is required. None.
-(MRR: 16.7)New Query: Simplicheck is a software tool de-
signed for automated vulnerability scanning and
penetration testing. It helps identify security weak-
nesses in web applications, networks, and systems,
enabling organizations to prioritize and remediate
risks effectively. - (MRR: 100)
Case 3:
factual errors
occur.Question:
Tricare service
number.New Query: TRICARE is a health program for
eligible active duty and retired military person-
nel, their families, and survivors. To access TRI-
CARE services, members need to provide their
unique identifier, such as a service number. he TRI-
CARE service number is a 12-digit identifier, typi-
cally formatted as NNN-NNN-NNNN-NN-NN-NN.
-(MRR: 11.11)New Query: TRICARE is a health program for
active-duty military, retirees, and their families. To
obtain specific information, contact TRICARE Cus-
tomer Service at 1-877-874-2273 (option 1) for en-
rollment, eligibility, and benefits inquiries. This
number ensures access to accurate and timely infor-
mation. - (MRR: 20)
Table 7: Case study of reformulated queries generated using Mistral-7b with Crafting the Path and Q2C. MRR
represents the retrieval performance when using the new query. We represent the factual error in red and the accurate
information in blue.
Crafting the Path Q2D Q2D davinci Q2E Q2C
Rewriting Methods02004006008001000Query length
MS MARCO passage dev dataset
Figure 3: The length of the new query for each rewriting
method in the MS-MARCO passage dev dataset.
instead. Hence, we measure the time of LLM call
latency to compare the speed of each method. We
retrieve results from the MS-MARCO passage dev
dataset (Campos et al., 2016) for 1,000 entries and
average the outcomes over 100 repetitions. We
measure the latency incurred when the model gen-
erates a new query and BM25 searches for relevant
passages. Our method generates less redundant in-
formation, resulting in lower latency compared to
Q2D. Also, our method shows comparable latency
to Q2C because both methods have a low context
length.
Case Study In Table 8, we conduct a represen-
tative case analysis of both the strengths and limi-LLM call Index search
CRAFTING THEPATH 5648.9ms 261.44ms
query2doc (Q2D) 8167.5ms 335.65ms
query2cot (Q2C) 6094.2ms 281.98ms
Table 8: Latency analysis of each method in BM25
search and LLM call from the MS-MARCO passage
dev dataset.
tations of our proposed method. Case 1 shows an
improvement in performance by minimizing fac-
tual errors by generating "None". In contrast, we
observe factual errors occurring in Q2C during the
reasoning process for answer generation. The re-
trieval performance of Q2C decreases compared
toCRAFTING THEPATH due to these factual er-
rors. Conversely, our method generates "None" for
unknown contexts, minimizing factual errors and
improving retrieval performance. In Case 1, RS is
the currency of India. Q2C generates an explana-
tion of the RSI performance indicator. In Case 2,
We demonstrate that errors in a previous step prop-
agate in subsequent generations. The CRAFTING
THEPATH method fails to provide a correct expla-
nation of Simplicheck in step 1, the Query Concept
Comprehension process. In subsequent steps, it in-
dicates that more information is needed. Also, as in
Case 3, our method does not completely eliminate
factual errors. In the case of Q2C, it generates the
correct number, resulting in better retrieval perfor-
mance. On the other hand, our method generates
7

--- PAGE 8 ---
the wrong number format. Additionally, the pro-
portion of queries that generated "None" among all
rewritten queries is 3.3%.
4 Related Work
Information Retrieval Information retrieval is
the process of obtaining relevant information from
the database based on given queries. The main
two methods for information retrieval are sparse
retrieval and dense retrieval. A prominent example
of the sparse retrieval method is BM25 (Robertson
and Jones, 1976; Robertson et al., 1995), which
serves as a ranking function to evaluate the rele-
vance between a given query and documents. In
contrast, dense retrieval method (Xiong et al., 2021;
Qu et al., 2021) involves fetching passages that ex-
hibit high similarity to the query using the docu-
ment embeddings. This approach typically utilizes
pre-trained language models such as BERT (De-
vlin et al., 2018) for the encoder, and some methods
fine-tune these encoders. (Karpukhin et al., 2020;
Wang et al., 2023b). In this work, we improve
the performance of both sparse and dense retrieval
methods by focusing on query rewriting method.
LLM Based Query Rewriting Query rewrit-
ing refers to the task that modifies the original
query to improve the search results for the infor-
mation retrieval systems. Some studies on query
rewriting have employed neural networks to pro-
duce or select expansion terms (Zheng et al., 2021;
Roy et al., 2016; Imani et al., 2019), typically
through training or fine-tuning a model. In con-
trast, our approach leverages the inherent capa-
bilities of Large Language Models (LLMs) with-
out requiring training or fine-tuning. Recent stud-
ies on query rewriting mainly use the large lan-
guage models to create relevant information for
the given query. query2doc (Q2D) (Wang et al.,
2023c) operates by generating a pseudo-document
based on the original query, which is then used
as input for retriever. Similarly, query2cot (Q2C)
employs a Chain of Thought (CoT) (Wei et al.,
2022) approach, and query2expand (Q2E) (Jager-
man et al., 2023) generates semantically equivalent
query. These approaches leverage the rewriting of
the original query into a form similar to passages
in the corpus. This technique leads to significant
performance improvement compared to using the
base query alone. Furthermore, Rewrite-Retrieve-
Read (Ma et al., 2023) introduces a methodology
that enhances rewriting performance by incorporat-ing reinforcement learning. Another study, ITER-
REGEN (Shao et al., 2023), improves query qual-
ity by feeding the query and retrieved documents
into a language model for rewriting, followed by
a repeated retrieval process. Rephrase and Re-
spond (Deng et al., 2023) argues that for effective
rewriting, queries should be rephrased in a manner
that is easier for LLMs to understand. For the con-
versational serach, (Yoon et al., 2024) proposes a
method that generates a variety of queries and uses
the rank of retrieved passages to train the LLMs on
only the optimal queries. This process is further re-
fined using a DPO (Rafailov et al., 2023) approach
to create optimal queries.
Our work has similarities with recent efforts like
Q2D (Wang et al., 2023c) and Q2E (Jagerman et al.,
2023), particularly in using linguistic techniques
to expand queries. Q2D and Q2C perform well on
queries where the rewriting model can produce
accurate answers, but their performance signifi-
cantly deteriorates when inaccurate answers occur.
However, unlike previous approaches, we focus on
rewriting queries to improve search performance
when the model generates incorrect answers. We
demonstrate significant improvements in QA tasks
through this problem mitigation. Additionally, we
aim to minimize the generation of inaccurate infor-
mation and concentrate on producing the necessary
data for information retrieval.
5 Conclusion
We present a Crafting the Path, an approach that in-
volves a structured three-step process, focusing not
merely on generating additional information for the
query but primarily on generating what information
to find from the query. Our approach shows supe-
rior retrieval performance compared to the existing
rewriting method, achieves improved performance
in open domain QA by being less reliant on internal
model knowledge and demonstrates robust perfor-
mance across various models. Additionally, the
method generates fewer factual errors and delivers
improved out-of-domain performance with lower
latency than previous methods.
Tasks such as query rewriting in information re-
trieval can benefit from the advancements in LLMs.
Additionally, as LLMs become more universally
accessible, they can become a central part of infor-
mation retrieval systems. This offers the advantage
of providing users with more accurate information.
8

--- PAGE 9 ---
Limitations
Our method outperforms existing ones, but using
an LLM for query rewriting inherently introduces
latency. However, we propose a rewriting method
that, compared to existing methods, results in rela-
tively lower latency while offering better retrieval
performance. Additionally, we experimentally
demonstrate scenarios where rewriting can and can-
not generate answers. However, we do not present
an automatic method to distinguish between these
scenarios in actual applications. We leave the study
of such methods for future research.
Ethics Statement
This study conducts query rewriting and QA tasks
using an LLM, and also searches for relevant docu-
ments. Since we carry out generation tasks based
on the LLM, it is important to be aware that the
LLM may produce inappropriate responses. Ad-
ditionally, as it may retrieve inappropriate content
from the searched documents, developing manage-
ment methods for this is essential. We believe this
is a crucial area for future work.
Acknowledgement
This research was supported by Institute for Infor-
mation & Communications Technology Planning
& Evaluation (IITP) through the Korea government
(MSIT) under Grant No. 2021-0-01341 (Artifi-
cial Intelligence Graduate School Program (Chung-
Ang University)).
References
Vera Boteva, Demian Gholipour, Artem Sokolov, and
Stefan Riezler. 2016. A full-text learning to rank
dataset for medical information retrieval. In Pro-
ceedings of the European Conference on Information
Retrieval (ECIR) . Springer.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg,
Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, Li Deng, and Bhaskar Mitra. 2016. Ms
marco: A human generated machine reading compre-
hension dataset. ArXiv , abs/1611.09268.
I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua
Feng, Chunting Zhou, Junxian He, Graham Neubig,
Pengfei Liu, et al. 2023. Factool: Factuality detec-
tion in generative ai–a tool augmented framework
for multi-task and multi-domain scenarios. arXiv
preprint arXiv:2307.13528 .
Yihe Deng, Weitong Zhang, Zixiang Chen, and Quan-
quan Gu. 2023. Rephrase and respond: Let large
language models ask better questions for themselves.
arXiv preprint arXiv:2311.04205 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen
Wang. 2023. Retrieval-augmented generation for
large language models: A survey. arXiv preprint
arXiv:2312.10997 .
Ayyoob Imani, Amir Vakili, Ali Montazer, and Azadeh
Shakery. 2019. Deep neural networks for query ex-
pansion using word embeddings. In Advances in
Information Retrieval: 41st European Conference on
IR Research, ECIR 2019, Cologne, Germany, April
14–18, 2019, Proceedings, Part II 41 , pages 203–210.
Springer.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022. Unsupervised dense informa-
tion retrieval with contrastive learning. Transactions
on Machine Learning Research .
Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui
Wang, and Michael Bendersky. 2023. Query expan-
sion by prompting large language models. arXiv
preprint arXiv:2305.03653 .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Ehsan Kamalloo, Nouha Dziri, Charles LA Clarke, and
Davood Rafiei. 2023. Evaluating open-domain ques-
tion answering in the era of large language models.
arXiv preprint arXiv:2305.06984 .
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6769–6781,
Online. Association for Computational Linguistics.
9

--- PAGE 10 ---
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Transactions of the Association of Compu-
tational Linguistics .
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela. 2020a.
Retrieval-augmented generation for knowledge-
intensive nlp tasks. In Proceedings of the 34th Inter-
national Conference on Neural Information Process-
ing Systems , NIPS ’20, Red Hook, NY , USA. Curran
Associates Inc.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020b. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and
Lemao Liu. 2022. A survey on retrieval-augmented
text generation. arXiv preprint arXiv:2202.01110 .
Zheheng Luo, Qianqian Xie, and Sophia Anani-
adou. 2023. Chatgpt as a factual inconsistency
evaluator for text summarization. arXiv preprint
arXiv:2303.15621 .
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,
and Nan Duan. 2023. Query rewriting in retrieval-
augmented large language models. In Proceedings of
the 2023 Conference on Empirical Methods in Natu-
ral Language Processing , pages 5303–5315, Singa-
pore. Association for Computational Linguistics.
Craig Macdonald and Nicola Tonellotto. 2020. Declar-
ative experimentation ininformation retrieval using
pyterrier. In Proceedings of ICTIR 2020 .
Microsoft. 2023. Microsoft research blog.
https://www.microsoft.com/en-us/research/blog/
phi-2-the-surprising-power-of-small-language-models/ .
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
Factscore: Fine-grained atomic evaluation of factual
precision in long form text generation. arXiv preprint
arXiv:2305.14251 .
OpenAI. 2022. Chatgpt blog post. https://openai.
com/blog/chatgpt .
OpenAI. 2023. Gpt-4 technical report.
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang
Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, andHaifeng Wang. 2021. RocketQA: An optimized train-
ing approach to dense passage retrieval for open-
domain question answering. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 5835–5847, On-
line. Association for Computational Linguistics.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
Stephen E Robertson and K Sparck Jones. 1976. Rel-
evance weighting of search terms. Journal of the
American Society for Information science , 27(3):129–
146.
Stephen E Robertson, Steve Walker, Susan Jones,
Micheline M Hancock-Beaulieu, Mike Gatford, et al.
1995. Okapi at trec-3. Nist Special Publication Sp ,
109:109.
Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, and
Utpal Garain. 2016. Using word embeddings
for automatic query expansion. arXiv preprint
arXiv:1606.07608 .
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie
Huang, Nan Duan, and Weizhu Chen. 2023. En-
hancing retrieval-augmented large language models
with iterative retrieval-generation synergy. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 9248–9274, Singapore.
Association for Computational Linguistics.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. BEIR:
A heterogeneous benchmark for zero-shot evaluation
of information retrieval models. In Thirty-fifth Con-
ference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2) .
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction
and VERification. In Proceedings of the 2018
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers) , pages 809–819, New Orleans, Louisiana.
Association for Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
E. V oorhees, Tasmeer Alam, Steven Bedrick, Dina
Demner-Fushman, W. Hersh, Kyle Lo, Kirk Roberts,
I. Soboroff, and Lucy Lu Wang. 2020. Trec-covid:
Constructing a pandemic information retrieval test
collection. ArXiv , abs/2005.04474.
10

--- PAGE 11 ---
Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xian-
gru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi
Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al.
2023a. Survey on factuality in large language models:
Knowledge, retrieval and domain-specificity. arXiv
preprint arXiv:2310.07521 .
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,
Linjun Yang, Daxin Jiang, Rangan Majumder, and
Furu Wei. 2023b. SimLM: Pre-training with repre-
sentation bottleneck for dense passage retrieval. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 2244–2258, Toronto, Canada.
Association for Computational Linguistics.
Liang Wang, Nan Yang, and Furu Wei. 2023c.
Query2doc: Query expansion with large language
models. arXiv preprint arXiv:2303.07678 .
Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Rus-
sell Reas, Jiangjiang Yang, Darrin Eide, K. Funk,
Rodney Michael Kinney, Ziyang Liu, W. Mer-
rill, P. Mooney, D. Murdick, Devvret Rishi, Jerry
Sheehan, Zhihong Shen, B. Stilson, A. Wade,
K. Wang, Christopher Wilhelm, Boya Xie, D. Ray-
mond, Daniel S. Weld, Oren Etzioni, and Sebastian
Kohlmeier. 2020. Cord-19: The covid-19 open re-
search dataset. ArXiv .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in neural
information processing systems , 35:24824–24837.
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul N. Bennett, Junaid Ahmed, and
Arnold Overwijk. 2021. Approximate nearest neigh-
bor negative contrastive learning for dense text re-
trieval. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021 . OpenReview.net.
Ikuya Yamada, Akari Asai, and Hannaneh Hajishirzi.
2021. Efficient passage retrieval with hashing for
open-domain question answering. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers) , pages 979–986, Online.
Association for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. HotpotQA: A dataset
for diverse, explainable multi-hop question answer-
ing. In Conference on Empirical Methods in Natural
Language Processing (EMNLP) .
Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon,
Sungdong Kim, Yohan Jo, and Jaewoo Kang. 2024.
Ask optimal questions: Aligning large language
models with retriever’s preference in conversational
search. arXiv preprint arXiv:2402.11827 .Qin Zhang, Shangsi Chen, Dongkuan Xu, Qingqing
Cao, Xiaojun Chen, Trevor Cohn, and Meng Fang.
2023. A survey for efficient open domain question an-
swering. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 14447–14465, Toronto,
Canada. Association for Computational Linguistics.
Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun,
and Andrew Yates. 2021. Contextualized query ex-
pansion via unsupervised chunk selection for text
retrieval. Information Processing & Management ,
58(5):102672.
Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming
Zheng, Soujanya Poria, and Tat-Seng Chua. 2021.
Retrieving and reading: A comprehensive survey on
open-domain question answering. arXiv preprint
arXiv:2101.00774 .
11

--- PAGE 12 ---
A Appendix
A.1 Hyperparameters
We present the detailed hyperparameters in Table 9.
Name Value
Learning rate 2e-5
PLM all-mpnet-base-v2
Batch 128
Epoch 3
Learning rate decay linear
Warmup steps 1000
Binary loss margin 2.0
Similarity function dot score
Query length 128
Passage length 128
Table 9: Hyperparameters used to train dense retrieval
model.
A.2 Prompts
12

--- PAGE 13 ---
promptsInstruction: Based on the example below, write 3 steps related to the Query and answer in the same
format as the example.
Requirements:
1. In step1, sub-information from the existing query is extracted.
2. In step2, please generate what information is needed to solve the question.
3. In step3, an answer is generated based on Query, step1, and step2.
4. If you don’t have certain information, generate ’None’.
5. Please prioritize your most confident predictions.
Example:
Query: where is the Danube?
step1: The Danube is Europe’s second-longest river, flowing through Central and Eastern Europe, from
Germany to the Black Sea.
step2: To locate the Danube precisely, geographical knowledge or a map of Europe highlighting rivers is
necessary.
step3: The Danube flows through 10 countries.
Query: what is the number one formula one car?
step1: Formula One (F1) is the highest class of international automobile racing competition held by the
FIA.
step2: To know the best car, you have to look at the race records.
step3: Red Bull Racing’s RB20 is the best car.
Query: which movie did Michael Winder write?
step1: Michael Winder is a screenwriter involved in the film industry, potentially credited with writing
one or more movies.
step2: To identify the movie(s) Michael Winder wrote, access to a film database or filmography reference
is needed.
step3: Michael Winder wrote the movie "In Time" (2011).
Query: who’s the director of Predators?
step1: "Predators" is a film, and like all films, it has a director responsible for overseeing the creative
aspects of the production.
step2: To identify the director of "Predators," one needs access to movie databases, film credits, or
industry knowledge about this specific film.
step3: Nimród Antal is the director of "Predators" (2010).
Query:
Table 10: The full prompt used for C RAFTING THEPATH method.
13

--- PAGE 14 ---
promptsInstruction:
You are good at writing Passage. You are asked to write a passage that answers the given query. Do not
ask the user for further clarification.
Requirements:
1. Please write it in a similar format to the example
2. Please prioritize your most confident predictions.
Example:
Query: what state is this zip code 85282
Passage: Welcome to TEMPE, AZ 85282. 85282 is a rural zip code in Tempe, Arizona. The population
is primarily white, and mostly single. At $200,200 the average home value here is a bit higher than
average for the Phoenix-Mesa-Scottsdale metro area, so this probably isn’t the place to look for housing
bargains.5282 Zip code is located in the Mountain time zone at 33 degrees latitude (Fun Fact: this is the
same latitude as Damascus, Syria!) and -112 degrees longitude.
Query: why is gibbs model of reflection good
Passage: In this reflection, I am going to use Gibbs (1988) Reflective Cycle. This model is a recognised
framework for my reflection. Gibbs (1988) consists of six stages to complete one cycle which is able
to improve my nursing practice continuously and learning from the experience for better practice in the
future.n conclusion of my reflective assignment, I mention the model that I chose, Gibbs (1988) Reflective
Cycle as my framework of my reflective. I state the reasons why I am choosing the model as well as some
discussion on the important of doing reflection in nursing practice.
Query: what does a thousand pardons means
Passage: Oh, that’s all right, that’s all right, give us a rest; never mind about the direction, hang the
direction - I beg pardon, I beg a thousand pardons, I am not well to-day; pay no attention when I
soliloquize,
it is an old habit, an old, bad habit, and hard to get rid of when one’s digestion is all disordered with eating
food that was raised forever and ever before he was born; good land! a man can’t keep his functions
regular on spring chickens thirteen hundred years old.
Query: what is a macro warning
Passage: Macro virus warning appears when no macros exist in the file in Word. When you open
a Microsoft Word 2002 document or template, you may receive the following macro virus warning,
even though the document or template does not contain macros: C:\<path>\<file name>contains macros.
Macros may contain viruses.
Query:
Table 11: The full prompt used for query2doc (Q2D) method.
14

--- PAGE 15 ---
promptsInstruction:
Based on the example below, write keywords. Do not ask the user for further clarification
Requirements:
1. Please write it in a similar format to the example
2. Please prioritize your most confident predictions.
Example:
Query: how to include bullets in excel
Keywords: insert bullet points in excel
Query: positive predictive value formula
Keywords: calculating positive predictive value
Query: house for sale bridgewater ma
Keywords: homes for sale in bridgewater
Query: r text command
Keywords: text processing in r
Query:
Table 12: The full prompt used for query2expand (Q2E) method.
promptsInstruction:
Answer the following query. Give the rationale before answering:
Requirements:
1. Please write it in a similar format to the example
2. Please prioritize your most confident predictions.
3. Let’s think step by step.
Query: what does folic acid do
Answer: Folic acid aids in DNA synthesis, cell division, and red blood cell formation. It’s vital for fetal
development during pregnancy, preventing neural tube defects, and supporting general health.
Query: what is calomel powder used for?
Answer: Calomel powder, historically used in medicine, served as a purgative, diuretic, and syphilis
treatment. Its usage declined due to the toxic effects of mercury, leading to safer alternatives. Today, it’s
largely obsolete in medical practice.
Query: what county is dewitt michigan in?
Answer: DeWitt, Michigan, is located in Clinton County. This geographic classification helps in
understanding local governance, services, and regional affiliations, essential for residents and researchers.
Query: the importance of minerals in diet
Answer: Minerals are crucial for bodily functions, including bone health, fluid balance, and muscle
function. They support metabolic processes and the nervous system, highlighting their essential role in
maintaining overall health and preventing deficiencies.
Query:
Table 13: The full prompt used for query2cot (Q2C) method.
15
