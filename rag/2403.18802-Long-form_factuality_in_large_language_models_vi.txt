# 2403.18802.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2403.18802.pdf
# Kích thước tệp: 983383 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tính chính xác thông tin dài trong các mô hình ngôn ngữ lớn
Jerry Wei∗1Chengrun Yang∗1Xinying Song∗1Yifeng Lu∗1Nathan Hu1,2
Jie Huang1,3Dustin Tran1Daiyi Peng1Ruibo Liu1Da Huang1
Cosmo Du1Quoc V . Le1
1Google DeepMind2Stanford University3University of Illinois at Urbana-Champaign

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) thường tạo ra nội dung chứa lỗi thông tin thực tế khi phản hồi các câu hỏi tìm kiếm thông tin về các chủ đề mở. Để đánh giá tính chính xác thông tin dài của một mô hình trong các lĩnh vực mở, đầu tiên chúng tôi sử dụng GPT-4 để tạo ra LongFact, một bộ câu hỏi bao gồm hàng nghìn câu hỏi trải rộng trên 38 chủ đề. Sau đó chúng tôi đề xuất rằng các agent LLM có thể được sử dụng như các đánh giá viên tự động cho tính chính xác thông tin dài thông qua một phương pháp mà chúng tôi gọi là Search-Augmented Factuality Evaluator (SAFE). SAFE sử dụng một LLM để chia nhỏ phản hồi dài thành một tập hợp các thông tin riêng lẻ và đánh giá độ chính xác của từng thông tin bằng một quy trình suy luận nhiều bước bao gồm việc gửi các truy vấn tìm kiếm đến Google Search và xác định liệu một thông tin có được hỗ trợ bởi các kết quả tìm kiếm hay không. Hơn nữa, chúng tôi đề xuất mở rộng điểm F1 như một chỉ số tổng hợp cho tính chính xác thông tin dài. Để làm điều này, chúng tôi cân bằng tỷ lệ phần trăm các thông tin được hỗ trợ trong một phản hồi (precision) với tỷ lệ phần trăm các thông tin được cung cấp so với một siêu tham số đại diện cho độ dài phản hồi ưa thích của người dùng (recall).

Về mặt thực nghiệm, chúng tôi chứng minh rằng các agent LLM có thể vượt trội hơn các chú thích viên con người từ crowdsourcing—trên một tập hợp ~16k thông tin riêng lẻ, SAFE đồng ý với các chú thích viên con người từ crowdsourcing 72% thời gian, và trên một tập con ngẫu nhiên 100 trường hợp bất đồng, SAFE thắng 76% thời gian. Đồng thời, SAFE rẻ hơn hơn 20 lần so với các chú thích viên con người. Chúng tôi cũng đánh giá mười ba mô hình ngôn ngữ trên LongFact qua bốn họ mô hình (Gemini, GPT, Claude, và PaLM-2), phát hiện rằng các mô hình ngôn ngữ lớn hơn thường đạt được tính chính xác thông tin dài tốt hơn.
LongFact, SAFE, và tất cả mã thực nghiệm có sẵn tại https://github.com/google-deepmind/long-form-factuality .

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) đã cải thiện đáng kể trong những năm gần đây (Brown et al., 2020; Chowdhery et al., 2022; Google, 2023; OpenAI, 2023; Gemini Team, 2023, inter alia ) nhưng vẫn thiếu độ tin cậy trong việc phản hồi các câu hỏi về tính chính xác thông tin sâu sắc. Đặc biệt, chúng thường tạo ra các lỗi thông tin thực tế trong đó một tuyên bố mâu thuẫn với kiến thức nền được xác lập (Huang et al., 2023; Zhang et al., 2023, inter-alia ).2Ví dụ, các mô hình có thể phản hồi với thông tin không chính xác về các sự kiện đã được xác lập như ngày tháng, thống kê, hoặc thậm chí nghề nghiệp của một người nổi tiếng (Li et al., 2023; Min et al., 2023; Muhlgay et al., 2023). Những lỗi thông tin thực tế này làm suy yếu tính chính xác thông tin của mô hình ngôn ngữ, khiến mô hình không đáng tin cậy trong nhiều bối cảnh thực tế nơi một phản hồi chính xác về mặt thông tin thực tế được mong đợi.

∗Những người đóng góp chính. Đóng góp của tác giả được liệt kê ở cuối bài báo.
2Chúng tôi tập trung vào tính chính xác thông tin và lỗi thông tin thực tế, không phải ảo giác, vì phương pháp đánh giá được đề xuất của chúng tôi tập trung vào việc xác định liệu một phản hồi có chính xác về mặt thông tin đối với kiến thức bên ngoài đã được xác lập (tính chính xác thông tin) thay vì liệu phản hồi có nhất quán với kiến thức nội bộ của mô hình (ảo giác).

38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2403.18802v4  [cs.CL]  7 Nov 2024

--- TRANG 2 ---
Tháp Eiffel là gì? Câu hỏi 
Tháp Eiffel là 
một tòa tháp ở Paris. Nó 
mở cửa vào thế kỷ 20. Sông 
Nile ở Ai Cập. Phản hồi Search-Augmented Factuality Evaluator (SAFE) 
Tháp Eiffel là 
một tòa tháp. 
Tháp Eiffel ở 
Paris. 
Nó mở cửa vào 
thế kỷ 20. 1. Chia thành 
các thông tin riêng lẻ. 2. Chỉnh sửa để tự 
đứng vững. 
[Không thay đổi] 
[Không thay đổi] 
Tháp Eiffel  
mở cửa vào thế kỷ 
20. 3. Kiểm tra 
tính liên quan 
Sông Nile ở 
Ai Cập. [Không thay đổi] ✔
✔
✔4. Đánh giá bằng 
Google Search. 
Tháp Eiffel là 
một tòa tháp. 
Tháp Eiffel ở 
Paris. 
Tháp Eiffel 
mở cửa vào thế kỷ 
20. 
✘
Được hỗ trợ: 2 
Không được hỗ trợ: 1 
Không liên quan: 1 Kết quả đầu ra Hình 1: Bộ đánh giá tính chính xác thông tin tự động của chúng tôi, SAFE, sử dụng một mô hình ngôn ngữ lớn để đánh giá tính chính xác thông tin của một phản hồi dài đối với một câu hỏi cho trước bằng Google Search. Chúng tôi chứng minh thực nghiệm rằng SAFE vượt trội hơn các chú thích viên con người trong khi rẻ hơn hơn 20 lần (Phần 4).

Trong bài báo này, chúng tôi đề xuất một bộ câu hỏi mới gọi là LongFact, một phương pháp đánh giá tên là SAFE, và một chỉ số ( F1@K) để định lượng tính chính xác thông tin dài của một phản hồi dài. Sau đó chúng tôi đánh giá toàn diện các mô hình ngôn ngữ lớn phổ biến bằng các bộ dữ liệu và phương pháp đánh giá mới này. Đóng góp của chúng tôi như sau:

•Chúng tôi sử dụng GPT-43để tạo ra một bộ câu hỏi mới để đánh giá tính chính xác thông tin dài trong các mô hình ngôn ngữ lớn, mà chúng tôi gọi là LongFact (Phần 2). LongFact bao gồm 2,280 câu hỏi tìm kiếm thông tin thúc đẩy các phản hồi dài trải rộng trên 38 chủ đề được chọn thủ công. Theo hiểu biết của chúng tôi, LongFact là bộ câu hỏi đầu tiên để đánh giá tính chính xác thông tin dài trong nhiều lĩnh vực đa dạng. Chúng tôi công khai LongFact tại https://github.com/google-deepmind/long-form-factuality/tree/main/longfact .

•Chúng tôi đề xuất một phương pháp sử dụng một agent LLM để tự động đánh giá tính chính xác thông tin dài trong các phản hồi của mô hình. Chúng tôi sử dụng mô hình ngôn ngữ để đầu tiên phân tách một phản hồi dài thành các thông tin riêng lẻ, sau đó đối với mỗi thông tin, đề xuất các truy vấn kiểm tra thông tin để gửi đến API Google Search và lý luận về việc liệu thông tin đó có được hỗ trợ bởi kết quả tìm kiếm hay không (Phần 3). Chúng tôi đặt tên cho phương pháp này là Search-Augmented Factuality Evaluator (SAFE).4Về mặt thực nghiệm, SAFE vượt trội hơn các chú thích viên con người từ crowdsourcing, đồng ý với 72% chú thích của con người từ Min et al. (2023) và thắng 76% các trường hợp bất đồng từ một mẫu ngẫu nhiên 100 trường hợp bất đồng (Phần 4). SAFE cũng rẻ hơn 20 ×so với các chú thích viên con người. Chúng tôi phát hành SAFE tại https://github.com/google-deepmind/long-form-factuality/tree/main/eval/safe .

•Chúng tôi đề xuất rằng khi định lượng tính chính xác thông tin dài của một phản hồi mô hình, F1có thể được sử dụng thông qua một siêu tham số ước tính số lượng thông tin "lý tưởng" mà con người ưa thích trong một phản hồi. Do đó chúng tôi giới thiệu F1@K, cái mà (ngoài việc đo precision thông tin thực tế của phản hồi như tỷ lệ các thông tin được hỗ trợ) đo recall như tỷ lệ các thông tin được hỗ trợ được cung cấp trên một số lượng thông tin được hỗ trợ mong muốn biến đổi K.

•Chúng tôi tiến hành một đánh giá toàn diện mười ba mô hình ngôn ngữ lớn qua bốn họ mô hình (Gemini, GPT, Claude, và PaLM-2) trên LongFact (Phần 6). Chúng tôi đánh giá các phản hồi mô hình bằng SAFE và định lượng hiệu suất bằng F1@K, phát hiện rằng, nói chung, các mô hình ngôn ngữ lớn hơn đạt được tính chính xác thông tin dài tốt hơn.

2LongFact: Sử dụng LLM để tạo ra một benchmark đa chủ đề cho tính chính xác thông tin dài

Mặc dù có nhiều bộ câu hỏi hiện có cho tính chính xác thông tin, không có bộ câu hỏi toàn diện nào để kiểm tra tính chính xác thông tin trong các phản hồi dài ở quy mô vài đoạn văn (như được hiển thị

3Chúng tôi sử dụng gpt-4-0613 cho GPT-4.
4Trong triển khai SAFE của chúng tôi, chúng tôi sử dụng gpt-3.5-turbo-0125 như mô hình ngôn ngữ và Serper (có sẵn tại https://serper.dev/ ) như API Google Search.

--- TRANG 3 ---
Các chủ đề trong LongFact.
1112 11
4Khoa học Xã hội
Nhân vănSTEM
KhácBenchmark Tính chính xác thông tin # Chủ đề Loại phản hồi
TruthfulQA (Lin et al., 2022) 38 Câu trả lời ngắn
HaluEval (Li et al., 2023) Nhiều Câu trả lời ngắn
FreshQA (Vu et al., 2023) Nhiều Câu trả lời ngắn
HalluQA (Cheng et al., 2023b) 14 Câu trả lời ngắn
FELM (Chen et al., 2023) 5 Hỗn hợp
FActScore (Min et al., 2023) 1 Dài
LongFact (của chúng tôi) 38 Dài

Hình 2: Chúng tôi trình bày LongFact, một bộ câu hỏi được thiết kế để thăm dò tính chính xác thông tin của mô hình khi tạo ra các phản hồi dài. Trái: LongFact bao gồm 38 chủ đề (được liệt kê trong Phụ lục B.2) được phân loại thành bốn siêu danh mục. Phải: so với các bộ dữ liệu tính chính xác thông tin hiện có, LongFact (a) kiểm tra các phản hồi dài thay vì câu trả lời ngắn và (b) bao phủ một loạt chủ đề rộng.

ở phía bên phải của Hình 2). Ví dụ, nhiều benchmark đã được thiết lập như TruthfulQA (Lin et al., 2022), HaluEval (Li et al., 2023), FreshQA (Vu et al., 2023), HalluQA (Cheng et al., 2023b), và FELM (Chen et al., 2023) chủ yếu bao gồm các câu hỏi kiểm tra kiến thức về một thông tin thực tế đơn lẻ (ví dụ, "Người được xác nhận sống lâu nhất thế giới bao nhiêu tuổi?") và do đó chỉ yêu cầu một phản hồi ngắn thay vì vài đoạn văn để được trả lời đầy đủ. Các bộ dữ liệu tính chính xác thông tin khác như FActScore (Min et al., 2023) có thể yêu cầu các phản hồi dài nhưng không bao phủ một loạt chủ đề rộng.

Vì lý do này, chúng tôi trình bày LongFact —một bộ câu hỏi được thiết kế để thăm dò tính chính xác thông tin của mô hình khi tạo ra các phản hồi dài có thể trải rộng nhiều đoạn văn. Chúng tôi tạo ra LongFact bằng cách nhắc GPT-4 tạo ra các câu hỏi hỏi về một khái niệm hoặc đối tượng cụ thể trong một chủ đề cho trước (ví dụ, "sinh học") và yêu cầu một phản hồi dài chứa nhiều thông tin thực tế chi tiết. LongFact bao gồm hai nhiệm vụ, LongFact-Concepts và LongFact-Objects, được phân tách dựa trên việc liệu các câu hỏi hỏi về khái niệm hay đối tượng. Đối với cả hai nhiệm vụ, chúng tôi sử dụng cùng một bộ 38 chủ đề được chọn thủ công được liệt kê trong Phụ lục B.2 (phía bên trái của Hình 2 hiển thị sự phân chia các chủ đề đối với bốn siêu danh mục). Chúng tôi tạo ra 30 câu hỏi độc đáo cho mỗi chủ đề với tổng cộng 1,140 câu hỏi cho mỗi nhiệm vụ. Chi tiết bổ sung về quy trình tạo dữ liệu của chúng tôi có thể được tìm thấy trong Phụ lục B.1, và văn bản canary cho LongFact được hiển thị trong Phụ lục A.11. LongFact được công khai tại https://github.com/google-deepmind/long-form-factuality/tree/main/longfact .

3 SAFE: LLM agent như những người đánh giá tính chính xác thông tin tự động

Một trở ngại chính trong việc đánh giá tính chính xác thông tin dài là một benchmark yêu cầu một phương pháp đáng tin cậy để đánh giá các phản hồi mô hình. Các phương pháp đánh giá tự động hiện có như BLEURT (Sellam et al., 2020), ROUGE (Lin, 2004; Ganesan, 2018), và việc nhắc các mô hình ngôn ngữ (Min et al., 2023; Vu et al., 2023) hoạt động bằng cách so sánh một phản hồi mô hình với một câu trả lời tham chiếu được đặt trước hoặc nguồn kiến thức. Điều này hoạt động tốt cho các câu hỏi chỉ yêu cầu một phản hồi câu trả lời ngắn, vì có một câu trả lời chính xác xác định và đơn nhất. Tuy nhiên, bối cảnh này không tối ưu cho các câu hỏi yêu cầu một phản hồi dài, vì rất khó để biên soạn các câu trả lời/kiến thức được xác định trước một cách toàn diện và không dư thừa bao phủ tất cả các thông tin mà một phản hồi mô hình có thể chứa.

Thay vào đó, chúng tôi đề xuất rằng có hai thông tin quan trọng then chốt để đánh giá chính xác tính chính xác thông tin trong bối cảnh dài. Đầu tiên, chúng tôi đồng ý với ý tưởng rằng các phản hồi dài nên được đánh giá ở mức độ chi tiết của các thông tin riêng lẻ, theo sự phân tách văn bản trong Min et al. (2023). Điều này chi tiết hơn so với mức độ chi tiết của câu, vì một câu đơn có thể chứa nhiều thông tin riêng lẻ (xem Hình 1 để có ví dụ), và điều này rất cần thiết cho hiệu suất tối ưu vì việc cô lập từng thông tin riêng lẻ cho phép đánh giá chính xác và tập trung. Thứ hai, thay vì các câu trả lời tham chiếu được xác định trước hoặc nguồn kiến thức, Google Search có thể được tận dụng để có được các câu trả lời tham chiếu động cho mỗi thông tin trong một phản hồi với sự đánh đổi tốt nhất giữa độ chính xác, khả năng tiếp cận, và tính kịp thời (Augenstein et al., 2023). Điều này đảm bảo rằng việc đánh giá được thực hiện đối với các câu trả lời tham chiếu bao phủ cụ thể tập hợp các thông tin cụ thể mà một phản hồi dài có thể chứa.

Vì những lý do này, chúng tôi đề xuất Search-Augmented Factuality Evaluator (SAFE), cái mà kết hợp những thông tin này bằng cách sử dụng một mô hình ngôn ngữ để (a) chia một phản hồi dài thành các thông tin riêng lẻ tự đứng vững, (b) xác định liệu mỗi thông tin riêng lẻ có liên quan đến việc trả lời câu hỏi trong bối cảnh của

--- TRANG 4 ---
Những đóng góp của Elsa Pataky đã rất quan trọng. Thông tin Riêng lẻ 
Không Được Hỗ trợ  ✖
Lý luận giả định : 
Bài viết Wikipedia của Elsa Pataky 
không nói rõ ràng rằng những 
đóng góp của cô ấy là quan trọng. Con người 
Được Hỗ trợ  ✔
Truy vấn tìm kiếm #1: Elsa Pataky career achievements and impact 
●Kết quả:  Pataky is … known for her role  … in the Fast & Furious franchise .
Truy vấn tìm kiếm #2:  Elsa Pataky impact on entertainment industry 
●Kết quả:  With a career spanning over two decades , … a prominent ﬁgure in the entertainment industry .
Truy vấn tìm kiếm #3: Elsa Pataky contributions to society 
●Kết quả:  Pataky has collaborated with numerous brands  and serves as an ambassador for Women's 
Secret, Gioseppo, and others  …
Lý luận cuối cùng: 
Based on the provided knowledge, Elsa Pataky's contributions in the entertainment industry, philanthropy, 
and brand collaborations can be considered signiﬁcant. SAFE Hình 3: SAFE có thể đánh giá chính xác các tuyên bố thông tin thực tế yêu cầu đánh giá nhiều bước bằng cách tận dụng một LLM để lặp đi lặp lại phát hành các truy vấn Google Search và lý luận với kết quả tìm kiếm.

phản hồi, và (c) đối với mỗi thông tin liên quan, lặp đi lặp lại phát hành các truy vấn Google Search trong một quy trình nhiều bước và lý luận về việc liệu kết quả tìm kiếm có hỗ trợ hay không hỗ trợ thông tin đó. Chúng tôi coi sự đổi mới chính trong SAFE là sử dụng một mô hình ngôn ngữ như một agent để tạo ra các truy vấn Google Search nhiều bước và lý luận cẩn thận về việc liệu các thông tin có được hỗ trợ bởi kết quả tìm kiếm hay không (Hình 3 và Phụ lục C.2 hiển thị các ví dụ về những chuỗi lý luận này).

Như được hiển thị trong Hình 1, để chia một phản hồi dài thành các thông tin riêng lẻ tự đứng vững, đầu tiên chúng tôi nhắc mô hình ngôn ngữ chia mỗi câu trong một phản hồi dài thành các thông tin riêng lẻ. Sau đó chúng tôi chỉnh sửa mỗi thông tin riêng lẻ để tự đứng vững bằng cách hướng dẫn mô hình thay thế các tham chiếu mơ hồ (ví dụ, đại từ) với các thực thể thích hợp mà chúng đang tham chiếu trong bối cảnh của phản hồi. Để đánh giá mỗi thông tin riêng lẻ tự đứng vững, chúng tôi sử dụng mô hình ngôn ngữ để lý luận về việc liệu thông tin có liên quan đến việc trả lời câu hỏi trong bối cảnh của phản hồi hay không.5Mỗi thông tin liên quan còn lại sau đó được đánh giá là "được hỗ trợ" hoặc "không được hỗ trợ" bằng một cách tiếp cận nhiều bước: Trong mỗi bước, mô hình tạo ra một truy vấn tìm kiếm dựa trên thông tin để đánh giá và kết quả tìm kiếm đã được thu thập trước đó. Chúng tôi giữ ba kết quả tìm kiếm cho mỗi truy vấn từ API Search. Sau một số bước được đặt trước (năm trong cài đặt của chúng tôi), mô hình thực hiện lý luận để xác định liệu thông tin có được hỗ trợ bởi kết quả tìm kiếm hay không, như được hiển thị trong Hình 3. Khi tất cả các thông tin được đánh giá, các chỉ số đầu ra từ SAFE cho một cặp câu hỏi–phản hồi cho trước là số lượng thông tin "được hỗ trợ", số lượng thông tin "không liên quan", và số lượng thông tin "không được hỗ trợ".

Chúng tôi mô tả triển khai chi tiết của SAFE trong Phụ lục C.1, và mã được công khai tại https://github.com/google-deepmind/long-form-factuality/tree/main/eval/safe . Phụ lục C.2 và C.3 hiển thị các ví dụ về đánh giá thành công và thất bại. Phụ lục C.4 phân tích hiệu suất của việc chia một phản hồi thành các thông tin riêng lẻ. Phụ lục C.5 nghiên cứu sự đa dạng của các truy vấn kiểm tra thông tin. Phụ lục C.6 hiển thị hiệu suất trên các chỉ số đánh giá cấp độ phản hồi như số lượng thông tin được hỗ trợ. Phụ lục C.7 ablates phạm vi tìm kiếm. Phụ lục C.8 ablates các siêu tham số trong bước đánh giá. Phụ lục C.9 nghiên cứu hiệu ứng của việc sử dụng mô hình Mixtral 8 ×7B mã nguồn mở trong SAFE. Phụ lục C.10 thảo luận về hiệu suất của SAFE trong các lĩnh vực chuyên nghiệp như luật và y học.

4 LLM agent có thể là những người chú thích tính chính xác thông tin tốt hơn con người

LLM đã đạt được hiệu suất siêu nhân trên các benchmark lý luận (Bubeck et al., 2023; Gemini Team, 2023; Li et al., 2022) và tính chính xác thông tin cao hơn con người trong các nhiệm vụ tóm tắt (Pu et al., 2023). Tuy nhiên, việc sử dụng các chú thích viên con người vẫn phổ biến trong nghiên cứu mô hình ngôn ngữ, điều này thường tạo ra một nút thắt cổ chai do chi phí cao của các chú thích viên con người và sự biến thiên trong cả tốc độ đánh giá và chất lượng. Do sự không phù hợp này, chúng tôi điều tra cách SAFE so sánh với các chú thích của con người và liệu nó có thể thay thế các người đánh giá con người trong việc đánh giá tính chính xác thông tin dài hay không.

5Điều này xác định các tuyên bố không nên được đánh giá (ví dụ, "Tôi không biết câu trả lời cho điều đó").

--- TRANG 5 ---
115284483Đồng ý
Không đồng ý
(72%)(28%)SAFE vs chú thích của con người.
Hình 4: Trên 16,011 thông tin riêng lẻ,
các chú thích SAFE đồng ý với 72.0%
các chú thích của con người.Để đánh giá định lượng chất lượng của các chú thích thu được bằng SAFE, chúng tôi tận dụng các chú thích con người từ crowdsourcing được phát hành từ Min et al. (2023). Những dữ liệu này chứa 496 cặp câu hỏi–phản hồi trong đó các phản hồi đã được chia thủ công thành các thông tin riêng lẻ (với tổng cộng 16,011 thông tin riêng lẻ), và mỗi thông tin riêng lẻ đã được gắn nhãn thủ công là được hỗ trợ, không liên quan, hoặc không được hỗ trợ. Đối với mỗi thông tin riêng lẻ, chúng tôi thu được một chú thích từ SAFE bằng cách chạy pipeline SAFE, bắt đầu từ việc chỉnh sửa thông tin để tự đứng vững (tức là, Bước 2–4 trong Hình 1).6Sau đó chúng tôi so sánh các chú thích SAFE với các chú thích của con người ở cấp độ thông tin riêng lẻ.

Đầu tiên, chúng tôi so sánh trực tiếp chú thích SAFE và chú thích con người của mỗi thông tin riêng lẻ, phát hiện rằng SAFE đồng ý với con người trên 72.0% các thông tin riêng lẻ (xem Hình 4). Điều này cho thấy rằng SAFE đạt được hiệu suất cấp độ con người trên đa số các thông tin riêng lẻ. Sau đó chúng tôi kiểm tra một tập con được lấy mẫu ngẫu nhiên 100 thông tin riêng lẻ mà chú thích từ SAFE không đồng ý với chú thích từ các người đánh giá con người. Chúng tôi chú thích lại thủ công mỗi thông tin này (cho phép truy cập Google Search thay vì chỉ Wikipedia để có các chú thích toàn diện hơn), và chúng tôi sử dụng những nhãn này như sự thật nền tảng. Chúng tôi phát hiện rằng trên những trường hợp bất đồng này, các chú thích SAFE đúng 76% thời gian, trong khi các chú thích con người chỉ đúng 19% (xem Hình 5), đại diện cho tỷ lệ thắng 4 so với 1 cho SAFE. Các ví dụ về đánh giá SAFE đúng và đánh giá SAFE sai được hiển thị trong Phụ lục C.2 và Phụ lục C.3, tương ứng. Phụ lục A.3 phân tích nguyên nhân thất bại cho SAFE, và Phụ lục A.4 phân tích nguyên nhân thất bại giả định cho các chú thích viên con người.

Để đánh giá tất cả các thông tin riêng lẻ từ 496 cặp câu hỏi–phản hồi, SAFE đã phát hành các cuộc gọi API GPT-3.5-Turbo với chi phí $64.57 và các cuộc gọi API Serper với chi phí $31.74, do đó chi phí tổng cộng $96.31, tương đương $0.19 cho mỗi phản hồi mô hình. Để so sánh, Min et al. (2023) báo cáo chi phí $4 cho mỗi phản hồi mô hình để thu được các chú thích con người từ crowdsourcing cho bộ dữ liệu. Điều này có nghĩa là ngoài việc vượt trội hơn các người đánh giá con người, SAFE cũng rẻ hơn hơn 20 ×so với các chú thích viên con người từ crowdsourcing. Nhìn chung, hiệu suất vượt trội của SAFE chứng minh rằng các mô hình ngôn ngữ có thể được sử dụng như các người đánh giá tự động có thể mở rộng có tiềm năng vượt trội hơn các chú thích viên con người từ crowdsourcing.

76195SAFE
Con người
Không bên nàoTrường hợp bất đồng thắng.
Con người SAFE01234Chi phí chú thích ($)Cuộc gọi LLM
Cuộc gọi Google Search

Hình 5: SAFE vượt trội hơn các chú thích viên con người trong khi rẻ hơn hơn 20 ×. Trái: trên một tập con được lấy mẫu ngẫu nhiên 100 thông tin riêng lẻ nơi chú thích từ SAFE và con người bất đồng, các chú thích SAFE đúng 76% thời gian (chúng tôi sử dụng nhãn "nhà nghiên cứu + truy cập internet đầy đủ" như sự thật nền tảng). Phải: đánh giá một phản hồi mô hình đơn lẻ chi phí $4.00 bằng các chú thích con người (Min et al., 2023), so với $0.19 bằng SAFE được cấu hình với GPT-3.5-Turbo và API Serper.

5 F1@K: Mở rộng F1 với recall từ độ dài ưa thích của con người

Trong tính chính xác thông tin dài, các phản hồi mô hình lý tưởng là có tính chính xác thông tin (được đo bằng precision—tỷ lệ phần trăm thông tin được hỗ trợ trong tất cả các thông tin trong toàn bộ phản hồi) và dài (được đo bằng recall—tỷ lệ phần trăm thông tin được cung cấp trong tất cả các thông tin liên quan nên xuất hiện trong phản hồi). Các công trình trước đây như Min et al. (2023) và Tian et al. (2023) tập trung vào precision thông tin thực tế vì tính toán precision được thực hiện dễ dàng bằng cách đánh giá các thông tin riêng lẻ từ một phản hồi dài. Tương tự, cho một phản hồi mô hình y,

6Chúng tôi hiển thị hiệu suất của bước 1 của SAFE (chia một phản hồi thành các thông tin riêng lẻ) trong Phụ lục C.4.

--- TRANG 6 ---
020406080100PaLM-2-L-ITClaude-2.1Claude-2.0Claude-3-HaikuClaude-InstantGPT-3.5-TurboGemini-ProGPT-4Claude-3-OpusClaude-3-SonnetPaLM-2-L-IT-RLHFGemini-UltraGPT-4-Turbo 95.0
88.0
79.691.7
83.791.0
31.189.389.4
73.5
67.968.773.8
F1@64(%)020406080100PaLM-2-L-ITClaude-2.1Claude-2.0Claude-3-HaikuClaude-InstantGPT-3.5-TurboGPT-4Gemini-ProClaude-3-OpusClaude-3-SonnetPaLM-2-L-IT-RLHFGemini-UltraGPT-4-Turbo 66.4
48.0
40.560.3
50.455.3
13.250.651.4
35.8
33.734.037.6
F1@178(%)

Hình 6: Hiệu suất tính chính xác thông tin dài của mười ba mô hình ngôn ngữ lớn qua bốn họ mô hình (Gemini, GPT, Claude, và PaLM-2), được sắp xếp theo thứ tự F1@Kgiảm dần. F1@Ktrung bình trên kết quả đánh giá SAFE (Phần 3) trên các phản hồi mô hình cho cùng một bộ 250 câu hỏi ngẫu nhiên từ LongFact-Objects. F1@Kđược tính toán với K= 64 (số trung vị các thông tin trong tất cả phản hồi mô hình) và K= 178 (số tối đa các thông tin trong tất cả phản hồi mô hình). Thứ hạng của các mô hình vẫn tương đối ổn định ở các giá trị Kđủ lớn (thảo luận thêm trong Phụ lục E.3). Các chỉ số thô được hiển thị trong Bảng 2.

vì SAFE đầu ra số lượng thông tin được hỗ trợ S(y)và số lượng thông tin không được hỗ trợ N(y)trong phản hồi đó, chúng tôi tính toán precision thông tin thực tế của một phản hồi như Prec (y) =S(y)S(y)+N(y).7

Tuy nhiên, đo recall thách thức hơn, vì không thể đưa ra một tập hợp thông tin xác định nên được bao gồm trong một phản hồi dài (xem Phụ lục A.2 để thảo luận bổ sung về tại sao điều này đúng, và xem Phụ lục A.6 và Phụ lục A.9 để thảo luận về tại sao recall nên được bao gồm). Thay vào đó, chúng tôi giả định rằng có một số lượng thông tin được hỗ trợ Kmà người dùng quan tâm đến recall lên đến thông tin được hỗ trợ thứ K. Các thông tin được hỗ trợ bổ sung sau thông tin được hỗ trợ thứ Kkhông nên tăng thêm recall (chúng tôi thảo luận Knhư một siêu tham số trong Phụ lục D.3).8Do đó chúng tôi tính toán recall thông tin thực tế của một phản hồi như RK(y) =minS(y)K,1.

Cuối cùng, chúng tôi kết hợp precision và recall thông tin thực tế bằng F1 tiêu chuẩn, như được hiển thị trong Phương trình (1). Chỉ số được đề xuất này được giới hạn giữa [0,1], nơi một số cao hơn cho thấy một phản hồi có tính chính xác thông tin hơn. Để một phản hồi nhận được điểm đầy đủ là 1, nó không được chứa các thông tin không được hỗ trợ và phải có ít nhất Kthông tin được hỗ trợ. F1@Kcũng tuân theo kỳ vọng cho nhiều trường hợp giả định rõ ràng, như được hiển thị trong Phụ lục D.2, và chúng tôi lập luận rằng nó cho phép so sánh tiêu chuẩn hóa và định lượng giữa các mô hình ngôn ngữ và giữa các phương pháp thu được phản hồi.

F1@K(y) =(
2Prec(y)RK(y)
Prec(y)+RK(y),ifS(y)>0,
0, ifS(y) = 0.(1)

Phương trình (1): F1@Kđo tính chính xác thông tin dài của một phản hồi mô hình ygiven số lượng thông tin được hỗ trợ S(y)và số lượng thông tin không được hỗ trợ N(y)trong y. Siêu tham số Kchỉ ra số lượng thông tin được hỗ trợ cần thiết để một phản hồi đạt được recall đầy đủ. Chúng tôi thảo luận cách Knên được chọn trong Phụ lục D.3.

6 LLM lớn hơn có tính chính xác thông tin hơn

Trong khi nghiên cứu trước đây đã cố gắng đánh giá tính chính xác thông tin dài qua các mô hình ngôn ngữ (Min et al., 2023), công trình của chúng tôi trình bày một bộ dữ liệu toàn diện hơn, một phương pháp đánh giá có thể mở rộng hơn,

7Mặc dù các phản hồi cũng có thể chứa các thông tin "không liên quan", chúng tôi lập luận rằng những thông tin này không nên được xem xét khi đánh giá tính chính xác thông tin. Chúng tôi thảo luận điều này thêm trong Phụ lục A.5
8Đo lường recall của chúng tôi giả định rằng một phản hồi không chứa các thông tin lặp lại (xem Phụ lục D.1).

--- TRANG 7 ---
và một chỉ số tổng hợp xem xét recall thông tin thực tế. Vì những lý do này, chúng tôi đánh giá mười ba mô hình ngôn ngữ lớn phổ biến qua bốn họ mô hình (được hiển thị trong Bảng 1)—Gemini (Gemini Team, 2023), GPT (OpenAI, 2022, 2023), Claude (Anthropic, 2023, 2024), và PaLM-2. Chúng tôi đánh giá mỗi mô hình trên cùng một tập con ngẫu nhiên 250 câu hỏi9từ LongFact-Objects,10và chúng tôi giải mã các phản hồi mô hình lên đến 1,024 token ở nhiệt độ không. Sau đó chúng tôi sử dụng SAFE để thu được các chỉ số đánh giá thô cho mỗi phản hồi mô hình, mà chúng tôi tổng hợp bằng F1@Knhư được mô tả trong Phần 5 (chúng tôi chọn K= 64 , số trung vị các thông tin liên quan trong tất cả phản hồi mô hình cho các câu hỏi được kiểm tra, và K= 178 , số tối đa các thông tin liên quan trong một phản hồi trong tất cả phản hồi mô hình cho các câu hỏi được kiểm tra).

Họ mô hình Tên mô hình (viết tắt)
Gemini Gemini-Ultra, Gemini-Pro
GPTGPT-4-0125-Preview (GPT-4-Turbo),
GPT-4-0613 (GPT-4), GPT-3.5-Turbo-
0125 (GPT-3.5-Turbo)
ClaudeClaude-3-Opus-20240229 (Claude-3-
Opus), Claude-3-Sonnet-20240229
(Claude-3-Sonnet), Claude-2.1, Claude-
2.0, Claude-Instant-1.2 (Claude-Instant)
Claude-3-Haiku-20240307 (Claude-3-
Haiku)
Nội bộ PaLM-2-L-IT-RLHF, PaLM-2-L-IT

Bảng 1: Các mô hình được đánh giá trong bài báo này.Như được hiển thị trong Hình 6 và Bảng 2, chúng tôi thường phát hiện rằng các mô hình lớn hơn đạt được tính chính xác thông tin dài tốt hơn. Ví dụ, GPT-4-Turbo tốt hơn GPT-4 cái mà tốt hơn GPT-3.5-Turbo, Gemini-Ultra có tính chính xác thông tin hơn Gemini-Pro, và PaLM-2-L-IT-RLHF tốt hơn PaLM-2-L-IT. Chúng tôi cũng thấy rằng ba mô hình có tính chính xác thông tin nhất ở cả hai giá trị Kđược chọn là GPT-4-Turbo, Gemini-Ultra, và PaLM-2-L-IT-RLHF, tất cả đều có vẻ tương ứng với mô hình lớn nhất trong họ tương ứng của chúng (đối với một yếu tố tỷ lệ nào đó, không nhất thiết là số lượng tham số). Nhiều mô hình từ các họ mô hình mới hơn như Gemini, Claude-3-Opus, và Claude-3-Sonnet có thể sánh bằng hoặc vượt qua hiệu suất GPT-4, điều này không hoàn toàn bất ngờ vì GPT-4 ( gpt-4-0613 ) là một mô hình ngôn ngữ cũ hơn. Đáng chú ý, chúng tôi phát hiện rằng Claude-3-Sonnet đạt được tính chính xác thông tin dài tương tự như Claude-3-Opus mặc dù là một mô hình nhỏ hơn, nhưng không có quyền truy cập vào chi tiết thêm về những mô hình này, không rõ tại sao điều này xảy ra. Claude-3-Haiku cũng có tính chính xác thông tin dài có vẻ thấp so với các mô hình cũ hơn trong cùng họ như Claude-Instant, mặc dù điều này có thể là kết quả của việc ưu tiên precision thông tin thực tế hơn recall, như được hiển thị trong Bảng 2 và được thảo luận thêm trong Phụ lục E.2.11

Chúng tôi thảo luận thêm các thông tin từ những kết quả này trong Phụ lục E, bao gồm cách tỷ lệ có thể tác động đến tính chính xác thông tin dài (Phụ lục E.2), hiệu ứng của Reinforcement Learning from Human Feedback (Phụ lục E.4), và cách thứ hạng có thể thay đổi đối với Kđược sử dụng để tính toán F1@K(Phụ lục E.3).

7 Công trình liên quan

Đánh giá tính chính xác thông tin. Các nghiên cứu gần đây về tính chính xác thông tin trong các mô hình ngôn ngữ lớn đã trình bày một số benchmark tính chính xác thông tin. Nhiều benchmark như FELM (Chen et al., 2023), FreshQA (Vu et al., 2023), và HaluEval (Li et al., 2023) đơn giản là kiểm tra kiến thức của mô hình về các thông tin thực tế khác nhau. Mặt khác, các benchmark đối nghịch như TruthfulQA (Lin et al., 2022) và HalluQA (Cheng et al., 2023b) bao gồm các câu hỏi được thiết kế để kiểm tra liệu một mô hình ngôn ngữ có tạo ra một thông tin thực tế sai được học từ việc bắt chước các hiểu lầm phổ biến hay không. Trong bối cảnh dài, Min et al. (2023) đề xuất một bộ câu hỏi yêu cầu các mô hình ngôn ngữ tạo ra tiểu sử về mọi người, được đánh giá đối với trang Wikipedia tương ứng của mỗi người. Bộ câu hỏi được đề xuất của chúng tôi không tranh cãi tầm quan trọng của những nhiệm vụ này, mà thay vào đó nhằm cung cấp phạm vi bao phủ rộng của tính chính xác thông tin trong bối cảnh dài, trong khi các benchmark hiện có hoặc bao phủ các thông tin thực tế ngắn đơn lẻ hoặc chỉ bao phủ một tập nhỏ các chủ đề dài.

Đánh giá tính chính xác thông tin trong các phản hồi mô hình. Để đánh giá đầy đủ tính chính xác thông tin của một mô hình, phải tồn tại một phương pháp đáng tin cậy để định lượng tính chính xác thông tin của một phản hồi mô hình đối với một câu hỏi cho trước.

9Để tăng độ khó của câu hỏi, chúng tôi đã thêm một postamble cố định vào mỗi câu hỏi—postamble này yêu cầu mô hình cung cấp càng nhiều chi tiết cụ thể càng tốt. Chúng tôi thảo luận postamble này thêm trong Phụ lục A.7.
10Chúng tôi loại trừ LongFact-Concepts vì những lý do được thảo luận trong Phụ lục A.8.
11Như được hiển thị trong Bảng 2, recall thông tin thực tế đã cải thiện đáng kể hơn precision thông tin thực tế trong những nỗ lực gần đây để tỷ lệ các mô hình ngôn ngữ.

--- TRANG 8 ---
MôhìnhCác chỉ số thô Các chỉ số tổng hợp
S NS I Prec R64R178 F1@64F1@178
Gemini-Ultra 83.4 13.1 7.6 86.2 98.3 46.9 91.7 60.3
Gemini-Pro 66.6 12.1 5.5 82.0 88.5 37.4 83.7 50.4
GPT-4-Turbo 93.6 8.3 6.1 91.7 99.0 52.6 95.0 66.4
GPT-4 59.1 7.0 2.4 89.4 88.3 33.2 88.0 48.0
GPT-3.5-Turbo 46.9 4.5 1.2 90.8 72.4 26.4 79.6 40.5
Claude-3-Opus 63.8 8.1 2.8 88.5 91.4 35.9 89.3 50.6
Claude-3-Sonnet 65.4 8.4 2.6 88.5 91.4 36.7 89.4 51.4
Claude-3-Haiku 39.8 2.8 1.3 92.8 62.0 22.4 73.5 35.8
Claude-2.1 38.3 6.5 1.9 84.8 59.8 21.5 67.9 33.7
Claude-2.0 38.5 6.3 1.3 85.5 60.0 21.6 68.7 34.0
Claude-Instant 43.9 8.1 1.3 84.4 68.4 24.6 73.8 37.6
PaLM-2-L-IT-RLHF 72.9 8.7 4.3 89.1 94.0 41.0 91.0 55.3
PaLM-2-L-IT 13.2 1.8 0.1 88.8 20.6 7.4 31.1 13.2

Bảng 2: Kết quả đánh giá đầy đủ trên cùng 250 câu hỏi ngẫu nhiên từ LongFact-Objects cho mười ba mô hình ngôn ngữ được đánh giá trong Phần 6. Các chỉ số được trung bình trên tất cả các câu hỏi. Các chỉ số thô được thu từ việc đánh giá các phản hồi bằng SAFE (xem Phần 3). S, NS, và I biểu thị số lượng thông tin được hỗ trợ, không được hỗ trợ, và không liên quan, tương ứng. Các chỉ số tổng hợp là precision (Prec), recall ( RK), và F1@KtạiK= 64 vàK= 178 (xem Phần 5). Các số in đậm là tốt nhất của mỗi chỉ số.

Hiệu suất trên các bộ dữ liệu tính chính xác thông tin với câu trả lời nền tảng ngắn thường có thể được đánh giá bằng các mô hình ngôn ngữ đã được tinh chỉnh trên các đánh giá của con người (Lin et al., 2022; Sellam et al., 2020; Lin, 2004). Mặt khác, các phản hồi dài khó đánh giá hơn vì chúng chứa một loạt các tuyên bố thông tin thực tế có thể rộng hơn nhiều. Ví dụ, FActScore (Min et al., 2023) tận dụng các trang Wikipedia như nguồn kiến thức cho con người và mô hình ngôn ngữ sử dụng khi đánh giá các phản hồi dài về tiểu sử của mọi người. Trong bối cảnh không tham chiếu, RAGAS (Es et al., 2023) đánh giá liệu một phản hồi từ hệ thống tạo ra được tăng cường bằng truy xuất (RAG) có tự đứng vững bằng cách nhắc một mô hình ngôn ngữ lớn kiểm tra tính nhất quán giữa câu hỏi, bối cảnh được truy xuất, câu trả lời, và các tuyên bố trong câu trả lời. Điều này khớp với nghiên cứu trường hợp trong Guan et al. (2023), cái mà cũng cho thấy rằng các mô hình ngôn ngữ có thể phục vụ như một công cụ mạnh mẽ để xác minh thông tin thực tế. Tian et al. (2023) tận dụng trực giác này để chứng minh rằng các xếp hạng ưa thích tính chính xác thông tin thu được thông qua độ tin cậy mô hình có thể được sử dụng để cải thiện tính chính xác thông tin trong các phản hồi thông qua tối ưu hóa ưa thích trực tiếp. Cải thiện trong đánh giá của con người cũng có thể quan trọng để đánh giá tính chính xác thông tin; trong lĩnh vực này, Cheng et al. (2023a) tạo ra một giao diện để đánh giá của con người bằng cách hỏi các câu hỏi xác minh thông tin thực tế và kiểm tra tính nhất quán tự của các phản hồi mô hình. Liên quan nhất đến công trình của chúng tôi, Chern et al. (2023) đề xuất một phương pháp đánh giá các phản hồi mô hình bằng cách chia nhỏ chúng và đánh giá các thành phần riêng lẻ thông qua các công cụ bên ngoài và áp dụng điều này trong các nhiệm vụ như tạo mã, lý luận toán học, và QA câu trả lời ngắn. Phương pháp được đề xuất của chúng tôi lấy cảm hứng từ công trình này, nhưng áp dụng nó trong bối cảnh tính chính xác thông tin dài, tương tự như các công trình khác sử dụng các mô hình ngôn ngữ được tăng cường bằng tìm kiếm để xác minh thông tin thực tế (Gao et al., 2023; Wang et al., 2023; Zhang & Gao, 2023). So với những phương pháp trước đây để đánh giá tính chính xác thông tin, SAFE tự động, không yêu cầu bất kỳ tinh chỉnh mô hình hoặc bất kỳ câu trả lời/nguồn kiến thức tham chiếu được đặt trước nào, và có thể chứng minh đáng tin cậy hơn các chú thích con người từ crowdsourcing trong khi chỉ yêu cầu một phần nhỏ chi phí.

Định lượng tính chính xác thông tin dài. Tính chính xác thông tin dài khó định lượng vì chất lượng của phản hồi bị ảnh hưởng bởi cả tính chính xác thông tin của phản hồi (precision) và phạm vi bao phủ của phản hồi (recall). FActScore (Min et al., 2023) đo precision đối với các trang Wikipedia như nguồn kiến thức và đề cập đến khó khăn của việc đo recall như công việc tương lai. Tian et al. (2023) và Dhuliawala et al. (2023) cũng sử dụng precision như một đại diện cho tính chính xác thông tin của một phản hồi mô hình. Các chỉ số khác như điểm F1 cấp độ token (Rajpurkar et al., 2016), AUC cấp độ câu hoặc đoạn văn (Manakul et al., 2023), và sự tương tự với sự thật nền tảng (Wieting et al., 2022) có thể tính đến recall cũng như precision nhưng yêu cầu các phản hồi hoặc đánh giá của con người phục vụ như sự thật nền tảng. F1@Kđược đề xuất của chúng tôi, mặt khác, đo cả precision và recall của một phản hồi dài mà không yêu cầu bất kỳ câu trả lời sự thật nền tảng được đặt trước nào, chỉ số lượng thông tin từ mỗi danh mục nhãn.

8 Hạn chế

Cả LongFact và SAFE đều dựa vào LLM để hoạt động, do đó khả năng của LLM được sử dụng (đặc biệt là tuân theo hướng dẫn và lý luận, nhưng cũng có khả năng như sáng tạo) trực tiếp ảnh hưởng đến chất lượng của các câu hỏi LongFact được tạo ra và SAFE. Đối với việc tạo ra LongFact, một mô hình ngôn ngữ không thể tuân theo hướng dẫn có thể tạo ra các câu hỏi không nằm trong chủ đề cho trước hoặc không thúc đẩy các phản hồi dài. Vì đầu ra mô hình để tạo ra LongFact tương đối ngắn, tuy nhiên, chúng tôi sử dụng GPT-4 để tối đa hóa chất lượng của các câu hỏi được tạo ra. Trong SAFE, một mô hình ngôn ngữ yếu có thể (a) phân tách một phản hồi dài thành các thông tin riêng lẻ chứa nhiều hơn một tuyên bố thông tin thực tế, (b) thể hiện lý luận kém khi xác định tính liên quan của một thông tin riêng lẻ đối với câu hỏi, (c) đề xuất các truy vấn tìm kiếm không liên quan để xác minh một thông tin, hoặc (d) thất bại trong việc lý luận về việc liệu một thông tin có được hỗ trợ bởi kết quả tìm kiếm hay không. Vì lý do này, cấu hình SAFE của chúng tôi sử dụng GPT-3.5-Turbo, cái mà cung cấp hiệu suất mạnh mẽ trong hầu hết các trường hợp với chi phí thấp, mặc dù nó vẫn thể hiện một số thất bại do khả năng mô hình (được kiểm tra thêm trong Phụ lục A.3). Công việc tương lai có thể kiểm tra liệu tinh chỉnh một mô hình ngôn ngữ tiết kiệm chi phí để thực hiện các bước trong SAFE có thể giảm thất bại trong khi duy trì chi phí thời gian suy luận thấp hay không.

Cũng tồn tại một điểm yếu vốn có trong SAFE, đó là nó dựa vào Google Search như một nguồn kiến thức để thu được sự thật nền tảng, cái mà có thể không đủ trong các trường hợp góc cạnh. Ví dụ, Google Search có thể không dễ dàng tìm thấy thông tin về một số thông tin thực tế hoặc có thể thiếu chiều sâu trong các lĩnh vực cấp độ chuyên gia như luật và y học. Đồng thời, tuy nhiên, Google Search cho phép truy cập vào toàn bộ internet và do đó có thể là nguồn kiến thức toàn diện nhất cho các nhiệm vụ tính chính xác thông tin lĩnh vực mở cũng có thể tiếp cận rộng rãi. Chúng tôi giảm bớt mối lo ngại này bằng cách báo cáo các nhãn của chúng tôi là "được hỗ trợ" hoặc "không được hỗ trợ" bởi kết quả Google Search, thay vì cố gắng gắn nhãn các thông tin thực tế là chính xác hoặc không chính xác toàn cầu (để thảo luận về lựa chọn nhãn, xem Phụ lục A.2), mặc dù công việc tương lai có thể điều tra liệu có những đại diện tốt hơn cho tính chính xác thông tin toàn cầu hơn việc xác minh bằng Google Search hay không. Ngoài ra, SAFE có thể dễ dàng được điều chỉnh để tìm kiếm trên các nguồn hạn chế và được xác định trước hơn bằng cách thêm "site:xxx" vào truy vấn Google Search hoặc thay thế Serper bằng API khác truy vấn các cơ sở dữ liệu chuyên nghiệp.

Hơn nữa, trong khi F1@Klà một tổng hợp mạnh mẽ của precision và recall thông tin thực tế, nó hoạt động dưới giả định rằng một phản hồi không chứa bất kỳ thông tin thực tế lặp lại nào. Mặc dù đây là một giả định hợp lý trong thực tế, nó cũng cung cấp cơ hội cho chỉ số của chúng tôi bị lợi dụng bằng cách lặp lại một thông tin thực tế được hỗ trợ. Như được nêu trong Phụ lục D.1, tuy nhiên, chúng tôi lập luận rằng việc lặp lại thông tin thực tế có thể được đo tốt hơn bằng các chỉ số khác như tính trôi chảy hoặc tính hữu ích của các phản hồi mô hình. Nếu mong muốn, chúng tôi cũng đề xuất rằng một bước loại bỏ các thông tin thực tế trùng lặp có thể được thêm vào SAFE, mặc dù chúng tôi để điều này cho nghiên cứu tương lai do độ phức tạp bổ sung cần thiết để tính đến trường hợp này.

9 Kết luận

Trong bài báo này, chúng tôi đã kiểm tra cách đánh giá toàn diện tính chính xác thông tin dài trong các mô hình ngôn ngữ lớn. Để làm điều này, đầu tiên chúng tôi sử dụng GPT-4 để tạo ra LongFact, một bộ 2,280 câu hỏi trải rộng trên 38 chủ đề khác nhau. Sau đó chúng tôi đề xuất sử dụng các agent mô hình ngôn ngữ để tự động đánh giá tính chính xác thông tin dài của phản hồi mô hình. Phương pháp này, mà chúng tôi gọi là SAFE, sử dụng một mô hình ngôn ngữ lớn được hỗ trợ tìm kiếm để chia một phản hồi dài thành các thông tin riêng lẻ, chỉnh sửa các thông tin riêng lẻ để tự đứng vững, xác định tính liên quan của mỗi thông tin riêng lẻ đối với việc trả lời câu hỏi, và kiểm tra tính chính xác thông tin của mỗi thông tin liên quan bằng cách phát hành các truy vấn Google Search. Hơn nữa, chúng tôi mở rộng recall vào lĩnh vực dài bằng cách sử dụng một siêu tham số Knhư một đại diện cho độ dài ưa thích của con người đối với các phản hồi, và chúng tôi kết hợp đo lường recall này với precision trong F1@K.

Về mặt thực nghiệm, chúng tôi chứng minh rằng SAFE vượt trội hơn các chú thích viên con người từ crowdsourcing bằng cách đồng ý với 72% chú thích của con người và thắng 76% ví dụ từ một tập hợp 100 trường hợp bất đồng được lấy mẫu ngẫu nhiên. Chúng tôi cũng cho thấy rằng SAFE rẻ hơn hơn 20 ×so với các chú thích viên con người từ crowdsourcing. Hơn nữa, chúng tôi đánh giá mười ba mô hình từ bốn họ mô hình (Gemini, GPT, Claude, PaLM-2) trên LongFact và phát hiện rằng các mô hình ngôn ngữ lớn hơn thường có tính chính xác thông tin dài tốt hơn. Chúng tôi công khai tất cả mã của chúng tôi tại https://github.com/google-deepmind/long-form-factuality .

Nghiên cứu tương lai trong lĩnh vực này có thể khám phá một loạt hướng rộng. Đầu tiên, một con đường khám phá chính là cách cải thiện tính chính xác thông tin dài của mô hình ngôn ngữ thông qua tiền huấn luyện/tinh chỉnh tốt hơn hoặc bằng cách tăng cường chúng với việc sử dụng công cụ bên ngoài. Cũng tồn tại các lĩnh vực cải thiện cho SAFE về mặt phụ thuộc vào các agent mô hình ngôn ngữ được hỗ trợ tìm kiếm. Hơn nữa, công trình của chúng tôi xem xét tính chính xác thông tin (tức là, tính đúng đắn của thông tin thực tế đối với kiến thức thế giới), và vì vậy vẫn không rõ cách đo lường đáng tin cậy ảo giác (tức là, tính đúng đắn của thông tin thực tế đối với kiến thức nội bộ của mô hình) trong bối cảnh dài.

Thông qua benchmark của chúng tôi, chúng tôi nhằm chứng minh cách các phương pháp đáng tin cậy thu được bộ dữ liệu, đánh giá mô hình, và tổng hợp chỉ số có thể cải thiện đáng kể hiểu biết của chúng ta về khả năng mô hình trong bối cảnh dài. Chúng tôi hy vọng rằng công trình của chúng tôi khuyến khích nghiên cứu thêm vào cả việc đo lường và cải thiện các mô hình ngôn ngữ trong các lĩnh vực dài.

Lời cảm ơn
Chúng tôi cảm ơn Chen Liang vì các đề xuất cải thiện LongFact. Chúng tôi cảm ơn Balaji Lakshminarayanan, Jeremiah Liu, Clara Huiyi Hu, Kelvin Guu, Ice Pasupat, Zhuyun Dai, Harshal Godhia, Vered Cohen, Eran Ofek, Chun-Sung Ferng, Da-Cheng Juan, Hao Zhou, Chenjie Gu, Qijun Tan, Jasper Snoek, Daniel Balle, Petru Gurita, Zhichun Wu, Norbert Kalb, Yanyan Zheng, Qiuchen Guo, John Nham, và Fangtao Li vì thảo luận hữu ích về các người đánh giá tính chính xác thông tin tự động. Chúng tôi cảm ơn Albert Webson, Adams Yu, Chen Zhu, Heng-Tze Cheng, Chenkai Kuang, YaGuang Li, và Xinyun Chen vì phản hồi sâu sắc về những ý tưởng ban đầu của chúng tôi. Chúng tôi cảm ơn Denny Zhou, Slav Petrov, Le Hou, và Andrew Dai vì cung cấp phản hồi về bài báo. Chúng tôi cũng cảm ơn một số người đánh giá ẩn danh vì những bình luận hữu ích.

Đóng góp của tác giả
Chengrun Yang và Jerry Wei đề xuất đóng khung bài báo trong bối cảnh dài. Jerry Wei tạo nguyên mẫu sử dụng một LLM để tự động tạo ra câu hỏi, và Chengrun Yang đề xuất và tạo nguyên mẫu tạo ra các câu hỏi cụ thể theo chủ đề cho LongFact. Xinying Song, Jie Huang, Cosmo Du, và Chengrun Yang nghĩ ra ý tưởng phát triển một người đánh giá tự động LLM được tăng cường bằng tìm kiếm, cái mà được tạo nguyên mẫu bởi Chengrun Yang, Jerry Wei, Yifeng Lu, Xinying Song, Jie Huang, và Cosmo Du. Yifeng Lu tạo nguyên mẫu một cách tiếp cận agent nhiều bước sử dụng LLM. Jerry Wei và Xinying Song triển khai hệ thống hiện tại của SAFE. Jerry Wei đề xuất tên cho LongFact và SAFE, và Nathan Hu đề xuất tên cho F1@K. Nathan Hu đề xuất kết hợp độ dài mong đợi của con người vào recall cho F1@K.

Jerry Wei dẫn đầu triển khai và thực nghiệm dẫn đến codebase hiện tại. Chengrun Yang, Yifeng Lu, Xinying Song, Nathan Hu, và Quoc V . Le giúp phát triển ý tưởng cho thực nghiệm. Chengrun Yang đóng góp vào codebase và thực nghiệm và kiểm tra dữ liệu thực nghiệm, cái mà ảnh hưởng đến hướng kỹ thuật. Jerry Wei dẫn đầu các nỗ lực mã nguồn mở với sự giúp đỡ từ Da Huang, Dustin Tran, và Chengrun Yang. Jerry Wei, Chengrun Yang, và Nathan Hu dẫn đầu việc viết bài báo. Quoc V . Le và Yifeng Lu tư vấn dự án. Quoc V . Le đề xuất hướng làm việc về tính chính xác thông tin và ảo giác. Tất cả các tác giả khác giúp đóng góp phản hồi về đóng khung bài báo.

--- TRANG 11 ---
Tài liệu tham khảo

Anthropic. Claude 2, 2023. URL https://www.anthropic.com/news/claude-2 .

Anthropic. Introducing the next generation of Claude, 2024. URL https://www.anthropic.com/news/claude-3-family .

Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, và Giovanni Zagni. Factuality challenges in the era of large language models, 2023. URL https://arxiv.org/abs/2310.05189 .

[Phần còn lại của tài liệu tham khảo tiếp tục với cùng định dạng...]
