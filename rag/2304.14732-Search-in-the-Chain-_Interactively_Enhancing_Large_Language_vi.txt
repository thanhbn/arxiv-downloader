# 2304.14732.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2304.14732.pdf
# Kích thước tệp: 2923600 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Search-in-the-Chain: Tăng cường tương tác cho các Mô hình Ngôn ngữ Lớn với Tìm kiếm cho các Tác vụ Chuyên sâu về Kiến thức
Shicheng Xu
Phòng thí nghiệm chính CAS về An ninh AI,
Viện Công nghệ Tính toán,
Viện Hàn lâm Khoa học Trung Quốc
Đại học Viện Hàn lâm Khoa học
Trung Quốc
Bắc Kinh, Trung Quốc
xushicheng21s@ict.ac.cnLiang Pang∗
Phòng thí nghiệm chính CAS về An ninh AI,
Viện Công nghệ Tính toán,
Viện Hàn lâm Khoa học Trung Quốc
Bắc Kinh, Trung Quốc
pangliang@ict.ac.cnHuawei Shen
Phòng thí nghiệm chính CAS về An ninh AI,
Viện Công nghệ Tính toán,
Viện Hàn lâm Khoa học Trung Quốc
Bắc Kinh, Trung Quốc
shenhuawei@ict.ac.cn
Xueqi Cheng
Phòng thí nghiệm chính CAS về An ninh AI,
Viện Công nghệ Tính toán,
Viện Hàn lâm Khoa học Trung Quốc
Bắc Kinh, Trung Quốc
cxq@ict.ac.cnTat-Seng Chua
Phòng thí nghiệm liên kết Sea-NExT, Đại học Quốc gia
Singapore
Singapore
dcscts@nus.edu.sg

TÓM TẮT
Việc tạo ra nội dung được sinh bởi Mô hình Ngôn ngữ Lớn (LLM) một cách chính xác, đáng tin cậy và có thể truy vết là rất quan trọng, đặc biệt trong các tác vụ phức tạp chuyên sâu về kiến thức đòi hỏi lập luận nhiều bước và mỗi bước cần kiến thức để giải quyết. Sinh nội dung tăng cường truy xuất có tiềm năng tốt để giải quyết vấn đề này. Tuy nhiên, ở đâu và làm thế nào để giới thiệu Truy xuất Thông tin (IR) vào LLM là một thách thức lớn. Các nghiên cứu trước có vấn đề là kiến thức sai được truy xuất bởi IR làm cho LLM hiểu nhầm và tương tác giữa IR và LLM phá vỡ chuỗi lập luận của LLM. Bài báo này đề xuất một khung mới có tên Search-in-the-Chain (SearChain) cho tương tác giữa LLM và IR để giải quyết các thách thức. Đầu tiên, LLM tạo ra chuỗi lập luận có tên Chain-of-Query (CoQ) trong đó mỗi nút bao gồm một cặp câu hỏi-câu trả lời hướng IR. Thứ hai, IR xác minh câu trả lời của mỗi nút của CoQ. Nó sửa câu trả lời không nhất quán với thông tin được truy xuất khi IR đưa ra độ tin cậy cao, điều này cải thiện độ tin cậy. Thứ ba, LLM có thể chỉ ra kiến thức còn thiếu trong CoQ và dựa vào IR để cung cấp kiến thức này cho LLM. Những thao tác này cải thiện độ chính xác về mặt lập luận và kiến thức. Cuối cùng, SearChain tạo ra quá trình lập luận và đánh dấu tham chiếu đến các tài liệu hỗ trợ cho mỗi bước lập luận, điều này cải thiện khả năng truy vết. Tương tác với IR trong SearChain tạo thành một đường lập luận mới dựa trên cây, cho phép LLM linh hoạt sửa đổi hướng lập luận. Các thí nghiệm cho thấy SearChain vượt trội hơn các baseline tiên tiến trên các tác vụ phức tạp chuyên sâu về kiến thức bao gồm hỏi đáp nhiều bước, điền slot, kiểm tra sự thật và hỏi đáp dạng dài.
∗Tác giả liên hệ
Tác phẩm này được cấp phép theo Giấy phép Creative Commons Attribution
International 4.0.
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore
©2024 Bản quyền thuộc về chủ sở hữu/tác giả.
ACM ISBN 979-8-4007-0171-9/24/05.
https://doi.org/10.1145/3589334.3645363CÁC KHÁI NIỆM CCS
•Phương pháp tính toán →Xử lý ngôn ngữ tự nhiên .
TỪ KHÓA
Mô hình tăng cường truy xuất, Mô hình Ngôn ngữ Lớn
Định dạng Tham chiếu ACM:
Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua.
2024. Search-in-the-Chain: Tăng cường tương tác cho các Mô hình Ngôn ngữ Lớn
với Tìm kiếm cho các Tác vụ Chuyên sâu về Kiến thức. Trong Proceedings of the ACM Web
Conference 2024 (WWW '24), 13–17 tháng 5, 2024, Singapore, Singapore. ACM,
New York, NY, USA, 13 trang. https://doi.org/10.1145/3589334.3645363

1 GIỚI THIỆU
Các Mô hình Ngôn ngữ Lớn (LLM) như ChatGPT đã cho thấy hiệu suất đầy hứa hẹn trong nhiều tác vụ xử lý ngôn ngữ tự nhiên khác nhau [ 2,47]. Tuy nhiên, đối với các tác vụ phức tạp chuyên sâu về kiến thức đòi hỏi lập luận nhiều bước và mỗi bước cần kiến thức để giải quyết [ 23,43,49], nhiều nghiên cứu đã cho thấy LLM gặp khó khăn trong: (1) lập luận tổng hợp trên nhiều kiến thức [ 24], (2) ghi nhớ kiến thức đuôi dài và thời gian thực [ 13] và (3) tránh ảo giác không nhất quán với sự thật [ 1], điều này ảnh hưởng đến độ chính xác và độ tin cậy của LLM cho các tác vụ phức tạp chuyên sâu về kiến thức. Bên cạnh đó, việc sinh nội dung chỉ dựa trên ngữ cảnh mà không có bất kỳ bằng chứng hỗ trợ nào gây ra ít khả năng truy vết và khiến mọi người ít tin tưởng vào nội dung được tạo bởi LLM. Phương pháp tăng cường truy xuất có tiềm năng tốt để giải quyết những vấn đề này vì nó kết hợp kiến thức của mô hình với cơ sở kiến thức bên ngoài [ 10,12,18].
Tuy nhiên, ở đâu và làm thế nào để giới thiệu IR vào LLM không phải là việc tầm thường. Có ba thách thức chính. C-1: Việc chèn trực tiếp IR vào quá trình lập luận của LLM như Self-Ask [ 24], LTM [ 48], React [ 42] và DSP [ 15] dẫn đến việc phá vỡ chuỗi lập luận của LLM. Vì trong những phương pháp này, LLM chỉ có thể lập luận một câu hỏi con cục bộ trong mỗi lần sinh. C-2: Khi có xung đột trong kiến thức của IR và LLM, đối với kiến thức mà LLM đã ghi nhớ chính xác, nó có nguy cơ bị IR hiểu nhầm nếu IR truy xuất thông tin sai. Điều quan trọng là đảm bảo rằng IR chỉ cung cấp kiến thức mà LLM thực sự cần. C-3: Các phương pháp trước không thể linh hoạt sửa đổi hướng lập luận.
Trong bài báo này, chúng tôi đề xuất một khung mới có tên Search-in-the-Chain (SearChain) để kết hợp hiệu quả LLM với IR nhằm giải quyết các thách thức trên (Hình 1). SearChain và các phương pháp trước đều cần nhiều vòng tương tác IR-LLM, nhưng phương pháp trước hoạt động ở cấp độ chuỗi, trong khi phương pháp sau chỉ xử lý một nút. Trong mỗi vòng, SearChain thực hiện lập luận, xác minh và hoàn thiện. Sau tương tác, SearChain thực hiện truy vết để tạo nội dung cuối cùng. Cụ thể, trong mỗi vòng, đầu tiên, LLM khai thác học trong ngữ cảnh để xây dựng một Chain-of-Query (CoQ), đây là một chuỗi lập luận để phân tách và giải quyết các câu hỏi phức tạp. Mỗi nút của chuỗi bao gồm một truy vấn hướng IR, câu trả lời được tạo bởi LLM cho truy vấn này, và một cờ chỉ ra liệu LLM có cần kiến thức bổ sung hay không. Khác với các phương pháp trước trong đó LLM chỉ có thể thực hiện lập luận một bước (chỉ một nút) khi tương tác với IR, CoQ là một chuỗi hoàn chỉnh. Thiết kế này tránh IR phá vỡ chuỗi lập luận ( C-1). Thứ hai, IR tương tác với mỗi nút của CoQ để thực hiện xác minh và hoàn thiện. Trong xác minh, IR xác minh câu trả lời của mỗi nút. Trong trường hợp câu trả lời được tạo bởi LLM không nhất quán với thông tin được truy xuất và IR đưa ra độ tin cậy cao, IR đưa ra phản hồi cho LLM để giúp nó sửa câu trả lời và tái tạo CoQ đúng. Trong hoàn thiện, IR xác định liệu nút có thiếu kiến thức từ cờ của nút hay không và cung cấp kiến thức này cho LLM để giúp nó tái tạo CoQ. LLM dần dần tạo ra CoQ đúng thông qua nhiều vòng tương tác với IR. Thiết kế trên cung cấp cho LLM kiến thức mà nó thực sự cần để giảm thiểu việc hiểu nhầm do IR gây ra cho LLM ( C-2), điều này cải thiện độ chính xác. IR xác minh và sửa kiến thức trong quá trình lập luận của LLM dựa trên cơ sở kiến thức bên ngoài, điều này cải thiện độ tin cậy. Sau tương tác, SearChain thực hiện truy vết để tạo ra quá trình lập luận và đánh dấu tham chiếu đến các tài liệu hỗ trợ cho mỗi bước lập luận, được sử dụng làm nội dung cuối cùng trả về cho người dùng. Điều này cải thiện khả năng truy vết kiến thức trong nội dung được tạo. Tương tác với IR trong SearChain biến đổi đường lập luận từ chuỗi thành tìm kiếm theo chiều sâu nhận diện nút trên cây gọi là Tree-of-Reasoning (ToR). Sinh CoQ có thể được xem như một phần của Tìm kiếm theo chiều sâu và IR có thể nhận diện các nút cần thêm thông tin ( C-3). Điều này cho phép LLM linh hoạt sửa đổi hướng lập luận. Những đóng góp chính của bài báo này là:
(1)Chúng tôi nổi bật các thách thức trong việc giới thiệu IR vào LLM từ góc độ lập luận và kiến thức.
(2)SearChain không chỉ cải thiện khả năng lập luận-kiến thức của LLM mà còn sử dụng IR để nhận diện và đưa ra kiến thức mà LLM thực sự cần. Bên cạnh đó, SearChain có thể đánh dấu tham chiếu đến các tài liệu hỗ trợ cho kiến thức liên quan trong nội dung được tạo.
(3)Tương tác với IR trong SearChain tạo thành một đường lập luận mới: tìm kiếm theo chiều sâu nhận diện nút trên cây, cho phép LLM linh hoạt sửa đổi hướng lập luận.
(4)Thí nghiệm cho thấy SearChain vượt trội hơn các baseline tiên tiến trên các tác vụ phức tạp chuyên sâu về kiến thức bao gồm hỏi đáp nhiều bước, điền slot, kiểm tra sự thật và hỏi đáp dạng dài. Mã được phát hành tại https://github.com/xsc1234/Search-in-the-Chain.

2 NGHIÊN CỨU LIÊN QUAN
2.1 Nhắc Chain-of-Thought
Chain-of-thought [ 36] đề xuất phương pháp sử dụng các ví dụ few-shot để cho phép LLM đưa ra kết quả lập luận trung gian để cải thiện khả năng lập luận. [ 16] sử dụng "Let's do it step by step" làm prompt để đạt được hiệu suất zero-shot đầy hứa hẹn. Auto-CoT khai thác các mô hình ngôn ngữ để tự động xây dựng các ví dụ học few-shot cho CoT [ 45]. Cũng có nhiều nghiên cứu bao gồm các khía cạnh khác của CoT như tự nhất quán [ 35], sử dụng các mô hình kích thước nhỏ và

--- TRANG 2 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

Q
A
B
C
D
Q
    Tree-of-Reasoning (Trace.)Nội dung Cuối cùng
với Tham chiếuA
B
C
DE
F H
I
J
KL
M
NGQ
A
B
C
DIRA
B
C
DQ
E
F
GLLM
LLMA
B
C
DQ
E
F
GLLMA
B
C
DQ
E
F
G H
I
J
KLLM
LLM
L
M
N
Vòng 4 (Sinh CoQ) Vòng 3 (Xác minh và Hoàn thiện)Vòng 3 (Sinh CoQ)Vòng 2 (Sinh CoQ) Vòng 2 (Xác minh) Vòng 1 (Xác minh) Vòng 1 (Sinh CoQ)IR IR
IRLLM
A
B
C
DQ
E
F
G H
I
J
KLLM
IR
IRA
B
C
DQ
E
F
GH
I
J
KLLM
L
M
NA
B
C
DQ
E
F
GH
I
J
KIR
IR
IRIRIR cung cấp 
tài liệu 
và câu trả lời
IR cung cấp 
tài liệu 
và câu trả lời
IR
Vòng 4 (Xác minh)IR cung cấp 
tài liệu 
và câu trả lờiNgười biểu diễn Spirit If... là Kevin Drew [1]. Kevin 
Drew sinh ra ở Toronto [2]. Xe buýt Greyhound ở 
Toronto khởi hành từ Toronto Coach Terminal [3]. Vậy 
câu trả lời cuối cùng là Toronto Coach Terminal.
[1] Spirit If... là album solo đầu tay của Kevin Drew. Nó 
được phát hành vào ngày 18 tháng 9, 2007 …
[2] Kevin Drew (sinh ngày 9 tháng 9, 1976 tại Toronto) ..
[3] Toronto Coach Terminal là trạm xe buýt trung tâm 
cho các dịch vụ liên thành phố ở Toronto, Ontario, 
Canada … khi nó được cho thuê toàn bộ cho các 
tuyến xe buýt Coach Canada và Greyhound Canada …Xe buýt greyhound ở nơi sinh của người 
biểu diễn Spirit If... khởi hành từ đâu? : Hoàn thiện (cần kiến thức bổ sung): Xác minh (cần được sửa): Xác minh (không cần được sửa)
Q CoQ 1
CoQ 2
CoQ 3
CoQ 4

Hình 1: Tương tác giữa IR và LLM trong SearChain. Đầu tiên, SearChain làm cho LLM lập kế hoạch một CoQ trong đó mỗi nút là một cặp câu hỏi-câu trả lời. Sau đó, IR tương tác với mỗi nút của CoQ để thực hiện xác minh và hoàn thiện. Nếu IR phát hiện một nút cần được sửa hoặc cung cấp kiến thức, nó đưa ra phản hồi cho LLM và LLM tái tạo một CoQ mới, đây là nhánh mới của cây. Quá trình này là tìm kiếm theo chiều sâu nhận diện nút trên cây gọi là Tree-of-Reasoning (đường lập luận đúng màu xanh). Nội dung cuối cùng bao gồm quá trình lập luận và tham chiếu đến các tài liệu hỗ trợ.

thông tin sai. Điều quan trọng là đảm bảo rằng IR chỉ cung cấp kiến thức mà LLM thực sự cần. C-3: Các phương pháp trước không thể linh hoạt sửa đổi hướng lập luận.

Trong bài báo này, chúng tôi đề xuất một khung mới có tên Search-in-the-Chain (SearChain) để kết hợp hiệu quả LLM với IR nhằm giải quyết các thách thức trên (Hình 1). SearChain và các phương pháp trước đều cần nhiều vòng tương tác IR-LLM, nhưng phương pháp trước hoạt động ở cấp độ chuỗi, trong khi phương pháp sau chỉ xử lý một nút. Trong mỗi vòng, SearChain thực hiện lập luận, xác minh và hoàn thiện. Sau tương tác, SearChain thực hiện truy vết để tạo nội dung cuối cùng. Cụ thể, trong mỗi vòng, đầu tiên, LLM khai thác học trong ngữ cảnh để xây dựng một Chain-of-Query (CoQ), đây là một chuỗi lập luận để phân tách và giải quyết các câu hỏi phức tạp. Mỗi nút của chuỗi bao gồm một truy vấn hướng IR, câu trả lời được tạo bởi LLM cho truy vấn này, và một cờ chỉ ra liệu LLM có cần kiến thức bổ sung hay không. Khác với các phương pháp trước trong đó LLM chỉ có thể thực hiện lập luận một bước (chỉ một nút) khi tương tác với IR, CoQ là một chuỗi hoàn chỉnh. Thiết kế này tránh IR phá vỡ chuỗi lập luận ( C-1). Thứ hai, IR tương tác với mỗi nút của CoQ để thực hiện xác minh và hoàn thiện. Trong xác minh, IR xác minh câu trả lời của mỗi nút. Trong trường hợp câu trả lời được tạo bởi LLM không nhất quán với thông tin được truy xuất và IR đưa ra độ tin cậy cao, IR đưa ra phản hồi cho LLM để giúp nó sửa câu trả lời và tái tạo CoQ đúng. Trong hoàn thiện, IR xác định liệu nút có thiếu kiến thức từ cờ của nút hay không và cung cấp kiến thức này cho LLM để giúp nó tái tạo CoQ. LLM dần dần tạo ra CoQ đúng thông qua nhiều vòng tương tác với IR. Thiết kế trên cung cấp cho LLM kiến thức mà nó thực sự cần để giảm thiểu việc hiểu nhầm do IR gây ra cho LLM ( C-2), điều này cải thiện độ chính xác. IR xác minh và sửa kiến thức trong quá trình lập luận của LLM dựa trên cơ sở kiến thức bên ngoài, điều này cải thiện độ tin cậy. Sau tương tác, SearChain thực hiện truy vết để tạo ra quá trình lập luận và đánh dấu tham chiếu đến các tài liệu hỗ trợ cho mỗi bước lập luận, được sử dụng làm nội dung cuối cùng trả về cho người dùng. Điều này cải thiện khả năng truy vết kiến thức trong nội dung được tạo. Tương tác với IR trong SearChain biến đổi đường lập luận từ chuỗi thành tìm kiếm theo chiều sâu nhận diện nút trên cây gọi là Tree-of-Reasoning (ToR). Sinh CoQ có thể được xem như một phần của Tìm kiếm theo chiều sâu và IR có thể nhận diện các nút cần thêm thông tin ( C-3). Điều này cho phép LLM linh hoạt sửa đổi hướng lập luận. Những đóng góp chính của bài báo này là:

(1)Chúng tôi nổi bật các thách thức trong việc giới thiệu IR vào LLM từ góc độ lập luận và kiến thức.
(2)SearChain không chỉ cải thiện khả năng lập luận-kiến thức của LLM mà còn sử dụng IR để nhận diện và đưa ra kiến thức mà LLM thực sự cần. Bên cạnh đó, SearChain có thể đánh dấu tham chiếu đến các tài liệu hỗ trợ cho kiến thức liên quan trong nội dung được tạo.
(3)Tương tác với IR trong SearChain tạo thành một đường lập luận mới: tìm kiếm theo chiều sâu nhận diện nút trên cây, cho phép LLM linh hoạt sửa đổi hướng lập luận.
(4)Thí nghiệm cho thấy SearChain vượt trội hơn các baseline tiên tiến trên các tác vụ phức tạp chuyên sâu về kiến thức bao gồm hỏi đáp nhiều bước, điền slot, kiểm tra sự thật và hỏi đáp dạng dài. Mã được phát hành tại https://github.com/xsc1234/Search-in-the-Chain.

2 NGHIÊN CỨU LIÊN QUAN
2.1 Nhắc Chain-of-Thought 
Chain-of-thought [ 36] đề xuất phương pháp sử dụng các ví dụ few-shot để cho phép LLM đưa ra kết quả lập luận trung gian để cải thiện khả năng lập luận. [ 16] sử dụng "Let's do it step by step" làm prompt để đạt được hiệu suất zero-shot đầy hứa hẹn. Auto-CoT khai thác các mô hình ngôn ngữ để tự động xây dựng các ví dụ học few-shot cho CoT [ 45]. Cũng có nhiều nghiên cứu bao gồm các khía cạnh khác của CoT như tự nhất quán [ 35], sử dụng các mô hình kích thước nhỏ và

--- TRANG 3 ---
Search-in-the-Chain: Tăng cường tương tác cho các Mô hình Ngôn ngữ Lớn với Tìm kiếm cho các Tác vụ Chuyên sâu về Kiến thức WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore

trung bình [ 44] và lựa chọn [ 8]. Bên cạnh đó, có các nghiên cứu sử dụng LLM lặp đi lặp lại để phân tách các câu hỏi phức tạp và trả lời các câu hỏi con từng bước. Những phương pháp này bao gồm Least-to-Most [ 48], Dynamic Least-to-Most [ 5], Self-Ask [ 24] và DSP [ 15].
Chain-of-Query của phương pháp chúng tôi cũng được lấy cảm hứng từ CoT. Tuy nhiên, các nghiên cứu trước tập trung vào việc đưa ra kết quả lập luận trung gian hoặc phân tách các câu hỏi phức tạp và trả lời các câu hỏi con từng bước. Chúng tập trung vào cách giải quyết các câu hỏi con cục bộ trong khi bỏ qua việc lập kế hoạch toàn cục của chuỗi lập luận. Mặc dù AgentGPT và PS [ 34] đầu tiên lập kế hoạch cho mỗi câu hỏi con và sau đó giải quyết chúng, chúng không phù hợp cho các tình huống mà câu hỏi con tiếp theo cần câu trả lời của các câu hỏi con trước để tạo ra, điều này phổ biến đối với các tác vụ phức tạp chuyên sâu về kiến thức (multi-hop QA). CoQ của phương pháp chúng tôi làm cho LLM xây dựng một chuỗi lập luận toàn cục trong đó mỗi nút là một cặp câu hỏi-câu trả lời. Thiết kế này không chỉ cải thiện khả năng lập luận-kiến thức mà còn cung cấp giao diện cho IR để tham gia sâu vào quá trình lập luận của LLM.

2.2 Mô hình Ngôn ngữ Tăng cường Truy xuất
Nhiều nghiên cứu đã cho thấy rằng các phương pháp tăng cường truy xuất đạt được hiệu suất đầy hứa hẹn trong nhiều tác vụ ngôn ngữ tự nhiên khác nhau như hỏi đáp miền mở [ 4,10,12,18,21,37,39], mô hình hóa ngôn ngữ [ 3,20,22] và tăng cường tính thực tế [ 25]. Gần đây, một số nghiên cứu cho phép LLM tương tác với IR thông qua học trong ngữ cảnh [15,24,28,42]. Trong những phương pháp này, tương tác giữa IR và LLM làm cho việc lập luận của LLM không liên tục. LLM chỉ có thể thực hiện lập luận một bước tại mỗi lần suy luận. Phương pháp của chúng tôi làm cho LLM tạo ra một chuỗi lập luận toàn cục gọi là Chain-of-Query tại mỗi lần suy luận, điều này giới thiệu mối quan hệ logic mạnh mẽ hơn giữa mỗi bước lập luận. Bên cạnh đó, các phương pháp trước chỉ có thể cung cấp thông tin cho LLM nhưng không thể hỗ trợ LLM trong việc sửa thông tin sai hoặc tránh tác động tiêu cực của IR lên LLM, điều này làm cho việc lập luận của LLM vẫn ở trong một chuỗi một chiều. Phương pháp của chúng tôi làm cho IR tương tác với mỗi nút của chuỗi. IR chỉ cung cấp cho LLM kiến thức còn thiếu và sửa các câu trả lời không nhất quán với thông tin được truy xuất khi IR đủ tin cậy. Điều này giảm thiểu tác động tiêu cực của IR lên LLM và biến đổi đường lập luận từ chuỗi thành Tìm kiếm theo chiều sâu nhận diện nút trên cây để cho phép LLM linh hoạt sửa đổi hướng lập luận.

3 PHƯƠNG PHÁP CỦA CHÚNG TÔI
Phần này giới thiệu thiết kế của Search-in-the-Chain (SearChain). Trong SearChain, IR và LLM tiến hành nhiều vòng tương tác. Trong mỗi vòng, đầu tiên, LLM đóng vai trò là chỉ huy để lập kế hoạch chuỗi lập luận toàn cục cho các câu hỏi đầu vào phức tạp gọi là Chain-of-Query (CoQ). Mỗi nút của CoQ bao gồm một truy vấn hướng IR, câu trả lời cho truy vấn này, và một cờ chỉ ra liệu LLM có cần kiến thức bổ sung hay không. Sau đó, IR tương tác với mỗi nút của CoQ và thực hiện hoàn thiện và xác minh để chỉ cung cấp cho LLM kiến thức còn thiếu và sửa các câu trả lời sai để giảm thiểu hiểu nhầm. LLM tái tạo CoQ mới dựa trên phản hồi từ IR. Nhiều vòng tương tác giúp LLM dần dần tạo ra CoQ đúng, điều này cải thiện độ chính xác và độ tin cậy. Cuối cùng, SearChain thực hiện truy vết để tạo ra toàn bộ quá trình lập luận và đánh dấu tham chiếu đến các tài liệu hỗ trợ cho mỗi

𝑄𝑄A LLMIR
B LLMIR
LLMIR
A
B
C
D
 E
F
GH
I
J
L
M
N
KNội dung Cuối cùng
với Tham chiếu... Câu trả lời CuốiTrước:
Của chúng tôi: 𝑄𝑄nút cục bộ nút cục bộ

Hình 2: So sánh với các phương pháp trước.

bước lập luận, được sử dụng làm nội dung cuối cùng trả về cho người dùng. Điều này cải thiện khả năng truy vết của nội dung được tạo. Tương tác với IR trong SearChain biến đổi đường lập luận từ chuỗi thành tìm kiếm theo chiều sâu nhận diện nút trên cây gọi là Tree-of-Reasoning (ToR), cho phép LLM linh hoạt sửa đổi hướng lập luận.

3.1 So sánh với Các phương pháp Trước
Hình 2 cho thấy sự khác biệt giữa phương pháp của chúng tôi và các phương pháp tăng cường truy xuất trước (Self-Ask [ 24], React [ 42], DSP [ 15], v.v.) trong việc giải quyết các câu hỏi phức tạp chuyên sâu về kiến thức.

(1) Cục bộ vs. Toàn cục. Đối với một câu hỏi phức tạp cần lập luận nhiều bước, các phương pháp trước chèn trực tiếp IR vào quá trình lập luận nhiều bước, khiến LLM chỉ có thể lập luận một câu hỏi con cục bộ như nút A○trong mỗi lần tạo. Điều này phá vỡ chuỗi lập luận của LLM. Phương pháp của chúng tôi đề xuất Chain-of-Query để cung cấp giao diện tương tác cho IR trên tiền đề đảm bảo tính nhất quán của chuỗi lập luận (lập kế hoạch một chuỗi toàn cục cho câu hỏi 𝑄 như A○→B○→C○→D○trong mỗi lần tạo). (giải quyết C-1)

(2) Cung cấp Trực tiếp vs. Xác minh và Hoàn thiện. Các phương pháp trước cung cấp trực tiếp thông tin được truy xuất cho LLM. Khi thông tin được truy xuất sai, LLM có nguy cơ bị hiểu nhầm. Trong phương pháp của chúng tôi, IR chỉ sửa thông tin không nhất quán trong Chain-of-Query khi IR đủ tin cậy, và cung cấp thông tin mà LLM không biết thông qua các cờ trên Chain-of-Query, điều này giảm thiểu tác động tiêu cực của IR lên LLM. (giải quyết C-2)

(3) Chuỗi vs. Cây. Các phương pháp trước không thể sửa đổi hướng lập luận kịp thời khi cần thiết. Phương pháp của chúng tôi biến đổi đường lập luận từ chuỗi thành tìm kiếm theo chiều sâu nhận diện nút trên cây bằng cách giới thiệu xác minh và hoàn thiện từ IR, cho phép LLM linh hoạt sửa đổi hướng lập luận. (giải quyết C-3)

--- TRANG 4 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

Xây dựng một chuỗi lập luận toàn cục cho [Câu hỏi] phức tạp này: Xe buýt 
Greyhound ở nơi sinh của người biểu diễn Spirit If... khởi hành từ đâu? 
Bạn nên tạo ra một truy vấn cho công cụ tìm kiếm dựa trên những gì bạn đã 
biết ở mỗi bước của chuỗi lập luận, bắt đầu với [Query]. 
Nếu bạn biết câu trả lời cho [Query], hãy tạo ra nó bắt đầu với [Answer].
Bạn có thể thử tạo ra câu trả lời cuối cùng cho [Câu hỏi] bằng cách tham khảo 
các cặp [Query]-[Answer], bắt đầu với [Final Content].
Nếu bạn không biết câu trả lời, hãy tạo ra một truy vấn cho công cụ tìm kiếm dựa trên những gì 
bạn đã biết và không biết, bắt đầu với [Unsolved Query].
Ví dụ:
[Câu hỏi]: Xe buýt Greyhound ở nơi sinh của người biểu diễn Spirit If... khởi hành từ đâu? 
[Query 1]: Ai là người biểu diễn Spirit If... ?
Nếu bạn không biết câu trả lời:
[Unsolved Query]: Ai là người biểu diễn Spirit If... ?
Nếu bạn biết câu trả lời:
[Answer 1]: Người biểu diễn Spirit If… là Kevin Drew.
[Query 2]: Kevin Drew sinh ra ở đâu?
Nếu bạn không biết câu trả lời:
[Unsolved Query]: Kevin Drew sinh ra ở đâu?
Nếu bạn biết câu trả lời:
[Answer 2]: Toronto.
[Query 3]: Xe buýt greyhound ở Toronto khởi hành từ đâu?
Nếu bạn không biết câu trả lời:
[Unsolved Query]: Xe buýt greyhound ở Toronto khởi hành từ đâu?
Nếu bạn biết câu trả lời:
[Answer 3]: Toronto Coach Terminal.
[Final Content]: Người biểu diễn Spirit If… là Kevin Drew [1]. Kevin Drew 
sinh ra ở Toronto [2]. Xe buýt Greyhound ở Toronto khởi hành từ Toronto 
Coach Terminal [3]. Vậy câu trả lời cuối cùng là Toronto Coach Terminal.Nút 1
Nút 2
Nút 3

Hình 3: Prompt để làm cho LLM tạo ra Chain-of-Query.

3.2 Sinh Chain-of-Query
Trong SearChain, chúng tôi sử dụng học trong ngữ cảnh [ 36] để nhắc mô hình ngôn ngữ lớn xây dựng một chuỗi lập luận toàn cục cho câu hỏi phức tạp 𝑄 có tên Chain-of-Query (CoQ):
CoQ=(𝑞1,𝑎1)→(𝑞2,𝑎2)→...→(𝑞𝑛,𝑎𝑛), (1)
đây là nhánh của Tree-of-Reasoning. Mỗi nút (𝑞𝑖,𝑎𝑖) của CoQ bao gồm một truy vấn hướng IR 𝑞𝑖 và câu trả lời của nó 𝑎𝑖. 𝑞1... 𝑞𝑛 là các câu hỏi con cần được giải quyết trong quá trình lập luận để giải quyết 𝑄. Sinh CoQ được áp dụng cho mỗi vòng tương tác giữa LLM và IR. Trong vòng đầu tiên, prompt được sử dụng để làm cho LLM tạo ra CoQ được hiển thị trong Hình 3. Prompt với "Xây dựng một chuỗi lập luận toàn cục" làm cho LLM biết rằng nhiệm vụ chính là tạo ra một chuỗi lập luận toàn cục trong mỗi lần tạo. "Toàn cục" có nghĩa là LLM cần lập kế hoạch một chuỗi lập luận hoàn chỉnh cho câu hỏi phức tạp, thay vì trả lời câu hỏi trực tiếp hoặc chỉ giải quyết các câu hỏi con "cục bộ" (so sánh được hiển thị trong Hình 2). Tại mỗi nút của chuỗi, LLM tập trung vào việc tạo ra truy vấn hướng IR và đưa ra câu trả lời nếu LLM biết. Nếu LLM không biết câu trả lời, nó nên đánh dấu truy vấn bằng "[Unsolved Query]", đây là cờ chỉ ra sự thiếu hụt kiến thức. Trong các vòng tiếp theo, khi một nút cần IR để sửa hoặc cung cấp kiến thức còn thiếu, LLM tạo ra CoQ mới theo phản hồi của IR để linh hoạt sửa đổi hướng lập luận. Thiết kế cho tình huống này sẽ được giới thiệu trong Phần 3.3. Sinh CoQ là một Tìm kiếm theo chiều sâu hoàn chỉnh cho 𝑄, điều này tránh IR phá vỡ chuỗi lập luận của LLM. Thí nghiệm (Phần 4.3.3) cũng cho thấy rằng đối với câu hỏi con khó, CoQ cho phép LLM

Thuật toán 1: Mô tả Tương tác với IR.
Khởi tạo: Các truy vấn đã xử lý: 𝑀=𝑛𝑢𝑙𝑙;
Đường lập luận đúng: 𝑅=𝑛𝑢𝑙𝑙;
Vòng tương tác: 𝑟=0;
Phản hồi:𝐹=𝑛𝑢𝑙𝑙; ToR: 𝑻=𝑄;
Hàm IR(𝑞𝑖,𝑎𝑖):
𝑑𝑖=Retrieval( 𝑞𝑖);// Truy xuất tài liệu Top-1 𝑑𝑖 cho 𝑞𝑖.
𝑔,𝑓=Reader(𝑞𝑖,𝑑𝑖);
// Trích xuất câu trả lời 𝑔 từ 𝑑𝑖 và đưa ra độ tin cậy 𝑓.
if 𝑞𝑖 is Unsolved Query then
// Hoàn thiện.
𝑅.add(𝑞𝑖,𝑔,𝑑𝑖);// Ghi lại nút đúng.
return PromptForComplete( 𝑞𝑖,𝑔,𝑑𝑖);
if 𝑓>𝜃 and NotEqual(𝑔,𝑎𝑖) then
// Xác minh.
𝑅.add(𝑞𝑖,𝑔,𝑑𝑖);// Ghi lại nút đúng.
return PromptForVerify( 𝑞𝑖,𝑔,𝑑𝑖);
𝑅.add(𝑞𝑖,𝑎𝑖,𝑑𝑖);return "Pass" ;
Hàm Traverse( 𝑪𝒐𝑸 ):
foreach(𝑞𝑖,𝑎𝑖) in 𝑪𝒐𝑸 do
ifnot DuplicateQuery( 𝑞𝑖,𝑀) then
// Nếu 𝑞𝑖 chưa được xử lý.
𝐹=IR(𝑞𝑖,𝑎𝑖);𝑀.add(𝑞𝑖);
if not F == "𝑃𝑎𝑠𝑠 " then return 𝐹;
return "𝐹𝑖𝑛𝑖𝑠ℎ " ;
Hàm Main(𝑄,𝐹):
while not (F == "Finish" or 𝑟>𝑟𝑚𝑎𝑥) do
𝑪𝒐𝑸 =ChainGenerate( 𝑄,𝐹);
// LLM tạo ra Chain-of-Query mới 𝑪𝒐𝑸 .
𝑻.AddChild( 𝑪𝒐𝑸 );// Thêm nhánh mới vào 𝑻.
𝐹=Traverse( 𝑪𝒐𝑸 );// Tương tác với IR.
𝑟=𝑟+1;// Cập nhật số vòng tương tác 𝑟.
return Tracing( 𝑻,𝑅)

giải quyết nó bằng nhiều bước lập luận hơn như viết lại hoặc phân tách thêm câu hỏi con trong khi các baseline có xu hướng dừng lập luận. Đó là vì các baseline tập trung vào việc giải quyết các câu hỏi con cục bộ hiện tại trong khi bỏ qua việc lập kế hoạch toàn cục của chuỗi lập luận. Góc nhìn toàn cục trong CoQ làm cho LLM cố gắng khám phá nhiều hơn các câu trả lời có thể khi đối mặt với khó khăn trung gian.

3.3 Tương tác với Truy xuất Thông tin
Trong mỗi vòng tương tác, LLM chuyển CoQ được tạo ra cho IR. IR xác minh và hoàn thiện thông tin cho mỗi nút (𝑞𝑖,𝑎𝑖) của CoQ và phản hồi cho LLM để giúp nó tạo ra CoQ chính xác hơn như nhánh mới của ToR (Tree-of-Reasoning). Bên cạnh đó, IR ghi lại các tài liệu được truy xuất tương ứng cho mỗi nút của CoQ như tài liệu hỗ trợ của nó, điều này tăng cường khả năng truy vết của nội dung được tạo bởi LLM. Mô tả tương tác được hiển thị trong Thuật toán 1. IR tương tác với mỗi nút (𝑞𝑖,𝑎𝑖) của CoQ, truy xuất tài liệu Top-1 𝑑𝑖 cho 𝑞𝑖 như tài liệu hỗ trợ, và đánh giá có nên xác minh hay hoàn thiện nó theo loại của 𝑞𝑖. Khi tất cả các truy vấn của CoQ không cần được sửa hoặc hoàn thiện, hoặc số vòng tương tác tối đa được đạt tới, tương tác kết thúc. SearChain truy vết ngược đường lập luận đúng của ToR và tham chiếu đến mỗi nút của đường để tạo ra nội dung cuối cùng với các tham chiếu được đánh dấu đến tài liệu hỗ trợ cho kiến thức của mỗi nút.

Xác minh. Xác minh nhằm đảm bảo tính đúng đắn của 𝑎𝑖 trong mỗi nút (𝑞𝑖,𝑎𝑖) của CoQ dựa trên cơ sở kiến thức bên ngoài, điều này cải thiện độ chính xác và độ tin cậy của nội dung được tạo. Cụ thể, với tài liệu Top-1 được truy xuất 𝑑𝑖 cho 𝑞𝑖, một Reader [ 14] đã được huấn luyện trên các bộ dữ liệu QA miền mở [ 14] được sử dụng để trích xuất câu trả lời 𝑔 cho 𝑞𝑖 từ 𝑑𝑖 với độ tin cậy 𝑓 của nó (𝑓 là giá trị dự đoán đo lường liệu 𝑔 có thể trả lời 𝑞𝑖 hay không):
𝑠=arg max(softmax(Hw𝑠)),𝑒=arg max(softmax(Hw𝑒)),
𝑔=𝑑𝑖[𝑠:𝑒],𝑓=H[𝐶𝐿𝑆]w𝑓,(w𝑠,w𝑡,w𝑓∈R𝐸),
trong đó H∈R𝐿×𝐸 là chuỗi các trạng thái ẩn cuối cùng cho văn bản đầu vào "[CLS]𝑞𝑖[SEP]𝑑𝑖", 𝐿 là độ dài và 𝐸 là chiều ẩn. H[CLS] là trạng thái ẩn cuối cùng của token [CLS]. Sau đó, SearChain đánh giá liệu câu trả lời 𝑎𝑖 được đưa ra bởi LLM có nhất quán với thông tin được truy xuất hay không theo (1) liệu 𝑔 có xuất hiện trong 𝑎𝑖 hay không (cho các tác vụ sinh dạng ngắn như hỏi đáp nhiều bước và điền slot) hoặc (2) liệu ROUGE [ 19] giữa 𝑎𝑖 và 𝑑𝑖 có lớn hơn ngưỡng 𝛼 hay không (cho các tác vụ sinh dài và tự do như ELI5 [ 7]). Nếu 𝑎𝑖 không nhất quán với thông tin được truy xuất và Reader đủ tin cậy ( 𝑓>𝜃, 𝜃 là ngưỡng để giảm thiểu tác động tiêu cực của IR lên LLM), một prompt được xây dựng để giúp LLM sửa câu trả lời 𝑎𝑖. Template của prompt là: "Theo Tham chiếu, câu trả lời cho 𝑞𝑖 nên là 𝑔, bạn có thể thay đổi câu trả lời của mình và tiếp tục xây dựng chuỗi lập luận cho [Câu hỏi]: 𝑄. Tham chiếu: 𝑑𝑖. ". Vòng này kết thúc. LLM nhận phản hồi của IR, đưa ra câu trả lời mới 𝑎′𝑖 cho 𝑞, và tạo ra CoQ mới với (𝑞𝑖,𝑎′𝑖) làm nút gốc, đây là nhánh mới của ToR.

Hoàn thiện. Hoàn thiện nhằm cung cấp cho LLM kiến thức còn thiếu trong các nút của CoQ, điều này cải thiện độ chính xác của nội dung được tạo. Cụ thể, trong sinh CoQ (Phần 3.2), LLM đánh dấu "[Unsolved Query]" cho truy vấn không thể giải quyết. Đối với truy vấn không thể giải quyết 𝑞∗𝑖, IR trích xuất câu trả lời 𝑔∗ từ tài liệu được truy xuất 𝑑∗𝑖 như được mô tả trong Xác minh. Bất kể 𝑓 có lớn hơn ngưỡng 𝜃 hay không, 𝑔∗ và 𝑑∗𝑖 sẽ được phản hồi cho LLM dưới dạng prompt vì LLM không thể giải quyết 𝑞∗𝑖. Template của prompt là: "Theo Tham chiếu, câu trả lời cho 𝑞∗𝑖 nên là 𝑔∗, bạn có thể đưa ra câu trả lời của mình và tiếp tục xây dựng chuỗi lập luận cho [Câu hỏi]: 𝑄. Tham chiếu: 𝑑∗𝑖. ". Vòng này kết thúc. LLM nhận phản hồi, đưa ra câu trả lời 𝑎∗𝑖 để giải quyết truy vấn 𝑞∗𝑖 và tạo ra CoQ mới với ( 𝑞∗𝑖,𝑎∗𝑖) làm nút gốc, đây là nhánh mới của ToR.

Truy vết. Truy vết nhằm tạo ra quá trình lập luận và đánh dấu tham chiếu đến tài liệu hỗ trợ cho mỗi bước lập luận, được sử dụng làm nội dung cuối cùng trả về cho người dùng. Điều này cải thiện khả năng truy vết của mỗi kiến thức trong nội dung được tạo. Cụ thể, SearChain ghi lại các tài liệu được truy xuất cho mỗi nút trên đường lập luận đúng của Tree-of-Reasoning như tài liệu hỗ trợ. SearChain nhắc LLM tạo ra nội dung cuối cùng bằng cách tham chiếu đến các nút trên đường đúng và đánh dấu tham chiếu đến tài liệu hỗ trợ cho các đoạn phụ tương ứng của nội dung được tạo (nội dung cuối cùng của Hình 1). Prompt là "Bạn có thể thử tạo ra câu trả lời cuối cùng cho [Câu hỏi] bằng cách tham chiếu đến các cặp [Query]-[Answer], bắt đầu với [Final Content]. [Query 1]:𝑞1.[Answer 1]:𝑎1...[Query𝑚]:𝑞𝑚.[Answer𝑚]: 𝑎𝑚. ". Thiết kế này cho phép người dùng có được các tài liệu liên quan của kiến thức liên quan trong mỗi bước lập luận. Chúng tôi tin rằng đó là một tác vụ đầy hứa hẹn để đánh dấu tham chiếu đến tài liệu hỗ trợ trên các đoạn phụ của nội dung phức tạp được tạo bởi LLM. Cách tiếp cận của chúng tôi cung cấp một cách tiếp cận mới và hiệu quả để giải quyết tác vụ này bằng cách truy xuất tài liệu hỗ trợ cho mỗi câu hỏi con liên quan trong quá trình lập luận của LLM mà không cần bất kỳ dữ liệu có giám sát nào (văn bản với chú thích trích dẫn) và huấn luyện LLM.

Tìm kiếm theo chiều sâu nhận diện nút. So với các phương pháp tăng cường truy xuất trước, tương tác với IR trong SearChain tạo thành một đường lập luận mới: tìm kiếm theo chiều sâu nhận diện nút trên cây. Trong mỗi lần tạo, LLM tạo ra CoQ để thực hiện lập luận liên tục trên các câu hỏi phức tạp cho đến khi câu trả lời cuối cùng được tạo ra hoặc gặp phải câu hỏi con không thể giải quyết. Điều này có thể được xem như một phần của Tìm kiếm theo chiều sâu (DFS). Tuy nhiên, khác với thuật toán DFS truyền thống [ 31], "nhận diện nút" trong SearChain có nghĩa là khi một tìm kiếm theo một hướng bị dừng, SearChain không quay về nút cha của nó, mà linh hoạt nhận diện nút cần được sửa hoặc hoàn thiện thông qua xác minh và hoàn thiện trong IR và tái tạo CoQ mới bắt đầu với nút này. Quá trình tương tác giữa IR và LLM trong SearChain là quá trình xây dựng cây sử dụng DFS nhận diện nút, cho phép LLM linh hoạt sửa đổi hướng lập luận.

4 THÍ NGHIỆM
4.1 Cài đặt Thí nghiệm
4.1.1 Bộ dữ liệu và Chỉ số Đánh giá .Chúng tôi chọn bốn tác vụ cổ điển phức tạp chuyên sâu về kiến thức bao gồm hỏi đáp nhiều bước (HotpotQA (HoPo) [ 40], Musique (MQ) [ 33], WikiMulti-HopQA (WQA) [ 11] và StrategyQA (SQA) [ 9]), điền slot (zsRE [ 17], T-REx [ 6]), kiểm tra sự thật (FEVER [ 32]) và hỏi đáp dạng dài (ELI5 [ 7]). Những tác vụ này đòi hỏi LLM thực hiện lập luận nhiều bước trên các câu hỏi phức tạp, và mỗi bước đòi hỏi kiến thức tương ứng để giải quyết. Về chỉ số đánh giá, đối với ELI5 có câu trả lời thật dài và tự do, chúng tôi sử dụng ROUGE-L [19] làm chỉ số. Đối với các tác vụ khác, chúng tôi sử dụng liệu câu trả lời thật có nằm trong câu trả lời được tạo hay không (tức là, cover-EM [ 26]) làm chỉ số. Theo DSP [ 15] và Self-Ask [ 24], chúng tôi đánh giá mô hình trên toàn bộ bộ dữ liệu phát triển của MQ và HoPo, bộ dữ liệu BIG-bench [ 29] trên SQA và các tập con của WQA, zsRE, T-REx, FEVER và ELI5 (mỗi tập con có 1.2k câu hỏi).

4.1.2 Baseline. Các baseline của chúng tôi có thể được chia thành hai loại, một là về cải thiện khả năng lập luận của LLM trên các tác vụ phức tạp (CoT [ 36], CoT-SC [ 35], Auto-CoT [ 45], Recite-and-answer [ 30] và Least-to-Most [ 48]), và loại khác không chỉ giới thiệu IR vào LLM mà còn cải thiện khả năng lập luận (Direct1, Self-Ask [ 24], ToolFormer2[28], React [ 42], DSP [ 15], Verify-and-Edit (kết hợp với CoT-SC) [ 46] và Tree-of-Thought [ 41]). AgentGPT và PS [ 34] sử dụng mô hình Plan-and-Solve, chúng tôi cũng tái tạo điều này như một trong các baseline.
1Truy xuất tài liệu và cung cấp chúng cho LLM trong prompt.
2Thực hiện ToolFormer trên gpt-3.5-turbo thông qua học trong ngữ cảnh.

--- TRANG 5 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

Bảng 1: Hiệu suất của SearChain và các baseline trên các tác vụ phức tạp chuyên sâu về kiến thức. In đậm biểu thị kết quả tốt nhất trong các cài đặt khác nhau. FC: Kiểm tra Sự thật, LFQA: Hỏi đáp Dạng dài. Chỉ số cho LFQA: ROUGE-L. Chỉ số cho các tác vụ khác: cover-EM.

Hỏi đáp Nhiều bước Điền Slot FC LFQA
HoPo MQ WQA SQA zsRE T-REx FEV. ELI5
Không có Truy xuất Thông tin
Nhắc Trực tiếp 31.95 5.91 25.82 66.25 22.75 43.85 73.45 21.90
Auto-CoT 33.53 10.55 29.15 65.40 21.30 43.98 76.61 21.55
CoT 35.04 9.46 30.41 65.83 22.36 44.51 76.98 21.79
CoT-SC 36.85 10.02 32.68 70.84 24.74 46.06 77.15 22.05
Recite-and-answer 36.49 10.97 32.53 70.47 24.98 46.14 77.35 22.10
Self-Ask w/o IR 33.95 11.10 35.65 65.45 20.16 44.71 75.31 21.73
Least-to-Most 34.05 11.45 32.88 65.78 21.86 44.98 75.98 21.95
Plan-and-Solve 36.33 12.95 35.68 73.21 25.15 47.58 77.08 22.23
SearChain w/o IR 38.36 13.61 40.49 75.62 30.14 52.69 77.06 22.54
Tương tác với Truy xuất Thông tin
Truy xuất Trực tiếp 34.09 10.22 30.01 66.78 52.29 59.28 78.25 23.40
ToolFormer 36.75 12.98 35.49 67.02 51.35 59.17 80.79 23.05
Self-Ask 40.05 14.28 39.58 67.65 50.51 59.12 79.41 23.25
Plan-and-Solve w/ IR 41.65 15.07 42.05 74.58 52.15 60.03 81.04 24.56
React→CoT-SC 43.15 15.49 40.36 70.43 53.27 60.42 80.59 24.05
Verify-and-Edit 44.03 15.57 40.83 71.09 53.95 61.10 80.67 23.80
Tree-of-Thought w/ IR 50.65 15.61 42.49 72.55 54.88 62.40 81.03 24.20
DSP 51.97 15.83 43.52 72.41 54.35 61.32 80.65 23.46
SearChain 56.91 17.07 46.27 76.95 57.29 65.07 81.15 25.57
- w/o Xác minh 46.11 14.70 42.67 75.98 43.58 55.46 78.79 22.98
- w/o Hoàn thiện 53.05 15.86 43.64 76.53 45.78 56.03 80.03 25.02

4.1.3 Triển khai .Mô hình ngôn ngữ lớn chúng tôi sử dụng là gpt-3.5-turbo được cung cấp từ API của OpenAI3 và mô hình truy xuất chúng tôi sử dụng là ColBERTv2 [ 27] (theo DSP). Mô hình IR suy luận trên một GPU Tesla V100. Đối với HotpotQA, chúng tôi sử dụng Wikipedia 2017 làm kho văn bản, được cung cấp bởi [ 40] trong cài đặt full-wiki. Đối với các bộ dữ liệu khác, chúng tôi sử dụng bộ sưu tập đoạn văn quy mô lớn được xây dựng trên Wikipedia làm kho văn bản [ 14,38]. Các baseline với truy xuất thông tin ở cùng cài đặt với SearChain. Chúng tôi tái tạo tất cả baseline trên gpt-3.5-turbo theo cài đặt trong các bài báo của họ. Số vòng tương tác tối đa 𝑟𝑚𝑎𝑥 là 5. Các ngưỡng 𝛼 và 𝜃 được đặt lần lượt là 0.35 và 1.5. Về việc lựa chọn ngưỡng độ tin cậy ( 𝜃), chúng tôi khởi tạo giá trị ban đầu của ngưỡng độ tin cậy (1.0) dựa trên kiến thức trước và dần dần tăng giá trị với bước nhảy 0.1. Chúng tôi xác thực F1-score trên các bộ dữ liệu QA miền mở hỗn hợp (NQ, TriviaQA, WebQ, và TREC) sau mỗi lần thay đổi giá trị. Chúng tôi thấy rằng khi ngưỡng độ tin cậy là 1.5, F1-score cao nhất có thể đạt được nên chúng tôi đặt ngưỡng độ tin cậy là 1.5. Về việc lựa chọn ngưỡng ROUGE (𝛼), chúng tôi xác định giá trị này bằng cách quan sát mối quan hệ ROUGE giữa văn bản được tạo và câu trả lời thật trong một vài ví dụ trong học ngữ cảnh. Các thí nghiệm tiếp theo của chúng tôi cho thấy rằng khi khoảng giá trị của ngưỡng ROUGE nằm giữa 0.3 và 0.5, sự thay đổi hiệu suất trên ELI5 không rõ ràng. Chi tiết về prompt và thí nghiệm được giới thiệu trong Phần A.3 của Phụ lục.

3https://openai.com/api/

4.2 Kết quả Chính
Hiệu suất trên các tác vụ chuyên sâu về kiến thức được hiển thị trong Bảng 1.
(1) Tác động của Chain-of-Query. CoQ là chuỗi lập luận cho các câu hỏi phức tạp trong SearChain. Chúng tôi so sánh nó với các baseline cạnh tranh gần đây trong cài đặt không có IR. SearChain w/o IR vượt trội hơn tất cả baseline dựa trên CoT (CoT, Auto-CoT, CoT-SC và Recite-and-answer), điều này cho thấy rằng tập trung vào xây dựng chuỗi lập luận toàn cục bao gồm các câu hỏi con tốt hơn việc chỉ đưa ra kết quả lập luận trung gian. SearChain w/o IR vượt trội hơn Self-Ask w/o IR và Least-to-Most, điều này cho thấy rằng việc tập trung vào xây dựng chuỗi lập luận toàn cục tại mỗi lần suy luận (góc nhìn toàn cục) hiệu quả hơn việc tạo ra và trả lời các câu hỏi con từng bước (góc nhìn cục bộ).

(2) Tác động của tương tác với IR. Trong cài đặt có tương tác với IR, SearChain một lần nữa vượt trội hơn tất cả baseline. Mô hình đầu tiên tạo ra CoQ toàn cục, sau đó IR tương tác với mỗi nút của CoQ đảm bảo tính nhất quán của việc lập luận LLM. Điều này giải quyết vấn đề trong Self-Ask, DSP và React. Bên cạnh đó, SearChain tách riêng kiến thức của LLM và IR. IR đánh giá có nên cung cấp thông tin cho LLM hay không theo độ tin cậy và cờ của nút trên CoQ, điều này hiệu quả giảm thiểu việc hiểu nhầm LLM. Cuối cùng, các baseline lập luận trong chuỗi một chiều. Chúng không thể linh hoạt sửa đổi hướng lập luận. Tương tác với IR trong SearChain biến đổi đường lập luận từ chuỗi thành tìm kiếm theo chiều sâu nhận diện nút trên cây, cho phép LLM linh hoạt sửa đổi hướng lập luận.

--- TRANG 6 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

Bảng 2: Phân bố nguồn kiến thức.
Nguồn Kiến thức HoPo MQ WQA SQA
LLM 74.56% 78.83% 75.83% 94.98%
Được sửa bởi IR 20.94% 14.60% 18.96% 2.78%
Được hoàn thiện bởi IR 4.50% 6.57% 5.21% 2.24%

Bảng 3: Tác động tích cực và tiêu cực của IR lên LLM.
(a) Độ chính xác trên S𝐼𝑅 và S (tác động tích cực↑).
HoPo MQ WQA SQA
w/o IR ( S) 38.36 13.61 40.49 75.62
w/o IR ( S𝐼𝑅) 31.38 10.20 32.60 68.96
w IR ( S𝐼𝑅) 60.86 18.49 50.52 78.42

(b) Tỷ lệ phần trăm IR làm cho LLM hiểu nhầm (tác động tiêu cực ↓).
HoPo MQ WQA SQA
Self-Ask 15.76 14.32 25.76 10.29
React 17.68 15.22 25.99 10.03
Plan-and-Solve w/ IR 16.42 15.25 22.31 7.59
Verify-and-Edit 9.78 10.75 16.44 6.52
Tree-of-Thought w/ IR 12.07 13.25 20.52 8.46
DSP 14.72 14.03 24.31 9.22
SearChain 6.33 6.50 12.71 5.31

4.3 Phân tích
Trong phần này, chúng tôi thảo luận và chứng minh chi tiết các ưu điểm của SearChain so với các baseline. Đầu tiên, chúng tôi phân tích nguồn kiến thức của SearChain trong việc giải quyết các câu hỏi phức tạp. Thứ hai, trong khi chúng tôi phân tích tác động tích cực của IR lên LLM trong việc giải quyết các câu hỏi khó, chúng tôi cũng chứng minh rằng phương pháp của chúng tôi có thể giảm thiểu tốt hơn tác động tiêu cực của IR lên LLM. Thứ ba, chúng tôi cho thấy các ưu điểm của SearChain so với các baseline về khả năng lập luận và truy vết. Cuối cùng, chúng tôi thực hiện phân tích hiệu quả để cho thấy phương pháp của chúng tôi cải thiện đáng kể hiệu suất tác vụ mà không tăng đáng kể thời gian tiêu thụ.

4.3.1 Tách riêng Kiến thức .Chúng tôi phân tích các nguồn kiến thức trên bốn bộ dữ liệu hỏi đáp nhiều bước. Cụ thể, chúng tôi phân loại nguồn kiến thức thành ba loại: (1) kiến thức của LLM, (2) kiến thức được sửa bởi IR trong xác minh, và (3) kiến thức mà LLM không biết và được cung cấp bởi IR trong hoàn thiện. Chúng tôi sử dụng nút của ToR làm đơn vị thống kê để tính tỷ lệ phần trăm các nút từ ba nguồn này trong tổng số nút tương ứng. Kết quả thí nghiệm được hiển thị trong Bảng 2. Đáng chú ý rằng mặc dù hầu hết kiến thức đến từ LLM, kiến thức này cũng được xác minh bởi IR. IR chỉ sửa câu trả lời không nhất quán được đưa ra bởi LLM khi nó đủ tin cậy và cung cấp cho LLM kiến thức còn thiếu, điều này giảm thiểu tác động tiêu cực của IR lên LLM và cải thiện việc sử dụng thông tin được truy xuất. Trên StrategyQA, LLM đã ghi nhớ hầu hết kiến thức mà IR có thể truy xuất, vì vậy IR cung cấp ít kiến thức hơn các bộ dữ liệu khác.

Bảng 4: Số bước lập luận. SearChain cố gắng nhiều hơn cho các câu hỏi con không thể giải quyết để đạt được độ chính xác tốt hơn.
2-bước 3-bước 4-bước Độ chính xác
CoT 2.25 2.23 2.19 9.46
Self-Ask w/o IR 2.04 2.21 2.15 11.10
Least-to-Most 2.52 2.68 2.70 11.45
SearChain w/o IR 4.16 4.66 5.06 13.61

Không rõ thực thể Robert J Chassell là thành viên của tổ chức nào. Câu hỏi không thể được trả lời.CoT
Self-Ask w/o Retrieval
Theo dõi: Robert J Chassell là ai? 
Câu trả lời trung gian: Robert J Chassell là một lập trình viên và tác giả.
Theo dõi: Robert J Chassell là thành viên của tổ chức nào? 
Câu trả lời trung gian: Tổ chức mà Robert J Chassell là thành viên không được chỉ định.
SearChain w/o Retrieval
[Query 1]: Robert J Chassell là thành viên của cái gì?
[Answer 1]: Không có thông tin về tổ chức hay nhóm nào mà Robert J Chassell là thành viên.
[Query 2]: Chúng ta có thể tìm thấy thông tin nào về các liên kết hoặc hiệp hội của Robert J Chassell không?
[Answer 2]: Có, Robert J Chassell được xác định là thành viên của Free Software Foundation.
[Query 3]: Ai đã thành lập Free Software Foundation?
[Answer 3]: Free Software Foundation được thành lập bởi Richard Stallman.
[Final Content]: Richard Stallman đã thành lập Free Software Foundation [3], mà Robert J Chassell là 
thành viên [2].  Vậy câu trả lời cuối cùng là Richard Stallman. Ai đã thành lập cái mà Robert J Chassell là thành viên?

Hình 4: Nghiên cứu trường hợp về sự khác biệt giữa SearChain và các baseline cho các câu hỏi con không thể giải quyết.

4.3.2 Tác động Tích cực và Tiêu cực của IR lên LLM .(1) Tích cực. Trong SearChain, IR có thể xác định khó khăn của LLM và hiệu quả giúp LLM sửa câu trả lời và có được kiến thức còn thiếu. Chúng tôi chọn các câu hỏi ( S𝐼𝑅) mà IR giúp sửa hoặc cung cấp kiến thức từ các bộ dữ liệu được sử dụng trong Bảng 1 ( S) và đánh giá độ chính xác của SearChain trên S𝐼𝑅. Chúng tôi cũng đánh giá độ chính xác của SearChain w/o IR trên S𝐼𝑅. Kết quả trong Bảng 3(a) cho thấy w/o IR hoạt động tệ hơn trên S𝐼𝑅 so với trên S, điều này cho thấy LLM thực sự gặp khó khăn với các câu hỏi đòi hỏi sự giúp đỡ của IR. w/ IR hoạt động tốt hơn trên S𝐼𝑅, điều này cho thấy IR hiệu quả xác định và giải quyết khó khăn của LLM. (2) Tiêu cực. Phần 1 chỉ ra nguy cơ IR làm cho LLM hiểu nhầm khi có xung đột trong kiến thức của IR và LLM. Chúng tôi chọn các câu hỏi ( S𝑡) mà LLM có thể đưa ra câu trả lời đúng và đếm tỷ lệ phần trăm LLM đưa ra câu trả lời sai sau khi thêm IR trên S𝑡. Bảng 3(b) cho thấy SearChain hiệu quả giảm thiểu tác động tiêu cực của IR lên LLM. SearChain sử dụng độ tin cậy của IR và thông tin của CoQ để đánh giá có nên sửa LLM hay cung cấp cho LLM kiến thức còn thiếu của nó.

4.3.3 CoQ vs Baseline trong Lập luận .CoQ hoạt động tốt hơn trong việc lập luận các câu hỏi phức tạp so với các baseline. Ngoài Bảng 1, chúng tôi tiếp tục phân tích khả năng lập luận từ hai khía cạnh:
(1) Số Bước Lập luận. Chúng tôi phân tích số bước lập luận trong các phương pháp khác nhau trong cài đặt không có IR. Chúng tôi tiến hành thí nghiệm trên Musique vì Musique có các câu hỏi phức tạp hơn. Bảng 4 cho thấy số bước lập luận trung bình trên các câu hỏi với số bước khác nhau. Phương pháp của chúng tôi có nhiều bước lập luận hơn, và số bước lập luận tăng theo số bước của câu hỏi. Điều này cho thấy phương pháp của chúng tôi có nhận thức tốt hơn về độ phức tạp của các câu hỏi.

--- TRANG 7 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

New Bing
Tôi xin lỗi, nhưng tôi không thể tìm thấy bất kỳ
thông tin nào về một loại nấm được Elsie Maud Wakefield 
mô tả là "Lepiota" trong một vườn thực vật ở London. 
Tôi đã tìm thấy rằng Elsie Maud Wakefield là nhà nghiên 
cứu nấm tại Royal Botanic Gardens, Kew từ năm 1910 
đến 1951 và là chuyên gia về Basidiomycetes và là 
chuyên gia quốc tế về Aphyllophorales [1,2].
[1]:…Elsie Maud Wakefield, là nhà nghiên cứu 
nấm và bệnh học thực vật người Anh …
[2]:…Elsie Wakefield là chuyên gia về 
Basidiomycetes và chuyên gia quốc tế về 
Aphyllophorales .…SearChain
Raymond Ochoa là một 
diễn viên nhí và diễn viên 
lồng tiếng người Mỹ [1]. 
Anh ấy đã đóng nhiều vai 
trong các bộ phim bao gồm 
Arlo trong "The Good 
Dinosaur" [2]. Arlo là một 
Apatosaurus [3]. Vậy câu 
trả lời cuối cùng là 
Apatosaurus.
[1]: Raymond Ochoa là 
diễn viên nhí và diễn viên 
lồng tiếng người Mỹ …
[2]: Raymond Ochoa … 
đóng vai Arlo trong phim 
Pixar "The Good 
Dinosaur" ...
[3]… Apatosaurus nhút nhát 
tên Arlo (Ochoa) …SearChain
Jimmy Bartel là người nhận 
Huy chương Brownlow năm 
2007 [1] và chơi bóng đá 
Úc chuyên nghiệp [2]. Anh 
ấy chơi cho Geelong Football 
Club trong Australian 
Football League [2]. Vậy 
câu trả lời cuối cùng là 
Geelong Football Club.
[1]:…Jimmy Bartel đã thắng 
huy chương trong mùa giải 
AFL 2007 …
[2]:…Jimmy Bartel là cựu 
cầu thủ bóng đá Úc chơi 
cho Geelong Football Club ...Đội nào mà người nhận Huy chương Brownlow 
năm 2007 chơi cho? Loại khủng long nào mà Raymond Ochoa 
lồng tiếng trong phim Pixar "The Good Dinosaur"? 
New Bing
Raymond Ochoa lồng 
tiếng cho nhân vật Arlo, 
một Apatosaurus trong 
phim Pixar "The Good 
Dinosaur" [1]
[1]:(Amazon.com: The 
Good Dinosaur: Ray-
mond Ochoa)SearChain
Elise Maud Wakefield là nhà nghiên cứu nấm 
và bệnh học thực vật người Anh [1]. "Lepiota" 
là một chi nấm [2]. Kew Gardens nổi tiếng với 
bộ sưu tập nấm [3], chẳng hạn như "Lepiota 
viridigleba" [4]. Vậy câu trả lời cuối cùng là 
Kew Gardens.
[1]:…Elsie Maud Wakefield, là nhà nghiên cứu 
nấm và bệnh học thực vật người Anh …
[2]:…Lepiota viridigleba là loài nấm sequestrate 
trong họ Agaricaceae …
[3]…Kew Gardens là vườn thực vật ở tây nam 
London chứa "bộ sưu tập thực vật học và nấm 
học lớn nhất và đa dạng nhất trên thế giới" ...
[4]…Bộ sưu tập nấm tại Kew Gardens bao gồm 
các loài như…"Lepiota viridigleba"…Trong vườn thực vật London nào bạn có thể tìm thấy 
loại nấm được Elise Maud Wakefield mô tả là "Lepiota"?
New Bing
Người nhận Huy chương 
Brownlow năm 2007 là 
Jimmy Bartel, người đã 
chơi cho Geelong Football 
Club. Anh ấy đã thắng huy 
chương bằng cách nhận 
được hai mươi chín phiếu 
bầu trong mùa giải AFL 
2007 [1]. Có điều gì khác 
bạn muốn biết về chủ đề 
này không?
[1]:…Jimmy Bartel đã 
thắng huy chương bằng cách 
nhận được hai mươi chín 
phiếu bầu trong mùa giải 
AFL 2007 …

Hình 5: Nghiên cứu trường hợp của SearChain và New Bing trong việc đánh dấu tham chiếu đến tài liệu hỗ trợ.

(2) Giải quyết Các câu hỏi con Khó. Các baseline tập trung vào việc giải quyết các câu hỏi con cục bộ trong khi bỏ qua việc lập kế hoạch toàn cục của chuỗi lập luận. Điều này dẫn đến LLM có xu hướng dừng lập luận thay vì cố gắng nhiều hơn khi một câu hỏi con không thể được giải quyết. Trong phương pháp của chúng tôi, LLM đóng vai trò là chỉ huy lập kế hoạch chuỗi lập luận toàn cục có thể giải quyết câu hỏi phức tạp, khi một câu hỏi con không thể được giải quyết, ngay cả không có sự giúp đỡ của IR, LLM có thể cố gắng phân tách thêm hoặc viết lại câu hỏi con để tiếp tục lập luận. Đó là vì phương pháp của chúng tôi tập trung vào việc xây dựng chuỗi toàn cục có thể giải quyết câu hỏi phức tạp (góc nhìn toàn cục), thay vì trả lời hoặc tạo ra các câu hỏi con từng bước (góc nhìn cục bộ). Nghiên cứu trường hợp trong Hình 4 cho thấy CoT và Self-Ask dừng lập luận trong khi SearChain tiếp tục lập luận bằng cách viết lại câu hỏi con. Nhiều bước lập luận hơn trong Bảng 4 cũng hỗ trợ rằng SearChain có thể cố gắng nhiều hơn cho các câu hỏi con khó. Nhiều nghiên cứu trường hợp được hiển thị trong Phần A.1.1 của Phụ lục.

4.3.4 SearChain vs New Bing trong Truy vết .Chúng tôi so sánh hiệu suất của SearChain và New Bing trong việc đánh dấu tham chiếu cho nội dung được tạo thông qua nghiên cứu trường hợp (Hình 5). Chúng tôi tiếp tục đề xuất hai chỉ số để đánh giá Phạm vi Bao phủ Kiến thức và Độ chính xác của Vị trí Đánh dấu để hiển thị khả năng truy vết một cách trực quan hơn:

•Phạm vi Bao phủ Kiến thức (SKC) [0, +] : Số lượng mục kiến thức được đánh dấu với tài liệu hỗ trợ trong nội dung được tạo. (thống kê, SearChain (2.882) tốt hơn New Bing (1.143))

•Độ chính xác của Vị trí Đánh dấu (AMP) [0, 1] : Độ chính xác của vị trí của các dấu tham chiếu. Tức là, liệu các tham chiếu có được đánh dấu chính xác trên các đoạn phụ cho kiến thức tương ứng trong nội dung được tạo hay không. (đánh giá con người, SearChain (0.80) tốt hơn New Bing (0.45))

Chúng tôi giới thiệu ba người có bằng thạc sĩ tham gia vào đánh giá con người của chúng tôi và kết quả cho thấy SearChain có thể đánh dấu tham chiếu cho mỗi kiến thức liên quan trong quá trình lập luận (tức là, các nút đúng của CoQ) một cách chi tiết. Trong khi các tham chiếu được đưa ra bởi New Bing không bao phủ tất cả kiến thức

Bảng 5: Phân tích hiệu quả.
#𝑛↓#𝑚↓#𝑟↓𝑡(𝑠)↓Hiệu suất (Trung bình)↑
Self-Ask 401 63 2.19 6.63 46.73
Plan-and-Solve w/ IR 450 71 1 6.05 48.89
React→CoT-SC 938 110 2.35 8.25 48.47
Verify-and-Edit 565 307 2.40 13.90 48.88
Tree-of-Thought w/ IR 622 341 2.29 13.28 50.47
DSP 1759 155 2.15 10.47 50.44
SearChain 390 189 2.21 8.52 53.29

và không thể được đánh dấu ở vị trí chính xác. Nhiều nghiên cứu trường hợp được hiển thị trong Phần A.1.2 của Phụ lục.

4.3.5 Phân tích Hiệu quả .Chúng tôi phân tích hiệu quả chạy giữa SearChain và các baseline về số từ trong văn bản đầu vào (𝑛) và đầu ra ( 𝑚) của LLM, số vòng tương tác giữa LLM và IR ( 𝑟) và thời gian chạy tổng thể ( 𝑡). Bảng 5 cho thấy phương pháp của chúng tôi cải thiện đáng kể hiệu suất tác vụ mà không tăng đáng kể thời gian tiêu thụ. Hầu hết các baseline cũng đòi hỏi nhiều vòng tương tác giữa IR và LLM.

5 KẾT LUẬN
Trong bài báo này, chúng tôi chỉ ra các thách thức của việc giới thiệu IR vào LLM từ góc độ lập luận và kiến thức. Sau đó chúng tôi đề xuất một khung mới có tên SearChain để cho phép IR và LLM tương tác với nhau một cách hiệu quả. SearChain không chỉ kích thích khả năng lập luận-kiến thức của LLM mà còn sử dụng IR để cung cấp kiến thức mà LLM thực sự cần dựa trên cơ sở kiến thức bên ngoài, điều này cải thiện cả độ chính xác và độ tin cậy. Bên cạnh đó, SearChain có thể đánh dấu tham chiếu đến tài liệu hỗ trợ cho kiến thức liên quan trong nội dung được tạo, điều này cải thiện khả năng truy vết của nội dung. Ngoài ra, tương tác giữa IR và LLM trong SearChain biến đổi đường lập luận từ chuỗi thành tìm kiếm theo chiều sâu nhận diện nút trên cây, cho phép

--- TRANG 8 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

LLM linh hoạt sửa đổi hướng lập luận. Kết quả thí nghiệm trên các tác vụ phức tạp chuyên sâu về kiến thức cho thấy SearChain hoạt động tốt hơn tất cả baseline.

LỜI CẢM ơN
Công trình này được hỗ trợ bởi Chương trình R&D Chính quốc gia Trung Quốc (2022YFB3103700, 2022YFB3103704), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) theo Các khoản tài trợ số 62276248 và U21B2046, và Hiệp hội Khuyến khích Đổi mới Thanh niên CAS theo Các khoản tài trợ số 2023111.

TÀI LIỆU THAM KHẢO
[1]Razvan Azamfirei, Sapna R Kudchadkar, và James Fackler. 2023. Các mô hình ngôn ngữ lớn và những nguy hiểm của ảo giác của chúng. Critical Care 27, 1 (2023), 1–2.
[2]Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, và Pascale Fung. 2023. Đánh giá Đa nhiệm vụ, Đa ngôn ngữ, Đa phương thức của ChatGPT về Lập luận, Ảo giác và Tương tác. CoRR abs/2302.04023 (2023). arXiv:2302.04023 https://doi.org/10.48550/arXiv.2302.04023
[3]Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, et al .2022. Cải thiện Mô hình Ngôn ngữ bằng cách Truy xuất từ Hàng nghìn tỷ Token. Trong International Conference on Machine Learning, ICML 2022, 17-23 tháng 7 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 2206–2240. https://proceedings.mlr.press/v162/borgeaud22a.html
[4]Xue-Qi Cheng và Hua-Wei Shen. 2010. Khám phá cấu trúc cộng đồng liên quan đến động lực khuếch tán trên mạng. Journal of Statistical Mechanics: Theory and Experiment 2010, 04 (2010), P04024.
[5]Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, và Denny Zhou. 2023. Phân tích Ngữ nghĩa Tổng hợp với Mô hình Ngôn ngữ Lớn. Trong The Eleventh International Conference on Learning Representations . https://openreview.net/forum?id=gJW8hSGBys8
[6]Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, et al .2018. T-REx: Căn chỉnh Quy mô Lớn Ngôn ngữ Tự nhiên với Bộ ba Cơ sở Kiến thức. Trong Proceedings of the 2018 Conference on LREC . European Language Resources Association (ELRA), Miyazaki, Japan. https://aclanthology.org/L18-1544
[7]Angela Fan, Yacine Jernite, Ethan Perez, et al .2019. ELI5: Trả lời Câu hỏi Dạng dài. Trong Proceedings of the 2019 Conference on ACL . Association for Computational Linguistics, Florence, Italy, 3558–3567. https://doi.org/10.18653/v1/P19-1346
[8]Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, và Tushar Khot. 2023. Nhắc dựa trên Độ phức tạp cho Lập luận Nhiều bước. Trong The Eleventh International Conference on Learning Representations . https://openreview.net/forum?id=yf1icZHC-l9
[9]Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, và Jonathan Berant. 2021. Aristotle có sử dụng Laptop không? Một Tiêu chuẩn Trả lời Câu hỏi với Chiến lược Lập luận Ngầm. Transactions of the Association for Computational Linguistics (TACL) (2021).
[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Ming-Wei Chang. 2020. REALM: Tiền huấn luyện Mô hình Ngôn ngữ Tăng cường Truy xuất. CoRR abs/2002.08909 (2020). arXiv:2002.08909 https://arxiv.org/abs/2002.08909
[11] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, và Akiko Aizawa. 2020. Xây dựng Bộ dữ liệu QA Nhiều bước để Đánh giá Toàn diện các Bước Lập luận. Trong Proceedings of the 2020 Conference on COLING . International Committee on Computational Linguistics, Barcelona, Spain (Online), 6609–6625. https://doi.org/10.18653/v1/2020.coling-main.580
[12] Gautier Izacard và Edouard Grave. 2020. Tận dụng Truy xuất Đoạn văn với Mô hình Sinh để Trả lời Câu hỏi Miền mở. CoRR abs/2007.01282 (2020). arXiv:2007.01282 https://arxiv.org/abs/2007.01282
[13] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, và Colin Raffel. 2022. Mô hình Ngôn ngữ Lớn Gặp khó khăn trong việc Học Kiến thức Đuôi dài. arXiv:2211.08411 [cs.CL]
[14] Vladimir Karpukhin, Barlas Oguz, Sewon Min, et al .2020. Truy xuất Đoạn văn Dày đặc cho Trả lời Câu hỏi Miền mở. Trong Proceedings of the 2020 Conference on EMNLP . Association for Computational Linguistics, Online, 6769–6781. https://doi.org/10.18653/v1/2020.emnlp-main.550
[15] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, et al .2023. Demonstrate-Search-Predict: Tổng hợp mô hình truy xuất và ngôn ngữ cho NLP chuyên sâu về kiến thức. arXiv:2212.14024 [cs.CL]
[16] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. 2022. Mô hình Ngôn ngữ Lớn là Người lập luận Zero-Shot. Trong Advances in Neural Information Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (Eds.). https://openreview.net/forum?id=e2TBb5y0yFf

[17] Omer Levy, Minjoon Seo, Eunsol Choi, và Luke Zettlemoyer. 2017. Trích xuất Quan hệ Zero-Shot thông qua Đọc hiểu. Trong Proceedings of the 2017 Conference on CoNLL) . Association for Computational Linguistics, Vancouver, Canada, 333–342. https://doi.org/10.18653/v1/K17-1034
[18] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, và Douwe Kiela. 2020. Sinh Tăng cường Truy xuất cho các Tác vụ NLP Chuyên sâu về Kiến thức. Trong Proceedings of the 2020 Conference on NeurIPS .
[19] Chin-Yew Lin. 2004. ROUGE: Gói Đánh giá Tự động các Bản tóm tắt. Trong Text Summarization Branches Out . Association for Computational Linguistics, Barcelona, Spain, 74–81. https://aclanthology.org/W04-1013
[20] Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, và Luke Zettlemoyer. 2022. Mô hình Ngôn ngữ Có mặt nạ Phi tham số. CoRR abs/2212.01349 (2022). https://doi.org/10.48550/arXiv.2212.01349 arXiv:2212.01349
[21] Xiangyang Mou, Chenghao Yang, Mo Yu, Bingsheng Yao, Xiaoxiao Guo, Saloni Potdar, và Hui Su. 2021. Trả lời Câu hỏi Tường thuật với Kỹ thuật QA Miền mở Tiên tiến: Một Nghiên cứu Toàn diện. Trans. Assoc. Comput. Linguistics 9 (2021), 1032–1046. https://doi.org/10.1162/tacl_a_00411
[22] Shuzi Niu, Jiafeng Guo, Yanyan Lan, và Xueqi Cheng. 2012. Học xếp hạng top-k: gán nhãn, xếp hạng và đánh giá. Trong Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval . 751–760.
[23] Fabio Petroni, Aleksandra Piktus, Angela Fan, et al .2021. KILT: một Tiêu chuẩn cho các Tác vụ Ngôn ngữ Chuyên sâu về Kiến thức. Trong Proceedings of the 2021 Conference on NAACL . Association for Computational Linguistics, Online, 2523–2544. https://doi.org/10.18653/v1/2021.naacl-main.200
[24] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, và Mike Lewis. 2023. Đo lường và Thu hẹp Khoảng cách Tổng hợp trong Mô hình Ngôn ngữ. https://openreview.net/forum?id=PUwbwZJz9dO
[25] Hongjing Qian, Yutao Zhu, Zhicheng Dou, et al .2023. WebBrain: Học Tạo Bài viết Chính xác về Sự thật cho Truy vấn bằng cách Dựa trên Kho Văn bản Web Lớn. arXiv:2304.04358 [cs.CL]
[26] Corby Rosset, Chenyan Xiong, Minh Phan, et al .2020. Tiền huấn luyện Mô hình Ngôn ngữ Nhận thức Kiến thức. CoRR abs/2007.00655 (2020). arXiv:2007.00655 https://arxiv.org/abs/2007.00655
[27] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, và Matei Zaharia. 2022. ColBERTv2: Truy xuất Hiệu quả và Có hiệu lực thông qua Tương tác Muộn Nhẹ. Trong Proceedings of the 2022 Conference on NAACL . Association for Computational Linguistics, Seattle, United States, 3715–3734. https://doi.org/10.18653/v1/2022.naacl-main.272
[28] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, và Thomas Scialom. 2023. Toolformer: Mô hình Ngôn ngữ Có thể Dạy Bản thân Sử dụng Công cụ. arXiv:2302.04761 [cs.CL]
[29] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al .2022. Vượt ra ngoài Trò chơi Bắt chước: Định lượng và ngoại suy khả năng của mô hình ngôn ngữ. arXiv:2206.04615 [cs.CL]
[30] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, và Denny Zhou. 2023. Mô hình Ngôn ngữ Tăng cường Đọc thuộc. Trong The Eleventh International Conference on Learning Representations . https://openreview.net/forum?id=-cqvvvb-NkI
[31] Robert Tarjan. 1971. Tìm kiếm theo chiều sâu và thuật toán đồ thị tuyến tính. Trong 12th Annual Symposium on Switching and Automata Theory (swat 1971) . 114–121. https://doi.org/10.1109/SWAT.1971.10
[32] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, và Arpit Mittal. 2018. FEVER: một Bộ dữ liệu Quy mô lớn cho Trích xuất Sự thật và Xác minh. Trong Proceedings of the 2018 Conference on NAACL . Association for Computational Linguistics, New Orleans, Louisiana, 809–819. https://aclanthology.org/N18-1074
[33] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, và Ashish Sabharwal. 2022. MuSiQue: Câu hỏi Nhiều bước thông qua Tổng hợp Câu hỏi Một bước. Transactions of the Association for Computational Linguistics 10 (2022), 539–554. https://doi.org/10.1162/tacl_a_00475
[34] Lei Wang, Wanyu Xu, Yihuai Lan, et al .2023. Plan-and-Solve Prompting: Cải thiện Lập luận Chain-of-Thought Zero-Shot bằng Mô hình Ngôn ngữ Lớn. arXiv:2305.04091 [cs.CL]
[35] Xuezhi Wang, Jason Wei, Dale Schuurmans, et al .2023. Tự Nhất quán Cải thiện Lập luận Chain of Thought trong Mô hình Ngôn ngữ. Trong The Eleventh International Conference on Learning Representations . https://openreview.net/forum?id=1PL1NIMMrw
[36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, và Denny Zhou. 2022. Nhắc Chain of Thought Kích thích Lập luận trong Mô hình Ngôn ngữ Lớn. CoRR abs/2201.11903 (2022). arXiv:2201.11903 https://arxiv.org/abs/2201.11903
[37] Shicheng Xu, Liang Pang, Huawei Shen, và Xueqi Cheng. 2022. Match-Prompt: Cải thiện Khả năng Tổng quát Đa nhiệm vụ cho Khớp Văn bản Thần kinh thông qua Học Prompt. Trong Proceedings of the 31st ACM International Conference on Information & Knowledge Management . 2290–2300.

--- TRANG 9 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

[38] Shicheng Xu, Liang Pang, Huawei Shen, và Xueqi Cheng. 2023. BERM: Huấn luyện Biểu diễn Cân bằng và Có thể Trích xuất cho Khớp để Cải thiện Khả năng Tổng quát của Truy xuất Dày đặc. arXiv preprint arXiv:2305.11052 (2023).
[39] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, và Xueqi Cheng. 2024. Mô hình Liên kết Nhận thức Danh sách-Xếp hạng lại-Cắt ngắn cho Tìm kiếm và Sinh Tăng cường Truy xuất. arXiv preprint arXiv:2402.02764 (2024).
[40] Zhilin Yang, Peng Qi, Saizheng Zhang, et al .2018. HotpotQA: Một Bộ dữ liệu cho Trả lời Câu hỏi Nhiều bước Đa dạng, Có thể Giải thích. Trong Proceedings of the 2018 Conference on EMNLP . Association for Computational Linguistics, Brussels, Belgium, 2369–2380. https://doi.org/10.18653/v1/D18-1259
[41] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, và Karthik Narasimhan. 2023. Tree of Thoughts: Giải quyết Vấn đề Cân nhắc với Mô hình Ngôn ngữ Lớn. arXiv:2305.10601 [cs.CL]
[42] Shunyu Yao, Jeffrey Zhao, Dian Yu, et al .2023. ReAct: Tổng hợp Lập luận và Hành động trong Mô hình Ngôn ngữ. Trong The Eleventh International Conference on Learning Representations . https://openreview.net/forum?id=WE_vluYUL-X
[43] Da Yin, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei Chang, Furu Wei, và Jianfeng Gao. 2022. Một Khảo sát về NLP Chuyên sâu về Kiến thức với Mô hình Ngôn ngữ Tiền huấn luyện. arXiv:2202.08772 [cs.CL]
[44] Eric Zelikman, Yuhuai Wu, Jesse Mu, và Noah Goodman. 2022. STaR: Bootstrap Lập luận với Lập luận. Trong Advances in Neural Information Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (Eds.). https://openreview.net/forum?id=_3ELRdg2sgI
[45] Zhuosheng Zhang, Aston Zhang, Mu Li, và Alex Smola. 2023. Nhắc Chain of Thought Tự động trong Mô hình Ngôn ngữ Lớn. Trong The Eleventh International Conference on Learning Representations . https://openreview.net/forum?id=5NTt8GFjUHkr
[46] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, và Lidong Bing. 2023. Verify-and-Edit: Một Khung Chain-of-Thought Tăng cường Kiến thức. Trong Proceedings of the 2023 Conference on ACL . Association for Computational Linguistics, Toronto, Canada, 5823–5840. https://doi.org/10.18653/v1/2023.acl-long.320
[47] Wayne Xin Zhao, Kun Zhou, Junyi Li, et al .2023. Một Khảo sát về Mô hình Ngôn ngữ Lớn. arXiv:2303.18223 [cs.CL]
[48] Denny Zhou, Nathanael Schärli, Le Hou, et al .2022. Nhắc Từ ít đến nhiều Cho phép Lập luận Phức tạp trong Mô hình Ngôn ngữ Lớn. arXiv:2205.10625 [cs.AI]
[49] Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, và Xueqi Cheng. 2021. Tìm kiếm thông tin thích ứng cho trả lời câu hỏi miền mở. arXiv preprint arXiv:2109.06747 (2021).

A PHỤ LỤC
A.1 Nghiên cứu Trường hợp
Trong phần này, chúng tôi so sánh hiệu suất của SearChain và New Bing4 trong việc thêm tham chiếu đến tài liệu hỗ trợ cho nội dung được tạo thông qua nghiên cứu trường hợp. Chúng tôi cũng sử dụng nghiên cứu trường hợp để tiếp tục phân tích tại sao CoQ có khả năng lập luận mạnh hơn so với Baseline.

A.1.1 Nghiên cứu Trường hợp cho CoQ vs Baseline trong Lập luận. Các baseline tập trung vào việc giải quyết các câu hỏi con cục bộ trong khi bỏ qua việc lập kế hoạch toàn cục của chuỗi lập luận, điều này dẫn đến LLM có xu hướng dừng lập luận thay vì cố gắng nhiều hơn khi một câu hỏi con không thể được giải quyết. Trong phương pháp của chúng tôi, LLM đóng vai trò là chỉ huy lập kế hoạch chuỗi lập luận toàn cục có thể giải quyết câu hỏi phức tạp, khi một câu hỏi con không thể được giải quyết, ngay cả không có sự giúp đỡ của IR, LLM có thể cố gắng phân tách thêm hoặc viết lại câu hỏi con để tiếp tục lập luận. Đó là vì phương pháp của chúng tôi tập trung vào việc xây dựng chuỗi toàn cục có thể giải quyết câu hỏi phức tạp (góc nhìn toàn cục), thay vì trả lời hoặc tạo ra các câu hỏi con từng bước (góc nhìn cục bộ). Điều này làm cho LLM cố gắng nhiều hơn khi đối mặt với khó khăn trung gian để cuối cùng giải quyết các câu hỏi phức tạp. Nghiên cứu trường hợp được hiển thị trong Hình 6 cho thấy SearChain tiếp tục lập luận nhưng các baseline dừng lại.

A.1.2 Nghiên cứu Trường hợp cho SearChain vs New Bing trong Truy vết. Chúng tôi so sánh hiệu suất của SearChain và New Bing trong việc đánh dấu tham chiếu cho nội dung được tạo thông qua nghiên cứu trường hợp (Hình 7). SearChain có thể đánh dấu tham chiếu cho mỗi kiến thức liên quan trong quá trình lập luận (tức là, mỗi nút đúng của CoQ) một cách chi tiết hơn

4https://www.bing.com/new

Câu hỏi không đầy đủ và không rõ ràng về cha của Menucha Rochel Slonim. CoT
Self-Ask
w/o RetrievalVui lòng cung cấp thêm thông tin về cha của Menucha Rochel Slonim để chúng tôi có thể 
trả lời câu hỏi.
SearChain
w/o Retrieval[Query 1]: Ai là cha của Menucha Rochel Slonim?
[Answer 1]: Rabbi Dovber Schneuri, Rebbe thứ hai của phong trào Chabad Lubavitch.
[Query 2]: Ai đã kế thừa Rabbi Dovber Schneuri làm Rebbe?
[Answer 2]: Con trai ông, Rabbi Menachem Mendel Schneersohn.
[Final Content]: Cha của Menucha Rochel Slonim là Rabbi Dovber Schneuri [1]. Con trai ông 
Rabbi Menachem Mendel Schneersohn đã kế thừa Rabbi Dovber Schneuri làm Rebbe[2]. 
Vậy câu trả lời cuối cùng là Richard Stallman. Ai đã theo sau cha của Menucha Rochel Slonim?

Thật không may, không có đủ thông tin được cung cấp trong câu hỏi để trả lời chính xác. CoT
Self-Ask
w/o Retrieval Theo dõi: Tên của quốc gia mới tuyên bố độc lập là gì?
Câu trả lời trung gian: Tên của quốc gia không rõ ràng.
SearChain
w/o 
Retrieval[Query 1]: Những quốc gia nào là thành viên của ủy ban Sự thật và Tình bạn với Inierie? 
[Answer 1]: Đông Timor và Indonesia 
[Query 2]: Quốc gia nào trong số này gần đây tuyên bố độc lập? 
[Answer 2]: Đông Timor 
[Query 3]: Ai là tổng thống của Đông Timor? 
[Answer 3]: Francisco Guterres 
[Final Content]: Đông Timor và Indonesia là thành viên của ủy ban Sự thật và Tình bạn 
với Inierie [1]. Đông Timor gần đây tuyên bố độc lập [2] và tổng thống của nó là 
Francisco Guterres . Vậy câu trả lời cuối cùng là Francisco Guterres.  Ai là tổng thống của quốc gia mới tuyên bố độc lập là thành viên của ủy ban Sự thật và 
Tình bạn với quốc gia nơi Inierie tọa lạc?

Con của William Beckford, một cô con gái tên Maria Hamilton, thông tin về cái chết 
của Maria Hamilton không rõ ràng CoT
Self-Ask
w/o RetrievalTheo dõi: Tên con của William Beckford là gì?
Câu trả lời trung gian: Tên con của William Beckford không được chỉ định trong câu hỏi.
SearChain
w/o Retrieval[Query 1]: William Beckford là ai?
[Answer 2]: William Beckford là tiểu thuyết gia, nhà sưu tập nghệ thuật và chính trị gia người Anh.
[Query 2]: William Beckford có con không?
[Answer 2]: Có, William Beckford có một đứa con, một cậu con trai tên William Thomas Beckford.
[Query 3]: William Thomas Beckford sống ở đâu?
[Answer 3]: William Thomas Beckford sống ở nhiều nơi khác nhau ở Anh, bao gồm Bath 
và Fonthill Abbey.
[Query 4]: Con của William Beckford chết ở đâu?
[Answer 4]: Con của William Thomas Beckford chết ở Bath.
[Final Content]: William Beckford là tiểu thuyết gia, nhà sưu tập nghệ thuật và chính trị gia người Anh [1]. 
William Beckford có một đứa con, một cậu con trai tên William Thomas Beckford [2]. William 
Thomas Beckford sống ở nhiều nơi khác nhau ở Anh, bao gồm Bath và Fonthill Abbey và 
chết ở Bath [3,4]. Vậy câu trả lời cuối cùng là Bath. Con của William Beckford chết ở đâu?

Chủ sở hữu của hãng thu âm của người biểu diễn "Groovy Little Summer Song" không 
được chỉ định. Cần thêm thông tin để trả lời câu hỏi này.CoT
Self-Ask
w/o RetrievalTheo dõi: Ai là người biểu diễn Groovy Little Summer Song?
Câu trả lời trung gian: Người biểu diễn Groovy Little Summer Song không được chỉ định.
SearChain
w/o Retrieval[Query 1]: Tên người biểu diễn Groovy Little Summer Song là gì?
[Answer 1]: Tôi không biết.
[Query 2]: Bạn có thể tìm ra tên người biểu diễn Groovy Little Summer Song không?
[Answer 2]: Người biểu diễn Groovy Little Summer Song là James Otto.
[Query 3]: Tên hãng thu âm mà James Otto ký hợp đồng là gì?
[Answer 3]: Warner Music Nashville là hãng thu âm mà James Otto ký hợp đồng.
[Query 4]: Ai là chủ sở hữu của Warner Music Nashville?
[Answer 4]: Warner Music Nashville là công ty con của Warner Music Group, vì vậy chủ sở hữu 
sẽ là Warner Music Group.
[Final Content]: Người biểu diễn Groovy Little Summer Song là James Otto [2]. Warner 
Music Nashville là hãng thu âm mà James Otto ký hợp đồng [3]. Chủ sở hữu của Warner 
Music Nashville là Warner Music Group [4]. Vậy câu trả lời cuối cùng là Warner Music Group .Ai là chủ sở hữu của hãng thu âm của người biểu diễn Groovy Little Summer Song?

Hình 6: Nghiên cứu trường hợp cho CoQ vs Baseline trong Lập luận.

cách. Trong khi các tham chiếu được đưa ra bởi New Bing không bao phủ tất cả kiến thức, và trong một số trường hợp New Bing không thể tìm thấy kiến thức. SearChain cung cấp một góc nhìn mới phân tách các tác vụ phức tạp nhiều bước chuyên sâu về kiến thức thành nhiều vấn đề lập luận kiến thức một bước, truy xuất tài liệu hỗ trợ của kiến thức cho mỗi bước lập luận, và tổ chức các bước lập luận này với các dấu tham chiếu của chúng như nội dung được tạo cuối cùng. Điều này cho phép các tài liệu hỗ trợ bao phủ mọi kiến thức liên quan trong nội dung được tạo, điều này tăng cường khả năng truy vết của nội dung được tạo.

--- TRANG 10 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

Bảng 6: Hiệu suất của SearChain và DSP trên các tác vụ phức tạp chuyên sâu về kiến thức trên Vicuna-13B. In đậm biểu thị kết quả tốt nhất trong các cài đặt khác nhau. FC: Kiểm tra Sự thật, LFQA: Hỏi đáp Dạng dài. Chỉ số cho LFQA: ROUGE-L. Chỉ số cho các tác vụ khác: cover-EM.

Hỏi đáp Nhiều bước Điền Slot FC LFQA
HoPo MQ WQA SQA zsRE T-REx FEV. ELI5
Tương tác với Truy xuất Thông tin
DSP 25.45 9.06 27.50 62.01 33.71 49.08 73.05 22.58
SearChain 29.77 10.59 32.32 63.75 36.86 52.75 75.47 24.05

      SearChain
Joan Crawford đóng vai chính trong The Duke Steps Out [1], được phát hành vào ngày 14 tháng 5, 1929 [2]. Viện Điện ảnh Mỹ (AFI) xếp hạng những ngôi sao nữ Hollywood vĩ đại nhất năm 1999 [3], và top năm là Katharine Hepburn, Bette Davis, Audrey Hepburn, Ingrid Bergman, và Greta Garbo [4]. Joan Crawford được xếp hạng thứ mười trong danh sách [5]. Vậy câu trả lời là 1999.
[1] .. The Duke Steps Out The Duke Steps Out được đạo diễn bởi James Cruze và có sự tham gia của William Haines và Joan Crawford …
[2] … The Duke Steps Out là bộ phim hài kịch-chính kịch câm năm 1929…
[3] … AFI's 100 Years...100 Stars là danh sách top 25 nam và 25 nữ ngôi sao Hollywood … Danh sách được Viện Điện ảnh Mỹ công bố vào ngày 15 tháng 6, 1999 …
[4] … "AFI's 100 Years...100 Stars" … Trong số 25 nữ diễn viên được liệt kê, Davis được xếp hạng thứ hai, sau Katharine Hepburn. Dịch vụ Bưu chính Hoa Kỳ vinh danh Davis với một con tem kỷ niệm năm 2008, đánh dấu kỷ niệm 100 năm sinh của bà …
[5] … Joan Crawford … cũng được bình chọn là ngôi sao nữ vĩ đại thứ mười của điện ảnh cổ điển Mỹ bởi Viện Điện ảnh Mỹ …The Duke Steps Out có sự tham gia của một nữ diễn viên được xếp hạng thứ mười trong danh sách những ngôi sao nữ Hollywood vĩ đại nhất vào năm nào?

      New Bing
Tôi xin lỗi, nhưng tôi không thể tìm thấy bất kỳ 
thông tin nào về nữ diễn viên từ The Duke Steps Out 
được xếp hạng thứ mười trong danh sách những ngôi sao 
nữ Hollywood vĩ đại nhất [1]. Bạn có thể cung cấp 
thêm chi tiết hoặc có thể diễn đạt lại câu hỏi của mình không?
[1]: AFI's 100 Years...100 Stars là danh sách của 
Viện Điện ảnh Mỹ xếp hạng top 25 nam và 25 nữ 
huyền thoại màn ảnh vĩ đại nhất trong lịch sử điện ảnh Mỹ 
và là danh sách thứ hai của loạt AFI 100 Years...

      SearChain
Lorenzo Monaco sống và làm việc từ năm 1370 đến 1425 [1]. Các họa sĩ và kiến trúc sư Ý nổi tiếng trong thời đại của ông bao gồm Giotto, Masaccio, Brunelleschi, và Donatello [2]. Giotto và Masaccio đặc biệt ảnh hưởng đến tác phẩm của ông [2], cũng như Piero di Giovanni ở Siena, Spinello Aretino, và Agnolo Gaddi [2]. Vậy, câu trả lời cho câu hỏi "Lorenzo Monaco chịu ảnh hưởng của họa sĩ và kiến trúc sư Ý nào?" là Giotto, Masaccio, Piero di Giovanni ở Siena, Spinello Aretino, và Agnolo Gaddi.
[1] …Lorenzo Monaco Lorenzo Monaco (sinh ra với tên Piero di Giovanni; 1370 – c. 1425) là họa sĩ Ý của …
[2] Lorenzo là họa sĩ Ý thời kỳ Gothic muộn-Phục hưng sớm. Ông sinh ra với tên Piero di Giovanni ở Siena. Ít được biết về tuổi trẻ của ông, ngoài việc ông được học nghề ở Florence. Ông chịu ảnh hưởng của Giotto và các môn đồ của ông là Spinello Aretino và Agnolo Gaddi. Năm 1390 ông gia nhập tu viện Camaldolese của Santa Maria degli Angeli. Từ đó ông thường được biết đến với tên "Lorenzo Monaco" (tiếng Anh: "Lawrence the Monk"). Trong những năm 1390 ông thực hiện ba tấm của Biblioteca Laurenziana cho tu viện của mình …Lorenzo Monaco chịu ảnh hưởng của họa sĩ và kiến trúc sư Ý nào?

      New Bing
Lorenzo Monaco (c. 1370 – c. 1425) là họa sĩ Ý 
thời kỳ Gothic muộn đến Phục hưng sớm. Ông sinh ra 
với tên Piero di Giovanni ở Siena, Ý và được học nghề 
ở Florence. Ông chịu ảnh hưởng của Giotto và các môn đồ 
của nghệ sĩ đó là Spinello Aretino và Agnolo Gaddi1 [1].
[1]: Lorenzo Monaco Lorenzo Monaco là họa sĩ Ý 
thời kỳ Gothic muộn-Phục hưng sớm. Ông sinh ra với tên 
Piero di Giovanni ở Siena. Ít được biết về tuổi trẻ của ông, 
ngoài việc ông được học nghề ở Florence. Ông chịu ảnh hưởng 
của Giotto và các môn đồ của ông là Spinello Aretino và 
Agnolo Gaddi. Năm 1390 ông gia nhập Camaldolese …

      SearChain
Paul Revere là Yêu nước trong Cách mạng Mỹ [1]. Mark Lindsay là nhạc sĩ, ca sĩ chính của Paul Revere & the Raiders [2]. Bài hát Arizona được thu âm bởi Paul Revere và Mark Lindsay, và được viết bởi Kenny Young [3]. Vậy câu trả lời cuối cùng là Kenny Young.
[1] .. Paul Revere là thợ bạc, thợ khắc, nhà công nghiệp sớm và Yêu nước Mỹ trong Cách mạng Mỹ. ..
[2] … Mark Lindsay (sinh ngày 9 tháng 3, 1942) là nhạc sĩ Mỹ, được biết đến nhiều nhất với tư cách ca sĩ chính của Paul Revere & the Raiders …
[3] … "Arizona" là bài hát được viết bởi Kenny Young và thu âm năm 1969 bởi Mark Lindsay, một nỗ lực solo trong khi vẫn là ca sĩ chính cho Paul Revere and the Raiders …

Bài hát Arizona được thu âm bởi Paul Revere và Mark Lindsay nhưng ai đã viết bài hát?

      New Bing
Bài hát "Arizona" được viết bởi Kenny Young và 
thu âm bởi Mark Lindsay, một nỗ lực solo trong khi 
vẫn là ca sĩ chính cho Paul Revere and the Raiders 
vào năm 1969 [1]. Có điều gì khác bạn muốn biết không?
[1]: … "Arizona" là bài hát được viết bởi Kenny Young 
và thu âm năm 1969 bởi Mark Lindsay, một nỗ lực solo trong khi 
vẫn là ca sĩ chính cho Paul Revere and the Raiders …

     SearChain
The Notorious B.I.G., còn được biết đến với tên Christopher Wallace, là rapper Mỹ [1]. Ông phát hành hai album phòng thu trong đời: "Ready to Die" năm 1994 và "Life After Death" năm 1997 [2]. Bài hát "Unbelievable" được đưa vào album đầu tay "Ready to Die [3],"được phát hành vào ngày 13 tháng 9, 1994 [4]. Vậy câu trả lời là 13 tháng 9, 1994 .
[1] … The Notorious B.I.G. Christopher George Latore Wallace (21 tháng 5, 1972 – 9 tháng 3, 1997), được biết đến chuyên nghiệp với tên The Notorious B.I.G., Biggie Smalls, hoặc Biggie, là rapper Mỹ…
[2] … Life After Death Life After Death là album phòng thu thứ hai và cuối cùng của rapper Mỹ The Notorious B.I.G., phát hành vào ngày 25 tháng 3, 1997 … The Notorious B.I.G. tiếp tục đi sâu vào thể loại mafioso rap. Album là phần tiếp theo của album đầu tiên của anh, "Ready to Die" …
[3] … Ready to Die … Các bài hát trong album dao động từ tường thuật giết người ("Warning") đến battle rap khoe khoang ("The What," "Unbelievable") …
[4] … Ready to Die | Ready to Die Ready to Die là album phòng thu đầu tay của rapper Mỹ The Notorious B.I.G., phát hành vào ngày 13 tháng 9, 1994 …Khi nào album có bài hát Unbelievable của rapper Mỹ The Notorious B.I.G được phát hành?

      New Bing
Bài hát "Unbelievable" của rapper Mỹ The Notorious 
B.I.G. được phát hành trong album phòng thu đầu tay 
Ready to Die vào ngày 13 tháng 9, 1994 bởi Bad Boy Records 
và Arista Records [1]. Có điều gì khác bạn muốn biết không?
[1] … Ready to Die | Ready to Die Ready to Die là 
album phòng thu đầu tay của rapper Mỹ The Notorious 
B.I.G., phát hành vào ngày 13 tháng 9, 1994 …

Hình 7: Nghiên cứu trường hợp cho SearChain vs New Bing trong Truy vết.

Bảng 8: Số ví dụ trong prompt được sử dụng cho học trong ngữ cảnh trên các bộ dữ liệu khác nhau.

Hỏi đáp Nhiều bước Điền Slot FC LFQA
HoPo MQ WQA SQA zsRE T-REx FEV. ELI5
Không có Truy xuất Thông tin
Nhắc Trực tiếp 0 0 0 0 0 0 0 0
Auto-CoT 4 4 4 6 4 4 4 2
CoT 4 4 4 6 4 4 4 2
CoT-SC 4 4 4 6 4 4 4 2
Recite-and-answer 4 4 4 6 4 4 4 2
Self-Ask w/o IR 4 4 4 6 4 4 4 2
Least-to-Most 4 4 4 6 4 4 4 2
Plan-and-Solve 4 4 4 6 4 4 4 2
SearChain w/o IR 2 2 2 6 2 2 4 2
Tương tác với Truy xuất Thông tin
Truy xuất Trực tiếp 0 0 0 0 0 0 0 0
ToolFormer 4 4 4 6 4 4 4 2
Self-Ask 4 4 4 6 4 4 4 2
Plan-and-Solve w/ IR 4 4 4 6 4 4 4 2
React→CoT-SC 6 4 4 6 4 4 4 2
Verify-and-Edit 2 2 2 2 2 2 4 2
Tree-of-Thought w/ IR 4 4 4 6 4 4 4 2
DSP 16 8 8 8 8 8 8 2
SearChain 2 2 2 2 2 2 4 2

Bảng 7: Thay đổi hiệu suất với ngưỡng ROUGE.
𝛼=0.30 𝛼=0.35 𝛼=0.40 𝛼=0.45 𝛼=0.50
Hiệu suất 25.50 25.57 25.58 25.57 25.55

A.2 Hiệu suất trên Vicuna-13B
Trong phần này, chúng tôi so sánh SearChain với baseline cạnh tranh DSP trên Vicuna-13B5, một mô hình lớn mã nguồn mở mạnh6 được huấn luyện bởi Stanford. Kết quả thí nghiệm trong Bảng 6 cho thấy SearChain một lần nữa vượt trội hơn DSP trên Vicuna-13B.

A.3 Chi tiết Thí nghiệm
A.3.1 Lựa chọn Ngưỡng. Về ngưỡng độ tin cậy ( 𝜃), chúng tôi khởi tạo giá trị ban đầu của ngưỡng độ tin cậy (1.0) dựa trên kiến thức trước và dần dần tăng giá trị với bước nhảy 0.1. Chúng tôi xác thực F1-score (một chỉ số toàn diện của

--- TRANG 11 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

Xây dựng một chuỗi lập luận toàn cục cho [Câu hỏi] phức tạp này : " {} " Bạn nên tạo ra một truy vấn cho công cụ tìm kiếm dựa 
trên những gì bạn đã biết ở mỗi bước của chuỗi lập luận, bắt đầu với [Query]. 
Nếu bạn biết câu trả lời cho [Query], hãy tạo ra nó bắt đầu với [Answer].
Bạn có thể thử tạo ra câu trả lời cuối cùng cho [Câu hỏi] bằng cách tham khảo các cặp [Query] -[Answer], bắt đầu với [Final 
Content].
Nếu bạn không biết câu trả lời, hãy tạo ra một truy vấn cho công cụ tìm kiếm dựa trên những gì bạn đã biết và không biết, bắt đầu với
[Unsolved Query].
Ví dụ:
[Câu hỏi]:"Có phổ biến khi thấy sương giá trong một số lễ tốt nghiệp đại học không?"
[Query 1]: Những mùa nào bạn có thể mong đợi thấy sương giá?Nếu bạn không biết câu trả lời:
[Unsolved Query]: Những mùa nào bạn có thể mong đợi thấy sương giá? Hướng dẫn: Vui lòng Dừng sinh ra của bạn.
Nếu bạn biết câu trả lời:
[Answer 1]: Mùa đông.
[Query 2]: Lễ tốt nghiệp đại học diễn ra vào những tháng nào?
Nếu bạn không biết câu trả lời:
[Unsolved Query]: Lễ tốt nghiệp đại học diễn ra vào những tháng nào?
Hướng dẫn: Vui lòng Dừng sinh ra của bạn.Nếu bạn biết câu trả lời:
[Answer 2]: Tháng 12, tháng 5, và đôi khi tháng 6.
[Query 3]: Có tháng nào trong số tháng 12, tháng 5, và đôi khi tháng 6 xảy ra trong mùa đông không?
Nếu bạn không biết câu trả lời:
[Unsolved Query]: Có tháng nào trong số tháng 12, tháng 5, và đôi khi tháng 6 xảy ra trong mùa đông không?Nếu bạn biết câu trả lời:
[Answer 3]: Tháng 12
Hướng dẫn: Vui lòng Dừng sinh ra của bạn.
Nếu bạn biết câu trả lời:
[Câu hỏi]:"Có phổ biến khi thấy sương giá trong một số lễ tốt nghiệp đại học không?" (Câu trả lời chỉ có thể là "Có" hoặc "Không")
[Final Content]: Bạn mong đợi thấy sương giá vào Mùa đông [1]. Lễ tốt nghiệp đại học diễn ra vào tháng 12, tháng 5, và đôi khi tháng 6 [2]. 
Tháng 12, tháng 5, và đôi khi tháng 6 xảy ra trong mùa đông [3]. Vậy câu trả lời cuối cùng là Có.
[Câu hỏi]:"Một quả lê có chìm trong nước không?"
[Query 1]: Mật độ của một quả lê là bao nhiêu?
[Answer 1]: 0.59 g/cm^3
[Query 2]: Mật độ của nước là bao nhiêu?
[Answer 2]: 1 g/cm^3
[Query 3]: 0.59 g/cm^3 có lớn hơn g/cm^3 không?
[Answer 3]: Không [Câu hỏi]:"Một quả lê có chìm trong nước không?" (Có hoặc Không)
[Final Content]: Mật độ của một quả lê là 0.59 g/cm^3 [1]. Mật độ của nước là 1 g/cm^3 [2]. 0.59 g/cm^3 không lớn hơn
g/cm^3 [3]. Vậy câu trả lời cuối cùng là Không.

Hình 10: Prompt để tạo ra Chain-of-Query ở vòng đầu tiên trên StragegyQA (trong cài đặt có truy xuất thông tin).

Xây dựng một chuỗi lập luận toàn cục cho [Câu hỏi] phức tạp này : " {} " Bạn nên tạo ra một truy vấn cho công cụ tìm kiếm dựa 
trên những gì bạn đã biết ở mỗi bước của chuỗi lập luận, bắt đầu với [Query]. 
Nếu bạn biết câu trả lời cho [Query], hãy tạo ra nó bắt đầu với [Answer].
Bạn có thể thử tạo ra câu trả lời cuối cùng cho [Câu hỏi] bằng cách tham khảo các cặp [Query] -[Answer], bắt đầu với [Final 
Content].
Nếu bạn không biết câu trả lời, hãy tạo ra một truy vấn cho công cụ tìm kiếm dựa trên những gì bạn đã biết và không biết, bắt đầu với
[Unsolved Query].
Ví dụ:
[Câu hỏi]: "Xe buýt greyhound ở nơi sinh của người biểu diễn Spirit If... khởi hành từ đâu? "
[Query 1]: Ai là người biểu diễn Spirit If... ?Nếu bạn không biết câu trả lời:
[Unsolved Query]: Ai là người biểu diễn Spirit If... ?Nếu bạn biết câu trả lời:
[Answer 1]: Người biểu diễn Spirit If… là Kevin Drew.
[Query 2]: Kevin Drew sinh ra ở đâu?
Nếu bạn không biết câu trả lời:
[Unsolved Query]: Kevin Drew sinh ra ở đâu?
Nếu bạn biết câu trả lời:
[Answer 2]: Toronto.[Query 3]: Xe buýt greyhound ở Toronto khởi hành từ đâu?
Nếu bạn không biết câu trả lời:
[Unsolved Query]: Xe buýt greyhound ở Toronto khởi hành từ đâu?
Nếu bạn biết câu trả lời:
[Answer 3]: Toronto Coach Terminal.[Final Content]: Người biểu diễn Spirit If… là Kevin Drew [1]. Kevin Drew sinh ra ở Toronto [2]. Xe buýt Greyhound ở 
Toronto khởi hành từ Toronto 
Coach Terminal [3]. Vậy câu trả lời cuối cùng là Toronto Coach Terminal.
[Câu hỏi]:"Tạp chí nào được khởi động trước Arthur's Magazine hay First for Women?"
[Query 1]: Arthur's Magazine được khởi động khi nào?
[Answer 1]: 1844.[Query 2]: First for Women được khởi động khi nào?
[Answer 2]: 1989
[Final Content]: Arthur's Magazine khởi động năm 1844 [1]. First for Women khởi động năm 1989 [2]. Vậy Arthur's Magazine được khởi động
trước. Vậy câu trả lời là Arthur's Magazine.

Hình 11: Prompt để tạo ra Chain-of-Query ở vòng đầu tiên trên HotpotQA, Musique, WikiMultiHopQA, zsRE và T-REx (trong cài đặt có truy xuất thông tin).

Xây dựng một chuỗi lập luận toàn cục cho [Câu hỏi] phức tạp này : " {} " Bạn nên tạo ra một truy vấn cho công cụ tìm kiếm dựa 
trên những gì bạn đã biết ở mỗi bước của chuỗi lập luận, bắt đầu với [Query]. 
Nếu bạn biết câu trả lời cho [Query], hãy tạo ra nó bắt đầu với [Answer].
Bạn có thể thử tạo ra câu trả lời cuối cùng cho [Câu hỏi] bằng cách tham khảo các cặp [Query] -[Answer], bắt đầu với [Final 
Content].
Nếu bạn không biết câu trả lời, hãy tạo ra một truy vấn cho công cụ tìm kiếm dựa trên những gì bạn đã biết và không biết, bắt đầu với
[Unsolved Query].
Ví dụ:
[Câu hỏi]:"Điều gì gây ra vệt sau máy bay phản lực ở độ cao lớn?"
[Query 1]: Vệt sau máy bay phản lực ở độ cao lớn là gì?Nếu bạn không biết câu trả lời:
[Unsolved Query]: Vệt sau máy bay phản lực ở độ cao lớn là gì?Nếu bạn biết câu trả lời:
[Answer 1]: Vệt sau máy bay phản lực ở độ cao lớn thường được gọi là contrail, viết tắt của "condensation trail." Nó được hình thành khi khí thải nóng từ động cơ phản lực trộn với không khí lạnh hơn ở độ cao lớn, khiến hơi nước trong không khí ngưng tụ và đóng băng thành các tinh thể băng nhỏ.
[Query 2]: Tại sao khí thải nóng trộn với không khí lạnh hơn ở độ cao lớn?
Nếu bạn không biết câu trả lời:
[Unsolved Query]: Tại sao khí thải nóng trộn với không khí lạnh hơn ở độ cao lớn?Nếu bạn biết câu trả lời:
[Answer 2]: Khí thải nóng từ động cơ phản lực trộn với không khí lạnh hơn ở độ cao lớn do sự chênh lệch áp suất giữa khí thải động cơ và không khí xung quanh. Ở độ cao lớn, không khí lạnh hơn và loãng hơn nhiều, khiến khí thải giãn nở và nguội đi nhanh chóng, tạo ra một dòng hỗn loạn phía sau máy bay.
[Query 3]: Tại sao hơi nước trong không khí ngưng tụ và đóng băng thành tinh thể băng?Nếu bạn không biết câu trả lời:
[Unsolved Query]: Tại sao hơi nước trong không khí ngưng tụ và đóng băng thành tinh thể băng?
Nếu bạn biết câu trả lời:
[Answer 3]: Hơi nước trong không khí ngưng tụ và đóng băng thành tinh thể băng vì khí thải từ động cơ phản lực chứa một lượng đáng kể hơi nước, được đun nóng đến nhiệt độ cao trong động cơ. Khi hơi nước nóng trộn với không khí lạnh ở độ cao lớn, nó nhanh chóng mất nhiệt và năng lượng, khiến hơi nước ngưng tụ thành giọt nước lỏng và sau đó đóng băng thành tinh thể băng.
[Final Content]: Vệt sau máy bay phản lực ở độ cao lớn được hình thành khi khí thải nóng từ động cơ phản lực trộn với không khí lạnh hơn ở độ cao lớn, khiến hơi nước trong không khí ngưng tụ và đóng băng thành các tinh thể băng nhỏ. Khí thải trộn với không khí lạnh hơn do sự chênh lệch áp suất giữa khí thải động cơ và không khí xung quanh, và hơi nước ngưng tụ và đóng băng vì nó được đun nóng đến nhiệt độ cao trong động cơ và sau đó nhanh chóng mất nhiệt và năng lượng khi trộn với không khí lạnh ở độ cao lớn.                       
[Câu hỏi]: Trong Trading Places (1983, Akroyd/Murphy) kế hoạch ở cuối phim hoạt động như thế nào? Tại sao việc mua nhiều OJ với giá cao lại làm phá sản Anh em Duke?[Query 1]: Kế hoạch ở cuối Trading Places là gì?
[Answer 1]: Trong phim, nhân vật chính, Billy Ray Valentine (Eddie Murphy), và đối tác của anh, Louis Winthorpe III (Dan Aykroyd), thực hiện một kế hoạch để làm phá sản Anh em Duke bằng cách thao túng thị trường hàng hóa tương lai nước cam cô đặc đông lạnh (FCOJ).[Query 2]: Billy Ray và Louis thao túng thị trường hàng hóa tương lai FCOJ như thế nào?
[Answer 2]: Billy Ray và Louis có được thông tin nội bộ về báo cáo mùa vụ sắp tới của Bộ Nông nghiệp, cho thấy một mùa đông khắc nghiệt đã phá hủy phần lớn vụ cam. Họ sử dụng thông tin này để mua hợp đồng tương lai FCOJ với giá thấp trước khi báo cáo được phát hành, và sau đó họ bán các hợp đồng với giá cao sau khi báo cáo được phát hành, khi thị trường đã phản ứng với tin tức về thiệt hại mùa vụ.[Query 3]: Tại sao việc mua nhiều OJ với giá cao lại làm phá sản Anh em Duke?
[Answer 3]: Anh em Duke, những người cũng đầu tư vào thị trường FCOJ, đã đặt cược rằng vụ cam sẽ dồi dào và giá FCOJ sẽ giữ ở mức thấp. Tuy nhiên, kế hoạch của Billy Ray và Louis đẩy giá FCOJ lên cao, khiến Anh em Duke mất một số tiền đáng kể và cuối cùng dẫn đến sự sụp đổ của họ.
[Final Content]: Trong Trading Places, Billy Ray Valentine và Louis Winthorpe III thao túng thị trường hàng hóa tương lai FCOJ bằng cách có được thông tin nội bộ về báo cáo mùa vụ và mua hợp đồng với giá thấp trước khi bán chúng với giá cao hơn sau khi báo cáo được phát hành. Anh em Duke, những người cũng đã đầu tư vào thị trường, mất tiền vì họ đặt cược vào vụ cam dồi dào và giá FCOJ thấp. Tuy nhiên, kế hoạch của Billy Ray và Louis khiến giá FCOJ tăng, điều này làm phá sản Anh em Duke và dẫn đến sự sụp đổ của họ.

Hình 12: Prompt để tạo ra Chain-of-Query ở vòng đầu tiên trên ELI5 (trong cài đặt có truy xuất thông tin).

Xây dựng một chuỗi lập luận toàn cục cho câu hỏi phức tạp [Câu hỏi]:"{}" và trả lời câu hỏi, và tạo ra một truy vấn cho 
công cụ tìm kiếm dựa trên những gì bạn đã biết ở mỗi bước của chuỗi lập luận, bắt đầu với [Query]. 
Bạn nên tạo ra câu trả lời cho mỗi [Query], bắt đầu với [Answer].
Bạn nên tạo ra câu trả lời cuối cùng cho [Câu hỏi] bằng cách tham khảo các cặp [Query] -[Answer], bắt đầu với [Final Content].
Ví dụ:
[Câu hỏi]:"Có bao nhiêu cơ sở giáo dục đại học trong thành phố nơi hoàng đế Yongle chào đón người mà sắc lệnh được gửi đến?"
[Query 1]: Sắc lệnh được gửi đến ai?
[Answer 1]: Karmapa
[Query 2]: Hoàng đế Yongle chào đón Karmapa ở đâu?[Answer 2]: Nanjing
[Query 3]: Có bao nhiêu cơ sở giáo dục đại học ở Nanjing?
[Answer 3]: 75 
[Final Content]: Sắc lệnh được gửi đến Karmapa [1]. Hoàng đế Yongle chào đón Karampa ở Nanjing [2]. Có 75 cơ sở giáo dục đại học ở Nanjing [3]. Vậy câu trả lời cuối cùng là 75.
[Câu hỏi]:"Tạp chí nào được khởi động trước Arthur's Magazine hay First for Women?"
[Query 1]: Arthur's Magazine được khởi động khi nào?
[Answer 1]: 1844.
[Query 2]: First for Women được khởi động khi nào?[Answer 2]: 1989
[Final Content]: Arthur's Magazine khởi động năm 1844 [1]. First for Women khởi động năm 1989 [2]. Vậy Arthur's Magazine được khởi động
trước. Vậy câu trả lời cuối cùng là Arthur's Magazine .

Hình 8: Prompt để tạo ra Chain-of-Query trên HotpotQA, Musique, WikiMultiHopQA, zsRE và T-REx (trong cài đặt không có truy xuất thông tin).

--- TRANG 12 ---
WWW '24, 13–17 tháng 5, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, và Tat-Seng Chua

Xây dựng một chuỗi lập luận toàn cục cho câu hỏi phức tạp [Câu hỏi]:"{}" và trả lời câu hỏi, và tạo ra một truy vấn cho
công cụ tìm kiếm dựa trên những gì bạn đã biết ở mỗi bước của chuỗi lập luận, bắt đầu với [Query]. Bạn nên tạo ra câu trả lời cho mỗi [Query], bắt đầu với [Answer].
Bạn nên tạo ra câu trả lời cuối cùng cho [Câu hỏi] bằng cách tham khảo các cặp [Query] -[Answer], bắt đầu với [Final Content].
Nếu bạn không biết câu trả lời, hãy tạo ra một truy vấn cho công cụ tìm kiếm dựa trên những gì bạn đã biết và không biết, bắt đầu 
với [Unsolved Query] và vui lòng dừng sinh ra của bạn.
Ví dụ:
[Câu hỏi]:"Có bao nhiêu cơ sở giáo dục đại học trong thành phố nơi hoàng đế Yongle chào đón người mà sắc lệnh được gửi đến?"[Query 1]: Sắc lệnh được gửi đến ai?
Nếu bạn không biết câu trả lời:[Unsolved Query]: Sắc lệnh được gửi đến ai? 
Nếu bạn biết câu trả lời:[Answer 1]: Karmapa
[Query 2]: Hoàng đế Yongle chào đón Karmapa ở đâu?
Nếu bạn không biết câu trả lời:[Unsolved Query]: Hoàng đế Yongle chào đón Karmapa ở đâu?
Nếu bạn biết câu trả lời:[Answer 2]: Nanjing
[Query 3]: Có bao nhiêu cơ sở giáo dục đại học ở Nanjing ?Nếu bạn không biết câu trả lời:
[Unsolved Query]: Có bao nhiêu cơ sở giáo dục đại học ở Nanjing?
Nếu bạn biết câu trả lời:
[Answer 3]: 75  
[Final Content]: Sắc lệnh được gửi đến Karmapa [1]. Hoàng đế Yongle chào đón Karampa ở Nanjing [2]. Có 75 cơ sở giáo dục đại học ở Nanjing [3]. Vậy câu trả lời cuối cùng là 75.
[Câu hỏi]:"Nicholas Brody là nhân vật trong Homeland. (HỖ TRỢ hay BÁC BỎ)?"
[Query 1]: Homeland là gì?
[Answer 1]: Homeland là một bộ phim truyền hình.
[Query 2]: Nicholas Brody có phải là nhân vật trong Homeland không?
[Answer 2]: Có.[Final Content]: Homeland là một bộ phim truyền hình [1]. Nicholas Brody là nhân vật trong Homeland [2]. Vậy câu trả lời cuối cùng là 
HỖ TRỢ.
[Câu hỏi]:"Brad Wilk đã giúp đồng sáng lập Rage năm 1962. (HỖ TRỢ hay BÁC BỎ)?"
[Query 1]: Brad Wilk có đồng sáng lập Rage không?
[Answer 1]: Có
[Query 2]: Brad Wilk có đồng sáng lập Rage năm 1962 không?
[Answer 2]: Không, Rage được thành lập năm 1991
[Final Content]: Brad Wilk đã đồng sáng lập Rage [1], nhưng không phải năm 1962 [2]. Vậy câu trả lời cuối cùng là BÁC BỎ.
[Câu hỏi]:"Aristotle đã dành thời gian ở Athens. (HỖ TRỢ hay BÁC BỎ)?"
[Query 1]: Aristotle là ai?[Answer 1]: Aristotle là triết gia Hy Lạp.
[Query 2]: Aristotle có dành thời gian ở Athens không?
[Answer 2]: Có, Aristotle đã học và dạy tại Học viện ở Athens trong 20 năm.
[Final Content]: Aristotle là triết gia Hy Lạp đã học và dạy tại Học viện ở Athens trong 20 năm [2]. Vậy câu trả lời cuối cùng là HỖ TRỢ.
[Câu hỏi]:"Telemundo là mạng truyền hình tiếng Anh. (HỖ TRỢ hay BÁC BỎ)?"
[Query 1]: Telemundo là gì?
[Answer 1]: Telemundo là một mạng truyền hình.
[Query 2]: Telemundo có phải là mạng truyền hình tiếng Anh không?
[Answer 2]: Không, Telemundo là mạng truyền hình tiếng Tây Ban Nha.
[Final Content]: Telemundo là một mạng truyền hình [1], nhưng nó không phải là mạng truyền hình tiếng Anh [2]. Vậy câu trả lời cuối cùng là BÁC BỎ.                        

Hình 9: Prompt để tạo ra Chain-of-Query ở vòng đầu tiên trên FEVER (trong cài đặt có truy xuất thông tin).

Recall và Precision của việc đánh giá liệu đoạn văn có thể trả lời câu hỏi hay không) trên các bộ dữ liệu QA miền mở hỗn hợp (NQ, TriviaQA, WebQ, và TREC) sau mỗi lần thay đổi giá trị. Chúng tôi thấy rằng khi ngưỡng độ tin cậy là 1.5, F1-score cao nhất có thể đạt được nên chúng tôi đặt ngưỡng độ tin cậy là 1.5. Về ngưỡng ROUGE (𝛼), chúng tôi xác định giá trị này bằng cách quan sát thủ công mối quan hệ ROUGE giữa văn bản được tạo và câu trả lời thật trong một vài ví dụ trong học ngữ cảnh. Các thí nghiệm tiếp theo của chúng tôi trong Bảng 7 cho thấy rằng khi khoảng giá trị của ngưỡng ROUGE nằm giữa 0.3 và 0.5, sự thay đổi hiệu suất trên ELI5 không rõ ràng.

A.3.2 Số Ví dụ trong Prompt. Chúng tôi hiển thị số ví dụ trong prompt được sử dụng cho học trong ngữ cảnh trên các bộ dữ liệu khác nhau (Bảng 8). Phương pháp của chúng tôi (SearChain) đạt được hiệu suất tốt nhất với ít ví dụ học hơn so với các baseline cạnh tranh.

A.3.3 Prompt trong Thí nghiệm. Chúng tôi hiển thị prompt được sử dụng trong thí nghiệm trên các bộ dữ liệu khác nhau trong Hình 8 ∼12.

5https://lmsys.org/blog/2023-03-30-vicuna/
6https://huggingface.co/lmsys/vicuna-13b-delta-v1.1/tree/main
