# 2407.12363.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2407.12363.pdf
# File size: 636164 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Conversational Query Reformulation with the Guidance of Retrieved
Documents
Jeonghyun Park andHwanhee Leeâ€ 
Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea
{tom0365, hwanheelee}@cau.ac.kr
Abstract
Conversational search seeks to retrieve rele-
vant passages for the given questions in con-
versational question answering. Conversational
Query Reformulation (CQR) improves conver-
sational search by refining the original queries
into de-contextualized forms to resolve the is-
sues in the original queries, such as omissions
and coreferences. Previous CQR methods fo-
cus on imitating human written queries which
may not always yield meaningful search results
for the retriever. In this paper, we introduce
GuideCQR , a framework that refines queries
for CQR by leveraging key information from
the initially retrieved documents. Specifically,
GuideCQR extracts keywords and generates ex-
pected answers from the retrieved documents,
then unifies them with the queries after filter-
ing to add useful information that enhances the
search process. Experimental results demon-
strate that our proposed method achieves state-
of-the-art performance across multiple datasets,
outperforming previous CQR methods. Addi-
tionally, we show that GuideCQR can get ad-
ditional performance gains in conversational
search using various types of queries, even for
queries written by humans.1
1 Introduction
In a conversational question-answering task (Con-
vQA), conversational search aims to retrieve rel-
evant passages that provide the necessary infor-
mation to answer the current query. This process
occurs within the framework of a multi-turn con-
versation, where each query builds upon the pre-
vious interactions. The questions in ConvQA of-
ten involve challenges like omissions and corefer-
ences, which make it difficult to achieve the desired
search results using the original query. Previous
0â€ Corresponding author.
1The code and pre-trained model will be publicly released
upon the acceptance of the paper.
Figure 1: Example queries for ConvQA where the re-
formulated query achieves a higher MRR score by ef-
fectively extracting clues from the initially retrieved
documents, compared to the base query.
research focuses on transforming queries into stand-
alone forms to understand their intent better, mak-
ing them more independent and robust (Mo et al.,
2023a). This process, known as Conversational
Query Reformulation (CQR), helps clarify the orig-
inal queries and enhances query understanding.
With the advent of Large Language Mod-
els (LLMs), their application in CQR methods
has become increasingly widespread. A recent
study (Jagerman et al., 2023) involves instructing
an LLM to generate relevant passages related to
the query. This method makes the query more rea-
sonable and human-understandable by expanding
the sentence. LLM4CS (Mao et al., 2023) also in-
volves using LLMs to rewrite the query through
different prompting methods and aggregation tech-
niques. However, although these approaches aim to
create human-friendly and easily comprehensible
queries, they may not always yield the effective re-
sults that retrievers desire. For instance, as shown at
the bottom of Figure 1, although the reformulated
query may seem less fluent due to the additional
keywords, it achieves a higher retriever score than a
base query because of its increased similarity to the
ground-truth document. This example highlights
the necessity of prioritizing retriever-optimized
1arXiv:2407.12363v5  [cs.CL]  15 May 2025

--- PAGE 2 ---
queries over human-friendly ones to enhance CQR
methods. To achieve this, we observe that the ini-
tially retrieved documents from a retrieval process
with a base query set can help the generation of
retrieval-optimized queries. Specifically, we find
that although some of the documents cannot pro-
vide clues for answering the questions, most of the
retrieved documents contain words or signals that
are highly influential in subsequent retrieval pro-
cesses. For instance, in the example illustrated in
Figure 1, the addition of the terms â€™interchange-
ableâ€™ , an important keyword found in the gold pas-
sage within the guided documents, enhances the
performance that the original query alone cannot
achieve. In this way, signals in the initially re-
trieved documents obtained through a base query
can guide the search for the gold passage.
In this paper, we propose GuideCQR , an effec-
tive CQR framework designed to generate retrieval-
friendly conversational queries by leveraging the
guidance of initially retrieved documents from the
base query. GuideCQR first obtains the initially re-
trieved documents through a retrieval process using
the baseline query reformulated by LLM. We then
perform a re-ranking process to refine the order of
the retrieved documents based on their similarity
to the query. Through this re-ranking process, we
carefully select a small number of documents from
the initially retrieved ones to extract signals that
are more likely to contain the gold passage. Next,
we extract the keywords and generate the expected
answer from the re-ranked guided documents to
obtain components for creating a retriever-friendly
query. We then independently filter both the key-
words and the expected answers based on their
similarity to the original query and previous utter-
ances to eliminate redundant information. Finally,
we construct the final query set by concatenating
the filtered keywords and expected answers with
the baseline query.
Experimental results show that GuideCQR
achieves state-of-the-art across several CQR
datasets compared to the baseline systems. We
also demonstrate the robustness of the GuideCQR
framework in enhancing the retrievability of
queries, making them more retriever-friendly. Fur-
thermore, we validate the effectiveness of our
method by analyzing the overlap between the aug-
mented keywords and the relevant documents for
the given question. Additionally, we show that the
proposed method achieves notable improvements
across various query sets, underscoring its adapt-ability.
2 Related Works
2.1 Conversational Search
The goal of conversational search (Gao et al., 2022)
is to retrieve passages containing the necessary in-
formation to answer the current query within a
multi-turn conversation. To get the desired answers
from the given conversational dialogue, understand-
ing the meaning of the query is important in this
field (Mo et al., 2023b). However, queries in Con-
vQA have several problems. For instance, queries
may include pronouns such as "he," "she," or "it,"
necessitating the identification of the referred en-
tity, and there is also the challenge of omission,
where essential information is not included (Mao
et al., 2023; Wang et al., 2023).
To address such challenges in conversational
search, two main methods that enable conversa-
tional search are Conversational Dense Retrieval
(CDR) and CQR. CDR focuses on enhancing the
representation of the current query by incorporating
historical context achieved through training dense
retrievers (Qu et al., 2020). On the other hand,
CQR transforms the conversational search into a
traditional ad-hoc search by converting the entire
search session into a single standalone query (El-
gohary et al., 2019).
In our work, we focus on utilizing the dialogue
history to contextualize query rewrites. Addition-
ally, we employ the multi-turn dialogue as a crite-
rion for filtering keywords and answers, which are
essential components of GuideCQR .
2.2 Conversational Query Reformulation
CQR methods aim to transform queries into de-
contextualized forms, enabling them to be under-
stood based solely on their content and ensur-
ing they convey the intended meaning effectively.
These methods are designed to enhance conversa-
tional search performance by refining and expand-
ing user queries within a conversational context.
While previous approaches, such as human-
rewritten queries (Lin et al., 2020; V oskarides et al.,
2020a; Yu et al., 2020), have attempted to improve
query clarity, they often fall short in optimizing re-
trieval performance. Although human rewrites may
enhance readability for users, they do not always
align with the needs of retrieval systems.
To address these challenges, recent studies have
introduced methods like CONQRR (Wu et al.,
2

--- PAGE 3 ---
ğ‘¸ğ’ƒğ’‚ğ’”ğ’†ğ’ğ’Šğ’ğ’†
Is throat cancer 
treatable?Baseline 
Query 
GenerationInitial 
Retrieval with 
ğ‘¸ğ’ƒğ’‚ğ’”ğ’†ğ’ğ’Šğ’ğ’†Ifdiagnosed early, throat cancer has ahigh
cure rate .Throat cancer may not be
curable once malignant cells spread to
parts ofthebody beyond theneck and (â€¦)Initial Guided Documents
(â€¦)
ğ‘¹ğ’‚ğ’˜ğ‘¸ğ’–ğ’†ğ’“ğ’š : Is it treatable?ğ‘¯ğ’Šğ’”ğ’•ğ’ğ’“ ğ’š:What isthroat cancer ?
Re-ranked Guided Documents
Re-rank
[ Generating Retriever -friendly Query ] 
[ Augmenting Keyword ] 
Keywords K
â€œearlyâ€ , â€œthroatâ€ , 
â€œcancerâ€ , â€œhighâ€ , â€œcureâ€ , 
â€œrateâ€ , â€œThroatâ€ , 
â€œCancerâ€ , â€œcurableâ€
[ Expected Answer Generation] 
Expected Answer A
throat cancer has ahigh cure rate
thecancer isnotcurable
tooadvanced tobecured (â€¦)
History
Queryâ€œthroatâ€ , â€œcancerâ€ , â€œThroatâ€ , 
â€œCancerâ€ , â€œcurableâ€Filtered Keywords ğ‘²ğ’‡ğ’Šğ’ğ’•ğ’†ğ’“ğ’†ğ’…
throat cancer has a high 
cure rate (. . .)Filtered Answer ğ‘¨ğ’‡ğ’Šğ’ğ’•ğ’†ğ’“ğ’†ğ’…Final Query ğ‘¸ğ’‡ğ’Šğ’ğ’‚ğ’Concat ( [ ğ‘¸ğ’ƒğ’‚ğ’”ğ’†ğ’ğ’Šğ’ğ’† , ğ‘²ğ’‡ğ’Šğ’ğ’•ğ’†ğ’“ğ’†ğ’… , ğ‘¨ğ’‡ğ’Šğ’ğ’•ğ’†ğ’“ğ’†ğ’… ] )Filtering based on 
Query and History
ğŸğŸğŸğŸ‘ğ’•ğ’‰
4 1 3 2
3.2
[ Filtering and Unify ] 3.3
3.1[ Guided Documents Retrieval ] 
Is throat cancer treatable? 
throat cancer Throat Cancer 
curable throat cancer has a 
high cure rate (. . .)
Post Retrieval 
with ğ‘¸ğ’‡ğ’Šğ’ğ’‚ğ’ğŸ“ğŸğŸğ’•ğ’‰ ğŸğŸğ’•ğ’‰ğŸ–ğ’•ğ’‰
ğŸğŸ–ğŸ•ğ’•ğ’‰ğŸ–ğŸ—ğŸ’ğ’•ğ’‰ğŸğŸ–ğŸ”ğŸ“ğ’•ğ’‰ğŸğŸğŸ•ğ’•ğ’‰
ğŸ’ğ’•ğ’‰ ğŸğ’”ğ’•ğŸğ’ğ’…ğŸ‘ğ’“ğ’…
Figure 2: Overall framework of GuideCQR : For easier understanding, we only visualize top-1 ranked document and
present keywords augmented from the top-1 document and 3 answer pairs from the top-3 documents.
2022), which directly optimizes query rewriting
for retrieval through Self-Critical Sequence Train-
ing (Rennie et al., 2017), and ConvGQR(Mo et al.,
2023a), which improves retrieval performance by
combining query rewriting with query expansion.
Additionally, IterCQR (Jang et al., 2024) iteratively
trains the CQR model by using information re-
trieval (IR) signals as rewards. The integration
of LLMs in CQR has also shown promise in recent
work (Mao et al., 2023).
To the best of our knowledge, no prior study
has directly utilized the signals within retrieved
documents to apply CQR. Our proposed method,
GuideCQR , focuses on transforming queries by
leveraging information from the retrieved docu-
ments alongside the baseline query during the re-
trieval process.
3 GuideCQR
We propose GuideCQR , a novel framework de-
signed to reformulate conversational queries by
utilizing guidance from initially retrieved docu-
ments. Figure 2 provides an overview of the pro-
posed query reformulation process. GuideCQR
consists of three stages: We first retrieve an initial
set of guided documents using a query reformu-
lated by LLM. This step retrieves documents that
are likely to contain gold passages to guide the
query reformulation process (Sec 3.1). Next, we
extract keywords and generate expected answersfrom the guided documents, creating components
that contribute to making the query more retriever-
friendly (Sec 3.2). Finally, we apply a filtering
process that evaluates both keywords and answers
based on their similarity scores relative to the base-
line query and dialogue history. We then unify and
concatenate these components with the baseline
query to construct the final query for post retrieval
(Sec 3.3).
3.1 Guided Documents Retrieval
3.1.1 Initial Documents Retrieval
In the process of developing a retriever-friendly
query, initially retrieved documents can play a cru-
cial role as guiding resources by providing founda-
tional insights for the CQR process. For example,
in the contents of retrieved documents, these sig-
nals can include critical keywords or contextual
signs that are necessary to the search the gold pas-
sages such as "cure," "rate," and "curable" from the
document shown in Figure 2.
Inspired by these points, GuideCQR starts from
getting initially retrieved documents to gain mean-
ingful signals to the retriever. We obtain these
initial documents by retrieving the documents us-
ing the baseline query set generated by LLM. We
denote baseline query as Qbaseline ,
Qbaseline =Rewrite LLM(History, RawQuery ),(1)
where Rewrite LLM represents an operation that
resolves omissions or coreferences using OpenAI
3

--- PAGE 4 ---
gpt3.5-turbo-16k andRawQuery denotes the raw
question in the dataset, without any reformulation
applied. History denotes the dialogue history of
RawQuery , especially consisting of previous dia-
loguesâ€™ queries, rewritten queries and responses by
human annotators. Based on both the RawQuery
andHistory , we generate the Qbaseline . Using
thisQbaseline , we obtain the initial documents com-
posed of 2,000 documents for each question. They
are crucial for creating our final query set and
strong foundation for identifying meaningful sig-
nals and guiding the retriever towards the most
relevant passages.
3.1.2 Re-ranking Documents
To further improve the quality of the guided set of
documents, we implement a re-ranking process to
generate initially retrieved documents. By reorder-
ing the initial documents using a different retrieval
model, we aim to capture better documents by re-
ducing biases that may arise from relying on a
single retriever. Specifically, we employ Sentence-
Transformer (Reimers and Gurevych, 2019) to re-
rank the top 2,000 documents for each question,
selecting the final guided set by choosing the top
10 documents based on their similarity scores to
the query.
3.2 Generating Retriever-friendly Query
Since many retrieved documents may contain influ-
ential words or signals that significantly impact sub-
sequent retrieval stages, identifying key elements
from these documents is crucial for constructing
more effective queries. Based on the guided docu-
ments obtained from the previous step, we generate
retriever-friendly queries by incorporating extra in-
formation to the query from two approaches: Aug-
menting Keywords ,Expected Answer generation
3.2.1 Augmenting Keywords
We find that keywords from the initial documents
play a critical role in forming retriever-friendly
queries, as they capture the most relevant and signif-
icant terms within the document. For example, key-
words such as "early", "throat", "cancer", "high",
and "cure" can be beneficial if they are augmented
to the search query as shown in Figure 2.
We leverage KeyBERT (Grootendorst, 2020)
to extract keywords from the re-ranked docu-
ments. The process begins by using BERT (De-
vlin et al., 2019) to compute embeddings of doc-
uments, which create a representation for the en-tire document. Embeddings for N-gram words or
phrases within the document are then extracted.
By calculating the cosine similarity between these
words/phrases and the document representation, the
method identifies which words/phrases are most
similar to the document.
In line with this principle, we enhance the base
queries by augmenting keywords through two hy-
perparameters. The first one involves determining
the number of documents to extract keywords from,
which establishes the level of guidance we intend
to provide. For each guided document, we extract
keywords of a specified span length. The second
aspect pertains to the span length, which indicates
the number of tokens or keywords selected per doc-
ument. For instance, when augmenting with top-2
span-3 keywords, we extract three keywords from
each of the top two documents, yielding a total of
six keywords.
Consequently, we augment a total keyword list
Kcomposed of nmkeywords from the top- ndoc-
uments and span length m:
K= [k11, k12, k13, ..., k 1m, k21, ..., k nm],(2)
where kdenotes unit keyword, nis the number of
documents and mis keyword span length.
3.2.2 Expected Answer Generation
Guided documents often include gold answers to
the query, serving as a valuable resource for effi-
ciently reformulating the query. Based on this idea,
we use guided documents as context to generate ex-
pected answers to enhance the query. As illustrated
in Figure 2, although itâ€™s not obtained from the gold
passage, expected answer is a concise and informa-
tive response that can potentially address the userâ€™s
query since they can provide direct insights into
the userâ€™s intent, thereby improving the relevance
and accuracy of search results. Specifically, we
generate these expected answers as follows:
A= [a1, a2, a3, ..., a n], (3)
where Arepresents answer list and andenotes a
unit answer extracted from a single document. To
generate these answers, both the query and rele-
vant context are necessary. We use the query as
the baseline query Qbaseline and the context as the
guided documents. We generate a single expected
answer for each document. Consequently, this pro-
cess produces k distinct answers, with one derived
from each of the top-k documents.
4

--- PAGE 5 ---
3.3 Filtering and Unify
We observe that redundant elements, such as the
keyword â€™rateâ€™ as shown in Figure 2, can emerge
from both the augmented keywords and the gen-
erated answers. We find that this is because
GuideCQR might augment keywords or answers
derived from irrelevant documents. Hence, irrel-
evant signals from irrelevant documents can have
a negative impact when creating retriever-friendly
queries.
To address this, we introduce an additional fil-
tering stage to more effectively remove irrelevant
keywords and answers from the reformulated query.
We guide this filtering process through the met-
ricFilterScore , which leverages both QueryScore
andHistoryScore , calculated using cosine simi-
larity, as follows:
cosSim (x, y) =xÂ·y
âˆ¥xâˆ¥âˆ¥yâˆ¥, (4)
where cosSim denotes cosine similarity ranging
from 0 to 1. Based on cosSim , we firstly define
QueryScore as follows:
QueryScore = 10Â·cosSim (query ,item),(5)
where query represents the embedding of the cur-
rent turnâ€™s query, and item refers to the embedding
of either a keyword or an answer sentence. So
QueryScore is cosine similarity between query
and item ranging from 0 to 10. And we define
HistoryScore :
HistoryScore = 10 Â·max ( cosSim (history [i],item)),(6)
where history is the history query list of current
utterances. So HistoryScore is the maximum cosine
similarity value between the dialogue history query
element in the history query list and current item.
Finally, we define the FilterScore as the average
of the QueryScore and the HistoryScore , ranging
from 1 to 10:
FilterScore =QueryScore +HistoryScore
2.(7)
History queries play a crucial role in understand-
ing the current query, as in a dialogue, comprehend-
ing the question is more effective when based on
the preceding conversation. Thus, we account for
history queries through the HistoryScore , which
allows us to traverse the entire dialogue and cap-
ture the global context from past to present. UsingthisFilterScore , we can eliminate signals that are
irrelevant to the current dialogue. Since keywords
and answers may vary in retriever-friendliness
across different datasets, we treat them as distinct
units and apply different filtering scores rather than
using the same score for both.
Using the FilterScore , we filter out keywords
and answers with a score below the specified thresh-
old. Finally, we unify the remaining keywords and
answers and integrate them into the Qbaseline to
construct the final reformulated query as follows:
Qfinal =Concat ([Qbaseline , Kfiltered , Afiltered ]),(8)
where Kfiltered andAfiltered are remaining key-
words, answers.
4 Experiments
4.1 Datasets and Metrics
We utilize three CQR benchmark datasets TREC
CAsT-19 (Dalton et al., 2020), TREC CAsT-
20 (Dalton et al., 2021) and QReCC dataset (Anan-
tha et al., 2021) for our work. CAsT-19 and CAsT-
20 consist of 50 and 25 conversations, respectively.
Both CAsT datasets share the same document col-
lection and provide passage-level relevance judg-
ments, as well as human rewrites for each turn.
Unlike CAsT-19, CAsT-20 is more realistic and
complex because its queries are based on informa-
tion needs drawn from commercial search logs, and
they can reference prior system responses. CAsT
series datasets assign a relevance score ranging
from 1 to 4 to each passage, indicating the degree
of relevance to the query, with documents scoring a
4 being considered gold passages. For the QReCC
dataset, each query is paired with a single gold pas-
sage different from CAsT dataset. QReCC dataset
includes a training set and a test set, and we sample
2K conversations from the training set to create a
development set, following the previous work (Kim
and Kim, 2022).
Following the prior CQR research (Mo et al.,
2023a; Mao et al., 2023; Jang et al., 2024), we use
three widely used evaluation metrics for CQR to
compare the performance: Mean Reciprocal Rank
(MRR), Normalized Discounted Cumulative Gain
at three documents (NDCG@3), Recall@10. We
utilize pytrec_eval (Van Gysel and de Rijke, 2018)
tool to compute the score.
5

--- PAGE 6 ---
CAsT-19 CAsT-20 QReCC
Methods MRR NDCG@3 R@10 MRR NDCG@3 R@10 MRR NDCG@3 R@10 Avg
RawQuery 41.2 24.3 5.8 23.2 15.4 5.5 10.2 9.3 15.7 16.7
Transformer++ 69.6 44.1 - 29.6 18.5 - - - - -
CQE-Sparse 67.1 42.3 - 42.3 27.1 - 32.0 30.1 51.3 -
QuReTeC 68.9 43.0 - 43.0 28.7 - 35.0 32.6 55.0 -
T5QR 70.1 41.7 - 42.3 29.9 - 34.5 31.8 53.1 -
ConvGQR 70.8 43.4 - 46.5 33.1 - 42.0 39.1 63.5 -
CONVINV 74.2 44.9 - 47.6 34.4 - - - - -
CHIQ 73.3 50.5 12.9 54.0 38.0 19.3 47.0 44.2 70.8 45.5
IterCQR - - - - - - 42.9 40.2 65.5 -
Qbaseline 72.2 45.2 12.2 53.4 37.2 18.2 43.8 51.9 66.4 44.5
LLM4CS 77.6 51.5 10.9 61.5 45.5 17.0 44.8 42.1 66.4 46.3
GuideCQR (ours) 81.8 53.7 13.9 59.0 42.7 21.1 47.2 54.4 67.0 48.9
Human Rewrite 74.0 46.1 12.9 59.1 42.2 21.0 43.1 51.6 58.6 45.4
Table 1: Performance comparison for conversational search on various CQR methods on CAsT-19, CAsT-20 and
QReCC dataset. We present MRR, NDCG@3, R@10, and the average of all scores for each method. The best
results are in bold, and the second best are underlined. The dashes (â€™-â€™) indicate performance values that could not
be measured due to differences in evaluation metrics or the unavailability of results from closed-source systems.
4.2 Implementation Details
To generate a baseline query Qbaseline for CAsT-19
and CAsT-20, we utilize OpenAI gpt3.5-turbo-16k
combined with the Maxprob approach as proposed
in LLM4CS. We simply generate this query by in-
structing LLM to rewrite the raw query based on
dialog history and sampling the highest generation
probability with LLMs, resolving only omissions
and coreferences. For the QReCC dataset, rather
than generating Qbaseline ourselves, we adopt the
final query output generated by InfoCQR (Ye
et al., 2023) as Qbaseline . This approach leverages
prompts from gpt3.5-turbo to enhance query gener-
ation.
Following previous studies (Mao et al., 2023;
Jang et al., 2024), we use ANCE (Xiong et al.,
2021) pre-trained on the MSMARCO (Campos
et al., 2016) as our retriever. For further implemen-
tation details, please refer to Appendix A.
4.3 Baselines
We compare GuideCQR with the following CQR
methods: (1) Transformer++ (Vakulenko et al.,
2021a): A GPT-2 based CQR model fine-tuned
on CANARD (Elgohary et al., 2019) dataset.
(2)CQE-Sparse (Lin et al., 2021b): A weakly-
supervised method to select important tokens only
from the context via contextualized query embed-
dings. (3) QuReTeC (V oskarides et al., 2020b):
A weakly-supervised method to train a sequence
tagger to decide whether each term contained in a
historical context should be added to the current
query. (4) T5QR (Lin et al., 2020): A conversa-tional query rewriter based on T5, trained using
human-generated rewrites. (5) ConvGQR (Mo
et al., 2023a): A query reformulation framework
that combines query rewriting with generative
query expansion. (6) CONVINV (Cheng et al.,
2024): Framework that transforms conversational
session embeddings into interpretable text using
Vec2Text. (7) CHIQ (Mo et al., 2024): A two-step
method that leverages the capabilities of LLMs to
resolve ambiguities in the conversation history be-
fore query rewriting. (8) IterCQR (Jang et al.,
2024): CQR framework through iterative refine-
ment based on the similarity between the passage
and the query. (9) LLM4CS : Query rewriting
based on LLM and various prompting methods.
4.4 Performance Comparison
Main Results As shown in Table 1, GuideCQR
significantly enhances performance metrics com-
pared to the Qbaseline across all datasets.
GuideCQR achieves state-of-the-art performance
in terms of average score. In addition, GuideCQR
achieves either the best or second-best performance
compared to all baselines and demonstrates its
robustness. Specifically, GuideCQR outperforms
LLM4CS by 5.4% in MRR on the CAsT-19 dataset
and by 29.2% in NDCG@3 on the QReCC dataset.
For CAsT-20, still its performance is almost the
second-best. These results highlight the robustness
and effectiveness of GuideCQR .
We attribute the lower score for CAsT-20 com-
pared to CAsT-19 to the increased complexity of
the topics in CAsT-20. Specifically, in the retrieval
6

--- PAGE 7 ---
process, we set the parameter rel threshold which
represents the minimum query relevance score for
a document to be considered relevant to the query.
CAsT-19 uses this threshold as 1 and CAsT-20
uses 2, so the minimum criteria for relevance is
higher in CAsT-20. As a result, the number of rele-
vant documents in CAsT-20 is significantly lower
than in CAsT-19. Furthermore, the query relevance
score file for CAsT-20 contains relatively few doc-
uments with high relevance scores; in most cases,
the score is 0. This makes CAsT-20 a more chal-
lenging dataset compared to CAsT-19 for apply-
ingGuideCQR . In other words, the signals derived
from irrelevant guiding documents may not per-
form effectively to GuideCQR in CAsT-20.
Ablation Study To evaluate the effectiveness
of individual components involved in creating
retriever-friendly queries in our proposed CQR
framework, we conduct ablation study. As demon-
strated in Table 2, removing any part of GuideCQR
leads to a decrease in performance, indicating
that each component plays an important role in
GuideCQR .
CAsT-19 CAsT-20
MRR NDCG@3 MRR NDCG@3
GuideCQR (ours) 81.8 53.7 59.0 42.7
- w/o keywords 72.0 47.3 54.7 38.6
- w/o answers 81.0 52.2 57.9 41.6
- w/o filtering 79.7 52.2 58.4 43.2
Table 2: Ablation study on each component of
GuideCQR .
4.5 Analysis
CAsT dataset includes multiple relevant passages,
each with a relevance score. We focus on CAsT
due to these unique label features.
Performance among Top-k Documents for Key-
word and Answer As shown in Table 3, we ver-
ify the impact of varying the number of documents
used in augmenting keywords and generating ex-
pected answers. Our findings indicate that incorpo-
rating a larger number of documents generally pro-
vides additional information, which can improve
performance. However, using too many documents
may cause a decline in overall performance similar
to the amount of initial guided documents.
Performance among Amount of Initial Guided
documents We evaluate the impact and robust-
ness of each step in the GuideCQR setup. Initially,Keyword Answer
MRR NDCG@3 Avg MRR NDCG@3 Avg
Qbaseline 72.2 45.2 58.7 72.2 45.2 58.7
top-2 78.3 50.1 64.2 76.7 48.3 62.5
top-3 80.4 52.1 66.2 77.5 49.0 63.2
top-4 81.0 52.2 66.6 76.9 50.0 63.4
top-5 80.2 52.3 66.2 77.5 49.6 63.5
Table 3: Performance on the CAsT-19 dataset with the
different numbers of documents for extracting keywords.
We augment span 15 keywords from each document.
# of initial guided documents MRR NDCG@3 Avg
10 75.7 50.5 63.1
100 78.2 51.4 64.8
1,000 79.5 52.1 65.8
2,000 81.0 52.2 66.6
Table 4: Evaluation on the CAsT-19 dataset with varying
amount of guided documents.
we adjust the number of guided documents to ob-
serve the proper quantity and present the results
in Table 4. Our findings demonstrate that increas-
ing the number of guided documents consistently
enhances performance. However, retrieving an ex-
cessive number of documents leads to longer infer-
ence times. Hence, considering the computation
cost, we decide to use 2000 documents which is
the point on the dev set where there is no additional
performance improvement in at least one metric.
We also use the same criteria for keyword span
length.
Query Relevance Analysis To demonstrate the
effectiveness of the keyword augmentation step,
we evaluate the precision score using the following
formula:
Precision =Krel
Ktotal, (9)
where Krelrepresents the number of unique
matched keywords found in the guided documents
that have query relevance score of relandKtotal
represents the total number of unique augmented
keywords. Note that the document that has a rel-
evance score of 1 or is regarded as the most rele-
vant document with user queries in CAsT datasets.
Matching keywords in these documents can con-
firm that augmenting keywords plays a crucial role
in generating retriever-friendly queries. As shown
in Table 6, the precision score for relevant passages
is higher than that for irrelevant passages, indicat-
ing that augmenting keywords play a crucial role
in retrieving relevant documents.
7

--- PAGE 8 ---
Qbaseline GuideCQR
MRR NDCG@3 R@10 MRR NDCG@3 R@10
RawQuery 41.2 24.3 5.8 47.0 (+5.8) 28.8 (+4.5) 6.6(+0.8)
Human Rewrite 74.0 46.1 12.9 82.8 (+8.8) 53.0 (+6.9) 14.4 (+1.5)
GPT3.5-turbo-16k 72.2 45.2 12.2 81.8 (+9.6) 53.7 (+8.5) 13.9 (+1.7)
Qwen2.5-7B-Instruct 62.9 39.3 10.2 65.0 (+2.1) 42.3 (+3.0) 11.7 (+1.5)
Mistral-7B-Instruct-v0.3 69.1 42.8 11.6 69.8 (+0.7) 46.0 (+3.2) 12.0 (+0.4)
EXAONE-3.5-7.8B-Instruct 66.4 41.5 11.1 72.6 (+6.2) 46.7 (+5.2) 12.5 (+1.4)
Table 5: Performance comparison of applying GuideCQR onRawQuery , Human rewritten query and Qbaseline
with different generator models. Metrics include MRR, NDCG@3, and Recall@10, with performance improvements
highlighted in red.
Precision
Irrelevant Passage (0) 57.21
RelevantPassage (1) 67.39
Passage (2) 71.36
Passage (3) 72.26
Passage (4) 70.45
Table 6: Precision for irrelevant and relevant passages
of CAsT-19 dataset.
Failure Case Study We analyze a representative
failure case of our proposed method by sampling
the final query in the CAsT-19 dataset. Follow-
ing previous works (Mao et al., 2023), we first set
the relevance threshold for CAsT-19 at 1, mean-
ing that we consider retrieved documents with a
score of 1 or higher relevant to the query. There-
fore, in the sampling failure case process, if the
top-ranked (first position) retrieved document has
a relevance score of 0, we classify it as a failure
case. As shown in Figure 3, when retrieving docu-
ments using Qfinal , the relevance score drops from
2 to 0 compared to Qbaseline , indicating a shift
from relevant to irrelevant content. In Qbaseline ,
the query pertains to the main character of The Nev-
erEnding Story , but the red-marked keywords and
blue-marked answers in the retrieved documents
refer to actual authors or actors, leading to a failure
case. This observation suggests that performance
tends to degrade when unrelated keywords and an-
swers are introduced into the query, deviating from
the original context of Qbaseline .
Adaptability of GuideCQR GuideCQR is a
highly effective framework as it operates as a query
expansion method, making it adaptable for use
with other methods or queries simultaneously. To
demonstrate its effectiveness, we conduct experi-
ments using human-rewritten queries, RawQuery
andQbaseline with different generator models from
the CAsT-19 dataset. We create Qbaseline using
GPT-3.5-turbo-16k to derive the final performanceof GuideCQR. Furthermore, we conduct experi-
ments with EXAONE-3.5-7.8B-Instruct (Research,
2024), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023),
and Qwen2.5-7B-Instruct (Team, 2024). As shown
in Table 5, RawQuery , human-rewritten queries
andQbaseline with different generators show per-
formance improvements. This demonstrates that
our framework can be applied to any query or CQR
method to achieve enhanced performance.
ğ‘¸ğ’ƒğ’‚ğ’”ğ’†ğ’ğ’Šğ’ğ’†
Who are the main characters of the 
Neverending Story?
ğ‘¸ğ’‡ğ’Šğ’ğ’‚ğ’
Who are the main characters of the 
Neverending Story? character information 
book neverending story film following list 
creatures featured appearances 
neverending story neverending story 
german die unendliche geschichte german
fantasy novel michael ende published 
novel adapted films neverending story 
character creatures 1 Plot summary. 
Bastian, Noah Hathaway as Atreyu bullies
German writer Michael Ende, first 
published in 1979.ğ‘¸ğ’“ğ’†ğ’ğ‘ºğ’„ğ’ğ’“ğ’† for 
Top-1 Retrieved 
Document
ğŸğŸ
(ğ‘¹ğ’†ğ’ğ’†ğ’—ğ’‚ğ’ğ’• )
(ğ‘°ğ’“ğ’“ğ’†ğ’ğ’†ğ’—ğ’‚ğ’ğ’• )
Figure 3: Failure case of GuideCQR for reformulating
conversational query, where the system generates irrele-
vant keywords and answers with regard to the Qbaseline .
5 Conclusion
We present GuideCQR , a novel query reformula-
tion framework utilizing initially retrieved docu-
ments from query set for conversational search.
GuideCQR effectively reformulates conversational
queries to be more retriever-friendly by extracting
meaningful information from these guided docu-
ments. Experimental results across various datasets
and metrics confirm the capability of GuideCQR
over human rewritten query and previous CQR
methods. We also show that GuideCQR can be
effectively adapted to various types of re-written
queries.
8

--- PAGE 9 ---
Limitations
Our proposed framework generally takes more time
to reformulate conversational queries compared
to conventional CQR methods. In particular, the
stages involving keyword augmentation and ex-
pected answer generation tend to be more time-
consuming in GuideCQR , especially as the key-
word span length and the number of documents
increase. Future research will need to address these
inefficiencies to reduce the overall processing time.
Moreover, the optimal keyword span length and
number of documents may vary depending on the
dataset. While incorporating more signals can
enhance search results, the ideal amount of sig-
nals across diverse datasets remains undetermined.
Thus, optimizing signal usage for different datasets
represents a promising avenue for future explo-
ration.
Despite these limitations, we believe our frame-
work offers a novel perspective on addressing CQR
challenges. As a result, we expect it will positively
influence the future development of CQR methods.
Ethics Statement
This research leverages multiple publicly available
datasets for conversational query reformulation.
We have accurately cited all the papers and sources
used in our study. We intend to publish the final
query and the code including the pre-trained model
for our proposed framework once the paper is ac-
cepted.
Acknowledgement
This research was supported by Institute for Infor-
mation & Communications Technology Planning
& Evaluation (IITP) through the Korea government
(MSIT) under Grant No. 2021-0-01341 (Artifi-
cial Intelligence Graduate School Program (Chung-
Ang University)).
References
Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,
Shayne Longpre, Stephen Pulman, and Srinivas
Chappidi. 2021. Open-domain question answering
goes conversational via question rewriting. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
520â€“534, Online. Association for Computational Lin-
guistics.Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg,
Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, Li Deng, and Bhaskar Mitra. 2016. Ms
marco: A human generated machine reading compre-
hension dataset. ArXiv , abs/1611.09268.
Yiruo Cheng, Kelong Mao, and Zhicheng Dou. 2024. In-
terpreting conversational dense retrieval by rewriting-
enhanced inversion of session embedding. In Pro-
ceedings of the 62nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 2879â€“2893, Bangkok, Thailand. As-
sociation for Computational Linguistics.
Jeffrey Dalton, Chenyan Xiong, and Jamie Callan.
2021. Cast 2020: The conversational assistance track
overview.
Jeffrey Dalton, Chenyan Xiong, Vaibhav Kumar, and
Jamie Callan. 2020. Cast-19: A dataset for conver-
sational information seeking. In Proceedings of the
43rd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval ,
pages 1985â€“1988.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171â€“4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Ahmed Elgohary, Denis Peskov, and Jordan Boyd-
Graber. 2019. Can you unpack that? learning to
rewrite questions-in-context. Can You Unpack That?
Learning to Rewrite Questions-in-Context .
Jianfeng Gao, Chenyan Xiong, Paul Bennett, and Nick
Craswell. 2022. Neural approaches to conversa-
tional information retrieval . Springer.
Maarten Grootendorst. 2020. Keybert: Minimal key-
word extraction with bert.
Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui
Wang, and Michael Bendersky. 2023. Query expan-
sion by prompting large language models. arXiv
preprint arXiv:2305.03653 .
Yunah Jang, Kang-il Lee, Hyunkyung Bae, Hwanhee
Lee, and Kyomin Jung. 2024. IterCQR: Iterative con-
versational query reformulation with retrieval guid-
ance. In Proceedings of the 2024 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (Volume 1: Long Papers) , pages 8121â€“8138,
Mexico City, Mexico. Association for Computational
Linguistics.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
9

--- PAGE 10 ---
Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019.
Billion-scale similarity search with gpus. IEEE
Transactions on Big Data , 7(3):535â€“547.
Sungdong Kim and Gangwoo Kim. 2022. Saving dense
retriever from shortcut dependency in conversational
search. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 10278â€“10287.
Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-
Hong Yang, Ronak Pradeep, and Rodrigo Frassetto
Nogueira. 2021. Pyserini: A python toolkit for re-
producible information retrieval research with sparse
and dense representations. In SIGIR .
Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.
2021b. Contextualized query embeddings for con-
versational search. In In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1004â€“1015.
Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,
Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.
2020. Conversational question reformulation via
sequence-to-sequence architectures and pretrained
language models.
Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou,
Haonan Chen, and Hongjin Qian. 2023. Large lan-
guage models know your contextual search intent:
A prompting framework for conversational search.
InFindings of the Association for Computational
Linguistics: EMNLP 2023 , pages 1211â€“1225.
Fengran Mo, Abbas Ghaddar, Kelong Mao, Mehdi Reza-
gholizadeh, Boxing Chen, Qun Liu, and Jian-Yun Nie.
2024. Chiq: Contextual history enhancement for
improving query rewriting in conversational search.
arXiv preprint arXiv:2406.05013 .
Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu,
Kaiyu Huang, and Jian-Yun Nie. 2023a. Convgqr:
Generative query reformulation for conversational
search. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 4998â€“5012.
Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao,
Yutao Zhu, Peng Li, and Yang Liu. 2023b. Learning
to relate to previous turns in conversational search. In
Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining , pages
1722â€“1732.
Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W Bruce
Croft, and Mohit Iyyer. 2020. Open-retrieval con-
versational question answering. In Proceedings of
the 43rd International ACM SIGIR conference on
research and development in Information Retrieval ,
pages 539â€“548.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9thInternational Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) , pages 3982â€“3992.
Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In Proceed-
ings of the IEEE conference on computer vision and
pattern recognition , pages 7008â€“7024.
LG AI Research. 2024. Exaone 3.5: Series of large
language models for real-world use cases. arXiv
preprint arXiv:https://arxiv.org/abs/2412.04862 .
Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends Â®in Information Re-
trieval , 3(4):333â€“389.
Qwen Team. 2024. Qwen2.5: A party of foundation
models.
Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu,
and Raviteja Anantha. 2021a. Question rewriting for
conversational question answering. In In Proceed-
ings of the 14th ACM International Conference on
Web Search and Data Mining , pages 355â€“363.
Christophe Van Gysel and Maarten de Rijke. 2018.
Pytrec_eval: An extremely fast python interface to
trec_eval. In The 41st International ACM SIGIR Con-
ference on Research & Development in Information
Retrieval , pages 873â€“876.
Nikos V oskarides, Dan Li, Pengjie Ren, Evangelos
Kanoulas, and Maarten de Rijke. 2020a. Query reso-
lution for conversational search with limited supervi-
sion. In Proceedings of the 43rd International ACM
SIGIR conference on research and development in
Information Retrieval , pages 921â€“930.
Nikos V oskarides, Dan Li, Pengjie Ren, Evangelos
Kanoulas, and Maarten de Rijke. 2020b. Query res-
olution for conversational search with limited super-
vision. In In Proceedings of the 43rd International
ACM SIGIR conference on research and development
in Information Retrieval , pages 921â€“930.
Liang Wang, Nan Yang, and Furu Wei. 2023.
Query2doc: Query expansion with large language
models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 9414â€“9423.
Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reit-
ter, Hannaneh Hajishirzi, Mari Ostendorf, and Gau-
rav Singh Tomar. 2022. Conqrr: Conversational
query rewriting for retrieval with reinforcement learn-
ing. In Proceedings of the 2022 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 10000â€“10014.
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul N. Bennett, Junaid Ahmed, and
Arnold Overwijk. 2021. Approximate nearest neigh-
bor negative contrastive learning for dense text re-
trieval. In 9th International Conference on Learning
10

--- PAGE 11 ---
Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021 . OpenReview.net.
Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yil-
maz. 2023. Enhancing conversational search: Large
language model-aided informative query rewriting.
InFindings of the Association for Computational Lin-
guistics: EMNLP 2023 , pages 5985â€“6006, Singapore.
Association for Computational Linguistics.
Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul
Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-
shot generative conversational query rewriting. In
Proceedings of the 43rd International ACM SIGIR
conference on research and development in Informa-
tion Retrieval , pages 1933â€“1936.
11

--- PAGE 12 ---
A Implementation Details
To re-rank the guided documents as in section 3.1.2,
we select the ember-v12andmxbai-embed-large-
v13models as the first and second re-ranking
Sentence-Transformer models, respectively. And
we use OpenAI text-embedding-3-small for gener-
ating embeddings for keywords and answers in the
filtering stage in GuideCQR . We use different em-
bedding models at each stage to mitigate potential
biases caused by a single model. We use Key-
BERT (Grootendorst, 2020) for augmenting key-
words. And we use RoBERTa-Base-Squad24for
generating the expected answer. We use the most
common query among the five candidate queries
provided by the baseline (Mao et al., 2023) during
the re-ranking, answer generation and filtering pro-
cess in CAsT datasets. This is because (Mao et al.,
2023) provides a set of five candidate queries when
generating their queries, so we make representa-
tive among them. And in augmenting keyword
process, the number of keywords extracted can be
fewer than the span length because the number of
keywords reasonable in each document can be less
than the length of the keyword span. This means
that the keyword span length is maximum number
of keywords for each document. We use roberta-
large-mnli5for measuring entailment score be-
tween augmented keywords and Qbaseline . In the
query relevance analysis, we ensure a fair compari-
son by standardizing the number of documents in
each set to 1,700 before conducting the experiment.
We leverage Faiss index (Johnson et al., 2019) for
ANCE retriever. We conduct our experiments using
an AMD EPYC 7313 CPU (3.0 GHz) paired with
four NVIDIA RTX 4090 GPUs. We use Python
3.11.5 and PyTorch 2.3.1 for the software environ-
ment.
Selecting FilterScore We experiment with vari-
ousFilterScore settings to find the optimal hyper-
parameter with the validation set for the proposed
method. The number of documents for keywords
and expected answers and optimal keyword span
length varies by dataset; these values were chosen
to improve retrieval results and enhance query qual-
ity for the retriever. Therefore, before filtering, we
use top-4 span 15 keywords and top-10 answers for
2https://huggingface.co/llmrails/ember-v1
3https://huggingface.co/mixedbread-ai/mxbai-embed-
large-v1
4https://huggingface.co/deepset/roberta-base-squad2
5https://huggingface.co/FacebookAI/roberta-large-mnliCAsT-19, and top-5 span 5 keywords and top-10
answers for CAsT-20 and top-1 span 10 keywords
and top-10 answers for QReCC. And GuideCQR
uses these keywords and answers as our final query
set. We select the FilterScore for each keyword
and answer by experimenting with validation sets
for the datasets to find the optimal scores. We
achieve our final results using a FilterScore of (1,
1.9) for each keyword and answer in CAsT-19, (0.1,
1.95) in CAsT-20, and (0.5, 9) in QReCC.
B Result using Sparse Retrieval
We provide the performance of GuideCQR using
sparse retrieval. This result is not included in the
main table because prior studies on the CAsT se-
ries have predominantly focused on dense retrieval.
Consequently, retrieval results for baseline meth-
ods are unavailable and the rewritten queries are
not provided either. Thus, we conduct experiments
with the available resources. We perform retrieval
using BM25 (Robertson et al., 2009), and indexing
is performed via Pyserini (Lin et al., 2021). For
sparse retrieval in LLM4CS (Mao et al., 2023), we
concatenate the rewritten query and response of
generated LLM4CS final query.
As shown in Table 7, GuideCQR significantly
enhances performance metrics compared to the
Qbaseline across all datasets. GuideCQR achieves
state-of-the-art performance in terms of the average
score. Similar to earlier observations, the scores for
CAsT-20 are lower due to the unique characteristics
of the CAsT datasets.
C Dataset Statistics
CAsT Table 8 presents statistics for the CAsT
datasets, including the number of conversations,
queries, and documents.
QReCC Table 9 shows the statistics for the
QReCC dataset, which contains approximately
54M documents. We use a final test set of 8,209
questions, following (Ye et al., 2023), after re-
moving invalid gold passage labels. This test set
includes 6,396 questions for QuAC-Conv, 1,442
for NQ-Conv, and 371 for TREC-Conv.
D Performance among Keyword Span
Length
We also investigate the impact of varying keyword
spans in augmenting the keyword stage and present
the result in Table 10. We observe that increasing
12

--- PAGE 13 ---
CAsT-19 CAsT-20 QReCC Avg
Methods MRR NDCG@3 R@10 MRR NDCG@3 R@10 MRR NDCG@3 R@10
RawQuery 31.9 13.1 3.8 9.9 6.8 2.5 6.5 5.5 11.1 10.1
Qbaseline 59.9 29.5 10.3 30.5 19.0 10.7 46.9 55.0 65.5 36.4
LLM4CS 67.5 41.9 11.5 53.7 38.8 20.5 47.8 45.0 69.1 44.0
GuideCQR (ours) 75.4 49.7 14.0 49.1 34.4 16.4 51.2 58.6 69.0 46.4
Human Rewrite 61.3 30.9 10.9 37.7 23.9 14.2 39.7 36.2 62.5 35.3
Table 7: Performance comparison on CAsT-19, CAsT-20, and QReCC datasets using sparse retriever (BM25). We
present MRR, NDCG@3, R@10, and the average of all scores for each method. The best results are in bold, and
the second best are underlined.
CAsT-19 CAsT-20
# Conv 50 25
# Questions 479 216
# Documents 38M 38M
Table 8: Dataset Statistics for CAsT-19 and CAsT-20.
Train Dev Test
QReCC
# Conv 8,823 2,000 2,775
# Questions 51,928 11,573 16,451
QuAC-Conv
# Conv 6,008 1,300 1,816
# Questions 41,395 8,965 12,389
NQ-Conv
# Conv 2,815 700 879
# Questions 10,533 2,608 3,314
TREC-Conv
# Conv 0 0 80
# Questions 0 0 748
Table 9: Dataset Statistics for QReCC, QuAC-Conv,
NQ-Conv, and TREC-Conv.
the keyword span generally enhances performance
to a certain extent. However, we find that too many
signals can degrade performance due to inclusion
of the redundant information in the query.
E Performance for removing duplicated
keywords
We conduct experiments by removing duplicate
keywords, as non-duplicated keywords may lead
to improved retrieval performance. Table 11 shows
that duplicated keywords performed slightly better
than unique ones, so we keep them duplicated.
F Process of generating Qbaseline of CAsT
We generate Qbaseline of CAsT datasets by utiliz-
ing the LLM4CS REW GitHub code which em-CAsT-19 CAsT-20
MRR NDCG@3 MRR NDCG@3
Qbaseline 72.2 45.2 53.4 37.2
5 span 76.9 49.9 57.7 41.5
10 span 78.8 52.0 53.7 39.8
15 span 81.0 52.2 54.1 40.0
20 span 80.1 52.0 54.6 39.8
Table 10: Performance among the different numbers of
span for extracting keywords. We augment all keywords
from top-4 documents.
MRR NDCG@3
Original 81.0 52.2
Duplicate_removed 78.3 49.9
Table 11: Evaluation on the CAsT-19 dataset with
duplicate-removed keywords. We augment span 15 key-
words from top-4 documents.
ploys GPT-3.5-turbo-16k for prompting (Mao
et al., 2023). As illustrated in Table 13, the
prompt consists of three parts: instruction, demon-
stration, tail instruction. The final prompt is a con-
catenation of these three components. Using this
prompt, GuideCQR generate Qbaseline by refining
RawQuery withHistory , resolving any corefer-
ences or omissions.
G Dialog history usage in GuideCQR
In conversational search, utilizing dialog history
is crucial. GuideCQR leverages the conversation
history in two key ways. First, we use dialog his-
tory when generating Qbaseline , as it helps refine
the current query by resolving any ambiguities,
coreferences, or omissions from previous interac-
tions. Additionally, we incorporate history in fil-
tering augmented keywords and expected answers.
By measuring the similarity with the history, we
can select the most relevant keywords or answers.
Through these methods, GuideCQR effectively in-
13

--- PAGE 14 ---
tegrates history, demonstrating its capability as a
conversational search system.
H Query Sample
We present example queries reformulated by our
method and by humans in Table 12. While our
final query, Qfinal , may not appear intuitive from
a human perspective, it yields better search results
during the retrieval stage.
ISamples of Positive Queries on Datasets
We provide positive query sample of each dataset.
For CAsT, if the top-ranked (first position) retrieved
document has a relevance score of 1 or higher,
we classify it as a positive case. For the QReCC
dataset, we refer to a retrieval result as a positive
case if the gold passage is included in the top-1
result. Table 14, Table 15 and Table 16 shows
positive query sample of CAsT-19, CAsT-20, and
QReCC dataset.
14

--- PAGE 15 ---
RawQuery What are its other competitors?
Human-rewritten What are Netflixâ€™s other competitors than Blockbuster?
Qbaseline What are Netflixâ€™s other competitors?
Qfinal What are Netflixâ€™s other competitors? netflix competitors subscription streaming
amazon instant hulu plus amazon netflix competes viaplay hbo nordic cmore asia
netflix competes hooq sky demand competitors apple itunes amazon cinemanow
hulu lovefilm google netflix fandangonow ultraflix voddler competitors Amazon
Instant Video Service and the Hulu Plus service Presto, Stan and Quickflix
FandangoNow, UltraFlix, and V oddler : Competitors brick and mortar video
rental stores
Table 12: Query sample for CAsT-19 (Conv_id: 49, Turn_id: 5)
Instruction For an information-seeking dialog, please help reformulate the question into a
rewrite that can fully express the userâ€™s information needs without the need for
context.
Demonstration I will give you several example multi-turn dialogs, where each turn contains a
question, a response, and a rewrite that you need to generate.
Tail Instruction Now, you should give me the rewrite of the **Current Question** under the **Con-
text**. The output format should always be: Rewrite: $Rewrite. Note that you
should always try to rewrite it. Never ask for clarification or say you donâ€™t under-
stand it in the generated rewrite. Go ahead!
Table 13: The prompt for generating Qbaseline is composed of three components: instruction, demonstration, and
tail instruction.
15

--- PAGE 16 ---
CAsT-19 Dataset
Previous Turns Query: What is throat cancer?
Response: â€œâ€
Query: Is it treatable?
Response: â€œâ€
Query: Tell me about lung cancer.
Response: â€œâ€
Original Query What are its symptoms?
Human Rewrite What are lung cancerâ€™s symptoms?
LLM What are the symptoms of lung cancer?
GuideCQR Query What are the symptoms of lung cancer? signs symptoms lung cancer symptoms
lung cancer signs symptoms lung cancer cough signs symptoms lung cancer
cough (hemoptysis), breath, wheezing, hoarseness coughing; pain chest, cough-
ing
Gold Passage (4) Signs and symptoms of lung cancer typically occur only when the disease is
advanced. Signs and symptoms of lung cancer may include: A new cough that
doesnâ€™t go away. Changes in a chronic cough or smokerâ€™s cough. Coughing up
blood, even a small amount. Shortness of breath. Chest pain.
Table 14: Reformulated Queries by GuideCQR on CAsT-19 Dataset. Overlapping words between GuideCQR
Query andGold Passage (4) are marked in red. Gold Passage(4) notates that the document has query relevance
score of 4 and LLM notates the Qbaseline .
16

--- PAGE 17 ---
CAsT-20 Dataset
Previous Turns Query: How do you know when your garage door opener is going bad?
Response: Light Socket or Logic Board. If the light is still not coming on, it
could be the light socket or the logic board. To determine which one it is, follow
the steps below. Unplug the garage door opener, then immediately plug the
opener back into the electrical outlet. Listen for the light relay on the logic
board to click. If you hear a click, replace the light socket. If you do not hear a
click, replace the logic board. There are four causes for the lights on a garage
door opener not to come on: 1. Light bulb. 2. Contacts in the light socket. 3.
Light socket.
Query: Now it stopped working. Why?
Response: Garage Door Opener Problems. So, when the garage door opener
decides to take a day off, it can leave you stuck outside, probably during a rain
or snowstorm. Though they may seem complicated, there really are several
things most homeowners can do to diagnose and repair opener failures. And, if
you are careful not to damage the door or the seal on the bottom of the door,
use a flat shovel or similar tool to chip away at the ice. Once you get the door
open, clear any water, ice or snow from the spot on the garage floor where the
door rests when closed.
Original Query How much does it cost for someone to fix it?
Human Rewrite How much does it cost for someone to repair a garage door opener?
LLM How much does it typically cost to have a professional fix a garage door opener
issue?
GuideCQR Query How much does it typically cost to have a professional fix a garage door opener
issue? garage door opener repair cost garage door opener repair cost cost repair
broken garage door opener garage door opener repair cost garage door opener
repair cost fix garage door opener
Gold Passage (4) On average, homeowners report paying $210 to have a garage door opener
repaired by a handyman. Most homeowners pay between $170 and $250 for
the cost of such a repair project. The minimum reported cost to repair a broken
garage door opener is $50, while the most costly repair may amount to $350.
If an electrician is needed to repair faulty wiring, the cost of the repair will
increase. The minimum reported cost to repair a broken garage door opener
is $50, while the most costly repair may amount to $350. If an electrician is
needed to repair faulty wiring, the cost of the repair will increase.
Table 15: Reformulated Queries by GuideCQR on CAsT-20 Dataset. Overlapping words between GuideCQR
Query andGold Passage (4) are marked in red. Gold Passage(4) notates that the document has query relevance
score of 4 and LLM notates the Qbaseline .
17

--- PAGE 18 ---
QReCC Dataset
Previous Turns Query: What are the main breeds of goat?,
Answer: Abaza...Zhongwei
Original Query Tell me about boer goats.
Human Rewrite Tell me about boer goats.
LLM What can you tell me about boer goats in relation to the main breeds of goats?
GuideCQR Query What can you tell me about boer goats in relation to the main breeds of goats?
goat boer breed south developed
Gold Answer The Boer goat is a breed of goat that was developed in South Africa in the early
1900s for meat production. Their name is derived from the Afrikaans (Dutch)
word boer, meaning farmer.
Gold Passage Boer goat - Wikipedia CentralNotice Boer goat From Wikipedia, the free
encyclopedia Jump to navigation Jump to search A commercial Boer goat buck
Commercial Boer goat buck Boer goat, Doe The Boer goat is a breed of goat
that was developed in South Africa in the early 1900s and is a popular breed for
meat production. Their name is derived from the Afrikaans ( Dutch ) word boer
, meaning farmer. Contents 1 Origins and characteristics 2 Doe 3 Crossbreeding
4 Show goats 5 References 6 External links Origins and characteristics [ edit
] The Boer goat was probably bred from the indigenous South African goats
kept by the Namaqua , San , and Fooku tribes, with some crossing of Indian
and European bloodlines being possible. They were selected for meat rather
than milk production; due to selective breeding and improvement, the Boer
goat has a fast growth rate and excellent carcass qualities, making it one of
the most popular breeds of meat goat in the world. Boer goats have a high
resistance to disease and adapt well to hot, dry semideserts. United States
production is centered in west-central Texas , particularly in and around San
Angelo and Menard . The original US breeding stock came from herds located
in New Zealand. Only later were they imported directly from Africa Boer goats
commonly have white bodies and distinctive brown heads. Some Boer goats can
be completely brown or white or paint, which means large spots of a different
color are on their bodies. Like the Nubian goat , they possess long, pendulous
ears. They are noted for being docile, fast-growing, and having high fertility
rates. Does are reported to have superior mothering skills as compared to other
breeds.
Table 16: Reformulated Queries by GuideCQR on QReCC Dataset. Overlapping words between GuideCQR Query
andGold Passage ,Gold Answer are marked in red. LLM notates the Qbaseline .
18
