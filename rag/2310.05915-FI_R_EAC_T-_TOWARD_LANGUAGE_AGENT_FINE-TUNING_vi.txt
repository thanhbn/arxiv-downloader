# 2310.05915.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2310.05915.pdf
# Kích thước tệp: 785239 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
FIREACT: HƯỚNG TỚI VIỆC TINH CHỈNH TÁC TỬ NGÔN NGỮ
Baian Chen∗
System2 ResearchChang Shu∗
University of CambridgeEhsan Shareghi
Monash University
Nigel Collier
University of CambridgeKarthik Narasimhan
PLI, Princeton UniversityShunyu Yao
PLI, Princeton University

TÓM TẮT
Các nỗ lực gần đây đã tăng cường các mô hình ngôn ngữ (LM) với các công cụ hoặc môi trường bên ngoài, dẫn đến việc phát triển các tác tử ngôn ngữ có thể lý luận và hành động. Tuy nhiên, hầu hết các tác tử này dựa vào các kỹ thuật prompting few-shot với các LM có sẵn. Trong bài báo này, chúng tôi điều tra và lập luận cho hướng bị bỏ qua của việc tinh chỉnh LM để có được các tác tử ngôn ngữ. Sử dụng thiết lập trả lời câu hỏi (QA) với API tìm kiếm Google, chúng tôi khám phá nhiều LM cơ sở, phương pháp prompting, dữ liệu tinh chỉnh và nhiệm vụ QA khác nhau, và phát hiện các tác tử ngôn ngữ được cải thiện một cách nhất quán sau khi tinh chỉnh LM nền tảng của chúng. Ví dụ, việc tinh chỉnh Llama2-7B với 500 quỹ đạo tác tử được tạo ra bởi GPT-4 dẫn đến tăng 77% hiệu suất HotpotQA. Hơn nữa, chúng tôi đề xuất FireAct, một cách tiếp cận mới để tinh chỉnh LM với các quỹ đạo từ nhiều nhiệm vụ và phương pháp prompting, và cho thấy việc có dữ liệu tinh chỉnh đa dạng hơn có thể cải thiện thêm các tác tử. Cùng với các phát hiện khác về hiệu ứng mở rộng, tính mạnh mẽ, khái quát hóa, hiệu quả và chi phí, công trình của chúng tôi thiết lập những lợi ích toàn diện của việc tinh chỉnh LM cho các tác tử, và cung cấp một bộ thiết kế thí nghiệm ban đầu, những hiểu biết sâu sắc, cũng như các câu hỏi mở hướng tới việc tinh chỉnh tác tử ngôn ngữ.

1 GIỚI THIỆU
Công trình gần đây đã khám phá việc nối các mô hình ngôn ngữ (LM; Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a) để tương tác với các công cụ hoặc môi trường bên ngoài, dẫn đến một lớp mới của các tác tử ngôn ngữ (Nakano et al., 2021; Yao et al., 2022b; Park et al., 2023) có thể thu được kiến thức mới từ phản hồi môi trường, đưa ra quyết định tuần tự thông qua lý luận ngôn ngữ, và cải thiện việc giải quyết nhiệm vụ bằng cách sử dụng tự phản ánh (Shinn et al., 2023; Wang et al., 2023a). Ngoài nghiên cứu, các phát triển công nghiệp như ChatGPT Plugins (OpenAI, 2023c) đã chỉ ra tiềm năng to lớn của các tác tử ngôn ngữ cho các ứng dụng thực tế.

Cho đến nay, hầu hết các tác tử ngôn ngữ prompt các LM có sẵn để thuận tiện và linh hoạt. Tuy nhiên, các LM hiện tại không được phát triển cho các trường hợp sử dụng tác tử (ví dụ: tạo ra các hành động hoặc tự đánh giá), mà việc prompting few-shot chỉ cung cấp hỗ trợ học tập hạn chế. Kết quả là, hầu hết các LM có hiệu suất và tính mạnh mẽ kém khi được sử dụng cho các tác tử, và một số tác tử tiên tiến (Yao et al., 2023; Wang et al., 2023a) chỉ có thể được hỗ trợ bởi GPT-4 (OpenAI, 2023b), dẫn đến chi phí và độ trễ cao, cùng với các vấn đề như khả năng kiểm soát và tái tạo.

Tinh chỉnh là một giải pháp phù hợp cho những vấn đề này: đã được chứng minh rằng các LM nhỏ hơn được tinh chỉnh có thể vượt trội hơn các LM lớn hơn được prompt cho các nhu cầu lý luận (Zelikman et al., 2022; Huang et al., 2022a) và hành động (Yao et al., 2022b) cụ thể, trong khi tận hưởng thời gian suy luận và chi phí giảm. Nhưng việc nghiên cứu tinh chỉnh LM cho các tác tử đã rất hạn chế, bất chấp lượng lớn các nghiên cứu xung quanh các tác tử ngôn ngữ và tinh chỉnh LM tương ứng (Hình 1). Chỉ có một vài công trình trước đây đã tinh chỉnh LM cho điều hướng web (Nakano et al., 2021; Yao et al., 2022a) hoặc sử dụng công cụ API (Schick et al., 2023; Patil et al., 2023; Qin et al., 2023), với phân tích mở rộng sơ bộ cụ thể cho một loại mô hình (Yao et al., 2022b; Schick et al., 2023; Nakano et al., 2021).

∗Đóng góp ngang nhau. Mã, dữ liệu và mô hình có sẵn tại https://fireact-agent.github.io .

--- TRANG 2 ---
Tác tử ngôn ngữTinh chỉnh LMSử dụng công cụlập kếhoạchtự phản ánhbộ nhớ…RLHFLoRAHuấn luyện hướng dẫnLựa chọn dữ liệu…
❏Ưu điểm so với prompting❏Hỗ trợ học tập đa dạng hơn❏Khái quát hóa và tính mạnh mẽ❏Chi phí và hiệu quả❏Khả năng kiểm soát❏Các câu hỏi nghiên cứu mới❏Hiệu ứng mở rộng của dữ liệu tinh chỉnh?❏Loại và kích thước LM nào để sử dụng?❏Làm thế nào để kết hợp các phương pháp và nhiệm vụ?❏Khi nào sử dụng công cụ hoặc phản ánh?Tinh chỉnh Tác tử Ngôn ngữ？công trình nàyHình 1: Trong khi các tác tử ngôn ngữ và tinh chỉnh mô hình ngôn ngữ đều là chủ đề phổ biến, giao điểm của chúng ít được nghiên cứu. Công trình này thực hiện bước đầu tiên để chỉ ra nhiều lợi thế của việc tinh chỉnh LM cho các mục đích sử dụng tác tử, và mở ra nhiều câu hỏi mới hướng tới việc tinh chỉnh tác tử ngôn ngữ.

Trong công trình này, chúng tôi thực hiện bước đầu tiên hướng tới một nghiên cứu có hệ thống hơn về tinh chỉnh tác tử ngôn ngữ. Chúng tôi đề xuất FireAct, một cách mới để tinh chỉnh LM với các quỹ đạo tác tử được tạo ra từ nhiều nhiệm vụ và phương pháp prompting, và được thống nhất trong định dạng ReAct (Yao et al., 2022b) (Hình 2). Chúng tôi triển khai FireAct sử dụng các nhiệm vụ trả lời câu hỏi (QA) miền mở với quyền truy cập vào API tìm kiếm Google, và GPT-4 (OpenAI, 2023b) để tạo dữ liệu tinh chỉnh. Bằng cách điều tra kỹ lưỡng nhiều LM cơ sở khác nhau (OpenAI, 2023a; Touvron et al., 2023a; Rozière et al., 2023), phương pháp prompting (Yao et al., 2022b; Wei et al., 2022b; Shinn et al., 2023), dữ liệu tinh chỉnh và nhiệm vụ (Yang et al., 2018; Press et al., 2022; Hendrycks et al., 2021; Geva et al., 2021), các thí nghiệm của chúng tôi minh họa nhiều lợi thế của tinh chỉnh và tầm quan trọng của tính đa dạng dữ liệu tinh chỉnh. Ví dụ, trong khi prompting ReAct few-shot GPT-3.5 trên HotpotQA đạt điểm khớp chính xác (EM) là 31.4, việc tinh chỉnh với 500 quỹ đạo ReAct cải thiện EM lên 39.2 (tăng 25%), và việc tinh chỉnh với hỗn hợp các quỹ đạo ReAct và CoT cải thiện thêm EM lên 41.0 (tăng 31%). Hơn nữa, tinh chỉnh giảm thời gian suy luận 4 lần, và cải thiện hiệu suất 64% khi đối mặt với các đầu ra công cụ gây nhiễu. Những lợi ích như vậy có thể thậm chí còn rõ ràng hơn đối với các LM mã nguồn mở nhỏ hơn nơi prompting few-shot hoạt động kém, ví dụ: tinh chỉnh Llama2-7B (Touvron et al., 2023a) dẫn đến tăng 77% EM trên HotpotQA.

Bên cạnh việc thể hiện những lợi ích này, các thí nghiệm của chúng tôi cũng khám phá các tương tác phức tạp giữa nhiều yếu tố khác nhau của tinh chỉnh và cung cấp những hiểu biết sâu sắc có thể hành động cho các nhà thực hành. Đối với LM cơ sở, chúng tôi thấy GPT-3.5 vượt trội đáng kể so với các LM mã nguồn mở khác khi tinh chỉnh với ít hơn 500 mẫu, nhưng khoảng cách có thể được bắt kịp dần dần bằng cách mở rộng lên nhiều mẫu tinh chỉnh hơn. Đối với các phương pháp prompting để tạo dữ liệu tinh chỉnh, chúng tôi thấy các LM khác nhau hưởng lợi từ các tỷ lệ hỗn hợp khác nhau, và trình bày thống kê quỹ đạo và phân tích oracle để hiểu thêm. Đối với các nhiệm vụ để tạo dữ liệu tinh chỉnh, kết quả sơ bộ của chúng tôi cho thấy việc thêm một nhiệm vụ có thể không cải thiện hiệu suất downstream trên các nhiệm vụ khác biệt đáng kể, nhưng cũng không làm tổn hại hiệu suất. Điều này gợi ý tiềm năng cho việc tinh chỉnh đa nhiệm vụ quy mô lớn để có được một LM duy nhất làm nền tảng tác tử cho nhiều ứng dụng khác nhau. Cùng với nhiều phát hiện, thảo luận khác và việc phát hành mã FireAct, dữ liệu và các checkpoint mô hình, chúng tôi hy vọng công trình của chúng tôi sẽ khơi dậy và truyền cảm hứng cho các nỗ lực tương lai hướng tới các tác tử ngôn ngữ được tinh chỉnh có khả năng và hữu ích hơn.

2 CÔNG TRÌNH LIÊN QUAN
Tác tử ngôn ngữ. Tác tử ngôn ngữ (Weng, 2023; Wang et al., 2023b) đại diện cho một loại hệ thống AI mới nổi sử dụng các mô hình ngôn ngữ (LM) để tương tác với thế giới. Trong khi các tác tử ngôn ngữ sớm nhất chỉ đơn giản sử dụng LM để tạo ra các lệnh hành động (Nakano et al., 2021; Huang et al., 2022b; Ahn et al., 2022; Schick et al., 2023), việc học các ánh xạ quan sát-hành động trực tiếp từ các minh chứng few-shot là thách thức khi miền phức tạp hoặc liên quan đến các hoạt động dài hạn. ReAct (Yao et al., 2022b) đề xuất sử dụng LM để tạo ra cả dấu vết lý luận (Wei et al., 2022b; Nye et al., 2021; Kojima et al., 2022) và hành động, để lý luận có thể hướng dẫn, theo dõi và điều chỉnh hành động một cách linh hoạt, dẫn đến những cải thiện đáng kể so với các phương pháp chỉ hành động. Các công trình tiếp theo đã áp dụng lý luận dựa trên LM cho nhiều mục đích hơn trong thiết kế tác tử, như phản ánh (Shinn et al., 2023; Park et al., 2023), lập kế hoạch (Yao et al., 2023; Dagan et al., 2023; Liu et al., 2023a), tổng hợp chương trình (Liang et al., 2023; Wang et al., 2023a), v.v. Các hình thức nối tiếp bên ngoài cũng đã đa dạng hóa, từ trò chơi kỹ thuật số (Huang et al., 2022b; Wang et al., 2023a), API ("công cụ"; Schick et al., 2023; Patil et al., 2023; Qin et al., 2023), trang web (Yao et al., 2022a; Deng et al., 2023; Zhou et al., 2023b), đến tương tác vật lý (Bharadhwaj et al., 2023; Vemprala et al., 2023; Driess et al., 2023), con người (Zhang et al., 2020), và đa tác tử (Park et al., 2023). Chúng tôi tham khảo độc giả đến Xi et al. (2023) cho một khảo sát thực nghiệm và Sumers et al. (2023) cho một khung lý thuyết có hệ thống về các tác tử ngôn ngữ. Đáng chú ý, hầu hết các tác tử ngôn ngữ hiện tại prompt các LM có sẵn.

Tinh chỉnh mô hình ngôn ngữ. Việc thích ứng các LM được tiền huấn luyện cho các nhiệm vụ downstream là một lĩnh vực nghiên cứu tích cực khác (Zhang et al., 2023b), bao gồm nhiều bộ dữ liệu tinh chỉnh dựa trên hướng dẫn khác nhau (Mishra et al., 2022; Sanh et al., 2022; Köpf et al., 2023; Wang et al., 2023d; Honovich et al., 2023; Longpre et al., 2023), mô hình (Taori et al., 2023; Chiang et al., 2023; Xu et al., 2023; Muennighoff et al., 2023; Ouyang et al., 2022), phương pháp tinh chỉnh hiệu quả tham số (Hu et al., 2022; Ding et al., 2023; Lv et al., 2023; Dettmers et al., 2023; Ivison et al., 2023), và các nguyên tắc lựa chọn dữ liệu (Zhou et al., 2023a; Gunasekar et al., 2023). Ngoài ra, có nhiều nghiên cứu khác nhau về tinh chỉnh các loại LM cụ thể, như LM lập trình (Li et al., 2023; Luo et al., 2023; Rozière et al., 2023), LM đa phương thức (Zhang et al., 2023c; Gong et al., 2023; Dai et al., 2023; Zhang et al., 2023a; Brooks et al., 2023; Su et al., 2023), và LM tăng cường tìm kiếm (Guu et al., 2020; Wang et al., 2023c). Tuy nhiên, việc tinh chỉnh LM cho các tác tử ngôn ngữ có thể lý luận và hành động đã bị hạn chế.

Tinh chỉnh tác tử ngôn ngữ. Bất chấp sự quan tâm rộng rãi đối với các tác tử ngôn ngữ và tinh chỉnh, giao điểm của chúng đã nhận được sự chú ý hạn chế, chỉ với một số nghiên cứu ban đầu về cách hiệu suất mở rộng theo kích thước mô hình cho một họ mô hình cụ thể (Nakano et al., 2021; Schick et al., 2023; Yao et al., 2022b), cách kết hợp thêm công cụ thông qua tìm kiếm (Patil et al., 2023; Qin et al., 2023), và một số phân tích cụ thể cho nhiệm vụ (Yao et al., 2022a; Le et al., 2022). Bài báo này thực hiện một cuộc điều tra có hệ thống hơn, đề xuất và trả lời các câu hỏi mới hướng tới việc tinh chỉnh tác tử ngôn ngữ.

3 FIREACT: TINH CHỈNH LM VỚI CÁC QUỸ ĐẠO REACT ĐA DẠNG

Công trình của chúng tôi phần lớn dựa trên ReAct (Yao et al., 2022b), một cách tiếp cận phổ biến đối với các tác tử ngôn ngữ. Một quỹ đạo giải quyết nhiệm vụ ReAct (Hình 5) bao gồm nhiều vòng suy nghĩ-hành động-quan sát, trong đó một LM tạo ra các "suy nghĩ" tự do cho các mục đích đa dạng (ví dụ: trích xuất thông tin từ các quan sát, đề xuất và điều chỉnh kế hoạch hành động, theo dõi tiến trình nhiệm vụ), và các "hành động" có cấu trúc để tương tác với môi trường (công cụ) và nhận phản hồi "quan sát". ReAct vượt trội hơn các đường cơ sở chỉ lý luận hoặc chỉ hành động, vì lý luận có thể hướng dẫn hành động, và hành động có thể hỗ trợ lý luận với thông tin mới. Do đó, định dạng ReAct đã trở thành cơ sở của nhiều tác tử ngôn ngữ tiếp theo, như Reflexion (Shinn et al., 2023), SwiftSage (Lin et al., 2023), và AutoGPT (Richards, 2023).

Cũng được thể hiện trong (Yao et al., 2022b) là một thí nghiệm tinh chỉnh PaLM (Chowdhery et al., 2022) sơ bộ trên HotpotQA (Yang et al., 2018), trong đó một PaLM-62B được tinh chỉnh vượt trội hơn một PaLM-540B được prompt. Nhưng vẫn chưa biết liệu phát hiện như vậy có khái quát hóa cho các loại LM, phương pháp prompting hoặc nhiệm vụ khác hay không. Các nghiên cứu tiếp theo về tinh chỉnh tác tử ngôn ngữ đã thưa thớt (xem Phần 2).

Do đó, chúng tôi đề xuất FireAct, một phương pháp tinh chỉnh mới cho các tác tử ngôn ngữ. Như được thể hiện trong Hình 2(a), FireAct cũng tận dụng prompting few-shot của một LM mạnh để tạo ra các quỹ đạo ReAct đa dạng để tinh chỉnh một LM nhỏ hơn (tức là chưng cất (Hinton et al., 2015)). Nhưng khác với Yao et al. (2022b), FireAct một cách rõ ràng thúc đẩy tính đa dạng dữ liệu bằng cách kết hợp nhiều nhiệm vụ huấn luyện và phương pháp prompting. Ở đây chúng tôi xem xét hai phương pháp khác tương thích với định dạng ReAct:

• Chuỗi Suy nghĩ (CoT) (Wei et al., 2022b) tạo ra lý luận trung gian để bắc cầu khoảng cách câu hỏi-câu trả lời. Mỗi quỹ đạo CoT có thể được chuyển thành một quỹ đạo ReAct một vòng đơn giản, với "suy nghĩ" là lý luận trung gian và "hành động" là trả về câu trả lời. CoT hữu ích cho các câu hỏi đơn giản không cần công cụ (Hình 2(b)).

• Reflexion (Shinn et al., 2023) chủ yếu tuân theo quỹ đạo ReAct, nhưng kết hợp thêm phản hồi và tự phản ánh. Trong công trình này, chúng tôi đơn giản prompt cho các phản ánh ở vòng ReAct thứ 6 và 10, để các quỹ đạo ReAct dài có thể xoay chiến lược để giải quyết nhiệm vụ hiện tại (ví dụ: "tìm kiếm phim chưa hữu ích, tôi nên tìm kiếm đạo diễn ngay bây giờ").

Trong quá trình suy luận (Hình 2(b)), một tác tử FireAct giảm bớt nhu cầu prompting few-shot, làm cho suy luận hiệu quả và thuận tiện hơn. Nó cũng có thể ngầm lựa chọn phương pháp phù hợp thích ứng với độ phức tạp nhiệm vụ, và thể hiện khái quát hóa và tính mạnh mẽ mạnh hơn so với prompting như một kết quả của hỗ trợ học tập rộng và đa dạng hơn.

4 THIẾT LẬP THÍ NGHIỆM

Nhiệm vụ. Theo công trình trước đây (Wei et al., 2022b; Yao et al., 2022b; Shinn et al., 2023), chúng tôi huấn luyện và kiểm tra trên các nhiệm vụ trả lời câu hỏi (QA) được thiết lập tốt, có dữ liệu huấn luyện phong phú và chất lượng cao cùng với đánh giá dễ dàng và trung thực (khớp chính xác câu trả lời). Chúng tôi sử dụng bốn bộ dữ liệu:

• HotpotQA (Yang et al., 2018) là một bộ dữ liệu QA thách thức lý luận đa bước và tìm kiếm kiến thức. Câu trả lời thường là một thực thể ngắn hoặc có/không. Chúng tôi sử dụng 2.000 câu hỏi huấn luyện ngẫu nhiên để tuyển chọn dữ liệu tinh chỉnh, và 500 câu hỏi dev ngẫu nhiên để đánh giá.

• Bamboogle (Press et al., 2022) là một tập kiểm tra gồm 125 câu hỏi đa bước với định dạng tương tự HotpotQA, nhưng được tạo ra cẩn thận để tránh giải quyết trực tiếp bằng tìm kiếm Google.

• StrategyQA (Geva et al., 2021) là một bộ dữ liệu QA có/không đòi hỏi các bước lý luận ngầm.

• MMLU (Hendrycks et al., 2021) bao gồm 57 nhiệm vụ QA đa lựa chọn trong nhiều lĩnh vực khác nhau như toán học cơ bản, lịch sử và khoa học máy tính.

Công cụ. Theo Press et al. (2022), chúng tôi sử dụng SerpAPI¹ để xây dựng một công cụ tìm kiếm Google trả về mục đầu tiên tồn tại từ "answer box", "answer snippet", "highlight words", hoặc "first result snippet", đảm bảo phản hồi ngắn và liên quan. Chúng tôi thấy một công cụ đơn giản như vậy đủ cho các nhu cầu QA cơ bản qua các nhiệm vụ, và tăng tính dễ sử dụng và tổng quát của các mô hình được tinh chỉnh.

LM. Chúng tôi điều tra ba họ LM:

• OpenAI GPT. Chúng tôi prompt GPT-4 (OpenAI, 2023b) để tạo ra tất cả dữ liệu tinh chỉnh, và sử dụng GPT-3.5 để tinh chỉnh (OpenAI, 2023a) cũng như prompting. Chúng tôi đã sử dụng cả hai mô hình trong chế độ ChatCompletion từ tháng 7 đến tháng 9 năm 2023.

• Llama-2 (Touvron et al., 2023b) với 7B và 13B tham số trong chế độ "chat".

¹https://serpapi.com .

--- TRANG 5 ---
• CodeLlama (Rozière et al., 2023) với 7B, 13B và 34B tham số trong chế độ "instruct", giúp hiểu thêm về mở rộng kích thước mô hình và tầm quan trọng của tinh chỉnh mã cho các nhiệm vụ tác tử.

Phương pháp tinh chỉnh. Chúng tôi sử dụng Low-Rank Adaptation (LoRA) (Hu et al., 2022) cho hầu hết các thí nghiệm tinh chỉnh, nhưng cũng sử dụng tinh chỉnh mô hình đầy đủ để so sánh.

Với các yếu tố khác nhau cơ bản của tinh chỉnh tác tử ngôn ngữ, chúng tôi chia các thí nghiệm thành ba phần với độ phức tạp tăng dần:
• Tinh chỉnh sử dụng một phương pháp prompting duy nhất trên một nhiệm vụ duy nhất (Phần 5);
• Tinh chỉnh sử dụng nhiều phương pháp trên một nhiệm vụ duy nhất (Phần 6);  
• Tinh chỉnh sử dụng nhiều phương pháp trên nhiều nhiệm vụ (Phần 7).

5 TINH CHỈNH NHIỆM VỤ ĐƠNGIẢN, PHƯƠNG PHÁP ĐƠN

Trong phần này, chúng tôi tập trung vào tinh chỉnh với dữ liệu từ một nhiệm vụ duy nhất (HotpotQA) và một phương pháp prompting duy nhất (ReAct). Sử dụng thiết lập đơn giản và được kiểm soát như vậy, chúng tôi xác nhận nhiều lợi ích khác nhau của tinh chỉnh so với prompting (hiệu suất, hiệu quả, tính mạnh mẽ, khái quát hóa), và nghiên cứu hiệu ứng của các LM khác nhau, kích thước dữ liệu và phương pháp tinh chỉnh. Theo mặc định, chúng tôi sử dụng 500 quỹ đạo prompting few-shot thành công được tạo ra bởi GPT-4 để huấn luyện và một tập con ngẫu nhiên gồm 500 câu hỏi dev HotpotQA để đánh giá. Các chi tiết thí nghiệm khác có thể được tìm thấy trong Phụ lục B.

5.1 HIỆU SUẤT VÀ HIỆU QUẢ

Bảng 1: Kết quả prompting.
Prompt EM
GPT-4IO 37.2
CoT 45.0
ReAct 42.0
GPT-3.5IO 22.4
CoT 28.0
ReAct 31.4

Bảng 2: Prompting so với tinh chỉnh, với sự khác biệt tuyệt đối/tương đối.
ReAct FireAct khác biệt tuyệt đối/tương đối
Llama-2-7B 14.8 26.2 +11.4 / 77%
Llama-2-13B 21.2 34.4 +13.1 / 62%
CodeLlama-7B 17.4 27.8 +10.4 / 60%
CodeLlama-13B 20.8 29.0 +8.2 / 39%
CodeLlama-34B 22.2 27.8 +5.6 / 25%
GPT-3.5 31.4 39.2 +7.8 / 25%

Tinh chỉnh tăng đáng kể hiệu suất tác tử. Như được thể hiện trong Bảng 2, tinh chỉnh cải thiện EM HotpotQA từ prompting một cách nhất quán và đáng kể. Trong khi các LM yếu hơn hưởng lợi nhiều hơn từ tinh chỉnh (ví dụ: Llama-2-7B tăng 77%), ngay cả các LM mạnh như GPT-3.5 cũng có thể cải thiện hiệu suất 25%, rõ ràng cho thấy lợi ích của việc học từ nhiều mẫu hơn. Khi so sánh với các đường cơ sở prompting mạnh trong Bảng 1, chúng tôi thấy Llama-2-13B được tinh chỉnh có thể vượt trội hơn tất cả các phương pháp prompting GPT-3.5 (prompting Đầu vào-Đầu ra, IO; Chuỗi suy nghĩ, CoT; ReAct). Đây là một tín hiệu đầy hứa hẹn rằng việc tinh chỉnh các LM mã nguồn mở nhỏ có thể vượt trội hơn prompting các LM thương mại mạnh hơn. Cuối cùng, GPT-3.5 được tinh chỉnh, là LM được tinh chỉnh mạnh nhất, có thể vượt trội hơn prompting GPT-4 + IO nhưng vẫn thua prompting GPT-4 + CoT/ReAct, gợi ý không gian cải thiện. Nhiều kết quả khác (ví dụ: sai số chuẩn) có trong Phụ lục A.1.

Tinh chỉnh rẻ hơn và nhanh hơn trong quá trình suy luận tác tử. Vì các ví dụ trong ngữ cảnh few-shot không cần thiết cho các LM được tinh chỉnh, suy luận của chúng trở nên hiệu quả hơn, đặc biệt cho các ứng dụng tác tử nơi ngữ cảnh được tích lũy lặp đi lặp lại. Ví dụ, phần đầu của Bảng 3 so sánh chi phí suy luận GPT-3.5 được tinh chỉnh so với được prompt, và thấy thời gian suy luận giảm 70% (9.0s xuống 2.7s mỗi thử nghiệm), và chi phí suy luận giảm mặc dù suy luận được tinh chỉnh được tính phí đắt gấp 8 lần. Trong khi những chi phí này sẽ thay đổi tùy theo điều kiện (ví dụ: triển khai song song), lợi thế của việc có ngữ cảnh nhỏ hơn nhiều là rõ ràng.

5.2 TÍNH MẠNH MẼ VÀ KHÁI QUÁT HÓA

Tính mạnh mẽ với các công cụ có nhiễu. Các công cụ hoặc môi trường mà các tác tử ngôn ngữ tương tác không phải lúc nào cũng đáng tin cậy, điều này đã dẫn đến các mối quan tâm về an toàn như jailbreaking (Liu et al., 2023b) hoặc injection prompt (Willison, 2023). Ở đây chúng tôi xem xét một thiết lập đơn giản và vô hại, trong đó API tìm kiếm có xác suất 0.5 để trả về 1) "None" hoặc 2) một phản hồi tìm kiếm ngẫu nhiên (từ tất cả các thí nghiệm và thử nghiệm trước đây), và hỏi liệu các tác tử ngôn ngữ có thể vẫn trả lời câu hỏi một cách mạnh mẽ hay không. Như được thể hiện trong phần thứ hai của Bảng 3, thiết lập "None" hóa ra là thách thức hơn, làm giảm EM ReAct 33.8% và EM FireAct chỉ 14.2%. Thú vị là, các quan sát ngẫu nhiên làm tổn hại ReAct ở mức độ tương tự (giảm 28.0%), nhưng không làm tổn hại FireAct nhiều (chỉ giảm 5.1%), có thể vì các quỹ đạo tinh chỉnh đã chứa các ví dụ về các truy vấn tìm kiếm có nhiễu và cách GPT-4 "phản ứng" với những nhiễu như vậy thành công. Những kết quả ban đầu này gợi ý về tầm quan trọng của hỗ trợ học tập đa dạng hơn cho tính mạnh mẽ. Nhiều kết quả về tính mạnh mẽ có thể được tìm thấy trong Phụ lục A.2.

Khái quát hóa cho các nhiệm vụ mới. Phần thứ ba của Bảng 3 cho thấy kết quả EM của GPT-3.5 được tinh chỉnh và được prompt trên Bamboogle (Press et al., 2022), một tập kiểm tra gồm 125 câu hỏi đa bước được tạo ra cẩn thận sao cho việc tìm kiếm các câu hỏi trên Google không thể đưa ra câu trả lời trực tiếp. Trong khi GPT-3.5 được tinh chỉnh hoặc được prompt HotpotQA đều khái quát hóa cho Bamboogle một cách hợp lý, cái trước (44.0 EM) vẫn đánh bại cái sau (40.8 EM), gợi ý lợi thế khái quát hóa của tinh chỉnh. Tương tự, kết hợp với các prompt few-shot, tinh chỉnh trên HotpotQA cải thiện đáng kể hiệu suất trên Bamboogle, trong khi cải thiện nhẹ trên MMLU và giảm trên StrategyQA so với các mô hình vanilla (Phụ lục A.9). Vì tinh chỉnh trên HotpotQA khó có thể khái quát hóa cho StrategyQA (câu hỏi có/không) hoặc MMLU (câu hỏi đa lựa chọn), hai bộ dữ liệu QA khác với các kiểu câu hỏi và định dạng câu trả lời khác nhau, điều này thúc đẩy các thí nghiệm tinh chỉnh đa nhiệm vụ của chúng tôi trong Phần 7.

5.3 PHÂN TÍCH CÁC YẾU TỐ TINH CHỈNH KHÁC NHAU

Hiệu ứng của phương pháp tinh chỉnh (LoRA so với Mô hình đầy đủ). Đối với Llama-2-7B, chúng tôi quan sát rằng tinh chỉnh mô hình đầy đủ (30.2 EM) vượt trội hơn tinh chỉnh LoRA (26.2 EM) 15.3% (xem Phụ lục A.5). Tuy nhiên, huấn luyện LoRA có giá cả phải chăng hơn nhiều, có thể huấn luyện 5.4 ví dụ mỗi giây trên một RTX 4090 với bộ nhớ GPU 24GB, trong khi huấn luyện 19.7 ví dụ bằng tinh chỉnh đầy đủ đòi hỏi bốn GPU A100 với bộ nhớ GPU 80GB. Do đó, việc chạy hầu hết các thí nghiệm với LoRA cho phép chúng tôi khám phá nhiều thiết lập huấn luyện hơn với ngân sách và khung thời gian hạn chế.

Hiệu ứng của quy mô dữ liệu tinh chỉnh. Hình 3 cho thấy cách hiệu suất FireAct mở rộng theo số lượng quỹ đạo tinh chỉnh (n∈ {100,200,500,1000}). GPT-3.5 xuất hiện rất hiệu quả về mẫu, chỉ cần 100 mẫu để đạt EM khoảng 35, và lợi ích sau 200 mẫu là nhỏ. Mặt khác, các mô hình Llama thậm chí không thể học định dạng ReAct sử dụng 100 hoặc 200 mẫu, nhưng điểm số không tầm thường "xuất hiện" với 500 mẫu, và hầu hết các mô hình (ngoại trừ CodeLlama-13B) cải thiện thêm với 1.000 mẫu. Xu hướng mở rộng dữ liệu như vậy gợi ý rằng các LM mã nguồn mở nhỏ hơn có thể bắt kịp các LM mạnh hơn trên một nhiệm vụ tác tử cụ thể nếu có đủ dữ liệu tinh chỉnh (ví dụ: Llama-2-13B được tinh chỉnh trên 1.000 mẫu có thể bằng GPT-3.5 được tinh chỉnh trên 100 mẫu).

Hiệu ứng của loại LM cơ sở. Bảng 2 tiết lộ rằng GPT-3.5 vượt trội hơn tất cả các mô hình dựa trên Llama trong cả cấu hình prompting và tinh chỉnh. Ngoài ra, CodeLlama-7B vượt trội hơn Llama-2-7B, trong khi CodeLlama-13B không hoạt động tốt bằng Llama-2-13B, gợi ý rằng tinh chỉnh lập trình có thể không phải lúc nào cũng có lợi cho các trường hợp sử dụng tác tử. CodeLlama hoạt động tốt hơn một chút khi sử dụng tokenizer CodeLlama mặc định thay vì tokenizer Llama (Phụ lục A.6).

Hiệu ứng của quy mô LM cơ sở. Như có thể thấy trong Bảng 2 hoặc các thanh màu xanh của Hình 4, các mô hình (Code)Llama với 13B tham số luôn vượt trội hơn những mô hình với 7B tham số, nhưng CodeLlama-34B dường như tệ hơn CodeLlama-13B khi được tinh chỉnh thuần túy trên các quỹ đạo ReAct. Nhưng như chúng ta sẽ thấy trong Phần 6 (và được gợi ý trong phần còn lại của Hình 4), các yếu tố khác như loại dữ liệu tinh chỉnh có thể ảnh hưởng đến kết luận và làm cho CodeLlama-34B vượt trội hơn CodeLlama-13B. Nói chung, nhiều thành phần (loại LM, quy mô LM, dữ liệu và phương pháp tinh chỉnh) có thể ảnh hưởng đến kết quả tinh chỉnh cùng nhau, vì vậy các xu hướng mở rộng và loại LM/dữ liệu khác nhau cũng nên được xem xét cùng nhau cho thiết kế tác tử.

6 TINH CHỈNH ĐA PHƯƠNG PHÁP

Tiếp theo, chúng tôi tích hợp CoT (Wei et al., 2022b) và Reflexion (Shinn et al., 2023) với ReAct để tinh chỉnh đa phương pháp trên HotpotQA. Đối với cả hai phương pháp, chúng tôi tạo ra 500 quỹ đạo prompting few-shot thông qua GPT-4, và sử dụng 47 quỹ đạo Reflexion dài đã kết hợp tự phản ánh sau 6 hoặc 10 vòng ReAct, và 187 quỹ đạo CoT thành công được định dạng lại như các quỹ đạo ReAct một vòng, trên cơ sở 500 quỹ đạo ReAct hiện có. Nhiều chi tiết hơn có trong Phụ lục B.

Tinh chỉnh đa phương pháp tăng tính linh hoạt của tác tử. Trước các kết quả định lượng, chúng tôi trình bày hai câu hỏi ví dụ trong Hình 5 và một số quỹ đạo GPT-3.5 được tinh chỉnh để minh họa lợi ích của tinh chỉnh FireAct đa phương pháp. Câu hỏi đầu tiên (a) đơn giản, nhưng tác tử được tinh chỉnh chỉ ReAct (a1) đã tìm kiếm một truy vấn quá phức tạp dẫn đến sự xao nhãng và câu trả lời sai. Ngược lại, một tác tử được tinh chỉnh với cả CoT và ReAct đã chọn giải quyết nhiệm vụ trong một vòng dựa vào kiến thức nội bộ tự tin. Câu hỏi thứ hai (b) khó hơn, và tác tử được tinh chỉnh chỉ ReAct

Bảng 3: So sánh chi phí, tính mạnh mẽ và khái quát hóa cho GPT-3.5 được tinh chỉnh so với được prompt.
Chi phí mỗi thử nghiệm Tính mạnh mẽ Obs. (EM) Khái quát hóa
Tiền ($) Thời gian (s) Bình thường "None" Ngẫu nhiên Bamboogle (EM)
FireAct 2.2×10−3 2.7 39.2 33.6 37.2 44.0
ReAct 2.6×10−3 9.0 31.4 20.8 22.6 40.8

[Hình 3: Mở rộng dữ liệu.] [Hình 4: Kết quả qua các LM và loại dữ liệu khác nhau.]

(b1) tiếp tục tìm kiếm các truy vấn kết thúc bằng "trong Nội chiến Libya" mà không có thông tin hữu ích. Ngược lại, một tác tử được tinh chỉnh với cả Reflexion và ReAct đã phản ánh về vấn đề này, và xoay chiến lược tìm kiếm để thay đổi ràng buộc thời gian thành "trong thời gian cầm quyền của ông", dẫn đến câu trả lời đúng. Tính linh hoạt để ngầm lựa chọn phương pháp cho các vấn đề khác nhau là một lợi thế chính khác của tinh chỉnh so với prompting.

Tinh chỉnh đa phương pháp ảnh hưởng khác nhau đến các LM khác nhau. Bất chấp lợi ích trực quan, Hình 4 cho thấy việc kết hợp nhiều phương pháp hơn không phải lúc nào cũng cải thiện kết quả, và hỗn hợp tối ưu của các phương pháp phụ thuộc vào LM cơ sở. Ví dụ, ReAct +CoT vượt trội hơn ReAct đối với GPT-3.5 và các mô hình Llama-2, nhưng có hại cho các mô hình CodeLlama. ReAct +CoT+ Reflexion là tệ nhất cho CodeLlama-7/13B, nhưng là tốt nhất cho CodeLlama-34B. Những kết quả không tầm thường này kêu gọi các nghiên cứu thêm về tương tác của các LM cơ sở và dữ liệu tinh chỉnh.

Các tác tử đa phương pháp có thể chọn phương pháp phù hợp không? Bảng 4 hiển thị kết quả kiểm tra HotpotQA của nhiều tác tử FireAct khác nhau dựa trên GPT-3.5, cũng như trung bình (μ) và độ lệch chuẩn (σ) của số vòng ReAct qua các quỹ đạo của chúng. So với tinh chỉnh chỉ ReAct, ReAct +CoT cải thiện EM và giảm độ dài quỹ đạo, trong khi ReAct +Reflexion làm tổn hại EM và tăng độ dài quỹ đạo. Điều này gợi ý hai hỗn hợp phương pháp chuyển lựa chọn phương pháp sang hai hướng khác nhau, và CoT có lẽ hữu ích hơn cho các câu hỏi HotpotQA. Để hiểu thêm liệu các tác tử đa phương pháp có thể chọn các phương pháp phù hợp hay không, chúng tôi tính toán kết quả của việc lựa chọn ngẫu nhiên một phương pháp trong quá trình suy luận. Kết quả 32.4 thấp hơn nhiều so với tất cả các tác tử đa phương pháp, gợi ý lựa chọn phương pháp không tầm thường. Nhưng việc áp dụng phương pháp tốt nhất cho mỗi thể hiện dẫn đến kết quả "oracle" là 52.0, gợi ý không gian cải thiện lựa chọn phương pháp prompting. Công trình tương lai có thể khám phá tìm kiếm lưới có hệ thống hơn hoặc kết nối giữa thống kê quỹ đạo và hiệu suất để thiết lập tỷ lệ hỗn hợp phương pháp tốt hơn.

7 TINH CHỈNH ĐA NHIỆM VỤ

Cho đến nay, tinh chỉnh chỉ sử dụng dữ liệu HotpotQA, nhưng các nghiên cứu thực nghiệm về tinh chỉnh LM đã cho thấy lợi ích của việc kết hợp các nhiệm vụ khác nhau (Longpre et al., 2023). Ở đây chúng tôi tinh chỉnh GPT-3.5 sử dụng hỗn hợp dữ liệu huấn luyện từ ba bộ dữ liệu: HotpotQA (500 mẫu ReAct, 277 mẫu CoT), StrategyQA (388 mẫu ReAct, 380 mẫu CoT), và MMLU (456 mẫu ReAct, 469 mẫu CoT). Những mẫu này được chọn từ các quỹ đạo prompting few-shot ReAct/CoT thành công được tạo ra thông qua GPT-4.

Như được thể hiện trong Bảng 5, khi dữ liệu StrategyQA/MMLU được thêm vào ("Đa nhiệm vụ"), hiệu suất HotpotQA/Bamboogle gần như không thay đổi. Một mặt, các quỹ đạo StrategyQA/MMLU chứa các câu hỏi rất khác nhau (ví dụ: câu hỏi MMLU là đa lựa chọn) và chiến lược sử dụng công cụ (ví dụ: các quỹ đạo ReAct MMLU có xu hướng tìm kiếm các lựa chọn câu trả lời), làm cho việc chuyển giao khó khăn. Mặt khác, bất chấp sự thay đổi phân phối, việc thêm StrategyQA/MMLU không làm tổn hại hiệu suất HotpotQA/Bamboogle, gợi ý về lời hứa của việc tinh chỉnh một tác tử đa nhiệm vụ để thay thế nhiều tác tử đơn nhiệm vụ, mà không lo lắng về ảnh hưởng tiêu cực qua nhiệm vụ.

Khi chúng tôi chuyển từ tinh chỉnh đa nhiệm vụ, phương pháp đơn sang tinh chỉnh đa nhiệm vụ, đa phương pháp, chúng tôi thấy hiệu suất tăng trên tất cả các nhiệm vụ, một lần nữa củng cố giá trị của tinh chỉnh tác tử đa phương pháp. Thú vị là, tất cả các tác tử được tinh chỉnh (cộng với prompting CoT/ReAct) kém hiệu suất hơn prompting đầu vào-đầu ra (IO) ngây thơ trên MMLU. Một giải thích có thể là những câu hỏi này có thể quá dễ để đòi hỏi lý luận và hành động, và một giải thích khác có thể là việc ghi nhớ lựa chọn câu trả lời. Điều này thúc đẩy nỗ lực cho các phương pháp prompting tốt hơn cũng như cho các bộ dữ liệu tác tử tốt hơn.

Bảng 4: Kết quả đa phương pháp trên HotpotQA.
phương pháp prompting#Vòng
EM μ σ
ReAct 39.4 3.2 1.4
ReAct + CoT 41.0 2.7 1.7
ReAct +Reflexion 38.8 3.8 2.8
ReAct + CoT + Reflexion 40.0 3.0 4.8
Lựa chọn phương pháp ngẫu nhiên 32.4 - -
Lựa chọn phương pháp Oracle 52.0 - -

Bảng 5: Kết quả đa nhiệm vụ với GPT-3.5.
HotpotQA StrategyQA Bamboogle MMLU
Prompting
IO 22.4 48.0 7.2 68.6
CoT 28.0 49.0 41.6 50.8
ReAct 31.4 61.0 40.8 58.6
Tinh chỉnh
HotpotQA 39.2 - 44.0 -
Đa nhiệm vụ 39.2 55.5 43.2 63.2
+ CoT 39.6 72.9 50.4 65.8

8 THẢO LUẬN

Khi nào tinh chỉnh so với prompt cho các tác tử ngôn ngữ? Trong khi hầu hết các tác tử ngôn ngữ hiện tại sử dụng prompting, công trình của chúng tôi kêu gọi suy nghĩ lại về các thực hành tốt nhất bằng cách cho thấy lợi ích đa mặt của tinh chỉnh như một kết quả của hỗ trợ học tập đa dạng hơn. Do đó, prompting và tinh chỉnh dường như phù hợp hơn cho các trường hợp sử dụng khám phá và khai thác tương ứng. Để phát triển các tác tử mới hoặc giải quyết các nhiệm vụ mới, prompting các LM có sẵn có thể cung cấp tính linh hoạt và thuận tiện. Mặt khác, khi nhiệm vụ downstream đã biết (ví dụ: QA), các phương pháp prompting hiệu quả cho các tác tử đã được khám phá (ví dụ: ReAct), và đủ dữ liệu có thể được thu thập (ví dụ: thông qua GPT-4), tinh chỉnh có thể cung cấp hiệu suất tốt hơn, khái quát hóa mạnh hơn cho các nhiệm vụ mới, tính mạnh mẽ hơn với các môi trường có nhiễu hoặc đối kháng, cũng như suy luận rẻ hơn và hiệu quả hơn. Những tính năng này làm cho tinh chỉnh đặc biệt hấp dẫn khi được sử dụng cho các giải pháp công nghiệp quy mô lớn.

LM nào để tinh chỉnh? Trong tất cả các mô hình chúng tôi xem xét, GPT-3.5 luôn vượt trội hơn các LM dựa trên Llama khác trong nhiều thiết lập khác nhau, điều này không đáng ngạc nhiên vì kích thước mô hình lớn hơn nhiều và việc tiếp tục huấn luyện từ GPT-3. Nó cũng có hiệu quả mẫu tốt hơn và chi phí hợp lý (khoảng $10 mỗi thí nghiệm tinh chỉnh trong trường hợp của chúng tôi). Tuy nhiên, chúng tôi cũng đã cho thấy rằng các mô hình Llama mã nguồn mở có thể được tinh chỉnh để bắt kịp hiệu suất GPT-3.5, với đủ dữ liệu tinh chỉnh với sự kết hợp đúng của các phương pháp prompting và nhiệm vụ. Các nhà thực hành nên cân bằng sự đánh đổi giữa sự thuận tiện và hiệu suất của GPT-3.5 so với khả năng kiểm soát và tái tạo của các LM mã nguồn mở để tinh chỉnh tác tử.

Khi nào sử dụng công cụ hoặc phản ánh cho các tác tử ngôn ngữ? Các tác tử ngôn ngữ dựa trên prompting chỉ có thể bắt chước một tập hợp nhỏ và cố định các quỹ đạo giải quyết nhiệm vụ thành công. Điều này có thể dẫn đến việc lạm dụng công cụ (ví dụ: tìm kiếm kiến thức đã được lưu trữ trong LM), và khả năng phục hồi khi quỹ đạo lệch khỏi các mẫu "thành công" (ví dụ: tiếp tục tìm kiếm các truy vấn tương tự với các quan sát vô ích). Tinh chỉnh đa phương pháp của FireAct giúp tăng tính linh hoạt và mạnh mẽ của tác tử ngôn ngữ, nhưng vấn đề biết khi nào cần giúp đỡ (sử dụng công cụ) và phản hồi (phản ánh) vẫn còn xa mới được giải quyết. Công trình về hiệu chuẩn (Ren et al., 2023) và meta-lý luận (Griffiths et al., 2019) có thể làm sáng tỏ thiết kế tác tử tốt hơn trong khía cạnh này.

Hạn chế và hướng tương lai. Công trình này là một bước đầu tiên hướng tới tinh chỉnh tác tử ngôn ngữ, và bị giới hạn ở một loại nhiệm vụ duy nhất (QA) và một công cụ duy nhất (tìm kiếm Google). Công trình tương lai có thể áp dụng các câu hỏi nghiên cứu được đưa ra bởi FireAct cho nhiều nhiệm vụ và thiết lập nối tiếp hơn (ví dụ: nhiều công cụ API hơn, web, thế giới vật lý). Ngoài ra, chúng tôi tập trung vào ba phương pháp (ReAct, CoT, Reflexion) duy trì một ngữ cảnh quỹ đạo tự hồi quy duy nhất, làm cho tinh chỉnh trở nên đơn giản. Vẫn chưa được khám phá cách tinh chỉnh các tác tử tiên tiến hơn liên quan đến nhiều prompt, vai trò và ngữ cảnh (Wang et al., 2023a; Park et al., 2023; Yao et al., 2023), hoặc kết hợp tốt nhất prompting và tinh chỉnh trong một hệ thống tác tử phức tạp. Cuối cùng, thiết lập đa nhiệm vụ trong công trình này bị giới hạn ở ba nhiệm vụ QA, và LM tốt nhất chúng tôi có thể tinh chỉnh là GPT-3.5. Một tinh chỉnh đa nhiệm vụ quy mô lớn (Wei et al., 2022a) sử dụng nền tảng LM tiên tiến nhất sẽ kiểm tra giới hạn của tinh chỉnh tác tử ngôn ngữ, nhưng các điểm chuẩn phù hợp và đa dạng hơn để phát triển và đánh giá các tác tử nên được khám phá trước.

LỜI CẢM ƠN
Chúng tôi cảm ơn Yuqian Sun vì sự giúp đỡ về hình ảnh, SerpAPI vì tài trợ một phần các cuộc gọi API, và Tianyu Gao, Ofir Press, Noah Shinn, Alex Witegg, Eric Zelikman, và Zexuan Zhong vì việc đọc lại và phản hồi có giá trị. SY và KN thừa nhận sự hỗ trợ từ giải thưởng Nghiên cứu Hợp tác Oracle và Quỹ Khoa học Quốc gia dưới Quyền cấp số 2239363. SY cũng được hỗ trợ bởi Học bổng Harold W. Dodds từ Princeton. Bất kỳ ý kiến, phát hiện, kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này đều của (các) tác giả và không nhất thiết phản ánh quan điểm của Quỹ Khoa học Quốc gia. Chúng tôi cũng biết ơn khi thừa nhận rằng công trình của đồng tác giả đầu tiên, CS, đã được hỗ trợ chung bởi một khoản đóng góp từ Toshiba Europe và Hội đồng Nghiên cứu Khoa học Kỹ thuật và Vật lý của UKRI (số cấp 2752931).

[Tiếp tục với phần còn lại của tài liệu...]
