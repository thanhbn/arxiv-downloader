# 2310.07075.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2310.07075.pdf
# Kích thước tệp: 781861 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đừng Fine-Tune, Hãy Decode:
Sử dụng Công cụ Không Lỗi Cú pháp thông qua Constrained Decoding
Kexun Zhang∗
Carnegie Mellon University
kexun@cmu.eduHongqiao Chen∗
California Institute of Technology
harrychen@caltech.edu
Lei Li
Carnegie Mellon University
leili@cs.cmu.eduWilliam Yang Wang
UC Santa Barbara
william@cs.ucsb.edu
Tóm tắt
Các mô hình ngôn ngữ lớn được điều chỉnh theo hướng dẫn (LLM) xuất sắc ở nhiều nhiệm vụ nhưng thường thất bại trong việc sử dụng các công cụ bên ngoài do các ràng buộc cú pháp phức tạp và không quen thuộc. Trong khi fine-tuning và prompting mở rộng có thể giảm thiểu vấn đề này, những cách tiếp cận này rất tốn kém và khó tổng quát hóa. Hơn nữa, vì các ràng buộc cú pháp chỉ được học một cách ngầm định trong quá trình fine-tuning, các mô hình vẫn mắc phải các lỗi cú pháp thường xuyên.
Được thúc đẩy bởi thực tế rằng những ràng buộc này có thể được thỏa mãn tốt hơn một cách rõ ràng với constrained decoding, chúng tôi đề xuất TOOLDEC, một thuật toán decoding sử dụng máy trạng thái hữu hạn để buộc LLM tuân theo cú pháp công cụ. Các thí nghiệm của chúng tôi cho thấy TOOLDECloại bỏ tất cả lỗi cú pháp, đạt được hiệu suất tốt hơn đáng kể trên các mô hình cơ sở và benchmark khác nhau. Đáng ngạc nhiên hơn, khi áp dụng cho các LLM tổng quát sẵn có như Mistral-Instruct, TOOLDECcải thiện độ chính xác trong việc sử dụng công cụ từ 0% ban đầu lên 52% ấn tượng, phù hợp với hiệu suất của các mô hình chuyên biệt được fine-tuned như ToolLLM. Chúng tôi phát hành mã của mình tại https://github.com/chenhongqiao/tooldec .
T oolLLM
(Fine-tuned)Mistral-Ins
(General)0204060Win Rate (%)40.50
0.0050.50 52.00T oolEval-I2-Category
T oolLLM
(Fine-tuned)Mistral-Ins
(General)Win Rate (%)41.00
0.0049.0060.00T oolEval-I3-Inst
T oolkenGPT
(Fine-tuned)Llama
(General)Accuracy (%)10.306.0013.20 13.20FuncQA
w/o T oolDec
w/ T oolDec
T oolLLM + ReAct T oolLLM + DFSDTMistral + ReAct Mistral + DFSDT ChatGPT + ReAct ChatGPT + DFSDTGPT-4 + ReAct GPT-4 + DFSDT020406080100Syntax Error (%) w/o T oolDec
T oolLLM + ReAct T oolLLM + DFSDTMistral + ReAct Mistral + DFSDTSyntax Error (%) w/ T oolDec
Không Có Lỗi Sau ToolDec
Hình 1: Trên các benchmark khác nhau, TOOLDECcải thiện cả mô hình chuyên gia fine-tuned (ToolLLM) và mô hình tổng quát (Mistral-Instruct và Vicuna). Mistral-Instruct được cải thiện từ 0% ban đầu để thậm chí tốt hơn ToolLLM. T OOLDECcũng loại bỏ tất cả lỗi cú pháp.
∗Đóng góp bằng nhau. Liên hệ với kexun@cmu.edu .
Preprint. Đang được xem xét.arXiv:2310.07075v3  [cs.CL]  4 Jun 2024

--- TRANG 2 ---
1 Giới thiệu
Tăng cường các mô hình ngôn ngữ lớn (LLM) với các công cụ bên ngoài (Mialon et al., 2023) cho phép chúng giải quyết các vấn đề phức tạp. Các LLM hiện tại có thể sử dụng bộ truy xuất (Shen et al., 2023; Gupta & Kembhavi, 2022; Schick et al., 2023), API RESTful (Qin et al., 2023; Song et al., 2023), trình thông dịch chương trình (Chen et al., 2022; Gao et al., 2023), và nhiều công cụ khác. Vì các công cụ hiện có đang được sửa đổi và các công cụ mới đang được tạo ra hàng ngày, điều quan trọng là LLM có thể sử dụng các công cụ chưa biết không có trong tập huấn luyện.
Các LLM sẵn có như Mistral và Llama, ngay cả khi được điều chỉnh theo hướng dẫn để rất có khả năng trên nhiều nhiệm vụ khác (Jiang et al., 2023), có thể thất bại trong việc sử dụng công cụ, như được chứng minh bởi hiệu suất thấp (và thậm chí bằng không) trong Hình 1. Một lý do chính cho hiệu suất kém của chúng là lỗi cú pháp. Ví dụ, Mistral-Instruct-7B có tỷ lệ lỗi cú pháp trên 90% khi sử dụng một số công cụ chưa biết trong ToolEval (Qin et al., 2023), dẫn đến độ chính xác 0%. Ngay cả các mô hình rất có khả năng như GPT-4 cũng mắc lỗi cú pháp trên các công cụ mới. Các cách tiếp cận trước đây sử dụng fine-tuning hoặc prompting mở rộng (Qin et al., 2023; Hao et al., 2023) để dạy LLM cú pháp công cụ, điều này làm giảm lỗi cú pháp nhưng không phải tất cả. Trong Hình 1, mặc dù ToolLLM được fine-tuned tạo ra ít lỗi cú pháp hơn đáng kể so với Mistral-Instruct tổng quát, nó vẫn có tỷ lệ lỗi trên 20%. Chúng tôi cho thấy các ví dụ về các chế độ thất bại phổ biến trong Hình 2.
Chúng tôi lập luận rằng fine-tuning hoặc prompting không phải tối ưu cũng không đủ để thực thi các ràng buộc cú pháp, vì các mô hình được điều chỉnh theo hướng dẫn tổng quát đã có "ý tưởng sơ bộ" về công cụ nào sử dụng trong các tình huống khác nhau. Các cách tiếp cận fine-tuning hoặc prompting mong đợi mô hình học và tuân theo các ràng buộc cú pháp từ các ví dụ sử dụng công cụ trong dữ liệu huấn luyện hoặc trong demonstration ngữ cảnh. Tuy nhiên, những ràng buộc này có thể được mô hình hóa một cách rõ ràng với các quy tắc ký hiệu. Áp dụng trực tiếp những quy tắc này trong việc tạo ra mô hình có thể chính xác hơn so với fine-tuning hoặc prompting, vì chúng không cần tính toán bổ sung hoặc prompting trong khi đảm bảo mô hình không có lỗi cú pháp.
Để đạt được điều này, chúng tôi đề xuất TOOLDEC, một thuật toán decoding được hướng dẫn bởi máy trạng thái hữu hạn (FSM) để đảm bảo LLM gọi công cụ đúng cách. Chúng tôi tự động chuyển đổi các schema cú pháp công cụ thành một máy trạng thái hữu hạn tương đương. Trong quá trình decoding, TOOLDECchuyển đổi từ trạng thái này sang trạng thái khác khi decoding tiến triển. Ở mỗi bước decoding, TOOLDEClấy mẫu từ tập hợp con hợp lệ của các token được cho phép bởi cú pháp công cụ. Bằng cách này, TOOLDECluôn có thể tạo ra các lời gọi công cụ đúng về mặt cú pháp. Vì các ràng buộc cú pháp được thực thi bởi FSM, không cần chúng xuất hiện trong prompts. Chúng tôi tiếp tục sử dụng một LLM để đơn giản hóa prompt mô tả công cụ bằng cách loại bỏ các ràng buộc cú pháp khỏi đó.TOOLDECkhông phụ thuộc vào mô hình và có thể được kết hợp với bất kỳ LLM cơ sở nào. Nhiều ví dụ so sánh TOOLDECvà các LLM công cụ khác có thể được tìm thấy trong Phụ lục A.1.
Hành động: 
random_anime
Hành động 
Đầu vào: 
{}
Lỗi Tên 
Công cụ
Q: 
Bạn có 
thể 
cung cấp 
cho 
tôi 
một 
hình ảnh 
anime 
ngẫu nhiên 
và 
phân tích 
nó 
để 
tìm 
nội dung 
NSFW 
không?
                          
...
Đầu vào Hành động:
 
random_img_anime
Đầu vào Hành động:
Lỗi Định dạng 
Lý luận
Hành động: 
nsfw_detection
Đầu vào Hành động: 
{"img": 
"..."}
Công cụ: 
random_img_anime, 
nsfw_detection, 
...
Schema: 
nsfw_detection: 
{"url": 
{type: 
"string"}}...
Lỗi Tham số 
Công cụ
Kết quả 
LLM 
1
Kết quả 
LLM 
2
Kết quả 
LLM 
3
Hình 2: Các chế độ thất bại cú pháp phổ biến của LLM sử dụng công cụ bao gồm lỗi định dạng lý luận, lỗi tên công cụ và lỗi tham số công cụ. Ngay cả các mô hình được fine-tuned cũng có mức độ lỗi cú pháp đáng kể.
Chúng tôi đánh giá TOOLDECbằng cách áp dụng nó cho 5 mô hình cơ sở và đánh giá trên 4 benchmark. Trên các mô hình cơ sở được fine-tuned cụ thể cho mục đích sử dụng công cụ, như ToolLLM (Qin et al., 2023) và ToolkenGPT (Hao et al., 2023), TOOLDECcải thiện đáng kể độ chính xác của chúng lên tới 21 điểm và giảm đáng kể số lượng token trong đầu ra. Trên các mô hình cơ sở ban đầu có độ chính xác bằng không trên ToolBench (Qin et al., 2023) như Mistral (Jiang et al., 2023) và Vicuna (Chiang et al., 2023), T OOLDECcải thiện độ chính xác của chúng từ 0 lên tối đa 60%.
2

--- TRANG 3 ---
Các đóng góp của chúng tôi có thể được tóm tắt như sau:
•Chúng tôi đề xuất TOOLDEC, một thuật toán constrained decoding cho LLM để sử dụng công cụ mà không có lỗi cú pháp. T OOLDECcó thể chuyển tải các ràng buộc cú pháp sang FSM, và loại bỏ tất cả lỗi cú pháp.
•Chúng tôi xác minh hiệu suất vượt trội của TOOLDEC bằng cách kết hợp nó với 5 mô hình cơ sở và đánh giá trên 4 benchmark. Các thí nghiệm của chúng tôi cho thấy T OOLDECcải thiện tất cả các mô hình cơ sở một cách đáng kể.
•Chúng tôi tiếp tục so sánh TOOLDECtrên các mô hình tổng quát với các mô hình sử dụng công cụ chuyên biệt. Chúng tôi thấy rằngTOOLDEC, như một giải pháp thay thế, có thể đạt được hiệu suất tương đương trong khi duy trì hiệu suất của mô hình trên các benchmark lý luận khác.
2 Nghiên cứu liên quan
Fine-tuning mô hình ngôn ngữ để sử dụng công cụ. Các mô hình ngôn ngữ có thể được fine-tuned để sử dụng công cụ với dữ liệu chứa văn bản xen kẽ và việc sử dụng công cụ. Các nghiên cứu trước đây làm cho các mô hình ngôn ngữ sử dụng một công cụ duy nhất như mô-đun truy xuất (Borgeaud et al., 2022; Guu et al., 2020) hoặc công cụ tìm kiếm (Nakano et al., 2021) bằng fine-tuning. Những tiến bộ gần đây trong các mô hình ngôn ngữ tăng cường công cụ sử dụng nhiều công cụ (Schick et al., 2023; Parisi et al., 2022) cũng fine-tune các mô hình ngôn ngữ để sử dụng công cụ bao gồm các mô hình QA, mô hình dịch thuật, máy tính và công cụ tìm kiếm. ToolkenGPT (Hao et al., 2023) đề xuất sử dụng một số token đặc biệt để đại diện cho công cụ và chỉ điều chỉnh các embedding của các token để việc áp dụng công cụ mới có thể hiệu quả hơn. Tuy nhiên, các cách tiếp cận fine-tuning không thể thích ứng với các công cụ mới mà không có dữ liệu huấn luyện.
In-context learning cho việc sử dụng công cụ. Các mô hình ngôn ngữ có thể học từ các ví dụ in-context (Brown et al., 2020) và tuân theo hướng dẫn (Ouyang et al., 2022). Điều này làm cho việc đơn giản chỉ đặt mô tả công cụ trong prompt và yêu cầu các mô hình ngôn ngữ sử dụng chúng trở nên khả thi. Các công trình gần đây đặt tài liệu công cụ và demonstration trong prompt để sử dụng các mô hình neural (Shen et al., 2023), API RESTful (Qin et al., 2023; Song et al., 2023), trình thông dịch chương trình (Chen et al., 2022; Gao et al., 2023) và nhiều công cụ khác để giải quyết vấn đề. In-context learning không cần điều chỉnh mô hình bổ sung để sử dụng các công cụ mới. Tuy nhiên, các ràng buộc cú pháp và ngữ nghĩa của các công cụ mới bị vướng vào trong prompts, dẫn đến prompts dài hơn và lỗi cú pháp.
Constrained decoding và máy trạng thái hữu hạn. Các phương pháp constrained decoding trước đây giảm không gian tìm kiếm lớn của lexically constrained decoding với máy trạng thái hữu hạn (Anderson et al., 2017), nhóm các ứng viên tương tự lại với nhau (Hokamp & Liu, 2017), và các thuật toán tìm kiếm tốt hơn (Miao et al., 2019; Lu et al., 2021, 2022). Tuy nhiên, các ràng buộc từ vựng không đủ biểu cảm để điều chỉnh các lời gọi công cụ. Trong khi máy trạng thái hữu hạn phải được có trọng số và xác suất để xử lý các ràng buộc mềm trong ngôn ngữ tự nhiên (Eisner, 2002; Rastogi et al., 2016), các ràng buộc cho các lời gọi công cụ cú pháp là các ràng buộc cứng dễ dàng hơn nhiều cho FSM. Grammar-constrained decoding đã được sử dụng cho các nhiệm vụ NLP có cấu trúc như tạo mã (Yin & Neubig, 2017), phân tích cú pháp ngữ nghĩa (Stengel-Eskin et al., 2024), phân giải đồng tham chiếu, gán nhãn POS và nhiều nhiệm vụ khác (Geng et al., 2023). Willard & Louf (2023) sử dụng máy trạng thái hữu hạn để hướng dẫn LLM tạo ra đầu ra một cách hiệu quả và tuân thủ ngữ pháp.
3 Phương pháp đề xuất: T OOL DEC
Để sử dụng một công cụ, một LLM trước tiên phải tham chiếu đến một công cụ hiện có bằng tên được chỉ định của nó. Sau đó, nó cần tạo ra các tham số tuân thủ ngữ pháp của công cụ đó (ví dụ một JSON Schema). Được thúc đẩy bởi thực tế rằng việc xác minh cú pháp của một lời gọi công cụ bằng máy trạng thái hữu hạn (FSM) là dễ dàng, chúng tôi đề xuất TOOLDEC, một framework sử dụng công cụ LLM sử dụng FSM để loại bỏ lỗi cú pháp.Trong mỗi bước decoding, mô hình lấy mẫu từ một tập hợp con của từ vựng chỉ chứa các token đúng về mặt cú pháp. Tập hợp con này được quyết định bởi trạng thái hiện tại của FSM và token được lấy mẫu xác định trạng thái tiếp theo để chuyển đổi.
Chúng tôi đã thiết kế một thuật toán để tự động xây dựng FSM được mô tả từ tài liệu công cụ một cách đệ quy (Phần 3.1). Nó tận dụng tài liệu endpoint có thể đọc được bằng máy (ví dụ, OpenAPI), rất phổ biến trong kỹ thuật phần mềm. Ngoài việc xây dựng FSM, chúng tôi sử dụng một LLM khác để loại bỏ các ràng buộc cú pháp khỏi tài liệu công cụ (Phần 3.2). Mục tiêu của bước này là rút ngắn prompts mở rộng và chứng minh rằng FSM một mình đủ để đảm bảo các lời gọi công cụ đúng về mặt cú pháp. Một ví dụ về constrained decoding với TOOLDECđược cung cấp trong Phần 3.3.
3

--- TRANG 4 ---
{
{
"from":
"from":
"
"
Any
"
,
"to":
"to":
"
"
Any
"
,
,
"type":
"
"
Any
}
flight_search_: 
Tìm kiếm 
chuyến bay.
Tham số:
  
- 
from: 
Mã 
sân bay 
khởi hành 
(Ví dụ: 
LHR).
  
- 
to: 
Mã 
sân bay 
đến 
(Ví dụ: 
DXB). 
  
- 
adult: 
Số 
hành khách 
người lớn.
  
- 
child: 
Số 
hành khách 
trẻ em.
  
- 
type: 
Loại 
hạng 
ưa thích 
(Tùy chọn, 
Ví dụ: 
economy).
{
  
name: 
"flight_search",
  
parameters: 
{
    
type: 
"object",
    
properties: 
{
      
from: 
{ 
type: 
"string", 
description: 
"Departure...", 
example_value: 
"LHR" 
},
      
to: 
{ 
type: 
"string", 
description: 
"Arrival...", 
example_value: 
"DXB" 
},
      
adult: 
{ 
type: 
"integer", 
description: 
"Number...", 
example_value: 
1 
},
      
child: 
{ 
type: 
"integer", 
description: 
"Number...", 
example_value: 
1 
},
      
type: 
{ 
type: 
"string", 
description: 
"Flight...", 
example_value: 
"economy" 
}
    
},
    
required: 
["from", 
"to", 
"passengers"],
    
optional: 
["type"]
  
}
}
"adult":
"adult":
0-9,
,
"child":
"child":
0-9},
,
,
}
Nén 
Prompt 
LLM
Prompt 
Gốc 
(200 
token)
Prompt 
ToolDec 
(86 
token)
FSM 
ToolDec
Xây dựng 
FSM 
Tự động
Bất kỳ 
ngoại trừ 
"
Bất kỳ 
ngoại trừ 
"
0-9
0-9
"type":
Bất kỳ 
ngoại trừ 
"
\n
"Hình 3: Chuyển đổi tài liệu công cụ thành prompt đơn giản và FSM. Chú thích trên mỗi trạng thái trong FSM biểu thị tập token hợp lệ ở bước đó. FSM sẽ chuyển đổi sang trạng thái tương ứng khi token trên cạnh đó được tạo ra bởi LLM.
3.1 Xây dựng T OOL DECFSM
Định nghĩa TOOL DECFSM. Một FSM là bộ 5 thành phần (S, V, g, s 0, R), bao gồm tập trạng thái hữu hạn S, bảng chữ cái V, hàm chuyển đổi g:S×V→S, trạng thái ban đầu s0và tập các trạng thái chấp nhận R.
Trong trường hợp của chúng tôi, Svàgđược xây dựng từ tài liệu công cụ. Vlà từ vựng token của mô hình ngôn ngữ. Rtương ứng với các token được định nghĩa trước có thể xác định LM đã hoàn thành nhiệm vụ, như '<EOS>'.
FSM-constrained decoding. Ở mỗi bước decoding t,TOOLDECduy trì trạng thái hiện tại s. LLM chỉ có thể lấy mẫu từ các token được cho phép bởi FSM, tức là các token mà g(s,·)được định nghĩa. Những token được cho phép này là tập hợp con của Vvà chúng tôi ký hiệu chúng là Vs. Sau khi tạo ra một token a,TOOLDECchuyển đổi sang trạng thái khác g(s, a)được chỉ định bởi hàm chuyển đổi FSM. Các token được cho phép cho mỗi trạng thái có thể là toàn bộ từ vựng, hoặc một tập hợp con hợp lệ tương ứng với tên công cụ và loại tham số. Với token tiếp theo, chúng ta chuyển sang bước decoding tiếp theo và chuyển đổi trạng thái hiện tại ssang trạng thái tiếp theo g(s, a). Mã giả của thuật toán này được liệt kê trong Thuật toán 1.
Xây dựng FSM từ Tài liệu Công cụ. Trong bài báo này, chúng tôi tập trung vào việc xây dựng tự động FSM hướng dẫn cho REST API và các hàm Python. Với thuật toán sau đây, chúng tôi có thể bao phủ hơn 16.000 công cụ trên 4 lĩnh vực khác nhau, đại diện cho các ứng dụng sử dụng công cụ khác nhau được khám phá trong các nghiên cứu trước (Qin et al., 2023; Hao et al., 2023; Yuan et al., 2024). Cách tiếp cận này có thể được mở rộng nếu các ứng dụng tương lai xuất hiện vì máy trạng thái hữu hạn có thể được xây dựng thuật toán cho bất kỳ ngữ pháp thông thường nào. Thuật toán xây dựng FSM của chúng tôi giả định rằng ngữ pháp cú pháp của một công cụ được ghi lại ở định dạng có thể đọc được bằng máy, ví dụ, trong đặc tả OpenAPI.
Trong Hình 3, chúng tôi minh họa một FSM được xây dựng tự động từ JSON Schema. Các nút đại diện cho các trạng thái. Chú thích bên trong trạng thái sminh họa các token hợp lệ Vsvà các cạnh đại diện cho các chuyển đổi có thể dựa trên token được lấy mẫu. Việc tạo ra bắt đầu ở trạng thái ban đầu và dừng với trạng thái chấp nhận (các nút màu xanh lá cây). Đối với mỗi tham số của một công cụ, chúng tôi tạo một máy con cho cú pháp của nó. Ví dụ, hàng đầu tiên với 4 nút trong Hình 3 là máy con cho tham số "from". Mỗi máy con có hai phần – một cho tên tham số và một cho giá trị tham số. Máy con tên (với nền màu xanh dương) chỉ chấp nhận một chuỗi – tên cho một tham số cụ thể. Máy con giá trị (với nền màu hồng) chấp nhận các chuỗi tuân theo định dạng của giá trị tham số. Ví dụ, đối với tham số "adult", nó chỉ chấp nhận các chuỗi chữ số kết thúc bằng dấu phẩy. Sau khi tạo ra tên và giá trị của một tham số, LLM có thể tạo ra một số token kết thúc tạo ra (với nền màu cam, như ' , ' hoặc ' } ') để chuyển sang tham số tiếp theo.
4

--- TRANG 5 ---
Đối với các tham số bắt buộc, máy con của chúng phải được đi qua, trong khi đối với các tham số tùy chọn, có một kết nối bỏ qua cho phép mô hình đi vòng qua nó mà không tạo ra tham số tùy chọn (ví dụ, tham số "type" trong Hình 3). Lưu ý rằng thuật toán này cũng có thể hoạt động cho các tham số lồng nhau (các tham số có các trường con), vì chúng ta chỉ cần áp dụng quy trình một cách đệ quy.
Dựa trên Vs, chúng tôi tính toán trước mặt nạ token cho mọi s∈Strong quá trình xây dựng để cho phép lọc O(1) trong quá trình suy luận. Quá trình xây dựng này chỉ cần được thực hiện một lần cho mỗi công cụ, và có thể được lưu trong bộ nhớ đệm. Kích thước của FSM tăng tuyến tính khi số lượng tham số của một công cụ tăng, giống như số lượng token nếu prompting được sử dụng. Tuy nhiên, cách tiếp cận của chúng tôi hiệu quả hơn nhiều vì chi phí tính toán và bộ nhớ của nó không đáng kể (ít hơn 0,1%) khi so sánh với chi phí GPU của LLM.
Liên kết FSM để Hướng dẫn Lý luận và Lựa chọn Công cụ. Giải quyết vấn đề tăng cường công cụ bao gồm nhiều bước lý luận và lựa chọn công cụ cho mỗi bước. Điều này có thể được hướng dẫn bằng cách liên kết nhiều FSM với nhau. Để hướng dẫn lựa chọn công cụ, chúng tôi tokenized tên của tất cả các công cụ có sẵn và xây dựng cấu trúc cây tương tự như trie (Fredkin, 1960). Mỗi nút lá có gdefined để chuyển đổi sang s0của FSM cho công cụ đó. Một ví dụ có thể được thấy trong Hình 4. Khi số lượng công cụ tăng (ví dụ, trong benchmark KAMEL (Kalo & Fichtel, 2022), kích thước bộ công cụ đạt 234), trie có thể tự động mở rộng bằng cách thêm nhiều nút vào cây. Cú pháp lý luận, như ReAct (Yao et al., 2023), cũng có thể được kết hợp vào FSM như được hiển thị trong Hình 4.
Action: 
s1
s10
air
flight
s2
s11
port
_
s3
_
s4
s7
de
ar
s5
part
s8
rival
ues
s
search
Action 
Input:
flight_search
 
FSM
Action 
Input:
airport_arrivals
 
FSM
Action 
Input:
airport_departures
 
FSM
Hình 4: Liên kết nhiều FSM để hướng dẫn LLM thông qua lý luận và lựa chọn công cụ. Sau khi một công cụ được chọn, FSM sau đó chuyển đổi để bắt đầu tạo ra tham số.
3.2 Nén Prompt
Vì các ràng buộc cú pháp có thể được xử lý hiệu quả bởi thuật toán decoding của chúng tôi, chúng tôi đã tạo ra một pipeline nén prompt để loại bỏ những ràng buộc này khỏi tài liệu công cụ. Đối với mỗi công cụ, chúng tôi đã yêu cầu LLM viết lại tài liệu công cụ trong khi bỏ qua cấu trúc phân cấp của JSON schema và việc gõ từng tham số. Kết quả là một danh sách đơn giản các tham số với mô tả ngắn gọn. Danh sách tham số này cần thiết để LLM sử dụng công cụ khớp thông tin ngữ cảnh mà nó có trong tay với một công cụ. Chúng tôi cho thấy một ví dụ về prompt nén trong Hình 3. Prompt chúng tôi sử dụng để viết lại tài liệu công cụ có thể được tìm thấy trong Phụ lục A.3.
Bằng cách loại bỏ thông tin cú pháp khỏi prompt, chúng tôi giảm số lượng token và cho thấy rằng các ràng buộc cú pháp được xử lý hiệu quả hơn bởi các thuật toán decoding thay vì prompting.
3.3 Suy luận với FSM và Prompt Nén
Suy luận với TOOLDECbao gồm việc sử dụng TOOLDECFSM kết hợp với prompt nén. Đầu tiên, prompt nén, ngắn gọn và không có các ràng buộc cú pháp, được cung cấp cho LLM để bắt đầu tạo ra. Ở mỗi bước t, chúng tôi không trực tiếp lấy mẫu từ phân phối token tiếp theo P(xt|x1..t−1)được tính bởi LLM. Thay vào đó, chúng tôi đặt bằng không xác suất của các token không hợp lệ mà hàm chuyển đổi không được định nghĩa, và chuẩn hóa xác suất,
˜P(xt=a) =(P(xt=a|x1..t−1)P
a′∈VsP(xt=a′|x1..t−1),∃g(s, a),
0, nếu không.
Token tiếp theo asau đó được lấy mẫu từ phân phối được sửa đổi ˜P(xt|x1..t−1, s).
5

--- TRANG 6 ---
Chúng tôi cho thấy một ví dụ trong Hình 5. Đầu tiên, mô tả ngữ nghĩa của flight_search được cung cấp cho LLM. Ở bước n, trạng thái hiện tại snchỉ cho phép tạo ra số nguyên hoặc dấu phẩy, tương ứng với kiểu dữ liệu số nguyên của adult. Bằng cách nhân mặt nạ token với P, chúng tôi có được ˜Pmà ở đó xác suất của tất cả các token khác được đặt bằng không. Từ xác suất đã dịch chuyển, chúng tôi đã lấy mẫu ' , '.
Khi chúng tôi chuyển sang bước n+ 1, FSM cũng chuyển đổi sang sn+1theo g(sn,' , '). Ở đây với bong bóng phóng to, chúng tôi cho thấy FSM thực tế chấp nhận tên tham số "child", chỉ chấp nhận ' " ' làm token đầu tiên. Một quá trình tương tự lặp lại cho đến khi LLM hoàn thành việc tạo ra một lời gọi công cụ.
FSM 
ToolDec
Đầu ra 
Đã tạo
"adult":
"adult":
0-9,
"child":
"child":
0-9},
,
0-9
0-9
Trạng thái 
Hiện tại
FSM 
ToolDec
...
...
{ 
"from": 
"LHR",
"to": 
"DXB",
"adult": 
2
1
1
1
0
1
2
1
1
0
0
9
,
a
...
Mặt nạ 
Token
Xác suất 
Token
,
Bước 
N
...
...
Token 
Được lấy mẫu
Prompt 
Nén
flight_search: 
...
Đầu ra 
Đã tạo
"adult":
"adult":
0-9,
"child":
"child":
0-9},
,
0-9
0-9
...
...
{ 
"from": 
"LHR",
"to": 
"DXB",
"adult": 
2
,
0
0
0
0
1
2
...
Mặt nạ 
Token
Xác suất 
Token
"
Bước 
N+1
...
Token 
Được lấy mẫu
Prompt 
Nén
flight_search: 
...
"
child
"
"
child
...
...
Trạng thái 
Hiện tại
"
0
0
0
1
9
,
a
...
"
...
...
Hình 5: Một bước decoding sử dụng TOOLDECFSM. Các token không hợp lệ ở trạng thái FSM hiện tại được che khuất khỏi xác suất token.
4 Thí nghiệm
Như một thuật toán decoding không phụ thuộc vào mô hình, TOOLDECcó thể được áp dụng cho bất kỳ LLM nào có quyền truy cập vào logits token. Chúng tôi đánh giá TOOLDECbằng cách áp dụng nó cho 5 LLM cơ sở trên 4 benchmark. Trong phần này, chúng tôi trước tiên giới thiệu các LLM cơ sở (Phần 4.1) và các benchmark mà chúng tôi đánh giá T OOLDECtrên đó (Phần 4.2). Sau đó chúng tôi báo cáo kết quả chính của chúng tôi (Phần 4.3) và tiếp tục phân tích tại sao TOOLDECcó thể là một sự bổ sung/thay thế tốt cho fine-tuning và prompting (Phần 4.4).
4.1 LLM Cơ sở
Chúng tôi đánh giá cách TOOLDECcó thể cải thiện 5 LLM — ToolLLM (Qin et al., 2023), ToolkenGPT (Hao et al., 2023), RestGPT (Song et al., 2023), Vicuna-7B (Chiang et al., 2023), và Mistral-7B-Instruct (Jiang et al., 2023). Ba mô hình đầu tiên là "chuyên gia", đặc biệt được thiết kế/fine-tuned cho các benchmark sử dụng công cụ trong cùng những bài báo đề xuất các mô hình. Chúng tôi đánh giá chúng trên các benchmark chuyên biệt của chúng. Ngoài ra, chúng tôi đánh giá Llama và Mistral-7B, hai mô hình được điều chỉnh theo hướng dẫn "tổng quát" để chứng minh rằng TOOLDECcũng có thể hoạt động trên các mô hình cơ sở không được thiết kế đặc biệt để sử dụng công cụ. Hai mô hình này không được fine-tuned trên các tập huấn luyện của benchmark. Do đó, hiệu suất ban đầu của chúng mà không có T OOLDECrất kém so với các đối tác chuyên gia của chúng.
ToolLLM (Qin et al., 2023). ToolLLM là một mô hình LLaMA-7B chuyên gia được fine-tuned để sử dụng công cụ trên RapidAPI ( https://rapidapi.com/ ). Với mỗi nhiệm vụ, ToolLLM được yêu cầu với tài liệu của các công cụ có liên quan. Sau đó nó được hướng dẫn tạo ra một lý luận ngôn ngữ tự nhiên, một công cụ để sử dụng, và các đầu vào của công cụ. Quá trình này tiếp tục trong một số lần lặp cho đến khi đường dẫn của các lời gọi công cụ dẫn đến một câu trả lời hoặc mô hình từ bỏ. ToolLLM có thể tạo ra một đường dẫn duy nhất của các lời gọi công cụ (được ký hiệu làToolLLM+ReAct ) hoặc chạy tìm kiếm cây để tìm đường dẫn tốt nhất (được ký hiệu là ToolLLM+DFSDT ). Lưu ý rằng ToolLLM được fine-tuned với cùng dữ liệu được định dạng được tổng hợp với cùng quy trình như benchmark đánh giá của nó, ToolEval.
ToolkenGPT (Hao et al., 2023). ToolkenGPT là một mô hình Llama-33B chuyên gia với từ vựng bổ sung của các token được học cho các công cụ. Các trọng số gốc của Llama bị đóng băng trong khi các token bổ sung được học. Những token này cho các công cụ, hoặc toolkens , mỗi cái tương ứng với một công cụ. Các khoảng văn bản của việc sử dụng công cụ trong dữ liệu huấn luyện được thay thế bằng toolkens, để mô hình có thể học khi nào bắt đầu một lời gọi công cụ. Mặc dù ToolkenGPT hiệu quả hơn nhiều so với fine-tuning đầy đủ một mô hình, nó vẫn cần học các biểu diễn của tất cả các công cụ trong tập đánh giá.
6

--- TRANG 7 ---
RestGPT (Song et al., 2023). RestGPT học sử dụng RESTful API từ tài liệu công cụ in-context. RestGPT sử dụng một số mô-đun dựa trên LLM khác nhau để tạo kế hoạch ngôn ngữ tự nhiên, chọn API nào để sử dụng, tạo lời gọi công cụ và phân tích kết quả từ chúng.
Mistral-Instruct (Jiang et al., 2023) và LLaMA (Touvron et al., 2023). Mistral-Instruct-7B và LLaMA-7B là những LLM tổng quát 7B được điều chỉnh mà không có fine-tuning cụ thể cho công cụ. Khi được đánh giá trực tiếp trên ToolEval với các ràng buộc cú pháp trong prompt, Mistral không thể giải quyết bất kỳ bài kiểm tra nào. Điều này cho thấy rằng Mistral không thể tuân theo các ràng buộc cú pháp khi được yêu cầu. Điều tương tự cũng đúng với LLaMA-7B. Chúng tôi chọn chúng làm mô hình cơ sở để cho thấy khi các ràng buộc cú pháp được TOOLDEC chăm sóc; ngay cả một mô hình tổng quát cũng có thể hoạt động tốt trên việc sử dụng công cụ.
4.2 Benchmark và Metrics
Chúng tôi đánh giá các baseline và TOOLDECtrên bốn benchmark – ToolEval (Qin et al., 2023), FuncQA (Hao et al., 2023), KAMEL (Kalo & Fichtel, 2022), và RestBench (Song et al., 2023). Chúng tôi đánh giá hiệu suất của tất cả các mô hình cơ sở có và không có TOOLDEC. Ngoài các metric đúng đắn, chúng tôi cũng đo tỷ lệ lỗi cú pháp , tỷ lệ các nhiệm vụ mà các mô hình mắc lỗi cú pháp.
Các thí nghiệm của chúng tôi được tiến hành trên GPU NVIDIA A6000 trong vòng 60 giờ GPU.
ToolEval (Qin et al., 2023). ToolEval là một tập dữ liệu được đề xuất trong bài báo ToolLLM. Các nhiệm vụ trong đó liên quan đến hơn 10000 REST API có sẵn công khai. Chúng tôi sử dụng các tập hợp con phức tạp hơn của nó để đánh giá phương pháp của chúng tôi – I2-Category và I3-Instruction. Chúng chứa các nhiệm vụ cần các công cụ phức tạp và chưa thấy từ nhiều danh mục để giải quyết. Trung bình, một nhiệm vụ trong các tập hợp con này cần hơn 7 công cụ để giải quyết. ToolEval có hai metric chính: tỷ lệ pass đo tỷ lệ phần trăm các nhiệm vụ mà mô hình đạt được câu trả lời trong các bước lý luận hạn chế. tỷ lệ win so sánh chất lượng và tính đúng đắn của câu trả lời của mô hình với câu trả lời tham chiếu từ ChatGPT. Qin et al. (2023) thấy rằng những metric tự động này có mối tương quan cao 75,8% với các chú thích viên người. Vì ToolEval yêu cầu prompting mở rộng tài liệu công cụ, chúng tôi đo tok / tool , số lượng token trung bình cần thiết cho mỗi công cụ, để xem T OOLDECcó thể rút ngắn prompts bao nhiêu.
FuncQA (Hao et al., 2023). FuncQA kiểm tra khả năng của LLM trong các nhiệm vụ lý luận số với 68 bài toán toán học. LLM được yêu cầu tạo ra một câu trả lời số sử dụng một số trong 13 phép toán số học làm công cụ (ví dụ multiply ,power ,lcm). Độ chính xác được xác định bằng cách đo tỷ lệ phần trăm các vấn đề mà câu trả lời đúng được tạo ra, với sai số dung sai 0,1%. Trung bình, một vấn đề trong FuncQA yêu cầu 2,78 lời gọi công cụ để giải quyết. Theo Hao et al. (2023), chúng tôi báo cáo kết quả của các baseline khác, bao gồm ChatGPT không có công cụ, LLaMA với chain-of-thought và công cụ, LLaMA với ReAct và công cụ.
KAMEL (Kalo & Fichtel, 2022). KAMEL là một tập dữ liệu hỏi đáp chứa tổng cộng 234 mối quan hệ kiến thức giống như đặc điểm của API (ví dụ number_of_children ). Các công cụ trong KAMEL cũng phức tạp và đa dạng hơn vì số lượng tham số của chúng thay đổi từ 1 đến 3, và các loại của chúng bao gồm chuỗi, vị trí, ngày tháng, số và các loại ad-hoc khác.
RestBench (Song et al., 2023). RestBench bao gồm các nhiệm vụ trong các tình huống thực tế, bao gồm TMDB, một trang web phim, và Spotify, một trình phát nhạc trực tuyến. Những nhiệm vụ này trực tiếp đến từ hướng dẫn người dùng thực và yêu cầu nhiều công cụ dưới dạng RESTful API để giải quyết. Chúng tôi sử dụng tỷ lệ đường dẫn đúng (CP%) được đề xuất bởi bài báo gốc làm metric để đo độ chính xác. Tỷ lệ đường dẫn đúng là tỷ lệ đầu ra chứa đường dẫn lời gọi công cụ đúng được chú thích bởi con người.
4.3 Kết quả
Chúng tôi liệt kê kết quả chính của chúng tôi trong Bảng 1. Mỗi hàng màu xanh lá chứa hiệu suất của TOOLDECđược áp dụng cho mô hình cơ sở ở hàng trước. Lưu ý rằng trên KAMEL, tỷ lệ lỗi cú pháp không có sẵn. Điều này là bởi vì KAMEL kiểm tra khả năng của mô hình để chọn công cụ phù hợp và không liên quan đến tham số công cụ và thực hiện. Vì lý do tương tự, ToolkenGPT + ToolDec cũng không có sẵn. Ngoài ra, tỷ lệ win trên ToolEval không có sẵn cho ChatGPT + ReAct, vì nó là baseline được so sánh khi tính tỷ lệ win. Một số quan sát có thể được thực hiện về kết quả:
7

--- TRANG 8 ---
Bảng 1: Khi được áp dụng cho các baseline khác nhau trên các benchmark khác nhau, TOOLDECcải thiện đáng kể việc tạo ra mô hình với ít token hơn trong prompt. Nó cũng hoàn toàn loại bỏ tất cả lỗi cú pháp.
"err." là viết tắt của tỷ lệ lỗi cú pháp. "tok" là viết tắt của số lượng token trung bình cho mỗi công cụ.
ToolEval : I2-Category ToolEval: I3-Instruction
win%↑pass% ↑tok↓err.%↓win%↑pass% ↑tok↓err.%↓
ToolLLM + ReAct 36.5 30.5 827 21 49 22 1380 32
+ TOOLDEC 46.5 47.5 397 0 61 48 627 0
ToolLLM + DFSDT 40.5 64.5 827 44 41 58 1380 49
+ TOOLDEC 50.5 69 397 0 49 59 627 0
Mistral + ReAct 0 0 827 92.5 0 0 1380 93
+ TOOLDEC 41 26 397 0 53 32 627 0
Mistral + DFSDT 0 0 827 100 0 0 1380 100
+ TOOLDEC 52 50.5 397 0 60 35 627 0
ChatGPT + ReAct n.a. 39 827 5 n.a. 23 1380 9
ChatGPT + DFSDT 63.0 64.5 827 28 70 60 1380 47
GPT-4 + ReAct 53.5 67.5 827 4 71 40 1380 2
GPT-4 + DFSDT 57 69.5 827 4 73 59 1380 5
FuncQA KAMEL
accuracy % ↑ err.%↓ accuracy % ↑ err.%↓
LLaMA 6 27.9 8.2 n.a.
+ TOOLDEC 13.2 0 42.4 n.a.
ToolkenGPT 10.3 27.9 25.4 n.a.
+ TOOLDEC 13.2 0 n.a. n.a.
RestBench: Spotify RestBench: TMDB
correct path % ↑ err.%↓ correct path % ↑ err.%↓
Vicuna + RestGPT 20.6 7.4 15 23
+TOOLDEC 36 0 23 0
ChatGPT 72.3 3.6 65 3
TOOL DECdẫn đến những cải thiện đáng kể của tất cả các mô hình cơ sở trên nhiều benchmark.
Trên ToolEval, ToolDec cải thiện tỷ lệ win và tỷ lệ pass đáng kể trên ToolLLM với chiến lược ReAct hoặc DFSDT. Tỷ lệ win được cải thiện 10 điểm, trong khi tỷ lệ pass được cải thiện 12 trung bình. Đặc biệt đáng chú ý là hiệu suất của ToolLLM+DFSDT với TOOLDEC, không chỉ vượt trội hơn ChatGPT mà còn đạt được hiệu suất ngang bằng với GPT-4. Những cải thiện đáng kể tương tự cũng được quan sát trên các benchmark khác trên các mô hình cơ sở khác.
TOOL DECgiúp các mô hình tổng quát phù hợp hoặc vượt trội hơn các mô hình chuyên gia có kích thước tương tự.
Không có TOOLDEC, Mistral-7B không thể vượt qua một nhiệm vụ duy nhất trên ToolEval, trong khi ToolLLM có tỷ lệ win trung bình >40% và tỷ lệ pass >40%. Với TOOLDEC, hiệu suất của Mistral-7B trên ToolEval ngang bằng với ToolLLM + TOOLDECvà thậm chí tốt hơn trong một số trường hợp và metric. Điều tương tự cũng đúng với FuncQA và KAMEL. Khi được tăng cường với TOOLDEC, hiệu suất của mô hình LLaMA-7B tổng quát, được tiền huấn luyện tốt hơn 2,2 lần trên FuncQA và 5,1 lần tốt hơn trên KAMEL, phù hợp hoặc đánh bại ToolkenGPT chuyên gia.
TOOL DECđạt được hiệu suất tốt hơn nhiều với prompts ngắn hơn. Trên ToolEval, số lượng token trung bình cần thiết cho mỗi công cụ được giảm 58% và 69% cho I2-Category và I3-Instruction. Với prompts ngắn hơn, TOOLDECcó thể bao gồm nhiều công cụ hơn trong kho mà không vượt quá giới hạn ngữ cảnh của LLM.
Tất cả các mô hình đều có lỗi cú pháp. Fine-tuning hoặc prompting không thể loại bỏ chúng, nhưng TOOL DEC
có thể. Khi đánh giá trên tập dữ liệu ToolEval: I3-Instruction, ToolLLM + DFSDT—mặc dù được fine-tuned với dữ liệu được định dạng giống hệt với dữ liệu kiểm tra—vẫn tạo ra lỗi cú pháp trong gần một nửa (49%) các trường hợp. Mistral thậm chí còn tệ hơn, với lỗi cú pháp xảy ra trong hơn 90% phản hồi của nó. Tỷ lệ lỗi cao này có thể giải thích tại sao nó tác động trực tiếp đến độ chính xác của nó chính xác là 0. Ngay cả các mô hình tiên tiến như ChatGPT và GPT-4, được biết đến với khả năng của chúng, cũng không miễn nhiễm với các sai lầm cú pháp trong tương tác công cụ. Điều tương tự cũng đúng với các benchmark khác và các mô hình cơ sở khác. Khi TOOLDECđược áp dụng, cả các mô hình chuyên gia như ToolLLM và các mô hình tổng quát đều ngừng mắc lỗi cú pháp.
8

--- TRANG 9 ---
Bảng 2: Trong khi các mô hình chuyên về công cụ, như ToolLLM, có thể hoạt động tốt như các mô hình tổng quát
+ TOOLDEC(†), hiệu suất của chúng trên các benchmark lý luận khác thấp hơn nhiều (được gạch chân).
ToolEval (I2-Cat) ToolEval (I2-Ins) GSM8K HumanEval MBPP BBH
ToolLLM-7B 36.5 49.0 1.2 2.4 5.3 28.1
Llama-2-chat-7B † 39.0 51.0 23.1 14.6 29.6 39.9
Mistral-Ins-7B † 42.0 53.0 42.0 43.9 43.4 50.6
I2-Category I3-Instruction0102030405060Win Rate (%)
0 0 0.0 0.543.553.0
40.560.0
(a) Tỷ lệ win của Mistral trên I2-Category và I3-
Instruction dưới bốn cài đặt khác nhau.
I2-Category I3-Instruction0102030405060Win Rate (%)36.541.0
36.549.046.561.0
44.062.0
Không Có Ràng buộc
In-Context
Decoding
Cả hai(b) Tỷ lệ win của ToolLLM trên I2-Category và I3-
Instruction dưới bốn cài đặt khác nhau.
Hình 6: Tỷ lệ win của Mistral và ToolLLM dưới các cài đặt khác nhau của ràng buộc cú pháp. Các ràng buộc cú pháp hữu ích hơn nhiều như các ràng buộc decoding hơn là mô tả in-context.
4.4 Nghiên cứu Phân tích
TOOL DECvs. Fine-Tuning Chuyên biệt Công cụ. Chúng tôi lập luận rằng việc sử dụng TOOLDEC trên LLM tổng quát lý tưởng hơn thay vì fine-tuning các LLM chuyên gia để sử dụng công cụ. Để chứng minh điều đó, chúng tôi đánh giá cả ToolLLM chuyên gia và Llama-2-chat và Mistral-Ins tổng quát trên các benchmark lý luận và mã hóa khác, bao gồm GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), và BigBenchHard (Suzgun et al., 2022). Chi tiết của những đánh giá này có thể được tìm thấy trong Phụ lục A.4. Như được báo cáo trong Bảng 2, mặc dù ToolLLM chuyên gia có thể có hiệu suất tương đương trên các benchmark liên quan đến công cụ như TOOLDECtrên LLM tổng quát, hiệu suất của nó trên các benchmark lý luận hoặc mã hóa tổng quát là không hợp lý tệ. Trên GSM8K, nó chỉ giải quyết 1,2% các vấn đề, trong khi Llama-2-chat, mô hình tổng quát với cùng tiền huấn luyện, giải quyết 23,1%. Điều này cho thấy rằng fine-tuning chuyên biệt công cụ có thể làm suy giảm nghiêm trọng khả năng tổng quát của mô hình, có thể do quên lãng thảm khốc.
TOOL DECvs. Prompting Ràng buộc Cú pháp . Chúng tôi lập luận rằng việc loại bỏ các ràng buộc cú pháp trong prompts không gây hại cho hiệu suất của TOOLDEC. Chúng tôi tiến hành một nghiên cứu phân tích chi tiết về hai cách tiếp cận (prompting và decoding) để kết hợp các ràng buộc cú pháp với hai mô hình cơ sở – ToolLLM và Mistral. Chúng tôi xem xét bốn cài đặt khác nhau của ràng buộc cú pháp: "không prompting, không decoding", "chỉ decoding", "chỉ prompting", "prompting và decoding". Chúng tôi đánh giá tỷ lệ win của ToolLLM và Mistral dưới bốn cài đặt này và báo cáo kết quả trong Hình 6. Các quan sát sau có thể được thực hiện: 1) Các ràng buộc cú pháp thời gian decoding hữu ích hơn nhiều so với các ràng buộc cú pháp in-prompt. Đối với cả hai mô hình, những cải thiện từ constrained decoding lớn hơn nhiều so với những cải thiện từ prompting các ràng buộc cú pháp. 2) Sử dụng các ràng buộc thời gian decoding hầu như đủ. Các ràng buộc in-context bổ sung ít giúp ích. Khoảng cách giữa cài đặt thứ ba, các ràng buộc decoding, và cài đặt thứ tư, cả hai ràng buộc đều rất nhỏ.
5 Kết luận
Chúng tôi đề xuất TOOLDEC, một thuật toán constrained decoding để hướng dẫn LLM sử dụng các công cụ bên ngoài.
Nó đảm bảo rằng LLM không mắc lỗi cú pháp khi tạo ra một lời gọi công cụ. TOOLDECbổ sung cho các cách tiếp cận hiện có và cải thiện hiệu suất của tất cả các LLM cơ sở mà chúng tôi đã kiểm tra. Đáng ngạc nhiên, TOOLDECcó thể cho phép các LLM tổng quát tốt như hoặc thậm chí tốt hơn các mô hình chuyên biệt công cụ. Vì các mô hình ngôn ngữ rất có khả năng, như GPT-4, vẫn có thể mắc lỗi cú pháp, chúng tôi tin rằng TOOLDECcó thể được sử dụng trong một số lĩnh vực nhạy cảm về bảo mật để tránh lỗi cú pháp, do đó có tác động tích cực đến xã hội. Tuy nhiên, chúng tôi thừa nhận rằng không có lỗi cú pháp không có nghĩa là không có lỗi. LLM với TOOLDECvẫn có thể mắc sai lầm ngoài cú pháp và nên được sử dụng cẩn thận.
9

--- TRANG 10 ---
Tài liệu tham khảo
Anderson, P., Fernando, B., Johnson, M., and Gould, S. Guided open vocabulary image captioning
with constrained beam search. In Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing , pp. 936–945, Copenhagen, Denmark, September 2017. Association
for Computational Linguistics. doi: 10.18653/v1/D17-1098. URL https://aclanthology.
org/D17-1098 .
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M.,
Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 ,
2021.
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche,
G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from
trillions of tokens. In International conference on machine learning , pp. 2206–2240. PMLR, 2022.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,
P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural
information processing systems , 33:1877–1901, 2020.
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y .,
Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint
arXiv:2107.03374 , 2021.
Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling
computation from reasoning for numerical reasoning tasks. arXiv e-prints , pp. arXiv–2211, 2022.
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y .,
Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with
90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .
Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton,
J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv
preprint arXiv:2110.14168 , 2021.
Eisner, J. Parameter estimation for probabilistic finite-state transducers. In Proceedings of the 40th
Annual Meeting of the Association for Computational Linguistics , pp. 1–8, 2002.
Fredkin, E. Trie memory. Communications of the ACM , 3(9):490–499, 1960.
Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y ., Callan, J., and Neubig, G. Pal: Program-
aided language models. In International Conference on Machine Learning , pp. 10764–10799.
PMLR, 2023.
Geng, S., Josifoski, M., Peyrard, M., and West, R. Grammar-constrained decoding for structured nlp
tasks without finetuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing , pp. 10932–10952, 2023.
Gupta, T. and Kembhavi, A. Visual programming: Compositional visual reasoning without training.
ArXiv , abs/2211.11559, 2022.
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model
pre-training. In International conference on machine learning , pp. 3929–3938. PMLR, 2020.
Hao, S., Liu, T., Wang, Z., and Hu, Z. Toolkengpt: Augmenting frozen language models with massive
tools via tool embeddings. arXiv preprint arXiv:2305.11554 , 2023.
Hokamp, C. and Liu, Q. Lexically constrained decoding for sequence generation using grid beam
search. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pp. 1535–1546, Vancouver, Canada, July 2017. Association for Computa-
tional Linguistics. doi: 10.18653/v1/P17-1141. URL https://aclanthology.org/P17-1141 .
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand,
F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L.,
Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023.
10

--- TRANG 11 ---
Kalo, J.-C. and Fichtel, L. Kamel : Knowledge analysis with multitoken entities in language models.
Automated Knowledge Base Construction , 2022.
Lu, X., West, P., Zellers, R., Le Bras, R., Bhagavatula, C., and Choi, Y . Neurologic decoding:(un)
supervised neural text generation with predicate logic constraints. In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pp. 4288–4299, 2021.
Lu, X., Welleck, S., West, P., Jiang, L., Kasai, J., Khashabi, D., Le Bras, R., Qin, L., Yu, Y ., Zellers,
R., et al. Neurologic a* esque decoding: Constrained text generation with lookahead heuristics.
InProceedings of the 2022 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies , pp. 780–799, 2022.
Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozière, B., Schick,
T., Dwivedi-Yu, J., Celikyilmaz, A., et al. Augmented language models: a survey. arXiv preprint
arXiv:2302.07842 , 2023.
Miao, N., Zhou, H., Mou, L., Yan, R., and Li, L. Cgmh: Constrained sentence generation by
metropolis-hastings sampling. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 33, pp. 6834–6842, 2019.
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V .,
Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv
preprint arXiv:2112.09332 , 2021.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,
Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback.
Advances in Neural Information Processing Systems , 35:27730–27744, 2022.
Parisi, A., Zhao, Y ., and Fiedel, N. Talm: Tool augmented language models. arXiv preprint
arXiv:2205.12255 , 2022.
Qin, Y ., Liang, S., Ye, Y ., Zhu, K., Yan, L., Lu, Y ., Lin, Y ., Cong, X., Tang, X., Qian, B., et al.
Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint
arXiv:2307.16789 , 2023.
Rastogi, P., Cotterell, R., and Eisner, J. Weighting finite-state transductions with neural context.
InProceedings of the 2016 conference of the North American chapter of the Association for
Computational Linguistics: human language technologies , pp. 623–633, 2016.
Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and
Scialom, T. Toolformer: Language models can teach themselves to use tools. arXiv preprint
arXiv:2302.04761 , 2023.
Shen, Y ., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y . Hugginggpt: Solving ai tasks with chatgpt
and its friends in hugging face, 2023.
Song, Y ., Xiong, W., Zhu, D., Wu, W., Qian, H., Song, M., Huang, H., Li, C., Wang, K., Yao, R.,
Tian, Y ., and Li, S. Restgpt: Connecting large language models with real-world restful apis, 2023.
Stengel-Eskin, E., Rawlins, K., and Durme, B. V . Zero and few-shot semantic parsing with ambiguous
inputs, 2024.
Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y ., Chung, H. W., Chowdhery, A., Le, Q. V .,
Chi, E. H., Zhou, D., , and Wei, J. Challenging big-bench tasks and whether chain-of-thought can
solve them. arXiv preprint arXiv:2210.09261 , 2022.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,
N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and
efficient foundation language models, 2023.
Willard, B. T. and Louf, R. Efficient guided generation for llms. arXiv preprint arXiv:2307.09702 ,
2023.
11

--- TRANG 12 ---
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y . React: Synergizing
reasoning and acting in language models, 2023.
Yin, P. and Neubig, G. A syntactic neural model for general-purpose code generation. In Proceedings
of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pp. 440–450, 2017.
Yuan, S., Song, K., Chen, J., Tan, X., Shen, Y ., Kan, R., Li, D., and Yang, D. EASYTOOL: Enhancing
LLM-based Agents with Concise Tool Instruction, January 2024. URL http://arxiv.org/
abs/2401.06201 . arXiv:2401.06201 [cs].
12

--- TRANG 13 ---
A Phụ lục
A.1 Ví dụ về Lỗi Cú pháp Công cụ
Câu hỏi: Tôi muốn làm bạn bè ngạc nhiên với một số meme và truyện cười Chuck Norris hài hước trong đêm chơi game hàng tuần của chúng tôi.Công cụ: jokes_random_from_chuck_norris, jokes_search_from_chuck_norris, get_random_joke_from_world_of_jokes, search_gifs_from_humor_jokes_and_memesThought: Lấy một meme Chuck NorrisAction: search_gifs_from_humor_jokes_and_memesAction Input: {"query": "Chuck Norris"}Response: {'images': [{'url': 'https://media.tenor.c…Thought: Lấy một meme Chuck NorrisAction: gifs_from_humor_jokes_and_memesAction Input: {"query": "funny"}Lỗi: Không có tên hàm như vậyCâu hỏi: Tôi đang làm dự án trường và cần chuyển đổi video YouTube thành MP3. Cung cấp liên kết tải xuống cho tệp MP3 của video có ID 'UxxajLWwzqY' bằng YouTube MP3 API. Ngoài ra, tạo một mã QR ngẫu nhiên chỉ để thử nghiệm.Công cụ: get_mp3_from_youtube_mp3, get_qr_code_from_tvb_qr_codeThought: Tạo một mã QR ngẫu nhiênAction: get_qr_code_from_tvb_qr_codeAction Input: {}Response: \\ufffdPNG\\r...Thought: Tạo một mã QR ngẫu nhiênAction: get_qr_code_from_tvb_qr_codeAction Input: {"is_id": "'UxxajLWwzqY"}Lỗi: Tham số từ khóa không mong muốnLỗi Tên Hàm
Lỗi Tham số Hàm…
Câu hỏi: Tôi cảm thấy hơi buồn và cần được cười. Bạn có thể tìm cho tôi một meme lập trình vui nhộn và một câu chuyện cười về lợn biển hài hước để làm sáng ngày của tôi không?Action: FinishAction Input: {"final_answer": "'…"}Finish with Answer: Đây là một cái gì đó vui nhộn…Action Input: FinishAction Input: {"final_answer": "'…"}Lỗi: Không thể phân tích phản hồiLỗi Cú pháp ReAct…
Hình 7: TOOLDECcó thể ngăn chặn lỗi tên hàm, lỗi tham số hàm, và cú pháp ReAct không hợp lệ trên ToolLLM.
13

--- TRANG 14 ---
A.2 Mã giả Thuật toán
Thuật toán 1 Finite-State Machine Guided Decoding cho Mô hình Ngôn ngữ
Đầu vào: dataxi, size m
Một DFSM được định nghĩa bởi (S, V, g, s 0, R);
Một mô hình ngôn ngữ Mtạo ra phân phối của token tiếp theo cho một chuỗi tiền tố;
Một chuỗi token ban đầu x1..k, đại diện cho prompt từ người dùng.
Đầu ra: dataxi, size m
s←s0
while s̸∈Fdo
Vs← {a|a∈V∧g(s, a)được định nghĩa }
P(xk+1|x1..k)←M(x1..k)
˜P(x=a|x1..k, s)←(P(x=a|x1..k)P
a′∈VsP(x=a′|x1..k), a ∈Vs
0, nếu không
x∼˜P(x|x1..k, s)
xk+1←x
k←k+ 1
s←g(s, x)
end while
return x1..k
A.3 Prompt để Loại bỏ Ràng buộc Cú pháp
Người dùng sẽ cung cấp cho bạn một danh sách các hàm trong JSON và bạn sẽ đơn giản hóa
mô tả của chúng. Đối với mỗi hàm, viết một mô tả ngắn gọn nhưng phong phú về mặt ngữ nghĩa
về mục đích của nó nhưng bạn không cần đề cập đến công cụ mà nó thuộc về. Liệt kê các tham số,
một tham số trên mỗi dòng, và viết một mô tả ngắn gọn nhưng phong phú về mặt ngữ nghĩa.
Bạn không cần bao gồm thông tin cú pháp (như loại tham số) nhưng vui lòng bao gồm giá trị ví dụ
nếu có. Phản hồi của bạn phải bằng văn bản thuần túy và được bao bọc trong một khối mã.
Đây là một ví dụ. Vui lòng tuân theo cùng cú pháp:
1. airport_arrivals_for_flight_fare_search
Mô tả: Truy xuất thông tin về các chuyến bay đến.
Tham số:
- airportcode: Mã sân bay (Ví dụ: LHR).
- carriercode: Mã hãng hàng không (Tùy chọn).
- date: Ngày để kiểm tra lượt đến (Tùy chọn).
A.4 Chi tiết Đánh giá
Chúng tôi báo cáo tỷ lệ win dưới cài đặt ReAct trên ToolEval. Chúng tôi sử dụng lm-eval-harness ( https://
github.com/EleutherAI/lm-evaluation-harness ) để đánh giá LLM trên GSM8K (5-shot) và
BigBenchHard (3-shot, CoT) bằng các tham số mặc định. Chúng tôi sử dụng eval-plus ( https://github.
com/evalplus/evalplus/ ) để đánh giá LLM trên HumanEval và MBPP với greedy decoding
và báo cáo kết quả pass@1.
14
