# 2407.12325.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2407.12325.pdf
# File size: 445329 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Optimizing Query Generation for Enhanced Document Retrieval in RAG
Hamin Koo*
Independent
hamin2065@google.comMinseon Kim
KAIST
minseonkim@kaist.ac.krSung Ju Hwang
KAIST, DeepAuto.ai
sjhwang82@kaist.ac.kr
Abstract
Large Language Models (LLMs) excel in vari-
ous language tasks but they often generate in-
correct information, a phenomenon known as
"hallucinations". Retrieval-Augmented Genera-
tion (RAG) aims to mitigate this by using docu-
ment retrieval for accurate responses. However,
RAG still faces hallucinations due to vague
queries. This study aims to improve RAG
by optimizing query generation with a query-
document alignment score, refining queries us-
ing LLMs for better precision and efficiency of
document retrieval. Experiments have shown
that our approach improves document retrieval,
resulting in an average accuracy gain of 1.6%.
1 Introduction
Although Large Language Models (LLMs) demon-
strate surprising performance in diverse language
tasks, hallucinations in LLMs have become an in-
creasingly critical problem. Hallucinations occur
when LLMs generate incorrect or misleading in-
formation, which can significantly undermine their
reliability and usefulness. One approach to mitigate
this problem is Retrieval-Augmented Generation
(RAG) (Lewis et al., 2021), which leverages docu-
ment retrieval to provide more accurate answers to
user queries by grounding the generated responses
in factual information from retrieved documents.
However, an incomplete RAG system often in-
duces hallucinations due to vague queries that fail
to accurately capture the user’s intent (Zhang et al.,
2023), highlighting a significant limitation of RAG
in LLMs (Niu et al., 2024; Wu et al., 2024). The
performance of RAG heavily depends on the clarity
of the queries, with short or ambiguous queries neg-
atively impacting search results (Jagerman et al.,
2023). Recent studies (Wang et al., 2023; Jager-
man et al., 2023) have demonstrated that query
*This work was done while the author was an intern at
KAIST MLAI.
Original QueryOriginal Query++Top-3 rephrasedQueries&ScoreRetrievalModelLLM as Optimizers
Document  Collection
Top-k docs&Model Score
Rephrased Query
QueryBucket
Figure 1: Concept figure of QOQA. Given expansion query
with top-k docs, we add top-3 rephrased queries and scores to
LLM. We optimize the query based on the scores and generate
the rephrased query.
expansion using LLMs can enhance the retrieval
of relevant documents. Pseudo Relevance Feed-
back (PRF) (Lavrenko and Croft, 2001; Lv and
Zhai, 2009) further refines search results by au-
tomatically modifying the initial query based on
top-ranked documents, without requiring explicit
user input. By assuming the top results are rele-
vant, PRF enhances the query, thereby improving
the accuracy of subsequent retrievals.
To address this issue, our goal is to generate con-
crete and precise queries for document retrieval in
RAG systems by optimizing the query. We pro-
pose QueryOptimization using Query exp Ansion
(QOQA ) for precise query for RAG systems. We
employ a top-k averaged query-document align-
ment score to refine the query using LLMs. This
approach is computationally efficient and improves
the precision of document retrieval, thereby reduc-
ing hallucinations. In our experiments, we demon-
strate that our approach enables the extraction of
correct documents with an average gain of 1.6%.
2 Related Works
Hallucination in RAG Despite the vast training
data of large language models (LLMs), the issue
1arXiv:2407.12325v1  [cs.IR]  17 Jul 2024

--- PAGE 2 ---
of hallucination of LLM continues to undermine
user belief. Among the strategies to mitigate, the
Retrieval-Augmented Generation (RAG) method
has proven effective in reducing hallucinations, en-
hancing the reliability and factual consistency of
LLM outputs, thus ensuring accuracy and relevance
in response to user queries (Shuster et al., 2021;
Béchard and Ayala, 2024). However, RAG does
not thoroughly eliminate hallucinations (Béchard
and Ayala, 2024; Niu et al., 2024) that encouraged
further refined RAG systems for lowered hallucina-
tion. LLM-Augmenter (Peng et al., 2023) leverages
external knowledge and automated feedback via
Plug and Play (Li et al., 2024) modules to enhance
model responses. Moreover, EVER (Kang et al.,
2024) introduces a real-time, step-wise generation
and hallucination rectification strategy that vali-
dates each sentence during generation, preventing
the propagation of errors.
Query Expansion Query expansion improves
search results by modifying the original query with
additional relevant terms, helping to connect the
user’s query with relevant documents. There are
two primary query expansion approaches: retriever-
based and generation-based. Retriever-based ap-
proaches expand queries by using results from a
retriever, while generation-based methods use ex-
ternal data, such as large language models (LLMs),
to enhance queries.
Several works (Wang et al., 2023; Mackie et al.,
2023; Jagerman et al., 2023) leverage LLMs for
expanding queries. Query2Doc (Wang et al., 2023)
demonstrated that LLM-generated outputs added
to a query significantly outperformed simple re-
trievers. However, this approach can introduce
inaccuracies, misalignment with target documents,
and highly susceptibility to LLM hallucinations.
Retrieval-based methods (Lv and Zhai, 2010; Yan
et al., 2003; Li et al., 2023; Lei et al., 2024) en-
hance search query effectiveness by incorporat-
ing related terms or phrases, enriching the query
with relevant information. Specifically, CSQE (Lei
et al., 2024) uses an LLM to extract key sentences
from retrieved documents for query expansion,
creating task-adaptive queries, although this can
lead to excessively long queries. When compar-
ing CSQE-expanded queries with those evaluated
by BM25 (Robertson and Zaragoza, 2009) and re-
ranked using a cross-encoder (Wang et al., 2020)
from BEIR (Thakur et al., 2021), the performance
improvement is minimal.
My goal is to make rephrased query to retrieve answer documents with high scores.This is original query with top-5 retrieved docs.Query:CCL19 is absent within dLNs. TOP-5 retrieved docs:1. Immobilized chemokine fields …2. The sphingosine-1-phosphate …3. Chemokine-like receptor 1 (CMKLR1) …4. Lack of Absent in Melanoma 2 (AIM2) e…5. Mycobacterium tuberculosis and …  I have some examples of rephrased query along with their corresponding scores. The texts are arranged in ascending order based on their scores, where higher scores indicate better quality.revised query:The absence of CCL19 is observed in the draining lymph nodes (dLNs).score:0.0(... more exemplars)Write your new rephrased query that is different from the old ones and has a score as high as possible. Write the text in square brackets.Figure 2: Prompt template used in QOQA. The black texts
describe instructions of the optimizing task. The blue texts
are original query with top- Nretrieved documents with the
original query. The purple texts are revised queries by LLM
optimizer and scores.
3 Query Optimization using Query
Expansion
3.1 Query optimization with LLM
To optimize the query, we utilize a Large Language
Model (LLM) to rephrase the query based on its
score. Initially, we input the original query and re-
trieve Ndocuments using a retriever. Next, we con-
catenate the original query with the top Nretrieved
documents to create an expanded query, which is
then sent to the LLM to generate R0rephrased
queries. These rephrased queries are evaluated
for alignment with the retrieved documents, and
the pair of query-document alignment scores and
queries are stored in a query bucket. The alignment
score is determined using a retrieval model that
measures the correlation between the query and the
retrieved documents (Section 3.2).
We update the prompt template with the origi-
nal query, the retrieved documents, and the top K
rephrased queries, as illustrated in Figure 2. To
ensure improved performance than original query,
we always include the original query information
in the template. In the later optimization steps i,
based on the scores, we generate a Rirephrased
query and add it to the query bucket.
2

--- PAGE 3 ---
3.2 Query-document alignment score
To employ query-document alignment score in op-
timization step, we use three types of evaluation
scores: BM25 scores from sparse retrievals, dense
scores from dense retrievals, hybrid scores that
combine the sparse and dense retrievals.
Given query qi, and documents set D={dj}J
j=1
the BM25 alignment score is as follow,
BM25 (qi, D) =IDF(qi)·f(qi, D)·(k1+ 1)
f(qi, D) +k1·(1−b+b·|D|
AVGDL)(1)
where f(qi, D)is frequency of query terms in the
document D,|D|is the length of the document,
AVGDLis average document length, and k1andb
are default hyper-parameters from Pyserini (Lin
et al., 2021). IDF(qi)is inverse document fre-
quency term as follow,
IDF(qi) = logN−n(qi) + 0.5
n(qi) + 0.5(2)
where IDF(qi)is calculated with total number of
documents N, and n(qi)as number of documents
containing qi.
Dense score is relevance score between queries
and documents using learned dense representations,
i.e., embedding space. As both queries and doc-
uments are embedded into the high-dimensional
continuous vector space, alignment score Dense is
calculated as follow,
Dense (qi, dj) =Eqi·Edj (3)
where EqiandEdjare the dense embedding vec-
tors of the query qiand the document dj∈D,
respectively, from dense retrieval model. For
our experiment, we employ BAAI/bge-base-en-
v1.5 (Xiao et al., 2024) model.
Hybrid score combines both BM25 scores and
Dense scores by appropriately tuning parameters
of alpha αas follow,
Hybrid (qi, dj) =α·BM25(qi, D) +Dense (qi, dj).
(4)
4 Results
Dataset We evaluate on three retrieval datasets
from BEIR (Thakur et al., 2021): SciFact (Wadden
et al., 2020), Trec-Covid (V oorhees et al., 2021)
and FiQA (Maia et al., 2018). We evaluated on fact
checking task about scientific claims, Bio-medical
information retrieval, and question answering task
on financial domain, respectively.Table 1: Results of document retrieval task. All scores denote
nDCG@10. Bold indicates the best result across all models,
and the second best is underlined .
Scifact Trec-covid FiQA
Sparse Retrieval
BM25 67.9 59.5 23.6
+ RM3 (Lv and Zhai, 2009) 64.6 59.3 19.2
+ Q2D/PRF (Jagerman et al., 2023) 71.7 73.8 29.0
+ CSQE (Lei et al., 2024) 69.6 74.2 25.0
+ QOQA (BM25 score) 67.5 61.1 21.4
+ QOQA (Dense score) 69.7 48.4 23.6
+ QOQA (Hybrid score) 66.4 43.2 22.4
Dense Retrieval
BGE-base-1.5 74.1 78.2 40.7
+ CSQE (Lei et al., 2024) 73.7 78.2 40.1
+ QOQA (BM25 score) 75.4 60.6 37.4
+ QOQA (Dense score) 74.3 77.9 40.6
+ QOQA (Hybrid score) 73.9 79.2 40.0
Baseline (1) Sparse Retrieval: (a) BM25 (Robert-
son and Zaragoza, 2009) model is a widely-
used bag-of-words retrieval function that relies
on token-matching between two high-dimensional
sparse vectors, which use TF-IDF token weights.
We used default setting from Pyserini (Lin
et al., 2021). (b) BM25+RM3 (Robertson and
Zaragoza, 2009; Lv and Zhai, 2009) is query ex-
pansion method using PRF. We also include (c)
BM25+Q2D/PRF (Robertson and Zaragoza, 2009;
Jagerman et al., 2023) that use both LLM-based
and PRF query expansion methods. (2) Dense
Retrieval: (a) BGE-base-en-v1.5 model is a state-
of-the-art embedding model designed for various
NLP tasks like retrieval, clustering, and classifica-
tion. For dense retrieval tasks, we added ’Represent
this sentence for searching relevant passages:’ as a
query prefix, following the default setting from Py-
serini. (Lin et al., 2021). We also used CSQE (Lei
et al., 2024) for both sparse retrieval and dense
retrieval.
Implementation details We utilize GPT-3.5-
Turbo (OpenAI, 2024) as the LLM optimizer. The
temperature is set to 1.0. We set the max opti-
mization iteration as i= 1,2,···,50. We use
N= 5,K= 3,R0= 3, and Ri= 1. All hyper-
parameters of k1= 1.2,b= 0.75, and α= 0.1
are set to default values from Pyserini (Lin et al.,
2021).
Retrieval results compared to baselines Table 1
illustrates the performance of various document re-
trieval models across the SciFact, Trec-Covid, and
FiQA datasets. For dense retrieval, our enhanced
models (+QOQA variants) exhibit superior perfor-
mance. Notably, QOQA (BM25 score) achieves the
best result in SciFact with a score of 75.4, demon-
3

--- PAGE 4 ---
Table 2: Examples from SciFact, and FiQA dataset. Blue texts are overlapping keywords between answer document and
rephrased query.
Original query 0-dimensional biomaterials show inductive properties.
Rephrased query Do nano-sized biomaterials possess unique properties that can trigger specific reactions in biological systems?
Answer document’title’: ’New opportunities: the use of nanotechnologies to manipulate and track stem cells.’
’text’: ’Nanotechnologies are emerging platforms that could be useful in measuring,
understanding, and manipulating stem cells. Examples include magnetic nanoparticles and quantum dots
for stem cell labeling and in vivo tracking; nanoparticles, carbon nanotubes, and polyplexes
for the intracellular delivery of genes/oligonucleotides and protein/peptides;
and engineered nanometer-scale scaffolds for stem cell differentiation and transplantation.
This review examines the use of nanotechnologies for stem cell tracking, differentiation, and transplantation.
We further discuss their utility and the potential concerns regarding their cytotoxicity.’,
Original query what is the origin of COVID-19
Rephrased query What molecular evidence supports bats and pangolins as the likely origin hosts of the COVID-19 virus?
Answer document’title’: ’Isolation and characterization of a bat SARS-like coronavirus that uses the ACE2 receptor’
’text’: ’The 2002–3 pandemic caused by severe acute respiratory syndrome coronavirus (SARS-CoV)
. . . syndrome coronavirus (MERS-CoV)(2) suggests that this group of viruses remains a major threat and that their distribution
is wider than previously recognized. Although bats have been suggested as the natural reservoirs of both viruses(3–5), attempts
to isolate the progenitor virus of SARS-CoV from bats have been unsuccessful. Diverse SARS-like coronaviruses (SL-CoVs)
have now been reported from bats in China, Europe and Africa(5–8), but none are considered a direct progenitor of SARS-CoV
because of their phylogenetic disparity from this virus and the inability of their spike proteins (S) to use the SARS-CoV
cellular receptor molecule, the human angiotensin converting enzyme II (ACE2)(9,10).
Here, we report whole genome sequences of two novel bat CoVs from Chinese horseshoe bats (Family: Rhinolophidae)
in Yunnan, China; RsSHC014 and Rs3367. These viruses . . . which has typical coronavirus morphology, . . . tropism.
Our results provide the strongest evidence to date that Chinese horseshoe bats are natural reservoirs of SARS-CoV ,
and that intermediate hosts may not . . . ’
Table 3: Ablation study results on SciFact. This table
presents the performance impact of excluding expansion com-
ponent and optimization component from QOQA, illustrating
the importance of each module, in enhancing retrieval accu-
racy. All scores denote nDCG@10 value.
QOQA (BM25 score) QOQA (Dense score)
Sparse Retrieval
Ours 67.5 69.7
w/o expansion 65.6 66.0
w/o optimization 67.6 67.6
Dense Retrieval
Ours 75.4 74.3
w/o expansion 72.9 74.2
w/o optimization 73.2 72.6
strates strong performance in Trec-Covid with a
79.2 with hybrid score. The consistent performance
gain of our QOQA across different datasets high-
lights effectiveness in improving retrievals.
Case Analysis As shown in Table 2, rephrased
queries generated with QOQA are more precise and
concrete than the original queries. When search-
ing for the answer document, queries generated
with our QOQA method include precise keywords,
such as "nano" or "molecular evidence," to retrieve
the most relevant documents. This precision in
keyword usage ensures that the rephrased queries
share more common words with the answer docu-
ments. Consequently, the queries utilizing QOQA
demonstrate effectiveness in retrieving documents
that contain the correct answers, highlighting the
superiority of our approach in retrieval tasks.
Ablation Studies In our ablation study, we eval-
uate the impact of the expansion and optimizationcomponents in QOQA using both BM25 and Dense
scores by systematically removing each compo-
nent and observing the nDCG@10 results. We
remove the document expansion (Blue text in the
Figure 2) in the "w/o expansion" setup while retain-
ing the optimization step. In the "w/o optimization"
setup, we use single-step optimization as i= 1. As
shown in Table 3, the optimization step improves
the search for better rephrased queries. Moreover,
without the expansion component, performance sig-
nificantly drops, especially with the BM25 score.
This demonstrates the critical role of the expan-
sion component in creating high-quality rephrased
queries and enhancing document retrieval.
5 Conclusion
In this paper, we tackled the issue of hallucina-
tions in Retrieval-Augmented Generation (RAG)
systems by optimizing query generation. Utilizing
a top-k averaged query-document alignment score,
we refined queries using Large Language Models
(LLMs) to improve precision and computational
efficiency in document retrieval. Our experiments
demonstrated that these optimizations significantly
reduce hallucinations and enhance document re-
trieval accuracy, achieving an average gain of 1.6%.
This study highlights the significance of precise
query generation in enhancing the dependability
and effectiveness of RAG systems. Future work
will focus on integrating more advanced query re-
finement techniques and applying our approach to
4

--- PAGE 5 ---
a broader range of RAG applications.
References
Patrice Béchard and Orlando Marquez Ayala. 2024.
Reducing hallucination in structured outputs
via retrieval-augmented generation. Preprint ,
arXiv:2404.08189.
Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui
Wang, and Michael Bendersky. 2023. Query expan-
sion by prompting large language models. Preprint ,
arXiv:2305.03653.
Haoqiang Kang, Juntong Ni, and Huaxiu Yao. 2024.
Ever: Mitigating hallucination in large language mod-
els through real-time verification and rectification.
Preprint , arXiv:2311.09114.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance
based language models. In Proceedings of the 24th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval ,
SIGIR ’01, page 120–127, New York, NY , USA. As-
sociation for Computing Machinery.
Yibin Lei, Yu Cao, Tianyi Zhou, Tao Shen, and An-
drew Yates. 2024. Corpus-steered query expan-
sion with large language models. arXiv preprint
arXiv:2402.18031 .
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela. 2021.
Retrieval-augmented generation for knowledge-
intensive nlp tasks. Preprint , arXiv:2005.11401.
Hang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang
Ma, Jimmy Lin, and Guido Zuccon. 2023. Improv-
ing query representations for dense retrieval with
pseudo relevance feedback: A reproducibility study.
Preprint , arXiv:2112.06400.
Miaoran Li, Baolin Peng, Michel Galley, Jianfeng Gao,
and Zhu Zhang. 2024. Self-checker: Plug-and-play
modules for fact-checking with large language mod-
els.Preprint , arXiv:2305.14623.
Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-
Hong Yang, Ronak Pradeep, and Rodrigo Nogueira.
2021. Pyserini: A Python toolkit for reproducible
information retrieval research with sparse and dense
representations. In Proceedings of the 44th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR
2021) , pages 2356–2362.
Yuanhua Lv and ChengXiang Zhai. 2009. A compara-
tive study of methods for estimating query language
models with pseudo feedback. In Proceedings of the
18th ACM Conference on Information and Knowl-
edge Management , CIKM ’09, page 1895–1898, New
York, NY , USA. Association for Computing Machin-
ery.Yuanhua Lv and ChengXiang Zhai. 2010. Positional
relevance model for pseudo-relevance feedback. In
Proceedings of the 33rd International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval , SIGIR ’10, page 579–586, New
York, NY , USA. Association for Computing Machin-
ery.
Iain Mackie, Shubham Chatterjee, and Jeffrey Dalton.
2023. Generative relevance feedback with large lan-
guage models. In Proceedings of the 46th Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval , SIGIR ’23, page
2026–2031, New York, NY , USA. Association for
Computing Machinery.
Macedo Maia, Siegfried Handschuh, André Freitas,
Brian Davis, Ross McDermott, Manel Zarrouk, and
Alexandra Balahur. 2018. Www’18 open challenge:
Financial opinion mining and question answering. In
Companion Proceedings of the The Web Conference
2018 , WWW ’18, page 1941–1942, Republic and
Canton of Geneva, CHE. International World Wide
Web Conferences Steering Committee.
Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun
Shum, Randy Zhong, Juntong Song, and Tong Zhang.
2024. Ragtruth: A hallucination corpus for develop-
ing trustworthy retrieval-augmented language models.
Preprint , arXiv:2401.00396.
OpenAI. 2024. Gpt-4 technical report. Preprint ,
arXiv:2303.08774.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,
Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou
Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check
your facts and try again: Improving large language
models with external knowledge and automated feed-
back. Preprint , arXiv:2302.12813.
Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Found. Trends Inf. Retr. , 3(4):333–389.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. Preprint ,
arXiv:2104.07567.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. Beir:
A heterogenous benchmark for zero-shot evalua-
tion of information retrieval models. arXiv preprint
arXiv:2104.08663 .
Ellen V oorhees, Tasmeer Alam, Steven Bedrick, Dina
Demner-Fushman, William R. Hersh, Kyle Lo, Kirk
Roberts, Ian Soboroff, and Lucy Lu Wang. 2021.
Trec-covid: constructing a pandemic information re-
trieval test collection. SIGIR Forum , 54(1).
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu
Wang, Madeleine van Zuylen, Arman Cohan, and
Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying
5

--- PAGE 6 ---
scientific claims. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 7534–7550, Online. As-
sociation for Computational Linguistics.
Liang Wang, Nan Yang, and Furu Wei. 2023.
Query2doc: Query expansion with large language
models. arXiv preprint arXiv:2303.07678 .
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao,
Nan Yang, and Ming Zhou. 2020. Minilm: Deep
self-attention distillation for task-agnostic com-
pression of pre-trained transformers. Preprint ,
arXiv:2002.10957.
Kevin Wu, Eric Wu, and James Zou. 2024. Clashe-
val: Quantifying the tug-of-war between an llm’s
internal prior and external evidence. Preprint ,
arXiv:2404.10198.
Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-
nighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:
Packaged resources to advance general chinese em-
bedding. Preprint , arXiv:2309.07597.
Rong Yan, Alexander Hauptmann, and Rong Jin. 2003.
Multimedia search with pseudo-relevance feedback.
InProceedings of the 2nd International Confer-
ence on Image and Video Retrieval , CIVR’03, page
238–247, Berlin, Heidelberg. Springer-Verlag.
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, et al. 2023. Siren’s song in the ai ocean:
a survey on hallucination in large language models.
arXiv preprint arXiv:2309.01219 .
6
