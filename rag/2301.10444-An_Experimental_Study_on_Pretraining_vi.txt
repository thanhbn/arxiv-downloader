# 2301.10444.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2301.10444.pdf
# Kích thước tập tin: 598115 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================


--- TRANG 1 ---
Nghiên cứu Thực nghiệm về Việc Tiền huấn luyện
Transformers từ Đầu cho IR
Carlos Lassance, Hervé Dejean, và Stéphane Clinchant
Naver Labs Europe, Meylan, France
{carlos.lassance,herve.dejean,stephane.clinchant}@naverlabs.com
Tóm tắt. Việc tinh chỉnh các Mô hình Ngôn ngữ Tiền huấn luyện (PLM) cho IR đã
trở thành thông lệ tiêu chuẩn kể từ khi chúng đạt được hiệu quả đột phá vài năm trước.
Nhưng, phương pháp này có được hiểu rõ không? Trong bài báo này, chúng tôi nghiên
cứu tác động của bộ sưu tập tiền huấn luyện đến hiệu quả IR cuối cùng. Đặc biệt, chúng
tôi thách thức giả thuyết hiện tại rằng PLM phải được huấn luyện trên một bộ sưu tập
chung đủ lớn và chúng tôi chỉ ra rằng việc tiền huấn luyện từ đầu trên bộ sưu tập quan
tâm có tính cạnh tranh đáng ngạc nhiên với phương pháp hiện tại. Chúng tôi đánh giá
các ranker giai đoạn đầu và cross-encoders cho reranking trên nhiệm vụ truy xuất đoạn
văn tổng quát trên MSMARCO, Mr-Tydi cho tiếng Ả Rập, Nhật Bản và Nga, và
TripClick cho miền cụ thể. Trái với niềm tin phổ biến, chúng tôi chỉ ra rằng, đối với
việc tinh chỉnh các ranker giai đoạn đầu, các mô hình được tiền huấn luyện chỉ trên bộ
sưu tập của chúng có hiệu quả tương đương hoặc tốt hơn so với các mô hình tổng quát
hơn. Tuy nhiên, có một sự giảm hiệu quả nhẹ đối với các reranker được tiền huấn luyện
chỉ trên bộ sưu tập mục tiêu. Nhìn chung, nghiên cứu của chúng tôi làm sáng tỏ vai trò
của bộ sưu tập tiền huấn luyện và sẽ khiến cộng đồng chúng ta suy ngẫm về việc xây
dựng các mô hình chuyên biệt bằng cách tiền huấn luyện từ đầu. Cuối cùng nhưng
không kém phần quan trọng, làm như vậy có thể cho phép kiểm soát tốt hơn về hiệu
quả, độ lệch dữ liệu và khả năng tái tạo, đây là những câu hỏi nghiên cứu quan trọng
cho cộng đồng IR.
Từ khóa: Mô hình Ngôn ngữ Tiền huấn luyện · Transformers · IR

1 Giới thiệu
Các mô hình Transformers là đột phá chính trong trí tuệ nhân tạo trong năm năm qua.
Việc tiền huấn luyện các mô hình transformers với Mô hình hóa Ngôn ngữ Che dấu
(MLM), một dạng tự giám sát, như được đề xuất trong mô hình BERT đầu tiên [11] đã
dẫn đến cải tiến lớn trong xử lý ngôn ngữ tự nhiên, thị giác máy tính và các lĩnh vực
khác. Tiền huấn luyện và tự giám sát sau đó đã mở đường cho cuộc đua về các mô
hình nền tảng lớn hơn. Truy xuất Thông tin (IR) đã đi theo cùng một quỹ đạo, nơi các
Mô hình Ngôn ngữ Tiền huấn luyện (PLM) đã vượt xa các mô hình neural trước đó
[13,15,30,23] mà còn cả các phương pháp bag-of-words truyền thống như BM25 [46].
Những tiến bộ này đều được thực hiện nhờ sự kết hợp của các tập dữ liệu lớn, PLMs
như BERT, nhưng cũng có các tiên nghiệm đến từ các phương pháp IR truyền thống
như BM25.arXiv:2301.10444v1 [cs.IR] 25 Jan 2023

--- TRANG 2 ---
2 C. Lassance et al.
Tuy nhiên, những PLM này không hoạt động tốt ngay từ đầu cho Truy xuất Thông tin
(ngược lại với NLP¹) vì chúng yêu cầu một quy trình tinh chỉnh đáng kể. Trong IR, có
hai loại PLM: cross-encoders để rerank một tập k tài liệu hàng đầu và dual encoders
để xử lý việc truy xuất giai đoạn đầu hiệu quả [30]. Tuy nhiên, nhiệm vụ tiền huấn
luyện MLM tiêu chuẩn có thể không phải là nhiệm vụ tốt nhất cho IR như đã tranh
luận trong [14]. Hơn nữa, một xu hướng ngày càng tăng là giới thiệu bước "middle-
training" [14] để thu hẹp khoảng cách này và thích ứng PLM không chỉ với lĩnh vực
truy xuất, mà còn với cách thức các câu sẽ được mã hóa. Ví dụ [15,14,34] thích ứng
loss Mô hình hóa Ngôn ngữ Che dấu (MLM), để mô hình học cách nén thông tin trên
token CLS, sẽ được sử dụng như embedding câu de-facto trong quá trình tinh chỉnh.
Tương tự, một số nhiệm vụ middle training đã được đề xuất để phù hợp tốt hơn với
các nhiệm vụ IR hoặc bằng cách sử dụng giám sát yếu.

Một mặt, có vẻ như có một niềm tin lan rộng rằng hiệu quả downstream chủ yếu do
việc tiền huấn luyện trên một bộ sưu tập bên ngoài lớn. Ví dụ, báo cáo mô hình nền
tảng [4] tuyên bố rằng "AI đang trải qua một sự thay đổi mô hình với sự nổi lên của
các mô hình (ví dụ: BERT, DALL-E, GPT-3) được huấn luyện trên dữ liệu rộng ở
quy mô lớn và có thể thích ứng với nhiều nhiệm vụ downstream." Mặt khác, quá trình
middle training dường như mâu thuẫn với quan điểm trước đó. Đây là lý do tại sao,
trong bài báo này, chúng tôi nhằm điều tra các câu hỏi sau: liệu chúng ta thực sự có
cần tiền huấn luyện quy mô lớn cho Neural IR không? Có bao nhiêu kiến thức thực
sự được mã hóa từ bộ sưu tập tiền huấn luyện lớn? Bên cạnh đó, những gì được biết
về tiền huấn luyện trong IR chủ yếu bị giới hạn bởi thiết lập MSMARCO và do đó
một câu hỏi liên quan là làm thế nào để giải quyết việc tiền huấn luyện mô hình ngôn
ngữ cho các ngôn ngữ mới hoặc lĩnh vực mới khi nói đến IR.

Trong bài báo này, chúng tôi nhằm xác minh xem những quan niệm tiên nghiệm này
có cần thiết không, hoặc chúng ta có thể chỉ kết hợp các bước tiền huấn luyện và
middle training để tạo ra PLM đã được thích ứng với vấn đề hiện tại với chi phí nhỏ
hơn so với việc thực hiện cả hai riêng biệt. Nhìn chung, bài báo này đóng góp những
điều sau:

1. Chúng tôi nghiên cứu transformers tiền huấn luyện từ đầu trên các bộ sưu tập IR;
2. Chúng tôi chỉ ra rằng các ranker giai đoạn đầu, được tiền huấn luyện trên MSMARCO, có hiệu quả bằng hoặc thậm chí tốt hơn trong lĩnh vực (MSMARCO), trong khi ngoài lĩnh vực những mô hình đó khái quát hóa cũng tốt (truy xuất thưa) hoặc tệ hơn (truy xuất dày đặc);
3. Chúng tôi đánh giá cross encoders được tiền huấn luyện từ đầu và xác minh rằng chúng thực sự được hưởng lợi từ tiền huấn luyện bên ngoài;
4. Chúng tôi chỉ ra rằng các retriever giai đoạn đầu, được huấn luyện từ đầu trên bộ sưu tập mục tiêu, có tính cạnh tranh hoặc vượt trội so với các mô hình cụ thể miền (ví dụ: SciBERT trên TripClick) và mô hình đa ngôn ngữ (ví dụ: MContriever trên Mr. TiDy);
5. Các biến thể của kiến trúc Transformers, như DeBERTa, được cho là tốt hơn trong các benchmark NLP không mang lại lợi ích cho IR, ngay cả khi được huấn luyện từ đầu.

¹Ví dụ, việc đóng băng mã hóa BERT và học thêm một lớp tuyến tính là đủ để có được hiệu suất tốt trong NLP [11], trong khi cách tiếp cận như vậy không hiệu quả trong IR.

--- TRANG 3 ---
Tiền huấn luyện Transformers từ Đầu 3

2 Công trình Liên quan
PLMs trong IR: ngày nay, thông lệ tiêu chuẩn của nhiều nhà nghiên cứu IR là chỉ
đơn giản tải xuống một mô hình tiền huấn luyện hiện có để tinh chỉnh nó trên nhiệm
vụ truy xuất của họ. Sau thành công của chúng trong reranking, PLMs đã được áp
dụng cho ranking giai đoạn đầu với mạng bi-encoder để giải quyết yêu cầu hiệu quả
[25,30,43] hoặc với late interaction [26]. Một số chiến lược huấn luyện đã được đề
xuất để cải thiện hiệu quả của bi-encoders, như distillation [20,31,22,48] và hard
negative mining [53,42,45]. Song song với những phát triển này, một hướng nghiên
cứu khác nhằm học các biểu diễn thưa từ PLM để hoạt động như khớp từ vựng. COIL
[16] (sau này được cải thiện trong uniCOIL [29]) học các biểu diễn dày đặc mức từ
để thực hiện khớp từ vựng theo ngữ cảnh. SPLADE [13,28] trực tiếp học biểu diễn
thưa chiều cao nhờ head MLM của PLM và sự trợ giúp của chính quy hóa thưa. Đáng
chú ý nhất, SPLADE đạt được hiệu quả tiên tiến nhất trên benchmark zero-shot BEIR
[51], sau đó bị vượt qua bởi các phương pháp khác với nhiều tính toán hơn [36].

Sự nổi lên của middle training: một số công trình gần đây đề xuất thực hiện một bước
tiền huấn luyện bổ sung, trước giai đoạn tinh chỉnh cuối cùng, một quy trình mà
chúng tôi sẽ gọi ở đây là middle training. Lý do là trọng số PLM hoặc cơ chế pooling
CLS của nó không phù hợp cho các nhiệm vụ truy xuất hoặc tương tự thường được
sử dụng trong IR. Hai ý tưởng chính nổi lên từ tài liệu này: i) sử dụng loss contrastive
trên các span tài liệu khác nhau, và ii) sử dụng information bottleneck để điều kiện
hóa tốt hơn mạng để dựa vào biểu diễn CLS của nó để thực hiện dự đoán [27]. Trong
[6], bài báo so sánh Inverse Cloze Task, Wiki Link Prediction và Body First Selection.
Kết quả của họ cho thấy sự kết hợp của tất cả các nhiệm vụ này có lợi so với chỉ tiền
huấn luyện MLM. Trong [35], hyperlinks được sử dụng cho tiền huấn luyện. Một tiền
huấn luyện khác dựa vào cấu trúc trang web và DOM của chúng trong mô hình
WebFormer [18]. Hơn nữa, Contriever [23] dựa vào loss contrastive từ các span văn
bản khác nhau, tương tự như Co-Condenser [15]. Co-Condenser mở rộng ý tưởng
Condenser [14] tập trung vào middle training token CLS. Gần đây, Retro-MAE [32]
xem xét lại cùng ý tưởng, bằng cách che dấu hai lần một đoạn văn đầu vào để việc
che dấu đầu tiên tạo ra biểu diễn CLS được tái sử dụng để giải mã việc che dấu thứ
hai của đoạn văn. Tiền huấn luyện cho các mô hình thưa đã được điều tra gần đây
trong [28] để điều kiện hóa tốt hơn mạng với tinh chỉnh SPLADE. Ý tưởng là tái sử
dụng chính quy hóa FLOPS [41] được sử dụng trong tinh chỉnh trong middle training
MLM. Trong [1], 14 nhiệm vụ tiền huấn luyện khác nhau được so sánh khi huấn
luyện mô hình BERT, bao gồm dự đoán điểm số tfidf của tài liệu, đã được chỉ ra là
có lợi. Sau đó họ đánh giá trên một số nhiệm vụ NLP, bao gồm tương tự câu. Ngoài
ra, [34,33] đề xuất tiền huấn luyện với dự đoán từ đại diện: cho mỗi tài liệu một tập
từ quan trọng được định nghĩa bởi một số heuristics và mô hình được tiền huấn
luyện để dự đoán tập từ đó.

Trong khi tiền huấn luyện cho IR có vẻ rất thịnh hành, ý tưởng tiền huấn luyện biểu
diễn cho các nhiệm vụ IR thực tế có thể được truy tìm trước sự ra đời của PLM (hoặc
Foundation Models). Ví dụ, hơn mười năm trước, mô hình semantic indexing có giám

--- TRANG 4 ---
4 C. Lassance et al.
sát [2], đã sử dụng anchor hyperlinks để xây dựng triplets trong một nhiệm vụ contrastive, có thể được xem như tổ tiên của các nhiệm vụ tiền huấn luyện trên Wikipedia. Tương tự, giám sát yếu từ BM25 đã được sử dụng để tiền huấn luyện các mô hình neural IR [10] trước khi sử dụng PLM.

Mô hình nền tảng và kiến trúc: một dòng công trình liên quan lỏng lẻo là tài liệu luật
quy mô cho các mô hình ngôn ngữ tiền huấn luyện lớn [24]. Luật quy mô nhằm hiểu
cách kiến trúc và kích thước mô hình ảnh hưởng đến perplexity và độ chính xác của
mô hình ngôn ngữ. Trong [50], Tay et al. cho thấy perplexity là một thước đo dự đoán
kém về hiệu quả downstream và đề xuất ưu tiên chiều sâu hơn là chỉ chiều rộng (tức
là hidden size) trong một mô hình họ đặt tên là kiến trúc sâu-hẹp. Mặc dù câu hỏi
này có thể thú vị với công việc của chúng tôi, hầu hết tài liệu đều tập trung vào chế
độ dữ liệu và mô hình lớn, trong khi trong IR chúng tôi nhìn vào phía bên kia của
quang phổ với các bộ sưu tập/mô hình có kích thước nhỏ hoặc vừa phải vì mục đích
hiệu quả.

Cuối cùng, một công trình rất thú vị của Tay et al. [49] lập luận rằng tiền huấn luyện
và tiến bộ kiến trúc đã bị nhầm lẫn. Ngoài ra, họ chỉ ra rằng các mô hình convolutional
thực tế có tính cạnh tranh với transformers khi chúng được tiền huấn luyện trên cùng
bộ sưu tập và cho các nhiệm vụ không yêu cầu cross attention giữa hai câu (ví dụ:
mạng bi-encoder). Cuối cùng, họ lập luận rằng cách tiếp cận hiện tại là sai lạc và cả
kiến trúc và tiền huấn luyện nên được xem xét độc lập. Cuối cùng, [7] nghiên cứu tác
động của bộ sưu tập tiền huấn luyện cho dịch máy và trong [12] cho các nhiệm vụ
liên quan đến hình ảnh với các mô hình lớn. Công việc của chúng tôi liên quan đến
những công trình này, vì chúng tôi nghiên cứu tác động của bộ sưu tập tiền huấn
luyện đến hiệu quả cuối cùng của hệ thống IR.

3 Tiền huấn luyện từ Đầu
Tóm lại, vai trò của việc tinh chỉnh biểu diễn hoặc thực hiện middle training có vẻ
quan trọng đối với các hệ thống IR. Các biểu diễn PLM có vẻ quan trọng nhưng để
có hiệu quả tốt, tác động của bộ sưu tập tiền huấn luyện đến hiệu quả cuối cùng
không rõ ràng vì những biểu diễn này sau đó được tinh chỉnh bằng các phương tiện
đa dạng. Liệu hiệu quả có bị ảnh hưởng mạnh mẽ bởi bộ sưu tập tiền huấn luyện và
thống kê đồng xuất hiện của nó không? Để điều tra câu hỏi này, chúng tôi thử nghiệm
với một số mô hình PLM được huấn luyện từ đầu trên bộ sưu tập mục tiêu. Bằng
cách này, chúng tôi sẽ có thể đo lường lợi ích hiệu quả thu được bằng việc tiền huấn
luyện trên một bộ sưu tập bên ngoài lớn hơn và trả lời các câu hỏi sau: có lợi thế gì
khi tiền huấn luyện trực tiếp và chỉ trên bộ sưu tập mục tiêu? Khi nào và lợi thế của
việc tiền huấn luyện trên một bộ sưu tập lớn hơn khác là gì?

Một mặt, tiền huấn luyện từ đầu trên bộ sưu tập mục tiêu có thể có lợi thế của việc
mô hình hóa tốt hơn bộ sưu tập mục tiêu bằng cách có thống kê đồng xuất hiện nhiều
thông tin hơn giữa các tokens. Hơn nữa, các mô hình có kích thước 'nhỏ hơn' có thể
đạt được cùng mức độ hiệu quả, tức là người ta cũng có thể bao gồm yêu cầu hiệu
quả khi huấn luyện những mô hình này từ đầu. Mặt khác, tiền huấn luyện trên một
bộ sưu tập lớn hơn có thể dẫn đến tính mạnh mẽ/tổng quát hơn vì

--- TRANG 5 ---
Tiền huấn luyện Transformers từ Đầu 5
mô hình đã thấy nhiều lĩnh vực hơn, cách sử dụng token khác nhau và có thể có 'nhiều'
kiến thức hơn.

Do đó, bằng cách so sánh hai cách tiếp cận này, chúng tôi hy vọng hiểu rõ hơn cách
tiền huấn luyện bên ngoài lớn đóng góp vào hiệu quả cuối cùng. Trong bài báo này,
câu hỏi nghiên cứu của chúng tôi liên quan đến mô hình đa mục đích vs mô hình cụ
thể mục đích. Cách tiếp cận chủ đạo là áp dụng mô hình đa mục đích, bằng cách đơn
giản thích ứng nó với nhiệm vụ IR. Ngược lại, bài báo này điều tra các mô hình cụ
thể mục đích để đánh giá hiệu quả của chúng (xem Hình 1). Tóm lại, liệu việc tiền
huấn luyện từ đầu có hoạt động cho các mô hình IR không? Cụ thể hơn, chúng tôi
xem xét các câu hỏi nghiên cứu sau:

1. Chúng ta có cần một mô hình ngôn ngữ tiền huấn luyện bên ngoài cho Truy xuất
Thông tin không?
2. Các mô hình được tiền huấn luyện trên bộ sưu tập mục tiêu có khái quát hóa sang
các lĩnh vực và nhiệm vụ khác không?
3. Chúng ta có thể tận dụng việc tiền huấn luyện cho các lĩnh vực chuyên biệt và
ngôn ngữ không phải tiếng Anh không?
4. Việc tiền huấn luyện hiệu quả có cho phép chúng ta sử dụng những tiến bộ kiến
trúc gần đây của transformers không?

Hình 1. Tiền huấn luyện từ Đầu: MLM chỉ được thực hiện trên bộ sưu tập truy xuất.

Để trả lời những câu hỏi này, chúng tôi so sánh hiệu suất của các mô hình tiêu chuẩn,
như BERT được tiền huấn luyện trên Wikipedia và Book Corpus, với việc tiền huấn
luyện từ đầu trên các bộ sưu tập truy xuất. Ví dụ, chúng tôi sử dụng tập dữ liệu
MSMARCO [39] kinh điển. Bằng cách tiền huấn luyện chỉ trên MSMARCO, chúng
tôi quan sát xem có thực sự có lợi ích khi tiền huấn luyện trên Wikipedia hay không.
Hơn nữa, chúng tôi đánh giá xem các mô hình như vậy có vẫn khái quát hóa tốt trên
tập dữ liệu BEIR [51] không. Thật vậy, một lợi thế của việc tiền huấn luyện trên một
bộ sưu tập lớn có thể là khái quát hóa tốt hơn trong zero shot

--- TRANG 6 ---
6 C. Lassance et al.
Bảng 1. So sánh Mô hình.
BERT DistilBERT MLM 6L MLM 12L
Bộ sưu tập MLM Wikipedia, BookCorpus Wikipedia, BookCorpus MSMARCO hoặc Mr. TyDi hoặc TripClick
Kích thước Tiền huấn luyện (từ) 3,300M 3,300M Từ 100M đến 500M
# tham số 110M 66M 67M 110M
# Lớp 12 6 6 12
# số heads 12 16 16 12

settings. Đối với các lĩnh vực chuyên biệt, như dữ liệu y tế và sinh y học, chúng tôi
sử dụng tập dữ liệu TripClick [44] và so sánh việc tiền huấn luyện từ đầu với hiệu
suất của SciBERT [3] và PubMedBERT [17]. Tương tự, bằng cách tiền huấn luyện
các mô hình chỉ trên TripClick, chúng tôi so sánh hiệu suất với các mô hình được
huấn luyện trên một bộ sưu tập lớn hơn nhiều (tức là PubMed). Cuối cùng, chúng
tôi sẽ mở rộng các thí nghiệm trước đó trên các tập dữ liệu không phải tiếng Anh,
trên tập dữ liệu Mr. TyDi [54] bằng cách so sánh với MContriever [23], đã dựa vào
việc tiền huấn luyện trên tập dữ liệu lớn với nhiều ngôn ngữ. Bằng cách tiến hành
những thí nghiệm đó, chúng tôi có thể đánh giá xem một mô hình ngôn ngữ tiền huấn
luyện bên ngoài có cần thiết cho Truy xuất Thông tin không và liệu việc tiền huấn
luyện từ đầu có phải là một lựa chọn thay thế thú vị không.

Chúng tôi sẽ tập trung nghiên cứu của mình vào các kiến trúc BERT [11] và DistilBERT
[47] vì chúng là những kiến trúc phổ biến nhất trong IR. Hơn nữa, chúng tôi sẽ đo
lường hiệu quả của các kiến trúc khác nhau: bi-encoders dày đặc ([43,30]), SPLADE,
một mô hình thưa tiên tiến [13,28] và cross-encoders cho reranking [40].

3.1 Tiền huấn luyện
Trong phần sau, chúng tôi luôn tiền huấn luyện ít nhất hai loại mô hình từ đầu: một
mô hình với 12 lớp dựa trên kiến trúc BERT [11] và một với 6 lớp dựa trên DistilBERT
[47]. Chúng tôi sử dụng BERT và DistilBERT làm baseline trong toàn bộ bài báo.
Bảng 1 tóm tắt các đặc điểm mô hình chính mà chúng tôi sẽ xem xét.

Để tiền huấn luyện từ đầu, chúng tôi luôn cố định kích thước từ vựng là 32k (hơi
lớn hơn 30.5k của BERT), sử dụng wordpiece [52] để tìm các tokens phổ biến nhất
của bộ sưu tập mục tiêu. Chúng tôi gọi những mô hình này là MLM 12L và MLM 6L.

Chúng tôi cũng sử dụng 2 mô hình được xây dựng trên setting gọi là MLM+FLOPS
12L và MLM+FLOPS 6L, bằng cách thêm chính quy hóa FLOPS [41], giúp (tiền)
điều kiện hóa PLM để sử dụng với SPLADE như đề xuất trong [28]. Loss MLM tiêu
chuẩn được sửa đổi như sau: logits MLM đi qua hàm kích hoạt SPLADE (tức là
log(1 + ReLU(ylogits))), định nghĩa loss MLM trên một tập logits thưa. Cuối cùng,
một thuật ngữ khác (chính quy hóa FLOPS) được thêm vào để buộc logits không chỉ
không âm, mà thực sự thưa. Như trong SPLADE, một max pooling của toàn bộ câu
được thực hiện để có được biểu diễn ở mức từ. Trên biểu diễn cuối cùng này, chính
quy hóa FLOPS buộc sparsification (và uniformity) trên toàn bộ từ vựng. Tổng loss
được cho bởi ℓMLM + ℓMLM-SPLADE + ℓFLOPS.

Thời gian tiền huấn luyện từ 6 giờ đến 1 ngày tùy thuộc vào mô hình và bộ sưu tập
trên 8 NVIDIA A100 80 Gb, so với chi phí tính toán gốc của BERT (3 ngày sử dụng
64 TPUs), và chi phí tinh chỉnh (khoảng 1 ngày trên 4 V100 32Gb), chúng tôi coi chi
phí tiền huấn luyện của mình là hợp lý.

--- TRANG 7 ---
Tiền huấn luyện Transformers từ Đầu 7

4 Thí nghiệm
Câu hỏi nghiên cứu của chúng tôi là đánh giá xem các mô hình được huấn luyện
hoàn toàn trên bộ sưu tập mục tiêu có hoạt động tốt như các mô hình DistilBERT
hoặc BERT tổng quát hay không. Đầu tiên, chúng tôi kiểm tra kết quả trên bộ sưu
tập tổng quát MSMARCO [39] (RQ1) và cách các mô hình được huấn luyện trên đó
khái quát hóa (RQ2) sang kịch bản zero-shot trong BEIR [51]. Sau đó chúng tôi xác
minh xem kết quả có khái quát hóa sang các bộ sưu tập chuyên biệt và ngôn ngữ
không phải tiếng Anh (RQ3). Cuối cùng, chúng tôi tận dụng thực tế là chúng tôi
đang huấn luyện mô hình từ đầu để kiểm tra các biến thể của kiến trúc transformer
(RQ4). Chi tiết tinh chỉnh bổ sung có sẵn ở cuối các phần tương ứng.

Thiết lập Thí nghiệm Tiền huấn luyện được thực hiện sử dụng 8 NVIDIAs A100 80Gb
trên MS-MARCO (RQ1 và RQ2), hoặc TripClick (RQ3), hoặc tập dữ liệu Mr. TyDi
(RQ3). Chúng tôi luôn sử dụng learning rate là 1e-4. Trong trường hợp bộ sưu tập
MSMARCO, chúng tôi tiền huấn luyện sử dụng MLM và MLM+FLOPS sử dụng toàn
bộ bộ sưu tập passage (8.8M) kết hợp với các truy vấn huấn luyện (~800k) cho tổng
cộng (9.6M "tài liệu"), trong khi đối với TripClick chúng tôi tách các tài liệu thành
huấn luyện và xác thực (chia 90/10). Đối với tất cả các bộ sưu tập, batch size trên
mỗi GPU là 150 (12L) hoặc 200 (6L). Chúng tôi sử dụng exponential warmup cho
FLOPS là 5k steps, warmup là 1k steps cho logits và learning rate warmup là 10k
steps. Hệ số của FLOPS được đặt là 1e-3, chiều dài tối đa trước truncation là 256
tokens và các mạng được tiền huấn luyện cho 125k steps. Đối với Mr. TyDi, chúng
tôi tiền huấn luyện 3 mạng (Ả Rập, Nga, Nhật Bản) sử dụng MLM+FLOPS trên
toàn bộ bộ sưu tập passage của ngôn ngữ (2M cho Ả Rập, 7M cho Nhật Bản và 9.6M
cho Nga) với batch size trên mỗi GPU là 200. Cuối cùng đối với TripClick, điểm khác
biệt duy nhất là số epochs: 60 epochs, và batch size trên mỗi GPU là 256 cho mô
hình 6L, và 128 cho mô hình 12L.

Để tinh chỉnh, chúng tôi sử dụng 4 V100 32gb cho mô hình thưa (SPLADE) và mô
hình neural bi-encoder dày đặc. Xin lưu ý rằng chúng tôi không sử dụng distillation
từ reranker vì điều đó sẽ đòi hỏi chuyển giao thông tin từ PLM hiện có. Đối với
SPLADE, chúng tôi sử dụng chính quy hóa L1 trên truy vấn (theo [28]) với λq = 1e-3,
và cho tài liệu một chính quy hóa FLOPS với λd = 1e-3 (λd = 5e-4 trên TripClick)
theo [28]. Learning rate được đặt lên 2e-5.

Trong tất cả các bảng của chúng tôi, superscripts biểu thị sự khác biệt có ý nghĩa
theo kiểm định Student's t-test có cặp với hiệu chỉnh Bonferroni và p ≤ 0.05 với hàng
bảng tương ứng; MRR@10 và nDCG@10 đã được nhân với 100.

4.1 RQ1: Các mô hình được huấn luyện hoàn toàn trên MSMARCO có tốt như
các mô hình được tiền huấn luyện trên tập bộ sưu tập đa dạng không?

Câu hỏi đầu tiên chúng tôi muốn giải quyết là liệu các mô hình chỉ được huấn luyện
trên MSMARCO có tốt như các mô hình đã sử dụng corpora bên ngoài để tiền

--- TRANG 8 ---
8 C. Lassance et al.
Bảng 2. So sánh trên MSMARCO của các mô hình neural thưa (SPLADE) giai đoạn
đầu. y biểu thị phương pháp được tiền huấn luyện chỉ trên MSMARCO.
#Mô hình Tiền huấn luyện R-FLOPSMSMARCO dev TREC DL 19 TREC DL 20
MRR@10 R@1k nDCG@10 R@1k nDCG@10 R@1k
aDistilbert 1.10 0.373 0.975b0.732 0.853 0.708 0.867b
bBERT 1.32 0.367 0.968 0.727 0.832 0.699 0.842
cMLM 6Ly8.2 0.370 0.982ab0.701 0.847 0.700 0.877
dMLM+Flops 6Ly0.72 0.382abc0.979b0.698 0.836 0.701 0.872
eMLM+Flops 12Ly0.97 0.379bc0.980ab0.709 0.835 0.709 0.865

huấn luyện (tức là BookCorpus và Wikipedia). Chúng tôi đầu tiên điều tra các mô
hình được huấn luyện như ranker giai đoạn đầu, sử dụng bi-encoder dày đặc [30,25]
hoặc mô hình SPLADE. Chúng tôi tinh chỉnh các mô hình sử dụng negatives được
lấy mẫu từ mô hình SPLADE được huấn luyện trước đó. Cho mỗi V100 chúng tôi sử
dụng batch có kích thước tối đa không vượt quá tổng bộ nhớ. Batches được xây dựng
sao cho "một phần tử" của batch bao gồm chính truy vấn, một passage tích cực liên
quan đến truy vấn và 16 hoặc 32 negatives, tùy thuộc vào kích thước mạng (kích
thước batch thực tế trên mỗi GPU thay đổi từ 54 đến 102 tùy thuộc vào kích thước
mô hình tiền huấn luyện). Tinh chỉnh được coi là hoàn thành sau hai epochs (quyết
định tùy ý dựa trên thí nghiệm ban đầu với tập xác thực). Lưu ý rằng thiết lập tinh
chỉnh cho cả ranker giai đoạn đầu và cross-encoders gần như giống nhau, ngoại trừ
thực tế là cross-encoders không sử dụng in-batch negatives và sử dụng learning rate
1e-4.

Chúng tôi báo cáo retrieval flops (ký hiệu R-FLOPS) cho các mô hình SPLADE, tức
là số lượng floating point operations trên inverted index để trả về danh sách tài liệu
cho một truy vấn đã cho. Metric R-FLOPS được định nghĩa bởi ước tính số lượng
floating-point operations trung bình giữa một truy vấn và một tài liệu được định
nghĩa là expectation Eq,d[∑j∈V pj(q)pj(d)] trong đó pj là xác suất kích hoạt cho
token j trong tài liệu d hoặc truy vấn q. Nó được ước tính thực nghiệm từ một tập
khoảng 100k truy vấn phát triển, trên bộ sưu tập MS MARCO. Do đó nó là chỉ báo
của sparsity inverted index và chi phí tính toán truy xuất cho sparse retriever (khác
với chi phí inference của mô hình).

Kết quả retriever giai đoạn đầu được mô tả trong Bảng 2 cho mô hình thưa và Bảng
3 cho mô hình dày đặc. Đáng ngạc nhiên, các mô hình được huấn luyện chỉ trên
MSMARCO với MLM+FLOPS thực sự có thể hoạt động tốt hơn về mặt thống kê có
ý nghĩa so với các đối tác được tiền huấn luyện trên các bộ sưu tập lớn hơn, trong
cả kịch bản thưa và dày đặc, trong khi không có sự khác biệt có ý nghĩa thống kê
khi chỉ xem xét MLM trên MSMARCO so với MLM trên corpora lớn hơn. Điều này
cho thấy rằng việc tiền huấn luyện không chỉ không quan tâm đến bộ sưu tập đa
dạng, mà bằng cách chỉ tập trung vào "mô hình có sẵn" chúng ta có thể mất những
lợi ích hiệu suất có thể có của các mô hình được khởi tạo tốt hơn. Cũng lưu ý rằng
ít tính toán hơn đã được sử dụng để tiền huấn luyện MLM+FLOPS 6L so với đối
tác DistilBERT.

--- TRANG 9 ---
Tiền huấn luyện Transformers từ Đầu 9
Bảng 3. So sánh trên MSMARCO của các mô hình neural dày đặc (DPR) giai đoạn
đầu. y biểu thị phương pháp được tiền huấn luyện chỉ trên MSMARCO.
#Mô hình Tiền huấn luyệnMSMARCO dev TREC DL 19 TREC DL 20
MRR@10 R@1k nDCG@10 R@1k nDCG@10 R@1k
a Distilbert 0.342 0.961 0.673 0.774 0.670 0.816
b BERT 0.347 0.961 0.697 0.785 0.682 0.809
c MLM 6Ly0.346 0.968ab0.664 0.783 0.657 0.818
dMLM+FLOPS 6Ly0.349 0.968ab0.670 0.781 0.668 0.837
eMLM+FLOPS 12Ly0.352a0.969ab0.672 0.800 0.680 0.848bc

Bảng 4. So sánh rerankers trên MSMARCO. Các mô hình với y được tiền huấn luyện
chỉ trên MSMARCO.
# Mô hình Tiền huấn luyệnMSMARCO dev TREC DL 2019 TREC DL 2020
MRR@10 nDCG@10 nDCG@10
aKhông reranking (Giai đoạn đầu) 0.384 0.718 0.737
b Distilbert 0.396af0.764 0.734
c Bert 0.404abf0.750 0.737
d MLM 6Ly0.398af0.743 0.716
e MLM+Flops 6Ly0.396af0.724 0.736
f MLM+Flops 12Ly0.381 0.730 0.722

Cuối cùng, chúng tôi cũng kiểm tra trong trường hợp reranking sử dụng cross-encoders.
Kết quả có sẵn trong Bảng 4. Nhìn chung không có lợi ích thống kê có ý nghĩa trên
các mô hình được tiền huấn luyện với các bộ sưu tập bên ngoài.

4.2 RQ2: Các mô hình được tiền huấn luyện trên MSMARCO có khái quát hóa
tốt trên các bộ sưu tập khác không?

Trong phần trước, chúng tôi chỉ xem xét kết quả trong lĩnh vực. Mặc dù chúng đã
cho thấy rằng chúng ta có thể vượt trội (hoặc ít nhất giữ hiệu quả tương đương) khi
chỉ sử dụng bộ sưu tập mục tiêu, chúng có thể che giấu khoảng cách có thể có trong
dữ liệu ngoài lĩnh vực. Để xác minh rằng các mô hình chỉ được tiền huấn luyện và
tinh chỉnh trên MSMARCO không mất hiệu quả trong dữ liệu ngoài lĩnh vực, chúng
tôi báo cáo kết quả dưới benchmark zero-shot BEIR trong Bảng 5. Chúng tôi thực
sự quan sát thấy sự tăng nhẹ về hiệu quả trong truy xuất thưa khi sử dụng các mô
hình chỉ được huấn luyện trên MSMARCO, trong khi trên Dense có sự giảm hiệu
suất rõ ràng hơn. Sự khác biệt lớn nhất đối với mô hình dày đặc là trên tập dữ liệu
TREC Covid xa với bộ sưu tập MSMARCO. Với tính chất của benchmark BEIR
(trung bình trên 13 tập dữ liệu), những khác biệt đó có thể không có ý nghĩa².

²Chúng tôi không thể tìm thấy trong tài liệu một cách dễ dàng/thực tế để thực hiện
kiểm định ý nghĩa thống kê trên BEIR.

--- TRANG 10 ---
10 C. Lassance et al.
Bảng 5. Thí nghiệm truy xuất zero-shot trên BEIR (nDCG@10) với các mô hình được
tinh chỉnh trên MSMARCO. Các mô hình với y được tiền huấn luyện chỉ trên MSMARCO.
SPLADE Dense
Distilbert Bert M 6LyM+F 6LyM+F 12LyDistilbert Bert M 6LyM+F 6LyM+F 12Ly
arguana 45.30 44.30 46.90 42.90 45.40 34.00 37.30 40.20 37.40 40.70
climate-fever 14.90 15.30 14.50 15.70 15.40 16.50 15.90 15.40 15.30 17.50
dbpedia-entity 39.10 38.80 38.30 38.80 37.90 31.50 31.70 31.10 31.70 30.60
fever 73.40 71.20 68.60 72.30 70.70 70.80 70.20 59.10 59.90 56.10
qa 31.20 29.90 33.20 32.30 31.70 25.70 24.10 27.30 27.00 27.60
hotpotqa 66.90 65.50 64.00 67.30 67.60 49.70 50.20 47.30 46.40 47.60
nfcorpus 33.40 31.30 35.40 35.90 34.70 26.40 25.80 28.80 28.70 28.80
nq 51.40 50.60 48.50 49.20 49.40 46.00 47.10 41.90 41.50 42.90
quora 77.20 76.60 80.40 78.10 73.80 78.50 82.20 82.20 83.30 80.30
scidocs 14.90 14.90 14.40 15.10 14.50 11.40 11.80 10.70 11.20 11.00
scifact 66.00 64.70 69.20 69.20 69.00 52.00 54.80 56.70 57.30 56.50
trec-covid 67.60 69.10 65.60 64.70 68.00 66.10 65.70 56.00 48.90 49.90
webis-touche2020 27.60 27.00 24.70 28.50 26.20 22.20 23.50 23.20 19.70 19.30
trung bình 46.84 46.09 46.43 46.92 46.48 40.83 41.56 39.99 39.10 39.14

4.3 RQ3: Chúng ta có thể tận dụng việc tiền huấn luyện từ đầu trong các bộ
sưu tập lĩnh vực chuyên biệt/ngôn ngữ không?

IR Cụ thể Miền trên TripClick Bộ sưu tập TripClick [44] chứa khoảng 1.5 triệu tài
liệu MEDLINE (tiêu đề và tóm tắt), và 692,000 truy vấn. Tập kiểm tra được chia
thành ba loại truy vấn: Head, Torso và Tail (tần suất giảm dần), chứa 1,175 truy vấn
mỗi loại. Đối với truy vấn Head, mô hình click DCTR [9] được sử dụng để tạo tín
hiệu relevance, hai tập còn lại sử dụng click thô. [21] đã chỉ ra rằng các triplets gốc
quá nhiễu, và phát hành tập huấn luyện mới mà chúng tôi sử dụng trong thí nghiệm
này (10 triệu triplets).

Như các mô hình tiền huấn luyện, chúng tôi sử dụng các mô hình BERT, DistilBERT
có sẵn, và tương tự như [21] SciBERT [3] và PubMedBERT [17] đều sử dụng kiến
trúc tương tự BERT (12 lớp) và được tiền huấn luyện sử dụng tài liệu khoa học (từ
đó chúng trích xuất từ vựng). Chúng tôi coi chúng như các mô hình cụ thể miền có
sẵn. Trong khi để tinh chỉnh, chúng tôi sử dụng batch size 200 truy vấn và chỉ một
negative mỗi truy vấn, tận dụng chỉ in-batch negatives cho 90,000 iterations, tương
đương với 1.8 epochs.

Như Bảng 6 cho thấy, các mô hình được tiền huấn luyện từ đầu cạnh tranh tốt với
các mô hình tổng quát có sẵn cũng như với các mô hình chuyên biệt. Đối với bộ sưu
tập này, các mô hình dày đặc hoạt động tốt hơn các mô hình thưa. Kết luận phụ
thuộc vào loại mô hình: thưa hoặc dày đặc. Đối với các mô hình thưa, chúng ta thấy
rằng ít nhất một mô hình dựa trên tiền huấn luyện từ đầu vượt trội so với các mô
hình có sẵn như BERT và DistilBERT, cũng như cả hai mô hình cụ thể miền có sẵn
(Scibert, PubMedBert). Về kiến trúc dày đặc, các mô hình tiền huấn luyện từ đầu
ngang bằng với các mô hình cụ thể miền có sẵn đã yêu cầu nhiều dữ liệu huấn luyện
hơn (1.4B tokens cho Scibert so với 300M cho TripClick). Tiền huấn luyện từ đầu
cho phép lựa chọn mô hình ngôn ngữ phù hợp nhất theo kiến trúc tinh chỉnh (thưa
hoặc dày đặc). Trong trường hợp của chúng tôi, mô hình ngôn ngữ 6 lớp tốt hơn
nhiều cho SPLADE trong khi 12 lớp phù hợp tốt hơn với dày đặc

--- TRANG 11 ---
Tiền huấn luyện Transformers từ Đầu 11
Bảng 6. Thí nghiệm trên Tripclick với các mô hình thưa (SPLADE) và dày đặc
(nDCG@10). y biểu thị phương pháp được tiền huấn luyện chỉ trên Tripclik.
#Mô hình Tiền huấn luyệnSPLADE Dense
Head dctr Head Torso Tail Head dctr Head Torso Tail
aDistilbert 22.1b30.3bd23.9b24.8bd24.9 34.8 29.2 27.5
bBERT 10.6 14.4 9.6 7.8 25.3 35.3 29.4 28.8a
cPubMedBert 22.5bd31.0bd24.5b24.3bd27.1abef37.6abef29.9e30.8a
dScibert 21.1b28.4b23.0b22.2b27.8abef38.1abef29.2 30.3a
eMLM+F 6Ly26.9abcd fgh36.7abcd fgh27.7abcd fgh27.2bcd fh25.7 35.7 28.3 29.7
fMLM 6Ly23.1bd31.9bd24.7b23.4b25.8 36.0 28.9 29.3
gMLM+F 12Ly23.7abd32.5abd26.2abd26.6bd f26.7ab37.5abef29.9e30.1a
hMLM 12Ly24.0abcd32.6abcd24.8bd24.6bd f28.0abefg38.6abef30.2e30.4a
Scibert[21] - - - - 24.3 - - -
PubMebBert [21] - - - - 23.5 - - -
BM25 [21] - - - - 14.0 - - -

mô hình. Chúng tôi đã thêm ở cuối Bảng 6 kết quả từ [21], cho thấy rằng lựa chọn
triển khai của chúng tôi rất cạnh tranh³ cho cả mô hình thưa và dày đặc.

Mr. TyDi là tập dữ liệu đa ngôn ngữ cho truy xuất đơn ngôn ngữ bao gồm 11 ngôn
ngữ đa dạng về mặt typology [54]. Cho nghiên cứu này chúng tôi tập trung vào ba
ngôn ngữ không phải tiếng Anh với nhiều dữ liệu huấn luyện nhất trên Mr. TyDi:
i) Ả Rập; ii) Nga; iii) Nhật Bản. Lưu ý rằng, có một lượng lớn dữ liệu có sẵn cho
những ngôn ngữ này (ngay cả bên ngoài Mr. TyDi), nhưng PLMs không được nghiên
cứu kỹ như trong tiếng Anh. Sự đồng thuận chính có vẻ là trong trường hợp này
người ta nên tập trung vào dữ liệu đa ngôn ngữ, mà hầu hết dựa trên tiếng Anh
như một neo, như trường hợp của nhiều công trình trước đây [23,38,37,54,8]. Chúng
tôi thách thức quan niệm này, bằng cách: a) sử dụng các mô hình đơn ngôn ngữ; b)
tiền huấn luyện chỉ trên Mr. TyDi.

Chúng tôi theo giao thức tinh chỉnh của MContriever [23], nơi họ đầu tiên tinh chỉnh
mô hình cho truy xuất trên MMarco [5], phiên bản dịch của MSMARCO thành nhiều
ngôn ngữ, mà chúng tôi chỉ sử dụng ngôn ngữ mục tiêu cho một mô hình đã cho (Ả
Rập, Nga hoặc Nhật Bản) và sau đó cuối cùng tinh chỉnh trên bộ sưu tập Mr. TyDi.
Trong trường hợp của chúng tôi, chúng tôi thực hiện huấn luyện ba bước, bước đầu
tiên trên MMarco sử dụng negatives được lấy mẫu với mô hình tiền huấn luyện (hoặc
MContriever cho baseline). Sau đó chúng tôi thực hiện hai bước huấn luyện trên Mr.
TyDi, đầu tiên sử dụng negatives được trích xuất từ mô hình được tinh chỉnh MMarco
và thứ hai từ giai đoạn đầu tiên của tinh chỉnh Mr.TyDi. Composition batch theo
tinh chỉnh MSMARCO trước đó, với batch size 3 truy vấn 32 negatives (kích thước
batch thực tế trên mỗi GPU do đó là 102). Tinh chỉnh dừng sau hai epochs.

Kết quả có sẵn trong Bảng 7. So với state of the art trước đó [23]⁴, là dense retriever
được tiền huấn luyện theo cách cụ thể trên bộ sưu tập lớn hơn nhiều⁵, chúng tôi cho
thấy cải thiện có ý nghĩa thống kê trên tất cả ngôn ngữ
³Chúng tôi không thể tìm thấy các tham số được sử dụng trong thí nghiệm.
⁴Lưu ý rằng MContriever TyDi (hàng đầu tiên) không có sẵn, không thể thực hiện
kiểm định thống kê. Chúng tôi cố gắng đánh giá công bằng dưới thiết lập huấn luyện
của chúng tôi (hàng thứ hai)
⁵Chúng tôi nghi ngờ họ sử dụng nhiều tính toán hơn, nhưng không thể tìm thấy thông
tin tính toán chính xác.

--- TRANG 12 ---
12 C. Lassance et al.
trong khi chỉ sử dụng các bộ sưu tập Mr. TyDi và MMarco trên ngôn ngữ mục tiêu.
Tuy nhiên, điều quan trọng cần lưu ý là khác với [23] chúng tôi thực tế chưa kiểm
tra trên tất cả ngôn ngữ và do đó chỉ có thể đánh giá hiệu ứng tiền huấn luyện trên
ba ngôn ngữ này, đây là ba ngôn ngữ lớn nhất từ Mr. TyDi.

Bảng 7. So sánh, trên tập dữ liệu Mr.TyDi, của các mô hình được huấn luyện từ đầu
với các mô hình được tiền huấn luyện trong bộ sưu tập bên ngoài lớn với Contriever.
y có nghĩa là phương pháp được tiền huấn luyện chỉ trên MrTiDy.
# Mô hình Tiền huấn luyện Ả Rập Nga Nhật Bản
MRR@100 R@100 MRR@100 R@100 MRR@100 R@100
MContriever [23] 72.4 94.0 59.7 92.4 54.9 88.8
aMContriever (tái tạo) 72.7 93.6 59.9b91.6 49.9b85.2b
bMLM+FLOPS 6L DPRy73.4 94.9 56.2 91.6 32.9 62.0
cMLM+FLOPS 6L SPLADEy75.7ab94.2 65.0ab93.4ab56.3ab88.9ab

4.4 RQ4: Tác động của Kiến trúc
Cuối cùng, một lợi thế của việc tiền huấn luyện mô hình từ đầu là thực tế rằng chúng
ta có thể dễ dàng thử nghiệm với các kiến trúc khác nhau. Thật vậy, xem xét rằng
hầu hết các công trình IR sử dụng một biến thể của BERT (RoBERTA, DistilBERT
hoặc BERT) điều này đặt ra câu hỏi liệu các biến thể của kiến trúc transformer,
được benchmark trong NLP, có thể thực sự cải thiện IR. Để giải quyết câu hỏi này,
chúng tôi sử dụng thiết lập truy xuất thưa từ RQ1, nhưng lần này cũng xem xét sử
dụng kiến trúc DeBERTa [19] đánh bại BERT trên nhiều nhiệm vụ NLP. Kết quả
được trình bày trong Bảng 8 và 9. Rất tiếc, chúng tôi thực tế không thấy cải thiện
lớn khi sử dụng những thay đổi kiến trúc này, do đó chúng tôi để lại như công việc
tương lai về cách bao gồm tốt hơn những thay đổi này trong các ranker giai đoạn
đầu.

5 Kết luận
Các mô hình nền tảng đến với lời hứa có tính tổng quát và mô-đun cao. Người ta tin
rằng chúng chứa "kiến thức" rộng do việc tiền huấn luyện trên bộ sưu tập lớn, sau
đó được tin là nguồn gốc của hiệu suất cải thiện của chúng. Chúng tôi đã kiểm tra
cách bộ sưu tập tiền huấn luyện này ảnh hưởng đến hiệu suất của các mô hình IR.
Câu hỏi nghiên cứu của chúng tôi là đánh giá có bao nhiêu kiến thức ngầm này, có
lợi cho hiệu suất cuối cùng, đến từ việc tiền huấn luyện trên bộ sưu tập bên ngoài
lớn. Đây là lý do tại sao chúng tôi đã thử nghiệm trên nhiều bộ sưu tập, lĩnh vực
và ngôn ngữ để nghiên cứu cách tiền huấn luyện từ đầu thực sự hoạt động so với
cách tiếp cận de facto của việc tinh chỉnh đơn giản.

Mặc dù chúng tôi mong đợi các mô hình tiền huấn luyện tiêu chuẩn hoạt động tốt
hơn, chúng tôi đã tiết lộ một cách đáng ngạc nhiên rằng tiền huấn luyện từ đầu hoạt
động tốt hơn cho truy xuất giai đoạn đầu trên MSMARCO, TripClick và một số ngôn
ngữ không phải tiếng Anh trên

--- TRANG 13 ---
Tiền huấn luyện Transformers từ Đầu 13
Bảng 8. So sánh trên MSMARCO của các mô hình neural thưa (SPLADE) giai đoạn
đầu sử dụng các kiến trúc khác nhau. Tất cả phương pháp được tiền huấn luyện chỉ
trên MSMARCO.
#Mô hình Tiền huấn luyện R-FLOPSMSMARCO dev TREC DL 19 TREC DL 20
MRR@10 R@1k nDCG@10 R@1k nDCG@10 R@1k
aBERT MLM+FLOPS 6L 0.72 0.382d0.979 0.698 0.836 0.701 0.872
bBERT MLM+FLOPS 12L 0.97 0.379 0.980 0.709 0.835 0.709 0.865
cDeBERTa MLM+FLOPS 6L 0.81 0.376 0.979 0.700 0.845 0.701 0.863
dDeBERTa MLM+FLOPS 12L 0.79 0.373 0.978 0.716 0.839 0.735 0.871

Bảng 9. So sánh trên TripClick của các mô hình thưa (SPLADE) và dày đặc giai đoạn
đầu sử dụng các kiến trúc khác nhau. Tất cả phương pháp được tiền huấn luyện chỉ
trên TripClick.
#Mô hình Tiền huấn luyệnSPLADE Dense
Head dctr Head Torso Tail Head dctr Head Torso Tail
aBERT MLM+FLOPS 6L 26.9bcdefh36.7bcd fh27.7bd fh27.2bd fh25.7e35.7e28.3 29.7e
bBERT MLM 6L 23.1 31.9 24.7 23.4 25.8e36.0e28.9 29.3
cBERT MLM+FLOPS 12L 23.7f32.5f26.2fh26.6bfh26.7ef37.5abef29.9aef30.1e
dBERT MLM 12L 24.0fh32.6f24.8 24.6 28.0abcefgh38.6abefgh30.2aeh30.4e
eDeBERTa MLM FLOPS 6L 26.1bcd fh35.9bcd fh29.0bcd fh28.4bcd fh24.3 34.0 27.8 27.5
fDeBERTa MLM 6L 22.2 30.7 23.3 23.3 24.9 34.7 28.7 28.9
gDeBERTa MLM FLOPS 12L 27.0bcd fh37.5bcdefh29.4abcd fh28.7abcd fh26.6ef36.7ef29.2 29.5
hDeBERTa MLM 12L 22.6 31.3 23.8 23.3 26.4ef36.4ef28.6 29.0

benchmark Mr. TyDi. Đặc biệt, chính quy hóa FLOPS đã đóng vai trò quan trọng
trong những kết quả đó, gợi ý rằng chính quy hóa hoặc kỹ thuật tiền huấn luyện tốt
hơn có thể cải thiện thêm kết quả. Hơn nữa, các mô hình tiền huấn luyện từ đầu
cũng hoạt động tốt trong kịch bản zero shot cho các mô hình thưa như SPLADE.
Tuy nhiên, tiền huấn luyện từ bộ sưu tập lớn có lợi thế nhẹ khi huấn luyện rerankers.

Nhìn chung, những kết quả này, cụ thể cho IR, thách thức giả thuyết mô hình nền
tảng cho các mô hình nhỏ, tức là một mô hình tổng quát hơn bao gồm kiến thức thế
giới sẽ tốt hơn một mô hình nhỏ hơn trong ứng dụng lĩnh vực cụ thể. Hơn nữa,
nghiên cứu của chúng tôi đóng góp vào cuộc tranh luận giữa mô hình đa mục đích
và mô hình cụ thể mục đích. Theo một cách nào đó, thí nghiệm của chúng tôi đã
chỉ ra rằng ít hơn là nhiều hơn. Ngoài ra, các mô hình ngôn ngữ tiền huấn luyện
cũng đến với nhiều thách thức như độ lệch xã hội trong dữ liệu mà chúng được huấn
luyện. Chúng tôi hy vọng rằng nghiên cứu của chúng tôi có thể thuyết phục các nhà
thực hành, cả từ công nghiệp và học thuật, xem xét lại các mô hình cụ thể mục đích
bằng cách tiền huấn luyện từ đầu. Cuối cùng nhưng không kém phần quan trọng,
làm như vậy cho phép kiểm soát tốt hơn hiệu quả, độ lệch dữ liệu và khả năng tái
tạo, đây là những câu hỏi nghiên cứu quan trọng cho cộng đồng IR.

Tài liệu tham khảo
1. Aroca-Ouellette, S., Rudzicz, F.: On Losses for Modern Language Models. In:
Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP). pp. 4970–4981. Association for Computational Linguistics,
Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.403, https://
aclanthology.org/2020.emnlp-main.403

--- TRANG 14 ---
14 C. Lassance et al.
2. Bai, B., Weston, J., Grangier, D., Collobert, R., Sadamasa, K., Qi, Y., Chapelle,
O., Weinberger, K.Q.: Supervised semantic indexing. In: Proceedings of the 18th
ACM International Conference on Information and Knowledge Management. pp.
187–196. ACM (2009). https://doi.org/10.1145/1645953.1645979
3. Beltagy, I., Lo, K., Cohan, A.: Scibert: A pretrained language model for scien-
tific text (2019). https://doi.org/10.48550/ARXIV.1903.10676, https://arxiv.
org/abs/1903.10676
4. Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bern-
stein, M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card,
D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J.Q., Demszky, D., Don-
ahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K.,
Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S.,
Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D.E., Hong, J., Hsu, K.,
Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G.,
Khani, F., Khattab, O., Koh, P.W., Krass, M., Krishna, R., Kuditipudi, R., Kumar,
A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X.L., Li, X., Ma, T.,
Malik, A., Manning, C.D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S.,
Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J.C., Nilforoshan, H.,
Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J.S., Piech, C., Portelance,
E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C.,
Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K.,
Tamkin, A., Taori, R., Thomas, A.W., Tramèr, F., Wang, R.E., Wang, W., Wu, B.,
Wu, J., Wu, Y., Xie, S.M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang,
T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., Liang, P.: On the opportunities and
risks of foundation models (2021). https://doi.org/10.48550/ARXIV.2108.07258,
https://arxiv.org/abs/2108.07258
5. Bonifacio, L.H., Campiotti, I., Jeronymo, V., Lotufo, R., Nogueira, R.: mmarco:
A multilingual version of the ms marco passage ranking dataset. arXiv preprint
arXiv:2108.13897 (2021)
6. Chang, W.C., Yu, F.X., Chang, Y.W., Yang, Y., Kumar, S.: Pre-training tasks for
embedding-based large-scale retrieval. In: International Conference on Learning
Representations (2020), https://openreview.net/forum?id=rkg-mA4FDr
7. Clinchant, S., Jung, K.W., Nikoulina, V.: On the use of BERT for neural ma-
chine translation. In: Proceedings of the 3rd Workshop on Neural Generation and
Translation. pp. 108–117. Association for Computational Linguistics, Hong Kong
(Nov 2019). https://doi.org/10.18653/v1/D19-5611, https://aclanthology.org/
D19-5611
8. Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán,
F., Grave, E., Ott, M., Zettlemoyer, L., Stoyanov, V.: Unsupervised cross-lingual
representation learning at scale. arXiv preprint arXiv:1911.02116 (2019)
9. Craswell, N., Zoeter, O., Taylor, M., Ramsey, B.: An experimental comparison of
click position-bias models. In: Proceedings of the 2008 international conference on
web search and data mining. pp. 87–94 (2008)
10. Dehghani, M., Zamani, H., Severyn, A., Kamps, J., Croft, W.B.: Neural rank-
ing models with weak supervision. In: Proceedings of the 40th International
ACM SIGIR Conference on Research and Development in Information Re-
trieval. p. 65–74. SIGIR '17, Association for Computing Machinery, New York,
NY, USA (2017). https://doi.org/10.1145/3077136.3080832, https://doi.org/
10.1145/3077136.3080832

--- TRANG 15 ---
Tiền huấn luyện Transformers từ Đầu 15
11. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec-
tional transformers for language understanding. CoRR abs/1810.04805 (2018),
http://arxiv.org/abs/1810.04805
12. El-Nouby, A., Izacard, G., Touvron, H., Laptev, I., Jegou, H., Grave, E.: Are
large-scale datasets necessary for self-supervised pre-training? arXiv preprint
arXiv:2112.10740 (2021)
13. Formal, T., Lassance, C., Piwowarski, B., Clinchant, S.: From distillation to hard
negative sampling: Making sparse neural ir models more effective. In: Proceedings
of the 45th International ACM SIGIR Conference on Research and Development
in Information Retrieval. p. 2353–2359. SIGIR '22, Association for Computing
Machinery, New York, NY, USA (2022). https://doi.org/10.1145/3477495.3531857,
https://doi.org/10.1145/3477495.3531857
14. Gao, L., Callan, J.: Condenser: a pre-training architecture for dense re-
trieval. In: Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing. pp. 981–993. Association for Compu-
tational Linguistics, Online and Punta Cana, Dominican Republic (Nov
2021). https://doi.org/10.18653/v1/2021.emnlp-main.75, https://aclanthology.
org/2021.emnlp-main.75
15. Gao, L., Callan, J.: Unsupervised corpus aware language model pre-training
for dense passage retrieval. In: Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers). pp.
2843–2853. Association for Computational Linguistics, Dublin, Ireland (May
2022). https://doi.org/10.18653/v1/2022.acl-long.203, https://aclanthology.
org/2022.acl-long.203
16. Gao, L., Dai, Z., Callan, J.: COIL: Revisit exact lexical match in information re-
trieval with contextualized inverted list. In: Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies. pp. 3030–3042. Association for Computational
Linguistics, Online (Jun 2021). https://doi.org/10.18653/v1/2021.naacl-main.241,
https://aclanthology.org/2021.naacl-main.241
17. Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T.,
Gao, J., Poon, H.: Domain-specific language model pretraining for biomedical
natural language processing. ACM Transactions on Computing for Healthcare
3(1), 1–23 (jan 2022). https://doi.org/10.1145/3458754, https://doi.org/10.
1145%2F3458754
18. Guo, Y., Ma, Z., Mao, J., Qian, H., Zhang, X., Jiang, H., Cao, Z., Dou, Z.: Web-
former: Pre-training with web pages for information retrieval. In: Proceedings of
the 45th International ACM SIGIR Conference on Research and Development in
Information Retrieval. p. 1502–1512. SIGIR '22, Association for Computing Ma-
chinery, New York, NY, USA (2022). https://doi.org/10.1145/3477495.3532086,
https://doi.org/10.1145/3477495.3532086
19. He, P., Liu, X., Gao, J., Chen, W.: Deberta: Decoding-enhanced bert with disentan-
gled attention. In: International Conference on Learning Representations (2021),
https://openreview.net/forum?id=XPZIaotutsD
20. Hofstätter, S., Althammer, S., Schröder, M., Sertkan, M., Hanbury, A.: Improv-
ing efficient neural ranking models with cross-architecture knowledge distillation
(2020)
21. Hofstätter, S., Althammer, S., Sertkan, M., Hanbury, A.: Establishing strong base-
lines for tripclick health retrieval (2022)

--- TRANG 16 ---
16 C. Lassance et al.
22. Hofstätter, S., Lin, S.C., Yang, J.H., Lin, J., Hanbury, A.: Efficiently Teaching
an Effective Dense Retriever with Balanced Topic Aware Sampling. In: Proc. of
SIGIR (2021)
23. Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., Grave,
E.: Towards unsupervised dense information retrieval with contrastive learning
(2021)
24. Kaplan, J., McCandlish, S., Henighan, T.J., Brown, T.B., Chess, B., Child, R.,
Gray, S., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language models.
ArXiv abs/2001.08361 (2020)
25. Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., Yih,
W.t.: Dense passage retrieval for open-domain question answering. In: Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP). pp. 6769–6781. Association for Computational Linguistics, Online (Nov
2020). https://doi.org/10.18653/v1/2020.emnlp-main.550, https://www.aclweb.
org/anthology/2020.emnlp-main.550
26. Khattab, O., Zaharia, M.: Colbert: Efficient and effective passage search via
contextualized late interaction over bert. In: Proceedings of the 43rd Interna-
tional ACM SIGIR Conference on Research and Development in Information Re-
trieval. p. 39–48. SIGIR '20, Association for Computing Machinery, New York,
NY, USA (2020). https://doi.org/10.1145/3397271.3401075, https://doi.org/
10.1145/3397271.3401075
27. Kim, T., Yoo, K.M., Lee, S.g.: Self-guided contrastive learning for BERT
sentence representations. In: Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long Pa-
pers). pp. 2528–2540. Association for Computational Linguistics, Online (Aug
2021). https://doi.org/10.18653/v1/2021.acl-long.197, https://aclanthology.
org/2021.acl-long.197
28. Lassance, C., Clinchant, S.: An efficiency study for splade models. In: Proceedings
of the 45th International ACM SIGIR Conference on Research and Development
in Information Retrieval. p. 2220–2226. SIGIR '22, Association for Computing
Machinery, New York, NY, USA (2022). https://doi.org/10.1145/3477495.3531833,
https://doi.org/10.1145/3477495.3531833
29. Lin, J., Ma, X.: A few brief notes on deepimpact, coil, and a conceptual framework
for information retrieval techniques. CoRR abs/2106.14807 (2021), https://
arxiv.org/abs/2106.14807
30. Lin, J., Nogueira, R., Yates, A.: Pretrained Transformers for Text Ranking:
BERT and Beyond. arXiv:2010.06467 [cs] (Oct 2020), http://arxiv.org/abs/
2010.06467 , zSCC: NoCitationData[s0] arXiv: 2010.06467
31. Lin, S.C., Yang, J.H., Lin, J.: In-batch negatives for knowledge distil-
lation with tightly-coupled teachers for dense retrieval. In: Proceedings
of the 6th Workshop on Representation Learning for NLP (RepL4NLP-
2021). pp. 163–173. Association for Computational Linguistics, Online (Aug
2021). https://doi.org/10.18653/v1/2021.repl4nlp-1.17, https://aclanthology.
org/2021.repl4nlp-1.17
32. Liu, Z., Shao, Y.: Retromae: Pre-training retrieval-oriented transformers
via masked auto-encoder (2022). https://doi.org/10.48550/ARXIV.2205.12035,
https://arxiv.org/abs/2205.12035
33. Ma, X., Guo, J., Zhang, R., Fan, Y., Ji, X., Cheng, X.: B-prop: Bootstrapped pre-
training with representative words prediction for ad-hoc retrieval. Proceedings of

--- TRANG 17 ---
Tiền huấn luyện Transformers từ Đầu 17
the 44th International ACM SIGIR Conference on Research and Development in
Information Retrieval (2021)
34. Ma, X., Guo, J., Zhang, R., Fan, Y., Ji, X., Cheng, X.: Prop: Pre-training with
representative words prediction for ad-hoc retrieval. Proceedings of the 14th ACM
International Conference on Web Search and Data Mining (2021)
35. Ma, Z., Dou, Z., Xu, W., Zhang, X., Jiang, H., Cao, Z., rong Wen, J.: Pre-training
for ad-hoc retrieval: Hyperlink is also you need. Proceedings of the 30th ACM
International Conference on Information & Knowledge Management (2021)
36. Muennighoff, N.: Sgpt: Gpt sentence embeddings for semantic search. arXiv
preprint arXiv:2202.08904 (2022)
37. Nair, S., Yang, E., Lawrie, D., Duh, K., McNamee, P., Murray, K., Mayfield, J.,
Oard, D.W.: Transfer learning approaches for building cross-language dense re-
trieval models. In: European Conference on Information Retrieval. pp. 382–396.
Springer (2022)
38. Nair, S., Yang, E., Lawrie, D., Mayfield, J., Oard, D.W.: Learning a sparse rep-
resentation model for neural clir. Design of Experimental Search & Information
REtrieval Systems (DESIRES) (2022)
39. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng,
L.: Ms marco: A human generated machine reading comprehension dataset. In:
CoCo@ NIPs (2016)
40. Nogueira, R., Cho, K.: Passage re-ranking with bert (2019)
41. Paria, B., Yeh, C.K., Yen, I.E.H., Xu, N., Ravikumar, P., Póczos, B.: Minimizing
flops to learn efficient sparse representations (2020)
42. Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W.X., Dong, D., Wu, H., Wang,
H.: Rocketqa: An optimized training approach to dense passage retrieval for open-
domain question answering. In: In Proceedings of NAACL (2021)
43. Reimers, N., Gurevych, I.: Sentence-bert: Sentence embeddings using siamese bert-
networks. In: Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics (11 2019), http:
//arxiv.org/abs/1908.10084
44. Rekabsaz, N., Lesota, O., Schedl, M., Brassey, J., Eickhoff, C.: Tripclick: The log
files of a large health web search engine. In: Proceedings of the 44th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
pp. 2507–2513 (2021). https://doi.org/10.1145/3404835.3463242
45. Ren, R., Qu, Y., Liu, J., Zhao, W.X., She, Q., Wu, H., Wang, H., Wen, J.R.:
RocketQAv2: A joint training method for dense passage retrieval and pas-
sage re-ranking. In: Proceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing. pp. 2825–2835. Association for Compu-
tational Linguistics, Online and Punta Cana, Dominican Republic (Nov 2021).
https://doi.org/10.18653/v1/2021.emnlp-main.224, https://aclanthology.org/
2021.emnlp-main.224
46. Robertson, S.E., Walker, S., Beaulieu, M., Gatford, M., Payne, A.: Okapi at trec-4.
Nist Special Publication Sp pp. 73–96 (1996)
47. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of bert:
smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019)
48. Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., Zaharia, M.: Colbertv2:
Effective and efficient retrieval via lightweight late interaction (2021)
49. Tay, Y., Dehghani, M., Gupta, J.P., Aribandi, V., Bahri, D., Qin, Z., Metzler,
D.: Are pretrained convolutions better than pretrained transformers? In: Pro-
ceedings of the 59th Annual Meeting of the Association for Computational Lin-

--- TRANG 18 ---
18 C. Lassance et al.
guistics and the 11th International Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers). pp. 4349–4359. Association for Computational
Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.335,
https://aclanthology.org/2021.acl-long.335
50. Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H.W., Narang, S.,
Yogatama, D., Vaswani, A., Metzler, D.: Scale efficiently: Insights from pre-training
and fine-tuning transformers. ArXiv abs/2109.10686 (2022)
51. Thakur, N., Reimers, N., Rückle, A., Srivastava, A., Gurevych, I.: BEIR: A het-
erogenous benchmark for zero-shot evaluation of information retrieval models.
CoRR abs/2104.08663 (2021), https://arxiv.org/abs/2104.08663
52. Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun,
M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X.,
Łukasz Kaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian,
G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O.,
Corrado, G., Hughes, M., Dean, J.: Google's neural machine translation system:
Bridging the gap between human and machine translation (2016)
53. Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett, P.N., Ahmed, J., Over-
wikj, A.: Approximate nearest neighbor negative contrastive learning for dense
text retrieval. In: International Conference on Learning Representations (2021),
https://openreview.net/forum?id=zeFrfgyZln
54. Zhang, X., Ma, X., Shi, P., Lin, J.: Mr. tydi: A multi-lingual benchmark for dense
retrieval. In: Proceedings of the 1st Workshop on Multilingual Representation
Learning. pp. 127–137 (2021)
