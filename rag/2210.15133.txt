# 2210.15133.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2210.15133.pdf
# File size: 290046 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Retrieval Oriented Masking Pre-training
Language Model for Dense Passage Retrieval
Dingkun Long, Yanzhao Zhang, Guangwei Xu, Pengjun Xie
Alibaba Group
dingkun.ldk,zhangyanzhao.zyz@alibaba-inc.com
kunka.xgw,pengjun.xpj@alibaba-inc.com
Abstract
Pre-trained language model (PTM) has been
shown to yield powerful text representations
for dense passage retrieval task. The Masked
Language Modeling (MLM) is a major sub-
task of the pre-training process. However, we
found that the conventional random masking
strategy tend to select a large number of to-
kens that have limited effect on the passage
retrieval task (e,g. stop-words and punctua-
tion). By noticing the term importance weight
can provide valuable information for passage
retrieval, we hereby propose alternative re-
trieval oriented masking (dubbed as ROM)
strategy where more important tokens will
have a higher probability of being masked out,
to capture this straightforward yet essential
information to facilitate the language model
pre-training process. Notably, the proposed
new token masking method will not change
the architecture and learning objective of orig-
inal PTM. Our experiments verify that the pro-
posed ROM enables term importance informa-
tion to help language model pre-training thus
achieving better performance on multiple pas-
sage retrieval benchmarks.
1 Introduction
Dense passage retrieval has drown much attention
recently due to its beneﬁts to a wide range of down-
streaming applications, such as open-domain ques-
tion answering (Karpukhin et al., 2020; Qu et al.,
2021; Zhu et al., 2021), conversational systems (Yu
et al., 2021) and web search (Lin et al., 2021; Fan
et al., 2021; Long et al., 2022). To balance efﬁ-
ciency and effectiveness, existing dense passage
retrieval methods usually leverage a dual-encoder
architecture. Speciﬁcally, query and passage are
encoded into continuous vector representations by
language models (LMs) respectively, then, a score
function is applied to estimate the semantic simi-
larity between the query-passage pair.
Based on the dual-encoder architecture, various
optimization methods have been proposed recently,including hard negative training examples min-
ing (Xiong et al., 2021), optimized PTMs specially
designed for dense retrieval (Gao and Callan, 2021,
2022; Ma et al., 2022) and alternative text represen-
tation methods or ﬁne-tuning strategies (Karpukhin
et al., 2020; Zhang et al., 2022a, 2021). In this pa-
per, we focus on studying the part of pre-trained lan-
guage model. We observe that the widely adopted
random token masking MLM pre-training objective
is sub-optimal for dense passage retrieval task. Re-
ferring to previous studies, introducing the weight
of each term (or token) to assist in estimating the
query-passage relevance is effective in both pas-
sage retrieval and ranking stages (Dai and Callan,
2020; Ma et al., 2021; Wu et al., 2022). How-
ever, the random masking strategy does not dis-
tinguish the term importance of tokens. Further,
we ﬁnd that about 40% of the masked tokens pro-
duced by the 15% random masking method are
stop-words or punctuation1. Nonetheless, the effect
of these tokens on passage retrieval is extremely
limited (Fawcett et al., 2020). Therefore, we infer
that LMs pre-trained with random token masking
MLM objective is sub-optimal for dense passage
retrieval due to its shortcoming in distinguishing
token importance.
To address the limitation above, we propose al-
ternative retrieval oriented masking (ROM) strat-
egy aiming to mask tokens that are required for
passage retrieval. Speciﬁcally, in the pre-training
process of LM, the probability of each token be-
ing masked is not random, but is superimposed by
the important weight of the token corresponded.
Here, the important weight is represented as a ﬂoat
number between 0and1. In this way, we can
greatly improve the probability of higher-weight to-
kens being masked out. Therefore, the pre-trained
language model will pay more attention to higher-
weight words thus making it more proper for down-
streaming dense passage retrieval applications.
1We used nltk and gensim stop-words lists.arXiv:2210.15133v1  [cs.CL]  27 Oct 2022

--- PAGE 2 ---
To verify the effectiveness and robustness of our
proposed retrieval oriented masking method, we
conduct experiments on two commonly used pas-
sage retrieval benchmarks: the MS MARCO pas-
sage ranking and Neural Questions (NQ) datasets.
Empirically experiment results demonstrate that
our method can remarkably improve the passage
retrieval performance.
2 Related Work
Existing dense passage retrieval methods usu-
ally adopts a dual-encoder architecture. In
DPR (Karpukhin et al., 2020), they ﬁrstly pre-
sented that the passage retrieval performance of
dense dual-encoder framework can remarkable out-
perform traditional term match based method like
BM25. Based on the dual-encoder framework, stud-
ies explore to various strategies to enhance dense
retrieval models, including mining hard negatives
in ﬁne-tuning stage (Xiong et al., 2021; Zhan et al.,
2021), knowledge distillation from more powerful
cross-encoder model (Ren et al., 2021; Zhang et al.,
2021; Lu et al., 2022), data augmentation (Qu et al.,
2021) and tailored PTMs (Chang et al., 2020; Gao
and Callan, 2021, 2022; Ma et al., 2022; Liu and
Shao, 2022; Wu et al., 2022).
For the pre-training of language model, previous
research attend to design additional pre-training
objectives tailored for dense passage retrieval (Lee
et al., 2019; Chang et al., 2020) or adjust the Trans-
former encoder architecture (Gao and Callan, 2021,
2022) to obtain more practicable language models.
In this paper, we seek to make simple transforma-
tions of the original MLM learning objective to
improve the model performance, thereby reducing
the complexity of the pre-training process.
3 Methodology
In this section, we describe our proposed pre-
training method for the dense passage retrieval task.
We ﬁrst give a brief overview of the conventional
BERT pre-training model with MLM loss. Then
we will introduce how to extend it to our model
with retrieval oriented masking pre-training.
3.1 BERT Pre-trained Model
MLM Pre-training Many popular Transformer
Encoder language models (e,g. BERT, RoBERTa)
adopts the MLM objective at pre-training phase.
MLM masks out a subset of input tokens and re-
quires the model to predict them. Speciﬁcally, theMLM loss can be formulated as:
Lmlm=X
i2maskedCrossEntropy( WhL
i;xi);
wherehL
iis the ﬁnal representation of masked to-
kenxiandLis the number of Transformer layers.
Random Masking In general, the selection of
masked out tokens is random, and the proportion
of masking in a sentence is set at 15%. Mathemati-
cally, for each token xi2x, the probability of xi
being masked out p(xi)is sampled from a uniform
distribution between 0and1. If the value of p(xi)
is in the top 15% of the entire input sequence, then
xiwill be masked out.
3.2 Disadvantages of Random Masking
The signiﬁcant issue of the random masking
method is that it does not distinguish the impor-
tant weight of each token. Statistic analysis illus-
trates that 40% of the tokens masked by the random
masking strategy are stop-words or punctuation. As
shown in previous studies, it is valuable to distin-
guish the weights of different terms for passage
retrieval. Whether for the query or passage, terms
with higher important weights should contribute
more to the query-passage relevance estimation
process. Although the pre-train language model
itself is contextualized aware, we still hope that
the language model has a stronger feature of distin-
guishing term importance for retrieval task. How-
ever, the language model trained by the random
masking strategy is ﬂawed.
3.3 Retrieval Oriented Masking
As mentioned above, term importance is instruc-
tive for passage retrieval. Here, we explore to in-
troduce term importance into the MLM training.
More speciﬁcally, we incorporate the term impor-
tance information into token masking. Different
from the random masking strategy, whether a token
xiis masked is not only determined by the ran-
dom probability pr(xi), but also determined by its
term weight pw(xi). Here,pw(xi)is normalized
between value 0and1. The ﬁnal probability of
tokenxibeing masked out is pr(xi)+pw(xi).
Then the problem now is to calculate the term
weight of each token. Previous studies have
proposed different methods to calculate word
weights (Mallia et al., 2021; Ma et al., 2021), which
can be roughly divided into unsupervised and su-
pervised categories. To maintain the unsupervised

--- PAGE 3 ---
Astomachistheresultofanallergicreaction0.00.40.00.00.10.00.00.30.2
[mask]stomachis…allergicreactionTransformer EncoderTransformer EncoderA
[CLS]… …Input sentenceTerm weight distribution
Random MaskingA[mask]is…allergicreactionTransformer EncoderTransformer Encoder[CLS]… …stomach
Retrieval Oriented MaskingFigure 1: An illustration of our retrieval oriented masking (ROM) method. “stomach” is the token with the highest
term weight in the input sentence, thus sharing a larger probability of being masked out.
pre-training paradigm of LM, we adopt the unsuper-
vised method presented in the BPROP work (Ma
et al., 2021).
The BPROP proposed to compute the term
weight distribution in a sentence based on BERT’s
vanilla [CLS]-Token attention weights by consid-
ering that the token [CLS] is an aggregate of the
entire sequence representation. However, the term
distribution obtained from BERT’s vanilla atten-
tion is a semantic distribution, but not an informa-
tive distribution. BPROP leverages a contrastive
method to produce the ﬁnal distribution. Formally,
Given an input sentence x= (x1;x2;;xn), let
aidenotes the attention weight of xi2xfor the
[CLS] token which is calculated as an average of
each head’s attention weights from the last layer
of BERT. The BPROP method will produce a new
contrastive term distribution pw(x)in an totally
unsupervised manner based on (a1;a2;;an),
wherePn
ipw(xijBPROP ) = 1 . Here, we omit
the speciﬁc calculation process and more details
of BPROP can be found in the original paper (Ma
et al., 2021).
Once we calculate the term weight distribution
of each sentence in the corpus in advance, we can
conduct the LM pre-training with MLM learning
objective by our ROM strategy. It should be noted
that in the ROM method, the masked probability
of each token still relies on the uniform random
probability since we still want to keep the basic
properties of the pre-trained LM, rather than let the
LM only focus on a small number of higher-weight
tokens. In practice, the proportion of mask tokens
is also set to 15%, and statistical analysis show that
the proportion of stop-words and punctuation token
masked out in the ROM method dropped to 14%.4 Experiments
4.1 Datasets
We evaluate the proposed model on the follow-
ing data sets. MS MARCO Passage Ranking is
a widely used benchmark dataset for passage re-
trieval task, and which is constructed from Bing’s
search query logs and web documents retrieved
by Bing (Nguyen et al., 2016). Neural Question
is another passage retrieval dataset derived from
Google search (Kwiatkowski et al., 2019). For
each dataset, we follow the standard data splits in
previous work (Gao and Callan, 2022).
4.2 Compared Methods
To verify the effectiveness of our proposed method,
we adopt the following methods which focused
on PTM optimization as our main baselines: Con-
denser (Gao and Callan, 2021) adapts the Trans-
former architecture in LM pre-training to enhance
the text representations thus facilitating down-
streaming passage retrieval; coCondenser (Gao
and Callan, 2022) is an extension of Condenser,
which uses an unsupervised corpus-level con-
trastive loss to warm up the passage embedding
space; COSTA (Ma et al., 2022) introduces a novel
contrastive span prediction task in LM pre-training
aiming to build a more discriminative text encoder.
We directly borrowed several other competi-
tive baselines from the coCondenser paper, in-
cluding lexical systems BM25, DeepCT (Dai and
Callan, 2020), DocT5Query (Cheriton, 2019) and
GAR (Mao et al., 2021); and dense systems
DPR (Karpukhin et al., 2020), ANCE (Xiong et al.,
2021), and ME-BERT (Luan et al., 2021).

--- PAGE 4 ---
Table 1: Experiment Results for MS MARCO Passage Retrieval and Natural Question Datasets. T-test demon-
strates the improvements of ROM and coROM to the baselines are statistically signiﬁcant ( p0:05).
MethodMS MARCO Passage Neural Question
MRR@10 R@1000 R@5 R@20 R@100
BM25 18.6 85.7 - 59.1 73.7
DeepCT (Dai and Callan, 2019) 24.3 - - - -
DocT5Query (Cheriton, 2019) 27.7 94.7 - - -
GAR (Mao et al., 2021) - - 60.9 74.4 85.3
DPR (Karpukhin et al., 2020) - - - 74.4 85.3
BERT base 33.4 95.5 - - -
ANCE (Xiong et al., 2021) 33.0 95.5 - 81.9 87.5
ME-BERT (Luan et al., 2021) 33.8 - - - -
RocketQA (Qu et al., 2021) 37.0 97.9 74.0 82.7 88.5
Condenser (Gao and Callan, 2021) 36.6 97.4 - 83.2 88.4
COSTA (Ma et al., 2022) 36.6 97.1 - - -
ROM 37.3 98.1 73.9 83.4 88.5
coCondenser (Gao and Callan, 2022) 38.2 98.4 75.8 84.3 89.0
coROM 39.1 98.6 76.2 84.6 88.8
4.3 Experimental Setup
Our ROM language model pre-training starts with
a vanilla 12-layer BERT-base model. Following
previous work, we use the same data as BERT in
pre-training: English Wikipedia and the BookCor-
pus. In addition, like the coCondenser model, we
also train a language model that adds a contrastive
learning loss function based on the ROM model and
target corpus (Wikipedia or MS-MARCO web col-
lection). Here, the co-training drove ROM model
is denoted as coROM2.
During the ﬁne-tuning process, we adopt the
AdamW optimizer with 5e-6learning rate and
batch size 64for3epochs for the MS MARCO
passage dataset. For the NQ dataset, we follow
the hyper-parameter setting presented in the DPR
toolkit (Karpukhin et al., 2020). For the MS
MARCO passage dataset, the test set label is not
available, we only report results on the dev set. We
follow the evaluation methodology used in previ-
ous work (Gao and Callan, 2022). All experiments
are conducted on 8 NVIDIA Tesla 32G V100.
2The ﬁne-tuned model on MS MARCO pas-
sage ranking dataset is available at https:
//modelscope.cn/models/damo/nlp_corom_
sentence-embedding_english-base/summary .
The original ROM and coROM models will be publicly
available in the future.4.4 Evaluation Results
The overall performance of all baselines and ROM
are reported in Table 1. The results indicate that
ROM achieves the best performance. Firstly, the
improvement of the ROM model is extremely sig-
niﬁcant compared with the vanilla BERT model.
For example, the MRR@10 metric on the MS
MARCO dataset has increased from 33:4to37:3,
which empirically proves the beneﬁts of tailored
LM. Compared with other dense retrieval tailored
LMs, the ROM model achieves consistent improve-
ment on both two datasets. Additionally, similar to
the coCondenser model, coROM model will further
improve the passage retrieval performance with the
help of the contrastive co-training method, indi-
cating that high-quality text representation is the
foundation of dense passage retrieval.
Table 2: MRR@10 metric on the MS MARCO Passage
Ranking leaderboard. We bold the best performances
of both the dev and eval set.
Model dev eval
Search LM (SLM) + HLATR 46.3 45.0
Listwise + Fusion reranker 45.4 44.0
Cot-MAE (Wu et al., 2022) 45.6 43.8
Lichee-xxlarge + deberta-v3-large 45.2 43.6
Adaptive Batch Scheduling (Choi et al., 2021) 45.3 43.1
coCondenser (Gao and Callan, 2022) 44.3 42.8

--- PAGE 5 ---
Table 3: ROM results on MS MARCO passage dataset
with different methods for producing term weights.
Term Wights MRR@10 R@1000
BPROP 37.3 98.1
DeepImpact 37.6 98.2
4.5 MS MARCO Passage Ranking
LeaderBoard
To further verify the effectiveness of the ROM
family model, we conduct an experiment with full
retrieval-then-reranking pipeline and submit our re-
sult to MS MARCO LeaderBoard. Table 2 presents
the top systems on the MS MARCO Passage
Ranking leaderboard3. For the model description
“Search LM (SLM) + HLATR”, “Search LM(SLM)”
is the coROM model and HLATR (Zhang et al.,
2022b) is a lightweight reranking module coupling
both retrieval and reranking features thus further
improving the ﬁnal ranking performance. The ﬁnal
submission is an ensemble of multiple “reranking +
HLATR” models trained on different pretrained lan-
guage models (e,g. BERT, ERINE and RoBERTa).
4.6 Analysis
Quality of Term Weights Intuitively, the quality
of term weights will directly affect the performance
of the ROM model and the supervised method
can produce higher quality term weight results.
Thus, in addition to the BPROP method, we also
conduct ROM pre-training by term weight distri-
bution generated by the supervised DeepImpact
method (Mallia et al., 2021). The experimental re-
sults of MS MARCO passage dataset are presented
in Table 3. From which we can observe that: 1)
High-quality term weight results do lead to better
passage retrieval performance; 2) The improvement
brought by DeepImpact is much smaller than that
of the ROM model compared to the vanilla BERT
LM, which indicates that the unsupervised term
weight computation method is decent by consid-
ering that supervised method will inevitably intro-
duce extra training cost.
Attention Weights Analysis To verify that the pro-
posed ROM model is more discriminative for to-
kens with different weights, we compare the [CLS]-
token distribution of the ROM and vanilla BERT
models. In table 4, we present the top term weight
tokes produced by these two different models. We
can observe that the two sets have overlapping to-
3https://microsoft.github.io/msmarco/Table 4: Top attention weight tokens produced by
BERT and ROM model.
Text: A business analyst’s daily job duties can vary greatly,
depending on the nature of the current organization and
project ...
BERT: ., the, analyst, organization, duties
ROM: business, vary, duties, analyst, depending
kens. However, the higher attention weight tokens
produced by the ROM model is obviously more
reasonable, and the token set of the BERT model
even contains stop-word marks.
5 Conclusion and Future Work
In this paper, we investigate that current random
token masking MLM pre-training method is sub-
optimal for LM pre-training, as this process tends
to focus on stop words and punctuation. We sug-
gested a novel retrieval oriented masking strategy
which incorporates term importance information.
We evaluated our ROM and extended coROM LMs
on two benchmarks. The results showed that our
method is highly effective, and our ﬁnal model
can achieve signiﬁcant improvements compared to
previous tailored LMs for dense passage retrieval.
In this paper, we intuitively use the BPROP
method for term weight computation, and have not
compared it with other unsupervised term weight
methods. More detailed studies of term weight dis-
tribution based on the ROM model may produce
an in-depth understanding. In further, we only con-
duct experiments based on BERT basemodel, and
validation based on extensive LMs pre-trained with
MLM objective (e,g. RoBERTa ) can further help
to demonstrate the generality of the proposed ROM
method.
References
Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yim-
ing Yang, and Sanjiv Kumar. 2020. Pre-training
tasks for embedding-based large-scale retrieval. In
8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020 .
David R. Cheriton. 2019. From doc2query to docttttt-
query.
Donghyun Choi, Myeongcheol Shin, Eunggyun Kim,
and Dong Ryeol Shin. 2021. Adaptive batch
scheduling for open-domain question answering.
IEEE Access , 9:112097–112103.

--- PAGE 6 ---
Zhuyun Dai and Jamie Callan. 2019. Context-
aware sentence/passage term importance estima-
tion for ﬁrst stage retrieval. arXiv preprint
arXiv:1910.10687 .
Zhuyun Dai and Jamie Callan. 2020. Context-aware
term weighting for ﬁrst stage passage retrieval. In
Proceedings of the 43rd International ACM SIGIR
conference on research and development in Infor-
mation Retrieval, SIGIR 2020, Virtual Event, China,
July 25-30, 2020 , pages 1533–1536.
Yixing Fan, Xiaohui Xie, Yinqiong Cai, Jia Chen,
Xinyu Ma, Xiangsheng Li, Ruqing Zhang, Jiafeng
Guo, and Yiqun Liu. 2021. Pre-training methods in
information retrieval. CoRR , abs/2111.13853.
Emily Fawcett, Michelle Helena van Velthoven, and
Edward Meinert. 2020. Long-term weight manage-
ment using wearable technology in overweight and
obese adults: Systematic review. JMIR mHealth and
uHealth , 8.
Luyu Gao and Jamie Callan. 2021. Condenser: a pre-
training architecture for dense retrieval. In Proceed-
ings of the 2021 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2021, Vir-
tual Event / Punta Cana, Dominican Republic, 7-11
November, 2021 , pages 981–993.
Luyu Gao and Jamie Callan. 2022. Unsupervised cor-
pus aware language model pre-training for dense
passage retrieval. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022,
Dublin, Ireland, May 22-27, 2022 , pages 2843–
2853.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. 2020. Dense passage re-
trieval for open-domain question answering. In Pro-
ceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2020,
Online, November 16-20, 2020 , pages 6769–6781.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur P. Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: a benchmark for question answer-
ing research. Trans. Assoc. Comput. Linguistics ,
7:452–466.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Proceedings of the
57th Conference of the Association for Computa-
tional Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers , pages
6086–6096.Jimmy Lin, Rodrigo Nogueira, and Andrew Yates.
2021. Pretrained Transformers for Text Ranking:
BERT and Beyond . Synthesis Lectures on Human
Language Technologies. Morgan & Claypool Pub-
lishers.
Zheng Liu and Yingxia Shao. 2022. Retromae: Pre-
training retrieval-oriented transformers via masked
auto-encoder. arXiv preprint arXiv:2205.12035 .
Dingkun Long, Qiong Gao, Kuan Zou, Guangwei Xu,
Pengjun Xie, Ruijie Guo, Jian Xu, Guanjun Jiang,
Luxi Xing, and Ping Yang. 2022. Multi-cpr: A multi
domain chinese dataset for passage retrieval. CoRR ,
abs/2203.03367.
Yuxiang Lu, Yiding Liu, Jiaxiang Liu, Yunsheng Shi,
Zhengjie Huang, Shikun Feng, Yu Sun, Hao Tian,
Hua Wu, Shuaiqiang Wang, Dawei Yin, and Haifeng
Wang. 2022. Ernie-search: Bridging cross-encoder
with dual-encoder via self on-the-ﬂy distillation for
dense passage retrieval. CoRR , abs/2205.09153.
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and
Michael Collins. 2021. Sparse, dense, and atten-
tional representations for text retrieval. Trans. Assoc.
Comput. Linguistics , 9:329–345.
Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan,
and Xueqi Cheng. 2022. Pre-train a discriminative
text encoder for dense retrieval via contrastive span
prediction. CoRR , abs/2204.10641.
Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan,
Yingyan Li, and Xueqi Cheng. 2021. B-PROP: boot-
strapped pre-training with representative words pre-
diction for ad-hoc retrieval. In SIGIR ’21: The 44th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, Virtual
Event, Canada, July 11-15, 2021 , pages 1318–1327.
ACM.
Antonio Mallia, Omar Khattab, Torsten Suel, and
Nicola Tonellotto. 2021. Learning passage impacts
for inverted indexes. In SIGIR ’21: The 44th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, Virtual Event,
Canada, July 11-15, 2021 , pages 1723–1727.
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong
Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.
2021. Generation-augmented retrieval for open-
domain question answering. In Proceedings of the
59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International
Joint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-
tual Event, August 1-6, 2021 , pages 4089–4100.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. MS MARCO: A human generated machine
reading comprehension dataset. In Proceedings

--- PAGE 7 ---
of the Workshop on Cognitive Computation: Inte-
grating neural and symbolic approaches 2016 co-
located with the 30th Annual Conference on Neu-
ral Information Processing Systems (NIPS 2016),
Barcelona, Spain, December 9, 2016 .
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang
Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and
Haifeng Wang. 2021. Rocketqa: An optimized train-
ing approach to dense passage retrieval for open-
domain question answering. In Proceedings of the
2021 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2021,
Online, June 6-11, 2021 , pages 5835–5847.
Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,
Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji rong
Wen. 2021. Rocketqav2: A joint training method
for dense passage retrieval and passage re-ranking.
ArXiv , abs/2110.07367.
Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin,
Zhongyuan Wang, and Songlin Hu. 2022. Contex-
tual mask auto-encoder for dense passage retrieval.
CoRR , abs/2208.07670.
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul N. Bennett, Junaid Ahmed, and
Arnold Overwijk. 2021. Approximate nearest neigh-
bor negative contrastive learning for dense text re-
trieval. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021 .
Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and
Zhiyuan Liu. 2021. Few-shot conversational dense
retrieval. In SIGIR ’21: The 44th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, Virtual Event, Canada, July
11-15, 2021 , pages 829–838.
Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo,
M. Zhang, and Shaoping Ma. 2021. Optimizing
dense retrieval model training with hard negatives.
Proceedings of the 44th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval .
Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng
Lv, Nan Duan, and Weizhu Chen. 2021. Adversar-
ial retriever-ranker for dense text retrieval. CoRR ,
abs/2110.03611.
Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang,
and Nan Duan. 2022a. Multi-view document repre-
sentation learning for open-domain dense retrieval.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), ACL 2022, Dublin, Ireland, May
22-27, 2022 , pages 5990–6000.
Yanzhao Zhang, Dingkun Long, Guangwei Xu, and
Pengjun Xie. 2022b. Hlatr: Enhance multi-stage
text retrieval with hybrid list aware transformer
reranking. arXiv preprint arXiv:2205.10569 .Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen,
and Xueqi Cheng. 2021. Adaptive information seek-
ing for open-domain question answering. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-
11 November, 2021 , pages 3615–3626.
