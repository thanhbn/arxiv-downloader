# 2309.12767.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2309.12767.pdf
# File size: 1323851 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FURTHEST REASONING WITH PLAN ASSESSMENT : STABLE
REASONING PATH WITH RETRIEVAL -AUGMENTED LARGE
LANGUAGE MODELS
Yin Zhu∗
Nanjing University
yinzhu@smail.nju.edu.cnZhiling Luo†
DAMO Academy, Alibaba Group
godot.lzl@alibaba-inc.comGong Cheng
Nanjing University
gcheng@nju.edu.cn
ABSTRACT
Large Language Models (LLMs), acting as a powerful reasoner and generator, exhibit extraordinary
performance across various natural language tasks, such as question answering (QA). Among these
tasks, Multi-Hop Question Answering (MHQA) stands as a widely discussed category, necessitating
seamless integration between LLMs and the retrieval of external knowledge. Existing methods employ
LLM to generate reasoning paths and plans, and utilize IR to iteratively retrieve related knowledge,
but these approaches have inherent flaws. On one hand, Information Retriever (IR) is hindered by
the low quality of generated queries by LLM. On the other hand, LLM is easily misguided by the
irrelevant knowledge by IR. These inaccuracies, accumulated by the iterative interaction between
IR and LLM, lead to a disaster in effectiveness at the end. To overcome above barriers, in this
paper, we propose a novel pipeline for MHQA called Furthest-Reasoning-with-Plan-Assessment
(FuRePA), including an improved framework (Furthest Reasoning) and an attached module (Plan
Assessor). 1) Furthest reasoning operates by masking previous reasoning path and generated queries
for LLM, encouraging LLM generating chain of thought from scratch in each iteration. This approach
enables LLM to break the shackle built by previous misleading thoughts and queries (if any). 2) The
Plan Assessor is a trained evaluator that selects an appropriate plan from a group of candidate plans
proposed by LLM. Our methods are evaluated on three highly recognized public multi-hop question
answering datasets and outperform state-of-the-art on most metrics (achieving a 10%-12% in answer
accuracy).
Keywords Multi-hop QA ·Plan Assessment ·Furthest Reasoning
1 Introduction
The problem of Question Answering (QA) has long attracted considerable research attention. As progressively improved
models provide excellent performance on simple QA tasks, researchers have recently turned their attention to more
complex questions. Among these, Multi-Hop QA (MHQA) has shown enormous potential and challenge over the
years. ([ 1], [2], [3], [4], [5], [6] etc.) To solve complicated multi-hop questions, the ability of multi-step reasoning
and retrieval is essential and required, thus promoting the comprehensive improvement of natural language processing
models. In this complex task, a continuous and accessible reasoning path is crucial for accuracy and interpretability, as
the answer is obtained after combining several reasoning steps.
With the rapid rise of powerful Large Language Models (LLMs) and their CoT [ 7] capability, an intuitive and widely
implemented basic method for solving MHQA is to use LLMs and IRs alternately. Typically, at each step, LLM takes
previous reasoning steps, executes queries and retrieves evidence, and then outputs the next query. IR then uses this
query to retrieve the relevant information. This loop continues until LLM has obtained all the necessary evidence and
1Work done during an internship at DAMO Academy, Alibaba Group
2Corresponding authorarXiv:2309.12767v1  [cs.CL]  22 Sep 2023

--- PAGE 2 ---
Running Title for Header
Figure 1: Instance that shows importance of selecting high-quality query. Information Retriever detects relevant
documents from corpus with the query. First query recalls the ground truth at 10thbecause it’s not distinctive and
several highly related documents confuses IR. Second query of high quality recalls ground truth at top- 1with less
highly related documents due to its distinction among the corpus.
answered the question. [ 8], [9], [10], [11] apply this basic framework. Despite the amazing performance of LLM on
natural language tasks, there are some difficulties in solving MHQA with retriever-augmented methods.
The first major challenge lies in the evaluation and selection of high-quality query decomposed by LLM. More
specifically, with the ability of generation diversity, LLM makes a group of available candidate plannings. Different
plannings raise the ceiling of high-quality queries, but low-quality queries also appear in the group, which need to be
eliminated. If IR takes the low-quality query, the evidence retrieved is very likely to be irrelevant or unhelpful. In the
next iteration, LLM trusts this false evidence and makes even worse plans that deviate significantly from the correct
reasoning path. For example, as shown in Figure 1, LLM generates some different queries for the original question.
The first query focuses on “the NCAA Division” , while the second query emphasises information about “a record of
2-6” . During the retrieval process, it turns out that huge documents describe Colorado Buffaloes teams of different
sports and seasons in the NCAA division, so the first query lacks distinction and leads to wrong retrieved evidence.
The corresponding ground truth ranks 10th, lower than many other documents containing “the NCAA Division” . On
the contrary, the second query has the distinctive content, which makes it easy for IR to select the target document.
However, existing methods do not take advantage of generation diversity and rely heavily on the single generated query
of LLM without any evaluation, which causes irreversible damage. As an additive mechanism, self-consistency [ 12] is
mainly targeted at tasks where a diverse set of reasoning paths could directly lead to the same results without any query
in the middle of the reasoning steps.
Repetitive interaction between LLM and IR interrupts the CoT reasoning path, which is important in MHQA, so
maintain reasoning path continuity is another important challenge. The expected reasoning path deviates at two
stages, LLM planning and IR retrieval, because LLM and IR are both black boxes in the solution process. First, when
taking previous reasoning content, queries and evidence as input, LLM makes incorrect planning due to hallucination
and forgetting. Second, although the query is good enough, IR still fails to acquire the correct evidence. As conversation
and reasoning progress, planning and retrieval deviations occur from time to time. Researchers have made considerable
efforts to address this challenge with various ways to avoid breaking the chain of reasoning. ChatCot [ 11] incorporates
LLM and IR with multi-turn conversations to prevent tool interruptions from affecting the continuity of reasoning in the
chain of thought. It is worth noting that ChatCot assesses the value of retrieved documents to help clarify the reasoning
process for LLM. However, in a few-shot setting, too many rounds of conversation inevitably lead to neglect of previous
reasoning or instructions, thus causing a divergent reasoning path. In [ 9], LLM generates a global reasoning chain
2

--- PAGE 3 ---
Running Title for Header
at each iteration to avoid discontinuity, but suffers from the same problem as ChatCot. Iter-RetGen [ 10] takes each
iteration as a complete QA process. This method constructs an intact reasoning path at each iteration with little risk of
ignorance, but without any adaptive mechanism such as an early stop or confidence threshold, excessive iterations and
supporting documents can lead to uncertain reasoning paths and worse results, as shown.
In this paper, we propose a framework called Furthest Reasoning with Plan Assessment (FuRePA) to address the above
challenges (Figure 2). At each iteration t, LLM has access only to the retrieved evidence and the original question,
and then predicts the next plans through in-context learning. Unlike conventional interactive methods, the previous
chains of reasoning and questions are always masked, forcing LLM to reason from the beginning to the end. Through
this mechanism, the LLM analyses the context and reasons without the influence of the previous path. The reasoning
process also recovers from false evidence caused by IR.
Plannings include answer determination when all required documents are presented, and query execution when more
relevant information is needed. A trained plan assessor (right part of Figure 2) then analyses the candidate plans and
selects the best one. For different queries, the scorer in the assessor estimates their MRR with respect to the sampled
closed-domain context. The query with the highest score is then executed to retrieve a new document. As the example
in the figure shows, at iteration 0some queries are generated and the plan assessor selects the second one. The retrieved
evidence e0is added to the set. Then, at iteration 1, LLM is fed only evidence and the original question and makes
some plans. The first question “what year was the 14th season ... ” is used to retrieve evidence e1. At iteration 2the
plan evaluator chooses to answer the question and finishes the process.
We evaluate our framework on three multi-hop tasks. Our method gains about 10% on most metrics over the state of
the art in a few-shot setting. With the ablation study and the case study, our pipeline and optimisation strategy draw
conclusions as expected. The masking mechanism and scratch reasoning in our framework effectively handle the
aforementioned challenge of reasoning path continuity, as well as avoid forgetting the LLM and improve the reasoning
process. The plan evaluation procedure generates and selects plans in a more creative way, including better rewriting
and more accurate queries.
We summarize the contributions as follows.
•A trained plan assessor is a module that estimates the MRR score and selects the high-quality query , which
recalls similar texts. High quality searches improve retrieval and identify documents containing potential
evidence.
•A framework named Furthest Reasoning maintains reasoning path continuity and effectively mitigates harm
from previous chain of thought or query to the minimum by masking them.
2 Related Work
Chain of Thought Prompting Since large language models offer exciting prospect of in-context few-shot learning
through various promptings, Chain of Thought [ 7] is proposed to improve reasoning ability by clarifying intermediate
reasoning steps. Following the pioneering work, more works on eliciting reasoning in LLM have been researched. "Step
by Step" [ 13] prompting is proved to be effective under zero-shot setting. Auto-CoT [ 14] automates the construction of
presentations, builds demonstrations by sampling rich questions and generates corresponding reasoning chains instead
of manual design. Self-consistency [ 12] is a decoding strategy that uses ensemble to replace greedy search on the basis
of few-shot-cot to improve decoding accuracy. Least-to-Most [ 15] guides the model to decompose the problem into
sub-questions and answer the sub-questions iteratively at each step. Similar methods include Self-Ask [ 16], ReAct [ 8]
etc, which all apply to multi-hop questions. To conclude, chain of thought, as a method to introduce structural and
logical coherence in generative models, expands the application of LLM to various tasks. By introducing targeted
hints, it is expected to improve the generating quality and play an important role in many complex natural language
processing tasks, most of which require a series of reasoning steps.
Query Decomposition Query decomposition and optimization is crucial in MHQA and other multi-step reasoning
tasks. [ 17] decomposes tough problems into simpler sub-tasks. LLMs in the decomposer and sub-task handlers
take corresponding few-shot prompts. Plan-and-Solve [ 18] also makes decomposition at the beginning and carries
it out. HPE [ 19] parses the multi-hop question into a structured intermediate representation composed of simple
questions, representing the relationships. [ 20] attempts to explore more effective methods than CoT for eliciting faithful
reasoning through factored decomposition and re-composition. Nevertheless, most of the methods above only make
LLMs generate the queries and trust them without any optimization or evaluations. Actually, LLMs sometimes make
unsatisfying plans, which are not beneficial to following steps.
3

--- PAGE 4 ---
Running Title for Header
Figure 2: Iteration details in our framework are shown in the right part. In each iteration t, LLM generates reasoning
chain and plans (e.g. query or answer) with retrieved context and original question as input. Module plan assessor
handles the plannings and chooses the best one. Left part is the solving process of example of Figure 1
Retrieval-augmentation The idea of retrieval-augmentaion is widely used in all kinds of models. For example, Realm
[21] explored open-domain extractive question answering, and models like RAG [ 22] bring retrieval into generative
models. Thanks to rapidly developing LLMs, different methods of retrieval-augmented large language models provide
promising solutions for complex knowledge-intensive QA tasks. In-context RALM [ 23] and REPLUG [ 24] leave the
LM architecture unchanged and prepend retrieved documents to the input.
Only retrieval of related documents has positive effect, while those irrelevant documents can also harm the quality of
generation and reasoning. [ 25] evaluates data importance and quality of retrieved data points. [ 26] trains the query
rewriter in retrieval with policy gradient reinforcement learning. In addition, the newly added retrieval module in the
pipeline brings resource computing pressure (not only the retrieval requires resources, but the generated model also
consumes more resources due to the need to consider the retrieval results), which also requires further evaluated.
When introducing external information into language models, recent works didn’t try to mitigate the impact of irrelevant
documents when they are retrieved and trusted. To leverage the improvement on retrieval quality and large computing
cost in LLMs, our work makes effort to undermine the inevitable effect from IR, avoiding more error accumulation in
the following procedure.
3 Method
The original question Qand a retrieval corpus D={d}serve as the input of the MHQA instance.
3.1 Furthest Reasoning
In each iteration twith upper limit T, LLM Maccepts retrieved evidences Etand the original question Qintegrated as
input. With regard of the temperature tpandnchoices in the parameters, LLM produces an output Pt={pi|1≤
i≤n}, including ncandidate plannings. The circulation is shown in Figure 2. The input and output of LLM are
represented as:
Pt=M(prompt( Et,Q,sgn(t)), tp0+ ∆tp, n),∀0≤t < T (1)
4

--- PAGE 5 ---
Running Title for Header
sgn(t)is a sign function, which tells whether the step is the first. The prompts at the beginning differ from the latter
ones because we definitely cannot get the answer without any retrieved documents. Therefore, the prompts in the first
iteration are designed to take original question and output queries only.
3.1.1 Prompting
Since the round of reasoning sgn(t)is known, the detailed prompting strategy is implemented on the retrieved evidences
Etand question Q. For example, the system instruction template at latter steps is:
System: “You are given problems requiring multi-step searching and reasoning with related contexts. You should tell
me the answer if all necessary information is provided in the context, otherwise tell me what specific information to
search next. Don’t answer with uncertainty. Lines only start with ’[Analysis]’ or ’[Answer]’ or ’[Search]’. ”
This instruction requires LLM to understand the context (retrieved evidences here) and question, and produce the
continuous reasoning chain from start to the furthest possible state, followed by planning like answer or query. The
planning is generated through chain of thought, consisting of a reasoning chain rand the following plan qora. The
exemplars and the expected response patterns are as follows:
User: “Context: EtQuestion: Q”
Assistant: “[Analysis] r[Search] q”or“[Analysis] r[Answer] a”
Here we prompt the LLM to plan for the next step only with retrieved evidences and the original question. The output
of LLM is a group of plannings Ptdefined by:
Pt:={pt
1, pt
2,···, pt
n}
={(rt
1, qt
1),(rt
2, qt
2),···,(rt
k, qt
k)} ∪ {(rt
k+1, at
1),(rt
k+2, at
2),···,(rt
n, at
l)}
=Qt∪At(2)
Unlike many other general methods, we mask the previous reasoning content and executed query because former
reasoning and plans have negative effect on the multi-step reasoning process, which is proved in section 4. Thus,
LLM will analyze the multi-hop question from scratch according to retrieved evidences, no matter whether they help
or not, preventing new plans affected by erroneous previous reasoning and evidences. It is noteworthy that in the
few-shot examples, we teach LLM to distinguish similar but irrelevant evidences from truly demanded ones, which
further mitigates the redundancy information caused by IR. The output plannings above include kquery executions
{qt
i|1≤i≤k}andlanswer determinations {at
j|1≤j≤l}in iteration t. Each planning is generated through CoT
rt
i.
3.1.2 Iteration for Furthest Reasoning
We iterate the furthest reasoning step for at most Ttimes. After we get Ptat step t, the plan assessor (section 3.2) is
used to select the best planning pbestamong them. If pbestclaims the final answer a, then the whole iteration ends with
reasoning path rand answer a. Otherwise pbestis a query execution q. Retriever module sorts the documents in Dand
adds the top new document dinto retrieved evidence set Et, which is defined by:
Et=∅ ift= 0
Et−1∪ {d},where d̸∈ Et−1andd∈ D,otherwise.(3)
Then the next iteration runs with new retrieved context Et+1. The answer will be forcibly generated if not determined
after last round.
3.2 Plan Assessor
The plan assessor Ais a module that takes the various candidate plannings Ptgenerated by LLM and outputs the best
planning pbest:
pbest=A(t,Pt, Et,D) (4)
In each iteration in Figure 2, LLM gives various chains of thought and subsequent plans, including query execution and
answer determination. What plan assessor do is to elect the most suitable one, so that the probability of low quality
query and randomly speculated answer is reduced and the whole reasoning process becomes more convincible. Etis
the set of executed queries.
5

--- PAGE 6 ---
Running Title for Header
3.2.1 Planning Decision
Apart from the first step which is impossible to induce the answer, the candidate plannings may include answer
determinations and query executions as shown in section 3.1. The planning decision is made by voting algorithm. When
the proportion of the answer plannings in Ptsurpasses the certain threshold θ, the reasoning process ends with one pair
of complete reasoning path and answer. If not, the rest plannings of query execution are sifted and ranked by query filter
(section 3.2.2) and scorer (section 3.2.3). At iteration 2in Figure 2, when most of the plans provide the final answer as
“2009, Big 12 Conference” , any of the answer plan and its CoT is taken as the outcome. The function is expressed as:
PlanDecision( t,Pt) =(
At
0,ift >0and|At|
|Pt|≥θ
Qt,otherwise(5)
3.2.2 Query Filter
We filter the query plannings Qtand get ˆQtwith density-based clustering method to eliminate the same queries as
before. ˆQtconsists of the centered examples from each cluster. In cases ˆQtis empty, which means similar queries
were executed before, this iteration ends with the temperature increasing ∆tp. When identical queries are generated
repetitively, which means procedure suffers from low-quality queries or absence of evidences. Temperature raise
(decoding probability threshold change) replaces the greedy decoding used in chain-of-thought prompting with more
diverse reasoning paths and plans. With this duplication detection and reasoning diversity, LLM can generate alternated
contents like rewriting or entity replacement.
3.2.3 Query Scorer
The query scorer calculates the score representing the quality of a query in a certain context:
s= QueryScorer( q,D) (6)
Sampling and Labeling We sampled all training data the same way as furthest reasoning in section 3.1. For each
query generated at different steps, IR scores and ranks the similarity between query and documents directly. The dataset
does not provide the exact match document of sub-query, so the ground truth is taken as the first gold document dg
in sorted documents. We label documents from head to the ground truth as positive ones (or highly related ones in
Figure 1) ( ˆyj,1≤j≤g) , and documents from ground truth to the end as negative ( ˆyj, g < j ≤ |D| ) because we will
use Mean Reciprocal Rank (MRR)[ 27] for reference in scoring system. The gold score of a query ˆsis the reciprocal of
the ground truth location 1/g.
In the example shown in section 1, if the first query “the NCAA Division” is executed, the top documents after IR
ranking will be “The 2014 Colorado Buffaloes ... ” and“The 2012 Colorado Buffaloes football ... ” , while the most
relevant ground truth “The 2009 Colorado Buffaloes football team represented the University of Colorado in the 2009
NCAA Division I FBS college football season. ” is ranked 10th. In this case, the quality score of this query is 1/10and
the documents ranked before ground truth are all labeled as true. Our neural network will be trained to fit the labels.
When the model predicts the quality of a query, the estimated true documents will be detected as highly related and
confusing ones and undermine the score of that query. We guarantee the query as much as accurate and distinctive with
this estimation.
Scoring We train and optimize the scorer with the following process.
Hj=Encoder (q;dj), sj=σ(Hj
[CLS]ws) (7)
strain=1PN
j=1sj(8)
The input query qis encoded with each document djand projected to a scalar sj, where Hjis the last hidden states for
“[CLS] q[SEP] dj” and Hj
[CLS] is the hidden states of [CLS] token. The reciprocal of the sum of each scalar sjis the
MRR score of query qin training process. The loss is expressed as follows. BCELoss is used for the document labels
and MSELoss is for regression fitting of MRR score. The introduction of MSELoss is to help supervise and predict the
document labels better.
L= BCELoss( sj,ˆyj) +α·MSELoss( strain,ˆs) (9)
In predicting phase, the score are calculated differently from Formula 8. Only the documents whose score exceeds 0.5
will be predicted as true. In addition, query score will be 0if all documents are negative because sometimes when an
6

--- PAGE 7 ---
Running Title for Header
invalid query is executed, the sum of sjwill be about the same as zero, which enlarges its reciprocal score spred. For
example, in iteration 1in Figure 2, the second query “What specific year and conference is being referred to in the
context” is an abstract description pointing at no concrete information. In this case sjof documents in Dare nearly
zero, strainwill be a huge number which poorly represents the real quality of the query.
spred=(
0PN
j=1sgn(2·sj−1) = 0
1PN
j=1sgn(2·sj−1)PN
j=1sgn(2·sj−1)>0(10)
At last, queries in ˆQtare sorted by their scores. The highest one is reckoned as the best query qbestand added to Et.
4 Experiments
4.1 Experimental Settings
Datasets and Evaluation Metrics We experimented on three recognized MHQA datasets that require information
retrieving process: HotPotQA[ 1], 2WikiMultiHopQA[ 28], and MuSiQue[ 29]. The example analyzed in section 1 is
from HotPotQA. As for the evaluation metrics, we adopt the offical metrics and code in each dataset. Besides, Acc†
from Iter-RetGen[10] and cover-EM[30] are also added for comparison.
Baselines Our baselines are some recent methods that introduces IR to LLM and explores their interaction. Apart
from the basic LLM reasoning baseline (Direct Retrieval and CoT), others are designed for tasks involving multi-step
interaction between LLM and IR (ChatCot[ 11], Iter-RetGen[ 10], SearChain[ 9]). All methods in our implementation
use the same retriever. We didn’t experiment with ReAct[ 8], Self-Ask[ 16] and similar methods for two reasons. First,
results of these universal prompting methods are shown in papers like Iter-RetGen. Besides, they are conceptually
similar to ChatCot and No mask setting (section 4.3) on our tasks, which are experimented and compared. In addition,
Iter-RetGen does not provide related code and prompts, therefore its results with training are directly taken from its
paper, while others are reproduced under our settings.
Implementation Details The large language model used in our experiment, including Acc†in evaluation metric is
gpt-3.5-turbo provided by OpenAI. The information retriever module is TART [ 31] (following Iter-RetGen). For all
three datasets, the experiments are done on top 1/10 of dev set under distractor setting. ( 1/10 due to the large cost on
API call, and proved to be distributed similarly with whole dev set)
In furthest reasoning framework, the initial temperature tp0of LLM is 0.2/1.0 and the increase ∆tpis0.8/0.5 . Choices
numnis set 5. The encoder of type classifier is roberta-large [32]. Original training data in three datasets is divided
into train and dev set under 9:1for classifer training process. The maximum iteration Tin reasoning process is 6.
For plan assessor module, the threshold θis0.6. In query filter, the algoritm of density-based clustering is DB-SCAN
[33] with epsas2. The features of query is bag of words [ 34]. In query scorer, the encoder is also roberta-large . The
training data sampled for the scorer only comes from hotpotQA dataset. Weight of MSELoss αis0.1.
4.2 Main Results
The results of baselines and our work are presented in Table 1. (1) Effective furthest reasoning framework We
compare our model with recent competitive baselines on these datasets. On three datasets, most of the metrics surpass
state of the art. In answering, all metrics exceed those of existing methods, especially on cover-EM and Acc†. In
supporting facts, despite that SearChain1doesn’t achieve excellent results on answer metrics compared to other methods,
its exact match and precision exhibit better performance. SearChain focuses on the credibility of external knowledge,
which leads to a more cautious retrieval strategy and a higher precision of supporting facts. On the contrary, direct
retrieval method simply recalls as many documents related to original question, reaching for better recall score. Our
model mainly concerns answering questions with external knowledge and pays attention to both precision and recall.
As to token cost, our model saves certain amount of tokens compared with other high-accuracy methods, nearly 1/10of
token used in ChatCoT. (2) Improvement with plan assessor With the plan assessor module, some confusing queries
are sifted and thus enhances and stablizes reasoning process to a certain extent. However, the module seems not helpful
on 2WikiMultiHop dataset. The major reason is that multi-hop questions in 2WikiMultiHopQA are not intuitive for
1Actually testing SearChain on MuSiQue is not appropriate. Multi-hop questions in MuSiQue is composed of single hop questions
like Natural Questions [ 35] and Squad [ 36]. However, dpr-reader-multiset-base, the reader used in SearChain is pretrained on these
datasets.
7

--- PAGE 8 ---
Running Title for Header
Table 1: All metrics of detail in results
MethodAnswer Supporting Facts Joint Cost
Cover-EM EM F1 Precision Recall Acc†EM F1 Prec Recall EM F1 Prec Recall Avg Token
HotPotQA
Direct 58.11 32.57 49.11 47.59 62.86 71.35 0.00 48.66 36.27 77.00 0.00 25.68 19.00 53.03 254
Cot 56.49 36.89 52.36 51.35 61.05 70.27 0.00 48.66 36.27 77.00 0.00 27.89 20.76 52.04 929
IterRetGen 45.80 - 61.10 - - 73.40 - - - - - - - - 3043
-w/o train 54.86 40.00 56.13 57.04 62.47 70.68 2.03 57.27 47.54 76.10 0.95 35.10 29.70 51.76 913
ChatCot 58.11 17.03 36.59 32.90 61.87 70.54 - - - - - - - - 40505
SearChain 49.59 0.00 16.02 10.16 55.15 63.38 29.05 65.84 75.45 62.57 - 12.49 8.43 38.26 4150
OurModel 70.41 38.92 59.21 57.24 74.32 83.65 26.49 68.09 75.06 69.31 7.70 41.13 44.66 53.14 4139
-w/o PA 67.03 35.68 55.56 53.13 70.87 79.73 25.14 66.23 73.11 67.58 8.78 38.74 41.07 50.64 2996
2WikiMultiHop
Direct 50.28 23.87 40.10 36.59 53.53 57.52 0.00 39.98 29.94 63.86 - - - - 230
Cot 39.62 14.96 30.06 26.28 42.49 52.72 0.00 39.98 29.94 63.86 - - - - 907
IterRetGen 36.00 - 47.40 - - 58.50 - - - - - - - - 5107
-w/o train 57.28 19.33 36.85 33.19 60.46 60.46 0.08 51.20 41.74 70.33 - - - - 1532
ChatCot 58.23 2.31 21.94 15.78 59.70 60.22 - - - - - - - - 53535
SearChain 50.28 0.00 15.63 9.83 53.81 56.01 32.14 65.07 71.18 62.89 - - - - 4507
OurModel 70.72 35.96 55.36 51.21 70.64 73.19 40.41 70.40 69.12 74.45 - - - - 5336
-w/o PA 70.33 34.61 54.18 50.06 70.10 74.07 36.20 70.02 65.57 80.83 - - - - 3726
MuSiQue
Direct 30.29 14.94 28.72 - - 43.57 0.00 39.72 27.8 69.50 - - - - 607
CoT 31.54 19.09 30.85 - - 38.17 0.00 39.72 27.8 69.50 - - - - 1276
IterRetGen 26.70 - 41.80 - - 40.80 - - - - - - - - 6533
-w/o train 38.59 26.56 39.28 - - 51.45 0.00 54.31 41.78 78.84 - - - - 1960
ChatCot 37.34 9.60 27.62 - - 44.81 - - - - - - - - 53674
SearChain 42.32 0.00 16.51 - - 53.94 48.96 76.87 87.83 71.78 - - - - 4289
OurModel 55.19 30.71 47.95 - - 64.31 33.20 70.43 66.85 84.44 - - - - 5920
-w/o PA 52.28 28.22 45.28 - - 60.17 26.97 67.09 62.40 84.02 - - - - 4309
Table 2: Ablation results on masking Reasoning and Query.
Framework without plan assessor Masking Settings HotPotQA 2WikiMultiHopQA
Type Reasoning Query Cover-EM EM F1 Acc†Token Cover-EM EM F1 Acc†Token
No mask ! ! 60.81 27.03 40.75 66.90 3772 57.77 13.55 33.45 54.98 4847
No mask (reasoning from scratch) ! ! 59.46 25.68 41.64 67.57 3907 - - - - -
No mask (mask at answering) ! ! 65.54 33.11 50.30 74.32 3598 - - - - -
Reasoning mask % ! 62.16 28.37 45.52 73.65 3361 - - - - -
Query mask ! % 62.16 25.68 41.77 72.97 3466 - - - - -
Double mask % % 72.30 43.24 57.42 82.43 3098 70.12 32.27 53.03 76.89 3714
Double mask (no CoT) % % 68.92 28.38 45.97 76.35 1982 - - - - -
LLM to decompose. For example, “father in law” requires one step of retrieving “husband/wife” , which is not explicit
and difficult for LLM to generate appropriate queries. We may have to apply targeted prompts for this kind of questions.
(3) Findings Considering the results of Direct Answer and Cot, their retrieved documents are exactly the same from the
identical retrieving strategy. The reason CoT performs worse than direct answering is that CoT demands precision of
supporting facts to reason from evidence like few-shot examples, which is not satisfied under direct retrieval of original
question. Direct answering does not involve such issues.
4.3 Ablation Study
Masking Reasoning and Query The framework of Furthest Reasoning makes sense in masking previous reasoning
and query process. Related ablation experiments are shown in Table 2. The exemplars in few shot and other settings are
exactly the same. Experiments are done in a small part of dev set in HotPotQA and 2WikiMultiHopQA.
Main results we focus on are No mask andDouble mask .No mask setting takes history reasoning and query content as
input for LLM at each iteration, while Double mask setting masks both of them and leaves only original question and
retrieved evidences. This comparison implies the negative effect history reasoning and query content have on multi-hop
process. Token cost is saved in Double mask apart from better answer metrics. Reasoning mask andQuery mask are
settings masking one each, leading to the same conclusion. To be more rigorous, No mask (reasoning from scratch)
imitates furthest reasoning in generating chain of thought from scratch instead of continue reasoning, and the result
resembles No mask . The solution actually consists of two parts: reasoning process, and forcible answering process. To
eliminate the influence of possible shortcuts in forcible answering, No mask (mask at answering) alternates forcible
answering with double mask setting. The gaps illustrate that redundant history reasoning chain adversely affects the
whole pipeline in multi-hop solution. In addition, CoT in reasoning and making following plans still matters as Double
mask (no CoT) presents.
8

--- PAGE 9 ---
Running Title for Header
Table 3: αin training scorer, F1 is document label prediction
Model α Doc F1
PARoBERTa-base 0 71.04
PARoBERTa-base 0.1 72.85
PARoBERTa-large 0.1 79.04
4.4 Analysis
Mixed Label Loss For those who may concern, here is the reason for an extra MSELoss in the loss function mentioned
in section 3.2.3. The loss function of training on possible document labels consists of two parts: BCELoss and MSELoss.
Although only document labels are used for query scores in predicting phase, the regression on MRR is beneficial
to label prediction. Regression on MRR connects the candidate documents of one query and enlarges the difference
among them, which turns out efficient. The weight αof MSELoss should not be to large at the beginning as model has
to laern basic classification first.
Limitation First, because of the high cost on gpt-3.5 , our experiments are done with a small proportion of the datasets,
which is about thousands of examples. We will study the performance of our model on other complex datasets and other
open-source LLMs. Secondly, despite the generations of LLM decoding is evaluated by plan assessor, the plannings
still largely rely on the capability of LLMs. Language models demonstrate more randomness when decoding probability
threshold is changed, thus impacting the generated plannings. Third, in real life, the IR may not be a traditional retriever
based on similarity. If IR stands for a database for instance, our method will need some modification.
5 Conclusion
Based on the experiments and results presented above, it is evident that Furthest Reasoning with Plan Assessment
exhibits exceptional performance in addressing multi-hop questions. Firstly, the Furthest Reasoning framework
significantly contributes not only to the reasoning process but also to the direct answering process. In complex and
long-term problem-solving scenarios such as multi-hop questions, FuRePA is designed to generate more effective plans,
avoiding the influence of convoluted previous reasoning processes. Moreover, the Plan Assessment component plays a
crucial role in selecting the most suitable query from the LLM-generated candidates, leading to more relevant evidence
retrieval by the Information Retriever (IR). This enhancement in query selection contributes to a more effective and
accurate multi-hop question-solving process.
References
[1]Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christo-
pher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint
arXiv:1809.09600 , 2018.
[2]Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. Hybridqa: A dataset of
multi-hop question answering over tabular and textual data. arXiv preprint arXiv:2004.07347 , 2020.
[3]Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehen-
sion across documents. Transactions of the Association for Computational Linguistics , 6:287–302, 2018.
[4] Tomáš Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward
Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational
Linguistics , 6:317–328, 2018.
[5]Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new
dataset for open book question answering. arXiv preprint arXiv:1809.02789 , 2018.
[6]Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. A survey on multi-hop question answering and generation.
arXiv preprint arXiv:2204.09140 , 2022.
[7]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information
Processing Systems , 35:24824–24837, 2022.
9

--- PAGE 10 ---
Running Title for Header
[8]Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:
Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 , 2022.
[9]Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-seng Chua. Search-in-the-chain: Towards
the accurate, credible and traceable content generation for complex knowledge-intensive tasks. arXiv preprint
arXiv:2304.14732 , 2023.
[10] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-
augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294 ,
2023.
[11] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, and Ji-Rong Wen. Chatcot: Tool-
augmented chain-of-thought reasoning on chat-based large language models. arXiv preprint arXiv:2305.14323 ,
2023.
[12] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny
Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 ,
2022.
[13] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models
are zero-shot reasoners. Advances in neural information processing systems , 35:22199–22213, 2022.
[14] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language
models. arXiv preprint arXiv:2210.03493 , 2022.
[15] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui,
Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models.
arXiv preprint arXiv:2205.10625 , 2022.
[16] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing
the compositionality gap in language models. arXiv preprint arXiv:2210.03350 , 2022.
[17] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal.
Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406 , 2022.
[18] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-
solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint
arXiv:2305.04091 , 2023.
[19] Ye Liu, Semih Yavuz, Rui Meng, Dragomir Radev, Caiming Xiong, and Yingbo Zhou. Answering complex
questions over text by hybrid question parsing and execution. arXiv preprint arXiv:2305.07789 , 2023.
[20] Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus,
Evan Hubinger, Jackson Kernion, Kamil ˙e Lukoši ¯ut˙e, et al. Question decomposition improves the faithfulness of
model-generated reasoning. arXiv preprint arXiv:2307.11768 , 2023.
[21] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language
model pre-training. In International conference on machine learning , pages 3929–3938. PMLR, 2020.
[22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-
intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020.
[23] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083 , 2023.
[24] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 , 2023.
[25] Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, and
Ce Zhang. Improving retrieval-augmented large language models via data importance learning. arXiv preprint
arXiv:2307.03027 , 2023.
[26] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for retrieval-augmented large
language models. arXiv preprint arXiv:2305.14283 , 2023.
[27] Ellen M V oorhees et al. The trec-8 question answering track report. In Trec, volume 99, pages 77–82, 1999.
[28] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for
comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060 , 2020.
10

--- PAGE 11 ---
Running Title for Header
[29] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via
single-hop question composition. Transactions of the Association for Computational Linguistics , 10:539–554,
2022.
[30] Corby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, and Saurabh Tiwary. Knowledge-aware
language model pretraining. arXiv preprint arXiv:2007.00655 , 2020.
[31] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and
Wen-tau Yih. Task-aware retrieval with instructions. arXiv preprint arXiv:2211.09260 , 2022.
[32] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692 , 2019.
[33] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering
clusters in large spatial databases with noise. In kdd, volume 96, pages 226–231, 1996.
[34] Zellig S Harris. Distributional structure. Word , 10(2-3):146–162, 1954.
[35] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle
Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering
research. Transactions of the Association for Computational Linguistics , 7:453–466, 2019.
[36] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine
comprehension of text. arXiv preprint arXiv:1606.05250 , 2016.
A Prompts
The prompts we use in our work are as follows, including prompts for the first iteration and for the latter ones. More
prompting settings can be found in code.
A.1 First Iteration
System You are given problems requiring multi-step searching and reasoning. You should follow the examples and tell
me what information to search first accurately.
User To solve this multi-hop problem: What was the name of the Infantry that was commanded by a Senator who was
know as Jim Lane? What specific information should I search from wiki at first step? Search for one target at first.
Assistant [Search] the Senator who was known as Jim Lane
User To solve this multi-hop problem: Which person born on March 23, 1902 did George Roby Dempster have political
battles with? What specific information should I search from wiki at first step? Search for one target at first.
Assistant [Search] the person George Roby Dempster have political battles with
User To solve this multi-hop problem: Do director of film Swami Dada and director of film Common Clay (1930 Film)
have the same nationality? What specific information should I search from wiki at first step? Search for one target at
first.
Assistant [Search] the nationality of director of film Swami Dada
User To solve this multi-hop problem: How many square miles is the state that houses the university that Joseph L.
Badaracco attended? What specific information should I search from wiki at first step? Search for one target at first.
Assistant [Search] the university Joseph L. Badaracco attended
User To solve this multi-hop problem: The Tudor-style Norfolk Hotel, now run by the Fairmont group, is a landmark
in the capital city of which west African country? What specific information should I search from wiki at first step?
Search for one target at first.
Assistant [Search] the Tudor-style Norfolk Hotel, now run by the Fairmont group, is a landmark in what city
A.2 Latter Iterations
System You are given problems requiring multi-step searching and reasoning with related contexts. You should tell me
the answer if all necessary information is provided in the context, otherwise tell me what specific information to search
next. Don’t answer with uncertainty. Lines only start with ‘[Analysis]’ or ‘[Answer]’ or ‘[Search]’.
11

--- PAGE 12 ---
Running Title for Header
User
Context:
Saint Motel: The band consists of A/J Jackson (lead vocals, guitar, piano), Aaron Sharp (lead guitar), Dak Lerdamorn-
pong (bass), and Greg Erwin (drums). Curve Lake First Nation: The Curve Lake First Nation registered a population on
these three reserves of 741, with an additional 1,050 registered band members living off-reserve.
Question:
Which band has more members, Saint Motel or Curve?
Assistant
[Analysis] According to the context, we now know Saint Motel has 3 members. The context provides information about
Curve Lake First Nation, not the band. We still need information about the band Curve.
[Search] the number of members in the band Curve
User
Context:
Dhyanchand Stadium is a cricket ground in Jhansi, India. The stadium is named after Major Dhyan Chand who lived in
Jhansi. Dhyan Chand: He is known for his extraordinary goal-scoring feats, in addition to earning three Olympic gold
medals (1928, 1932, and 1936) in field hockey, during an era where India was the most dominant team in Hockey.
Question:
Where is the stadium located which is named after the man who won three Olympic gold medals in 1928, 1932, and
1936?
Assistant
[Analysis] The man who won three Olympic gold medals in 1928, 1932, and 1936 is Dhyan Chand. The stadium named
after him is Dhyanchand Stadium. Dhyanchand Stadium is located in Jhansi, India. Then we get the answer to the
question.
[Answer] Jhansi, India
User
Context:
Matthias W. Day (August 8, 1853 – September 12, 1927) was a career American army officer who received the Medal
of Honor, the United States’ highest military decoration, for his actions during the American Indian Wars in the latter
half of the 19th century. Day was a longtime officer with the African-American 9th Cavalry Regiment, seeing action
during the Apache Wars against the Apache leaders Victorio and Geronimo.
Question:
In 2010, who controlled the Senate and the body who presides over any impeachment trial of the president of the
country Matthias W. Day holds citizenship in
Assistant
[Analysis] Matthias W. Day holds citizenship in the United States. Then we need to know the body who presides over
any impeachment trial of the president of the United States?
[Search] Who presides over any impeachment trial of the president of the United States
User
Context:
The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.
Question:
The Oberoi family is part of a hotel company that has a head office in what city?
Assistant
[Analysis] The Oberoi family is part of the hotel company called The Oberoi Group. Then we need to know the Oberoi
Group has a head office in what city.
[Search] the Oberoi Group has a head office in what city
User
Context:
"Human Error" is the twenty-fourth episode and season finale of the third season of "House" and the seventieth episode
overall. House (also called House, M.D.) is an American television medical drama that originally ran on the Fox
network for eight seasons, from November 16, 2004 to May 21, 2012.
Question:
"Human Error" is the season finale of the third season of a tv show that aired on what network?
12

--- PAGE 13 ---
Running Title for Header
Assistant
[Analysis] "Human Error" is the season finale of the third season of "House". The tv show "House" ran on the Fox
network. Then we get the answer.
[Answer] Fox network
B Case Study
See tables below.
13

--- PAGE 14 ---
Running Title for Header
Question: Onika Tanya Maraj is a judge on a television show hosted by whom?
Answer: Ryan Seacrest
FuRePA
Iteration 0
Plans
the television show Onika Tanya Maraj is a judge on
Onika Tanya Maraj to find out on which television show she is a judge
Onika Tanya Maraj’s television show
Onika Tanya Maraj television judge
Evidence
Onika Tanya Maraj (born December 8, 1982), known professionally as Nicki Minaj, is a rapper, singer, songwriter and model.
Iteration 1
Analysis
Onika Tanya Maraj, also known as Nicki Minaj, is a judge on a television show. We need to find out the name of the host of that television
show.
Plans
Name of the host of the television show where Nicki Minaj is a judge
Nicki Minaj is a judge on a television show hosted by whom?
Who hosts the television show that Onika Tanya Maraj is a judge on?
Nicki Minaj recent television appearances or judge roles
Evidence
American Idol (season 12): Randy Jackson returned for his twelfth season as judge and was joined by new judges Mariah Carey, Nicki
Minaj and Keith Urban, who replaced Jennifer Lopez and Steven Tyler.
Iteration 2
Analysis
Onika Tanya Maraj, known professionally as Nicki Minaj, is a judge on a television show. The television show is ’American Idol’ and the
host of ’American Idol’ is not mentioned in the given context.
Plans
What television show is Onika Tanya Maraj a judge on?
Who is the host of "American Idol"
Host of the television show where Onika Tanya Maraj is a judge.
Who hosts the television show that Onika Tanya Maraj is a judge on?
Host of the television show with Onika Tanya Maraj as a judge
Evidence
American Idol (season 12): Ryan Seacrest returned to host.
Iteration 3
Analysis
Onika Tanya Maraj is a judge on a television show. The name of the television show is not provided in the context. However, we know
that Ryan Seacrest returned to host American Idol (season 12) and Onika Tanya Maraj is one of the judges. Then we get the answer.
Answer
Ryan Seacrest.
FuRePA without PA
Iteration 0
Plan
the television show Onika Tanya Maraj is a judge on
Evidence Onika Tanya Maraj (born December 8, 1982), known professionally as Nicki Minaj, is a rapper, singer, songwriter and model.
Iteration 1
Analysis
The context mentions that Onika Tanya Maraj, also known as Nicki Minaj, is a judge on a television show. However, it does not provide
information about the host of the show. We need to find out who hosts the television show that Onika Tanya Maraj is a judge on.
Plan
the host of the television show that Onika Tanya Maraj is a judge on.
Evidence wrong
Rewrapped: The show is a loose spinoff of the show Ünwrapped¨, whose host Marc Summers acts as ¨Head Judge ¨for each episode,
alongside a representative of the company that produces the food of the day, and a third impartial judge involved in the food industry in
some way (chef, critic, blogger, etc.).
Iteration 2
......
Iteration 5
Answer
Judge Andrew Napolitano
Table 4: Case Study of Plan Assessor on diverse plans
14

--- PAGE 15 ---
Running Title for Header
Question: North American Aviation was the company that created what single-engined training aircraft?
Answer: North American Aviation T-6 Texan
FuRePA
Iteration 0
Plans
the single-engined training aircraft created by North American Aviation
North American Aviation to find information about the company
the single-engined training aircraft created by North American Aviation"
Evidence
North American Aviation (NAA) was a major American aerospace manufacturer, responsible for a number of historic aircraft, including
the T-6 Texan trainer, the P-51 Mustang fighter, the B-25 Mitchell bomber, the F-86 Sabre jet fighter, the X-15 rocket plane, and the
XB-70, as well as Apollo Command and Service Module, the second stage of the Saturn V rocket, the Space Shuttle orbiter and the B-1
Lancer.
Iteration 1
Analysis
According to the context, North American Aviation was responsible for creating several historic aircraft. We need to know which
single-engined training aircraft they created.
Plans
North American Aviation single-engined training aircraft
Evidence
The North American Aviation T-6 Texan is an American single-engined advanced trainer aircraft used to train pilots of the United States
Army Air Forces (USAAF), United States Navy, Royal Air Force, and other air forces of the British Commonwealth during World War II
and into the 1970s.
Iteration 2
Analysis
North American Aviation was the company that created the T-6 Texan single-engined training aircraft.
Answer
T-6 Texan
ChatCot
Plan
Information about North American Aviation
Evidence
The North American Aviation NA-16 was the first trainer aircraft built by North American Aviation, and was the beginning of a line of
closely related North American trainer aircraft that would eventually number more than 17,000 examples. Is this evidence useful?
Analysis
Yes, this evidence is useful. We can see that North American Aviation created a line of trainer aircraft, starting with the NA-16.
Plan
the single-engined training aircraft created by North American Aviation
Evidence
The North American Aviation NA-16 was the first trainer aircraft built by North American Aviation, and was the beginning of a line of
closely related North American trainer aircraft that would eventually number more than 17,000 examples. Is this evidence useful?
Analysis Wrong
Yes, this evidence is useful. We can see that the North American Aviation NA-16 was the first trainer aircraft built by North American
Aviation.
Answer
We get the answer. The answer is the North American Aviation NA-16.
Table 5: Case Study of FuRePA and ChatCot
15

--- PAGE 16 ---
Running Title for Header
Question: Where did Charles Stewart, 3Rd Duke Of Richmond’s father graduate from?
Answer: University of Paris
FuRePA
Iteration 0
Plans
the educational institution where Charles Stewart, 3rd Duke of Richmond’s father graduated from.
the father of Charles Stewart, 3rd Duke of Richmond’s alma mater or graduation institution
the name of Charles Stewart, 3rd Duke of Richmond’s father
the educational institution from which Charles Stewart, 3rd Duke of Richmond’s father graduated
Evidence
Charles Stewart, 3rd Duke of Richmond: The titles Duke of Richmond, Duke of Lennox and Earl of March, were resurrected for Charles
II’s illegitimate son by Louise de Kérouaille, Charles Lennox, 1st Duke of Richmond and Lennox, in 1675.
Iteration 1
Analysis
Charles Stewart, 3rd Duke of Richmond’s father is not mentioned in the given context. We need information about Charles Stewart, 3rd
Duke of Richmond’s father to find out where he graduated from.
Plans
Educational background of Charles Stewart, 3rd Duke of Richmond’s father.
the educational background or university of Charles Stewart’s father
Charles Lennox, 1st Duke of Richmond and Lennox’s education
the educational background or where the father of Charles Stewart, 3rd Duke of Richmond graduated from.
Where did Charles Lennox, 1st Duke of Richmond and Lennox’s father graduate from?
Evidence
Charles Stewart, 3rd Duke of Richmond, 6th Duke of Lennox KG (7 March 1639December 1672) was the only son of George Stewart,
9th Seigneur d’Aubigny and Katherine Howard, daughter of Theophilus Howard, 2nd Earl of Suffolk.
Iteration 2
Analysis
According to the context, Charles Stewart, 3rd Duke of Richmond’s father graduated from a certain place. However, the context does not
provide information about the place. We need more specific information to answer the question.
Plans
Where did George Stewart, 9th Seigneur d’Aubigny graduate from?
George Stewart’s graduation information
George Stewart’s alma mater or where George Stewart graduated from.
the graduation details of George Stewart, 9th Seigneur d’Aubigny.
Where did Charles Stewart, 3rd Duke of Richmond’s father graduate from?
Evidence
Charles Stewart, 3rd Duke of Richmond: On 10 December 1645 he was created Baron Stuart of Newbury, Berkshire, and Earl of Lichfield,
titles conferred on him ¨to perpetuate the titles which were intended to have been conferred on his uncle ¨Lord Bernard Stewart, youngest
son of the Duke of Lennox, who had been killed in the Battle of Rowton Heath in the English Civil War in September of that year.
Iteration 3
Analysis
The context mentions that Charles Stewart, 3rd Duke of Richmond’s father was George Stewart, 9th Seigneur d’Aubigny. We need to
know where George Stewart, 9th Seigneur d’Aubigny graduated from.
Plans
Where did Charles Stewart, 3rd Duke of Richmond’s father graduate from?
Where did George Stewart, 9th Seigneur d’Aubigny graduate from?
The educational background or alma mater of George Stewart, 9th Seigneur d’Aubigny.
Evidence
George Stewart, 9th Seigneur d’Aubigny: By 1633, he was a student at the Collège de Navarre, part of the University of Paris, and he did
homage to Louis XIII of France for the lordship of Aubigny on 5 August 1636, shortly after his eighteenth birthday.
Iteration 4
Analysis
Charles Stewart, 3rd Duke of Richmond’s father is George Stewart, 9th Seigneur d’Aubigny. The information provided states that George
Stewart was a student at the Collège de Navarre, part of the University of Paris. So, George Stewart graduated from the Collège de
Navarre, which is a part of the University of Paris.
Answer
George Stewart, 9th Seigneur d’Aubigny graduated from the Collège de Navarre, University of Paris.
Table 6: Case Study of FuRePA recovering from error
16
