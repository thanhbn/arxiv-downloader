# 2210.15133.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2210.15133.pdf
# Kích thước file: 290046 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Mô hình Ngôn ngữ Tiền huấn luyện với Che mặt Định hướng Truy xuất
cho Truy xuất Đoạn văn Dày đặc
Dingkun Long, Yanzhao Zhang, Guangwei Xu, Pengjun Xie
Alibaba Group
dingkun.ldk,zhangyanzhao.zyz@alibaba-inc.com
kunka.xgw,pengjun.xpj@alibaba-inc.com
Tóm tắt
Mô hình ngôn ngữ tiền huấn luyện (PTM) đã được
chứng minh tạo ra các biểu diễn văn bản mạnh mẽ
cho tác vụ truy xuất đoạn văn dày đặc. Mô hình hóa
Ngôn ngữ Che mặt (MLM) là một tiểu tác vụ chính
của quá trình tiền huấn luyện. Tuy nhiên, chúng tôi
phát hiện rằng chiến lược che mặt ngẫu nhiên thông
thường có xu hướng lựa chọn một số lượng lớn các
token có tác động hạn chế đến tác vụ truy xuất đoạn
văn (ví dụ: từ dừng và dấu câu). Bằng cách nhận ra
rằng trọng số quan trọng của từ có thể cung cấp thông
tin có giá trị cho truy xuất đoạn văn, chúng tôi đề xuất
chiến lược che mặt định hướng truy xuất thay thế
(được gọi là ROM) trong đó các token quan trọng hơn
sẽ có xác suất cao hơn bị che mặt, để nắm bắt thông
tin đơn giản nhưng thiết yếu này nhằm hỗ trợ quá
trình tiền huấn luyện mô hình ngôn ngữ. Đáng chú ý,
phương pháp che mặt token mới được đề xuất sẽ
không thay đổi kiến trúc và mục tiêu học tập của PTM
gốc. Các thí nghiệm của chúng tôi xác minh rằng ROM
được đề xuất cho phép thông tin quan trọng của từ
giúp tiền huấn luyện mô hình ngôn ngữ từ đó đạt
được hiệu suất tốt hơn trên nhiều điểm chuẩn truy
xuất đoạn văn.

1 Giới thiệu
Truy xuất đoạn văn dày đặc đã thu hút nhiều sự chú ý
gần đây do lợi ích của nó đối với một loạt các ứng
dụng hạ nguồn, chẳng hạn như trả lời câu hỏi miền
mở (Karpukhin et al., 2020; Qu et al., 2021; Zhu et al.,
2021), hệ thống hội thoại (Yu et al., 2021) và tìm kiếm
web (Lin et al., 2021; Fan et al., 2021; Long et al.,
2022). Để cân bằng hiệu quả và hiệu suất, các phương
pháp truy xuất đoạn văn dày đặc hiện tại thường tận
dụng kiến trúc bộ mã hóa kép. Cụ thể, truy vấn và
đoạn văn được mã hóa thành các biểu diễn vector liên
tục bởi các mô hình ngôn ngữ (LMs) tương ứng, sau
đó, một hàm điểm số được áp dụng để ước tính độ
tương tự ngữ nghĩa giữa cặp truy vấn-đoạn văn.

Dựa trên kiến trúc bộ mã hóa kép, nhiều phương
pháp tối ưu hóa khác nhau đã được đề xuất gần đây, bao gồm khai thác các ví dụ âm khó trong huấn luyện (Xiong et al., 2021), các PTM được tối ưu hóa được thiết kế đặc biệt cho truy xuất dày đặc (Gao và Callan, 2021, 2022; Ma et al., 2022) và các phương pháp biểu diễn văn bản thay thế hoặc chiến lược tinh chỉnh (Karpukhin et al., 2020; Zhang et al., 2022a, 2021). Trong bài báo này, chúng tôi tập trung vào nghiên cứu phần mô hình ngôn ngữ tiền huấn luyện. Chúng tôi quan sát thấy rằng mục tiêu tiền huấn luyện MLM che mặt token ngẫu nhiên được áp dụng rộng rãi là không tối ưu cho tác vụ truy xuất đoạn văn dày đặc. Tham khảo các nghiên cứu trước đây, việc giới thiệu trọng số của mỗi từ (hoặc token) để hỗ trợ ước tính mức độ liên quan truy vấn-đoạn văn có hiệu quả trong cả giai đoạn truy xuất và xếp hạng đoạn văn (Dai và Callan, 2020; Ma et al., 2021; Wu et al., 2022). Tuy nhiên, chiến lược che mặt ngẫu nhiên không phân biệt tầm quan trọng của các token. Hơn nữa, chúng tôi phát hiện rằng khoảng 40% các token bị che mặt được tạo ra bởi phương pháp che mặt ngẫu nhiên 15% là từ dừng hoặc dấu câu¹. Tuy nhiên, tác động của các token này đối với truy xuất đoạn văn là cực kỳ hạn chế (Fawcett et al., 2020). Do đó, chúng tôi suy luận rằng các LM được tiền huấn luyện với mục tiêu MLM che mặt token ngẫu nhiên là không tối ưu cho truy xuất đoạn văn dày đặc do thiếu sót trong việc phân biệt tầm quan trọng của token.

Để giải quyết hạn chế trên, chúng tôi đề xuất chiến lược che mặt định hướng truy xuất (ROM) thay thế nhằm che mặt các token cần thiết cho truy xuất đoạn văn. Cụ thể, trong quá trình tiền huấn luyện của LM, xác suất mỗi token bị che mặt không phải là ngẫu nhiên, mà được chồng chất bởi trọng số quan trọng của token tương ứng. Ở đây, trọng số quan trọng được biểu diễn như một số thực giữa 0 và 1. Bằng cách này, chúng ta có thể cải thiện đáng kể xác suất các token có trọng số cao hơn bị che mặt. Do đó, mô hình ngôn ngữ tiền huấn luyện sẽ chú ý nhiều hơn đến các từ có trọng số cao hơn từ đó làm cho nó phù hợp hơn cho các ứng dụng truy xuất đoạn văn dày đặc hạ nguồn.

¹Chúng tôi đã sử dụng danh sách từ dừng của nltk và gensim.

--- TRANG 2 ---
Để xác minh tính hiệu quả và độ bền vững của phương pháp che mặt định hướng truy xuất được đề xuất, chúng tôi tiến hành thí nghiệm trên hai điểm chuẩn truy xuất đoạn văn thường được sử dụng: các bộ dữ liệu xếp hạng đoạn văn MS MARCO và Neural Questions (NQ). Kết quả thí nghiệm thực nghiệm chứng minh rằng phương pháp của chúng tôi có thể cải thiện đáng kể hiệu suất truy xuất đoạn văn.

2 Nghiên cứu Liên quan
Các phương pháp truy xuất đoạn văn dày đặc hiện tại thường áp dụng kiến trúc bộ mã hóa kép. Trong DPR (Karpukhin et al., 2020), họ đầu tiên trình bày rằng hiệu suất truy xuất đoạn văn của khung bộ mã hóa kép dày đặc có thể vượt trội đáng kể so với phương pháp dựa trên khớp từ truyền thống như BM25. Dựa trên khung bộ mã hóa kép, các nghiên cứu khám phá nhiều chiến lược khác nhau để tăng cường các mô hình truy xuất dày đặc, bao gồm khai thác các mẫu âm khó trong giai đoạn tinh chỉnh (Xiong et al., 2021; Zhan et al., 2021), chưng cất kiến thức từ mô hình bộ mã hóa chéo mạnh mẽ hơn (Ren et al., 2021; Zhang et al., 2021; Lu et al., 2022), tăng cường dữ liệu (Qu et al., 2021) và các PTM được thiết kế riêng (Chang et al., 2020; Gao và Callan, 2021, 2022; Ma et al., 2022; Liu và Shao, 2022; Wu et al., 2022).

Đối với việc tiền huấn luyện mô hình ngôn ngữ, nghiên cứu trước đây tập trung vào thiết kế các mục tiêu tiền huấn luyện bổ sung được thiết kế riêng cho truy xuất đoạn văn dày đặc (Lee et al., 2019; Chang et al., 2020) hoặc điều chỉnh kiến trúc bộ mã hóa Transformer (Gao và Callan, 2021, 2022) để có được các mô hình ngôn ngữ thực tế hơn. Trong bài báo này, chúng tôi tìm cách thực hiện các biến đổi đơn giản của mục tiêu học tập MLM gốc để cải thiện hiệu suất mô hình, từ đó giảm độ phức tạp của quá trình tiền huấn luyện.

3 Phương pháp
Trong phần này, chúng tôi mô tả phương pháp tiền huấn luyện được đề xuất cho tác vụ truy xuất đoạn văn dày đặc. Chúng tôi trước tiên đưa ra tổng quan ngắn gọn về mô hình tiền huấn luyện BERT thông thường với loss MLM. Sau đó chúng tôi sẽ giới thiệu cách mở rộng nó thành mô hình của chúng tôi với tiền huấn luyện che mặt định hướng truy xuất.

3.1 Mô hình Tiền huấn luyện BERT
Tiền huấn luyện MLM Nhiều mô hình ngôn ngữ bộ mã hóa Transformer phổ biến (ví dụ: BERT, RoBERTa) áp dụng mục tiêu MLM trong giai đoạn tiền huấn luyện. MLM che mặt một tập con các token đầu vào và yêu cầu mô hình dự đoán chúng. Cụ thể, loss MLM có thể được công thức hóa như sau:

Lmlm=X
i2maskedCrossEntropy( WhL
i;xi);

trong đó hL
i là biểu diễn cuối cùng của token bị che mặt xi và L là số lớp Transformer.

Che mặt Ngẫu nhiên Nói chung, việc lựa chọn các token bị che mặt là ngẫu nhiên, và tỷ lệ che mặt trong một câu được đặt ở 15%. Về mặt toán học, đối với mỗi token xi∈x, xác suất xi bị che mặt p(xi) được lấy mẫu từ phân phối đều giữa 0 và 1. Nếu giá trị của p(xi) nằm trong top 15% của toàn bộ chuỗi đầu vào, thì xi sẽ bị che mặt.

3.2 Nhược điểm của Che mặt Ngẫu nhiên
Vấn đề đáng kể của phương pháp che mặt ngẫu nhiên là nó không phân biệt trọng số quan trọng của mỗi token. Phân tích thống kê cho thấy 40% các token bị che mặt bởi chiến lược che mặt ngẫu nhiên là từ dừng hoặc dấu câu. Như đã thể hiện trong các nghiên cứu trước đây, việc phân biệt trọng số của các từ khác nhau cho truy xuất đoạn văn là có giá trị. Dù cho truy vấn hay đoạn văn, các từ có trọng số quan trọng cao hơn nên đóng góp nhiều hơn vào quá trình ước tính mức độ liên quan truy vấn-đoạn văn. Mặc dù bản thân mô hình ngôn ngữ tiền huấn luyện có khả năng nhận thức ngữ cảnh, chúng tôi vẫn hy vọng rằng mô hình ngôn ngữ có đặc tính mạnh mẽ hơn trong việc phân biệt tầm quan trọng của từ cho tác vụ truy xuất. Tuy nhiên, mô hình ngôn ngữ được huấn luyện bởi chiến lược che mặt ngẫu nhiên có khiếm khuyết.

3.3 Che mặt Định hướng Truy xuất
Như đã đề cập ở trên, tầm quan trọng của từ có tính hướng dẫn cho truy xuất đoạn văn. Ở đây, chúng tôi khám phá việc giới thiệu tầm quan trọng của từ vào huấn luyện MLM. Cụ thể hơn, chúng tôi kết hợp thông tin tầm quan trọng của từ vào che mặt token. Khác với chiến lược che mặt ngẫu nhiên, việc một token xi có bị che mặt hay không không chỉ được xác định bởi xác suất ngẫu nhiên pr(xi), mà còn được xác định bởi trọng số từ pw(xi) của nó. Ở đây, pw(xi) được chuẩn hóa giữa giá trị 0 và 1. Xác suất cuối cùng của token xi bị che mặt là pr(xi)+pw(xi).

Sau đó vấn đề bây giờ là tính toán trọng số từ của mỗi token. Các nghiên cứu trước đây đã đề xuất các phương pháp khác nhau để tính toán trọng số từ (Mallia et al., 2021; Ma et al., 2021), có thể được chia thành các danh mục không giám sát và có giám sát. Để duy trì

--- TRANG 3 ---
A stomach is the result of an allergic reaction 0.0 0.4 0.0 0.0 0.1 0.0 0.0 0.3 0.2
[mask] stomach is ... allergic reaction Transformer Encoder Transformer Encoder A
[CLS] ... ... Input sentence Term weight distribution
Random Masking A [mask] is ... allergic reaction Transformer Encoder Transformer Encoder [CLS] ... ... stomach
Retrieval Oriented Masking

Hình 1: Minh họa phương pháp che mặt định hướng truy xuất (ROM) của chúng tôi. "stomach" là token có trọng số từ cao nhất trong câu đầu vào, do đó có xác suất lớn hơn bị che mặt.

mô hình tiền huấn luyện không giám sát của LM, chúng tôi áp dụng phương pháp không giám sát được trình bày trong nghiên cứu BPROP (Ma et al., 2021).

BPROP đề xuất tính toán phân phối trọng số từ trong một câu dựa trên trọng số attention [CLS]-Token của BERT gốc bằng cách xem xét rằng token [CLS] là một tập hợp của toàn bộ biểu diễn chuỗi. Tuy nhiên, phân phối từ thu được từ attention gốc của BERT là một phân phối ngữ nghĩa, nhưng không phải là phân phối thông tin. BPROP tận dụng một phương pháp đối lập để tạo ra phân phối cuối cùng. Chính thức, Cho một câu đầu vào x = (x1; x2; ...; xn), cho ai biểu thị trọng số attention của xi∈x cho token [CLS] được tính như một trung bình của trọng số attention của mỗi head từ lớp cuối cùng của BERT. Phương pháp BPROP sẽ tạo ra một phân phối từ đối lập mới pw(x) một cách hoàn toàn không giám sát dựa trên (a1; a2; ...; an), trong đó ∑n
i pw(xi|BPROP) = 1. Ở đây, chúng tôi bỏ qua quá trình tính toán cụ thể và chi tiết hơn về BPROP có thể được tìm thấy trong bài báo gốc (Ma et al., 2021).

Một khi chúng ta tính toán phân phối trọng số từ của mỗi câu trong corpus trước, chúng ta có thể tiến hành tiền huấn luyện LM với mục tiêu học tập MLM bằng chiến lược ROM của chúng tôi. Cần lưu ý rằng trong phương pháp ROM, xác suất che mặt của mỗi token vẫn dựa vào xác suất ngẫu nhiên đều vì chúng tôi vẫn muốn giữ các tính chất cơ bản của LM tiền huấn luyện, thay vì để LM chỉ tập trung vào một số lượng nhỏ các token có trọng số cao hơn. Trong thực tế, tỷ lệ token che mặt cũng được đặt ở 15%, và phân tích thống kê cho thấy tỷ lệ từ dừng và token dấu câu bị che mặt trong phương pháp ROM đã giảm xuống 14%.

4 Thí nghiệm
4.1 Bộ dữ liệu
Chúng tôi đánh giá mô hình được đề xuất trên các bộ dữ liệu sau đây. MS MARCO Passage Ranking là một bộ dữ liệu điểm chuẩn được sử dụng rộng rãi cho tác vụ truy xuất đoạn văn, và được xây dựng từ nhật ký truy vấn tìm kiếm của Bing và các tài liệu web được truy xuất bởi Bing (Nguyen et al., 2016). Neural Question là một bộ dữ liệu truy xuất đoạn văn khác được phát sinh từ tìm kiếm Google (Kwiatkowski et al., 2019). Đối với mỗi bộ dữ liệu, chúng tôi tuân theo phân chia dữ liệu tiêu chuẩn trong công trình trước đây (Gao và Callan, 2022).

4.2 Phương pháp So sánh
Để xác minh tính hiệu quả của phương pháp được đề xuất, chúng tôi áp dụng các phương pháp sau đây tập trung vào tối ưu hóa PTM làm baseline chính của chúng tôi: Condenser (Gao và Callan, 2021) điều chỉnh kiến trúc Transformer trong tiền huấn luyện LM để tăng cường biểu diễn văn bản từ đó hỗ trợ truy xuất đoạn văn hạ nguồn; coCondenser (Gao và Callan, 2022) là phần mở rộng của Condenser, sử dụng loss đối lập cấp độ corpus không giám sát để khởi động không gian nhúng đoạn văn; COSTA (Ma et al., 2022) giới thiệu một tác vụ dự đoán span đối lập mới trong tiền huấn luyện LM nhằm xây dựng một bộ mã hóa văn bản phân biệt hơn.

Chúng tôi trực tiếp mượn một số baseline cạnh tranh khác từ bài báo coCondenser, bao gồm hệ thống từ vựng BM25, DeepCT (Dai và Callan, 2020), DocT5Query (Cheriton, 2019) và GAR (Mao et al., 2021); và hệ thống dày đặc DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2021), và ME-BERT (Luan et al., 2021).

--- TRANG 4 ---
Bảng 1: Kết quả Thí nghiệm cho Truy xuất Đoạn văn MS MARCO và Bộ dữ liệu Neural Question. T-test chứng minh các cải thiện của ROM và coROM so với baseline có ý nghĩa thống kê (p≤0.05).

| Phương pháp | MS MARCO Passage | Neural Question |
|-------------|------------------|-----------------|
|             | MRR@10 | R@1000 | R@5 | R@20 | R@100 |
| BM25 | 18.6 | 85.7 | - | 59.1 | 73.7 |
| DeepCT (Dai và Callan, 2019) | 24.3 | - | - | - | - |
| DocT5Query (Cheriton, 2019) | 27.7 | 94.7 | - | - | - |
| GAR (Mao et al., 2021) | - | - | 60.9 | 74.4 | 85.3 |
| DPR (Karpukhin et al., 2020) | - | - | - | 74.4 | 85.3 |
| BERT base | 33.4 | 95.5 | - | - | - |
| ANCE (Xiong et al., 2021) | 33.0 | 95.5 | - | 81.9 | 87.5 |
| ME-BERT (Luan et al., 2021) | 33.8 | - | - | - | - |
| RocketQA (Qu et al., 2021) | 37.0 | 97.9 | 74.0 | 82.7 | 88.5 |
| Condenser (Gao và Callan, 2021) | 36.6 | 97.4 | - | 83.2 | 88.4 |
| COSTA (Ma et al., 2022) | 36.6 | 97.1 | - | - | - |
| ROM | 37.3 | 98.1 | 73.9 | 83.4 | 88.5 |
| coCondenser (Gao và Callan, 2022) | 38.2 | 98.4 | 75.8 | 84.3 | 89.0 |
| coROM | 39.1 | 98.6 | 76.2 | 84.6 | 88.8 |

4.3 Thiết lập Thí nghiệm
Tiền huấn luyện mô hình ngôn ngữ ROM của chúng tôi bắt đầu với mô hình BERT-base 12 lớp gốc. Tuân theo công trình trước đây, chúng tôi sử dụng cùng dữ liệu như BERT trong tiền huấn luyện: Wikipedia tiếng Anh và BookCorpus. Ngoài ra, giống như mô hình coCondenser, chúng tôi cũng huấn luyện một mô hình ngôn ngữ thêm hàm loss học đối lập dựa trên mô hình ROM và corpus mục tiêu (Wikipedia hoặc bộ sưu tập web MS-MARCO). Ở đây, mô hình ROM được đồng huấn luyện được ký hiệu là coROM².

Trong quá trình tinh chỉnh, chúng tôi áp dụng bộ tối ưu AdamW với tốc độ học 5e-6 và batch size 64 trong 3 epochs cho bộ dữ liệu đoạn văn MS MARCO. Đối với bộ dữ liệu NQ, chúng tôi tuân theo thiết lập siêu tham số được trình bày trong bộ công cụ DPR (Karpukhin et al., 2020). Đối với bộ dữ liệu đoạn văn MS MARCO, tập kiểm tra không có sẵn nhãn, chúng tôi chỉ báo cáo kết quả trên tập phát triển. Chúng tôi tuân theo phương pháp đánh giá được sử dụng trong công trình trước đây (Gao và Callan, 2022). Tất cả thí nghiệm được tiến hành trên 8 NVIDIA Tesla 32G V100.

²Mô hình tinh chỉnh trên bộ dữ liệu xếp hạng đoạn văn MS MARCO có sẵn tại https://modelscope.cn/models/damo/nlp_corom_sentence-embedding_english-base/summary. Các mô hình ROM và coROM gốc sẽ được công khai trong tương lai.

4.4 Kết quả Đánh giá
Hiệu suất tổng thể của tất cả baseline và ROM được báo cáo trong Bảng 1. Kết quả cho thấy ROM đạt được hiệu suất tốt nhất. Thứ nhất, cải thiện của mô hình ROM là cực kỳ đáng kể so với mô hình BERT gốc. Ví dụ, metric MRR@10 trên bộ dữ liệu MS MARCO đã tăng từ 33.4 lên 37.3, điều này chứng minh thực nghiệm về lợi ích của LM được thiết kế riêng. So với các LM được thiết kế riêng cho truy xuất dày đặc khác, mô hình ROM đạt được cải thiện nhất quán trên cả hai bộ dữ liệu. Ngoài ra, tương tự như mô hình coCondenser, mô hình coROM sẽ cải thiện thêm hiệu suất truy xuất đoạn văn với sự trợ giúp của phương pháp đồng huấn luyện đối lập, cho thấy rằng biểu diễn văn bản chất lượng cao là nền tảng của truy xuất đoạn văn dày đặc.

Bảng 2: Metric MRR@10 trên bảng xếp hạng Xếp hạng Đoạn văn MS MARCO. Chúng tôi in đậm hiệu suất tốt nhất của cả tập dev và eval.

| Mô hình | dev | eval |
|---------|-----|------|
| Search LM (SLM) + HLATR | 46.3 | 45.0 |
| Listwise + Fusion reranker | 45.4 | 44.0 |
| Cot-MAE (Wu et al., 2022) | 45.6 | 43.8 |
| Lichee-xxlarge + deberta-v3-large | 45.2 | 43.6 |
| Adaptive Batch Scheduling (Choi et al., 2021) | 45.3 | 43.1 |
| coCondenser (Gao và Callan, 2022) | 44.3 | 42.8 |

--- TRANG 5 ---
Bảng 3: Kết quả ROM trên bộ dữ liệu đoạn văn MS MARCO với các phương pháp khác nhau để tạo trọng số từ.

| Trọng số Từ | MRR@10 | R@1000 |
|-------------|--------|--------|
| BPROP | 37.3 | 98.1 |
| DeepImpact | 37.6 | 98.2 |

4.5 Bảng xếp hạng Xếp hạng Đoạn văn MS MARCO
Để xác minh thêm tính hiệu quả của mô hình họ ROM, chúng tôi tiến hành thí nghiệm với đường ống truy xuất-sau đó-xếp hạng lại đầy đủ và gửi kết quả của chúng tôi lên Bảng xếp hạng MS MARCO. Bảng 2 trình bày các hệ thống hàng đầu trên bảng xếp hạng Xếp hạng Đoạn văn MS MARCO³. Đối với mô tả mô hình "Search LM (SLM) + HLATR", "Search LM(SLM)" là mô hình coROM và HLATR (Zhang et al., 2022b) là một module xếp hạng lại nhẹ kết hợp cả đặc tính truy xuất và xếp hạng lại từ đó cải thiện thêm hiệu suất xếp hạng cuối cùng. Gửi cuối cùng là một tập hợp của nhiều mô hình "xếp hạng lại + HLATR" được huấn luyện trên các mô hình ngôn ngữ tiền huấn luyện khác nhau (ví dụ: BERT, ERINE và RoBERTa).

4.6 Phân tích
Chất lượng Trọng số Từ Một cách trực quan, chất lượng của trọng số từ sẽ ảnh hưởng trực tiếp đến hiệu suất của mô hình ROM và phương pháp có giám sát có thể tạo ra kết quả trọng số từ chất lượng cao hơn. Do đó, ngoài phương pháp BPROP, chúng tôi cũng tiến hành tiền huấn luyện ROM bằng phân phối trọng số từ được tạo ra bởi phương pháp DeepImpact có giám sát (Mallia et al., 2021). Kết quả thí nghiệm của bộ dữ liệu đoạn văn MS MARCO được trình bày trong Bảng 3. Từ đó chúng ta có thể quan sát rằng: 1) Kết quả trọng số từ chất lượng cao thực sự dẫn đến hiệu suất truy xuất đoạn văn tốt hơn; 2) Cải thiện do DeepImpact mang lại nhỏ hơn nhiều so với mô hình ROM so với LM BERT gốc, điều này cho thấy phương pháp tính toán trọng số từ không giám sát là phù hợp khi xem xét rằng phương pháp có giám sát sẽ không tránh khỏi việc đưa thêm chi phí huấn luyện.

Phân tích Trọng số Attention Để xác minh rằng mô hình ROM được đề xuất phân biệt hơn đối với các token có trọng số khác nhau, chúng tôi so sánh phân phối [CLS]-token của các mô hình ROM và BERT gốc. Trong bảng 4, chúng tôi trình bày các token trọng số từ hàng đầu được tạo ra bởi hai mô hình khác nhau này. Chúng ta có thể quan sát thấy rằng hai tập hợp có các token chồng chéo. Tuy nhiên, các token trọng số attention cao hơn được tạo ra bởi mô hình ROM rõ ràng hợp lý hơn, và tập token của mô hình BERT thậm chí còn chứa các ký hiệu từ dừng.

Bảng 4: Các token trọng số attention hàng đầu được tạo ra bởi mô hình BERT và ROM.

Văn bản: Nhiệm vụ hàng ngày của một nhà phân tích kinh doanh có thể rất khác nhau, tùy thuộc vào bản chất của tổ chức và dự án hiện tại...
BERT: ., the, analyst, organization, duties
ROM: business, vary, duties, analyst, depending

5 Kết luận và Nghiên cứu Tương lai
Trong bài báo này, chúng tôi nghiên cứu rằng phương pháp tiền huấn luyện MLM che mặt token ngẫu nhiên hiện tại là không tối ưu cho tiền huấn luyện LM, vì quá trình này có xu hướng tập trung vào từ dừng và dấu câu. Chúng tôi đề xuất một chiến lược che mặt định hướng truy xuất mới kết hợp thông tin tầm quan trọng của từ. Chúng tôi đánh giá các LM ROM và coROM mở rộng của chúng tôi trên hai điểm chuẩn. Kết quả cho thấy phương pháp của chúng tôi rất hiệu quả, và mô hình cuối cùng của chúng tôi có thể đạt được cải thiện đáng kể so với các LM được thiết kế riêng trước đây cho truy xuất đoạn văn dày đặc.

Trong bài báo này, chúng tôi trực quan sử dụng phương pháp BPROP để tính toán trọng số từ, và chưa so sánh với các phương pháp trọng số từ không giám sát khác. Các nghiên cứu chi tiết hơn về phân phối trọng số từ dựa trên mô hình ROM có thể tạo ra hiểu biết sâu sắc. Hơn nữa, chúng tôi chỉ tiến hành thí nghiệm dựa trên mô hình BERT base, và xác nhận dựa trên các LM mở rộng được tiền huấn luyện với mục tiêu MLM (ví dụ: RoBERTa) có thể giúp chứng minh thêm tính tổng quát của phương pháp ROM được đề xuất.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ như trong bản gốc...]

--- TRANG 6 ---
[Tài liệu tham khảo tiếp tục...]

--- TRANG 7 ---
[Tài liệu tham khảo tiếp tục cho đến hết...]
