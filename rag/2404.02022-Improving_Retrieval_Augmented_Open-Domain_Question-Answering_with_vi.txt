# 2404.02022.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2404.02022.pdf
# Kích thước tệp: 3445556 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cải thiện Hệ thống Hỏi-Đáp Miền Mở với Tăng cường Truy xuất
thông qua Ngữ cảnh Vector hóa
Zhuo Chen1, Xinyu Wang2∗, Yong Jiang2∗, Pengjun Xie2, Fei Huang2, Kewei Tu1∗
1Trường Khoa học và Công nghệ Thông tin, Đại học ShanghaiTech
1Trung tâm Nghiên cứu Kỹ thuật Shanghai về Thị giác và Hình ảnh Thông minh
2Viện Điện toán Thông minh, Tập đoàn Alibaba
{chenzhuo,tukw}@shanghaitech.edu.cn
{tomas.wxy,yongjiang.jy,chengchen.xpj}@alibaba-inc.com
Tóm tắt
Trong thời đại của các mô hình ngôn ngữ lớn, việc áp dụng các kỹ thuật như Tạo sinh Tăng cường Truy xuất có thể giải quyết tốt hơn các vấn đề Hỏi-Đáp Miền Mở. Do các ràng buộc bao gồm kích thước mô hình và tài nguyên tính toán, độ dài ngữ cảnh thường bị giới hạn, và việc trao quyền cho mô hình để bao phủ các ngữ cảnh quá dài trong khi trả lời câu hỏi từ các miền mở trở nên thách thức. Bài báo này đề xuất một phương pháp tổng quát và tiện lợi để bao phủ ngữ cảnh dài hơn trong các nhiệm vụ Hỏi-Đáp Miền Mở. Nó tận dụng một bộ mã hóa nhỏ và cơ chế chú ý chéo và mã hóa hiệu quả các ngữ cảnh. Với phương pháp của chúng tôi, các mô hình ngôn ngữ gốc có thể bao phủ ngữ cảnh dài gấp nhiều lần trong khi giữ các yêu cầu tính toán gần với đường cơ sở. Các thí nghiệm của chúng tôi chứng minh rằng sau khi tinh chỉnh, có sự cải thiện hiệu suất trên hai tập dữ liệu held-in, bốn tập dữ liệu held-out, và cả trong hai cài đặt Học Ngữ cảnh. Mã nguồn của chúng tôi sẽ được phát hành tại https://github.com/Alibaba-NLP/Vec-RA-ODQA .

1 Giới thiệu
Các kiến trúc dựa trên Transformer (Vaswani et al., 2017) với việc tiền huấn luyện trên kho văn bản lớn đã trở nên phổ biến trong nghiên cứu Xử lý Ngôn ngữ Tự nhiên gần đây (Brown et al., 2020; Workshop et al., 2022; Chowdhery et al., 2023). Một số lượng ngày càng tăng các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên (NLP) cần xử lý ngữ cảnh dài như Hỏi-Đáp Miền Mở (ODQA) với Tạo sinh Tăng cường Truy xuất (RAG) (Lewis et al., 2020; Izacard and Grave, 2020; Gu et al., 2018). Tuy nhiên, các giai đoạn tinh chỉnh và suy luận trong các nhiệm vụ downstream vẫn bị ràng buộc bởi độ dài đầu vào, ví dụ, 2048 token cho Bloomz (Muennighoff et al., 2022) và Llama-1 (Touvron et al., 2023).

Với RAG, đầu vào có thể dễ dàng vượt quá độ dài tối đa mà mô hình có thể xử lý và việc

∗Tác giả liên hệ

Kiến thức Cũng có một tập của "The Alvin Show" từ những năm 1960 đã được phát hành. Alvin and the Chipmunks Alvin and the Chipmunks, ban đầu là David Seville and the Chipmunks hoặc đơn giản là the Chipmunks, là một nhóm nhạc hoạt hình Mỹ được tạo ra bởi Ross Bagdasarian Sr. cho một đĩa hát độc đáo vào năm 1958. Nhóm bao gồm ba con sóc chuột hoạt hình nhân cách hóa biết hát: Alvin, kẻ gây rối nghịch ngợm, người nhanh chóng trở thành ngôi sao của nhóm; Simon, người cao, đeo kính và trí thức; và ...

MôHìnhNhiệmVụ
Kiến thức: Cũng có một tập của "The Alvin Show" từ những năm 1960 đã được phát hành. Alvin and the Chipmunks Alvin and the Chipmunks, ban đầu là David Seville and the Chipmunks hoặc đơn giản là the Chipmunks, là một nhóm nhạc hoạt hình Mỹ được tạo ra bởi Ross Bagdasarian Sr. cho một đĩa hát độc đáo vào năm 1958. Nhóm bao gồm ba con sóc chuột hoạt hình nhân cách hóa biết hát: Alvin, kẻ nghịch ngợm...Kiến thức: ...Kiến thức ...
...
[⋮⋮⋮]
Dạng dày đặc
Đầu ra: David Seville 
Bộ mã hóa
Kiến thức: Alvin and the Chipmunks Alvin and the Chipmunks, ban đầu là David Seville and the ...H: Ai là người đứng sau The Chipmunks?Đ: Dạng văn bản(Độ dài Token tối đa)
Kiến thức: Alvin and the Ch...H: Ai là người đứng sau The Chipmunks?Đ: Dạng văn bản
MôHìnhNhiệmVụ(Độ dài Token tối đa)
Đầu ra: David Seville 

Hình 1: So sánh phương pháp của chúng tôi (phía dưới) và ODQA tăng cường truy xuất không có vector hóa (phía trên). Ở phần trên, các ngữ cảnh truy xuất giới hạn được xử lý bởi mô hình nhiệm vụ để hoàn thành nhiệm vụ. Phần dưới minh họa phương pháp của chúng tôi trong đó một bộ mã hóa được kết hợp để mã hóa các ngữ cảnh truy xuất quá dài.

thách thức đối với mô hình để thực hiện cả tinh chỉnh và suy luận trên các ngữ cảnh quá dài trở nên khó khăn. Hơn nữa, trong cài đặt học ngữ cảnh (ICL) (Dong et al., 2022; Kim et al., 2022), ngữ cảnh sẽ dài hơn nhiều cùng với các ngữ cảnh được truy xuất. Trong những trường hợp như vậy, nhu cầu cho mô hình xử lý văn bản đầu vào dài hơn tăng lên đáng kể.

Để cho phép mô hình bao phủ ngữ cảnh dài hơn trong cả giai đoạn tinh chỉnh và suy luận, bài báo này đề xuất một phương pháp tận dụng một mô hình mã hóa cấp 100 triệu trong các nhiệm vụ ODQA downstream với một mô hình ngôn ngữ cấp 1 tỷ như được minh họa trong phần dưới của Hình 1. Với phương pháp của chúng tôi, độ dài ngữ cảnh mà mô hình có thể bao phủ tăng từ 2k (ở dạng văn bản) lên tối đa 10k (ở dạng dày đặc, được nén bởi bộ mã hóa).

Các thí nghiệm được thiết kế dưới ba cài đặt để xác thực tính hiệu quả của phương pháp chúng tôi. Trong các thí nghiệm, trước tiên chúng tôi tinh chỉnh mô hình, tùy chọn bao gồm bộ mã hóa, trên hai tập dữ liệu ODQA phổ biến arXiv:2404.02022v3  [cs.CL]  23 Dec 2024

--- TRANG 2 ---
ĐườngCơSởPhươngPhápChúngTôi2k10k5k500HuấnLuyệnBậcHaiSuyLuậnThời gian chạy (ms)
Độ dài chuỗi (token)Hình 2: Minh họa tốc độ. Thời gian chạy được đo trên một GPU A100 duy nhất và kích thước batch được đặt là 1 cho tất cả các đường cong. "2k" trên trục hoành đại diện cho thời gian chạy của mô hình cơ sở để huấn luyện hoặc suy luận trên dữ liệu có độ dài 2k. "5k" và "10k" tương ứng với hai biến thể của phương pháp chúng tôi có thể bao phủ tối đa 5k và 10k token khi huấn luyện và suy luận. Thời gian huấn luyện đo trung bình trên năm bước huấn luyện liên tiếp. Thời gian suy luận đo trung bình trên năm bước tạo sinh liên tiếp. Cụ thể, chúng tôi đo thời gian thực thi của các hàm Trainer.training_step và model.generate dựa trên huggingface.

tập dữ liệu với các ngữ cảnh được truy xuất và đánh giá phương pháp của chúng tôi trong các cài đặt held-in, held-out, và ICL. Kết quả thí nghiệm cho thấy phương pháp của chúng tôi vượt trội so với đường cơ sở, được tinh chỉnh trên dữ liệu có độ dài 2k, trong cả ba cài đặt.

Về tốc độ của phương pháp chúng tôi, chúng tôi đo thời gian chạy của mỗi bước huấn luyện và suy luận. So với công trình nén các ngữ cảnh bằng mô hình nhiệm vụ gốc (Chevalier et al., 2023), yêu cầu các kỹ thuật để giảm đồ thị tính toán trong quá trình lan truyền ngược, chúng tôi sử dụng một mô hình nhỏ hơn 10 lần để thực hiện mã hóa các văn bản quá dài, do đó có thể giữ được quy trình gradient descent hoàn chỉnh. Tóm lại, các đóng góp của chúng tôi như sau:

1.Chúng tôi đề xuất một phương pháp kết hợp một mô hình mã hóa nhỏ cho việc mã hóa ngữ cảnh quá dài bằng cách áp dụng cơ chế chú ý chéo với mô hình nhiệm vụ gốc.

2.Chúng tôi đánh giá phương pháp của mình trong hai cài đặt held-in, bốn held-out, và hai ICL sau khi được tinh chỉnh trên hai tập dữ liệu ODQA và đạt được hiệu suất cải thiện.

3.Các yêu cầu tài nguyên tính toán của phương pháp chúng tôi nhất quán với những của đường cơ sở và thời gian chạy vẫn cạnh tranh.

Chú-ý-chéo
MLP
Tự-chú-ý
MLP×	𝑁
Bộmãhóakhóa  giátrị
Dạng văn bản 𝒙(𝒒,𝒌𝒎𝒂𝒙,𝑷)
Trình chiếu
Nhúng truy vấn⋯
NgữcảnhBổsung𝒌𝒂𝒅𝒅	

Hình 3: Minh họa phương pháp về kiến trúc mô hình (khối tím) và luồng dữ liệu (theo mũi tên đen/tím). Các mũi tên nét đứt tím có nghĩa là đầu ra của module MLP sẽ là "truy vấn" cho lớp tiếp theo của module Cross-attn. ×N có nghĩa là các module có nền chấm được lặp lại với nhiều lớp trong mô hình nhiệm vụ.

2 Phương pháp
2.1 Bối cảnh
Xem xét một ví dụ truy vấn q với câu trả lời vàng a và C mảnh thông tin ngữ cảnh tương ứng độc lập k={k1,k2, ...,kC}, với mỗi mảnh là một chuỗi token, trong đó k được truy xuất bởi một số retriever từ một corpus cho trước1

k=Retriever (q,corpus )

Lý tưởng nhất, C ngữ cảnh được truy xuất chứa kiến thức cần thiết để trả lời q một cách chính xác, nhưng cũng có thể có nhiễu. Cho một mô hình giải mã Dec được tham số hóa bởi θ, chuỗi đầu ra y thường được mô hình hóa bởi

Pθ(y|q,kmax,P) =Dec (y|q,kmax,P)

trong đó kmax ={k1,k2, ...,km} ∈k, m < C .
m đề cập đến số lượng ngữ cảnh đạt đến thông lượng của mô hình. P đại diện cho các prompt kết nối nội dung liên quan2. Cho mô hình,

1Tham khảo Mục 3.1 để biết định nghĩa chi tiết về corpus và retriever trong các thí nghiệm của chúng tôi.
2Các dạng của P khác nhau với các cài đặt khác nhau, và sẽ có các định nghĩa chi tiết trong Mục 3.1.

--- TRANG 3 ---
kmax thường là một tập con của k vì độ dài tối đa của ngữ cảnh thường bị ràng buộc bởi thông lượng của mô hình hoặc tài nguyên tính toán, và

Trong quá trình huấn luyện, chúng tôi nhằm tối đa hóa số hạng Pθ(a|q,kmax,P), và hình thức hóa bài toán ODQA như một nhiệm vụ mô hình hóa ngôn ngữ. Cụ thể, cho một truy vấn q, câu trả lời vàng a và ngữ cảnh kmax, chúng được kết nối ngôn ngữ học với các prompt thích hợp P, cùng nhau được ký hiệu là một chuỗi đầu vào x(q,a,kmax,P) ={x1, x2, ...}. Sau đó chúng tôi nhằm tối thiểu hóa loss mô hình hóa ngôn ngữ trên tập D của tất cả các ví dụ huấn luyện:

Lθ(D) =−X
x(q,a,kmax,P)∈DX
i
log(Pθ(xi|x<i))(1)

2.2 Mã hóa và Chú ý Chéo
Chúng tôi đề xuất một phương pháp có thể sử dụng các ngữ cảnh bổ sung kadd ={km+1,km+2, ...} dài gấp nhiều lần so với kmax. Đầu tiên, chúng tôi giới thiệu một bộ mã hóa được tham số hóa bởi ϕ. Sau đó chúng tôi áp dụng chú ý chéo với mô hình nhiệm vụ gốc và giới thiệu một trình chiếu, một module chú ý chéo và một Perceptron Đa lớp (MLP) trong mỗi lớp, cùng nhau ký hiệu các tham số là π. Ký hiệu ω={ϕ, π, θ} là tất cả các tham số trong mô hình của chúng tôi. Tổng thể, phương pháp của chúng tôi mô hình hóa đầu ra y bằng một mô hình mã hóa-giải mã Enc-Dec

Qω(y|q,kmax,P,kadd)
=Enc-Dec (y|q,kmax,P,kadd)

Trong quá trình huấn luyện, các đầu vào x(q,a,kmax,P) được nhúng bởi lớp nhúng của mô hình nhiệm vụ gốc Emb

hq=Emb (x(q,a,kmax,P))

và mỗi ngữ cảnh bổ sung ki trong kadd được mã hóa bởi bộ mã hóa Enc

h(i)
add=Enc (ki)

Lưu ý rằng độ dài mã hóa từ bộ mã hóa là linh hoạt trong thực tế và chúng tôi nén mỗi ki thành một vector. Theo đầu ra của bộ mã hóa, một trình chiếu Proj được sử dụng để căn chỉnh các không gian ẩn chiều cao giữa bộ mã hóa và mô hình nhiệm vụ trong mỗi lớp

hkv=Proj (hadd)trong đó hadd được nối từ tất cả h(i)
add được tính toán từ bước trước. Mỗi lớp của mô hình nhiệm vụ được gán cho một trình chiếu độc lập vì các lớp khác nhau có thể học các biểu diễn khác nhau.

Trong mỗi lớp, để kết hợp thông tin được lưu trữ trong kadd chúng tôi thêm một module chú ý chéo, trong đó các biểu diễn của ngữ cảnh bổ sung hkv phục vụ như "khóa" và "giá trị", theo sau là một MLP. Trong lớp đầu tiên, các nhúng của đầu vào gốc hq hoạt động như "truy vấn", và trong các lớp còn lại đầu ra h′
q từ lớp trước hoạt động như "truy vấn" ( h′
q sẽ được định nghĩa sau).

hc=Cross -attn (hq/h′
q,hkv)
hm=MLP (hc)

Cross -attn (hq,hkv) được tính toán như sau

Q=WQhq
K, V =WKhkv, WVhkv
o=softmax (QKT
√dk)V
hc=WOo

trong đó WQ, WK, WV, WO đề cập đến các ma trận trọng số và dk đề cập đến chiều của mỗi đầu chú ý. Sau đó đầu ra của chú ý chéo và MLP được xử lý bình thường bởi một tự chú ý và một module MLP khác. Đầu ra hoạt động như đầu vào "truy vấn" cho module chú ý chéo ở lớp tiếp theo.

h′
q=MLP (Self -attn (hm))

Cuối cùng, đầu ra của lớp cuối được mở rộng đến chiều kích thước từ vựng để dự đoán token tiếp theo (không hiển thị trong Hình 3 để đơn giản), và chúng tôi nhằm tối đa hóa xác suất

Qω(a|q,kmax,P,kadd)

Nhất quán với thiết lập đã đề cập trước đó, để tối đa hóa số hạng Qω(a|q,kmax,P,kadd), chúng tôi chuyển nó thành tối thiểu hóa loss mô hình hóa ngôn ngữ

Jω(D) =−X
x(q,a,kmax,P),kadd∈DX
i
log(Qω(xi|x<i,kadd))
(2)

--- TRANG 4 ---
2.3 Cài đặt ICL
Phương pháp của chúng tôi cũng có thể được áp dụng cho các cài đặt ICL. Dựa trên thiết lập đã đề cập ở trên, chúng tôi ký hiệu các mẫu ICL là lmax ={l1,l2, ...,lm}, với mỗi li bao gồm một cặp truy vấn và câu trả lời khác. Chúng tôi tối ưu hóa mục tiêu 3 dưới đây trên dữ liệu trong đó mỗi li(q′,a′) chỉ đề cập đến các mẫu ICL truy vấn-câu trả lời (không có ngữ cảnh) và q′a′ đề cập đến một cặp truy vấn-câu trả lời khác:

J′
ω(D) =−X
s(q,a,lmax,P),kadd∈DX
i
log(Q′
ω(si|s<i,kadd))
(3)

s={s1, s2, ...} đề cập đến các đầu vào được cấu thành từ (q,a,lmax,P) và Q′ chia sẻ định nghĩa tương tự với Q trong mục tiêu 2. Các ngữ cảnh bổ sung kadd được sử dụng theo cách tương tự như trong Mục 2.2 bằng cách thực hiện mã hóa, chú ý chéo, v.v.

2.4 Huấn luyện
Về mặt lý thuyết, các quá trình huấn luyện được nêu trong Mục 2.2 đều vẫn có thể vi phân và do đó tất cả các tham số có thể được tối ưu hóa thông qua gradient descent bình thường đối với mục tiêu 2. Lưu ý rằng các tham số ϕ của bộ mã hóa có thể được khởi tạo từ một mô hình được tiền huấn luyện tốt trên corpus quy mô lớn và các tham số được tiền huấn luyện có hiệu suất tốt trong nhiều nhiệm vụ downstream dựa trên mã hóa văn bản.

Tuy nhiên, các tham số trong module trình chiếu được khởi tạo ngẫu nhiên. Do đó ở đầu quá trình huấn luyện, theo quy tắc chuỗi, các gradient đến toàn bộ bộ mã hóa cũng sẽ ngẫu nhiên, điều này đặt ra rủi ro phá vỡ tiện ích mã hóa của bộ mã hóa. Trực giác này được chứng minh là đúng trong các thí nghiệm của chúng tôi.

Do đó, chúng tôi thiết kế hai chiến lược huấn luyện:

1.Trực tiếp đóng băng các tham số ϕ và làm cho các tham số (π, θ) có thể huấn luyện trong suốt quá trình huấn luyện.

2.Trong một vài bước huấn luyện đầu tiên (ví dụ, một epoch), ϕ được giữ đóng băng để ngăn các gradient ngẫu nhiên phá vỡ các tham số được tiền huấn luyện tốt của nó. Sau đó, ϕ được tối ưu hóa đối với mục tiêu 2 cùng với các module khác (π, θ).

3 Thí nghiệm
3.1 Cài đặt thí nghiệm
Cài đặt Định dạng dữ liệu
Held-in
Held-outTrả lời câu hỏi:
Kiến thức : {ngữ cảnh k1}
...{ngữ cảnh km}.
H: Ai đã nhận giải Nobel vật lý đầu tiên
Đ:

Định dạng ICL
với ngữ cảnhTrả lời các câu hỏi sau dựa trên Kiến thức:
Kiến thức : {ngữ cảnh k′
1}
H: Ai đã phát triển máy in đầu tiên vào những năm 1430s
Đ: Johannes Gutenberg
...(Kiến thức : ...H: ...Đ: ...)
Kiến thức : {ngữ cảnh k′′
1}
H: Ai đã nhận giải nobel vật lý đầu tiên
Đ:

Định dạng ICL
không có ngữ cảnh
(Mục 2.3)Trả lời các câu hỏi sau :
H: Ai đã phát triển máy in đầu tiên vào những năm 1430s
Đ: Johannes Gutenberg
(H: ...Đ: ...)...
H: Ai đã nhận giải nobel vật lý đầu tiên
Đ:

Ngữ cảnh
Bổ sung{ngữ cảnh km+1};
{ngữ cảnh km+2};
...

Bảng 1: Ví dụ về định dạng dữ liệu. Token màu xám đề cập đến prompt P được đề cập trong Mục 2 và ngữ cảnh được bỏ qua ở đây.

Dữ liệu Để đánh giá phương pháp của chúng tôi, trước tiên chúng tôi tinh chỉnh mô hình của mình trên hai tập dữ liệu ODQA riêng biệt, TriviaQA (Joshi et al., 2017) và Natural Questions (NQ) (Kwiatkowski et al., 2019). Bên cạnh việc đánh giá phương pháp của chúng tôi trên dữ liệu held-in, chúng tôi cũng đánh giá bốn dữ liệu held-out, cụ thể là CommonsenseQA (Talmor et al., 2019), SQuAD2.0 (Rajpurkar et al., 2016), Webquestions (Berant et al., 2013) và ComplexWebQuestions (Talmor and Berant, 2018). Cụ thể, các mẫu trong tập dữ liệu CommonsenseQA được xây dựng như các bài toán trắc nghiệm, và chúng tôi đánh giá hiệu suất ở cả định dạng trắc nghiệm và chuỗi-đến-chuỗi. Tham khảo Phụ lục A.1 để biết định dạng chi tiết.

Định dạng của đầu vào x trong Mục 2.2 được xây dựng như định dạng "Held-in Held-out" trong Bảng 1, và chúng tôi đánh giá hiệu suất của mô hình trên các mẫu định dạng ICL với ngữ cảnh. Định dạng của đầu vào s trong Mục 2.3 được xây dựng như "Định dạng ICL không có ngữ cảnh" trong Bảng 1.

Các ngữ cảnh bổ sung km+1,km+2 được mã hóa bởi bộ mã hóa riêng biệt và độc lập mà không có prompt. Các dạng của prompt P được định nghĩa

--- TRANG 5 ---
trước đó được hiển thị trong các token màu xám trong Bảng 1.

Retriever Đối với ngữ cảnh của các tập dữ liệu TriviaQA và NQ, chúng tôi sử dụng những ngữ cảnh được thu thập bởi Karpukhin et al. (2020), được thu thập với các kỹ thuật BM25 (Robertson et al., 2009) và Dense Passage Retrieval. Đối với ngữ cảnh của bốn tập dữ liệu held-out, chúng tôi làm theo Izacard et al. (2022) và Shi et al. (2023) và sử dụng Contriver (Izacard et al., 2021) làm retriever của chúng tôi. Các ngữ cảnh k được truy xuất từ Wikipedia dump ngày 20 tháng 12 năm 2018, phiên bản được phát hành bởi Karpukhin et al. (2020).

Đường cơ sở Các mô hình decoder-only gần đây như Bloomz (Muennighoff et al., 2022) và GPTs (Radford et al., 2019; Achiam et al., 2023) đã cho thấy hiệu suất tốt trong các nhiệm vụ giống như tạo sinh, và chúng tôi sử dụng Bloomz-1b73 cho mô hình nhiệm vụ θ. Khi tinh chỉnh mô hình cơ sở, các đầu vào được xây dựng theo cài đặt "Held-in Held-out" như được nêu trong Bảng 1. Độ dài của đầu vào được mở rộng để sử dụng càng nhiều ngữ cảnh càng tốt, nhất quán với độ dài đầu vào tối đa (2k) của mô hình trong khi thực hiện tiền huấn luyện (Workshop et al., 2022).

Ngoài ra, lưu ý rằng thông tin ngữ cảnh kmax được cung cấp trong các đầu vào được xếp hạng từ tốt nhất đến tệ nhất dựa trên Dense Retrieval (Karpukhin et al., 2020), có nghĩa là đường cơ sở chúng tôi áp dụng mạnh hơn so với việc cung cấp ngẫu nhiên càng nhiều ngữ cảnh càng tốt mà không xem xét chất lượng. Đường cơ sở có thể được xem như một mô hình được tinh chỉnh trên các ngữ cảnh có liên quan nhất kết hợp các kỹ thuật xếp hạng lại (Karpukhin et al., 2020; Khalifa et al., 2023).

Cài đặt Khởi tạo và Huấn luyện Trọng số của các mô hình mã hóa được tiền huấn luyện phổ biến như BERT (Devlin et al., 2018) nên là khởi tạo tốt cho bộ mã hóa ϕ và do đó chúng tôi áp dụng BERT-base-uncased4 để khởi tạo ϕ. Các tham số của các module chú ý và MLP cũng được điều chỉnh từ Bloomz-1b7. Để giữ quá trình mã hóa hiệu quả, chúng tôi sử dụng một module Linear đơn giản làm trình chiếu được khởi tạo ngẫu nhiên và tinh chỉnh để căn chỉnh chiều ẩn 768 (BERT-base-uncased) với 2048 (Bloomz-1b7).

Trong thí nghiệm của chúng tôi, chúng tôi sử dụng BERT để mã hóa độc lập các ngữ cảnh bổ sung trên 10 hoặc 20 ngữ cảnh, có thể bao phủ khoảng 5k đến 10k token ngữ cảnh bổ sung. Sau đó các trạng thái ẩn của

3https://huggingface.co/bigscience/bloomz-1b7
4https://huggingface.co/bert-base-uncased

Tốc độ học 2e-5
Bộ tối ưu AdamW
Bộ lập lịch Lr cosine
Tỷ lệ khởi động 0.03
FP 16 True
FP 16 eval True
Kích thước batch toàn cục 8
Bước lưu 4000
Bước đánh giá 4000
Epoch tối đa 4
Tên GPU NVIDIA A100-SXM 80G

Bảng 3: Siêu tham số

token [CLS] được nối và đưa tiếp vào các module tiếp theo như được minh họa trong Hình 3. Đối với cả đường cơ sở và phương pháp của chúng tôi, chúng tôi đánh giá checkpoint mô hình với loss mô hình hóa ngôn ngữ thấp nhất trên tập phát triển và báo cáo metric Exact Match (EM).

Như đã thảo luận trong Mục 2.4, chủ yếu có hai lựa chọn chiến lược huấn luyện về những phần nào của mô hình đề xuất được tối ưu hóa. Chúng tôi thí nghiệm với cả hai chiến lược và báo cáo kết quả của cài đặt "bộ mã hóa đóng băng" trong Mục 3.2 và cài đặt "huấn luyện bộ mã hóa" trong Mục 4.1 tương ứng.

Siêu tham số Chúng tôi liệt kê các siêu tham số quan trọng trong các thí nghiệm của chúng tôi trong Bảng 3.

3.2 Kết quả chính
Chúng tôi trình bày kết quả chính của chiến lược huấn luyện đầu tiên được thảo luận trong Mục 2.4 trong Bảng 4. Sau khi tinh chỉnh trên hai tập dữ liệu và đánh giá trên ba cài đặt (held-in, held-out và ICL), phương pháp của chúng tôi đạt được hiệu suất vượt trội so với đường cơ sở trong năm trong số sáu cài đặt, ngoại trừ một cài đặt trên một tập dữ liệu.

Trong các cài đặt held-in (huấn luyện trên TriviaQA/NQ và đánh giá trên TriviaQA/NQ), mô hình của chúng tôi liên tục thể hiện hiệu suất vượt trội so với đường cơ sở. Hơn nữa, nó thể hiện hiệu suất cải thiện ổn định khi nhiều ngữ cảnh hơn được mã hóa bởi phương pháp của chúng tôi, cho thấy tiềm năng của mô hình chúng tôi để mã hóa các ngữ cảnh thậm chí còn dài hơn.

Trong các cài đặt held-out, phương pháp của chúng tôi vượt trội so với đường cơ sở trong tất cả các tập dữ liệu sau khi được tinh chỉnh trên TriviaQA và vượt trội so với ba trong số bốn tập dữ liệu sau khi được tinh chỉnh trên NQ, gợi ý tính ứng dụng tổng quát của phương pháp chúng tôi. Từ cài đặt "Com.QA

--- TRANG 6 ---
Huấn luyện \ Đánh giáTriviaQA NQ Com.QA test SQuAD Web.Q Comp.Q Triviaqa (ICL) NQ (ICL)
dev test dev test choice seq2seq test test test dev test dev test
TriviaQAcơ sở 45.740 46.203 14.868 16.288 17.199 2.785 10.191 9.524 4.490 31.764 31.857 8.999 9.058
+ 5k 47.686 47.742 17.506 19.307 19.328 3.194 12.684 10.053 5.513 32.341 32.034 10.677 11.136
+ 10k 47.901 48.245 18.465 19.529 17.363 2.539 12.667 11.111 6.024 34.027 34.235 11.671 11.801
NQcơ sở 42.809 43.976 37.159 37.978 19.410 4.095 21.199 14.815 13.498 35.521 35.967 19.242 21.136
+ 5k 43.669 44.657 37.01 38.698 19.656 4.095 22.724 15.344 13.214 35.883 35.985 19.265 21.413
+ 10k 44.189 45.107 37.581 39.114 21.294 4.423 22.918 15.873 13.413 36.381 36.569 19.447 21.662

Bảng 4: Kết quả chính của hiệu suất với bộ mã hóa đóng băng trên các cài đặt held-in, held-out và ICL. Chữ đậm đánh dấu kết quả tốt nhất trong mỗi cài đặt. Com.QA đề cập đến CommonsenseQA. Web.Q đề cập đến WebQuestions. Comp.Q đề cập ComplexWebQuestions. TriviaQA (ICL) và NQ (ICL) hiển thị kết quả được đánh giá trên cài đặt ICL trong đó dữ liệu được hình thành như minh họa trong Bảng 1 ICL.

choice" chúng ta có thể thấy rằng mặc dù mô hình của chúng tôi không được huấn luyện để trả lời các câu hỏi trắc nghiệm, nó hoạt động tốt hơn trong việc lựa chọn so với đường cơ sở.

Trong hai cột cuối TriviaQA (ICL) và NQ (ICL), chúng tôi đánh giá liệu mô hình được tối ưu hóa có thể khái quát hóa thành một cài đặt ICL tương tự hay không. Cụ thể, với tham số tối ưu hóa ω∗ sau khi tinh chỉnh mục tiêu 2, chúng tôi đánh giá mô hình Qω∗(a|q,lmax,P,kadd) hoạt động tốt như thế nào, trong đó mỗi li(q′,a′,k′) là một mẫu ICL được cấu thành từ một truy vấn, ngữ cảnh và câu trả lời khác. Đáng ngạc nhiên, chúng tôi đạt được hiệu suất cải thiện tương tự với cài đặt held-in. Hiệu suất cải thiện ổn định chỉ ra rằng phương pháp huấn luyện chúng tôi áp dụng là mạnh mẽ, duy trì cả hiệu quả của bộ mã hóa và bộ giải mã trong việc truy xuất thông tin hữu ích trong khi định dạng dữ liệu đánh giá khác với dữ liệu huấn luyện.

Tóm lại, từ kết quả được trình bày trong Bảng 4, có thể quan sát thấy rằng so với đường cơ sở, việc sử dụng phương pháp của chúng tôi để mã hóa một lượng lớn hơn thông tin truy xuất mang lại sự cải thiện tích cực chủ yếu cho hiệu suất của mô hình trên các cài đặt khác nhau, bao gồm held-in, held-out, và ICL.

4 Phân tích
Trong phần này, chúng tôi trình bày kết quả của ba thí nghiệm phân tích. Thí nghiệm đầu tiên cho thấy kết quả của chiến lược huấn luyện khác được thảo luận trong Mục 2.4. Thí nghiệm thứ hai cho thấy kết quả đánh giá của việc tối ưu hóa mục tiêu 3. Thí nghiệm thứ ba cho thấy tính hiệu quả của phương pháp chúng tôi trong một cài đặt thách thức hơn.

4.1 Huấn luyện Bộ mã hóa
Trong các thí nghiệm của chúng tôi, trước tiên chúng tôi thử tối ưu hóa bộ mã hóa ϕ với các tham số khác (π, θ) từ đầu quá trình huấn luyện. Kết quả chứng minh dự đoán của chúng tôi: các tham số ngẫu nhiên mới được giới thiệu (trình chiếu) dễ dàng gây rối với các tham số trong bộ mã hóa, do đó làm suy yếu khả năng mã hóa thông tin của nó và dẫn đến hiệu suất tệ hơn so với đường cơ sở.

Ở đây chúng tôi đánh giá chiến lược huấn luyện mà chúng tôi đề xuất trong Mục 2.4 nhằm khắc phục vấn đề này. Bộ mã hóa được tối ưu hóa sau một vài bước huấn luyện, và trong thí nghiệm của chúng tôi, chúng tôi đặt nó thành một epoch. Bên cạnh đó, các tham số trong module chú ý chéo được khởi tạo bởi những tham số trong module tự chú ý được tiền huấn luyện để giảm thiểu số lượng tham số được khởi tạo ngẫu nhiên.

Các đánh giá được thực hiện trong cùng các cài đặt như trong Bảng 4. Bằng cách áp dụng phương pháp huấn luyện hai bước này, chúng tôi thành công trong việc đạt được hiệu suất tốt hơn so với đường cơ sở trong hầu hết các cài đặt. Có thể suy ra rằng so với cài đặt bộ mã hóa đóng băng (tức là, ϕ không được tối ưu hóa), việc tiếp tục giới thiệu các tham số bộ mã hóa có thể huấn luyện không cải thiện hiệu suất của mô hình như mong đợi.

Mặc dù chúng tôi có thể đạt được kết quả tốt hơn trong hầu hết các cài đặt so với đường cơ sở, hiệu suất trong các cài đặt held-in và held-out có vẻ kém ổn định hơn so với cài đặt "bộ mã hóa đóng băng". Đặc biệt, chúng tôi thấy rằng việc tối ưu hóa bộ mã hóa dẫn đến hiệu suất giảm sút trong cài đặt ICL, đặc biệt là sau khi được tinh chỉnh trên các tập dữ liệu TriviaQA. Chúng tôi cho rằng điều này là do các mô hình tham số cấp triệu, sau khi tinh chỉnh trên dữ liệu nhất định, không thể đảm bảo khái quát hóa khả năng mã hóa sang một phạm vi rộng hơn các tình huống, ví dụ cài đặt ICL, như được định nghĩa trong Bảng 1. Chúng tôi trình bày kết quả của chiến lược huấn luyện thứ hai được thảo luận trong Mục 2.4 trong Bảng 5.

--- TRANG 7 ---
Huấn luyện \ Đánh giáTriviaQA NQ Com.QA test SQuAD Web.Q Comp.Q Triviaqa (ICL) NQ (ICL)
dev test dev test choice seq2seq test test test dev test dev test
TriviaQAcơ sở 45.740 46.203 14.868 16.288 17.199 2.785 10.191 9.524 4.490 31.764 31.857 8.999 9.058
+ 5k 48.082 47.803 15.896 16.898 14.333 3.112 10.755 11.640 4.945 16.646 16.645 6.178 6.676
+ 10k 47.980 47.750 16.079 17.008 15.807 2.867 10.823 11.111 5.058 16.103 16.300 6.383 7.008
NQcơ sở 42.809 43.976 37.159 37.978 19.410 4.095 21.199 14.815 13.498 35.521 35.967 19.242 21.136
+ 5k 43.397 44.524 37.387 39.03 15.889 4.095 22.092 14.286 12.958 33.993 34.341 17.951 19.640
+ 10k 43.284 44.047 37.205 39.28 16.790 3.931 22.143 16.402 12.788 33.122 33.404 18.933 20.914

Bảng 5: Phân tích về việc huấn luyện bộ mã hóa cùng với mô hình nhiệm vụ khi tinh chỉnh. Thí nghiệm được tiến hành dưới cùng cài đặt với Mục 3.2

Mẫu ICL
không có ngữ cảnhTriviaQA NQ
dev test dev test
cơ sở 20.052 20.083 19.242 19.529
+ 10 vec 19.939 20.233 19.539 19.668
+ 20 vec 20.358 20.578 19.333 19.501

Bảng 6: Kết quả tinh chỉnh trên dữ liệu với các mẫu ICL (không có thông tin ngữ cảnh) và đánh giá trên cài đặt held-in.

4.2 Cài đặt ICL không có Ngữ cảnh
Chúng tôi cũng thí nghiệm với việc tối ưu hóa mục tiêu 3 được định nghĩa trong Mục 2.3 trong đó chỉ các cặp truy vấn-câu trả lời được cung cấp trong đầu vào định dạng ICL. Định dạng dữ liệu chi tiết được hiển thị trong Bảng 1 "Định dạng ICL không có ngữ cảnh" và cặp truy vấn-câu trả lời được lấy mẫu càng nhiều càng tốt từ tập dữ liệu held-in. Tiện ích của bộ mã hóa vẫn giống như nó mã hóa 10 (+ 10 vec) hoặc 20 (+ 20 vec) mảnh ngữ cảnh và được giữ đóng băng trong quá trình huấn luyện.

Mô hình được tinh chỉnh trên TriviaQA và NQ và được đánh giá trong các cài đặt held-in. Chúng tôi báo cáo kết quả trong Bảng 6. Đầu tiên, chúng ta thấy rằng phương pháp của chúng tôi vẫn có thể tăng cường mô hình trong cài đặt này nhưng các cải thiện có vẻ không nhất quán hoặc nổi bật. Thứ hai, lưu ý rằng sự cải thiện trên mỗi tập dữ liệu không đáng kể như trong cài đặt ICL trong Bảng 4, trong đó mỗi mẫu ICL được cung cấp cùng với một mảnh ngữ cảnh.

Để tóm tắt các phát hiện ở đây, phương pháp của chúng tôi để mã hóa ngữ cảnh thể hiện sự cải thiện hiệu suất rõ rệt hơn trong các cài đặt ICL kết hợp thông tin ngữ cảnh. Chúng tôi cho rằng lý do cơ bản cho điều này là cơ chế chú ý chéo, tạo điều kiện trao đổi thông tin giữa các đầu vào (được nhúng bởi mô hình nhiệm vụ) và thông tin ngữ cảnh dày đặc (được mã hóa bởi bộ mã hóa), đặc biệt hiệu quả khi ngữ cảnh tương tác với ngữ cảnh, thay vì ngữ cảnh với các mẫu ICL chỉ có các cặp truy vấn-câu trả lời.

4.3 Cài đặt Thách thức hơn
Trong phương pháp của chúng tôi được trình bày trong Mục 2.2, chúng tôi áp dụng một module trình chiếu được áp dụng để căn chỉnh các không gian ẩn chiều cao và áp dụng cơ chế chú ý chéo để kết hợp thông tin ngữ cảnh dày đặc trong mỗi lớp. Trong phần này, chúng tôi đánh giá tính hiệu quả của phương pháp chúng tôi trong một cài đặt thách thức hơn.

Cụ thể, so với định dạng dữ liệu được nêu trong cài đặt "Held-in Held-out" trong Bảng 1, chúng tôi loại bỏ các ngữ cảnh trong đầu vào x và chỉ giữ lại câu hỏi và câu trả lời trong dữ liệu huấn luyện, tức là, x trong mục tiêu 2 trở thành (q,a,{},P). Chỉ một vài ngữ cảnh được cung cấp dưới dạng "Ngữ cảnh Bổ sung" được mã hóa bởi bộ mã hóa. Lưu ý rằng mặc dù việc cung cấp ngữ cảnh dạng văn bản có thể tăng cường đáng kể các mô hình trong các nhiệm vụ ODQA, ở đây chúng tôi loại bỏ chúng để kiểm tra tính hiệu quả của bộ mã hóa và cơ chế chú ý chéo trong một cài đặt thách thức hơn.

Kết quả được hiển thị trong Bảng 7. "+ 1/5/10 vec" có nghĩa là chúng tôi sử dụng 1/5/10 mảnh ngữ cảnh và mã hóa chúng thành 1/5/10 vector bằng cách lấy các trạng thái ẩn của token [CLS]

kmax={}TriviaQA NQ
dev test dev test
cơ sở 20.391 20.472 18.968 19.889
+ 1 vec 21.636 21.533 20.041 20.637
+ 5 vec 21.942 22.010 20.258 20.942
+ 10 vec 21.964 22.072 22.268 22.632

Bảng 7: Tính hiệu quả của phương pháp chúng tôi về mã hóa khi chúng tôi loại bỏ ảnh hưởng của thông tin ngữ cảnh dạng văn bản trong x.

--- TRANG 8 ---
token. Có thể suy ra rằng, đầu tiên, chỉ với một vector được mã hóa, phương pháp của chúng tôi có thể tăng cường mô hình. Thứ hai, chúng tôi quan sát sự cải thiện nhất quán trên hai tập dữ liệu và ba biến thể của phương pháp chúng tôi rằng việc kết hợp nhiều ngữ cảnh hơn dẫn đến hiệu suất tốt hơn.

5 Công trình liên quan
5.1 Tăng cường Truy xuất
Gần đây, tăng cường truy xuất đã được sử dụng để cải thiện một lượng lớn các nhiệm vụ Xử lý Ngôn ngữ Tự nhiên downstream như hỏi-đáp (Chen et al., 2017; Lewis et al., 2020; Kwiatkowski et al., 2019; Fan et al., 2019), đối thoại (Moghe et al., 2018), mô hình hóa ngôn ngữ (Khandelwal et al., 2020), NER (Wang et al., 2022, 2021) và dịch máy (Gu et al., 2018; Xu et al., 2022). Trong công trình đã đề cập, việc sử dụng thông tin truy xuất về cơ bản đã có khả năng tăng cường hiệu suất mô hình trên tất cả các chiều.

5.2 Kiến trúc Mô hình Liên quan
Đề cập đến mô hình cơ sở, đã có sự quan tâm ngày càng tăng trong việc sử dụng các mô hình kiến trúc mã hóa-giải mã hoặc chỉ giải mã trong việc giải quyết các nhiệm vụ downstream với tăng cường truy xuất gần đây.

Allaouzi et al. (2019) và Zhou et al. (2023) sử dụng các mô hình kiến trúc mã hóa-giải mã để giải quyết nhiệm vụ hỏi-đáp hình ảnh trong lĩnh vực y tế. Trong công trình của họ, mô hình mã hóa chịu trách nhiệm trích xuất các đặc trưng nổi bật từ hình ảnh y tế và phần giải mã tạo sinh câu trả lời. Li et al. (2023) sử dụng một mô hình mã hóa-giải mã với giải mã có ràng buộc để giải quyết nhiệm vụ hỏi-đáp trích xuất.

Các mô hình chỉ giải mã, ví dụ, ChatGPT và GPT-4 (Achiam et al., 2023), nổi tiếng hơn về hiệu suất tuyệt vời đáng ngạc nhiên của chúng trong các nhiệm vụ như hỏi-đáp (Ali et al., 2022) và có nhiều công trình cố gắng cải thiện hiệu suất dựa trên GPTs (Pereira et al., 2023). Kim and Min (2024) giới thiệu một mô hình chatbot sử dụng AI tạo sinh và phương pháp Tạo sinh Tăng cường Truy xuất để giải quyết vấn đề rằng việc đạt được tuân thủ quy định đòi hỏi điều hướng phức tạp các hướng dẫn cực kỳ phức tạp và đồ sộ trong ngành dược phẩm.

Trong công trình của chúng tôi, chúng tôi cũng kết hợp một bộ mã hóa để mã hóa ngữ cảnh. Tuy nhiên, so với các mô hình mã hóa-giải mã truyền thống, phần mã hóa trong phương pháp của chúng tôi nhỏ hơn nhiều lần so với phần giải mã. Mặc dù phương pháp của chúng tôi không thay đổi độ phức tạp bậc hai của cơ chế chú ý, thay vào đó nó xử lý các ngữ cảnh dài trong một chiều thấp hơn nhiều, do đó có thể tăng gấp năm lần khả năng bao phủ thông tin ngữ cảnh mà không cần sử dụng thêm tài nguyên tính toán.

5.3 Sử dụng Ngữ cảnh Dài
Để xử lý ngữ cảnh có độ dài quá mức, các kỹ thuật được đề xuất gần đây như nén ngữ cảnh đang được nghiên cứu ngày càng nhiều trong nghiên cứu NLP.

Chevalier et al. (2023) đề xuất "AutoCompressors" sử dụng OPT (Zhang et al., 2022) và Llama-2 (Touvron et al., 2023) để nén văn bản thành các vector tóm tắt và cho thấy rằng việc sử dụng ngữ cảnh dài có thể cải thiện perplexity. Trong phương pháp của họ, việc nén được thực hiện bởi mô hình ngôn ngữ cấp tỷ, và trong một trong các thí nghiệm của họ, họ huấn luyện trên chuỗi có 30720 token với 20 bước nén. Tuy nhiên, đồ thị tính toán hoàn chỉnh không thể được giữ đầy đủ trong những cài đặt như vậy, và quá trình tối ưu hóa phải dựa vào việc dừng gradient, điều này đặt ra rủi ro tiềm ẩn cho nguyên lý toán học đằng sau gradient descent. Tương tự trong công trình của Zhang et al. (2024), ngữ cảnh dài đầu tiên được phân chia thành nhiều khoảng, và sau đó một cửa sổ trượt được sử dụng để xử lý tuần tự từng khoảng một và các nhúng token nén được giữ lại cho việc dự đoán token tiếp theo. Nó được triển khai bằng cách giới thiệu các tham số có thể huấn luyện bổ sung cho mô hình ngôn ngữ gốc để hoàn thành nhiệm vụ "Activation Condensing", và các tham số gốc được đóng băng trong suốt quá trình huấn luyện.

6 Kết luận
Trong bài báo này, chúng tôi đề xuất một phương pháp kết hợp một mô hình mã hóa nhỏ cho việc mã hóa ngữ cảnh quá dài bằng cách áp dụng cơ chế chú ý chéo với mô hình nhiệm vụ gốc. Phương pháp này đơn giản và tổng quát cho các mô hình ngôn ngữ dựa trên transformer. Trong các thí nghiệm của chúng tôi, sau khi tinh chỉnh trên tập dữ liệu ODQA, chúng tôi thấy hiệu suất được cải thiện trên hai cài đặt held-in, bốn held-out và hai ICL, so với đường cơ sở kết hợp kỹ thuật xếp hạng lại trên dữ liệu huấn luyện, cho thấy tính hiệu quả của phương pháp chúng tôi trong việc sử dụng ngữ cảnh dài. Chúng tôi lưu ý rằng các giải thích trực quan cho sự cải thiện hiệu suất như sau: 1) mô hình mã hóa cung cấp khả năng mã hóa

--- TRANG 9 ---
ngữ cảnh dài hơn; 2) cơ chế chú ý chéo hữu ích trong việc chú ý có chọn lọc các phần đúng của đầu vào. Về hiệu quả, nhu cầu về số lượng GPU vẫn không thay đổi và thời gian chạy vẫn cạnh tranh với đường cơ sở.

7 Hạn chế
Đầu tiên, chúng tôi chỉ kiểm tra phương pháp của mình trong các mô hình 1B7 với một bộ mã hóa 110M, và chưa kiểm tra tính hiệu quả của phương pháp chúng tôi trên các mô hình ngôn ngữ lớn hơn, ví dụ, 7B và 70B, do tài nguyên tính toán hạn chế.

Thứ hai, chúng tôi quan sát thấy rằng phương pháp của chúng tôi thể hiện hiệu suất khiêm tốn tương đối dưới cài đặt 4.2, với chỉ một sự cải thiện nhỏ so với đường cơ sở. Chúng tôi cho rằng các lý do tiềm ẩn cho điều này là cơ chế chú ý chéo không phù hợp để mô hình hóa mối quan hệ giữa ngữ cảnh và các mẫu ICL (không có ngữ cảnh).

Lời cảm ơn
Công trình này được hỗ trợ bởi Tập đoàn Alibaba thông qua Chương trình Nghiên cứu Đổi mới Alibaba.

Tài liệu tham khảo
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Báo cáo kỹ thuật gpt-4. arXiv preprint arXiv:2303.08774 .

Rohaid Ali, Oliver Y Tang, Ian D Connolly, Jared S Fridley, John H Shin, Patricia L Zadnik Sullivan, Deus Cielo, Adetokunbo A Oyelese, Curtis E Doberstein, Albert E Telfeian, et al. 2022. Hiệu suất của chatgpt, gpt-4, và google bard trên ngân hàng câu hỏi chuẩn bị thi vấn đáp phẫu thuật thần kinh. Neurosurgery , trang 10–1227.

Imane Allaouzi, Mohamed Ben Ahmed, và Badr Benamrou. 2019. Một mô hình mã hóa-giải mã cho hỏi-đáp hình ảnh trong lĩnh vực y tế. Trong CLEF (working notes) .

Jonathan Berant, Andrew Chou, Roy Frostig, và Percy Liang. 2013. Phân tích cú pháp ngữ nghĩa trên Freebase từ các cặp câu hỏi-câu trả lời. Trong Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , trang 1533–1544, Seattle, Washington, USA. Association for Computational Linguistics.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Các mô hình ngôn ngữ là những người học ít mẫu. Advances in neural information processing systems , 33:1877–1901.

Danqi Chen, Adam Fisch, Jason Weston, và Antoine Bordes. 2017. Đọc Wikipedia để trả lời câu hỏi miền mở. Trong Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 1870–1879, Vancouver, Canada. Association for Computational Linguistics.

Alexis Chevalier, Alexander Wettig, Anirudh Ajith, và Danqi Chen. 2023. Điều chỉnh các mô hình ngôn ngữ để nén ngữ cảnh. arXiv preprint 2305.14788 .

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Mở rộng mô hình hóa ngôn ngữ với pathways. Journal of Machine Learning Research , 24(240):1–113.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Tiền huấn luyện các transformer hai chiều sâu cho hiểu ngôn ngữ. arXiv preprint arXiv:1810.04805 .

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, và Zhifang Sui. 2022. Một khảo sát về học ngữ cảnh. arXiv preprint arXiv:2301.00234 .

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, và Michael Auli. 2019. ELI5: Hỏi-đáp dạng dài. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , trang 3558–3567, Florence, Italy. Association for Computational Linguistics.

Jiatao Gu, Yong Wang, Kyunghyun Cho, và Victor OK Li. 2018. Dịch máy thần kinh được hướng dẫn bởi công cụ tìm kiếm. Trong Proceedings of the AAAI Conference on Artificial Intelligence , tập 32.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, và Edouard Grave. 2021. Truy xuất thông tin dày đặc không giám sát với học đối chiếu. arXiv preprint arXiv:2112.09118 .

Gautier Izacard và Edouard Grave. 2020. Tận dụng truy xuất đoạn văn với các mô hình tạo sinh cho hỏi-đáp miền mở. arXiv preprint arXiv:2007.01282 .

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, và Edouard Grave. 2022. Học ít mẫu với các mô hình ngôn ngữ tăng cường truy xuất. arXiv preprint arXiv:2208.03299 .

Mandar Joshi, Eunsol Choi, Daniel Weld, và Luke Zettlemoyer. 2017. TriviaQA: Một tập dữ liệu thách thức quy mô lớn được giám sát từ xa cho đọc hiểu. Trong Proceedings of the 55th Annual Meeting of

--- TRANG 10 ---
the Association for Computational Linguistics (Volume 1: Long Papers) , trang 1601–1611, Vancouver, Canada. Association for Computational Linguistics.

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và Wen-tau Yih. 2020. Truy xuất đoạn văn dày đặc cho hỏi-đáp miền mở. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics.

Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, và Lu Wang. 2023. Xếp hạng lại ít mẫu cho QA đa bước thông qua prompting mô hình ngôn ngữ. Trong Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 15882–15897, Toronto, Canada. Association for Computational Linguistics.

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, và Mike Lewis. 2020. Khái quát hóa thông qua ghi nhớ: Các mô hình ngôn ngữ láng giềng gần nhất.

Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, và Sang-goo Lee. 2022. Học ngữ cảnh tự tạo: Tận dụng các mô hình ngôn ngữ tự hồi quy như một bộ tạo minh họa. arXiv preprint arXiv:2206.08082 .

Jaewoong Kim và Moohong Min. 2024. Từ rag đến qa-rag: Tích hợp ai tạo sinh cho quy trình tuân thủ quy định dược phẩm. arXiv preprint arXiv:2402.01717 .

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, và Slav Petrov. 2019. Natural questions: Một điểm chuẩn cho nghiên cứu hỏi-đáp. Transactions of the Association for Computational Linguistics , 7:452–466.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Tạo sinh tăng cường truy xuất cho các nhiệm vụ nlp chuyên sâu kiến thức. Advances in Neural Information Processing Systems , 33:9459–9474.

Shaobo Li, Chengjie Sun, Bingquan Liu, Yuanchao Liu, và Zhenzhou Ji. 2023. Mô hình hóa hỏi-đáp trích xuất sử dụng các mô hình mã hóa-giải mã với giải mã có ràng buộc và học tăng cường dựa trên đánh giá. Mathematics , 11(7).

Nikita Moghe, Siddhartha Arora, Suman Banerjee, và Mitesh M. Khapra. 2018. Hướng tới khai thác kiến thức nền cho xây dựng hệ thống đối thoại. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , trang 2322–2332, Brussels, Belgium. Association for Computational Linguistics.

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Khái quát hóa đa ngôn ngữ thông qua tinh chỉnh đa nhiệm vụ. arXiv preprint arXiv:2211.01786 .

Jayr Pereira, Robson Fidalgo, Roberto Lotufo, và Rodrigo Nogueira. 2023. Visconde: Qa đa tài liệu với gpt-3 và xếp hạng lại thần kinh. Trong European Conference on Information Retrieval , trang 534–543. Springer.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Các mô hình ngôn ngữ là những người học đa nhiệm vụ không giám sát. OpenAI blog, 1(8):9.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. Squad: 100,000+ câu hỏi cho hiểu máy về văn bản. Trong Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , trang 2383–2392.

Stephen Robertson, Hugo Zaragoza, et al. 2009. Khung liên quan xác suất: bm25 và hơn thế. Foundations and Trends ®in Information Retrieval , 3(4):333–389.

Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, và Wen-tau Yih. 2023. Replug: Các mô hình ngôn ngữ hộp đen tăng cường truy xuất. arXiv preprint arXiv:2301.12652 .

Alon Talmor và Jonathan Berant. 2018. Web như một cơ sở kiến thức để trả lời các câu hỏi phức tạp.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, và Jonathan Berant. 2019. Commonsenseqa: Một thách thức hỏi-đáp nhắm mục tiêu kiến thức thường thức. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , trang 4149–4158.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. arXiv preprint arXiv:2302.13971 .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems , 30.

Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, và Kewei Tu. 2021. Cải thiện Nhận dạng Thực thể Có tên bằng Truy xuất Ngữ cảnh Bên ngoài và Học Hợp tác. Trong the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing ( ACL-IJCNLP 2021 ). Association for Computational Linguistics.

Xinyu Wang, Yongliang Shen, Jiong Cai, Tao Wang, Xiaobin Wang, Pengjun Xie, Fei Huang, Weiming Lu, Yueting Zhuang, Kewei Tu, Wei Lu, và Yong Jiang. 2022. DAMO-NLP tại SemEval-2022 task 11: Một hệ thống dựa trên kiến thức cho nhận dạng thực thể có tên đa ngôn ngữ. Trong Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022) , trang 1457–1468, Seattle, United States. Association for Computational Linguistics.

BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. Bloom: Một mô hình ngôn ngữ đa ngôn ngữ truy cập mở 176b-tham số. arXiv preprint arXiv:2211.05100 .

Jitao Xu, Josep Crego, và Jean Senellart. 2022. Tăng cường dịch máy thần kinh với các bản dịch tương tự. Trong Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track) , trang 282–292, Orlando, USA. Association for Machine Translation in the Americas.

Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, và Zhicheng Dou. 2024. Bay lên từ 4k đến 400k: Mở rộng ngữ cảnh của llm với activation beacon. arXiv preprint arXiv:2401.03462 .

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, và Luke Zettlemoyer. 2022. Opt: Các mô hình ngôn ngữ transformer được tiền huấn luyện mở.

Yuan Zhou, Jing Mei, Yiqin Yu, và Tanveer Syeda-Mahmood. 2023. Hỏi-đáp hình ảnh y tế sử dụng học tự giám sát kết hợp. arXiv preprint arXiv:2302.13069 .

A Phụ lục
A.1 Định dạng CommonsenseQA
Chúng tôi cho thấy cách chúng tôi định dạng lại dữ liệu từ CommonsenseQA trong Bảng 8. Định dạng lại lựa chọn A/B/C/D/E thành 1/2/3/4/5 để tránh gây nhầm lẫn với "A:" trong prompt P. Các lựa chọn được loại bỏ trong định dạng seq2seq và bài toán trở nên thách thức hơn.

--- TRANG 11 ---
Cài đặt Định dạng
Định dạng GốcMột cửa xoay thuận tiện cho việc di chuyển hai chiều, nhưng nó cũng phục vụ như một biện pháp bảo mật tại một cái gì?
A: ngân hàng
B: thư viện
C: cửa hàng bách hóa
D: trung tâm mua sắm
E: new york
Câu trả lời: A

Lựa chọn được định dạng lạiH: Một cửa xoay thuận tiện cho việc di chuyển hai chiều, nhưng nó cũng phục vụ như một biện pháp bảo mật tại một cái gì? Chọn từ 1-5 được đưa ra dưới đây.
1: ngân hàng
2: thư viện
3: cửa hàng bách hóa
4: trung tâm mua sắm
5: new york
Đ:
Câu trả lời: 1 hoặc ngân hàng

Seq2seq được định dạng lạiH: Một cửa xoay thuận tiện cho việc di chuyển hai chiều, nhưng nó cũng phục vụ như một biện pháp bảo mật tại một cái gì?
Đ:
Câu trả lời: ngân hàng

Bảng 8

--- TRANG 12 ---
