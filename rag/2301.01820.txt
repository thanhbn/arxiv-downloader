# 2301.01820.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2301.01820.pdf
# File size: 62634 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2301.01820v4  [cs.IR]  26 May 2023InPars-v2: Large Language Models as Efﬁcient
Dataset Generators for Information Retrieval
Vitor Jeronymo
NeuralMind, Brazil
FEEC-UNICAMP, BrazilLuiz Bonifacio
NeuralMind, Brazil
FEEC-UNICAMP, BrazilHugo Abonizio
NeuralMind, Brazil
FEEC-UNICAMP, Brazil
Marzieh Fadaee
Zeta Alpha, NetherlandsRoberto Lotufo
NeuralMind, Brazil
FEEC-UNICAMP, BrazilJakub Zavrel
Zeta Alpha, Netherlands
Rodrigo Nogueira
NeuralMind, Brazil
FEEC-UNICAMP, Brazil
Zeta Alpha, Netherlands
Abstract
Recently, InPars introduced a method to efﬁciently use larg e language models
(LLMs) in information retrieval tasks: via few-shot exampl es, an LLM is induced
to generate relevant queries for documents. These syntheti c query-document
pairs can then be used to train a retriever. However, InPars a nd, more recently,
Promptagator, rely on proprietary LLMs such as GPT-3 and FLA N to generate
such datasets. In this work we introduce InPars-v2, a datase t generator that
uses open-source LLMs and existing powerful rerankers to se lect synthetic
query-document pairs for training. A simple BM25 retrieval pipeline followed
by a monoT5 reranker ﬁnetuned on InPars-v2 data achieves new state-of-the-art
results on the BEIR benchmark. To allow researchers to furth er improve
our method, we open source the code, synthetic data, and ﬁnet uned models:
https://github.com/zetaalphavector/inPars/tree/mast er/legacy/inpars-v2
1 Introduction and Background
Data augmentation has been a reliable tool to improve the eff ectiveness of AI models in the face
of the scarcity of high-quality in-domain training data, wh ich is a common problem in practical
applications. Previous work by Bonifacio et al. [1] and Dai e t al. [2] successfully leveraged the
few-shot capabilities of LLMs to generate reliable synthet ic training data for information retrieval
models. These training data helped their models achieve sta te-of-the-art (SOTA) results on the BEIR
benchmark [6].
Bonifacio et al. [1] propose InPars where they generate quer ies from documents in the corpus using
LLMs. Similarly to Bonifacio et al. [1], the recently publis hed Promptagator [2] model also feeds
prompts to LLMs in order to generate alternative queries for a given document in an unsupervised
manner. It differs primarily from InPars in that it uses data set-speciﬁc prompts, a larger LLM to
generate queries, and a fully trainable retrieval pipeline with smaller models.
This work extends the method of Bonifacio et al. [1] by using a reranker as a ﬁltering mechanism
to select the best synthetically generated examples and fur ther improving retrieval effectiveness
Preprint.

--- PAGE 2 ---
on BEIR. We also use an open-source query generator as oppose d to the proprietary one used by
Bonifacio et al. and provide the source code and data to repro duce our results on TPUs. We refer to
Bonifacio et al. [1] model as Inpars-v1 and the model present ed in this paper as Inpars-v2.
2 Methodology
In this section, we explain the experiments we performed and how they differ from InPars-v1 [1].
To generate synthetic queries, we use the open-source GPT-J [8] with 6B parameters to replace
OpenAI’s curie model used in InPars-v1. For each dataset in t he BEIR benchmark, we sample 100k
documents from its corpus and generate one synthetic query p er document using GPT-J prompted
with 3 examples from MS MARCO. We use greedy decoding and the “ gbq” prompt template from
InPars-v1. Some corpora in BEIR such as ArguAna [7] have less than 100k documents. In these
cases, we generate as many synthetic queries as there are doc uments in the corpus. It takes on
average 30 hours on an A100 GPU to generate 100k queries.
Once the synthetic queries are generated, we apply a ﬁlterin g step to select query-document pairs
that are more likely to be relevant to each other. In InPars-v 1, this ﬁltering step consisted of selecting
the top 10k query-document pairs with the highest log probab ilities of generating a query given the
3-shot examples and the document as input. In InPars-v2, we u se monoT5-3B [4] already ﬁnetuned
on MS MARCO for one epoch1to estimate a relevancy score for each of the 100k query-docu ment
pairs. Then, we keep only the top 10k pairs with the highest sc ores as our positive query-document
pairs for training. It takes approximately 1.5 hours to scor e 100k query-document pairs on a TPU
v3-8. It should take twice as much on a A100.
To obtain negatives (i.e., non-relevant) query-document p airs, we randomly sample one document
from the top 1000 retrieved by BM25 when issued the synthetic query. Thus, our training set consists
of 10k positive query-document pairs and 10k negative query -document pairs.
The rerankers are ﬁnetuned in the same manner as in InPars-v1 : monoT5-3B is ﬁnetuned on MS
MARCO for one epoch and then further ﬁnetuned for one epoch on the synthetic data. We use the
Adafactor optimizer [5] with a constant learning rate of 1e- 3. Each batch has 64 positive and 64
negative query-document pairs randomly sampled from the tr aining dataset. We ﬁnetune one model
on each synthetic dataset from BEIR, that is, we end up with 18 different rerankers, one per dataset,
which are then evaluated on the corresponding test sets. Fin etuning on each synthetic dataset takes
less than 10 minutes on a TPU v3-8.
Evaluation is performed using the following pipeline: ﬁrst we use Pyserini’s [3] ﬂat indexes2to
retrieve a thousand documents for each query using BM25 with default parameters (k1=0.9, b=0.4),
for each dataset. Then we use the ﬁnetuned monoT5-3B models t o rerank these documents.
3 Results
Table 1 presents results for BM25 (2nd column), monoT5-3B ﬁn etuned on MS MARCO (3rd col-
umn), monoT5-3b ﬁnetuned on MS MARCO and further ﬁnetuned on InPars-v1 (4th column), and
monoT5-3B ﬁnetuned on MS MARCO and then ﬁnetuned on InPars-v 2 data (5th column). Com-
pared to InPars-v1, our approach is substantially better on TREC-News, Climate-FEVER, Robust
and Touche. Additionally, we compare our method with Prompt agator [2] and RankT5 [10]. Taking
into account the average of all BEIR datasets, these results represent a new state of the art on BEIR.
Promptagator and RankT5 strive on datasets that monoT5 and I nPars-v2 cannot even surpass BM25,
such as Touche and ArguAna. Note that these datasets focus on argument retrieval, which is slightly
different from other datasets in the BEIR benchmark. As a res ult, they beneﬁt from using cus-
tom prompts.3Promptagator does this without using supervised data from M S MARCO and using
smaller T5 models with 110M parameters for the retrieval and reranking steps.
1https://huggingface.co/castorini/monot5-3b-msmarco- 10k
2As opposed to the multiﬁeld index.
3In preliminary experiments, we also observed an improvemen t of more than 10 nDCG@10 points on
ArguAna by using a dataset-speciﬁc prompt to generate synth etic queries. More details and results on the full
BEIR benchmark will appear in an upcoming paper.
2

--- PAGE 3 ---
BM25monoT5-3BPrGator RankT5MARCO +InPars-v1 +InPars-v2
TREC-Covid 0.594 0.801 0.846 0.846 0.762 0.823
Robust 0.407 0.615 0.610 0.632 - -
FiQA 0.236 0.509 0.492 0.509 0.494 0.493
DBPedia 0.318 0.472 0.494 0.498 0.434 0.459
SciDocs 0.149 0.197 0.206 0.208 0.201 0.191
SciFact 0.678 0.774 0.774 0.774 0.731 0.760
NFCorpus 0.321 0.383 0.385 0.385 0.370 0.399
BioASQ 0.522 0.566 0.607 0.595 - 0.579
Natural Questions 0.305 0.625 0.625 0.638 - 0.647
HotpotQA 0.633 0.760 0.790 0.791 0.736 0.753
TREC-News 0.395 0.477 0.458 0.490 - -
Quora 0.788 0.835 0.874 0.845 - 0.819
FEVER 0.651 0.848 0.852 0.872 0.866 0.848
Climate-FEVER 0.165 0.288 0.287 0.323 0.241 0.275
Signal 0.328 0.302 0.319 0.308 - 0.319
ArguAna 0.397 0.379 0.371 0.369 0.630 0.406
Touche 0.442 0.309 0.260 0.291 0.381 0.486
CQADupstack 0.302 0.449 0.449 0.448 - -
Avg 0.424 0.533 0.539 0.545 - -
Avg PrGator 0.417 0.520 0.523 0.533 0.531 0.536
Table 1: nDCG@10 on BEIR. “Avg PrGator” is the average of data sets reported by Promptagator.
Promptagator uses a proprietary model, FLAN [9], to generat e synthetic queries. The RankT5 model
is a modiﬁed version of the monoT5 reranker, but its checkpoi nt and code are not published. In this
work, we make the code, models, and data open-source and publ icly available.
4 Conclusion
In this work, we presented InPars-v2, an improved version of InPars [1] that uses a publicly available
language model to generate queries and a better query-docum ent pair selection process. Our results
show that we achieve effectiveness on par with the state of th e art on BEIR. The synthetic data and
ﬁnetuned models were publicly released.
Acknowledgments
This research was partially supported by Fundação de Amparo à Pesquisa do Estado de São Paulo
(FAPESP) (project id 2022/01640-2). We also thank Centro Na cional de Processamento de Alto
Desempenho (CENAPAD-SP) and Google Cloud for computing cre dits.
References
[1] L. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira. In pars: Data augmentation for infor-
mation retrieval using large language models. arXiv preprint arXiv:2202.05144 , 2022.
[2] Z. Dai, V . Y . Zhao, J. Ma, Y . Luan, J. Ni, J. Lu, A. Bakalov, K . Guu, K. B. Hall, and
M.-W. Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint
arXiv:2209.11755 , 2022.
[3] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nog ueira. Pyserini: An easy-to-use
python toolkit to support replicable ir research with spars e and dense representations. arXiv
preprint arXiv:2102.10073 , 2021.
[4] R. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. Document ra nking with a pretrained sequence-
to-sequence model. In Proceedings of the 2020 Conference on Empirical Methods in N atural
Language Processing: Findings , pages 708–718, 2020.
3

--- PAGE 4 ---
[5] N. Shazeer and M. Stern. Adafactor: Adaptive learning ra tes with sublinear memory cost. In
International Conference on Machine Learning , pages 4596–4604. PMLR, 2018.
[6] N. Thakur, N. Reimers, A. Rücklé, A. Srivastava, and I. Gu revych. Beir: A heteroge-
neous benchmark for zero-shot evaluation of information re trieval models. arXiv preprint
arXiv:2104.08663 , 2021.
[7] H. Wachsmuth, S. Syed, and B. Stein. Retrieval of the best counterargument without prior
topic knowledge. In Proceedings of the 56th Annual Meeting of the Association fo r Computa-
tional Linguistics (Volume 1: Long Papers) , pages 241–251, Melbourne, Australia, July 2018.
Association for Computational Linguistics.
[8] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Paramet er Autoregressive Language
Model.https://github.com/kingoflolz/mesh-transformer-jax , May 2021.
[9] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. D u, A. M. Dai, and Q. V . Le.
Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.
[10] H. Zhuang, Z. Qin, R. Jagerman, K. Hui, J. Ma, J. Lu, J. Ni, X. Wang, and M. Bendersky.
Rankt5: Fine-tuning t5 for text ranking with ranking losses .arXiv preprint arXiv:2210.10634 ,
2022.
4
