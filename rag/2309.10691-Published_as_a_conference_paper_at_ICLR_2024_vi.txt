# 2309.10691.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2309.10691.pdf
# Kích thước tệp: 1638103 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024
MINT: ĐÁNH GIÁ LLM TRONG TƯƠNG TÁC ĐA LƯỢT VỚI
CÔNG CỤ VÀ PHẢN HỒI NGÔN NGỮ
Xingyao Wang1∗, Zihan Wang1,2∗†, Jiateng Liu1, Yangyi Chen1, Lifan Yuan1†, Hao Peng1,
Heng Ji1
1Đại học Illinois Urbana-Champaign,2Đại học Nhân dân Trung Quốc
1{xingyao6,zihanw,jiateng5,yangyic3,haopeng,hengji }@illinois.edu
TÓM TẮT
Để giải quyết các nhiệm vụ phức tạp, các mô hình ngôn ngữ lớn (LLM) thường yêu cầu nhiều
vòng tương tác với người dùng, đôi khi được hỗ trợ bởi các công cụ bên ngoài. Tuy nhiên,
các giao thức đánh giá hiện tại thường nhấn mạnh hiệu suất benchmark với trao đổi một lượt,
bỏ qua những tương tác tinh tế giữa người dùng, LLM và các công cụ bên ngoài, đồng thời
cũng đánh giá thấp tầm quan trọng của phản hồi ngôn ngữ tự nhiên từ người dùng. Những
thiếu sót này góp phần tạo ra sự khác biệt giữa đánh giá benchmark nghiên cứu và các trường
hợp sử dụng thực tế. Chúng tôi giới thiệu MINT, một benchmark đánh giá khả năng của LLM
trong việc giải quyết các nhiệm vụ thách thức với tương tác đa lượt bằng cách (1) sử dụng
công cụ và (2) tận dụng phản hồi ngôn ngữ tự nhiên. Để đảm bảo tính tái tạo, chúng tôi cung
cấp một khung đánh giá nơi LLM có thể truy cập công cụ bằng cách thực thi mã Python và
nhận phản hồi ngôn ngữ tự nhiên của người dùng được mô phỏng bởi GPT-4. Chúng tôi tái
sử dụng một tập hợp đa dạng các bộ dữ liệu đánh giá đã được thiết lập tập trung vào lý luận,
lập trình và ra quyết định và cẩn thận tuyển chọn chúng thành một tập con nhỏ gọn để đánh
giá hiệu quả. Phân tích của chúng tôi về 20 LLM mở và đóng mang lại những phát hiện thú vị.
(a) LLM nói chung được hưởng lợi từ công cụ và phản hồi ngôn ngữ, với mức tăng hiệu suất
(tuyệt đối, tương tự dưới đây) 1–8% cho mỗi lượt sử dụng công cụ và 2–17% với phản hồi
ngôn ngữ tự nhiên. (b) Hiệu suất một lượt tốt hơn không đảm bảo hiệu suất đa lượt tốt hơn.
(c) Đáng ngạc nhiên, trong số các LLM được đánh giá, tinh chỉnh hướng dẫn có giám sát
(SIFT) và học tăng cường từ phản hồi con người (RLHF) nói chung làm tổn hại khả năng đa
lượt. Chúng tôi hy vọng MINT có thể giúp đo lường tiến độ và khuyến khích nghiên cứu cải
thiện khả năng của LLM trong tương tác đa lượt, đặc biệt cho các cộng đồng mã nguồn mở
nơi đánh giá đa lượt của con người có thể ít được tiếp cận hơn so với LLM thương mại với
cơ sở người dùng lớn hơn.1

1 GIỚI THIỆU
Để giải quyết các nhiệm vụ phức tạp, các Mô hình Ngôn ngữ Lớn (LLM) thường cần nhiều vòng
tương tác với người dùng, đôi khi được hỗ trợ bởi các công cụ bên ngoài (Schick et al., 2023b; ChatGPT Plugins;
Mialon et al., 2023). Hiệu suất của LLM trong nhiều lượt trao đổi người dùng-LLM là rất quan trọng trong
các ứng dụng thực tế: khoảng 73% cuộc hội thoại Người-ChatGPT chứa hơn một lượt dựa trên 94k
mục dữ liệu ShareGPT (2023). Đồng thời, khả năng thích ứng với phản hồi ngôn ngữ tự nhiên do
người dùng cung cấp cũng rất quan trọng cho tính hữu dụng thực tế của chúng. Tuy nhiên, các đánh giá
LLM hiện tại chủ yếu tập trung vào đầu vào-đầu ra một lượt (Hendrycks et al., 2020; Chen et al., 2021) và
thường bỏ qua phản hồi ngôn ngữ tự nhiên do người dùng cung cấp (Liu et al., 2023d; Deng et al., 2023b; Yang
et al., 2023a; Shridhar et al., 2020), tạo ra sự khác biệt giữa các trường hợp sử dụng thực tế và đánh
giá. Việc đo lường mức độ LLM có thể được hưởng lợi từ cả công cụ và phản hồi ngôn ngữ tự nhiên
trong tương tác đa lượt là cần thiết để khuyến khích nghiên cứu tương lai cải thiện khả năng của LLM
trong phạm vi rộng hơn các tình huống thực tế.

Để thu hẹp những khoảng cách này, chúng tôi giới thiệu MINT. Đây là một benchmark cho LLM đo
lường hiệu suất của chúng trong tương tác đa lượt, tập trung vào hai khả năng cụ thể (§2.1): (1) giải quyết
nhiệm vụ được tăng cường bởi công cụ; (2) tận dụng phản hồi ngôn ngữ tự nhiên. MINT phản ánh
môi trường giải quyết vấn đề hợp tác người dùng-LLM-công cụ trong thế giới thực. Để giải quyết một
vấn đề, LLM có thể sử dụng các công cụ bên ngoài bằng cách tạo ra và thực thi các chương trình Python
(Wang et al., 2024) và/hoặc thu thập phản hồi ngôn ngữ tự nhiên để tinh chỉnh các giải pháp của mình;
phản hồi được cung cấp bởi GPT-4 (OpenAI, 2023), nhằm mô phỏng người dùng con người một cách
có thể tái tạo và có thể mở rộng.2 Để có một đánh giá toàn diện, chúng tôi bao gồm tám bộ dữ liệu đã
được thiết lập trải rộng lý luận, tạo mã và ra quyết định (§2.2). Để tạo điều kiện cho đánh giá đa lượt
có giá cả phải chăng, sau khi thu thập 29.307 trường hợp đa dạng từ các bộ dữ liệu hiện có (Bảng 1),
chúng tôi xây dựng một tập con gồm 586 trường hợp thách thức và đại diện yêu cầu tương tác đa lượt
để giải quyết3.

Chúng tôi đánh giá 4 LLM đóng và 16 LLM mã nguồn mở với MINT. Chúng tôi đo lường khả năng
giải quyết nhiệm vụ được tăng cường bởi công cụ của LLM bằng cách phân tích hiệu suất của chúng từ
việc sử dụng công cụ đa lượt (§3.2). Để đánh giá khả năng tận dụng phản hồi ngôn ngữ tự nhiên,
chúng tôi đo lường hiệu suất của chúng khi có phản hồi ngôn ngữ tự nhiên từ GPT-4 (§3.3). Kết quả
của chúng tôi cho thấy:

• Tất cả các mô hình đều được hưởng lợi từ tương tác công cụ và phản hồi ngôn ngữ tự nhiên, với
mức tăng hiệu suất tuyệt đối 1–8% cho mỗi lượt sử dụng công cụ bổ sung, và 2–17% với phản hồi
ngôn ngữ tự nhiên.

• Hiệu suất một lượt tốt hơn không nhất thiết dẫn đến hiệu suất đa lượt tốt hơn. Ví dụ, trong khi
Claude-2 vượt trội so với người tiền nhiệm Claude-1 trong đánh giá một lượt, Claude-1 lại được
hưởng lợi nhiều hơn từ tương tác và hoạt động tốt hơn với >2 lượt.

• Có một khoảng cách đáng chú ý giữa LLM mã nguồn mở và đóng trong hiệu suất tương tác đa lượt.
Ví dụ, với sự giúp đỡ của phản hồi ngôn ngữ, ngay cả mô hình mã nguồn mở tốt nhất, Lemur-70b-chat-v1,
cũng tụt hậu so với mô hình đóng tốt nhất 8,7% về tỷ lệ thành công tuyệt đối.

• Trên hầu hết các LLM chúng tôi đánh giá, các mô hình được huấn luyện với tinh chỉnh hướng dẫn
có giám sát (SIFT, Wei et al., 2022) và học tăng cường từ phản hồi con người (RLHF, Ouyang et al., 2022a)
hoạt động kém hơn trong môi trường đa lượt bất kể có sự hiện diện của phản hồi ngôn ngữ. Ví dụ,
SIFT làm tổn hại hiệu suất đa lượt của Codellama-34B 11,1% và 15,4% (w/ phản hồi), và RLHF ảnh
hưởng tiêu cực đến LLaMA-2-70B lần lượt 8,5% và 8,7%. Các ngoại lệ đáng chú ý là Vicuna-7B và
Lemur-70b-chat-v1, nơi SIFT cải thiện tương tác đa lượt.

Bằng cách cố định LLM để đánh giá và thay đổi LLM cung cấp phản hồi, MINT có thể đo lường khả
năng cung cấp phản hồi hữu ích của các LLM khác nhau (§3.4); Chúng tôi phát hiện rằng khả năng
cung cấp phản hồi có thể trực giao với khả năng giải quyết nhiệm vụ: mặc dù hoạt động kém nhất trong
giải quyết nhiệm vụ, CodeLLaMA-34B-Instruct có thể cung cấp phản hồi để cải thiện GPT-3.5 mạnh hơn.
Ngoài ra, đánh giá thách thức của MINT tiết lộ các artifact không mong muốn trong dữ liệu ShareGPT
(2023), một bộ dữ liệu được sử dụng rộng rãi cho tinh chỉnh hướng dẫn (§3.5). Hơn nữa, chúng tôi
cho thấy rằng phản hồi ngôn ngữ được mô phỏng bởi GPT4 cũng hữu ích như phản hồi do con người
viết dựa trên đánh giá của con người và hiệu suất nhiệm vụ (§3.6).

Chúng tôi hy vọng rằng MINT có thể giúp theo dõi tiến độ và khuyến khích nghiên cứu tương lai trong
việc cải thiện khả năng giải quyết nhiệm vụ đa lượt và/hoặc cung cấp phản hồi của LLM, đặc biệt cho
các cộng đồng mã nguồn mở nơi đánh giá con người có thể ít được tiếp cận hơn so với LLM thương mại
với cơ sở người dùng lớn.

2 MINT

Trong phần này, chúng tôi thảo luận (1) cách đánh giá tương tác đa lượt (§2.1) với việc sử dụng công cụ
và phản hồi ngôn ngữ trong các thiết lập khác nhau; (2) cách chúng tôi tái sử dụng các bộ dữ liệu hiện
có cho đánh giá MINT (§2.2). Chúng tôi sử dụng Hình 1 làm ví dụ chạy.

2.1 KHUNG TƯƠNG TÁC

MINT nhằm mô phỏng các ứng dụng thực tế của LLM, nhấn mạnh tương tác người dùng-LLM và LLM-công cụ.
Trong giải quyết vấn đề hợp tác người dùng-LLM, một người dùng con người cung cấp hướng dẫn ban
đầu và nhằm thu được một giải pháp đúng với ít nỗ lực để giúp đỡ LLM. Mặt khác, việc tăng cường
LLM với các công cụ có thể cải thiện hiệu quả khả năng giải quyết nhiệm vụ của LLM (Mialon et al., 2023),
cho thấy tầm quan trọng của tương tác LLM-công cụ. Chúng tôi hướng dẫn LLM (§F.4.1) thực hiện các
bước sau trong mỗi lượt: (1) tùy chọn thể hiện quá trình lý luận của nó ("Thought:" trong Hình 1, tương
tự như Yao et al. 2022); (2) sau đó hoặc tương tác với công cụ bằng cách tạo mã Python và thực thi
nó thông qua một trình thông dịch Python ("Execute:" trong Hình 1), hoặc đề xuất một giải pháp cho
người dùng ("Propose Solution:" trong Hình 1). Chúng tôi áp dụng mã như một giao diện công cụ thống
nhất do tính linh hoạt và hiệu suất của nó, như được chứng minh bởi Wang et al. (2024). Trong việc
triển khai của chúng tôi, mô hình được hướng dẫn bọc các hành động "Execute" và "Propose Solution"
của chúng bằng các cặp thẻ <execute> và <solution> để dễ dàng phân tích. Chúng tôi tiêu chuẩn hóa
các prompt và ví dụ trong ngữ cảnh cho các biến thể LLM khác nhau (base vs. chat) và cho việc giải
quyết nhiệm vụ và cung cấp phản hồi, nhằm so sánh công bằng và có thể tái tạo (Phụ lục §F.4.1, §F.4.2,
và §F.5). Trong phần tiếp theo, chúng tôi giới thiệu ba thiết lập với độ phức tạp tương tác tăng dần để
đo lường các khía cạnh khác nhau của tương tác đa lượt.

LLM tương tác với người dùng lười biếng. Chúng tôi xem xét tình huống nơi người dùng cung cấp
hướng dẫn ban đầu và nỗ lực tối thiểu để hướng dẫn LLM hướng tới giải pháp cuối cùng. Thiết lập này
bao gồm các vấn đề thực tế "khó giải quyết trực tiếp, nhưng dễ xác minh": người dùng có thể không
biết cách giải quyết vấn đề hoặc hiểu quá trình lý luận, nhưng vẫn có thể cung cấp phản hồi tối thiểu
về kết quả cuối cùng (ví dụ, "điều này không phải là những gì tôi muốn"). Điều này sẽ phục vụ như
một đường cơ sở để đánh giá khả năng của LLM trong việc giải quyết các nhiệm vụ được tăng cường
bởi công cụ và tận dụng phản hồi ngôn ngữ tự nhiên. LLM được cho hai lần thử để đề xuất giải pháp
cho mỗi vấn đề, với giới hạn về số lượt tương tác k (§3.1). Khi có một giải pháp được đề xuất, MINT
mô phỏng người dùng kiểm tra tính đúng đắn của giải pháp với các câu trả lời đúng. Khi lần thử đầu
tiên sai, người dùng phản hồi "Your answer is wrong." Tương tác kết thúc hoặc sau khi LLM đã thực
hiện hai lần thử để đề xuất giải pháp, hoặc khi một giải pháp được đề xuất là đúng (lượt thứ 5 của
Hình 1), hoặc khi lượt tương tác thứ k được đạt tới. Chúng tôi coi đây là trường hợp Tương tác Người
dùng Lười biếng-LLM vì người dùng được mô phỏng cung cấp tối đa một phản hồi nhị phân bổ sung
trong tương tác. Chúng tôi tuân theo thực hành đánh giá tiêu chuẩn và sử dụng các metric đánh giá
đã được thiết lập cho mỗi nhiệm vụ trong §2.2.

LLM tương tác với người dùng lười biếng và công cụ. Trong thiết lập tương tác Người dùng Lười
biếng-LLM, chúng tôi đo lường khả năng của LLM trong việc giải quyết nhiệm vụ bằng công cụ bằng
cách so sánh tỷ lệ thành công giải quyết nhiệm vụ của chúng

--- TRANG 2 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024

Hình 1: Một quỹ đạo tương tác được tạo ra bằng cách đánh giá gpt-3.5-turbo-0613 với MINT
trên một nhiệm vụ lý luận toán học. Đầu ra của mô hình được đánh giá nằm trong các hộp màu xanh,
và phản hồi bởi gpt-4-0613 trong các hộp màu đỏ, nét đứt. Một số chi tiết đã được bỏ qua để rõ ràng.

qua các giới hạn tương tác k khác nhau. Đối với mỗi lượt, LLM có thể chọn tương tác với công cụ (tạo
mã để gọi equation-solver trong Hình 1) hoặc đề xuất một giải pháp (lượt thứ 5 trong Hình 1). Để giữ
cho LLM không bị kẹt trong vòng lặp vô hạn của việc gọi công cụ mà không đề xuất giải pháp, MINT
nhắc nhở LLM: "You have X steps left and Y chances to propose solution left," và cung cấp một hướng
dẫn bổ sung ở lượt cuối: "You should take the last step to propose a solution." Theo trực giác, với
nhiều tương tác hơn với công cụ, LLM có thể nhận được nhiều quan sát hữu ích hơn thông qua trình
thông dịch Python (ví dụ, kết quả tính toán, thông báo lỗi). Chúng tôi thay đổi k ∈ {1,2,3,4,5} và so
sánh tỷ lệ thành công của các mô hình với mỗi k. Chúng tôi coi mức tăng hiệu suất của LLM w.r.t.
k và hiệu suất tuyệt đối tại k = 5 như khả năng giải quyết nhiệm vụ được tăng cường bởi công cụ của
chúng (§3.2).

Tương tác người dùng-LLM thông tin với phản hồi ngôn ngữ. Ngoài tương tác Người dùng Lười
biếng-LLM, chúng tôi điều tra cách LLM hoạt động khi người dùng giống như một giáo viên kiên nhẫn
cung cấp những gợi ý hữu ích (ví dụ, người dùng kỹ thuật hiểu quá trình giải quyết vấn đề). Tuy nhiên,
việc thu thập phản hồi ngôn ngữ con người cho đánh giá LLM đặt ra những thách thức về tính tái tạo
do các tiêu chuẩn không nhất quán và có thể tốn kém, đặc biệt cho các cộng đồng mã nguồn mở với
tài nguyên tương đối ít hơn4. Để giải quyết những vấn đề này, chúng tôi prompt GPT-4 (§F.4.2) để mô
phỏng phản hồi ngôn ngữ của người dùng (các hộp nét đứt trong Hình 1). Chúng tôi xác thực hiệu quả
của phản hồi GPT-4 trong một đánh giá con người (§3.6). Chúng tôi so sánh hiệu suất giữa (1) phản hồi
ngôn ngữ được mô phỏng và (2) tương tác người dùng lười biếng-LLM, cả hai trong thiết lập tương tác
được tăng cường bởi công cụ với giới hạn tương tác k = 5. Chúng tôi coi hiệu suất (tuyệt đối) và cải
thiện từ phản hồi ngôn ngữ như khả năng tận dụng phản hồi ngôn ngữ tự nhiên của LLM.

2.2 TÁI SỬ DỤNG CÁC BỘ DỮ LIỆU HIỆN CÓ CHO MINT

Đánh giá LLM trong tương tác đa lượt có thể tốn kém do nhu cầu suy luận lặp đi lặp lại. Ví dụ, HotpotQA
(Yang et al., 2018) có 7.405 ví dụ kiểm tra. Đánh giá với năm lượt yêu cầu ít nhất 7.405 × 5 = 37K lần
chạy suy luận LLM. Các phương pháp trước đây (Yao et al., 2022; Shinn et al., 2023) chọn đánh giá
trên các ví dụ kiểm tra được rút ra ngẫu nhiên, tăng rào cản cho so sánh công bằng. Chúng tôi chọn
các nhiệm vụ đa dạng từ các bộ dữ liệu đã được thiết lập yêu cầu tương tác đa lượt để giải quyết đồng
thời duy trì tập con được chọn nhỏ gọn để đánh giá dễ tiếp cận. Đoạn sau mô tả phương pháp ba bước
của chúng tôi để tái sử dụng các bộ dữ liệu cho MINT. Chúng tôi cung cấp nguồn dữ liệu và thống kê
trong Bảng 1. Để biết thêm chi tiết, vui lòng tham khảo §D trong Phụ lục.

Nguồn dữ liệu của MINT. Mục tiêu chính của chúng tôi là tạo một đánh giá toàn diện bao gồm các
nhiệm vụ được hưởng lợi từ tương tác. Chúng tôi chọn ba loại nhiệm vụ:

• Lý luận, bao gồm lý luận toán học (GSM8K, MATH, TheoremQA), trả lời câu hỏi đa bước (HotpotQA),
và giải quyết vấn đề kiến thức (MMLU). Chúng tôi ngầm lọc ra các câu hỏi chuyên sâu về kiến thức
không yêu cầu lý luận đa bước trong bước tiếp theo.

• Tạo mã, bao gồm HumanEval và MBPP.

• Nhiệm vụ ra quyết định trong ALFWorld, một trình mô phỏng gia đình được nhúng với giao diện
chỉ văn bản dựa trên TextWorld (Côté et al., 2018).

Từ tám bộ dữ liệu, chúng tôi tạo một tập kiểm tra ban đầu gồm 29.307 trường hợp. Tất cả các trường
hợp ban đầu được thiết kế cho đánh giá một vòng mà không có tương tác, ngoại trừ ra quyết định
(ALFWorld). Tương tự như Yao et al. (2022); Gao et al. (2023), chúng tôi điều chỉnh các nhiệm vụ lý
luận thành các nhiệm vụ tương tác đa lượt bằng cách tăng cường LLM với công cụ để giải quyết vấn
đề (§F.5.3). Thông qua prompting (§F.5.2), chúng tôi khuyến khích LLM sử dụng trình thông dịch
Python để kiểm tra mã được tạo ra của chúng trên bộ kiểm tra công khai được cung cấp cho các vấn
đề tạo mã trước khi cam kết với một giải pháp.

Giữ các trường hợp yêu cầu tương tác đa lượt. Để trả lời tốt hơn câu hỏi nghiên cứu của chúng tôi
"LLM được hưởng lợi như thế nào từ tương tác đa lượt," chúng tôi chỉ giữ các trường hợp thách thức
và yêu cầu tương tác đa lượt. Vì chúng tôi cho phép LLM đề xuất giải pháp nhiều hơn một lần, chúng
tôi lọc ra các trường hợp mà một đường cơ sở đoán ngẫu nhiên có thể làm tốt, ví dụ, các trường hợp
lựa chọn nhiều với <4 tùy chọn. Sau đó chúng tôi chạy gpt-3.5-turbo-0613 (OpenAI API) trên bộ
dữ liệu ban đầu và loại trừ các trường hợp hoàn thành trong hai lượt (ví dụ, các vấn đề dễ có thể được
giải quyết mà không cần đa lượt).

Lấy mẫu phân tầng con để đánh giá hiệu quả. Chúng tôi sử dụng lấy mẫu phân tầng (Neyman, 1992)
để tạo một tập đại diện gồm 586 ví dụ, đảm bảo rằng tỷ lệ các ví dụ đúng và sai trong tập kết quả phản
ánh tỷ lệ của dữ liệu gốc để cân bằng độ khó của các mẫu kết quả.

3 THỰC NGHIỆM

3.1 THIẾT LẬP

LLM được đánh giá. Để đo lường toàn diện khả năng tương tác đa lượt và xác định khoảng cách tiềm
năng giữa LLM mã nguồn mở và đóng, chúng tôi đánh giá 4 LLM đóng và 16 LLM mã nguồn mở.
Chúng tôi bao gồm các kích thước và kỹ thuật huấn luyện khác nhau để hiểu rõ hơn cách chúng ảnh
hưởng đến khả năng tương tác đa lượt của LLM. Các kỹ thuật huấn luyện dẫn đến ba biến thể mô hình:
các mô hình được huấn luyện trước (base), các mô hình được tinh chỉnh hướng dẫn có giám sát (SIFT,
Wei et al., 2022), và các mô hình được huấn luyện với học tăng cường từ phản hồi con người (RLHF,
Ouyang et al., 2022a). Đối với các mô hình đóng, chúng tôi đánh giá các LLM thương mại phổ biến,
bao gồm gpt-3.5-turbo-0613 từ OpenAI API; claude-instant-1, claude-2 từ Anthropic Claude
API5; Bard chat-bison-001 từ Bard API. Đối với LLM mã nguồn mở, chúng tôi đánh giá họ mô hình
LLaMA-2 (7B, 13B, 70B) (Touvron et al., 2023), bao gồm base và chat (RLHF); Vicuna-v1.5 (7B, 13B)
(Zheng et al., 2023), một mô hình SIFT được tinh chỉnh trên các cuộc hội thoại đa lượt dựa trên LLaMA-2-base;
họ mô hình CodeLLaMA (7B, 13B, 34B) (Rozière et al., 2023) tiền huấn luyện LLaMA-2-base trên mã,
bao gồm base và instruct (SIFT); Lemur-v1-70B (Xu et al., 2023) tiền huấn luyện LLaMA-2 trên dữ
liệu chuyên sâu về mã, bao gồm base và chat (SIFT).

Metric. Chúng tôi coi Tỷ lệ Thành công SR như metric đánh giá của chúng tôi, đo lường phần trăm
các trường hợp nhiệm vụ thành công. Đối với giới hạn tương tác k, chúng tôi bắt đầu từ đầu và cho
phép mỗi LLM tương tác lên đến lượt thứ k và đo lường SRk tương ứng của chúng. Trừ khi có ghi chú
khác, chúng tôi giới hạn k ∈ [1,5] nơi k = 1 có nghĩa là không có tương tác và k = 5 tối đa hóa các
lượt tương tác trong cửa sổ ngữ cảnh của hầu hết LLM hiện đại (4.096 token).

3.2 ĐO LƯỜNG KHIẢ NĂNG GIẢI QUYẾT NHIỆM VỤ ĐƯỢC TĂNG CƯỜNG BỞI CÔNG CỤ CỦA LLM
TRONG TƯƠNG TÁC ĐA LƯỢT

Chúng tôi yêu cầu LLM giải quyết các nhiệm vụ (§2.2) với các giới hạn tương tác k ∈ {1,2,3,4,5} khác
nhau mà không có phản hồi ngôn ngữ tự nhiên (Hình 1 không có hộp nét đứt màu đỏ), và định lượng
khả năng giải quyết nhiệm vụ được tăng cường bởi công cụ của LLM bằng (1) hiệu suất tuyệt đối SR5
và (2) cải thiện cho mỗi lượt tương tác bổ sung Δtools được ước tính như độ dốc b từ hồi quy bình
phương tối thiểu minb,a ∑k(b · k + a − SRk)² (Bảng 2). Vì mối quan hệ SRk vs. k cơ bản có thể không
tuyến tính, chúng tôi chỉ sử dụng hệ số hồi quy (với R²) như một ước tính thô về tỷ lệ cải thiện để
bổ sung cho tỷ lệ thành công tuyệt đối SR5 để hiểu toàn diện hơn về khả năng của các mô hình.

Quan sát tổng thể. Trong Hình 2, chúng tôi thấy tất cả các mô hình mã nguồn mở đều tụt hậu so với
các mô hình đóng thương mại tốt nhất trong cả SR5 và Δtools, với claude-2 và claude-instant-1
vượt trội hơn tất cả LLM mã nguồn mở trong Δtools với R² cao, gợi ý cải thiện gần tuyến tính. Đáng
chú ý, mặc dù hoạt động kém tại k = 1, claude-instant-1 vượt qua claude-2 khi k tăng lên 3, thậm
chí đạt được SR5 cao hơn (45,9% so với 39,9%), gợi ý khả năng vượt trội của claude-instant-1 trong
việc cải thiện với tương tác đa lượt.

Hiệu suất tuyệt đối và cải thiện mỗi lượt mở rộng theo kích thước mô hình. Đối với CodeLLaMA và
LLaMA-2 mã nguồn mở, chúng tôi quan sát một xu hướng trên tất cả các biến thể (Base, SIFT, và RLHF)
rằng Δtools và SR5 tăng khi mở rộng LLM. Như chúng tôi thảo luận trong §3.5, các mô hình Vicuna-v1.5
là một ngoại lệ, có thể do các artifact huấn luyện của chúng làm tổn hại hiệu suất nhiệm vụ.

SIFT trên dữ liệu đa lượt có thể hữu ích. Mặc dù có vấn đề trên, Vicuna-v1.5 (7B, SIFT) thực sự cho
thấy hiệu suất mạnh hơn so với LLaMA-2 (Base và RLHF, 7B) trong Δtools (+3,4% so với +2,2% /+1,5%)
và SR5 (12,6% so với 9,7% /7,3%). Lemur-v1 (70B, SIFT) cũng cho thấy hiệu suất mạnh hơn so với
biến thể Base của nó. Tuy nhiên, ngoại trừ CodeLLaMA (7B), chúng tôi không tìm thấy cải thiện tương
tự trên CodeLLaMA (SIFT). Chúng tôi giả thuyết rằng mức tăng hiệu suất trên Vicuna-v1.5 và Lemur-v1
có thể được quy cho việc tinh chỉnh trên các cuộc hội thoại đa lượt Người-ChatGPT của ShareGPT.

RLHF có thể làm tổn hại tương tác đa lượt LLM-công cụ. Chúng tôi thấy rằng trên dòng LLaMA-2,
việc căn chỉnh RLHF nói chung làm tổn hại hiệu suất của các mô hình trong cả Δtools (-0,7% đến -2,6%)
và SR5 (-2,4% đến -8,5%), tương tự như quan sát trước đây rằng căn chỉnh có thể làm giảm hiệu suất
nhiệm vụ (Ouyang et al., 2022b). Tuy nhiên, khó để kết luận rằng RLHF nói chung làm tổn hại hiệu
suất mô hình. Chúng tôi để lại cho công việc tương lai khám phá vai trò của RLHF trong tương tác đa lượt.

--- TRANG 3 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024

Bảng 1: Thống kê bộ dữ liệu về các trường hợp dữ liệu được tái sử dụng từ các bộ dữ liệu hiện có
thành MINT. Chúng tôi lọc và giảm mẫu các bộ dữ liệu hiện có để xây dựng một tập các nhiệm vụ
phức tạp nhỏ gọn yêu cầu tương tác đa lượt để giải quyết (§2.2).

| Loại Nhiệm vụ | Tên Nhiệm vụ | Kích thước Gốc | Kích thước Giảm trong MINT |
|---|---|---|---|
| Tạo Mã | HumanEval (Chen et al., 2021) | 164 | 45 |
| | MBPP (Austin et al., 2021) | 500 | 91 |
| Ra Quyết định | ALFWorld (Shridhar et al., 2020) | 134 | 134 |
| Lý luận | GSM8K (Cobbe et al., 2021) | 1319 | 48 |
| | HotpotQA (Yang et al., 2018) | 7.405 | 43 |
| | MATH (Hendrycks et al., 2021) | 5.000 | 100 |
| | MMLU (Hendrycks et al., 2020) | 13.985 | 76 |
| | TheoremQA (Chen et al., 2023a) | 800 | 49 |
| Tổng | | 29.307 | 586 |

trên các giới hạn tương tác k khác nhau. Đối với mỗi lượt, LLM có thể chọn tương tác với công cụ (tạo
mã để gọi equation-solver trong Hình 1) hoặc đề xuất một giải pháp (lượt thứ 5 trong Hình 1). Để giữ
cho LLM không bị kẹt trong vòng lặp vô hạn của việc gọi công cụ mà không đề xuất giải pháp, MINT
nhắc nhở LLM: "You have X steps left and Y chances to propose solution left," và cung cấp một hướng
dẫn bổ sung ở lượt cuối: "You should take the last step to propose a solution." Theo trực giác, với
nhiều tương tác hơn với công cụ, LLM có thể nhận được nhiều quan sát hữu ích hơn thông qua trình
thông dịch Python (ví dụ, kết quả tính toán, thông báo lỗi). Chúng tôi thay đổi k ∈ {1,2,3,4,5} và so
sánh tỷ lệ thành công của các mô hình với mỗi k. Chúng tôi coi mức tăng hiệu suất của LLM w.r.t.
k và hiệu suất tuyệt đối tại k = 5 như khả năng giải quyết nhiệm vụ được tăng cường bởi công cụ của
chúng (§3.2).

Tương tác người dùng-LLM thông tin với phản hồi ngôn ngữ. Ngoài tương tác Người dùng Lười
biếng-LLM, chúng tôi điều tra cách LLM hoạt động khi người dùng giống như một giáo viên kiên nhẫn
cung cấp những gợi ý hữu ích (ví dụ, người dùng kỹ thuật hiểu quá trình giải quyết vấn đề). Tuy nhiên,
việc thu thập phản hồi ngôn ngữ con người cho đánh giá LLM đặt ra những thách thức về tính tái tạo
do các tiêu chuẩn không nhất quán và có thể tốn kém, đặc biệt cho các cộng đồng mã nguồn mở với
tài nguyên tương đối ít hơn4. Để giải quyết những vấn đề này, chúng tôi prompt GPT-4 (§F.4.2) để mô
phỏng phản hồi ngôn ngữ của người dùng (các hộp nét đứt trong Hình 1). Chúng tôi xác thực hiệu quả
của phản hồi GPT-4 trong một đánh giá con người (§3.6). Chúng tôi so sánh hiệu suất giữa (1) phản hồi
ngôn ngữ được mô phỏng và (2) tương tác người dùng lười biếng-LLM, cả hai trong thiết lập tương tác
được tăng cường bởi công cụ với giới hạn tương tác k = 5. Chúng tôi coi hiệu suất (tuyệt đối) và cải
thiện từ phản hồi ngôn ngữ như khả năng tận dụng phản hồi ngôn ngữ tự nhiên của LLM.

2.2 TÁI SỬ DỤNG CÁC BỘ DỮ LIỆU HIỆN CÓ CHO MINT

Đánh giá LLM trong tương tác đa lượt có thể tốn kém do nhu cầu suy luận lặp đi lặp lại. Ví dụ, HotpotQA
(Yang et al., 2018) có 7.405 ví dụ kiểm tra. Đánh giá với năm lượt yêu cầu ít nhất 7.405 × 5 = 37K lần
chạy suy luận LLM. Các phương pháp trước đây (Yao et al., 2022; Shinn et al., 2023) chọn đánh giá
trên các ví dụ kiểm tra được rút ra ngẫu nhiên, tăng rào cản cho so sánh công bằng. Chúng tôi chọn
các nhiệm vụ đa dạng từ các bộ dữ liệu đã được thiết lập yêu cầu tương tác đa lượt để giải quyết đồng
thời duy trì tập con được chọn nhỏ gọn để đánh giá dễ tiếp cận. Đoạn sau mô tả phương pháp ba bước
của chúng tôi để tái sử dụng các bộ dữ liệu cho MINT. Chúng tôi cung cấp nguồn dữ liệu và thống kê
trong Bảng 1. Để biết thêm chi tiết, vui lòng tham khảo §D trong Phụ lục.

Nguồn dữ liệu của MINT. Mục tiêu chính của chúng tôi là tạo một đánh giá toàn diện bao gồm các
nhiệm vụ được hưởng lợi từ tương tác. Chúng tôi chọn ba loại nhiệm vụ:

• Lý luận, bao gồm lý luận toán học (GSM8K, MATH, TheoremQA), trả lời câu hỏi đa bước (HotpotQA),
và giải quyết vấn đề kiến thức (MMLU). Chúng tôi ngầm lọc ra các câu hỏi chuyên sâu về kiến thức
không yêu cầu lý luận đa bước trong bước tiếp theo.

• Tạo mã, bao gồm HumanEval và MBPP.

• Nhiệm vụ ra quyết định trong ALFWorld, một trình mô phỏng gia đình được nhúng với giao diện
chỉ văn bản dựa trên TextWorld (Côté et al., 2018).

--- TRANG 4 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024

Bảng 2: Tỷ lệ thành công giải quyết nhiệm vụ được tăng cường bởi công cụ với giới hạn tương tác k
khác nhau (tức là số lượt tương tác tối đa được phép) và tỷ lệ cải thiện (ước tính với hệ số hồi quy
bình phương tối thiểu, R² hồi quy cũng được bao gồm). Độ dốc (tức là hệ số) chỉ ra tỷ lệ cải thiện
trong khi R² biểu thị mức độ phù hợp của mô hình hồi quy với dữ liệu.

| Mô hình | Kích thước | Loại | SR (Trung bình vi mô qua các nhiệm vụ) | | | | | Tỷ lệ Cải thiện |
|---|---|---|---|---|---|---|---|---|
| | | | k=1 | k=2 | k=3 | k=4 | k=5 | Độ dốc | R² |
| **LLM Mã nguồn mở** |
| CodeLLaMA | 7B | Base* | 0.3 | 4.1 | 7.2 | 7.2 | 4.3 | +1.1 | 0.38 |
| | | SIFT | 0.3 | 7.8 | 10.2 | 9.7 | 8.7 | +1.9 | 0.53 |
| | 13B | Base | 0.5 | 13.7 | 17.9 | 19.3 | 18.4 | +4.1 | 0.70 |
| | | SIFT* | 1.5 | 12.6 | 13.1 | 15.0 | 14.5 | +2.8 | 0.64 |
| | 34B | Base | 0.2 | 16.2 | 23.0 | 25.9 | 28.2 | +6.6 | 0.85 |
| | | SIFT*† | 2.6 | 10.1 | 14.7 | 15.4 | 17.1 | +3.4 | 0.86 |
| LLaMA-2 | 7B | Base | 0.2 | 5.6 | 7.3 | 8.9 | 9.7 | +2.2 | 0.87 |
| | | RLHF* | 1.0 | 4.3 | 6.7 | 6.5 | 7.3 | +1.5 | 0.83 |
| | 13B | Base | 0.2 | 11.4 | 15.5 | 15.2 | 14.5 | +3.2 | 0.63 |
| | | RLHF | 4.1 | 12.5 | 12.5 | 13.3 | 11.9 | +1.7 | 0.47 |
| | 70B | Base | 1.9 | 19.4 | 24.6 | 26.4 | 26.4 | +5.6 | 0.73 |
| | | RLHF | 4.3 | 14.3 | 15.7 | 16.6 | 17.9 | +3.0 | 0.73 |
| Lemur-v1 | 70B | Base | 1.0 | 17.9 | 23.6 | 25.3 | 26.3 | +5.8 | 0.77 |
| | | SIFT | 3.8 | 27.0 | 35.7 | 37.5 | 37.0 | +7.7 | 0.73 |
| Vicuna-v1.5 | 7B | SIFT† | 0.0 | 6.7 | 12.3 | 15.4 | 12.6 | +3.4 | 0.77 |
| | 13B | SIFT† | 0.0 | 2.2 | 4.4 | 6.7 | 8.4 | +2.1 | 1.00 |
| **LLM Đóng** |
| chat-bison-001 | - | -* | 0.3 | 15.9 | 14.2 | 13.0 | 14.5 | +2.5 | 0.40 |
| claude-2 | - | - | 26.4 | 35.5 | 36.0 | 39.8 | 39.9 | +3.1 | 0.81 |
| claude-instant-1 | - | - | 12.1 | 32.2 | 39.2 | 44.4 | 45.9 | +8.0 | 0.84 |
| gpt-3.5-turbo-0613 | - | - | 2.7 | 16.9 | 24.1 | 31.7 | 36.2 | +8.2 | 0.96 |
| gpt-4-0613 | - | - | - | - | - | - | 69.5 | - | - |

*LLM được đánh giá không thể tạo ra đầu ra có thể phân tích theo hướng dẫn trong một số trường hợp. Xem §3.5 và Bảng A.7 để biết chi tiết.
†Chúng tôi xác định các artifact không mong muốn tiềm năng trong dữ liệu huấn luyện của nó, làm tổn hại hiệu suất của nó. Xem §3.5 để biết chi tiết.

Hình 2: Với giới hạn tương tác k tăng dần, tỷ lệ thành công (%, trung bình vi mô) cải thiện với tỷ lệ
khác nhau cho các LLM khác nhau. Để rõ ràng, chỉ một tập con của các LLM được đánh giá được hình dung.

SIFT có thể có lợi cho dữ liệu đa lượt. Mặc dù có vấn đề trên, Vicuna-v1.5 (7B, SIFT) thực sự cho
thấy hiệu suất mạnh hơn so với LLaMA-2 (Base và RLHF, 7B) trong Δtools (+3.4% so với +2.2% /+1.5%)
và SR5 (12.6% so với 9.7% /7.3%). Lemur-v1 (70B, SIFT) cũng cho thấy hiệu suất mạnh hơn so với
biến thể Base của nó. Tuy nhiên, ngoại trừ CodeLLaMA (7B), chúng tôi không tìm thấy cải thiện tương
tự trên CodeLLaMA (SIFT). Chúng tôi giả thuyết rằng mức tăng hiệu suất trên Vicuna-v1.5 và Lemur-v1
có thể được quy cho việc tinh chỉnh trên các cuộc hội thoại đa lượt Người-ChatGPT của ShareGPT.

RLHF có thể làm tổn hại tương tác đa lượt LLM-công cụ. Chúng tôi thấy rằng trên dòng LLaMA-2,
việc căn chỉnh RLHF nói chung làm tổn hại hiệu suất của các mô hình trong cả Δtools (-0.7% đến -2.6%)
và SR5 (-2.4% đến -8.5%), tương tự như quan sát trước đây rằng căn chỉnh có thể làm giảm hiệu suất
nhiệm vụ (Ouyang et al., 2022b). Tuy nhiên, khó để kết luận rằng RLHF nói chung làm tổn hại hiệu
suất mô hình. Chúng tôi để lại cho công việc tương lai khám phá vai trò của RLHF trong tương tác đa lượt.

--- TRANG 5 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024

3.3 ĐO LƯỜNG KHẢ NĂNG TẬN DỤNG PHẢN HỒI NGÔN NGỮ TỰ NHIÊN CỦA LLM

Trên tầng tương tác LLM-công cụ, chúng tôi sử dụng gpt-4-0613 để mô phỏng phản hồi người dùng
cho các LLM được đánh giá (Hình 1 với hộp nét đứt màu đỏ). Với giới hạn tương tác k = 5, chúng tôi
đo lường khả năng tận dụng phản hồi ngôn ngữ tự nhiên của LLM bằng hiệu suất tuyệt đối SRfeedback5
và sự khác biệt hiệu suất sau khi được đưa ra phản hồi: Δfeedback = SRfeedback5 − SR5. Chúng tôi
trình bày kết quả trong Bảng 3.

Quan sát tổng thể. Chúng tôi không thấy sự khác biệt đáng kể giữa các mô hình mã nguồn mở và đóng
về Δfeedback. Các mô hình mã nguồn mở nhận được +1.7–+17.2% từ phản hồi, trong khi các mô hình
đóng nhận được +6.5–+15.2%. Tuy nhiên, vẫn có khoảng cách giữa chúng về tỷ lệ thành công tuyệt
đối SRfeedback5, vì mô hình mã nguồn mở tốt nhất Lemur-v1 (70B, SIFT) vẫn tụt hậu so với mô hình
đóng tốt nhất claude-instant-1 8.7%. Đáng ngạc nhiên, chúng tôi thấy rằng CodeLLaMA-34B-base
có thể đạt được hiệu suất tương đương với GPT-4 trên các nhiệm vụ ra quyết định với phản hồi ngôn
ngữ từ nó, cho thấy khả năng mạnh mẽ trong việc tận dụng phản hồi ngôn ngữ.

Ảnh hưởng của SIFT và RLHF. Tương tự như §3.2, chúng tôi thấy rằng SIFT và RLHF làm tổn hại
khả năng tận dụng phản hồi của các mô hình. Kết quả trên CodeLLaMA (ngoại trừ 7B) và LLaMA-2
cho thấy các mô hình SIFT/RLHF đều có Δfeedback và SRfeedback5 thấp hơn so với các biến thể base
của chúng. Hai ngoại lệ khác là Vicuna-v1.5 (7B) và Lemur-v1 (70B). Chúng tôi suy đoán việc sử
dụng các cuộc hội thoại đa lượt (ShareGPT) cho SIFT góp phần vào hai ngoại lệ này.

3.4 ĐO LƯỜNG HIỆU QUẢ CỦA KHẢ NĂNG CUNG CẤP PHẢN HỒI CỦA CÁC LLM KHÁC NHAU

Cố định mô hình được đánh giá là gpt-3.5-turbo-0613, chúng tôi đánh giá khả năng cung cấp phản
hồi của bảy LLM thông qua Δfeedback (Bảng 4). Phát hiện chính của chúng tôi là khả năng giải quyết
nhiệm vụ có thể trực giao với khả năng cung cấp phản hồi: hiệu suất giải quyết nhiệm vụ cao hơn của
LLM không đảm bảo khả năng cung cấp phản hồi tốt hơn và ngược lại. Ví dụ, mặc dù GPT-3.5 (16k)
hoạt động tốt trong giải quyết nhiệm vụ (SR5 xếp thứ 3 trong Bảng 4), nó dẫn đến sự suy giảm hiệu
suất -10.4% trong GPT-3.5; Tương tự, GPT-4 với tự phản hồi trong Bảng 3 cũng trải qua sự suy giảm
hiệu suất. Mặt khác, mặc dù hoạt động kém nhất trong việc giải quyết nhiệm vụ trong Bảng 4,
CodeLLaMA-34B-Instruct có thể cung cấp phản hồi cải thiện GPT-3.5 mạnh hơn.

3.5 MINT CÓ THỂ GIÚP PHÁT HIỆN CÁC MẪU THẤT BẠI CỦA LLM ĐƯỢC ĐÁNH GIÁ

Đáng ngạc nhiên, ngoài việc đánh giá khả năng tương tác đa lượt của LLM, chúng tôi thấy các nhiệm
vụ đa lượt phức tạp trong MINT cũng có thể kiểm tra LLM về hành vi bất ngờ. Chúng tôi tìm thấy hai
loại bất thường chính: (1) không thể tuân theo hướng dẫn định dạng và (2) đầu ra bất ngờ có thể do
artifacts.

Không thể tuân theo hướng dẫn định dạng. Chúng tôi thấy rằng một số mô hình (ví dụ, CodeLLaMA
và LLaMA nhỏ hơn, chat-bison-001) gặp khó khăn trong việc tạo ra định dạng có thể phân tích theo
hướng dẫn, cản trở việc giải quyết nhiệm vụ (thống kê có thể được tìm thấy trong Bảng A.7).

Đầu ra bất ngờ có thể do artifact dữ liệu. Chúng tôi thấy rằng các mô hình Vicuna (SIFT trên dữ liệu
ShareGPT) tạo ra dấu gạch dưới thoát (" \") thay vì dấu gạch dưới (" ") trên tất cả các nhiệm vụ, gây
ra lỗi cú pháp khi thực thi mã và giảm hiệu suất. Chúng tôi kiểm tra dữ liệu ShareGPT (2023) và tìm
thấy ít nhất một dấu gạch dưới thoát (" \") artifact trên 15% ví dụ, gợi ý artifacts trong dữ liệu huấn
luyện có thể gây ra vấn đề này. Chúng tôi quan sát một vấn đề tương tự với CodeLLaMA-Instruct:
Chúng tôi thấy rằng CodeLLaMA-Instruct (34B) luôn bỏ qua hướng dẫn do người dùng đưa ra trên
các nhiệm vụ tạo mã "wrap your code with <execute> tag" và sử dụng [PYTHON] để bọc mã (xảy ra
trên 100% các nhiệm vụ tạo mã, 0% trên các nhiệm vụ khác). Touvron et al. (2023) sử dụng [PYTHON]
như thẻ để tạo ra dữ liệu tự hướng dẫn trên các vấn đề mã cho SIFT. Chúng tôi nghi ngờ các mô hình
CodeLLaMA-Instruct được huấn luyện và quá khớp với token [PYTHON], khiến chúng tạo ra [PYTHON]
bất kể hướng dẫn của người dùng. Chúng tôi tham khảo §E.1 và §E.2 để biết ví dụ và kết quả định lượng.

3.6 GPT-4 CÓ THỂ TẠO RA PHẢN HỒI NGÔN NGỮ TỰ NHIÊN CẤP ĐỘ CON NGƯỜI KHÔNG?

Chúng tôi thực hiện một đánh giá con người so sánh định lượng phản hồi được tạo ra bởi GPT-4 và
được viết bởi con người. Chi tiết có thể được tìm thấy trong Phụ lục §B. Trong Bảng 5, các chú thích
viên con người coi 91.2% phản hồi ngôn ngữ được tạo ra bởi GPT-4 là hữu ích như, nếu không tốt
hơn, phản hồi được viết bởi con người. Cũng khó để con người phân biệt phản hồi được tạo ra bởi
GPT-4 với phản hồi của con người

--- TRANG 6 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024

Bảng 3: Khả năng tận dụng phản hồi ngôn ngữ tự nhiên của LLM, được đo bằng Δfeedback giữa hiệu
suất của các mô hình có và không có phản hồi được tạo ra bởi gpt-4-0613. Tất cả các mô hình được
đánh giá với giới hạn lượt tương tác k = 5. Đối với cả LLM mã nguồn mở và đóng, hiệu suất tốt nhất
được in đậm, và hiệu suất tốt thứ hai được gạch dưới.

[Bảng phức tạp với nhiều cột và hàng - tôi sẽ tóm tắt nội dung chính:]
Bảng này hiển thị hiệu suất của các LLM khác nhau (CodeLLaMA, LLaMA-2, Lemur-v1, Vicuna-v1.5, và các mô hình đóng) trên ba loại nhiệm vụ (Lý luận, Ra quyết định, Mã) với và không có phản hồi GPT-4, cùng với Δfeedback cho mỗi mô hình.

*LLM được đánh giá không thể tạo ra đầu ra có thể phân tích theo hướng dẫn trong một số trường hợp (§2.1). Xem §3.5 và Bảng A.7 để biết chi tiết.
†Chúng tôi xác định các artifact không mong muốn tiềm năng trong dữ liệu huấn luyện của nó, làm tổn hại hiệu suất của nó. Xem §3.5 để biết chi tiết.

--- TRANG 7 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024

Bảng 4: Khả năng cung cấp phản hồi của LLM, được đo bằng Δfeedback với LLM được đánh giá cố
định (GPT-3.5). Chúng tôi cũng báo cáo sự khác biệt SR5 giữa LLM cung cấp phản hồi và LLM được
đánh giá.

| LLM Cung cấp Phản hồi | Sự khác biệt SR5 | Δfeedback |
|---|---|---|
| gpt-4-0613 | +33.3 | +15.2 |
| claude-instant-1 | +9.7 | +1.5 |
| gpt-3.5-turbo-16k-0613 | +4.1 | -10.4 |
| CodeLlama-34b (Base) | -8.0 | +2.4 |
| Llama-2-70b (Base) | -9.7 | -0.5 |
| Llama-2-70b-chat (RLHF) | -18.3 | -14.0 |
| CodeLlama-34b-Instruct (SIFT) | -19.1 | +3.2 |

Bảng 5: Đánh giá Con người về Phản hồi Được tạo ra bởi GPT-4 so với phản hồi được viết bởi con
người, đo lường tính hữu ích và giống con người.

| Phản hồi nào hữu ích hơn/giống con người hơn | Hữu ích | Giống Con người |
|---|---|---|
| Cả hai đều tương đương | 36.3% | 69.9% |
| Phản hồi GPT-4 | 54.9% | 22.1% |
| Phản hồi Con người | 8.8% | 8.0% |

(giống con người) trong 92% các trường hợp. Chúng tôi cũng so sánh phản hồi được tạo ra bởi GPT-4
và được viết bởi con người bằng cách yêu cầu gpt-3.5-turbo-0613 tiếp tục giải quyết vấn đề với một
lượt (1) phản hồi ngôn ngữ của con người hoặc (2) phản hồi GPT-4. Kết quả cho thấy phản hồi của
con người và phản hồi GPT-4 dẫn đến hiệu suất mô hình tương tự SRfeedback5 (32.7% so với 33.6%).

4 CÔNG TRÌNH LIÊN QUAN

4.1 LLM TRONG TƯƠNG TÁC

Tương tác với người dùng. LLM đã chứng minh tiềm năng rộng lớn trong tương tác liền mạch với
người dùng con người và trong việc đồng hóa phản hồi con người thời gian thực trong quá trình suy
luận (Fernandes et al., 2023). Theo các nghiên cứu gần đây, sự hợp tác cộng sinh này giữa con người
và LLM đã được khám phá trên nhiều lĩnh vực và ứng dụng khác nhau, bao gồm chỉnh sửa câu (Reid
& Neubig, 2022; Schick et al., 2023c; Chen et al., 2023b), phân tích cú pháp ngữ nghĩa (Yan et al., 2023;
Yao et al., 2019), tạo mã (Nijkamp et al., 2023; Elgohary et al., 2020), tinh chỉnh đầu ra lặp (Saunders
et al., 2022), và viết sáng tạo (Lee et al., 2022a; Shu et al., 2023; Wang et al., 2023b), tìm kiếm thông
tin tạo sinh (Kamalloo et al., 2023), và thậm chí chứng minh định lý (Yang et al., 2023b). Sự hợp tác
giữa người dùng và LLM tiếp tục định nghĩa lại khả năng trên các lĩnh vực nghiên cứu đa dạng.

Tương tác với công cụ. Tương tác với các công cụ bên ngoài cho phép LLM có thể dẫn đến đầu ra
chính xác và đáng tin cậy hơn (Peng et al., 2023; Gou et al., 2023; Qin et al., 2023a). LLM có thể được
kết nối với các Giao diện Lập trình Ứng dụng (API) thế giới thực, cho phép chúng tích cực tương tác
với các công cụ bên ngoài đa dạng (Qin et al., 2023b; Parisi et al., 2022; Schick et al., 2023a; Tang
et al., 2023; Patil et al., 2023; Song et al., 2023; Hao et al., 2023; Yuan et al., 2023). Ví dụ, LLM có thể
kết nối với (1) Internet để thu thập thông tin mới nhất (Nakano et al., 2021; Shuster et al., 2022;
Paranjape et al., 2023; Liu et al., 2023b); (2) trình thông dịch chương trình để chạy mã được tạo ra
(Chen et al., 2022; Gao et al., 2023; Drori et al., 2022; Pan et al., 2023; Wang et al., 2023a; 2024); (3)
bộ cảm nhận đa phương tiện để thu thập thông tin ngoài phương thức ngôn ngữ (Huang et al., 2023a;
Lu et al., 2023); (4) trình mô phỏng vật lý để hiểu rõ hơn về quy luật vật lý (Liu et al., 2023a).

4.2 ĐÁNH GIÁ TƯƠNG TÁC

Công trình hiện có về đánh giá tương tác chủ yếu tập trung vào một nhiệm vụ hoặc chiều cụ thể, như
hoàn thành nhiệm vụ (Liu et al., 2023c), tạo mã (Yang et al., 2023a), giải quyết nhiệm vụ hợp tác giữa
con người và LLM (Lee et al., 2022b; Huang et al., 2023b; Fu et al., 2023), thao tác công cụ (Tang et al.,
2023), và điều hướng web (Zhou et al., 2023; Deng et al., 2023a). Nghĩa là, chúng chỉ tập trung vào
tương tác với môi trường hoặc con người, thường trên một nhiệm vụ cụ thể, bỏ qua tầm quan trọng
cơ bản của cả hai yếu tố trong tương tác LLM. Khác với công trình trước đây, MINT bao gồm một
loạt các nhiệm vụ đa dạng và được thiết kế để đo lường khả năng tương tác đa lượt của LLM với cả
công cụ và phản hồi người dùng phù hợp hơn với các ứng dụng thế giới thực.

5 KẾT LUẬN

Trong công trình này, chúng tôi trình bày MINT, một benchmark đánh giá được thiết kế để đánh giá
khả năng giải quyết nhiệm vụ của LLM trong tương tác đa lượt bằng cách sử dụng công cụ và tận dụng
phản hồi ngôn ngữ tự nhiên, mà chúng tôi mô phỏng bằng GPT-4. Chúng tôi hy vọng MINT có thể
phục vụ như một tài nguyên hữu ích để giúp theo dõi tiến độ và khuyến khích nghiên cứu tương lai
trong việc cải thiện khả năng giải quyết nhiệm vụ đa lượt của LLM. Chúng tôi tham khảo §A để thảo
luận về các hạn chế và công việc tương lai.

--- TRANG 8 ---
Được xuất bản như một bài báo hội thảo tại ICLR 2024

LỜI CẢM ơN

Chúng tôi cảm ơn các nhà đánh giá ẩn danh về các gợi ý và nhận xét của họ. Nghiên cứu này dựa
trên công việc được hỗ trợ bởi Chương trình ECOLE của U.S. DARPA Số HR00112390060 và Chương
trình ITM Số FA8650-23-C-7316 và Chương trình KAIROS Số FA8750-19-2-1004. Các quan điểm và
kết luận có trong tài liệu này là của các tác giả và không nên được hiểu là nhất thiết đại diện cho
các chính sách chính thức, được thể hiện rõ ràng hoặc ngụ ý, của DARPA, hoặc Chính phủ Hoa Kỳ.
Chính phủ Hoa Kỳ được ủy quyền sao chép và phân phối bản in lại cho mục đích chính phủ bất kể
bất kỳ chú thích bản quyền nào trong đó. HP được hỗ trợ một phần bởi một món quà từ Apple.

TÀI LIỆU THAM KHẢO

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021.

Bard API. URL https://www.googlecloudcommunity.com/gc/AI-ML/
Google-Bard-API/m-p/538517/ .

ChatGPT Plugins. URL https://openai.com/blog/chatgpt-plugins .

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts
prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR,
abs/2211.12588, 2022. doi: 10.48550/arXiv.2211.12588. URL https://doi.org/10.
48550/arXiv.2211.12588 .

Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi
Wang, and Pan Lu. Theoremqa: A theorem-driven question answering dataset. arXiv preprint
arXiv:2305.12524, 2023a.

Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Dress: Instructing
large vision-language models to align and interact with humans via natural language feedback.
arXiv preprint arXiv:2311.10081, 2023b.

Claude API. URL https://docs.anthropic.com/claude/reference/
getting-started-with-the-api .

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine,
James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam
Trischler. Textworld: A learning environment for text-based games. In Tristan Cazenave,
Abdallah Saffidine, and Nathan R. Sturtevant (eds.), Computer Games -7thWorkshop, CGW
2018, Held inConjunction with the27th International Conference onArtificial Intelligence,
IJCAI 2018, Stockholm, Sweden, July 13,2018, Revised Selected Papers, volume 1017 of
Communications inComputer andInformation Science, pp. 41–75. Springer, 2018. doi: 10.1007/
978-3-030-24337-1_3. URLhttps://doi.org/10.1007/978-3-030-24337-1_3 .

Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and
Yu Su. Mind2web: Towards a generalist agent for the web. CoRR, abs/2306.06070, 2023a. doi:
10.48550/arXiv.2306.06070. URL https://doi.org/10.48550/arXiv.2306.06070 .

Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and
Yu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070,
2023b.

--- TRANG 9 ---
[Tiếp tục với phần còn lại của tài liệu tham khảo và các phần phụ lục...]

Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu,
Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and generates
university math problems by program synthesis and few-shot learning at human level. Proceedings
of the National Academy of Sciences, 119(32):e2123433119, 2022.

Ahmed Elgohary, Saghar Hosseini, and Ahmed Hassan Awadallah. Speak to your parser: Interactive text-to-SQL with natural language feedback. In Dan Jurafsky, Joyce Chai, Natalie
Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 2065–2077, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.187. URL https://aclanthology.org/
2020.acl-main.187 .

[Tiếp tục dịch phần còn lại của tài liệu...]

A HẠN CHẾ VÀ CÔNG VIỆC TƯƠNG LAI

Chúng tôi mô phỏng phản hồi ngôn ngữ tự nhiên của người dùng con người bằng GPT-4. Mặc dù cho
thấy trong một thí nghiệm con người rằng nó tương tự như phản hồi được viết bởi con người, tuy nhiên,
GPT-4 được mô phỏng có thể không bao gồm tất cả các phản hồi có thể từ người dùng thực và có thể
không mô phỏng phù hợp mọi khía cạnh của phản hồi con người, đặc biệt trong các nhiệm vụ (ví dụ,
xây dựng chính sách) liên quan đến đánh giá tinh tế về giá trị con người. Trong khi trọng tâm của
công việc chúng tôi nằm ở tương tác đa lượt trong ngữ cảnh của LLM, chúng tôi chưa khám phá tiềm
năng tận dụng trực tiếp phản hồi ngôn ngữ để huấn luyện và cải thiện mô hình tương tự như Wang
et al. (2023a), điều mà chúng tôi để lại cho công việc tương lai. Hơn nữa, các metric của chúng tôi có
thể không đánh giá đầy đủ chất lượng của quá trình tương tác ngoài kết quả. Ví dụ, các mô hình đoán
lặp đi lặp lại để có điểm số cao hơn nên bị phạt. Mặc dù nỗ lực tốt nhất của chúng tôi để đảm bảo
benchmark của chúng tôi chứa các nhiệm vụ thách thức và toàn diện, vẫn có một loạt rộng các công
cụ (Qin et al., 2023c) và các trường hợp sử dụng thế giới thực (ví dụ, duyệt web Deng et al. (2023b),
hệ điều hành Liu et al. (2023d)) mà MINT không bao gồm. Thay vì làm cho benchmark này thành một
nỗ lực một lần, chúng tôi hy vọng liên tục cải thiện benchmark này bằng cách tích hợp các nhiệm vụ
và công cụ thách thức hơn khi LLM trở nên tốt hơn.

[Tiếp tục với các phần còn lại của phụ lục...]
