# 2304.13649.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2304.13649.pdf
# Kích thước tệp: 1468531 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Một Khung Truy Xuất Mật Độ Mã Hóa Đối Xứng Kép cho
Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức Chuyên Sâu
Alireza Salemi
Đại học Massachusetts Amherst
Hoa Kỳ
asalemi@cs.umass.eduJuan Altmayer Pizzorno
Đại học Massachusetts Amherst
Hoa Kỳ
jpizzorno@cs.umass.eduHamed Zamani
Đại học Massachusetts Amherst
Hoa Kỳ
zamani@cs.umass.edu
TÓM TẮT
Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức (KI-VQA) đề cập đến việc trả lời một câu hỏi về một hình ảnh mà câu trả lời không nằm trong hình ảnh. Bài báo này trình bày một pipeline mới cho các tác vụ KI-VQA, bao gồm một bộ truy xuất và một bộ đọc. Đầu tiên, chúng tôi giới thiệu DEDR, một khung truy xuất mật độ mã hóa đối xứng kép trong đó các tài liệu và truy vấn được mã hóa vào một không gian nhúng chung sử dụng các bộ mã hóa đơn phương thức (văn bản) và đa phương thức. Chúng tôi giới thiệu một phương pháp chưng cất kiến thức lặp đi lặp lại để thu hẹp khoảng cách giữa các không gian biểu diễn trong hai bộ mã hóa này. Đánh giá toàn diện trên hai bộ dữ liệu KI-VQA đã được thiết lập, tức là OK-VQA và FVQA, cho thấy DEDR vượt trội hơn các baseline tiên tiến lần lượt 11,6% và 30,9% trên OK-VQA và FVQA. Sử dụng các đoạn văn được truy xuất bởi DEDR, chúng tôi tiếp tục giới thiệu MM-FiD, một mô hình hợp nhất-trong-bộ giải mã đa phương thức encoder-decoder, để tạo ra câu trả lời văn bản cho các tác vụ KI-VQA. MM-FiD mã hóa câu hỏi, hình ảnh và mỗi đoạn văn đã truy xuất riêng biệt và sử dụng tất cả các đoạn văn cùng nhau trong bộ giải mã của nó. So với các baseline cạnh tranh trong tài liệu, phương pháp này dẫn đến cải thiện 5,5% và 8,5% về độ chính xác trả lời câu hỏi lần lượt trên OK-VQA và FVQA.

CCS CONCEPTS
•Hệ thống thông tin →Truy xuất thông tin ;Trả lời câu hỏi ;Truy xuất đa phương tiện và đa phương thức ;•Phương pháp tính toán →Thị giác máy tính .

TỪ KHÓA
Truy Xuất Mật Độ; Chưng Cất Kiến Thức; Trả Lời Câu Hỏi Hình Ảnh; Truy Xuất Đa Phương Thức

Định dạng Tham chiếu ACM:
Alireza Salemi, Juan Altmayer Pizzorno, và Hamed Zamani. 2023. Một Khung Truy Xuất Mật Độ Mã Hóa Đối Xứng Kép cho Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức Chuyên Sâu. Trong Kỷ yếu Hội nghị Quốc tế ACM SIGIR lần thứ 46 về Nghiên cứu và Phát triển trong Truy xuất Thông tin (SIGIR '23), 23–27 tháng 7, 2023, Taipei, Đài Loan. ACM, New York, NY, USA, 11 trang.
https://doi.org/10.1145/3539618.3591629

Được phép tạo bản sao kỹ thuật số hoặc bằng giấy toàn bộ hoặc một phần công trình này cho mục đích cá nhân hoặc lớp học miễn phí với điều kiện các bản sao không được tạo hoặc phân phối để kiếm lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ ở trang đầu. Bản quyền đối với các thành phần của công trình này thuộc sở hữu của những người khác ngoài (các) tác giả phải được tôn trọng. Được phép trích dẫn với tín dụng. Để sao chép theo cách khác, hoặc tái xuất bản, để đăng trên máy chủ hoặc để phân phối lại cho danh sách, cần có sự cho phép cụ thể trước và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.
SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan.
©2023 Bản quyền thuộc về chủ sở hữu/tác giả. Quyền xuất bản được cấp phép cho ACM.
ACM ISBN 978-1-4503-9408-6/23/07.
https://doi.org/10.1145/3539618.3591629

Hình 1: Một ví dụ câu hỏi KI-VQA. Trả lời những câu hỏi này đòi hỏi kiến thức bên ngoài.
Hình ảnh ©zrim, https://www.flickr.com/photos/zrimshots/2788695458

1 GIỚI THIỆU
Trả lời câu hỏi hình ảnh đòi hỏi kiến thức¹(KI-VQA) là một biến thể của các tác vụ trả lời câu hỏi hình ảnh mà các câu hỏi không thể được trả lời chỉ bằng hình ảnh. Do đó, việc truy cập các nguồn kiến thức bên ngoài là cần thiết để trả lời những câu hỏi này. KI-VQA có một số lượng lớn các ứng dụng thực tế. Hãy tưởng tượng khách hàng của các trang web thương mại điện tử chụp ảnh một sản phẩm hoặc một phần của sản phẩm và đặt câu hỏi về nó. Trong bối cảnh giáo dục, sinh viên có thể đặt câu hỏi về một hình ảnh trong sách giáo khoa của họ. Người dùng có thể chụp ảnh một biển báo hình ảnh hoặc một tác phẩm nghệ thuật và đặt câu hỏi về ý nghĩa hoặc lịch sử của nó. Đây chỉ là một vài ví dụ về các ứng dụng KI-VQA. Hình 1 cho thấy một ví dụ về các tác vụ KI-VQA: hình ảnh đủ để xác định "động vật" là hươu cao cổ, và có thể cả phân loài, nhưng không đủ để trả lời câu hỏi chúng cao đến mức nào.

Phần lớn công trình trước đây về KI-VQA, như [12,14,37,50,57], giả định rằng kiến thức bên ngoài có thể được lấy từ một cơ sở kiến thức có cấu trúc. Tuy nhiên, một cơ sở kiến thức chất lượng cao và đầy đủ có thể không có sẵn cho một số lĩnh vực [1]. Bên cạnh đó, việc duy trì các cơ sở kiến thức với thông tin cập nhật là thách thức [54]. Để ngăn chặn những vấn đề này, theo Qu et al. [40], chúng tôi có một phương pháp tiếp cận thay thế cho KI-VQA: sử dụng một kho văn bản lớn làm nguồn kiến thức bên ngoài. Trong thiết lập này, một pipeline hai giai đoạn cho các hệ thống KI-VQA là đầu tiên truy xuất một danh sách các đoạn văn cho một cặp câu hỏi-hình ảnh đã cho và sau đó xử lý các đoạn văn đã truy xuất để tạo ra một câu trả lời.² Pipeline này được mô tả trong Hình 2.

Hiệu suất hiệu quả của các mô hình truy xuất mật độ trong các tác vụ truy xuất thông tin khác nhau [23,62,63] và tính linh hoạt mở rộng của chúng cho đầu vào đa phương thức³ đã thúc đẩy chúng tôi tập trung vào truy xuất mật độ để thực hiện giai đoạn đầu tiên của pipeline KI-VQA (tức là truy xuất đoạn văn). Một thuộc tính của tác vụ truy xuất này là nó xử lý các phương thức đầu vào bất đối xứng: nhu cầu thông tin của người dùng là đa phương thức (cặp câu hỏi-hình ảnh) trong khi các mục thông tin (đoạn văn) là đơn phương thức. Kết quả của thuộc tính này, Qu et al. [40] gần đây đã chỉ ra rằng một mô hình truy xuất mật độ KI-VQA sử dụng một bộ mã hóa đa phương thức để biểu diễn cặp câu hỏi-hình ảnh và một bộ mã hóa văn bản để biểu diễn các đoạn văn trong bộ sưu tập dẫn đến hiệu suất truy xuất đoạn văn tiên tiến. Chúng tôi lập luận rằng việc sử dụng kiến trúc bi-encoder bất đối xứng như vậy là không tối ưu, vì các bộ mã hóa tạo ra đầu ra trong các không gian ngữ nghĩa khác nhau và việc tinh chỉnh các bộ mã hóa không thể luôn đóng khoảng cách này. Chúng tôi đầu tiên nghiên cứu hai phương án thay thế để phát triển các mô hình truy xuất mật độ đối xứng:⁴(1) tạo ra một biểu diễn văn bản của hình ảnh và sử dụng kiến trúc bi-encoder đơn phương thức đối xứng cho truy xuất mật độ, và (2) chuyển đổi các đoạn văn thành định dạng đầu vào đa phương thức và sử dụng kiến trúc bi-encoder đa phương thức đối xứng. Chúng tôi quan sát thấy rằng cả hai phương án thay thế đều gặp phải mất mát thông tin, nhưng cũng cho thấy chúng tạo ra các biểu diễn bổ sung. Quan sát này thúc đẩy chúng tôi không chỉ kết hợp hai mã hóa này, mà còn chuyển giao kiến thức giữa chúng. Chi tiết hơn, chúng tôi đề xuất một phương pháp chưng cất kiến thức lặp đi lặp lại để chuyển giao kiến thức giữa hai mô hình truy xuất mật độ đối xứng thay thế này.

Phương pháp mã hóa đối xứng kép được đề xuất dẫn đến cải thiện MRR 11,6% và 30,9% so với baseline tiên tiến lần lượt trên các tập thử nghiệm OK-VQA [38] và FVQA [56].

Đối với giai đoạn thứ hai của pipeline, không giống như nhiều công trình trước đây về phát hiện khoảng trả lời cho KI-VQA [12,14,37,50,57] (tức là trích xuất câu trả lời từ các đoạn văn đã truy xuất), chúng tôi tập trung vào việc tạo ra câu trả lời tự động hồi quy được tăng cường bằng truy xuất. Chúng tôi đề xuất MM-FiD, một phần mở rộng đơn giản nhưng hiệu quả của kiến trúc Fusion-in-Decoder (FiD) [20] cho đầu vào đa phương thức. FiD là một mô hình tạo văn bản được tăng cường bằng truy xuất đã gần đây cho thấy hiệu suất hiệu quả trong các tác vụ trả lời câu hỏi [20]. MM-FiD sử dụng một bộ mã hóa đa phương thức để biểu diễn câu hỏi, hình ảnh và các đoạn văn đã truy xuất và sử dụng một bộ giải mã đơn phương thức tạo ra câu trả lời và được huấn luyện bằng mục tiêu khả năng tương tự tối đa. Các thí nghiệm toàn diện trên cả hai bộ dữ liệu OK-VQA và FVQA chứng minh rằng MM-FiD vượt trội đáng kể so với các phương pháp thay thế cho việc tạo ra câu trả lời. Nó cũng hoạt động tốt hơn so với các baseline phát hiện khoảng trả lời. Chi tiết hơn, pipeline end-to-end của chúng tôi đạt được cải thiện 5,5% và 8,5% so với các baseline lần lượt trên các tác vụ trả lời câu hỏi OK-VQA và FVQA. Chúng tôi mở nguồn mã của chúng tôi và phát hành các tham số mô hình đã học để phục vụ mục đích nghiên cứu⁵.

2 CÔNG TRÌNH LIÊN QUAN
Truy Xuất Mật Độ (Đa Phương Thức). Việc sử dụng các vectơ mật độ để truy xuất các tài liệu văn bản liên quan đến một truy vấn văn bản đã được nghiên cứu từ khi xuất hiện Phân tích Ngữ nghĩa Tiềm ẩn [7]. Tuy nhiên, hiệu suất của các bộ truy xuất mật độ vẫn kém hơn so với các bộ truy xuất thưa thớt như BM25 cho đến khi Karpukhin et al. [23] ra đời với Dense Passage Retriever (DPR), sử dụng đầu ra token [CLS] bởi BERT [8], một mô hình ngôn ngữ được huấn luyện trước. Trong khi nhiều bộ truy xuất mật độ chỉ sử dụng một vectơ duy nhất để biểu diễn truy vấn và tài liệu [23,41,59], việc sử dụng nhiều vectơ cho mỗi tài liệu và truy vấn cũng đã được nghiên cứu [11, 19, 24, 36, 49].

Truy xuất mật độ đa phương thức gần đây đã được điều tra dưới các hình thức khác nhau: (1) truy vấn đơn phương thức và tài liệu đa phương thức [15,34,52], (2) truy vấn đa phương thức và tài liệu đơn phương thức [40], (3) truy vấn đa phương thức và tài liệu đa phương thức [51], và (4) truy vấn đơn phương thức và tài liệu đơn phương thức với truy vấn và tài liệu từ các phương thức khác nhau, tức là truy xuất chéo phương thức [21, 42].

Trong công trình này, chúng tôi tập trung vào trường hợp thứ hai, nơi truy vấn là đa phương thức trong khi các tài liệu chỉ chứa văn bản. Qu et al. [40] đã sử dụng một bi-encoder bất đối xứng với LXMERT [53], một mô hình ngôn ngữ thị giác được huấn luyện trước dựa trên BERT [8] để mã hóa truy vấn, và chính BERT để mã hóa tài liệu. Như chúng tôi chỉ ra, kiến trúc bất đối xứng như vậy là không tối ưu; việc sử dụng các bộ mã hóa khác nhau tạo ra một "khoảng cách" ngữ nghĩa trong không gian nhúng và việc tinh chỉnh không thể dễ dàng vượt qua vấn đề này. Thay vào đó, chúng tôi đề xuất một khung mã hóa đối xứng kép mới giải quyết vấn đề này.

Chưng Cất Kiến Thức cho Truy Xuất Đoạn Văn Mật Độ. Do số lượng lớn các tham số có thể học trong các bộ truy xuất đoạn văn mật độ, đôi khi các bộ dữ liệu có sẵn không đủ để huấn luyện chúng [62]. Do đó, chưng cất kiến thức, trong đó một mô hình giáo viên cung cấp nhãn cho một mô hình học sinh, đã trở thành một phương pháp tiêu chuẩn để huấn luyện các mô hình truy xuất mật độ và đã cho thấy kết quả thuyết phục [17,30]. Công trình hiện có trong lĩnh vực này thường sử dụng các bộ xếp hạng lại cross-encoder đầu vào cả truy vấn và tài liệu làm mô hình giáo viên cho các mô hình truy xuất mật độ với kiến trúc bi-encoder [44]. Một phương pháp khác là chưng cất kiến thức từ các mô hình truy xuất mật độ đa vectơ, như ColBERT [24] sang các bộ truy xuất mật độ đơn vectơ [63].

Trong các thí nghiệm của chúng tôi, chúng tôi không thấy chưng cất kiến thức từ các bộ xếp hạng lại cross-encoder hữu ích cho các bộ dữ liệu KI-VQA. Do đó, chúng tôi giới thiệu một phương pháp mới: chưng cất kiến thức lặp đi lặp lại cho mã hóa kép, trong đó chưng cất kiến thức diễn ra lặp đi lặp lại giữa hai bi-encoder sử dụng các phương thức khác nhau trong đầu vào của chúng. Theo đó, mỗi mô hình học hỏi quan điểm của mô hình khác, điều chỉnh không gian biểu diễn của chúng để truy xuất mật độ hiệu quả hơn.

Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức. Đòi hỏi kiến thức đề cập đến một danh mục các vấn đề học máy được tăng cường bằng truy xuất [61] mà đầu vào không đủ để tạo ra đầu ra và thông tin bên ngoài cần được cung cấp. KILT [39] là một benchmark cho các tác vụ ngôn ngữ tự nhiên đòi hỏi kiến thức như trả lời câu hỏi miền mở, kiểm tra sự thật, liên kết thực thể, điền slot và đối thoại dựa trên kiến thức. Tất cả các tác vụ đã đề cập đều là các tác vụ đòi hỏi kiến thức chỉ có văn bản. Theo hiểu biết của chúng tôi, không có benchmark thống nhất nào về các tác vụ đa phương thức đòi hỏi kiến thức, điều này tương đối ít được khám phá. Do đó, bài báo này tập trung vào trả lời câu hỏi hình ảnh đòi hỏi kiến thức.

Trả lời câu hỏi hình ảnh (VQA) là một tác vụ trả lời câu hỏi đa phương thức có mục tiêu trả lời một câu hỏi ngôn ngữ tự nhiên về một hình ảnh [3]. VQA chủ yếu được thiết kế để đo lường khả năng của các mô hình trong việc biểu diễn và hiểu dữ liệu đa phương thức. Do đó, các câu hỏi trong VQA thường liên quan đến các đặc điểm hình ảnh (ví dụ: màu sắc hoặc hình dạng của các đối tượng) và đôi khi yêu cầu kiến thức thông thường. Nói cách khác, con người có thể trả lời các câu hỏi VQA chỉ bằng cách nhìn vào hình ảnh, mà không cần truy cập thông tin bên ngoài.

Với công thức này, VQA có các trường hợp sử dụng thực tế hạn chế. Trái ngược với VQA, trả lời câu hỏi hình ảnh đòi hỏi kiến thức

--- TRANG 2 ---
SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan. Alireza Salemi, Juan Altmayer Pizzorno, và Hamed Zamani

là tác vụ trả lời một câu hỏi về một hình ảnh cần một phần thông tin bên ngoài không có sẵn trong hình ảnh để trả lời các câu hỏi. Trả Lời Câu Hỏi Hình Ảnh Dựa Trên Sự Thật (FVQA) [56] là một bộ dữ liệu trả lời câu hỏi hình ảnh trong đó việc trả lời các câu hỏi về một hình ảnh cần mô hình xem xét một sự thật liên quan. Thay vào đó, trả lời câu hỏi hình ảnh kiến thức bên ngoài (OK-VQA) [38] là một bộ dữ liệu tương tự như FVQA, nhưng kiến thức cần thiết không bị giới hạn bởi các sự thật.

Các phương pháp hiện tại cho OK-VQA sử dụng các chiến lược khác nhau để giải quyết vấn đề: một số dựa vào kiến thức ẩn được lưu trữ trong các mô hình ngôn ngữ hoặc thị giác-ngôn ngữ để trả lời các câu hỏi mà không sử dụng bất kỳ nguồn kiến thức bên ngoài nào [48,60], trong khi những cái khác sử dụng các nguồn kiến thức bên ngoài cho mục đích này ngoài kiến thức ẩn [12–14,37,57]. Việc sử dụng OCR và nhãn đối tượng mật độ cũng có thể hiệu quả cho tác vụ này [10,32], nhưng nằm ngoài phạm vi của bài báo này. Để sử dụng một nguồn kiến thức rõ ràng, cần thiết phải thiết kế một bộ truy xuất truy xuất một tập nhỏ các đoạn văn liên quan đến hình ảnh và câu hỏi [40] và một bộ đọc chọn hoặc tạo ra phản hồi từ các đoạn văn đã truy xuất [13,14,35]. Bài báo này đề xuất các giải pháp hiệu quả cho cả hai bước này.

3 PHÁT BIỂU VẤN ĐỀ
Trong trả lời câu hỏi hình ảnh đòi hỏi kiến thức, người dùng đặt một câu hỏi ngôn ngữ tự nhiên về một hình ảnh, điều này đòi hỏi truy cập thông tin bên ngoài. Nói cách khác, câu trả lời cho câu hỏi không tồn tại trong hình ảnh, điều này cần thiết phải sử dụng các tài nguyên bên ngoài. Xem Hình 1 để biết ví dụ về các tác vụ KI-VQA. Những tài nguyên này có thể ở nhiều hình thức khác nhau, từ các cơ sở kiến thức có cấu trúc và bán cấu trúc đến văn bản không có cấu trúc được truy xuất từ web. Trong bài báo này, chúng tôi xem xét một kịch bản trong đó câu trả lời nên được truy xuất và trích xuất từ một bộ sưu tập các đoạn văn ngôn ngữ tự nhiên không có cấu trúc. Sau đây, chúng tôi chỉ định cụ thể hơn tác vụ KI-VQA được nghiên cứu trong bài báo này.

Gọi 𝑇={(𝑄1,𝐼1,𝐴1,𝑅1),(𝑄2,𝐼2,𝐴2,𝑅2),···,(𝑄𝑛,𝐼𝑛,𝐴𝑛,𝑅𝑛)} là tập huấn luyện cho một tác vụ KI-VQA. Mỗi instance huấn luyện bao gồm một câu hỏi ngôn ngữ tự nhiên 𝑄𝑖, một hình ảnh 𝐼𝑖, một tập các câu trả lời văn bản ngắn 𝐴𝑖, và một tập các đoạn văn liên quan 𝑅𝑖. Điều đó có nghĩa là mỗi câu hỏi có thể có nhiều câu trả lời trong tập huấn luyện (tức là |𝐴𝑖|≥1), thường giống nhau về mặt ngữ nghĩa nhưng khác nhau về mặt cú pháp. Tương tự, có thể tồn tại nhiều đoạn văn liên quan đến câu hỏi (tức là |𝑅𝑖|≥1). Tất cả các đoạn văn liên quan được chọn và chú thích từ một bộ sưu tập quy mô lớn C. Do đó, 𝑅𝑖⊆C:∀1≤𝑖≤𝑛. Chúng tôi nghiên cứu hai tác vụ liên quan sau:

Truy Xuất Đoạn Văn cho KI-VQA: tác vụ truy xuất là sử dụng tập huấn luyện 𝑇 để huấn luyện một bộ truy xuất truy xuất các đoạn văn liên quan từ bộ sưu tập C cho một cặp câu hỏi-hình ảnh đã cho (𝑄,𝐼).

Tạo Câu Trả Lời Được Tăng Cường Bằng Truy Xuất cho KI-VQA: tác vụ tạo câu trả lời được tăng cường bằng truy xuất là tạo ra một câu trả lời văn bản ngắn cho bất kỳ cặp câu hỏi-hình ảnh chưa thấy nào (𝑄,𝐼) bằng cách có quyền truy cập vào bộ sưu tập C. Do đó, các mô hình trong tác vụ này tự nhiên truy xuất các đoạn văn từ C và sử dụng chúng để tạo ra câu trả lời.

Chúng tôi đầu tiên đề xuất một kiến trúc mã hóa đối xứng kép cho truy xuất mật độ trong KI-VQA và sau đó giới thiệu một mô hình hợp nhất-trong-bộ giải mã đa phương thức như một phương pháp tạo câu trả lời được tăng cường bằng truy xuất.

4 DEDR: KHUNG BỘ TRUY XUẤT MẬT ĐỘ MÃ HÓA KÉP
Hình 2 mô tả một pipeline cho các tác vụ trả lời câu hỏi hình ảnh đòi hỏi kiến thức. Như được hiển thị trong pipeline, đầu vào cho mô hình truy xuất mật độ là bất đối xứng – bộ mã hóa truy vấn nhận đầu vào đa phương thức (tức là một câu hỏi và một hình ảnh), trong khi bộ mã hóa đoạn văn nhận đầu vào văn bản đơn phương thức (tức là một đoạn văn từ C). Thuộc tính bất đối xứng này trong các phương thức đầu vào khiến việc thiết kế một mô hình truy xuất mật độ đối xứng hiệu quả trở nên thách thức. Đây là lý do tại sao mô hình truy xuất mật độ tiên tiến hiện tại được đề xuất bởi Qu et al. [40] sử dụng một kiến trúc bất đối xứng, trong đó một mô hình ngôn ngữ đa phương thức được huấn luyện trước (tức là LXMERT [53]) được sử dụng để mã hóa truy vấn và một mô hình ngôn ngữ đơn phương thức được huấn luyện trước (tức là BERT [8]) được sử dụng để mã hóa tài liệu. Vì các kiến trúc bất đối xứng như vậy bắt đầu từ các không gian nhúng cơ bản khác nhau, chúng gặp phải tốc độ hội tụ chậm và hiệu suất truy xuất mật độ không tối ưu. Ngược lại, nghiên cứu rộng rãi về truy xuất mật độ cho dữ liệu đơn phương thức (truy vấn và tài liệu văn bản) cho thấy rằng các kiến trúc đối xứng dẫn đến hiệu suất tốt hơn đáng kể. Các mô hình truy xuất đoạn văn mật độ tiên tiến, như TAS-B [17], ColBERT [24,49], RocketQA [41,44], và CLDRD [62], sử dụng các kiến trúc đối xứng. Được thúc đẩy bởi quan sát này, mục tiêu của chúng tôi là học một mô hình truy xuất mật độ đối xứng cho các tác vụ KI-VQA.

Để đạt được mục tiêu này, chúng tôi nghiên cứu hai giải pháp thay thế. Đầu tiên, chúng tôi chuyển đổi tất cả đầu vào mô hình sang dạng văn bản đơn phương thức và sau đó sử dụng các mô hình ngôn ngữ đơn phương thức cho cả mã hóa truy vấn và tài liệu (Phần 4.1). Thứ hai, chúng tôi chuyển đổi tất cả đầu vào sang cùng dạng đa phương thức (văn bản và hình ảnh) và sau đó sử dụng các mô hình ngôn ngữ đa phương thức cho cả hai bộ mã hóa (Phần 4.2). Chúng tôi giả thuyết rằng hai mô hình này học các biểu diễn bổ sung vì các lý do sau: (1) chúng nhận các định dạng đầu vào khác nhau, và (2) quá trình và dữ liệu huấn luyện trước trong các mô hình ngôn ngữ đơn phương thức và đa phương thức là khác nhau. Kết quả thí nghiệm của chúng tôi cũng xác thực giả thuyết này (xem Phần 6.3). Theo quan sát này, chúng tôi đề xuất một phương pháp chưng cất kiến thức lặp đi lặp lại xen kẽ giữa hai phương pháp mã hóa này như các mô hình giáo viên và học sinh. Cuối cùng, bằng cách kết hợp hai phương pháp mã hóa này, DEDR học một bộ mã hóa đối xứng kép đơn phương thức/đa phương thức cho cả truy vấn và tài liệu.

4.1 Mã Hóa Đơn Phương Thức Thống Nhất
Để sử dụng một bộ mã hóa đơn phương thức (văn bản) chung để biểu diễn cả truy vấn và đoạn văn, chúng ta cần chuyển đổi hình ảnh trong truy vấn thành văn bản. Mô hình này có thể được công thức hóa như sau:

𝑆𝑇((𝑄,𝐼),𝑃)=𝐸𝑇(concat(𝑄,𝜙𝐼→𝑇(𝐼)))·𝐸𝑇(𝑃) (1)

trong đó · biểu thị tích vô hướng giữa hai vectơ, 𝐸𝑇 là một bộ mã hóa văn bản đơn phương thức, và 𝜙𝐼→𝑇 là một module chuyển đổi phương thức nhận một hình ảnh và tạo ra một mô tả văn bản cho nó.

Có một số phương pháp để thực hiện bộ chuyển đổi phương thức 𝜙𝐼→𝑇. Một phương pháp là tạo ra tên của các đối tượng có trong hình ảnh bằng các phương pháp phát hiện đối tượng. Chúng tôi có một phương pháp thay thế bằng cách sử dụng các mục tiêu chú thích hình ảnh để huấn luyện mô hình chuyển đổi phương thức 𝜙𝐼→𝑇. Lý do là các phương pháp chú thích hình ảnh tạo ra các mô tả mở về hình ảnh, trái ngược với các danh mục được xác định trước trong các mô hình phát hiện đối tượng. Ngoài ra, việc thu thập dữ liệu huấn luyện cho các mô hình chú thích hình ảnh là rẻ, với sự có sẵn của hình ảnh quy mô lớn với chú thích trên web. Chi tiết hơn, chúng tôi sử dụng kiến trúc ExpansionNet v2 [18] để thực hiện 𝜙𝐼→𝑇. Expansion V2 là một kiến trúc encoder-decoder được thiết kế trên Swin-Transformer [33] được mở rộng bởi Block Dynamic and Static Expansion [18] và multi-head attention [55]. Mô hình đầu tiên được huấn luyện trước bằng hình ảnh và chú thích từ bộ dữ liệu Microsoft COCO [31]. Sau đó, tối ưu hóa tự phê bình [46] được thực hiện để hoàn thành việc huấn luyện mô hình. Khi mô hình được huấn luyện để tạo ra các mô tả văn bản của hình ảnh, chúng tôi đóng băng các tham số của ExpansionNet v2 và chỉ tối ưu hóa bộ mã hóa văn bản 𝐸𝑇. Điều này giảm đáng kể số lượng tham số cần được học bằng tập huấn luyện KI-VQA.

Để thực hiện bộ mã hóa văn bản 𝐸𝑇, có nhiều mô hình ngôn ngữ có sẵn. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng BERT-base [8] và biểu diễn liên quan đến token [CLS] được coi là đầu ra của bộ mã hóa 𝐸𝑇. Lưu ý rằng trong Phương trình (1), các bộ mã hóa truy vấn và đoạn văn là giống nhau và chúng sử dụng các tham số chung, đảm bảo một kiến trúc đối xứng cho truy xuất mật độ.

4.2 Mã Hóa Đa Phương Thức Thống Nhất
Mặc dù việc sử dụng một bộ mã hóa đa phương thức cho cả mã hóa truy vấn và đoạn văn có vẻ đơn giản, hầu hết các mô hình ngôn ngữ đa phương thức không chấp nhận đầu vào chỉ có văn bản. Do đó, chúng ta cần phát triển một kỹ thuật để lấp đầy khoảng cách phương thức này. Phương pháp mã hóa đa phương thức thống nhất của chúng tôi có thể được công thức hóa như sau:

𝑆𝑀𝑀((𝑄,𝐼),𝑃)=𝐸𝑀𝑀(𝑄,𝐼)·𝐸𝑀𝑀(𝑃,𝐼[MASKED]) (2)

trong đó 𝐸𝑀𝑀 là một bộ mã hóa đa phương thức được huấn luyện trước biểu diễn một cặp văn bản và hình ảnh làm đầu vào. Để giải quyết khoảng cách phương thức giữa phía truy vấn và đoạn văn, chúng tôi sử dụng một mô hình ngôn ngữ đa phương thức đã sử dụng kỹ thuật che dấu ở phía hình ảnh trong quá trình huấn luyện trước. Chi tiết hơn, chúng tôi sử dụng LXMERT [53] sử dụng Faster R-CNN [45] để nhận dạng 36 đối tượng trong hình ảnh đã cho và tạo ra biểu diễn và hộp bao quanh của chúng. Sau đó, thông tin này về các đối tượng ngoài các token văn bản được đưa vào một mạng transformer dual-encoder với một bộ mã hóa chéo phương thức ở trên. Cuối cùng, token [CLS] được sử dụng để biểu diễn toàn bộ đầu vào. Vì LXMERT đã được huấn luyện trước với các mục tiêu huấn luyện trước khác nhau, bao gồm Dự đoán Đối tượng Bị Che, nó phù hợp hoàn hảo cho việc học biểu diễn đa phương thức thống nhất của chúng tôi.

Để vượt qua vấn đề đã đề cập với việc mã hóa dữ liệu chỉ có văn bản cho biểu diễn đoạn văn, chúng tôi đề xuất một kỹ thuật đơn giản nhưng hiệu quả mà chúng tôi gọi là Mở Rộng Đoạn Văn sử dụng Biểu Diễn Hình Ảnh Bị Che (PEMIR). Trong kỹ thuật này, chúng tôi đưa một đoạn văn vào LXMERT làm đầu vào văn bản và số không (bị che) làm đầu vào hình ảnh với các hộp bao quanh của [0.0,0.0,1.0,1.0]. Đầu vào hình ảnh này có nghĩa là 36 đối tượng bị che với các hộp bao quanh của toàn bộ hình ảnh. Theo trực giác, chúng tôi yêu cầu mô hình tạo ra một biểu diễn cho đoạn văn đầu vào trong khi cố gắng tạo ra các biểu diễn đối tượng hình ảnh tốt nhất dựa trên dữ liệu đầu vào văn bản. Do đó, điều này gần như tương đương với việc mở rộng đoạn văn với biểu diễn hình ảnh dựa trên nội dung đoạn văn.

Sử dụng phương pháp này, chúng ta có thể tạo ra biểu diễn cho truy vấn và tài liệu bằng cùng một bộ mã hóa đa phương thức với các tham số chung. Điều này có lợi vì nó giúp kiến trúc dual encoder bắt đầu từ cùng một không gian nhúng chung. Ngoài ra, chúng ta có thể chỉ sử dụng một bộ mã hóa duy nhất làm cả bộ mã hóa truy vấn và tài liệu, dẫn đến việc giảm số lượng tham số của mô hình.

4.3 Tối Ưu Hóa Mã Hóa Kép thông qua Chưng Cất Kiến Thức Lặp Đi Lặp Lại
𝐸𝑇 và 𝐸𝑀𝑀 biểu diễn các đầu vào KI-VQA từ các góc độ khác nhau; 𝐸𝑇 chỉ xem xét biểu diễn văn bản của các đầu vào, trong khi 𝐸𝑀𝑀 xem xét các biểu diễn đa phương thức của chúng. Những mô hình này cũng sử dụng dữ liệu và mục tiêu huấn luyện trước độc đáo. Do đó, chúng tôi giả thuyết rằng hai mô hình học biểu diễn này cung cấp thông tin bổ sung. Phân tích thực nghiệm của chúng tôi xác thực

--- TRANG 3 ---
Một Khung Truy Xuất Mật Độ Mã Hóa Đối Xứng Kép cho Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức Chuyên Sâu SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan.

Hình 3: Quy trình huấn luyện và suy luận trong khung DEDR. DEDR đầu tiên huấn luyện các bộ mã hóa đơn phương thức và đa phương thức riêng biệt (trái), sau đó sử dụng chưng cất kiến thức lặp đi lặp lại để điều chỉnh cả hai không gian biểu diễn (giữa). Khi suy luận, DEDR sử dụng sự tổng hợp của cả hai mã hóa để xây dựng một bộ truy xuất mật độ mã hóa đối xứng kép (phải).

giả thuyết này, xem Hình 5. Với quan sát này, chúng tôi đề xuất sử dụng một phương pháp chưng cất kiến thức để cải thiện cả hai mô hình này.

Huấn Luyện Riêng Biệt các Bộ Mã Hóa. Chúng tôi đầu tiên tối ưu hóa các tham số của hai mô hình này riêng biệt bằng tập huấn luyện truy xuất cho mỗi tác vụ KI-VQA. Theo DPR [23], chúng tôi sử dụng một hàm mất mát đối lập dựa trên cross-entropy để huấn luyện các mô hình:

𝐿isolated =−log𝑒𝑆𝑋((𝑄,𝐼),𝑃𝑝𝑜𝑠)
𝑒𝑆𝑋((𝑄,𝐼),𝑃𝑝𝑜𝑠)+∑︁
𝑃′∈Pneg𝑒𝑆𝑋((𝑄,𝐼),𝑃′)(3)

trong đó 𝑋∈{𝑇,𝑀𝑀} là phương pháp mã hóa được sử dụng để tạo điểm số, và 𝑃𝑝𝑜𝑠 là một đoạn văn tích cực (liên quan) và Pneg là một tập các đoạn văn tiêu cực cho cặp câu hỏi-hình ảnh (𝑄,𝐼). Để xây dựng Pneg, chúng tôi sử dụng một phương pháp lấy mẫu tiêu cực khó được sử dụng trong [40] ngoài các tiêu cực trong batch – trong đó tất cả các tài liệu tích cực và tiêu cực của các truy vấn khác trong batch huấn luyện được coi là các tài liệu tiêu cực đối với truy vấn.

Chưng Cất Kiến Thức Lặp Đi Lặp Lại giữa các Bộ Mã Hóa. Để điều chỉnh không gian biểu diễn trong 𝐸𝑇 và 𝐸𝑀𝑀 và cải thiện khả năng tổng quát của chúng, chúng tôi thiết kế một phương pháp chưng cất kiến thức lặp đi lặp lại. Trong phương pháp này, chúng tôi đầu tiên sử dụng bộ mã hóa hiệu quả hơn, dựa trên hiệu suất của tập xác thực, làm giáo viên và bộ mã hóa khác làm học sinh. Sau đó, chúng tôi huấn luyện học sinh bằng các điểm số được cung cấp bởi giáo viên. Chúng tôi sử dụng hàm mất mát KL-divergence listwise sau:

𝐿IKD=−∑︂
𝑃′∈Pneg∪{𝑃𝑝𝑜𝑠}𝑆′
𝑌((𝑄,𝐼),𝑃′)log𝑆′
𝑋((𝑄,𝐼),𝑃′)
𝑆′
𝑌((𝑄,𝐼),𝑃′)(4)

trong đó 𝑋 và 𝑌 lần lượt biểu thị mô hình học sinh và giáo viên. 𝑆′((𝑄,𝐼),𝑃′) là điểm số được chuẩn hóa của 𝑆((𝑄,𝐼),𝑃′) cho 𝑃′∈Pneg∪{𝑃𝑝𝑜𝑠} được tạo ra bởi mô hình học sinh hoặc giáo viên bằng hàm softmax. Tương tự như Phương trình (3), 𝑃𝑝𝑜𝑠 là một đoạn văn tích cực, và cùng phương pháp được sử dụng cho việc lấy mẫu tiêu cực. Chúng tôi tiếp tục huấn luyện học sinh bằng điểm số của giáo viên cho đến khi tiêu chí dừng được đáp ứng: hoặc ngân sách tính toán kết thúc (tức là nó đạt đến số epoch tối đa được đặt trong thí nghiệm) hoặc dừng sớm dựa trên hiệu suất xác thực.

Trong vòng chưng cất tiếp theo, chúng tôi hoán đổi giáo viên và học sinh. Nói cách khác, học sinh của vòng trước đóng vai trò là giáo viên trong vòng này để cung cấp điểm số cho việc huấn luyện giáo viên trước đây trong vòng này. Phương pháp lặp đi lặp lại này đối với chưng cất kiến thức hữu ích vì, trong mỗi vòng, mô hình học sinh học được quan điểm của mô hình giáo viên trong việc chấm điểm tài liệu, đặc biệt khi hai mô hình này dựa vào hai không gian nhúng khác nhau để tạo điểm số. Chúng tôi tiếp tục quá trình chưng cất lặp đi lặp lại này và sử dụng dừng sớm dựa trên hiệu suất xác thực để kết thúc.

4.4 Suy Luận Truy Xuất sử dụng Mã Hóa Kép
Như chúng tôi đã đề cập trước đó, 𝐸𝑇 và 𝐸𝑀𝑀, được giới thiệu trong các phần trước, truy xuất các đoạn văn để đáp ứng một truy vấn đa phương thức bằng các không gian nhúng riêng biệt. Cái trước tập trung vào không gian nhúng văn bản, trong khi cái sau mã hóa truy vấn và đoạn văn vào một không gian nhúng đa phương thức. Một ý tưởng để kết hợp hai mô hình này trong một không gian nhúng duy nhất là nối biểu diễn của mỗi mô hình cho các đầu vào của nó lại với nhau. Nếu mỗi bộ mã hóa này tạo ra một mã hóa 𝑑-chiều cho mỗi đầu vào, không gian nhúng cuối cùng bao gồm 2𝑑 chiều.

Có hai phương pháp để sử dụng không gian nhúng kết hợp này: (1) chúng ta có thể sử dụng biểu diễn được nối của các mô hình để huấn luyện một bộ xếp hạng mới từ đầu bằng hàm mất mát trong Phương trình 3, và (2) chúng ta có thể sử dụng các bộ xếp hạng tốt nhất của mỗi loại sau chưng cất kiến thức và kết hợp biểu diễn của chúng mà không cần huấn luyện thêm để tạo ra biểu diễn cho truy vấn và đoạn văn. Cái sau không cần huấn luyện vì mỗi mô hình đã được huấn luyện trước đó. Chúng tôi chỉ kết hợp biểu diễn của chúng để lập chỉ mục các nhúng bằng Faiss [22] và tìm kiếm chỉ mục để truy xuất đoạn văn.

5 HỢP NHẤT ĐA PHƯƠNG THỨC TRONG BỘ GIẢI MÃ
Fusion-in-Decoder (FiD) [20] là một mô hình encoder-decoder tạo sinh tiên tiến dựa trên T5 [43]. Nó được thiết kế để tổng hợp kiến thức qua nhiều đoạn văn và tạo ra một phản hồi duy nhất dựa trên chúng. Nó đã tạo ra hiệu suất mạnh mẽ trên một loạt rộng các tác vụ ngôn ngữ đòi hỏi kiến thức (KILTs). Mô hình tiên tiến hiện tại trên sáu benchmark từ bảng xếp hạng KILT⁶ dựa trên một biến thể nhẹ của FiD [16]. Chúng tôi mở rộng kiến trúc FiD cho dữ liệu đa phương thức và đề xuất MM-FiD.

Để hình thức hóa tác vụ, đối với một truy vấn bao gồm một hình ảnh 𝐼 và một câu hỏi 𝑄, chúng tôi giả định một tập 𝑃={𝑝1,...,𝑝𝑛} bao gồm 𝑛 đoạn văn hỗ trợ được cung cấp (ví dụ: bởi một mô hình truy xuất). Mục tiêu của fusion-in-decoder đa phương thức (MM-FiD) là tạo ra một câu trả lời văn bản cho 𝑄 về 𝐼 bằng cách xem xét tất cả các đoạn văn trong 𝑃.

Chúng tôi sử dụng kiến trúc VL-T5 [6], một mô hình thị giác-ngôn ngữ tạo sinh văn bản đa phương thức được huấn luyện trước với các tác vụ tạo văn bản khác nhau, làm điểm khởi đầu để thiết kế kiến trúc fusion-in-decoder đa phương thức. VL-T5 nhận một đoạn văn bản và các đặc trưng của các đối tượng hình ảnh được phát hiện bởi Faster R-CNN [45] làm đầu vào và tạo ra một đoạn văn bản làm đầu ra. Giải pháp đơn giản nhất cho công thức vấn đề đã đề cập trong phần này là đưa VL-T5 với sự nối kết của câu hỏi, hình ảnh và mỗi đoạn văn để tạo ra một câu trả lời dựa trên mỗi tài liệu hỗ trợ; tuy nhiên, việc tổng hợp và giảm thiểu các câu trả lời được tạo ra cho mỗi tài liệu là thách thức vì mô hình có thể tạo ra một câu trả lời khác nhau cho mỗi tài liệu. Một phương pháp khác để giải quyết vấn đề là nối kết câu hỏi, hình ảnh và tất cả tài liệu và đưa nó vào VL-T5 để tạo ra một câu trả lời duy nhất dựa trên chúng; phương pháp này gặp phải hai thiếu sót: (1) việc nối kết tất cả các đoạn văn dẫn đến một chuỗi dài, làm giảm tốc độ của mô hình và tăng tiêu thụ bộ nhớ và cũng có thể đạt đến giới hạn token tối đa, và (2) việc nối kết các đoạn văn khác nhau lại với nhau khiến mô hình khó xem xét ngữ cảnh của mỗi đoạn văn vì tập đoạn văn 𝑃 có thể về các chủ đề không liên quan khác nhau.

MM-FiD sử dụng một phương pháp thay thế, được hiển thị trong Hình 4. Trong kiến trúc này, câu hỏi và hình ảnh được nối kết với mỗi tài liệu trong tập hỗ trợ một cách độc lập và được đưa vào bộ mã hóa của VL-T5 để được mã hóa. Sau đó, biểu diễn được mã hóa của mỗi câu hỏi, hình ảnh và mỗi đoạn văn được nối kết lại với nhau để được đưa vào bộ giải mã của VL-T5. Trong phương pháp này, hình ảnh, câu hỏi và mỗi tài liệu được mã hóa riêng biệt, giúp mô hình giảm sử dụng bộ nhớ và xem xét ngữ cảnh tốt hơn so với hai phương án khác đã đề cập ở trên. Ngoài ra, việc nối kết các biểu diễn được mã hóa của tất cả các đoạn văn tại bộ giải mã giúp mô hình xem xét thông tin trong tất cả tài liệu để tạo ra một câu trả lời duy nhất cho câu hỏi về hình ảnh.

Để huấn luyện MM-FiD cho việc tạo văn bản, chúng tôi sử dụng hàm mất mát cross-entropy với công thức sau:

𝐿mm-fid =−∑︂
𝑘log𝑃(𝑦𝑘|𝑦𝑖<𝑘,{𝐼,𝑄,𝑝 1},{𝐼,𝑄,𝑝 2},...,{𝐼,𝑄,𝑝𝑛})

trong đó 𝑦𝑘 là token đầu ra thứ 𝑘, 𝐼 là hình ảnh, 𝑄 là câu hỏi, và 𝑝𝑖 là đoạn văn hỗ trợ (ví dụ: được truy xuất) thứ 𝑖.

6 CÁC THÍ NGHIỆM
6.1 Bộ Dữ Liệu
Trả Lời Câu Hỏi Hình Ảnh Kiến Thức Bên Ngoài (OK-VQA) [38]: Bộ dữ liệu này bao gồm các bộ ba, bao gồm một hình ảnh, một câu hỏi về hình ảnh và một câu trả lời cho câu hỏi đã đề cập. Trả lời hầu hết các câu hỏi trong bộ dữ liệu này cần một phần thông tin không được cung cấp trong hình ảnh. Do đó, việc truy cập một nguồn thông tin bên ngoài là cần thiết cho tác vụ này. Một bộ dữ liệu truy xuất dựa trên một bản dump Wikipedia⁷ với 11 triệu đoạn văn sau đó được xây dựng bởi Qu et al. [40], mà chúng tôi sử dụng để huấn luyện và đánh giá các bộ truy xuất của chúng tôi. Bộ dữ liệu này chứa 9009 câu hỏi để huấn luyện, 2523 câu hỏi để xác thực và 2523 để thử nghiệm [41]. Chúng tôi cũng sử dụng bộ dữ liệu OK-VQA gốc để đánh giá hiệu suất của pipeline truy xuất và tạo câu trả lời end-to-end của chúng tôi.

Trả Lời Câu Hỏi Hình Ảnh Dựa Trên Sự Thật (FVQA) [56]: Mỗi điểm dữ liệu trong bộ dữ liệu này bao gồm một hình ảnh, một câu hỏi về hình ảnh, câu trả lời và một sự thật hỗ trợ câu trả lời. Bộ dữ liệu này cũng đã cung cấp một nguồn kiến thức không có cấu trúc chứa tất cả các sự thật (tức là câu) chúng ta cần để trả lời câu hỏi về hình ảnh. Chúng tôi sử dụng 70% mẫu (4077) trong bộ dữ liệu này cho tập huấn luyện và tăng cường chúng tương tự như Qu et al. [40] với năm tiêu cực khó được truy xuất bởi BM25, 15% để xác thực (874) và 15% cho tập thử nghiệm (874). Chúng tôi sử dụng bộ dữ liệu FVQA gốc để đánh giá hiệu suất của pipeline truy xuất và tạo câu trả lời end-to-end của chúng tôi.

6.2 Thiết Lập Thí Nghiệm
Thiết Lập Huấn Luyện Bộ Truy Xuất. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng bộ tối ưu hóa Adam [26] với kích thước batch là 16 và tốc độ học là 10⁻⁵. Chúng tôi sử dụng một bộ lập lịch tốc độ học tuyến tính với 10% tổng số bước như các bước khởi động. Chúng tôi cũng sử dụng gradient clipping với giá trị 1.0. Độ dài đầu vào tối đa của mỗi bộ mã hóa được đặt thành 400 token. Chúng tôi huấn luyện mỗi mô hình trong 2 epoch trên OK-VQA và 4 epoch trên FVQA. Tất cả các thí nghiệm được thực hiện trên một máy với GPU Nvidia RTX8000 với 49GB bộ nhớ và 256GB RAM. Chúng tôi sử dụng Faiss [22] để lập chỉ mục các nhúng đã học với một chỉ mục phẳng để truy xuất mật độ hiệu quả. Đối với BM25, Pyserini được sử dụng.

Thiết Lập Huấn Luyện Multi-Modal Fusion-in-Decoder. Chúng tôi sử dụng bộ tối ưu hóa AdamW với kích thước batch là 1 với 32 bước tích lũy gradient, dẫn đến kích thước batch hiệu quả là 32. Chúng tôi sử dụng tốc độ học 5×10⁻⁵ và suy giảm trọng số 0.1 để huấn luyện mô hình MM-FiD. Với kích thước bộ dữ liệu huấn luyện, chúng tôi sử dụng bộ lập lịch tốc độ học tuyến tính với 800 và 200 bước khởi động lần lượt cho các bộ dữ liệu OK-VQA và FVQA. Chúng tôi huấn luyện mô hình trong 5000 (OK-VQA) và 2000 (FVQA) bước cập nhật gradient. Chúng tôi tạo một checkpoint của mô hình cứ sau 500 bước huấn luyện và chọn checkpoint tốt nhất dựa trên hiệu suất của nó trên tập xác thực. Chúng tôi cũng sử dụng gradient clipping ở 1.0 để huấn luyện. Độ dài đầu vào tối đa của bộ mã hóa của mỗi cặp câu hỏi và đoạn văn được đặt thành 420 (OK-VQA) và 64 (FVQA) token. Vì các câu trả lời trong cả hai bộ dữ liệu đều ngắn, chúng tôi đặt độ dài đầu ra của MM-FiD thành 16. Bộ giải mã của MM-FiD sử dụng beam search [58] với kích thước beam là 2 để tạo câu trả lời. Chúng tôi huấn luyện MM-FiD bằng 32(OK-VQA) và 5(FVQA) đoạn văn hỗ trợ cho mỗi cặp câu hỏi và hình ảnh, trong đó các đoạn văn hỗ trợ được truy xuất bằng mô hình DEDR được đề xuất. Các thí nghiệm MM-FiD được thực hiện trên một máy với một GPU Nvidia RTX8000 với 49GB bộ nhớ và 128GB RAM. Theo [56], chúng tôi sử dụng xác thực chéo năm fold cho bộ dữ liệu FVQA. Lưu ý rằng chúng tôi huấn luyện một DEDR riêng cho mỗi fold để tránh rò rỉ dữ liệu.

Metrics Đánh Giá. Để nhất quán với tài liệu, chúng tôi sử dụng các metrics phổ biến được đề xuất cho mỗi bộ dữ liệu. Theo Qu et al. [40], chúng tôi sử dụng mean reciprocal rank (MRR) và precision của năm tài liệu truy xuất hàng đầu (MRR@5 và P@5) để đánh giá các mô hình truy xuất trên bộ dữ liệu OK-VQA. Chúng tôi sử dụng MRR@5 và P@1 để đánh giá truy xuất trên bộ dữ liệu FVQA. Vì bộ dữ liệu FVQA chỉ cung cấp một đoạn văn ground truth cho mỗi câu hỏi, chúng tôi sử dụng Precision@1 làm metric đánh giá. Chúng tôi sử dụng paired t-test hai đuôi với hiệu chỉnh Bonferroni để xác định các cải thiện có ý nghĩa thống kê (p-value <0.05).

Để đánh giá mô hình tạo câu trả lời cho bộ dữ liệu OK-VQA, chúng tôi tuân theo script đánh giá chính thức được cung cấp bởi Marino et al. [38]. Nó sử dụng metric đánh giá cho tác vụ VQA [3], dựa vào các chú thích của con người. Để đánh giá trên bộ dữ liệu FVQA, chúng tôi tuân theo Wang et al. [56] và sử dụng Top-1 Accuracy hoặc Exact Match (EM) làm metric đánh giá, trong đó chúng tôi chuyển thành chữ thường và loại bỏ các mạo từ (ví dụ: a, an, the) và dấu câu.

6.3 Kết Quả Truy Xuất Đoạn Văn cho Các Tác Vụ KI-VQA
Baseline. Chúng tôi so sánh khung truy xuất mật độ được đề xuất với các baseline sau:

•Mô Hình Truy Xuất Thưa Thớt (Khớp Thuật Ngữ): Chúng tôi sử dụng hai baseline truy xuất thưa thớt: (1) BM25: baseline này sử dụng công thức BM25 [47] với các câu hỏi làm truy vấn, bỏ qua các hình ảnh, và các đoạn văn làm tài liệu. (2) BM25-Obj (CombMax): phương pháp này trích xuất 36 đối tượng từ hình ảnh (các đối tượng được tạo ra bởi một mô hình Faster R-CNN [45] được huấn luyện trước trên Visual Genome [2,27]) và nối kết tên của mỗi đối tượng với câu hỏi làm truy vấn và sử dụng công thức BM25 để truy xuất đoạn văn. Sau đó nó sử dụng CombMax [9,28] để tổng hợp 36 danh sách xếp hạng này.

--- TRANG 4 ---
SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan. Alireza Salemi, Juan Altmayer Pizzorno, và Hamed Zamani

Hình 4: Kiến trúc của MM-FiD. Nó sử dụng bộ mã hóa đa phương thức để mã hóa mỗi bộ ba câu hỏi-hình ảnh-đoạn văn riêng biệt và sau đó nối kết các mã hóa của chúng làm đầu vào cho bộ giải mã để tổng hợp kiến thức và tạo câu trả lời.

Qu et al. [40] cũng khám phá các phương pháp tổng hợp xếp hạng khác và CombMax được tìm thấy là giải pháp tốt nhất cho tác vụ này.

•Mô Hình Truy Xuất Mật Độ Đối Xứng Mã Hóa Đơn: Chúng tôi sử dụng ba baseline trong danh mục này: (3, 4) Dense-BERT: một mô hình truy xuất mật độ dựa trên BERT (tương tự như DPR [23]) với cùng mục tiêu huấn luyện như của chúng tôi. Chúng tôi cung cấp kết quả cho hai biến thể của mô hình này, một phương pháp độc lập với hình ảnh mà bộ mã hóa truy vấn chỉ mã hóa câu hỏi, và một phương pháp phụ thuộc vào hình ảnh mà bộ mã hóa truy vấn nhận sự nối kết của câu hỏi và chú thích hình ảnh (chú thích được tạo ra bởi ExpansionNet v2 [18]). Cái sau giống như bộ mã hóa 𝐸𝑇 của chúng tôi. (5) Dense-LXMERT là một mô hình sử dụng một bộ mã hóa đa phương thức, tức là LXMERT, để mã hóa truy vấn và đoạn văn. Nó sử dụng các token hình ảnh bị che ở phía đoạn văn. Phương pháp này là bộ mã hóa 𝐸𝑀𝑀 của chúng tôi.

•Mô Hình Truy Xuất Mật Độ Mã Hóa Kép Bất Đối Xứng: Trong danh mục này, chúng tôi sử dụng BERT-LXMERT, được đề xuất trong [40], sử dụng BERT để mã hóa đoạn văn và LXMERT để mã hóa truy vấn.⁸

Để so sánh công bằng, chúng tôi sử dụng cùng quá trình huấn luyện và đánh giá cho tất cả các mô hình (của chúng tôi và baseline). Theo hiểu biết của chúng tôi, kết quả baseline của chúng tôi là cao nhất được báo cáo trong tài liệu.

So Sánh với Các Baseline Truy Xuất. Kết quả truy xuất đoạn văn được báo cáo trong Bảng 1. Chúng tôi quan sát thấy rằng các mô hình truy xuất mật độ nói chung vượt trội hơn các baseline truy xuất thưa thớt, xác nhận lựa chọn thiết kế của chúng tôi tập trung vào truy xuất mật độ cho các tác vụ KI-VQA. Thú vị là, các mô hình truy xuất thưa thớt đạt MRR cao hơn trên FVQA so với OK-VQA, trong khi các mô hình truy xuất mật độ hoạt động tốt hơn đáng kể trên bộ dữ liệu OK-VQA. Quan sát này cho thấy rằng các tài liệu liên quan trong FVQA có lẽ có sự trùng lặp thuật ngữ cao hơn với các câu hỏi và đối tượng hình ảnh. Kết quả cho thấy các mô hình độc lập với hình ảnh (tức là BM25 và Dense-BERT chỉ mã hóa câu hỏi) hoạt động kém hơn biến thể của chính chúng với thông tin hình ảnh (tức là các đối tượng hoặc chú thích được tạo ra). Điều này nêu bật tầm quan trọng của việc biểu diễn hình ảnh trong các tác vụ KI-VQA. Hơn nữa, Bảng 1 cho thấy rằng baseline truy xuất mật độ bất đối xứng (BERT-LXMERT) không hoạt động tốt bằng các baseline truy xuất mật độ đối xứng. Cụ thể, BERT-LXMERT cho thấy hiệu suất kém trên bộ dữ liệu FVQA. Điều đó có thể do tập huấn luyện nhỏ hơn trong FVQA,⁹ vì các mô hình bất đối xứng thường yêu cầu nhiều dữ liệu huấn luyện hơn.

Một quan sát khác từ những kết quả này là hiệu suất trên các tập xác thực và thử nghiệm tương đối gần nhau, do đó có thể nói một cách an toàn rằng hiệu suất trên các tập xác thực có thể tổng quát hóa cho các tập thử nghiệm và không quan sát thấy overfitting tích cực. Kết quả cho thấy rằng các bộ mã hóa 𝐸𝑇 và 𝐸𝑀𝑀 của chính chúng tôi là các baseline hoạt động tốt nhất. Chúng tôi đã thực hiện một thí nghiệm để xem liệu hai phương pháp mã hóa này có cung cấp thông tin bổ sung hay không. Để làm điều này, chúng tôi tính toán reciprocal rank (RR) thu được bởi 𝐸𝑇 và 𝐸𝑀𝑀 cho mỗi truy vấn trong OK-VQA và vẽ sự khác biệt của chúng trong Hình 5. Để hình dung tốt hơn, hình này sắp xếp các truy vấn theo Δ𝑀𝑅𝑅 của chúng theo thứ tự giảm dần. Hình 5 cho thấy rằng 𝐸𝑇 và 𝐸𝑀𝑀 hoạt động tương tự đối với khoảng một nửa số truy vấn, trong khi 𝐸𝑇 hoạt động tốt hơn đối với khoảng 25% truy vấn và 𝐸𝑀𝑀 hoạt động tốt hơn đối với ~25% còn lại. Điều này cho thấy hai bộ mã hóa này chứa thông tin bổ sung và sự tổng hợp của chúng có thể cải thiện kết quả – xác nhận động lực thiết kế DEDR.

Kết quả thu được bởi DEDR cho thấy các cải thiện có ý nghĩa thống kê so với tất cả các baseline. Chúng tôi đạt được MRR cao hơn 11,6% và P@5 cao hơn 14,5% so với baseline tốt nhất (bao gồm

⁸Bài báo gốc [40] gọi baseline này là Dense-LXMERT. Chúng tôi đổi tên thành BERT-LXMERT để tránh nhầm lẫn.

⁹FVQA không chỉ có ít câu hỏi huấn luyện hơn, mà chỉ có một đoạn văn liên quan cho mỗi câu hỏi.

--- TRANG 5 ---
Một Khung Truy Xuất Mật Độ Mã Hóa Đối Xứng Kép cho Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức Chuyên Sâu SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan.

Bảng 1: Hiệu suất truy xuất đoạn văn cho các tác vụ KI-VQA trên các bộ dữ liệu OK-VQA và FVQA. Chỉ số trên∗ biểu thị cải thiện có ý nghĩa thống kê so với tất cả các baseline dựa trên paired t-test hai đuôi với hiệu chỉnh Bonferroni (𝑝<0.05).

| Bộ dữ liệu | OK-VQA | | | | FVQA | | | |
|---|---|---|---|---|---|---|---|---|
| Mô hình | Xác thực | | Thử nghiệm | | Xác thực | | Thử nghiệm | |
| | MRR@5 | P@5 | MRR@5 | P@5 | MRR@5 | P@1 | MRR@5 | P@1 |
| **Bộ Truy Xuất Thưa Thớt** | | | | | | | | |
| BM25 | 0.2565 | 0.1772 | 0.2637 | 0.1755 | 0.3368 | 0.2700 | 0.3509 | 0.2848 |
| BM25-Obj (CombMax) | 0.3772 | 0.2667 | 0.3686 | 0.2541 | 0.3903 | 0.3272 | 0.4057 | 0.3421 |
| **Bộ Truy Xuất Mật Độ Đối Xứng Mã Hóa Đơn** | | | | | | | | |
| Dense-BERT (câu hỏi, đoạn văn) | 0.4555 | 0.3155 | 0.4325 | 0.3058 | 0.3860 | 0.3089 | 0.3836 | 0.3020 |
| Dense-BERT (câu hỏi + chú thích, đoạn văn) →𝐸𝑇 của chúng tôi | 0.5843 | 0.4445 | 0.5797 | 0.4420 | 0.4409 | 0.3558 | 0.4292 | 0.3409 |
| Dense-LXMERT→𝐸𝑀𝑀 của chúng tôi | 0.5722 | 0.4276 | 0.5465 | 0.4066 | 0.4293 | 0.3478 | 0.4269 | 0.3409 |
| **Bộ Truy Xuất Mật Độ Mã Hóa Kép Bất Đối Xứng** | | | | | | | | |
| BERT-LXMERT | 0.4704 | 0.3364 | 0.4526 | 0.3329 | 0.1455 | 0.1006 | 0.1477 | 0.1029 |
| **Bộ Truy Xuất Mật Độ Mã Hóa Kép Đối Xứng** | | | | | | | | |
| DEDR | 0.6260∗ | 0.4890∗ | 0.6469∗ | 0.5059∗ | 0.5833∗ | 0.4931∗ | 0.5618∗ | 0.4713∗ |
| % cải thiện tương đối so với baseline tốt nhất | 7.1%↑ | 10.0%↑ | 11.6%↑ | 14.5%↑ | 32.3%↑ | 38.6%↑ | 30.9%↑ | 37.8%↑ |

Hình 5: Sự khác biệt giữa reciprocal rank (RR) thu được bởi 𝐸𝑇 và 𝐸𝑀𝑀 cho mỗi truy vấn trên tập thử nghiệm OK-VQA. Màu xanh / cam biểu thị các truy vấn mà 𝐸𝑇/𝐸𝑀𝑀 thắng.

các bộ mã hóa 𝐸𝑇 và 𝐸𝑀𝑀 của chính chúng tôi) trên tập thử nghiệm OK-VQA và MRR cao hơn 30,9% và P@1 cao hơn 37,8% trên tập thử nghiệm FVQA. Chúng tôi tin rằng phương pháp mã hóa đối xứng kép của chúng tôi hoạt động tốt mà không yêu cầu tập huấn luyện quy mô lớn, điều này biện minh cho lợi ích lớn hơn đáng kể trên bộ dữ liệu FVQA. Lưu ý rằng DEDR cũng sử dụng BERT và LXMERT và các cải thiện thu được không phải do các tham số mô hình lớn hơn hoặc huấn luyện trước khác nhau. Tuy nhiên, so với Dense-BERT và Dense-LXMERT, DEDR có khả năng tận dụng kiến thức từ cả hai mô hình ngôn ngữ đơn phương thức và đa phương thức. Tuy nhiên, BERT-LXMERT không thể tận dụng "kiến thức" ẩn bổ sung như vậy một cách hiệu quả do thiết kế bất đối xứng của nó.

Ablation DEDR. Để nghiên cứu thực nghiệm tác động của mỗi quyết định chúng tôi đưa ra trong việc thiết kế DEDR, chúng tôi báo cáo kết quả ablation trong Bảng 2. Hàng đầu tiên của bảng liên quan đến một mô hình sử dụng chung cả hai bộ mã hóa (𝐸𝑇 và 𝐸𝑀𝑀) trong cả huấn luyện và đánh giá. Hàng thứ hai, mặt khác, huấn luyện mỗi bộ mã hóa riêng biệt cho đến khi hội tụ và chỉ nối kết chúng khi suy luận. Không có chưng cất kiến thức trong cả hai mô hình này. Kết quả cho thấy DEDR vượt trội hơn cả hai mô hình này, chứng minh hiệu quả của phương pháp chưng cất kiến thức lặp đi lặp lại được đề xuất cho mã hóa kép. Các cải thiện có ý nghĩa thống kê trong gần như tất cả các trường hợp, ngoại trừ MRR@5 trên tập xác thực OK-VQA. Chúng tôi vẽ thêm hiệu suất truy xuất tại mỗi bước huấn luyện chưng cất kiến thức trong Hình 6. Chúng tôi quan sát thấy rằng trong ba lần lặp đầu tiên, hiệu suất của cả hai bộ mã hóa nói chung tăng lên trên cả hai bộ dữ liệu. Các mô hình cho thấy hành vi khác nhau trên các bộ dữ liệu khác nhau trong Hình 6. Điều này cho thấy rằng số lần lặp trong phương pháp chưng cất kiến thức phụ thuộc vào bộ dữ liệu và nên được điều chỉnh để có kết quả tốt nhất.

6.4 Kết Quả Trả Lời Câu Hỏi cho KI-VQA
Trong phần này, chúng tôi báo cáo và thảo luận về kết quả truy xuất và tạo câu trả lời end-to-end. Một loạt rộng các phương pháp trả lời câu hỏi đã được áp dụng cho các tác vụ KI-VQA. Không phải tất cả các phương pháp này đều có sẵn công khai và không phải tất cả đều sử dụng cùng nguồn kiến thức. Chúng tôi so sánh các phương pháp của mình với các mô hình hoạt động tốt nhất trong tài liệu với kích thước mô hình tương đối tương tự. Lưu ý rằng MM-FiD chứa 220 triệu tham số. Trong tập thí nghiệm đầu tiên của chúng tôi, chúng tôi cũng loại trừ các mô hình sử dụng GPT-3 (175 tỷ tham số) như một nguồn "kiến thức" ẩn. Kết quả được báo cáo trong Bảng 3. Như đã đề cập trong bảng, các phương pháp khác nhau trên OK-VQA sử dụng các nguồn kiến thức khác nhau, như ConceptNet, Wikidata, Wikipedia, Google Search và Google Images. Bộ dữ liệu FVQA đã phát hành một corpus sự thật được sử dụng bởi một số mô hình. Chúng tôi quan sát thấy rằng MM-FiD sử dụng các đoạn văn được truy xuất bởi DEDR để tạo câu trả lời vượt trội hơn tất cả các baseline được liệt kê trong Bảng 3. Chúng tôi quan sát thấy cải thiện 8,5% trên FVQA so với baseline hoạt động tốt nhất. Bảng này cũng bao gồm kết quả cho MM-FiD mà không có bất kỳ tài liệu hỗ trợ nào (tức là không có truy xuất). Chúng tôi quan sát thấy rằng MM-FiD có thể tạo ra hiệu suất cạnh tranh ngay cả khi không sử dụng kết quả truy xuất. Lý do là các mô hình ngôn ngữ lớn này chứa rất nhiều thông tin trong các tham số của chúng từ giai đoạn huấn luyện trước và chúng có thể trả lời nhiều câu hỏi dựa trên "kiến thức" ẩn bên trong của chúng. MM-FiD không có truy xuất thậm chí còn vượt trội hơn tất cả các baseline trên OK-VQA. Lưu ý rằng số lượng tham số trong MM-FiD (220M) có thể so sánh với các baseline. Kết quả cho thấy rằng việc sử dụng kết quả truy xuất dẫn đến lợi ích lớn hơn trên FVQA so với OK-VQA, có thể do bản chất của các câu hỏi trong trả lời câu hỏi hình ảnh dựa trên sự thật.

Lưu ý rằng một số mô hình sử dụng các đầu ra được tạo ra bởi GPT-3 như một nguồn kiến thức và thường vượt trội hơn những mô hình ở trên sử dụng kiến thức rõ ràng. Tuy nhiên, khó để rút ra kết luận, vì tập huấn luyện GPT-3 không được biết và nó có 175B tham số, khiến nó cực kỳ đắt đỏ để chạy và không thực sự có thể so sánh với bất kỳ mô hình nào khác được đề cập ở trên về khả năng. Dù vậy, chúng tôi vẫn so sánh mô hình của mình với các baseline tiên tiến với kích thước mô hình có thể so sánh sử dụng đầu ra của GPT-3 làm bằng chứng hỗ trợ. Kết quả trên bộ dữ liệu OK-VQA được báo cáo trong Bảng 4.¹⁰ Khi mô hình của chúng tôi sử dụng đầu ra của GPT-3 ngoài các đoạn văn, nó vẫn vượt trội hơn các phương án thay thế, nhưng với biên độ nhỏ hơn, nêu bật tác động của chất lượng tài liệu trong các tác vụ KI-VQA.

Ablation và Phân Tích Trả Lời Câu Hỏi. Để hiểu sâu hơn về giải pháp tạo câu trả lời được đề xuất, chúng tôi thực hiện các nghiên cứu ablation cẩn thận có kết quả được báo cáo trong Bảng 5. Kết quả trên cả hai bộ dữ liệu cho thấy rằng khi sử dụng cùng

¹⁰Chúng tôi không có quyền truy cập vào đầu ra của GPT-3 cho các câu hỏi FVQA.

--- TRANG 6 ---
SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan. Alireza Salemi, Juan Altmayer Pizzorno, và Hamed Zamani

Bảng 2: Nghiên cứu ablation cho chưng cất kiến thức lặp đi lặp lại trong DEDR. Chỉ số trên∗ biểu thị cải thiện có ý nghĩa thống kê so với tất cả các trường hợp ablation dựa trên paired t-test hai đuôi với hiệu chỉnh Bonferroni (𝑝<0.05).

| Bộ dữ liệu | OK-VQA | | | | FVQA | | | |
|---|---|---|---|---|---|---|---|---|
| Mô hình | Xác thực | | Thử nghiệm | | Xác thực | | Thử nghiệm | |
| | MRR@5 | P@5 | MRR@5 | P@5 | MRR@5 | P@1 | MRR@5 | P@1 |
| DEDR (bộ mã hóa chung, không KD) | 0.5930 | 0.4507 | 0.5405 | 0.4263 | 0.4669 | 0.3764 | 0.4498 | 0.3684 |
| DEDR (bộ mã hóa riêng biệt, không KD) | 0.6236 | 0.4771 | 0.6330 | 0.4972 | 0.5551 | 0.4668 | 0.5313 | 0.4439 |
| DEDR | 0.6260 | 0.4890∗ | 0.6469∗ | 0.5059∗ | 0.5833∗ | 0.4931∗ | 0.5618∗ | 0.4713∗ |

Hình 6: Hiệu suất DEDR tại các lần lặp khác nhau của phương pháp chưng cất kiến thức lặp đi lặp lại được đề xuất trên tập thử nghiệm của cả hai bộ dữ liệu OK-VQA và FVQA.

bộ truy xuất (tức là DEDR), MM-FiD vượt trội hơn biến thể đơn phương thức của nó, FiD [20], cũng đã được sử dụng cho các tác vụ KI-VQA, ví dụ trong KAT [13]. Hơn nữa, Bảng 5 chứng minh tác động của các mô hình truy xuất khác nhau đối với việc tạo câu trả lời cuối cùng. Ví dụ, việc sử dụng DEDR để truy xuất thay vì baseline truy xuất hoạt động tốt nhất từ Bảng 1 sẽ dẫn đến độ chính xác cao hơn 13% trong FVQA, nêu bật tầm quan trọng của truy xuất trong pipeline KI-VQA.

Hình 7 vẽ độ nhạy của hiệu suất MM-FiD đối với số lượng đoạn văn được truy xuất bởi DEDR. Nó cũng vẽ tỷ lệ hit, tức là tỷ lệ thành công trong việc truy xuất ít nhất một đoạn văn liên quan để được trình bày cho MM-FiD. Nói chung, càng nhiều tài liệu chúng ta đưa vào MM-FiD trên OK-VQA, độ chính xác trả lời câu hỏi càng cao. Độ chính xác của nó trở nên tương đối ổn định sau khi truy xuất 16 tài liệu. Đường cong độ chính xác tuân theo hành vi tương tự như đường cong hit trên OK-VQA. Tuy nhiên, FVQA thể hiện hành vi khác biệt đáng kể. Độ chính xác cao nhất đạt được khi chỉ truy xuất năm đoạn văn hỗ trợ. Điều đó là do bản chất của bộ dữ liệu, nơi chỉ có một sự thật liên quan cho mỗi câu hỏi và việc truy xuất nhiều sự thật (có thể không chính xác) hơn có thể làm nhầm lẫn mô hình tạo câu trả lời. Lưu ý rằng DEDR đạt tỷ lệ hit 70% chỉ bằng cách truy xuất hai đoạn văn trên FVQA, trong khi cùng mô hình cần truy xuất 4 đoạn văn để đạt cùng mức tỷ lệ hit trên OK-VQA. Phát hiện này cho thấy rằng việc nghiên cứu dự đoán tự động ranking cut-off cho các tác vụ KI-VQA trong tương lai là đáng giá.

7 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Bài báo này trình bày DEDR, một khung truy xuất mật độ đối xứng mới dựa trên mã hóa đơn phương thức và đa phương thức kép. Chúng tôi đề xuất một phương pháp chưng cất kiến thức lặp đi lặp lại để cập nhật hai không gian biểu diễn mã hóa này và tổng hợp chúng khi suy luận. Nó cũng đề xuất MM-FiD, một phần mở rộng của kiến trúc fusion-in-decoder [20] cho dữ liệu đa phương thức. Các thí nghiệm toàn diện trên hai bộ dữ liệu được thiết lập tốt, OK-VQA và FVQA, cho thấy rằng việc truy xuất đoạn văn bằng DEDR và sử dụng chúng để tạo câu trả lời thông qua MM-FiD vượt trội đáng kể so với các baseline tiên tiến với khả năng tương đương. Ví dụ, phương pháp này dẫn đến cải thiện truy xuất 37,8% về P@1 và cải thiện độ chính xác exact match 8,5% trên tập thử nghiệm FVQA so với các baseline hoạt động tốt nhất. Chúng tôi chứng minh tác động của mọi quyết định thiết kế chúng tôi đưa ra trong cả DEDR và MM-FiD thông qua các nghiên cứu ablation toàn diện và nêu bật các lĩnh vực mở để khám phá trong tương lai. Ví dụ, kết quả của chúng tôi cho thấy dự đoán chính xác về 'khi nào nên truy xuất' là một lĩnh vực có tác động đối với các tác vụ KI-VQA. Do đó, việc khám phá dự đoán hiệu suất truy xuất và cắt ngắn ranking cut-off trong các tác vụ KI-VQA có thể là một hướng tương lai có kết quả. Chúng tôi cũng có ý định khám phá các mô hình đòi hỏi kiến thức toàn cầu cho cả đầu vào văn bản và đa phương thức. Chúng tôi tiếp tục có kế hoạch mở rộng các ứng dụng của các tác vụ đa phương thức đòi hỏi kiến thức ngoài trả lời câu hỏi.

--- TRANG 7 ---
Một Khung Truy Xuất Mật Độ Mã Hóa Đối Xứng Kép cho Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức Chuyên Sâu SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan.

Bảng 3: Hiệu suất trả lời câu hỏi cho các mô hình với nguồn kiến thức rõ ràng trên cả hai bộ dữ liệu OK-VQA và FVQA. Tất cả các mô hình này tương đối có thể so sánh về kích thước mô hình và chứa ít hơn 1 tỷ tham số.

| OK-VQA | | FVQA | |
|---|---|---|---|
| Mô hình | Nguồn Kiến thức | Độ chính xác | Mô hình | Nguồn Kiến thức | Độ chính xác Top-1 |
| KAT-base [13] | WikiData | 40.93 | BAN [25] | Không có | 35.69 |
| RVL [50] | ConceptNet | 39.04 | RVL [50] | ConceptNet | 54.27 |
| MAVEx [57] | Wikipedia, ConceptNet, Google Images | 40.28 | BAN [25] + KG-AUG [29] | Không có | 38.58 |
| UnifER+ViLT [14] | ConceptNet | 42.13 | UnifER + ViLT [14] | FVQA Corpus | 55.04 |
| VRR [35] | Google Search | 39.20 | Top1-QQmaping [56] | FVQA Corpus | 52.56 |
| LaKo-base [5] | ConceptNet, DBPedia, WebChild | 42.21 | Top3-QQmaping [56] | FVQA Corpus | 56.91 |
| MM-FiD | Không có | 42.82 | MM-FiD | Không có | 52.78 |
| DEDR +MM-FiD | Wikipedia | 44.57 | DEDR +MM-FiD | FVQA Corpus | 61.80 |
| % cải thiện tương đối so với baseline tốt nhất | | 5.5%↑ | % cải thiện tương đối so với baseline tốt nhất | | 8.5%↑ |

Bảng 4: Hiệu suất QA của các mô hình SOTA dựa vào đầu ra của GPT-3 như một "nguồn kiến thức" trên OK-VQA.

| Mô hình | Nguồn kiến thức | Độ chính xác |
|---|---|---|
| PICa-full [60] | Frozen GPT-3 | 48.00 |
| KAT-base [13] | Frozen GPT-3, Wikidata | 50.58 |
| DEDR + MM-FiD | Frozen GPT-3, Wikipedia | 51.02 |

Bảng 5: Nghiên cứu ablation của pipeline KI-VQA được đề xuất. Chỉ số trên∗ biểu thị cải thiện có ý nghĩa thống kê so với tất cả các trường hợp ablation dựa trên paired t-test hai đuôi với hiệu chỉnh Bonferroni (𝑝<0.05).

| Bộ Truy Xuất | Bộ Tạo Câu Trả Lời | OK-VQA | FVQA |
|---|---|---|---|
| | | Độ chính xác | Độ chính xác Top-1 |
| DEDR | FiD | 39.48 | 60.85 |
| BM25-Obj (CombMax) [40] | MM-FiD | 41.97 | 54.65 |
| Baseline truy xuất tốt nhất từ Bảng 1 | MM-FiD | 41.82 | 52.78 |
| DEDR | MM-FiD | 44.57∗ | 61.80∗ |

Hình 7: Độ chính xác MM-FiD và tỷ lệ hit trong các đoạn văn hỗ trợ tại các mức cắt ngắn xếp hạng khác nhau.

bộ truy xuất (tức là DEDR), MM-FiD vượt trội hơn biến thể đơn phương thức của nó, FiD [20], cũng đã được sử dụng cho các tác vụ KI-VQA, ví dụ trong KAT [13]. Hơn nữa, Bảng 5 chứng minh tác động của các mô hình truy xuất khác nhau đối với việc tạo câu trả lời cuối cùng. Ví dụ, việc sử dụng DEDR để truy xuất thay vì baseline truy xuất hoạt động tốt nhất từ Bảng 1 sẽ dẫn đến độ chính xác cao hơn 13% trong FVQA, nêu bật tầm quan trọng của truy xuất trong pipeline KI-VQA.

Hình 7 vẽ độ nhạy của hiệu suất MM-FiD đối với số lượng đoạn văn được truy xuất bởi DEDR. Nó cũng vẽ tỷ lệ hit, tức là tỷ lệ thành công trong việc truy xuất ít nhất một đoạn văn liên quan để được trình bày cho MM-FiD. Nói chung, càng nhiều tài liệu chúng ta đưa vào MM-FiD trên OK-VQA, độ chính xác trả lời câu hỏi càng cao. Độ chính xác của nó trở nên tương đối ổn định sau khi truy xuất 16 tài liệu. Đường cong độ chính xác tuân theo hành vi tương tự như đường cong hit trên OK-VQA. Tuy nhiên, FVQA thể hiện hành vi khác biệt đáng kể. Độ chính xác cao nhất đạt được khi chỉ truy xuất năm đoạn văn hỗ trợ. Điều đó là do bản chất của bộ dữ liệu, nơi chỉ có một sự thật liên quan cho mỗi câu hỏi và việc truy xuất nhiều sự thật (có thể không chính xác) hơn có thể làm nhầm lẫn mô hình tạo câu trả lời. Lưu ý rằng DEDR đạt tỷ lệ hit 70% chỉ bằng cách truy xuất hai đoạn văn trên FVQA, trong khi cùng mô hình cần truy xuất 4 đoạn văn để đạt cùng mức tỷ lệ hit trên OK-VQA. Phát hiện này cho thấy rằng việc nghiên cứu dự đoán tự động ranking cut-off cho các tác vụ KI-VQA trong tương lai là đáng giá.

7 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Bài báo này trình bày DEDR, một khung truy xuất mật độ đối xứng mới dựa trên mã hóa đơn phương thức và đa phương thức kép. Chúng tôi đề xuất một phương pháp chưng cất kiến thức lặp đi lặp lại để cập nhật hai không gian biểu diễn mã hóa này và tổng hợp chúng khi suy luận. Nó cũng đề xuất MM-FiD, một phần mở rộng của kiến trúc fusion-in-decoder [20] cho dữ liệu đa phương thức. Các thí nghiệm toàn diện trên hai bộ dữ liệu được thiết lập tốt, OK-VQA và FVQA, cho thấy rằng việc truy xuất đoạn văn bằng DEDR và sử dụng chúng để tạo câu trả lời thông qua MM-FiD vượt trội đáng kể so với các baseline tiên tiến với khả năng tương đương. Ví dụ, phương pháp này dẫn đến cải thiện truy xuất 37,8% về P@1 và cải thiện độ chính xác exact match 8,5% trên tập thử nghiệm FVQA so với các baseline hoạt động tốt nhất. Chúng tôi chứng minh tác động của mọi quyết định thiết kế chúng tôi đưa ra trong cả DEDR và MM-FiD thông qua các nghiên cứu ablation toàn diện và nêu bật các lĩnh vực mở để khám phá trong tương lai. Ví dụ, kết quả của chúng tôi cho thấy dự đoán chính xác về 'khi nào nên truy xuất' là một lĩnh vực có tác động đối với các tác vụ KI-VQA. Do đó, việc khám phá dự đoán hiệu suất truy xuất và cắt ngắn ranking cut-off trong các tác vụ KI-VQA có thể là một hướng tương lai có kết quả. Chúng tôi cũng có ý định khám phá các mô hình đòi hỏi kiến thức toàn cầu cho cả đầu vào văn bản và đa phương thức. Chúng tôi tiếp tục có kế hoạch mở rộng các ứng dụng của các tác vụ đa phương thức đòi hỏi kiến thức ngoài trả lời câu hỏi.

--- TRANG 8 ---
SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan. Alireza Salemi, Juan Altmayer Pizzorno, và Hamed Zamani

LỜI CẢM ƠN
Công trình này được hỗ trợ một phần bởi Trung tâm Truy xuất Thông tin Thông minh, một phần bởi tài trợ NSF #2106282, và một phần bởi Lowe's. Mọi ý kiến, phát hiện và kết luận hoặc khuyến nghị được thể hiện trong tài liệu này thuộc về các tác giả và không nhất thiết phản ánh quan điểm của nhà tài trợ.

TÀI LIỆU THAM KHẢO
[1]Bilal Abu-Salih. 2021. Domain-specific knowledge graphs: A survey. Journal of Network and Computer Applications 185 (2021), 103076. https://doi.org/10.1016/j.jnca.2021.103076

[2]Peter Anderson, X. He, C. Buehler, Damien Teney, Mark Johnson, Stephen Gould, và Lei Zhang. 2018. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), 6077–6086.

[3]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, và Devi Parikh. 2015. VQA: Visual Question Answering. Trong International Conference on Computer Vision (ICCV).

[4]Danqi Chen, Adam Fisch, Jason Weston, và Antoine Bordes. 2017. Reading Wikipedia to Answer Open-Domain Questions. https://doi.org/10.48550/ARXIV.1704.00051

[5]Zhuo Chen, Yufeng Huang, Jiaoyan Chen, Yuxia Geng, Yin Fang, Jeff Z. Pan, Ningyu Zhang, và Wen Zhang. 2022. LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection. CoRR abs/2207.12888 (2022).

[6]Jaemin Cho, Jie Lei, Hao Tan, và Mohit Bansal. 2021. Unifying Vision-and-Language Tasks via Text Generation. Trong Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila và Tong Zhang (Eds.). PMLR, 1931–1942.

[7]Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, và Richard A. Harshman. 1990. Indexing by Latent Semantic Analysis. J. Am. Soc. Inf. Sci. 41 (1990), 391–407.

[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423

[9]Edward A. Fox và Joseph A. Shaw. 1993. Combination of Multiple Searches. Trong Text Retrieval Conference.

[10] Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti, Ying Nian Wu, và Prem Natarajan. 2022. Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering. Trong 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 5057–5067. https://doi.org/10.1109/CVPR52688.2022.00501

[11] Luyu Gao, Zhuyun Dai, và Jamie Callan. 2020. Modularized Transfomer-based Ranking Framework. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 4180–4190. https://doi.org/10.18653/v1/2020.emnlp-main.342

[12] François Gardères, Maryam Ziaeefard, Baptiste Abeloos, và Freddy Lecue. 2020. ConceptBert: Concept-Aware Representation for Visual Question Answering. Trong Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 489–498. https://doi.org/10.18653/v1/2020.findings-emnlp.44

[13] Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, và Jianfeng Gao. 2022. KAT: A Knowledge Augmented Transformer for Vision-and-Language. Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Seattle, United States, 956–968. https://doi.org/10.18653/v1/2022.naacl-main.70

[14] Yangyang Guo, Liqiang Nie, Yongkang Wong, Yibing Liu, Zhiyong Cheng, và Mohan Kankanhalli. 2022. A Unified End-to-End Retriever-Reader Framework for Knowledge-Based VQA. Trong Proceedings of the 30th ACM International Conference on Multimedia (Lisboa, Portugal) (MM '22). Association for Computing Machinery, New York, NY, USA, 2061–2069. https://doi.org/10.1145/3503161.3547870

[15] Darryl Hannan, Akshay Jain, và Mohit Bansal. 2020. ManyModalQA: Modality Disambiguation and QA over Diverse Inputs. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020), 7879–7886. https://doi.org/10.1609/aaai.v34i05.6294

[16] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, và Hamed Zamani. 2022. FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation. ArXiv abs/2209.14290 (2022).

[17] Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, và Allan Hanbury. 2021. Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling. Trong Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21). Association for Computing Machinery, New York, NY, USA, 113–122. https://doi.org/10.1145/3404835.3462891

[18] Jia Cheng Hu, Roberto Cavicchioli, và Alessandro Capotondi. 2022. ExpansionNet v2: Block Static Expansion in fast end to end training for Image Captioning. https://doi.org/10.48550/ARXIV.2208.06551

[19] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, và Jason Weston. 2020. Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring. Trong International Conference on Learning Representations. https://openreview.net/forum?id=SkxgnnNFvH

[20] Gautier Izacard và Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Association for Computational Linguistics, Online, 874–880. https://doi.org/10.18653/v1/2021.eacl-main.74

[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, và Tom Duerig. 2021. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. Trong International Conference on Machine Learning.

[22] Jeff Johnson, Matthijs Douze, và Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 7, 3 (2019), 535–547.

[23] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 6769–6781. https://doi.org/10.18653/v1/2020.emnlp-main.550

[24] Omar Khattab và Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. Trong Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20). Association for Computing Machinery, New York, NY, USA, 39–48. https://doi.org/10.1145/3397271.3401075

[25] Jin-Hwa Kim, Jaehyun Jun, và Byoung-Tak Zhang. 2018. Bilinear Attention Networks. Trong Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, và R. Garnett (Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2018/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf

[26] Diederik P. Kingma và Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. https://doi.org/10.48550/ARXIV.1412.6980

[27] R. Krishna, Yuke Zhu, O. Groth, J. Johnson, Kenji Hata, J. Kravitz, Stephanie Chen, Yannis Kalantidis, L. Li, David A. Shamma, Michael S. Bernstein, và Li Fei-Fei. 2016. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision 123 (2016), 32–73.

[28] Joon Ho Lee. 1997. Analyses of Multiple Evidence Combination. SIGIR Forum 31, SI (jul 1997), 267–276. https://doi.org/10.1145/278459.258587

[29] Guohao Li, Xin Wang, và Wenwu Zhu. 2020. Boosting Visual Question Answering with Context-aware Knowledge Aggregation. Proceedings of the 28th ACM International Conference on Multimedia (2020).

[30] Sheng-Chieh Lin, Jheng-Hong Yang, và Jimmy Lin. 2021. In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval. Trong Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021). Association for Computational Linguistics, Online, 163–173. https://doi.org/10.18653/v1/2021.repl4nlp-1.17

[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. Trong Computer Vision – ECCV 2014, David Fleet, Tomas Pajdla, Bernt Schiele, và Tinne Tuytelaars (Eds.). Springer International Publishing, Cham, 740–755.

[32] Weizhe Lin và Bill Byrne. 2022. Retrieval Augmented Visual Question Answering with Outside Knowledge. https://doi.org/10.48550/ARXIV.2210.03809

[33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 10012–10022.

[34] Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, và Ge Yu. 2022. Universal Multi-Modality Retrieval with One Unified Embedding Space. https://doi.org/10.48550/ARXIV.2209.00179

[35] Man Luo, Yankai Zeng, Pratyay Banerjee, và Chitta Baral. 2021. Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 6417–6431. https://doi.org/10.18653/v1/2021.emnlp-main.517

[36] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, và Ophir Frieder. 2020. Efficient Document Re-Ranking for Transformers by Precomputing Term Representations. Trong Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20). Association for Computing Machinery, New York, NY, USA, 49–58. https://doi.org/10.1145/3397271.3401093

--- TRANG 9 ---
Một Khung Truy Xuất Mật Độ Mã Hóa Đối Xứng Kép cho Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức Chuyên Sâu SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan.

[37] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Kumar Gupta, và Marcus Rohrbach. 2020. KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 14106–14116.

[38] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, và Roozbeh Mottaghi. 2019. OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge. Trong Conference on Computer Vision and Pattern Recognition (CVPR).

[39] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, và Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 2523–2544. https://doi.org/10.18653/v1/2021.naacl-main.200

[40] Chen Qu, Hamed Zamani, Liu Yang, W. Bruce Croft, và Erik Learned-Miller. 2021. Passage Retrieval for Outside-Knowledge Visual Question Answering. Trong Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21). Association for Computing Machinery, New York, NY, USA, 1753–1757. https://doi.org/10.1145/3404835.3462987

[41] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, và Haifeng Wang. 2021. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 5835–5847. https://doi.org/10.18653/v1/2021.naacl-main.466

[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, và Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. Trong Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila và Tong Zhang (Eds.). PMLR, 8748–8763. https://proceedings.mlr.press/v139/radford21a.html

[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 1–67. http://jmlr.org/papers/v21/20-074.html

[44] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, và Ji-Rong Wen. 2021. RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2825–2835. https://doi.org/10.18653/v1/2021.emnlp-main.224

[45] Shaoqing Ren, Kaiming He, Ross Girshick, và Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Trong Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1 (Montreal, Canada) (NIPS'15). MIT Press, Cambridge, MA, USA, 91–99.

[46] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, và Vaibhava Goel. 2016. Self-Critical Sequence Training for Image Captioning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), 1179–1195.

[47] Stephen Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, và M. Gatford. 1995. Okapi at TREC-3. Trong Proceedings of the Third Text REtrieval Conference (TREC-3). Gaithersburg, MD: NIST, 109–126.

[48] Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa, và Eneko Agirre. 2023. Image captioning for effective use of language models in knowledge-based visual question answering. Expert Systems with Applications 212 (2023), 118669. https://doi.org/10.1016/j.eswa.2022.118669

[49] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, và Matei Zaharia. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Seattle, United States, 3715–3734. https://doi.org/10.18653/v1/2022.naacl-main.272

[50] Violetta Shevchenko, Damien Teney, Anthony Dick, và Anton van den Hengel. 2021. Reasoning over Vision and Language: Exploring the Benefits of Supplemental Knowledge. Trong Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN). Association for Computational Linguistics, Kyiv, Ukraine, 1–18. https://aclanthology.org/2021.lantern-1.1

[51] Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, và Balaji Vasan Srinivasan. 2021. MIMOQA: Multimodal Input Multimodal Output Question Answering. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 5317–5332. https://doi.org/10.18653/v1/2021.naacl-main.418

[52] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, và Jonathan Berant. 2021. MultiModal{QA}: complex question answering over text, tables and images. Trong International Conference on Learning Representations. https://openreview.net/forum?id=ee6W5UgQLa

[53] Hao Tan và Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 5100–5111. https://doi.org/10.18653/v1/D19-1514

[54] Jizhi Tang, Yansong Feng, và Dongyan Zhao. 2019. Learning to Update Knowledge Graphs by Reading News. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 2632–2641. https://doi.org/10.18653/v1/D19-1265

[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, và Illia Polosukhin. 2017. Attention is All you Need. Trong Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

[56] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, và Anton van den Hengel. 2018. FVQA: Fact-Based Visual Question Answering. IEEE Trans. Pattern Anal. Mach. Intell. 40, 10 (oct 2018), 2413–2427. https://doi.org/10.1109/TPAMI.2017.2754246

[57] Jialin Wu, Jiasen Lu, Ashish Sabharwal, và Roozbeh Mottaghi. 2022. Multi-Modal Answer Validation for Knowledge-Based VQA. Proceedings of the AAAI Conference on Artificial Intelligence 36, 3 (Jun. 2022), 2712–2721. https://doi.org/10.1609/aaai.v36i3.20174

[58] Yonghui Wu, Mike Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, và Jeffrey Dean. 2016. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. ArXiv abs/1609.08144 (2016).

[59] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, và Arnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. Trong International Conference on Learning Representations. https://openreview.net/forum?id=zeFrfgyZln

[60] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, và Lijuan Wang. 2021. An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA. Trong AAAI Conference on Artificial Intelligence.

[61] Hamed Zamani, Fernando Diaz, Mostafa Dehghani, Donald Metzler, và Michael Bendersky. 2022. Retrieval-Enhanced Machine Learning. Trong Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR '22). Association for Computing Machinery, New York, NY, USA, 2875–2886. https://doi.org/10.1145/3477495.3531722

[62] Hansi Zeng, Hamed Zamani, và Vishwa Vinay. 2022. Curriculum Learning for Dense Retrieval Distillation. Trong Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR '22). Association for Computing Machinery, New York, NY, USA, 1979–1983. https://doi.org/10.1145/3477495.3531791

[63] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, và Shaoping Ma. 2021. Optimizing Dense Retrieval Model Training with Hard Negatives. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (2021).

--- TRANG 10 ---
SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan. Alireza Salemi, Juan Altmayer Pizzorno, và Hamed Zamani

--- TRANG 11 ---
Một Khung Truy Xuất Mật Độ Mã Hóa Đối Xứng Kép cho Trả Lời Câu Hỏi Hình Ảnh Đòi Hỏi Kiến Thức Chuyên Sâu SIGIR '23, 23–27 tháng 7, 2023, Taipei, Đài Loan.
