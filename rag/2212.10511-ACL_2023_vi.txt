# 2212.10511.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2212.10511.pdf
# Kích thước file: 1119401 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
ACL 2023
Khi Nào Không Nên Tin Tưởng Mô Hình Ngôn Ngữ: Nghiên Cứu Hiệu Quả của
Bộ Nhớ Tham Số và Phi-Tham Số
Alex Mallen∗♢Akari Asai∗♢Victor Zhong♢Rajarshi Das♢
Daniel Khashabi♠Hannaneh Hajishirzi♢♡
♢Đại học Washington♠Đại học Johns Hopkins
♡Viện Allen cho AI
{atmallen,akari,vzhong,rajarshi,hannaneh}@cs.washington.edu
danielk@jhu.edu
Tóm tắt
Mặc dù có hiệu suất ấn tượng trên các nhiệm vụ
đa dạng, các mô hình ngôn ngữ lớn (LMs) vẫn
gặp khó khăn với các nhiệm vụ đòi hỏi kiến thức
phong phú về thế giới, ngụ ý khó khăn trong việc
mã hóa một kho tàng kiến thức thế giới trong các
tham số của chúng. Bài báo này nhằm hiểu điểm
mạnh và hạn chế của LMs trong việc ghi nhớ
kiến thức thực tế, bằng cách thực hiện các thí
nghiệm thăm dò kiến thức quy mô lớn trên hai
bộ dữ liệu QA tập trung vào thực thể miền mở:
POPQA, bộ dữ liệu mới của chúng tôi với 14k
câu hỏi về các thực thể đuôi dài, và EntityQuestions, một bộ dữ liệu QA miền mở được sử dụng
rộng rãi. Chúng tôi phát hiện rằng LMs gặp khó
khăn với kiến thức thực tế ít phổ biến hơn, và
việc tăng cường truy xuất giúp ích đáng kể trong
những trường hợp này. Mặt khác, việc mở rộng
chủ yếu cải thiện việc ghi nhớ kiến thức phổ biến,
và không thể cải thiện đáng kể việc ghi nhớ kiến
thức thực tế ở đuôi dài. Dựa trên những phát
hiện đó, chúng tôi đưa ra một phương pháp mới
cho việc tăng cường truy xuất nhằm cải thiện hiệu
suất và giảm chi phí suy luận bằng cách chỉ truy
xuất bộ nhớ phi-tham số khi cần thiết.¹
1 Giới thiệu
Các mô hình ngôn ngữ lớn (LMs; Brown et al. 2020;
Raffel et al. 2020) đã được chứng minh là có khả
năng cạnh tranh trên các nhiệm vụ NLP đa dạng,
bao gồm các nhiệm vụ chuyên sâu về kiến thức đòi
hỏi việc ghi nhớ tỉ mỉ kiến thức thực tế (Chowdhery
et al., 2022; Yu et al., 2022). Trong khi đó, LMs
cũng được chứng minh có khả năng ghi nhớ hạn
chế đối với các thực thể ít thường xuyên hơn (Kandpal et al., 2022), dễ bị ảo giác (Shuster et al., 2021),
và chịu sự suy giảm theo thời gian (Kasai et al.,
2022; Jang et al., 2022). Việc kết hợp kiến thức
phi-tham số (tức là các đoạn văn bản được truy
xuất) phần lớn giúp giải quyết những vấn đề này
bắt nguồn từ việc phụ thuộc vào kiến thức tham số
của LMs —kiến thức được lưu trữ
¹Mã và dữ liệu của chúng tôi có sẵn tại https://github.
com/AlexTMallen/adaptive-retrieval .
Không được ghi nhớ trong tham số  
⇨ sử dụng truy xuấtĐược ghi nhớ trong tham số
⇨ không sử dụng truy xuất
Nghề nghiệp của Kathy
Saltzman là gì?Thủ đô của
Louisiana là gì?
Hình 1: Mối quan hệ giữa độ phổ biến của thực thể chủ
đề trong câu hỏi và hiệu suất GPT-3 trong QA miền mở,
có và không có các đoạn được truy xuất. Truy Xuất Thích
Ứng chỉ truy xuất khi cần thiết (thanh màu cam) dựa trên
ngưỡng được quyết định theo heuristic (đường màu đỏ).
trong các tham số của chúng (Izacard et al., 2022b)
—nhưng vẫn chưa rõ liệu nó có hoàn toàn vượt trội
hay bổ sung cho kiến thức tham số. Hiểu khi nào
chúng ta không nên tin tưởng kết quả đầu ra của
LMs cũng rất quan trọng để triển khai chúng an toàn
trong các ứng dụng thực tế (Kadavath et al., 2022).
Công trình này thực hiện thăm dó kiến thức quy
mô lớn của LMs về việc ghi nhớ kiến thức thực tế,
để hiểu khi nào chúng ta nên và không nên dựa vào
kiến thức tham số của LMs, và việc mở rộng cũng
như bộ nhớ phi-tham số (ví dụ: LMs tăng cường
truy xuất) có thể giúp ích như thế nào. Cụ thể,
chúng tôi nhằm giải quyết các câu hỏi nghiên cứu
sau:
(RQ 1) LMs ghi nhớ bao nhiêu kiến thức thực tế
và những yếu tố nào ảnh hưởng đến việc
ghi nhớ? (Phần 4)
(RQ 2) Bộ nhớ phi-tham số có thể giảm thiểu các
hạn chế của bộ nhớ tham số của LMs đến
mức nào? (Phần 5)
(RQ 3) Chúng ta có thể xây dựng một hệ thống để
kết hợp thích ứng bộ nhớ phi-tham số và
tham số không? (Phần 6)
Chúng tôi đưa ra giả thuyết rằng kiến thức thực
tế được thảo luận thường xuyên trên web dễ dàng
được ghi nhớarXiv:2212.10511v4  [cs.CL]  2 Jul 2023

--- TRANG 2 ---
bởi LMs, trong khi kiến thức ít được thảo luận có
thể không được nắm bắt tốt và do đó chúng cần
truy xuất bộ nhớ phi-tham số bên ngoài. Chúng tôi
đánh giá mười LMs lớn thuộc ba họ (tức là GPT-
Neo, OPT, và GPT-3) với các quy mô khác nhau
trên nhiệm vụ trả lời câu hỏi miền mở (QA) theo
cách prompting zero-shot hoặc few-shot. Chúng
tôi xây dựng một bộ dữ liệu mới, POPQA, bao gồm
14k câu hỏi để bao phủ thông tin thực tế ở đuôi
dài có thể đã bị bỏ lỡ trong các bộ dữ liệu QA
phổ biến (Kwiatkowski et al., 2019). Chúng tôi sử
dụng lượt xem trang Wikipedia như một thước đo
độ phổ biến và chuyển đổi các bộ ba kiến thức từ
Wikidata, với các mức độ phổ biến đa dạng, thành
các câu hỏi ngôn ngữ tự nhiên, được neo vào các
thực thể gốc và loại mối quan hệ. Chúng tôi cũng
sử dụng EntityQuestions (Sciavolino et al., 2021),
một bộ dữ liệu QA miền mở với phân phối đuôi dài.
Trên cả hai bộ dữ liệu, việc ghi nhớ của LMs (RQ
1) thường bị giới hạn ở kiến thức thực tế phổ biến
và ngay cả GPT-3 davinci-003 cũng không trả lời
được phần lớn các câu hỏi đuôi dài. Hơn nữa, đối
với những câu hỏi như vậy, việc mở rộng mô hình
không cải thiện đáng kể hiệu suất (ví dụ: đối với
4.000 câu hỏi ít phổ biến nhất trong POPQA, GPT-j
6B có độ chính xác 16% và GPT-3 davinci-003 có
độ chính xác 19%). Điều này cũng gợi ý rằng chúng
ta có thể dự đoán LMs có ghi nhớ kiến thức nhất
định hay không dựa trên thông tin được trình bày
chỉ trong câu hỏi đầu vào.
Tiếp theo, chúng tôi điều tra liệu một cách tiếp
cận bán-tham số tăng cường LMs với bằng chứng
được truy xuất có thể giảm thiểu hiệu suất thấp
trên các câu hỏi về các thực thể ít phổ biến hơn
(RQ 2). Bộ nhớ phi-tham số cải thiện đáng kể hiệu
suất trên các phân phối đuôi dài trên các mô hình.
Cụ thể, chúng tôi phát hiện rằng LMs tăng cường
truy xuất đặc biệt cạnh tranh khi các thực thể chủ
đề không phổ biến: một bộ truy xuất dense neural
(Izacard et al., 2022a) tăng cường GPT-neo 2.7B
vượt trội hơn GPT-3 davinci-003 trên 4.000 câu
hỏi ít phổ biến nhất. Đáng ngạc nhiên, chúng tôi
cũng phát hiện rằng việc tăng cường truy xuất có
thể làm tổn hại hiệu suất của LMs lớn trên các câu
hỏi về các thực thể phổ biến vì ngữ cảnh được truy
xuất có thể gây hiểu lầm.
Kết quả là, chúng tôi đưa ra một phương pháp
LM tăng cường truy xuất đơn giản nhưng hiệu quả,
Truy Xuất Thích Ứng, kết hợp thích ứng bộ nhớ
tham số và phi-tham số dựa trên độ phổ biến (RQ
3). Phương pháp này cải thiện thêm hiệu suất trên
POPQA lên đến 10%, đồng thời giảm đáng kể chi
phí suy luận, đặc biệt với các LMs lớn hơn (ví dụ:
giảm chi phí API GPT-3 một nửa), cho thấy tiềm
năng cho nghiên cứu tương lai về LMs tăng cường
truy xuất hiệu quả và mạnh mẽ hơn.
2 Công trình liên quan
Kiến thức tham số và phi-tham số.
Petroni et al. (2019) chứng minh rằng các LMs
được tiền huấn luyện lớn như BERT (Devlin et al.,
2019) ghi nhớ một lượng đáng kể kiến thức thế giới
trong các tham số của chúng (kiến thức tham số),
và Roberts et al. (2020) cho thấy T5 được tinh chỉnh
mà không có bất kỳ tài liệu tham khảo nào (QA
sách đóng) có thể đạt được hiệu suất cạnh tranh
trên QA miền mở. Các LMs gần đây và mạnh mẽ
hơn (Brown et al., 2020; Chowdhery et al., 2022)
cải thiện thêm hiệu suất trên các nhiệm vụ chuyên
sâu về kiến thức đa dạng, tận dụng bộ nhớ tham số
mạnh mẽ của chúng (Kandpal et al., 2022; Yu et
al., 2022). Tuy nhiên, việc chỉ dựa vào các tham
số của chúng để mã hóa một kho tàng kiến thức
thế giới đòi hỏi một số lượng tham số cấm đoán
lớn và kiến thức có thể nhanh chóng trở nên lỗi
thời (Kasai et al., 2022; Jang et al., 2022). Công
trình gần đây cho thấy việc tăng cường LMs với
bộ nhớ phi-tham số (tức là các đoạn văn bản được
truy xuất) cho phép các mô hình nhỏ hơn nhiều
khớp với hiệu suất của các mô hình lớn hơn (Izacard et al., 2022b; Khandelwal et al., 2020; Min et
al., 2022), mặc dù Chen et al. (2022) và Longpre
et al. (2021) cho thấy ngay cả những mô hình đó
cũng có thể bỏ qua kiến thức phi-tham số và dựa
vào kiến thức tham số.
Hiểu về việc ghi nhớ. Một số công trình trước
đây thiết lập mối quan hệ tích cực giữa tần suất
chuỗi trong corpus tiền huấn luyện và việc ghi
nhớ (Carlini et al., 2022; Razeghi et al., 2022).
Đồng thời với công trình này, Kandpal et al. (2022)
cho thấy sự đồng xuất hiện của các thực thể câu
hỏi và câu trả lời trong corpus tiền huấn luyện có
tương quan tích cực với độ chính xác QA của mô
hình trên các benchmark QA miền mở phổ biến
như Natural Questions (Kwiatkowski et al., 2019).
Thay vào đó, công trình này cố gắng dự đoán việc
ghi nhớ chỉ sử dụng các biến có sẵn trong câu hỏi
đầu vào và sử dụng độ phổ biến để có được một
proxy cho việc một thực thể có khả năng được thảo
luận thường xuyên trên web như thế nào. Quan
trọng, bằng cách xây dựng một bộ dữ liệu mới,
chúng tôi có thể thực hiện các thí nghiệm được
kiểm soát tỉ mỉ trên một phạm vi rộng các độ phổ
biến, cho phép điều tra các giả thuyết có thể đã bị
bỏ lỡ trong phân tích trước đây sử dụng các bộ dữ
liệu QA mở hiện có. Chúng tôi phân tích thêm về
hiệu quả và hạn chế của

--- TRANG 3 ---
Lượt xem trang Wikipedia  
(Kathy Saltzman , 
nghề nghiệp , chính trị gia ) (Louisiana , thủ đô của , 
Baton Rouge ) 
H: Thủ đô của Louisiana  
là gì ? 
Đ: Baton Rouge  
H: Nghề nghiệp của Kathy  
Saltzman là gì ? 
Đ: Chính trị gia  
Bộ ba kiến thức 
từ Wikidata  2. Chuyển đổi bộ ba thành câu hỏi 1. Lấy mẫu kiến thức thực tế 
3. Thu thập độ phổ biến Hình 2: POPQA được tạo bằng cách lấy mẫu các bộ ba
kiến thức từ Wikidata và chuyển đổi chúng thành các câu
hỏi ngôn ngữ tự nhiên, tiếp theo là tính toán độ phổ biến.
LMs tăng cường truy xuất và giới thiệu Truy Xuất
Thích Ứng. Công trình trước đây điều tra hiệu quả
của việc quyết định khi nào sử dụng bộ nhớ phi-
tham số ở mức token trong kNN LM (He et al.,
2021; Drozdov et al., 2022). Công trình này là công
trình đầu tiên nghiên cứu hiệu quả của việc quyết
định có nên truy xuất cho mỗi truy vấn hay không
và cho thấy hiệu quả của chúng trong prompting
LM tăng cường truy xuất.
3 Thiết lập đánh giá
Chúng tôi đánh giá khả năng ghi nhớ kiến thức thực
tế của LMs thông qua các nhiệm vụ QA sách đóng
với các mẫu few-shot. Chúng tôi đánh giá LMs trên
bộ dữ liệu mới của chúng tôi, POPQA (Hình 2), và
EntityQuestions, cả hai đều có phân phối đuôi dài
(Hình 3).
101103105107
Độ phổ biến0.00.20.40.6Mật độNQ
EntityQuestions
PopQA
Hình 3: Phân phối độ phổ biến thực thể chủ đề cho En-
tityQuestions, POPQA, và cho NQ-open để tham khảo.
Chi tiết về các thực thể NQ có thể tìm thấy trong Phụ lục A.
3.1 Trọng tâm và nhiệm vụ
Trọng tâm: kiến thức thực tế. Trong số các loại
kiến thức thế giới đa dạng, công trình này tập trung
vào kiến thức thực tế (Adams, 2015) của các thực
thể—kiến thức về các chi tiết cụ thể của các thực
thể mục tiêu. Chúng tôi định nghĩa kiến thức thực
tế như một bộ ba (chủ thể, mối quan hệ, đối tượng)
như trong Hình 2 bên trái.
Định dạng nhiệm vụ: QA miền mở. Chúng tôi
công thức hóa nhiệm vụ như QA miền mở (Roberts
et al., 2020): cho một câu hỏi, một mô hình dự đoán
một câu trả lời mà không có bất kỳ đoạn văn chân
lý cơ sở nào được đưa trước.² Như trong Kandpal
et al. (2022), chúng tôi nghiên cứu các thiết lập
few-shot và prompt LMs mà không có bất kỳ cập
nhật tham số nào, thay vì tinh chỉnh chúng trên các
bộ dữ liệu QA như trong Roberts et al. (2020).
Thước đo: độ chính xác. Chúng tôi đánh dấu một
dự đoán là đúng nếu bất kỳ chuỗi con nào của dự
đoán khớp chính xác với bất kỳ câu trả lời vàng nào.
3.2 Các chiều kích phân tích
Chúng tôi đưa ra giả thuyết rằng kiến thức thực tế
ít được thảo luận thường xuyên trên web có thể
không được LMs ghi nhớ tốt. Nghiên cứu trước
đây thường sử dụng tần suất thuật ngữ của các
thực thể đối tượng trong corpus tiền huấn luyện
để hiểu việc ghi nhớ (Févry et al., 2020; Kandpal
et al., 2022; Razeghi et al., 2022). Thay vào đó,
chúng tôi điều tra liệu có thể dự đoán việc ghi nhớ
dựa trên chỉ thông tin đầu vào hay không, và sau
đó áp dụng những phát hiện để cải thiện mô hình,
khác với các phân tích trước đây. Do đó, công trình
của chúng tôi tập trung vào hai biến khác trong bộ
ba kiến thức thực tế: thực thể chủ thể và loại mối
quan hệ.
Độ phổ biến thực thể chủ thể. Chúng tôi sử dụng
độ phổ biến của các thực thể được đo bằng lượt
xem trang Wikipedia hàng tháng như một proxy
cho việc các thực thể có khả năng được thảo luận
thường xuyên trên web như thế nào, thay vì sử
dụng sự xuất hiện của các thực thể hoặc chuỗi
trong corpus tiền huấn luyện (Carlini et al., 2022;
Kandpal et al., 2022; Razeghi et al., 2022). Tính
toán tần suất trên các corpus tiền huấn luyện lớn
đòi hỏi tính toán khổng lồ để liên kết các thực thể
trên hàng tỷ token, hoặc có thể dẫn đến các ước
tính nhiễu.³ Các nghiên cứu ban đầu của chúng tôi
cho thấy điều này rẻ hơn nhiều⁴ và phù hợp tốt
với trực giác của chúng tôi.
Loại mối quan hệ. Chúng tôi cũng xem xét các
loại mối quan hệ như các yếu tố chính cho việc ghi
nhớ kiến thức thực tế. Ví dụ, ngay cả khi cho cùng
một sự kết hợp của các thực thể chủ thể và đối
tượng, hiệu suất mô hình có thể phụ thuộc vào các
loại mối quan hệ; các loại mối quan hệ được thảo
luận rộng rãi có thể dễ dàng được ghi nhớ hơn,
trong khi các loại ít được thảo luận có thể không
được ghi nhớ nhiều.
²Một số công trình thực hiện thăm dò kiến thức của các mô
hình chỉ encoder bằng cách điền các token [MASK] (Petroni
et al., 2019). Chúng tôi sử dụng các mô hình chỉ decoder và
do đó không sử dụng sơ đồ điền-vào-chỗ-trống này.
³Hơn nữa, một số mô hình gần đây như GPT-3 không công
bố corpus tiền huấn luyện của chúng, và đó là một câu hỏi mở
liệu tần suất trong corpus tiền huấn luyện có phản ánh tần suất
trong corpus riêng của chúng hay không.
⁴Chúng tôi có thể lấy lượt xem trang bằng cách gọi API
Wikipedia.

--- TRANG 4 ---
3.3 Benchmark
POPQA. Trong các nghiên cứu sơ bộ của chúng
tôi, chúng tôi phát hiện rằng các bộ dữ liệu QA
miền mở phổ biến hiện có như Natural Questions
(NQ; Kwiatkowski et al. 2019) thường bị chi phối
bởi các thực thể chủ thể có độ phổ biến cao, và
thường khó xác định các loại mối quan hệ do các
hình thức bề mặt câu hỏi đa dạng. Để có thể phân
tích tỉ mỉ về việc ghi nhớ dựa trên các chiều kích
phân tích nói trên, chúng tôi xây dựng POPQA,
một bộ dữ liệu QA miền mở tập trung vào thực thể
quy mô lớn mới về các thực thể với nhiều loại độ
phổ biến đa dạng, như được hiển thị trong Hình 3.
Để xây dựng POPQA, chúng tôi ngẫu nhiên lấy
mẫu các bộ ba kiến thức của 16 loại mối quan hệ
đa dạng từ Wikidata và chuyển đổi chúng thành
các câu hỏi ngôn ngữ tự nhiên, sử dụng một mẫu
ngôn ngữ tự nhiên (được mô tả trong Hình 2).
Chúng tôi ngôn ngữ hóa một bộ ba kiến thức (S, R,
O) thành một câu hỏi bao gồm việc thay thế chủ
thể S vào một mẫu được viết thủ công cho loại mối
quan hệ R. Danh sách đầy đủ các mẫu được tìm
thấy trong Bảng 2 của Phụ lục. Tập hợp các câu
trả lời chấp nhận được cho câu hỏi là tập hợp các
thực thể E sao cho (S, R, E) tồn tại trong đồ thị
kiến thức. Chúng tôi đã thử nhiều mẫu khác nhau
và nhận thấy rằng kết quả khá vững với các mẫu.
Vì POPQA được căn cứ vào một cơ sở kiến thức,
các liên kết đến các thực thể Wikidata cho phép
phân tích đáng tin cậy về độ phổ biến và loại mối
quan hệ.
EntityQuestions. Chúng tôi kiểm tra trên một bộ
dữ liệu QA miền mở phổ biến khác, EntityQuestions
(Sciavolino et al., 2021), cũng bao phủ một phân
phối thực thể đuôi dài. Họ sử dụng số lượng liên
kết Wikipedia như một proxy của tần suất của các
thực thể và lấy mẫu các bộ ba kiến thức từ WikiData, từ các phân phối tần suất. Khác với POPQA,
EntityQuestions không cung cấp chú thích thực thể,
vì vậy chúng tôi chỉ sử dụng 82% câu hỏi, nơi sự
đề cập đến thực thể chủ thể có một khớp duy nhất
với một thực thể Wikidata.
4 Việc ghi nhớ phụ thuộc vào độ phổ biến
và loại mối quan hệ
Chúng tôi đánh giá một loạt LMs với số lượng tham
số khác nhau, để định lượng LMs ghi nhớ bao nhiêu
kiến thức thực tế và các yếu tố khác nhau ảnh hưởng
đến những hành vi ghi nhớ đó như thế nào (RQ 1).
4.1 Thiết lập thí nghiệm
Các mô hình. Chúng tôi đánh giá mười mô hình
với quy mô kích thước mô hình khác nhau: OPT
(Zhang et al. 2022; 1.3, 2.7, 6.7, và 13 tỷ), GPT-
Neo (Black et al. 2022; 1.3, 2.7, 6, và 20 tỷ), và
GPT-3 (Brown et al. 2020; davinci-002, davinci-
003) trên benchmark của chúng tôi mà không có
bất kỳ tinh chỉnh nào.⁵
Hướng dẫn và minh họa. Chúng tôi sử dụng một
mẫu đơn giản "Q: <câu hỏi> A: " để định dạng
tất cả các câu hỏi của chúng tôi cho dự đoán sinh
tạo. Các hướng dẫn phức tạp hơn đã được thử
trong các thí nghiệm sơ bộ nhưng chúng không
cải thiện so với mẫu đơn giản đủ đáng kể để xứng
đáng sử dụng chúng, đặc biệt là xét đến chúng có
thể overfitting với mô hình. Trong khi chúng tôi sử
dụng prompting zero-shot cho GPT-3 để giảm chi
phí API,⁶ chúng tôi sử dụng prompting 15-shot cho
tất cả các mô hình GPT-neo và OPT.
4.2 Kết quả
Hiệu suất tổng thể của mô hình. Cột trên cùng bên
trái của Hình 4 minh họa hiệu suất tổng thể trên
POPQA. Như được hiển thị, ngay cả khi không sử
dụng các ví dụ trong ngữ cảnh, các LMs lớn hơn
thể hiện hiệu suất hợp lý: GPT-3 đạt độ chính xác
35%, và GPT-Neo 20B đạt độ chính xác 25%. Điều
này chỉ ra rằng các LMs lớn ghi nhớ kiến thức thực
tế trong các tham số của chúng ở một mức độ nào
đó. Phần này xem xét những loại kiến thức nào
được ghi nhớ tốt hơn và những yếu tố nào ảnh
hưởng đến việc ghi nhớ.
Độ phổ biến thực thể chủ thể dự đoán việc ghi
nhớ. Hình 4 (dưới) cho thấy có tương quan tích
cực giữa độ phổ biến thực thể chủ thể và độ chính
xác của mô hình cho hầu như tất cả các loại mối
quan hệ. Điều này hỗ trợ giả thuyết của chúng tôi
rằng độ phổ biến thực thể chủ thể có thể là một
chỉ báo đáng tin cậy về việc ghi nhớ kiến thức thực
tế của LMs. Nhìn chung, các tương quan giữa độ
phổ biến thực thể chủ thể và độ chính xác mạnh
hơn đối với các LMs lớn hơn; GPT-3 003 cho thấy
tương quan tích cực cao nhất (khoảng 0.4) trong
khi GPT-Neo-1.3B cho thấy tương quan tích cực
tương đối yếu (khoảng 0.1).
Loại mối quan hệ ảnh hưởng đến việc ghi nhớ.
Chúng tôi thấy rằng các mô hình có hiệu suất trung
bình cao hơn cho một số loại mối quan hệ so với
những loại khác. Mặc dù đây là bằng chứng rằng
kiến thức thực tế của một số loại mối quan hệ dễ
dàng được ghi nhớ hơn những loại khác, chúng tôi
cũng quan sát thấy rằng các câu hỏi về những loại
mối quan hệ nhất định có thể dễ dàng được đoán
mà không cần ghi nhớ bộ ba kiến thức. Cụ thể,
những loại mối quan hệ nhất định (ví dụ: quốc tịch)
cho phép các mô hình
⁵Chúng tôi không khám phá các mô hình encoder-decoder
được sử dụng rộng rãi như T5, vì việc tiền huấn luyện có
giám sát của chúng bao gồm QA.
⁶Sử dụng prompt 15-shot cho GPT-3 sẽ tốn hơn $3000 cho
sự kết hợp của các đánh giá vanilla, Contriever, BM25, và
GenRead trên davinci-002 và davinci-003.

--- TRANG 5 ---
0.00.20.40.60.81.0Độ chính xác
Tổng thể
(n=14267)Tổng thể
(n=14267)nghề nghiệp
(n=532)tác giả
(n=1514)đạo diễn
(n=1999)quốc gia
(n=838)thủ đô của
(n=363)thủ đô
(n=645)tôn giáo
(n=338)thể thao
(n=547)
Mối quan hệ0.00.20.40.60.81.0Tương quan giữa
log(độ phổ biến) và độ chính xácGPT-neo 1.3B
OPT 1.3BGPT-neo 2.7B
OPT 2.7BGPT-j 6B
OPT 6.7BOPT 13B
GPT-neox 20BGPT-3 davinci 002
GPT-3 davinci 003Hình 4: Kết quả theo loại mối quan hệ (n=số lượng câu hỏi) trên POPQA theo mô hình, cho thấy độ chính xác
tổng thể và tương quan giữa độ chính xác và log độ phổ biến. Chúng tôi báo cáo tương quan giữa log-độ phổ biến của
thực thể chủ thể câu hỏi và chỉ báo nhị phân của việc câu hỏi có được trả lời đúng hay không. Chúng tôi thấy rằng cả
độ phổ biến thực thể chủ thể và loại mối quan hệ đều là những yếu tố dự đoán mạnh mẽ của việc ghi nhớ trên các
mô hình. Tương quan với độ phổ biến tồn tại trên các loại mối quan hệ và mạnh hơn đối với các LMs lớn hơn. Chúng
tôi hiển thị một tập hợp con đại diện của các loại mối quan hệ và kết quả đầy đủ có trong Hình 16 và 17 trong Phụ lục
C.1, bao gồm kết quả trên EntityQuestions.
khai thác các hiện tượng bề mặt trong tên thực thể
chủ thể (Poerner et al., 2020; Cao et al., 2021).
Ngoài ra, các mô hình thường xuất ra các thực thể
câu trả lời chi phối nhất cho các câu hỏi về loại
mối quan hệ với ít thực thể câu trả lời hơn (ví dụ:
đỏ cho loại mối quan hệ màu sắc). Trong Hình 4,
các mối quan hệ với tương quan thấp hơn (ví dụ:
quốc gia, thể thao) thường cho thấy độ chính xác
cao hơn, chỉ ra rằng trên những loại mối quan hệ đó,
các mô hình có thể khai thác các manh mối bề mặt.
Mặt khác, đối với các loại mối quan hệ với độ chính
xác tương đối thấp (ví dụ: nghề nghiệp, tác giả,
đạo diễn), các LMs lớn hơn thường cho thấy tương
quan cao. Chi tiết thêm có trong Phụ lục C.1.
Việc mở rộng có thể không giúp ích với kiến thức
đuôi. Như được thấy trong cột bên trái của Hình
4, có những cải thiện hiệu suất tổng thể rõ ràng
với quy mô trên bộ dữ liệu POPQA. Tuy nhiên,
Hình 5 cho thấy rằng trên cả POPQA và EntityQuestions, phần lớn tác động tích cực của việc mở
rộng đối với kiến thức tham số đến từ các câu hỏi
có độ phổ biến cao. Cụ thể, đối với các câu hỏi về
các thực thể có log10(độ phổ biến) lớn hơn 4, có
sự cải thiện độ chính xác khi kích thước mô hình
tăng (đường màu đỏ và vàng), trong khi hiệu suất
trên các câu hỏi với độ phổ biến thấp hơn vẫn tương
đối không đổi (đường màu xanh lam và xanh lá).
Đối với 4.000 câu hỏi ít phổ biến nhất, GPT-Neo
6B, 20B, và GPT-3 davinci-003 có độ chính xác
15%, 16%, và 19%, tương ứng.
Điều này phần nào làm giảm những phát hiện
của các công trình trước đây rằng việc mở rộng mô
hình cải thiện đáng kể việc
1.3B 2.7B 6B 20B ~175B0.00.20.40.60.81.0Độ chính xác
PopQA
GPT-neo
OPTGPT-3 002
GPT-3 003
102103104105
Độ phổ biến
1.3B 2.7B 6B 20B ~175B
Tham số mô hình0.00.20.40.6Độ chính xác
EntityQuestions
102103104105
Độ phổ biến
Hình 5: Kết quả mở rộng POPQA, được chia theo mức
độ phổ biến câu hỏi. Việc mở rộng chủ yếu cải thiện việc
ghi nhớ kiến thức thực tế phổ biến hơn. Thanh lỗi là
khoảng tin cậy 95%.

--- TRANG 6 ---
ghi nhớ kiến thức thực tế của chúng (Roberts et al.,
2020; Kandpal et al., 2022). Chúng tôi đưa ra giả
thuyết rằng điều này là do các đánh giá của họ thường được thực hiện trên các bộ dữ liệu QA với các
thực thể phổ biến. Tóm lại, việc mở rộng hạ thấp
ngưỡng độ phổ biến để kiến thức được ghi nhớ một
cách đáng tin cậy, nhưng không được dự kiến sẽ
di chuyển ngưỡng xa vào đuôi dài cho các quy mô
mô hình thực tế.
Kết quả phân tích loại mối quan hệ. Hình 6 cung
cấp cái nhìn gần hơn về mối quan hệ giữa độ phổ
biến, độ chính xác, và loại mối quan hệ; nó cho
thấy độ chính xác mô hình trên các phân phối độ
phổ biến cho director và country. Đối với hai loại
đầu tiên, chúng ta có thể thấy một xu hướng tích
cực rõ ràng giữa độ phổ biến và độ chính xác trên
các mô hình, và khi kích thước mô hình lớn hơn,
các LMs ghi nhớ nhiều hơn. Mặt khác, trong loại
mối quan hệ "country", không có mô hình nào cho
thấy xu hướng, trong khi tổng thể độ chính xác cao,
chỉ ra các LMs thường khai thác các hiện tượng để
trả lời các câu hỏi ít phổ biến hơn. Chúng tôi hiển
thị các ví dụ dự đoán của mô hình trong Phần Phụ
lục C.3.
5 Bộ nhớ phi-tham số bổ sung cho
bộ nhớ tham số
Phân tích của chúng tôi chỉ ra rằng ngay cả các
LMs tiên tiến nhất hiện tại cũng gặp khó khăn với
các chủ thể ít phổ biến hơn hoặc những loại mối
quan hệ nhất định, và việc tăng kích thước mô hình
không dẫn đến những cải thiện hiệu suất thêm. Có
tâm đến điều này, chúng tôi mở rộng phân tích của
mình đến các nguồn kiến thức phi-tham số, như
được nêu trong Phần (RQ 2). Cụ thể, chúng tôi
điều tra hiệu quả của các LMs tăng cường truy xuất
(Borgeaud et al., 2022; Lewis et al., 2020), tận
dụng bộ nhớ phi-tham số (tức là văn bản được truy
xuất) để cải thiện hiệu suất.
5.1 Thiết lập thí nghiệm
Tăng cường đầu vào. Trong công trình này, chúng
tôi thử một cách tiếp cận LM tăng cường truy xuất
đơn giản, nơi chúng tôi chạy một hệ thống truy
xuất có sẵn ngoại tuyến để truy xuất ngữ cảnh từ
Wikipedia có liên quan đến một câu hỏi,⁷ và sau đó
chúng tôi nối ngữ cảnh được truy xuất với câu hỏi
gốc. Mặc dù việc tăng kích thước ngữ cảnh thường
dẫn đến các lợi ích hiệu suất (Izacard and Grave,
2021; Asai et al., 2022), chúng tôi chỉ sử dụng đoạn
văn được truy xuất hàng đầu cho sự đơn giản.
⁶630 câu hỏi POPQA và 26 câu hỏi EntityQuestions có độ
phổ biến thấp hơn bin độ phổ biến nhỏ nhất, và được loại trừ
để tránh hiển thị kết quả cho các kích thước mẫu nhỏ.
⁷Chúng tôi sử dụng dump Wikipedia từ tháng 12 năm 2018.
0.00.51.0GPT-neo 1.3B OPT 13BdirectorGPT-3 davinci-003
100102104106
Độ phổ biến0.00.51.0Độ chính xác
100102104106100102104106 countryHình 6: Việc ghi nhớ so với độ phổ biến cho ba mô hình
và các loại mối quan hệ với tương quan lớn nhất và nhỏ
nhất. Trong một loại mối quan hệ, nhìn chung, có một
liên kết tăng đơn điệu giữa độ phổ biến và hiệu suất,
ngoại trừ "country". Thanh lỗi hiển thị khoảng tin cậy
Wilson 95%.
Các mô hình truy xuất. Chúng tôi sử dụng hai hệ
thống truy xuất được sử dụng rộng rãi: BM25 (Robertson et al., 2009) và Contriever (Izacard et al.,
2022a). BM25 là một retriever tĩnh dựa trên thuật
ngữ không có huấn luyện, trong khi Contriever
được tiền huấn luyện trên corpus không nhãn lớn,
tiếp theo là tinh chỉnh trên MS MARCO (Bajaj et
al., 2016). Chúng tôi cũng thử nghiệm với một
phương pháp tăng cường tham số, GenRead (Yu et
al., 2022), prompt các LMs để sinh ra thay vì truy
xuất một tài liệu ngữ cảnh để trả lời một câu hỏi.
Chúng tôi sử dụng mười LMs trong Phần 4, dẫn
đến 40 LMs và LMs tăng cường truy xuất.
GPT-neo 1.3B GPT-neo 2.7B GPT-j 6B GPT-3 davinci 0030.00.10.20.30.40.5Độ chính xácVanilla
GenReadBM25
Contriever
Hình 7: Độ chính xác POPQA của các LMs được tăng
cường với BM25, Contriever, GenRead, và không hỗ trợ
(vanilla). Việc truy xuất bộ nhớ phi-tham số cải thiện
đáng kể hiệu suất của các mô hình nhỏ hơn. Kết quả
đầy đủ trên POPQA được tìm thấy trong Hình 13. Kết
quả EntityQuestions có trong Hình 14 của Phụ lục.
5.2 Kết quả
Truy xuất cải thiện đáng kể hiệu suất. Hình 7
cho thấy việc tăng cường LMs với bộ nhớ phi-tham
số vượt trội đáng kể so với LMs vanilla không hỗ
trợ. Một LM nhỏ hơn nhiều (ví dụ: GPT-Neo 2.7B)
được tăng cường bởi kết quả truy xuất Contriever
vượt trội hơn vanilla GPT-3. Các LMs lớn như
GPT-3 cũng hưởng lợi từ bộ nhớ phi-tham số. Contriever mang lại 7% cải thiện độ chính xác trên đỉnh
GPT-3 davinci-003. GenRead cho thấy cải thiện
hiệu suất ít hoặc không có gì so với kiến thức tham
số vanilla đối với các mô hình nhỏ hơn, trong khi
kỹ thuật này cho thấy những lợi ích đáng kể đối
với GPT-3, đặc biệt là davinci-003. Ngoài hiệu
quả hạn chế với các LMs nhỏ hơn, GenRead có chi
phí thời gian suy luận có thể cấm đoán, với GPT-
NeoX 20B mất 70 giây mỗi truy vấn.
Vanilla GenRead BM25 Contriever
2
 1
 0 1 20.20.40.60.8Độ chính xácPopQA
2
 1
 0 1 2 3
Độ phổ biến tương đối0.20.30.40.5Độ chính xácEntityQuestions
Hình 8: Độ chính xác GPT-3 davinci-003 so với độ
phổ biến tương đối (mức độ phổ biến của một câu hỏi
so với các câu hỏi khác cùng loại mối quan hệ). LMs
tăng cường truy xuất (nét đứt) vượt trội hơn bộ nhớ
tham số của LMs (nét liền) đối với các thực thể ít phổ
biến hơn, trong khi bộ nhớ tham số cạnh tranh đối với
các thực thể phổ biến hơn. Độ phổ biến tương đối được
định nghĩa là log-độ phổ biến của một câu hỏi, được
chuẩn hóa bởi trung bình và độ lệch chuẩn của log-độ
phổ biến cho loại mối quan hệ của câu hỏi (nhỏ hơn
đối với các thực thể ít phổ biến hơn).⁸ Hình 17 hiển thị
kết quả theo từng mối quan hệ.
Bộ nhớ phi-tham số hiệu quả đối với các sự thật
ít phổ biến hơn. Việc tăng cường truy xuất dẫn đến
những cải thiện đáng kể như vậy như thế nào? Hình
8 cho thấy mối quan hệ giữa độ phổ biến thực thể
và hiệu suất QA của mô hình. Có thể thấy rằng các
LMs tăng cường truy xuất được hướng dẫn bởi Contriever hoặc BM25 có lợi thế rõ ràng so với LMs
vanilla không hỗ trợ, đặc biệt trên các thực thể ít
phổ biến hơn, dẫn đến một lợi ích hiệu suất đáng
kể. Tổng thể, các LMs được hướng dẫn Contriever
vượt trội hơn những LMs dựa trên BM25 trên POPQA, trong khi các mô hình dựa trên BM25 hoạt
động tốt hơn trên các thực thể ít phổ biến nhất,
phù hợp với những phát hiện từ Sciavolino et al.
(2021). Mặt khác, đối với các thực thể phổ biến
hơn, kiến thức tham số cho thấy độ chính xác bằng
hoặc cao hơn, chỉ ra rằng các LMs tiên tiến nhất
đã ghi nhớ các câu trả lời, và việc tăng cường đầu
vào với ngữ cảnh được truy xuất không giúp ích
nhiều hoặc thậm chí làm tổn hại hiệu suất. Thú vị,
GenRead nhìn chung vượt trội hơn LMs vanilla mặc
dù dựa vào bộ nhớ tham số của LMs. Điều này
chứng minh hiệu quả của prompting elicitive (Wei
et al., 2022; Sun et al., 2022) như quan sát thấy
trong công trình trước đây. Tuy nhiên, giống như
LMs vanilla, GenRead cho thấy hiệu suất thấp trên
các thực thể ít phổ biến hơn.
Bộ nhớ phi-tham số có thể làm hiểu lầm LMs.
Chúng tôi thực hiện phân tích chuyên sâu về lý do
tại sao các mô hình tăng cường truy xuất gặp khó
khăn ở các thực thể phổ biến hơn. Chúng tôi đưa
ra giả thuyết rằng kết quả truy xuất có thể không
luôn đúng hoặc hữu ích, và có thể làm hiểu lầm
LMs. Để kiểm tra giả thuyết này, chúng tôi nhóm
các câu hỏi dựa trên hai trục: liệu GPT-3 davinci-
003 không hỗ trợ có dự đoán đúng hay không, và
liệu các dự đoán tăng cường truy xuất có đúng hay
không. Đối với mỗi trong bốn danh mục, chúng
tôi tính recall@1 (liệu một câu trả lời vàng có được
bao gồm trong tài liệu top 1 hay không; Karpukhin
et al. 2020).
Bảng 1 hiển thị recall@1 cho mỗi nhóm với tỷ
lệ phần trăm các câu hỏi rơi vào mỗi danh mục.
Đối với 10% câu hỏi, việc tăng cường truy xuất
khiến LM trả lời không đúng một câu hỏi mà nó
có thể trả lời đúng. Chúng tôi thấy rằng trên những
câu hỏi đó, recall@1 thấp hơn đáng kể so với
recall@1 tổng thể (0.14 so với 0.42 tổng thể), chỉ
ra rằng truy xuất thất bại có thể dẫn đến giảm hiệu
suất. Ngược lại, đối với 17% câu hỏi mà việc truy
xuất khiến LM trả lời đúng một câu hỏi mà nó có
thể đã thất bại, recall@1 là 0.88. Chúng tôi bao
gồm các ví dụ của cả hai trường hợp trong Phần
Phụ lục C.3.
⁸Thanh lỗi hiển thị khoảng tin cậy Wilson 95%. Các bins
với ít hơn 40 mẫu đã được loại trừ để tránh hiển thị kết quả
với thanh lỗi cực kỳ rộng.

--- TRANG 7 ---
LM tăng cường Contriever
thành công thất bại
LM thành công 0.83 (24%) 0.14 (10%)
LM thất bại 0.88 (17%) 0.11 (49%)
Bảng 1: Recall@1 của Contriever cho các câu hỏi mà
GPT-3 davinci-003 trả lời đúng và sai với và không có
truy xuất trên POPQA. Tỷ lệ phần trăm câu hỏi rơi vào
mỗi danh mục được hiển thị trong ngoặc đơn. Đối với
10% câu hỏi, truy xuất có hại do văn bản được truy xuất
chất lượng thấp (0.14 recall@1).
6 Truy Xuất Thích Ứng: Chỉ sử dụng truy xuất
khi nó giúp ích
Trong khi việc kết hợp bộ nhớ phi-tham số giúp
ích trong các phân phối đuôi dài, các LMs mạnh
mẽ đã ghi nhớ kiến thức thực tế cho các thực thể
phổ biến, và việc tăng cường truy xuất có thể có
hại. Như được nêu trong (RQ 3), chúng ta có thể
đạt được điều tốt nhất của cả hai thế giới không?
Chúng tôi đề xuất một phương pháp đơn giản nhưng
hiệu quả, Truy Xuất Thích Ứng, quyết định khi
nào truy xuất các đoạn văn chỉ dựa trên thông tin
truy vấn đầu vào và tăng cường đầu vào với bộ
nhớ phi-tham số được truy xuất chỉ khi cần thiết.
Chúng tôi cho thấy rằng điều này không chỉ mạnh
mẽ hơn các LMs hoặc LMs tăng cường truy xuất
luôn truy xuất ngữ cảnh, mà còn hiệu quả hơn thiết
lập tăng cường truy xuất tiêu chuẩn.
6.1 Phương pháp
Truy Xuất Thích Ứng dựa trên những phát hiện
của chúng tôi: vì các LMs tốt nhất hiện tại đã ghi
nhớ kiến thức phổ biến hơn, chúng ta có thể sử
dụng truy xuất chỉ khi chúng không ghi nhớ kiến
thức thực tế và do đó cần tìm kiến thức phi-tham
số bên ngoài. Cụ thể, chúng tôi sử dụng truy xuất
cho các câu hỏi có độ phổ biến thấp hơn một ngưỡng
(ngưỡng độ phổ biến), và đối với các thực thể phổ
biến hơn, hoàn toàn không sử dụng truy xuất.
Sử dụng một tập phát triển, ngưỡng được chọn
để tối đa hóa độ chính xác thích ứng, mà chúng
tôi định nghĩa là độ chính xác đạt được bằng cách
lấy các dự đoán của hệ thống tăng cường truy xuất
cho các câu hỏi dưới ngưỡng độ phổ biến và các
dự đoán dựa trên kiến thức tham số cho phần còn
lại. Chúng tôi xác định ngưỡng độ phổ biến độc
lập cho mỗi loại mối quan hệ.
GPT-neo 2.7B GPT-j 6B GPT-neox 20B GPT-3 davinci 0030.00.20.40.6Độ chính xácVanilla
BM25Adaptive Vanilla/BM25
Hình 9: Hiệu suất POPQA của các mô hình GPT-neo
và GPT3 davinci-003, với các phương pháp truy xuất
khác nhau. Truy Xuất Thích Ứng vượt trội vững chắc
hơn các cách tiếp cận luôn truy xuất, đặc biệt đối với
các LMs lớn hơn.
6.2 Kết quả
Truy Xuất Thích Ứng cải thiện hiệu suất.
Hình 9 cho thấy kết quả khi chúng tôi truy xuất
thích ứng bộ nhớ phi-tham số dựa trên ngưỡng
theo loại mối quan hệ. Chúng ta có thể thấy rằng
việc truy xuất thích ứng bộ nhớ phi-tham số hiệu
quả đối với các mô hình lớn hơn. Hiệu suất tốt
nhất trên POPQA là sử dụng GPT-3 davinci-003
thích ứng với GenRead và Contriever, mang lại độ
chính xác 46.5%, cao hơn 5.3% so với bất kỳ phương pháp không thích ứng nào.
1.3B 2.7B 6B 20B ~175B
Tham số mô hình0.00.20.40.60.81.0Tỷ lệ sử dụng truy xuất
Adaptive Vanilla/BM25
Adaptive Vanilla/Contriever
Adaptive GenRead/BM25
Adaptive GenRead/Contriever
GPT-neo
GPT-3 davinci 003
Hình 10: Tỷ lệ câu hỏi mà các mô hình khác nhau sử
dụng truy xuất trong thiết lập Truy Xuất Thích Ứng trên
POPQA. Khi sử dụng Truy Xuất Thích Ứng, các mô
hình nhỏ vẫn phải dựa vào bộ nhớ phi-tham số cho
hầu hết các câu hỏi, trong khi các mô hình lớn hơn có
bộ nhớ tham số đáng tin cậy hơn cho phép chúng sử
dụng truy xuất ít thường xuyên hơn.
Ngưỡng thay đổi theo quy mô LM. Trong khi
Truy Xuất Thích Ứng cho thấy lợi ích hiệu suất
đối với các mô hình lớn hơn, các mô hình nhỏ hơn
không nhận ra những lợi ích tương tự; như được
hiển thị trong Hình 9, lợi ích hiệu suất từ Truy
Xuất Thích Ứng nhỏ hơn nhiều khi chúng tôi sử
dụng các mô hình nhỏ hơn 10 tỷ. Tại sao điều này
xảy ra? Hình 10 cho thấy các LMs nhỏ hơn hầu
như luôn truy xuất, chỉ ra rằng không có nhiều câu
hỏi mà kiến thức tham số của LMs nhỏ đáng tin
cậy hơn bộ nhớ phi-tham số. Ngược lại, các mô
hình lớn thường truy xuất ít hơn nhiều. Ví dụ: GPT-
3 davinci-003 chỉ truy xuất cho 40% câu hỏi khi
được ghép nối với BM25, và ngay cả GPT-neox
20B nhỏ hơn nhiều cũng không truy xuất tài liệu
trên hơn 20% câu hỏi. Trên EntityQuestions (Phụ
lục Hình 15) tất cả các LMs truy xuất nhiều hơn
nhiều, vì các câu hỏi chủ yếu về các thực thể ít
phổ biến hơn.
Truy Xuất Thích Ứng giảm chi phí thời gian suy
luận. Chúng tôi cũng thấy rằng Truy Xuất Thích
Ứng cải thiện hiệu quả; nếu chúng ta biết chúng ta
không cần truy xuất tài liệu, chúng ta có thể bỏ
qua các thành phần truy xuất và độ dài đầu vào trở
nên ngắn hơn, điều này cải thiện độ trễ trong cả
thành phần truy xuất và mô hình ngôn ngữ. Hình
11 hiển thị độ trễ suy luận của GPT-J 6B và GPT-
neox 20B, và chi phí API của GPT-3. Đặc biệt đối
với các LMs lớn hơn, việc nối

--- TRANG 8 ---
GPT-j 6B GPT-neox 20B0.02.55.07.5Độ trễ (giây)Vanilla
BM25
Adaptive Vanilla/BM25
GPT-3 davinci 0030123Chi phí ($/1000 truy vấn)Hình 11: Độ trễ POPQA cho các mô hình GPT-neo lớn
được chạy trên máy của chúng tôi, và chi phí API cho
GPT3. Truy xuất thích ứng giảm độ trễ và chi phí API.
GPT-3 davinci 0030.000.250.500.75Độ chính xácVanilla
BM25
Adaptive Vanilla/BM25
GPT-3 davinci 0030123Chi phí ($/1000 truy vấn)
Hình 12: Độ chính xác và tiết kiệm chi phí của Truy
Xuất Thích Ứng cho EntityQuestions. Mặc dù thiếu
các thực thể phổ biến của EntityQuestions (xem Hình
3), Truy Xuất Thích Ứng có thể giảm chi phí API 15%
trong khi duy trì hiệu suất tương đương với chỉ truy xuất.
ngữ cảnh được truy xuất dẫn đến độ trễ tăng đáng
kể (ví dụ: đối với GPT-J 6B, thời gian độ trễ suy
luận gần như tăng gấp đôi). Truy xuất thích ứng
cho phép giảm thời gian suy luận lên đến 9% từ
truy xuất tiêu chuẩn. Chúng tôi cũng quan sát thấy
giảm chi phí trên EntityQuestions, như được hiển
thị trong Hình 12.
7 Thảo luận và kết luận
Công trình này thực hiện thăm dò kiến thức quy
mô lớn để xem xét hiệu quả và hạn chế của việc
dựa vào các tham số của LMs để ghi nhớ kiến thức
thực tế và để hiểu những yếu tố nào ảnh hưởng đến
việc ghi nhớ kiến thức thực tế. Kết quả của chúng
tôi cho thấy việc ghi nhớ có tương quan mạnh với
độ phổ biến thực thể và việc mở rộng các mô hình
trên các phân phối đuôi dài có thể chỉ mang lại
những cải thiện biên. Chúng tôi cũng chứng minh
rằng bộ nhớ phi-tham số có thể hỗ trợ đáng kể các
LMs trên những phân phối đuôi dài này, nhưng
cũng có thể làm hiểu lầm LMs trên các câu hỏi về
các thực thể nổi tiếng, vì các LMs mạnh mẽ đã ghi
nhớ chúng trong các tham số của chúng. Dựa trên
những phát hiện đó, chúng tôi đưa ra Truy Xuất
Thích Ứng đơn giản nhưng hiệu quả, chỉ truy xuất
khi cần thiết, sử dụng một heuristic dựa trên độ
phổ biến thực thể và loại mối quan hệ. Kết quả thí
nghiệm của chúng tôi cho thấy phương pháp này
không chỉ mạnh mẽ hơn các LMs hoặc LMs tăng
cường truy xuất trước đây mà còn hiệu quả hơn.
Hạn chế
Công trình này tập trung vào kiến thức thực tế tập
trung vào thực thể và chứng minh rằng việc ghi
nhớ của LMs bị ảnh hưởng mạnh mẽ bởi độ phổ
biến của các thực thể và khía cạnh của các thực
thể được hỏi trong các câu hỏi. Quan trọng là nhấn
mạnh rằng để chạy các thí nghiệm được kiểm soát,
chúng tôi đã dựa vào hai bộ dữ liệu tổng hợp, và
mức độ mà kết quả của chúng tôi áp dụng cho kiến
thức thực tế xảy ra tự nhiên chưa được thiết lập
chắc chắn. Trong khi chúng ta có thể khá tự tin về
mối quan hệ giữa việc mở rộng, truy xuất, độ phổ
biến, loại mối quan hệ, và hiệu suất cho các loại
kiến thức được nghiên cứu ở đây, hiệu quả của
Truy Xuất Thích Ứng sẽ phụ thuộc vào nhiều chi
tiết của pipeline trả lời câu hỏi. Hơn nữa, công
trình của chúng tôi phụ thuộc vào một định nghĩa
về độ phổ biến phụ thuộc vào thời gian và có thể
không phản ánh hoàn hảo việc các thực thể được
thảo luận thường xuyên trên web như thế nào. Lượt
xem trang Wikipedia là một định nghĩa có thể về
độ phổ biến mà chúng tôi quan sát thấy kết quả
của mình, và chúng tôi mời những người khác cải
thiện nó trong công trình tương lai. Nghiên cứu
thêm có thể mở rộng cách tiếp cận đơn giản này,
có lẽ dựa trên những hiểu biết từ Kadavath et al.
(2022) để cải thiện hiệu quả của Truy Xuất Thích
Ứng.
Đó là một câu hỏi mở liệu những phát hiện tương
tự có áp dụng được cho các loại kiến thức thế giới
khác như thường thức hay không. Chúng tôi phỏng
đoán rằng khái niệm về chủ đề chủ thể (thực thể),
cũng như khía cạnh (loại mối quan hệ), có thể được
áp dụng với một số sửa đổi nhỏ, mà công trình
tương lai có thể định lượng việc ghi nhớ theo sơ
đồ của chúng tôi.
Cân nhắc đạo đức
Công trình gần đây (Huang et al., 2022) cho thấy
LMs ghi nhớ thông tin cá nhân có sẵn trên web,
có những vấn đề bảo mật đáng kể. Đánh giá của
chúng tôi tập trung vào việc ghi nhớ kiến thức tập
trung vào thực thể tổng quát, nhưng những phát
hiện của chúng tôi có thể áp dụng cho những lĩnh
vực đó. Những phát hiện của chúng tôi gợi ý rằng
LMs có khả năng có kiến thức ít đáng tin cậy hơn
về các nhóm thiểu số. Parrish et al. (2022) thiết
lập rằng các mô hình thường dựa vào các khuôn
mẫu để trả lời trong các trường hợp không chắc
chắn, vì vậy kết quả của chúng tôi chỉ ra rằng LMs
có khả năng dựa vào các khuôn mẫu không cân
xứng đối với các nhóm thiểu số. Công trình tương
lai có thể điều tra liệu việc tăng cường truy xuất có
giảm thiên kiến trong những trường hợp này hay
không.

--- TRANG 9 ---
Lời cảm ơn
Chúng tôi cảm ơn các thành viên nhóm UW NLP
vì những thảo luận hữu ích của họ, và Joongwon
Kim, Wenya Wang, và Sean Welleck vì phản hồi
sâu sắc của họ về bài báo này. Nghiên cứu này
được hỗ trợ bởi NSF IIS-2044660, ONR N00014-
18-1-2826, ONR MURI N00014-18-1-2670, và
Allen Distinguished Award. AM được tài trợ bởi
Học bổng Goldwater và AA được tài trợ bởi Học
bổng Tiến sĩ IBM.
Tài liệu tham khảo
Nancy E Adams. 2015. Bloom's taxonomy of cognitive
learning objectives. Journal of the Medical Library
Association.
Akari Asai, Matt Gardner, and Hannaneh Ha-
jishirzi. 2022. Evidentiality-guided generation for
knowledge-intensive NLP tasks. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies.
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,
Jianfeng Gao, Xiaodong Liu, Rangan Majumder, An-
drew McNamara, Bhaskar Mitra, Tri Nguyen, et al.
2016. MS MARCO: A human generated machine
reading comprehension dataset.
Sidney Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace
He, Connor Leahy, Kyle McDonell, Jason Phang,
Michael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-
hit, Laria Reynolds, Jonathan Tow, Ben Wang, and
Samuel Weinbach. 2022. GPT-NeoX-20B: An open-
source autoregressive language model. In Proceed-
ings of BigScience Episode #5 – Workshop on Chal-
lenges & Perspectives in Creating Large Language
Models.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, Diego
De Las Casas, Aurelia Guy, Jacob Menick, Roman
Ring, Tom Hennigan, Saffron Huang, Loren Mag-
giore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals,
Simon Osindero, Karen Simonyan, Jack Rae, Erich
Elsen, and Laurent Sifre. 2022. Improving language
models by retrieving from trillions of tokens. In
Proceedings of the 39th International Conference on
Machine Learning.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. In Advances in Neural Information Process-
ing systems.
Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-
ong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.
Knowledgeable or educated guess? revisiting lan-
guage models as knowledge bases. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing.
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
Katherine Lee, Florian Tramèr, and Chiyuan Zhang.
2022. Quantifying memorization across neural lan-
guage models.
Hung-Ting Chen, Michael JQ Zhang, and Eunsol Choi.
2022. Rich knowledge sources bring complex knowl-
edge conflicts: Recalibrating models to reflect con-
flicting evidence. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, et al. 2022. PaLM: Scaling language
modeling with pathways.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies.
Andrew Drozdov, Shufan Wang, Razieh Rahimi, An-
drew McCallum, Hamed Zamani, and Mohit Iyyer.
2022. You can't pick your neighbors, or can you?
when and how to rely on retrieval in the kNN-LM. In
Findings of EMNLP.
Paolo Ferragina and Ugo Scaiella. 2010. TAGME:
on-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management.
Thibault Févry, Livio Baldini Soares, Nicholas FitzGer-
ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-
tities as experts: Sparse memory access with entity
supervision. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing.
Junxian He, Graham Neubig, and Taylor Berg-
Kirkpatrick. 2021. Efficient nearest neighbor lan-
guage models. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language
Processing.
Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.
2022. Are large pre-trained language models leaking
your personal information? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing.

--- TRANG 10 ---
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022a. Unsupervised dense informa-
tion retrieval with contrastive learning. Transactions
on Machine Learning Research.
Gautier Izacard and Edouard Grave. 2021. Leveraging
passage retrieval with generative models for open
domain question answering. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave. 2022b. Few-shot learning with retrieval aug-
mented language models.
Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,
Joongbo Shin, Janghoon Han, Gyeonghun Kim, and
Minjoon Seo. 2022. Temporalwiki: A lifelong bench-
mark for training and evaluating ever-evolving lan-
guage models. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli
Tran-Johnson, et al. 2022. Language models (mostly)
know what they know.
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric
Wallace, and Colin Raffel. 2022. Large language
models struggle to learn long-tail knowledge.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing.
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,
Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir
Radev, Noah A Smith, Yejin Choi, and Kentaro Inui.
2022. Realtime QA: What's the answer right now?
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through memorization: Nearest neighbor language
models. In International Conference on Learning
Representations.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela. 2020.
Retrieval-augmented generation for knowledge-
intensive nlp tasks. In Advances in Neural Infor-
mation Processing Systems.
Shayne Longpre, Kartik Perisetla, Anthony Chen,
Nikhil Ramesh, Chris DuBois, and Sameer Singh.
2021. Entity-based knowledge conflicts in question
answering. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 7052–7063, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-
tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.
2022. Nonparametric masked language model.
Alicia Parrish, Angelica Chen, Nikita Nangia,
Vishakh Padmakumar, Jason Phang, Jana Thompson,
Phu Mon Htut, and Samuel Bowman. 2022. BBQ:
A hand-built bias benchmark for question answering.
In Findings of the Association for Computational
Linguistics: ACL 2022.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on
Natural Language Processing.
Nina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.
E-BERT: Efficient-yet-effective entity embeddings
for BERT. In Findings of the Association for Compu-
tational Linguistics: EMNLP 2020.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. Journal of Machine Learning Research.
Yasaman Razeghi, Robert L Logan IV, Matt Gardner,
and Sameer Singh. 2022. Impact of pretraining term
frequencies on few-shot reasoning.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing.
Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends in Information Re-
trieval.
Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,
and Danqi Chen. 2021. Simple entity-centric ques-
tions challenge dense retrievers. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing.

--- TRANG 11 ---
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. In Findings
of the Association for Computational Linguistics:
EMNLP 2021.
Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and
Denny Zhou. 2022. Recitation-augmented language
models.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2022. Generate
rather than retrieve: Large language models are
strong context generators.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An
open bilingual pre-trained model.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.

--- TRANG 12 ---
Phụ lục
A Chi tiết xây dựng POPQA
Danh sách các loại mối quan hệ và mẫu. Trong
công trình này, chúng tôi sử dụng 16 loại mối quan
hệ sau, và các tác giả của bài báo này đã chú thích
thủ công các mẫu để ngôn ngữ hóa bộ ba kiến thức
thành các câu hỏi ngôn ngữ tự nhiên. Chúng tôi
hiển thị danh sách cuối cùng các mẫu được sử dụng
để tạo POPQA trong Bảng 2.
Hình 3 hiển thị phân phối độ phổ biến chủ thể
của POPQA và EntityQuestions so với benchmark
NQ phổ biến. NQ có thể có nhiều thực thể nên
phân phối của thực thể ít phổ biến nhất mỗi câu
hỏi được hiển thị. Các thực thể chủ thể từ NQ được
trích xuất bằng TagMe (Ferragina and Scaiella,
2010) trên tập phát triển NQ-open với ngưỡng điểm
0.22. TagMe trả về tiêu đề của một thực thể Wikidata có thể được sử dụng trực tiếp để tìm độ phổ
biến.
Mối quan hệ Mẫu
nghề nghiệp Nghề nghiệp của [subj] là gì?
nơi sinh Thành phố nào [subj] sinh ra?
thể loại Thể loại của [subj] là gì?
cha Ai là cha của [subj]?
quốc gia [subj] ở quốc gia nào?
nhà sản xuất Ai là nhà sản xuất của [subj]?
đạo diễn Ai là đạo diễn của [subj]?
thủ đô của [subj] là thủ đô của cái gì?
biên kịch Ai là biên kịch cho [subj]?
nhà soạn nhạc Ai là nhà soạn nhạc của [subj]?
màu sắc Màu của [subj] là gì?
tôn giáo Tôn giáo của [subj] là gì?
thể thao [subj] chơi môn thể thao gì?
tác giả Ai là tác giả của [subj]?
mẹ Ai là mẹ của [subj]?
thủ đô Thủ đô của [subj] là gì?
Bảng 2: Danh sách đầy đủ các mẫu được chú thích thủ
công được sử dụng để tạo POPQA. [subj] biểu thị một
placeholder cho các thực thể chủ thể.
Lấy mẫu bộ ba kiến thức. Trong việc xây dựng
bộ dữ liệu POPQA, các bộ ba kiến thức được lấy
mẫu với trọng số cao hơn dành cho các thực thể
phổ biến hơn, nếu không thì phân phối sẽ bị chi
phối bởi đuôi và chúng ta sẽ không có đủ các thực
thể có độ phổ biến cao để hoàn thành phân tích
của chúng ta. Cụ thể, khi xem xét có nên lấy mẫu
một bộ ba kiến thức cụ thể hay không, chúng tôi
bao gồm bộ ba kiến thức khi và chỉ khi f > exp(8R-
6), trong đó R~U(0,1) là một số giả ngẫu nhiên
đồng nhất đơn vị và f là tần suất thuật ngữ khớp
chính xác của các bí danh của thực thể chủ thể trong
một mẫu 800 MB ngẫu nhiên của C4. Để tăng tính
đa dạng, khi 2000 bộ ba kiến thức của một loại
mối quan hệ cụ thể đã được lấy mẫu, chúng không
còn được lấy mẫu nữa.
B Chi tiết thí nghiệm
Tài nguyên tính toán và chi phí API. Việc sử dụng
API GPT-3 tổng cộng là $275. Chúng tôi chạy
14.282 câu hỏi qua hai mô hình GPT-3 davinci sử
dụng bốn phương pháp khác nhau: các thí nghiệm
vanilla tốn $13 ($0.46 trên 1000 câu hỏi), các thí
nghiệm tăng cường Contriever tốn $88 ($3.08 trên
1000 câu hỏi), các thí nghiệm tăng cường BM25
tốn $81 ($2.80 trên 1000 câu hỏi), và các thí nghiệm
GenRead tốn $93 ($3.25 trên 1000 câu hỏi).
Để chạy các thí nghiệm sử dụng LMs lớn hơn
hai tỷ tham số, chúng tôi sử dụng một GPU V100
Volta duy nhất với bộ nhớ GPU 32GB. Chúng tôi
sử dụng lượng tử hóa int8bit (Zeng et al., 2022)
với các mô hình OPT 13 tỷ và GPT-Neo 20 tỷ để
làm cho chúng phù hợp với GPU của chúng tôi.
Trong các thí nghiệm sơ bộ của chúng tôi sử dụng
GPT-Neo 6 tỷ, chúng tôi không quan sát thấy sự
sụt giảm hiệu suất đáng chú ý bằng cách sử dụng
lượng tử hóa.
Xây dựng ngữ cảnh few-shot. Đối với POPQA,
chúng tôi lấy mẫu các ví dụ few-shot được phân
tầng theo loại mối quan hệ để đa dạng hóa các mẫu:
đối với mỗi trong 15 loại mối quan hệ khác với loại
trong câu hỏi kiểm tra, chúng tôi lấy mẫu một cặp
câu hỏi-câu trả lời ngẫu nhiên để bao gồm trong
ngữ cảnh. Đối với EntityQuestions, chúng tôi lấy
một mẫu ngẫu nhiên đơn giản của 15 cặp câu hỏi-
câu trả lời vì có hơn 16 loại mối quan hệ.
Chi tiết quyết định ngưỡng. Chúng tôi sử dụng
75% của POPQA để xác định ngưỡng độ phổ biến
cho mỗi loại mối quan hệ. Sử dụng tìm kiếm vũ
phu, chúng tôi chọn ngưỡng để tối đa hóa độ chính
xác thích ứng, mà chúng tôi định nghĩa là độ chính
xác đạt được bằng cách lấy các dự đoán của hệ
thống tăng cường truy xuất cho các câu hỏi dưới
ngưỡng độ phổ biến và các dự đoán dựa trên kiến
thức tham số cho phần còn lại.
Sau đó chúng tôi đánh giá độ chính xác thích ứng
sử dụng các ngưỡng đã học trên 25% còn lại của
POPQA, và lặp lại với 100 phân chia ngẫu nhiên
khác nhau và lấy trung bình để có được phép đo
độ chính xác thích ứng được báo cáo.

--- TRANG 13 ---
GPT-neo 1.3BOPT 1.3B
GPT-neo 2.7BOPT 2.7B GPT-j 6B OPT 6.7B OPT 13B
GPT-neox 20B
GPT-3 davinci 002 GPT-3 davinci 0030.000.050.100.150.200.250.300.350.40Độ chính xácVanilla
GenRead
BM25
ContrieverHình 13: Độ chính xác theo LMs và LMs tăng cường
truy xuất trên POPQA. Đây là phần mở rộng của Hình 7
GPT-neo 1.3B GPT-neo 2.7BGPT-j 6B OPT 13B
GPT-neox 20B0.00.10.20.30.40.5Độ chính xác
GPT-3 davinci 0030.00.10.20.30.40.5Độ chính xácVanilla
GenReadBM25
Contriever
Hình 14: Độ chính xác theo LMs và LMs tăng cường
truy xuất trên EntityQuestions.
1.3B 2.7B 6B 20B ~175B
Tham số mô hình0.00.20.40.60.81.0Tỷ lệ sử dụng truy xuất
Adaptive Vanilla/BM25
Adaptive Vanilla/Contriever
Adaptive GenRead/BM25
Adaptive GenRead/Contriever
GPT-neo
GPT-3 davinci 003
Hình 15: Tỷ lệ câu hỏi mà Truy Xuất Thích Ứng sử
dụng truy xuất so với kích thước mô hình cho EntityQuestions.
C Kết quả chi tiết
C.1 Kết quả LM
Kết quả đầy đủ về độ chính xác và tương quan
theo loại mối quan hệ. Hình 16 hiển thị kết quả
đầy đủ về độ chính xác theo loại mối quan hệ cho
tất cả các loại mối quan hệ trong POPQA. Hình
17 hiển thị các tương quan cho tất cả các loại mối
quan hệ. Hình 19 và 18 hiển thị cùng kết quả cho
bộ dữ liệu EntityQuestions.
Tương quan âm của capital trên EntityQuestions.
Như được hiển thị trong Hình 19, các loại mối quan
hệ capital trên EntityQuestions, trong khi trên POPQA, mối quan hệ này cho thấy tương quan tương
đối cao. Chúng tôi thấy rằng trong EntityQuestions,
loại mối quan hệ capital này có nhiều câu hỏi có
độ phổ biến thấp mà câu trả lời được bao gồm trong
tên thực thể chủ thể (ví dụ: chủ thể="canton của
Marseille-Belsunce", đối tượng="Marseille"). Điều
này khiến hiệu suất có mối quan hệ hình chữ U với
độ phổ biến đối với loại mối quan hệ capital, vì vậy
nếu hầu hết các câu hỏi được lấy mẫu đến từ nửa
trên của độ phổ biến, tương quan tuyến tính sẽ tích
cực, và ngược lại.
C.2 Kết quả LM tăng cường truy xuất
Hiệu suất tổng thể của LMs tăng cường truy xuất.
Hình 13 hiển thị hiệu suất tổng thể của 40 LMs
và LMs tăng cường truy xuất trên POPQA. Việc
tăng cường truy xuất cải thiện đáng kể hiệu suất
trên các LMs khác nhau, và các mô hình nhỏ hơn
nhiều (GPT-Neo 1.3B) có thể hoạt động ngang với
GPT-3. Hình 14 hiển thị kết quả trên EntityQuestions. Do các ràng buộc về tính toán và thời gian,
chúng tôi chỉ có thể chạy kết quả vanilla và Contriever cho hầu hết các mô hình.
Truy Xuất Thích Ứng cho EntityQuestions. Hình
15 hiển thị tỷ lệ câu hỏi trên ngưỡng truy xuất cho
các mô hình khác nhau sử dụng Truy Xuất Thích
Ứng trên EntityQuestions. Vì EntityQuestions có
một lượng lớn câu hỏi có độ phổ biến thấp, các mô
hình (đặc biệt là những mô hình nhỏ hơn) phải dựa
vào truy xuất rất nhiều.
Kết quả đầy đủ trên tất cả các loại mối quan hệ.
Hình 20 hiển thị kết quả đầy đủ trên POPQA của
các LMs tăng cường truy xuất và LMs không hỗ
trợ trên 16 loại mối quan hệ sử dụng ba LMs khác
nhau làm xương sống. Hình 21 hiển thị những kết
quả này cho GPT-3 davinci-003 trên EntityQuestions.
C.3 Kết quả định tính
Bảng 3 hiển thị một số ví dụ trên POPQA, nơi
GPT-3 davinci-003 trả lời đúng trong khi phiên
bản tăng cường Contriever không trả lời được.
Cùng với recall@1 thấp là 0.14 cho nhóm này,
Bảng 3 gợi ý rằng lý do phổ biến nhất việc truy
xuất có thể có hại là nó truy xuất một tài liệu về
một thực thể nhầm lẫn, chẳng hạn như một người
có cùng tên với chủ thể, hoặc một thực thể đơn giản
không liên quan đến câu hỏi (như trong trường hợp
của "Noel Black").

--- TRANG 14 ---
nghề nghiệp
(n=532)tác giả
(n=1514)đạo diễn
(n=1999)quốc gia
(n=838)thủ đô của
(n=363)thủ đô
(n=645)tôn giáo
(n=338)thể thao
(n=547)0.00.20.40.60.81.0Độ chính xác
cha
(n=570)nơi sinh
(n=584)nhà soạn nhạc
(n=978)nhà sản xuất
(n=1520)thể loại
(n=1619)mẹ
(n=187)biên kịch
(n=1999)màu sắc
(n=34)0.00.20.40.60.81.0Độ chính xácGPT-neo 1.3B
OPT 1.3BGPT-neo 2.7B
OPT 2.7BGPT-j 6B
OPT 6.7BOPT 13B
GPT-neox 20BGPT-3 davinci 002
GPT-3 davinci 003Hình 16: Độ chính xác trên PopQA cho tất cả các loại mối quan hệ và mô hình. Đây là phần mở rộng của Hình 4.
nghề nghiệp
(n=532)tác giả
(n=1514)đạo diễn
(n=1999)quốc gia
(n=838)thủ đô của
(n=363)thủ đô
(n=645)tôn giáo
(n=338)thể thao
(n=547)0.00.20.40.60.8Tương quan giữa
log(độ phổ biến) và độ chính xác
cha
(n=570)nơi sinh
(n=584)nhà soạn nhạc
(n=978)nhà sản xuất
(n=1520)thể loại
(n=1619)mẹ
(n=187)biên kịch
(n=1999)màu sắc
(n=34)0.00.20.40.60.8Tương quan giữa
log(độ phổ biến) và độ chính xácGPT-neo 1.3B
OPT 1.3BGPT-neo 2.7B
OPT 2.7BGPT-j 6B
OPT 6.7BOPT 13B
GPT-neox 20BGPT-3 davinci 002
GPT-3 davinci 003
Hình 17: Tương quan trên PopQA cho tất cả các loại mối quan hệ và mô hình. Đây là phần mở rộng của Hình 4.
thể loại
(n=739)tác phẩm
nổi bật
(n=192)nghề nghiệp
(n=878)nơi sinh
(n=891)được giáo dục
tại
(n=874)vị trí trụ sở
chính...
(n=917)người sáng tạo
(n=623)nơi hình
thành...
(n=736)quốc gia
(n=837)quốc gia
xuất xứ
(n=672)nhà sản
xuất
(n=861)vị trí
(n=891)0.00.20.40.60.81.0Độ chính xác
người biểu diễn
(n=644)vị trí chơi
trong ...
(n=846)tác giả
(n=756)sở hữu bởi
(n=872)ngôn ngữ của
tác phẩm hoặc...
(n=429)vợ/chồng
(n=876)con
(n=856)nằm trong
đơn vị lãnh thổ...
(n=791)hãng thu âm
(n=658)được thành lập bởi
(n=439)thủ đô
(n=685)nơi chết
(n=882)0.00.20.40.60.81.0Độ chính xácGPT-neo 1.3B
GPT-neo 2.7BGPT-j 6B
OPT 13BGPT-neox 20B
GPT-3 davinci 003
Hình 18: Độ chính xác trên EntityQuestions cho tất cả các loại mối quan hệ và mô hình.

--- TRANG 15 ---
thể loại
(n=739)tác phẩm
nổi bật
(n=192)nghề nghiệp
(n=878)nơi sinh
(n=891)được giáo dục
tại
(n=874)vị trí trụ sở
chính...
(n=917)người sáng tạo
(n=623)nơi hình
thành...
(n=736)quốc gia
(n=837)quốc gia
xuất xứ
(n=672)nhà sản
xuất
(n=861)vị trí
(n=891)0.00.20.40.60.8Tương quan giữa
log(độ phổ biến) và độ chính xác
người biểu diễn
(n=644)vị trí chơi
trong ...
(n=846)tác giả
(n=756)sở hữu bởi
(n=872)ngôn ngữ của
tác phẩm hoặc...
(n=429)vợ/chồng
(n=876)con
(n=856)nằm trong
đơn vị lãnh thổ...
(n=791)hãng thu âm
(n=658)được thành lập bởi
(n=439)thủ đô
(n=685)nơi chết
(n=882)0.2
0.00.20.40.6Tương quan giữa
log(độ phổ biến) và độ chính xácGPT-neo 1.3B
GPT-neo 2.7BGPT-j 6B
OPT 13BGPT-neox 20B
GPT-3 davinci 003Hình 19: Tương quan trên EntityQuestions cho tất cả các loại mối quan hệ và mô hình.
0.00.51.0nghề nghiệp tác giả đạo diễn quốc gia thủ đô của thủ đô tôn giáo thể thao
0.00.51.0nhà sản xuất mẹ nơi sinh nhà soạn nhạc thể loại cha màu sắc biên kịch
0.00.51.0nghề nghiệp tác giả đạo diễn quốc gia thủ đô của thủ đô tôn giáo thể thao
0.00.51.0nhà sản xuất mẹ nơi sinh nhà soạn nhạc thể loại cha màu sắc biên kịch
0.00.51.0nghề nghiệp tác giả đạo diễn quốc gia thủ đô của thủ đô tôn giáo thể thao
103106
Độ phổ biến0.00.51.0Độ chính xácnhà sản xuất
Vanilla
GenRead
BM25
Contriever
103106mẹ
103106nơi sinh
103106nhà soạn nhạc
103106thể loại
103106cha
103106màu sắc
103106biên kịchGPT-neo 1.3B OPT 13B GPT-3 DaVinci 003
Hình 20: Độ chính xác cho 3 mô hình trên POPQA so với độ phổ biến như được hiển thị trong Hình 8 được chia
theo loại mối quan hệ. Các bins độ phổ biến với ít hơn 5 mẫu được loại trừ để tránh làm lộn xộn các hình với kết
quả nhiễu có thanh lỗi rộng.

--- TRANG 16 ---
0.00.51.0thể loại tác phẩm nổi bật nghề nghiệp nơi sinh được giáo dục tạivị trí trụ sở
chính người sáng tạonơi hình
thành
0.00.51.0quốc gia quốc gia xuất xứ nhà sản xuất vị trí người biểu diễnvị trí chơi
trong đội / chuyên môn tác giả sở hữu bởi
103106
Độ phổ biến0.00.51.0Độ chính xácngôn ngữ của tác phẩm
hoặc tên
Vanilla
GenRead
BM25
Contriever
103106vợ/chồng
103106con
103106nằm trong
đơn vị lãnh thổ hành chính
103106hãng thu âm
103106được thành lập bởi
103106thủ đô
103106nơi chếtHình 21: Độ chính xác so với độ phổ biến cho GPT-3 davinci-003 trên EntityQuestions được chia theo loại mối
quan hệ. Các bins độ phổ biến với ít hơn 5 mẫu được loại trừ.
Bảng 4 hiển thị một số ví dụ trên POPQA, nơi
GPT-3 davinci-003 chỉ trả lời đúng khi được tăng
cường với Contriever. Recall@1 cho trường hợp
này là 0.88, cao hơn đáng kể so với recall tổng thể.
Lưu ý rằng trong ví dụ thứ hai, việc truy xuất khiến
LM trả lời đúng, nhưng chỉ do tình cờ: thực thể
chủ thể "Pierre" thực sự đề cập đến thành phố ở
South Dakota, không phải cầu thủ bóng rổ. Ngoài
ra, việc truy xuất dường như hữu ích vì nó cung
cấp thông tin liên quan trực tiếp.

--- TRANG 17 ---
Câu hỏi (câu trả lời) LM Retrieval-LM Truy xuất
Ai là đạo diễn của
Black? (Sanjay Leela
Bhansali)Đạo diễn của
Black là Sanjay
Leela Bhansali.
✓Noel Black ✗ Noel Black (30 tháng 6, 1937 – 5 tháng 7, 2014) là một
đạo diễn phim và truyền hình, biên kịch, và nhà sản xuất
người Mỹ. Black sinh ra ở Chicago, Illinois. Ông đã giành
được các giải thưởng tại năm 1966... và sau đó tập trung
vào việc đạo diễn cho truyền hình, thỉnh thoảng đạo diễn
các bộ phim như "Private School".
Ai là nhà sản xuất
của The Faculty? (Robert
Rodriguez)The Faculty
được sản xuất
bởi Elizabeth
Avellan và
Robert Ro-
driguez. ✓Nhà sản xuất của
The Faculty là
Elizabeth Avel-
lan.✗The Faculty là một bộ phim kinh dị teen khoa học viễn
tưởng Mỹ năm 1998 được viết bởi Kevin Williamson, đạo
diễn bởi Robert Rodriguez, và có sự tham gia của Elijah
Wood,... Bộ phim được phát hành vào Ngày Giáng sinh,
1998, bởi Miramax Films thông qua Dimension Films. ...
Bộ phim đã phát triển một lượng người hâm mộ cuồng
nhiệt trong những năm kể từ khi phát hành.
Nghề nghiệp của Michael
Shelley là gì?
(singer-songwriter)Michael Shelley
là một ca sĩ-
nhạc sĩ và
nhạc sĩ. ✓Michael Shelley
là một nhà toán
học ứng dụng
người Mỹ. ✗Michael Shelley (nhà toán học) Michael J. Shelley (sinh
ngày 17 tháng 8, 1959) là một nhà toán học ứng dụng người
Mỹ làm việc về mô hình hóa và mô phỏng các hệ thống
phức tạp phát sinh trong vật lý và sinh học. Điều này đã
bao gồm các vấn đề ranh giới tự do trong chất lỏng và khoa
học vật liệu... Ông cũng là đồng sáng lập và đồng giám đốc
của Phòng thí nghiệm Toán học Ứng dụng của Viện Courant.
Zijah Sokolović sinh ra
ở thành phố nào? (Sara-
jevo)Zijah Sokolović
sinh ra ở
Sarajevo ✓Zijah Sokolović
sinh ra ở
Orahovac,
Vương quốc ✗Ali Sokol... (sinh ngày 8 tháng 5, 1921 tại Orahovac,
Vương quốc Serbia, Croatia và Slovenia, mất ngày 23 tháng
9, 1974) là một bác sĩ phổi người Yugoslavia. Ali Sokol
sinh ra trong một gia đình nông nghiệp. Ông là con thứ ba
trong bốn đứa con của cha và mẹ Hatixhes Solomon. Ngày
sinh chính xác không rõ nhưng ngày đáng tin cậy nhất là
ngày 8 tháng 5 năm 1921.
Bảng 3: Các ví dụ định tính về những câu hỏi mà chỉ LMs tăng cường truy xuất thất bại trong việc trả lời đúng.
Văn bản gạch chân màu xanh cho thấy các chuỗi con khớp với các câu trả lời vàng trong ngữ cảnh được truy xuất.

--- TRANG 18 ---
Câu hỏi (câu trả lời) LM Retrieval-LM Truy xuất
Thể loại của Un-
known là gì? (fantasy)Unknown không
phải là một thể
loại âm nhạc
cụ thể. Nó có thể
đề cập đến ✗Unknown là một
tạp chí viễn tưởng
pulp.
✓Unknown (tạp chí) Unknown (còn được gọi là Unknown
Worlds) là một tạp chí viễn tưởng pulp người Mỹ, được
xuất bản từ 1939 đến 1943 bởi Street & Smith, và được
biên tập bởi John W. Campbell. ... Tạp chí viễn tưởng hàng
đầu trong những năm 1930 là "Weird Tales", tập trung vào
shock và kinh dị.
Pierre ở quốc gia nào?
(United States)Pierre ở
Pháp. ✗Pierre đến từ
Hoa Kỳ.✓Jonas Farley Pierre (sinh ngày 30 tháng 5, 1980) là một
cầu thủ bóng rổ chuyên nghiệp người Mỹ. Một trung phong,
anh đã chơi với ba đội trong National Basketball League
của Canada (NBL). Pierre cuối cùng chơi với Saint John
Mill Rats ở Canada. ... Sau đó anh gia nhập Lincoln Uni-
versity ở Missouri cho năm thứ ba. Vào mùa hè năm 2003,
anh đã ký một thư ý định chơi với Nicholls State trong
NCAA Division I.
Ai là nhà sản xuất
của The Cocoanuts?
(Walter Wanger)The Cocoanuts
được sản xuất
bởi Florenz
Ziegfeld. ✗The Cocoanuts
được sản xuất
cho Paramount
Pictures bởi
Walter Wanger,
người✓The Cocoanuts là một bộ phim hài nhạc kịch năm 1929
có sự tham gia của Marx Brothers. Được sản xuất cho
Paramount Pictures bởi Walter Wanger, người không được
ghi nhận, bộ phim có sự tham gia của bốn Marx Brothers,
Oscar Shaw, Mary Eaton, và Margaret Dumont. Đây là
bộ phim có âm thanh đầu tiên ghi nhận hơn một đạo diễn
(Robert Florey và Joseph Santley), và được chuyển thể
lên màn ảnh bởi Morrie Ryskind từ vở nhạc kịch Broadway
của George S. Kaufman. ...
Ai là đạo diễn của
The White Suit? (Lazar
Ristovski)The White Suit
được đạo diễn
bởi Sachin
Kundalkar. ✗Lazar Ristovski
✓Năm 1999 "The White Suit" một bộ phim auteur của Ris-
tovski (đạo diễn, biên kịch, diễn viên chính, và nhà sản
xuất) được tham gia Liên hoan phim Cannes trong chương
trình Critics Week. "The White Suit" là tác phẩm của Serbia
cho Giải Oscar 1999. Lazar Ristovski là chủ sở hữu duy
nhất của Công ty Phim Zillion. Năm 2006, ông xuất hiện
nhỏ trong bộ phim James Bond "Casino Royale". Ông đóng
vai Caruso trong bộ phim "King of Thieves" năm 2004.
Ông đóng vai Đorđe trong bộ phim đoạt giải "St. George
Shoots the Dragon" năm 2009.
Bảng 4: Các ví dụ định tính về những câu hỏi mà chỉ LMs tăng cường truy xuất trả lời đúng thành công. Văn bản
gạch chân màu xanh cho thấy các chuỗi con khớp với các câu trả lời vàng trong ngữ cảnh được truy xuất.
