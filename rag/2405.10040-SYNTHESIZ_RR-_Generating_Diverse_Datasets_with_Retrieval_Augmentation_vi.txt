# 2405.10040.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2405.10040.pdf
# Kích thước file: 2250260 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
SYNTHESIZ RR: Tạo ra Các Tập Dữ liệu Đa dạng với Tăng cường Truy xuất
Abhishek Divekar♠♢*Greg Durrett♢
♠Amazon
♢Khoa Khoa học Máy tính, Đại học Texas tại Austin
adivekar@amazon.com gdurrett@cs.utexas.edu
Tóm tắt
Việc chưng cất các khả năng của các mô hình ngôn ngữ lớn (LLM) thành các mô hình học sinh nhỏ hơn thường được mong muốn do các ràng buộc về tính toán và bộ nhớ. Một cách để thực hiện điều này cho các nhiệm vụ phân loại là thông qua tổng hợp tập dữ liệu, có thể được thực hiện bằng cách tạo ra các ví dụ của mỗi nhãn từ LLM. Các phương pháp tổng hợp trước đây sử dụng prompting few-shot, dựa vào kiến thức tham số của LLM để tạo ra các ví dụ có thể sử dụng được. Tuy nhiên, điều này dẫn đến các vấn đề về lặp lại, thiên vị đối với các thực thể phổ biến, và sự khác biệt về phong cách so với văn bản do con người viết. Trong nghiên cứu này, chúng tôi đề xuất Synthesize by Retrieval and Refinement (SYNTHESIZ RR), sử dụng tăng cường truy xuất để đưa sự đa dạng vào quá trình tổng hợp tập dữ liệu: khi các đoạn văn được truy xuất thay đổi, LLM được "cung cấp" với nội dung khác nhau để tạo ra các ví dụ của nó. Chúng tôi nghiên cứu thực nghiệm việc tổng hợp sáu tập dữ liệu, bao gồm phân loại chủ đề, phân tích cảm xúc, phát hiện âm điệu, và hài hước, đòi hỏi các chiến lược tổng hợp phức tạp. Chúng tôi thấy rằng SYNTHESIZ RR¹ cải thiện đáng kể tính đa dạng từ vựng và ngữ nghĩa, sự tương tự với văn bản do con người viết, và hiệu suất chưng cất, khi so sánh với prompting 32-shot và bốn phương pháp trước đây.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) như GPT-4 (OpenAI, 2023; Bubeck et al., 2023), LLaMa (Touvron et al., 2023b) và Claude (Bai et al., 2022) là các mô hình tổng quát đa năng, có khả năng giải quyết nhiều nhiệm vụ mà không cần điều chỉnh tham số thông qua prompting zero-shot hoặc few-shot. Ngược lại, các phương pháp trước đây fine-tune các biến thể của BERT (Devlin et al., 2019) trên các minh chứng cụ thể cho nhiệm vụ, tạo ra các mô hình chuyên môn. Các mô hình chuyên môn nhỏ hơn này tiết kiệm hơn tại thời điểm suy luận, nhưng đòi hỏi ít nhất hàng nghìn ví dụ để huấn luyện.

*Công việc được hoàn thành khi làm việc tại Amazon.
¹https://github.com/amazon-science/synthesizrr

[Tiếp tục với hình ảnh và nội dung còn lại...]

Sen. Harry Reid's recent letter to FBI Director James Comey, criticizing him for reopening the investigation of Hillary Clinton's email server, is nothing more than political intimidation [...] Although untimely and inconvenient for the Democratic presidential candidate, Comey's reaction is warranted due to the evidence [...]

LLM Giáo viên
Hypocrisy reigns supreme as Harry Reid, the Senate minority leader, unleashes a scathing attack on FBI Director James Comey. Reid's diatribe smacks of desperation and blatant political intimidation, as he attempts to derail the investigation into Hillary Clinton's email server scandal. [...] 

Bài báo tin tức được truy xuất duy nhất
Bài báo tin tức tổng hợp duy nhất

LLM Giáo viên
Billionaire, California Sen. Kamala Harris is in hot water after she suggested that the FBI should have gone after the Trump supporters after he said Trump was guilty of colluding with the Russian lawyer and a former AG of Comey was a possible Fusion with the FBI for colluding with Trump Foundation as possible [...]

Lấy mẫu lặp lại
Tạo ra Few-shot

SYNTHESIZ RR
Do con người viết
The FBI just blew the Hillary Clinton case wide open she could literally be going to jail. FBI just dropped could put her in the category of bona fide traitor. FBI uncovered Russian bribery plot before Obama administration approved controversial n-u-c-l-e-a-r deal with Moscow. Senate Judiciary opens probe into Obama-era Russian uranium bribery [...]

Bài báo tin tức tổng hợp
Bài báo tin tức thật

Hình 1: Các ví dụ tổng hợp từ tạo ra few-shot (giữa) và SYNTHESIZ RR (dưới). Phương pháp của chúng tôi kết hợp bước tìm nguồn nội dung truy xuất tài liệu từ một kho ngữ liệu: đối với nhiệm vụ phát hiện thiên vị chính trị, một bài báo tin tức được truy xuất và LLM giáo viên được nhắc tạo ra một phiên bản thiên vị. Quy trình tổng hợp kết quả mang lại các ví dụ đa dạng phù hợp hơn với các ví dụ do con người viết.

Nghiên cứu gần đây đã tìm cách tránh sự phụ thuộc vào các ví dụ được tạo thủ công bằng cách fine-tune các mô hình chuyên môn trên các tập dữ liệu tổng hợp thông qua chưng cất giáo viên-học sinh (West et al., 2022). Điều này có ứng dụng trong phân loại (Yu et al., 2023a; Ye et al., 2022a,b), điều chỉnh sở thích con người (Lee et al., 2023; Bai et al., 2022), hiểu ngôn ngữ (Meng et al., 2022; Schick và Schütze, 2021), và thậm chí dữ liệu dạng bảng (Borisov et al., 2022). Tuy nhiên, dữ liệu tổng hợp có những hạn chế. Như Yu et al. (2023a) lưu ý, các prompt ngây thơ tạo ra các văn bản với tính đa dạng hạn chế và phản ánh các thiên vị của LLM giáo viên.

Hình 1 minh họa phương pháp tổng hợp few-shot (Ye et al., 2022a,b; Yehudai et al., 2024a), mà chúng tôi gọi là FEWGEN, cho nhiệm vụ phát hiện các bài báo thiên vị chính trị. Với một prompt phù hợp và các ví dụ trong ngữ cảnh, việc lấy mẫu tiếp tục từ một LLM tạo ra tin tức hợp lý theo phong cách thiên vị mà chúng ta muốn phát hiện. Tuy nhiên, khi hàng nghìn hoàn thành được lấy mẫu từ một prompt cố định, chúng tôi quan sát thấy sự lặp lại, thiên vị đối với các thực thể phổ biến, và sự khác biệt về phong cách so với các văn bản do con người viết. Các mô hình chuyên môn được chưng cất từ các tập dữ liệu đa dạng thấp như vậy có thể không học được nhiệm vụ tốt.

Trong nghiên cứu này, chúng tôi tìm cách giảm thiểu việc thiếu đa dạng trong dữ liệu tổng hợp. Chúng tôi đề xuất rằng việc tổng hợp tập dữ liệu có thể được phân tách thành hai năng lực LLM riêng biệt: tìm nguồn nội dung, nơi LLM thu thập thông tin liên quan cho nhiệm vụ, và đảo ngược nhiệm vụ, nơi LLM tạo ra một đầu vào tổng hợp sử dụng prompt có điều kiện mục tiêu. Nghiên cứu trước đây chủ yếu tập trung vào đảo ngược nhiệm vụ, trong khi ngầm sử dụng bộ nhớ tham số của LLM để tìm nguồn nội dung. Ngược lại, chúng tôi điều tra tầm quan trọng của một giai đoạn tìm nguồn nội dung rõ ràng.

Chúng tôi đề xuất Synthesize by Retrieval and Refinement (SYNTHESIZ RR), một quy trình tổng hợp ví dụ được hướng dẫn bởi một kho ngữ liệu truy xuất. Trong bước tìm nguồn nội dung, chúng tôi sử dụng các biến đồng thay đổi học trong ngữ cảnh làm truy vấn truy xuất để trích xuất hàng chục tài liệu cho mỗi truy vấn từ một kho ngữ liệu cụ thể cho miền. Sau đó, một LLM tổng quát thực hiện đảo ngược nhiệm vụ trên mỗi tài liệu được truy xuất. Vì mỗi prompt sử dụng một tài liệu được truy xuất duy nhất, quy trình tổng hợp của chúng tôi tạo ra các ví dụ đa dạng, được làm giàu với một phổ rộng các thực thể và khẳng định trong thế giới thực.

Chúng tôi đánh giá SYNTHESIZ RR so với FEWGEN trên sáu nhiệm vụ phân loại văn bản, được lựa chọn cẩn thận để đo lường nhiều kiểu tổng hợp tập dữ liệu khác nhau. Các thực nghiệm của chúng tôi (§5) cho thấy SYNTHESIZ RR vượt trội đáng kể so với FEWGEN về tính đa dạng và sự giống nhau với các văn bản do con người viết, mặc dù cả hai quy trình đều sử dụng cùng một LLM đông lạnh. Trong §6, chúng tôi thấy rằng các bộ phân loại học sinh được fine-tune trên dữ liệu do SYNTHESIZ RR tạo ra hoạt động tốt hơn so với những bộ được fine-tune trên FEWGEN. Cuối cùng, trong §7, chúng tôi so sánh SYNTHESIZ RR với bốn phương pháp tiên tiến để tổng hợp các tập dữ liệu phân loại, và thấy SYNTHESIZ RR mang lại các tập dữ liệu đa dạng hơn, phù hợp tốt hơn với các trường hợp do con người viết, và dẫn đến độ chính xác học sinh cao hơn trong hầu hết các trường hợp.

Đóng góp của chúng tôi như sau: (1) chúng tôi đề xuất một phương pháp mới để tổng hợp ví dụ cho chưng cất giáo viên-học sinh, làm nền tảng cho bước đảo ngược nhiệm vụ sử dụng một kho ngữ liệu truy xuất; (2) chúng tôi giới thiệu thuật toán SYNTHESIZ RR R ETRICL để tạo ra một tập học trong ngữ cảnh thực tế cho phương pháp của chúng tôi; (3) chúng tôi phân tích thực nghiệm việc tổng hợp sáu nhiệm vụ phân loại đầy thách thức, so sánh tính đa dạng văn bản và độ tương tự của phương pháp chúng tôi cũng như độ chính xác nhiệm vụ xuôi dòng với các phương pháp hiện có; (4) chúng tôi xác định các yếu tố ảnh hưởng đến chất lượng của các tập dữ liệu tổng hợp của chúng tôi bằng cách thay đổi lượng dữ liệu có giám sát, mức độ liên quan của kho ngữ liệu đối với nhiệm vụ, số lượng ví dụ trong ngữ cảnh, và truy xuất thưa thớt so với dày đặc.

--- TRANG 2 ---
LLM Tổng quát
[ĐẢO NGƯỢC NHIỆM VỤ]
[TÌM NGUỒN NỘI DUNG]
Tài liệu được Truy xuất    Minh chứng ICL    Kho ngữ liệu    Dữ liệu Tổng hợp
Prompt Tinh chỉnh Ngữ cảnh

Hình 2: Mô tả trừu tượng của quy trình SYNTHESIZ RR. Trong giai đoạn tìm nguồn nội dung, chúng tôi truy xuất K tài liệu duy nhất {r1, . . . , rK} từ một kho ngữ liệu lớn cho mỗi biến đồng thay đổi trong ngữ cảnh xICL. Giai đoạn đảo ngược nhiệm vụ của tổng hợp sử dụng một prompt tinh chỉnh ngữ cảnh được tham số hóa Pτ, nhận các tham số Rinv (hướng dẫn đảo ngược), rk (một tài liệu được truy xuất), và V(yICL) (nhãn mục tiêu được diễn đạt). Một LLM giáo viên tổng quát tự hồi quy tạo ra một biến đồng thay đổi tổng hợp. Mỗi ví dụ trong ngữ cảnh do đó tạo ra K ví dụ tổng hợp duy nhất {˜x1, . . . , ˜xK}, mà chúng tôi bao gồm trong tập dữ liệu với mục tiêu yICL.

[Tiếp tục dịch nội dung còn lại theo cùng cách...]

--- TRANG 3 ---
và đầu ra y∈ Y cho không gian đầu ra Y có C lớp. Mục tiêu của chúng tôi là tạo ra một tập dữ liệu tổng hợp DSYNTH = {(˜xi, yi)}ᵢ₌₁ᵐ và huấn luyện một mô hình ngôn ngữ chuyên môn MS (ví dụ một mô hình được tiền huấn luyện theo kiểu BERT (Devlin et al., 2019)). Chúng tôi tạo ra DSYNTH thông qua đảo ngược nhiệm vụ: lặp đi lặp lại việc nhắc một mô hình ngôn ngữ giáo viên MLLM để tạo ra các biến đồng thay đổi tổng hợp ˜x cho các nhãn tương ứng y. Chúng tôi ký hiệu nhiệm vụ của học sinh (dự đoán y từ x) là τ và nhiệm vụ của giáo viên (tạo ra x cho y) là τinv.

SYNTHESIZ RR nhằm giải quyết việc thiếu đa dạng bằng cách tận dụng truy xuất trong bước tìm nguồn nội dung. Chúng tôi giả định sự tồn tại của một kho ngữ liệu R nơi mỗi tài liệu có thể chứa thông tin liên quan đến nhiệm vụ. Tuy nhiên, các tài liệu không cần phải có nguồn gốc từ cùng một phân phối với các biến đồng thay đổi nhiệm vụ của chúng tôi; ngay cả các tài liệu liên quan xa cũng có thể mang lại các ví dụ tổng hợp có giá trị. Ví dụ, chúng tôi cho thấy rằng chúng ta có thể tạo ra thành công các đánh giá và câu hỏi hài hước từ một kho ngữ liệu mô tả sản phẩm. Chúng tôi cũng giả định quyền truy cập vào một tập hạt giống của các ví dụ DSEED = {(x1, y1), . . . , (xn, yn)} đủ lớn để đại diện cho các lớp nhưng đủ nhỏ để có thể được biên soạn thủ công bởi người dùng trong vài giờ; trong các thực nghiệm, chúng tôi sử dụng tập học trong ngữ cảnh làm DSEED. Quan trọng là, chúng tôi giả định tập hạt giống không đủ để huấn luyện một học sinh hiệu quả, và cần một DSYNTH lớn hơn (m >> n).

Hình 2 minh họa phương pháp của chúng tôi để tạo ra các biến đồng thay đổi tương tự về phân phối. Ban đầu, chúng tôi truy xuất các tài liệu dựa trên các ví dụ trong DSEED, giả định rằng kho ngữ liệu chứa đủ các tài liệu tương tự về miền. Sau đó chúng tôi xây dựng một hướng dẫn tinh chỉnh ngữ cảnh để thực hiện đảo ngược nhiệm vụ trên mỗi tài liệu được truy xuất. Cách tiếp cận này cung cấp cho LLM một prompt duy nhất và có căn cứ cho mỗi ví dụ được tạo ra, do đó tránh được nhu cầu cho LLM giáo viên ghi nhớ dữ liệu kho ngữ liệu rộng lớn trong các tham số hạn chế của nó. Đảo ngược nhiệm vụ có thể đầy thách thức do sự không khớp giữa các tài liệu được truy xuất và các ví dụ kiểm tra; để khắc phục điều này, chúng tôi giới hạn nghiên cứu của mình đối với các LLM giáo viên thể hiện khả năng tuân theo hướng dẫn mạnh mẽ (Ouyang et al., 2022; Touvron et al., 2023b; Bai et al., 2022).

3 Phương pháp
Thuật toán 1 cho thấy phương pháp tạo tập dữ liệu của chúng tôi. Chúng tôi chưng cất một mô hình học sinh trong các bước sau:

Bước 1. Tìm nguồn nội dung sử dụng truy xuất:
SYNTHESIZ RR sử dụng mỗi biến đồng thay đổi trong ngữ cảnh xICL làm truy vấn để truy xuất thông tin, ngoài vai trò tiếp theo của nó trong quá trình học trong ngữ cảnh. Đối với mỗi truy vấn, chúng tôi truy xuất K tài liệu ΓK = [r1, . . . , rK] có độ tương tự cosine giảm dần sử dụng bộ truy xuất dày đặc Mret. Chúng tôi giữ lại các tài liệu có độ tương tự cosine trong (0.4, 0.9), để đảm bảo độ tương tự tối thiểu trong khi loại trừ các tài liệu quá tương tự như các bản sao tiềm năng của xICL. Mỗi bộ ba kết quả (xICL, yICL, ΓK) được thêm vào tập DRETR.

Bước 2. Xây dựng tập trong ngữ cảnh: Bước đảo ngược nhiệm vụ tiếp theo cũng hưởng lợi từ các minh chứng trong ngữ cảnh, nhưng việc xây dựng các minh chứng nắm bắt hiệu quả nhiệm vụ tinh chỉnh ngữ cảnh của chúng tôi rᵢₖ → ˜xᵢ là một thách thức. Chúng tôi đã khám phá hai cách tiếp cận cho việc học trong ngữ cảnh.

1. RETRICL: chúng tôi sử dụng truy xuất để xây dựng một tập các ví dụ ICL DICL, sao cho mỗi ví dụ ICL phản ánh định dạng của các prompt đảo ngược nhiệm vụ của chúng tôi. Chúng tôi chọn kết quả được truy xuất top-1 và top-2 từ các kết quả được truy xuất dày đặc, và sử dụng tiêu chí độ tương tự cosine sα ≤ cos(xICL, rk) ≤ sβ để đánh giá sự khớp tiềm năng giữa tài liệu được truy xuất rk và xICL. Mặc dù cặp trong ngữ cảnh có thể không khớp chính xác, chúng thể hiện định dạng yêu cầu như trong Phụ lục G.

2. NON-RETRICL: một phương pháp cơ sở, sử dụng truy xuất để tìm nguồn nội dung, nhưng không cho việc học trong ngữ cảnh. Đối với mỗi thế hệ, chúng tôi chọn N = 32 ví dụ ICL ngẫu nhiên từ DSEED. Mỗi ví dụ được thêm với một tiền tố như "Bài báo Tin tức:" hoặc "Chi tiết Sản phẩm:" nhưng chúng tôi không thêm hướng dẫn tinh chỉnh ngữ cảnh. Sau các ví dụ ICL, chúng tôi thêm tài liệu được truy xuất rk và hướng dẫn tinh chỉnh ngữ cảnh Rinv để tạo thành prompt cuối cùng. Định dạng này gần giống với prompt học trong ngữ cảnh được sử dụng bởi FEWGEN, nhưng cũng kết hợp các yếu tố tìm nguồn nội dung rk và Rinv. Cơ sở này làm nổi bật giá trị được thêm vào bằng cách xây dựng DICL trong cách tiếp cận RETRICL.

Thuật toán 1 SynthesizRR RETRICL
Đầu vào: Một tập các ví dụ hạt giống DSEED, kho ngữ liệu truy xuất R = {rk}, mô hình truy xuất Mret, hệ số mở rộng K, tiêu chí độ tương tự cosine (sα, sβ), mô hình giáo viên MLLM, mẫu prompt Pτ, hướng dẫn tinh chỉnh ngữ cảnh Rinv, verbalizer V: {y1, . . . , yC} → {v1, . . . , vC}.
Đầu ra: Tập dữ liệu tổng hợp DSYNTH

Quy trình SYNTHESIZ RR(DSEED, R):
DRETR ← ∅
DICL ← ∅
DSYNTH ← ∅

▷ Tìm nguồn nội dung sử dụng truy xuất:
for (x, y) ∈ DSEED do
    [r1, . . . , rK] ← Mret(x)
    ΓK ← [r1, . . . , rK]
    DRETR ← DRETR ∪ {(x, y, ΓK)}

▷ Xây dựng tập học trong ngữ cảnh:
for (x, y, ΓK) ∈ DRETR do
    for rk ∈ ΓK do
        DICL ← DICL ∪ {(rk, x)} if sα ≤ cos(x, rk) ≤ sβ

▷ Đảo ngược nhiệm vụ:
for (x, y, ΓK) ∈ DRETR do
    for rk ∈ ΓK do
        DSHOTS ∼ DICL
        for j ∈ [1, . . .] until ˜xᵢⱼ = <eos> do
            ˜xᵢⱼ ∼ MLLM(·|˜xᵢ<j, Pτ(Rinv, rk, V(y)), DSHOTS)
        DSYNTH ← DSYNTH ∪ {(˜xᵢ, y)}

return DSYNTH

--- TRANG 4 ---
sử dụng truy xuất để tìm nguồn nội dung, nhưng không cho việc học trong ngữ cảnh. Đối với mỗi thế hệ chúng tôi chọn N = 32 ví dụ ICL ngẫu nhiên từ DSEED. Mỗi ví dụ được thêm với một tiền tố như "Bài báo Tin tức:" hoặc "Chi tiết Sản phẩm:" nhưng chúng tôi không thêm hướng dẫn tinh chỉnh ngữ cảnh. Sau các ví dụ ICL, chúng tôi thêm tài liệu được truy xuất rk và hướng dẫn tinh chỉnh ngữ cảnh Rinv để tạo thành prompt cuối cùng. Định dạng này gần giống với prompt học trong ngữ cảnh được sử dụng bởi FEWGEN, nhưng cũng kết hợp các yếu tố tìm nguồn nội dung rk và Rinv. Cơ sở này làm nổi bật giá trị được thêm vào bằng cách xây dựng DICL trong cách tiếp cận RETRICL.

Bước 3. Đảo ngược nhiệm vụ sử dụng tinh chỉnh ngữ cảnh:
Các yếu tố tối thiểu của một prompt đảo ngược nhiệm vụ Pτ là hướng dẫn tinh chỉnh ngữ cảnh Iinv và mục tiêu y. Chúng tôi sử dụng một hàm verbalizer V (Schick và Schütze, 2021; van de Kar et al., 2022) để cung cấp một biểu diễn văn bản duy nhất của mỗi nhãn, tức là V: Y → {v1, . . . , vC}. Chúng tôi tuân theo nghiên cứu trước đây về đảo ngược nhiệm vụ dựa trên phân loại (Schick và Schütze, 2021; Ye et al., 2022a,b; Yu et al., 2023b; Gao et al., 2023) và sử dụng các verbalization mô tả để tạo ra khả năng phân tách nhãn trong tập dữ liệu cuối cùng.

FEWGEN sử dụng mục tiêu mô hình ngôn ngữ nhân quả tiêu chuẩn để tạo ra xác suất token tiếp theo từ LLM giáo viên, MLLM. Nucleus sampling (Holtzman et al., 2019) được sử dụng để lấy mẫu tự hồi quy các token tiếp theo cho đến khi token <eos> được tạo ra. Điều này trở thành ví dụ tổng hợp ˜xi.

˜xᵢⱼ ∼ pMLLM(·|˜xᵢ<j, Pτ(Iinv, V(y)))                    (1)

Đối với mỗi nhãn y, chúng tôi cố định prompt này và lấy mẫu m/C lần để tạo ra tập dữ liệu tổng hợp.

Trong SYNTHESIZ RR, chúng tôi tạo ra tập dữ liệu tổng hợp từ mỗi bộ ba trong DRETR. Các tài liệu được truy xuất ΓK = [r1, . . . , rK] có sự chồng chéo từ vựng và ngữ nghĩa với truy vấn xICL. Tuy nhiên, các tài liệu kho ngữ liệu có thể khác biệt về phân phối so với các biến đồng thay đổi nhiệm vụ thực, do bản chất của các tài liệu hoặc quá trình phân đoạn (Mialon et al., 2023). Để giải quyết điều này, chúng tôi sử dụng MLLM để thực hiện đảo ngược nhiệm vụ từ nội dung của mỗi tài liệu được truy xuất, một quá trình mà chúng tôi gọi là tinh chỉnh ngữ cảnh. Pτ do đó được cấu thành từ hướng dẫn tinh chỉnh ngữ cảnh Rinv, mỗi tài liệu rk ∈ ΓK, và mục tiêu được verbalize cho truy vấn, tức là V(yICL). Cửa sổ ngữ cảnh của LLM do đó thấy một prompt duy nhất và có căn cứ khi tự hồi quy tạo ra mỗi đầu vào tổng hợp ˜xi:

˜xᵢⱼ ∼ pMLLM(·|˜xᵢ<j, Pτ(Rinv, rk, V(yICL)))                (2)

đối với tất cả các tài liệu rk ∈ ΓK. Chúng tôi tiếp tục sử dụng nucleus sampling để có được các thế hệ đa dạng. Mỗi ví dụ trong ngữ cảnh ban đầu do đó tạo ra K ví dụ tổng hợp duy nhất {˜x1, . . . , ˜xK}; chúng tôi gọi K là "hệ số mở rộng". Để thúc đẩy việc tuân thủ Rinv, chúng tôi lấy mẫu các cặp từ DICL để tạo ra các ví dụ trong ngữ cảnh theo cùng định dạng. Tập dữ liệu cuối cùng của chúng tôi được xây dựng như:

DSYNTH = ⋃(x,y,ΓK)∈DRETR ⋃rk∈ΓK {(˜xi, y)}

Bước 4. Chưng cất học sinh: Học sinh được fine-tune trên DSYNTH bằng cách truyền embedding token [CLS] của BERT của ˜x qua một lớp feedforward. Điều này tạo ra một phân phối xác suất trên không gian nhãn C. Chúng tôi tối ưu hóa tổn thất cross-entropy của nhãn thực y. Vì chúng tôi dẫn xuất ˜x từ một LLM giáo viên, điều này có thể được coi là một hình thức chưng cất kiến thức tượng trưng (West et al., 2022).

4 Thiết lập Thực nghiệm
Các nhiệm vụ và độ khó của chúng. Chúng tôi thực hiện các thực nghiệm chính của mình trên 6 tập dữ liệu đầu tiên trong Bảng 1, được lựa chọn cẩn thận để đo lường cách LLM giáo viên thực hiện trên các nhiệm vụ đảo ngược nhiệm vụ có độ khó khác nhau. Nghiên cứu trước đây chỉ đánh giá các tập dữ liệu phân loại cảm xúc và chủ đề như IMDB (Maas et al., 2011) và AG NEWS (Zhang et al., 2015). Chúng tôi mở rộng từ phân loại chủ đề, chủ yếu liên quan đến tóm tắt trong bước đảo ngược nhiệm vụ, mà LLM rất giỏi (Goyal et al., 2022). HYPERPARTISAN (Kiesel et al., 2019) phát hiện thiên vị trong tin tức chính trị, vì vậy bước đảo ngược nhiệm vụ bao gồm việc viết lại đáng kể các bài báo trung lập được truy xuất để tạo thành các ví dụ thiên vị. CATEGORY và POLARITY là các nhiệm vụ đánh giá sản phẩm phổ biến (Yu et al., 2023a,b; Gao et al., 2023); chúng tôi tạo ra các đánh giá từ các sản phẩm được truy xuất phải tuân thủ các lớp phân loại và cảm xúc. Đảo ngược nhiệm vụ cho HUMOR (Ziser et al., 2020) liên quan đến việc tạo ra các câu hỏi hài hước từ chi tiết sản phẩm được truy xuất, đòi hỏi các kỹ năng bổ sung từ giáo viên.

[THIS IS TABLE: Bảng 1 hiển thị thống kê tập dữ liệu và ước tính về độ khó đảo ngược nhiệm vụ, bao gồm các tập dữ liệu như AG NEWS, TOI HEADLINES, HYPERPARTISAN, POLARITY, CATEGORY, HUMOR, IMDB, SST-2 với số lượng lớp, kích thước train/test, kho ngữ liệu và mức độ khó]

Bảng 2 mô tả các kho ngữ liệu được sử dụng để truy xuất. Chúng tôi xem xét năm kho ngữ liệu trong các miền khác nhau, mỗi kho có số lượng bản ghi khác nhau. Ba kho là các tập con của REALNEWS (Zellers et al., 2019), như được mô tả trong Phụ lục I: REALNEWS/DOMINANT (Tin tức Mỹ/EU), REALNEWS/REGIONAL (Tin tức Khu vực), REALNEWS/INDIA (Tin tức Ấn Độ). Chúng tôi cũng sử dụng PRODUCTS (metadata sản phẩm Amazon, (Ni et al., 2019)) và MOVIE SUMMARY (tóm tắt phim, (Bamman et al., 2013)). Mỗi nhiệm vụ trong Bảng 1 được liên kết với kho ngữ liệu mà chúng tôi coi là phù hợp nhất. Trong §7, chúng tôi so sánh với bốn phương pháp trước đây trên ba nhiệm vụ khác: IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013) và AG NEWS. Các nhiệm vụ cảm xúc và chủ đề này ít phù hợp với mục tiêu của chúng tôi và do đó được loại trừ khỏi đánh giá chính.

[THIS IS TABLE: Bảng 2 hiển thị thống kê kho ngữ liệu với tokenizer LLAMA2, bao gồm tên miền, kích thước, định dạng tài liệu và số token]

Các mô hình. Chúng tôi sử dụng CONTRIEVER (Izacard et al., 2022) để truy xuất dày đặc từ mỗi kho ngữ liệu. Điều này thực hiện một khớp ngữ nghĩa giữa truy vấn và mỗi tài liệu sử dụng độ tương tự cosine. Trong Phụ lục E, chúng tôi cũng thực hiện một nghiên cứu loại bỏ sử dụng BM25 như một bộ truy xuất thưa thớt, thực hiện khớp từ vựng giữa mỗi cặp truy vấn-tài liệu.

Như các mô hình giáo viên, chúng tôi chủ yếu sử dụng Llama-2 Chat 13B đông lạnh (Touvron et al., 2023b) cho bước đảo ngược nhiệm vụ trong SYNTHESIZ RR và FEWGEN. Chúng tôi cũng thử nghiệm với CLAUDE INSTANT-V1 như được mô tả trong Phụ lục J.

Đối với việc học trong ngữ cảnh (ICL) (Brown et al., 2020), chúng tôi chọn các ví dụ ngẫu nhiên từ tập huấn luyện: 50 ví dụ ICL/lớp cho đa lớp và 100/lớp cho các nhiệm vụ nhị phân. Chúng tôi tin rằng đây là một số lượng ví dụ thực tế mà một nhà thiết kế hệ thống có thể nguồn nếu họ nỗ lực xây dựng một mô hình chuyên môn. Chúng tôi khám phá các cách tiếp cận để bootstrap tập hạt giống này trong các thiết lập giám sát hạn chế Phụ lục C.

Hiệu suất chuyên môn hóa được đo lường trên các LM học sinh DEBERTA-V3-LARGE (435M tham số, He et al. (2021)) và DISTILBERT (66M tham số, Sanh et al. (2019)).

Tiêu chí đánh giá. Việc tạo văn bản có thể khó đánh giá một cách khách quan trong các kịch bản đa nhiệm vụ (Chang et al., 2024). Do đó trong §5 chúng tôi đánh giá văn bản tổng hợp dựa trên nhiều tiêu chí, để phát hiện các hành vi mà chúng tôi quan sát trong quá trình tổng hợp như trong Bảng 3. Self-BLEU (Papineni et al., 2002; Zhu et al., 2018) đo lường tính đa dạng từ vựng của tập dữ liệu dựa trên sự chồng chéo n-gram giữa các cặp ví dụ. Entity entropy đo lường tính đa dạng của các thực thể sử dụng phân phối xác suất của mỗi loại trong 16 loại thực thể, được suy luận bằng cách sử dụng en_core_web_lg của spaCy (Honnibal et al., 2020). Các tập dữ liệu đại diện quá mức cho các thực thể phổ biến ghi điểm thấp hơn về entropy. Mặt khác, Entity recall và Entity KL divergence so sánh sự tương tự của các thực thể so với GOLD, và các tập dữ liệu tái tạo các thực thể thường xuyên được thấy trong dữ liệu GOLD ghi điểm cao hơn. MAUVE (Liu et al., 2021) đo lường sự tương tự với văn bản do con người viết bằng cách sử dụng các biểu diễn được tiền huấn luyện từ một mô hình gpt2-xl, chỉ ra sự khác biệt phân phối trong văn bản được tạo ra.

[Tiếp tục với các hình và bảng còn lại...]

Hình 3: Self-BLEU (↓) cho ngrams n=1-5. So sánh: GOLD, FEWGEN 0-shot, FEWGEN 32-shot, SYNTHESIZ RR 0-shot, SYNTHESIZ RR 3-shot RETRICL, SYNTHESIZ RR 32-shot NON-RETRICL.

Hình 4: Entity entropy (↑) trên TOI (tiêu đề) và CATEGORY (đánh giá). So sánh: GOLD, FEWGEN 32-shot, SYNTHESIZ RR 3-shot RETRICL và SYNTHESIZ RR 32-shot NON-RETRICL. Kết quả zero-shot tương tự cho SYNTHESIZ RR và tệ hơn cho FEWGEN; chúng tôi bỏ qua chúng.

--- TRANG 5 ---
[Tiếp tục dịch nội dung từ trang 5 trở đi theo cùng cách...]
