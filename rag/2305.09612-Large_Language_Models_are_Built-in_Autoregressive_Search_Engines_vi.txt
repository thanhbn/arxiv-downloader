# Các Mô hình Ngôn ngữ Lớn là Công cụ Tìm kiếm Tự hồi quy Tích hợp sẵn
Noah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang
Đại học Notre Dame
{nziems2, wyu1, zzhang23, mjiang2}@nd.edu

## Tóm tắt
Truy xuất tài liệu là giai đoạn quan trọng của các công cụ tìm kiếm Web tiêu chuẩn. Các bộ truy xuất dense dual-encoder hiện có thu được biểu diễn cho câu hỏi và tài liệu một cách độc lập, chỉ cho phép tương tác nông giữa chúng. Để khắc phục hạn chế này, các công cụ tìm kiếm tự hồi quy gần đây thay thế kiến trúc dual-encoder bằng việc trực tiếp tạo ra các định danh cho tài liệu liên quan trong nhóm ứng viên. Tuy nhiên, chi phí huấn luyện của các công cụ tìm kiếm tự hồi quy này tăng mạnh khi số lượng tài liệu ứng viên tăng lên. Trong bài báo này, chúng tôi phát hiện rằng các mô hình ngôn ngữ lớn (LLM) có thể làm theo hướng dẫn của con người để trực tiếp tạo ra URL cho việc truy xuất tài liệu. Đáng ngạc nhiên, khi cung cấp một vài cặp Query-URL làm minh họa trong ngữ cảnh, LLM có thể tạo ra các URL Web mà gần 90% các tài liệu tương ứng chứa câu trả lời đúng cho các câu hỏi miền mở. Theo cách này, LLM có thể được coi là các công cụ tìm kiếm tích hợp sẵn, vì chúng không được huấn luyện rõ ràng để ánh xạ câu hỏi đến các định danh tài liệu. Các thí nghiệm chứng minh rằng phương pháp của chúng tôi có thể đạt được hiệu suất truy xuất tốt hơn một cách nhất quán so với các phương pháp truy xuất hiện có với biên độ đáng kể trên ba benchmark trả lời câu hỏi miền mở, trong cả hai cài đặt zero-shot và few-shot. Mã nguồn cho nghiên cứu này có thể tìm thấy tại https://github.com/Ziems/llm-url.

## 1 Giới thiệu
Cùng với thành công của deep learning, các bộ truy xuất dựa trên dual-encoder đã trở thành phương pháp chủ đạo cho tìm kiếm Web (Zhu et al., 2021; Zhao et al., 2022). Ví dụ, DPR (Karpukhin et al., 2020) sử dụng hai encoder độc lập để mã hóa câu hỏi và tài liệu tương ứng, sau đó ước tính mức độ liên quan của chúng bằng cách tính một điểm tương tự duy nhất giữa hai biểu diễn.

Tuy nhiên, các phương pháp này có hai nhược điểm chính. Thứ nhất, các biểu diễn của câu hỏi và tài liệu thường được thu thập độc lập trong các mô hình truy xuất dense dual-encoder hiện đại (Karpukhin et al., 2020), chỉ cho phép tương tác nông giữa chúng (Khattab et al., 2021). Thứ hai, biểu diễn câu hỏi hoặc tài liệu được nhúng vào một vector dense duy nhất, có thể bỏ lỡ thông tin chi tiết khi tính toán độ tương tự giữa hai biểu diễn vector (Khattab and Zaharia, 2020).

Thay vì tính toán độ tương tự giữa các embedding câu hỏi và tài liệu, các công cụ tìm kiếm tự hồi quy nhằm mục đích trực tiếp tạo ra các định danh tài liệu sau đó ánh xạ chúng đến các tài liệu hoàn chỉnh trong nhóm ứng viên được xác định trước. Phương pháp này đã thu hút sự quan tâm ngày càng tăng trong lĩnh vực truy xuất thông tin (IR) và các lĩnh vực liên quan (Tay et al., 2022; Bevilacqua et al., 2022; Wang et al., 2022). So với các phương pháp truy xuất dense dual-encoder, các công cụ tìm kiếm tự hồi quy có một số ưu điểm. Thứ nhất, các mô hình tạo sinh tự hồi quy tạo ra các định danh tài liệu bằng cách thực hiện cross-attention sâu ở mức token, dẫn đến ước tính tốt hơn so với tương tác nông trong các bộ truy xuất dense. Thứ hai, các công cụ tìm kiếm tự hồi quy đã được chứng minh có khả năng tổng quát hóa mạnh, vượt trội hơn BM25 trong cài đặt zero-shot (Tay et al., 2022). Mặc dù về mặt lý thuyết có thể mở rộng một công cụ tìm kiếm tự hồi quy đến kích thước của một mô hình ngôn ngữ lớn (LLM), chẳng hạn như GPT-3 với 175B tham số, trong thực tế điều này không khả thi do chi phí tính toán của việc huấn luyện một công cụ tìm kiếm tự hồi quy lớn như vậy từ đầu (Tay et al., 2022). Để giảm chi phí huấn luyện cao của công cụ tìm kiếm tự hồi quy, kích thước mô hình nhỏ hơn được ưa chuộng. Tuy nhiên, kết quả của nghiên cứu thí điểm của chúng tôi trong Hình 1 cho thấy các mô hình ngôn ngữ nhỏ hơn kém hơn đáng kể trong việc ánh xạ đoạn văn đến các định danh tài liệu so với các mô hình lớn hơn. Hơn nữa, các tác vụ truy xuất khác nhau có thể có yêu cầu truy xuất độc đáo. Một tác vụ có thể yêu cầu mô hình truy xuất bằng chứng thực tế để hỗ trợ hoặc bác bỏ một tuyên bố (tức là, kiểm tra thực tế) (Onoe et al., 2021) trong khi tác vụ khác có thể yêu cầu mô hình truy xuất thông tin trivia cụ thể về một thực thể (tức là, liên kết thực thể) (Petroni et al., 2021; Zhang et al., 2022). Sẽ tốt hơn nếu bộ truy xuất có khả năng tổng quát hóa cho các tác vụ truy xuất mới chỉ với một vài ví dụ.

Trong nghiên cứu này, chúng tôi khám phá việc sử dụng các minh họa trong ngữ cảnh để nhắc nhở LLM trực tiếp tạo ra các URL web cho việc truy xuất tài liệu, được gọi là LLM-URL. Đáng ngạc nhiên, chúng tôi phát hiện rằng bằng cách cung cấp một vài cặp (truy vấn, URL) làm minh họa ngữ cảnh, các mô hình ngôn ngữ lớn (ví dụ GPT-3) tạo ra các URL Web mà gần 90% các tài liệu tương ứng chứa câu trả lời cho các câu hỏi miền mở. Theo cách này, LLM có thể được coi là các công cụ tìm kiếm tích hợp sẵn, vì chúng không được huấn luyện rõ ràng để ánh xạ câu hỏi hoặc tài liệu đến các định danh. Thay vì sử dụng các định danh tài liệu mới được tạo, LLM-URL tận dụng trực tiếp các định danh tài liệu hiện có và được sử dụng rộng rãi, tức là URL. Chúng tôi so sánh phương pháp của mình với các phương pháp truy xuất tài liệu hiện có trên ba bộ dữ liệu trả lời câu hỏi miền mở (QA) khác nhau: WebQ (Berant et al., 2013), NQ (Kwiatkowski et al., 2019), và TriviaQA (Joshi et al., 2017). Hơn nữa, để tránh vượt quá giới hạn số lượng token đầu vào của LLM, chúng tôi sử dụng một mô-đun lọc đoạn văn không giám sát để loại bỏ các phần không liên quan của tài liệu hỗ trợ. Tóm lại, các đóng góp chính của chúng tôi như sau:

1. Chúng tôi tiết lộ rằng LLM là các công cụ tìm kiếm tự hồi quy tích hợp sẵn có khả năng truy xuất tài liệu bằng cách trực tiếp tạo ra URL trang Web trong cả hai cài đặt zero-shot và few-shot.

2. Chúng tôi chỉ ra rằng việc truy xuất tài liệu bằng cách tạo URL với LLM vượt trội đáng kể so với các phương pháp hiện có để truy xuất tài liệu, được đo bằng Recall@K. Hơn nữa, chúng tôi chỉ ra rằng việc chia các tài liệu đã truy xuất thành các đoạn văn sau đó sử dụng một bộ xếp hạng để lọc các đoạn văn giảm đáng kể số lượng đoạn văn hỗ trợ trong khi duy trì recall cao.

3. Chúng tôi chỉ ra rằng các tài liệu đã truy xuất cải thiện hiệu suất QA downstream được đo bằng EM khi so sánh với các phương pháp baseline.

## 2 Nghiên cứu Liên quan

### 2.1 Các Bộ Truy xuất Tài liệu Truyền thống
Các phương pháp truyền thống như TF-IDF và BM25 khám phá các chiến lược truy xuất thưa thớt bằng cách khớp nội dung chồng lấp giữa câu hỏi và đoạn văn (Robertson and Zaragoza, 2009; Chen et al., 2017; Yang et al., 2019). DPR (Karpukhin et al., 2020) đã cách mạng hóa lĩnh vực này bằng cách sử dụng các vector ngữ cảnh dense để lập chỉ mục đoạn văn. Nó được khởi tạo đầu tiên như một mô hình BERT được huấn luyện trước, sau đó được huấn luyện một cách phân biệt sử dụng các cặp truy vấn và tài liệu liên quan, với các negatives khó từ BM25. Nghiên cứu gần đây đã cải thiện DPR thông qua các chiến lược huấn luyện tốt hơn (Xiong et al., 2020; Qu et al., 2021; Zhang et al., 2023a) và xếp hạng lại đoạn văn (Mao et al., 2021; Yu et al., 2021; Ju et al., 2022). Tuy nhiên, các biểu diễn của câu hỏi và tài liệu thường được thu thập độc lập trong các mô hình truy xuất dense dual-encoder hiện đại (Karpukhin et al., 2020; Xiong et al., 2020), chỉ cho phép tương tác nông giữa chúng (Khattab et al., 2021).

### 2.2 Các Công cụ Tìm kiếm Tự hồi quy
Các nghiên cứu gần đây đã điều tra việc sử dụng các mô hình ngôn ngữ tự hồi quy để tạo ra các chuỗi định danh cho tài liệu như một mục tiêu trung gian cho việc truy xuất (Yu et al., 2022), chẳng hạn như tiêu đề trang Wikipedia (De Cao et al., 2020), đường dẫn root-to-leaf trong cây phân cụm phân cấp (Tay et al., 2022), hoặc các n-gram đặc biệt có thể được ánh xạ đến các đoạn văn đầy đủ (Bevilacqua et al., 2022). Vì chuỗi nghiên cứu này được thực hiện gần như đồng thời bởi các nhóm nghiên cứu khác nhau, chúng thường được gọi bằng nhiều tên khác nhau trong tài liệu, chẳng hạn như công cụ tìm kiếm tự hồi quy, chỉ mục tìm kiếm khác biệt (DSI), và bộ lập chỉ mục tài liệu neural (NDI). So với các bộ truy xuất tài liệu dense truyền thống, các phương pháp này tận dụng một mô hình tạo sinh để tạo ra các chỉ mục tài liệu. Bằng cách buộc mô hình tạo sinh giải thích mọi token trong câu hỏi và tài liệu bằng cross-attention, khả năng tạo sinh của mô hình được cải thiện đáng kể. Nghiên cứu của chúng tôi có liên quan chặt chẽ đến các nghiên cứu này, chỉ ra thông qua thí nghiệm rằng việc nhắc nhở đúng cách các mô hình ngôn ngữ lớn được huấn luyện trước có thể đạt được hiệu suất tốt hơn so với các mô hình truy xuất dense truyền thống (Ouyang et al., 2022; Yu et al., 2023).

## 3 Phương pháp Đề xuất

Trong phần này, chúng tôi mô tả một phương pháp mới, mà chúng tôi gọi là LLM-URL, sử dụng một mô hình ngôn ngữ lớn (LLM) để thực hiện truy xuất tài liệu web hiệu quả và hiệu suất cho các tác vụ NLP cần nhiều kiến thức như trả lời câu hỏi miền mở (ODQA).

ODQA là một quy trình hai bước bao gồm một bộ truy xuất và một bộ đọc. Cho một câu hỏi q, mục tiêu của bộ truy xuất là tìm top-n đoạn văn Pn liên quan để trả lời q. Cho q và top-n đoạn văn liên quan Pn, mục tiêu của bộ đọc là sử dụng kiến thức nội bộ cùng với Pn để tạo ra một câu trả lời đúng a cho câu hỏi q. Bộ truy xuất đoạn văn đóng vai trò thiết yếu trong quy trình này. Khi Pn chứa nhiều đoạn văn có câu trả lời đúng hơn, bộ đọc có cơ hội cao hơn để tìm thấy nó. Thay vì huấn luyện nặng một bộ truy xuất chuyên dụng, LLM-URL của chúng tôi giải quyết vấn đề theo cách khác như được thể hiện trong Hình 2.

Cho một câu hỏi q, LLM-URL của chúng tôi nên tìm một tập hợp các đoạn văn liên quan Pn và đưa nó cho bộ đọc. Đầu tiên, nó nhắc nhở một LLM (ví dụ, GPT-3) để trực tiếp tạo ra m URL cho q. Theo mặc định, nó sử dụng "Which m Wikipedia URLs would have the answer?" làm hướng dẫn được nối vào cuối mỗi câu hỏi đầu vào làm prompt. Chúng tôi cũng nối phần đầu của URL Wikipedia (https://en.wikipedia.org/wiki) vào cuối prompt để khuyến khích việc tạo ra URL và hạn chế việc tạo sinh theo định dạng URL bài viết Wikipedia. Vì LLM có khả năng học trong ngữ cảnh, chúng tôi tận dụng lợi thế này để cho phép cài đặt few-shot trong prompt. Prompt được mô tả ở trên cũng bao gồm một chuỗi các minh họa trong ngữ cảnh. Mỗi minh họa chứa một câu hỏi được lấy mẫu từ tập huấn luyện theo prompt được mô tả ở trên. Ở cuối mỗi minh họa, m URL trỏ đến các tài liệu được gán nhãn vàng được liệt kê. Trong cài đặt zero-shot, prompt gốc được sử dụng mà không có bất kỳ minh họa nào. Trong cài đặt few-shot, prompt gốc được nối với một chuỗi d minh họa (d=10 trong nghiên cứu này).

Cho prompt, LLM trả về một chuỗi token được tạo ra. Lý tưởng nhất, các token này sẽ xây dựng một chuỗi m URL được phân tách. Trong thực tế, chuỗi được tạo ra thường có thông tin bổ sung như một câu trả lời được đề xuất không đáng tin cậy và cần được lọc. Chúng tôi sử dụng một biểu thức chính quy để trích xuất tất cả URL từ chuỗi và loại bỏ tất cả thông tin bổ sung. Điều này cũng lọc ra nhiều URL không được định dạng đúng. Sau khi trích xuất, các yêu cầu GET được thực hiện bằng các URL đã trích xuất và nội dung của mỗi lần truy xuất được sử dụng để tạo ra một tập hợp các tài liệu đã lấy Df. Thường thì |Df| < m vì một số URL được tạo ra không tuân theo định dạng đúng hoặc không trỏ đến các trang web thực tế trên Internet.

Tập hợp các tài liệu đã lấy Df có thể được chuyển trực tiếp cho một bộ đọc nếu m là một giá trị nhỏ hoặc bộ đọc được sử dụng có thể xử lý nhiều tài liệu lớn. Tuy nhiên, đây thường không phải là trường hợp. Thường thì, Df cần được lọc sao cho chỉ một số lượng nhỏ các đoạn văn liên quan nhất được đưa cho bộ đọc. Để làm điều này, LLM-URL của chúng tôi đầu tiên chia mỗi tài liệu d ∈ Df thành một tập hợp các đoạn văn nhỏ. Các đoạn văn từ mỗi tài liệu được thu thập vào một tập hợp mới, Pf. Một hàm tính điểm được sử dụng để định lượng mức độ liên quan của mỗi đoạn văn đối với câu hỏi q, với giá trị cao cho biết mức độ liên quan cao đối với q và điểm thấp cho biết mức độ liên quan thấp. Một hàm tính điểm đơn giản như BM25 có thể được sử dụng hoặc một hàm phức tạp hơn như DPR (Karpukhin et al., 2020). Các đoạn văn trong Pf sau đó được sắp xếp từ cao đến thấp và top n được giữ lại làm Pn. Cuối cùng, Pn được đưa cho một bộ đọc cùng với q để tạo ra một câu trả lời.

**Ưu điểm của LLM-URL**: Các phương pháp truy xuất tự hồi quy hiện có như DSI và SEAL sử dụng một mô hình ngôn ngữ lớn được huấn luyện trước sau đó tinh chỉnh nó để nhận câu hỏi làm đầu vào và tạo ra các định danh tài liệu liên quan làm đầu ra (Tay et al., 2022; Bevilacqua et al., 2022). Cả DSI và SEAL đều thực hiện các thí nghiệm mở rộng trên nhiều định danh tài liệu khác nhau được tạo ra bởi một mô hình ngôn ngữ được huấn luyện nặng. Các ví dụ về những định danh này bao gồm các định danh nguyên tử không có cấu trúc, các định danh chuỗi có cấu trúc ngây thơ, phân cụm tài liệu phân cấp, và các định danh khác. Thay vào đó, LLM-URL sử dụng các định danh tài liệu có sẵn trước đó tồn tại trên internet: URL. Việc sử dụng URL thay vì các định danh được đề cập ở trên có nhiều ưu điểm. URL thường chứa các từ liên quan đến thông tin mà chúng liên kết, cho phép liên kết mạnh mẽ các chủ đề với URL của chúng. Ví dụ, tiêu đề của mỗi trang Wikipedia được sử dụng trong URL của nó, cho phép LLM có thể trực tiếp tạo ra URL bằng cách tận dụng thông tin ngữ nghĩa từ câu hỏi. Để xác nhận tầm quan trọng của chính bản thân URL, chúng tôi cũng thí nghiệm với việc nhắc nhở LLM tạo ra tiêu đề Wikipedia thay vì URL và phát hiện Recall@1 giảm đáng kể so với việc nhắc nhở tạo URL. Chúng tôi tin rằng điều này là do bản thân định dạng URL giúp nhắc nhở mô hình cho thông tin cụ thể theo định dạng cụ thể. Hơn nữa, việc sử dụng URL cho phép chúng tôi đơn giản thu được tài liệu bằng chứng thông qua một yêu cầu HTTP mà không cần huấn luyện mô hình hoặc xây dựng chỉ mục để tìm ánh xạ giữa định danh và tài liệu.

## 4 Thí nghiệm

Trong phần này, chúng tôi trình bày và thảo luận kết quả từ các thí nghiệm của mình để "trực tiếp" chứng minh rằng LLM-URL của chúng tôi là một bộ truy xuất mạnh và "gián tiếp" chỉ ra rằng nó đạt được hiệu suất cạnh tranh trên tác vụ ODQA so với các giải pháp tiên tiến.

**Mô hình Ngôn ngữ Lớn**: Theo Hình 1, mô hình ngôn ngữ lớn mà chúng tôi sử dụng để tạo URL cho các thí nghiệm của mình là GPT-3 text-davinci-003 với giải mã tham lam và nhiệt độ 0. Nhiều prompt khác nhau được thử nghiệm để tạo URL, nhưng ít sự khác biệt về hiệu suất được quan sát, vì vậy chúng tôi đơn giản sử dụng prompt có hiệu suất tốt nhất được thảo luận trong Phần 3.

**Bộ dữ liệu**: Chúng tôi sử dụng ba bộ dữ liệu ODQA bao gồm Web Questions, Natural Questions, và Trivia QA. Chúng tôi sử dụng chúng để thực hiện đánh giá trên cả tác vụ truy xuất tài liệu hoặc đoạn văn và chính ODQA.

### 4.1 Truy xuất

Chúng tôi mong đợi các bộ truy xuất tìm thấy các tài liệu và/hoặc đoạn văn liên quan nhất. Chúng tôi tiến hành thí nghiệm trên cả truy xuất tài liệu và truy xuất đoạn văn.

**Chỉ số đánh giá**: Recall@k (k=1, 10, 100) được tính bằng cách đo phần trăm tài liệu hoặc đoạn văn trong top-k chứa một trong các câu trả lời được gán nhãn vàng trong khi exact match được tính bằng phần trăm câu trả lời dự đoán khớp với một trong các câu trả lời được gán nhãn vàng. Mặc dù LLM-URL không bị ràng buộc bởi URL nào có thể được tạo ra để truy xuất tài liệu, chúng tôi hạn chế tất cả các lần tạo sinh chỉ đối với URL Wikipedia để so sánh công bằng, như được thảo luận trong Phần 3. Tất cả các mô hình baseline cũng sử dụng Wikipedia để truy xuất, với một số lấy tài liệu theo thời gian thực và những mô hình khác lấy từ một corpus offline.

#### 4.1.1 Truy xuất Tài liệu

**Baseline**: Contriever (Izacard et al., 2021) và BM25 (Robertson and Zaragoza, 2009) thường được sử dụng để truy xuất đoạn văn. Contriever là một dual encoder sử dụng tích vô hướng giữa các biểu diễn dense của câu hỏi và đoạn văn để tính toán mức độ liên quan. BM25 là một bộ truy xuất thưa thớt sử dụng nội dung chồng lấp giữa câu hỏi và đoạn văn để tính toán mức độ liên quan. Vì chúng tôi sử dụng cùng kích thước đoạn văn để chia nhỏ các tài liệu Wikipedia, chúng tôi có thể ánh xạ các đoạn văn được truy xuất của chúng trở lại các tài liệu gốc. Chúng tôi sử dụng Google API (Brin and Page, 1998) bị hạn chế đối với Wikipedia như một baseline thứ ba để truy xuất các tài liệu liên quan cho một câu hỏi.

Các nghiên cứu hiện có như DSI và SEAL đã điều tra việc sử dụng các mô hình ngôn ngữ tự hồi quy để tạo ra các chuỗi định danh cho tài liệu như một mục tiêu trung gian cho việc truy xuất. DSI là một Transformer đã được huấn luyện để ánh xạ trực tiếp từ câu hỏi đến các định danh tài liệu bằng cách ghi nhớ nội dung của toàn bộ corpus (Tay et al., 2022). SEAL là một biến thể của DSI sử dụng ngrams làm id tài liệu để cải thiện hiệu suất truy xuất (Bevilacqua et al., 2022). Cả DSI và SEAL đều không báo cáo kết quả truy xuất trên các tài liệu đầy đủ và không có triển khai công khai, vì vậy chúng được bỏ qua và thảo luận trong Bảng 3 và Phần 4.1.2 về truy xuất đoạn văn.

Không giống như các baseline, LLM-URL của chúng tôi sử dụng một LLM. Nó có hai cài đặt: zero-shot và few-shot. Trong cài đặt zero-shot, không có minh họa trong ngữ cảnh nào được đưa ra trong khi ở cài đặt few-shot một vài minh họa được nối vào prompt.

**Kết quả**: Kết quả của các thí nghiệm truy xuất tài liệu của chúng tôi được thể hiện trong Bảng 1. Trong cài đặt này, Recall@k được tính trực tiếp sau khi các tài liệu được truy xuất mà không có các bước trung gian. LLM-URL vượt trội đáng kể so với các phương pháp baseline trên tất cả các bộ dữ liệu cho cả Recall@1 và Recall@10. Cụ thể, LLM-URL zero-shot cải thiện Recall@1 tài liệu tương đối 20.4%, 11.2%, và 13.2% so với baseline mạnh nhất trên WebQ, NQ, và TriviaQA, tương ứng. LLM-URL few-shot tiếp tục mở rộng cải thiện lên 24.9%, 12.8%, và 16.7%, tương ứng. URL có thể được trích xuất từ các tham số quy mô lớn của LLM, và những URL này có thể dẫn đến các tài liệu chính xác hơn so với những gì các phương pháp hiện có có thể truy xuất. Cả các tham số LLM và minh họa trong ngữ cảnh đều hữu ích đáng kể trong việc truy xuất tài liệu.

Hình 3 cho thấy điểm Recall hội tụ khi số lượng URL được tạo ra m tăng lên. Do lợi nhuận giảm dần từ việc tăng m, các thí nghiệm của chúng tôi không khám phá các giá trị m lớn hơn 10.

**Các URL được tạo ra có hợp lệ không?** Điều đáng chú ý là các URL được tạo ra không phải lúc nào cũng hợp lệ. Một số URL được tạo ra không có cú pháp URL hợp lệ và một số trỏ đến các trang Wikipedia không tồn tại. Hiếm khi, các URL sẽ được tạo ra cho các domain ngoài Wikipedia. Để so sánh công bằng, tất cả những URL lỗi này được loại bỏ và chỉ các tài liệu đến từ các bài viết Wikipedia hợp lệ được giữ lại.

Phân tích sâu hơn được thực hiện để đo tỷ lệ URL Wikipedia hợp lệ trong khi tổng số URL được tạo ra m tăng từ 1 đến 10, được thể hiện trong Hình 4. Số lượng URL hợp lệ được tạo ra vẫn cao một cách đáng ngạc nhiên (tức là, cao hơn 68%) khi m tăng từ 1 đến 10. Tuy nhiên, tỷ lệ tạo ra hợp lệ dường như giảm khi m tăng, cho thấy có lợi nhuận giảm dần từ mỗi sự gia tăng biên của m.

#### 4.1.2 Truy xuất Đoạn văn

**Baseline**: Bốn phương pháp, bao gồm Contriever, BM25, DSI (Tay et al., 2022), và SEAL (Bevilacqua et al., 2022), đã được giới thiệu trong Phần 4.1.1. Google API được sử dụng để truy xuất tài liệu và không được áp dụng cho đoạn văn.

**Kết quả**: Kết quả của các thí nghiệm truy xuất đoạn văn của chúng tôi được thể hiện trong Bảng 2. Trong cài đặt này, Recall@k được tính trên top-k đoạn văn được xếp hạng bởi bộ xếp hạng thay vì chỉ trên các tài liệu thô được thể hiện trong Bảng 1. LLM-URL hoạt động tốt hơn một chút so với các phương pháp baseline cho Recall@1 và Recall@10 và cũng tốt như các phương pháp baseline cho Recall@100. Trong cài đặt zero-shot, LLM-URL cải thiện Recall@1 tương đối 16.2%, 5.3%, và 1.1% so với baseline mạnh nhất trên WebQ, NQ, và TriviaQA tương ứng. Cài đặt few-shot của LLM-URL mở rộng cải thiện lên 16.8%, 11.8%, và 6.3%, tương ứng. Đối với Recall@10, các cải thiện tương tự có thể được thấy.

Đối với Recall@100, hiệu suất tốt hơn so với các mô hình baseline trên tất cả các bộ dữ liệu trừ NQ. Trong cài đặt zero-shot, LLM-URL cải thiện Recall@100 tương đối 5.0% cho WebQ và hoạt động kém hơn một chút so với phương pháp baseline tốt nhất trên NQ và TriviaQA lần lượt là 1.7% và 0.4%. Cài đặt few-shot của LLM-URL cho Recall@100 cho thấy cải thiện nhẹ trên WebQ và TriviaQA, nhưng hoạt động kém hơn một chút so với baseline mạnh nhất trên NQ.

Mặc dù bị hạn chế chỉ với các đoạn văn từ 10 tài liệu, LLM-URL hoạt động tốt hơn các phương pháp baseline cho k nhỏ hơn và hoạt động cũng tốt như các phương pháp baseline cho các giá trị k cao hơn.

So sánh giữa LLM-URL và các phương pháp dựa trên định danh tài liệu hiện có như DSI và SEAL được thể hiện trong Bảng 3. Đối với Recall@1, LLM-URL zero-shot hoạt động kém hơn một chút so với baseline tốt nhất 8.8%. Khoảng cách hiệu suất này nhỏ hơn một chút trong cài đặt few-shot với LLM-URL hoạt động kém hơn 3.1% so với baseline tốt nhất. Đối với Recall@10, LLM-URL zero-shot hoạt động kém hơn baseline tốt nhất 18.7%. LLM-URL few-shot chỉ hoạt động tốt hơn một chút so với cài đặt zero-shot, hoạt động kém hơn baseline tốt nhất 18.4%.

### 4.2 Trả lời Câu hỏi Miền Mở

**Chỉ số đánh giá**: Chúng tôi sử dụng exact match (EM), viết tắt của exact string match với câu trả lời đúng, vì mục tiêu của ODQA là tìm một câu trả lời chính xác cho bất kỳ câu hỏi nào sử dụng các bài viết Wikipedia.

**Kết quả**: Ở đây chúng tôi thảo luận hiệu suất QA downstream của LLM-URL. Trong cài đặt này, một câu trả lời chỉ có exact match nếu văn bản được tạo ra đã chuẩn hóa nằm trong danh sách các câu trả lời được chấp nhận cho một câu hỏi. Khi kết hợp với InstructGPT làm bộ đọc, LLM-URL hoạt động tốt hơn đáng kể trên WebQ và tốt hơn một chút trên TriviaQA khi so sánh với các phương pháp baseline có hiệu suất tốt nhất. Trên NQ, LLM-URL + InstructGPT hoạt động kém hơn các NDI baseline và chỉ kém hơn một chút so với baseline còn lại tốt nhất. Trong cài đặt zero-shot, LLM-URL + InstructGPT cải thiện so với phương pháp baseline tốt nhất 13.3% và 1.3% trên WebQ và TriviaQA tương ứng. LLM-URL + InstructGPT hoạt động kém hơn phương pháp baseline tốt nhất 39.5% trên NQ. Trong cài đặt few-shot, LLM-URL + InstructGPT hoạt động tốt hơn phương pháp baseline tốt nhất 16.9% và 2.3% trên WebQ và TriviaQA tương ứng. LLM-URL + InstructGPT hoạt động kém hơn phương pháp baseline tốt nhất 37.4% trên NQ.

Mặc dù không được huấn luyện rõ ràng để truy xuất, LLM-URL + InstructGPT hoạt động tốt hơn đáng kể so với các phương pháp baseline cho WebQ, đạt được hiệu suất ngang bằng với các phương pháp hiện có cho TriviaQA, và hoạt động kém hơn một chút so với các phương pháp hiện có cho NQ.

Kết quả của chúng tôi cho thấy LLM-URL có thể là một giải pháp đầy hứa hẹn cho việc truy xuất cho một loạt rộng các tác vụ cần nhiều kiến thức với ít hoặc không cần dữ liệu huấn luyện.

### 4.3 Thảo luận

#### 4.3.1 Truy vấn Nhạy cảm với Thời gian

Có một số lợi ích định tính bổ sung mà LLM-URL có so với các phương pháp hiện có. Một lợi thế lớn của LLM-URL là các tài liệu được truy xuất theo thời gian thực từ nguồn. Miễn là nguồn luôn cập nhật mà không thay đổi chính URL đó, phương pháp đề xuất của chúng tôi có khả năng trả lời các truy vấn nhạy cảm với thời gian mà không cần bất kỳ sửa đổi bổ sung nào.

Ngược lại, các phương pháp dual encoder hiện có như Contriever yêu cầu một tài liệu phải được mã hóa lại mỗi khi nó thay đổi. Các phương pháp hiện có như SEAL (Bevilacqua et al., 2022) và DSI cũng khó để cập nhật cho các truy vấn nhạy cảm với thời gian vì LLM sẽ phải được huấn luyện lại để học nội dung mới của các tài liệu được cập nhật.

#### 4.3.2 Phân tích Thực thể Thường gặp

Theo (Mallen et al., 2022), chúng tôi phân tích hiệu suất truy xuất của LLM-URL khi thực thể câu trả lời được gán nhãn vàng là phổ biến so với khi nó không phổ biến. Đối với mỗi cặp câu hỏi-câu trả lời trong một bộ dữ liệu cho trước, chúng tôi kiểm tra xem thực thể được gán nhãn có tồn tại trong top một triệu thực thể phổ biến từ Wikipedia hay không. Sử dụng điều này, chúng tôi chia bộ dữ liệu của mình thành hai tập con riêng biệt: các cặp câu hỏi-câu trả lời chứa một thực thể phổ biến và những cặp không chứa. Trong việc đo lường hiệu suất của mô hình trên hai tập hợp riêng biệt này trên Web Questions, Natural Questions, và TriviaQA, chúng tôi thấy LLM-URL hoạt động tốt hơn đáng kể trên các cặp câu hỏi-câu trả lời thực thể phổ biến. Kết quả của phân tích của chúng tôi được thể hiện trong Hình 5. Trên cả ba bộ dữ liệu, recall của các cặp câu hỏi-câu trả lời thực thể phổ biến cao gấp nhiều lần so với recall từ phần còn lại của bộ dữ liệu.

Nghiên cứu trước đây đã chỉ ra LLM trong cài đặt closed-book, nơi mô hình phải dựa hoàn toàn vào thông tin chứa trong các weight của nó, hoạt động tốt hơn nhiều trên các thực thể phổ biến so với những thực thể không phổ biến (Mallen et al., 2022). Kết quả của chúng tôi cho thấy vấn đề này mở rộng ra ngoài cài đặt closed-book và cũng áp dụng cho việc truy xuất khi sử dụng LLM-URL. Điều này cũng có thể giải thích số từ cao từ các tài liệu mà chúng tôi tìm thấy khi đánh giá LLM-URL. Bài viết Wikipedia trung bình có 644 từ, nhưng số từ trung bình từ các tài liệu Wikipedia được truy xuất qua LLM-URL là 10k. Chúng tôi tin rằng sự chênh lệch này được gây ra bởi các thực thể phổ biến có nhiều chi tiết hơn trong các bài viết Wikipedia của chúng và do đó có số từ cao hơn nhiều.

#### 4.3.3 Nghiên cứu Trường hợp

Trong Bảng 5, chúng tôi trình bày một nghiên cứu trường hợp so sánh LLM-URL với hai phương pháp truy xuất baseline, BM25 và Contriever, trên câu hỏi "A 'smack' is a collective noun for a group of which sea creatures?" có trong tập test TriviaQA. Câu trả lời được gán nhãn vàng cho câu hỏi này là "jellyfish".

Trong cài đặt closed-book, InstructGPT nhầm lẫn dự đoán "dolphins" làm câu trả lời. Khi sử dụng Contriever để truy xuất 10 đoạn văn từ Wikipedia cho truy vấn, không có đoạn văn nào chứa câu trả lời vàng. Ví dụ, Contriever truy xuất các đoạn văn về "smack", một loại tàu đánh cá, cùng với các đoạn văn khác về cá voi tinh trùng, phù du, và các chủ đề không liên quan khác. Kết quả tương tự được tìm thấy khi sử dụng BM25 làm bộ truy xuất.

Ngược lại, LLM-URL hoạt động tốt hơn nhiều trong tình huống này, truy xuất 7 tài liệu chứa câu trả lời. Tài liệu được truy xuất hàng đầu chính xác về câu trả lời vàng "jellyfish". Các tài liệu thứ tư đến thứ mười đều nói về các loại sứa khác nhau. Sau khi được chia nhỏ thành các đoạn văn rồi được sắp xếp bởi bộ xếp hạng, top 10 đoạn văn được nối lại. Trong số đó, nó chứa "A group of jellyfish is called a smack," chứa câu trả lời cho câu hỏi và đến trực tiếp từ tài liệu được truy xuất đầu tiên, có tiêu đề "jellyfish." Khi InstructGPT sau đó được nhắc nhở với 10 đoạn văn này cùng với câu hỏi, câu trả lời vàng "jellyfish" được tạo ra một cách chính xác.

Nghiên cứu trường hợp này làm nổi bật nhiều ưu điểm của LLM-URL. Thứ nhất, LLM-URL tìm thấy các tài liệu liên quan đến cả câu hỏi và câu trả lời. Nó trực tiếp định vị các tài liệu nói về "jellyfish" thay vì trong khi BM25 và Contriever định vị các tài liệu liên quan chỉ đến câu hỏi—không phải câu trả lời. Thứ hai, LLM-URL chính xác hơn BM25 hoặc Contriever. Trong trường hợp này, 7 trong số 10 URL được tạo ra từ LLM-URL trỏ đến một tài liệu Wikipedia chứa câu trả lời. Tuy nhiên, cả BM25 và Contriever đều thất bại trong việc truy xuất bất kỳ tài liệu nào chứa câu trả lời. Thứ ba, tập hợp các tài liệu được truy xuất bởi LLM-URL bổ sung cho nhau, trong khi ở BM25 hoặc contriever, mỗi tài liệu trong top-10 được chọn độc lập. Điều này là do LLM có thể tham chiếu đến các URL được tạo ra trước đó trước khi nó tạo ra URL tiếp theo, cho phép mỗi URL mới được tạo ra được điều kiện hóa trên tất cả các URL trước đó. Điều này dẫn đến một ngữ cảnh bằng chứng thông tin hơn trong trả lời câu hỏi miền mở.

## 5 Kết luận và Nghiên cứu Tương lai

Trong bài báo này, chúng tôi đã khám phá liệu các mô hình ngôn ngữ lớn có thể tạo ra URL được nhắc nhở bởi hướng dẫn của con người để truy xuất tài liệu hay không. Đáng ngạc nhiên, chúng tôi thấy rằng bằng cách cung cấp một vài cặp (truy vấn, URL) làm minh họa trong ngữ cảnh, các mô hình ngôn ngữ lớn (ví dụ GPT-3) tạo ra các URL Web mà gần 90% các tài liệu tương ứng chứa câu trả lời đúng cho các câu hỏi miền mở trong WebQ. Hơn nữa, bằng cách chia các tài liệu được truy xuất thành các đoạn văn rồi xếp hạng chúng với BM25, chúng tôi đã chỉ ra rằng một số lượng đáng kể các đoạn văn không cần thiết có thể được lọc ra trong khi vẫn giữ được recall cao, vượt trội hơn các phương pháp baseline với biên độ đáng kể.

Có nhiều hướng nghiên cứu thú vị cho công việc tương lai. Mặc dù một số benchmark truy xuất phổ rộng như BIER (Thakur et al., 2021) tồn tại, vẫn còn phải xem liệu các minh họa few-shot được thể hiện trong nghiên cứu này có thể được điều chỉnh thêm cho các tác vụ truy xuất cụ thể hay không. Promptagator (Dai et al., 2022) cho thấy các cải thiện hiệu suất đáng kể có thể đạt được bằng cách điều chỉnh prompt theo cách tương tự.

Hơn nữa, vẫn còn phải xem liệu việc tinh chỉnh prompt cho từng câu hỏi riêng lẻ có thể cải thiện thêm hiệu suất truy xuất hay không. Như với Promptagator, nghiên cứu trước đây đã chỉ ra việc sử dụng clustering để chọn các minh họa đa dạng cho bất kỳ câu hỏi nào cho trước cũng cải thiện thêm hiệu suất truy xuất cũng như hiệu suất QA downstream.

## Hạn chế

Mặc dù có hiệu suất mạnh trên các bộ dữ liệu được trình bày, phương pháp của chúng tôi bị hạn chế trong khả năng cập nhật trạng thái kiến thức và thích ứng với các domain mới. Một tính năng chính của retrieve-then-read là khả năng thay thế các tài liệu mới khi thông tin mới được học, chẳng hạn như các tài liệu gần đây hơn về mặt thời gian, hoặc thêm vào các tài liệu từ một domain mới để nhanh chóng thích ứng với một tác vụ downstream mới. Phương pháp của chúng tôi dựa vào một mô hình ngôn ngữ lớn để chứa tất cả kiến thức này và việc thêm kiến thức mới có thể sẽ yêu cầu một số huấn luyện lại. Ngoài ra, các mô hình tạo sinh lớn vẫn gặp phải lỗi ảo giác, dẫn đến các dự đoán không chính xác. Khi được giao nhiệm vụ tạo ra 10 URL, LLM-URL có thể chỉ tạo ra 6 hoặc 7 URL liên kết đến các tài liệu hợp lệ. Cuối cùng, phương pháp của chúng tôi liên quan đến các mô hình ngôn ngữ rất lớn, các yêu cầu web chậm, và xử lý tài liệu có thể làm cho nó trở nên cồng kềnh để sử dụng trong thực tế.

## Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi NSF IIS-2119531, IIS-2137396, IIS-2142827, CCF-1901059, và ONR N00014-22-1-2507. Wenhao Yu được hỗ trợ một phần bởi Bloomberg Data Science Fellowship.
