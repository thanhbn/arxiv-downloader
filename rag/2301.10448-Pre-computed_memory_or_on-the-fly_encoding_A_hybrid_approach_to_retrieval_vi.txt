# 2301.10448.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2301.10448.pdf
# Kích thước tệp: 750042 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn
Michiel de Jong* 1Yury Zemlyanskiy* 2Nicholas FitzGerald2Joshua Ainslie2
Sumit Sanghai2Fei Sha2William W. Cohen2
Tóm tắt
Các mô hình ngôn ngữ được tăng cường truy xuất như
Fusion-in-Decoder rất mạnh mẽ, thiết lập đỉnh cao nghệ thuật
trong nhiều tác vụ thâm dụng kiến thức. Tuy nhiên, chúng
cũng rất tốn kém, do cần phải mã hóa một số lượng lớn
các đoạn văn được truy xuất. Một số nghiên cứu tránh chi phí
này bằng cách mã hóa trước một kho văn bản thành bộ nhớ
và truy xuất trực tiếp các biểu diễn dày đặc. Tuy nhiên, việc
mã hóa trước bộ nhớ gây ra hình phạt chất lượng nghiêm trọng
vì các biểu diễn bộ nhớ không được điều kiện hóa
trên đầu vào hiện tại. Chúng tôi đề xuất LUMEN, một phương pháp
lai giữa hai cực trị này, tính toán trước
phần lớn biểu diễn truy xuất và hoàn thành việc mã hóa
theo thời gian thực sử dụng một bộ mã hóa trực tiếp được điều kiện hóa
trên câu hỏi và được tinh chỉnh cho tác vụ. Chúng tôi cho thấy LUMEN
vượt trội đáng kể so với bộ nhớ thuần túy trên nhiều
tác vụ trả lời câu hỏi trong khi rẻ hơn nhiều so với FiD,
và vượt trội cả hai với bất kỳ ngân sách tính toán nào. Hơn nữa,
lợi thế của LUMEN so với FiD tăng theo kích thước mô hình.

1. Giới thiệu
Các mô hình ngôn ngữ được tăng cường truy xuất như Fusion-in-
Decoder (Izacard & Grave, 2021) đạt được hiệu suất mạnh
trên các tác vụ thâm dụng kiến thức, thường vượt trội hơn
các mô hình lớn hơn nhiều (Izacard et al., 2022). Chúng truy xuất
các đoạn văn bản liên quan và xử lý các đoạn văn cùng với
đầu vào để trích xuất thông tin ngữ cảnh có liên quan. Tuy nhiên,
việc mã hóa các đoạn văn được truy xuất có thể tốn kém về mặt tính toán. Nghiên cứu gần đây đã phát hiện rằng với một bộ giải mã
tối ưu hóa (Shazeer, 2019; de Jong et al., 2022a; Pope et al., 2022)
việc mã hóa các đoạn văn được truy xuất chiếm phần lớn tổng chi phí

*Đóng góp ngang nhau1Đại học Nam California2Google
Research. Liên hệ: Michiel de Jong <msde-
jong@usc.edu >, Yury Zemlyanskiy <urikz@google.com >.

Kỷ yếu Hội nghị Quốc tế lần thứ 40 về Học máy
, Honolulu, Hawaii, USA. PMLR 202, 2023. Bản quyền
2023 thuộc về (các) tác giả.0 0.2 0.4 0.6 0.8 1455055
Tỷ lệ các lớp bộ mã hóa trực tiếpĐúng khớpBộ nhớ
LUMEN
FiD
Hình 1. Độ khớp chính xác trên tập dev Natural Questions cho LUMEN-
XXL như một hàm của tỷ lệ các lớp bộ mã hóa trực tiếp (được tinh chỉnh và điều kiện hóa
trên câu hỏi) so với bộ nhớ (được tính toán trước). LUMEN
thu hẹp khoảng cách giữa các phương pháp bộ nhớ thuần túy và FiD với một
phần nhỏ các lớp trực tiếp và do đó là tính toán.

cho việc tinh chỉnh và suy luận.

Một phương pháp ngày càng phổ biến để giảm chi phí mã hóa
này là truy xuất và trích xuất thông tin từ một bộ nhớ
các biểu diễn được tính toán trước thay vì văn bản thô, phân bổ
việc mã hóa một đoạn văn cho mọi mẫu mà
truy xuất mục từ bộ nhớ (Li et al., 2022;
de Jong et al., 2022b; Wu et al., 2022a; Zhong et al., 2022;
Chen et al., 2022; Wu et al., 2022b).1

Tuy nhiên, các phương pháp bộ nhớ gây ra hình phạt chất lượng lớn
so với các mô hình được tăng cường truy xuất (Li et al., 2022), bởi
vì bộ nhớ được mã hóa trước không được điều kiện hóa trên
tác vụ hoặc trên đầu vào hoặc câu hỏi cụ thể. Do đó, biểu diễn
được mã hóa trước phải đủ toàn diện
để trả lời bất kỳ câu hỏi nào, một nhiệm vụ đầy thách thức. Sự tương tự
với con người là sự khác biệt giữa việc ghi nhớ toàn bộ
cuốn sách và được hỏi sau đó so với việc tra cứu
câu trả lời cho một câu hỏi theo thời gian thực.

Do đó, các phương pháp dựa trên bộ nhớ cần phải mở rộng quy mô lớn

1Ở đây chúng tôi không đề cập đến việc tính toán trước các biểu diễn được sử dụng
để chọn các đoạn văn cho việc truy xuất (như là thông lệ phổ biến cho các
phương pháp truy xuất dày đặc), mà là việc tính toán trước các biểu diễn
thực tế được truy xuất và tích hợp vào mô hình ngôn ngữ.

1arXiv:2301.10448v2  [cs.CL]  2 Jun 2023

--- TRANG 2 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

Hình 2. Tổng quan về kiến trúc LUMEN. Trước khi tinh chỉnh, mỗi đoạn văn trong kho được mã hóa bởi một bộ mã hóa bộ nhớ. Trong khi
xử lý một mẫu, trước tiên bộ mã hóa câu hỏi tạo ra một biểu diễn của câu hỏi, sau đó được nối riêng biệt với mỗi
biểu diễn đoạn văn được tính toán trước. Một bộ mã hóa trực tiếp được tinh chỉnh sau đó cập nhật các biểu diễn đoạn văn điều kiện hóa trên câu hỏi,
cuối cùng được đưa vào bộ giải mã như trong FiD tiêu chuẩn. Các thành phần đông lạnh có màu cam, các thành phần được tinh chỉnh có màu xanh.

kích thước mô hình để đạt được hiệu suất tương đương. Như
chúng tôi sẽ chỉ ra, điều này dẫn đến tổng FLOPs ròng cao hơn do
chú ý chéo và giải mã, cũng như sự gia tăng không thực tế trong
chi phí tiền huấn luyện, tính toán trước và lưu trữ.

Chúng tôi đề xuất LUMEN (Live Update Memory Network), một
điểm giữa giữa truy xuất và bộ nhớ. LUMEN chia
nhiệm vụ mã hóa các đoạn văn giữa một bộ mã hóa bộ nhớ đông lạnh
tính toán trước các biểu diễn bộ nhớ đoạn văn,
và một bộ mã hóa trực tiếp được tinh chỉnh cập nhật các biểu diễn bộ nhớ
được điều kiện hóa trên câu hỏi. Hình 2 cung cấp một
tổng quan chi tiết về kiến trúc. Như có thể thấy trong Hình
1, một tỷ lệ nhỏ các lớp trực tiếp đã có thể đạt được
hiệu suất gần với Fusion-in-Decoder tiêu chuẩn.

Chúng tôi bắt đầu với một tập hợp các thí nghiệm khởi tạo LUMEN từ
T5, phân chia bộ mã hóa T5 tiêu chuẩn thành một bộ mã hóa bộ nhớ và
bộ mã hóa trực tiếp. Chúng tôi đánh giá trên các bộ dữ liệu trả lời câu hỏi
Natural Questions (Kwiatkowski et al., 2019) và Trivi-
aQA (Joshi et al., 2017). LUMEN đạt được hiệu suất
mạnh hơn đáng kể so với FiD và FiD với bộ nhớ được cung cấp
cùng ngân sách tính toán, và lợi thế tăng
với kích thước mô hình. Ở kích thước T5-XXL, LUMEN thực hiện tương
đương với FiD chỉ với một phần ba tỷ lệ các lớp trực tiếp
và FLOPs.

Tiếp theo, chúng tôi thí nghiệm với các cải tiến cho thiết lập
LUMEN tiêu chuẩn, cho thấy rằng sự đánh đổi hiệu suất-tính toán
có thể được cải thiện thêm so với FiD bằng cách chuyển giao
một bộ nhớ được huấn luyện và bộ mã hóa trực tiếp từ một tác vụ liên quan.
Đặc biệt, chúng tôi phát hiện rằng việc chuyển giao từ Natural Questions
có thể thu hẹp phần lớn khoảng cách về hiệu suất giữa LUMEN
và FiD với các bộ mã hóa trực tiếp nhỏ hơn hoặc trên các bộ dữ liệu nhỏ.

Cuối cùng, LUMEN đại diện cho một sự đánh đổi mong muốn giữa
các phương pháp dựa trên truy xuất và bộ nhớ, đạt được hiệu suất tốt hơn
cho bất kỳ ngân sách tính toán nào.

2. Kiến thức nền tảng

Chúng tôi quan tâm đến việc đạt được hiệu suất tốt nhất có thể
cho bất kỳ ngân sách tài nguyên nào. Tuy nhiên, có các loại
tài nguyên tính toán khác nhau, và các phương pháp thuật toán
khác nhau tạo ra các sự đánh đổi riêng biệt giữa những tài nguyên đó.
Trong phần này, chúng tôi cung cấp kiến thức nền tảng về các mô hình
được tăng cường truy xuất hiện có và mô tả chi phí của những mô hình đó
theo các chiều tính toán khác nhau.

2.1. Tài nguyên tính toán cho các mô hình được tăng cường truy xuất

Chu kỳ sống thông thường của các mô hình hiện tại bắt đầu với tiền
huấn luyện, tiếp theo là tinh chỉnh trên nhiều tác vụ. Cuối cùng,
mô hình được sử dụng cho suy luận, hoặc trực tuyến hoặc cho việc chưng cất
theo lô thành một mô hình nhỏ hơn. Mỗi giai đoạn này có
chi phí khác nhau cho mỗi mẫu. Gọi Npt, Nft và NI là
số lượng mẫu được xử lý cho tiền huấn luyện, tinh chỉnh
và suy luận, và Fpt, Fft và FI là chi phí tính toán cho mỗi
mẫu cho mỗi giai đoạn, được đo bằng FLOPs (phép toán
điểm nổi). Khi đó chi phí tính toán cho mô hình là
FLOPs tiền huấn luyện = NptFpt
FLOPs tinh chỉnh = NftFft·số lượng tác vụ
FLOPs suy luận = NIFI

Các phương pháp được đề xuất trong bài báo này không phụ thuộc vào
phương pháp được sử dụng để chọn các đoạn văn được truy xuất, vì vậy chúng tôi không
xem xét phương pháp truy xuất trong so sánh chi phí tính toán
của chúng tôi. Trong khi suy luận FiD có thể chậm hơn so với những gì
FLOPs sẽ chỉ ra do các ràng buộc băng thông bộ nhớ của bộ giải mã
(Hofstätter et al., 2022; de Jong et al., 2022a),

2

--- TRANG 3 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

0 0.2 0.4 0.6 0.8 14050
Tỷ lệ trực tiếp αĐúng khớp
NaturalQ
0 0.2 0.4 0.6 0.8 1506070
Tỷ lệ trực tiếp αTriviaQA
Base Large XL XXL
Hình 3. KẾT QUẢ CHÍNH: LUMEN đạt được hiệu suất gần với FiD với một phần nhỏ các lớp trực tiếp. Tỷ lệ cần thiết giảm
với quy mô. Độ khớp chính xác trên các tập xác thực Natural Questions (NaturalQ) và TriviaQA như một hàm của tỷ lệ các lớp bộ mã hóa trực tiếp
cho các mô hình LUMEN Base, Large, XL, và XXL.

việc sửa đổi bộ giải mã giảm thiểu khoảng cách (de Jong et al.,
2022a). Do đó, chúng tôi sử dụng FLOPs làm thước đo chi phí tính toán
của chúng tôi phù hợp với các nghiên cứu liên quan (Yu et al., 2022a; Varshney
et al., 2022). Phụ lục B chứa bằng chứng bổ sung về
mối quan hệ giữa FLOPs và độ trễ.

Đối với các mô hình được tăng cường truy xuất, có những chi phí bổ sung.
Tập truy xuất phải được lưu trữ và các truy xuất được truyền
đến bộ tăng tốc. Cũng có thể có chi phí tiền xử lý
cho tập truy xuất, chẳng hạn như tính toán trước các biểu diễn bộ nhớ.
Gọi Nrs là kích thước của tập truy xuất và
Fpc là FLOPs liên quan đến việc tiền xử lý một
ứng viên truy xuất. Khi đó yêu cầu lưu trữ và chi phí tính toán trước
được cho bởi
Lưu trữ = Kích thước kho·Kích thước của một mẫu đơn
FLOPs tính toán trước = Kích thước kho·Ftính toán trước

Nếu các biểu diễn truy xuất được tinh chỉnh, thì một phiên bản khác
của tập truy xuất phải được tính toán trước và lưu trữ
cho mỗi tác vụ. Băng thông cần thiết cho truyền được xác định
bởi tích của số lượng và kích thước của các biểu diễn được truy xuất.

2.2. Fusion-in-Decoder

Fusion-in-Decoder (Izacard & Grave, 2021) bao gồm một
mô hình mã hóa-giải mã T5. Đối với mỗi đầu vào, một số đoạn văn
bản liên quan được truy xuất, và đầu vào được đặt trước
mỗi đoạn văn. Các đoạn văn kết quả được mã hóa riêng biệt
bởi bộ mã hóa, và các biểu diễn được mã hóa sau đó
được nối và được chú ý bởi bộ giải mã để tạo ra
một đầu ra mục tiêu. Đối với mỗi mô hình, các thành phần được tinh chỉnh
có màu xanh và các thành phần đông lạnh có màu cam.

G=Dech
Enc(Q;Đoạnvăn1);. . .Enc(Q;Đoạnvănk)i

Gọi ns là số lượng token nguồn, nt là số lượng token
mục tiêu, L là số lượng lớp, và d là chiều
của mô hình. Theo phân tích từ de Jong et al.
(2022a), FLOPs cho một mẫu suy luận đơn của FiD
(bỏ qua việc tính toán điểm chú ý) được cho bởi2

FI= ns·L·14d2
|{z}
Bộ mã hóa và chú ý chéo+nt·L·14d2
|{z}
Bộ giải mã

Phụ lục A thảo luận về việc dẫn xuất độ phức tạp này
chi tiết hơn. Fpt và Fft bằng 3FI do bước ngược.
Đối với tinh chỉnh và suy luận ns≫nt vì
số lượng lớn token từ các đoạn văn được truy xuất. Do đó,
FLOPs tinh chỉnh và suy luận của FiD cho mỗi mẫu
cao hơn đáng kể so với tiền huấn luyện. Ngược lại,
yêu cầu lưu trữ và băng thông thấp vì tập truy xuất
bao gồm các đoạn văn của token thô. FiD không có
chi phí tính toán trước.

2.3. Bộ nhớ

Ngày càng nhiều nghiên cứu giảm chi phí của các mô hình
được tăng cường truy xuất bằng cách tính toán trước các biểu diễn dày đặc
của các ứng viên truy xuất và lưu trữ chúng trong một bộ nhớ. Một
nghiên cứu như vậy sửa đổi FiD bằng cách tính toán trước các biểu diễn
bộ mã hóa đoạn văn và cung cấp đầu vào như một tiền tố cho
bộ giải mã (Li et al., 2022). Chúng tôi gọi nó là MemoryFiD:

G=Dech
Q;MemEnc (Đoạnvăn1);..MemEnc (Đoạnvănk)i

MemoryFiD tiết kiệm tính toán tinh chỉnh và suy luận với
chi phí tăng tính toán trước, lưu trữ và băng thông

2Chúng tôi ước tính FLOPs của khối MLP là 8d2, FLOPs
từ MLP Transformer gốc. MLP T5 có chiều
giữa 2.5d và 3d và ba phép nhân ma trận
bao gồm GEGLU, tạo ra tổng FLOPs gần 8d.

3

--- TRANG 4 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

52 53 54 55 56 570102030
LXLXXL
L
XLXXL
Đúng khớpTFLOPs cho mỗi mẫuNaturalQ
67 68 69 70 71 72 730102030
LXLXXL
L
XLXXL
Đúng khớpTriviaQA
LUMEN FiD
Hình 4. KẾT QUẢ CHÍNH: LUMEN sử dụng tính toán ít hơn đáng kể so với FiD cho cùng hiệu suất, và lợi thế này tăng
theo quy mô. TFLOPs như một hàm của độ khớp chính xác trên các tập kiểm tra Natural Questions (NaturalQ) và TriviaQA. FLOPs dành cho bước tiến đơn
và loại trừ tính toán trước. So sánh FiD và LUMEN với tỷ lệ trực tiếp 0.33 các mô hình Large, XL và XXL. Thấp hơn là tốt hơn.

yêu cầu. Vì MemoryFiD không mã hóa các đoạn văn được truy xuất
theo thời gian thực, chi phí bộ mã hóa được loại bỏ và
chỉ còn lại chú ý chéo và tính toán bộ giải mã khác:

FI=ns·L·2d2
|{z}
Chú ý chéo+nt·L·14d2
|{z}
Bộ giải mã

Thay vào đó, nó tính toán trước các biểu diễn đoạn văn, sử dụng
FLOPs tính toán trước = Kích thước kho·npL·12d2
trong đó np là số lượng token trong một đoạn văn đơn. Memo-
ryFiD lưu trữ các biểu diễn lớp cuối cho mỗi token đoạn văn, chiếm
Lưu trữ = Kích thước kho·2npd

Để cho thấy chi phí lưu trữ, việc áp dụng
MemoryFiD-XXL cho Wikipedia yêu cầu khoảng 16
terabyte. Giữ kích thước mô hình cố định, MemoryFiD tiết kiệm
tính toán miễn là kho truy xuất không quá lớn so
với số lượng mẫu được xử lý cho tinh chỉnh
và suy luận. Tuy nhiên, vì các biểu diễn đoạn văn không
được điều kiện hóa trên câu hỏi, MemoryFiD gây ra hình phạt
hiệu suất đáng kể so với FiD tiêu chuẩn. Do
đó, để đạt được hiệu suất tương đương với FiD tiêu chuẩn,
MemoryFiD phải sử dụng một mô hình lớn hơn nhiều, gây ra
chi phí chú ý chéo, bộ giải mã, tiền huấn luyện,
tính toán trước, lưu trữ và băng thông lớn hơn nhiều. Li et al.
(2022) cũng tinh chỉnh bộ mã hóa bộ nhớ, điều này yêu cầu
tính toán trước và lưu trữ một bộ nhớ riêng cho mỗi tác vụ.
Điều này không khả thi đối với các ứng dụng thực tế liên quan đến
kho có kích thước internet, vì vậy đối với các kết quả chính của chúng tôi, chúng tôi giả định bộ nhớ
được tính toán trước từ một mô hình đơn mà không tinh chỉnh
trên các tác vụ riêng lẻ. Không có tinh chỉnh, hình phạt hiệu suất
thậm chí còn cao hơn. Hình 8 cho thấy ảnh hưởng của việc tinh
chỉnh bộ nhớ; kết quả LUMEN tương tự về chất
trong thiết lập đó.

Bảng 1. FLOPs của mô hình cho mỗi lớp và byte lưu trữ cho mỗi token cho
FiD, MemoryFiD và LUMEN.
Mô hình FLOPs Lưu trữ
FID 14nsd2+ 14ntd210
MEMFID 2nsd2+ 14ntd22d
LUMEN (2 + 12 α)nsd2+ 14ntd22d

3.LUMEN

Một cách trực quan khi đọc một đoạn văn, việc biết thông tin nào
cần thiết và cho mục đích gì là hữu ích. Đối với
Fusion-in-Decoder, điều này được thực hiện bằng cách đặt trước đầu vào
vào các đoạn văn được truy xuất và tinh chỉnh bộ mã hóa đoạn văn,
trong khi MemoryFiD không có lợi thế như vậy.
Với LUMEN, chúng tôi khám phá khả năng rằng một hiệu ứng tương tự
có thể được đạt được bằng một quá trình hai bước, trong đó một
mô hình lớn tạo ra một biểu diễn chung cho mỗi đoạn văn
có thể được đặt trong bộ nhớ, và một mô hình nhỏ hơn
biến đổi biểu diễn chung này thành một biểu diễn cụ thể theo đầu vào
bằng cách điều kiện hóa trên đầu vào và tác vụ. Hình
2 cung cấp tổng quan về kiến trúc LUMEN.

3.1. Kiến trúc

LUMEN được khởi tạo từ một mô hình mã hóa-giải mã
T5 được tiền huấn luyện. Bộ giải mã hoạt động giống như bộ giải mã
FiD tiêu chuẩn, nhưng LUMEN có ba bộ mã hóa. Bộ mã hóa T5
được chia thành một bộ mã hóa bộ nhớ lớn chứa
tỷ lệ 1−α đầu tiên của các lớp, và một bộ mã hóa trực tiếp nhỏ hơn
với tỷ lệ α còn lại của các lớp.
Bộ mã hóa bộ nhớ được áp dụng ngoại tuyến cho các đoạn văn trong
kho để tính toán trước các biểu diễn bộ nhớ, sau đó

4

--- TRANG 5 ---
Bộ nhờ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

0 1 2 3 4455055
LXLXXL
BLXL
TFLOPs cho mỗi mẫuĐúng khớpNaturalQ
0 1 2 3 4606570
LXLXXL
BLXL
TFLOPs cho mỗi mẫuTriviaQA
LUMEN MemoryFiD
Hình 5. LUMEN đạt được hiệu suất tốt hơn nhiều so với MemoryFiD ở bất kỳ ngân sách tính toán nào. Hiệu suất độ khớp chính xác trên tập
kiểm tra Natural Questions như một hàm của TFLOPs cho mỗi mẫu so sánh LUMEN1/3Base, Large và XL với MemoryFiD Large,
XL, và XXL. FLOPs dành cho bước tiến đơn và loại trừ tính toán trước. Lưu ý rằng các trục được hoán đổi so với Hình 4
vì MemoryFiD yêu cầu quá nhiều tính toán để đạt hiệu suất LUMEN.

được cập nhật điều kiện hóa trên đầu vào và tác vụ theo thời gian thực bởi
bộ mã hóa trực tiếp được tinh chỉnh. Để đảm bảo rằng các biểu diễn bộ nhớ
và đầu vào tương thích, LUMEN áp dụng một
bộ mã hóa câu hỏi cho đầu vào trước khi đặt trước biểu diễn câu
hỏi vào biểu diễn bộ nhớ. Bộ mã hóa câu hỏi
chia sẻ cấu trúc và trọng số ban đầu với
bộ mã hóa bộ nhớ, nhưng được tinh chỉnh.

G=Dech
Q;LiveEnc (H1);. . .LiveEnc (Hk)i
Hi=h
QEnc (Q);MemEnc (Đoạnvăni)i

Chọn α= 0 phục hồi MemoryFiD, trong khi α= 1 tạo ra
một mô hình rất gần với FiD.

3.2. Phân tích tính toán

Trong quá trình tinh chỉnh và suy luận, LUMEN chỉ áp dụng một
tỷ lệ α của các lớp, dẫn đến một phần α của FLOPs
đọc FiD cho bất kỳ kích thước mô hình nào.

FI=ns·αL·12d2
|{z }
Bộ mã hóa+ns·L·2d2
|{z}
Chú ý chéo+nt·L·14d2
|{z}
Bộ giải mã

Chi phí tính toán trước ở cùng kích thước mô hình là một hệ số
1−α của chi phí tính toán trước MemoryFiD (không tinh
chỉnh bộ mã hóa bộ nhớ). Chi phí lưu trữ và băng thông
giống như MemoryFiD (ở cùng kích thước mô hình và
không tinh chỉnh bộ mã hóa bộ nhớ). Bảng 1 cho thấy
so sánh FLOPs và yêu cầu lưu trữ của FiD,
MemoryFiD và LUMEN. Như chúng tôi sẽ chỉ ra, LUMEN có thể
đạt hiệu suất FiD chỉ với sự gia tăng khiêm tốn về kích thước,
dẫn đến giảm lớn chi phí tính toán mà không có
sự gia tăng tương ứng trong yêu cầu tiền huấn luyện, tính toán trước,
và lưu trữ gây ra với MemoryFiD.

4. Thí nghiệm

4.1. Thiết lập thí nghiệm

Quy trình huấn luyện Tất cả các thí nghiệm sử dụng các mô hình dựa
trên kiến trúc T5.1.1 (Raffel et al., 2020). Các thí nghiệm chính
sử dụng các mô hình được khởi tạo từ các checkpoint T5 công khai
(Google, 2022). FiD được huấn luyện theo công thức
tiêu chuẩn (Izacard & Grave, 2021). Đối với LUMEN, với tỷ lệ
các lớp trực tiếp α, bộ mã hóa bộ nhớ và bộ mã hóa câu hỏi
mỗi cái được khởi tạo với tỷ lệ 1 - α đầu tiên
của các lớp của bộ mã hóa T5, và bộ mã hóa trực tiếp được khởi tạo
với tỷ lệ α cuối cùng của các lớp của bộ mã hóa T5.

Các mô hình được tinh chỉnh với framework T5X (Roberts
et al., 2022) dựa trên JAX (Bradbury et al., 2018) và FLAX
(Heek et al., 2020) sử dụng bộ tối ưu hóa Adafactor (Shazeer & Stern,
2018) với kích thước lô 64 và tốc độ học 0.0001.
Kết quả kiểm tra được tạo ra từ các checkpoint với kết quả
dev tốt nhất. Các thí nghiệm trong Phần 4.4 tiền huấn luyện các mô hình
từ đầu. Tiền huấn luyện tuân theo công thức huấn luyện T5 tiêu chuẩn
ngoại trừ việc chúng tôi huấn luyện trong 500k bước, và vô hiệu hóa
lịch cập nhật moment thứ hai Adafactor và factoring.

Dữ liệu Chúng tôi đánh giá LUMEN trên các bộ dữ liệu trả lời câu hỏi
mở Natural Questions (Kwiatkowski et al.,
2019), TriviaQA (Joshi et al., 2017), và WebQuestions (Be-
rant et al., 2013) (trong Phần 4.3). Đối với tất cả các bộ dữ liệu, mỗi mẫu
được ghép với 20 đoạn văn Wikipedia 100 từ liên quan nhất
được xếp hạng theo điểm DPR (Karpukhin et al., 2020). Đối với
FiD, các cặp câu hỏi và đoạn văn được nối được cắt
thành 256 token. Đối với LUMEN, câu hỏi và đoạn văn
được cắt riêng biệt thành 48 và 208 token để cung cấp
so sánh công bằng, vì chúng được xử lý riêng biệt.

5

--- TRANG 6 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

0.85 0.9 0.95Base
Large
XL
XXLLUMEN 1/3
0.3 0.4 0.5 0.6 0.7 0.8Large
XL
XXL
Tỷ lệ khoảng cách Memory-FiD được thu hẹpLUMEN 1/8
Hình 6. LUMEN thu hẹp khoảng cách với FiD khi quy mô tăng.
Tỷ lệ chênh lệch độ khớp chính xác trên Natural Questions giữa
MemoryFiD và FiD được thu hẹp bởi LUMEN như một hàm của quy mô mô hình.

4.2. Kết quả chính

Hình 3 cho thấy hiệu suất LUMEN như một hàm của tỷ lệ trực tiếp
cho các kích thước mô hình khác nhau. Quan sát chính đầu tiên
là một tỷ lệ tương đối nhỏ các lớp trực tiếp là
đủ để đạt được chất lượng gần với FiD. Quan sát chính thứ hai
là khi kích thước mô hình tăng, tỷ lệ trực tiếp cần thiết
để phục hồi hiệu suất FiD giảm. Mô hình này
được hỗ trợ thêm bởi kết quả từ Hình 6, đo lường một cách rõ ràng
bao nhiều khoảng cách giữa MemoryFiD và FiD được thu hẹp bởi LUMEN và cho thấy khoảng cách này
tăng theo quy mô.

Hình 4 so sánh FLOPs như một hàm của hiệu suất
cho LUMEN và FiD, chứng minh rằng LUMEN đạt được
hiệu suất tương tự ở FLOPs thấp hơn cho tinh chỉnh và suy
luận (giả định tính toán trước được phân bổ đủ
để có hiệu quả miễn phí). Hơn nữa, lợi thế trở nên
rõ rệt hơn với kích thước mô hình lớn hơn, phù hợp với
phát hiện từ Hình 3 và 6. Hình 5 cho thấy LUMEN
cũng có hiệu suất mạnh hơn nhiều so với MemoryFiD cho
bất kỳ giá trị FLOP nào. Cuối cùng, Bảng 3 so sánh LUMEN với
kết quả đã xuất bản trong tài liệu.

4.3. Chuyển giao

Vì bộ mã hóa bộ nhớ không được tinh chỉnh trên mỗi
tác vụ riêng lẻ, bộ mã hóa trực tiếp phải thích ứng các biểu diễn bộ nhớ
với tác vụ ngoài việc điều kiện hóa trên đầu vào.
Đặc biệt đối với các bộ mã hóa trực tiếp nhỏ hơn, điều này có thể khó
học trong khi tinh chỉnh trên một tác vụ đơn. Ở đây chúng tôi đánh giá
liệu LUMEN có thể hưởng lợi từ việc chuyển giao từ các
tác vụ thâm dụng kiến thức khác.

Đặc biệt, chúng tôi xem xét hai thiết lập chuyển giao. Trong thiết lập
Trực tiếp, chúng tôi chuyển giao Bộ mã hóa Trực tiếp bằng cách huấn luyện trên Natu-
ral Questions với bộ nhớ đông lạnh trước khi chuyển giao sang
tác vụ mục tiêu. Trong thiết lập Bộ nhớ, mô hình được huấn luyện trên
Natural Questions với bộ nhớ được tinh chỉnh trước khi chuyển giao
cả Bộ mã hóa Trực tiếp và Bộ nhớ sang tác vụ mục tiêu.
Thiết lập Bộ nhớ tuân theo trực giác rằng, mặc dù không
khả thi khi sử dụng một bộ nhớ khác cho mỗi tác vụ, có thể
thực hiện tinh chỉnh đa tác vụ trước khi
mã hóa bộ nhớ.

Hình 7 cho thấy kết quả chuyển giao từ Natural Ques-
tions sang TriviaQA và WebQuestions. Chúng tôi lưu ý một số
mô hình thú vị. Thứ nhất, lợi ích từ chuyển giao cao hơn cho
tỷ lệ trực tiếp nhỏ hơn, với lợi ích tối thiểu cho FiD và
lợi ích lớn cho LUMEN 1/8. Thứ hai, việc chuyển giao bộ nhớ
chỉ hữu ích cho tỷ lệ trực tiếp nhỏ, nơi Bộ mã hóa Trực tiếp
không chứa đủ lớp để thích ứng đầy đủ bộ nhớ
với tác vụ. Thứ ba, lợi ích từ chuyển giao
cao hơn đáng kể cho WebQuestions, một tác vụ với lượng dữ liệu
rất nhỏ.

4.4. Hình dạng bộ nhớ

Trong các thí nghiệm chính của chúng tôi, chúng tôi khởi tạo LUMEN từ các
checkpoint T5 công khai để tránh tiền huấn luyện tốn kém và phân chia
bộ mã hóa thành bộ mã hóa bộ nhớ và bộ mã hóa trực tiếp. Liệu chúng ta
có thể đạt được sự đánh đổi tốt hơn bằng cách tiền huấn luyện một mô hình với
cấu hình tùy chỉnh? Việc cố định đầu ra của bộ mã hóa trực tiếp
có chiều mô hình thấp cho phép chúng ta mở rộng bộ mã hóa bộ nhớ
mà không sử dụng thêm FLOPs, vì FLOPs chú ý chéo
không bị ảnh hưởng bởi kích thước của bộ mã hóa bộ nhớ.

Bảng 2 cho thấy hiệu ứng của việc thêm một bộ mã hóa bộ nhớ gồm
24 lớp Base bổ sung vào một cấu hình T5-Base hiện có,
tạo ra hiệu suất tăng mà không tăng
tính toán. Đưa đến cực trị, những kết quả này gợi ý rằng
việc kết hợp một mô hình ngôn ngữ lớn với một bộ mã hóa trực tiếp
có kích thước vừa phải có thể mang lại kết quả mạnh với chi phí khiêm tốn.

Bảng 2. Việc thêm bộ nhớ vào FiD dẫn đến lợi ích hiệu suất đáng kể
mà không có FLOPs tinh chỉnh hoặc suy luận bổ sung.
Hiệu suất độ khớp chính xác trên Natural Questions và
TriviaQA cho FiD-Base và LUMEN1/3 với bộ giải mã Base và bộ mã hóa trực tiếp,
và bộ mã hóa bộ nhớ với 24 lớp Base.

Mô hình NQ TQA
FiD Base 47.3 64.4
LUMEN Base 24-12 48.9 65.4

4.5. Ablations

Hai khác biệt chính giữa FiD, LUMEN, và Memo-
ryFiD là mức độ mà các đoạn văn được truy xuất được điều kiện
hóa trên đầu vào và mức độ mà các bộ mã hóa đoạn văn
được tinh chỉnh trên các tác vụ cụ thể. Ablation đầu tiên của chúng tôi điều tra
cách sự khác biệt hiệu suất giữa LUMEN và

6

--- TRANG 7 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

67 68 69 70 71FiD
L1/3
L1/8 +1.0 +1.0+1.0TriviaQA
40 42 44 46 48 50FiD
L1/3
L1/8 +1.4 +5.00.8 +3.6WebQuestions
Baseline Trực tiếp Bộ nhớ
Hình 7. Việc chuyển giao bộ nhớ và đặc biệt là bộ mã hóa trực tiếp từ một bộ dữ liệu liên quan có thể thu hẹp một phần khoảng cách với FiD, với
lợi ích tăng cho tỷ lệ trực tiếp thấp hơn và bộ dữ liệu cuối cùng nhỏ hơn. Độ khớp chính xác trên các tập dev TriviaQA và WebQuestions với và
không chuyển giao từ Natural Questions cho FiD và các mô hình LUMEN XL với tỷ lệ trực tiếp 1/3 và 1/8. Trực tiếp giữ bộ mã hóa bộ nhớ
đông lạnh trong quá trình huấn luyện trên Natural Questions trong khi Bộ nhớ cũng huấn luyện bộ nhớ trên Natural Questions (vẫn đông lạnh sau chuyển giao). Lợi ích
từ chuyển giao rõ rệt hơn nhiều cho tỷ lệ trực tiếp nhỏ hơn và trên WebQuestions, bộ dữ liệu nhỏ hơn.

0 0.1 0.2 0.3 0.4 0.5404550
Tỷ lệ trực tiếp αĐúng khớp
NaturalQ
0 0.1 0.2 0.3 0.4 0.5556065
Tỷ lệ trực tiếp αTriviaQA
LUMEN Tinh chỉnh bộ nhớ Điều kiện hóa bộ nhớ FiD
Hình 8. Không điều kiện hóa bộ nhớ trên đầu vào hay tinh chỉnh bộ nhớ đều đủ để phục hồi hiệu suất FiD. Cả hai
thành phần đều quan trọng, mặc dù điều kiện hóa dường như đóng góp nhiều hơn. Độ khớp chính xác trên các tập dev Natural Questions (NaturalQ) và
TriviaQA như một hàm của tỷ lệ các lớp bộ mã hóa trực tiếp cho LUMEN-Large và hai nới lỏng: một trong đó các lớp bộ nhớ
được tinh chỉnh, và một khác trong đó các lớp bộ nhớ được điều kiện hóa trên câu hỏi.

MemoryFiD một bên và FiD bên kia
xuất phát từ việc điều kiện hóa trên đầu vào và tinh chỉnh. Chúng tôi
xây dựng hai thiết lập ablation như các mô hình trung gian giữa LUMEN và FiD: tinh chỉnh bộ mã hóa bộ nhớ,
và điều kiện hóa bộ mã hóa bộ nhớ trên câu hỏi (nhưng
không tinh chỉnh nó). Hình 8 so sánh hiệu suất như một
hàm của tỷ lệ trực tiếp cho những thiết lập này. Không điều kiện hóa
bộ nhớ trên đầu vào hay tinh chỉnh bộ nhớ
đến gần việc phục hồi hiệu suất FiD bởi chính chúng:
cả hai đều cần thiết. Tuy nhiên, có vẻ như điều kiện hóa có thể
hữu ích hơn bởi chính nó so với tinh chỉnh bộ nhớ.

Bộ mã hóa trực tiếp LUMEN xử lý cùng nhau các biểu diễn
đoạn văn và đầu vào được nối. Do đó bộ giải mã
nhận được các đoạn văn được điều kiện hóa trên đầu vào cũng như
đầu vào trên đoạn văn. Để tách rời những hiệu ứng điều kiện hóa
này, chúng tôi thí nghiệm với các ablation không cho phép
token câu hỏi chú ý đến đoạn văn ("no q2mem") hoặc
token đoạn văn đến câu hỏi ("no mem2q"). Hình 9 trình bày
kết quả cho thấy việc điều kiện hóa đoạn văn trên đầu vào
là quan trọng, mặc dù câu hỏi được điều kiện hóa đoạn văn vẫn
hữu ích.

Cuối cùng, LUMEN cũng sử dụng một bộ mã hóa câu hỏi được tinh chỉnh để
tạo ra một biểu diễn câu hỏi được tối ưu hóa cho bộ mã hóa trực tiếp
để điều kiện hóa các bộ nhớ đoạn văn. Hình
10 so sánh hiệu suất giữa tinh chỉnh và đông lạnh
bộ mã hóa câu hỏi này, chứng minh tầm quan trọng của việc thích ứng
bộ mã hóa câu hỏi với tác vụ.

5. Nghiên cứu liên quan

Các mô hình được tăng cường truy xuất Có một lượng đáng kể
nghiên cứu về các mô hình ngôn ngữ được tăng cường truy xuất.
Một số phương pháp đáng chú ý bao gồm REALM (Guu et al.,
2020), RAG (Lewis et al., 2020), kNN-LM (Khandelwal
et al., 2020), RETRO (Borgeaud et al., 2022), và Fusion-in-

7

--- TRANG 8 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

0 0.2 0.4 0.6 0.8 1404550
Tỷ lệ trực tiếp αĐúng khớpLUMEN
no q2mem
no mem2q
Hình 9. Những lợi ích chính từ bộ mã hóa trực tiếp trong LUMEN
xuất phát từ việc cập nhật các biểu diễn bộ nhớ được điều kiện hóa trên
câu hỏi. Độ khớp chính xác trên tập dev Natural Question như một
hàm của tỷ lệ các lớp bộ mã hóa trực tiếp cho LUMEN-Large
và hai sửa đổi với chú ý tự giới hạn của bộ mã hóa. Trong
thiết lập 'no q2mem' các token câu hỏi không thể chú ý đến các token đoạn văn,
và ngược lại cho 'no mem2q'.

0 0.2 0.4 0.6 0.8 1404550
Tỷ lệ trực tiếp αĐúng khớpLUMEN
QEncoder đông lạnh
Hình 10. Việc tinh chỉnh bộ mã hóa câu hỏi cải thiện hiệu suất
đáng kể. Độ khớp chính xác trên tập dev Natural Question như một
hàm của tỷ lệ các lớp bộ mã hóa trực tiếp cho LUMEN-Large
và một sửa đổi mà bộ mã hóa câu hỏi bị đông lạnh (sao cho
bộ mã hóa bộ nhớ và bộ mã hóa câu hỏi được chia sẻ).

Decoder (FiD) (Izacard & Grave, 2021). FiD đặc biệt
đã chứng minh hiệu suất đỉnh cao nghệ thuật trên một loạt
các tác vụ (Izacard & Grave, 2021; Izacard et al., 2022; Yu
et al., 2022b). Nghiên cứu này tập trung vào việc cải thiện hiệu quả
của FiD thông qua một phương pháp bộ nhớ lai.

Các mô hình được tăng cường truy xuất hiệu quả Việc tăng cường truy xuất
có thể tốn kém cho huấn luyện và suy luận, và một
khối lượng lớn nghiên cứu điều tra các mô hình được tăng cường truy xuất
hiệu quả hơn. Chi phí tính toán của các mô hình được tăng cường
truy xuất có thể được phân chia thành chi phí từ
việc đọc các đoạn văn được truy xuất, giải mã, và chú ý tầm xa.
Nghiên cứu gần đây đã cho thấy FiD dành phần lớn thời gian suy luận
trong bộ giải mã (Hofstätter et al., 2022) do
các ràng buộc băng thông bộ nhớ trong chú ý chéo (de Jong
et al., 2022a). Tuy nhiên, với những sửa đổi thích hợp
(de Jong et al., 2022a) ràng buộc có thể được cải thiện

Bảng 3. So sánh LUMEN với kết quả đã xuất bản trên các tập kiểm tra Natural
Questions và TriviaQA. Chúng tôi tập trung vào việc so sánh với
FiD vì các nghiên cứu khác nâng cao hiệu suất với việc cải thiện truy xuất
(như ATLAS), điều này trực giao và bổ sung cho các
đóng góp của chúng tôi. LUMEN không phụ thuộc vào phương pháp truy xuất, và có thể được
sử dụng với truy xuất ATLAS.

Mô hình NQ TQA
REALM (Guu et al., 2020) 40.4 -
RAG (Lewis et al., 2020) 44.5 56.8
RETRO (Borgeaud et al., 2022) 45.5 -
T5-XXL (Roberts et al., 2020) 35.2 51.9
ATLAS (Izacard et al., 2022) 60.4 79.8
FiD-L (Izacard & Grave, 2021) 51.4 67.6
FiD-XXL (của chúng tôi) 57.3 73.0
LUMEN-XXL 57.1 73.1

được, sau đó phần lớn chi phí huấn luyện và suy luận
xuất phát từ việc đọc các đoạn văn được truy xuất.

Gánh nặng tính toán từ việc mã hóa các đoạn văn được truy xuất
có thể được giảm bằng cách xếp hạng lại và chỉ sử dụng những
truy xuất tốt nhất (Yu et al., 2022a; Wang et al., 2018; Mao et al.,
2021). Ngoài ra, các tài nguyên dành cho truy xuất có thể
được thích ứng với độ khó của đầu vào, truy xuất ít hơn
hoặc không có đoạn văn nếu mô hình tin tưởng rằng nó đã biết
câu trả lời (Kratzwald & Feuerriegel, 2018; Varshney et al.,
2022). Để mô hình hóa tương tác giữa
các đoạn văn được truy xuất khác nhau một cách hiệu quả, thông thường người ta sử dụng
chú ý tầm xa thưa thớt (Guo et al., 2022; Ainslie et al., 2020;
Zemlyanskiy et al., 2021). Cuối cùng, có một khối lượng lớn
nghiên cứu cố gắng cải thiện hiệu quả của các mô hình Transformer
nói chung. Các phương pháp tinh chỉnh hiệu quả (Zaken
et al., 2022; Hu et al., 2022) chỉ cập nhật một phần tham số
trong quá trình tinh chỉnh, nhưng không giống LUMEN, những phương pháp này
vẫn sử dụng toàn bộ mô hình cho suy luận. Các cải tiến hiệu quả chung khác
bao gồm song song hóa (Pope et al.,
2022), lượng tử hóa (Dettmers et al., 2022; Zeng et al., 2022),
và chưng cất (Hinton et al., 2015; Gou et al., 2021).

Các mô hình bộ nhớ LUMEN liên quan gần nhất với
tài liệu về bộ nhớ. Một phương pháp khác để giảm chi phí mã hóa
của các mô hình được tăng cường truy xuất là tính toán trước
các biểu diễn cho kho truy xuất và thu thập những biểu diễn này
thành một bộ nhớ, do đó phân bổ chi phí mã hóa
trên tất cả các thể hiện mà một mẫu được truy xuất.
Đặc biệt, LUMEN có liên quan chặt chẽ với Li et al. (2022),
người đề xuất một mô hình FiD bộ nhớ với các biểu diễn
bộ mã hóa được tính toán trước. LUMEN có thể được xem như một lai của Li
et al. (2022) và FiD mà tính toán trước một phần các biểu diễn bộ mã hóa
cho hiệu quả, và hoàn thiện các biểu diễn bộ mã hóa
theo thời gian thực được điều kiện hóa trên câu hỏi và tác vụ để
tránh hình phạt hiệu suất mạnh từ tính toán trước.

8

--- TRANG 9 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

Việc tính toán trước một phần này là một dạng tương tác muộn,
đã được sử dụng trước đây để cải thiện việc lựa chọn
các đoạn văn được truy xuất (Khattab & Zaharia, 2020; Santhanam
et al., 2022). Thay vào đó, chúng tôi sử dụng tương tác muộn trong mô hình đọc.
Milbauer et al. (2023) cũng sử dụng tương tác muộn
trong mô hình đọc, nhưng trong bối cảnh NLI thay vì
sinh văn bản được tăng cường truy xuất.

LUMEN sử dụng bộ nhớ một cách đơn giản, đơn
giản tính toán trước các biểu diễn token từ một mô hình
được tiền huấn luyện và truy xuất các đoạn văn với một bộ truy xuất đoạn văn
dày đặc tiêu chuẩn. Các mô hình bộ nhớ khác có thể phức tạp hơn, tích
hợp truy xuất đầu cuối đến cuối trong mô hình (de Jong
et al., 2022b; Wu et al., 2022a), lưu trữ các biểu diễn tiềm ẩn
cấp cao hơn (de Jong et al., 2022b; Chen et al., 2022; Wu
et al., 2022b), và tiền huấn luyện cụ thể cho bộ nhớ (de Jong
et al., 2022b; Zhong et al., 2022). Ý tưởng chính đằng sau LU-
MEN để cập nhật các biểu diễn bộ nhớ được truy xuất điều kiện
hóa trên đầu vào là bổ sung và có thể được kết hợp
với những mô hình bộ nhớ phức tạp hơn này.

6. Kết luận

Các mô hình ngôn ngữ được tăng cường truy xuất như Fusion-in-
Decoder mạnh mẽ nhưng tốn kém. Việc tính toán trước các biểu diễn
bộ mã hóa thành bộ nhớ dày đặc, một phương pháp phổ biến
để giảm chi phí tính toán của các mô hình được tăng cường truy xuất,
dẫn đến sự giảm mạnh về hiệu suất. Chúng tôi đề xuất
LUMEN, một lai giữa Fusion-in-Decoder và bộ nhớ
dày đặc. Các biểu diễn đoạn văn được mã hóa trước một phần
thành bộ nhớ dày đặc, và sau đó được xử lý lại theo thời gian thực bởi
một bộ mã hóa được tinh chỉnh điều kiện hóa trên câu hỏi. Chúng tôi
cho thấy LUMEN đạt được hiệu suất mạnh hơn với cùng FLOPs,
và lợi thế này tăng theo quy mô.

Lời cảm ơn

Chúng tôi cảm ơn DeLesley Hutchins, Santiago Ontanon, Pat Verga,
Markus Rabe, Yuhai Wu, Emma Strubell, và những người khác
tại Google Research vì lời khuyên và thảo luận sâu sắc.
Michiel de Jong được hỗ trợ một phần bởi các Giải thưởng NSF
IIS-1513966/ 1632803/1833137, CCF-1139148, Giải thưởng DARPA
#: FA8750-18-2-0117, FA8750-19-1-0504, DARPA-
D3M - Giải thưởng UCB-00009528, Giải thưởng Nghiên cứu Google,
quà tặng từ Facebook và Netflix, và ARO# W911NF-12-1-
0241 và W911NF-15-1-0484.

Tài liệu tham khảo

Ainslie, J., Ontañón, S., Alberti, C., Cvicek, V., Fisher,
Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., và
Yang, L. ETC: mã hóa đầu vào dài và có cấu trúc trong
transformers. Trong Webber, B., Cohn, T., He, Y., và Liu, Y.
(biên tập), Kỷ yếu Hội nghị 2020 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, EMNLP 2020,
Trực tuyến, 16-20 tháng 11, 2020, trang 268–284. Hiệp hội
Ngôn ngữ học Tính toán, 2020. doi: 10.18653/v1/
2020.emnlp-main.19. URL https://doi.org/10.
18653/v1/2020.emnlp-main.19.

Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,
Lebrón, F., và Sanghai, S. Gqa: Huấn luyện các mô hình transformer
đa truy vấn tổng quát từ các checkpoint đa đầu, 2023.

Berant, J., Chou, A., Frostig, R., và Liang, P. Phân tích ngữ nghĩa
trên freebase từ các cặp câu hỏi-trả lời. Trong
Kỷ yếu Hội nghị 2013 về Phương pháp Thực nghiệm
trong Xử lý Ngôn ngữ Tự nhiên, EMNLP 2013, 18-
21 tháng 10, 2013, Grand Hyatt Seattle, Seattle, Wash-
ington, USA, Một cuộc họp của SIGDAT, một Nhóm Quan tâm Đặc biệt
của ACL, trang 1533–1544. ACL, 2013. URL
https://aclanthology.org/D13-1160/.

Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther-
ford, E., Millican, K., van den Driessche, G., Lespiau,
J., Damoc, B., Clark, A., de Las Casas, D., Guy, A.,
Menick, J., Ring, R., Hennigan, T., Huang, S., Mag-
giore, L., Jones, C., Cassirer, A., Brock, A., Paganini,
M., Irving, G., Vinyals, O., Osindero, S., Simonyan,
K., Rae, J. W., Elsen, E., và Sifre, L. Cải thiện các mô hình ngôn ngữ
bằng cách truy xuất từ hàng nghìn tỷ token. Trong
Chaudhuri, K., Jegelka, S., Song, L., Szepesv´ari, C., Niu,
G., và Sabato, S. (biên tập), Hội nghị Quốc tế về
Học máy, ICML 2022, 17-23 tháng 7, 2022, Balti-
more, Maryland, USA, tập 162 của Kỷ yếu
Nghiên cứu Học máy, trang 2206–2240. PMLR,
2022. URL https://proceedings.mlr.press/
v162/borgeaud22a.html.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,
Wanderman-Milne, S., và Zhang, Q. JAX: các phép biến đổi có thể kết hợp
của các chương trình Python+NumPy, 2018. URL
http://github.com/google/jax.

Chen, W., Verga, P., de Jong, M., Wieting, J., và Co-
hen, W. W. Tăng cường các mô hình ngôn ngữ được tiền huấn luyện
với qa-memory cho trả lời câu hỏi miền mở.
CoRR, abs/2204.04581, 2022. doi: 10.48550/arXiv.
2204.04581. URL https://doi.org/10.48550/
arXiv.2204.04581.

de Jong, M., Zemlyanskiy, Y., Ainslie, J., FitzGerald, N.,
Sanghai, S., Sha, F., và Cohen, W. Fido: Fusion-in-
decoder được tối ưu hóa cho hiệu suất mạnh hơn và suy luận nhanh hơn.
arXiv preprint arXiv:2212.08153, 2022a.

de Jong, M., Zemlyanskiy, Y., FitzGerald, N., Sha, F., và
Cohen, W. W. Mention memory: tích hợp kiến thức văn bản

9

--- TRANG 10 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

vào transformers thông qua chú ý đề cập thực thể. Trong Hội nghị Quốc tế lần thứ Mười về Biểu diễn Học tập, ICLR 2022, Sự kiện Ảo, 25-29 tháng 4, 2022. OpenReview.net, 2022b. URL https://
openreview.net/forum?id=OY1A8ejQgEX.

Dettmers, T., Lewis, M., Belkada, Y., và Zettlemoyer, L.
Llm.int8(): phép nhân ma trận 8-bit cho transformers ở
quy mô. CoRR, abs/2208.07339, 2022. doi: 10.48550/arXiv.
2208.07339. URL https://doi.org/10.48550/
arXiv.2208.07339.

Google. Các mô hình t5 được tiền huấn luyện. https://github.
com/google-research/t5x/blob/main/
docs/models.md, 2022. Truy cập: 2022-12-20.

Gou, J., Yu, B., Maybank, S. J., và Tao, D.
Chưng cất kiến thức: Một khảo sát. Int. J. Com-
put. Vis., 129(6):1789–1819, 2021. doi: 10.1007/
s11263-021-01453-z. URL https://doi.org/10.
1007/s11263-021-01453-z.

Guo, M., Ainslie, J., Uthus, D. C., Ontañón, S., Ni, J., Sung,
Y., và Yang, Y. Longt5: Transformer văn bản thành văn bản hiệu quả
cho các chuỗi dài. Trong Carpuat, M., de Marneffe, M., và
Ruíz, I. V. M. (biên tập), Phát hiện của Hiệp hội
Ngôn ngữ học Tính toán: NAACL 2022, Seattle, WA,
Hoa Kỳ, 10-15 tháng 7, 2022, trang 724–736. Hiệp hội
Ngôn ngữ học Tính toán, 2022. doi: 10.18653/
v1/2022.findings-naacl.55. URL https://doi.org/
10.18653/v1/2022.findings-naacl.55.

Guu, K., Lee, K., Tung, Z., Pasupat, P., và Chang,
M. REALM: tiền huấn luyện mô hình ngôn ngữ được tăng cường truy xuất.
CoRR, abs/2002.08909, 2020. URL https:
//arxiv.org/abs/2002.08909.

Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre,
B., Steiner, A., và van Zee, M. Flax: Một thư viện mạng neural
và hệ sinh thái cho JAX, 2020. URL http://
github.com/google/flax.

Hinton, G. E., Vinyals, O., và Dean, J. Chưng cất kiến thức
trong một mạng neural. CoRR, abs/1503.02531, 2015.
URL http://arxiv.org/abs/1503.02531.

Hofstätter, S., Chen, J., Raman, K., và Zamani, H. Fid-
light: Sinh văn bản được tăng cường truy xuất hiệu quả và hiệu quả.
CoRR, abs/2209.14290, 2022. doi: 10.
48550/arXiv.2209.14290. URL https://doi.org/
10.48550/arXiv.2209.14290.

Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
S., Wang, L., và Chen, W. Lora: Thích ứng thứ hạng thấp
của các mô hình ngôn ngữ lớn. Trong Hội nghị Quốc tế lần thứ Mười
về Biểu diễn Học tập, ICLR
2022, Sự kiện Ảo, 25-29 tháng 4, 2022. OpenReview.net,

2022. URL https://openreview.net/forum?
id=nZeVKeeFYf9.

Izacard, G. và Grave, E. Tận dụng truy xuất đoạn văn
với các mô hình sinh để trả lời câu hỏi miền mở.
Trong Merlo, P., Tiedemann, J., và Tsarfaty,
R. (biên tập), Kỷ yếu Hội nghị lần thứ 16 của
Chương Châu Âu của Hiệp hội Ngôn ngữ học Tính toán: Tập Chính, EACL 2021, Trực tuyến, 19 - 23 tháng 4, 2021, trang 874–880. Hiệp hội
Ngôn ngữ học Tính toán, 2021. doi: 10.18653/
v1/2021.eacl-main.74. URL https://doi.org/10.
18653/v1/2021.eacl-main.74.

Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,
F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., và
Grave, E. Học vài mẫu với các mô hình ngôn ngữ được tăng cường truy xuất.
CoRR, abs/2208.03299, 2022. doi: 10.
48550/arXiv.2208.03299. URL https://doi.org/
10.48550/arXiv.2208.03299.

Joshi, M., Choi, E., Weld, D. S., và Zettlemoyer, L. Trivi-
aqa: Một bộ dữ liệu thách thức được giám sát từ xa quy mô lớn
cho hiểu đọc. Trong Barzilay, R. và Kan, M.
(biên tập), Kỷ yếu Cuộc họp Thường niên lần thứ 55 của
Hiệp hội Ngôn ngữ học Tính toán, ACL 2017,
Vancouver, Canada, 30 tháng 7 - 4 tháng 8, Tập 1: Bài báo Dài, trang 1601–1611. Hiệp hội Ngôn ngữ học Tính toán, 2017. doi: 10.18653/v1/P17-1147. URL
https://doi.org/10.18653/v1/P17-1147.

Karpukhin, V., Oguz, B., Min, S., Lewis, P. S. H., Wu,
L., Edunov, S., Chen, D., và Yih, W. Truy xuất đoạn văn dày đặc
cho trả lời câu hỏi miền mở. Trong Web-
ber, B., Cohn, T., He, Y., và Liu, Y. (biên tập), Kỷ
yếu Hội nghị 2020 về Phương pháp Thực nghiệm trong
Xử lý Ngôn ngữ Tự nhiên, EMNLP 2020, Trực tuyến,
16-20 tháng 11, 2020, trang 6769–6781. Hiệp hội
Ngôn ngữ học Tính toán, 2020. doi: 10.18653/v1/
2020.emnlp-main.550. URL https://doi.org/10.
18653/v1/2020.emnlp-main.550.

Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L.,
và Lewis, M. Tổng quát hóa thông qua ghi nhớ:
Các mô hình ngôn ngữ láng giềng gần nhất. Trong Hội nghị Quốc tế lần thứ 8
về Biểu diễn Học tập, ICLR 2020, Ad-
dis Ababa, Ethiopia, 26-30 tháng 4, 2020. OpenReview.net,
2020. URL https://openreview.net/forum?
id=HklBjCEKvH.

Khattab, O. và Zaharia, M. Colbert: Tìm kiếm đoạn văn hiệu quả và hiệu quả
thông qua tương tác muộn có ngữ cảnh trên BERT. Trong Huang, J. X., Chang, Y., Cheng,
X., Kamps, J., Murdock, V., Wen, J., và Liu, Y.
(biên tập), Kỷ yếu hội nghị ACM SI-
GIR quốc tế lần thứ 43 về nghiên cứu và phát triển trong Truy xuất Thông tin, SIGIR 2020, Sự kiện Ảo, Trung Quốc,

10

--- TRANG 11 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

25-30 tháng 7, 2020, trang 39–48. ACM, 2020. doi: 10.
1145/3397271.3401075. URL https://doi.org/
10.1145/3397271.3401075.

Kratzwald, B. và Feuerriegel, S. Truy xuất tài liệu thích ứng
cho trả lời câu hỏi sâu. Trong Riloff, E.,
Chiang, D., Hockenmaier, J., và Tsujii, J. (biên tập), Kỷ
yếu Hội nghị 2018 về Phương pháp Thực nghiệm
trong Xử lý Ngôn ngữ Tự nhiên, Brussels, Bel-
gium, 31 tháng 10 - 4 tháng 11, 2018, trang 576–581.
Hiệp hội Ngôn ngữ học Tính toán, 2018. doi:
10.18653/v1/d18-1055. URL https://doi.org/
10.18653/v1/d18-1055.

Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,
Parikh, A. P., Alberti, C., Epstein, D., Polosukhin, I.,
Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey,
M., Chang, M., Dai, A. M., Uszkoreit, J., Le, Q., và
Petrov, S. Natural questions: một benchmark cho nghiên cứu
trả lời câu hỏi. Trans. Assoc. Comput. Linguistics,
7:452–466, 2019. doi: 10.1162/tacl\a\00276. URL
https://doi.org/10.1162/tacl_a_00276.

Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F.,
Karpukhin, V., Goyal, N., Küttler, H., Lewis, M.,
Yih, W., Rocktäschel, T., Riedel, S., và Kiela, D.
Sinh được tăng cường truy xuất cho các tác vụ
NLP thâm dụng kiến thức. Trong Larochelle, H., Ranzato, M., Hadsell,
R., Balcan, M., và Lin, H. (biên tập), Tiến bộ trong
Hệ thống Xử lý Thông tin Neural 33: Hội nghị Thường niên
về Hệ thống Xử lý Thông tin Neural
2020, NeurIPS 2020, 6-12 tháng 12, 2020,
ảo, 2020. URL https://proceedings.
neurips.cc/paper/2020/hash/
6b493230205f780e1bc26945df7481e5-Abstract.
html.

Li, Z., Guo, R., và Kumar, S. Xử lý ngữ cảnh tách rời
cho mô hình ngôn ngữ được tăng cường ngữ cảnh.
CoRR, abs/2210.05758, 2022. doi: 10.48550/arXiv.
2210.05758. URL https://doi.org/10.48550/
arXiv.2210.05758.

Mao, Y., He, P., Liu, X., Shen, Y., Gao, J., Han, J., và
Chen, W. Xếp hạng lại đoạn văn được hướng dẫn bởi đọc giả cho trả lời câu hỏi
miền mở. Trong Zong, C., Xia, F., Li,
W., và Navigli, R. (biên tập), Phát hiện của Hiệp
hội Ngôn ngữ học Tính toán: ACL/IJCNLP 2021,
Sự kiện Trực tuyến, 1-6 tháng 8, 2021, tập ACL/IJCNLP
2021 của Phát hiện của ACL, trang 344–350. Hiệp hội
Ngôn ngữ học Tính toán, 2021. doi: 10.18653/v1/
2021.findings-acl.29. URL https://doi.org/10.
18653/v1/2021.findings-acl.29.

Milbauer, J. L., Louis, A., Hosseini, J., Fabrikant, A., Met-
zler, D., và Schuster, T. Lait: Mã hóa đa phân đoạn hiệu quả
trong transformers với tương tác có thể điều chỉnh theo lớp. Trong Kỷ yếu Hiệp hội Ngôn ngữ học Tính toán: ACL 2023, 2023.

Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Brad-
bury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal,
S., và Dean, J. Mở rộng suy luận transformer một cách hiệu quả.
CoRR, abs/2211.05102, 2022. doi: 10.48550/arXiv.
2211.05102. URL https://doi.org/10.48550/
arXiv.2211.05102.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., và Liu, P. J. Khám phá
giới hạn của học chuyển giao với một transformer văn bản thành văn bản
thống nhất. J. Mach. Learn. Res., 21:140:1–140:67, 2020.
URL http://jmlr.org/papers/v21/20-074.
html.

Roberts, A., Raffel, C., và Shazeer, N. Bạn có thể nhét bao nhiêu kiến thức
vào các tham số của một mô hình ngôn ngữ? Trong Webber, B., Cohn, T., He, Y., và Liu, Y.
(biên tập), Kỷ yếu Hội nghị 2020 về Phương pháp Thực nghiệm
trong Xử lý Ngôn ngữ Tự nhiên, EMNLP 2020,
Trực tuyến, 16-20 tháng 11, 2020, trang 5418–5426. Hiệp hội
Ngôn ngữ học Tính toán, 2020. doi: 10.18653/
v1/2020.emnlp-main.437. URL https://doi.org/
10.18653/v1/2020.emnlp-main.437.

Roberts, A., Chung, H. W., Levskaya, A., Mishra, G., Brad-
bury, J., Andor, D., Narang, S., Lester, B., Gaffney, C.,
Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Sal-
cianu, A., van Zee, M., Austin, J., Goodman, S., Soares,
L. B., Hu, H., Tsvyashchenko, S., Chowdhery, A., Bast-
ings, J., Bulian, J., Garcia, X., Ni, J., Chen, A., Kenealy,
K., Clark, J. H., Lee, S., Garrette, D., Lee-Thorp, J.,
Raffel, C., Shazeer, N., Ritter, M., Bosma, M., Passos,
A., Maitin-Shepard, J., Fiedel, N., Omernick, M., Saeta,
B., Sepassi, R., Spiridonov, A., Newlan, J., và Ges-
mundo, A. Mở rộng các mô hình và dữ liệu với t5x và
seqio. arXiv preprint arXiv:2203.17189, 2022. URL
https://arxiv.org/abs/2203.17189.

Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., và
Zaharia, M. Colbertv2: Truy xuất hiệu quả và hiệu quả
thông qua tương tác muộn nhẹ. Trong Carpuat, M., de Marn-
effe, M., và Ruíz, I. V. M. (biên tập), Kỷ yếu
Hội nghị 2022 của Chương Bắc Mỹ của
Hiệp hội Ngôn ngữ học Tính toán: Công nghệ Ngôn ngữ Con người, NAACL 2022, Seattle, WA, Hoa
Kỳ, 10-15 tháng 7, 2022, trang 3715–3734. Hiệp hội
Ngôn ngữ học Tính toán, 2022. doi: 10.18653/v1/
2022.naacl-main.272. URL https://doi.org/10.
18653/v1/2022.naacl-main.272.

Shazeer, N. Giải mã transformer nhanh: Một đầu ghi
là tất cả những gì bạn cần. CoRR, abs/1911.02150, 2019. URL
http://arxiv.org/abs/1911.02150.

11

--- TRANG 12 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

Shazeer, N. và Stern, M. Adafactor: Tốc độ học thích ứng
với chi phí bộ nhớ dưới tuyến tính. Trong Dy,
J. G. và Krause, A. (biên tập), Kỷ yếu
Hội nghị Quốc tế lần thứ 35 về Học máy,
ICML 2018, Stockholmsmässan, Stockholm, Sweden,
10-15 tháng 7, 2018, tập 80 của Kỷ yếu Nghiên cứu Học máy, trang 4603–4611. PMLR,
2018. URL http://proceedings.mlr.press/
v80/shazeer18a.html.

Varshney, N., Luo, M., và Baral, C. Liệu đọc giả QA miền mở có thể
sử dụng kiến thức bên ngoài một cách hiệu quả như con người?
CoRR, abs/2211.12707, 2022. doi: 10.48550/arXiv.
2211.12707. URL https://doi.org/10.48550/
arXiv.2211.12707.

Wang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., Zhang,
W., Chang, S., Tesauro, G., Zhou, B., và Jiang, J. R3:
Ranker-reader được củng cố cho trả lời câu hỏi miền mở. Trong McIlraith, S. A. và Weinberger, K. Q. (biên tập),
Kỷ yếu Hội nghị AAAI lần thứ Ba mươi hai về
Trí tuệ Nhân tạo, (AAAI-18), Ứng dụng Sáng tạo lần thứ 30 của Trí tuệ Nhân tạo (IAAI-18), và Hội thảo AAAI lần thứ 8 về Tiến bộ Giáo dục trong Trí tuệ Nhân tạo (EAAI-18), New Orleans, Louisiana, USA,
2-7 tháng 2, 2018, trang 5981–5988. AAAI Press, 2018.
URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/16712.

Wu, Y., Rabe, M. N., Hutchins, D., và Szegedy, C.
Ghi nhớ transformers. Trong Hội nghị Quốc tế lần thứ Mười
về Biểu diễn Học tập, ICLR 2022,
Sự kiện Ảo, 25-29 tháng 4, 2022. OpenReview.net,
2022a. URL https://openreview.net/forum?
id=TrjbxzRcnf-.

Wu, Y., Zhao, Y., Hu, B., Minervini, P., Stene-
torp, P., và Riedel, S. Một transformer được tăng cường bộ nhớ hiệu quả
cho các tác vụ NLP thâm dụng kiến thức.
CoRR, abs/2210.16773, 2022b. doi: 10.48550/
arXiv.2210.16773. URL https://doi.org/10.
48550/arXiv.2210.16773.

Yu, D., Zhu, C., Fang, Y., Yu, W., Wang, S., Xu, Y., Ren,
X., Yang, Y., và Zeng, M. Kg-fid: Truyền đồ thị kiến thức
vào fusion-in-decoder cho trả lời câu hỏi miền mở. Trong Muresan, S., Nakov, P., và Villavi-
cencio, A. (biên tập), Kỷ yếu Cuộc họp Thường niên lần thứ 60 của
Hiệp hội Ngôn ngữ học Tính toán
(Tập 1: Bài báo Dài), ACL 2022, Dublin, Ireland,
22-27 tháng 5, 2022, trang 4961–4974. Hiệp hội
Ngôn ngữ học Tính toán, 2022a. doi: 10.18653/v1/2022.
acl-long.340. URL https://doi.org/10.18653/
v1/2022.acl-long.340.

Yu, W., Iter, D., Wang, S., Xu, Y., Ju, M., Sanyal, S., Zhu,
C., Zeng, M., và Jiang, M. Sinh ra thay vì truy xuất:
Các mô hình ngôn ngữ lớn là những người tạo ngữ cảnh mạnh mẽ.
CoRR, abs/2209.10063, 2022b. doi: 10.48550/arXiv.
2209.10063. URL https://doi.org/10.48550/
arXiv.2209.10063.

Zaken, E. B., Goldberg, Y., và Ravfogel, S. Bitfit: Tinh chỉnh
tham số hiệu quả đơn giản cho các mô hình ngôn ngữ có mặt nạ
dựa trên transformer. Trong Muresan, S., Nakov, P., và
Villavicencio, A. (biên tập), Kỷ yếu Cuộc họp Thường niên lần thứ 60 của
Hiệp hội Ngôn ngữ học Tính toán
(Tập 2: Bài báo Ngắn), ACL 2022, Dublin, Ireland,
22-27 tháng 5, 2022, trang 1–9. Hiệp hội
Ngôn ngữ học Tính toán, 2022. doi: 10.18653/v1/2022.acl-short.
1. URL https://doi.org/10.18653/v1/2022.
acl-short.1.

Zemlyanskiy, Y., Ainslie, J., de Jong, M., Pham, P., Eck-
stein, I., và Sha, F. Readtwice: Đọc các tài liệu rất lớn
với bộ nhớ. Trong Toutanova, K., Rumshisky,
A., Zettlemoyer, L., Hakkani-Tür, D., Beltagy, I., Bethard,
S., Cotterell, R., Chakraborty, T., và Zhou, Y. (biên tập), Kỷ
yếu Hội nghị 2021 của Chương Bắc Mỹ
của Hiệp hội Ngôn ngữ học Tính toán: Công nghệ Ngôn ngữ Con người, NAACL-HLT 2021,
Trực tuyến, 6-11 tháng 6, 2021, trang 5189–5195. Hiệp hội
Ngôn ngữ học Tính toán, 2021. doi: 10.18653/v1/
2021.naacl-main.408. URL https://doi.org/10.
18653/v1/2021.naacl-main.408.

Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,
Yang, Z., Xu, Y., Zheng, W., Xia, X., Tam, W. L.,
Ma, Z., Xue, Y., Zhai, J., Chen, W., Zhang, P., Dong,
Y., và Tang, J. GLM-130B: một mô hình song ngữ được tiền huấn luyện mở.
CoRR, abs/2210.02414, 2022. doi: 10.
48550/arXiv.2210.02414. URL https://doi.org/
10.48550/arXiv.2210.02414.

Zhong, Z., Lei, T., và Chen, D. Huấn luyện các mô hình ngôn ngữ
với tăng cường bộ nhớ. CoRR, abs/2205.12674,
2022. doi: 10.48550/arXiv.2205.12674. URL https:
//doi.org/10.48550/arXiv.2205.12674.

A. Dẫn xuất Độ phức tạp

Bảng 1 cho thấy FLOPs cho mỗi lớp cho FiD, MemoryFiD
và LUMEN. Chúng tôi đi vào chi tiết hơn về những giá trị đó ở đây.
Để làm nền, chúng ta có đầu vào nguồn với độ dài ns, một
đầu ra mục tiêu với độ dài nt, và một mô hình với L lớp và
chiều d. Một lớp tuyến tính từ k đến l sử dụng kl FLOPs.
Mỗi lớp Transformer bao gồm một lớp chú ý, một lớp
feed-forward, và (trong trường hợp của bộ giải mã) một lớp chú ý chéo.
Mỗi lớp chú ý thực hiện các phép chiếu query, key, value và
output, cũng như tính toán điểm chú ý và giá trị.
Các phép chiếu ánh xạ từ d đến d và sử dụng d2
FLOPs cho mỗi token, tổng cộng 4d2 cho mỗi token. Lớp feedforward
của một Transformer vanilla ánh xạ mỗi token từ d đến
4d và ngược lại, tiêu thụ 8d2 FLOPs cho mỗi token.

Việc tính toán điểm chú ý và giá trị liên quan đến việc lấy
tích trong của nq·nv cặp vector có chiều
d, tổng cộng nqnvd FLOPs mỗi cái. Đối với một lớp bộ mã hóa FiD,
số lượng query bằng độ dài nguồn, trong khi
số lượng value bằng độ dài của mỗi đoạn văn
và câu hỏi (vì FiD mã hóa mỗi đoạn văn riêng biệt).
Do đó việc tính toán điểm chú ý và giá trị tổng cộng
2nsnpd FLOPs, dẫn đến tổng FLOPs lớp bộ mã hóa là
12nsd2+ 2nsnpd

Vì các đoạn văn được truy xuất ngắn so với chiều mô hình
(np<< d), đặc biệt đối với các mô hình lớn hơn, hạng từ tính toán điểm chú ý
rất nhỏ và chúng tôi bỏ qua nó trong ước tính của mình. Việc dẫn xuất cho bộ giải mã tương tự, ngoại trừ
sự hiện diện của các lớp chú ý chéo. Đối với chú ý chéo,
các phép chiếu key và value áp dụng cho đầu vào nguồn và
các phép chiếu query và output áp dụng cho đầu ra mục tiêu.
Do đó, FLOPs lớp bộ giải mã được cho bởi
12ntd2+ 2ntd2+ 2nsd2+ 2ntnsd

trong đó hạng từ tính toán chú ý một lần nữa không đáng kể.
Tổng FLOPs từ cả hai thành phần, chúng ta có FLOPs cho mỗi
lớp là
14nsd2+ 14ntd2

Cuối cùng, MemoryFiD đơn giản loại bỏ FLOPs bộ mã hóa,
trong khi LUMEN nhân FLOPs bộ mã hóa với tỷ lệ trực tiếp
α, tạo ra các con số trong Bảng 1.

B. FLOPs vs Độ trễ

Các kết quả thí nghiệm chính của chúng tôi đo chi phí tính toán
theo FLOPs. Tuy nhiên, tùy thuộc vào cấu hình mô hình
và thiết lập phần cứng, suy luận có thể chậm hơn so với những gì
FLOPs sẽ gợi ý do các ràng buộc băng thông bộ nhớ.
Trong phần này chúng tôi cho thấy rằng lợi ích hiệu quả FLOPs của LUMEN
chuyển thành cải thiện thông lượng thực tế.

Chúng tôi xem xét hai thiết lập. Thứ nhất, chúng tôi điều tra cách LU-
MEN ảnh hưởng đến tốc độ huấn luyện, vì băng thông bộ nhớ chủ yếu
là một nút thắt cổ chai cho suy luận tự hồi quy. Thứ hai,
FiDO (de Jong et al., 2022a) đã chỉ ra rằng chi phí băng thông bộ nhớ
từ suy luận bộ giải mã có thể được giảm drastically
với sự giảm tối thiểu về chất lượng thông qua chú ý chéo
thưa theo lớp hoặc chú ý đa truy vấn. Tại thời điểm
các thí nghiệm chính của chúng tôi, chúng tôi không có quyền truy cập vào các checkpoint T5
với chú ý đa truy vấn, nhưng trong sản xuất LU-
MEN sẽ được triển khai với chú ý đa truy vấn. Hơn
nữa, nghiên cứu gần đây đã chỉ ra rằng các checkpoint đa truy vấn

có thể được thu được một cách hiệu quả từ các checkpoint đa đầu hiện có
(Ainslie et al., 2023). Do đó, chúng tôi báo cáo các phép đo suy luận
cho T5 với chú ý đa truy vấn.

Hình 11 cho thấy độ trễ như một hàm của tỷ lệ trực tiếp,
chứng minh rằng độ trễ thay đổi gần như tuyến tính với
số lượng các lớp trực tiếp còn lại. Hình 12 thay vào đó cho thấy
sự đánh đổi giữa độ trễ và chất lượng, tạo ra cùng
mô hình như Hình 1.

13

--- TRANG 13 ---
Bộ nhớ được tính toán trước hay mã hóa theo thời gian thực? Một phương pháp tiếp cận lai cho tăng cường truy xuất tận dụng tối đa tài nguyên tính toán của bạn

0 0.2 0.4 0.6 0.8 10.511.52
Tỷ lệ các lớp bộ mã hóa trực tiếpThời gian cho mỗi mẫu (s)Tinh chỉnh
0 0.2 0.4 0.6 0.8 100.20.40.6
Tỷ lệ các lớp bộ mã hóa trực tiếpSuy luận
Hình 11. Thời gian tinh chỉnh cho mỗi mẫu cho LUMEN-XXL như một hàm của tỷ lệ trực tiếp.

0.5 1 1.5 2455055
Thời gian cho mỗi mẫu (s)Đúng khớpTinh chỉnh
0 0.1 0.2 0.3 0.4 0.5 0.6455055
Thời gian cho mỗi mẫu (s)Suy luận
Hình 12. Độ khớp chính xác trên Natural Questions như một hàm của thời gian suy luận cho mỗi mẫu cho LUMEN-XXL, quét qua tỷ lệ trực tiếp.
Giả định đa truy vấn cho các phép đo suy luận.

14
