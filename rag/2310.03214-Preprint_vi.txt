# 2310.03214.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2310.03214.pdf
# Kích thước file: 2314447 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Bản thảo
FRESH LLM S:
LÀM MỚI CÁC MÔ HÌNH NGÔN NGỮ LỚN
BẰNG TĂNG CƯỜNG CÔNG CỤ TÌM KIẾM
Tu Vu1Mohit Iyyer2Xuezhi Wang1Noah Constant1Jerry Wei1
Jason Wei3∗Chris Tar1Yun-Hsuan Sung1Denny Zhou1Quoc Le1Thang Luong1
Google1University of Massachusetts Amherst2OpenAI3
freshllms@google.com
TÓM TẮT
Hầu hết các mô hình ngôn ngữ lớn (LLM) được huấn luyện một lần và không bao giờ được cập nhật; do đó,
chúng thiếu khả năng thích ứng động với thế giới luôn thay đổi của chúng ta. Trong công trình này, chúng tôi
thực hiện một nghiên cứu chi tiết về tính chính xác sự thật của văn bản được tạo ra bởi LLM trong bối cảnh
trả lời các câu hỏi kiểm tra kiến thức thế giới hiện tại. Cụ thể, chúng tôi giới thiệu FRESH QA, một benchmark QA
động mới bao gồm nhiều loại câu hỏi và câu trả lời đa dạng, bao gồm các câu hỏi yêu cầu kiến thức thế giới
thay đổi nhanh cũng như các câu hỏi với tiền đề sai cần được bác bỏ. Chúng tôi đo lường một loạt đa dạng
các LLM đóng và mã nguồn mở dưới một quy trình đánh giá hai chế độ cho phép chúng tôi đo cả tính đúng đắn
và ảo giác. Thông qua đánh giá của con người liên quan đến hơn 50K nhận định, chúng tôi làm sáng tỏ các
hạn chế của những mô hình này và chứng minh khoảng trống đáng kể để cải thiện: ví dụ, tất cả các mô hình
(bất kể kích thước mô hình) đều gặp khó khăn với các câu hỏi liên quan đến kiến thức thay đổi nhanh và tiền đề sai.
Được thúc đẩy bởi những kết quả này, chúng tôi trình bày FRESH PROMPT, một phương pháp prompting few-shot
đơn giản giúp tăng đáng kể hiệu suất của LLM trên FRESH QA bằng cách kết hợp thông tin liên quan và cập nhật
được lấy từ công cụ tìm kiếm vào prompt. Các thí nghiệm của chúng tôi cho thấy FRESH PROMPT vượt trội hơn
cả các phương pháp prompting tăng cường công cụ tìm kiếm cạnh tranh như SELF-ASK (Press et al., 2022) cũng như
các hệ thống thương mại như PERPLEXITY.AI.1 Phân tích sâu hơn về FRESH PROMPT cho thấy cả số lượng bằng chứng
được lấy và thứ tự của chúng đều đóng vai trò quan trọng trong việc ảnh hưởng đến tính đúng đắn của các câu trả lời
được tạo ra bởi LLM. Ngoài ra, hướng dẫn LLM tạo ra các câu trả lời ngắn gọn và trực tiếp giúp giảm ảo giác so với
khuyến khích các câu trả lời dài dòng hơn. Để tạo điều kiện cho công việc tương lai, chúng tôi phát hành FRESH QA
tại github.com/freshllms/freshqa và cam kết cập nhật nó theo định kỳ.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) gần đây như BARD và CHATGPT/GPT-42 được thiết kế để trở thành
các chatbot mở đa năng có thể tham gia vào các cuộc trò chuyện nhiều lượt về các chủ đề đa dạng.
Mặc dù có khả năng ấn tượng, những LLM này thường "ảo giác" thông tin hợp lý nhưng không chính xác
về mặt sự thật (Maynez et al., 2020; Liu et al., 2023b), điều này làm giảm độ tin cậy của các phản hồi
của chúng, đặc biệt trong các tình huống mà thông tin chính xác và cập nhật là quan trọng. Hành vi này
có thể được gán một phần cho sự hiện diện của kiến thức lỗi thời được mã hóa trong các tham số của chúng.
Trong khi việc huấn luyện bổ sung sử dụng phản hồi của con người (Ouyang et al., 2022) hoặc các tác vụ
tăng cường kiến thức có thể giảm thiểu vấn đề này, nó không dễ dàng mở rộng cho các cập nhật kiến thức
thời gian thực (ví dụ, giá cổ phiếu của một công ty). Học in-context (Brown et al., 2020) là một lựa chọn
hấp dẫn trong đó kiến thức thời gian thực có thể được tiêm vào prompt của LLM để điều kiện hóa việc tạo ra.
Trong khi công việc gần đây đã bắt đầu khám phá việc tăng cường LLM bằng kết quả tìm kiếm web
(Lazaridou et al., 2022; Press et al., 2022), vẫn chưa rõ làm thế nào để tận dụng đầy đủ đầu ra của công cụ
tìm kiếm để tăng tính chính xác sự thật của LLM.

∗Công việc được thực hiện khi ở Google.
1https://www.perplexity.ai
2https://bard.google.com ,https://chat.openai.com
1arXiv:2310.03214v2 [cs.CL] 22 Nov 2023

--- TRANG 2 ---
Bản thảo
LoạiCâu hỏiCâu trả lời (tính đến thời điểm viết này)không bao giờ thay đổiTiểu thuyết của Virginia Woolf về gia đình Ramsay đã được phép sử dụng công cộng ở Hoa Kỳ chưa?Có, tiểu thuyết năm 1927 To the Lighthouse của Virginia Woolf đã được phép sử dụng công cộng vào năm 2023.không bao giờ thay đổiNữ hoàng Elizabeth II của Anh nổi tiếng với việc nuôi giống chó gì?Chó Pembroke Welsh Corgi.thay đổi chậmTesla cung cấp bao nhiêu mẫu xe?Tesla cung cấp năm mẫu xe: Model S, Model X, Model 3, Model Y, và Tesla Semi.thay đổi chậmĐội nào giữ kỷ lục về mức độ thiệt hại lớn nhất được khắc phục để thắng một trận đấu NFL?Kỷ lục về sự trở lại lớn nhất của NFL được giữ bởi Minnesota Vikings.thay đổi nhanh Trò chơi nào đã giành giải thưởng Spiel des Jahres gần đây nhất?Dorfromantik đã thắng Spiel des Jahres 2023.thay đổi nhanh Bộ phim gần đây nhất của Brad Pitt với vai trò diễn viên là gì?Brad Pitt gần đây đã đóng vai chính trong Babylon, do Damien Chazelle đạo diễn.tiền đề saiVăn bản của tweet đầu tiên của Donald Trump vào năm 2022 là gì, được đăng sau khi ông được Elon Musk bỏ lệnh cấm trên Twitter?Ông đã không tweet vào năm 2022.tiền đề saiNovak Djokovic đã thua ở vòng nào tại Australian Open 2022?Ông không được phép thi đấu tại giải đấu do tình trạng tiêm chủng của mình.

Hình 1: Các ví dụ FRESH QA. Các câu hỏi của chúng tôi được chia rộng rãi thành bốn danh mục chính
dựa trên bản chất của câu trả lời: không bao giờ thay đổi, trong đó câu trả lời hầu như không bao giờ thay đổi;
thay đổi chậm, trong đó câu trả lời thường thay đổi trong vòng vài năm; thay đổi nhanh, trong đó câu trả lời
thường thay đổi trong vòng một năm hoặc ít hơn; và tiền đề sai, bao gồm các câu hỏi có tiền đề không chính xác
về mặt sự thật và do đó phải được bác bỏ.

Trong công trình này, chúng tôi thu thập một benchmark QA mới, được gọi là FRESH QA, để đánh giá tính chính xác
sự thật của các LLM hiện có. FRESH QA bao gồm 600 câu hỏi tự nhiên được chia rộng rãi thành bốn danh mục chính
được hiển thị trong Hình 1. Các câu hỏi của FRESH QA bao gồm một tập hợp đa dạng các chủ đề với các mức độ khó
khác nhau (yêu cầu lý luận một bước và nhiều bước), và yêu cầu một mô hình "hiểu" kiến thức cập nhật của thế giới
để có thể trả lời chính xác. Ngoài ra, FRESH QA có tính chất động: một số câu trả lời chuẩn có thể thay đổi theo thời gian,
và một câu hỏi được phân loại dưới một danh mục cụ thể có thể trải qua việc phân loại lại tại một thời điểm nào đó
sau này (ví dụ, câu hỏi tiền đề sai hiện tại "Elon Musk đã kết hôn với vợ hiện tại của mình bao lâu?" sẽ rơi vào
danh mục thay đổi nhanh nếu Elon Musk kết hôn lại trong tương lai).

Chúng tôi đo lường mức độ hoạt động tốt của các LLM khác nhau trên FRESH QA bằng cách prompting chúng với các câu hỏi
và tùy chọn một vài minh chứng câu hỏi-câu trả lời rồi sampling một phản hồi. Sau đó, chúng tôi tiến hành một đánh giá
con người mở rộng về độ chính xác sự thật của các phản hồi của mô hình, bao gồm hơn 50K nhận định. Chúng tôi
đánh giá mỗi phản hồi trong một quy trình đánh giá hai chế độ: RELAXED, đo chỉ liệu câu trả lời chính có đúng không;
và STRICT, đo liệu tất cả các tuyên bố trong phản hồi có chính xác về mặt sự thật và cập nhật không (tức là, không có ảo giác).
Nghiên cứu của chúng tôi làm sáng tỏ tính chính xác sự thật của các LLM cũ và mới và tiết lộ các hành vi mô hình khác nhau
qua các loại câu hỏi. Không ngạc nhiên, có những đường cong tỷ lệ phẳng trên các câu hỏi liên quan đến kiến thức thay đổi nhanh:
việc chỉ đơn giản tăng kích thước mô hình không dẫn đến tăng hiệu suất đáng tin cậy. Chúng tôi cũng quan sát các xu hướng
tương tự trên các câu hỏi tiền đề sai, mặc dù một số LLM có thể bác bỏ một câu hỏi tiền đề sai nếu được yêu cầu một cách
rõ ràng "Vui lòng kiểm tra xem câu hỏi có chứa tiền đề hợp lệ trước khi trả lời". Nhìn chung, FRESH QA là thách thức
cho các LLM hiện tại và để lại khoảng trống rộng rãi để cải thiện.

Được thúc đẩy bởi những phát hiện này, chúng tôi tiếp tục điều tra cách cải thiện hiệu quả tính chính xác sự thật của LLM
bằng cách căn cứ các phản hồi của chúng vào thông tin chính xác và cập nhật từ các công cụ tìm kiếm. Với sự phát triển
nhanh chóng của các LLM ngày càng lớn hơn và bản chất luôn thay đổi của kiến thức, chúng tôi khám phá các phương pháp
học in-context cho phép LLM chú ý đến kiến thức được cung cấp tại thời điểm suy luận thông qua prompt của nó.
Chúng tôi phát triển FRESH PROMPT, một phương pháp đơn giản nhưng hiệu quả mà, đối với một câu hỏi nhất định,
tận dụng đầy đủ công cụ tìm kiếm bằng cách trích xuất tất cả thông tin cập nhật và liên quan (bao gồm kiến thức từ
các câu hỏi liên quan mà người dùng tìm kiếm cũng hỏi) và sử dụng học in-context few-shot để dạy mô hình lý luận
trên các bằng chứng được lấy và tìm ra câu trả lời đúng. Chúng tôi cho thấy FRESH PROMPT tăng đáng kể tính chính xác
sự thật của LLM: ví dụ, biến thể GPT-4 + FRESH PROMPT tốt nhất của chúng tôi mang lại cải thiện 32.6% và 49.0%
độ chính xác so với GPT-4 vanilla trên FRESH QA dưới RELAXED và STRICT, tương ứng. Vì phương pháp của chúng tôi
không yêu cầu huấn luyện bổ sung, nó linh hoạt và có thể áp dụng cho nhiều tình huống khác nhau.

Kết hợp lại, các đóng góp chính của chúng tôi bao gồm:

--- TRANG 3 ---
Bản thảo
• Chúng tôi giới thiệu một benchmark QA động mới, FRESH QA, có đặc trưng là một tập hợp đa dạng
các loại câu hỏi và câu trả lời, bao gồm các câu hỏi có câu trả lời có thể thay đổi theo thời gian và
các câu hỏi có tiền đề không chính xác về mặt sự thật. Chúng tôi cung cấp miễn phí tập dữ liệu
của mình và cam kết cập nhật các câu trả lời chuẩn theo lịch trình thường xuyên để khuyến khích
khám phá các phương pháp cải thiện tính chính xác sự thật của LLM.
• Chúng tôi đo lường một loạt rộng rãi các LLM đóng và mã nguồn mở trên tập dữ liệu của chúng tôi.
Thông qua một nghiên cứu đánh giá con người mở rộng và nghiêm ngặt, chúng tôi làm sáng tỏ các
hạn chế của các LLM hiện tại: chúng gặp khó khăn với các câu hỏi thay đổi nhanh, tiền đề sai, và
nhiều bước, và đánh giá hai chế độ của chúng tôi ghi lại sự gia tăng ảo giác được tạo ra bởi các
kỹ thuật như prompting chain-of-thought (Wei et al., 2022).
• Chúng tôi trình bày FRESH PROMPT, một phương pháp học in-context đơn giản có thể tăng đáng kể
tính chính xác sự thật của LLM so với các phương pháp tăng cường tìm kiếm cạnh tranh bằng cách
kết hợp hiệu quả thông tin sự thật và cập nhật từ công cụ tìm kiếm vào prompt của mô hình.
Hơn nữa, chúng tôi thực hiện một loạt phân tích độ nhạy và ablation để hiểu rõ hơn những khía cạnh
nào của FRESH PROMPT đóng góp vào thành công của nó.

2 FRESH QA
Trong phần này, chúng tôi giải quyết nhu cầu ngày càng tăng để đánh giá tính chính xác sự thật của LLM
bằng cách tuyển chọn một benchmark QA mới, FRESH QA, với 600 câu hỏi bao gồm một phổ rộng các
loại câu hỏi và câu trả lời.

2.1 THU THẬP DỮ LIỆU
Chúng tôi thu thập FRESH QA bằng cách tuyển dụng cả các nhà nghiên cứu NLP (bao gồm các tác giả và
đồng nghiệp của họ) và các freelancer trực tuyến3 để viết các câu hỏi với các mức độ khó và chủ đề khác nhau
có câu trả lời có thể thay đổi dựa trên các phát triển mới trong thế giới. Các người chú thích được hiển thị
một vài ví dụ về bốn loại câu hỏi rộng được định nghĩa trong Hình 1. Trong mỗi bốn danh mục này,
chúng tôi yêu cầu người chú thích viết câu hỏi ở hai mức độ khó khác nhau: một bước, trong đó câu hỏi
đề cập rõ ràng tất cả thông tin liên quan cần thiết để trả lời nó, và do đó không cần lý luận bổ sung
(ví dụ, "Ai là CEO của Twitter"); và nhiều bước, trong đó câu hỏi yêu cầu một hoặc nhiều bước lý luận
bổ sung để thu thập tất cả thông tin liên quan cần thiết để trả lời nó (ví dụ, "Tổng chiều cao của tòa nhà
cao nhất thế giới là bao nhiêu?"). Người chú thích được khuyến khích viết các câu hỏi liên quan đến kiến thức
mới (kiến thức đã thay đổi gần đây hoặc các sự kiện mới) và xuất hiện tự nhiên (tức là, hợp lý cho một
người thực sự gõ vào công cụ tìm kiếm). Đối với các câu hỏi tiền đề sai, chúng tôi yêu cầu một lời giải thích
ngắn gọn làm rõ tại sao câu hỏi có lỗi.4

Kiểm soát chất lượng: Sau khi thu được tập dữ liệu ban đầu, chúng tôi tiến hành nhiều đánh giá chất lượng
và làm sạch dữ liệu kỹ lưỡng. Điều này bao gồm xem xét thủ công từng ví dụ để đảm bảo các câu hỏi
được tạo tốt, loại bỏ trùng lặp và câu hỏi không hợp lệ (ví dụ, quá dễ hoặc gây tranh cãi), và xác minh
các câu trả lời và URL bằng chứng hỗ trợ. Chúng tôi cũng thu thập thủ công các câu trả lời hợp lệ bổ sung
cho mỗi câu hỏi (ví dụ, các tên khác nhau của cùng một người, các định dạng ngày tháng khác nhau, v.v.).
Để tạo điều kiện cho các cập nhật câu trả lời trong tương lai, chúng tôi loại trừ các câu hỏi có câu trả lời
có khả năng thay đổi thường xuyên hơn một lần mỗi tuần, và ngoài ra kết hợp ngày xem xét tiếp theo
dự kiến cho mỗi câu hỏi.

Kích thước và chia dữ liệu: Tập dữ liệu kết quả được chia thành một testset bao gồm 125 câu hỏi cho
mỗi bốn loại câu hỏi rộng (tổng cộng 500 ví dụ) và một development set bao gồm 25 câu hỏi cho mỗi
loại câu hỏi (tổng cộng 100 ví dụ), được lấy mẫu ngẫu nhiên trong các loại. Ngoài ra, 15 ví dụ bao gồm
các loại câu hỏi khác nhau được trích xuất cho mục đích minh chứng (tức là, để sử dụng trong học
in-context few-shot), và dữ liệu còn lại được loại bỏ. Development set được dành riêng cho các nghiên cứu
tương lai và không được sử dụng trong bài báo này.5

FRESH QA yêu cầu cập nhật thường xuyên: Tập dữ liệu của chúng tôi có tính nhạy cảm thời gian vì
các câu trả lời chuẩn có thể thay đổi với các phát triển mới trong thế giới. Do đó, chúng tôi cam kết
cập nhật tập dữ liệu thường xuyên và khuyến khích các nhà nghiên cứu đánh giá trên phiên bản mới nhất
của tập dữ liệu, càng gần với ngày phát hành của tập dữ liệu cập nhật càng tốt.

3Chúng tôi sử dụng UPWORK (https://www.upwork.com) với mức bồi thường $2 mỗi ví dụ.
4Ngoài ra, người chú thích được yêu cầu bao gồm năm mà câu trả lời cho câu hỏi đã thay đổi lần cuối và một
URL đến một trang web có uy tín hỗ trợ câu trả lời.
5Mặc dù test set của chúng tôi hiện tại được cân bằng qua các loại câu hỏi, phân phối có thể thay đổi theo thời gian
do việc phân loại lại các câu hỏi từ danh mục này sang danh mục khác.

--- TRANG 4 ---
Bản thảo

2.2 ĐÁNH GIÁ
Tất cả các phản hồi của mô hình được đánh giá bởi các tác giả trong một quy trình đánh giá hai chế độ:
RELAXED, tập trung chỉ vào việc đánh giá tính đúng đắn của câu trả lời chính; và STRICT, ngoài ra còn
kiểm tra liệu tất cả các sự thật trong câu trả lời có chính xác không (tức là, không có ảo giác). Nhìn chung,
thiết lập của chúng tôi cung cấp cả hai đầu của phổ để đánh giá tính chính xác sự thật (sự khác biệt giữa
hiệu suất strict và relaxed của mô hình cung cấp một cách để đo ảo giác), mang lại sự hiểu biết toàn diện
và sắc thái hơn về hiệu suất của chúng.

Giao thức đánh giá: Trong cả hai chế độ đánh giá, chúng tôi chỉ ghi nhận phản hồi của mô hình nếu nó
cung cấp một câu trả lời tự tin và dứt khoát, hoặc câu trả lời đúng có thể được suy ra rõ ràng từ phản hồi.
Câu trả lời chính hoặc cuối cùng khi đứng một mình phải chính xác. Bất kỳ thông tin bổ sung nào được
cung cấp không được mâu thuẫn với câu trả lời chính hoặc định hình lại nhận thức của người ta về nó.
Đối với các câu hỏi tiền đề sai, mô hình phải chỉ ra sự hiện diện của tiền đề sai để nhận được điểm.
Đối với các câu trả lời liên quan đến tên của các thực thể (ví dụ, người), các tên đầy đủ hoặc tên được
công nhận rộng rãi được mong đợi. Về các câu trả lời số, các số gần đúng thường không được chấp nhận
trừ khi được bao gồm rõ ràng trong các câu trả lời chuẩn. Dưới RELAXED, chúng tôi chấp nhận các phản hồi
sai định dạng (bao gồm những phản hồi bằng ngôn ngữ không phải tiếng Anh), cũng như thông tin ảo giác
hoặc lỗi thời không ảnh hưởng đáng kể đến câu trả lời chính. Dưới STRICT, tuy nhiên, một phản hồi chứa
bất kỳ ảo giác nào, dù nhỏ đến đâu, sẽ không nhận được điểm. Hơn nữa, chúng tôi chấp nhận một phản hồi
trong STRICT khi mô hình chỉ ra rằng thông tin có thể đã lỗi thời (ví dụ, "Tính đến ngày cắt kiến thức của tôi
vào tháng 9 năm 2021") chỉ khi rõ ràng rằng kiến thức đã không thay đổi.6 Hình 4 trong Phụ lục A hiển thị
các ví dụ cụ thể của mỗi tiêu chí đánh giá.

Thỏa thuận giữa các người đánh giá và đánh giá tự động: Hai tác giả độc lập đánh giá một tập con
100 câu trả lời trong cả hai chế độ và có thỏa thuận 99% cho RELAXED và 96% cho STRICT, cho thấy
giao thức đáng tin cậy để so sánh các LLM khác nhau. Ngoài ra, để tạo điều kiện cho các đánh giá tương lai,
chúng tôi phát triển FRESH EVAL, một metric tự động đơn giản sử dụng học in-context few-shot để dạy
LLM đánh giá các phản hồi của mô hình, đạt được thỏa thuận trung bình 96.5% với đánh giá con người
cho RELAXED và 96% cho STRICT. Xem Phụ lục B để biết chi tiết.

3 CÁC LLM ĐÃ ĐƯỢC HUẤN LUYỆN TRƯỚC GẶP KHÓ KHĂN TRÊN FRESH QA
Chúng tôi sử dụng FRESH QA để đo lường các LLM không có quyền truy cập vào dữ liệu thời gian thực
hoặc khả năng duyệt Internet để biết thông tin hiện tại.7 Trong khi tất cả LLM (bất kể kích thước) có thể
dự đoán gặp khó khăn với các câu hỏi yêu cầu kiến thức cập nhật, chúng cũng hoạt động kém trên các
câu hỏi tiền đề sai. Trong các thí nghiệm của chúng tôi, chúng tôi chỉ đơn giản đưa các câu hỏi riêng lẻ
làm prompt vào mỗi mô hình và decode các dự đoán của mô hình sử dụng nhiệt độ 0 mà không fine-tuning
(xem Phụ lục C để biết thêm chi tiết).

Baseline: Chúng tôi thí nghiệm với một loạt mô hình khác nhau về kích thước từ 770M đến 540B tham số,
bao gồm các mô hình được huấn luyện trước cơ bản như T5 (Raffel et al., 2020; Lester et al., 2021),
PALM và PALM CHILLA (Chowdhery et al., 2022), tùy chọn sử dụng prompting FEW-SHOT (Brown et al., 2020)
và Chain-of-Thought (COT, Wei et al., 2022);8 các mô hình được điều chỉnh instruction bao gồm FLAN-T5
và FLAN-PALM (Chung et al., 2022; Longpre et al., 2023), và GPT-3.5 (Ouyang et al., 2022),
CODEX (Chen et al., 2021a), CHATGPT, và GPT-4 (OpenAI, 2023) của OPENAI.

3.1 KẾT QUẢ VÀ THẢO LUẬN
FRESH QA trình bày một thách thức cho LLM: Chúng tôi hiển thị độ chính xác của các LLM khác nhau
trên FRESH QA trong cả hai chế độ đánh giá trong Hình 2.9 Một điều hiển nhiên đầu tiên là tất cả các mô hình
đều gặp khó khăn

6Lưu ý rằng ngay cả khi không có quyền truy cập vào dữ liệu thời gian thực, một mô hình vẫn có thể cung cấp
câu trả lời chính xác cho một số câu hỏi liên quan đến thông tin hiện tại, có thể thông qua đoán ngẫu nhiên
hoặc bằng cách tận dụng các phản hồi hợp lệ trước đây (ví dụ, đối với câu hỏi "Series phim truyền hình nào
đã giành Giải Emmy Primetime gần đây nhất cho Outstanding Drama Series?", trong khi "Succession" đã thắng
giải gần đây nhất (tính đến thời điểm viết này), nó cũng là người chiến thắng vào năm 2020, vì vậy một mô hình
được huấn luyện vào năm 2021 có thể cung cấp câu trả lời đúng).
7Ngoại trừ CHATGPT và GPT-4, có quyền truy cập vào ngày hiện tại. Lưu ý rằng các phiên bản mới nhất của
những mô hình này hiện có thể duyệt Internet.
8Vì chúng tôi quan tâm đến việc khám phá cách những phương pháp này hoạt động mà không được thiết kế
cụ thể cho FRESH QA, chúng tôi sử dụng các minh chứng 5-shot cho TRIVIA QA (Joshi et al., 2017) được
sử dụng trong Sun et al. (2023).
9Bảng 3 và Bảng 4 trong Phụ lục D chứa các số cụ thể dưới STRICT và RELAXED, tương ứng.

--- TRANG 5 ---
Bản thảo
Tổng thểCâu hỏi thay đổi nhanh Câu hỏi tiền đề sai

Hình 2: Độ chính xác của các LLM khác nhau trên FRESH QA dưới đánh giá RELAXED và STRICT (không có ảo giác).
Các mô hình được đo lường cùng ngày 26 tháng 4 năm 2023. Tất cả các mô hình (bất kể kích thước mô hình)
đều gặp khó khăn với các câu hỏi liên quan đến kiến thức thay đổi nhanh và tiền đề sai.

trên FRESH QA: độ chính xác tổng thể dao động từ 0.8% đến 32.0% dưới STRICT, và 0.8% đến 46.4%
dưới RELAXED. Chuyển từ RELAXED sang STRICT dẫn đến giảm đáng kể độ chính xác cho CHATGPT và GPT-4.
Điều này chủ yếu do thiếu quyền truy cập vào thông tin cập nhật, khi chúng tạo ra các câu trả lời "lỗi thời"
(thường bắt đầu với tiền tố "Tính đến ngày cắt kiến thức của tôi vào tháng 9 năm 2021"), và trong nhiều
trường hợp, "từ chối" cung cấp câu trả lời (ví dụ, "Là một mô hình ngôn ngữ AI, tôi không thể cung cấp
thông tin thời gian thực."). Tương tự, độ chính xác của PALM (qua các kích thước mô hình) giảm đáng kể
dưới STRICT. Phần lớn sự giảm này là do các artifact như phản hồi giống cuộc trò chuyện với các token
đặc biệt không mong đợi (ví dụ, end-of-turn [eot]), và ảo giác. Ngược lại, FLAN-PALM và CODEX thể hiện
ảo giác tối thiểu do các câu trả lời ngắn gọn và trực tiếp của chúng.

LLM gặp khó khăn với các câu hỏi về thông tin hiện tại: Việc thiếu kiến thức tham số cập nhật dẫn đến
độ chính xác giảm đáng kể qua các mô hình trên các câu hỏi liên quan đến kiến thức thay đổi nhanh hoặc
gần đây. GPT-4 thường đạt được độ chính xác cao nhất trên những câu hỏi này, ngoại trừ các câu hỏi về
kiến thức gần đây (tức là, từ 2022) dưới STRICT nơi nó hoạt động kém hơn FLAN-PALM và CODEX,
nhưng nó không bao giờ vượt quá 15% qua cả hai chế độ đánh giá. Đánh giá của chúng tôi xác nhận rằng
CHATGPT và GPT-4 đã được tiếp xúc với dữ liệu chứa thông tin vượt quá ngày cắt kiến thức của chúng
(Phụ lục E). Ngoài ra, GPT-4 miễn cưỡng hơn trong việc trả lời các câu hỏi thay đổi nhanh (từ chối trả lời
60% thời gian) so với CHATGPT (16%).

Các câu hỏi với tiền đề sai tạo ra rào cản cho LLM: Tất cả các mô hình đều gặp khó khăn với các câu hỏi
có tiền đề sai, và việc sử dụng các mô hình lớn hơn không tăng độ chính xác cho T5 và PALM ("tỷ lệ phẳng"),
với hiệu suất trong phạm vi từ 0.0% đến 1.6%. GPT-3.5, CHATGPT, và GPT-4 thể hiện độ chính xác vượt trội
hơn nhiều so với tất cả các mô hình khác, đạt được độ chính xác từ 25.8% đến 42.7% dưới STRICT và 32.3%
đến 66.9% dưới RELAXED. CHATGPT hoạt động tốt nhất dưới STRICT (42.7%) trong khi GPT-4 là mô hình
chính xác nhất dưới RELAXED (66.9%), với độ chính xác ấn tượng 83.9% trên các câu hỏi về kiến thức
trước 2022. Những kết quả này cho thấy các mô hình của OPENAI có khả năng được huấn luyện để đối phó
với các câu hỏi tiền đề sai.

COT tăng ảo giác: Nhìn chung, prompting FEW-SHOT và COT có lợi cho các mô hình lớn và đôi khi có lợi
cho các mô hình cỡ trung bình trên các câu hỏi với tiền đề hợp lệ, đặc biệt trên các câu hỏi về kiến thức
không bao giờ thay đổi hoặc kiến thức cũ. Dưới STRICT, FEW-SHOT và COT mang lại cải thiện độ chính xác
tương ứng +36.1% và +26.9% so với prompting zero-shot với PALM 540B trên các câu hỏi liên quan đến
kiến thức trước 2022 (+21.9% và +29.7% dưới RELAXED). COT chủ yếu thể hiện hiệu suất vượt trội so với
FEW-SHOT dưới RELAXED, trong khi FEW-SHOT đạt được kết quả tốt hơn dưới STRICT, vì COT giới thiệu
thêm chỗ cho ảo giác.

Lý luận nhiều bước là thách thức đối với một số mô hình: T5 LARGE và XL không có khả năng xử lý các
câu hỏi nhiều bước, trong khi FLAN-PALM 540B, CODEX, và GPT-3.5 chịu thiệt hại nhiều nhất khi chuyển
từ câu hỏi một bước sang nhiều bước. GPT-4 vẫn ổn định qua hai loại câu hỏi này (với sự khác biệt ít hơn
2% về độ chính xác qua các thiết lập). Xem Phụ lục D để biết chi tiết.

4 PROMPTING CÁC MÔ HÌNH NGÔN NGỮ TĂNG CƯỜNG CÔNG CỤ TÌM KIẾM
Độ chính xác thấp được báo cáo trong phần trước phần lớn không ngạc nhiên, vì không có mô hình nào
chúng tôi đánh giá có quyền truy cập vào thông tin thời gian thực. Trong phần này, chúng tôi đánh giá
tác động của tăng cường công cụ tìm kiếm

--- TRANG 6 ---
Bản thảo

{source_webpage} 
{publication_date} 
{title} 
{text_snippet} 

{highlighted_words} 
{demonstrations}   # chi tiết được bỏ qua vì ngắn gọn 
{question} 
{retrieved_evidences}   # thứ tự thời gian 
câu hỏi: {question} 
câu trả lời: {reasoning_and_answer} 

Hình 3: Định dạng của FRESH PROMPT. Chúng tôi đưa tất cả các bằng chứng được lấy vào một định dạng
thống nhất với thông tin hữu ích, bao gồm trang web nguồn, ngày, tiêu đề, đoạn văn bản, và từ được
làm nổi bật (trái). Các minh chứng few-shot được cung cấp ở đầu prompt. Mỗi minh chứng cho mô hình
thấy một câu hỏi ví dụ và danh sách các bằng chứng được lấy cho câu hỏi, theo sau là một số lý luận
trên các bằng chứng để tìm ra câu trả lời liên quan và cập nhật nhất (phải).

đến LLM trên FRESH QA. Chúng tôi trình bày FRESH PROMPT, một phương pháp prompting few-shot đơn giản
làm tăng đáng kể hiệu suất FRESH QA của LLM bằng cách kết hợp thông tin liên quan và cập nhật được lấy
từ công cụ tìm kiếm (GOOGLE SEARCH) vào prompt.

4.1 FRESH PROMPT
Phương pháp FRESH PROMPT của chúng tôi tận dụng một text prompt để (1) giới thiệu thông tin liên quan
theo ngữ cảnh và cập nhật (bao gồm câu trả lời cho các câu hỏi liên quan) từ công cụ tìm kiếm đến LLM
được huấn luyện trước, và (2) dạy mô hình lý luận trên các bằng chứng được lấy. Cụ thể hơn, với một
câu hỏi q, trước tiên chúng tôi sử dụng q nguyên văn để truy vấn công cụ tìm kiếm, trong trường hợp
của chúng tôi là GOOGLE SEARCH.10 Chúng tôi lấy tất cả kết quả tìm kiếm, bao gồm answer box,
organic results, và thông tin hữu ích khác, như knowledge graph, câu hỏi và câu trả lời từ các nền tảng
QA crowdsourced, và các câu hỏi liên quan mà người dùng tìm kiếm cũng hỏi (xem Hình 9 trong Phụ lục F).
Đối với mỗi kết quả này, chúng tôi trích xuất đoạn văn bản liên quan x cùng với thông tin khác, như
nguồn s (ví dụ, WIKIPEDIA), ngày d, tiêu đề t, từ được làm nổi bật h, và sau đó tạo một danh sách
k bằng chứng được lấy E={(s,d,t,x,h)}.

Những bằng chứng này sau đó được đưa vào một định dạng chung (Hình 3, trái) và được sử dụng để
điều kiện hóa mô hình thông qua học in-context. Để khuyến khích mô hình tập trung vào các bằng chứng
gần đây hơn, phù hợp với các phát hiện gần đây (Liu et al., 2023a), chúng tôi sắp xếp các bằng chứng E
trong prompt từ cũ nhất đến mới nhất.

Để giúp mô hình "hiểu" nhiệm vụ và đầu ra mong muốn, chúng tôi cung cấp các minh chứng few-shot
của các ví dụ input-output ở đầu input prompt. Mỗi minh chứng cho mô hình thấy một câu hỏi ví dụ và
danh sách các bằng chứng được lấy cho câu hỏi, theo sau là lý luận chain-of-thought trên các bằng chứng
để tìm ra câu trả lời liên quan và cập nhật nhất (Hình 3, phải). Mặc dù chúng tôi bao gồm một vài ví dụ
về các câu hỏi với tiền đề sai trong các minh chứng, chúng tôi cũng thí nghiệm với việc kiểm tra tiền đề
sai rõ ràng trong prompt: "Vui lòng kiểm tra xem câu hỏi có chứa tiền đề hợp lệ trước khi trả lời".
Hình 10 trong Phụ lục G hiển thị một prompt thực tế.

4.2 THIẾT LẬP THÍ NGHIỆM
Chúng tôi tuân theo chặt chẽ thiết lập trong Phần 3 ngoại trừ các trường hợp chúng tôi thiếu kiểm soát
đối với việc decode của mô hình thông qua API (ví dụ, PERPLEXITY.AI). Một số mô hình chúng tôi đánh giá
có thể thay đổi theo thời gian, điều này tạo ra thách thức cho tính tái tạo của kết quả đánh giá của chúng tôi;
do đó, chúng tôi đánh giá tất cả các mô hình cùng ngày 26 tháng 4 năm 2023. Ngoài GPT-3.5 và GPT-4,
chúng tôi đánh giá GOOGLE SEARCH bằng cách đơn giản truy vấn GOOGLE SEARCH và sử dụng câu trả lời
trong answer box (nếu có) hoặc đoạn văn bản của kết quả tìm kiếm đầu tiên; PERPLEXITY.AI (PPLX.AI),
một answer engine kết hợp LLM và công cụ tìm kiếm để tạo ra các phản hồi hữu ích cho truy vấn của
người dùng;11 và SELF-ASK (Press et al., 2022), một phương pháp sử dụng học in-context few-shot để
dạy LLM phân tách mỗi câu hỏi thành các câu hỏi phụ đơn giản hơn được trả lời thông qua GOOGLE SEARCH.12

Thiết lập FRESH PROMPT: Chúng tôi áp dụng FRESH PROMPT cho cả GPT-3.5 và GPT-4 bằng cách tuần tự
kết hợp các bằng chứng được lấy sau đây vào input prompt: o kết quả tìm kiếm organic, r

10Chúng tôi scrape các kết quả từ GOOGLE SEARCH sử dụng SERPAPI (https://serpapi.com).
11https://www.perplexity.ai. Tại thời điểm đánh giá, PPLX.AI là sự kết hợp của GPT-3.5 và BING SEARCH,
và có thể cung cấp cả câu trả lời ngắn gọn và chi tiết. Chúng tôi đánh giá các câu trả lời ngắn gọn của nó.
12Chúng tôi sử dụng prompt few-shot được cung cấp bởi các tác giả của SELF-ASK và áp dụng nó cho cả GPT-3.5
và GPT-4. Để đơn giản, chúng tôi chỉ đánh giá câu trả lời cuối cùng từ SELF-ASK, bỏ qua các câu trả lời trung gian.

--- TRANG 7 ---
Bản thảo

Bảng 1: Độ chính xác của các LLM tăng cường công cụ tìm kiếm khác nhau trên FRESH QA dưới đánh giá
STRICT (không có ảo giác). Các mô hình được đo lường cùng ngày 26 tháng 4 năm 2023. Chúng tôi báo cáo
độ chính xác qua các danh mục câu hỏi khác nhau, bao gồm thay đổi nhanh (fast), thay đổi chậm (slow),
không bao giờ thay đổi (never), tiền đề sai, các câu hỏi liên quan đến kiến thức trước 2022 (<2022) và
từ 2022 (≥2022), câu hỏi một bước (1-hop) và nhiều bước (m-hop). + chỉ ra mô hình có quyền truy cập
vào ngày hiện tại. UTD viết tắt của "up-to-date".

Mô hình (kích thước) ngày cắt kiến thức tất cả tiền đề hợp lệ tiền đề sai
tất cả fast slow never <2022 ≥2022 1-hop m-hop tất cả <2022

so sánh với baseline
GOOGLE SEARCH (N/A) UTD 39.6 48.9 32.0 46.4 68.3 67.4 37.9 55.6 32.4 11.3 9.7
GPT-3.5 (N/A) 2021 26.0 26.1 4.0 15.2 58.7 61.0 5.1 28.0 21.3 25.8 34.4
GPT-3.5 + SELF-ASK(N/A) UTD 41.6 51.1 36.8 43.2 73.0 73.8 37.4 52.2 48.1 12.9 17.2
GPT-3.5 + FRESH PROMPT UTD 56.0 62.5 46.4 60.8 80.2 71.6 57.0 68.7 47.2 36.3 43.0
PPLX.AI (N/A) UTD 52.2 57.2 38.4 53.6 79.4 73.0 47.7 63.8 40.7 37.1 38.7
GPT-4 (N/A) 2021+ 28.6 26.9 12.0 4.0 64.3 58.2 8.1 27.2 25.9 33.9 41.9
GPT-4 + SELF-ASK(N/A) UTD 47.8 47.1 39.2 46.4 55.6 51.8 44.3 43.7 55.6 50.0 61.3
GPT-4 + FRESH PROMPT UTD 75.6 77.1 59.2 77.6 94.4 88.7 70.2 81.3 66.7 71.0 77.4

nghiên cứu độ nhạy và ablation
GPT-3.5 (N/A) 2021 26.0 26.1 4.0 15.2 58.7 61.0 5.1 28.0 21.3 25.8 34.4
GPT-3.5 + FRESH PROMPT UTD 56.0 62.5 46.4 60.8 80.2 71.6 57.0 68.7 47.2 36.3 43.0
w/PREMISE CHECK UTD 35.2 27.1 14.4 28.0 38.9 36.2 21.7 31.0 17.6 59.7 67.7
GPT-4 (N/A) 2021+ 28.6 26.9 12.0 4.0 64.3 58.2 8.1 27.2 25.9 33.9 41.9
GPT-4 w/ SNIPPETS ONLY & SEARCH ORDER UTD 74.0 75.5 56.8 75.2 94.4 87.9 68.1 79.9 64.8 69.4 77.4
GPT-4 w/ SNIPPETS ONLY & TIME ORDER UTD 74.8 75.5 58.4 74.4 93.7 87.9 68.1 79.9 64.8 72.6 82.8
GPT-4 w/ SNIPPETS ONLY & RANDOM ORDER UTD 72.4 73.7 56.8 69.6 94.4 87.9 65.1 78.4 62.0 68.5 76.3
GPT-4 + FRESH PROMPT UTD 75.6 77.1 59.2 77.6 94.4 88.7 70.2 81.3 66.7 71.0 77.4
w/PREMISE CHECK UTD 75.0 74.2 56.8 76.0 89.7 85.1 67.7 79.5 61.1 77.4 79.6
w/o ANSWER BOX UTD 74.2 74.7 57.6 74.4 92.1 88.7 66.4 79.1 63.9 72.6 78.5
w/o ANSWER BOX & RELEVANT INFO UTD 72.4 72.9 54.4 71.2 92.9 87.2 64.3 78.0 60.2 71.0 78.5
w/ 1 EVIDENCE UTD 61.4 60.9 40.0 55.2 87.3 79.4 49.8 66.8 46.3 62.9 75.3
w/ 5 EVIDENCES UTD 70.6 72.1 56.0 69.6 90.5 81.6 66.4 78.0 57.4 66.1 73.1
w/ 15 EVIDENCES UTD 77.6 78.5 60.8 78.4 96.0 88.7 72.3 81.7 70.4 75.0 80.6
w/ 15 DEMONSTRATIONS UTD 74.6 75.5 56.8 76.0 93.7 87.9 68.1 79.9 64.8 71.8 76.3
w/LONG DEMONSTRATION ANSWERS UTD 73.0 72.6 55.2 71.2 91.3 83.7 66.0 77.6 60.2 74.2 81.7

các câu hỏi liên quan mà người dùng tìm kiếm cũng hỏi, a câu hỏi và câu trả lời từ các nền tảng QA
crowdsourced, và các đoạn từ knowledge graph và answer box (nếu có). Những bằng chứng này được
sắp xếp theo trình tự lên đến cuối prompt. Với giới hạn ngữ cảnh của các mô hình, chúng tôi chỉ giữ
n bằng chứng hàng đầu (gần cuối prompt hơn) sau khi sắp xếp chúng dựa trên ngày tương ứng. Trừ khi
được chỉ định khác, chúng tôi sử dụng (o,r,a,n,m) = (10,2,2,5) cho GPT-3.5, và (o,r,a,n,m) = (10,3,3,10)
cho GPT-4. Ngoài ra, chúng tôi bao gồm m = 5 minh chứng câu hỏi-câu trả lời ở đầu prompt.

4.3 KẾT QUẢ VÀ THẢO LUẬN
FRESH PROMPT cải thiện đáng kể độ chính xác FRESH QA: Bảng 1 trình bày các số cụ thể dưới STRICT
(xem Phụ lục H cho kết quả dưới RELAXED). FRESH PROMPT mang lại cải thiện lớn so với GPT-3.5 và GPT-4
vanilla trên toàn bộ. GPT-4 + FRESH PROMPT đạt được cải thiện độ chính xác tuyệt đối 47% và 31.4% so với
GPT-4 dưới STRICT và RELAXED, tương ứng. Việc giảm khoảng cách độ chính xác tuyệt đối giữa STRICT và
RELAXED (từ 17.8% xuống 2.2%) cũng cho thấy FRESH PROMPT làm giảm đáng kể sự hiện diện của các câu trả lời
lỗi thời và ảo giác. Không ngạc nhiên, những cải thiện đáng kể nhất cho cả GPT-3.5 và GPT-4 là trên các
danh mục câu hỏi thay đổi nhanh và thay đổi chậm, cả hai đều liên quan đến kiến thức gần đây. Điều đó
nói rằng, các câu hỏi về kiến thức cũ cũng hưởng lợi từ FRESH PROMPT. Ví dụ, GPT-4 + FRESH PROMPT
mang lại độ chính xác cao hơn +30.5% so với GPT-4 trên các câu hỏi với tiền đề hợp lệ liên quan đến kiến thức
trước 2022 (+9.9% dưới RELAXED). Ngoài ra, FRESH PROMPT tạo ra lợi ích đáng chú ý trên các câu hỏi tiền đề sai
(+37.1% và +8.1% cải thiện độ chính xác tương ứng dưới STRICT và RELAXED cho GPT-4).

FRESH PROMPT vượt trội hơn các phương pháp tăng cường tìm kiếm khác với biên độ lớn: GPT-4 +
FRESH PROMPT thể hiện độ chính xác vượt trội qua các loại câu hỏi, vượt qua tất cả các phương pháp khác
với biên độ đáng kể. Biến thể tốt nhất của nó (với 15 bằng chứng được lấy mỗi câu hỏi) đạt được độ chính xác
tổng thể ấn tượng 77.6% và 79.0% dưới STRICT và RELAXED, tương ứng. GPT-3.5 + FRESH-PROMPT vượt qua
PPLX.AI và SELF-ASK (tất cả được thực hiện trên GPT-3.5) về độ chính xác tổng thể +3.8% và +14.4% dưới STRICT.
Dưới RELAXED, tuy nhiên, PPLX.AI đạt được độ chính xác tổng thể cao hơn +4.2% so với GPT-3.5 + FRESH PROMPT,
phần lớn do độ chính xác vượt trội của nó trên các câu hỏi tiền đề sai (58.1% so với 41.1%). Khoảng cách
độ chính xác lớn 14.0% giữa STRICT và RELAXED cho PPLX.AI cho thấy đầu ra của nó chứa một lượng lớn ảo giác.
Nhìn chung, tất cả các phương pháp tăng cường công cụ tìm kiếm (SELF-ASK, PPLX.AI, và FRESH PROMPT) cung cấp
lợi ích đáng kể qua các loại câu hỏi so với GPT-3.5 và GPT-4 vanilla. GOOGLE SEARCH thường cung cấp kết quả
tốt hơn cả GPT-3.5 và GPT-4, ngoại trừ trên các câu hỏi với tiền đề sai, nhưng tụt hậu xa so với PPLX.AI và
GPT-3.5/GPT-4 + FRESH PROMPT trên toàn bộ.

Kiểm tra tiền đề tăng độ chính xác trên các câu hỏi tiền đề sai nhưng có thể làm tổn hại độ chính xác
trên những câu hỏi với tiền đề hợp lệ: Như đã thảo luận trong Phần 3.1, các LLM của OPENAI như GPT-3.5
và GPT-4 có khả năng được điều chỉnh để xử lý các câu hỏi tiền đề sai, và điều này cũng đúng cho PPLX.AI.
Ngoài ra, chúng tôi tìm thấy thực nghiệm rằng một số LLM có khả năng bác bỏ một câu hỏi tiền đề sai nếu
được yêu cầu rõ ràng, ví dụ "Vui lòng kiểm tra xem câu hỏi có chứa tiền đề hợp lệ trước khi trả lời".
Thêm kiểm tra tiền đề này vào GPT-3.5 và GPT-4 mang lại cải thiện độ chính xác tương ứng +23.4% và +6.4%
trên các câu hỏi tiền đề sai dưới STRICT (+22.6% và +11.3% dưới RELAXED). Tuy nhiên, điều này có hại
cho GPT-3.5 đối với các loại câu hỏi khác, làm giảm độ chính xác tổng thể 20.8% và 21% dưới STRICT và
RELAXED, tương ứng. Điều này không phải là vấn đề đối với GPT-4, với sự giảm nhẹ 0.6% dưới STRICT
và tăng nhẹ 1.2% dưới RELAXED.

Có nhiều bằng chứng liên quan và cập nhật ở cuối ngữ cảnh đầu vào là hữu ích: Chúng tôi cũng phân tích
cách thứ tự của các bằng chứng trong prompt ảnh hưởng đến độ chính xác của GPT-4. Kết quả của chúng tôi
cho thấy việc sử dụng thứ tự được trả về bởi GOOGLE SEARCH (SEARCH ORDER, kết quả tìm kiếm hàng đầu
ở cuối ngữ cảnh đầu vào) hoặc sắp xếp các bằng chứng theo thông tin ngày liên quan của chúng (TIME ORDER,
kết quả gần đây hơn ở cuối) thường dẫn đến độ chính xác tốt hơn so với việc sử dụng thứ tự ngẫu nhiên
(RANDOM ORDER), với độ chính xác tổng thể cao hơn lên đến +2.2% trong STRICT và RELAXED. Việc chỉ sử dụng
đoạn văn bản cho mỗi bằng chứng mà không có thông tin bổ sung (như nguồn, ngày, v.v.) như trong
GPT-4 + FRESH PROMPT làm giảm nhẹ độ chính xác, với ít hơn 1% trong cả hai thiết lập.

Thông tin được lấy bổ sung ngoài các kết quả tìm kiếm organic cung cấp lợi ích thêm: Việc kết hợp
các bằng chứng được lấy bổ sung khác ngoài organic search results, như answer box hoặc các câu hỏi liên quan
mà người dùng tìm kiếm cũng hỏi, là hữu ích. Loại bỏ answer box làm giảm độ chính xác tổng thể của
GPT-4 + FRESH PROMPT dưới STRICT 1.4% (1.6% dưới RELAXED). Loại bỏ cả answer box và thông tin liên quan
khác (bao gồm các câu hỏi liên quan) làm giảm độ chính xác tổng thể của GPT-4 + FRESH PROMPT 3.2% (3.0% dưới RELAXED).

Tăng số lượng bằng chứng được lấy cải thiện thêm FRESH PROMPT: Chúng tôi khám phá hiệu ứng của số lượng
bằng chứng được lấy cho mỗi câu hỏi cũng như số lượng minh chứng bằng cách thay đổi những số này trong
các thí nghiệm của chúng tôi với GPT-4. Lưu ý rằng thiết lập mặc định của chúng tôi cho GPT-4 + FRESH PROMPT
sử dụng 10 bằng chứng được lấy cho mỗi câu hỏi và 5 minh chứng. Kết quả của chúng tôi cho thấy số lượng
bằng chứng được lấy cho mỗi câu hỏi là thành phần quan trọng nhất để đạt được độ chính xác cao nhất.
Dưới STRICT, tăng số này từ 1 lên 5, 10, và 15 dẫn đến cải thiện độ chính xác tổng thể tương ứng +9.2%,
+14.2%, và +16.2%. Điều này cho thấy GPT-4 có thể xử lý hiệu quả số lượng bằng chứng được lấy ngày càng
tăng (bao gồm các câu trả lời mâu thuẫn) và căn cứ các phản hồi của nó vào thông tin chính xác và cập nhật nhất.
Mặt khác, tăng số lượng minh chứng từ 5 lên 15 làm tổn hại nhẹ độ chính xác trong cả hai thiết lập đánh giá
(giảm 1% độ chính xác tổng thể dưới STRICT).

Minh chứng dài dòng cải thiện trên các câu hỏi phức tạp nhưng cũng tăng ảo giác: Để đánh giá hiệu ứng
của phong cách viết của câu trả lời (bao gồm lý luận) trong mỗi minh chứng, chúng tôi viết lại thủ công
những câu trả lời này thành phiên bản dài dòng hơn (LONG DEMONSTRATION ANSWERS). Kiểm tra thủ công
của chúng tôi tiết lộ rằng việc sử dụng câu trả lời minh chứng dài dòng hơn có thể hữu ích khi xử lý các
câu hỏi phức tạp nhưng có thể có hại hơn vì nó cung cấp chỗ cho ảo giác (giảm 2.6% độ chính xác tổng thể dưới STRICT).

5 CÔNG VIỆC LIÊN QUAN
LLM tăng cường kiến thức: Nhiều công trình trước đây nghiên cứu tăng cường kiến thức bán tham số
trong LLM thông qua fine-tuning bổ sung (Guu et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022;
Izacard et al., 2022), trong khi những công trình khác ủng hộ việc tạo kiến thức thay vì truy xuất (Yu et al.,
2023a; Sun et al., 2023). FRESH PROMPT phù hợp với xu hướng mới nổi gần đây trong các ứng dụng QA
tăng cường prompt của LLM bằng kiến thức được truy xuất từ các công cụ tìm kiếm để căn chỉnh thời gian thực
với thông tin hiện tại và chính xác sự thật (Nakano et al., 2021; Lazaridou et al., 2022; Menick et al., 2022; Yao

--- TRANG 8 ---
Bản thảo

et al., 2022; Press et al., 2022; Khattab et al., 2022; Schick et al., 2023; Luo et al., 2023). Tương tự như
phương pháp của chúng tôi, Lazaridou et al. (2022) đề xuất một phương pháp học in-context few-shot chèn
tài liệu từ GOOGLE SEARCH vào prompt. Chúng tôi không so sánh với phương pháp này do chi phí suy luận
đắt đỏ của nó, vì nó chia nhỏ các tài liệu được truy xuất thành các đoạn bằng chứng và thực hiện k = 50
lời gọi suy luận đến LLM để tạo k câu trả lời theo sau bằng việc xếp hạng lại LLM. Ngược lại, FRESH PROMPT
chỉ thực hiện một lời gọi suy luận duy nhất đến LLM. SELF-ASK (Press et al., 2022) cũng sử dụng học
in-context few-shot để dạy LLM tự hỏi các câu hỏi theo dõi trước khi trả lời câu hỏi ban đầu, mặc dù
nó tập trung nhiều hơn vào việc phân tách.

QA nhạy cảm thời gian: FRESH QA phù hợp với một nhóm công việc ngày càng tăng về đo lường khả năng
lý luận thời gian của LLM (Chen et al., 2021b; Zhang & Choi, 2021; Liska et al., 2022; Kasai et al., 2022).
Chen et al. (2021b) tạo ra TIMEQA bằng cách trích xuất các sự thật phát triển từ WIKIDATA cùng với các
đoạn WIKIPEDIA được căn chỉnh để tổng hợp 20K cặp câu hỏi-câu trả lời có dấu thời gian. Zhang & Choi (2021)
xây dựng SITUATED QA bằng cách chú thích 9K câu hỏi thực tế từ các tập dữ liệu QA mở hiện có với ngữ cảnh
thời gian (tức là, dấu thời gian). STREAMING QA (Liska et al., 2022) bao gồm cả câu hỏi do LLM tạo ra và
do con người viết (tổng cộng 146K câu hỏi) có thể trả lời từ một tập hợp các bài báo tin tức có dấu thời gian.
Cũng liên quan là benchmark REALTIME QA động (Kasai et al., 2022), đánh giá các mô hình hàng tuần trên
một tập hợp khoảng 30 câu hỏi trắc nghiệm về các sự kiện mới được trích xuất từ các trang web tin tức.
Ngược lại, FRESH QA chứa một tập hợp cố định các câu hỏi mở do con người viết có câu trả lời theo bản chất
có thể thay đổi dựa trên các phát triển mới trong thế giới và do đó cung cấp một đánh giá tạo sinh bổ sung
của QA nhạy cảm thời gian.

QA về tiền đề có vấn đề hoặc phản thực tế: Công việc gần đây cũng đã giới thiệu các benchmark QA với
tiền đề có vấn đề (Yu et al., 2023c; Kim et al., 2023) hoặc tiền đề phản thực tế (Yu et al., 2023b).
CREPE (Yu et al., 2023c) bao gồm 8400 câu hỏi Reddit (trong đó 25% câu hỏi chứa tiền đề sai được chú thích
bởi nhân viên con người) được chia thành các tập train/dev/test. Kim et al. (2023) xây dựng (QA)2, một tập
đánh giá 602 câu hỏi dựa trên các truy vấn công cụ tìm kiếm thường xuyên, được chú thích bởi các chuyên gia
và crowdworker, và chia đều giữa những câu hỏi có và không có tiền đề có vấn đề. Phù hợp với những nỗ lực này,
chúng tôi thấy rằng các LLM hiện tại gặp khó khăn với việc xử lý các câu hỏi tiền đề sai; ngoài ra, một số LLM
có thể bác bỏ một câu hỏi tiền đề sai nếu được yêu cầu rõ ràng kiểm tra tính hợp lệ của tiền đề. Tương tự như
trên, những benchmark này có tính bổ sung và việc kết hợp chúng là một hướng đầy hứa hẹn cho công việc tương lai.

6 HẠN CHẾ VÀ CÔNG VIỆC TƯƠNG LAI
Một thách thức rõ ràng với FRESH QA là nhu cầu cập nhật câu trả lời thường xuyên bởi các người duy trì;
trong khoảng thời gian tạm thời giữa các cập nhật, câu trả lời cho một số câu hỏi có thể trở nên cũ.
Điều này có thể được giải quyết bằng sự hỗ trợ từ cộng đồng mã nguồn mở (ví dụ, cập nhật thông qua
pull request của GITHUB). Về mặt phương pháp, FRESH PROMPT giao tiếp với GOOGLE SEARCH, và không rõ
nó hoạt động như thế nào với các công cụ tìm kiếm khác mà một số loại ngữ cảnh (ví dụ, answer box)
không có sẵn. Ngoài ra, chúng tôi chỉ thực hiện một truy vấn tìm kiếm mỗi câu hỏi, và do đó phương pháp
của chúng tôi có thể được cải thiện thêm thông qua việc phân tách câu hỏi và nhiều truy vấn tìm kiếm
(Khattab et al., 2022). Vì FRESH QA bao gồm các câu hỏi tiếng Anh tương đối đơn giản, cũng không rõ
FRESH PROMPT hoạt động tốt như thế nào trong bối cảnh QA đa ngôn ngữ/xuyên ngôn ngữ và QA dạng dài
(Fan et al., 2019). Cuối cùng, FRESH PROMPT dựa vào học in-context và do đó có thể hoạt động kém hơn
các phương pháp fine-tune LLM cơ sở trên kiến thức mới.

7 KẾT LUẬN
Công việc của chúng tôi cung cấp một đánh giá chi tiết và toàn diện về khả năng của các LLM hiện đại
để thích ứng với kiến thức thế giới luôn thay đổi có và không có tăng cường công cụ tìm kiếm. Trong
quá trình này, chúng tôi phát triển một tập dữ liệu mới—FRESH QA—gồm 600 câu hỏi kiểm tra một loạt
rộng khả năng lý luận, từ việc kết hợp kiến thức thay đổi nhanh đến nhận dạng các câu hỏi với tiền đề sai.
Đánh giá hai chế độ của chúng tôi cũng cung cấp một cách để đo cả tính đúng đắn và ảo giác. Ngoài ra,
chúng tôi đề xuất một thuật toán học in-context few-shot đơn giản gọi là FRESH PROMPT kết hợp các bằng chứng
liên quan được truy xuất từ GOOGLE SEARCH vào prompt của LLM. FRESH PROMPT cải thiện đáng kể hiệu suất
so với các phương pháp tăng cường công cụ tìm kiếm cạnh tranh trên FRESH QA, và một ablation tiết lộ rằng
các yếu tố như số lượng bằng chứng được kết hợp và thứ tự của chúng ảnh hưởng đến tính đúng đắn của
các câu trả lời được tạo ra bởi LLM. Chúng tôi phát hành FRESH QA và cam kết cập nhật câu trả lời của nó
thường xuyên để tạo điều kiện cho nghiên cứu tương lai.

--- TRANG 9 ---
Bản thảo

8 LỜI CẢM ƠN
Chúng tôi cảm ơn Colin Raffel, Hamed Zamani, và Subhransu Maji vì những thảo luận và phản hồi hữu ích.
Chúng tôi cũng muốn cảm ơn Chengrun Yang, Xinyun Chen vì những nhận xét sâu sắc của họ về bản thảo này.
Cuối cùng, chúng tôi biết ơn những người sau đây vì đóng góp của họ trong việc tạo ra tập dữ liệu FRESH QA
của chúng tôi: Marzena Karpinska, Dustin Tran, Daniel Cer, Sam Fullerton, Elizabeth Clark, Nishant Raj,
Xiaoyu Song, Yapei Chang, Yixiao Song, Nader Akoury, Ankita Gupta, Bill Ray, Chau Pham, Wenlong Zhao,
Maximilian Mozes, Simeng Sun, Ronan Salz, Kalpesh Krishna, Katherine Thai, Kanishka Misra, Salaheddin Alzu'bi,
Erica Cai, Thibault Sellam, Jiao Sun, Dhruv Agarwal, Tessa Masis, Andrew Drozdov, Brian Lester, George Wei,
Naveen Jafer Nizar, Shufan Wang, Youngwoo Kim, và Shib Sankar Dasgupta. Dự án này được hỗ trợ một phần
bởi giải thưởng IIS-2046248 từ National Science Foundation (NSF), cũng như chương trình CLOUD BANK của NSF.

TÀI LIỆU THAM KHẢO
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego
De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren
Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol
Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, và Laurent Sifre. Improving
language models by retrieving from trillions of tokens. Trong Proceedings of the 39th International
Conference on Machine Learning (ICML), volume 162 of Proceedings of Machine Learning
Research (PMLR), pp. 2206–2240. PMLR, 2022. URL https://proceedings.mlr.press/
v162/borgeaud22a.html.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
và Dario Amodei. Language models are few-shot learners. Trong Advances in Neural Information
Processing Systems (NeurIPS), volume 33, pp. 1877–1901, 2020. URL https://proceedings.
neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021a. URL https://arxiv.
org/abs/2107.03374.

Wenhu Chen, Xinyi Wang, William Yang Wang, và William Yang Wang. A
dataset for answering time-sensitive questions. Trong Proceedings of the Neural Infor-
mation Processing Systems Track on Datasets and Benchmarks (NeurIPS), volume 1,
2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/
paper/2021/file/1f0e3dad99908345f7439f8ffabdffc4-Paper-round2.pdf.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https:
//arxiv.org/abs/2204.02311.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416, 2022. URL https://arxiv.org/abs/2210.11416.

Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, và Michael Auli. ELI5:
Long form question answering. Trong Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 3558–3567, 2019. URL https://aclanthology.org/
P19-1346.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Mingwei Chang. Retrieval augmented
language model pre-training. Trong Proceedings of the 37th International Conference on Machine

--- TRANG 10 ---
Bản thảo

Learning (ICML), volume 119 of Proceedings of Machine Learning Research (PMLR), pp. 3929–
3938. PMLR, 2020. URL https://proceedings.mlr.press/v119/guu20a.html.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, và Edouard Grave. Few-shot learning with
retrieval augmented language models. 2022. URL https://arxiv.org/abs/2208.03299.

Mandar Joshi, Eunsol Choi, Daniel Weld, và Luke Zettlemoyer. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehension. Trong Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (ACL), pp. 1601–1611, 2017. URL
https://aclanthology.org/P17-1147.

Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir
Radev, Noah A Smith, Yejin Choi, và Kentaro Inui. Realtime qa: What's the answer right now?
2022. URL https://arxiv.org/abs/2207.13332.

Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,
và Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for
knowledge-intensive nlp. 2022. URL https://arxiv.org/abs/2212.14024.

Najoung Kim, Phu Mon Htut, Samuel R. Bowman, và Jackson Petty. (QA)2: Question answering
with questionable assumptions. Trong Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 8466–8487, 2023. URL https://aclanthology.org/
2023.acl-long.472.

Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, và Nikolai Grigorev. Internet-
augmented language models through few-shot prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115, 2022.

Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt
tuning. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 3045–3059, November 2021. URL https://aclanthology.org/
2021.emnlp-main.243.

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel,
và Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. Trong
Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 9459–
9474, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
6b493230205f780e1bc26945df7481e5-Paper.pdf.

Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal,
Cyprien De Masson D'Autume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-
Mcmahon, Sophia Austin, Phil Blunsom, và Angeliki Lazaridou. StreamingQA: A benchmark for
adaptation to new knowledge over time in question answering models. Trong Proceedings of the 39th
International Conference on Machine Learning (ICML), volume 162 of Proceedings of Machine
Learning Research (PMLR), pp. 13604–13622. PMLR, 2022. URL https://proceedings.mlr.
press/v162/liska22a.html.

Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
và Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint
arXiv:2307.03172, 2023a. URL https://arxiv.org/abs/2307.03172.

Nelson F Liu, Tianyi Zhang, và Percy Liang. Evaluating verifiability in generative search engines.
2023b. URL https://arxiv.org/abs/2304.09848.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V.
Le, Barret Zoph, Jason Wei, và Adam Roberts. The flan collection: Designing data and methods
for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. URL https://arxiv.
org/abs/2301.13688.

Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny
Fox, Helen Meng, và James Glass. Sail: Search-augmented instruction learning. 2023. URL
https://arxiv.org/abs/2305.15225.

--- TRANG 11 ---
Bản thảo

Joshua Maynez, Shashi Narayan, Bernd Bohnet, và Ryan McDonald. On faithfulness and factuality
in abstractive summarization. Trong Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 1906–1919, 2020. URL https://aclanthology.org/
2020.acl-main.173.

Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,
Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching
language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.
URL https://arxiv.org/abs/2203.11147.

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. 2021. URL https://arxiv.org/abs/2112.09332.

OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.0877, 2023. URL https://arxiv.org/
abs/2303.0877.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Chris-
tiano, Jan Leike, và Ryan Lowe. Training language models to follow instructions with human
feedback. Trong Advances in Neural Information Processing Systems (NeurIPS), volume 35, pp.
27730–27744, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/
file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.

Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, và Mike Lewis. Measuring
and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350,
2022. URL https://arxiv.org/abs/2210.03350.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-
text transformer. Journal of Machine Learning Research (JMLR), 21(140):1–67, 2020. URL
https://jmlr.org/papers/v21/20-074.html.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,
Nicola Cancedda, và Thomas Scialom. Toolformer: Language models can teach themselves to
use tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.04761.

Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, và Denny Zhou. Recitation-augmented language
models. Proceedings of the 11th International Conference on Learning Representations (ICLR
2023), 2023. URL https://openreview.net/forum?id=-cqvvvb-NkI.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022. URL https://arxiv.org/abs/2201.11903.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, và Yuan Cao.
React: Synergizing reasoning and acting in language models. 2022. URL https://arxiv.org/
abs/2210.03629.

Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, và Meng Jiang. Generate rather than retrieve: Large language models are strong
context generators. Proceedings of the 11th International Conference on Learning Representations
(ICLR 2023), 2023a. URL https://openreview.net/forum?id=fB0hRu9GZUS.

Wenhao Yu, Meng Jiang, Peter Clark, và Ashish Sabharwal. Ifqa: A dataset for open-domain
question answering under counterfactual presuppositions. 2023b. URL https://arxiv.org/
abs/2305.14010.

Xinyan Yu, Sewon Min, Luke Zettlemoyer, và Hannaneh Hajishirzi. CREPE: Open-domain
question answering with false presuppositions. Trong Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (ACL), pp. 10457–10480, 2023c. URL https:
//aclanthology.org/2023.acl-long.583.

--- TRANG 12 ---
Bản thảo

Michael Zhang và Eunsol Choi. SituatedQA: Incorporating extra-linguistic contexts into QA. Trong
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 7371–7387, 2021. URL https://aclanthology.org/2021.emnlp-main.586.

--- TRANG 13 ---
Bản thảo

PHỤ LỤC

A GIAO THỨC ĐÁNH GIÁ
Hình 4 hiển thị các ví dụ cụ thể của mỗi tiêu chí đánh giá.

B THỎA THUẬN GIỮA CÁC NGƯỜI ĐÁNH GIÁ VÀ ĐÁNH GIÁ TỰ ĐỘNG
Hai tác giả độc lập đánh giá một tập con được lấy mẫu ngẫu nhiên gồm 100 câu trả lời qua các mô hình
(bao gồm 50 câu hỏi với tiền đề hợp lệ và 50 câu hỏi với tiền đề sai) trong cả hai chế độ RELAXED và STRICT.

Để tạo điều kiện cho các đánh giá tương lai, chúng tôi cũng phát triển FRESH EVAL, một metric tự động
đơn giản sử dụng học in-context few-shot để dạy LLM đánh giá các phản hồi của mô hình. Trong mỗi
đánh giá, mô hình được điều kiện hóa trên một câu hỏi nhất định, một danh sách các câu trả lời hợp lệ
cho câu hỏi, và một phản hồi của mô hình, và sau đó được mong đợi tạo ra một nhận xét về tính đúng đắn
của phản hồi, theo sau bởi một phán quyết cuối cùng. Ở đầu mỗi input prompt, chúng tôi cũng cung cấp
một hướng dẫn về nhiệm vụ đánh giá, và các nhận xét và đánh giá mẫu của các ví dụ trong Hình 4 làm
minh chứng.13 Xem Hình 5 và Hình 6 cho prompt của FRESH EVAL cho đánh giá RELAXED và STRICT,
và Hình 7 cho đầu ra mẫu của FRESH EVAL cho đánh giá STRICT.

Bảng 2 báo cáo thỏa thuận giữa các người đánh giá giữa hai người đánh giá con người, và giữa FRESH EVAL
và mỗi người đánh giá con người, về độ chính xác tuyệt đối. Hai người đánh giá con người có thỏa thuận
99% cho RELAXED và 96% cho STRICT, trong khi FRESH EVAL đạt được thỏa thuận trung bình 96.5% với
đánh giá con người cho RELAXED và 96% cho STRICT. Nhìn chung, độ chính xác cao thể hiện rằng giao thức
đánh giá của chúng tôi có thể tái tạo và đáng tin cậy, và FRESH EVAL có thể được sử dụng thay cho đánh giá
con người trên FRESH QA.

C CHI TIẾT THIẾT LẬP THÍ NGHIỆM BỔ SUNG CHO PHẦN 3
Để tăng tính tái tạo, chúng tôi chọn token có khả năng cao nhất tại mỗi timestep giải mã (tức là, với
nhiệt độ 0) và tạo ra tối đa 256 token cho tất cả các mô hình. Lưu ý rằng API cho một số mô hình
không xác định theo mặc định, ngay cả với nhiệt độ 0. Đối với các mô hình không phải chat không được
huấn luyện trước với nhiệm vụ QA, chúng tôi đưa cho chúng một text prompt có định dạng: "Q: <question> \nA: "
("\n" là ký tự xuống dòng).

Đối với các mô hình OPENAI, chúng tôi sử dụng API 2023-03-15-preview trong AZURE OPENAI SERVICE.
Chúng tôi sử dụng các tên mô hình text-davinci-003, code-davinci-002, gpt-3.5-turbo, và gpt-4
cho GPT-3.5, CODEX, CHATGPT, và GPT-4, tương ứng.

D KẾT QUẢ THÍ NGHIỆM BỔ SUNG CHO PHẦN 3
Bảng 3 và Bảng 4 hiển thị độ chính xác của các LLM khác nhau trên FRESH QA dưới đánh giá STRICT
(không có ảo giác) và RELAXED, tương ứng.

E NHẬN THỨC CỦA CHATGPT/GPT-4 VỀ KIẾN THỨC GẦN ĐÂY
Mặc dù CHATGPT và GPT-4 ban đầu được huấn luyện vào năm 2021, đánh giá thủ công của chúng tôi
cho thấy họ đã được tiếp xúc với dữ liệu chứa thông tin vượt quá ngày cắt kiến thức của họ vào tháng 9
năm 2021. Hình 8 chỉ ra rằng CHATGPT biết về cuộc xâm lược Ukraina gần đây của Nga vào ngày 24
tháng 2 năm 2022.

13Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng việc sử dụng các prompt riêng biệt cho đánh giá
RELAXED và STRICT dẫn đến hiệu suất tốt hơn so với việc sử dụng một prompt kết hợp duy nhất cho cả hai
chế độ đánh giá. Chúng tôi cũng thấy rằng việc kết hợp thêm các bằng chứng được truy xuất cho câu hỏi vào
prompt không cải thiện thỏa thuận giữa các người đánh giá giữa FRESH EVAL và người đánh giá con người.

--- TRANG 14 ---
Bản thảo

F KẾT QUẢ TÌM KIẾM GOOGLE
Hình 9 hiển thị các loại kết quả tìm kiếm khác nhau từ GOOGLE SEARCH cho một truy vấn nhất định.

G MỘT PROMPT THỰC TẾ CHO FRESH PROMPT
Hình 10 hiển thị một prompt thực tế cho FRESH PROMPT.

H KẾT QUẢ THÍ NGHIỆM BỔ SUNG CHO PHẦN 4
Bảng 5 trình bày độ chính xác của các LLM tăng cường công cụ tìm kiếm khác nhau trên FRESH QA dưới RELAXED.

--- TRANG 15 ---
Bản thảo

IDCâu hỏiLoạiCâu trả lời chính (tính đến thời điểm viết này)Phản hồi của mô hìnhRelaxedStrictGiải thích1Người sống lâu nhất được xác minh trên thế giới bao nhiêu tuổi?thay đổi nhanh116 tuổiNgười sống lâu nhất được xác minh là Maria Branyas Morera, sinh ngày 4 tháng 3 năm 1907, khiến bà 116 tuổi.✓✓câu trả lời chính là đúng, và tất cả thông tin được cung cấp đều chính xác và cập nhật2Vương quốc Anh đã áp dụng Euro khi nào?tiền đề saiVương quốc Anh chưa bao giờ áp dụng Euro.Vương quốc Anh chưa bao giờ áp dụng Euro làm tiền tệ chính thức. Đất nước đã giữ lại đồng bảng Anh (GBP) làm tiền tệ.✓✓phản hồi bác bỏ tiền đề sai của câu hỏi, và tất cả thông tin được cung cấp đều chính xác và cập nhật3Brittney Griner sẽ ở tù Nga thêm bao lâu nữa?tiền đề saiCô ấy đã được thả vào năm 2022.Tôi xin lỗi, nhưng tôi không có thông tin nào cho thấy Brittney Griner hiện đang ở tù Nga.✗✗cần có câu trả lời tự tin và dứt khoát4Ngôn ngữ nào có dân số lớn thứ ba theo số người nói tiếng mẹ đẻ?thay đổi chậmTiếng Anh1. Tiếng Quan Thoại 2. Tiếng Tây Ban Nha 3. Tiếng Anh✓✗mặc dù câu trả lời đúng có thể được suy ra rõ ràng từ phản hồi, một phản hồi không có tuyên bố rõ ràng là kém mong muốn hơn5Tiểu thuyết của Virginia Woolf về gia đình Ramsay đã được phép sử dụng công cộng ở Hoa Kỳ chưa?không bao giờ thay đổiCó"To the Lighthouse" năm 1927 đã được phép sử dụng công cộng ở Hoa Kỳ. Câu trả lời cuối cùng là To the Lighthouse.✗✗câu trả lời cuối cùng khi đứng một mình phải đúng6Giá cổ phiếu Tesla có trên 200 đô la một cổ phiếu không?thay đổi nhanh CóGiá cổ phiếu Tesla trên 200 đô la một cổ phiếu. Cổ phiếu đang giao dịch ở mức 120 đô la.✗✗thông tin mâu thuẫn7Lionel Messi đã giành FIFA World Cup lần thứ hai khi nào?tiền đề saiMessi chưa giành FIFA World Cup lần thứ hai.Câu hỏi này không chứa tiền đề hợp lệ. Lionel Messi chưa bao giờ giành FIFA World Cup.✗✗mặc dù bác bỏ tiền đề sai của câu hỏi, phản hồi không chính xác định hình nhận thức kiến thức của người ta8Jeremy Corbyn đã phục vụ làm Thủ tướng Vương quốc Anh bao nhiều lần?tiền đề saiCorbyn chưa bao giờ phục vụ làm Thủ tướng Vương quốc Anh.Không lần nào.✗✗luôn yêu cầu bác bỏ cho các câu hỏi tiền đề sai9Ai đã thành lập Amazon?không bao giờ thay đổiJeff Bezos100% người nói Jeff.✗✗cần tên đầy đủ hoặc tên được công nhận rộng rãi10Colleen Hoover đã xuất bản bao nhiêu cuốn sách?thay đổi nhanh25Colleen Hoover đã xuất bản hơn 20 cuốn sách và tiểu thuyết.✓✗các số gần đúng không được chấp nhận trừ khi được bao gồm trong câu trả lời chuẩn11Kích thước mô hình lớn nhất của GPT-3 là bao nhiêu?không bao giờ thay đổi175B tham số175 tỷ tham số. Đó là 175 tỷ tham số, 1.37TB, 137B token, 137B wordpiece, 137B BPE, 137B byte pair encoding, 137B BPE token, 137B BPE wordpiece, 137B BPE token, 137B BPE wordpiece,...✓✗câu trả lời chính là đúng, nhưng phản hồi sai định dạng12Có bao nhiêu quốc gia thành viên trong Liên Hợp Quốc?thay đổi chậm193Tính đến ngày cắt kiến thức của tôi vào tháng 9 năm 2021, có 193 quốc gia thành viên trong Liên Hợp Quốc.✓✗câu trả lời chính là đúng, nhưng không rõ liệu thông tin có còn cập nhật không13Ai đã giành tiền thưởng lớn nhất cho một giải đấu trong lịch sử quần vợt?thay đổi chậmNovak DjokovicTiền thưởng lớn nhất cho một giải đấu trong lịch sử quần vợt được giành bởi Novak Djokovic tại Australian Open 2021.✓✗câu trả lời chính là đúng, nhưng phản hồi chứa thông tin ảo giác14Ai là cầu thủ bóng đá đầu tiên ghi một trăm bàn thắng quốc tế?không bao giờ thay đổiAli DaeiTính đến ngày cắt kiến thức của tôi vào tháng 9 năm 2021, Ali Daei, một cựu cầu thủ bóng đá Iran, là cầu thủ đầu tiên ghi 100 bàn thắng quốc tế.✓✓tất cả thông tin đều đúng và rõ ràng rằng kiến thức đã không thay đổi15Ai là người chiến thắng gần đây nhất của giải vô địch thế giới Formula 1?thay đổi nhanh Max VerstappenMax Verstappen đã giành giải vô địch thế giới Formula 1 năm 2022. Anh ấy là một tay đua người Hà Lan thi đấu trong Formula One cho Red Bull Racing. Anh ấy là con trai của cựu tay đua Formula One Jos Verstappen. Verstappen bắt đầu sự nghiệp đua xe của mình trong karting ở tuổi bảy. Anh ấy đã giành Giải vô địch Karting châu Âu 2013 và Giải vô địch Karting thế giới 2013. Năm 2014, anh ấy chuyển sang đua xe đơn chỗ ngồi, thi đấu trong Giải vô địch Formula Three châu Âu. Anh ấy đã giành giải vô địch trong mùa giải đầu tiên, trở thành tay đua đầu tiên làm được điều đó kể từ Nico Rosberg năm 2005. Năm 2015, Verstappen chuyển sang Formula One, lái xe cho Toro Rosso. Anh ấy trở thành tay đua trẻ nhất thi đấu trong Formula One ở tuổi 17. Anh ấy đã ghi điểm đầu tiên trong Formula One tại Hungarian Grand Prix 2015. Năm 2016, Verstappen chuyển sang Red Bull Racing. Anh ấy đã giành chiến thắng đầu tiên tại Spanish Grand Prix 2016. Anh ấy trở thành tay đua trẻ nhất giành chiến thắng trong một cuộc đua Formula One ở tuổi 18. Verstappen kết thúc mùa giải 2016 ở vị trí thứ ba trong bảng xếp hạng tay đua. Năm 2017, Verstappen đã giành bốn cuộc đua và kết thúc mùa giải ở vị trí thứ hai trong bảng xếp hạng tay đua. Năm 2018, Verstappen đã giành bảy cuộc đua và kết thúc mùa giải ở vị trí thứ hai trong bảng xếp hạng tay đua. Năm 2019, Verstappen đã giành chín cuộc đua và kết thúc mùa giải ở vị trí thứ nhất trong bảng xếp hạng tay đua. Anh ấy là tay đua người Hà Lan đầu tiên giành giải vô địch thế giới Formula One.✓✗câu trả lời chính là đúng, nhưng phản hồi chứa một số ảo giác (ví dụ, Max Verstappen đã không giành Giải vô địch Formula Three châu Âu năm 2014)

Hình 4: Đánh giá mẫu FRESH QA. Để được ghi nhận trong cả hai chế độ đánh giá RELAXED và STRICT,
tất cả thông tin trong câu trả lời phải chính xác và cập nhật (ví dụ 1 và 2). Trong cả hai chế độ,
chúng tôi chỉ ghi nhận phản hồi của mô hình nếu nó cung cấp câu trả lời tự tin và dứt khoát (ví dụ 3),
hoặc câu trả lời đúng có thể được suy ra rõ ràng từ phản hồi (với điều kiện tất cả yêu cầu khác được
thỏa mãn, xem ví dụ 4). Câu trả lời chính hoặc cuối cùng khi đứng một mình phải chính xác (ví dụ 5).
Bất kỳ thông tin bổ sung nào được cung cấp không được mâu thuẫn với câu trả lời chính (ví dụ 6)
hoặc định hình lại nhận thức của người ta về nó (ví dụ 7). Đối với các câu hỏi tiền đề sai, mô hình
phải chỉ ra sự hiện diện của tiền đề sai để nhận được điểm (ví dụ 8). Đối với các câu trả lời liên quan
đến tên của các thực thể (ví dụ, người), tên đầy đủ hoặc tên được công nhận rộng rãi được mong đợi
(ví dụ 9). Về các câu trả lời số, các số gần đúng thường không được chấp nhận trừ khi được bao gồm
rõ ràng trong các câu trả lời chuẩn (ví dụ 10). Dưới RELAXED, chúng tôi chấp nhận các phản hồi
sai định dạng (bao gồm những phản hồi bằng ngôn ngữ không phải tiếng Anh), cũng như thông tin
ảo giác hoặc lỗi thời không ảnh hưởng đáng kể đến câu trả lời chính; dưới STRICT, tuy nhiên,
một phản hồi chứa bất kỳ ảo giác nào, dù nhỏ đến đâu, sẽ không nhận được điểm (ví dụ 11, 12, và 13).
Hơn nữa, chúng tôi chấp nhận một phản hồi trong STRICT khi mô hình chỉ ra rằng thông tin có thể
đã lỗi thời (ví dụ, "Tính đến ngày cắt kiến thức của tôi vào tháng 9 năm 2021") chỉ khi rõ ràng
rằng kiến thức đã không thay đổi (ví dụ 14).

--- TRANG 16 ---
Bản thảo

Bảng 2: Thỏa thuận giữa các người đánh giá giữa hai tác giả (RATER 1 và RATER 2), và giữa FRESH EVAL
và mỗi người đánh giá con người, về độ chính xác tuyệt đối qua 100 phán quyết RELAXED, 100 phán quyết
STRICT, và tất cả 200 phán quyết. Trong mỗi ba danh mục này, ngoài độ chính xác tổng thể (overall),
chúng tôi báo cáo độ chính xác qua các câu hỏi với tiền đề hợp lệ (vp) và các câu hỏi với tiền đề sai (fp).
Độ chính xác cao thể hiện rằng giao thức đánh giá của chúng tôi có thể tái tạo và đáng tin cậy, và FRESH EVAL
có thể được sử dụng thay cho đánh giá con người trên FRESH QA.

RELAXED STRICT ALL
overall vp fp overall vp fp overall vp fp
RATER 1 vs. RATER 2 99.0 98.0 100 96.0 100.0 92.0 97.5 99.0 96.0
FRESH EVAL vs. RATER 1 97.0 98.0 96.0 97.0 100.0 94.0 97.0 99.0 95.0
FRESH EVAL vs. RATER 2 96.0 96.0 96.0 95.0 100.0 90.0 95.5 98.0 93.0

--- TRANG 17 ---
Bản thảo

Vui lòng đánh giá phản hồi cho mỗi câu hỏi đã cho dưới một đánh giá relaxed, nơi ảo giác, thông tin lỗi thời,
và câu trả lời sai định dạng được cho phép, miễn là câu trả lời chính chính xác. Vui lòng chỉ ghi nhận phản hồi
nếu nó cung cấp câu trả lời tự tin và dứt khoát, hoặc câu trả lời đúng có thể được suy ra rõ ràng từ phản hồi.
Câu trả lời chính hoặc cuối cùng khi đứng một mình phải chính xác. Bất kỳ thông tin bổ sung nào được cung cấp
không được mâu thuẫn với câu trả lời chính hoặc định hình lại nhận thức của người ta về nó. Đối với các câu hỏi
tiền đề sai, phản hồi phải chỉ ra sự hiện diện của tiền đề sai để nhận được điểm. Đối với các câu trả lời liên quan
đến tên của các thực thể (ví dụ, người), tên đầy đủ hoặc tên được công nhận rộng rãi được mong đợi. Về các
câu trả lời số, các số gần đúng thường không được chấp nhận trừ khi được bao gồm rõ ràng trong các câu trả lời
chuẩn. Chúng tôi chấp nhận các phản hồi sai định dạng (bao gồm những phản hồi bằng ngôn ngữ không phải
tiếng Anh), cũng như thông tin ảo giác hoặc lỗi thời không ảnh hưởng đáng kể đến câu trả lời chính.
# một số minh chứng được bỏ qua vì ngắn gọn
câu hỏi: Giá cổ phiếu Tesla có trên 250 đô la một cổ phiếu không?
câu trả lời đúng: Có
phản hồi: Có, đúng vậy. Giá cổ phiếu hiện tại là 207 đô la.
nhận xét: Đây là một câu hỏi hợp lệ. Trong khi câu trả lời chính trong phản hồi (Có) chính xác, thông tin bổ sung
mâu thuẫn với câu trả lời chính (207 đô la không trên 250 đô la). Do đó, phản hồi không được ghi nhận.
đánh giá: không chính xác
câu hỏi: Ai là người chiến thắng gần đây nhất của giải vô địch thế giới Formula 1?
câu trả lời đúng: Max Verstappen | Max Emilian Verstappen
phản hồi: Max Verstappen đã giành giải vô địch thế giới Formula 1 năm 2022. Anh ấy là một tay đua người Hà Lan
thi đấu trong Formula One cho Red Bull Racing. Anh ấy là con trai của cựu tay đua Formula One Jos Verstappen.
Verstappen bắt đầu sự nghiệp đua xe của mình trong karting ở tuổi bảy. Anh ấy đã giành Giải vô địch Karting
châu Âu 2013 và Giải vô địch Karting thế giới 2013. Năm 2014, anh ấy chuyển sang đua xe đơn chỗ ngồi, thi đấu
trong Giải vô địch Formula Three châu Âu. Anh ấy đã giành giải vô địch trong mùa giải đầu tiên, trở thành tay đua
đầu tiên làm được điều đó kể từ Nico Rosberg năm 2005... # phần còn lại được bỏ qua vì ngắn gọn
nhận xét: Đây là một câu hỏi hợp lệ. Mặc dù phản hồi chứa một số trường hợp thông tin ảo giác (ví dụ, Max
Verstappen đã không giành Giải vô địch Formula Three châu Âu năm 2014), câu trả lời chính trong phản hồi
(Max Verstappen) vẫn chính xác. Do đó, phản hồi được ghi nhận.
đánh giá: chính xác
câu hỏi: Bodybuilding trở thành môn thể thao Olympic vào năm nào?
câu trả lời đúng: Bodybuilding không phải là môn thể thao Olympic.
phản hồi: Bodybuilding hiện không phải là môn thể thao Olympic. Nó đã được Ủy ban Olympic Quốc tế (IOC)
công nhận nhưng chưa được đưa vào chương trình Olympic. Tuy nhiên, bodybuilding đã được đưa vào các sự kiện
thể thao đa môn khác như Pan American Games và Asian Games.
nhận xét:

Hình 5: Prompt của FRESH EVAL cho đánh giá RELAXED.

--- TRANG 18 ---
Bản thảo

Vui lòng đánh giá phản hồi cho mỗi câu hỏi đã cho dưới một đánh giá strict, nơi không có ảo giác, thông tin lỗi thời,
hoặc câu trả lời sai định dạng được cho phép. Vui lòng chỉ ghi nhận phản hồi nếu nó cung cấp câu trả lời tự tin và
dứt khoát, hoặc câu trả lời đúng có thể được suy ra rõ ràng từ phản hồi. Câu trả lời chính hoặc cuối cùng khi đứng
một mình phải chính xác. Bất kỳ thông tin bổ sung nào được cung cấp không được mâu thuẫn với câu trả lời chính
hoặc định hình lại nhận thức của người ta về nó. Đối với các câu hỏi tiền đề sai, phản hồi phải chỉ ra sự hiện diện
của tiền đề sai để nhận được điểm. Đối với các câu trả lời liên quan đến tên của các thực thể (ví dụ, người), tên
đầy đủ hoặc tên được công nhận rộng rãi được mong đợi. Về các câu trả lời số, các số gần đúng thường không được
chấp nhận trừ khi được bao gồm rõ ràng trong các câu trả lời chuẩn. Một phản hồi chứa bất kỳ ảo giác nào, dù nhỏ
đến đâu, sẽ không nhận được điểm. Hơn nữa, khi phản hồi chỉ ra rằng thông tin có thể đã lỗi thời, chúng tôi chấp nhận
nó chỉ khi rõ ràng rằng kiến thức đã không thay đổi (ví dụ, thông qua lẽ thường hoặc sự thật được biết đến rộng rãi).
# một số minh chứng được bỏ qua vì ngắn gọn
câu hỏi: Người sống lâu nhất được xác minh trên thế giới bao nhiêu tuổi?
câu trả lời đúng: 116 tuổi | 116
phản hồi: Tính đến hôm nay {current_date}, thông tin cập nhật và liên quan nhất về truy vấn này như sau. Người
sống lâu nhất được xác minh là Maria Branyas Morera, sinh ngày 4 tháng 3 năm 1907, khiến bà 116 tuổi.
nhận xét: Đây là một câu hỏi hợp lệ. Câu trả lời chính trong phản hồi (116 tuổi) chính xác, và tất cả thông tin
được cung cấp đều chính xác và cập nhật. Do đó, phản hồi được ghi nhận.
đánh giá: chính xác
câu hỏi: Ai là người chiến thắng gần đây nhất của giải vô địch thế giới Formula 1?
câu trả lời đúng: Max Verstappen | Max Emilian Verstappen
phản hồi: Max Verstappen đã giành giải vô địch thế giới Formula 1 năm 2022. Anh ấy là một tay đua người Hà Lan
thi đấu trong Formula One cho Red Bull Racing. Anh ấy là con trai của cựu tay đua Formula One Jos Verstappen.
Verstappen bắt đầu sự nghiệp đua xe của mình trong karting ở tuổi bảy. Anh ấy đã giành Giải vô địch Karting
châu Âu 2013 và Giải vô địch Karting thế giới 2013. Năm 2014, anh ấy chuyển sang đua xe đơn chỗ ngồi, thi đấu
trong Giải vô địch Formula Three châu Âu. Anh ấy đã giành giải vô địch trong mùa giải đầu tiên, trở thành tay đua
đầu tiên làm được điều đó kể từ Nico Rosberg năm 2005... # phần còn lại được bỏ qua vì ngắn gọn
nhận xét: Đây là một câu hỏi hợp lệ. Trong khi câu trả lời chính trong phản hồi (Max Verstappen) chính xác,
phản hồi chứa một số trường hợp thông tin ảo giác (ví dụ, Max Verstappen đã không giành Giải vô địch Formula
Three châu Âu năm 2014). Do đó, phản hồi không được ghi nhận.
đánh giá: không chính xác
câu hỏi: Vào tháng 1 năm 2023, NHC đã sửa đổi dữ liệu tử vong của Hurricane Katrina, tăng số người chết được
báo cáo từ 1.800 lên con số nào?
câu trả lời đúng: Số người chết được báo cáo đã giảm xuống 1.392.
phản hồi: Tôi không thể cung cấp dữ liệu tử vong cập nhật cho Hurricane Katrina vào tháng 1 năm 2023, vì kiến thức
của tôi chỉ mở rộng đến tháng 9 năm 2021. Vui lòng tham khảo một nguồn gần đây hơn để biết thông tin mới nhất.
nhận xét:

Hình 6: Prompt của FRESH EVAL cho đánh giá STRICT.

--- TRANG 19 ---
Bản thảo

câu hỏi: Elon Musk đã mua Activision Blizzard với giá bao nhiêu?
câu trả lời đúng: Elon Musk chưa mua Activision Blizzard cho đến nay (Microsoft đã mua Activision Blizzard
với giá 68.7 tỷ đô la Mỹ vào tháng 1 năm 2022).
phản hồi: Tính đến ngày cắt kiến thức của tôi vào tháng 9 năm 2021, Elon Musk đã không mua Activision Blizzard.
Nếu sự kiện này xảy ra sau ngày đó, tôi không thể cung cấp thông tin.
câu hỏi: Tiền điện tử nào có vốn hóa thị trường cao hơn Bitcoin?
câu trả lời đúng: Hiện không có tiền điện tử nào có vốn hóa thị trường cao hơn Bitcoin.
phản hồi: Tính đến bản cập nhật kiến thức cuối cùng của tôi vào tháng 9 năm 2021, không có tiền điện tử nào
có vốn hóa thị trường cao hơn Bitcoin. Tuy nhiên, thị trường tiền điện tử rất biến động, và thứ hạng có thể thay đổi
theo thời gian. Để có thông tin cập nhật nhất, vui lòng tham khảo một nguồn đáng tin cậy như CoinMarketCap hoặc CoinGecko.
câu hỏi: Kỷ lục thế giới về thời gian chạy một dặm là bao nhiêu?
câu trả lời đúng: 3:43.13 | 3 phút 43 giây
phản hồi: Tính đến hôm nay {current_date}, thông tin cập nhật và liên quan nhất về truy vấn này như sau. Kỷ lục
thế giới về thời gian chạy một dặm là 3 phút 43.13 giây, được thiết lập bởi Hicham El Guerrouj của Morocco vào
ngày 7 tháng 7 năm 1999.
nhận xét: Câu hỏi chứa tiền đề sai. Phản hồi xác định chính xác rằng Elon Musk chưa mua Activision Blizzard,
điều này chính xác và bác bỏ tiền đề sai. Tuy nhiên, phản hồi bao gồm thông tin không cần thiết về ngày cắt kiến thức
không liên quan đến độ chính xác của câu trả lời. Câu trả lời chính là đúng, nhưng thông tin bổ sung về ngày cắt
kiến thức có thể gây nhầm lẫn về vấn đề. Do đó, phản hồi không được ghi nhận.
đánh giá: không chính xác
nhận xét: Câu hỏi chứa tiền đề sai. Phản hồi không cung cấp câu trả lời hiện tại và thay vào đó dựa vào thông tin
lỗi thời mà không xác nhận tính chính xác tiếp tục của nó. Do đó, phản hồi không được ghi nhận.
đánh giá: không chính xác
nhận xét: Đây là một câu hỏi hợp lệ. Câu trả lời chính trong phản hồi (3 phút 43.13 giây) chính xác, và tất cả
thông tin được cung cấp đều chính xác và cập nhật. Do đó, phản hồi được ghi nhận.
đánh giá: chính xác

Hình 7: Đầu ra mẫu của FRESH EVAL cho đánh giá STRICT.

--- TRANG 20 ---
Bản thảo

Bảng 3: Độ chính xác của các LLM khác nhau trên FRESH QA dưới đánh giá STRICT (không có ảo giác).
Các mô hình được đo lường cùng ngày 26 tháng 4 năm 2023. Chúng tôi báo cáo độ chính xác qua các danh mục
câu hỏi khác nhau, bao gồm thay đổi nhanh (fast), thay đổi chậm (slow), không bao giờ thay đổi (never),
tiền đề sai, các câu hỏi liên quan đến kiến thức trước 2022 (<2022) và từ 2022 (≥2022), câu hỏi một bước
(1-hop) và nhiều bước (m-hop). + chỉ ra mô hình có quyền truy cập vào ngày hiện tại.

Mô hình (kích thước) ngày cắt kiến thức tất cả tiền đề hợp lệ tiền đề sai
tất cả fast slow never <2022 ≥2022 1-hop m-hop tất cả <2022

không có quyền truy cập vào công cụ tìm kiếm
OPENAI CODEX (N/A) 2021 25.0 31.4 5.6 28.0 60.3 64.5 11.5 34.7 23.1 5.6 7.5
GPT 3.5 (N/A) 2021 26.0 26.1 4.0 15.2 58.7 61.0 5.1 28.0 21.3 25.8 34.4
CHATGPT (N/A) 2021+ 32.0 28.5 7.2 16.0 61.9 63.1 7.7 29.9 25.0 42.7 52.7
GPT 4 (N/A) 2021+ 28.6 26.9 12.0 4.0 64.3 58.2 8.1 27.2 25.9 33.9 41.9
FLAN-PALM (540B) 2022 23.4 30.3 10.4 24.8 55.6 60.3 12.3 32.5 25.0 2.4 3.2
PALM (540B) 2021 7.2 9.3 0.8 11.2 15.9 20.6 2.6 9.3 9.3 0.8 1.1
w/FEW-SHOT 20.0 26.3 5.6 19.2 54.0 56.7 8.1 25.7 27.8 0.8 1.1
w/COT 15.4 19.1 0.8 9.6 46.8 47.5 2.1 20.5 15.7 4.0 5.4
PALM CHILLA (62B) 2022 12.2 16.0 2.4 15.2 30.2 35.5 4.3 17.2 13.0 0.8 1.1
PALM (62B) 2021 6.2 8.2 1.6 8.8 14.3 16.3 3.4 7.8 9.3 0.0 0.0
w/FEW-SHOT 12.8 16.8 3.2 15.2 31.7 35.5 5.5 17.9 13.9 0.8 1.1
w/COT 7.0 9.0 0.8 6.4 19.8 21.3 1.7 10.1 6.5 0.8 1.1
PALM (8B) 2021 5.6 7.5 0.8 5.6 16.0 16.2 2.1 8.6 4.6 0.0 0.0
w/FEW-SHOT 8.4 11.2 0.8 9.6 23.0 24.8 3.0 14.2 3.7 0.0 0.0
w/COT 7.8 10.4 0.0 6.4 24.6 24.8 1.7 11.2 8.3 0.0 0.0
FLAN-T5 XXL (11B) 2022 6.6 8.8 3.2 10.4 12.7 13.5 6.0 10.1 5.6 0.0 0.0
T5 XXL (11B) 2019 7.0 8.8 2.4 4.8 19.0 16.3 4.3 10.4 4.6 1.6 2.2
w/FEW-SHOT 8.4 11.2 5.6 11.2 16.7 17.7 7.2 13.4 5.6 0.0 0.0
w/COT 6.2 8.2 2.4 6.4 15.9 15.6 3.8 8.6 7.4 0.0 0.0
T5 XL (3B) 2019 4.4 5.9 2.4 4.8 10.3 10.6 3.0 7.5 1.9 0.0 0.0
w/FEW-SHOT 6.0 8.0 4.0 8.8 11.1 13.5 4.7 8.2 7.4 0.0 0.0
w/COT 2.8 3.7 2.4 1.6 7.1 7.8 1.3 4.1 2.8 0.0 0.0
T5 LARGE (770M) 2019 2.6 3.5 0.8 4.0 5.6 5.7 2.1 3.7 2.8 0.0 0.0
w/FEW-SHOT 0.8 1.1 0.0 0.0 3.2 2.8 0.0 1.1 0.9 0.0 0.0
w/COT 0.8 1.1 0.8 0.0 2.4 2.1 0.4 1.1 0.9 0.0 0.0

--- TRANG 21 ---
Bản thảo

Bảng 4: Độ chính xác của các LLM khác nhau trên FRESH QA dưới đánh giá RELAXED. Các mô hình được
đo lường cùng ngày 26 tháng 4 năm 2023. Chúng tôi báo cáo độ chính xác qua các danh mục câu hỏi khác nhau,
bao gồm thay đổi nhanh (fast), thay đổi chậm (slow), không bao giờ thay đổi (never), tiền đề sai, các câu hỏi
liên quan đến kiến thức trước 2022 (<2022) và từ 2022 (≥2022), câu hỏi một bước (1-hop) và nhiều bước
(m-hop). + chỉ ra mô hình có quyền truy cập vào ngày hiện tại.

Mô hình (kích thước) ngày cắt kiến thức tất cả tiền đề hợp lệ tiền đề sai
tất cả fast slow never <2022 ≥2022 1-hop m-hop tất cả <2022

không có quyền truy cập vào công cụ tìm kiếm
OPENAI CODEX (N/A) 2021 25.6 32.2 6.4 29.6 60.3 66.0 11.9 35.4 24.1 5.6 7.5
GPT 3.5 (N/A) 2021 32.4 32.4 8.0 28.0 61.1 68.1 11.1 34.7 26.9 32.3 43.0
CHATGPT (N/A) 2021+ 41.4 36.7 10.4 32.8 66.7 76.6 12.8 36.2 38.0 55.6 66.7
GPT 4 (N/A) 2021+ 46.4 39.6 14.4 35.2 69.0 80.9 14.9 39.2 40.7 66.9 83.9
FLAN-PALM (540B) 2022 23.6 30.3 10.4 24.8 55.6 60.3 12.3 32.5 25.0 3.2 4.3
PALM (540B) 2021 12.2 16.0 2.4 14.4 31.0 34.8 4.7 16.4 14.8 0.8 1.1
w/FEW-SHOT 20.2 26.3 5.6 19.2 54.0 56.7 8.1 25.7 27.8 1.6 2.2
w/COT 22.8 28.2 4.0 20.0 60.3 64.5 6.4 28.4 27.8 6.5 8.6
PALM CHILLA (62B) 2022 15.0 19.4 2.4 19.2 36.5 43.3 5.1 20.1 17.6 1.6 2.2
PALM (62B) 2021 8.6 11.2 2.4 11.2 19.8 22.0 4.7 11.6 10.2 0.8 1.1
w/FEW-SHOT 14.2 18.4 4.0 15.2 35.7 39.0 6.0 18.7 17.6 1.6 2.2
w/COT 12.8 16.2 2.4 15.2 31.0 34.8 5.1 17.5 13.0 2.4 3.2
PALM (8B) 2021 8.8 11.2 0.8 11.2 21.6 21.1 5.2 13.1 6.5 1.6 2.1
w/FEW-SHOT 9.2 12.2 0.8 10.4 25.4 27.0 3.4 15.3 4.6 0.0 0.0
w/COT 11.4 15.2 2.4 11.2 31.7 32.6 4.7 16.8 11.1 0.0 0.0
FLAN-T5 XXL (11B) 2022 7.2 9.6 3.2 12.0 13.5 14.2 6.8 10.8 6.5 0.0 0.0
T5 XXL (11B) 2019 10.8 13.8 3.2 12.8 25.4 22.7 8.5 16.0 8.3 1.6 2.2
w/FEW-SHOT 9.0 12.0 5.6 11.2 19.0 19.1 7.7 14.6 5.6 0.0 0.0
w/COT 13.0 17.3 4.0 17.6 30.2 31.2 8.9 19.0 13.0 0.0 0.0
T5 XL (3B) 2019 5.8 7.7 4.0 5.6 13.5 13.5 4.3 9.0 4.6 0.0 0.0
w/FEW-SHOT 6.0 8.0 4.0 8.8 11.1 13.5 4.7 8.2 7.4 0.0 0.0
w/COT 5.2 6.9 3.2 4.0 13.5 14.2 2.6 8.6 2.8 0.0 0.0
T5 LARGE (770M) 2019 4.4 5.3 2.4 4.8 8.7 7.1 4.3 5.6 4.6 1.6 2.2
w/FEW-SHOT 0.8 1.1 0.0 0.0 3.2 2.8 0.0 1.1 0.9 0.0 0.0
w/COT 2.2 2.9 0.8 0.8 7.1 7.1 0.4 3.4 1.9 0.0 0.0

--- TRANG 22 ---
Bản thảo

Hình 8: Mặc dù ngày cắt kiến thức của nó vào tháng 9 năm 2021, CHATGPT biết về cuộc xâm lược
Ukraina gần đây của Nga vào ngày 24 tháng 2 năm 2022. Câu hỏi được hỏi vào ngày 9 tháng 4 năm 2023.

--- TRANG 23 ---
Bản thảo

answer box answer box related questions related questions

knowledge graph truy vấn tìm kiếm truy vấn tìm kiếm

questions and answers
organic results organic results

Hình 9: GOOGLE SEARCH tạo ra các loại kết quả tìm kiếm khác nhau cho một truy vấn nhất định, bao gồm
answer box, organic results, và thông tin hữu ích khác, như knowledge graph, câu hỏi và câu trả lời từ
các nền tảng QA crowdsourced, và các câu hỏi liên quan mà người dùng tìm kiếm cũng hỏi. Mỗi kết quả
này chứa một đoạn văn bản liên quan cùng với thông tin khác, như trang web nguồn, ngày, tiêu đề,
và từ được làm nổi bật.

--- TRANG 24 ---
Bản thảo

{other_demonstrations} # bỏ qua vì ngắn gọn
{current_date}
{retrieved_evidences} # bỏ qua vì ngắn gọn

Hình 10: Một prompt thực tế cho FRESH PROMPT. Chúng tôi đưa tất cả các bằng chứng được lấy vào một
định dạng thống nhất với thông tin hữu ích, bao gồm trang web nguồn, ngày, tiêu đề, đoạn văn bản, và
từ được làm nổi bật. Các minh chứng few-shot được cung cấp ở đầu prompt. Mỗi minh chứng cho mô hình
thấy một câu hỏi ví dụ và danh sách các bằng chứng được lấy cho câu hỏi, theo sau là một số lý luận
trên các bằng chứng để tìm ra câu trả lời liên quan và cập nhật nhất.

--- TRANG 25 ---
Bản thảo

Bảng 5: Độ chính xác của các LLM tăng cường công cụ tìm kiếm khác nhau trên FRESH QA dưới đánh giá
RELAXED. Các mô hình được đo lường cùng ngày 26 tháng 4 năm 2023. Chúng tôi báo cáo độ chính xác
qua các danh mục câu hỏi khác nhau, bao gồm thay đổi nhanh (fast), thay đổi chậm (slow), không bao giờ
thay đổi (never), tiền đề sai, các câu hỏi liên quan đến kiến thức trước 2022 (<2022) và từ 2022 (≥2022),
câu hỏi một bước (1-hop) và nhiều bước (m-hop). + chỉ ra mô hình có quyền truy cập vào ngày hiện tại.
UTD viết tắt của "up-to-date".

Mô hình ngày cắt kiến thức tất cả tiền đề hợp lệ tiền đề sai
tất cả fast slow never <2022 ≥2022 1-hop m-hop tất cả <2022

so sánh với baseline
GOOGLE SEARCH UTD 47.4 58.8 42.4 56.0 77.8 74.5 49.4 66.4 39.8 12.9 11.8
GPT-3.5 2021 32.4 32.4 8.0 28.0 61.1 68.1 11.1 34.7 26.9 32.3 43.0
GPT-3.5 + SELF-ASK UTD 42.0 51.6 36.8 44.8 73.0 74.5 37.9 53.0 48.1 12.9 17.2
GPT-3.5 + FRESH PROMPT UTD 62.0 68.9 51.2 70.4 84.9 78.0 63.4 75.0 53.7 41.1 49.5
PPLX.AI UTD 66.2 68.9 48.8 67.2 90.5 85.1 59.1 76.1 50.9 58.1 60.2
GPT-4 2021+ 46.4 39.6 14.4 35.2 69.0 80.9 14.9 39.2 40.7 66.9 83.9
GPT-4 + SELF-ASK UTD 50.4 48.4 40.0 49.6 55.6 52.5 46.0 45.1 56.5 56.5 69.9
GPT-4 + FRESH PROMPT UTD 77.8 78.7 61.6 79.2 95.2 90.8 71.5 83.2 67.6 75.0 80.6

nghiên cứu độ nhạy và ablation
GPT-3.5 2021 32.4 32.4 8.0 28.0 61.1 68.1 11.1 34.7 26.9 32.3 43.0
GPT-3.5 + FRESH PROMPT UTD 62.0 68.9 51.2 70.4 84.9 78.0 63.4 75.0 53.7 41.1 49.5
w/PREMISE CHECK UTD 41.0 33.5 23.2 32.0 45.2 44.0 27.2 37.7 23.1 63.7 72.0
GPT-4 2021+ 46.4 39.6 14.4 35.2 69.0 80.9 14.9 39.2 40.7 66.9 83.9
GPT-4 w/SNIPPETS ONLY & SEARCH ORDER UTD 77.6 78.2 59.2 80.0 95.2 90.8 70.6 82.1 68.5 75.8 83.9
GPT-4 w/SNIPPETS ONLY & TIME ORDER UTD 77.6 78.2 59.2 79.2 96.0 90.1 71.1 82.1 68.5 75.8 86.0
GPT-4 w/SNIPPETS ONLY & RANDOM ORDER UTD 75.4 76.1 58.4 73.6 96.0 90.8 67.2 80.6 64.8 73.4 81.7
GPT-4 + FRESH PROMPT UTD 77.8 78.7 61.6 79.2 95.2 90.8 71.5 83.2 67.6 75.0 80.6
w/PREMISE CHECK UTD 78.8 76.3 59.2 76.8 92.9 87.2 69.8 82.1 62.0 86.3 90.3
w/o ANSWER BOX UTD 76.2 76.6 59.2 76.0 94.4 90.1 68.5 81.0 65.7 75.0 80.6
w/o ANSWER BOX & RELEVANT INFO UTD 74.8 75.0 56.0 74.4 94.4 89.4 66.4 80.6 61.1 74.2 81.7
w/1 EVIDENCE UTD 67.2 67.3 47.2 66.4 88.1 85.8 56.2 72.0 55.6 66.9 79.6
w/5 EVIDENCES UTD 74.2 75.0 56.8 74.4 93.7 87.2 67.7 81.7 58.3 71.8 77.4
w/15 EVIDENCES UTD 79.0 79.5 62.4 80.0 96.0 90.1 73.2 83.2 70.4 77.4 81.7
w/15 DEMONSTRATIONS UTD 77.2 78.2 60.0 78.4 96.0 91.5 70.2 82.8 66.7 74.2 79.6
w/LONG DEMONSTRATION ANSWERS UTD 77.8 77.9 60.8 77.6 95.2 90.1 70.6 82.8 65.7 77.4 83.9

--- TRANG 26 ---
Bản thảo
