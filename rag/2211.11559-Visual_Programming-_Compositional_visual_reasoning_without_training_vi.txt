# 2211.11559.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2211.11559.pdf
# Kích thước tệp: 22028448 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Lập trình Trực quan: Lý luận trực quan tổng hợp không cần huấn luyện
Tanmay Gupta, Aniruddha Kembhavi
PRIOR @ Allen Institute for AI
https://prior.allenai.org/projects/visprog
Hình 1. VISPROG là một hệ thống neuro-symbolic mô-đun và có thể diễn giải cho lý luận trực quan tổng hợp. Được cung cấp một vài ví dụ về hướng dẫn ngôn ngữ tự nhiên và các chương trình cấp cao mong muốn, VISPROG tạo ra một chương trình cho bất kỳ hướng dẫn mới nào bằng cách sử dụng học tập trong ngữ cảnh trong GPT-3 và sau đó thực thi chương trình trên (các) hình ảnh đầu vào để có được dự đoán. VISPROG cũng tóm tắt các đầu ra trung gian thành một cơ sở lý luận trực quan có thể diễn giải (Hình 4). Chúng tôi trình bày VISPROG về các nhiệm vụ yêu cầu kết hợp một tập hợp đa dạng các mô-đun để hiểu và thao tác hình ảnh, truy xuất kiến thức, và các phép toán số học và logic.

Tóm tắt
Chúng tôi trình bày VISPROG, một phương pháp neuro-symbolic để giải quyết các nhiệm vụ trực quan phức tạp và tổng hợp được đưa ra các hướng dẫn ngôn ngữ tự nhiên. VISPROG tránh nhu cầu huấn luyện đặc thù cho từng nhiệm vụ. Thay vào đó, nó sử dụng khả năng học tập trong ngữ cảnh của các mô hình ngôn ngữ lớn để tạo ra các chương trình mô-đun giống python, sau đó được thực thi để có được cả giải pháp và một cơ sở lý luận toàn diện và có thể diễn giải. Mỗi dòng của chương trình được tạo có thể gọi một trong số nhiều mô hình thị giác máy tính có sẵn, các chương trình con xử lý hình ảnh, hoặc các hàm python để tạo ra các đầu ra trung gian có thể được tiêu thụ bởi các phần tiếp theo của chương trình. Chúng tôi trình bày tính linh hoạt của VISPROG trên 4 nhiệm vụ đa dạng - trả lời câu hỏi trực quan tổng hợp, lý luận zero-shot trên các cặp hình ảnh, gắn thẻ đối tượng kiến thức thực tế, và chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ. Chúng tôi tin rằng các phương pháp neuro-symbolic như VISPROG là một hướng đi thú vị để dễ dàng và hiệu quả mở rộng phạm vi của các hệ thống AI để phục vụ đuôi dài của các nhiệm vụ phức tạp mà mọi người có thể muốn thực hiện.

--- TRANG 2 ---
1. Giới thiệu
Việc theo đuổi các hệ thống AI mục đích chung đã dẫn đến sự phát triển của các mô hình có khả năng huấn luyện đầu cuối có thể [1, 5, 8, 13, 19, 25, 27], nhiều trong số chúng mong muốn cung cấp một giao diện ngôn ngữ tự nhiên đơn giản để người dùng tương tác với mô hình. Phương pháp chủ đạo để xây dựng các hệ thống này là tiền huấn luyện không giám sát quy mô lớn tiếp theo là huấn luyện đa nhiệm vụ có giám sát. Tuy nhiên, phương pháp này đòi hỏi một tập dữ liệu được tuyển chọn tốt cho mỗi nhiệm vụ, điều này khiến việc mở rộng quy mô đến đuôi dài vô hạn của các nhiệm vụ phức tạp mà cuối cùng chúng ta muốn các hệ thống này thực hiện trở nên khó khăn. Trong công trình này, chúng tôi khám phá việc sử dụng các mô hình ngôn ngữ lớn để giải quyết đuôi dài của các nhiệm vụ phức tạp bằng cách phân tách các nhiệm vụ được mô tả bằng ngôn ngữ tự nhiên thành các bước đơn giản hơn có thể được xử lý bởi các mô hình được huấn luyện đầu cuối chuyên biệt hoặc các chương trình khác.

Hãy tưởng tượng hướng dẫn một hệ thống thị giác "Gắn thẻ 7 nhân vật chính trong chương trình truyền hình Big Bang Theory trong hình ảnh này." Để thực hiện nhiệm vụ này, hệ thống trước tiên cần hiểu ý định của hướng dẫn và sau đó thực hiện một chuỗi các bước - phát hiện khuôn mặt, truy xuất danh sách các nhân vật chính trong Big Bang Theory từ cơ sở kiến thức, phân loại khuôn mặt bằng danh sách nhân vật, và gắn thẻ hình ảnh với khuôn mặt và tên của nhân vật được nhận diện. Mặc dù các hệ thống thị giác và ngôn ngữ khác nhau tồn tại để thực hiện từng bước này, việc thực thi nhiệm vụ được mô tả bằng ngôn ngữ tự nhiên này vượt ra ngoài phạm vi của các hệ thống được huấn luyện đầu cuối.

Chúng tôi giới thiệu VISPROG nhận đầu vào dữ liệu trực quan (một hình ảnh đơn hoặc một tập hợp hình ảnh) cùng với một hướng dẫn ngôn ngữ tự nhiên, tạo ra một chuỗi các bước, một chương trình trực quan nếu bạn muốn, và sau đó thực thi các bước này để tạo ra đầu ra mong muốn. Mỗi dòng trong một chương trình trực quan gọi một trong số một loạt rộng các mô-đun hiện được hỗ trợ bởi hệ thống. Các mô-đun có thể là các mô hình thị giác máy tính có sẵn, mô hình ngôn ngữ, các chương trình con xử lý hình ảnh trong OpenCV [4], hoặc các toán tử số học và logic. Các mô-đun tiêu thụ đầu vào được tạo ra bằng cách thực thi các dòng mã trước đó và đầu ra các kết quả trung gian có thể được tiêu thụ ở phía downstream. Trong ví dụ trên, chương trình trực quan được tạo bởi VISPROG gọi một bộ phát hiện khuôn mặt [18], GPT-3 [5] như một hệ thống truy xuất kiến thức, và CLIP [23] như một bộ phân loại hình ảnh từ vựng mở để tạo ra đầu ra mong muốn (xem Hình 1).

VISPROG cải thiện các phương pháp trước đó để tạo và thực thi các chương trình cho các ứng dụng thị giác. Đối với nhiệm vụ trả lời câu hỏi trực quan (VQA), Neural Module Networks (NMN) [2,9,10,12] tổng hợp một mạng có thể huấn luyện đầu cuối đặc thù cho câu hỏi từ các mô-đun neural chuyên biệt, có thể vi phân. Các phương pháp này hoặc sử dụng các bộ phân tích cú pháp ngữ nghĩa có sẵn dễ vỡ để tính toán xác định bố cục của các mô-đun, hoặc học một bộ tạo bố cục thông qua giám sát trả lời yếu qua REINFORCE [33]. Ngược lại, VISPROG sử dụng một mô hình ngôn ngữ mạnh mẽ (GPT-3)

Hình 2. Các mô-đun hiện được hỗ trợ trong VISPROG. Các mô-đun màu đỏ sử dụng các mô hình neural (OWL-ViT [21], DSFD [18], MaskFormer [6], CLIP [23], ViLT [16], và Stable Diffusion [28]). Các mô-đun màu xanh sử dụng xử lý hình ảnh và các chương trình con python khác. Các mô-đun này được gọi trong các chương trình được tạo từ hướng dẫn ngôn ngữ tự nhiên. Việc thêm các mô-đun mới để mở rộng khả năng của VISPROG là đơn giản (Mã 1).

và một số lượng nhỏ các ví dụ trong ngữ cảnh để tạo ra các chương trình phức tạp mà không cần bất kỳ huấn luyện nào¹. Các chương trình được tạo bởi VISPROG cũng sử dụng một mức độ trừu tượng cao hơn so với NMNs và gọi các mô hình được huấn luyện tối tân và các chương trình con python không neural (Hình 2). Những ưu điểm này làm cho VISPROG trở thành một hệ thống neuro-symbolic dễ sử dụng, hiệu năng cao và mô-đun.

VISPROG cũng có tính diễn giải cao. Thứ nhất, VISPROG tạo ra các chương trình dễ hiểu mà người dùng có thể xác minh tính đúng đắn logic. Thứ hai, bằng cách chia nhỏ dự đoán thành các bước đơn giản, VISPROG cho phép người dùng kiểm tra các đầu ra của các bước trung gian để chẩn đoán lỗi và nếu cần, can thiệp vào quá trình lý luận. Tổng thể, một chương trình được thực thi với các kết quả bước trung gian (ví dụ: văn bản, hộp giới hạn, mặt nạ phân đoạn, hình ảnh được tạo, v.v.) được liên kết với nhau để mô tả luồng thông tin phục vụ như một cơ sở lý luận trực quan cho dự đoán.

Để chứng minh tính linh hoạt của nó, chúng tôi sử dụng VISPROG cho 4 nhiệm vụ khác nhau chia sẻ một số kỹ năng chung (ví dụ: để phân tích hình ảnh) trong khi cũng yêu cầu một mức độ lý luận chuyên biệt và khả năng thao tác trực quan. Các nhiệm vụ này là - (i) trả lời câu hỏi trực quan tổng hợp; (ii) lý luận trực quan ngôn ngữ tự nhiên zero-shot (NLVR) trên các cặp hình ảnh; (iii) gắn thẻ đối tượng kiến thức thực tế từ hướng dẫn ngôn ngữ tự nhiên; và (iv) chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ. Chúng tôi nhấn mạnh rằng cả mô hình ngôn ngữ và bất kỳ mô-đun nào đều không được tinh chỉnh theo bất kỳ cách nào. Việc điều chỉnh VISPROG cho bất kỳ nhiệm vụ nào đơn giản như cung cấp một vài ví dụ trong ngữ cảnh bao gồm hướng dẫn ngôn ngữ tự nhiên và các chương trình tương ứng. Mặc dù dễ sử dụng, VISPROG cho thấy một cải thiện ấn tượng 2,7 điểm so với mô hình VQA cơ sở trên nhiệm vụ VQA tổng hợp, độ chính xác zero-shot mạnh mẽ 62,4% trên NLVR mà không bao giờ huấn luyện trên các cặp hình ảnh, và các kết quả định tính và định lượng thú vị trên các nhiệm vụ gắn thẻ kiến thức và chỉnh sửa hình ảnh.

¹Chúng tôi sử dụng "huấn luyện" để chỉ học tập dựa trên gradient để phân biệt nó với học tập trong ngữ cảnh chỉ liên quan đến một lần truyền tiến.

--- TRANG 3 ---
Các đóng góp chính của chúng tôi bao gồm - (i) VISPROG - một hệ thống sử dụng khả năng học tập trong ngữ cảnh của mô hình ngôn ngữ để tạo ra các chương trình trực quan từ hướng dẫn ngôn ngữ tự nhiên cho các nhiệm vụ trực quan tổng hợp (Phần 3); (ii) chứng minh tính linh hoạt của VISPROG trên các nhiệm vụ trực quan phức tạp như gắn thẻ đối tượng kiến thức thực tế và chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ (Phần 4.3 và 4.4) đã tránh khỏi hoặc thấy thành công hạn chế với một mô hình đầu cuối đơn; và (iii) tạo ra các cơ sở lý luận trực quan cho các nhiệm vụ này và cho thấy tiện ích của chúng cho phân tích lỗi và điều chỉnh hướng dẫn do người dùng điều khiển để cải thiện hiệu suất của VISPROG một cách đáng kể (Phần 5.3).

2. Công trình liên quan
Các phương pháp neuro-symbolic đã thấy động lực đổi mới nhờ vào khả năng hiểu biết, tạo sinh và học tập trong ngữ cảnh đáng kinh ngạc của các mô hình ngôn ngữ lớn (LLMs). Bây giờ chúng tôi thảo luận về các phương pháp tạo và thực thi chương trình trước đó cho các nhiệm vụ trực quan, công trình gần đây trong việc sử dụng LLMs cho thị giác, và tiến bộ trong các phương pháp lý luận cho các nhiệm vụ ngôn ngữ.

Tạo và thực thi chương trình cho các nhiệm vụ trực quan.
Neural module networks (NMN) [2] đi tiên phong trong các phương pháp mô-đun và tổng hợp cho nhiệm vụ trả lời câu hỏi trực quan (VQA). NMNs tổng hợp các mô-đun neural thành một mạng có thể vi phân đầu cuối. Mặc dù các nỗ lực ban đầu sử dụng các bộ phân tích cú pháp có sẵn [2], các phương pháp gần đây [9,10,12] học mô hình tạo bố cục cùng với các mô-đun neural bằng cách sử dụng REINFORCE [33] và giám sát trả lời yếu.

Mặc dù tương tự về tinh thần với NMNs, VISPROG có một số ưu điểm so với NMNs. Thứ nhất, VISPROG tạo ra các chương trình cấp cao gọi các mô hình neural được huấn luyện tối tân và các hàm python khác ở các bước trung gian thay vì tạo ra các mạng neural đầu cuối. Điều này làm cho việc kết hợp các mô-đun symbolic, không thể vi phân trở nên dễ dàng. Thứ hai, VISPROG tận dụng khả năng học tập trong ngữ cảnh của LLMs [5] để tạo ra các chương trình bằng cách nhắc LLM (GPT-3) với một hướng dẫn ngôn ngữ tự nhiên (hoặc một câu hỏi trực quan hoặc một phát biểu cần được xác minh) cùng với một vài ví dụ về hướng dẫn tương tự và các chương trình tương ứng của chúng, do đó loại bỏ nhu cầu huấn luyện các bộ tạo chương trình chuyên biệt cho mỗi nhiệm vụ.

LLMs cho các nhiệm vụ trực quan. LLMs và học tập trong ngữ cảnh đã được áp dụng cho các nhiệm vụ trực quan. PICa [34] sử dụng LLMs cho một nhiệm vụ VQA dựa trên kiến thức [20]. PICa biểu diễn thông tin trực quan trong hình ảnh dưới dạng văn bản thông qua chú thích, đối tượng và thuộc tính và đưa biểu diễn văn bản này vào GPT-3 cùng với câu hỏi và các ví dụ trong ngữ cảnh để tạo ra trực tiếp câu trả lời. Socratic models (SMs) [36], tổng hợp các mô hình được tiền huấn luyện từ các phương thức khác nhau như ngôn ngữ (BERT [7], GPT-2 [24]), thị giác-ngôn ngữ (CLIP [23]), và âm thanh-ngôn ngữ (mSLAM [3]), để thực hiện một số nhiệm vụ zero-shot, bao gồm chú thích hình ảnh, truy xuất video-thành-văn bản, và lập kế hoạch robot. Tuy nhiên, trong SMs, việc tổng hợp được xác định trước và cố định cho mỗi nhiệm vụ. Ngược lại, VISPROG xác định cách tổng hợp các mô hình cho mỗi trường hợp bằng cách tạo ra các chương trình dựa trên hướng dẫn, câu hỏi hoặc phát biểu. Chúng tôi chứng minh khả năng của VISPROG để xử lý các hướng dẫn phức tạp liên quan đến khả năng đa dạng (20 mô-đun) và đầu vào đa dạng (văn bản, hình ảnh, và cặp hình ảnh), trung gian (văn bản, hình ảnh, hộp giới hạn, mặt nạ phân đoạn), và các phương thức đầu ra (văn bản và hình ảnh). Tương tự như VISPROG, ProgPrompt [29] là một công trình đồng thời chứng minh khả năng của LLMs để tạo ra các kế hoạch hành động robot có vị trí giống python từ hướng dẫn ngôn ngữ tự nhiên. Trong khi các mô-đun ProgPrompt (như "find" hoặc "grab") nhận các chuỗi (thường là tên đối tượng) làm đầu vào, các chương trình VISPROG tổng quát hơn. Trong mỗi bước trong chương trình VISPROG, một mô-đun có thể chấp nhận nhiều đối số bao gồm chuỗi, số, biểu thức số học và logic, hoặc các đối tượng python tùy ý (như các thể hiện list() hoặc dict() chứa hộp giới hạn hoặc mặt nạ phân đoạn) được tạo ra bởi các bước trước đó.

Lý luận thông qua Prompting trong NLP. Có một thân tài liệu ngày càng tăng [14, 17] về việc sử dụng LLMs cho các nhiệm vụ lý luận ngôn ngữ thông qua prompting. Chain-of-Thought (CoT) prompting [32], trong đó một mô hình ngôn ngữ được nhắc với các ví dụ trong ngữ cảnh về đầu vào, cơ sở lý luận chuỗi suy nghĩ (một loạt các bước lý luận trung gian), và đầu ra, đã cho thấy khả năng ấn tượng để giải quyết các vấn đề lý luận toán học. Trong khi CoT dựa vào khả năng của LLMs để cả tạo ra một đường dẫn lý luận và thực thi nó, các phương pháp tương tự như VISPROG đã được áp dụng cho các nhiệm vụ ngôn ngữ, trong đó một decomposer prompt [15] được sử dụng đầu tiên để tạo ra một chuỗi các nhiệm vụ con sau đó được xử lý bởi các bộ xử lý nhiệm vụ con.

3. Lập trình Trực quan
Trong vài năm qua, cộng đồng AI đã tạo ra các mô hình hiệu suất cao, đặc thù cho nhiệm vụ cho nhiều nhiệm vụ thị giác và ngôn ngữ như phát hiện đối tượng, phân đoạn, VQA, chú thích và tạo văn bản thành hình ảnh. Mặc dù mỗi mô hình này giải quyết một vấn đề được định nghĩa rõ nhưng hẹp, các nhiệm vụ mà chúng ta thường muốn giải quyết trong thế giới thực thường rộng hơn và được định nghĩa lỏng lẻo.

Để giải quyết các nhiệm vụ thực tế như vậy, người ta phải hoặc thu thập một tập dữ liệu đặc thù cho nhiệm vụ mới, có thể tốn kém, hoặc tỉ mỉ tổng hợp một chương trình gọi nhiều mô hình neural, các chương trình con xử lý hình ảnh (ví dụ: thay đổi kích thước, cắt xén, lọc và chuyển đổi không gian màu hình ảnh), và tính toán khác (ví dụ: tra cứu cơ sở dữ liệu, hoặc các phép toán số học và logic). Việc tạo thủ công các chương trình này cho đuôi dài vô hạn của các nhiệm vụ phức tạp mà chúng ta gặp phải

--- TRANG 4 ---
Hình 3. Tạo chương trình trong VISPROG.

hàng ngày không chỉ đòi hỏi chuyên môn lập trình mà còn chậm, tốn nhiều lao động và cuối cùng không đủ để bao phủ không gian của tất cả các nhiệm vụ. Điều gì sẽ xảy ra nếu chúng ta có thể mô tả nhiệm vụ bằng ngôn ngữ tự nhiên và có một hệ thống AI tạo ra và thực thi chương trình trực quan tương ứng mà không cần bất kỳ huấn luyện nào?

Các mô hình ngôn ngữ lớn cho lập trình trực quan. Các mô hình ngôn ngữ lớn như GPT-3 đã cho thấy khả năng đáng chú ý để tổng quát hóa cho các mẫu mới cho một nhiệm vụ sau khi thấy một số ít ví dụ đầu vào và đầu ra trong ngữ cảnh. Ví dụ, việc nhắc GPT-3 với hai ví dụ dịch Anh-Pháp và một cụm từ tiếng Anh mới
good morning -> bonjour
good day -> bonne journée
good evening ->
tạo ra bản dịch tiếng Pháp "bonsoir". Lưu ý rằng chúng ta không phải tinh chỉnh GPT-3 để thực hiện nhiệm vụ dịch thuật trên cụm từ thứ ba. VISPROG sử dụng khả năng học tập trong ngữ cảnh này của GPT-3 để đầu ra các chương trình trực quan cho hướng dẫn ngôn ngữ tự nhiên.

Tương tự như các cặp dịch tiếng Anh và tiếng Pháp trong ví dụ trên, chúng tôi nhắc GPT-3 với các cặp hướng dẫn và chương trình cấp cao mong muốn. Hình 3 cho thấy một prompt như vậy cho nhiệm vụ chỉnh sửa hình ảnh. Các chương trình trong các ví dụ trong ngữ cảnh được viết thủ công và thường có thể được xây dựng mà không cần hình ảnh đi kèm. Mỗi dòng của chương trình VISPROG, hoặc một bước chương trình, bao gồm tên của một mô-đun, tên đối số đầu vào của mô-đun và giá trị của chúng, và tên biến đầu ra. Các chương trình VISPROG thường sử dụng các biến đầu ra từ các bước quá khứ làm đầu vào cho các bước tương lai. Chúng tôi sử dụng tên mô-đun có tính mô tả (ví dụ: "Select", "ColorPop", "Replace"), tên đối số (ví dụ: "image", "object", "query"), và tên biến (ví dụ: "IMAGE", "OBJ") để cho phép GPT-3 hiểu loại đầu vào và đầu ra, và chức năng của mỗi mô-đun. Trong quá trình thực thi, các biến đầu ra có thể được sử dụng để lưu trữ các loại dữ liệu tùy ý. Ví dụ "OBJ"s là danh sách các đối tượng trong hình ảnh, với mặt nạ, hộp giới hạn và văn bản (ví dụ: tên danh mục) liên kết với mỗi đối tượng.

Các ví dụ trong ngữ cảnh này được đưa vào GPT-3 cùng với một hướng dẫn ngôn ngữ tự nhiên mới. Mà không quan sát hình ảnh hoặc nội dung của nó, VISPROG tạo ra một chương trình (dưới cùng của Hình 3) có thể được thực thi trên (các) hình ảnh đầu vào để thực hiện nhiệm vụ được mô tả.

class VisProgModule():
def __init__(self):
# tải một mô hình được huấn luyện; di chuyển lên GPU
def html(self,inputs: List,output: Any):
# trả về một chuỗi html hình dung I/O bước
def parse(self,step: str):
# phân tích bước và trả về danh sách giá trị đầu vào
# và biến, và tên biến đầu ra
def execute(self,step: str,state: Dict):
inputs, input_var_names, output_var_name = \
self.parse(step)
# lấy giá trị của biến đầu vào từ state
for var_name in input_var_names:
inputs.append(state[var_name])
# thực hiện tính toán bằng mô hình được tải
output = some_computation(inputs)
# cập nhật state
state[output_var_name] = output
# tóm tắt trực quan của tính toán bước
step_html = self.html(inputs,output)
return output, step_html

Mã 1. Triển khai của một mô-đun VISPROG.

Các mô-đun. VISPROG hiện hỗ trợ 20 mô-đun (Hình 2) để kích hoạt các khả năng như hiểu hình ảnh, thao tác hình ảnh (bao gồm tạo sinh), truy xuất kiến thức, và thực hiện các phép toán số học và logic. Trong VISPROG, mỗi mô-đun được triển khai như một lớp Python (Mã 1) có các phương thức để: (i) phân tích dòng để trích xuất tên và giá trị đối số đầu vào, và tên biến đầu ra; (ii) thực thi tính toán cần thiết có thể liên quan đến các mô hình neural được huấn luyện và cập nhật trạng thái chương trình với tên và giá trị biến đầu ra; và (iii) tóm tắt tính toán của bước một cách trực quan bằng html (được sử dụng sau để tạo một cơ sở lý luận trực quan). Việc thêm các mô-đun mới vào VISPROG chỉ đơn giản là triển khai và đăng ký một

--- TRANG 5 ---
Hình 4. Các cơ sở lý luận trực quan được tạo bởi VISPROG. Các cơ sở lý luận này tóm tắt trực quan đầu vào và đầu ra của mỗi bước tính toán trong chương trình được tạo trong quá trình suy luận cho một nhiệm vụ chỉnh sửa hình ảnh (trên) và NLVR (dưới).

lớp mô-đun, trong khi việc thực thi các chương trình sử dụng mô-đun này được xử lý tự động bởi trình thông dịch VISPROG, được mô tả tiếp theo.

Thực thi Chương trình. Việc thực thi chương trình được xử lý bởi một trình thông dịch. Trình thông dịch khởi tạo trạng thái chương trình (một từ điển ánh xạ tên biến với giá trị của chúng) với các đầu vào, và bước qua chương trình từng dòng một trong khi gọi mô-đun đúng với các đầu vào được chỉ định trong dòng đó. Sau khi thực thi mỗi bước, trạng thái chương trình được cập nhật với tên và giá trị của đầu ra của bước.

Cơ sở Lý luận Trực quan. Ngoài việc thực hiện tính toán cần thiết, mỗi lớp mô-đun cũng triển khai một phương thức gọi là html() để tóm tắt trực quan các đầu vào và đầu ra của mô-đun trong một đoạn HTML. Trình thông dịch chỉ đơn giản ghép tóm tắt HTML của tất cả các bước chương trình thành một cơ sở lý luận trực quan (Hình 4) có thể được sử dụng để phân tích tính đúng đắn logic của chương trình cũng như kiểm tra các đầu ra trung gian. Các cơ sở lý luận trực quan cũng cho phép người dùng hiểu lý do thất bại và điều chỉnh hướng dẫn ngôn ngữ tự nhiên một cách tối thiểu để cải thiện hiệu suất. Xem Phần 5.3 để biết thêm chi tiết.

Hình 5. Chúng tôi đánh giá VISPROG trên một tập hợp đa dạng các nhiệm vụ. Các nhiệm vụ trải rộng nhiều đầu vào và đầu ra và tái sử dụng các mô-đun (Loc, FaceDet, VQA) bất cứ khi nào có thể.

4. Các nhiệm vụ
VISPROG cung cấp một khung linh hoạt có thể được áp dụng cho một loạt đa dạng các nhiệm vụ trực quan phức tạp. Chúng tôi đánh giá VISPROG trên 4 nhiệm vụ yêu cầu khả năng từ lý luận không gian, lý luận về nhiều hình ảnh, truy xuất kiến thức, và tạo sinh và thao tác hình ảnh. Hình 5 tóm tắt các đầu vào, đầu ra và mô-đun được sử dụng cho các nhiệm vụ này. Bây giờ chúng tôi mô tả các nhiệm vụ này, thiết lập đánh giá của chúng, và lựa chọn các ví dụ trong ngữ cảnh.

4.1. Trả lời Câu hỏi Trực quan Tổng hợp
VISPROG có tính tổng hợp theo cấu trúc làm cho nó phù hợp cho nhiệm vụ trả lời câu hỏi trực quan tổng hợp, đa bước: GQA [11]. Các mô-đun cho nhiệm vụ GQA bao gồm những mô-đun cho định vị từ vựng mở, một mô-đun VQA, các hàm để cắt xén các vùng hình ảnh được đưa ra tọa độ hộp giới hạn hoặc giới từ không gian (như above, left, v.v.), mô-đun để đếm hộp, và một mô-đun để đánh giá các biểu thức Python. Ví dụ, xem xét câu hỏi: "Chiếc xe tải nhỏ ở bên trái hay bên phải của những người đang đội mũ bảo hiểm?". VISPROG đầu tiên định vị "những người đội mũ bảo hiểm", cắt xén vùng bên trái (hoặc phải) của những người này, kiểm tra xem có "chiếc xe tải nhỏ" ở phía đó không, và trả về "left" nếu có và "right" nếu không. VISPROG sử dụng mô-đun trả lời câu hỏi dựa trên ViLT [16], nhưng thay vì chỉ đơn giản chuyển câu hỏi phức tạp gốc cho ViLT, VISPROG gọi nó cho các nhiệm vụ đơn giản hơn như xác định nội dung trong một bản vá hình ảnh. Kết quả là, VISPROG cho GQA của chúng tôi không chỉ có thể diễn giải hơn ViLT mà còn chính xác hơn (Bảng 1). Ngoài ra, người ta có thể hoàn toàn loại bỏ nhu cầu cho một mô hình QA như ViLT và sử dụng các hệ thống khác như CLIP và bộ phát hiện đối tượng, nhưng chúng tôi để lại điều đó cho nghiên cứu tương lai.

Đánh giá. Để hạn chế tiền bạc chi cho việc tạo chương trình với GPT-3, chúng tôi tạo một tập con của GQA để đánh giá. Mỗi câu hỏi trong GQA được chú thích với một loại câu hỏi. Để đánh giá trên một tập hợp đa dạng các loại câu hỏi (100 loại chi tiết), chúng tôi lấy mẫu ngẫu nhiên tối đa k mẫu cho mỗi loại câu hỏi từ các tập balanced val (k=5) và testdev (k=20).

--- TRANG 6 ---
Hình 6. Kết quả định tính cho chỉnh sửa hình ảnh (trên) và các nhiệm vụ gắn thẻ kiến thức (dưới).

Prompts. Chúng tôi chú thích thủ công 31 câu hỏi ngẫu nhiên từ tập balanced train với các chương trình VISPROG mong muốn. Việc chú thích câu hỏi với chương trình là dễ dàng và đòi hỏi viết ra chuỗi lý luận cần thiết để trả lời câu hỏi cụ thể đó. Chúng tôi cung cấp một tập con nhỏ hơn các ví dụ trong ngữ cảnh cho GPT-3, được lấy mẫu ngẫu nhiên từ danh sách này để giảm chi phí trả lời mỗi câu hỏi GQA.

4.2. Lý luận Zero-Shot trên Cặp Hình ảnh
Các mô hình VQA được huấn luyện để trả lời câu hỏi về một hình ảnh đơn. Trong thực tế, người ta có thể yêu cầu một hệ thống trả lời câu hỏi về một bộ sưu tập hình ảnh. Ví dụ, người dùng có thể yêu cầu một hệ thống phân tích album ảnh kỳ nghỉ của họ và trả lời câu hỏi: "Chúng ta đã ghé thăm địa danh nào, ngày sau khi chúng ta thấy Tháp Eiffel?". Thay vì tập hợp một tập dữ liệu đắt đỏ và huấn luyện một mô hình đa hình ảnh, chúng tôi chứng minh khả năng của VISPROG để sử dụng một hệ thống VQA hình ảnh đơn để giải quyết một nhiệm vụ liên quan đến nhiều hình ảnh mà không huấn luyện trên các ví dụ đa hình ảnh.

Chúng tôi trình bày khả năng này trên điểm chuẩn NLVR V2 [30], liên quan đến việc xác minh các phát biểu về cặp hình ảnh. Thông thường, việc giải quyết thách thức NLVR V2 đòi hỏi huấn luyện các kiến trúc tùy chỉnh nhận cặp hình ảnh làm đầu vào trên tập train của NLVR V2. Thay vào đó, VISPROG đạt được điều này bằng cách phân tách một phát biểu phức tạp thành các câu hỏi đơn giản hơn về các hình ảnh riêng lẻ và một biểu thức python liên quan đến các toán tử số học và logic và câu trả lời cho các câu hỏi cấp hình ảnh. Mô hình VQA ViLT-VQA được sử dụng để có câu trả lời cấp hình ảnh, và biểu thức python được đánh giá để xác minh phát biểu.

Đánh giá. Chúng tôi tạo một tập validation nhỏ bằng cách lấy mẫu 250 mẫu ngẫu nhiên từ tập dev của NLVR V2 để hướng dẫn lựa chọn prompt, và kiểm tra tổng quát hóa trên tập test công khai đầy đủ của NLVR V2.

Prompts. Chúng tôi lấy mẫu và chú thích các chương trình VISPROG cho 16 phát biểu ngẫu nhiên trong tập train của NLVR V2. Vì một số ví dụ này dư thừa (cấu trúc chương trình tương tự), chúng tôi cũng tạo một tập con được tuyển chọn gồm 12 ví dụ bằng cách loại bỏ 4 ví dụ dư thừa.

4.3. Gắn Thẻ Đối tượng Kiến thức Thực tế
Chúng ta thường muốn xác định người và đối tượng trong hình ảnh có tên không biết đối với chúng ta. Ví dụ, chúng ta có thể muốn xác định người nổi tiếng, chính trị gia, nhân vật trong chương trình truyền hình, lá cờ của các quốc gia, logo của các tập đoàn, ô tô phổ biến và nhà sản xuất của chúng, loài sinh vật, v.v. Giải quyết nhiệm vụ này đòi hỏi không chỉ định vị người, khuôn mặt và đối tượng mà còn tra cứu kiến thức thực tế trong cơ sở kiến thức bên ngoài để xây dựng một tập hợp các danh mục để phân loại, như tên của các nhân vật trong một chương trình truyền hình. Chúng tôi gọi nhiệm vụ này là Gắn Thẻ Đối tượng Kiến thức Thực tế hoặc viết tắt là Gắn Thẻ Kiến thức.

Để giải quyết Gắn Thẻ Kiến thức, VISPROG sử dụng GPT-3 như một cơ sở kiến thức ngầm có thể được truy vấn với các prompts ngôn ngữ tự nhiên như "Liệt kê các nhân vật chính trong chương trình truyền hình Big Bang Theory cách nhau bằng dấu phẩy." Danh sách danh mục được tạo này sau đó có thể được sử dụng bởi một mô-đun phân loại hình ảnh CLIP phân loại các vùng hình ảnh được tạo ra bởi các mô-đun định vị và phát hiện khuôn mặt. Bộ tạo chương trình của VISPROG tự động xác định có sử dụng bộ phát hiện khuôn mặt hay bộ định vị từ vựng mở tùy thuộc vào ngữ cảnh trong hướng dẫn ngôn ngữ tự nhiên. VISPROG cũng ước tính kích thước tối đa của danh sách danh mục được truy xuất. Ví dụ, "Gắn thẻ logo của 5 công ty ô tô hàng đầu của Đức" tạo ra một danh sách 5 danh mục, trong khi "Gắn thẻ logo của các công ty ô tô Đức" tạo ra một danh sách có độ dài tùy ý được xác định bởi GPT-3 với ngưỡng cắt ở 20. Điều này cho phép người dùng dễ dàng kiểm soát nhiễu trong quá trình phân loại bằng cách điều chỉnh hướng dẫn của họ.

Đánh giá. Để đánh giá VISPROG về nhiệm vụ này, chúng tôi chú thích 100 hướng dẫn gắn thẻ trên 46 hình ảnh yêu cầu kiến thức bên ngoài để gắn thẻ 253 thể hiện đối tượng bao gồm các nhân vật trong văn hóa đại chúng, chính trị, thể thao và nghệ thuật, cũng như nhiều loại đối tượng (ví dụ: ô tô, lá cờ, trái cây, thiết bị, đồ nội thất, v.v.). Đối với mỗi hướng dẫn, chúng tôi đo cả hiệu suất định vị và gắn thẻ thông qua precision (phần các hộp dự đoán đúng) và recall (phần các đối tượng ground truth được dự đoán đúng). Các metric gắn thẻ yêu cầu cả hộp giới hạn dự đoán và nhãn hoặc nhãn lớp liên kết phải đúng, trong khi định vị bỏ qua thẻ. Để xác định tính đúng đắn của định vị, chúng tôi sử dụng ngưỡng IoU là 0,5. Chúng tôi tóm tắt hiệu suất định vị và gắn thẻ bằng điểm F1 (trung bình điều hòa của precision và recall trung bình trên các hướng dẫn).

Prompts. Chúng tôi tạo 14 ví dụ trong ngữ cảnh cho nhiệm vụ này. Lưu ý rằng các hướng dẫn cho những ví dụ này được ảo hóa tức là không có hình ảnh nào được liên kết với những ví dụ này.

4.4. Chỉnh sửa Hình ảnh với Ngôn ngữ Tự nhiên
Tạo sinh văn bản thành hình ảnh đã có những bước tiến ấn tượng trong vài năm qua với các mô hình như DALL-E [26], Parti [35], và Stable Diffusion [28]. Tuy nhiên, vẫn vượt quá khả năng của các mô hình này để xử lý các prompts như "Ẩn khuôn mặt của Daniel Craig với :p" (khử nhận dạng hoặc bảo vệ quyền riêng tư), hoặc "Tạo một color pop của Daniel Craig và làm mờ nền" (làm nổi bật đối tượng) mặc dù những điều này tương đối đơn giản để đạt được theo chương trình bằng cách sử dụng kết hợp phát hiện khuôn mặt, phân đoạn và các mô-đun xử lý hình ảnh. Việc đạt được một chỉnh sửa tinh vi như "Thay thế Barack Obama bằng Barack Obama đeo kính râm" (thay thế đối tượng), đầu tiên yêu cầu xác định đối tượng quan tâm, tạo ra một mặt nạ của đối tượng cần được thay thế và sau đó gọi một mô hình image inpainting (chúng tôi sử dụng Stable Diffusion) với hình ảnh gốc, mặt nạ chỉ định các pixel cần thay thế, và một mô tả về các pixel mới cần tạo ra tại vị trí đó. VISPROG, khi được trang bị các mô-đun cần thiết và các chương trình ví dụ, có thể xử lý các hướng dẫn rất phức tạp một cách dễ dàng.

Đánh giá. Để kiểm tra VISPROG về các hướng dẫn chỉnh sửa hình ảnh cho khử nhận dạng, làm nổi bật đối tượng và thay thế đối tượng, chúng tôi thu thập 107 hướng dẫn trên 65 hình ảnh. Chúng tôi chấm điểm thủ công các dự đoán cho tính đúng đắn và báo cáo độ chính xác. Lưu ý rằng chúng tôi không phạt các hiện tượng trực quan

cho nhiệm vụ con thay thế đối tượng sử dụng Stable Diffusion miễn là hình ảnh được tạo ra đúng về mặt ngữ nghĩa.

Prompts. Tương tự như gắn thẻ kiến thức, chúng tôi tạo 10 ví dụ trong ngữ cảnh cho nhiệm vụ này mà không có hình ảnh đi kèm.

5. Thí nghiệm và Phân tích
Các thí nghiệm của chúng tôi đánh giá ảnh hưởng của số lượng prompts đối với hiệu suất GQA và NLVR (Phần 5.1), tổng quát hóa của VISPROG trên bốn nhiệm vụ so sánh các chiến lược prompting khác nhau (Phần 5.2), phân tích các nguồn lỗi cho mỗi nhiệm vụ (Hình 8), và nghiên cứu tiện ích của các cơ sở lý luận trực quan để chẩn đoán lỗi và cải thiện hiệu suất của VISPROG thông qua điều chỉnh hướng dẫn (Phần 5.3).

5.1. Ảnh hưởng của kích thước prompt
Hình 7 cho thấy hiệu suất validation tăng dần với số lượng ví dụ trong ngữ cảnh được sử dụng trong prompts cho cả GQA và NLVR. Mỗi lần chạy ngẫu nhiên chọn một tập con của các ví dụ trong ngữ cảnh được chú thích dựa trên một hạt giống ngẫu nhiên. Chúng tôi cũng thấy rằng việc bỏ phiếu đa số trên các hạt giống ngẫu nhiên dẫn đến hiệu suất tốt hơn một cách nhất quán so với hiệu suất trung bình trên các lần chạy. Điều này phù hợp với các phát hiện trong tài liệu lý luận Chain-of-Thought [32] cho các vấn đề lý luận toán học [31]. Trên NLVR, hiệu suất của VISPROG bão hòa với ít prompts hơn so với GQA. Chúng tôi tin rằng điều này là do các chương trình NLVR V2 yêu cầu ít mô-đun hơn và do đó ít minh chứng hơn để sử dụng các mô-đun đó so với GQA.

5.2. Tổng quát hóa
GQA. Trong Bảng 1, chúng tôi đánh giá các chiến lược prompting khác nhau trên tập testdev của GQA. Đối với kích thước prompt lớn nhất được đánh giá trên tập val (24 ví dụ trong ngữ cảnh), chúng tôi so sánh chiến lược ngẫu nhiên bao gồm prompt tốt nhất của VISPROG được chọn trong số 5 lần chạy trên tập validation (mỗi lần chạy ngẫu nhiên lấy mẫu các ví dụ trong ngữ cảnh từ 31 ví dụ được chú thích) và chiến lược bỏ phiếu đa số lấy dự đoán đồng thuận tối đa cho mỗi câu hỏi

Độ chính xác# ví dụ ngữ cảnh# ví dụ ngữ cảnh
GQA NLVR v2
Hình 7. Hiệu suất cải thiện với số lượng ví dụ trong ngữ cảnh trên các tập validation GQA và NLVR V2. Các thanh lỗi đại diện cho khoảng tin cậy 95% trên 5 lần chạy. Dự đoán từ cùng các lần chạy được sử dụng cho bỏ phiếu đa số. (Phần 5.1)

trên 5 lần chạy. Trong khi prompts "ngẫu nhiên" chỉ vượt trội một chút so với ViLT-VQA, việc bỏ phiếu dẫn đến một cải thiện đáng kể 2,7 điểm. Điều này là do việc bỏ phiếu trên nhiều lần chạy, mỗi lần với một tập hợp các ví dụ trong ngữ cảnh khác nhau, hiệu quả tăng tổng số ví dụ trong ngữ cảnh được thấy cho mỗi dự đoán. Chúng tôi cũng đánh giá một prompt được tuyển chọn thủ công bao gồm 20 ví dụ - 16 từ 31 ví dụ được chú thích, và 4 ví dụ ảo hóa bổ sung nhằm cung cấp bao phủ tốt hơn cho các trường hợp thất bại quan sát được trong tập validation. Prompt được tuyển chọn hoạt động tốt như chiến lược bỏ phiếu trong khi sử dụng ít tính toán hơn 5 lần, làm nổi bật triển vọng của kỹ thuật prompt engineering.

NLVR. Bảng 2 cho thấy hiệu suất của VISPROG trên tập test NLVR V2 và so sánh các chiến lược prompting ngẫu nhiên, bỏ phiếu và được tuyển chọn như đã làm với GQA. Trong khi VISPROG thực hiện nhiệm vụ NLVR zero-shot mà không bao giờ huấn luyện trên các cặp hình ảnh, chúng tôi báo cáo ViLT-NLVR, một mô hình VILT được tinh chỉnh trên NLVR V2 như một giới hạn trên về hiệu suất. Mặc dù kém vài điểm so với giới hạn trên, VISPROG cho thấy hiệu suất zero-shot mạnh mẽ chỉ sử dụng một mô hình VQA hình ảnh đơn để hiểu hình ảnh, và một LLM để lý luận. Lưu ý rằng, VISPROG sử dụng ViLT-VQA cho mô-đun VQA của nó được huấn luyện trên VQA V2 một nhiệm vụ trả lời câu hỏi hình ảnh đơn, nhưng không phải NLVR V2.

Gắn Thẻ Kiến thức. Bảng 3 cho thấy hiệu suất định vị và gắn thẻ cho nhiệm vụ Gắn Thẻ Kiến thức. Tất cả các hướng dẫn cho nhiệm vụ này không chỉ yêu cầu định vị từ vựng mở mà còn truy vấn cơ sở kiến thức để lấy các danh mục để gắn thẻ các đối tượng được định vị. Điều này làm cho nó trở thành một nhiệm vụ không thể thực hiện chỉ với các bộ phát hiện đối tượng. Với các hướng dẫn gốc, VISPROG đạt được điểm F1 ấn tượng 63,7% cho gắn thẻ, liên quan đến cả việc định vị đúng và đặt tên cho các đối tượng, và điểm F1 80,6% chỉ cho định vị. Các cơ sở lý luận trực quan trong VISPROG cho phép cải thiện hiệu suất thêm bằng cách sửa đổi các hướng dẫn. Xem Hình 6 cho các ví dụ định tính và Phần 5.3 để biết thêm chi tiết về điều chỉnh hướng dẫn.

Chỉnh sửa Hình ảnh. Bảng 4 cho thấy hiệu suất trên nhiệm vụ chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ. Hình 6 cho thấy phạm vi rộng các thao tác có thể thực hiện với tập hợp mô-đun hiện tại trong VISPROG bao gồm thao tác khuôn mặt, làm nổi bật một hoặc nhiều đối tượng trong hình ảnh thông qua các hiệu ứng phong cách như color popping và làm mờ nền, và thay đổi ngữ cảnh cảnh bằng cách thay thế các yếu tố chính trong cảnh (ví dụ: sa mạc).

5.3. Tiện ích của Các Cơ sở Lý luận Trực quan
Phân tích Lỗi. Các cơ sở lý luận được tạo bởi VISPROG cho phép phân tích triệt để các chế độ thất bại như được hiển thị trong Hình 8. Đối với mỗi nhiệm vụ, chúng tôi kiểm tra thủ công các cơ sở lý luận cho 100 mẫu để chia nhỏ các nguồn lỗi. Phân tích như vậy cung cấp một con đường rõ ràng hướng tới việc cải thiện

Phương pháp Chiến lược Prompting Lần chạy Ví dụ ngữ cảnh mỗi lần chạy Độ chính xác
VILT-VQA - 1 - 47.8
VISPROG được tuyển chọn 1 20 50.0
VISPROG ngẫu nhiên 1 24 48.2
VISPROG bỏ phiếu 5 24 50.5

Bảng 1. Kết quả GQA testdev. Chúng tôi báo cáo hiệu suất trên một tập con của tập testdev GQA gốc như được mô tả trong Phần 4.1.

Phương pháp Chiến lược Prompting Được tinh chỉnh Lần chạy Ví dụ ngữ cảnh mỗi lần chạy Độ chính xác
VILT-NLVR - ✓ 1 - 76.3
VISPROG được tuyển chọn ✗ 1 12 61.8
VISPROG ngẫu nhiên ✗ 1 16 61.3
VISPROG bỏ phiếu ✗ 5 16 62.4

Bảng 2. Kết quả NLVR V2 test. VISPROG thực hiện NLVR zero-shot tức là không huấn luyện bất kỳ mô-đun nào trên các cặp hình ảnh. ViLT-NLVR, một mô hình VILT được tinh chỉnh trên NLVR V2, phục vụ như một giới hạn trên.

Hướng dẫn Gắn thẻ Định vị
precision recall F1 precision recall F1
Gốc 69.0 59.1 63.7 87.2 74.9 80.6
Đã sửa đổi 77.6 73.9 75.7 87.4 82.5 84.9

Bảng 3. Kết quả gắn thẻ kiến thức. Bảng cho thấy hiệu suất trên các hướng dẫn gốc cũng như các hướng dẫn đã sửa đổi được tạo sau khi kiểm tra các cơ sở lý luận trực quan để hiểu các nguồn lỗi đặc thù cho từng trường hợp.

Gốc Đã sửa đổi
Độ chính xác 59.8 66.4

Bảng 4. Kết quả chỉnh sửa hình ảnh. Chúng tôi đánh giá thủ công mỗi dự đoán về tính đúng đắn ngữ nghĩa.

hiệu suất của VISPROG trên các nhiệm vụ khác nhau. Ví dụ, vì các chương trình không đúng là nguồn lỗi hàng đầu trên GQA ảnh hưởng đến 16% mẫu, hiệu suất trên GQA có thể được cải thiện bằng cách cung cấp thêm các ví dụ trong ngữ cảnh tương tự như các hướng dẫn mà VISPROG hiện đang thất bại. Hiệu suất cũng có thể được cải thiện bằng cách nâng cấp các mô hình được sử dụng để triển khai các mô-đun có lỗi cao thành những mô hình hiệu năng hơn. Ví dụ, việc thay thế mô hình ViLT-VQA bằng một mô hình VQA tốt hơn cho NLVR có thể cải thiện hiệu suất lên đến 24%. Tương tự, việc cải thiện các mô hình được sử dụng để triển khai các mô-đun "List" và "Select", nguồn lỗi chính cho các nhiệm vụ gắn thẻ kiến thức và chỉnh sửa hình ảnh, có thể giảm đáng kể các lỗi.

GQA NLVR
Gắn Thẻ Kiến thức Chỉnh sửa Hình ảnh
Hình 8. Các nguồn lỗi trong VISPROG.

Điều chỉnh hướng dẫn. Để hữu ích, một cơ sở lý luận trực quan cuối cùng phải cho phép người dùng cải thiện hiệu suất của hệ thống trên nhiệm vụ của họ. Đối với các nhiệm vụ gắn thẻ kiến thức và chỉnh sửa hình ảnh, chúng tôi nghiên cứu xem các cơ sở lý luận trực quan có thể giúp người dùng sửa đổi hoặc điều chỉnh các hướng dẫn để đạt được hiệu suất tốt hơn không. Hình 9 cho thấy các hướng dẫn đã sửa đổi: (i) dẫn đến một truy vấn tốt hơn cho mô-đun định vị (ví dụ: "thiết bị nhà bếp" thay vì "vật dụng"); (ii) cung cấp một truy vấn thông tin hơn cho truy xuất kiến thức (ví dụ: "CEO gần đây nhất của IBM" thay vì "CEO của IBM"); (iii) cung cấp tên danh mục (ví dụ: "table-merged") cho mô-đun Select để hạn chế tìm kiếm đến các vùng được phân đoạn thuộc danh mục được chỉ định; hoặc (iv) kiểm soát số lượng danh mục phân loại cho gắn thẻ kiến thức thông qua đối số max trong mô-đun List. Bảng 3 và 4 cho thấy điều chỉnh hướng dẫn dẫn đến cải thiện đáng kể cho các nhiệm vụ gắn thẻ kiến thức và chỉnh sửa hình ảnh.

--- TRANG 9 ---
Gốc: Gắn thẻ các Nhà vô địch Triwizard Tournament Lý do thất bại:
# List hạn chế độ dài đầu ra xuống 3
LIST0 = List(query='Triwizard Tournament Champions', max=3)

Lý do thành công:
# List đầu ra tất cả 4 nhà vô địch
LIST0 = List(query='Triwizard Tournament Champions', max=4)
Đã sửa đổi: Gắn thẻ 4 Nhà vô địch Triwizard Tournament

Gốc: Thay thế bàn cà phê bằng một bàn cà phê hiện đại với mặt kính
Đã sửa đổi: Thay thế bàn cà phê (table-merged) bằng một bàn cà phê hiện đại với mặt kính

Lý do thất bại:
# Mô-đun selection chọn một vùng không đúng (thảm)
OBJ1 = Select(query='coffee table', category=None)
Lý do thành công:
# Danh mục hạn chế không gian tìm kiếm
OBJ1 = Select(query='coffee table', category='table-merged')

Gốc: Gắn thẻ CEO của IBM
Đã sửa đổi: Gắn thẻ CEO gần đây nhất của IBM

Lý do thất bại:
# Truy vấn kiến thức trả về một trong những CEO trước đây của IBM
LIST0 = List(query='CEO of IBM',max=1)
Lý do thành công:
# Truy vấn kiến thức trả về CEO hiện tại của IBM
LIST0 = List(query='most recent CEO of IBM',max=1)

Gốc: Gắn thẻ vật dụng được sử dụng để pha cà phê
Lý do thất bại:
# Các mô-đun định vị thất bại trong việc phát hiện bất kỳ đối tượng nào
OBJ0 = Loc(image=IMAGE, object='item')

Lý do thành công:
# Các mô-đun định vị phát hiện nhiều thiết bị sau đó được lọc bởi Select
OBJ0 = Loc(image=IMAGE,object='kitchen appliance that makes coffee')
Đã sửa đổi: Gắn thẻ thiết bị nhà bếp được sử dụng để pha cà phê

Hình 9. Điều chỉnh hướng dẫn sử dụng các cơ sở lý luận trực quan. Bằng cách tiết lộ lý do thất bại, VISPROG cho phép người dùng sửa đổi hướng dẫn gốc để cải thiện hiệu suất.

6. Kết luận
VISPROG đề xuất lập trình trực quan như một cách đơn giản và hiệu quả để mang khả năng lý luận của LLMs vào các nhiệm vụ trực quan phức tạp. VISPROG chứng minh hiệu suất mạnh trong khi tạo ra các cơ sở lý luận trực quan có tính diễn giải cao. Việc nghiên cứu các chiến lược prompting tốt hơn và khám phá những cách mới để kết hợp phản hồi của người dùng để cải thiện hiệu suất của các hệ thống neuro-symbolic như VISPROG là một hướng thú vị để xây dựng thế hệ tiếp theo của các hệ thống thị giác mục đích chung.

7. Lời cảm ơn
Chúng tôi cảm ơn Kanchan Aggarwal đã giúp đỡ quá trình chú thích cho các nhiệm vụ chỉnh sửa hình ảnh và gắn thẻ kiến thức. Chúng tôi cũng biết ơn hệ sinh thái Hugging Face tuyệt vời đã đơn giản hóa việc sử dụng các mô hình neural tối tân để triển khai các mô-đun VISPROG.

Tài liệu tham khảo
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, và Karen Simonyan. Flamingo: một mô hình ngôn ngữ trực quan cho học tập few-shot. ArXiv, abs/2204.14198, 2022. 2

--- TRANG 10 ---
[2] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, và Dan Klein. Neural module networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), trang 39–48, 2016. 2, 3

[3] Ankur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin Johnson, Yong Cheng, Simran Khanuja, Jason Riesa, và Alexis Conneau. mslam: Massively multilingual joint pre-training for speech and text. ArXiv, abs/2202.01374, 2022. 3

[4] Gary Bradski. The opencv library. Dr. Dobb's Journal: Software Tools for the Professional Programmer, 25(11):120–123, 2000. 2

[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. 2, 3

[6] Bowen Cheng, Alexander G. Schwing, và Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. Trong NeurIPS, 2021. 2

[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019. 3

[8] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, và Derek Hoiem. Towards general purpose vision systems. ArXiv, abs/2104.00743, 2021. 2

[9] Ronghang Hu, Jacob Andreas, Trevor Darrell, và Kate Saenko. Explainable neural computation via stack neural module networks. Trong ECCV, 2018. 2, 3

[10] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, và Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. 2017 IEEE International Conference on Computer Vision (ICCV), trang 804–813, 2017. 2, 3

[11] Drew A. Hudson và Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 6693–6702, 2019. 5

[12] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, và Ross B. Girshick. Inferring and executing programs for visual reasoning. 2017 IEEE International Conference on Computer Vision (ICCV), trang 3008–3017, 2017. 2, 3

[13] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, và Aniruddha Kembhavi. Webly supervised concept expansion for general purpose vision models. Trong ECCV, 2022. 2

[14] Tushar Khot, Kyle Richardson, Daniel Khashabi, và Ashish Sabharwal. Learning to solve complex tasks by talking to agents. ArXiv, abs/2110.08542, 2021. 3

[15] Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, và Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. ArXiv, abs/2210.02406, 2022. 3

[16] Wonjae Kim, Bokyung Son, và Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. Trong ICML, 2021. 2, 5

[17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022. 3

[18] Jian Li, Yabiao Wang, Changan Wang, Ying Tai, Jianjun Qian, Jian Yang, Chengjie Wang, Jilin Li, và Feiyue Huang. Dsfd: Dual shot face detector. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 5055–5064, 2019. 2

[19] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, và Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. ArXiv, abs/2206.08916, 2022. 2

[20] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, và Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 3190–3199, 2019. 3

[21] Matthias Minderer, Alexey A. Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, và Neil Houlsby. Simple open-vocabulary object detection with vision transformers. ArXiv, abs/2205.06230, 2022. 2

[22] Zoe Papakipos và Joanna Bitton. Augly: Data augmentations for robustness. ArXiv, abs/2201.06494, 2022. 12

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, và cộng sự. Learning transferable visual models from natural language supervision. Trong International Conference on Machine Learning, trang 8748–8763. PMLR, 2021. 2, 3, 12

[24] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 3

[25] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683, 2020. 2

[26] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, và Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021. 7

[27] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley D. Edwards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, và Nando de Freitas. A generalist agent. ArXiv, abs/2205.06175, 2022. 2

--- TRANG 11 ---
[28] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 10674–10685, 2022. 2, 7

[29] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, và Animesh Garg. Progprompt: Generating situated robot task plans using large language models. ArXiv, abs/2209.11302, 2022. 3

[30] Alane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai, và Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. ArXiv, abs/1811.00491, 2019. 6

[31] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, và Denny Zhou. Self-consistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171, 2022. 7

[32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022. 3, 7

[33] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. 2, 3

[34] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, và Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. Trong AAAI, 2022. 3

[35] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, và Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. ArXiv, abs/2206.10789, 2022. 7

[36] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, và Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv, 2022. 3

--- TRANG 12 ---
A. Phụ lục
Phụ lục này bao gồm
• Prompts nhiệm vụ cho VISPROG (Phần A.1)
• Chi tiết triển khai mô-đun (Phần A.2)
• Nhiều kết quả định tính hơn với các cơ sở lý luận trực quan cho cả trường hợp thành công và thất bại có thể được tìm thấy tại https://prior.allenai.org/projects/visprog.

A.1. Prompts Nhiệm vụ
Chúng tôi cho thấy cấu trúc prompt cho GQA (Hình 10), NLVR (Hình 11), gắn thẻ kiến thức (Hình 13), và các nhiệm vụ chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ (Hình 12) với mỗi prompt có 3 ví dụ trong ngữ cảnh.

Hình 10. Prompt GQA

A.2. Chi tiết Mô-đun
Để giúp hiểu các chương trình được tạo tốt hơn, bây giờ chúng tôi cung cấp một vài chi tiết triển khai về một số mô-đun.

Select. Mô-đun nhận một đối số query và category. Khi category được cung cấp, việc lựa chọn chỉ được thực hiện trên các vùng đã được xác định là thuộc danh mục đó bởi một mô-đun trước đó trong chương trình (thường là mô-đun Seg). Nếu category là None, việc lựa chọn được thực hiện trên tất cả các vùng. Query là văn bản được sử dụng để chấm điểm vùng-văn bản để thực hiện lựa chọn. Chúng tôi sử dụng CLIP-ViT [23] để chọn vùng có điểm số tối đa cho query. Khi query chứa nhiều cụm từ được phân tách bằng dấu phẩy, vùng có điểm số cao nhất được chọn cho mỗi cụm từ.

Hình 11. Prompt NLVR

Hình 12. Prompt chỉnh sửa hình ảnh. Lưu ý rằng prompt bao gồm một ánh xạ của emoji đến tên của chúng trong thư viện AugLy [22] được sử dụng để triển khai mô-đun Emoji. Ví dụ thứ ba cho thấy cách cung cấp giá trị category cho mô-đun Select.

Classify. Mô-đun Classify nhận danh sách các vùng đối tượng và danh mục và cố gắng gán một trong các danh mục cho mỗi vùng. Để đơn giản, chúng tôi giả định các hình ảnh trong nhiệm vụ gắn thẻ có nhiều nhất 1 thể hiện của mỗi danh mục. Mô-đun Classify hoạt động khác nhau dựa trên việc danh sách danh mục có 1 hoặc nhiều phần tử. Nếu danh sách danh mục

--- TRANG 13 ---
Hình 13. Prompt gắn thẻ kiến thức. Lưu ý rằng prompt có một placeholder bổ sung để cấu hình giá trị max mặc định cho mô-đun List. Trong khi ví dụ đầu tiên suy luận max từ một hướng dẫn tự nhiên, ví dụ thứ ba chứng minh cách người dùng có thể tối thiểu tăng cường một hướng dẫn tự nhiên để cung cấp giá trị đối số.

chỉ có 1 phần tử, danh mục được gán cho vùng có điểm số CLIP cao nhất, tương tự như mô-đun Select. Khi nhiều hơn một danh mục được cung cấp, đầu tiên, mỗi vùng được gán danh mục có điểm số tốt nhất. Do lỗi phân loại, điều này có thể dẫn đến nhiều vùng được gán cùng một danh mục. Do đó, đối với mỗi danh mục được gán (trừ những danh mục không được gán cho bất kỳ vùng nào), chúng tôi thực hiện một bước khử trùng lặp chỉ giữ lại vùng có điểm số tối đa cho mỗi danh mục.

List. Mô-đun List sử dụng GPT3 để tạo một bộ truy xuất kiến thức linh hoạt và mạnh mẽ. Hình 14 cho thấy prompt được cung cấp cho GPT3 để truy xuất kiến thức thực tế.

Tạo danh sách cách nhau bằng dấu phẩy dựa trên truy vấn.
Truy vấn: Liệt kê nhiều nhất 3 màu cơ bản cách nhau bằng dấu phẩy
Danh sách: đỏ, xanh dương, xanh lá cây
Truy vấn: Liệt kê nhiều nhất 2 bang Bắc Mỹ cách nhau bằng dấu phẩy
Danh sách: California, Washington
Truy vấn: Liệt kê nhiều nhất {list_max} {new_query} cách nhau bằng dấu phẩy
Danh sách:

Hình 14. Prompt cho mô-đun List. list_max biểu thị độ dài danh sách tối đa mặc định và new_query là placeholder cho truy vấn truy xuất mới
