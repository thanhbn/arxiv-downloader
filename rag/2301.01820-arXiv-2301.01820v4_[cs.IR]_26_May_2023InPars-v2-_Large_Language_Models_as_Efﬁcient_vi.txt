# 2301.01820.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2301.01820.pdf
# Kích thước tệp: 62634 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2301.01820v4  [cs.IR]  26 Tháng 5 2023InPars-v2: Các Mô hình Ngôn ngữ Lớn như những
Trình tạo Tập dữ liệu Hiệu quả cho Tìm kiếm Thông tin
Vitor Jeronymo
NeuralMind, Brazil
FEEC-UNICAMP, BrazilLuiz Bonifacio
NeuralMind, Brazil
FEEC-UNICAMP, BrazilHugo Abonizio
NeuralMind, Brazil
FEEC-UNICAMP, Brazil
Marzieh Fadaee
Zeta Alpha, NetherlandsRoberto Lotufo
NeuralMind, Brazil
FEEC-UNICAMP, BrazilJakub Zavrel
Zeta Alpha, Netherlands
Rodrigo Nogueira
NeuralMind, Brazil
FEEC-UNICAMP, Brazil
Zeta Alpha, Netherlands
Tóm tắt
Gần đây, InPars đã giới thiệu một phương pháp để sử dụng hiệu quả các mô hình ngôn ngữ lớn
(LLMs) trong các tác vụ tìm kiếm thông tin: thông qua các ví dụ few-shot, một LLM được hướng dẫn
để tạo ra các truy vấn liên quan cho các tài liệu. Những cặp truy vấn-tài liệu tổng hợp này sau đó có thể
được sử dụng để huấn luyện một bộ truy xuất. Tuy nhiên, InPars và gần đây hơn là
Promptagator, dựa vào các LLMs độc quyền như GPT-3 và FLAN để tạo ra
các tập dữ liệu như vậy. Trong công trình này, chúng tôi giới thiệu InPars-v2, một trình tạo tập dữ liệu
sử dụng các LLMs mã nguồn mở và các bộ xếp hạng lại mạnh mẽ hiện có để lựa chọn các cặp
truy vấn-tài liệu tổng hợp để huấn luyện. Một pipeline truy xuất BM25 đơn giản theo sau bởi
một bộ xếp hạng lại monoT5 được tinh chỉnh trên dữ liệu InPars-v2 đạt được kết quả
state-of-the-art mới trên benchmark BEIR. Để cho phép các nhà nghiên cứu tiếp tục cải thiện
phương pháp của chúng tôi, chúng tôi mở mã nguồn, dữ liệu tổng hợp và các mô hình đã tinh chỉnh:
https://github.com/zetaalphavector/inPars/tree/master/legacy/inpars-v2
1 Giới thiệu và Bối cảnh
Augmentation dữ liệu đã là một công cụ đáng tin cậy để cải thiện hiệu quả của các mô hình AI đối mặt
với sự khan hiếm dữ liệu huấn luyện chất lượng cao trong miền cụ thể, đây là một vấn đề phổ biến trong các
ứng dụng thực tiễn. Công trình trước đây của Bonifacio et al. [1] và Dai et al. [2] đã thành công trong việc tận dụng
khả năng few-shot của các LLMs để tạo ra dữ liệu huấn luyện tổng hợp đáng tin cậy cho các
mô hình tìm kiếm thông tin. Những dữ liệu huấn luyện này đã giúp mô hình của họ đạt được kết quả state-of-the-art (SOTA) trên
benchmark BEIR [6].
Bonifacio et al. [1] đề xuất InPars nơi họ tạo ra các truy vấn từ các tài liệu trong kho ngữ liệu bằng cách sử dụng
LLMs. Tương tự như Bonifacio et al. [1], mô hình Promptagator [2] được công bố gần đây cũng cung cấp
các lời nhắc cho LLMs để tạo ra các truy vấn thay thế cho một tài liệu nhất định theo cách không giám sát.
Nó khác biệt chủ yếu với InPars ở chỗ nó sử dụng các lời nhắc cụ thể cho tập dữ liệu, một LLM lớn hơn để
tạo truy vấn, và một pipeline truy xuất có thể huấn luyện hoàn toàn với các mô hình nhỏ hơn.
Công trình này mở rộng phương pháp của Bonifacio et al. [1] bằng cách sử dụng một bộ xếp hạng lại như một cơ chế lọc
để lựa chọn các ví dụ được tạo tổng hợp tốt nhất và tiếp tục cải thiện hiệu quả truy xuất
Preprint.

--- TRANG 2 ---
trên BEIR. Chúng tôi cũng sử dụng một trình tạo truy vấn mã nguồn mở thay vì trình độc quyền được sử dụng bởi
Bonifacio et al. và cung cấp mã nguồn và dữ liệu để tái tạo kết quả của chúng tôi trên TPUs. Chúng tôi gọi
mô hình của Bonifacio et al. [1] là Inpars-v1 và mô hình được trình bày trong bài báo này là Inpars-v2.
2 Phương pháp
Trong phần này, chúng tôi giải thích các thí nghiệm mà chúng tôi đã thực hiện và cách chúng khác biệt với InPars-v1 [1].
Để tạo ra các truy vấn tổng hợp, chúng tôi sử dụng GPT-J [8] mã nguồn mở với 6B tham số để thay thế
mô hình curie của OpenAI được sử dụng trong InPars-v1. Đối với mỗi tập dữ liệu trong benchmark BEIR, chúng tôi lấy mẫu 100k
tài liệu từ kho ngữ liệu của nó và tạo ra một truy vấn tổng hợp cho mỗi tài liệu bằng cách sử dụng GPT-J được nhắc
với 3 ví dụ từ MS MARCO. Chúng tôi sử dụng giải mã tham lam và template lời nhắc "gbq" từ
InPars-v1. Một số kho ngữ liệu trong BEIR như ArguAna [7] có ít hơn 100k tài liệu. Trong những
trường hợp này, chúng tôi tạo ra nhiều truy vấn tổng hợp bằng số lượng tài liệu có trong kho ngữ liệu. Trung bình mất 30 giờ trên một GPU A100 để tạo ra 100k truy vấn.
Khi các truy vấn tổng hợp được tạo ra, chúng tôi áp dụng một bước lọc để lựa chọn các cặp truy vấn-tài liệu
có khả năng liên quan với nhau cao hơn. Trong InPars-v1, bước lọc này bao gồm việc lựa chọn
top 10k cặp truy vấn-tài liệu có xác suất log cao nhất để tạo ra một truy vấn cho
3 ví dụ shot và tài liệu làm đầu vào. Trong InPars-v2, chúng tôi sử dụng monoT5-3B [4] đã được tinh chỉnh
trên MS MARCO trong một epoch để ước tính điểm liên quan cho mỗi cặp truy vấn-tài liệu trong 100k cặp.
Sau đó, chúng tôi chỉ giữ lại top 10k cặp có điểm cao nhất làm các cặp truy vấn-tài liệu
tích cực cho việc huấn luyện. Mất khoảng 1.5 giờ để chấm điểm 100k cặp truy vấn-tài liệu trên một TPU
v3-8. Sẽ mất gấp đôi thời gian trên A100.
Để có được các mẫu tiêu cực (tức là, không liên quan) cặp truy vấn-tài liệu, chúng tôi lấy mẫu ngẫu nhiên một tài liệu
từ top 1000 được truy xuất bởi BM25 khi đưa ra truy vấn tổng hợp. Do đó, tập huấn luyện của chúng tôi bao gồm
10k cặp truy vấn-tài liệu tích cực và 10k cặp truy vấn-tài liệu tiêu cực.
Các bộ xếp hạng lại được tinh chỉnh theo cùng cách như trong InPars-v1: monoT5-3B được tinh chỉnh trên MS
MARCO trong một epoch và sau đó được tinh chỉnh thêm trong một epoch trên dữ liệu tổng hợp. Chúng tôi sử dụng
bộ tối ưu Adafactor [5] với tốc độ học tập không đổi là 1e-3. Mỗi batch có 64 cặp truy vấn-tài liệu tích cực và 64
cặp truy vấn-tài liệu tiêu cực được lấy mẫu ngẫu nhiên từ tập dữ liệu huấn luyện. Chúng tôi tinh chỉnh một mô hình
trên mỗi tập dữ liệu tổng hợp từ BEIR, tức là, chúng tôi có 18 bộ xếp hạng lại khác nhau, một cho mỗi tập dữ liệu,
sau đó được đánh giá trên các tập kiểm tra tương ứng. Tinh chỉnh trên mỗi tập dữ liệu tổng hợp mất
ít hơn 10 phút trên một TPU v3-8.
Đánh giá được thực hiện bằng cách sử dụng pipeline sau: đầu tiên chúng tôi sử dụng các chỉ mục phẳng của Pyserini [3] để
truy xuất một nghìn tài liệu cho mỗi truy vấn bằng cách sử dụng BM25 với các tham số mặc định (k1=0.9, b=0.4),
cho mỗi tập dữ liệu. Sau đó chúng tôi sử dụng các mô hình monoT5-3B đã tinh chỉnh để xếp hạng lại các tài liệu này.
3 Kết quả
Bảng 1 trình bày kết quả cho BM25 (cột thứ 2), monoT5-3B được tinh chỉnh trên MS MARCO (cột thứ 3),
monoT5-3b được tinh chỉnh trên MS MARCO và tiếp tục tinh chỉnh trên InPars-v1 (cột thứ 4), và
monoT5-3B được tinh chỉnh trên MS MARCO và sau đó tinh chỉnh trên dữ liệu InPars-v2 (cột thứ 5). So
với InPars-v1, phương pháp của chúng tôi tốt hơn đáng kể trên TREC-News, Climate-FEVER, Robust
và Touche. Ngoài ra, chúng tôi so sánh phương pháp của chúng tôi với Promptagator [2] và RankT5 [10]. Tính
đến trung bình của tất cả các tập dữ liệu BEIR, những kết quả này đại diện cho một state of the art mới trên BEIR.
Promptagator và RankT5 thành công trên các tập dữ liệu mà monoT5 và InPars-v2 thậm chí không thể vượt qua BM25,
như Touche và ArguAna. Lưu ý rằng những tập dữ liệu này tập trung vào truy xuất luận cứ, hơi
khác so với các tập dữ liệu khác trong benchmark BEIR. Do đó, chúng có lợi từ việc sử dụng các
lời nhắc tùy chỉnh. Promptagator làm điều này mà không sử dụng dữ liệu giám sát từ MS MARCO và sử dụng
các mô hình T5 nhỏ hơn với 110M tham số cho các bước truy xuất và xếp hạng lại.
1https://huggingface.co/castorini/monot5-3b-msmarco-10k
2Trái ngược với chỉ mục đa trường.
3Trong các thí nghiệm sơ bộ, chúng tôi cũng quan sát thấy sự cải thiện hơn 10 điểm nDCG@10 trên
ArguAna bằng cách sử dụng một lời nhắc cụ thể cho tập dữ liệu để tạo ra các truy vấn tổng hợp. Thêm chi tiết và kết quả trên
benchmark BEIR đầy đủ sẽ xuất hiện trong một bài báo sắp tới.
2

--- TRANG 3 ---
BM25monoT5-3BPrGator RankT5MARCO +InPars-v1 +InPars-v2
TREC-Covid 0.594 0.801 0.846 0.846 0.762 0.823
Robust 0.407 0.615 0.610 0.632 - -
FiQA 0.236 0.509 0.492 0.509 0.494 0.493
DBPedia 0.318 0.472 0.494 0.498 0.434 0.459
SciDocs 0.149 0.197 0.206 0.208 0.201 0.191
SciFact 0.678 0.774 0.774 0.774 0.731 0.760
NFCorpus 0.321 0.383 0.385 0.385 0.370 0.399
BioASQ 0.522 0.566 0.607 0.595 - 0.579
Natural Questions 0.305 0.625 0.625 0.638 - 0.647
HotpotQA 0.633 0.760 0.790 0.791 0.736 0.753
TREC-News 0.395 0.477 0.458 0.490 - -
Quora 0.788 0.835 0.874 0.845 - 0.819
FEVER 0.651 0.848 0.852 0.872 0.866 0.848
Climate-FEVER 0.165 0.288 0.287 0.323 0.241 0.275
Signal 0.328 0.302 0.319 0.308 - 0.319
ArguAna 0.397 0.379 0.371 0.369 0.630 0.406
Touche 0.442 0.309 0.260 0.291 0.381 0.486
CQADupstack 0.302 0.449 0.449 0.448 - -
Avg 0.424 0.533 0.539 0.545 - -
Avg PrGator 0.417 0.520 0.523 0.533 0.531 0.536
Bảng 1: nDCG@10 trên BEIR. "Avg PrGator" là trung bình của các tập dữ liệu được báo cáo bởi Promptagator.
Promptagator sử dụng một mô hình độc quyền, FLAN [9], để tạo ra các truy vấn tổng hợp. Mô hình RankT5
là một phiên bản được sửa đổi của bộ xếp hạng lại monoT5, nhưng checkpoint và mã của nó không được công bố. Trong
công trình này, chúng tôi làm cho mã, mô hình và dữ liệu mã nguồn mở và có sẵn công khai.
4 Kết luận
Trong công trình này, chúng tôi đã trình bày InPars-v2, một phiên bản cải tiến của InPars [1] sử dụng một
mô hình ngôn ngữ có sẵn công khai để tạo ra các truy vấn và một quy trình lựa chọn cặp truy vấn-tài liệu tốt hơn. Kết quả của chúng tôi
cho thấy rằng chúng tôi đạt được hiệu quả ngang bằng với state of the art trên BEIR. Dữ liệu tổng hợp và
các mô hình đã tinh chỉnh đã được phát hành công khai.
Lời cảm ơn
Nghiên cứu này được hỗ trợ một phần bởi Fundação de Amparo à Pesquisa do Estado de São Paulo
(FAPESP) (mã dự án 2022/01640-2). Chúng tôi cũng cảm ơn Centro Nacional de Processamento de Alto
Desempenho (CENAPAD-SP) và Google Cloud vì các khoản tín dụng tính toán.
Tài liệu tham khảo
[1] L. Bonifacio, H. Abonizio, M. Fadaee, và R. Nogueira. Inpars: Data augmentation for infor-
mation retrieval using large language models. arXiv preprint arXiv:2202.05144, 2022.
[2] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. B. Hall, và
M.-W. Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint
arXiv:2209.11755, 2022.
[3] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, và R. Nogueira. Pyserini: An easy-to-use
python toolkit to support replicable ir research with sparse and dense representations. arXiv
preprint arXiv:2102.10073, 2021.
[4] R. Nogueira, Z. Jiang, R. Pradeep, và J. Lin. Document ranking with a pretrained sequence-
to-sequence model. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: Findings, pages 708–718, 2020.
3

--- TRANG 4 ---
[5] N. Shazeer và M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In
International Conference on Machine Learning, pages 4596–4604. PMLR, 2018.
[6] N. Thakur, N. Reimers, A. Rücklé, A. Srivastava, và I. Gurevych. Beir: A heteroge-
neous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint
arXiv:2104.08663, 2021.
[7] H. Wachsmuth, S. Syed, và B. Stein. Retrieval of the best counterargument without prior
topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages 241–251, Melbourne, Australia, July 2018.
Association for Computational Linguistics.
[8] B. Wang và A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language
Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[9] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, và Q. V. Le.
Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[10] H. Zhuang, Z. Qin, R. Jagerman, K. Hui, J. Ma, J. Lu, J. Ni, X. Wang, và M. Bendersky.
Rankt5: Fine-tuning t5 for text ranking with ranking losses. arXiv preprint arXiv:2210.10634,
2022.
4
