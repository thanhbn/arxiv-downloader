# 2304.14732.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rag/2304.14732.pdf
# File size: 2923600 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Search-in-the-Chain: Interactively Enhancing Large Language
Models with Search for Knowledge-intensive Tasks
Shicheng Xu
CAS Key Laboratory of AI Security,
Institute of Computing Technology,
Chinese Academy of Sciences
University of Chinese Academy of
Sciences
Beijing, China
xushicheng21s@ict.ac.cnLiang Pangâˆ—
CAS Key Laboratory of AI Security,
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
pangliang@ict.ac.cnHuawei Shen
CAS Key Laboratory of AI Security,
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
shenhuawei@ict.ac.cn
Xueqi Cheng
CAS Key Laboratory of AI Security,
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
cxq@ict.ac.cnTat-Seng Chua
Sea-NExT Joint Lab, National
University of Singapore
Singapore
dcscts@nus.edu.sg
ABSTRACT
Making the content generated by Large Language Model (LLM),
accurate, credible and traceable is crucial, especially in complex
knowledge-intensive tasks that require multi-step reasoning and
each step needs knowledge to solve. Retrieval-augmented genera-
tion is good potential to solve this problem. However, where and
how to introduce Information Retrieval (IR) to LLM is a big chal-
lenge. Previous work has the problems that wrong knowledge re-
trieved by IR misleads the LLM and interaction between IR and
LLM breaks the reasoning chain of LLM. This paper proposes a
novel framework named Search-in-the-Chain (SearChain) for the
interaction between LLM and IR to solve the challenges. First, LLM
generates the reasoning chain named Chain-of-Query (CoQ) where
each node consists of an IR-oriented query-answer pair. Second,
IR verifies the answer of each node of CoQ. It corrects the answer
that is not consistent with the retrieved information when IR gives
high confidence, which improves the credibility. Third, LLM can
indicate its missing knowledge in CoQ and rely on IR to provide
this knowledge to LLM. These operations improve the accuracy in
terms of reasoning and knowledge. Finally, SearChain generates the
reasoning process and marks references to supporting documents
for each reasoning step, which improves traceability. Interaction
with IR in SearChain forms a novel reasoning path based on a
tree, which enables LLM to dynamically modify the direction of
reasoning. Experiments show that SearChain outperforms state-of-
the-art baselines on complex knowledge-intensive tasks including
multi-hop Q&A, slot filling, fact checking, and long-form Q&A.
âˆ—Corresponding author
This work is licensed under a Creative Commons Attribution
International 4.0 License.
WWW â€™24, May 13â€“17, 2024, Singapore, Singapore
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0171-9/24/05.
https://doi.org/10.1145/3589334.3645363CCS CONCEPTS
â€¢Computing methodologies â†’Natural language processing .
KEYWORDS
Retrieval-augmented model, Large Language Models
ACM Reference Format:
Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua.
2024. Search-in-the-Chain: Interactively Enhancing Large Language Models
with Search for Knowledge-intensive Tasks. In Proceedings of the ACM Web
Conference 2024 (WWW â€™24), May 13â€“17, 2024, Singapore, Singapore. ACM,
New York, NY, USA, 13 pages. https://doi.org/10.1145/3589334.3645363
1 INTRODUCTION
Large Language Models (LLMs) such as ChatGPT have shown
promising performance in various natural language processing
tasks [ 2,47]. However, for the complex knowledge-intensive tasks
that require multi-step reasoning and each step needs knowledge to
solve [ 23,43,49], many studies have shown that LLMs have trouble
in: (1) compositional reasoning over multiple knowledge [ 24], (2)
memorization of long-tail and real-time knowledge [ 13] and (3)
avoiding hallucination that is inconsistent with the facts [ 1], which
affects the accuracy and credibility of LLMs for complex knowledge-
intensive tasks. Besides, context-only generation without any sup-
porting evidence causes less traceability and makes people less
trust in the LLM-generated content. Retrieval-augmented method
has good potential to solve these problems because it combines the
knowledge of the model with external knowledge bases [ 10,12,18].
However, where and how to introduce IR into LLM is not a trivial
thing. There are three main challenges. C-1: Directly inserting IR
into the reasoning process of LLM such as Self-Ask [ 24], LTM [ 48],
React [ 42] and DSP [ 15] leads to breaking the reasoning chain of
LLM. Because in these methods, LLM can only reason a local sub-
question in each generation. C-2: When there is a conflict in the
knowledge of IR and LLM, for the knowledge that the LLM has
correctly memorized, it risks being misled by IR if IR retrieves thearXiv:2304.14732v7  [cs.CL]  24 Feb 2024

--- PAGE 2 ---
WWW â€™24, May 13â€“17, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua
Q
A
B
C
D
Q
    Tree-of-Reasoning (Trace.)Final Content
with ReferencesA
B
C
DE
F H
I
J
KL
M
NGQ
A
B
C
DIRA
B
C
DQ
E
F
GLLM
LLMA
B
C
DQ
E
F
GLLMA
B
C
DQ
E
F
G H
I
J
KLLM
LLM
L
M
N
Round 4 (CoQ Gen.) Round 3 (Verif. and Comp.)Round 3 (CoQ Gen.)Round 2 (CoQ Gen.) Round 2 (Verif.) Round 1 (Verif.) Round 1 (CoQ Gen.)IR IR
IRLLM
A
B
C
DQ
E
F
G H
I
J
KLLM
IR
IRA
B
C
DQ
E
F
GH
I
J
KLLM
L
M
NA
B
C
DQ
E
F
GH
I
J
KIR
IR
IRIRIR provides 
document 
and answer
IR provides 
document 
and answer
IR
Round 4 (Verif.)IR provides 
document 
and answerThe performer of Spirit If... is Kevin Drew [1]. Kevin 
Drew was born in Toronto [2]. Greyhound buses in 
Toronto leave from Toronto Coach Terminal [3]. So the 
final answer is Toronto Coach Terminal.
[1] Spirit If... is the debut solo album by Kevin Drew. It 
was released on September 18, 2007 â€¦
[2] Kevin Drew (born September 9, 1976 in Toronto) ..
[3] The Toronto Coach Terminal is the central bus 
station for inter-city services in Toronto, Ontario, 
Canada â€¦ when it was leased out in its entirety to bus 
lines Coach Canada and Greyhound Canada â€¦Where do greyhound buses that are in 
the birthplace of Spirit If...'s performer 
leave from? : Completion (need additional knowledge): Verification (need to be corrected): Verification (do not need to be corrected)
Q CoQ 1
CoQ 2
CoQ 3
CoQ 4
Figure 1: Interaction between IR and LLM in SearChain. First, SearChain makes LLM plan a CoQ where each node is a query-
answer pair. Then, IR interacts with each node of CoQ to perform verification and completion. If IR detects that a node needs to
be corrected or provided with knowledge, it gives feedback to LLM and LLM re-generates a new CoQ, which is the new branch
of the tree. This process is the node-identify Depth-first Search on a tree called Tree-of-Reasoning (the correct reasoning path
is green). The final content includes the reasoning process and references to supporting documents.
wrong information. It is important to make sure that IR only pro-
vides the knowledge that LLM really needs. C-3: Previous methods
cannot dynamically modify the reasoning direction.
In this paper, we propose a novel framework named Search-
in-the-Chain (SearChain) to effectively combine LLM with IR to
solve the above challenges (Figure 1). SearChain and previous meth-
ods both need multiple IR-LLM interaction rounds, but the former
works at the chain level, while the latter only deals with a node. In
each round, SearChain performs reasoning, verification, and com-
pletion. After the interaction, SearChain performs tracing to gener-
ate the final content. Specifically, in each round, first, LLM exploits
in-context learning to construct a Chain-of-Query (CoQ), which
is a reasoning chain to decompose and solve complex questions.
Each node of the chain consists of an IR-oriented query, the answer
generated by LLM for this query, and a flag indicating whether
LLM needs additional knowledge. Different from previous methods
in which LLM can only perform one-step reasoning (only a node)
when interacting with IR, CoQ is a complete chain. This design
avoids IR from breaking the reasoning chain ( C-1). Second, IR inter-
acts with each node of CoQ to perform verification and completion.
In verification, IR verifies the answer of each node. In case when the
LLM-generated answer is not consistent with the retrieved informa-
tion and IR gives high confidence, IR gives feedback to LLM to help
it correct the answer and re-generate the correct CoQ. In comple-
tion, IR determines whether the node has missing knowledge from
the flag of the node and provides this knowledge to LLM to help it
re-generate CoQ. LLM gradually generates the correct CoQ through
multiple rounds of interaction with IR. The above design provides
LLM with the knowledge it really needs to alleviate the misleading
caused by IR to LLM ( C-2), which improves accuracy. IR verifies
and corrects the knowledge in the reasoning process of LLM based
on external knowledge bases, which improves credibility. After the
interaction, SearChain performs tracing to generate the reasoningprocess and marks references to supporting documents for each
reasoning step, which is used as the final content returned to the
user. This improves the traceability of knowledge in the generated
content. Interaction with IR in SearChain transforms the reasoning
path from a chain to node-identify Depth-first Search on a tree
called Tree-of-Reasoning (ToR). CoQ generation can be seen as a
part of Depth-first Search and IR can identify the nodes that need
more information ( C-3). This enables LLM to dynamically modify
the reasoning direction. This paperâ€™s main contributions are:
(1)We highlight the challenges in introducing IR into LLM from
the perspectives of reasoning and knowledge.
(2)SearChain not only improves the knowledge-reasoning ability
of LLM but also uses IR to identify and give the knowledge that LLM
really needs. Besides, SearChain can mark references to supporting
documents for the knowledge involved in the generated content.
(3)Interaction with IR in SearChain forms a novel reasoning
path: node-identify Depth-first Search on a tree, which enables
LLM to dynamically modify the direction of reasoning.
(4)Experiment shows that SearChain outperforms state-of-the-
art baselines on complex knowledge-intensive tasks including multi-
hop Q&A, slot filling, fact checking and long-form Q&A. Code is
released at https://github.com/xsc1234/Search-in-the-Chain.
2 RELATED WORK
2.1 Chain-of-Thought Prompting
Chain-of-thought [ 36] proposes the method that uses few-shot ex-
amples to enable LLM to give intermediate reasoning results to
improve the reasoning ability. [ 16] uses "Letâ€™s do it step by step" as
prompt to achieve promising zero-shot performance. Auto-CoT ex-
ploits language models to automatically construct few-shot learning
examples for CoT [ 45]. There are also many studies that cover other
aspects of CoT such as self-consistency [ 35], usage of small and

--- PAGE 3 ---
Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks WWW â€™24, May 13â€“17, 2024, Singapore, Singapore
medium size models [ 44] and selection [ 8]. Besides, there are stud-
ies that iteratively use LLM to decompose complex questions and
answer sub-questions step by step. These methods include Least-to-
Most [ 48], Dynamic Least-to-Most [ 5], Self-Ask [ 24] and DSP [ 15].
Chain-of-Query of our method is also inspired by CoT. However,
previous studies focus on giving intermediate reasoning results or
decomposing complex questions and answering sub-questions step
by step. They focus on how to solve local sub-questions while ignor-
ing the global planning of the reasoning chain. Although AgentGPT
and PS [ 34] first plan each sub-question and then solve them, they
are not suitable for scenarios where the next sub-question needs the
answer of the previous sub-questions to generate, which is common
for complex knowledge-intensive tasks (multi-hop QA). CoQ of our
method makes LLM construct a global reasoning chain where each
node is a query-answer pair. This design not only improves the
knowledge-reasoning ability but also provides the interface for IR
to be deeply involved in the reasoning process of LLM.
2.2 Retrieval-augmented Language Models
Many studies have shown that retrieval-augmented methods get
promising performance in various natural language tasks such as
open-domain question answering [ 4,10,12,18,21,37,39], language
modeling [ 3,20,22] and enhancing the factuality [ 25]. Recently,
some studies enable LLM to interact with IR via in-context learn-
ing [15,24,28,42]. In these methods, the interaction between IR and
LLM makes the reasoning of LLM not continuous. LLM can only
perform one-step reasoning at each inference. Our method makes
LLM generate a global reasoning chain called Chain-of-Query at
each inference, which introduces stronger logical relationship be-
tween each reasoning step. Besides, previous methods can only
provide information to the LLM but cannot assist LLM in correct-
ing erroneous information or avoid the negative effect of IR on LLM,
which makes the reasoning of LLM still in a one-dimensional chain.
Our method makes IR interact with each node of the chain. IR only
provides LLM with its missing knowledge and corrects the answers
that are not consistent with the retrieved information when IR is
confident enough. This mitigates the negative effect of IR on LLM
and transforms the reasoning path from chain to node-identify
Depth First Search on a tree to enable LLM to dynamically modify
the reasoning direction.
3 OUR METHOD
This section introduces the design of Search-in-the-Chain (SearChain).
In SearChain, IR and LLM conduct multiple rounds of interaction.
In each round, first, LLM acts as the commander to plan the global
reasoning chain for the complex input questions called Chain-of-
Query (CoQ). Each node of the CoQ consists of an IR-oriented
query, the answer for this query, and a flag indicating whether LLM
needs additional knowledge. Then, IR interacts with each node of
CoQ and performs the completion and verification to only provide
LLM with missing knowledge and correct the wrong answers to
alleviate the misleading. LLM re-generates new CoQ based on feed-
back from IR. Multiple rounds of interaction help LLM gradually
generate the correct CoQ, which improves accuracy and credibility.
Finally, SearChain performs tracing to generate the whole reason-
ing process and marks references to supporting documents for each
ğ‘„ğ‘„A LLMIR
B LLMIR
LLMIR
A
B
C
D
 E
F
GH
I
J
L
M
N
KFinal Content
with References... Final AnswerPreviousï¼š
Oursï¼š ğ‘„ğ‘„local node local nodeFigure 2: Comparison with previous methods.
reasoning step, which is used as the final content returned to the
user. This improves the traceability of generated content. Interac-
tion with IR in SearChain transforms the reasoning path from a
chain to node-identify Depth-first Search on a tree called Tree-of-
Reasoning (ToR), which enables LLM to dynamically modify the
reasoning direction.
3.1 Comparison with Previous Methods
Figure 2 shows the difference between our method and previous
retrieval-augmented methods (Self-Ask [ 24], React [ 42], DSP [ 15],
etc.) in solving complex knowledge-intensive questions.
(1) Local vs. Global. For a complex question that needs multi-
step reasoning, previous methods directly insert IR into the multi-
step reasoning process, causing LLM can only reason a local sub-
question such as node Aâ—‹in each generation. This breaks the rea-
soning chain of LLM. Our method proposes Chain-of-Query to
provide the interactive interface for IR on the premise of ensuring
the coherence of reasoning chain (plans a global chain for question
ğ‘„such as Aâ—‹â†’Bâ—‹â†’Câ—‹â†’Dâ—‹in each generation). (solves C-1)
(2) Directly Provide vs. Verify and Complete. Previous methods
directly provide the retrieved information to the LLM. When the
retrieved information is incorrect, the LLM runs the risk of being
misled. In our method, IR only corrects inconsistent information
in Chain-of-Query when IR is confident enough, and provides the
information that LLM does not know via flags on Chain-of-Query,
which mitigates the negative effect of IR on LLM. (solves C-2)
(3) Chain vs. Tree. Previous methods cannot modify the reasoning
direction in time as necessary. Our method transforms the reasoning
path from a chain to node-identify Depth-first Search on a tree by
introducing the verification and completion from IR, which enables
LLM to dynamically modify the direction of reasoning. (solves C-3)

--- PAGE 4 ---
WWW â€™24, May 13â€“17, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua
Construct a global reasoning chain for this complex [Question] : Where do 
Greyhound buses that are in the birthplace of Spirit If...'s performer leave from? 
You should generate a query to the search engine based on what you already 
know at each step of the reasoning chain, starting with [Query]. 
If you know the answer for [Query], generate it starting with [Answer].
You can try to generate the final answer for the [Question] by referring to the 
[Query]-[Answer] pairs, starting with [Final Content].
If you don't know the answer, generate a query to search engine based on what 
you already know and do not know, starting with [Unsolved Query].
For example:
[Question]: Where do Greyhound buses that are in the birthplace of Spirit If...'s 
performer leave from? 
[Query 1]: Who is the performer of Spirit If... ?
If you don't know the answer:
[Unsolved Query]: Who is the performer of Spirit If... ?
If you know the answer:
[Answer 1]: The performer of Spirit Ifâ€¦ is Kevin Drew.
[Query 2]: Where was Kevin Drew born?
If you don't know the answer:
[Unsolved Query]: Where was Kevin Drew born?
If you know the answer:
[Answer 2]: Toronto.
[Query 3]: Where do greyhound buses in Toronto leave from?
If you don't know the answer:
[Unsolved Query]: Where do greyhound buses in Toronto leave from?
If you know the answer:
[Answer 3]: Toronto Coach Terminal.
[Final Content]: The performer of Spirit Ifâ€¦ is Kevin Drew [1]. Kevin Drew 
was born in Toronto [2]. Greyhound buses in Toronto leave from Toronto 
Coach Terminal [3]. So the final answer is Toronto Coach Terminal.Node 1
Node 2
Node 3
Figure 3: Prompt to make LLM generate Chain-of-Query.
3.2 Chain-of-Query Generation
In SearChain, we use in-context learning [ 36] to prompt large lan-
guage model to construct a global reasoning chain for complex
questionğ‘„named Chain-of-Query (CoQ):
CoQ=(ğ‘1,ğ‘1)â†’(ğ‘2,ğ‘2)â†’...â†’(ğ‘ğ‘›,ğ‘ğ‘›), (1)
which is the branch of Tree-of-Reasoning. Each node (ğ‘ğ‘–,ğ‘ğ‘–)of
CoQ consists of an IR-oriented query ğ‘ğ‘–and its answer ğ‘ğ‘–.ğ‘1...
ğ‘ğ‘›are the sub-questions that need to be solved in the reasoning
process of solving ğ‘„. CoQ generation is applied to each round of
interaction between LLM and IR. In the first round, the prompt
used to make LLM generate CoQ is shown in Figure 3. The prompt
with â€œ Construct a global reasoning chain â€ makes LLM know that the
main task is to generate a global reasoning chain in each genera-
tion. "Global" means that LLM needs to plan a complete reasoning
chain for the complex question, rather than answer the question
directly or only solve "local" sub-questions (comparison shown in
Figure 2). At each node of the chain, LLM focuses on generating the
IR-oriented query and gives the answer if LLM knows. If LLM does
not know the answer, it should mark the query with â€œ[Unsolved
Query]â€, which is a flag indicating the missing of knowledge. In sub-
sequent rounds, when a node needs IR to correct or provide missing
knowledge, LLM generates a new CoQ according to the feedback
of IR to dynamically modify the reasoning direction. The design for
this scenario will be introduced in Section 3.3. The generation of
CoQ is a complete Depth-first Search for ğ‘„, which avoids IR from
breaking the reasoning chain of LLM. Experiments (Section 4.3.3)
also show that for the difficult sub-question, CoQ enables LLM toAlgorithm 1: Description of the Interaction with IR.
Initialize: Processed queries: ğ‘€=ğ‘›ğ‘¢ğ‘™ğ‘™;
Correct reasoning path: ğ‘…=ğ‘›ğ‘¢ğ‘™ğ‘™;
Interaction rounds: ğ‘Ÿ=0;
Feedback:ğ¹=ğ‘›ğ‘¢ğ‘™ğ‘™; ToR: ğ‘»=ğ‘„;
Function IR(ğ‘ğ‘–,ğ‘ğ‘–):
ğ‘‘ğ‘–=Retrieval( ğ‘ğ‘–);// Retrieve Top-1 document ğ‘‘ğ‘–forğ‘ğ‘–.
ğ‘”,ğ‘“=Reader(ğ‘ğ‘–,ğ‘‘ğ‘–);
// Extract answer ğ‘”fromğ‘‘ğ‘–and give confidence ğ‘“.
ifğ‘ğ‘–is Unsolved Query then
// Completion.
ğ‘….add(ğ‘ğ‘–,ğ‘”,ğ‘‘ğ‘–);// Record the correct node.
return PromptForComplete( ğ‘ğ‘–,ğ‘”,ğ‘‘ğ‘–);
ifğ‘“>ğœƒandNotEqual(ğ‘”,ğ‘ğ‘–)then
// Verification.
ğ‘….add(ğ‘ğ‘–,ğ‘”,ğ‘‘ğ‘–);// Record the correct node.
return PromptForVerify( ğ‘ğ‘–,ğ‘”,ğ‘‘ğ‘–);
ğ‘….add(ğ‘ğ‘–,ğ‘ğ‘–,ğ‘‘ğ‘–);return â€œPassâ€ ;
Function Traverse( ğ‘ªğ’ğ‘¸ ):
foreach(ğ‘ğ‘–,ğ‘ğ‘–)inğ‘ªğ’ğ‘¸ do
ifnotDuplicateQuery( ğ‘ğ‘–,ğ‘€)then
// Ifğ‘ğ‘–has not been processed.
ğ¹=IR(ğ‘ğ‘–,ğ‘ğ‘–);ğ‘€.add(ğ‘ğ‘–);
ifnot F == â€œğ‘ƒğ‘ğ‘ ğ‘  â€then return ğ¹;
return â€œğ¹ğ‘–ğ‘›ğ‘–ğ‘ â„ â€ ;
Function Main(ğ‘„,ğ¹):
while not (F == â€œFinishâ€ or ğ‘Ÿ>ğ‘Ÿğ‘šğ‘ğ‘¥)do
ğ‘ªğ’ğ‘¸ =ChainGenerate( ğ‘„,ğ¹);
// LLM generate the new Chain-of-Query ğ‘ªğ’ğ‘¸ .
ğ‘».AddChild( ğ‘ªğ’ğ‘¸ );// Add the new branch to ğ‘».
ğ¹=Traverse( ğ‘ªğ’ğ‘¸ );// Interact with IR.
ğ‘Ÿ=ğ‘Ÿ+1;// Update the number of interaction rounds ğ‘Ÿ.
return Tracing( ğ‘»,ğ‘…)
solve it by more reasoning steps such as rewriting or further de-
composing the sub-question while baselines tend to stop reasoning.
It is because baselines focus on solving current local sub-questions
while ignoring the global planning of the reasoning chain. The
global perspective in CoQ makes LLM try more to explore possible
answers when facing intermediate difficulties.
3.3 Interaction with Information Retrieval
In each round of interaction, LLM passes the generated CoQ to
IR. IR verifies and completes the information for each node (ğ‘ğ‘–,ğ‘ğ‘–)
of CoQ and feeds back to LLM to help it generate more correct
CoQ as the new branch of ToR (Tree-of-Reasoning). Besides, IR
records the corresponding retrieved documents for each node of
CoQ as its supporting documents, which enhances the traceabil-
ity of LLM-generated content. The description of interaction is
shown in Algorithm 1. IR interacts with each node (ğ‘ğ‘–,ğ‘ğ‘–)of CoQ,
retrieves the Top-1 document ğ‘‘ğ‘–forğ‘ğ‘–as the supporting document,
and judges whether to verify or complete it according to the type of
ğ‘ğ‘–. When all the queries of CoQ do not need to be corrected or com-
pleted, or the maximum number of interaction rounds is reached,

--- PAGE 5 ---
Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks WWW â€™24, May 13â€“17, 2024, Singapore, Singapore
the interaction ends. SearChain traces back the correct reasoning
path of ToR and refers to each node of the path to generate the
final content with marked references to supporting documents for
knowledge of each node.
Verification. Verification aims to guarantee the correctness of
ğ‘ğ‘–in each node(ğ‘ğ‘–,ğ‘ğ‘–)of CoQ based on the external knowledge
base, which improves the accuracy and credibility of generated
content. Specifically, given the retrieved Top-1 document ğ‘‘ğ‘–forğ‘ğ‘–, a
Reader [ 14] that has been trained on open-domain QA datasets [ 14]
is used to extract the answer ğ‘”forğ‘ğ‘–fromğ‘‘ğ‘–with its confidence ğ‘“
(ğ‘“is a predicted value that measures whether ğ‘”can answer ğ‘ğ‘–):
ğ‘ =arg max(softmax(Hwğ‘ )),ğ‘’=arg max(softmax(Hwğ‘’)),
ğ‘”=ğ‘‘ğ‘–[ğ‘ :ğ‘’],ğ‘“=H[ğ¶ğ¿ğ‘†]wğ‘“,(wğ‘ ,wğ‘¡,wğ‘“âˆˆRğ¸),
where HâˆˆRğ¿Ã—ğ¸is the sequence of last hidden states for the input
text â€œ[CLS]ğ‘ğ‘–[SEP]ğ‘‘ğ‘–â€,ğ¿is the length and ğ¸is hidden dimension.
H[CLS] is the last hidden state of [CLS] token. Then, SearChain
judges whether the answer ğ‘ğ‘–given by LLM is consistent with
the retrieved information according to (1) whether ğ‘”appears in
ğ‘ğ‘–(for short-form generation tasks such as multi-hop QA and slot
filling) or (2) whether ROUGE [ 19] betweenğ‘ğ‘–andğ‘‘ğ‘–is greater than
the threshold ğ›¼(for long and free-form generation tasks such as
ELI5 [ 7]). Ifğ‘ğ‘–is not consistent with retrieved information and the
Reader is confident enough ( ğ‘“>ğœƒ,ğœƒis a threshold to alleviate the
negative effect of IR on LLM), a prompt is constructed to help LLM
correct the answer ğ‘ğ‘–. The template of the prompt is: â€œAccording
totheReference, theanswer forğ‘ğ‘–should beğ‘”,youcanchange your
answer andcontinue construct ingthereasoningchain for[Ques tion]:
ğ‘„.Reference:ğ‘‘ğ‘–. â€. This round is over. LLM receives the feedback of
IR, gives the new answer ğ‘â€²
ğ‘–forğ‘, and generates a new CoQ with
(ğ‘ğ‘–,ğ‘â€²
ğ‘–)as the root node, which is the new branch of ToR.
Completion. Completion aims to provide LLM with missing
knowledge in nodes of CoQ, which improves the accuracy of gen-
erated content. Specifically, in CoQ generation (Section 3.2), LLM
marks â€œ[Unsolved Query]â€ for the unsolvable query. For the unsolv-
able queryğ‘âˆ—
ğ‘–, IR extracts the answer ğ‘”âˆ—from retrieved document
ğ‘‘âˆ—
ğ‘–as described in Verification. Regardless of whether ğ‘“is greater
than the threshold ğœƒ,ğ‘”âˆ—andğ‘‘âˆ—
ğ‘–will be fed back to the LLM in the
form of a prompt because the LLM cannot solve ğ‘âˆ—
ğ‘–. The template of
the prompt is: â€œAccordingtotheReference, theanswer forğ‘âˆ—
ğ‘–should
beğ‘”âˆ—,youcangive your answer andcontinue construct ingtherea-
soningchain for[Ques tion]:ğ‘„.Reference:ğ‘‘âˆ—
ğ‘–. â€. This round is over.
LLM receives the feedback, gives the answer ğ‘âˆ—
ğ‘–to solve the query
ğ‘âˆ—
ğ‘–and generates a new CoQ with ( ğ‘âˆ—
ğ‘–,ğ‘âˆ—
ğ‘–) as the root node, which
is the new branch of ToR.
Tracing. Tracing aims to generate the reasoning process and
mark references to supporting documents for each reasoning step,
which is used as the final content returned to the user. This im-
proves the traceability of each knowledge in the generated content.
Specifically, SearChain records the documents retrieved for each
node on the correct reasoning path of Tree-of-Reasoning as the
supporting documents. SearChain prompts LLM to generate the
final content by referring to nodes on the correct path and marking
references to the supporting documents for the corresponding sub-
fragments of the generated content (final content of Figure 1). Theprompt is â€œYou cantrytogeneratethefinalanswer forthe[Ques -
tion] byreferring tothe[Query] -[Answer] pairs, start ingwith [Final
Content]. [Query 1]:ğ‘1.[Answer 1]:ğ‘1...[Queryğ‘š]:ğ‘ğ‘š.[Answerğ‘š]:
ğ‘ğ‘š. â€. This design enables the user to acquire the related documents
of the knowledge involved in each step of reasoning. We believe
that it is a promising task to mark references to supporting doc-
uments on sub-fragments of complex content generated by LLM.
Our approach provides a novel and effective approach to solve this
task by retrieving supporting documents for each sub-questions
involved in the reasoning process of LLM without any supervised
data (texts with citation annotations) and training of the LLM.
Node-identify Depth-first Search. Compared with previous
retrieval-augmented methods, interaction with IR in SearChain
forms a novel reasoning path: node-identify Depth-first Search
on a tree. In each generation, LLM generates a CoQ to perform
continuous reasoning on complex questions until the final answer
is generated or an unsolvable sub-question is encountered. This can
be seen as a part of Depth-first Search (DFS). However, different
from traditional DFS algorithm [ 31], "node-identify" in SearChain
means that when a search in one direction is terminated, SearChain
does not return to its parent node, but dynamically identifies the
node that needs to be corrected or completed via verification and
completion in IR and re-generates a new CoQ started with this
node. The interaction process between IR and LLM in SearChain is
the process of constructing a tree using node-identify DFS, which
enables LLM to dynamically modify the reasoning direction.
4 EXPERIMENTS
4.1 Experimental Setup
4.1.1 Datasets and Evaluation Metric .We select four classic
complex knowledge-intensive tasks including multi-hop question-
answering (HotpotQA (HoPo) [ 40], Musique (MQ) [ 33], WikiMulti-
HopQA (WQA) [ 11] and StrategyQA (SQA) [ 9]), slot filling (zsRE [ 17],
T-REx [ 6]), fact checking (FEVER [ 32]) and long-form question-
answering (ELI5 [ 7]). These tasks require LLM to perform multi-
step reasoning on complex questions, and each step requires cor-
responding knowledge to solve. As for the evaluation metrics, for
ELI5 whose ground truth is long and free-form, we use ROUGE-
L [19] as the metric. For other tasks, we use whether the ground
truth answer is contained within the generated answer (i.e, cover-
EM [ 26]) as the metric. Following DSP [ 15] and Self-Ask [ 24], we
evaluate the model on full development datasets of MQ and HoPo,
BIG-bench [ 29] datasets on SQA and subsets of WQA, zsRE, T-REx,
FEVER and ELI5 (each subset has 1.2k questions).
4.1.2 Baselines. Our baselines can be divided into two categories,
one is about improving the reasoning ability of LLM on complex
tasks (CoT [ 36], CoT-SC [ 35], Auto-CoT [ 45], Recite-and-answer [ 30]
and Least-to-Most [ 48]), and the other is not only introducing IR
to LLM but also improving the reasoning ability (Direct1, Self-
Ask [ 24], ToolFormer2[28], React [ 42], DSP [ 15], Verify-and-Edit
(combined with CoT-SC) [ 46] and Tree-of-Thought [ 41]). AgentGPT
and PS [ 34] use Plan-and-Solve paradigm, we also reproduce this
as one of the baselines.
1Retrieve documents and provide them to LLM in a prompt.
2Perform ToolFormer on gpt-3.5-turbo via in-context learning.

--- PAGE 6 ---
WWW â€™24, May 13â€“17, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua
Table 1: Performance of SearChain and baselines on complex knowledge-intensive tasks. Bold denotes the best result in
different settings. FC: Fact Checking, LFQA: Long-Form QA. Metric for LFQA: ROUGE-L. Metric for others: cover-EM.
Muti-Hop QA Slot Filling FC LFQA
HoPo MQ WQA SQA zsRE T-REx FEV. ELI5
Without Information Retrieval
Direct Prompting 31.95 5.91 25.82 66.25 22.75 43.85 73.45 21.90
Auto-CoT 33.53 10.55 29.15 65.40 21.30 43.98 76.61 21.55
CoT 35.04 9.46 30.41 65.83 22.36 44.51 76.98 21.79
CoT-SC 36.85 10.02 32.68 70.84 24.74 46.06 77.15 22.05
Recite-and-answer 36.49 10.97 32.53 70.47 24.98 46.14 77.35 22.10
Self-Ask w/o IR 33.95 11.10 35.65 65.45 20.16 44.71 75.31 21.73
Least-to-Most 34.05 11.45 32.88 65.78 21.86 44.98 75.98 21.95
Plan-and-Solve 36.33 12.95 35.68 73.21 25.15 47.58 77.08 22.23
SearChain w/o IR 38.36 13.61 40.49 75.62 30.14 52.69 77.06 22.54
Interaction with Information Retrieval
Direct Retrieval 34.09 10.22 30.01 66.78 52.29 59.28 78.25 23.40
ToolFormer 36.75 12.98 35.49 67.02 51.35 59.17 80.79 23.05
Self-Ask 40.05 14.28 39.58 67.65 50.51 59.12 79.41 23.25
Plan-and-Solve w/ IR 41.65 15.07 42.05 74.58 52.15 60.03 81.04 24.56
Reactâ†’CoT-SC 43.15 15.49 40.36 70.43 53.27 60.42 80.59 24.05
Verify-and-Edit 44.03 15.57 40.83 71.09 53.95 61.10 80.67 23.80
Tree-of-Thought w/ IR 50.65 15.61 42.49 72.55 54.88 62.40 81.03 24.20
DSP 51.97 15.83 43.52 72.41 54.35 61.32 80.65 23.46
SearChain 56.91 17.07 46.27 76.95 57.29 65.07 81.15 25.57
- w/o Verification 46.11 14.70 42.67 75.98 43.58 55.46 78.79 22.98
- w/o Completion 53.05 15.86 43.64 76.53 45.78 56.03 80.03 25.02
4.1.3 Implementation .The large language model we used is gpt-
3.5-turbo provided from API of OpenAI3and the retrieval model
we used is ColBERTv2 [ 27] (following DSP). IR model infers on
one Tesla V100 GPU. For HotpotQA, we use Wikipedia 2017 as
the corpus, which is provided by [ 40] in full-wiki setting. For the
other datasets, we use the large-scale passage collection built on
Wikipedia as the corpus [ 14,38]. Baselines with information re-
trieval are in the same setting as SearChain. We reproduce all base-
lines on gpt-3.5-turbo following the settings in their papers. The
maximum number of interaction rounds ğ‘Ÿğ‘šğ‘ğ‘¥ is5. The thresholds
ğ›¼andğœƒare set as 0.35 and 1.5 respectively. As for the selection
of confidence threshold ( ğœƒ), we initialize the initial value of the
confidence threshold (1.0) based on prior knowledge and gradually
increase the value with a step size of 0.1. We validate the F1-score
on the mixed open-domain QA datasets (NQ, TriviaQA, WebQ, and
TREC) after each value change. We find that when the confidence
threshold is 1.5, the highest F1-score can be achieved so we set
the confidence threshold as 1.5. As for the selection of ROUGE
threshold (ğ›¼), we determine this value by observing the ROUGE
relationship between the generated text and the ground truth in
the few examples in in-context learning. Our further experiments
show that when the value range of ROUGE threshold is between
0.3 and 0.5, the performance change on ELI5 is not obvious. De-
tails of prompts and experiments are introduced in Section A.3 of
Appendix.
3https://openai.com/api/4.2 Main Results
Performance on knowledge-intensive tasks is shown in Table 1.
(1) Effect of Chain-of-Query. CoQ is the reasoning chain for
complex questions in SearChain. We compare it with recent com-
petitive baselines in the setting without IR. SearChain w/o IR out-
performs all baselines based on CoT (CoT, Auto-CoT, CoT-SC and
Recite-and-answer), which indicates that focusing on construct-
ing a global reasoning chain consisting of sub-questions is better
than just giving intermediate reasoning results. SearChain w/o IR
outperforms Self-Ask w/o IR and Least-to-Most, which indicates
that it is more effective to focus on constructing a global reasoning
chain at each inference (global perspective) than generating and
answering sub-questions step by step (local perspective).
(2) Effect of interaction with IR. In the setting with interaction
with IR, SearChain again outperforms all the baselines. The para-
digm of first generating global CoQ, and then IR interacting with
each node of CoQ ensures the coherence of LLM reasoning. This
solves the problem in Self-Ask, DSP and React. Besides, SearChain
decouples the knowledge of LLM and IR. IR judges whether to
provide information to LLM according to the confidence and the
flag of the node on CoQ, which effectively alleviates misleading
LLM. Last but not least, baselines reason in the one-dimensional
chain. They cannot dynamically modify the reasoning direction.
Interaction with IR in SearChain transforms the reasoning path
from a chain to node-identify Depth-first Search on a tree, which
enables LLM to dynamically modify the reasoning direction.

--- PAGE 7 ---
Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks WWW â€™24, May 13â€“17, 2024, Singapore, Singapore
Table 2: Distribution of knowledge sources.
Knowledge Src. HoPo MQ WQA SQA
LLM 74.56% 78.83% 75.83% 94.98%
Corrected by IR 20.94% 14.60% 18.96% 2.78%
Completed by IR 4.50% 6.57% 5.21% 2.24%
Table 3: Positive and negative effects of IR on LLM.
(a) Accuracy on Sğ¼ğ‘…andS(positive effectâ†‘).
HoPo MQ WQA SQA
w/o IR ( S) 38.36 13.61 40.49 75.62
w/o IR ( Sğ¼ğ‘…) 31.38 10.20 32.60 68.96
w IR ( Sğ¼ğ‘…) 60.86 18.49 50.52 78.42
(b) Percentage that IR misleads LLM (negative effect â†“).
HoPo MQ WQA SQA
Self-Ask 15.76 14.32 25.76 10.29
React 17.68 15.22 25.99 10.03
Plan-and-Solve w/ IR 16.42 15.25 22.31 7.59
Verify-and-Edit 9.78 10.75 16.44 6.52
Tree-of-Thought w/ IR 12.07 13.25 20.52 8.46
DSP 14.72 14.03 24.31 9.22
SearChain 6.33 6.50 12.71 5.31
4.3 Analysis
In this section, we discuss and demonstrate the advantages of
SearChain compared to baselines in detail. First, we analyze the
source of the knowledge of SearChain in solving complex ques-
tions. Second, while we analyze the positive effect of IR on LLM in
solving difficult questions, we also demonstrate that our method
can better mitigate the negative effect of IR on LLM. Third, we
show the advantages of SearChain compared to baselines in terms
of reasoning and tracing capabilities. Last but not least, we perform
efficiency analysis to show our method significantly improves task
performance with no significant increase in time consumption.
4.3.1 Knowledge Decoupling .We analyze the knowledge sources
on the four multi-hop QA datasets. Specifically, we classify knowl-
edge sources into three categories: (1) knowledge of LLM, (2) knowl-
edge that corrected by IR in verification, and (3) knowledge that
LLM does not know and is provided by IR in completion. We use
node of ToR as the statistical granularity to calculate the percentage
of nodes from these three sources in the total nodes respectively.
The experimental results are shown in Table 2. It is worth noting
that even though most of the knowledge comes from LLM, this
knowledge is also verified by IR. IR only corrects the inconsistent
answer given by LLM when it is confident enough and provides
LLM with the missing knowledge, which alleviates the negative
effect of IR on LLM and improves the utilization of retrieved infor-
mation. On StrategyQA, LLM has memorized most knowledge that
IR can retrieve, so IR provides less knowledge than other datasets.Table 4: Number of reasoning steps. SearChain tries more for
unsolvable sub-questions to achieve better accuracy.
2-hop 3-hop 4-hop Accuracy
CoT 2.25 2.23 2.19 9.46
Self-Ask w/o IR 2.04 2.21 2.15 11.10
Least-to-Most 2.52 2.68 2.70 11.45
SearChain w/o IR 4.16 4.66 5.06 13.61
It is not clear what entity Robert J Chassell is a member of. The question cannot be answered.CoT
Self-Ask w/o Retrieval
Follow up: Who is Robert J Chassell? 
Intermediate answer: Robert J Chassell is a programmer and author.
Follow up: What organization is Robert J Chassell a member of? 
Intermediate answer: The organization that Robert J Chassell is a member of is not specified.
SearChain w/o Retrieval
[Query 1]: What is Robert J Chassell a member of?
[Answer 1]: There is no information about what organization or group Robert J Chassell is a member of.
[Query 2]: Can we find any information about Robert J Chassell's affiliations or associations?
[Answer 2]: Yes, Robert J Chassell is identified as a member of the Free Software Foundation.
[Query 3]: Who founded the Free Software Foundation?
[Answer 3]: The Free Software Foundation was founded by Richard Stallman.
[Final Content]: Richard Stallman founded the Free Software Foundation [3], which Robert J Chassell is 
a member of [2].  So the final answer is Richard Stallman. Who founded what Robert J Chassell is a member of?
Figure 4: Case study of the difference between SearChain and
baselines for unsolvable sub-questions.
4.3.2 Positive and Negative Effects of IR on LLM .(1) Positive.
In SearChain, IR can identify the trouble of LLM and effectively
help LLM to correct the answers and acquire missing knowledge.
We select the questions ( Sğ¼ğ‘…) that IR helps to correct or provide
knowledge from the datasets used in Table 1 ( S) and evaluate the
accuracy of SearChain on Sğ¼ğ‘…. We also evaluate the accuracy of
SearChain w/o IR on Sğ¼ğ‘…. The results in Table 3(a) show that w/o
IR performs worse on Sğ¼ğ‘…than on S, which indicates that LLM
does have trouble with the questions that require IR help. w/ IR
performs better on Sğ¼ğ‘…, which indicates that IR effectively identifies
and solves the trouble of LLM. (2) Negative. Section 1 points out the
risk of IR misleading LLM when there is a conflict in the knowledge
of IR and LLM. We select the questions ( Sğ‘¡) that LLM can give
the correct answers to and count the percentage that LLM gives
incorrect answers after adding IR on Sğ‘¡. Table 3(b) shows SearChain
effectively mitigates the negative effect of IR on LLM. SearChain
uses the confidence of IR and the information of CoQ to judge
whether to correct LLM or provide LLM with its missing knowledge.
4.3.3 CoQ vs Baselines in Reasoning .CoQ performs better
on reasoning complex questions than the baselines. In addition to
Table 1, we further analyze the reasoning ability from two aspects:
(1) Number of Reasoning Steps. We analyze the number of
reasoning steps in different methods in the setting without IR. We
conduct the experiment on Musique because Musique has more
complex questions. Table 4 shows the average number of reason-
ing steps on questions with different hops. Our method has more
reasoning steps, and the number of reasoning steps increases with
the hops of the question. This shows that our method has a better
perception of the complexity of the questions.

--- PAGE 8 ---
WWW â€™24, May 13â€“17, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua
New Bing
Iâ€™m sorry, but Icouldnâ€™t find any
information about afungus described by
Elsie Maud Wakefield asâ€œLepiota â€ina
London botanical garden .Ididfind that
Elsie Maud Wakefield wasamycologist at
theRoyal Botanic Gardens, Kew from
1910 to1951 and was aspecialist in
Basidiomycetes and an international
authority onAphyllophorales [1,2].
[1]:â€¦Elsie Maud Wakefield, was an
English mycologist and plant
pathologist â€¦
[2]:â€¦Elsie Wakefield wasaspecialist in
Basidiomycetes andaninternational auth-
ority onAphyllophorales .â€¦SearChain
Raymond Ochoa isan
American child actor
andvoice actor [1].He
hasplayed various roles
in movies including
Arlo in"The Good
Dinosaur" [2].Arlo is
anApatosaurus [3].So
the final answer is
Apatosaurus .
[1]:Raymond Ochoa is
anAmerican child actor
andvoice actor â€¦
[2]:Raymond Ochoa â€¦
played, Arlo, inthe
Pixar film "The Good
Dinosaurâ€œ ...
[3]â€¦ timid Apato -
saurus named Arlo
(Ochoa) â€¦SearChain
Jimmy Bartel was thereci-
pient ofthe2007 Brown -low
Medal [1]and played
Australian rules football
professionally [2].Heplayed
for the Geelong Football
Club in the Australian
Football League [2].Sothe
final answer isGeelong
Football Club .
[1]:â€¦Jimmy Bartel won the
medal during the2007 AFL
season â€¦
[2]:â€¦Jimmy Bartel isa
former Australian rules
footballer plays for the
Geelong Football Club ...What team did the recipient of
the 2007 Brownlow Medal play for? What kind of dinosaur does Raymond Ochoa 
voice in the Pixar film "The Good Dinosaur"? 
New Bing
Raymond Ochoa vo-
ices the character of
Arlo, anApatosaurus
inthePixar film â€œThe
Good Dinosaurâ€ [1]
[1]:(Amazon .com :The
Good Dinosaur :Ray-
mond Ochoa)SearChain
Elise Maud Wakefield was anEnglish
mycologist and plant pathologist [1].
"Lepiota "isagenus offungi [2].Kew
Gardens isknown foritsfungi collection [3],
such as"Lepiota viridigleba "[4].Sothefinal
answer isKew Gardens .
[1]:â€¦Elsie Maud Wakefield, wasanEnglish
mycologist andplant pathologist â€¦
[2]:â€¦Lepiota viridigleba isaspecies ofseq-
uestrate fungus inthefamily Agaricaceae â€¦
[3]â€¦Kew Gardens isabotanical garden in
southwest London thathouses the"largest and
most diverse botanical and mycological
collections intheworld" ...
[4]â€¦Thecollection offungi atKew Gardens
includes species such asâ€¦"Lepiota viri-
digleba â€œâ€¦In what London botanical garden can you find the 
fungus described by Elise Maud Wakefield as " Lepiota "?
New Bing
Therecipient ofthe2007
Brownlow Medal was
Jimmy Bartel who
played fortheGeelong
Football Club .Hewon
the medal by polling
twenty -nine votes during
the2007 AFL season [1].
Isthere anything else
you would like toknow
about thistopic?
[1]:â€¦Jimmy Bartel
won themedal bypolling
twenty -nine votes during
the2007 AFL season â€¦
Figure 5: Case study of SearChain and New Bing in marking references to supporting documents.
(2) Solving Difficult Sub-questions. The baselines focus on
solving local sub-questions while ignoring the global planning of
the reasoning chain. This leads LLM to tend to stop reasoning rather
than try more when a sub-question cannot be solved. In our method,
LLM acts as a commander that plans a global reasoning chain that
can solve the complex question, when a sub-question cannot be
solved, even without the help of IR, LLM can try to further decom-
pose or rewrite the sub-question to continue reasoning. It is because
our method focuses on building a global chain that can solve the
complex question (global perspective), rather than answering or
generating the sub-questions step by step (local perspective). Case
study in Figure 4 shows that CoT and Self-Ask stop the reasoning
while SearChain continues reasoning by rewriting the sub-question.
More reasoning steps in Table 4 also support that SearChain can
try more for difficult sub-questions. More case studies are shown
in Section A.1.1 of Appendix.
4.3.4 SearChain vs New Bing in Tracing .We compare the
performance of SearChain and New Bing in marking references for
generated content via case study (Figure 5). We further propose two
metrics to evaluate the Scope of Knowledge Coverage and Accuracy
of Marking Position to show traceability more intuitively:
â€¢Scope of Knowledge Coverage (SKC) [0, +] : The number of
knowledge items marked with supporting documents in the gener-
ated content. (statistics, SearChain (2.882) is better than New Bing
(1.143))
â€¢Accuracy of Marking Position (AMP) [0, 1] : The accuracy of
the position of the reference marks. That is, whether the references
are correctly marked on the sub-fragments for the corresponding
knowledge in the generated content. (human evaluation, SearChain
(0.80) is better than New Bing (0.45))
We introduce three humans with masterâ€™s degrees to participate
in our human evaluation and the results show that SearChain can
mark references for each knowledge involved in the reasoning
process (i.e., correct nodes of CoQ) in a fine-grained manner. While
the references given by New Bing do not cover all of the knowledgeTable 5: Efficiency analysis.
#ğ‘›â†“#ğ‘šâ†“#ğ‘Ÿâ†“ğ‘¡(ğ‘ )â†“Perf. (Avg)â†‘
Self-Ask 401 63 2.19 6.63 46.73
Plan-and-Solve w/ IR 450 71 1 6.05 48.89
Reactâ†’CoT-SC 938 110 2.35 8.25 48.47
Verify-and-Edit 565 307 2.40 13.90 48.88
Tree-of-Thought w/ IR 622 341 2.29 13.28 50.47
DSP 1759 155 2.15 10.47 50.44
SearChain 390 189 2.21 8.52 53.29
and cannot be marked on the correct position. More case studies
are shown in Section A.1.2 of Appendix.
4.3.5 Efficiency Analysis .We analyze the running efficiency
between SearChain and baselines on the number of words in the
input (ğ‘›) and output ( ğ‘š) text of LLM, number of rounds of interac-
tion between LLM and IR ( ğ‘Ÿ) and overall running time ( ğ‘¡). Table 5
shows our method significantly improves task performance with
no significant increase in time consumption. Most baselines also
require multiple rounds of interaction between IR and LLM.
5 CONCLUSION
In this paper, we point out the challenges of introducing IR into
LLM from the perspectives of reasoning and knowledge. We then
propose a novel framework named SearChain to enable IR and LLM
to interact with each other effectively. SearChain not only stimu-
lates the knowledge-reasoning ability of LLM but also uses IR to
provide the knowledge that LLM really needs based on the external
knowledge base, which improves both accuracy and credibility. Be-
sides, SearChain can mark references to supporting documents for
the knowledge involved in the generated content, which improves
the traceability of the content. In addition, the interaction between
IR and LLM in SearChain transforms the reasoning path from a
chain to node-identify Depth-first Search on a tree, which enables

--- PAGE 9 ---
Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks WWW â€™24, May 13â€“17, 2024, Singapore, Singapore
LLM to dynamically modify the reasoning direction. Experimental
results on complex knowledge-intensive tasks show that SearChain
performs better than all baselines.
ACKNOWLEDGMENTS
This work was supported by the National Key R&D Program of
China (2022YFB3103700, 2022YFB3103704), the National Natural
Science Foundation of China (NSFC) under Grants No. 62276248
and U21B2046, and the Youth Innovation Promotion Association
CAS under Grants No. 2023111.
REFERENCES
[1]Razvan Azamfirei, Sapna R Kudchadkar, and James Fackler. 2023. Large language
models and the perils of their hallucinations. Critical Care 27, 1 (2023), 1â€“2.
[2]Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan
Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,
and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation of
ChatGPT on Reasoning, Hallucination, and Interactivity. CoRR abs/2302.04023
(2023). arXiv:2302.04023 https://doi.org/10.48550/arXiv.2302.04023
[3]Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, et al .2022. Improv-
ing Language Models by Retrieving from Trillions of Tokens. In International
Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Mary-
land, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 2206â€“2240.
https://proceedings.mlr.press/v162/borgeaud22a.html
[4]Xue-Qi Cheng and Hua-Wei Shen. 2010. Uncovering the community structure
associated with the diffusion dynamics on networks. Journal of Statistical Me-
chanics: Theory and Experiment 2010, 04 (2010), P04024.
[5]Andrew Drozdov, Nathanael SchÃ¤rli, Ekin AkyÃ¼rek, Nathan Scales, Xinying Song,
Xinyun Chen, Olivier Bousquet, and Denny Zhou. 2023. Compositional Semantic
Parsing with Large Language Models. In The Eleventh International Conference
on Learning Representations . https://openreview.net/forum?id=gJW8hSGBys8
[6]Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, et al .2018. T-REx: A Large Scale
Alignment of Natural Language with Knowledge Base Triples. In Proceedings of
the 2018 Conference on LREC . European Language Resources Association (ELRA),
Miyazaki, Japan. https://aclanthology.org/L18-1544
[7]Angela Fan, Yacine Jernite, Ethan Perez, et al .2019. ELI5: Long Form Question
Answering. In Proceedings of the 2019 Conference on ACL . Association for Compu-
tational Linguistics, Florence, Italy, 3558â€“3567. https://doi.org/10.18653/v1/P19-
1346
[8]Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023.
Complexity-Based Prompting for Multi-step Reasoning. In The Eleventh Interna-
tional Conference on Learning Representations . https://openreview.net/forum?id=
yf1icZHC-l9
[9]Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan
Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with
Implicit Reasoning Strategies. Transactions of the Association for Computational
Linguistics (TACL) (2021).
[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
2020. REALM: Retrieval-Augmented Language Model Pre-Training. CoRR
abs/2002.08909 (2020). arXiv:2002.08909 https://arxiv.org/abs/2002.08909
[11] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.
Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Rea-
soning Steps. In Proceedings of the 2020 Conference on COLING . International
Committee on Computational Linguistics, Barcelona, Spain (Online), 6609â€“6625.
https://doi.org/10.18653/v1/2020.coling-main.580
[12] Gautier Izacard and Edouard Grave. 2020. Leveraging Passage Retrieval with
Generative Models for Open Domain Question Answering. CoRR abs/2007.01282
(2020). arXiv:2007.01282 https://arxiv.org/abs/2007.01282
[13] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raf-
fel. 2022. Large Language Models Struggle to Learn Long-Tail Knowledge.
arXiv:2211.08411 [cs.CL]
[14] Vladimir Karpukhin, Barlas Oguz, Sewon Min, et al .2020. Dense Passage Retrieval
for Open-Domain Question Answering. In Proceedings of the 2020 Conference on
EMNLP . Association for Computational Linguistics, Online, 6769â€“6781. https:
//doi.org/10.18653/v1/2020.emnlp-main.550
[15] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, et al .2023. Demonstrate-Search-
Predict: Composing retrieval and language models for knowledge-intensive NLP.
arXiv:2212.14024 [cs.CL]
[16] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwa-
sawa. 2022. Large Language Models are Zero-Shot Reasoners. In Advances in Neu-
ral Information Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=e2TBb5y0yFf[17] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-Shot
Relation Extraction via Reading Comprehension. In Proceedings of the 2017 Confer-
ence on CoNLL) . Association for Computational Linguistics, Vancouver, Canada,
333â€“342. https://doi.org/10.18653/v1/K17-1034
[18] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock-
tÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Genera-
tion for Knowledge-Intensive NLP Tasks. In Proceedings of the 2020 Conference
on NeurIPS .
[19] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.
InText Summarization Branches Out . Association for Computational Linguistics,
Barcelona, Spain, 74â€“81. https://aclanthology.org/W04-1013
[20] Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Ha-
jishirzi, and Luke Zettlemoyer. 2022. Nonparametric Masked Language Mod-
eling. CoRR abs/2212.01349 (2022). https://doi.org/10.48550/arXiv.2212.01349
arXiv:2212.01349
[21] Xiangyang Mou, Chenghao Yang, Mo Yu, Bingsheng Yao, Xiaoxiao Guo, Saloni
Potdar, and Hui Su. 2021. Narrative Question Answering with Cutting-Edge
Open-Domain QA Techniques: A Comprehensive Study. Trans. Assoc. Comput.
Linguistics 9 (2021), 1032â€“1046. https://doi.org/10.1162/tacl_a_00411
[22] Shuzi Niu, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2012. Top-k learning to
rank: labeling, ranking and evaluation. In Proceedings of the 35th international
ACM SIGIR conference on Research and development in information retrieval . 751â€“
760.
[23] Fabio Petroni, Aleksandra Piktus, Angela Fan, et al .2021. KILT: a Benchmark for
Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference on
NAACL . Association for Computational Linguistics, Online, 2523â€“2544. https:
//doi.org/10.18653/v1/2021.naacl-main.200
[24] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike
Lewis. 2023. Measuring and Narrowing the Compositionality Gap in Language
Models. https://openreview.net/forum?id=PUwbwZJz9dO
[25] Hongjing Qian, Yutao Zhu, Zhicheng Dou, et al .2023. WebBrain: Learning to
Generate Factually Correct Articles for Queries by Grounding on Large Web
Corpus. arXiv:2304.04358 [cs.CL]
[26] Corby Rosset, Chenyan Xiong, Minh Phan, et al .2020. Knowledge-Aware
Language Model Pretraining. CoRR abs/2007.00655 (2020). arXiv:2007.00655
https://arxiv.org/abs/2007.00655
[27] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei
Zaharia. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late
Interaction. In Proceedings of the 2022 Conference on NAACL . Association for
Computational Linguistics, Seattle, United States, 3715â€“3734. https://doi.org/10.
18653/v1/2022.naacl-main.272
[28] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli,
Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer:
Language Models Can Teach Themselves to Use Tools. arXiv:2302.04761 [cs.CL]
[29] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb,
Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta,
AdriÃ  Garriga-Alonso, et al .2022. Beyond the Imitation Game: Quantifying and
extrapolating the capabilities of language models. arXiv:2206.04615 [cs.CL]
[30] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023.
Recitation-Augmented Language Models. In The Eleventh International Conference
on Learning Representations . https://openreview.net/forum?id=-cqvvvb-NkI
[31] Robert Tarjan. 1971. Depth-first search and linear graph algorithms. In 12th
Annual Symposium on Switching and Automata Theory (swat 1971) . 114â€“121.
https://doi.org/10.1109/SWAT.1971.10
[32] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In
Proceedings of the 2018 Conference on NAACL . Association for Computational
Linguistics, New Orleans, Louisiana, 809â€“819. https://aclanthology.org/N18-1074
[33] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
2022. MuSiQue: Multihop Questions via Single-hop Question Composition.
Transactions of the Association for Computational Linguistics 10 (2022), 539â€“554.
https://doi.org/10.1162/tacl_a_00475
[34] Lei Wang, Wanyu Xu, Yihuai Lan, et al .2023. Plan-and-Solve Prompting: Im-
proving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.
arXiv:2305.04091 [cs.CL]
[35] Xuezhi Wang, Jason Wei, Dale Schuurmans, et al .2023. Self-Consistency Im-
proves Chain of Thought Reasoning in Language Models. In The Eleventh Inter-
national Conference on Learning Representations . https://openreview.net/forum?
id=1PL1NIMMrw
[36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc
Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in
Large Language Models. CoRR abs/2201.11903 (2022). arXiv:2201.11903 https:
//arxiv.org/abs/2201.11903
[37] Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. 2022. Match-Prompt:
Improving Multi-task Generalization Ability for Neural Text Matching via Prompt
Learning. In Proceedings of the 31st ACM International Conference on Information
& Knowledge Management . 2290â€“2300.

--- PAGE 10 ---
WWW â€™24, May 13â€“17, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua
[38] Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. 2023. BERM: Training
the Balanced and Extractable Representation for Matching to Improve General-
ization Ability of Dense Retrieval. arXiv preprint arXiv:2305.11052 (2023).
[39] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-
aware Reranking-Truncation Joint Model for Search and Retrieval-augmented
Generation. arXiv preprint arXiv:2402.02764 (2024).
[40] Zhilin Yang, Peng Qi, Saizheng Zhang, et al .2018. HotpotQA: A Dataset for
Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018
Conference on EMNLP . Association for Computational Linguistics, Brussels, Bel-
gium, 2369â€“2380. https://doi.org/10.18653/v1/D18-1259
[41] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao,
and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving
with Large Language Models. arXiv:2305.10601 [cs.CL]
[42] Shunyu Yao, Jeffrey Zhao, Dian Yu, et al .2023. ReAct: Synergizing Reasoning and
Acting in Language Models. In The Eleventh International Conference on Learning
Representations . https://openreview.net/forum?id=WE_vluYUL-X
[43] Da Yin, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei Chang, Furu Wei, and
Jianfeng Gao. 2022. A Survey of Knowledge-Intensive NLP with Pre-Trained
Language Models. arXiv:2202.08772 [cs.CL]
[44] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrap-
ping Reasoning With Reasoning. In Advances in Neural Information Processing
Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
(Eds.). https://openreview.net/forum?id=_3ELRdg2sgI
[45] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic
Chain of Thought Prompting in Large Language Models. In The Eleventh Interna-
tional Conference on Learning Representations . https://openreview.net/forum?id=
5NTt8GFjUHkr
[46] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023.
Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework. In Pro-
ceedings of the 2023 Conference on ACL . Association for Computational Linguistics,
Toronto, Canada, 5823â€“5840. https://doi.org/10.18653/v1/2023.acl-long.320
[47] Wayne Xin Zhao, Kun Zhou, Junyi Li, et al .2023. A Survey of Large Language
Models. arXiv:2303.18223 [cs.CL]
[48] Denny Zhou, Nathanael SchÃ¤rli, Le Hou, et al .2022. Least-to-Most Prompting
Enables Complex Reasoning in Large Language Models. arXiv:2205.10625 [cs.AI]
[49] Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. 2021.
Adaptive information seeking for open-domain question answering. arXiv
preprint arXiv:2109.06747 (2021).
A APPENDIX
A.1 Case Study
In this section, we compare the performance of SearChain and New
Bing4in adding references to supporting documents for generated
content via case study. We also use case study to further analyze
why CoQ has stronger reasoning ability than Baselines.
A.1.1 Case Study for CoQ vs Baselines in Reasoning. Baselines
focus on solving local sub-questions while ignoring the global plan-
ning of the reasoning chain, which leads LLM to tend to stop reason-
ing rather than try more when a sub-question cannot be solved. In
our method, LLM acts as a commander that plans a global reasoning
chain that can solve the complex question, when a sub-question
cannot be solved, even without the help of IR, LLM can try to fur-
ther decompose or rewrite the sub-question to continue reasoning.
It is because our method focuses on building a global chain that can
solve the complex question (global perspective), rather than answer-
ing or generating sub-questions step by step (local perspective).
This makes LLM try more when faced with intermediate difficulties
to finally solve complex questions. Case study shown in Figure 6
indicates that SearChain continues reasoning but baselines stop.
A.1.2 Case Study for SearChain vs New Bing in Tracing. We com-
pare the performance of SearChain and New Bing in marking ref-
erences for generated content via case study (Figure 7). SearChain
can mark references for each knowledge involved in the reason-
ing process (i.e., each correct node of CoQ) in a more fine-grained
4https://www.bing.com/new
The question is incomplete and unclear about Menucha Rochel Slonim's father. CoT
Self-Ask
w/o RetrievalPlease provide additional information about Menucha Rochel Slonim's father so we can 
answer the question.
SearChain
w/o Retrieval[Query 1]: Who was the father of Menucha Rochel Slonim?
[Answer 1]: Rabbi Dovber Schneuri, the second Rebbe of the Chabad Lubavitch movement.
[Query 2]: Who succeeded Rabbi Dovber Schneuri as Rebbe?
[Answer 2]: His son, Rabbi Menachem Mendel Schneersohn.
[Final Content]: The father of Menucha Rochel Slonim is Rabbi Dovber Schneuri [1]. His 
son Rabbi Menachem Mendel Schneersohn succeeded Rabbi Dovber Schneuri as Rebbe[2]. 
So the final answer is Richard Stallman. Who followed the father of Menucha Rochel Slonim?
Unfortunately, there is not enough information provided in the question to accurately answer it. CoT
Self-Ask
w/o Retrieval Follow up: What is the name of the newly declared independent country?
Intermediate answer: The name of the country is not clear.
SearChain
w/o 
Retrieval[Query 1]: What countries are part of the commission of Truth and Friendship with Inierie? 
[Answer 1]: East Timor and Indonesia 
[Query 2]: Which of these countries recently declared independence? 
[Answer 2]: East Timor 
[Query 3]: Who is the president of East Timor? 
[Answer 3]: Francisco Guterres 
[Final Content]: East Timor and Indonesia are part of the commission of Truth and Friendship 
with Inierie [1]. East Timor recently declared independence [2] and the president of it is 
Francisco Guterres . So the final answer is Francisco Guterres.  Who is the president of the newly declared independent country that is part of the commission of Truth and 
Friendship with the country where Inierie is located?
William Beckford's child, a daughter named Maria Hamilton, the information about the death 
of Maria Hamilton is not clear CoT
Self-Ask
w/o RetrievalFollow up: What was the name of William Beckford's child?
Intermediate answer: The name of William Beckford's child is not specified in the question.
SearChain
w/o Retrieval[Query 1]: Who is William Beckford?
[Answer 2]: William Beckford was an English novelist, art collector, and politician.
[Query 2]: Did William Beckford have any children?
[Answer 2]: Yes, William Beckford had one child, a son named William Thomas Beckford.
[Query 3]: Where did William Thomas Beckford live?
[Answer 3]: William Thomas Beckford lived in various places in England, including Bath 
and Fonthill Abbey.
[Query 4]: Where did William Beckford's child die?
[Answer 4]: William Thomas Beckford's child died in Bath.
[Final Content]: William Beckford was an English novelist, art collector, and politician [1]. 
William Beckford had one child, a son named William Thomas Beckford [2]. William 
Thomas Beckford lived in various places in England, including Bath and Fonthill Abbey and 
died in Bath [3,4]. So the final answer is Bath. Where did William Beckford's child die?
The owner of the record label of the performer of "Groovy Little Summer Song" is not 
specified. More information is needed to answer this question.CoT
Self-Ask
w/o RetrievalFollow up: Who is the performer of Groovy Little Summer Song?
Intermediate answer: The performer of Groovy Little Summer Song is not specified.
SearChain
w/o Retrieval[Query 1]: What is the name of the performer of Groovy Little Summer Song?
[Answer 1]: I don't know.
[Query 2]: Can you find out the name of the performer of Groovy Little Summer Song?
[Answer 2]: The performer of Groovy Little Summer Song is James Otto.
[Query 3]: What is the name of the record label that James Otto is signed to?
[Answer 3]: Warner Music Nashville is the record label that James Otto is signed to.
[Query 4]: Who is the owner of Warner Music Nashville?
[Answer 4]: Warner Music Nashville is a subsidiary of Warner Music Group, so the owner 
would be Warner Music Group.
[Final Content]: The performer of Groovy Little Summer Song is James Otto [2]. Warner 
Music Nashville is the record label that James Otto is signed to [3]. The owner of Warner 
Music Nashville is Warner Music Group [4]. So the final answer is Warner Music Group .Who is the owner of the record label of the performer of Groovy Little Summer Song?Figure 6: Case study for CoQ vs Baselines in Reasoning.
manner. While references given by New Bing do not cover all of
the knowledge, and in some cases New Bing cannot find the knowl-
edge. SearChain provides a novel perspective that decomposes com-
plex multi-step knowledge-intensive tasks into multiple single-step
knowledge reasoning problems, retrieving the supporting docu-
ments of knowledge for each step of reasoning, and organizing
these reasoning steps with their reference marks as final generated
content. This enables the supporting documents to cover every
knowledge involved in the generated content, which enhances the
traceability of the generated content.

--- PAGE 11 ---
Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks WWW â€™24, May 13â€“17, 2024, Singapore, Singapore
Table 6: Performance of SearChain and DSP on complex knowledge-intensive tasks on Vicuna-13B. Bold denotes the best result
in different settings. FC: Fact Checking, LFQA: Long-Form QA. Metric for LFQA: ROUGE-L. Metric for others: cover-EM.
Muti-Hop QA Slot Filling FC LFQA
HoPo MQ WQA SQA zsRE T-REx FEV. ELI5
Interaction with Information Retrieval
DSP 25.45 9.06 27.50 62.01 33.71 49.08 73.05 22.58
SearChain 29.77 10.59 32.32 63.75 36.86 52.75 75.47 24.05
      SearChain
Joan Crawford starred in The Duke Steps Out [1], which was released on 
May 14, 1929 [2]. The American Film Institute (AFI) ranked the greatest 
female Hollywood stars in 1999 [3], and the top five were Katharine 
Hepburn, Bette Davis, Audrey Hepburn, Ingrid Bergman, and Greta 
Garbo [4]. Joan Crawford was ranked tenth on the list [5]. So the answer 
is 1999.
[1] .. The Duke Steps Out The Duke Steps Out is directed by James 
Cruze and starring William Haines and Joan Crawford â€¦
[2] â€¦ The Duke Steps Out is a 1929 silent comedy-drama filmâ€¦
[3] â€¦ AFI's 100 Years...100 Stars is a list of the top 25 male and 25 
female Hollywood stars â€¦. The list was unveiled by the American Film 
Institute on June 15, 1999 â€¦
[4] â€¦ â€œAFIâ€˜s 100 Years...100 Starsâ€œ â€¦ Of the 25 actresses listed, Davis 
was ranked at number two, behind Katharine Hepburn. The United States 
Postal Service honored Davis with a commemorative postage stamp in 
2008, marking the 100th anniversary of her birth â€¦
[5] â€¦ Joan Crawford â€¦ was also voted the tenth greatest female star of 
the classic American cinema by the American Film Institute â€¦The Duke Steps Out stars an actress who was ranked tenth on a list of greatest female Hollywood stars in what year?
      New Bing
Iâ€™m sorry, but I couldnâ€™t find any 
information about the actress from 
TheDuke Steps Out who was ranked 
tenth on a list of greatest female 
Hollywood stars [1]. Could you provide 
more details or perhaps rephrase your 
question?
[1]: AFI's 100 Years...100 Stars is the 
American Film Institute's list ranking 
the top 25 male and 25 female greatest 
screen legends of American film history 
and is the second list of the AFI 100 
Years... series.
      SearChain
Lorenzo Monaco lived and worked between 1370 and 1425 [1]. The 
famous Italian painters and architects during his time included Giotto, 
Masaccio, Brunelleschi, and Donatello [2]. Giotto and Masaccio 
specifically influenced his work [2], as well as Piero di Giovanni in Siena, 
Spinello Aretino, and Agnolo Gaddi [2]. So, the answer to the question 
"Lorenzo Monaco was influenced by which Italian painter and architect?" 
is Giotto, Masaccio, Piero di Giovanni in Siena, Spinello Aretino, and 
Agnolo Gaddi.
[1] â€¦Lorenzo Monaco Lorenzo Monaco (born Piero di Giovanni; 1370 â€“ 
c. 1425) was an Italian painter of the â€¦
[2] Lorenzo was an Italian painter of the late Gothic-early Renaissance 
age. He was born Piero di Giovanni in Siena. Little is known about his 
youth, apart from the fact that he was apprenticed in Florence. He was 
influenced by Giotto and his followers Spinello Aretino and Agnolo 
Gaddi. In 1390 he joined the Camaldolese monastery of Santa Maria 
degli Angeli. He was thenceforth generally known as â€œLorenzo Monacoâ€ 
(English: â€œLawrence the Monkâ€). In the 1390s he executed three panels 
of the Biblioteca Laurenziana for his convent â€¦Lorenzo Monaco was influenced by which Italian painter and architect?
      New Bing
Lorenzo Monaco (c. 1370 â€“ c. 1425) 
was an Italian painter of the late Gothic 
to early Renaissance age. He was born 
Piero di Giovanni in Siena, Italy and 
was apprenticed in Florence. He was 
influenced by Giotto and that artistâ€™s 
followers Spinello Aretino and Agnolo 
Gaddi1 [1].
[1]: Lorenzo Monaco Lorenzo Monaco 
was an Italian painter of the late Gothic-
early Renaissance age. He was born 
Piero di Giovanni in Siena. Little is 
known about his youth, apart from the 
fact that he was apprenticed in Florence. 
He was influenced by Giotto and his 
followers Spinello Aretino and Agnolo 
Gaddi. In 1390 he joined the 
Camaldolese â€¦
      SearChain
Paul Revere is Patriot in the American Revolution [1]. Mark Lindsay is a 
musician who is the lead singer of Paul Revere & the Raiders [2]. The 
song Arizona was recorded by Paul Revere and Mark Lindsay, and 
written by Kenny Young [3]. So the final answer is Kenny Young.
[1] .. Paul Revere was an American silversmith, engraver, early 
industrialist, and Patriot in the American Revolution. ..
[2] â€¦ Mark Lindsay (born March 9, 1942) is an American musician, best 
known as the lead singer of Paul Revere & the Raiders â€¦
[3] â€¦ "Arizona" is a song written by Kenny Young and recorded in 1969 
by Mark Lindsay, a solo effort while still lead singer for Paul Revere and 
the Raiders â€¦
The song Arizona was recorded by Paul Revere and Mark Lindsay but who wrote the song?
      New Bing
The song â€œArizonaâ€ was written by 
Kenny Young and recorded by Mark 
Lindsay, a solo effort while still lead 
singer for Paul Revere and the Raiders 
in 1969 [1]. Is there anything else you 
would like to know?
[1]: â€¦ "Arizona" is a song written by 
Kenny Young and recorded in 1969 by 
Mark Lindsay, a solo effort while still 
lead singer for Paul Revere and the 
Raiders â€¦
     SearChain
The Notorious B.I.G., also known as Christopher Wallace, was an 
American rapper [1]. He released two studio albums during his lifetime: 
"Ready to Die" in 1994 and "Life After Death" in 1997 [2]. The song 
"Unbelievable" was included on his debut album "Ready to Die  
[3],â€œwhich was released on September 13, 1994 [4]. So the answer is 
September 13, 1994 .
[1] â€¦ The Notorious B.I.G. Christopher George Latore Wallace (May 21, 
1972 â€“ March 9, 1997), known professionally as the Notorious B.I.G., 
Biggie Smalls, or Biggie, was an American rapperâ€¦
[2] â€¦ Life After Death Life After Death is the second and final studio 
album by American rapper The Notorious B.I.G., released on March 25, 
1997 â€¦ The Notorious B.I.G. further delving into the mafioso rap 
subgenre. The album is a sequel to his first album, "Ready to Dieâ€œ â€¦
[3] â€¦ Ready to Die â€¦ Songs on the album range from homicide 
narratives ("Warning") to braggadocios battle raps ("The What," 
"Unbelievable") â€¦
[4] â€¦ Ready to Die | Ready to Die Ready to Die is the debut studio 
album by American rapper The Notorious B.I.G., released on September 
13, 1994 â€¦When was the album with the song  Unbelievable by American rapper The Notorious B.I.G released?
      New Bing
The song â€œUnbelievableâ€ by American 
rapper The Notorious B.I.G. was 
released on his debut studio album 
Ready to Die on September 13, 1994 by 
Bad Boy Records and Arista Records 
[1]. Is there anything else you would 
like to know?
[1] â€¦ Ready to Die | Ready to Die 
Ready to Die is the debut studio album 
by American rapper The Notorious 
B.I.G., released on September 13, 
1994 â€¦
Figure 7: Case study for SearChain vs New Bing in Tracing.Table 8: Number of examples in prompt used for in-content
learning on different datasets.
Muti-Hop QA Slot Filling FC LFQA
HoPo MQ WQA SQA zsRE T-REx FEV. ELI5
Without Information Retrieval
Direct Prompting 0 0 0 0 0 0 0 0
Auto-CoT 4 4 4 6 4 4 4 2
CoT 4 4 4 6 4 4 4 2
CoT-SC 4 4 4 6 4 4 4 2
Recite-and-answer 4 4 4 6 4 4 4 2
Self-Ask w/o IR 4 4 4 6 4 4 4 2
Least-to-Most 4 4 4 6 4 4 4 2
Plan-and-Solve 4 4 4 6 4 4 4 2
SearChain w/o IR 2 2 2 6 2 2 4 2
Interaction with Information Retrieval
Direct Retrieval 0 0 0 0 0 0 0 0
ToolFormer 4 4 4 6 4 4 4 2
Self-Ask 4 4 4 6 4 4 4 2
Plan-and-Solve w/ IR 4 4 4 6 4 4 4 2
Reactâ†’CoT-SC 6 4 4 6 4 4 4 2
Verify-and-Edit 2 2 2 2 2 2 4 2
Tree-of-Thought w/ IR 4 4 4 6 4 4 4 2
DSP 16 8 8 8 8 8 8 2
SearChain 2 2 2 2 2 2 4 2
Table 7: Performance change with ROUGE threshold.
ğ›¼=0.30ğ›¼=0.35ğ›¼=0.40ğ›¼=0.45ğ›¼=0.50
Performance 25.50 25.57 25.58 25.57 25.55
A.2 Performance on Vicuna-13B
In this section, we compare SearChain with the competitive baseline
DSP on Vicuna-13B5, a strong open source large model6trained by
Stanford. The experimental results in Table 6 show that SearChain
again outperforms DSP on Vicuna-13B.
A.3 Experimental Details
A.3.1 Threshold Selection. As for the confidence threshold ( ğœƒ), we
initialize the initial value of the confidence threshold (1.0) based
on prior knowledge and gradually increase the value with a step
size of 0.1. We validate the F1-score (a comprehensive metric of the

--- PAGE 12 ---
WWW â€™24, May 13â€“17, 2024, Singapore, Singapore Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua
Construct a global reasoning chain for this complex [Question] : " {} " You should generate a query to the search engine base d on 
what you already know at each step of the reasoning chain, starting with [Query]. 
If you know the answer for [Query], generate it starting with [Answer].
You can try to generate the final answer for the [Question] by referring to the [Query] -[Answer] pairs, starting with [Final 
Content].
If you don't know the answer, generate a query to search engine based on what you already know and do not know, starting with
[Unsolved Query].
For exmaple:
[Question]:"Is it common to see frost during some college commencements?"
[Query 1]: What seasons can you expect see frost?If you don't know the answer:
[Unsolved Query]: What seasons can you expect see frost? Instruction: Please Stop your generation.
If you know the answer:
[Answer 1]: Winter.
[Query 2]: What months do college commencements occur?
If you don't know the answer:
[Unsolved Query]: What months do college commencements occur?
Instruction: Please Stop your generation.If you know the answer:
[Answer 2]: December, May, and sometimes June.
[Query 3]: Do any of December, May, and sometimes June occur during winter?
If you don't know the answer:
[Unsolved Query]: Do any of December, May, and sometimes June occur during winter?If you know the answer:
[Answer 3]: December
Instruction: Please Stop your generation.
If you know the answer:
[Question]:"Is it common to see frost during some college commencements?" (The answer can only be "Yes" or "No")
[Final Content]: You expect see frost in Winter [1]. College commencements occur on December, May, and sometimes June [2]. 
December, May, and sometimes June occur during winter [3]. So the final answer is Yes.
[Question]:"Would a pear sink in water?"
[Query 1]: What is the density of a pear?
[Answer 1]: 0.59 g/cm^3
[Query 2]: What is the density of water?
[Answer 2]: 1 g/cm^3
[Query 3]: Is 0.59 g/cm^3 greater than  g/cm^3?
[Answer 3]: No [Question]:"Would a pear sink in water?" (Yes or No)
[Final Content]: The density of a pear is 0.59 g/cm^3 [1]. The density of water is 1 g/cm^3 [2]. 0.59 g/cm^3 is not greater t han
g/cm^3 [3]. So the final answer is No.
Figure 10: Prompt for generating Chain-of-Query at the first
round on StragegyQA (in the setting with information re-
trieval).
Construct a global reasoning chain for this complex [Question] : " {} " You should generate a query to the search engine base d on 
what you already know at each step of the reasoning chain, starting with [Query]. 
If you know the answer for [Query], generate it starting with [Answer].
You can try to generate the final answer for the [Question] by referring to the [Query] -[Answer] pairs, starting with [Final 
Content].
If you don't know the answer, generate a query to search engine based on what you already know and do not know, starting with
[Unsolved Query].
For example:
[Question]: "Where do greyhound buses that are in the birthplace of Spirit If...'s performer leave from? "
[Query 1]: Who is the performer of Spirit If... ?If you don't know the answer:
[Unsolved Query]: Who is the performer of Spirit If... ?If you know the answer:
[Answer 1]: The performer of Spirit Ifâ€¦ is Kevin Drew.
[Query 2]: Where was Kevin Drew born?
If you don't know the answer:
[Unsolved Query]: Where was Kevin Drew born?
If you know the answer:
[Answer 2]: Toronto.[Query 3]: Where do greyhound buses in Toronto leave from?
If you don't know the answer:
[Unsolved Query]: Where do greyhound buses in Toronto leave from?
If you know the answer:
[Answer 3]: Toronto Coach Terminal.[Final Content]: The performer of Spirit Ifâ€¦ is Kevin Drew [1]. Kevin Drew was born in Toronto [2]. Greyhound buses in 
Toronto leave from Toronto 
Coach Terminal [3]. So the final answer is Toronto Coach Terminal.
[Question]:"Which magazine was started first Arthurâ€™s Magazine or First for Women?"
[Query 1]: When was Arthurâ€™s Magazine started?
[Answer 1]: 1844.[Query 2]: When was First for Women started?
[Answer 2]: 1989
[Final Content]: Arthurâ€™s Magazine started in 1844 [1]. First for Women started in 1989 [2]. So Arthurâ€™s Magazine was started
first. So the answer is Arthurâ€™s Magazine.
Figure 11: Prompt for generating Chain-of-Query at the first
round on HotpotQA, Musique, WikiMultiHopQA, zsRE and
T-REx (in the setting with information retrieval).
Construct a global reasoning chain for this complex [Question] : " {} " You should generate a query to the search engine base d 
on what you already know at each step of the reasoning chain, starting with [Query]. 
If you know the answer for [Query], generate it starting with [Answer].
You can try to generate the final answer for the [Question] by referring to the [Query] -[Answer] pairs, starting with [Final 
Content].
If you don't know the answer, generate a query to search engine based on what you already know and do not know, starting with
[Unsolved Query].
For exmaple:
[Question]:"What causes the trail behind jets at high altitude?"
[Query 1]: What is the trail behind jets at high altitude?If you don't know the answer:
[Unsolved Query]: What is the trail behind jets at high altitude?If you know the answer:
[Answer 1]: The trail behind jets at high altitude is commonly referred to as a contrail, which is short for "condensation tr ail." It 
is formed when the hot exhaust gases from a jet engine mix with the colder air at high altitudes, causing the water vapor in theair 
to condense and freeze into tiny ice crystals.
[Query 2]: Why do the hot exhaust gases mix with the colder air at high altitudes?
If you don't know the answer:
[Unsolved Query]: Why do the hot exhaust gases mix with the colder air at high altitudes?If you know the answer:
[Answer 2]: The hot exhaust gases from a jet engine mix with the colder air at high altitudes due to the pressure difference 
between the engine exhaust and the surrounding air. At high altitudes, the air is much colder and thinner, which causes the 
exhaust gases to rapidly expand and cool, creating a turbulent wake behind the aircraft.
[Query 3]: Why does the water vapor in the air condense and freeze into ice crystals?If you don't know the answer:
[Unsolved Query]: Why does the water vapor in the air condense and freeze into ice crystals?
If you know the answer:
[Answer 3]: The water vapor in the air condenses and freezes into ice crystals because the exhaust gases from the jet engine 
contain a significant amount of water vapor, which is heated to high temperatures in the engine. When the hot water vapor mixes 
with the cold air at high altitudes, it quickly loses heat and energy, causing the water vapor to condense into liquid drople ts and 
then freeze into ice crystals.
[Final Content]: The trail behind jets at high altitude is formed when the hot exhaust gases from a jet engine mix with the c older 
air at high altitudes, causing the water vapor in the air to condense and freeze into tiny ice crystals. The exhaust gases mi x with 
the colder air due to the pressure difference between the engine exhaust and the surrounding air, and the water vapor condens es 
and freezes because it is heated to high temperatures in the engine and then quickly loses heat and energy when mixed with the cold air at high altitudes.                       
[Question]: In Trading Places (1983, Akroyd/Murphy) how does the scheme at the end of the movie work? Why would buying a 
lot of OJ at a high price ruin the Duke Brothers?[Query 1]: What is the scheme at the end of Trading Places?
[Answer 1]: In the movie, the main character, Billy Ray Valentine (Eddie Murphy), and his partner, Louis Winthorpe III (Dan 
Aykroyd), execute a plan to bankrupt the Duke Brothers by manipulating the frozen concentrated orange juice (FCOJ) futures 
market.[Query 2]: How do Billy Ray and Louis manipulate the FCOJ futures market?
[Answer 2]: Billy Ray and Louis obtain insider information about the Department of Agriculture's upcoming crop report, which 
indicates that a harsh winter has destroyed much of the orange crop. They use this information to purchase FCOJ futures 
contracts at a low price before the report is released, and then they sell the contracts at a high price after the report's r elease, 
when the market has responded to the news of the crop damage.[Query 3]: Why does buying a lot of OJ at a high price ruin the Duke Brothers?
[Answer 3]: The Duke Brothers, who are also investing in the FCOJ market, have bet that the orange crop will be abundant and 
that the price of FCOJ will remain low. However, Billy Ray and Louis's scheme drives up the price of FCOJ, causing the Duke 
Brothers to lose a significant amount of money and ultimately leading to their downfall.
[Final Content]: In Trading Places, Billy Ray Valentine and Louis Winthorpe III manipulate the FCOJ futures market by 
obtaining insider information about the crop report and purchasing contracts at a low price before selling them at a higher pric e 
after the report is released. The Duke Brothers, who have also invested in the market, lose money because they bet on an 
abundant orange crop and low FCOJ prices. However, Billy Ray and Louis's scheme causes the price of FCOJ to rise, which 
ruins the Duke Brothers and leads to their downfall.Figure 12: Prompt for generating Chain-of-Query at the first
round on ELI5 (in the setting with information retrieval).
Construct a global reasoning chain for this complex question [Question]:"{}" and answer the question, and generate a query to the 
search engine based on what you already know at each step of the reasoning chain, starting with [Query]. 
You should generate the answer for each [Query], starting with [Answer].
You should generate the final answer for the [Question] by referring the [Query] -[Answer] pairs, starting with [Final Content].
For exmaple:
[Question]:"How many places of higher learning are in the city where the Yongle emperor greeted the person to whom the edict 
was addressed?"
[Query 1]: Who was the edict addressed to?
[Answer 1]: the Karmapa
[Query 2]: Where did the Yongle Emperor greet the Karmapa?[Answer 2]: Nanjing
[Query 3]: How many places of higher learning are in Nanjing?
[Answer 3]: 75 
[Final Content]: The edict was addressed to Karmapa [1]. Yongle Emperor greet the Karampa in Nanjing [2]. There are 75 places 
of higher learning are in Nanjing [3]. So the final answer is 75.
[Question]:"Which magazine was started first Arthurâ€™s Magazine or First for Women?"
[Query 1]: When was Arthurâ€™s Magazine started?
[Answer 1]: 1844.
[Query 2]: When was First for Women started?[Answer 2]: 1989
[Final Content]: Arthurâ€™s Magazine started in 1844 [1]. First for Women started in 1989 [2]. So Arthurâ€™s Magazine was started
first. So the final answer is Arthurâ€™s Magazine .
Figure 8: Prompt for generating Chain-of-Query on Hot-
potQA, Musique, WikiMultiHopQA, zsRE and T-REx (in the
setting without information retrieval).

--- PAGE 13 ---
Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks WWW â€™24, May 13â€“17, 2024, Singapore, Singapore
Construct a global reasoning chain for this complex question [Question]:"{}" and answer the question, and generate a query to
the search engine based on what you already know at each step of the reasoning chain, starting with [Query]. You should generate the answer for each [Query], starting with [Answer].
You should generate the final answer for the [Question] by referring the [Query] -[Answer] pairs, starting with [Final Content].
If you don't know the answer, generate a query to the search engine based on what you already know and donot know, starting 
with [Unsolved Query] and please stop your generation.
For exmaple:
[Question]:"How many places of higher learning are in the city where the Yongle emperor greeted the person to whom the edict was addressed?"[Query 1]: Who was the edict addressed to?
If you don't know the answer:[Unsolved Query]: Who was the edict addressed to? 
If you know the answer:[Answer 1]: the Karmapa
[Query 2]: Where did the Yongle Emperor greet the theKarmapa?
If you don't know the answer:[Unsolved Query]: here did the Yongle Emperor greet the theKarmapa?
If you know the answer:[Answer 2]: Nanjing
[Query 3]: How many places of higher learning are in Nanjing ?If you don't know the answer:
[Unsolved Query]: How many places of higher learning are in Nanjing?
If you know the answer:
[Answer 3]: 75  
[Final Content]: The edict was addressed to Karmapa [1]. Yongle Emperor greet the Karampa in Nanjing [2]. There are 75 places 
of higher learning are in Nanjing [3]. So the final answer is 75.
[Question]:"Nicholas Brody is a character on Homeland. (SUPPORTS or REFUTES)?"
[Query 1]: What is Homeland?
[Answer 1]: Homeland is a television series.
[Query 2]: Is Nicholas Brody a character in Homeland?
[Answer 2]: Yes.[Final Content]: Homeland is a television series [1]. Nicholas Brody is a character in Homeland [2]. So the final answer is 
SUPPORTS.
[Question]:"Brad Wilk helped co -found Rage in 1962. (SUPPORTS or REFUTES)?"
[Query 1]: Did Brad Wilk co -found Rage?
[Answer 1]: Yes
[Query 2]: Did Brad Wilk co -found Rage in 1962?
[Answer 2]: No, Rage was founded in 1991
[Final Content]: Brad Wilk did co -found Rage [1], but not in 1962 [2]. So the final answer is REFUTES.
[Question]:"Aristotle spent time in Athens. (SUPPORTS or REFUTES)?"
[Query 1]: Who is Aristotle?[Answer 1]: Aristotle was a Greek philosopher.
[Query 2]: Did Aristotle spend time in Athens?
[Answer 2]: Yes, Aristotle studied and taught at the Academy in Athens for 20 years.
[Final Content]: Aristotle was a Greek philosopher who studied and taught at the Academy in Athens for 20 years [2]. So the f inal 
answer is SUPPORTS.
[Question]:"Telemundo is a English -language television network. (SUPPORTS or REFUTES)?"
[Query 1]: What is Telemundo?
[Answer 1]: Telemundo is a television network.
[Query 2]: Is Telemundo an English- language television network?
[Answer 2]: No, Telemundo is a Spanish- language television network.
[Final Content]: Telemundo is a television network [1], but it is not an English- language television network [2]. So the final 
answer is REFUTES.                        
Figure 9: Prompt for generating Chain-of-Query at the first
round on FEVER (in the setting with information retrieval).Recall and Precision of judging whether the passage can answer the
question) on the mixed open-domain QA datasets (NQ, TriviaQA,
WebQ, and TREC) after each value change. We find that when the
confidence threshold is 1.5, the highest F1-score can be achieved so
we set the confidence threshold as 1.5. As for the ROUGE threshold
(ğ›¼), we determine this value by manually observing the ROUGE
relationship between the generated text and the ground truth in
the few examples in in-context learning. Our further experiments
in Table 7 show that when the value range of ROUGE threshold is
between 0.3 and 0.5, the performance change on ELI5 is not obvious.
A.3.2 Number of Examples in Prompt. We show the number of ex-
amples in prompt used for in-context learning on different datasets
(Table 8). Our method (SearChain) achieves the best performance
with fewer learning examples than competitive baselines.
A.3.3 Prompts in Experiment. We show the prompt used in experi-
ment on different datasets in Figure 8 âˆ¼12.
5https://lmsys.org/blog/2023-03-30-vicuna/
6https://huggingface.co/lmsys/vicuna-13b-delta-v1.1/tree/main
