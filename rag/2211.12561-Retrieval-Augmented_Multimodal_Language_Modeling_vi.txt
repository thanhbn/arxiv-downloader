# 2211.12561.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rag/2211.12561.pdf
# Kích thước tệp: 10088386 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mô hình Ngôn ngữ Đa phương thức Tăng cường Truy xuất
Michihiro Yasunaga,1 *Armen Aghajanyan,2Weijia Shi,3Rich James,2Jure Leskovec,1Percy Liang1
Mike Lewis,2Luke Zettlemoyer,3 2Wen-tau Yih2
Bộ nhớ(Tài liệu đa phương thức) 
Trình truy xuất Bộ nhớ(Tài liệu đa phương thức) Chó Labrador ngồi trên ghế.
Labrador ngồi trên ghế gần nước. Bộ sinh (Mô hình đa phương thức)
Chó Labrador ngồi trên ghế.Chó Labrador ngồi bên nước.TransformerTài liệu truy xuất 1Tài liệu truy xuất 2Labrador ngồi trên ghế gần nước.
Tài liệu chính
Mỗi hình ảnh được ánh xạ thành 1024 token sử dụng VQGAN Tổn thất cho tài liệu chínhTổn thất cho các tài liệu truy xuất
Bộ mã hóa hỗn hợp phương thứcBộ mã hóa hỗn hợp phương thứcĐiểm tương tự cosine
Tài liệu truy vấn(hình ảnh hoặc văn bản hoặc hỗn hợp)Tài liệu bộ nhớ(hình ảnh hoặc văn bản hoặc hỗn hợp)Tài liệu đầu vào
Bộ mã hóa văn bản CLIP Bộ mã hóa hình ảnh CLIP Gộp trung bìnhVí dụ. Mở rộng của CLIP          f(truy vấn, bộ nhớ) → điểm số
Chó Labrador ngồi trên ghế.Labrador ngồi trên ghế gần nước. Bộ sinh (Mô hình đa phương thức)
(a)  Tổng quan về Mô hình Đa phương thức Tăng cường Truy xuất(b)  Trình truy xuất đa phương thức dày đặc(c)  Bộ sinh tăng cường truy xuấtTrình truy xuất

Bộ nhớ(Tài liệu đa phương thức) Chó Labrador ngồi trên ghế.Chó Labrador ngồi bên nước.
TransformerTài liệu truy xuất 1Tài liệu truy xuất 2Labrador ngồi trên ghế gần nước.
Tài liệu chính
Mỗi hình ảnh được ánh xạ thành 1024 token sử dụng VQGAN Tổn thất cho tài liệu chínhTổn thất cho các tài liệu truy xuất
Bộ mã hóa hỗn hợp phương thứcBộ mã hóa hỗn hợp phương thứcĐiểm tương tự cosine
Tài liệu truy vấn(hình ảnh hoặc văn bản hoặc hỗn hợp)Tài liệu bộ nhớ(hình ảnh hoặc văn bản hoặc hỗn hợp)Tài liệu đầu vào
Bộ mã hóa văn bản CLIP Bộ mã hóa hình ảnh CLIP Gộp trung bìnhVí dụ. Mở rộng của CLIP          f(truy vấn, bộ nhớ) → điểm số

Chó Labrador ngồi trên ghế.
Labrador ngồi trên ghế gần nước. Bộ sinh (Mô hình đa phương thức)
(a)  Tổng quan về Mô hình Đa phương thức Tăng cường Truy xuất(b)  Trình truy xuất đa phương thức dày đặc(c)  Bộ sinh tăng cường truy xuấtTrình truy xuất 

Bộ nhớ(Tài liệu đa phương thức) Chó Labrador ngồi trên ghế.Labrador ngồi trên ghế gần nước. Bộ sinh (Mô hình đa phương thức)
Tổng quan về Mô hình Đa phương thức Tăng cường Truy xuấtTrình truy xuất 

Hình 1. Phương pháp của chúng tôi, mô hình hóa đa phương thức tăng cường truy xuất .(a)Tổng quan: với một tài liệu đa phương thức đầu vào, chúng tôi sử dụng một trình truy xuất để truy xuất các tài liệu đa phương thức liên quan từ bộ nhớ ngoài, và sử dụng bộ sinh để tham chiếu đến các tài liệu đã truy xuất và đưa ra dự đoán cho đầu vào (ví dụ, sinh ra phần tiếp theo). (b)Trình truy xuất đa phương thức là một trình truy xuất dày đặc với bộ mã hóa hỗn hợp phương thức có thể mã hóa hỗn hợp văn bản và hình ảnh ( §3.2). (c)Bộ sinh tăng cường truy xuất sử dụng kiến trúc CM3 Transformer, và chúng tôi thêm các tài liệu đã truy xuất vào đầu tài liệu đầu vào chính mà chúng tôi đưa vào mô hình (§3.3).

Tóm tắt
Các mô hình đa phương thức gần đây như DALL-E và CM3 đã đạt được tiến bộ đáng kể trong việc sinh ảnh từ văn bản và sinh văn bản từ ảnh. Tuy nhiên, các mô hình này lưu trữ tất cả kiến thức của chúng (ví dụ, hình dạng của Tháp Eiffel) trong các tham số mô hình, đòi hỏi các mô hình ngày càng lớn hơn và dữ liệu huấn luyện để nắm bắt thêm kiến thức. Để tích hợp kiến thức theo cách có thể mở rộng và modular hơn, chúng tôi đề xuất một mô hình đa phương thức tăng cường truy xuất, cho phép một mô hình đa phương thức cơ sở ( bộ sinh ) tham chiếu đến văn bản và hình ảnh liên quan được truy xuất bởi một trình truy xuất từ bộ nhớ ngoài (ví dụ, tài liệu trên web). Cụ thể, cho trình truy xuất, chúng tôi sử dụng CLIP đã được huấn luyện trước, và cho bộ sinh, chúng tôi huấn luyện một CM3 Transformer trên tập dữ liệu LAION. Mô hình kết quả của chúng tôi, được đặt tên là Retrieval-Augmented CM3 (RA-CM3), là mô hình đa phương thức đầu tiên có thể truy xuất và sinh cả văn bản và hình ảnh. Chúng tôi cho thấy rằng RA-CM3 vượt trội đáng kể so với các mô hình đa phương thức cơ sở như DALL-E và CM3 trong cả nhiệm vụ sinh ảnh và chú thích (cải thiện 12 FID và 17 CIDEr trên MS-COCO), trong khi yêu cầu ít tài nguyên tính toán hơn nhiều để huấn luyện ( <30% của DALL-E). Hơn nữa, chúng tôi cho thấy rằng RA-CM3 thể hiện các khả năng mới, như sinh ảnh trung thực và học trong ngữ cảnh đa phương thức (ví dụ, sinh ảnh từ các minh họa).

1. Giới thiệu
Các mô hình đa phương thức gần đây đã đạt được tiến bộ đáng kể trong việc sinh ảnh và văn bản. DALL-E (Ramesh et al., 2021) và Parti (Yu et al., 2022) thực hiện sinh ảnh từ văn bản, Flamingo (Alayrac et al., 2022) thực hiện sinh văn bản từ ảnh, và CM3 (Aghajanyan et al., 2022) cung cấp một mô hình Transformer thống nhất sinh cả văn bản và hình ảnh. Thông thường, các mô hình này lưu trữ tất cả kiến thức của chúng (ví dụ, hình dạng của Tháp Eiffel) một cách ngầm định trong các tham số của mạng nơ-ron cơ bản, đòi hỏi rất nhiều tham số (ví dụ, 10–80B) và dữ liệu huấn luyện (ví dụ, 1–10B hình ảnh) để bao phủ tất cả kiến thức. Điều này thúc đẩy việc phát triển các mô hình đa phương thức có thể tham chiếu đến bộ nhớ ngoài của kiến thức (ví dụ, dữ liệu web) để tăng khả năng kiến thức. Truy cập đến bộ nhớ ngoài rất hữu ích để thích ứng với sự tăng trưởng và cập nhật kiến thức theo thời gian, và đặc biệt hữu ích cho các nhiệm vụ liên quan đến kiến thức thực thể, như sinh ảnh cho các chú thích giàu thực thể như "George Washington đứng trước Tháp Eiffel". Tham chiếu đến bộ nhớ ngoài cũng có thể mang lại các lợi ích như dự đoán có thể giải thích và trung thực (Metzler et al., 2021).

Gần đây, các mô hình ngôn ngữ tăng cường truy xuất đã cho thấy triển vọng trong xử lý ngôn ngữ tự nhiên (NLP) (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020b; Borgeaud et al., 2022). Với văn bản đầu vào, một mô hình như vậy sử dụng một trình truy xuất để truy xuất các tài liệu liên quan từ bộ nhớ ngoài, và sử dụng một bộ sinh để tạo ra dự đoán dựa trên các tài liệu đã truy xuất. Tuy nhiên, các phương pháp tăng cường truy xuất này được nghiên cứu ban đầu cho văn bản, và việc mở rộng chúng đến môi trường đa phương thức vẫn là một vấn đề mở với những thách thức. Cụ thể, chúng ta cần thiết kế một trình truy xuất và một bộ sinh xử lý các tài liệu đa phương thức, bao gồm cả hình ảnh và văn bản. Một số công trình đồng thời nghiên cứu truy xuất cho dữ liệu đa phương thức (Chen et al., 2022a;b), nhưng bộ sinh của chúng được giới hạn ở một phương thức duy nhất, hoặc là sinh văn bản hoặc sinh ảnh (Bảng 1).

Trong công trình này, chúng tôi giải quyết thách thức trên và trình bày mô hình đa phương thức tăng cường truy xuất đầu tiên có thể truy xuất và sinh cả văn bản và hình ảnh. Như trong Hình 1, dữ liệu đầu vào và bộ nhớ ngoài của chúng tôi bao gồm một tập hợp các tài liệu đa phương thức , mỗi tài liệu là một chuỗi tùy ý của văn bản/hình ảnh (ví dụ, văn bản, hình ảnh, hoặc sự kết hợp của chúng như cặp chú thích-hình ảnh). Đầu tiên, để có được một trình truy xuất đa phương thức, chúng tôi sử dụng phương pháp Dense Retrieval (Karpukhin et al., 2020) với một bộ mã hóa hỗn hợp phương thức có thể mã hóa các kết hợp của văn bản và hình ảnh (ví dụ, CLIP đã được huấn luyện trước; Radford et al. 2021). Với trình truy xuất này, chúng tôi thiết kế một kỹ thuật để truy xuất các tài liệu đa dạng và thông tin cho tài liệu đầu vào. Thứ hai, chúng tôi thiết kế bộ sinh tăng cường truy xuất dựa trên kiến trúc CM3 (Aghajanyan et al., 2022), đây là một mô hình chuỗi Transformer có khả năng sinh cả văn bản và hình ảnh. Cụ thể, chúng tôi thêm các tài liệu đã truy xuất như các ví dụ trong ngữ cảnh vào tài liệu đầu vào chính, và huấn luyện bộ sinh bằng cách tối ưu hóa tổn thất dự đoán token chung cho tài liệu chính và các tài liệu đã truy xuất.

Chúng tôi huấn luyện retrieval-augmented CM3 ( RA-CM3 ) của chúng tôi, sử dụng 150M cặp văn bản-hình ảnh từ tập dữ liệu LAION (Schuhmann et al., 2021). RA-CM3 đạt được hiệu suất mạnh mẽ trên sinh ảnh và chú thích MS-COCO, vượt trội đáng kể so với CM3 cơ sở không có truy xuất (cải thiện 12 FID và 17 CIDEr). Nó cũng vượt trội so với các mô hình hiện có như DALL-E và Flamingo, mặc dù sử dụng ít tham số hơn ( <30%) và tài nguyên tính toán để huấn luyện ( <30%).

Chúng tôi tiếp tục chứng minh các khả năng mới của RA-CM3 ( §5). Đầu tiên, nó có thể thực hiện sinh trung thực cho các nhiệm vụ đòi hỏi kiến thức thực thể, mà các mô hình hiện có gặp khó khăn (Hình 3, 4). Thứ hai, RA-CM3 thể hiện khả năng học trong ngữ cảnh đa phương thức: nó có thể thực hiện sinh ảnh có kiểm soát bằng cách nhắc với các ví dụ minh họa trong ngữ cảnh (Hình 7), và nó cũng có thể thực hiện phân loại ảnh few-shot. RA-CM3 là mô hình đầu tiên có thể thực hiện học trong ngữ cảnh cho cả sinh văn bản và hình ảnh (Bảng 1).

Rộng hơn, công trình của chúng tôi cung cấp một khung tăng cường truy xuất tổng quát và modular cho các mô hình đa phương thức, và mở ra các hướng nghiên cứu khác nhau, như cải tiến thêm các trình truy xuất và bộ sinh đa phương thức.

2. Công trình liên quan
Chúng tôi thảo luận các công trình liên quan chi tiết trong §B.

3. Phương pháp
Chúng tôi trình bày một mô hình đa phương thức tăng cường truy xuất có thể truy xuất và sinh cả văn bản và hình ảnh. Như minh họa trong Hình 1, với một tài liệu đa phương thức đầu vào (tức là chuỗi tùy ý của văn bản/hình ảnh), chúng tôi sử dụng một trình truy xuất để truy xuất các tài liệu đa phương thức liên quan từ bộ nhớ ngoài, và sử dụng một bộ sinh để tham chiếu đến các tài liệu đã truy xuất và đưa ra dự đoán cho tài liệu đầu vào (tức là sinh ra phần tiếp theo). Chúng tôi thiết kế trình truy xuất đa phương thức như một trình truy xuất dày đặc với bộ mã hóa hỗn hợp phương thức có thể mã hóa các kết hợp của văn bản và hình ảnh (ví dụ, CLIP đã được huấn luyện trước; §3.2). Chúng tôi xây dựng bộ sinh tăng cường truy xuất sử dụng kiến trúc CM3 Transformer, và chúng tôi thêm các tài liệu đã truy xuất vào tài liệu đầu vào chính mà chúng tôi đưa vào bộ sinh ( §3.3). Chúng tôi mô tả cách chúng tôi huấn luyện mô hình này và sử dụng nó để sinh văn bản từ ảnh hoặc sinh ảnh từ văn bản trong §3.4.

--- TRANG 2 ---
Mô hình Ngôn ngữ Đa phương thức Tăng cường Truy xuất

Phương pháp Loại mô hình Sinh ảnh Sinh văn bản Truy xuất Học trong ngữ cảnh
DALL-E, Parti (Ramesh et al.; Yu et al.) Tự hồi quy ✔
DALL-E 2, Imagen (Ramesh et al.; Saharia et al.) Khuếch tán ✔
Re-Imagen (Chen et al.) Khuếch tán ✔ ✔
Flamingo, MetaLM (Alayrac et al.; Hao et al.) Tự hồi quy ✔ ✔
MuRAG (Chen et al.) Tự hồi quy ✔†✔
CM3 (Aghajanyan et al.) Tự hồi quy ✔ ✔
RA-CM3 (Của chúng tôi) Tự hồi quy ✔ ✔ ✔ ✔

Bảng 1. So sánh với các mô hình đa phương thức khác . RA-CM3 của chúng tôi là mô hình tăng cường truy xuất đầu tiên có thể thực hiện cả sinh ảnh và sinh văn bản. RA-CM3 cũng thể hiện khả năng học trong ngữ cảnh mạnh mẽ nhờ vào huấn luyện tăng cường truy xuất được đề xuất ( §3.3).
†Tập trung vào trả lời câu hỏi.

Đáng chú ý, mô hình kết quả của chúng tôi, Retrieval-Augmented CM3 (RA-CM3) , là mô hình đa phương thức đầu tiên có thể truy xuất và sinh các kết hợp của văn bản và hình ảnh, đây là khả năng tổng quát nhất trong số các mô hình đa phương thức hiện có (Bảng 1). Hơn nữa, trong khi chúng tôi xây dựng dựa trên các kỹ thuật hiện có như CLIP và CM3, chúng tôi là những người đầu tiên thiết lập một phương pháp để thống nhất chúng thành một mô hình tăng cường truy xuất hiệu quả thông qua phân tích mở rộng các lựa chọn thiết kế (§C.3).

3.1. Kiến thức cơ bản
Mô hình ngôn ngữ tăng cường truy xuất. Khung này bao gồm một module truy xuất R và một module sinh G (ví dụ, mô hình ngôn ngữ). Module truy xuất R lấy một chuỗi đầu vào x và một bộ nhớ ngoài của các tài liệu M, và trả về một danh sách các tài liệu M⊆ M . Bộ sinh G sau đó lấy chuỗi đầu vào x và các tài liệu đã truy xuất M= (m1, ..., m K), và trả về mục tiêu y, trong đó y là phần tiếp theo của x trong một nhiệm vụ mô hình hóa ngôn ngữ điển hình.

Mô hình đa phương thức có mặt nạ nhân quả (CM3). CM3 (Aghajanyan et al., 2022) là một mô hình Transformer decoder (Vaswani et al., 2017) cho các tài liệu đa phương thức. Một tài liệu đa phương thức được định nghĩa là một chuỗi tùy ý của văn bản/hình ảnh (ví dụ, văn bản, hình ảnh, hoặc sự kết hợp của chúng như cặp chú thích-hình ảnh). Đặc biệt, CM3 định dạng mỗi tài liệu đa phương thức như một chuỗi HTML, chẳng hạn như " <img alt=[text] src=[image]> ", trong đó [text] là một chuỗi các token văn bản, và [image] là một chuỗi các token hình ảnh được thu được bởi một bộ tokenizer hình ảnh như VQGAN (Esser et al., 2021), ánh xạ một hình ảnh thô thành 1024 token.

Trong thời gian huấn luyện, CM3 hoặc lấy chuỗi gốc làm đầu vào (ví dụ, xinput="Photo of a cat: [image] ") hoặc chuyển đổi nó thành một instance infilling bằng cách che một số span và di chuyển chúng đến cuối (ví dụ, xinput="Photo of <mask> :[image] <infill> a cat"), và sau đó tối ưu hóa tổn thất dự đoán token tiếp theo tiêu chuẩn cho đầu vào, −logp(xinput). Điều này cung cấp một mô hình linh hoạt học thực hiện infilling bên cạnh sinh tự hồi quy tiêu chuẩn. Đặc biệt, mô hình có thể thực hiện cả sinh ảnh và văn bản: cho chú thích-sang-ảnh, CM3 sinh một phần tiếp theo từ prompt "Photo of a cat:". Cho ảnh-sang-chú thích, CM3 sinh từ prompt "Photo of <mask> :[image] <infill> ".

Thiết lập của chúng tôi. Chúng tôi nhằm tổng quát hóa khung mô hình ngôn ngữ tăng cường truy xuất đến môi trường đa phương thức. Đầu vào x + mục tiêu y của chúng tôi sẽ là một tài liệu đa phương thức, và bộ nhớ M của chúng tôi sẽ là một tập hợp các tài liệu đa phương thức. Chúng tôi thiết kế module truy xuất R cho dữ liệu đa phương thức ( §3.2), và thiết kế bộ sinh đa phương thức G dựa trên CM3 (§3.3).

3.2. Truy xuất đa phương thức
Trình truy xuất dày đặc. Một trình truy xuất r lấy một truy vấn q (ví dụ, chuỗi đầu vào x) và một tài liệu ứng viên m từ bộ nhớ M, và trả về một điểm liên quan r(q, m). Chúng tôi tuân theo phương pháp Dense Retrieval (Karpukhin et al., 2020), trong đó trình truy xuất r là một kiến trúc bi-encoder,
r(q, m) =EQ(q)⊤EM(m) (1)
trong đó bộ mã hóa truy vấn EQ và bộ mã hóa bộ nhớ EM tạo ra các vector dày đặc cho truy vấn và tài liệu bộ nhớ, tương ứng (Hình 1b). Vì đầu vào và bộ nhớ của chúng tôi là các tài liệu đa phương thức, chúng tôi để EQ và EM là các bộ mã hóa hỗn hợp phương thức mã hóa một kết hợp của văn bản và hình ảnh. Trong khi bất kỳ bộ mã hóa hỗn hợp phương thức nào cũng có thể được sử dụng trong khung của chúng tôi, chúng tôi thấy rằng một mở rộng đơn giản của CLIP (Ramesh et al., 2021) hoạt động tốt về mặt thực nghiệm, vì vậy chúng tôi áp dụng nó trong hệ thống cuối cùng của chúng tôi. Cụ thể, như được hiển thị trong Hình 1b (phải), với một tài liệu đa phương thức, chúng tôi chia nó thành một phần văn bản và một phần hình ảnh, mã hóa hai phần riêng biệt sử dụng các bộ mã hóa văn bản và hình ảnh CLIP đóng băng có sẵn, và sau đó tính trung bình hai phần, với chuẩn L2 được chia tỷ lệ thành 1, như biểu diễn vector của tài liệu. Chúng tôi sử dụng phương pháp mã hóa này cho cả EQ và EM. Đánh giá nội tại của trình truy xuất dựa trên CLIP này có thể được tìm thấy trong §C.1.

Với trình truy xuất r này, chúng tôi thực hiện Maximum Inner Product Search (MIPS; §4.1) trên bộ nhớ để có được một danh sách các tài liệu ứng viên được sắp xếp theo điểm liên quan. Sau đó chúng tôi lấy mẫu K tài liệu đã truy xuất cuối cùng từ danh sách này.

Chiến lược truy xuất. Chúng tôi thảo luận ba yếu tố chính trong việc thu thập/lấy mẫu các tài liệu đã truy xuất thông tin cho bộ sinh trong thực tế.

Liên quan : Các tài liệu đã truy xuất cần có liên quan đến chuỗi đầu vào; nếu không, các tài liệu đã truy xuất không cung cấp thông tin hữu ích để mô hình hóa chuỗi đầu vào chính (xem §C.3 cho nghiên cứu loại bỏ). Điểm trình truy xuất dày đặc dựa trên CLIP nắm bắt yếu tố liên quan này.

Phương thức : Trong khi các công trình hiện có về truy xuất (Chen et al., 2022b) thường chỉ truy xuất một hình ảnh hoặc văn bản cho bộ sinh, chúng tôi thấy rằng truy xuất một tài liệu đa phương thức bao gồm cả hình ảnh và văn bản dẫn đến hiệu suất bộ sinh tốt hơn (xem §C.3). Trực giác của chúng tôi là một tài liệu đa phương thức có thể thông tin hơn bởi vì văn bản và hình ảnh trong đó có thể ngữ cảnh hóa lẫn nhau. Do đó, trong hệ thống cuối cùng của chúng tôi, chúng tôi truy xuất các tài liệu đa phương thức thô giữ lại cả hình ảnh và văn bản cho bộ sinh.

Đa dạng : Chúng tôi thấy rằng đảm bảo sự đa dạng trong các tài liệu đã truy xuất là quan trọng. Đầu tiên, đơn giản là lấy mẫu hoặc lấy top K từ danh sách tài liệu dựa trên điểm liên quan có thể dẫn đến hình ảnh hoặc văn bản trùng lặp hoặc rất tương tự, dẫn đến hiệu suất bộ sinh kém. Điều này đặc biệt quan trọng trong môi trường đa phương thức bởi vì ngay cả khi hai tài liệu đa phương thức không phải là bản sao của nhau, hình ảnh hoặc văn bản chứa trong chúng có thể là bản sao, làm tổn hại hiệu suất bộ sinh. Để tránh sự dư thừa, khi chúng tôi lấy tài liệu từ đầu danh sách, chúng tôi bỏ qua một ứng viên nếu nó quá tương tự (ví dụ, điểm liên quan >0.9) với truy vấn hoặc với các tài liệu chúng tôi đã truy xuất. Thứ hai, để khuyến khích thêm sự đa dạng, chúng tôi cũng đề xuất Query Dropout, loại bỏ một số token của truy vấn được sử dụng trong truy xuất (ví dụ, 20% token). Kỹ thuật này đóng vai trò như regularization cho huấn luyện, và dẫn đến cải thiện thêm trong hiệu suất bộ sinh. Do đó, hệ thống cuối cùng của chúng tôi sử dụng hai kỹ thuật này (Tránh dư thừa + Query Dropout) cho huấn luyện, và sử dụng Tránh dư thừa cho suy luận. Xem §C.3 để phân tích chi tiết.

3.3. Bộ sinh đa phương thức
Chúng tôi sử dụng CM3 làm cơ sở cho bộ sinh đa phương thức G của chúng tôi. Để kết hợp các tài liệu đã truy xuất M= (m1, ..., m K) vào bộ sinh, chúng tôi thêm chúng vào chuỗi đầu vào chính x, và đưa chuỗi kết quả (m1, ..., m K, x) vào Transformer (Hình 1c). Nói cách khác, các tài liệu đã truy xuất là các ví dụ trong ngữ cảnh cho đầu vào chính.

Để huấn luyện bộ sinh, chúng tôi tối ưu hóa tổn thất sau:
L=Lmain+αL retr (2)
=−logp(x|m1, ..., m K)−αlogp(m1, ..., m K)(3)
trong đó Lmain và Lretr là tổn thất dự đoán token CM3 cho chuỗi đầu vào chính x và cho các tài liệu đã truy xuất (m1, ..., m K), tương ứng. Ở đây chúng tôi đề xuất tối ưu hóa hai số hạng tổn thất cùng nhau, với α≥0. Các mô hình tăng cường truy xuất hiện có (ví dụ, Lewis et al. 2020b) thường chỉ tối ưu hóa tổn thất cho chuỗi chính, Lmain (tức là α= 0). Tuy nhiên, vì Transformer tính toán logits cho các token trong các tài liệu đã truy xuất khi nó tính toán logits cho các token trong chuỗi chính, chúng tôi có thể dễ dàng bao gồm tổn thất cho các tài liệu đã truy xuất, Lretr. Do đó, α > 0 mang lại hiệu ứng tương tự như tăng kích thước batch (số lượng token tham gia vào tối ưu hóa) mà không tốn thêm nhiều tài nguyên tính toán, và tăng hiệu quả huấn luyện. Kỹ thuật này đặc biệt hữu ích trong môi trường mô hình hóa đa phương thức, bởi vì mỗi hình ảnh chiếm nhiều token (ví dụ, 1024 token), và α= 0 sẽ bỏ qua tính toán được sử dụng cho các token hình ảnh trong các tài liệu đã truy xuất. Trong thực tế, chúng tôi thấy α= 0.1 hoạt động tốt. Xem §C.3 để phân tích chi tiết.

3.4. Huấn luyện và suy luận
Huấn luyện. Với một tài liệu đầu vào đầy đủ x, chúng tôi sử dụng phần văn bản hoặc phần hình ảnh của nó làm truy vấn q để truy xuất tài liệu ( §3.2). Sau đó chúng tôi tối ưu hóa tổn thất dự đoán token bộ sinh trên toàn bộ chuỗi nối (Phương trình 2) bằng teacher forcing tiêu chuẩn. Chúng tôi chỉ sử dụng phần văn bản hoặc hình ảnh làm truy vấn bởi vì (1) truy xuất tài liệu dựa trên tài liệu đầu vào đầy đủ có thể làm cho nhiệm vụ dự đoán token của bộ sinh quá dễ trong quá trình huấn luyện, và (2) thiết lập huấn luyện này gần với các kịch bản suy luận điển hình của sinh văn bản từ ảnh và sinh ảnh từ văn bản.

Vì trình truy xuất dựa trên CLIP có sẵn của chúng tôi đã hoạt động tốt, chúng tôi cố định trình truy xuất và chỉ huấn luyện bộ sinh trong công trình này. Một hướng nghiên cứu tương lai thú vị sẽ là khám phá co-training hoặc fine-tuning trình truy xuất.

Suy luận. Phương pháp của chúng tôi lấy một chuỗi đầu vào (prompt) x, sử dụng x làm truy vấn để truy xuất, và sau đó để bộ sinh lấy các tài liệu đã truy xuất như một phần của đầu vào để giải mã phần tiếp theo của x. Ví dụ, đối với sinh ảnh từ văn bản, prompt x lấy chú thích nguồn, và phần tiếp theo sẽ là hình ảnh mục tiêu. Đối với sinh văn bản từ ảnh, prompt x lấy hình ảnh nguồn, và phần tiếp theo sẽ là chú thích mục tiêu. Do đó, trình truy xuất chỉ sử dụng prompt làm truy vấn và không bao giờ thấy phần tiếp theo ground-truth y cần được đánh giá, đảm bảo không có rò rỉ thông tin.

4. Thí nghiệm
Để thí nghiệm với phương pháp đề xuất của chúng tôi, chúng tôi huấn luyện các mô hình sử dụng tập dữ liệu đa phương thức LAION ( §4.1), và đánh giá trên các nhiệm vụ sinh ảnh và chú thích MS-COCO ( §4.2). Chúng tôi cho thấy rằng mô hình tăng cường truy xuất (RA-CM3) của chúng tôi cải thiện đáng kể cả hiệu suất sinh ảnh và văn bản ( §4.3). Sau đó chúng tôi phân tích các quy luật tỷ lệ và các lựa chọn thiết kế chính của mô hình ( §C.2, C.3). Cuối cùng, §5 trình bày các kết quả định tính và khả năng của mô hình, như sinh thâm dụng kiến thức và học trong ngữ cảnh.

4.1. Thiết lập huấn luyện
Dữ liệu. Để huấn luyện mô hình của chúng tôi, chúng tôi sử dụng LAION (Schuhmann et al., 2021), một tập dữ liệu mã nguồn mở bao gồm các cặp văn bản-hình ảnh được thu thập từ web. Theo bước tiền xử lý của Stable Diffusion (Rombach et al., 2022), chúng tôi đã làm sạch một tập con của LAION1 và thu được tổng cộng 150M cặp văn bản-hình ảnh. Theo CM3, chúng tôi định dạng mỗi cặp văn bản-hình ảnh như một tài liệu HTML, " <img alt=[text] src=[image]> ", trong đó [image] là một chuỗi 1024 token hình ảnh được thu được bằng cách tokenize hình ảnh thô sử dụng VQGAN (Esser et al., 2021; Gafni et al., 2022). 150M tài liệu này được sử dụng làm dữ liệu huấn luyện cuối cùng của mô hình chúng tôi. Chúng tôi cũng sử dụng cùng 150M tài liệu cho bộ nhớ ngoài M của chúng tôi.

Triển khai. Trong module truy xuất R của chúng tôi, chúng tôi sử dụng mô hình CLIP có sẵn ( ViT-L/14 ) (Radford et al., 2021) cho cả bộ mã hóa truy vấn và bộ nhớ EQ và EM. Chúng tôi sử dụng FAISS (Johnson et al., 2019) để lập chỉ mục bộ nhớ ngoài M (Flat Index) và thực hiện truy xuất dựa trên MIPS.

1Chúng tôi lọc ra các hình ảnh có xác suất watermark trên 0.5, xác suất không an toàn trên 0.5, hoặc độ phân giải dưới 256 ×256.

--- TRANG 3 ---
Mô hình Ngôn ngữ Đa phương thức Tăng cường Truy xuất

Đối với bộ sinh G của chúng tôi, chúng tôi sử dụng một Transformer (Vaswani et al., 2017) với 2.7B tham số. Độ dài chuỗi là 4096, có thể chứa tối đa 3 tài liệu. Với mỗi tài liệu đầu vào x, chúng tôi truy xuất K∼Uniform( {0,1,2}) tài liệu và thêm chúng vào x. Tại thời điểm suy luận, chúng tôi cũng có thể truy xuất và thêm K >2 tài liệu thông qua ensemble (xem §5.4).

Mô hình được huấn luyện từ đầu trong năm ngày trên 256 GPU A100. Triển khai của chúng tôi được viết bằng PyTorch (Paszke et al., 2019) sử dụng Metaseq (Zhang et al., 2022). Chúng tôi sử dụng song song mô hình trên 4 GPU và kích thước batch 16 chuỗi mỗi GPU. Tối ưu hóa sử dụng giảm tốc độ học tuyến tính với 1500 bước khởi động, tốc độ học đỉnh 1e-4, gradient clipping 1.0, và bộ tối ưu Adam với β1= 0.9, β2= 0.98 (Kingma & Ba, 2015).

Baseline. Cho baseline của chúng tôi, chúng tôi huấn luyện một CM3 vanilla không có tăng cường truy xuất, sử dụng cùng kiến trúc mô hình, dữ liệu huấn luyện, và lượng tài nguyên tính toán, để so sánh công bằng. Vì bộ nhớ ngoài của RA-CM3 bao gồm cùng dữ liệu huấn luyện, tổng thông tin có thể truy cập đối với RA-CM3 và CM3 vanilla được kiểm soát giống nhau.

4.2. Thiết lập đánh giá
Để đánh giá chính, chúng tôi sử dụng benchmark tiêu chuẩn, MS-COCO (Lin et al., 2014), để đánh giá cả sinh ảnh từ văn bản và sinh văn bản từ ảnh. Chúng tôi đánh giá mô hình đã huấn luyện mà không cần fine-tuning thêm.

Đối với sinh ảnh từ văn bản, theo các công trình trước (Ramesh et al., 2021; Nichol et al., 2021), chúng tôi sinh ảnh cho các chú thích tập validation MS-COCO và đo điểm FID (Heusel et al., 2017) so với ảnh ground-truth. Để sinh một ảnh cho mỗi chú thích, chúng tôi lấy mẫu 10 ảnh từ mô hình và sau đó chọn ảnh tốt nhất dựa trên điểm CLIP (Radford et al., 2021) đối với chú thích đầu vào, như được thực hiện trong Aghajanyan et al. (2022).

Đối với sinh văn bản từ ảnh, theo các công trình trước (Alayrac et al., 2022), chúng tôi sinh chú thích cho các ảnh tập validation MS-COCO và đo điểm CIDEr (Vedantam et al., 2015) so với chú thích ground-truth. Để sinh một chú thích cho mỗi ảnh, chúng tôi lấy mẫu 32 chú thích từ mô hình và chọn chú thích tốt nhất dựa trên perplexity (Fried et al., 2022).

4.3. Kết quả chính
Sinh ảnh từ chú thích. Bảng 2 cho thấy hiệu suất sinh ảnh từ chú thích trên MS-COCO. Metric là điểm FID, trong đó thấp hơn là tốt hơn. CM3 tăng cường truy xuất của chúng tôi đạt được điểm FID 16 mà không cần fine-tuning, vượt trội đáng kể so với CM3 baseline không có truy xuất (FID 29) và các mô hình khác như DALL-E (FID 28), mà lớn hơn mô hình của chúng tôi 3 lần. Điều này cho thấy tăng cường truy xuất cung cấp sự hỗ trợ đáng kể trong việc sinh ảnh chất lượng cao hơn.

Để cũng tính đến hiệu quả huấn luyện, Hình 2 hiển thị hiệu suất sinh ảnh ( trục y: điểm FID) so với lượng tài nguyên tính toán được sử dụng trong huấn luyện mô hình ( trục x: giờ GPU A100 chuẩn hóa) cho mô hình RA-CM3 và các mô hình baseline. Chúng tôi thấy rằng các mô hình hiện có trong paradigm Transformer tự hồi quy theo một đường dốc âm trong biểu đồ này (các điểm và đường màu xanh trong Hình 2). RA-CM3 được đặt đáng kể dưới đường này, tức là đạt được FID tốt hơn với ít tài nguyên tính toán huấn luyện hơn. Điều này cho thấy phương pháp tăng cường truy xuất đề xuất đạt được hiệu quả huấn luyện tốt hơn đáng kể so với các công trình hiện có.

Trực giác của chúng tôi là tăng cường truy xuất cho phép mô hình tập trung vào việc học cách sử dụng các tài liệu đã truy xuất trong ngữ cảnh thay vì khớp tất cả các tài liệu vào các tham số của mô hình, tăng tốc quá trình huấn luyện.

Sinh chú thích từ ảnh. Bảng 3 cho thấy hiệu suất sinh chú thích từ ảnh trên MS-COCO, mà không cần fine-tuning. Metric là điểm CIDEr, trong đó cao hơn là tốt hơn. CM3 tăng cường truy xuất của chúng tôi đạt được điểm CIDEr 89, vượt trội đáng kể so với CM3 baseline không có truy xuất (CIDEr 72). Hơn nữa, RA-CM3 vượt trội so với các mô hình mạnh khác như Parti (20B tham số) và Flamingo (3B; 4-shot), mặc dù chỉ sử dụng ∼3B tham số và các ví dụ trong ngữ cảnh 2-shot.

Những kết quả này xác nhận rằng mô hình của chúng tôi có thể thực hiện tốt cả sinh ảnh và văn bản, cung cấp mô hình đa phương thức tăng cường truy xuất thống nhất đầu tiên (Bảng 1).

4.4. Phân tích
Chúng tôi phân tích các quy luật tỷ lệ của RA-CM3 trong §C.2 và các lựa chọn thiết kế chính của RA-CM3 trong §C.3.

5. Kết quả định tính
Chúng tôi cho thấy các khả năng định tính mới của RA-CM3, như sinh đa phương thức thâm dụng kiến thức ( §5.1) và học trong ngữ cảnh đa phương thức ( §5.2, 5.3, 5.4). Trong khi GPT-3 (Brown et al., 2020) và Flamingo (Alayrac et al., 2022) đã cho thấy học trong ngữ cảnh cho sinh văn bản từ văn bản hoặc sinh văn bản từ ảnh, chúng tôi cho thấy rằng RA-CM3 có thể thực hiện học trong ngữ cảnh cho cả sinh văn bản (§5.4) và sinh ảnh (§5.2, 5.3).

5.1. Sinh đa phương thức thâm dụng kiến thức
Do có khả năng truy xuất, RA-CM3 đặc biệt tốt trong các nhiệm vụ đòi hỏi kiến thức thế giới hoặc kết hợp kiến thức (sinh thâm dụng kiến thức). Hình 3, 4 cho thấy các đầu ra ví dụ từ RA-CM3. Với mỗi chú thích, các ảnh đầu ra được thu thập bằng cách lấy mẫu 256 ảnh từ mô hình và sau đó xếp hạng lại chúng sử dụng điểm CLIP đối với chú thích đầu vào. Sau đó chúng tôi áp dụng một công cụ super-resolution có sẵn (Rombach et al., 2022).

Kiến thức thế giới. Hình 3 cho thấy đầu ra mô hình cho sinh ảnh từ chú thích liên quan đến kiến thức thế giới (ví dụ, các thực thể cụ thể). Chúng tôi thấy rằng mô hình RA-CM3 có thể sinh ảnh chính xác từ các chú thích giàu thực thể nhờ vào việc truy cập các ảnh đã truy xuất trong ngữ cảnh. Ví dụ, đầu ra của RA-CM3 trung thực nắm bắt các đặc điểm thị giác của các thực thể khác nhau (ví dụ, hình dạng và tranh vẽ của bình triều Minh, số lượng đá đứng Callanish). Mặt khác, các mô hình baseline không có khả năng truy xuất (CM3 vanilla, Stable Diffusion) có xu hướng gặp khó khăn, đặc biệt khi chú thích liên quan đến các thực thể hiếm (ví dụ, "bình triều Minh", "tháp Oriental Pearl", "chùa Rồng và Hổ").

Kết hợp kiến thức. Hình 4 cho thấy đầu ra mô hình cho sinh ảnh từ chú thích liên quan đến kết hợp kiến thức hiếm. Chúng tôi thấy rằng mô hình tăng cường truy xuất có thể sinh ảnh trung thực từ các chú thích chứa kết hợp hiếm hoặc chưa thấy của các thực thể (ví dụ, "cờ Pháp" + "mặt trăng", "Mount Rushmore" + "hoa anh đào Nhật Bản"). Mặt khác, các mô hình baseline không có khả năng truy xuất (CM3 vanilla, Stable Diffusion) có xu hướng gặp khó khăn trên những ví dụ này, ví dụ sinh cờ Mỹ thay vì cờ Pháp trên mặt trăng (Hình 4 trên). Điều này có thể là do cờ Mỹ là cờ phổ biến nhất xuất hiện cùng với mặt trăng trong dữ liệu huấn luyện.

5.2. Infilling và chỉnh sửa ảnh
Vì mô hình của chúng tôi xây dựng dựa trên CM3, nó cũng có thể thực hiện infilling .2 Hình 5 cho thấy rằng RA-CM3 có thể thực hiện infilling ảnh được cải thiện nhờ vào khả năng truy xuất. Infilling một ảnh đòi hỏi kiến thức thế giới, ví dụ, để khôi phục các vùng bị che của ảnh trong Hình 5, mô hình cần biết về trượt tuyết. Trong khi CM3 vanilla (không truy xuất) có xu hướng đơn giản infill chân, RA-CM3 (với truy xuất) thành công khôi phục cả chân và ván trượt.

2Để thực hiện infilling ảnh, mô hình được đưa " [phần không bị che của ảnh] <mask> [phần không bị che của ảnh] " làm prompt và sinh phần <mask> như completion. Ảnh đầu ra cuối cùng được xây dựng bằng cách cắm đầu ra được sinh vào phần <mask> của đầu vào.

Hơn nữa, thay vì sử dụng các ví dụ đã truy xuất trong ngữ cảnh RA-CM3, chúng tôi cũng có thể can thiệp và chỉ định thủ công các ví dụ trong ngữ cảnh để kiểm soát infilling ảnh. Hình 6 cho thấy các ví dụ. Chẳng hạn, chúng tôi có thể đặt một ảnh của một người mặc áo khoác đỏ trong ngữ cảnh để chỉnh sửa áo khoác đen trong ảnh gốc thành màu đỏ (Hình 6 trên).

5.3. Sinh ảnh có kiểm soát
Sinh có kiểm soát—kiểm soát hành vi của các mô hình trong sinh (ví dụ, phong cách đầu ra)—là một vấn đề chính trong các mô hình sinh (Keskar et al., 2019; Li et al., 2019). RA-CM3 có thể kiểm soát phong cách sinh ảnh từ chú thích bằng cách thêm các ví dụ minh họa vào ngữ cảnh của bộ sinh (Hình 7). Ví dụ, khi sinh ảnh cho "Photo of a house taken on an autumn day" (Hình 7 trên), chúng tôi có thể chỉ định phong cách cụ thể bằng cách cung cấp các ảnh minh họa (ví dụ, ảnh của một ngôi nhà gỗ tam giác và ảnh nền lá mùa thu màu cam). Do đó, RA-CM3 có thể sinh ảnh thực sự theo các đặc điểm thị giác của những ảnh trong ngữ cảnh này. Đây là một khả năng rất hữu ích vì chúng ta có thể kiểm soát sinh không chỉ thông qua văn bản (chú thích) mà còn thông qua các minh họa ảnh—đặc biệt hữu ích khi một số đặc điểm thị giác chúng ta muốn chỉ định có thể khó diễn đạt bằng văn bản.

Hơn nữa, phát hiện rằng RA-CM3 có thể sử dụng các ví dụ trong ngữ cảnh để sinh có kiểm soát cho thấy nó đã có được một dạng khả năng học trong ngữ cảnh đa phương thức. Trực giác của chúng tôi là vì bộ sinh RA-CM3 đã thấy các tài liệu đa phương thức liên quan được thêm vào tài liệu chính trong ngữ cảnh trong quá trình huấn luyện tăng cường truy xuất, nó đã học cách sử dụng các ví dụ trong ngữ cảnh một cách hiệu quả.

5.4. Phân loại ảnh one-shot và few-shot
Cho đến nay chúng ta đã thấy hành vi học trong ngữ cảnh của RA-CM3 cho sinh ảnh ( §5.3). Ở đây chúng tôi nghiên cứu khả năng học trong ngữ cảnh của nó cho sinh văn bản từ ảnh, thông qua phân loại ảnh one-shot và few-shot.

Hình 8 minh họa thí nghiệm. Để đánh giá khả năng học trong ngữ cảnh thật loại trừ kiến thức trước của mô hình, chúng tôi xem xét một nhiệm vụ phân loại ảnh nhị phân với các nhãn không có nghĩa ngữ (ví dụ, "animal X" và "animal Y" thay vì "dog" và "cat"). Cụ thể, chúng tôi sử dụng ImageNet (Deng et al., 2009) để xây dựng các tập đánh giá như vậy trong đó mỗi lớp (ví dụ, animal X hoặc Y) chứa cùng số lượng ảnh test (ví dụ, 100 ảnh). Đối với phân loại one-shot (Hình 8 trên), chúng tôi đưa vào mô hình một cặp ví dụ minh họa ( [image X] , "animal X", [image Y] , "animal Y"), theo sau là một ví dụ test ( [test image] , "animal "), mà chúng tôi dự đoán xác suất của "X" và "Y". Đối với phân loại k-shot (Hình 8 giữa), chúng tôi lặp lại quy trình trên k lần, mỗi lần sử dụng một cặp ví dụ minh họa khác nhau, và lấy ensemble trung bình của xác suất dự đoán ("X" và "Y") qua k lần chạy.3

Bảng trong Hình 8 dưới cho thấy kết quả của độ chính xác phân loại k-shot (nhị phân), với k= 1,2,4,8. Qua tất cả k, RA-CM3 của chúng tôi đạt được độ chính xác cải thiện đáng kể so với CM3 baseline, mà không được huấn luyện với các tài liệu đã truy xuất trong ngữ cảnh. Đặc biệt, RA-CM3 đã hoạt động khá tốt ở one-shot (độ chính xác 0.78 tại k= 1). Kết quả này cho thấy RA-CM3 đã có được khả năng học trong ngữ cảnh mạnh, đặc biệt là với việc chúng tôi sử dụng các nhãn không có nghĩa ngữ cho các lớp ảnh trong đánh giá này. Hơn nữa, chúng tôi thấy rằng tăng k cải thiện độ chính xác một cách nhất quán cho các giá trị k trên (độ chính xác 0.90 tại k= 8). Quan sát này cho thấy ensemble là một phương pháp hiệu quả để tăng số lượng ví dụ trong ngữ cảnh cung cấp cho mô hình.

6. Kết luận
Chúng tôi đã trình bày một mô hình đa phương thức tăng cường truy xuất có thể truy xuất và tham chiếu đến bộ nhớ ngoài để sinh ảnh và văn bản. Cụ thể, chúng tôi triển khai một trình truy xuất đa phương thức sử dụng CLIP đã được huấn luyện trước và thiết kế một bộ sinh tăng cường truy xuất sử dụng kiến trúc CM3. Mô hình kết quả của chúng tôi, được đặt tên là RA-CM3, vượt trội so với các mô hình đa phương thức hiện có trong cả nhiệm vụ sinh ảnh và chú thích, trong khi yêu cầu ít tài nguyên tính toán huấn luyện hơn nhiều. Hơn nữa, RA-CM3 thể hiện các khả năng mới như sinh ảnh thâm dụng kiến thức và học trong ngữ cảnh đa phương thức.

Công trình này nhằm cung cấp một khung tăng cường truy xuất tổng quát và modular cho các mô hình đa phương thức. Chúng tôi tin rằng điều này mở ra các hướng nghiên cứu thú vị khác nhau, như cải thiện trình truy xuất và bộ sinh đa phương thức, mở rộng các phương thức vượt ra ngoài ảnh và văn bản, và nghiên cứu thêm về prompting đa phương thức và học trong ngữ cảnh.

Lời cảm ơn
Chúng tôi cảm ơn các thành viên của nhóm Meta AI, các nhóm Stanford P-Lambda và SNAP, cũng như các reviewer ẩn danh đã cung cấp phản hồi có giá trị.

3Một cách khác để sử dụng các ví dụ k-shot có thể là thêm trực tiếp tất cả k cặp minh họa vào ngữ cảnh của RA-CM3, nhưng điều này sẽ chiếm độ dài chuỗi đáng kể trong Transformer và có thể không dễ mở rộng. Chúng tôi thấy rằng phương pháp dựa trên ensemble hoạt động tốt về mặt thực nghiệm, và đi kèm với các lợi ích như có thể mở rộng hơn (chạy song song các lần chạy độ dài ngắn hơn) và có nguyên tắc (không phụ thuộc vào thứ tự của k ví dụ).

--- TRANG 4 ---
[Tiếp tục với các trang còn lại...]
