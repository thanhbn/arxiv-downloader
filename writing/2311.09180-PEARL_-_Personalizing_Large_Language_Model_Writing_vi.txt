# PEARL: Cá nhân hóa Trợ lý Viết Large Language Model với Bộ Truy xuất Được Hiệu chỉnh Sinh tạo

Sheshera Mysore1△†Zhuoran Lu2†Mengting Wan1Longqi Yang1
Bahar Sarrafzadeh1Steve Menezes1Tina Baghaee1
Emmanuel Barajas Gonzalez1Jennifer Neville1Tara Safavi1△
2Đại học Purdue, IN, Hoa Kỳ1Microsoft, WA, Hoa Kỳ
△Tác giả liên hệ: smysore@cs.umass.edu ,tarasafavi@microsoft.com

## Tóm tắt

Các mô hình ngôn ngữ lớn mạnh mẽ đã tạo điều kiện cho việc phát triển các trợ lý viết hứa hẹn cải thiện đáng kể chất lượng và hiệu quả của việc soạn thảo và giao tiếp. Tuy nhiên, một rào cản đối với việc hỗ trợ hiệu quả là thiếu cá nhân hóa trong đầu ra LLM theo phong cách giao tiếp, kiến thức chuyên môn và giá trị của tác giả. Trong bài báo này, chúng tôi giải quyết thách thức này bằng cách đề xuất PEARL, một trợ lý viết LLM được cá nhân hóa với bộ truy xuất được huấn luyện để hiệu chỉnh sinh tạo cho cá nhân hóa. Hiệu chỉnh sinh tạo đảm bảo rằng bộ truy xuất của chúng tôi chọn các tài liệu lịch sử do người dùng viết để bổ sung prompt LLM sao cho chúng có khả năng giúp sinh tạo LLM tuân thủ tốt hơn sở thích của người dùng. Chúng tôi đề xuất hai điểm mới chính để huấn luyện bộ truy xuất như vậy: (1) Phương pháp lựa chọn dữ liệu huấn luyện xác định các yêu cầu lịch sử của người dùng có khả năng được hưởng lợi từ cá nhân hóa và các tài liệu cung cấp lợi ích đó; và (2) Mục tiêu KL-divergence hiệu chỉnh quy mô đảm bảo rằng điểm số truy xuất của chúng tôi vẫn tỷ lệ thuận với chất lượng sinh tạo downstream từ việc sử dụng tài liệu cho sinh tạo cá nhân hóa. Trong một loạt đánh giá toàn diện, chúng tôi chứng minh hiệu quả của PEARL trong việc sinh tạo văn bản dài trên nhiều bộ dữ liệu mạng xã hội. Cuối cùng, chúng tôi chứng minh cách một bộ truy xuất được hiệu chỉnh sinh tạo có thể đóng vai trò kép như một bộ dự đoán hiệu suất - phát hiện truy xuất chất lượng thấp và cải thiện các đầu ra có thể hoạt động kém thông qua sửa đổi với LLM.

## 1 Giới thiệu

Việc viết hỗ trợ bằng máy đã có lịch sử phát triển lâu dài, tiến bộ từ việc cung cấp các kiểm tra cú pháp đơn giản, đến sửa đổi văn bản do con người viết, đến các trợ lý gần đây có thể soạn thảo hoàn chỉnh các văn bản theo chỉ dẫn từ tác giả (Mahlow, 2023; Dale và Viethen, 2021). Khả năng sinh tạo văn bản của các LLM hiện tại đã dẫn nghiên cứu hiện tại khám phá một biên giới mới của các trợ lý viết cho các ứng dụng phức tạp như tổng hợp kiến thức (Shen et al., 2023), đánh giá đồng nghiệp (Chen et al., 2023), và báo chí (Wang et al., 2023c). Một yếu tố quan trọng của các trợ lý viết hiệu quả là có thể cá nhân hóa văn bản được sinh tạo để giữ lại kiến thức, phong cách và giá trị của người dùng - một yếu tố thiết yếu của giao tiếp giữa các cá nhân (Pickering và Garrod, 2013). Với các LLM hiện tại có xu hướng sinh tạo văn bản quá chung chung (Pu và Demberg, 2023), cá nhân hóa tác giả của LLM là một vấn đề quan trọng.

Cá nhân hóa đầu ra LLM có thể được xem như một dạng alignment với từng người dùng của LLM (Kirk et al., 2023). Tuy nhiên, việc tận dụng fine-tuning cho alignment trong cài đặt cá nhân hóa đặt ra thách thức trong việc phục vụ các mô hình được huấn luyện cho từng người dùng và thu thập đủ dữ liệu huấn luyện alignment cho từng người dùng. Do đó, chúng tôi theo đuổi alignment trong ngữ cảnh thông qua bổ sung truy xuất (Salemi et al., 2023; Li et al., 2023a). Đầu tiên, chúng tôi giả định có quyền truy cập vào một tập các tài liệu lịch sử do người dùng viết (ví dụ: email, bài đăng mạng xã hội, v.v.) và một yêu cầu của người dùng cho sinh tạo cá nhân hóa. Để cá nhân hóa đầu ra LLM, chúng tôi đề xuất một phương pháp huấn luyện mô hình truy xuất chọn các tài liệu lịch sử của người dùng để bổ sung prompt của LLM. Các tài liệu lịch sử nắm bắt phong cách cá nhân, kiến thức và giá trị của người dùng và có thể phục vụ như ngữ cảnh hữu ích cho sinh tạo cá nhân hóa. Trong khi việc huấn luyện bộ truy xuất cho các ứng dụng không cá nhân hóa đã được khám phá trong công trình trước đây (Gonen et al., 2022), việc khám phá này còn hạn chế trong sinh tạo văn bản cá nhân hóa. Cuối cùng, chúng tôi theo đuổi cá nhân hóa của các LLM chỉ có thể truy cập qua API dựa trên prompt vì điều này đại diện cho một dạng phổ biến của việc truy cập các LLM quy mô lớn có hiệu suất cao.

Điểm khởi đầu cho bộ truy xuất của chúng tôi trong công trình trước đây xem xét các prompt hiệu quả cho các ứng dụng không cá nhân hóa: Gonen et al. (2022) cho thấy các prompt tốt nhất là những prompt có khả năng có điều kiện cao nhất để sinh tạo văn bản mục tiêu, và Rubin et al. (2022) sử dụng những khả năng này để huấn luyện các mô hình truy xuất cho bổ sung truy xuất không cá nhân hóa của LLM. Trong khi phương pháp này hoạt động tốt trong cài đặt không cá nhân hóa, sinh tạo văn bản cá nhân hóa đưa ra những thách thức và cơ hội độc đáo: Có ít tài liệu lịch sử hơn cho mỗi người dùng (~hàng trăm) so với các bộ sưu tập truy xuất không cá nhân hóa thông thường, và các yêu cầu của người dùng có thể khác biệt với lịch sử của họ khi sở thích của người dùng thay đổi. Một corpus truy xuất nhỏ hơn và sở thích thay đổi có nghĩa là tất cả các yêu cầu không thể được thỏa mãn bằng truy xuất từ các tài liệu lịch sử của người dùng - kết quả là, tất cả các yêu cầu và tài liệu lịch sử không có khả năng hữu ích cho việc huấn luyện bộ truy xuất. Đóng góp đầu tiên của chúng tôi giải quyết điều này: Chúng tôi trình bày một phương pháp mới dựa trên hiệu số khả năng xác định chỉ các yêu cầu có thể cá nhân hóa của người dùng và các tài liệu liên quan có khả năng cá nhân hóa các sinh tạo downstream, và sử dụng những điều này để huấn luyện bộ truy xuất của chúng tôi.

Tiếp theo, cài đặt cá nhân hóa mang lại một cơ hội: Ít tài liệu lịch sử hơn cho mỗi người dùng cho phép sử dụng các bộ truy xuất cross-encoder biểu cảm thay vì các biencoder có thể mở rộng nhưng ít biểu cảm hơn thường được sử dụng cho các tác vụ không cá nhân hóa (Rubin et al., 2022). Tuy nhiên, các cross-encoder tạo ra điểm số lệch ở các đầu của phạm vi điểm số của chúng (Menon et al., 2022; Yadav et al., 2022), cản trở khả năng theo dõi chặt chẽ tiện ích của một tài liệu cho sinh tạo cá nhân hóa. Chúng tôi khắc phục điều này với đóng góp thứ hai của chúng tôi - một mục tiêu huấn luyện hiệu chỉnh quy mô cá nhân hóa (Yan et al., 2022). Điều này đảm bảo rằng điểm số từ bộ truy xuất của chúng tôi được hiệu chỉnh sinh tạo cho cá nhân hóa - tức là điểm số mà nó tạo ra cho các cặp yêu cầu-tài liệu tỷ lệ thuận với chất lượng đầu ra của LLM được prompt với cặp đó. Trong một nghiên cứu tình huống, chúng tôi cho thấy cách hiệu chỉnh sinh tạo cho phép điểm số của bộ truy xuất được sử dụng để dự đoán hiệu suất truy xuất - phát hiện truy xuất chất lượng thấp và sửa đổi các sinh tạo có khả năng chất lượng thấp.

Chúng tôi cụ thể hóa PEARL với nhiều LLM, davinci-003 và gpt-35-turbo, tại các endpoint API tuân thủ quyền riêng tư doanh nghiệp và đánh giá nó trên một bộ dữ liệu riêng tư về giao tiếp nơi làm việc và một bộ dữ liệu công khai về các bình luận Reddit. Để đánh giá, chúng tôi sử dụng nhiều phương pháp đánh giá khác nhau bao gồm đánh giá nội tại, ngoại tại và LLM-as-judge cá nhân hóa để chứng minh giá trị của PEARL. Hơn nữa, vì chúng tôi huấn luyện các mô hình truy xuất được hiệu chỉnh, chúng tôi trình bày các đánh giá bổ sung cho hiệu chỉnh, ablation và phân tích trong Phụ lục. Các đánh giá của chúng tôi chứng minh rằng PEARL nhất quán khớp hoặc vượt trội so với các phương pháp baseline mạnh.

## 2 Công trình Liên quan

**Lựa chọn ví dụ cho LLM** Công trình sớm về huấn luyện bộ truy xuất để bổ sung ngữ cảnh LLM trong các ứng dụng không cá nhân hóa được đề xuất bởi Rubin et al. (2022). Họ huấn luyện các mô hình truy xuất bằng cách chưng cất khả năng LLM của các hoàn thành mục tiêu có điều kiện trên prompt. Tương tự, Wang et al. (2023b) huấn luyện các mô hình truy xuất trên phản hồi chi tiết hơn từ một mô hình phần thưởng được huấn luyện thông qua chưng cất. Xa hơn, Zhang et al. (2022) huấn luyện các mô hình lựa chọn instance trên phần thưởng từ một metric đánh giá downstream sử dụng học tăng cường. Song song với công trình của chúng tôi, Salemi et al. (2024) huấn luyện bi-encoder cho phân loại cá nhân hóa và sinh tạo văn bản ngắn và thấy rằng chưng cất kiến thức từ LLM downstream vượt trội so với huấn luyện bộ truy xuất dựa trên học tăng cường. Về mặt này, Salemi et al. (2024) và Rubin et al. (2022) có liên quan chặt chẽ và đại diện cho công trình gần nhất với chúng tôi - chúng tôi so sánh với phương pháp như vậy trong ablation (Phụ lục C.2). Mặc dù có sự tương đồng với công trình của chúng tôi, tất cả công trình trước đây đã khám phá việc huấn luyện bộ truy xuất cho lựa chọn tài liệu trong khi giả định rằng có thể đưa ra dự đoán thỏa đáng cho tất cả đầu vào/yêu cầu. Ngoài việc chọn tài liệu để huấn luyện, chúng tôi cũng chọn các yêu cầu huấn luyện được hưởng lợi từ bổ sung truy xuất - một điều cần thiết trong cá nhân hóa nơi truy xuất được thực hiện trên một tập tài liệu lịch sử nhỏ hơn thay vì một corpus chia sẻ lớn. Hơn nữa, không có phương pháp trước đây nào khám phá hiệu chỉnh cho bộ truy xuất và khả năng của chúng để xác định truy xuất chất lượng thấp, và sửa đổi có chọn lọc đầu ra LLM - chúng tôi khám phá điều này. Phụ lục D thảo luận thêm về công trình tối ưu hóa prompt, độ bền.

**Trợ lý viết cá nhân hóa** Trong khi các trợ lý viết đã thấy khám phá đáng kể, chỉ một số công trình trước đây tập trung vào cá nhân hóa tác giả. Những ứng dụng này dao động từ email (Chen et al., 2019; Trajanovski et al., 2021), đến mạng xã hội (Gero et al., 2022), và sửa lỗi ngữ pháp (GEC) (Nadejde và Tetreault, 2019). Những hệ thống này thường tận dụng các mô hình nearest-neighbor (Chen et al., 2019; Trajanovski et al., 2021) và fine-tuning hiệu quả tham số cấp nhóm người dùng cho cá nhân hóa (Nadejde và Tetreault, 2019). Ngược lại, chúng tôi khám phá các mô hình truy xuất cho alignment/cá nhân hóa trong ngữ cảnh với LLM. Công trình song song cũng đã khám phá viết cá nhân hóa với LLM. Li et al. (2023b) xây dựng prompt với các mô hình truy xuất và tóm tắt được huấn luyện trước và fine-tune một LLM cho hoàn thành cá nhân hóa. Công trình tiếp theo đã khám phá việc huấn luyện một bộ viết lại prompt để điều chỉnh prompt cho một LLM cố định (Li et al., 2023a). Viết lại prompt là một phương pháp bổ sung cho bộ truy xuất được huấn luyện, với các hệ thống tương lai có khả năng được hưởng lợi từ cả hai. Phụ lục D thảo luận các trợ lý viết không cá nhân hóa và cá nhân hóa người đọc.

## 3 Định nghĩa Vấn đề

Chúng tôi xem xét một tác vụ sinh tạo văn bản cá nhân hóa có điều kiện yêu cầu. Như đầu vào cho hệ thống, chúng tôi giả định một người dùng u được liên kết với một tập Nu tài liệu lịch sử Du={d(i)u}Nu i=1, trong đó mỗi tài liệu du có thể là một bài đăng mạng xã hội, email, v.v. được viết trước đây. Người dùng u còn được liên kết với một yêu cầu văn bản qu được gửi đến trợ lý viết. Yêu cầu có thể được viết bởi người dùng hoặc được xây dựng từ ngữ cảnh tác vụ. Các yêu cầu được viết một cách rõ ràng ngày càng phổ biến trong các giao diện LLM đối thoại (Papenmeier et al., 2021), và các ngữ cảnh tác vụ có thể được xem như các yêu cầu ngầm định, ví dụ: tiền tố email yêu cầu hoàn thành (Chen et al., 2019). Cuối cùng, chúng tôi giả định có quyền truy cập vào một mô hình ngôn ngữ lớn fLLM có sẵn qua API sinh tạo văn bản dựa trên prompt.

Cho Du, qu, và fLLM, bộ truy xuất của chúng tôi, fretr được huấn luyện để chọn một tập con của các tài liệu lịch sử D'u ⊂ Du như các ví dụ few-shot cho LLM. Sau đó LLM sinh tạo một văn bản mục tiêu tu lên đến 300 từ: tu=fLLM(φ(qu,D'u)), trong đó φ là một hàm xây dựng prompt nhận đầu vào yêu cầu của người dùng và các tài liệu lịch sử được truy xuất, tu phản ánh phong cách, kiến thức và giá trị của u.

## 4 Phương pháp Đề xuất

Chúng tôi trình bày PEARL, một mô hình dựa trên LLM được alignment trong ngữ cảnh cho hỗ trợ viết cá nhân hóa. Phương pháp của chúng tôi (Hình 2) bao gồm một giai đoạn huấn luyện bộ truy xuất offline và một giai đoạn suy luận LLM online. Offline, chúng tôi huấn luyện một bộ truy xuất fretr: (qu, du) → R cho điểm các tài liệu lịch sử của người dùng về khả năng cá nhân hóa đầu ra cho một yêu cầu của người dùng. Hơn nữa, chúng tôi đảm bảo rằng fretr được hiệu chỉnh sinh tạo tức là điểm số mà nó tạo ra cho các cặp (qu, du) tỷ lệ thuận với chất lượng của văn bản được sinh tạo từ việc sử dụng (qu, du) trong một prompt. Chúng tôi huấn luyện bộ truy xuất như vậy thông qua hai điểm mới chính: (1) Lựa chọn dữ liệu huấn luyện dựa trên hiệu số khả năng mới từ một mô hình sinh tạo văn bản phụ trợ - chúng tôi xác định các yêu cầu được hưởng lợi từ cá nhân hóa và các tài liệu có khả năng giúp cá nhân hóa một mục tiêu, và (2) Một mục tiêu huấn luyện hiệu chỉnh quy mô đảm bảo rằng các bộ truy xuất theo dõi chặt chẽ lợi ích của các cặp yêu cầu-tài liệu cho sinh tạo. Cho một yêu cầu mới, LLM của chúng tôi được prompt để sinh tạo một văn bản mục tiêu tu có điều kiện trên yêu cầu và các tài liệu được truy xuất bởi fretr. Tiếp theo, chúng tôi mô tả việc xây dựng tập huấn luyện bộ truy xuất (Thuật toán 1), cách chúng tôi tối ưu hóa bộ truy xuất, và các chi tiết triển khai của chúng tôi.

### 4.1 Thiết lập Dữ liệu Huấn luyện

Để tối ưu hóa fretr cho một tác vụ sinh tạo văn bản cá nhân hóa, chúng tôi cẩn thận tạo một tập huấn luyện cho fretr từ các tài liệu lịch sử của người dùng bằng cách sử dụng một mô hình sinh tạo văn bản phụ trợ faux để xác định các yêu cầu và tài liệu nào sẽ giúp cá nhân hóa việc sinh tạo văn bản mục tiêu.

**Tổ chức dữ liệu** Chúng tôi tổ chức dữ liệu huấn luyện để tạo ra một thiết lập gần với vấn đề được định nghĩa trong §3. Cho một tập M người dùng và các tập tài liệu lịch sử của họ {Du}M u=1, đối với mỗi người dùng u chúng tôi phân chia Du thành hai tập không chồng lấp, một tập tài liệu ứng viên Dcu ⊂ Du, và một tập văn bản "mục tiêu" Dtu ⊂ Du, sao cho Dcu + Dtu = Du. Việc phân chia được thực hiện theo thời gian, tức là các văn bản mục tiêu xảy ra sau các tài liệu ứng viên, mô phỏng kịch bản cá nhân hóa nơi các văn bản trong quá khứ được sử dụng để cá nhân hóa các mục tiêu sau này. Nếu dữ liệu thời gian không có sẵn, việc phân chia có thể được thực hiện ngẫu nhiên.

Tiếp theo, đối với mỗi văn bản mục tiêu tu trong Dtu của mỗi người dùng, chúng tôi ghép văn bản với một yêu cầu tương ứng qu. Để huấn luyện, các yêu cầu có thể có mặt tự nhiên trong dữ liệu, ví dụ: tiền tố email yêu cầu hoàn thành (Chen et al., 2019), hoặc chúng có thể được sinh tạo một cách tổng hợp (Bonifacio et al., 2022). Chúng tôi chi tiết việc sinh tạo yêu cầu trong §5.1.

**Chấm điểm mô hình phụ trợ** Tiếp theo, chúng tôi sử dụng mô hình sinh tạo văn bản phụ trợ faux để chấm điểm mỗi tài liệu ứng viên du trong Dcu để tạo ra tu cá nhân hóa tương ứng với qu cho mỗi (qu, tu) trong Dtu. Chúng tôi định nghĩa điểm số như một hiệu số trong khả năng, theo faux, của mục tiêu cho yêu cầu có và không có tài liệu lịch sử:

yduqu = log paux(tu|du, qu) - log paux(tu|qu), (1)

Quan trọng, Phương trình (1) cao nhất khi yêu cầu phù hợp cho cá nhân hóa và tài liệu ứng viên là ví dụ "đúng" cho cá nhân hóa. Tức là, chỉ riêng yêu cầu không đủ để sinh tạo văn bản mục tiêu (tức là, lượng được định nghĩa bởi số hạng thứ hai thấp hơn), và tài liệu ứng viên này đặc biệt có lợi cho sinh tạo (tức là, lượng được định nghĩa bởi số hạng đầu tiên cao hơn). Cuối cùng, chúng tôi giả định mô hình faux nhỏ hơn fLLM để hỗ trợ tạo dữ liệu huấn luyện hiệu quả, và chúng tôi có quyền truy cập vào khả năng token của nó. Phụ lục A cho thấy các prompt được sử dụng cho faux.

### 4.2 Lựa chọn Dữ liệu Huấn luyện

Chúng tôi sử dụng các điểm số từ Phương trình 1 để xác định: (1) một tập con của các yêu cầu huấn luyện có khả năng được hưởng lợi từ cá nhân hóa; và (2) các tài liệu ứng viên có khả năng có lợi cho những yêu cầu đó, tức là các tài liệu huấn luyện tích cực.

**Lựa chọn yêu cầu** Sử dụng Phương trình 1, chúng tôi chấm điểm tất cả các cặp yêu cầu-mục tiêu của một người dùng trong Dtu với tất cả các tài liệu ứng viên du của họ trong Dcu, trên tất cả M người dùng. Sau khi chấm điểm, chúng tôi giữ lại T cặp yêu cầu-mục tiêu có điểm cao nhất. Trong thực tế, chúng tôi thấy rằng đặt T là hai phần ba hàng đầu trên toàn bộ bộ dữ liệu hoạt động tốt. Bước này phản ánh trực giác rằng không phải tất cả các cặp yêu cầu-mục tiêu sẽ được hưởng lợi từ bổ sung truy xuất, hoặc do thiếu các tài liệu ứng viên phù hợp trong tập tài liệu lịch sử của người dùng, hoặc do các yêu cầu được chỉ định không đầy đủ khiến văn bản mục tiêu đơn giản là quá khó để sinh tạo tốt - điều này trái ngược với các thiết lập RAG trong các kịch bản không cá nhân hóa nơi một corpus truy xuất lớn đảm bảo rằng hầu hết các yêu cầu có khả năng được hưởng lợi từ truy xuất.

Sau khi có được một tập chất lượng cao của các yêu cầu huấn luyện {q*u}T t=1, chúng tôi loại bỏ các văn bản mục tiêu, vì chúng không được sử dụng để huấn luyện fretr hoặc để suy luận.

**Lựa chọn tài liệu ứng viên** Tiếp theo, chúng tôi sử dụng Phương trình 1 để chọn các tài liệu tốt nhất cho các yêu cầu được giữ lại, tức là xác định các tài liệu huấn luyện tích cực. Cho một yêu cầu q*u được chọn để huấn luyện, chúng tôi lấy P tài liệu ứng viên du có điểm cao nhất trong Dcu theo Phương trình (1) như các tích cực, {d+u}P p=1. Chúng tôi lấy mẫu N mẫu tiêu cực cho mỗi tích cực một cách ngẫu nhiên từ tập tài liệu ứng viên cho người dùng.

### 4.3 Tối ưu hóa Bộ truy xuất

fretr của chúng tôi là một cross-encoder được khởi tạo với một bộ mã hóa LM được huấn luyện trước và được huấn luyện sử dụng dữ liệu được chọn theo Thuật toán 1, thông qua chưng cất các điểm số trong Phương trình 1. Trong khi các cross-encoder có tính biểu cảm, chúng tạo ra điểm số nằm ở các cực của phạm vi điểm số của chúng (Menon et al., 2022; Yadav et al., 2022) - điều này cản trở khả năng theo dõi chặt chẽ lợi ích của các tài liệu ứng viên để cá nhân hóa các yêu cầu. Chúng tôi đề xuất khắc phục điều này thông qua một mục tiêu huấn luyện hiệu chỉnh quy mô.

**Hiệu chỉnh quy mô** Đặt yq = [y+q, ..., y-q], trong đó y+q tương ứng với điểm số của một tài liệu tích cực và y-q tương ứng với điểm số của một tài liệu tiêu cực từ Phương trình 1. Ở đây, yq chứa N tiêu cực và 1 tài liệu tích cực. Tương tự, đặt các logit được dự đoán từ fretr: (qu, du) → R được ký hiệu là sq = [s+q, ..., s-q]. Sau đó, một mất mát KL-divergence tiêu chuẩn được viết là KL(yq,sq) = -Σi sm(yq,i) log sm(sq,i), trong đó sm đại diện cho hàm softmax. Hiệu chỉnh quy mô được đề xuất của chúng tôi sửa đổi mất mát KL divergence bằng cách thêm một ví dụ "neo" với điểm mục tiêu y0, là một siêu tham số có thể điều chỉnh, và logit s0 được đặt thành 0, dẫn đến các vector điểm y'q = [y0,yq] và s'q = [s0,sq]. Mất mát KL-divergence được hiệu chỉnh quy mô do đó là

KL(y'q,s'q) = -Σi sm(y'q,i) log sm(s'q,i) (2)

= -Σi (eyq,i)/(Σj eyq,j + ey0) log (esq,i)/(Σj esq,j + 1) + (ey0)/(Σj eyq,j + ey0) log (1)/(Σj esq,j + 1). (3)

Chúng tôi thấy rằng đặt y0 thành giá trị trung vị của các điểm số từ Phương trình (1) cho các tài liệu ứng viên tích cực hoạt động tốt. Điều này đảm bảo rằng các điểm số rất lớn từ fretr bị phạt (số hạng thứ hai Phương trình 3) và các điểm số nhỏ hơn được ngăn không bị đẩy thấp hơn (số hạng đầu tiên Phương trình 3). Do đó, các điểm số fretr được phân bố đều hơn trên phạm vi điểm số. Trong thực tế, điều này đảm bảo rằng các điểm số được dự đoán từ fretr phản ánh chính xác hơn phân phối của faux, từ đó theo dõi chặt chẽ hơn tiện ích của các cặp yêu cầu-tài liệu cho cá nhân hóa. Chúng tôi so sánh PEARL với các baseline trong §5.2 và trình bày ablation trong §C.2.

### 4.4 Chi tiết Hệ thống

Sau khi huấn luyện bộ truy xuất fretr offline, PEARL có thể được sử dụng để phục vụ các yêu cầu online. Cho một yêu cầu chưa thấy, fretr truy xuất k văn bản lịch sử hàng đầu từ Du, những văn bản này được định dạng thành một prompt và đưa vào fLLM để sinh tạo một văn bản mục tiêu cá nhân hóa tu.

fretr của chúng tôi được khởi tạo với một bộ mã hóa MPNET 110M tham số (Song et al., 2020). Đối với fLLM, chúng tôi xem xét hai LLM có hiệu suất cao, davinci-003 và gpt-3.5-turbo. Đối với faux, chúng tôi sử dụng FLAN T5-XL với 3 tỷ tham số (Chung et al., 2022). Phụ lục A chi tiết các prompt và triển khai của chúng tôi.

## 5 Thí nghiệm

Chúng tôi chứng minh hiệu quả của PEARL trên hai bộ dữ liệu sinh tạo văn bản cá nhân hóa từ các nền tảng mạng xã hội. Để đánh giá, chúng tôi sử dụng các đánh giá nội tại tiêu chuẩn, đánh giá ngoại tại dựa trên các tác vụ downstream sử dụng văn bản được sinh tạo, và LLM-as-judge cá nhân hóa được đề xuất gần đây (Wang et al., 2023d). Sau đó, trong §5.3 chúng tôi cho thấy cách một bộ truy xuất được hiệu chỉnh có thể được sử dụng để sửa đổi có chọn lọc các yêu cầu hoạt động kém. Chúng tôi trình bày ablation trong §C.2 và chúng tôi chứng minh hiệu suất hiệu chỉnh cho bộ truy xuất của chúng tôi trong §C.3.

### 5.1 Thiết lập Thí nghiệm

**Dữ liệu** Để đánh giá, chúng tôi sử dụng hai bộ dữ liệu sinh tạo văn bản dài mở cho mạng xã hội: (1) Viết bài đăng cá nhân hóa trên WORKSM và (2) Viết bình luận cá nhân hóa trên AITA.

**WORKSM** WORKSM là một mạng xã hội doanh nghiệp được sử dụng để giao tiếp trong các tổ chức trình bày một nền tảng rất thực tế cho hỗ trợ viết. Chúng tôi có được một mẫu ngẫu nhiên của ~18k bài đăng được viết bởi 1116 người dùng từ tháng 11 năm 2020 đến tháng 7 năm 2023. Để tạo một tập đánh giá, chúng tôi kiểm tra thủ công các bài đăng lớn hơn 50 từ và nhận được >=2 bình luận, khoảng 1K bài đăng, và chọn 163 bài đăng gần đây nhất từ ~80 người dùng để phục vụ như văn bản mục tiêu tham chiếu t*u. Những bài đăng này đại diện cho một tập đa dạng, hấp dẫn có thể được hưởng lợi từ hỗ trợ viết cá nhân hóa và phục vụ như tham chiếu chất lượng cao. Ở mức độ cao, những bài đăng này chia sẻ các sự kiện, nghiên cứu, chiến dịch và tin tức tổ chức. Vì WORKSM không chứa các yêu cầu đến trợ lý viết, hai tác giả không tham gia vào phát triển mô hình đã viết thủ công các yêu cầu qu cho mỗi văn bản mục tiêu. Lưu ý rằng điều này là cần thiết do dữ liệu doanh nghiệp được quản lý chặt chẽ và riêng tư trong WORKSM ngăn việc tiếp xúc với các crowdworker bên ngoài. Các yêu cầu của chúng tôi được viết theo Hướng dẫn 1. Để xây dựng Du, các bài đăng được tạo trước t*u được sử dụng: Trung bình, người dùng có 31 bài đăng lịch sử (tối đa 169). Để tạo tập huấn luyện của chúng tôi, chúng tôi chỉ giữ lại các bài đăng >10 từ và người dùng có >=5 bài đăng lịch sử trong khi loại trừ các bài đăng trong tập đánh giá của chúng tôi. Chúng tôi sinh tạo các yêu cầu tổng hợp với GPT-4 để huấn luyện do chi phí của các yêu cầu được viết thủ công - dẫn đến một tập ~7k yêu cầu huấn luyện. Các hợp đồng doanh nghiệp với các nhà cung cấp API đảm bảo quyền riêng tư của dữ liệu người dùng được chia sẻ qua API.

**AITA** AITA là một diễn đàn con Reddit trong đó những người đăng bài gốc (OP) mô tả các xung đột đạo đức cá nhân và nhận được bình luận từ người dùng khác đánh giá họ là "người tồi tệ" hoặc "không phải người tồi tệ". Bộ dữ liệu này đã được sử dụng trong công trình trước đây về mô hình hóa các giá trị cá nhân của người dùng (Plepi et al., 2022). Chúng tôi xây dựng một tác vụ sinh tạo bình luận cá nhân hóa từ dữ liệu này. Chúng tôi coi các bài đăng OP như yêu cầu qu, bình luận của người dùng như văn bản mục tiêu tham chiếu t*u, và các bình luận trước đây của người dùng như Du. Vì bộ dữ liệu thiếu metadata thời gian, chúng tôi xây dựng một tập đánh giá bằng cách lấy mẫu 10% các bài đăng như yêu cầu thử nghiệm, và lọc thêm đến 600 văn bản mục tiêu ngẫu nhiên cho tập đánh giá của chúng tôi để giữ cho các thí nghiệm LLM khả thi. Người dùng đánh giá có trung bình 29 bài đăng trong Du (tối đa 590). Tập huấn luyện của chúng tôi sử dụng các cặp bài đăng-bình luận lịch sử từ người dùng trong Du, dẫn đến ~84k yêu cầu. Lưu ý rằng trong khi các bình luận Reddit không phải là nền tảng lý tưởng cho hỗ trợ viết, AITA là một trong số ít bộ dữ liệu công khai có sẵn cho tác vụ và giống với các ứng dụng như sinh tạo phản hồi email (Kannan et al., 2016). Phụ lục B chi tiết thêm về các bộ dữ liệu của chúng tôi.

**Metric sinh tạo** Vì sinh tạo văn bản cá nhân hóa nhằm tuân thủ phong cách, kiến thức và giá trị của người dùng cụ thể, đánh giá hiệu quả cho sinh tạo cá nhân hóa vẫn là một vấn đề mở (Wang et al., 2023d,a). Điều này trái ngược với sinh tạo không cá nhân hóa, nơi các khía cạnh mong muốn của đầu ra có thể được định nghĩa thống nhất trên tất cả các trường hợp thử nghiệm. Kết quả là, chúng tôi trình bày các đánh giá sử dụng một loạt các thiết lập đánh giá tiêu chuẩn nhằm chứng minh hiệu quả của PEARL từ nhiều góc độ khác nhau. Các đánh giá của chúng tôi bao gồm các thiết lập tiêu chuẩn sau đây (Dou et al., 2023): đánh giá nội tại dựa trên độ tương đồng n-gram/embedding với văn bản tham chiếu, đánh giá ngoại tại thông qua độ chính xác phân loại dựa trên văn bản được sinh tạo, và đánh giá theo cặp với LLM-as-judge cá nhân hóa.

Cụ thể, đối với WORKSM chúng tôi báo cáo các biện pháp đánh giá tiêu chuẩn dựa trên độ tương đồng n-gram và embedding giữa các sinh tạo và mục tiêu tham chiếu: ROUGE-1 (R1), ROUGE-2 (R2), và BertScore-F1 (BS-F1) (Zhang* et al., 2020). Điều này phục vụ như một đánh giá nội tại cho WORKSM đo lường mức độ mà các sinh tạo tương tự với văn bản do người dùng viết. Tiếp theo, vì các bình luận của người dùng AITA chủ yếu đưa ra lập trường dựa trên giá trị đạo đức của người dùng, chúng tôi đo lường xem lập trường trong các bình luận được sinh tạo có khớp với lập trường của người dùng thông qua một tác vụ dự đoán lập trường downstream - phục vụ như một đánh giá ngoại tại. Đánh giá này có thể được xem như đánh giá mức độ mà các sinh tạo mô hình tuân thủ giá trị của người dùng. Chúng tôi ánh xạ các bình luận được sinh tạo thành nhãn nhị phân "YTA" hoặc "NTA" dựa trên các quy tắc chính xác cao đơn giản ánh xạ các biến thể từ vựng của "bạn là người tồi tệ" và "không phải người tồi tệ" thành các nhãn. Thủ tục này cũng được thấy đáng tin cậy để xây dựng nhãn sự thật cơ bản trong AITA (Plepi et al., 2022). Lưu ý rằng các nỗ lực sớm sử dụng các biện pháp đánh giá độ tương đồng n-gram/embedding (BS-F1, R1, R2) dẫn đến các đánh giá không đáng tin cậy cho AITA do sự biến đổi lớn (độ dài, từ vựng, emoji, v.v.) trong các bình luận AITA, do đó chúng tôi chọn các đánh giá ngoại tại ổn định hơn và các đánh giá dựa trên LLM được mô tả tiếp theo.

Đối với cả AITA và WORKSM, chúng tôi tiến hành đánh giá theo cặp với LLM-as-judge cá nhân hóa được đề xuất gần đây (Wang et al., 2023d). Wang et al. cho thấy các xác định tác giả dựa trên LLM là một tác vụ proxy đáng tin cậy để phân biệt các mô hình có chất lượng khác nhau và có tương quan với đánh giá chất lượng của con người. Ở đây, một LLM thẩm phán được trình bày với một văn bản tham chiếu từ một người dùng và các sinh tạo từ cặp hệ thống được so sánh, sau đó, nó được prompt để chọn sinh tạo hệ thống có nhiều khả năng được viết bởi tác giả của văn bản tham chiếu hơn. Một tác vụ xác định tác giả nhằm nắm bắt một số khía cạnh phân biệt việc viết của các cá nhân, bao gồm phong cách, kiến thức và giá trị của họ. Trong đánh giá của chúng tôi, chúng tôi so sánh đầu ra PEARL với đầu ra từ baseline tốt nhất như được chỉ ra bởi các đánh giá nội tại/ngoại tại và sử dụng văn bản tham chiếu mục tiêu t*u trong prompt LLM như một ví dụ về việc viết của người dùng. Chúng tôi sử dụng GPT-4o làm LLM thẩm phán của chúng tôi và trình bày prompt thẩm phán trong Phụ lục B.4. Trong đánh giá của chúng tôi, chúng tôi tránh đánh giá các khía cạnh như tính lưu loát, không dư thừa, v.v. (Celikyilmaz et al., 2021) vì chúng tôi chủ yếu quan tâm đến hiệu suất cá nhân hóa và những chất lượng này có thể xung đột với việc viết cụ thể của người dùng.

**Baseline** Làm baseline, chúng tôi xem xét các mô hình không cá nhân hóa dựa trên prompting zero shot (ZSHOT-NP) và prompting few-shot với k tài liệu ví dụ được chọn ngẫu nhiên (KSHOT-NP). Chúng tôi xem xét các baseline cá nhân hóa bổ sung truy xuất, chọn từ các tài liệu lịch sử Du của người dùng. Chúng bao gồm lựa chọn ngẫu nhiên từ Du (Random), với truy xuất thưa thớt bởi BM25, với truy xuất dày đặc bởi một mô hình MPNET mạnh được huấn luyện trên 1 tỷ cặp văn bản (MPNET-1B), một crossencoder không giám sát (Sachan et al., 2022) xếp hạng tài liệu với khả năng FLAN T5-BASE: p(qu|du) (UPR), và một crossencoder có giám sát được tối ưu hóa trên bộ dữ liệu của chúng tôi với các cặp yêu cầu-tài liệu, (qu, du) trong Du (RelevanceCE). Phụ lục B.3 chi tiết các baseline của chúng tôi.

### 5.2 Đánh giá Sinh tạo

Bảng 1 và 2 báo cáo các đánh giá của chúng tôi. Phụ lục C trình bày kết quả ablation (C.2) và hiệu chỉnh (C.3).

**Đánh giá dựa trên tham chiếu** Bảng 1b và 1a báo cáo các metric tự động trên AITA và WORKSM. Đầu tiên chúng tôi quan sát rằng cá nhân hóa thông qua truy xuất, ngay cả ở Random, thường cải thiện so với các phương pháp không cá nhân hóa (NP), phù hợp với công trình trước đây (Salemi et al., 2023). Tiếp theo, chúng tôi lưu ý rằng baseline tốt nhất không nhất quán, thay đổi giữa BM25 và unsupervised crossencoder (UPR) - cho thấy rằng các mô hình truy xuất được thiết kế cho relevance yêu cầu-tài liệu thay đổi trong hiệu suất tùy thuộc vào bộ dữ liệu và LLM suy luận. Cuối cùng, chúng tôi lưu ý rằng PEARL nhất quán hoạt động ngang bằng hoặc tốt hơn so với các baseline tốt nhất trên các bộ dữ liệu và LLM, cho thấy hiệu quả của việc huấn luyện fretr cho sinh tạo cá nhân hóa. Đối với các metric phân loại đáng tin cậy hơn có thể đạt được trong AITA, PEARL vượt trội so với tất cả baseline với cải thiện từ 1,5 đến 5 điểm Macro F1. Tiếp theo, chúng tôi báo cáo hiệu suất trong các đánh giá LLM-as-judge biểu cảm hơn.

**Đánh giá LLM-as-judge theo cặp** Trong Bảng 2 chúng tôi báo cáo kết quả của đánh giá cá nhân hóa theo thiết lập được mô tả trong §5.1. Ở đây, chúng tôi so sánh với BM25-augmented vì nó hoạt động trong top 2 baseline của chúng tôi trong các đánh giá tự động - hiệu suất mạnh này phù hợp với công trình trước đây (Izacard et al., 2022; Thakur et al., 2021). Chúng tôi sử dụng GPT-4o làm LLM thẩm phán và chạy mỗi cặp đầu vào qua LLM thẩm phán 3 lần, chúng tôi báo cáo tỷ lệ thắng trung bình trên tất cả các instance trong tập thử nghiệm của chúng tôi và trên 3 lần chạy lặp lại. Hơn nữa, chúng tôi hoán đổi ngẫu nhiên vị trí của các sinh tạo phương pháp baseline và được đề xuất trong prompt để tính đến bias vị trí trong LLM thẩm phán. Cuối cùng, chúng tôi cũng báo cáo sự đồng ý giữa 3 lần chạy LLM thẩm phán sử dụng alpha Krippendorff (alpha) để đảm bảo rằng các phán quyết LLM nhất quán qua các lần chạy.

Trong Bảng 2, PEARL đạt được tỷ lệ thắng lớn hơn BM25 trong 3 trong 4 thiết lập. Trong những thiết lập này, chúng tôi cũng lưu ý rằng các phán quyết LLM vẫn nhất quán qua 3 lần chạy lặp lại với alpha Krippendorff giữa 0,41−0,56 (0 cho thấy đồng ý ngẫu nhiên). Trong khi BM25 thấy tỷ lệ thắng lớn hơn trong WORKSM với gpt-35-turbo, các phán quyết thấy đồng ý thấp hơn (alpha = 0,28) cho thấy các đầu ra khó phân biệt hơn. Cuối cùng, so sánh với Bảng 1 chúng tôi thấy rằng xu hướng của các đánh giá dựa trên tham chiếu ngoại tại và nội tại được giữ lại trong các đánh giá LLM-as-judge - nhất quán cho thấy lợi ích của PEARL qua các thiết lập đánh giá, LLM suy luận và bộ dữ liệu. Trong Phụ lục C chúng tôi cho thấy một ví dụ từ AITA để hiển thị các loại truy xuất và đầu ra làm cho PEARL hiệu quả.

### 5.3 Sửa đổi Có chọn lọc với PEARL

Sau khi thiết lập PEARL là một mô hình hiệu quả cho sinh tạo, chúng tôi cho thấy fretr được hiệu chỉnh sinh tạo trong Phụ lục C.3. Ở đây, chúng tôi chứng minh tính hữu ích của một bộ truy xuất được hiệu chỉnh trong một nghiên cứu tình huống sử dụng điểm số bộ truy xuất để sửa đổi có chọn lọc các sinh tạo. Cụ thể, chúng tôi coi các điểm số từ fretr như một bộ dự đoán hiệu suất truy xuất, và từ đó hiệu suất sinh tạo văn bản. Chúng tôi giả định rằng nếu fretr không thể tìm thấy một ví dụ trong ngữ cảnh được chấm điểm cao, phản hồi được sinh tạo sẽ có chất lượng thấp và có thể được hưởng lợi từ sửa đổi LLM (Hình 3).

**Thiết lập** Cho bộ truy xuất được huấn luyện của chúng tôi, chúng tôi lấy tất cả điểm số tài liệu top-1 cho mỗi yêu cầu s1 = max du∈Du fretr(qu, du) và học một ngưỡng theta trên s1 tối đa hóa một metric hiệu suất downstream trên một tập phát triển được giữ lại (R2 trong WORKSM và Macro-F1 trong AITA). Sau đó, cho một văn bản mục tiêu được sinh tạo tu với s1 < theta, chúng tôi sửa đổi có chọn lọc tu nơi fLLM được prompt để chỉnh sửa văn bản mục tiêu. Chúng tôi báo cáo kết quả của sửa đổi có chọn lọc so với một vòng sinh tạo duy nhất (tức là, không sửa đổi) và sửa đổi đầy đủ trên toàn bộ bộ dữ liệu (tức là, 100% sửa đổi). Chúng tôi lặp lại điều này cho BM25. Chúng tôi cung cấp thêm chi tiết và phân tích trong Phụ lục C.4.

**Kết quả** Trong Bảng 3 chúng tôi thấy rằng sửa đổi có chọn lọc cải thiện hoặc giữ lại hiệu suất sau một vòng sinh tạo duy nhất ("Giai đoạn 1") bằng 2-4% trong các metric hiệu suất downstream với fretr=Proposed và BM25 cho WORKSM. Tuy nhiên, đối với AITA chúng tôi thấy rằng sửa đổi có chọn lọc dựa trên BM25 cho thấy sự giảm đáng kể trong hiệu suất cho thấy hiệu suất hiệu chỉnh phụ thuộc vào bộ dữ liệu. Quan trọng, lưu ý rằng PEARL chọn 75,8% và 77,9% instance để chỉnh sửa trong WORKSM và AITA, tương ứng. Điều này cho thấy tiềm năng cho các bộ truy xuất được hiệu chỉnh sinh tạo giảm số lượng cuộc gọi LLM đắt đỏ được thực hiện trong khi đảm bảo hiệu suất cá nhân hóa tốt hơn. Trong Hình 5 (Phụ lục C.4) chúng tôi phân tích hiệu suất của sửa đổi có chọn lọc so với độ dài yêu cầu và hồ sơ người dùng. Trong một kiểm tra thủ công các yêu cầu với điểm số s1 thấp bởi fretr PEARL, chúng tôi thấy các yêu cầu được chỉ định không đầy đủ và thường yêu cầu thông tin thêm từ người dùng ví dụ: yêu cầu "Viết một bài đăng về cách tôi thích thư giãn sau giờ làm", nhằm sinh tạo một mục tiêu thảo luận về các hình thức thư giãn cụ thể hơn không có trong bất kỳ tài liệu lịch sử nào. Điều này cho thấy rằng các bộ truy xuất được hiệu chỉnh sinh tạo có thể được sử dụng cho các hình thức dự đoán có chọn lọc và tương tác người dùng khác - ví dụ: giữ lại có chọn lọc các dự đoán khi các sinh tạo thỏa đáng không có khả năng hoặc thu thập thêm thông tin từ người dùng thông qua các câu hỏi tiếp theo. Chúng tôi để lại những khám phá như vậy cho công trình tương lai thú vị.

## 6 Kết luận

Trong bài báo này chúng tôi trình bày PEARL - một trợ lý viết dựa trên LLM được cá nhân hóa với các bộ truy xuất được hiệu chỉnh sinh tạo. Chúng tôi đề xuất một phương pháp để huấn luyện các bộ truy xuất được hiệu chỉnh sinh tạo thông qua việc lựa chọn cẩn thận dữ liệu huấn luyện và một mục tiêu được hiệu chỉnh quy mô. Trong một loạt đánh giá toàn diện, chúng tôi chứng minh hiệu quả của phương pháp của chúng tôi trong các bộ dữ liệu giao tiếp mạng xã hội so với các baseline (§5.2) cũng như các mô hình được ablated (Phụ lục C.2). Chúng tôi chứng minh hiệu suất hiệu chỉnh cho bộ truy xuất của chúng tôi (Phụ lục C.3), và cho thấy cách mô hình truy xuất của chúng tôi có thể đóng vai trò kép như một bộ dự đoán hiệu suất (§5.3) và có thể xác định các đầu ra có thể được hưởng lợi từ sửa đổi LLM.

## 7 Tác động Đạo đức và Rộng hơn

Sau khi giới thiệu PEARL như một chiến lược cá nhân hóa hiệu quả cho hỗ trợ viết và thảo luận về lợi ích của nó, chúng tôi xem xét hai tác động đáng lo ngại phát sinh từ sinh tạo văn bản cá nhân hóa tốt hơn: thách thức đối với tính factual, và ảnh hưởng dài hạn đến việc sử dụng ngôn ngữ và giao tiếp.

**Thách thức đối với tính factual** Sự xuất hiện của LLM và khả năng sinh tạo văn bản hấp dẫn của chúng đã thấy sự gia tăng tiếp theo trong các trường hợp sử dụng độc hại của những công nghệ này. Augenstein et al. (2023) tổng quan bốn lớp tác hại như vậy: các cuộc tấn công cá nhân hóa vào cá nhân dưới dạng các cuộc tấn công lừa đảo và thông tin sai lệch có mục tiêu, mạo danh các nhân vật đáng tin cậy (ví dụ: nhà báo hoặc cơ quan quản lý), một lượng lớn thông tin sai lệch được diễn giải lại tránh phát hiện bởi các công cụ tự động thường được sử dụng bởi các fact checker, và tạo ra quy mô lớn các hồ sơ mạng xã hội giả và nội dung đạo văn (Brewster et al., 2023). Có thể những cải thiện trong sinh tạo văn bản cá nhân hóa có khả năng làm trầm trọng thêm mỗi vấn đề này. Để tính đến điều này, một số sáng kiến công nghệ và chính sách đang được phát triển tích cực (Augenstein et al., 2023). Những điều này bao gồm phát hiện nội dung được sinh tạo bởi AI, chữ ký mật mã nhằm chứng minh tính xác thực của nội dung, đến quy định của chính phủ và giáo dục công chúng, tuy nhiên, hiệu quả của chúng vẫn đang được điều tra.

**Sử dụng ngôn ngữ và giao tiếp** Hiểu biết hiện tại về giao tiếp được trung gian bởi máy tính cho thấy rằng các mẫu giao tiếp giữa các cá nhân của người dùng bị ảnh hưởng bởi công cụ/phương tiện được sử dụng để giao tiếp (Poddar et al., 2023) với tiềm năng cho những ảnh hưởng này có ảnh hưởng dài hạn đến giao tiếp khi không có những công cụ này (Hancock et al., 2020). Hancock et al. phác thảo những tác động này dao động từ thay đổi trong việc sử dụng ngôn ngữ (ví dụ: kỳ vọng xã hội về các phản hồi tích cực hơn (Hohenstein và Jung, 2018)), các vấn đề về cách các cá nhân miêu tả bản thân và đánh giá người khác, đến các vòng phản hồi dài hạn dẫn đến cách chúng ta nhận thức bản thân. Tuy nhiên, hiểu biết về tác động của giao tiếp được trung gian bởi AI, đặc biệt là những giao tiếp được hỗ trợ bởi LLM mạnh mẽ, phần lớn đang phát triển (Hancock et al., 2020). Có khả năng rằng cá nhân hóa rộng rãi trong các agent giao tiếp LLM, sẽ cần thiết hiểu biết thêm về những yếu tố này và thiết kế các hệ thống kết hợp hiểu biết này để giảm thiểu tác hại.
