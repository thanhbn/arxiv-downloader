# 2311.09180.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/writing/2311.09180.pdf
# File size: 1666691 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PEARL : Personalizing Large Language Model Writing
Assistants with Generation-Calibrated Retrievers
Sheshera Mysore1△†Zhuoran Lu2†Mengting Wan1Longqi Yang1
Bahar Sarrafzadeh1Steve Menezes1Tina Baghaee1
Emmanuel Barajas Gonzalez1Jennifer Neville1Tara Safavi1△
2Purdue University, IN, USA1Microsoft, WA, USA
△Corresponding authors: smysore@cs.umass.edu ,tarasafavi@microsoft.com
Abstract
Powerful large language models have facili-
tated the development of writing assistants that
promise to significantly improve the quality
and efficiency of composition and communi-
cation. However, a barrier to effective assis-
tance is the lack of personalization in LLM
outputs to the author’s communication style,
specialized knowledge, and values. In this pa-
per, we address this challenge by proposing
PEARL , a LLM writing assistant personalized
with a retriever that is trained to be generation-
calibrated for personalization. Generation cal-
ibration ensures that our retriever selects his-
toric user authored documents to augment an
LLM prompt such that they are likely to help
an LLM generation better adhere to a users’
preferences. We propose two key novelties for
training such a retriever: (1) A training data
selection method that identifies historical user
requests likely to benefit from personalization
anddocuments that provide that benefit; and
(2) A scale-calibrating KL-divergence objective
that ensures that our retriever scores remain pro-
portional to the downstream generation qual-
ity from using the document for personalized
generation. In a series of holistic evaluations,
we demonstrate the effectiveness of PEARL in
generating long-form texts on multiple social
media datasets. Finally, we demonstrate how
a generation-calibrated retriever can double as
a performance predictor – detecting low qual-
ity retrieval, and improving potentially under-
performing outputs via revision with LLMs.
1 Introduction
Machine-assisted writing has seen a long history of
development, progressing from providing simple
syntactic checks, to revising human authored text,
to recent assistants being able to fully compose
texts on direction from authors (Mahlow, 2023;
Dale and Viethen, 2021). The text-generation ca-
pabilities of current LLMs and has led current re-
†Work done during internship at Microsoft Research.
Figure 1: PEARL is a request-driven generation model
that personalizes LLM outputs through retrieval aug-
mentation with a generation calibrated retriever.
search to explore a new frontier of writing assis-
tants for complex applications such as knowledge
synthesis (Shen et al., 2023), peer review (Chen
et al., 2023), and journalism (Wang et al., 2023c).
An important element of effective writing assistants
is being able to personalize generated text to retain
the knowledge, style, and values of a user – an
essential element of interpersonal communication
(Pickering and Garrod, 2013). With current LLMs
prone to generating overly generic text (Pu and
Demberg, 2023), author personalization of LLMs
is an important problem.
Personalizing LLM outputs may be seen as a
form of alignment to individual users of the LLM
(Kirk et al., 2023). However, leveraging fine-tuning
for alignment in a personalization setup poses chal-
lenges to serving trained per-user models and ob-
taining sufficient per-user alignment training data.
Therefore, we pursue in-context alignment through
retrieval augmentation (Salemi et al., 2023; Li et al.,
2023a). First, we assume access to a set of his-
toric user-authored documents (e.g. emails, social
media posts, etc.) and a user request for a per-
sonalized generation. To personalize LLM outputs
we propose an approach to train a retrieval model
that selects historic user documents to augment
an LLM’s prompt. Historic documents capturearXiv:2311.09180v2  [cs.CL]  5 Nov 2024

--- PAGE 2 ---
users’ personal style, knowledge, and values and
can serve as useful context for personalized genera-
tion. While training retrievers for non-personalized
applications have been explored in prior work (Go-
nen et al., 2022), this exploration has been limited
in personalized text generation. Finally, we pur-
sue personalization of LLMs only accessible via
prompt-based APIs since this represents a common
form of accessing performant large scale LLMs.
The starting point for our retriever in prior work
examining effective prompts for non-personalized
applications: Gonen et al. (2022) show the best
prompts to be those with the highest conditional
likelihood of generating a target text, and Rubin
et al. (2022) use these likelihoods to train retrieval
models for non-personalized retrieval augmenta-
tion of LLMs. While this approach performs well
in non-personalized setups, personalized text gen-
eration presents unique challenges and opportuni-
ties: There are fewer historic documents per user
(∼hundreds) than common non-personalized re-
trieval collections, and user requests may diverge
from their history as users’ preferences change. A
smaller retrieval corpus and shifting interests mean
that all requests cannot be satisfied by retrieval
from a users’ historical documents – as a result, all
historic requests and documents are unlikely to be
useful for training a retriever. Our first contribution
addresses this: We present a novel difference of
likelihoods -based method that identifies only the
personalizable user requests and associated docu-
ments that are likely to personalize downstream
generations, and use these to train our retriever.
Next, the personalization setup offers an oppor-
tunity: Fewer historical documents per user permits
the use of expressive cross-encoder retrievers in-
stead of scalable but less expressive biencoders
commonly used for non-personalized tasks (Rubin
et al., 2022). However, cross-encoders produce
skewed scores at the ends of their score ranges
(Menon et al., 2022; Yadav et al., 2022), hamper-
ing their ability to closely track the utility of a
document for personalized generation. We remedy
this with our second contribution – a personalized
scale-calibrating training objective (Yan et al.,
2022). This ensures that scores from our retriever
aregeneration-calibrated for personalization – i.e.
the score it produces for request-document pairs
is proportional to the output quality of an LLM
prompted with the pair. In a case study, we show
how generation calibration enables the retriever’sscores to be used for retrieval performance predic-
tion– detecting low-quality retrievals, and revising
potentially low-quality generations.
We instantiate PEARL with multiple LLMs,
davinci-003 andgpt-35-turbo , at privacy com-
pliant enterprise API endpoints and evaluate it on a
private dataset of workplace communications and a
public dataset of Reddit comments. For evaluation,
we use a variety of evaluation methods spanning in-
trinsic, extrinsic, and personalized LLM-as-judge
evaluations to demonstrate the value of PEARL .
Further, since we train calibrated retrieval models,
we present additional evaluations for calibration,
ablations, and analysis in Appendices. Our evalua-
tions demonstrate that PEARL consistently matches
or outperforms strong baseline approaches.
2 Related Work
Example selection for LLMs Early work on train-
ing retrievers for augmenting LLM contexts in
non-personalized applications was proposed by Ru-
bin et al. (2022). They train retrieval models by
distilling LLM likelihoods of the target comple-
tions conditioned on the prompt. Similarly Wang
et al. (2023b) train retrieval models on finer-grained
feedback from a trained reward model through
distillation. More distantly, Zhang et al. (2022)
train instances selection models on rewards from
a downstream evaluation metric using reinforce-
ment learning. Parallel with our work, Salemi et al.
(2024) train bi-encoders for personalized classifica-
tion and short text generation and find knowledge
distillation from downstream LLMs to outperform
reinforcement learning based training of retriev-
ers. In this regard, Salemi et al. (2024) and Rubin
et al. (2022) are closely related and represent clos-
est work to ours – we compare to such an approach
in ablations (Appendix C.2). Despite similarities
to our work, all prior work has explored training
retrievers for document selection while assuming
that satisfactory predictions can be made for all
inputs/requests. In addition to selecting documents
for training, we also select training requests that
benefit from retrieval augmentation – a necessity in
personalization where retrieval is performed over a
smaller historical document set instead of a large
shared corpus. Further, no prior approaches explore
calibration for retrievers and their ability to identify
low-quality retrievals, and selectively revise LLM
outputs – we explore this. Appendix D discusses
additional work on optimizing prompts, robustness

--- PAGE 3 ---
Figure 2: To train retriever, fretr, an auxiliary language model is first used to identify historical requests that can be
personalized and the best document to use for personalization 1⃝. Then, fretris trained on the selected data with a
scale calibrating loss function 2⃝. Given an unseen request, fretris used to select the best instances from historical
texts for augmenting an LLM prompt for personalized generation 3⃝. Our training results in a generation calibrated
retriever where scores for documents are proportional to the quality of the LLM output.
to prompt errors, and calibrated retrievers.
Personalized writing assistants While writing
assistants have seen considerable exploration, only
some prior work has focused on author person-
alization. These applications range from email
(Chen et al., 2019; Trajanovski et al., 2021), to
social media (Gero et al., 2022), and grammati-
cal error correction (GEC) (Nadejde and Tetreault,
2019). These systems commonly leverage nearest-
neighbor models (Chen et al., 2019; Trajanovski
et al., 2021) and user-group level parameter-
efficient fine-tuning for personalization (Nadejde
and Tetreault, 2019). In contrast, we explore re-
trieval models for in-context alignment/personal-
ization with LLMs. Parallel work has also explored
personalized writing with LLMs. Li et al. (2023b)
construct prompts with pre-trained retrieval and
summarization models and fine-tune an LLM for
personalized completion. Follow-on work has ex-
plored training a prompt-re-writer to tune prompts
for a fixed LLM (Li et al., 2023a). Prompt re-
writing is a complementary approach to a trained
retriever, with future systems likely to benefit from
both. Appendix D discusses non-personalized writ-
ing assistants and reader personalization.
3 Problem Definition
We consider a request-conditional, personalized
text generation task. As input to the system, we
assume a user uwho is associated with a set of
Nuhistorical documents Du={d(i)
u}Nu
i=1, where
each document dumay be a previously-authored
social media post, email, etc. The user uis fur-
ther associated with a textual request qusubmittedto the writing assistant. The request may be au-
thored by the user or constructed from the task
context. Explicitly authored requests are increas-
ingly common in conversational LLM interfaces
(Papenmeier et al., 2021), and task contexts may
be seen as implicit requests e.g. email prefixes that
require completion (Chen et al., 2019). Finally, we
assume access to a large language model fLLM
available via a prompt-based text generation API.
Given Du,qu, and fLLM, our retriever, fretris
trained to select a subset of historical documents
D′
u⊂ D uas few-shot examples for the LLM.
Then the LLM generates a target text tuof up
to 300 words: tu=fLLM(ϕ(qu,D′
u)), where ϕ
is a prompt construction function that inputs the
user’s request and retrieved historical documents,
tureflects the style, knowledge, and values of u.
4 Proposed Approach
We present PEARL , an in-context aligned LLM-
based model for personalized writing assistance.
Our approach (Figure 2) consists of an offline re-
triever training stage and an online LLM infer-
ence stage. Offline, we train a retriever fretr:
(qu, du)→Rthat scores the user’s historical doc-
uments for their ability to personalize the output
for a user request. Further, we ensure that fretris
generation calibrated i.e. the scores it produces for
(qu, du)pairs are proportional to the quality of the
generated text from using (qu, du)in a prompt. We
train such a retriever through two key novelties: (1)
Training data selection based on a novel difference
of likelihoods from an auxiliary text generation
model – we identify requests which benefit from

--- PAGE 4 ---
personalization anddocuments which likely help
personalize a target, and (2) A scale-calibrating
training objective which ensures that retrievers
closely track the benefit of request-document pairs
for generation. Given a new request, our LLM is
prompted to generate a target text tuconditioned
on the request and the documents retrieved by fretr.
Next, we describe the retriever training set construc-
tion (Algorithm 1), how we optimize the retriever,
and the details of our implementation.
4.1 Training Data Setup
To optimize fretrfor a personalized text generation
task, we carefully create a training set for fretrfrom
historical user documents by using an auxiliary
text generation model fauxto identify which re-
quests and documents will help to personalize the
generation of a target text.
Data organization We organize the training data
to create a setup close to the problem defined in
§3. Given a set of Musers and their historical
document sets {Du}M
u=1, for each user uwe parti-
tionDuinto two non-overlapping sets, a candidate
document set Dc
u⊂ D u, and a “target” text set
Dt
u⊂ D u, such that Dc
u+Dt
u=Du. The partition-
ing is done temporally, i.e. the target texts occur
after the candidate documents, mimicking the per-
sonalization scenario where past texts are used to
personalize later targets. If time data isn’t available,
the partitioning may be done randomly.
Next, for each target text tuin each users Dt
u,
we pair the text with a corresponding request qu.
For training, requests may be naturally present in
the data, e.g., email prefixes that require comple-
tion (Chen et al., 2019), or they may be generated
synthetically (Bonifacio et al., 2022). We detail
request generation in §5.1.
Auxiliary model scoring Next, we use the aux-
iliary text generation model fauxto score each can-
didate document in du∈ Dc
ufor producing the
personalized tucorresponding to the qufor each
(qu, tu)∈ Dt
u. We define the score as a difference
in the likelihood, per faux, of the target given the
request with and without the historical document:
yduqu=logpaux(tu|du, qu)−logpaux(tu|qu),(1)
Importantly, Eq. (1)is highest when the request
is suitable for personalization and the candidate
document is the “right” example for personaliza-
tion. That is, the request alone is not sufficient for
generating the target text (i.e., the quantity definedby the second term is lower), and this candidate
document is particularly beneficial to generation
(i.e., the quantity defined by the first term is higher).
Finally, we assume model fauxto be smaller than
fLLMto support efficient creation of training data,
and that we have access to its token likelihoods.
Appendix A shows prompts used for faux.
4.2 Training Data Selection
We use the scores from Eq. 1 to identify: (1) a
subset of training requests that are likely to benefit
from personalization; and (2) candidate documents
that are likely to benefit those requests i.e. positive
training documents.
Request selection Using Eq. 1, we score all
request-target pairs of a user in Dt
uagainst all of
their candidate documents du∈ Dc
u, across all M
users. After scoring, we retain the top scoring T
request-target pairs. In practice, we find that setting
Tto the top two-thirds across the dataset works
well. This step reflects the intuition that not all
request-target pairs will benefit from retrieval aug-
mentation, either due to the lack of suitable candi-
date documents in a user’s historical document set,
or due to underspecified requests making the target
text simply too difficult to generate well – this is
contrast with RAG setups in non-personalized sce-
narios where a large retrieval corpus ensures that
most requests are likely to benefit from retrieval.
After obtaining a high-quality set of training re-
quests {q∗
u}T
t=1, we discard the target texts, since
they aren’t used for training fretror for inference.
Candidate document selection Next, we use
Eq. 1 to select the best documents for the retained
requests, i.e. identify positive training documents.
Given a request q∗
uselected for training, we take the
Phighest-scoring candidate documents du∈ Dc
u
as per Eq. (1)as positives, {d+
u}P
p=1. We sample N
negative samples per positive randomly from the
candidate document set for the user.
4.3 Retriever Optimization
Ourfretris a cross-encoder initialized with a pre-
trained LM encoder and trained using data selected
per Algorithm 1, through distillation of scores in
Equation 1. While cross-encoders are expressive
they produce scores which lie at the extremes of
their score ranges (Menon et al., 2022; Yadav et al.,
2022) – this hampers their ability to closely track
the benefit of candidate documents for personaliz-
ing requests. We propose to remedy this through a
scale calibrating training objective.

--- PAGE 5 ---
Algorithm 1 Selecting requests and positive candi-
date documents to train fretr
1:Input :{Du}M
u=1,faux ▷Historical documents for M
users and an auxiliary LM
2:foreach user udo
3: Dc
u,Dt
u←TemporalPartition( Du)▷Temporally
partition Duinto candidate and target documents
4: foreach target text tu∈ Dt
udo
5: qu←GetRequest( tu)▷Obtain a synthetic or
natural request
6: end for
7: foreach(qu, tu)pair in Dt
udo▷Compute benefit of
personalization for request-target pairs
8: foreach candidate duinDc
udo
9: Y[qu, du] = log paux(tu|du, qu)−
logpaux(tu|qu) ▷Equation (1)
10: end for
11: end for
12:end for
13:{q∗
u}T
t=1←TopK( Y[qu, du]) ▷Retain the top T
unique requests which are personalizable
14:foreach retained request in {q∗
u}T
k=1do
15: {d+
u}P
p=1←TopK( Y[q∗
u, du])▷Retain the top P
candidates that best personalize the target
16:end for
17:return {q∗
u,{d+
u}P
p=1}T
t=1
Scale calibration Letyq= [y+
q, . . . , y−
q], where
y+
qcorresponds to the score of a positive docu-
ment and y−
qcorresponds to the score of a nega-
tive document from Eq. 1. Here, yqcontains N
negatives and 1 positive document. Similarly, let
the predicted logits from fretr: (qu, du)→Rbe
denoted as sq= [s+
q, . . . , s−
q]. Then, a standard
KL-divergence loss is written as KL(yq,sq) =
−P
ism(yq,i)logsm(sq,i), where smrepresents the
softmax function. Our proposed scale calibration
modifies the KL divergence loss by adding an “an-
chor” example with target score y0, which is a tun-
able hyperparameter, and logit s0set to 0, resulting
in score vectors y′
q= [y0,yq]ands′
q= [s0,sq].
The scale-calibrated KL-divergence loss is thus
KL(y′
q,s′
q) =−X
ism(y′
q,i)logsm(s′
q,i)(2)
=−X
ieyq,i
P
jeyq,j+ey0logesq,i
P
jesq,j+ 1
+ey0
P
jeyq,j+ey0log(X
jesq,j+ 1).(3)
We find that setting y0to the median value of
scores from Eq (1)for positive candidate docu-
ments works well. This ensures that very large
scores from fretrare penalized (second term Eq 3)
and smaller scores are prevented from being driven
lower (first term Eq 3). Therefore fretrscores are
more evenly distributed over the score range. In
practice, this ensures that predicted scores fromfretrmore accurately reflect the distribution of faux,
which in turn more closely tracks the utility of
request-document pairs for personalization. We
compare PEARL to baselines in §5.2 and present
ablations in §C.2.
4.4 System Details
After training retriever fretroffline, PEARL may be
used to serve requests online. Given a unseen re-
quest, fretrretrieves the top- khistorical texts from
Du, these are formatted into a prompt and input to
fLLMto generate a personalized target text tu.
Ourfretris initialized with a 110M parameter
MPN ETencoder (Song et al., 2020). For fLLMwe
consider two performant LLMs, davinci-003 and
gpt-3.5-turbo . For fauxwe use FLANT5-XL
with 3 billion parameters (Chung et al., 2022). Ap-
pendix A details our prompts and implementation.
5 Experiments
We demonstrate the effectiveness of PEARL on
two personalized text generation datasets from so-
cial media platforms. For evaluation, we employ
standard intrinsic evaluations, extrinsic evaluation
based on downstream tasks using the generated
text, and recently proposed personalized LLM-as-
judge (Wang et al., 2023d). Then, in §5.3 we show
how a calibrated retriever can be used for selective
revision of underperforming requests. We present
ablations in §C.2 and we demonstrate the calibra-
tion performance for our retriever in §C.3.
5.1 Experimental Setup
Data For evaluation, we use two open-ended long-
form text generation datasets for social media: (1)
Personalized post writing on WORKSMand (2)
Personalized comment writing on AITA.
WORKSM W ORKSMis an enterprise social net-
work used for communication within organizations
presenting a highly realistic platform for writing
assistance. We obtain a random sample of ∼18k
posts written by 1116 users from November 2020
to July 2023. To create an evaluation set , we man-
ually examine posts greater than 50 words and re-
ceiving ≥2 comments, about 1K posts, and select
163 of the most recent posts from ∼80 users to
serve as reference target texts t∗
u. These posts repre-
sent a diverse, engaging set that could benefit from
personalized writing assistance and serve as high
quality target references. At a high level, these
posts share events, research studies, campaigns,

--- PAGE 6 ---
and organizational news. Since WORKSMdoes
not contain requests to the writing assistant, two
authors not involved in model development man-
ually wrote requests quper target text. Note that
this was necessary given the highly regulated and
private enterprise data in WORKSMpreventing ex-
posure to external crowdworkers. Our requests
were authored following Guideline 1. To construct
Duposts created before t∗
uwere used: On aver-
age, users had 31historic posts (max of 169). To
create our training set , we only retain posts >10
words and users with ≥5historic posts while ex-
cluding posts in our evaluation set. We generate
synthetic requests with GPT-4 for training given
the expense of manually authored requests – result-
ing in a set of ∼7k training requests. Enterprise
contracts with API providers ensured the privacy
of user data shared over the API.
AITA AITA is a Reddit subforum in which orig-
inal posters (OP) describe personal moral conflicts
and receive comments from other users judging
them to be “the a**hole” or “not the a**hole”. This
dataset has been used in prior work on modeling
the personal values of users (Plepi et al., 2022). We
construct a personalized comment generation task
from this data. We treat the OP posts as requests qu,
user comments as reference target texts t∗
u, and a
user’s previous comments as Du. Since the dataset
lacks time metadata, we construct an evaluation
setby sampling 10% of the posts as test requests,
and further filter to 600random target texts for our
evaluation set to keep LLM experiments feasible.
Evaluation users had 29posts in Duon average
(max of 590). Our training set used the historical
post-comment pairs from users in Du, resulting in
∼84k requests. Note that while Reddit comments
are not the ideal platform for writing assistance,
AITA is one of the few public datasets available
for the task and resembles applications such as
email response generation (Kannan et al., 2016).
Appendix B details our datasets further.
Generation metrics Since personalized text gen-
eration aims to adhere to the style, knowledge, and
values of specific users, effective evaluation for
personalized generation remains an open problem
(Wang et al., 2023d,a). This is in contrast to non-
personalized generation, where desirable aspects
of outputs can be defined uniformly across all test
cases. As a result, we present evaluations using a
host of standard evaluation setups aiming to demon-
strate the effectiveness of PEARL from various per-spectives. Our evaluations span the following stan-
dard setups (Dou et al., 2023): intrinsic evaluations
based on n-gram/embedding similarity to reference
texts, extrinsic evaluation through a classification
accuracy based on generated text, and pairwise
evaluation with personalized LLM-as-judge.
Specifically, for WORKSMwe report standard
evaluation measures based on n-gram and em-
bedding similarity between generations and ref-
erence targets: ROUGE-1 (R1), ROUGE-2 (R2), and
BertScore-F1 (BS-F1) (Zhang* et al., 2020). This
serves as an intrinsic evaluation for WORKSMmea-
suring the extent to which generations are similar
to user authored texts. Next, since AITA users’
comments primarily make a stance based on users’
moral values, we measure if the stance in gener-
ated comments matches that of the user through a
downstream stance prediction task – serving as an
extrinsic evaluation. This evaluation may be seen
as evaluating the extent to which model generations
adhere to a user’s values. We map generated com-
ments to a binary “YTA” or “NTA” label based on
simple high-precision rules mapping lexical varia-
tions of “you’re the a**hole” and “not the a**hole”
to the labels. This procedure was also found reli-
able for constructing ground truth labels in AITA
(Plepi et al., 2022). Note that early attempts of us-
ing n-gram/embedding similarity measures for eval-
uation (BS-F1, R1, R2) resulted in unreliable eval-
uations for AITA due the large variation (length,
vocabulary, emojis etc.) in AITA comments, there-
fore we opt for more stable extrinsic evaluations
and LLM based evaluations described next.
For both AITA andWORKSMwe conduct a pair-
wise evaluation with a recently proposed person-
alized LLM-as-judge (Wang et al., 2023d). Wang
et al. show LLM based author identifications to be
a reliable proxy task for distinguishing models of
various qualities and being correlated with human
quality ratings. Here, a judge LLM is presented
with a reference text from a user and generations
from the pair of systems being compared, then, it
is prompted to select the system generation more
likely to be authored by the author of the reference
text. An author identification task aims to capture
several aspects which distinguish individuals’ writ-
ing, spanning style, knowledge and their values.
In our evaluation, we compare PEARL outputs to
the outputs from the best baseline as indicated by
intrinsic/extrinsic evaluations and use the target ref-
erence text t∗
uin the LLM prompt as an example

--- PAGE 7 ---
LLM→ davinci-003 gpt-35-turbo
Method ↓Macro F1(%) Macro F1(%)
ZSHOT-NP 41.97 50.43
KSHOT-NP 51.71 59.76
Random 55.52 59.47
BM25 57.26 61.66
MPN ET-1B 53.72 59.23
UPR 55.76 58.15
RelevanceCE 56.85 59.59
PEARL 61.21 65.34
(a) Extrinsic classification accuracy in AITA .LLM→ davinci-003 gpt-35-turbo
Method ↓ BS-F1 R1 R2 BS-F1 R1 R2
ZSHOT-NP 36.25 0.5029 0.2516 31.03 0.4627 0.2091
KSHOT-NP 34.08 0.4931 0.2431 32.51 0.4825 0.2258
Random 35.04 0.5036 0.2505 33.46 0.4893 0.2345
BM25 37.96 0.5287 0.2911 36.57 0.5089 0.2673
MPN ET-1B 38.30 0.5281 0.2931 36.02 0.5063 0.2639
UPR 38.70 0.5337 0.3019 35.98 0.5054 0.2642
RelevanceCE 37.81 0.5288 0.2953 35.99 0.5038 0.2613
PEARL 39.60 0.5419 0.3094 36.49 0.5082 0.2676
(b) Intrinsic reference based metrics in W ORKSM.
Table 1: P EARL is compared to non-personalized (NP) and LLMs personalized with retrieval on datasets of social
media communication: (a) a dataset constructed from Reddit and (b) a workplace social media dataset.
of the users writing. We use GPT-4o as our judge
LLM and present the judge prompt in Appendix
B.4. In our evaluation we avoid rating aspects such
as fluency, non-redundancy, etc. (Celikyilmaz et al.,
2021) since we are primarily concerned with per-
sonalization performance and these qualities may
be in conflict with specific users writing.
Baselines As baselines, we consider non-
personalized models based on zero shot prompt-
ing ( ZSHOT-NP) and few-shot prompting with
krandomly chosen example documents ( KSHOT-
NP). We consider retrieval-augmented personal-
ized baselines, which selecting from a user’s histor-
ical documents Du. They span selection at random
fromDu(Random), with sparse retrieval by BM25,
with dense retrieval by a strong MPN ETmodel
trained on 1 billion text pairs ( MPNET-1B), an un-
supervised crossencoder (Sachan et al., 2022) rank-
ing documents with FLANT5-B ASE likelihood:
p(qu|du)(UPR), and a supervised crossencoder
optimized on our dataset with request-document
pairs, (qu, du)inDu(RelevanceCE). Appendix
B.3 details our baselines.
5.2 Generation Evaluation
Table 1 and 2 report our evaluations. Appendix C
presents ablation (C.2) and calibration (C.3) results.
Reference based evaluation Tables 1b and 1a
reports automated metrics on AITA andWORKSM.
First we observe that personalization through re-
trieval, even at Random, generally improves upon
non-personalized approaches (NP), which is con-
sistent with prior work (Salemi et al., 2023). Next,
we note that the best baseline is not consistent,
varying between BM25, and unsupervised crossen-
coder (UPR) – indicating that retrieval models de-
signed for request-document relevance vary in per-davinci-003 gpt-35-turbo
P / B / T (%) P / B / T (%)
AITA 46.8 /40.3 /12.8 α=0.56 46.6 /44.9 /8.3 α=0.55
WORKSM46.6 /42.5 /10.8 α=0.42 38.9 / 42.6 /18.5 α=0.28
Table 2: LLM-as-judge win-rate evaluation for AITA
andWORKSMselecting a generation to be more aligned
with an authors writing sample. The LLM could prefer
the Proposed system ( PEARL ), the Baseline (BM25), or
judge the outputs as Tied – denoted with P, B, and T.
formance depending on the dataset and inference
LLM. Finally, we note that PEARL consistently
performs at par or better than the best baselines
across datasets and LLMs, indicating the effective-
ness of training fretrfor personalized generation.
For the more reliable classification metrics obtain-
able in AITA ,PEARL outperforms all baselines
with improvements of 1.5 to 5 Macro F1 points.
Next, we report performance in more expressive
LLM-as-judge evaluations.
Pairwise LLM-as-judge evaluation In Table 2
we report the results of personalization evaluation
following the setup described in §5.1. Here, we
compare against BM25-augmented as it performs
within our top 2 baselines in automatic evaluations
- this strong performance is consistent with prior
work (Izacard et al., 2022; Thakur et al., 2021). We
use GPT-4o as a judge LLM and run every pair of
inputs through the judge LLM 3 times, we report
average win rates over all the instances in our test
set and over 3 repeated runs. Further, we randomly
swap the position of the baseline and proposed
method generations in the prompt to account for
position biases in the judge LLM. Finally, we also
report the agreement between the 3 judge LLM
runs using Krippendorff’s alpha ( α) to ensure that

--- PAGE 8 ---
LLM judgements are consistent across runs.
In Table 2, PEARL achieves a greater win-rate
than BM25 in 3 of 4 settings. In these settings we
also note that the LLM judgments remain consis-
tent across 3 repeated runs with Krippendorff’s al-
pha between 0.41−0.56(0 indicates chance agree-
ment). While BM25 sees a greater win-rate in
WORKSMwith gpt-35-turbo , the judgments see
lower agreement ( α= 0.28) indicating the outputs
to be harder to distinguish. Finally, comparing to
Table 1 we see that the trends of extrinsic and in-
trinsic reference based evaluations are retained in
LLM-as-judge evaluations – consistently indicat-
ing the benefit of PEARL across evaluation setups,
inference LLMs, and datasets. In Appendix C we
show an example from AITA to show the kinds of
retrievals and outputs that make P EARL effective.
5.3 Selective Revision with P EARL
Having established PEARL to be an effective model
for generation, we show fretrto be generation cali-
brated in Appendix C.3. Here, we demonstrate the
usefulness of a calibrated retriever in a case study
using the retriever scores to selectively revise gen-
erations. Specifically, we treat the scores from fretr
as a predictor of retrieval performance, and in-turn
text generation performance. We assume that if
fretrcannot find a highly scored in-context exam-
ple, the generated response will be of low quality
and can benefit from LLM revision (Figure 3).
Setup Given our trained retriever, we take all
top-1 document scores for each request s1=
max du∈Dufretr(qu, du)and learn a threshold θon
s1that maximizes a downstream performance met-
ric on a held-out development set ( R2inWORKSM
and Macro-F1 in AITA ). Then, given a generated
target text tuwiths1< θ, we selectively revise
tuwhere fLLMis prompted to edit the target text.
We report results of selective revision compared
to a single round of generation (i.e., no revision)
and full revision over the entire dataset (i.e., 100%
revision). We repeat this for BM25. We provide
further details and analysis in Appendix C.4.
Results In Table 3 we see that selective revi-
sion improves or retains performance upon a single
round of generation (“Stage 1”) by 2-4% in down-
stream performance metrics with fretr=Proposed
and BM25 for WORKSM. However, for AITA we
see that selective revision based on BM25 shows a
marked drop in performance indicating its dataset
dependent calibration performance. Importantly,Dataset → AITA W ORKSM
Method ↓/ LLM → gpt-35-turbo gpt-35-turbo
fretr=BM25 Macro F1 (%) BS-F1 R1 R2
Stage 1 (no revision) 59.99 36.15 0.5052 0.2611
All (100% revision) 58.36 35.45 0.5096 0.2573
Selective revision 57.71 37.29 0.5206 0.2738
fretr=Proposed Macro F1 (%) BS-F1 R1 R2
Stage 1 (no revision) 65.15 37.02 0.5124 0.2709
All (100% revision) 64.85 35.47 0.5045 0.2520
Selective revision 65.36 37.71 0.5236 0.2818
Table 3: Selectively revising target texts tubased on
scores from our retriever vs BM25. Also present are
results of no revision and revising all outputs (100%
revision) from Stage 1 outputs.
Figure 3: Generation calibration of fretrallows us to
use its predicted scores for performance prediction and
selectively revise potentially bad generations.
note that Macro F1 doesn’t measure aspects of style
which may have changed in revision. Finally, edit-
ingalloutputs produced by Stage 1 generation
consistently leads to degraded performance (“All”),
indicating that editing is not always helpful.
We also observe that PEARL chooses 75.8%
and77.9%instances for editing in WORKSMand
AITA , respectively. This indicates the potential
for generation calibrated retrievers to reduce the
number of expensive LLM calls made while ensur-
ing better personalization performance. In Figure
5 (Appendix C.4) we analyze the performance of
selective revision against request and user profile
length. In a manual examination of requests with
a low s1score by the PEARL fretr, we find the
requests to be underspecified and often require fur-
ther information from a user e.g. the request “Write
a post about how I like to relax after work”, aims to
generate a target discussing more specific forms of
relaxation not present in any historical documents.
This indicates that generation calibrated retrievers
may be used for other forms of selective prediction
and user interaction – e.g. selectively withholding
predictions when satisfactory generations are un-
likely or obtaining more information from users
through follow-up questions. We leave such explo-
rations to exciting future work.

--- PAGE 9 ---
6 Conclusion
In this paper we present PEARL – an LLM based
writing assistant personalized with generation cali-
brated retrievers. We propose a method for training
generation calibrated retrievers through a careful
selection of training data and a scale calibrated
objective. In a series of holistic evaluations, we
demonstrate the effectiveness of our approach in
datasets of social media communication compared
to baselines ( §5.2) as well as ablated models (Ap-
pendix C.2). We demonstrate the calibration perfor-
mance for our retriever (Appendix C.3), and show
how our retrieval model can double as a perfor-
mance predictor ( §5.3) and can identify outputs
which can benefit from LLM revision.
7 Ethical and broader impact
Having introduced PEARL as an effective personal-
ization strategy for writing assistance and discussed
its benefits we review two implications of concern
arising from better personalized text generation:
challenges to factuality, and longer term influence
on language use and communication.
Challenges to factuality The emergence of
LLMs and their ability to generate compelling text
has seen a subsequent rise in the cases of malicious
use of these technologies. Augenstein et al. (2023)
overview four such classes of harm: personalized
attacks on individuals in the form of phishing at-
tacks and tailored misinformation, impersonation
of trusted figures (e.g. journalists or regulators), a
glut of paraphrased misinformation evading detec-
tion by automatic tools often used by fact checkers,
and large scale creation of fake social media pro-
files and plagiarized content (Brewster et al., 2023).
It is possible that improvements in personalized
text generation are likely to excacertabe each of
these problems. To account for this, several tech-
nology and policy initiatives are under active de-
velopment (Augenstein et al., 2023). These span
detection of AI-generated content, cryptographic
signatures intended to prove the authenticity of
content, to government regulation and public edu-
cation, however, their effectiveness remains under
investigation.
Language use and communication Current un-
derstanding of computer mediated communication
suggests that users interpersonal communication
patterns are influenced by the tool/medium used
for communication (Poddar et al., 2023) with a
potential for these influences to have longer terminfluences on communication in the absence of
these tools (Hancock et al., 2020). Hancock et al.
outline these implications as ranging from shifts
in language use (e.g a social expectation of more
positive responses (Hohenstein and Jung, 2018)),
issues of how individuals portray themselves and
evaluate others, to long term feedback loops re-
sulting in how we perceive ourselves. However,
understanding of the implications of AI mediated
communication, specially those powered by pow-
erful LLMs, is largely developing (Hancock et al.,
2020). It is likely that wide spread personalization
in LLM communication agents, will necessitate fur-
ther understanding of these factors and the design
of systems that incorporates this understanding to
ameliorate harms.
References
Nader Akoury, Shufan Wang, Josh Whiting, Stephen
Hood, Nanyun Peng, and Mohit Iyyer. 2020. STO-
RIUM: A Dataset and Evaluation Platform for
Machine-in-the-Loop Story Generation. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
6470–6484, Online. Association for Computational
Linguistics.
Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He,
and Xing Xie. 2021. PENS: A dataset and generic
framework for personalized news headline genera-
tion. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and
the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 82–92, Online. Association for Computational
Linguistics.
Simran Arora, Avanika Narayan, Mayee F Chen, Lau-
rel Orr, Neel Guha, Kush Bhatia, Ines Chami, and
Christopher Re. 2023. Ask me anything: A sim-
ple strategy for prompting language models. In The
Eleventh International Conference on Learning Rep-
resentations .
Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha,
Tanmoy Chakraborty, Giovanni Luca Ciampaglia,
David Corney, Renee DiResta, Emilio Ferrara, Scott
Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo
Menczer, Ruben Miguez, Preslav Nakov, Dietram
Scheufele, Shivam Sharma, and Giovanni Zagni.
2023. Factuality challenges in the era of large lan-
guage models. arXiv preprint arXiv:2310.05189 .
Tal August, Katharina Reinecke, and Noah A. Smith.
2022. Generating scientific definitions with control-
lable complexity. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 8298–8317,

--- PAGE 10 ---
Dublin, Ireland. Association for Computational Lin-
guistics.
Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and
Rodrigo Nogueira. 2022. Inpars: Unsupervised
dataset generation for information retrieval. In Pro-
ceedings of the 45th International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval , SIGIR ’22, page 2387–2392, New York,
NY , USA. Association for Computing Machinery.
Jack Brewster, Macrina Wang, and Coalter Palmer. 2023.
Plagiarism-bot? how low-quality websites are using
ai to deceptively rewrite content from mainstream
news outlets. NewsGaurd, The Internet Trust Tool.
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.
2021. Evaluation of text generation: A survey. arXiv
preprint arXiv:2006.14799 .
Tuhin Chakrabarty, Vishakh Padmakumar, and He He.
2022. Help me write a poem: Instruction tuning as a
vehicle for collaborative poetry writing. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 6848–6863,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Mia Xu Chen, Benjamin N. Lee, Gagan Bansal, Yuan
Cao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yi-
nan Wang, Andrew M. Dai, Zhifeng Chen, Timothy
Sohn, and Yonghui Wu. 2019. Gmail smart com-
pose: Real-time assisted writing. In Proceedings
of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining , KDD ’19,
page 2287–2295, New York, NY , USA. Association
for Computing Machinery.
Shiping Chen, Duncan P Brumby, and Anna L Cox.
2023. Using writing assistants to accelerate the peer
review process. Second Workshop on Intelligent and
Interactive Writing Assistants, CHI 2023 .
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Elizabeth Clark, Anne Spencer Ross, Chenhao Tan,
Yangfeng Ji, and Noah A. Smith. 2018. Creative
writing with a machine in the loop: Case studies on
slogans and stories. In 23rd International Confer-
ence on Intelligent User Interfaces , IUI ’18, page
329–340, New York, NY , USA. Association for Com-
puting Machinery.
Daniel Cohen, Bhaskar Mitra, Oleg Lesota, Navid Rek-
absaz, and Carsten Eickhoff. 2021. Not all relevance
scores are equal: Efficient uncertainty and calibration
modeling for deep retrieval models. In Proceedings
of the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval ,
SIGIR ’21, page 654–664, New York, NY , USA. As-
sociation for Computing Machinery.Antonia Creswell, Murray Shanahan, and Irina Higgins.
2023. Selection-inference: Exploiting large language
models for interpretable logical reasoning. In The
Eleventh International Conference on Learning Rep-
resentations .
Robert Dale and Jette Viethen. 2021. The automated
writing assistance landscape in 2021. Natural Lan-
guage Engineering , 27(4):511–518.
Shehzaad Dhuliawala, Leonard Adolphs, Rajarshi Das,
and Mrinmaya Sachan. 2022. Calibration of machine
reading systems at scale. In Findings of the Asso-
ciation for Computational Linguistics: ACL 2022 ,
pages 1682–1693, Dublin, Ireland. Association for
Computational Linguistics.
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,
Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Ja-
son Weston. 2023. Chain-of-verification reduces hal-
lucination in large language models. arXiv preprint
arXiv:2309.11495 .
Yao Dou, Philippe Laban, Claire Gardent, and Wei Xu.
2023. Automatic and human-ai interactive text gen-
eration. arXiv preprint arXiv:2310.03878 .
Asma Farajidizaji, Vatsal Raina, and Mark Gales. 2023.
Is it possible to modify text to a target readability
level? an initial investigation using zero-shot large
language models. arXiv preprint arXiv:2309.12551 .
Katy Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022.
Sparks: Inspiration for science writing using lan-
guage models. In Proceedings of the 2022 ACM
Designing Interactive Systems Conference , DIS ’22,
page 1002–1019, New York, NY , USA. Association
for Computing Machinery.
Hugo Gon c ¸alo Oliveira. 2017. A survey on intelligent
poetry generation: Languages, features, techniques,
reutilisation and evaluation. In Proceedings of the
10th International Conference on Natural Language
Generation , pages 11–20, Santiago de Compostela,
Spain. Association for Computational Linguistics.
Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith,
and Luke Zettlemoyer. 2022. Demystifying prompts
in language models via perplexity estimation. arXiv
preprint arXiv:2212.04037 .
Yue Guo, Wei Qiu, Yizhong Wang, and Trevor Co-
hen. 2021. Automated lay language summariza-
tion of biomedical scientific reviews. Proceedings
of the AAAI Conference on Artificial Intelligence ,
35(1):160–168.
Shivanshu Gupta, Matt Gardner, and Sameer Singh.
2023. Coverage-based example selection for in-
context learning. arXiv preprint arXiv:2305.14907 .
Jeffrey T Hancock, Mor Naaman, and Karen Levy. 2020.
AI-Mediated Communication: Definition, Research
Agenda, and Ethical Considerations. Journal of
Computer-Mediated Communication , 25(1):89–100.

--- PAGE 11 ---
Jess Hohenstein and Malte Jung. 2018. Ai-supported
messaging: An investigation of human-human text
conversation with ai support. In Extended Abstracts
of the 2018 CHI Conference on Human Factors in
Computing Systems , CHI EA ’18, page 1–6, New
York, NY , USA. Association for Computing Machin-
ery.
Krystal Hu. 2023. Chatgpt sets record for fastest-
growing user base - analyst note. National Bureau of
Economic Research, Digest No. 6.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022. Unsupervised dense informa-
tion retrieval with contrastive learning. Transactions
on Machine Learning Research .
Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kauf-
mann, Andrew Tomkins, Balint Miklos, Greg Cor-
rado, Laszlo Lukacs, Marina Ganea, Peter Young,
and Vivek Ramavajjala. 2016. Smart reply: Auto-
mated response suggestion for email. In Proceedings
of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining , KDD ’16,
page 955–964, New York, NY , USA. Association for
Computing Machinery.
Omar Khattab, Keshav Santhanam, Xiang Lisa
Li, David Hall, Percy Liang, Christopher Potts,
and Matei Zaharia. 2023. Demonstrate-search-
predict: Composing retrieval and language mod-
els for knowledge-intensive nlp. arXiv preprint
arXiv:2212.14024 .
Hannah Rose Kirk, Bertie Vidgen, Paul R ¨ottger, and
Scott A Hale. 2023. Personalisation within bounds:
A risk taxonomy and policy framework for the align-
ment of large language models with personalised
feedback. arXiv preprint arXiv:2303.05453 .
Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong,
and Michael Bendersky. 2023a. Automatic prompt
rewriting for personalized text generation. arXiv
preprint arXiv:2310.00152 .
Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing
Wang, Spurthi Amba Hombaiah, Yi Liang, and
Michael Bendersky. 2023b. Teach llms to person-
alize – an approach inspired by writing education.
arXiv preprint arXiv:2308.07968 .
Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin
Wang, Michal Lukasik, Andreas Veit, Felix Yu, and
Sanjiv Kumar. 2023c. Large language models with
controllable working memory. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 1774–1793, Toronto, Canada. Association for
Computational Linguistics.
Zihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng
Han, Matthew Shardlow, and Goran Nenadic. 2023d.
Large language models and control mechanisms im-
prove text readability of biomedical abstracts.Zhiyu Lin, Upol Ehsan, Rohan Agarwal, Samihan
Dani, Vidushi Vashishth, and Mark Riedl. 2023. Be-
yond prompts: Exploring the design space of mixed-
initiative co-creativity systems. In ICCC .
Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tian-
hua Zhang, Yoon Kim, Xixin Wu, Danny Fox, He-
len Meng, and James Glass. 2023. Sail: Search-
augmented instruction learning. arXiv preprint
arXiv:2305.15225 .
Cerstin Mahlow. 2023. Writing tools: Looking back
to look ahead. Second Workshop on Intelligent and
Interactive Writing Assistants, CHI 2023 .
Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni,
and Julian McAuley. 2019. Generating personalized
recipes from historical user preferences. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 5976–5982, Hong
Kong, China. Association for Computational Linguis-
tics.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 9802–9822, Toronto,
Canada. Association for Computational Linguistics.
Pierre-Emmanuel Mazar ´e, Samuel Humeau, Martin Rai-
son, and Antoine Bordes. 2018. Training millions of
personalized dialogue agents. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing , pages 2775–2779, Brussels,
Belgium. Association for Computational Linguistics.
Aditya Menon, Sadeep Jayasumana, Ankit Singh Rawat,
Seungyeon Kim, Sashank Reddi, and Sanjiv Kumar.
2022. In defense of dual-encoders for neural ranking.
InProceedings of the 39th International Conference
on Machine Learning , volume 162 of Proceedings
of Machine Learning Research , pages 15376–15400.
PMLR.
Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman,
and Richard Evans. 2023. Co-writing screenplays
and theatre scripts with language models: Evaluation
by industry professionals. In Proceedings of the 2023
CHI Conference on Human Factors in Computing
Systems , CHI ’23, New York, NY , USA. Association
for Computing Machinery.
Sonia K. Murthy, Kyle Lo, Daniel King, Chandra Bha-
gavatula, Bailey Kuehl, Sophie Johnson, Jonathan
Borchardt, Daniel S. Weld, Tom Hope, and Doug
Downey. 2022. Accord: A multi-document approach
to generating diverse descriptions of scientific con-
cepts.

--- PAGE 12 ---
Maria Nadejde and Joel Tetreault. 2019. Personalizing
grammatical error correction: Adaptation to profi-
ciency level and L1. In Proceedings of the 5th Work-
shop on Noisy User-generated Text (W-NUT 2019) ,
pages 27–33, Hong Kong, China. Association for
Computational Linguistics.
Jianmo Ni, Zachary C. Lipton, Sharad Vikram, and Ju-
lian McAuley. 2017. Estimating reactions and recom-
mending products with generative models of reviews.
InProceedings of the Eighth International Joint Con-
ference on Natural Language Processing (Volume 1:
Long Papers) , pages 783–791, Taipei, Taiwan. Asian
Federation of Natural Language Processing.
Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu,
Dong Yu, and Jianshu Chen. 2023. Knowledge-in-
context: Towards knowledgeable semi-parametric
language models. In The Eleventh International Con-
ference on Learning Representations .
Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Al-
fred Sliwa, Ahmet Aker, and Norbert Fuhr. 2021.
Starting conversations with search engines - inter-
faces that elicit natural language queries. In Proceed-
ings of the 2021 Conference on Human Information
Interaction and Retrieval , CHIIR ’21, page 261–265,
New York, NY , USA. Association for Computing
Machinery.
Martin J Pickering and Simon Garrod. 2013. An inte-
grated theory of language production and comprehen-
sion. Behavioral and brain sciences , 36(4):329–347.
Joan Plepi, B ´ela Neuendorf, Lucie Flek, and Charles
Welch. 2022. Unifying data perspectivism and per-
sonalization: An application to social norms. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing , pages 7391–
7402, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Ritika Poddar, Rashmi Sinha, Mor Naaman, and Mau-
rice Jakesch. 2023. Ai writing assistants influence
topic choice in self-presentation. In Extended Ab-
stracts of the 2023 CHI Conference on Human Fac-
tors in Computing Systems , CHI EA ’23, New York,
NY , USA. Association for Computing Machinery.
Dongqi Pu and Vera Demberg. 2023. ChatGPT vs
human-authored text: Insights into controllable text
summarization and sentence style transfer. In Pro-
ceedings of the 61st Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 4:
Student Research Workshop) , pages 1–18, Toronto,
Canada. Association for Computational Linguistics.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2655–2671, Seattle, United States.
Association for Computational Linguistics.Devendra Sachan, Mike Lewis, Mandar Joshi, Armen
Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke
Zettlemoyer. 2022. Improving passage retrieval with
zero-shot question generation. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 3781–3797, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Alireza Salemi, Surya Kallumadi, and Hamed Zamani.
2024. Optimization methods for personalizing large
language models through retrieval augmentation. In
Proceedings of the 47th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval , SIGIR ’24, page 752–762, New
York, NY , USA. Association for Computing Machin-
ery.
Alireza Salemi, Sheshera Mysore, Michael Bendersky,
and Hamed Zamani. 2023. Lamp: When large lan-
guage models meet personalization.
Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo,
Jonathan Bragg, Jeff Hammerbacher, Doug Downey,
Joseph Chee Chang, and David Sontag. 2023. Be-
yond summarization: Designing ai support for real-
world expository writing tasks. Second Workshop on
Intelligent and Interactive Writing Assistants, CHI
2023 .
Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia
Tsvetkov, Luke Zettlemoyer, and Scott Wen tau
Yih. 2023. Trusting your evidence: Hallucinate
less with context-aware decoding. arXiv preprint
arXiv:2305.14739 .
Kumar Shridhar, Harsh Jhamtani, Hao Fang, Ben-
jamin Van Durme, Jason Eisner, and Patrick Xia.
2023. Screws: A modular framework for reasoning
with revisions. arXiv preprint arXiv:2309.13075 .
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2020. Mpnet: Masked and permuted pre-
training for language understanding. In Advances in
Neural Information Processing Systems , volume 33.
Simeng Sun, Wenlong Zhao, Varun Manjunatha, Ra-
jiv Jain, Vlad Morariu, Franck Dernoncourt, Bal-
aji Vasan Srinivasan, and Mohit Iyyer. 2021. IGA:
An intent-guided authoring assistant. In Proceedings
of the 2021 Conference on Empirical Methods in Nat-
ural Language Processing , pages 5972–5985, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Nandan Thakur, Nils Reimers, Andreas R ¨uckl´e, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. BEIR:
A heterogeneous benchmark for zero-shot evaluation
of information retrieval models. In Thirty-fifth Con-
ference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2) .
Stojan Trajanovski, Chad Atalla, Kunho Kim, Vipul
Agarwal, Milad Shokouhi, and Chris Quirk. 2021.
When does text prediction benefit from additional

--- PAGE 13 ---
context? an exploration of contextual signals for chat
and email messages. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies: Industry Papers , pages 1–9,
Online. Association for Computational Linguistics.
Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng
Yang, Andrew Cohen, Lei Li, and Yuandong Tian.
2023a. Learning personalized story evaluation.
arXiv preprint arXiv:2310.03304 .
Liang Wang, Nan Yang, and Furu Wei. 2023b. Learning
to retrieve in-context examples for large language
models. arXiv preprint arXiv:2307.07164 .
Sitong Wang, Lydia B Chilton, and Jeffrey V Nickerson.
2023c. Writing with generative ai: Multi-modal and
multi-dimensional tools for journalists. Second Work-
shop on Intelligent and Interactive Writing Assistants,
CHI 2023 .
Yaqing Wang, Jiepu Jiang, Mingyang Zhang, Cheng
Li, Yi Liang, Qiaozhu Mei, and Michael Bender-
sky. 2023d. Automated evaluation of personalized
text generation using large language models. arXiv
preprint arXiv:2310.11593 .
Tongshuang Wu, Michael Terry, and Carrie Jun Cai.
2022. Ai chains: Transparent and controllable
human-ai interaction by chaining large language
model prompts. In Proceedings of the 2022 CHI
Conference on Human Factors in Computing Sys-
tems, CHI ’22, New York, NY , USA. Association for
Computing Machinery.
Nishant Yadav, Nicholas Monath, Rico Angell, Manzil
Zaheer, and Andrew McCallum. 2022. Efficient near-
est neighbor search for cross-encoder models using
matrix factorization. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 2171–2194, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Le Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky,
and Marc Najork. 2022. Scale calibration of deep
ranking models. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining , KDD ’22, page 4300–4309, New York,
NY , USA. Association for Computing Machinery.
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan
Berant. 2023. Making retrieval-augmented language
models robust to irrelevant context.
Hamed Zamani and Michael Bendersky. 2023. Mul-
tivariate representation learning for information re-
trieval. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in
Information Retrieval , SIGIR ’23, page 163–173,
New York, NY , USA. Association for Computing
Machinery.Prompt 1 fLLMprompt used to generate a target
text given historical examples retrieved by fretrand
a target request for AITA.
 
For a POST from the subreddit Am I The
Asshole write a COMMENT explaining if the
author of a post is an asshole or not
the asshole as a COMMENTER.
Use the following instructions for your
response :
1. Read the below example comments by
the COMMENTER.
2. Write the comment as the COMMENTER
mimicing the length , style , reasoning ,
and stances of their comments .
Here are some example comments by the
COMMENTER: {{historical examples }}
POST: {{target request }}
Write the COMMENT mimicing the length ,
style , reasoning , and stances of the
COMMENTERS comments . 
Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,
and Dawei Song. 2023a. A survey of controllable
text generation using transformer-based pre-trained
language models. ACM Comput. Surv. , 56(3).
Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah,
and Chi Wang. 2023b. Ecoassistant: Using llm assis-
tant more affordably and accurately. arXiv preprint
arXiv:2310.03046 .
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you
have pets too? In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2204–2213,
Melbourne, Australia. Association for Computational
Linguistics.
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with bert. In International
Conference on Learning Representations .
Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-
tive example selection for in-context learning. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing , pages 9134–
9148, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
A Model Details
Retriever We instantiate fretr with the
pre-trained MP NET, which is relatively
lightweight at 110M parameters (Song et al.,
2020). We obtain an output score from fretr
aswTtanh 
WTENC([qu, du])
, where ENC
represents the CLStoken from the final layer of the
encoder, and quandduare the text of the input

--- PAGE 14 ---
Prompt 2 fLLMprompt used to generate a target
text given historical examples retrieved by fretrand
a target request for W ORKSM.
 
Given a REQUEST from a USER to author a
POST, write a POST for an enterprise
social media site mimicking the user to
satisfy the REQUEST.
Use the following instructions for your
response :
1. You should maintain consistency in
tone and style with the USER 's historical
posts .
2. You should imitate the language style
of the USER 's historical posts .
3. You should employ similar rhetorical
methods as the USER 's historical posts .
Here are some historical posts by the
USER: {{historical examples }}
REQUEST: {{target request }}
Write the POST to satisfy the REQUEST
mimicing the tone , style , and rhetorical
methods of the USER 's historical posts . 
request and historical document. The encoder
parameters, w, and Ware trained.
Text generation models ForfLLM we con-
sider two performant LLMs offered via API by
Azure OpenAI, davinci-003 andgpt-3.5-turbo .
Forfauxwe consider a smaller but still effective
encoder-decoder language model, FLANT5-XL ,
with 3 billion parameters (Chung et al., 2022). The
latter model is open-sourced, allowing us to ac-
cess its token likelihoods directly, a requirement of
Eq. 1. We obtain target text likelihoods by taking
the average of log-probabilities of individual token
likelihoods from F LANT5-XL.
LLM prompts We use Prompts 2 and 1 for
LLM inference. The same prompts are used with
davinci-003 andgpt-35-turbo . For construct-
ing training data in Eq 1 with a FLANT5-XL ,faux
we use Prompts 5, 6, 3, 4. Note that computing
paux(tu|qu)uses a set of randomly chosen few shot
examples from the training set fixed across requests
rather than the request alone.
PEARL implementation In constructing train-
ing data for fretrwe use |Dt
u|= 8, i.e we treat the
8 most recent texts per user as their target texts.
To train fretr, we consider the top two canadidate
documents per Eq. (1)as positive examples per
request and use three negatives per positive, i.e.,
P= 2andN= 3. In our LLM prompts, we use
k= 3retrieved examples for WORKSMandk= 4
forAITA , tuned on a dev set, and set generation
temperature to zero.Prompt 3 fauxprompt used to compute paux(tu|qu)
in Eq (1) for AITA.
 
Here are some example posts on the Am I
The Asshole subreddit :
{{random fewshot examples }}. Target post :
{{target post}}. Write a users comment for
this post : 
Prompt 4 faux prompt used to compute
paux(tu|du, qu)in Eq (1) for AITA.
 
Here is an comment on a post by a user
on the Am I the Asshole subreddit :
{{candidate comment }}. Target post :
{{target post}}. Write a users comment for
this post : 
We also use temperatures for target scores input
to softmax functions in Eq. (2),y′
u/τwithτ=
5. Finally, we set y0= 110 forWORKSMand
y0= 5forAITA , which are the median values of
Eq.(1)for each respective dataset on the training
data. We tuned y0on a dev set constructed similar
to our training set to 25 and 75 percentile values
of Eq. (1). Our retrievers were trained on Nvidia
V100 GPUs with 16GB memory or Nvidia RTX
A6000 GPUs with 48GB memory. Experiments for
training retrievers required about 300 hours in total.
B Experimental Details
Here we present various details of datasets, base-
lines, and manual evaluation.
B.1 Evaluation Requests in W ORK SM
For evaluation in WORKSMtwo authors not in-
volved in model development manually authored
requests for each of the 163 target posts in our
evaluation set. Guidelines presented to annotators
for the requests are presented in Guideline 1. The
requests are intended to be brief and include the
salient information contained in the post. Note that
annotators external to the authors weren’t recruited
for authoring requests due to the private and highly
regulated nature of W ORKSM.
B.2 Training Requests in W ORK SM
Section 5.1 notes that our training set for WORKSM
was constructed from synthetic requests generated
by GPT4. The prompt for this is presented in
Prompt 11. We follow an incremental approach

--- PAGE 15 ---
Prompt 5 fauxprompt used to compute paux(tu|qu)
in Eq (1) for W ORKSM.
 
Here is are some posts by a user on an
enterprise social network :
{{random fewshot examples }}
Here is an outline for a target post by
the user : {{target request }}. Write the
target post : 
Prompt 6 faux prompt used to compute
paux(tu|du, qu)in Eq (1) for W ORKSM.
 
Here is an example post by a user on an
enterprise social network :
{{candidate document }}. Here is an outline
for a target post by the user :
{{target request }}. Write the target post : 
to construct the synthetic requests: first extracting
the salient aspects of the post, followed by con-
catenation of these aspects to result in the request.
The salient aspects span: an overview of the post,
proper nouns mentioned in the post, contact in-
formation, links to webpages, and any specialized
knowledge or anecdotes in the post. Given the suc-
cess of chain-of-thought prompting, we generate
an explanation followed by salient aspects of the
post – the explanations are not used elsewhere. En-
terprise contracts ensure the privacy of user data
shared over the API.
B.3 Baselines
We consider the following non-personalized base-
lines: ZSHOT-NP: This represents a non-
personalized approach prompting only with the re-
quest. KSHOT-NP: A zero-shot non-personalized
approach using a fixed randomly selected set of k
documents for all requests. For AITA , the exam-
ples are balanced across labels.
We consider the following retrieval-augmented
personalized baselines, selecting from a user’s his-
torical documents Du:Random: Random selection
ofkdocuments from Du.BM25: Represents a
classic performant retrieval model based on query-
document term overlap. MPNET-1B: This a strong
MPN ETbi-encoder trained on 1 billion text pairs
from numerous domains.1Documents are ranked
for a request using cosine similarity between em-
beddings. QL-FT5: An approach which ranks
documents based on p(qu|du)with a pretrained
1HF model: sentence-transformers/all-mpnet-base-v2Prompt 7 Judge LLM prompt used to select a gen-
erated post more likely to align with a reference
post authored by a user for W ORKSM.
 
You an an experienced linguist who helps
people compare social media texts .
Given a REFERENCE POST and two
TARGET POSTS judge which of the TARGET
POSTs is significantly more likely to be
written by the same author as the
REFERENCE POST.
For your response use the following
instructions :
1. Make your judgement based on
stylistic patterns , ordering of
information , and tone used .
2. Output POST ONE if it is significantly
more likely to be written by the same
author as the REFERENCE POST.
3. Output POST TWO if it is significantly
more likely to be written by the same
author as the REFERENCE POST.
4. Output BOTH if either post could have
been written by the same author or
neither could have been written by the
same author .
Here are the POSTS :
REFERENCE POST: {{reference post}}
POST ONE: {{post one}}
POST TWO: {{post two}}
Output a justification for your
judgement , then output POST ONE , POST
TWO , or BOTH to indicate your final
decision . 
FLANT5-B ASE with 250M parameters (Sachan
et al., 2022). This may be seen as an unsupervised
crossencoder. RelevanceCE: A supervised crossen-
coder with the same architecture as fretrin P EARL
but differing in training. This is trained on pairs of
(qu, du)inDutreated as positive training pairs with
a crossentropy loss, with negatives selected as a ran-
dom historical document from the same user not
but corresponding to qu. Note that this corresponds
to a crossencoder optimized for request-document
relevance, i.e. p(relevance = 1|qu, du), rather than
personalized target text generation.
B.4 Judge LLM prompts
In Prompt 8 and 7 we present prompts for GPT-4o
as a judge LLM discussed in §5.2.
C Additional Results
Here we present additional results in addition to
those presented in §5.2. We present these here
primarily in the interest of space.

--- PAGE 16 ---
Figure 4: A qualitative example illustrating the effectiveness of PEARL on AITA : Given a request post qudescribing
an ambiguous interpersonal situation regarding sharing medical information, PEARL retrieves a historical user
comment duthat demonstrates the user’s characteristic tone and values, and generates a comment tuhighly similar
to the ground-truth user comment t∗
u. We bold qualitatively similar phrases about individual liberties and italicize
phrases about self-care and mental health. All texts are abbreviated for space, and provided in full in Appendix C.
C.1 Qualitative example from AITA
Here we provide the full texts of the qualitative
AITA examples from Figure 4, alongside the top-
retrieved comment from BM25 to show the quali-
tative difference in retriever effectiveness.
Original post qu: going for surgery without
telling my parents. So , I guess I ’ll start off with
background information .I am 21 , and trans mas-
culine , or female to male if that term works for
you
Gold user-authored comment t∗
u: “NTA, hon-
estly you can do whatever you choose feels best to
you. Your parents have not been supportive of your
transition so they have elected not to be a part of
the process. You can tell them any amount of detail
you like whether before, after, or at different times.
It’s good you have helpful friends. Good luck!”
TopPEARL -retrieved comment du: “NTA, you
don’t owe your abusive ex anything. That being
said, in your position I would consider getting a
friend to tell him just so you yourself can get clo-
sure and not be wondering when he’s going to ha-
rass you about it.”
Top BM25-retrieved comment du: “YTA, just
another lame guy who is only interested in a girl if
he has a chance to sleep with her”
PEARL LLM output : “NTA. You are an adult
and have the right to make your own medical de-
cisions. It’s understandable that you don’t have a
great relationship with your parents and don’t feel
comfortable sharing this information with them.
It’s also important to note that being transgender is
not self-harm and it’s great that you are taking care
of your mental health by seeing a counselor and
nurse regularly. Wishing you a speedy recovery!”Dataset → AITA W ORKSM
LLM→ gpt-35-turbo gpt-35-turbo
Method ↓ Macro F1(%) BS-F1 R1 R2
PEARL 65.34 36.49 0.5082 0.2676
no calibrated sm 63.01 36.69 0.5095 0.2654
distill paux(tu|du, qu) 62.32 35.64 0.5057 0.2652
Table 4: PEARL compared to alternate training of fretr
forgpt-35-turbo .
C.2 Ablating Training Strategies
In Table 4 and 5, we compare common alterna-
tive training of fretrwhile keeping inference steps
identical for gpt-35-turbo anddavinci-003 re-
spectively. These serve to ablate our specific con-
tributions: selection of training requests which ben-
efit from personalization and our scale calibrating
objective.
No calibrated objective Removing the scale
calibration and using a standard KL divergence (–
calibrated sm) degrades performance for AITA and
sees comparable performance in WORKSMwith
gpt-35-turbo in Table 4. With davinci-003 we
see scale calibration consistently improves perfor-
mance (Table 5). This indicates the importance
of calibration for estimating the benefit of a his-
torical document to a request consistently across
datasets and LLMs. Appendix C.3 shows scale cal-
ibration also consistently improves the correlation
of retriever scores with task performance.
Distill paux(tu|du, qu)tofretr.The proposed
fretris trained on documents which benefit person-
alization andrequests which benefit from person-
alization. Here, we compare to an approach that
only selects documents that benefit personaliza-
tion by maximizing paux(tu|qu, du). This assumes
thatalltraining requests benefit from personaliza-

--- PAGE 17 ---
Prompt 8 Judge LLM prompt used to select a gen-
erated comment more likely to align with a refer-
ence comment authored by a user for AITA.
 
You an an experienced linguist who helps
people compare social media texts .
Given a REFERENCE POST and two
TARGET POSTS judge which of the TARGET
POSTs is significantly more likely to be
written by the same author as the
REFERENCE POST.
For your response use the following
instructions :
1. Make your judgement based on
similarity of stylistic patterns ,
arguments , stances , and word choices .
2. Output POST ONE if it is significantly
more likely to be written by the same
author as the REFERENCE POST.
3. Output POST TWO if it is significantly
more likely to be written by the same
author as the REFERENCE POST.
4. Output BOTH if either post could have
been written by the same author or
neither could have been written by the
same author .
Here are the POSTS :
REFERENCE POST: {{reference post}}
POST ONE: {{post one}}
POST TWO: {{post two}}
Output a justification for your
judgement , then output POST ONE , POST
TWO , or BOTH to indicate your final
decision . 
tion. We train fretrwith a KL-divergence objec-
tive. This approach, also, closely resembles prior
work example selection in non-personalized tasks
(Rubin et al., 2022) as well as personalized tasks
(Salemi et al., 2024). We see in Table 4 and 5 (dis-
tillpaux(tu|du, qu)) that this lowers performance
markedly, indicating the value of our approach.
C.3 Calibration Evaluation
Since we aim to train generation calibrated retriev-
ers, we evaluate calibration performance i.e a re-
trieval models scores to be predictive of down-
stream generation performance (Table 6). Here,
Dataset → AITA W ORKSM
LLM→ davinci-003 davinci-003
Method ↓Macro F1(%) BS-F1 R1 R2
PEARL 61.21 39.60 0.5419 0.3094
no calibrated sm 57.27 38.88 0.5350 0.3033
distill paux(tu|du, qu) 55.52 39.34 0.5359 0.3059
Table 5: PEARL compared to alternate training of fretr
fordavinci-003 .Method ↓/ LLM → davinci-003 gpt-35-turbo
Pearson r Pearson r
AITABM25 0.08 -0.05
MPN ET-1B 0.07 -0.14
UPR -0.48 -0.02
RelevanceCE 0.07 -0.19
PEARL fretr 0.11 0.45
– calibrated sm -0.48 0.12
WORKSMBM25 0.42 0.52
MPN ET-1B 0.54 0.52
UPR -0.05 -0.02
RelevanceCE 0.56 0.49
PEARL fretr 0.64 0.64
– calibrated sm 0.58 0.55
Table 6: Calibration performance of PEARL evaluated
through correlation between score for top-1 document
and Macro-F1 for AITA, and R2for W ORKSM.
Pearson ris reported between the top-1 document
score for a request and the downstream generation
evaluation metric – R2forWORKSM, and Macro-
F1 for AITA . To do this for AITA , we first bin
evaluation requests into equal sized bins by top-1
document score, s1, and then measure Pearson r
between the bin start and the average evaluation
metric per bin. Our metric is in contrast with prior
work (Dhuliawala et al., 2022; Yan et al., 2022)
that focuses on classification tasks, where model-
predicted class probabilities can be used for mea-
suring calibration, missing in our setup.
Among baseline methods, we see sparse and
dense retrieval methods, BM25 and MPN ET-1B
scores to be better calibrated with downstream per-
formance compared to likelihood-based methods
like QL-FT5. Next, we see PEARL to be bet-
ter correlated with downstream performance for
WORKSMandAITA - indicating the effectiveness
of our training strategy. Further, we also report
on an ablated model, not using the scale-calibrated
objective of Eq (3)(– calibrated sm). We see this
approach underperform P EARL , indicating the im-
portance of the scale-calibrated objective for a well-
calibrated crossencoder. The poorer calibration
of crossencoders also finds support in prior work
showing their scores to lie at extremes of the score
distribution (Menon et al., 2022; Yadav et al., 2022).
C.4 Selective Revision with P EARL –
Extended Results
In§5.3 we demonstrate how our trained retrieval
model can be used for selective revision with
gpt-35-turbo . Prompt 9, 10 present the prompts

--- PAGE 18 ---
Prompt 9 fLLMprompt used to for selective revi-
sion given a Stage 1 draft for AITA.
 
Given a POST from the subreddit Am I the
Asshole and a DRAFT comment from the USER
responding if the author of the POST is
an asshole or not the asshole , edit the
DRAFT comment .
Use the following instructions for your
response :
1. Maintain consistency in tone and
style with the USER 's historical
comments .
2. Edit the DRAFT to use more reddit
lingo .
3. Remove statements of the POST from the
DRAFT.
4. Output a justification for your edits
starting with the word JUSTIFICATION .
5. Output the edited DRAFT comment
starting with the words EDITED DRAFT .
Here are some historical comments by the
USER : {{historical examples }}
REQUEST: {{target request }}
DRAFT: {{target draft}}
Output a justification for your edits ,
then output the edited DRAFT starting
with the words EDITED DRAFT . 
used for revision with both LLMs.
In Figure 5, we examine the impact of selec-
tive revision in WORKSMfor requests of different
lengths and users with different number of histor-
ical posts. We see that revision benefits requests
of medium length and users with few posts. From
Figure 5a, we hypothesize that requests that are too
short may require additional user input and see no
gains from revision. On the other hand requests
that are too long, may be more challenging to fol-
low and are unlikely to improve from revisions.
From Figure 5b, we see that users with few posts
benefit from revision indicating that these users
see poorer retrievals. On the other hand users with
larger profiles see a drop in performance indicat-
ing that even better calibration performance may
improve performance of selective revision further.
Note that we don’t report results with
davinci-003 since our procedure for learning a
threshold θfor selective revision failed to find a
threshold where dev set performance was improved
from selective revision. Finally note that metrics re-
ported for selective revision in Table 3 isn’t directly
comparable to those of Tables 1, 4, and 5 since they
represent different LLM runs and exclude a dev set
from WORKSMandAITA for learning θ(50 and
200 instances respectively).Prompt 10 fLLMprompt used to for selective revi-
sion given a Stage 1 draft for W ORKSM.
 
Given a REQUEST and a DRAFT from a USER to
author a social media POST, edit the
DRAFT to satisfy the REQUEST.
Use the following instructions for your
response :
1. Enumerate any missing missing
information from the REQUEST in the DRAFT.
2. Enumerate any irrelevant information
for the REQUEST in the DRAFT.
3. Then output the edited DRAFT starting
with the words EDITED DRAFT .
REQUEST: {{target request }}
DRAFT: {{target draft}}
Output missing or irrelevant information
for the REQUEST, then output the EDITED
DRAFT satisfying the REQUEST. 
D Extended Related Work
Having discussed the closest body of related work
in §2 we discuss additional related work here.
Dynamic prompts for LLMs Besides train-
ing retrievers for in-context example selection dis-
cussed in §2, other approaches have explored better
use of pre-trained models for example selection.
Creswell et al. (2023) select examples based on
the target LLM likelihood - necessitating access
to LLM likelihoods and incurring latency in re-
trieval. Gupta et al. (2023) explore selecting setsof
examples with dense retrieval models, presenting
a complementary approach to ours. Finally, Pan
et al. (2023) use retrieval models to select exam-
ples from multiple knowledge sources and train a
routing model to decide the source of knowledge
to retrieve from – selective revision ( §5.3) based
on a retriever may be seen as a form of routing.
Prompt robustness in LLMs Simultaneous
routing and retrieval also relates our approach to
work ensuring that LLMs are robust to noisy re-
trievals. Prior approaches ensured robustness by
only using retrieved documents based on simple
frequency filters on entities mentioned in the input
query (Mallen et al., 2023) or based on predictions
from an NLI model that determines if the query
entails the retrieved contexts (Yoran et al., 2023).
Other approaches have sought to fine-tune the LLM
to be robust to irrelevant contexts (Li et al., 2023c;
Luo et al., 2023; Yoran et al., 2023) or modify the
decoding procedure (Shi et al., 2023). In contrast,
we determine the quality of the input context based
on scale-calibrated retrieval model scores.
LLM chaining In selectively editing generations

--- PAGE 19 ---
(a) Effectiveness of selective revision for requests of differ-
ent lengths (in words).
(b) Effectiveness of selective revision for users of different
numbers of historical posts.
Figure 5: The impact of selective revision ( §5.3) in PEARL onWORKSMcompared for requests of different length
and users with varying number of historical posts.
with an LLM for low-performing requests, our ap-
proach also relates to recent work on composing
LLMs with other models to build more complex
systems (Wu et al., 2022; Arora et al., 2023; Khat-
tab et al., 2023). Close work is presented by ap-
proaches that leverage repeated LLM calls to verify
the reasoning or factuality of previous generations
(Shridhar et al., 2023; Dhuliawala et al., 2023). In
contrast, our work leverages an efficient retrieval
model to selectively direct low-performing genera-
tions for further revision, reducing the total num-
ber of expensive LLM calls necessary. In this re-
spect, our approach bears similarity to Zhang et al.
(2023b), who progressively run larger LLMs only
when necessary for an input.
Calibrated retrievers A small body of work
has explored calibrated ranking models. Yan et al.
(2022) train scale-calibrated ranking models for
recommendation models used for advertisement
pricing systems. On the other hand, our work lever-
ages scale-calibration for personalized writing as-
sistance. Other work has explored joint training
of retrievers and generative models to obtain cal-
ibrated retrievers (Dhuliawala et al., 2022), using
Gaussian embeddings to estimate retriever uncer-
tainty (Zamani and Bendersky, 2023), or estimat-
ing retriever confidence with monte-carlo dropout
(Cohen et al., 2021). In contrast with probabilistic
uncertainty estimation, PEARL minimally modifies
training to result in a calibrated model and does
not require extensive changes to training, model
architecture, or additional inference costs.
Writing assistants A sizable body of work has
explored the development of writing assistants.Compared to assistants for communication appli-
cations, these have been targeted at authors of
creative texts like screenplays (Mirowski et al.,
2023), stories (Akoury et al., 2020), and poems
(Gon c ¸alo Oliveira, 2017) – consequently, they fo-
cus on diverse generations and long-range coher-
ence, rather than personalization. Further, while
our work leverages a request-driven assistant, prior
systems have used a variety of interaction and con-
trol methods. While text completion presents a
common form of interaction (Clark et al., 2018),
recent work has seen use of infilling, tag-based
control (Sun et al., 2021), and instruction guided
generations (Chakrabarty et al., 2022) – a deeper
examination of control and interaction strategies
and their trade offs are presented in related reviews
(Zhang et al., 2023a; Lin et al., 2023). While our ap-
proach to personalization may be extended to some
alternative interaction paradigms, other interaction
techniques are likely to necessitate additional work.
Personalized text generation While we have
focussed on author personalization that aims to
mimic stylistic patterns, interests, and values of
an author, we briefly review reader-personalized
text generation – a setup aiming to generate texts
that are engaging and relevant to readers’ pref-
erences. This has historically been explored for
generating personalized reviews (Ni et al., 2017),
recipes (Majumder et al., 2019), news headlines
(Ao et al., 2021) and in dialogue agents (Mazar ´e
et al., 2018; Zhang et al., 2018). Related work is
also found in text simplification and lay summariza-
tion in the context of scientific text – this work has
explored generating definitions for scientific con-

--- PAGE 20 ---
cepts at varying levels of complexity (August et al.,
2022; Murthy et al., 2022) or summarizing scien-
tific text for lay readers (Guo et al., 2021). While
recent work has explored this with modern LLMs
(Li et al., 2023d; Farajidizaji et al., 2023), reader
personalization remains an understudied problem
and presents a rich area for future work.
E Limitations
Here, we discuss limitations of our work derive
from our choice of fauxandfLLM, our evaluation
setup, and the design of our method.
Choice of LLMs Our experiments use two
closed LLMs through API access ( davinci-003 ,
gpt-35-turbo ). While we show the value of
PEARL with LLM’s of varying performance, es-
tablishing its effectiveness with other LLMs will
require further work. We also acknowledge that
closed LLMs limit experimental reproducibility -
however, given the widespread use of GPT models
(Hu, 2023) we believe our investigation is mean-
ingful. Finally, in constructing training data for
instance selection models for an LLM, prior work
has noted the best empirical performance from
matching fauxandfLLM(Rubin et al., 2022). While
we demonstrate benefits from using significantly
smaller models for faux, using an open LLM will
allow further validation of this result in the context
of our approach. However, using a larger (open)
model for fauxwill incur additional costs in cre-
ating training data, and smaller models for fLLM
are likely to see a worse generation performance -
exploring this tradeoff requires future work.
Evaluation setup Next, while WORKSMrepre-
sents an impactful and realistic use case for writing
assistants, we acknowledge that its private nature
limits reproducibility. Further, our evaluation set of
WORKSMandAITA represents a limited set of sce-
narios that are likely to leverage writing assistants.
While we believe our work represents a meaningful
first step, additional future work, and online evalua-
tions are necessary to establish the value of PEARL
across the myriad of scenarios where writing assis-
tants may be used. Finally, while we leverage sev-
eral evaluation strategies to demonstrate the value
ofPEARL , evaluating text generations under per-
sonalization setups represents is an under-explored
and a currently emerging body of work (Wang et al.,
2023a,d).
Method design Finally, we note that the current
design of PEARL is likely to have some drawbacks.It is possible that our proposed method for train-
ing instance selection biases system performance
toward some users or requests – we leave examina-
tion of this to future work. It is also possible that
formulating fretras an expressive crossencoder and
the use of large LLMs will present latency limita-
tions for interactive applications – exploration of
models supporting faster retrieval and text genera-
tion inference represent important future work.

--- PAGE 21 ---
Prompt 11 GPT4 prompt used to generate synthetic requests for W ORKSMposts in our training set.
 
## TASK
Given an enterprise social media post , generate a set of writing instructions that
explain how to
" reverse - engineer "; the post . Use the following steps :
- The instructions must give a high - level overview of what the post aims to
communicate . Example : [ readcted ]
- The instructions must include specific proper nouns ( people , places , organzations )
. Example : [ redacted ]
- The instructions must include contact information if available . Example : [ redacted
]
- The instructions must include specific links to websites or files if available .
Example : [ redacted ]
- The instructions must contain any knowledge that is highly specialized and is
likely to be only known to the individual who wrote the post , if available . Example :
[ redacted ]
- The instructions must contain rough sketches of any personal anecdotes in the post
, if available . Example : [ redacted ]
- The instruction must ** not ** contain any formatting or ordering information from
the post .
## OUTPUT
Output the following :
<Explanation >{ explanation of your reasoning for how you generated the instructions ,
in 3 sentences or fewer }</ Explanation >
< Instruction . Overview >{1 -2 sentences overview of what the post aims to communicate
}</ Instruction . Overview >
< Instruction .Names >{1 -2 sentences about the people , places , or organizations
mentioned in the post , _NONE_ if not applicable }</ Instruction .Names >
< Instruction . Contacts >{1 -2 sentences about the contact information copied verbatim
in the post , _NONE_ if not applicable }</ Instruction . Contacts >
< Instruction .Links >{1 -2 sentences including the links copied verbatim from the post ,
_NONE_ if not applicable }</ Instruction .Links >
< Instruction . Knowledge >{1 -2 sentences paraphrasing the specialized knowledge
included in the post , _NONE_ if not applicable }</ Instruction . Knowledge >
< Instruction . Anecdotes >{1 -2 sentences paraphrasing the anecdotes included in the
post , _NONE_ if not applicable }</ Instruction . Anecdotes >
## INPUT
{{input post}} 

--- PAGE 22 ---
Guideline 1 Instructions provided to annotators for authoring requests for our evaluation set in WORKSM. 
Overview :
In this study , we are developing LLM - based approaches for writing
social media posts on enterprise social networks . Your task is as
follows : Given a social media post from an enterprise social media
platform , write a short outline of the post . In writing your outline ,
imagine you are a manager , social media manager , or event organizer
writing a rough sketch of the post with the key information you would
like to share .
Data Format :
You are given a spreadsheet consisting of ˜150 English posts . Each
row corresponds to a single post . The spreadsheet contains the
following columns : PostId , InputPost , OutputShortOutline . The first
column is the ID of the post ; you can ignore this column . The second
column is the full text of the input post . In the third column , you
will write your short outline based on the input post .
DO’s for your outline :
When writing your short outline , do include the following :
- One sentence about the goal of the post : Include a brief
description of what the post is trying to communicate . Example : [
redacted ]
- Specific proper nouns ( people , places , things ): Include names of
specific people , places , or things in your outline . Example : [
redacted ]
- Specialized knowledge : If the knowledge contained in the post is
highly specialized and is likely to be only known to the individual
writing the post , include a rough sketch of that information in your
outline . Example : [ redacted ]
- Personal anecdotes : If the post contains specific personal
anecdotes , include a rough sketch of that information in your outline
. Example : [ redacted ]
- Special emphasis or call to action : If the post makes a special
emphasis , include a rough sketch of that emphasis or call to action
in your outline . Example : [ redacted ]
- External website links : If the post links to an external website ,
include the link in your outline . Example : [ redacted ]
DONT’s for your outline :
When writing your short outline , donot include the following :
- Anything related to the ordering of content .
- Formatting instructions .
- Any verbatim text other than specific proper nouns .
 
