# 2401.17268.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/writing/2401.17268.pdf
# File size: 1373871 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Weaver : Foundation Models for Creative Writing
Tiannan Wang Jiamin Chen Qingrui Jia Shuai Wang Ruoyu Fang Huilin Wang
Zhaowei Gao Chunzhao Xie Chuou Xu Jihong Dai Yibin Liu Jialong Wu Shengwei Ding
Long Li Zhiwei Huang Xinle Deng Teng Yu Gangan Ma Han Xiao Zixin Chen
Danjun Xiang Yunxia Wang Yuanyuan Zhu Yi Xiao Jing Wang Yiru Wang Siran Ding
Jiayang Huang Jiayi Xu Yilihamu Tayier Zhenyu Hu Yuan Gao Chengfeng Zheng
Yueshu Ye Yihang Li Lei Wan Xinyue Jiang Yujie Wang Siyu Cheng Zhule Song
Xiangru Tang Xiaohua Xu Ningyu Zhang Huajun Chen
Yuchen Eleanor Jiang*Wangchunshu Zhou*
AIWaves Inc.
Abstract
This work introduces Weaver , our first family of large language models (LLMs) dedicated to
content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving
the writing capabilities of large language models. We then fine-tune Weaver for creative and
professional writing purposes and align it to the preference of professional writers using a suit of
novel methods for instruction data synthesis and LLM alignment, making it able to produce more
human-like texts and follow more diverse instructions for content creation. The Weaver family
consists of models of Mini(1.8B), Base(6B), Pro(14B), and Ultra(34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent according to query
complexity to balance response quality and computation cost. Evaluation on a carefully curated
benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform
generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model
surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the
advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports
retrieval-augmented generation (RAG) and function calling (tool usage). We present various use
cases of these abilities on improving AI-assisted writing systems, including integration of external
knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we dis-
cussandsummarizeaguidelineandbestpracticesforpre-trainingandfine-tuningdomain-specificLLMs.
Weaver is currently accessible at www.wawawriter.com , our innovative human-AI collaborative writing
platform (For the English version of WawaWriter , see www.wawawriter.com/en ). We discuss a few
innovations of the platform from the perspective of human-computer interaction to explain how it will
revolutionize traditional AI-assisted writing systems.
*Corresponding authors: {eleanor,chunshu}@aiwaves.cnarXiv:2401.17268v1  [cs.CL]  30 Jan 2024

--- PAGE 2 ---
Contents
1 Introduction 4
2 Pre-training 6
2.1 Model Family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Pre-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.3 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Data Synthesis 8
3.1 Abilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.1.1 Instruction Following . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.1.2 Instruction Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.1.3 Evaluation (Literary Critic) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.4 Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.5 Function Calling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 Instruction Backtranslation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.3 Constitutional DPO: Learning From Principled Negative Examples . . . . . . . . . . . 12
4 Alignment 14
4.1 Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.1.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.1.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2 Preference Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5 Evaluation 14
5.1 WriteBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.2 Compared Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5.3 LLM-based Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5.4 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5.5 User Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6 Introducing WawaWriter 17
6.1 Human-AI Collaborative Writing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.2 Integration of External Knowledge and Tools . . . . . . . . . . . . . . . . . . . . . . . 17
6.3 Personalized Writing Assistance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2

--- PAGE 3 ---
6.4 Infinite Long Text Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
7 Discussion 18
A Appendix 24
A.1 Author Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.2 Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A.3 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3

--- PAGE 4 ---
1. Introduction
Large language models (LLMs) (Anthropic, 2023; Brown et al., 2020; Google, 2023; Jiang et al.,
2023a; OpenAI, 2022, 2023; Radford et al., 2018, 2019; Gemini Team , 2023; Touvron et al., 2023a,b;
Yin et al., 2023; Zhao et al., 2023) based on Transformers (Vaswani et al., 2017) have become a
prominent pathway to Artificial General Intelligence (AGI). LLMs acquire massive world knowledge
by learning to predict the next word on large-scale web corpora. The capabilities of LLMs have been
continuously increasing by scaling model sizes, dataset sizes, and computation. After pre-training,
LLMs can be aligned to support real-world use cases by supervised fine-tuning (Chung et al., 2022;
Sanh et al., 2022) and preference optimization techniques including reinforcement learning from
human feedback (RLHF) (Ouyang et al., 2022a; Wang et al., 2024; Zheng et al., 2023b) and direct
preference optimization (DPO) (Rafailov et al., 2023). The capabilities of LLMs have empowered
various applications including ChatGPT, Claude, Bard, Microsoft Copilot, Character.AI, Notion AI, etc.
Recently, many specialized LLMs have been trained for different targeted usage scenarios. In general,
LLMs specialize according to the targeted domains (e.g., finance (Wu et al., 2023), healthcare (Yang
et al., 2022b), legal (Cui et al., 2023), etc.) and tasks (e.g., role-playing (Wang et al., 2023d),
coding (Rozi√®re et al., 2023), etc.). However, the ability of LLMs to write human-like texts and
produce creative content, which is a critical use case of LLM applications such as ChatGPT, is mostly
overlooked by the community.
In this report, we focus on the literature domain and the task of writing, orcontent creation ,
and introduce Weaver , a family of LLMs dedicatedly pre-trained and aligned for this purpose. The
name "Weaver" symbolizes the model‚Äôs proficiency in skillfully amalgamating linguistic elements,
akin to the way a craftsman weaves threads to form a fabric. We answer four main questions in this
technical report: why we need Weaver ,how we train Weaver ,howWeaver performs , and what
we build with Weaver .
34.0 UnknownUnknown 14.0 34.0 Unknown 72.0 6.0 14.0 1.8 6.0 1.8
Number of Model Parameters (Billions)7.27.47.67.88.08.28.48.6WriteBench Overall ScoreWeaver Ultra
GLM4
GPT-4
Weaver Pro
Yi-34B
Claude2
Qwen-72BWeaver BaseQwen-14BWeaver Mini
Yi-6B
Qwen-1.8B
Figure 1|Comparison between Weaver and
generalist LLMs on WriteBench .Why we need Weaver ?Despite generalist LLMs
such as GPTs already possessing general writ-
ing skills and helping billions of users in vari-
ous writing scenarios, they often struggle to pro-
duce human-like texts in specific writing scenar-
ios such as writing stories, fiction, social media
copies, blogs, papers/thesis, etc. We analyze
the behavior of pre-trained base LLMs such as
LLaMA and aligned LLMs such as ChatGPT and
LLaMA-chat and believe this limitation originates
from both the pre-training stage and the align-
ment stage. On one hand, generalist LLMs are
pre-trained on massive low-quality web texts or
machine/AI-generated texts. Consequently, ex-
isting LLM backbones tend to produce seemingly
fluent texts that are not creative enough and lack
human-like styles. On the other hand, during
the alignment stage, state-of-the-art LLMs such
as GPT-4 are instruction-tuned using instruction-response pairs annotated by crowdsource annotators
(Ji et al., 2023; Shen et al., 2023; Wang et al., 2023c). However, most of the annotators are not
professional writers or content creators and the annotation guidelines only require them to produce
helpful and harmless responses (Ouyang et al., 2022b). As a result, the crowdsourced data for
supervised fine-tuning is less stylish and lacks creativity. Furthermore, most popular preference
4

--- PAGE 5 ---
optimization methods such as RLHF and DPO optimize the model on model-generated data pairs,
making them less suitable for enhancing the creativity of LLMs.
These factors make current generalist LLMs lack creativity and unable to produce human-style
texts despite they are super powerful in other applications such as writing codes and answering
general questions. We believe this phenomenon will continue to be amplified given that the amount
of LLM-generated texts on the internet is exponentially growing and most LLMs are aligned using
texts produced by other LLMs. Therefore, we believe it is necessary to train domain-specific LLMs
dedicated to writing purposes that are creative and generate human-like texts in order to fully exploit
the potential of AI-generated content (AIGC).
How we train Weaver ?To address the aforementioned issues limiting generalist LLMs‚Äô creative
writing ability, we carefully design a suite of strategies for automated data collection, data annotation,
anddatafilteringforpre-trainingandalignment. Thismakesusabletopre-trainandalign Weaver on
diverse, human-like, and stylish texts. To be specific, we conduct extensive pre-training data filtering
and only keep high-quality content such as books, fiction, stories, and articles in the pre-training
corpus, making the pre-trained backbones more likely to produce human-like texts.
As for the alignment stage, we propose a new instruction backtranslation framework inspired by
LongForm (K√∂ksal et al., 2023) and Humpback (Li et al., 2023) that synthesize diverse and natural
instructions that correspond to high-quality outputs written by professional writers and preferred by
human consumers. Our instruction backtranslation framework translated the work of crowdsource
annotators from writing both instructions and outputs to simply collecting high-quality content such
as stories, fiction, articles, social media copies, and blog posts. This massively reduces the cost of
instruction data annotation and the requirement for crowdsource annotators while significantly
improving the quality of annotated data.
Moreover, we also propose a novel Constitutional DPO algorithm for preference optimization to
better align Weaver to the preference of professional writers and content creators. Constitutional
DPO is inspired by and combines the advantages of a few previous works including DPO (Rafailov
et al., 2023), Constitutional AI (Bai et al., 2022), Self-Align (Sun et al., 2023), and RLCD (Yang
et al., 2023a). Specifically, Constitutional DPO exploits expert (e.g., professional editors in our case)
annotated principles to synthesize negative examples that violate certain principles based on positive
examples that are sampled from the optimal policy (e.g., texts produced by professional writers or
content creators in our case). In contrast to the common practice of using DPO that uses LLMs to
produce preference annotation on two model-generated responses such as Zephyr (Tunstall et al.,
2023), the pairwise preference data synthesized by our approach contains less noise since the negative
example are deliberately synthesized to be of lower quality compared to the positive example. The
pairwise preference data generated by Consitutional DPO also contains more principled and targeted
learning signals that can be adjusted by human experts according to target domains and applications.
Furthermore, we propose to transform the annotation instructions and responses used in the
instruction backtranslation and Constitutional DPO stages into annotation instructions and evaluation
instructions. In this way, Weaver not only possesses abilities to follow writing instructions but can
also annotate writing instructions and evaluate writing outputs. We also curate instruction data for
retrieval-augmented generation (RAG) and function calling to enable Weaver to exploit external
knowledge and tools. The combination of different data sources makes Weaver a versatile foundation
model while specializing in creative writing.
How Weaver performs? Evaluating the content creation/writing ability of LLMs remains an
open problem since existing benchmarks for LLMs such as MMLU (Hendrycks et al., 2020) or MT-
Bench (Zheng et al., 2023a) mostly focus on reasoning, math, coding, or general questions instead of
5

--- PAGE 6 ---
creative writing. Moreover, it is already notoriously hard to evaluate LLMs on general instructions,
and it becomes much harder for creative writing tasks since literary critic is non-trivial even for human
experts, not to mention LLMs. To better evaluate Weaver and help the LLM community better
measure progress on AIGC, we carefully curate WriteBench , a benchmark for assessing the creative
writing capabilities of LLMs and collect outputs from 10 +popular LLMs covering both open-source
and proprietary models.
We then conduct both LLM-based and human evaluation of Weaver and generalist LLMs on the
benchmark. Evaluation results confirm the superiority of Weaver compared to generalist LLMs. We
find that Weaver Ultra , the most-capable model in the Weaver family, advances the state-of-the-art
in creative writing despite being 10 +smaller compared to GPT-41, the previous best performing LLM.
Other models in the Weaver family also surpass competitive generalist LLMs several times larger
than them. Our analysis and case studies show that the main source of improvements is because
Weaver can generate texts that are creative and human-like while generalist LLMs tend to produce
too ‚Äúpredictable‚Äù texts. To confirm that Weaver istruly helpful in real-world applications, we also
conduct a user study where human writers are asked to write stories (fiction writing) and blog posts
(non-fictionwriting)with Weaver andGPT-4. OuruserstudyshowsthatcomparedtoGPT-4, Weaver
improves the writers‚Äô productivity by 47% and helps writer produce better stories and articles at the
same time.
What we build with Weaver ?Training specialized LLMs for writing is one side of enhancing AI-
assisted writing experience. We believe it is also very important to build a better human-AI interface to
fully exploit the potential of Weaver on AI-assisted writing. To this end, we introduce WawaWriter ,
our innovative human-AI collaborative writing platform. Similar to recent AI writing products such
as Notion AI, WawaWriter provides a chat interface that allows users to provide diverse writing
instructions, instead of merely suggesting the next one or few sentences based on the current context
or polishing the content as in traditional applications. WawaWriter also takes a few steps further:
(1) we enable human-AI co-editing by allowing users to customize language agents (Zhou et al.,
2023b) that acts like a human collaborator by operating inside the editor simultaneously with users;
(2) we allow users to build personal knowledge bases by saving websites or uploading documents
and build a RAG pipeline that integrates knowledge bases to Weaver ; (3) we propose to provide
personalized writing assistance by analyzing users‚Äô personal writing styles using LLMs based on
their writing history on the platform and using the results to guide Weaver ‚Äôs text generation process.
By combining these innovations, WawaWriter aims to provide next-generation AI-assisted writing
experience that is more helpful and enjoyable.
In the following sections, we first describe the architectures and sizes of the Weaver family and
their pre-training stage. We then present details on the abilities of Weaver , how we synthesize
training data to help Weaver acquire these abilities and learn to produce human-like stylish texts,
and the details for the alignment stage. We also present our benchmark for evaluating the writing
abilities of LLMs and the evaluation results. Finally, we introduce the details of WawaWriter and
present how Weaver paves the way for next-generation AI-assisted writing experiences.
2. Pre-training
2.1. Model Family
Weaver models are language models built on top of Transformer decoders. We have adopted the
recent improvements from the design of LLaMA (Touvron et al., 2023a,b), the most popular open-
1According to non-official rumor about the size of GPT-4
6

--- PAGE 7 ---
source LLM, including a Pre-Norm structure with RMSNorm (Zhang and Sennrich, 2019) function,
SwiGLU (Shazeer, 2020) as the activation function for the Feed-Forward Network, Rotary Embedding
(Su et al., 2024) for positional encoding, and Grouped-Query Attention (GQA) (Ainslie et al., 2023).
TheWeaver family consists of models of four different sizes: Mini,Base,Pro, and Ultra,
ranging from 1.8B to 34B parameters. We train different model sizes to support different applications
as the complexity of writing tasks varies a lot across different domains and use cases. All Weaver
models are initialized from powerful open-source LLMs. We provide detailed configurations and
descriptions of Weaver models in Table 1.
2.2. Pre-training Data
We then present an overview of pre-training data selection strategies and the resulting pre-training
data mixture. Since Weaver models are initialized from powerful open-source LLMs and thus already
possess adequate world knowledge, the amount of continual pre-training data does not need to be
super large. We consider the continual pre-training stage to be the process where Weaver learns to
reallocate or re-balance its capabilities: the model allocates more capabilities to writing and content
creation while reducing the capabilities on other domains such as mathematics and coding.
Therefore, we only include manually verified data sources including various kinds of content such
as books, fiction, stories, news articles, papers, reports, social media copies, etc., in the pre-training
data. We combine rule-based and machine-learning-based methods to filter low-quality texts. In
addition to data sources and filtering, we also carefully control the data mixture between different
domains. Specifically, we mix fiction data (i.e., fiction and stories) and non-fiction data (i.e., articles,
papers, reports, etc.) with a ratio of 1 : 1. We also mix Chinese and English data with a portion of
4 : 1to make Weaver supports both Chinese and English.
2.3. Training Details
We train Weaver using the standard autoregressive language modeling task where the model learns
to predict the next token based on the context of previous tokens. We train Weaver models with a
context length of 4096. We shuffle and merge the documents, and then truncate them to the specified
context lengths to create training batches. We incorporate Megatron-Deepspeed (Shoeybi et al., 2019)
and Flash Attention2 (Dao, 2023; Dao et al., 2022) to improve computational efficiency and reduce
memory usage. We adopt the standard optimizer AdamW (Loshchilov and Hutter, 2017) and set the
hyperparameters ùõΩ1=0.9,ùõΩ2=0.95, and ùúÄ=10‚àí8. We use a cosine learning rate schedule with a
specified peak learning rate for each model. The learning rate is decayed to a minimum learning rate
of10%of the peak learning rate. All models are trained with BFloat16 mixed precision for training
stability. We present detailed pre-training configurations for each model in Table 1.
Name Params ùëõlayers ùëëmodel ùëõheadsContext Sequence LearningTokensLength Batch Size Rate
Weaver Mini 1.8B 24 2048 16 4096 512 1e-4 50B
Weaver Base 6B 32 4096 32 4096 512 1e-4 50B
Weaver Pro 14B 40 5120 40 4096 512 1e-4 40B
Weaver Ultra 34B 60 7168 56 4096 520 5e-5 18B
Table 1|Description for the Weaver family.
7

--- PAGE 8 ---
3. Data Synthesis
After pre-training, Weaver models contain a large amount of world knowledge and writing skills
and can produce human-like texts conditioning on high-quality contexts. To unlock these capabilities
for real-world applications, we need to curate a high-quality dataset for alignment. The format
and quality of the dataset significantly affect the coverage of abilities and the quality of aligned
models. As discussed in the Introduction, the common practice for alignment data collection of
existing generalist LLMs severely limits their writing capabilities. In this section, we describe our data
synthesis framework in detail. We first describe the abilities we want to unlock during the alignment
stage and then present our proposed data synthesis methods for both the supervised fine-tuning and
the preference optimization stage.
3.1. Abilities
We first describe the categories of abilities we want to unlock for Weaver during the alignment stage.
3.1.1. Instruction Following
The first obvious ability we need to unlock is the ability to follow writing instructions and produce
human-like stylish texts. We cover various domains and tasks as listed below during data collection
and alignment training.
3.1.1.1 Domains
Fiction Writing: Fiction writing refers to the abilities of models to write stories and fiction. We divide
fiction writing into several subdomains with respect to the length and the genre of the fiction. We
cover fiction and stories of lengths ranging from a few hundred to a few million characters, and fiction
types including sci-fiction, romance, fantasy, horror, mystery, and thriller.
Creative Non-Fiction Writing: Creative non-fiction writing is a genre of writing that uses literary
styles and techniques to create factually accurate narratives. We cover typical creative non-fiction
writing cases including writing memoirs, biography, travelogue, journalism, social media copy, blog
posts, news articles, commentary, etc.
Marketing Writing: We also consider marketing writing, which involves writing business plans,
advertising copies, product promoting, marketing plans, etc. Marketing writing differs from previous
categories because it is highly application-oriented and the style of generated texts is not the most
important. However, marketing writing still requires human-like creativity to attract potential users.
Technical Writing: Technical writing includes tasks such as paper writing, patent writing, report
writing, etc. Technical writing requires more accuracy compared to creativity. However, writing-
specific training can still be helpful because it can help model produce texts that accurately adhere to
the style required for specific scenarios.
3.1.1.2 Tasks
Content writing: Content writing is the basic task that requires the model to generate content
(i.e., fiction, articles, etc.) based on certain instructions. Writing instructions vary in terms of whether
the previous context is provided and how fine-grained the given instructions are. The task requires
the LLM to be able to understand and adhere to specific requirements expressed in the instructions
8

--- PAGE 9 ---
while also producing texts that are consistent and coherent with previous contexts. For example, a
typical content writing instruction is: ‚ÄúPlease help me write a sci-fi about what will happen after
people finally achieve AGI.‚Äù
Outlining: Outlining is the task of writing outlines, which is a common practice for writers in both
fiction and non-fiction writing. As discussed in the literature of long text generation (Sun et al.,
2022; Yang et al., 2022a, 2023b; Zhou et al., 2019, 2023a), it is often helpful to let the model first
generate an outline before generating long texts. Outlines vary according to different domains and
the granularity/length of outlines. One example for the task of outlining is ‚ÄúPlease help me write an
outline of my annual work report.‚Äù
Polishing & Editing: Polishing and editing require the model to improve the quality of a paragraph or
rewrite it following the requirements expressed in the instructions. The task is closely related to the
task of grammatical error correction (Bryant et al., 2019; Ng et al., 2014) with a key difference that
the modifications are not necessarily grammatical errors. Compared to the task of academic writing
polishing described in Diao et al. (2023), we support customized fine-grained control of polishing or
editing requirements, which is important for human-AI interaction in AI-assisted writing systems. A
typical polishing instruction may look like this: ‚ÄúPlease help me revise the following texts, keep in
mind that the revised texts should be suitable for an academic paper.‚Äù
Style Transferring: The task of text style transfering requires the model to transform texts in one
style into another style. For example, one may want to transform a story into a script or turn a report
into a speechwriting. We cover both template-based style transfer that uses a template to provide
target style information (Guu et al., 2018; Lewis et al., 2020) and description-based style transfer
which uses either a keyword (Hu et al., 2017) or a short description (Zhou et al., 2023c) for the target
style. For example, one may ask the model to ‚ÄúTransform the following book chapter into a script.‚Äù
Expanding/Simplifying: Text expanding and simplifying requires the model to revise an input
paragraph to make it longer or shorter according to certain instructions. Text summarization and
summary-to-article generation can be regarded as two extreme cases of this task. One exemplar
instruction is: ‚ÄúPlease help me summarize this paragraph into one sentence.‚Äù.
Brainstorming: Brainstorming requires the model to help users come up with creative ideas based
on the current context and user instructions. A typical brainstorming instruction is: ‚ÄúPlease give
me 5 possible character descriptions for a villain to appear in the next chapter, including his name,
appearance, occupation, and background.‚Äù
Reviewing: Reviewing refers to the task of reading and analyzing a given piece of text critically and
then producing comments or revising suggestions. For example, one may ask the model to ‚ÄúPlease
take a look at my essay and list 5 suggestions to improve it.‚Äù
3.1.2. Instruction Annotation
We also train Weaver to support the instruction annotation task. As described in Humpback (Li
et al., 2023) and LongForm (K√∂ksal et al., 2023), given a piece of text, the task requires the model
to generate an instruction to which the input texts may be the answer. However, vanilla instruction
backtranslation only supports the writing task. Therefore, for instruction annotation, we require
the model to synthesize an instruction-response pair based on a text span. The response can be the
text span, a part of the text span, or inferred from the text span. This substantially broadens the
scope for vanilla instruction backtranslation since most automatically mined text spans may not be
suitable for a certain instruction on itself while a part of the text span can be a valid response or
one may construct a high-quality instruction-response pair based on it. The instruction annotation
9

--- PAGE 10 ---
ability enables Weaver to mine training data for itself on large-scale corpus, opening the possibility
ofscalable self-training on web data .
3.1.3. Evaluation (Literary Critic)
ManyrecentworkexploredusingortrainingLLMstoevaluategeneralinstructionfollowingtasks(Chan
et al., 2023; Jiang et al., 2023b; Wang et al., 2023b). However, we find generalist LLMs require
extensive prompting skills to make them suitable for evaluating tasks related to creative writing.
Moreover, since almost all students majoring in creative writing are also required to take literary
critic courses, we think learning to perform literary critic may be helpful for the model to produce
better texts as well. Therefore, we also train Weaver to judge the quality of the responses to writing
instructions and do pairwise comparison of two responses.
We collect human preference between model outputs in WawaWriter , our AI-assisted writing
platform and convert the collected preference data to training data for LLM-based evaluation with
carefully curated templates.
3.1.4. Retrieval-Augmented Generation
The ability of retrieval-augmented generation (RAG) (Gao et al., 2023; Lewis et al., 2020), i.e.,
generating responses by referring to external knowledge or references as context. RAG is an important
technique that helps LLMs generate more accurate and informed responses. It can be especially
helpful for writing purposes since it‚Äôs common for human writers to refer to other text samples when
writing fiction or articles. However, most existing LLMs purely rely on prompt engineering to do RAG
and do not perform RAG training during alignment. We believe this limits the ability of LLMs to
make use of retrieved contexts. Therefore, we propose to include RAG-aware training data during
alignment to enhance Weaver ‚Äôs retrieval-augmented generation ability. Specifically, we augment
10% percent of training data by appending a relevant context obtained by retrieving the paragraph
most similar to the target response. In this way, Weaver learns to write by referring to external
contexts and is thus more compatible with RAG techniques compared to most existing LLMs.
3.1.5. Function Calling
The ability to use tools is also very important for LLMs (Schick et al., 2023). This ability, also referred
to as ‚Äúfunction calling‚Äù, is also helpful for writing because the model may need to search the internet
for references or call editor APIs when doing human-AI collaborative writing. To unlock the function
calling ability, we include an open-source function calling dataset2into supervised fine-tuning data.
We also propose a new pipeline to synthesize more diverse function calling data by first using GPT-4
to synthesize diverse environments with multiple tools and APIs, as well as their documentation.
We then randomly select one API at a time and ask GPT-4 to imagine a situation where the API can
be helpful and the plausible arguments for the API. We then reason what one may instruct an LLM
in that situation so that the API should be used with the arguments. Finally, similar to how GPTs
support function calling, we train Weaver to use tools by selecting the right API and generating the
arguments given the instructions and the contexts.
3.2. Instruction Backtranslation
We then describe our proposed improved pipeline for instruction backtranslation. The motivation
for doing instruction backtranslation instead of instruction augmentation methods such as self-
2https://huggingface.co/glaiveai
10

--- PAGE 11 ---
Domain Subdomain Description Source
Fiction Writing Full Novel Web novel, over 1M
wordsProprietary
Short Story Web stories, 10k-20k
wordsProprietary
Creative Non-Fiction Writing Red Top liked and com-
mented posts on RedPicked
Zhihu Top upvoted posts on
ZhihuPicked
Weibo Top liked posts on
WeiboPicked
WeChat Articles Top read articles on
WeChatPicked
DouBan Top liked posts on
DouBanPicked
News & Blogs Popular news/blogs Picked
Technical Writing Papers Academic papers on
CNKIPicked
Essay Online essays Picked
Contract Contracts from online
sourcesPicked
Reports Reports for work,
business, science, etc.Proprietary
Copies Business & Govern-
ment copiesProprietary
Marketing Writing Business Plans Business plans for
projects and startupsProprietary
Industry Report Research report for
different industriesProprietary
Advertising Copy Popular copies for ad-
vertisementsPicked
Marketing Plan Marketing plans for
products & servicesPicked
Product Overview Articles advertising
productsPicked
Table 2|Description of SFT Data sources. We combine similar subdomains in the same fields for
simplicity. The entire training set covers 34 subdomains and around 500,000 instruction-output pairs.
‚ÄúPicked‚Äù means the raw data in the corresponding domains are manually selected.
instruct (Wang et al., 2023a) is very simple: we want to align Weaver on high-quality, stylish, and
11

--- PAGE 12 ---
human-written texts. To achieve this goal, we first collect high-quality stories, fiction chapters, and
copies of different domains. We list the categories of collected texts in Table 2.
We then use a carefully designed few-shot prompt template to synthesize instruction-response
pairs for all aforementioned writing tasks. Specifically, for each subdomain-task pair, we annotate 5
cases of how one can write an instruction-response pair, including both the annotated results and the
rationales for the annotation process: we first select a text span from a case as the output (except
for outlining, brainstorming, and reviewing tasks where the output is transformed from the selected
text span with an additional prompt). We then identify or produce the context for the output. For
example, for the polishing task, the context should be a worse version of the target output, so we
can modify the wording and structure of the target output to make it look worse. Then we infer
the instruction that one may use to transform the context to the output. Taking the polishing task
as an example again, we need to reason what modifications are made and synthesize the polishing
instructions accordingly. For each unlabeled case, we use the annotated cases as few-shot exemplars
and ask GPT-4 to first generate the annotation process in the Chain-of-Thought style (Wei et al.,
2022) and then produce the synthesized instruction-response pairs. The instruction backtranslation
pipeline is illustrated in Figure 1. We synthesize 500,000 high-quality instruction-response pairs
across all domains and tasks with this pipeline. Finally, we do an instruction data selection procedure
following the practice described in (Liu et al., 2023): we first score all instruction-response pairs with
GPT-3.5-turbo and then select top-ranked data in each subdomain-task pair for supervised fine-tuning.
Specifically, we score each instruction-response pair based on the quality and the diversity of the
instruction and the relevance between the instruction and the response.
Editors[Domain]TechnicalWriting[Principle1]Theargumentsshouldbeillustratedlogically‚Ä¶[Instruction]Helpmewriteapaper‚Ä¶[Example-Accepted]Theresultswelldemonstratethe‚Ä¶.[Example-Rejected]Asweconductedexperiments,itisgood‚Ä¶[Domain]CreativeNon-fictionWriting[Principle2]Thecontentsshouldbeeye-catching‚Ä¶[Instruction]Reportanaccidentwithblackbears‚Ä¶[Example-Accepted]Blackbearsrarelyattack.Buthere‚Äôs‚Ä¶[Example-Rejected]Blackbearsattackinsomecases‚Ä¶
[Domain] Technical Writing[Instruction] Help me write a popular science article ‚Ä¶.[Output] √†[Output-Accepted]: In the ever-evolving landscape of artificial intelligence, the emergence of large language models (LLMs) has sparked a revolution in how machines understand and generate human language. These colossal algorithms, with their billions of parameters ‚Ä¶[Guidelines]Youareteachingstudentstheprincipleswhilewriting[domain].Iwillgiveyouaninstructionanditsacceptedoutput.Basedonthisoutput,youneedtoproposearejectedoutputwhichviolatessuchprinciple.Hereisanexample:[Domain]TechnicalWriting[Principle]Theargumentsshouldbeillustratedlogically‚Ä¶[Instruction]Helpmewriteapaper‚Ä¶[Example-Accepted]Theresultswelldemonstratethe‚Ä¶.[Example-Rejected]Asweconductedexperiments,itisgood‚Ä¶Nowwhatyouneedtodois:GPT-4Preference Data for Direct Preference Optimization (DPO)Here is a rejected output regarding the Technical Writing Principlethattheargumentsshould be logically illustrated: [Output-Rejected]: The accidental discovery of small language models (LLMs) has led to a minor shift in how machines mimic and scramble human language. These tiny formulas, with their handful of parameters, have stumbled beyond their intended‚Ä¶SFT DatasetsRandom sampling
Figure 2|Illustration of the Constitutional DPO framework.
3.3. Constitutional DPO: Learning From Principled Negative Examples
Finally, we propose Constitutional DPO, a novel alignment method that encourages LLMs to learn from
preference data consisting of samples from the optimal policy and ‚Äúprincipled‚Äù negative examples
synthesized with AI feedback. Our approach combines the advantages of Constitutional AI (Bai et al.,
2022; Sun et al., 2023), which train reward models based on principles written by human experts,
RLCD (Yang et al., 2023a), which prompt LLMs to generate positive/negative examples and train
reward models with AI-generated preference data, and DPO (Rafailov et al., 2023), which omits
12

--- PAGE 13 ---
Table 3|Examples of expert-annotated principles in four domains and sampled tasks.
Domain Task Principles
Creative Non-
fiction WritingContent WritingThe content should be created to encourage readers to
engage in interactions, comments, etc.
Polishing & Editing The revised content should align with the original text.
Brainstorming The content should refrain from pre-judging ideas.
Technical WritingContent WritingThe generated content should avoid bias toward certain
genders, professions, regions, etc.
Style TransferringThe style of the content should be consistent with the
language style specified in the instructions.
FictionContent WritingThe perspective should remain consistent with the out-
line or previous content.
OutliningThe global outline should not be too brief or general,
omitting key plot points.
Marketing WritngContent Writing The content of the market writing should be accurate.
SummarizingThe summarized content should be all-encompassing,
leaving out no crucial points.
reward model training and does direct preference optimization.
Specifically, we first invite human experts including professional writers, editors, and content
creators to annotate principles for different writing tasks. Different from previous ‚Äúprinciple-based‚Äù
approaches that only write a short description of the principles, for each principle we also collect
one case adhering to the principle and one case violating the principle, as well as natural language
rationales explaining why the cases adhere or violate the principle. Then we sample a subset of the
instructiondatawiththehighestscoresintheaforementioneddatafilteringprocessandconsiderthem
as samples from the optimal policy as the output texts are carefully selected and instruction-output
pairs are top-ranked. For each sample, we first present the principles for the task and ask GPT to
analyze which principle can best explain why the response is of good quality. We then ask GPT to
synthesize a counterpart of the response violating the principle while adding minimal modifications
and do not affect other good aspects of the original response.
With the collected data, we consider the original-perturbed response pairs as (ùë¶ùë§, ùë¶ùëô)pairs and
do standard DPO training. In this way, each data pair contains critical training signals about the
corresponding principles and helps fine-tune the model to follow the principles. The preference data
synthesizedbyourapproachcontainsmuchlessnoisecomparedtostandardRLAIFpipeline, especially
in writing domains since LLMs struggles to do literary critic. Compared to RLCD, the most related
method for preference data generation, we consider high-quality SFT data instead of LLM-generated
as positive examples and use expert-written principles for negative example generation. This makes
the training signal less noisy and more principled.
13

--- PAGE 14 ---
4. Alignment
4.1. Supervised Fine-tuning
4.1.1. Data
To collect the dataset for supervised fine-tuning, we first collect high-quality content written by
human writers and content creators according to their metadata including their ratings, number of
reads, upvotes, and comments. We adopt the aforementioned data synthesis framework to synthesize
instructionfollowingdatacovering30+fine-graineddomainsandover10tasks,instructionannotation
data, text generation evaluation data, retrieval-augmented generation data, and function calling data.
The combined instruction tuning dataset consists of around 1,000,000 samples. We then run the data
filtering process and select 400,000 data points as the final dataset for supervised fine-tuning.
4.1.2. Training
We fine-tune the continual pre-trained models for 3 to 5 epochs. We use a cosine learning rate
scheduler with a peak learning rate of 1e-5 and 2e-5 for larger models (i.e., Weaver Ultra and
Weaver Pro ) and 4e-5 for smaller models (i.e., Weaver Base andWeaver Mini ) with 5% warmup
steps. We train all models with a global batch size of 256. After supervised fine-tuning, we select the
best-performing checkpoint on an internal validation set for preference optimization.
4.2. Preference Optimization
4.2.1. Data
For preference optimization, we select 500 highest-rated samples in the data filtering stage for each
subdomain as positive examples for the Constitutional DPO pipeline. We collect over 200 principles
and their corresponding few-shot exemplars. We generate one negative example per positive example,
resulting in 25,000 preference data pairs.
4.2.2. Training
We fine-tune the supervised fine-tuned models using the conventional DPO algorithm. We train our
models for three to five epochs. We use a linear learning rate scheduler with a peak learning rate of
5e-7 and 5% warmup steps. We train Weaver Ultra using a global batch size of 40, while for the
others we use 32 and set ùõΩ= 0.1. We select the best-performing checkpoint on the internal validation
set as the final Weaver models.
5. Evaluation
5.1.WriteBench
MostexistingbenchmarksforLLMs(Zhengetal.,2023a)andnaturallanguagegeneration(Jiangetal.,
2023c; Lin et al., 2020) focus on the reasoning ability or the general-purpose instruction following
ability instead of the ability of LLMs to produce creative, stylish, and human-like text content. To this
end, we construct WriteBench , a new benchmark for assessing the writing capabilities of LLMs3.
Similar to how we collect training data for Weaver ,WriteBench is designed to cover multiple
domains and tasks. To ensure a fair comparison between Weaver and compared generalist LLMs, the
3WriteBench will be publically available at https://github.com/aiwaves-cn/WriteBench
14

--- PAGE 15 ---
data collection and data selection process for instructions in WriteBench is done by our independent
evaluation team. The resulting WriteBench consists of over 1000 testing instructions covering
four domains including fiction writing, creative non-fiction writing, technical writing, and marketing
writing. The first release of the WriteBench benchmark is in Chinese since we want to measure the
Chinese writing capabilities of the compared models.
5.2. Compared Models
We compare Weaver with competitive Chinese LLMs including both open-sourced models and
proprietary models of different sizes, including GPT-4, GPT-3.5, GLM-4, Claude2, Gemini Pro, ERNIE-
Bot-4.0, ERNIE-Bot-3.5, Qwen-72B-Chat, Qwen-14B-Chat, Qwen-7B-Chat, Qwen-1.8B-Chat, YI-34B-
Chat, YI-6B-Chat, and ChatGLM3-6B. We directly use the same instructions in WriteBench as input
prompts for all tested LLMs and collect the model outputs as responses.
Table 4|LLM-based Evaluation Results
Models Style Relevance Creativity Overall
Weaver Ultra 8.94 8.96 7.71 8.54
GLM-4 8.83 9.55 6.58 8.32
GPT-4 8.80 9.45 6.32 8.19
Weaver Pro 8.52 8.45 7.3 8.09
YI-34B-Chat 8.70 9.17 6.26 8.04
Claude2 8.42 8.89 6.41 7.91
Qwen-72B-Chat 8.47 8.98 5.95 7.80
Weaver Base 8.61 8.81 5.89 7.77
Qwen-14B-Chat 8.51 8.85 5.89 7.75
Weaver Mini 8.41 8.38 6.35 7.71
Gemini Pro 8.39 8.79 5.88 7.69
Qwen-7B-Chat 8.40 8.80 5.81 7.67
Yi-6B-Chat 8.24 8.67 6.00 7.64
ChatGLM3-6B 8.16 8.70 5.86 7.57
GPT-3.5 8.37 8.65 5.60 7.54
ERNIE-Bot-3.5 8.24 8.22 5.71 7.39
ERNIE-Bot-4.0 8.15 8.05 5.61 7.27
Qwen-1.8B-Chat 7.97 7.86 5.66 7.16
5.3. LLM-based Evaluation
We first perform an LLM-based evaluation to do a coarse-grained evaluation of the compared models.
We use GPT-4 as the judge to score each instruction-response pair following the practice and prompt
templates in MT-Bench. The results are shown in Table 4. We find that in terms of writing style
and creativity, Weaver Ultra significantly outperforms all proprietary models including strong
competitors such as GPT-4 and GLM-4. GPT-4 and GLM-4 are better at the relevance metric because
they are at least few times larger than Weaver Ultra and thus have better instruction-following
ability. As for Weaver of other sizes, we can see that with only 14B parameters, Weaver Pro
outperforms all open-source models including those with 70B and 34B parameters, as well as most
proprietary models. Similarly, Weaver Base andWeaver Mini are also comparable with generalist
LLMs with more than two times their sizes. Overall, the results confirm the effectiveness of our data
synthesis and training framework for LLMs specialized in creative writing.
15

--- PAGE 16 ---
Table 5|Human Preference on Fiction Writing with the Elo Ranking System
Models Creativity Style Relevance Fluency Overall
Weaver Ultra 1682 1661 1689 1641 1657
GPT-4 1507 1513 1421 1534 1508
ERNIE-Bot-4.0 1404 1409 1564 1544 1477
Gemini Pro 1513 1469 1409 1360 1430
GLM-4 1391 1445 1415 1417 1425
Table 6|Overall Human Preference with the Elo Ranking System
Models Creativity Style Relevance Fluency Overall
Weaver Ultra 1589 1590 1593 1588 1576
GLM-4 1482 1527 1491 1513 1521
GPT-4 1468 1505 1427 1501 1501
Gemini Pro 1548 1490 1434 1380 1454
ERNIE-Bot-4.0 1410 1385 1552 1515 1445
5.4. Human Evaluation
We then perform a human evaluation to compare Weaver with a few representative LLMs including
GPT-4, GLM-4, ERNIE-Bot-4.0, and Gemini-pro. We recruit 44 professional Chinese writers or editors
as human annotators in human evaluation. We adopt the practice in the ChatBot Arena4benchmark
and let human annotators perform three-way pairwise comparisons between two model outputs
according to their creativity, stylish, relevance, and fluency. We collect 3540 comparison results
and compute the ELO rating of the compared models. The results on fiction writing and the overall
comparison are shown in Table 5 and Table 6, respectively. We can see that professional writers and
editors rates Weaver Ultra significantly better than compared models across all metrics. As for
other compared models, we find that GPT-4 and Gemini Pro are considered to produce more creative
and human-like texts compared to GLM-4 and ERNIE-Bot, we suspect this is because GLM and ERNIE
are aligned using GPT distillation data, which probably harms their creativity.
5.5. User Study
A good LLM for AI-assisted writing should not only be best-performing on benchmarks but also truly
helpful in real-world writing scenarios. To evaluate how truly helpful Weaver is, we conduct a user
study where 5 professional writers are recruited as subjects. Each subject is provided with two chat
interfaces, one with Weaver Ultra and the other with GPT-4. We then let each subject write two
short stories (with two carefully selected topic) of around 6,000 words with two same chat interfaces
powered by GPT-4 and Weaver Ultra respectively5. We measure the time used by the same writer
for finishing the two stories and ask a professional editor to judge their quality. We find that compared
to GPT-4, Weaver Ultra improves the efficiency of the writer by around 3 times. Furthermore, out
of 5 topics, the human editor prefer Weaver generated story for 4 times and can not decide the
winner for the remaining topic. Our user interview reveals that the efficiency improvement mainly
comes from the fact that Weaver is faster and generates more human-like texts that require less
4https://chat.lmsys.org/
5To ensure fair comparison, we give enough time and trials for the writers to get familiar with the interface and the
models.
16

--- PAGE 17 ---
post-editing.
6. Introducing WawaWriter
In this section, we describe WawaWriter , a next-generation AI-assisted writing platform we build to
fully unleash the capabilities of Weaver .WawaWriter integrates key features of recent AI-assisted
writing platforms (e.g., Notion AI) including AI-assisted generation, polishment, and summarization
whilealsoimplementingafewnewinnovationsfornext-generationAI-writingexperience. Wedescribe
these innovations in the following sections.
6.1. Human-AI Collaborative Writing
One major innovation in WawaWriter is a new interface for human-AI collaborative writing, which
delivers a drastically different user experience compared to traditional AI-assisted writing platforms.
Thanks to the Agents (Zhou et al., 2023b) framework, we are able to build controllable writing agents
that act like independent human collaborators/co-authors in standard collaborative editors such
as Google Docs or Notion. The writing agents understands the goal of the current document by
reading customized settings such as the title or a short description of the document. It then takes
actions according to the current content in the document and the recent actions of human users (or
other writing agents) that reveal their focus. Human users can also chat with the writing agents
in a chat interface to instruct them what to do. The ability of writing agents to use both external
APIs such as web search and build-in editor APIs such as bolding or adjusting the line space enables
them to accomplish tasks much more complex than what conventional AI assistants can do. With the
human-agent interaction feature in the Agents framework, WriteBench also supports collaborative
editing between multiple human writers and language agents. Users can customize their multiple
writing agents and collaborate with one or a few of them when writing stories or articles. Users can
specify tasks for each writing agent while multiple writing agents can also communicate with each
other to autonomously distribute labors.
6.2. Integration of External Knowledge and Tools
Another new feature of WawaWriter is that users can now build their own personal knowledge
bases via document uploading or saving web pages. WawaWriter automatically organizes and
summarizes the knowledge base and then uses them as references when writing stories and articles.
Specifically, we prompt an LLM to split documents into chunks based on their semantics, embed them
with our embedding model, and store them in a VectorDB. During writing, we dynamically retrieve
the entries of the user‚Äôs personal knowledge base using semantic search using the current context in
the user‚Äôs editor as the query. Following Socratic Models (Zeng et al., 2023), our knowledge base
also supports images in documents by using GPT-4V to do detailed captioning for each image and
then using the captions as entries representing the corresponding images. Users can also edit the
documents in their personal knowledge bases using all AI-writing features in WawaWriter . In
addition, writing agents described in the previous section can also access the personal knowledge
base of a user through function calling.
6.3. Personalized Writing Assistance
Different from current AI-assisted writing systems, WawaWriter provides personalized writing
assistance for different users that suits their writing styles and content preferences. To achieve, we
maintain a text-based user profile for each user which describes some basic writing habits and styles
17

--- PAGE 18 ---
(e.g., choice of words and punctuation, preference for the length of sentences, etc.) of the user. The
user profile is periodically updated using an LLM according to the recent texts written by the user with
a carefully a designed prompt. The user profile is then used as a prefix in the prompt for Weaver . In
addition to text-based user profiles, we also retrieve paragraphs that are most similar to the current
context in the editor and use them as references for RAG.
6.4. Infinite Long Text Generation
WawaWriter alsosupportsinfinitelongtextgenerationsince Weaver nativelysupportstherecurrent
prompting technique proposed by (Zhou et al., 2023a). Specifically, to generate a very long text, we
iteratively prompt Weaver to generate an outline based on the current context and then generate a
paragraph of text based on the generated outline. WawaWriter integrates the ‚Äústep-by-step‚Äù mode
and the ‚Äúcontinuous‚Äù mode in RecurrentGPT, where the next outline is either manually selected by
the user or automatically selected by an LLM. As discussed in Zhou et al. (2023a), this recurrent
prompting mechanism drastically improves the creativity, consistency, and relevance of the generated
long text, this is especially helpful for story/fiction writing with WawaWriter .
7. Discussion
In this technical report, we introduce Weaver , a family of LLMs specialized for writing endeavors.
Weaver is continually pre-trained on carefully curated datasets and then aligned to the preferences of
professionalwritersandeditorsusinganoveldatasynthesisframework. Wealsorelease WriteBench ,
the first benchmark for evaluating the writing capabilies of LLMs. WriteBench covers multiple
domains and tasks related to writing. We compare Weaver with 10+ popular generalist LLMs
and find that Weaver Ultra is the current state-of-the-art on the benchmark. Our user study also
confirms the superiority of Weaver in real-world AI-assisted writing scenarios. The results also
confirm the effectiveness of our data synthesis pipeline for training domain-specific LLMs.
References
J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr√≥n, and S. Sanghai. Gqa: Train-
ing generalized multi-query transformer models from multi-head checkpoints. arXivpreprint
arXiv:2305.13245, 2023.
Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/introducin
g-claude .
Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,
C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-
Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt,
M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer,
S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan,
T.Hume, S.R.Bowman, Z.Hatfield-Dodds, B.Mann, D.Amodei, N.Joseph, S.McCandlish, T.Brown,
and J. Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot
learners, 2020.
18

--- PAGE 19 ---
C. Bryant, M. Felice, √ò. E. Andersen, and T. Briscoe. The BEA-2019 shared task on grammatical
error correction. In H. Yannakoudakis, E. Kochmar, C. Leacock, N. Madnani, I. Pil√°n, and T. Zesch,
editors,Proceedings oftheFourteenth Workshop onInnovative UseofNLPforBuilding Educational
Applications , pages 52‚Äì75, Florence, Italy, Aug. 2019. Association for Computational Linguistics.
doi: 10.18653/v1/W19-4406. URL https://aclanthology.org/W19-4406 .
C.-M. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and Z. Liu. Chateval: Towards better
llm-based evaluators through multi-agent debate, 2023.
H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma,
A.Webson,S.S.Gu,Z.Dai,M.Suzgun,X.Chen,A.Chowdhery,A.Castro-Ros,M.Pellat,K.Robinson,
D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean,
J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei. Scaling instruction-finetuned language models,
2022.
J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan. Chatlaw: Open-source legal large language model with
integrated external knowledge bases, 2023.
T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R√©. FlashAttention: Fast and memory-efficient exact
attention with IO-awareness. In Advances inNeuralInformation Processing Systems, 2022.
S. Diao, Y. Lei, L. Pan, T. Fang, W. Zhou, S. S. Keh, M.-Y. Kan, and T. Zhang. Doolittle: Benchmarks
and corpora for academic writing formalization. 2023.
Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang. Retrieval-augmented
generation for large language models: A survey. arXivpreprint arXiv:2312.10997, 2023.
Google. An important next step on our AI journey, 2023. URL https://blog.google/technolo
gy/ai/bard-google-ai-search-updates/ .
K. Guu, T. B. Hashimoto, Y. Oren, and P. Liang. Generating sentences by editing prototypes.
Transactions oftheAssociation forComputational Linguistics , 6:437‚Äì450, 2018. doi: 10.116
2/tacl_a_00030. URL https://aclanthology.org/Q18-1031 .
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive
multitask language understanding. arXivpreprint arXiv:2009.03300, 2020.
Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In
International conference onmachine learning, pages 1587‚Äì1596. PMLR, 2017.
J. Ji, T. Qiu, B. Chen, B. Zhang, H. Lou, K. Wang, Y. Duan, Z. He, J. Zhou, Z. Zhang, et al. Ai alignment:
A comprehensive survey. arXivpreprint arXiv:2310.19852, 2023.
A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,
G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXivpreprint arXiv:2310.06825, 2023a.
D. Jiang, Y. Li, G. Zhang, W. Huang, B. Y. Lin, and W. Chen. Tigerscore: Towards building explainable
metric for all text generation tasks, 2023b.
Y.E.Jiang,T.Liu,S.Ma,D.Zhang,R.Cotterell,andM.Sachan. Discoursecentricevaluationofmachine
translation with a densely annotated parallel corpus. In Proceedings ofthe2023Conference of
theAssociation forComputational Linguistics: Human Language Technologies , pages 1550‚Äì1565,
Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/v1/2023
.main.111. URL https://aclanthology.org/2023.acl-main.111 .
19

--- PAGE 20 ---
A. K√∂ksal, T. Schick, A. Korhonen, and H. Sch√ºtze. Longform: Optimizing instruction tuning for long
text generation with corpus extraction, 2023.
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K√ºttler, M. Lewis, W.-t. Yih,
T. Rockt√§schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances
inNeuralInformation Processing Systems, 33:9459‚Äì9474, 2020.
X. Li, P. Yu, C. Zhou, T. Schick, L. Zettlemoyer, O. Levy, J. Weston, and M. Lewis. Self-alignment with
instruction backtranslation, 2023.
B. Y. Lin, W. Zhou, M. Shen, P. Zhou, C. Bhagavatula, Y. Choi, and X. Ren. CommonGen: A constrained
text generation challenge for generative commonsense reasoning. In Findings oftheAssociation
forComputational Linguistics: EMNLP 2020, pages 1823‚Äì1840, Online, Nov. 2020. Association for
Computational Linguistics. URL https://www.aclweb.org/anthology/2020.findings-e
mnlp.165 .
W. Liu, W. Zeng, K. He, Y. Jiang, and J. He. What makes good data for alignment? a comprehensive
study of automatic data selection in instruction tuning, 2023.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXivpreprint arXiv:1711.05101 ,
2017.
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H. Susanto, and C. Bryant. The CoNLL-2014
shared task on grammatical error correction. In H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto,
R. H. Susanto, and C. Bryant, editors, Proceedings oftheEighteenth Conference onComputational
Natural Language Learning: SharedTask,pages1‚Äì14,Baltimore,Maryland,June2014.Association
for Computational Linguistics. doi: 10.3115/v1/W14-1701. URL https://aclanthology.org
/W14-1701 .
OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt .
OpenAI. GPT4 technical report. arXivpreprint arXiv:2303.08774, 2023.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, et al. Training language models to follow instructions with human feedback. Advances in
NeuralInformation Processing Systems, 35:27730‚Äì27744, 2022a.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano,
J. Leike, and R. Lowe. Training language models to follow instructions with human feedback,
2022b.
A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by
generative pre-training. 2018.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised
multitask learners. OpenAI blog, 1(8):9, 2019.
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference
optimization: Your language model is secretly a reward model. 2023.
B. Rozi√®re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin,
A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. D√©fossez,
J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve. Code llama:
Open foundation models for code, 2023.
20

--- PAGE 21 ---
V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja,
M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak,
D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden,
T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao,
S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multitask prompted training enables zero-shot
task generalization. In International Conference onLearning Representations , 2022. URL https:
//openreview.net/forum?id=9Vrb9D0WI4 .
T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda,
and T. Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh
Conference onNeuralInformation Processing Systems , 2023. URL https://openreview.net
/forum?id=Yacmpz84TH .
N. Shazeer. Glu variants improve transformer. arXivpreprint arXiv:2002.05202, 2020.
T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong. Large language model
alignment: A survey. arXivpreprint arXiv:2309.15025, 2023.
M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training
multi-billionparameterlanguagemodelsusingmodelparallelism. arXivpreprint arXiv:1909.08053 ,
2019.
J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary
position embedding. Neurocomputing, 568:127063, 2024.
X. Sun, Z. Sun, Y. Meng, J. Li, and C. Fan. Summarize, outline, and elaborate: Long-text generation
via hierarchical supervision from extractive summaries. In N. Calzolari, C.-R. Huang, H. Kim,
J. Pustejovsky, L. Wanner, K.-S. Choi, P.-M. Ryu, H.-H. Chen, L. Donatelli, H. Ji, S. Kurohashi,
P. Paggio, N. Xue, S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S.-H. Na, editors,
Proceedings ofthe29thInternational Conference onComputational Linguistics , pages 6392‚Äì6402,
Gyeongju, Republic of Korea, Oct. 2022. International Committee on Computational Linguistics.
URLhttps://aclanthology.org/2022.coling-1.556 .
Z. Sun, Y. Shen, H. Zhang, Q. Zhou, Z. Chen, D. Cox, Y. Yang, and C. Gan. Salmon: Self-alignment
with principle-following reward models, 2023.
Gemini Team. Gemini: A family of highly capable multimodal models, 2023.
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,
E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXivpreprint
arXiv:2302.13971, 2023a.
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,
S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes,
J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan,
M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee,
D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton,
J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan,
B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur,
S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and
fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL
https://doi.org/10.48550/arXiv.2307.09288 .
21

--- PAGE 22 ---
L.Tunstall,E.Beeching,N.Lambert,N.Rajani,K.Rasul,Y.Belkada,S.Huang,L.vonWerra,C.Fourrier,
N. Habib, N. Sarrazin, O. Sanseviero, A. M. Rush, and T. Wolf. Zephyr: Direct distillation of lm
alignment, 2023.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin.
Attention is all you need. Advances inneuralinformation processing systems, 30, 2017.
B. Wang, R. Zheng, L. Chen, Y. Liu, S. Dou, C. Huang, W. Shen, S. Jin, E. Zhou, C. Shi, et al. Secrets of
rlhf in large language models part ii: Reward modeling. arXivpreprint arXiv:2401.06080, 2024.
Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning
language models with self-generated instructions. In A. Rogers, J. Boyd-Graber, and N. Okazaki,
editors, Proceedings ofthe61stAnnualMeeting oftheAssociation forComputational Linguistics
(Volume 1:LongPapers), pages 13484‚Äì13508, Toronto, Canada, July 2023a. Association for
Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology
.org/2023.acl-long.754 .
Y. Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang, R. Xie, J. Wang, X. Xie, W. Ye, S. Zhang,
andY.Zhang. Pandalm: Anautomaticevaluationbenchmarkforllminstructiontuningoptimization,
2023b.
Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang, and Q. Liu. Aligning large
language models with human: A survey. arXivpreprint arXiv:2307.12966, 2023c.
Z. M. Wang, Z. Peng, H. Que, J. Liu, W. Zhou, Y. Wu, H. Guo, R. Gan, Z. Ni, M. Zhang, Z. Zhang,
W. Ouyang, K. Xu, W. Chen, J. Fu, and J. Peng. Rolellm: Benchmarking, eliciting, and enhancing
role-playing abilities of large language models, 2023d.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou.
Chain of thought prompting elicits reasoning in large language models. In A. H. Oh, A. Agarwal,
D. Belgrave, and K. Cho, editors, Advances inNeuralInformation Processing Systems , 2022. URL
https://openreview.net/forum?id=_VjQlMeSB_J .
S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and
G. Mann. Bloomberggpt: A large language model for finance, 2023.
K. Yang, Y. Tian, N. Peng, and D. Klein. Re3: Generating longer stories with recursive reprompting and
revision. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings ofthe2022Conference
onEmpirical Methods inNatural Language Processing , pages 4393‚Äì4479, Abu Dhabi, United Arab
Emirates, Dec. 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp
-main.296. URL https://aclanthology.org/2022.emnlp-main.296 .
K. Yang, D. Klein, A. Celikyilmaz, N. Peng, and Y. Tian. Rlcd: Reinforcement learning from contrast
distillation for language model alignment, 2023a.
K. Yang, D. Klein, N. Peng, and Y. Tian. DOC: Improving long story coherence with detailed outline
control. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings ofthe61stAnnual
Meeting oftheAssociation forComputational Linguistics (Volume 1:LongPapers), pages 3378‚Äì
3465, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/
2023.acl-long.190. URL https://aclanthology.org/2023.acl-long.190 .
X. Yang, A. Chen, N. PourNejatian, H. C. Shin, K. E. Smith, C. Parisien, C. Compas, C. Martin, A. B.
Costa, M. G. Flores, et al. A large language model for electronic health records. NPJDigital
Medicine, 5(1):194, 2022b.
22

--- PAGE 23 ---
S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen. A survey on multimodal large language
models. arXivpreprint arXiv:2306.13549, 2023.
A. Zeng, M. Attarian, brian ichter, K. M. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit,
M. S. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, and P. Florence. Socratic models: Composing zero-
shot multimodal reasoning with language. In TheEleventh International Conference onLearning
Representations, 2023. URL https://openreview.net/forum?id=G2Q2Mh3avow .
B. Zhang and R. Sennrich. Root mean square layer normalization. Advances inNeuralInformation
Processing Systems, 32, 2019.
W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. A
survey of large language models. arXivpreprint arXiv:2303.18223, 2023.
L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang,
J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. 2023a.
R. Zheng, S. Dou, S. Gao, Y. Hua, W. Shen, B. Wang, Y. Liu, S. Jin, Q. Liu, Y. Zhou, et al. Secrets of
rlhf in large language models part i: Ppo. arXivpreprint arXiv:2307.04964, 2023b.
W. Zhou, T. Ge, K. Xu, F. Wei, and M. Zhou. Hierarchical summary-to-article generation, 2019. URL
https://openreview.net/forum?id=Hkl8Ia4YPH .
W. Zhou, Y. E. Jiang, P. Cui, T. Wang, Z. Xiao, Y. Hou, R. Cotterell, and M. Sachan. Recurrentgpt:
Interactive generation of (arbitrarily) long text, 2023a.
W. Zhou, Y. E. Jiang, L. Li, J. Wu, T. Wang, S. Qiu, J. Zhang, J. Chen, R. Wu, S. Wang, S. Zhu, J. Chen,
W. Zhang, N. Zhang, H. Chen, P. Cui, and M. Sachan. Agents: An open-source framework for
autonomous language agents, 2023b.
W. Zhou, Y. E. Jiang, E. Wilcox, R. Cotterell, and M. Sachan. Controlled text generation with natural
language instructions. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
editors, Proceedings ofthe40thInternational Conference onMachine Learning , volume 202 of
Proceedings ofMachine Learning Research , pages 42602‚Äì42613. PMLR, 23‚Äì29 Jul 2023c. URL
https://proceedings.mlr.press/v202/zhou23g.html .
23

--- PAGE 24 ---
A. Appendix
A.1. Author Contributions
Tiannan Wang is the core contributor of Weaver . Tiannan is responsible for continual pre-training,
supervised fine-tuning, and preference optimization. Tiannan is also a main contributor for the data
synthesis and the benchmark/evaluation process.
Jiamin Chen is a main contributor of Weaver . Jiamin is responsible for WriteBench and is also
main contributor for data synthesis and model evaluation process.
Qingrui Jia is a main contributor for the data synthesis and supervised fine-tuning stages for fiction
writing. Qingrui also contributes to the data synthesis process for non-fiction writing.
Shuai Wang is responsible for the application and the deployment of Weaver and the prompt
engineering for WawaWriter .
Ruoyu Fang is a main contributor for the data synthesis process for continual pre-training and
supervised fine-tuning.
Huilin Wang ,Chunzhao Xie , andShengwei Ding are main contributors for the prompts inside
WawaWriter .
Zhaowei Gao ,Chunzhao Xie ,Jihong Dai ,Jialong Wu ,Long Li,Zhiwei Huang contributed to the
data synthesis process for non-fiction writing.
Chuou Xu ,Yibin Liu ,Xinle Deng contributed to the evaluation and benchmarking process.
Teng Yu ,Jiayang Huang ,Gangan Ma ,Han Xiao ,Zixin Chen Gangan Ma ,Yiru Wang ,Siran Ding
are responsible for marketing and operation of WawaWriter and contributed to the product.
Jiayi Xu ,Yilihamu Tayier ,Zhenyu Hu ,Yuan Gao ,Chegnfeng Zheng ,Yueshu Ye are responsible
for the implementation of WawaWriter .
Lei Wan ,Siyu Cheng ,Xinyue Jiang ,Siyu Cheng , andZhule Song are responsible for the product
design of Weaver .
Xiangru Tang ,Xiaohua Xu ,Ningyu Zhang ,Huajun Chen are academic collaborators that con-
tributed to the discussion process and the revision of the technical report.
Yuchen Eleanor Jiang andWangchunshu Zhou are project lead and are responsible for the conceptu-
alization, division of labor, and project management for all parts of Weaver training, WriteBench
construction, and the productization of WawaWriter . They write the technical report together.
A.2. Acknowledgments
We would like to thank Canwen Xu for his insightful discussion, help on revising the draft, and
especially for his suggestion on naming the paper. We would also like to thank APUS for their support
on computation resources, ABAKA.AI for their support on data collection, and Zhejiang University for
general supports.
A.3. Case Study
We present a few case studies of content generated by Weaver Ultra and GPT-4:
24

--- PAGE 25 ---
I. ÂÖ¨Âè∏Âü∫Êú¨‰ø°ÊÅØA. ÂÖ¨Âè∏ÁÆÄ‰ªã1. ÂÖ¨Âè∏ÂêçÁß∞2. ÊàêÁ´ãÊó∂Èó¥3. ÂÖ¨Âè∏Âú∞ÁÇπ4. Ê≥ï‰∫∫‰ª£Ë°®B. ÂÖ¨Âè∏ÊúçÂä°Âèä‰∫ßÂìÅ1. ‰∏ªË¶ÅÊúçÂä°2. ‰∫ßÂìÅÁâπÁÇπ3. ‰∫ßÂìÅ‰∏éÊúçÂä°ÁöÑ‰∏ªË¶ÅÂÆ¢Êà∑Áæ§‰ΩìII. ÂÖ¨Âè∏ÂèëÂ±ïÂéÜÁ®ãA. ÂÖ¨Âè∏ÊàêÁ´ãÂàùÊúüB. ‰∏ªË¶ÅÂèëÂ±ïÈò∂ÊÆµÂèäÈáçÂ§ß‰∫ã‰ª∂C. ÁõÆÂâçÁöÑÂèëÂ±ïÁä∂ÂÜµIII. ÂÖ¨Âè∏‰∏öÁª©ÂíåÂ∏ÇÂú∫ËÆ§Áü•Â∫¶A. ÂéÜÂπ¥‰∏öÁª©B. ÊâÄËé∑Â•ñÈ°πÂíåËÆ§ËØÅC. ‰∏éÂêåË°å‰∏öÁöÑÊØîËæÉIV. ÂÖ¨Âè∏ÁöÑÊ†∏ÂøÉÁ´û‰∫â‰ºòÂäøA. ÊäÄÊúØ‰ºòÂäøB. Â∏ÇÂú∫‰ΩçÁΩÆC. ‰∫ßÂìÅÊàñÊúçÂä°ÁöÑ‰ºòÂäøD. Âõ¢ÈòüËµÑË¥®V. ÂÖ¨Âè∏ÊñáÂåñ‰∏é‰ª∑ÂÄºËßÇA. ÂÖ¨Âè∏ÁöÑÊÑøÊôØÂíå‰ΩøÂëΩB. ÂÖ¨Âè∏ÁöÑÊñáÂåñÂíå‰ª∑ÂÄºËßÇC. Á§æ‰ºöË¥£‰ªªÂíåË¥°ÁåÆVI. ÂÖ¨Âè∏ÁöÑÊú™Êù•Â±ïÊúõA. ÂèëÂ±ïËßÑÂàíB. ÁõÆÊ†áÂ∏ÇÂú∫‘Ö‘ß÷¢⁄´Áçç›™’ï·ï®‡π≠Á¢òÁâßË•ë·•ù·ñ´⁄´”û”ªÁ£™Ÿâ‡∑≠‡∑≠·∂≤◊Ä‡¨´·±æ·ëÄ‡≤¶ËÉÇÁä©Á£™Ë§ñÁçç›™Áçç›é›™ÁñªÁúêŸ≠·å±‡•ü·ïêÁâß÷¶ËÉºÁéΩ‡µâ◊Ä”ûÁä©›á·òçÁâ´
‰∏ãÈù¢ÊòØÊó•Êó•È°∫‰æõÂ∫îÈìæÁßëÊäÄËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏ÂÖ¨Âè∏ÂèëÂ±ïÊÉÖÂÜµÁöÑÂ§ßÁ∫≤ÁöÑÂèÇËÄÉÁ§∫‰æãÔºöI. ÂÖ¨Âè∏Ê¶ÇÂÜµ‰∏éÂèëÂ±ïÂéÜÁ®ãA. ÂÖ¨Âè∏ÂêçÁß∞ÂèäÊàêÁ´ãËÉåÊôØB. ÈáçË¶ÅÂèëÂ±ïÈáåÁ®ãÁ¢ëII. ‰∏ªË¶Å‰∏öÂä°ËåÉÂõ¥A. ÊúçÂä°È¢ÜÂüüÊ¶ÇËø∞B. ÁâπËâ≤ÊúçÂä°‰∏éÂàõÊñ∞Ê®°ÂºèIII. ÂÆ¢Êà∑‰ª∑ÂÄºÂàõÈÄ†A. Âõ¥ÁªïÁî®Êà∑‰ΩìÈ™åÁöÑÊúçÂä°ÊàòÁï•B. ‰æõÂ∫îÈìæËß£ÂÜ≥ÊñπÊ°àÁöÑÊï¥Âêà‰∏é‰ºòÂåñIV. ÊäÄÊúØÈ©±Âä®‰∏éÂàõÊñ∞A. ÁßëÊäÄÂåñÁâ©ÊµÅÂπ≥Âè∞‰ªãÁªçB. ÊäÄÊúØÊúçÂä°ÁîüÊÄÅÁ≥ªÁªüÁöÑÊûÑÂª∫V. Âêà‰Ωú‰ºô‰º¥‰∏éÂÆ¢Êà∑ÊúçÂä°ÁΩëÁªúA. ÁîüÊÄÅÂêà‰ΩúÂπ≥Âè∞B. ÂÖ®ÁêÉ‰æõÂ∫îÈìæÁÆ°ÁêÜËÉΩÂäõVI. Êú™Êù•ÂèëÂ±ïÂ±ïÊúõA. Â∫îÂØπÁé∞‰ª£Áâ©ÊµÅË°å‰∏öÂèòÈù©B. ÊèêÂçáÂÆ¢Êà∑ÂÖ®ÈìæË∑ØÁâ©ÊµÅÊúçÂä°Ë¥®Èáè
I. ÂºïË®ÄA. Â®±‰πêÁöÑÂÆö‰πâÂíå‰∏çÂêåÂΩ¢ÂºèB. ÊèèËø∞Â®±‰πêË°å‰∏ö‰∏≠ÁöÑ‰∏Ä‰∫õÂ∏∏ËßÅÊÇ¨ÂøµÂíåÂ•áÈÅáC. ÊèêÂá∫ÊñáÁ´†ÁöÑ‰∏ªÈ¢òÔºöÊé¢ËÆ®Â®±‰πêËÉåÂêéÁöÑÊÇ¨ÁñëÊïÖ‰∫ã‰∏éÁîüÊ¥ªÂ•áÈÅáII. Â®±‰πê‰∫ß‰∏öÁöÑÂÜÖÁßòÂíåÊÇ¨ÁñëÊïÖ‰∫ãA. ÂàÜÊûêÂ®±‰πêË°å‰∏öÁöÑËøê‰ΩúÊ®°ÂºèB. Êè≠Èú≤‰∏Ä‰∫õÊú™Áü•ÊàñÈ≤ú‰∏∫‰∫∫Áü•ÁöÑË°å‰∏öÁßòÂØÜÂíåÊÇ¨ÁñëÊïÖ‰∫ãC. ÂàÜ‰∫´‰∏Ä‰∫õÂ®±‰πêÂúà‰∫∫Â£´ÁöÑ‰∫≤Ë∫´ÁªèÂéÜÂíåÊïÖ‰∫ãIII. ÁîüÊ¥ª‰∏≠ÁöÑÂ•áÈÅá‰∏éÂ®±‰πêÁöÑÂÖ≥Á≥ªA. ËØ¶ÁªÜËß£Êûê‰∏Ä‰∫õÁîüÊ¥ª‰∏≠ÁöÑÂ•áÈÅáÊòØÂ¶Ç‰ΩïÊàê‰∏∫Â®±‰πêÈ¢òÊùêÁöÑB. ÂàÜ‰∫´‰∏Ä‰∫õ‰ªéÁîüÊ¥ª‰∏≠Â•áÈÅáËé∑ÂæóÁöÑÂ®±‰πêËßÇÁÇπÂíåËßÅËß£C. ËÆ®ËÆ∫Â®±‰πêÂíåÁîüÊ¥ªÂ•áÈÅáÂØπ‰∫é‰∏™‰ΩìÂíåÁ§æ‰ºöÁöÑÈáçË¶ÅÊÑè‰πâIV. Â®±‰πêË°å‰∏öÁöÑÂèëÂ±ïÂâçÊôØ‰∏éÂΩ±ÂìçA. ÂàÜÊûêÂΩìÂâçÂ®±‰πêË°å‰∏öÁöÑÂèëÂ±ïË∂ãÂäøÂíåÊú™Êù•ÂâçÊôØB. ËÆ®ËÆ∫Â®±‰πêË°å‰∏öÁöÑÂèëÂ±ïÂØπ‰∫éÁ§æ‰ºöÊñáÂåñÂíå‰∏™‰∫∫ÁîüÊ¥ªÁöÑÂΩ±ÂìçC. Êèê‰æõ‰∏Ä‰∫õÂÖ≥‰∫éÂ¶Ç‰ΩïÁêÜËß£ÂíåÂ∫îÂØπÂ®±‰πêË°å‰∏öÂèòÂåñÁöÑÂª∫ËÆÆV. ÁªìËÆ∫A. ÈáçÁî≥Â®±‰πêË°å‰∏öËÉåÂêéÁöÑÊÇ¨Áñë‰∏éÁîüÊ¥ªÂ•áÈÅáÁöÑÂÖ≥ËÅîÊÄßB. Ê¶ÇÊã¨ÊñáÁ´†ÁöÑ‰∏ªË¶ÅËßÇÁÇπÂíåÂèëÁé∞C. ÂØπËØªËÄÖÁöÑÂêØÁ§∫ÂíåÂ∏åÊúõËØªËÄÖÂØπÂ®±‰πê‰∫ß‰∏öÊúâÊõ¥Â§öÁöÑ‰∫ÜËß£ÂíåËÆ§ËØÜ‘Ö‡±ØË∑•‡®ß”û”ª‡•ü·ïêÁâß‡¢±·ï∞‘Ü·∑å‚Äú‡ßà‘î·å±ËÉô›∏Áâ©‡∞èÁ∂°Á°≤‘™Áäå·äû·Åö‡•∞ËùΩ‚Äù·å±Áçç’º›©‡∑à·íç‡¶á÷úË±îÁâ´
1. ÊÇ¨ÁñëÊïÖ‰∫ãÁöÑÂºïÂÖ•-ÁÆÄÂçï‰ªãÁªçÊÇ¨ÁñëÊïÖ‰∫ãÂØπÊôÆÁΩóÂ§ß‰ºóÁöÑÂê∏ÂºïÂäõÂèäÂ®±‰πê‰ª∑ÂÄº„ÄÇ-Á™ÅÂá∫ËøôÁ±ªÊïÖ‰∫ãËÉΩÂ§üÂºïÂèë‰∫∫Âº∫ÁÉàÂ•ΩÂ•áÂøÉÂíåÊé¢Á¥¢Ê¨≤ÁöÑÁâπÊÄß„ÄÇ2. Â®±‰πêËÉåÂêéÁöÑÁúüË∞õ-ÂàÜÊûêÂ®±‰πê‰∏≠Ëï¥Âê´ÁöÑÊôÆÈÅçÂøÉÁêÜÈúÄÊ±ÇÂíå‰∫∫ÊÄßÊé¢Á©∂„ÄÇ-Êé¢ËÆ®ÊÇ¨ÁñëÊïÖ‰∫ãÂ¶Ç‰ΩïÊàê‰∏∫‰∫∫‰ª¨ÊîæÊùæÂíåÂøÉÁêÜÁñóÊÑàÁöÑÊâãÊÆµ„ÄÇ3. ÊÇ¨ÁñëÂâßÊ°à‰æãÂàÜÊûê-ÈÄâÂèñ„Ää‰∏çÈÄüÊù•ÂÆ¢„ÄãÁ≠âÊÇ¨ÁñëÁîµÂΩ±ÔºåÊ∑±ÂÖ•ÂâñÊûêÊïÖ‰∫ãÊÉÖËäÇÂíåËßíËâ≤ÊûÑÈÄ†„ÄÇ-ÂàÜÊûêÂâß‰∏≠ÁöÑÊÇ¨ÁñëÂÖÉÁ¥†Â¶Ç‰Ωï‰∏éËßÇ‰ºóÁöÑÂøÉÁêÜÈ¢ÑÊúüÁõ∏ÂåπÈÖç„ÄÇ4. ÁîüÊ¥ªÊïÖ‰∫ã‰∏éËßíËâ≤ÊâÆÊºî-Â∞Ü‰∏™‰∫∫ÁîüÊ¥ª‰∏éÊÇ¨ÁñëÊïÖ‰∫ãÁõ∏ÂØπÊØîÔºåÂ±ïÁé∞‰∫∫‰ª¨Âú®Âπ≥Âá°ÁîüÊ¥ª‰∏≠ÈÅáÂà∞ÁöÑÂ•áÂ¶ôÂíå‰∏çÁ°ÆÂÆö„ÄÇ-ÈòêÈáäÁé∞‰ª£ÁîüÊ¥ªÊïÖ‰∫ã‰∏≠Â¶Ç‰ΩïÂÄüÈâ¥ÊÇ¨ÁñëÊïÖ‰∫ã‰∏≠ÁöÑÊÉÖËäÇÂíåÁ≠ñÁï•„ÄÇ5. Á§æ‰ºöÁé∞Ë±°‰∏éÂêØÁ§∫-ËÄÉÂØüÁé∞‰ª£Á§æ‰ºö‰∏≠ÊÇ¨ÁñëÊïÖ‰∫ãÁöÑÊµÅË°åÔºåÊâÄÂ∏¶Êù•ÁöÑÊ≠£Èù¢ÂíåË¥üÈù¢ÂΩ±Âìç„ÄÇ-ÊèêÂá∫ÊÇ¨ÁñëÊïÖ‰∫ã‰Ωú‰∏∫Êó•Â∏∏Â®±‰πê‰∏éËá™ÊàëÂèçÁúÅÁõ∏ÁªìÂêàÁöÑÊΩúÂäõ„ÄÇ6. ËØªËÄÖÂèÇ‰∏éÂíåÂàÜ‰∫´-ÈºìÂä±ËØªËÄÖÂàÜ‰∫´Ëá™Â∑±ÁªèÂéÜ‰∏≠ÁöÑ‚ÄúÊÇ¨ÁñëÊó∂Âàª‚Äù„ÄÇ-ÁªÑÁªáËØÑËÆ∫Âå∫ÂíåÊäïÁ•®Ê¥ªÂä®ÔºåÂ¢ûÂä†ËØªËÄÖÁöÑÂèÇ‰∏éÊÑüÂíåÂáùËÅöÂäõ„ÄÇ7. ÁªìÊùüËØ≠-Ê¶ÇÊã¨ÊñáÁ´†Ë¶ÅÁÇπÔºåÊèêÂá∫‰∏Ä‰∏™Ê∑±ÂàªÁöÑÁªìËÆ∫„ÄÇ-ÊøÄÂèëËØªËÄÖÂØπÊó•Â∏∏ÁîüÊ¥ª‰∏≠ÊÇ¨ÁñëÊïÖ‰∫ãÁöÑÂÖ¥Ë∂£ÂíåËÆ§ËØÜÔºå‰ΩøÊñáÁ´†ÂÖ∑ÊúâÂêØÂèëÊÄßÂíåÊÄùËÄÉ‰ª∑ÂÄº„ÄÇ25

--- PAGE 26 ---
ÂπªÁÅØÁâá1ÔºöÊ†áÈ¢òÈ°µ-‰∏ªÈ¢òÔºöËãπÊûúVision ProÂèëÂ∏ÉÂêéÁöÑÂêÑÊñπÈù¢ÂΩ±ÂìçÂèäÂØπÊú™Êù•ÁöÑÈ¢ÑÊµã-‰∏ªËÆ≤‰∫∫ÔºöÔºà‰Ω†ÁöÑÂêçÂ≠óÔºâÂπªÁÅØÁâá2-4ÔºöÁõÆÂΩï-ÂπªÁÅØÁâá2ÔºöËãπÊûúVision ProÁöÑÁâπÊÄßÁÆÄ‰ªã-ÂπªÁÅØÁâá3ÔºöÂØπÂ∏ÇÂú∫ÁöÑÂΩ±Âìç-ÂπªÁÅØÁâá4ÔºöÂØπÊ∂àË¥πËÄÖÁöÑÂΩ±ÂìçÂπªÁÅØÁâá5-6ÔºöËãπÊûúVision ProÁöÑÁâπÊÄßÁÆÄ‰ªã-ÂπªÁÅØÁâá5ÔºöËÆæÂ§áÁâπÊÄßÂíåÊäÄÊúØËßÑÊ†º-ÂπªÁÅØÁâá6Ôºö‰∏éÂ∏ÇÂú∫‰∏äÂÖ∂‰ªñÁõ∏‰ºº‰∫ßÂìÅÁöÑÊØîËæÉÂπªÁÅØÁâá7-10ÔºöÂØπÂ∏ÇÂú∫ÁöÑÂΩ±Âìç-ÂπªÁÅØÁâá7ÔºöÂØπÊô∫ËÉΩÊâãÊú∫ÂíåÂπ≥ÊùøÁîµËÑëÂ∏ÇÂú∫ÁöÑÂΩ±Âìç-ÂπªÁÅØÁâá8ÔºöÂØπËôöÊãüÁé∞ÂÆûÂíåÂ¢ûÂº∫Áé∞ÂÆûÂ∏ÇÂú∫ÁöÑÂΩ±Âìç-ÂπªÁÅØÁâá9ÔºöÂØπÂõæÂΩ¢ËÆæËÆ°ÂíåËßÜÈ¢ëÂà∂‰ΩúÂ∏ÇÂú∫ÁöÑÂΩ±Âìç-ÂπªÁÅØÁâá10ÔºöÂØπÁ´û‰∫âÂØπÊâãÁöÑÂΩ±ÂìçÂπªÁÅØÁâá11-14ÔºöÂØπÊ∂àË¥πËÄÖÁöÑÂΩ±Âìç-ÂπªÁÅØÁâá11ÔºöÊ∂àË¥πËÄÖ‰ΩìÈ™åÂíåÊúüÊúõÁöÑÊîπÂèò-ÂπªÁÅØÁâá12ÔºöÊ∂àË¥πËÄÖË¥≠‰π∞Ë°å‰∏∫ÁöÑÊîπÂèò-ÂπªÁÅØÁâá13ÔºöÊ∂àË¥πËÄÖÂØπÂìÅÁâåÁöÑËÆ§Áü•ÂíåÂø†ËØöÂ∫¶ÁöÑÊîπÂèò-ÂπªÁÅØÁâá14ÔºöÊ∂àË¥πËÄÖÊï∞ÊçÆÂíåÈöêÁßÅÁöÑÈóÆÈ¢òÂπªÁÅØÁâá15-18ÔºöÂØπÊú™Êù•ÁöÑÈ¢ÑÊµã-ÂπªÁÅØÁâá15ÔºöËãπÊûúVision ProÁöÑÊú™Êù•ÂºÄÂèëË∂ãÂäø-ÂπªÁÅØÁâá16ÔºöÂ∏ÇÂú∫Ë∂ãÂäøÂíåÁ´û‰∫âÊ†ºÂ±ÄÁöÑÈ¢ÑÊµã-ÂπªÁÅØÁâá17ÔºöÊ∂àË¥πËÄÖÈúÄÊ±ÇÂíåË°å‰∏∫ÁöÑÈ¢ÑÊµã-ÂπªÁÅØÁâá18ÔºöÊäÄÊúØÂèëÂ±ïÁöÑÊú™Êù•È¢ÑÊµã[...]·¶°‡∞Æ”û”ªPPT‡•ü·ïêÁâßËÇû·ÜåËà©‡∫éVision Pro›é‡´≤›∏·å±›±‡∑ú·∂é‡≠Ωﬂ•Áä•Áé∞‡©í‡πö‡π∂·å±·∂º·Å•ÁâßÁπäÁãíŸñ‡®ª·ßá‡©±‡¨öËùí‡£Å›≥”û‡£ã·Ñç·¶ñ”æÁñªÁ∫àÁâß‡Øõ·∂≠Ë∏û·ïÖ‘Ö19·∂≠Áâê
1. ÂºïË®Ä(1-2È°µ)ÂπªÁÅØÁâá1: ÊºîËÆ≤ÂºÄÂú∫‰∏éÁõÆÁöÑ-‰ªãÁªçËãπÊûúVision ProÁöÑÂèëÂ∏ÉËÉåÊôØ-Ê¶ÇËø∞Vision ProÁöÑ‰∏ªË¶ÅÁâπÁÇπ‰∏éÊΩúÂú®ÂΩ±Âìç-ÈòêËø∞Á†îÁ©∂ËãπÊûúÊñ∞‰∫ßÂìÅÂØπ‰∏ñÁïåÂΩ±ÂìçÁöÑÈáçË¶ÅÊÄßÂπªÁÅØÁâá2: ÊäÄÊúØÂèëÂ±ïË∂ãÂäøÁÆÄËø∞-ÂõûÈ°æÊäÄÊúØÂèëÂ±ïÂéÜÂè≤ÔºåÂ∞§ÂÖ∂ÊòØVR„ÄÅARÁöÑÂèëÂ±ïË∂ãÂäø-ÊèèËø∞ËãπÊûúÂú®ÊäÄÊúØÂèëÂ±ï‰∏≠ÁöÑËßíËâ≤ÂíåÂ∏ÇÂú∫Âú∞‰Ωç2. ËãπÊûúVision ProÊ¶ÇËø∞(2-3È°µ)ÂπªÁÅØÁâá3: Vision ProÊäÄÊúØËßÑÊ†º-Â±ïÁ§∫Vision ProÁöÑÊäÄÊúØËßÑÊ†ºÂíåÂàõÊñ∞ÁâπÊÄß-‰∏éÂâç‰ª£‰∫ßÂìÅËøõË°åÊØîËæÉÔºåÁ™ÅÂá∫ÂÖ∂ËøõÊ≠•‰∏éÊîπËøõÂπªÁÅØÁâá4: Vision ProÁöÑËÆæËÆ°‰∏éÁî®Êà∑‰ΩìÈ™å-ÈòêËø∞Vision ProÁöÑËÆæËÆ°ÁêÜÂøµÔºåÂåÖÊã¨ÂÆ°ÁæéÂíåÂäüËÉΩ‰∏äÁöÑÁâπÁÇπ-ÂàÜÊûêVision ProÂú®Áî®Êà∑‰ΩìÈ™å‰∏äÁöÑ‰ºòÂäøÂíåÊΩúÂú®ÂΩ±ÂìçÂπªÁÅØÁâá5: ËãπÊûúÁîüÊÄÅÁ≥ªÁªüÂÜÖÁöÑÊï¥Âêà-ËÆ®ËÆ∫Vision ProÂ¶Ç‰Ωï‰∏éËãπÊûúÂÖ∂‰ªñ‰∫ßÂìÅÂíåÊúçÂä°Êï¥Âêà-ÂàÜÊûêÂÖ∂Êï¥ÂêàÊÄßÂØπÊ∂àË¥πËÄÖÁöÑÂê∏ÂºïÂäõ3. Vision ProÂèëÂ∏ÉÂØπÂêÑË°åÂêÑ‰∏öÁöÑÂΩ±Âìç(3-4È°µ)ÂπªÁÅØÁâá6: ÂΩ±ÂìçÊ∂àË¥πËÄÖÊäÄÊúØÊ†áÂáÜ-Êé¢ËÆ®Vision ProÂØπÊ∂àË¥πËÄÖÊúüÊúõÂíåÊäÄÊúØÊ†áÂáÜÁöÑÂΩ±Âìç-ÂàÜÊûêVision ProÂèØËÉΩÂºïÂèëÁöÑÊ∂àË¥πÂçáÁ∫ßÂíåËøûÈîÅÂèçÂ∫îÂπªÁÅØÁâá7: ÂΩ±ÂìçÂºÄÂèëËÄÖ‰∏éÂÜÖÂÆπÂàõÈÄ†-ËßÇÂØüVision ProÂØπÂ∫îÁî®ÂºÄÂèëËÄÖÂíåÂÜÖÂÆπÂàõÈÄ†ËÄÖÂ∏¶Êù•ÁöÑÊú∫ÈÅá-Êé¢ËÆ®Êñ∞‰∫ßÂìÅÂØπÁßªÂä®ÂíåÊ°åÈù¢ËΩØ‰ª∂ÂèëÂ±ïÁöÑÊΩúÂú®Êé®Âä®‰ΩúÁî®ÂπªÁÅØÁâá8: Á´û‰∫âÂØπÊâãÂíåË°å‰∏öÂèçÂ∫î-ÂàÜÊûêVision ProÂèëÂ∏ÉÂØπÂÖ∂‰ªñÁßëÊäÄÂÖ¨Âè∏Á≠ñÁï•ÁöÑÂΩ±Âìç-È¢ÑÊµãÂ∏ÇÂú∫‰∏äÁöÑÁõ∏Â∫îÂèçÂ∫îÂíåÊΩúÂú®Êñ∞‰∫ßÂìÅ[...]
ÊûóÈ¶ôÁã¨Ëá™Ëµ∞Âú®ÊûóËç´ÈÅì‰∏äÔºåÂÜ∞ÂÜ∑ÁöÑÈ£éÂàÆËøáÔºåÊí©Êã®Ëµ∑Â•πÂ§¥‰∏äÂèàÈªëÂèàÂπ≥ÁöÑÁü≠Âèë„ÄÇÂ•πÊääË£πÂú®Ë∫´‰∏äÁöÑÂ§ßË°£Á¥ßÁ¥ßÊãâËøëÔºå‰∏ÄÂèåÁ∫§ÁªÜÂäõÈÅìÁöÑÊâãÂú®ÊÄÄÈáåÁ¥ßÁ¥ßÊè°Êã≥ÔºåÁ¥ßÂº†ÂéãÊäëÁöÑÊ∞îÊ∞õÂÉèÊòØÈ¢ÑÁ§∫ÁùÄÂç≥Â∞ÜÊù•‰∏¥ÁöÑÈ£éÊö¥„ÄÇÁ™ÅÁÑ∂Ôºå‰∏ÄÂ£∞ÂÜ∑Á¨ëÂà∫ÂÖ•Â•πÁöÑËÄ≥ËæπÔºåÊûóÈ¶ôÂçä‰ΩéÁùÄÂ§¥ÔºåÈöèÂç≥‰æøÁúãÂà∞ÂâçÊñπ‰∏ÄÁæ§È´òÈ´òÂú®‰∏äÁöÑÂ∞ëÂπ¥Â∞ëÂ•≥Â†µÂú®Â•πÁöÑÂéªË∑Ø‰∏ä„ÄÇÁãÇÊ¨¢ÊÅ∂Ê∞ëÁöÑÁõÆÂÖâËêΩÂú®Â•πË∫´‰∏äÔºåËØ≠Ê∞î‰∏≠ÂÖÖÊª°‰∫ÜÊÅ∂ÊÑèÔºå‚ÄúÁúãÔºåÈÇ£‰∏çÂ∞±ÊòØÂí±‰ª¨Â≠¶Ê†°ÈÇ£ÊûóÂ∞èÈ¶ôÂòõÔºåÂ•πÈÇ£ÈöæÂ∫¶ÁúãÁöÑËÑ∏ÈÉΩËÆ©ÊàëÊ≤°ËÉÉÂè£ÂêÉÈ•≠Âï¶„ÄÇ‚ÄùÈÇ£‰∫õÂà∫ËÄ≥ÁöÑËÆ•Á¨ëÂ£∞ÔºåÂ∞ñÈîêÁöÑË¥£ÈóÆÂ£∞ÔºåÂ•πÈÉΩÊâøÂèó‰∫Ü‰∏ãÊù•ÔºåÁ°¨ÁîüÁîüÁöÑÊääÂÜÖÂøÉÁöÑÂßîÂ±àÂíåÊó†Â•àÂêûÂíΩ‰∏ãÂéª„ÄÇÂ•πÂ∞ùËØïÁùÄÂèåÊâãÊä°ÂúÜÔºåÂéªÊå°‰ΩèÈÇ£‰∫õÊâëÈù¢ËÄåÊù•ÁöÑÁæûËæ±ÔºåÁÑ∂ËÄå‰∏ÄÂàáÈÉΩÊòØÂæíÂä≥ÁöÑ„ÄÇÂõûÂà∞ÂÆ∂‰∏≠ÔºåÊûóÈ¶ôÂùêÂú®ÁîµËÑëÂâçÔºå‰∫ÆÊô∂Êô∂ÁöÑÂ±èÂπïÂèçÂ∞ÑÁùÄÂ•πËãçÁôΩËÄåÂèàÊûØÈªÑÁöÑËÑ∏Â∫û„ÄÇÂ•πÂèëÁé∞‰∫Ü‰∏Ä‰ΩçÂÖ≥Ê≥®Â•πÁöÑÁΩëÁªúÂ•ΩÂèã‚ÄúÂ≠§Êòü‚ÄùÔºåÈÇ£ÊòØ‰∏Ä‰∏™ÊÄªÊòØÂú®Ê∑±Êõ¥ÂçäÂ§úÈô™ÁùÄÂ•πËÅäÂ§©ÁöÑ‰∫∫ÔºåÂØπÂ•πÊó†ÂæÆ‰∏çËá≥ÁöÑÂÖ≥ÂøÉËÆ©Â•πÊÑüÂà∞Ê∏©Êöñ„ÄÇÊûóÈ¶ôÁªô‰ªñÂèë‰∫Ü‰∏ÄÂº†Ëá™Â∑±Ë¢´Ê¨∫Ë¥üÁöÑÁÖßÁâáÔºåÊ∏¥ÊúõÂæóÂà∞ÂØπÊñπÁöÑ‰∏ÄÁÇπÂêåÊÉÖÔºå‰∏ÄÁÇπÊÖ∞Ëóâ„ÄÇÂ•πÂèëÂÆåÁÖßÁâá‰πãÂêéÔºåÊó©Â∑≤Ê≥™ÊµÅÊª°Èù¢„ÄÇÁúãÁùÄÂ±èÂπï‰∏äÁöÑËØªËÄÖ‰ΩìÈ™å‰∫∫Êï∞Ôºå‰∏Ä‰∏™‰∏™ÈôåÁîüÁöÑÂêçÂ≠óÔºåÂ•πÊâçÊÉ≥Ëµ∑Ëá™Â∑±Âπ∂‰∏çÊòØ‰∏Ä‰∏™‰∫∫ÔºåÂ•πÊúâ‚ÄúÂ≠§Êòü‚ÄùÔºåÊúâ‰ªñÁöÑÊîØÊåÅÂíå‰øùÊä§„ÄÇÁÑ∂ËÄåÊ≠§Êó∂Ôºå‰ªñÂç¥Âõ†‰∏∫Êúâ‰∫ãËÄåÂåÜÂøôÁ¶ªÂºÄÔºå‰ªñÁöÑÁÖßÁâáÂú®Â±èÂπï‰∏ä‰∏ÄÈó™ËÄåËøáÔºåÈÇ£ÂîØÁæéÁöÑÁÅµÈ≠ÇÂäõÈáèËÆ©ÊûóÈ¶ôÊó†Ê≥ïÊäµÊå°„ÄÇ[...]‡≠èÁñªÃΩ›í‡®ªÁâ©Ëùç‡©î·õî‡±ØÁäå·ÜΩ·å±Á°≤‘™Ãæ‡©ú·ßî·å±‡∫Ö‡Øèﬁæ·ñ´Ë±îÁâß·ïπ‡µóÁçäÁº∞‘Ü·•ùÈÉ¢·ï∂Áâê‡•ü·ïêÁâ©‚Äú‡±Ø‡£Å‡®ç‡ªäËûó⁄©‡æÇÁçõÁâß‡£Å÷ï·óë”§ËùΩ⁄©‘ß”û”ªŸâ‡Æû‡±Ø·å±’àÁâê‡±Ø‘Ö‘ß›íÁ¨ñÁäùÁ¢â‡®ªÁâßËùΩ⁄©‘ß”û”ª·ÉÖÁ§ñ·å±·ä´‡®éÁ∫∑‡∏àÁâß‡±Ø’™‡≠è‡¶§·çò‘∞·Üô·∂∂Áâß‡≠åÁºè‘ßÁ≤¨‡æõ·å±ŸâÁæ¨ÁâêÁ∞Å·òíÁâß‡±Ø‡©íÁä¢·å±Áú§Áúê›í‡ÆëÁì∞ÊùÇÁâß‡±Ø‡≥Ö‡ÆûÁä¢’ø‡ß®‡≠í‡±ØÁ¢â‡®ª·å±‘™Ë¥∂ÁâêÁ£ß·ï£Áâß‡±ØË±ô‡®ßﬁæÁä¢Áç§‡≤ãÁâß‡±Ø‡≠è‡¶§‡£Å·óë·ï∂”§‡•üÈÖÑ‡±Æ€ëÁâß·òíÁä¢‡¢©‘Ö‡•¶›Ñ‡±Ø·å±Ë∂Ö€ó·òíËùõ·Çπ‡¥ßÁΩ≠ÁâêÁä¢·§©Á≤ÅÁ∏åÁΩ≠Áâß‡±ØÁüÆË•∑‘ß·ç•·çò›∏ÁâßÁä¢‡ÆÅÁöàŸ∑ËùöÁâêÁ£ß›∏Áâß‡±Ø‡±Æ‘ÖÁé±‡ªä‡¢ÆË•πÁçõ·®¥›©·å±‡Æ™Áú≠·òèÁâßË∂Ö€ó·ÆéÁäöËöé”§·ï∑·™†·å±‡®é‡ßºÁâßÁú§⁄©·õî‡´©·å±’∞Íßäﬁæ‡∞∫‘éÁâê‚Äù·ßó·Ä≥‡∞∫ÁúÖ÷¢⁄†·å±ﬁæﬁó›ó·•§Ëñ´‡¨´·ßÜ‡∏é·í´”£’à·ëçÁâß‘Ü’àÁçç·å±’à·áî‡≠µËô°‘ÖÁâ©‡∫ãË©ÇÁâê
„ÄåÂìêÂìêÔºÅ„ÄçÂèàÊòØ‰∏§ËÆ∞Âìç‰∫ÆÁöÑËÄ≥ÂÖâÔºåÊûóÈ¶ôÂ∑≤ÁªèË¢´Ê¨∫Ë¥üÂæóËøûÁ´ôËµ∑Êù•ÁöÑÂäõÊ∞îÈÉΩÊ≤°Êúâ‰∫Ü„ÄÇÁ®ãÊòü‰ªñ‰ª¨‰∏ÄËµ∞ÔºåÂ•π‰æøÁò´Âú®Âú∞Êùø‰∏äÔºåÁúºÁ•ûÁ©∫Ê¥ûÂú∞ÁúãÁùÄÊïôÂÆ§ÁöÑÂ§©Ëä±Êùø„ÄÇ„ÄåÁúüÂèØÁ¨ë‚Ä¶‚Ä¶Êàë‰∏çËøáÂ∞±ÊòØ‰∏çÊÉ≥ÂèÇÂä†ÈÇ£‰∏™Ê¥ªÂä®ÔºåÂ∞±ÈÅ≠ÂèóËøôÊ†∑ÁöÑÈú∏Âáå„ÄÇ„Äç„Äå‰ªñ‰ª¨ÊÄé‰πàËøô‰πàÊÅ∂ÊØíÂïäÔºÅ„Äç„ÄåÊòéÊòéÊàëÊâçÊòØÂèóÂÆ≥ËÄÖÂïäÔºÅ„ÄçÊûóÈ¶ôÁöÑÂøÉÈáåÔºåÁ™ÅÁÑ∂ÂçáËµ∑‰∏ÄËÇ°Â•áÊÄ™ÁöÑÊÉÖÁª™„ÄÇ‰ªø‰Ωõ‰∏ã‰∏ÄÁßíÔºåÂ•πÂ∞±ÂèØ‰ª•ÂéªÊ≠ª‰∫Ü„ÄÇÂ•π‰∏çÁü•ÈÅìËá™Â∑±ÊòØÂ¶Ç‰ΩïÂõûÂà∞ÂÆ∂ÁöÑÔºåÊÄª‰πãÂà∞ÂÆ∂ÁöÑÊó∂ÂÄôÔºåÂ∑≤ÁªèÊòØÊôö‰∏ä„ÄÇÂõ†‰∏∫Ë∫´‰∏äÈÉΩÊòØ‰º§ÔºåÂ•πËøûÊæ°ÈÉΩÊ≤°Ê¥óÔºåÂ∞±Ê≤âÊ≤âÂú∞Áù°ÁùÄ‰∫Ü„ÄÇËøô‰∏ÄÁù°ÔºåÂ∞±Âà∞‰∫ÜÂáåÊô®‰∏âÁÇπ„ÄÇÂ•πË¢´È•øÈÜíÂêéÔºåÁ™ÅÁÑ∂ÊÉ≥Ëµ∑‰∫ÜÂ•πËøòÊúâ‰∏Ä‰∏™ÁΩëÂèãÔºåÂè´„ÄåÂ≠§Êòü„ÄçÔºå‰∏Ä‰∏™ÂÖ≥ÂøÉÂ•πÂà∞È™®Â≠êÈáåÁöÑ‰∫∫„ÄÇÂ•πÊâìÂºÄÊâãÊú∫ÔºåÁäπË±´‰∏ÄÈòµÂêéÔºåÁªô‰ªñÂèëÂéª‰∫ÜÊ∂àÊÅØ„ÄÇ„ÄåÊàë‚Ä¶‚Ä¶ÂèØ‰ª•ÈóÆ‰Ω†‰∏™ÈóÆÈ¢òÂêóÔºü„ÄçÂ≠§ÊòüÂá†‰πéÁßíÂõûÔºö„Äå‰ªÄ‰πàÔºü„Äç„Äå‚Ä¶‚Ä¶Âú®ÂêóÔºü„Äç‰ªñÂ•ΩÂÉèÊúâ‰∫õ‰∏çËÄêÁÉ¶‰∫ÜÔºö„ÄåÂà´ÈóÆ‰∫õÊúâÁöÑÊ≤°ÁöÑÔºåÁõ¥Êé•ËØ¥‰∫ã„ÄÇ„ÄçÂ•πÁäπË±´ÂÜç‰∏âÔºåÈºìË∂≥ÂãáÊ∞îÂõûÂ§çÈÅìÔºö„Äå‰Ω†‚Ä¶‚Ä¶‰ºöÂõ†‰∏∫Âà´‰∫∫ÁöÑÂ§ñË≤åÂíåËÉåÊôØËÆ®ÂéåÂ•πÂêóÔºü„ÄçËøá‰∫ÜÂæà‰πÖÔºå‰ªñÊ≤°ÊúâÂõûÂ§ç„ÄÇÂ•πÁ≠â‰∫ÜÂ•Ω‰πÖÔºå‰ªñÊâçÂõûÂ§çÂ•π„ÄÇ„Äå‰∏∫‰ªÄ‰πàËøô‰πàÈóÆÔºü„ÄçÂ•π‰∏ÄÂí¨ÁâôÔºåË±ÅÂá∫Âéª‰∫ÜÔºåÂ∞±ÁÆóÊòØÂàÜÊâãÔºåÂ•π‰πüË¶ÅÈóÆ‰∏™ÊòéÁôΩÔºÅ[...]26

--- PAGE 27 ---
ÁöáÂÆ´ÁπÅÂçéÁöÑ‰∏≠ÂøÉÔºåË¥µÂ¶ÉÂøµÂøµÁöÑÂØùÂÆ´„ÄÇÂ§úÂÖÅÔºåËøô‰ΩçÂ§©‰∏ãÊéåÊùÉÁöÑÁöáÂ∏ùÔºåÂú®ÈÄâÂ´°Â¶ÉÂâç‰∏ÄÂ§úÂÖâ‰∏¥‰∫ÜÂ•πÁöÑÂçßÂÆ§„ÄÇÂ§úÊ∑±‰∫∫ÈùôÔºåÂç≥‰ΩøÊòØËøô‰Ωç‰∏á‰∫∫‰πã‰∏äÁöÑÁöáÂ∏ùÔºåÊ≠§Âàª‰πüÁ•ûËâ≤ÈÜâ‰∫∫„ÄÇ"ÂøµÂøµÔºå‰Ω†Ë∏èË∂≥ÂÆ´Âª∑‰πãÂàùÔºåÊàëÂ∞±Ê≥®ÊÑè‰Ω†‰∫Ü„ÄÇ"‰ªñËæπ‰ΩéÂêüËæπÊãâÁùÄÂøµÂøµÁöÑÊâã„ÄÇ"Èôõ‰∏ãÔºåÊàë‚Ä¶‚Ä¶"ÂøµÂøµÊÉ≥ËØ¥‰∫õ‰ªÄ‰πàÔºå‰ΩÜÊòØ‰ªñÊâ∂ÁùÄÂ•πÁöÑÊâãËáÇÔºåÂ•π‰æøÊîæ‰∏ã‰∫ÜÂøÉ‰∏≠ÁöÑÁñëÈóÆÔºå‰ªªÁî±‰ªñÂºïÈ¢ÜÔºåÁº†ÁªµÂú®Â•πÁöÑÊ¢¶‰∏≠„ÄÇÂ§úÊ∑±ÔºåÂ§úÂÖÅÈÖíÈÖ£„ÄÅÁ•ûËâ≤Êô¥Êúó„ÄÇÂè™ÊòØ‰ªñÁöÑÊâãÊ≥ï‰∏çÂ§üÁÜüÁªÉÔºå‰ΩøÂæóËøôÂú∫‰πÖËøùÁöÑ‰∫≤ÊòµÂ§öÂ∞ëÊúâ‰∫õÁîüÁñèÔºå‰ΩÜÂøµÂøµÂç¥‰∏çËàçÂàÜÁ¶ªÔºåÂÆ≥ÊÄï‰∏ÄÊó¶ÈáäÊîæÔºåËøô‰ªΩÂæÆÂ¶ôÁöÑÊÉÖÊÑ´Â∞Ü‰ºöÊ∂àÊï£Âú®È£é‰∏≠„ÄÇÊ¨°Êó•ÁôΩÊòºÔºåÂ§úÂÖÅÈ°æÂèäÊúùÊîøÔºåË¥µÂ¶ÉÂøµÂøµÂç¥Âõ†Â§úÈó¥ÁöÑÁ∫†Áº†ÔºåÊÑüËßâË∫´‰ΩìÊúâ‰∫õ‰∏çÈÄÇÔºå‰ΩÜÊòØ‰ªäÂ§©ÊòØÈÄâÂ´°Â¶ÉÁöÑÂ§ßÊó•ÔºåÂ•π‰∏çËÉΩËÄΩÊêÅ„ÄÇÂøµÂøµÊî∂ÊãæÂ•ΩËá™Â∑±ÔºåÂèÇ‰∏éËøõÂ§ßÈÄâÁöÑË°åÂàó„ÄÇÂ§ßÈÄâ‰∏äÔºåÂ•πÈÄâ‰∏≠‰∫ÜÊùéÂÆ∂ÁöÑÂ´°Â•≥ÔºåÁ¨ëÂÆπÂ¶ÇËä±ÔºåËôΩÁÑ∂ÈïøÁõ∏‰∏≠Á≠âÔºåÂç¥ËÆ©‰∫∫ÊÑüÂà∞ÁâπÂà´ÂñúÊ¨¢„ÄÇÊúâ‰∫õ‰∫∫ÁîüÊù•Â∞±‰ºöÊï£ÂèëÂá∫Âñú‰πêÁöÑÊ∞îÊÅØÔºå‰Ωø‰ªñ‰∫∫ÂøÉÁîüÊÑøÊÑèÔºåÂ•πÂ∞±ÊòØËøôÊ†∑ÁöÑ‰∫∫„ÄÇ[...]ÁúÖÈÖÑ‡Æë·å±·íç·úì◊û‡Ø≥‡¶á”•Áâ©‡•õ‡£Åÿ±‡•üÈâùÁçÆ”û‡•†‡∏†”§‡π∂‡≤§‡±ØÁâß‡±Ø’™›é·äû‘ß”ûÁäö‘™ÁúêÁâêÁä¢ÁêäË†ú‘ßÁâß÷ïÁä≤‘íÈâï·ÄåÁ£™Á≤¨⁄¶Ë†úÁâßÁä¢Ëöé·¨∞‡π∂‡∑∏·íû‡Æë·ç•‡≥∂Áâß‡±Ø’™Áüëﬁó‘ßÁâß÷ï‡±ØÁú§·•ßÁä¢Áäã‡•°·Üß·ïûÁâê·í´‘´‡•†Áâß‡±ØÁú§⁄©Ëõ™÷õÁäãËùíÁâß÷ïÈâï‡∏é›áÁäå‘ß‡•üÈâùÁâê‡±ØÈâù”æ‘ßÁ£∑Áñë·å±‡ßØÁ©á÷¢‘ÖÁä°‡¨ô·å±Á∫®Á©áÁâßÁîá·≥©·çò”æÁºõÁâß÷ï·ÜΩ·íûÁâß·ï≥’à‡Ωª‡°Ö·å±Áú§·•ßÁâê‡•üÈâù·ïÆ‡π≥›∏Áâß‡•õÿ±‡π∂‡≤§‡±ØÁâß‡±Ø’™Á£™”û‡æß‡©í·¶æÁâêÁä¢·ëç·©©‡±ØËú¢Ëàö‘ßÁâß‡±Ø‡¢ß‡¨´·ßîÁõì‡¶àÁäñËú¢Ëàö‘ßÁâêÁõì‡¶à‡µâ⁄©‡±ØÈâù’àÁõÑŸµÁâß‡±Ø·≥ØÁîá’ã‘ç‡∞∫‡ØèÁâßÁîá‹©·ÄåÁ£™‡¢ß·íºÁâêÁâê·àø‡£ÅÁâß·ßó‡£ÅÁ®Æ‡§ö·èê”§·¨∞”û‡æç‡∫Ö‡≠åÁ°≤‘™ÁâßŸâ·Ä≥Áúê·úì·¨≥·®Ω‡ØîÁäå‡∑à‡®ç‡Øî·®∂·å±‡µâ‹ãÁâßÁπäÁãí‡≤ÖŸñ÷¢⁄†‡®ªËÉº‡•úÁèÖ‡≠ö·ßõ·òè·ñÄ·ñÖËùç·ßõÁâê·ßó·Ä≥‡∞∫ÁúÖ÷¢⁄†·å±ﬁæﬁó›ó·•§Ëñ´‡¨´·ßÜ‡∏é·í´”£’à·ëçÁâßŸµ‡•ìÁ†∞Ë±î‡≠ö·∂æÁ°≤‘™·å±‡≠èË≥ªÁâê
Â§úÂÖÅÂ§ßÈÄâÂâç‰∏ÄÂ§©Êôö‰∏äÊù•Êâæ‰∫ÜÂ•π„ÄÇËøô‰∏™Êó∂Èó¥ÁÇπÊù•ÂèØ‰∏ç‰∏ÄËà¨ÔºåÂ•πÁåú‰∏çÂáÜ‰ªñÊÉ≥Ê≥ïÔºå‰πüÊ≤°Â§öËØ¥‰ªÄ‰πàÔºåÂÉèÂæÄÂ∏∏ÈÇ£Ê†∑Áªô‰ªñÊñüÈÖíÔºåÁªô‰ªñÂàáÊ∞¥ÊûúÔºå‰ªñÂñùÂ§ö‰∫ÜÔºå‰ΩÜ‰πüÊ≤°Â§™ÈÜâÔºåÂè™ÊòØËµ∞Ë∑ØÊôÉËç°ÔºåÁ¨ëÂæóÊúâ‰∫õÁúüÊåö„ÄÇ‰ªñÈóÆËá™Â∑±‰∏∫‰ªÄ‰πàËøòÊòØÈÇ£‰πàÂ•ΩÁúã„ÄÇÂ•ΩÁúãÂà∞ËÆ©Â•πËßâÂæó‰ªñ‰∏çÂÉèÊòØËá™Â∑±ÁöÑ‰∏àÂ§´„ÄÇ‰ªñË∫´‰∏äÊúâÈÖíÂë≥ÔºåÂ•π‰∏çÂéåÊÅ∂ÔºåÂèçËÄåËßâÂæóËøôÈÖíÂë≥Êõ¥Âãæ‰∫∫ÂøÉÂº¶Ôºå‰ªñËµ∞ËøëÂ•πÁöÑÊó∂ÂÄôÔºåÂ•πÂøÉË∑≥Â∞±‰ºöËé´ÂêçÂä†ÈÄüÔºåËøôÊòØÂ•πÂØπ‰ªñÊó†Ê≥ïÂéãÊäëÁöÑÂñúÊ¨¢„ÄÇÂ•πÊÉ≥Ôºå‰ªñ‰ªäÂ§©‰∏ç‰ºöÊòØÊïÖÊÑèÂØπÂ•πÂñùÈÖíÂêß„ÄÇ‰∏çÁÑ∂‰ªñÊÄé‰πàËµ∞‰∏çÁ®≥ÁöÑ„ÄÇÊòéÊòéÔºå‰ªñÂæÄÊó•ÈáåÂñù‰∫ÜÈÇ£‰πàÂ§öÈÖíÈÉΩÂ•ΩÂ•ΩÁöÑ„ÄÇÂ•πËßâÂæó‰ªñÁ¶ªÂ•πÂ§™Ëøë‰∫ÜÔºåÂèØÂ•π‰πüËàç‰∏çÂæóÊé®ÂºÄ‰ªñ„ÄÇ‰ªñË∫´‰∏äÂ•ΩÈ¶ôÔºåÂ•πÂøç‰∏ç‰ΩèÊääËÑ∏Ë¥¥Ëøë‰ªñ„ÄÇÂ•π‰ª•‰∏∫‰ªñ‰ºöÂ¶ÇÂæÄÂ∏∏ÈÇ£Ê†∑ÈÅøÂºÄÁöÑ‚Äî‚ÄîÂÖ∂ÂÆûÔºåÂ•πËßâÂæó‰ªñÂ∫îËØ•‰∏çÂñúÊ¨¢Âà´‰∫∫Èù†‰ªñÂ§™ËøëÔºåÂ•π‰ª•‰∏∫‰ªñÂè™ÊòØ‰∏çÂñúÊ¨¢ËøôÊ†∑ËÄåÂ∑≤„ÄÇ‰ΩÜÂ•πÊÉ≥Èîô‰∫ÜÔºå‰ªñÂ•ΩÂÉèÊúâ‰∫õ‰∏çÂ§™ÁÜüÁªÉ‰ººÁöÑÔºåËÑ∏È¢äËÇâË¢´Â•πËøô‰πà‰∏ÄËπ≠ÔºåÊúâ‰∫õÁóíÁóíÁöÑÔºåËøû‰ªñËá™Â∑±ÈÉΩ‰ΩéÂ£∞Á¨ë‰∫Ü„ÄÇ‰πãÂêéÁöÑ‰∫ãËá™ÁÑ∂È°∫ÁêÜÊàêÁ´†„ÄÇÂèØÊ¨°Êó•Â•πÈÜíÊù•ÁöÑÊó∂ÂÄôÔºåÂ§¥ÂæàÁóõ„ÄÇÂ•πÂèëÁé∞Ëá™Â∑±ËøòÊòØÊ≤°ËÉΩÂøç‰ΩèÔºåÂ•πÂØπÂ§úÂÖÅËøòÊòØË¥™ÂøÉÁöÑ„ÄÇÂ•π‰∏çÊÉ≥Ë¶Å‰ªñÁöÑÂñúÊ¨¢Ôºå‰ªñÁªôÂ•πÁöÑÂ∑≤ÁªèË∂≥Â§üËÆ©Â•πÂøÉÁîòÊÉÖÊÑø‰∏ÄËæàÂ≠êÂæÖÂú®ÁöáÂÆ´‰∫Ü„ÄÇÂ•πÂè™ÊòØÂ∏åÊúõÔºå‰ªñËÉΩÁ®çÂæÆ‚Ä¶‚Ä¶Á®çÂæÆÂñúÊ¨¢‰∏ÄÁÇπÂ•π„ÄÇÂ•πÁü•ÈÅìËøô‰∏™ÊÉ≥Ê≥ïÂæàÂÇª‰πüÂæàÂ§©ÁúüÔºåÂèØÂ•πÂ∞±ÊòØ‰ºöÂ∏åÊúõ„ÄÇÂ•πÊÉ≥ÂÜçË¥™ÂøÉÁÇπ„ÄÇÂ•πÈÄâ‰∏≠‰∫ÜÊùéÂÆ∂ÁöÑÂ´°Â•≥Ôºå‰∏Ä‰∏™ÈïøÁõ∏‰∏≠Á≠âÔºå‰ΩÜÊòØÁà±Á¨ëÔºåËÆ©‰∫∫ËßâÂæóÊ¨¢ÂñúÁöÑ‰∫∫ÔºåËá≥‰∫éÂÖ∂‰ªñ‰∫∫ÔºåÂ•πÂ∞±ÁúãÁöá‰∏äÁöÑÊÑèÊÄù‰∫Ü„ÄÇ27
