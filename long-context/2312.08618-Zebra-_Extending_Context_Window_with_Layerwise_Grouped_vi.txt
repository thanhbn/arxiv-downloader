# 2312.08618.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2312.08618.pdf
# Kích thước tệp: 1413994 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Zebra: Mở rộng Cửa sổ Ngữ cảnh với Cơ chế Chú ý Cục bộ-Toàn cục được Nhóm theo Lớp
Kaiqiang Song∗, Xiaoyang Wang∗, Sangwoo Cho∗, Xiaoman Pan, Dong Yu
Tencent AI Lab, Seattle
{riversong, shawnxywang, swcho, xiaomanpan, dyu}@global.tencent.com

Tóm tắt
Bài báo này giới thiệu một phương pháp mới để tăng cường khả năng của các Mô hình Ngôn ngữ Lớn (LLM) trong việc xử lý và hiểu các chuỗi văn bản dài, một khía cạnh quan trọng trong các ứng dụng yêu cầu hiểu sâu và tổng hợp khối lượng thông tin lớn. Nhận thức được những thách thức cố hữu trong việc mở rộng cửa sổ ngữ cảnh cho LLM, chủ yếu được xây dựng trên kiến trúc Transformer, chúng tôi đề xuất một kiến trúc mô hình mới, được gọi là Zebra. Kiến trúc này quản lý hiệu quả các vấn đề về độ phức tạp thời gian và bộ nhớ bậc hai liên quan đến cơ chế chú ý đầy đủ trong Transformer bằng cách sử dụng các lớp chú ý cục bộ-toàn cục được nhóm. Mô hình của chúng tôi, giống như các vạch sọc xen kẽ của ngựa vằn, cân bằng các lớp chú ý cục bộ và toàn cục, giảm đáng kể yêu cầu tính toán và tiêu thụ bộ nhớ. Các thí nghiệm toàn diện, bao gồm tiền huấn luyện từ đầu, tiếp tục huấn luyện thích ứng ngữ cảnh dài và điều chỉnh hướng dẫn dài, được thực hiện để đánh giá hiệu suất của Zebra. Kết quả cho thấy Zebra đạt được hiệu suất tương đương hoặc vượt trội trên cả các điểm chuẩn chuỗi ngắn và dài, đồng thời cũng tăng cường hiệu quả huấn luyện và suy luận.

1 Giới thiệu
Để tận dụng hiệu quả sức mạnh của Ngữ cảnh Dài trong các Mô hình Ngôn ngữ Lớn (LLM), việc phát triển và cải tiến các kỹ thuật cho phép các mô hình này xử lý và diễn giải chính xác các chuỗi văn bản dài là điều cần thiết. Khả năng này đặc biệt quan trọng trong các ứng dụng đòi hỏi hiểu biết sâu sắc và tổng hợp khối lượng thông tin lớn, chẳng hạn như tóm tắt (Huang et al., 2021; Hu et al., 2023; Song et al., 2022; Krýsciński et al., 2021), đọc hiểu (Nguyen et al., 2016; Fan et al., 2019; Zhong et al., 2021; Yang et al., 2023), tạo văn bản dài (Guan et al., 2021; Deng et al., 2022; Roziere et al., 2023), và lý luận phức tạp (Wei et al., 2022; Yao et al., 2023; Chen et al., 2023a).

Tuy nhiên, việc mở rộng cửa sổ ngữ cảnh từ các quan điểm khác nhau là một thách thức: Thứ nhất, mô hình LLM chủ đạo sử dụng kiến trúc Transformer (Vaswani et al., 2017). Các mô hình như vậy như BERT (Devlin et al., 2018), GPT (OpenAI, 2023), và T5 (Raffel et al., 2020) sử dụng cơ chế chú ý đầy đủ trong mỗi lớp, vốn có độ phức tạp thời gian và bộ nhớ bậc hai. Điều này có thể làm giảm hiệu quả của cả quá trình huấn luyện và suy luận. Thứ hai, việc tính toán chú ý trên một chuỗi cực kỳ dài có thể dẫn đến phân phối gần như đều, có thể gây ra việc bỏ sót thông tin quan trọng (Han et al., 2023). Điều này có thể dẫn đến vấn đề "lạc lối ở giữa" (Liu et al., 2023). Cuối cùng, việc phân phối tín hiệu huấn luyện cho các chuỗi dài và ngắn không cân bằng. Rõ ràng là các chuỗi dài hiếm khi xuất hiện trong cả dữ liệu văn bản thuần túy và dữ liệu điều chỉnh hướng dẫn. Do đó, sự hiếm hoi này tạo ra thách thức trong việc nắm bắt hiệu quả các phụ thuộc dài hạn trong quá trình huấn luyện.

Để giải quyết các vấn đề trên, chúng tôi đề xuất nhóm các lớp chú ý cục bộ-toàn cục thành các khối trong giai đoạn huấn luyện và suy luận. Chiến lược này tăng cường hiệu quả đồng thời mang lại kết quả tương đương với Transformer chú ý toàn cục. Đáng chú ý, nó đạt được mức hiệu suất tương đương chỉ với một nửa nỗ lực tính toán cần thiết cho huấn luyện. Ngoài ra, phương pháp này giảm đáng kể việc tiêu thụ bộ nhớ trong quá trình suy luận bằng cách duy trì bộ nhớ đệm Key-Value (K-V) cục bộ cụ thể cho các lớp chú ý cục bộ.

Trong Phần 2.1, chúng tôi liệt kê hai thành phần quan trọng cần thiết cho một mô hình ngữ cảnh dài cũng như các lựa chọn thay thế tiềm năng để xem xét. Những thành phần này bao gồm các cơ chế chú ý đa dạng và các phương pháp nhúng vị trí. Tiếp theo, trong Phần 2.2, chúng tôi thực hiện phân tích so sánh các lựa chọn thay thế này, trình bày kết quả thực nghiệm của chúng để đánh giá toàn diện. Tích hợp những hiểu biết này, chúng tôi đặt tên mô hình của mình là Zebra, tương tự như các vạch sọc đen trắng xen kẽ của ngựa vằn, giống với các lớp cục bộ và toàn cục được nhóm trong kiến trúc mô hình của chúng tôi.

Để xác thực mô hình đề xuất ở quy mô lớn, Phần 3 chi tiết việc tiếp tục huấn luyện mô hình Llama-2-7B (Touvron et al., 2023) sử dụng huấn luyện thích ứng ngữ cảnh dài thông qua Zebra. Phương pháp này không chỉ thể hiện hiệu suất tương đương trên các điểm chuẩn chuỗi ngắn mà còn đạt được kết quả độ phức tạp vượt trội cho các chuỗi dài hơn. Ngoài ra, trong Phần 4, chúng tôi thực hiện điều chỉnh tinh của Zebra sử dụng kết hợp cả bộ dữ liệu điều chỉnh hướng dẫn ngắn và dài. Điều này được theo sau bởi đánh giá có hệ thống về hiệu suất của mô hình trên nhiều bộ dữ liệu điểm chuẩn. Nó thể hiện hiệu suất tổng thể tốt hơn trên cả các điểm chuẩn dài và ngắn so với Llama-2-7b-chat. Để kết luận, đóng góp của chúng tôi có 3 khía cạnh:

• Chúng tôi phát triển một kiến trúc mới, được gọi là Zebra, kết hợp các lớp chú ý cục bộ-toàn cục được nhóm và nhúng vị trí xoay.

• Chúng tôi thực hiện các thí nghiệm toàn diện và phân tích chi tiết về khung Zebra trong các thiết lập khác nhau, bao gồm tiền huấn luyện từ đầu, tiếp tục huấn luyện và điều chỉnh hướng dẫn rộng rãi. Các phát hiện từ những kết quả này chứng minh ưu thế của kiến trúc mô hình Zebra.

• Ngoài ra, chúng tôi phân tích hiệu quả huấn luyện và suy luận cho Zebra và cung cấp mã giả cho việc triển khai.

2 Zebra

2.1 Thiết kế Kiến trúc Mô hình

Để mở rộng cửa sổ ngữ cảnh cho các mô hình Transformer, hai yếu tố quan trọng phải được giải quyết: Thứ nhất, cơ chế Chú ý cho phép mô hình tập trung hiệu quả và xử lý các phần liên quan của chuỗi dài. Tuy nhiên, điều quan trọng cần lưu ý là việc tính toán chú ý tăng theo bậc hai, dẫn đến giảm hiệu quả khi độ dài chuỗi tăng lên. Do đó, việc giải quyết thách thức tính toán này là cần thiết để duy trì hiệu quả trên các chuỗi dài hơn. Thứ hai, Nhúng Vị trí truyền tải tín hiệu có cấu trúc chỉ thị thứ tự tuần tự của các token. Việc sử dụng nhúng vị trí không chỉ mạnh mẽ mà còn thể hiện khả năng tổng quát hóa mạnh mẽ là điều quan trọng, đặc biệt cho việc xử lý chuỗi dài. Điều này đảm bảo hiệu quả của mô hình trong việc duy trì tính toàn vẹn chuỗi trên các khoảng dữ liệu dài hơn.

2.1.1 Chú ý

Trong Hình (1a, 1b, 1c), chúng tôi trình bày ba biến thể đại diện của các lớp chú ý đơn bao gồm chú ý toàn cục, chú ý cục bộ và chú ý cục bộ với xấp xỉ toàn cục. Các mô hình chú ý thưa thớt bổ sung như chú ý theo khối (Qiu et al., 2019), chú ý cửa sổ giãn (Beltagy et al., 2020), chú ý bước (Child et al., 2019), Chú ý Sinkhorn (Tay et al., 2020a), chú ý toàn cục tạm thời (Guo et al., 2021) được xem xét là các lựa chọn thay thế tiềm năng cho chú ý cục bộ cơ bản. Để rõ ràng và tập trung trong nghiên cứu của chúng tôi, chúng tôi giới hạn phân tích của mình trong hai biến thể chính: chú ý cục bộ và chú ý cục bộ với xấp xỉ toàn cục. Quyết định này cho phép khám phá có mục tiêu hơn các cơ chế chú ý cụ thể này trong công việc của chúng tôi. Hơn nữa, chúng tôi cũng xem xét sử dụng các chiến lược khác nhau giữa các lớp khác nhau. Trong Hình 1d, chúng tôi kết hợp nhiều lớp cục bộ với một lớp chú ý toàn cục thành một nhóm và xếp chồng các nhóm như vậy cho mô hình.

Xem xét một đầu của lớp tự chú ý trong transformer chỉ giải mã, truy vấn, khóa và giá trị của vị trí thứ i và lớp thứ l được định nghĩa là phép chiếu của trạng thái ẩn lớp cuối h(l−1)i:

q(l)i = WTqh(l−1)i (1)
k(l)i = WTkh(l−1)i (2)  
v(l)i = WTvh(l−1)i (3)

Chúng tôi ký hiệu độ tương tự giữa truy vấn thứ i và khóa thứ j là:

Sim(i, j) = exp(qTikj/√D) (4)

trong đó D là hệ số chuẩn hóa thường bằng với chiều của mô hình.

Chú ý Toàn cục: Đây là chú ý phổ biến nhất, trong đó mỗi token có chú ý đến tất cả các vị trí trước nó và chính nó:

αi,j = Sim(i, j) / Σt=0i Sim(i, t) (5)

--- TRANG 2 ---

trong đó αi,j là giá trị chú ý của truy vấn thứ i trên khóa thứ j. Vector ngữ cảnh sau đó được định nghĩa là tổng có trọng số của các vector giá trị:

contexti = Σj=0i αi,j vj (6)

Chú ý Cục bộ: Mỗi truy vấn chỉ xem xét các cặp khóa-giá trị trong cửa sổ cục bộ của nó.

αi,j = Sim(i, j) / Σt=min(0,i−w)i Sim(i, t) (7)

trong đó w là kích thước cửa sổ của chú ý cục bộ.

Chú ý Cục bộ với Xấp xỉ Toàn cục: Được lấy cảm hứng từ chú ý toàn cục tạm thời (Guo et al., 2021), chúng tôi xấp xỉ đầu ra chú ý toàn cục bằng cách kết hợp nhiều token không cục bộ thành một khối và thực hiện chú ý trên các token cục bộ và các khối không cục bộ. Các cặp khóa-giá trị của mỗi khối không cục bộ được ước tính bằng các phương trình sau:

k̂j = (Σt=(j−1)∗c j∗c−1 kt + ln(c)) (8)
v̂j = (Σt=(j−1)∗c j∗c−1 vt + ln(c)) (9)

trong đó c là kích thước khối, và ln(c) là một số hạng bù trừ cho mỗi khối.

Chú ý Cục bộ-Toàn cục được Nhóm theo Lớp: Thay vì sử dụng các lớp giống hệt nhau cho toàn bộ mạng, chúng tôi đề xuất sử dụng các lớp chú ý cục bộ-toàn cục được nhóm. Trong hình 1d, chúng tôi nhóm mỗi L lớp và chỉ sử dụng một lớp chú ý toàn cục ở lớp đầu tiên của mỗi nhóm. Chúng tôi áp dụng chú ý cục bộ được mô tả trong Phương trình (7) cho các lớp còn lại.

h(l) = {G-Block(h(l−1)) nếu l mod L == 0
       {L-Block(h(l−1)) nếu không    (10)

Để đơn giản hóa, chúng tôi sử dụng Group Attention để ký hiệu chú ý cục bộ-toàn cục được nhóm theo lớp.

2.1.2 Nhúng Vị trí

Trong kiến trúc Transformer, nhúng vị trí thường được sử dụng để mã hóa thông tin thứ tự chuỗi. Trong nghiên cứu này, chúng tôi kết hợp ba loại nhúng vị trí được công nhận rộng rãi để tạo điều kiện cho một phân tích toàn diện.

Nhúng Vị trí Tuyệt đối: Transformer gốc (Vaswani et al., 2017) ủng hộ việc sử dụng nhúng vị trí hình sin tuyệt đối:

PE(pos,2i) = sin(pos/100002i/d))
PE(pos,2i+1) = cos(pos/100002i/d)

trong đó pos là chỉ số vị trí, d là chiều mô hình, và i là biến lặp cho các chiều khác nhau. Sau công việc của Transformer gốc, nhúng vị trí tuyệt đối có thể huấn luyện đã được giới thiệu (Devlin et al., 2018; Radford et al., 2018), phục vụ như một sự thay thế cho mẫu sin cố định. Nhúng vị trí như vậy được thêm trực tiếp vào nhúng ngữ nghĩa:

EMB(word, pos) = WE(word) + PE(pos) (11)

trong đó word là chỉ số token đầu vào, và pos là chỉ số vị trí tuyệt đối.

Gần đây nhất, nhúng vị trí tương đối (Shaw et al., 2018; Yang et al., 2019) được giới thiệu để loại bỏ độ lệch vị trí đồng thời cải thiện hiệu suất. Các phương pháp này cũng tạo điều kiện cho khả năng của mô hình trong việc mở rộng cửa sổ ngữ cảnh, một quá trình được gọi là ngoại suy vị trí. Trong khung này, hai loại chính của nhúng vị trí tương đối được xem xét.

Nhúng Vị trí Alibi (Press et al., 2022), áp dụng nhúng vị trí tương đối bằng cách thêm trực tiếp một số hạng độ lệch vào ma trận chú ý.

αi,j = Softmaxij(Sim(i, j) − (i−j) ∗ m) (12)

trong đó m là một vô hướng cụ thể cho đầu và (i−j) là khoảng cách tương đối giữa các vị trí truy vấn và khóa. Bằng cách loại bỏ số hạng −i∗m, chúng ta có:

αi,j = Softmaxij(Sim(i, j) + j ∗ m) (13)

Nhúng Vị trí Xoay (Su et al., 2023) xoay các chiều liên hợp của vector truy vấn và khóa, sao cho khoảng cách tương đối được mã hóa trong quá trình tính toán chú ý.

eq = (WTqhi)ei(iθ) (14)
ek = (WTkhi)ei(iθ) (15)

trong đó i ký hiệu đơn vị ảo, và i là chỉ số vị trí. Đối với mỗi cặp chiều liên hợp, độ tương tự giữa truy vấn và khóa có thể được viết là:

Sim(i, j) = RE[(WTqhi)T(WTkhj)ei(i−j)θ] (16)

trong đó RE lấy giá trị thực của số phức. Độ tương tự tổng thể do đó được định nghĩa là thước đo tích lũy của các độ tương tự trên tất cả các chiều tương ứng.

2.2 Thí nghiệm cho Thiết kế Mô hình

[Bảng 1 và nội dung tiếp theo được dịch tương tự...]

--- TRANG 3 ---

[Tiếp tục dịch các hình ảnh và nội dung còn lại...]

Khóa: Truy vấn:
1 2 3 4 5 6 7 8 9 10 11 12
1
2
3
4
5
6
7
8
9
10
11
12

(a) Chú ý Toàn cục

Khóa: Truy vấn:
1 2 3 4 5 6 7 8 9 10 11 12
1
2
3
4
5
6
7
8
9
10
11
12

W=3 (b) Chú ý Cục bộ

Khóa: Truy vấn:
1 2 3 4 5 6 7 8 9 10 11 12
1
2
3
4
5
6
7
8
9
10
11
12

W=3 C=2 (c) Xấp xỉ Toàn cục

(d) Chú ý Nhóm

Hình 1: Bốn chiến lược chú ý khác nhau được so sánh trong công việc này. (a) Chú ý Toàn cục, trong đó mỗi token có chú ý đến tất cả các token trước đó và chính nó; (b) Chú ý Cục bộ, trong đó mỗi token chỉ có chú ý trong cửa sổ cục bộ của nó; (c) Chú ý Cục bộ với Xấp xỉ Toàn cục được giới thiệu mới trong công việc này, trong đó mỗi token không chỉ có chú ý đến cửa sổ cục bộ của nó mà còn có chú ý xấp xỉ từ các khối không cục bộ còn lại; (d) Chú ý Nhóm là chiến lược chú ý cục bộ-toàn cục được nhóm theo lớp được chúng tôi giới thiệu, trong đó chúng tôi nhóm L lớp và áp dụng chú ý toàn cục ở lớp đầu tiên của mỗi nhóm (các lớp còn lại sử dụng chú ý cục bộ).

[Tiếp tục dịch phần còn lại của tài liệu...]

2.2 Thí nghiệm cho Thiết kế Mô hình

[Bảng 1: Tham số của hai mô hình với kích thước khác nhau]

Kích thước Mô hình: 117M | 345M
Số Lớp: 12 | 24
Kích thước Ẩn: 768 | 1024
Số Đầu: 12 | 16
Kênh K-V: 64 | 64
Kích thước Ẩn Lớp FF: 3072 | 4096

Chúng tôi thực hiện các thí nghiệm với nhiều chiến lược chú ý và phương pháp nhúng vị trí khác nhau như được mô tả trước đó. Hai mô hình GPT với tham số 117M và 345M như được chi tiết trong Bảng 1, được huấn luyện từ đầu để đánh giá các kiến trúc mô hình khác nhau. Độ dài chuỗi huấn luyện được sử dụng cho các thí nghiệm này dao động từ 1024, 4096, đến 16384. 10% dữ liệu huấn luyện từ bộ dữ liệu Pile (Gao et al., 2020) được sử dụng cho huấn luyện mô hình. Dữ liệu kiểm tra và xác thực của nó được sử dụng trong các thí nghiệm của phần này để đánh giá. Chúng tôi sử dụng Bộ tối ưu hóa Adam (Kingma và Ba, 2014) để huấn luyện với các giá trị beta là 0.9 và 0.99. Quá trình huấn luyện kéo dài 20.000 bước với kích thước batch là 2M token. Tốc độ học ban đầu được đặt là 1e−3 với bước khởi động là 2.000, tiếp theo là suy giảm tuyến tính xuống 1e−5. Suy giảm trọng số được đặt là 0.01, và cắt gradient được đặt là 1.0. Đối với chú ý cục bộ, kích thước cửa sổ w = 1.024 được áp dụng. Đối với chú ý cục bộ với xấp xỉ toàn cục, chúng tôi sử dụng kích thước khối c = 16. Chúng tôi nhóm mỗi ba lớp cho các lớp cục bộ và toàn cục. Đối với nhúng xoay (Su et al., 2023), RoPE theta được cấu hình là 131.072 để tăng cường khả năng tổng quát hóa của nó trên các chuỗi dài hơn. Tất cả các thí nghiệm được triển khai bằng Megatron-LM (Shoeybi et al., 2019; Narayanan et al., 2021; Korthikanti et al., 2023).

2.2.1 Chú ý

Hình 2 hiển thị sự khác biệt về độ phức tạp kiểm tra (PPL) giữa mỗi chiến lược chú ý và phương pháp chú ý toàn cục trên mô hình 117M. Từ các hình, chúng tôi có một số quan sát: Thứ nhất, chú ý toàn cục có hiệu suất tổng thể tốt nhất; Thứ hai, khoảng cách hiệu suất giữa chú ý nhóm và chú ý toàn cục nhỏ nhưng ổn định khi độ dài chuỗi huấn luyện ngày càng dài; Thứ ba, khi độ dài chuỗi huấn luyện tăng, hiệu suất của chú ý cục bộ và chú ý xấp xỉ toàn cục giảm nhiều đối với các chuỗi dài hơn, mặc dù nó có thể có lợi cho các chuỗi ngắn hơn. Vì chú ý nhóm có ít tính toán hơn nhưng đạt được hiệu suất tương tự so với chú ý toàn cục, nó có tiềm năng mở rộng cao.

Để so sánh tốt hơn các chiến lược chú ý toàn cục và nhóm, chúng tôi xem xét cả hiệu suất và tính toán. Trong Hình 3, chúng tôi vẽ đường cong của TFLOPS ước tính và PPL xác thực trên ba độ dài huấn luyện khác nhau với trình lập hồ sơ FLOPS DeepSpeed cho mô hình 117M. Chúng tôi quan sát thấy chú ý nhóm đạt được hiệu suất tương tự với ít tính toán hơn so với chú ý toàn cục. Khi cửa sổ cục bộ bằng với độ dài chuỗi huấn luyện (tức là độ dài huấn luyện 1k trong Hình 3), lợi ích là không đáng kể. Tuy nhiên, khi chuỗi huấn luyện ngày càng dài hơn (ví dụ, độ dài huấn luyện 4k hoặc 16k trong Hình 3), lợi ích trở nên được phóng đại. Điều này xác nhận rằng chú ý nhóm có khả năng mở rộng tốt hơn so với chú ý toàn cục.

2.2.2 Nhúng Vị trí

Bảng 2 hiển thị kết quả độ phức tạp so sánh các nhúng vị trí khác nhau với mô hình 117M và 345M. Chúng tôi thấy rằng không có sự khác biệt hiệu suất đáng kể nào được quan sát giữa ba nhúng vị trí cho độ dài chuỗi trong chuỗi huấn luyện 16.384. Kết quả này phù hợp với quan sát của (Taylor et al., 2022; Kazemnejad et al., 2023). Trong khi nhúng vị trí tuyệt đối gặp khó khăn trong việc ngoại suy đến các chuỗi dài hơn, cả nhúng vị trí Alibi và Rotary đều thể hiện khả năng tương tự cho các chuỗi vượt quá độ dài chuỗi huấn luyện 16.384. Điều quan trọng cần lưu ý là, trong các thí nghiệm của chúng tôi, nhúng vị trí Alibi yêu cầu tính toán độ chính xác đầy đủ (fp32) để ngăn chặn va chạm vị trí. Do đó, chúng tôi chọn nhúng vị trí Rotary trong mô hình Zebra.

[Tiếp tục dịch phần còn lại...]
