# Khám phá Sự Ngoại suy của Transformer
1Zhen Qin*,1Yiran Zhong*†,2Hui Deng
1OpenNLPLab, Phòng thí nghiệm Trí tuệ nhân tạo Thượng Hải,2Đại học Bách khoa Tây Bắc
https://github.com/OpenNLPLab/Rpe
Tóm tắt
Sự ngoại suy độ dài đã thu hút sự chú ý đáng kể gần đây vì nó cho phép các transformer được kiểm tra trên các chuỗi dài hơn so với những chuỗi được sử dụng trong quá trình huấn luyện. Nghiên cứu trước đây đã chỉ ra rằng thuộc tính này có thể đạt được bằng cách sử dụng các Mã hóa Vị trí Tương đối (RPE) được thiết kế cẩn thận. Mặc dù các phương pháp này hoạt động tốt trên nhiều loại kho ngữ liệu khác nhau, các điều kiện để ngoại suy độ dài vẫn chưa được nghiên cứu. Bài báo này cố gắng xác định những loại RPE nào cho phép ngoại suy độ dài thông qua phân tích toán học và thực nghiệm kỹ lưỡng. Chúng tôi phát hiện ra rằng một transformer chắc chắn sở hữu thuộc tính này miễn là chuỗi tương ứng với hàm mũ của RPE hội tụ. Hai thực hành được suy ra từ các điều kiện và được kiểm tra trong các nhiệm vụ mô hình hóa ngôn ngữ trên nhiều kho ngữ liệu khác nhau. Như một phần thưởng từ các điều kiện, chúng tôi suy ra một Trường Tiếp nhận Lý thuyết (TRF) mới để đo lường trường tiếp nhận của các RPE mà không cần thực hiện bất kỳ bước huấn luyện nào.
Các thí nghiệm mở rộng được thực hiện trên các bộ dữ liệu Wikitext-103, Books, Github, và WikiBook để chứng minh tính khả thi của các điều kiện được phát hiện. Chúng tôi cũng so sánh TRF với Trường Tiếp nhận Thực nghiệm (ERF) trên các mô hình khác nhau, cho thấy các xu hướng phù hợp nhất quán trên các bộ dữ liệu này.

Giới thiệu
Transformer (Vaswani et al. 2017) đang phát triển ổn định trong các lĩnh vực xử lý ngôn ngữ tự nhiên (Qin et al. 2023b; Devlin et al. 2019; Liu et al. 2019; Qin et al. 2022b,a; Liu et al. 2022; Qin and Zhong 2023), thị giác máy tính (Dosovitskiy et al. 2020; Sun et al. 2022b; Lu et al. 2022; Hao et al. 2024), và xử lý âm thanh (Gong, Chung, and Glass 2021; Akbari et al. 2021; Gulati et al. 2020; Sun et al. 2022a). Mặc dù nó vượt trội hơn các kiến trúc khác như RNN (Cho et al. 2014; Qin, Yang, and Zhong 2023) và CNN (Kim 2014; Hershey et al. 2016; Gehring et al. 2017) trong nhiều nhiệm vụ mô hình hóa chuỗi, sự thiếu khả năng ngoại suy độ dài hạn chế khả năng xử lý một phạm vi rộng các độ dài chuỗi, tức là các chuỗi suy luận cần phải bằng hoặc ngắn hơn các chuỗi huấn luyện. Tăng độ dài chuỗi huấn luyện chỉ là một giải pháp tạm thời vì độ phức tạp không gian-thời gian tăng theo hàm bậc hai với độ dài chuỗi. Một lựa chọn khác là mở rộng độ dài chuỗi suy luận bằng cách chuyển đổi các khối attention đầy đủ đã được huấn luyện thành các khối attention cửa sổ trượt (Beltagy, Peters, and Cohan 2020), nhưng điều này sẽ dẫn đến hiệu quả tệ hơn đáng kể so với tốc độ attention đầy đủ (Press, Smith, and Lewis 2022). Làm thế nào để giải quyết vĩnh viễn vấn đề này mà không phát sinh chi phí bổ sung đã nổi lên như một chủ đề mới.

Một giải pháp chính thống cho ngoại suy độ dài là thiết kế một Mã hóa Vị trí Tương đối (RPE) (Qin et al. 2023c) tập trung attention vào các token lân cận. Ví dụ, ALiBi (Press, Smith, and Lewis 2022) áp dụng các bias phân rã tuyến tính cho attention để giảm đóng góp từ các token xa. Kerple (Chi et al. 2022) nghiên cứu các kernel bất biến dịch chuyển điều kiện xác định dương trong RPE và đề xuất một tập hợp các kernel thúc đẩy thuộc tính ngoại suy độ dài. Nó cũng chỉ ra rằng ALiBi là một trong những trường hợp của nó. Sandwich (Chi, Fan, and Rudnicky 2022) đề xuất một giả thuyết để giải thích bí mật đằng sau ALiBi và chứng minh thực nghiệm bằng cách tích hợp giả thuyết vào các embedding vị trí sinusoidal.

Để nghiên cứu sự ngoại suy của transformer, trước tiên chúng tôi thiết lập một giả thuyết về lý do tại sao các phương pháp ngoại suy độ dài dựa trên RPE hiện có (Qin et al. 2023a) có khả năng ngoại suy các chuỗi trong suy luận dựa trên phân tích thực nghiệm. Sau đó chúng tôi xác định các điều kiện của RPE thỏa mãn giả thuyết thông qua phân tích toán học. Cuối cùng, các điều kiện được phát hiện được xác thực thực nghiệm trên nhiều kho ngữ liệu khác nhau. Cụ thể, chúng tôi giả định rằng do các bias phân rã, các phương pháp ngoại suy độ dài dựa trên RPE hiện có hoạt động tương tự như attention cửa sổ trượt, tức là chỉ các token trong một phạm vi nhất định mới có thể ảnh hưởng đến điểm attention. Một transformer có thể ngoại suy chắc chắn trong tình huống này vì các token ngoài phạm vi không có ảnh hưởng đến kết quả attention. Chúng tôi suy ra rằng một transformer được đảm bảo thỏa mãn giả thuyết này nếu chuỗi tương ứng với hàm mũ của RPE của nó hội tụ. Dựa trên quan sát, chúng tôi chỉ ra rằng các phương pháp dựa trên RPE trước đây (Press, Smith, and Lewis 2022; Chi et al. 2022) có thể được xem là các trường hợp cụ thể trong các điều kiện. Hai thực hành mới từ các điều kiện được suy ra và đánh giá trong mô hình hóa ngôn ngữ.

Các điều kiện quan sát được không chỉ làm sáng tỏ bí mật của ngoại suy độ dài mà còn cung cấp một góc nhìn mới về tính toán Trường Tiếp nhận Lý thuyết (TRF) của các RPE. Trái ngược với các phương pháp trước đây yêu cầu gradient huấn luyện để tính toán TRF, chúng tôi đề xuất một cách mới để tính toán TRF chỉ dựa trên công thức của các RPE. Các thí nghiệm mở rộng trên Wikitext-103 (Merity et al. 2016), Books (Zhu et al. 2015), Github (Gao et al. 2020), và WikiBook (Wettig et al. 2022) xác thực các điều kiện. TRF được tính toán bằng phương pháp của chúng tôi phù hợp đáng kể với xu hướng của Trường Tiếp nhận Thực nghiệm (ERF) trong các tình huống thực tế.

Kiến thức chuẩn bị
Trước khi bắt đầu hành trình khám phá, chúng tôi giới thiệu một số khái niệm chuẩn bị sẽ được sử dụng xuyên suốt bài báo, chẳng hạn như softmax attention, mã hóa vị trí tương đối, ngoại suy độ dài, và attention cửa sổ trượt. Chúng tôi cũng cung cấp các ký hiệu toán học cần thiết cho phân tích tiếp theo, tức là chúng tôi sử dụng M để biểu thị một ma trận và m⊤i để đại diện cho hàng thứ i của M. Các ký hiệu toán học đầy đủ có thể được tìm thấy trong Phụ lục. Theo nghiên cứu trước đây (Press, Smith, and Lewis 2022), chúng tôi giới hạn phân tích của mình đối với các mô hình ngôn ngữ nhân quả và giả định rằng độ dài chuỗi tối đa trong quá trình huấn luyện là m.

Softmax attention
Softmax attention là một thành phần chính của transformer hoạt động trên các ma trận query Q, key K và value V. Mỗi ma trận là một ánh xạ tuyến tính nhận X∈Rn×d làm đầu vào:
Q=XWQ,K=XWK,V=XWV∈Rn×d, (1)
trong đó n là độ dài chuỗi và d là chiều của đặc trưng ẩn. Ma trận attention đầu ra O∈Rn×d có thể được công thức hóa như:
O= Softmax( QKT/√d)V. (2)
Để ngăn ngừa rò rỉ thông tin trong mô hình hóa ngôn ngữ nhân quả, một ma trận mask M∈Rn×n được sử dụng để đảm bảo rằng các token hiện tại chỉ có thể nhìn thấy các token trước đó và chính chúng. Các phần tử tam giác dưới của M là 0, và các phần tử tam giác trên, ngoại trừ đường chéo, là −∞. Sau đó ma trận attention đầu ra O cho các mô hình ngôn ngữ nhân quả sẽ là:
O= Softmax( QK⊤/√d+M)V. (3)
Lưu ý rằng Phương trình 3 có thể được xem là dạng tổng quát của attention, tức là khi các phần tử của M đều là 0, Phương trình 3 suy biến thành Phương trình 2. Để dễ thảo luận, chúng tôi sử dụng Phương trình 3 để biểu diễn tính toán attention.

Mã hóa vị trí tương đối
Mã hóa vị trí được thiết kế để tiêm bias vị trí vào transformer. Mã hóa Vị trí Tuyệt đối (APE) (Vaswani et al. 2017; Gehring et al. 2017) và Mã hóa Vị trí Tương đối (RPE) (Su et al. 2021; Liutkus et al. 2021; Press, Smith, and Lewis 2022; Chi et al. 2022) là hai loại mã hóa vị trí phổ biến nhất. Trong bài báo này, chúng tôi tập trung vào RPE vì nó là chìa khóa cho ngoại suy độ dài, như được chỉ ra trong (Press, Smith, and Lewis 2022). Một attention với RPE có thể được viết như:
O= Softmax( QK⊤/√d+M+P)V, (4)
trong đó P∈Rn×n là một ma trận Toeplitz mã hóa thông tin vị trí tương đối, tức là pij=pi−j. Đáng chú ý rằng M và P có thể được hợp nhất, và ma trận được hợp nhất vẫn là một ma trận Toeplitz. Chúng tôi sử dụng R để biểu diễn ma trận được hợp nhất và viết lại Phương trình 4 như:
O= Softmax( QK⊤/√d+R)V. (5)

Định nghĩa ngoại suy độ dài
Thuộc tính ngoại suy độ dài cho phép một mô hình được kiểm tra trên các chuỗi dài hơn so với những chuỗi được sử dụng trong huấn luyện. Các cấu trúc mô hình hóa chuỗi trước đây như RNN (Hochreiter and Schmidhuber 1997) và CNN (Gehring et al. 2017) thường tự nhiên sở hữu thuộc tính này, nhưng đó là một nhiệm vụ khó khăn đối với transformer. Thuộc tính này chỉ có mặt trong transformer cửa sổ trượt và một số biến thể transformer với RPE được thiết kế đặc biệt (Chi et al. 2022; Press, Smith, and Lewis 2022; Chi, Fan, and Rudnicky 2022).

Trong mô hình hóa ngôn ngữ, một token chỉ có thể nhìn thấy chính nó và các token trước đó. Do đó, bất kể độ dài chuỗi, hiệu suất phải ổn định đối với các token lân cận nằm trong độ dài chuỗi huấn luyện (Beltagy, Peters, and Cohan 2020). Đối với các token nằm ngoài phạm vi, hiệu suất sẽ giảm nếu mô hình không hỗ trợ ngoại suy độ dài (Press, Smith, and Lewis 2022). Dựa trên quan sát trên, chúng tôi đưa ra định nghĩa về ngoại suy độ dài:

Định nghĩa 0.1. Đối với một mô hình ngôn ngữ F, cho bộ dữ liệu X, nếu với bất kỳ n nào, có:
|ppln(X,F)−pplm(X,F)|/pplm(X,F)< δ, (6)
thì F được coi là có thuộc tính ngoại suy.
Ở đây δ > 0 là một hằng số nhỏ, ppln(X,F) có nghĩa là F tính toán perplexity với độ dài chuỗi tối đa n trên bộ dữ liệu X. Thực nghiệm, nếu |ppln(X,F)−pplm(X,F)|/pplm(X,F) trở nên rất lớn (≫1) khi n tăng, chúng tôi coi rằng F không có thuộc tính ngoại suy.

Attention cửa sổ trượt
Để thuận tiện cho các thảo luận tiếp theo, chúng tôi định nghĩa một window attention tại vị trí i và kích thước cửa sổ j như sau:
oji=∑i−j+1≤s≤iexp(q⊤iks/√d) exp( ris)vs∑i−j+1≤t≤iexp(q⊤ikt/√d) exp( rit)≜∑i−j+1≤s≤icisvsCij,(7)
trong đó Cij=∑i−j+1≤t≤icit, cij=aijbij, aij=exp(q⊤ikj/√d), bij= exp( rij), j≤i.
Chúng tôi tiếp tục giả định ∥xi∥ ≤l,x∈ {q,k,v}, trong đó l >0 là một hằng số. oji biểu diễn đầu ra attention của token thứ i, tương tác với j token đứng trước nó. Lưu ý rằng window attention tự nhiên sở hữu khả năng ngoại suy độ dài.

Có hai cách để suy luận window attention: suy luận không chồng lấp và suy luận cửa sổ trượt như được hiển thị ở bên phải của Hình 1. Trong suy luận cửa sổ trượt, các token trong mỗi cửa sổ trượt phải được mã hóa lại nhiều lần, làm cho nó chậm hơn đáng kể so với suy luận không chồng lấp. Trong bảng 1 chúng tôi so sánh thời gian suy luận trung bình trên một nhóm kích thước cửa sổ giữa suy luận cửa sổ trượt và suy luận cửa sổ không chồng lấp. Suy luận cửa sổ trượt chậm hơn hơn 44 lần so với suy luận không chồng lấp. Tuy nhiên, như được hiển thị ở bên trái của Hình 1, suy luận cửa sổ trượt có ppl thấp hơn nhiều so với suy luận không chồng lấp.

Khám phá Ngoại suy Transformer
Trong phần này, trước tiên chúng tôi mô tả giả thuyết về lý do tại sao các phương pháp ngoại suy độ dài dựa trên RPE hiện có có thể ngoại suy các chuỗi trong suy luận và cung cấp bằng chứng thực nghiệm cho nó. Sau đó chúng tôi suy ra chi tiết các điều kiện cho ngoại suy độ dài và chứng minh rằng các phương pháp ngoại suy độ dài dựa trên RPE gần đây (Chi et al. 2022; Press, Smith, and Lewis 2022) thỏa mãn các điều kiện.

Giả thuyết
Một sliding window attention với kích thước cửa sổ w tương đương với RPE sau đây trên full attention:
mij=0, i −j≤w.−∞,khác .(8)

Bằng cách so sánh Phương trình 8 và RPE tương ứng của Alibi (Press, Smith, and Lewis 2022) trong Hình 2, chúng ta có thể thấy rằng cả hai đều có cùng hành vi trong việc tập trung các token bên trong một phạm vi được chỉ định. Ngoài ra, trong Hình 1, chúng tôi chỉ ra rằng hiệu suất của Alibi tương tự như sliding window attention khi kích thước cửa sổ đủ lớn. Dựa trên hai quan sát này, chúng tôi đưa ra giả thuyết sau:

Giả thuyết 0.1. Một RPE làm cho transformer có thể ngoại suy cần có hành vi tương tự như sliding window attention, tức là δ(i, j) phải thỏa mãn:
∀ϵ >0,∃j0, s.t, j > j0, δ(i, j)< ϵ, (9)
trong đó δ(i, j)≜∥oii−oji∥, và độ dài cửa sổ j cần đủ lớn.

Trong các phần tiếp theo, chúng tôi sẽ suy ra các điều kiện cho RPE thỏa mãn Phương trình 9.

Các điều kiện
Hãy giới thiệu bổ đề đầu tiên:

Bổ đề 0.2. Khi điều kiện sau được thỏa mãn, Phương trình 9 có giá trị.
limi→∞Cii≜C <∞. (10)

Chứng minh. Khi i≤m, độ dài chuỗi kiểm tra nhỏ hơn độ dài chuỗi tối đa m trong quá trình huấn luyện, lấy j=i, chúng ta có ∥oii−oji∥=∥oii−oii∥= 0. Khi i > m, chúng ta có thể công thức hóa lại Phương trình 7 như:
oii=∑i−j+1≤s≤icisvs+∑1≤s≤i−jcisvsCii
=∑i−j+1≤s≤icisvsCijCijCii+∑1≤s≤i−jcisvsCii−CijCii−CijCii
=∑i−j+1≤s≤icisvsCijCijCii+∑1≤s≤i−jcisvsCii−Cij1−CijCii.

Do đó chúng ta có:
oii−oji=1−CijCii ∑i−j+1≤s≤icisvsCij−∑1≤s≤i−jcisvsCii−Cij!.(11)

Đối với phần thứ hai:
|∑i−j+1≤s≤icisvsCij−∑1≤s≤i−jcisvsCii−Cij|≤∑i−j+1≤s≤icis∥vs∥Cij+∑1≤s≤i−jcis∥vs∥Cii−Cij≤∑i−j+1≤s≤icislCij+∑1≤s≤i−jcislCii−Cij= 2l(12)

Chúng ta có
δ(i, j)≤21−CijCiil. (13)

Theo Phương trình 10 và đuôi của chuỗi hội tụ có thể nhỏ tùy ý. ∀C/2> ϵ > 0, chúng ta có thể tìm một j0, sao cho nếui≥j > j0,Cii−Cij< ϵ. Chúng ta cũng có thể tìm một j1, sao cho nếu i≥j > j1,C−ϵ < Cii< C +ϵ. Nếu chúng ta lấy j2= max( j0, j1), vậy nếu i≥j≥j2, chúng ta có:
Cii−Cij< ϵ, C −ϵ < Cii< C+ϵ (14)

Vậy khi i≥j≥j2, chúng ta có
δ(i, j)≤21−CijCiil= 2Cii−CijCiil≤2ϵC−ϵl≤2lϵC−C/2=4lϵC(15)

Theo định nghĩa giới hạn, Phương trình 10 có giá trị.

Bổ đề này ngụ ý rằng đối với bất kỳ token nào nếu attention của mô hình tập trung vào j (j≥j2) token lân cận của nó, mô hình có thuộc tính ngoại suy độ dài. Bổ đề đi kèm với trực giác của chúng ta. Có nghĩa là miễn là một RPE tuân theo cùng nguyên tắc, tức là đặt nhiều trọng số hơn vào j token lân cận, mô hình được đảm bảo có thuộc tính ngoại suy độ dài? Trong các phần tiếp theo, chúng tôi sẽ chứng minh rằng việc tập trung nhiều trọng số hơn vào các token lân cận không đảm bảo transformer có thuộc tính ngoại suy độ dài. Cụ thể, chúng tôi sẽ cung cấp một chứng minh toán học về các điều kiện đủ để RPE có thuộc tính ngoại suy độ dài.

Định lý 0.3. Khi điều kiện sau được thỏa mãn, Phương trình 9 có giá trị.
limi→∞Bii<∞, Bii=∑1≤t≤ibit<∞. (16)

Chứng minh. Vì chúng ta giả định ∥qi∥ ≤l,∥ki∥ ≤l, thì:
aij= exp( q⊤ikj)≤exp(l2), (17)
cij=aijbij≤exp(l2)bij, Cii≤exp(l2)Bii. (18)

Do đó, Phương trình 10 có thể được suy ra từ Phương trình 16. Kết hợp với Bổ đề 0.2, chứng minh được kết luận.

Bằng cách tận dụng thuộc tính của RPE, Định lý 0.3 có thể được đơn giản hóa thêm như:

Định lý 0.4. Khi điều kiện sau được thỏa mãn, Phương trình 9 có giá trị.
limi→∞∑t=1ibi−t= limi→∞∑t=0i−1bt<∞. (19)

Chứng minh. Theo định nghĩa của RPE:
Bii=∑1≤t≤ibit=∑t=1ibi−t=∑t=0i−1bt. (20)

Điều này có nghĩa là Phương trình 16 tương đương với:
limi→∞Bii= limi→∞∑t=0i−1bt<∞. (21)

Định lý 0.4 chỉ ra rằng miễn là chuỗi exp( RPE) hội tụ, mô hình được đảm bảo có thuộc tính ngoại suy độ dài. Dựa trên nguyên tắc này, chúng ta có thể xác định toán học liệu một RPE có cho phép ngoại suy độ dài trước khi thực hiện thí nghiệm hoặc thiết kế nhiều RPE có thể thực hiện ngoại suy độ dài. Trong Phụ lục, chúng tôi chỉ ra rằng các phương pháp trước đây như Alibi (Press, Smith, and Lewis 2022), Kerple (Chi et al. 2022), và Sandwich (Chi, Fan, and Rudnicky 2022) thỏa mãn các điều kiện được suy ra của chúng tôi cho ngoại suy độ dài.

Trường tiếp nhận lý thuyết
Trong phần trước, chúng tôi đã thiết lập các điều kiện cho ngoại suy độ dài. Như một phần thưởng bổ sung, chúng ta có thể suy ra Trường Tiếp nhận Lý thuyết (TRF) cho bất kỳ phương pháp ngoại suy độ dài dựa trên RPE nào. Hãy bắt đầu với định nghĩa về Trường Tiếp nhận Thực nghiệm (ERF). ERF có thể được xem như một cửa sổ chứa phần lớn thông tin có trong attention.

Nhớ lại Phương trình 13, bằng cách đặt 1−CijCii=ϵ, chúng ta có thể định nghĩa:
Cij=Cii(1−ϵ), nemp(ϵ) = infj(Cij> Cii(1−ϵ)),
nemp(ϵ) là ERF biểu diễn độ dài chuỗi tối thiểu cần thiết để duy trì hiệu suất trong khoảng cách ϵ. Một cách trực quan, ERF có thể được xem như cửa sổ nhỏ nhất chứa phần lớn thông tin trong một attention. Vì nó liên quan đến cả aij và bij, nó chỉ có thể được tính toán sau khi huấn luyện.

Bây giờ chúng tôi định nghĩa TRF, cho phép chúng ta ước tính trường tiếp nhận mà không cần huấn luyện. Để thực hiện điều này, chúng ta xem xét cận trên của Cij. Từ định nghĩa của Cij và Phương trình 17, Cij được giới hạn trên bởi Bij. Do đó, chúng ta có thể định nghĩa TRF nbthe(ϵ) đối với chuỗi bt như:
nthe(ϵ) = infj(Bij> B(1−ϵ))= infj ∑t=0j−1bt> B(1−ϵ)!= infj∑t≥jbt< Bϵ(22)
trong đó B= limj→∞∑t=0j−1bt. Đôi khi chúng ta có thể khó đưa ra dạng phân tích của tổng riêng phần của chuỗi, nhưng chúng ta vẫn có thể tính TRF một cách số học hoặc so sánh TRF của các RPE khác nhau bằng định lý dưới đây:

Định lý 0.5. Nếu các điều kiện sau có giá trị:
αtα≤βtβ, t→ ∞ , α≜limj→∞∑t=0j−1αt, β≜limj→∞∑t=0j−1βt. (23)
Thì:
nαthe(ϵ)≤nβthe(ϵ), ϵ→0. (24)

Chứng minh. Theo Phương trình 23, tồn tại t0>0, sao cho, khi t > t0, chúng ta có:
αtα≤βtβ. (25)
Cho ϵ < ϵ0, trong đó
nβthe(ϵ0) =t0, (26)
thì chúng ta có:
∑t≥nβthe(ϵ)βt≤βϵ, nβthe(ϵ)> t0. (27)
Cuối cùng:
∑t≥nβthe(ϵ)αt≤∑t≥nβthe(ϵ)αβtβ≤αβϵβ=αϵ.
Theo Phương trình 22, chúng ta có:
nathe(ϵ)≤nbthe(ϵ). (28)

Chuỗi exp( RPE) tuân theo cùng xu hướng với TRF, chuỗi càng nhỏ, TRF càng nhỏ.

Chúng tôi cung cấp một số ví dụ về cách tính TRF trong Phụ lục.

Hai RPE mới
Dựa trên các điều kiện đã chứng minh của ngoại suy độ dài, chúng ta có thể thiết kế vô số loại RPE có thuộc tính ngoại suy độ dài. Ở đây, chúng tôi đề xuất hai RPE mới để chứng minh thực nghiệm các điều kiện và giả thuyết, cụ thể là:
Type1 : bn=1n2= exp( −2 lnn),
Type2 : bn= exp( −ln2n);

TRF tương ứng của Type 1 là:
Bij=∑i=0j−11(i+ 1)2≈∫j11x2dx= 1−1j, B= 1.
nthe(ϵ) = infj(Bij> B(1−ϵ))= infj1−1j>1−ϵ= Θ1ϵ(29)

Đối với Type 2, khó cung cấp dạng phân tích của TRF của nó. Tuy nhiên, chúng ta có thể chứng minh rằng TRF của Type 2 nhỏ hơn TRF của Type 1 bằng Định lý 0.5 và bất đẳng thức dưới đây:
∀c1, c2>0,exp(−ln2n)c1<1/n2c2, n→ ∞ .

Xác thực thực nghiệm
Thiết lập Tất cả các mô hình được triển khai trong Fairseq (Ott et al. 2019) và được huấn luyện trên 8 GPU V100. Chúng tôi sử dụng cùng kiến trúc mô hình và cấu hình huấn luyện cho tất cả các biến thể RPE để đảm bảo công bằng. Đối với Wikitext-103 (Merity et al. 2016), vì nó là một bộ dữ liệu tương đối nhỏ, chúng tôi sử dụng cấu trúc decoder transformer 6 lớp với kích thước embedding là 512. Đối với các bộ dữ liệu khác, cụ thể, chúng tôi sử dụng cấu trúc decoder transformer 12 lớp với kích thước embedding là 768. Thước đo đánh giá là perplexity (PPL) và độ dài huấn luyện tối đa trong quá trình huấn luyện là 512. Các thiết lập siêu tham số chi tiết được liệt kê trong Phụ lục.

Bộ dữ liệu Chúng tôi tiến hành thí nghiệm trên Wikitext-103 (Merity et al. 2016), Books (Zhu et al. 2015), Github (Gao et al. 2020) và WikiBook (Wettig et al. 2022). Wikitext-103 là một bộ dữ liệu nhỏ chứa một phiên bản được tiền xử lý của bộ dữ liệu Wikipedia. Nó được sử dụng rộng rãi trong nhiều bài báo NLP. Books có một số lượng lớn tiểu thuyết, làm cho nó trở thành một kho ngữ liệu tốt cho xử lý chuỗi dài. Github bao gồm một lượng đáng kể các kho mã nguồn mở, phần lớn được viết bằng các ngôn ngữ lập trình. WikiBook là một kho ngữ liệu 22 gigabyte gồm các bài viết Wikipedia và sách được tuyển chọn bởi (Wettig et al. 2022). Kho ngữ liệu này được sử dụng để xác thực hiệu suất của các mô hình khác nhau trên các bộ dữ liệu lớn.

Xác thực tính đủ. Để xác thực thực nghiệm tính đủ của các điều kiện được phát hiện, chúng tôi tích hợp hai RPE được đề xuất trong phần trước vào transformer và kiểm tra khả năng ngoại suy độ dài của chúng trên các bộ dữ liệu Wikitext-103, Books, Github, và WikiBook. Chúng tôi tăng độ dài của chuỗi suy luận từ 512 đến 9216 token và vẽ đồ thị PPL kiểm tra của các RPE được đề xuất cũng như của các phương pháp hiện có như Alibi, Kerple, và Sandwich trong Hình 3. Kết quả số chi tiết có thể được tìm thấy trong Bảng 4 và Bảng 5 từ Phụ lục. Tất cả các phương pháp này đều thể hiện khả năng ngoại suy độ dài tốt. Tuy nhiên, PPL được ổn định có thể khác nhau do hiệu quả của các chiến lược mã hóa vị trí khác nhau, điều này không được xem xét trong bài báo này. Chúng tôi bao gồm mã hóa vị trí Sinusoidal (Vaswani et al. 2017) như một phương pháp tham chiếu không thể ngoại suy, tăng nhanh khi độ dài chuỗi suy luận tăng.

Xác thực tính cần thiết. Mặc dù chúng tôi chỉ cung cấp chứng minh toán học cho tính đủ của các điều kiện được phát hiện, chúng tôi cũng cố gắng xác minh tính cần thiết của chúng một cách thực nghiệm trong phần này. Cụ thể, chúng tôi chọn hai RPE rất gần với việc thỏa mãn Định lý 0.4 như sau. Lưu ý rằng cả hai đều tập trung trọng số của chúng vào các token lân cận.
Example1 : bn=1n,Example2 : bn=1nlnn

Dưới đây là một chứng minh toán học ngắn gọn rằng các RPE trên không thỏa mãn Định lý 0.4.
∑n=1k1n>∫k+111xdx= ln( k+ 1),
∑n=3k1nlnn>∫k+131xlnxdx= ln ln( k+ 1)−ln ln 3 .

Sau đó chúng tôi kiểm tra thực nghiệm khả năng ngoại suy độ dài của chúng trên các bộ dữ liệu Wikitext-103, Books, Github, và WikiBook bằng cách mở rộng độ dài chuỗi suy luận từ 512 đến 9216 token. Như được hiển thị trong Hình 4, PPL của cả hai RPE tăng nhanh khi độ dài của chuỗi kiểm tra tăng. Nó chứng minh rằng cả hai đều không thể ngoại suy. Chúng tôi cũng bao gồm Type 1 RPE trong Hình 4 như một tham chiếu có thể ngoại suy. Kết quả số chi tiết có thể được tìm thấy trong Bảng 6 từ Phụ lục.

Xác thực TRF Chúng tôi xác thực TRF được đề xuất bằng cách so sánh xu hướng giữa TRF và ERF. Chúng tôi vẽ đồ thị TRF và ERF của Alibi, Kerple, Sandwich, và các RPE được đề xuất trên các bộ dữ liệu nói trên. Như quan sát trong Hình 6 và Hình 5, trong khi các đường cong khác nhau trên các bộ dữ liệu, TRF ước tính xu hướng tổng thể tương tự của ERF.

Trực quan hóa RPE Chúng tôi trực quan hóa các sơ đồ trọng số của Type 1 và Type 2 trong Hình 7, tức là bản đồ nhiệt của exp( RPE). Type 2 tập trung trọng số vào các token lân cận gần hơn so với Type 1, cho thấy TRF và ERF nhỏ hơn như được hiển thị trong Hình 6 và Hình 5. Chúng tôi cũng trực quan hóa các phương pháp khác trong Phụ lục.

Kết luận
Trong bài báo này, chúng tôi khám phá bí mật của ngoại suy độ dài transformer trong mô hình hóa ngôn ngữ. Trước tiên chúng tôi đưa ra một giả thuyết về ngoại suy và sau đó suy ra các điều kiện đủ để RPE có thuộc tính ngoại suy độ dài. Phân tích toán học kỹ lưỡng cho thấy rằng một mô hình transformer chắc chắn có khả năng ngoại suy độ dài nếu chuỗi tương ứng với hàm mũ của RPE của nó hội tụ. Quan sát này mang lại một phần thưởng bổ sung: chúng ta có thể ước tính TRF của các RPE chỉ dựa trên công thức của chúng. Chúng tôi chọn hai RPE mới thỏa mãn các điều kiện và hai RPE không thỏa mãn để chứng minh thực nghiệm tính đủ của các điều kiện trên bốn bộ dữ liệu được sử dụng rộng rãi. Chúng tôi cũng xác thực TRF của mình bằng cách so sánh chúng với ERF trên các bộ dữ liệu này. Kết quả cho thấy TRF của chúng tôi có thể phản ánh chính xác các trường tiếp nhận thực tế của RPE trước khi huấn luyện.

Lời cảm ơn
Công việc này được hỗ trợ một phần bởi Chương trình R&D Trọng điểm Quốc gia của Trung Quốc (SỐ.2022ZD0160100).

Tài liệu tham khảo
Akbari, H.; Yuan, L.; Qian, R.; Chuang, W.-H.; Chang, S.-F.; Cui, Y.; and Gong, B. 2021. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. In arXiv preprint arXiv:2104.11178.

Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The Long-Document Transformer. In arXiv:2004.05150.

Chi, T.-C.; Fan, T.-H.; Ramadge, P. J.; and Rudnicky, A. I. 2022. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. ArXiv, abs/2205.09921.

Chi, T.-C.; Fan, T.-H.; and Rudnicky, A. I. 2022. Receptive Field Alignment Enables Transformer Length Extrapolation. ArXiv, abs/2212.10356.

Cho, K.; van Merriënboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1724–1734. Doha, Qatar: Association for Computational Linguistics.

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186. Minneapolis, Minnesota: Association for Computational Linguistics.

Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.

Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.; Presser, S.; and Leahy, C. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. In arXiv preprint arXiv:2101.00027.

Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin, Y. N. 2017. Convolutional sequence to sequence learning. In International Conference on Machine Learning, 1243–1252. PMLR.

Gong, Y.; Chung, Y.-A.; and Glass, J. 2021. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, 571–575.

Gulati, A.; Chiu, C.-C.; Qin, J.; Yu, J.; Parmar, N.; Pang, R.; Wang, S.; Han, W.; Wu, Y.; Zhang, Y.; and Zhang, Z., eds. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition.

Hao, D.; Mao, Y.; He, B.; Han, X.; Dai, Y.; and Zhong, Y. 2024. Improving Audio-Visual Segmentation with Bidirectional Generation. In Proceedings of the AAAI Conference on Artificial Intelligence.

Hershey, S.; Chaudhuri, S.; Ellis, D. P. W.; Gemmeke, J. F.; Jansen, A.; Moore, R. C.; Plakal, M.; Platt, D.; Saurous, R. A.; Seybold, B.; Slaney, M.; Weiss, R. J.; and Wilson, K. W. 2016. CNN architectures for large-scale audio classification. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 131–135.

Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term memory. Neural computation, 9(8): 1735–1780.

Kim, Y. 2014. Convolutional Neural Networks for Sentence Classification. In Conference on Empirical Methods in Natural Language Processing.

Knopp, K. 1956. Infinite sequences and series. Courier Corporation.

Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Liu, Z.; Li, D.; Lu, K.; Qin, Z.; Sun, W.; Xu, J.; and Zhong, Y. 2022. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955.

Liutkus, A.; Cífka, O.; Wu, S.-L.; Simsekli, U.; Yang, Y.-H.; and Richard, G. 2021. Relative positional encoding for transformers with linear complexity. In International Conference on Machine Learning, 7067–7079. PMLR.

Lu, K.; Liu, Z.; Wang, J.; Sun, W.; Qin, Z.; Li, D.; Shen, X.; Deng, H.; Han, X.; Dai, Y.; and Zhong, Y. 2022. Linear video transformer with feature fixation. arXiv preprint arXiv:2210.08164.

Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016. Pointer Sentinel Mixture Models. In arXiv:1609.07843.

Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grangier, D.; and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038.

Press, O.; Smith, N.; and Lewis, M. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations.

Qin, Z.; Han, X.; Sun, W.; He, B.; Li, D.; Li, D.; Dai, Y.; Kong, L.; and Zhong, Y. 2023a. Toeplitz Neural Network for Sequence Modeling. In The Eleventh International Conference on Learning Representations.

Qin, Z.; Han, X.; Sun, W.; Li, D.; Kong, L.; Barnes, N.; and Zhong, Y. 2022a. The Devil in Linear Transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 7025–7041. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics.

Qin, Z.; Li, D.; Sun, W.; Sun, W.; Shen, X.; Han, X.; Wei, Y.; Lv, B.; Yuan, F.; Luo, X.; Qiao, Y.; and Zhong, Y. 2023b. Scaling TransNormer to 175 Billion Parameters. In arXiv preprint 2307.14995.

Qin, Z.; Sun, W.; Deng, H.; Li, D.; Wei, Y.; Lv, B.; Yan, J.; Kong, L.; and Zhong, Y. 2022b. cosFormer: Rethinking Softmax In Attention. In International Conference on Learning Representations.

Qin, Z.; Sun, W.; Lu, K.; Deng, H.; Li, D.; Han, X.; Dai, Y.; Kong, L.; and Zhong, Y. 2023c. Linearized Relative Positional Encoding. arXiv preprint arXiv:2307.09270.

Qin, Z.; Yang, S.; and Zhong, Y. 2023. Hierarchically gated recurrent neural network for sequence modeling. NeurIPS.

Qin, Z.; and Zhong, Y. 2023. Accelerating Toeplitz Neural Network with Constant-time Inference Complexity. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.

Su, J.; Lu, Y.; Pan, S.; Wen, B.; and Liu, Y. 2021. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864.

Sun, J.; Zhong, G.; Zhou, D.; Li, B.; and Zhong, Y. 2022a. Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition. arXiv preprint arXiv:2203.15609.

Sun, W.; Qin, Z.; Deng, H.; Wang, J.; Zhang, Y.; Zhang, K.; Barnes, N.; Birchfield, S.; Kong, L.; and Zhong, Y. 2022b. Vicinity Vision Transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence, (01): 1–14.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Wettig, A.; Gao, T.; Zhong, Z.; and Chen, D. 2022. Should You Mask 15% in Masked Language Modeling? In arXiv:2202.08005.

Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV).

Tài liệu Bổ sung
Ký hiệu toán học
Ký hiệu Ý nghĩa
X Trạng thái ẩn.
Q,K,V Query, key, value.
O Đầu ra attention.
d Chiều đặc trưng.
m⊤s Hàng thứ s của ma trận M.
Bảng 2: Các ký hiệu toán học được sử dụng trong bài báo.

Ví dụ
Trong phần này, chúng tôi sử dụng Alibi (Press, Smith, and Lewis 2022), Kerple (Chi et al. 2022), và Sandwich (Chi, Fan, and Rudnicky 2022) làm ví dụ để hỗ trợ các điều kiện đủ được phát hiện 16 cho ngoại suy độ dài.

Alibi Dạng của Alibi có thể được viết như:
bt= exp( −kt), k > 0. (30)
Theo sự hội tụ của chuỗi hình học:
limi→∞Bii=∑t=0i−1exp(−kt)<∞, (31)
thỏa mãn các điều kiện quan sát của chúng tôi.

Kerple Kerple đề xuất hai dạng RPE: log và power. Chúng tôi thảo luận chúng riêng biệt.
Công thức của biến thể Log có thể được biểu diễn như:
bt= exp( −rlog (1 + kt)) =1(1 +kt)r(32)
trong đó r,k>0. Trong Kerple, r,k có thể học được. Dựa trên Định lý 0.4, để cho phép mô hình ngoại suy, chúng ta phải thêm ràng buộc rằng r>1 vì:
1(1 +kt)r∼1krtr, (33)
Trong phân tích thực nghiệm, chúng tôi sẽ chỉ ra rằng khi r=1, mô hình không thể ngoại suy. Chúng tôi cũng kiểm tra mô hình đã huấn luyện từ Kerple và thấy điều kiện này được đáp ứng.

Biến thể Poly có thể được viết như:
bt= exp( −ktr),0< r≤2. (34)
Vì
bt≤exp(−kt), t→ ∞ . (35)
theo sự hội tụ của chuỗi hình học, chúng ta có:
limi→∞Bii= limi→∞∑t=0i−1exp(−ktr)<∞. (36)
thỏa mãn các điều kiện quan sát của chúng tôi.

Sandwich Cho công thức của Sandwich:
bt= exp k∑j=1d/2cos tr2j/d −d2, k > 0, r > 0.
Trước tiên chúng ta thực hiện các phép biến đổi sau:
bt= exp k∑j=1d/2cos tr2j/d −1=∏j=1d/2exp kcos tr2j/d −1,(37)

sau đó thực hiện phân vùng trên j:
tr2j/d≥π2, (38)
tương đương với:
2t≥πr2j/d,2tπ≥r2j/d,
logr2tπ≥2j/d, j ≤dlogr2tπ2≜f(t).(39)

Do đó chúng ta có:
bt=∏1≤j≤f(t)exp kcos tr2j/d −1×∏f(t)<j≤d/2exp kcos tr2j/d −1.(40)

Đối với phần đầu:
cos tr2j/d −1<−1. (41)

Đối với phần thứ hai:
cos tr2j/d −1<0. (42)

Thì:
βt≤∏1≤j≤f(t)exp(−k)
= exp( −k⌊f(t)⌋)
≤exp(k) exp(−kf(t))
= exp( k) exp −kdlogr2tπ2!≜g(t).(43)

Theo kiểm tra Rabbe (Knopp 1956):
tg(t)g(t+ 1)−1
=t expkd2logr2t+ 22t−1
=t expkd2logr1 +1t−1
∼tkd2logr1 +1t+O1t2
∼tkd2 lnr1t−12t2+O1t2
→kd2 lnr,(44)

nếu:
kd2 lnr<1, d <2 lnrk, (45)
thì chuỗi hội tụ¹.

Ví dụ TRF
Chúng tôi sử dụng Alibi làm ví dụ để chỉ ra cách tính TRF. Bij của Alibi có thể được viết như:
Bij=∑i=0j−1exp(−i) =1−exp(−j)1−exp(−1), B=11−exp(−1)(46)

TRF của Alibi có thể được tính như:
nthe(ϵ) = infj(Bij> B(1−ϵ))= infj(1−exp(−j)>1−ϵ) = Θ( −logϵ).(47)
trong đó Θ biểu diễn cận trên và dưới tiệm cận.

¹Lưu ý rằng ở đây chúng tôi chỉ chỉ ra rằng chuỗi tương ứng với Sandwich hội tụ dưới một số điều kiện nhất định. Cận trên ở đây tương đối lỏng, và các điều kiện được sử dụng trong thực tế rộng hơn.

Cấu hình
Dữ liệu WikiText-103 Khác
Lớp Decoder 6 12
Chiều ẩn 512 768
Số head 8 12
Chiều FFN 2048 3072
Phương pháp Tokenizer BPE BPE
Kích thước từ vựng nguồn 50265 50265
Độ dài chuỗi 512 512
Tổng kích thước batch 128 128
Số bước cập nhật/epoch 50k bước cập nhật 50k bước cập nhật
Bước khởi động/epoch 4k bước 4k bước
Tốc độ học đỉnh 5e-4 5e-4
Bộ lập lịch tốc độ học Căn bậc hai nghịch đảo Căn bậc hai nghịch đảo
Tối ưu hóa Adam Adam
Adam ϵ 1e-8 1e-8
Adam (β1, β2) (0.9, 0.98) (0.9, 0.98)
Phân rã trọng số 0.01 0.01

Bảng 3: Cấu hình huấn luyện chi tiết được sử dụng trong các thí nghiệm của chúng tôi. "Tổng kích thước batch" có nghĩa là batch per gpu × update freq × num gpus.

Mã giả cho trực quan hóa TRF và ERF
1import torch
2
3def draw(array, n=50):
4 epsilon = torch.flip(torch.linspace(0, 1, n), dims=[0])
5 index = torch.zeros(n)
6 cusum = torch.sum(array)
7 m = len(array)
8 s = 0
9 i = 0
10 for j in range(m):
11 eps = epsilon[i]
12 while s >= cusum *(1 - eps) and i < n:
13 index[i] = j
14 if i < n - 1:
15 i += 1
16 else:
17 break
18 eps = epsilon[i]
19 s += array[j]
20 while i < n:
21 index[i] = m
22 i += 1
23
24 return index / m, epsilon

Kết quả thực nghiệm chi tiết
Bản đồ nhiệt

[Tiếp tục với các bảng và hình chi tiết như trong bản gốc...]

Wikitext-103
Độ dài Sinusoidal Alibi Kerple-Log Kerple-Power Sandwich Type1 Type2
512 24.73 24.22 24.12 24.18 24.76 24.25 24.29
768 41.08 23.45 23.36 23.42 24.04 23.43 23.51
1024 62.71 23.06 22.93 22.98 23.63 23.03 23.09
1280 83.81 22.83 22.67 22.73 23.39 22.77 22.83
1536 102.28 22.66 22.45 22.54 23.21 22.55 22.63
1792 121.98 22.60 22.41 22.47 23.18 22.50 22.59
2048 138.17 22.52 22.28 22.37 23.08 22.38 22.48
3072 194.43 22.33 22.02 22.14 22.91 22.15 22.27
4096 259.55 22.26 21.97 22.08 22.96 22.09 22.21
5120 289.79 22.20 21.86 22.00 22.93 21.99 22.14
6144 337.46 22.17 21.87 21.96 23.06 21.97 22.11
7168 376.41 22.16 21.84 21.95 23.13 21.96 22.10
8192 406.95 22.14 21.82 21.94 23.20 21.95 22.08
9216 423.92 22.12 21.80 21.90 23.26 21.90 22.06

Books
Độ dài Sinusoidal Alibi Kerple-Log Kerple-Power Sandwich Type1 Type2
512 7.49 7.28 7.34 7.31 7.64 7.30 7.35
768 10.43 7.15 7.21 7.18 7.55 7.18 7.22
1024 13.32 7.09 7.15 7.11 7.49 7.11 7.15
1280 15.53 7.06 7.11 7.08 7.47 7.08 7.12
1536 17.47 7.04 7.08 7.05 7.44 7.05 7.09
1792 19.02 7.03 7.06 7.03 7.42 7.03 7.06
2048 20.55 7.02 7.05 7.02 7.41 7.03 7.05
3072 24.70 7.00 7.02 7.00 7.38 7.00 7.03
4096 27.57 6.99 7.00 6.99 7.37 6.99 7.03
5120 29.54 6.99 7.00 6.99 7.36 6.99 7.03
6144 31.59 6.99 7.00 6.98 7.35 6.99 7.02
7168 32.41 6.98 7.00 6.98 7.35 6.99 7.02
8192 34.35 6.98 7.00 6.98 7.35 6.99 7.02
9216 34.70 6.98 7.01 6.98 7.35 6.99 7.02

Bảng 4: Xác thực tính đủ trên các bộ dữ liệu Wikitext-103 và Books. Để kiểm tra khả năng ngoại suy độ dài, chúng tôi kéo dài chuỗi suy luận từ 512 đến 9216 và tính toán PPL của các RPE Type 1 và Type 2 được đề xuất, cũng như Alibi, Kerple, và Sandwich. Tất cả các phương pháp này đều ổn định về PPL. Đối với các phương pháp không thể ngoại suy, ví dụ Sinusoidal, PPL của nó tăng nhanh.

Github
Độ dài Sinusoidal Alibi Kerple-Log Kerple-Power Sandwich Type1 Type2
512 2.29 2.25 2.24 2.25 2.29 2.25 2.25
768 3.98 2.16 2.16 2.16 2.20 2.16 2.16
1024 7.91 2.12 2.12 2.12 2.16 2.12 2.12
1280 12.97 2.10 2.09 2.09 2.14 2.09 2.09
1536 18.66 2.09 2.07 2.08 2.12 2.08 2.08
1792 24.08 2.08 2.06 2.07 2.11 2.07 2.07
2048 30.02 2.07 2.05 2.07 2.10 2.06 2.06
3072 51.64 2.06 2.03 2.05 2.08 2.04 2.04
4096 70.62 2.05 2.02 2.04 2.07 2.03 2.03
5120 89.78 2.05 2.02 2.04 2.07 2.03 2.03
6144 101.28 2.05 2.01 2.04 2.06 2.03 2.03
7168 117.21 2.05 2.01 2.04 2.06 2.03 2.03
8192 130.15 2.05 2.01 2.03 2.06 2.02 2.03
9216 143.17 2.05 2.01 2.03 2.05 2.02 2.03

Wikibook
Độ dài Sinusoidal Alibi Kerple-Log Kerple-Power Sandwich Type1 Type2
512 17.98 17.64 17.65 17.62 18.21 17.68 17.70
768 29.66 17.04 17.03 17.03 17.63 17.07 17.08
1024 47.31 16.76 16.73 16.73 17.35 16.76 16.80
1280 65.13 16.62 16.54 16.56 17.16 16.58 16.61
1536 83.16 16.51 16.41 16.44 17.04 16.44 16.49
1792 100.46 16.49 16.33 16.41 16.95 16.39 16.44
2048 116.94 16.43 16.26 16.36 16.89 16.31 16.39
3072 172.09 16.39 16.13 16.31 16.75 16.23 16.33
4096 231.86 16.39 16.14 16.29 16.73 16.24 16.36
5120 277.59 16.35 16.10 16.26 16.68 16.21 16.32
6144 312.17 16.35 16.12 16.25 16.66 16.22 16.32
7168 349.08 16.34 16.19 16.24 16.71 16.26 16.33
8192 390.81 16.35 16.22 16.25 16.71 16.30 16.36
9216 412.06 16.33 16.23 16.24 16.70 16.28 16.33

Bảng 5: Xác thực tính đủ trên các bộ dữ liệu Github, WikiBook. Để kiểm tra khả năng ngoại suy độ dài, chúng tôi kéo dài chuỗi suy luận từ 512 đến 9216 và tính toán PPL của các RPE Type 1 và Type 2 được đề xuất, cũng như Alibi, Kerple, và Sandwich. Tất cả các phương pháp này đều ổn định về PPL. Đối với các phương pháp không thể ngoại suy, ví dụ Sinusoidal, PPL của nó tăng nhanh.

Wikitext-103 Books Github Wikibook
Độ dài 1/n 1/(nlnn) 1/n 1/(nlnn) 1/n 1/(nlnn) 1/n 1/(nlnn)
512 24.67 24.64 7.40 7.35 2.28 2.27 17.91 17.90
768 23.87 23.81 7.28 7.24 2.19 2.18 17.28 17.28
1024 23.53 23.44 7.27 7.19 2.17 2.15 17.21 17.12
1280 23.50 23.25 7.41 7.20 2.19 2.15 17.70 17.28
1536 23.66 23.13 7.65 7.24 2.31 2.19 18.86 17.72
1792 24.20 23.22 7.97 7.30 2.52 2.28 20.74 18.65
2048 24.80 23.26 8.34 7.39 2.85 2.39 23.31 19.82
3072 28.31 23.65 9.97 7.82 5.05 3.15 38.46 27.94
4096 33.18 24.41 11.84 8.29 9.13 4.35 59.51 40.06
5120 37.65 25.01 14.15 8.80 15.79 5.93 84.60 55.12
6144 43.20 25.80 16.64 9.27 25.00 8.03 112.30 71.08
7168 48.07 26.39 18.99 9.73 36.01 10.26 140.34 89.79
8192 52.85 27.00 22.06 10.06 49.20 12.80 173.55 108.85
9216 58.46 27.63 26.34 10.65 76.78 16.35 204.10 133.36

Bảng 6: Xác thực tính cần thiết trên các bộ dữ liệu Wikitext-103, Books, Github, WikiBook. Chúng tôi chọn hai RPE không thỏa mãn Định lý 0.4, ví dụ bn=1/n và bn=1/(nlnn). Chúng tôi tăng độ dài chuỗi suy luận từ 512 đến 9216 và tính toán PPL kiểm tra của chúng. PPL của chúng tăng nhanh khi độ dài chuỗi suy luận kéo dài.
