# 2405.14591.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2405.14591.pdf
# File size: 1607077 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Base of RoPE Bounds Context Length
Xin Men∗
Baichuan Inc.Mingyu Xu∗
Baichuan Inc.Bingning Wang∗†
Baichuan Inc.
Qingyu Zhang
ISCASHongyu Lin
ISCASXianpei Han
ISCAS
Weipeng Chen
Baichuan Inc.
Abstract
Position embedding is a core component of current Large Language Models
(LLMs). Rotary position embedding (RoPE), a technique that encodes
the position information with a rotation matrix, has been the de facto
choice for position embedding in many LLMs, such as the Llama series.
RoPE has been further utilized to extend long context capability, which
is roughly based on adjusting the base parameter of RoPE to mitigate out-
of-distribution (OOD) problems in position embedding. However, in this
paper, we find that LLMs may obtain a superficial long-context ability
based on the OOD theory. We revisit the role of RoPE in LLMs and propose
a novel property of long-term decay, we derive that the base of RoPE bounds
context length : there is an absolute lower bound for the base value to obtain
certain context length capability. Our work reveals the relationship between
context length and RoPE base both theoretically and empirically, which
may shed light on future long context training.
103104105106
Context length104105106107108Lower bound of RoPE’s base
y= 0.0424x1.628
Figure 1: Context length and its corresponding lower bound of RoPE’s base value.
∗Equal contribution
†Corresponding author, daniel@baichuan-inc.com
1arXiv:2405.14591v1  [cs.CL]  23 May 2024

--- PAGE 2 ---
1 Introduction
In the past few years, large language models have demonstrated surprising capabilities and
undergone rapid development. By now, LLMs have been widely applied across various
domains, including chatbots, intelligent agents, and code assistants (Achiam et al., 2023;
Jiang et al., 2023b). The Transformer (Vaswani et al., 2017), based on the attention mechanism,
has been the most popular backbone of LLMs due to its good performance and scaling
properties (Tay et al., 2022). One of the key component modules in the Transformer is
position embedding, which is introduced to embed positional information that is vital for
processing sequential data. Rotary position embedding (RoPE), which encodes relative
distance information in the form of absolute position embedding (Su et al., 2024), has been a
popular choice and applied in many LLMs (Touvron et al., 2023a; Yang et al., 2023; Bai et al.,
2023).
RoPE introduces no training parameters and shows improvement in language modeling
and many other tasks (Su et al., 2024; Heo et al., 2024). One reason that RoPE is widely used
is its ability for context length extrapolation (Peng et al., 2023b; Chen et al., 2023), which
extends the context length of a trained LLM without expensive retraining. In practice, many
works (Touvron et al., 2023a; Liu et al., 2024a; Young et al., 2024) have successfully extended
the window length by simply increasing base value, the only one hyper-parameter in RoPE,
and fine-tuning on long texts.
The reasons behind the success of these long context extensions are often explained as
avoiding out-of-distribution (OOD) rotation angles (Liu et al., 2024b; Han et al., 2023) in
RoPE, meaning the extended context length (OOD) can be mapped to the in-distribution
context length that has been properly trained. Based on the OOD theory, a recent study (Liu
et al., 2024b) finds that a smaller base can mitigate OOD and is beneficial for the model’s
ability to process long contexts, which inspires us to further study the relationship between
the base of RoPE and the length of context the model can process.
In this paper, we find that the model may show superficial long context capability with
an inappropriate RoPE base value, in which case the model can only preserve low per-
plexity but loses the ability to retrieve long context information. We also show that the
out-of-distribution (OOD) theory in position embedding, which motivates most length
extrapolation works (Peng et al., 2023b; Chen et al., 2023; Liu et al., 2024b), is insufficient
to fully reflect the model’s ability to process long contexts. Therefore, we revisit the role
of RoPE in LLMs and derive a novel property of long-term decay in RoPE: the ability to
attend more attention to similar tokens than random tokens decays as the relative distance
increases. While previous long context works often focus on the relative scale of the RoPE
base, based on our theory, we derive an absolute lower bound for the base value of RoPE
to obtain a certain context length ability, as shown in Figure 1. To verify our theory, we
conducted thorough experiments on various LLMs such as Llama2-7B (Touvron et al.,
2023b), Baichuan2-7B (Yang et al., 2023) and a 2-billion model we trained from scratch,
demonstrating that this lower bound holds not only in the fine-tuning stage but also in the
pre-training stage.
We summarize the contributions of the paper as follows:
•Theoretical perspective : we derive a novel property of long-term decay in RoPE,
indicating the model’s ability to attend more to similar tokens than random tokens,
which is a new perspective to study the long context capability of the LLMs.
•Lower Bound of RoPE’s Base : to achieve the expected context length capability, we
derive an absolute lower bound for RoPE’s base according to our theory. In short,
the base of RoPE bounds context length.
•Superficial Capability : we reveal that if the RoPE’s base is smaller than a lower
bound, the model may obtain superficial long context capability, which can preserve
low perplexity but lose the ability to retrieve information from long context.
2

--- PAGE 3 ---
2 Background
In this section, we first introduce the Transformer and RoPE, which are most commonly
used in current LLMs. Then we discuss long context methods based on the OOD of rotation
angle theory.
2.1 Attention and RoPE
The LLMs in current are primarily based on the Transformer (Vaswani et al., 2017). The core
component of it is the calculation of the attention mechanism. The naive attention can be
written as:
Aij=qT
ikj (1)
ATTN (X) =softmax (A/√
d)v, (2)
where A∈RL×Lq,k,v∈Rd. Position embedding is introduced to make use of the order of
the sequence in attention.
RoPE (Su et al., 2024) implements relative position embedding through absolute position
embedding, which applies rotation matrix into the calculation of the attention score in Eq. 1,
which can be written as:
Aij= (Ri,θqi)T(Rj,θki) =qT
iRj−i,θkj=qT
iRm,θkj, (3)
where m=j−iis the relative distance of iand j,Rm,θis a rotation matrix denoted as:

cos(mθ0)−sin(mθ0) 0 0 · · · 0 0
sin(mθ0) cos(mθ0) 0 0 · · · 0 0
0 0 cos(mθ1)−sin(mθ1)· · · 0 0
0 0 sin(mθ1) cos(mθ1)· · · 0 0
.....................
0 0 0 0 · · · cos(mθd/2−1)−sin(mθd/2−1)
0 0 0 0 · · · sin(mθd/2−1) cos(mθd/2−1)
(4)
Generally, the selection of rotation angles satisfies θi=base−2i/d, the typical base value for
current LLMs is 10,000, and the base of RoPE in LLMs is shown in Table 1.
Table 1: The setting of RoPE’s base and context length in various LLMs.
Model Llama-7B 38 Llama2-7B 39 Llama3-8B Mistral-7B-v0.2 18 Baichuan2-7B 42
Base 10,000 10,000 500,000 1,000,000 10,000
Length 2,048 4,096 8,192 32,768 4,096
2.2 OOD theory of relative rotation angle
Based on RoPE, researchers have proposed various methods to extend the long context
ability of LLMs, among which representatives are PI (Chen et al., 2023) and NTK-series
(NTK-aware (bloc97, 2023), YaRN (Peng et al., 2023b), and Dynamical-NTK (emozilla, 2023)).
Those methods depend on the relative scale s=Tnew/Torigin , where Torigin is the training
length of the original pre-trained model and Tnewis the training length in long-context
fine-tuning.
PI PI directly interpolates the position embedding, and the calculation of Aijbecomes:
Aij= (Ri/sqi)T(Rj/ski) =qT
iR(j−i)/skj=qT
iRm/skj, (5)
In other words, the position embedding of the token at position iin pre-training becomes
i/sin fine-tuning, ensuring the position embedding range of the longer context remains the
same as before.
3

--- PAGE 4 ---
0 10000 20000 30000
Context-size−1.0−0.50.00.51.0RoPEExtended context(a) base=1e4
0 10000 20000 30000
Context-size−1.0−0.50.00.51.0
Extended context (b) base=500
0 10000 20000 30000
Context-size0.900.920.940.960.981.00
Extended context (c) base= b·sd
d−2
Figure 2: An illustration of OOD in RoPE when we extend context length from 4k to 32k, and
two solutions to avoid the OOD. We show the last dimension as it is the lowest frequency
part of RoPE, which suffers OOD mostly in extrapolation. (a) For a 4k context-length model
with base value as 1e4, when we extend the context length to 32k without changing the
base value, the context length from 4k to 32k is OOD for RoPE (red area in the figure).
(b) OOD can be avoided with a small base value like 500 (Liu et al., 2024b), since the full
period has been fitted during fine-tuning stage. (c) We set base as b·sd
d−2from NTK (Peng
et al., 2023b).The blue line denotes the pre-training stage (base=1e4) and the red dashed line
denotes the fine-tuning stage (base= b·sd
d−2), we can observe that the RoPE’s rotation angle
of extended positions is in-distribution.
NTK-series The idea is that neural networks are difficult to learn high-frequency features,
and direct interpolation can affect the high-frequency parts. Therefore, the NTK-aware
method achieves high-frequency extrapolation and low-frequency interpolation by modify-
ing the base value of RoPE. Specifically, it modifies the base bof the RoPE to:
bnew=b sd
d−2. (6)
The derivation of this expression is derived from Tnewb−d−2
dnew =Torigin b−d−2
dto ensure that
the lowest frequency part being interpolated.
A recent study (Liu et al., 2024b) proposes to set a much smaller base (e.g. 500), in which
case θi=base−2i
dis small enough and typical training length (say 4,096) fully covers the
period of cos (t−s)θi, so the model can obtain longer context capabilities.
One perspective to explain current extrapolation methods is the OOD of rotation angle (Liu
et al., 2024b; Han et al., 2023). If all possible values of cos(t−s)θihave been fitted during
the pre-training stage, OOD would be avoided when processing longer context. Figure 2
demonstrates how these methods avoid OOD of RoPE.
3 Motivation
NTK-based methods are widely adopted in long-context extension (Touvron et al., 2023a;
Liu et al., 2024a; Young et al., 2024). To obtain better long-context capability, however,
practitioners often adopt a much larger base than the original NTK-aware method suggested.
This leads to speculation that there is another bound of RoPE’s base determined by context
length.
On the other hand, a recent work (Liu et al., 2024b) proposes to set a much smaller base for
RoPE to extend the context length. However, we find it may be a superficial long-context
capability as shown in Figure 3. This method can obtain a low perplexity even at 128k
context length, which can be explained by the OOD theory as explained above, but the
model could not retrieve related information for context length as short as 1k, even much
shorter than the model’s pre-trained length. Our findings support previous research (Hu
4

--- PAGE 5 ---
0 25000 50000 75000 100000 125000
Context size20406080100Perplexityﬁnetune on 32k(base=500)
Llama2-7B-Baseline(a) Perplexity
1000 2000 3000 4000 5000
Context length0.00.20.40.60.81.0Accuracyﬁnetune on 32k(base=500)
Llama2-7B-Baseline (b) Long-eval (Li* et al.,
2023)
01002003004005006007008009001000110012001300140015001600170018001900200021002200230024002500260027002800290030003100320033003400350036003700380039004000
Token Limit0.0
11.0
22.0
33.0
44.0
56.0
67.0
78.0
89.0
100.0Context length
0246810
Score
(c) Needle in Haystack (G, 2023)
Figure 3: The superficial long context capability of avoiding OOD by the smaller base.
Following the recent work (Liu et al., 2024b), we fine-tune Llama2-7B with a small base (500)
to a context length of 32k.
et al., 2024) on the limitations of perplexity in evaluating long-context abilities. To delve
deep into this phenomenon, we do the theoretical exploration in the next section.
4 Theory Perspective
For attention mechanism in language modeling, we have the following desiderata:
Desiderata 1 The closer token gets more attention : the current token tends to pay more attention
to the token that has a smaller relative distance.
Desiderata 2 The similar token gets more attention : the token tends to pay more attention to
the token whose key value is more similar to the query value of the current token.
Then we examine the desiderata when we apply RoPE to the attention mechanism in LLMs.
4.1 Long-term Decay of Upper Bound of Attention Score
For Desiderata 1, the property of RoPE makes the model attend more to closer tokens. This
kind of long-term decay has been thoroughly discussed in previous work (Su et al., 2024;
Sun et al., 2022). It comes from the upper bound of attention score calculation, which can be
written as:
|Aij|=|qT
iRmkj| ≤max
l(|hl−hl+1|)d/2
∑
n=1|Sn|
=max
l(|hl−hl+1|)d/2
∑
n=1|n−1
∑
l=0e(j−i)θl√−1|, (7)
where hl=qT
i[2l:l2+1]kj[2l:2l+1]. Equation 7 indicates that the upper bound of the
attention score |Aij|decays as the relative distance increases. Figure 4 shows the long-term
decay curve of this upper bound, which is in accordance with previous findings (Su et al.,
2024; Sun et al., 2022).
4.2 Long-term Decay of the Ability to Attend More to Similar Tokens than Random
Tokens
In addition to the attention score’s upper bound, we also find there exists another long-term
decay property in RoPE: the ability to attend more to similar tokens than random tokens
decays as the relative distance increases. We define the ability to attend more to similar
tokens than random tokens as:
Eq,k∗h
qTRm,θk∗i
−Eq,kh
qTRm,θki
, (8)
5

--- PAGE 6 ---
0 2000 4000
Relative distance2.55.07.510.012.515.0Relative upper boundbase:1e4
base:1e2
base:1e3
0 10000 20000 30000
Relative distance51015base:1e4
base:1e5
base:1e6Figure 4: The upper bound of attention score
with respect to the relative distance.
0 2000 4000
Relative distance0.00.20.40.6Bm,θbase:1e4
base:1e2
base:1e3
0 10000 20000 30000
Relative distance−0.20.00.20.40.6base:1e4
base:1e5
base:1e6Figure 5: The ability to attend more to simi-
lar tokens than random tokens.
where q∈Rdis the query vector for the current token, k∗=q+ϵis the key value of a
similar token, where ϵis a small random variable, k∈Rdis the key vector of a random
token, Rm,θis the rotation matrix in RoPE. The first term in Eq. 8 is the attention score of q
and a similar token k∗, the second term in Eq. 8 is the attention score of qand random token
k. Then we derive the following theorem:
Theorem 1 Assuming that the components of query q∈Rdand key k∈Rdare independent and
identically distributed, their standard deviations are denoted as σ∈R. The key k∗=q+ϵis a
token similar to the query, where ϵis a random variable with a mean of 0. Then we have:
1
2σ2(Eq,k∗h
qTRm,θk∗i
−Eq,kh
qTRm,θki
) =d/2−1
∑
i=0cos(mθi) (9)
The proof is shown in Appendix A. We denote ∑d/2−1
i=0cos(mθi)asBm,θ, and according to
Theorem 1, Bm,θmeasures the ability to give more attention to similar tokens than random
tokens, which decreases as the relative distance mincreases, as shown in Figure 5. For a
very small base value, we can observe that the Bm,θis even below zero at a certain distance,
meaning the random tokens have larger attention scores than the similar tokens, which may
be problematic for long context modeling.
4.3 Base of RoPE Bounds the Context Length
To satisfy the Desiderata 2, we will get Eq,k∗
qTRm,θk∗≥Eq,k
qTRm,θk
. According to
Theorem 1, Bm,θneeds to be larger than zero. Given the θin RoPE, the context length Lθ
that can be truly obtained satisfies:
Lθ=sup{L|Bm,θ≥0,∀m∈[0, 1, ..., L]} (10)
In other word, if we follow the setting that θi=base−2i/d, in order to get the expected
context length L, there is a lower bound of the base value base L:
base L=inf{base|Bm,θ≥0,∀m∈[0, 1, ..., L]} (11)
In summary, the RoPE’s base determines the upper bound of context length the model
can truly obtain. Although there exists the absolute lower bound, Eq. 9 and Eq. 11 are
hard to get the closed-form solution since Bm,θis a summation of many cosine functions.
Therefore, in this paper, we get the numerical solution. Table 2 shows this lower bound for
context length ranging from 1,000 to one million. In Figure 1, we plot the context length
and corresponding lower bound, we can observe that as the context length increases, the
required base also increases.
Note: this boundary is not very strict because the stacking of layers in LLMs allows the model to
extract information beyond the single layers’ range, which may increase the context length in Eq. 10
and decrease the base in Eq. 11 . Notwithstanding, in Section 5 we find that the derived bound
approximates the real context length in practice.
Long-term decay from different perspectives. The long-term decay in section 4.1 and
section 4.2 are from different perspectives. The former refers to the long-term decay of the
6

--- PAGE 7 ---
Table 2: Context length and its corresponding lower bound of RoPE’s base.
Context Len. 1k 2k 4k 8k 16k 32k 64k 128k 256k 512k 1M
Lower Bound 4.3e3 1.6e4 2.7e4 8.4e4 3.1e5 6.4e5 2.1e6 7.8e6 3.6e7 6.4e7 5.1e8
attention score as the relative distance increases. This ensures that current tokens tend to pay
more attention to the tokens closer to them. The latter indicates that with the introduction of
the rotation matrix in attention, the ability to discriminate the relevant tokens from irrelevant
tokens decreases as the relative distance increases. Therefore, a large Bm,θ, corresponding to
a large base value, is important to keep the model’s discrimination ability in long context
modeling.
5 Experiment
In this section, we conduct thorough experiments. The empirical result can be summarized
in Table 3, the details are in the following sections.
Table 3: In Section 5, we aim to answer the following questions.
Questions Answers
Q: Does RoPE’s base bounds the context
length during the fine-tuning stage?Yes. When the base is small, it is difficult to get extrapolation
for specific context length.
Q: Does RoPE’s base bounds the context
length during the pre-training stage?Yes. Our proposed lower bound for RoPE’s base also applies
to pre-training. If we train a model from scratch with a small
base but the context length is large (larger than the bounded
length), the resulting model has very limited the context length
capabilities, meaning some of context in pre-training is wasted.
Q: What happened when base is set
smaller than the lower bound?The model will get the superficial long context capability.
The model can keep perplexity low, but can’t retrieve useful
information from long context.
5.1 Experiments Setup
For fine-tuning, we utilized Llama2-7B (Touvron et al., 2023a) and Baichuan2-7B (Yang et al.,
2023), both of which are popular open-source models employing RoPE with a base of 1e4.
We utilized a fixed learning rate of 2e-5 and a global batch size of 128 and fine-tuning for
1000 steps. For pre-training, we trained a Llama-like 2B model from scratch for a total of
1 trillion tokens. We set the learning rate to 1e-4 and adopted a cosine decay schedule,
with models trained on a total of 1T tokens. The dataset we used is a subset of RedPajama
(Computer, 2023). More details of the experimental setup are provided in Appendix B.
Our evaluation focused on two aspects: (1) Perplexity : we use PG19 dataset (Rae et al., 2019)
which are often used in long context evaluation; (2) Retrieval : in addition to perplexity,
we also adopt retrieval since it represents the real long-context understanding ability of
LLMs. We choose a) Long-eval benchmark from (Li* et al., 2023) and b) needle in a haystack
(NIH) (G, 2023). The Long-eval benchmark generates numerous random similar sentences
and asks the model to answer questions based on a specific sentence within the context,
while the NIH requires the model to retrieve information from various positions in the long
context.
5.2 Base of RoPE bounds context length in fine-tuning stages
According to Eq. 11, there is a lower bound of RoPE’s base determined by expected context
length. We fine-tune Llama2-7b-Base on 32k context with varying bases. As depicted in
Figure 6, although the difference in perplexity between different bases is negligible, the
accuracy of Long-eval varies significantly. In Figure 6b, the dotted line denotes the lower
bound derived from Eq. 11, below which the Long-eval accuracy declines significantly.
7

--- PAGE 8 ---
5000 10000 15000 20000 25000 30000
Context68101214161820Perplexity32K-base:1e4
32K-base:2e5
32K-base:9e5
32K-base:5e6
32K-base:1e9
32K-base:1e12
4K-Baseline(a) Perplexity
1e4 2e59e55e6 1e9 1e12
Base value0.00.10.20.3Accuracy on 32K
Lower bound (b) Long-eval 32k
Figure 6: Fine-tuning Llama2-7B-Base on 32k context length with varying RoPE’s base.
Although the perplexity remains low with varying bases, the Long-eval accuracy reveals
a discernible bound for the base value, below which the Long-eval accuracy declines
significantly. The dotted line denotes the lower bound derived from Eq. 11.
0 1000 2000 3000 4000
Context1015202530Perplexity
1000 2000 3000 4000 5000
Context length0.00.10.20.30.40.5Accuracy
01022043064085106127148169181020112212241327142915311633173518371939204121432245234724492551265327552857295930613163326533673469357136733776387839804082418442864388449045924694479648985000
Token Limit0.0
5.0
11.0
16.0
21.0
26.0
32.0
37.0
42.0
47.0
53.0
58.0
63.0
68.0
74.0
79.0
84.0
89.0
95.0
100.0Context length
0246810
Score
0 1000 2000 3000 4000
Context1015202530Perplexity
1000 2000 3000 4000 5000
Context length0.00.10.20.30.40.50.6Accuracy
01022043064085106127148169181020112212241327142915311633173518371939204121432245234724492551265327552857295930613163326533673469357136733776387839804082418442864388449045924694479648985000
Token Limit0.0
5.0
11.0
16.0
21.0
26.0
32.0
37.0
42.0
47.0
53.0
58.0
63.0
68.0
74.0
79.0
84.0
89.0
95.0
100.0Context length
0246810
Score
0 1000 2000 3000 4000
Context1015202530Perplexity
1000 2000 3000 4000 5000
Context length0.00.10.20.30.40.5Accuracy
01022043064085106127148169181020112212241327142915311633173518371939204121432245234724492551265327552857295930613163326533673469357136733776387839804082418442864388449045924694479648985000
Token Limit0.0
5.0
11.0
16.0
21.0
26.0
32.0
37.0
42.0
47.0
53.0
58.0
63.0
68.0
74.0
79.0
84.0
89.0
95.0
100.0Context length
0246810
Score
Figure 7: The first row: the results of a 2B model training from scratch with base=1e2. The
second row: The results of fine-tuning the 2B model with base=1e4. The third row: The
results of fine-tuning the 2B model with base=1e6.
Additional results are provided in Appendix C. Notably, this empirically observed lower
bound closely aligns with our theoretical derivation. On the other hand, we can see that
base=2e5achieves the best perplexity, but the accuracy of Long-eval is very low, which
indicates the limitations of perplexity in evaluating long context capabilities.
5.3 The Base of RoPE bounds context length in pre-training stages
According to and Theorem 1 and Eq. 11 , this constraints could also apply to pre-training
stage. To validate this, we trained a 2B model from scratch with RoPE base=100. The results,
depicted in the first row of Figure 7, indicate that even though the model was trained with a
8

--- PAGE 9 ---
context length of 4,096 tokens, it was capable of retrieving information from only the most
recent approximately 500 tokens. This demonstrates that the base parameter bounds the
context length during the pre-training stage as well. We define the context length from
which the model can effectively retrieve information as the effective context length.
And according to our theory, the effective context length can be extended as the RoPE’s
base increases. To validate this, we further fine-tune this 2B model on 32k context length,
with RoPE’s base set to 1e4, as shown in the second row of Figure 7. While the effective
context length increased, it remains significantly below 32k since the effective context length
bounded by base=1e4 is much smaller than 32k. Further, when we increase the base to 1e6
and fine-tune the base 2B model on 32K (the third row in Figure 7), the model could obtain
a larger context length than base=1e4, which is in accodance with our theory.
To further remove the influence of model size, we also fine-tuned a larger 7B model on a
32k context length with a RoPE base set to 1e4 and observed an effective context length
nearly identical to that of the 2B model with the same RoPE base (see Appendix D). This is
empirical proof that the effective context length is determined by RoPE’s base.
5.4 Interpretation for the superficial long context capability for small base
Based on our theory and empirical observations, it is easy to explain what happens in Figure
3.
Better Extrapolation (Perplexity)? Due to the small base, Bm,θcan be smaller than zero as m
increases, which is shown in Figure 5. The model can’t attend more to similar tokens than
random tokens with a large relative distance, so the model tends to focus more on nearby
tokens, this will lead to a smaller empirical receptive field, even smaller than the training
length. In this case, the model has a strong ability to maintain perplexity stability (Chi et al.,
2023).
Worse Ability (Long-eval and NIH)! According to our previous analysis, RoPE’s base
bounds the context length, and the context length bounded by 500 is much lower than that
bound by 10,000. Therefore, when the base is set to 500, the effective context length drops
sharply, even after training on 32k context length.
5.5 OOD theory is insufficient to reveal long context capability
Table 4: The comparison of "Method 1" and "Method 2". These methods are designed
carefully. They both are no OOD, but they are very different under our theory.
Method OODLong-eval numbers of mwhose Bm,θ≥0
15k 30k 15k 30k
Method 1 0.33 0.27 0 0
Method 2 0.40 0.00 97 2554
Section 3 mentions that methods based on the OOD theory of rotation angles may not
fully reflect the long context capability. In this section, we conduct further experiments to
substantiate and explain this observation. We present two methods to extend the context
length of Llama2 from 4k to 32k. Both of them are devoid of OOD angles. These methods
are delineated mathematically as follows:
• Method 1: θi= (5e6)−2i/d,
• Method 2: θi=(
(1e4)−2i/128/8, i≥44
(1e4∗8128/88)−2i/128,i<44.
We can see from Table 4 that these two methods exhibit significantly different long context
capabilities. Under the perspective of OOD rotation angle, both methods avoid OOD
rotation angle, suggesting effective extrapolation. However, despite being trained on a
9

--- PAGE 10 ---
context length of 32k, "method 2" struggles in completing the retrieval task at a context
length of 32k. This phenomenon is beyond the scope which the OOD theory can explain.
Under our perspective, "method 2" is severely violating Bm,θ≥0when m∈[15k, 30k],
thereby impeding its ability to achieve long-context discrimination. We speculate that the
model may achieve better extrapolation in the fine-tuning stage if the base is sufficiently
large to surpass a lower bound and avoid OOD of rotation angles.
6 Related Work
Position embedding. Since its introduction, Transformer (Vaswani et al., 2017) has
achieved remarkable results in the field of natural language processing. To make full
use of the order of sequence, researchers have introduced position embedding. The ear-
liest position embedding was based on sinusoidal functions (Vaswani et al., 2017) for
absolute positions, learnable absolute position embedding (Devlin et al., 2018) and many
variants (Kiyono et al., 2021; Li et al., 2019) were proposed. Nevertheless, absolute position
embedding has difficulties in extending directly to texts longer than the training length.
Subsequently, researchers proposed relative position embedding methods (Shaw et al., 2018;
Ke et al., 2020). With the development of large language models, rotary position embedding
and its variants (Su et al., 2024; Sun et al., 2022) has become widely used, such as Llama2
(Touvron et al., 2023a), Baichuan2 (Yang et al., 2023), Mistral-7B-(Jiang et al., 2023a). A recent
study reveals that no position embedding is also potential (Kazemnejad et al., 2024).
Long context learning. Implementing models with longer or even infinitely long contexts
has always been an important goal in the field of natural language processing. Due to the
squared complexity of the transformer model over time, a significant portion of the work
focuses on improving the model structure (Gu & Dao, 2023;?; Peng et al., 2023a; Qin et al.,
2024). However, most of the work is still based on the transformer architecture. The other
part of the work is aimed at reducing the computational complexity of attention itself, such
as sparse attention (Beltagy et al., 2020) and group query attention (Ainslie et al., 2023). In
addition, there are also some optimizations in engineering efficiency, such as flash attention
(Dao et al., 2022) and ring attention (Liu et al., 2023). In the model inference stage, to save
time and space, there are also some methods for accelerating long context, such as KV
cache compression (Hooper et al., 2024), etc. And the position embedding is important in
extrapolation. In the process of fine-tuning, methods such as PI (Chen et al., 2023), NTK, and
YARN (Peng et al., 2023b) are used to change the original position embedding information.
FoT (Tworkowski et al., 2024) assigns the position information of the tokens outside the
local context as the first token in the local context.
7 Limitation
In this work, we investigate the relationship between the base of RoPE and context length.
Although we have derived that there exists a lower bound for the base of RoPE determined
by context length, the existence of the upper bound for RoPE’s base remains an open
question that warrants further exploration. In addition, because of the lack of effective
benchmarks for assessing long-context capabilities, the scope of long-context capabilities
discussed in this paper may be limited.
8 Conclusion
Our work presents a comprehensive study on the role of RoPE in LLMs for effectively
modeling long context. Our main contribution lies in uncovering a novel property of RoPE
through theoretical analysis, demonstrating that as the relative distance between tokens
increases, the model’s ability to attend more to similar tokens decreases. According to our
theory, we derive a lower bound for RoPE’s base in accommodating to expected context
lengths. Our experimental results validate that the base of RoPE bounds context length for
not only fine-tuning but also the pre-training stage. Our theory offers a new perspective on
10

--- PAGE 11 ---
understanding the functionality of RoPE in long-context modeling. By shedding light on
the relationship between context length and position embedding, we hope our work could
provide insights for enhancing the long context capability of LLMs.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron,
and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from
multi-head checkpoints. In The 2023 Conference on Empirical Methods in Natural Language
Processing , 2023.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin
Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 ,
2023.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-
former. arXiv preprint arXiv:2004.05150 , 2020.
bloc97. Ntk-aware scaled rope allows llama models to have extended (8k+)
context size without any fine-tuning and minimal perplexity degradation.
https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_
rope_allows_llama_models_to_have/ , 2023.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending con-
text window of large language models via positional interpolation. arXiv preprint
arXiv:2306.15595 , 2023.
Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting trans-
former length extrapolation via the lens of receptive field analysis. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pp. 13522–13537, 2023.
Together Computer. Redpajama: An open source recipe to reproduce llama training dataset,
April 2023. URL https://github.com/togethercomputer/RedPajama-Data .
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast
and memory-efficient exact attention with io-awareness. Advances in Neural Information
Processing Systems , 35:16344–16359, 2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018.
emozilla. Dynamically scaled rope further increases performance of long context llama
with zero fine-tuning. https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/
dynamically_scaled_rope_further_increases/ , 2023.
Kamradt G. Needle in a haystack - pressure testing llms. https://github.com/gkamradt/
LLMTest_NeedleInAHaystack , 2023.
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
arXiv preprint arXiv:2312.00752 , 2023.
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite:
Simple on-the-fly length generalization for large language models. arXiv preprint
arXiv:2308.16137 , 2023.
Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding
for vision transformer. arXiv preprint arXiv:2403.13298 , 2024.
11

--- PAGE 12 ---
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia
Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm
inference with kv cache quantization. arXiv preprint arXiv:2401.18079 , 2024.
Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, and Yansong Feng. Can perplexity
reflect large language model’s ability in long text understanding? In The Second Tiny
Papers Track at ICLR 2024 , 2024.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut
Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023a.
Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. Impact of code language models on
automated program repair. In 2023 IEEE/ACM 45th International Conference on Software
Engineering (ICSE) , pp. 1430–1442. IEEE, 2023b.
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and
Siva Reddy. The impact of positional encoding on length generalization in transformers.
Advances in Neural Information Processing Systems , 36, 2024.
Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training.
InInternational Conference on Learning Representations , 2020.
Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute
position embedding for transformers. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , pp. 3309–3321, 2021.
Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion
Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on
context length?, June 2023. URL https://lmsys.org/blog/2023-06-29-longchat .
Hailiang Li, YC Adele, Yang Liu, Du Tang, Zhibin Lei, and Wenye Li. An augmented
transformer architecture for natural language generation tasks. In 2019 International
Conference on Data Mining Workshops (ICDMW) , pp. 1–7. IEEE, 2019.
Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for
near-infinite context. In NeurIPS 2023 Foundation Models for Decision Making Workshop ,
2023.
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length
video and language with ringattention. arXiv preprint arXiv:2402.08268 , 2024a.
Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of roPE-
based extrapolation. In The Twelfth International Conference on Learning Representations ,
2024b. URL https://openreview.net/forum?id=JO7k0SJ5V6 .
Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for
transformers. Advances in Neural Information Processing Systems , 36, 2024.
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman,
Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns
for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP
2023 , pp. 14048–14077, 2023a.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context
window extension of large language models. In The Twelfth International Conference on
Learning Representations , 2023b.
Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network
for sequence modeling. Advances in Neural Information Processing Systems , 36, 2024.
12

--- PAGE 13 ---
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.
Compressive transformers for long-range sequence modelling. In International Conference
on Learning Representations , 2019.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position
representations. In Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short
Papers) , pp. 464–468, 2018.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and
Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using
model parallelism, 2020.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. Neurocomputing , 568:127063, 2024.
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav
Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. arXiv preprint
arXiv:2212.10554 , 2022.
Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao,
Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model
architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551 ,
2022.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and
efficient foundation language models, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning
Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat
models, 2023b.
Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk
Michalewski, and Piotr Miło´ s. Focused transformer: Contrastive training for context
scaling. Advances in Neural Information Processing Systems , 36, 2024.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems , 30, 2017.
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv,
Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models.
arXiv preprint arXiv:2309.10305 , 2023.
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai.
arXiv preprint arXiv:2403.04652 , 2024.
13

--- PAGE 14 ---
A The proof of Theorem 1.
Assuming that the components of query q∈Rdand key k∈Rdare independent, their
standard deviations are denoted as σ∈Rdand the means are donated as µ∈Rd. The key
k∗similar to qisq+ϵ, where ϵis a random variable with a mean of 0. Then, we have:
Eq,k∗qTRmk∗−Eq,kqTRmk
=EqqTRmq+Eq,ϵqTRmϵ−Eq,kqTRmk
=Eqd/2−1
∑
i=0(q2
2icos(mθi)−q2iq2i+1sin(mθi) +q2i+1q2isin(mθi) +q2
2i+1cos(mθi)) +EqqTRmEϵϵ
−Eq,kd/2−1
∑
i=0(q2ik2icos(mθi)−q2ik2i+1sin(mθi) +q2i+1k2isin(mθi) +q2i+1k2i+1cos(mθi))
=d/2−1
∑
i=0E(q2
2i)cos(mθi)−µ2iµ2i+1sin(mθi) +µ2iµ2i+1sin(mθi) +E(q2
2i+1)cos(mθi)) + µRm0
−d/2−1
∑
i=0(µ2
2icos(mθi)−µ2iµ2i+1sin(mθi) +µiµ2i+1sin(mθi) +µ2
2i+1cos(mθi))
=d/2−1
∑
i=0(E(q2
2i+q2
2i+1)−µ2
2i−µ2
2i+1)cos(mθi)
=d/2−1
∑
i=0(σ2
i+σ2
i+1)cos(mθi) (12)
Then we can get:
d/2−1
∑
i=0(σ2
2i+σ2
2i+1)cos(mθi) =Eq,k∗qTRmk∗−Eq,kqTRmk (13)
And when all σare equal, we can get:
d/2−1
∑
i=0cos(mθi) =1
2σ2(Eq,k∗qTRmk∗−Eq,kqTRmk) (14)
B The detail setting of experiment.
For training, we mainly conducted experiments on Llama2-7B (Touvron et al., 2023a) and
Baichuan2-7B (Yang et al., 2023). In addition, we also trained a 2B model from scratch, whose
structure is the same with Baichuan2-7B-Base but with a smaller hidden size = 2048. Both
training and testing are accelerated by FlashAttention-2 (Dao et al., 2022) and Megatron-LM
(Shoeybi et al., 2020). The dataset of both fine-tuning and training from scratch is a subset of
RedPajama (Computer, 2023). The hyper parameters of training are list in Appendix 5. All
experiments are conducted on a cluster of 16 machines with 128 NVIDIA A100 80G.
For evaluation, we test the long context capabilities comprehensively, the benchmarks are
listed below: perplexity on PG19 (Rae et al., 2019) test split. We evaluate the perplexity of
each sample and get the mean value across samples.
Long-eval (Li* et al., 2023). This test generates massive random similar sentences and asks
the model to answer questions according to a specific sentence in the context. Because the
long context consists of many similar patterns, it’s more difficult to get the right answer.
We find this test is harder than other long context evaluations such as Perplexity, Passkey
14

--- PAGE 15 ---
Table 5: Training hyper-parameters in our experiments
Model Training length Training tokens Batchsize Base LR LR decay Weight decay
Llama2-7B-Base 32K 4B 128 2e5 constant 0
Baichuan2-7B-Base 32K 4B 128 2e5 constant 0
Our-2B-Base 4K 1T 1024 2e4 cosine 0.1
Question: Below is a record of lines I want you to remember. Each line begins with 'line <line index>' and contains a '<REGISTER_CONTENT>' at the end of the line as a numerical value. For each line index, memorize its corresponding <REGISTER_CONTENT>. At the end of the record, I will ask you to retrieve the corresponding <REGISTER_CONTENT> of a certain line index. Now the record start:…line swift-baby: REGISTER_CONTENT is <12821>line dangerous-breast: REGISTER_CONTENT is <28051>line bad-sculptural: REGISTER_CONTENT is <32916>line flashy-college: REGISTER_CONTENT is <34027>line voiceless-brochure: REGISTER_CONTENT is <8964>line fast-peony: REGISTER_CONTENT is <5218>…Now the record is over. Tell me what is the <REGISTER_CONTENT> in line dangerous-breast? I need the number. Answer:
Figure 8: Long-eval sample prompt
Retrieval (Mohtashami & Jaggi, 2024), Needle in Haystack (G, 2023). A test sample is list in
Figure 8
needle in haystack(NIH) (G, 2023). NIH tests the long context capability not only under
different context lengths but also at different positions where the correct answer is located
in the context, which provides a more detailed view of the long context capability.
C Baichuan2-7B-Base: Lower bound Base of RoPE
5000 10000 15000 20000 25000 30000
Context68101214161820Perplexity32K-base:1e4
32K-base:2e5
32K-base:9e5
32K-base:5e6
32K-base:1e9
32K-base:1e12
4K-Baseline
(a) Perplexity
1e4 2e59e55e6 1e9 1e12
Base value0.00.10.20.30.40.50.6Accuracy on 32K
Lower bound (b) Long-eval 32k
Figure 9: Fine-tuning Baichuan2-7B-Base on 32k context length with varying RoPE’s base.
Although the perplexity remains low with varying bases, the Long-eval accuracy revels
a discernible bound for the base value, below which the Long-eval accuracy declines
significantly. the dotted line denotes the lower bound derived from Eq. 11.
15

--- PAGE 16 ---
D Long Context Test Results on Various LLMs
0 10000 20000 30000
Context size68101214Perplexity
0 2500 5000 7500 10000 12500
Context length0.20.40.60.81.0Accuracy
1000200030004000500060007000800090001000011000120001300014000150001600017000180001900020000210002200023000240002500026000270002800029000300003100032000
Token Limit0.0
5.0
11.0
16.0
21.0
26.0
32.0
37.0
42.0
47.0
53.0
58.0
63.0
68.0
74.0
79.0
84.0
89.0
95.0
100.0Context length
0246810
Score
Figure 10: Llama2-7B-Base with base=1e4 fine-tuned on 32k context (original context=4096)
0 10000 20000 30000
Context size68101214Perplexity
0 2500 5000 7500 10000 12500
Context length0.20.40.60.81.0Accuracy
1000 2000 3000 4000 5000 6000 7000 8000 900010000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000
Token Limit0.0
4.0
8.0
12.0
16.0
20.0
24.0
29.0
33.0
37.0
41.0
45.0
49.0
53.0
57.0
61.0
65.0
69.0
73.0
78.0
82.0
86.0
90.0
94.0
98.0Context length
0246810
Score
Figure 11: Llama2-7B-Base with base=2e5 fine-tuned on 32k context (original context=4096)
0 10000 20000 30000
Context8101214Perplexity
0 2500 5000 7500 10000 12500
Context length0.00.20.40.60.81.0Accuracy
1000200030004000500060007000800090001000011000120001300014000150001600017000180001900020000210002200023000240002500026000270002800029000300003100032000
Token Limit0.0
4.0
8.0
12.0
16.0
20.0
24.0
29.0
33.0
37.0
41.0
45.0
49.0
53.0
57.0
61.0
65.0
69.0
73.0
78.0
82.0
86.0
90.0
94.0
98.0Context length
0246810
Score
Figure 12: Baichuan2-7B-Base with base=1e4 fine-tuned on 32k context (original con-
text=4096)
16

--- PAGE 17 ---
0 10000 20000 30000
Context size68101214Perplexity
0 2500 5000 7500 10000 12500
Context length0.20.40.60.8Accuracy
1000200030004000500060007000800090001000011000120001300014000150001600017000180001900020000210002200023000240002500026000270002800029000300003100032000
Token Limit0.0
5.0
11.0
16.0
21.0
26.0
32.0
37.0
42.0
47.0
53.0
58.0
63.0
68.0
74.0
79.0
84.0
89.0
95.0
100.0Context length
0246810
Score
Figure 13: Baichuan2-7B-Base with base=2e5 fine-tuned on 32k context (original con-
text=4096)
0 10000 20000 30000
Context8101214Perplexity
0 2000 4000 6000
Context length0.20.40.60.81.0Accuracy
1000 2000 3000 4000 5000 6000 7000 8000 900010000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000
Token Limit0.0
5.0
11.0
16.0
21.0
26.0
32.0
37.0
42.0
47.0
53.0
58.0
63.0
68.0
74.0
79.0
84.0
89.0
95.0
100.0Context length
0246810
Score
Figure 14: Qwen1.5-7B-Base (Bai et al., 2023) with base=1e4 fine-tuned on 32k context
(original context=4096)
17
