# CAT-LM
 Huấn luyện Mô hình Ngôn ngữ trên Mã và Bài kiểm tra Được căn chỉnh

Nikitha Rao∗, Kush Jain∗, Uri Alon, Claire Le Goues, Vincent J. Hellendoorn
Carnegie Mellon University
United States
{nikitharao, kdjain, urialon, legoues, vhellendoorn }@cmu.edu

Tóm tắt — Kiểm thử là một phần không thể thiếu nhưng thường bị bỏ qua trong quá trình phát triển phần mềm. Các công cụ tạo bài kiểm tra cổ điển như EvoSuite tạo ra các bộ bài kiểm tra hành vi bằng cách tối ưu hóa cho độ bao phủ, nhưng có xu hướng tạo ra các bài kiểm tra khó hiểu. Các mô hình ngôn ngữ được huấn luyện trên mã có thể tạo ra mã rất giống với mã được viết bởi con người, nhưng các mô hình hiện tại được huấn luyện để tạo từng tệp riêng biệt, như là thực hành tiêu chuẩn trong xử lý ngôn ngữ tự nhiên, và do đó không thể xem xét ngữ cảnh mã-dưới-kiểm-thử khi tạo ra tệp kiểm tra. Trong công trình này, chúng tôi đề xuất Mô hình Ngôn ngữ Mã và Bài kiểm tra Được căn chỉnh (CAT-LM), một mô hình ngôn ngữ kiểu GPT với 2.7 tỷ tham số, được huấn luyện trên một kho dữ liệu các dự án Python và Java. Chúng tôi sử dụng một tín hiệu tiền huấn luyện mới rõ ràng xem xét ánh xạ giữa mã và các tệp kiểm tra khi có sẵn. Chúng tôi cũng tăng mạnh chiều dài chuỗi đầu vào tối đa lên 8,192 token, gấp 4 lần so với các mô hình tạo mã điển hình, để đảm bảo ngữ cảnh mã có sẵn cho mô hình khi tạo mã kiểm tra. Chúng tôi phân tích tính hữu ích của nó cho các ứng dụng thực tế, cho thấy việc lấy mẫu có lọc (ví dụ: bằng khả năng biên dịch, độ bao phủ) cho phép nó hiệu quả tạo ra các bài kiểm tra đạt được độ bao phủ tương tự như những bài được viết bởi các nhà phát triển trong khi giống với phong cách viết của họ. Bằng cách sử dụng ngữ cảnh mã, CAT-LM tạo ra nhiều bài kiểm tra hợp lệ hơn ngay cả các mô hình ngôn ngữ lớn hơn nhiều được huấn luyện với nhiều dữ liệu hơn (CodeGen 16B và StarCoder) và vượt trội đáng kể so với một mô hình dành riêng cho kiểm tra gần đây (TeCo) trong việc hoàn thành bài kiểm tra. Nhìn chung, công trình của chúng tôi làm nổi bật tầm quan trọng của việc kết hợp các hiểu biết cụ thể về phần mềm khi huấn luyện các mô hình ngôn ngữ cho mã và mở đường cho việc tạo bài kiểm tra tự động mạnh mẽ hơn.

Từ khóa chỉ mục — tạo bài kiểm tra, hoàn thành bài kiểm tra, mô hình ngôn ngữ lớn, căn chỉnh mã-kiểm tra

I. GIỚI THIỆU

Kiểm thử phần mềm là một thành phần quan trọng của quá trình phát triển phần mềm. Trong các dự án được kiểm thử tốt, hầu hết các tệp mã được ghép nối với ít nhất một tệp kiểm tra tương ứng triển khai các hàm kiểm tra đơn vị và/hoặc tích hợp đánh giá chức năng của mã. Tuy nhiên, viết các bài kiểm tra chất lượng cao có thể tốn thời gian [1], [2] và thường bị bỏ qua một phần hoặc hoàn toàn. Điều này đã dẫn đến công việc rộng rãi trong tạo bài kiểm tra tự động, bao gồm cả các phương pháp cổ điển [3], [4], [5], [6] và dựa trên mạng neural [3], [7], [8].

Các công cụ tạo bài kiểm tra cổ điển như EvoSuite [4] trực tiếp tối ưu hóa để tạo ra các bài kiểm tra có độ bao phủ cao. Tuy nhiên, các bài kiểm tra được tạo thường khó đọc và có thể không thực tế hoặc thậm chí sai [9]. Điều này đòi hỏi thời gian và nỗ lực từ các nhà phát triển để xác minh tính đúng đắn của bài kiểm tra được tạo [5].

∗Đóng góp ngang nhau

Trong khi đó, các Mô hình Ngôn ngữ Lớn (LLM) được huấn luyện trên mã đã có những bước tiến lớn trong việc tạo ra các hàm giống con người, chất lượng cao dựa trên ngữ cảnh cấp tệp của chúng [10], [11], [12], [13]. Các công cụ như Copilot xuất sắc trong tạo mã, và có thể cải thiện đáng kể năng suất của người dùng [14]. Hiện tại, các mô hình này ít phù hợp hơn cho việc tạo bài kiểm tra, bởi vì chúng có xu hướng được huấn luyện để tạo mã trong từng tệp riêng biệt, thực hành tiêu chuẩn trong xử lý ngôn ngữ tự nhiên. Tạo ra các bài kiểm tra có ý nghĩa, tất nhiên, đòi hỏi phải xem xét sự căn chỉnh giữa các bài kiểm tra và mã dưới kiểm tra tương ứng. Một số công trình trước đây về các phương pháp tạo bài kiểm tra dựa trên mạng neural đã tập trung vào việc mô hình hóa sự căn chỉnh này [3], [7], [15]. Tuy nhiên, công trình này thường tập trung vào nhiệm vụ tương đối hẹp của việc tạo ra các khẳng định riêng lẻ trong các bài kiểm tra hoàn chỉnh khác, dựa trên một phương thức duy nhất dưới kiểm tra. Mở khóa khả năng có tác động mạnh hơn để tạo ra toàn bộ bài kiểm tra đòi hỏi tận dụng cả toàn bộ tệp mã và các bài kiểm tra hiện có làm ngữ cảnh, điều này lần lượt đòi hỏi các mô hình lớn hơn đáng kể.

Trong công trình này, chúng tôi thực hiện một bước quan trọng hướng tới việc tạo toàn bộ bài kiểm tra chính xác thông qua CAT-LM, một mô hình ngôn ngữ được huấn luyện trên Mã và Bài kiểm tra Được căn chỉnh. CAT-LM là một LLM kiểu GPT song ngữ với 2.7B tham số. Nó được huấn luyện trên một kho dữ liệu lớn các dự án Python và Java sử dụng một tín hiệu tiền huấn luyện mới rõ ràng xem xét ánh xạ giữa mã và các tệp kiểm tra, khi có sẵn, trong khi cũng tận dụng khối lượng (lớn hơn nhiều) mã chưa được kiểm tra. Mô hình hóa tệp mã cùng với bài kiểm tra dẫn đến các thách thức bổ sung liên quan đến chiều dài ngữ cảnh của mô hình. Hầu hết các mô hình tạo mã hỗ trợ cửa sổ ngữ cảnh lên tới 2,048 token. Tuy nhiên, phân tích dữ liệu của chúng tôi chỉ ra rằng nhiều cặp tệp mã-kiểm tra bao gồm hơn 8K token. Do đó chúng tôi tăng chiều dài đầu vào chuỗi tối đa, huấn luyện CAT-LM với cửa sổ ngữ cảnh 8,192 token.

Kết quả của chúng tôi cho thấy mô hình hiệu quả tận dụng ngữ cảnh tệp mã để tạo ra nhiều bài kiểm tra hợp lệ về mặt cú pháp hơn đạt được độ bao phủ cao hơn. Mô hình cung cấp một tiền nghiệm mạnh để tạo ra các bài kiểm tra hợp lý: kết hợp với các bộ lọc cơ bản cho khả năng biên dịch và độ bao phủ, CAT-LM thường xuyên tạo ra các bài kiểm tra có độ bao phủ gần với những bài được viết bởi các nhà phát triển con người.

Chúng tôi đánh giá CAT-LM so với một số đường chuẩn mạnh qua hai ứng dụng thực tế: tạo phương thức kiểm tra và hoàn thành phương thức kiểm tra. Đối với tạo phương thức kiểm tra, chúng tôi so sánh CAT-LM với cả các bài kiểm tra do con người viết cũng như các bài kiểm tra được tạo bởi StarCoder [16] và, họ mô hình CodeGen [11]

arXiv:2310.01602v1  [cs.SE]  2 Oct 2023

--- TRANG 2 ---

Tiền huấn luyện CAT-LM
cặp tệp
mã
kiểm tra
LM
Transformer Tự-hồi quy
Tiền xử lý Dữ liệu
+<|codetestpair|>+ codetest 
Dữ liệu Huấn luyện
Căn chỉnh Cặp Mã-Kiểm tra
Đánh giá CAT-LM
Thiết lập Dự án Thực thi
Thực thi Bài kiểm tra
Tính toán Chỉ số                 dữ liệu huấn luyện
Tạo Bài kiểm tra
LM
+<|codetestpair|>+ 
mã
ngữ cảnh đầu vào kiểm tra
Chuẩn bị Đầu vào Kiểm tra
Đầu ra Bài kiểm tra Được tạo

Hình 1: Tổng quan phương pháp. Chúng tôi trích xuất các dự án Java và Python có bài kiểm tra từ GitHub và căn chỉnh heuristic các tệp mã và kiểm tra (trên), điều này, cùng với các tệp không được căn chỉnh, huấn luyện CAT-LM, một mô hình ngôn ngữ lớn, tự-hồi quy. Chúng tôi đánh giá các bài kiểm tra được tạo bởi CAT-LM trên một bộ dự án thực thi (dưới), đo lường khả năng tạo ra các bài kiểm tra hợp lệ về mặt cú pháp mang lại độ bao phủ có thể so sánh với những bài được viết bởi các nhà phát triển.

gia đình, bao gồm các mô hình đơn ngữ được huấn luyện với ngân sách lớn hơn nhiều so với chúng tôi. Chúng tôi cũng so sánh với TeCo [15], một mô hình dành riêng cho kiểm tra gần đây, cho việc hoàn thành bài kiểm tra. CAT-LM tạo ra nhiều bài kiểm tra hợp lệ hơn trung bình so với StarCoder và tất cả các mô hình CodeGen, và vượt trội đáng kể so với TeCo trong việc hoàn thành bài kiểm tra. Kết quả của chúng tôi làm nổi bật giá trị của việc kết hợp sức mạnh của các phương pháp neural lớn với một tín hiệu tiền huấn luyện dựa trên chuyên môn kỹ thuật phần mềm—trong trường hợp này, tầm quan trọng của mối quan hệ giữa mã và các tệp kiểm tra.

Tóm lại, chúng tôi có những đóng góp sau:
• Chúng tôi phát hành một kho dữ liệu 1.1M cặp tệp mã-kiểm tra cùng với 14.4M tệp Java và Python qua 196K dự án mã nguồn mở. Chúng tôi tin rằng kho dữ liệu này sẽ hữu ích cho nhiều nhiệm vụ liên quan đến kiểm thử.
• Chúng tôi phát hành CAT-LM, LLM tiền huấn luyện đầu tiên mô hình hóa các tệp mã và kiểm tra được căn chỉnh từ các dự án Java và Python trên GitHub.
• Chúng tôi phát hành khung kiểm thử được sử dụng để đánh giá các bài kiểm tra được tạo bởi CAT-LM.
• Chúng tôi tiến hành đánh giá rộng rãi CAT-LM với các đường chuẩn mạnh trên các nhiệm vụ downstream như tạo phương thức kiểm tra và hoàn thành bài kiểm tra.

Các checkpoint mô hình cùng với các mẫu mã sử dụng có thể được tìm thấy tại https://github.com/RaoNikitha/CAT-LM.

II. TỔNG QUAN

CAT-LM là một mô hình kiểu GPT có thể tạo ra bài kiểm tra cho ngữ cảnh mã. Hình 1 cho thấy tổng quan về toàn bộ hệ thống của chúng tôi, bao gồm thu thập và tiền xử lý dữ liệu (chi tiết trong Mục IV-A), tiền huấn luyện CAT-LM (Mục V), và đánh giá (Mục VI).

Trước tiên chúng tôi thu thập một kho dữ liệu khoảng 200K repository Python và Java GitHub, tập trung vào những repository có ít nhất 10 sao. Chúng tôi chia chúng ở cấp dự án thành tập huấn luyện và tập kiểm tra (Mục IV-A). Chúng tôi lọc tập huấn luyện theo tiêu chuẩn CodeParrot [17] (bao gồm khử trùng lặp), dẫn đến ~15M tệp mã và kiểm tra. Chúng tôi căn chỉnh các tệp mã và kiểm tra sử dụng heuristic khớp chuỗi mờ (Mục IV-B).

Sau đó chúng tôi chuẩn bị dữ liệu huấn luyện, bao gồm các cặp tệp mã-kiểm tra, được ghép nối với một token duy nhất (<|codetestpair|>), cũng như các tệp mã và kiểm tra không được ghép nối. Chúng tôi tokenize các tệp sử dụng tokenizer sentencepiece được huấn luyện tùy chỉnh [18]. Sau đó chúng tôi xác định kích thước mô hình thích hợp, 2.7B tham số dựa trên ngân sách huấn luyện của chúng tôi và luật scale Chinchilla [19]. Chúng tôi sử dụng bộ công cụ GPT-NeoX [20] được tăng cường với Flash Attention [21] để tiền huấn luyện CAT-LM sử dụng mục tiêu tiền huấn luyện tự-hồi quy (tiêu chuẩn từ trái sang phải) nắm bắt ánh xạ giữa mã và các tệp kiểm tra, trong khi học cấu trúc mã và kiểm tra tổng quát.

--- TRANG 3 ---

public class Bank {    public String methodName() {...}    ...}<|codetestpair|>public class BankTest {    @Test         public void FirstTest() {...}    ...    @Test         public void Test_k() {        assertNotNull(Bank());    }    ...    @Test         public void LastTest() {...}    @Test         public void ExtraTest() {...}}

Tạo bài kiểm tra với ngữ cảnh mã

Hình 2: Các nhiệm vụ đánh giá, với ngữ cảnh mã được hiển thị để hoàn chỉnh: tạo bài kiểm tra cho phương thức kiểm tra đầu tiên, phương thức kiểm tra cuối cùng, và phương thức kiểm tra bổ sung, cùng với hoàn thành bài kiểm tra cho Java.

Cuối cùng, chúng tôi đánh giá CAT-LM trên dữ liệu kiểm tra đã giữ lại. Chúng tôi thiết lập thủ công tất cả các dự án với bộ bài kiểm tra thực thi từ tập kiểm tra để tạo thành khung kiểm thử của chúng tôi. Chúng tôi chuẩn bị đầu vào kiểm tra cho CAT-LM bằng cách nối ngữ cảnh mã với ngữ cảnh kiểm tra tương ứng cho việc tạo bài kiểm tra. Ngữ cảnh kiểm tra thay đổi dựa trên nhiệm vụ. Chúng tôi đánh giá khả năng của mô hình để tạo ra (1) phương thức kiểm tra đầu tiên, (2) phương thức kiểm tra cuối cùng, thêm (3) một bài kiểm tra bổ sung, mới vào một bộ bài kiểm tra đã hoàn chỉnh. Chúng tôi cũng đánh giá việc hoàn thành một câu lệnh trong một hàm kiểm tra. Chúng tôi tokenize đầu vào đã chuẩn bị và giao cho CAT-LM nhiệm vụ lấy mẫu nhiều (thường là 10) đầu ra kiểm tra, mỗi đầu ra bao gồm một phương thức duy nhất. Sau đó chúng tôi cố gắng thực thi các bài kiểm tra được tạo với khung kiểm thử của chúng tôi và tính toán các chỉ số như số lượng bài kiểm tra được tạo có thể biên dịch và vượt qua, cùng với độ bao phủ mà chúng cung cấp, để đánh giá chất lượng bài kiểm tra.

III. NHIỆM VỤ

Chúng tôi mô tả hai nhiệm vụ mà CAT-LM có thể được sử dụng, đó là tạo phương thức kiểm tra (với ba thiết lập) và hoàn thành bài kiểm tra. Hình 2 minh họa thiết lập cho tất cả các nhiệm vụ bao gồm ngữ cảnh mã.

A. Tạo Phương thức Kiểm tra

Cho một tệp kiểm tra hoàn chỉnh một phần và tệp mã tương ứng của nó, mục tiêu của tạo phương thức kiểm tra là tạo ra phương thức kiểm tra tiếp theo. Các nhà phát triển có thể sử dụng tạo bài kiểm tra để tạo ra toàn bộ bộ bài kiểm tra, hoặc thêm bài kiểm tra vào một bộ kiểm tra hiện có

BẢNG I: Thống kê tóm tắt của toàn bộ tập dữ liệu.

Thuộc tính | Python | Java | Tổng
Dự án
Tổng | 148,605 | 49,125 | 197,730
Đã khử trùng | 147,970 | 48,882 | 196,852
Không có Kiểm tra | 84,186 | 15,128 | 99,314
Không có Cặp tệp | 108,042 | 23,933 | 131,975
Kích thước (GB)
Thô | 123 | 157 | 280
Đã khử trùng | 53 | 94 | 147
Tệp
Tổng | 8,101,457 | 14,894,317 | 22,995,774
Đã lọc | 7,375,317 | 14,698,938 | 22,074,255
Đã khử trùng | 5,101,457 | 10,418,609 | 15,520,066
Mã | 4,128,813 | 8,380,496 | 12,509,309
Kiểm tra | 972,644 | 2,038,113 | 3,010,757
Cặp tệp | 412,881 | 743,882 | 1,156,763
Huấn luyện | 4,688,576 | 9,674,727 | 14,363,303

để kiểm tra chức năng mới. Chúng tôi đánh giá ba thiết lập khác nhau, tương ứng với các giai đoạn khác nhau trong quá trình kiểm thử, đó là tạo ra (1) bài kiểm tra đầu tiên trong tệp, đại diện cho sự khởi đầu nỗ lực kiểm thử của nhà phát triển. Trong thiết lập này, chúng tôi giả định rằng các import cơ bản và khung cao cấp đã được đặt tại chỗ, nhưng chưa có trường hợp kiểm tra nào được viết, (2) bài kiểm tra cuối cùng trong một tệp, đánh giá khả năng của mô hình để suy ra những gì còn thiếu từ một bộ bài kiểm tra gần hoàn chỉnh. Chúng tôi chỉ đánh giá khả năng này trên các tệp kiểm tra có hai hoặc nhiều bài kiểm tra (do con người viết) để tránh các trường hợp chỉ một bài kiểm tra duy nhất là phù hợp, và (3) một bài kiểm tra bổ sung hoặc thêm, điều này khảo sát liệu mô hình có thể tạo ra các bài kiểm tra mới cho một bộ bài kiểm tra phần lớn hoàn chỉnh hay không. Lưu ý rằng điều này thường có thể không cần thiết trong thực tế.

B. Hoàn thành Bài kiểm tra

Mục tiêu của hoàn thành bài kiểm tra là tạo ra câu lệnh tiếp theo trong một phương thức kiểm tra chưa hoàn chỉnh cho trước. Hoàn thành bài kiểm tra nhằm giúp các nhà phát triển viết bài kiểm tra nhanh hơn. Mặc dù hoàn thành bài kiểm tra có những điểm tương đồng với hoàn thành mã tổng quát, nó khác biệt theo hai cách: (1) phương thức dưới kiểm tra cung cấp nhiều ngữ cảnh hơn về những gì đang được kiểm tra, và (2) mã nguồn và mã kiểm tra thường có các phong cách lập trình khác biệt, với mã kiểm tra thường bao gồm thiết lập, gọi phương thức dưới kiểm tra, và các khẳng định về đầu ra (oracle kiểm tra).

IV. TẬP DỮ LIỆU

Mục này mô tả việc chuẩn bị tập dữ liệu cho cả việc huấn luyện và đánh giá CAT-LM. Bảng I cung cấp thống kê cấp cao liên quan đến thu thập và lọc dữ liệu.

A. Thu thập Dữ liệu

Chúng tôi sử dụng GitHub API [22] để khai thác các repository Python và Java có ít nhất 10 sao và có commit mới sau ngày 1 tháng 1 năm 2020. Theo [23] và [24], chúng tôi cũng loại bỏ các fork, để ngăn chặn trùng lặp dữ liệu. Điều này dẫn đến tổng cộng 148,605 repository Python và 49,125 repository Java với tổng cộng ~23M tệp (khoảng 280 GB). Chúng tôi chia ngẫu nhiên điều này thành tập huấn luyện và tập kiểm tra, đảm bảo rằng tập kiểm tra bao gồm 500 repository cho Python và Java mỗi loại.

--- TRANG 4 ---

B. Chuẩn bị Dữ liệu Huấn luyện

Trước tiên chúng tôi loại bỏ tất cả các tệp không phải mã nguồn (ví dụ: các tệp cấu hình và README) để đảm bảo rằng mô hình chỉ được huấn luyện trên mã nguồn. Sau đó chúng tôi áp dụng một loạt bộ lọc theo tiêu chuẩn của CodeParrot [17] để giảm thiểu nhiễu từ tín hiệu huấn luyện của chúng tôi. Điều này bao gồm loại bỏ các tệp lớn hơn 1MB, cũng như các tệp có bất kỳ dòng nào dài hơn 1000 ký tự; độ dài dòng trung bình >100 ký tự; hơn 25% ký tự không phải chữ và số, và các chỉ số cho thấy được tạo tự động. Điều này loại bỏ 9% cả tệp Python và Java. Chúng tôi khử trùng lặp các tệp bằng cách kiểm tra md5 hash của mỗi tệp với tất cả các tệp khác trong kho dữ liệu của chúng tôi. Điều này loại bỏ khoảng 30% cả tệp Python và Java.

Chúng tôi trích xuất các cặp tệp mã-kiểm tra từ dữ liệu này sử dụng kết hợp heuristic khớp chính xác và mờ. Cho một tệp mã có tên <CFN>, trước tiên chúng tôi tìm kiếm các tệp kiểm tra có mẫu test_<CFN>, <CFN>_test, <CFN>Test hoặc Test<CFN>. Nếu không tìm thấy kết quả nào, chúng tôi thực hiện khớp chuỗi mờ [25] giữa tên tệp mã và kiểm tra, và nhóm chúng thành một cặp nếu chúng đạt được điểm tương đồng lớn hơn 0.85. Nếu tìm thấy nhiều kết quả, chúng tôi giữ cặp có điểm số cao nhất.

Sau khi trích xuất cặp tệp, chúng tôi chuẩn bị dữ liệu huấn luyện bằng cách thay thế các tệp mã và kiểm tra bằng một tệp mới nối nội dung của tệp mã và tệp kiểm tra, phân tách chúng bằng token duy nhất <|codetestpair|>. Điều này đảm bảo rằng mô hình học ánh xạ giữa mã và các tệp kiểm tra từ tín hiệu tiền huấn luyện. Lưu ý rằng chúng tôi luôn kết hợp các tệp này bắt đầu với mã, vì vậy mô hình (hoạt động từ trái sang phải) chỉ được hưởng lợi từ thông tin ghép nối này khi tạo ra bài kiểm tra. Chúng tôi bổ sung bao gồm tất cả các tệp mã và kiểm tra khác mà chúng tôi không tìm thấy cặp trong dữ liệu huấn luyện của chúng tôi, dẫn đến 4.7M tệp Python và 9.7M tệp Java. Chúng tôi bao gồm những tệp không khớp này để tối đa hóa lượng dữ liệu mà mô hình có thể học từ đó. Hình 3 tóm tắt phân phối các tệp trong dữ liệu huấn luyện cùng với các đoạn mã mẫu cho mỗi loại tệp.

Phân phối các tệp và cặp tệp: Hình 4 tóm tắt phân phối các tệp trong các dự án theo số sao của chúng. Chúng tôi quan sát thấy xu hướng giảm không chỉ trong số lượng tệp mã và tệp kiểm tra, mà còn trong các cặp tệp. Sau khi kiểm tra thủ công một vài dự án được chọn ngẫu nhiên, chúng tôi thấy rằng các dự án phổ biến với số sao cao có xu hướng được kiểm tra tốt hơn, phù hợp với tài liệu trước đây [26], [27]. Lưu ý rằng chúng tôi chuẩn hóa biểu đồ để giúp minh họa xu hướng bằng cách tổng hợp các dự án trong các nhóm dựa trên percentile, sau khi sắp xếp chúng dựa trên sao. Phân phối dữ liệu khác nhau giữa Python và Java: Python có khoảng 3x nhiều dự án hơn Java, nhưng Java có khoảng gấp đôi số cặp tệp mã-kiểm tra.

C. Chuẩn bị Dữ liệu Kiểm tra và Thiết lập Thực thi

Để chuẩn bị dữ liệu kiểm tra của chúng tôi, trước tiên chúng tôi loại trừ tất cả các dự án không có cặp tệp mã-kiểm tra. Điều này dẫn đến tổng cộng 97 dự án Java và 152 dự án Python. Sau đó chúng tôi cố gắng thiết lập tất cả các dự án cho việc thực thi kiểm tra tự động.

public class UserController {    public String getAllUsers() {    ...    }}Tệp Mã

public class AppTest {    @Test    public void homePage() {    ...    }}Tệp Kiểm tra

public class Bank {    public String customerSummary() {    ...    }}<|codetestpair|>public class BankTest {    @Test    public void customerSummary() {    ...    }}Cặp Tệp Mã-Kiểm tra

11.35M | 1.15M | 1.85M

Hình 3: Phân phối các tệp với các đoạn mã mẫu

Hình 4: Phân phối các tệp trong các dự án được sắp xếp theo sao GitHub, được chuẩn hóa theo percentile

Thiết lập Thực thi cho Java: Các dự án có thể sử dụng các phiên bản Java khác nhau (bao gồm Java 8, 11, 14, và 17) và hệ thống build (chủ yếu là Maven và Gradle). Chúng tôi thiết lập thủ công các Docker image cho mỗi tổ hợp. Sau đó chúng tôi cố gắng thực thi các lệnh build cho mỗi dự án trong một container từ mỗi image. Chúng tôi thành công build 54 trong số 97 dự án Java, chứa 61 cặp tệp mã-kiểm tra.

Thiết lập Thực thi cho Python: Chúng tôi thiết lập thủ công các Docker container cho Python 3.8 và 3.10 với khung pytest và cố gắng chạy các lệnh build cho mỗi dự án cho đến khi build thành công. Chúng tôi thành công build 41 trong số 152 dự án Python, chứa 1080 cặp tệp mã-kiểm tra.

Chúng tôi tiếp tục loại bỏ tất cả các cặp trong những dự án này chỉ có một phương thức mã duy nhất hoặc một phương thức kiểm tra duy nhất để đảm bảo rằng các cặp tệp mã-kiểm tra trong tập kiểm tra của chúng tôi tương ứng với các bộ bài kiểm tra không tầm thường. Chúng tôi bổ sung yêu cầu các dự án Java và Python tương thích với các thư viện Jacoco và coverage tương ứng. Điều này để lại tổng cộng 27 cặp tệp mã-kiểm tra qua 26 dự án Java duy nhất và 517 cặp tệp mã-kiểm tra qua 26 dự án Python duy nhất. Trong Python, chúng tôi lấy mẫu ngẫu nhiên tối đa 10 cặp tệp mỗi dự án để giảm thiên lệch hướng về các dự án lớn (hai dự án hàng đầu chiếm 346 bài kiểm tra) dẫn đến tập cuối cùng gồm 123 cặp tệp qua 26 dự án Python duy nhất

--- TRANG 5 ---

Hình 5: Phân phối token cặp tệp

dự án Python. Lưu ý rằng chúng tôi tái sử dụng những Docker container này trong khung kiểm thử của chúng tôi (Xem Mục VI-A5).

V. CAT-LM

Mục này mô tả chi tiết về việc chuẩn bị đầu vào, tiền huấn luyện CAT-LM và tạo đầu ra.

A. Biểu diễn Đầu vào cho Tiền huấn luyện CAT-LM

Chúng tôi sử dụng kho dữ liệu 14M tệp Java và Python mà chúng tôi đã chuẩn bị cho việc tiền huấn luyện mô hình của chúng tôi (xem Mục IV-A). Trước tiên chúng tôi huấn luyện một tokenizer subword [28] sử dụng bộ công cụ SentencePiece [18] với kích thước từ vựng 64K token. Tokenizer được huấn luyện trên 3 GB dữ liệu sử dụng mười dòng ngẫu nhiên được lấy mẫu từ mỗi tệp. Sau đó chúng tôi tokenize các tệp đầu vào của chúng tôi thành định dạng nhị phân được sử dụng để stream dữ liệu hiệu quả trong quá trình huấn luyện.

Phân tích phân phối token: Các mô hình ngôn ngữ thường bị ràng buộc về lượng văn bản chúng có thể chứa trong cửa sổ ngữ cảnh của chúng. Hầu hết các mô hình tạo mã hiện tại sử dụng cửa sổ ngữ cảnh lên tới 2,048 token [11], [29].* Phân tích của chúng tôi về phân phối token, được trực quan hóa trong Hình 5, cho thấy điều này chỉ bao phủ 35% tổng số cặp tệp. Như vậy, trong khi nó có thể phù hợp cho đa số (nhẹ) các tệp riêng lẻ, nó sẽ không cho phép mô hình của chúng tôi tận dụng ngữ cảnh của tệp mã khi dự đoán văn bản trong tệp kiểm tra. Đây là một hạn chế đáng kể vì chúng tôi muốn huấn luyện mô hình sử dụng ngữ cảnh từ tệp mã khi tạo ra bài kiểm tra.

Phân tích thêm cho thấy khoảng 82% tất cả các cặp tệp cho Java và Python có ít hơn 8,192 token. Vì chi phí của phép toán attention tăng bậc hai với chiều dài ngữ cảnh, chúng tôi chọn ngưỡng này để cân bằng chi phí và lợi ích huấn luyện. Do đó, chúng tôi chọn huấn luyện một mô hình với cửa sổ ngữ cảnh dài hơn là 8192 token để chứa thêm ~550K cặp tệp. Lưu ý rằng điều này không dẫn đến việc loại bỏ bất kỳ mẫu nào; các cặp có nhiều token hơn sẽ đơn giản bị (ngẫu nhiên) chia nhỏ bởi bộ công cụ huấn luyện.

B. Chi tiết Mô hình và Huấn luyện

Chúng tôi xác định kích thước mô hình dựa trên ngân sách điện toán đám mây của chúng tôi là $20,000 và lượng dữ liệu huấn luyện có sẵn, dựa trên luật scale Chinchilla [19], đề xuất rằng tổn thất huấn luyện cho một ngân sách điện toán cố định có thể được tối thiểu hóa (thấp hơn là tốt hơn) bằng cách huấn luyện một mô hình với khoảng (và không ít hơn) 20 lần nhiều token như nó có tham số. Dựa trên các lần chạy sơ bộ, chúng tôi xác định kích thước mô hình thích hợp là 2.7B tham số (không embedding), một kích thước phổ biến cho các mô hình ngôn ngữ trung bình đến lớn [29], [11], mà chúng tôi do đó nhằm huấn luyện với ít nhất 54B token. Kiến trúc mô hình này bao gồm một mô hình Transformer 32 lớp 2,560 chiều với cửa sổ ngữ cảnh 8,192 token. Chúng tôi huấn luyện mô hình với kích thước batch 256 chuỗi, tương ứng với ~2M token. Chúng tôi sử dụng bộ công cụ GPT-NeoX [20] để huấn luyện mô hình hiệu quả với 8 GPU Nvidia A100 80GB trên một máy duy nhất trên Google Cloud Platform. Chúng tôi huấn luyện mô hình trong 28.5K bước, với tổng cộng gần 60B token, qua 18 ngày, do đó trung bình khoảng 1,583 bước mỗi ngày.* Chúng tôi lưu ý rằng thời gian huấn luyện này ngắn hơn nhiều so với nhiều mô hình phổ biến [11], [30];* mô hình do đó có thể được cải thiện đáng kể với việc huấn luyện thêm. Mô hình cuối cùng được đặt tên là CAT-LM vì nó được huấn luyện trên Mã và Bài kiểm tra Được căn chỉnh.

*Độ dài trung bình của một token phụ thuộc vào từ vựng và tập dữ liệu, nhưng thường có thể được giả định là khoảng 3 ký tự.

C. Nhắc CAT-LM để tạo đầu ra:

Vì CAT-LM đã được huấn luyện sử dụng tín hiệu tiền huấn luyện tự hồi quy từ trái sang phải, nó có thể được nhắc để tạo ra một số mã dựa trên ngữ cảnh trước đó. Trong trường hợp của chúng tôi, chúng tôi giao cho nó nhiệm vụ tạo ra toàn bộ phương thức kiểm tra cho ngữ cảnh tệp kiểm tra (và thường là mã) trước đó, hoặc tạo ra một dòng để hoàn thành phương thức kiểm tra (cho cùng một ngữ cảnh). Chúng tôi nhắc CAT-LM với đầu vào cho mỗi nhiệm vụ, cả có và không có ngữ cảnh mã, và lấy mẫu 10 đầu ra từ CAT-LM với "nhiệt độ" 0.2, khuyến khích tạo ra các đầu ra khác nhau nhưng có tính hợp lý cao (đối với mô hình). Lấy mẫu nhiều đầu ra tương đối rẻ so với kích thước của một phương thức so với kích thước ngữ cảnh, và cho phép mô hình hiệu quả tạo ra nhiều phương thức từ một ngữ cảnh được mã hóa. Sau đó chúng tôi có thể lọc ra các bài kiểm tra không biên dịch được, thiếu assert, hoặc thất bại (vì chúng tôi đang tạo ra các bài kiểm tra hành vi), bằng cách thực thi chúng trong khung kiểm thử. Chúng tôi chuẩn bị đầu ra để thực thi bằng cách thêm phương thức kiểm tra được tạo vào vị trí tương ứng trong các tệp kiểm tra cơ sở, mà không thực hiện bất kỳ thay đổi nào đối với các bài kiểm tra khác trong tệp.

*Chúng tôi tiếp tục huấn luyện mô hình đến 35.3K bước, nhờ một khoản tài trợ bổ sung nhận được $5000, sau khi bài báo được xuất bản. Checkpoint mới nhất này hiện có sẵn trên HuggingFace. Vui lòng xem https://github.com/RaoNikitha/CAT-LM để biết thêm chi tiết về việc sử dụng. Lưu ý rằng các con số được báo cáo trong bài báo này sử dụng checkpoint cũ hơn (28.5K bước), và có thể không khớp với các con số từ checkpoint công khai mới hơn (35.3K bước).
*Tối ưu "Chinchilla" không tập trung vào việc tối đa hóa hiệu suất cho một kích thước mô hình cho trước, chỉ cho tổng ngân sách điện toán.

VI. THIẾT LẬP THỰC NGHIỆM

Chúng tôi mô tả thiết lập để đánh giá CAT-LM qua cả hai nhiệm vụ được nêu trong Mục III, đó là tạo phương thức kiểm tra, và hoàn thành bài kiểm tra.

A. Tạo Phương thức Kiểm tra

Nhiệm vụ tạo phương thức kiểm tra bao gồm ba trường hợp khác nhau: tạo ra bài kiểm tra đầu tiên, bài kiểm tra cuối cùng, và một bài kiểm tra bổ sung trong một bộ bài kiểm tra (xem Mục III). Chúng tôi đánh giá CAT-LM về tạo phương thức kiểm tra cả có ngữ cảnh mã và, như một phép loại bỏ, không có ngữ cảnh mã.

1) Các Mô hình Cơ sở: CodeGen là một họ LLM dựa trên Transformer được huấn luyện tự hồi quy (từ trái sang phải) [11]. Các mô hình CodeGen tiền huấn luyện có sẵn trong một loạt kích thước rộng, bao gồm 350M, 2.7B, 6.1B và 16.1B tham số. Những mô hình này được huấn luyện trên ba tập dữ liệu khác nhau, bắt đầu với một kho dữ liệu lớn, chủ yếu tiếng Anh, theo sau bởi một kho dữ liệu ngôn ngữ lập trình đa ngữ (bao gồm Java và Python), và kết thúc bằng việc fine-tune chỉ trên dữ liệu Python. Mô hình lớn nhất được huấn luyện theo cách này có tính cạnh tranh với Codex [10] trên một benchmark Python [11].

Để đánh giá của chúng tôi, chúng tôi so sánh với CodeGen-2.7B-multi, có kích thước tương đương với mô hình của chúng tôi và được huấn luyện trên nhiều ngôn ngữ lập trình, như mô hình của chúng tôi. Chúng tôi cũng xem xét CodeGen-16B-multi (với 16B tham số, khoảng 6 lần lớn hơn CAT-LM) là mô hình lớn nhất có sẵn được huấn luyện trên nhiều ngôn ngữ lập trình. Đối với tất cả các nhiệm vụ Python, chúng tôi cũng so sánh với CodeGen-2.7B-mono và CodeGen-16B-mono, các biến thể của các mô hình nói trên được fine-tune chỉ trên mã Python trong 150k bước huấn luyện bổ sung.

Chúng tôi cũng so sánh hiệu suất của CAT-LM với StarCoder [16], một mô hình 15.5B tham số được huấn luyện trên hơn 80 ngôn ngữ lập trình, bao gồm Java và Python, từ The Stack (v1.2). StarCoder có cửa sổ ngữ cảnh 8,192 token. Nó được huấn luyện sử dụng mục tiêu Fill-in-the-Middle [13] trên 1 trillion token mã, sử dụng phương pháp mẫu ngẫu nhiên hóa thứ tự tài liệu như CodeGen.

2) Chỉ số Từ vựng: Mặc dù mục tiêu của chúng tôi không phải là sao chép chính xác các bài kiểm tra do con người viết, chúng tôi cung cấp các thước đo tương đồng từ vựng giữa các bài kiểm tra được tạo và các đối tác thực tế của chúng như chỉ số về tính thực tế của chúng. Các bài kiểm tra được tạo thường xuyên trùng lặp trong cách diễn đạt với các bài kiểm tra ground-truth có khả năng tương tự trong cấu trúc và do đó tương đối dễ đọc cho các nhà phát triển. Cụ thể, chúng tôi báo cáo cả tỷ lệ khớp chính xác và một số thước đo tương đồng gần đúng, bao gồm ROUGE [31] (chuỗi con chồng chéo dài nhất của các token) và điểm CodeBLEU [32] (chồng chéo n-gram có tính đến AST mã và đồ thị luồng dữ liệu). Chúng tôi chỉ báo cáo chỉ số từ vựng cho thiết lập bài kiểm tra đầu tiên và bài kiểm tra cuối cùng, vì không có bài kiểm tra vàng để so sánh trong thiết lập bài kiểm tra bổ sung của chúng tôi. Những chỉ số này đã được sử dụng rộng rãi trong công trình trước đây về tạo mã và hoàn thành bài kiểm tra [15], [33], [34], [35].

3) Chỉ số Thời gian chạy: Chúng tôi cũng báo cáo các chỉ số thời gian chạy đánh giá tính hữu ích của bài kiểm tra tốt hơn so với các chỉ số từ vựng. Điều này bao gồm số lượng bài kiểm tra được tạo có thể biên dịch, và bài kiểm tra được tạo vượt qua bộ bài kiểm tra. Chúng tôi cũng đo độ bao phủ của các bài kiểm tra được tạo. Đối với bài kiểm tra đầu tiên và cuối cùng, chúng tôi so sánh điều này với độ bao phủ được thực hiện bởi các bài kiểm tra do con người viết tương ứng. Chúng tôi hy vọng rằng công trình này sẽ khuyến khích việc áp dụng rộng rãi hơn các chỉ số thời gian chạy (là một phần quan trọng của tính hữu ích của bài kiểm tra), vì công trình trước đây chủ yếu tập trung vào tương đồng từ vựng [3], [7], [15]. Xem Mục 2.2 trong tài liệu bổ sung để có mô tả chi tiết bổ sung về tất cả các chỉ số từ vựng và thời gian chạy.

4) Chuẩn bị Ngữ cảnh Đầu vào và Tệp Kiểm tra Cơ sở: Chúng tôi sử dụng trình phân tích AST trên các tệp kiểm tra ground-truth để chuẩn bị các bài kiểm tra một phần với chúng để nhắc CAT-LM. Đối với tạo bài kiểm tra đầu tiên, chúng tôi loại bỏ tất cả các trường hợp kiểm tra (nhưng không phải import, cũng không phải

BẢNG II: Độ bao phủ cơ sở cho các bài kiểm tra do con người viết trên số lượng cặp tệp cho trước.

PL | Trường hợp | Cov Imp % | # Cặp tệp
Python
Bài kiểm tra đầu tiên | 59.3% | 112
Bài kiểm tra cuối cùng | 5.0% | 93
Bài kiểm tra bổ sung | 0.0% | 123
Java
Bài kiểm tra đầu tiên | 50.5% | 27
Bài kiểm tra cuối cùng | 5.3% | 18
Bài kiểm tra bổ sung | 0.0% | 27

bất kỳ mã thiết lập nào khác đến trước bài kiểm tra đầu tiên); đối với tạo bài kiểm tra cuối cùng, chúng tôi để lại tất cả trừ phương thức kiểm tra cuối cùng, và đối với tạo bài kiểm tra cuối cùng chúng tôi chỉ loại bỏ mã sau bài kiểm tra cuối cùng. Sau đó chúng tôi nối ngữ cảnh mã với ngữ cảnh kiểm tra sử dụng token phân tách của chúng tôi cho điều kiện 'có ngữ cảnh mã'.

Chúng tôi bổ sung thu được độ bao phủ với các tệp kiểm tra gốc, do con người viết dưới cùng điều kiện, chỉ giữ bài kiểm tra đầu tiên hoặc tất cả các bài kiểm tra làm đường chuẩn cho dự đoán bài kiểm tra đầu tiên và cuối cùng tương ứng. Lưu ý rằng không có đường chuẩn cho nhiệm vụ tạo bài kiểm tra bổ sung. Xem Mục 1 trong tài liệu bổ sung để biết phân phối độ bao phủ của các bài kiểm tra do con người viết.

5) Khung Kiểm thử: Chúng tôi đánh giá chất lượng của các bài kiểm tra được tạo sử dụng các container mà chúng tôi thiết lập để thực thi các dự án trong Mục IV-C. Chúng tôi chèn bài kiểm tra được tạo vào tệp kiểm tra gốc, thực thi các lệnh thiết lập tương ứng của dự án và kiểm tra lỗi, ghi lại số lượng bài kiểm tra được tạo có thể biên dịch và vượt qua bộ bài kiểm tra (xem Mục VI-A3). Nếu bài kiểm tra được tạo biên dịch thành công (hoặc, đối với Python, không có lỗi import hoặc cú pháp), chúng tôi chạy bộ bài kiểm tra và ghi lại liệu bài kiểm tra được tạo có vượt qua hay thất bại. Chúng tôi tính toán độ bao phủ mã cho tất cả các bài kiểm tra vượt qua, đối chiếu điều này với độ bao phủ đạt được bởi các trường hợp kiểm tra do con người viết (khi có sẵn) làm đường chuẩn.

B. Hoàn thành Bài kiểm tra

Nhớ lại nhiệm vụ hoàn thành bài kiểm tra bao gồm tạo ra một dòng duy nhất trong một phương thức kiểm tra cho trước, cho các dòng trước đó của bài kiểm tra. Chúng tôi thực hiện đánh giá cho hoàn thành bài kiểm tra dưới hai điều kiện, có ngữ cảnh mã và không có ngữ cảnh mã.

1) Mô hình Cơ sở: Chúng tôi so sánh với TeCo [15], một đường chuẩn hiện đại nhất về hoàn thành câu lệnh kiểm tra đã vượt trội so với nhiều mô hình hiện có, bao gồm CodeT5 [35], CodeGPT [36] và TOGA [3]. TeCo [15] là một mô hình transformer encoder-decoder dựa trên kiến trúc CodeT5 [35]. TeCo lấy chữ ký phương thức kiểm tra, các câu lệnh trước đó trong bài kiểm tra, phương thức dưới kiểm tra, các loại biến, các loại vắng mặt và thiết lập và dọn dẹp phương thức làm đầu vào.

Ban đầu, chúng tôi dự định so sánh CAT-LM với TeCo trên tập kiểm tra của chúng tôi. Tuy nhiên, TeCo thực hiện lọc rộng rãi bao gồm yêu cầu JUnit, Maven, các bài kiểm tra được đặt tên tốt, ánh xạ một-một giữa bài kiểm tra và phương thức dưới kiểm tra, và không có câu lệnh if hoặc luồng điều khiển không tuần tự trong phương thức kiểm tra. Do đó chúng tôi so sánh CAT-LM với TeCo cho 1000 câu lệnh được lấy mẫu ngẫu nhiên từ tập kiểm tra của họ.

--- TRANG 7 ---

Bài kiểm tra đầu tiên | Bài kiểm tra cuối cùng | Bài kiểm tra bổ sung
0 20 40 60 80 100 120
# số thế hệ vượt qua
CodeGen-multi-2B
CodeGen-multi-16B
CodeGen-mono-2B
CodeGen-mono-16B
StarCoder
CAT-LM
(a) Python.

Bài kiểm tra đầu tiên | Bài kiểm tra cuối cùng | Bài kiểm tra bổ sung
0 5 10 15 20 25 30 35
# số thế hệ vượt qua
CodeGen-multi-2B
CodeGen-multi-16B
StarCoder
CAT-LM
(b) Java.

Hình 6: Các bài kiểm tra vượt qua theo mô hình cho Python và Java.

2) Chỉ số: Chúng tôi so sánh CAT-LM với TeCo qua tất cả các chỉ số từ vựng (được nêu trong Mục VI-A2).

VII. ĐÁNH GIÁ

Chúng tôi đánh giá khả năng của CAT-LM để tạo ra các bài kiểm tra hợp lệ đạt được độ bao phủ, so sánh với các đường chuẩn hiện đại cho cả tạo mã và hoàn thành bài kiểm tra. Kết quả bổ sung có thể được tìm thấy trong tài liệu bổ sung.

A. Tạo Phương thức Kiểm tra

1) Tỷ lệ Vượt qua: Hình 6 cho thấy số lượng bài kiểm tra vượt qua được tạo bởi mỗi mô hình cho Python và Java. Lưu ý rằng đây là các con số tuyệt đối, trong tổng số khác nhau cho mỗi thiết lập.* CAT-LM vượt trội so với StarCoder và tất cả các mô hình CodeGen, bao gồm những mô hình lớn hơn nhiều và đặc trung cho ngôn ngữ trong hầu hết các thiết lập. Đối với Python, tất cả các mô hình hoạt động tệ nhất trong thiết lập bài kiểm tra đầu tiên, nơi chúng có ít ngữ cảnh nhất để xây dựng. Tuy nhiên, được trang bị ngữ cảnh của tệp mã tương ứng, mô hình của chúng tôi tạo ra nhiều bài kiểm tra vượt qua hơn đáng kể so với StarCoder (với 15.5B tham số) và các đường chuẩn CodeGen đa ngữ (được huấn luyện với nhiều token hơn rất nhiều) trong cả thiết lập bài kiểm tra đầu tiên và bổ sung. Chỉ trong thiết lập bài-kiểm-tra-cuối-cùng một số mô hình mới cạnh tranh với mô hình của chúng tôi, mặc dù chúng tôi lưu ý rằng hiệu suất của chúng có thể bị thổi phồng vì các mô hình có thể đã thấy các tệp trong tập kiểm tra của chúng tôi trong quá trình huấn luyện (tập kiểm tra loại trừ rõ ràng các tệp được CAT-LM thấy trong quá trình huấn luyện). Đối với Java, chúng tôi thấy rằng CAT-LM tạo ra nhiều bài kiểm tra vượt qua hơn so với StarCoder và hai mô hình CodeGen đa ngữ (không tồn tại mô hình chỉ dành cho Java). Sự khác biệt rõ rệt nhất trong thiết lập bài kiểm tra bổ sung, nơi CAT-LM tạo ra gần gấp đôi số bài kiểm tra vượt qua so với StarCoder và các mô hình CodeGen cơ sở. Nhìn chung, mặc dù bị thiếu huấn luyện, CAT-LM tạo ra nhiều bài kiểm tra vượt qua hơn trung bình qua tất cả các thiết lập. Cả StarCoder và các mô hình CodeGen đều không cho thấy mức tăng đáng kể với nhiều tham số hơn hoặc ngữ cảnh dài hơn (StarCoder có thể

Bài kiểm tra đầu tiên | Bài kiểm tra cuối cùng | Bài kiểm tra bổ sung
0.0 0.2 0.4 0.6 0.8 1.0
Cải thiện Độ bao phủ
CAT-LM cov impr.
Human cov impr.
(a) Cải thiện độ bao phủ của mô hình chúng tôi so với con người cho Python.

Bài kiểm tra đầu tiên | Bài kiểm tra cuối cùng | Bài kiểm tra bổ sung
0.0 0.2 0.4 0.6 0.8
Cải thiện Độ bao phủ
CAT-LM cov impr.
Human cov impr.
(b) Cải thiện độ bao phủ của mô hình chúng tôi so với con người cho Java.

Hình 7: Cải thiện độ bao phủ của mô hình chúng tôi so với con người cho các ngôn ngữ khác nhau.

sử dụng 8,192 token), làm nổi bật rằng huấn luyện với ngữ cảnh mã là quan trọng.

2) Độ bao phủ: Hình 7 cho thấy phân phối độ bao phủ của CAT-LM, đối chiếu với của các bài kiểm tra do con người viết. Đối với cả thiết lập bài kiểm tra đầu tiên và cuối cùng, mô hình của chúng tôi hoạt động chủ yếu tương đương với con người, với cả hai phân phối có khoảng cùng median và dải quartile. Nhiệm vụ bài kiểm tra bổ sung rõ ràng đặc biệt khó: trong khi mô hình của chúng tôi có thể tạo ra nhiều bài kiểm tra trong thiết lập này (Hình 6), chúng hiếm khi chuyển thành độ bao phủ bổ sung, vượt quá những gì được cung cấp bởi phần còn lại của bộ bài kiểm tra, một phần vì hầu hết các bộ bài kiểm tra do nhà phát triển viết trong tập dữ liệu của chúng tôi đã có độ bao phủ mã cao (độ bao phủ trung bình 78.6% cho Java và 81.6% cho Python), và có thể không cần các bài kiểm tra bổ sung. Bảng II cho thấy cải thiện độ bao phủ con người trung bình cho bài kiểm tra đầu tiên và cuối cùng được thêm vào một bộ bài kiểm tra. Lưu ý rằng trung bình thấp hơn đáng kể cho bài kiểm tra cuối cùng, vì độ bao phủ cơ sở đã cao cho chế độ này (74.7% cho Java và 76.1% cho Python).

Chúng tôi lưu ý rằng chúng tôi không thể tính toán độ bao phủ cho tất cả các cặp tệp trong mỗi thiết lập. Chúng tôi loại trừ các cặp tệp chỉ có một bài kiểm tra từ thiết lập bài kiểm tra cuối cùng để phân biệt nó với thiết lập bài kiểm tra đầu tiên của chúng tôi. Đối với thiết lập bài kiểm tra đầu tiên, một số tệp cơ sở thiếu các phương thức helper giữa bài kiểm tra đầu tiên và cuối cùng trong tệp, ngăn chúng tôi tính toán độ bao phủ.

3) Tương đồng Từ vựng: Bảng III cho thấy kết quả chỉ số tương đồng từ vựng so với các bài kiểm tra do con người viết cho CAT-LM, cả có và không có ngữ cảnh, cùng với các đường chuẩn StarCoder và CodeGen. CAT-LM báo cáo điểm tương đồng từ vựng cao khi tận dụng ngữ cảnh mã, thường ở hoặc trên mức của mô hình tốt nhất khác, StarCoder (với 15B tham số). Hiệu ứng này nhất quán qua việc tạo bài kiểm tra đầu tiên và cuối cùng.

4) Tác động của Ngữ cảnh Mã: Như được mong đợi, CAT-LM được hưởng lợi rất nhiều từ sự có mặt của ngữ cảnh mã. Khi nó được truy vấn không có ngữ cảnh này, hiệu suất của nó trên các chỉ số từ vựng có xu hướng giảm xuống dưới mức của CodeGen-2B, có kích thước tương đương nhưng được huấn luyện với nhiều token hơn. Sự khác biệt trong hiệu suất chỉ số từ vựng đôi khi khá rõ rệt, với mức tăng lên tới 9.2% trong điểm Rouge và lên tới 5.1% trong điểm CodeBLEU.

Về các chỉ số thời gian chạy, ngữ cảnh mã chủ yếu giúp ích trong nhiệm vụ dự đoán bài kiểm tra đầu tiên và cuối cùng, với mức tăng đặc biệt lớn ở cái trước. Ngữ cảnh không dường như giúp tạo ra nhiều bài kiểm tra vượt qua hơn trong thiết lập bài kiểm tra bổ sung. Điều này có thể một phần vì bộ bài kiểm tra đã toàn diện, vì vậy mô hình có thể suy ra hầu hết thông tin nó cần về mã dưới kiểm tra từ các bài kiểm tra. Nó cũng có thể do các bộ bài kiểm tra thường (gần như) hoàn chỉnh trong thiết lập này, vì vậy việc tạo ra các bài kiểm tra bổ sung vượt qua (nhưng không mang lại độ bao phủ có ý nghĩa) tương đối đơn giản (ví dụ: bằng cách sao chép một bài kiểm tra hiện có Mục VII-C). Nhìn chung, những kết quả này hỗ trợ giả thuyết cốt lõi của chúng tôi rằng các mô hình của mã nên xem xét mối quan hệ giữa mã và các tệp kiểm tra để tạo ra các bài kiểm tra có ý nghĩa.

5) Các Chỉ số Thời gian chạy Khác: Bảng III cũng cho thấy so sánh giữa CAT-LM và các đường chuẩn StarCoder và CodeGen cho tất cả các chỉ số thời gian chạy. CAT-LM vượt trội so với cả StarCoder

*Mẫu số cho mỗi nhóm là số cặp tệp được hiển thị trong Bảng II nhân với 10, số lượng mẫu mỗi ngữ cảnh.

--- TRANG 8 ---

và các đường chuẩn CodeGen trong cả Python và Java qua việc biên dịch và vượt qua các thế hệ, với CAT-LM thường tạo ra nhiều mẫu biên dịch và vượt qua nhất. Thiết lập duy nhất mà các đường chuẩn CodeGen hoạt động hơi tốt hơn là trong việc tạo ra nhiều bài kiểm tra cuối cùng vượt qua cho Python. Tuy nhiên, tỷ lệ biên dịch của những bài kiểm tra do CodeGen tạo ra này thấp hơn đáng kể so với những bài được tạo bởi CAT-LM. Chúng tôi lưu ý rằng hiệu suất của CodeGen có thể bị thổi phồng trong thiết lập bài kiểm tra cuối cùng, vì nó có thể đã thấy các tệp từ tập kiểm tra trong quá trình huấn luyện.

CAT-LM vượt trội so với StarCoder và CodeGen cho cả Python và Java, tạo ra nhiều bài kiểm tra vượt qua hơn trung bình qua tất cả các thiết lập. Chúng tôi thấy rằng ngữ cảnh mã cải thiện hiệu suất qua hầu hết các thiết lập về cả chỉ số từ vựng và thời gian chạy.

B. Hoàn thành Bài kiểm tra

Đối với hoàn thành bài kiểm tra (xem Mục III-B để biết định nghĩa nhiệm vụ), chúng tôi so sánh CAT-LM với TeCo [15] trên các chỉ số từ vựng được nêu trong Mục VI-A2. Cụ thể, chúng tôi lấy mẫu 1000 câu lệnh ngẫu nhiên từ tập kiểm tra được phát hành bởi các tác giả của TeCo, trên đó chúng tôi thu được hiệu suất tương tự với TeCo như những gì được báo cáo trong bài báo gốc. Bảng IV cho thấy kết quả. CAT-LM vượt trội so với TeCo qua tất cả các chỉ số từ vựng, với mức tăng 36.6% trong khớp chính xác, 22.6% trong ROUGE và 40.4% trong điểm CodeBLEU. Ngay cả việc nhắc CAT-LM chỉ với ngữ cảnh kiểm tra (tức là không có ngữ cảnh mã) cũng mang lại kết quả tốt hơn đáng kể so với TeCo. Điều này nhấn mạnh rằng việc cung cấp toàn bộ tệp kiểm tra trước câu lệnh được hoàn thành làm ngữ cảnh, thích hơn chỉ các phương thức thiết lập, là hữu ích cho các mô hình để lý luận về những gì đang được kiểm tra.

Trái ngược với nhiệm vụ tạo bài kiểm tra, ngữ cảnh mã chỉ giúp ích nhẹ cho CAT-LM trong thiết lập này, với mức tăng trong điểm CodeBLEU 1.2% và tăng trong độ chính xác khớp chính xác 1.5%. Có vẻ như nhiều câu lệnh riêng lẻ trong các trường hợp kiểm tra có thể được hoàn thành tương đối dễ dàng dựa trên các mẫu được tìm thấy trong tệp kiểm tra, mà không cần xem xét mã dưới kiểm tra. Điều này gợi ý rằng hoàn thành câu lệnh ít đòi hỏi ngữ cảnh hơn đáng kể so với tạo toàn bộ trường hợp kiểm tra. Do đó chúng tôi lập luận rằng tạo toàn bộ bài kiểm tra là một nhiệm vụ phù hợp hơn để đánh giá các mô hình được huấn luyện cho việc tạo bài kiểm tra.

CAT-LM vượt trội so với TeCo qua tất cả các chỉ số từ vựng, với cải thiện 40.4% trong điểm CodeBLEU và 36.6% cải thiện trong độ chính xác khớp chính xác. Chúng tôi thấy rằng ngữ cảnh chỉ giúp ích nhẹ với dự đoán câu lệnh kiểm tra, chỉ ra rằng hoàn thành bài kiểm tra phần lớn có thể được thực hiện mà không có mã dưới kiểm tra, trái ngược với tạo toàn bộ bài kiểm tra.

Xem Mục 3 và 4 trong tài liệu bổ sung để có kết quả bổ sung về tất cả các nhiệm vụ.

--- TRANG 9 ---

BẢNG III: So sánh hiệu suất chỉ số từ vựng và thời gian chạy của các mô hình trên tập kiểm tra đã giữ lại cho Java và Python. CodeGen đề cập đến CodeGen-multi cho Java và kết quả CodeGen-mono cho Python. Chúng tôi chỉ báo cáo chỉ số từ vựng cho thiết lập bài kiểm tra đầu tiên và cuối cùng, vì không có bài kiểm tra vàng để so sánh trong thiết lập bài kiểm tra bổ sung của chúng tôi.

Java | Python
Chỉ số Từ vựng | Chỉ số Thời gian chạy | Chỉ số Từ vựng | Chỉ số Thời gian chạy
Mô hình | CodeBLEU | XMatch | Rouge | Compile | Pass | CodeBLEU | XMatch | Rouge | Compile | Pass

Bài kiểm tra Đầu tiên (Tổng: Java = 270, Python = 1120)
CAT-LM w Context | 41.4% | 15.4% | 60.9% | 50 | 22 | 21.0% | 0.3% | 39.4% | 384 | 44
CAT-LM w/o Context | 37.5% | 15.4% | 56.5% | 9 | 9 | 17.7% | 0.4% | 30.2% | 236 | 31
Codegen-2B | 35.5% | 7.7% | 56.8% | 24 | 14 | 18.2% | 0.0% | 30.9% | 259 | 37
Codegen-16B | 42.2% | 7.7% | 61.8% | 25 | 7 | 20.8% | 0.3% | 35.1% | 361 | 42
StarCoder | 44.6% | 10.9% | 62.2% | 28 | 16 | 24.0% | 1.8% | 38.8% | 269 | 23

Bài kiểm tra Cuối cùng (Tổng: Java = 180, Python = 930)
CAT-LM w Context | 55.4% | 20.8% | 70.8% | 54 | 17 | 38.3% | 4.8% | 54.9% | 335 | 77
CAT-LM w/o Context | 53.6% | 20.8% | 68.9% | 33 | 14 | 33.2% | 1.4% | 51.9% | 350 | 79
Codegen-2B | 51.7% | 13.0% | 69.2% | 43 | 16 | 36.3% | 2.2% | 53.2% | 326 | 84
Codegen-16B | 56.5% | 14.3% | 70.9% | 24 | 9 | 37.9% | 3.4% | 54.0% | 349 | 83
StarCoder | 56.9% | 21.0% | 69.9% | 34 | 17 | 37.6% | 4.2% | 54.5% | 227 | 65

Bài kiểm tra Bổ sung (Tổng: Java = 270, Python = 1230)
CAT-LM w Context | – | – | – | 41 | 17 | – | – | – | 380 | 98
CAT-LM w/o Context | – | – | – | 29 | 20 | – | – | – | 425 | 104
Codegen-2B | – | – | – | 17 | 8 | – | – | – | 376 | 90
Codegen-16B | – | – | – | 15 | 7 | – | – | – | 384 | 89
StarCoder | – | – | – | 17 | 10 | – | – | – | 269 | 36

BẢNG IV: So sánh CAT-LM và TeCo trên 1000 câu lệnh được lấy mẫu ngẫu nhiên trong tập kiểm tra của họ.

Mô hình | CodeBLEU | XMatch | Rouge
CAT-LM w/ Context | 67.1% | 50.4% | 82.8%
CAT-LM w/o Context | 65.9% | 48.9% | 82.2%
TeCo | 26.7% | 13.8% | 60.2%

C. So sánh Định tính

Cuối cùng, chúng tôi tiến hành một nghiên cứu trường hợp định tính quy mô nhỏ về các bài kiểm tra được tạo bởi CAT-LM, CodeGen-2B-multi [11], GPT-4 [37] và EvoSuite [4]. GPT-4 là một mô hình ngôn ngữ lớn hơn rất nhiều so với mô hình của chúng tôi, được huấn luyện với ngân sách không được tiết lộ bởi OpenAI. EvoSuite là một công cụ tạo bài kiểm tra phổ biến cho Java dựa trên thuật toán tiến hóa.

Chúng tôi phân tích một thế hệ vượt qua được lấy mẫu ngẫu nhiên từ CAT-LM trái ngược với các bài kiểm tra được tạo bởi các công cụ khác trong cùng ngữ cảnh qua từng thiết lập ba của chúng tôi (bài kiểm tra đầu tiên, bài kiểm tra cuối cùng và bài kiểm tra bổ sung). Các bài kiểm tra ở đây được tạo cho một lớp Bank, bao gồm các phương thức để thêm khách hàng, mở tài khoản và in tóm tắt tất cả tài khoản và khách hàng. Mục tiêu của chúng tôi là hiểu rõ hơn những lợi ích và nhược điểm của các bài kiểm tra được tạo bởi mỗi công cụ. Cụ thể, chúng tôi tìm kiếm các đặc điểm của bài kiểm tra chất lượng cao, như tên phương thức và biến có ý nghĩa, gọi phương thức dưới kiểm tra đúng cách và các khẳng định chất lượng cao. Chúng tôi chủ yếu thảo luận về thế hệ bài kiểm tra đầu tiên ở đây; để có tập đầy đủ các ví dụ của chúng tôi, xem Tài liệu Bổ sung.

CAT-LM: Danh sách 1 cho thấy thế hệ bài kiểm tra đầu tiên bởi CAT-LM. Tên của bài kiểm tra có tính thông tin, cùng với các biến của nó. Nó cũng tuân theo quy ước kiểm tra đơn vị của việc kiểm tra một phương thức cụ thể trong lớp Bank. Điều này nhất quán qua các ví dụ cho bài kiểm tra cuối cùng và bài kiểm tra bổ sung. Tuy nhiên, đối với ví dụ bài kiểm tra bổ sung của chúng tôi, CAT-LM đã sao chép bài kiểm tra trước đó và thay đổi tên của phương thức kiểm tra, không kiểm tra chức năng mới.

CodeGen: trong Danh sách 2, bài kiểm tra được tạo bởi CodeGen khá dễ đọc, đúng về mặt ngữ nghĩa, và trông tự nhiên. Tuy nhiên, nó sử dụng nhiều phương thức không tồn tại từ mã dưới kiểm tra—một hiện tượng được gọi phổ biến là "ảo giác"—vì nó thiếu nhận thức về việc triển khai của Bank. StarCoder hoạt động tương tự, tạo ra các bài kiểm tra dễ đọc, đúng về mặt ngữ nghĩa, và trông tự nhiên nhưng gặp phải ảo giác.

GPT-4: GPT-4 nhất quán hoạt động tốt nhất trong tất cả ba công cụ, tạo ra các bài kiểm tra hoặc giống hệt với ground truth hoặc kiểm tra chức năng mới mà không có bài kiểm tra hiện có nào làm. Danh sách 3 cho thấy thế hệ của GPT-4 cho trường hợp kiểm tra đầu tiên. Tương tự như CAT-LM, bài kiểm tra do GPT-4 tạo ra có tên định danh và khẳng định có ý nghĩa. GPT-4 có các bài kiểm tra tương tự tốt cho thiết lập bài kiểm tra cuối cùng và bài kiểm tra bổ sung của chúng tôi. Tuy nhiên, những kết quả này đi kèm với một số cảnh báo. Trước tiên, GPT-4 được huấn luyện trên một khối lượng dữ liệu rất lớn, bao gồm mã công khai, vì vậy khá có khả năng nó được huấn luyện trên dữ liệu kiểm tra của chúng tôi và do đó đã thấy các bài kiểm tra gốc.* Thứ hai, GPT-4 là một mô hình lớn hơn nhiều, với ngân sách huấn luyện cao hơn cỡ bậc độ lớn so với chúng tôi. Cho hiệu suất mạnh của chúng tôi so với các mô hình CodeGen (đã đắt đỏ hơn nhiều), chúng tôi mong đợi rằng việc mở rộng khiêm tốn phương pháp huấn luyện của chúng tôi có thể mang lại kết quả tương tự hoặc tốt hơn.

EvoSuite: EvoSuite hoạt động tệ nhất trong tất cả ba thiết lập.

*Thực tế, một cảnh báo tương tự áp dụng cho CodeGen, mà chúng tôi vượt trội.

--- TRANG 10 ---

1@Test
2public void testCustomerSummary() {
3 Bank bank = new Bank();
4 assertEquals("Customer Summary", "Customer
Summary", bank.customerSummary());
5}
Danh sách 1: Thế hệ bài kiểm tra đầu tiên CAT-LM

1@Test
2public void testBank() {
3 Bank bank = new Bank();
4 assertEquals(0, bank.getBalance(),
DOUBLE_DELTA);
5 bank.deposit(100);
6 assertEquals(100, bank.getBalance(),
DOUBLE_DELTA);
7 bank.withdraw(50);
8 assertEquals(50, bank.getBalance(),
DOUBLE_DELTA);
9}
Danh sách 2: Thế hệ bài kiểm tra đầu tiên CodeGen

1@Before
2public void setUp() {
3 bank = new Bank();
4}
5
6@Test
7public void testAddCustomerAndCustomerSummary()
{
8 Customer customer1 = new Customer("John
Doe");
9 bank.addCustomer(customer1);
10
11 String expectedSummary = "Customer
Summary\n - John Doe (0 accounts)";
12 String actualSummary =
bank.customerSummary();
13
14 assertEquals(expectedSummary,
actualSummary);
15}
Danh sách 3: Thế hệ bài kiểm tra đầu tiên GPT-4

1@Test(timeout = 4000)
2public void test0() throws Throwable {
3 Bank bank0 = new Bank();
4 Customer customer0 = new Customer("v\"PD");
5 bank0.addCustomer(customer0);
6 Account account0 = new Account(0);
7 account0.deposit(148.3628547);
8 customer0.openAccount(account0);
9 double double0 = bank0.totalInterestPaid();
10 assertEquals(0.14836285470000002, double0,
0.01);
11}
Danh sách 4: Thế hệ bài kiểm tra đầu tiên EvoSuite

Hình 8: Ví dụ các bài kiểm tra đầu tiên được tạo bởi CAT-LM, CodeGen, GPT-4, và EvoSuite. CAT-LM và GPT-4 đều tạo ra các bài kiểm tra thực tế và dễ đọc; EvoSuite gặp khó khăn với quy ước đặt tên kém và các bài kiểm tra không thực tế. CodeGen tạo ra các trường hợp kiểm tra dễ đọc, nhưng ảo giác các phương thức trong mã dưới kiểm tra. Xem Mục 5 trong tài liệu bổ sung để có thêm ví dụ.

Danh sách 4 cho thấy việc hoàn thành EvoSuite cho lớp bank. Bài kiểm tra được tạo sử dụng quy ước đặt tên rất kém, như đặt tên phương thức test0, và mỗi biến bank0, customer0, và account0. Các số tiền gửi không có ý nghĩa logic, vì chúng không được làm tròn đến cent gần nhất. Cũng có timeout 4000 millisecond. Những timeout như vậy rất có khả năng dẫn đến các bài kiểm tra không ổn định, nơi bài kiểm tra này có thể vượt qua trong một môi trường và timeout trong một môi trường khác. Các thế hệ khác bởi EvoSuite, gặp các vấn đề tương tự, bao gồm thiếu assert và sử dụng xử lý ngoại lệ giả tạo. Do quy ước đặt tên kém này và việc sử dụng các assert tầm thường, rất khó hiểu những gì đang được kiểm tra trong thế hệ của EvoSuite.

Cả GPT-4 và CAT-LM đều tạo ra các bài kiểm tra chất lượng cao, kiểm tra các tình huống thực tế với các assert dễ đọc. Tuy nhiên CAT-LM gặp khó khăn để tạo ra các bài kiểm tra khác biệt có ý nghĩa trong thiết lập bài kiểm tra bổ sung. CodeGen và StarCoder tạo ra các bài kiểm tra rất dễ đọc, nhưng không chính xác. EvoSuite gặp khó khăn để tạo ra các bài kiểm tra có ý nghĩa; nó sử dụng quy ước đặt tên kém và xử lý ngoại lệ giả tạo.

VIII. CÔNG TRÌNH LIÊN QUAN

Tạo Bài kiểm tra Cổ điển: Các kỹ thuật tạo bài kiểm tra cổ điển sử dụng cả kỹ thuật black-box và white-box để tạo ra đầu vào kiểm tra và mã kiểm tra. Các kỹ thuật ngẫu nhiên/fuzzing như Randoop [38], aflplusplus [39] và honggfuzz sử dụng độ bao phủ để hướng dẫn tạo ra các tiền tố kiểm tra. Các công cụ kiểm tra thuộc tính như Korat [40], QuickCheck [41] và Hypothesis [42] cho phép nhà phát triển chỉ định một tập hợp thuộc tính và sau đó tạo ra một bộ bài kiểm tra kiểm tra các thuộc tính được chỉ định. PeX [43] và Eclipser [44] sử dụng thực thi tượng trưng động để lý luận về nhiều đường dẫn chương trình và tạo ra các đầu vào thú vị. Vấn đề cốt lõi với fuzzing và các kỹ thuật tạo bài kiểm tra cổ điển là sự phụ thuộc của chúng vào việc chương trình gặp sự cố hoặc hành vi ngoại lệ trong việc điều khiển tạo bài kiểm tra [3], điều này hạn chế mức độ kiểm thử mà chúng cung cấp. EvoSuite [4] giải quyết những thách thức này bằng cách sử dụng kiểm thử đột biến để làm cho bộ bài kiểm tra được tạo ra gọn gàng, mà không mất độ bao phủ. Tuy nhiên, EvoSuite tạo ra các bài kiểm tra trông "không tự nhiên", và khác biệt đáng kể so với các bài kiểm tra của con người, gặp phải cả vấn đề về phong cách và khả năng đọc [5], [45], [46].

Tạo Bài kiểm tra Neural: Gần đây hơn, các phương pháp tạo bài kiểm tra neural đã được phát triển để tạo ra các bài kiểm tra tự nhiên và dễ hiểu hơn cho con người. ConTest[8] sử dụng một mô hình transformer chung, sử dụng biểu diễn cây của mã để tạo ra các câu lệnh assert. ATLAS [7], ReAssert [47], AthenaTest [48] và TOGA [3] mở rộng công trình này bằng cách tận dụng kiến trúc transformer cho nhiệm vụ này. Chúng cho thấy rằng các assert được tạo ra của chúng tự nhiên hơn và được các nhà phát triển ưa thích khi so sánh với các công cụ hiện có như EvoSuite. TeCo [15] mở rộng phạm vi hoàn thành bài kiểm tra bằng cách hoàn thành các câu lệnh trong một bài kiểm tra, từng câu lệnh một. Chúng tận dụng ngữ cảnh thực thi và thông tin thực thi để thông báo cho dự đoán câu lệnh tiếp theo của chúng, vượt trội so với TOGA và ATLAS trên một loạt chỉ số từ vựng. Trong khi những phương pháp neural này giải quyết nhiều vấn đề về khả năng đọc của các phương pháp tạo bài kiểm tra cổ điển, chúng tập trung vào việc tạo ra các câu lệnh riêng lẻ trong một bài kiểm tra, điều này cung cấp lợi ích tiết kiệm thời gian ít hơn đáng kể so với việc tạo ra toàn bộ bài kiểm tra.

Mô hình Ngôn ngữ Lớn của Mã: Các mô hình ngôn ngữ lớn (LLM) có thể hoạt động tốt qua nhiều nhiệm vụ khi được nhắc với hướng dẫn và ví dụ [49], [30]. Codex [10] là một LLM tự hồi quy (tạo từ trái sang phải) với 12B tham số, được fine-tune từ GPT-3 trên 54 triệu repository Python GitHub. CodeGen-16B, mà chúng tôi so sánh, vượt trội so với mô hình này [11]. Sau đó, các lần lặp chưa được xuất bản của Codex cũng đã được áp dụng cho các thiết lập thương mại, cung cấp sức mạnh cho GitHub's Copilot [14]. TestPilot [50] sử dụng Codex để tạo ra các bài kiểm tra đơn vị. Tuy nhiên, nó đòi hỏi khối lượng lớn tài liệu làm đầu vào, thường không có sẵn cho các dự án mã nguồn mở. Trong khi tất cả những mô hình này hoạt động tốt trong việc tạo mã, chúng tương đối kém (cho kích thước của chúng) trong việc tạo ra bài kiểm tra cho mã. Những mô hình này thường được huấn luyện trên một kho dữ liệu được xáo trộn ngẫu nhiên của toàn bộ tệp, và do đó không học được sự căn chỉnh của các bài kiểm tra với mã dưới kiểm tra. Chúng tôi tiền huấn luyện một mô hình ngôn ngữ tương đối nhỏ với ngân sách khiêm tốn hơn nhiều mà rõ ràng học được việc căn chỉnh mã và các tệp kiểm tra tương ứng, điều này mang lại hiệu suất tốt hơn đáng kể so với các mô hình được huấn luyện cổ điển lớn hơn một cách khiêm tốn.

IX. KẾT LUẬN

Chúng tôi phát triển CAT-LM, một mô hình ngôn ngữ kiểu GPT với 2.7 tỷ tham số được tiền huấn luyện sử dụng một tín hiệu mới rõ ràng xem xét ánh xạ giữa mã và các tệp kiểm tra khi có sẵn. Chúng tôi chọn sử dụng cửa sổ ngữ cảnh lớn hơn 8,192 token, gấp 4 lần so với các mô hình tạo mã điển hình, để đảm bảo rằng ngữ cảnh mã có sẵn khi tạo ra mã kiểm tra. Chúng tôi đánh giá CAT-LM trên cả tạo phương thức kiểm tra và hoàn thành bài kiểm tra, với CAT-LM vượt trội so với các đường chuẩn hiện đại CodeGen, StarCoder, và TeCo, ngay cả với các đường chuẩn CodeGen và StarCoder có ngân sách huấn luyện và kích thước mô hình lớn hơn đáng kể. Chúng tôi cho thấy rằng việc thêm ngữ cảnh bổ sung giúp ích cho CAT-LM, với ngữ cảnh mã cải thiện đáng kể cả hiệu suất chỉ số từ vựng và thời gian chạy. Mặc dù có hiệu suất mạnh, CAT-LM có những hạn chế bao gồm việc nó có thể gặp khó khăn để tổng quát hóa cho các dự án không phổ biến, và việc so sánh với TeCo có khả năng có rò rỉ dữ liệu (CAT-LM có khả năng đã thấy tập kiểm tra của TeCo trong tiền huấn luyện). Chúng tôi hy vọng rằng những hạn chế này có thể được khắc phục trong công trình tương lai. Nhìn chung, chúng tôi làm nổi bật cách kết hợp kiến thức miền, đó là mối quan hệ giữa mã và các tệp kiểm tra, có thể được sử dụng để tạo ra các mô hình mạnh mẽ hơn cho việc tạo bài kiểm tra tự động.

Tính khả dụng dữ liệu: Các trọng số mô hình cho CAT-LM, mã và tập dữ liệu để huấn luyện và đánh giá CAT-LM, kết quả của các thí nghiệm bổ sung và so sánh với TeCo, CodeGen và StarCoder có sẵn tại: https://doi.org/10.5281/zenodo.7901830.

X. LỜI CẢM ơN

Các tác giả muốn cảm ơn Charles Sutton vì sự hướng dẫn của ông trong vai trò là một phần của Google Collab Ph.D. Fellowship, cũng bao gồm $20,000 tín dụng đám mây mà không có đó công trình này sẽ không thể thực hiện được. Chúng tôi bổ sung cảm ơn các tác giả của TeCo vì đã cung cấp cho chúng tôi dữ liệu và mã cho các thí nghiệm đường chuẩn của chúng tôi. Chúng tôi cũng cảm ơn chương trình tín dụng nghiên cứu Google Cloud cho $5,000 tín dụng đám mây bổ sung cho phép chúng tôi tiếp tục huấn luyện mô hình. Công trình này được hỗ trợ một phần bởi Quỹ Khoa học Quốc gia Hoa Kỳ, giải thưởng CCF-2129388 và CCF-1910067.

TÀI LIỆU THAM KHẢO

[1] M. Beller, G. Gousios, A. Panichella, và A. Zaidman, "When, how, and why developers (do not) test in their ides," trong Joint Meeting of the European Software Engineering Conference and the Symposium on the Foundations of Software Engineering, ser. ESEC/FSE '15, 2015, p. 179–190.

[2] M. Beller, G. Gousios, và A. Zaidman, "How (much) do developers test?" trong International Conference on Software Engineering, ser. ICSE '15, 2015, p. 559–562.

[3] E. Dinella, G. Ryan, T. Mytkowicz, và S. Lahiri, "Toga: A neural method for test oracle generation," trong International Conference on Software Engineering, ser. ICSE '22, 2022, p. 2130–2141.

[4] G. Fraser và A. Arcuri, "Evosuite: Automatic test suite generation for object-oriented software," trong Joint Meeting of the European Software Engineering Conference and the Symposium on the Foundations of Software Engineering, ser. ESEC/FSE '11, 2011, p. 416–419.

[5] C. Brandt và A. Zaidman, "Developer-centric test amplification: The interplay between automatic generation human exploration," Empirical Software Engineering, vol. 27, no. 4, 2022.

[6] R. Baldoni, E. Coppa, D. C. D'Elia, C. Demetrescu, và I. Finocchi, "A Survey of Symbolic Execution Techniques," ACM Computing Survey, vol. 51, no. 3, pp. 50–88, 2018.

[7] C. Watson, M. Tufano, K. Moran, G. Bavota, và D. Poshyvanyk, "On learning meaningful assert statements for unit test cases," CoRR, vol. abs/2002.05800, 2020.

[8] J. Villmow, J. Depoix, và A. Ulges, "ConTest: A Unit Test Completion Benchmark featuring Context," trong Workshop on Natural Language Processing for Programming, Aug. 2021, pp. 17–25.

[9] A. Panichella, S. Panichella, G. Fraser, A. A. Sawant, và V. J. Hellendoorn, "Revisiting Test Smells in Automatically Generated Tests: Limitations, Pitfalls, and Opportunities," trong International Conference on Software Maintenance and Evolution, 2020, pp. 523–533.

[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. W. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, I. Babuschkin, S. A. Balaji, S. Jain, A. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, và W. Zaremba, "Evaluating Large Language Models Trained on Code," CoRR, vol. abs/2107.03374, 2021.

[11] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, và C. Xiong, "A Conversational Paradigm for Program Synthesis," CoRR, vol. abs/2203.13474, 2022.

[12] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, và M. Lewis, "InCoder: A Generative Model for Code Infilling and Synthesis," CoRR, vol. abs/2204.05999, 2022.

[13] M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek, và M. Chen, "Efficient Training of Language Models to Fill in the Middle," CoRR, vol. abs/2207.14255, 2022.

[14] "GitHub Copilot," 2021. [Trực tuyến]. Có sẵn: https://github.com/features/copilot

[15] P. Nie, R. Banerjee, J. J. Li, R. J. Mooney, và M. Gligoric, "Learning deep semantics for test completion," trong International Conference on Software Engineering, ser. ICSE '23, 2023, p. 2111–2123.

[16] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. Murthy, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy,

--- TRANG 12 ---

U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, và H. de Vries, "Starcoder: may the source be with you!" 2023.

[17] L. v. Werra, "Codeparrot," https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot.

[18] "SentencePiece." [Trực tuyến]. Có sẵn: https://github.com/google/sentencepiece

[19] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark et al., "Training compute-optimal large language models," CoRR, vol. arXiv:2203.15556, 2022.

[20] "GPT-neox Toolkit." [Trực tuyến]. Có sẵn: https://github.com/EleutherAI/gpt-neox

[21] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, và C. Ré, "FlashAttention: Fast and memory-efficient exact attention with IO-awareness," trong Advances in Neural Information Processing Systems, 2022.

[22] "GitHub REST API." [Trực tuyến]. Có sẵn: https://docs.github.com/en/rest

[23] M. Allamanis, "The adverse effects of code duplication in machine learning models of code," trong International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, ser. SPLASH '19, 2019, pp. 143–153.

[24] C. V. Lopes, P. Maj, P. Martins, V. Saini, D. Yang, J. Zitny, H. Sajnani, và J. Vitek, "Déjà vu: a map of code duplicates on GitHub," trong Proceedings of the ACM on Programming Languages, ser. OOPSLA '17, vol. 1, 2017, pp. 1–28.

[25] "TheFuzz: Fuzzy String Matching in Python." [Trực tuyến]. Có sẵn: https://github.com/seatgeek/thefuzz

[26] P. S. Kochhar, T. F. Bissyandé, D. Lo, và L. Jiang, "An empirical study of adoption of software testing in open source projects," trong International Conference on Quality Software, ser. ICQS '13, 2013, pp. 103–112.

[27] H. H. F. d. Souza, I. Wiese, I. Steinmacher, và R. Ré, "A characterization study of testing contributors and their contributions in open source projects," trong Brazilian Symposium on Software Engineering, ser. SBES '22, 2022, pp. 95–105.

[28] T. Kudo, "Subword regularization: Improving neural network translation models with multiple subword candidates," trong Annual Meeting of the Association for Computational Linguistics, ser. ACL '18, 2018, pp. 66–75.

[29] F. F. Xu, U. Alon, G. Neubig, và V. J. Hellendoorn, "A Systematic Evaluation of Large Language Models of Code," CoRR, vol. abs/2202.13169, 2022.

[30] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, và G. Lample, "LLaMA: Open and Efficient Foundation Language Models," CoRR, vol. abs/2302.13971, 2023.

[31] C.-Y. Lin, "ROUGE: A package for automatic evaluation of summaries," trong Conference on Text Summarization Branches Out, 2004, pp. 74–81.

[32] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan, M. Zhou, A. Blanco, và S. Ma, "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis," CoRR, vol. abs/2009.10297, 2020.

[33] X. Hu, G. Li, X. Xia, D. Lo, và Z. Jin, "Deep code comment generation with Hybrid lexical and syntactical information," Empirical Software Engineering, vol. 25, no. 3, pp. 2179–2217, 2020.

[34] A. LeClair, S. Jiang, và C. McMillan, "A Neural model for generating natural language summaries of program subroutines," trong International Conference on Software Engineering, ser. ICSE '19, 2019, pp. 795–806.

[35] Y. Wang, W. Wang, S. Joty, và S. C. Hoi, "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation," trong Conference on Empirical Methods in Natural Language Processing, 2021, pp. 8696–8708.

[36] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, và S. Liu, "Codexglue: A machine learning benchmark dataset for code understanding and generation," CoRR, vol. abs/2102.04664, 2021.

[37] OpenAI, "Gpt-4 technical report," 2023.

[38] C. Pacheco và M. D. Ernst, "Randoop: Feedback-directed random testing for Java," trong Conference on Object-Oriented Programming Systems and Applications Companion, ser. OOPSLA '07, 2007, pp. 815–816.

[39] A. Fioraldi, D. Maier, H. Eißfeldt, và M. Heuse, "AFL++: Combining incremental steps of fuzzing research," trong Conference on Offensive Technologies, ser. WOOT '20, 2020, pp. 10–10.

[40] C. Boyapati, S. Khurshid, và D. Marinov, "Korat: Automated testing based on Java predicates," SIGSOFT Software Engineering Notes, vol. 27, no. 4, pp. 123–133, 2002.

[41] K. Claessen và J. Hughes, "Quickcheck: A lightweight tool for random testing of haskell programs," trong International Conference on Functional Programming, ser. ICFP '00, 2000, p. 268–279.

[42] D. MacIver, Z. Hatfield-Dodds, và M. Contributors, "Hypothesis: A New Approach to property-based testing," Journal of Open Source Software, vol. 4, no. 43, p. 1891, 2019.

[43] N. Tillmann và P. de Halleux, "Pex - white box test generation for .net," trong Tests and Proofs, ser. TAP '08, vol. 4966, April 2008, pp. 134–153.

[44] J. Choi, J. Jang, C. Han, và S. K. Cha, "Grey-box concolic testing on binary code," trong International Conference on Software Engineering, ser. ICSE '19, 2019, pp. 736–747.

[45] E. Daka, J. M. Rojas, và G. Fraser, "Generating unit tests with descriptive names or: Would you name your children Thing1 and Thing2?" trong International Symposium on Software Testing and Analysis, ser. ISSTA '17, 2017, pp. 57–67.

[46] B. Robinson, M. D. Ernst, J. H. Perkins, V. Augustine, và N. Li, "Scaling up automated test generation: Automatically generating maintainable regression unit tests for programs," trong Joint Meeting of the European Software Engineering Conference and the Symposium on the Foundations of Software Engineering, ser. ASE '11, 2011, pp. 23–32.

[47] R. White và J. Krinke, "Reassert: Deep learning for assert generation," CoRR, vol. abs/2011.09784, 2020.

[48] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, và N. Sundaresan, "Unit test case generation with transformers," CoRR, vol. abs/2009.05617, 2020.

[49] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và D. Amodei, "Language models are Few-Shot learners," trong Advances in Neural Information Processing Systems, 2020, pp. 1877–1901.

[50] M. Schäfer, S. Nadi, A. Eghbali, và F. Tip, "Adaptive Test Generation Using a Large Language Model," CoRR, vol. abs/2302.06527, 2023.
