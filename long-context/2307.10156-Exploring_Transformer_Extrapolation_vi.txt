# 2307.10156.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2307.10156.pdf
# Kích thước tệp: 793723 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Khám phá Ngoại suy Transformer
1Zhen Qin*,1Yiran Zhong*†,2Hui Deng
1OpenNLPLab, Phòng thí nghiệm Trí tuệ nhân tạo Thượng Hải,2Đại học Bách khoa Tây Bắc
https://github.com/OpenNLPLab/Rpe
Tóm tắt
Ngoại suy độ dài đã thu hút sự chú ý đáng kể gần đây vì nó cho phép các transformer được kiểm tra trên các chuỗi dài hơn so với những chuỗi được sử dụng trong huấn luyện. Nghiên cứu trước đây đã chỉ ra rằng tính chất này có thể đạt được bằng cách sử dụng các Mã hóa Vị trí Tương đối (RPE) được thiết kế cẩn thận. Mặc dù các phương pháp này hoạt động tốt trên nhiều kho ngữ liệu khác nhau, các điều kiện cho ngoại suy độ dài vẫn chưa được điều tra. Bài báo này cố gắng xác định loại RPE nào cho phép ngoại suy độ dài thông qua phân tích toán học và thực nghiệm kỹ lưỡng. Chúng tôi khám phá ra rằng một transformer chắc chắn sẽ sở hữu tính chất này miễn là chuỗi tương ứng với hàm mũ của RPE hội tụ. Hai thực hành được rút ra từ các điều kiện và được kiểm tra trong các tác vụ mô hình hóa ngôn ngữ trên nhiều kho ngữ liệu khác nhau. Như một phần thưởng từ các điều kiện, chúng tôi rút ra một Trường Tiếp nhận Lý thuyết (TRF) mới để đo trường tiếp nhận của RPE mà không cần thực hiện bất kỳ bước huấn luyện nào. Các thí nghiệm mở rộng được thực hiện trên các tập dữ liệu Wikitext-103, Books, Github và WikiBook để chứng minh tính khả thi của các điều kiện chúng tôi phát hiện. Chúng tôi cũng so sánh TRF với Trường Tiếp nhận Thực nghiệm (ERF) trên các mô hình khác nhau, cho thấy xu hướng phù hợp nhất quán trên các tập dữ liệu này.

Giới thiệu
Transformer (Vaswani et al. 2017) đang tiến bộ đều đặn trong các lĩnh vực xử lý ngôn ngữ tự nhiên (Qin et al. 2023b; Devlin et al. 2019; Liu et al. 2019; Qin et al. 2022b,a; Liu et al. 2022; Qin and Zhong 2023), thị giác máy tính (Dosovitskiy et al. 2020; Sun et al. 2022b; Lu et al. 2022; Hao et al. 2024), và xử lý âm thanh (Gong, Chung, and Glass 2021; Akbari et al. 2021; Gulati et al. 2020; Sun et al. 2022a). Mặc dù nó vượt trội hơn các kiến trúc khác như RNN (Cho et al. 2014; Qin, Yang, and Zhong 2023) và CNN (Kim 2014; Hershey et al. 2016; Gehring et al. 2017) trong nhiều tác vụ mô hình hóa chuỗi, việc thiếu khả năng ngoại suy độ dài hạn chế khả năng xử lý một phạm vi rộng các độ dài chuỗi của nó, tức là các chuỗi suy luận cần bằng hoặc ngắn hơn các chuỗi huấn luyện. Tăng độ dài chuỗi huấn luyện chỉ là một giải pháp tạm thời vì độ phức tạp không gian-thời gian tăng theo bậc hai với độ dài chuỗi. Một lựa chọn khác là mở rộng độ dài chuỗi suy luận bằng cách chuyển đổi các khối attention đầy đủ đã được huấn luyện thành các khối attention cửa sổ trượt (Beltagy, Peters, and Cohan 2020), nhưng điều này sẽ dẫn đến hiệu quả tệ hơn đáng kể so với tốc độ attention đầy đủ (Press, Smith, and Lewis 2022). Làm thế nào để giải quyết vĩnh viễn vấn đề này mà không phát sinh chi phí bổ sung đã nổi lên như một chủ đề mới.

Một giải pháp chính thống cho ngoại suy độ dài là thiết kế một Mã hóa Vị trí Tương đối (RPE) (Qin et al. 2023c) tập trung attention vào các token lân cận. Ví dụ, ALiBi (Press, Smith, and Lewis 2022) áp dụng các bias phân rã tuyến tính cho attention để giảm đóng góp từ các token xa. Kerple (Chi et al. 2022) điều tra các hạt nhân xác định tích cực có điều kiện bất biến dịch chuyển trong RPE và đề xuất một tập hợp các hạt nhân thúc đẩy tính chất ngoại suy độ dài. Nó cũng chỉ ra rằng ALiBi là một trong những thể hiện của nó. Sandwich (Chi, Fan, and Rudnicky 2022) đề xuất một giả thuyết để giải thích bí mật đằng sau ALiBi và chứng minh thực nghiệm nó bằng cách tích hợp giả thuyết vào các embedding vị trí hình sin.

Để điều tra ngoại suy transformer, trước tiên chúng tôi thiết lập một giả thuyết về lý do tại sao các phương pháp ngoại suy độ dài dựa trên RPE hiện có (Qin et al. 2023a) có khả năng ngoại suy các chuỗi trong suy luận dựa trên phân tích thực nghiệm. Sau đó, chúng tôi xác định các điều kiện của RPE thỏa mãn giả thuyết thông qua phân tích toán học. Cuối cùng, các điều kiện đã khám phá được xác thực thực nghiệm trên nhiều kho ngữ liệu khác nhau. Cụ thể, chúng tôi giả định rằng do các bias phân rã, các phương pháp ngoại suy độ dài dựa trên RPE hiện có hoạt động tương tự như attention cửa sổ trượt, tức là chỉ có các token trong một phạm vi nhất định mới có thể ảnh hưởng đến điểm attention. Một transformer có thể ngoại suy chắc chắn trong tình huống này vì các token ngoài phạm vi không có tác động đến kết quả attention. Chúng tôi rút ra rằng một transformer được đảm bảo thỏa mãn giả thuyết này nếu chuỗi tương ứng với hàm mũ của RPE của nó hội tụ. Dựa trên quan sát này, chúng tôi chỉ ra rằng các phương pháp dựa trên RPE trước đây (Press, Smith, and Lewis 2022; Chi et al. 2022) có thể được coi là các thể hiện cụ thể dưới các điều kiện. Hai thực hành mới từ các điều kiện được rút ra và đánh giá trong mô hình hóa ngôn ngữ.

Các điều kiện quan sát được không chỉ làm sáng tỏ bí mật của ngoại suy độ dài mà còn cung cấp một góc nhìn mới về việc tính toán Trường Tiếp nhận Lý thuyết (TRF) của RPE. Trái ngược với các phương pháp trước đây yêu cầu gradient huấn luyện để tính toán TRF, chúng tôi đề xuất một cách mới để tính toán TRF chỉ dựa trên công thức của RPE. Các thí nghiệm mở rộng trên Wikitext-103 (Merity et al. 2016), Books (Zhu et al. 2015), Github (Gao et al. 2020), và WikiBook (Wettig et al. 2022) xác thực các điều kiện. TRF được tính toán bằng phương pháp của chúng tôi phù hợp đáng kể với xu hướng của Trường Tiếp nhận Thực nghiệm (ERF) trong các tình huống thực tế.

Khái niệm sơ bộ
Trước khi bắt đầu hành trình khám phá, chúng tôi giới thiệu một số khái niệm sơ bộ sẽ được sử dụng xuyên suốt bài báo, như attention softmax, mã hóa vị trí tương đối, ngoại suy độ dài, và attention cửa sổ trượt. Chúng tôi cũng cung cấp các ký hiệu cần thiết cho phân tích tiếp theo, tức là chúng tôi sử dụng M để biểu thị một ma trận và m⊤ᵢ để đại diện cho hàng thứ i của M. Các ký hiệu toán học đầy đủ có thể được tìm thấy trong Phụ lục. Theo nghiên cứu trước đây (Press, Smith, and Lewis 2022), chúng tôi giới hạn phân tích của mình đối với các mô hình ngôn ngữ nhân quả và giả định rằng độ dài chuỗi tối đa trong quá trình huấn luyện là m.

Attention softmax
Attention softmax là một thành phần chính của transformer hoạt động trên các ma trận query Q, key K và value V. Mỗi ma trận là một ánh xạ tuyến tính nhận X∈Rⁿˣᵈ làm đầu vào:
Q=XWᵠ,K=XWᴷ,V=XWᵛ∈Rⁿˣᵈ, (1)
trong đó n là độ dài chuỗi và d là chiều của đặc trưng ẩn. Ma trận attention đầu ra O∈Rⁿˣᵈ có thể được công thức hóa như:
O= Softmax( QK⊤/√d)V. (2)
Để ngăn chặn rò rỉ thông tin trong mô hình hóa ngôn ngữ nhân quả, một ma trận mặt nạ M∈Rⁿˣⁿ được sử dụng để đảm bảo rằng các token hiện tại chỉ có thể nhìn thấy các token trước đó và chính chúng. Các phần tử tam giác dưới của M là 0, và các phần tử tam giác trên, ngoại trừ đường chéo, là −∞. Khi đó ma trận attention đầu ra O cho các mô hình ngôn ngữ nhân quả sẽ là:
O= Softmax( QK⊤/√d+M)V. (3)
Lưu ý rằng Phương trình 3 có thể được coi là một dạng tổng quát của attention, tức là khi các phần tử của M đều là 0, Phương trình 3 bị thoái hóa thành Phương trình 2. Để thuận tiện cho thảo luận, chúng tôi sử dụng Phương trình 3 để đại diện cho tính toán attention.

Mã hóa vị trí tương đối
Mã hóa vị trí được thiết kế để tiêm bias vị trí vào transformer. Mã hóa Vị trí Tuyệt đối (APE) (Vaswani et al. 2017; Gehring et al. 2017) và Mã hóa Vị trí Tương đối (RPE) (Su et al. 2021; Liutkus et al. 2021; Press, Smith, and Lewis 2022; Chi et al. 2022) là hai loại mã hóa vị trí phổ biến nhất. Trong bài báo này, chúng tôi tập trung vào RPE vì nó là chìa khóa cho ngoại suy độ dài, như được chỉ ra trong (Press, Smith, and Lewis 2022). Một attention với RPE có thể được viết như:
O= Softmax( QK⊤/√d+M+P)V, (4)
trong đó P∈Rⁿˣⁿ là một ma trận Toeplitz mã hóa thông tin vị trí tương đối, tức là pᵢⱼ=pᵢ₋ⱼ. Đáng chú ý rằng M và P có thể được hợp nhất, và ma trận hợp nhất vẫn là một ma trận Toeplitz. Chúng tôi sử dụng R để đại diện cho ma trận hợp nhất và viết lại Phương trình 4 như:
O= Softmax( QK⊤/√d+R)V. (5)

Định nghĩa ngoại suy độ dài
Tính chất ngoại suy độ dài cho phép một mô hình được kiểm tra trên các chuỗi dài hơn so với những chuỗi được sử dụng trong huấn luyện. Các cấu trúc mô hình hóa chuỗi trước đây như RNN (Hochreiter and Schmidhuber 1997) và CNN (Gehring et al. 2017) thường sở hữu tính chất này một cách tự nhiên, nhưng đó là một tác vụ khó khăn đối với transformer. Tính chất này chỉ có mặt trong các transformer cửa sổ trượt và một số biến thể transformer với RPE được thiết kế đặc biệt (Chi et al. 2022; Press, Smith, and Lewis 2022; Chi, Fan, and Rudnicky 2022).

Trong mô hình hóa ngôn ngữ, một token chỉ có thể nhìn thấy chính nó và các token trước đó. Do đó, bất kể độ dài chuỗi, hiệu suất nên ổn định đối với các token lân cận nằm trong độ dài chuỗi huấn luyện (Beltagy, Peters, and Cohan 2020). Đối với các token nằm ngoài phạm vi, hiệu suất sẽ giảm nếu mô hình không hỗ trợ ngoại suy độ dài (Press, Smith, and Lewis 2022). Dựa trên quan sát trên, chúng tôi đưa ra định nghĩa về ngoại suy độ dài:

Định nghĩa 0.1. Đối với một mô hình ngôn ngữ F, cho tập dữ liệu X, nếu với bất kỳ n nào, có:
|pplₙ(X,F)−pplₘ(X,F)|/pplₘ(X,F)< δ, (6)
thì F được coi là có tính chất ngoại suy.
Ở đây δ > 0 là một hằng số nhỏ, pplₙ(X,F) có nghĩa là F tính toán độ phức tạp với độ dài chuỗi tối đa n trên tập dữ liệu X. Theo kinh nghiệm, nếu |pplₙ(X,F)− pplₘ(X,F)|/pplₘ(X,F) trở nên rất lớn (≫1) khi n tăng, chúng tôi coi rằng F không có tính chất ngoại suy.

Attention cửa sổ trượt
Để thuận tiện cho các thảo luận tiếp theo, chúng tôi định nghĩa một attention cửa sổ tại vị trí i và kích thước cửa sổ j như sau:
oʲᵢ=∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢexp(q⊤ᵢkₛ/√d) exp( rᵢₛ)vₛ / ∑ᵢ₋ⱼ₊₁≤ₜ≤ᵢexp(q⊤ᵢkₜ/√d) exp( rᵢₜ)
≜∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢcᵢₛvₛ / Cᵢⱼ,(7)
trong đó Cᵢⱼ=∑ᵢ₋ⱼ₊₁≤ₜ≤ᵢcᵢₜ, cᵢⱼ=aᵢⱼbᵢⱼ, aᵢⱼ= exp(q⊤ᵢkⱼ/√d), bᵢⱼ= exp( rᵢⱼ), j≤i.

Chúng tôi tiếp tục giả định ∥xᵢ∥ ≤l,x∈ {q,k,v}, trong đó l >0 là một hằng số. oʲᵢ đại diện cho đầu ra attention của token thứ i, tương tác với j token đứng trước nó. Lưu ý rằng attention cửa sổ tự nhiên sở hữu khả năng ngoại suy độ dài.

Có hai cách để suy luận attention cửa sổ: suy luận không chồng lấp và suy luận cửa sổ trượt như được hiển thị ở phía bên phải của Hình 1. Trong suy luận cửa sổ trượt, các token trong mỗi cửa sổ trượt phải được mã hóa lại nhiều lần, làm cho nó chậm hơn đáng kể so với cửa sổ không chồng lấp. Trong bảng 1, chúng tôi so sánh thời gian suy luận trung bình trên một nhóm kích thước cửa sổ giữa suy luận cửa sổ trượt và suy luận cửa sổ không chồng lấp. Cửa sổ trượt chậm hơn gấp 44 lần so với cửa sổ không chồng lấp. Tuy nhiên, như được hiển thị ở phía bên trái của Hình 1, suy luận cửa sổ trượt có ppl thấp hơn nhiều so với cửa sổ không chồng lấp.

--- TRANG 2 ---
import torch

def draw(array, n=50):
    epsilon = torch.flip(torch.linspace(0, 1, n), dims=[0])
    index = torch.zeros(n)
    cusum = torch.sum(array)
    m = len(array)
    s = 0
    i = 0
    for j in range(m):
        eps = epsilon[i]
        while s >= cusum *(1 - eps) and i < n:
            index[i] = j
            if i < n - 1:
                i += 1
            else:
                break
            eps = epsilon[i]
        s += array[j]
    while i < n:
        index[i] = m
        i += 1

    return index / m, epsilon

Khám phá Ngoại suy Transformer

Trong phần này, trước tiên chúng tôi mô tả giả thuyết về lý do tại sao các phương pháp ngoại suy độ dài dựa trên RPE hiện có có thể ngoại suy các chuỗi trong suy luận và cung cấp bằng chứng thực nghiệm cho nó. Sau đó, chúng tôi rút ra chi tiết các điều kiện cho ngoại suy độ dài và chứng minh rằng các phương pháp ngoại suy độ dài dựa trên RPE gần đây (Chi et al. 2022; Press, Smith, and Lewis 2022) thỏa mãn các điều kiện.

Giả thuyết
Một attention cửa sổ trượt với kích thước cửa sổ w tương đương với RPE sau đây trên attention đầy đủ:
mᵢⱼ = {0, i−j≤w. 
       −∞,khác. (8)

Bằng cách so sánh Phương trình 8 và RPE tương ứng của Alibi (Press, Smith, and Lewis 2022) trong Hình 2, chúng ta có thể thấy rằng cả hai đều có cùng hành vi ở chỗ cả hai đều tập trung các token bên trong một phạm vi được chỉ định. Ngoài ra, trong Hình 1, chúng tôi chỉ ra rằng hiệu suất của Alibi tương tự như attention cửa sổ trượt khi kích thước cửa sổ đủ lớn. Dựa trên hai quan sát này, chúng tôi đưa ra giả thuyết sau:

Giả thuyết 0.1. Một RPE làm cho transformer có thể ngoại suy cần có hành vi tương tự như attention cửa sổ trượt, tức là δ(i, j) phải thỏa mãn:
∀ϵ >0,∃j₀, sao cho, j > j₀, δ(i, j)< ϵ, (9)
trong đó δ(i, j)≜∥oᵢᵢ−oʲᵢ∥, và độ dài cửa sổ j cần đủ lớn.

Trong các phần sau, chúng tôi sẽ rút ra các điều kiện cho RPE thỏa mãn Phương trình 9.

Các điều kiện
Hãy giới thiệu bổ đề đầu tiên:

Bổ đề 0.2. Khi điều kiện sau được thỏa mãn, Phương trình 9 được thực hiện.
limᵢ→∞Cᵢᵢ≜C <∞. (10)

Chứng minh. Khi i≤m, độ dài chuỗi kiểm tra nhỏ hơn độ dài chuỗi tối đa m trong quá trình huấn luyện, lấy j=i, chúng ta có ∥oᵢᵢ−oʲᵢ∥=∥oᵢᵢ−oᵢᵢ∥= 0. Khi i > m, chúng ta có thể viết lại Phương trình 7 như:

oᵢᵢ=∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢcᵢₛvₛ+∑₁≤ₛ≤ᵢ₋ⱼcᵢₛvₛ / Cᵢᵢ
=∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢcᵢₛvₛ / CᵢⱼCᵢⱼ/Cᵢᵢ+∑₁≤ₛ≤ᵢ₋ⱼcᵢₛvₛ / Cᵢᵢ−CᵢⱼCᵢᵢ−Cᵢⱼ/Cᵢᵢ
=∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢcᵢₛvₛ / CᵢⱼCᵢⱼ/Cᵢᵢ+∑₁≤ₛ≤ᵢ₋ⱼcᵢₛvₛ / Cᵢᵢ−Cᵢⱼ1−Cᵢⱼ/Cᵢᵢ.

Do đó chúng ta có:
oᵢᵢ−oʲᵢ=1−Cᵢⱼ/Cᵢᵢ(∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢcᵢₛvₛ/Cᵢⱼ−∑₁≤ₛ≤ᵢ₋ⱼcᵢₛvₛ/Cᵢᵢ−Cᵢⱼ). (11)

Đối với phần thứ hai:
∥∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢcᵢₛvₛ/Cᵢⱼ−∑₁≤ₛ≤ᵢ₋ⱼcᵢₛvₛ/Cᵢᵢ−Cᵢⱼ∥
≤∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢcᵢₛ∥vₛ∥/Cᵢⱼ+∑₁≤ₛ≤ᵢ₋ⱼcᵢₛ∥vₛ∥/Cᵢᵢ−Cᵢⱼ
≤∑ᵢ₋ⱼ₊₁≤ₛ≤ᵢcᵢₛl/Cᵢⱼ+∑₁≤ₛ≤ᵢ₋ⱼcᵢₛl/Cᵢᵢ−Cᵢⱼ= 2l (12)

Chúng ta có
δ(i, j)≤2(1−Cᵢⱼ/Cᵢᵢ)l. (13)

Theo Phương trình 10 và đuôi của chuỗi hội tụ là tùy ý nhỏ. ∀C/2> ϵ > 0, chúng ta có thể tìm một j₀, sao cho nếu i≥j > j₀,Cᵢᵢ−Cᵢⱼ< ϵ. Chúng ta cũng có thể tìm một j₁, sao cho nếu i≥j > j₁,C−ϵ < Cᵢᵢ< C+ϵ. Nếu chúng ta lấy j₂= max(j₀, j₁), vậy nếu i≥j≥j₂, chúng ta có:
Cᵢᵢ−Cᵢⱼ< ϵ, C−ϵ < Cᵢᵢ< C+ϵ (14)

Vậy khi i≥j≥j₂, chúng ta có
δ(i, j)≤2(1−Cᵢⱼ/Cᵢᵢ)l= 2(Cᵢᵢ−Cᵢⱼ)/Cᵢᵢl≤2ϵ/(C−ϵ)l
≤2lϵ/(C−C/2)=4lϵ/C (15)

Theo định nghĩa của giới hạn, Phương trình 10 được thực hiện.

Bổ đề này ngụ ý rằng đối với bất kỳ token nào nếu attention của mô hình tập trung vào j (j≥j₂) token lân cận của nó, mô hình có tính chất ngoại suy độ dài. Bổ đề đi kèm với trực giác của chúng ta. Điều đó có nghĩa là miễn là một RPE tuân theo cùng nguyên tắc, tức là đặt trọng số nhiều hơn vào j token lân cận, mô hình được đảm bảo có tính chất ngoại suy độ dài không? Trong các phần sau, chúng tôi sẽ chứng minh rằng việc tập trung trọng số nhiều hơn vào các token lân cận không đảm bảo transformer có tính chất ngoại suy độ dài. Cụ thể, chúng tôi sẽ cung cấp một chứng minh toán học về các điều kiện đủ để RPE có tính chất ngoại suy độ dài.

Định lý 0.3. Khi điều kiện sau được thỏa mãn, Phương trình 9 được thực hiện.
limᵢ→∞Bᵢᵢ<∞, Bᵢᵢ=∑₁≤ₜ≤ᵢbᵢₜ<∞. (16)

Chứng minh. Vì chúng ta giả định ∥qᵢ∥ ≤l,∥kᵢ∥ ≤l, thì:
aᵢⱼ= exp( q⊤ᵢkⱼ)≤exp(l²), (17)
cᵢⱼ=aᵢⱼbᵢⱼ≤exp(l²)bᵢⱼ, Cᵢᵢ≤exp(l²)Bᵢᵢ. (18)

Do đó, Phương trình 10 có thể được rút ra từ Phương trình 16. Kết hợp với Bổ đề 0.2, chứng minh được kết thúc.

Bằng cách tận dụng tính chất của RPE, Định lý 0.3 có thể được đơn giản hóa thêm như:

Định lý 0.4. Khi điều kiện sau được thỏa mãn, Phương trình 9 được thực hiện.
limᵢ→∞∑ᵢₜ₌₁bᵢ₋ₜ= limᵢ→∞∑ᵢ⁻¹ₜ₌₀bₜ<∞. (19)

Chứng minh. Theo định nghĩa của RPE:
Bᵢᵢ=∑₁≤ₜ≤ᵢbᵢₜ=∑ᵢₜ₌₁bᵢ₋ₜ=∑ᵢ⁻¹ₜ₌₀bₜ. (20)

Điều này có nghĩa là Phương trình 16 tương đương với:
limᵢ→∞Bᵢᵢ= limᵢ→∞∑ᵢ⁻¹ₜ₌₀bₜ<∞. (21)

Định lý 0.4 chỉ ra rằng miễn là chuỗi của exp( RPE) hội tụ, mô hình được đảm bảo có tính chất ngoại suy độ dài. Dựa trên nguyên tắc này, chúng ta có thể xác định về mặt toán học liệu một RPE có cho phép ngoại suy độ dài trước khi tiến hành thí nghiệm hay thiết kế nhiều RPE khác nhau có thể thực hiện ngoại suy độ dài. Trong Phụ lục, chúng tôi chỉ ra rằng các phương pháp trước đây như Alibi (Press, Smith, and Lewis 2022), Kerple (Chi et al. 2022), và Sandwich (Chi, Fan, and Rudnicky 2022) thỏa mãn các điều kiện chúng tôi rút ra cho ngoại suy độ dài.

Trường tiếp nhận lý thuyết
Trong phần trước, chúng tôi đã thiết lập các điều kiện cho ngoại suy độ dài. Như một phần thưởng bổ sung, chúng ta có thể rút ra Trường Tiếp nhận Lý thuyết (TRF) cho bất kỳ phương pháp ngoại suy độ dài dựa trên RPE nào. Hãy bắt đầu với định nghĩa về Trường Tiếp nhận Thực nghiệm (ERF). ERF có thể được xem như một cửa sổ chứa phần lớn thông tin được chứa trong attention.

Nhớ lại Phương trình 13, bằng cách đặt (1−Cᵢⱼ)/Cᵢᵢ=ϵ, chúng ta có thể định nghĩa:
Cᵢⱼ=Cᵢᵢ(1−ϵ), nₑₘₚ(ϵ) = inf_j(Cᵢⱼ> Cᵢᵢ(1−ϵ)),

nₑₘₚ(ϵ) là ERF đại diện cho độ dài chuỗi tối thiểu cần thiết để duy trì hiệu suất trong một khoảng cách ϵ. Theo trực giác, ERF có thể được xem như cửa sổ nhỏ nhất chứa phần lớn thông tin trong một attention. Vì nó liên quan đến cả aᵢⱼ và bᵢⱼ, nó chỉ có thể được tính toán sau khi huấn luyện.

Bây giờ chúng tôi định nghĩa TRF, cho phép chúng ta ước tính trường tiếp nhận mà không cần huấn luyện. Để thực hiện điều này, chúng ta xem xét cận trên của Cᵢⱼ. Từ định nghĩa của Cᵢⱼ và Phương trình 17, Cᵢⱼ bị chặn trên bởi Bᵢⱼ. Do đó, chúng ta có thể định nghĩa TRF nᵇₜₕₑ(ϵ) đối với chuỗi bₜ như:

nₜₕₑ(ϵ) = inf_j(Bᵢⱼ> B(1−ϵ))
= inf_j(∑ʲ⁻¹ₜ₌₀bₜ> B(1−ϵ))
= inf_j(∑ₜ≥ⱼbₜ< Bϵ) (22)

trong đó B= limⱼ→∞∑ʲ⁻¹ₜ₌₀bₜ. Chúng ta có thể thấy khó khăn trong việc đưa ra dạng phân tích của tổng riêng phần của chuỗi đôi khi, nhưng chúng ta vẫn có thể tính toán TRF một cách số học hoặc so sánh TRF của các RPE khác nhau bằng cách sử dụng định lý dưới đây:

--- TRANG 3 ---
Định lý 0.5. Nếu các điều kiện sau được thỏa mãn:
αₜ/α≤βₜ/β, t→ ∞ , α≜limⱼ→∞∑ʲ⁻¹ₜ₌₀αₜ, β≜limⱼ→∞∑ʲ⁻¹ₜ₌₀βₜ. (23)

Thì:
nᵅₜₕₑ(ϵ)≤nᵝₜₕₑ(ϵ), ϵ→0. (24)

Chứng minh. Theo Phương trình 23, tồn tại t₀>0, sao cho, khi t > t₀, chúng ta có:
αₜ/α≤βₜ/β. (25)

Cho ϵ < ϵ₀, trong đó
nᵝₜₕₑ(ϵ₀) =t₀, (26)

thì chúng ta có:
∑ₜ≥nᵝₜₕₑ(ϵ)βₜ≤βϵ, nᵝₜₕₑ(ϵ)> t₀. (27)

Cuối cùng:
∑ₜ≥nᵝₜₕₑ(ϵ)αₜ≤∑ₜ≥nᵝₜₕₑ(ϵ)α(βₜ/β)≤α(βϵ/β)=αϵ.

Theo Phương trình 22, chúng ta có:
nᵅₜₕₑ(ϵ)≤nᵝₜₕₑ(ϵ). (28)

Chuỗi exp( RPE) tuân theo cùng xu hướng như TRF, chuỗi càng nhỏ, TRF càng nhỏ.

Chúng tôi cung cấp một số ví dụ về cách tính toán TRF trong Phụ lục.

Hai RPE mới
Dựa trên các điều kiện đã chứng minh của ngoại suy độ dài, chúng ta có thể thiết kế vô số loại RPE với tính chất ngoại suy độ dài. Ở đây, chúng tôi đề xuất hai RPE mới để chứng minh thực nghiệm các điều kiện và giả thuyết, cụ thể là:

Loại 1 : bₙ=1/n²= exp( −2 lnn),
Loại 2 : bₙ= exp( −ln²n);

TRF tương ứng của Loại 1 là:
Bᵢⱼ=∑ʲ⁻¹ᵢ₌₀1/(i+ 1)²≈∫ʲ₁1/x²dx= 1−1/j, B= 1.
nₜₕₑ(ϵ) = inf_j(Bᵢⱼ> B(1−ϵ))
= inf_j(1−1/j>1−ϵ)
= Θ(1/ϵ) (29)

Đối với Loại 2, khó cung cấp dạng phân tích của TRF của nó. Tuy nhiên, chúng ta có thể chứng minh rằng TRF của Loại 2 nhỏ hơn TRF của Loại 1 bằng cách sử dụng Định lý 0.5 và bất đẳng thức dưới đây:
∀c₁, c₂>0,exp(−ln²n)/c₁<1/n²/c₂, n→ ∞ .

Xác thực Thực nghiệm

Cài đặt Tất cả các mô hình được triển khai trong Fairseq (Ott et al. 2019) và được huấn luyện trên 8 GPU V100. Chúng tôi sử dụng cùng kiến trúc mô hình và cấu hình huấn luyện cho tất cả các biến thể RPE để đảm bảo công bằng. Đối với Wikitext-103 (Merity et al. 2016), vì nó là một tập dữ liệu tương đối nhỏ, chúng tôi sử dụng cấu trúc giải mã transformer 6 lớp với kích thước embedding là 512. Đối với các tập dữ liệu khác, đặc biệt, chúng tôi sử dụng cấu trúc giải mã transformer 12 lớp với kích thước embedding là 768. Thước đo đánh giá là độ phức tạp (PPL) và độ dài huấn luyện tối đa trong quá trình huấn luyện là 512. Các cài đặt siêu tham số chi tiết được liệt kê trong Phụ lục.

Tập dữ liệu Chúng tôi tiến hành thí nghiệm trên Wikitext-103 (Merity et al. 2016), Books (Zhu et al. 2015), Github (Gao et al. 2020) và WikiBook (Wettig et al. 2022). Wikitext-103 là một tập dữ liệu nhỏ chứa phiên bản được tiền xử lý của tập dữ liệu Wikipedia. Nó được sử dụng rộng rãi trong nhiều bài báo NLP. Books có số lượng lớn tiểu thuyết, làm cho nó trở thành một kho ngữ liệu tốt cho xử lý chuỗi dài. Github bao gồm một lượng đáng kể các kho mã nguồn mở, phần lớn được viết bằng các ngôn ngữ lập trình. WikiBook là một kho ngữ liệu 22 gigabyte các bài viết Wikipedia và sách được tuyển chọn bởi (Wettig et al. 2022). Kho ngữ liệu này được sử dụng để xác thực hiệu suất của các mô hình khác nhau trên các tập dữ liệu lớn.

Xác thực tính đầy đủ. Để xác thực thực nghiệm tính đầy đủ của các điều kiện chúng tôi phát hiện, chúng tôi tích hợp hai RPE được đề xuất trong phần trước vào transformer và kiểm tra khả năng ngoại suy độ dài của chúng trên các tập dữ liệu Wikitext-103, Books, Github và WikiBook. Chúng tôi tăng độ dài chuỗi suy luận từ 512 đến 9216 token và vẽ đồ thị PPL kiểm tra của các RPE chúng tôi đề xuất cũng như các phương pháp hiện có như Alibi, Kerple và Sandwich trong Hình 3. Kết quả số chi tiết có thể được tìm thấy trong Bảng 4 và Bảng 5 từ Phụ lục. Tất cả các phương pháp này đều thể hiện khả năng ngoại suy độ dài tốt. Tuy nhiên, PPL ổn định có thể khác nhau do hiệu quả của các chiến lược mã hóa vị trí khác nhau, điều này không được xem xét trong bài báo này. Chúng tôi bao gồm mã hóa vị trí Sinusoidal (Vaswani et al. 2017) như một phương pháp tham chiếu không thể ngoại suy, tăng nhanh khi độ dài chuỗi suy luận tăng.

Xác thực tính cần thiết. Mặc dù chúng tôi chỉ cung cấp chứng minh toán học cho tính đầy đủ của các điều kiện chúng tôi phát hiện, chúng tôi cũng cố gắng xác minh tính cần thiết của chúng một cách thực nghiệm trong phần này. Cụ thể, chúng tôi chọn hai RPE rất gần với việc thỏa mãn Định lý 0.4 như sau. Lưu ý rằng cả hai đều tập trung trọng số của chúng vào các token lân cận.

Ví dụ 1 : bₙ=1/n,Ví dụ 2 : bₙ=1/(nlnn)

Dưới đây là một chứng minh toán học ngắn gọn rằng các RPE trên không thỏa mãn Định lý 0.4.

∑ᵏₙ₌₁1/n>∫ᵏ⁺¹₁1/xdx= ln(k+ 1),
∑ᵏₙ₌₃1/(nlnn)>∫ᵏ⁺¹₃1/(xlnx)dx= ln ln(k+ 1)−ln ln 3 .

Sau đó, chúng tôi kiểm tra thực nghiệm khả năng ngoại suy độ dài của chúng trên các tập dữ liệu Wikitext-103, Books, Github và WikiBook bằng cách

--- TRANG 4 ---
tăng độ dài chuỗi suy luận từ 512 đến 9216 token. Như được hiển thị trong Hình 4, PPL của cả hai RPE tăng nhanh khi độ dài chuỗi kiểm tra tăng. Nó chứng minh rằng cả hai đều không thể ngoại suy. Chúng tôi cũng bao gồm Type 1 RPE trong Hình 4 như một tham chiếu có thể ngoại suy. Kết quả số chi tiết có thể được tìm thấy trong Bảng 6 từ Phụ lục.

Xác thực TRF Chúng tôi xác thực TRF đề xuất của mình bằng cách so sánh xu hướng giữa TRF và ERF. Chúng tôi vẽ đồ thị TRF và ERF của Alibi, Kerple, Sandwich và các RPE chúng tôi đề xuất trên các tập dữ liệu nêu trên. Như được quan sát trong Hình 6 và Hình 5, trong khi các đường cong khác nhau trên các tập dữ liệu, TRF ước tính một xu hướng tổng thể tương tự của ERF.

Trực quan hóa RPE Chúng tôi trực quan hóa các sơ đồ trọng số của Type 1 và Type 2 trong Hình 7, tức là bản đồ nhiệt của exp( RPE). Type 2 tập trung trọng số vào các token lân cận gần hơn so với Type 1, cho thấy TRF và ERF nhỏ hơn như được hiển thị trong Hình 6 và Hình 5. Chúng tôi cũng trực quan hóa các phương pháp khác trong Phụ lục.

Kết luận
Trong bài báo này, chúng tôi khám phá bí mật của ngoại suy độ dài transformer trong mô hình hóa ngôn ngữ. Trước tiên, chúng tôi đưa ra một giả thuyết về ngoại suy và sau đó rút ra các điều kiện đầy đủ để RPE có tính chất ngoại suy độ dài. Một phân tích toán học kỹ lưỡng tiết lộ rằng một mô hình transformer chắc chắn có khả năng ngoại suy độ dài nếu chuỗi tương ứng với hàm mũ của RPE của nó hội tụ. Quan sát này mang lại một phần thưởng bổ sung: chúng ta có thể ước tính TRF của RPE chỉ dựa trên công thức của chúng. Chúng tôi chọn hai RPE mới thỏa mãn các điều kiện và hai RPE không thỏa mãn để chứng minh thực nghiệm tính đầy đủ của các điều kiện trên bốn tập dữ liệu được sử dụng rộng rãi. Chúng tôi cũng xác thực TRF của mình bằng cách so sánh chúng với ERF trên các tập dữ liệu này. Kết quả cho thấy rằng TRF của chúng tôi có thể phản ánh chính xác các trường tiếp nhận thực tế của RPE trước khi huấn luyện.

--- TRANG 5 ---
Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Chương trình R&D Trọng điểm Quốc gia của Trung Quốc (SỐ.2022ZD0160100).

Tài liệu tham khảo
Akbari, H.; Yuan, L.; Qian, R.; Chuang, W.-H.; Chang, S.-F.; Cui, Y .; and Gong, B. 2021. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. In arXiv preprint arXiv:2104.11178 .

Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The Long-Document Transformer. In arXiv:2004.05150 .

Chi, T.-C.; Fan, T.-H.; Ramadge, P. J.; and Rudnicky, A. I. 2022. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. ArXiv , abs/2205.09921.

Chi, T.-C.; Fan, T.-H.; and Rudnicky, A. I. 2022. Receptive Field Alignment Enables Transformer Length Extrapolation. ArXiv , abs/2212.10356.

Cho, K.; van Merri ¨enboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , 1724–1734. Doha, Qatar: Association for Computational Linguistics.

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , 4171–4186. Minneapolis, Minnesota: Association for Computational Linguistics.

Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 .

Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.; Presser, S.; and Leahy, C. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. In arXiv preprint arXiv:2101.00027 .

Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin, Y . N. 2017. Convolutional sequence to sequence learning. In International Conference on Machine Learning , 1243–1252. PMLR.

Gong, Y .; Chung, Y .-A.; and Glass, J. 2021. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021 , 571–575.

Gulati, A.; Chiu, C.-C.; Qin, J.; Yu, J.; Parmar, N.; Pang, R.; Wang, S.; Han, W.; Wu, Y .; Zhang, Y .; and Zhang, Z., eds. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition .

Hao, D.; Mao, Y .; He, B.; Han, X.; Dai, Y .; and Zhong, Y . 2024. Improving Audio-Visual Segmentation with Bidirectional Generation. In Proceedings of the AAAI Conference on Artificial Intelligence .

Hershey, S.; Chaudhuri, S.; Ellis, D. P. W.; Gemmeke, J. F.; Jansen, A.; Moore, R. C.; Plakal, M.; Platt, D.; Saurous, R. A.; Seybold, B.; Slaney, M.; Weiss, R. J.; and Wilson, K. W. 2016. CNN architectures for large-scale audio classification. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 131–135.

Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term memory. Neural computation , 9(8): 1735–1780.

Kim, Y . 2014. Convolutional Neural Networks for Sentence Classification. In Conference on Empirical Methods in Natural Language Processing .

Knopp, K. 1956. Infinite sequences and series . Courier Corporation.

Liu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 .

Liu, Z.; Li, D.; Lu, K.; Qin, Z.; Sun, W.; Xu, J.; and Zhong, Y . 2022. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955 .

Liutkus, A.; C ´ıfka, O.; Wu, S.-L.; Simsekli, U.; Yang, Y .- H.; and Richard, G. 2021. Relative positional encoding for transformers with linear complexity. In International Conference on Machine Learning , 7067–7079. PMLR.

Lu, K.; Liu, Z.; Wang, J.; Sun, W.; Qin, Z.; Li, D.; Shen, X.; Deng, H.; Han, X.; Dai, Y .; and Zhong, Y . 2022. Linear video transformer with feature fixation. arXiv preprint arXiv:2210.08164 .

Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016. Pointer Sentinel Mixture Models. In arXiv:1609.07843 .

Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grangier, D.; and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038 .

Press, O.; Smith, N.; and Lewis, M. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations .

Qin, Z.; Han, X.; Sun, W.; He, B.; Li, D.; Li, D.; Dai, Y .; Kong, L.; and Zhong, Y . 2023a. Toeplitz Neural Network for Sequence Modeling. In The Eleventh International Conference on Learning Representations .

Qin, Z.; Han, X.; Sun, W.; Li, D.; Kong, L.; Barnes, N.; and Zhong, Y . 2022a. The Devil in Linear Transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , 7025–7041. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics.

Qin, Z.; Li, D.; Sun, W.; Sun, W.; Shen, X.; Han, X.; Wei, Y .; Lv, B.; Yuan, F.; Luo, X.; Qiao, Y .; and Zhong, Y . 2023b. Scaling TransNormer to 175 Billion Parameters. In arXiv preprint 2307.14995 .

Qin, Z.; Sun, W.; Deng, H.; Li, D.; Wei, Y .; Lv, B.; Yan, J.; Kong, L.; and Zhong, Y . 2022b. cosFormer: Rethinking Softmax In Attention. In International Conference on Learning Representations .

--- TRANG 6 ---
Qin, Z.; Sun, W.; Lu, K.; Deng, H.; Li, D.; Han, X.; Dai, Y .; Kong, L.; and Zhong, Y . 2023c. Linearized Relative Positional Encoding. arXiv preprint arXiv:2307.09270 .

Qin, Z.; Yang, S.; and Zhong, Y . 2023. Hierarchically gated recurrent neural network for sequence modeling. NeurIPS .

Qin, Z.; and Zhong, Y . 2023. Accelerating Toeplitz Neural Network with Constant-time Inference Complexity. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics.

Su, J.; Lu, Y .; Pan, S.; Wen, B.; and Liu, Y . 2021. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 .

Sun, J.; Zhong, G.; Zhou, D.; Li, B.; and Zhong, Y . 2022a. Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition. arXiv preprint arXiv:2203.15609 .

Sun, W.; Qin, Z.; Deng, H.; Wang, J.; Zhang, Y .; Zhang, K.; Barnes, N.; Birchfield, S.; Kong, L.; and Zhong, Y . 2022b. Vicinity Vision Transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence , (01): 1–14.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems , 30.

Wettig, A.; Gao, T.; Zhong, Z.; and Chen, D. 2022. Should You Mask 15% in Masked Language Modeling? In arXiv:2202.08005 .

Zhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In The IEEE International Conference on Computer Vision (ICCV) .

--- TRANG 7 ---
Tài liệu Bổ sung

Ký hiệu toán học
Ký hiệu | Ý nghĩa
X | Trạng thái ẩn.
Q,K,V | Query, key, value.
O | Đầu ra attention.
d | Chiều đặc trưng.
m⊤ₛ | Hàng thứ s của ma trận M.

Bảng 2: Ký hiệu toán học được sử dụng trong bài báo.

Ví dụ
Trong phần này, chúng tôi sử dụng Alibi (Press, Smith, and Lewis 2022), Kerple (Chi et al. 2022), và Sandwich (Chi, Fan, and Rudnicky 2022) làm ví dụ để hỗ trợ các điều kiện đầy đủ đã khám phá 16 cho ngoại suy độ dài.

Alibi Dạng của Alibi có thể được viết như:
bₜ= exp( −kt), k > 0. (30)

Theo sự hội tụ của chuỗi hình học:
limᵢ→∞Bᵢᵢ=∑ᵢ⁻¹ₜ₌₀exp(−kt)<∞, (31)

điều này thỏa mãn các điều kiện quan sát của chúng tôi.

Kerple Kerple đề xuất hai dạng RPE: log và power . Chúng tôi thảo luận chúng riêng biệt.

Công thức của biến thể Log có thể được biểu diễn như:
bₜ= exp( −r log (1 + kt)) =1/(1 +kt)^r (32)

trong đó r,k>0. Trong Kerple, r,k có thể học được. Dựa trên Định lý 0.4, để cho phép mô hình ngoại suy, chúng ta phải thêm ràng buộc rằng r>1 vì:
1/(1 +kt)^r∼1/(k^r t^r), (33)

Trong phân tích thực nghiệm, chúng tôi sẽ chỉ ra rằng khi r=1, mô hình không thể ngoại suy. Chúng tôi cũng kiểm tra mô hình đã được huấn luyện từ Kerple và thấy rằng điều kiện này được đáp ứng.

Biến thể Power có thể được viết như:
bₜ= exp( −kt^r),0< r≤2. (34)

Vì
bₜ≤exp(−kt), t→ ∞ . (35)

theo sự hội tụ của chuỗi hình học, chúng ta có:
limᵢ→∞Bᵢᵢ= limᵢ→∞∑ᵢ⁻¹ₜ₌₀exp(−kt^r)<∞. (36)

điều này thỏa mãn các điều kiện quan sát của chúng tôi.

Sandwich Cho công thức của Sandwich:
bₜ= exp[k(∑^(d/2)ⱼ₌₁cos(t/r^(2j/d))−d/2)], k > 0, r > 0.

Trước tiên, chúng tôi thực hiện các phép biến đổi sau:
bₜ= exp[k(∑^(d/2)ⱼ₌₁(cos(t/r^(2j/d))−1))]
=∏^(d/2)ⱼ₌₁exp[k(cos(t/r^(2j/d))−1)], (37)

--- TRANG 8 ---
sau đó tạo một phân vùng trên j:
t/r^(2j/d)≥π/2, (38)

điều này tương đương với:
2t≥πr^(2j/d), 2t/π≥r^(2j/d), log_r(2t/π)≥2j/d, j ≤(d log_r(2t/π))/2≜f(t). (39)

Do đó chúng ta có:
bₜ=∏_{1≤j≤f(t)}exp[k(cos(t/r^(2j/d))−1)]×∏_{f(t)<j≤d/2}exp[k(cos(t/r^(2j/d))−1)]. (40)

Đối với phần đầu tiên:
cos(t/r^(2j/d))−1<−1. (41)

Đối với phần thứ hai:
cos(t/r^(2j/d))−1<0. (42)

Thì:
βₜ≤∏_{1≤j≤f(t)}exp(−k)
= exp( −k⌊f(t)⌋)
≤exp(k) exp(−kf(t))
= exp( k) exp(−k(d log_r(2t/π))/2)
≜g(t). (43)

Theo kiểm tra Rabbe (Knopp 1956):
t[g(t)/g(t+ 1)−1]
=t[exp(kd/2 log_r((2t+ 2)/(2t)))−1]
=t[exp(kd/2 log_r(1 +1/t))−1]
∼t[kd/2 ln r(1 +1/t)+O(1/t²)]
∼t[kd/2 ln r(1/t−1/(2t²))+O(1/t²)]
→kd/2 ln r, (44)

nếu:
kd/2 ln r<1, d <2/(k ln r), (45)

thì chuỗi hội tụ¹.

Ví dụ TRF
Chúng tôi sử dụng Alibi làm ví dụ để hiển thị việc tính toán TRF. Bᵢⱼ của Alibi có thể được viết như:
Bᵢⱼ=∑ʲ⁻¹ᵢ₌₀exp(−i) =(1−exp(−j))/(1−exp(−1)), B=1/(1−exp(−1)) (46)

TRF của Alibi có thể được tính như:
nₜₕₑ(ϵ) = inf_j(Bᵢⱼ> B(1−ϵ))
= inf_j(1−exp(−j)>1−ϵ) = Θ( −log ϵ). (47)

trong đó Θ đại diện cho cận tiệm cận trên và dưới.

¹Lưu ý rằng ở đây chúng tôi chỉ chỉ ra rằng chuỗi tương ứng với Sandwich hội tụ dưới các điều kiện nhất định. Cận trên ở đây tương đối lỏng lẻo, và các điều kiện được sử dụng trong thực tế rộng hơn.

--- TRANG 9 ---
Cấu hình

Dữ liệu | WikiText-103 | Khác
Lớp giải mã | 6 | 12
Chiều ẩn | 512 | 768
Số đầu | 8 | 12
Chiều FFN | 2048 | 3072
Phương pháp tokenizer | BPE | BPE
Kích thước từ vựng nguồn | 50265 | 50265
Độ dài chuỗi | 512 | 512
Tổng kích thước batch | 128 | 128
Số cập nhật/epochs | 50k cập nhật | 50k cập nhật
Bước khởi động | 4k bước | 4k bước
Tốc độ học tối đa | 5e-4 | 5e-4
Bộ lập lịch tốc độ học | Inverse sqrt | Inverse sqrt
Optimizer | Adam | Adam
Adam ϵ | 1e-8 | 1e-8
Adam (β₁, β₂) | (0.9, 0.98) | (0.9, 0.98)
Phân rã trọng số | 0.01 | 0.01

Bảng 3: Cấu hình huấn luyện chi tiết được sử dụng trong các thí nghiệm của chúng tôi. "Tổng kích thước batch" có nghĩa là batch per gpu×update freq×num gpus .

Mã giả cho trực quan hóa TRF và ERF
```python
import torch

def draw(array, n=50):
    epsilon = torch.flip(torch.linspace(0, 1, n), dims=[0])
    index = torch.zeros(n)
    cusum = torch.sum(array)
    m = len(array)
    s = 0
    i = 0
    for j in range(m):
        eps = epsilon[i]
        while s >= cusum *(1 - eps) and i < n:
            index[i] = j
            if i < n - 1:
                i += 1
            else:
                break
            eps = epsilon[i]
        s += array[j]
    while i < n:
        index[i] = m
        i += 1

    return index / m, epsilon
```

Kết quả Thí nghiệm Chi tiết

Bản đồ nhiệt

--- TRANG 10 ---
Wikitext-103
Độ dài | Sinusoidal | Alibi | Kerple-Log | Kerple-Power | Sandwich | Type1 | Type2
512 | 24.73 | 24.22 | 24.12 | 24.18 | 24.76 | 24.25 | 24.29
768 | 41.08 | 23.45 | 23.36 | 23.42 | 24.04 | 23.43 | 23.51
1024 | 62.71 | 23.06 | 22.93 | 22.98 | 23.63 | 23.03 | 23.09
1280 | 83.81 | 22.83 | 22.67 | 22.73 | 23.39 | 22.77 | 22.83
1536 | 102.28 | 22.66 | 22.45 | 22.54 | 23.21 | 22.55 | 22.63
1792 | 121.98 | 22.60 | 22.41 | 22.47 | 23.18 | 22.50 | 22.59
2048 | 138.17 | 22.52 | 22.28 | 22.37 | 23.08 | 22.38 | 22.48
3072 | 194.43 | 22.33 | 22.02 | 22.14 | 22.91 | 22.15 | 22.27
4096 | 259.55 | 22.26 | 21.97 | 22.08 | 22.96 | 22.09 | 22.21
5120 | 289.79 | 22.20 | 21.86 | 22.00 | 22.93 | 21.99 | 22.14
6144 | 337.46 | 22.17 | 21.87 | 21.96 | 23.06 | 21.97 | 22.11
7168 | 376.41 | 22.16 | 21.84 | 21.95 | 23.13 | 21.96 | 22.10
8192 | 406.95 | 22.14 | 21.82 | 21.94 | 23.20 | 21.95 | 22.08
9216 | 423.92 | 22.12 | 21.80 | 21.90 | 23.26 | 21.90 | 22.06

Books
Độ dài | Sinusoidal | Alibi | Kerple-Log | Kerple-Power | Sandwich | Type1 | Type2
512 | 7.49 | 7.28 | 7.34 | 7.31 | 7.64 | 7.30 | 7.35
768 | 10.43 | 7.15 | 7.21 | 7.18 | 7.55 | 7.18 | 7.22
1024 | 13.32 | 7.09 | 7.15 | 7.11 | 7.49 | 7.11 | 7.15
1280 | 15.53 | 7.06 | 7.11 | 7.08 | 7.47 | 7.08 | 7.12
1536 | 17.47 | 7.04 | 7.08 | 7.05 | 7.44 | 7.05 | 7.09
1792 | 19.02 | 7.03 | 7.06 | 7.03 | 7.42 | 7.03 | 7.06
2048 | 20.55 | 7.02 | 7.05 | 7.02 | 7.41 | 7.03 | 7.05
3072 | 24.70 | 7.00 | 7.02 | 7.00 | 7.38 | 7.00 | 7.03
4096 | 27.57 | 6.99 | 7.00 | 6.99 | 7.37 | 6.99 | 7.03
5120 | 29.54 | 6.99 | 7.00 | 6.99 | 7.36 | 6.99 | 7.03
6144 | 31.59 | 6.99 | 7.00 | 6.98 | 7.35 | 6.99 | 7.02
7168 | 32.41 | 6.98 | 7.00 | 6.98 | 7.35 | 6.99 | 7.02
8192 | 34.35 | 6.98 | 7.00 | 6.98 | 7.35 | 6.99 | 7.02
9216 | 34.70 | 6.98 | 7.01 | 6.98 | 7.35 | 6.99 | 7.02

Bảng 4: Xác thực tính đầy đủ trên các tập dữ liệu Wikitext-103 và Books. Để kiểm tra khả năng ngoại suy độ dài, chúng tôi kéo dài chuỗi suy luận từ 512 đến 9216 và tính toán PPL của các RPE Type 1 và Type 2 chúng tôi đề xuất, cũng như Alibi, Kerple và Sandwich. Tất cả các phương pháp này đều ổn định về PPL. Đối với các phương pháp không thể ngoại suy, ví dụ, Sinusoidal, PPL của nó tăng nhanh.

--- TRANG 11 ---
Github
Độ dài | Sinusoidal | Alibi | Kerple-Log | Kerple-Power | Sandwich | Type1 | Type2
512 | 2.29 | 2.25 | 2.24 | 2.25 | 2.29 | 2.25 | 2.25
768 | 3.98 | 2.16 | 2.16 | 2.16 | 2.20 | 2.16 | 2.16
1024 | 7.91 | 2.12 | 2.12 | 2.12 | 2.16 | 2.12 | 2.12
1280 | 12.97 | 2.10 | 2.09 | 2.09 | 2.14 | 2.09 | 2.09
1536 | 18.66 | 2.09 | 2.07 | 2.08 | 2.12 | 2.08 | 2.08
1792 | 24.08 | 2.08 | 2.06 | 2.07 | 2.11 | 2.07 | 2.07
2048 | 30.02 | 2.07 | 2.05 | 2.07 | 2.10 | 2.06 | 2.06
3072 | 51.64 | 2.06 | 2.03 | 2.05 | 2.08 | 2.04 | 2.04
4096 | 70.62 | 2.05 | 2.02 | 2.04 | 2.07 | 2.03 | 2.03
5120 | 89.78 | 2.05 | 2.02 | 2.04 | 2.07 | 2.03 | 2.03
6144 | 101.28 | 2.05 | 2.01 | 2.04 | 2.06 | 2.03 | 2.03
7168 | 117.21 | 2.05 | 2.01 | 2.04 | 2.06 | 2.03 | 2.03
8192 | 130.15 | 2.05 | 2.01 | 2.03 | 2.06 | 2.02 | 2.03
9216 | 143.17 | 2.05 | 2.01 | 2.03 | 2.05 | 2.02 | 2.03

Wikibook
Độ dài | Sinusoidal | Alibi | Kerple-Log | Kerple-Power | Sandwich | Type1 | Type2
512 | 17.98 | 17.64 | 17.65 | 17.62 | 18.21 | 17.68 | 17.70
768 | 29.66 | 17.04 | 17.03 | 17.03 | 17.63 | 17.07 | 17.08
1024 | 47.31 | 16.76 | 16.73 | 16.73 | 17.35 | 16.76 | 16.80
1280 | 65.13 | 16.62 | 16.54 | 16.56 | 17.16 | 16.58 | 16.61
1536 | 83.16 | 16.51 | 16.41 | 16.44 | 17.04 | 16.44 | 16.49
1792 | 100.46 | 16.49 | 16.33 | 16.41 | 16.95 | 16.39 | 16.44
2048 | 116.94 | 16.43 | 16.26 | 16.36 | 16.89 | 16.31 | 16.39
3072 | 172.09 | 16.39 | 16.13 | 16.31 | 16.75 | 16.23 | 16.33
4096 | 231.86 | 16.39 | 16.14 | 16.29 | 16.73 | 16.24 | 16.36
5120 | 277.59 | 16.35 | 16.10 | 16.26 | 16.68 | 16.21 | 16.32
6144 | 312.17 | 16.35 | 16.12 | 16.25 | 16.66 | 16.22 | 16.32
7168 | 349.08 | 16.34 | 16.19 | 16.24 | 16.71 | 16.26 | 16.33
8192 | 390.81 | 16.35 | 16.22 | 16.25 | 16.71 | 16.30 | 16.36
9216 | 412.06 | 16.33 | 16.23 | 16.24 | 16.70 | 16.28 | 16.33

Bảng 5: Xác thực tính đầy đủ trên các tập dữ liệu Github, WikiBook. Để kiểm tra khả năng ngoại suy độ dài, chúng tôi kéo dài chuỗi suy luận từ 512 đến 9216 và tính toán PPL của các RPE Type 1 và Type 2 chúng tôi đề xuất, cũng như Alibi, Kerple và Sandwich. Tất cả các phương pháp này đều ổn định về PPL. Đối với các phương pháp không thể ngoại suy, ví dụ, Sinusoidal, PPL của nó tăng nhanh.

Wikitext-103 | Books | Github | Wikibook
Độ dài | 1/n | 1/(nlnn) | 1/n | 1/(nlnn) | 1/n | 1/(nlnn) | 1/n | 1/(nlnn)
512 | 24.67 | 24.64 | 7.40 | 7.35 | 2.28 | 2.27 | 17.91 | 17.90
768 | 23.87 | 23.81 | 7.28 | 7.24 | 2.19 | 2.18 | 17.28 | 17.28
1024 | 23.53 | 23.44 | 7.27 | 7.19 | 2.17 | 2.15 | 17.21 | 17.12
1280 | 23.50 | 23.25 | 7.41 | 7.20 | 2.19 | 2.15 | 17.70 | 17.28
1536 | 23.66 | 23.13 | 7.65 | 7.24 | 2.31 | 2.19 | 18.86 | 17.72
1792 | 24.20 | 23.22 | 7.97 | 7.30 | 2.52 | 2.28 | 20.74 | 18.65
2048 | 24.80 | 23.26 | 8.34 | 7.39 | 2.85 | 2.39 | 23.31 | 19.82
3072 | 28.31 | 23.65 | 9.97 | 7.82 | 5.05 | 3.15 | 38.46 | 27.94
4096 | 33.18 | 24.41 | 11.84 | 8.29 | 9.13 | 4.35 | 59.51 | 40.06
5120 | 37.65 | 25.01 | 14.15 | 8.80 | 15.79 | 5.93 | 84.60 | 55.12
6144 | 43.20 | 25.80 | 16.64 | 9.27 | 25.00 | 8.03 | 112.30 | 71.08
7168 | 48.07 | 26.39 | 18.99 | 9.73 | 36.01 | 10.26 | 140.34 | 89.79
8192 | 52.85 | 27.00 | 22.06 | 10.06 | 49.20 | 12.80 | 173.55 | 108.85
9216 | 58.46 | 27.63 | 26.34 | 10.65 | 76.78 | 16.35 | 204.10 | 133.36

Bảng 6: Xác thực tính cần thiết trên các tập dữ liệu Wikitext-103, Books, Github, WikiBook. Chúng tôi chọn hai RPE không thỏa mãn Định lý 0.4, ví dụ, bₙ=1/n và bₙ=1/(nlnn). Chúng tôi tăng độ dài chuỗi suy luận từ 512 đến 9216 và tính toán PPL kiểm tra của chúng. PPL của chúng tăng nhanh khi độ dài chuỗi suy luận kéo dài.

--- TRANG 12 ---
Hình 8: Trực quan hóa RPE. Chúng tôi bổ sung vẽ bản đồ nhiệt của exp( RPE) cho tất cả các phương pháp. Lưu ý rằng tất cả các phương pháp đều tập trung gần đường chéo, bao gồm 1/n, 1/(nlnn), điều này cho thấy rằng sự tập trung không đủ để đảm bảo ngoại suy.
