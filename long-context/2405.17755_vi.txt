# XL3M: Một Framework Không Cần Huấn Luyện cho Việc Mở Rộng Độ Dài LLM Dựa trên Suy Luận Theo Phân Đoạn

Shengnan Wang∗
Theory Lab, 2012 Labs
Huawei Technologies Co., Ltd

Youhui Bai
Theory Lab, 2012 Labs
Huawei Technologies Co., Ltd

Lin Zhang
Theory Lab, 2012 Labs
Huawei Technologies Co., Ltd

Pingyi Zhou
Huawei Technologies Co., Ltd.

Shixiong Zhao
Theory Lab, 2012 Labs
Huawei Technologies Co., Ltd

Gong Zhang
Theory Lab, 2012 Labs
Huawei Technologies Co., Ltd

Sen Wang
Theory Lab, 2012 Labs
Huawei Technologies Co., Ltd

Renhai Chen
Theory Lab, 2012 Labs
Huawei Technologies Co., Ltd

Hua Xu
Huawei Technologies Co., Ltd

Hongwei Sun
Huawei Technologies Co., Ltd

## Tóm tắt

Vấn đề thất bại trong tổng quát hóa độ dài, cụ thể là mô hình ngôn ngữ lớn (LLM) không thể tổng quát hóa cho các văn bản dài hơn độ dài huấn luyện tối đa của nó, đã hạn chế nghiêm trọng việc ứng dụng LLM trong các tình huống có đầu vào dài liên tục. Để giải quyết vấn đề này, các phương pháp hiện tại hoặc yêu cầu chi phí đáng kể hoặc gây ra mất mát độ chính xác. Trong bài báo này, chúng tôi tìm thấy thực nghiệm rằng độ chính xác của dự đoán LLM có mối tương quan cao với độ chắc chắn của nó. Dựa trên điều này, chúng tôi đề xuất một framework hiệu quả không cần huấn luyện, tên là XL3M (có nghĩa là mô hình ngôn ngữ lớn cực dài), cho phép các LLM được huấn luyện trên các chuỗi ngắn có thể suy luận các chuỗi cực dài mà không cần bất kỳ huấn luyện hoặc tinh chỉnh thêm nào. Dưới framework XL3M, ngữ cảnh đầu vào sẽ được phân tách thành nhiều ngữ cảnh con ngắn, trong đó mỗi ngữ cảnh con chứa một phân đoạn độc lập và một "câu hỏi" chung là một vài token từ cuối ngữ cảnh gốc. Sau đó XL3M đưa ra một phương pháp để đo lường mức độ liên quan giữa mỗi phân đoạn và "câu hỏi", và xây dựng một ngữ cảnh chính ngắn gọn bằng cách nối tất cả các phân đoạn liên quan theo thứ tự thời gian. Ngữ cảnh chính được sử dụng thay thế cho ngữ cảnh gốc để hoàn thành tác vụ suy luận. Các đánh giá trên các benchmark toàn diện cho thấy tính ưu việt của XL3M. Sử dụng framework của chúng tôi, một mô hình Llama2-7B có thể suy luận các chuỗi dài 20M trên một máy NPU Huawei Ascend 910B 8 card với bộ nhớ 64GB mỗi card.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) dựa trên Transformer Vaswani et al. (2017); Brown et al. (2020); Touvron et al. (2023); Scao et al. (2022) đã cho thấy hiệu suất ấn tượng trong nhiều tác vụ ngôn ngữ Zhao et al. (2023). Tuy nhiên, do các vấn đề ngoài miền và mất tập trung Xiao et al. (2024), chất lượng sinh của LLM giảm đáng kể khi độ dài chuỗi vượt quá kích thước cửa sổ ngữ cảnh của nó, đây là độ dài huấn luyện lớn nhất. Nhược điểm này cản trở việc ứng dụng LLM trong đối thoại nhiều vòng, tiến hành cuộc trò chuyện, tóm tắt tài liệu, và các tác vụ thực tế khác thường gặp phải các chuỗi rất dài.

Một số công trình tiên phong đã được thực hiện cho việc ngoại suy độ dài ngữ cảnh. Hầu hết trong số đó tập trung vào việc tối ưu hóa mã hóa vị trí (PE), vì PE của độ dài chưa thấy được xác định là yếu tố chính dẫn đến thất bại tổng quát hóa độ dài. So với PE tuyệt đối thông thường, PE tương đối được đề xuất sau đó Raffel et al. (2020); Su et al. (2021), ALiBi Press et al. (2021), và NoPE Kazemnejad et al. (2023) được chứng minh là cung cấp khả năng tổng quát hóa tốt hơn. Tuy nhiên, tất cả chúng đều không hoạt động tốt khi độ dài chuỗi dài hơn đáng kể so với độ dài huấn luyện lớn nhất. Một cách tiếp cận hiệu quả hơn là tiếp tục huấn luyện hoặc tinh chỉnh mô hình trên dữ liệu độ dài dài hơn Chen et al. (2023); Peng et al. (2023). Tuy nhiên, cách thức như vậy chỉ có thể mở rộng cửa sổ ngữ cảnh đến một độ dài hạn chế do chi phí huấn luyện không thể chấp nhận được Xiong et al. (2023). Hơn nữa, khi độ dài rất dài, ngay cả việc thu thập dữ liệu huấn luyện cũng là một nhiệm vụ khó khăn.

Gần đây, một số phương pháp mở rộng độ dài không cần huấn luyện đã thu hút sự chú ý rộng rãi. LM-Infinite Han et al. (2023) và StreamLLM Xiao et al. (2023) ngoại suy độ dài bằng cách loại bỏ hầu hết ngữ cảnh nhưng chỉ giữ ngữ cảnh ở cuối và ở đầu. Mặc dù các phương pháp này có thể xử lý hiệu quả các ngữ cảnh cực dài, chúng mất nhiều phụ thuộc khoảng cách xa, dẫn đến sai lệch hoặc thậm chí lỗi trong hiểu văn bản. PCW Ratner et al. (2023) thiết kế mặt nạ attention theo khối và tái sử dụng mã hóa vị trí cho các khối khác nhau, giúp giảm bớt hạn chế của cửa sổ ngữ cảnh. Tuy nhiên, PCW chỉ có thể mở rộng cửa sổ ngữ cảnh đến một độ dài rất hạn chế, và hiệu quả của PCW cần được nghiên cứu thêm Ratner et al. (2023).

Trong công trình này, chúng tôi đề xuất một framework suy luận hiệu quả không cần huấn luyện có tên XL3M (có nghĩa là mô hình ngôn ngữ lớn cực dài) cho phép LLM phá vỡ giới hạn độ dài. Hiệu quả của framework XL3M là do một nguyên tắc quan trọng: độ dài của chuỗi được xử lý bởi LLM tại một thời điểm không nên vượt quá kích thước cửa sổ ngữ cảnh của nó.

Các đóng góp của bài báo này được tóm tắt như sau:

• Chúng tôi tìm thấy thực nghiệm rằng độ chính xác của dự đoán LLM có mối tương quan cao với độ chắc chắn của nó được đo bằng entropy. Dựa trên điều này, chúng tôi đề xuất XL3M, một framework suy luận mới cho phép bất kỳ LLM nào đọc và hiểu các chuỗi cực dài. Lấy cảm hứng từ thói quen đọc theo phân đoạn của con người, dưới framework của chúng tôi, mỗi ngữ cảnh dài đầu vào sẽ được phân tách thành nhiều ngữ cảnh con ngắn với một "câu hỏi" chung cần được trả lời. Đối với mỗi ngữ cảnh con, chúng tôi sử dụng LLM để tính toán phân phối xác suất có điều kiện cục bộ (cpd) cũng như entropy tương ứng của nó. Sau đó các ngữ cảnh con liên quan với giá trị entropy nhỏ được chọn và tổ chức lại thành ngữ cảnh chính theo thứ tự thời gian. Vì hầu hết ngữ cảnh không liên quan được loại bỏ, LLM có thể tạo ra kết quả chất lượng cao theo ngữ cảnh chính được trích xuất.

• Chúng tôi đánh giá framework được đề xuất trên các tác vụ LongBench toàn diện và tác vụ "Needle in a Haystack" được sử dụng rộng rãi. Chúng tôi so sánh hiệu suất của XL3M với các phương pháp tiên tiến, bao gồm cả phương pháp tinh chỉnh và không tinh chỉnh. Kết quả chứng minh tính ưu việt của framework được đề xuất.

• Framework XL3M được đề xuất không thay đổi cấu trúc chính của LLM, và nó không cần bất kỳ huấn luyện hoặc tinh chỉnh bổ sung nào. Nó hiệu quả cả về bộ nhớ và thời gian. Dưới framework XL3M, LLM có thể suy luận các chuỗi dài hơn 20M trên một máy NPU Huawei Ascend 910B 8 card với bộ nhớ 64GB mỗi card.

## 2 Công trình liên quan

Do nhu cầu mạnh mẽ về suy luận chuỗi dài, rất nhiều kỹ thuật mở rộng cửa sổ ngữ cảnh đã được đề xuất Naveed et al. (2023); Kaddour et al. (2023). Các phương pháp này có thể được chia chủ yếu thành ba loại: 1) Mở rộng bằng tinh chỉnh; 2) Mở rộng không cần tinh chỉnh; 3) Mở rộng bằng bộ nhớ ngoại Bertsch et al. (2024); Wu et al. (2022); Xiao et al. (2024).

### 2.1 Mở rộng bằng tinh chỉnh

Các LLM thường được huấn luyện trên các chuỗi tương đối ngắn, do yêu cầu tính toán và bộ nhớ đắt đỏ (tứ phương với độ dài chuỗi) trong cơ chế attention, và LLM thất bại trong việc tổng quát hóa đến độ dài chưa thấy ở giai đoạn suy luận Han et al. (2023); Chen et al. (2023). Một ý tưởng đơn giản cho tổng quát hóa độ dài là tinh chỉnh mô hình trên các chuỗi dài hơn. Tuy nhiên, người ta phát hiện rằng việc tinh chỉnh một cách ngây thơ một LLM đã được huấn luyện trước cho ngoại suy cửa sổ là kém hiệu quả và không hiệu quả Kaddour et al. (2023); Anil et al. (2022). Chen et al. (2023) cho thấy rằng việc sử dụng nội suy vị trí (PI) thay vì ngoại suy trong quá trình tinh chỉnh có thể mở rộng cửa sổ ngữ cảnh của LLM đã được huấn luyện trước lên 32k mà không mất hiệu suất. Hơn nữa, Yarn Peng et al. (2023) đề xuất một phương pháp nội suy nhận thức NTK mới và đạt được sự mở rộng hàng chục lần kích thước cửa sổ ngữ cảnh. Các phương pháp dựa trên tinh chỉnh khác bao gồm Giraffe Pal et al. (2023), FoT Tworkowski et al. (2023), và v.v.

Tuy nhiên, loại phương pháp này yêu cầu tài nguyên huấn luyện lớn vì nó cần huấn luyện LLM trên dữ liệu chuỗi dài. Ngoài ra, việc thu thập đủ dữ liệu chuỗi dài cho tinh chỉnh cũng là một công việc thách thức nếu người ta muốn mở rộng cửa sổ ngữ cảnh đến cực dài.

### 2.2 Mở rộng không cần tinh chỉnh

Để tiết kiệm tài nguyên, một số phương pháp mở rộng cửa sổ ngữ cảnh không cần huấn luyện được đề xuất. Ngay từ đầu, hầu hết các nhà nghiên cứu tập trung vào việc tối ưu hóa mã hóa vị trí. Mã hóa vị trí tuyệt đối thông thường hạn chế nghiêm ngặt độ dài suy luận của LLM. Để giải quyết vấn đề này, nhiều sơ đồ mã hóa vị trí tiên tiến đã được đề xuất, như RoPE Su et al. (2021), ALiBi Press et al. (2021), và NoPE được đề xuất gần đây Kazemnejad et al. (2023). Tuy nhiên, tất cả chúng chỉ làm cho mô hình có khả năng kiến trúc để xử lý đầu vào dài thay vì thực sự hoạt động tốt trên các tác vụ suy luận chuỗi dài Li et al. (2023); Kaddour et al. (2023).

Thay vì mã hóa độ dài chưa thấy, StreamLLM Xiao et al. (2023) chọn loại bỏ hầu hết ngữ cảnh nhưng chỉ giữ các token gần đây (token ở cuối) và sink token (token ở đầu), đảm bảo rằng tổng độ dài của ngữ cảnh còn lại không vượt quá kích thước cửa sổ của LLM. Phương pháp như vậy không chỉ cho phép LLM xử lý ngữ cảnh dài hơn, mà còn đạt được tăng tốc đáng kể. Ý tưởng tương tự cũng được áp dụng bởi LM-Infinite Han et al. (2023). Tuy nhiên, cả StreamLLM và LM-Infinite đều bỏ lỡ nhiều phụ thuộc khoảng cách xa, dẫn đến sai lệch hoặc thậm chí lỗi trong hiểu văn bản.

Công trình liên quan nhất nên là cửa sổ ngữ cảnh song song (PCW) Ratner et al. (2023). Bằng cách sửa đổi nhúng vị trí và mặt nạ attention, PCW giảm bớt hạn chế cửa sổ ngữ cảnh cho bất kỳ LLM có sẵn nào mà không cần huấn luyện thêm. Tuy nhiên, được chỉ ra rằng PCW chỉ hiệu quả cho một sự mở rộng hạn chế (khoảng ba lần mở rộng kích thước cửa sổ ngữ cảnh gốc), và hiệu suất giảm khi mở rộng đến độ dài dài hơn nhiều. Hơn nữa, hiệu quả của PCW chỉ được chứng minh trên các tác vụ như phân loại đa lớp và trích xuất thông tin. Vẫn còn là một câu hỏi mở liệu nó có phù hợp cho các tác vụ tổng quát hơn hay không.

### 2.3 Mở rộng bằng bộ nhớ ngoại

Không giống như các cách tiếp cận trước đó giữ nguyên kiến trúc chính của mô hình, các phương pháp trong loại này thường liên quan đến việc sửa đổi mô hình. Nói chung, loại phương pháp này giới thiệu bộ nhớ ngoại để khôi phục thông tin của ngữ cảnh quá khứ, và truy xuất các token liên quan từ bộ nhớ để sinh dựa trên một số cơ chế tìm kiếm, như KNN Bertsch et al. (2024); Wu et al. (2022). Nhược điểm chính là các phương pháp này yêu cầu overhead bộ nhớ bổ sung, và chúng thường cần huấn luyện hoặc tinh chỉnh thêm để đảm bảo hiệu quả.

Xiao et al. (2024) và Munkhdalai et al. (2024) lần lượt đề xuất một cơ chế offload và một cơ chế nén để giảm áp lực bộ nhớ.

Trong bài báo này, chúng tôi nhằm đề xuất một framework suy luận hiệu quả không cần huấn luyện cho phép bất kỳ LLM nào suy luận chuỗi cực dài.

## 3 XL3M: mô hình ngôn ngữ lớn cực dài

Mô hình ngôn ngữ, bao gồm LLM, nghiên cứu một phân phối xác suất có điều kiện (cpd) p(xt+1|Xt) cho chuỗi văn bản Xt = [x1, ..., xt] Wei et al. (2023). Trong giai đoạn suy luận, cho một chuỗi đầu vào, LLM cũng đầu tiên tính toán cpd và sau đó sinh một token theo một chế độ sinh được xác định trước, như tìm kiếm tham lam, tìm kiếm top-k, tìm kiếm top-p, v.v. Vì độ dài của dữ liệu huấn luyện bị giới hạn trong kích thước cửa sổ ngữ cảnh C, LLM chỉ nghiên cứu cpd của các trường hợp khi t ≤ C trong quá trình huấn luyện, nên nó thất bại trong việc tạo ra một cpd hiệu quả ở giai đoạn suy luận khi độ dài của chuỗi đầu vào lớn hơn C. Nói cách khác, kích thước cửa sổ ngữ cảnh C có thể được xem như một giới hạn trên của khả năng LLM cho một quá trình xử lý đơn lẻ. Giới hạn như vậy cũng tồn tại trong hiểu đọc của con người. Con người cũng khó có thể hiểu một ngữ cảnh rất dài bằng cách đọc nó từ đầu đến cuối một lần. Thực tế, chúng ta con người hầu như không bao giờ xử lý ngữ cảnh dài theo cách một lần như vậy. Ngược lại, cho một bài báo dài và một câu hỏi cần được trả lời, chúng ta thường sử dụng phương pháp sau để có được câu trả lời:

**Phân đoạn ngữ cảnh và trích xuất thông tin chính** Phân đoạn ngữ cảnh dài trước và đọc nó từng phân đoạn với câu hỏi. Sau đó nhanh chóng xác định phân đoạn nào liên quan đến tác vụ hiện tại, và xây dựng một ngữ cảnh chính ngắn bằng cách tổ chức lại các phân đoạn liên quan. Cuối cùng, trả lời câu hỏi dựa trên ngữ cảnh chính ngắn.

Lấy cảm hứng từ cách tiếp cận của con người đối với việc đọc và hiểu văn bản dài được đề cập ở trên, chúng tôi đề xuất một framework suy luận mới XL3M, cho phép bất kỳ LLM nào suy luận chuỗi cực dài mà không cần bất kỳ huấn luyện tiếp tục hoặc tinh chỉnh nào. Framework XL3M tuân theo một nguyên tắc quan trọng: độ dài của chuỗi được xử lý bởi LLM tại một thời điểm không nên vượt quá kích thước cửa sổ ngữ cảnh của nó.

### 3.1 Ít không chắc chắn ngụ ý độ chính xác cao hơn

Nói chung, LLM được huấn luyện bằng cách tối ưu hóa mất mát cross-entropy, buộc cpd đầu ra của LLM p(xt+1|Xt) dần tiếp cận vector nhãn one-hot thực tế trong quá trình huấn luyện. Lưu ý rằng phân phối 0-1 one-hot có độ không chắc chắn tối thiểu, nên quá trình huấn luyện cũng là quá trình giảm độ không chắc chắn của dự đoán LLM. Hình 1 cho thấy mối quan hệ giữa mất mát cross-entropy và độ không chắc chắn của cpd đầu ra LLM được định nghĩa bằng entropy. Chúng ta có thể thấy rằng mất mát cross-entropy và giá trị entropy có tương quan dương cao, có nghĩa là độ chắc chắn của dự đoán LLM ngụ ý độ chính xác ở mức độ lớn.

**Hình 1**: Mối quan hệ giữa độ chính xác và độ chắc chắn của dự đoán LLM.

### 3.2 Phương pháp chính

Trong tiểu mục này, chúng tôi giới thiệu quy trình chi tiết của XL3M. XL3M tuân theo con đường phân đoạn ngữ cảnh và trích xuất thông tin chính, tức là trích xuất ngữ cảnh liên quan trước, và sau đó suy luận dựa trên ngữ cảnh liên quan. Khác với StreamLLM và các phương pháp hiện tại khác thủ công loại bỏ hầu hết token và chỉ giữ một phần nhỏ ngữ cảnh, XL3M để LLM tự quyết định những gì cần giữ.

**Phân tách ngữ cảnh dài thành các ngữ cảnh con ngắn** Cho một LLM Φ(·|θ) và một chuỗi đầu vào X có độ dài lớn hơn nhiều so với kích thước cửa sổ ngữ cảnh C của LLM, tương tự như Ratner et al. (2023), chúng tôi đầu tiên chia toàn bộ chuỗi thành một chuỗi tác vụ Xt và một chuỗi nội dung Xc, như được hiển thị trong Hình 2. Chuỗi tác vụ là một vài token từ cuối chuỗi đầu vào gốc, và nó hoạt động như một "câu hỏi" cần được trả lời. Chuỗi nội dung được phân đoạn thêm thành m chuỗi ngắn X¹c, X²c, ..., Xᵐc. Để tránh cắt một câu hoàn chỉnh thành hai phần, chúng tôi sử dụng cách thức cửa sổ trượt chồng lấp. Sau đó bằng cách nối mỗi phân đoạn ngắn Xⁱc với chuỗi tác vụ Xt, chúng tôi thu được m ngữ cảnh con Xⁱ = [Xⁱc, Xt], cho i = 1, 2, ..., m. Để thuận tiện, chúng tôi gọi Xⁱc và Xt lần lượt là phần đầu và phần cuối của ngữ cảnh con Xⁱ.

**Sử dụng LLM để chọn các phân đoạn liên quan** Chúng tôi sử dụng mô hình LLM Φ(·|θ) để tính toán cpd cục bộ pⁱ = Φ(Xⁱ|θ) cho mỗi ngữ cảnh con Xⁱ. Để hiệu quả, tất cả các ngữ cảnh con có thể được xử lý song song. Nhớ lại rằng độ chắc chắn của dự đoán LLM có tương quan cao với độ chính xác. Sau đó chúng tôi tính toán thêm entropy cho mỗi pⁱ, cụ thể là

entropy(pⁱ) = Σⱼ₌₁ᵛ -pⱼⁱ log pⱼⁱ,    (1)

trong đó v là chiều của pⁱ và pⱼⁱ là phần tử thứ j trong pⁱ. Một ngữ cảnh con Xⁱ với giá trị entropy nhỏ ngụ ý rằng phân đoạn Xⁱc liên quan đến "câu hỏi" Xt. Chúng tôi chọn các ngữ cảnh con Xⁱ với top-k giá trị entropy nhỏ nhất và loại bỏ tất cả ngữ cảnh nhiễu khác. Sau đó chúng tôi xây dựng một ngữ cảnh chính ngắn gọn bằng cách nối tất cả các phân đoạn được chọn Xⁱc (phần đầu của các ngữ cảnh con được chọn) cũng như chuỗi tác vụ Xt theo thứ tự thời gian. Bằng cách thiết kế các chiến lược phân đoạn và nối phù hợp, chúng tôi có thể đảm bảo rằng độ dài của ngữ cảnh chính nằm trong cửa sổ ngữ cảnh huấn luyện. Ngữ cảnh chính được xây dựng được sử dụng thay thế cho ngữ cảnh dài gốc để hoàn thành tác vụ suy luận. Vì hầu hết nội dung không liên quan được loại bỏ và độ dài của ngữ cảnh chính không vượt quá độ dài huấn luyện lớn nhất, cả vấn đề ngoài miền và mất tập trung đều được giải quyết. Toàn bộ quá trình của XL3M được hiển thị trong Hình 2.

**Hình 2**: Quy trình chính của XL3M.

## 4 Đánh giá

Trong phần này, chúng tôi đánh giá XL3M trên benchmark toàn diện LongBench Bai et al. (2023) và tác vụ suy luận chuỗi dài được sử dụng rộng rãi "Needle in a Haystack". Tất cả các thí nghiệm được tiến hành trên một máy NPU Huawei Ascend 910B 8 card với bộ nhớ 64GB mỗi card.

**Baseline** Chúng tôi so sánh framework XL3M được đề xuất với các phương pháp không tinh chỉnh PCW Ratner et al. (2023), StreamLLM Xiao et al. (2023) và các mô hình tinh chỉnh PI-7B-32k và Yarn-7B-64k. PI-7B-32k được thu được bằng cách tinh chỉnh mô hình Llama2-7B-4k trên các chuỗi có độ dài 32k, và Yarn-7B-64k được thu được bằng cách tinh chỉnh Mistral-7b-8k trên các chuỗi có độ dài 64k. Cách so sánh công bằng các phương pháp tinh chỉnh và không tinh chỉnh bản thân cũng là một câu hỏi mở. Lưu ý rằng mô hình sau tinh chỉnh thực sự sử dụng nhiều dữ liệu huấn luyện hơn, nên mô hình nên mạnh hơn nhiều. Ngay cả đối với suy luận chuỗi ngắn, mô hình được tinh chỉnh vẫn có hiệu suất tốt hơn, so với cùng mô hình trước khi tinh chỉnh. Để so sánh công bằng, mô hình chuỗi ngắn được chọn làm mô hình cơ sở cho các phương pháp không tinh chỉnh nên có hiệu suất tương tự với các mô hình được tinh chỉnh trên các tác vụ suy luận chuỗi ngắn. Để đạt được mục tiêu này, chúng tôi xây dựng mô hình PI-7B-2k bằng cách sửa đổi siêu tham số max_position_embeddings trong PI-7B-32k thành 2k. Mô hình PI-7B-2k được sửa đổi có cùng hiệu suất với PI-7B-32k khi độ dài chuỗi không dài hơn 2k, nhưng nó không thể xử lý các chuỗi dài hơn 2k. Cụ thể, PI-7B-2k chỉ kế thừa khả năng của PI-7B-32k trong suy luận chuỗi ngắn. Để thuận tiện, chúng tôi sử dụng PCW-7B-2k, StreamLLM-7B-2k, XL3M-7B-2k để đại diện cho các mô hình PI-7B-2k được xây dựng được trang bị với các phương pháp mở rộng không tinh chỉnh tương ứng.

**Thiết lập** Đối với XL3M, chúng tôi sử dụng 128 token cuối của một chuỗi cho trước làm chuỗi tác vụ, và phần còn lại làm chuỗi nội dung. Chuỗi nội dung được phân đoạn đều bằng cửa sổ trượt có chồng lấp. Kích thước cửa sổ trượt là 512. Kích thước chồng lấp là 128 và đối với phân đoạn cuối cùng, kích thước chồng lấp được điều chỉnh để đảm bảo độ dài nhất quán của tất cả các phân đoạn. Các token ban đầu là lời nhắc tác vụ, nên chúng tôi thêm 128 token ban đầu vào đầu mỗi ngữ cảnh con. Chúng tôi chọn các ngữ cảnh con có top-k (k=3) giá trị entropy nhỏ nhất làm ngữ cảnh con liên quan, và sử dụng chúng để xây dựng ngữ cảnh chính. Tổng độ dài của ngữ cảnh chính là 1792, nằm trong 2k. Đối với PCW, chúng tôi đặt cửa sổ ngữ cảnh nmax là 1792 và đặt số lượng token tác vụ là 128. Đối với StreamLLM, chúng tôi sử dụng 1792 token cuối làm token gần đây và 128 token đầu làm sink token để đảm bảo rằng các lời nhắc tác vụ được bao gồm và đồng thời tổng độ dài không vượt quá 2k.

### 4.1 Đánh giá trên LongBench-E Bai et al. (2023): một benchmark đa tác vụ

LongBench là một benchmark đa tác vụ đưa ra đánh giá toàn diện về khả năng hiểu ngữ cảnh dài của LLM. LongBench hỗ trợ một tập con LongBench-E, có độ dài ngữ cảnh phân phối đều hơn. LongBench-E chứa sáu loại chính và mười ba tác vụ khác nhau, bao gồm các tình huống ứng dụng văn bản dài chính, như QA tài liệu đơn, QA tài liệu đa, tóm tắt, học few-shot, tác vụ tổng hợp, và hoàn thành mã. LongBench-E bao gồm tiếng Anh, tiếng Trung, và ngôn ngữ mã. Tổng quan ngắn gọn về các tập dữ liệu LongBench-E được hiển thị trong Bảng 1. Để giới thiệu chi tiết về LongBench và LongBench-E, người ta có thể tham khảo Bai et al. (2023). Trong phần này, chúng tôi chỉ đánh giá trên các tác vụ tiếng Anh và mã trong LongBench-E, nên tập dữ liệu MultiFieldQA-en được loại bỏ, vì nó liên quan đến cả tiếng Anh và tiếng Trung.

Chúng tôi hiển thị hiệu suất của tất cả các phương pháp được so sánh trên độ dài 0-4k, 4-8k, 8k+, tương ứng. Kết quả được hiển thị trong Hình 3. Đối với mỗi tác vụ chính, chúng tôi báo cáo điểm trung bình của tất cả các tập dữ liệu thuộc về nó. Từ hình, chúng ta thấy rằng XL3M vượt trội hơn tất cả các phương pháp không tinh chỉnh khác trong hầu hết các trường hợp. Đồng thời, XL3M đạt được kết quả có thể so sánh và đôi khi thậm chí tốt hơn so với các mô hình tinh chỉnh PI-7B-32k và Yarn-7B-64k. Điều này chủ yếu là do phương pháp của chúng tôi có thể lọc ra các token nhiễu và cho phép mô hình tập trung vào thông tin liên quan. Lưu ý rằng hiệu suất của PCW-2k giảm nhanh chóng khi độ dài tăng. Điều này phù hợp với quan sát được hiển thị trong Ratner et al. (2023) rằng PCW chỉ hiệu quả cho một phạm vi mở rộng hạn chế (khoảng ba lần mở rộng kích thước cửa sổ ngữ cảnh gốc). Đáng ngạc nhiên, StreamLLM cũng có thể theo kịp hiệu suất của các phương pháp khác trong một số tác vụ, mặc dù nó loại bỏ hầu hết token. Điều này có thể do các chuỗi trong LongBench tương đối ngắn và câu trả lời thường xuất hiện gần cuối. Chúng tôi sẽ đánh giá tất cả các phương pháp này trên các chuỗi dài hơn và các tình huống đa dạng hơn trong tiểu mục tiếp theo.

**Bảng 1**: Thông tin cơ bản về thống kê tập dữ liệu trong LongBench-E, bao gồm phân phối độ dài dữ liệu, metric, và loại ngôn ngữ.

| Tập dữ liệu | 0-4k | 4-8k | 8k+ | Metric | Ngôn ngữ |
|-------------|------|------|-----|--------|----------|
| **QA Tài liệu Đơn** |
| Qasper | 100 | 100 | 24 | F1 | Tiếng Anh |
| MultiFieldQA-en | 67 | 70 | 13 | F1 | Tiếng Anh/Trung |
| **QA Tài liệu Đa** |
| HotpotQA | 100 | 100 | 100 | F1 | Tiếng Anh |
| 2WikiMultihopQA | 100 | 100 | 100 | F1 | Tiếng Anh |
| **Tóm tắt** |
| GovReport | 100 | 100 | 100 | Rouge-L | Tiếng Anh |
| MultiNews | 100 | 100 | 94 | Rouge-L | Tiếng Anh |
| **Học Few-shot** |
| TREC | 100 | 100 | 100 | Accuracy (CLS) | Tiếng Anh |
| TriviaQA | 100 | 100 | 100 | F1 | Tiếng Anh |
| SAMSum | 100 | 100 | 100 | Rouge-L | Tiếng Anh |
| **Tác vụ Tổng hợp** |
| PassageCount | 100 | 100 | 100 | Accuracy (EM) | Tiếng Anh |
| PassageRetrieval-en | 100 | 100 | 100 | Accuracy (EM) | Tiếng Anh |
| **Hoàn thành Mã** |
| LCC | 100 | 100 | 100 | Edit Sim | Python/C#/Java |
| RepoBench-P | 100 | 100 | 100 | Edit Sim | Python/Java |

**Hình 3**: Điểm trung bình (%) dưới độ dài ngữ cảnh khác nhau trên LongBench-E.

### 4.2 Đánh giá trên tác vụ "Needle in a Haystack"

"Needle in a Haystack" là một tác vụ được sử dụng rộng rãi gần đây để kiểm tra khả năng truy xuất trong ngữ cảnh của LLM ngữ cảnh dài. Quy trình chính của "Needle in a Haystack" là: 1. Đặt một sự kiện hoặc tuyên bố ngẫu nhiên (cái "kim") ở đâu đó trong một ngữ cảnh dài (cái "đống cỏ khô"); 2. Yêu cầu mô hình truy xuất tuyên bố này.

Chúng tôi xây dựng các ngữ cảnh có độ dài khác nhau, từ 16k đến 128k, để đo hiệu suất của tất cả các phương pháp trên. Hình 4 hiển thị điểm nhớ lại của tất cả các phương pháp được so sánh với "kim" được đặt ở mười phạm vi độ sâu khác nhau. Đối với mỗi phạm vi độ sâu, kết quả được tính trung bình bởi mười lần chạy độc lập với "kim" được đặt ngẫu nhiên trong phạm vi tương ứng mỗi lần. Chúng ta có thể thấy rằng XL3M thể hiện hiệu suất mạnh cho tất cả các độ dài, trong khi các mô hình PI-7B-32k và Yarn-7B-64k chỉ hoạt động tốt khi độ dài nằm trong kích thước cửa sổ ngữ cảnh tinh chỉnh của chúng, và hiệu suất của chúng giảm nhanh chóng khi độ dài vượt quá độ dài tinh chỉnh. Đối với PCW, lưu ý rằng độ dài được xem xét trong tác vụ này là lớn hơn 8 đến 64 lần so với kích thước cửa sổ ngữ cảnh của PCW-7B-2k, vượt quá phạm vi mở rộng hiệu quả của PCW, nên nó khó có thể truy xuất câu trả lời đúng. StreamLLM cũng không hoạt động tốt trong tác vụ này. Chỉ khi "kim" được đặt ngay ở cuối chuỗi (độ sâu nông) thì mới có thể nắm bắt thông tin liên quan. Điều này phù hợp với cơ chế của nó chủ yếu giữ các token gần đây (token gần cuối) và một vài token lời nhắc ban đầu.

**Hình 4**: Kiểm tra áp lực trên "Needle in a Haystack". Kiểm tra được chạy ở 4 độ dài khác nhau (16k → 128k) và 10 phạm vi độ sâu tài liệu khác nhau (dưới → trên). Mỗi kết quả được tính trung bình bởi 10 lần chạy độc lập.

Hình 5 báo cáo thêm hiệu suất của framework XL3M được đề xuất trên một phạm vi độ dài lớn hơn. Đồng thời chúng tôi điều tra tác động của kích thước cửa sổ ngữ cảnh của mô hình cơ sở gốc đối với hiệu suất của phương pháp được đề xuất. Sử dụng cách thức tương tự, chúng tôi xây dựng mô hình PI-7B-4k bằng cách sửa đổi siêu tham số max_position_embeddings trong PI-7B-32k thành 4k. So với PI-7B-2k được xây dựng trước đó, sự khác biệt duy nhất là PI-7B-4k có kích thước cửa sổ ngữ cảnh lớn hơn. Chúng tôi sử dụng XL3M-7B-4k để đại diện cho mô hình PI-7B-4k được trang bị XL3M. Kích thước cửa sổ trượt được mở rộng thành 1024. Tất cả các thiết lập khác không thay đổi, nên tổng độ dài của ngữ cảnh chính là 3328, ít hơn 4k. Từ hình, chúng ta thấy rằng, XL3M-7B-2k hoạt động tốt trong hầu hết độ dài và độ sâu, nhưng trong một số trường hợp, nó không đạt được độ chính xác 100%, trong khi XL3M-7B-4k hầu như có thể truy xuất câu trả lời đúng cho tất cả các trường hợp. Mặc dù framework XL3M được đề xuất có thể cho phép bất kỳ LLM nào suy luận các chuỗi dài, mô hình cơ sở với kích thước cửa sổ ngữ cảnh lớn hơn cho phép cửa sổ trượt lớn hơn, điều này thường góp phần đạt được hiệu suất tốt hơn. Hơn nữa, framework XL3M rất hiệu quả về bộ nhớ. Độ dài chuỗi có thể lên đến 20M hoặc thậm chí lớn hơn chỉ với 8 card NPU.

**Hình 5**: Kiểm tra áp lực trên "Needle in a Haystack" trên phạm vi độ dài lớn hơn. Trái: độ chính xác nhớ lại của XL3M-7B-2k. Phải: độ chính xác nhớ lại của XL3M-7B-4k.

Chúng tôi cũng kiểm tra framework XL3M của chúng tôi sử dụng mô hình quy mô lớn hơn nhiều Llama-65B. Kích thước cửa sổ ngữ cảnh huấn luyện trước của Llama-65B là 2k, nên chúng tôi tuân theo thiết lập của XL3M-7B-2k để tiền xử lý ngữ cảnh đầu vào. Chúng tôi sử dụng XL3M-65B-2k để đại diện cho mô hình Llama-65B được áp dụng với XL3M. Kết quả được hiển thị trong Hình 6. Chúng ta thấy rằng XL3M-65B-2k có thể nhớ lại 100% câu trả lời cho tất cả các trường hợp. Điều này phù hợp với kỳ vọng của chúng tôi. Vì Llama-65B mạnh hơn nhiều so với Llama2-7B, nó xứng đáng đạt được hiệu suất tốt hơn.

**Hình 6**: Kiểm tra hiệu suất của các phương pháp được đề xuất trên Llama-65B-2k. Phương pháp của chúng tôi đạt được 100% nhớ lại trên tác vụ "Needle in a Haystack" cho tất cả các trường hợp.

### 4.3 Đánh giá về hiệu quả thời gian

Chúng tôi so sánh hiệu quả thời gian của framework XL3M được đề xuất với các baseline, về cả thời gian prefill (tiêu thụ thời gian để sinh token đầu tiên) và thời gian giải mã (tiêu thụ thời gian để sinh tất cả các token trừ token đầu tiên). Chúng tôi đánh giá tất cả các phương pháp trên tác vụ "Needle in a Haystack" dài 128k, và chúng tôi đặt độ dài giải mã là 128. Kết quả so sánh được hiển thị trong Bảng 2. Các phương pháp StreamLLM và XL3M hiệu quả về thời gian hơn nhiều so với các đối thủ khác. Tuy nhiên, StreamLLM sẽ gây ra mất mát độ chính xác nghiêm trọng, trong khi XL3M có thể đảm bảo cả hiệu quả và hiệu quả.

**Bảng 2**: Hiệu quả thời gian của các phương pháp được so sánh.

| Mô hình | Tổng thời gian (s) | Thời gian Prefill (s) | Thời gian Giải mã (s) |
|---------|-------------------|----------------------|---------------------|
| PI-7B-32k | 118.3 | 84.2 | 34.1 |
| Yarn-7B-64k | 132.7 | 82.7 | 50.0 |
| PCW-7B-2k | 51.2 | 17.8 | 33.4 |
| StreamLLM-7B-2k | 17.4 | 4.3 | 13.1 |
| XL3M-7B-2k | 35.3 | 23.1 | 12.2 |

## 5 Kết luận

Chúng tôi tìm thấy thực nghiệm rằng độ chính xác của dự đoán LLM có mối tương quan cao với độ chắc chắn của nó được đo bằng entropy. Dựa trên điều này, chúng tôi đề xuất một framework suy luận mới XL3M, cho phép bất kỳ LLM nào phá vỡ giới hạn độ dài mà không cần bất kỳ huấn luyện tiếp tục hoặc tinh chỉnh nào. Trong framework XL3M, bất kỳ ngữ cảnh dài đầu vào nào sẽ được phân tách thành nhiều ngữ cảnh con ngắn chứa một "câu hỏi" chung là một vài token từ cuối ngữ cảnh đầu vào gốc. XL3M cung cấp một phương pháp để trích xuất các ngữ cảnh con liên quan đến tác vụ hiện tại và loại bỏ hầu hết ngữ cảnh không liên quan. Sau đó một ngữ cảnh chính ngắn gọn được xây dựng bằng cách nối các ngữ cảnh con liên quan theo thứ tự thời gian. Ngữ cảnh chính được xây dựng được sử dụng thay thế cho ngữ cảnh gốc để hoàn thành tác vụ suy luận. Kết quả thực nghiệm chứng minh hiệu quả và hiệu quả của XL3M. Chúng tôi cho thấy rằng được trang bị framework XL3M, một mô hình Llama2-7B có thể suy luận các chuỗi dài 20M trên một máy NPU Huawei Ascend 910B 8 card với bộ nhớ 64GB mỗi card.

**Hạn chế của framework được đề xuất** Framework XL3M giả định rằng chỉ một phần nhỏ của ngữ cảnh gốc liên quan đến tác vụ hoặc câu hỏi cho trước, cụ thể là độ dài của ngữ cảnh chính ít hơn kích thước cửa sổ ngữ cảnh huấn luyện. Do đó, khi cửa sổ ngữ cảnh huấn luyện của LLM rất nhỏ và ngữ cảnh liên quan chứa một số lượng đáng kể token, một số token chính phải được loại bỏ. Hơn nữa, khi nội dung liên quan được phân phối rộng rãi trong các phần khác nhau của ngữ cảnh gốc, rất khó để nắm bắt tất cả ngữ cảnh chính chỉ bằng cách chọn một vài phân đoạn. Đối với các trường hợp này, XL3M có thể không đạt được hiệu suất thỏa đáng.

## Tài liệu tham khảo

Anil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V., Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546–38556, 2022.

Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.

Bertsch, A., Alon, U., Neubig, G., and Gormley, M. Unlimiformer: Long-range transformers with unlimited length input. Advances in Neural Information Processing Systems, 36, 2024.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.

Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.

Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., and McHardy, R. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023.

Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers. arXiv preprint arXiv:2305.19466, 2023.

Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise on context length, 2023.

Munkhdalai, T., Faruqui, M., and Gopal, S. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 2024.

Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Barnes, N., and Mian, A. A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023.

Pal, A., Karkhanis, D., Roberts, M., Dooley, S., Sundararajan, A., and Naidu, S. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint arXiv:2308.10882, 2023.

Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.

Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.

Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Magar, I., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., and Shoham, Y. Parallel context windows for large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6383–6402, 2023.

Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Miłoś, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Wei, C., Wang, Y.-C., Wang, B., and Kuo, C.-C. J. An overview on language models: Recent developments and outlook. arXiv preprint arXiv:2303.05759, 2023.

Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022.

Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. arXiv preprint arXiv:2402.04617, 2024.

Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.

Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankaraman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.

Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
