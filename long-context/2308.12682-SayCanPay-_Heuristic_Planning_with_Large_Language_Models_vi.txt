# SayCanPay: Lập kế hoạch heuristic với các mô hình ngôn ngữ lớn sử dụng kiến thức miền có thể học được

Rishi Hazra1Pedro Zuidberg Dos Martires1Luc De Raedt1,2
1Đại học Örebro2KU Leuven
{rishi.hazra, pedro.zuidberg-dos-martires, luc.de-raedt }@oru.se
https://rishihazra.github.io/SayCanPay/

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng lập kế hoạch ấn tượng do "kiến thức thế giới" rộng lớn của chúng. Tuy nhiên, việc thu được các kế hoạch vừa khả thi (dựa trên khả năng thực hiện) vừa hiệu quả về chi phí (về độ dài kế hoạch) vẫn là một thách thức, mặc dù đã có tiến bộ gần đây. Điều này tương phản với các phương pháp lập kế hoạch heuristic sử dụng kiến thức miền (được hình thức hóa trong các mô hình hành động như PDDL) và tìm kiếm heuristic để tạo ra các kế hoạch khả thi, tối ưu. Lấy cảm hứng từ điều này, chúng tôi đề xuất kết hợp sức mạnh của LLM và lập kế hoạch heuristic bằng cách tận dụng kiến thức thế giới của LLM và các nguyên tắc tìm kiếm heuristic. Phương pháp của chúng tôi, SayCanPay, sử dụng LLM để tạo ra các hành động (Say) được hướng dẫn bởi kiến thức miền có thể học được, đánh giá tính khả thi của hành động (Can) và phần thưởng/lợi ích dài hạn (Pay), và tìm kiếm heuristic để chọn chuỗi hành động tốt nhất. Các đóng góp của chúng tôi là (1) một cách tiếp cận mới về vấn đề lập kế hoạch LLM trong bối cảnh lập kế hoạch heuristic, (2) tích hợp các yếu tố cơ sở và hiệu quả chi phí vào các kế hoạch được tạo ra, và (3) sử dụng tìm kiếm heuristic trên các hành động. Các đánh giá rộng rãi của chúng tôi cho thấy mô hình của chúng tôi vượt trội hơn các phương pháp lập kế hoạch LLM khác.

## 1 Giới thiệu

Với sự gia tăng của các mô hình ngôn ngữ lớn (LLM), đã có sự quan tâm ngày càng tăng đối với việc tận dụng khả năng tạo sinh của chúng cho các nhiệm vụ lập kế hoạch (Huang et al. 2022a; Valmeekam et al. 2022; Silver et al. 2022; Liu et al. 2023). Các mô hình này có khả năng tạo ra các kế hoạch dài hạn, tận dụng "kiến thức thế giới" rộng lớn có được từ việc huấn luyện trên lượng dữ liệu khổng lồ (ví dụ: trứng thường được bảo quản trong tủ lạnh, và việc đặt một quả táo vào tủ lạnh sẽ làm mát nó). Kiến thức rộng lớn như vậy có thể được khai thác để lập kế hoạch trong bối cảnh thế giới mở (Ding et al. 2023). Hơn nữa, việc lập kế hoạch trong không gian ngôn ngữ tự nhiên mang lại sự linh hoạt đáng kể, đặc biệt với sự xuất hiện của các mô hình nền tảng đa phương thức (Lakhotia et al. 2021; Du et al. 2022; Brohan et al. 2023). Các mô hình như vậy đã giúp việc biểu diễn các phương thức khác nhau như thị giác, giọng nói, và thậm chí cả hành động dưới dạng ngôn ngữ tự nhiên trở nên dễ dàng hơn, do đó bỏ qua nhu cầu có kiến thức miền cụ thể (ví dụ: PDDL) mà các phương pháp lập kế hoạch truyền thống yêu cầu. Tuy nhiên, việc lập kế hoạch dựa trên LLM thường gặp phải các thách thức, đặc biệt là trong việc tạo ra các kế hoạch khả thi. Nó có thể thất bại trong việc mô hình hóa khả năng thực hiện hành động (hoặc điều kiện tiên quyết)1 do khó khăn trong việc mô hình hóa trạng thái của thế giới (ví dụ: lấy sữa từ tủ lạnh ngay cả khi cửa tủ đang đóng) hoặc có mô hình thế giới được huấn luyện trước không phù hợp với môi trường hiện tại (ví dụ: sử dụng bộ điều khiển để điều chỉnh máy sưởi khi chỉ có núm vặn), dẫn đến các kế hoạch không khả thi. Hơn nữa, các mô hình như vậy tập trung một cách tham lam vào bước hành động tiếp theo mà không xem xét mức độ liên quan của nó đến mục tiêu cuối cùng, dẫn đến các kế hoạch dài hơn, không hiệu quả về chi phí (Valmeekam et al. 2023). Các nghiên cứu gần đây như SayCan (Ahn et al. 2022) đã tìm cách giải quyết vấn đề khả năng thực hiện bằng cách sử dụng các kỹ năng được huấn luyện trước để đánh giá khả năng thực thi của hành động – Liệu hành động có thể được thực hiện trong trạng thái hiện tại không? Tuy nhiên, chi phí kế hoạch vẫn là một mối quan tâm.

Ngược lại, lập kế hoạch truyền thống cung cấp một phương pháp đã được thiết lập để phát triển một chuỗi hành động để chuyển từ trạng thái ban đầu sang trạng thái mục tiêu. Nó sử dụng tệp miền (với các mô hình hành động được định nghĩa trong PDDL chỉ định điều kiện tiên quyết và hậu quả) và các bộ lập kế hoạch tìm kiếm heuristic như Fast Downward (Helmert 2006) để đảm bảo tính khả thi thông qua việc dựa trên điều kiện tiên quyết, và tạo ra các kế hoạch hiệu quả về chi phí bằng cách sử dụng cây tìm kiếm để chọn chuỗi hành động tốt nhất (hoặc ngắn nhất). Tuy nhiên, việc thu được tệp miền cho các môi trường thế giới thực phức tạp là khó khăn, và việc sử dụng nó hạn chế việc lập kế hoạch trong một bối cảnh thế giới đóng. Các phương pháp này cũng gặp khó khăn trong việc xử lý các quan sát một phần, mặc dù lập kế hoạch xấp xỉ (Kaelbling, Littman, and Cassandra 1998) có thể giảm bớt điều này.

Việc tích hợp LLM với lập kế hoạch cổ điển mang lại một hướng nghiên cứu đầy hứa hẹn, kết hợp khả năng tạo sinh và kiến thức thế giới (mở) của LLM với tính nghiêm ngặt về phương pháp luận của các thuật toán lập kế hoạch. Để đạt được điều này, chúng tôi mở rộng các đóng góp sau. (1) Chúng tôi đề xuất đóng khung việc lập kế hoạch mô hình ngôn ngữ trong bối cảnh lập kế hoạch heuristic, mà theo hiểu biết của chúng tôi, đây là lần đầu tiên có loại này (§ 4). (2) Chúng tôi kết hợp các yếu tố tính khả thi và hiệu quả chi phí vào các kế hoạch được tạo ra bằng cách sử dụng một cách tính điểm kết hợp được đặt tên là SayCanPay. Như được hiển thị trong Hình 1, nó hướng dẫn việc lập kế hoạch thông qua ba bước chính: (i) Say: Với một mục tiêu và quan sát ban đầu, LLM tạo ra các hành động ứng viên có khả năng tại mỗi bước; (ii) Can: Một mô hình khả năng thực hiện đánh giá tính khả thi của các hành động này, phản ánh việc đánh giá điều kiện tiên quyết; (iii) Pay: Một mô hình khác đánh giá các hành động theo phần thưởng/lợi ích ước tính của chúng, tương tự như các bộ ước tính heuristic (§ 5). Các mô hình Can và Pay được huấn luyện theo miền cụ thể để căn chỉnh các kế hoạch với môi trường hiện tại (§ 6). (3) Sử dụng điểm kết hợp này như một heuristic, chúng tôi tìm kiếm kế hoạch khả thi và hiệu quả chi phí nhất (§ 5.2). Chúng tôi chứng minh cách cách tính điểm kết hợp và tìm kiếm heuristic được đề xuất của chúng tôi cải thiện so với các khung lập kế hoạch LLM hiện tại (§ 7.3).

## 2 Nghiên cứu liên quan về lập kế hoạch với LLM

| Mô hình | I/O | Bộ lập kế hoạch | Kiến thức miền | Tìm kiếm | Lập kế hoạch |
|---------|-----|-----------------|----------------|----------|--------------|
|         |     |                 | Khả năng thực hiện | Heuristic |              |
| HSP (Bonet and Geffner 2001) | Ký hiệu | Ký hiệu | ✓ | ✓ | Heuristic | Ngoại tuyến |
| LLM+P (Liu et al. 2023) | Kết hợp | Ký hiệu | ✓ | ✓ | Heuristic | Ngoại tuyến |
| Planning LM (Huang et al. 2022a) | NL | LLM | ✗ | ✗ | Tham lam* | Ngoại tuyến |
| SayCan (Ahn et al. 2022) | NL | LLM | ✓ | ✗ | Tham lam* | Trực tuyến |
| Grounded Decoding (Huang et al. 2023) | NL | LLM | ✓ | ✗ | Tham lam* | Trực tuyến |
| Text2Motion (Lin et al. 2023) | NL | LLM | ✓ | ✗ | Tham lam* | Trực tuyến |
| ProgPrompt (Singh et al. 2023) | Ký hiệu | LLM | ✓ | ✗ | Tham lam* | Ngoại tuyến |
| Plansformer (Pallagani et al. 2022) | Ký hiệu | LLM | ✓ | ✗ | Tham lam* | Ngoại tuyến |
| SayCanPay (Beam-Action) | NL | LLM | ✓ | ✓ | Heuristic | Ngoại tuyến |

**Bảng 1:** Bảng so sánh SayCanPay với các nghiên cứu hiện có. I/O: đầu vào (mục tiêu/nhiệm vụ, quan sát/trạng thái) / đầu ra (hành động), NL: ngôn ngữ tự nhiên. Ở đây, Tham lam* gợi ý thuật toán chọn hành động một cách tham lam trong khi (có thể) tìm kiếm trên các token.

Bảng 1 phân loại các nghiên cứu lập kế hoạch LLM thành hai loại rộng dựa trên việc liệu các đầu vào (mục tiêu, trạng thái) và đầu ra hành động (I/O) là ngôn ngữ tự nhiên (NL) hay ký hiệu (PDDL, ngôn ngữ kịch bản). Các phương pháp trong loại đầu tiên (Huang et al. 2022a; Valmeekam et al. 2022) thường thất bại trong việc mô hình hóa khả năng thực hiện hành động và trạng thái của thế giới, dẫn đến việc tạo ra các kế hoạch không khả thi (Valmeekam et al. 2022). Để cải thiện tính cơ sở, các nghiên cứu gần đây đã khám phá việc lập kế hoạch được hướng dẫn bởi các mô hình miền cụ thể có thể học được để đánh giá tính khả thi của hành động tương tự như điều kiện tiên quyết (Huang et al. 2023; Lin et al. 2023). Đặc biệt, SayCan (Ahn et al. 2022) sử dụng các kỹ năng cấp thấp được huấn luyện trước để cơ sở hóa các hành động do LM tạo ra. Những nghiên cứu khác đã sử dụng lập kế hoạch trực tuyến với phản hồi môi trường và con người (Huang et al. 2022b). Tuy nhiên, một hạn chế của các mô hình như vậy là bản chất cận thị của chúng, vì chúng tập trung một cách tham lam vào hành động khả thi tiếp theo mà không xem xét mức độ liên quan dài hạn của nó đối với mục tiêu. Hơn nữa, các kế hoạch được tạo ra theo cách trực tuyến, xen kẽ việc tạo ra hành động và thực hiện, do đó đơn giản hóa việc theo dõi trạng thái. Ngược lại, SayCanPay thực hiện lập kế hoạch ngoại tuyến (tức là tạo ra kế hoạch hoàn chỉnh trong khi duy trì trạng thái thế giới nội bộ) với cả bộ ước tính điều kiện tiên quyết và heuristic, cải thiện tính khả thi và hiệu quả chi phí của kế hoạch.

Một hướng nghiên cứu khác sử dụng LLM để tạo ra các kế hoạch ký hiệu ngoại tuyến, tận dụng việc huấn luyện LLM trên các cơ sở mã nguồn mở, nơi các hành động xuất hiện như các lời gọi hàm (Singh et al. 2023; Liang et al. 2023). Tính khả thi của các kế hoạch được đảm bảo thông qua kiểm tra khẳng định (assert⟨điều kiện tiên quyết⟩), có thể kích hoạt các hành động phục hồi. Tuy nhiên, nó chỉ dựa vào kiến thức miền của LLM bị hạn chế trong dữ liệu huấn luyện và có thể không phù hợp với môi trường hiện tại của tác nhân (ví dụ: hoạt động của máy pha cà phê espresso rất đa dạng). Ngược lại, SayCanPay sử dụng các mô hình bổ sung được huấn luyện với kiến thức miền cụ thể thu thập từ môi trường hiện tại. Cũng có những nỗ lực để tinh chỉnh LLM như Code-T5 (Wang et al. 2021) để tạo ra các kế hoạch trong PDDL (Pallagani et al. 2022). Điều này đòi hỏi một lượng dữ liệu huấn luyện đáng kể (do tiếp xúc tối thiểu của LLM với PDDL) mà không hoàn toàn được biện minh bởi hiệu suất của chúng.

Một hướng nghiên cứu thú vị khác khám phá các hệ thống I/O kết hợp như LLM+P (Liu et al. 2023) trong đó, cho một tệp miền PDDL (với mô hình hành động được định nghĩa trước), LLM ánh xạ các đầu vào NL (mô tả nhiệm vụ, quan sát đầu vào) thành tệp vấn đề PDDL. Một bộ lập kế hoạch ký hiệu sau đó tạo ra kế hoạch. Tuy nhiên, hiệu quả của nó bị hạn chế bởi ràng buộc thế giới đóng của tệp miền, nhu cầu về các trạng thái có thể quan sát hoàn toàn, và khả năng hạn chế của LLM trong việc dịch NL sang PDDL (Xie et al. 2023).

## 3 Kiến thức cơ bản

**Khung lập kế hoạch.** Chúng tôi hình thức hóa vấn đề lập kế hoạch của mình, dựa trên lập kế hoạch xấp xỉ (Golowich, Moitra, và Rohatgi 2022), như một Quá trình quyết định Markov có thể quan sát một phần (POMDP) theo thời gian hữu hạn được cho bởi bộ ⟨S, SG, b0, A, O, R, T⟩. Ở đây, S là không gian trạng thái, SG ⊆ S là tập hợp các trạng thái mục tiêu, b0 là trạng thái tin tưởng ban đầu, A là tập hợp các hành động, O là tập hợp các quan sát được truy xuất từ các trạng thái thông qua hàm quan sát O, R: O → R là hàm phần thưởng đã biết, T: S × A → ΔS là hàm chuyển đổi ngẫu nhiên đã biết và ΔS là một phân phối trên các trạng thái. Các trạng thái tin tưởng đại diện cho kiến thức của tác nhân về môi trường tại bất kỳ thời điểm nào, được cho như b ∈ ΔS. Ngoài ra, để Ht := (A × O)t−1 biểu thị tập hợp các lịch sử tại bước t, cụ thể là tập hợp các chuỗi hành động/quan sát (o0, a1, o1, . . . , at−1, ot−1) hoặc (a1:t−1, o0:t−1) mà tác nhân có quyền truy cập trước khi chọn hành động at. Người ta giả định rằng các trạng thái mục tiêu có thể quan sát hoàn toàn. Không giống như MDP, chính sách tối ưu trong POMDP thường thực hiện các hành động phụ thuộc không chỉ vào quan sát gần đây nhất mà còn vào toàn bộ lịch sử. Mục tiêu của thuật toán lập kế hoạch là tìm chuỗi hành động tối ưu a1:T (tức là một kế hoạch tối ưu) từ trạng thái tin tưởng ban đầu b0 đến trạng thái mục tiêu cho trước g ∈ SG. Ở đây, T là độ dài của chân trời.

**Lập kế hoạch tìm kiếm heuristic.** Trong các tình huống thế giới thực nơi không gian trạng thái có thể có kích thước cực lớn để khám phá một cách đầy đủ, lập kế hoạch tìm kiếm heuristic (HSP) trở nên hữu ích (Bonet và Geffner 2001). Về cơ bản, nó sử dụng các hàm heuristic fheur: Ht × SG → R để hướng dẫn quá trình tìm kiếm trong vấn đề lập kế hoạch, bằng cách tính toán ước tính chi phí từ lịch sử hành động và quan sát cho trước. Một ví dụ là các thuật toán Tìm kiếm tốt nhất đầu tiên chọn (các) hành động tiếp theo hứa hẹn nhất bằng cách sử dụng tổ hợp tuyến tính của chi phí tích lũy trước đây facc cho lịch sử ht−1, và chi phí ước tính fheur từ lịch sử cập nhật ht = (ht−1, at) và mục tiêu g.

f(ht) = z1 · facc(ht−1) + z2 · fheur(ht, g)    (1)

Ở đây z1, z2 ∈ {0, 1}. Hành động tiếp theo at = arg minht f(ht). Các trường hợp đặc biệt là thuật toán A* (z1 = 1 và z2 = 1) và Tìm kiếm tham lam tốt nhất đầu tiên (z1 = 0 và z2 = 1).

## 4 Khung lập kế hoạch mô hình ngôn ngữ

Chúng tôi giữ nguyên công thức POMDP trong khi cập nhật cách diễn giải của chúng tôi về bộ. Các nghiên cứu trước đây đã chỉ ra rằng các mô hình ngôn ngữ (LM) được huấn luyện trên dữ liệu rộng lớn sẽ nội tại hóa kiến thức thế giới phong phú có thể được truy vấn cho các nhiệm vụ hạ nguồn như lập kế hoạch (Hao et al. 2023). Điều này tương tự như một hàm chuyển đổi nội bộ Tint. Tương tự, LM cũng duy trì và cập nhật trạng thái tin tưởng nội bộ bint_t trên các token (hoặc hành động). Hàm quan sát ánh xạ các trạng thái thành các quan sát NL, O: S → O. POMDP được cập nhật bây giờ được cho như ⟨S, SG, bint_0, A, O, R, Tint⟩. Trong các thí nghiệm lập kế hoạch ngoại tuyến của chúng tôi, chúng tôi giả định như sau: (i) O = {o0} tạo ra trạng thái tin tưởng bint_0 = 1s0, trong khi ot = ∅ ∀t > 0, do thiếu phản hồi môi trường; (ii) phần thưởng thưa thớt = 1 cho thành công kế hoạch, ngược lại 0. Trong khi LM của chúng tôi không sử dụng hàm phần thưởng, người ta có thể sử dụng nó để căn chỉnh (Ziegler et al. 2020).

**Phát biểu vấn đề:** Cho mục tiêu NL g, lịch sử h0 = (o0), và LM tạo ra hành động at với xác suất p(at|ht−1, g), tạo ra kế hoạch có khả năng nhất (a1:T) để đi từ bint_0 đến g, tức là arg max_a1:T P(a1:T|h0, g).

Chúng tôi nhằm tối đa hóa xác suất của kế hoạch, đóng khung lại việc lập kế hoạch LM như một vấn đề tìm kiếm cổ điển, nơi chúng tôi liên tục mở rộng kế hoạch hiện tại a1:t−1 bằng cách thêm hành động at. Viết lại xác suất P(a1:T|h0, g) một cách đệ quy như:

= P(a1:t−1, at, at+1:T|h0, g)
= p(a1:t−1|h0, g)p(at|h0, a1:t−1, g)p(at+1:T|h0, a1:t, g)
= p(a1:t−1|h0, g) · p(at|ht−1, g) · p(at+1:T|ht, g)

Để căn chỉnh với Eq 1 của vấn đề lập kế hoạch, chúng tôi lấy log trên cả hai vế và tối đa hóa thay vì tối thiểu hóa. Chúng tôi có giá trị tích lũy facc(ht−1) = log p(a1:t−1|h0, g), lợi ích heuristic fheur(ht, g) = p(at+1:T|ht, g), và f(ht) = log P(a1:T|h0, g). Viết lại phương trình trên:

f(ht) = facc(ht−1) + log[p(at|ht−1, g) · fheur(ht, g)]    (2)

p(at|ht−1, g) bổ sung phản ánh rằng, không giống như lập kế hoạch cổ điển chỉ đánh giá các hành động khả thi dựa trên điều kiện tiên quyết, LM gán xác suất cho mỗi hành động. Ở đây, hành động tiếp theo at = arg maxht f(ht).

Về mặt kỹ thuật, LM tạo ra các hành động trong đó mỗi hành động là một chuỗi các token cho đến token kết thúc chuỗi, ⟨EOS⟩. Đối với mỗi bước hành động a = (w1, . . . , wn) bao gồm các token wi, LM tính toán xác suất hành động như p(a) = p(w1)∏n_i=2 p(wi|w1:i−1). Planning LM (Huang et al. 2022a) đã đề xuất một chiến lược giải mã tham lam trong đó LM tham lam chọn token tiếp theo, từ nay được gọi là baseline Greedy-Token (Hình 2 Trái). Hành động được tạo ra sau đó được thêm vào lịch sử ht = (ht−1, at), và quá trình tạo ra lặp lại cho đến khi hành động "done task" được tạo ra. Các nghiên cứu tiếp theo (Lin et al. 2023) đã điều tra tìm kiếm chùm trên các token. Tuy nhiên, chúng tôi chủ yếu quan tâm đến việc tìm kiếm ở cấp độ hành động chứ không phải token.

## 5 Suy luận SayCanPay

Khái niệm cốt lõi của SayCanPay là hướng dẫn LM trong việc tạo ra các kế hoạch khả thi và hiệu quả về chi phí. Quá trình diễn ra trong ba bước chính: (1) Say: Tại mỗi bước t, LM tạo ra top-m hành động ứng viên với xác suất liên quan {p(ai_t|ht−1, g)}m_i=1. Việc tạo ra này sử dụng tìm kiếm chùm trên các token. (2) Can: Tiếp theo, một mô hình miền cụ thể được huấn luyện cân nhắc các hành động ứng viên này về tính khả thi của chúng, phản ánh việc đánh giá điều kiện tiên quyết. (3) Pay: Cuối cùng, một bộ ước tính miền cụ thể được huấn luyện cân nhắc các hành động ứng viên theo lợi ích ước tính của chúng. Các xác suất từ ba thành phần này sau đó được kết hợp để chọn hành động tiếp theo. Tổng quan về SayCanPay được cung cấp trong Hình 1.

Trong phần sau, chúng tôi khởi tạo vấn đề lập kế hoạch LM với hai chiến lược giải mã (hoặc thuật toán tìm kiếm chọn (các) hành động tiếp theo): Greedy Action (§ 5.1) và Beam Action (§ 5.2). Mỗi chiến lược được khám phá bằng cách sử dụng ba điểm giải mã khác biệt (tức là điểm được sử dụng bởi thuật toán tìm kiếm để chọn hành động tiếp theo) – Say, SayCan, SayCanPay. Sau đó chúng tôi trình bày chi tiết việc huấn luyện các mô hình Can và Pay (§ 6).

### 5.1 Greedy-Action

Trong chiến lược giải mã này, chúng tôi duy trì một chuỗi hành động duy nhất và tại mỗi bước, tham lam chọn hành động tốt nhất tiếp theo dựa trên điểm giải mã cụ thể. Điều này tương tự như thực hiện Tìm kiếm tham lam tốt nhất đầu tiên với z1 = 0 và z2 = 1. Điểm giải mã cho mỗi hành động ứng viên ai_t được cho như:

f(hi_t) = log[p(ai_t|ht−1, g) · fheur(hi_t, g)]

Ở đây, hành động tốt nhất a*_t = arg maxhi_t f(hi_t), trong đó hi_t = (ht−1, ai_t) biểu thị lịch sử hiện tại với hành động ứng viên thứ i. Như được hiển thị trong Hình 2, phương pháp này có thể được xem là "tham lam" đối với các hành động trong khi sử dụng "chùm" trên các token. Bây giờ, chúng tôi khám phá ba biến thể của chiến lược dựa trên cách điểm giải mã được tính toán.

• **Say**: Trong điểm giải mã này, chúng tôi đặt lợi ích ước tính fheur(hi_t, g) = 1 ∀i ∈ {1, . . . , m}. Do đó, hành động được chọn chỉ dựa trên xác suất tạo ra LM, mà không xem xét tính khả thi hoặc lợi ích.

f(hi_t) = log[p(ai_t|ht−1, g)] =: psay_ai_t    (3)

• **SayCan**: Ở đây, tính khả thi của hành động cũng được xem xét. Đặt σt = (at, pre(at)) trong đó pre(at) biểu thị điều kiện tiên quyết của at. Xác suất "can"2 được ký hiệu bởi p(pre(at)|ht−1, g). Một lần nữa, fheur(hi_t, g) = 1 ∀i.

f(hi_t) = log[p(σi_t|ht−1, g)]
= log[p(ai_t|ht−1, g)] =: psay_ai_t · p(pre(ai_t)|ht−1, g) =: pcan_ai_t    (4)

• **SayCanPay**: Điểm giải mã này tính đến lợi ích ước tính ngoài các điểm được đề cập ở trên. Do đó, hành động tốt nhất được chọn dựa trên điểm kết hợp của điểm Say, Can và Pay.

log[p(ai_t|ht−1, g)] =: psay_ai_t · p(pre(ai_t)|ht−1, g) =: pcan_ai_t · fheur(hi_t, g) =: ppay_ai_t    (5)

### 5.2 Beam-Action

Trong lập kế hoạch heuristic, nhiều kế hoạch tiềm năng (tức là chuỗi hành động) được duy trì đồng thời và mở rộng lặp đi lặp lại cho đến khi đạt được mục tiêu. Để mô phỏng hành vi này, chúng tôi đề xuất quản lý k chuỗi hành động. Nó hoạt động như sau – mỗi chuỗi được mở rộng với m hành động ứng viên (trong đó m ≥ k) từ LM, dẫn đến tổng cộng k × m chuỗi. Sau đó, top-k chuỗi được giữ lại bằng cách sử dụng điểm giải mã cụ thể được tích lũy trên chuỗi, như được hiển thị bên dưới. Một khi tất cả k-beam đã kết thúc, chúng tôi chọn chuỗi có điểm tích lũy (được chuẩn hóa độ dài)3 cao nhất. Để tránh lặp lại, chúng tôi chỉ hiển thị phiên bản SayCanPay. Phần còn lại có thể được hình thức hóa tương tự.

top-k 1/|hij_t| [facc(hi_t−1) + log p(σj_t|hi_t−1, g) · fheur(hij_t, g)]

Ở đây, i ∈ {1, . . . , k}, j ∈ {1, . . . , m}, k ≤ m. Lịch sử cập nhật hij_t = (hi_t−1, aj_t) được thu được bằng cách thêm hành động aj_t vào lịch sử beam thứ i hi_t−1. Kết quả trở thành giá trị cho facc(ht) cho lần lặp tiếp theo. Lưu ý rằng việc đặt k = 1 dẫn đến giải mã Greedy-Action.

Giải mã được đề xuất của chúng tôi có điểm tương đồng với suy luận Tree-of-Thoughts (Yao et al. 2023) cũng duy trì nhiều đường suy luận để quyết định bước tiếp theo. Tuy nhiên, phương pháp của chúng tôi được thiết kế đặc biệt cho các vấn đề lập kế hoạch. Nó sử dụng các kỹ thuật tìm kiếm và đánh giá tương tự như các phương pháp lập kế hoạch, làm cho nó phù hợp hơn cho những thách thức như vậy. Bây giờ, chúng tôi thảo luận về chi tiết huấn luyện của các mô hình Can và Pay.

## 6 Học các mô hình Can và Pay

Để huấn luyện các mô hình Can và Pay miền cụ thể của chúng tôi, chúng tôi thu thập N quỹ đạo chuyên gia E = {τ}N_n=1 cho mỗi môi trường bằng cách sử dụng bộ lập kế hoạch oracle, trong đó τi = (o0, g, a1, a2, . . . , aT, r). Lưu ý, r = 1 cho tất cả quỹ đạo chuyên gia.

### 6.1 Mô hình Can

Chúng tôi mô hình nó như một vấn đề phân loại, trong đó hành động dương (tức là hành động có điều kiện tiên quyết được thỏa mãn) được gán xác suất cao nhất từ một tập hợp một hành động dương và một số hành động âm. Cụ thể, chúng tôi lấy mẫu một lô hành động [ht−1, g, at, a¯t≠t, ã]1:B từ quỹ đạo chuyên gia E. Sau đó chúng tôi huấn luyện mô hình Mcan với mục tiêu tối thiểu hóa tổn thất InfoNCE (van den Oord, Li, và Vinyals 2019):

−1/B ∑B_i=1 log [Mcan(hi_t−1, gi, ai_t) / ∑a∈{ai_t,ai_¯t≠t,ãi} Mcan(hi_t−1, gi, a)]

Ở đây, B là kích thước lô, at là hành động dương từ quỹ đạo τi được thực hiện trong bối cảnh lịch sử ht−1 với mục tiêu g, a¯t≠t là hành động âm được lấy mẫu từ cùng quỹ đạo τi, nhưng tại bước thời gian khác ¯t, và ã là hành động âm được lấy mẫu từ quỹ đạo khác τj≠i với quan sát ban đầu o0 và mục tiêu g khác nhau. Mcan bao gồm mô hình Bert không phân biệt chữ hoa thường (Devlin et al. 2019) với lớp thăm dò và được huấn luyện end-to-end để xác định chính xác hành động dương. Đầu vào cho Mcan có định dạng '⟨Goal⟩{g} ⟨History⟩{ht−1} ⟨NXT⟩{at}'. Ở đây, '⟨*⟩' phục vụ như các token đặc biệt. Đầu ra là xác suất Can pcan_at := Mcan(ht−1, g, at). Mô hình được huấn luyện qua nhiều lô để hội tụ điểm F1 trên tập xác thực. Phương pháp của chúng tôi khác với SayCan (Ahn et al. 2022) huấn luyện nhiều hàm khả năng thực hiện (tương ứng với các kỹ năng khác nhau), thông qua học tăng cường dựa trên sự khác biệt thời gian để dự đoán khả năng một kỹ năng cụ thể thành công (tức là thực hiện) trong trạng thái hiện tại. Ở đây, chúng tôi hiển thị hai ví dụ huấn luyện I/O, một với hành động dương và một với hành động âm.

**Đầu vào** ⟨Goal⟩pick up the purple box. ⟨Initial State⟩Room 1 has yellow key, agent. Room 2 has purple box. The door connecting Room 1 and Room 2 is locked. ⟨Step 1⟩pick up yellow key. ⟨NXT⟩toggle yellow door.
**Đầu ra** 1.0 //khả thi

**Đầu vào** ⟨Goal⟩pick up the purple box. ⟨Initial State⟩Room 1 has yellow key, agent. Room 2 has purple box. The door connecting Room 1 and Room 2 is locked. ⟨Step 1⟩pick up yellow key. ⟨NXT⟩pick up purple box.
**Đầu ra** 0.0 //không khả thi

### 6.2 Mô hình Pay

Chúng tôi mô hình nó như một vấn đề hồi quy để ước tính lợi ích hành động. Sử dụng quỹ đạo chuyên gia E, chúng tôi tạo ra một tập dữ liệu với mỗi lô như [g, ht−1, at, r]1:B. Cho phần thưởng thưa thớt (tức là rT = 1), chúng tôi sử dụng chiết khấu thời gian δ ∈ (0, 1) để gán phần thưởng chiết khấu cho các hành động trước đó trong quỹ đạo4. Điều này đảm bảo rằng các hành động gần cuối nhận được phần thưởng cao hơn và ngược lại. Cụ thể, rT−1 = δ, rT−2 = δ2, và cứ như vậy. Chúng tôi cũng lấy mẫu các hành động âm từ các đường dẫn khác (tương tự như mô hình Can) với phần thưởng 0. Mô hình được huấn luyện để căn chỉnh phần thưởng chiết khấu của hành động và phần thưởng dự đoán từ Mpay bằng cách tối thiểu hóa tổn thất trung bình bình phương (MSE) 1/B ∑B_i=1 (ri − Mpay(gi, hi_t−1, ai_t))2. Mô hình sử dụng Bert không phân biệt chữ hoa thường cộng với lớp hồi quy có đầu ra được giới hạn trong [0, 1] thông qua kích hoạt sigmoid. δ cho việc huấn luyện mô hình Pay không liên quan đến POMDP.

**Đầu vào** ⟨Goal⟩pick up the purple box. ⟨Initial State⟩Room 1 has yellow key, agent. Room 2 has purple box. The door connecting Room 1 and Room 2 is locked. ⟨Step 1⟩pick up yellow key. ⟨Step 2⟩toggle yellow door. ⟨Step 3⟩drop key in void. ⟨Step 4⟩pick up blue box. ⟨NXT⟩done picking up.
**Đầu ra** 1.0 //kết thúc kế hoạch

**Đầu vào** ⟨Goal⟩pick up the purple box. ⟨Initial State⟩Room 1 has yellow key, agent. Room 2 has purple box. The door connecting Room 1 and Room 2 is locked. ⟨Step 1⟩pick up yellow key. ⟨Step 2⟩toggle yellow door. ⟨Step 3⟩drop key in void. ⟨NXT⟩pick up blue box.
**Đầu ra** 0.6 //δ·r

**Đầu vào** ⟨Goal⟩pick up the purple box. ⟨Initial State⟩Room 1 has yellow key, agent. Room 2 has purple box. The door connecting Room 1 and Room 2 is locked. ⟨Step 1⟩pick up yellow key. ⟨Step 2⟩toggle yellow door. ⟨Step 3⟩drop key in void. ⟨NXT⟩pick up green box.
**Đầu ra** 0 //lợi ích rất thấp

## 7 Thiết lập thí nghiệm

### 7.1 Mô hình Say

Mô hình Say không trải qua bất kỳ tinh chỉnh nào và chỉ được sử dụng để suy luận. Chúng tôi thí nghiệm với hai loại kiến trúc transformer. (i) **Loại decoder**: mô hình Vicuna 13b-parameter (Chiang et al. 2023) được huấn luyện bằng cách tinh chỉnh LLaMA (Touvron et al. 2023). (ii) **Loại encoder-decoder**: Flan-T5-11b (Chung et al. 2022) là phiên bản được tinh chỉnh theo hướng dẫn của transformer T5 (Raffel et al. 2020). Các nghiên cứu hiện có đã chứng minh khả năng lập kế hoạch của cả kiến trúc loại decoder (Pallagani et al. 2022) và loại encoder-decoder (Valmeekam et al. 2023, 2022). Vì kế hoạch được tạo ra ở dạng ngôn ngữ tự do và có thể chứa các từ không thể nhận dạng (đối với môi trường) hoặc cú pháp không chính xác, nó không thể được dịch trực tiếp thành các bước có thể thực hiện trong môi trường. Theo Huang et al. (2022a), chúng tôi sử dụng danh sách đầy đủ các hành động có thể chấp nhận (khả thi và không), và ở cuối mỗi bước hành động, ánh xạ hành động được tạo ra với hành động có thể chấp nhận gần nhất bằng cách sử dụng khoảng cách chỉnh sửa tối thiểu. Việc xen kẽ tạo ra hành động và ánh xạ đảm bảo rằng tất cả các bước tiếp theo được điều kiện hóa trên các hành động có thể chấp nhận, do đó giảm thiểu các lỗi tích lũy. Chúng tôi cung cấp 3 ví dụ (mục tiêu đầu vào và quan sát, kế hoạch đầu ra) cho mô hình thông qua prompting few-shot.

### 7.2 Môi trường

Chúng tôi đã thử nghiệm trong ba môi trường, được mô tả chi tiết trong Bảng 2.

• **Ravens** (Zeng et al. 2021) là một tập nhiệm vụ mô phỏng PyBullet tập trung vào "pick and place". Nó bao gồm 10 nhiệm vụ tabletop, trong đó chúng tôi sử dụng hai: (i) Tower of Hanoi (sequence), một biến thể của câu đố cổ điển tập trung vào các mục tiêu trung gian cụ thể, như di chuyển một đĩa cụ thể đến thanh được chỉ định trong khi giữ các ràng buộc truyền thống. Điều này tạo ra sự đa dạng mục tiêu hơn; (ii) Put blocks in bowls yêu cầu đặt các khối vào bát dựa trên các quy tắc như đặt khối vàng vào bát xanh lá cây. Chúng tôi điều chỉnh môi trường cho các nhiệm vụ ngôn ngữ, quan sát và hành động.

• **BabyAI** (Chevalier-Boisvert et al. 2019) là môi trường lưới 2D nơi một bot được cung cấp nhiệm vụ ngôn ngữ được lấy mẫu từ ngữ pháp được định nghĩa trước. Chúng tôi tập trung vào các nhiệm vụ pickup nơi tác nhân điều hướng để thu thập đối tượng, thường mở khóa cửa hoặc di chuyển chướng ngại vật. Độ khó của nhiệm vụ thay đổi với phòng, chướng ngại vật và đối tượng gây xao nhãng. Các hành động của tác nhân bao gồm các lệnh cấp cao như pickup và drop được cấu tạo từ các hành động nguyên tử: "left", "right", "forward", "pick", và "drop" (xem Hình 1)

• **VirtualHome** (Puig et al. 2018) là một nền tảng tương tác để mô phỏng các hoạt động gia đình phức tạp thông qua tương tác với môi trường, như nhặt đồ vật, bật/tắt thiết bị. Chúng tôi sử dụng tập dữ liệu VirtualHome-Env (Liao et al. 2019), bao gồm các hoạt động gia đình hàng ngày từ 7 cảnh được thu thập thông qua crowdsourcing. Chúng tôi chỉ sử dụng mục tiêu làm đầu vào (xem Bảng 2).

| Môi trường | Ví dụ mục tiêu | Ví dụ quan sát ban đầu | Độ dài kế hoạch | \|A\| |
|------------|---------------|------------------------|----------------|-------|
| Ravens (Tower of Hanoi seq) | Move the gray disk in rod 2 | Blue disk on top of gray disk. Gray disk on top of green disk. Green disk in rod 1. The disks can be moved in rod 1, rod 2, rod 3. | 3.3 | 7.5 |
| Ravens (Put Blocks in Bowls) | Put the yellow blocks in gray bowls | There is a gray bowl 1, gray bowl 2, gray bowl 3, yellow block 1, yellow block 2, yellow block 3, blue bowl 1, red block 1, green bowl 1, orange block 1. | 6.1 | 25 |
| BabyAI (Pickup) | Pick up the ball | Room 1 has purple ball. Room 2 has yellow key, agent. Room 3 has red key. The door connecting Room 1 and Room 2 is locked. The door connecting Room 2 and Room 3 is locked. | 6.7 | 7.7 |
| VirtualHome | Read book | | 5.9 | 150 |

**Bảng 2:** Bảng hiển thị các nhiệm vụ từ mỗi môi trường, độ dài kế hoạch trung bình và kích thước không gian hành động trung bình |A|. Đối với VirtualHome, chúng tôi không chỉ định quan sát ban đầu vì khó mô tả môi trường phòng. Ở đây, không gian hành động thay đổi với các tập phim, phụ thuộc chẳng hạn vào số lượng đối tượng.

**Phân chia dữ liệu và đánh giá.** Chúng tôi nhằm đánh giá thành công, hiệu quả chi phí và khả năng tổng quát hóa ngoài phân phối (OOD) của các kế hoạch được tạo ra. Chúng tôi đã tạo ra ba phân chia dữ liệu cho mỗi môi trường bằng cách sử dụng quỹ đạo chuyên gia. (i) **phân chia train** để huấn luyện mô hình Can, Pay và prompting few-shot của mô hình Say; (ii) **phân chia test** đánh giá khả năng của các bộ lập kế hoạch LM trong việc tạo ra các kế hoạch thành công (tức là đạt được mục tiêu trong các bước hạn chế), và cũng đánh giá khả năng của các bộ lập kế hoạch trong việc tạo ra các kế hoạch hiệu quả chi phí (tức là các kế hoạch thành công và cũng có cùng độ dài kế hoạch như kế hoạch chuyên gia5). (iii) **phân chia test-generalize** tập trung vào khả năng tổng quát hóa như xử lý các quan sát ban đầu mới lạ (ví dụ: màu sắc chưa thấy của khối và bát, yếu tố gây xao nhãng trong BabyAI), độ dài chuỗi dài hơn (ví dụ: nhiều khối hoặc đĩa hơn trong Ravens, nhiều phòng hơn trong BabyAI), và các nhiệm vụ chưa thấy trong VirtualHome. Tất cả phân chia test có # tổng số tập phim = 100 trừ khi được chỉ định khác. Hơn nữa, tất cả các phân chia là rời rạc (tức là không chồng lấn).

**Baseline.** Ở cấp độ hành động, chúng tôi đánh giá các điểm giải mã của chúng tôi (Say, SayCan, SayCanPay) bằng cách sử dụng các chiến lược giải mã khác nhau (Greedy và Beam-Action). Do đó, các baseline của chúng tôi sử dụng hỗn hợp các chiến lược và điểm này. Đối với các token, chúng tôi sử dụng chiến lược giải mã Greedy-Token làm tham chiếu. Đáng chú ý, Greedy-Action SayCan là phiên bản lập kế hoạch ngoại tuyến của bài báo SayCan gốc (Ahn et al. 2022).

**Chi tiết huấn luyện và suy luận.** Chúng tôi sử dụng 800 quỹ đạo chuyên gia train cho mỗi nhiệm vụ Ravens và 400 cho BabyAI. Đối với VirtualHome, chúng tôi giữ lại ≈800 quỹ đạo tương thích cho trình mô phỏng hiện tại. Thêm 100 quỹ đạo chuyên gia được thu thập cho mỗi phân chia test (20 cho VirtualHome test-generalize). Các mô hình Can và Pay được huấn luyện trên 7 GPU NVIDIA-DGX V-100 sử dụng khung Distributed Data-Parallel qua 20 epoch. Các tham số huấn luyện bao gồm tốc độ học 1e-4, bộ tối ưu hóa AdamW với trọng số suy giảm 1e-5, kích thước lô 50, phân chia train-validation 80-20. Để suy luận, mô hình Say được tải bằng Model Parallel trên cùng các GPU. Các siêu tham số suy luận được liệt kê trong Bảng 6. Các tham số như beam groups và diversity penalty khuyến khích sự đa dạng giữa các beam, do đó tránh nhiều chuỗi tương tự. Chúng tôi sử dụng độ chính xác 8-bit để tải mô hình hiệu quả GPU (Dettmers et al. 2022).

| Thiết lập | Mô hình Say | Greedy-Token | Greedy-Action | Beam-Action |
|-----------|-------------|--------------|---------------|-------------|
|           |             |              | Say | SayCan | SayCanPay | Say | SayCan | SayCanPay |
| Ravens (tower of hanoi) | Vicuna | 45 | 48 | 48 | 50 | 54 | 68 | 70 |
|                         | Flan-T5 | 30 | 30 | 39 | 42 | 38 | 50 | 50 |
| Ravens (put blocks in bowls) | Vicuna | 30 | 51 | 52 | 54 | 52 | 52 | 56 |
|                              | Flan-T5 | 96 | 96 | 96 | 96 | 98 | 98 | 98 |
| BabyAI (pickup) | Vicuna | 59 | 62 | 81 | 88 | 72 | 94 | 94 |
|                 | Flan-T5 | 0 | 0 | 30 | 36 | 1 | 36 | 30 |
| VirtualHome | Vicuna | 0 | 32 | 49 | 52 | 48 | 52 | 53 |
|             | Flan-T5 | 0 | 0 | 30 | 48 | 30 | 41 | 50 |

**Bảng 3:** Bảng hiển thị thành công lập kế hoạch (tức là số kế hoạch trên 100 đạt được mục tiêu trong các bước hạn chế) trên phân chia test qua các môi trường khác nhau sử dụng mô hình Vicuna, Flan-T5. Có thể quan sát thấy rằng chiến lược giải mã tốt nhất là Beam-Action và điểm giải mã tốt nhất là SayCanPay.

| Thiết lập | Mô hình Say | Greedy-Token | Greedy-Action | Beam-Action |
|-----------|-------------|--------------|---------------|-------------|
|           |             |              | Say | SayCan | SayCanPay | Say | SayCan | SayCanPay |
| Ravens (tower of hanoi) | Vicuna | 12 | 24 | 55 | 58 | 20 | 47 | 52 |
|                         | Flan-T5 | 34 | 34 | 46 | 47 | 38 | 54 | 56 |
| Ravens (put blocks in bowls) | Vicuna | 16 | 36 | 40 | 48 | 38 | 42 | 56 |
|                              | Flan-T5 | 63 | 65 | 71 | 74 | 67 | 74 | 74 |
| BabyAI (pickup) | Vicuna | 48 | 50 | 53 | 54 | 56 | 56 | 62 |
|                 | Flan-T5 | 0 | 0 | 26 | 28 | 1 | 30 | 34 |
| VirtualHome | Vicuna | 0 | 14 | 23 | 29 | 20 | 26 | 30 |
|             | Flan-T5 | 0 | 0 | 6 | 15 | 4 | 19 | 26 |

**Bảng 4:** Bảng hiển thị hiệu quả chi phí (tức là số kế hoạch trên 100 đạt được mục tiêu trong các bước hạn chế và cũng có cùng độ dài kế hoạch như kế hoạch chuyên gia) trên phân chia test qua các môi trường khác nhau sử dụng mô hình Vicuna, Flan-T5. Có thể quan sát thấy rằng chiến lược giải mã tốt nhất là Beam-Action và điểm giải mã tốt nhất là SayCanPay.

| Thiết lập | Mô hình Say | Greedy-Token | Greedy-Action | Beam-Action |
|-----------|-------------|--------------|---------------|-------------|
|           |             |              | Say | SayCan | SayCanPay | Say | SayCan | SayCanPay |
| Ravens (tower of hanoi) | Vicuna | 32 | 30 | 18 | 18 | 27 | 34 | 34 |
|                         | Flan-T5 | 24 | 22 | 18 | 16 | 26 | 26 | 26 |
| Ravens (put blocks in bowls) | Vicuna | 8 | 30 | 10 | 6 | 30 | 10 | 6 |
|                              | Flan-T5 | 94 | 94 | 26 | 18 | 96 | 22 | 24 |
| BabyAI (pickup) | Vicuna | 0 | 1 | 4 | 12 | 9 | 12 | 10 |
|                 | Flan-T5 | 0 | 1 | 28 | 28 | 1 | 15 | 28 |
| VirtualHome | Vicuna | 0/20 | 2/20 | 3/20 | 3/20 | 5/20 | 5/20 | 5/20 |
|             | Flan-T5 | 0/20 | 0/20 | 0/20 | 3/20 | 1/20 | 3/20 | 5/20 |

**Bảng 5:** Bảng hiển thị kết quả tổng quát hóa (tức là số kế hoạch trên 100 đạt được mục tiêu) trên phân chia test-generalize qua các môi trường khác nhau sử dụng mô hình Vicuna và Flan-T5. Có thể quan sát thấy rằng Beam-Action vượt trội hơn các chiến lược giải mã khác.

5 Chúng tôi chia test thành hai phần của 100 mẫu để đánh giá thành công, hiệu quả chi phí. Đối với VirtualHome, chúng tôi sử dụng các kế hoạch được chú thích từ tập dữ liệu của nó.

### 7.3 Kết quả

Chúng tôi phân tích kết quả theo các trục sau: chiến lược giải mã, điểm giải mã và kiến trúc transformer. Chúng tôi đánh giá thành công lập kế hoạch và khả năng tổng quát hóa bằng cách thực hiện các kế hoạch được tạo ra trong các trình mô phỏng như Ravens và BabyAI, có kiểm tra xác thực tích hợp để xác định việc đạt được mục tiêu. Đối với môi trường VirtualHome mở hơn, chúng tôi xem xét thủ công các kế hoạch được thực hiện đầy đủ để đảm bảo chúng đáp ứng các mục tiêu nhiệm vụ dự định. Để đánh giá hiệu quả chi phí, chúng tôi có được quỹ đạo chuyên gia cho mỗi mẫu test bằng cách sử dụng bộ lập kế hoạch oracle.

**So sánh điểm giải mã.** Từ Bảng 3, 4, hiệu suất qua các điểm giải mã khác nhau có thể được tóm tắt như Say < SayCan ≤ SayCanPay. (i) **thành công lập kế hoạch**: Các điểm SayCanPay và SayCan dẫn đến hiệu suất tương đương, thường vượt trội hơn Say. Lợi thế hiệu suất nhỏ của mô hình Pay có thể do tập trung vào việc chọn hành động dựa trên mức độ liên quan dài hạn, có thể tránh các trạng thái không thể đảo ngược (làm vỡ trứng) hoặc thậm chí hấp thụ (điện thoại di động hết pin) từ đó không thể đạt được mục tiêu (tức là lập kế hoạch không ergodic). (ii) **hiệu quả chi phí**: SayCanPay dẫn đến cải thiện đáng kể so với cả Say (≈11−97% cho Beam-Action) và SayCan (≈0−33% cho Beam-Action và ≈1−150% cho Greedy-Action). (iii) **tổng quát hóa**: Từ Bảng 5, trong khi hiệu suất tổng thể cho SayCan và SayCanPay cải thiện so với Say, một sự sụt giảm đáng chú ý trong hiệu suất được quan sát cho Ravens. Điều này dẫn đến giả thuyết rằng các mô hình miền đã học (Can, Pay) không tổng quát hóa tốt với dữ liệu OOD trong một số môi trường (xem § 7.5 để biết các giải pháp tiềm năng).

**So sánh chiến lược giải mã.** Từ Bảng 3, 4, 5, hiệu suất tổng thể qua các chiến lược giải mã theo mẫu: Greedy-Token < Greedy-Action < Beam-Action qua tất cả các phân chia. Các phiên bản Beam-Action Say, SayCan và SayCanPay cho thấy cải thiện so với các đối tác Greedy-Action tương ứng. (i) **thành công lập kế hoạch**: Beam-Action SayCanPay thắng Greedy-Action SayCanPay ≈1−40%. Những cải thiện tương tự cũng được quan sát với các điểm giải mã khác. (ii) **hiệu quả chi phí**: Beam-Action SayCanPay cải thiện so với Greedy-Action SayCanPay ≈0−73%. (iii) **tổng quát hóa**: Beam-Action SayCanPay thắng Greedy-Action SayCanPay ≈0−89%.

**So sánh kiến trúc Transformer.** Chúng tôi không quan sát được cải thiện hiệu suất nhất quán cho bất kỳ kiến trúc cụ thể nào, gợi ý rằng cả hai đều thích hợp cho lập kế hoạch. Chúng tôi thiếu giải thích rõ ràng và cần nghiên cứu thêm để hiểu cách mỗi thành phần LM tác động đến suy luận.

### 7.4 Chi tiết phân tích loại bỏ

• **Tác động của kích thước beam k**: Như được thấy trong Hình 3, nói chung, cả thành công kế hoạch và hiệu quả chi phí đều tăng với việc tăng kích thước beam với k=1 (Greedy-Action), 2, 3 (Beam-Action). Tất cả các thí nghiệm đã sử dụng điểm giải mã SayCanPay. Tuy nhiên, không có mẫu rõ ràng nào được quan sát cho kết quả tổng quát hóa.

• **Tác động của mô hình Say**: Thất bại lập kế hoạch có thể phát sinh vì mô hình Say thất bại trong việc đề xuất hành động đúng trong số các ứng viên, làm cho Can và Pay không hiệu quả. Chúng tôi nghiên cứu tác động của mô hình Say đối với hiệu suất tổng thể bằng cách sử dụng Perfect Say luôn đề xuất hành động chính xác cùng với các yếu tố gây xao nhãng ngẫu nhiên. Từ Bảng 7, chúng tôi quan sát được cải thiện 16-84% trong hiệu suất SayCan và SayCanPay qua các môi trường khác nhau, cho thấy tiềm năng của mô hình Say được cải thiện. Do đó, việc sử dụng mô hình lớn hơn được huấn luyện trên nhiều dữ liệu hơn có thể nâng cao hiệu suất.

• **So sánh độ dài kế hoạch**: Chúng tôi tính toán độ dài tương đối = độ dài kế hoạch oracle / độ dài kế hoạch được tạo ra, so sánh độ dài kế hoạch được tạo ra và oracle. Giá trị = 1 chỉ ra độ dài bằng nhau và giá trị = 0 rằng độ dài kế hoạch là vô hạn (tức là kế hoạch không thành công). Như được hiển thị trong Hình 4, Beam-Action cải thiện nhẹ so với Greedy-Action. Hơn nữa, điểm SayCanPay đạt được độ dài tương đối tốt nhất (≈1) cho cả chiến lược Greedy và Beam-Action biểu thị hiệu quả chi phí của các kế hoạch được tạo ra.

• **Tác động của kích thước vấn đề đối với thời gian lập kế hoạch.** **Tác động của không gian hành động**: Thời gian lập kế hoạch không bị ảnh hưởng vì mô hình Say tạo ra thay vì phân biệt giữa các hành động. **Tác động của độ dài kế hoạch**: Thời gian chạy Greedy-Token tăng ∼2s cho mỗi bước hành động. **Tác động của chiến lược giải mã**: ∼9s cho Greedy-Token, ∼17s cho Greedy-Action, ∼35s cho Beam-Action. **Tác động của điểm giải mã**: Thời gian lập kế hoạch không bị ảnh hưởng vì Can và Pay là các LM nhỏ với chi phí phụ không đáng kể. Các kỹ thuật lượng tử hóa và phần cứng tiên tiến có thể giảm thêm thời gian chạy và là một lĩnh vực nghiên cứu tích cực (Dettmers et al. 2023; Frantar et al. 2023).

• **Phân tích định tính**: Mô hình Can hiệu quả lựa chọn các hành động khả thi (Hình 1). Mô hình Pay ưu tiên các hành động dẫn đến đạt được mục tiêu nhanh hơn. Trong khi Pay cho xác suất cao cho hành động "done task" liên kết nó với việc hoàn thành kế hoạch, điểm Can phủ định nó do điều kiện tiên quyết "done task" không được thỏa mãn.

| Tham số | Giá trị | Ngoại lệ |
|---------|---------|----------|
| max new tokens | 10 | 11 Vicuna (Ravens-Blocks), 3 (VirtualHome) |
| beam groups | 3 | 4 cho Flan-T5 (BabyAI) |
| diversity penalty | 2.0 | |
| candidates (m) | 6 | 8 cho Flan-T5 (Baby-AI) |
| beam-size (k) | 3 | |

**Bảng 6:** Siêu tham số suy luận. Ở đây các tham số candidates (m) và beam-size (k) là trên các hành động. Phần còn lại của các tham số beam search là trên các token.

### 7.5 Hạn chế và nghiên cứu tương lai

| Điểm | LM | Perfect |
|------|----|---------| 
| Ravens-Hanoi | SayCan | 48 | 88 |
|               | SayCanPay | 50 | 92 |
| Ravens-Blocks | SayCan | 52 | 70 |
|               | SayCanPay | 54 | 75 |
| BabyAI | SayCan | 81 | 90 |
|        | SayCanPay | 88 | 92 |
| VirtualHome | SayCan | 49 | 60 |
|             | SayCanPay | 52 | 64 |

**Bảng 7:** Bảng mô tả tác động của mô hình Say đối với hiệu suất thành công lập kế hoạch. Trong bối cảnh này, cả "LM" và "Perfect" đều đại diện cho các mô hình Say. "LM" tương ứng với mô hình Vicuna, trong khi "Perfect Say" là mô hình Say oracle luôn đề xuất hành động chính xác cùng với hai hành động gây xao nhãng khác như các ứng viên tiếp theo. Đối với tất cả các thí nghiệm, chúng tôi đã sử dụng chiến lược giải mã Greedy-Action.

Các hạn chế chính là (i) nhu cầu về quỹ đạo chuyên gia để huấn luyện các mô hình miền, và (ii) khả năng thích ứng hạn chế của các mô hình miền với dữ liệu OOD. Những thách thức này là bản chất của các mô hình học sâu. Tuy nhiên, những tiến bộ gần đây trong LLM mang lại những giải pháp đầy hứa hẹn. Ví dụ, các nghiên cứu mới hơn đã tận dụng LLM để thiết kế phần thưởng do khả năng suy ra ý định từ prompts tối thiểu (Kwon et al. 2023). Đáng chú ý, LLM vượt trội hơn các đối tác nhỏ hơn như Bert trong tổng quát hóa. Vì cả điểm Can và Pay đều giống với phần thưởng, các nghiên cứu tương lai có thể sử dụng LLM để giảm thiểu huấn luyện và cải thiện tổng quát hóa. Một hướng tiềm năng khác có thể là thí nghiệm với các phương pháp ký hiệu và heuristic không tham số như so sánh kế hoạch được tạo ra hiện tại với các quỹ đạo thành công/chuyên gia trong bộ đệm.

## 8 Kết luận

Chúng tôi đã đề xuất kết hợp kiến thức thế giới và khả năng tạo sinh của LLM với tính hệ thống của lập kế hoạch cổ điển bằng cách hình thức hóa khung lập kế hoạch dựa trên tìm kiếm heuristic cho LLM. Chúng tôi đã chứng minh cách tạo ra các kế hoạch vừa khả thi vừa hiệu quả về chi phí. Trong khi LLM vẫn chưa thể tạo ra các kế hoạch dài hạn ngang bằng với các bộ lập kế hoạch cổ điển, phương pháp của chúng tôi khắc phục các vấn đề vốn có trong lập kế hoạch dựa trên LLM và mở rộng lập kế hoạch truyền thống với những ưu điểm của các mô hình ngôn ngữ, đánh dấu tiến bộ đáng kể cho nghiên cứu lập kế hoạch với LLM.

## Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi Chương trình Wallenberg AI, Autonomous Systems and Software Program (WASP) được tài trợ bởi Quỹ Knut and Alice Wallenberg, và cũng là một phần của dự án EU H2020 ICT48 "TAILOR" theo hợp đồng 952215, và Quỹ nghiên cứu KU Leuven (C14/18/062).

## Tài liệu tham khảo

[Phần tài liệu tham khảo giữ nguyên định dạng gốc với tất cả các trích dẫn]
