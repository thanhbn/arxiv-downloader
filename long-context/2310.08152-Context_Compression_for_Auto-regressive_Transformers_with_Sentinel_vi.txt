# 2310.08152.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.08152.pdf
# Kích thước tệp: 338338 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Nén Ngữ cảnh cho Transformer Tự hồi quy với Token
Sentinel
Siyu Ren Qi Jia
Shanghai Jiao Tong University, China
{roy0702, Jia_qi}@sjtu.edu.cnKenny Q. Zhu∗
University of Texas at Arlington, USA
kenny.zhu@uta.edu

Tóm tắt
Độ phức tạp bậc hai của mô-đun attention làm
cho nó dần trở thành phần lớn của tính toán
trong các LLM dựa trên Transformer trong quá
trình sinh. Hơn nữa, bộ nhớ đệm key-value quá
mức khi xử lý các đầu vào dài cũng gây ra vấn
đề nghiêm trọng về dung lượng bộ nhớ và độ
trễ suy luận. Trong công trình này, chúng tôi đề
xuất một phương pháp plug-and-play có thể nén
gia tăng các activation trung gian của một đoạn
token được chỉ định thành các token compact,
từ đó giảm cả chi phí bộ nhớ và tính toán khi
xử lý ngữ cảnh tiếp theo. Các thí nghiệm về
mô hình hóa ngôn ngữ trong domain và sinh
tài liệu mở zero-shot đều chứng minh ưu điểm
của phương pháp chúng tôi so với các baseline
attention thưa thớt về độ trôi chảy, khớp n-gram
và độ tương tự ngữ nghĩa. Cuối cùng, chúng tôi
đánh giá toàn diện lợi ích của nén ngữ cảnh trong
việc cải thiện throughput hệ thống. Mã nguồn
có sẵn tại https://github.com/DRSY/KV_
Compression .

1 Giới thiệu
Kiến trúc Transformer (Vaswani et al., 2017)
đã trở thành thành phần nền tảng của các mô
hình ngôn ngữ lớn hiện đại (LLM) (Radford et al.,
2019; Devlin et al., 2019; Zhang et al., 2022; Tou-
vron et al., 2023) trong những năm gần đây. Tuy
nhiên, độ phức tạp tính toán và dung lượng bộ nhớ
bậc hai của cơ chế attention đã hạn chế lớn việc
áp dụng Transformer cho các ngữ cảnh ngày càng
dài với tài nguyên tính toán hạn chế.

Để giảm thiểu những vấn đề này, các công trình
trước đây (Child et al., 2019; Beltagy et al., 2020;
Zaheer et al., 2020; Kitaev et al., 2020) đã khám
phá nhiều biến thể Transformer hiệu quả, chủ yếu
bằng cách thay thế phép toán attention bậc hai ban
đầu bằng các dạng xấp xỉ tuyến tính hóa khác nhau.
Mặc dù đầy hứa hẹn, việc pre-training quy mô lớn
và các kernel CUDA chuyên biệt thường được yêu
cầu để các mô hình này đạt được hiệu suất tương
đương với các LLM có sẵn và đạt được lợi ích
hiệu quả thực sự.

∗Tác giả liên hệ.

Trong công trình này, chúng tôi nhằm cải thiện
hiệu quả của các LLM dựa trên Transformer hiện
có mà không có bất kỳ thay đổi kiến trúc nào. Cụ
thể, chúng tôi tập trung vào bộ nhớ đệm key-value,
chiếm phần lớn dung lượng bộ nhớ và chi phí di
chuyển dữ liệu (I/O) khi xử lý đầu vào ngày càng
dài bằng LLM. Chúng tôi đề xuất một phương pháp
plug-and-play để nén gia tăng bộ nhớ key-value
của một đoạn token liền kề thành các token compact.

Cụ thể, chúng tôi giới thiệu một cặp token sentinel
đặc biệt <CL> và <CR> vào từ vựng của LLM và
sử dụng chúng để đánh dấu ranh giới của đoạn cần
nén. Trong quá trình huấn luyện, chúng tôi sửa
đổi mặt nạ attention nhân quả sao cho các token
tương lai sau <CR> không được phép attend đến
các token giữa <CL> và <CR>. Bằng cách tiếp tục
huấn luyện LLM với mục tiêu dự đoán token tiếp
theo, mô hình học cách trích xuất và cô đọng thông
tin liên quan đến nhiệm vụ của đoạn được giới hạn
vào token sentinel kết thúc. Độ dài ngữ cảnh giảm
giúp giảm bớt cả chi phí bộ nhớ và tính toán khi
xử lý các token tiếp theo, từ đó cải thiện throughput
hệ thống với kích thước batch lớn hơn và tốc độ
decoding nhanh hơn trong quá trình suy luận.

Chúng tôi tiến hành thí nghiệm trên benchmark
mô hình hóa ngôn ngữ WikiText-2 và cho thấy
phương pháp của chúng tôi có thể tổng quát hóa
cho các LLM với kích thước khác nhau (từ 1.3B
đến 3B) và các sơ đồ mã hóa vị trí như absolute
position embedding (Devlin et al., 2019) và rotary
position encoding (Su et al., 2021). So với các
baseline attention thưa thớt, phương pháp của
chúng tôi có thể nén hiệu quả bộ nhớ đệm key-
value lịch sử với sự suy giảm perplexity giảm
đáng kể. Hơn nữa, chúng tôi chứng minh rằng
phương pháp của chúng tôi vượt trội hơn attention
thưa thớt trong sinh tài liệu mở zero-shot qua các
tỷ lệ nén khác nhau được đánh giá bởi perplexity,
ROUGE (Lin, 2004), vàarXiv:2310.08152v2 [cs.CL] 15 Oct 2023

--- TRANG 2 ---
BERTScore (Zhang et al., 2019). Cuối cùng, chúng
tôi chứng minh thực nghiệm rằng nén ngữ cảnh có
thể mang lại cải thiện đáng kể trong throughput
hệ thống cho việc sinh văn bản.

2 Kiến thức nền và Công trình liên quan
Trong phần này, chúng tôi trình bày kiến thức nền
cần thiết cũng như phân biệt phương pháp của
chúng tôi với các tài liệu hiện có.

Độ phức tạp của Cơ chế Attention Cơ chế self-
attention của LLM tạo ra một vector query cho
mỗi token để chọn và truy xuất thông tin từ tất cả
các vector key và value đã tính toán trước đó. Do
đó, một bộ nhớ key-value được yêu cầu tại runtime
để lưu trữ thông tin quá khứ. Cho một LLM đã
được pre-train với M layer Transformer, H head
attention, chiều ẩn per-head là dhead, kích thước
batch b, và độ dài ngữ cảnh hiện tại L, mô hình
lưu trữ các vector key và value đã tính toán cho
đến nay dưới dạng tensor có shape (M, 2, b, H,
L, dhead). Trong quá trình decoding tự hồi quy,
kích thước bộ nhớ đệm key-value tăng tuyến tính
với độ dài ngữ cảnh L, dẫn đến tăng đáng kể về
dung lượng bộ nhớ và độ trễ.

Transformer Hiệu quả Nhiều biến thể hiệu quả
của Transformer đã được đề xuất để giải quyết
độ phức tạp có vấn đề của attention. Sparse
Transformer (Child et al., 2019) giới hạn trường
tiếp nhận của mỗi token trong một cửa sổ cục bộ.
Longformer (Beltagy et al., 2020) và BigBird
(Zaheer et al., 2020) giới thiệu thêm các token có
thể truy cập ngẫu nhiên và toàn cục để bù đắp cho
việc mất thông tin. Linear Transformer (Katharo-
poulos et al., 2020) tái công thức hóa self-attention
thành tích dot-product tuyến tính của các feature
map kernel và sử dụng tính chất kết hợp của tích
ma trận để giảm độ phức tạp từ O(N²) xuống O(N).

Tuy nhiên, những xấp xỉ này đã được chứng minh
là làm suy giảm khả năng biểu đạt của full-attention
và gần đây đã bị các LLM giống GPT vượt qua.
Trong công trình này, chúng tôi giải quyết vấn đề
này bằng cách nén đầu vào dài thành các biểu diễn
compact hơn, điều này trực giao với những nỗ lực
đang diễn ra để tối ưu hóa kiến trúc.

Token Gisting Mu et al. (2023) đã khám phá việc
sử dụng token gisting để nén các prompt văn bản
trong quá trình instruction tuning. Họ chỉ ra rằng
các hướng dẫn nhiệm vụ dài dòng có thể được nén
thành những cái ngắn hơn nhiều. Tuy nhiên, phương
pháp của họ chỉ áp dụng cho văn bản prefix khoảng
20 token. Ngược lại, chúng tôi nhằm mục đích

[Hình ảnh mô tả: Một sơ đồ minh họa việc sửa đổi attention mask cho nén ngữ cảnh. Các ô màu xanh lá và xám chỉ ra các vị trí được phép và không được phép attend (hướng attention từ hàng → cột).]

Hình 1: Attention mask đã sửa đổi cho nén ngữ cảnh.
Các ô màu xanh lá và xám chỉ ra các vị trí được phép
và không được phép attend (hướng attention từ hàng
→ cột).

một sơ đồ nén với các lựa chọn nén linh hoạt hơn
và tỷ lệ nén lớn hơn.

3 Phương pháp
Trong phần này, chúng tôi trình bày một phương
pháp plug-and-play có thể nén các biểu diễn ngữ
cảnh đầy đủ chiều dài thành những cái ngắn hơn
trong khi vẫn bảo toàn thông tin cần thiết để hoàn
thành nhiệm vụ cuối.

3.1 Nén Ngữ cảnh với Token Sentinel
Để giảm bớt tính toán và cường độ bộ nhớ của
bộ nhớ key-value khi xử lý ngữ cảnh ngày càng
dài, chúng tôi giới thiệu hai token sentinel <CL>
và <CR> vào từ vựng của LLM, được đặt xung
quanh một đoạn token liền kề mà bộ nhớ key-value
tương ứng sẽ được nén.

Để củng cố thông tin của các token được bao quanh
bởi <CL> và <CR>, chúng tôi thiết kế một attention
mask nhân quả đã sửa đổi để tạo điều kiện cho hành
vi này. Một ví dụ minh họa được hiển thị trong
Hình 1. Cụ thể, <CL> phục vụ như một ký hiệu
bắt đầu nén và chỉ có thể attend đến chính nó.
Các token bình thường (token ngoại trừ <CL> và
<CR>) có quyền truy cập vào tất cả các token <CR>
và token bình thường trước đó. Điều này đảm bảo
rằng các biểu diễn ngữ cảnh được xây dựng dựa
trên cả thông tin quá khứ đã nén (có mất mát) và
hoàn chỉnh (không mất mát). <CR> sau đó hoạt
động như một dạng lựa chọn và truy xuất thông
tin từ các biểu diễn ngữ cảnh của các token xung
quanh. Sơ đồ masking đã sửa đổi này, kết hợp với
fine-tuning cụ thể cho nhiệm vụ của LLM, khuyến
khích chưng cất thông tin liên quan đến nhiệm vụ
của các chuỗi token có thể dài thành một biểu diễn
compact, từ đó giảm kích thước bộ nhớ key-value
cần thiết cho việc xử lý tiếp theo.

--- TRANG 3 ---
Model MethodTỷ lệ Nén ( r)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
OPT-1.3BScattered Attention 15.0 18.2 22.6 28.2 35.8 47.2 59.6 81.0 106.4 151.6
Local Attention 15.0 15.1 15.2 15.7 16.3 17.2 18.2 19.9 23.0 29.8
KV Compression 15.0 15.0 15.2 15.6 15.9 17.3 17.8 18.0 18.1 18.3
OPT-2.7BScattered Attention 13.1 16.2 19.8 25.5 32.4 41.7 56.3 75.7 101.4 142.3
Local Attention 13.1 13.1 13.4 13.9 14.4 15.3 16.1 17.5 20.1 26.6
KV Compression 13.1 13.2 13.5 13.7 14.0 15.3 15.8 16.0 16.1 16.3
RedPajama-3BScattered Attention 11.2 13.1 16.5 20.7 26.6 34.9 47.2 65.7 91.6 135.9
Local Attention 11.2 11.3 11.5 11.8 12.2 12.9 13.6 14.8 17.2 22.9
KV Compression 11.2 11.4 11.5 11.8 11.9 12.3 13.5 13.7 13.8 14.0

Bảng 1: Perplexity (càng thấp càng tốt) của ba LLM trên benchmark mô hình hóa ngôn ngữ WikiText-2.

Biến đổi Đầu vào Cho một đầu vào x với L token,
chúng tôi định nghĩa tỷ lệ nén r là phần trăm các
token được bao quanh bởi một hoặc nhiều cặp
<CL> và <CR>, và độ dài nén tối đa l là số lượng
token lớn nhất được nén bởi một cặp token <CL>
và <CR>. Chúng tôi lặp lại việc lấy mẫu đoạn
token với độ dài tuân theo phân phối đều U(2, l)
làm mục tiêu nén, cho đến khi đạt tỷ lệ r. Nhưng
những đoạn này không được chồng lặp. Chúng tôi
ràng buộc position encoding của <CL> và <CR>
với token trước của chúng, sao cho chúng không
chiếm các vị trí có giá trị cho LLM sử dụng absolute
position embedding.

Huấn luyện Chúng tôi chọn một quy trình nhẹ để
điều chỉnh LLM cho các nhiệm vụ downstream
thông qua fine-tuning. Cụ thể, tất cả các tham số
của LLM được đóng băng ngoại trừ embedding
của <CL> và <CR> và các mô-đun LoRA (Hu
et al., 2021) được áp dụng cho tất cả các layer
attention. Điều này không chỉ làm dễ dàng quá
trình huấn luyện của các mô hình với hàng tỷ tham
số mà còn đảm bảo khả năng mô hình hóa ngôn
ngữ cố có và kiến thức tham số trong mạng feed-
forward được giữ nguyên (Geva et al., 2021).

Thao tác Bộ nhớ Key-Value cho Nén thời gian
Suy luận Cho một đoạn văn bản làm prefix, chúng
tôi áp dụng cùng chiến lược biến đổi đầu vào được
định nghĩa ở trên để đạt tỷ lệ nén được chỉ định.
Để thực hiện việc giảm bộ nhớ và tính toán có thể
cảm nhận được, bộ nhớ đệm key-value của các
token được bao quanh bởi token sentinel được
giải phóng khỏi GPU theo cách tuần tự (ví dụ,
sử dụng vòng lặp for qua các khối token) nếu prefix
ban đầu quá dài để vừa với GPU. Ngược lại, chúng
tôi chỉ đưa prefix đã biến đổi qua mô hình và giải
phóng bộ nhớ đệm key-value của các token đã
đánh dấu trong một lần.

MethodTỷ lệ Nén ( r)
0.7 0.8 0.9
Local Attention 23.1 25.2 29.5
KV Compression 21.8 22.0 22.5

Bảng 2: Perplexity (càng thấp càng tốt) của RedPajama-
3B trên tập test WritingPrompts.

4 Thí nghiệm
Chúng tôi chủ yếu đánh giá phương pháp của mình
trên nhiệm vụ mô hình hóa ngôn ngữ và sau đó
khám phá khả năng tổng quát hóa zero-shot của
nó trên sinh tài liệu mở. Cuối cùng, chúng tôi đo
lường định lượng ảnh hưởng của nén ngữ cảnh
lên throughput hệ thống.

4.1 Mô hình hóa Ngôn ngữ
Benchmark Chúng tôi chủ yếu tiến hành thí nghiệm
trên dataset Wikitext-2 (Merity et al., 2016) bao
gồm các bài viết Wikipedia, đã được sử dụng rộng
rãi để đánh giá mô hình hóa ngôn ngữ. Chúng tôi
báo cáo perplexity trung bình cấp token (loại trừ
<CL> và <CR> cho KV Compression) trên tập
test như chỉ số đánh giá. Chúng tôi cũng báo cáo
kết quả trên dataset WritingPrompts (Fan et al.,
2018) để điều tra tính tổng quát của phương pháp.

Mô hình Để chứng minh tính tổng quát của phương
pháp chúng tôi, được gọi là KV Compression,
chúng tôi sử dụng OPT-1.3B, OPT-2.7B (Zhang
et al., 2022), và RedPajama-3B (Computer, 2023)
làm ba LLM với kích thước khác nhau và phương
pháp mã hóa vị trí khác nhau.

Baseline Chúng tôi so sánh phương pháp của mình
với hai baseline attention thưa thớt: (1) Scattered
Attention lấy mẫu các vị trí cho phép attention từ
phân phối Bernoulli B(1-r); và (2) Local Attention
(Beltagy et al., 2020) giới hạn attention của token
xt trong phạm vi (⌊t∗r⌋, t). Cho KV Compression,
chúng tôi đặt l là 25 trong suốt thí nghiệm.

--- TRANG 4 ---
[Bốn biểu đồ hiển thị hiệu suất của RedPajama-3B trên sinh mở với 200 tài liệu C4 được lấy mẫu, đo chất lượng sinh theo độ trôi chảy (perplexity), khớp n-gram (ROUGE-L), và độ tương tự ngữ nghĩa (BERTScore). Cũng hiển thị throughput hệ thống dưới dạng số token được sinh mỗi giây với các giới hạn GPU VRAM khác nhau.]

Hình 2: RedPajama-3B trên sinh mở với 200 tài liệu C4 được lấy mẫu. Chất lượng sinh được đo bằng độ trôi chảy (perplexity), khớp n-gram (ROUGE-L), và độ tương tự ngữ nghĩa (BERTScore). Chúng tôi báo cáo throughput hệ thống là số token được sinh mỗi giây với GPU VRAM tối đa khác nhau.

Phương pháp đề xuất của chúng tôi, cùng với các
baseline attention thưa thớt mà chúng tôi đã chọn,
có chung một mục tiêu: nâng cao tỷ lệ tính toán
so với truy cập bộ nhớ bằng cách giảm bớt bộ nhớ
đệm key-value được lưu trữ. Để biết chi tiết thiết
lập huấn luyện, vui lòng tham khảo Phụ lục A.

Kết quả Kết quả trên WikiText-2 được hiển thị
trong Bảng 1. Khi tỷ lệ nén r tăng lên, cả ba
phương pháp đều dẫn đến tăng perplexity do: (1)
các token quan trọng nằm ngoài phạm vi attention,
hoặc (2) khả năng của token <CR> đơn lẻ không
đủ để bao quát toàn bộ thông tin của đoạn token
đã nén. Chúng tôi quan sát thấy Local Attention
vượt trội đáng kể so với Scattered Attention, cho
thấy tầm quan trọng của ngữ cảnh cục bộ cho mô
hình hóa ngôn ngữ. KV Compression đạt perplexity
tốt nhất qua nhiều tỷ lệ nén khác nhau. Đáng chú
ý, ở tỷ lệ nén cao, ví dụ r≥0.7, KV Compression
gây ra suy giảm perplexity ít hơn đáng kể so với
Local Attention, chứng minh ưu điểm của nó trong
việc nén thông tin cục bộ rải rác trong khi vẫn giữ
thông tin toàn cục mạch lạc. Trong Bảng 2, chúng
tôi báo cáo perplexity của RedPajama-3B trên
dataset WritingPrompts sử dụng Local Attention
và KV Compression đề xuất, với tỷ lệ nén từ
{0.7, 0.8, 0.9}. KV Compression đạt perplexity
thấp hơn một cách nhất quán so với Local Attention,
cho thấy tính ưu việt của nó như một phương pháp
có thể tổng quát hóa domain cho nén ngữ cảnh.

4.2 Sinh Mở Zero-shot
Dữ liệu Chúng tôi chọn ngẫu nhiên 200 tài liệu từ
tập validation C4 (Raffel et al., 2020) để đánh giá.
Chúng tôi sử dụng 128 từ đầu làm prefix và coi
64 từ tiếp theo làm tham chiếu ground-truth.

Mô hình Chúng tôi trực tiếp lấy mô hình RedPajama-
3B được huấn luyện trên Wikitext-2 để thực hiện
sinh tài liệu mở zero-shot. Cho một văn bản prefix
p, nucleus sampling (Holtzman et al., 2019) được
sử dụng để sinh hoàn thiện c cho nó.

Baseline Vì Scattered Attention hoạt động kém
theo Bảng 1, chúng tôi chỉ so sánh KV Compression
với Local Attention với tỷ lệ nén r từ 0.0 đến 0.9
được áp dụng cho prefix p sử dụng biến đổi đầu
vào được định nghĩa trong Phần 3 cho KV
Compression và attention bị giới hạn được định
nghĩa trong Phần 4. Để Local Attention đạt được
nén thời gian suy luận, nó tương đương với việc
duy trì một hàng đợi FIFO để lưu trữ bộ nhớ đệm
key-value: khi bước thời gian trong quá trình sinh
tăng lên, bộ nhớ key-value cũ trong hàng đợi được
loại bỏ và bộ nhớ key-value mới được sinh được
đẩy vào.

Chỉ số Đánh giá Chúng tôi đánh giá chất lượng
của các hoàn thiện được sinh từ ba khía cạnh: (1)
độ trôi chảy, (2) khớp n-gram với phần tiếp theo
ground-truth, và (3) độ tương tự ngữ nghĩa với
phần tiếp theo ground-truth. Độ trôi chảy được
đánh giá bằng perplexity được tính từ mô hình
Pythia-1B (Biderman et al., 2023) được pre-train
sử dụng C4. Khớp n-gram và độ tương tự ngữ
nghĩa được đo bằng ROUGE-L và BERTScore
(Zhang et al., 2019) tương ứng. Để giải quyết tính
ngẫu nhiên do nucleus sampling gây ra, đối với
mỗi prefix, chúng tôi sinh 8 hoàn thiện và báo cáo
kết quả trung bình.

Kết quả Hình 2 cho thấy rằng, khi r tăng, các
hoàn thiện được sinh của Local Attention có xu
hướng đi lệch khỏi chủ đề ban đầu, dẫn đến giảm
ROUGE-L/BERTScore và tăng perplexity. Một
lần nữa, KV Compression xuất sắc trong việc bảo
toàn thông tin liên quan và vẫn có thể sinh các
phần tiếp theo chất lượng tốt lên đến tỷ lệ nén 0.5.
Đáng chú ý, KV Compression có thể sinh văn bản
trôi chảy ngay cả khi prefix bị nén cực mạnh. Lý
do là, trong quá trình huấn luyện, Local Attention
nhận thông tin phai mờ nhanh chóng từ quá khứ
xa, làm cho cấu trúc diễn ngôn cho các sinh tiếp
theo trở nên không mạch lạc. Ngược lại, KV
Compression bảo toàn tốt hơn thông tin như vậy
bằng cách củng cố nó vào các token sentinel.

--- TRANG 5 ---
4.3 Lợi ích Throughput từ Nén Ngữ cảnh
Với KV Compression, bộ nhớ đệm key-value tương
ứng với các token được bao quanh bởi token sentinel
có thể được giải phóng khỏi bộ nhớ. Theo cách
này, nó cho phép kích thước batch lớn hơn và cải
thiện throughput hệ thống để đổi lại. Để đo lường
định lượng tác động của nén ngữ cảnh lên throughput,
chúng tôi tiến hành thí nghiệm trên sinh mở bằng
cách kiểm tra với GPU VRAM tối đa có sẵn khác
nhau. Độ dài đầy đủ của prefix đầu vào giả được
đặt là 800 và chúng tôi sử dụng RedPajama-3B
với nucleus sampling để sinh phần tiếp theo cho
nó. Throughput của hệ thống sinh văn bản được
định nghĩa là số token được sinh mỗi giây. Kết
quả được hiển thị ở góc dưới bên phải của Hình 2.
Chúng ta có thể thấy rằng, ở tỷ lệ nén cực mạnh
(ví dụ, r≥0.8), nén ngữ cảnh mang lại cải thiện
throughput hơn 1.5x với 12GB GPU VRAM và
cải thiện 1.3x nhỏ hơn một chút với 24GB GPU
VRAM. Ở tỷ lệ nén vừa phải (ví dụ, r≈0.5), KV
compression vẫn có thể mang lại cải thiện throughput
1.2x-1.4x trong khi chỉ chịu suy giảm chất lượng
nhẹ (Phần 4.2). Tương quan memory-compression
ratio trực quan hơn được hoãn lại đến Phụ lục D.

5 Kết luận
Trong công trình này, chúng tôi đề xuất một phương
pháp đơn giản nhưng hiệu quả cho phép LLM tóm
tắt bộ nhớ key-value của đoạn token được chỉ
định. Các thí nghiệm về mô hình hóa ngôn ngữ
và sinh mở cho thấy phương pháp của chúng tôi
vượt trội đáng kể so với các baseline attention
thưa thớt về nén thông tin và chất lượng sinh.

Hạn chế
Trong thiết lập đánh giá hiện tại, chúng tôi áp dụng
cùng chiến lược được sử dụng để huấn luyện để
chọn các đoạn token được bao quanh bởi <CL>
và <CR> cho nén ngữ cảnh thời gian suy luận.
Tuy nhiên, các đoạn văn bản khác nhau có thể
hiển thị mức độ quan trọng khác nhau cho các
nhiệm vụ downstream. Ví dụ, một cụm danh từ
hoàn chỉnh về ngữ pháp và ngữ nghĩa có thể nén
được hơn một cụm không ngữ pháp chỉ chứa các
đơn vị ngôn ngữ từng phần. Mặc dù quy trình biến
đổi đầu vào của chúng tôi về mặt lý thuyết bao
gồm các đoạn văn bản của tất cả các cấu trúc ngôn
ngữ có thể, nó vẫn có thể hưởng lợi từ một chiến
lược/thuật toán được thiết kế tỉ mỉ để chọn mục
tiêu nén trong một ngữ cảnh nhất định.

Tài liệu tham khảo
Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer.

Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mo-
hammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, Aviya Skowron, Lintang
Sutawika, and Oskar van der Wal. 2023. Pythia:
A suite for analyzing large language models across
training and scaling.

Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. 2019. Generating long sequences with
sparse transformers. CoRR , abs/1904.10509.

Together Computer. 2023. Redpajama: An open source
recipe to reproduce llama training dataset.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.

Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.

Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021. Transformer feed-forward layers are key-
value memories. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 5484–5495, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.

Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin
Choi. 2019. The curious case of neural text degener-
ation. CoRR , abs/1904.09751.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. 2021. Lora: Low-rank adaptation of large
language models. CoRR , abs/2106.09685.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-
pas, and François Fleuret. 2020. Transformers are
rnns: Fast autoregressive transformers with linear
attention. In International Conference on Machine
Learning , pages 5156–5165. PMLR.

--- TRANG 6 ---
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. CoRR ,
abs/2001.04451.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.

Ilya Loshchilov and Frank Hutter. 2017. Fixing
weight decay regularization in adam. CoRR ,
abs/1711.05101.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.

Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.
Learning to compress prompts with gist tokens.
arXiv preprint arXiv:2304.08467 .

Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario
Amodei, and Ilya Sutskever. 2019. Language models
are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng
Liu. 2021. Roformer: Enhanced transformer with
rotary position embedding. CoRR , abs/2104.09864.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. 2020. Transform-
ers: State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.

Manzil Zaheer, Guru Guruganesh, Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontañón,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
and Amr Ahmed. 2020. Big bird: Transformers for
longer sequences. CoRR , abs/2007.14062.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2019. Bertscore:
Evaluating text generation with BERT. CoRR ,
abs/1904.09675.

--- TRANG 7 ---
A Chi tiết Triển khai
Mô hình hóa Ngôn ngữ Đối với các thí nghiệm
mô hình hóa ngôn ngữ, chúng tôi huấn luyện OPT-
1.3B, OPT-2.7B, và RedPajama-3B với mục tiêu
dự đoán token tiếp theo. Loss được tính trên tất
cả các token ngoại trừ các token <CL> và <CR>
mới được thêm vào. Đối với các token ngay trước
<CL> và <CR>, chúng tôi điều chỉnh nhãn ground-
truth của chúng thành các token ngay sau các token
<CL> và <CR> tương ứng.

Các tham số có thể huấn luyện cho tất cả LLM bao
gồm hai token embedding của <CL> và <CR>,
và các mô-đun LoRA được áp dụng cho tất cả các
layer attention. Hạng của các ma trận trọng số của
LoRA được đặt là 16. Phần trăm tham số có thể
huấn luyện chiếm khoảng 5% tổng số tham số của
LLM. Chúng tôi sử dụng optimizer AdamW
(Loshchilov and Hutter, 2017) với learning rate
2e-5. Kích thước batch được đặt là 12 và độ dài
chuỗi tối đa trong quá trình huấn luyện được đặt
là 256 do ngân sách tính toán hạn chế. Triển khai
dựa trên thư viện Huggingface transformers (Wolf
et al., 2020) và tất cả thí nghiệm được tiến hành
sử dụng một GPU RTX 3090.

Sinh Tài liệu Mở Đối với sinh tài liệu mở, chúng
tôi sử dụng nucleus sampling với top p = 0.9. Do
tính ngẫu nhiên được gây ra bởi sampling, chúng
tôi thực hiện 8 lần nucleus sampling cho mỗi prefix
đã nén và báo cáo các chỉ số đánh giá trung bình
được sử dụng trong bài báo chính. Điều này giúp
giảm phương sai và đảm bảo kết quả đánh giá đáng
tin cậy hơn.

B Khả năng Tổng quát hóa của KV
Compression

[Biểu đồ hiển thị hiệu suất tổng quát hóa của các tỷ lệ nén thời gian huấn luyện khác nhau.]

Hình 3: Hiệu suất tổng quát hóa của các tỷ lệ nén thời
gian huấn luyện khác nhau.

Trong KV Compression đề xuất của chúng tôi, một
LLM được huấn luyện trên dữ liệu đã biến đổi trong
đó phần trăm token được bao quanh bởi token
sentinel chiếm một tỷ lệ được chỉ định của tổng
số token. Ở đây chúng tôi nghiên cứu hiệu suất
tổng quát hóa của các tỷ lệ nén thời gian huấn
luyện khác nhau và khám phá thực hành tốt nhất.
Chúng tôi trực quan hóa perplexity của RedPajama-
3B được huấn luyện với các tỷ lệ nén khác nhau
ở các tỷ lệ nén thời gian test khác nhau.

Hình 3 minh họa kết quả. Chúng ta có thể thấy
rằng huấn luyện với tỷ lệ nén cao hơn luôn dẫn
đến khả năng tổng quát hóa tốt hơn qua các tỷ lệ
nén thời gian test khác nhau. Lý do là, ở tỷ lệ nén
nhỏ, các mô hình ngôn ngữ đôi khi có thể khai
thác ngữ cảnh cục bộ hạn chế và vẫn có thể dự
đoán token tiếp theo một cách đáng tin cậy. Trong
trường hợp này, các token sentinel không được
bảo đảm có được khả năng nén ngữ cảnh mong
muốn. Khi phần lớn ngữ cảnh không được phép
attend, các token sentinel buộc phải nuôi dưỡng
đủ khả năng nén để tối thiểu hóa hàm loss.

C Sử dụng Bộ nhớ với Tỷ lệ Nén Khác
nhau

[Biểu đồ hiển thị peak GPU VRAM được sử dụng với các tỷ lệ nén khác nhau.]

Hình 4: Peak cached memory được profiled sử dụng
Pytorch khi sử dụng RedPajama-3B để tạo ra một phần
tiếp theo 100-token cho các prefix có độ dài thay đổi.

Chúng tôi đã chỉ ra rằng nén ngữ cảnh mang lại
cải thiện đáng kể trong throughput hệ thống, đặc
biệt cho các GPU kích thước vừa phải. Ở đây
chúng tôi báo cáo việc sử dụng bộ nhớ chi tiết
giả định một kịch bản thực tế tương tự như đối
thoại nhiều lượt: độ dài prefix (độ dài của đối
thoại lịch sử) tăng dần, và mô hình được yêu cầu
xuất ra một phản hồi khoảng 100 token. Để tối
đa hóa throughput của dịch vụ đối thoại, chúng
tôi giả định mô hình đồng thời sinh phản hồi cho
nhiều instance, tức là kích thước batch lớn hơn một.

Việc sử dụng bộ nhớ được trực quan hóa được
hiển thị trong Hình 4. Nén ngữ cảnh có thể giảm
hơn

--- TRANG 8 ---
[Ba biểu đồ hiển thị kết quả sinh mở của OPT-1B trên 200 tài liệu C4 được lấy mẫu, đo chất lượng sinh theo độ trôi chảy (perplexity), khớp n-gram (ROUGE-L), và độ tương tự ngữ nghĩa (BERTScore).]

Hình 5: OPT-1B trên sinh mở với 200 tài liệu C4 được lấy mẫu. Chất lượng sinh được đo bằng độ trôi chảy (perplexity), khớp n-gram (ROUGE-L), và độ tương tự ngữ nghĩa (BERTScore).

3GB peak cached GPU memory. Trong thực tế,
điều này dịch ra khả năng sinh nhiều token hơn
cho một instance duy nhất và cho phép nhiều
instance song song hơn.

D Kết quả Sinh Mở của OPT
Hình 5 tóm tắt kết quả của OPT-1B trên sinh tài
liệu mở. So với RedPajama-3B (Hình 2), chất
lượng sinh của OPT-1B thấp hơn đáng kể. So sánh
các phương pháp nén ngữ cảnh khác nhau, KV
Compression hoạt động tốt hơn đồng đều qua cả
ba chỉ số đánh giá.
