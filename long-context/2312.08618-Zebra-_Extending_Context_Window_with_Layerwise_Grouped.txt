# 2312.08618.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2312.08618.pdf
# File size: 1413994 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Zebra: Extending Context Window with Layerwise Grouped
Local-Global Attention
Kaiqiang Song∗, Xiaoyang Wang∗, Sangwoo Cho∗, Xiaoman Pan, Dong Yu
Tencent AI Lab, Seattle
{riversong, shawnxywang, swcho, xiaomanpan, dyu}@global.tencent.com
Abstract
This paper introduces a novel approach to en-
hance the capabilities of Large Language Mod-
els (LLMs) in processing and understanding
extensive text sequences, a critical aspect in
applications requiring deep comprehension and
synthesis of large volumes of information. Rec-
ognizing the inherent challenges in extending
the context window for LLMs, primarily built
on Transformer architecture, we propose a new
model architecture, referred to as Zebra . This
architecture efficiently manages the quadratic
time and memory complexity issues associated
with full attention in the Transformer by em-
ploying grouped local-global attention layers.
Our model, akin to a zebra’s alternating stripes,
balances local and global attention layers, sig-
nificantly reducing computational requirements
and memory consumption. Comprehensive ex-
periments, including pretraining from scratch,
continuation of long context adaptation train-
ing, and long instruction tuning, are conducted
to evaluate the Zebra ’s performance. The re-
sults show that Zebra achieves comparable or
superior performance on both short and long
sequence benchmarks, while also enhancing
training and inference efficiency.
1 Introduction
To effectively leverage the power of Long Context
in Large Language Models (LLMs), it is essen-
tial to develop and refine techniques that enable
these models to process and interpret extensive text
sequences accurately. This capability is particu-
larly significant in applications that demand deep
understanding and synthesis of large volumes of
information, such as summarization (Huang et al.,
2021; Hu et al., 2023; Song et al., 2022; Kry ´sci´nski
et al., 2021), reading comprehension (Nguyen et al.,
2016; Fan et al., 2019; Zhong et al., 2021; Yang
et al., 2023), long-form generation (Guan et al.,
2021; Deng et al., 2022; Roziere et al., 2023), and
*Equal Contributioncomplex reasoning (Wei et al., 2022; Yao et al.,
2023; Chen et al., 2023a).
However, it is challenging to extend the con-
text window from different viewpoints: First, the
predominant LLM model uses Transformer archi-
tecture (Vaswani et al., 2017). Such models like
BERT (Devlin et al., 2018), GPT (OpenAI, 2023),
and T5 (Raffel et al., 2020) employ full attention
in each layer which inherently incurs quadratic
time and memory complexity. This may potentially
diminish the efficiency of both the training and in-
ference processes. Second, attention computation
over an extremely long sequence might lead to an
almost even distribution, potentially causing the
omission of vital information (Han et al., 2023).
This may further lead to the issue of being “lost in
the middle” (Liu et al., 2023). Finally, the distribu-
tion of training signals for long and short sequences
is imbalanced. It is evident that longer sequences
are infrequent in both plain text and instruction-
tuning data. Consequently, this rarity poses a chal-
lenge in effectively capturing long-term dependen-
cies during the training process.
To tackle the above issues, we propose to group
local-global attention layers into blocks during the
training and inference phases. This strategy en-
hances efficiency while yielding results comparable
to those of a global attention Transformer. Notably,
it attains equivalent performance levels with merely
half the computational effort required for training.
Additionally, this approach significantly reduces
memory consumption during inference by main-
taining a local Key-Value (K-V) cache specifically
for the local attention layers.
In Section 2.1, we list the two critical com-
ponents essential for a long-context model as
well as the potential alternatives for consideration.
These encompass diverse attention mechanisms
and methodologies for positional embedding. Sub-
sequently, in Section 2.2, we conduct a comparative
analysis of these alternatives, presenting their em-arXiv:2312.08618v1  [cs.CL]  14 Dec 2023

--- PAGE 2 ---
pirical outcomes for a comprehensive evaluation.
Integrating these insights, we name our model Ze-
bra, drawing an analogy to the alternating black
and white stripes of a zebra, which resemble the
grouped local and global layers in our model’s ar-
chitecture.
To validate the proposed model at large scales,
Section 3 details the continuation of training the
Llama-2-7B model (Touvron et al., 2023) using
long-context adaptation training through Zebra .
This approach not only exhibits comparable per-
formance on short-sequence benchmarks but also
achieves superior perplexity results for longer se-
quences. Additionally, in Section 4, we con-
duct fine-tuning of Zebra using a combination
of both short and long instruction-tuning datasets.
This is followed by a systematic evaluation of the
model’s performance across a range of benchmark
datasets. It demonstrates generally better perfor-
mance on both long and short benchmarks com-
pared to Llama-2-7b-chat. To Conclude, our con-
tribution is 3-fold:
•We develop a novel architecture, referred to
asZebra , which incorporates grouped local-
global attention layers and rotary positional
embedding.
•We conduct comprehensive experiments and
detailed analyses of the Zebra framework
across various settings, including pretraining
from scratch, continuation of training, and ex-
tensive instruction tuning. The findings from
these results demonstrate the advantage of Ze-
bramodel architecture.
•Additionally, we analyze the training and in-
ference efficiency for Zebra and provide the
pseudocode for implementation.
2 Zebra
2.1 Model Architecture Design
To extend the context window for Transformer
models, two critical elements must be addressed:
First, the Attention mechanism that allows the
model to efficiently focus on and process relevant
parts of long sequences. However, it is important
to note that the computational of attention escalates
quadratically, leading to a decrease in efficiency
as the length of the sequence increases. Conse-
quently, addressing this computational challenge is
essential for maintaining effectiveness over longersequences. Second, the Positional Embedding that
imparts a structured signal indicative of the sequen-
tial order of tokens. It is vital to employ a posi-
tional embedding that is not only robust but also
exhibits strong generalization capabilities, particu-
larly for processing long sequences. This ensures
the model’s effectiveness in maintaining sequence
integrity over longer spans of data.
2.1.1 Attention
In Figure (1a, 1b, 1c), we showcase three rep-
resentative variants of single attention layers in-
cluding global attention, local attention, and lo-
cal attention with global approximations. Addi-
tional sparse attention models like blockwise at-
tention(Qiu et al., 2019), dilated window atten-
tion (Beltagy et al., 2020), stride attention (Child
et al., 2019), Sinkhorn Attention (Tay et al., 2020a),
transient global attention (Guo et al., 2021) are
considered potential alternatives for basic local at-
tention. For the sake of clarity and focus in our
research, we confine our analysis to two primary
variants: local attention and local attention with
global approximations. This decision allows for a
more targeted exploration of these specific atten-
tion mechanisms within our work. Moreover, we
also consider using different strategies among dif-
ferent layers. In Figure 1d, we combine several
local layers with one global attention layer as a
group and stack such groups for the model.
Considering one head of the self-attention layer
in a decoder-only transformer, the query, key, and
value of i-th position and l-th layer are defined as
projections of the last layer hidden states h(l−1)
i :
q(l)
i=WT
qh(l−1)i(1)
k(l)
i=WT
kh(l−1)i(2)
v(l)
i=WT
vh(l−1)i(3)
We denote the similarity between i-th query and
j-th key as:
Sim(i, j) =exp(qT
ikj/√
D) (4)
where Dis a normalized factor usually equal to the
model dimension.
Global Attention : It is the most common attention,
where each token has attention to all the positions
before it and itself:
αi,j=Sim(i, j)Pi
t=0Sim(i, t)(5)

--- PAGE 3 ---
Keys: Queries:
1 2 3 4 5 6 7 8 910 11 121
2
3
4
5
6
7
8
9
10
11
12(a) Global Attention
Keys: Queries:
1 2 3 4 5 6 7 8 910 11 121
2
3
4
5
6
7
8
9
10
11
12W=3 (b) Local Attention
Keys: Queries:
1 2 3 4 5 6 7 8 910 11 121
2
3
4
5
6
7
8
9
10
11
12W=3 C=2 (c) Global-Approximation
 (d) Group Attention
Figure 1: Four different attention strategies to be compared in this work. (a) Global Attention, where each token has its attention
to all previous tokens and itself; (b) Local Attention, where each token only has the attention within its local window; (c) Local
Attention with Global Approximation is newly introduced in this work, where each token not only has attention to its local
window but also has an approximated attention from the remaining non-local chunks; (d) Group Attention is our introduced
layerwise grouped local-global attention strategy, where we group Llayers and apply the global attention at the first layer of
each group (the remaining layers use local attention).
where αi,jis the attention value of i-th query over
j-th key. The context vector is then defined as a
weighted sum of value vectors:
context i=iX
j=0αi,jvj (6)
Local Attention : Each query only considers the
key-value pairs within its local window.
αi,j=Sim(i, j)Pi
t=min (0,i−w)Sim(i, t)(7)
where wis the window size of local attention.
Local Attention w/ Global Approximation : In-
spired by transient global attention (Guo et al.,
2021), we approximate the global attention output
by combining multiple non-local tokens as a chunk,
and take the attention over local tokens and non-
local chunks. Each non-local chunk’s key-value
pairs are estimated using the following equations:
ˆkj=j∗c−1X
t=(j−1)∗ckt+ln(c) (8)
ˆ vj=j∗c−1X
t=(j−1)∗cvt+ln(c) (9)
where cis the chunk size, and ln(c)is a compensa-
tion term for each chunk.
Layerwise Grouped Local-Global Attention : In-
stead of using identical layers for the entire net-
work, we propose to use grouped local-global at-
tention layers. In figure 1d, we group every Llayer
and use only one global attention layer at the first
layer of each group. We apply local attention de-
scribed in Equation (7) for the remaining layers.
h(l)=
G-Block (h(l−1))lmod L== 0
L-Block (h(l−1)) otherwise(10)
To simplify, we use Group Attention to denote the
layerwise grouped local-global attention.2.1.2 Positional Embedding
In the Transformer architecture, positional embed-
dings are commonly used to encode the sequence
order information. In this study, we incorporate
three widely recognized types of positional embed-
dings to facilitate a comprehensive analysis.
Absolute Positional Embedding : The vanilla
Transformer (Vaswani et al., 2017) advocates to
use an absolute sinusoidal positional embedding:
PE(pos,2i) =sin(pos/100002i/d))
PE(pos,2i+ 1) = cos(pos/100002i/d)
where posis the position index, dis the model di-
mension, and iis the iterative variable for different
dimensions. After the work of the vanilla Trans-
former, a trainable absolute positional embedding
has been introduced (Devlin et al., 2018; Radford
et al., 2018), serving as a replacement for the fixed
sinusoidal pattern. Such positional embedding is
directly added to the semantic embedding:
EMB(word, pos ) =WE(word ) +PE(pos) (11)
where word is the input token index, and posis the
absolute position index.
Most recently, the relative positional embed-
dings (Shaw et al., 2018; Yang et al., 2019) are
introduced to eliminate the positional bias while
improving the performance. These approaches also
facilitate the model’s ability to extend its contex-
tual window, a process known as position extrapo-
lation. Within this framework, two principal types
of relative positional embeddings are taken into
consideration.
Alibi Positional Embedding (Press et al., 2022),
which applies the relative positional embedding by
directly adding a bias term to the attention matrix.
αi,j=Softmaxi
j(Sim(i, j)−(i−j)∗m) (12)

--- PAGE 4 ---
where mis a head-specific scalar and (i−j)is the
relative distance between query and key positions.
By canceling out the −i∗mterm, we have
αi,j=Softmaxi
j(Sim(i, j) +j∗m) (13)
Rotary Positional Embedding (Su et al., 2023)
rotates the conjugate dimensions of query and key
vectors, such that the relative distance is encoded
during the attention calculation.
eq=(WT
qhi)ei(iθ)(14)
ek=(WT
khi)ei(iθ)(15)
where idenotes the imaginary unit, and iis the
positional index. For each pair of conjugate dimen-
sions, the similarity between query and key can be
written as:
Sim(i, j) =RE[(WT
qhi)T(WT
khj)ei(i−j)θ] (16)
where REtakes the real value of the complex num-
ber. The overall similarity is consequently defined
as the cumulative measure of similarities across all
corresponding dimensions.
2.2 Experiments for Model Design
Model Size 117M 345M
Num Layers 12 24
Hidden Size 768 1024
Num Heads 12 16
K-V Channel 64 64
FF Layer Hidden Size 3072 4096
Table 1: Parameters of two models with different sizes.
We conduct experiments with various attention
strategies and positional embedding methods as de-
scribed earlier. Two GPT models with 117M and
345M parameters as detailed in Table 1, are trained
from scratch to assess different model architectures.
The training sequence length used for these exper-
iments ranges from 1024 ,4096 , to16384 . A 10%
of the training data from the Pile dataset (Gao et al.,
2020) is utilized for model training. Its testing
and validation data is used in experiments of this
section for evaluation. We employ an Adam Opti-
mizer (Kingma and Ba, 2014) for training with the
beta values of 0.9and0.99. The training process
spans 20,000steps with a batch size of 2M tokens.
The initial learning rate is set to 1e−3with a warm-
up step of 2,000, followed by linear decay to 1e−5.
Weight decay is set to 0.01, and the gradient clip-
ping is set to 1.0. For the local attention, a window
size of w= 1,024is applied. For local attentionwith global approximation, we employ a chunk size
ofc= 16 . We group every three layers for local
and global layers. For rotary embedding (Su et al.,
2023), the RoPE theta is configured as 131,072to
enhance its generalization performance on longer
sequences. All experiments are implemented using
Megatron-LM1(Shoeybi et al., 2019; Narayanan
et al., 2021; Korthikanti et al., 2023).
2.2.1 Attention
Figure 2 shows the testing perplexity (PPL) differ-
ence between each attention strategy and the global
attention method on the 117M model. From the
figures, we have a few observations: First, global
attention has the best overall performance; Sec-
ond, the performance gap between group attention
and global attention is small but stable when the
training sequence length is getting longer; Third,
as the training sequence length grows, the perfor-
mance of local attention and global approximation
attention drops a lot for longer sequences, though
it may benefit the shorter ones. As group attention
has less computation but achieves a similar perfor-
mance compared to global attention, it has a high
scalability potential.
To better compare the global and group attention
strategies, we take both performance and compu-
tation into consideration. In Figure 3, we draw
the curve of the estimated TFLOPS and the vali-
dation PPL on three different training lengths with
DeepSpeed FLOPS profiler2for the 117M model.
We observe that group attention achieves a simi-
lar performance with less computation than global
attention. When the local window is equal to the
training sequence length (i.e., 1k training length
in Figure 3), the gain is negligible. However, as
the training sequence gets longer (e.g., 4k or 16k
training length in Figure 3), the gain becomes mag-
nified. This verifies that group attention has better
scalability compared to global attention.
2.2.2 Positional Embedding
Table 2 shows the perplexity results comparing dif-
ferent positional embeddings with the 117M and
345M models. We find that no significant perfor-
mance differences are observed among the three
positional embeddings for sequence lengths within
the training sequence 16,384. This result is in line
with (Taylor et al., 2022; Kazemnejad et al., 2023)
1https://github.com/NVIDIA/Megatron-LM
2https://www.deepspeed.ai/tutorials/
flops-profiler/

--- PAGE 5 ---
Figure 2: The testing PPL gap between each method and the baseline system (global attention) on 1024, 4096, and 16384
training sequence length. The smaller the better. In this experiment, we split the entire testing set into different splits according
to their length. Each split contains the instances within the length range ofx
2+ 1tox, except the first one (length ≤128).
Figure 3: The validation PPL vs TFLOPs for global attention(red) and group attention(blue) on 1024, 4096, and 16384 training
sequence lengths.
observation. While the absolute positional em-
bedding encounters challenges in extrapolating to
longer sequences, both Alibi and Rotary positional
embeddings demonstrate similar capabilities for
sequences exceeding the training sequence length
of 16,384. It is important to note that, in our ex-
periments, the Alibi positional embedding requires
full precision (fp32) computation to prevent posi-
tion collision. Consequently, we opt for the Rotary
positional embedding in the Zebra model.
2.2.3 Training Sequence length
Figure 4: Perplexity on test sequences with 1k, 4k, and 16k
training sequence lengths. In this experiment, we split the
entire testing set into different splits according to their length.
Each split contains the instances within the length range of
x
2+ 1tox, except the first one (length ≤128).We experiment with training sequence lengths
of 1024, 4096, and 16384 with a 117M model.
The corresponding validation perplexity with the
three training sequence lengths is 14.29,12.98, and
12.76, respectively. In Figure 4, we observe train-
ing with longer sequence length generally performs
better than those training with shorter sequence
length, especially on longer test splits. Meanwhile,
the perplexity of the model with longer training
length drops a little on short test splits. Interest-
ingly, as we only train with 16k length, the perplex-
ity is still going down on the 32k test split. The
results suggest that training with a longer sequence
length helps with performance.
2.3 Conclusion on Model Architecture
Based on the experiments and analysis above, we
decide to apply Rotary positional embedding and
group attention strategy for extending LLM’s con-
text window. The model is denoted as Zebra due
to the analogous arrangement of grouped local and
global attention layers, resembling the black and
white color stripes on a zebra.
3 Long Context Adaptation Training
In this section, we expand our examination of the
Zebra architecture by applying it to a larger-scale
model, utilizing Long Context Adaptation Train-
ing (LCAT). LCAT is essential for handling large

--- PAGE 6 ---
Pos. Emb. PPL on Pile Testset with Different Lengths
Min Len 1 129 257 513 1025 2049 4097 8193 16385 32789 65537
Max Len 128 256 512 1024 2048 4096 8192 16384 32768 65536 131072117MAbsolute 24.62 20.34 17.00 17.06 16.42 11.84 10.02 8.84 - - -
Alibi 24.54 20.29 17.01 17.05 16.41 11.87 10.08 8.93 7.60 8.83 18.47
Rotary 24.70 20.37 17.03 17.05 16.41 11.84 10.06 8.92 7.62 8.86 18.51
∆PPL (Max - Min) 0.16 0.08 0.03 0.01 0.01 0.03 0.06 0.09 0.02 0.03 0.04345MAbsolute 19.41 16.12 13.57 13.61 13.19 9.86 8.37 7.46 - - -
Alibi 19.27 16.02 13.51 13.55 13.11 9.78 8.36 7.44 6.44 7.38 14.84
Rotary 19.25 16.01 13.48 13.51 13.08 9.74 8.32 7.42 6.43 7.37 14.77
∆PPL (Max - Min) 0.16 0.11 0.09 0.10 0.11 0.12 0.05 0.04 0.01 0.01 0.07
Table 2: Perplexity with different positional embeddings and model sizes on the Pile test set. The minimum value of each model
is indicated in bold. All the systems are trained with 16,384sequence length. In this experiment, we split the entire testing set
into different splits according to their length. Each split contains the instances within the length range of its minimum length to
its maximum length. ∆PPL is the gap between the best-performing system and the worst-performing system of each test split.
contexts. This modification enables the model to
adapt to an expanded context through Zebra ar-
chitecture, while also providing extended signals
during training. This approach allows the model to
effectively learn and utilize long-range dependen-
cies. Training such a model from scratch requires
a lot of computing power, so we start with an al-
ready developed model called LLAMA -2 (Touvron
et al., 2023) as our base. From there, we train the
Zebra model with different volumes of data. All
our experiments are conducted with the 7B-sized
model.
3.1 Training Details
Zebra layers are organized into groups, each con-
sisting of four layers ( L= 4). The first layer within
each group functions as the global layer, while the
remaining three layers serve as local layers with an
attention local window size of 512. Each batch con-
tains 1,572,864tokens. As shown in Table 5, Ze-
bramodels are trained with 24,576or32,768se-
quence lengths with different data sizes where mul-
tiple documents are packed with a BOS token and
anEOS token. The Adam Optimizer (Kingma and
Ba, 2014) is utilized, with beta values of (0.9,0.95)
and an Adam epsilon of 1e−5. Training incorpo-
rates a cosine learning schedule with a linear warm-
up of 100 steps. The maximum learning rate is
1e−4, and the minimum learning rate is 1e−5. Gra-
dient clipping is applied with a 1.0threshold and
weight decay is set to 0.1.
3.2 Data Recipes
Figure 5 presents an analysis of the distribution
imbalance in the Pile dataset (Gao et al., 2020), par-
ticularly highlighting the variations due to different
sequence lengths. A notable observation is that
doubling the sequence length, when measured in
bytes, results in a reduction of the dataset’s volume
Figure 5: Sequence Length vs. Number of Instances on
The Pile Dataset.
to merely one-fourth of its initial size. In order
to sufficiently capture longer text segments in our
model, we have strategically integrated the LCAT
data with sources that typically contain longer data,
such as books. All data is extracted from diverse
sources within the Pile dataset, followed by recom-
bination with various domains. Table 3 illustrates
the detailed mapping of the original data source
to the domain split. The data within each domain
split is shuffled, and the specified data quantities,
as outlined in Table 3, are selected.
3.3 Evaluation Results
The long context adaptation training is essential
forZebra to refine the initial parameters from the
LLAMA -2 model. Due to the architectural dis-
parities between the LLAMA model and the Ze-
bramodel, our objective is to achieve compara-
ble performance with L LAMA -2 on both the short-
context tasks and the long-context tasks after train-
ing with long sequences. We first evaluate the
pre-trained models in Table 4 on Gutenberg (PG-
19) (Rae et al., 2019) by computing the perplex-

--- PAGE 7 ---
Domain Data SourceData Version
v0 v1 v2
Book BookCorpus2, Books3 100GB 50GB 100GB
WebPage Pile-CC, OpenWebText2, HackerNews 0 50GB 100GB
KnowledgeUSPTO Backgrounds, PubMed Abstracts0 50GB 100GBWikipedia(en), NIH ExPorter
Q&A Stack Exchange 0 20GB 30GB
Code Github 0 20GB 20GB
Translation EuroParl 0 10GB 10GB
Total - 100GB 200GB 360GB
# Token Used - 15.7B 50.3B 100.7B
Table 3: Data source for each domain and various combinations of data size employed for LCAT training.
Model ArchitectureTraining RoPE Parameters PPL with Different Test Length
Len. Data Scale theta 4096 8192 16384 24576 32768 Average
LLAMA 2 - - 1 10,000 7.57 126.53 1008.50 2,037.35 2814.72 1198.93
Llama2-PI - -4 10,000 15.27 15.31 18.35 64.12 178.97 58.40
4 40,000 48.71 56.96 71.75 82.38 101.17 72.19
16 160,000 195.06 202.61 212.50 220.79 240.52 214.30
16 160,000 239.38 255.36 339.49 427.84 532.97 359.01
LLAMA 2-LCAT 32k v1 16 160,000 7.13 6.85 6.75 6.63 6.55 6.78
Zebra -LCAT24k v0 16 160,000 9.02 8.80 8.50 8.52 8.41 8.65
24k v1 16 160,000 7.32 7.06 6.92 6.85 6.91 7.02
32k v1 16 160,000 7.36 6.99 6.84 6.73 6.75 6.93
32k v2 16 160,000 7.14 6.90 6.71 6.62 6.57 6.79
Table 4: PPL on Gutenberg (PG-19). The data for this evaluation is from the Pile training split but excluded in LCAT training.
ity across various lengths. Notably, Zebra -LCAT
trained with an extended context length of 32k and
larger data (v2) exhibits the most favorable perplex-
ity results, comparable to the performance of the
LLAMA -2-LCAT model.
Additionally, we assess the performance of our
models on a set of common pretraining bench-
marks to ensure robust performance across standard
short-context tasks (Touvron et al., 2023; Xiong
et al., 2023). As shown in Table 5, the LLAMA -
2-LCAT 32k model continually trained from the
original LLAMA -2 model using the 50B-token v1
data in Table 3 results in a slight degradation in
performance compared to LLAMA -2. This can
be attributed to the impact of long context adapta-
tion training, as observed in previous work (Chen
et al., 2023b), which results in degradation on short-
context benchmarks. Given that global attention
typically yields superior performance compared
to local attention (Rae and Razavi, 2020; Beltagy
et al., 2020; Sun et al., 2023), the LLAMA -2-LCAT
performance indicates potential performance upper
limit of models with local attention. Comparing
LLAMA -2-LCAT to Zebra -LCAT trained with the
same amount of data (50B), the results demonstrate
similarities except for MMLU. We speculate the
performance decrease in MMLU is originated from
the architectural change of the model, potentially
resulting in the forgetting of partial knowledge inModel Tks MMLU CS OQA Avg
LLAMA 2 2T 45.3 63.9 48.9 52.7
LLAMA 2 LONG 2T+400B 47.8 64.9 51.0 54.6
LLAMA 2-LCAT 2T+50B 44.4 61.4 45.6 50.5
Zebra-LCAT 2T+15B 32.6 59.4 41.0 44.3
Zebra-LCAT 2T+50B 39.1 61.2 46.3 48.9
Zebra-LCAT 2T+100B 41.8 61.3 46.0 49.7
Table 5: Perfomance of 7B models on short-context bench-
marks. The scores in the first two rows are from (Xiong et al.,
2023). Commonsense (CS) score as the average of PIQA (Bisk
et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al.,
2019), WinoGrande (Sakaguchi et al., 2019), ARC easy and
challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al.,
2018) and CommonsenseQA (Talmor et al., 2019); OpenQA
(OQA) score as the average of 5-shot performance on Natu-
ralQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi
et al., 2017).
LLAMA -2. Finally, as illustrated in Table 5, train-
ingZebra models with varying token quantities
indicates that more tokens contribute to narrowing
the performance gaps relative to the LLAMA -2-
LCAT model.
3.4 Conclusion on LCAT
Our model exhibits comparable performance to the
full attention model ( LLAMA -2-LCAT) in both per-
plexity and downstream tasks. This equivalence is
achieved while ensuring faster training and higher
GPU throughput. Based on the experimental find-
ings, we choose Zebra -LCAT trained with a 32k
context window and 100B tokens for subsequent

--- PAGE 8 ---
instruction tuning (a.k.a. supervised fine-tuning) in
long context window, as outlined in Section 4.
4 Long Instruction Tuning
The Long Context Adaptation Training (LCAT), as
discussed in Section 3, facilitates the adaptation of
theZebra model architecture from its LLAMA -2
foundation model architecture. This adaptation in-
volves the incorporation of grouped local-global
layers and position interpolation by training the
model with the pre-training learning objectives
in long context. Furthermore, we hypothesize
that LCAT contributes to enhancing the model by
incorporating additional knowledge and capabili-
ties that can only be acquired through training in
longer context windows, as suggested by prior re-
search (Xiong et al., 2023).
To align the Zebra -LCAT model for compre-
hensive open-domain language understanding and
generation tasks based on its acquired knowledge,
we conduct supervised fine-tuning on the Zebra -
LCAT model with Long Instruction Tuning (LIT).
The objective is to empower the model to profi-
ciently handle both short and long-context tasks,
adhering to the specifications outlined in human
instructions.
4.1 Instruction Tuning Data Recipe
To align the model for both short and long-context
tasks, our instruction tuning (IT) data comprises
instances of both types. Table 6 shows the statistics
on the average token number per instance. The
LLAMA tokenizer is utilized for tokenizing the in-
stances to calculate the token numbers.
# of Instances Avg. Tokens # / Instance
Short IT Data 344,818 208.3
Long IT Data 108,963 6113.5
Overall 453,781 1626.3
Table 6: Statistics of our instruction tuning (IT) data for
Zebra LIT. Our short instruction tuning data (Short IT Data)
contains more instances than our long instruction tuning data
(Long IT Data) but Long IT Data has a significantly larger
average token number per instance.
4.1.1 Short Instruction Tuning Data
Our short instruction tuning data primarily incorpo-
rates publicly available English instruction tuning
datasets, including LIMA (Zhou et al., 2023), Al-
paca (Taori et al., 2023), and ShareGPT (Tey). Ad-
ditionally, we introduce samples from hh-rlhf (Gan-
guli et al., 2022) with “selected” responses for
multi-turn dialogue. To customize the profile of ourZebra assistant, our dataset further encompasses
carefully curated short instances.
4.1.2 Long Instruction Tuning Data
For long instruction tuning, we address two cases:
First case, where the user provides a lengthy doc-
ument and instructs the system to process the sub-
stantial information and complete the specified task
succinctly; Second case, where the input is rel-
atively short, but the desired output needs to be
more extensive. The former case encompass tasks
of summarization, question-answering (QA), and
machine reading comprehension (MRC), while the
latter involves writing tasks. In writing tasks, the
user provides key information, major characters, or
an outline, tasking the AI assistant with composing
a natural language document, such as a letter or a
chapter in a novel.
To systematically empower LLMs for long tasks
in both scenarios, we carefully curate high-quality
instruction tuning data for three specific tasks: sum-
marization, long-MRC, and writing.
Summarization : We select 3,000 news reports
from CNN / Daily Mail (See et al., 2017) and 2,000
multi-turn long dialogue transcripts from Media-
Sum (Zhu et al., 2021) as documents to summarize.
Given an document, we randomly choose one out
of ten predefined prompts to instruct GPT-4 (Ope-
nAI, 2023) to generate a summary. Our long in-
struction tuning data for summarization hence con-
sists of 5,000instances with a document and the
randomly selected summarization prompt as input,
and the GPT-4 generated summary as output.
Long-MRC : We create synthetic long-MRC data
utilizing the long documents from Guttenberg PG-
19 corpus (Rae et al., 2019). Given a long docu-
ment (e.g., a chapter or a chapter chunk) in the cor-
pus, we first divide the long document into text seg-
ments in approximately even length. For each text
segment, we prompt GPT-4 to write one question-
answer pair grounding on the information from the
text segment. Each long-MRC instruction tuning
instance then consists of the long document and the
generated question as input x, and the generated
answer as output y. We collect in total of 1,245
such instances.
Writing : We further utilize texts from Gutten-
berg PG-19 corpus to generate the data for writ-
ing. Given a document (e.g., a chapter or a chapter
chunk) in the corpus, we prompt ChatGPT (Ope-
nAI, 2022) to extract its key elements includ-
ing “central idea”, “outline”, “keywords”, “time”,

--- PAGE 9 ---
“place”, “key figures”, “cause”, “process”, and “re-
sult”. We use predefined rules to randomly select a
subset of these elements, dynamically fill them into
the instruction template, and hence use the com-
pleted instruction containing selected key elements
as our input. The original document is then the cor-
responding output. We collect 328 such instances.
Besides our curated data for tasks of summa-
rization, long-MRC, and writing, we further incor-
porate 102k training instances randomly selected
from public datasets including BigPatent (Sharma
et al., 2019), GovReport (Huang et al., 2021),
GSM8k (Cobbe et al., 2021), CUAD (Hendrycks
et al., 2021), MultiDoc2Dial (Feng et al., 2021),
Multi-News (Fabbri et al., 2019), Natural Ques-
tion (Kwiatkowski et al., 2019), Musique (Trivedi
et al., 2022), NarrativeQA (Ko ˇciský et al., 2018),
Qasper (Dasigi et al., 2021), QMSum (Zhong et al.,
2021), QuALITY (Pang et al., 2022), SPACE (An-
gelidis et al., 2021), SQuALITY (Wang et al.,
2022), SummScreen (Chen et al., 2022), and
TOEFL-QA (Tseng et al., 2016; Chung et al., 2018).
These datasets cover traditional long tasks like sum-
marization, QA, and multi-choice QA.
4.2 LIT Training Details
The Zebra -LIT training uses the Zebra -LCAT
model as backbone and thus inherits Zebra -
LCAT’s model structure including the grouped at-
tention setting. Zebra -LIT is trained with ls=
16,384 sequence length. We pack training in-
stances with less than lstokens into the ls-token
sequences. The padding tokens are added to the
right. For long instruction instances with more
thanlstokens, we truncate only its input docu-
ment but keep all the instruction text. The batch
size is 2,097,152tokens per batch. We use Adam
Optimizer with beta values of (0.9,0.95)and an
Adam epsilon of 1e−8. Training incorporates a co-
sine learning schedule with a linear warm-up of
32 steps. The maximum learning rate is 2e−5, and
the minimum learning rate is 6e−6. We conduct
4-epoch training with our instruction tuning data.
In LIT, we calculate the loss on the output tokens
yonly. The rest hyper-parameters are the same as
LCAT training.
4.3 Evaluation Results
We assess the performance of our instruction-tuned
model on short and long-context benchmarks, as
presented in Table 7 and 8, respectively. Fine-
tuning models on long instruction data revealssome degradation in performance on short-context
benchmarks, such as MMLU, as depicted in Ta-
ble 7. However, the model consistently outper-
forms the LLAMA -2-chat model overall. Addi-
tionally, we evaluate the model on long-context
benchmarks (Shaham et al., 2023). The results,
as shown in Table 8, indicate that our model per-
forms comparably or surpasses LLAMA 2-chat in
QA datasets but falls short in datasets employing
Rouge (Lin, 2004) evaluation metrics. It is notewor-
thy that the long-context evaluation of LLMs poses
a non-trivial challenge, as highlighted in (Xiong
et al., 2023). Automatic metrics, such as Rouge,
employed in benchmarks, only consider n-gram
overlaps with a reference, which may not consis-
tently align with human preferences. We anticipate
that incorporating more substantial and diverse fine-
tuning data can significantly contribute to improved
model performance. Overall, our instruction-tuned
model demonstrates better performance on both
short and long-context benchmarks, affirming the
effectiveness of the architecture.
5 Discussion
5.1 Scalability
Figure 6: Perplexity (PPL) on Pile validation set vs number of
training tokens for different model sizes and training sequence
lengths.
Due to resource constraints, our scalability ex-
periments are limited to models with 117M and
345M parameters. As illustrated in Figure 6, em-
ploying a larger context window enhances model
performance for the 117M model. However, a no-
table observation is the difficulty in differentiating
the performance curves when the sequence length
is increased from 4,096to16,384tokens. We at-
tribute this marginal difference to two primary fac-
tors. Firstly, the amount of training data, capped at

--- PAGE 10 ---
Model (7B) cot/gsm8k cot/math cot/bbh cot/mmlu human-eval-plus Avg.
LLAMA 2-chat 25.0 3.8 30.7 40.7 9.8 22.0
Zebra-LIT 30.4 2.2 33.6 38.8 15.2 24.0
Table 7: Performance of instruction tuned 7B models on short-context benchmarks. In this evaluation, we follow the setting of
FastEval (tju01), which focuses mostly on the zero-shot setting.
Model (7B)Summarization QA
GR SS QM SQAL Qspr Nrtv QALT MuSQ Avg.
LLAMA 2-chat 11.1 12.7 14.2 19.0 14.8 11.0 38.1 6.7 15.9
Zebra-LIT 13.6 5.3 13.6 17.3 21.4 22.6 33.3 12.0 17.4
Table 8: Performance of instruction tuned 7B models on long-context benchmarks, validation set on ZeroScrolls.
20billion tokens, is likely insufficient to manifest
a significant difference. Secondly, both the train-
ing and validation datasets predominantly comprise
shorter sequences, thereby inherently constraining
the potential improvement in PPL. Despite these
limitations, we anticipate that the benefits of the
increased model size will become more apparent
with larger and more varied datasets. Evidently,
as the model size increases from 117M to 345M,
the performance gains significantly, notably from
12.87to10.34.
5.2 Efficiency
MethodOComplexity
Attention Other Total
Global DN2
D2NDN2+D2N
Local DWN D(D+W)N
GAD
C2N2+DWND
C2N2+D(D+W)N
GroupD
LN2+DWND
LN2+D(D+W)N
Table 9: Complexities of attention and other operations.
Training Efficiency : In Table 9, we present a de-
tailed comparison of computational complexities
for different attention operations, as well as other
relevant operations, across the four distinct atten-
tion strategies outlined in Section 2.1.1. This com-
parison indicates that with an increase in sequence
length, the computational demand of global atten-
tion exhibits a quadratic growth pattern. On the
other hand, both the Global Approximation and
Group Attention strategies, though also following
a quadratic growth trajectory, do so with signif-
icantly lower coefficients. Notably, Local Atten-
tion demonstrates the best computational efficiency,
requiring the least amount of resources as the se-
quence length extends.
Inference Efficiency : Because of the implemen-
tation of local attention in the majority of Trans-
former layers, Zebra does not require retaining all
Key-Value (K-V) pairs in the cache. Consequently,
this approach significantly reduces the GPU mem-
ory requirements during inference, thereby poten-
tially increasing the inference throughput.The detailed pseudo-code for Zebra training and
inference can be found in Appendix A.1.
6 Related Work
Attention . The Transformer architecture has a
self-attention component with O(N2)computation
complexity. Numerous studies have been proposed
to enhance the time and memory efficiency of
Transformer models. One approach involves lever-
aging sparse attention patterns, enabling the trans-
formation of full quadratic attention computations
toO(NlogN)or linear complexity. Our work
falls within this method by grouping sparse and full
attention patterns. Methods such as Sinkhorn (Tay
et al., 2020b), Longformer (Beltagy et al., 2020),
ETC (Ainslie et al., 2020), and BigBird (Zaheer
et al., 2020) have been introduced to incorporate
both sparse and full attention mechanisms. Another
set of approaches involves utilizing the low-rank
approximation of the attention matrix. This in-
cludes methods such as Linformer (Wang et al.,
2020), Performer (Choromanski et al., 2022), and
Random Feature Attention (Peng et al., 2021).
Positional Embedding . In Transformer models,
positional embeddings can be primarily catego-
rized into two types: absolute and relative. Ear-
lier versions of Transformers utilize absolute po-
sitional encoding. For instance, the vanilla Trans-
former (Vaswani et al., 2017) model adds sinu-
soidal positional embeddings to the word embed-
dings, whereas GPT (Radford et al., 2018) and
BERT (Devlin et al., 2018) introduce learnable po-
sitional embeddings. Currently, it has become more
common to use relative positional embedding. For
instance, Transformer-XL (Dai et al., 2019) and
T5 (Raffel et al., 2020) adds learnable attention
logit bias into attention layers. Alibi (Press et al.,
2022) biases attention scores based on the distance
between key and query elements. RoPE (Su et al.,
2023) multiplies the keys and queries of every at-
tention layer by sinusoidal embeddings. The Alibi

--- PAGE 11 ---
and RoPE methods are further improved through
the incorporation of an additional bias term (Sun
et al., 2022; Chi et al., 2023).
LLM . In the early stages, open-source large lan-
guage models such as OPT (Zhang et al., 2022),
BLOOM (Workshop et al., 2023), and Llama-
1 (Touvron et al., 2023) have a context window
length limited to 2048 tokens. Later, a smaller
open-source model Starcoder (Li et al., 2023), with
15B parameters, manage to extend its context win-
dow length to 8000 by optimizing computational
efficiency with Multi-Query Attention (Shazeer,
2019) and FlashAttention (Dao et al., 2022). Fol-
lowing this, LLAMA -2 (Touvron et al., 2023), an-
other open-source model, expands its default length
to 4096 tokens. The open-source community then
discover that by interpolating Rotary Positional Em-
beddings3, the context window length could be fur-
ther extended to 8192 tokens. Subsequently, (Chen
et al., 2023b) expand and validate this approach,
known as Position Interpolation, to further enhance
the window length capability. LLAMA -2 under-
goes extended training with long-context contin-
ual pertaining, extending up to 32,768 tokens, the
positional interpolation method, and FlashAtten-
tion (Dao et al., 2022) showing its enhanced effec-
tiveness (Xiong et al., 2023). Similar approaches
are employed to extend the context length (Peng
et al., 2023; Du et al., 2022; Dacheng Li* and
Zhang, 2023) by fine-tuning pretrained models
with long documents. LongLoRA (Chen et al.,
2023c) adopts a fine-tuning approach with shifted
local attention for more efficient training to fur-
ther extend context length. As of December 2023,
closed-source large language models have signifi-
cantly expanded their context window capabilities,
reaching scales of up to 100,000 tokens. For in-
stance, GPT-4-Turbo4supports a context window
of 128,000 tokens, while Claude-2.15supports up
to 200,000 tokens. The commercialization of these
closed-source models is heavily reliant on long con-
text understanding. For example, it allows users to
upload long text files for queries or engage in ex-
tended dialogues with extensive historical records.
Long Evaluation . The majority of benchmarks
for evaluating large language models are primarily
3https://www.reddit.com/r/LocalLLaMA/comments/
14fgjqj/a_simple_way_to_extending_context_to_8k
4https://platform.openai.com/docs/models/
gpt-4-and-gpt-4-turbo
5https://docs.anthropic.com/claude/reference/
selecting-a-modelfocused on tasks involving short context. How-
ever, the evaluation dimensions for long context
and short context may differ significantly. For ex-
ample, (Liu et al., 2023) develop a multi-document
question answering task, which demonstrates the
phenomenon of being lost in the middle , a chal-
lenge not present in short context. The foundation
for comprehensive evaluation of long context un-
derstanding is currently still underdeveloped. Re-
cently, there have been efforts to develop bench-
marks specifically for long context analysis, such
as (Shaham et al., 2023; Kwan et al., 2023; Dong
et al., 2023; Bai et al., 2023; An et al., 2023).
7 Conclusion
In this work, we introduce Zebra , a novel architec-
ture designed to enhance the capabilities of Large
Language Models (LLMs) in processing and inter-
preting long text sequences. Through the innova-
tive use of grouped local-global attention layers
and rotary positional embedding, Zebra addresses
critical challenges associated with extending the
context window in LLMs. Our extensive exper-
iments and analyses demonstrate that Zebra not
only maintains comparable performance on short-
sequence benchmarks but also excels in handling
longer sequences, as evidenced by its superior long
benchmark performances and perplexity results on
Gutenberg (PG-19). This indicates that our ap-
proach effectively balances the need for efficiency
with the demand for high performance in long-
context scenarios. The grouped local-global at-
tention mechanism, in particular, proves to be a
crucial component in achieving this balance, offer-
ing a significant reduction in computational and
memory requirements while maintaining, and in
some cases enhancing model performance. More-
over, the application of Zebra among diverse short
and long downstream tasks showcases its versatility
and robustness across various NLP tasks.
In conclusion, Zebra represents a significant
step forward in the realm of long-context language
processing. Its ability to efficiently handle exten-
sive text sequences without compromising on per-
formance opens up new possibilities for the ap-
plication of LLMs in a variety of complex and
information-rich environments. We believe that Ze-
brasets a new standard for long-context modeling
and will inspire further innovation in the field.

--- PAGE 12 ---
Limitations
While our work introduces a novel model architec-
ture and exhibits promising accuracy and efficiency,
it is not without limitations.
Due to computation resource constraints, we
have not yet evaluated the model architecture with
a parameter size larger than 7B. Though a larger
model typically brings stronger performance, it is
still valuable to further verify with Zebra .
Moreover, our current evaluation, especially for
long-context alignment tasks, largely relies on au-
tomatic metrics like Rouge and F-1 employed by
public benchmarks. Such metrics evaluating n-
gram overlapping with a reference have been under
debate before the era of LLMs. We anticipate a
comprehensive evaluation strategy for long-context
alignment to be proposed in the future.
References
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Va-
clav Cvicek, Zachary Fisher, Philip Pham, Anirudh
Ravula, Sumit Sanghai, Qifan Wang, and Li Yang.
2020. ETC: Encoding long and structured inputs in
transformers. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 268–284, Online. Association
for Computational Linguistics.
Chenxin An, Shansan Gong, Ming Zhong, Mukai Li,
Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.
L-eval: Instituting standardized evaluation for long
context language models.
Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko
Suhara, Xiaolan Wang, and Mirella Lapata. 2021.
Extractive opinion summarization in quantized trans-
former spaces. Transactions of the Association for
Computational Linguistics , 9:277–293.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
A bilingual, multitask benchmark for long context
understanding. arXiv preprint arXiv:2308.14508 .
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
Gao, and Yejin Choi. 2020. Piqa: Reasoning about
physical commonsense in natural language. In Thirty-
Fourth AAAI Conference on Artificial Intelligence .
Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song,
Xiaoyang Wang, Dong Yu, and Jianshu Chen. 2023a.
Skills-in-context prompting: Unlocking composi-
tionality in large language models. arXiv preprint
arXiv:2308.00304 .Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin
Gimpel. 2022. SummScreen: A dataset for abstrac-
tive screenplay summarization. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
8602–8615, Dublin, Ireland. Association for Compu-
tational Linguistics.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023b. Extending context window
of large language models via positional interpolation.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Zhijian Liu, Song Han, and Jiaya Jia. 2023c. Lon-
glora: Efficient fine-tuning of long-context large lan-
guage models.
Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and
Peter Ramadge. 2023. Dissecting transformer length
extrapolation via the lens of receptive field analysis.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 13522–13537, Toronto, Canada.
Association for Computational Linguistics.
Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509 .
Krzysztof Choromanski, Valerii Likhosherstov, David
Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
Lukasz Kaiser, David Belanger, Lucy Colwell, and
Adrian Weller. 2022. Rethinking attention with per-
formers.
Yu-An Chung, Hung-Yi Lee, and James Glass. 2018.
Supervised and unsupervised transfer learning for
question answering. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers) , pages
1585–1594, New Orleans, Louisiana. Association for
Computational Linguistics.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question
answering? try arc, the ai2 reasoning challenge.
arXiv:1803.05457v1 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .
Anze Xie Ying Sheng Lianmin Zheng Joseph E. Gonza-
lez Ion Stoica Xuezhe Ma Dacheng Li*, Rulin Shao*
and Hao Zhang. 2023. How long can open-source
llms truly promise on context length?

--- PAGE 13 ---
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-XL: Attentive language models beyond
a fixed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 2978–2988, Florence, Italy. Asso-
ciation for Computational Linguistics.
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,
and Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,
Noah A. Smith, and Matt Gardner. 2021. A dataset
of information-seeking questions and answers an-
chored in research papers. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 4599–4610, On-
line. Association for Computational Linguistics.
Yuntian Deng, V olodymyr Kuleshov, and Alexander M
Rush. 2022. Model criticism for long-form text gen-
eration. arXiv preprint arXiv:2210.08444 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao,
and Ji-Rong Wen. 2023. Bamboo: A comprehen-
sive benchmark for evaluating long text modeling
capacities of large language models. arXiv preprint
arXiv:2309.13345 .
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 320–335.
Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and
Dragomir Radev. 2019. Multi-news: A large-scale
multi-document summarization dataset and abstrac-
tive hierarchical model. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 1074–1084, Florence, Italy. Asso-
ciation for Computational Linguistics.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. Eli5:
Long form question answering. arXiv preprint
arXiv:1907.09190 .
Song Feng, Siva Sankalp Patel, Hui Wan, and Sachin-
dra Joshi. 2021. Multidoc2dial: Modeling dialogues
grounded in multiple documents. In Proceedings
of the 2021 Conference on Empirical Methods in
Natural Language Processing . Association for Com-
putational Linguistics.
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda
Askell, Yuntao Bai, Saurav Kadavath, Ben Mann,
Ethan Perez, Nicholas Schiefer, Kamal Ndousse,Andy Jones, Sam Bowman, Anna Chen, Tom Con-
erly, Nova DasSarma, Dawn Drain, Nelson Elhage,
Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds,
Tom Henighan, Danny Hernandez, Tristan Hume,
Josh Jacobson, Scott Johnston, Shauna Kravec,
Catherine Olsson, Sam Ringer, Eli Tran-Johnson,
Dario Amodei, Tom Brown, Nicholas Joseph, Sam
McCandlish, Chris Olah, Jared Kaplan, and Jack
Clark. 2022. Red teaming language models to re-
duce harms: Methods, scaling behaviors, and lessons
learned.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wen-
biao Ding, Xiaoxi Mao, Changjie Fan, and Minlie
Huang. 2021. Openmeva: A benchmark for eval-
uating open-ended story generation metrics. arXiv
preprint arXiv:2105.08920 .
Mandy Guo, Joshua Ainslie, David Uthus, Santiago On-
tanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.
2021. Longt5: Efficient text-to-text transformer for
long sequences. arXiv preprint arXiv:2112.07916 .
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng
Ji, and Sinong Wang. 2023. Lm-infinite: Simple
on-the-fly length generalization for large language
models. arXiv preprint arXiv:2308.16137 .
Dan Hendrycks, Collin Burns, Anya Chen, and Spencer
Ball. 2021. Cuad: An expert-annotated nlp dataset
for legal contract review.
Yebowen Hu, Tim Ganter, Hanieh Deilamsalehy, Franck
Dernoncourt, Hassan Foroosh, and Fei Liu. 2023.
Meetingbank: A benchmark dataset for meeting sum-
marization. arXiv preprint arXiv:2305.17529 .
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng
Ji, and Lu Wang. 2021. Efficient attentions for long
document summarization. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1419–1436, Online.
Association for Computational Linguistics.
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. triviaqa: A Large Scale Distantly
Supervised Challenge Dataset for Reading Compre-
hension. arXiv e-prints , page arXiv:1705.03551.
Amirhossein Kazemnejad, Inkit Padhi,
Karthikeyan Natesan Ramamurthy, Payel Das,
and Siva Reddy. 2023. The impact of positional
encoding on length generalization in transformers.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

--- PAGE 14 ---
Tomáš Ko ˇciský, Jonathan Schwarz, Phil Blunsom, Chris
Dyer, Karl Moritz Hermann, Gábor Melis, and Ed-
ward Grefenstette. 2018. The NarrativeQA reading
comprehension challenge. Transactions of the Asso-
ciation for Computational Linguistics , 6:317–328.
Vijay Anand Korthikanti, Jared Casper, Sangkug Lym,
Lawrence McAfee, Michael Andersch, Mohammad
Shoeybi, and Bryan Catanzaro. 2023. Reducing ac-
tivation recomputation in large transformer models.
Proceedings of Machine Learning and Systems , 5.
Wojciech Kry ´sci´nski, Nazneen Rajani, Divyansh Agar-
wal, Caiming Xiong, and Dragomir Radev. 2021.
Booksum: A collection of datasets for long-
form narrative summarization. arXiv preprint
arXiv:2105.08209 .
Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen
Sun, Liangyou Li, Lifeng Shang, Qun Liu, and
Kam-Fai Wong. 2023. M4le: A multi-ability multi-
range multi-task multi-domain long-context evalua-
tion benchmark for large language models. arXiv
preprint arXiv:2310.19240 .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Transactions of the Association of Compu-
tational Linguistics .
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim,
Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,
Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko,
Nicolas Gontier, Nicholas Meade, Armel Zebaze,
Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,
Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo
Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,
Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,
Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo
Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel
Romero, Tony Lee, Nadav Timor, Jennifer Ding,
Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri
Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
Carolyn Jane Anderson, Brendan Dolan-Gavitt, Dan-
ish Contractor, Siva Reddy, Daniel Fried, Dzmitry
Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
Sean Hughes, Thomas Wolf, Arjun Guha, Leandro
von Werra, and Harm de Vries. 2023. Starcoder: may
the source be with you!
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023. Lost in the middle: How lan-
guage models use long contexts. arXiv preprint
arXiv:2307.03172 .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. In EMNLP .
Deepak Narayanan, Mohammad Shoeybi, Jared Casper,
Patrick LeGresley, Mostofa Patwary, Vijay Kor-
thikanti, Dmitri Vainbrand, Prethvi Kashinkunti,
Julie Bernauer, Bryan Catanzaro, et al. 2021. Ef-
ficient large-scale language model training on gpu
clusters using megatron-lm. In Proceedings of the
International Conference for High Performance Com-
puting, Networking, Storage and Analysis , pages 1–
15.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human-generated machine read-
ing comprehension dataset.
OpenAI. 2022. Introducing chatgpt.
OpenAI. 2023. Gpt-4 technical report.
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,
Nikita Nangia, Jason Phang, Angelica Chen, Vishakh
Padmakumar, Johnny Ma, Jana Thompson, He He,
and Samuel Bowman. 2022. QuALITY: Question
answering with long input texts, yes! In Proceedings
of the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 5336–5358,
Seattle, United States. Association for Computational
Linguistics.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context window
extension of large language models.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy
Schwartz, Noah A. Smith, and Lingpeng Kong. 2021.
Random feature attention.
Ofir Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Confer-
ence on Learning Representations .
Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih,
Sinong Wang, and Jie Tang. 2019. Blockwise self-
attention for long document understanding. arXiv
preprint arXiv:1911.02972 .
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.
Jack Rae and Ali Razavi. 2020. Do transformers need
deep long-range memory? In Proceedings of the 58th
Annual Meeting of the Association for Computational

--- PAGE 15 ---
Linguistics , pages 7524–7529, Online. Association
for Computational Linguistics.
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
Chloe Hillier, and Timothy P Lillicrap. 2019. Com-
pressive transformers for long-range sequence mod-
elling. arXiv preprint .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2019. Winogrande: An adver-
sarial winograd schema challenge at scale.
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019. Social IQa: Com-
monsense reasoning about social interactions. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 4463–
4473, Hong Kong, China. Association for Computa-
tional Linguistics.
Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1073–
1083, Vancouver, Canada. Association for Computa-
tional Linguistics.
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-
rant, and Omer Levy. 2023. Zeroscrolls: A zero-
shot benchmark for long text understanding. arXiv
preprint arXiv:2305.14196 .
Eva Sharma, Chen Li, and Lu Wang. 2019. BIG-
PATENT: A large-scale dataset for abstractive and
coherent summarization. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 2204–2213, Florence, Italy. Asso-
ciation for Computational Linguistics.
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.
Self-attention with relative position representations.
arXiv preprint arXiv:1803.02155 .
Noam Shazeer. 2019. Fast transformer decoding: One
write-head is all you need.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. 2019. Megatron-lm: Training multi-billion
parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 .Kaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu,
and Fei Liu. 2022. Towards abstractive grounded
summarization of podcast transcripts. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 4407–4418, Dublin, Ireland. Association for
Computational Linguistics.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2023. Roformer: En-
hanced transformer with rotary position embedding.
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2022. A length-extrapolatable
transformer.
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2023. A length-extrapolatable
transformer. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers) , pages 14590–14604,
Toronto, Canada. Association for Computational Lin-
guistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4149–4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and
Da-Cheng Juan. 2020a. Sparse sinkhorn attention.
InInternational Conference on Machine Learning ,
pages 9438–9447. PMLR.
Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-
Cheng Juan. 2020b. Sparse sinkhorn attention. In
Proceedings of the 37th International Conference on
Machine Learning , ICML’20. JMLR.org.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas
Scialom, Anthony Hartshorn, Elvis Saravia, Andrew
Poulton, Viktor Kerkez, and Robert Stojnic. 2022.
Galactica: A large language model for science.
Steven Tey. Sharegpt.
tju01. Fasteval.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,

--- PAGE 16 ---
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. MuSiQue: Multi-
hop questions via single-hop question composition.
Transactions of the Association for Computational
Linguistics , 10:539–554.
Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and
Lin-Shan Lee. 2016. Towards machine comprehen-
sion of spoken content: Initial toefl listening compre-
hension test by machine.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Ja-
son Phang, and Samuel R. Bowman. 2022. SQuAL-
ITY: Building a long-document summarization
dataset the hard way. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 1139–1156, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in Neural
Information Processing Systems , 35:24824–24837.
BigScience Workshop, :, Teven Le Scao, Angela Fan,
Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luc-
cioni, François Yvon, Matthias Gallé, Jonathan
Tow, Alexander M. Rush, Stella Biderman, Albert
Webson, Pawan Sasanka Ammanamanchi, Thomas
Wang, Benoît Sagot, Niklas Muennighoff, Albert Vil-
lanova del Moral, Olatunji Ruwase, Rachel Bawden,
Stas Bekman, Angelina McMillan-Major, Iz Belt-
agy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-
dro Ortiz Suarez, Victor Sanh, Hugo Laurençon,Yacine Jernite, Julien Launay, Margaret Mitchell,
Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor
Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,
Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,
Chris Emezue, Christopher Klamm, Colin Leong,
Daniel van Strien, David Ifeoluwa Adelani, Dragomir
Radev, Eduardo González Ponferrada, Efrat Lev-
kovizh, Ethan Kim, Eyal Bar Natan, Francesco De
Toni, Gérard Dupont, Germán Kruszewski, Giada
Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,
Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar
Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse
Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,
Joseph Tobing, Joydeep Bhattacharjee, Khalid Al-
mubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra,
Leon Weber, Long Phan, Loubna Ben allal, Lu-
dovic Tanguy, Manan Dey, Manuel Romero Muñoz,
Maraim Masoud, María Grandury, Mario Šaško,
Max Huang, Maximin Coavoux, Mayank Singh,
Mike Tian-Jian Jiang, Minh Chien Vu, Moham-
mad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,
Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas, Pe-
ter Henderson, Pierre Colombo, Priscilla Amuok,
Quentin Lhoest, Rheza Harliman, Rishi Bommasani,
Roberto Luis López, Rui Ribeiro, Salomey Osei,
Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
Shayne Longpre, Somaieh Nikpoor, Stanislav Silber-
berg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-
rent, Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-
lat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Davut Emre Ta¸ sar, Elizabeth Salesky, Sabrina J.
Mielke, Wilson Y . Lee, Abheesht Sharma, Andrea
Santilli, Antoine Chaffin, Arnaud Stiegler, Debajy-
oti Datta, Eliza Szczechla, Gunjan Chhablani, Han
Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan
Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-
ful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-
hal Nayak, Ryan Teehan, Samuel Albanie, Sheng
Shen, Srulik Ben-David, Stephen H. Bach, Taewoon
Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-
mish Thakker, Vikas Raunak, Xiangru Tang, Zheng-
Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,
Hadar Tojarieh, Adam Roberts, Hyung Won Chung,
Jaesung Tae, Jason Phang, Ofir Press, Conglong Li,
Deepak Narayanan, Hatim Bourfoune, Jared Casper,
Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia
Zhang, Mohammad Shoeybi, Myriam Peyrounette,
Nicolas Patry, Nouamane Tazi, Omar Sanseviero,
Patrick von Platen, Pierre Cornette, Pierre François
Lavallée, Rémi Lacroix, Samyam Rajbhandari, San-
chit Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aurélie Névéol, Charles Lover-
ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,
Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bog-
danov, Genta Indra Winata, Hailey Schoelkopf, Jan-
Christoph Kalo, Jekaterina Novikova, Jessica Zosa
Forde, Jordan Clive, Jungo Kasai, Ken Kawamura,

--- PAGE 17 ---
Liam Hazan, Marine Carpuat, Miruna Clinciu, Na-
joung Kim, Newton Cheng, Oleg Serikov, Omer
Antverg, Oskar van der Wal, Rui Zhang, Ruochen
Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani
Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun,
Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov,
Vladislav Mikhailov, Yada Pruksachatkun, Yonatan
Belinkov, Zachary Bamberger, Zden ˇek Kasner, Al-
ice Rueda, Amanda Pestana, Amir Feizpour, Ammar
Khan, Amy Faranak, Ana Santos, Anthony Hevia,
Antigona Unldreaj, Arash Aghagol, Arezoo Abdol-
lahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh
Behroozi, Benjamin Ajibade, Bharat Saxena, Car-
los Muñoz Ferrandis, Daniel McDuff, Danish Con-
tractor, David Lansky, Davis David, Douwe Kiela,
Duong A. Nguyen, Edward Tan, Emi Baylor, Ez-
inwanne Ozoani, Fatima Mirza, Frankline Onon-
iwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-
tacharya, Irene Solaiman, Irina Sedenko, Isar Ne-
jadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis
Sanz, Livia Dutra, Mairon Samagaio, Maraim El-
badri, Margot Mieskes, Marissa Gerchick, Martha
Akinlolu, Michael McKenna, Mike Qiu, Muhammed
Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-
jani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,
Ran An, Rasmus Kromann, Ryan Hao, Samira Al-
izadeh, Sarmad Shubber, Silas Wang, Sourav Roy,
Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,
Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,
Alfredo Palasciano, Alison Callahan, Anima Shukla,
Antonio Miranda-Escalada, Ayush Singh, Benjamin
Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel León
Periñán, Daniel Molano, Dian Yu, Enrique Manjava-
cas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,
Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec,
Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi,
Jonas Golde, Jose David Posada, Karthik Ranga-
sai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa
Shinzato, Madeleine Hahn de Bykhovetz, Maiko
Takeuchi, Marc Pàmies, Maria A Castillo, Mari-
anna Nezhurina, Mario Sänger, Matthias Samwald,
Michael Cullan, Michael Weinberg, Michiel De
Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,
Myungsun Kang, Natasha Seelam, Nathan Dahlberg,
Nicholas Michio Broad, Nikolaus Muellner, Pascale
Fung, Patrick Haller, Ramya Chandrasekhar, Renata
Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline
Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,
Shlok S Deshmukh, Shubhanshu Mishra, Sid Ki-
blawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-
mar, Stefan Schweter, Sushil Bharati, Tanmay Laud,
Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-
nis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,
Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli
Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2023. Bloom: A 176b-parameter
open-access multilingual language model.
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,
Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
Madian Khabsa, Han Fang, Yashar Mehdad, Sharan
Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale,Sergey Edunov, Mike Lewis, Sinong Wang, and Hao
Ma. 2023. Effective long-context scaling of founda-
tion models.
Xianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang
Wang, Xiaoman Pan, Linda Petzold, and Dong Yu.
2023. OASum: Large-scale open domain aspect-
based summarization. In Findings of the Association
for Computational Linguistics: ACL 2023 , pages
4381–4401, Toronto, Canada. Association for Com-
putational Linguistics.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding. Advances in neural informa-
tion processing systems , 32.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023. Tree of thoughts: Deliberate
problem solving with large language models. arXiv
preprint arXiv:2305.10601 .
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. 2020. Big bird: Transformers for
longer sequences. Advances in Neural Information
Processing Systems , 33.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-
chine really finish your sentence? In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics , pages 4791–4800, Florence,
Italy. Association for Computational Linguistics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models.
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia
Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli
Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir
Radev. 2021. QMSum: A new benchmark for query-
based multi-domain meeting summarization. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
5905–5921, Online. Association for Computational
Linguistics.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
L. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke
Zettlemoyer, and Omer Levy. 2023. Lima: Less is
more for alignment. ArXiv , abs/2305.11206.
Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.
2021. Mediasum: A large-scale media interview

--- PAGE 18 ---
dataset for dialogue summarization. arXiv preprint
arXiv:2103.06410 .

--- PAGE 19 ---
A Appendix
A.1 Group Attention Implementation
As detailed in Section 2.2, our architecture employs a layerwise grouped local-global attention approach,
segregating the application of local and global attention across different layers. The methodology for
local attention is outlined in Algorithm 1. In contrast, the global attention mechanism adheres to the
conventional multi-head attention paradigm found in transformers. It is important to note that during
inference when leveraging the Key-Value (K-V) cache, the process for local attention layers deviates
from Algorithm 1. Instead, we exclusively implement the standard global attention framework while
maintaining the latest wK-V states. This approach streamlines the attention process while ensuring
efficiency and effectiveness in inference.
A.2 Case Study
As discussed in Section 4.3, evaluating long-context tasks presents challenges. Firstly, common evaluation
metrics may misrepresent response quality, particularly in summarization datasets where the reference
summary may not align with diverse reader interests or knowledge levels. Furthermore, the limited
diversity in long-context tasks may result in inadequate training for instruction tuning, often stemming
from summarization, QA, or information extraction datasets with long documents. A more varied set of
tasks could potentially enhance the capability of instruction-tuned models. Lastly, as demonstrated in
the subsequent section, instances are frequently constructed with inaccurate gold labels or information
not covered in given documents, intensifying the difficulty of model evaluation and potentially yielding
erroneous results. We present exemplars and results employing Zebra ,LLAMA , and ChatGPT-3.5 on
ZeroScrolls (Shaham et al., 2023). Due to the limited space and long documents, summarization examples
are not presented.

--- PAGE 20 ---
Algorithm 1: Local Attention
Data: Q, K, V ∈ Rbsz×len×n_heads×dim _per_head
Data: M∈[0,1]bsz×n_heads×len×lenis attention mask
Data: wis the window size
Data: λis the normalizing factor
// padding the sequence length to multiple of window size
// after padding shape:
//bsz×padded _len×n_heads ×dim_per_head
1Q←pad_to_multiple (Q, w);
2K←pad_to_multiple (K, w);
3V←pad_to_multiple (V, w);
// split Q, K, V into blocks
// after spliting shape:
//bsz×n_blocks ×w×n_heads ×dim_per_head
4Qlocal←split_into_blocks (Q);
5Klocal←split_into_blocks (K);
6Vlocal←split_into_blocks (V);
// for KandVmerge each block and the blocks before it
// after merging shape:
//bsz×n_blocks ×2∗w×n_heads ×dim_per_head
7Klocal←concatenate_2_blocks (K);
8Vlocal←concatenate_2_blocks (V);
// calculate attention score
// the attention score shape:
//bsz×n_heads ×n_blocks ×w×2∗w
9attn_score ←;
10 torch.einsum (′...qhd, ...khd −> ...hqk′, Qlocal, Klocal);
11 .transpose (1,2);
// multiply with the normalizing factor
12attn_score ←λ∗attn_score ;
// Extract the attention from the original attention mask
13Moriginal ←extract_original_blocks (M, w );
// Prepare the attention for a single block ( w×2∗w
14Mblock←create_blockwise_mask (w);
// Taking both attention mask into consideration
15Moverall ←Moriginal ∩Mblock
// merge the sequence length dim
// after merging shape:
//bsz×n_heads ×padded _len×2∗w
16new_shape ←(bsz, n _heads, padded _len,2∗w);
17attn_score ←attn_score .reshape (new_shape );
18Moverall ←Moverall .reshape (new_shape );
// softmax
19attn_prob ←softmax (attn_score , Moverall );
// reshape back in block format
// shape: bsz×n_blocks ×w×n_heads ×2∗w
20new_shape ←(bsz, n _heads, n _blocks, w, 2∗w);
21attn_prob ←attn_prob .reshape (new_shape ).transpose (1,2);
// get context vector
// shape: bsz×n_blocks ×w×n_heads ×dim_per_head
22attn_outputs ←;
23 torch.einsum (′...hqd, ...khd −> ...qhd′,attn_prob , Vlocal);
// reshape to output format
24new_shape ←(bsz, padded _len, n _heads, dim _per_head);
// Don’t forget to remove the padding ones
25attn_outputs ←attn_outputs .reshape (new_shape )[:, seq _len,:,:];
Result: attn _outputs

--- PAGE 21 ---
NarrativeQA
Instruction You are given a story, which can be either a novel or a movie script, and a question.
Answer the question as concisely as you can, using a single phrase if possible. Do not provide any
explanation.
Document [...] INTRODUCTION. The Crito seems intended to exhibit the character of Socrates
in one light only, not as the philosopher, fulfilling a divine mission and trusting in the will of
heaven, but simply as the good citizen, who having been unjustly condemned is willing to give up
his life in obedience to the laws of the state... Thedays ofSocrates aredraw ingtoaclose; the fatal
ship has been seen off Sunium, as he is informed by his aged friend and contemporary Crito, who
visits him before the dawn has broken; he himself has been warned in a dream that on the third day
he must depart. Time is precious, and Crito hascome early inordertogain hisconsenttoaplan of
escape. This can be easily accomplished by his friends, who will incur no danger in making the
attempt to save him, but will be disgraced for ever if they allow him to perish. He should think of
his duty to his children, and not play into the hands of his enemies. Money is already provided
by Crito as well as by Simmias and others, and he will have no difficulty in finding friends in
Thessaly and other places. Socrates is afraid that Crito is but pressing upon him the opinions of
the many: whereas, all his life long he has followed the dictates of reason only and the opinion
of the one wise or skilled man. There was a time when Crito himself had allowed the propriety
of this. And although some one will say ’the many can kill us,’ that makes no difference; but a
good life, in other words, a just and honourable life, is alone to be valued. All considerations of
loss of reputation or injury to his children should be dismissed: the only question is whether he
would be right in attempting to escape. Crito, who is a disinterested person not having the fear of
death before his eyes, shall answer this for him. Before he was condemned they had often held
discussions, in which they agreed that no man should either do evil, or return evil for evil, or betray
the right. Are these principles to be altered because the circumstances of Socrates are altered?
Crito admits that they remain the same. Then is his escape consistent with the maintenance of
them? To this Crito is unable or unwilling to reply. Socrates proceeds:–Suppose the Laws of
Athens to come and remonstrate with him: they will ask ’Why does he seek to overturn them?’ and
if he replies, ’they have injured him,’ will not the Laws answer, ’Yes, but was that the agreement?
Has he any objection to make to them which would justify him in overturning them? Was he not
brought into the world and educated by their help, and are they not his parents? He might have
left Athens and gone where he pleased, but he has lived there for seventy years more constantly
than any other citizen.’ Thus he has clearly shown that he acknowledged the agreement, which he
cannot now break without dishonour to himself and danger to his friends. Even in the course of the
trial he might have proposed exile as the penalty, but then he declared that he preferred death to
exile. And whither will he direct his footsteps? In any well-ordered state the Laws will consider
him as an enemy. Possibly in a land of misrule like Thessaly he may be welcomed at first, and
the unseemly narrative of his escape will be regarded by the inhabitants as an amusing tale. But if
he offends them he will have to learn another sort of lesson. Will he continue to give lectures in
virtue? That would hardly be decent. [...]
Question Who persuades Socrates to escape?
Ground Truth Crito, Crito convinces Socrates to leave his cell.
Zebra Output Crito✓
LLAMA Output According to the text, it is Crito who persuades Socrates to escape. ✓
ChatGPT-3.5 Output Crito✓
In this example, while all models provide correct answers, LLAMA responds in a more verbose manner,
deviating from the given instruction.

--- PAGE 22 ---
Qasper
Instruction You are given a scientific article and a question. Answer the question as concisely as
you can, using a single phrase or sentence if possible. If the question cannot be answered based on
the information in the article, write “unanswerable”. If the question is a yes/no question, answer
“yes”, “no”, or “unanswerable”. Do not provide any explanation.
Document [...] For all the features, we separated the words in the tweets based on the language
annotation (Section 2.3) and prepared the feature vector for each tweet by combining the vectors
for both the languages . Previous researches shows that letter n-grams are very efficient for clas-
sifying text. They are language independent and does not require expensive text pre-processing
techniques like tokenization, stemming and stop words removal, hence in the case of code-mix
texts, this could yield good results BIBREF16 , BIBREF17 . Since the number of n-grams can be
very large we took trigrams which occur more than ten times in the corpus. For classifying humor
in texts, it is important to understand the semantics of the sentence. Thus, we took a three word
window as a feature to train our classification models to incorporate the contextual information.
Many jokes and idioms sometimes have common words. We identified those words and took
them as as a feature for classification. In the preprocessing step, we decomposed hashtags using
camel cases and added them along with the words. Hence, common words in the hashtags were
also included in the feature vector. ClassificationApproach and
ResultsWe experimented with fourdifferentclassifiers, namely, supportvectormachine BIBREF18
,random forest,extratreeandnaive bayes classifierBIBREF19 . Chi square feature selection
algorithm is applied to reduces the size of our feature vector. For training our system classifier, we
used Scikit-learn BIBREF19 . 10-fold cross validation on 3543 code-mixed tweets was carried out
by dividing the corpus into 10 equal parts with nine parts as training corpus and rest one for testing.
Mean accuracy is calculated by taking the average of the accuracy obtained in each iteration of
the testing process. Table 2 shows the accuracy for each feature when trained using mentioned
classifiers along with the accuracy when all the features are used along with the overall accuracy.
Support vector machine with radial basis function kernel and extra tree classifier performs better
than other classifiers and yields 69.3% and 67.8% accuracy respectively. The reason kernel SVM
yields the best result is because the number of observations is greator than the number of features
BIBREF20 . N-grams proved to be the most efficient in all classification models followed by
common words and hastags. Bag-of-words feature performed the worst in SVM, random forest
and extra tree classifier but yielded better result in naive bayes classifiers. Accuracies mentioned in
table 2 were calculated using fine tuning of model parameters using grid search. Conclusion and
Future Work
In this paper, we describe a freely available corpus of 3453 English-Hindi code-mixed tweets. The
tweets are annotated with humorous(H) and non-humorous(N) tags along with the language tags at
the word level. The task of humor identification in social media texts is analyzed as a classification
problem and several machine learning classification models are used. Thefeatures used inour
classificationsystemaren-grams, bag-of-words, common words andhashtags. N-grams when
trained with support vector machines with radial basis function kernel performed better than other
features and yielded an accuracy of 68.5%. The best accuracy (69.3%) was given by support vector
machines with radial basis function kernel. This paper describes the initial efforts in automatic
humor detection in code-mixed social media texts. Corpus can be annotated with part-of-speech
tags at the word level which may yield better results in language detection. [...]
Question What type of system does the baseline classification use?
Ground Truth support vector machine BIBREF18 , random forest, extra tree and naive bayes
classifier BIBREF19, Classification system use n-grams, bag-of-words, common words and
hashtags as features and SVM, random forest, extra tree and NB classifiers.
Zebra Output The baseline classification uses n-grams, bag-of-words, common words and
hashtags. ✓
LLAMA Output to the article, the baseline classification system uses a support vector machine
(SVM) classifier. ✗
ChatGPT-3.5 Output Supervised classification system ✗

--- PAGE 23 ---
Musique
Instruction You are given several paragraphs from Wikipedia and a question. Answer the question
as concisely as you can, using a single phrase if possible. If the question cannot be answered based
on the information in the paragraphs, write “unanswerable”. Do not provide any explanation.
Document [...] Title: Israel
Israel, officially the State of Israel, is a country in the Middle East, on the southeastern shore of the
Mediterranean Sea and the northern shore of the Red Sea. It has land borders with Lebanon to the
north, Syria to the northeast, Jordan on the east, the Palestinian territories of the West Bank and
Gaza Strip to the east and west, respectively, and Egypt to the southwest. The country contains
geographically diverse features within its relatively small area. Israel’s economy and technology
center is Tel Aviv, while its seat of government and proclaimed capital is Jerusalem, although the
state’s sovereignty over East Jerusalem is not recognised internationally. The population of Israel
was estimated in 2017 to be 8,777,580 people, of whom 74.7% were Jewish, 20.8% Arab and 4.5%
others.
Title: The World (archipelago)
The World or The World Islands, (“Juzur al- ¯Alam”) is an artificial archipelago of various small
islands constructed in the rough shape of a world map, located in the waters of the Persian Gulf,
off the coast of Dubai, United Arab Emirates. The World islands are composed mainly of sand
dredged from Dubai’s shallow coastal waters, and are one of several artificial island developments
in Dubai. The World’s developer is Nakheel Properties, and the project was originally conceived by
Sheikh Mohammed bin Rashid Al Maktoum, the ruler of Dubai. The actual construction was done
by two Dutch (joint venture) specialist companies, Van Oord and Boskalis. The same companies
also created the Palm Jumeirah.
Title: Israel
Israel is considered the most advanced country in Southwest Asia and the Middle East in economic
and industrial development. Israel’s quality university education and the establishment of a highly
motivated and educated populace is largely responsible for spurring the country’s high technology
boom and rapid economic development. In 2010, it joined the OECD. The country is ranked 3rd in
the region and 38th worldwide on the World Bank’s Ease of Doing Business Index as well as in
the World Economic Forum’s Global Competitiveness Report. It has the second-largest number of
startup companies in the world (after the United States) and the largest number of NASDAQ-listed
companies outside North America.
Title: Israel
Israel is one of the world’s technological leaders in water technology. In 2011, its water technology
industry was worth around $2 billion a year with annual exports of products and services in the
tens of millions of dollars. The ongoing shortage of water in the country has spurred innovation in
water conservation techniques, and a substantial agricultural modernization, drip irrigation, was
invented in Israel. [...]
Question When was the region that lies to the north of the region Israel is located and where The
World is located created?
Ground Truth 1930
Zebra Output 1946✗
LLAMA Output The region that lies to the north of Israel was created in the mid-18th century. ✗
ChatGPT-3.5 Output unanswerable ✗
This example underscores that the pertinent information for the given question cannot be discerned
within the context, indicating that certain examples may not have been accurately constructed. The
paragraphs do not provide information to answer when the region north of Israel and The World archipelago
was created. There is information about Israel and The World islands located in Dubai, but no mention of
the region to the north of Israel. The Zebra andLLAMA models manifest hallucinations, whereas GPT
answers accurately following the given instruction.

--- PAGE 24 ---
Musique
Instruction You are given several paragraphs from Wikipedia and a question. Answer the question
as concisely as you can, using a single phrase if possible. If the question cannot be answered based
on the information in the paragraphs, write “unanswerable”. Do not provide any explanation.
Document Title: Jerome Quinn
Born in Green Bay, Wisconsin, Quinn was a realtor and served on the Green Bay Common
Council, the Brown County, Wisconsin Board of Supervisors, the local Board of Education, and
the Wisconsin State Assembly from 1955 until 1973. He was a Republican.
Title: Max Rosenthal Tenement in Bydgoszcz
Max Rosenthal House is a historical tenement located at Gda ´nska Street N °42 in downtown
Bydgoszcz, Poland, built when the city was part of the Kingdom of Prussia. It is registered on the
Kuyavian-Pomeranian V oivodeship Heritage List.
Title: Borders of China
China shares international borders with 14 sovereign states. In addition, there is a 30 - km border
with the special administrative region of Hong Kong, which was a British dependency before 1997,
and a 3 km border with Macau, a Portuguese territory until 1999. With a land border of 22,117
kilometres (13,743 mi) in total it also has the longest land border of any country.
Title: Notogawa, Shiga
Notogawa Station (Location: N35.179899,E136.165913) is the only Japan Railway station in
Higashiomi. The station is a rapid stop on the JR Biwako Line, located between stations in
Omi-Hachiman to the east and Hikone to the west. The town shares a small border with Lake Biwa
to the northwest.
Title: Tenement at Gdanska street 75, Bydgoszcz
Tenement at Gdanska street 75 is a historical habitation building located at Gda ´nska Street N °75,
in Bydgoszcz. It is registered on the Kuyavian-Pomeranian V oivodeship Heritage List.
Title: Tenement (band)
Tenement isathree piece rock band from Appleton,Wisconsinformed in2006. They are often
associated with the American hardcore punk scene. Their recorded output has been described
as everything from “noise pop” to “fuzz punk”, while in a live setting they are often known for
experimentation, improvisation, and high-energy performance. The visual art of singer/guitarist
Amos Pitsch is associated with most of their records, as well as several records by other notable
punk and hardcore bands. In 2016, they were included in “Rolling Stone’s” “10 Great Modern
Punk Bands”.
Title: Pulaski High School
Pulaski High School is a public high school in Pulaski, Wisconsin, in Brown County, Wisconsin
(school district also serves parts of Shawano, Outagamie and Oconto counties), that serves students
in grades 9 through 12. Its mascot is the Red Raider.[...]
Question What is the capital of the county that shares a border with the county where the band
Tenement was formed?
Ground Truth Green Bay
Zebra Output Appleton ✗
LLAMA Output Appleton. ✗
ChatGPT-3.5 Output Appleton, Wisconsin ✗
This example shows that the ground truth is not correct. The paragraph states that the band Tenement
is “a three piece rock band from Appleton, Wisconsin formed in 200”. There is no information about
the capital of the county that shares a border with Appleton in the document but Appleton is the closest
answer given the context.
