# 2306.13421.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/long-context/2306.13421.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 801468 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
Retrieval-Pretrained Transformer: MÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa vá»›i
tá»± truy xuáº¥t
Ohad Rubin Jonathan Berant
TrÆ°á»ng Khoa há»c MÃ¡y tÃ­nh Blavatnik, Äáº¡i há»c Tel Aviv
{ohad.rubin,joberant}@cs.tau.ac.il
TÃ³m táº¯t
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tÄƒng cÆ°á»ng truy xuáº¥t
(LMs) Ä‘Ã£ nháº­n Ä‘Æ°á»£c nhiá»u sá»± chÃº Ã½ gáº§n Ä‘Ã¢y.
Tuy nhiÃªn, thÃ´ng thÆ°á»ng bá»™ truy xuáº¥t khÃ´ng Ä‘Æ°á»£c
huáº¥n luyá»‡n chung nhÆ° má»™t thÃ nh pháº§n tá»± nhiÃªn
cá»§a LM, mÃ  Ä‘Æ°á»£c thÃªm vÃ o sau khi LM Ä‘Ã£ Ä‘Æ°á»£c
tiá»n huáº¥n luyá»‡n, Ä‘iá»u nÃ y háº¡n cháº¿ kháº£ nÄƒng thÃ­ch
á»©ng cá»§a LM vÃ  bá»™ truy xuáº¥t vá»›i nhau. Trong
cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t Retrieval-
Pretrained Transformer (RPT), má»™t kiáº¿n trÃºc
vÃ  quy trÃ¬nh huáº¥n luyá»‡n Ä‘á»ƒ huáº¥n luyá»‡n chung
má»™t LM tÄƒng cÆ°á»ng truy xuáº¥t tá»« Ä‘áº§u vÃ  Ã¡p dá»¥ng
nÃ³ cho nhiá»‡m vá»¥ mÃ´ hÃ¬nh hÃ³a vÄƒn báº£n dÃ i.
Vá»›i má»™t Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c táº¡o gáº§n Ä‘Ã¢y trong
má»™t tÃ i liá»‡u dÃ i, LM tÃ­nh toÃ¡n cÃ¡c biá»ƒu diá»…n
truy váº¥n, sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ truy xuáº¥t
cÃ¡c Ä‘oáº¡n trÆ°á»›c Ä‘Ã³ trong tÃ i liá»‡u, cÃ³ thá»ƒ náº±m
cÃ¡ch hÃ ng chá»¥c nghÃ¬n token. ThÃ´ng tin tá»«
cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c truy xuáº¥t Ä‘Æ°á»£c há»£p nháº¥t vÃ o
cÃ¡c biá»ƒu diá»…n LM Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘oáº¡n má»¥c tiÃªu
tiáº¿p theo. ChÃºng tÃ´i huáº¥n luyá»‡n thÃ nh pháº§n
truy xuáº¥t vá»›i má»™t má»¥c tiÃªu ngá»¯ nghÄ©a, trong
Ä‘Ã³ má»¥c tiÃªu lÃ  truy xuáº¥t cÃ¡c Ä‘oáº¡n lÃ m tÄƒng
xÃ¡c suáº¥t cá»§a Ä‘oáº¡n tiáº¿p theo, theo má»™t LM
tham chiáº¿u. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT trÃªn bá»‘n
nhiá»‡m vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa, bao gá»“m
sÃ¡ch, mÃ£ vÃ  vÄƒn báº£n toÃ¡n há»c, vÃ  chá»©ng minh
ráº±ng RPT cáº£i thiá»‡n cháº¥t lÆ°á»£ng truy xuáº¥t vÃ 
do Ä‘Ã³ giáº£m perplexity so vá»›i cÃ¡c baseline
máº¡nh máº½.
1 Giá»›i thiá»‡u
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LMs) Ä‘Ã£ cÃ³ thÃ nh cÃ´ng
to lá»›n gáº§n Ä‘Ã¢y (Brown et al., 2020; Chowdhery
et al., 2022; Zhang et al., 2022; Touvron et al.,
2023), trá»Ÿ thÃ nh má»™t cÃ´ng cá»¥ há»¯u Ã­ch trÃªn nhiá»u
lÄ©nh vá»±c. Tuy nhiÃªn, thÃ nh cÃ´ng cá»§a chÃºng Ä‘i kÃ¨m
vá»›i chi phÃ­ tÃ­nh toÃ¡n, do sá»‘ lÆ°á»£ng tham sá»‘ tÄƒng
Ä‘á»ƒ lÆ°u trá»¯ kiáº¿n thá»©c tháº¿ giá»›i (Fedus et al., 2022)
vÃ  Ä‘á»™ dÃ i ngá»¯ cáº£nh tÄƒng cho phÃ©p truy cáº­p thÃ´ng
tin á»Ÿ xa, nhÆ°ng phÃ¡t sinh chi phÃ­ phá»©c táº¡p báº­c hai.
Lexically SimilarBook orLong textSemantically Similar
Retrieveâ‹®Fuseâ‹®
Training SignalPÎ¸ (â€¦â€¦....|â€¦â€¦â€¦â€¦â€¦...)ğ‘!" ğ‘#$! ğ‘#$# >PÎ¸ (â€¦â€¦....|â€¦â€¦â€¦â€¦â€¦...)ğ‘!$$ ğ‘#$! ğ‘#$# Chunk 100Chunk 13Past States
PredictCausal Language ModelChunk 201
Chunk 202Query
TargetInput
RefRefThe killer left a room full of evidence, a puzzle for forensics.As a kid, Lt. Johnfound a dead dog; since then, crimson always unnervedhim.Lt.  John looked around, "Another victim, The Crimson Murderer strikes again."
"I bet the forensic guys  would love this."HÃ¬nh 1: Retrieval-Pretrained Transformer (RPT) lÃ  má»™t
mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u vá»›i kháº£ nÄƒng
truy xuáº¥t tá»± nhiÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho vÄƒn báº£n dÃ i
(vÃ­ dá»¥: sÃ¡ch). RPT nháº­n má»™t Ä‘oáº¡n vÄƒn báº£n lÃ m Ä‘áº§u vÃ o,
truy xuáº¥t cÃ¡c Ä‘oáº¡n cÃ³ liÃªn quan ngá»¯ nghÄ©a tá»« quÃ¡ khá»© Ä‘á»ƒ
dá»± Ä‘oÃ¡n tá»‘t hÆ¡n Ä‘oáº¡n tiáº¿p theo, vÃ  há»£p nháº¥t cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c
truy xuáº¥t nÃ y vÃ o cÃ¡c biá»ƒu diá»…n cá»§a nÃ³. TrÃªn má»™t loss LM
tiÃªu chuáº©n, bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ truy xuáº¥t cÃ¡c
Ä‘oáº¡n lÃ m tÄƒng xÃ¡c suáº¥t cá»§a Ä‘oáº¡n tiáº¿p theo theo má»™t LM
tham chiáº¿u.
MÃ´ hÃ¬nh ngÃ´n ngá»¯ tÄƒng cÆ°á»ng truy xuáº¥t (RALM)
giáº£m bá»›t chi phÃ­ nÃ y (Khandelwal et al., 2020; Yo-
gatama et al., 2021; Borgeaud et al., 2022; Ram
et al., 2023), vÃ¬ viá»‡c truy xuáº¥t chÃ­nh xÃ¡c thÃ´ng tin
liÃªn quan cÃ³ thá»ƒ giáº£m yÃªu cáº§u bá»™ nhá»› vÃ  tÃ­nh toÃ¡n.
HÆ¡n ná»¯a, RALM cÃ³ lá»£i cho tÃ­nh chÃ­nh xÃ¡c, tÃ­nh
má»›i vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a mÃ  khÃ´ng cáº§n huáº¥n
luyá»‡n láº¡i, chá»‰ Ä‘Æ¡n giáº£n báº±ng cÃ¡ch thay Ä‘á»•i chá»‰ má»¥c
truy xuáº¥t (Guu et al., 2020; Lewis et al., 2020;
Huang et al., 2023).
Tuy nhiÃªn, cÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» RALM pháº§n
lá»›n Ä‘Ã£ khÃ´ng huáº¥n luyá»‡n bá»™ truy xuáº¥t nhÆ° má»™t
thÃ nh pháº§n háº¡ng nháº¥t cá»§a LM. Trong má»™t sá»‘ trÆ°á»ng
há»£p (Khandelwal et al., 2020;arXiv:2306.13421v2  [cs.CL]  21 Jul 2024

--- TRANG 2 ---
Yogatama et al., 2021; Borgeaud et al., 2022), bá»™
truy xuáº¥t chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng táº¡i thá»i Ä‘iá»ƒm kiá»ƒm tra,
hoáº·c váº«n cá»‘ Ä‘á»‹nh trong suá»‘t quÃ¡ trÃ¬nh huáº¥n luyá»‡n,
ngÄƒn cáº£n nÃ³ thÃ­ch á»©ng vá»›i bá»™ táº¡o LM. Trong cÃ¡c
trÆ°á»ng há»£p khÃ¡c, thÃ nh pháº§n truy xuáº¥t Ä‘Æ°á»£c huáº¥n
luyá»‡n chung nhÆ°ng chá»‰ sau má»™t giai Ä‘oáº¡n tiá»n huáº¥n
luyá»‡n riÃªng biá»‡t cho cáº£ bá»™ truy xuáº¥t vÃ  LM (Sachan
et al., 2021; Izacard et al., 2022b; Jiang et al.,
2022; Bertsch et al., 2023). Do Ä‘Ã³, bá»™ truy xuáº¥t
khÃ´ng Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n tá»« Ä‘áº§u cÃ¹ng vá»›i LM,
vÃ  chá»‰ má»™t pháº§n ngÃ¢n sÃ¡ch huáº¥n luyá»‡n Ä‘Æ°á»£c phÃ¢n
bá»• cho huáº¥n luyá»‡n chung.
Gáº§n Ä‘Ã¢y, Zhong et al. (2022) Ä‘Ã£ trÃ¬nh bÃ y má»™t
LM tÄƒng cÆ°á»ng truy xuáº¥t huáº¥n luyá»‡n bá»™ truy xuáº¥t
tá»« Ä‘áº§u cÃ¹ng vá»›i LM, nhÆ°ng (a) bá»™ truy xuáº¥t Ä‘Æ°á»£c
huáº¥n luyá»‡n Ä‘á»ƒ khai thÃ¡c chá»‰ thÃ´ng tin tá»« vá»±ng, vÃ 
(b) thÃ´ng tin Ä‘Æ°á»£c truy xuáº¥t khÃ´ng Ä‘Æ°á»£c há»£p nháº¥t
á»Ÿ cáº¥p Ä‘á»™ biá»ƒu diá»…n trá»Ÿ láº¡i vÃ o LM.
Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y
Retrieval-Pretrained Transformer (RPT), má»™t LM
tÄƒng cÆ°á»ng truy xuáº¥t, trong Ä‘Ã³ bá»™ truy xuáº¥t lÃ  má»™t
thÃ nh pháº§n háº¡ng nháº¥t, Ä‘Æ°á»£c huáº¥n luyá»‡n chung tá»«
Ä‘áº§u vá»›i LM. RPT dá»±a trÃªn hai Ä‘Ã³ng gÃ³p ká»¹ thuáº­t.
Thá»© nháº¥t, vá» máº·t kiáº¿n trÃºc (xem HÃ¬nh 1), cÃ¡c biá»ƒu
diá»…n Ä‘áº§u vÃ o cho bá»™ truy xuáº¥t Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»«
chÃ­nh cÃ¡c biá»ƒu diá»…n LM (má»™t khÃ¡i niá»‡m chÃºng tÃ´i
gá»i lÃ  tá»± truy xuáº¥t), vÃ  cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c truy xuáº¥t
Ä‘Æ°á»£c há»£p nháº¥t trá»Ÿ láº¡i vÃ o bá»™ giáº£i mÃ£ LM Ä‘á»ƒ Ä‘Æ°a ra
dá»± Ä‘oÃ¡n tá»« tiáº¿p theo. Thá»© hai, chÃºng tÃ´i huáº¥n luyá»‡n
bá»™ truy xuáº¥t vá»›i má»™t hÃ m loss phá»¥ trá»£ khuyáº¿n khÃ­ch
truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn báº£n lÃ m tÄƒng xÃ¡c suáº¥t táº¡o
ra vÄƒn báº£n tiáº¿p theo. Cá»¥ thá»ƒ, vá»›i má»™t Ä‘oáº¡n Ä‘Æ°á»£c
táº¡o gáº§n Ä‘Ã¢y ct, bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ
truy xuáº¥t cÃ¡c Ä‘oáº¡n ci lÃ m tÄƒng xÃ¡c suáº¥t cá»§a viá»‡c
cháº¥m Ä‘iá»ƒm (ct+1|ci, ct) theo má»™t LM cháº¥m Ä‘iá»ƒm
tham chiáº¿u. HÃ¬nh 1 cung cáº¥p má»™t vÃ­ dá»¥ minh há»a
cho trÆ°á»ng há»£p má»™t hiá»‡n trÆ°á»ng tá»™i pháº¡m Ä‘Æ°á»£c
mÃ´ táº£, vÃ  má»™t LM cháº¥m Ä‘iá»ƒm cho tháº¥y lá»£i Ã­ch cá»§a
viá»‡c truy xuáº¥t má»™t Ä‘oáº¡n cÃ¡ch hÃ ng nghÃ¬n token
(Ä‘oáº¡n 13) so vá»›i truy xuáº¥t tá»« vá»±ng, dáº«n Ä‘áº¿n má»™t
Ä‘oáº¡n chá»‰ liÃªn quan vá» máº·t bá» ngoÃ i (Ä‘oáº¡n 100).
KhÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh tÄƒng cÆ°á»ng truy xuáº¥t hiá»‡n táº¡i
sá»­ dá»¥ng má»™t encoder phá»¥ trá»£ cho truy xuáº¥t (Izacard
and Grave, 2021a; Izacard et al., 2022b; Sachan
et al., 2021), RPT cÃ³ thá»ƒ táº­n dá»¥ng cÃ¡c tráº¡ng thÃ¡i
áº©n ná»™i bá»™ cho truy xuáº¥t sau má»™t giai Ä‘oáº¡n tiá»n huáº¥n
luyá»‡n duy nháº¥t, Ä‘Æ¡n giáº£n hÃ³a Ä‘Ã¡ng ká»ƒ viá»‡c huáº¥n
luyá»‡n chung.
ChÃºng tÃ´i Ã¡p dá»¥ng RPT cho váº¥n Ä‘á» mÃ´ hÃ¬nh hÃ³a
cÃ¡c tÃ i liá»‡u dÃ i, nhÆ° sÃ¡ch, bÃ i viáº¿t vÃ  mÃ£, vÃ¬ Ä‘Ã¢y
lÃ  nhá»¯ng vÃ­ dá»¥ tá»± nhiÃªn cá»§a ná»™i dung dáº¡ng dÃ i,
trong Ä‘Ã³ toÃ n bá»™ chá»‰ má»¥c cÃ³ thá»ƒ Ä‘Æ°á»£c giá»¯ trong
bá»™ nhá»› trong má»™t láº§n truyá»n xuÃ´i.ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT trÃªn bá»‘n nhiá»‡m vá»¥ mÃ´ hÃ¬nh
ngÃ´n ngá»¯ vÃ  tháº¥y ráº±ng nÃ³ cáº£i thiá»‡n perplexity trÃªn
táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥, vÆ°á»£t trá»™i hÆ¡n cÃ¡c nghiÃªn cá»©u
trÆ°á»›c (Hutchins et al., 2022; Wu et al., 2022) cÅ©ng
nhÆ° cÃ¡c baseline máº¡nh máº½ (Borgeaud et al., 2022;
Zhong et al., 2022). HÆ¡n ná»¯a, chÃºng tÃ´i cho tháº¥y
ráº±ng RPT truy xuáº¥t cÃ¡c Ä‘oáº¡n cháº¥t lÆ°á»£ng cao so vá»›i
cÃ¡c bá»™ truy xuáº¥t dá»±a trÃªn thÃ´ng tin tá»« vá»±ng. Dá»±a
trÃªn cÃ¡c phÃ¡t hiá»‡n thá»±c nghiá»‡m, chÃºng tÃ´i láº­p luáº­n
ráº±ng RPT cÃ³ thá»ƒ má»Ÿ Ä‘Æ°á»ng cho tháº¿ há»‡ tiáº¿p theo
cá»§a cÃ¡c LM Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n, trong Ä‘Ã³ cÃ¡c
kho ngá»¯ liá»‡u lá»›n Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh
tiá»n huáº¥n luyá»‡n, dáº«n Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯
trong Ä‘Ã³ truy xuáº¥t lÃ  má»™t thÃ nh pháº§n Ä‘Æ°á»£c nhÃºng
máº¡nh máº½. MÃ£ nguá»“n cá»§a chÃºng tÃ´i Ä‘Æ°á»£c cÃ´ng khai
táº¡i https://github.com/OhadRubin/RPT.
2 Ná»n táº£ng
Äá»ƒ Ä‘á»‹nh vá»‹ Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i, chÃºng tÃ´i xem
xÃ©t cÃ¡c nghiÃªn cá»©u RALM liÃªn quan gáº§n Ä‘Ã¢y. ChÃºng
tÃ´i má»Ÿ rá»™ng thÃªm cÃ¡c nghiÃªn cá»©u liÃªn quan khÃ¡c
trong Â§6.
CÃ¡c nghiÃªn cá»©u Ä‘áº§u tiÃªn vá» RALMs, nhÆ° kNN-LM
(Khandelwal et al., 2020) Ä‘Ã£ sá»­ dá»¥ng truy xuáº¥t Ä‘á»ƒ
cáº£i thiá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ báº±ng cÃ¡ch ná»™i suy phÃ¢n
phá»‘i tá»« tiáº¿p theo Ä‘Æ°á»£c táº¡o ra bá»Ÿi LM vá»›i má»™t phÃ¢n
phá»‘i Ä‘Æ°á»£c Ä‘á» xuáº¥t thÃ´ng qua cÆ¡ cháº¿ truy xuáº¥t chá»‰
táº¡i thá»i Ä‘iá»ƒm kiá»ƒm tra. Borgeaud et al. (2022) sau
Ä‘Ã³ Ä‘á» xuáº¥t Chunked Cross-Attention (CCA), trong
Ä‘Ã³ truy xuáº¥t cÅ©ng Ä‘Æ°á»£c thá»±c hiá»‡n táº¡i thá»i Ä‘iá»ƒm
huáº¥n luyá»‡n, vÃ  káº¿t quáº£ truy xuáº¥t Ä‘Æ°á»£c há»£p nháº¥t
sÃ¢u vÃ o cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c táº¡o ra bá»Ÿi má»™t bá»™ giáº£i
mÃ£ Transformer thÃ´ng qua attention. Tuy nhiÃªn,
bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n riÃªng biá»‡t vÃ  giá»¯
cá»‘ Ä‘á»‹nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, Ä‘iá»u nÃ y ngÄƒn
cáº£n nÃ³ thÃ­ch á»©ng vá»›i LM trong suá»‘t quÃ¡ trÃ¬nh huáº¥n
luyá»‡n.
TRIME (Zhong et al., 2022), giá»‘ng nhÆ° cÃ´ng
trÃ¬nh nÃ y, Ä‘Ã£ huáº¥n luyá»‡n má»™t LM tÄƒng cÆ°á»ng truy
xuáº¥t tá»« Ä‘áº§u trong Ä‘Ã³ thÃ nh pháº§n truy xuáº¥t vÃ  LM
giáº£i mÃ£ Ä‘Æ°á»£c huáº¥n luyá»‡n chung. CÃ´ng trÃ¬nh cá»§a
chÃºng tÃ´i khÃ¡c vá»›i TRIME á»Ÿ hai khÃ­a cáº¡nh: Thá»©
nháº¥t, TRIME, giá»‘ng nhÆ° kNN-LM, káº¿t há»£p thÃ´ng
tin tá»« bá»™ truy xuáº¥t theo cÃ¡ch nÃ´ng qua ná»™i suy
phÃ¢n phá»‘i, trong khi chÃºng tÃ´i Ã¡p dá»¥ng CCA nhÆ°
má»™t cÆ¡ cháº¿ há»£p nháº¥t sÃ¢u hÆ¡n. Thá»© hai, TRIME
táº­n dá»¥ng cÃ¡c manh má»‘i tá»« vá»±ng Ä‘á»ƒ giÃ¡m sÃ¡t bá»™
truy xuáº¥t, tá»©c lÃ  vá»›i má»™t truy váº¥n, bá»™ truy xuáº¥t
TRIME há»c cÃ¡ch truy xuáº¥t cÃ¡c ngá»¯ cáº£nh sáº½ dáº«n
Ä‘áº¿n viá»‡c táº¡o ra cÃ¹ng má»™t token vá»›i truy váº¥n. NgÆ°á»£c
láº¡i, chÃºng tÃ´i sá»­ dá»¥ng má»™t LM cháº¥m Ä‘iá»ƒm Ä‘á»ƒ Ä‘Ã¡nh
giÃ¡ nhá»¯ng Ä‘oáº¡n vÄƒn báº£n nÃ o cÃ³ liÃªn quan Ä‘á»ƒ tÄƒng
xÃ¡c suáº¥t cá»§a Ä‘oáº¡n Ä‘Æ°á»£c táº¡o ra, dáº«n Ä‘áº¿n truy xuáº¥t
ngá»¯ nghÄ©a hÆ¡n. Äiá»u nÃ y tÆ°Æ¡ng tá»± nhÆ° EPR (Rubin
et al., 2022), Ä‘Ã£ sá»­ dá»¥ng Ã½ tÆ°á»Ÿng nÃ y Ä‘á»ƒ há»c cÃ¡ch
truy xuáº¥t prompts cho há»c trong ngá»¯ cáº£nh, vÃ  chÆ°ng
cáº¥t perplexity trong Atlas (Izacard et al.,

--- TRANG 3 ---
2022b). Tuy nhiÃªn, Atlas khÃ´ng huáº¥n luyá»‡n bá»™
truy xuáº¥t vÃ  LM tá»« Ä‘áº§u vÃ  lÃ  má»™t mÃ´ hÃ¬nh encoder-
decoder, phÃ¹ há»£p hÆ¡n cho cÃ¡c nhiá»‡m vá»¥ Ä‘Ã²i há»i
kiáº¿n thá»©c nhiá»u. NgÆ°á»£c láº¡i, chÃºng tÃ´i huáº¥n luyá»‡n
tá»« Ä‘áº§u vÃ  sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh decoder, phÃ¹ há»£p
hÆ¡n cho viá»‡c mÃ´ hÃ¬nh hÃ³a vÄƒn báº£n dÃ i.
3 Retrieval-Pretrained Transformer
Thiáº¿t láº­p váº¥n Ä‘á» Giá»‘ng nhÆ° RETRO (Borgeaud
et al., 2022), RPT lÃ  má»™t LM tÄƒng cÆ°á»ng truy
xuáº¥t theo Ä‘oáº¡n chia chuá»—i Ä‘áº§u vÃ o thÃ nh cÃ¡c Ä‘oáº¡n
Ä‘á»ƒ truy xuáº¥t. Cá»¥ thá»ƒ, vá»›i má»™t chuá»—i L token Ä‘áº§u
vÃ o, (x1, x2, . . . , xL), chÃºng tÃ´i phÃ¢n chia nÃ³ thÃ nh
má»™t chuá»—i â„“ = L/m Ä‘oáº¡n khÃ´ng chá»“ng láº¥p cÃ³ Ä‘á»™
dÃ i m, kÃ½ hiá»‡u lÃ  C = (c1, c2, . . . , câ„“). Äá»‘i vá»›i
má»—i Ä‘oáº¡n truy váº¥n cÃ³ thá»ƒ, cq = ci, mÃ´ hÃ¬nh sáº½
truy xuáº¥t má»™t táº­p con tá»‘i Ä‘a K â‰ª â„“ Ä‘oáº¡n, R(cq) âŠ‚
C<i = (c1, c2, ..., ciâˆ’w), trong Ä‘Ã³ C<i lÃ  táº­p há»£p
cÃ¡c Ä‘oáº¡n cÃ³ thá»ƒ truy xuáº¥t cho ci, loáº¡i trá»« w Ä‘oáº¡n
mÃ  nÃ³ Ä‘Ã£ cÃ³ quyá»n truy cáº­p thÃ´ng qua causal self-
attention. Má»¥c tiÃªu lÃ  há»c má»™t mÃ´ hÃ¬nh truy xuáº¥t
má»™t táº­p con Ä‘oáº¡n, R(cq), lÃ m tÄƒng xÃ¡c suáº¥t táº¡o
tá»± Ä‘á»™ng há»“i quy cá»§a Ä‘oáº¡n má»¥c tiÃªu ct = ci+1.
ChÃºng tÃ´i trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i
trong hai pháº§n. Thá»© nháº¥t, kiáº¿n trÃºc cá»§a chÃºng tÃ´i
(Â§3.1), táº­n dá»¥ng CCA Ä‘á»ƒ há»£p nháº¥t cÃ¡c biá»ƒu diá»…n
Ä‘Æ°á»£c truy xuáº¥t vÃ o LM, nhÆ°ng thÃªm má»™t thÃ nh
pháº§n truy xuáº¥t Ä‘Æ°á»£c há»c. Thá»© hai, chÃºng tÃ´i trÃ¬nh
bÃ y phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n (Â§3.2-Â§3.3), trong
Ä‘Ã³ bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ truy xuáº¥t cÃ¡c
Ä‘oáº¡n há»¯u Ã­ch cho viá»‡c táº¡o ra má»™t Ä‘oáº¡n tÆ°Æ¡ng lai
theo má»™t LM tham chiáº¿u.
3.1 Kiáº¿n trÃºc mÃ´ hÃ¬nh
HÃ¬nh 2 minh há»a kiáº¿n trÃºc cá»§a chÃºng tÃ´i, trong Ä‘Ã³
Ä‘áº§u vÃ o cÃ³ 45 token Ä‘áº§u vÃ o Ä‘Æ°á»£c chia thÃ nh 9
Ä‘oáº¡n, vÃ  causal self-attention Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn
w = 3 Ä‘oáº¡n (15 token). PhÃ­a bÃªn trÃ¡i mÃ´ táº£ ngÄƒn
xáº¿p decoder ("reader"), vÃ  phÃ­a bÃªn pháº£i lÃ  bá»™
truy xuáº¥t. Reader Ä‘Æ°á»£c chia thÃ nh hai, trong Ä‘Ã³
nlayers/2 lá»›p dÆ°á»›i (lower decoder) lÃ  cÃ¡c lá»›p
Transformer decoder tiÃªu chuáº©n nháº­n w Ä‘oáº¡n lÃ m
Ä‘áº§u vÃ o vÃ  xuáº¥t ra cÃ¡c biá»ƒu diá»…n sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng
bá»Ÿi bá»™ truy xuáº¥t vÃ  cÃ¡c lá»›p decoder trÃªn.
nlayers/2 lá»›p trÃªn (upper decoder) sá»­ dá»¥ng
Chunked Cross-Attention (CCA) Ä‘á»ƒ há»£p nháº¥t thÃ´ng
tin tá»« cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m top-K Ä‘Æ°á»£c truy xuáº¥t
bá»Ÿi bá»™ truy xuáº¥t trá»Ÿ láº¡i vÃ o LM. ChÃºng tÃ´i sá»­ dá»¥ng
cÃ¡c lá»›p CCA tiÃªu chuáº©n tá»« RETRO (Borgeaud et
al., 2022), trong Ä‘Ã³ Ä‘á»‘i vá»›i má»—i má»™t trong â„“ Ä‘oáº¡n,
cÃ¡c truy váº¥n lÃ  cÃ¡c biá»ƒu diá»…n m token cá»§a Ä‘oáº¡n
Ä‘Ã³ Ä‘Æ°á»£c xuáº¥t ra bá»Ÿi causal attention, vÃ  cÃ¡c keys
vÃ  values lÃ  cÃ¡c biá»ƒu diá»…n token cho cÃ¡c Ä‘oáº¡n
hÃ ng xÃ³m top-K Ä‘Æ°á»£c xuáº¥t ra bá»Ÿi bá»™ truy xuáº¥t.Â¹
Tiáº¿p theo, chÃºng tÃ´i mÃ´ táº£ thÃ nh pháº§n truy xuáº¥t,
cÃ¹ng vá»›i má»™t cÆ¡ cháº¿ neighbor gating Ä‘á»ƒ Ä‘iá»u chá»‰nh
tÃ¡c Ä‘á»™ng cá»§a cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c truy xuáº¥t.
Bá»™ truy xuáº¥t Bá»™ truy xuáº¥t nháº­n cÃ¡c biá»ƒu diá»…n
Ä‘Æ°á»£c xuáº¥t ra bá»Ÿi lower decoder lÃ m Ä‘áº§u vÃ o vÃ 
táº¡o ra má»™t Ä‘iá»ƒm tÆ°Æ¡ng tá»± cho má»—i cáº·p Ä‘oáº¡n. Vá»›i
má»™t Ä‘oáº¡n truy váº¥n cq, Ä‘iá»ƒm dá»±a trÃªn truy váº¥n cho
má»—i Ä‘oáº¡n cÃ³ thá»ƒ truy xuáº¥t c lÃ  sq(c) = âŸ¨WQcq, WKcâŸ©,
trong Ä‘Ã³ WQ, WK âˆˆ RdÃ—d lÃ  cÃ¡c phÃ©p chiáº¿u tuyáº¿n
tÃ­nh Ä‘Æ°á»£c há»c, vÃ  cq vÃ  c lÃ  cÃ¡c biá»ƒu diá»…n Ä‘oáº¡n.
Äá»‘i vá»›i má»™t Ä‘oáº¡n c dÃ i m token, chÃºng tÃ´i tÃ­nh
toÃ¡n biá»ƒu diá»…n c cá»§a nÃ³ báº±ng cÃ¡ch Ã¡p dá»¥ng bidirectional
attention trÃªn cÃ¡c token cá»§a Ä‘oáº¡n, sau Ä‘Ã³ lÃ  mean-
pooling qua chiá»u thá»i gian. Äiá»u nÃ y duy trÃ¬ tÃ­nh
nhÃ¢n quáº£, vÃ¬ cÃ¡c biá»ƒu diá»…n nÃ y chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng
trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n Ä‘oáº¡n tiáº¿p theo.
Má»™t khi Ä‘iá»ƒm sá»‘ cho táº¥t cáº£ cÃ¡c cáº·p Ä‘oáº¡n Ä‘Æ°á»£c
tÃ­nh toÃ¡n, cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m Ä‘Æ°á»£c truy xuáº¥t R(cq),
cho má»—i Ä‘oáº¡n truy váº¥n, cq, bao gá»“m cÃ¡c Ä‘oáº¡n cÃ³
thá»ƒ truy xuáº¥t cÃ³ Ä‘iá»ƒm cao nháº¥t top-K. Sau Ä‘Ã³, Ä‘á»‘i
vá»›i má»—i Ä‘oáº¡n cj âˆˆ R(cq), chÃºng tÃ´i ná»‘i cÃ¡c biá»ƒu
diá»…n cá»§a Ä‘oáº¡n tiáº¿p theo cj+1 Ä‘á»ƒ cung cáº¥p ngá»¯
cáº£nh bá»• sung, vÃ  biá»ƒu diá»…n cuá»‘i cÃ¹ng cho táº¥t cáº£
cÃ¡c hÃ ng xÃ³m cá»§a táº¥t cáº£ cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c cho bá»Ÿi
má»™t tensor C âˆˆ Râ„“Ã—KÃ—2mÃ—d.Â²
Tá»•ng thá»ƒ (vÃ  khÃ¡c vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ°
TRIME vÃ  kNN-LM), bá»™ truy xuáº¥t lÃ  má»™t pháº§n
khÃ´ng thá»ƒ thiáº¿u cá»§a LM, trong Ä‘Ã³ lower decoder
tÃ­nh toÃ¡n cÃ¡c biá»ƒu diá»…n cho bá»™ truy xuáº¥t (mÃ  chÃºng
tÃ´i gá»i lÃ  tá»± truy xuáº¥t), vÃ  upper decoder tiÃªu thá»¥
cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c táº¡o ra bá»Ÿi bá»™ truy xuáº¥t.
Neighbor gating ChÃºng tÃ´i thÃªm má»™t cÆ¡ cháº¿
neighbor gating Ä‘á»ƒ lá»±a chá»n má»m cÃ¡c biá»ƒu diá»…n
hÃ ng xÃ³m há»¯u Ã­ch Ä‘á»ƒ há»£p nháº¥t vÃ o upper decoder.
Gá»i Ci,k âˆˆ R2mÃ—d lÃ  cÃ¡c biá»ƒu diá»…n token cho hÃ ng
xÃ³m thá»© k cá»§a Ä‘oáº¡n ci. ChÃºng tÃ´i mean-pool qua
chiá»u thá»i gian Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t vector Ä‰i,k cho
má»—i Ä‘oáº¡n hÃ ng xÃ³m. Sau Ä‘Ã³, chÃºng tÃ´i lÃ m phong
phÃº biá»ƒu diá»…n hÃ ng xÃ³m cá»§a má»—i Ä‘oáº¡n báº±ng cÃ¡ch
Ã¡p dá»¥ng causal attention â€“ má»™t biá»ƒu diá»…n Ä‘oáº¡n
hÃ ng xÃ³m Ä‰i,k attend tá»›i cÃ¡c Ä‘oáº¡n Ä‘i trÆ°á»›c nÃ³
hoáº·c tá»›i cÃ¡c hÃ ng xÃ³m cá»§a cÃ¹ng má»™t Ä‘oáº¡n ci cÃ³
thá»© háº¡ng cao hÆ¡n. Cuá»‘i cÃ¹ng, Ä‘á»‘i vá»›i má»—i Ä‘oáº¡n
chÃºng tÃ´i cÃ³ Ä‘Æ°á»£c biá»ƒu diá»…n Ä‘Æ°á»£c truy xuáº¥t cÃ³ gating
báº±ng cÃ¡ch nhÃ¢n cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c tÄƒng cÆ°á»ng vá»›i
má»™t Ä‘iá»ƒm gating:
Â¹Äá»ƒ biáº¿t Ä‘áº§y Ä‘á»§ chi tiáº¿t cá»§a CCA, xem Borgeaud et al. (2022).
Â²TÆ°Æ¡ng tá»± nhÆ° RETRO, cÃ¡c biá»ƒu diá»…n token cá»§a cÃ¡c Ä‘oáº¡n
Ä‘Æ°á»£c truy xuáº¥t cÅ©ng Ä‘Æ°á»£c tÄƒng cÆ°á»ng thÃ´ng qua cross-attention
trÃªn cÃ¡c token cá»§a Ä‘oáº¡n truy váº¥n, cq.

--- TRANG 4 ---
Causal AttentionFeed ForwardChunked Cross AttentionTop-KQKVCausal Attention
Pool + ProjectFeed Forward
Bi-directionalAttentionâŸ¨|||||||||â‹…aaaa		=7.1ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘1âŸ¨|||||||||â‹…aaaa		=0.3ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘2âŸ¨|||||||||â‹…aaaa		=5.2ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘3âŸ¨|||||||||â‹…aaaa		=10.8ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘!âŸ¨|||||||||â‹…aaaa		=âˆ’âˆğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘" â‹®âŸ¨|||||||||â‹…aaaa		=4.8ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘# â‹®READğ‘1ğ‘2ğ‘3ğ‘4ğ‘5ğ‘6ğ‘7ğ‘8ğ‘9Ã—	ğ‘›!"#$%&2Encoded neighbors
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1Input Tokens
Lower DecoderChunk scoringNeighbor gating
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1RetrieverUpper Decoder
Ã—	ğ‘›!"#$%&2HÃ¬nh 2: Kiáº¿n trÃºc cá»§a Retrieval-Pretrained Transformer, trong Ä‘Ã³ má»™t Ä‘áº§u vÃ o 45 token Ä‘Æ°á»£c hiá»ƒn thá»‹, bao gá»“m
9 Ä‘oáº¡n, vÃ  causal self-attention Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn 15 token. PhÃ­a bÃªn trÃ¡i hiá»ƒn thá»‹ ngÄƒn xáº¿p decoder, trong Ä‘Ã³
nlayers/2 lá»›p dÆ°á»›i lÃ  cÃ¡c lá»›p Transformer decoder tiÃªu chuáº©n, vÃ  nlayers/2 lá»›p trÃªn cÅ©ng bao gá»“m cÃ¡c lá»›p chunked
cross-attention há»£p nháº¥t thÃ´ng tin tá»« cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c truy xuáº¥t. PhÃ­a bÃªn pháº£i hiá»ƒn thá»‹ bá»™ truy xuáº¥t, nháº­n má»™t Ä‘oáº¡n
vÃ  truy xuáº¥t K Ä‘oáº¡n cÃ³ Ä‘iá»ƒm cao nháº¥t Ä‘Ã£ xuáº¥t hiá»‡n trÆ°á»›c Ä‘Ã³ trong tÃ i liá»‡u.
Cg
i,k = max {Î·, Ïƒ(wng Ä‰i,k/âˆšd)} Â· Ci,k trong Ä‘Ã³ wng
lÃ  má»™t vector tham sá»‘ Ä‘Æ°á»£c há»c, Î· lÃ  má»™t giÃ¡ trá»‹
nhá» nháº±m duy trÃ¬ dÃ²ng gradient,Â³ vÃ  Ïƒ lÃ  hÃ m kÃ­ch
hoáº¡t sigmoid. Cuá»‘i cÃ¹ng, trong upper decoder, khi
CCA Ä‘Æ°á»£c thá»±c hiá»‡n, cÃ¡c keys vÃ  values lÃ  Cg
i,k.
3.2 TÃ­n hiá»‡u giÃ¡m sÃ¡t
Äá»‘i vá»›i má»—i Ä‘oáº¡n truy váº¥n cq = ci, chÃºng tÃ´i muá»‘n
xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m sáº½ há»¯u Ã­ch cho viá»‡c
táº¡o ra ct = ci+1, vÃ  sá»­ dá»¥ng nhá»¯ng Ä‘oáº¡n hÃ ng
xÃ³m Ä‘Ã³ lÃ m tÃ­n hiá»‡u giÃ¡m sÃ¡t cho bá»™ truy xuáº¥t.
TÆ°Æ¡ng tá»± nhÆ° Rubin et al. (2022), chÃºng tÃ´i cÃ³
thá»ƒ khai thÃ¡c thá»±c táº¿ ráº±ng chÃºng tÃ´i Ä‘ang táº¡o ra
dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  sá»­ dá»¥ng thÃ´ng tin tá»« chÃ­nh
ct Ä‘á»ƒ táº¡o ra Ä‘iá»ƒm sá»‘ nhÆ° váº­y. KhÃ¡c vá»›i Zhong et
al. (2022), ngÆ°á»i chá»‰ sá»­ dá»¥ng cÃ¡c manh má»‘i tá»«
vá»±ng, chÃºng tÃ´i sáº½ sá»­ dá»¥ng má»™t LM cháº¥m Ä‘iá»ƒm
Ä‘á»™c láº­p cho má»¥c Ä‘Ã­ch nÃ y.
Viá»‡c cháº¥m Ä‘iá»ƒm má»—i Ä‘oáº¡n so vá»›i táº¥t cáº£ cÃ¡c Ä‘oáº¡n
trÆ°á»›c Ä‘Ã³ lÃ  báº­c hai theo sá»‘ lÆ°á»£ng Ä‘oáº¡n trong má»™t
tÃ i liá»‡u, vÃ  do Ä‘Ã³ khÃ³ khÄƒn vá» máº·t tÃ­nh toÃ¡n. Do
Ä‘Ã³, chÃºng tÃ´i sá»­ dá»¥ng má»™t bá»™ truy xuáº¥t khÃ´ng giÃ¡m
sÃ¡t BM25 Ä‘Æ¡n giáº£n (Robertson and Zaragoza, 2009)
nháº­n lÃ m Ä‘áº§u vÃ o sá»± ná»‘i cá»§a cÃ¡c Ä‘oáº¡n (cq, ct) =
(ci, ci+1) vÃ  tráº£ vá» má»™t táº­p há»£p cÃ¡c Ä‘oáº¡n hÃ ng
xÃ³m á»©ng viÃªn, RÌ„ âŠ‚ C(cq), cÃ³ Ä‘á»™ chá»“ng láº¯p tá»«
vá»±ng cao vá»›i Ä‘oáº¡n hiá»‡n táº¡i vÃ  tiáº¿p theo. Bá»™ truy
xuáº¥t nÃ y cÃ³ quyá»n truy cáº­p vÃ o cÃ¡c token cáº§n Ä‘Æ°á»£c
táº¡o ra bá»Ÿi LM, Ä‘iá»u nÃ y Ä‘Æ°á»£c phÃ©p táº¡i thá»i Ä‘iá»ƒm
huáº¥n luyá»‡n.
Gá»i Ä lÃ  má»™t LM Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p, vÃ  gá»i
cÌ„j lÃ  sá»± ná»‘i (cj, cj+1). ChÃºng tÃ´i tÃ­nh toÃ¡n má»™t
Ä‘iá»ƒm sá»‘ st(cÌ„j) pháº£n Ã¡nh liá»‡u thÃ´ng tin trong cÌ„j
cÃ³ há»¯u Ã­ch hÆ¡n cho viá»‡c giáº£i mÃ£ ct so vá»›i cÃ¡c Ä‘oáº¡n
gáº§n vá»›i cq hay khÃ´ng. Cá»¥ thá»ƒ, Ä‘iá»ƒm sá»‘ dá»±a trÃªn
má»¥c tiÃªu cho má»™t Ä‘oáº¡n á»©ng viÃªn lÃ 
st(cÌ„j) = log Prob Ä(ct|cj, cj+1, cq) / Prob Ä(ct|ciâˆ’2, ciâˆ’1, cq).
Äiá»ƒm sá»‘ nÃ y lÃ  dÆ°Æ¡ng khi thÃ´ng tin trong cÌ„j há»¯u
Ã­ch hÆ¡n cho viá»‡c giáº£i mÃ£ ct so vá»›i thÃ´ng tin trong
hai Ä‘oáº¡n trÆ°á»›c Ä‘Ã³ (ciâˆ’2, ciâˆ’1).
ChÃºng tÃ´i Ã¡p dá»¥ng hÃ m cháº¥m Ä‘iá»ƒm nÃ y cho táº¥t
cáº£ cÃ¡c Ä‘oáº¡n, vÃ  Ä‘á»‹nh nghÄ©a cho má»—i Ä‘oáº¡n truy váº¥n
cq táº­p há»£p cÃ¡c Ä‘oáº¡n dÆ°Æ¡ng Rq
pos, bao gá»“m cÃ¡c á»©ng
viÃªn mÃ  st(Â·) > 0. Äiá»u nÃ y sáº½ dáº«n Ä‘áº¿n cÃ¡c Ä‘oáº¡n
há»¯u Ã­ch, vÃ¬ má»—i Ä‘oáº¡n á»©ng viÃªn Ã­t nháº¥t cÅ©ng tá»‘t
báº±ng ngá»¯ cáº£nh cá»¥c bá»™. Vá»›i thá»© tá»± nÃ y, chÃºng tÃ´i
cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n truy
xuáº¥t tiÃªu chuáº©n.
3.3 Huáº¥n luyá»‡n
Äá»ƒ huáº¥n luyá»‡n cÃ¡c tham sá»‘ cá»§a thÃ nh pháº§n truy
xuáº¥t, chÃºng tÃ´i Ä‘iá»u chá»‰nh loss LambdaRank Ä‘Æ°á»£c
sá»­ dá»¥ng rá»™ng rÃ£i (Burges et al., 2006). Loss cho
má»—i Ä‘oáº¡n truy váº¥n
Â³ChÃºng tÃ´i Ä‘áº·t Î· = 0.1 trong táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i.

--- TRANG 5 ---
cq (w.r.t cÃ¡c Ä‘oáº¡n cÃ³ thá»ƒ truy xuáº¥t cá»§a nÃ³) lÃ :
Lret(cq) =
X
{j,l:cÌ„lâˆˆRq
pos,st(cÌ„l)>st(cÌ„j)}Î»jl max (0 , Ï„âˆ’(sq(cl)âˆ’sq(cj)))
trong Ä‘Ã³ Ï„ lÃ  má»™t siÃªu tham sá»‘ margin, vÃ  Î»jl lÃ 
tá»· lá»‡ LambdaRank xem xÃ©t thá»© háº¡ng tÆ°Æ¡ng Ä‘á»‘i cá»§a
má»—i á»©ng viÃªn. Loss nÃ y khÃ¡c khÃ´ng khi Ä‘á»‘i vá»›i
má»™t sá»‘ cáº·p á»©ng viÃªn, Ä‘iá»ƒm sá»‘ dá»±a trÃªn má»¥c tiÃªu
khÃ´ng Ä‘á»“ng Ã½ (vá»›i margin Ï„) vá»›i thá»© háº¡ng cá»§a
Ä‘iá»ƒm sá»‘ dá»±a trÃªn truy váº¥n cho cÃ¡c á»©ng viÃªn trong
Rq
pos. Tá»‘i Æ°u hÃ³a hÃ m loss nÃ y cho phÃ©p RPT phÃ¢n
biá»‡t giá»¯a cÃ¡c Ä‘oáº¡n liÃªn quan vÃ  khÃ´ng liÃªn quan.
Loss cuá»‘i cÃ¹ng cá»§a chÃºng tÃ´i lÃ  LLM + Î±retLret,
trong Ä‘Ã³ LLM lÃ  loss LM tiÃªu chuáº©n vÃ  Î±ret lÃ  há»‡
sá»‘ loss truy xuáº¥t, tÄƒng tuyáº¿n tÃ­nh trong 100K bÆ°á»›c
Ä‘áº§u tiÃªn. ChÃºng tÃ´i cÅ©ng tÄƒng Ï„ tuyáº¿n tÃ­nh trong
quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
3.4 Chi tiáº¿t triá»ƒn khai quan trá»ng
Scheduled sampling Äá»ƒ giáº£m sá»± khÃ´ng khá»›p
train-test, chÃºng tÃ´i Ã¡p dá»¥ng scheduled sampling
(Bengio et al., 2015) trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
Cá»¥ thá»ƒ, sau khi tÃ­nh toÃ¡n cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m top-K,
chÃºng tÃ´i sá»­ dá»¥ng nhá»¯ng hÃ ng xÃ³m nÃ y vá»›i xÃ¡c
suáº¥t 1âˆ’pss, vÃ  vá»›i xÃ¡c suáº¥t pss cÃ¡c á»©ng viÃªn cÃ³
Ä‘iá»ƒm cao nháº¥t top-K tá»« Rq
pos lÃ m Ä‘áº§u vÃ o cho
CCA. ChÃºng tÃ´i giáº£m dáº§n pss tá»« 1 vá» 0 trong 90%
Ä‘áº§u cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n vá»›i lá»‹ch trÃ¬nh cosine.
Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh dáº§n dáº§n há»c cÃ¡ch sá»­
dá»¥ng cÃ¡c dá»± Ä‘oÃ¡n cá»§a chÃ­nh nÃ³. ChÃºng tÃ´i bÃ¡o cÃ¡o
tÃ¡c Ä‘á»™ng cá»§a Ä‘iá»u nÃ y trong Â§5.3.
Sliding window attention táº¡i thá»i Ä‘iá»ƒm huáº¥n
luyá»‡n vÃ  suy luáº­n NhÆ° Ä‘Ã£ mÃ´ táº£ trong Â§3, decoder
nháº­n w Ä‘oáº¡n lÃ m Ä‘áº§u vÃ o, má»—i Ä‘oáº¡n cÃ³ m token
lÃ m Ä‘áº§u vÃ o, vÃ  Ã¡p dá»¥ng causal attention trÃªn
chÃºng. Trong thá»±c táº¿, Ä‘á»ƒ cho cÃ¡c token Ä‘áº§u tiÃªn
truy cáº­p vÃ o cÃ¡c token quÃ¡ khá»©, chÃºng tÃ´i sá»­ dá»¥ng
cÆ¡ cháº¿ sliding-window attention (Dai et al., 2019;
Beltagy et al., 2020; Ivgi et al., 2023), trong Ä‘Ã³
sá»‘ lÆ°á»£ng token trong má»™t cá»­a sá»• lÃ  2,048 vÃ  stride
lÃ  1,024. Do Ä‘Ã³, Ä‘áº§u vÃ o cho má»—i cá»­a sá»• lÃ  2,048
token vÃ  Ä‘áº§u ra lÃ  cÃ¡c biá»ƒu diá»…n cho 1,024 token
cuá»‘i cÃ¹ng, sá»­ dá»¥ng cÃ¡c keys vÃ  values cá»§a 1,024
token trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ táº¡o ngá»¯ cáº£nh.
Táº¡i thá»i Ä‘iá»ƒm suy luáº­n, má»™t quy trÃ¬nh tÆ°Æ¡ng tá»±
Ä‘Æ°á»£c Ã¡p dá»¥ng. ChÃºng tÃ´i tÃ­nh toÃ¡n vÃ  cache cÃ¡c
biá»ƒu diá»…n key vÃ  value cho cÃ¡c Ä‘oáº¡n 1,024 token,
sá»­ dá»¥ng chÃºng lÃ m ngá»¯ cáº£nh Ä‘á»ƒ táº¡o ra hoáº·c Æ°á»›c
tÃ­nh xÃ¡c suáº¥t cá»§a Ä‘oáº¡n tiáº¿p theo.
Truy xuáº¥t táº¡i thá»i Ä‘iá»ƒm suy luáº­n Trong quÃ¡
trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i mÃ£ hÃ³a trong má»—i batch
cÃ¡c chuá»—i cÃ³ Ä‘á»™ dÃ i 16K vÃ  truy xuáº¥t cÃ¡c Ä‘oáº¡n tá»«
nhá»¯ng 16k token Ä‘Æ°á»£c mÃ£ hÃ³a Ä‘Ã³.TÃªn Tokens (Train/Test) Äá»™ dÃ i Trung vá»‹
ArXiv 12,000 / 16 16,368
CodeParrot 5,000 / 5 29,269
PG19 3,000 / 9 82,659
Books3 25,000 / 35 113,496
Báº£ng 1: Sá»‘ lÆ°á»£ng token (tÃ­nh báº±ng triá»‡u) cho má»—i dataset
vÃ  Ä‘á»™ dÃ i tÃ i liá»‡u trung vá»‹.
Tuy nhiÃªn, táº¡i thá»i Ä‘iá»ƒm suy luáº­n, bá»™ truy xuáº¥t
cung cáº¥p quyá»n truy cáº­p vÃ o táº¥t cáº£ cÃ¡c token tá»«
Ä‘áº§u tÃ i liá»‡u, trong Ä‘Ã³ chÃºng tÃ´i lÆ°u trá»¯ cÃ¡c biá»ƒu
diá»…n key vÃ  lower-decoder trong má»™t chá»‰ má»¥c Faiss
(Douze et al., 2024) trÃªn CPU. Äá»‘i vá»›i má»—i Ä‘oáº¡n,
chÃºng tÃ´i truy váº¥n chá»‰ má»¥c sá»­ dá»¥ng cÃ¡c biá»ƒu diá»…n
truy váº¥n cá»§a Ä‘oáº¡n vÃ  truy xuáº¥t cÃ¡c biá»ƒu diá»…n lower-
decoder top-K cÃ³ tÃ­ch vÃ´ hÆ°á»›ng cao nháº¥t.
Chi tiáº¿t bá»• sung Táº¡i thá»i Ä‘iá»ƒm huáº¥n luyá»‡n, chÃºng
tÃ´i sá»­ dá»¥ng cÃ¡c chuá»—i cÃ³ Ä‘á»™ dÃ i L = 16,384 token,
Ä‘Æ°á»£c chia thÃ nh 4 thiáº¿t bá»‹, má»—i thiáº¿t bá»‹ tiÃªu thá»¥
4,096 token. NhÆ° Ä‘Ã£ Ä‘á» cáº­p, ngÄƒn xáº¿p decoder
nháº­n 2,048 token lÃ m Ä‘áº§u vÃ o (theo phÆ°Æ¡ng phÃ¡p
sliding window), chá»©a â„“ = 32 Ä‘oáº¡n cÃ³ Ä‘á»™ dÃ i m = 64.
ChÃºng tÃ´i sá»­ dá»¥ng Rotary Positional embedding
(Su et al., 2024), vÃ  huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh
trong 500K bÆ°á»›c trÃªn TPUv4-64, vá»›i kÃ­ch thÆ°á»›c
batch hiá»‡u quáº£ lÃ  217 token dáº«n Ä‘áº¿n tá»•ng ngÃ¢n
sÃ¡ch huáº¥n luyá»‡n 65 tá»· token.
Äá»‘i vá»›i táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n, chÃºng
tÃ´i sá»­ dá»¥ng tokenizer GPT-NeoX (Black et al.,
2022), Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn Pile (Gao et al., 2020)
vÃ  bao phá»§ cÃ¡c miá»n chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ (xem Â§4).
LÃ m mÃ´ hÃ¬nh ngÃ´n ngá»¯ cháº¥m Ä‘iá»ƒm, chÃºng tÃ´i sá»­
dá»¥ng phiÃªn báº£n deduplicated 1.4B tham sá»‘ cá»§a
Pythia (Biderman et al., 2023), vÃ  cháº¥m Ä‘iá»ƒm vá»›i
nÃ³ 20 á»©ng viÃªn BM25 hÃ ng Ä‘áº§u. MÃ´ hÃ¬nh cá»§a chÃºng
tÃ´i cÃ³ 12 lá»›p, chiá»u áº©n d = 1024, vÃ  8 attention
head vá»›i chiá»u head lÃ  128. ChÃºng tÃ´i Ã¡p dá»¥ng
CCA vá»›i 2 hÃ ng xÃ³m, trá»« khi Ä‘Æ°á»£c Ä‘á» cáº­p khÃ¡c.
Chi tiáº¿t triá»ƒn khai bá»• sung cÃ³ trong Phá»¥ lá»¥c A vÃ 
Ä‘á»™ phá»©c táº¡p lÃ½ thuyáº¿t cá»§a cÃ¡c lá»›p CCA cÃ³ trong
Phá»¥ lá»¥c B.
4 Datasets MÃ´ hÃ¬nh NgÃ´n ngá»¯ Táº§m xa
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT trÃªn bá»‘n dataset, bao phá»§
cÃ¡c miá»n nhÆ° sÃ¡ch, mÃ£ vÃ  vÄƒn báº£n toÃ¡n há»c, Ä‘Ã²i
há»i kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin trÃªn khoáº£ng cÃ¡ch
xa. Báº£ng 1 vÃ  HÃ¬nh 3 cung cáº¥p thá»‘ng kÃª vá» kÃ­ch
thÆ°á»›c dataset vÃ  phÃ¢n phá»‘i Ä‘á»™ dÃ i tÃ i liá»‡u, cho
tháº¥y ráº±ng cÃ¡c tÃ i liá»‡u dÃ i trÃªn táº¥t cáº£ cÃ¡c dataset
vÃ  Ä‘áº·c biá»‡t lÃ  PG19 vÃ  Books3, nÆ¡i cÃ¡c tÃ i liá»‡u
thÆ°á»ng chá»©a 105 token hoáº·c nhiá»u hÆ¡n. ChÃºng tÃ´i
xem xÃ©t ngáº¯n gá»n cÃ¡c dataset.

--- TRANG 6 ---
0510%ArXiv
0510%CodeParrot
0510%PG19
102103104105106107
Sequence length0510%Books3HÃ¬nh 3: Biá»ƒu Ä‘á»“ phÃ¢n phá»‘i Ä‘á»™ dÃ i tÃ i liá»‡u tÃ­nh báº±ng token
trÃªn táº¥t cáº£ cÃ¡c dataset. Trá»¥c x á»Ÿ thang Ä‘o log.
PG19 ÄÆ°á»£c giá»›i thiá»‡u trong Rae et al. (2020),
PG19 lÃ  má»™t benchmark mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa
Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i chá»©a cÃ¡c cuá»‘n sÃ¡ch tá»« Project
Gutenberg, vÃ  bao phá»§ má»™t loáº¡t cÃ¡c thá»ƒ loáº¡i vÄƒn
há»c, phong cÃ¡ch vÃ  chá»§ Ä‘á». ChÃºng tÃ´i Ã¡p dá»¥ng thiáº¿t
láº­p vÃ  phÃ¢n chia dá»¯ liá»‡u chÃ­nh xÃ¡c tá»« cÃ¡c nghiÃªn
cá»©u trÆ°á»›c (Wu et al., 2022; Hutchins et al., 2022;
Mehta et al., 2023).
Books3 lÃ  má»™t corpus sÃ¡ch Ä‘Æ°á»£c phÃ¡t hÃ nh nhÆ°
má»™t pháº§n cá»§a Pile (Gao et al., 2020), chá»©a má»™t
bá»™ sÆ°u táº­p rá»™ng lá»›n cÃ¡c tÃ¡c pháº©m vÄƒn há»c tá»« cÃ¡c
miá»n khÃ¡c nhau. Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i,
chÃºng tÃ´i lÃ  nhá»¯ng ngÆ°á»i Ä‘áº§u tiÃªn sá»­ dá»¥ng corpus
nÃ y nhÆ° má»™t benchmark mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m
xa.â´
CodeParrot (Wolf et al., 2023) lÃ  má»™t corpus
mÃ£ Python sáº¡ch, gáº§n nhÆ° khÃ´ng trÃ¹ng láº·p tá»« cÃ¡c
kho GitHub khÃ¡c nhau. MÃ´ hÃ¬nh hÃ³a mÃ£ Ä‘Ã²i há»i
hiá»ƒu cÃ¡c máº«u vÃ  ngá»¯ cáº£nh hÃ³a thÃ´ng tin trÃªn khoáº£ng
cÃ¡ch xa, lÃ m cho nÃ³ trá»Ÿ thÃ nh má»™t á»©ng viÃªn tá»±
nhiÃªn Ä‘á»ƒ kiá»ƒm tra cÃ¡c LM táº§m xa. Trong cÃ¡c thÃ­
nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i lÃ m theo phÆ°Æ¡ng
phÃ¡p cá»§a Wu et al. (2022), káº¿t há»£p cÃ¡c tá»‡p tá»« cÃ¹ng
má»™t kho Ä‘á»ƒ xÃ¢y dá»±ng má»™t corpus vá»›i cÃ¡c chuá»—i dÃ i
hÆ¡n, vÃ  táº¡o ra má»™t phÃ¢n chia train/test (xem Báº£ng 1).
ArXiv lÃ  má»™t corpus cÃ¡c bÃ i bÃ¡o preprint Ä‘Æ°á»£c
trÃ­ch xuáº¥t tá»« ArXiv. NÃ³ bao gá»“m cÃ¡c vÄƒn báº£n toÃ¡n
há»c Ä‘Ã²i há»i duy trÃ¬ tÃ­nh nháº¥t quÃ¡n vÃ  tham chiáº¿u
Ä‘áº¿n thÃ´ng tin Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³ trÃªn vÄƒn báº£n má»Ÿ
rá»™ng.
â´ChÃºng tÃ´i khÃ´ng phÃ¡t hÃ nh benchmark nÃ y do cÃ¡c háº¡n cháº¿
báº£n quyá»n.CÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã£ Ä‘Ã¡nh giÃ¡ cÃ¡c LM táº§m xa
trÃªn corpus nÃ y (Wu et al., 2022; Hutchins et al.,
2022; Mehta et al., 2023), nhÆ°ng khÃ´ng phÃ¡t hÃ nh
corpus cá»§a há». Do Ä‘Ã³, chÃºng tÃ´i sá»­ dá»¥ng corpus
Ä‘Æ°á»£c tiá»n xá»­ lÃ½ vÃ  phÃ¢n chia dá»¯ liá»‡u do Azerbayev
et al. (2023) cung cáº¥p.
5 ThÃ­ nghiá»‡m
BÃ¢y giá» chÃºng tÃ´i chuyá»ƒn sang cÃ¡c thÃ­ nghiá»‡m Ä‘á»ƒ
so sÃ¡nh RPT vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c trÃªn bá»‘n
dataset cá»§a chÃºng tÃ´i.
5.1 Thiáº¿t láº­p thÃ­ nghiá»‡m
ChÃºng tÃ´i so sÃ¡nh vá»›i cÃ¡c baseline vÃ  oracle sau.
Transformer-XL Baseline Ä‘Æ¡n giáº£n nháº¥t cá»§a chÃºng
tÃ´i lÃ  má»™t ngÄƒn xáº¿p transformer decoder tiÃªu chuáº©n
vá»›i sliding window attention. NÃ³i cÃ¡ch khÃ¡c, chÃºng
tÃ´i chá»‰ Ä‘Æ¡n giáº£n loáº¡i bá» khá»i RPT thÃ nh pháº§n truy
xuáº¥t vÃ  cÃ¡c lá»›p CCA trong upper decoder. Sá»­ dá»¥ng
sliding window attention (nhÆ° Ä‘Ã£ mÃ´ táº£ trong Â§3.4)
cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t biáº¿n thá»ƒ cá»§a Transformer-
XL (Dai et al., 2019). ChÃºng tÃ´i so sÃ¡nh RPT vá»›i
Transformer-XL trong nhiá»u thiáº¿t láº­p, má»™t trong
Ä‘Ã³ chÃºng tÃ´i cÃ³ cÃ¹ng sá»‘ lá»›p vÃ  bÆ°á»›c huáº¥n luyá»‡n
cho cáº£ hai mÃ´ hÃ¬nh, vÃ  hai thiáº¿t láº­p khÃ¡c trong Ä‘Ã³
chÃºng tÃ´i cÃ¢n báº±ng sá»‘ tham sá»‘ vÃ  FLOPs giá»¯a cÃ¡c
mÃ´ hÃ¬nh.
RETRO ChÃºng tÃ´i triá»ƒn khai má»™t phiÃªn báº£n chá»‰nh
sá»­a cá»§a Borgeaud et al. (2022), má»™t mÃ´ hÃ¬nh tÄƒng
cÆ°á»ng truy xuáº¥t, trong Ä‘Ã³ chÃºng tÃ´i cung cáº¥p cÃ¡c
hÃ ng xÃ³m top-K Ä‘Æ°á»£c truy xuáº¥t bá»Ÿi BM25âµ lÃ m
Ä‘áº§u vÃ o cho cÃ¡c lá»›p CCA trong upper decoder.
Cá»¥ thá»ƒ, Borgeaud et al. (2022) Ä‘Ã£ thá»±c hiá»‡n CCA
trÃªn biá»ƒu diá»…n tá»« má»™t encoder song hÆ°á»›ng riÃªng
biá»‡t, trong khi biáº¿n thá»ƒ cá»§a chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c
biá»ƒu diá»…n lower-decoder nhÆ° má»™t sá»± thay tháº¿. Äiá»u
nÃ y lÃ m cho kiáº¿n trÃºc RPT vÃ  RETRO tÆ°Æ¡ng tá»±
nhau hÆ¡n vÃ  cho phÃ©p Ä‘Ã¡nh giÃ¡ táº­p trung vÃ o táº§m
quan trá»ng cá»§a viá»‡c huáº¥n luyá»‡n bá»™ truy xuáº¥t, Ä‘Ã¢y
lÃ  trá»ng tÃ¢m cá»§a cÃ´ng trÃ¬nh chÃºng tÃ´i. Trong quÃ¡
trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i sá»­ dá»¥ng truy váº¥n (cq, ct),
vÃ¬ chÃºng tÃ´i cÃ³ quyá»n truy cáº­p vÃ o Ä‘oáº¡n má»¥c tiÃªu.
Trong quÃ¡ trÃ¬nh suy luáº­n, chÃºng tÃ´i sá»­ dá»¥ng cq.
RPT-Lex Má»™t phiÃªn báº£n cá»§a RPT, trong Ä‘Ã³ tÃ­n
hiá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c láº¥y hoÃ n toÃ n tá»« thÃ´ng tin
tá»« vá»±ng, tÆ°Æ¡ng tá»± nhÆ° TRIME (Zhong et al., 2022).
RÃµ rÃ ng, táº­p há»£p cÃ¡c Ä‘oáº¡n dÆ°Æ¡ng Rq
pos cho má»™t
Ä‘oáº¡n cq chá»©a 20 Ä‘oáº¡n hÃ ng Ä‘áº§u cÃ³ Ä‘iá»ƒm BM25
cao nháº¥t vá»›i (cq, ct).
RPT-Sem MÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c mÃ´
táº£ trong Â§3.
âµNghiÃªn cá»©u Ä‘á»“ng thá»i (Doostmohammadi et al., 2023) cho
tháº¥y ráº±ng huáº¥n luyá»‡n RETRO sá»­ dá»¥ng BM25 vÆ°á»£t trá»™i hÆ¡n
cÃ¡c phÆ°Æ¡ng phÃ¡p truy xuáº¥t dense.

--- TRANG 7 ---
Model ArXiv Code PG19 Books3 Params Time/update
TRANSFORMER -XL(OUR IMPL .) 3.11 2.30 11.48 15.00 202M 1 Ã—
+2LAYERS 3.07 2.26 11.2 14.52 228M 1.14 Ã—
1.5Ã—ADDITIONAL STEPS 3.11 2.26 11.39 14.70 202M 1 Ã—
RETRO W . BM25 ( OUR IMPL .) 2.94 2.17 11.44 14.60 236M 1.35 Ã—
RPT-L EX 2.92 2.23 11.59 14.32 242M 1.51 Ã—
RPT-S EM 2.77 2.17 10.96 13.91 242M 1.51 Ã—
W. 3NEIGHBOURS 2.75 2.16 10.92 13.87
W. 4NEIGHBOURS 2.74 2.15 10.93 13.91
MEMORIZING TRANSFORMER (32K) 2.92 2.18 10.97 14.40 212M 1.82 Ã—
MEMORIZING TRANSFORMER (65K) 2.93 2.15 10.99 14.3 212M 2.12 Ã—
BLOCK -RECURRENT TRANSFORMER 2.89 2.73 10.95 14.64 212M 1.56 Ã—
GRIFFIN 3.08 2.24 11.26 14.16 240M 1.15 Ã—
RPT-L EX W . ORACLE 2.80 2.12 10.88 13.30 242M 1.51 Ã—
RPT-S EM W . ORACLE 2.69 2.10 10.26 12.74 242M 1.51 Ã—
Báº£ng 2: Perplexity táº­p kiá»ƒm tra cho táº¥t cáº£ cÃ¡c dataset cÃ¹ng vá»›i sá»‘ lÆ°á»£ng tham sá»‘ vÃ  má»©c tÄƒng tÆ°Æ¡ng Ä‘á»‘i vá» thá»i gian
má»—i láº§n cáº­p nháº­t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n so vá»›i Transformer-XL. Trá»« khi Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n
luyá»‡n trong 500k bÆ°á»›c vÃ  sá»­ dá»¥ng 2 hÃ ng xÃ³m trong quÃ¡ trÃ¬nh suy luáº­n.
Block-Recurrent Transformer ChÃºng tÃ´i sá»­ dá»¥ng
triá»ƒn khai huáº¥n luyá»‡n chÃ­nh thá»©câ¶ cá»§a Block-
Recurrent Transformer (Hutchins et al., 2022) vá»›i
cáº¥u hÃ¬nh máº·c Ä‘á»‹nh.
Memorizing Transformer ChÃºng tÃ´i sá»­ dá»¥ng triá»ƒn
khai chÃ­nh thá»©câ¶ cá»§a Memorizing Transformers
(Wu et al., 2022), vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh vÃ  kÃ­ch
thÆ°á»›c bá»™ nhá»› 32K vÃ  65K token.
Griffin Má»™t lá»±a chá»n thay tháº¿ cho mÃ´ hÃ¬nh táº§m
xa lÃ  sá»­ dá»¥ng má»™t hybrid cá»§a attention vÃ  RNN
tuyáº¿n tÃ­nh (Orvieto et al., 2023; Gupta et al., 2023).
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Griffin (De et al., 2024), má»™t
mÃ´ hÃ¬nh tiÃªn tiáº¿n trong loáº¡i nÃ y. ChÃºng tÃ´i Ä‘iá»u
chá»‰nh triá»ƒn khai chÃ­nh thá»©c, vÃ  bá»• sung baseline
Transformer-XL cá»§a chÃºng tÃ´i vá»›i 5 lá»›p há»“i quy
trong cÃ¡c lá»›p cuá»‘i cÃ¹ng Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh ngang
báº±ng vá» tham sá»‘. ChÃºng tÃ´i sá»­ dá»¥ng chiá»u tráº¡ng
thÃ¡i 2,048 vÃ  chiá»u thá»i gian 3.
Oracle Äá»‘i vá»›i má»—i Ä‘oáº¡n kiá»ƒm tra, chÃºng tÃ´i cÃ³
thá»ƒ tÃ¬m kiáº¿m toÃ n diá»‡n vÃ  sá»­ dá»¥ng táº¡i thá»i Ä‘iá»ƒm
kiá»ƒm tra cÃ¡c hÃ ng xÃ³m tá»‘t nháº¥t cÃ³ thá»ƒ cho má»™t
mÃ´ hÃ¬nh theo LM cháº¥m Ä‘iá»ƒm. Äiá»u nÃ y cung cáº¥p
má»™t cáº­n trÃªn cho hiá»‡u suáº¥t cá»§a RPT-Sem, vÃ¬ nÃ³
Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ báº¯t chÆ°á»›c thá»© háº¡ng Ä‘Æ°á»£c táº¡o
ra bá»Ÿi oracle nÃ y.
Metrics ChÃºng tÃ´i sá»­ dá»¥ng perplexity Ä‘á»ƒ Ä‘Ã¡nh
giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh. NgoÃ i ra, chÃºng tÃ´i
sá»­ dá»¥ng Ä‘iá»ƒm má»¥c tiÃªu st(Â·) tá»« LM cháº¥m Ä‘iá»ƒm Ä‘á»ƒ
tÃ­nh toÃ¡n cho má»—i Ä‘oáº¡n má»™t thá»© háº¡ng vÃ ng trÃªn
táº¥t cáº£ cÃ¡c Ä‘oáº¡n trÆ°á»›c Ä‘Ã³, vÃ  gÃ¡n nhÃ£n cÃ¡c Ä‘oáº¡n
lÃ  dÆ°Æ¡ng/Ã¢m náº¿u Ä‘iá»ƒm má»¥c tiÃªu cá»§a chÃºng lÃ 
dÆ°Æ¡ng/Ã¢m, tÆ°Æ¡ng á»©ng. Vá»›i thÃ´ng tin nÃ y, chÃºng
tÃ´i cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ Precision@k, lÃ  tá»· lá»‡ cÃ¡c Ä‘oáº¡n
top-k theo Ä‘iá»ƒm dá»±a trÃªn truy váº¥n mÃ  lÃ  dÆ°Æ¡ng,
vÃ  Recall@k, lÃ  tá»· lá»‡ cÃ¡c Ä‘oáº¡n dÆ°Æ¡ng náº±m trong
cÃ¡c Ä‘oáº¡n top-k theo Ä‘iá»ƒm dá»±a trÃªn truy váº¥n. ChÃºng
tÃ´i cÅ©ng sá»­ dá»¥ng thá»© háº¡ng vÃ ng Ä‘á»ƒ tÃ­nh NDCG@k,
lÃ  má»™t metric truy xuáº¥t tiÃªu chuáº©n (JÃ¤rvelin and
KekÃ¤lÃ¤inen, 2002).
â¶https://github.com/google-research/
meliad .5.2 Káº¿t quáº£
Báº£ng 2 hiá»ƒn thá»‹ káº¿t quáº£ chÃ­nh cá»§a chÃºng tÃ´i, cho
tháº¥y ráº±ng RPT-Sem cÃ³ thá»ƒ so sÃ¡nh hoáº·c tá»‘t hÆ¡n
táº¥t cáº£ cÃ¡c baseline khÃ¡c trong má»i trÆ°á»ng há»£p.
Sá»­ dá»¥ng bá»™ truy xuáº¥t cá»‘ Ä‘á»‹nh (RETRO) cáº£i thiá»‡n
hiá»‡u suáº¥t so vá»›i Transformer-XL; RPT-Lex dáº«n
Ä‘áº¿n cáº£i thiá»‡n trong Books3 nhÆ°ng giáº£m trong
PG19 so vá»›i RETRO, vÃ  RPT-Sem vÆ°á»£t trá»™i hÆ¡n
Transformer-XL, RETRO vÃ  RPT-Lex trÃªn ArXiv,
PG19 vÃ  Books3, vÃ  cÃ³ hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i
RETRO trÃªn CodeParrot. Ngay cáº£ trong thiáº¿t láº­p
cÃ¢n báº±ng tham sá»‘ vÃ  tÃ­nh toÃ¡n, Transformer-XL
váº«n hoáº¡t Ä‘á»™ng kÃ©m hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i RPT. So
vá»›i Block-Recurrent Transformer, Memorizing
Transformers vÃ  Griffin, khÃ´ng sá»­ dá»¥ng CCA, hiá»‡u
suáº¥t láº¡i tÆ°Æ¡ng tá»± hoáº·c tá»‘t hÆ¡n, vá»›i nhá»¯ng cáº£i thiá»‡n
Ä‘Ã¡ng ká»ƒ trÃªn ArXiv vÃ  Books3.
CCA cho phÃ©p tÄƒng Ä‘á»™ng sá»‘ lÆ°á»£ng hÃ ng xÃ³m táº¡i
thá»i Ä‘iá»ƒm suy luáº­n. Khi sá»­ dá»¥ng 3 hoáº·c 4 hÃ ng
xÃ³m (thay vÃ¬ 2), hiá»‡u suáº¥t cáº£i thiá»‡n, cho phÃ©p trao
Ä‘á»•i tÃ­nh toÃ¡n-hiá»‡u suáº¥t.
Cuá»‘i cÃ¹ng, cÃ¡c mÃ´ hÃ¬nh oracle liÃªn tá»¥c Ä‘áº¡t Ä‘Æ°á»£c
perplexity tá»‘t nháº¥t trÃªn táº¥t cáº£ cÃ¡c dataset, cáº£i thiá»‡n
tá»« 2.74â†’2.69 trÃªn ArXiv, 2.15â†’2.10 trÃªn CodeParrot,
10.92â†’10.26 trÃªn PG19 vÃ  13.87â†’12.74 cho Books3.
Äiá»u nÃ y cho tháº¥y ráº±ng cáº£i thiá»‡n huáº¥n luyá»‡n bá»™
truy xuáº¥t cÃ³ thá»ƒ cáº£i thiá»‡n thÃªm hiá»‡u suáº¥t.

--- TRANG 8 ---
Dataset Precision@2 Recall@10 nDCG@20
BM25 RPT-L RPT-S BM25 RPT-L RPT-S BM25 RPT-L RPT-S
ArXiv 27% 26% 32% 55% 54% 58% 24% 24% 30%
Code 29% 26% 34% 53% 52% 56% 25% 23% 30%
PG19 22% 22% 28% 55% 55% 61% 18% 18% 23%
Books3 23% 19% 26% 55% 50% 58% 18% 16% 22%
Avg 25.2% 23.2% 30.0% 54.5% 52.7% 58.2% 21.2% 20.2% 26.2%
Báº£ng 3: Metrics truy xuáº¥t kiá»ƒm tra trÃªn cÃ¡c dataset.
Metrics truy xuáº¥t Báº£ng 3 trÃ¬nh bÃ y cÃ¡c metrics
truy xuáº¥t w.r.t cÃ¡c Ä‘oáº¡n dÆ°Æ¡ng oracle. Má»™t láº§n
ná»¯a, truy xuáº¥t vá»›i RPT-Sem vÆ°á»£t trá»™i hÆ¡n cáº£ RPT-
Lex vÃ  BM25 trong má»i trÆ°á»ng há»£p. Äiá»u nÃ y cho
tháº¥y táº§m quan trá»ng cá»§a viá»‡c huáº¥n luyá»‡n bá»™ truy
xuáº¥t, vÃ  hÆ¡n ná»¯a viá»‡c sá»­ dá»¥ng giÃ¡m sÃ¡t ngá»¯ nghÄ©a
dáº«n Ä‘áº¿n truy xuáº¥t tá»‘t hÆ¡n so vá»›i chá»‰ tÃ­n hiá»‡u tá»«
vá»±ng.
5.3 Ablations
Báº£ng 4 hiá»ƒn thá»‹ káº¿t quáº£ cá»§a má»™t nghiÃªn cá»©u ablation
trÃªn táº¥t cáº£ cÃ¡c dataset.
Only Teacher Forcing ChÃºng tÃ´i buá»™c mÃ´ hÃ¬nh
attend tá»›i cÃ¡c hÃ ng xÃ³m vÃ ng theo LM cháº¥m Ä‘iá»ƒm,
mÃ  khÃ´ng giáº£m dáº§n pss trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
Äiá»u nÃ y dáº«n Ä‘áº¿n giáº£m hiá»‡u suáº¥t trÃªn táº¥t cáº£ cÃ¡c
dataset, vÃ  Ä‘áº·c biá»‡t cho PG19 vÃ  Books3.
No Teacher Forcing á» Ä‘Ã¢y, chÃºng tÃ´i lÃ m ngÆ°á»£c
láº¡i vÃ  cá»‘ Ä‘á»‹nh pss = 0 trong suá»‘t quÃ¡ trÃ¬nh huáº¥n
luyá»‡n, tá»©c lÃ  chÃºng tÃ´i chá»‰ sá»­ dá»¥ng cÃ¡c hÃ ng xÃ³m
Ä‘Æ°á»£c dá»± Ä‘oÃ¡n chá»© khÃ´ng pháº£i hÃ ng xÃ³m vÃ ng.
Äiá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n viá»‡c huáº¥n luyá»‡n thiáº¿u
cÃ¡c lá»›p CCA vÃ¬ chÃºng Ä‘Æ°á»£c tiáº¿p xÃºc vá»›i cÃ¡c hÃ ng
xÃ³m cháº¥t lÆ°á»£ng tháº¥p á»Ÿ Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n luyá»‡n
vÃ  káº¿t quáº£ giáº£m tháº­m chÃ­ nhiá»u hÆ¡n so vá»›i Only
Teacher Forcing.
No neighbor gating ChÃºng tÃ´i vÃ´ hiá»‡u hÃ³a neighbor
gating Ä‘iá»u khiá»ƒn dÃ²ng thÃ´ng tin tá»« cÃ¡c Ä‘oáº¡n hÃ ng
xÃ³m vÃ  phÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng lÃªn hiá»‡u suáº¥t mÃ´ hÃ¬nh.
ChÃºng tÃ´i quan sÃ¡t tháº¥y giáº£m hiá»‡u suáº¥t trÃªn táº¥t
cáº£ cÃ¡c dataset, Ä‘Ã¡ng chÃº Ã½ trÃªn Books3, nÆ¡i perplexity
tÄƒng 4.5 Ä‘iá»ƒm.
Model ArXiv Code PG19 Books3
RETRO W . BM25 ( OUR IMPL .) 2.94 2.17 11.44 14.60
W. DPR- STYLE RETRIEVER 2.97 2.28 11.7 14.86
RPT-L EX 2.92 2.23 11.59 14.32
W. DPR- STYLE RETRIEVER 2.84 2.26 11.11 14.17
RPT-S EM 2.77 2.17 10.96 13.91
W. DPR- STYLE RETRIEVER 2.98 2.33 11.62 14.66
RPT-S EM- ONLY TEACHER FORCING 2.91 2.22 11.54 14.66
RPT-S EM- NOTEACHER FORCING 2.95 2.26 13.10 14.40
RPT-S EM- NONEIGHBOR GATING 2.92 2.20 11.50 18.68
Báº£ng 4: Káº¿t quáº£ nghiÃªn cá»©u ablation cá»§a chÃºng tÃ´i.DPR-style retriever Äá»ƒ nghiÃªn cá»©u táº§m quan trá»ng
cá»§a huáº¥n luyá»‡n chung, chÃºng tÃ´i kiá»ƒm tra hiá»‡u suáº¥t
khi sá»­ dá»¥ng cÃ¡c bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n riÃªng
biá»‡t tá»« LM, do Ä‘Ã³ gÃ¢y ra sá»± khÃ´ng khá»›p train-test.
ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c bá»™ truy xuáº¥t dense sá»­
dá»¥ng quy trÃ¬nh huáº¥n luyá»‡n DPR tiÃªu chuáº©n (Karpukhin
et al., 2020) trÃªn má»—i dataset (xem Phá»¥ lá»¥c C cho
chi tiáº¿t huáº¥n luyá»‡n), vÃ  cho má»—i mÃ´ hÃ¬nh CCA cá»§a
chÃºng tÃ´i sá»­ dá»¥ng bá»™ truy xuáº¥t nÃ y thay vÃ¬ bá»™ mÃ 
nÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ¹ng. ThÃº vá»‹, chÃºng tÃ´i quan
sÃ¡t tháº¥y RPT-Lex cÃ³ thá»ƒ sá»­ dá»¥ng hiá»‡u quáº£ cÃ¡c
hÃ ng xÃ³m kiá»ƒu DPR mang láº¡i cáº£i thiá»‡n hiá»‡u suáº¥t
nháº¹ trÃªn 3 trong 4 dataset.
NhÆ° mong Ä‘á»£i, hai mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i
cÃ¡c bá»™ truy xuáº¥t máº¡nh hÆ¡n bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi sá»±
khÃ´ng khá»›p train-test, viá»‡c thay tháº¿ bá»™ truy xuáº¥t
BM25 vÃ  bá»™ truy xuáº¥t RPT-Sem báº±ng bá»™ truy xuáº¥t
kiá»ƒu DPR khiáº¿n cáº£ hai mÃ´ hÃ¬nh bá»‹ giáº£m hiá»‡u suáº¥t
trÃªn táº¥t cáº£ cÃ¡c dataset, cho tháº¥y ráº±ng hiá»‡u suáº¥t
khÃ´ng bá»‹ ablation lÃ  káº¿t quáº£ cá»§a sá»± phá»‘i há»£p giá»¯a
bá»™ truy xuáº¥t vÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯.
5.4 PhÃ¢n tÃ­ch
121416182022T oken overlapArXiv Books3
121416182022T oken overlapCodeParrot PG19
RPT-Sem RPT-Lex RETRO+BM25 Query T argetHÃ¬nh 4: ChÃºng tÃ´i Ä‘o sá»‘ lÆ°á»£ng token duy nháº¥t chá»“ng láº¯p
giá»¯a cÃ¡c Ä‘oáº¡n query/target vÃ  hÃ ng xÃ³m Ä‘Æ°á»£c truy xuáº¥t
tá»‘t nháº¥t.
Token overlap HÃ¬nh 4 váº½ sá»‘ token trung bÃ¬nh
chá»“ng láº¯p giá»¯a cÃ¡c Ä‘oáº¡n query/target trong hÃ ng
xÃ³m Ä‘Æ°á»£c truy xuáº¥t tá»‘t nháº¥t cho RETRO, RPT-Lex
vÃ  RPT-Sem. RPT-Sem truy xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn cÃ³
Ä‘á»™ chá»“ng láº¯p cao hÆ¡n vá»›i Ä‘oáº¡n má»¥c tiÃªu so vá»›i
RPT-Lex. Tá»± nhiÃªn, BM25 truy xuáº¥t cÃ¡c Ä‘oáº¡n cÃ³
Ä‘á»™ chá»“ng láº¯p cao nháº¥t vá»›i Ä‘oáº¡n truy váº¥n

--- TRANG 9 ---
nhÆ°ng Ä‘iá»u nÃ y khÃ´ng dáº«n Ä‘áº¿n Ä‘á»™ chá»“ng láº¯p tá»« vá»±ng
cao hÆ¡n cho Ä‘oáº¡n má»¥c tiÃªu.
1234567891011121314151617181920
T op-K element according to BM250.000.050.100.150.20Average maximum target score across chunks
Dataset
Books3
ArXiv
CodeParrot
PG19
HÃ¬nh 5: Äiá»ƒm má»¥c tiÃªu tá»‘i Ä‘a st(Â·) cho cÃ¡c Ä‘oáº¡n top-K
Ä‘Æ°á»£c truy xuáº¥t bá»Ÿi BM25 trung bÃ¬nh trÃªn cÃ¡c Ä‘oáº¡n vÃ 
cho táº¥t cáº£ cÃ¡c dataset. VÃ¬ Ä‘iá»ƒm má»¥c tiÃªu tá»‘i Ä‘a cho 20
Ä‘oáº¡n hÃ ng Ä‘áº§u cao hÆ¡n nhiá»u so vá»›i top-2, viá»‡c há»c
cÃ¡ch xáº¿p háº¡ng láº¡i 20 á»©ng viÃªn BM25 hÃ ng Ä‘áº§u cÃ³ thá»ƒ
dáº«n Ä‘áº¿n cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trong cháº¥t lÆ°á»£ng truy xuáº¥t.
Cháº¥t lÆ°á»£ng giÃ¡m sÃ¡t ChÃºng tÃ´i huáº¥n luyá»‡n RPT-
Sem sá»­ dá»¥ng thÃ´ng tin tá»« hÃ m cháº¥m Ä‘iá»ƒm má»¥c tiÃªu
st(Â·), mÃ  chÃºng tÃ´i tháº¥y dáº«n Ä‘áº¿n cáº£i thiá»‡n mÃ´ hÃ¬nh.
Tuy nhiÃªn, hÃ m cháº¥m Ä‘iá»ƒm má»¥c tiÃªu chá»‰ cung cáº¥p
má»™t sá»± xáº¿p háº¡ng láº¡i cá»§a 20 á»©ng viÃªn hÃ ng Ä‘áº§u theo
BM25. Do Ä‘Ã³, má»™t cÃ¢u há»i tá»± nhiÃªn lÃ  cháº¥t lÆ°á»£ng
giÃ¡m sÃ¡t cáº£i thiá»‡n bao nhiá»u thÃ´ng qua viá»‡c xáº¿p
háº¡ng láº¡i nÃ y. HÃ¬nh 5 hiá»ƒn thá»‹ cho má»—i thá»© háº¡ng K
Ä‘iá»ƒm má»¥c tiÃªu tá»‘i Ä‘a trong sá»‘ cÃ¡c Ä‘oáº¡n top-K theo
BM25, trung bÃ¬nh trÃªn cÃ¡c Ä‘oáº¡n vÃ  trÃªn 4 dataset
cá»§a chÃºng tÃ´i. RÃµ rÃ ng, viá»‡c xáº¿p háº¡ng láº¡i 20 á»©ng
viÃªn BM25 hÃ ng Ä‘áº§u cÃ³ nhiá»u tiá»m nÄƒng, vÃ¬ Ä‘iá»ƒm
má»¥c tiÃªu tá»‘i Ä‘a cao hÆ¡n nhiá»u cho 20 á»©ng viÃªn
hÃ ng Ä‘áº§u so vá»›i top-2. Äiá»u nÃ y gá»£i Ã½ ráº±ng viá»‡c
huáº¥n luyá»‡n dÃ i hÆ¡n vÃ  tá»‘t hÆ¡n cá»§a bá»™ truy xuáº¥t cÃ³
thá»ƒ cáº£i thiá»‡n thÃªm hiá»‡u suáº¥t cá»§a RPT-Sem.
ThÃº vá»‹, phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i lÃ m sÃ¡ng tá» táº¡i
sao RPT-Sem vÆ°á»£t trá»™i rÃµ rÃ ng hÆ¡n RETRO trÃªn
Books3 vÃ  PG19 nhÆ°ng Ã­t hÆ¡n trÃªn CodeParrot.
Äiá»ƒm má»¥c tiÃªu tá»‘i Ä‘a cho CodeParrot khi k = 2
Ä‘Ã£ khÃ¡ cao â€“ khoáº£ng 0.1, tÆ°Æ¡ng á»©ng vá»›i cáº£i thiá»‡n
hÆ¡n 10% trong xÃ¡c suáº¥t cá»§a Ä‘oáº¡n má»¥c tiÃªu so vá»›i
ngá»¯ cáº£nh cá»¥c bá»™. NgÆ°á»£c láº¡i, Ä‘á»‘i vá»›i PG19 vÃ  Books3,
Ä‘iá»ƒm má»¥c tiÃªu khi k = 2 gáº§n vá»›i 0 hÆ¡n.
PhÃ¢n tÃ­ch nhÃ³m con HÃ¬nh 6 hiá»ƒn thá»‹ cáº£i thiá»‡n tÆ°Æ¡ng
Ä‘á»‘i trung bÃ¬nh (trÃªn cÃ¡c Ä‘oáº¡n) cá»§a RETRO, RPT-
Lex vÃ  RPT-Sem so vá»›i Transformer-XL,
0102030% ImprovmentArXiv Books3
0102030% ImprovmentCodeParrot PG19
RPT-Sem RPT-Lex RETRO+BM25 Incorrect Correct AllHÃ¬nh 6: Cáº£i thiá»‡n tÆ°Æ¡ng Ä‘á»‘i vá»›i/khÃ´ng cÃ³ truy xuáº¥t Ä‘Ãºng.
khi phÃ¢n biá»‡t giá»¯a cÃ¡c trÆ°á»ng há»£p má»™t Ä‘oáº¡n oracle
"vÃ ng" Ä‘Æ°á»£c truy xuáº¥t vÃ  cÃ¡c trÆ°á»ng há»£p khÃ´ng cÃ³
Ä‘oáº¡n vÃ ng nÃ o Ä‘Æ°á»£c truy xuáº¥t.
NhÆ° mong Ä‘á»£i, RPT-Sem dáº«n Ä‘áº¿n cáº£i thiá»‡n trÃªn
táº¥t cáº£ cÃ¡c dataset, vÃ  vÆ°á»£t trá»™i hÆ¡n cÃ¡c baseline
khÃ¡c trá»« RETRO trÃªn CodeParrot nÆ¡i hiá»‡u suáº¥t
tÆ°Æ¡ng tá»±. Thá»© hai, cÃ¡c trÆ°á»ng há»£p má»™t Ä‘oáº¡n vÃ ng
Ä‘Æ°á»£c truy xuáº¥t thá»±c sá»± thÆ°á»ng dáº«n Ä‘áº¿n cáº£i thiá»‡n
lá»›n hÆ¡n, nhÆ°ng chÃºng tÃ´i chá»©ng kiáº¿n cáº£i thiá»‡n ngay
cáº£ trong cÃ¡c trÆ°á»ng há»£p khÃ´ng cÃ³ Ä‘oáº¡n vÃ ng nÃ o
Ä‘Æ°á»£c truy xuáº¥t, cho tháº¥y ráº±ng mÃ´ hÃ¬nh váº«n cÃ³ thá»ƒ
hÆ°á»Ÿng lá»£i tá»« nhá»¯ng truy xuáº¥t nhÆ° váº­y.
PhÃ¢n tÃ­ch Ä‘á»‹nh tÃ­nh Kiá»ƒm tra cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c truy
xuáº¥t, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng bá»™ truy xuáº¥t
RPT cÃ³ tÃ­nh ngá»¯ cáº£nh cao. Khi Ã¡p dá»¥ng trÃªn mÃ£,
nÃ³ truy xuáº¥t cÃ¡c Ä‘á»‹nh nghÄ©a hÃ m, gÃ¡n biáº¿n, v.v.,
trÃªn ArXiv nÃ³ truy xuáº¥t cÃ¡c Ä‘á»‹nh nghÄ©a cá»§a lemma,
Ä‘á»‹nh lÃ½, v.v. HÃ¬nh 7 hiá»ƒn thá»‹ má»™t vÃ­ dá»¥, trong Ä‘Ã³
chÃºng tÃ´i cung cáº¥p codebase Ä‘Æ°á»£c sá»­ dá»¥ng cho
bÃ i bÃ¡o nÃ y lÃ m Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh cá»§a chÃºng
tÃ´i vÃ  trÃ¬nh bÃ y má»™t vÃ­ dá»¥ Ä‘oáº¡n truy váº¥n nÆ¡i RPT
táº¡o ra truy xuáº¥t tá»‘t hÆ¡n BM25. ChÃºng tÃ´i quan sÃ¡t
tháº¥y ráº±ng ngá»¯ cáº£nh trÆ°á»›c Ä‘Ã³ cho phÃ©p RPT truy
xuáº¥t hiá»‡u quáº£ má»™t Ä‘á»‹nh nghÄ©a Ä‘á»‘i tÆ°á»£ng liÃªn quan,
dáº«n Ä‘áº¿n loss tháº¥p hÆ¡n.
6 Tháº£o luáº­n vÃ  NghiÃªn cá»©u LiÃªn quan
Má»‘i quan há»‡ vá»›i Fusion-in-Decoder RPT cÃ³ Ä‘iá»ƒm
tÆ°Æ¡ng Ä‘á»“ng vá»›i Fusion-in-Decoder (FiD) (Izacard
and Grave, 2021b; Ivgi et al., 2023). Trong khi
cáº£ RPT vÃ  FiD Ä‘á»u sá»­ dá»¥ng cÃ¡c cÆ¡ cháº¿ cross-
attention Ä‘á»ƒ tÃ­ch há»£p ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t
trong cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng, chÃºng khÃ¡c nhau á»Ÿ
hai cÃ¡ch. (a) Trong FiD, truy xuáº¥t Ä‘Æ°á»£c thá»±c hiá»‡n

--- TRANG 10 ---
@flax.struct.dataclass
class FlaxRPTRetrieverEncodedOutput(ModelOutput):
original_hidden_states: jnp.ndarray = None
encoded_hidden_states: jnp.ndarray = None
attention_mask: jnp.ndarray = None
key_chunks: jnp.ndarray = None
query_chunks: jnp.ndarray = None
chunk_mask: jnp.ndarray = None
...
class FlaxRPTModule(nn.Module):
...
def __call__(...
...
hidden_states = self.ln_f (hidden_states)
if not return_dict:
return (hidden_states,) + upcoder_outputs + lowcoder_outputs
return FlaxRPTModelOutput(
last_hidden_state=upcoder_outputs.last_hidden_state,
upcoder_hidden_states=upcoder_outputs.hidden_states,
upcoder_attentions=upcoder_outputs.attentions,
lowcoder_last_hidden_state=lowc oder_outputs.last_hidden_state,
...)
...
def forward_loglikelihood(params, rng, batch, memory):
...
outputs, lowcoder_state = _forward_loglikelihood_lowcoder (params, rng, batch)
if 'cache' in lowcoder_state:
params['cache'] = lowcoder_state['cache']
outputs = jax.tree_map(lambda x: jax.device_get(x).astype(np.float32), outputs)
neighbor_hidden_states, neighbor_mask, *_ = memory.add(
input_tokens=batch["input_tokens"],
encoded_hidden_states=outputs.encoded_hidden_states,
key_chunks=outputs. key_chunks,
query_chunks=outputs.query_chunks,
)
...
HÃ¬nh 7: Má»™t vÃ­ dá»¥ minh há»a cho tháº¥y cÃ¡c hÃ ng xÃ³m top-1 Ä‘Æ°á»£c truy xuáº¥t cho cáº£ mÃ´ hÃ¬nh RPT-Sem vÃ  BM25
Ä‘Æ°á»£c Ã¡p dá»¥ng cho mÃ£ cá»§a RPT. Biáº¿n outputs trong Ä‘oáº¡n truy váº¥n lÃ  má»™t thÃ nh viÃªn cá»§a lá»›p
FlaxRPTRetrieverEncodedOutput. RPT-Sem thÃ nh cÃ´ng truy xuáº¥t Ä‘á»‹nh nghÄ©a cá»§a Ä‘á»‘i tÆ°á»£ng dáº«n Ä‘áº¿n
giáº£m loss trÃªn Ä‘oáº¡n má»¥c tiÃªu, so vá»›i BM25.
chá»‰ má»™t láº§n dá»±a trÃªn prompt/truy váº¥n ban Ä‘áº§u, trong
khi RPT liÃªn tá»¥c thá»±c hiá»‡n truy xuáº¥t á»Ÿ cáº¥p Ä‘á»™ Ä‘oáº¡n
trong suá»‘t quÃ¡ trÃ¬nh táº¡o. (b) FiD mÃ£ hÃ³a cÃ¡c hÃ ng
xÃ³m Ä‘Æ°á»£c truy xuáº¥t riÃªng biá»‡t sá»­ dá»¥ng má»™t encoder
song hÆ°á»›ng vÃ  chá»‰ sau Ä‘Ã³ Ã¡p dá»¥ng cross-attention
trong decoder. Trong RPT, decoder tÃ­nh toÃ¡n cÃ¡c
embedding Ä‘oáº¡n vÃ  thá»±c hiá»‡n truy xuáº¥t tá»± nhiÃªn,
vÃ  sau Ä‘Ã³ chunked cross-attention Ä‘Æ°á»£c Ã¡p dá»¥ng
Ä‘á»ƒ há»£p nháº¥t ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t vá»›i cÃ¡c dá»±
Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh. ChÃºng tÃ´i xem RPT, sá»­ dá»¥ng
cÃ¡c encoding lower-decoder, lÃ  tá»± nhiÃªn hÆ¡n trong
ngá»¯ cáº£nh táº¡o liÃªn tá»¥c (vÃ­ dá»¥: chatbot hoáº·c agent),
vÃ¬ mÃ´ hÃ¬nh táº¡o ra cÃ¡c biá»ƒu diá»…n vÃ  sá»­ dá»¥ng chÃºng
sau nÃ y lÃ m keys, vÃ  do Ä‘Ã³ viá»‡c táº¡o ra cÃ¡c biá»ƒu
diá»…n truy xuáº¥t khÃ´ng tá»‘n chi phÃ­.MÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa Má»™t trá»ng tÃ¢m chÃ­nh
trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa Ä‘Ã£ lÃ  giáº£i quyáº¿t
Ä‘á»™ phá»©c táº¡p báº­c hai cá»§a attention Ä‘á»ƒ phÃ¡t triá»ƒn
cÃ¡c cÆ¡ cháº¿ hiá»‡u quáº£ hÆ¡n cho viá»‡c xá»­ lÃ½ vÄƒn báº£n
dÃ i. VÃ­ dá»¥, Transformer-XL (Dai et al., 2019) xá»­
lÃ½ Ä‘áº§u vÃ o sá»­ dá»¥ng cÆ¡ cháº¿ cáº¥p Ä‘á»™ Ä‘oáº¡n trong khi
giá»¯ láº¡i cache tá»« cÃ¡c Ä‘oáº¡n trÆ°á»›c Ä‘Ã³. Longformer
(Beltagy et al., 2020) má»Ÿ rá»™ng Ã½ tÆ°á»Ÿng nÃ y Ä‘á»ƒ
chá»©a cÃ¡c ngá»¯ cáº£nh tháº­m chÃ­ dÃ i hÆ¡n. Má»™t sá»‘ nghiÃªn
cá»©u trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ xem truy xuáº¥t nhÆ° má»™t váº¥n Ä‘á»
táº§m xa. Memorizing Transformers (Wu et al., 2022)
sá»­ dá»¥ng má»™t lá»›p k-NN duy nháº¥t vÃ  truy xuáº¥t cÃ¡c
keys vÃ  values Ä‘Æ°á»£c cache, nhÆ°ng chÃºng khÃ´ng
lan truyá»n ngÆ°á»£c gradient qua phÃ©p toÃ¡n truy xuáº¥t
thÆ°a. TÆ°Æ¡ng tá»±, Bertsch et al. (2023) chá»©ng minh
ráº±ng phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i
báº¥t ká»³ mÃ´ hÃ¬nh Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n hiá»‡n cÃ³ nÃ o
vÃ  Ã¡p dá»¥ng nÃ³ á»Ÿ má»—i lá»›p attention cho cÃ¡c nhiá»‡m
vá»¥ tÃ³m táº¯t dÃ i. Tá»« gÃ³c Ä‘á»™ phÃ¢n tÃ­ch, cÃ¡c nghiÃªn
cá»©u trÆ°á»›c (Press et al., 2021) chá»©ng minh ráº±ng
cÃ¡c benchmark LM tiÃªu chuáº©n khÃ´ng lÃ½ tÆ°á»Ÿng Ä‘á»ƒ
Ä‘o kháº£ nÄƒng táº§m xa cá»§a cÃ¡c mÃ´ hÃ¬nh. Sun et al.
(2021) tháº£o luáº­n vá» cÃ¡c loáº¡i chuá»—i khÃ¡c nhau hÆ°á»Ÿng
lá»£i tá»« viá»‡c cÃ³ ngá»¯ cáº£nh dÃ i, vÃ  Rae and Razavi
(2020) Ä‘iá»u tra cÃ¡c lá»±a chá»n kiáº¿n trÃºc táº§m xa vÃ 
khuyáº¿n nghá»‹ tÄƒng kháº£ nÄƒng táº§m xa trong cÃ¡c lá»›p
trÃªn.
MÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡u quáº£ CÃ¡c chiáº¿n lÆ°á»£c thÆ°a,
nhÆ° nhá»¯ng chiáº¿n lÆ°á»£c Ä‘Æ°á»£c Ä‘á» xuáº¥t trong Zaheer
et al. (2020); Roy et al. (2021); Kitaev et al.
(2020), tÆ°Æ¡ng tá»± nhÆ° RPT, attend chá»‰ má»™t táº­p
con cÃ¡c token thÃ´ng qua cÃ¡c phÆ°Æ¡ng phÃ¡p clustering
hoáº·c hashing, Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng cÃ¡ch lan truyá»n
gradient qua phÃ©p toÃ¡n thÆ°a. Trong RPT, tÃ­nh thÆ°a
lÃ  do phÃ©p toÃ¡n top-K cá»§a bá»™ truy xuáº¥t, Ä‘Æ°á»£c huáº¥n
luyá»‡n sá»­ dá»¥ng giÃ¡m sÃ¡t cháº¥t lÆ°á»£ng cao tá»« má»™t mÃ´
hÃ¬nh ngÃ´n ngá»¯ tham chiáº¿u. Má»™t phÆ°Æ¡ng phÃ¡p khÃ¡c
Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a hiá»‡u quáº£ vÄƒn báº£n dÃ i bao gá»“m nÃ©n
Ä‘áº§u vÃ o vÃ  attend trÃªn chuá»—i Ä‘Æ°á»£c nÃ©n (Martins
et al., 2022; Rae et al., 2020), hoáº·c há»c cÃ¡ch bá»
qua cÃ¡c token khÃ´ng liÃªn quan (Sukhbaatar et al.,
2021). Tuy nhiÃªn, vá» máº·t thá»±c nghiá»‡m, háº§u háº¿t
cÃ¡c kiáº¿n trÃºc transformer hiá»‡u quáº£ trao Ä‘á»•i hiá»‡u
quáº£ láº¥y cháº¥t lÆ°á»£ng. Gáº§n Ä‘Ã¢y, cÃ¡c mÃ´ hÃ¬nh state-
space (Mehta et al., 2023; Gu and Dao, 2023; Fu
et al., 2023) xuáº¥t hiá»‡n nhÆ° má»™t lá»±a chá»n thay tháº¿
hiá»‡u quáº£, tiáº¿p cáº­n cháº¥t lÆ°á»£ng Transformer. Trong
bÃ i bÃ¡o nÃ y, chÃºng tÃ´i khÃ¡m phÃ¡ cÃ¡c mÃ´ hÃ¬nh dá»±a
trÃªn Transformer báº­c hai cá»• Ä‘iá»ƒn. ChÃºng tÃ´i láº­p
luáº­n ráº±ng mÃ´ hÃ¬nh cÆ¡ báº£n lÃ  trá»±c giao vá»›i Ä‘Ã³ng
gÃ³p cá»§a chÃºng tÃ´i vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c thay tháº¿ báº±ng
cÃ¡c lá»±a chá»n thay tháº¿ hiá»‡u quáº£ khÃ¡c vÃ  káº¿t há»£p
vá»›i truy xuáº¥t. ChÃºng tÃ´i Ä‘á»ƒ viá»‡c khÃ¡m phÃ¡ nÃ y
cho cÃ´ng viá»‡c tÆ°Æ¡ng lai.
LM tÄƒng cÆ°á»ng truy xuáº¥t LM tÄƒng cÆ°á»ng truy xuáº¥t
Ä‘Ã£ ná»•i lÃªn nhÆ° má»™t phÆ°Æ¡ng phÃ¡p ná»•i báº­t Ä‘á»ƒ khai
thÃ¡c hiá»‡u quáº£ kiáº¿n thá»©c bÃªn ngoÃ i trong khi táº¡o
vÄƒn báº£n. CÃ¡c mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c chia rá»™ng
thÃ nh nhá»¯ng mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng á»Ÿ má»©c Ä‘á»™ chi tiáº¿t
token vÃ  nhá»¯ng mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng á»Ÿ má»©c Ä‘á»™ chi
tiáº¿t chuá»—i. CÃ¡c phÆ°Æ¡ng phÃ¡p cáº¥p token, nhÆ° kNN-
LM (Khandelwal et al., 2020), TRIME (Zhong et
al., 2022), vÃ  SPALM (Yogatama et al., 2021),
truy xuáº¥t thÃ´ng tin cho cÃ¡c token riÃªng láº». CÃ¡c
phÆ°Æ¡ng phÃ¡p cáº¥p chuá»—i nhÆ° RAG (Lewis et al.,
2020) sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh encoder-decoder Ä‘Æ°á»£c
tiá»n huáº¥n luyá»‡n vá»›i cÃ¡c bá»™ truy xuáº¥t Ä‘Æ°á»£c tiá»n
huáº¥n luyá»‡n cho cÃ¡c nhiá»‡m vá»¥ nhÆ° tráº£ lá»i cÃ¢u há»i
miá»n má»Ÿ. TÆ°Æ¡ng tá»±, FiD (Izacard and Grave, 2021b)
sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh encoder-decoder táº¡o sinh
há»£p nháº¥t báº±ng chá»©ng tá»« nhiá»u Ä‘oáº¡n vÄƒn trong quÃ¡
trÃ¬nh giáº£i mÃ£, liÃªn quan cháº·t cháº½ Ä‘áº¿n cÆ¡ cháº¿ CCA.
Gáº§n Ä‘Ã¢y, Wang et al. (2023) chá»©ng minh cÃ¡c lá»£i
Ã­ch tiá»m nÄƒng cá»§a viá»‡c thá»±c hiá»‡n truy xuáº¥t vÃ  chunked
cross-attention á»Ÿ má»—i bÆ°á»›c thá»i gian, so vá»›i bÃ i
bÃ¡o RETRO (Borgeaud et al., 2022) gá»‘c, truy xuáº¥t
má»—i m = 64 bÆ°á»›c.
Huáº¥n luyá»‡n chung retriever-reader CÃ¡c phÆ°Æ¡ng
phÃ¡p huáº¥n luyá»‡n chung thÆ°á»ng táº­p trung vÃ o viá»‡c
chuyá»ƒn thÃ´ng tin giá»¯a má»™t reader Ä‘Æ°á»£c tiá»n huáº¥n
luyá»‡n vÃ o má»™t retriever Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n. CÃ¡c
phÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng bao gá»“m viá»‡c cáº­p nháº­t
chá»‰ má»¥c retriever trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n trong
ngá»¯ cáº£nh cá»§a cÃ¡c nhiá»‡m vá»¥ Ä‘Ã²i há»i kiáº¿n thá»©c nhiá»u,
nhÆ° tráº£ lá»i cÃ¢u há»i miá»n má»Ÿ. VÃ­ dá»¥, REALM (Guu
et al., 2020) sá»­ dá»¥ng masked language modeling
nhÆ° má»™t tÃ­n hiá»‡u há»c Ä‘á»ƒ cáº­p nháº­t retriever. EMDR2
(Sachan et al., 2021) má»Ÿ rá»™ng FiD báº±ng cÃ¡ch sá»­
dá»¥ng cÃ¡c mÃ´ hÃ¬nh encoder-decoder Ä‘á»ƒ lan truyá»n
ngÆ°á»£c lá»—i tá»« cÃ¢u tráº£ lá»i Ä‘Æ°á»£c dá»± Ä‘oÃ¡n Ä‘áº¿n retriever.
TÆ°Æ¡ng tá»±, Izacard and Grave (2021a); Jiang et
al. (2022) sá»­ dá»¥ng Ä‘iá»ƒm attention tá»« reader Ä‘á»ƒ
giÃ¡m sÃ¡t retriever trá»±c tiáº¿p sá»­ dá»¥ng ma tráº­n attention
nhÆ° má»™t tÃ­n hiá»‡u huáº¥n luyá»‡n Ä‘á»ƒ cho phÃ©p huáº¥n
luyá»‡n end-to-end chung vá»›i giÃ¡m sÃ¡t cá»§a nhiá»‡m
vá»¥ downstream. ÄÃ¡ng chÃº Ã½, Izacard et al. (2022b)
tiáº¿p tá»¥c má»Ÿ rá»™ng quy mÃ´ cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y vÃ 
huáº¥n luyá»‡n chung má»™t retriever vá»›i má»™t mÃ´ hÃ¬nh
encoder-decoder, chá»©ng minh kháº£ nÄƒng há»c few-
shot máº¡nh máº½. Há» cÅ©ng Ä‘iá»u tra cÃ¡c ká»¹ thuáº­t cáº­p
nháº­t retriever khÃ¡c nhau Ä‘á»ƒ giáº£i quyáº¿t sá»± khÃ´ng
khá»›p train-test trong quÃ¡ trÃ¬nh truy xuáº¥t. ChÃºng
tÃ´i khÃ´ng gáº·p pháº£i váº¥n Ä‘á» cáº­p nháº­t chá»‰ má»¥c vÃ¬
chÃºng tÃ´i tÃ­nh toÃ¡n toÃ n bá»™ chá»‰ má»¥c thÃ´ng qua má»™t
láº§n truyá»n xuÃ´i.
Tiá»n huáº¥n luyá»‡n Retriever CÃ¡c nghiÃªn cá»©u Ä‘áº§u
tiÃªn vá» tiá»n huáº¥n luyá»‡n retriever dá»±a vÃ o Inverse
Cloze Task khÃ´ng giÃ¡m sÃ¡t Ä‘á»ƒ tiá»n huáº¥n luyá»‡n
retriever (Lee et al., 2019; Guu et al., 2020). Sau
Ä‘Ã³ Ä‘Æ°á»£c chá»‰ ra ráº±ng viá»‡c sá»­ dá»¥ng trá»±c tiáº¿p BERT
(Devlin et al., 2019) vá»›i má»™t má»¥c tiÃªu cÃ³ giÃ¡m
sÃ¡t lÃ  Ä‘á»§ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t trÃªn cÃ¡c benchmark
tiÃªu chuáº©n (Karpukhin et al., 2020). Tuy nhiÃªn,
paradigm nÃ y cho tháº¥y hiá»‡u suáº¥t kÃ©m trÃªn cÃ¡c
thá»±c thá»ƒ long-tail so vá»›i BM25 (Amouyal et al.,
2023; Sciavolino et al., 2021). Gáº§n Ä‘Ã¢y, cÃ¡c phÆ°Æ¡ng
phÃ¡p tiá»n huáº¥n luyá»‡n khÃ´ng giÃ¡m sÃ¡t (Gao and
Callan, 2022; Ram et al., 2022; Izacard et al.,
2022a) cho phÃ©p cáº£i thiá»‡n hiá»‡u suáº¥t. Tuy nhiÃªn,
cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« má»™t mÃ´
hÃ¬nh encoder BERT (Devlin et al., 2019) Ä‘Æ°á»£c
tiá»n huáº¥n luyá»‡n, trong khi RPT lÃ  má»™t kiáº¿n trÃºc
retriever-reader Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u vÆ°á»£t trá»™i
hÆ¡n BM25 mÃ  khÃ´ng cáº§n báº¥t ká»³ tiá»n huáº¥n luyá»‡n
bá»• sung nÃ o.
GiÃ¡m sÃ¡t retriever vá»›i LLM EPR (Rubin et al.,
2022) chá»©ng minh ráº±ng LLM cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng
Ä‘á»ƒ huáº¥n luyá»‡n má»™t retriever cho truy xuáº¥t prompt
báº±ng cÃ¡ch Æ°á»›c tÃ­nh xÃ¡c suáº¥t cá»§a má»™t Ä‘áº§u ra cho
trÆ°á»›c Ä‘áº§u vÃ o vÃ  má»™t vÃ­ dá»¥ huáº¥n luyá»‡n á»©ng viÃªn
nhÆ° prompt. CÃ¡c ká»¹ thuáº­t tÆ°Æ¡ng tá»± Ä‘Æ°á»£c Ã¡p dá»¥ng
cho tráº£ lá»i cÃ¢u há»i miá»n má»Ÿ thÃ´ng qua xáº¿p háº¡ng
láº¡i káº¿t quáº£ truy xuáº¥t (Sachan et al., 2022; Ram
et al., 2023) vÃ  Ä‘á»ƒ giÃ¡m sÃ¡t retriever thÃ´ng qua
chÆ°ng cáº¥t perplexity (Izacard et al., 2022b). Gáº§n
Ä‘Ã¢y, Shi et al. (2024) sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p giÃ¡m
sÃ¡t nÃ y Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a cÃ¡c LLM khÃ¡c
nhau theo cÃ¡ch black-box.
7 Káº¿t luáº­n
Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y Retrieval-
Pretrained Transformer (RPT), má»™t LM tÄƒng cÆ°á»ng
truy xuáº¥t trong Ä‘Ã³ retriever Ä‘Æ°á»£c huáº¥n luyá»‡n nhÆ°
má»™t thÃ nh pháº§n tá»± nhiÃªn cá»§a LM Ä‘á»ƒ truy xuáº¥t cÃ¡c
Ä‘oáº¡n cÃ³ liÃªn quan ngá»¯ nghÄ©a cho dá»± Ä‘oÃ¡n vÄƒn báº£n
tÆ°Æ¡ng lai. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT trÃªn bá»‘n nhiá»‡m
vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa, bao gá»“m sÃ¡ch, mÃ£
vÃ  vÄƒn báº£n toÃ¡n há»c. ChÃºng tÃ´i chá»©ng minh ráº±ng
báº±ng cÃ¡ch tÃ­ch há»£p liá»n máº¡ch retriever vÃ o kiáº¿n
trÃºc vÃ  quy trÃ¬nh huáº¥n luyá»‡n, RPT hÆ°á»Ÿng lá»£i tá»«
viá»‡c há»£p nháº¥t ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t, cáº£i thiá»‡n
so vá»›i cÃ¡c baseline tÄƒng cÆ°á»ng truy xuáº¥t máº¡nh máº½.
Trong khi cÃ´ng trÃ¬nh nÃ y táº­p trung vÃ o truy xuáº¥t
tá»« vÄƒn báº£n dÃ i, chÃºng tÃ´i láº­p luáº­n ráº±ng cÃ¡c phÃ¡t
hiá»‡n thá»±c nghiá»‡m cá»§a chÃºng tÃ´i cho tháº¥y viá»‡c Ä‘iá»u
chá»‰nh quy trÃ¬nh cá»§a chÃºng tÃ´i cho truy xuáº¥t corpus
dá»±a trÃªn web tá»•ng quÃ¡t lÃ  má»™t hÆ°á»›ng tÆ°Æ¡ng lai
thÃº vá»‹. Äiá»u nÃ y sáº½ Ä‘Ã²i há»i vÆ°á»£t qua cÃ¡c khÃ³ khÄƒn
ká»¹ thuáº­t liÃªn quan Ä‘áº¿n má»Ÿ rá»™ng quy mÃ´ vÃ  xÃ¢y
dá»±ng corpus tiá»n huáº¥n luyá»‡n. ChÃºng tÃ´i hÃ¬nh dung
RPT sáº½ má»Ÿ Ä‘Æ°á»ng cho tháº¿ há»‡ má»›i cá»§a cÃ¡c mÃ´ hÃ¬nh
ngÃ´n ngá»¯ Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n vá»›i truy xuáº¥t Ä‘Æ°á»£c
tÃ­ch há»£p sÃ¢u trong suá»‘t kiáº¿n trÃºc vÃ  quy trÃ¬nh huáº¥n
luyá»‡n cá»§a chÃºng.
Lá»i cáº£m Æ¡n
NghiÃªn cá»©u nÃ y Ä‘Æ°á»£c há»— trá»£ vá»›i Cloud TPU tá»«
TPU Research Cloud (TRC) cá»§a Google vÃ  Há»™i
Ä‘á»“ng NghiÃªn cá»©u ChÃ¢u Ã‚u (ERC) dÆ°á»›i chÆ°Æ¡ng
trÃ¬nh nghiÃªn cá»©u vÃ  Ä‘á»•i má»›i Horizons 2020 cá»§a
LiÃªn minh ChÃ¢u Ã‚u (grant ERC DELPHI 802800).
Ohad muá»‘n cáº£m Æ¡n Iz Beltagy vÃ¬ Ä‘Ã£ gá»£i Ã½ chÆ°Æ¡ng
trÃ¬nh TRC, vÃ  toÃ n bá»™ phÃ²ng thÃ­ nghiá»‡m TAU NLP
vÃ  Ä‘áº·c biá»‡t lÃ  Guy Dar vÃ  Itay Itzhak. CÃ´ng trÃ¬nh
nÃ y Ä‘Æ°á»£c hoÃ n thÃ nh Ä‘á»ƒ hoÃ n thiá»‡n má»™t pháº§n báº±ng
Tiáº¿n sÄ© cá»§a Ohad Rubin.

--- TRANG 12 ---
TÃ i liá»‡u tham kháº£o
Samuel Amouyal, Tomer Wolfson, Ohad Rubin,
Ori Yoran, Jonathan Herzig, and Jonathan Be-
rant. 2023. QAMPARI: A benchmark for open-
domain questions with many answers. In Proc.
of the Third Workshop on GEM. ACL.
Zhangir Azerbayev, Edward Ayers, and Bartosz
Piotrowski. 2023. Proof-Pile: A Pre-training
Dataset of Mathematical Text.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document trans-
former.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for
sequence prediction with recurrent neural net-
works. In Proc. of NeurIPS .
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R. Gormley. 2023. Unlimiformer:
Long-range transformers with unlimited length
input. In Proc. of NeurIPS .
Stella Biderman, Hailey Schoelkopf, Quentin An-
thony, Herbie Bradley, Kyle O'Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Puro-
hit, USVSN Sai Prashanth, Edward Raff, Aviya
Skowron, Lintang Sutawika, and Oskar van der
Wal. 2023. Pythia: A suite for analyzing large
language models across training and scaling.
Sidney Black, Stella Biderman, Eric Hallahan,
Quentin Anthony, Leo Gao, Laurence Golding,
Horace He, Connor Leahy, Kyle McDonell, Ja-
son Phang, Michael Pieler, Usvsn Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan
Tow, Ben Wang, and Samuel Weinbach. 2022.
GPT-NeoX-20B: An open-source autoregressive
language model. In Proc. of the BigScience
Workshop .
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Mil-
lican, George van den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, Diego
de Las Casas, Aurelia Guy, Jacob Menick, Ro-
man Ring, Tom Hennigan, Saffron Huang, Loren
Maggiore, Chris Jones, Albin Cassirer, Andy
Brock, Michela Paganini, Geoffrey Irving, Oriol
Vinyals, Simon Osindero, Karen Simonyan,
Jack W. Rae, Erich Elsen, and Laurent Sifre.

--- TRANG 13 ---
2022. Improving language models by retrieving
from trillions of tokens. In Proc. of ICML .
Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Proc.
of NeurIPS .
Christopher J. C. Burges, Robert Ragno, and
Quoc Viet Le. 2006. Learning to rank with non-
smooth cost functions. In Proc. of NeurIPS .
Aakanksha Chowdhery, Sharan Narang, Jacob De-
vlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, Parker
Schuh, Kensen Shi, Sasha Tsvyashchenko,
Joshua Maynez, Abhishek Rao, Parker Barnes,
Yi Tay, Noam Shazeer, Vinodkumar Prab-
hakaran, Emily Reif, Nan Du, Ben Hutchin-
son, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay
Ghemawat, Sunipa Dev, Henryk Michalewski,
Xavier Garcia, Vedant Misra, Kevin Robin-
son, Liam Fedus, Denny Zhou, Daphne Ip-
polito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omer-
nick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Diaz, Orhan Firat, Michele
Catasta, Jason Wei, Kathy Meier-Hellstern, Dou-
glas Eck, Jeff Dean, Slav Petrov, and Noah
Fiedel. 2022. Palm: Scaling language model-
ing with pathways.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime
Carbonell, Quoc Le, and Ruslan Salakhutdinov.
2019. Transformer-XL: Attentive language mod-
els beyond a fixed-length context. In Proc. of
ACL.Soham De, Samuel L. Smith, Anushan Fernando,
Aleksandar Botev, George Cristian-Muraru, Al-
bert Gu, Ruba Haroun, Leonard Berrada, Yu-
tian Chen, Srivatsan Srinivasan, Guillaume
Desjardins, Arnaud Doucet, David Budden,
Yee Whye Teh, Razvan Pascanu, Nando De Fre-
itas, and Caglar Gulcehre. 2024. Griffin: Mixing
gated linear recurrences with local attention for
efficient language models.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proc. of NAACL-HLT .
Ehsan Doostmohammadi, Tobias Norlund, Marco
Kuhlmann, and Richard Johansson. 2023.
Surface-based retrieval reduces perplexity of
retrieval-augmented language models. In Proc.
of ACL .
Matthijs Douze, Alexandr Guzhva, Chengqi
Deng, Jeff Johnson, Gergely Szilvasy, Pierre-
Emmanuel MazarÃ©, Maria Lomeli, Lucas Hos-
seini, and HervÃ© JÃ©gou. 2024. The faiss library.
William Fedus, Barret Zoph, and Noam Shazeer.
2022. Switch transformers: Scaling to trillion
parameter models with simple and efficient spar-
sity. J. Mach. Learn. Res. , 23:1â€“39.
Daniel Y Fu, Tri Dao, Khaled Kamal Saab,
Armin W Thomas, Atri Rudra, and Christopher
Re. 2023. Hungry hungry hippos: Towards lan-
guage modeling with state space models. In
Proc. of ICLR .
Leo Gao, Stella Biderman, Sid Black, Laurence
Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima,
Shawn Presser, and Connor Leahy. 2020. The
pile: An 800gb dataset of diverse text for lan-
guage modeling.
Luyu Gao and Jamie Callan. 2022. Unsupervised
corpus aware language model pre-training for
dense passage retrieval. In Proc. of ACL .
Albert Gu and Tri Dao. 2023. Mamba: Linear-time
sequence modeling with selective state spaces.
Ankit Gupta, Harsh Mehta, and Jonathan Berant.
2023. Simplifying and understanding state space
models with diagonal linear rnns.

--- TRANG 14 ---
Kelvin Guu, Kenton Lee, Zora Tung, Panupong
Pasupat, and Ming-Wei Chang. 2020. Realm:
Retrieval-augmented language model pre-
training. In Proc. of ICML .
Yangsibo Huang, Daogao Liu, Zexuan Zhong, Wei-
jia Shi, and Yin Tat Lee. 2023. knn-adapter:
Efficient domain adaptation for black-box lan-
guage models.
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu,
Ethan Dyer, and Behnam Neyshabur. 2022.
Block-recurrent transformers. In Proc. of
NeurIPS .
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023.
Efficient Long-Text Understanding with Short-
Text Models. In Transactions of the Association
for Computational Linguistics , volume 11, pages
284â€“299.
Gautier Izacard, Mathilde Caron, Lucas Hosseini,
Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. 2022a. Unsu-
pervised dense information retrieval with con-
trastive learning. Transactions on Machine
Learning Research .
Gautier Izacard and Edouard Grave. 2021a. Dis-
tilling knowledge from reader to retriever for
question answering. In Proc. of ICLR .
Gautier Izacard and Edouard Grave. 2021b. Lever-
aging passage retrieval with generative models
for open domain question answering. In Proc.
of EACL .
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
and Edouard Grave. 2022b. Atlas: Few-shot
learning with retrieval augmented language mod-
els.J. Mach. Learn. Res. , 24:1â€“43.
Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002.
Cumulated gain-based evaluation of ir tech-
niques. ACM Transactions on Information Sys-
tems, 20:422â€“446.
Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun
Araki, Haibo Ding, Jamie Callan, and Graham
Neubig. 2022. Retrieval as attention: End-to-
end learning of retrieval and reading within a
single transformer. In Proc. of EMNLP .Vladimir Karpukhin, Barlas Oguz, Sewon Min,
Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. 2020. Dense passage
retrieval for open-domain question answering.
InProc. of EMNLP .
Urvashi Khandelwal, Omer Levy, Dan Juraf-
sky, Luke Zettlemoyer, and Mike Lewis. 2020.
Generalization through memorization: Nearest
neighbor language models. In Proc. of ICLR .
Nikita Kitaev, Lukasz Kaiser, and Anselm Lev-
skaya. 2020. Reformer: The efficient trans-
former. In Proc. of ICLR .
Kenton Lee, Ming-Wei Chang, and Kristina
Toutanova. 2019. Latent retrieval for weakly
supervised open domain question answering. In
Proc. of ACL .
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau
Yih, Tim RocktÃ¤schel, Sebastian Riedel, and
Douwe Kiela. 2020. Retrieval-augmented gen-
eration for knowledge-intensive NLP tasks. In
Proc. of NeurIPS .
Pedro Henrique Martins, Zita Marinho, and An-
dre Martins. 2022. âˆ-former: Infinite memory
transformer. In Proc. of ACL .
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and
Behnam Neyshabur. 2023. Long range language
modeling via gated state spaces. In Proc. of
ICLR .
Antonio Orvieto, Samuel L Smith, Albert Gu,
Anushan Fernando, Caglar Gulcehre, Razvan
Pascanu, and Soham De. 2023. Resurrecting
recurrent neural networks for long sequences. In
Proc. of ICML .
Ofir Press, Noah A. Smith, and Mike Lewis. 2021.
Shortformer: Better language modeling using
shorter inputs. In Proc. of ACL .
Ofir Press and Lior Wolf. 2017. Using the out-
put embedding to improve language models. In
Proc. of EACL .
Jack Rae and Ali Razavi. 2020. Do transformers
need deep long-range memory? In Proc. of ACL .

--- TRANG 15 ---
Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku-
mar, Chloe Hillier, and Timothy P. Lillicrap.
2020. Compressive transformers for long-range
sequence modelling. In Proc. of ICLR .
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor
Muhlgay, Amnon Shashua, Kevin Leyton-
Brown, and Yoav Shoham. 2023. In-context
retrieval-augmented language models. Trans-
actions of the Association for Computational
Linguistics , 11:1316â€“1331.
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Be-
rant, and Amir Globerson. 2022. Learning to
retrieve passages without supervision. In Proc.
of NAACL-HLT .
Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: Bm25 and
beyond. Foundations and Trends in Information
Retrieval , 3:333â€“389.
Aurko Roy, Mohammad Saffar, Ashish Vaswani,
and David Grangier. 2021. Efficient content-
based sparse attention with routing transformers.
Transactions of the Association for Computa-
tional Linguistics , 9:53â€“68.
Ohad Rubin, Jonathan Herzig, and Jonathan Be-
rant. 2022. Learning to retrieve prompts for
in-context learning. In Proc. of NAACL-HLT .
Devendra Sachan, Mike Lewis, Mandar Joshi, Ar-
men Aghajanyan, Wen-tau Yih, Joelle Pineau,
and Luke Zettlemoyer. 2022. Improving passage
retrieval with zero-shot question generation. In
Proc. of EMNLP .
Devendra Singh Sachan, Siva Reddy, William L.
Hamilton, Chris Dyer, and Dani Yogatama. 2021.
End-to-end training of multi-document reader
and retriever for open-domain question answer-
ing. In Proc. of NeurIPS .
Christopher Sciavolino, Zexuan Zhong, Jinhyuk
Lee, and Danqi Chen. 2021. Simple entity-
centric questions challenge dense retrievers. In
Proc. of EMNLP .
Weijia Shi, Sewon Min, Michihiro Yasunaga,
Minjoon Seo, Rich James, Mike Lewis, Luke
Zettlemoyer, and Wen tau Yih. 2024. Replug:
Retrieval-augmented black-box language mod-
els. In Proc. of NAACL-HLT .Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng
Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer:
Enhanced transformer with rotary position em-
bedding. Neurocomput. , 568.
Sainbayar Sukhbaatar, Da Ju, Spencer Poff,
Stephen Roller, Arthur Szlam, Jason Weston,
and Angela Fan. 2021. Not all memories are
created equal: Learning to forget by expiring. In
Proc. of ICML .
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-
Micke, and Mohit Iyyer. 2021. Do long-range
language models actually use long-range con-
text? In Proc. of EMNLP .
Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux, Timo-
thÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal,
Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume
Lample. 2023. Llama: Open and efficient foun-
dation language models.
Boxin Wang, Wei Ping, Peng Xu, Lawrence
McAfee, Zihan Liu, Mohammad Shoeybi,
Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei
Xiao, Anima Anandkumar, and Bryan Catanzaro.
2023. Shall we pretrain autoregressive language
models with retrieval? a comprehensive study.
InProc. of EMNLP .
Thomas Wolf, Loubna Ben Allal, Leandro von
Werra, Li Jia, and Armel Zebaze. 2023. A
dataset of python files from github.
Yuhuai Wu, Markus Norman Rabe, DeLesley
Hutchins, and Christian Szegedy. 2022. Memo-
rizing transformers. In Proc. of ICLR .
Dani Yogatama, Cyprien de Masson d'Autume, and
Lingpeng Kong. 2021. Adaptive semiparametric
language models. Transactions of the Associa-
tion for Computational Linguistics , 9:362â€“373.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, and Amr Ahmed. 2020. Big
bird: Transformers for longer sequences. In
Proc. of NeurIPS .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin,

--- TRANG 16 ---
Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt
Shuster, Daniel Simig, Punit Singh Koura, An-
jali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
2022. Opt: Open pre-trained transformer lan-
guage models.
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.
Training language models with memory augmen-
tation. In Proc. of EMNLP .
Juntang Zhuang, Tommy Tang, Yifan Ding,
Sekhar C. Tatikonda, Nicha C. Dvornek,
Xenophon Papademetris, and James S. Duncan.
2020. Adabelief optimizer: Adapting stepsizes
by the belief in observed gradients. In Proc. of
NeurIPS .
A Chi tiáº¿t Triá»ƒn khai Bá»• sung
CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c triá»ƒn khai trong JAX vá»›i tá»· lá»‡
dropout lÃ  0.05, vÃ  bá»™ tá»‘i Æ°u AdaBelief (Zhuang
et al., 2020) vá»›i weight decay lÃ  1e-8, cosine decay
vá» 0.1 cá»§a learning rate tá»‘i Ä‘a, global gradient
norm clipping lÃ  1, vÃ  tied input embedding (Press
and Wolf, 2017). Grid search xÃ¡c Ä‘á»‹nh cÃ¡c giÃ¡ trá»‹
Ï„: 128 cho Books3, 4 cho PG19, 2 cho CodeParrot,
vÃ  8 cho ArXiv. ChÃºng tÃ´i Ä‘áº·t Î±ret = 1eâˆ’9 cho
táº¥t cáº£ cÃ¡c dataset vÃ  base learning rate lÃ  5eâˆ’3,
sá»­ dá»¥ng táº­p validation Ä‘á»ƒ lá»±a chá»n siÃªu tham sá»‘.
B Äá»™ phá»©c táº¡p TÃ­nh toÃ¡n
Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n má»—i token cá»§a má»™t lá»›p attention
trong má»™t mÃ´ hÃ¬nh transformer vá»›i chiá»u d, |Q|
truy váº¥n vÃ  |K| keys lÃ  2Â·dÂ·(|K|Â·|Q|+|K|Â·d+|Q|Â·d)
flops.â· Báº±ng cÃ¡ch Ä‘áº·t N=|Q|=|K| vÃ  thÃªm chi phÃ­
cá»§a lá»›p feed-forward, chÃºng tÃ´i cÃ³ Ä‘Æ°á»£c chi phÃ­
má»—i token cho má»™t khá»‘i transformer khi dâ‰«N lÃ 
2d(N+ 2d) + 8dÂ² â‰ˆ 12dÂ² flops. Äá»‘i vá»›i CCA, chi
phÃ­ phá»¥ thuá»™c vÃ o kÃ­ch thÆ°á»›c Ä‘oáº¡n C, vÃ  sá»‘ lÆ°á»£ng
hÃ ng xÃ³m k. Äáº·t |K|= 2Ck vÃ  |Q|=C, vÃ  giáº£ sá»­
dâ‰«Ck, chi phÃ­ má»—i token cho má»™t lá»›p CCA lÃ 
2d(2Ck+ 2dk+d)â‰ˆ(4k+ 2)Â·dÂ² flops. Chi phÃ­ má»—i
token cho Î±âˆˆ[0,1] cá»§a cÃ¡c khá»‘i bao gá»“m CCA lÃ 
â‰ˆÎ±(k/3+1/6). Trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i,
chÃºng tÃ´i sá»­ dá»¥ng CCA trong 5 trong 12 lá»›p nÃªn
Î±=5/12 vÃ 
â·Äá»‘i vá»›i ma tráº­n truy váº¥n QâˆˆR|Q|Ã—d vÃ  ma tráº­n key/value
KâˆˆR|K|Ã—d, nÃ³ bao gá»“m cÃ¡c phÃ©p toÃ¡n sau: nhÃ¢n vá»›i WQ,
WK, vÃ  WV cho cÃ¡c truy váº¥n, keys, vÃ  values, má»—i cÃ¡i tá»‘n
|Q|Â·dÂ², |K|Â·dÂ², vÃ  |K|Â·dÂ² flops tÆ°Æ¡ng á»©ng. TÃ­nh toÃ¡n ma tráº­n
attention vÃ  nhÃ¢n nÃ³ vá»›i cÃ¡c values má»—i cÃ¡i Ä‘Ã²i há»i |Q|Â·|K|Â·d
flops. Cuá»‘i cÃ¹ng, nhÃ¢n vá»›i ma tráº­n Ä‘áº§u ra lÃ  thÃªm |Q|Â·dÂ²
flops.k= 2, vÃ  cÃ³ Ä‘Æ°á»£c ráº±ng CCA Ä‘Ã³ng gÃ³p má»™t overhead
khoáº£ng 1.29x. Sá»­ dá»¥ng logic tÆ°Æ¡ng tá»±, chi phÃ­ cá»‘
Ä‘á»‹nh cho thÃ nh pháº§n retriever lÃ  hai phÃ©p chiáº¿u
tuyáº¿n tÃ­nh, hai lá»›p bidirectional attention bá»• sung,
vÃ  lá»›p query augmentation dáº«n Ä‘áº¿n 1/nlayersÂ·(7k/6+1/2),
hoáº·c overhead cuá»‘i cÃ¹ng lÃ  1.49x phÃ¹ há»£p vá»›i
overhead runtime hiá»‡u quáº£ Ä‘o Ä‘Æ°á»£c cá»§a chÃºng tÃ´i
lÃ  1.51x (xem Báº£ng 2).
C Chi tiáº¿t huáº¥n luyá»‡n retriever kiá»ƒu DPR
ChÃºng tÃ´i Ä‘Ã£ lÃ m theo cÃ´ng thá»©c huáº¥n luyá»‡n cá»§a
DPR (Karpukhin et al., 2020) trong viá»‡c huáº¥n
luyá»‡n má»™t retriever BERT-base vá»›i contrastive
loss. Má»¥c tiÃªu DPR Ä‘Ã²i há»i cÃ¡c positive vÃ  hard
negative Ä‘á»ƒ há»™i tá»¥ thÃ nh cÃ´ng, vÃ  á»Ÿ Ä‘Ã¢y chÃºng tÃ´i
sá»­ dá»¥ng Ä‘oáº¡n BM25 cÃ³ Ä‘iá»ƒm cao nháº¥t top-1 nhÆ°
vÃ­ dá»¥ positive vÃ  Ä‘oáº¡n Ä‘Æ°á»£c xáº¿p háº¡ng thá»© 5 bá»Ÿi
BM25 nhÆ° vÃ­ dá»¥ hard negative. Äá»ƒ Ä‘áº£m báº£o so
sÃ¡nh cÃ´ng báº±ng, chÃºng tÃ´i huáº¥n luyá»‡n retriever
contrastive cá»§a chÃºng tÃ´i trÃªn sá»‘ vÃ­ dá»¥ nhiá»u gáº¥p
16 láº§n so vá»›i cÃ´ng thá»©c DPR gá»‘c mÃ´ táº£.
