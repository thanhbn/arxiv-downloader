# 2306.13421.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2306.13421.pdf
# Kích thước tệp: 801468 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Retrieval-Pretrained Transformer: Mô hình ngôn ngữ tầm xa với
tự truy xuất
Ohad Rubin Jonathan Berant
Trường Khoa học Máy tính Blavatnik, Đại học Tel Aviv
{ohad.rubin,joberant}@cs.tau.ac.il
Tóm tắt
Các mô hình ngôn ngữ tăng cường truy xuất
(LMs) đã nhận được nhiều sự chú ý gần đây.
Tuy nhiên, thông thường bộ truy xuất không được
huấn luyện chung như một thành phần tự nhiên
của LM, mà được thêm vào sau khi LM đã được
tiền huấn luyện, điều này hạn chế khả năng thích
ứng của LM và bộ truy xuất với nhau. Trong
công trình này, chúng tôi đề xuất Retrieval-
Pretrained Transformer (RPT), một kiến trúc
và quy trình huấn luyện để huấn luyện chung
một LM tăng cường truy xuất từ đầu và áp dụng
nó cho nhiệm vụ mô hình hóa văn bản dài.
Với một đoạn văn bản được tạo gần đây trong
một tài liệu dài, LM tính toán các biểu diễn
truy vấn, sau đó được sử dụng để truy xuất
các đoạn trước đó trong tài liệu, có thể nằm
cách hàng chục nghìn token. Thông tin từ
các đoạn được truy xuất được hợp nhất vào
các biểu diễn LM để dự đoán đoạn mục tiêu
tiếp theo. Chúng tôi huấn luyện thành phần
truy xuất với một mục tiêu ngữ nghĩa, trong
đó mục tiêu là truy xuất các đoạn làm tăng
xác suất của đoạn tiếp theo, theo một LM
tham chiếu. Chúng tôi đánh giá RPT trên bốn
nhiệm vụ mô hình ngôn ngữ tầm xa, bao gồm
sách, mã và văn bản toán học, và chứng minh
rằng RPT cải thiện chất lượng truy xuất và
do đó giảm perplexity so với các baseline
mạnh mẽ.
1 Giới thiệu
Các mô hình ngôn ngữ lớn (LMs) đã có thành công
to lớn gần đây (Brown et al., 2020; Chowdhery
et al., 2022; Zhang et al., 2022; Touvron et al.,
2023), trở thành một công cụ hữu ích trên nhiều
lĩnh vực. Tuy nhiên, thành công của chúng đi kèm
với chi phí tính toán, do số lượng tham số tăng
để lưu trữ kiến thức thế giới (Fedus et al., 2022)
và độ dài ngữ cảnh tăng cho phép truy cập thông
tin ở xa, nhưng phát sinh chi phí phức tạp bậc hai.
Lexically SimilarBook orLong textSemantically Similar
Retrieve⋮Fuse⋮
Training SignalPθ (……....|……………...)𝑐!" 𝑐#$! 𝑐#$# >Pθ (……....|……………...)𝑐!$$ 𝑐#$! 𝑐#$# Chunk 100Chunk 13Past States
PredictCausal Language ModelChunk 201
Chunk 202Query
TargetInput
RefRefThe killer left a room full of evidence, a puzzle for forensics.As a kid, Lt. Johnfound a dead dog; since then, crimson always unnervedhim.Lt.  John looked around, "Another victim, The Crimson Murderer strikes again."
"I bet the forensic guys  would love this."Hình 1: Retrieval-Pretrained Transformer (RPT) là một
mô hình ngôn ngữ được huấn luyện từ đầu với khả năng
truy xuất tự nhiên có thể được áp dụng cho văn bản dài
(ví dụ: sách). RPT nhận một đoạn văn bản làm đầu vào,
truy xuất các đoạn có liên quan ngữ nghĩa từ quá khứ để
dự đoán tốt hơn đoạn tiếp theo, và hợp nhất các đoạn được
truy xuất này vào các biểu diễn của nó. Trên một loss LM
tiêu chuẩn, bộ truy xuất được huấn luyện để truy xuất các
đoạn làm tăng xác suất của đoạn tiếp theo theo một LM
tham chiếu.
Mô hình ngôn ngữ tăng cường truy xuất (RALM)
giảm bớt chi phí này (Khandelwal et al., 2020; Yo-
gatama et al., 2021; Borgeaud et al., 2022; Ram
et al., 2023), vì việc truy xuất chính xác thông tin
liên quan có thể giảm yêu cầu bộ nhớ và tính toán.
Hơn nữa, RALM có lợi cho tính chính xác, tính
mới và khả năng tổng quát hóa mà không cần huấn
luyện lại, chỉ đơn giản bằng cách thay đổi chỉ mục
truy xuất (Guu et al., 2020; Lewis et al., 2020;
Huang et al., 2023).
Tuy nhiên, các nghiên cứu trước về RALM phần
lớn đã không huấn luyện bộ truy xuất như một
thành phần hạng nhất của LM. Trong một số trường
hợp (Khandelwal et al., 2020;arXiv:2306.13421v2  [cs.CL]  21 Jul 2024

--- TRANG 2 ---
Yogatama et al., 2021; Borgeaud et al., 2022), bộ
truy xuất chỉ được sử dụng tại thời điểm kiểm tra,
hoặc vẫn cố định trong suốt quá trình huấn luyện,
ngăn cản nó thích ứng với bộ tạo LM. Trong các
trường hợp khác, thành phần truy xuất được huấn
luyện chung nhưng chỉ sau một giai đoạn tiền huấn
luyện riêng biệt cho cả bộ truy xuất và LM (Sachan
et al., 2021; Izacard et al., 2022b; Jiang et al.,
2022; Bertsch et al., 2023). Do đó, bộ truy xuất
không được tiền huấn luyện từ đầu cùng với LM,
và chỉ một phần ngân sách huấn luyện được phân
bổ cho huấn luyện chung.
Gần đây, Zhong et al. (2022) đã trình bày một
LM tăng cường truy xuất huấn luyện bộ truy xuất
từ đầu cùng với LM, nhưng (a) bộ truy xuất được
huấn luyện để khai thác chỉ thông tin từ vựng, và
(b) thông tin được truy xuất không được hợp nhất
ở cấp độ biểu diễn trở lại vào LM.
Trong công trình này, chúng tôi trình bày
Retrieval-Pretrained Transformer (RPT), một LM
tăng cường truy xuất, trong đó bộ truy xuất là một
thành phần hạng nhất, được huấn luyện chung từ
đầu với LM. RPT dựa trên hai đóng góp kỹ thuật.
Thứ nhất, về mặt kiến trúc (xem Hình 1), các biểu
diễn đầu vào cho bộ truy xuất được tính toán từ
chính các biểu diễn LM (một khái niệm chúng tôi
gọi là tự truy xuất), và các biểu diễn được truy xuất
được hợp nhất trở lại vào bộ giải mã LM để đưa ra
dự đoán từ tiếp theo. Thứ hai, chúng tôi huấn luyện
bộ truy xuất với một hàm loss phụ trợ khuyến khích
truy xuất các đoạn văn bản làm tăng xác suất tạo
ra văn bản tiếp theo. Cụ thể, với một đoạn được
tạo gần đây ct, bộ truy xuất được huấn luyện để
truy xuất các đoạn ci làm tăng xác suất của việc
chấm điểm (ct+1|ci, ct) theo một LM chấm điểm
tham chiếu. Hình 1 cung cấp một ví dụ minh họa
cho trường hợp một hiện trường tội phạm được
mô tả, và một LM chấm điểm cho thấy lợi ích của
việc truy xuất một đoạn cách hàng nghìn token
(đoạn 13) so với truy xuất từ vựng, dẫn đến một
đoạn chỉ liên quan về mặt bề ngoài (đoạn 100).
Khác với các mô hình tăng cường truy xuất hiện tại
sử dụng một encoder phụ trợ cho truy xuất (Izacard
and Grave, 2021a; Izacard et al., 2022b; Sachan
et al., 2021), RPT có thể tận dụng các trạng thái
ẩn nội bộ cho truy xuất sau một giai đoạn tiền huấn
luyện duy nhất, đơn giản hóa đáng kể việc huấn
luyện chung.
Chúng tôi áp dụng RPT cho vấn đề mô hình hóa
các tài liệu dài, như sách, bài viết và mã, vì đây
là những ví dụ tự nhiên của nội dung dạng dài,
trong đó toàn bộ chỉ mục có thể được giữ trong
bộ nhớ trong một lần truyền xuôi.Chúng tôi đánh giá RPT trên bốn nhiệm vụ mô hình
ngôn ngữ và thấy rằng nó cải thiện perplexity trên
tất cả các nhiệm vụ, vượt trội hơn các nghiên cứu
trước (Hutchins et al., 2022; Wu et al., 2022) cũng
như các baseline mạnh mẽ (Borgeaud et al., 2022;
Zhong et al., 2022). Hơn nữa, chúng tôi cho thấy
rằng RPT truy xuất các đoạn chất lượng cao so với
các bộ truy xuất dựa trên thông tin từ vựng. Dựa
trên các phát hiện thực nghiệm, chúng tôi lập luận
rằng RPT có thể mở đường cho thế hệ tiếp theo
của các LM được tiền huấn luyện, trong đó các
kho ngữ liệu lớn được sử dụng trong quá trình
tiền huấn luyện, dẫn đến các mô hình ngôn ngữ
trong đó truy xuất là một thành phần được nhúng
mạnh mẽ. Mã nguồn của chúng tôi được công khai
tại https://github.com/OhadRubin/RPT.
2 Nền tảng
Để định vị đóng góp của chúng tôi, chúng tôi xem
xét các nghiên cứu RALM liên quan gần đây. Chúng
tôi mở rộng thêm các nghiên cứu liên quan khác
trong §6.
Các nghiên cứu đầu tiên về RALMs, như kNN-LM
(Khandelwal et al., 2020) đã sử dụng truy xuất để
cải thiện mô hình ngôn ngữ bằng cách nội suy phân
phối từ tiếp theo được tạo ra bởi LM với một phân
phối được đề xuất thông qua cơ chế truy xuất chỉ
tại thời điểm kiểm tra. Borgeaud et al. (2022) sau
đó đề xuất Chunked Cross-Attention (CCA), trong
đó truy xuất cũng được thực hiện tại thời điểm
huấn luyện, và kết quả truy xuất được hợp nhất
sâu vào các biểu diễn được tạo ra bởi một bộ giải
mã Transformer thông qua attention. Tuy nhiên,
bộ truy xuất được huấn luyện riêng biệt và giữ
cố định trong quá trình huấn luyện, điều này ngăn
cản nó thích ứng với LM trong suốt quá trình huấn
luyện.
TRIME (Zhong et al., 2022), giống như công
trình này, đã huấn luyện một LM tăng cường truy
xuất từ đầu trong đó thành phần truy xuất và LM
giải mã được huấn luyện chung. Công trình của
chúng tôi khác với TRIME ở hai khía cạnh: Thứ
nhất, TRIME, giống như kNN-LM, kết hợp thông
tin từ bộ truy xuất theo cách nông qua nội suy
phân phối, trong khi chúng tôi áp dụng CCA như
một cơ chế hợp nhất sâu hơn. Thứ hai, TRIME
tận dụng các manh mối từ vựng để giám sát bộ
truy xuất, tức là với một truy vấn, bộ truy xuất
TRIME học cách truy xuất các ngữ cảnh sẽ dẫn
đến việc tạo ra cùng một token với truy vấn. Ngược
lại, chúng tôi sử dụng một LM chấm điểm để đánh
giá những đoạn văn bản nào có liên quan để tăng
xác suất của đoạn được tạo ra, dẫn đến truy xuất
ngữ nghĩa hơn. Điều này tương tự như EPR (Rubin
et al., 2022), đã sử dụng ý tưởng này để học cách
truy xuất prompts cho học trong ngữ cảnh, và chưng
cất perplexity trong Atlas (Izacard et al.,

--- TRANG 3 ---
2022b). Tuy nhiên, Atlas không huấn luyện bộ
truy xuất và LM từ đầu và là một mô hình encoder-
decoder, phù hợp hơn cho các nhiệm vụ đòi hỏi
kiến thức nhiều. Ngược lại, chúng tôi huấn luyện
từ đầu và sử dụng một mô hình decoder, phù hợp
hơn cho việc mô hình hóa văn bản dài.
3 Retrieval-Pretrained Transformer
Thiết lập vấn đề Giống như RETRO (Borgeaud
et al., 2022), RPT là một LM tăng cường truy
xuất theo đoạn chia chuỗi đầu vào thành các đoạn
để truy xuất. Cụ thể, với một chuỗi L token đầu
vào, (x1, x2, . . . , xL), chúng tôi phân chia nó thành
một chuỗi ℓ = L/m đoạn không chồng lấp có độ
dài m, ký hiệu là C = (c1, c2, . . . , cℓ). Đối với
mỗi đoạn truy vấn có thể, cq = ci, mô hình sẽ
truy xuất một tập con tối đa K ≪ ℓ đoạn, R(cq) ⊂
C<i = (c1, c2, ..., ci−w), trong đó C<i là tập hợp
các đoạn có thể truy xuất cho ci, loại trừ w đoạn
mà nó đã có quyền truy cập thông qua causal self-
attention. Mục tiêu là học một mô hình truy xuất
một tập con đoạn, R(cq), làm tăng xác suất tạo
tự động hồi quy của đoạn mục tiêu ct = ci+1.
Chúng tôi trình bày phương pháp của chúng tôi
trong hai phần. Thứ nhất, kiến trúc của chúng tôi
(§3.1), tận dụng CCA để hợp nhất các biểu diễn
được truy xuất vào LM, nhưng thêm một thành
phần truy xuất được học. Thứ hai, chúng tôi trình
bày phương pháp huấn luyện (§3.2-§3.3), trong
đó bộ truy xuất được huấn luyện để truy xuất các
đoạn hữu ích cho việc tạo ra một đoạn tương lai
theo một LM tham chiếu.
3.1 Kiến trúc mô hình
Hình 2 minh họa kiến trúc của chúng tôi, trong đó
đầu vào có 45 token đầu vào được chia thành 9
đoạn, và causal self-attention được áp dụng trên
w = 3 đoạn (15 token). Phía bên trái mô tả ngăn
xếp decoder ("reader"), và phía bên phải là bộ
truy xuất. Reader được chia thành hai, trong đó
nlayers/2 lớp dưới (lower decoder) là các lớp
Transformer decoder tiêu chuẩn nhận w đoạn làm
đầu vào và xuất ra các biểu diễn sẽ được sử dụng
bởi bộ truy xuất và các lớp decoder trên.
nlayers/2 lớp trên (upper decoder) sử dụng
Chunked Cross-Attention (CCA) để hợp nhất thông
tin từ các đoạn hàng xóm top-K được truy xuất
bởi bộ truy xuất trở lại vào LM. Chúng tôi sử dụng
các lớp CCA tiêu chuẩn từ RETRO (Borgeaud et
al., 2022), trong đó đối với mỗi một trong ℓ đoạn,
các truy vấn là các biểu diễn m token của đoạn
đó được xuất ra bởi causal attention, và các keys
và values là các biểu diễn token cho các đoạn
hàng xóm top-K được xuất ra bởi bộ truy xuất.¹
Tiếp theo, chúng tôi mô tả thành phần truy xuất,
cùng với một cơ chế neighbor gating để điều chỉnh
tác động của các biểu diễn được truy xuất.
Bộ truy xuất Bộ truy xuất nhận các biểu diễn
được xuất ra bởi lower decoder làm đầu vào và
tạo ra một điểm tương tự cho mỗi cặp đoạn. Với
một đoạn truy vấn cq, điểm dựa trên truy vấn cho
mỗi đoạn có thể truy xuất c là sq(c) = ⟨WQcq, WKc⟩,
trong đó WQ, WK ∈ Rd×d là các phép chiếu tuyến
tính được học, và cq và c là các biểu diễn đoạn.
Đối với một đoạn c dài m token, chúng tôi tính
toán biểu diễn c của nó bằng cách áp dụng bidirectional
attention trên các token của đoạn, sau đó là mean-
pooling qua chiều thời gian. Điều này duy trì tính
nhân quả, vì các biểu diễn này chỉ được sử dụng
trong quá trình dự đoán đoạn tiếp theo.
Một khi điểm số cho tất cả các cặp đoạn được
tính toán, các đoạn hàng xóm được truy xuất R(cq),
cho mỗi đoạn truy vấn, cq, bao gồm các đoạn có
thể truy xuất có điểm cao nhất top-K. Sau đó, đối
với mỗi đoạn cj ∈ R(cq), chúng tôi nối các biểu
diễn của đoạn tiếp theo cj+1 để cung cấp ngữ
cảnh bổ sung, và biểu diễn cuối cùng cho tất cả
các hàng xóm của tất cả các đoạn được cho bởi
một tensor C ∈ Rℓ×K×2m×d.²
Tổng thể (và khác với các phương pháp như
TRIME và kNN-LM), bộ truy xuất là một phần
không thể thiếu của LM, trong đó lower decoder
tính toán các biểu diễn cho bộ truy xuất (mà chúng
tôi gọi là tự truy xuất), và upper decoder tiêu thụ
các biểu diễn được tạo ra bởi bộ truy xuất.
Neighbor gating Chúng tôi thêm một cơ chế
neighbor gating để lựa chọn mềm các biểu diễn
hàng xóm hữu ích để hợp nhất vào upper decoder.
Gọi Ci,k ∈ R2m×d là các biểu diễn token cho hàng
xóm thứ k của đoạn ci. Chúng tôi mean-pool qua
chiều thời gian để có được một vector ĉi,k cho
mỗi đoạn hàng xóm. Sau đó, chúng tôi làm phong
phú biểu diễn hàng xóm của mỗi đoạn bằng cách
áp dụng causal attention – một biểu diễn đoạn
hàng xóm ĉi,k attend tới các đoạn đi trước nó
hoặc tới các hàng xóm của cùng một đoạn ci có
thứ hạng cao hơn. Cuối cùng, đối với mỗi đoạn
chúng tôi có được biểu diễn được truy xuất có gating
bằng cách nhân các biểu diễn được tăng cường với
một điểm gating:
¹Để biết đầy đủ chi tiết của CCA, xem Borgeaud et al. (2022).
²Tương tự như RETRO, các biểu diễn token của các đoạn
được truy xuất cũng được tăng cường thông qua cross-attention
trên các token của đoạn truy vấn, cq.

--- TRANG 4 ---
Causal AttentionFeed ForwardChunked Cross AttentionTop-KQKVCausal Attention
Pool + ProjectFeed Forward
Bi-directionalAttention⟨|||||||||⋅aaaa		=7.1𝑐1𝑐1𝑐1𝑐8𝑐1𝑐1𝑐1𝑐1⟨|||||||||⋅aaaa		=0.3𝑐1𝑐1𝑐1𝑐8𝑐1𝑐1𝑐1𝑐2⟨|||||||||⋅aaaa		=5.2𝑐1𝑐1𝑐1𝑐8𝑐1𝑐1𝑐1𝑐3⟨|||||||||⋅aaaa		=10.8𝑐1𝑐1𝑐1𝑐8𝑐1𝑐1𝑐1𝑐!⟨|||||||||⋅aaaa		=−∞𝑐1𝑐1𝑐1𝑐8𝑐1𝑐1𝑐1𝑐" ⋮⟨|||||||||⋅aaaa		=4.8𝑐1𝑐1𝑐1𝑐8𝑐1𝑐1𝑐1𝑐# ⋮READ𝑐1𝑐2𝑐3𝑐4𝑐5𝑐6𝑐7𝑐8𝑐9×	𝑛!"#$%&2Encoded neighbors
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1Input Tokens
Lower DecoderChunk scoringNeighbor gating
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1
𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1𝑐1RetrieverUpper Decoder
×	𝑛!"#$%&2Hình 2: Kiến trúc của Retrieval-Pretrained Transformer, trong đó một đầu vào 45 token được hiển thị, bao gồm
9 đoạn, và causal self-attention được áp dụng trên 15 token. Phía bên trái hiển thị ngăn xếp decoder, trong đó
nlayers/2 lớp dưới là các lớp Transformer decoder tiêu chuẩn, và nlayers/2 lớp trên cũng bao gồm các lớp chunked
cross-attention hợp nhất thông tin từ các đoạn được truy xuất. Phía bên phải hiển thị bộ truy xuất, nhận một đoạn
và truy xuất K đoạn có điểm cao nhất đã xuất hiện trước đó trong tài liệu.
Cg
i,k = max {η, σ(wng ĉi,k/√d)} · Ci,k trong đó wng
là một vector tham số được học, η là một giá trị
nhỏ nhằm duy trì dòng gradient,³ và σ là hàm kích
hoạt sigmoid. Cuối cùng, trong upper decoder, khi
CCA được thực hiện, các keys và values là Cg
i,k.
3.2 Tín hiệu giám sát
Đối với mỗi đoạn truy vấn cq = ci, chúng tôi muốn
xác định các đoạn hàng xóm sẽ hữu ích cho việc
tạo ra ct = ci+1, và sử dụng những đoạn hàng
xóm đó làm tín hiệu giám sát cho bộ truy xuất.
Tương tự như Rubin et al. (2022), chúng tôi có
thể khai thác thực tế rằng chúng tôi đang tạo ra
dữ liệu huấn luyện và sử dụng thông tin từ chính
ct để tạo ra điểm số như vậy. Khác với Zhong et
al. (2022), người chỉ sử dụng các manh mối từ
vựng, chúng tôi sẽ sử dụng một LM chấm điểm
độc lập cho mục đích này.
Việc chấm điểm mỗi đoạn so với tất cả các đoạn
trước đó là bậc hai theo số lượng đoạn trong một
tài liệu, và do đó khó khăn về mặt tính toán. Do
đó, chúng tôi sử dụng một bộ truy xuất không giám
sát BM25 đơn giản (Robertson and Zaragoza, 2009)
nhận làm đầu vào sự nối của các đoạn (cq, ct) =
(ci, ci+1) và trả về một tập hợp các đoạn hàng
xóm ứng viên, R̄ ⊂ C(cq), có độ chồng lắp từ
vựng cao với đoạn hiện tại và tiếp theo. Bộ truy
xuất này có quyền truy cập vào các token cần được
tạo ra bởi LM, điều này được phép tại thời điểm
huấn luyện.
Gọi ĝ là một LM được huấn luyện độc lập, và gọi
c̄j là sự nối (cj, cj+1). Chúng tôi tính toán một
điểm số st(c̄j) phản ánh liệu thông tin trong c̄j
có hữu ích hơn cho việc giải mã ct so với các đoạn
gần với cq hay không. Cụ thể, điểm số dựa trên
mục tiêu cho một đoạn ứng viên là
st(c̄j) = log Prob ĝ(ct|cj, cj+1, cq) / Prob ĝ(ct|ci−2, ci−1, cq).
Điểm số này là dương khi thông tin trong c̄j hữu
ích hơn cho việc giải mã ct so với thông tin trong
hai đoạn trước đó (ci−2, ci−1).
Chúng tôi áp dụng hàm chấm điểm này cho tất
cả các đoạn, và định nghĩa cho mỗi đoạn truy vấn
cq tập hợp các đoạn dương Rq
pos, bao gồm các ứng
viên mà st(·) > 0. Điều này sẽ dẫn đến các đoạn
hữu ích, vì mỗi đoạn ứng viên ít nhất cũng tốt
bằng ngữ cảnh cục bộ. Với thứ tự này, chúng tôi
có thể áp dụng các phương pháp huấn luyện truy
xuất tiêu chuẩn.
3.3 Huấn luyện
Để huấn luyện các tham số của thành phần truy
xuất, chúng tôi điều chỉnh loss LambdaRank được
sử dụng rộng rãi (Burges et al., 2006). Loss cho
mỗi đoạn truy vấn
³Chúng tôi đặt η = 0.1 trong tất cả các thí nghiệm của chúng tôi.

--- TRANG 5 ---
cq (w.r.t các đoạn có thể truy xuất của nó) là:
Lret(cq) =
X
{j,l:c̄l∈Rq
pos,st(c̄l)>st(c̄j)}λjl max (0 , τ−(sq(cl)−sq(cj)))
trong đó τ là một siêu tham số margin, và λjl là
tỷ lệ LambdaRank xem xét thứ hạng tương đối của
mỗi ứng viên. Loss này khác không khi đối với
một số cặp ứng viên, điểm số dựa trên mục tiêu
không đồng ý (với margin τ) với thứ hạng của
điểm số dựa trên truy vấn cho các ứng viên trong
Rq
pos. Tối ưu hóa hàm loss này cho phép RPT phân
biệt giữa các đoạn liên quan và không liên quan.
Loss cuối cùng của chúng tôi là LLM + αretLret,
trong đó LLM là loss LM tiêu chuẩn và αret là hệ
số loss truy xuất, tăng tuyến tính trong 100K bước
đầu tiên. Chúng tôi cũng tăng τ tuyến tính trong
quá trình huấn luyện.
3.4 Chi tiết triển khai quan trọng
Scheduled sampling Để giảm sự không khớp
train-test, chúng tôi áp dụng scheduled sampling
(Bengio et al., 2015) trong quá trình huấn luyện.
Cụ thể, sau khi tính toán các đoạn hàng xóm top-K,
chúng tôi sử dụng những hàng xóm này với xác
suất 1−pss, và với xác suất pss các ứng viên có
điểm cao nhất top-K từ Rq
pos làm đầu vào cho
CCA. Chúng tôi giảm dần pss từ 1 về 0 trong 90%
đầu của quá trình huấn luyện với lịch trình cosine.
Điều này cho phép mô hình dần dần học cách sử
dụng các dự đoán của chính nó. Chúng tôi báo cáo
tác động của điều này trong §5.3.
Sliding window attention tại thời điểm huấn
luyện và suy luận Như đã mô tả trong §3, decoder
nhận w đoạn làm đầu vào, mỗi đoạn có m token
làm đầu vào, và áp dụng causal attention trên
chúng. Trong thực tế, để cho các token đầu tiên
truy cập vào các token quá khứ, chúng tôi sử dụng
cơ chế sliding-window attention (Dai et al., 2019;
Beltagy et al., 2020; Ivgi et al., 2023), trong đó
số lượng token trong một cửa sổ là 2,048 và stride
là 1,024. Do đó, đầu vào cho mỗi cửa sổ là 2,048
token và đầu ra là các biểu diễn cho 1,024 token
cuối cùng, sử dụng các keys và values của 1,024
token trước đó để tạo ngữ cảnh.
Tại thời điểm suy luận, một quy trình tương tự
được áp dụng. Chúng tôi tính toán và cache các
biểu diễn key và value cho các đoạn 1,024 token,
sử dụng chúng làm ngữ cảnh để tạo ra hoặc ước
tính xác suất của đoạn tiếp theo.
Truy xuất tại thời điểm suy luận Trong quá
trình huấn luyện, chúng tôi mã hóa trong mỗi batch
các chuỗi có độ dài 16K và truy xuất các đoạn từ
những 16k token được mã hóa đó.Tên Tokens (Train/Test) Độ dài Trung vị
ArXiv 12,000 / 16 16,368
CodeParrot 5,000 / 5 29,269
PG19 3,000 / 9 82,659
Books3 25,000 / 35 113,496
Bảng 1: Số lượng token (tính bằng triệu) cho mỗi dataset
và độ dài tài liệu trung vị.
Tuy nhiên, tại thời điểm suy luận, bộ truy xuất
cung cấp quyền truy cập vào tất cả các token từ
đầu tài liệu, trong đó chúng tôi lưu trữ các biểu
diễn key và lower-decoder trong một chỉ mục Faiss
(Douze et al., 2024) trên CPU. Đối với mỗi đoạn,
chúng tôi truy vấn chỉ mục sử dụng các biểu diễn
truy vấn của đoạn và truy xuất các biểu diễn lower-
decoder top-K có tích vô hướng cao nhất.
Chi tiết bổ sung Tại thời điểm huấn luyện, chúng
tôi sử dụng các chuỗi có độ dài L = 16,384 token,
được chia thành 4 thiết bị, mỗi thiết bị tiêu thụ
4,096 token. Như đã đề cập, ngăn xếp decoder
nhận 2,048 token làm đầu vào (theo phương pháp
sliding window), chứa ℓ = 32 đoạn có độ dài m = 64.
Chúng tôi sử dụng Rotary Positional embedding
(Su et al., 2024), và huấn luyện tất cả các mô hình
trong 500K bước trên TPUv4-64, với kích thước
batch hiệu quả là 217 token dẫn đến tổng ngân
sách huấn luyện 65 tỷ token.
Đối với tất cả các mô hình được huấn luyện, chúng
tôi sử dụng tokenizer GPT-NeoX (Black et al.,
2022), được huấn luyện trên Pile (Gao et al., 2020)
và bao phủ các miền chúng tôi đánh giá (xem §4).
Làm mô hình ngôn ngữ chấm điểm, chúng tôi sử
dụng phiên bản deduplicated 1.4B tham số của
Pythia (Biderman et al., 2023), và chấm điểm với
nó 20 ứng viên BM25 hàng đầu. Mô hình của chúng
tôi có 12 lớp, chiều ẩn d = 1024, và 8 attention
head với chiều head là 128. Chúng tôi áp dụng
CCA với 2 hàng xóm, trừ khi được đề cập khác.
Chi tiết triển khai bổ sung có trong Phụ lục A và
độ phức tạp lý thuyết của các lớp CCA có trong
Phụ lục B.
4 Datasets Mô hình Ngôn ngữ Tầm xa
Chúng tôi đánh giá RPT trên bốn dataset, bao phủ
các miền như sách, mã và văn bản toán học, đòi
hỏi khả năng nhớ lại thông tin trên khoảng cách
xa. Bảng 1 và Hình 3 cung cấp thống kê về kích
thước dataset và phân phối độ dài tài liệu, cho
thấy rằng các tài liệu dài trên tất cả các dataset
và đặc biệt là PG19 và Books3, nơi các tài liệu
thường chứa 105 token hoặc nhiều hơn. Chúng tôi
xem xét ngắn gọn các dataset.

--- TRANG 6 ---
0510%ArXiv
0510%CodeParrot
0510%PG19
102103104105106107
Sequence length0510%Books3Hình 3: Biểu đồ phân phối độ dài tài liệu tính bằng token
trên tất cả các dataset. Trục x ở thang đo log.
PG19 Được giới thiệu trong Rae et al. (2020),
PG19 là một benchmark mô hình ngôn ngữ tầm xa
được sử dụng rộng rãi chứa các cuốn sách từ Project
Gutenberg, và bao phủ một loạt các thể loại văn
học, phong cách và chủ đề. Chúng tôi áp dụng thiết
lập và phân chia dữ liệu chính xác từ các nghiên
cứu trước (Wu et al., 2022; Hutchins et al., 2022;
Mehta et al., 2023).
Books3 là một corpus sách được phát hành như
một phần của Pile (Gao et al., 2020), chứa một
bộ sưu tập rộng lớn các tác phẩm văn học từ các
miền khác nhau. Theo hiểu biết của chúng tôi,
chúng tôi là những người đầu tiên sử dụng corpus
này như một benchmark mô hình ngôn ngữ tầm
xa.⁴
CodeParrot (Wolf et al., 2023) là một corpus
mã Python sạch, gần như không trùng lặp từ các
kho GitHub khác nhau. Mô hình hóa mã đòi hỏi
hiểu các mẫu và ngữ cảnh hóa thông tin trên khoảng
cách xa, làm cho nó trở thành một ứng viên tự
nhiên để kiểm tra các LM tầm xa. Trong các thí
nghiệm của chúng tôi, chúng tôi làm theo phương
pháp của Wu et al. (2022), kết hợp các tệp từ cùng
một kho để xây dựng một corpus với các chuỗi dài
hơn, và tạo ra một phân chia train/test (xem Bảng 1).
ArXiv là một corpus các bài báo preprint được
trích xuất từ ArXiv. Nó bao gồm các văn bản toán
học đòi hỏi duy trì tính nhất quán và tham chiếu
đến thông tin đã đề cập trước đó trên văn bản mở
rộng.
⁴Chúng tôi không phát hành benchmark này do các hạn chế
bản quyền.Các nghiên cứu trước đã đánh giá các LM tầm xa
trên corpus này (Wu et al., 2022; Hutchins et al.,
2022; Mehta et al., 2023), nhưng không phát hành
corpus của họ. Do đó, chúng tôi sử dụng corpus
được tiền xử lý và phân chia dữ liệu do Azerbayev
et al. (2023) cung cấp.
5 Thí nghiệm
Bây giờ chúng tôi chuyển sang các thí nghiệm để
so sánh RPT với các nghiên cứu trước trên bốn
dataset của chúng tôi.
5.1 Thiết lập thí nghiệm
Chúng tôi so sánh với các baseline và oracle sau.
Transformer-XL Baseline đơn giản nhất của chúng
tôi là một ngăn xếp transformer decoder tiêu chuẩn
với sliding window attention. Nói cách khác, chúng
tôi chỉ đơn giản loại bỏ khỏi RPT thành phần truy
xuất và các lớp CCA trong upper decoder. Sử dụng
sliding window attention (như đã mô tả trong §3.4)
có thể được xem như một biến thể của Transformer-
XL (Dai et al., 2019). Chúng tôi so sánh RPT với
Transformer-XL trong nhiều thiết lập, một trong
đó chúng tôi có cùng số lớp và bước huấn luyện
cho cả hai mô hình, và hai thiết lập khác trong đó
chúng tôi cân bằng số tham số và FLOPs giữa các
mô hình.
RETRO Chúng tôi triển khai một phiên bản chỉnh
sửa của Borgeaud et al. (2022), một mô hình tăng
cường truy xuất, trong đó chúng tôi cung cấp các
hàng xóm top-K được truy xuất bởi BM25⁵ làm
đầu vào cho các lớp CCA trong upper decoder.
Cụ thể, Borgeaud et al. (2022) đã thực hiện CCA
trên biểu diễn từ một encoder song hướng riêng
biệt, trong khi biến thể của chúng tôi sử dụng các
biểu diễn lower-decoder như một sự thay thế. Điều
này làm cho kiến trúc RPT và RETRO tương tự
nhau hơn và cho phép đánh giá tập trung vào tầm
quan trọng của việc huấn luyện bộ truy xuất, đây
là trọng tâm của công trình chúng tôi. Trong quá
trình huấn luyện, chúng tôi sử dụng truy vấn (cq, ct),
vì chúng tôi có quyền truy cập vào đoạn mục tiêu.
Trong quá trình suy luận, chúng tôi sử dụng cq.
RPT-Lex Một phiên bản của RPT, trong đó tín
hiệu huấn luyện được lấy hoàn toàn từ thông tin
từ vựng, tương tự như TRIME (Zhong et al., 2022).
Rõ ràng, tập hợp các đoạn dương Rq
pos cho một
đoạn cq chứa 20 đoạn hàng đầu có điểm BM25
cao nhất với (cq, ct).
RPT-Sem Mô hình đầy đủ của chúng tôi được mô
tả trong §3.
⁵Nghiên cứu đồng thời (Doostmohammadi et al., 2023) cho
thấy rằng huấn luyện RETRO sử dụng BM25 vượt trội hơn
các phương pháp truy xuất dense.

--- TRANG 7 ---
Model ArXiv Code PG19 Books3 Params Time/update
TRANSFORMER -XL(OUR IMPL .) 3.11 2.30 11.48 15.00 202M 1 ×
+2LAYERS 3.07 2.26 11.2 14.52 228M 1.14 ×
1.5×ADDITIONAL STEPS 3.11 2.26 11.39 14.70 202M 1 ×
RETRO W . BM25 ( OUR IMPL .) 2.94 2.17 11.44 14.60 236M 1.35 ×
RPT-L EX 2.92 2.23 11.59 14.32 242M 1.51 ×
RPT-S EM 2.77 2.17 10.96 13.91 242M 1.51 ×
W. 3NEIGHBOURS 2.75 2.16 10.92 13.87
W. 4NEIGHBOURS 2.74 2.15 10.93 13.91
MEMORIZING TRANSFORMER (32K) 2.92 2.18 10.97 14.40 212M 1.82 ×
MEMORIZING TRANSFORMER (65K) 2.93 2.15 10.99 14.3 212M 2.12 ×
BLOCK -RECURRENT TRANSFORMER 2.89 2.73 10.95 14.64 212M 1.56 ×
GRIFFIN 3.08 2.24 11.26 14.16 240M 1.15 ×
RPT-L EX W . ORACLE 2.80 2.12 10.88 13.30 242M 1.51 ×
RPT-S EM W . ORACLE 2.69 2.10 10.26 12.74 242M 1.51 ×
Bảng 2: Perplexity tập kiểm tra cho tất cả các dataset cùng với số lượng tham số và mức tăng tương đối về thời gian
mỗi lần cập nhật trong quá trình huấn luyện so với Transformer-XL. Trừ khi được chỉ định, các mô hình được huấn
luyện trong 500k bước và sử dụng 2 hàng xóm trong quá trình suy luận.
Block-Recurrent Transformer Chúng tôi sử dụng
triển khai huấn luyện chính thức⁶ của Block-
Recurrent Transformer (Hutchins et al., 2022) với
cấu hình mặc định.
Memorizing Transformer Chúng tôi sử dụng triển
khai chính thức⁶ của Memorizing Transformers
(Wu et al., 2022), với cấu hình mặc định và kích
thước bộ nhớ 32K và 65K token.
Griffin Một lựa chọn thay thế cho mô hình tầm
xa là sử dụng một hybrid của attention và RNN
tuyến tính (Orvieto et al., 2023; Gupta et al., 2023).
Chúng tôi đánh giá Griffin (De et al., 2024), một
mô hình tiên tiến trong loại này. Chúng tôi điều
chỉnh triển khai chính thức, và bổ sung baseline
Transformer-XL của chúng tôi với 5 lớp hồi quy
trong các lớp cuối cùng để đảm bảo tính ngang
bằng về tham số. Chúng tôi sử dụng chiều trạng
thái 2,048 và chiều thời gian 3.
Oracle Đối với mỗi đoạn kiểm tra, chúng tôi có
thể tìm kiếm toàn diện và sử dụng tại thời điểm
kiểm tra các hàng xóm tốt nhất có thể cho một
mô hình theo LM chấm điểm. Điều này cung cấp
một cận trên cho hiệu suất của RPT-Sem, vì nó
được huấn luyện để bắt chước thứ hạng được tạo
ra bởi oracle này.
Metrics Chúng tôi sử dụng perplexity để đánh
giá hiệu suất của các mô hình. Ngoài ra, chúng tôi
sử dụng điểm mục tiêu st(·) từ LM chấm điểm để
tính toán cho mỗi đoạn một thứ hạng vàng trên
tất cả các đoạn trước đó, và gán nhãn các đoạn
là dương/âm nếu điểm mục tiêu của chúng là
dương/âm, tương ứng. Với thông tin này, chúng
tôi có thể đánh giá Precision@k, là tỷ lệ các đoạn
top-k theo điểm dựa trên truy vấn mà là dương,
và Recall@k, là tỷ lệ các đoạn dương nằm trong
các đoạn top-k theo điểm dựa trên truy vấn. Chúng
tôi cũng sử dụng thứ hạng vàng để tính NDCG@k,
là một metric truy xuất tiêu chuẩn (Järvelin and
Kekäläinen, 2002).
⁶https://github.com/google-research/
meliad .5.2 Kết quả
Bảng 2 hiển thị kết quả chính của chúng tôi, cho
thấy rằng RPT-Sem có thể so sánh hoặc tốt hơn
tất cả các baseline khác trong mọi trường hợp.
Sử dụng bộ truy xuất cố định (RETRO) cải thiện
hiệu suất so với Transformer-XL; RPT-Lex dẫn
đến cải thiện trong Books3 nhưng giảm trong
PG19 so với RETRO, và RPT-Sem vượt trội hơn
Transformer-XL, RETRO và RPT-Lex trên ArXiv,
PG19 và Books3, và có hiệu suất tương đương với
RETRO trên CodeParrot. Ngay cả trong thiết lập
cân bằng tham số và tính toán, Transformer-XL
vẫn hoạt động kém hơn đáng kể so với RPT. So
với Block-Recurrent Transformer, Memorizing
Transformers và Griffin, không sử dụng CCA, hiệu
suất lại tương tự hoặc tốt hơn, với những cải thiện
đáng kể trên ArXiv và Books3.
CCA cho phép tăng động số lượng hàng xóm tại
thời điểm suy luận. Khi sử dụng 3 hoặc 4 hàng
xóm (thay vì 2), hiệu suất cải thiện, cho phép trao
đổi tính toán-hiệu suất.
Cuối cùng, các mô hình oracle liên tục đạt được
perplexity tốt nhất trên tất cả các dataset, cải thiện
từ 2.74→2.69 trên ArXiv, 2.15→2.10 trên CodeParrot,
10.92→10.26 trên PG19 và 13.87→12.74 cho Books3.
Điều này cho thấy rằng cải thiện huấn luyện bộ
truy xuất có thể cải thiện thêm hiệu suất.

--- TRANG 8 ---
Dataset Precision@2 Recall@10 nDCG@20
BM25 RPT-L RPT-S BM25 RPT-L RPT-S BM25 RPT-L RPT-S
ArXiv 27% 26% 32% 55% 54% 58% 24% 24% 30%
Code 29% 26% 34% 53% 52% 56% 25% 23% 30%
PG19 22% 22% 28% 55% 55% 61% 18% 18% 23%
Books3 23% 19% 26% 55% 50% 58% 18% 16% 22%
Avg 25.2% 23.2% 30.0% 54.5% 52.7% 58.2% 21.2% 20.2% 26.2%
Bảng 3: Metrics truy xuất kiểm tra trên các dataset.
Metrics truy xuất Bảng 3 trình bày các metrics
truy xuất w.r.t các đoạn dương oracle. Một lần
nữa, truy xuất với RPT-Sem vượt trội hơn cả RPT-
Lex và BM25 trong mọi trường hợp. Điều này cho
thấy tầm quan trọng của việc huấn luyện bộ truy
xuất, và hơn nữa việc sử dụng giám sát ngữ nghĩa
dẫn đến truy xuất tốt hơn so với chỉ tín hiệu từ
vựng.
5.3 Ablations
Bảng 4 hiển thị kết quả của một nghiên cứu ablation
trên tất cả các dataset.
Only Teacher Forcing Chúng tôi buộc mô hình
attend tới các hàng xóm vàng theo LM chấm điểm,
mà không giảm dần pss trong quá trình huấn luyện.
Điều này dẫn đến giảm hiệu suất trên tất cả các
dataset, và đặc biệt cho PG19 và Books3.
No Teacher Forcing Ở đây, chúng tôi làm ngược
lại và cố định pss = 0 trong suốt quá trình huấn
luyện, tức là chúng tôi chỉ sử dụng các hàng xóm
được dự đoán chứ không phải hàng xóm vàng.
Điều này có thể dẫn đến việc huấn luyện thiếu
các lớp CCA vì chúng được tiếp xúc với các hàng
xóm chất lượng thấp ở đầu quá trình huấn luyện
và kết quả giảm thậm chí nhiều hơn so với Only
Teacher Forcing.
No neighbor gating Chúng tôi vô hiệu hóa neighbor
gating điều khiển dòng thông tin từ các đoạn hàng
xóm và phân tích tác động lên hiệu suất mô hình.
Chúng tôi quan sát thấy giảm hiệu suất trên tất
cả các dataset, đáng chú ý trên Books3, nơi perplexity
tăng 4.5 điểm.
Model ArXiv Code PG19 Books3
RETRO W . BM25 ( OUR IMPL .) 2.94 2.17 11.44 14.60
W. DPR- STYLE RETRIEVER 2.97 2.28 11.7 14.86
RPT-L EX 2.92 2.23 11.59 14.32
W. DPR- STYLE RETRIEVER 2.84 2.26 11.11 14.17
RPT-S EM 2.77 2.17 10.96 13.91
W. DPR- STYLE RETRIEVER 2.98 2.33 11.62 14.66
RPT-S EM- ONLY TEACHER FORCING 2.91 2.22 11.54 14.66
RPT-S EM- NOTEACHER FORCING 2.95 2.26 13.10 14.40
RPT-S EM- NONEIGHBOR GATING 2.92 2.20 11.50 18.68
Bảng 4: Kết quả nghiên cứu ablation của chúng tôi.DPR-style retriever Để nghiên cứu tầm quan trọng
của huấn luyện chung, chúng tôi kiểm tra hiệu suất
khi sử dụng các bộ truy xuất được huấn luyện riêng
biệt từ LM, do đó gây ra sự không khớp train-test.
Chúng tôi huấn luyện các bộ truy xuất dense sử
dụng quy trình huấn luyện DPR tiêu chuẩn (Karpukhin
et al., 2020) trên mỗi dataset (xem Phụ lục C cho
chi tiết huấn luyện), và cho mỗi mô hình CCA của
chúng tôi sử dụng bộ truy xuất này thay vì bộ mà
nó được huấn luyện cùng. Thú vị, chúng tôi quan
sát thấy RPT-Lex có thể sử dụng hiệu quả các
hàng xóm kiểu DPR mang lại cải thiện hiệu suất
nhẹ trên 3 trong 4 dataset.
Như mong đợi, hai mô hình được huấn luyện với
các bộ truy xuất mạnh hơn bị ảnh hưởng bởi sự
không khớp train-test, việc thay thế bộ truy xuất
BM25 và bộ truy xuất RPT-Sem bằng bộ truy xuất
kiểu DPR khiến cả hai mô hình bị giảm hiệu suất
trên tất cả các dataset, cho thấy rằng hiệu suất
không bị ablation là kết quả của sự phối hợp giữa
bộ truy xuất và mô hình ngôn ngữ.
5.4 Phân tích
121416182022T oken overlapArXiv Books3
121416182022T oken overlapCodeParrot PG19
RPT-Sem RPT-Lex RETRO+BM25 Query T argetHình 4: Chúng tôi đo số lượng token duy nhất chồng lắp
giữa các đoạn query/target và hàng xóm được truy xuất
tốt nhất.
Token overlap Hình 4 vẽ số token trung bình
chồng lắp giữa các đoạn query/target trong hàng
xóm được truy xuất tốt nhất cho RETRO, RPT-Lex
và RPT-Sem. RPT-Sem truy xuất các đoạn văn có
độ chồng lắp cao hơn với đoạn mục tiêu so với
RPT-Lex. Tự nhiên, BM25 truy xuất các đoạn có
độ chồng lắp cao nhất với đoạn truy vấn

--- TRANG 9 ---
nhưng điều này không dẫn đến độ chồng lắp từ vựng
cao hơn cho đoạn mục tiêu.
1234567891011121314151617181920
T op-K element according to BM250.000.050.100.150.20Average maximum target score across chunks
Dataset
Books3
ArXiv
CodeParrot
PG19
Hình 5: Điểm mục tiêu tối đa st(·) cho các đoạn top-K
được truy xuất bởi BM25 trung bình trên các đoạn và
cho tất cả các dataset. Vì điểm mục tiêu tối đa cho 20
đoạn hàng đầu cao hơn nhiều so với top-2, việc học
cách xếp hạng lại 20 ứng viên BM25 hàng đầu có thể
dẫn đến cải thiện đáng kể trong chất lượng truy xuất.
Chất lượng giám sát Chúng tôi huấn luyện RPT-
Sem sử dụng thông tin từ hàm chấm điểm mục tiêu
st(·), mà chúng tôi thấy dẫn đến cải thiện mô hình.
Tuy nhiên, hàm chấm điểm mục tiêu chỉ cung cấp
một sự xếp hạng lại của 20 ứng viên hàng đầu theo
BM25. Do đó, một câu hỏi tự nhiên là chất lượng
giám sát cải thiện bao nhiều thông qua việc xếp
hạng lại này. Hình 5 hiển thị cho mỗi thứ hạng K
điểm mục tiêu tối đa trong số các đoạn top-K theo
BM25, trung bình trên các đoạn và trên 4 dataset
của chúng tôi. Rõ ràng, việc xếp hạng lại 20 ứng
viên BM25 hàng đầu có nhiều tiềm năng, vì điểm
mục tiêu tối đa cao hơn nhiều cho 20 ứng viên
hàng đầu so với top-2. Điều này gợi ý rằng việc
huấn luyện dài hơn và tốt hơn của bộ truy xuất có
thể cải thiện thêm hiệu suất của RPT-Sem.
Thú vị, phân tích của chúng tôi làm sáng tỏ tại
sao RPT-Sem vượt trội rõ ràng hơn RETRO trên
Books3 và PG19 nhưng ít hơn trên CodeParrot.
Điểm mục tiêu tối đa cho CodeParrot khi k = 2
đã khá cao – khoảng 0.1, tương ứng với cải thiện
hơn 10% trong xác suất của đoạn mục tiêu so với
ngữ cảnh cục bộ. Ngược lại, đối với PG19 và Books3,
điểm mục tiêu khi k = 2 gần với 0 hơn.
Phân tích nhóm con Hình 6 hiển thị cải thiện tương
đối trung bình (trên các đoạn) của RETRO, RPT-
Lex và RPT-Sem so với Transformer-XL,
0102030% ImprovmentArXiv Books3
0102030% ImprovmentCodeParrot PG19
RPT-Sem RPT-Lex RETRO+BM25 Incorrect Correct AllHình 6: Cải thiện tương đối với/không có truy xuất đúng.
khi phân biệt giữa các trường hợp một đoạn oracle
"vàng" được truy xuất và các trường hợp không có
đoạn vàng nào được truy xuất.
Như mong đợi, RPT-Sem dẫn đến cải thiện trên
tất cả các dataset, và vượt trội hơn các baseline
khác trừ RETRO trên CodeParrot nơi hiệu suất
tương tự. Thứ hai, các trường hợp một đoạn vàng
được truy xuất thực sự thường dẫn đến cải thiện
lớn hơn, nhưng chúng tôi chứng kiến cải thiện ngay
cả trong các trường hợp không có đoạn vàng nào
được truy xuất, cho thấy rằng mô hình vẫn có thể
hưởng lợi từ những truy xuất như vậy.
Phân tích định tính Kiểm tra các đoạn được truy
xuất, chúng tôi quan sát thấy rằng bộ truy xuất
RPT có tính ngữ cảnh cao. Khi áp dụng trên mã,
nó truy xuất các định nghĩa hàm, gán biến, v.v.,
trên ArXiv nó truy xuất các định nghĩa của lemma,
định lý, v.v. Hình 7 hiển thị một ví dụ, trong đó
chúng tôi cung cấp codebase được sử dụng cho
bài báo này làm đầu vào cho mô hình của chúng
tôi và trình bày một ví dụ đoạn truy vấn nơi RPT
tạo ra truy xuất tốt hơn BM25. Chúng tôi quan sát
thấy rằng ngữ cảnh trước đó cho phép RPT truy
xuất hiệu quả một định nghĩa đối tượng liên quan,
dẫn đến loss thấp hơn.
6 Thảo luận và Nghiên cứu Liên quan
Mối quan hệ với Fusion-in-Decoder RPT có điểm
tương đồng với Fusion-in-Decoder (FiD) (Izacard
and Grave, 2021b; Ivgi et al., 2023). Trong khi
cả RPT và FiD đều sử dụng các cơ chế cross-
attention để tích hợp ngữ cảnh được truy xuất
trong các mô hình của chúng, chúng khác nhau ở
hai cách. (a) Trong FiD, truy xuất được thực hiện

--- TRANG 10 ---
@flax.struct.dataclass
class FlaxRPTRetrieverEncodedOutput(ModelOutput):
original_hidden_states: jnp.ndarray = None
encoded_hidden_states: jnp.ndarray = None
attention_mask: jnp.ndarray = None
key_chunks: jnp.ndarray = None
query_chunks: jnp.ndarray = None
chunk_mask: jnp.ndarray = None
...
class FlaxRPTModule(nn.Module):
...
def __call__(...
...
hidden_states = self.ln_f (hidden_states)
if not return_dict:
return (hidden_states,) + upcoder_outputs + lowcoder_outputs
return FlaxRPTModelOutput(
last_hidden_state=upcoder_outputs.last_hidden_state,
upcoder_hidden_states=upcoder_outputs.hidden_states,
upcoder_attentions=upcoder_outputs.attentions,
lowcoder_last_hidden_state=lowc oder_outputs.last_hidden_state,
...)
...
def forward_loglikelihood(params, rng, batch, memory):
...
outputs, lowcoder_state = _forward_loglikelihood_lowcoder (params, rng, batch)
if 'cache' in lowcoder_state:
params['cache'] = lowcoder_state['cache']
outputs = jax.tree_map(lambda x: jax.device_get(x).astype(np.float32), outputs)
neighbor_hidden_states, neighbor_mask, *_ = memory.add(
input_tokens=batch["input_tokens"],
encoded_hidden_states=outputs.encoded_hidden_states,
key_chunks=outputs. key_chunks,
query_chunks=outputs.query_chunks,
)
...
Hình 7: Một ví dụ minh họa cho thấy các hàng xóm top-1 được truy xuất cho cả mô hình RPT-Sem và BM25
được áp dụng cho mã của RPT. Biến outputs trong đoạn truy vấn là một thành viên của lớp
FlaxRPTRetrieverEncodedOutput. RPT-Sem thành công truy xuất định nghĩa của đối tượng dẫn đến
giảm loss trên đoạn mục tiêu, so với BM25.
chỉ một lần dựa trên prompt/truy vấn ban đầu, trong
khi RPT liên tục thực hiện truy xuất ở cấp độ đoạn
trong suốt quá trình tạo. (b) FiD mã hóa các hàng
xóm được truy xuất riêng biệt sử dụng một encoder
song hướng và chỉ sau đó áp dụng cross-attention
trong decoder. Trong RPT, decoder tính toán các
embedding đoạn và thực hiện truy xuất tự nhiên,
và sau đó chunked cross-attention được áp dụng
để hợp nhất ngữ cảnh được truy xuất với các dự
đoán của mô hình. Chúng tôi xem RPT, sử dụng
các encoding lower-decoder, là tự nhiên hơn trong
ngữ cảnh tạo liên tục (ví dụ: chatbot hoặc agent),
vì mô hình tạo ra các biểu diễn và sử dụng chúng
sau này làm keys, và do đó việc tạo ra các biểu
diễn truy xuất không tốn chi phí.Mô hình ngôn ngữ tầm xa Một trọng tâm chính
trong mô hình ngôn ngữ tầm xa đã là giải quyết
độ phức tạp bậc hai của attention để phát triển
các cơ chế hiệu quả hơn cho việc xử lý văn bản
dài. Ví dụ, Transformer-XL (Dai et al., 2019) xử
lý đầu vào sử dụng cơ chế cấp độ đoạn trong khi
giữ lại cache từ các đoạn trước đó. Longformer
(Beltagy et al., 2020) mở rộng ý tưởng này để
chứa các ngữ cảnh thậm chí dài hơn. Một số nghiên
cứu trước đây đã xem truy xuất như một vấn đề
tầm xa. Memorizing Transformers (Wu et al., 2022)
sử dụng một lớp k-NN duy nhất và truy xuất các
keys và values được cache, nhưng chúng không
lan truyền ngược gradient qua phép toán truy xuất
thưa. Tương tự, Bertsch et al. (2023) chứng minh
rằng phương pháp này có thể được sử dụng với
bất kỳ mô hình được tiền huấn luyện hiện có nào
và áp dụng nó ở mỗi lớp attention cho các nhiệm
vụ tóm tắt dài. Từ góc độ phân tích, các nghiên
cứu trước (Press et al., 2021) chứng minh rằng
các benchmark LM tiêu chuẩn không lý tưởng để
đo khả năng tầm xa của các mô hình. Sun et al.
(2021) thảo luận về các loại chuỗi khác nhau hưởng
lợi từ việc có ngữ cảnh dài, và Rae and Razavi
(2020) điều tra các lựa chọn kiến trúc tầm xa và
khuyến nghị tăng khả năng tầm xa trong các lớp
trên.
Mô hình ngôn ngữ hiệu quả Các chiến lược thưa,
như những chiến lược được đề xuất trong Zaheer
et al. (2020); Roy et al. (2021); Kitaev et al.
(2020), tương tự như RPT, attend chỉ một tập
con các token thông qua các phương pháp clustering
hoặc hashing, được huấn luyện bằng cách lan truyền
gradient qua phép toán thưa. Trong RPT, tính thưa
là do phép toán top-K của bộ truy xuất, được huấn
luyện sử dụng giám sát chất lượng cao từ một mô
hình ngôn ngữ tham chiếu. Một phương pháp khác
để mô hình hóa hiệu quả văn bản dài bao gồm nén
đầu vào và attend trên chuỗi được nén (Martins
et al., 2022; Rae et al., 2020), hoặc học cách bỏ
qua các token không liên quan (Sukhbaatar et al.,
2021). Tuy nhiên, về mặt thực nghiệm, hầu hết
các kiến trúc transformer hiệu quả trao đổi hiệu
quả lấy chất lượng. Gần đây, các mô hình state-
space (Mehta et al., 2023; Gu and Dao, 2023; Fu
et al., 2023) xuất hiện như một lựa chọn thay thế
hiệu quả, tiếp cận chất lượng Transformer. Trong
bài báo này, chúng tôi khám phá các mô hình dựa
trên Transformer bậc hai cổ điển. Chúng tôi lập
luận rằng mô hình cơ bản là trực giao với đóng
góp của chúng tôi và có thể được thay thế bằng
các lựa chọn thay thế hiệu quả khác và kết hợp
với truy xuất. Chúng tôi để việc khám phá này
cho công việc tương lai.
LM tăng cường truy xuất LM tăng cường truy xuất
đã nổi lên như một phương pháp nổi bật để khai
thác hiệu quả kiến thức bên ngoài trong khi tạo
văn bản. Các mô hình này có thể được chia rộng
thành những mô hình hoạt động ở mức độ chi tiết
token và những mô hình hoạt động ở mức độ chi
tiết chuỗi. Các phương pháp cấp token, như kNN-
LM (Khandelwal et al., 2020), TRIME (Zhong et
al., 2022), và SPALM (Yogatama et al., 2021),
truy xuất thông tin cho các token riêng lẻ. Các
phương pháp cấp chuỗi như RAG (Lewis et al.,
2020) sử dụng các mô hình encoder-decoder được
tiền huấn luyện với các bộ truy xuất được tiền
huấn luyện cho các nhiệm vụ như trả lời câu hỏi
miền mở. Tương tự, FiD (Izacard and Grave, 2021b)
sử dụng các mô hình encoder-decoder tạo sinh
hợp nhất bằng chứng từ nhiều đoạn văn trong quá
trình giải mã, liên quan chặt chẽ đến cơ chế CCA.
Gần đây, Wang et al. (2023) chứng minh các lợi
ích tiềm năng của việc thực hiện truy xuất và chunked
cross-attention ở mỗi bước thời gian, so với bài
báo RETRO (Borgeaud et al., 2022) gốc, truy xuất
mỗi m = 64 bước.
Huấn luyện chung retriever-reader Các phương
pháp huấn luyện chung thường tập trung vào việc
chuyển thông tin giữa một reader được tiền huấn
luyện vào một retriever được tiền huấn luyện. Các
phương pháp này thường bao gồm việc cập nhật
chỉ mục retriever trong quá trình huấn luyện trong
ngữ cảnh của các nhiệm vụ đòi hỏi kiến thức nhiều,
như trả lời câu hỏi miền mở. Ví dụ, REALM (Guu
et al., 2020) sử dụng masked language modeling
như một tín hiệu học để cập nhật retriever. EMDR2
(Sachan et al., 2021) mở rộng FiD bằng cách sử
dụng các mô hình encoder-decoder để lan truyền
ngược lỗi từ câu trả lời được dự đoán đến retriever.
Tương tự, Izacard and Grave (2021a); Jiang et
al. (2022) sử dụng điểm attention từ reader để
giám sát retriever trực tiếp sử dụng ma trận attention
như một tín hiệu huấn luyện để cho phép huấn
luyện end-to-end chung với giám sát của nhiệm
vụ downstream. Đáng chú ý, Izacard et al. (2022b)
tiếp tục mở rộng quy mô các phương pháp này và
huấn luyện chung một retriever với một mô hình
encoder-decoder, chứng minh khả năng học few-
shot mạnh mẽ. Họ cũng điều tra các kỹ thuật cập
nhật retriever khác nhau để giải quyết sự không
khớp train-test trong quá trình truy xuất. Chúng
tôi không gặp phải vấn đề cập nhật chỉ mục vì
chúng tôi tính toán toàn bộ chỉ mục thông qua một
lần truyền xuôi.
Tiền huấn luyện Retriever Các nghiên cứu đầu
tiên về tiền huấn luyện retriever dựa vào Inverse
Cloze Task không giám sát để tiền huấn luyện
retriever (Lee et al., 2019; Guu et al., 2020). Sau
đó được chỉ ra rằng việc sử dụng trực tiếp BERT
(Devlin et al., 2019) với một mục tiêu có giám
sát là đủ để có được hiệu suất tốt trên các benchmark
tiêu chuẩn (Karpukhin et al., 2020). Tuy nhiên,
paradigm này cho thấy hiệu suất kém trên các
thực thể long-tail so với BM25 (Amouyal et al.,
2023; Sciavolino et al., 2021). Gần đây, các phương
pháp tiền huấn luyện không giám sát (Gao and
Callan, 2022; Ram et al., 2022; Izacard et al.,
2022a) cho phép cải thiện hiệu suất. Tuy nhiên,
các phương pháp này được khởi tạo từ một mô
hình encoder BERT (Devlin et al., 2019) được
tiền huấn luyện, trong khi RPT là một kiến trúc
retriever-reader được huấn luyện từ đầu vượt trội
hơn BM25 mà không cần bất kỳ tiền huấn luyện
bổ sung nào.
Giám sát retriever với LLM EPR (Rubin et al.,
2022) chứng minh rằng LLM có thể được sử dụng
để huấn luyện một retriever cho truy xuất prompt
bằng cách ước tính xác suất của một đầu ra cho
trước đầu vào và một ví dụ huấn luyện ứng viên
như prompt. Các kỹ thuật tương tự được áp dụng
cho trả lời câu hỏi miền mở thông qua xếp hạng
lại kết quả truy xuất (Sachan et al., 2022; Ram
et al., 2023) và để giám sát retriever thông qua
chưng cất perplexity (Izacard et al., 2022b). Gần
đây, Shi et al. (2024) sử dụng phương pháp giám
sát này để cải thiện hiệu suất của các LLM khác
nhau theo cách black-box.
7 Kết luận
Trong công trình này, chúng tôi trình bày Retrieval-
Pretrained Transformer (RPT), một LM tăng cường
truy xuất trong đó retriever được huấn luyện như
một thành phần tự nhiên của LM để truy xuất các
đoạn có liên quan ngữ nghĩa cho dự đoán văn bản
tương lai. Chúng tôi đánh giá RPT trên bốn nhiệm
vụ mô hình ngôn ngữ tầm xa, bao gồm sách, mã
và văn bản toán học. Chúng tôi chứng minh rằng
bằng cách tích hợp liền mạch retriever vào kiến
trúc và quy trình huấn luyện, RPT hưởng lợi từ
việc hợp nhất ngữ cảnh được truy xuất, cải thiện
so với các baseline tăng cường truy xuất mạnh mẽ.
Trong khi công trình này tập trung vào truy xuất
từ văn bản dài, chúng tôi lập luận rằng các phát
hiện thực nghiệm của chúng tôi cho thấy việc điều
chỉnh quy trình của chúng tôi cho truy xuất corpus
dựa trên web tổng quát là một hướng tương lai
thú vị. Điều này sẽ đòi hỏi vượt qua các khó khăn
kỹ thuật liên quan đến mở rộng quy mô và xây
dựng corpus tiền huấn luyện. Chúng tôi hình dung
RPT sẽ mở đường cho thế hệ mới của các mô hình
ngôn ngữ được tiền huấn luyện với truy xuất được
tích hợp sâu trong suốt kiến trúc và quy trình huấn
luyện của chúng.
Lời cảm ơn
Nghiên cứu này được hỗ trợ với Cloud TPU từ
TPU Research Cloud (TRC) của Google và Hội
đồng Nghiên cứu Châu Âu (ERC) dưới chương
trình nghiên cứu và đổi mới Horizons 2020 của
Liên minh Châu Âu (grant ERC DELPHI 802800).
Ohad muốn cảm ơn Iz Beltagy vì đã gợi ý chương
trình TRC, và toàn bộ phòng thí nghiệm TAU NLP
và đặc biệt là Guy Dar và Itay Itzhak. Công trình
này được hoàn thành để hoàn thiện một phần bằng
Tiến sĩ của Ohad Rubin.

--- TRANG 12 ---
Tài liệu tham khảo
Samuel Amouyal, Tomer Wolfson, Ohad Rubin,
Ori Yoran, Jonathan Herzig, and Jonathan Be-
rant. 2023. QAMPARI: A benchmark for open-
domain questions with many answers. In Proc.
of the Third Workshop on GEM. ACL.
Zhangir Azerbayev, Edward Ayers, and Bartosz
Piotrowski. 2023. Proof-Pile: A Pre-training
Dataset of Mathematical Text.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document trans-
former.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for
sequence prediction with recurrent neural net-
works. In Proc. of NeurIPS .
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R. Gormley. 2023. Unlimiformer:
Long-range transformers with unlimited length
input. In Proc. of NeurIPS .
Stella Biderman, Hailey Schoelkopf, Quentin An-
thony, Herbie Bradley, Kyle O'Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Puro-
hit, USVSN Sai Prashanth, Edward Raff, Aviya
Skowron, Lintang Sutawika, and Oskar van der
Wal. 2023. Pythia: A suite for analyzing large
language models across training and scaling.
Sidney Black, Stella Biderman, Eric Hallahan,
Quentin Anthony, Leo Gao, Laurence Golding,
Horace He, Connor Leahy, Kyle McDonell, Ja-
son Phang, Michael Pieler, Usvsn Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan
Tow, Ben Wang, and Samuel Weinbach. 2022.
GPT-NeoX-20B: An open-source autoregressive
language model. In Proc. of the BigScience
Workshop .
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Mil-
lican, George van den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, Diego
de Las Casas, Aurelia Guy, Jacob Menick, Ro-
man Ring, Tom Hennigan, Saffron Huang, Loren
Maggiore, Chris Jones, Albin Cassirer, Andy
Brock, Michela Paganini, Geoffrey Irving, Oriol
Vinyals, Simon Osindero, Karen Simonyan,
Jack W. Rae, Erich Elsen, and Laurent Sifre.

--- TRANG 13 ---
2022. Improving language models by retrieving
from trillions of tokens. In Proc. of ICML .
Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Proc.
of NeurIPS .
Christopher J. C. Burges, Robert Ragno, and
Quoc Viet Le. 2006. Learning to rank with non-
smooth cost functions. In Proc. of NeurIPS .
Aakanksha Chowdhery, Sharan Narang, Jacob De-
vlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, Parker
Schuh, Kensen Shi, Sasha Tsvyashchenko,
Joshua Maynez, Abhishek Rao, Parker Barnes,
Yi Tay, Noam Shazeer, Vinodkumar Prab-
hakaran, Emily Reif, Nan Du, Ben Hutchin-
son, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay
Ghemawat, Sunipa Dev, Henryk Michalewski,
Xavier Garcia, Vedant Misra, Kevin Robin-
son, Liam Fedus, Denny Zhou, Daphne Ip-
polito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omer-
nick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Diaz, Orhan Firat, Michele
Catasta, Jason Wei, Kathy Meier-Hellstern, Dou-
glas Eck, Jeff Dean, Slav Petrov, and Noah
Fiedel. 2022. Palm: Scaling language model-
ing with pathways.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime
Carbonell, Quoc Le, and Ruslan Salakhutdinov.
2019. Transformer-XL: Attentive language mod-
els beyond a fixed-length context. In Proc. of
ACL.Soham De, Samuel L. Smith, Anushan Fernando,
Aleksandar Botev, George Cristian-Muraru, Al-
bert Gu, Ruba Haroun, Leonard Berrada, Yu-
tian Chen, Srivatsan Srinivasan, Guillaume
Desjardins, Arnaud Doucet, David Budden,
Yee Whye Teh, Razvan Pascanu, Nando De Fre-
itas, and Caglar Gulcehre. 2024. Griffin: Mixing
gated linear recurrences with local attention for
efficient language models.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proc. of NAACL-HLT .
Ehsan Doostmohammadi, Tobias Norlund, Marco
Kuhlmann, and Richard Johansson. 2023.
Surface-based retrieval reduces perplexity of
retrieval-augmented language models. In Proc.
of ACL .
Matthijs Douze, Alexandr Guzhva, Chengqi
Deng, Jeff Johnson, Gergely Szilvasy, Pierre-
Emmanuel Mazaré, Maria Lomeli, Lucas Hos-
seini, and Hervé Jégou. 2024. The faiss library.
William Fedus, Barret Zoph, and Noam Shazeer.
2022. Switch transformers: Scaling to trillion
parameter models with simple and efficient spar-
sity. J. Mach. Learn. Res. , 23:1–39.
Daniel Y Fu, Tri Dao, Khaled Kamal Saab,
Armin W Thomas, Atri Rudra, and Christopher
Re. 2023. Hungry hungry hippos: Towards lan-
guage modeling with state space models. In
Proc. of ICLR .
Leo Gao, Stella Biderman, Sid Black, Laurence
Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima,
Shawn Presser, and Connor Leahy. 2020. The
pile: An 800gb dataset of diverse text for lan-
guage modeling.
Luyu Gao and Jamie Callan. 2022. Unsupervised
corpus aware language model pre-training for
dense passage retrieval. In Proc. of ACL .
Albert Gu and Tri Dao. 2023. Mamba: Linear-time
sequence modeling with selective state spaces.
Ankit Gupta, Harsh Mehta, and Jonathan Berant.
2023. Simplifying and understanding state space
models with diagonal linear rnns.

--- TRANG 14 ---
Kelvin Guu, Kenton Lee, Zora Tung, Panupong
Pasupat, and Ming-Wei Chang. 2020. Realm:
Retrieval-augmented language model pre-
training. In Proc. of ICML .
Yangsibo Huang, Daogao Liu, Zexuan Zhong, Wei-
jia Shi, and Yin Tat Lee. 2023. knn-adapter:
Efficient domain adaptation for black-box lan-
guage models.
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu,
Ethan Dyer, and Behnam Neyshabur. 2022.
Block-recurrent transformers. In Proc. of
NeurIPS .
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023.
Efficient Long-Text Understanding with Short-
Text Models. In Transactions of the Association
for Computational Linguistics , volume 11, pages
284–299.
Gautier Izacard, Mathilde Caron, Lucas Hosseini,
Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. 2022a. Unsu-
pervised dense information retrieval with con-
trastive learning. Transactions on Machine
Learning Research .
Gautier Izacard and Edouard Grave. 2021a. Dis-
tilling knowledge from reader to retriever for
question answering. In Proc. of ICLR .
Gautier Izacard and Edouard Grave. 2021b. Lever-
aging passage retrieval with generative models
for open domain question answering. In Proc.
of EACL .
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
and Edouard Grave. 2022b. Atlas: Few-shot
learning with retrieval augmented language mod-
els.J. Mach. Learn. Res. , 24:1–43.
Kalervo Järvelin and Jaana Kekäläinen. 2002.
Cumulated gain-based evaluation of ir tech-
niques. ACM Transactions on Information Sys-
tems, 20:422–446.
Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun
Araki, Haibo Ding, Jamie Callan, and Graham
Neubig. 2022. Retrieval as attention: End-to-
end learning of retrieval and reading within a
single transformer. In Proc. of EMNLP .Vladimir Karpukhin, Barlas Oguz, Sewon Min,
Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. 2020. Dense passage
retrieval for open-domain question answering.
InProc. of EMNLP .
Urvashi Khandelwal, Omer Levy, Dan Juraf-
sky, Luke Zettlemoyer, and Mike Lewis. 2020.
Generalization through memorization: Nearest
neighbor language models. In Proc. of ICLR .
Nikita Kitaev, Lukasz Kaiser, and Anselm Lev-
skaya. 2020. Reformer: The efficient trans-
former. In Proc. of ICLR .
Kenton Lee, Ming-Wei Chang, and Kristina
Toutanova. 2019. Latent retrieval for weakly
supervised open domain question answering. In
Proc. of ACL .
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
Yih, Tim Rocktäschel, Sebastian Riedel, and
Douwe Kiela. 2020. Retrieval-augmented gen-
eration for knowledge-intensive NLP tasks. In
Proc. of NeurIPS .
Pedro Henrique Martins, Zita Marinho, and An-
dre Martins. 2022. ∞-former: Infinite memory
transformer. In Proc. of ACL .
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and
Behnam Neyshabur. 2023. Long range language
modeling via gated state spaces. In Proc. of
ICLR .
Antonio Orvieto, Samuel L Smith, Albert Gu,
Anushan Fernando, Caglar Gulcehre, Razvan
Pascanu, and Soham De. 2023. Resurrecting
recurrent neural networks for long sequences. In
Proc. of ICML .
Ofir Press, Noah A. Smith, and Mike Lewis. 2021.
Shortformer: Better language modeling using
shorter inputs. In Proc. of ACL .
Ofir Press and Lior Wolf. 2017. Using the out-
put embedding to improve language models. In
Proc. of EACL .
Jack Rae and Ali Razavi. 2020. Do transformers
need deep long-range memory? In Proc. of ACL .

--- TRANG 15 ---
Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku-
mar, Chloe Hillier, and Timothy P. Lillicrap.
2020. Compressive transformers for long-range
sequence modelling. In Proc. of ICLR .
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor
Muhlgay, Amnon Shashua, Kevin Leyton-
Brown, and Yoav Shoham. 2023. In-context
retrieval-augmented language models. Trans-
actions of the Association for Computational
Linguistics , 11:1316–1331.
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Be-
rant, and Amir Globerson. 2022. Learning to
retrieve passages without supervision. In Proc.
of NAACL-HLT .
Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: Bm25 and
beyond. Foundations and Trends in Information
Retrieval , 3:333–389.
Aurko Roy, Mohammad Saffar, Ashish Vaswani,
and David Grangier. 2021. Efficient content-
based sparse attention with routing transformers.
Transactions of the Association for Computa-
tional Linguistics , 9:53–68.
Ohad Rubin, Jonathan Herzig, and Jonathan Be-
rant. 2022. Learning to retrieve prompts for
in-context learning. In Proc. of NAACL-HLT .
Devendra Sachan, Mike Lewis, Mandar Joshi, Ar-
men Aghajanyan, Wen-tau Yih, Joelle Pineau,
and Luke Zettlemoyer. 2022. Improving passage
retrieval with zero-shot question generation. In
Proc. of EMNLP .
Devendra Singh Sachan, Siva Reddy, William L.
Hamilton, Chris Dyer, and Dani Yogatama. 2021.
End-to-end training of multi-document reader
and retriever for open-domain question answer-
ing. In Proc. of NeurIPS .
Christopher Sciavolino, Zexuan Zhong, Jinhyuk
Lee, and Danqi Chen. 2021. Simple entity-
centric questions challenge dense retrievers. In
Proc. of EMNLP .
Weijia Shi, Sewon Min, Michihiro Yasunaga,
Minjoon Seo, Rich James, Mike Lewis, Luke
Zettlemoyer, and Wen tau Yih. 2024. Replug:
Retrieval-augmented black-box language mod-
els. In Proc. of NAACL-HLT .Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng
Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer:
Enhanced transformer with rotary position em-
bedding. Neurocomput. , 568.
Sainbayar Sukhbaatar, Da Ju, Spencer Poff,
Stephen Roller, Arthur Szlam, Jason Weston,
and Angela Fan. 2021. Not all memories are
created equal: Learning to forget by expiring. In
Proc. of ICML .
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-
Micke, and Mohit Iyyer. 2021. Do long-range
language models actually use long-range con-
text? In Proc. of EMNLP .
Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal,
Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume
Lample. 2023. Llama: Open and efficient foun-
dation language models.
Boxin Wang, Wei Ping, Peng Xu, Lawrence
McAfee, Zihan Liu, Mohammad Shoeybi,
Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei
Xiao, Anima Anandkumar, and Bryan Catanzaro.
2023. Shall we pretrain autoregressive language
models with retrieval? a comprehensive study.
InProc. of EMNLP .
Thomas Wolf, Loubna Ben Allal, Leandro von
Werra, Li Jia, and Armel Zebaze. 2023. A
dataset of python files from github.
Yuhuai Wu, Markus Norman Rabe, DeLesley
Hutchins, and Christian Szegedy. 2022. Memo-
rizing transformers. In Proc. of ICLR .
Dani Yogatama, Cyprien de Masson d'Autume, and
Lingpeng Kong. 2021. Adaptive semiparametric
language models. Transactions of the Associa-
tion for Computational Linguistics , 9:362–373.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, and Amr Ahmed. 2020. Big
bird: Transformers for longer sequences. In
Proc. of NeurIPS .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin,

--- TRANG 16 ---
Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt
Shuster, Daniel Simig, Punit Singh Koura, An-
jali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
2022. Opt: Open pre-trained transformer lan-
guage models.
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.
Training language models with memory augmen-
tation. In Proc. of EMNLP .
Juntang Zhuang, Tommy Tang, Yifan Ding,
Sekhar C. Tatikonda, Nicha C. Dvornek,
Xenophon Papademetris, and James S. Duncan.
2020. Adabelief optimizer: Adapting stepsizes
by the belief in observed gradients. In Proc. of
NeurIPS .
A Chi tiết Triển khai Bổ sung
Các mô hình được triển khai trong JAX với tỷ lệ
dropout là 0.05, và bộ tối ưu AdaBelief (Zhuang
et al., 2020) với weight decay là 1e-8, cosine decay
về 0.1 của learning rate tối đa, global gradient
norm clipping là 1, và tied input embedding (Press
and Wolf, 2017). Grid search xác định các giá trị
τ: 128 cho Books3, 4 cho PG19, 2 cho CodeParrot,
và 8 cho ArXiv. Chúng tôi đặt αret = 1e−9 cho
tất cả các dataset và base learning rate là 5e−3,
sử dụng tập validation để lựa chọn siêu tham số.
B Độ phức tạp Tính toán
Độ phức tạp tính toán mỗi token của một lớp attention
trong một mô hình transformer với chiều d, |Q|
truy vấn và |K| keys là 2·d·(|K|·|Q|+|K|·d+|Q|·d)
flops.⁷ Bằng cách đặt N=|Q|=|K| và thêm chi phí
của lớp feed-forward, chúng tôi có được chi phí
mỗi token cho một khối transformer khi d≫N là
2d(N+ 2d) + 8d² ≈ 12d² flops. Đối với CCA, chi
phí phụ thuộc vào kích thước đoạn C, và số lượng
hàng xóm k. Đặt |K|= 2Ck và |Q|=C, và giả sử
d≫Ck, chi phí mỗi token cho một lớp CCA là
2d(2Ck+ 2dk+d)≈(4k+ 2)·d² flops. Chi phí mỗi
token cho α∈[0,1] của các khối bao gồm CCA là
≈α(k/3+1/6). Trong các thí nghiệm của chúng tôi,
chúng tôi sử dụng CCA trong 5 trong 12 lớp nên
α=5/12 và
⁷Đối với ma trận truy vấn Q∈R|Q|×d và ma trận key/value
K∈R|K|×d, nó bao gồm các phép toán sau: nhân với WQ,
WK, và WV cho các truy vấn, keys, và values, mỗi cái tốn
|Q|·d², |K|·d², và |K|·d² flops tương ứng. Tính toán ma trận
attention và nhân nó với các values mỗi cái đòi hỏi |Q|·|K|·d
flops. Cuối cùng, nhân với ma trận đầu ra là thêm |Q|·d²
flops.k= 2, và có được rằng CCA đóng góp một overhead
khoảng 1.29x. Sử dụng logic tương tự, chi phí cố
định cho thành phần retriever là hai phép chiếu
tuyến tính, hai lớp bidirectional attention bổ sung,
và lớp query augmentation dẫn đến 1/nlayers·(7k/6+1/2),
hoặc overhead cuối cùng là 1.49x phù hợp với
overhead runtime hiệu quả đo được của chúng tôi
là 1.51x (xem Bảng 2).
C Chi tiết huấn luyện retriever kiểu DPR
Chúng tôi đã làm theo công thức huấn luyện của
DPR (Karpukhin et al., 2020) trong việc huấn
luyện một retriever BERT-base với contrastive
loss. Mục tiêu DPR đòi hỏi các positive và hard
negative để hội tụ thành công, và ở đây chúng tôi
sử dụng đoạn BM25 có điểm cao nhất top-1 như
ví dụ positive và đoạn được xếp hạng thứ 5 bởi
BM25 như ví dụ hard negative. Để đảm bảo so
sánh công bằng, chúng tôi huấn luyện retriever
contrastive của chúng tôi trên số ví dụ nhiều gấp
16 lần so với công thức DPR gốc mô tả.
