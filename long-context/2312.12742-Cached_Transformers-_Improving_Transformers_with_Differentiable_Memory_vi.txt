# 2312.12742.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2312.12742.pdf
# Kích thước tệp: 1332635 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cached Transformers: Cải thiện Transformers với Differentiable Memory
Cache
Zhaoyang Zhang1, Wenqi Shao1, Yixiao Ge3, Xiaogang Wang1, Jinwei Gu1, Ping Luo2
1The Chinese University of Hong Kong2The University of Hong Kong3Tencent Inc
Tóm tắt
Công trình này giới thiệu một mô hình Transformer mới được gọi là
Cached Transformer, sử dụng Gated Recurrent Cached
(GRC) attention để mở rộng cơ chế self-attention với
một memory cache khả vi của các token. GRC attention cho
phép chú ý đến cả token quá khứ và hiện tại, tăng
trường tiếp nhận của attention và cho phép khám phá các phụ
thuộc tầm xa. Bằng cách sử dụng một đơn vị gating hồi quy để
liên tục cập nhật cache, mô hình của chúng tôi đạt được những tiến bộ
đáng kể trong sáu nhiệm vụ ngôn ngữ và thị giác, bao gồm mô
hình hóa ngôn ngữ, dịch máy, ListOPs, phân loại hình ảnh, phát
hiện đối tượng và phân đoạn instance. Hơn nữa, phương pháp của
chúng tôi vượt qua các kỹ thuật dựa trên bộ nhớ trước đây trong
các nhiệm vụ như mô hình hóa ngôn ngữ và thể hiện khả năng
được áp dụng cho một phạm vi tình huống rộng hơn.
Giới thiệu
Thiết kế của Transformer (Vaswani et al. 2017), một mô
hình sâu xếp chồng các lớp self-attention và feed-forward, đã
đạt được tiến bộ đáng chú ý trong nhiều nhiệm vụ khác nhau.
So với các mô hình sâu truyền thống, một đặc điểm chính của
Transformer là cơ chế self-attention, cho phép trường tiếp
nhận toàn cục và cho phép mỗi token truy cập tất cả các
token khác trong một batch dữ liệu, cung cấp một sơ đồ linh
hoạt để nắm bắt biểu diễn ngữ cảnh (Vaswani et al. 2017;
Dosovitskiy et al. 2021; Carion et al. 2020). Tuy nhiên, mô
hình này có độ phức tạp bình phương với độ dài chuỗi, do đó
không phù hợp để mô hình hóa các phụ thuộc dài hạn. Trong
công trình này, chúng tôi nhằm mở rộng các mô hình transformer
thông thường sử dụng attention với một biểu diễn token dài
hạn trong một memory cache, cho phép trường tiếp nhận lớn
hơn và dài hơn với tính toán bổ sung tối thiểu.

Việc nắm bắt các mối quan hệ tầm xa giữa các token và
mẫu là rất quan trọng đối với nhiều nhiệm vụ khác nhau vì
một số lý do. (i) Trong dữ liệu tuần tự như câu ngôn ngữ, có
thể tồn tại các phụ thuộc giữa các token cách xa nhau. Ví dụ,
một sự kiện hoặc nhân vật có thể được tham chiếu thỉnh
thoảng qua nhiều đoạn văn trong một bài báo. Việc không
nắm bắt được các phụ thuộc như vậy có thể dẫn đến hiệu suất
kém trong các nhiệm vụ xử lý ngôn ngữ tự nhiên. (ii) Mô
hình hóa các mối quan hệ giữa các mẫu cũng có thể hữu ích
cho dữ liệu không tuần tự như hình ảnh. Ví dụ, việc kết hợp

Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

5 10 15 20 25
Model Capacity (G FLOPs)7678808284 Accuracy 
ViT
PVT
Cached PVT (ours)
PVTv2
Cached PVTv2 (ours)
Swin
CvTHình 1: Đường cong Hiệu suất-Độ phức tạp: So sánh độ
chính xác Top-1(%) trên ImageNet theo dung lượng mô hình
(G FLOPs) sử dụng vision transformers (Đường cong hiệu quả
tham số). Đường cong của các mô hình cached của chúng tôi luôn
ở trên các baseline tương ứng (PVT và PVTv2), cho thấy hiệu quả
của các mô hình GRC-cached xét cả độ phức tạp và độ chính xác.

một module bộ nhớ lưu trữ các biểu diễn đặc trưng nguyên
mẫu có thể cho phép học đặc trưng bất biến instance, dẫn đến
hiệu suất cải thiện trong các nhiệm vụ thị giác (Long et al.
2022; Deng et al. 2022). Hơn nữa, các nghiên cứu khác
(Wang et al. 2020b; Zhong et al. 2019) đã chứng minh rằng
việc sử dụng bộ nhớ cross-batch để lưu trữ các embedding
trước đây có thể có lợi cho việc học biểu diễn thị giác. (iii)
Attention tầm xa hơn cũng đã được chứng minh là tăng cường
khả năng học biểu diễn của các mô hình, như được chứng
minh trong các công trình như (Dai et al. 2019; Wu et al.
2022; Tay et al. 2021b).

Tuy nhiên, mô hình hóa phụ thuộc dài hơn làm cho tính
toán trở nên đắt đỏ hơn. Ví dụ, Transformer vanilla có độ
phức tạp tính toán O(T2) trong mỗi module attention khi xử
lý một chuỗi token có độ dài T. Mặc dù một số công trình áp
dụng các phương án hiệu quả, như phân rã thứ hạng thấp
(Wang et al. 2020a; Zhu et al. 2021), sparsification dựa trên
khối (Zaheer et al. 2020), và local sensitive hashing (Kitaev,
Kaiser, and Levskaya 2020), chúng vẫn có độ phức tạp tuyến
tính với độ dài token (O(T)) và do đó không thể nắm bắt hiệu
quả phụ thuộc tầm xa thưa thớt. Một hướng nghiên cứu khác
(Wu et al. 2022) giảm độ phức tạp của module attention bằng
cách chọn các cặp token top-k

arXiv:2312.12742v1 [cs.CV] 20 Dec 2023

--- TRANG 2 ---
từ một memory cache cho các token hiện tại, nhưng chi phí
duy trì một cache khổng lồ các token cho tất cả các lớp vẫn
đáng kể. Do đó, việc phát triển các cơ chế hiệu quả và hiệu
quả để nắm bắt các phụ thuộc tầm xa vẫn là một lĩnh vực
nghiên cứu tích cực.

Để giải quyết những vấn đề này, chúng tôi đề xuất một họ
mô hình Transformer mới được gọi là Cached Transformer,
có một Gated Recurrent Cache (GRC) cho phép Transform-
ers truy cập kiến thức lịch sử, như được minh họa trong
Hình 2. GRC được triển khai như một meta-learner nén biểu
diễn lịch sử thành các vector embedding và cập nhật chúng
một cách thích ứng với một cơ chế gating, tránh nhu cầu cho
một memory cache lớn. GRC cập nhật biểu diễn quá khứ với
một reset gate ngăn chặn cache lịch sử và một update gate
tiếp tục cập nhật các cache bị ngăn chặn bằng cách sử dụng
các chuỗi token hiện tại. Thiết kế này cho phép GRC truy
cập kiến thức đã thấy trước đây một cách hiệu quả về mặt
tính toán. Dựa trên GRC, chúng tôi triển khai một cơ chế
semi-cached attention chú ý đến cả token tiềm ẩn và hiện tại.

Chúng tôi đề xuất Cached Transformer với Gated Recurrent
Cache (GRC) và có những đóng góp sau, làm cho nó hấp dẫn
hơn các công trình trước đây ở một số khía cạnh.

• GRC được xây dựng trên một công thức khả vi tổng quát
và tương thích với nhiều sơ đồ attention, mạng Transform-
er và nhiệm vụ khác nhau. Chúng tôi chứng minh rằng
GRC có thể dễ dàng được cắm vào các biến thể Transform-
er đa dạng như Transformer-XL (Dai et al. 2019), ViT
(Dosovitskiy et al. 2021), PVT (Wang et al. 2021, 2022),
Swin (Liu et al. 2021) Bigbird (Zaheer et al. 2020), và
Reformer (Kitaev, Kaiser, and Levskaya 2020).

• GRC có thể cache tất cả các biểu diễn có độ dài tùy ý một
cách hồi quy, độc lập với độ dài chuỗi, trong khi các
phương pháp dựa trên cache hiện có chỉ có thể nắm bắt
các token gần đây (Rae et al. 2019; Dai et al. 2019) hoặc
yêu cầu tìm kiếm KNN tại mỗi bước (Wu et al. 2022).

• Ngoài hiệu quả, GRC vượt qua các phương pháp dựa trên
bộ nhớ trước đây (Dai et al. 2019; Burtsev et al. 2020;
Bulatov, Kuratov, and Burtsev 2022) với biên độ lớn trên
cả nhiệm vụ thị giác (Bảng 2) và ngôn ngữ (Bảng 5).

• GRC mang lại những cải thiện nhất quán không chỉ trong
dữ liệu tuần tự như văn bản mà còn trong ngữ cảnh không
gian như phân loại hình ảnh (Bảng 1) và phát hiện đối
tượng (Bảng 3). Theo hiểu biết của chúng tôi, các công
trình hiện có của Vision Transformers chủ yếu tập trung
vào việc học các token intra-sample, trong khi GRC là nỗ
lực đầu tiên để mô hình hóa các mối quan hệ cross-sample
bằng cách chú ý đến các token inter-sample, như các token
từ các hình ảnh độc lập khác nhau.

• Chúng tôi quan sát thấy rằng các mô hình với GRC có thể
chú ý nhiều hơn đến cache so với self-attention thông
thường. Chúng tôi điều tra hành vi này trong phân loại
hình ảnh và thấy rằng GRC có thể tách các đặc trưng thành
hai phần, chú ý đến cache tạo ra các đặc trưng bất biến
instance, cũng như chú ý đến self, tạo ra các đặc trưng
đặc trưng cho instance (Xem trong Hình 4). Hành vi này
tương tự như của một vector prototype (Caron et al. 2020),
cho phép cross-sample regularization để tránh overfitting.

Các thí nghiệm rộng rãi cho thấy rằng Cached Transformer

với GRC đạt được kết quả hứa hẹn trên nhiều backbone
Transformer thị giác và ngôn ngữ khác nhau. (i) Ngôn ngữ:
Trong benchmark IWSLT14 De-En cho dịch máy, PreNormed
Transformer+GRC đạt 36.0 BLEU, vượt trội hơn các baseline
0.5. Trong benchmark long-range-arena đầy thách thức (Tay
et al. 2021a), GRC cải thiện các phương pháp state-of-the-art
với các loại attention khác nhau bao gồm Reformer (Kitaev,
Kaiser, and Levskaya 2020), Bigbird (Zaheer et al. 2020), và
Transformer thông thường (Vaswani et al. 2017) một cách
nhất quán lên đến 1.2% độ chính xác. (ii) Thị giác: Đối với
phân loại hình ảnh trên ImageNet (Krizhevsky, Sutskever,
and Hinton 2012), chúng tôi cắm GRC vào các vision trans-
formers gần đây có quy mô khác nhau, như ViT (Dosovitskiy
et al. 2021), PVT (Wang et al. 2021), PVTv2 (Wang et al.
2022), Swin (Liu et al. 2021), và đạt được lên đến 3.3% cải
thiện độ chính xác. Như được hiển thị trong Hình 1, mô hình
cached của chúng tôi với backbone PVTv2 đạt hiệu suất vượt
trội xét cả độ phức tạp mô hình và độ chính xác. Chúng tôi
tiếp tục đánh giá GRC trên tập dữ liệu COCO (Lin et al.
2014) cho phát hiện đối tượng và phân đoạn instance, nơi
PVT+GRC có thể mang lại hơn 4.0 cải thiện box AP.

Các công trình liên quan
Cached Language Models. Các mô hình cache hiệu quả
trong mô hình hóa tầm xa, và được giới thiệu lần đầu bởi
(Kupiec 1989; Kuhn and De Mori 1990) cho nhận dạng giọng
nói. Nói chung, một mô hình cache lưu trữ các biểu diễn của
quá khứ, thường là unigram hoặc các cặp key-value cho tính
toán tương lai. Transformer-XL (Dai et al. 2019) tiếp tục áp
dụng kỹ thuật này cho transformers, nơi cache lưu trữ các cặp
key-value trước đây trong attention từ các bước huấn luyện
trước đó. Nhiều phương pháp dựa trên bộ nhớ được khám phá
theo sau Transformer-XL: Ví dụ, MT (Burtsev et al. 2020) và
RMT (Bulatov, Kuratov, and Burtsev 2022) sử dụng các
token bộ nhớ bổ sung để lưu trữ thông tin cục bộ và toàn cục
cho các phân đoạn đầu vào khác nhau. (Rae et al. 2019) nén
các token trước khi chúng được lưu trong cache để giảm bộ
nhớ và tính toán. Tuy nhiên, các phương pháp này thường sử
dụng cache theo cách độ dài cố định và first-in-first-out
(FIFO), điều này hạn chế số lượng token có thể được ghi nhớ
trong chuỗi. Ngược lại, Cached Transformers dựa trên GRC
đề xuất của chúng tôi học để xây dựng cache một cách thích
ứng với độ phức tạp độc lập với phạm vi attention.

Vision Transformers. Vision transformers và các biến thể
của chúng gần đây đã đạt được thành công đáng chú ý trong
nhiều nhiệm vụ thị giác khác nhau. Mô hình Vision Trans-
former (ViT) ban đầu (Dosovitskiy et al. 2021) là mô hình
đầu tiên chia hình ảnh thành các chuỗi patch và đưa chúng
vào các encoder transformer. Mặc dù tạo ra kết quả cạnh
tranh so với mạng nơ-ron tích chập (CNNs), ViTs yêu cầu
pretraining tốn kém trên các tập dữ liệu quy mô lớn như
JFT-300M (Sun et al. 2017). Để giải quyết vấn đề này, một
số công trình (Shao et al. 2022) cho rằng điều này do thiếu
inductive bias trong ViTs và đề xuất giới thiệu convolutional
priors để mã hóa inductive bias như ngữ cảnh cục bộ. Ví dụ,
DeiT (Touvron et al. 2021b) sử dụng một giáo viên tích chập
để chưng cất kiến thức cho transformers, Swin-Transformer
(Liu et al. 2021) thực hiện attention trong các cửa sổ trượt,
và ConViT (d'Ascoli

--- TRANG 3 ---
AttentionGRC Updates
Cached AttentionGRC Updates
............
Attention
Self Attention......Hình 2: So sánh của vanilla self-attention và cached atten-
tions tại giai đoạn huấn luyện. Self-attention chỉ chú ý đến token
chính nó (Xt). Trong khi ở cached attention, đầu ra tại bước huấn
luyện t (ký hiệu bởi Yt) được tạo ra bằng cách chú ý đến một Gated
Recurrent Cache (GRC, tức là Ct được tạo từ các token lịch sử X0
đến Xt), và token hiện tại (Xt).

et al. 2021) sử dụng một module tích chập "mềm" để mã hóa
locality. Tuy nhiên, các phương pháp hiện có chủ yếu tập
trung vào các token intra-sample, trong khi GRC đề xuất của
chúng tôi tăng cường vision transformers bằng cách học các
đặc trưng bất biến instance thông qua việc chú ý đến các
token inter-sample. Điều này cho phép các transformers dựa
trên GRC nắm bắt thông tin ngữ cảnh phong phú hơn và đạt
được hiệu suất tốt hơn nữa trong các nhiệm vụ thị giác.

Phương pháp học
Trong phần này, trước tiên chúng tôi xem xét lại các mô hình
transformer ngôn ngữ và thị giác vanilla, sau đó giới thiệu
triển khai của Cached Transformers với Gated Recurrent
Cache(GRC).

Vanilla Transformer
Chúng tôi bắt đầu với một đánh giá ngắn gọn về kiến trúc
transformer tiêu chuẩn. Mô hình transformer (Vaswani et al.
2017) được xây dựng bằng cách xếp chồng các khối multi-head
self-attention và các lớp feed-forward thường là một biến
đổi tuyến tính hai lớp với activation. Mỗi khối transformer
được cung cấp với T×D token đầu vào, trong đó T là số
lượng token và D biểu thị kích thước của token embedding.

Cơ chế Self-attention. Như được hiển thị trong Hình 2,
module self-attention đầu tiên chiếu mỗi đầu vào X thành
Q(query), K(key), và V(value) sử dụng các biến đổi tuyến
tính. Thông thường, self-attention được thực hiện theo cách
multi-head nơi đầu vào sẽ được chia thành nhiều head cho
tính toán song song. Đầu ra của attention head h có thể được
viết như:

oh
self= softmax( QhKT
h/√
D/H )Vh, (1)

trong đó oh
self là đầu ra của head h của self-attention và H
là số lượng head. Đầu ra từ các head sẽ được nối lại và sau
đó được đưa vào các biến đổi tuyến tính khác với chuẩn hóa
và kết nối residual.

Hạn chế. Như được hiển thị trong Eqn.(1), cơ chế vanilla
self-attention được sử dụng trong Transformers rất nhạy cảm
với độ dài chuỗi, với độ phức tạp tính toán O(T2) đối với
độ dài chuỗi T. Điều này

có nghĩa là chi phí tính toán tăng nhanh khi độ dài chuỗi
tăng, điều này hạn chế khả năng của mô hình trong việc nắm
bắt các mối quan hệ dài hạn trong dữ liệu. Kết quả là,
Transformers vanilla chỉ có thể mô hình hóa các chuỗi token
tương đối ngắn trong các nhiệm vụ ngôn ngữ, và điều này
cũng làm cho việc phát triển các module bộ nhớ cross-task
(Wang et al. 2020b; Zhong et al. 2019) theo cách dựa trên
attention cho các nhiệm vụ thị giác trở nên thách thức. Đối
với vấn đề này, chúng tôi giới thiệu Cached Transformers đề
xuất, cung cấp một mô hình linh hoạt hơn để nắm bắt các
phụ thuộc dài hạn, dẫn đến những cải thiện nhất quán cho cả
nhiệm vụ thị giác và ngôn ngữ.

Cached Transformer
Để mở rộng trường tiếp nhận của cả transformer ngôn ngữ
và thị giác, trong phần này chúng tôi sẽ giới thiệu các triển
khai của Cached Transformers, duy trì một cache liên tục
được gọi là Gated Recurrent Cache (GRC) để hỗ trợ việc
học biểu diễn dài hạn hiệu quả. Ý tưởng cốt lõi là giữ token
embedding như cache có thể ghi lại động các mẫu lịch sử
theo tính quan trọng của chúng. Cached Transformer sau đó
sẽ có thêm khả năng mã hóa cả thông tin hiện tại và tích lũy
bằng cách chú ý đến tập hợp của cache C và đầu vào X. Một
sơ đồ attention như vậy được mô tả như GRC-Attention, và
các phần sau trình bày chi tiết hơn.

Triển khai tổng quát. Cached Transformers đề xuất cho
phép chú ý đến cache trên các kiến trúc multi-layer tùy ý
chấp nhận đầu vào tuần tự. Thông thường, các mô hình
Cached Transformer có thể được tạo ra bằng cách thay thế
các khối self-attention của chúng bằng GRC-Attention đề
xuất. Hình 3 (b) đưa ra minh họa tổng thể về cách GRC-
Attention được thực hiện.

Xem xét chuỗi đầu vào Xt∈RB×T×D, trong đó B là kích
thước batch và t biểu thị các bước huấn luyện, GRC-atten-
tion chú ý đến cả memory cache và các token hiện tại.
Chúng tôi công thức hóa GRC-attention bằng

Oh=σ(λh)∗oh
mem+ (1−σ(λh))∗oh
self, (2)

trong đó Oh và oh
mem là các đầu ra của GRC-attention và
Cached attention (tức là, attention trên memory cache) trong
head h, tương ứng. oh
self là đầu ra của self-attention trong
Eqn.(1). Hơn nữa, trong Eqn.(2), σ(·) là hàm sigmoid và λh
là một tỷ lệ học được theo head để đánh đổi giữa self-atten-
tion và Cached attention1.

Để xây dựng bộ ba key, query và value cho Cached atten-
tion, chúng tôi chọn một phần của Xt làm đầu vào ¯Xt∈
RB×T×Dm, được tạo ra bằng cách cắt Xt trên chiều kênh.
Lưu ý rằng Dm=rD2 chỉ ra các kênh được sử dụng để ghi
nhớ embedding token quá khứ, trong đó r là tỷ lệ caching.
Với ¯Xt, cache tích lũy Ct−1 sau đó sẽ được cập nhật thành
Ct theo các quy tắc cập nhật GRC như được hiển thị trong
Hình 3. Chúng tôi mô tả việc xây dựng GRC trong phần chi
tiết. Cached attention sau đó có thể được thực hiện

1Tất cả λh được khởi tạo là 0.
2Ở hầu hết các trường hợp chúng tôi áp dụng Dm=D/2 để giảm độ
phức tạp của Cached attention, có nghĩa là chúng tôi chọn một nửa
đầu vào để cập nhật cache

--- TRANG 4 ---
Tokens 
Caches  FC
Updated cachesSelf-Attention
GRC
Updates
Cached Attention + Outputs
(a) GRC Updates (b) GRC-AttentionReset caches
: reset gates : update gates Hình 3: Minh họa về GRC-Attention đề xuất trong Cached Transformers. (a) Chi tiết về quá trình cập nhật của Gated Recurrent Cache.
Cache cập nhật Ct được tạo dựa trên token hiện tại ¯Xt và cache của bước cuối Ct−1. Reset gates gr reset cache trước đó Ct−1
thành reset cache ˜Ct, và update gates gu điều khiển cường độ cập nhật. (b) Pipeline tổng thể của GRC-Attention. Đầu vào sẽ chú ý đến cache
và chính chúng tương ứng, và các đầu ra được công thức hóa như interpolation của hai kết quả attention.

bằng cách sử dụng ¯Xt làm query và Ct làm key và value,
được viết như:

oh
mem = softmax( ¯Qh¯KT
h/√
Dm/H)¯Vh, (3)

trong đó ¯Qh, ¯Kh và ¯Vh được thu được bằng các phép chiếu
tuyến tính của head thứ h của ¯Xt, Ct và Ct tương ứng.

Tổng quát hóa. Lưu ý rằng trong khi chúng tôi thường công
thức hóa Cached Transformer như một mô hình dựa trên
self-attention, nó cũng có thể là một biến thể transformer
tùy ý. Nói cách khác, cơ chế attention được sử dụng để thu
được oh
self và oh
mem trong Eqn.(2) có thể được thay thế
bằng bất kỳ hàm giống attention nào khác, như sparse atten-
tions (Zaheer et al. 2020) hoặc local hashing (Kitaev, Kaiser,
and Levskaya 2020). Các thí nghiệm tiếp theo sẽ cung cấp
các xác nhận của Cached Transformers trên một số biến thể
transformer.

Cập nhật Gated Recurrent Cache
Phần này mô tả công thức và cập nhật của Gated Recurrent
Cache (GRC) đề xuất.

Khởi tạo Cache. GRC được đặc trưng là các vector có độ
dài cố định Ct∈RTm×Dm. Không giống như các công trình
trước đây công thức hóa cache trực tiếp là token hoặc từ (Tu
et al. 2018; Dai et al. 2019), GRC nhúng các token lịch sử
một cách ngầm định. Bằng cách học để nhúng các mẫu có
độ dài tùy ý vào Ct, GRC cho phép duyệt cache trong thời
gian hằng số độc lập với số lượng token được ghi nhớ. Cache
C0 sẽ được khởi tạo là các vector không có độ dài Tm trước
huấn luyện, và sau đó được cập nhật như được mô tả trong
Hình 3(a).

Cơ chế Gating. Lấy cảm hứng từ gated RNNs (Cho et al.
2014), chúng tôi áp dụng cơ chế gating để cho phép GRC
nắm bắt động các phụ thuộc ở các thang thời gian khác nhau.
Cụ thể, quá trình cập nhật của Ct được lọc bởi update gates
gu và reset gates gr. Xem xét việc cập nhật GRC tại bước
thời gian t, trước tiên chúng tôi tính toán các gate gu và gr:

gu=σ(Wu[¯Xt, Ct−1]) và gr=σ(Wr[¯Xt, Ct−1]),(4)

trong đó σ biểu thị hàm sigmoid và [·,·] nối các token theo
chiều kênh. Để nối hợp lệ, ¯Xt được nội suy thành một token
Tm-by-Dm. Cache cập nhật Ct được công thức hóa bằng một
nội suy tuyến tính như được cho bởi:

Ct= (1−gu)Ct−1+gu˜Ct và ˜Ct=Wc[¯Xt, gr⊙Ct−1]
(5)

trong đó ⊙ là phép nhân theo phần tử. Trong quá trình trên,
update gates gu quyết định mức độ mẫu hiện tại ¯Xt cập
nhật cache và reset gates gr ngăn chặn cache tích lũy để
quên các thành phần không quan trọng. Lưu ý rằng hình
dạng của Ct được tạo là B×Tm×Dm khi Xt được liên quan,
và do đó chúng tôi trung bình theo chiều batch để phù hợp
với kích thước cache.

Thí nghiệm
Phần này đánh giá rộng rãi hiệu quả của Cached Transform-
er và Gated Recurrent Cache (GRC) đề xuất trong cả nhiệm
vụ thị giác và ngôn ngữ, bao gồm mô hình hóa ngôn ngữ
trên WikiText-103, Long Listops của Long Range Arena
(Tay et al. 2021a), dịch máy trên IWSLT14 (Cettolo et al.
2014) / IWSLT15 (Cettolo et al. 2015), phân loại hình ảnh
trên ImageNet (Krizhevsky, Sutskever, and Hinton 2012),
và phát hiện đối tượng và phân đoạn instance trên COCO2017
(Lin et al. 2014). Ngoài ra, vì các mô hình cached mới được
giới thiệu vào vision transformers, chúng tôi cũng thực hiện
các cuộc thảo luận kỹ lưỡng về vai trò của cache đề xuất và
tính quan trọng của chúng. Tất cả các thí nghiệm được thực
hiện trên GPU Tesla V100.

Phân loại Hình ảnh
Thiết lập Thí nghiệm. Trước tiên chúng tôi đánh giá
phương pháp của mình trên Imagenet-1k cho phân loại hình
ảnh. Chúng tôi triển khai GRC-Attention như một module
pytorch tổng quát duy trì các buffer có độ dài cố định như
cache. Trong nhiệm vụ phân loại hình ảnh, chúng tôi đặt tỷ
lệ cache r là 0.5 và giữ độ dài cache Tm bằng độ dài của
patch hình ảnh T. Để so sánh công bằng, chúng tôi trực tiếp
thay thế các lớp self-attention trong các transformer tương
ứng bằng module GRC-Attention của chúng tôi mà không
thay đổi kiến trúc và siêu tham số. Để duy trì cấu trúc token
không gian, chúng tôi thêm positional encoding vào GRC-
Attention đề xuất giống như các vision transformers khác.
Cả baseline và các đối tác cached của chúng đều được

--- TRANG 5 ---
Inputs
Self-Attention 
Cached Attention Hình 4: Visualization của các đặc trưng đầu ra trung bình từ self-attention và cached attention, được thu được bằng cách đưa hình ảnh
của tập validation ImageNet vào cached ViT-S đã huấn luyện. Kết quả được thu được bằng cách trung bình hóa các đặc trưng theo chiều kênh (và head).
Cả ¯oself và ¯omem đều được unflattened thành 14×14 để so sánh tốt hơn. Pixel tối có nghĩa là giá trị nhỏ.

Bảng 1: Hiệu suất của các Cached Transformers khác nhau được đánh giá
trên ImageNet. "(Cached)" chỉ ra các mô hình được triển khai với
GRC-Attention đề xuất. Top-1 / Top-5 / ∆Top-1 biểu thị độ chính xác
top-1 / độ chính xác top-5 / sự khác biệt độ chính xác top-1 tương ứng.
Các mô hình cached vượt trội hơn các baseline tương ứng một cách
nhất quán.

Architecture Top-1 (%) Top-5 (%) ∆Top-1 (%)
ViT-S 79.9 95.0 -
ViT-S (Cached) 81.3 95.5 + 1.4
PVT-Tiny 75.1 92.3 -
PVT-Tiny (Cached) 78.4 94.2 + 3.3
PVT-Small 79.9 95.0 -
PVT-Small (Cached) 81.8 95.9 + 1.9
PVT-Medium 81.2 95.7 -
PVT-Medium (Cached) 83.0 96.4 + 1.8
Swin-T 81.2 95.5 -
Swin-T (Cached) 82.1 95.9 + 0.9
PVTv2-B2 82.0 95.9 -
PVTv2-B2 (Cached) 82.6 96.2 + 0.6
PVTv2-B 83.2 96.3 -
PVTv2-B3 (Cached) 83.7 96.4 + 0.5
PVTv2-B4 83.6 96.3 -
PVTv2-B4 (Cached ) 84.1 96.6 + 0.5

huấn luyện với đầu vào kích thước 224×224 sử dụng 16 GPU.
Để xác thực đầy đủ cơ chế cache đề xuất, chúng tôi đánh giá
GRC-Attention trên bốn vision transformers gần đây bao
gồm: ViTs (Dosovitskiy et al. 2021), PVT (Wang et al. 2021),
Swin-Transformer (Liu et al. 2021) và PVT-v2 (Wang et al.
2022). Không có chuông còi, tất cả các thiết lập huấn luyện
cho các mô hình cached được giữ nhất quán với các baseline
ban đầu bao gồm data augmentation, loại optimizer, learning
rate và epoch huấn luyện.

Kết quả Phân loại. Bảng 1 báo cáo hiệu suất tổng thể của
cached transformers trên các baseline tương ứng. Như được
hiển thị, các transformers được triển khai với GRC-Attention
vượt trội hơn các đối tác no-cache của chúng một cách nhất
quán bằng cách mang lại độ chính xác cao hơn đáng kể,
chứng minh hiệu quả của cơ chế caching đề xuất của chúng
tôi. Ví dụ, bằng cách kích hoạt cache, PVT-Tiny có thể đạt
78.4% độ chính xác top-1 và 94.2% độ chính xác top-5, vượt
qua PVT-Tiny ban đầu lần lượt 3.3% và 1.9%. Hơn nữa,
ngay cả đối với backbone PVTv2 mạnh hơn gần đây, cơ chế
cached đề xuất của chúng tôi vẫn có thể duy trì >0.5 cải thiện
top-1.

Phân tích Độ phức tạp. Trong các thiết lập hiện tại nơi
tỷ lệ cache r= 0.5, việc thay thế tất cả các lớp attention
bằng GRC-Attention sẽ tốn khoảng 10%−15% FLOPs và
Params bổ sung. Xem xét các cải thiện hiệu suất, tính toán
bổ sung là chấp nhận được (Xem trong Hình 1) và hiệu quả
hơn việc tăng độ sâu và chiều rộng của các mô hình.

Tính quan trọng của Cached Attention. Để xác minh rằng
các cải thiện hiệu suất trên chủ yếu đến từ việc chú ý đến
cache, chúng tôi phân tích đóng góp của omem bằng cách
visualization tỷ lệ attention học được σ(λh). Xin lưu ý rằng
trong Eq 2, đầu ra của GRC-Attention được tạo bằng cách
nội suy đầu ra của cached attention oh
mem và self-attention
oh
self theo σ(λh). Do đó, σ(λh) có thể được sử dụng để biểu
thị tính quan trọng tương đối của oh
mem và oh
self. Hình 5
mô tả σ(λh) học được cho mỗi head đối với các lớp trong
ViT-S, PVT-Tiny và PVT-Small. Như chúng ta có thể thấy,
đối với hơn một nửa số lớp, σ(λh) lớn hơn 0.5, biểu thị rằng
đầu ra của những lớp đó phụ thuộc rất nhiều vào cached
attention. Bên cạnh đó, chúng tôi cũng nhận thấy một thực
tế thú vị là các mô hình luôn ưa thích cached attention hơn
ngoại trừ một số lớp cuối cùng. Điều này khiến chúng tôi tò
mò về vai trò của cached attention: đặc trưng nào mà các mô
hình thực sự học được bằng cách chú ý đến cache? Đoạn
văn sau trả lời câu hỏi này.

Vai trò của Cached Attention. Chúng tôi điều tra chức
năng của GRC-Attention bằng cách visualization các bản
đồ đặc trưng bên trong của chúng. Chúng tôi chọn các lớp
giữa của cached ViT-S, trung bình hóa các đầu ra từ self-
attention oself và cached attention (omem) theo chiều head
và kênh, và sau đó chuẩn hóa chúng thành [0,1]. Các kết quả
tương ứng được ký hiệu là ¯oself và ¯omem, tương ứng. Hình
4 cung cấp visualization của ¯oself và ¯omem thu được bằng
cách đưa hình ảnh của tập validation ImageNet vào cached
ViT-S đã huấn luyện. Vì ¯oself và ¯omem là các chuỗi patch,
chúng được unflattened thành hình dạng 14×14 để so sánh
tốt hơn. Từ Hình 4 chúng ta có thể thấy, các đặc trưng được
tạo bởi hai attention trên có tính bổ sung trực quan. Trong
GRC-Attention, omem được tạo bằng cách chú ý đến cache
đề xuất (GRC) chứa các biểu diễn nén của các mẫu lịch sử,
và do đó

--- TRANG 6 ---
Hình 5: Visualizations của σ(λh) học được cho mỗi head đối với số lớp (từ nông đến sâu) trong các mô hình khác nhau: ViT-S,
PVT-Tiny và PVT-Small. Lưu ý rằng ViT-S có 6 head cho tất cả các lớp, trong khi PVT-Tiny và PVT-Small áp dụng chiến lược head
progressive nơi số lượng head tăng từ 1 đến 8 dần dần. Các vòng tròn với màu sắc khác nhau biểu thị những head khác nhau đó. σ(λh)
điều khiển tỷ lệ nội suy của đầu ra cached attention omem phản ánh đóng góp theo head của cached attention vào đầu ra cuối cùng. Lưu ý rằng
σ(λh)>0.5 có nghĩa là cached attention đóng góp nhiều hơn self-attention. Như được hiển thị, trong tất cả ba mô hình, σ(λh)>0.5 giữ cho
hơn một nửa số lớp GRC-Attention, suy ra rằng đầu ra mô hình phụ thuộc đáng kể vào cache.

Bảng 2: So sánh hiệu suất (Độ chính xác Top-1) của các mô hình cached
sử dụng GRC và attention-based
Model No cache Attention-based cache GRC
ViT-S 79.9 80.0 81.3
PVT-Tiny 75.1 74.8 78.4
PVT-Small 79.9 79.6 81.8

thành thạo trong việc nhận dạng các patch công khai và
thường xuyên xuất hiện của lớp này. Trong khi đối với oself
từ nhánh self-attention, nó có thể tập trung vào việc tìm ra
các đặc trưng riêng tư và đặc trưng hơn của instance hiện tại.

Với các giả định trên, chúng tôi có thể cố gắng giải thích
tính quy luật của σ(λh) trong Hình 5: sử dụng nhiều omem
hơn (σ(λh) lớn hơn) trong các lớp trước có thể giúp mạng
phân biệt instance này một cách thô, và sử dụng nhiều oself
hơn (σ(λh) nhỏ hơn) cho phép mô hình đưa ra quyết định
tinh vi.

Cross-sample regularization. Đoạn trên cũng cho thấy
rằng cache đề xuất của chúng tôi hoạt động tương tự như
vector prototypes (Caron et al. 2020), lưu trữ các đặc trưng
công khai của cùng một lớp một cách ngầm định và cho
phép các mô hình phân loại đầu vào với cả biểu diễn công
khai và đặc trưng. Theo cách này, các dự đoán không chỉ
phụ thuộc vào đầu vào hiện tại mà còn vào các mẫu cached
liên quan, do đó cung cấp một cross-sample regularization
để tránh overfitting.

GRC v.s. các phương pháp dựa trên bộ nhớ khác. Chúng
tôi thực hiện các ablation tiếp theo để so sánh GRC và bộ
nhớ dựa trên attention cho phân loại hình ảnh trong ImageNet-
1k. Chúng tôi triển khai cache kiểu Transformer-XL cho
Vision Transformers (bao gồm ViT-S, PVT-Tiny và PVT-
Small) và so sánh chúng với các mô hình GRC-cached tương
ứng. Như được hiển thị trong Bảng 2, các mô hình GRC-
cached vượt trội hơn các đối tác attention-based cache và
no-cache của chúng một cách nhất quán. Bên cạnh đó, có
thể lưu ý rằng attention-based cache khó có thể cải thiện
hiệu suất mô hình.

Phát hiện Đối tượng và Phân đoạn Instance.
Thiết lập Thí nghiệm. Chúng tôi tiếp tục đánh giá tính tổng
quát của GRC-Attention trên track phát hiện đối tượng /
phân đoạn instance sử dụng tập dữ liệu COCO2017 (Lin
et al. 2014). Các mô hình được huấn luyện trên COCO
train2017 (118k hình ảnh) và đánh giá trên val2017 (5k
hình ảnh). Chúng tôi sử dụng cached

Bảng 3: Hiệu suất phát hiện đối tượng và phân đoạn instance
trên COCO val2017 theo thiết lập Mask R-CNN 1×.
Architecture APbAPb
50 APb
75 APmAPm
50 APm
75
PVT-Tiny 36.7 59.2 39.3 35.1 56.7 37.3
+ Cached 41.0 (+ 4.6) 63.4 44.8 38.3 (+ 3.2) 60.4 41.1
PVT-Small 40.4 62.9 43.8 36.3 60.1 40.3
+ Cached 44.5 (+ 4.1) 67.1 48.6 41.0 (+ 4.7) 64.0 44.1
PVT-Medium 42.0 64.4 45.6 39.0 61.6 42.1
+ Cached 46.6 (+ 4.6) 68.2 51.0 42.3 (+ 3.3) 65.3 45.5

PVT làm backbone và áp dụng detector Mask R-CNN (He
et al. 2017) để xác minh hiệu quả của GRC-Attention. Các
chỉ số COCO tiêu chuẩn của Average Precision (AP) cho
phát hiện bounding box (APbb) và phân đoạn instance (APm)
được sử dụng để đánh giá phương pháp của chúng tôi. Tất
cả các thiết lập huấn luyện và siêu tham số được giữ giống
như triển khai PVT ban đầu (Wang et al. 2021), và tất cả
các mô hình liên quan được huấn luyện trong 12 epoch sử
dụng 8 GPU. Đối với cả cached PVT và baseline, backbone
được pretrain trước trên ImageNet và sau đó fine-tune cho
detection.

Kết quả. Như được hiển thị trong Bảng 3, khi sử dụng
Mask R-CNN cho phát hiện đối tượng, các cached PVT vượt
trội hơn baseline của chúng một cách đáng kể. Ví dụ, AP
của cached PVT-Medium tốt hơn 4.6 (46.6 vs. 42.0) điểm
so với các đối tác no-cache của nó. Kết quả tương tự cũng
có thể được tìm thấy trong kết quả phân đoạn instance, nơi
cached PVT-Medium đạt 3.3 APm cao hơn (39.0 vs. 42.3).
Những kết quả này chứng minh tính tổng quát của cơ chế
caching đề xuất.

Mô hình hóa Ngôn ngữ
Thiết lập Thí nghiệm Trong công trình này, chúng tôi thực
hiện các thí nghiệm để so sánh hiệu suất của Gated Recur-
rent Cache (GRC) với Transformer-XL (Dai et al. 2019)
trên một nhiệm vụ mô hình hóa ngôn ngữ sử dụng benchmark
WikiText-103. Để triển khai các mô hình ngôn ngữ GRC-
cached, chúng tôi sử dụng framework fairseq có sẵn công
khai và tuân theo các cấu hình Transformer-XL dựa trên bộ
nhớ mặc định làm baseline của chúng tôi, bao gồm kiến trúc
mô hình và thiết lập huấn luyện. Để đảm bảo so sánh công
bằng, chúng tôi so sánh các mô hình GRC-cached với hai
phương pháp dựa trên bộ nhớ khác, Memory Transformer
(MT) (Burtsev et al. 2020) và Recurrent Memory Transform-
er (RMT) (Bulatov, Kuratov, and Burtsev 2022). Chúng tôi
triển khai các mô hình GRC-cached bằng cách thay thế

--- TRANG 7 ---
Bảng 4: Kết quả dịch máy neural sử dụng Pre-Norm Transformers tính theo điểm BLEU.
ArchitectureIWSLT14 IWSLT15
De-En Es-En En-Fr De-En En-Vi Cs-En
Transformer 35.5 41.4 41.5 36.1 29.8 28.8
Transformer (GRC-cached) 36.0(+ 0.5) 41.8(+ 0.4) 41.7(+ 0.2) 36.3(+ 0.2) 30.2(+ 0.4) 29.4(+ 0.6)

Bảng 5: So sánh hiệu suất (Test PPL) cho GRC và các
phương pháp dựa trên Bộ nhớ khác (Burtsev et al. 2020; Bulatov, Kuratov, and
Burtsev 2022) trên WikiText-103. Số nhỏ hơn là tốt hơn. GRC vượt
trội hơn Transformer-XL và các phương pháp dựa trên bộ nhớ trước đây cho
mô hình hóa ngôn ngữ với biên độ lớn 1.1 PPL.
Architecture baseline MT-cached RMT-cached GRC-cached
Transformer-XL base 24.0 23.99 23.95 22.9
Transformer-XL large 18.3 - - 17.9

sơ đồ caching với phương pháp GRC trong khi giữ tất cả
data augmentation và hyper-parameter không thay đổi để
so sánh công bằng hơn.

So sánh với Các Phương pháp Dựa trên Bộ nhớ Khác
Chúng tôi trình bày hiệu suất của các mô hình GRC-cached
so với baseline Transformer-XL và các phương pháp dựa
trên bộ nhớ khác trong Bảng 5. Kết quả cho thấy rằng các
mô hình GRC-cached vượt trội hơn Transformer-XL và các
phương pháp dựa trên bộ nhớ khác về perplexity trên cả mô
hình quy mô base và large. Ví dụ, GRC-cached Transform-
er-XL base đạt lên đến 1.1 PPL thấp hơn so với baseline
Transformer-XL và 1.05 PPL thấp hơn so với RMT, chứng
minh sự vượt trội của GRC so với các phương pháp Trans-
former dựa trên bộ nhớ trước đây.

Long Range Arena
Thiết lập Thí nghiệm. Chúng tôi thực hiện rộng rãi các thí
nghiệm trên benchmark Long Range Arena (LRA) được đề
xuất gần đây (Tay et al. 2021a) để xác thực các phương pháp
đề xuất của chúng tôi trong kịch bản long-context. Để chứng
minh khả năng mô hình hóa chuỗi tầm xa của GRC-Atten-
tion và cơ chế cache tương ứng, chúng tôi chọn nhiệm vụ
Long ListOps đầy thách thức trong LRA, đây là một biến
thể dài hơn của nhiệm vụ ListOps (Nangia and Bowman
2018) với các chuỗi dài lên đến 2k và khó khăn đáng kể.
Trong nhiệm vụ này, chúng tôi cũng mở rộng GRC-Atten-
tion cho các biến thể attention hiệu quả bằng cách thay thế
hàm self-attention (Xem phần). Cụ thể, chúng tôi so sánh
GRC-Attention với các đối tác no-cache của chúng trên
baseline bao gồm Transformer (Vaswani et al. 2017), Big-
Bird (Zaheer et al. 2020) và Reformer (Kitaev, Kaiser, and
Levskaya 2020). Đối với những attention hiệu quả như
BigBird và Reformer, chúng tôi chỉ nhập gated recurrent
cache và duy trì hàm attention bên trong của chúng không
thay đổi. Tất cả các thí nghiệm được thực hiện dưới thiết
lập mặc định trong (Tay et al. 2021a).

Kết quả. Bảng 6 báo cáo kết quả Long ListOps. Như được
hiển thị, các mô hình cached vượt trội hơn baseline của
chúng (bao gồm các phương pháp SOTA Reformer) một cách
đáng kể. Ví dụ, bằng cách sử dụng GRC, mô hình BigBird
có thể đạt 1.39 độ chính xác cao hơn. Những kết quả này
cho thấy khả năng mô hình hóa chuỗi tầm xa của GRC cũng
như tính tổng quát của nó đối với các biến thể attention khác.

Bảng 6: Kết quả trên nhiệm vụ Long ListOPs trong LRA tính theo độ
chính xác. Cột "cached" chỉ ra các mô hình cached có các lớp
attention được triển khai như GRC-Attention tổng quát. ∆
biểu thị sự khác biệt giữa các mô hình cached đề xuất và base-
line.
Architecture baseline GRC-cached ∆
Transformer 36.23 37.40 + 1.17
BigBird 36.06 37.45 + 1.39
Reformer 37.27 37.85 + 0.58

Dịch máy Neural
Thiết lập Thí nghiệm. Chúng tôi thử nghiệm phương pháp
của mình trên các tập dữ liệu công khai được sử dụng rộng
rãi IWSLT14 và IWSLT15. Nhiều nguồn ngôn ngữ3 được
bao gồm để xác minh đầy đủ hiệu quả của GRC đề xuất, và
các mô hình được huấn luyện cho từng track riêng lẻ. Chúng
tôi áp dụng thiết lập Pre-Norm Transformer trong (Wang
et al. 2019) và triển khai các mô hình sử dụng framework
fairseq-py (Ott et al. 2019). Theo (Wang et al. 2019; Ott
et al. 2019), chúng tôi thường tăng learning rate lên 2 và
trung bình 10 checkpoint cuối cho suy luận. Chúng tôi sử
dụng các mô hình GRC-cached đề xuất bằng cách thay thế
tất cả các module attention của các lớp encoder transformer
bằng GRC-Attention. Độ dài cache Tm được đặt là 64 cho
tất cả các mô hình cached. Tất cả các transformer trong
nhiệm vụ này sử dụng sáu lớp encoder và sáu lớp decoder.
Để so sánh công bằng, cả baseline và các mô hình cached
đều được huấn luyện dưới thiết lập giống hệt nhau.

Kết quả. Chúng tôi sử dụng BLEU (Papineni et al. 2002)
làm chỉ số đánh giá và so sánh GRC cached transformers
với baseline của chúng trong Bảng 4. Có thể thấy rằng
những cải thiện nhất quán có thể đạt được bằng cách áp
dụng GRC-Attention vào baseline. Đối với các track như
IWSLT14 De-En và IWSLT15 Cs-En, các gia tăng có thể
đạt 0.5/0.6 điểm, điều này thực sự đáng kể cho những nhiệm
vụ này.

Thảo luận
Chúng tôi giới thiệu Cached Transformer với Gated Recur-
rent Cache (GRC), một mở rộng đơn giản cho các mô hình
dựa trên Transformer tăng đáng kể độ dài của ngữ cảnh
attention bằng cách cho phép truy cập vào các trạng thái
lịch sử thông qua một cơ chế gating. GRC nhúng các token
trước đây, dù chúng gần hay xa, dưới dạng các vector có độ
dài cố định, mà không phụ thuộc độ phức tạp vào số lượng
token được cached. Do đó, GRC mô hình các phụ thuộc
token trên một phạm vi đầu vào rộng hơn, dẫn đến cải thiện
độ chính xác và hiệu suất trên các biến thể Transformers
đa dạng với các kiến trúc và hàm attention khác nhau, trên
nhiều nhiệm vụ thị giác và ngôn ngữ khác nhau.

3IWSLT14: German-English(De-En), Spanish-English(Es-En)
và English-French(En-Fr), IWSLT15: German-English(De-En),
English-Vietnamese(En-Vi) và Czech-English(Cs-En)

--- TRANG 8 ---
Tài liệu tham khảo
Ainslie, J.; Ontanon, S.; Alberti, C.; Cvicek, V .; Fisher, Z.; Pham,
P.; Ravula, A.; Sanghai, S.; Wang, Q.; and Yang, L. 2020. ETC: En-
coding long and structured inputs in transformers. arXiv preprint
arXiv:2004.08483 .
Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The
long-document transformer. arXiv preprint arXiv:2004.05150 .
Brahma, S. 2018. Improved language modeling by decoding the
past. arXiv preprint arXiv:1808.05908 .
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhari-
wal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al.
2020. Language models are few-shot learners. Advances in neural
information processing systems , 33: 1877–1901.
Bulatov, A.; Kuratov, Y .; and Burtsev, M. 2022. Recurrent memory
transformer. Advances in Neural Information Processing Systems ,
35: 11079–11091.
Burtsev, M. S.; Kuratov, Y .; Peganov, A.; and Sapunov, G. V . 2020.
Memory transformer. arXiv preprint arXiv:2006.11527 .
Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.;
and Zagoruyko, S. 2020. End-to-end object detection with trans-
formers. In European conference on computer vision , 213–229.
Springer.
Caron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.; and
Joulin, A. 2020. Unsupervised learning of visual features by con-
trasting cluster assignments. Advances in Neural Information Pro-
cessing Systems , 33: 9912–9924.
Cettolo, M.; Niehues, J.; St ¨uker, S.; Bentivogli, L.; Cattoni, R.; and
Federico, M. 2015. The IWSLT 2015 Evaluation Campaign. In
Proceedings of the 12th International Workshop on Spoken Lan-
guage Translation: Evaluation Campaign , 2–14. Da Nang, Viet-
nam.
Cettolo, M.; Niehues, J.; St ¨uker, S.; Bentivogli, L.; and Federico,
M. 2014. Report on the 11th IWSLT evaluation campaign. In Pro-
ceedings of the 11th International Workshop on Spoken Language
Translation: Evaluation Campaign , 2–17. Lake Tahoe, California.
Cho, K.; Van Merri ¨enboer, B.; Bahdanau, D.; and Bengio, Y . 2014.
On the properties of neural machine translation: Encoder-decoder
approaches. arXiv preprint arXiv:1409.1259 .
Choromanski, K.; Likhosherstov, V .; Dohan, D.; Song, X.; Gane,
A.; Sarlos, T.; Hawkins, P.; Davis, J.; Mohiuddin, A.; Kaiser, L.;
et al. 2020. Rethinking attention with performers. arXiv preprint
arXiv:2009.14794 .
Dai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q. V .; and Salakhut-
dinov, R. 2019. Transformer-xl: Attentive language models beyond
a fixed-length context. arXiv preprint arXiv:1901.02860 .
Deng, W.; Marsh, J.; Gould, S.; and Zheng, L. 2022. Fine-Grained
Classification via Categorical Memory Networks. IEEE Transac-
tions on Image Processing , 31: 4186–4196.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert:
Pre-training of deep bidirectional transformers for language under-
standing. arXiv preprint arXiv:1810.04805 .
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai,
X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.;
Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth
16x16 Words: Transformers for Image Recognition at Scale. In
International Conference on Learning Representations .
d'Ascoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.; Biroli, G.;
and Sagun, L. 2021. Convit: Improving vision transformers with
soft convolutional inductive biases. In International Conference on
Machine Learning , 2286–2296. PMLR.

Grave, E.; Joulin, A.; and Usunier, N. 2016. Improving neu-
ral language models with a continuous cache. arXiv preprint
arXiv:1612.04426 .
He, K.; Gkioxari, G.; Doll ´ar, P.; and Girshick, R. 2017. Mask r-cnn.
InProceedings of the IEEE international conference on computer
vision , 2961–2969.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE conference
on computer vision and pattern recognition , 770–778.
Khandelwal, U.; Levy, O.; Jurafsky, D.; Zettlemoyer, L.; and
Lewis, M. 2019. Generalization through memorization: Nearest
neighbor language models. arXiv preprint arXiv:1911.00172 .
Kitaev, N.; Kaiser, Ł.; and Levskaya, A. 2020. Reformer: The effi-
cient transformer. arXiv preprint arXiv:2001.04451 .
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet
classification with deep convolutional neural networks. Advances
in neural information processing systems , 25.
Kuhn, R.; and De Mori, R. 1990. A cache-based natural language
model for speech recognition. IEEE transactions on pattern anal-
ysis and machine intelligence , 12(6): 570–583.
Kupiec, J. 1989. Probabilistic models of short and long distance
word dependencies in running text. In Speech and Natural Lan-
guage: Proceedings of a Workshop Held at Philadelphia, Pennsyl-
vania, February 21-23, 1989 .
Lin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ´ar, P. 2017.
Focal loss for dense object detection. In Proceedings of the IEEE
international conference on computer vision , 2980–2988.
Lin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan,
D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft coco: Common
objects in context. In European conference on computer vision ,
740–755. Springer.
Liu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.; and
Guo, B. 2021. Swin transformer: Hierarchical vision transformer
using shifted windows. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , 10012–10022.
Long, A.; Yin, W.; Ajanthan, T.; Nguyen, V .; Purkait, P.; Garg, R.;
Blair, A.; Shen, C.; and van den Hengel, A. 2022. Retrieval aug-
mented classification for long-tail visual recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 6959–6969.
Loshchilov, I.; and Hutter, F. 2018. Decoupled Weight Decay Reg-
ularization. In International Conference on Learning Representa-
tions .
Nangia, N.; and Bowman, S. R. 2018. Listops: A diagnostic dataset
for latent tree learning. arXiv preprint arXiv:1804.06028 .
Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grang-
ier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible Toolkit for
Sequence Modeling. In NAACL-HLT (Demonstrations) .
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a
Method for Automatic Evaluation of Machine Translation. In ACL.
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018.
Improving language understanding by generative pre-training.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.;
et al. 2019. Language models are unsupervised multitask learners.
OpenAI blog , 1(8): 9.
Rae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap, T. P.
2019. Compressive transformers for long-range sequence mod-
elling. arXiv preprint arXiv:1911.05507 .
Ramachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Levskaya,
A.; and Shlens, J. 2019. Stand-alone self-attention in vision mod-
els.Advances in Neural Information Processing Systems , 32.

--- TRANG 9 ---
Roy, A.; Saffar, M.; Vaswani, A.; and Grangier, D. 2021. Efficient
content-based sparse attention with routing transformers. Transac-
tions of the Association for Computational Linguistics , 9: 53–68.
Shao, W.; Ge, Y .; Zhang, Z.; XU, X.; Wang, X.; Shan, Y .; and Luo,
P. 2022. Dynamic Token Normalization improves Vision Trans-
formers. In International Conference on Learning Representations .
Sukhbaatar, S.; Ju, D.; Poff, S.; Roller, S.; Szlam, A.; Weston, J.;
and Fan, A. 2021. Not all memories are created equal: Learning to
forget by expiring. In International Conference on Machine Learn-
ing, 9902–9912. PMLR.
Sun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Revisiting
unreasonable effectiveness of data in deep learning era. In Pro-
ceedings of the IEEE international conference on computer vision ,
843–852.
Tay, Y .; Dehghani, M.; Abnar, S.; Shen, Y .; Bahri, D.; Pham, P.;
Rao, J.; Yang, L.; Ruder, S.; and Metzler, D. 2021a. Long Range
Arena : A Benchmark for Efficient Transformers. In International
Conference on Learning Representations .
Tay, Y .; Dehghani, M.; Aribandi, V .; Gupta, J.; Pham, P. M.; Qin,
Z.; Bahri, D.; Juan, D.-C.; and Metzler, D. 2021b. Omninet: Om-
nidirectional representations from transformers. In International
Conference on Machine Learning , 10193–10202. PMLR.
Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.;
and J ´egou, H. 2021a. Training data-efficient image transformers
& distillation through attention. In International Conference on
Machine Learning , 10347–10357. PMLR.
Touvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and J ´egou,
H. 2021b. Going deeper with image transformers. In Proceedings
of the IEEE/CVF International Conference on Computer Vision ,
32–42.
Tu, Z.; Liu, Y .; Shi, S.; and Zhang, T. 2018. Learning to remember
translation history with a continuous cache. Transactions of the
Association for Computational Linguistics , 6: 407–420.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is
all you need. Advances in neural information processing systems ,
30.
Wang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; and Chao,
L. S. 2019. Learning deep transformer models for machine trans-
lation. arXiv preprint arXiv:1906.01787 .
Wang, S.; Li, B. Z.; Khabsa, M.; Fang, H.; and Ma, H. 2020a.
Linformer: Self-attention with linear complexity. arXiv preprint
arXiv:2006.04768 .
Wang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.; Lu, T.;
Luo, P.; and Shao, L. 2021. Pyramid vision transformer: A versa-
tile backbone for dense prediction without convolutions. In Pro-
ceedings of the IEEE/CVF International Conference on Computer
Vision , 568–578.
Wang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.; Lu,
T.; Luo, P.; and Shao, L. 2022. PVT v2: Improved baselines with
Pyramid Vision Transformer. Computational Visual Media , 1–10.
Wang, X.; Zhang, H.; Huang, W.; and Scott, M. R. 2020b. Cross-
batch memory for embedding learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 6388–6397.
Wu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.; and
Zhang, L. 2021. Cvt: Introducing convolutions to vision transform-
ers. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , 22–31.
Wu, Y .; Rabe, M. N.; Hutchins, D.; and Szegedy, C. 2022. Memo-
rizing transformers. arXiv preprint arXiv:2203.08913 .

Yuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.-H.; Tay,
F. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Training
vision transformers from scratch on imagenet. In Proceedings of
the IEEE/CVF International Conference on Computer Vision , 558–
567.
Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Alberti, C.;
Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang, L.; et al. 2020.
Big bird: Transformers for longer sequences. Advances in Neural
Information Processing Systems , 33: 17283–17297.
Zhong, Z.; Zheng, L.; Luo, Z.; Li, S.; and Yang, Y . 2019. In-
variance matters: Exemplar memory for domain adaptive person
re-identification. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 598–607.
Zhu, C.; Ping, W.; Xiao, C.; Shoeybi, M.; Goldstein, T.; Anandku-
mar, A.; and Catanzaro, B. 2021. Long-short transformer: Efficient
transformers for language and vision. Advances in Neural Infor-
mation Processing Systems , 34.

--- TRANG 10 ---
Đánh giá Tài liệu Đầy đủ
Language Transformers. Transformer được giới thiệu lần
đầu trong xử lý ngôn ngữ bởi ((Vaswani et al. 2017)), và
rất nhiều công trình đã được thực hiện để cải thiện nó. Các
công trình có ảnh hưởng nhất của language transformers là
GPT (Radford et al. 2018, 2019; Brown et al. 2020) và
BERT (Devlin et al. 2018). Họ GPT/BERT hoạt động theo
cách 'pretraining-finetuning', đạt hiệu suất state-of-art trên
nhiều benchmark ngôn ngữ khác nhau. Nhưng chúng cũng
đắt đỏ về cả phần cứng và năng lượng. Một loạt phương
pháp khác cải thiện vanilla transformers và cũng tìm kiếm
sự đánh đổi giữa hiệu suất và hiệu quả. Để bao phủ thông
tin tầm xa trong ngôn ngữ, một số kỹ thuật attention hiệu
quả được đề xuất như kernel approximation (Wang et al.
2020a; Choromanski et al. 2020), sparsification (Zaheer
et al. 2020; Roy et al. 2021) và local hashing (Kitaev, Kaiser,
and Levskaya 2020)). Thay vì những attention nhẹ này, các
công trình khác cố gắng áp dụng attention một cách chọn
lọc với các quy tắc được định nghĩa trước như sliding win-
dows (Beltagy, Peters, and Cohan 2020), kiến trúc phân
cấp (Ainslie et al. 2020) và token pruning (Sukhbaatar et al.
2021).

Cache Language Models và Các Phương pháp dựa trên
Bộ nhớ. Các mô hình cache hiệu quả trong mô hình hóa
tầm xa, và được giới thiệu lần đầu bởi (Kupiec 1989; Kuhn
and De Mori 1990) cho nhận dạng giọng nói. Nói chung,
một mô hình cache lưu trữ các biểu diễn của quá khứ, thường
là unigram hoặc các cặp key-value cho tính toán tương lai.
(Grave, Joulin, and Usunier 2016) mở rộng những loại
phương pháp này cho các mô hình ngôn ngữ neural (RNNs),
nơi cache lưu trữ các cặp đầu vào và trạng thái ẩn gần nhất,
và (Brahma 2018) sau đó cải thiện các mô hình neural cache
bằng cách giải mã các token quá khứ như một regularization.
Transformer-XL (Dai et al. 2019) tiếp tục áp dụng kỹ thuật
này cho transformers, nơi cache lưu trữ các cặp key-value
trước đây trong attention từ các bước huấn luyện trước đó.

Nhiều phương pháp dựa trên bộ nhớ được khám phá theo
sau Transformer-XL: Ví dụ, MT (Burtsev et al. 2020) và
RMT (Bulatov, Kuratov, and Burtsev 2022) sử dụng các
token bộ nhớ bổ sung để lưu trữ thông tin cục bộ và toàn
cục cho các phân đoạn đầu vào khác nhau. (Rae et al. 2019)
nén các token trước khi chúng được lưu trong cache để giảm
bộ nhớ và tính toán. Ngoài các biểu diễn tổng quát, một số
công trình cũng lưu trữ thông tin cụ thể theo nhiệm vụ trong
cache để cải thiện hiệu suất. Ví dụ, (Tu et al. 2018) đề xuất
tăng cường các mô hình dịch máy neural bằng cách ghi nhớ
lịch sử dịch. Tuy nhiên, các phương pháp này thường sử
dụng cache theo cách độ dài cố định và first-in-first-out
(FIFO), điều này hạn chế số lượng token có thể được ghi
nhớ trong chuỗi.

Để giải quyết vấn đề này, công trình dựa trên bộ nhớ gần
đây (Khandelwal et al. 2019; Wu et al. 2022) đề xuất lưu
trữ các cặp key-value trong một cache lớn mà không nén và
thực hiện tìm kiếm K-nearest neighbor (KNN) để tìm kiếm
trên chúng. Trong khi phương pháp này mang lại kết quả
cạnh tranh trong mô hình hóa ngôn ngữ, nó vẫn yêu cầu dung
lượng bộ nhớ lớn và thời gian đáng kể để tìm kiếm, đặc biệt
cho các phạm vi attention dài hơn. Ngược lại, Cached Trans-
formers dựa trên GRC đề xuất của chúng tôi học để xây
dựng cache một cách thích ứng với độ phức tạp độc lập với
phạm vi attention.

Vision Transformers. Vision transformers (và các biến thể)
gần đây đã đạt được thành công to lớn trong nhiều nhiệm vụ
thị giác khác nhau. ViTs (Dosovitskiy et al. 2021) đầu tiên
đề xuất chia hình ảnh thành các chuỗi patch và đưa chúng
vào các encoder transformer. Mặc dù tạo ra kết quả cạnh
tranh với CNN, ViTs có vấn đề yêu cầu pretraining tốn kém
trên các tập dữ liệu quy mô lớn như JFT-300M (Sun et al.
2017). Nhiều công trình (Shao et al. 2022) cho rằng điều
này do thiếu inductive bias và đề xuất giới thiệu convolutional
priors vào ViTs để mã hóa inductive bias như ngữ cảnh cục
bộ. Ví dụ, DeiT (Touvron et al. 2021b) sử dụng các giáo
viên tích chập để chưng cất kiến thức cho transformers,
Swin-Transformer (Liu et al. 2021) thực hiện attention trong
sliding windows, và ConViT (d'Ascoli et al. 2021) sử dụng
một module tích chập "mềm" để mã hóa locality. Hơn nữa,
các phương pháp khác như PVT (Wang et al. 2022), T2T
(Yuan et al. 2021), và CVT (Wu et al. 2021) tiếp tục cải
thiện vision transformers bằng cách nhập convolutional priors
trong CNNs (He et al. 2016). Khác với các phương pháp
hiện có tập trung chủ yếu vào các token intra-sample, GRC
đề xuất tiếp tục tăng cường vision transformers bằng cách
mô hình hóa các phụ thuộc của các token inter-sample.

Chi tiết Triển khai
Thuật toán Huấn luyện và Suy luận
Thuật toán 1 cung cấp các quy trình chi tiết của GRC-
Attention đề xuất trong một lượt forward. Trong quá trình
huấn luyện, mỗi module GRC-Attention duy trì một cache
liên tục Ct, sẽ được cập nhật tại mỗi lần lặp. Lưu ý rằng tất
cả các tính toán liên quan đến GRC-Attention đều khả vi và
các tham số tương ứng do đó có thể được tối ưu hóa sử dụng
các phương pháp dựa trên gradient. Các cache tích lũy Ct
được lưu trữ với các tham số mạng sau huấn luyện, và sẽ
được sử dụng trực tiếp cho suy luận mà không cần cập nhật
thêm.

Chi tiết triển khai khác.
GRC hỗ trợ huấn luyện end-to-end với các tham số mạng
khác sử dụng các phương pháp dựa trên gradient. Cache Ct
sẽ được lưu dưới dạng buffer cùng với các mô hình đã huấn
luyện và được sử dụng cho đánh giá. Tương tự như thống kê
huấn luyện trong batch normalization, Ct sẽ bị đóng băng
tại thời gian suy luận và do đó không yêu cầu cập nhật.
Chúng tôi cũng nhận thấy rằng một số nhiệm vụ chấp nhận
đầu vào có độ dài thay đổi (T thay đổi, như phát hiện đối
tượng hoặc dịch máy). Trong những trường hợp như vậy,
chúng tôi sẽ nội suy các chuỗi đầu vào hợp lệ (không có
zero-padding) thành độ dài cache cố định Tm và sau đó tiếp
tục cập nhật cache.

Chi tiết thí nghiệm
Phân loại hình ảnh trên ImageNet. Chúng tôi đánh giá hiệu
suất của GRC-Attention sử dụng nhiều vision transformers
khác nhau ((bao gồm ViTs, PVT, Swin, và PVTv2)) trên
ImageNet-1k (Krizhevsky, Sutskever, and Hinton 2012),
bao gồm 1.28M hình ảnh huấn luyện và 50K hình ảnh
validation từ 1K lớp. Đối với mỗi baseline, chúng tôi triển
khai các biến thể cached của chúng bằng cách thay thế trực
tiếp tất cả các lớp self-attention của chúng bằng GRC-
Attention, giữ kiến trúc của chúng không thay đổi. Theo mặc
định, tỷ lệ cache được đặt là 0.5 và độ dài cache bằng số
patch Tm=T. Như được đề xuất bởi (Ramachandran et al.
2019), positional encoding được

--- TRANG 11 ---
Bảng 7: Hiệu suất phát hiện đối tượng trên COCO val2017 theo thiết lập RetinaNet 1×.
Architecture AP AP 50 AP75 APSAPM APL
PVT-Tiny 36.7 56.9 38.9 22.6 38.8 50.0
PVT-Tiny (Cached) 40.2 (+ 3.5) 61.1 43.1 25.0 43.7 53.4
PVT-Small 40.4 61.3 43.0 25.0 42.9 55.7
PVT-Small (Cached) 44.0 (+ 3.6) 65.4 47.4 29.7 47.7 57.5
PVT-Medium 41.9 63.1 44.3 25.0 44.9 57.6
PVT-Medium (Cached) 45.7 (+ 3.8) 67.1 49.1 29.0 49.3 60.2

Thuật toán 1: Forward pass của GRC-Attention tại giai
đoạn huấn luyện.
Require: bước huấn luyện t(t > 0), đầu vào mini batch
X∈RB×T×D, tham số học được λh cho head h∈
{0,1, ..., H−1}, cache tích lũy Ct−1∈RTm×Dm, trong đó
Dm=rD và r là tỷ lệ caching.
Ensure: khởi tạo C0 thành zero vectors, λh= 0 cho tất cả head,
tỷ lệ caching r= 0.5, và đặt Tm=T (cho phân loại hình ảnh
/ Long ListOps / Phát hiện Đối tượng) hoặc Tm= 64 (cho
Dịch máy).
Output: đầu ra attention Oh trên cả cache và đầu vào.
1: tính toán ¯Xt∈RB×T×Dm bằng cách cắt đầu vào Xt với tỷ lệ r.
2: nội suy ¯Xt thành độ dài Tm nếu T̸=Tm.
3: tính toán update gates gu và reset gates gr theo Eqn.(4).
4: tính toán Ct theo Eqn.(5) và trung bình Ct theo chiều batch.
5: cập nhật Ct−1←−Ct và lưu trữ nó.
6: tính toán đầu ra self-attention oh
self theo Eqn.(1).
7: tính toán đầu ra cached attention oh
mem theo Eqn.(3).
8: tính toán Oh theo Eqn.(2).

thêm vào GRC-Attentions. Để so sánh công bằng các mô
hình cached với baseline của chúng, chúng tôi áp dụng thiết
lập huấn luyện ban đầu của chúng bao gồm data augmenta-
tion, optimizer và các siêu tham số khác. Cụ thể, chúng tôi
sử dụng optimizer Adam với momentum 0.9 và weight decay
0.05. Tất cả các mô hình được huấn luyện trong hình ảnh
224×224 trong 300 epoch, với cosine learning rate scheduler.
Cả baseline và các mô hình cached sử dụng augmentation
timm tiêu chuẩn như (Touvron et al. 2021a), bao gồm nor-
malization, random cropping, horizontal flipping và color
jittering. Global average pooling được sử dụng trong PVT
và Swin, nơi kích thước pooling cho hai khối đầu tiên lần
lượt là 4 và 2. Tất cả các mô hình được huấn luyện trên 16
Nvidia Tesla V100 GPU, với bộ nhớ 32 GB.

Phát hiện đối tượng và phân đoạn instance trên COCO
2017. Các mô hình được huấn luyện trên COCO train2017
(118K hình ảnh) và đánh giá trên val2017 (5K hình ảnh).
Chúng tôi sử dụng cached PVT làm backbone và áp dụng
detector Mask R-CNN (He et al. 2017) để xác minh hiệu
quả của GRC-Attention. Trước khi huấn luyện, chúng tôi
sử dụng trọng số pretrained trên ImageNet (từ các thí nghiệm
trước) để khởi tạo backbone ngoại trừ cache C, sẽ được khởi
tạo thành zero. Vì độ dài đầu vào (T) thay đổi trong phát
hiện đối tượng, tại giai đoạn huấn luyện ¯X sẽ được nội suy
thành độ dài Tm để cập nhật cache. Các chỉ số COCO tiêu
chuẩn của Average Precision (AP) cho phát hiện bounding
box (APbb) và phân đoạn instance (APm) được sử dụng để
đánh giá

phương pháp của chúng tôi. Tất cả các thiết lập huấn luyện
và siêu tham số được giữ giống như triển khai PVT ban đầu
(Wang et al. 2021), và tất cả các mô hình liên quan được
huấn luyện trong 12 epoch (lịch trình huấn luyện 1×) sử
dụng 8 V100 GPU. Optimizer AdamW (Loshchilov and
Hutter 2018) được áp dụng với learning rate ban đầu
1×10−4. Các hình ảnh huấn luyện được thay đổi kích thước
thành 800×1333, có nghĩa là cạnh ngắn hơn là 800 pixel và
cạnh dài hơn không vượt quá 1333 pixel. Tại giai đoạn kiểm
tra, cạnh ngắn hơn của hình ảnh đầu vào được cố định ở 800.
Đối với cả cached PVT và baseline, backbone được pretrain
trước trên ImageNet và sau đó fine-tune cho phát hiện đối
tượng.

Bảng 8: Hiệu suất của GRC trên ImageNet-22k.
Model Top-1 Acc Top-5 Acc
Swin-T 80.9 96.0
Swin-T (Cached) 81.7 (+0.8) 96.4 (+0.4)

Long ListOps trên LRA. Đối với tất cả các thí nghiệm trên
benchmark LRA, chúng tôi tuân theo các code được phát
hành của (Tay et al. 2021a), triển khai GRC-Attention sử
dụng Flax và giữ tất cả các thiết lập huấn luyện khác không
thay đổi. Cụ thể, tất cả các mô hình được đánh giá được xây
dựng với 512 chiều embedding, 1024 chiều mlp, 8 head và
6 lớp, chỉ có các hàm attention được thay thế bằng các biến
thể attention khác nhau và các phiên bản cached của chúng.
Giống như thực hành trong phân loại hình ảnh, các module
GRC được khởi tạo với r= 0.5. Mỗi mô hình được huấn
luyện trong 5K bước (với 1K bước cho warmup) trên các
chuỗi độ dài 2K riêng lẻ với kích thước batch 32. Optimizer
Adam được áp dụng với learning rate ban đầu 0.05 và weight
decay 0.1.

Dịch máy trên IWSLT14 và IWSLT15. Chúng tôi thử
nghiệm phương pháp của mình trên các tập dữ liệu công khai
được sử dụng rộng rãi IWSLT14 (Cettolo et al. 2014) và
IWSLT15 (Cettolo et al. 2015). Đối với mỗi tập dữ liệu,
chúng tôi chọn ba track để xác thực GRC-Attention đề xuất,
bao gồm German-English(De-En), Spanish-English(Es-En)
và English-French(En-Fr) trong IWSLT14 và German-
English(De-En), English-Vietnamese(En-Vi) và Czech-
English(Cs-En) trong IWSLT15. Pre-Norm Transformer
trong (Wang et al. 2019) được sử dụng làm baseline và các
mô hình được triển khai sử dụng framework fairseq-py (Ott
et al. 2019). Theo (Wang et al. 2019; Ott et al. 2019), chúng
tôi thường tăng learning rate lên 2 và trung bình 10 check-
point cuối cho suy luận. Các mô hình GRC-cached được
tạo bằng cách thay thế các hàm attention của chúng trong
Transformer encoder

--- TRANG 12 ---
Bảng 9: Thời gian huấn luyện/suy luận cho các mô hình GRC-cached trên ImageNet.
Model Training throughput Testing throughput FLOPs Top-1 Accuracy
PVT-Tiny 313 930 1.90G 75.1
PVT-Tiny(Cached) 257 768 2.15G 78.4
PVT-Small 181 689 3.80G 79.9
PVT-Small(Cached) 146 561 4.29G 81.8
PVT-Medium 101 393 6.70G 81.2
PVT-Medium(Cached) 84 319 7.61G 83.0

Bảng 10: Hiệu suất của GRC với các tỷ lệ caching khác nhau.
Model Ratio FLOPs Acc
PVT-Tiny 0.000 1.90G 75.1
PVT-Tiny 0.125 1.93G 75.7
PVT-Tiny 0.250 1.96G 76.8
PVT-Tiny 0.500 2.15G 78.4
PVT-Tiny 1.000 2.97G 78.5

bằng các module GRC-Attention, được khởi tạo với tỷ lệ
caching r= 0.5 và độ dài cache Tm= 64. Tất cả các trans-
former trong nhiệm vụ này bao gồm 6 lớp encoder và 6 lớp
decoder, được huấn luyện với độ dài tối đa 512 và optimizer
Adam. Learning rate ban đầu là 0.0015 và sau đó giảm theo
inverse square root scheduler (Ott et al. 2019).

Kết quả Rộng rãi và Ablations
Kết quả Rộng rãi trên ImageNet-22k
Chúng tôi cũng tuân theo triển khai Swin-Transformer để
pretrain mô hình cached Swin-T của chúng tôi trên ImageNet-
22k. Như được hiển thị trong Tab. 8, GRC hiệu quả tăng
cường hiệu suất của mô hình Swin-T được pretrain trên tập
dữ liệu lớn hơn. Trong phiên bản cuối cùng, chúng tôi sẽ
bao gồm các kết quả bổ sung của các biến thể ViT / Swin-
Transformer khác trên ImageNet22k.

Kết quả Rộng rãi trên Phát hiện Đối tượng
Chúng tôi áp dụng rộng rãi GRC-Attention cho RetinaNet
(Lin et al. 2017), một mạng dự đoán dày đặc đại diện khác
cho phát hiện đối tượng. Chúng tôi chọn PVTs (Wang et al.
2021) với các kích thước khác nhau làm backbone, bao gồm
PVT-Tiny, PVT-Small, và PVT-Medium. Giống như thực
hành cho Mask R-CNN, chúng tôi sử dụng PVTs pretrained
được cached bởi GRC-Attention để khởi tạo backbone của
RetinaNet và huấn luyện các mô hình trong 12 epoch (lịch
trình RetinaNet 1×) với kích thước batch 16 trên 8 GPU.
Theo thực hành trong (Wang et al. 2021), chúng tôi áp dụng
optimizer AdamW (Loshchilov and Hutter 2018) với learning
rate ban đầu 1×10−4 để cập nhật các tham số. Chỉ số COCO
tiêu chuẩn Average Precision(AP) được sử dụng để đánh giá
các mô hình. Tab. 10 hiển thị kết quả detection sử dụng
RetinaNet. Nhất quán với Mask R-CNN, cached PVTs cải
thiện đáng kể baseline của chúng về precision. Ví dụ, cached
PVT-Medium có thể đạt 3.8 AP cao hơn so với vanilla PVT-
Medium, điều này khá đáng kể cho nhiệm vụ này. Tóm lại,
những thí nghiệm này trên các nhiệm vụ downstream (phát
hiện đối tượng và phân đoạn instance) chứng minh khả năng
tổng quát của cơ chế GRC-Attention đề xuất trong các nhiệm
vụ thị giác dày đặc.

Lựa chọn Tỷ lệ Caching
Đối với các siêu tham số chính như tỷ lệ caching và độ dài
bộ nhớ, chúng tôi thực hiện một loạt thí nghiệm sơ bộ và
chọn những cái phù hợp để đạt được sự đánh đổi tốt hơn
giữa độ phức tạp và hiệu suất. Tab. 10 cung cấp ablation
đối với tỷ lệ caching trên ImageNet. Như được hiển thị,
chúng ta có thể quan sát rằng các cải thiện hiệu suất từ tỷ lệ
caching lớn hơn (r) trở nên biên khi r >0.5.

Throughput Huấn luyện và Suy luận
Chúng tôi so sánh throughput (hình ảnh/giây, mỗi GPU) của
các mô hình GRC-cached và baseline trên ImageNet và kết
quả được hiển thị trong Tab. 9. Mô hình GPU là Tesla V100.
Chúng ta có thể thấy rằng GRC cải thiện hiệu suất của các
mô hình PVT có kích thước khác nhau trong khi giới thiệu
một sự gia tăng chi phí tính toán biên. Cụ thể, các mô hình
GRC-cached vượt qua các baseline no-cache tương ứng của
chúng 1.8%-3.3% độ chính xác top-1 với khoảng 15%-20%
giảm throughput. Xin cũng lưu ý rằng mặc dù GRC cải
thiện hiệu suất mô hình với tốc độ giảm nhẹ, nó vẫn hiệu
quả hơn đáng kể so với việc cải thiện mô hình bằng cách
tăng độ sâu/chiều rộng mô hình. Ví dụ, GRC-cached PVT-
Small đạt 81.8% độ chính xác huấn luyện với 146 throughput
huấn luyện, thậm chí vượt trội hơn no-cache PVT-Medium
mang lại 81.2% độ chính xác với 101 throughput huấn luyện.
