# E2LLM: CÁC MÔ HÌNH NGÔN NGỮ LỚN ĐƯỢC MỞ RỘNG BẰNG BỘ MÃ HÓA
CHO HIỂU BIẾT VÀ LẬP LUẬN NGỮ CẢNH DÀI
BẢN THẢO

Zihan Liao∗
Đại học Sư phạm Hoa Đông
52215901015@stu.ecnu.edu.cn

Jun Wang*
Ant Group
yangrong.wj@antgroup.com

Hang Yu*
Ant Group
hyu.hugo@antgroup.com

Lingxiao Wei
Đại học Sư phạm Hoa Đông
51265901053@stu.ecnu.edu.cn

Jianguo Li†
Ant Group
lijg.zero@antgroup.com

Jun Wang†
Đại học Sư phạm Hoa Đông
wongjun@gmail.com

Wei Zhang†
Đại học Sư phạm Hoa Đông
zhangwei.thu2011@gmail.com

11 tháng 9, 2024

## TÓM TẮT

Trong lĩnh vực Mô hình Ngôn ngữ Lớn (LLMs), khả năng xử lý ngữ cảnh dài ngày càng trở nên quan trọng cho các tác vụ như đối thoại nhiều vòng, tạo mã và tóm tắt tài liệu. Bài báo này giải quyết các thách thức về nâng cao hiệu suất ngữ cảnh dài, giảm độ phức tạp tính toán và tận dụng các mô hình đã được tiền huấn luyện—được gọi chung là "tam giác bất khả thi". Chúng tôi giới thiệu E2LLM (Encoder Elongated Large Language Models), một phương pháp tiếp cận mới hiệu quả điều hướng nghịch lý này. Phương pháp bao gồm việc chia ngữ cảnh dài thành các khối, nén mỗi khối thành vector nhúng thông qua bộ mã hóa văn bản đã được tiền huấn luyện, và sử dụng bộ chuyển đổi để căn chỉnh các biểu diễn này với LLM chỉ có bộ giải mã. Hai mục tiêu huấn luyện, tập trung vào tái tạo đầu ra bộ mã hóa và tinh chỉnh hướng dẫn ngữ cảnh dài, được sử dụng để tạo điều kiện cho LLM hiểu các lời nhắc mềm. Kết quả thực nghiệm chứng minh rằng E2LLM đạt được hiệu suất vượt trội trong các tình huống ngữ cảnh dài trong khi cân bằng hiệu quả, hiệu suất và khả năng tương thích với các mô hình đã được tiền huấn luyện. Do đó, khung làm việc của chúng tôi đại diện cho một tiến bộ đáng kể trong lĩnh vực này, góp phần vào việc mô hình hóa văn bản dài hiệu quả. Mã nguồn sẽ có sẵn khi xuất bản.

## 1 Giới thiệu

Hiểu biết và lập luận về ngữ cảnh dài đã trở nên thiết yếu cho các Mô hình Ngôn ngữ Lớn (LLMs), đặc biệt cho các tác vụ như đối thoại nhiều vòng Bai et al. [2024], tạo mã (nhiều) kho lưu trữ Zhang et al. [2023], và tóm tắt (nhiều) tài liệu Giorgi et al. và trả lời câu hỏi Singh et al. [2021]. Các tác vụ này thường yêu cầu xử lý hàng nghìn hoặc thậm chí hàng triệu token để đảm bảo tính mạch lạc và chính xác. Mặt khác, để tăng cường hiệu suất của LLMs, các kỹ thuật kích hoạt hiệu quả kiến thức chuyên môn của LLMs—như lập luận chuỗi suy nghĩ Wei et al. [2022], học ngữ cảnh Dong et al. [2022], và truy xuất tài liệu liên quan hoặc hội thoại lịch sử Ding et al. [2024]—cũng đang thúc đẩy nhu cầu cho độ dài chuỗi dài hơn.

Nỗ lực đáng kể đã được và đang được đầu tư vào việc phát triển các mô hình có thể tăng độ dài ngữ cảnh của LLMs, nhằm đạt được hiệu suất mạnh cho ngữ cảnh dài hơn (T1), đồng thời giảm độ phức tạp huấn luyện và suy luận

*Đóng góp bằng nhau. Công việc này được thực hiện khi Zihan Liao là thực tập sinh nghiên cứu tại Ant Group.
†Tác giả liên hệ.

(T2), và cùng lúc tương thích với các mô hình đã được tiền huấn luyện (T3) sao cho kiến thức đã được tiền huấn luyện trong các mô hình này có thể được khai thác hiệu quả. Tuy nhiên, đạt được cả ba mục tiêu đồng thời đặt ra một thách thức ghê gớm thường dẫn đến một số thỏa hiệp, một hiện tượng chúng tôi gọi là "tam giác bất khả thi", như được minh họa trong Hình 1.

Hiện tại, nghiên cứu trong lĩnh vực này chủ yếu tập trung vào ba hướng chính: sửa đổi nhúng vị trí, cơ chế chú ý và chính chuỗi đầu vào dài. Nhóm phương pháp đầu tiên, được gọi là mở rộng độ dài, bao gồm điều chỉnh nhúng vị trí của LLMs để phù hợp với việc mở rộng ngữ cảnh dài hơn. Điều này thường bao gồm việc chọn một giá trị cơ sở lớn cho RoPE Su et al. [2024] và sau đó tiếp tục tiền huấn luyện hoặc tinh chỉnh trên độ dài mục tiêu. Trong khi các phương pháp này hiệu quả mở rộng độ dài của LLMs với những thay đổi mô hình tối thiểu (T1&T3), chúng thường phát sinh chi phí tính toán đáng kể trong cả quá trình huấn luyện và suy luận (T2). Ví dụ, ngay cả với khả năng mở rộng độ dài chuỗi lên 2M, như thấy trong LongRoPE Ding et al., nguồn lực khổng lồ được yêu cầu để huấn luyện và triển khai mô hình, và thời gian suy luận có thể cấm đoán dài cho các chuỗi mở rộng. Trái ngược với nhóm đầu tiên, nhóm thứ hai, được gọi là chú ý thưa thớt, thay thế chú ý đầy đủ trong LLMs bằng chú ý cục bộ hoặc kết hợp chú ý toàn cầu và cục bộ. Phương pháp này giảm đáng kể độ phức tạp bậc hai liên quan đến chú ý đầy đủ, thậm chí đạt độ phức tạp tuyến tính về mặt lý thuyết (T2). Tuy nhiên, một mối quan tâm đáng chú ý với chú ý thưa thớt là tiềm năng bỏ lỡ lịch sử thông tin, vì một số token có thể không được chú ý đến trong quá trình tính toán chú ý (T1). Hơn nữa, vì LLMs không được tiền huấn luyện ban đầu với chú ý thưa thớt, việc thích ứng chúng với chú ý thưa thớt có thể yêu cầu huấn luyện hoặc tinh chỉnh rộng rãi (T3). Khác với hai nhóm trước đó thay đổi LLMs, nhóm chiến lược thứ ba trực tiếp nén chuỗi đầu vào để giảm độ dài của nó (T2), có thể được chia thành hai tiểu danh mục. Tiểu nhóm đầu tiên, được gọi là nén lời nhắc cứng—được minh họa bằng các phương pháp như Tạo Tăng cường Truy xuất (RAG) Ding et al. [2024] và LLMLingua Jiang et al. [2023a]—có xu hướng xử lý nén và suy luận theo cách hai bước. Kết quả là, bất kỳ mất mát thông tin hoặc giới thiệu nội dung không liên quan nào trong giai đoạn nén có thể ảnh hưởng tiêu cực đến hiệu suất trong bước suy luận tiếp theo (T1). Thay vào đó, tiểu nhóm thứ hai xem xét nén lời nhắc mềm, tóm tắt ngữ cảnh dài thành các vector nhúng. Tuy nhiên, việc sử dụng LLMs trong các phương pháp này để trực tiếp tạo nhúng cấp câu khác với mục tiêu tiền huấn luyện ban đầu của chúng là dự đoán token tiếp theo. Do đó, đạt được hiệu suất thỏa đáng trong bối cảnh này thường đòi hỏi huấn luyện hoặc tinh chỉnh nghiêm ngặt để căn chỉnh khả năng của mô hình với mục tiêu mới (T3).

Trong bài báo này, chúng tôi đề xuất một phương pháp dựa trên nén mới có tên E2LLM (Encoder Elongated Large Language Models) khéo léo điều hướng sự phức tạp của "tam giác bất khả thi". Cụ thể, như được hiển thị trong Hình 2, phương pháp của chúng tôi đầu tiên chia ngữ cảnh dài thành các khối và nén mỗi khối thành một vector nhúng sử dụng bộ mã hóa văn bản đã được tiền huấn luyện (ví dụ, BERT Kenton and Toutanova [2019]). Sau đó, một bộ chuyển đổi căn chỉnh đầu ra của bộ mã hóa với không gian nhúng đầu vào của LLM chỉ có bộ giải mã, sao cho LLM có thể hiểu các vector nhúng từ bộ mã hóa. Cuối cùng, chúng tôi thiết lập hai mục tiêu huấn luyện để căn chỉnh bộ mã hóa và bộ giải mã, bao gồm tái tạo văn bản đầu vào được mã hóa bởi bộ mã hóa ("hiểu biết") và tinh chỉnh hướng dẫn ngữ cảnh dài ("lập luận"). Chúng tôi đưa ra giả định rằng LLMs vốn dĩ phong phú về kiến thức; do đó, các lời nhắc mềm được nén đúng cách (hoặc các vector nhúng) có thể truyền đạt một cách ngắn gọn thông tin đầy đủ để LLMs tạo ra câu trả lời chính xác. Hơn nữa, vì các mô hình bộ mã hóa đã được tiền huấn luyện vốn được tạo ra để sản xuất nhúng câu, thiết kế này cho phép E2LLM tận dụng cả bộ mã hóa và bộ giải mã đã được tiền huấn luyện, giảm thiểu yêu cầu cho việc huấn luyện bổ sung rộng rãi (T3). Ngoài ra, việc nén mỗi khối gốc thành một vector (tức là, một token khối duy nhất) không chỉ nâng cao hiệu quả huấn luyện và suy luận (T2) mà còn mở rộng độ dài ngữ cảnh đáng kể (T1). Thực sự, độ dài chuỗi lý thuyết bằng tích của độ dài chuỗi của bộ mã hóa và bộ giải mã. Kết quả thực nghiệm cung cấp bằng chứng thuyết phục về hiệu suất vượt trội của E2LLM trong các tình huống ngữ cảnh dài, chứng minh hiệu quả của phương pháp chúng tôi trong việc duy trì sự cân bằng tinh tế giữa hiệu suất, hiệu quả và khả năng tương thích.

Tóm lại, những đóng góp chính của công việc chúng tôi là:

• Chúng tôi đề xuất E2LLM, một khung mô hình văn bản dài LLM mới được xây dựng dựa trên các mô hình nhúng câu đã được tiền huấn luyện và LLMs chỉ có bộ giải mã, hiệu quả giải quyết các thách thức do "tam giác bất khả thi" đặt ra.

• Chúng tôi giới thiệu hai mục tiêu huấn luyện, bao gồm tái tạo lời nhắc mềm được cung cấp bởi bộ mã hóa và tinh chỉnh hướng dẫn ngữ cảnh dài, cho phép LLM hiểu lời nhắc mềm trong khi tạo ra đầu ra chính xác cho đầu vào dài.

• Các thí nghiệm toàn diện được thực hiện trên các tác vụ và bộ dữ liệu đa dạng chứng minh hiệu quả và tính thực tế của mô hình được đề xuất và tiết lộ hiệu suất cạnh tranh với công nghệ tiên tiến.

## 2 Công trình liên quan

Như đã đề cập trong phần giới thiệu, các phương pháp phổ biến có thể được phân loại thành ba nhóm: sửa đổi nhúng vị trí (tức là, mở rộng độ dài), cơ chế chú ý (tức là, chú ý thưa thớt), và chuỗi đầu vào (tức là, nén lời nhắc).

**Huấn luyện Mở rộng Độ dài** Huấn luyện LLMs trên các chuỗi với độ dài chuỗi tối đa hạn chế trong khi đảm bảo khái quát hóa cho các chuỗi dài hơn là thách thức. Để giải quyết điều này, các phương pháp ngoại suy và nội suy vị trí đã được đề xuất. Ngoại suy vị trí mở rộng mã hóa vị trí vượt quá độ dài huấn luyện; ví dụ, ALiBi Press et al. nâng cao chú ý với độ lệch tuyến tính điều chỉnh điểm số dựa trên khoảng cách giữa vị trí khóa và truy vấn. Thay vào đó, xPOS Sun et al. [2023] sử dụng nhúng vị trí tương đối để có độ phân giải chú ý tốt hơn và độ dài mở rộng. Tuy nhiên, các phương pháp này chưa được tích hợp vào các LLMs gần đây như Llama2 Touvron et al. [2023] và Qwen2 Bai et al. [2023a], chủ yếu do hiệu suất không tối ưu của ngoại suy vị trí. Nội suy vị trí, mặt khác, thu nhỏ các chỉ số vị trí đầu vào và mở rộng cửa sổ ngữ cảnh để duy trì hiệu suất trên các chuỗi dài hơn. Ví dụ, Chen et al. [2023a] áp dụng nội suy tuyến tính cho RoPE để căn chỉnh các chỉ số vị trí tối đa với các ràng buộc tiền huấn luyện. Nội suy NTK bloc97. [2023] sửa đổi cơ sở của RoPE để điều chỉnh vận tốc quay của các chiều của nó. Để kết hợp điểm mạnh của các phương pháp này, YaRN Peng et al. [2023] hợp nhất nội suy tuyến tính và NTK với hàm ramping và yếu tố nhiệt độ, giảm thiểu dịch chuyển phân phối trong ma trận chú ý với đầu vào dài hơn. LongRoPE Ding et al. nâng cao hiệu suất hơn nữa bằng cách khai thác hai dạng không đồng đều trong nhúng vị trí RoPE thông qua tìm kiếm tiến hóa hiệu quả.

Bất chấp những tiến bộ này, hầu hết các phương pháp yêu cầu tiền huấn luyện liên tục hoặc tinh chỉnh để đạt được độ dài mong muốn, do đó gây ra gánh nặng huấn luyện đáng kể. Ngoài ra, suy luận trên các mô hình mở rộng này có thể chậm do độ phức tạp bậc hai của chú ý đầy đủ. Ngược lại, E2LLM được đề xuất không thay đổi độ dài ban đầu của LLM mà nén chuỗi đầu vào thành các khối vector nhúng. Điều này cho phép E2LLM duy trì hiệu quả của LLM ban đầu trong cả quá trình huấn luyện và suy luận.

**Chú ý Thưa thớt** Danh mục phương pháp này nhằm giảm độ phức tạp suy luận của các mô hình ngôn ngữ lớn (LLMs) bằng cách thao tác cơ chế chú ý với mặt nạ chú ý mới, cho phép các mô hình này xử lý các chuỗi dài hơn. StreamingLLM Xiao et al. [2024] chứng minh rằng việc tập trung vào đầu chuỗi và các token gần đây nhất trong một cửa sổ xác định (tức là, chú ý cục bộ) trong quá trình suy luận duy trì hiệu suất trong khi giảm đáng kể chi phí tính toán xuống quy mô tuyến tính. Tuy nhiên, các phương pháp không cần huấn luyện này thường không đạt hiệu quả trong nhiều tình huống Anagnostidis et al. [2023], Lou et al. [2024], vì chúng có thể bỏ lỡ các token thông tin nằm ở giữa chuỗi. Để cải thiện hiệu suất, LM-Infinite Han et al. [2024] giới thiệu lại top-k token từ giữa, nhưng phương pháp này cần tính toán tất cả điểm chú ý, do đó tăng yêu cầu tính toán. Như một giải pháp, Lou et al. [2024] đề xuất chú ý SparseK, sử dụng mạng chấm điểm bổ sung để đánh giá tầm quan trọng của mỗi cặp khóa-giá trị và chọn các cặp top-k. Thay vào đó, LongLoRA Chen et al. [2023a] sử dụng chú ý thưa thớt dịch chuyển (một biến thể của chú ý cục bộ) và tinh chỉnh LLMs với LoRA Hu et al. [2021] để thích ứng với cơ chế này. Thật không may, như được ghi nhận bởi Tan et al. [2024], vẫn còn khoảng cách đáng kể giữa chú ý thưa thớt và đầy đủ, điều này làm phức tạp việc tinh chỉnh LLMs đã được tiền huấn luyện sang các mô hình chú ý mới. Ngược lại, phương pháp E2LLM tóm tắt đầu vào ngữ cảnh dài thành các vector lời nhắc mềm, do đó giảm độ dài chuỗi mà không thay đổi cơ chế chú ý đầy đủ trong LLMs.

**Nén Lời nhắc** Nén lời nhắc nâng cao hiệu quả xử lý đầu vào LLM bằng cách hoặc ngưng tụ các lời nhắc dài (nén lời nhắc cứng) hoặc học các biểu diễn lời nhắc nhỏ gọn (nén lời nhắc mềm). Các kỹ thuật nén lời nhắc cứng bao gồm RAG Ding et al. [2024], LLMlingua Jiang et al. [2023a], Selective-Context Li [2023], và LongLLMlingua Jiang et al. [2023b]. RAG tối ưu hóa đầu vào bằng cách chỉ truy xuất các đoạn văn liên quan đến truy vấn, trong khi LLMlingua và Selective-Context nén ngữ cảnh dài mà không tham chiếu đến truy vấn. LongLLMlingua bắc cầu các phương pháp này bằng cách sử dụng nén thô đến tinh biết câu hỏi, sắp xếp lại tài liệu, tỷ lệ động và phục hồi chuỗi con để cải thiện hiệu suất. Tuy nhiên, các phương pháp này tách nén và suy luận thành các bước riêng biệt, dẫn đến lan truyền lỗi tiềm năng làm giảm hiệu suất. Ngược lại, E2LLM được huấn luyện end-to-end, hiệu quả giảm thiểu vấn đề trên.

Nén lời nhắc mềm, được đề xuất bởi Mu et al. [2023] và Ge et al. [2023], bao gồm việc huấn luyện LLMs để chưng cất lời nhắc thành một tập hợp token ngắn gọn hơn bao gồm kiến thức của lời nhắc ban đầu để sử dụng trong tương lai. Chevalier et al. [2023] mở rộng điều này bằng cách phát triển AutoCompressor, chuyển đổi ngữ cảnh văn bản dài hơn thành các vector tóm tắt phục vụ như lời nhắc mềm, mở rộng cửa sổ ngữ cảnh của LLM và giảm chi phí tính toán, như được minh họa trong LLoCO Tan et al. [2024]. Tuy nhiên, việc trực tiếp sử dụng LLMs để tạo nhúng cấp câu khác với mục tiêu ban đầu của chúng là dự đoán token tiếp theo. Kết quả là, đạt được hiệu suất thỏa đáng trong bối cảnh này thường yêu cầu huấn luyện hoặc tinh chỉnh rộng rãi để căn chỉnh mô hình với mục tiêu mới. Để khắc phục vấn đề này, E2LLM của chúng tôi tận dụng mô hình nhúng câu đã được tiền huấn luyện để biểu diễn lời nhắc, căn chỉnh với các mục tiêu huấn luyện ban đầu của các mô hình nhúng.

## 3 Phương pháp của chúng tôi: E2LLM

Trong phần này, chúng tôi trình bày chi tiết khung E2LLM được đề xuất để hiểu biết và lập luận về ngữ cảnh dài, hiệu quả kết hợp điểm mạnh của các bộ mã hóa văn bản đã được tiền huấn luyện và bộ giải mã LLM.

### 3.1 Kiến trúc Mô hình

Hình 1 minh họa kiến trúc của khung E2LLM, bao gồm bốn thành phần chính: Chunker, Bộ Mã hóa Văn bản Eθ, Bộ Chuyển đổi Aϕ, và Bộ Giải mã LLM Dη. Ở đây, θ, ϕ, và η biểu thị các tham số (có thể học) cụ thể cho từng thành phần. Điều quan trọng cần lưu ý là việc lựa chọn mô hình cho bộ mã hóa và bộ giải mã, phương pháp chia khối, và kiến trúc mạng của bộ chuyển đổi có thể được tùy chỉnh để đáp ứng nhu cầu của các lĩnh vực khác nhau. E2LLM phục vụ như một khung linh hoạt, tích hợp liền mạch các thành phần này để quản lý hiệu quả ngữ cảnh dài trong khi có khả năng tận dụng sức mạnh của các thành phần tiên tiến hơn khi có sẵn. Bây giờ chúng tôi sẽ giới thiệu chi tiết từng thành phần, theo dòng dữ liệu trong quá trình suy luận trong E2LLM.

**Chunker** Chunker có trách nhiệm chia ngữ cảnh dài thành các khối nhỏ hơn, dễ quản lý trong khi đảm bảo rằng độ dài token của mỗi khối không vượt quá độ dài chuỗi tối đa của bộ mã hóa văn bản. Tương tự như RAG, việc lựa chọn chiến lược chia khối có thể ảnh hưởng đến hiệu suất tổng thể của E2LLM. Ở đây, chúng tôi áp dụng một phương pháp đơn giản nhưng hiệu quả: chúng tôi đầu tiên xác định kích thước khối, trích xuất khối ban đầu, và sau đó lùi trong khối này để định vị các điểm ngắt, như dấu chấm hoặc ngắt dòng. Tiếp theo, chúng tôi bắt đầu một khối mới ở cuối khối trước đó và áp dụng phương pháp lùi lại một lần nữa. Chúng tôi lặp lại quá trình này cho đến khi tất cả văn bản được chia khối. Phương pháp này giúp duy trì tính toàn vẹn ngữ nghĩa của văn bản gốc. Lưu ý rằng các phương pháp khác như giới thiệu sự chồng chéo giữa các khối cũng có thể có lợi cho E2LLM. Ngoài ra, kích thước của các khối là quan trọng cho hiệu suất của E2LLM. Các thí nghiệm của chúng tôi chỉ ra rằng bao gồm ngữ cảnh quá mức trong một khối duy nhất có thể làm giảm hiệu suất, chủ yếu vì tỷ lệ nén cao có thể làm cho vector nhúng quá chung, làm tổn hại tính cụ thể.

**Bộ Mã hóa Văn bản E** Sau khi chia khối, chúng tôi đưa mỗi khối vào bộ mã hóa văn bản để tạo ra vector nhúng tương ứng. Đáng chú ý, hầu hết các bộ mã hóa đã được tiền huấn luyện, như GTE Li et al. [2023] và BGE Xiao et al. [2023], được huấn luyện thông qua học tương phản. Điều này có nghĩa là token [CLS], phục vụ như vector nhúng, thường chỉ nắm bắt thông tin phân biệt cần thiết để phân biệt giữa các khối, trong khi thông tin thiết yếu cho bộ giải mã LLM để trả lời truy vấn có thể bị loại bỏ. Để giảm thiểu hạn chế này, chúng tôi áp dụng thích ứng thứ hạng thấp (LoRA) Hu et al. [2021] để làm cho bộ mã hóa văn bản có thể huấn luyện trong quá trình căn chỉnh. Điều này cho phép bộ mã hóa bảo tồn thông tin có lợi cho hiệu suất của LLM.

**Bộ Chuyển đổi A** Để tạo điều kiện cho LLM hiểu ngữ nghĩa theo khối được lấy từ đầu ra của bộ mã hóa, chúng tôi sử dụng Bộ Chuyển đổi để ánh xạ đầu ra của bộ mã hóa vào nhúng đầu vào của LLM. Vì các chiều ẩn của bộ mã hóa văn bản và bộ giải mã LLM có thể khác nhau, Bộ Chuyển đổi là một thành phần quan trọng. Cụ thể, chúng tôi sử dụng Perceptron Đa tầng (MLP) hai tầng với hàm kích hoạt GELU Hendrycks and Gimpel [2016] như mạng bộ chuyển đổi. Bộ Chuyển đổi này được áp dụng cho từng nhúng khối riêng lẻ, và chúng tôi gọi đầu ra của nó là token khối hoặc lời nhắc mềm, sau đó được xử lý bởi LLM tiếp theo. Bộ Chuyển đổi được khởi tạo ngẫu nhiên và huấn luyện từ đầu trong giai đoạn căn chỉnh.

**Bộ Giải mã LLM D** Cuối cùng, chúng tôi nối các token khối (các token xanh trong Hình 2) và các token văn bản tương ứng với lời nhắc và truy vấn lại với nhau, và yêu cầu LLM tạo ra câu trả lời cho truy vấn. Trong các thí nghiệm của chúng tôi, chúng tôi chọn Llama2 Touvron et al. [2023] làm Bộ Giải mã LLM do việc sử dụng rộng rãi của nó trong cả nghiên cứu học thuật và ứng dụng công nghiệp. Ngoài ra, chúng tôi sử dụng LoRA để tiếp tục huấn luyện Bộ Giải mã như một phần của quá trình căn chỉnh giữa bộ mã hóa và bộ giải mã.

### 3.2 Các Tác vụ Huấn luyện

Bây giờ chúng tôi tập trung vào việc huấn luyện hai tầng cuối cùng của bộ mã hóa, bộ chuyển đổi, và nhánh LoRA của bộ giải mã để nâng cao khả năng của E2LLM trong việc hiểu ngữ cảnh đầu vào dài và lập luận hiệu quả về các câu trả lời tương ứng. Để hoàn thành điều này, chúng tôi giới thiệu hai tác vụ huấn luyện riêng biệt.

Tác vụ đầu tiên được thiết kế để cải thiện khả năng hiểu biết đầu vào của LLM. Như được mô tả trong Hình 2, một khi LLM nhận được các token khối từ bộ chuyển đổi, chúng tôi nhắc nó để phát lại hoặc tái tạo đầu vào. Chúng tôi gọi đây là tác vụ "hiểu biết". Lời nhắc cụ thể được sử dụng là "Cho các ngữ cảnh: [token khối] \nVui lòng làm theo hướng dẫn: \nPhát lại ngữ cảnh đã nói trước đó". Đáng chú ý, tác vụ này tự giám sát, cho phép chúng tôi thu thập một lượng lớn dữ liệu huấn luyện để đảm bảo rằng LLM hiểu toàn diện các nhúng được cung cấp bởi bộ chuyển đổi. Tuy nhiên, trong các thí nghiệm của chúng tôi, chúng tôi chỉ sử dụng đầu vào từ dữ liệu tinh chỉnh hướng dẫn ngữ cảnh dài cho tác vụ này. Cho rằng các đầu vào này thường quá dài để được tái tạo đầy đủ cùng một lúc, chúng tôi sử dụng phương pháp cửa sổ trượt, tái tạo ngữ cảnh gốc theo từng đoạn dựa trên một vài khối liên tiếp cho đến khi toàn bộ đầu vào đã được phát lại.

Mặt khác, tác vụ huấn luyện thứ hai cho phép LLM tạo ra câu trả lời dựa trên các token khối (tức là, ngữ cảnh dài) và truy vấn của người dùng. Chúng tôi gọi đây là tác vụ "lập luận", và lời nhắc được tạo cho mục đích này là "Cho các ngữ cảnh: [token khối] \nVui lòng làm theo hướng dẫn: \nTrả lời câu hỏi: {truy vấn}".

**Độ dài Chuỗi Tối đa** Về mặt lý thuyết, độ dài chuỗi tối đa của E2LLM bằng tích của độ dài chuỗi của bộ mã hóa và bộ giải mã. Tuy nhiên, như đã đề cập trước đây, việc đặt kích thước khối để khớp với độ dài chuỗi của bộ mã hóa đặt ra thách thức, vì nó có thể cản trở khả năng của bộ mã hóa giữ lại tất cả thông tin liên quan trong một khối duy nhất. Do đó, việc thiết lập kích thước khối phù hợp là quan trọng. Vì vậy, độ dài chuỗi hiệu quả của E2LLM được xác định là kích thước khối nhân với độ dài chuỗi của bộ giải mã LLM. Trên thực tế, chúng tôi đặt kích thước khối tối đa là 512 ký tự trong các thí nghiệm, xấp xỉ tương đương với 100 token. Do đó, độ dài ngữ cảnh đã được mở rộng gần 100 lần.

**Độ phức tạp Thời gian và Không gian trong Suy luận** Hãy ký hiệu độ dài đầu vào ban đầu là L và kích thước khối trong E2LLM là C. Do đó, tổng số khối trở thành L/C. Đối với mỗi khối, độ phức tạp thời gian và không gian từ bộ mã hóa văn bản là O(C²). Cho rằng có L/C khối, độ phức tạp tổng thể cho bước mã hóa là O(CL). Trong thực tế, vì tất cả các khối có thể được xử lý song song, độ phức tạp thời gian có thể được giảm thêm bởi một hệ số không đổi. Tiếp theo, chúng tôi chuyển L/C token khối đến bộ giải mã LLM, tạo ra độ phức tạp O(L²/C²). Tóm lại, độ phức tạp tổng thời gian và không gian là O(LC+L²/C²). Để chứng thực hiệu quả của E2LLM trong suy luận, chúng tôi thực hiện các thí nghiệm thực nghiệm đánh giá cả thời gian suy luận và sử dụng bộ nhớ. Chi tiết thêm có thể được tìm thấy trong Phần 4.5.

### 3.3 Mối quan hệ với Các phương pháp Khác

**Mối quan hệ với VLMs** E2LLM lấy cảm hứng từ những tiến bộ gần đây trong Mô hình Thị giác-Ngôn ngữ (VLMs) Zhang et al. [2024], bao gồm mini-GPT4 Zhu et al., LLaVa Liu et al. [2024], Qwen-VL Bai et al. [2023b], và InternVL Chen et al. [2024]. Những VLMs này sử dụng bộ chuyển đổi để căn chỉnh các bộ mã hóa thị giác đã được tiền huấn luyện với các bộ giải mã LLM, cho phép LLMs xử lý các token hình ảnh được xuất ra bởi các bộ mã hóa thị giác. Trong khung này, cả bộ mã hóa thị giác và bộ giải mã LLM đều được tiền huấn luyện độc lập, cung cấp một phương pháp linh hoạt cho phép căn chỉnh các mô hình thị giác và ngôn ngữ hiệu suất cao, do đó tối đa hóa khả năng của chúng. Đáng chú ý, VLMs xuất sắc trong việc thực hiện các tác vụ OCR (Nhận dạng Ký tự Quang học) Islam et al. [2017], hiệu quả nhận dạng và xuất ra văn bản có trong hình ảnh. Được thúc đẩy bởi thành công của VLMs, chúng tôi đề xuất rằng bằng cách căn chỉnh các bộ mã hóa văn bản (tức là, các mô hình nhúng) với các bộ giải mã LLM sử dụng bộ chuyển đổi, LLMs có thể tương tự diễn giải các câu được mã hóa bởi các bộ mã hóa văn bản và rút ra suy luận dựa trên sự hiểu biết này. Hơn nữa, vì cả bộ mã hóa và bộ giải mã trong phương pháp của chúng tôi hoạt động trong cùng một phương thức, chúng tôi dự đoán rằng quá trình căn chỉnh sẽ đơn giản hơn so với điều yêu cầu cho các mô hình hoạt động trên các phương thức khác nhau, có khả năng giảm lượng dữ liệu cần thiết cho căn chỉnh. Ngược lại, tác vụ tái tạo được sử dụng trong việc huấn luyện E2LLM là tự giám sát, cho phép chúng tôi tích lũy một bộ dữ liệu văn bản khổng lồ để nâng cao hiểu biết ngữ cảnh của LLM. Ngược lại, tác vụ căn chỉnh trong VLMs dựa vào các cặp hình ảnh-văn bản được giám sát, đáng chú ý khó thu thập hơn.

**Mối quan hệ với RAG** RAG (Tạo Tăng cường Truy xuất) Ding et al. [2024] thường hoạt động bằng cách sử dụng bộ truy xuất dựa trên bộ mã hóa văn bản để xác định các đoạn văn liên quan từ cơ sở kiến thức để đáp ứng truy vấn của người dùng. Những văn bản được truy xuất này sau đó được đưa vào LLM (tức là, bộ tạo) để nâng cao phản hồi. RAG có thể tăng cường E2LLM bằng cách truy xuất các đoạn văn liên quan nhất, trong khi E2LLM có thể mở rộng độ dài ngữ cảnh của bộ tạo trong RAG. Ngoài ra, một thách thức đáng kể đối với RAG là sự không nhất quán có thể phát sinh giữa cách bộ truy xuất và bộ tạo diễn giải cùng một văn bản Li et al. [2024], Ding et al. [2024]. E2LLM giải quyết vấn đề này bằng cách căn chỉnh bộ truy xuất (bộ mã hóa văn bản) với bộ tạo (bộ giải mã LLM), tạo điều kiện cho sự mạch lạc tốt hơn trong diễn giải. Hơn nữa, E2LLM cho phép giao tiếp hiệu quả hơn giữa bộ truy xuất và bộ tạo thông qua các vector nhúng thay vì văn bản thô.

**Mối quan hệ với LLoCO** So với E2LLM, LLoCO Tan et al. [2024] sử dụng AutoCompressor Chevalier et al. [2023] làm bộ mã hóa văn bản dài của nó và bỏ qua bộ chuyển đổi vì nó sử dụng cùng LLM (tức là, Llama2) như AutoCompressor. Kết quả là, nó có thể hiểu hiệu quả các vector tóm tắt—tương tự như token khối hoặc lời nhắc mềm—được tạo bởi AutoCompressor sau khi tinh chỉnh LLM với LoRA. Một lợi thế của LLoCO là bộ mã hóa văn bản của nó, AutoCompressor, xem xét sự phụ thuộc lẫn nhau của các khối ngữ cảnh dài. Tuy nhiên, điều này cũng đặt ra một hạn chế: ngữ cảnh dài chỉ có thể được xử lý tuần tự, từng khối một. Ngược lại, E2LLM có thể xử lý tất cả các khối song song và phù hợp hơn cho ngữ cảnh dài. Hơn nữa, bộ mã hóa bị hạn chế đối với AutoCompressor, làm cho việc nâng cao hiệu suất của LLoCO trở nên thách thức mà không cập nhật AutoCompressor. Đáng chú ý, AutoCompressor yêu cầu một quá trình tinh chỉnh rộng rãi trên Llama2 ban đầu sử dụng 2 tỷ token để cho phép Llama2 tạo ra các token tóm tắt. Ngược lại, E2LLM được thiết kế để dễ dàng kết hợp các bộ mã hóa văn bản mạnh hơn và bộ giải mã LLM khi chúng có sẵn trong suốt năm.

## 4 Thí nghiệm

Trong phần này, chúng tôi đánh giá hiệu suất của E2LLM trên hai tác vụ chính, bao gồm trả lời câu hỏi tài liệu (QA) và tóm tắt tài liệu. Để so sánh, chúng tôi đánh giá E2LLM với bốn đường cơ sở, bao gồm Yarn Peng et al. [2023], LongLoRA Chen et al. [2023b], RAG Gao et al. [2024], và LLoCO Tan et al. [2024], lần lượt đại diện cho các phương pháp hiện đại (SOTA) trong mở rộng độ dài, chú ý thưa thớt, nén lời nhắc cứng và nén lời nhắc mềm. Lưu ý rằng ngoại trừ RAG không cần huấn luyện, chúng tôi đặt thứ hạng của LoRA cho LLM giống nhau trong tất cả các phương pháp, dẫn đến 17M tham số có thể huấn luyện trong YaRN và LLoCO và 140M cho LongLoRA.

**Bảng 1: Thống kê Bộ dữ liệu.**

| Bộ dữ liệu | Loại Tác vụ | #Mẫu Huấn luyện | #Mẫu Đánh giá | Độ dài Mẫu |
|------------|-------------|------------------|---------------|-------------|
| QMSum | Tóm tắt | 1,257 | 272 | 14,428.78 |
| GovReport | Tóm tắt | 10,000 | 500 | 11,204.00 |
| Quality | Trả lời câu hỏi Tài liệu | 5,046 | 2,086 | 6,797.66 |
| NarrativeQA | Trả lời câu hỏi Tài liệu | 3,000 | 200 | 52,158.88 |
| TriviaQA | Trả lời câu hỏi Tài liệu | 10,000 | 500 | 1,075.90 |

### 4.1 Mô tả Bộ dữ liệu

Để đánh giá hiệu quả của E2LLM, chúng tôi tận dụng năm bộ dữ liệu có sẵn công khai bao gồm cả các tác vụ Tóm tắt và Trả lời Câu hỏi Tài liệu (DocumentQA). Thống kê dữ liệu được hiển thị trong Bảng 1.

• **QMSum³** Zhong et al. [2021] là một tiêu chuẩn được chú thích bởi con người mới được thiết kế cho tác vụ tóm tắt cuộc họp đa lĩnh vực dựa trên truy vấn. Nó bao gồm một phạm vi rộng lớn các cặp truy vấn-tóm tắt trên 232 cuộc họp trong các lĩnh vực đa dạng. Cụ thể, chúng tôi bao gồm 1,257 mẫu huấn luyện và sử dụng 272 mẫu để suy luận. Độ dài trung bình của các mẫu trong bộ dữ liệu này là 14,428.78 token.

• **GovReport⁴** Huang et al. [2021] chứa các báo cáo dài của Văn phòng Trách nhiệm Chính phủ Hoa Kỳ và Dịch vụ Nghiên cứu Quốc hội, được bổ sung bởi các tóm tắt và bản tóm tắt viết tay bởi chuyên gia, thuộc thể loại tác vụ tóm tắt. Với mục đích huấn luyện, 10,000 mẫu ngẫu nhiên được sử dụng, và để suy luận, 500 mẫu được chọn ngẫu nhiên từ tập xác thực. Độ dài trung bình của dữ liệu được lấy mẫu là 11,204.00 token.

• **Quality⁵** Bowman et al. [2022] là bộ dữ liệu DocumentQA bao gồm 5,046 mẫu huấn luyện và 2,086 mẫu suy luận với ngữ cảnh có độ dài trung bình 6,797.66 token. Hơn nữa, chúng tôi chuyển đổi định dạng dữ liệu lựa chọn đơn ban đầu của bộ dữ liệu thành định dạng QA.

• **NarrativeQA⁶** Kovcisky et al. [2018] là một bộ dữ liệu DocumentQA khác, chủ yếu được trích xuất từ văn bản sách toàn diện và kịch bản phim từ các nguồn đa dạng. Thách thức ở đây nằm ở việc tạo ra câu trả lời ngắn gọn từ các văn bản có thể bị lộn xộn và dài hơn. Chúng tôi lấy mẫu ngẫu nhiên 3,000 mẩu dữ liệu để huấn luyện, trong khi chọn ngẫu nhiên 200 mẫu để suy luận. Độ dài mẫu trung bình là 52,158.88 token.

• **TriviaQA⁷** Joshi et al. [2017] cũng là một bộ dữ liệu DocumentQA chất lượng cao chứa hơn 650K bộ ba câu hỏi-câu trả lời-bằng chứng. Nó bao gồm 95K cặp câu hỏi-câu trả lời được viết bởi những người đam mê trivia và các tài liệu bằng chứng được lấy nguồn độc lập. Chúng tôi chọn lần lượt 10,000 và 500 mẫu để huấn luyện và suy luận, với độ dài mẫu trung bình là 1,075.90 token.

### 4.2 Chỉ số Đánh giá

Đối với tác vụ Tóm tắt, hiệu suất của tất cả các phương pháp được đo lường bằng cách sử dụng chỉ số Rouge Lin [2004], hoạt động bằng cách so sánh n-gram của văn bản được tạo với văn bản tham chiếu. Cụ thể, chúng tôi tận dụng Rouge-1, Rouge-2, và Rouge-L để đánh giá sự chồng chéo giữa token đơn, token đôi liên tiếp, và chuỗi con chung dài nhất (LCS) trong văn bản được tạo bởi LLM và văn bản tham chiếu. Chúng tôi cũng tính toán trung bình hình học của chúng, ký hiệu là G-mean, và các giá trị cao hơn phản ánh chất lượng cao hơn của các bản tóm tắt được tạo.

Về tác vụ DocumentQA, chúng tôi áp dụng phương pháp được chứng minh bởi Shaham et al. [2023], tính toán sự chồng chéo unigram giữa câu trả lời được tạo và tham chiếu. Điều này được thực hiện bằng cách chuẩn hóa khoảng trắng, chữ thường, loại trừ từ dừng và dấu câu. Dựa trên số lượng token unigram, kết hợp với số lượng token của câu trả lời được tạo và tham chiếu, chúng tôi tính toán độ chính xác, recall, và F1. Một lần nữa, giá trị cao hơn cho thấy câu trả lời chính xác hơn của mô hình.

### 4.3 Đường cơ sở và Chi tiết Triển khai

Ở đây chúng tôi mô tả các đường cơ sở và chi tiết triển khai của chúng như sau:

• **Llama2-7B** Touvron et al. [2023] đề cập đến phiên bản 7B ban đầu của mô hình Llama2 không có tinh chỉnh. Chúng tôi sử dụng Llama2-7b-Chat⁸ cho các thí nghiệm và sử dụng nó làm xương sống của các phương pháp khác.

• **YaRN** Peng et al. [2023] đề xuất một phương pháp nội suy phân đoạn mới dựa trên chu kỳ của các chiều khác nhau để mở rộng kích thước cửa sổ ngữ cảnh. Chúng tôi đặt hệ số tỷ lệ là 16, và LoRA Hu et al. [2021] được áp dụng trên mô-đun tự chú ý với thứ hạng 16. Số lượng tham số có thể huấn luyện là 17M.

• **LongLoRA** Chen et al. [2023b] đề xuất sử dụng chú ý ngắn dịch chuyển thay vì chú ý đầy đủ ban đầu trong quá trình huấn luyện, và sử dụng Nội suy Vị trí Chen et al. [2023a] và LoRA để tinh chỉnh LLM để mở rộng cửa sổ ngữ cảnh. Chúng tôi đặt thứ hạng LoRA là 16, và tinh chỉnh các mô-đun tự chú ý, nhúng, và chuẩn hóa trong quá trình huấn luyện theo bài báo gốc, dẫn đến tổng cộng 140M tham số có thể huấn luyện.

• **RAG** Gao et al. [2024] bao gồm hai quá trình cốt lõi: truy xuất và tạo. Trong giai đoạn truy xuất, chúng tôi áp dụng GTE-Large-en Li et al. [2023] làm bộ truy xuất và thu hồi top-40 khối ngữ cảnh liên quan với độ dài tối đa 512 ký tự dựa trên độ tương tự cosine, và sau đó chúng được tận dụng làm lời nhắc cho LLMs trong quá trình tạo, và nó không liên quan đến quá trình huấn luyện.

• **LLoCO** Tan et al. [2024] là một phương pháp nén mềm sử dụng Autocompressors Chevalier et al. [2023] để mã hóa ngữ cảnh dài ngoại tuyến để đạt được mở rộng cửa sổ ngữ cảnh. Nhất quán với các phương pháp khác, chúng tôi sử dụng LoRA trên mô-đun tự chú ý với thứ hạng 16, dẫn đến số lượng tham số có thể huấn luyện là 17M.

• **E2LLM** Đối với E2LLM của chúng tôi, chúng tôi sử dụng GTE-Large-en Li et al. [2023] làm bộ mã hóa, sau đó được tinh chỉnh bằng LoRA với tham số thứ hạng được đặt là 8. Ngoài ra, chúng tôi sử dụng mạng mlp hai tầng với hàm kích hoạt GeLU Hendrycks and Gimpel [2016] làm bộ chuyển đổi. Đối với thành phần bộ giải mã, chúng tôi tận dụng Llama2-7B-Chat, cũng tinh chỉnh nó thông qua LoRA với thứ hạng 8, và số lượng tham số có thể huấn luyện cuối cùng là 16M.

**Bảng 2: Hiệu suất trên các bộ dữ liệu Ngữ cảnh Dài.**

| | Tham số Có thể Huấn luyện | Cửa sổ Ngữ cảnh | Phương pháp Mở rộng | QmSum | | | | GovReport | | | | Quality | | | NarrativeQA | | | TriviaQA | | |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| | | | | R1 | R2 | RL | G-mean | R1 | R2 | RL | G-mean | Prec. | Recall | F1 | Prec. | Recall | F1 | Prec. | Recall | F1 |
| Llama2-7B | 0M | 4K | - | 0.2190 | 0.0491 | 0.1421 | 0.1151 | 0.1068 | 0.0286 | 0.0546 | 0.0550 | 0.0616 | 0.2546 | 0.0938 | 0.0304 | 0.1352 | 0.0465 | 0.0672 | 0.7666 | 0.1206 |
| LongLoRA | 140M | 100K | Sparse Attn. | 0.0741 | 0.0999 | 0.0765 | 0.0741 | 0.2704 | 0.0992 | 0.1629 | 0.1635 | 0.0741 | 0.0999 | 0.0765 | OOM | OOM | OOM | 0.1303 | 0.4928 | 0.1969 |
| YaRN | 17M | 64K | Len. Ext. | 0.2154 | 0.0534 | 0.1624 | 0.1231 | 0.1293 | 0.0413 | 0.0569 | 0.0672 | 0.1320 | 0.1942 | 0.1380 | OOM | OOM | OOM | 0.1353 | 0.4945 | 0.2022 |
| RAG | 0M | +∞ | Hard Comp. | 0.0981 | 0.0172 | 0.0799 | 0.0512 | 0.0893 | 0.0229 | 0.0490 | 0.0465 | 0.0341 | 0.3408 | 0.0583 | 0.0041 | 0.0097 | 0.0055 | 0.0582 | 0.6743 | 0.1047 |
| LLoCO | 17M | 128K | Soft Comp. | 0.2371 | 0.0551 | 0.1679 | 0.1299 | 0.1169 | 0.0311 | 0.0518 | 0.0573 | 0.1681 | 0.1503 | 0.1437 | 0.1185 | 0.1134 | 0.1087 | 0.6404 | 0.6403 | 0.6321 |
| Của chúng tôi | 16M | 400K | Soft Comp. | 0.2537 | 0.0655 | 0.1875 | 0.1461 | 0.3314 | 0.1075 | 0.1859 | 0.1878 | 0.1344 | 0.1495 | 0.1294 | 0.1353 | 0.1379 | 0.1235 | 0.3322 | 0.3451 | 0.3337 |

### 4.4 So sánh Hiệu suất

Ở đây, chúng tôi sử dụng ba bộ dữ liệu cho QA, đó là Quality, NarrativeQA, và TriviaQA, cũng như hai bộ dữ liệu cho tóm tắt, đó là QMsum và GovReport. Chi tiết của những bộ dữ liệu này được tóm tắt trong Bảng 1. Lưu ý rằng Quality và TriviaQA có độ dài ngắn hơn so với các bộ dữ liệu khác, nhưng NarrativeQA có độ dài dài hơn. Đối với các thí nghiệm của chúng tôi, chúng tôi sử dụng tập xác thực của mỗi bộ dữ liệu để kiểm tra, và chúng tôi chia tập huấn luyện thành các tập con huấn luyện và xác thực sử dụng tỷ lệ 95:5. Chúng tôi cũng bao gồm LLama2-7B-chat ban đầu như một đường cơ sở. Để đánh giá hiệu suất, chúng tôi sử dụng điểm Rouge-N cho các tác vụ tóm tắt, trong khi đối với các tác vụ QA, chúng tôi sử dụng các chỉ số Độ chính xác, Recall, và F1-score (xem các định nghĩa liên quan trong Phần 4.2). Kết quả cho tất cả các phương pháp đường cơ sở được trình bày trong Bảng 2.

Rõ ràng là E2LLM được đề xuất liên tục đạt được hiệu suất tốt nhất hoặc tốt thứ hai trên tất cả các phương pháp được đánh giá. Thú vị, Yarn cho thấy kết quả đầy hứa hẹn trong cả hai tác vụ, tuy nhiên, nó gặp phải các vấn đề hết bộ nhớ (OOM) khi độ dài chuỗi vượt quá khoảng 74,000 trên GPU A100 với 80G, hệ quả của độ phức tạp không gian bậc hai vốn có trong LLMs. Hơn nữa, hiệu suất của Yarn giảm sút trên các chuỗi ngắn hơn, như được tìm thấy trong TriviaQA. Như đã lưu ý trong nghiên cứu trước Chen et al. [2023a], cơ chế chú ý có thể trở nên phân tán trong ngữ cảnh cực dài, lan rộng quá mỏng trên nhiều vị trí token và do đó làm giảm hiệu suất trên ngữ cảnh ngắn hơn. Về một vấn đề liên quan, LongLoRA, sử dụng chú ý thưa thớt dịch chuyển trong quá trình huấn luyện, thể hiện những thách thức tương tự. Nó có xu hướng bỏ qua lịch sử thông tin, dẫn đến hiệu suất kém hơn so với E2LLM trên tất cả các bộ dữ liệu. LongLoRA cũng gặp phải các vấn đề OOM với bộ dữ liệu NarrativeQA rộng lớn do việc sử dụng chú ý đầy đủ trong quá trình suy luận. Mặt khác, Llama2-7B-Chat ban đầu thể hiện kém trong các tác vụ QA, chủ yếu do độ dài ngữ cảnh hạn chế là 4K token. RAG cho thấy hiệu suất tệ nhất trong các tác vụ QA, có thể đến từ việc mất thông tin quan trọng cần thiết cho truy vấn cụ thể, bên cạnh đó, nó kém hiệu suất hơn Llama2-7B ban đầu trong tác vụ tóm tắt vì tiếng ồn được giới thiệu trong quá trình truy xuất có thể ảnh hưởng tiêu cực đến khả năng tạo của LLMs Wu et al. [2024]. So với Yarn, LongLoRA, và RAG, E2LLM chứng minh khả năng vượt trội trong việc phân biệt thông tin thiết yếu từ ngữ cảnh dài trong khi hiệu quả loại bỏ dữ liệu không liên quan.

Cuối cùng, chúng tôi quan sát rằng LLoCO thể hiện khá tốt trong các tác vụ QA, đặc biệt cho Quality và TriviaQA, có ngữ cảnh tương đối ngắn. Tuy nhiên, hiệu suất của nó giảm đáng kể trong các tác vụ tóm tắt, phù hợp với những phát hiện trong ấn phẩm riêng của nó (xem Bảng 1 trong Tan et al. [2024]). LLoCO sử dụng AutoCompressor Chevalier et al. [2023] làm bộ mã hóa văn bản của nó, sử dụng Llama2 để tạo ra các vector tóm tắt cho mỗi khối. Những vector này được thiết kế để chỉ giữ lại thông tin liên quan đến các khối tiếp theo, loại bỏ nội dung có giá trị khác, như được chỉ ra trong Rau et al. [2024]. Trong các tác vụ QA, chỉ các phần liên quan của ngữ cảnh dài được yêu cầu để nhắc LLM cho câu trả lời chính xác, điều này phù hợp khá tốt với các mục tiêu huấn luyện của AutoCompressor. Tuy nhiên, các tác vụ tóm tắt đòi hỏi hiểu biết toàn diện về toàn bộ ngữ cảnh dài. Vì các vector tóm tắt được tạo bởi AutoCompressor không bao gồm tất cả thông tin trong mỗi khối, hiệu suất của LLoCO trong tóm tắt bị ảnh hưởng. Ngược lại, E2LLM xuất sắc trong việc nắm bắt thông tin trên tất cả các khối do tác vụ "hiểu biết" hoặc tái tạo độc đáo của nó, góp phần vào hiệu suất vượt trội của nó trong tóm tắt.

### 4.5 Hiệu quả Suy luận

Trong tiểu mục này, chúng tôi kiểm tra hiệu quả suy luận của các phương pháp khác nhau. Chúng tôi bắt đầu bằng cách chọn bảy độ dài ngữ cảnh được phân phối đều giữa 1K và 73K, vì cả Yarn và LongLoRA đều gặp phải các vấn đề hết bộ nhớ (OOM) ở độ dài ngữ cảnh 74K. Đối với mỗi độ dài ngữ cảnh được chọn, chúng tôi chọn ngẫu nhiên mười mẫu và cắt bớt chúng đến độ dài được xác định trước. Sau đó chúng tôi tính trung bình thời gian chạy và chi phí bộ nhớ GPU trên mười mẫu này và minh họa kết quả như một hàm của độ dài ngữ cảnh trong Hình 3.

Mô hình của chúng tôi, E2LLM, thể hiện thời gian chạy và sử dụng bộ nhớ thấp nhất, đặc biệt cho các chuỗi rất dài ở 73K. Ngược lại, cả YaRN và LongLoRA đều chứng minh tiêu thụ tài nguyên cao hơn đáng kể, chủ yếu do độ phức tạp bậc hai của chú ý đầy đủ trong quá trình suy luận. Tương tự như E2LLM, LLoCO cũng giảm thời gian suy luận thông qua nén lời nhắc mềm. Tuy nhiên, bộ mã hóa văn bản của nó, AutoCompressor, có thể nén văn bản gốc tối đa 32 lần, so với khả năng nén của E2LLM khoảng 100 lần. Ngoài ra, AutoCompressor xử lý tất cả các khối tuần tự, trong khi E2LLM có thể xử lý chúng song song, giảm thêm thời gian suy luận. Đáng chú ý là bộ truy xuất trong mô hình RAG truy xuất 40 khối liên quan nhất từ ngữ cảnh dài, bất kể độ dài ngữ cảnh. Do đó, thời gian suy luận của nó không phụ thuộc vào độ dài ngữ cảnh nhưng phát sinh chi phí thời gian lớn hơn E2LLM do quá trình truy xuất bổ sung. Hơn nữa, việc chỉ truy xuất 40 đoạn văn, bất kể độ dài ngữ cảnh, có thể ảnh hưởng tiêu cực đến hiệu suất, đặc biệt trong các tác vụ tóm tắt, như được chứng minh trong Bảng 2.

### 4.6 Nghiên cứu Loại bỏ

Trong tiểu mục này, chúng tôi tiến hành các nghiên cứu loại bỏ E2LLM sử dụng các bộ dữ liệu QMSum và NarrativeQA, phục vụ như các tiêu chuẩn đại diện cho các tác vụ tóm tắt ngữ cảnh dài và trả lời câu hỏi tài liệu, tương ứng. Chi tiết của mỗi biến thể được kiểm tra trong Bảng 3 được nêu dưới đây.

• Biến thể **−und** bao gồm việc loại trừ tác vụ "hiểu biết" khỏi mô hình của chúng tôi và chỉ sử dụng tác vụ "lập luận" cho mục đích huấn luyện, nhấn mạnh vai trò quan trọng mà tác vụ "hiểu biết" đóng trong hiệu suất của mô hình.

• **−E** biểu thị việc đóng băng các tham số bộ mã hóa, do đó chỉ cho phép bộ chuyển đổi và LLM chỉ có bộ giải mã có thể huấn luyện. Cấu hình này nhằm chứng thực giả thuyết của chúng tôi rằng một bộ mã hóa đã được tiền huấn luyện một mình không có khả năng bảo tồn thông tin liên quan có tác động đáng kể đến hiệu suất của LLM. Do đó, việc duy trì các tham số của bộ mã hóa có thể huấn luyện là quan trọng.

• **−D** bao gồm việc giữ LLM chỉ có bộ giải mã đóng băng, để kiểm tra xem LLM có thể hiểu đầy đủ các token đầu ra từ bộ chuyển đổi mà không có bất kỳ huấn luyện chuyên dụng nào hay không.

• Biến thể **+overlap** giới thiệu sự chồng chéo 30% kích thước khối giữa các khối tuần tự trong quá trình chia khối. Hơn nữa, trong phạm vi hoạt động phát lại của tác vụ "hiểu biết", mô hình được yêu cầu phát lại phần chồng chéo của những khối này một lần.

• Kiểm tra biến thể **+bge**, mặt khác, bao gồm việc thay thế mô hình GTE-Large-en bằng mô hình bge-m3 làm bộ mã hóa. Nghiên cứu này nhằm khẳng định rằng mô hình của chúng tôi duy trì tương thích với các mô hình nhúng câu khác nhau phục vụ như bộ mã hóa.

• Cấu hình **+Llama2−13B**, tương tự trong việc kiểm tra như biến thể +bge, được thiết kế để xác minh tính tương thích của E2LLM của chúng tôi với các LLM khác phục vụ như bộ giải mã.

Đầu tiên, chúng tôi đánh giá tầm quan trọng của tác vụ "hiểu biết" trong E2LLM. Những phát hiện của chúng tôi cho thấy sự giảm sút hiệu suất đáng kể—16.39%—khi tác vụ này bị bỏ qua, làm nổi bật vai trò quan trọng của nó trong việc giúp E2LLM diễn giải các nhúng khối được tạo bởi bộ mã hóa và nâng cao hơn nữa hiệu suất của tác vụ "lập luận". Tiếp theo, chúng tôi kiểm tra sự cần thiết của việc huấn luyện các nhánh LoRA của bộ mã hóa và bộ giải mã trong quá trình căn chỉnh. Như được hiển thị trong Bảng 3, kết quả cho các cấu hình −E và −D nhấn mạnh tầm quan trọng của việc huấn luyện các thành phần này; không có việc huấn luyện này, hiệu suất của E2LLM giảm lần lượt 9.08% và 12.03%. Cuối cùng, chúng tôi khám phá tác động của việc thay thế chunker, bộ mã hóa văn bản, và bộ giải mã LLM trong E2LLM (được ký hiệu là +overlap, +bge, và +Llama2-13B).

**Bảng 3: Nghiên cứu Loại bỏ trên QMSum và NarrativeQA.**

| | QMsum | | | | NarrativeQA | | | Trung bình |
|---|---|---|---|---|---|---|---|---|
| | R1 | R2 | RL | G-mean | Prec. | Recall | F1 | Rel. Diff. |
| E2LLM | 0.2537 | 0.0655 | 0.1875 | 0.1461 | 0.1353 | 0.1379 | 0.1235 | - |
| -Und | 0.2264 | 0.0486 | 0.1620 | 0.1213 | 0.1113 | 0.1004 | 0.0994 | -16.39% |
| -E | 0.2343 | 0.0541 | 0.1731 | 0.1299 | 0.1247 | 0.1125 | 0.1083 | -9.08% |
| -D | 0.2309 | 0.0493 | 0.1711 | 0.1249 | 0.1223 | 0.1095 | 0.1046 | -12.03% |
| +Overlap | 0.2523 | 0.0639 | 0.1795 | 0.1425 | 0.1328 | 0.1394 | 0.1241 | +1.78% |
| +BGE | 0.2377 | 0.0607 | 0.1784 | 0.1370 | 0.1289 | 0.1203 | 0.1136 | -4.33% |
| +Llama2-13B | 0.2577 | 0.0672 | 0.1889 | 0.1484 | 0.1375 | 0.1374 | 0.1268 | +4.70% |

Phân tích của chúng tôi tiết lộ rằng các chunker với các đoạn chồng chéo (ví dụ, chồng chéo 30%) cung cấp một sự cải thiện hiệu suất khiêm tốn. Ngoài ra, việc sử dụng các bộ mã hóa và bộ giải mã tiên tiến hơn càng nâng cao hiệu suất của E2LLM, gợi ý rằng các cải thiện trong các thành phần riêng lẻ có thể ảnh hưởng tích cực đến hệ thống tổng thể.

### 4.7 Độ nhạy Siêu tham số

Trong phần này, chúng tôi khám phá các hiệu ứng của siêu tham số trên hiệu suất của E2LLM, cụ thể tập trung vào trọng số được gán cho tác vụ "hiểu biết", thứ hạng LoRA của bộ mã hóa và bộ giải mã, và số lượng tầng trong mạng bộ chuyển đổi.

Trọng số được gán cho tác vụ "hiểu biết" cho biết tầm quan trọng tương đối của nó so với tác vụ "lập luận". Nhớ lại rằng ngữ cảnh đầu vào thường có độ dài dài hơn nhiều so với câu trả lời, làm cho nó quá dài để được tái tạo đầy đủ cùng một lúc. Để giải quyết điều này, chúng tôi sử dụng phương pháp cửa sổ trượt, tái tạo ngữ cảnh gốc theo từng đoạn dựa trên một vài khối liên tiếp cho đến khi toàn bộ đầu vào đã được tái tạo. Do đó, các mẫu cho tác vụ "hiểu biết" nhiều hơn đáng kể so với những mẫu cho các tác vụ "lập luận". Để duy trì sự cân bằng mẫu, chúng tôi thường gán trọng số nhỏ hơn cho tác vụ phát lại. Như được mô tả trong Hình 4, trọng số tối ưu có thể thay đổi trên các bộ dữ liệu khác nhau, có thể bị ảnh hưởng bởi các yếu tố như độ dài ngữ cảnh và khả năng của mô hình nhúng câu để hiểu ngữ nghĩa cụ thể của ngữ cảnh.

Hơn nữa, chúng tôi điều tra thứ hạng LoRA tối ưu của bộ mã hóa (tức là, GTE-Large-en) và bộ giải mã (tức là, Llama2-7B-Chat) trong phạm vi {0, 4, 8, 12, 16, 20, 24} và {0, 2, 4, 6, 8, 10, 12}, tương ứng. Những phát hiện gợi ý rằng việc không có tham số có thể huấn luyện—nói cách khác, hoàn toàn "đóng băng" bộ mã hóa và bộ giải mã—cản trở việc trích xuất nội dung ngữ cảnh gốc hiệu quả và căn chỉnh giữa bộ mã hóa và bộ giải mã, như đã thảo luận trong Phần 3.1. Khi thứ hạng của hai mô-đun tăng, một sự cải thiện hiệu suất tương ứng được quan sát, do đó nhấn mạnh tầm quan trọng của việc huấn luyện. Sự nâng cao hiệu suất tiếp tục cho đến khi nó đạt đỉnh trong một phạm vi thứ hạng cụ thể. Tuy nhiên, vượt quá phạm vi tối ưu này, sự gia tăng thêm về thứ hạng dẫn đến sự suy giảm hiệu suất, do quá khớp trên các bộ dữ liệu huấn luyện.

Chúng tôi cũng kiểm tra tác động của số lượng tầng trong mạng bộ chuyển đổi. Hình 4 cho thấy rằng MLP hai tầng liên tục mang lại hiệu suất vượt trội trên các bộ dữ liệu khác nhau, cho thấy sự ổn định trong kết quả. Chúng tôi đưa ra giả thuyết rằng MLP một tầng có thể gặp khó khăn với tác vụ căn chỉnh, trong khi MLP ba tầng có thể dẫn đến quá khớp trên dữ liệu huấn luyện.

## 5 Kết luận

Trong bài báo này, chúng tôi trình bày E2LLM, một phương pháp tiếp cận mới để giải quyết các thách thức của việc nâng cao hiệu suất ngữ cảnh dài trong LLMs. Nó hiệu quả điều hướng "tam giác bất khả thi" bằng cách chiến lược chia ngữ cảnh dài thành các khối, nén chúng thành các vector nhúng, và sử dụng bộ chuyển đổi để căn chỉnh các biểu diễn này với LLM chỉ có bộ giải mã. Hai mục tiêu huấn luyện được sử dụng để tạo điều kiện cho việc hiểu biết lời nhắc mềm bởi LLMs, dẫn đến hiệu suất vượt trội trong các tình huống ngữ cảnh dài. Những phát hiện thực nghiệm tiết lộ rằng E2LLM hiệu quả vượt trội hơn các phương pháp hiện tại trong việc cân bằng hiệu suất ngữ cảnh dài, hiệu quả tính toán và tính tương thích mô hình.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo đầy đủ được dịch giữ nguyên định dạng và nội dung như bản gốc]
