# Không Bao Giờ Lạc Giữa Đường: Thành Thạo Trả Lời Câu Hỏi Ngữ Cảnh Dài với Huấn Luyện Phân Rã Bất Định Vị Trí

Junqing He, Kunhao Pan, Xiaoqun Dong,
Zhuoyang Song, Yibo Liu, Qianguo Sun,
Yuxin Liang, Hao Wang, Enming Zhang, Jiaxing Zhang
International Digital Economy Academy, Shenzhen, China
hejunqing@idea.edu.cn

## Tóm tắt

Trong khi các mô hình ngôn ngữ lớn (LLMs) được trang bị khả năng nhập văn bản dài hơn trước đây, chúng gặp khó khăn trong việc tìm kiếm thông tin chính xác trong ngữ cảnh dài. Vấn đề "lạc giữa đường" thách thức hầu hết các LLMs, đề cập đến sự sụt giảm đáng kể về độ chính xác khi thông tin chính xác được đặt ở giữa. Để khắc phục vấn đề quan trọng này, bài báo này đề xuất tăng cường khả năng tìm kiếm thông tin và phản chiếu của LLMs trong ngữ cảnh dài thông qua các tác vụ được thiết kế đặc biệt gọi là Position-Agnostic Multi-step QA (PAM QA). Được huấn luyện với tác vụ này, mô hình của chúng tôi xuất sắc trong việc tập trung chính xác hơn vào thông tin mong muốn. Kết quả thực nghiệm cho thấy cải thiện đáng kể trong Multi-doc QA và các tiêu chuẩn khác, vượt qua các mô hình tiên tiến với mức tăng tuyệt đối 13.7% trong cài đặt xáo trộn và 21.5% trong tác vụ truy xuất đoạn văn. Chúng tôi phát hành mô hình và mã nguồn để thúc đẩy nghiên cứu liên quan trong cộng đồng.

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLMs), nổi tiếng với khả năng sinh sản và học zero-shot đặc biệt trên các lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) đa dạng, đã tìm thấy các ứng dụng hạ nguồn rộng rãi (OpenAI, 2023; Boiko et al., 2023; Cheng et al., 2023; Waisberg et al., 2023; Hu et al., 2023). Tuy nhiên, LLMs gặp phải các ảo giác nghiêm trọng, làm giảm đáng kể hiệu suất của chúng trong QA hướng tri thức, đối thoại và viết (Roberts et al., 2020; Agrawal et al., 2023). Retrieval Augmented Generation (RAG) là một giải pháp hiệu quả cho các ảo giác, và những cải tiến đáng chú ý đã đạt được bằng cách kết hợp kiến thức hỗ trợ vào đầu vào của LLMs (Lewis et al., 2020b; Shuster et al., 2021; Thoppilan et al., 2022; Shi et al., 2023a). Thách thức cơ bản nhất cần giải quyết trong RAG là ngữ cảnh dài và trả lời câu hỏi đa tài liệu (Multi-doc QA).

Một số công trình nghiên cứu xung quanh vấn đề với một hệ thống hoặc đường ống phức tạp (Chen et al., 2023a; Lee et al., 2024), nhưng chúng tôi nhằm cải thiện các mô hình nền tảng vì chúng là thành phần cốt lõi của những phương pháp đó. Nghiên cứu kỹ lưỡng đã được tiến hành để xử lý đầu vào ngữ cảnh dài, được phân loại thành ba dòng chính: Đầu tiên là mở rộng cửa sổ ngữ cảnh sử dụng cửa sổ trượt (Dai et al., 2019; Xiao et al., 2023). Các nhà nghiên cứu khác đề xuất tăng cường khả năng ngoại suy bằng cách cải thiện Relative Positional Encoding trong Transformers, xương sống của hầu hết LLMs (Su et al., 2021; Press et al., 2021; Luo et al., 2022; Vaswani et al., 2017). Hai loại chỉnh sửa này đều cho thấy cải thiện đáng kể trong mô hình hóa ngôn ngữ (LM). Loại nghiên cứu thứ ba tập trung vào nén tuần hoàn của bộ nhớ cho học chuỗi tầm xa (Rae et al., 2019; Peng et al., 2023). Phương pháp này hiệu quả trong việc học biểu diễn toàn diện của ngữ cảnh, thể hiện năng lực đáng chú ý trong tính toán nhanh và hiệu quả chi phí trong quá trình suy luận. Mặc dù các phương pháp trên cho thấy hiệu suất mạnh trong các tác vụ cụ thể và hỗ trợ LLMs với cửa sổ ngữ cảnh cực dài, tức là GPT3.5-Turbo-16K, Claude-v1.3-100K và Longchat (Dacheng et al., 2023), LLMs thất bại trong việc tạo ra câu trả lời chính xác nếu các tài liệu liên quan được đặt ở giữa ngữ cảnh, được gọi là "lạc giữa đường" (Liu et al.). Điều này là chết người đối với Multi-doc QA. Tuy nhiên, liệu có tồn tại sự suy giảm tương tự trong LLMs tiếng Trung hay không vẫn chưa được khám phá và các giải pháp cho vấn đề này hiếm khi được nghiên cứu.

Chúng tôi giả định rằng quy mô của điểm attention ở ngữ cảnh đầu tiên trở nên lớn sau tiền huấn luyện và điều chỉnh hướng dẫn trong khi quy mô của ngữ cảnh giữa, có vị trí ít được huấn luyện hơn, vẫn nhỏ trong khoảng cách dài đến token hiện tại. Điều này hạn chế đóng góp của thông tin liên quan vào câu trả lời và dẫn đến độ chính xác QA thấp hơn.

Để khắc phục cạm bẫy này, chúng tôi đề xuất huấn luyện phân rã bất định vị trí để cân bằng điểm attention trên ngữ cảnh đầu vào. Cụ thể, chúng tôi thiết kế một tác vụ Multi-doc QA được điều chỉnh trong đó các tài liệu tích cực được đặt ở các vị trí tùy ý trong ngữ cảnh giữa các tài liệu nhiễu. Tác vụ này đưa ra một thách thức đáng kể, buộc các mô hình phải trích xuất và tóm tắt thông tin bất chấp sự can thiệp của những thông tin vô dụng (Ye et al., 2022). Vì con người thường xuyên giải quyết các tác vụ phức tạp bằng phân rã để có được kết quả chất lượng cao hơn (Cheng et al., 2015; Correa et al., 2023), chúng tôi chỉnh sửa tác vụ Multi-doc QA thành một tác vụ lý luận đa bước, được gọi là Position-Agnostic Multi-step QA (PAM QA), kết hợp Chain-of-Thought (COT, Wei et al.) và Multi-doc QA bất định vị trí. Được huấn luyện với việc trích xuất rõ ràng câu hỏi và chỉ số của các tài liệu hỗ trợ trước khi tạo câu trả lời, các mô hình học cách phân biệt thông tin chính xác khỏi những thông tin nhiễu và chú ý đến chúng. Nó cũng buộc attention đến câu hỏi và các chỉ số hỗ trợ mạnh hơn mặc dù quy mô attention giảm với khoảng cách tăng (Su et al., 2021). Kết quả thực nghiệm trên Multi-doc QA và các tiêu chuẩn khác cho thấy rằng, chỉ với kích thước cửa sổ ngữ cảnh 1/2 hoặc 1/4, mô hình của chúng tôi cải thiện so với các mô hình tiên tiến (SOTA) 7.0% trong cài đặt xếp hạng cao nhất và 13.7% trong cài đặt xáo trộn. Kết quả cạnh tranh được hiển thị trong các tác vụ phụ thuộc attention khác bao gồm truy xuất đoạn văn và tóm tắt.

Đóng góp của bài báo này gồm ba phần:

• Bài báo này đề xuất một tác vụ mới có tên PAM QA để giải quyết vấn đề "lạc giữa đường", điều này có thể gây chết người cho các tình huống tập trung vào kiến thức. Theo hiểu biết của chúng tôi, đây là nỗ lực đầu tiên giải quyết vấn đề bằng cách huấn luyện các mô hình trên các tác vụ đặc biệt.

• Chúng tôi điều tra hành vi của mô hình một cách sâu sắc, tiết lộ rằng việc thất bại trong tập trung vào thông tin mục tiêu có thể là nguyên nhân của "lạc giữa đường".

• Các thí nghiệm toàn diện đã cho thấy rằng PAM QA được đề xuất có hiệu quả trong việc giải quyết vấn đề "lạc giữa đường". Mô hình của chúng tôi vượt qua SOTA trong Multi-doc QA và các tác vụ liên quan khác trên các tiêu chuẩn tiếng Trung nổi tiếng. Điều đáng chú ý là khả năng QA tổng quát của mô hình cũng mạnh và thỏa mãn. Mô hình được mã nguồn mở để thúc đẩy nghiên cứu tương lai trong cộng đồng.

## 2 Position-Agnostic Multi-step QA

Multi-doc QA đề cập đến một loại tác vụ QA trong đó một mô hình được trình bày với nhiều tài liệu và được yêu cầu trả lời câu hỏi một cách chính xác. Nó khó khăn cho cả mô hình và con người, đòi hỏi truy xuất chính xác, tổng hợp thông tin và hiểu từ các ứng cử viên nhiễu trong khi đấu tranh với bộ nhớ mờ dần.

Trong tình huống này, phân rã tác vụ, xác định các vấn đề phụ và lý luận về chúng, trở nên thiết yếu (Correa et al., 2023). Chúng tôi phân rã Multi-doc QA khó khăn thành PAM QA. Tác vụ sáng tạo này bao gồm ba bước, như được mô tả trong Hình 1.

Toàn bộ quá trình của PAM QA diễn ra như sau: khi nhận được một câu hỏi, một tập hợp các tài liệu ứng cử viên, và một hướng dẫn cụ thể, mô hình bắt đầu bằng việc tạo tiền tố 1. Sau đó nó tiến hành phát biểu lại câu hỏi, dự đoán các chỉ số của bằng chứng liên quan sau khi kết hợp một cụm từ kết nối, được ký hiệu là tiền tố 2. Cuối cùng, nó công thức hóa một câu trả lời bằng cách tổng hợp thông tin trước đó, theo sau một chỉ báo câu trả lời, tiền tố 3.

### 2.1 Lặp lại câu hỏi

Bước đầu tiên là lặp lại câu hỏi (QR). Các câu hỏi được đặt ở phía trước như một biểu diễn nhận thức ngữ cảnh (Liu et al.). Tác vụ phụ được bắt đầu với tiền tố 1, "Đối với câu hỏi:" (hoặc các biểu thức có ý nghĩa giống hệt) để nhắc nhở mô hình.

### 2.2 Dự đoán chỉ số

Bằng chứng hỗ trợ không chỉ giúp LLMs tự xác minh mà còn hỗ trợ người dùng trong việc đánh giá phản hồi (Menick et al., 2022). Kết quả đáng chú ý đã được hiển thị trong việc tạo ra trích dẫn và tài liệu tham khảo (Thoppilan et al., 2022; Menick et al., 2022). Chúng tôi giả định rằng chỉ báo giúp mã hóa và điều hướng attention đến các tài liệu tương ứng. Theo đó, bước thứ hai là dự đoán chỉ số (IP), tức là dự đoán các chỉ số của các tài liệu được hỗ trợ cho câu hỏi như một tác vụ MRC, bắt đầu với tiền tố 2: "Dựa trên thông tin được đánh số". Không giống như các công trình trước đó dự đoán một trích dẫn từng từ được trích xuất từ một nguồn dài hơn được truy xuất, các chỉ số của bằng chứng tương ứng là các mục tiêu. Đối với trường hợp trong Hình 1, nhãn của bước này là "Dựa trên thông tin được đánh số [1],[3]". Xem xét các chỉ số trong bước thứ hai chỉ chiếm rất ít token và khó nhấn mạnh trong mất mát cross-entropy tuần tự trong quá trình huấn luyện, một tác vụ MRC chỉ yêu cầu dự đoán các chỉ số của các tài liệu chính xác được thêm vào như một bổ sung.

### 2.3 Tóm tắt câu trả lời

Bước thứ ba là tạo ra câu trả lời cuối cùng sau khi tổng hợp thông tin. Nhờ các bước trên, nó có thể được đơn giản hóa thành một tác vụ tóm tắt câu trả lời (AS). Bước này bắt đầu với một chỉ báo như "câu trả lời của tôi là" như tiền tố 3.

Phù hợp với câu tục ngữ "mực nhạt nhất tốt hơn trí nhớ tốt nhất," chúng tôi dạy mô hình ghi chú, biến những chú thích này thành đường cao tốc đến kiến thức liên quan. Nó có thể giảm sự phân tâm của thông tin ngoại lai và làm cho attention đến câu hỏi và chỉ số hỗ trợ mạnh hơn vì quy mô attention giảm với khoảng cách tăng.

## 3 Xây dựng Dữ liệu Huấn luyện

Chúng tôi trang bị cho mô hình của mình khả năng phân biệt thông qua điều chỉnh hướng dẫn. Quy trình huấn luyện bao gồm hai giai đoạn. Chúng tôi mở rộng cửa sổ ngữ cảnh của LLM lên 8K trong giai đoạn đầu tiên. Trong giai đoạn thứ hai, mô hình được huấn luyện với dữ liệu PAM QA để giải quyết thất bại attention (hoặc bộ nhớ) được gọi là "lạc giữa đường".

### 3.1 Mở rộng Cửa sổ Ngữ cảnh

Chúng tôi sử dụng khoảng 300k dữ liệu được chọn cho điều chỉnh giám sát tổng quát (SFT). Dữ liệu bao gồm các loại tác vụ khác nhau bao gồm QA, MRC, nhập vai, viết, mã hóa, dịch thuật, động não, toán học, Mô hình hóa Ngôn ngữ (LM), và các tác vụ hiểu ngôn ngữ tự nhiên (NLU) khác như phân loại văn bản. Dữ liệu được đóng gói thành kích thước cửa sổ 8K theo kiểu cuộc trò chuyện đa lượt trừ tác vụ LM, tính toán mất mát cross-entropy trên toàn bộ chuỗi.

### 3.2 PAM QA

Dữ liệu được xây dựng bằng cách định dạng đầu vào và nối các đầu ra mục tiêu của các bước trong PAM QA. Đầu tiên chúng tôi tạo dữ liệu Multi-doc QA và thích ứng nó thành dữ liệu PAM QA.

Đầu tiên, chúng tôi lọc ra 30K mẫu của danh mục Fact với một câu trả lời duy nhất từ tập dữ liệu DuReader2.0 (He et al., 2018) và 20K mẫu từ WebCPM (Qin et al., 2023). DuReader2.0 là tập dữ liệu MRC tiếng Trung lớn nhất được thu thập từ các tài liệu Web và QA cộng đồng, chứa 200K câu hỏi, 420K câu trả lời và 1M tài liệu. Để đảm bảo chất lượng dữ liệu, chúng tôi sáng tạo sử dụng một mô hình phần thưởng để chấm điểm các mẫu và chọn phần chất lượng cao của chúng với ngưỡng, được lấy cảm hứng từ Li et al.. Mô hình phần thưởng được huấn luyện với 69K mẫu được xếp hạng bởi con người để căn chỉnh trong các tác vụ tổng quát, theo Köpf et al. và Ouyang et al.. Vì cả hai tập dữ liệu chỉ chứa các mẫu tích cực, các mẫu tiêu cực được tạo ra một cách khéo léo sau đó.

Vì học tập hợp tác có lợi cho RAG (Izacard et al., 2022), chúng tôi xây dựng một công cụ tìm kiếm với tất cả các tài liệu trong tập dữ liệu tương ứng. Đối với mỗi mẫu, các tài liệu trong toàn bộ bộ sưu tập trừ những tài liệu tích cực được coi là các mẫu tiêu cực. Chúng tôi truy xuất các tài liệu từ công cụ tìm kiếm như các ứng cử viên tiêu cực cho một phân vùng bao gồm 70% dữ liệu, trong khi chúng tôi lấy mẫu ngẫu nhiên từ các ứng cử viên tiêu cực ban đầu cho phần còn lại của dữ liệu. Các mẫu tiêu cực được truy xuất có liên quan hơn đến câu hỏi và khó phân biệt hơn với những mẫu tích cực so với các mẫu ngẫu nhiên. Tiếp theo, các tài liệu được xáo trộn trong mỗi mẫu trong 50% dữ liệu để ngăn những tài liệu tích cực liên tục ở đầu ngữ cảnh. Tiếp theo, 25K mẫu được lấy mẫu từ các tiêu chuẩn truy xuất, T2Rank (Xie et al., 2023) như MRC liên quan, một bổ sung cho tác vụ 2. Các mẫu tiêu cực được lấy mẫu ngẫu nhiên từ các bộ sưu tập tiêu cực khó và xáo trộn với các ứng cử viên tích cực. Các chỉ số của các tài liệu tích cực được ghi lại.

Độ dài tối đa của mỗi mẫu được lấy mẫu từ 1K đến 8K dưới phân phối đều. Điều này đảm bảo mô hình của chúng tôi có thể xử lý các mẫu với độ dài đầu vào khác nhau với các tài liệu chính xác được đặt ở bất kỳ vị trí nào.

Để cho phép mô hình nhận ra các tình huống mà tài liệu chính xác vắng mặt, chúng tôi tạo ra các mẫu "Synthetic Unknown", trong đó tất cả các tài liệu đều tiêu cực. Câu trả lời cho những mẫu này là một thuật ngữ hằng số chỉ ra "Tôi không biết." Loại dữ liệu này chiếm tỷ lệ 5%.

Cuối cùng, Chúng tôi lấy mẫu một số dữ liệu SFT tổng quát, chiếm tỷ lệ 20% trong giai đoạn này để giảm thiểu quên thảm khốc (McCloskey and Cohen, 1989; Rebuffi et al., 2017). Tổng số mẫu huấn luyện trong giai đoạn này lên đến 90K.

### 3.3 Huấn luyện

Chúng tôi huấn luyện mô hình của mình dựa trên một LLM được tiền huấn luyện được thích ứng từ LLaMA2, được gọi là Ziya2-13B-Base (Touvron et al., 2023; Gan et al., 2023; Zhang et al., 2023). Chúng tôi huấn luyện trong 2 epoch trên 16 GPU A100 trong cả hai giai đoạn với dữ liệu được xây dựng. Tốc độ học bắt đầu với 1e-5 sau đó giảm xuống 1e-6 với warmup cho 0.05% bước đầu tiên trong giai đoạn đầu tiên. Tốc độ học tối đa cho giai đoạn thứ hai là 5e-6. Flash Attention (Dao et al., 2022) được sử dụng để tăng tốc quy trình huấn luyện. Lấy mẫu được bật cho tất cả các mô hình trong quá trình kiểm tra trong các tiêu chuẩn. Các siêu tham số cho kiểm tra được liệt kê trong Phụ lục A.

## 4 Thí nghiệm

Trong phần này, chúng tôi đánh giá khả năng QA ngữ cảnh dài của mô hình của chúng tôi và các LLMs đại diện hiện có. Bằng cách kiểm tra hiệu suất, chúng tôi có thể xác minh liệu mô hình của chúng tôi có khắc phục được vấn đề "lạc giữa đường" được gọi là (Liu et al.) hay không.

### 4.1 Tiêu chuẩn

Chúng tôi tiến hành thí nghiệm trên một tiêu chuẩn ngữ cảnh dài, LongBench (Bai et al., 2023) và Retrieval-Augmented Generation Benchmark (RGB, Chen et al.). LongBench đo lường các khả năng khác nhau của người được kiểm tra với các ngữ cảnh đầu vào dài. Cụ thể, chúng tôi kiểm tra các mô hình trên bốn tác vụ liên quan trong LongBench: Chinese Multi-doc QA, Synthetic tasks, summarization và single-doc QA. Chúng tôi cũng sử dụng testbed độ bền nhiễu trong RGB để kiểm tra khả năng QA trong văn bản ngắn, kiểm tra khả năng trích xuất thông tin với tỷ lệ nhất định các tài liệu nhiễu.

Tác vụ synthetic là một tác vụ truy xuất tài liệu, trong đó cho một bản tóm tắt, mục tiêu là tìm tài liệu tương ứng từ một số lượng lớn ứng cử viên. Tác vụ này đánh giá khả năng truy xuất thông tin của LLMs trong ngữ cảnh dài. Tác vụ tóm tắt đưa ra các bản ghi cuộc họp cực dài từ nhiều người nói và yêu cầu một bản tóm tắt. Nó đánh giá khả năng bộ nhớ và tóm tắt của mô hình. Single-doc QA là một tác vụ QA ngữ cảnh dài ít tương tự với multi-doc QA hơn. Chúng tôi tiến hành thí nghiệm trên tác vụ này để kiểm tra tính robust của mô hình. Độ dài ngữ cảnh và các thống kê khác của các tập dữ liệu được liệt kê trong Bảng 1. Các script đánh giá được cung cấp bởi trang web chính thức LongBench và kho chính thức RGB.

Chúng tôi cũng tái xây dựng tác vụ synthetic để kiểm tra liệu các mô hình có "lạc giữa đường" hay không. Các đoạn văn chính xác được đặt lại ở vị trí thứ 1, 5, 10, 15 và 20 với các đoạn văn được đặt ngoài vị trí thứ 20 bị loại bỏ. Kết quả trong Hình 2.

Xem xét rằng các tài liệu trong các mẫu của các tác vụ Multi-doc QA về cơ bản được sắp xếp theo mức độ liên quan, chúng tôi xáo trộn 10 tài liệu ứng cử viên đầu tiên trong mỗi mẫu để làm lộ hiệu suất thực tế, được gọi là Multi-doc QA shuffled.

Ngoài ra, chúng tôi tiến hành đánh giá con người toàn diện về khả năng mô hình để xem liệu việc huấn luyện trên PAM QA có làm tổn hại đến khả năng tổng quát của LLM hay không. Tập kiểm tra chứa 200 câu hỏi từ một loạt các danh mục rộng.

### 4.2 Đường cơ sở

Chúng tôi so sánh hiệu suất của các LLMs phổ biến nhất với cửa sổ ngữ cảnh dài. Những đường cơ sở mạnh này bao gồm: GPT3.5T-Turbo-16K mở rộng cửa sổ ngữ cảnh đến 16K token, trong khi cả Longchat-v1.5-7B-32K (Dacheng et al., 2023) và ChatGLM2(3)-6B-32K (Du et al., 2022) đẩy ranh giới xa hơn đến 32K token. Vicuna-v1.5-7B-16K (Zheng et al., 2023) và Xgen-7B-8K (Nijkamp et al., 2023) cung cấp các mô hình được tinh chỉnh trên các cuộc trò chuyện được chia sẻ bởi người dùng và chuỗi 8K tương ứng. Baichuan2-13B-Chat (Yang et al., 2023) nổi bật trong học few-shot với cửa sổ token 4K, cùng với một biến thể nguồn đóng lớn hơn. Cuối cùng, Qwen-14B-Chat giới thiệu một mô hình tham số 14B với NTK động (dyn, 2023), được huấn luyện trên kích thước cửa sổ lên đến 8K token.

Chúng tôi đề cập đến các mô hình tăng cường truy xuất như những mô hình được huấn luyện với dữ liệu hoặc mô hình tăng cường truy xuất. Baichuan2-13B-Chat và Baichuan2-Turbo-192k đều là các mô hình tăng cường truy xuất (Yang et al., 2023).

## 5 Kết quả và Thảo luận

Trong phần này, chúng tôi phân tích kết quả thực nghiệm của các LLMs và thảo luận lý do của các phát hiện. Một nghiên cứu ablation cũng được tiến hành để phân tích sâu. Các chi tiết khác trong Phụ lục.

### 5.1 Kích thước cửa sổ dài hơn không đảm bảo hiệu suất tốt hơn

Như được hiển thị trong Bảng 2, mô hình của chúng tôi có Rouge-L 44.6% trong tác vụ Multi-doc QA, cao hơn 7.0% so với ChatGLM2-6B-32K, là mô hình SOTA. Chỉ với kích thước cửa sổ 1/4, mô hình của chúng tôi có thể vượt trội ChatGLM2-6B-32K trong tác vụ này. Nó tiết lộ khả năng attention mạnh của mô hình chúng tôi vì đây là một tác vụ QA mở sách. Tập dữ liệu Chinese Multi-doc QA này không cần xem xét tất cả các ngữ cảnh, vì các tài liệu chính xác được đặt ở đầu ngữ cảnh.

Trong Synthetic Task, tức là một tác vụ truy xuất trừu tượng, mô hình của chúng tôi đạt được kết quả cao nhất với độ chính xác 98.5%, trong số các mô hình có khả năng ngữ cảnh dài hơn. Điều này chỉ ra rằng vấn đề "lạc giữa đường" gần như được giải quyết bởi phương pháp được đề xuất trong bài báo này, miễn là độ dài trung bình được bao phủ.

Về tóm tắt, ChatGLM2-6B-32K và GPT3.5-Turbo-16K có hiệu suất tương tự với kích thước cửa sổ ngữ cảnh khác nhau, cho thấy rằng kích thước cửa sổ ngữ cảnh dài hơn không đảm bảo hiệu suất tốt hơn. Rouge-L của mô hình chúng tôi chỉ thấp hơn SOTA 0.5%, mà không có bất kỳ dữ liệu tóm tắt nào trong huấn luyện PAM QA. Vì độ dài trung bình của tác vụ dài hơn nhiều so với 8K token, mô hình của chúng tôi với độ dài ngữ cảnh dài hơn sẽ có cải thiện đầy hứa hẹn.

Chúng tôi quan sát kết quả vừa phải trong Single-doc QA từ mô hình của chúng tôi và thấy nó cạnh tranh trong số các mô hình 8K. GPT3.5-Turbo-16K đạt kết quả cao nhất 61.2% điểm F1 (trước ChatGLM3-6B-32K), vượt qua mô hình dài nhất, Baichuan2-Turbo-192K.

### 5.2 PAM QA giảm thiểu vấn đề lạc giữa đường (và cuối)

Thí nghiệm trên tiêu chuẩn Synthetic (passage retrieval) được tái xây dựng hiển thị hiệu suất của các mô hình trên các vị trí tài liệu tích cực khác nhau. Cụ thể, đoạn văn chính xác cho mỗi mẫu được chèn vào vị trí thứ 1, 5, 10, 15 và 20 tương ứng giữa các tài liệu khác trong mỗi thí nghiệm. Về mặt lý thuyết, chúng ta nên thấy một đường cong U được mô tả trong (Liu et al.), được gọi là "lạc giữa đường". Kết quả được hiển thị trong Hình 2.

Tuy nhiên, hình này gợi ý rằng hầu hết các LLMs nguồn mở bị lạc không chỉ ở giữa mà còn ở cuối. Một sự giảm đáng kể được quan sát khi các tài liệu tích cực được đặt ở vị trí thứ 10. Bất chấp việc sử dụng các kỹ thuật như Alibi (Press et al., 2022) hoặc NTK để mở rộng cửa sổ ngữ cảnh (tức là Baichuan2-13B-Chat và Qwen-14B-Chat), các mô hình vẫn thể hiện kết quả thấp. Ngược lại, mô hình của chúng tôi có thể tồn tại trong các cài đặt vị trí khác nhau, giữ kỷ lục 99%. Nó tiết lộ hiệu quả của huấn luyện PAM QA.

### 5.3 Các mô hình bị đánh bại bởi tập dữ liệu xáo trộn, thất bại attention là thủ phạm

Hình 4 minh họa hiệu suất của các mô hình trên Multi-doc QA trước và sau khi xáo trộn. Chúng ta có thể thấy một sự sụt giảm mạnh trong tất cả ba mô hình trừ mô hình của chúng tôi. Khoảng cách lớn nhất đạt 17.3%, từ ChatGLM2-6B-32K. Trong khi đó, Baichuan2-13B-Chat cũng có sự giảm 7% mặc dù các tài liệu xáo trộn nằm trong độ dài ngữ cảnh của nó. Do đó, LLMs không có cửa sổ ngữ cảnh cực dài cũng gặp khó khăn trong việc đối phó với thách thức. Mô hình của chúng tôi là mô hình mạnh nhất với sự giảm 3.7%.

Để khám phá nguyên nhân của sự sụt giảm và kiểm tra khả năng attention của các mô hình, chúng tôi trực quan hóa điểm attention của lớp cuối cùng cho đầu vào giống hệt nhau. Chúng tôi lặp lại một câu bao gồm câu trả lời chính xác 20 lần như ngữ cảnh để tìm hiểu xem liệu tất cả chúng có được làm nổi bật trong quy trình self-attention trong các mô hình hay không. Điểm attention của ChatGLM2-6B-32K và Mô hình của chúng tôi trên đầu vào được mô tả trong Hình 3.

Chúng ta có thể thấy điểm attention trên các tài liệu đang mờ dần trong ChatGLM2-6B-32K, vì ngữ cảnh sau 100 token đầu tiên gần như bị bỏ qua. Tình huống khá khác biệt khi nói đến mô hình của chúng tôi. 20 đỉnh của điểm attention được quan sát (cái cuối cùng nằm cạnh đầu hướng dẫn), tương ứng với các câu trả lời trong câu. Nó tiết lộ rằng attention đến các token liên quan là chìa khóa cho khoảng cách hiệu suất giữa các mô hình. Các mô hình đấu tranh để tập trung chính xác vào các token chính xác, chú ý quá mức đến các token đầu và cuối (nơi hướng dẫn và truy vấn thường xuyên được đặt), đây là thủ phạm của vấn đề "lạc giữa đường".

### 5.4 Kết quả cạnh tranh được quan sát trong Multi-doc QA văn bản ngắn

Như được báo cáo trong Bảng 3, mô hình của chúng tôi có hiệu suất cạnh tranh trong số các mô hình nguồn mở trên multi-doc QA văn bản ngắn mặc dù không được huấn luyện với bất kỳ văn bản ngắn nào. Ngay cả so với các LLMs tiếng Trung phổ biến mới nhất, Qwen-14B-Chat và ChatGLM3-6B-32K, kết quả của mô hình chúng tôi cao hơn dưới cài đặt tỷ lệ nhiễu trong [0,0.4,0.6].

### 5.5 Khả năng tổng quát được bảo tồn với Huấn luyện PAM QA

Một so sánh song song (SBS) được thực hiện bởi 3 người chú thích con người để kiểm tra khả năng tổng quát của mô hình chúng tôi. Các khả năng tổng quát bao gồm lẽ thường, toán học, lý luận, QA, viết, vô hại, v.v. được kiểm tra trong bài kiểm tra, như được hiển thị trong Hình 5. Các người chú thích được yêu cầu chọn một câu trả lời tốt hơn trong số hai câu trả lời đã cho trừ khi các câu trả lời đều tệ hoặc giống nhau, như trong (Zheng et al., 2023). Các người chú thích đều là sinh viên thạc sĩ. Họ không biết về các mô hình và thông tin khác. Kết quả so sánh với các mô hình kích thước tương tự, Ziya-LLaMa-13B-v1.1 và Baichuan2-13B-Chat tương ứng được minh họa trong Hình 6. Chúng tôi cũng so sánh mô hình của mình với cùng mô hình cơ sở sau huấn luyện SFT đầy đủ, Ziya2-13B-SFT.

Hình 6 tóm tắt sở thích con người giữa mô hình của chúng tôi và các LLMs nguồn mở khác. Mặc dù mô hình của chúng tôi chỉ được huấn luyện với dữ liệu PAM QA, nó hoạt động hơi kém hơn Baichuan2-13B-Chat nhưng tốt hơn Ziya-LLaMa-13B-v1.1 và Ziya2-13B-SFT một cách đáng kể. Do đó, các khả năng tổng quát được duy trì sau huấn luyện PAM QA.

### 5.6 Nghiên cứu Ablation

Mỗi bước trong PAM QA đều quan trọng. Ở đây chúng tôi kiểm tra đóng góp của mỗi bước trong PAM QA. Các biến thể được đánh giá trên Multi-doc QA và Synthetic tasks. Kết quả của nghiên cứu ablation này được liệt kê trong Bảng 4.

Không có lặp lại câu hỏi, bước đầu tiên trong PAM QA, có thể quan sát thấy sự giảm 5.8% trong Multi-doc QA, cho thấy đóng góp không thể tránh khỏi của nó đối với hiệu suất cao. Nó tăng cường attention của câu hỏi bằng cách lặp lại câu hỏi trước. Sau đó mô hình có thể trực tiếp chú ý đến câu hỏi trong các bước tiếp theo mà không cần đi qua ngữ cảnh dài, giảm sự phân tâm của ngữ cảnh khi thực hiện self-attention.

Khi bước dự đoán chỉ số (IP) bị loại bỏ, sự giảm rõ rệt trong Synthetic tasks nhấn mạnh tầm quan trọng của nó. Nó không chỉ dạy LLMs phân biệt giữa thông tin liên quan và không liên quan mà còn thay đổi hành vi trước đó của mô hình (tức là tìm kiếm thông tin từ đầu và cuối ngữ cảnh). Trong khi đó, nó đơn giản hóa quá trình bằng cách cho phép các mô hình tập trung vào thông tin trừu tượng liên quan, thay vì quét lặp đi lặp lại các token đầu vào rộng lớn. Một sự sụt giảm hiệu suất trong multi-doc QA cũng cho thấy đóng góp của IP. Vì quy mô của điểm attention giảm khi khoảng cách tăng (Su et al., 2021), các mô hình với rotary position embeddings (RoPE) đấu tranh để nhớ các token xa mà không cần huấn luyện. Với hai bước trước đó, câu hỏi và bằng chứng có thể chính xác được liệt kê chỉ một vài token phía trước. Điều này giảm xác suất quên câu hỏi và ngữ cảnh bằng cách giảm khoảng cách.

Một khoảng cách khổng lồ giữa kết quả của Mô hình của chúng tôi và mô hình không có QR và IP, chỉ ra sự cải thiện đáng kể từ huấn luyện PAM QA. Chúng tôi trực quan hóa điểm attention khi dự đoán token đầu tiên và khám phá các câu hỏi và chỉ số được tạo ra được làm nổi bật, được hiển thị trong Hình 7 trong Phụ lục D.

So với Only-CWE, mô hình biến thể không có bước QR và IP cũng đạt được cải thiện 29.1%, cho thấy hiệu quả của các ứng cử viên tiêu cực bất định vị trí và thách thức. Bằng cách biến đổi Multi-doc QA thành PAM QA, cùng dữ liệu có thể thúc đẩy hiệu suất 6.8% trong Multi-doc QA, và 97.2% trong Synthetic Tasks, tiết lộ sức mạnh của huấn luyện phân rã tác vụ.

Sự cần thiết của Huấn luyện. Để điều tra liệu huấn luyện (tinh chỉnh) có cần thiết hay không, chúng tôi thực hiện nhắc nhở COT đa bước theo kiểu "trước tiên dự đoán các chỉ số của các tài liệu liên quan" và "theo thông tin, câu trả lời cuối cùng là" trên các mô hình khác nhau. Chúng tôi loại bỏ bước lặp lại câu hỏi trong COT để có hiệu suất tốt hơn. Kết quả trong Bảng 5.

Như được minh chứng bởi kết quả vượt trội của các mô hình chúng tôi so với suy luận COT đa bước, huấn luyện là thiết yếu để tối ưu hóa hiệu suất. Đặc biệt trong Multi-doc QA, LLMs có xu hướng tạo ra một câu trả lời với nhiều ảo giác hơn sau khi dự đoán một danh sách các chỉ số của các tài liệu liên quan mà không cần tinh chỉnh. Chúng tôi thấy các mô hình với COT đa bước đấu tranh để xử lý các hướng dẫn phức tạp và duy trì bộ nhớ ngữ cảnh dài.

Khả năng Tổng quát của Phương pháp. Để minh họa khả năng tổng quát của phương pháp, chúng tôi cũng thực hiện huấn luyện giống hệt trên một mô hình tiền huấn luyện tiếng Trung khác, Yi-34B-Base, một mô hình tiền huấn luyện 34B hàng đầu trong LLM Benchmarks. Bảng 6 liệt kê so sánh của mô hình chúng tôi đã huấn luyện (Yi-34B-Reader) và phiên bản điều chỉnh hướng dẫn chính thức, Yi-34B-Chat.

Kết quả cho thấy phương pháp của chúng tôi có thể được tổng quát hóa cho các LLMs khác. Tuy nhiên, khó hơn để thay đổi hành vi của Yi-34B-Base chỉ sử dụng 100K dữ liệu PAM QA so với mô hình 13B vì nó đã được tiền huấn luyện khá (có thể quá) đầy đủ. Nó dẫn đến kết quả thấp hơn trong tác vụ synthetic so với Ziya2-Reader 13B.

## 6 Công trình Liên quan

### 6.1 Mô hình Ngôn ngữ Tăng cường Truy xuất

Các Mô hình Ngôn ngữ Tăng cường Truy xuất (RALMs) đánh dấu tiến bộ đáng chú ý trong NLP bằng cách hợp nhất các khả năng của LMs mở rộng với độ chính xác và sự phức tạp được cung cấp bởi các nguồn kiến thức bên ngoài. (Guu et al., 2020; Lewis et al., 2020a; Izacard et al., 2022). Những mô hình này sử dụng một retriever để tìm kiếm qua một khối lượng lớn bằng chứng, như Wikipedia, để tìm các tài liệu cụ thể liên quan đến truy vấn của người dùng. Sau đó, một thành phần reader được sử dụng để kiểm tra cẩn thận những tài liệu này và tạo ra một phản hồi. Quá trình hai bước này đảm bảo cả sự liên quan và độ sâu trong các câu trả lời được tạo ra. Các nỗ lực nghiên cứu gần đây đã tập trung vào việc tăng cường hiệu suất của retriever (Karpukhin et al., 2020; Sachan et al., 2023) hoặc reader (Izacard and Grave, 2020; Cheng et al., 2021), huấn luyện hệ thống end-to-end (Lewis et al., 2020a; Sachan et al., 2021), và tích hợp các hệ thống truy xuất với các mô hình ngôn ngữ lớn hộp đen (Shi et al., 2023b; Yu et al., 2023; Trivedi et al., 2023)

### 6.2 RALMs Thích ứng với Ngữ cảnh Dài và Nhiễu

Nghiên cứu gần đây nhấn mạnh ảnh hưởng của độ dài ngữ cảnh và vị trí của ngữ cảnh liên quan đến hiệu suất của LLMs (Krishna et al., 2023; Bai et al., 2023; Liu et al.). Nghiên cứu gắn liền chặt chẽ với nghiên cứu của chúng tôi là nghiên cứu của (Yoran et al., 2023), huấn luyện RALMs để bỏ qua các ngữ cảnh không liên quan. Một phương pháp huấn luyện giống COT đồng cấu được đề xuất để giải quyết các vấn đề toán học và mã hóa, phát ra các bước tính toán trung gian vào một "scratchpad" (Nye et al., 2021). Tuy nhiên, họ đã bỏ qua các tình huống ngữ cảnh dài, cụ thể là vấn đề "lạc giữa đường", một cân nhắc quan trọng trong công trình của chúng tôi.

Một công trình trước đó xem xét mô hình hóa multi-doc trong huấn luyện được đề xuất bởi Caciularu et al.. Sau khi chia ngữ cảnh dài thành các mảnh và tạo ra các cặp QA dựa trên những mảnh nổi bật được chọn, họ yêu cầu các mô hình dự đoán các câu nổi bật bị che và câu trả lời, với các mảnh khác và các câu hỏi được tạo ra. Cải thiện đáng kể trong các tiêu chuẩn QA đa bước sau tinh chỉnh với tập huấn luyện được báo cáo với chi phí đắt đỏ của tiền huấn luyện. Tuy nhiên, không có tập huấn luyện trong hầu hết các tiêu chuẩn ngày nay và nó thất bại trong việc thực hiện các tác vụ đa dạng trong cài đặt zero-shot.

## 7 Kết luận

Trong bài báo này, chúng tôi giả định rằng hiện tượng "lạc giữa đường" được công nhận rộng rãi có thể do attention yếu đến thông tin mục tiêu. Chúng tôi khám phá các LLMs tiếng Trung phổ biến bị "lạc" cả ở giữa và cuối. Một phương pháp mới được đề xuất để giải quyết sự thiếu hụt trong LLMs bằng cách huấn luyện các mô hình với Position-Agnostic Multi-step (PAM) QA. Kết quả thực nghiệm cho thấy sự vượt trội và hiệu quả của phương pháp chúng tôi, vượt qua các LLMs SOTA trong Multi-doc QA và truy xuất đoạn văn một cách đáng kể, chỉ với kích thước cửa sổ ngữ cảnh 1/4. Bằng cách xáo trộn các tài liệu ứng cử viên trong các tiêu chuẩn mở, hiệu suất suy giảm được quan sát trong tất cả các mô hình, trong đó mô hình của chúng tôi là mô hình mạnh nhất. Nghiên cứu ablation cũng tiết lộ hiệu quả đáng kể của PAM QA và đóng góp tích cực của các thành phần của nó. Nghiên cứu của chúng tôi cũng thấy rằng LMs với cửa sổ ngữ cảnh cực dài không đảm bảo hiệu suất tốt hơn trong các tác vụ Multi-doc QA và truy xuất đoạn văn. Chúng tôi hy vọng nghiên cứu của mình cung cấp cái nhìn sâu sắc về vấn đề "lạc giữa đường" và làm sáng tỏ việc phát triển các LLMs thông minh hơn.

## Hạn chế

Công trình của chúng tôi bao gồm vấn đề quan trọng "lạc giữa đường" và thí nghiệm với Tiêu chuẩn tiếng Trung trên các LLMs tiếng Trung và tiếng Anh phổ biến với khả năng ngữ cảnh dài. Những cải thiện trong các tác vụ được kiểm tra không ngụ ý những nâng cấp tương tự trong tất cả các khía cạnh, như toán học và lý luận. Dữ liệu PAM QA được xây dựng được sử dụng sau hoặc trong SFT, với hiệu quả trong giai đoạn tiền huấn luyện và RLHF chưa được khám phá.

Phương pháp xây dựng dữ liệu chủ yếu dựa trên multi-doc QA và cho thấy lợi ích đáng kể trong các tác vụ liên quan. Cải thiện trong các tác vụ ngữ cảnh dài khác không ấn tượng như multi-doc QA và synthetic tasks vì các khả năng khác được yêu cầu nhiều hơn là phân biệt và tập trung. Những tình huống đó không được xem xét trong bài báo này.

Phương pháp được đề xuất là độc lập với ngôn ngữ và có thể được áp dụng cho các tập dữ liệu của các ngôn ngữ khác. Cốt lõi của phương pháp nằm trong việc xây dựng các mẫu với các tài liệu tiêu cực liên quan thách thức, các vị trí đa dạng của các mẫu tích cực và các câu trả lời lý luận đa bước. Do đó, phương pháp có thể giảm thiểu vấn đề "lạc giữa đường" trong các ngôn ngữ khác.

## Tài liệu tham khảo

[Các tài liệu tham khảo được giữ nguyên như trong bản gốc]

## Phụ lục A Siêu tham số

Chúng tôi đã sử dụng các cài đặt sau trong giai đoạn suy luận trong bài kiểm tra LongBench: do_sample = True, topp = 0.85, temperature = 0.8, repetition_penalty = 1.0, early_stopping = True.

Đối với testbed độ bền nhiễu RGB, temperature được đặt thành 0.2.

## Phụ lục B Đánh giá Dự đoán Chỉ số

Chúng tôi áp dụng chú thích con người cho 140 mẫu trong Multi-doc QA để điều tra độ chính xác và recall của bước dự đoán chỉ số (IP). Độ chính xác macro là 43.0% và recall là 70.9%. Chúng xuất phát từ nhiều chỉ số được dự đoán hơn so với ground truths. Các LLMs có thể có khả năng phản chiếu để kiểm tra các chỉ số và cuối cùng tạo ra câu trả lời chính xác. Do đó, bước IP có lợi cho việc tạo câu trả lời ngay cả với độ chính xác thấp.

## Phụ lục C Minh họa PAM QA

Một trường hợp của Multi-doc QA và câu trả lời được tạo ra bởi Mô hình của chúng tôi và ChatGPT3.5 được hiển thị trong Bảng 7.

## Phụ lục D Trực quan hóa điểm attention trên các token trong bước tóm tắt câu trả lời

Bản đồ nhiệt của điểm attention của mỗi token khi tạo ra câu trả lời cuối cùng được hiển thị trong Hình 7.

## Phụ lục E Hệ thống Chú thích

Ảnh chụp màn hình của hệ thống chú thích trong đánh giá khả năng tổng quát SBS được hiển thị trong Hình E.
