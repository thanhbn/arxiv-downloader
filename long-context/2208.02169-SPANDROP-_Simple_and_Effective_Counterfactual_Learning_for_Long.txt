# 2208.02169.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2208.02169.pdf
# File size: 468079 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SPANDROP: Simple and Effective Counterfactual Learning for Long
Sequences
Peng Qiy1Guangtao Wangy2Jing Huangy3
1AWS AI, Seattle, WA2TikTok, Mountain View, CA3Alexa AI, Sunnyvale, CA
pengqi@cs.stanford.edu
Abstract
Distilling supervision signal from a long sequence to make
predictions is a challenging task in machine learning, espe-
cially when not all elements in the input sequence contribute
equally to the desired output. In this paper, we propose
SPANDROP, a simple and effective data augmentation tech-
nique that helps models identify the true supervision signal
in a long sequence with very few examples. By directly ma-
nipulating the input sequence, SPANDROPrandomly ablates
parts of the sequence at a time and ask the model to perform
the same task to emulate counterfactual learning and achieve
input attribution. Based on theoretical analysis of its proper-
ties, we also propose a variant of SPANDROP based on the
beta-Bernoulli distribution, which yields diverse augmented
sequences while providing a learning objective that is more
consistent with the original dataset. We demonstrate the
effectiveness of SPANDROP on a set of carefully designed
toy tasks, as well as various natural language processing
tasks that require reasoning over long sequences to arrive at
the correct answer, and show that it helps models improve
performance both when data is scarce and abundant.
1 Introduction
Building effective machine learning systems for long se-
quences is a challenging and important task, which helps us
better understand underlying patterns in naturally occurring
sequential data like long texts (Radford et al., 2019), pro-
tein sequences (Jumper et al., 2021), ﬁnancial time series
(Bao et al., 2017), etc. Recently, there is growing interest
in studying neural network models that can capture long-
range correlations in sequential data with high computa-
tional, memory, and statistical efﬁciency, especially widely
adopted Transformer models (Vaswani et al., 2017).
Previous work approach long-sequence learning in Trans-
formers largely by introducing computational approaches to
replace the attention mechanism with more efﬁcient coun-
terparts. These approaches include limiting the scope of the
*Equal contribution.
†Work done at JD AI Research.
Original InputWord MaskingSubstitutionSPANDROP①②LegendSequence ElementSupport-ingFactAll-0 / trainable representationRemoval CandidateFigure 1: SPANDROP compared to masking- and
substitution-based augmentation techniques ( e.g., word
dropout) commonly used in sequence learning. SPANDROP
can not only work with arbitrary granularities of input units,
but also introduce augmented sequences without artiﬁcial
tokens and alleviates potential positional bias in the data.
attention mechanism (Kitaev et al., 2019), limiting sequence-
level attention to only a handful of positions (Beltagy et al.,
2020; Zaheer et al., 2020), or borrowing ideas from the
kernel trick to eliminate the need to compute or instantiate
the costly attention matrix (Peng et al., 2020; Katharopou-
los et al., 2020; Choromanski et al., 2020). Essentially,
these approaches aim to approximate the original pair-wise
interaction with lower cost, and are often interested in cap-
turing the effect of every input element on the outcome ( e.g.,
arithmetic operations over a long list of numbers and oper-
ators, as proposed by Tay et al., 2020). While these tasks
present great challenges for designing effective models to
handle long sequences, many real-world problems involving
long sequences share properties that make them amenable
to more effective approaches than modeling the raw input-
output relationship directly ( e.g., via decomposition).
In this paper, we focus on a learning problem for long
sequences motivated by real-world tasks, where not all in-
put elements might contribute to the desired output. Nat-
ural examples that take this form include sentiment clas-
siﬁcation for long customer review documents (where a
few salient sentiment words and conjunctions contribute the
most), question answering from a long document (where
each question typically requires a small number of support-
ing sentences to answer), key phrase detection in audio pro-
cessing (where a small number of recorded frames actually
1arXiv:2208.02169v1  [cs.LG]  3 Aug 2022

--- PAGE 2 ---
determine the prediction), as well as detecting a speciﬁc ob-
ject from a complex scene (where, similarly, a small amount
of pixels determine the outcome), to name a few. In these
problems, it is usually counterproductive to try and make
direct use of the entire input if the contributing portion is
small or sparse, which results in a problem of underspeciﬁ-
cation (i.e., the training data does not sufﬁciently deﬁne the
goal for statistical models).
One approach to address this problem is to annotate the
parts of input that directly contribute to the outcome. This
could take the form of a subset of sentences that answer
a question or describe the relation between entities in a
paragraph (Yang et al., 2018; Yao et al., 2019). However,
such annotation is not always technically or ﬁnancially fea-
sible, so researchers and practitioners often need to resort
to either collecting more input-output pairs or designing
problem-speciﬁc data augmentation techniques to make up
for the data gap. For real-valued data, augmentation often
translates to random transformations ( e.g., shifting or ﬂip-
ping an image); for symbolic data like natural language,
techniques like masking or substitution are more commonly
used ( e.g., randomly swapping words with a special mask
token or other words). While these approaches have proven
effective in some tasks, each has limitations that prevents it
from being well-suited for the underspeciﬁcation scenario.
For instance, while global feature transformations enhance
group-invariance in learned representations, they do not di-
rectly help with better locating the underlying true stimulus.
On the other hand, while replacement techniques like mask-
ing and substitution help ablate parts of the input, they are
susceptible to the position bias of where the true stimulus
might occur in the input. Furthermore, while substitution
techniques can help create challenging contrastive examples,
they are often signiﬁcantly more difﬁcult to implement ( e.g.,
replacing a phrase in a sentence without losing ﬂuency).
To address these challenges, we propose SPANDROP,
a simple and effective technique that helps models distill
sparse supervision signal from long sequences when the
problem is underspeciﬁed. SPANDROP randomly ablates
parts of the input to construct counterfactual examples
that preserves the original output and supervision signal
with high probability. Unlike replacement-based techniques,
however, SPANDROP removes ablated elements from the
input and concatenate the remainder. This avoids intro-
ducing artiﬁcial representations that are not used at test
time, and mitigates potential spurious correlation to abso-
lute positions of outcome-determining parts of the input
(see Figure 1). We further propose a theoretically moti-
vated, more effective variant of SPANDROP based on the
beta-Bernoulli distribution that enhances the consistency
of the augmented objective function with the original one.
We demonstrate via carefully designed toy experiments
thatSPANDROP not only helps models achieve up to 20
sample-efﬁciency in low-data settings, but also further re-duces overﬁtting even when training data is abundant. We
ﬁnd that it is very effective at mitigating position bias com-
pared to replacement-based counterfactual approaches, and
enhances out-of-distribution generalization effectively. We
further experiment on four natural language processing tasks
that require models to answer question or extract entity rela-
tions from long texts, where, motivated by our theoretical
analysis, we further propose an adaptive span segmentation.
Our experiments demonstrate that SPANDROP can improve
the performance of competitive neural models without any
architectural change.
To summarize, our contributions in this paper are: 1)
we propose SPANDROPand Beta- SPANDROP, two effective
approaches to generate counterfactual examples for tasks
of long sequence learning; 2) we study the theoretical prop-
erties of these approaches and verify them with carefully
designed experiments on synthetic data; 3) we demonstrate
that both approaches improve upon strong neural models on
four NLP datasets, which can be furthered by a theoretically
motivated span segmentation approach we propose.
2 Method
In this section, we ﬁrst formulate the problem of sequence
inference, where the model takes sequential data as input
to make predictions. Then, we introduce SPANDROP, a
simple and effective data augmentation technique for long
sequence inference, and analyze its theoretical properties.
2.1 Problem Deﬁnition
Sequence Inference . We consider a task where a model
takes a sequence Sas input and predicts the output y. We
assume that S= (s1;:::;sn)consists ofndisjoint but
contiguous spans , and each span represents a part of the
sequence in order. One example of sequence inference is
sentiment classiﬁcation from a paragraph of text, where S
is the paragraph and ythe sentiment label. Spans could
be words, phrases, sentences, or a mixture of these in the
paragraph. Another example is time series prediction, where
Sis historical data, yis the value at the next time step.
Supporting Facts . Given an input-output pair (S;y)for
sequence prediction, we assume that yis truly determined
by only a subset of spans in S. More formally, we assume
that there is a subset of spans Ssupfs1;s2;:::;sngsuch
thatyis independent of si, ifsi=2Ssup. In sentiment
classiﬁcation, Ssupcould consist of important sentiment
words or conjunctions (like “good”, “bad”, “but”); in time
series prediction, it could reﬂect the most recent time steps
as well as those a few cycles away if the series is periodic.
For simplicity, we will denote the size of this set m=jSsupj,
and restrict our attention to tasks where mn, such as
those described in the previous section.
2

--- PAGE 3 ---
2.2 S PANDROP
In a long sequence inference task with sparse support facts
(mn), most of the spans in the input sequence will not
contribute to the prediction of y, but they will introduce
spurious correlation in a low-data scenario. SPANDROP
generates new data instances (~S;y)by ablating these spans
at random, while preserving the supporting facts with high
probability so that the model is still trained to make the
correct prediction y. This is akin to counterfactually deter-
mining whether each span truly determines the outcome y
by asking what the prediction would have been without it.
Deﬁnition 1 (SPANDROP).Formally, given a sequence S
that consists of spans (s1;s2;sn),SPANDROPgenerates
a new sequence ~Sas follows:
ii:i:d:Bernoulli(1 p); ~S=(si)n
i=1;i=1;
wherepis the hyperparameter that determines the probabil-
ity to drop a span.
Note that SPANDROP does not require introducing sub-
stitute spans or artiﬁcial symbols when ablating spans from
the input sequence. It makes the most of the natural se-
quence as it occurs in the original training data, and pre-
serves the relative order between spans that are not dropped,
which is often helpful in understanding sequential data (e.g.,
time series or text). It is also not difﬁcult to establish that the
resulting sequence ~Scan preserve all of the msupporting
facts with high probability regardless of how large nis.
Remark 1. The new sequence length n0=j~Sjand the
number of preserved supporting facts m0=j~S\Ssupj
follow binomial distributions Bin(n;p)andBin(m;p), re-
spectively, where P(x=kjN;p) = N
k
(1 p)kpN kfor
XBin(N;p).
Therefore, the proportion of sequences where all sup-
porting facts are retained ( i.e.,m0=m) is(1 p)m, which
is independent of n. This means that as long as the total
number of supporting facts in the sequence is bounded, then
regardless of the sequence length, we can always choose p
carefully such that we end up with many valid new examples
with bounded noise introduced to supporting facts. Note
that our analysis so far relies only on the assumption that
mis known or can be estimated, and thus it can be applied
to tasks where the precise set of supporting facts Ssupis
unknown. More formally, the amount of new examples can
be characterized by the size of the typical set of~S,i.e., the
set of sequences that the randomly ablated sequence will
fall into with high probability. The size of the typical set
forSPANDROPis approximately 2nH(p), whereH(p)is the
binary entropy of a Bernoulli random variable with proba-
bilityp. Intuitively, these results indicate that the amount
of total counterfactual examples generated by SPANDROP0 20 40 60 80 1000510
Sequence length ( n0)Proportion (%)SPANDROP
Beta-S PANDROP
(a) Length of ~S(n= 100;p= 0:2)
0 10 20 4 202
Supporting facts ( m)Log noise-free prob.=1 = 100
= 10 = 1
= 0:1 = 0:01
(b)Supporting fact noise ( p= 0:2)10010210400:20:40:6
Sequence length ( n)Avg. nats/span=1 = 100
= 10 = 1
= 0:1 = 0:01
(c)Typical set size ( p= 0:1)
Figure 2: Theoretical comparison between SPANDROP and
Beta-S PANDROP.
scales exponentially in n, but the level of supporting fact
noise can be bounded as long as mis small.
However, this formulation of SPANDROP does have a
notable drawback that could potentially hinder its efﬁcacy.
The new sequence length n0follows a binomial distribution,
thus for sufﬁciently large n, most ~Slengths will concentrate
around the mean n(1 p)with a width of O(pn). This
creates an artiﬁcial and permanent distribution drift from
the original length (see Figure 2(a)). Furthermore, even if
we know the identity of Ssupand keep these spans during
training, this length reduction will bias the training set to-
wards easier examples to locate spans in Ssup, potentially
hurting generalization performance. In the next subsection,
we will introduce a variant of SPANDROP based on the
beta-Bernoulli distribution that alleviates this issue.
Relation to word dropout . A commonly used data aug-
mentation/regularization technique in NLP, word dropout
Dai & Le (2015); Gal & Ghahramani (2016), is closely
related to SPANDROP. Two crucial difference, however,
setSPANDROP apart from it and similar techniques: 1)
word dropout masks or replaces symbols in the input, while
SPANDROP directly removes them. Thus SPANDROP can
avoid introducing artiﬁcial representations not used at test
time, and affect sequence length and the absolute position of
remaining elements in the sequence, which we will demon-
strate alleviates the effect of spurious correlations between
the outcome and absolute element positions in the training
data; 2) SPANDROP can operate on input spans segmented
at arbitrary granularity (not just words, or even necessarily
contiguous), and our theoretical analysis holds as long as
these spans are disjoint . We will show that task-informed
span segmentation leads to further performance gains.
3

--- PAGE 4 ---
2.3 Beta-S PANDROP
To address the problem of distribution drift with SPANDROP,
we introduce a variant that is based on the beta-Bernoulli
distribution. The main idea is that instead of dropping each
span in a sequence independently with a ﬁxed probability
p, we ﬁrst sample a sequence-level probability at which
spans are dropped from a Beta distribution, then use this
probability to perform S PANDROP.
Deﬁnition 2 (Beta- SPANDROP).Let=;=1 p
p,
where >0is a scaling hyperparameter. Beta- SPANDROP
generates ~SoverSas:
B(;); ii:i:d:Bernoulli(1 );~S=(si)n
i=1;i=1;
where B(;)is the beta-distribution with parameters >
0and >0.
It can be easily demonstrated that in Beta- SPANDROP,
the probability that each span is dropped is still pon average:
E[ijp] =E[E[ij]jp] =E[1 jp] = 1 
+= 1 p.
In fact, we can show that as !1 , Beta- SPANDROP de-
generates into SPANDROP since the beta-distribution would
assign all probability mass on =p. Despite the simplicity
in its implementation, Beta- SPANDROP is signiﬁcantly less
likely to introduce unwanted data distribution drift, while
is capable of generating diverse counterfactual examples to
regularize the training of sequence inference models. This
is due to the following properties:
Remark 2. The new sequence length n0=j~Sjand the num-
ber of preserved supporting facts m0=j~S\Ssupjfollow
beta-binomial distributions B-Bin(n;; )andB-Bin(m;; ),
respectively, where
P(x=kjN;; ) = (N+1)
 (k+1) (N k+1) (k+) (N k+)
 (N++) (+)
 () ()
forXB-Bin(N;; ), and  (z) =R1
0xz 1e xdxthe
gamma function.
As a result, we can show that the probability that Beta-
SPANDROP preserves the entire original sequence with the
following probability
P(n0=njn;; ) = (n+) (+)
 (n++) ():
When= 1, this expression simply reduces to
n+; when
6= 1, this quantity tends to O(n )asngrows sufﬁciently
large. Comparing this to the O((1 p)n)rate from SPAN-
DROP, we can see that when nis large, Beta- SPANDROP
recovers more of the original distribution represented by
(~S;y)compared to SPANDROP. In fact, as evidenced by
Figure 2(a), the counterfactual sequences generated by Beta-
SPANDROPare also more spread-out in their length distribu-
tion besides covering the original length nwith signiﬁcantly
higher probability. A similar analysis can be performed
by substituting nandn0withmandm0, where we canconclude that as mgrows, Beta- SPANDROP is much bet-
ter at generating counterfactual sequences that preserve the
entire supporting fact set Ssup. This is shown in Figure
2(b), where the proportion of “noise-free” examples ( i.e.,
m0=m) decays exponentially with SPANDROP (=1)
while remaining much higher when is sufﬁciently small.
For instance, when p= 0:1,= 1andm= 10 , the propor-
tion of noise-free examples for SPANDROP is just 34.9%,
while that for Beta-S PANDROP is 47.4%.
As we have seen, Beta- SPANDROP is signiﬁcantly bet-
ter than its Bernoulli counterpart at assigning probability
mass to the original data as well as generated sequences that
contain the entire set of supporting facts. A natural question
is,does this come at the cost of diverse counterfactual exam-
ples? To answer this question we study the entropy of the
distribution that ~Sfollows by varying andn, and normal-
ize it bynto study the size of typical set of this distribution.
As can be seen in Figure 2(c), as long as is large enough,
the average entropy per span Hdegrades very little from
the theoretical maximum, which is H(p), attained when
=1. Therefore, to balance between introducing noise in
the supporting facts and generating diverse examples, we
set= 1in our experiments.
Beta-Bernoulli distribution in dropout . The beta-Bernoulli
distribution has been studied in prior work in seeking re-
placements for the (Bernoulli) dropout mechanism (Srivas-
tava et al., 2014). Liu et al. (2019a) set =for the beta
distribution in their formulation, which limits the dropout
rate to always be 0.5. Lee et al. (2018) ﬁx = 1and vary
to control the sparsity of the result of dropout, which is
similar to Beta- SPANDROP when= 1. However, we note
that these approaches (as with dropout) are focused more on
adding noise to internal representations of neural networks
to introduce regularization, while SPANDROP operates di-
rectly on the input to ablate different components therein,
and thus orthogonal (and potentially complementary) to
these approaches. Further, SPANDROP has the beneﬁt of
not having to make any assumption about the model or any
change to it during training, which makes it much more
widely applicable.
3 F INDANIMALS : Distilling Supervi-
sion from Long-Sequences
In this section, we design a synthetic task of ﬁnding a spe-
ciﬁc subsequence in a character sequence to: a) demonstrate
the effectiveness of SPANDROP and Beta- SPANDROP in
promoting the performance over a series of problems with
different settings, b) analyze the various factors that may
affect the efﬁcacy of these approaches, and c) compare it to
other counterfactual augmentation techniques like masking
on mitigating position bias.
4

--- PAGE 5 ---
3.1 Experimental Setup
FINDANIMALS . To understand the effectiveness of SPAN-
DROPand Beta- SPANDROPempirically, we designed a syn-
thetic task called FINDANIMALS where the model is trained
to discern that given an animal name a,e.g., “cat”, whether
a character string contains it as a subsequence ( i.e., contains
characters in “cat” in order, for instance, “ abcdafgbijk tma”)
or not ( e.g., “abcdefhtijkamn”). This allows us to easily
control the total sequence length n, the supporting facts size
m, as well as easily estimate the supporting fact noise that
each S PANDROP variant might introduce.1
In all of our experiments, we evaluate model perfor-
mance on a held-out set of 10,000 examples to observe
classiﬁcation error. We set sequence length to n= 300
where each letter is a separate span, and chose positions for
the letters in the animal name a(jaj=m= 3) uniformly
at random in the sequence unless otherwise mentioned.
Model . We employ a three-layer Transformer model (Vaswani
et al., 2017) with position embeddings (Devlin et al., 2019)
as the sequence encoder implemented with HuggingFace
Transformers (Wolf et al., 2019). For each example (a;S;y),
we feed “[CLS] a[SEP] S[SEP]” to the model and then
perform binary classiﬁer over the “[CLS]” representation
to predicty2f0;1g, wherey= 1means aappears in S.
To investigate the effectiveness of SPANDROP, we apply
SPANDROP toSﬁrst before feeding the resulting sequence
into the Transformer classiﬁer.
3.2 Results and Analysis
In each experiment, we compare SPANDROP and Beta-
SPANDROP at the same drop ratio p. And we further use
rejection sampling to remove examples that do not preserve
the desired supporting facts to understand the effect of sup-
porting fact noise.
Data efﬁciency . We begin by analyzing the contribution of
SPANDROP and Beta- SPANDROP to improving the sample
efﬁciency of the baseline model. To achieve this goal, we
vary the training set size from 10 to 50,000 and observe
the prediction error on the held-out set. We observe from
the results in Figure 3(a) that: 1) Both SPANDROP and
Beta- SPANDROP signiﬁcantly improve data efﬁciency in
low-data settings. For instance, when trained on only 200
training examples, SPANDROP variants can achieve the gen-
eralization performance of the baseline model trained on
5x to even 20x data. 2) Removing supporting fact noise
typically improves data efﬁciency further by about 2x. This
1To generate the synthetic training data of FINDANIMALS , we ﬁrst
generate sequences consisting of lowercase letters (a to z) that each se-
quence Sdoes not contain the animal name a; then randomly choose half
of these sequences, and replace letters with those in afrom arbitrary (but
not necessarily contiguous) positions in Sto generate positive examples,
and label the rest negative.indicates it is helpful not to drop spans in Ssupduring train-
ing when possible, so that the model is always trained with
true counterfactual examples rather than sometimes noisy
ones. 3) Beta- SPANDROP consistently improves upon the
baseline model even when data is abundant. This is likely
due to the difﬁculty of the task when n= 300 andm= 3.
Similar to many real-world tasks, the task remains under-
speciﬁed even when the generalization error is already very
low, thanks to the large amount of training data available.
4)SPANDROP introduces inconsistent training objective
with the original training set, which leads to performance
deterioration when there is sufﬁcient training data, which is
consistent with our theoretical observation.
Effect of supporting fact noise . Since SPANDROP intro-
duces noise in the supporting facts (albeit with a low prob-
ability), it is natural to ask if such noise is negatively cor-
related with model performance. We study this by varying
the drop ratio pfromf0:05;0:1;0:15;0:2;0:3;0:4;0:5gon
ﬁxed training sets of size 1,000, and observe the resulting
model performance and supporting fact error. As can be
seen in Figure 3(b), supporting fact noise increases rapidly
aspgrows.2However, we note that although the perfor-
mance of SPANDROP deteriorates as pincreases, that of
Beta- SPANDROP stays relatively stable. Inspecting these
results more closely, we ﬁnd that even the performance of
the noise-free variants follow a similar trend, which should
not be affected by supporting fact noise.
Effect of sequence and supporting fact lengths . In our
theoretical analysis, sequence length ndetermines regular-
ization strength and the drift in sequence length distribution,
and supporting fact size mdetermines the probability that
the counterfactual example retains the correct supervision
signal. To study their effects empirically, we conduct three
separate sets of experiments: 1) training and testing the
model on varying sequence lengths f10, 20, 30, 50, 100,
200, 300, 500g; 2) varying supporting fact size between 2
and 10; and 3) testing the model trained on n= 300 on test
sets of different lengths.
As can be seen from Figures 3(c) and 3(e), our experi-
mental results seem to well supporting this hypothesis that
the gap between the two SPANDROP variants has a lot to
do with the discrepancy in length distributions. Speciﬁcally,
in Figure 3(c), while the performance of both SPANDROP
variants deteriorates as ngrows and the task becomes more
challenging and underspeciﬁed, SPANDROP deteriorates at
a faster speed even when we remove the effect of supporting
fact noise. On the other hand, we see in Figure 3(e) that
SPANDROP performance peak around sequences of length
270 ( =n(1 p)=300(1-0.1)) before rapidly deteriorat-
2Note that the noise in our experiments are lower than what would
be predicted by theory, because in practice the initial sequence Smight
already contain parts of abefore it is inserted. This creates redundant sets
of supporting facts for this task and reduces supporting fact noise especially
whennis large.
5

--- PAGE 6 ---
101103105131030
# Training examplesError/Noise (%)(a) Data efﬁciency
00:10:20:30:40:5131030
Drop ratep(b) Noise in supporting facts
Noise, S PANDROP
Noise, Beta-S PANDROP
10 30 100 3000.1110
Train/test sequence length(c) Varying sequence length
Noise, S PANDROP
Noise, Beta-S PANDROP
2 4 6 8 10131030
Supporting fact size(d) Varying supporting fact size
Noise, S PANDROP
Noise, Beta-S PANDROPBaseline SPANDROP SPANDROP (noise-free) Beta-S PANDROP Beta-S PANDROP (noise-free)
2002252502753003250102030
Test sequence lengthError(%)(e) OOD length generalization
Bernoulli Beta-Bernoulli02040
Drop/mask distribution(f) S PANDROP vs S PANMASK
SPANMASK
Fixed First 1000204060
Setting(g) Position&0-shot generalization
SPANMASK
0 2.5 510431030
Training steps(h) Dev error during training
Figure 3: Experimental results of S PANDROP variants and S PANMASK on the F INDANIMALS synthetic tasks.
ing, while Beta- SPANDROPis unaffected until test sequence
length exceeds that of all examples seen during training.
In Figure 3(d), we can also see that as predicted by
Remarks 1 and 2, Beta- SPANDROPis better at preserving all
supporting facts than SPANDROPagnostic of their positions,
which translates to superior performance.
Mitigating position bias . Besides SPANDROP, replacement-
based techniques like masking can also be applied to con-
struct counterfactual examples, where elements in the se-
quence are replaced by a special symbol that is not used at
test time. We implement SPANMASK in the same way as
SPANDROP except spans are replaced rather than removed
when the sampled “drop mask” iis 0. We ﬁrst inspect
whether SPANMASK beneﬁts from the same beta-Bernoulli
distribution we use in SPANDROP. As can be seen in Figure
3(f), the gain from switching to a beta-Bernoulli distribution
provides negligible beneﬁt to SPANMASK, which does not
alter the sequence length of the input to begin with. We
also see that SPANMASK results in signiﬁcantly higher error
than both S PANDROP and Beta-S PANDROP in this setting.
We further experiment with introducing position bias
into the training data (but not the test data) to test whether
these method help the model generalize to an unseen set-
ting. Speciﬁcally, instead of selecting the position for the
characters in auniformly at random, we train the model
with a “ﬁxed position” dataset where they always occur at
indices (10, 110, 210), and a “ﬁrst 100” dataset where they
are uniformly distributed among the ﬁrst 100 letters. As
can be seen in Figure 3(g), both the baseline and SPAN-
MASK models overﬁt to the position bias in the “ﬁxed” set-
ting, while SPANDROPtechniques signiﬁcantly reduce zero-
shot generalization error. In the “ﬁrst 100” setting, Beta-
SPANDROP consistently outperforms its Bernoulli counter-part and SPANMASK at improving the performance of the
baseline model as well, indicating that SPANDROP variants
are effective at reducing the position bias of the model.
Impact on convergence speed . Regularization is com-
monly shown to slow down model convergence, resulting in
the need for much longer training time for marginal improve-
ments in performance. We show in Figure 3(h), however,
thatSPANDROP approaches do not seem to suffer from this
issue on the synthetic task. In the contrary, both approaches
help the model generalize signiﬁcantly better within the
same amount of training time.
4 Experiments on Natural Language
Data
To examine the efﬁcacy of the proposed SPANDROP tech-
niques on realistic data, we conduct experiments on four
NLP datasets that represent a variety of tasks. We focus on
showing the effect of SPANDROP instead of pursuing the
state-of-the-art in these experiments.
4.1 Setup and Main Results
Datasets . We use four natural language processing datasets:
SQuAD 1.1 (Rajpurkar et al., 2016), where models answer
questions on a paragraph of text from Wikipedia; MultiRC
(Khashabi et al., 2018), which is a multi-choice reading com-
prehension task in which questions can only be answered
by taking into account information from multiple sentences ;
HotpotQA (Yang et al., 2018), which requires models to
perform multi-hop reasoning over multiple Wikipedia pages
6

--- PAGE 7 ---
(a) HotpotQA dev
Model Ans F 1Sup F 1Joint F 1
RoBERTa-base 73.5 83.4 63.5
Longformer-base 74.3 84.4 64.4
SAE BERT-base 73.6 84.6 65.0
Our implementation
ELECTRA-base 74.2 86.3 66.2
+ SPANDROP 74.7 86.7 66.8
+ Beta-S PANDROP 74.7 86.9 67.1(b) MultiRC dev/test
Model EM F 1
BERT-base 26.6/24.1 71.8/70.1
RoBERTa-base 38.7/ — 77.1/ —
REPT RoBERTa-base 40.4/ — 80.0/ —
Our implementation
ELECTRA-base 40.1/39.1 80.4/78.2
+ SPANDROP 42.3/39.9 81.7/78.5
+ Beta-S PANDROP 44.8/41.1 81.6/79.8
(c) DocRED dev
Model Ign F 1RE F 1Evi F 1
E2GRE BERT-base 55.2 58.7 47.1
ATLOP BERT-base 59.2 61.1 —
SSAN BERT-base 57.0 59.2 —
Our implementation
ELECTRA-base 59.6 61.6 50.8
+ SPANDROP 59.9 61.9 51.2
+ Beta-S PANDROP 60.1 62.1 51.2(d) SQuAD dev
Model EM F 1
RoBERTa-base — 90.6
ELECTRA-base 84.5 90.8
XLNet-large 89.7 95.1
Our implementation
ELECTRA-base 86.6 92.4
+ SPANDROP w/ adaptive spans 87.4 92.9
+ Beta-S PANDROP w/ adaptive spans 87.3 92.8
Table 1: Main results on four natural language processing datasets.
to answer questions; and DocRED (Yao et al., 2019), which
is adocument-level data set for relation extraction.
For the SQuAD dataset, we deﬁne spans as collections
of one or more consecutive tokens to show that SPANDROP
can be applied to different granularities. Guided by our the-
oretical analysis, we also experiment with an adaptive span
segmentation approach to reduce the number of supporting
facts (m) by combining all N-gram overlaps between the
question and context into a single span ( N2). For the
rest three datasets, we deﬁne spans to be sentences since
supporting facts are provided at sentence level. For all of
these tasks, we report standard exact match (EM) and F 1
metrics where applicable, for which higher scores are better.
We refer the reader to the appendix for details about the
statistics and metrics of these datasets.
Model . We build our models based on ELECTRA (Clark
et al., 2019), since it is shown to perform well across a range
of NLP tasks recently. We introduce randomly initialized
task-speciﬁc parameters designed for each task following
prior work on each dataset, and ﬁnetune these models on
each dataset to report results. We refer the reader to the
appendix for training details and hyperparameter settings.
Main Results . We ﬁrst present the performance of our im-
plemented models and their combination with SPANDROP
variants on the four natural language processing tasks. We
also include results from representative prior work on each
dataset for reference (detailed in the appendix), and sum-
marize the results in Table 1. We observe that: 1) our
implemented models achieve competitive and sometimes
signiﬁcantly better performance (in the cases of HotpotQA,
SQuAD, and DocRED) compared to published results, es-pecially considering that we do not tailor our models to
each task too much; 2) SPANDROP improves the perfor-
mance over these models even when the training set is large
and that the model is already performing well; 3) Models
trained with Beta- SPANDROP consistently perform better
or equally well with their SPANDROP counterparts across
all datasets, demonstrating that our observations on the syn-
thetic datasets generalize well to real-world ones. We note
that the performance gains on real-world data is less signiﬁ-
cant, which likely results from the fact that non-supporting
spans in the synthetic task are independent from each other,
which is not the case in natural language data.
4.2 Analysis
To understand whether the properties of SPANDROP and
Beta- SPANDROP we observe on FINDANIMALS generalize
to real-world data, we further perform a set of analysis
experiments on SQuAD. Speciﬁcally, we are interested in
studying the effect of the amount of training data, the span
drop ratiop, and the choice of span size on performance.
Effect of low data . To understand SPANDROP’s regulariz-
ing effect when training data is scarce, we study the model’s
generalization performance when training on only 0.1% of
the training data (around 100 examples) to using the entire
training set (around 88k examples). As shown in Figure 4
(left), both SPANDROP and Beta- SPANDROP signiﬁcantly
improve model performance when the amount of training
data is extremely low. As the amount of training data in-
creases, this gap slowly closes but remains consistently posi-
tive. When all training data is used, the gap is still sufﬁcient
to separate top-2 performing systems on this dataset.
7

--- PAGE 8 ---
0.1 110100406080
% of training set usedDev F 1(%)Baseline SPANDROP Beta-S PANDROP
00:20:4919293
Drop ratiopad141664919293
Span size
Figure 4: Effect analysis of training data size, drop ratio
and span size on performance of models trained with SPAN-
DROP and Beta-S PANDROP over SQuAD
Impact of drop ratio . We compare SPANDROP and Beta-
SPANDROP by controlling how likely each span is dropped
on average (drop ratio p). Recall from our experiments
onFINDCATSthat largerpwill result in distribution drift
from the original training set for SPANDROP but not Beta-
SPANDROP, thus the performance of the former deteriorates
aspincreases while the latter is virtually not affected. As
can be seen in Figure 4 (middle), our observation on real-
world data is consistent with this theoretical prediction, and
indicate that Beta- SPANDROP is a better technique for data
augmentation should one want to increase sequence diver-
sity by setting pto a larger value.
Impact of span selection . We train the model with SPAN-
DROP on SQuAD with varying ﬁxed span sizes of f1, 2, 4,
8, 16, 32, 64gtokens per span, as well as task-motivated
adaptive span sizes, to understand the effect of this hyper-
parameter. We observe in Figure 4 (right) that as span size
grows, the generalization performance of the model ﬁrst
holds roughly constant, then slowly deteriorates as span size
grows too large. Adaptive spans (“ad”) notably outperforms
all ﬁxed span sizes, suggesting that while larger ﬁxed span
sizes reduce mand thus noise in supporting facts, the re-
sulting reduction in nhinders regularization strength and
thus hurting generalization. This observation is consistent
with that on our synthetic data as well as what would be
predicted by our theoretical analysis. This also suggests
that while SPANDROP works with arbitrary span sizes, the
optimal choice of spans for different tasks warrants further
investigation, which we leave to future work.
5 Related Work
Long Sequence Inference . Many applications require the
prediction/inference over long sequences, such as multi-
hop reading comprehension (Yang et al., 2018; Welbl et al.,
2018), long document summarization (Huang et al., 2021),
document-level information extraction (Yao et al., 2019) in
natural language processing, long sequence time-series pre-
diction (Zhou et al., 2021a), promoter region and chromatin-
proﬁle prediction in DNA sequence (Oubounyt et al., 2019;
Zhou & Troyanskaya, 2015) in Genomics etc, where not
all elements in the long sequence contribute equally to the
desired output. Aside from approaches that attempt to ap-proximate all pair-wise interactions between elements in a
sequence, more recent work has also investigated compress-
ing long sequences into shorter ones to distill the informa-
tion therein for prediction or representation learning (Rae
et al., 2020; Goyal et al., 2020; Kim & Cho, 2021).
Sequence Data Augmentation . Data augmentation is an
effective common technique for underspeciﬁed tasks like
long sequence inference. Feng et al. (2021) propose to
group common data augmentation techniques in natural
language processing into three categories: 1) rule-based
methods (Zhang et al., 2015; Wei & Zou, 2019; S ¸ahin &
Steedman, 2018), which apply a set of predeﬁned operations
over the raw input, such as removing, adding, shufﬂing and
replacement; 2) example mixup-based methods (Guo et al.,
2019; Guo, 2020; Chen et al., 2020; Jindal et al., 2020),
which, inspired from Mixup in computer vision (Zhang et al.,
2018), perform interpolation between continuous features
like word embeddings and sentence embeddings; 3) model-
based methods (Xie et al., 2020; Sennrich et al., 2016),
which use trained models to generate new examples ( e.g.,
back translation Xie et al., 2020).
Most existing rule-based data augmentation methods
operate at the token/word level (Feng et al., 2021), such
as word shufﬂe/replacement/addition (Wei & Zou, 2019).
Shufﬂe-based techniques are less applicable when order
information is crucial in the raw data (Lan et al., 2019,
e.g., in natural language). Clark et al. (2019) and Lewis
et al. (2020) have recently shown that well designed word-
level augmentation can also lead to improved pretrained
models, but generalizing this idea to phrases or sentences is
less straightforward. In contrast, our proposed SPANDROP
supports data augmentation in multiple granularity, and is
able to reserve sequence order since drop operation does
not change the relative order of the original input, which is
important in many kinds of sequence data such as natural
language.
6 Conclusion
In this paper, we presented SPANDROP, a simple and effec-
tive method for learning from long sequences, which ablates
parts of the sequence at random to generate counterfactual
data to distill the sparse supervision signal that is predictive
of the desired output. We show via theoretical analysis and
carefully designed synthetic datasets that SPANDROP and
its variant based on the beta-Bernoulli distribution, Beta-
SPANDROP, help models achieve competitive performance
with a fraction of the data by introducing diverse augmented
training examples, and generalize better to previously un-
seen data. Our experiments on four real-world NLP datasets
conﬁrm these theoretical ﬁndings, and demonstrate SPAN-
DROP’s efﬁcacy on strong neural models even when data is
abundant.
8

--- PAGE 9 ---
References
Wei Bao, Jun Yue, and Yulei Rao. A deep learning frame-
work for ﬁnancial time series using stacked autoencoders
and long-short term memory. PloS one , 12(7):e0180944,
2017.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-
former: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020.
Jiaao Chen, Zichao Yang, and Diyi Yang. MixText:
Linguistically-informed interpolation of hidden space for
semi-supervised text classiﬁcation. In Proceedings of
the 58th Annual Meeting of the Association for Computa-
tional Linguistics , pp. 2147–2157, 2020.
Krzysztof Choromanski, Valerii Likhosherstov, David Do-
han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,
et al. Rethinking attention with Performers. In Interna-
tional Conference on Learning Representations , 2020.
Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-
pher D Manning. ELECTRA: Pre-training text encoders
as discriminators rather than generators. In International
Conference on Learning Representations , 2019.
Andrew M Dai and Quoc V Le. Semi-supervised sequence
learning. Advances in neural information processing
systems , 28:3079–3087, 2015.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In NAACL-
HLT, 2019.
Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chan-
dar, Soroush V osoughi, Teruko Mitamura, and Eduard
Hovy. A survey of data augmentation approaches for nlp.
Findings of ACL , 2021.
Yarin Gal and Zoubin Ghahramani. A theoretically
grounded application of dropout in recurrent neural net-
works. In Proceedings of the 30th International Confer-
ence on Neural Information Processing Systems , 2016.
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje,
Venkatesan Chakaravarthy, Yogish Sabharwal, and
Ashish Verma. Power-bert: Accelerating bert inference
via progressive word-vector elimination. In Interna-
tional Conference on Machine Learning , pp. 3690–3699.
PMLR, 2020.
Hongyu Guo. Nonlinear mixup: Out-of-manifold data aug-
mentation for text classiﬁcation. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , volume 34,
pp. 4044–4051, 2020.Hongyu Guo, Yongyi Mao, and Richong Zhang. Augment-
ing data with mixup for sentence classiﬁcation: An em-
pirical study. arXiv preprint arXiv:1905.08941 , 2019.
Kevin Huang, Qi Peng, Guangtao Wang, Tengyu Ma, and
Jing Huang. Entity and evidence guided relation extrac-
tion for docred. arXiv preprint arXiv:2008.12283 , 2020.
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji,
and Lu Wang. Efﬁcient attentions for long document sum-
marization. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies , pp.
1419–1436, 2021.
Fangkai Jiao, Yangyang Guo, Yilin Niu, Feng Ji, Feng-Lin
Li, and Liqiang Nie. REPT: Bridging language models
and machine reading comprehensionvia retrieval-based
pre-training. arXiv preprint arXiv:2105.04201 , 2021.
Amit Jindal, Arijit Ghosh Chowdhury, Aniket Didolkar,
Di Jin, Ramit Sawhney, and Rajiv Shah. Augmenting nlp
models using latent feature interpolations. In Proceedings
of the 28th International Conference on Computational
Linguistics , pp. 6931–6936, 2020.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green,
Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasu-
vunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko,
et al. Highly accurate protein structure prediction with
alphafold. Nature , 596(7873):583–589, 2021.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
and Fran c ¸ois Fleuret. Transformers are RNNs: Fast au-
toregressive transformers with linear attention. In Inter-
national Conference on Machine Learning . PMLR, 2020.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. Looking beyond the
surface: A challenge set for reading comprehension over
multiple sentences. In Proceedings of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies , 2018.
Gyuwan Kim and Kyunghyun Cho. Length-adaptive trans-
former: Train once with length drop, use anytime with
search. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language
Processing , pp. 6501–6511, 2021.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-
former: The efﬁcient transformer. In International Con-
ference on Learning Representations , 2019.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
Gimpel, Piyush Sharma, and Radu Soricut. ALBERT:
9

--- PAGE 10 ---
A lite bert for self-supervised learning of language rep-
resentations. In International Conference on Learning
Representations , 2019.
Juho Lee, Saehoon Kim, Jaehong Yoon, Hae Beom Lee,
Eunho Yang, and Sung Ju Hwang. Adaptive network
sparsiﬁcation with dependent variational beta-bernoulli
dropout. arXiv preprint arXiv:1805.10896 , 2018.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-
jad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy-
anov, and Luke Zettlemoyer. BART: Denoising sequence-
to-sequence pre-training for natural language generation,
translation, and comprehension. In Proceedings of the
58th Annual Meeting of the Association for Computa-
tional Linguistics , 2020.
Lei Liu, Yuhao Luo, Xu Shen, Mingzhai Sun, and Bin Li.
-dropout: A uniﬁed dropout. IEEE Access , 7:36140–
36153, 2019a.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. RoBERTa: A robustly
optimized BERT pretraining approach. arXiv preprint
arXiv:1907.11692 , 2019b.
Mhaned Oubounyt, Zakaria Louadi, Hilal Tayara, and Kil To
Chong. Deepromoter: robust promoter predictor using
deep learning. Frontiers in genetics , 10:286, 2019.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz,
Noah Smith, and Lingpeng Kong. Random feature atten-
tion. In International Conference on Learning Represen-
tations , 2020.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog , 1(8):9,
2019.
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,
Chloe Hillier, and Timothy P. Lillicrap. Compressive
transformers for long-range sequence modelling. In Inter-
national Conference on Learning Representations , 2020.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. SQuAD: 100, 000+ questions for machine
comprehension of text. In EMNLP , 2016.
G¨ozde G ¨ulS ¸ahin and Mark Steedman. Data augmenta-
tion via dependency tree morphing for low-resource lan-
guages. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing , pp.
5004–5009, 2018.Rico Sennrich, Barry Haddow, and Alexandra Birch. Im-
proving neural machine translation models with monolin-
gual data. In 54th Annual Meeting of the Association for
Computational Linguistics , pp. 86–96, 2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overﬁtting. Journal
of Machine Learning Research , 15(1):1929–1958, 2014.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,
Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebas-
tian Ruder, and Donald Metzler. Long range arena: A
benchmark for efﬁcient transformers. In International
Conference on Learning Representations , 2020.
Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang, Xi-
aodong He, and Bowen Zhou. Select, answer and explain:
Interpretable multi-hop reading comprehension over mul-
tiple documents. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence , volume 34, pp. 9073–9080,
2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and
Illia Polosukhin. Attention is all you need. In Advances in
Neural Information Processing Systems , pp. 5998–6008,
2017.
Jason Wei and Kai Zou. EDA: Easy data augmentation tech-
niques for boosting performance on text classiﬁcation
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Pro-
cessing , pp. 6382–6388, 2019.
Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel.
Constructing datasets for multi-hop reading comprehen-
sion across documents. Transactions of the Association
for Computational Linguistics , 6:287–302, 2018.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac,
Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Hug-
gingface’s transformers: State-of-the-art natural language
processing. arXiv preprint arXiv:1910.03771 , 2019.
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and
Quoc Le. Unsupervised data augmentation for consis-
tency training. In Advances in Neural Information Pro-
cessing Systems , 2020.
Benfeng Xu, Quan Wang, Yajuan Lyu, Yong Zhu, and Zhen-
dong Mao. Entity structure within and throughout: Mod-
eling mention dependencies for document-level relation
extraction. In Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence , volume 35, pp. 14149–14157, 2021.
10

--- PAGE 11 ---
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christopher D
Manning. HotpotQA: A dataset for diverse, explainable
multi-hop question answering. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing , pp. 2369–2380, 2018.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,
Russ R Salakhutdinov, and Quoc V Le. XLNet: Gen-
eralized autoregressive pretraining for language under-
standing. In Advances in Neural Information Processing
Systems , 2019.
Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,
Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and
Maosong Sun. DocRED: A large-scale document-level
relation extraction dataset. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pp. 764–777, 2019.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip
Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big
Bird: Transformers for longer sequences. In Advances in
Neural Information Processing Systems , 2020.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk mini-
mization. In International Conference on Learning Rep-
resentations , 2018.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-
level convolutional networks for text classiﬁcation. In
Advances in Neural Information Processing Systems , pp.
649–657, 2015.
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang,
Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Be-
yond efﬁcient transformer for long sequence time-series
forecasting. In Proceedings of AAAI , 2021a.
Jian Zhou and Olga G Troyanskaya. Predicting effects of
noncoding variants with deep learning–based sequence
model. Nature methods , 12(10):931–934, 2015.
Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang.
Document-level relation extraction with adaptive thresh-
olding and localized context pooling. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , 2021b.
11

--- PAGE 12 ---
A Appendix
A.1 Statistics of Benchmark Data Sets
In this section, we summarize the statistics of the four natu-
ral language processing datasets used in our experiments in
Table 2. For SQuAD, since the dataset does not annotate sup-
porting facts, we approximately estimate supporting facts
by counting tokens that are part of a bigram that appears in
the question.
A.2 Experimental Setup for Different Tasks
This section introduces the detailed implementations of our
methods on four benchmark data sets, as well as the hyper-
parameter setting for model optimization and baselines to
compare with our implementations.
A.2.1 HotpotQA
Implementation details . The objective of HotpotQA is an-
swering questions from a set of 10 paragraphs where two
paragraphs are relevant to the question and the rest are dis-
tractors. HotpotQA presents two tasks: answer span predic-
tion and evidence sentence (i.e., supporting fact) prediction.
Our HotpotQA model consists of two stages: the ﬁrst stage
selects top 4 paragraphs from 10 candidates by a retrieval
model. The second stage ﬁnds the ﬁnal answer span and
evidence over the selected 4 paragraphs. We particularly
feed the following input format to encoder: “[CLS] question
[SEP] sent 1;1[SEP] sent 1;2[SEP] sent 4;1[SEP] sent 4;2
[SEP]”. And we apply the proposed span drop methods
over all the sentences except for supporting facts.
For answer span prediction, we use the answer span
prediction model in (Devlin et al., 2019) with an additional
task of question type (yes/no/span) classiﬁcation head over
the ﬁrst special token ([CLS]). For evidence extraction, we
apply two-layer MLPs on top of the representations corre-
sponding to sentence and paragraph to get the corresponding
evidence prediction scores and use binary cross entropy loss
to train the model. Finally, we combine answer span, ques-
tion type, sentence evidence, and relevant paragraph losses
and train the model in a multitask way using linear combi-
nation of losses. The hyper-parameter search space for our
models on HotpotQA is given in Table 3.
Baselines . We compare our implementation of HotpotQA
model over Electra with the following strong baselines: 1)
RoBERTa (Liu et al., 2019b) based model, 2) long sequence
encoder Longformer (Beltagy et al., 2020) based model and
3) SAE (Tu et al., 2020) which combines graph neural net-
work and pretrained language models for multi-hop question
answer and is the current SOTA model on HotpotQA. The
numbers reported in our paper for the ﬁrst two models come
from (Zaheer et al., 2020).A.2.2 MultiRC
Implementation details . MultiRC is a multi-choice ques-
tion answer task, which supplies a set of alternatives or
possible answers to the question and requires to select the
best answer(s) based on multiple sentences while a few
of these sentences (i.e., supporting facts) are relevant to
the questions. For given example of kcandidate answer
andnsentences, we ﬁrst feed the following input to the
encoder: “[CLS] question [SEP] answer 1[SEP] answer 2
[SEP]answerk[SEP] sentence 1[SEP] sentence 2[SEP]
sentencen[SEP]”. For answer prediction, we apply two-
layer MLPs on top of the representations corresponding to
candidate answers and sentences to get the corresponding
answer and sentences scores, and use a combination of two
binary cross entropy losses to train the model in a multi-
task way. We apply our span drop methods over all input
sentences except for the evidence sentences. The hyper-
parameter search space for our MultiRC models is given in
Table 3.
Baselines . We compare our implementation of MultiRC
model over Electra with the following baselines: 1) BERT
based model, 2) RoBERTa (Liu et al., 2019b) based model
and 3) REPT (RoBERTa-base) trained with new additional
training tasks (Jiao et al., 2021).
A.2.3 Relation extraction task: DocRED
Implementation details . DocRED is document-level rela-
tion extraction which consists of two tasks: relation predic-
tion of a given pair of entities and the evidence prediction.
We construct the entity-guided inputs to the encoder follow-
ing prior work Huang et al. (2020). Each training example is
organized by concatenating the head entity, together with the
nsentences in the document as “[CLS] head entity [SEP]
sentence 1[SEP] sentence 2[SEP]sentencen[SEP]”.
For both relation extraction and evidence prediction, we
apply a biafﬁne transformation that combines entity repre-
sentations and entity/sentence representations, respectively,
and score them using the adaptive threshold loss proposed
by Zhou et al. (2021b). We train the model in a multi-task
setting by using a linear combination of relation extraction
and evidence prediction losses. And we apply the proposed
span drop methods over all input sentences except for those
that serve as evidence for entity relations with the head en-
tity. Please refer to Table 3 for the hyper-parameter search
space of our DocRED models.
Baselines . We compare against a set of strong baselines
w.r.t. document-level relation including: 1) E2GRE (Huang
et al., 2020), 2) ATLOP (Zhou et al., 2021b) and 3) SSAN (Xu
et al., 2021).
A.2.4 SQuAD
Implementation details . SQuAD aims at extracting a seg-
12

--- PAGE 13 ---
Data Train Dev # of Spans # of Supporting facts
HotpotQA(Yang et al., 2018)y90,564 7,405 14.45/11/17/2/962.38/2/3/2/12
MultiRC(Khashabi et al., 2018)y5,131 953 14.72/12/18/6/41 2.32/2/2/2/6
DocRED(Yao et al., 2019)y38,180 12,323 8.24/6/10/3/25 1.67/1/2/1/11
SQuAD(Rajpurkar et al., 2016)?87,599 10,570 156.26/114/186/25/853 7.74/3/11/0/82z
ySentence-level span and supporting facts, and MultiRC is the second version and available at https:
//cogcomp.seas.upenn.edu/multirc/ , where the column “train” and “dev” w.r.t. MultiRC
reports the number of questions in training data and dev data, respectively.
?Token-level span, and the statistics of # of spans/supporting facts are collected by setting span = 1 token;
This is collected based on the top 4 documents selected from 10 candidate documents by document
retriever. The ﬁve numbers correspond to Average, 25% Percentile, 75% Percentile, Min and Max,
respectively.
zWe deﬁne the supporting facts as the spans of the context which have bigrams appearing in the question.
Table 2: Statistics of Benchmark Data sets
Parameter name HotpotQA MultiRC DocRED SQuAD
Batch size f4, 8g f8, 16g f4, 8g f16, 32g
Learning ratef3e-5, 2e-5, 1e-5, 1e-4 gf3e-5, 2e-5, 1e-5, 1e-4 gf1e-4, 5e-5gf1e-4, 5e-5g
Span Drop ratio f0.1, 0.15, 0.2, 0.25gf0.1, 0.15, 0.2, 0.25gf0.05, 0.1, 0.15, 0.2gf0.03, 0.05, 0.1, 0.15, 0.2, 0.25 g
Optimizer AdamW AdamW AdamW AdamW
Epochs 10 15 f10,20,30g f2,4,6,8g
Table 3: Hyper-parameter search space for models on different benchmarks
ment of text from a given paragraph as answer to the ques-
tion. We format the input example as “[CLS] question [SEP]
paragraph [SEP]” and feed to the encoder. Following prior
work, we employ a span prediction mechanism where the
end of the span is conditioned on the representation of the
start of the span, originally presented in the XLNet paper
(Yang et al., 2019). And we apply our proposed span drop
methods over the paragraph before feeding it into the model
for training, where spans that contain question bigrams
are preserved. The hyper-parameter space to optimize the
SQuAD model is shown in Table 3.
Baselines . We implemented models for SQuAD based on
ELECTRA (Clark et al., 2019) and compare them with
existing model implementations for SQuAD based on 1)
ELECTRA (Clark et al., 2019), 2) RoBERTa (Liu et al.,
2019b) and 3) XLNet (Yang et al., 2019), respectively.
13
