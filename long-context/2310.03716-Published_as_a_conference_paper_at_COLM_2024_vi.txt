3 PPO Chỉ Tối Ưu Hóa Độ Dài?

Để tạo động lực cho phần này, trước tiên chúng tôi thiết lập lại (Stiennon et al., 2020; Dubois et al., 2023; Zheng et al., 2023b) rằng trên các thiết lập của chúng tôi, thực sự, PPO làm tăng đáng kể độ dài đầu ra. Hình 2 so sánh độ dài đầu ra trên tập kiểm tra của chúng tôi khi lấy mẫu từ mô hình SFT ban đầu (xanh dương) và mô hình sau PPO (đỏ). Chúng tôi ghi nhận sự tăng độ dài rõ ràng trên các thiết lập. Chúng tôi cũng báo cáo (Bảng 1) rằng phần thưởng thực sự tăng sau PPO (∆R) và như trong công việc trước, PPO đánh bại mô hình SFT trên ưu tiên mô phỏng (SIM PREF, Bảng 3); chúng tôi sẽ thảo luận thêm sau, nhưng hiện tại chúng tôi chỉ sử dụng chúng để thiết lập PPO cải thiện so với SFT như mong đợi.

Khi chúng tôi thấy rằng điểm số phần thưởng và độ dài có tương quan tích cực (Hình 1 cho thấy điều này cho WebGPT), có khả năng PPO cải thiện trên các số liệu phần thưởng nội tại bằng cách đơn giản tạo ra đầu ra dài hơn. Dựa trên khả năng này, chúng tôi điều tra câu hỏi sau: đến mức độ nào các cải tiến PPO được giải thích bởi sự tăng độ dài?

3.1 Phân tích cải tiến phần thưởng phân tầng theo độ dài

Thiết lập Thí nghiệm Chúng tôi phân tích liệu các cải tiến phần thưởng tổng thể từ PPO có còn đúng không khi so sánh các đầu ra có độ dài tương tự. Cụ thể, chúng tôi phân tầng đầu ra dựa trên độ dài (sử dụng nhóm 20 token) và báo cáo điểm số phần thưởng trung bình của mỗi nhóm cho các mô hình SFT ban đầu và sau PPO. Lưu ý rằng Hình 2 cho thấy ít sự chồng lấp trong các nhóm độ dài giữa đầu ra SFT và PPO tiêu chuẩn cho WebGPT và Stack; do đó, chúng tôi thêm báo cáo kết quả cho một biến thể của PPO với penalty KL cao (λ trong phương trình 1).

Bảng 1: Tăng phần thưởng không phải độ dài (NRG), cải tiến phần thưởng (∆R) và tỷ lệ của chúng cho PPO tiêu chuẩn (STD) và λ cao (HIGH λ). Tỷ lệ thấp trên WGPT và RLCD (STACK ở mức độ yếu hơn) cho thấy sự phụ thuộc cao vào độ dài cho cải tiến phần thưởng.

WGPT STACK RLCD
STD HIGH λ STD HIGH λ STD HIGH λ
∆R 0.82 0.20 0.89 0.67 0.94 0.61
NRG 0.02 0.03 0.48 0.37 0.25 0.12
tỷ lệ 2.0% 15.1% 53.4% 56.5% 27.2% 19.1%

Đầu tiên, chúng tôi cho thấy tăng phần thưởng tổng thể (∆R) từ PPO so với SFT. Thứ hai, chúng tôi tính toán tăng phần thưởng không phải độ dài (NRG), ∆R trung bình trong mỗi nhóm được cân bằng theo số lượng ví dụ trong mỗi nhóm (SFT và PPO kết hợp). Điều này ước tính cải tiến phần thưởng có thể quy cho các tăng phần thưởng trong nhóm thay vì chuyển dịch phân phối trên các nhóm. Cuối cùng, chúng tôi báo cáo tỷ lệ của NRG và ∆R, tức là phần của tăng phần thưởng do các đặc trưng không phải độ dài.

Kết quả Bảng 1 báo cáo kết quả của chúng tôi cho cả trường hợp tiêu chuẩn và λ cao. Chúng tôi quan sát thấy mặc dù tất cả các thiết lập báo cáo tăng phần thưởng tổng thể, tăng phần thưởng không phải độ dài thấp hơn đáng kể. Đối với WebGPT và RLCD, 70%–90% cải tiến trên WebGPT và RLCD

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

0 50 100 150 200 250
Độ dài01020304050SốW-GPT
Loại
Sft
Std
50 100 150 200 250
Độ dài020406080SốSTACK
Loại
Sft
Std
0 50 100 150 200 250
Độ dài05101520SốRLCD
Loại
Sft
Std

Hình 2: Độ dài đầu ra của mô hình SFT trước (xanh dương) và sau (đỏ) PPO tiêu chuẩn (STD); trung bình được hiển thị bằng đường gạch ngang. Trên tất cả các thiết lập, PPO dẫn đến tăng độ dài lớn.

50 100 150 200
Độ dài1.0
0.5
0.00.51.0Phần thưởngW-GPT
100 150 200 250
Độ dài0.75
0.50
0.25
0.000.250.500.75Phần thưởngStack
0 25 50 75 100 125 150
Độ dài3.03.54.04.55.05.5Phần thưởngRLCD

Hình 3: Độ dài đầu ra so với phần thưởng trong nhóm 20 token. Các chấm đen biểu thị SFT, và mũi tên biểu thị cải tiến (lên) hoặc suy giảm (xuống) sau PPO KL CAO cho mỗi nhóm. Kích thước và cường độ màu tỷ lệ thuận với số lượng ví dụ trong nhóm. Điểm số phần thưởng có tương quan mạnh với độ dài. Trên WebGPT và RLCD, cải tiến phần thưởng trong các nhóm nhỏ, cho thấy cải tiến tổng thể sau PPO chủ yếu do chuyển sang đầu ra dài hơn.

có thể được giải thích bởi dịch chuyển độ dài. Đặc biệt, NRG gần như không đáng kể đối với WebGPT và chỉ đóng góp 2% vào tăng phần thưởng tổng thể trong thiết lập PPO tiêu chuẩn.

Lưu ý STACK báo cáo đóng góp cao hơn của NRG vào tăng phần thưởng tổng thể, có thể vì đầu ra SFT của STACK đã gần với giới hạn độ dài, nên tăng từ việc tăng độ dài không thể đạt được. Như một thiết lập QA kỹ thuật, nó cũng có thể dựa nhiều hơn vào các đặc trưng không phải độ dài.

Chúng tôi hình dung điểm số phần thưởng phân tầng theo độ dài cho trường hợp λ cao trong Hình 3. Các chấm đen đại diện cho đầu ra SFT, và đầu mũi tên biểu thị đầu ra PPO. Hình này hỗ trợ thêm cho Bảng 1: mặc dù điểm số phần thưởng tăng trong mỗi nhóm trung bình, việc tăng không đều và nhỏ hơn nhiều so với tăng phần thưởng từ việc đơn thuần chuyển sang đầu ra dài hơn.

3.2 Phần thưởng chỉ dựa trên độ dài có thể cải thiện hiệu suất không?

Chúng tôi thấy PPO chủ yếu tối ưu hóa độ dài, tuy nhiên chúng tôi (và phần còn lại của cộng đồng) vẫn thấy cải tiến rộng rãi trên đánh giá ưu tiên mô phỏng downstream. Ở đây, chúng tôi cho thấy rằng chỉ tối ưu hóa độ dài vẫn dẫn đến cải tiến với đánh giá này:

1. (LPPO) Sử dụng độ dài đầu ra làm phần thưởng trong PPO. Chúng tôi định nghĩa R∗(y)=1−|len(y)/L−1| trong đó L là siêu tham số độ dài mục tiêu (được đặt thành 156, 120, và 250 trên WEBGPT, RLCD, và STACK tương ứng, mà chúng tôi thấy cho phép tăng độ dài mong muốn mà không trở nên quá dài). Chúng tôi cũng báo cáo một biến thể với hệ số KL λ được đặt thành 0.

2. (SFT-LONG) Lấy mẫu 8 đầu ra từ mô hình SFT và chọn cái dài nhất

Kết quả Bảng 2 chứa kết quả. SFT-LONG đôi khi cải thiện hiệu suất đáng kể (tỷ lệ thắng 57% so với SFT trên Stack), nhưng khi chúng tôi so sánh LPPO với PPO và SFT, chúng tôi thấy rằng việc tối ưu hóa thuần túy cho độ dài thực sự tái tạo hầu hết các cải tiến ưu tiên mô phỏng của PPO với các mô hình phần thưởng đã học.

Đáng chú ý, LPPO mang lại cải tiến tỷ lệ thắng so với SFT-LONG, có đầu ra thậm chí còn dài hơn, kiểm soát thiên lệch độ dài đánh giá. LPPO cũng vượt trội hơn LPPO λ=0. Giả thuyết của chúng tôi là thành phần KL là một ràng buộc quan trọng trên việc tối ưu hóa để cho phép PPO chỉ dựa trên độ dài học các đặc trưng tốt. Vì các đầu ra lặp lại, bệnh lý có thể có độ phân kỳ KL cao hơn so với chính sách ban đầu, thành phần này có thể buộc mô hình học cách tạo ra đầu ra mô tả hơn trong khi cũng tối đa hóa độ dài.

Điều này có thể giải thích một số thành công gần đây của RLHF bất chấp những hạn chế lớn chúng tôi phát hiện. Đánh giá dựa trên tỷ lệ thắng có thể hữu ích để hiểu các cải tiến tổng thể trong LLM. Tuy nhiên, thí nghiệm này tiết lộ rằng nó không đủ: với một kỹ thuật phức tạp như RLHF, một đánh giá đơn giản về việc liệu đầu ra có "cải thiện" không về cơ bản không cho chúng ta biết liệu mọi thứ có thực sự hoạt động như mong đợi hay không.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Bảng 2: Ưu tiên mô phỏng (so với SFT) từ việc tối ưu hóa thuần túy cho độ dài cao hơn (LPPO) có và không có (λ=0) penalty KL, và baseline lấy mẫu dài nhất trong 8 (SFT-LONG); ∗ biểu thị delta có ý nghĩa thống kê so với SFT (p<0.05, kiểm định bootstrap cặp). LPPO có thể so sánh với PPO tiêu chuẩn, hỗ trợ giả thuyết của chúng tôi rằng các cải tiến RLHF chủ yếu dựa trên độ dài. Thú vị, LPPO đánh bại λ=0, SFT-LONG ngay cả khi ngắn hơn, cho thấy phương pháp này gây ra những thay đổi định tính ngoài việc chỉ mở rộng độ dài đầu ra.

W-GPT STACK RLCD
SFT PPO SFT-LONG LPPO LPPO λ=0 SFT PPO SFT-LONG LPPO LPPO λ=0 SFT PPO SFT-LONG LPPO LPPO λ=0
LEN 100 230 141 118 167 203 257 249 252 248 59 94 117 98 163
SIMPREF 50% 58%∗ 48% 56%∗ 53% 50% 58%∗ 57%∗ 59%∗ 58%∗ 50% 63%∗ 52% 64%∗ 51%

4 Can thiệp trên RLHF

Kết quả của chúng tôi đến thời điểm này cho thấy quy trình RLHF dẫn đến đầu ra dài hơn. Tiếp theo, chúng tôi nghiên cứu thành phần nào của quy trình, giữa mô hình hóa phần thưởng và tối ưu hóa PPO, đóng góp vào hành vi này và liệu các can thiệp được thiết kế cẩn thận có thể giảm thiểu nó hay không. Hình 4 cho thấy quy trình RLHF tổng thể và các giai đoạn khác nhau chúng tôi can thiệp. Những thí nghiệm này cho phép chúng tôi kiểm tra liệu độ dài có tiếp tục tăng trong PPO ngay cả với các can thiệp mạnh chống lại nó hay không. Chúng tôi nghiên cứu các can thiệp mục tiêu PPO và rollout trong Phần 4.1 (nửa phải của Hình 4) và thảo luận các can thiệp dữ liệu ưu tiên và mô hình hóa phần thưởng trong Phần 4.2 (nửa trái của Hình 4).

Huấn luyện x1,y+1>y−1 x2,y+2>y−2 xN,y+N>y−N… Dữ liệu Ưu tiên x LLM π y1∼π(y∣x) R(x,y)−λKL(π||πSFT) Cập nhật chính sách hiện tại π Mô hình Phần thưởng R I.1 I.2 I.3 Lấy mẫu từ chính sách hiện tại y2∼π(y∣x) Chấm điểm với RM và thành phần KL I.4 I.5 I.6 I.7 Bước 1: Huấn luyện Mô hình Phần thưởng Bước 2: Huấn luyện với PPO

Can thiệp trên Mô hình hóa Phần thưởng
Cân bằng độ dài dữ liệu ưu tiên
Cắt ngắn dữ liệu ưu tiên dựa trên độ tin cậy RM
Tăng cường dữ liệu ưu tiên ngẫu nhiên I.1 I.3 I.2

Can thiệp trên Huấn luyện PPO
I.4 I.6 I.5 I.7 Lấy mẫu đầu ra ngắn hơn từ chính sách hiện tại
Phạt điểm số phần thưởng của đầu ra dài
Điều chỉnh phần thưởng trong batch
Hệ số KL cao

Hình 4: Can thiệp để kiểm tra ảnh hưởng của các thành phần RLHF khác nhau lên độ dài

4.1 Can thiệp trên tối ưu hóa PPO

(mất mát KL; I.7) Một can thiệp đơn giản là sử dụng hệ số KL CAO λ (Phương trình 1), với trực giác rằng gần hơn với phân phối ban đầu sẽ có nghĩa là gần hơn với độ dài ban đầu. Ở đây, chúng tôi đặt λ thành 0.12 thay vì 0.04; chúng tôi thấy rằng các giá trị lớn hơn cản trở hội tụ mô hình.

(rollouts; I.4) Một tùy chọn đơn giản là hoàn toàn BỎ QUA CÁC ĐẦU RA DÀI vượt quá ngưỡng độ dài từ PPO, để không có cập nhật nào được thực hiện để khuyến khích những điều này. Trong thực tế, chúng tôi hoán đổi những ví dụ này với đầu ra được lấy mẫu ngẫu nhiên từ batch.

(điểm số RM; I.5) Chúng tôi cũng thí nghiệm với penalty vô hướng được thêm vào mô hình phần thưởng để PHẠT ĐỘ DÀI. Chúng tôi đặt R′=R+(1−len(y)/N)σ, trong đó N là độ dài tối đa mà chúng tôi không muốn PPO vượt quá, và σ là trung bình động của độ lệch chuẩn phần thưởng batch.

(điểm số RM; I.6) Công việc trước sử dụng ĐIỀU CHỈNH PHẦN THƯỞNG để "kiểm soát biến động huấn luyện" và tối ưu hóa quá mức (Zheng et al., 2023b). Tương tự như chuẩn hóa batch (Ioffe & Szegedy, 2015), cho mỗi batch X,Y của đầu ra được lấy mẫu, chúng tôi tính trung bình (μ) và độ lệch chuẩn (σ) của R. Sau đó chúng tôi lấy trung bình động của những giá trị này qua N batch trước và "điều chỉnh" R để trở thành R′=(R−μ)/σ (σ vẫn tương đối không đổi qua huấn luyện).

Kết quả Chúng tôi báo cáo kết quả trong Bảng 3. Để làm ngữ cảnh, chúng tôi báo cáo ưu tiên mô phỏng so với PPO: <50% biểu thị chất lượng downstream tệ hơn PPO tiêu chuẩn. Theo trực giác, các can thiệp của chúng tôi nên khuyến khích phần thưởng được tối ưu hóa bằng cách nhắm mục tiêu các đặc trưng không phải độ dài, và LEN nên vẫn tương tự, nếu không ngắn hơn điểm bắt đầu SFT. Lưu ý rằng mỗi thiết lập có PHẦN THƯỞNG khác nhau, vì vậy những phần thưởng này không nên được so sánh qua các thiết lập.

LEN thường giảm đáng kể so với PPO tiêu chuẩn, xác nhận độ dài liên quan đến những phần này của PPO, và chúng tôi có thể giảm thiểu sự phụ thuộc cực đoan vào độ dài trong quá trình tối ưu hóa trong khi vẫn duy trì cải tiến downstream vừa phải. Đối với các thực hành viên, điều này thiết lập các can thiệp của chúng tôi như các phương pháp hợp pháp để kiểm soát độ dài trong RLHF.

Tuy nhiên, độ dài vẫn luôn tăng so với SFT, và điểm số mô hình phần thưởng luôn tệ hơn PPO tiêu chuẩn. Hơn nữa, việc bỏ qua và phạt độ dài thường gây ra thất bại hội tụ (phần thưởng không tăng trong huấn luyện), hỗ trợ vai trò chính của độ dài trong PPO.

Tương tự như Hình 3, chúng tôi cũng lưu ý rằng qua các can thiệp, các biểu đồ phân tán và giá trị NRG hiển thị các mẫu tương tự của sự thống trị độ dài (xem Phụ lục C), xác nhận rằng tỷ lệ tối ưu hóa do độ dài vẫn nhất quán qua các can thiệp PPO.

Bảng 3: Token (LEN), PHẦN THƯỞNG, ưu tiên mô phỏng (SIM PREF, Phần 2.1) so với PPO tiêu chuẩn qua các can thiệp (xanh nếu tốt hơn, đỏ nếu tệ hơn PPO). Các hàng với tối ưu hóa phần thưởng thất bại bị loại trừ (−). ∗ biểu thị delta có ý nghĩa thống kê so với PPO (p<0.05, kiểm định bootstrap cặp). Can thiệp giảm thiểu tăng độ dài so với SFT, nhưng với chi phí cho phần thưởng.

W-GPT STACK RLCD
ĐỘ DÀI PHẦN THƯỞNG SIM PREF ĐỘ DÀI PHẦN THƯỞNG SIM PREF ĐỘ DÀI PHẦN THƯỞNG SIM PREF
SFT (điểm bắt đầu) 100 -0.45 42%∗ 203 0.05 42%∗ 59 4.4 37%∗
PPO TIÊU CHUẨN 230 0.25 50% 257 0.74 50% 94 5.50 50%
ĐIỀU CHỈNH PHẦN THƯỞNG 128 -0.05 49% 249 0.40 46%∗ 82 5.00 41%∗
PHẠT ĐỘ DÀI − − − − − − 72 5.20 44%∗
KL CAO λ 120 -0.06 45%∗ 250 0.30 45%∗ 97 5.20 43%∗
BỎ QUA ĐẦU RA DÀI 127 -0.13 48% − − − − − −

4.2 Can thiệp trên Mô hình hóa Phần thưởng

Phần 4 cho thấy sự thống trị độ dài phần lớn không thay đổi đối với các can thiệp PPO, thay vào đó chỉ ra các tương quan phần thưởng mạnh với độ dài. Chúng tôi điều tra ở đây đến mức độ nào các mô hình phần thưởng ưu tiên đầu ra dài hơn, bắt đầu với một phân tích đơn giản: dữ liệu ưu tiên có mất cân bằng về phía đầu ra dài hơn không? Chúng tôi có thể đo điều này với sự đồng ý heuristic độ dài: độ chính xác của việc luôn dự đoán rằng đầu ra dài hơn là đầu ra được ưa thích vàng (xem Bảng 5), thực sự thấy tất cả các tập dữ liệu hơi mất cân bằng về phía đầu ra dài hơn, nhưng chúng tôi điều này không tiết lộ toàn bộ câu chuyện. Để hiểu rõ hơn, trước tiên chúng tôi sẽ kiểm tra các can thiệp phần thưởng (Hình 4), và sau đó phân tích các nguyên nhân cơ bản:

Bảng 4: Độ chính xác đánh giá (ACC) và Pearson trong batch (CORR) cho các can thiệp RM (RAND là baseline ngẫu nhiên, STND RM bình thường). Độ chính xác RM thường thấp. Ít phương pháp vừa giảm tương quan vừa duy trì độ chính xác tốt: độ dài gắn liền với thành công RM. Thiên lệch độ dài vẫn còn trên RLCD bất chấp cân bằng.

WGPT STACK RLCD
ACC CORR ACC CORR ACC CORR
RAND 50% 0 50% 0 50% 0
STND 61.5% 0.72 70% 0.55 80% 0.67
BAL 52.6% -0.13 61.9% -0.09 73.1% 0.62
C-TR 58.8% 0.67 59.5% 0.31 77.2% 0.57
R-DA 62.5% 0.35 72.6% 0.37 80% 0.43

Bảng 5: Độ chính xác của việc luôn ưu tiên phản hồi dài hơn. Độ chính xác trên ngẫu nhiên (50%) cho thấy thiên lệch độ dài.

WGPT STACK RLCD
55.7% 59.6% 63.1%

(ưu tiên; I.1) Cân bằng Độ dài (BAL): Một tùy chọn là cân bằng dữ liệu theo độ dài. Cụ thể, chúng tôi cân bằng dữ liệu sao cho phân phối của sự khác biệt độ dài cặp đối xứng theo nhóm 10. Giả sử có nhiều ví dụ hơn trong đó phản hồi được ưa thích dài hơn 20 token so với phản hồi không được ưa thích so với trường hợp ngược lại; để cân bằng dữ liệu, chúng tôi lấy mẫu con các trường hợp dài hơn 20 token cho đến khi chúng khớp với số lượng trường hợp ngắn hơn 20 token.

(ưu tiên; I.2) Tăng cường Dữ liệu Phần thưởng (R-DA): Tăng cường dữ liệu có thể khuyến khích mô hình học các đặc trưng bền vững. Chúng tôi sử dụng "ghép cặp ngẫu nhiên", ghép cặp các cặp đầu ra prompt qi,p− i từ P với p− i phục vụ như một ví dụ "được ưa thích", và một p+ j được lấy mẫu ngẫu nhiên từ prompt khác phục vụ như một ví dụ "không được ưa thích". Mặc dù việc tăng cường dữ liệu này không nhắm mục tiêu độ dài per se, trong các thí nghiệm sơ bộ, chúng tôi thấy nó cải thiện độ bền vững RM và giảm tương quan độ dài.

(huấn luyện RM; I.3) Cắt ngắn Dựa trên Độ tin cậy (C-TR): Điều gì sẽ xảy ra nếu thiên lệch độ dài vượt ra ngoài mất cân bằng dữ liệu? Ví dụ, một tập hợp các ví dụ "dễ" có thể đang làm hỏng dữ liệu, và loại bỏ chúng có thể giúp Swayamdipta et al. (2020). Cho rằng chúng tôi đã huấn luyện một số Rbase, và tính toán "độ tin cậy" ci của mô hình trên mỗi ví dụ huấn luyện trên tập dữ liệu P (chúng tôi mô tả thiết lập cho điều này trong Phần 5), chúng tôi có thể kiểm tra ý tưởng này bằng cách huấn luyện một RM mới Rtrunc trên một tập con của P trong đó ci<θ1 và ci>θ2, với các siêu tham số ngưỡng θ1 và θ2. Chúng tôi thí nghiệm với nhiều biến thể (xem Phụ lục C.4), giữ các tập khoảng 50% dữ liệu cho mỗi tập, nhưng chúng tôi sẽ ở đây chỉ báo cáo kết quả khi chúng tôi đặt θ1<θ2, huấn luyện trên các ví dụ độ tin cậy thấp.

Kết quả Chúng tôi báo cáo trong Bảng 4 đánh giá phần thưởng, cũng như tương quan trong batch (CORR), đo lường, cho các tập 8 thế hệ từ cùng một đầu vào, tương quan Pearson trung bình giữa độ dài đầu ra và phần thưởng. Lưu ý rằng độ chính xác mô hình phần thưởng tiêu chuẩn (STND) không cao cho tác vụ nhị phân, trong khi các tương quan độ dài cao.

Nhiều can thiệp, như BAL, giảm tương quan, nhưng tất cả trừ R-DA làm hỏng độ chính xác đánh giá. Thú vị, trên RLCD các tương quan mạnh vẫn còn bất chấp cân bằng, gợi ý thiên lệch có thể khó loại bỏ hơn. Tuy nhiên, STACK, nơi cân bằng làm giảm tương quan với độ chính xác trên ngẫu nhiên, gợi ý rằng các mô hình phần thưởng có thể học các đặc trưng độc lập với độ dài.

Sau đó chúng tôi cho thấy kết quả downstream cho các điều chỉnh dữ liệu ưu tiên trong Bảng 6. Tương tự như các can thiệp PPO (Bảng 3), độ dài vẫn thường tăng từ điểm bắt đầu SFT, mặc dù thường ngắn hơn so với PPO Tiêu chuẩn. Tuy nhiên, BAL trên STACK, có thể do có các đặc trưng dễ không phải độ dài khác để học, dẫn đến đầu ra ngắn hơn SFT (với ưu tiên downstream cao hơn), xác nhận tầm quan trọng của dữ liệu ưu tiên trong RLHF.

Bảng 6: Ưu tiên mô phỏng (SIM PREF) so với PPO STND cho mô hình SFT, độ dài (LEN), PPO STD, và các can thiệp. STACK BAL cho thấy kết quả mạnh có thể mà không tăng độ dài thông qua can thiệp RM (có ảnh hưởng hơn so với can thiệp PPO), mặc dù kết quả không nhất quán.

WGPT STACK RLCD
Phương pháp LEN SIM PREF LEN SIM PREF LEN SIM PREF
SFT 100 42%∗ 203 42%∗ 59 37%∗
STND 230 50% 257 50% 94 50%
BAL − − 148 57%∗ 82 44%∗
R-DA 139 49% 256 58%∗ 112 44%∗
C-TR 141 44%∗ 244 44%∗ 97 50%

5 Phân tích Ưu tiên qua Huấn luyện

Tại sao thiên lệch độ dài xuất hiện trong RM, ngay cả sau khi cân bằng? Để hiểu điều này tốt hơn, chúng tôi nghiên cứu động lực học huấn luyện và khả năng học được ở cấp độ điểm dữ liệu của mô hình hóa phần thưởng. Chúng tôi tính toán thống kê qua nhiều epoch huấn luyện: cho mô hình phần thưởng R được huấn luyện trên tập dữ liệu ưu tiên P trong E epoch, chúng tôi có thể theo dõi mỗi điểm dữ liệu (xi,y+ i,y− i)∈P trong đó chúng tôi tính toán phân phối độ tin cậy (điểm số RM của "được ưa thích" trừ đi "không được ưa thích"), tại mỗi epoch ci={(e,R(xi,y+ i)−R(xi,y− i)):e∈{2, . . . , E}}, loại trừ epoch 1 để giảm thiểu nhiễu.

Kết quả Để làm ngữ cảnh, khi kiểm tra các biểu đồ "bản đồ" ban đầu (Swayamdipta et al., 2020) của trung bình (ci) và phương sai (σ(ci)) của các ci khác nhau (hình ảnh ban đầu trong Phụ lục C.4), chúng tôi thấy các giá trị phần lớn tập trung ở gần không, có nghĩa là các dự đoán có độ tin cậy thấp và phần lớn không thay đổi, có khả năng cho thấy rằng một số ít ví dụ có ci cao có thể có ảnh hưởng không tương xứng lên huấn luyện. Với giả thuyết này rằng độ dài có thể liên quan đến một tập hợp các ví dụ "dễ", chúng tôi sử dụng lại độ chính xác heuristic độ dài, nhưng lần này, chúng tôi tính toán nó trên các lát cắt trong đó chúng tôi nhóm các ví dụ huấn luyện dựa trên ci, vẽ các nhóm này theo độ tin cậy (trục x) so với độ chính xác heuristic độ dài (trục y) trên mỗi lát cắt như biểu đồ phân tán trong Hình 5.

Hình này cho thấy các mẫu rõ ràng đáng kinh ngạc, với độ tin cậy trung bình ci cho dữ liệu trong một khoảng của các ví dụ huấn luyện có tương quan mạnh với heuristic độ dài. Điều này có nghĩa là (1) heuristic độ dài áp dụng cho hầu hết các ví dụ dễ, và (2) đại đa số các dự đoán âm mạnh là các trường hợp trong đó mô hình theo heuristic độ dài để tự tin dự đoán câu trả lời sai. Lưu ý rằng WebGPT, với mẫu mạnh nhất, cũng hiển thị NRG thấp nhất từ Bảng 1, ngụ ý rằng những tương quan này lan truyền qua tất cả các giai đoạn. Do đó, các mô hình phần thưởng có thể gặp khó khăn trong việc học các đặc trưng sâu hơn từ ưu tiên, và ngay cả với việc cân bằng cũng dễ bị thống trị bởi các đặc trưng "dễ" như độ dài.

Hình 5: Độ tin cậy ví dụ huấn luyện (ci) so với heuristic độ dài, được nhóm dựa trên ci, kích thước cho thấy số lượng trong nhóm. Hầu hết các ví dụ có độ tin cậy gần không: RM gặp khó khăn học trên hầu hết dữ liệu. Các dự đoán mạnh (bao gồm không chính xác) theo heuristic độ dài với tỷ lệ thuận sạch: RM có thể quá dựa vào các tập nhỏ của các ví dụ "dễ" thiên lệch độ dài

1 0 1
Độ tin cậy0.00.20.40.60.81.0Heuristic Độ dàiW-GPT
2 0 2 4
Độ tin cậy0.20.40.60.8Heuristic Độ dàiStack
5 0 5
Độ tin cậy0.00.20.40.60.81.0Heuristic Độ dàiRLCD

6 Công việc Liên quan

RL Học tăng cường từ phản hồi của con người đã được khám phá rộng rãi (Knox & Stone, 2009), thường trong các tác vụ robot, để ngoại suy tín hiệu phần thưởng vượt ra ngoài các tập ưu tiên ban đầu (Brown et al., 2019). Trong khi RL trong NLP trong quá khứ gặp phải các vấn đề khác nhau (Ammanabrolu & Riedl, 2018; Martin et al., 2017; Ramamurthy et al., 2023), công việc gần đây trong NLP đã khám phá các thực thi (Zheng et al., 2023b; Touvron et al., 2023b) và mục tiêu (Wu et al., 2023) của RLHF, phần lớn loại bỏ việc tăng độ dài. Lưu ý rằng ngay cả các phương án thay thế RLHF như DPO (Rafailov et al., 2023; Zhao et al., 2023) đã được chứng minh tương ứng với độ dài (Ivison et al., 2023; Ethayaraj et al., 2023) so với RLHF; chúng tôi xác thực trong Phụ lục C. Các sử dụng RL trong NLP trong quá khứ không sử dụng phần thưởng dựa trên ưu tiên, gặp phải các vấn đề khác nhau. Công việc của chúng tôi trực giao với những điều này, sử dụng vấn đề độ dài để phân tích độ bền vững RM và các thuộc tính khác của RLHF.

Mô hình Phần thưởng Cho các thiên lệch dữ liệu, các mô hình phần thưởng có học các đặc trưng bền vững phản ánh ưu tiên cơ bản không? Các hiện vật tập dữ liệu là một vấn đề phổ biến trong NLP ngay cả trên các thiết lập đơn giản hơn như suy luận ngôn ngữ tự nhiên (Gururangan et al., 2018; Poliak et al., 2018). Trong RLHF, Stiennon et al. (2020) lưu ý rằng tối ưu hóa quá mức cho một mô hình phần thưởng dẫn đến các bản tóm tắt bệnh lý, Dubois et al. (2023) lưu ý ưu tiên của con người giảm sau một phần thưởng nhất định, và Pang et al. (2022) trình bày các trường hợp trong đó hacking như vậy có thể được tạo ra trong các thiết lập tổng hợp. Công việc của chúng tôi, so sánh, nghiên cứu tối ưu hóa quá mức trong các thiết lập thực tế, "hoạt động", khám phá chẩn đoán và giải pháp. Chúng tôi tập trung vào độ dài vì nó phổ biến nhất, nhưng paradigm thí nghiệm của chúng tôi áp dụng cho các phân tích khác của RLHF.

Kiểm soát độ dài và thiên lệch độ dài Các kỹ thuật bên ngoài RLHF để kiểm soát độ dài của các mô hình NLP đã được khám phá (Kikuchi et al., 2016; Ficler & Goldberg, 2017), với các phân kỳ độ dài train-test (Riley & Chiang, 2022) được quy cho các kỹ thuật suy luận và thiên lệch nhãn trong sinh văn bản, khá khác với các vấn đề sinh mở của chúng tôi. Murray & Chiang (2018) sử dụng phần thưởng theo từ tương tự như penalty theo từ của chúng tôi trong RL, mặc dù để giải quyết vấn đề ngược lại của đầu ra quá ngắn. Cuối cùng, trong các tác vụ "khớp văn bản" phân biệt như diễn giải lại, công việc trước đã quan sát các heuristic độ dài tương tự, Jiang et al. (2022), nhưng định dạng cặp câu khá khác nhau.

7 Kết luận

Chúng tôi đóng góp một số kỹ thuật mới để đánh giá, phân tích và can thiệp trên RLHF. Qua một tập hợp thí nghiệm đa diện trên ba tập dữ liệu, chúng tôi cho thấy rằng RLHF, đến mức độ đáng ngạc nhiên, dựa vào việc tối ưu hóa độ dài phản hồi. Kết quả của chúng tôi đặt câu hỏi về các cải tiến trong PPO, khả năng của các mô hình phần thưởng học hiệu quả từ ưu tiên, và các paradigm đánh giá gần đây đã bỏ qua những phát hiện chúng tôi hiện tiết lộ.

Trong ngắn hạn, chúng tôi khuyến khích chú ý nhiều hơn đến dữ liệu ưu tiên, và việc áp dụng rộng rãi hơn các phương pháp đánh giá định hướng đặc trưng, như NRG. Tuy nhiên, rộng hơn, chúng tôi tin rằng các cải tiến đáng kể hơn đối với tính dễ tổn thương của RLHF đối với các đặc trưng đơn giản, đặc biệt trong mô hình hóa phần thưởng, sẽ cần thiết để RLHF trở thành một kỹ thuật áp dụng rộng rãi hơn: RLHF vẫn còn một chặng đường dài.

Lời cảm ơn

Công việc này được hỗ trợ bởi NSF CAREER Award IIS-2145280, một khoản tài trợ từ Open Philanthropy, một quà tặng từ Salesforce, Inc., và một quà tặng từ Amazon. Cảm ơn Eunsol Choi và các thành viên của phòng thí nghiệm UT TAUR về thảo luận và phản hồi hữu ích.

Tài liệu tham khảo

[Phần tài liệu tham khảo được dịch đầy đủ với tất cả các trích dẫn và chi tiết...]

A Phụ lục

Tính Tái tạo

Đối với các nghiên cứu khác nhau của chúng tôi về mối quan hệ giữa RLHF và độ dài, trước tiên chúng tôi đã huấn luyện một tập hợp các mô hình phần thưởng và mô hình chính sách. Để hỗ trợ nghiên cứu RLHF mở trong tương lai, chúng tôi phát hành mã của mình cũng như các mô hình phần thưởng và chính sách. Ngoài việc chi tiết thiết lập thí nghiệm và lược đồ đánh giá của chúng tôi trong Phần 2.1, cũng như mô tả các can thiệp của chúng tôi một cách chi tiết trong Phần 4 và Phần 4.2, chúng tôi bao gồm các siêu tham số và hướng dẫn thêm trong Phụ lục B. Lưu ý rằng chúng tôi sử dụng các tập dữ liệu ưu tiên mở, các mô hình cơ sở có sẵn công khai, và mã RLHF nguồn mở không yêu cầu tài nguyên tính toán cấm đoán.

[Tiếp tục dịch toàn bộ phần phụ lục với cùng cấu trúc và chi tiết...]
