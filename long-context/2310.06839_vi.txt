# 2310.06839.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.06839.pdf
# Kích thước tệp: 2530020 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
LongLLMLingua : Tăng tốc và Nâng cao LLMs trong Các Tình huống Ngữ cảnh Dài
thông qua Nén Prompt
Huiqiang Jiang, Qianhui Wu, Xufang Luo,
Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
Microsoft Corporation
{hjiang,qianhuiwu,xufluo,dongsli,cyl,yuqyang,liliqiu}@microsoft.com
Tóm tắt
Trong các tình huống ngữ cảnh dài, các mô hình ngôn ngữ lớn (LLMs) đối mặt với ba thách thức chính: chi phí tính toán cao hơn, giảm hiệu suất, và thiên vị vị trí. Nghiên cứu cho thấy hiệu suất LLM phụ thuộc vào mật độ và vị trí của thông tin quan trọng trong prompt đầu vào. Được truyền cảm hứng từ những phát hiện này, chúng tôi đề xuất LongLLMLingua để nén prompt nhằm cải thiện nhận thức của LLMs về thông tin quan trọng để đồng thời giải quyết ba thách thức này. Đánh giá toàn diện của chúng tôi trên các tình huống ngữ cảnh dài khác nhau cho thấy LongLLMLingua không chỉ nâng cao hiệu suất mà còn giảm đáng kể chi phí và độ trễ. Ví dụ, trong benchmark NaturalQuestions, LongLLMLingua tăng hiệu suất lên đến 21.4% với khoảng 4x ít token hơn trong GPT-3.5-Turbo, dẫn đến tiết kiệm chi phí đáng kể. Nó đạt được giảm 94.0% chi phí trong benchmark LooGLE. Hơn nữa, khi nén prompts khoảng 10k tokens ở tỷ lệ 2x-6x, LongLLMLingua có thể tăng tốc độ trễ end-to-end 1.4x-2.6x.1
1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) đã cách mạng hóa các công nghệ ngôn ngữ hướng người dùng và đang đóng vai trò là thành phần quan trọng trong ngày càng nhiều ứng dụng. Thiết kế prompts cẩn thận là cần thiết để đạt được hiệu suất tốt hơn trong các tác vụ downstream cụ thể. Các công nghệ thường được sử dụng như In-Context Learning (ICL) (Min et al., 2022; Dong et al., 2023), Retrieval Augment Generation (RAG) (Lewis et al., 2020; Asai et al., 2024), và Multi-turn Agent (Shen et al., 2024; Park et al., 2023; Wu et al., 2023a) đang thúc đẩy prompts ngày càng dài hơn, thậm chí đạt đến hàng nghìn tokens. Các tình huống như multi-document question answering, code completion, và document summarization cũng đòi hỏi xử lý ngữ cảnh dài.
1Truy cập mã của chúng tôi tại https://aka.ms/LongLLMLingua .Có ba thách thức chính khi LLMs được sử dụng trong các tình huống ngữ cảnh dài: (1) Chi phí tính toán cao hơn, bao gồm cả chi phí tài chính và độ trễ. (2) Prompts dài hơn giới thiệu thông tin không liên quan và dư thừa, có thể làm suy yếu hiệu suất của LLMs (Shi et al., 2023), như được minh họa trong Hình 1a. (3) LLMs thể hiện thiên vị vị trí (Kamradt, 2023), còn được gọi là vấn đề "lost in the middle" (Liu et al., 2024), cho thấy rằng việc đặt thông tin quan trọng trong prompt ảnh hưởng đáng kể đến hiệu suất của LLMs. Điều này được thể hiện bằng đường cong màu tím trong Hình 1b.

Được truyền cảm hứng từ những quan sát này, chúng tôi đề xuất LongLLMLingua để giải quyết ba thách thức này. Cụ thể, chúng tôi sử dụng LLMLingua (Jiang et al., 2023a) làm backbone cho nén prompt để giải quyết thách thức đầu tiên, tức là giảm chi phí và độ trễ. Tuy nhiên, trong trường hợp ngữ cảnh dài, phân phối thông tin quan trọng liên quan đến câu hỏi trong prompt thường là động và thưa thớt. Các phương pháp nén prompt hiện có như LLMLingua (Jiang et al., 2023a) và Selective-Context (Li et al., 2023c) thường không xem xét câu hỏi trong quá trình nén, dẫn đến việc giữ lại nhiều nhiễu và giảm hiệu suất. LongLLMLingua nhằm cải thiện nhận thức của LLMs về thông tin quan trọng liên quan đến câu hỏi, từ đó vượt qua các vấn đề nhiễu và thiên vị vị trí trong ngữ cảnh dài, được thể hiện trong Hình 1b. Nguyên lý cơ bản của LongLLMLingua là các LM nhỏ có khả năng nắm bắt phân phối thông tin quan trọng liên quan đến một câu hỏi cho trước.

Các đóng góp chính của chúng tôi gồm năm khía cạnh: (1) Chúng tôi đề xuất phương pháp nén thô đến tinh có nhận thức câu hỏi để cải thiện mật độ thông tin quan trọng trong prompt (Sec. 4.1); (2) Chúng tôi giới thiệu chiến lược sắp xếp lại tài liệu để giảm thiểu thiên vị vị trí trong LLMs. (Sec. 4.2); (3) Chúng tôi thiết lập tỷ lệ nén động cho việc kiểm soát chính xác giữa các mức nén thô và tinh (Sec. 4.3); (4) Chúng tôi đề xuất chiến lược khôi phục subsequence sau nén để cải thiện tính toàn vẹn của thông tin quan trọng (4.4). (5) Chúng tôi đánh giá LongLLMLingua trên năm benchmarks, tức là NaturalQuestions (Liu et al., 2024), LongBench (Bai et al., 2023), ZeroSCROLLS (Shaham et al., 2023), MuSicQue (Trivedi et al., 2022), và LooGLE (Li et al., 2023b), bao phủ nhiều tình huống ngữ cảnh dài khác nhau. Kết quả thử nghiệm cho thấy prompts nén của LongLLMLingua vượt trội so với prompts gốc về hiệu suất, hiệu quả chi phí và độ trễ hệ thống.

2 Công thức Bài toán
Theo LLMLingua (Jiang et al., 2023a), chúng tôi sử dụng x= (xins,xdoc1,···,xdocK,xque) để biểu diễn một prompt, bao gồm hướng dẫn xins, K tài liệu xdoci, và câu hỏi xque. Tuy nhiên, định nghĩa này có thể được điều chỉnh cho các tình huống cụ thể. Mục tiêu của hệ thống nén prompt có thể được công thức hóa như:

min ex Dϕ(y,ey) +λ∥ex∥0, (1)

trong đó ex biểu diễn prompt nén, một subsequence cấp token của x. y và ey biểu diễn kết quả do LLM tạo ra từ x và ex, tương ứng. Dϕ đo hàm khoảng cách, như KL divergence. λ đóng vai trò là siêu tham số cân bằng tỷ lệ nén. Ngoài ra, nghiên cứu này khám phá không gian hoạt động hoán vị trên K tài liệu (xdoc1,···,xdocK) để tối ưu hóa chung.

3 Sơ bộ: LLMLingua
LLMLingua (Jiang et al., 2023a) sử dụng mô hình ngôn ngữ nhỏ MS để đánh giá perplexity của mỗi token prompt, loại bỏ những token có perplexities thấp hơn. Phương pháp này dựa trên ý tưởng rằng các token với perplexities thấp hơn có ảnh hưởng không đáng kể đến tổng entropy gain của mô hình ngôn ngữ, ngụ ý rằng việc loại bỏ chúng ít ảnh hưởng đến hiểu biết ngữ cảnh của LLMs. Quá trình này được xem là ứng dụng của "LM is Compression" (Delétang et al., 2023). LLMLingua bao gồm ba thành phần chính: budget controller, iterative token-level prompt compression, và distribution alignment, được đánh dấu bằng văn bản in nghiêng trong Hình 2.

Budget controller gán các tỷ lệ nén khác nhau cho các phần khác nhau của prompt (tức là, instruction, demonstrations, question), thực hiện nén prompt cấp thô. Các bước tiếp theo bao gồm chia kết quả trung gian thành các segment và áp dụng nén cấp token một cách lặp đi lặp lại, trong đó perplexity của mỗi token dựa trên các segment nén trước đó. Để nhận biết các LLMs mục tiêu khác nhau, LLMLingua fine-tunes MS sử dụng dữ liệu từ LLM mục tiêu.

4 LongLLMLingua
LongLLMLingua xây dựng trên LLMLingua để nén prompts tốt hơn trong các tình huống ngữ cảnh dài. Nó giải quyết ba vấn đề chính trong việc xử lý ngữ cảnh dài, như được giới thiệu trong Sec. 1. Cách tiếp cận này tập trung vào việc làm cho LLMs hiệu quả hơn trong việc nhận diện thông tin quan trọng

--- TRANG 3 ---
Prompt Gốc LongLLMLingua
Prompt Nén Hướng dẫn: Trả lời câu hỏi dựa trên các đoạn văn cho sẵn. Chỉ đưa ra câu trả lời và không xuất ra bất kỳ từ nào khác. Sau đây là các đoạn văn cho sẵn.
Tài liệu 1: Alberic III of Dammartin
Alberic III of Dammartin (Aubry de Dammartin) (c. 1138 – 19 September 1200) là một bá tước Pháp và con trai của Alberic II, Count of Dammartin, và Clémence de Bar, con gái của Reginald I, Count of Bar…
Tài liệu 2:
…
Tài liệu N: Pope Agapetus II
Pope Agapetus II (chết 8 November 955) là giám mục của Rome và người cai trị Papal States từ 10 May 946 đến khi chết. Một ứng cử viên của princeps của Rome, Alberic II of Spoleto, nhiệm kỳ của ông diễn ra trong …
Câu hỏi: Ai đã trao cho mẹ của Alberic II of Spoleto danh hiệu "patricia" của Rome?

Mô hình 
Nhỏ Black-box LLMs
~13k tokens Trả lời câu hỏi dựa trên các đoạn văn cho sẵn. …Đoạn văn 4: là một quý tộc Roman được cho là tình nhân của Pope Sergius III và được trao danh hiệu chưa từng có senatrix ("senatoress") và patricia of Rome bởi Pope John X., khi Ottosys, của và, được để thảo luận Rome và những vấn đề quan trọng khác, được.us là để tranh chấp the the của Re… Ai đã trao cho mẹ của Alberic II of Spoleto danh hiệu "patricia" của Rome? ~2k tokens

I Budget Controller
Nén Thô có Nhận thức Câu hỏi
                
w/ sắp xếp lại tài liệu
II Nén Tinh cấp Token Lặp đi lặp lại            
có Nhận thức Câu hỏi
w/ tỷ lệ nén động
0 Distribution 
    Alignment
III Thực thi với  
Prompt Nén
Khôi phục 
Subsequence
Phản hồi IV Hình 2: Framework của LongLLMLingua. Nội dung in nghiêng màu xám: Như trong LLMLingua.

liên quan đến câu hỏi trong prompt. Nó bao gồm ba góc độ và tích hợp thêm chiến lược khôi phục subsequence, như được thể hiện trong Hình 2, để nâng cao độ chính xác và độ tin cậy của thông tin cung cấp cho người dùng. Trong phần này, chúng tôi mô tả chi tiết cách mỗi phần của LongLLMLingua hoạt động để cải thiện LLMs xử lý ngữ cảnh dài.

4.1 Làm thế nào để cải thiện mật độ thông tin quan trọng trong prompt?

Nén Thô có Nhận thức Câu hỏi
Trong nén thô, chúng tôi nhằm tìm ra một metric rk để đánh giá tầm quan trọng của mỗi tài liệu xdock={xdock,i}Nki=1, trong đó Nk là số token trong xdock. Chúng tôi chỉ giữ xdock với rk cao hơn làm kết quả nén trung gian. Một cách tiếp cận để cải thiện mật độ thông tin quan trọng trong prompts nén là tính perplexity cấp tài liệu có điều kiện trên câu hỏi p(xdock|xque). Tuy nhiên, phương pháp này có thể không hiệu quả vì tài liệu thường chứa một lượng lớn thông tin không liên quan. Ngay cả khi có điều kiện trên xque, điểm perplexity được tính cho toàn bộ tài liệu có thể không đủ khác biệt, làm cho chúng trở thành metric không phù hợp cho nén cấp tài liệu.

Chúng tôi đề xuất sử dụng perplexity của câu hỏi xque có điều kiện trên các ngữ cảnh khác nhau xdock p(xque|xdock) để biểu diễn mối liên kết giữa chúng. Chúng tôi cũng thêm một câu hạn chế2 xrestrict sau xque để tăng cường sự liên kết giữa xque và xdock. Nó có thể được coi là một regularization term giảm thiểu tác động của hallucinations.

2Cụ thể, "Chúng ta có thể nhận được câu trả lời cho câu hỏi này trong các tài liệu đã cho".

Điều này có thể được công thức hóa như:

rk=−1NcNc∑i logp(xque,restricti |xdock),
k∈ {1,2,···, K},(2)

trong đó xque,restricti là token thứ i trong chuỗi nối của xque và xrestrict và Nc là số token.

Hình 3a hiển thị phân phối recall của các phương pháp retrieval khác nhau, bao gồm các phương pháp relevance truyền thống (BM25, Gzip (Jiang et al., 2023b)), các phương pháp dựa trên embedding (OpenAI-embedding, Voyageai3, BGE-large-en v1.5 (Xiao et al., 2023), Sentence-BERT (Reimers and Gurevych, 2019), Jina (Günther et al., 2023)), và các phương pháp reranker (Cohere-Rerank4, BGE-llmembeder, BGE-Ranker-large), điều này chứng minh rằng cách tiếp cận nén cấp thô của chúng tôi đạt được recall cao nhất với số lượng tài liệu được giữ lại khác nhau, cho thấy rằng nó bảo tồn thông tin quan trọng nhất từ ngữ cảnh trong kết quả nén.

Nén Tinh có Nhận thức Câu hỏi
Trong nén tinh, chúng tôi đánh giá tầm quan trọng của mỗi token trong hướng dẫn xins, câu hỏi xque, và K′ tài liệu {xdoci}K′i=1 được giữ lại sau nén thô. Chúng tôi tích hợp

3https://www.voyageai.com/
4https://cohere.com/rerank

--- TRANG 4 ---
[Hình 3 và mô tả tiếp theo đã được dịch...]

cơ chế nén lặp theo LLMLingua và tính toán trực tiếp perplexities token để nén xins và xque. Trong phần này, chúng tôi nghiên cứu cách làm cho nén cấp token tinh trên {xdock}K′k=1 nhận thức về câu hỏi xque, để kết quả nén có thể chứa nhiều thông tin quan trọng liên quan đến câu hỏi hơn.

Một giải pháp đơn giản cho việc nhận thức xque là nối nó vào đầu toàn bộ ngữ cảnh. Tuy nhiên, điều này sẽ dẫn đến perplexities thấp của các token liên quan trong ngữ cảnh theo điều kiện của câu hỏi xque, làm giảm thêm sự khác biệt của chúng với các token khác.

Trong bài báo này, chúng tôi đề xuất contrastive perplexity, tức là, sự dịch chuyển phân phối do điều kiện của câu hỏi gây ra, để biểu diễn mối liên kết giữa token và câu hỏi. Metric tầm quan trọng dựa trên contrastive perplexity si cho mỗi token xi trong {xdock}K′k=1 có thể được công thức hóa như:

si=perplexity (xi|x<i)−perplexity (xi|xque, x<i).
(3)

Ngoài ra, chúng tôi cung cấp sự dẫn xuất ý nghĩa toán học của nó trong Phụ lục A, kết luận rằng nó tương đương với conditional pointwise mutual information (Church and Hanks, 1989).

Hình 3b minh họa sự khác biệt giữa perplexities và contrastive perplexities. Phân phối perplexities xuất hiện ngẫu nhiên, làm cho việc trích xuất thông tin liên quan đến câu hỏi trở nên thách thức. Tuy nhiên, các token với contrastive perplexities cao có xu hướng tụ lại gần tài liệu ground-truth, chứa thông tin liên quan đến câu hỏi. Điều này cho thấy contrastive perplexity được đề xuất có thể phân biệt tốt hơn các token liên quan đến câu hỏi, từ đó cải thiện mật độ thông tin quan trọng trong kết quả nén.

4.2 Làm thế nào để giảm mất mát thông tin ở giữa?

Như đã chứng minh trong Hình 1b, LLM đạt hiệu suất cao nhất khi thông tin liên quan xuất hiện ở đầu và giảm đáng kể nếu thông tin liên quan nằm ở giữa ngữ cảnh dài. Sau nén thô, chúng ta đã có được một tập tài liệu {xdock}K′k=1 với điểm tầm quan trọng tương ứng {rk}K′k=1 chỉ ra mối liên kết của chúng với câu hỏi xque. Do đó, chúng tôi sắp xếp lại tài liệu sử dụng điểm tầm quan trọng của chúng để tận dụng tốt hơn sự khác biệt nhận thức thông tin của LLMs ở các vị trí:

(xins,xdoc1,···,xdocK′,xque)rk−→
(xins,xdocr1,···,xdocrK′,xque)(4)

4.3 Làm thế nào để đạt được kiểm soát granular thích ứng trong quá trình nén?

Trong nén tinh, LLMLingua áp dụng cùng tỷ lệ nén trên tất cả tài liệu thu được từ budget controller. Tuy nhiên, mật độ thông tin quan trọng của các tài liệu khác nhau là khác nhau. Tài liệu càng liên quan đến câu hỏi

--- TRANG 5 ---
Prompt Gốc Tài liệu [1](Tiêu đề: Danh sách người nhận giải Nobel Vật lý) Giải Nobel Vật lý đầu tiên được trao vào năm 1901 cho {Wilhelm Conrad Röntgen}{Wilhelm Conrad Röntgen}, của Đức, …Tài liệu [1](Tiêu đề: Danh sách người nhận Nobel Vật lý) Giải Nobel đầu tiên 1 {Wilhelmgen}{Wilhelm gen}, của, người nhận, ….{Wilhelmgen} {Wilhelm gen}

Prompt Nén Phản hồi của LLMs Hình 4: Ví dụ về Khôi phục Subsequence, văn bản màu đỏ biểu diễn văn bản gốc, và văn bản màu xanh là kết quả sau khi sử dụng tokenizer LLaMA 2-7B.

thì càng nhiều ngân sách (tức là, tỷ lệ nén thấp hơn) chúng ta nên phân bổ cho nó. Do đó, chúng tôi kết nối nén thô với nén tinh và sử dụng điểm tầm quan trọng {rk}K′k=1 thu được từ nén thô để hướng dẫn phân bổ ngân sách trong nén tinh. Bằng cách này, chúng ta có thể đạt được kiểm soát granular thích ứng trên toàn bộ.

Cụ thể, chúng tôi đầu tiên xác định ngân sách ban đầu cho các tài liệu được giữ lại5 τdoc sử dụng budget controller của LLMLingua. Trong nén tinh, chúng tôi theo thuật toán nén cấp token lặp trong LLMLingua nhưng phân bổ động ngân sách nén τdock cho mỗi tài liệu xdock theo chỉ số xếp hạng I(rk) (ví dụ, 0, 1) của điểm tầm quan trọng từ nén thô. Trong bài báo này, chúng tôi sử dụng linear scheduler cho phân bổ thích ứng. Ngân sách của mỗi token xi có thể được công thức hóa như:

τi=τdock,∀xi∈xdock,
τdock= max(min((1 −2I(rk)K′)δτ+τdoc,1),0),
(5)

trong đó i và k là chỉ số của token và tài liệu, K′ biểu thị số tài liệu, và δτ là siêu tham số kiểm soát ngân sách tổng thể cho phân bổ động.

4.4 Làm thế nào để cải thiện tính toàn vẹn của thông tin quan trọng?

Trong quá trình tạo, LLMs có xu hướng sao chép các entity được tìm thấy trong prompt, như tên, địa điểm và tổ chức. Nén các entity này ở cấp token không ảnh hưởng đến hiểu biết nội dung semantic của LLMs nhưng có thể dẫn đến lỗi trong nội dung được tạo.

Do đó, chúng tôi đề xuất phương pháp khôi phục subsequence để khôi phục nội dung gốc trong phản hồi của LLMs. Phương pháp này dựa vào mối quan hệ subsequence giữa các token trong prompt gốc, prompt nén và phản hồi của LLMs, như được thể hiện trong Hình 4.

5Trong LLMLingua, đó là τdems cho demonstrations.

Quy trình tổng thể bao gồm: i) Lặp qua các token yl trong phản hồi của LLMs và chọn substring dài nhất eykey,l={yl, yl+1, ..., yr} xuất hiện trong prompt nén ex. ii) Tìm maximum common shortest subsequence xi,j={xi, xi+1, ..., xj} trong prompt gốc x, tương ứng với biểu diễn eykey,l trong prompt gốc (được tăng tốc sử dụng prefix trees hoặc sequence automata). iii) Thay thế các token phù hợp eykey,l trong phản hồi của LLMs với subsequence tương ứng xi,j từ prompt gốc. Để biết thêm chi tiết, vui lòng tham khảo Thuật toán 1.

Thuật toán 1 Thuật toán Khôi phục Subsequence cấp Token
Input: Prompt gốc x; prompt nén ex; phản hồi tạo của LLMs y.
1:Đặt danh sách phản hồi cuối cùng yrec=ϕ, chỉ số token bên trái của subsequence l là 0.
2:while l <y.len()do
3: ifSubstring yl∈exthen
4: Tìm substring dài hơn eykey,l={yl, yl+1, ..., yr} ∈ex.
5: Tìm maximum common shortest subsequence xi,j={xi, xi+1, ..., xj} trong prompt gốc x.
6: Thêm subsequence xi,j={xi, xi+1, ..., xj} vào phản hồi yrec.
7: Đặt chỉ số bên trái l thành r+ 1.
8: else
9: Thêm token yl vào phản hồi yrec.
10: Đặt chỉ số bên trái l thành l+ 1.
11: end if
12:end while
Output: Danh sách phản hồi cuối cùng yrec.

5 Thử nghiệm
Ở đây, chúng tôi nghiên cứu: (1) LongLLMLingua hiệu quả như thế nào? (2) LongLLMLingua hiệu quả như thế nào?

Chi tiết triển khai Trong bài báo này, chúng tôi sử dụng GPT-3.5-Turbo-06136 và LongChat-13B-16k làm LLMs mục tiêu, cả hai đều có thể truy cập qua OpenAI7 và HuggingFace8. Để đảm bảo kết quả ổn định và có thể tái tạo

6Đối với thử nghiệm với prompts gốc vượt quá 4k tokens, chúng tôi sử dụng GPT-3.5-Turbo-16k-0613.
7https://platform.openai.com
8https://huggingface.co/lmsys/longchat-13b-16k

--- TRANG 6 ---
kết quả, chúng tôi sử dụng greedy decoding và đặt temperature thành 0 trong tất cả thử nghiệm. Đối với các mô hình ngôn ngữ nhỏ được sử dụng để nén, chúng tôi áp dụng LLaMA-2-7B-Chat9, đã được căn chỉnh bằng supervised fine-tuning và RLHF. Chúng tôi triển khai cách tiếp cận của mình với PyTorch 1.13.1 và HuggingFace Transformers. Chúng tôi thiết lập siêu tham số theo LLMLingua ngoại trừ kích thước segment được sử dụng trong nén cấp token lặp được đặt thành 200 ở đây. Chi tiết thêm được cung cấp trong Phụ lục B.

Dataset & metric đánh giá Chúng tôi sử dụng NaturalQuestions cho tác vụ multi-document QA, và sử dụng LongBench và ZeroSCROLLS cho các tình huống ngữ cảnh dài chung. Chúng tôi cũng thử nghiệm trên các tác vụ multi-hop QA sử dụng dataset MuSiQue (Trivedi et al., 2022), và các tác vụ long dependency QA sử dụng benchmark LooGLE (Li et al., 2023b). Vui lòng tham khảo Phụ lục C để biết thêm chi tiết về datasets.

Baselines Chúng tôi bao gồm hai tập baselines trong các thử nghiệm sau:
(i) Phương pháp Dựa trên Retrieval. Chúng tôi đánh giá mối liên kết question-document trong prompt sử dụng năm phương pháp retrieval SoTA: BM25, Gzip (Jiang et al., 2023b), SentenceBERT (Reimers and Gurevych, 2019), OpenAI Embedding, và metric quan trọng rk của LongLLMLingua ranker cho nén thô. Đáng chú ý, nén dựa trên embedding model phản ánh phương pháp trong Xu et al. (2024). Chúng tôi loại bỏ các câu hoặc đoạn văn có độ liên quan thấp để đáp ứng giới hạn nén, duy trì trình tự tài liệu gốc.

(ii) Phương pháp Dựa trên Nén. Chúng tôi so sánh cách tiếp cận của mình với hai phương pháp tiên tiến cho nén prompt, tức là, Selective Context (Li et al., 2023c) và LLMLingua (Jiang et al., 2023a). Cả hai phương pháp đều sử dụng LLaMA-2-7B-Chat làm mô hình ngôn ngữ nhỏ để nén. Trong LLMLingua, cách tiếp cận coarse-to-fine được sử dụng để xử lý ràng buộc của tỷ lệ nén: prompt gốc đầu tiên được nén thành k lần ràng buộc ở cấp thô, trong đó k là hệ số kiểm soát granular; sau đó token-level được thực hiện để đạt ràng buộc tổng thể. Phương pháp của chúng tôi theo cùng logic coarse-to-fine để đạt được ràng buộc.

Kết quả chính Bảng 1 và 2 trình bày hiệu suất của các phương pháp khác nhau dưới các ràng buộc nén khác nhau. Có nhiều quan sát và kết luận: (1) LongLLMLingua của chúng tôi đạt hiệu suất tốt nhất trên các tác vụ và ràng buộc tỷ lệ nén khác nhau. So với prompt gốc, prompt nén của chúng tôi có thể đạt hiệu suất cao hơn với chi phí thấp hơn nhiều. Ví dụ, LongLLMLingua đạt được sự tăng hiệu suất 21.4% trên NaturalQuestions với tài liệu ground-truth ở vị trí thứ 10, trong khi số token đầu vào cho GPT3.5-Turbo ít hơn ~4x. (2) Các phương pháp dựa trên nén như Selective Context (Li et al., 2023c) và LLMLingua (Jiang et al., 2023a) hoạt động kém trên hầu hết các tác vụ, đặc biệt là những tác vụ có nhiều thông tin không liên quan trong prompt gốc. Điều này là do cơ chế nén dựa trên information entropy thuần túy của chúng, bao gồm quá nhiều nhiễu trong kết quả nén và thậm chí dẫn đến hiệu suất tệ hơn so với thiết lập zero-shot, ví dụ, trên NaturalQuestions. (3) Các phương pháp dựa trên retrieval hoạt động tốt với tỷ lệ nén thấp. Tuy nhiên, hiệu suất của chúng giảm khi nén tiến triển, ví dụ, 2x→4x; 3000 tokens → 2000 tokens. Điều này có thể do recall giảm. Hình 3a là minh họa các trường hợp trên NaturalQuestions. (4) LongLLMLingua cũng như metric nén thô rk của chúng tôi mạnh mẽ hơn nhiều so với tất cả baselines khác dưới các tác vụ và ràng buộc nén khác nhau. Với sự gia tăng của tỷ lệ nén, ví dụ, 2x→4x, LongLLMLingua thậm chí đạt được một chút tăng hiệu suất. Chúng tôi chủ yếu gán công này cho nén coarse-to-fine có nhận thức câu hỏi, có thể tìm ra thông tin quan trọng tốt hơn và đạt mật độ thông tin quan trọng cao hơn với tỷ lệ nén cao hơn. (5) Phương pháp reordering được đề xuất giúp ích không chỉ cho cách tiếp cận của chúng tôi mà còn cho các baselines khác, chứng minh rõ hiệu quả của nó. (6) So với kết quả với ràng buộc 2,000 tokens, hiệu suất tổng thể của 3,000 tokens đã cải thiện. LongLLMLingua thấy sự gia tăng 1.2 điểm trong điểm trung bình và tăng tốc 1.6x trong độ trễ end-to-end. Trong tình huống này, tỷ lệ recall của các phương pháp dựa trên retrieval đã tăng, dẫn đến cải thiện đáng kể trong độ chính xác của chúng. Ví dụ, BM25 đạt điểm trung bình 48.9.

Ngoài ra, chúng tôi cũng trình bày kết quả thử nghiệm trên các datasets như MuSicQue, LooGLE, ZEROSCROLLS, v.v., trong Phụ lục C.

Nghiên cứu Ablation Để đánh giá đóng góp của các thành phần khác nhau trong LongLLMLingua, chúng tôi

9https://ai.meta.com/llama/
9https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder

--- TRANG 7 ---
[Bảng 1 được dịch với đầy đủ nội dung và định dạng...]

giới thiệu các biến thể sau của nó cho nghiên cứu ablation. (1) Các biến thể về Question-aware Coarse-grained Compression, bao gồm: ours w/o Question-awareness, tính toán mức độ liên quan question-text rk sử dụng information entropy trong LLMLingua, ours w/ SBERT, sử dụng SBERT để tính rk, ours w/ p(xdock|xque,restricti), thay thế p(xque,restricti|xdock) bằng p(xdock|xque,restricti) trong Eq. (2), và ours w/o restrict, chỉ tính xác suất có điều kiện tương ứng với xque. (2) Ours w/o Question-aware Fine-grained, bỏ qua Eq. (3) và chỉ áp dụng Iterative Token-level Prompt Compression như LLMLingua. (3) Ours w/o Dynamic Compression Ratio, trong đó tất cả tài liệu chia sẻ cùng tỷ lệ nén trong nén tinh. (4) Ours w/o và (5) LLMLingua w/ Subsequence Recovery, loại bỏ hoặc thêm chiến lược khôi phục subsequence hậu xử lý. (6) Ours w/ GPT2-small, sử dụng mô hình GPT2-small làm MS.

Bảng 3, 4, và 7 cho thấy kết quả của nghiên cứu ablation trong các tác vụ khác nhau. Tóm lại, việc loại bỏ bất kỳ thành phần nào được đề xuất cho LongLLMLingua sẽ dẫn đến giảm hiệu suất bất kể vị trí của câu trả lời ground-truth. Điều này xác nhận rõ sự cần thiết và hiệu quả của cơ chế nhận thức câu hỏi được đề xuất trong quá trình nén coarse-to-fine, tỷ lệ nén động, và chiến lược khôi phục subsequence. Nó cũng cho thấy việc áp dụng SBERT cho nén thô sẽ dẫn đến hiệu suất kém hơn, ngụ ý sự vượt trội của metric tầm quan trọng có nhận thức câu hỏi của chúng tôi trong Eq. (2) so với SBERT. Ngoài ra, việc thay thế p(xque,restricti|xdock) bằng p(xdock|xque,restricti) có thể ảnh hưởng lớn đến hiệu suất do nhiễu lớn trong việc tính p(xdock) vì perplexity của tài liệu phụ thuộc vào nhiều thông tin khác ngoài câu hỏi. Việc loại bỏ câu hạn chế có thể tăng hallucination của các mô hình ngôn ngữ nhỏ, dẫn đến giảm hiệu suất. Hơn nữa, chiến lược khôi phục subsequence của chúng tôi cũng có thể mang lại tăng hiệu suất cho LLMLingua. Tuy nhiên, không có cơ chế nhận thức câu hỏi của chúng tôi, kết quả từ LLMLingua vẫn ít thỏa mãn hơn. Để biết thêm các trường hợp chi tiết, vui lòng đến Phụ lục E.

--- TRANG 8 ---
[Bảng 2 được dịch đầy đủ...]

[Bảng 3 được dịch đầy đủ...]

Đánh giá Độ trễ Chúng tôi thực hiện thử nghiệm độ trễ end-to-end trên V100-32G, sử dụng prompts từ Multi-document QA, LongBench, và ZeroSCROLLS trong API call, và kết quả được hiển thị trong Bảng 1, 2 và 6. Độ trễ bao gồm thời gian chi phí cho nén prompt và thời gian request cho LLMs, với nhiều phép đo được thực hiện và tính trung bình. Kết quả chứng minh rằng LongLLMLingua thực sự tăng tốc suy luận tổng thể dưới các tỷ lệ nén và tình huống khác nhau. Hơn nữa, với tỷ lệ nén tăng, hiệu ứng tăng tốc trở nên rõ rệt hơn lên đến 2.6x. Tuy nhiên, OpenAI embedding và Selective-Context dẫn đến thời gian độ trễ dài hơn, do các API calls lặp lại và tính toán entropy tuần tự của các semantic units, tương ứng.

6 Các Công trình Liên quan
Ngữ cảnh dài cho LLMs. Nghiên cứu gần đây đã tập trung vào việc mở rộng window size của LLMs. Các cách tiếp cận chính bao gồm: (1) Staged pre-training (Nijkamp et al., 2023) tăng dần context window; (2) Sửa đổi (Press et al., 2022) hoặc nội suy position embeddings (Chen et al., 2023; Peng et al., 2024); (3) Sử dụng cơ chế attention tuyến tính hoặc thưa thớt (Ding et al., 2023; Sun et al., 2023); (4) Sử dụng các mô-đun bộ nhớ ngoài để lưu trữ ngữ cảnh (Bertsch et al., 2023; Tworkowski et al., 2023). Trong khi các phương pháp này giải quyết việc mở rộng context window, tác động của chúng đến hiệu suất tác vụ downstream vẫn chưa được thảo luận.

Phân phối thông tin trong prompt. Các thử nghiệm thực nghiệm gần đây đã cho thấy hiệu suất LLM giảm với thông tin hiệu quả ít hơn

--- TRANG 9 ---
trong prompt (Bai et al., 2023; Li et al., 2023a; Shi et al., 2023). Hơn nữa, vị trí của thông tin liên quan trong prompt có tác động đáng kể đến hiệu suất (Wu et al., 2023b). Liu et al. (2024) cho rằng LLMs gặp khó khăn hơn trong việc hiểu thông tin nằm ở giữa prompt so với những thông tin ở các cạnh.

Các phương pháp retrieval có thể được phân loại là phương pháp retrieval dense hoặc sparse. Các phương pháp retrieval sparse, như BM25, xác định mức độ liên quan giữa queries và documents dựa trên thông tin n-gram. Ngược lại, các phương pháp retrieval dense đánh giá mức độ liên quan giữa queries và documents trong không gian latent sử dụng embedding model (Reimers and Gurevych, 2019; Xiao et al., 2023; Günther et al., 2023) và reranker model (Xiao et al., 2023). Gần đây, Jiang et al. (2023b) đã đề xuất một phương pháp retrieval dense không giám sát tận dụng các thuật toán nén truyền thống, như gzip, và k-nearest neighbors.

Các phương pháp nén prompt có thể được nhóm thành ba loại chính: (1) Token pruning (Goyal et al., 2020; Kim and Cho, 2021; Modarressi et al., 2022) và token merging (Bolya et al., 2023), cần fine-tuning mô hình hoặc kết quả trung gian trong quá trình suy luận và đã được sử dụng với các mô hình quy mô BERT. (2) Các phương pháp soft prompt tuning như GIST (Mu et al., 2023), AutoCompressor (Chevalier et al., 2023), và ICAE (Ge et al., 2024), yêu cầu fine-tuning tham số của LLMs, làm cho chúng phù hợp cho các miền cụ thể nhưng không áp dụng trực tiếp cho black-box LLMs. (3) Các cách tiếp cận dựa trên information-entropy như Selective Context (Li et al., 2023c) và LLMLingua (Jiang et al., 2023a), sử dụng mô hình ngôn ngữ nhỏ để tính self-information hoặc perplexity của mỗi token trong prompt gốc và sau đó loại bỏ các token có perplexities thấp hơn.

7 Kết luận
Chúng tôi đề xuất LongLLMLingua để giải quyết ba thách thức, tức là, chi phí tính toán cao hơn, giảm hiệu suất, và thiên vị vị trí cho LLMs trong các tình huống ngữ cảnh dài. Chúng tôi phát triển LongLLMLingua từ góc độ nén prompt hiệu quả, từ đó giảm chi phí tính toán. Chúng tôi tiếp tục thiết kế bốn thành phần, tức là, phương pháp nén coarse-to-fine có nhận thức câu hỏi, cơ chế sắp xếp lại tài liệu, tỷ lệ nén động, và chiến lược khôi phục subsequence để cải thiện nhận thức của LLMs về thông tin quan trọng, với đó LongLLMLingua chứng minh hiệu suất vượt trội. Các thử nghiệm trên multi-document QA, multi-hop QA, và các benchmark ngữ cảnh dài chứng minh rằng prompt nén LongLLMLingua có thể đạt hiệu suất cao hơn so với prompts gốc trong khi cả chi phí API để suy luận và độ trễ hệ thống end-to-end đều được giảm đáng kể.

Hạn chế
Mặc dù các thử nghiệm trước đây chứng minh hiệu quả và hiệu suất của LongLLMLingua trên một loạt rộng các tác vụ, phương pháp vẫn có những hạn chế sau: 1) LongLLMLingua là cách tiếp cận có nhận thức câu hỏi, có nghĩa là nó yêu cầu nén lại cho các câu hỏi khác nhau, ngay cả với cùng ngữ cảnh, ngăn cản việc caching ngữ cảnh. Hơn nữa, về chi phí tính toán, LongLLMLingua tăng tính toán gấp đôi so với LLMLingua. Điều này có thể dẫn đến overhead lớn hơn trong các ứng dụng thực tế. Tuy nhiên, vấn đề này có thể được giảm thiểu bằng cách mở rộng cách tiếp cận có nhận thức câu hỏi thành cách tiếp cận có nhận thức tác vụ, cho phép tái sử dụng và caching. 2) Trong khi hiệu quả của LongLLMLingua đã được thử nghiệm trên một loạt rộng các tác vụ, đặc biệt là trên dataset multi-hop QA MuSicQue (Trivedi et al., 2022), hiệu quả của nó có thể bị ảnh hưởng khi mối quan hệ giữa ngữ cảnh và prompt phức tạp và tinh tế hơn do cách tiếp cận có nhận thức câu hỏi cấp thô.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được dịch đầy đủ theo cấu trúc gốc...]

--- TRANG 10 ---
[Tiếp tục dịch danh sách tài liệu tham khảo...]

--- TRANG 11 ---
[Tiếp tục dịch danh sách tài liệu tham khảo...]

--- TRANG 12 ---
[Tiếp tục dịch và kết thúc danh sách tài liệu tham khảo...]

A Dẫn xuất Nén Tinh có Nhận thức Câu hỏi
Dựa trên định nghĩa của Eq. (3), chúng ta có thể dẫn xuất rằng,

si=perplexity (xi|x<i)−perplexity (xi|xque, x<i)
=q(xi) logp(xi|xque, x<i)−q(xi) logp(xi|x<i)
=q(xi) logp(xi|xque, x<i)p(xi|x<i)
(6)

Trong việc tính toán thực tế của perplexity, một phép toán log được thực hiện để tránh overflow, và q(xi) biểu diễn phân phối xác suất của ground-truth.

Đồng thời, chúng ta có thể dẫn xuất biểu thức mở rộng sau dựa trên định lý Bayes.

p(xque|xi, x<i) =p(xi|xque, x<i)p(xque)p(xi|x<i)
=p(xque)p(xi|xque, x<i)p(xi|x<i)(7)

Phân phối xác suất p(xque) của câu hỏi và phân phối ground-truth q(xi) của xi là các hằng số, do đó si có thể được coi là biểu diễn của Eq. (7).

si∝p(xque|xi, x<i) (8)

Vậy chúng ta có thể sử dụng Eq. (3) để biểu diễn phân phối xác suất p(xque|xi, x<i), biểu diễn likelihood có điều kiện của việc tạo xque cho token xi. Do đó, chúng ta có thể biểu diễn phân phối sensitive cấp token cho câu hỏi xque chỉ sử dụng một lần suy luận. Đối với các token không liên quan đến xque, như các token ở phía bên phải của Hình 3b, lượng thông tin gốc của chúng có thể cao, nhưng contrastive perplexity vẫn ở mức tương đối thấp. Cuối cùng, chúng ta quan sát rằng dạng của contrastive perplexity tương đương với conditional pointwise mutual information (Church and Hanks, 1989).

B Chi tiết Thử nghiệm
B.1 Chi tiết Dataset
[Nội dung phần B.1 được dịch đầy đủ...]

--- TRANG 13 ---
[Tiếp tục dịch phần B và các phần tiếp theo...]

--- TRANG 14 ---
[Tiếp tục dịch các bảng và nội dung...]

--- TRANG 15 ---
[Tiếp tục dịch các bảng và nội dung...]

--- TRANG 16 ---
[Tiếp tục dịch các bảng và nội dung...]

--- TRANG 17 ---
[Tiếp tục dịch các bảng và nội dung...]

--- TRANG 18 ---
[Tiếp tục dịch các hình ảnh case study và nội dung...]

--- TRANG 19 ---
[Tiếp tục dịch các hình ảnh case study và nội dung...]

--- TRANG 20 ---
[Dịch hình ảnh case study cuối cùng và kết thúc]
