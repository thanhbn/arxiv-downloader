## Tài liệu tham khảo

amazon. Mistrallite model. https://huggingface.co/amazon/MistralLite , 2023. [Trực tuyến; truy cập 29-December-2023].

An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., và Qiu, X. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088 , 2023.

Anothropic. Long context prompting for claude 2.1. https://www.anthropic.com/news/claude-2-1-prompting , 2023.

Bai, T., Luo, J., Zhao, J., Wen, B., và Wang, Q. Recent advances in adversarial training for adversarial robustness. arXiv preprint arXiv:2102.01356 , 2021.

Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508 , 2023.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems , 33: 1877–1901, 2020.

Chen, G., Li, X., Meng, Z., Liang, S., và Bing, L. Clex: Continuous length extrapolation for large language models.arXiv preprint arXiv:2310.16450 , 2023a.

Chen, S., Wong, S., Chen, L., và Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595 , 2023b.

Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., và Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307 , 2023c.

Chuang, Y.-N., Xing, T., Chang, C.-Y., Liu, Z., Chen, X., và Hu, X. Learning to compress prompt in natural language formats. arXiv preprint arXiv:2402.18700 , 2024.

Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., và Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 , 2018.

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., và Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.

Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., và Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 , 2019.

Dao, T., Fu, D., Ermon, S., Rudra, A., và Ré, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344–16359, 2022.

Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., và Zou, A. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836 .

gkamradt. Llmtest needleinahaystack: Doing simple retrieval from llm models. https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main , 2023. [Trực tuyến; truy cập 29-December-2023].

Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., và Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137 , 2023.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., và Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.

Javaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck, S., Mendes, C. C. T., Chen, W., Giorno, A. D., Eldan, R., Gopi, S., Gunasekar, S., Javaheripi, M., Kauffmann, P., Lee, Y. T., Li, Y., Nguyen, A., de Rosa, G., Saarikivi, O., Salim, A., Shah, S., Santacroce, M., Behl, H. S., Kalai, A. T., Wang, X., Ward, R., Witte, P., Zhang, C., và Zhang, Y. Phi-2: The surprising power of small language models, 2023.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023a.

Jiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., và Qiu, L. LongLLMLingua: Accelerating and enhancing llms in long context scenarios via prompt compression. ArXiv preprint , abs/2310.06839, 2023b. URL https://arxiv.org/abs/2310.06839 .

Ke, G., He, D., và Liu, T.-Y. Rethinking positional encoding in language pre-training. arXiv preprint arXiv:2006.15595 , 2020.

Kim, D., Park, C., Kim, S., Lee, W., Song, W., Kim, Y., Kim, H., Kim, Y., Lee, H., Kim, J., et al. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166 , 2023.

Lin, S., Hilton, J., và Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958 , 2021.

Liu, J., Shen, Z., He, Y., Zhang, X., Xu, R., Yu, H., và Cui, P. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624 , 2021.

Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman, V., Chen, B., và Hu, X. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750 , 2024.

Mohtashami, A. và Jaggi, M. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300 , 2023.

Pal, A., Karkhanis, D., Roberts, M., Dooley, S., Sundararajan, A., và Naidu, S. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint arXiv:2308.10882 , 2023.

Peng, B., Quesnelle, J., Fan, H., và Shippole, E. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071 , 2023.

Press, O., Smith, N. A., và Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409 , 2021.

Rae, J. W., Potapenko, A., Jayakumar, S. M., và Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507 , 2019.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., và Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.

Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 , 2023.

Shen, Z., Liu, J., He, Y., Zhang, X., Xu, R., Yu, H., và Cui, P. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624 , 2021.

Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., và Kwok, J. T.-Y. Sparsebert: Rethinking the importance analysis in self-attention. In International Conference on Machine Learning , pp. 9547–9557. PMLR, 2021.

Su, J. Rectified rotary position embeddings. https://github.com/bojone/rerope , 2023.

Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., và Liu, Y. RoFormer: Enhanced transformer with rotary position embedding, 2022. arXiv: 2104.09864.

Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., và Wei, F. A length-extrapolatable transformer. In Rogers, A., Boyd-Graber, J., và Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 14590–14604, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.816. URL https://aclanthology.org/2023.acl-long.816 .

Team, M. N. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b . Truy cập: 2023-05-05.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., và Polosukhin, I. Attention is all you need. Advances in neural information processing systems , 30, 2017.

Wu, K., Peng, H., Chen, M., Fu, J., và Chao, H. Rethinking and improving relative position encoding for vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 10033–10041, 2021.

Xiao, G., Tian, Y., Chen, B., Han, S., và Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453 , 2023.

Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039 , 2023.

Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., và Raffel, C. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934 , 2020.

Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., Yin, B., và Hu, X. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712 , 2023.

Yin Song và Chen Wu và Eden Duthie. amazon/Mistral-Lite, 2023. URL https://huggingface.co/amazon/MistralLite .

Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems , 33:17283–17297, 2020.

Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., và Choi, Y. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.

Zhang, J., Chao, H., Dhurandhar, A., Chen, P.-Y., Tajer, A., Xu, Y., và Yan, P. When neural networks fail to generalize? a model sensitivity perspective. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pp. 11219–11227, 2023.

Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.

Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223 , 2023.

Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., và Li, S. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400 , 2023.

## A. Mã giả của SelfExtend

**Thuật toán 1** Mã giả theo kiểu PyTorch của SelfExtend
```
q, k, v # queries, keys, và values
seq_len, pos # độ dài chuỗi đầu vào, position_idx
g_size, w_size = G, w_n

# self-attention bình thường
ngb_q = apply_pos_emcode(q, pos)
ngb_k = apply_pos_emcode(k, pos)
ngb_attn = matmul(ngb_q, ngb_k)
ngb_attn = causal_mask(ngb_attn)

# grouped self-attention
g_pos = pos // g_size # phép toán floor
shift = w_size - w_size // g_size
s_g_pos = g_pos + shift
g_q = apply_pos_emcode(q, s_g_pos)
g_k = apply_pos_emcode(k, g_pos)
g_attn = matmul(g_q, g_k)
g_attn = causal_mask(g_attn)

g_mask = tril(ones([seq_len - w_size, seq_len - w_size]))
mask = ones([seq_len, seq_len])
mask[w_size:, :-w_size] -= g_mask
attn = where(mask, ngb_attn, g_attn) # hợp nhất bằng thay thế

attn_weights = softmax(attn)
output = matmul(attn_weights, v)
```

## B. Perplexity như một Metric cho Khả năng Ngữ Cảnh Dài

PPL không phải là một metric hiệu quả để đo khả năng của LLM xử lý ngữ cảnh dài. Trong Hình 7, chúng tôi giới thiệu một phương pháp mở rộng cửa sổ ngữ cảnh có vẻ khả thi tên là 'Infinite'. Khi được đánh giá trên PG19 sử dụng cùng protocol, Llama-2-7b-chat với 'Infinite' đạt được điểm PPL tương đương hoặc thậm chí thấp hơn so với những điểm đạt được bởi SelfExtend, như được chứng minh trong Bảng 6. Tuy nhiên, 'Infinite' về cơ bản bắt chước quá trình chia một chuỗi dài thành các chuỗi con ngắn trước khi xử lý chúng với LLM, cho thấy rằng nó không thực sự giải quyết việc xử lý ngữ cảnh dài.

**Bảng 6.** Perplexity trên dataset PG19: Đối với 'Infinite', chúng tôi đặt ba kích thước cửa sổ địa phương khác nhau: 1024, 2048, và 4096. Chúng tôi cũng đã bao gồm kết quả từ Bảng 1 để so sánh.

| Tên Mô hình | Kích thước Cửa sổ Ngữ cảnh Đánh giá |
|-------------|-------------------------------------|
|             | 4096 | 6144 | 8192 | 10240 | 12288 | 14336 | 16384 |
| Llama-2-7b-chat | 9.181 | >103 | >103 | >103 | >103 | >103 | >103 |
| SelfExtend-Llama-2-7b-chat | 8.885 | 8.828 | 9.220 | 8.956 | 9.217 | 9.413 | 9.274 |
| 1024-'Infinite'–Llama-2-7b-chat | 9.556 | 9.393 | 9.728 | 9.266 | 9.400 | 9.369 | 9.142 |
| 2048-'Infinite'–Llama-2-7b-chat | 9.288 | 9.045 | 9.478 | 8.993 | 9.128 | 9.105 | 8.872 |
| 4096-'Infinite'–Llama-2-7b-chat | 9.181 | 9.045 | 9.506 | 8.993 | 9.165 | 9.105 | 8.856 |
| Mistral-7b-instruct-0.1 w/ SWA | 9.295 | 9.197 | 9.532 | 9.242 | 9.198 | 9.278 | 9.294 |
| Mistral-7b-instruct-0.1 w/o SWA | 9.295 | 9.205 | 10.20 | 55.35 | >103 | >103 | >103 |
| SelfExtend-Mistral-7b-instruct-0.1 | 9.272 | 9.103 | 9.369 | 9.070 | 8.956 | 9.022 | 9.128 |

Sự khác biệt giữa Perplexity (PPL) và khả năng ngữ cảnh dài chủ yếu xuất phát từ cách PPL được tính toán bằng cách lấy trung bình trên nhiều token. Miễn là phần lớn token được mô hình hóa chính xác, PPL sẽ vẫn thấp. Điều này liên quan chặt chẽ đến ảnh hưởng của các neighbor token. Thông tin từ các neighbor token—chẳng hạn như những token trong cửa sổ attention địa phương của 'Infinite'—có thể đủ để dự đoán hầu hết token, do đó dẫn đến PPL thấp. Tuy nhiên, một vài token quan trọng, rất quan trọng để hiểu ngữ cảnh dài và trả lời câu hỏi, có thể không được dự đoán chính xác.

Ngoài ra, không giống như quá trình tiền huấn luyện nơi loss cross-entropy tương ứng trực tiếp với perplexity, việc đo PPL trong quá trình suy luận là tĩnh. Nó giống như một điểm cụ thể trên đường cong loss quan sát được trong quá trình tiền huấn luyện. Trong khi xu hướng giảm trong loss trong quá trình tiền huấn luyện chỉ ra hiệu suất tốt, một điểm duy nhất trên đường cong training loss không thể xác định hiệu suất.

Tóm lại, trong khi PPL thấp là cần thiết cho một mô hình tốt, PPL thấp hơn không nhất thiết bằng hiệu suất tốt hơn trong việc hiểu ngữ cảnh dài.

## C. SelfExtend với Kích thước Nhóm và Cửa sổ Neighbor Thay đổi

Để hiểu toàn diện ảnh hưởng của SelfExtend đối với LLM, không giống như các thí nghiệm trước đây sử dụng cài đặt ngữ cảnh dài, chúng tôi đánh giá với kích thước cửa sổ neighbor nhỏ hơn trên bốn tác vụ benchmark tiêu chuẩn: ARC-c, GSM8k, Hellaswag và MMLU. Chúng tôi sử dụng Phi-2 như LLM mở rộng. Kết quả được hiển thị trong Hình 8. Chúng tôi không bao gồm TruthfulQA vì độ dài trung bình của nó ít hơn 300 từ, trong khi bốn dataset chúng tôi sử dụng có độ dài trung bình lớn hơn 700 từ.

Nói chung, SelfExtend có ảnh hưởng nhỏ đối với Phi-2 miễn là kích thước cửa sổ neighbor trên 128. Trong nhiều trường hợp, SelfExtend thậm chí hoạt động tốt hơn một chút so với Phi-2 vanilla. Khi cửa sổ neighbor quá nhỏ (ví dụ 64 token), nếu kích thước nhóm lớn, như dự kiến, mất thông tin vị trí quá cao và hiệu suất của Phi-2 suy giảm. Ngoài ra, trên các tác vụ khó như MMLU và Helleswag, chúng tôi quan sát sự giảm đơn điệu trong hiệu suất với kích thước nhóm tăng cho tất cả cửa sổ neighbor. Tóm lại, ngay cả khi áp dụng SelfExtend cho các tác vụ ngữ cảnh ngắn, miễn là các hyperparameter không cực đoan, SelfExtend không làm hại mô hình.

## D. Cài đặt Thí nghiệm Chi tiết

Trong phụ lục này, chúng tôi trình bày chi tiết các thí nghiệm trong bài báo của chúng tôi.

### D.1. Cài đặt Thí nghiệm trên Các Tác vụ Mô hình hóa Ngôn ngữ

Đây không phải là cài đặt tiêu chuẩn để thử nghiệm PPL trên PG-19. Chúng tôi sử dụng câu đầu tiên của mỗi cuốn sách trong tập test của PG19 (100 cuốn sách) để thử nghiệm khả năng mô hình hóa ngôn ngữ. Kết quả không thể được so sánh trực tiếp với PPL được báo cáo bởi các bài báo khác. Chúng tôi chọn cài đặt này vì tài nguyên tính toán của chúng tôi rất hạn chế. Cài đặt này tiết kiệm rất nhiều và vẫn có thể hiển thị hành vi của LLM đối với PPL. Tất cả kết quả PPL được tính toán sử dụng phương pháp sliding window (Press et al., 2021) với S= 256. Chúng tôi đánh giá cách PPL thay đổi khi độ dài đầu vào tăng. Trong Bảng 1, SelfExtend mở rộng độ dài cửa sổ ngữ cảnh ban đầu của Llama-2 từ 4096 (4k) lên hơn 16384 (16k) với kích thước nhóm Gs được đặt là 8 và cửa sổ neighbor wn được đặt là 1024 (1k). Đối với mô hình Mistral, không có SWA, cửa sổ ngữ cảnh là 8192 (8k) và nó cũng được mở rộng bởi SelfExtend với cùng cài đặt lên lớn hơn 16k. Với SWA, Mistral có thể tiêu hóa một độ dài vô hạn của các chuỗi và sliding window mặc định của nó là 4096.

### D.2. Cài đặt Thí nghiệm trên Tác vụ Passkey Retrieval

So với các tác vụ tổng hợp khác, như "Needle in a Haystack" (gkamradt, 2023), hiệu suất của mô hình trên điều này không nhạy cảm với prompt (Anothropic, 2023). Điều này có thể đến từ thực tế rằng câu mang passkey rất khác so với những văn bản ngẫu nhiên lặp lại xung quanh nó. Theo kinh nghiệm, trong cửa sổ ngữ cảnh hiệu quả, hầu như tất cả LLM, bao gồm cả những LLM không có bất kỳ instruction tuning hoặc alignment nào, có thể định vị câu mang passkey. Mặc dù tác vụ này dễ và xa rời các tình huống thực tế, nó kiểm tra hai khả năng cơ bản của LLM: 1. Mô hình nên có thể nhận ra và định vị thông tin hữu ích trên tất cả vị trí của chuỗi đầu vào (khả năng hiểu cơ bản nhất); 2. Mô hình nên có thể sử dụng thông tin được nhận thức để hoàn thành các tác vụ (khả năng tạo ra cơ bản nhất).

Một ví dụ về passkey như sau:

**Ví dụ:**
Prompt: Có một thông tin quan trọng được ẩn bên trong rất nhiều văn bản không liên quan. Tìm và ghi nhớ nó. Tôi sẽ hỏi bạn về thông tin quan trọng đó... trở lại một lần nữa. Cỏ màu xanh lá. Bầu trời màu xanh dương. Mặt trời màu vàng. Đây chúng ta đi. Đi và quay lại.Cỏ màu xanh lá. Bầu trời màu xanh dương. Mặt trời màu vàng. Đây chúng ta đi. Đi và quay lại.Cỏ màu xanh lá. Bầu trời màu xanh dương. Mặt trời màu vàng. Đây chúng ta đi. Đi và quay lại. Khóa pass là 60151. Hãy nhớ nó. 60151 là khóa pass. Cỏ màu xanh lá. Bầu trời màu xanh dương. Mặt trời màu vàng. Đây chúng ta đi. Đi và quay lại.Cỏ màu xanh lá. Bầu trời... Khóa pass là gì?

Đáp án đúng: 60151

### D.3. Cài đặt Thí nghiệm trên Tác vụ Passkey Retrieval Với Độ Dài Thay đổi

Trong thí nghiệm này, chúng tôi sử dụng các mô hình sau: Llama2-7b-chat với SelfExtend, LongLora-7b-16k6, vicuna-1.5-7b-16k, Together AI's Llama-2-7b-32k7, và Yarn-Llama-2-7b-64k.

## E. Chi tiết về LLM

Ở đây, chúng tôi liệt kê các liên kết đến chi tiết của các LLM được sử dụng trong các thí nghiệm của chúng tôi.

**Bảng 7.** LLM được sử dụng trong các thí nghiệm

| Tên Mô hình | URL |
|-------------|-----|
| Llama-2-7b-chat-hf (Touvron et al., 2023) | https://huggingface.co/meta-llama/Llama-2-7b-chat-hf |
| Mistral-7B-Instruct-v0.1 (Jiang et al., 2023a) | https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 |
| Phi-2 (Javaheripi et al., 2023) | https://huggingface.co/microsoft/phi-2 |
| SOLAR-10.7B-Instruct-v1.0 (Kim et al., 2023) | https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0 |
| LongChat-7b-v1.5-32k | https://huggingface.co/lmsys/longchat-7b-v1.5-32k |
| togethercomputer/LLaMA-2-7B-32K | https://huggingface.co/togethercomputer/LLaMA-2-7B-32K |
| CLEX-7B-16K (Chen et al., 2023a) | https://huggingface.co/DAMO-NLP-SG/CLEX-7B-16K |
| CodeLlama-7b-hf (Rozière et al., 2023) | https://huggingface.co/codellama/CodeLlama-7b-hf |
| vicuna-7b-v1.5-16k | https://huggingface.co/lmsys/vicuna-7b-v1.5-16k |
| MistralLite (amazon, 2023) | https://huggingface.co/amazon/MistralLite |

## F. SelfExtend với Các Mã hóa Vị trí Khác

Trong phần này, chúng tôi thử nghiệm SelfExtend trên các LLM sử dụng mã hóa vị trí không phải RoPE. Chúng tôi triển khai SelfExtend cho MPT-7b-chat (Team, 2023), sử dụng Alibi (Press et al., 2021) như phương pháp embedding vị trí của nó. Chúng tôi tiến hành thí nghiệm trên dataset PG19. Kết quả được hiển thị trong Bảng 8. Kết quả cho thấy SelfExtend có thể hoạt động với các mã hóa vị trí không phải RoPE, khá tương tự như các mô hình với mã hóa vị trí RoPE.

**Bảng 8.** Perplexity của MPT-7b-chat trên PG19 với độ dài chuỗi khác nhau. MPT-7b-chat vanilla có cửa sổ ngữ cảnh 2k token. Đối với SelfExtend, chúng tôi đặt cửa sổ neighbor là 512 và đặt kích thước nhóm là 6.

| MPT-7b-chat | 1024 | 2048 | 3172 | 4096 | 5120 | 6144 | 8192 |
|-------------|------|------|------|------|------|------|------|
| Vanilla (2k) | 8.8 | 9.6 | 12.0 | 26.3 | 52.0 | 115.5 | 196.0 |
| SelfExtend | 8.9 | 9.9 | 10.6 | 10.8 | 11.1 | 11.3 | 11.6 |

## G. Lựa chọn Hyperparameter cho SelfExtend

Chúng tôi tiến hành thí nghiệm trên "Needle in a Haystack" (gkamradt, 2023), để điều tra tác động của kích thước nhóm và kích thước cửa sổ neighbor. Kết quả được hiển thị trong Hình 10.

Kết quả thí nghiệm cho thấy SelfExtend không quá nhạy cảm với việc lựa chọn hyperparameter. Các giá trị được định nghĩa trước, heuristic cho kích thước nhóm và kích thước cửa sổ neighbor thường đủ để đạt được hiệu suất thỏa đáng, miễn là kích thước nhóm và cửa sổ neighbor không quá lớn hoặc quá nhỏ. Chúng tôi kết luận những kết quả này như một quy tắc kinh nghiệm. Ký hiệu cửa sổ ngữ cảnh tiền huấn luyện là L, độ dài mở rộng mục tiêu là N, cửa sổ neighbor là W, và kích thước nhóm là G, quy tắc kinh nghiệm để lựa chọn hyperparameter là đảm bảo rằng bất đẳng thức sau được thỏa mãn:

$$\frac{1}{2} \times L > W + \frac{N-W}{G}$$ (5)

Chúng tôi tin rằng quy tắc kinh nghiệm này là do thực tế rằng: các vị trí tương đối lớn không được huấn luyện tốt. Theo kinh nghiệm, chỉ một phần (~1/2) vị trí được huấn luyện tốt và SelfExtend chỉ nên tận dụng những vị trí tương đối được huấn luyện tốt này cho việc mở rộng. Phát hiện này giải thích: kích thước nhóm quá nhỏ có thể làm suy giảm hiệu suất, vì chúng cung cấp thông tin vị trí chính xác nhưng yêu cầu SelfExtend sử dụng các vị trí tương đối ít được huấn luyện hơn cho việc mở rộng; kích thước cửa sổ neighbor quá lớn cũng có thể làm suy giảm hiệu suất, vì chúng cung cấp thông tin neighbor nhiều hơn nhưng cần thiết phải sử dụng các vị trí tương đối ít được huấn luyện hơn cho việc mở rộng.

Tuy nhiên, quan sát hiện tại có thể không áp dụng cho tất cả mô hình. Ví dụ, chúng tôi đã phát hiện rằng chuỗi Llama3 nên sử dụng cửa sổ neighbor nhỏ hơn nhiều (~100). Chúng tôi có thể đi sâu hơn để điều tra tương tác giữa những hyperparameter và mô hình này. Bên cạnh việc tuân theo quy tắc kinh nghiệm. Người ta có thể sử dụng một tác vụ đại diện đơn giản và dễ chạy để tìm hyperparameter phù hợp.
