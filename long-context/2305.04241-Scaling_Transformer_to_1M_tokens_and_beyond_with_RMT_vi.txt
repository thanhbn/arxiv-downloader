# 2305.04241.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2305.04241.pdf
# Kích thước tệp: 1770664 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
VCC: Mở rộng Transformers lên 128K Tokens hoặc hơn
bằng cách Ưu tiên Các Tokens Quan trọng
Zhanpeng Zeng
University of Wisconsin, Madison
zzeng38@wisc.eduCole Hawkins
AWS AI
colehawk@amazon.com
Mingyi Hong
University of Minnesota, Minneapolis
mhong@umn.eduAston Zhang
AWS AI
astonz@amazon.comNikolaos Pappas
AWS AI
nppappa@amazon.com
Vikas Singh
University of Wisconsin, Madison
vsingh@biostat.wisc.eduShuai Zheng
AWS AI
shzheng@amazon.com

Tóm tắt
Transformers đóng vai trò trung tâm trong các ứng dụng xử lý ngôn ngữ tự nhiên và thị giác máy tính hiện đại. Mặc dù có những nghiên cứu gần đây nhằm giảm chi phí bậc hai của những mô hình này (như một hàm của độ dài chuỗi), việc xử lý các chuỗi cực dài (ví dụ, với hơn 16K tokens) vẫn là thách thức. Các ứng dụng như trả lời câu hỏi dựa trên một cuốn sách hoặc tóm tắt một bài báo khoa học là không hiệu quả hoặc không khả thi. Ở đây, chúng tôi đề xuất cải thiện đáng kể hiệu quả của Transformers cho các chuỗi cực dài, bằng cách nén chuỗi thành một biểu diễn nhỏ hơn nhiều ở mỗi lớp. Cụ thể, bằng cách khai thác thực tế rằng trong nhiều tác vụ, chỉ một tập con nhỏ các tokens đặc biệt (chúng tôi gọi là VIP-tokens) liên quan nhất đến dự đoán cuối cùng, chúng tôi đề xuất một sơ đồ nén tập trung vào VIP-token (VCC) có thể nén chuỗi một cách có chọn lọc dựa trên tác động của chúng lên việc xấp xỉ biểu diễn của VIP-tokens. So với các baseline cạnh tranh, thuật toán của chúng tôi không chỉ hiệu quả (đạt được hơn 3× cải thiện hiệu quả so với baseline trên độ dài 4K và 16K), mà còn mang lại hiệu suất cạnh tranh/tốt hơn trên một số lượng lớn các tác vụ. Hơn nữa, chúng tôi cho thấy thuật toán của chúng tôi có thể mở rộng lên 128K tokens (hoặc hơn) trong khi luôn mang lại cải thiện độ chính xác.

1 Giới thiệu
103104105
Độ dài chuỗi050010001500Thời gian chạy (ms)
103104105
Độ dài chuỗi0102030Bộ nhớ (GB)
Transformer Big Bird Longformer Ours
Hình 1: Hiệu quả mô hình khi xử lý một chuỗi trên A100 khi độ dài chuỗi tăng (lưu ý trục x logarit).Transformer [30] là một mô hình nền tảng cho xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính. Nó đã cho thấy hiệu suất đáng chú ý trên các ứng dụng NLP bao gồm dịch máy [30], suy luận ngôn ngữ [9], và tóm tắt [13]. Transformers cũng đã được áp dụng thành công cho các tác vụ nhận dạng hình ảnh khác nhau và đạt được kết quả ấn tượng [10,3,38]. Thật không may, nhu cầu thời gian chạy/bộ nhớ của Transformers liên quan đến sự phụ thuộc không thuận lợi vào độ dài chuỗi đầu vào, khiến việc sử dụng Transformers cho các ứng dụng chuỗi cực dài trở nên khó khăn. Do đó, nhiều Transformers sử dụng các chiến lược

Preprint. Đang được xem xét.arXiv:2305.04241v2  [cs.CL]  27 May 2023

--- TRANG 2 ---
như cắt bỏ để đảm bảo rằng độ dài câu đầu vào tối đa là 512, ví dụ, BERT, T5, và các mô hình ngôn ngữ dựa trên Transformer khác [33,21,26]. Thật không may, việc cắt bỏ như vậy, và các chiến lược liên quan khác, không thể tránh khỏi dẫn đến mất độ chính xác, mức độ có thể thay đổi từ tác vụ/tập dữ liệu này sang tác vụ/tập dữ liệu khác. Do đó, cải thiện hiệu quả cho độ dài chuỗi đầu vào dài hơn là trọng tâm chính của nhiều đề xuất. Những phát triển này là những cột mốc quan trọng, và chúng đã giảm sự phụ thuộc bậc hai vào độ dài chuỗi xuống tuyến tính [5,24,32,2,35,37,36]. Bây giờ, nhiều mô hình Transformer có thể xử lý các mẫu có độ dài chuỗi lên đến 4K (hoặc 16K nhiều nhất). Gần đây, một số báo cáo về các mô hình mới hơn có thể xử lý các chuỗi dài hơn nhiều đã xuất hiện.

Lý do. Tự nhiên có thể hỏi liệu khả năng xử lý các chuỗi dài hơn có đáng công sức hay không. Câu trả lời ngắn gọn là có. Độ chính xác được cải thiện đã được báo cáo trên các tác vụ chuỗi dài [2,35,13]. Vậy, điều gì ngăn chúng ta khỏi việc thu hoạch những lợi ích thậm chí mạnh hơn về độ chính xác bằng cách cung cấp các chuỗi thậm chí dài hơn cho những mô hình như vậy? Các mô hình như Longformer [2] và Big Bird [35] trở nên chậm và tiêu thụ một lượng bộ nhớ quá mức khi độ dài chuỗi tiếp tục tăng. Xem Hình 1 để minh họa. Tại sao? Việc cập nhật biểu diễn của mỗi token liên quan đến việc tính toán attention hiệu quả cho chuỗi và mạng feed-forward ở mỗi lớp. Điều này phát sinh chi phí tuyến tính theo độ dài chuỗi và đắt đỏ cho các chuỗi dài hơn nhiều so với 4K (hoặc 16K) tokens. Để trang bị cho các mô hình khả năng học phụ thuộc tầm xa cực dài, chúng ta cần giảm chi phí này. Những gì chúng tôi mô tả trong bài báo này là một bước tiến cụ thể - dựa trên một số giả định cụ thể theo tác vụ mà có vẻ như thường xuyên đúng, chúng tôi phác thảo một công thức hoạt động và mang lại những cải thiện mong đợi.

(1) Tập trung vào những gì chúng ta cần cho một tác vụ: Nén tập trung vào VIP-token (VCC). Chúng tôi đưa ra giả thuyết/phát hiện rằng trong nhiều tác vụ mà Transformers có hiệu quả, chỉ một tập con nhỏ các tokens (mà chúng tôi gọi là VIP-tokens) có liên quan đến đầu ra cuối cùng (và độ chính xác) của một Transformer. Nếu những tokens này đã được xác định bằng cách nào đó, chúng ta có thể bảo tồn thông tin này một cách toàn vẹn và chỉ chịu mất mát hiệu suất vừa phải. Bây giờ, có điều kiện trên những VIP-tokens cụ thể này, một sự nén mạnh mẽ trên các non-VIP-tokens khác, có thể phục vụ để giảm (và thường xuyên, phục hồi hoàn toàn) sự mất mát hiệu suất trong khi giảm đáng kể độ dài chuỗi. Việc nén này phải tận dụng thông tin liên quan đến VIP-tokens, với mục tiêu cải thiện việc xấp xỉ biểu diễn của VIP-tokens. Nói cách khác, một xấp xỉ độ trung thực cao của toàn bộ chuỗi là không cần thiết. Một khi đầu vào "được nén có chọn lọc" này đi qua một lớp Transformer, chuỗi đầu ra được giải nén về chuỗi đầy đủ ban đầu cho phép các lớp tiếp theo truy cập chuỗi đầy đủ.

(2) Cấu trúc dữ liệu chuyên biệt cho nén/giải nén. Một vấn đề thứ hai, nhưng quan trọng về mặt thực tế, là giảm overhead khi nén/giải nén các chuỗi đầu vào/đầu ra trong mạng. Bỏ qua vấn đề này sẽ ảnh hưởng đến hiệu quả. Chúng tôi đưa ra một cấu trúc dữ liệu đơn giản nhưng chuyên biệt để duy trì các trạng thái ẩn của các lớp trung gian, nơi việc nén có thể được truy cập dễ dàng từ cấu trúc dữ liệu, và việc giải nén rõ ràng có thể được tránh bằng cách cập nhật cấu trúc dữ liệu: chuỗi không bao giờ được materialize đầy đủ trong các lớp trung gian.

Đóng góp thực tế. Ngoài các mô-đun thuật toán ở trên, chúng tôi cho thấy rằng mặc dù có sự nén mạnh mẽ của các chuỗi đầu vào, chúng tôi đạt được hiệu suất tốt hơn/cạnh tranh trên một rổ rộng các thí nghiệm chuỗi dài. So với baseline, chúng tôi có hiệu quả thời gian chạy/bộ nhớ tốt hơn nhiều. Chúng tôi cho thấy rằng bây giờ có thể chạy các mô hình Transformer tiêu chuẩn trên các chuỗi có độ dài 128K token, với lợi ích hiệu suất nhất quán (và không có thay đổi kiến trúc phức tạp).

2 Kiến thức cơ bản
Chúng tôi xem xét lớp Transformer, công việc liên quan về Transformers hiệu quả và định nghĩa các ký hiệu/đơn giản hóa. Các chữ cái in hoa ĐẬM biểu thị ma trận, các chữ cái thường đậm biểu thị vectơ, và các chữ cái thường thông thường biểu thị scalars.

Xem xét ngắn gọn Mô hình Transformer. Cố định n là độ dài chuỗi và để d là chiều embedding. Định nghĩa một ma trận embedding X∈Rn×d cung cấp n vectơ đặc trưng đầu vào cho một lớp Transformer. Đầu ra của lớp Transformer này, Xnew, được định nghĩa là
Xnew=β(α(X,X,X) +X) +α(X,X,X) +X (1)
trong đó α(·,·,·) là multi-head attention (MHA) và β(·) là một mạng feed-forward (FFN). Layer norms [1] được bỏ qua để giảm lộn xộn. Để các đầu vào cho α(·,·,·) là Q,K,V∈Rn×d cho queries, keys, và values. MHA được định nghĩa là:
α(Q,K,V) := cati=g
i=1
softmax (QW Q,iW⊤
K,iK⊤)VW V,i
W (2)

--- TRANG 3 ---
trong đó g là số lượng attention heads, {WQ,i,WK,i,WV,i} là các phép chiếu có thể train được, và 'cat' nối các đầu ra của nhiều mô-đun self-attention. Chúng tôi bỏ qua các bias để đơn giản ký hiệu. Để dễ thảo luận, hãy tiếp tục đơn giản hóa ký hiệu trên bằng cách giả định rằng g=1, và loại bỏ WQ,1,WK,1,WV,1,W cũng như chuẩn hóa trong softmax: chúng vẫn sẽ được ước tính trong mô hình (tức là, mô-đun này vẫn không thay đổi) nhưng không có liên quan đến việc mô tả ý tưởng của chúng tôi. Với những đơn giản hóa này, α(·,·,·) có thể được biểu diễn là:
α(Q,K,V) := exp( QK⊤)V. (3)

Để γ(·) là một placeholder cho tất cả các tính toán nặng trong lớp Transformer ở trên:
γ(X) :=β(α(X,X,X) +X) +α(X,X,X). (4)

Chúng ta có thể xác minh rằng đầu ra của một khối Transformer (các tham số được loại bỏ để giảm lộn xộn) là,
Xnew=γ(X) +X. (5)

Một mô hình Transformer bao gồm nhiều lớp như vậy: đầu vào của mỗi lớp là đầu ra Xnew từ lớp trước đó. Để l là số lượng lớp, thì độ phức tạp tổng thể là O(ln2d+lnd2).

Transformers Hiệu quả. Nhiều phương pháp self-attention hiệu quả có sẵn để giảm chi phí O(ln2d). Chúng tôi liệt kê một số mô hình lưu ý rằng danh sách này không đầy đủ. Performer [5], Random Feature Attention [24], và Nyströmformer [32] đề xuất các xấp xỉ rank thấp khác nhau của các ma trận self-attention. Longformer [2] và Big Bird [35] mô tả attention thưa thớt toàn cục + cục bộ. Reformer [16] và YOSO [37] khai thác locality sensitive hashing để xấp xỉ ma trận self-attention. MRA attention [36] đưa ra một xấp xỉ đa độ phân giải của các ma trận self-attention.

Self-Attention Hiệu quả không mở rộng tốt cho các chuỗi cực dài. Các cơ chế self-attention hiện tại thường giảm chi phí bậc hai của MHA xuống tuyến tính. Nhưng cho đến nay, hầu hết các thí nghiệm báo cáo độ dài chuỗi lên đến 4K, với một số ngoại lệ [2,35,13]. Vượt quá 4K, chi phí tuyến tính (trên n) cho cả việc tính toán attentions hiệu quả và FFN khiến chi phí trở nên cấm đoán, đặc biệt cho các mô hình lớn. Ví dụ, mặc dù LongT5 [13] có thể train trên độ dài chuỗi lên đến 16K tokens với một self-attention hiệu quả và cho thấy kết quả hứa hẹn cho các chuỗi dài hơn, nó chậm hơn và cần một lượng tính toán đáng kể (ví dụ, xem Hình 1).

Các lựa chọn thay thế khác cho nén chuỗi? Nén các chuỗi đầu vào vì lý do hiệu quả trong Transformers không phải là ý tưởng mới. Ví dụ, [7] và [15] đề xuất các biến thể pyramid Transformer tiến triển nén chuỗi khi các lớp phát triển sâu hơn thông qua pooling hoặc lựa chọn core-set. [22] đề xuất nén chuỗi một cách thích nghi dựa trên các ranh giới ngữ nghĩa được dự đoán trong chuỗi. [25] đề xuất nén các activations quá khứ chi tiết thành các bộ nhớ thô hơn. Có ba điểm khác biệt chính với phương pháp của chúng tôi. Thứ nhất, tất cả các phương pháp được liệt kê ở trên là task agnostic. Chúng tìm kiếm các biểu diễn nén/nhỏ hơn để đại diện tốt cho chuỗi ban đầu. Công thức của chúng tôi không nhấn mạnh việc đại diện cho chuỗi ban đầu, miễn là thông tin liên quan đến VIP-tokens được bảo tồn càng nhiều càng tốt. Thứ hai, một khi những phương pháp này nén chuỗi, thông tin còn lại bị mất (cho các lớp sâu hơn hoặc các bước thời gian sau đó). Toàn bộ phương pháp của chúng tôi dựa trên việc tránh sự mất mát này - chúng tôi duy trì quyền truy cập vào chuỗi đầy đủ ở mỗi lớp (thông qua kết nối dư ít nhất). Cuối cùng, một số ý tưởng này thường liên quan đến sự phụ thuộc n2 vào độ dài chuỗi trong các giai đoạn ban đầu của công thức của chúng, khiến các thí nghiệm chuỗi dài trở thành vấn đề.

3 Nén Tập trung VIP-Token (VCC)
Mục tiêu chính của chúng tôi là giảm sự phụ thuộc vào n (nhưng không bằng cách sửa đổi nội bộ Transformer). Để làm điều này, chúng tôi mô tả một sơ đồ nén chuỗi đầu vào của một lớp Transformer và giải nén chuỗi đầu ra, tạo ra một mô hình có độ phức tạp là O(lrd2+lr2d+lrlog(nc)d+lrnpd+nd). Ở đây, r là kích thước của chuỗi nén, np là kích thước của VIP-tokens được mô tả ngay sau đây, và nc là kích thước của các tokens non-VIP/còn lại. Vậy, chúng ta có np+nc=n và giả định np≪r≪n. (Xem phân tích độ phức tạp trong Phụ lục.)

Phân tích thuật ngữ độ phức tạp: Hãy giải nén thuật ngữ để đánh giá tính liên quan của nó. Hai thuật ngữ đầu tiên O(lrd2+lr2d) đưa ra chi phí cho một Transformer, trong khi các thuật ngữ còn lại là overhead của nén và giải nén. Thuật ngữ O(lrlog(nc)d+lrnpd) là overhead của nén và cập nhật cấu trúc dữ liệu của chúng tôi ở mỗi lớp. Thuật ngữ O(nd) liên quan đến tiền xử lý liên quan đến việc chuyển đổi các trạng thái ẩn sang cấu trúc dữ liệu của chúng tôi và hậu xử lý để khôi phục các trạng thái ẩn từ cấu trúc dữ liệu. Lưu ý rằng không giống như sự phụ thuộc vào n cho Transformers vanilla, O(nd) này chỉ phát sinh ở giai đoạn đầu vào/đầu ra của Transformer, nhưng không ở bất kỳ lớp trung gian nào.

Lựa chọn thiết kế cấp cao. Chúng tôi sử dụng các lớp Transformer tiêu chuẩn với một mạng feed-forward tiêu chuẩn (dẫn đến d2 trong thuật ngữ đầu tiên) và self-attention chi phí bậc hai tiêu chuẩn (tạo ra hệ số r2 trong thuật ngữ thứ hai). Tại sao? Những lựa chọn này giúp tách biệt tác động của việc kết hợp các đối tác hiệu quả của chúng. Thuật toán được đề xuất hoạt động trên đầu vào/đầu ra của mỗi lớp Transformer để lại mô-đun Transformer không thay đổi. Do đó, mục tiêu của chúng tôi khác biệt với văn học điều tra các self-attentions hiệu quả và các mạng feed-forward hiệu quả. Điều này là bởi vì người ta có thể thay thế hai mô-đun vanilla này bằng bất kỳ lựa chọn thay thế hiệu quả nào khác để giảm thêm các thuật ngữ r2 và d2 trực tiếp. Mặc dù có những thuật ngữ bậc hai này, phương pháp của chúng tôi nhanh hơn so với baseline (§4).

Trước tiên chúng tôi sẽ mô tả ý tưởng chung của chúng tôi, như được hiển thị trong Hình 2, sử dụng VIP-tokens để hướng dẫn nén/giải nén đầu vào/đầu ra của một lớp Transformer để nó chỉ cần xử lý chuỗi nén (§3.1, §3.2). Sau đó, chúng tôi sẽ thảo luận về một cụ thể hóa của quá trình nén, bằng cách điều chỉnh một kỹ thuật phân tích đa độ phân giải (§3.3). Cuối cùng, chúng tôi sẽ giới thiệu một cấu trúc dữ liệu cho phép nén/giải nén hiệu quả hơn (§3.4).

3.1 Nâng cao Tầm quan trọng của Một vài Tokens: VIP-Tokens
Hãy bắt đầu với việc nén đơn giản nhất, xác định một phép biến đổi tuyến tính S∈Rr×n hoạt động trên đầu vào, tạo ra một biểu diễn nhỏ hơn SX∈Rr×d. Tất nhiên, một r nhỏ hơn ngụ ý rằng nhiều thông tin về X bị mất. Nhưng chúng tôi thấy rằng trong nhiều tác vụ, chỉ có các biểu diễn embedding của một số token thúc đẩy dự đoán cuối cùng: chúng tôi gọi những tokens này là VIP-tokens.

Ví dụ về VIP-tokens: Quan sát rằng chỉ có các đầu ra embedding của các masked tokens trong masked language modeling [9] và CLS token trong phân loại chuỗi [9,10] được/là sử dụng cho dự đoán. Trong trả lời câu hỏi, chỉ có các câu hỏi và các câu trả lời có thể liên quan đến các câu hỏi được sử dụng cho dự đoán. Điều quan trọng cần lưu ý là các masked tokens, CLS tokens, và question tokens là (1) được định nghĩa bởi các tác vụ và (2) được biết đến bởi mô hình (mặc dù biểu diễn embedding của những tokens này là không biết). Những VIP-tokens này có thể được xem như một tác vụ hoặc câu hỏi được đưa ra cho mô hình. Mô hình có thể xử lý chuỗi với một mục tiêu cụ thể trong tâm trí để mô hình có thể bỏ qua/lướt qua các phân đoạn ít liên quan hơn. Nguyên tắc chung của chúng tôi liên quan đến việc chọn một tập hợp các tokens làm VIP-tokens mà (1) quan trọng đối với các mục tiêu tác vụ cụ thể và (2) dễ dàng được xác định trước bởi người dùng.

Lưu ý. Không phải tất cả các tokens quan trọng có thể được xác định trước. Ví dụ, các tokens trong span câu trả lời đúng trong dự đoán span câu trả lời cũng quan trọng đối với các mục tiêu cụ thể, nhưng khó có thể xác định trước, vì vậy chỉ có các question tokens (và không phải answer tokens) được sử dụng làm VIP-tokens. Chúng tôi giả định rằng bất kỳ tokens khác có liên quan cho dự đoán nên có sự phụ thuộc cao với những VIP-tokens này. Ví dụ, các answer tokens nên có sự phụ thuộc cao (trong self-attention) với các question tokens.

Nén Tập trung VIP-Token ()ScLớp Transformer (  )γVIP-tokens PTokens còn lại CKết nối dưKết nối dưGiải nén ()S†cScC
Hình 2: Sơ đồ minh họa một lớp Transformer với nén chuỗi tập trung VIP-token.VIP-tokens chiếm những ghế đầu. VIP-tokens có thể xảy ra ở bất cứ đâu trong một chuỗi. Nhưng chúng ta có thể sắp xếp lại chuỗi cũng như các positional encodings để VIP-tokens luôn ở đầu chuỗi để làm cho phân tích/triển khai dễ dàng hơn. Với bố cục này, để P∈Rnp×d là VIP-tokens và C∈Rnc×d là các non-VIP/remaining tokens, X có thể được biểu diễn là
X=
P
C
(6)

Điều này có thể thực hiện được vì Transformer bất biến hoán vị khi hoán vị positional encodings (embeddings hoặc IDs) cùng với tokens. Việc sắp xếp lại này chỉ được thực hiện một lần cho đầu vào của mô hình Transformer, sau đó các đầu ra được tạo ra bởi mô hình được sắp xếp lại về vị trí ban đầu của chúng.

Từ cuộc thảo luận trên, rõ ràng là một người cần đảm bảo rằng sau khi nén các tokens đầu vào X, VIP-tokens vẫn phải giữ nguyên (ít nhiều) như cũ, và ma trận nén S phải phụ thuộc vào VIP-token. Chúng tôi đưa ra giả thuyết rằng những ma trận nén phụ thuộc VIP-token như vậy yêu cầu một chiều r nhỏ hơn nhiều, so với các ma trận nén không phụ thuộc VIP-token.

--- TRANG 4 ---
3.2 Nén Tập trung VIP-Token (VCC): Một Đề xuất Ban đầu
Đối với một lớp Transformer, để X biểu thị ma trận đầu vào của nó. Biểu diễn đầu ra của lớp này như sau:
Xnew=S†γ(SX) +X (7)
trong đó S∈Rr×n là ma trận nén nén X thành một biểu diễn nhỏ hơn và S† là pseudo inverse cho giải nén. Với bố cục trong (6), chúng ta có thể viết (7) là

Pnew
Cnew
=S†γ(S
P
C
) +
P
C
(8)
trong đó Pnew và Cnew là các embeddings mới cho P và C.

Luôn dành chỗ cho VIP-tokens. Cấu trúc hữu ích của S là gì? Vì Pnew là đầu ra embedding cho VIP-tokens P, chúng ta muốn chúng được bảo tồn hoàn toàn. Để đạt được điều này, chúng tôi áp đặt cấu trúc sau cho S và S†:
S=
Inp×np0
0 Sc
S†=Inp×np0
0 S†
c
. (9)

Việc sắp xếp lại đơn giản nói rằng chúng ta sẽ tránh nén P. Nhưng viết lại nó theo cách này giúp chúng ta dễ dàng giải nén (8) để kiểm tra chức năng mong muốn của Sc.

Ưu tiên thông tin trong VIP-tokens. Mục tiêu của chúng tôi là đảm bảo Pnew được tạo ra từ chuỗi nén trong (8) sẽ tương tự như đối tác của nó từ chuỗi không nén. Hãy kiểm tra (8) sử dụng ma trận nén S được định nghĩa trong (9) trước. Chúng ta thấy rằng
Inp×np0
0 S†
c
γ(
P
ScC
) =β(α(P,SX,SX) +P) +α(P,SX,SX)
S†
cβ(α(ScC,SX,SX) +ScC) +S†
cα(ScC,SX,SX)
.(10)

Màu cam xác định các thuật ngữ nơi Pnew tương tác với các thuật ngữ liên quan đến nén khác C và/hoặc Sc. Chúng tôi chủ yếu quan tâm đến Pnew trong (8), vì vậy hàng đầu tiên (màu cam) trong (10) là mối quan tâm chính. Chúng ta thấy rằng Pnew chỉ phụ thuộc vào SX nén thông qua α(P,SX,SX). Chúng ta có thể tiếp tục giải nén,
α(P,SX,SX) = exp( PX⊤S⊤)SX= exp( PP⊤)P+ exp( PC⊤S⊤
c)ScC. (11)

Một lần nữa, α(P,SX,SX) phụ thuộc vào C và Sc thông qua thuật ngữ thứ hai (màu cam). Chuẩn hóa trong softmax được bỏ qua để đơn giản hóa thảo luận. Điều này giúp chúng ta tập trung vào thuật ngữ chính quan trọng: exp(PC⊤S⊤
c)ScC. Miễn là xấp xỉ sau sử dụng Sc là tốt
exp(PC⊤S⊤
c)Sc≈exp(PC⊤), (12)
chúng ta sẽ có được một xấp xỉ tốt của Pnew. Nhiệm vụ còn lại của chúng tôi là phác thảo một sơ đồ tìm một ma trận nén Sc sao cho tiêu chí này có thể được đảm bảo.

3.3 Một Cụ thể hóa Cụ thể thông qua Nén Đa độ phân giải
Cơ chế của việc nén của chúng tôi nên như thế nào để (12) đúng? Nói chung, để có được Sc, chúng ta có thể sử dụng bất kỳ ý tưởng sketching dựa trên dữ liệu hợp lý nào để giảm thiểu lỗi của (12). Làm điều này một cách hiệu quả cần một chút công việc; chúng tôi mô tả ý tưởng cấp cao dưới đây và các chi tiết cấp thấp được cung cấp trong Phụ lục.

Ý tưởng cấp cao. Lý tưởng, một sơ đồ hiệu quả để xây dựng Sc nên hoạt động như sau. Nếu một số vùng của chuỗi C có tác động không đáng kể đến (12) (thông qua các thuật ngữ màu cam ở trên), thủ tục nên nén các vùng một cách mạnh mẽ. Nếu các vùng khác được xác định có tác động cao hơn đến (12) (một lần nữa do các thuật ngữ màu cam ở trên), thủ tục nên quét những vùng này cẩn thận hơn để có sự nén tinh tế hơn. Điều này gợi ý rằng về mặt thủ tục, một chiến lược từ thô đến tinh có thể hoạt động. Ví dụ, phân tích đa độ phân giải giúp xấp xỉ các ma trận self-attention trong Transformers [36], nhưng công thức trong [36] có thể không được viết dưới dạng tương tự như (12), khiến nó không tương thích với thiết kế của chúng tôi. Tuy nhiên, chúng tôi suy ra một dạng tương tự (chi tiết trong Phụ lục) có thể được đại diện dưới dạng tương tự như (12) và đưa ra một chiến lược để có được ScC.

--- TRANG 5 ---
CScC
c81
[C]3
[C]4
[C]1
[C]2
[C]7
[C]8
[C]5
[C]6
c41
c42
c21
c22
c13
c14
P
P
c21
c42
c13
c14
Nén
Pdày hơn = điểm attention cao hơn
Hình 3: Minh họa về nén đa độ phân giải. nc= 8. Đường tím: tính điểm attention giữa P và các phân đoạn khác nhau. Mũi tên xanh lá cây: phân đoạn có điểm attention cao hơn được chia thành hai phân đoạn con. Tương ứng, ScC= c2
1c1
3c1
4c4
2⊤ được xây dựng.Cụ thể, hãy định nghĩa một biểu diễn nén (thông qua averaging) của phân đoạn s-length thứ x của chuỗi C:cs
x∈Rd
cs
x:=1
sX
sx−s<i≤sx[C]i(13)
s∈ {k0, k1, k2,···, nc} giả định nc là lũy thừa của k và x∈ {1,2,···, nc/s}. [·]i đề cập đến hàng thứ i của ma trận đầu vào. Chúng tôi cố định tỷ lệ tăng k= 2 để đơn giản hóa thảo luận. s đại diện cho độ phân giải của xấp xỉ: nó đại diện cho số lượng embeddings non-VIP token được tính trung bình thành một vectơ cs
x. s cao hơn (ví dụ, s= 8 trong c8
1 trong Hình 3) có nghĩa là độ phân giải thấp hơn và nén nặng hơn của phân đoạn tương ứng. x đại diện cho vị trí của phân đoạn s-length trong chuỗi C. Trong sơ đồ của chúng tôi, chúng tôi nén chuỗi C và sử dụng một tập hợp cs
x cho một số s và x được chọn làm hàng của ScC nén như thấy trong Hình 3. Chuỗi C được chia thành nhiều phân đoạn có độ dài khác nhau, sau đó mỗi phân đoạn được nén thành một vectơ cs
x.

Về mặt thủ tục, như thể hiện trong Hình 3, sơ đồ của chúng tôi bắt đầu với sự nén nặng nhất và tiến triển tinh tế một số phân đoạn của C được hướng dẫn bởi VIP-tokens P. Sơ đồ bắt đầu với sự nén nặng nhất coi C như một phân đoạn nc-length và nén nó thành một cnc
1 duy nhất. Sau đó, bắt đầu với s=nc (nút gốc), thủ tục (1) tính các điểm attention trung bình giữa VIP-tokens P và các cs
x cho các x khác nhau (trung bình trên tất cả các attention heads và tất cả VIP-tokens; chỉ một cnc
1 ở mức s=nc). Chúng tôi lưu ý rằng các điểm attention được thu được bằng cách trích xuất các ma trận attention từ mô-đun MHA (2) của lớp Transformer hiện tại khi sử dụng P làm queries và cs
x làm keys. Sau đó, nó (2) chia các phân đoạn s-length tương ứng cs
x với điểm attention trung bình cao hơn (một phân đoạn được chia trong Hình 3 nhưng chúng tôi có thể chia nhiều phân đoạn hơn, một lần nữa chỉ một cnc
1 ở mức s=nc) thành các phân đoạn con (s/2)-length: cs/2
x tương ứng (13) của mỗi phân đoạn con được tính toán để có biểu diễn tinh tế hơn. Sau đó, ở mức tiếp theo cho s=nc/2, cùng một thủ tục tiếp tục. Quá trình này tiếp tục cho đến khi các phân đoạn con có độ dài 1. Chúng tôi lưu ý rằng thủ tục này được hướng dẫn bởi VIP-tokens P và được thiết kế để giảm thiểu tối đa lỗi xấp xỉ (12). Không có tham số có thể học bổ sung nào được giới thiệu cho sơ đồ này. Các chi tiết kỹ thuật của thuật toán này ít liên quan hơn đối với phương pháp tổng thể của chúng tôi, nhưng đối với các độc giả quan tâm, các chi tiết được thảo luận trong Phụ lục.

c81Δc15Δc16Δc23Δc17Δc18Δc24Δc42Δc11Δc12Δc21Δc13Δc14Δc22Δc41
Hình 4: Cấu trúc dữ liệu đề xuất T(C)Xấp xỉ này tốt như thế nào? Đầu ra Pnew (8) được xấp xỉ tốt vì phương pháp bảo tồn các thành phần liên quan của C có tác động cao đến đầu ra Pnew. Hơn nữa, nếu VIP-tokens P có trọng số attention cao cho một số hàng của C, thì hàng tương ứng trong C sẽ được xấp xỉ với tần số cao hơn (nén ít hơn). Vì vậy, đầu ra trong Cnew (8) cho một tập con các non-VIP tokens có sự phụ thuộc cao hơn với VIP-tokens sẽ có xấp xỉ tốt hơn so với những cái khác, như mong muốn. Tính chất này hữu ích vì một số tokens có vị trí không xác định nhưng thể hiện sự phụ thuộc cao với VIP-tokens cũng có thể liên quan đến dự đoán cuối cùng của mô hình Transformer trong một số tác vụ. Câu trả lời trong các tác vụ trả lời câu hỏi dựa trên span là một ví dụ, và cấu trúc của chúng tôi đảm bảo rằng chúng cũng sẽ được xấp xỉ tốt.

3.4 Cấu trúc Dữ liệu Hiệu quả cho Nén/Giải nén
Bằng cách sử dụng thủ tục trong §3.3 được minh họa trong Hình 3, chúng ta có thể tìm ScC nén với chi phí O(ncd+rnpd) ở mỗi lớp. Chi phí chính O(ncd) là do tính cs
x được định nghĩa trong (13) cho tất cả độ phân giải s và vị trí x bằng cách sử dụng quan hệ đệ quy từ dưới lên:
c2s
x=1
2cs
2x−1+1
2cs
2x c1
x= [C]x (14)

Chúng tôi thấy rằng những bước này có thể đưa ra một overhead lớn. Hơn nữa, lưu ý rằng nếu chúng ta giải nén (áp dụng S† cho) đầu ra của γ cho chuỗi nén như trong (8), chi phí là O(nd) vì số lượng entries khác không trong S† là n (chi tiết hơn trong Phụ lục). Như một giải pháp, bây giờ chúng tôi giới thiệu một cấu trúc dữ liệu T(·) để lưu trữ C và Cnew, như thể hiện trong 4, cho phép tính toán hiệu quả cs
x và loại bỏ giải nén rõ ràng. Chúng tôi lưu ý rằng cấu trúc dữ liệu này chỉ có thể do cấu trúc cụ thể của ScC được xây dựng trong §3.3. Cụ thể, T(C) lưu trữ cnc
1 và ∆cs
x được định nghĩa là
∆cs
x:=c2s
⌈x/2⌉−cs
x (15)

cho mỗi độ phân giải s̸=nc và vị trí x. Tương tự, T(Cnew) lưu trữ (cnew)nc
1 và ∆(cnew)s
x trong đó (cnew)s
x và ∆(cnew)s
x được định nghĩa tương tự như (13) và (15) nhưng sử dụng Cnew thay vì C.

Sau đó, cho T(C), bất kỳ cs
x nào có thể được truy xuất hiệu quả với chi phí O(log(nc)d) thông qua đệ quy:
cs
x=c2s
⌈x/2⌉−∆cs
x=c4s
⌈x/4⌉−∆c2s
⌈x/2⌉−∆cs
x=··· (16)

Lý do duy nhất chúng ta cần giải nén S† là chúng ta cần có được biểu diễn mới Cnew (không có giải nén cho Pnew vì P không được nén). Giả sử chúng ta có T(Cnew), thì chúng ta có một cách thay thế để có được Cnew tương tự như (16) (lưu ý (cnew)1
x= [Cnew]x) mà không cần giải nén rõ ràng. Lợi ích chính của cấu trúc dữ liệu này là chúng ta có thể có được T(Cnew) bằng cách thay đổi một số nút trong T(C). Điều này chỉ cần cập nhật O(r) nút, và mỗi cập nhật tốn chi phí O(d).

Một ví dụ. Chúng tôi hiển thị một T(C) cho nc= 8 trong Hình 4. Để ScC= c2
1c1
3c1
4c4
2⊤ như trong Hình 3. Vì phân đoạn c2
1 không được chia thành các phân đoạn con c1
1 và c1
2, chúng ta có (chi tiết trong Phụ lục):
(cnew)1
1−c1
1= (cnew)1
2−c1
2= (cnew)2
1−c2
1 (17)

Bằng cách sắp xếp lại (17), chúng ta có thể xác minh rằng ∆(cnew)1
1,∆(cnew)1
2 trong T(Cnew) giữ nguyên như ∆c1
1,∆c1
2 trong T(C) và do đó không cần được cập nhật:
∆(cnew)1
1= (cnew)2
1−(cnew)1
1=c2
1−c1
1= ∆c1
1
∆(cnew)1
2= (cnew)2
1−(cnew)1
2=c2
1−c1
2= ∆c1
2(18)

Hơn nữa, chúng ta có thể xác minh rằng chỉ có các nút màu xanh lá cây trong Hình 4 sẽ được cập nhật. Những nút này tương ứng với các nút trong Hình 3 đã được duyệt qua. Tóm lại, đối với mỗi hàng cs
x của ScC (một nút lá trong Hình 3), chỉ có nút lưu trữ ∆(c)s
x và các nút tổ tiên của nó trong T(C) phải được cập nhật, vì vậy tổng số nút (bao gồm các tổ tiên của chúng) được cập nhật là O(r). Tiếp theo, chúng ta có thể cập nhật các nút như sau: đầu tiên, chúng ta có được biểu diễn (cnew)2
1,(cnew)1
3,(cnew)1
4,(cnew)4
2 bằng cách cung cấp ScC vào lớp Transformer (chi tiết trong Phụ lục). Ở mức s= 1, cho (cnew)1
3 và (cnew)1
4, chúng ta (1) tính (cnew)2
2 thông qua (14), và sau đó (2) tính ∆(cnew)1
3 và ∆(cnew)1
4 thông qua (15). Hai giá trị cuối cùng là các giá trị mới cho ∆c1
3 và ∆c1
4 trong T(C). Ở mức s= 2, cho (cnew)1
2 và (cnew)2
2 được tính ở mức trước đó, chúng ta áp dụng thủ tục tương tự để có được (cnew)4
1,∆(cnew)1
2,∆(cnew)2
2, và hai giá trị cuối cùng được sử dụng để cập nhật hai nút trong T(C). Rõ ràng là mỗi cập nhật nút tốn chi phí O(d).

Kết hợp lại: độ phức tạp của việc sửa đổi T(C) thành T(Cnew) là O(rd). Thuật toán chi tiết và phân tích độ phức tạp được mô tả trong Phụ lục.

Bằng cách duy trì cấu trúc dữ liệu này, chúng ta không bao giờ cần materialize toàn bộ C hoặc Cnew trong bất kỳ lớp trung gian nào, mà thay vào đó chúng ta sử dụng (16) để xây dựng các hàng của ScC và thực hiện cập nhật T(C) để có được Cnew (được đại diện như T(Cnew)) ở mỗi lớp trung gian. Ở đầu ra của một Transformer, Cnew được materialize từ T(Cnew) với chi phí O(ncd) thông qua đệ quy (16) từ dưới lên.

4 Thí nghiệm
Chúng tôi thực hiện một tập hợp rộng các thí nghiệm để đánh giá thực nghiệm hiệu suất của việc nén được đề xuất của chúng tôi. Chúng tôi đánh giá phương pháp của chúng tôi trên cả kiến trúc encoder-only và encoder-decoder. Chúng tôi so sánh phương pháp của chúng tôi với baseline trên một danh sách lớn các tác vụ trả lời câu hỏi và tóm tắt, nơi chúng tôi thấy các chuỗi dài xảy ra thường xuyên nhất. Sau đó, chúng tôi nghiên cứu hiệu suất mô hình của việc mở rộng lên các chuỗi cực dài được kích hoạt bởi phương pháp của chúng tôi. Vì hiệu quả là trọng tâm của các baseline hiệu quả và công việc của chúng tôi, chúng tôi bao gồm hiệu quả thời gian chạy (của một chuỗi duy nhất) bằng millisecond trong mỗi bảng. (Xem hyperparameters/thống kê tập dữ liệu trong Phụ lục.)

Để dễ triển khai và lựa chọn hyperparameter, chúng tôi hạn chế các hàng của ScC có chính xác hai độ phân giải cho các thí nghiệm. Cụ thể, đối với một tỷ lệ tăng k được định nghĩa trước, chúng tôi chia và tinh chỉnh tất cả các phân đoạn cs
x với s > k thành các phân đoạn con k-length, và chọn h (được định nghĩa trước) phân đoạn k-length để tiếp tục chia thành các phân đoạn con 1-length. Vì vậy, các hàng của ScC sẽ bao gồm (nc/k−h) của ck
x và hk của c1
x cho một số x. Hơn nữa, chúng tôi thấy rằng một vài lớp của các lớp Transformer tiêu chuẩn để tiền xử lý tokens giúp ích cho hiệu suất. Do đó, trong giai đoạn ban đầu của một Transformer, chúng tôi phân đoạn chuỗi đầu vào thành nhiều phân đoạn 512-length. Đối với mỗi phân đoạn, chúng tôi sử dụng tính toán vanilla trong 4 lớp đầu tiên (cho các mô hình base và 6 lớp cho các mô hình lớn hơn) của một Transformer. Sau đó, đối với các lớp còn lại, các phân đoạn được nối lại thành một chuỗi và được xử lý bằng cách sử dụng nén được đề xuất của chúng tôi. Không có giao tiếp giữa bất kỳ phân đoạn nào, vì vậy giai đoạn ban đầu chỉ được sử dụng để có được một biểu diễn hợp lý cho việc nén hoạt động. Để đơn giản hóa việc triển khai, chúng tôi chỉ sử dụng nén được đề xuất trong encoder, và sử dụng tính toán vanilla trong decoder của các mô hình encoder-decoder. Chúng tôi lưu ý rằng phương pháp của chúng tôi có thể được áp dụng cho decoder (chi tiết hơn trong Phụ lục).

Bảng 1: Kết quả tập dev cho các mô hình encoder-only.
Phương pháp Kích thước Độ dài HotpotQA QuALITY WikiHop
Thời gian EM F1 Thời gian Độ chính xác Thời gian Độ chính xác
RoBERTa base 512 19.9 35.1 44.9 21.2 39.0 19.6 67.6
RoBERTa base 4K 422.3 62.2 76.1 403.2 39.5 414.1 75.2
Big Bird base 4K 297.9 59.5 73.2 307.0 38.5 293.3 74.5
Longformer base 4K 371.0 59.9 73.6 368.0 27.9 369.7 74.3
MRA Attention base 4K 203.5 63.4 77.0 200.5 38.7 199.2 76.1
Ours base 4K 114.6 60.9 74.6 126.4 39.6 108.0 75.9
Ours* base 4K 114.6 61.4 75.0 125.7 39.5 108.0 76.1
Ours* large 4K 285.8 66.7 80.0 390.8 41.8 394.3 79.6Các Mô hình Encoder-Only. Đối với kiến trúc encoder-only, chúng tôi so sánh phương pháp của chúng tôi với RoBERTa [21] và ba baseline mạnh: Longformer [2], Big Bird [35], và MRA Attention [36]. Trước tiên chúng tôi pretrain một mô hình RoBERTa sử dụng tác vụ masked language modeling, sau đó cho mỗi phương pháp, chúng tôi thực hiện pretraining liên tục từ checkpoint RoBERTa để mở rộng positional embeddings đến độ dài 4K và điều chỉnh các tham số mô hình để thích nghi với các xấp xỉ được sử dụng trong các baseline hiệu quả và phương pháp của chúng tôi. Chúng tôi xác minh rằng phương pháp được đề xuất của chúng tôi có thể được tích hợp vào một Transformer đã được pretrain với một số pretraining liên tục. Nhưng chúng tôi lưu ý rằng lượng giảm log perplexity cho phương pháp của chúng tôi (−0.114) trong quá trình pre-training lớn hơn nhiều so với Longformer (−0.017) và Big Bird (−0.025) từ 50K steps đến 250K steps. Việc pretraining liên tục cho những baseline này có thể đã bão hòa vì chỉ có self-attention được xấp xỉ trong khi phương pháp của chúng tôi có thể yêu cầu nhiều pretraining hơn để điều chỉnh các tham số cho xấp xỉ mạnh mẽ hơn. Vì vậy, chúng tôi chạy một pretraining quy mô lớn hơn cho phương pháp của chúng tôi; kết quả downstream trong Tab. 1 và Hình 5, được ký hiệu với *. Chúng tôi sử dụng HotpotQA [34], QuALITY [23], và WikiHop [31] để đánh giá các mô hình ngôn ngữ. HotpotQA là một tác vụ trích xuất span câu trả lời, trong khi QuALITY và WikiHop là các tác vụ trả lời câu hỏi đa lựa chọn. Chúng tôi đặt câu hỏi và câu trả lời đa lựa chọn (cho QuALITY và WikiHop) làm VIP-tokens.

1022×1026×102
Thời gian chạy (ms)6971737577Độ chính xác
RoBERTa
Big Bird
Longformer
Ours
Ours*
Hình 5: Thời gian chạy mô hình vs độ chính xác WikiHop dev khi sử dụng các hyperparameters cụ thể mô hình khác nhauNhư thể hiện trong Tab. 1, chúng tôi xác minh rằng phương pháp của chúng tôi luôn tốt hơn so với Longformer và Big Bird. Phương pháp của chúng tôi có được độ chính xác tốt hơn trong QuALITY và WikiHop so với mô hình RoBERTa độ dài 4K, nhưng nó hơi tệ hơn so với mô hình RoBERTa độ dài 4k trên HotpotQA. Pretraining nhiều hơn giúp thu hẹp khoảng cách. Chúng tôi cũng sử dụng WikiHop để thí nghiệm với các hyperparameters cụ thể phương pháp (như kích thước khối trong Big Bird, kích thước cửa sổ trong Longformer, và kích thước nén r trong phương pháp của chúng tôi). Như thể hiện trong Hình 5, biên hiệu quả thời gian chạy của chúng tôi luôn tốt hơn so với các baseline. Điểm chính là phương pháp của chúng tôi có hiệu quả thời gian chạy tốt hơn nhiều so với các baseline có cùng độ dài chuỗi mà không hy sinh hiệu suất mô hình của nó. Hơn nữa, chúng tôi lưu ý rằng phương pháp của chúng tôi có thể được mở rộng lên các mô hình lớn hơn để cải thiện độ chính xác.

Các Mô hình Encoder-Decoder. Chúng tôi so sánh phương pháp của chúng tôi với T5 [26], LongT5 [13], và LED [2]. Chúng tôi sử dụng các checkpoint pretrained công khai cho baseline. Các mô hình pretrained cho phương pháp của chúng tôi được thu được bằng cách thực hiện pretraining liên tục từ các checkpoint T5 công khai sử dụng tác vụ pretraining của T5 [26]. Chúng tôi lưu ý rằng LED-base có 6 lớp encoder/decoder so với 12 lớp encoder/decoder trong các mô hình base của các phương pháp khác, độ chính xác của nó thường thấp hơn. Vì vậy, LED được đánh giá trong các tác vụ hạn chế. Chúng tôi sử dụng HotpotQA [34], WikiHop [31], CNN/Dailymail [28], MediaSum [40], Arxiv [6], và GovReport [14], SummScreenFD [4], QMSum [39], NarrativeQA [18], Qasper [8], QuALITY [23], ContractNLI [20] từ benchmark SCROLLS [29] để đánh giá các mô hình ngôn ngữ. Đối với các tác vụ trả lời câu hỏi, chúng tôi đặt câu hỏi và câu trả lời đa lựa chọn (cho QuALITY và WikiHop) làm VIP-tokens trong phương pháp của chúng tôi. Đối với tóm tắt dựa trên query, như QMSum, chúng tôi sử dụng query làm VIP-tokens trong phương pháp của chúng tôi. Đối với các tác vụ tóm tắt chung, chúng tôi thêm "summarize:" vào đầu mỗi instance và sử dụng nó làm VIP-tokens trong phương pháp của chúng tôi. Phương pháp của chúng tôi đạt được hiệu suất khớp hoặc tốt hơn trong hầu hết các tác vụ so với T5,

--- TRANG 8 ---
Bảng 2: Kết quả tập dev cho các mô hình encoder-decoder. Các giá trị trái / phải của các cột thời gian chạy là thời gian chạy cho toàn bộ mô hình / encoder.
Phương pháp Kích thước # Param Độ dài WikiHop HotpotQA CNN/Dailymail MediaSum
Thời gian chạy EM F1 Thời gian chạy EM F1 Thời gian chạy R-1 R-2 R-L Thời gian chạy R-1 R-2 R-L
T5 base 223M 512 25.7 / 20.5 66.7 69.1 26.3 / 20.5 34.1 44.4 40.0 / 20.5 43.3 20.5 40.4 39.9 / 20.5 30.7 14.5 28.1
T5 base 223M 4K 594.3 / 553.7 76.2 78.1 594.3 / 550.6 64.2 77.5 614.4 / 549.4 43.8 20.9 41.0 613.5 / 552.9 34.9 17.2 31.9
LongT5 base 248M 4K 270.7 / 233.9 72.7 74.8 271.3 / 233.7 62.3 75.7 291.6 / 234.9 43.3 20.6 40.5 287.3 / 229.5 34.9 17.3 32.0
LED base 162M 4K 236.6 / 222.9 70.0 72.4 237.4 / 222.9 55.1 67.9 249.4 / 221.8 43.3 20.0 40.5 - / - - - -
Ours base 223M 4K 181.7 / 148.1 76.7 78.4 155.4 / 127.4 64.5 77.7 195.8 / 139.9 43.6 20.7 40.7 196.7 / 140.2 34.8 17.3 31.9
T5 large 738M 512 83.5 / 67.0 69.1 71.4 84.1 / 67.0 36.9 47.8 124.6 / 67.0 43.8 20.7 40.9 124.5 / 67.0 31.9 15.5 29.1
T5 large 738M 4K 1738.7 / 1601.0 79.1 80.7 1598.1 / 1598.1 68.0 81.3 1824.8 / 1600.4 44.3 21.0 41.4 - / - - - -
Ours large 738M 4K 561.4 / 460.6 79.0 80.6 485.3 / 382.8 67.8 81.0 608.1 / 433.8 44.4 21.4 41.5 609.7 / 434.4 35.8 18.2 32.8
Ours 3b 3B 4K 1821.5 / 1441.2 80.8 82.3 1547.7 / 1197.1 70.2 83.2 1930.7 / 1364.8 44.8 21.5 41.9 1930.7 / 1364.8 36.3 18.5 33.3
Phương pháp Kích thước # Param Độ dài Qasper QuALITY Arxiv SummScreenFD
Thời gian chạy EM F1 Thời gian chạy EM F1 Thời gian chạy R-1 R-2 R-L Thời gian chạy R-1 R-2 R-L
T5 base 223M 512 31.8 / 20.5 10.8 16.4 29.3 / 20.5 33.6 47.3 59.0 / 20.5 28.9 8.6 25.6 59.1 / 20.5 27.0 4.8 23.5
T5 base 223M 4K 608.2 / 551.7 13.2 29.1 596.3 / 551.2 34.7 47.4 645.4 / 549.1 44.4 18.4 39.9 647.9 / 551.1 31.6 6.8 27.6
LongT5 base 248M 16K 1628.5 / 1421.3 16.2 33.4 1633.1 / 1439.7 35.8 48.5 1699.7 / 1370.4 48.5 21.7 43.7 1763.4 / 1427.8 33.1 7.3 28.5
LED base 162M 16K - / - - - - / - - - 1055.8 / 923.6 47.8 20.6 43.2 - / - - - -
Ours base 223M 16K 538.3 / 391.6 16.0 30.8 557.1 / 419.2 36.5 48.7 672.8 / 392.1 48.5 21.4 43.9 670.5 / 390.9 33.1 7.3 28.6
T5 large 738M 512 101.9 / 66.4 11.3 17.0 95.8 / 67.1 35.3 49.0 182.2 / 67.1 30.5 9.1 27.1 180.9 / 66.5 28.3 4.9 24.9
T5 large 738M 4K - / - - - 1760.5 / 1596.4 37.8 50.5 1901.5 / 1598.8 46.0 19.4 41.4 - / - - - -
Ours large 738M 16K 1679.6 / 1120.2 16.3 33.7 1753.6 / 1210.7 40.3 52.5 1959.1 / 1111.0 49.5 22.2 44.7 1957.1 / 1109.2 34.3 7.6 29.6
Ours 3b 3B 16K 6165.4 / 4637.3 19.0 38.2 6398.8 / 4962.7 45.2 56.0 7676.3 / 4642.2 49.8 22.4 45.0 7641.5 / 4631.3 34.7 7.8 30.1
Phương pháp Kích thước # Param Độ dài ContractNLI NarrativeQA GovReport QMSum
Thời gian chạy EM F1 Thời gian chạy EM F1 Thời gian chạy R-1 R-2 R-L Thời gian chạy R-1 R-2 R-L
T5 base 223M 512 24.0 / 20.5 73.5 73.5 26.8 / 20.5 2.0 11.3 59.1 / 20.5 40.5 14.8 38.2 43.5 / 20.5 30.2 8.0 26.5
T5 base 223M 4K 579.0 / 551.6 86.8 86.8 593.4 / 547.6 3.8 13.3 648.3 / 551.5 54.0 25.2 51.4 620.2 / 551.5 31.1 8.2 27.4
LongT5 base 248M 16K 1564.2 / 1462.5 85.1 85.1 1541.7 / 1370.2 5.2 15.6 1726.4 / 1387.7 55.8 27.9 53.2 1721.4 / 1450.7 35.7 11.7 31.4
Ours base 223M 16K 484.2 / 393.1 87.0 87.0 518.2 / 394.4 5.0 15.8 674.0 / 391.6 55.2 27.1 52.6 623.1 / 396.5 31.8 8.8 27.9
T5 large 738M 512 78.1 / 67.1 74.3 74.3 - / - - - 180.9 / 67.0 43.3 16.2 41.1 136.4 / 67.1 31.7 8.1 27.6
T5 large 738M 4K 1702.4 / 1601.2 87.2 87.2 - / - - - - / - - - - - / - - - -
Ours large 738M 16K 1440.6 / 1122.6 87.8 87.8 1551.7 / 1133.9 6.6 18.7 1955.5 / 1113.8 56.3 28.0 53.8 1816.4 / 1134.6 34.8 10.4 30.7
Ours 3b 3B 16K 5850.2 / 4665.9 88.5 88.5 6055.4 / 4659.4 8.2 21.2 7668.2 / 4642.7 56.9 28.5 54.3 7146.7 / 4655.6 35.7 10.9 31.1

LongT5, và LED với hiệu quả cao hơn nhiều (xem Tab. 2). Hơn nữa, hiệu suất tăng đơn điệu theo kích thước mô hình, vì vậy phương pháp của chúng tôi có thể mở rộng lên các mô hình lớn hơn.

Bảng 3: Kết quả dev của NarrativeQA trên mô hình base khi mở rộng độ dài chuỗi từ 16K đến 128K.
Độ dài Thời gian chạy (ms) k h EM F1
16K 518.2 / 394.4 / 162.4 16 90 5.9 16.6
32K 946.8 / 671.6 / 212.6 32 55 6.6 17.5
32K 1027.9 / 751.0 / 298.0 16 90 6.4 17.5
64K 1848.7 / 1177.2 / 254.8 64 30 7.2 18.4
64K 2244.8 / 1574.2 / 659.4 16 90 7.5 19.3
128K 6267.8 / 5125.9 / 1902.2 16 90 8.0 19.6Mở rộng lên Các Chuỗi Dài hơn. Các thí nghiệm trước đó giới hạn độ dài chuỗi tối đa là 4K hoặc 16K vì các baseline chỉ có thể được mở rộng lên những độ dài này. Tuy nhiên, phương pháp của chúng tôi có thể được mở rộng lên các chuỗi dài hơn nhiều. Chúng tôi lưu ý rằng NarrativeQA [18] là một testbed lý tưởng như thể hiện trong thống kê tập dữ liệu trong Phụ lục. Kết quả được hiển thị trong Tab. 3. Các giá trị trái / giữa / phải của cột thời gian chạy là cho toàn bộ mô hình / encoder / 8 lớp cuối cùng (trong tổng số 12 lớp) sử dụng nén của chúng tôi. Hiệu suất tăng đơn điệu khi độ dài chuỗi tăng. Chúng tôi lưu ý rằng đối với độ dài chuỗi 64K, hiệu suất của mô hình với k= 64 thấp hơn so với mô hình với k= 16. Chúng tôi nghi ngờ rằng vì kết quả được finetune từ cùng một mô hình được pretrain với k= 16, khoảng cách lớn giữa hai k khác nhau có thể có tác động tiêu cực đến hiệu suất finetuning. Tuy nhiên, hiệu suất vẫn cao hơn so với các mô hình độ dài 32K.

Tại sao tập trung vào độ dài 4K - 128K? Chúng tôi tin rằng tính toán mà Transformers đầy đủ yêu cầu khi xử lý các chuỗi ngắn hơn không phải là nút thắt cổ chai hiệu quả. Kết quả là, chúng tôi không profile hiệu suất của phương pháp chúng tôi cho các chuỗi độ dài nhỏ hơn, vì Transformers tiêu chuẩn đủ nhanh trong trường hợp này. Hơn nữa, trong khi mô hình của chúng tôi có thể được áp dụng cho các chuỗi ngắn hơn, chúng tôi nghi ngờ rằng đối với các chuỗi ngắn hơn, có thể có ít thông tin không liên quan hơn cho VIP-tokens. Vì vậy, việc nén thông tin không liên quan sẽ không mang lại tăng tốc có ý nghĩa. Đây là một hạn chế của phương pháp chúng tôi vì việc nén hoạt động tốt hơn khi có nhiều thông tin có thể nén hơn. Chúng tôi chỉ đẩy độ dài chuỗi lên 128K vì độ dài này đủ để bao phủ phần lớn độ dài chuỗi gặp phải trong các tác vụ chuỗi dài (ví dụ, mô hình của chúng tôi có thể xử lý toàn bộ một cuốn sách cùng một lúc).

Hạn chế. Phương pháp của chúng tôi giả định rằng trong nhiều tác vụ, một tập con các tokens chịu trách nhiệm không tương xứng cho dự đoán mô hình, các non-VIP-tokens còn lại có thể đóng vai trò nhưng ít quan trọng hơn. Phương pháp của chúng tôi xuất sắc trong các tác vụ như vậy bằng cách chọn lọc định vị thông tin liên quan trong chuỗi cho các VIP-tokens đã cho. Như các thí nghiệm cho thấy, lựa chọn này hiệu quả trong nhiều trường hợp nhưng hành vi này không phổ quát. Thỉnh thoảng, một embedding được tính trước phải phục vụ nhiều tác vụ đồng thời, ví dụ, cả text retrieval và natural language inference. Trong trường hợp này, nếu chúng ta không biết các tác vụ trước, việc lựa chọn VIP-token không thể được thực hiện một cách có ý nghĩa.

5 Kết luận
Chúng tôi đề xuất một phương pháp nén chuỗi tập trung VIP-token để nén/giải nén các chuỗi đầu vào/đầu ra của các lớp Transformer từ đó giảm sự phụ thuộc độ phức tạp vào độ dài chuỗi n mà không hy sinh độ chính xác mô hình. Cụ thể, chúng tôi thiết kế sự nén

Đánh giá thực nghiệm của chúng tôi cho thấy phương pháp của chúng tôi có thể được tích hợp trực tiếp vào các mô hình pretrained hiện có với một số training bổ sung. Ngoài ra, nó thường có hiệu quả cao hơn nhiều so với baseline với cùng độ dài chuỗi trong khi mang lại độ chính xác mô hình tốt hơn hoặc cạnh tranh. Hơn nữa, chúng tôi cho thấy thuật toán của chúng tôi mở rộng lên 128K tokens (hoặc hơn) trong khi luôn mang lại cải thiện độ chính xác.

Tài liệu tham khảo
[1]Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

[2]Iz Beltagy, Matthew E Peters, và Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

[3]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213–229. Springer, 2020.

[4]Mingda Chen, Zewei Chu, Sam Wiseman, và Kevin Gimpel. SummScreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602–8615, Dublin, Ireland, May 2022. Association for Computational Linguistics.

[5]Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, và Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations (ICLR), 2021.

[6]Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, và Nazli Goharian. A discourse-aware attention model for abstractive summarization of long documents. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018.

[7]Zihang Dai, Guokun Lai, Yiming Yang, và Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, và H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 4271–4282. Curran Associates, Inc., 2020.

[8]Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, và Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599–4610, Online, June 2021. Association for Computational Linguistics.

[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019.

--- TRANG 10 ---
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.

[11] Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, và Dragomir Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074–1084, Florence, Italy, July 2019. Association for Computational Linguistics.

[12] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, và Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

[13] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, và Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724–736, Seattle, United States, July 2022. Association for Computational Linguistics.

[14] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, và Lu Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419–1436, Online, June 2021. Association for Computational Linguistics.

[15] Xin Huang, Ashish Khetan, Rene Bidart, và Zohar Karnin. Pyramid-BERT: Reducing complexity via successive core-set based token selection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8798–8817, Dublin, Ireland, May 2022. Association for Computational Linguistics.

[16] Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations (ICLR), 2020.

[17] Bryan Klimt và Yiming Yang. The enron corpus: A new dataset for email classification research. In Proceedings of the 15th European Conference on Machine Learning, ECML'04, page 217–226, Berlin, Heidelberg, 2004. Springer-Verlag.

[18] Tomáš Ko ˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, và Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328, 2018.

[19] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pages 79–86, Phuket, Thailand, September 13-15 2005.

[20] Yuta Koreeda và Christopher Manning. ContractNLI: A dataset for document-level natural language inference for contracts. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1907–1919, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[22] Piotr Nawrot, Jan Chorowski, Adrian Ła ´ncucki, và Edoardo M. Ponti. Efficient transformers with dynamic token pooling, 2022.

[23] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, và Samuel Bowman. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336–5358, Seattle, United States, July 2022. Association for Computational Linguistics.

--- TRANG 11 ---
[24] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, và Lingpeng Kong. Random feature attention. In International Conference on Learning Representations (ICLR), 2021.

[25] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, và Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020.

[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.

[27] David Saxton, Edward Grefenstette, Felix Hill, và Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2019.

[28] Abigail See, Peter J. Liu, và Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–1083, Vancouver, Canada, July 2017. Association for Computational Linguistics.

[29] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, và Omer Levy. SCROLLS: Standardized comparison over long language sequences. In EMNLP, 2022.

[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

[31] Johannes Welbl, Pontus Stenetorp, và Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287–302, 2018.

[32] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, và Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.

[33] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, và R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.

[34] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, và Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.

[35] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

[36] Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, và Vikas Singh. Multi resolution analysis (MRA) for approximate self-attention. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, và Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 25955–25972. PMLR, 17–23 Jul 2022.

[37] Zhanpeng Zeng, Yunyang Xiong, Sathya Ravi, Shailesh Acharya, Glenn M Fung, và Vikas Singh. You only sample (almost) once: Linear cost self-attention via bernoulli sampling. In International Conference on Machine Learning (ICML), 2021.

--- TRANG 12 ---
[38] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.

[39] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, và Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921, Online, June 2021. Association for Computational Linguistics.

[40] Chenguang Zhu, Yang Liu, Jie Mei, và Michael Zeng. Mediasum: A large-scale media interview dataset for dialogue summarization. arXiv preprint arXiv:2103.06410, 2021.

--- TRANG 13 ---
6 Phụ lục
6.1 Định nghĩa Ký hiệu
Bảng 4: Các ký hiệu chính được sử dụng
Ký hiệu Mô tả
l số lượng lớp của một mô hình Transformer
n số lượng tokens của một chuỗi đầu vào
d chiều embedding mô hình
np số lượng VIP-tokens
nc số lượng non-VIP/remaining tokens, vì vậy np+nc=n
r độ dài của một chuỗi nén
α(·,·,·) multi-head attention nhận ba đầu vào cho query/key/value embeddings
β(·) mạng feed-forward hai lớp
γ(·) hàm đại diện cho tất cả tính toán nặng của một lớp Transformer
X ma trận embedding đại diện cho một chuỗi đầu vào
P ma trận embedding đại diện cho VIP-tokens
C ma trận embedding đại diện cho non-VIP/remaining tokens
Xnew ma trận embedding cập nhật của một chuỗi đầu vào, đầu ra của một lớp Transformer
Pnew ma trận embedding cập nhật đại diện cho VIP-tokens
Cnew ma trận embedding cập nhật đại diện cho non-VIP/remaining tokens
S ma trận nén
Sc ma trận con nén cho non-VIP/remaining tokens
S† ma trận giải nén
S†
c ma trận con giải nén cho non-VIP/remaining tokens
s độ phân giải của xấp xỉ, đại diện cho số lượng embeddings non-VIP token được tính trung bình
x vị trí của phân đoạn s-length với chuỗi ban đầu
k tỷ lệ tăng của độ phân giải, s∈ {1, k, k2, k3,···, nc}
h số lượng phân đoạn k-length được chia thành các phân đoạn con 1-length, được sử dụng cho thí nghiệm
hs số lượng phân đoạn s-length được chia thành các phân đoạn con ngắn hơn, được sử dụng trong thảo luận Phụ lục
J tập hợp các thành phần bs
x được xây dựng qua Alg. 1, các phần tử là hàng của Sc
bs
x thành phần được sử dụng cho 1-D wavelet transform của scaling s và translation x
cs
x trung bình cục bộ của phân đoạn x thứ s-length của chuỗi C
(cnew)s
x trung bình cục bộ của phân đoạn x thứ s-length của chuỗi Cnew
T(·) cấu trúc dữ liệu để lưu trữ chuỗi đầu vào C hoặc Cnew
∆cs
x trạng thái được lưu trữ trong T(C) được định nghĩa là ∆cs
x:=c2s
⌈x/2⌉−cs
x
∆(cnew)s
x trạng thái được lưu trữ trong T(Cnew) được định nghĩa là ∆(cnew)s
x:= (cnew)2s
⌈x/2⌉−(cnew)s
x
[·]i entry/hàng thứ i của vectơ/ma trận đầu vào
[·]i,j entry (i, j) thứ của ma trận đầu vào

Chúng tôi cung cấp bảng 4 các ký hiệu được sử dụng nhiều hơn một lần để độc giả có thể dễ dàng tham khảo định nghĩa của chúng.

6.2 Chi tiết Nén Đa độ phân giải
Chúng tôi mô tả các chi tiết kỹ thuật bị bỏ qua của một công thức sửa đổi của [36] để xây dựng ScC và Sc tương ứng thỏa mãn xấp xỉ tốt của
exp(PC⊤S⊤
c)Sc≈exp(PC⊤). (19)

Trước khi đi sâu vào các chi tiết kỹ thuật của việc xây dựng Sc, chúng tôi giới thiệu một số ký hiệu và công cụ sẽ được sử dụng sau này. Chúng tôi sử dụng [·]i để đề cập đến entry/hàng thứ i của vectơ/ma trận đầu vào và [·]i,j để đề cập đến entry (i, j) thứ của ma trận đầu vào. Các chữ cái in hoa ĐẬM biểu thị ma trận, các chữ cái thường đậm biểu thị vectơ, và các chữ cái thường thông thường biểu thị scalars. Để vi là một vectơ, khi chúng ta viết một ma trận dạng
[v1v2···vm], (20)
chúng ta coi vi như một cột của ma trận. Khi chúng ta viết một ma trận dạng

v1
v2
···
vm
, (21)
chúng ta coi vi như một hàng của ma trận.

--- TRANG 14 ---
6.2.1 Thiết lập Vấn đề Cơ bản
sx−ss1s
Hình 6: Trực quan hóa bs
x cho một số scaling s và translation x. Trục y cho các đồ thị khác nhau không giống nhau.Để bs
x∈Rnc là một thành phần đa độ phân giải được định nghĩa là
[bs
x]i:=1
s nếu sx−s < i≤sx
0 ngược lại (22)
cho s∈ {k0, k1, k2,···, nc} giả định nc là lũy thừa của k (chúng tôi giả định k= 2 để đơn giản). Ở đây, s và x đại diện cho scaling và translation (tương tự như các khái niệm trong wavelet basis) của thành phần, tương ứng. Hình 6 là trực quan hóa của bs
x.

Sau đó, bất kỳ tín hiệu 1D f∈Rnc nào có thể được đại diện như một tổ hợp tuyến tính của bs
x:
f=X
s,xcs
xbs
x (23)
trong đó cs
x là các hệ số cho tổ hợp tuyến tính. Đối với một tín hiệu có cấu trúc đa độ phân giải (tức là, tín hiệu có tần số cao trong một số vùng và có tần số thấp trong các vùng khác), chúng ta có thể tìm một xấp xỉ ˆf∗ có thể được biểu diễn như một tổ hợp tuyến tính thưa thớt trong đó hầu hết các hệ số là zero, như thể hiện trong Hình 7.
f≈ˆf∗:=X
bsx∈Jcs
xbs
x (24)

Chúng tôi ký hiệu J là tập hợp các thành phần chính bs
x tương ứng với các hệ số lớn, tức là,
J:={bs
x| |cs
x| lớn }. Vì tập hợp tất cả bs
x có thể là một từ điển over-complete, có nhiều tổ hợp tuyến tính có thể. Để giảm không gian tìm kiếm của tập hợp J tốt nhất, chúng tôi đặt một hạn chế nhẹ trên tập hợp J:
X
bsx∈Jbs
x

i̸= 0∀i ⟨bs
x,bs′
x′⟩= 0∀bs
x,bs′
x′∈ J,bs
x̸=bs′
x′ (25)

Các điều kiện nói rằng mỗi entry của tín hiệu f được bao gồm trong vùng hỗ trợ của chính xác một thành phần trong J. Với những công cụ này, trước tiên chúng tôi sẽ mô tả xấp xỉ khi J được cho, sau đó thảo luận cách xấp xỉ kết nối tập hợp J với ScandScC mục tiêu của chúng tôi. Cuối cùng, chúng tôi sẽ thảo luận cách xây dựng J này.

0.00.5f(x)tín hiệu
0 100 200 300 400 500
x0.00.5f(x)xấp xỉ
Hình 7: Một ví dụ về xấp xỉ một tín hiệu 1D sử dụng wavelet transform bị cắt ngắn với các thành phần được định nghĩa trong (22). Nó sử dụng một tập hợp J có kích thước 79 để đại diện cho một tín hiệu trong R512.

6.2.2 Kết nối Vấn đề của chúng tôi vào Thiết lập
Một kết quả gần đây cho thấy ma trận self-attention exp(XX⊤) có cấu trúc đa độ phân giải được thảo luận ở trên [36]. Vì exp(PC⊤) là một ma trận con của exp(XX⊤), chúng tôi phỏng đoán rằng cấu trúc đa độ phân giải cũng giữ trong exp(PC⊤). Kết quả là, chúng ta có thể tìm một tổ hợp thưa thớt của bs
x để đại diện cho các hàng của exp(PC⊤).

Mệnh đề 6.1. Cho tập hợp J thỏa mãn hạn chế (25), chúng ta có thể định nghĩa một xấp xỉ của hàng thứ i của exp(PC⊤) tương tự như (24) như được minh họa trong Hình 7
h\exp(PC⊤)∗i
i:=X
bsx∈Jcs
xbs
x (26)
trong đó cs
x là nghiệm tối ưu giảm thiểu




exp(PC⊤)
i−h\exp(PC⊤)∗i
i


2
2(27)

Sau đó, xấp xỉ có thể được viết là:
h\exp(PC⊤)∗i
i,j=⟨
exp(PC⊤)
i,bs
x⟩ (28)
trong đó bs
x∈ J là thành phần được hỗ trợ trên j (a.k.a. [bs
x]j̸= 0 và có chính xác một bs
x∈ J thỏa mãn điều kiện này do hạn chế (25)).

Chứng minh. Nếu J được cho, để B∈Rnc×|J| là một ma trận có các cột là phần tử bs
x∈ J và để c∈R|J| là một vectơ có các entries là cs
x tương ứng:
B:=
bs1x1bs2x2···bs|J|
x|J|
c:=
cs1x1cs2x2···cs|J|
x|J|⊤(29)

sau đó xấp xỉ có thể được biểu diễn là
h\exp(PC⊤)∗i
i=X
bsx∈Jcs
xbs
x=Bc (30)

Nếu chúng ta giải cho
c:= arg min
β


exp(PC⊤)
i−Bβ

 (31)
thì
c= (B⊤B)−1B⊤
exp(PC⊤)
i(32)

Do hạn chế (25), các cột của B là trực giao, vì vậy B⊤B là một ma trận chéo:
B⊤B=
1/s1
1/s2
···
1/s|J|
 (33)

Chúng ta cũng có thể viết ra B⊤
exp(PC⊤)
i
B⊤
exp(PC⊤)
i=
⟨
exp(PC⊤)
i,bs1x1⟩
⟨
exp(PC⊤)
i,bs2x2⟩
···
⟨
exp(PC⊤)
i,bs|J|
x|J|⟩
(34)

Kết hợp tất cả lại, chúng ta có
Bc=
s1bs1x1s2bs2x2···s|J|bs|J|
x|J|
⟨
exp(PC⊤)
i,bs1x1⟩
⟨
exp(PC⊤)
i,bs2x2⟩
···
⟨
exp(PC⊤)
i,bs|J|
x|J|⟩
(35)

Chúng tôi lưu ý rằng sbs
x đơn giản tái scale entry của bs
x sao cho bất kỳ entry khác không nào trở thành 1. Sau đó, hãy xem xét entry thứ j của Bc. Do hạn chế (25), chúng ta có chính xác một bs
x∈ J có vùng hỗ trợ chứa j, vì vậy hàng thứ j của ma trận đầu tiên ở phía bên phải của (35) chứa chính xác một 1 và các entries còn lại là 0. Do đó, chúng ta có
h\exp(PC⊤)∗i
i,j= [Bc]j=⟨
exp(PC⊤)
i,bs
x⟩ (36)
trong đó bs
x∈ J là thành phần được hỗ trợ trên j, kết thúc chứng minh của chúng tôi.

--- TRANG 15 ---
6.2.3 Xấp xỉ Hiệu quả
Chúng tôi lưu ý rằng việc tính (28) cho tất cả j sẽ yêu cầu truy cập vào toàn bộ
exp(PC⊤)
i. Chúng tôi khai thác cùng chiến lược như được mô tả trong [36], vì vậy exponential của inner product được sử dụng như một xấp xỉ cho inner product của exponential.
h\exp(PC⊤)i
i,j:= exp( ⟨
PC⊤
i,bs
x⟩) (37)

Chúng tôi lưu ý rằng ⟨
PC⊤
i,bs
x⟩ là trung bình cục bộ của vùng hỗ trợ của bs
x, cũng là phân đoạn s-length thứ x của chuỗi
PC⊤
i:
⟨[PC⊤]i,bs
x⟩=1
sX
[bsx]j̸=0
PC⊤
i,j(38)

Bằng cách sử dụng một số thao tác số học, (37) có thể được tính hiệu quả
exp(⟨[PC⊤]i,bs
x⟩) = exp(1
sX
[bsx]j̸=0
PC⊤
i,j) = exp(1
sX
(bsx)j̸=0⟨[P]i,[C]j⟩)
= exp( ⟨[P]i,1
sX
[bsx]j̸=0[C]j⟩) = exp( ⟨[P]i,cs
x⟩)(39)

trong đó cs
x được định nghĩa trong văn bản chính:
cs
x:=1
sX
sx−s<i≤sx[C]i=bs
xC(40)

Chúng tôi lưu ý rằng cs
x là trung bình cục bộ của phân đoạn s-length thứ x của chuỗi C. cs
x có thể được tính hiệu quả thông qua
c2s
x=1
2cs
2x−1+1
2cs
2x c1
x= [C]x(41)

Mệnh đề 6.2. Cho tập hợp J thỏa mãn hạn chế (25), để Sc là ma trận có các hàng là phần tử bs
x∈ J
Sc=
bs1x1bs2x2···
bs|J|
x|J|
 D=
s1
s2
···
s|J|
 (42)

Thì,
exp(PC⊤S⊤
c)DSc=\exp(PC⊤) (43)
trong đó \exp(PC⊤) được định nghĩa như (37).

Chứng minh. Xem xét hàng thứ i của exp(PC⊤S⊤
c),

exp(P(ScC)⊤)
i= exp([ P]i(ScC)⊤)
= exp([ P]i
cs1x1cs2x2···cs|J|
x|J|
)
=
exp(⟨[P]i,cs1x1⟩)··· exp(⟨[P]i,cs|J|
x|J|⟩)(44)

Sau đó, chúng ta có

exp(P(ScC)⊤)DSc
i=
exp(⟨[P]i,cs1x1⟩)··· exp(⟨[P]i,cs|J|
x|J|⟩)
s1bs1x1···
s|J|bs|J|
x|J|
 (45)

Chúng tôi lưu ý rằng sbs
x đơn giản tái scale entry của bs
x sao cho bất kỳ entry khác không nào trở thành 1. Sau đó, hãy xem xét entry thứ j của
exp(PC⊤S⊤
c)DSc
i. Do hạn chế (25), chúng ta có chính xác một bs
x∈ J có vùng hỗ trợ chứa j, vì vậy cột thứ j của ma trận thứ hai ở phía bên phải của (45) chứa chính xác một 1 và các entries còn lại là 0. Do đó, chúng ta có

exp(P(ScC)⊤)DSc
i,j= exp( ⟨[P]i,cs
x⟩) = exp( ⟨[PC⊤]i,bs
x⟩) =h\exp(PC⊤)i
i,j(46)

trong đó bs
x∈ J là thành phần được hỗ trợ trên j. Đẳng thức thứ hai dựa trên (39).

Mệnh đề 6.3. Nếu Sc và D được định nghĩa như Mệnh đề 6.2, pseudo inverse của Sc đơn giản là S†
c=S⊤
cD, vì vậy mỗi hàng của S†
c và S† chứa chính xác một 1 (vì vậy số lượng entries khác không của S†
c và S† là nc và n tương ứng).

Chứng minh. Vì mỗi hàng của Sc là một số bs
x∈ J, do hạn chế (25), với i̸=j,

ScS⊤
cD
i,i=⟨[Sc]i,[DSc]i⟩=⟨bsi
xi, sibsi
xi⟩=si1
si= 1

ScS⊤
cD
i,j=⟨[Sc]i,[DSc]j⟩=⟨bsi
xi, sjbsj
xi⟩=sj0 = 0(47)

Kết quả là, ScS⊤
cD=I. Hơn nữa, S⊤
cDSc là một ma trận đối xứng. Vì vậy, tất cả điều kiện Moore-Penrose được xác minh. S†
c=S⊤
cD.

Từ hạn chế (25), chúng ta có mỗi cột của Sc chứa chính xác một entry khác không. Ngoài ra,
S†
c=S⊤
cD=
s1bs1x1s2bs2x2···s|J|bs|J|
x|J|
(48)

Vì entry khác không của bs
x đơn giản là 1
s theo định nghĩa, sbs
x đơn giản tái scale entry của bs
x sao cho bất kỳ entry khác không nào trở thành 1. Kết quả là, mỗi hàng của S†
c có chính xác một 1. Ngoài ra, bởi quan hệ giữa S và Sc:
S=
Inp×np0
0 Sc
S†=Inp×np0
0 S†
c
(49)
mỗi hàng của S† có chính xác một 1.

Cuối cùng, xấp xỉ
\exp(PC⊤) = exp( PC⊤S⊤
c)DSc≈exp(PC⊤) (50)
không trông chính xác như (19), nhưng chúng ta có thể chèn một ma trận chéo đơn giản D vào công thức (19) và làm cho toàn bộ thứ hoạt động.

6.2.4 Cách Xây dựng J cho Sc và ScC?
Sự dẫn xuất cho đến nay giả định truy cập vào J, nhưng trong thực tế, chúng ta không có kiến thức về J và cần xây dựng J dẫn đến xấp xỉ tốt. Với sơ đồ xấp xỉ tại chỗ, giờ đây chúng ta có thể phân tích lỗi xấp xỉ, sẽ được tận dụng sau để tìm một tập hợp thành phần hợp lý J. Lỗi xấp xỉ của hàng thứ i của exp(PC⊤) có thể được biểu diễn là
Ei:=



exp(PC⊤)
i−h\exp(PC⊤)i
i


2
F
=ncX
j=1(
exp(PC⊤)
i,j−h\exp(PC⊤)i
i,j)2
=X
bsx∈JX
[bsx]j̸=0(
exp(PC⊤)
i,j−exp(⟨[PC⊤]i,bs
x⟩))2
=X
Bsx∈Jexp(⟨[PC⊤]i,bs
x⟩)X
(Bsx)j̸=0(exp(
PC⊤
i,j− ⟨[PC⊤]i,bs
x⟩)−1)2
=X
Bsx∈Jexp(⟨[P]i,cs
x⟩)X
(Bsx)j̸=0(exp(⟨[P]i,[C]j⟩ − ⟨[P]i,cs
x⟩)−1)2
=X
Bsx∈Jexp(⟨[P]i,cs
x⟩)X
(Bsx)j̸=0(exp(⟨[P]i,[C]j−cs
x⟩)−1)2(51)

--- TRANG 18 ---
Thuật toán 1 Xây dựng J
Đầu vào: VIP-tokens P và cs
x cho tất cả s và x
Đầu vào: hs: số lượng phân đoạn s-length để tinh chỉnh cho mỗi s∈ {2,4,···, nc}
Khởi tạo J rỗng
Tính µnc
1 (53) và thêm bnc
1 (22) vào J (tính nút gốc)
cho s←nc, nc/2,···,2 do
Pop h phần tử bs
x có µs
x lớn nhất (53) (chọn các nút với điểm attention cao hơn)
cho mỗi bs
x do
Tính µs/2
2x−1, µs/2
2x (53) và thêm bs/2
2x−1,bs/2
2x (22) vào J (chia các nút được chọn)
end for
end for
Đầu ra: J

Đẳng thức thứ hai và thứ tư là do (25) và (39). Lỗi xấp xỉ được chi phối bởi hai thành phần nhân với nhau: điểm attention giữa [P]i và trung bình cục bộ cs
x của phân đoạn s-length thứ x của chuỗi C và inner product của [P]i với lượng độ lệch của [C]j từ trung bình cục bộ cs
x của nó.

Khi s= 1, độ lệch đơn giản là zero:
c1
x= [C]x. (52)

Có lý khi giả định rằng độ lệch [C]j−cs
x nhỏ hơn nếu s nhỏ hơn. Do đó, điều này thực sự đề xuất một heuristic đơn giản để chọn J: khi exp(⟨[P]i,cs
x⟩) lớn, chúng ta nên xấp xỉ phân đoạn s-length thứ x của C với độ phân giải cao hơn (bằng cách chia phân đoạn thành các phân đoạn con ngắn hơn và sử dụng xấp xỉ tinh tế hơn). Heuristic này mô tả tiêu chí lựa chọn cho một hàng của exp(PC⊤), tương ứng với một VIP-token duy nhất, cho nhiều hàng của exp(PC⊤) (cho nhiều VIP-tokens), chúng tôi đơn giản sử dụng
µs
x=npX
i=1exp(⟨[P]i,cs
x⟩) (53)
như tiêu chí lựa chọn vì J được chia sẻ bởi tất cả VIP-tokens.

Việc xây dựng J được mô tả trong Alg. 1. Thuật toán này mô tả cùng thủ tục như Hình 3 trong văn bản chính. Các bs
x trong J là các hàng của Sc, và các cs
x tương ứng (40) là các hàng của ScC. Ngân sách h2, h4,···, hnc yêu cầu bởi Alg. 1 được sử dụng để xác định số lượng thành phần ở mỗi độ phân giải sẽ được thêm vào J. Cụ thể, có 2h2s−hs số lượng thành phần bs
x cho s̸= 1 dựa trên tính toán đơn giản. Chúng ta có thể chọn ngân sách sao cho kích thước cuối cùng của J là r−np để làm cho độ dài của chuỗi nén là r.

6.2.5 Xấp xỉ này Tốt như thế nào?
Ở cấp độ cao, việc nén Sc thực hiện nén nhiều hơn trên các tokens không liên quan đến VIP-tokens và ít nén hơn đối với các tokens quan trọng đối với VIP-tokens. Chúng tôi sẽ thảo luận chi tiết hơn. Vì mỗi hàng của S† chứa chính xác một 1 như đã nêu trong Mệnh đề 6.3, S† có thể giao hoán với β, vì vậy tóm lại, chúng ta có thể viết xấp xỉ của tính toán của một lớp Transformer là
α(P,SX,SX) = exp( PP⊤)P+ exp( PC⊤S⊤
c)DScC
S†
cα(ScC,SX,SX) =S†
cexp(ScCP⊤)P+S†
cexp(ScCC⊤S⊤
c)DScC
Pnew
Cnew
=β(α(P,SX,SX) +P) +α(P,SX,SX)
β(S†
cα(ScC,SX,SX) +S†
cScC) +S†
cα(ScC,SX,SX)
+
P
C(54)

Lưu ý rằng D được thêm vào như đã thảo luận trong (50).

Có bốn thành phần xấp xỉ chính (màu tím) trong (54). Lấy thực tế rằng DSc= (S†
c)⊤, tất cả những xấp xỉ này là xấp xỉ không gian hàng hoặc cột đa độ phân giải được chi phối bởi ma trận Sc. Trọng số attention cao ngụ ý sự phụ thuộc cao hơn, và thủ tục trong Alg. 1 tinh chỉnh các vùng có trọng số attention lớn với độ phân giải cao hơn. Do đó, embedding token trong C có sự phụ thuộc cao hơn với P được xấp xỉ tốt hơn. Đầu ra Pnew được xấp xỉ tốt theo thiết kế vì xấp xỉ bảo tồn các thành phần tần số cao hơn của tập con các hàng của C có tác động cao đến đầu ra Pnew. Hơn nữa, đầu ra trong Cnew tương ứng với tập con các hàng của C có sự phụ thuộc cao hơn với VIP-tokens sẽ có xấp xỉ tốt hơn so với các hàng còn lại của C. Tính chất này giải quyết vấn đề là một số tokens có vị trí không xác định cũng liên quan đến dự đoán cuối cùng của Transformer trong một số tác vụ. Ví dụ, trong các tác vụ trả lời câu hỏi, các câu trả lời ứng viên thường được mong đợi có sự phụ thuộc lớn với question tokens (VIP-tokens), vì vậy chúng cũng được xấp xỉ tốt. Tính chất xấp xỉ này chính xác là những gì chúng ta cần.

6.2.6 Quan hệ với [36] truyền cảm hứng cho Nén Đa độ phân giải
Công việc của chúng tôi và [36] có thể được xem như hoạt động ở các mức độ trừu tượng hơi khác nhau. Trong khi [36] cố gắng xấp xỉ tính toán self-attention một cách hiệu quả, bài báo của chúng tôi đề xuất một khung tổng quát để thực hiện nén tập trung VIP-token trên chuỗi để xử lý hiệu quả các chuỗi cực dài (mô-đun self-attention vẫn hoàn toàn không thay đổi). Việc nén tập trung VIP-token của chúng tôi liên quan đến một số bước được mô tả trong văn bản chính. Nhưng một trong những bước chính liên quan đến việc xây dựng một ma trận nén Sc có một số tính chất mong muốn, cụ thể là thỏa mãn (19) mà chúng tôi elaboration thêm dưới đây.

Lưu ý rằng đối với phương trình (19), chúng ta cần một ma trận Sc sao cho ma trận attention xấp xỉ liên quan đến P và C tương tự như ma trận attention thực sự liên quan đến P và C. Đây chính xác là nơi ý tưởng chung của [36] có thể được sử dụng. Nhưng công thức trong [36] không thể được áp dụng trực tiếp trong dạng ban đầu của nó vì nó không thể cho chúng ta Sc. Tại sao? Một lý do là công thức trong [36] không thể được viết dưới dạng ma trận tương tự như phương trình (19). Đây có thể là lý do tại sao [36] phải sử dụng các CUDA kernels tùy chỉnh trong triển khai của họ. Tuy nhiên, các tính chất của [36] hữu ích. Vì vậy, chúng tôi suy ra dạng tương tự nhưng cho 1D thay vì: trường hợp 1D này được biểu diễn như áp dụng một ma trận (đây là Sc chúng ta đang tìm kiếm) cho tín hiệu C.

Một bonus của sửa đổi này là nó cũng loại bỏ nhu cầu về các CUDA kernels tùy chỉnh. Ở cấp độ cao, [36] cung cấp một cái nhìn đa độ phân giải của các ma trận self-attention, và phiên bản sửa đổi của chúng tôi tốt nhất được coi như một cái nhìn đa độ phân giải tương tự của chính chuỗi. Nhưng chúng ta cũng có thể thay thế bằng một phương tiện khác để có được Sc (có thể đơn giản là một ma trận sketching). Cuối cùng, chúng tôi lưu ý rằng một triển khai naïve của sửa đổi kết quả vẫn yêu cầu chi phí O(ncd) do tính toán cs
x cho tất cả scaling s và translation x có thể. Có một chi phí tương tự trong [36] (đoạn thứ hai trong phần 4.4 trong [36]). Cấu trúc dữ liệu chúng tôi đề xuất giảm chi phí này.

6.3 Chi tiết Cấu trúc Dữ liệu Đề xuất
Trong phần này, chúng tôi mô tả một số chi tiết kỹ thuật bị bỏ qua của cấu trúc dữ liệu đề xuất T(·).

6.3.1 Tại sao (cnew)1
1−c1
1= (cnew)1
2−c1
2= (cnew)2
1−c2
1 nếu ScC= c2
1c1
3c1
4c4
2⊤?

Mệnh đề 6.4. Cho tập hợp J thỏa mãn hạn chế (25), nếu bs
x∈ J, thì (cnew)s
x−cs
x= (cnew)s′
x′−cs′
x′ cho tất cả bs′
x′ thỏa mãn hỗ trợ của bs′
x′ được chứa trong hỗ trợ của bs
x (phân đoạn s′-length thứ x′ của C là một phân đoạn con của phân đoạn s-length thứ x của C).

Chứng minh. Để đơn giản hóa ký hiệu một chút, không mất tính tổng quát, chúng tôi giả định x= 1. Sau đó, với i≤s, xem xét (cnew)1
i:
(cnew)1
i= [Cnew]i=
S†
cβ(α(ScC,SX,SX) +ScC) +S†
cα(ScC,SX,SX)
i+ [C]i
=
S†
c
iβ(α(ScC,SX,SX) +ScC) +
S†
c
iα(ScC,SX,SX) +c1
i(55)

Bởi Mệnh đề 6.3, S†
c=S⊤
cD và hàng thứ i của S†
c chứa chính xác một 1. Cột chứa 1 trong hàng thứ i của S†
c chính xác là sbs
1 vì i được chứa trong hỗ trợ của chính xác một thành phần trong J do hạn chế (25). Ký hiệu chỉ số cột này là j, thì
(cnew)1
i= [β(α(ScC,SX,SX) +ScC)]j+ [α(ScC,SX,SX)]j+c1
i (56)

Lưu ý rằng điều này đúng cho tất cả i≤s. Kết quả là, với i, i′≤s,
(cnew)1
i−c1
i= [β(α(ScC,SX,SX) +ScC)]j+ [α(ScC,SX,SX)]j= (cnew)1
i′−c1
i′(57)

--- TRANG 20 ---
Sau đó,
(cnew)2
⌈i/2⌉−c2
⌈i/2⌉=1
2(cnew)1
2⌈i/2⌉−1+1
2(cnew)1
2⌈i/2⌉−1
2c1
2⌈i/2⌉−1−1
2c1
2⌈i/2⌉
=1
2((cnew)1
2⌈i/2⌉−1−c1
2⌈i/2⌉−1) +1
2((cnew)1
2⌈i/2⌉−c1
2⌈i/2⌉)
= (cnew)1
2⌈i/2⌉−1−c1
2⌈i/2⌉−1(58)

Phần còn lại theo từ induction.

6.3.2 Làm thế nào chúng ta có được (cnew)2
1,(cnew)1
3,(cnew)1
4,(cnew)4
2 nếu ScC= c2
1c1
3c1
4c4
2⊤?

Mệnh đề 6.5. Chúng ta có
ScCnew=β(α(ScC,SX,SX) +ScC) +α(ScC,SX,SX) +ScC. (59)

Và biểu diễn cập nhật (cnew)s
x của cs
x tương ứng (một hàng của ScC) là hàng tương ứng của ScCnew.

Chứng minh. Theo định nghĩa,
Cnew=S†
cβ(α(ScC,SX,SX) +ScC) +S†
cα(ScC,SX,SX) +C (60)

Sau đó,
ScCnew=ScS†
cβ(α(ScC,SX,SX) +ScC) +ScS†
cα(ScC,SX,SX) +ScC
=β(α(ScC,SX,SX) +ScC) +α(ScC,SX,SX) +ScC(61)

Vì Sc giống nhau cho ScCnew và ScC, phát biểu thứ hai theo sau.

6.3.3 Thuật toán để Làm T(C) thành T(Cnew)
Trong phần này, chúng tôi mô tả thuật toán chính xác để cập nhật T(C) thành T(Cnew). Mã giả được mô tả trong Alg. 2 trong đó cs
x được tính thông qua
cs
x=c2s
⌈x/2⌉−∆cs
x=c4s
⌈x/4⌉−∆c2s
⌈x/2⌉−∆cs
x=··· (63)

Chúng tôi sử dụng thuật ngữ "dirty" trong Alg. 2 để chỉ ra nút cần được xử lý do cập nhật nút. Thuật ngữ này thường được sử dụng trong triển khai cache máy tính để chỉ ra rằng dữ liệu của một vị trí cụ thể đã được cập nhật và cần được tính đến.

Thuật toán 2 Tính toán một lớp Transformer với T(C)
Đầu vào: VIP-tokens P và cấu trúc dữ liệu T(C)
Sử dụng Algo. 1 để xây dựng J nhưng sử dụng (63) để truy xuất cs
x từ T(C)
Xây dựng Sc,ScC liên kết với J sử dụng Mệnh đề 6.2
Tính

Pnew
ScCnew
=
β(α(P,SX,SX) +P) +α(P,SX,SX) +P
β(α(ScC,SX,SX) +ScC) +α(ScC,SX,SX) +ScC
(62)

Đặt T(Cnew)← T(C)
cho s←1,2,4,···nc/2 do
cho bs
x∈ J do
Định vị vị trí hàng bs
x trong Sc, tham chiếu chỉ số như j
Tính (cnew)s
x= [ScCnew]j
Đánh dấu ∆(cnew)s
x dirty
end for
cho ∆(cnew)s
x dirty do
Tính (cnew)2s
⌈x/2⌉ và cập nhật ∆(cnew)s
x
Đánh dấu ∆(cnew)2s
⌈x/2⌉ dirty
end for
end for
Cập nhật (cnew)nc
1
Đầu ra: VIP-tokens Pnew và cấu trúc dữ liệu T(Cnew)

6.4 Phân tích Độ phức tạp
Trong phần này, chúng tôi sẽ thảo luận phân tích độ phức tạp chi tiết của phương pháp đề xuất của chúng tôi. Độ phức tạp tổng thể của phương pháp đề xuất của chúng tôi là O(lrd2+lr2d+lrlog(nc)d+lrnpd+nd) khi sử dụng cấu trúc dữ liệu hiệu quả đề xuất.

6.4.1 Chuẩn bị Chuỗi Đầu vào cho T(C): O(nd)
Ở lớp đầu tiên, chúng ta cần hoán vị các hàng của X thành [P;C], tốn chi phí O(nd). Sau đó, chúng ta xử lý C thành T(C). Điều này yêu cầu (1) tính cs
x được định nghĩa trong (41). c1
x= [C]x, vì vậy không cần tính toán. Với tất cả c1
x được cho, tính tất cả c2
x tốn O(ncd/2). Với tất cả c2
x được cho, tính tất cả c4
x tốn O(ncd/4)... Vì vậy, chi phí là
O(ncd/2 +ncd/4 +···+d) =O(ncd). (64)

Sau đó (2) tính ∆cs
x cho tất cả s và x. Tính mỗi ∆cs
x tốn O(d) khi có cs
x và c2s
⌈x/2⌉. Lượng chi phí giống như số lượng nút trong cây T(C), vì vậy chi phí là O(ncd). Lưu ý rằng nc< n, vì vậy độ phức tạp tổng thể của các hoạt động trên là O(nd).

6.4.2 Xây dựng J,Sc,ScC: O(lrlog(nc)d+lrnpd)
Chúng ta có thể phân tích độ phức tạp của việc xây dựng J sử dụng Algo. 1. Chỉ có một µnc
x có thể. Sau đó cho mỗi s, có 2hs số lượng µs/2
x được tính vì có 2 thành phần bs/2
x cho mỗi bs
x′. Kết quả là, chúng ta cần tính O(1 +P
s2hs) số lượng µs/2
x. Khi cs/2
x được cho, chi phí tính một µs/2
x là O(npd), vì vậy chi phí tổng thể của việc xây dựng J là O((1 +P
s2hs)npd).

Hơn nữa, ở mỗi s, kích thước của J được tăng bởi hs vì hs phân đoạn được chia thành 2hs phân đoạn con, vì vậy kích thước của J là O(P
shs). Vì Sc∈R(r−np)×n và |J |=r−np như đã thảo luận trong §6.2.4, O(r−np) =O(P
shs). Chúng tôi sử dụng O(r) để đơn giản thay vì O(r−np). Kết quả là, chi phí tổng thể của việc xây dựng J là O(rnpd).

Chi phí trên giả định cs/2
x được cho. Nếu chúng ta tính tất cả cs/2
x có thể sử dụng (41), chi phí sẽ là O(ncd) như được phân tích trong §6.4.1. Tuy nhiên, nếu chúng ta sử dụng cấu trúc dữ liệu đề xuất, mỗi cs/2
x có thể được truy xuất trong tối đa O(log(nc)d) bằng cách tính đệ quy (63). Vì chúng ta cần truy xuất O(1 +P
s2hs) =O(r) số lượng cs
x, độ phức tạp của việc tính cs
x cần thiết là O(rlog(nc)d).

Kết quả là, độ phức tạp của việc xây dựng J là O(rnpd+rlog(nc)d) ở mỗi lớp. Khi tổng chi phí trên tất cả các lớp, độ phức tạp là O(lrnpd+lrlog(nc)d).

Bởi Mệnh đề 6.2, các hàng của Sc và ScC đơn giản là bs
x∈ J và cs
x tương ứng, đã được tính trong quá trình xây dựng J, vì vậy chúng ta về cơ bản có thể có được những Sc và ScC này miễn phí.

6.4.3 Cung cấp Chuỗi Nén vào một Lớp Transformer: O(lrd2+lr2d)
Ở mỗi lớp, chúng ta cần tính

Pnew
ScCnew
=
β(α(P,SX,SX) +P) +α(P,SX,SX) +P
β(α(ScC,SX,SX) +ScC) +α(ScC,SX,SX) +ScC
(65)

để cập nhật T(C). Đây là phần của một lớp Transformer yêu cầu tính toán nặng. Có thể xác minh rằng độ phức tạp của một lớp Transformer là O(nd2+n2d) cho một chuỗi đầu vào có độ dài n. Bây giờ một chuỗi nén có độ dài r được cung cấp vào một lớp Transformer, chi phí đơn giản là O(rd2+r2d). Chúng tôi lưu ý rằng có một tái scale bổ sung để cắm D vào exp(PC⊤S⊤
c)DSc trong quá trình tính toán multi-head attention được thảo luận trong 50. Tuy nhiên, chi phí bổ sung của việc áp dụng D là O(rd), không thay đổi độ phức tạp. Khi tổng chi phí của tất cả các lớp, độ phức tạp tổng thể là O(lrd2+lr2d).

6.4.4 Cập nhật T(C) thành T(Cnew): O(lrd)
Một khi (65) được tính, chúng ta cần thay đổi T(C) thành T(Cnew). Chi phí thay đổi T(C) thành T(Cnew) là O(rd) như được phân tích trong văn bản chính. Để phân tích cụ thể hơn, hãy xem ba lần lặp đầu tiên:

--- TRANG 22 ---
(1) Ở lần lặp đầu tiên, có O(2h2) số lượng (cnew)1
x cần được tính ở vòng lặp trong đầu tiên, và có O(2h2) số lượng ∆(cnew)1
x cần được cập nhật trong vòng lặp trong thứ hai. Thêm O(h2) số lượng ∆(cnew)2
⌈x/2⌉ được đánh dấu dirty.

(2) Ở lần lặp thứ hai, có O(2h4) số lượng (cnew)2
x cần được tính ở vòng lặp trong đầu tiên, và có O(2h4+h2) số lượng ∆(cnew)2
x cần được cập nhật trong vòng lặp trong thứ hai. Thuật ngữ thứ hai là do ∆(cnew)2
⌈x/2⌉ dirty từ lần lặp đầu tiên. Thêm O(h4+h2
2) số lượng ∆(cnew)4
⌈x/2⌉ được đánh dấu dirty.

(3) Ở lần lặp thứ ba, có O(2h8) số lượng (cnew)4
x cần được tính ở vòng lặp trong đầu tiên, và có O(2h8+h4+h2
2) số lượng ∆(cnew)4
x cần được cập nhật trong vòng lặp trong thứ hai. Thuật ngữ thứ hai và thứ ba là do ∆(cnew)4
⌈x/2⌉ dirty từ lần lặp thứ hai. Thêm O(h8+h4
2+h2
4) số lượng ∆(cnew)8
⌈x/2⌉ được đánh dấu dirty.

Rõ ràng rằng nếu chúng ta tổng số lượng tính toán của (cnew)s
x và cập nhật của ∆(cnew)s
x, tổng số là O(P
s2hs+ 2P
sPlog(s)
j=1hs
2j) =O(P
shs+P
shs) =O(r). Vì mỗi tính toán và cập nhật tốn chi phí O(d), độ phức tạp tổng thể của việc thay đổi T(C) thành T(Cnew) là O(rd). Khi tổng chi phí của tất cả các lớp, độ phức tạp tổng thể là O(lrd).

6.4.5 Materialize Cnew từ T(Cnew) ở Lớp Cuối cùng: O(nd)
Ở đầu ra của lớp cuối cùng, chúng ta có thể (1) tính tất cả (cnew)nc/2
x thông qua (63) với chi phí O(2d), (2) tính (cnew)nc/4
x thông qua (63) với chi phí O(4d)... cho đến khi tất cả (cnew)1
x được tính. Sau đó, [Cnew]x=c1
x được materialize từ T(Cnew) với tổng chi phí
O(d+ 2d+ 4d+···+ncd) =O(ncd). (66)

Cuối cùng, hoàn tác hoán vị để [Pnew;Cnew] được sắp xếp lại về vị trí ban đầu có độ phức tạp O(nd). Kết quả là, độ phức tạp tổng thể là O(nd).

6.4.6 Độ phức tạp Tổng thể
Tóm lại, độ phức tạp tổng thể của phương pháp chúng tôi là
O(lrd2+lr2d+lrlog(nc)d+lrnpd+nd) (67)

6.5 Ứng dụng Tiềm năng cho Decoders
Để giảm độ phức tạp của triển khai, phương pháp được đề xuất cho mô-đun encoder của Transformer giả định truy cập đầy đủ vào toàn bộ chuỗi. Việc nén được đề xuất có thể được mở rộng để xấp xỉ tính toán trong decoder, nhưng nó yêu cầu nhiều nỗ lực triển khai hơn, vì vậy chúng tôi để lại cho công việc tương lai. Chúng tôi mô tả ngắn gọn hai lựa chọn có thể để làm như vậy. (1) Chúng ta có thể sử dụng các input tokens của decoder làm VIP-tokens để nén các biểu diễn của chuỗi context được tạo ra bởi encoder trước tính toán Cross Attention để giảm chi phí của Cross Attention. (2) Giải mã tự hồi quy hoạt động sử dụng Causal Attention ở mỗi bước. Hoạt động Causal Attention này yêu cầu bộ nhớ và tính toán tuyến tính theo độ dài của prefix. Chúng ta có thể giữ cùng VIP-token Causal Attention (biểu diễn của token hiện đang được tạo) và áp dụng phương pháp của chúng tôi để nén các biểu diễn của các tokens được tạo trước đó. Điều này giảm độ phức tạp tuyến tính của hoạt động Causal Attention xuống dưới tuyến tính. Điều này hữu ích để giảm chi phí suy luận. Đối với training, chúng ta có thể chia chuỗi thành hai phân đoạn: phân đoạn prefix và phân đoạn decoding. Sau đó, chúng ta có thể sử dụng nén được đề xuất trong phân đoạn prefix và tính toán vanilla trong phân đoạn decoding. Để ngăn look ahead đến các tokens tương lai, chúng ta có thể chỉ sử dụng token đầu tiên trong phân đoạn decoding làm VIP-token.

6.6 Thí nghiệm
Chúng tôi chạy tất cả thí nghiệm trên NVIDIA A100 GPUs. Tất cả mã được triển khai sử dụng framework PyTorch tiêu chuẩn. Không cần CUDA kernels tùy chỉnh. Kết quả là, nó có thể được triển khai dễ dàng cho các nền tảng hoặc frameworks ML khác. Chúng tôi sẽ công bố tất cả mã và checkpoints cần thiết cho tính tái tạo đồng thời với việc công bố bài báo.

--- TRANG 23 ---
Bảng 5: Thống kê độ dài của mỗi tập dữ liệu. Các giá trị là percentiles của số lượng tokens cho các tokenizers cụ thể. Đối với tokenizer T5, giá trị trái của là cho độ dài chuỗi của encoder input, và giá trị phải là cho độ dài chuỗi của decoder input.
RoBERTa T5
Percentile HotpotQA QuALITY WikiHop WikiHop HotpotQA Qasper QuALITY ContractNLI
75th 1535 7603 2204 2399 / 6 1692 / 6 7029 / 29 7747 / 17 2991 / 4
95th 1928 8495 3861 4206 / 9 2129 / 10 10920 / 71 8603 / 28 5061 / 4
T5
Percentile NarrativeQA CNN/Dailymail MediaSum Arxiv SummScreenFD GovReport QMSum MultiNews
75th 90482 / 10 1242 / 87 2621 / 29 13477 / 364 12119 / 188 13304 / 811 19988 / 110 3032 / 379
95th 260533 / 18 1946 / 130 5061 / 64 26024 / 759 16722 / 330 23795 / 983 31749 / 162 6676 / 468

Bảng 6: Kết quả tập dev cho các mô hình encoder-only finetuning trên HotpotQA, QuALITY, và WikiHop.
Phương pháp Kích thước Độ dài HotpotQA QuALITY WikiHop
Thời gian chạy EM F1 Thời gian chạy Độ chính xác Thời gian chạy Độ chính xác
RoBERTa base 512 19.9 35.1 44.9 21.2 39.0 19.6 67.6
RoBERTa base 4k 422.3 62.2 76.1 403.2 39.5 414.1 75.2
Big Bird base 4k 297.9 59.5 73.2 307.0 38.5 293.3 74.5
Longformer base 4k 371.0 59.9 73.6 368.0 27.9 369.7 74.3
Ours base 4k 114.6 60.9 74.6 126.4 39.6 108.0 75.9
Ours-150k base 4k 114.6 60.7 74.1 126.4 39.4 108.0 76.1

6.6.1 Pretraining
Chúng tôi sử dụng tập dữ liệu The Pile được lọc [12] cho tất cả pretrainings. Vì chúng tôi đang sử dụng các tokenizers pretrained công khai, chúng tôi muốn kích hoạt phân phối của corpus pretraining phù hợp tốt với phân phối của corpus được sử dụng để tạo tokenizers. Kết quả là, chúng tôi sử dụng tokens per byte như một proxy cho sự phù hợp của phân phối và lọc ra các thành phần PubMed Central, ArXiv, Github, StackExchange, DM Mathematics [27], Ubuntu IRC, EuroParl [19], YoutubeSubtitles, và Enron Emails [17], có tokens per byte lớn hơn 0.3. Sau đó, corpus còn lại của tập dữ liệu The Pile được sử dụng cho pretraining.

1022×1026×102
Thời gian chạy (ms)6971737577Độ chính xác
RoBERTa
Big Bird
Longformer
Ours
Ours*
Ours-150k
Hình 8: Thời gian chạy mô hình vs độ chính xác WikiHop dev khi sử dụng các hyperparameters cụ thể mô hình khác nhauĐối với các mô hình encoder-only, chúng tôi pretrain RoBERTa cho 750K steps. Một batch bao gồm 8,192 chuỗi có độ dài 512. Tỷ lệ masking cho masked language modeling (MLM) là 15%. Sau đó, các mô hình độ dài 4K được pretrain liên tục từ các checkpoints RoBERTa cho 300k steps. Các positional embeddings được mở rộng bằng cách nhân đôi embedding positional 512 đã pretrain nhiều lần. Đối với RoBERTa độ dài 4K, Longformer, Big Bird và MRA Attention, batch size là 64, và tỷ lệ masking là 15%. Với tỷ lệ masking 15%, có khoảng 616 masked tokens rải rác trong các chuỗi. Chúng tôi thấy rằng việc sử dụng 616 scattered masked tokens làm VIP tokens cho các chuỗi độ dài 4,096 có thể không chỉ ra cho việc nén tập trung VIP-token, vì vậy chúng tôi sử dụng tỷ lệ masking 7.5% và batch size 128 cho phương pháp của chúng tôi. Số lượng masked tokens mỗi chuỗi được giảm, và số lượng tổng dự đoán masked token vẫn giữ nguyên trong quá trình pretraining. Chúng tôi lưu ý rằng với batch size lớn hơn, thời gian chạy wall clock pretraining cho phương pháp của chúng tôi vẫn nhỏ hơn so với baselines. Trong trường hợp có ai quan tâm, chúng tôi cũng cho thấy downstream finetuning trên phương pháp của chúng tôi được pretrain trên cùng số lượng tokens nhưng ít dự đoán masked token hơn trong Tab. 6 và Hình 8, được ký hiệu là Ours-150k. Độ chính xác nhất quán với mô hình của chúng tôi được pretrain trên 300k steps. Đối với pretraining quy mô lớn hơn được ký hiệu với *, chúng tôi pretrain phương pháp của chúng tôi cho 250K steps với batch size 512 và tỷ lệ masking 7.5%.

Đối với kiến trúc encoder-decoder của phương pháp chúng tôi, chúng tôi thực hiện pretraining liên tục từ các checkpoints công khai của T5 cho 250K steps với batch size 256 sử dụng dự đoán masked span. Vì mỗi masked span (bao gồm nhiều tokens) được thay thế bằng một token đặc biệt duy nhất, khi sử dụng tỷ lệ masking là 15%, số lượng tokens đặc biệt trong một chuỗi không quá lớn, chúng tôi giữ tỷ lệ masking 15% không thay đổi.

--- TRANG 24 ---
Bảng 7: Kết quả dev cho các mô hình encoder-decoder trên MultiNews.
Phương pháp Kích thước # Param Độ dài MultiNews
Thời gian chạy R-1 R-2 R-L
T5 base 223M 512 59.2 / 20.5 42.5 15.3 39.0
T5 base 223M 4K 651.2 / 551.8 46.4 18.2 42.6
LongT5 base 248M 8K 721.7 / 550.6 46.7 18.3 42.9
LED base 162M 8K 526.5 / 454.2 46.6 17.8 42.7
Ours base 223M 8K 377.0 / 224.6 46.4 18.1 42.7
T5 large 738M 512 180.8 / 67.0 43.4 15.6 39.8
Ours large 738M 8K 1140.3 / 651.5 48.2 19.2 44.2
Ours 3b 3B 8K 4094.5 / 2696.0 48.9 19.4 44.7

6.6.2 Downstream Finetuning
Thống kê về độ dài chuỗi của các instance trong mỗi tập dữ liệu được tóm tắt trong Tab 5. Các hyperparameters của tất cả thí nghiệm được tóm tắt trong Tab 8. Khi có nhiều giá trị trong một entry, có nghĩa là chúng tôi thực hiện tìm kiếm hyperparameter trên những giá trị này. Lượng tìm kiếm được xác định bởi kích thước của tập dữ liệu. Nếu một tập dữ liệu tương đối lớn, chúng tôi chỉ tìm kiếm learning rate. Nếu một tập dữ liệu nhỏ, chúng tôi bao gồm batch size và số lượng epochs trong tìm kiếm. Đối với tất cả các tác vụ, nếu độ dài chuỗi dài hơn so với độ dài mô hình m, các chuỗi sẽ được cắt ngắn và chỉ có m tokens đầu tiên sẽ được sử dụng. Đối với các mô hình encoder-decoder, chúng tôi sử dụng greedy decoding trong việc tạo chuỗi để đơn giản. Độ dài đầu ra decoder tối đa, được chỉ định trong Tab. 8, được đặt sao cho độ dài tối đa bao phủ độ dài đầu ra của hơn 99% instances. Khi độ dài của việc bao phủ 99% instances lớn hơn 512, chúng tôi chỉ đặt độ dài đầu ra decoder tối đa là 512.

Ngoài ra, chúng tôi cho thấy một thí nghiệm bổ sung trên MultiNews [11] trong Tab. 7, không có trong văn bản chính do giới hạn không gian.

Bảng 8: Hyperparameters cho tất cả thí nghiệm.
LM Encoder-Only Encoder-Decoder
Task HotpotQA QuALITY WikiHop WikiHop HotpotQA CNN/Dailymail MediaSum Qasper
Optimizer Adam Adam Adam Adam Adam Adam Adam Adam
Weight Decay 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
LR Decay Linear Linear Linear Linear Linear Linear Linear Linear
Precision FP16 FP16 FP16 BF16 BF16 BF16 BF16 BF16
Batch Size 32 16 32 32 32 32 32 {16, 32}
Learning Rate {3e-5, 5e-5} {3e-5, 5e-5} {3e-5, 5e-5} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4}
Epochs 10 {10, 20} 10 10 10 10 10 {10, 20}
Warmup Steps 1000 200 1000 1000 1000 1000 1000 200
Max Output Length - - - 32 40 256 256 128
LM Encoder-Decoder
Task QuALITY ContractNLI NarrativeQA Arxiv SummScreenFD GovReport QMSum MultiNews
Optimizer Adam Adam Adam Adam Adam Adam Adam Adam
Weight Decay 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
LR Decay Linear Linear Linear Linear Linear Linear Linear Linear
Precision BF16 BF16 BF16 BF16 BF16 BF16 BF16 BF16
Batch Size {16, 32} {16, 32} 32 32 {16, 32} {16, 32} {16, 32} 32
Learning Rate {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4}
Epochs {10, 20} {10, 20} 5 {10, 20} {10, 20} {10, 20} {10, 20} 10
Warmup Steps 200 1000 1000 1000 200 1000 100 1000
Max Output Length 90 4 47 512 512 512 310 512

6.7 Câu hỏi Thực tế
Tại sao hiệu suất của phương pháp chúng tôi lại tốt hơn so với các mô hình tiêu chuẩn?
Phương pháp của chúng tôi là một xấp xỉ của các mô hình tiêu chuẩn, nên lẽ ra phải kém hơn so với các mô hình tiêu chuẩn, nhưng trong một số trường hợp, hiệu suất của phương pháp chúng tôi tốt hơn so với các mô hình tiêu chuẩn. Chúng tôi tin rằng lý do là inductive bias đúng cải thiện hiệu suất cho các tác vụ với lượng dữ liệu hạn chế. Phương pháp của chúng tôi bị buộc nén thông tin không liên quan và attention được thực hiện trên các chuỗi nén, nhưng trong mô hình tiêu chuẩn với attention tiêu chuẩn, mỗi token có quyền truy cập vào toàn bộ chuỗi, cho phép một mức độ tự do lớn hơn. Kết quả là, nhiều dữ liệu training hơn có thể được yêu cầu để mô hình học được pattern hoặc bias đúng.