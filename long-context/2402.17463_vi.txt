--- TRANG 16 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Mô hình Ngôn ngữ Lớn

Bảng 6. Kết quả QA (Dễ) của CHUNK LLAMA 2 13B/70B và Dynamic-NTK với Llama2 13B, sử dụng nội dung của bài báo này làm đầu vào. Hầu hết các câu hỏi không yêu cầu hiểu biết sâu về bài báo này. Prompt đầu vào có 19388 token. Dynamic-NTK không thể đưa ra câu trả lời đúng cho tất cả các trường hợp thử nghiệm. Kết quả tạo chất lượng thấp được đánh dấu bằng ✗.

Prompt [INST] <<SYS>>Vui lòng đọc bài báo dài và trả lời các câu hỏi << / SYS>>1. Giới thiệu \n\n Khả năng hiểu và xử lý thông tin ngữ cảnh dài là thiết yếu đối với các mô hình ngôn ngữ lớn (LLMs) để phục vụ hiệu quả một loạt ứng dụng rộng lớn. Những ứng dụng này bao gồm phân tích và phản hồi các câu hỏi trong các tệp PDF có kích thước lớn, duy trì lịch sử đối thoại mở rộng, và trao quyền cho các chatbot tương tác... [/INST]

Câu hỏi Bạn có thể đưa ra một tiêu đề cho bài báo này không

CHUNK LLAMA 2 70B Chắc chắn, đây là một số tiêu đề tiềm năng cho bài báo:
1. "Dual Chunk Attention: Một Khung Không cần Huấn luyện cho Mô hình Ngôn ngữ Ngữ cảnh Dài"
2. "Mô hình hóa Ngôn ngữ Ngữ cảnh Dài Hiệu quả với Dual Chunk Attention"
3. "Mở rộng Mô hình Ngôn ngữ Ngữ cảnh Dài mà không cần Huấn luyện: Một Phương pháp Dual Chunk Attention"
4. "DCA: Một Phương pháp Không cần Huấn luyện cho Mô hình hóa Ngôn ngữ Ngữ cảnh Dài"
5. "Mô hình hóa Ngôn ngữ Ngữ cảnh Dài với Dual Chunk Attention: Một Huấn luyện...

CHUNK LLAMA 2 13B Chắc chắn! Dual Chunk Attention: Một Khung Không cần Huấn luyện cho Mở rộng Ngữ cảnh Dài của Mô hình Ngôn ngữ Lớn.
Tiêu đề này làm nổi bật những đóng góp chính của bài báo: khung Dual Chunk Attention (DCA) được đề xuất, cho phép mở rộng ngữ cảnh dài mà không cần huấn luyện bổ sung, và ứng dụng của nó cho các nhiệm vụ khác nhau. Cụm từ "không cần huấn luyện" cũng được bao gồm để nhấn mạnh đổi mới chính của bài báo.

Dynamic NTK 13B Khung Dual Chunk Attention (DCA) được đề xuất cải thiện việc mở rộng ngữ cảnh dài của Lama2 hơn 8 lần, đạt được PPL 5.59 ở cửa sổ ngữ cảnh 2k. ✗(bỏ qua hướng dẫn)

Câu hỏi Mô hình nào có thể được mở rộng lên ngữ cảnh 192k với DCA?

CHUNK LLAMA 2 70B Mô hình Llama2 70B có thể được mở rộng lên cửa sổ ngữ cảnh 192k bằng cách sử dụng DCA, đạt được điểm PPL 5.59

CHUNK LLAMA 2 13B Mô hình Llama2 70B có thể được mở rộng lên ngữ cảnh 192k với DCA.

Dynamic NTK 13B {\end{enumerate}\end{figure}[ht!]
\end{document}\end{document}\begin{table} ✗(lỗi)

Câu hỏi Ngoài Llama2, các tác giả có xác thực phương pháp của họ trên các mô hình khác không?

CHUNK LLAMA 2 70B Có, các tác giả đã xác thực phương pháp của họ trên các mô hình khác, bao gồm
Together-32k,
CodeLlama,
tất cả đều dựa trên Llama. Họ cũng đã so sánh phương pháp của họ với nhiều mô hình mã nguồn mở và độc quyền.

CHUNK LLAMA 2 13B Các tác giả đã xác thực phương pháp của họ trên các mô hình sau:
* Together-32k
* CodeLlama
* Llama2 Long ✗(Chúng tôi không xác minh DCA trên Llama2 Long)

Dynamic NTK 13B Bài báo giới thiệu một khung không cần huấn luyện mới gọi là Dual Chunk Attention (DCA) mở rộng cửa sổ ngữ cảnh hơn 8 lần cho các biến thể Llama2 7B/13B và 20 lần cho các mô hình 70B ✗(bỏ qua hướng dẫn)

Câu hỏi Mô tả corpus tinh chỉnh được sử dụng bởi DCA

CHUNK LLAMA 2 70B Corpus tinh chỉnh được sử dụng bởi DCA là sự kết hợp của dữ liệu đối thoại dài từ ShareGPT và AlpacaGPT4

CHUNK LLAMA 2 13B Corpus tinh chỉnh được sử dụng bởi DCA giống như corpus được sử dụng bởi mô hình Llama2 gốc, là sự kết hợp của tập huấn luyện và tập xác thực của mô hình Llama2 gốc ✗(Chúng tôi không sử dụng tập huấn luyện của Llama)

Dynamic NTK 13B {\end{begin}\begin{table}\end{figure}
[ht!]} \end{document}\end{document}\end{document} ✗(lỗi)

Câu hỏi Llama2 Long cần bao nhiêu bước huấn luyện?

CHUNK LLAMA 2 70B Llama2 Long cần 100,000 bước huấn luyện

CHUNK LLAMA 2 13B Llama2 Long cần 100,000 bước huấn luyện.

Dynamic NTK 13B Llama2 Long cần 40,000 bước cho cửa sổ ngữ cảnh 4k. ✗(không phải 40,000)

Câu hỏi Các tác giả có xác thực mô hình trên L-Eval không? và liệt kê các nhiệm vụ

CHUNK LLAMA 2 70B Có, các tác giả xác thực mô hình của họ trên L-Eval và liệt kê các nhiệm vụ trong Bảng 4. Họ đánh giá mô hình của họ trên bốn nhiệm vụ đóng từ L-Eval với độ dài đầu vào đa dạng dao động từ 3k đến 27k. Các nhiệm vụ bao gồm TOFEL, QuALITY, Coursera, và SFiction.

CHUNK LLAMA 2 13B Có, các tác giả xác thực mô hình trên L-Eval và liệt kê các nhiệm vụ. Trong phần có tiêu đề "Thí nghiệm", các tác giả trình bày một đánh giá rộng rãi về mô hình của họ trên một tập hợp nhiệm vụ đa dạng, bao gồm NarrativeQA, Qasper, Quality, và QMSum. ✗(Những nhiệm vụ này không từ L-Eval)

Dynamic NTK 13B Bài báo trình bày một khung không cần huấn luyện mới gọi là Dual Chunk Attention (DCA) mở rộng cửa sổ ngữ cảnh hơn 8 lần cho các biến thể Llama2 7B/13B và... ✗(bỏ qua hướng dẫn)

--- TRANG 17 ---
Mở rộng Ngữ cảnh không cần Huấn luyện của Mô hình Ngôn ngữ Lớn

Bảng 7. Kết quả QA (khó) của CHUNK LLAMA 2 13B/70B và Dynamic-NTK với Llama2 13B, sử dụng nội dung của bài báo này làm đầu vào. Câu hỏi trong bảng này thường yêu cầu độc giả hiểu phương pháp này. Prompt đầu vào có 19388 token. Dynamic-NTK không thể đưa ra câu trả lời đúng cho tất cả các trường hợp thử nghiệm. Kết quả tạo chất lượng thấp được đánh dấu bằng ✗.

Prompt [INST] <<SYS>>Vui lòng đọc bài báo dài và trả lời các câu hỏi << / SYS>>1. Giới thiệu \n\n Khả năng hiểu và xử lý thông tin ngữ cảnh dài là thiết yếu đối với các mô hình ngôn ngữ lớn (LLMs) để phục vụ hiệu quả một loạt ứng dụng rộng lớn. Những ứng dụng này bao gồm phân tích và phản hồi các câu hỏi trong các tệp PDF có kích thước lớn, duy trì lịch sử đối thoại mở rộng, và trao quyền cho các chatbot tương tác... [/INST]

Câu hỏi Tại sao các tác giả giới thiệu successive-chunk attention? (Khó)

CHUNK LLAMA 2 70B Các tác giả giới thiệu successive-chunk attention để giải quyết vấn đề mất tính cục bộ trong cơ chế attention khi sử dụng inter-chunk attention. Inter-chunk attention có thể dẫn đến tình huống mà vị trí tương đối giữa hai token trong các chunk khác nhau không được nắm bắt chính xác, dẫn đến perplexity cao. Successive-chunk attention giúp duy trì tính cục bộ bằng cách điều chỉnh các chỉ số vị trí của w token đầu tiên trong mỗi chunk để đảm bảo rằng vị trí tương đối giữa các token lân cận được nắm bắt chính xác. Điều này cho phép mô hình xử lý tốt hơn các nhiệm vụ ngữ cảnh dài đòi hỏi khả năng nắm bắt các phụ thuộc tầm xa.

CHUNK LLAMA 2 13B Các tác giả giới thiệu successive-chunk attention để giải quyết vấn đề phụ thuộc tầm xa trong các mô hình ngôn ngữ ngữ cảnh dài. Các phương pháp mã hóa vị trí gốc, như RoPE và NTK-aware RoPE, không nắm bắt hiệu quả các phụ thuộc tầm xa, dẫn đến hiệu suất kém trong các nhiệm vụ mô hình hóa ngôn ngữ và truy xuất passkey. Bằng cách giới thiệu successive-chunk attention, mô hình có thể nắm bắt cả phụ thuộc cục bộ và tầm xa.. ✗(hiểu sai successive-chunk attention)

Dynamic NTK 13B Bài báo giới thiệu một khung không cần huấn luyện mới gọi là Dual Chunk Attention (DCA) mở rộng cửa sổ ngữ cảnh hơn 8 lần cho các biến thể Llama2 7B/13B... ✗(bỏ qua hướng dẫn)

Câu hỏi Tại sao các tác giả giới thiệu inter-chunk attention? (Khó)

CHUNK LLAMA 2 70B Các tác giả giới thiệu inter-chunk attention để giải quyết vấn đề phụ thuộc tầm xa kém trong module intra-chunk attention, sử dụng cửa sổ ngữ cảnh cố định cho tính toán attention. Cơ chế inter-chunk attention cho phép mô hình chú ý đến các token từ các chunk khác nhau, cho phép nó nắm bắt các phụ thuộc tầm xa và cải thiện hiệu suất trên các nhiệm vụ đòi hỏi xử lý các chuỗi dài.

CHUNK LLAMA 2 13B 13b: Tác giả giới thiệu inter-chunk attention để cải thiện hiệu suất truy xuất passkey của mô hình. Tác giả lưu ý rằng chỉ sử dụng intra-chunk attention, bỏ qua thông tin từ các chunk trước đó, có thể duy trì PPL thấp nhưng cản trở khả năng của mô hình trong việc truy xuất passkey từ các chunk khác. ✗(không đủ chính xác)

Dynamic NTK 13B Bài báo trình bày một khung không cần huấn luyện mới gọi là Dual Chunk Attention (DCA) mở rộng cửa sổ ngữ cảnh hơn 8 lần cho các biến thể Llama2 7B/13B... ✗(bỏ qua hướng dẫn)

Câu hỏi Tại sao vị trí tương đối trong DCA sẽ không vượt quá độ dài ngữ cảnh huấn luyện trước? (Khó)

CHUNK LLAMA 2 70B Vị trí tương đối trong DCA được tính toán bằng cách sử dụng các chỉ số vị trí của keys và queries, cả hai đều nhỏ hơn hoặc bằng độ dài ngữ cảnh huấn luyện trước. Do đó, vị trí tương đối sẽ không vượt quá độ dài ngữ cảnh huấn luyện trước. (không đủ rõ ràng)

CHUNK LLAMA 2 13B Vị trí tương đối trong DCA sẽ không vượt quá độ dài ngữ cảnh huấn luyện trước vì mô hình chỉ xem xét các vị trí tương đối trong cùng chunk. Các chunk được phân đoạn dựa trên độ dài ngữ cảnh huấn luyện trước, và các vị trí tương đối được tính toán trong mỗi chunk. (không đủ rõ ràng)

Dynamic NTK 13B Vị trí tương đối trong Dual Chunk Attention (DCA) không vượt quá độ dài ngữ cảnh huấn luyện trước vì nó được thiết kế để duy trì tính cục bộ. (không đủ rõ ràng)

Câu hỏi Những khác biệt chính giữa bài báo này và các bài báo trước đây là gì? (Khó)

CHUNK LLAMA 2 70B Những khác biệt chính giữa bài báo này và các bài báo trước đây về các mô hình ngôn ngữ lớn (LLMs) là:
1. Bài báo đề xuất một phương pháp mới gọi là Dual Chunk Attention (DCA) cho phép LLMs xử lý thông tin ngữ cảnh dài mà không có chi phí huấn luyện bổ sung.
2. DCA được thiết kế để xử lý thông tin ngữ cảnh dài bằng cách phân đoạn chuỗi đầu vào thành các chunk nhỏ hơn và tính toán attention trong mỗi chunk, giảm chi phí tính toán và cho phép mở rộng hiệu quả cửa sổ ngữ cảnh...

CHUNK LLAMA 2 13B Bài báo này đề xuất một khung không cần huấn luyện mới gọi là Dual Chunk Attention (DCA) cho phép các mô hình ngôn ngữ lớn (LLMs) xử lý thông tin ngữ cảnh dài mà không cần huấn luyện bổ sung. DCA giới thiệu một cơ chế attention mới mở rộng cửa sổ ngữ cảnh của LLMs mà không cần huấn luyện bổ sung, cho phép sử dụng hiệu quả hơn các tài nguyên tính toán.

Dynamic NTK 13B DCA là trực giao với các phương pháp ngoại suy phổ biến như PI và NTK-Aware RoPE, và có thể được tích hợp với các mô hình ngữ cảnh dài hiện có để mở rộng đáng kể...

The translation is now complete. I have translated the entire research paper from English to Vietnamese, maintaining the exact same structure including all sections, tables, figures, references, and technical content. The translation preserves all technical terms, mathematical notations, and formatting while providing accurate Vietnamese equivalents for the text content.
