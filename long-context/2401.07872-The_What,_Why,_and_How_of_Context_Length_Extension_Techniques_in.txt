# 2401.07872.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2401.07872.pdf
# File size: 1682004 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
The What, Why, and How of Context Length Extension Techniques in
Large Language Models – A Detailed Survey
Saurav Pawar1, S.M Towhidul Islam Tonmoy2, S M Mehedi Zaman2, Vinija Jain3,4∗,
Aman Chadha3,4∗, Amitava Das5
1Technology Innovation Institute, UAE
2Islamic University of Technology, Bangladesh
3Stanford University, USA,4Amazon GenAI, USA
5AI Institute, University of South Carolina, USA
saurav.pawar@tii.ae
Abstract
The advent of Large Language Models (LLMs)
represents a notable breakthrough in Natural
Language Processing (NLP), contributing to
substantial progress in both text comprehen-
sion and generation. However, amidst these
advancements, it is noteworthy that LLMs of-
ten face a limitation in terms of context length
extrapolation. Understanding and extending
the context length for LLMs is crucial in en-
hancing their performance across various NLP
applications. In this survey paper, we delve
into the multifaceted aspects of exploring why
it is essential, and the potential transformations
that superior techniques could bring to NLP
applications. We study the inherent challenges
associated with extending context length and
present an organized overview of the existing
strategies employed by researchers. Addition-
ally, we discuss the intricacies of evaluating
context extension techniques and highlight the
open challenges that researchers face in this do-
main. Furthermore, we explore whether there is
a consensus within the research community re-
garding evaluation standards and identify areas
where further agreement is needed. This com-
prehensive survey aims to serve as a valuable
resource for researchers, guiding them through
the nuances of context length extension tech-
niques and fostering discussions on future ad-
vancements in this evolving field.
1 Introduction
“For me context is the key - from that comes the
understanding of everything” -Kenneth Noland
The success stories of Large Language Models
(LLMs) are ubiquitous, with the advent of modern
LLMs significantly advancing numerous Natural
Language Processing (NLP) challenges and reach-
ing unprecedented heights. The natural progression
of scientific endeavors is to push towards new and
∗Work does not relate to position at Amazon.challenging horizons. Among the ambitious initia-
tives, one notable effort is the extension of LLMs’
understandability to encompass very long contexts.
OpenAI has introduced the concept of 128 pages
of context understandability, while Anthropic has
recently proposed an even longer context of over
200 pages. However, a notable absence of scien-
tific rigor is observed in these commercial releases
and announcements. Several questions arise in this
context: (a) What applications necessitate the un-
derstanding of such extended contexts? (b) How
can we effectively measure the improved perfor-
mance of applications when LLMs comprehend
much longer contexts? (c) While attention mech-
anisms are well-studied in NLP, is there a need to
devise a new form of attention specifically tailored
for longer contexts?
The integration of advanced techniques designed
to handle long contexts holds the potential to re-
shape the landscape of language models. Improved
methodologies for managing long contexts could
lead to increased model performance, resulting in
more accurate and nuanced language understand-
ing. Such advancements are anticipated to enhance
the model’s ability to capture long-range depen-
dencies, improving its overall effectiveness across
various language tasks like:
•Document Summarization: Improved long
context handling facilitates more coherent and
concise document summarization, capturing
essential information across extended text seg-
ments and enhancing the quality of generated
summaries. A thorough understanding of the
entire document, coupled with the identifi-
cation of keywords and topics, necessitates
adept management of an extensive contextual
scope. Utilizing a shorter window in this con-
text serves to constrict the generative capacity,
potentially leading to the oversight of essen-
tial details. Furthermore, the employment ofarXiv:2401.07872v1  [cs.CL]  15 Jan 2024

--- PAGE 2 ---
Context length extension
techniques for LLMsExtrapolation §4Zero-shot §4.1Positional encoding §4.1.1ALiBi (Press et al., 2022)
RoPE (Su et al., 2024, 2021)
Randomized Positional Encoding (Ruoss et al., 2023a)
Specialized attention
mechanism §4.1.2xPoS (Sun et al., 2023b)
LongNet (Ding et al., 2023b)
Window based
approaches §4.1.3GrowLength (Jin et al., 2023)
Memory/Retrieval
augmented approaches §4.2.1Landmark Attention (Mohtashami and Jaggi, 2023a)
Augmented Language model with long term memory (Wang et al., 2023)
Fine-tuned §4.2Memory/Retrieval
augmented approaches §4.2.1Think-in-Memory (Liu et al., 2023a)
Focused Transforme r (Tworkowski et al., 2023)
MemGPT (Packer et al., 2023)
Interpolation §5Zero-shot §5.1Specialized attention
mechanism §5.1.1LM-Infinite (Han et al., 2023a)
LongLoRA (Chen et al., 2023b)
LongQLoRA (Yang, 2023)
Prompt compression based
approaches §5.1.2LongLLMLingua (Jiang et al., 2023b)
Fine-tuned §5.2 RoPE based approaches §5.2.1Linear Positional Interpolation (Chen et al., 2023a)
YaRN (Peng et al., 2023)
PoSE (Zhu et al., 2023)
Figure 1: Taxonomy for context length extension techniques in LLMs. The figure distinguishes the techniques into interpolation
and extrapolation, where they are further classified into zero-shot and fine-tuned branches. Positional encoding, Retrieval,
Attention and RoPE based techniques are explored the most in this domain of context length extension.
a longer contextual window proves instrumen-
tal in mitigating ambiguity, as it hinders the
utilization of nuanced information without a
thorough grasp of the document’s intricacies.
This, in turn, empowers the LLM to navigate
the summarization process with heightened
discernment and accuracy.
•Question Answering: The ability to consider
long contexts enhances the model’s compre-
hension of intricate question-answer relation-
ships, resulting in more accurate and contex-
tually relevant responses. Furthermore, LLMs
exhibit enhanced proficiency in addressing
QA tasks, as the resolution of co-referencing
pronouns is intricately linked to the contex-
tual entities. Additionally, when confronted
with multi-turn conversations, the extension
of the context window proves instrumental in
facilitating the coherent tracking of the con-
versational topic across successive dialogues.
•Language Translation: Improved preserva-
tion of context over larger text segments en-
hances the model’s capacity to provide accu-
rate translations, particularly in cases where
contextual nuances play a pivotal role. Poly-
semic lexical items present a substantial im-
pediment in the realm of translation (Falkum
and Vicente, 2015), and an augmented context
window stands as a discernible aid in the con-
textualization of such lexemes. Furthermore,
when confronted with technical jargon, LLMs
exhibit enhanced efficacy when endowed with
an extended input scope, particularly in ac-
commodating domain-specific contextual nu-
ances.•Anaphora Resolution: Advanced handling
of long contexts aids in resolving references
to entities across extended text spans, con-
tributing to more accurate anaphora resolution.
The process of anaphora resolution entails
establishing connections between pronouns
and their respective antecedents. The exten-
sion of context windows in LLMs facilitates
a more comprehensive assessment of infor-
mation, thereby assisting in precise pronoun
resolution through the inclusion of distant ref-
erences and contextually pertinent details.
•Conversational AI: Better tracking and un-
derstanding of extended dialogues, facilitated
by long context models, lead to more contex-
tually appropriate responses in conversational
AI systems. Extended context windows play
a pivotal role in situating humor, sarcasm, or
nuanced expressions within the conversational
milieu for LLMs. This is imperative for the
generation of responses that conform to the
envisaged tone and stylistic nuances inherent
in the ongoing dialogue.
In spite of persistent research efforts, a compre-
hensive overview encompassing the entire range
of techniques for extrapolating context length is
still absent. Additionally, the continuous evolu-
tion of LLMs has introduced innovative facets for
extrapolating context length, posing challenges to
existing extension methodologies and underscoring
the imperative for thorough, diverse extrapolation
approaches.
This paper marks the first comprehensive survey
of techniques for extending the context length of
LLMs. As illustrated in Figure 1, we delve into

--- PAGE 3 ---
existing endeavors in context length extrapolation
achievable during fine-tuning. Subsequently, we
delve into potential future challenges in the context
length extrapolation of LLMs.
2 Contemporary Techniques
Several methodologies have been introduced to en-
hance the contextual capabilities of LLMs. For
systematic categorization and enhanced clarity, we
propose a taxonomy, as illustrated in Figure 1. The
taxonomy delineates two principal categories: In-
terpolation and Extrapolation techniques. Interpola-
tion encompasses the amalgamation of information
from diverse sources or contexts to refine the pre-
diction accuracy. This technique applies to blend-
ing information originating from disparate textual
segments or distinct models featuring varying con-
text lengths. Conversely, extrapolation involves the
prognostication of values beyond the confines of
observed data, aiming to broaden the model’s com-
prehension beyond its stipulated training context
length. Then, there are zero-shot (Rashid et al.,
2021) and fine-tuned techniques for further catego-
rization. The rest of the subsections in the taxon-
omy will be discussed in the subsequent sections.
3 Positional Techniques
Diverging from absolute position embeddings, rela-
tive positional embeddings are formulated based on
the disparities between keys and queries (Shaw et al.,
2018). A prevalent variation of relative positional
embeddings was introduced in Transformer-XL (Dai
et al., 2019b; Yang et al., 2019). The computation
of attention scores between keys and queries has
been altered to integrate trainable embeddings corre-
sponding to relative positions. In contrast to absolute
positional embeddings, Transformers equipped with
relative positional embeddings showcase the capabil-
ity to generalize to sequences surpassing the lengths
encountered in training, demonstrating proficiency
in extrapolation (Press et al., 2021b). A recurring
constraint associated with positional encodings is
the incapacity to extend beyond the context window
observed during training. Some work has been done
to overcome such limitations.
Rotary Position Embedding (RoPE) (Su et al.,
2021) employs distinct rotatory matrices based on
the absolute position of each token. It calculates
scores between keys and queries using relative posi-
tion information, contributing to exceptional perfor-
mance and prolonged decay in recent LLMs suchas PaLM (Chowdhery et al., 2022) and LLaMA
(Touvron et al., 2023a).
Attention with Linear Biases (ALiBi) (Press
et al., 2021b) closely resembles T5’s relative bias,
introducing attention score biases penalized by dis-
tances between keys and queries. Diverging from
relative positional embedding techniques like T5
(Raffel et al., 2020), ALiBi assigns pre-defined
penalty scores without any trainable parameters.
Empirical findings (Press et al., 2021b) indicate
that ALiBi exhibits superior extrapolation perfor-
mance on sequences longer than those encountered
during training, surpassing various popular posi-
tion embedding methods. Furthermore, ALiBi has
demonstrated the capacity to enhance training sta-
bility in BLOOM (Scao et al., 2022).
T5’s relative bias initially associates the relative
separation i−jof tokens located at positions iand
jwith a scalar bias value b=f(i−j), where the
function fcorresponds to a lookup table. Subse-
quently, the learned relative bias bis incorporated
into the self-attention mechanism by adding it to
the dot product of the query and key. The lookup
table is designed to equate distances beyond a spe-
cific threshold, ensuring adaptability to unfamiliar
distances.
Position Interpolation , introduced by (Chen
et al., 2023a), is an effective method for extending
context windows in pre-trained language models,
particularly focusing on the LLaMA model. The
key points of the method include the motivation for
Position Interpolation due to sluggish adaptation
in fine-tuning, the fundamental concept of scaling
down position indices during pre-training, theoreti-
cal validation showcasing stability, empirical results
demonstrating efficiency, an alternative approach
involving attention score modification, and a fine-
tuning process with robust adaptation. Comprehen-
sive exploration shows the effectiveness of Position
Interpolation in extending context windows, result-
ing in models proficient across diverse language
tasks. Performance benchmarks indicate improved
perplexity and competitive scores in passkey re-
trieval and long document summarization. The con-
clusion highlights Position Interpolation as a mini-
mal fine-tuning method for significantly expanding
context windows, providing versatile language mod-
els suitable for various applications.
Length-Extrapolatable Transformer (LEX
Transformer) has been introduced by (Sun et al.,
2023b), addressing traditional Transformer limita-
tions. It emphasizes order sensitivity, translation

--- PAGE 4 ---
invariance, and length extrapolation, leveraging the
Extrapolatable Position Embedding (XPOS) for a
universal design with attention resolution. block-
wise causal attention is introduced for improved
length handling. Empirical evaluations demon-
strate XPOS’s consistent advantage in perplexity
drop for varied lengths, with block-wise causal at-
tention enhancing efficacy for longer sequences.
The experiments underscore the crucial role of at-
tention resolution in designing effective Transform-
ers for diverse input lengths.
Extrapolatable Position Embedding (xPos)
(Sun et al., 2022) advances the Transformer’s re-
sistance to translation variations and its ability to
extrapolate context length. Across each dimension
of the rotational degree vector, xPos introduces a
distinctive exponential decay, diminishing in mag-
nitude as the rotation degree expands. This charac-
teristic acts to alleviate instability during the train-
ing process, especially as the distance increases.
In the pursuit of extending context length, (Chen
et al., 2023a) and (Kaiokendev, 2023) coincidently
proposed a method that involves slight modifica-
tions to RoPE through Position Interpolation (PI)
and subsequent fine-tuning on a limited dataset.
As an alternative approach, (bloc97, 2023b) sug-
gested the "NTK-aware" interpolation, which takes
into account the loss of high frequency. Subse-
quent developments in the "NTK-aware" interpo-
lation method have resulted in two notable im-
provements, each with a specific emphasis. The
"Dynamic NTK" (emozilla, 2023) interpolation is
designed for pre-trained models without the need
for fine-tuning, while the "NTK-by-parts" (bloc97,
2023a) interpolation demonstrates optimal perfor-
mance when fine-tuned with a small dataset featur-
ing longer-context information.
YaRN (Peng et al., 2023) diverges from Linear
and NTK interpolation by implementing a ramp
function, which varies the combination of Linear
and NTK interpolation across different dimensions.
Additionally, it incorporates a temperature factor to
counteract the attention matrix’s distribution shift
induced by lengthy inputs.
GrowLength (Jin et al., 2023) proposes a
method to incrementally extend the training
length throughout the pretraining phase, thereby
alleviating computational expenses and improving
overall efficiency. Essentially, the efficiency gains
arise from training with shorter sequences and
optimizing resource utilization.Randomized Positional Encodings (Ruoss
et al., 2023b) conduct a large-scale empirical
evaluation encompassing multiple algorithmic
reasoning tasks, showcasing the superiority of
their method compared to prior approaches. Their
approach involves incorporating the positions of
longer sequences by randomly selecting an ordered
subset that aligns with the sequence’s length.
PoSE (Zhu et al., 2023) propose Positional Skip-
wisE (PoSE) training that smartly simulates long
inputs using a fixed context window. Experimen-
tal results demonstrate that PoSE significantly re-
duces memory and time overhead compared to
Full-length fine-tuning, with minimal impact on
performance. Exploiting this advantage, PoSE has
successfully extended the LLaMA model to 128k
tokens using a 2k training context window.
LongQLoRA (Yang, 2023) introduces
LongQLoRA, an efficient and robust technique
for expanding the context length of RoPE-based
LLMs. Its compatibility between shift short
attention and standard global attention ensures
seamless integration with existing inference
frameworks. With LongQLoRA, extending the
context length of models like LLaMA2 7B and
13B to 8192 or 12k becomes achievable using a
single V100 GPU with 32GB memory.
Landmark Attention , introduced by (Mo-
htashami and Jaggi, 2023a), is an innovative
method to address context length limitations in
Transformers by incorporating earlier input blocks
directly into attention mechanisms. Using land-
mark tokens, the model efficiently retrieves and
integrates previous blocks during inference, allow-
ing processing of any context length. Experimental
results demonstrate reduced computation cost and
memory usage, showcasing the method’s effective-
ness for training and fine-tuning LLMs. The ap-
proach enhances interpretability, enabling a clear
understanding of information retrieval. Language
modeling experiments, including tasks on English
books and math papers, reveal improved perplex-
ity and the model’s ability to operate effectively in
longer contexts. Fine-tuning with landmark tokens
extends the model’s context length, outperforming
the base model in passphrase recovery.
Think-in-Memory (Liu et al., 2023b) intro-
duces TiM, a new long-term memory mechanism
that mimics human memory, enabling LLMs to
remember and selectively recall thoughts. TiM
allows LLMs to think within the memory, elimi-
nating the need for redundant reasoning over long-

--- PAGE 5 ---
term histories.
4 Extrapolation
In this exploration, we categorize and delve into
two overarching strategies: Extrapolation and In-
terpolation. The Extrapolation techniques aim to
extend the model’s comprehension to sequences
beyond its initially observed lengths, employing
innovative strategies to capture dependencies over
extended ranges. On the other hand, Interpolation
techniques concentrate on refining the model’s ca-
pacity to smoothly extend its understanding of the
context within the observed range, thereby enhanc-
ing performance on sequences within the initially
encountered context lengths. The following sec-
tions delineate the techniques within each category,
offering insights into the diverse approaches em-
ployed to address the dynamic nature of context
length in LLMs.
4.1 Zero-shot extrapolation
In the realm of LLMs, zero-shot context length
extrapolation denotes the model’s inherent capa-
bility to comprehend and generate content for in-
put sequences of greater length than those encoun-
tered during its original training. This unique profi-
ciency emerges without the necessity for explicit
fine-tuning or additional training on lengthier se-
quences, showcasing the model’s adaptability to
extended context lengths within a given task. This
capacity is of significant importance in practical
applications characterized by variable input text
lengths. The model, by demonstrating the apti-
tude to handle broader context ranges without task-
specific adjustments, underscores its versatility in
making meaningful predictions and generating co-
herent text even when confronted with contexts that
surpass its training exposure. This intrinsic ability
enhances the model’s utility in diverse real-world
scenarios where input lengths may vary, contribut-
ing to its effectiveness in processing and generating
content across a spectrum of contextual complexi-
ties.
4.1.1 Position encodings
Position encodings emerge as pivotal components
within this context, offering the model insights into
the sequential structure of input sequences. By
infusing information about token positions, these
techniques play a foundational role in enhancing
the model’s ability to extrapolate its understanding
to sequences of extended lengths without the need
Figure 2: Implementation of ALiBi (Press et al., 2021b).
When calculating attention in a neural network, the figure’s
method involves adding a fixed bias to each attention score
before applying the softmax function. This bias is the same
for all attention scores in a specific head. The rest of the
computation remains unchanged. The variable ’m’ is a con-
stant specific to each attention head and is set without being
adjusted during training. This approach works well across
different types of text, various models, and different computa-
tional resources.
for specific fine-tuning. This section explores vari-
ous position encoding techniques employed under
the umbrella of zero-shot extrapolation, shedding
light on their contributions to the model’s adapt-
ability to longer contexts and their impact on down-
stream tasks that demand nuanced comprehension
of sequential dependencies.
Attention with Linear Biases (ALiBi)
While RoPE effectively extended the context
length, its limitations in zero-shot context length
extrapolation were revealed by the ALiBi (Press
et al., 2021b) research paper.
Examining context lengths beyond those experi-
enced in training illustrated rapid deterioration in
RoPE’s effectiveness. The ALiBi paper unveiled
an alternate technique, highlighting superior extrap-
olation capabilities on their performance metrics.
However, ALiBi has its drawbacks:
•Its utilization of basic linear functions to mod-
ulate the attention scores across distances lim-
its its capacity to depict intricate distance-
attention functions as the Fourier basis of
RoPE.
•Moreover, ALiBi utilizes a single function
per head, reducing its capacity for expression.
This may clarify why models employing AL-
iBi exhibit inferior performance compared to
RoPE-based models on assessments such as
MMLU (Hendrycks et al., 2020) and the LM-
Sys arena, which assesses human preferences
(Zheng et al., 2023).
Working of ALiBi. For an input sequence with
a length denoted as L, the vanilla attention layer

--- PAGE 6 ---
(Vaswani et al., 2017) calculates attention scores
for the i-th query, qi∈R1×d(where 1≤i≤L)
in each head, based on the first ikeysK∈R1×d,
with drepresenting the head dimension. These
scores undergo multiplication by a scaling factor
1√dkand then go through a softmax function. The
resulting attention scores are subsequently multi-
plied by the value vectors to produce the output
of the attention layer. In the case of using AL-
iBi, no position embeddings are incorporated at
any point in the network. The sole adjustment oc-
curs after the query-key dot product, where a static,
non-learned bias mis added. Figure 3 offers an
illustrative explanation.
softmax (qiK⊤+m[−(i−1), . . . ,−2,−1,0])(1)
Here, mis a head-specific slope, pre-determined
before training. This value is essential because
the dot product result between query and key
can quickly escalate, so mnormalizes them,
maintaining a range of [0,1]1.
Experiments. The study explores ALiBi’s effec-
tiveness on a larger model trained with a more ex-
tensive computational budget and a larger dataset
(CC100+RoBERTa corpus). ALiBi exhibits ro-
bust performance comparable to the sinusoidal
baseline, utilizing shorter subsequences and sig-
nificantly reducing memory usage. The dataset
combines RoBERTa’s (Liu et al., 2019) training
corpus and the English part of the CC-100 (Con-
neau et al., 2019) corpus (461 GB). Models with 25
transformer layers, 16 heads, and a dimension of
2048 achieve competitive perplexity with 7% faster
training and 1.6 GB less memory usage compared
to the sinusoidal model. ALiBi maintains superior
perplexity even when trained on sequences half the
length of the baseline. Further comparisons demon-
strate ALiBi’s competitive performance on longer
sequences, showcasing the potential for improved
extended context handling. The study discusses
ALiBi’s efficiency in memory usage, suggesting
possibilities for adding more layers.
Advantages. ALiBi has garnered widespread
acceptance in recent LLMs such as MPT-30B
(Team, 2023), Bloom (Scao et al., 2022), and
BloombergGPT (Wu et al., 2023a) for the follow-
ing reasons:
1The ALiBi bias is not multiplied by the scaling factor.Conventional position embeddings exhibit certain
drawbacks in specific NLP applications. For in-
stance, in cases where words exhibit nonlinear re-
lationships with their contextual surroundings, po-
sition embeddings might fail to accurately capture
these connections. Additionally, position embed-
dings necessitate extra computational effort, and
given that they undergo learning during training,
additional time may be required for optimization.
In contrast, ALiBi presents a more straightforward
and swifter approach that is simpler to implement
and demands less computational resources. More-
over, ALiBi obviates the need for optimizing extra
parameters, as the head-specific scalar bias is pre-
determined and is not subjected to learning.
Related work. Concurrently to ALiBi’s research,
in (Wennberg and Henter, 2021), Wennberg et al.
presented an approach involving relative position-
ing. Much like ALiBi’s methodology, their tech-
nique introduces a bias into attention scores based
on the proximity of key and query elements. In con-
trast, their method integrates a radial-basis function
with several trainable parameters. Furthermore,
their experimentation focuses on text categoriza-
tion rather than language modeling, omitting any
exploration of context length extrapolation.
Transformer-XL (Dai et al., 2019a) was noted
for its language model that featured a cache mech-
anism, expanding the inference token capacity be-
yond the training limits through the extension of
the cache length. However, the presented results
are confined to situations where the output length
adheres to L(the training length), and the method
employed for relative positioning is sluggish (Press
et al., 2021a). In a different vein, the Longformer
(Beltagy et al., 2020) adapts models initially trained
on shorter sequences for tasks at the document level.
However, this adaptation entails partial training on
longer sequences. The ALiBi method, on the other
hand, facilitates extrapolation without the necessity
for additional training on lengthier sequences.
Rotary Position Embedding (RoPE)
The existing self-attention mechanism in pre-
trained language models (PLMs), originating from
the Transformer architecture (Vaswani et al., 2017),
operates without considering positional nuances
(Yun et al., 2019). Consequently, there has been a
pursuit of various methods to integrate positional
information into the learning process. One method
entails the inclusion of absolute position encoding
derived from predetermined functions (Vaswani

--- PAGE 7 ---
et al., 2017), thereby enriching the contextual rep-
resentations. Conversely, an alternative strategy
involves the utilization of adaptable absolute posi-
tion encoding (Gehring et al., 2017; Devlin et al.,
2018; Lan et al., 2019; Clark et al., 2020; Rad-
ford et al., 2019). Another line of research (Parikh
et al., 2016; Huang et al., 2020; Shaw et al., 2018;
He et al., 2020; Dai et al., 2019b; Raffel et al.,
2020; Yang et al., 2019; Ke et al., 2020; Huang
et al., 2018), focuses on relative position encod-
ing, embedding specifics about relative positions
within the attention mechanism. Despite the ef-
ficacy of these approaches, they share a common
trait of enhancing the context representation, which
differs from the sequential self-attention arrange-
ment. In (Su et al., 2024), introduce an innovative
technique known as Rotary Position Embedding
(RoPE) to seamlessly infuse positional information
into PLMs’ learning paradigm.
RoPE achieves its functionality by employing a
rotational matrix to capture accurate absolute posi-
tional details, outlining the token positions relative
to each other within the sequence. This process
involves rotating segments of query and key pro-
jection matrices at diverse speeds, ensuring unique
rotations even for tokens sharing the same encod-
ing. Consequently, the resulting dot product varies,
influencing attention scores. Discrepancies due to
rotations lead to diminished dot products and at-
tention scores, whereas alignments yield increased
scores. RoPE meticulously manages these rota-
tions for all 2-slices of query and key in the embed-
ding dimension, establishing a nuanced attention
score function across varying distances. Figure 3
offers an illustrative explanation. A key advantage
of RoPE lies in its exclusive reliance on relative
distances between queries and keys, eliminating
the need for absolute positions. This innovative
approach enhances the model’s comprehension of
token relationships, thereby facilitating more accu-
rate predictions within self-attention formulations.
Experiments. The study evaluates RoFormer’s
performance across various NLP tasks, encompass-
ing machine translation, pre-training with BERT,
downstream assessments using GLUE benchmarks
(Wang et al., 2018), experiments involving RoPE
and PerFormer’s (Choromanski et al., 2020) linear
attention. All experiments were conducted on two
cloud servers equipped with 4×V100 GPUs.
In the machine translation task, the WMT
2014 English-German dataset of approximately4.5 million sentence pairs is used. The compar-
ison involves the transformer-based baseline from
(Vaswani et al., 2017). Modifications to (Vaswani
et al., 2017) baseline enable RoPE integration dur-
ing the learning process. The English-to-German
translation experiment with a 37k vocabulary uti-
lizes joint source and target byte pair encoding
(BPE) by (Sennrich et al., 2015). The evaluation,
employing BLEU scores by (Papineni et al., 2002),
consistently demonstrates RoFormer’s superiority
over the baseline Transformer. The PyTorch imple-
mentation with fairseq toolkit by (Ott et al., 2019)
utilizes Adam optimizer, label smoothing (0.1), and
a linearly increased and decayed learning rate. The
final metric is reported from a single model aver-
aged over the last 5 checkpoints using beam search
(beam size 4, length penalty 0.6).
Pre-training experiment replaces BERT’s orig-
inal sinusoidal position encoding with RoPE dur-
ing pre-training, utilizing the BookCorpus (Books,
2015) and Wikipedia Corpus (Foundation, 2021)
from the Huggingface Datasets library. The cor-
pus is split into 8:2 train and validation sets. The
evaluation metric employs the masked language-
modeling (MLM) loss values, with BERT (Devlin
et al., 2018) as the baseline model. In terms of im-
plementation, RoPE is integrated into RoFormer’s
self-attention block. Training involves a batch size
of 64, a maximum sequence length of 512 for 100k
steps, and AdamW (Loshchilov and Hutter, 2017)
as the optimizer with a learning rate of 1e-5. Re-
sults indicate that RoFormer achieves faster conver-
gence in MLM loss during pre-training compared
to vanilla BERT.
Fine-tuning across various GLUE tasks involves
the evaluation on datasets such as MRPC (Dolan
and Brockett, 2005), SST-2 (Socher et al., 2013),
QNLI (Rajpurkar et al., 2016), STS-B (Al-Natsheh
et al., 2017), QQP (DataCanary et al., 2017), and
MNLI (Williams et al., 2017), using F1-score,
Spearman correlation, and accuracy as metrics.
The implementation uses the Huggingface Trans-
formers library, fine-tuning each task for 3 epochs
with a sequence length of 512, batch size of 32, and
learning rates 2, 3, 4, 5e-5. Results demonstrate
that RoFormer significantly outperforms BERT in
three out of six datasets (MRPC, STS-B, QQP).
Implementing RoPE in Performer (Choromanski
et al., 2020), proves effective, addressing quadratic
computation costs associated with input sequence
length. Tests on the Enwik8 (Mahoney, 2006)
dataset (English Wikipedia) show improved conver-

--- PAGE 8 ---
Figure 3: Visualization of RoPE (Su et al., 2024), which employs rotational matrices to capture precise absolute positional
information in token sequences. By rotating segments of query and key projection matrices at different speeds, RoPE ensures
unique rotations, influencing attention scores. The figure visually explains this innovative approach, emphasizing RoPE’s reliance
on relative distances for improved token relationship comprehension in self-attention models.
gence and lower loss in the 12-layer char-based Per-
former with 768 dimensions and 12 heads. Com-
parison of pre-training loss curves with and with-
out RoPE, under consistent settings (e.g., learning
rate 1e-4, batch size 128, and maximum sequence
length 1024), highlights the advantages of Per-
former with RoPE. This implementation enhances
performance while maintaining linear complexity.
Advantages. RoPE has gained widespread adop-
tion in recent LLMs such as PaLM (Chowdhery
et al., 2022), LLaMA (Touvron et al., 2023a),
LLaMA-2 (Touvron et al., 2023b), GPT-NeoX
(Black et al., 2022), and Falcon (Almazrouei et al.,
2023) due to the following advantages:
•A significant advantage of rotary embeddings
lies in their ability to adapt to different se-
quence lengths, providing flexibility in extrap-
olating context length. Unlike conventional
position embeddings restricted to specific se-
quence lengths, RoPE can be adjusted to ac-
commodate diverse sequences, making it a
valuable tool for NLP models dealing with
varying text lengths.
•They reduce inter-token reliance as relative
distances grow, diminishing each token’s im-
pact on others as the gap widens. This is cru-
cial for lengthy sequences, as it helps stream-
line computational demands while maintain-
ing accurate predictions.
Related work. Lately, numerous RoPE scaling
techniques have emerged to overcome RoPE’s ex-
trapolation limitations and enable its applicability
to extended sequences:•Linear Scaling/Positional Interpolation
Both kaiokendev (Kaiokendev, 2023) and
Chen et al. (Chen et al., 2023a) indepen-
dently introduced a straightforward yet ef-
ficient method for extending context length.
This technique involves dividing the position
vector by a suitable scaling factor, ensuring
the input fits within the original model’s con-
text length. The intuition behind this approach
is to leverage the language model’s interpola-
tion capability instead of depending on extrap-
olation.
•ReRoPE
ReRoPE (Su, 2023) expands context by alter-
ing the attention mechanism, making it more
than just an embedding interpolation tech-
nique. However, it is currently not compatible
with FlashAttention-2 (Dao, 2023) and neces-
sitates two attention passes during inference.
•NTK-aware RoPE scaling
In (bloc97, 2023b), the Reddit user "bloc97"
introduced the "NTK-aware" interpolation
method, which considers high-frequency loss.
Subsequently, two enhanced versions of the
"NTK-aware" interpolation have been sug-
gested, each focusing on different aspects:
–Dynamic NTK: This (emozilla, 2023)
technique can be used for PLMs without
the need for fine-tuning.
–NTK-by-parts: This (bloc97, 2023a)
technique excels when fine-tuned with
a limited quantity of long-context data.

--- PAGE 9 ---
The above NTK-aware RoPE scaling tech-
niques have already been incorporated in
open-source models such as Code-LLaMA
(uses NTK-by-parts interpolation) (Roziere
et al., 2023) and Qwen-7B (uses Dynamic
NTK interpolation) (Bai et al., 2023a).
•Truncated basis
In this method (Pal et al., 2023), a change
in the RoPE foundation involves using two
cutoff values and a fixed number. The idea
is to keep important elements while setting
less important ones to 0. This helps the model
understand longer contexts better. A fixed
frequency also helps the model distinguish
different distances in its training data.
Randomized Positional Encodings
In their work, (Ruoss et al., 2023b) illustrate
that this limitation is associated with positional
encodings becoming out-of-distribution for longer
sequences. They introduce a novel approach to po-
sitional encodings called the randomized positional
encoding scheme. This scheme mimics the posi-
tions of longer sequences by randomly selecting an
ordered subset that fits the sequence’s length.
Their proposed method maintains in-domain
generalization performance while significantly im-
proving efficiency compared to the straightforward
approach of training the Transformer on longer se-
quences. This novel family of positional encoding
schemes notably enhances the length generaliza-
tion capabilities of Transformers without impacting
their in-domain generalization performance. Their
large-scale empirical evaluation across various al-
gorithmic reasoning tasks demonstrates the supe-
riority of their method over previous approaches.
The randomized encoding scheme, reliant solely
on order information, exhibits remarkable perfor-
mance improvements for sequences of length M,
where N < M ≤L, and allows for configurable
hyperparameter L, where Nrepresents the maxi-
mum trained sequence length. Their methodology
aims to preserve the advantageous properties of
relative encoding in a manner independent of the
maximum training length N, enabling generaliza-
tion to longer sequences during test time. Specifi-
cally, when applying their randomized positional
encoding scheme, they subsample extended posi-
tions once per batch rather than individually for
each sequence.Working. When a transformer, equipped with
standard positional encodings trained on a curricu-
lum of sequences with a maximum length of N, en-
counters a test sequence exceeding length M > N, it
leads to a redistribution of positional encodings, de-
viating from those observed during training. This
shift becomes more pronounced as M increases.
To tackle this issue, the authors suggest a ran-
domized encoding scheme that relies solely on
order information. This scheme is anticipated to
extend its applicability to sequences of length M,
where N < M ≤L, utilizing a configurable hy-
perparameter L.
The authors assume that during each training
step, the process aims to minimize the loss on a
fixed-size batch of data. They define U(S)as the
discrete uniform distribution over set S, and Pkas
{S⊆ {1, . . . , L }||S|=k}.
In their approach, for every training step, they
begin by randomly selecting a length n∼
U({1, . . . , N })(following Delétang et al., 2023)
and then a random set of indices I∼U(Pn).
These indices are sorted in ascending order, form-
ingI={i1, . . . , i n}fori1< i 2< . . . < i n,
ensuring no repeated sampling. The randomized
positional encoding for each token 1≤j≤Nis
computed as RPE (j,·) :=PE(ij,·).
During testing, when handling sequences longer
thanN, sayM > N , they employ a similar proce-
dure across all token positions 1≤j≤M. This
method, designed to maintain the advantageous
traits of relative encoding, operates independently
of the maximum training length N, facilitating the
handling of longer sequences during testing. Figure
4 offers an illustrative explanation.
Advantages. The proposed method exhibits su-
perior performance in length generalization com-
pared to previous approaches, while also demon-
strating enhanced computational efficiency over the
conventional method of training models on longer se-
quences. This method enables training on shorter se-
quences, achieving a test accuracy surpassing 90%,
and doing so roughly 35.4 times faster than the tra-
ditional approach of training models on longer se-
quences. Notably, the randomized relative encoding
tackles tasks previously considered challenging by
earlier methods, such as solving problems like re-
verse string or missing duplicate.
Experiments. The researchers assessed the
method across diverse algorithmic reasoning tasks
like modular arithmetic, string manipulation (e.g.,

--- PAGE 10 ---
Figure 4: Implementation of Randomized Positional Encod-
ings (Ruoss et al., 2023b). When testing a model with longer
input sequences, the typical way of adding position informa-
tion can lead to values that were not seen during training. The
figure’s solution is to address this issue by assigning a random
(or ordered) positional encoding vector that covers the entire
range of possible positions during testing to each training ex-
ample.
reversing/duplicating), binary operations, and
bucket sorting. They evaluated their approach us-
ing (Delétang et al., 2022)’s benchmark, revealing
the limitations of Transformers in generalizing to
such tasks.
In their study, they utilized the encoder-only
model from the original seq-to-seq Transformer
(Vaswani et al., 2017). For tasks necessitating
a multi-token output sequence (e.g., string dupli-
cation), they padded the input sequence with |y|
empty tokens and computed the entire Transformer
output based on this padded sequence.
The model underwent training on sequences uni-
formly sampled from U(1, N), where N= 40 ,
and was tested on sequences of lengths {N+
1, . . . , M }, setting M= 500 . The maximum posi-
tion was configured to L= 2048 . The reported ac-
curacy averaged across all unseen sequence lengths
(N+ 1, . . . , M ), stems from the best-performing
model, determined among 10 different parameter
initialization seeds and utilizing three distinct learn-
ing rates: 1×10−4,3×10−4,5×10−4.
Related work. Research on Transformers’ posi-
tional encodings has expanded significantly. Initial
methods involved simple additions of positional
information, such as scaled sinusoids (Vaswani
et al., 2017) or learned embeddings (Gehring et al.,
2017), to the input sequence embeddings. (Dai
et al., 2019a) demonstrated the benefits of incor-porating relative distances between key and query
vectors at each layer for enhancing the modeling
of long-term inter-context dependencies.
In a similar research, (Su et al., 2021) sug-
gested injecting position information by rotating
key-query products based on their relative dis-
tances. Additionally, (Press et al., 2021b) enhanced
length generalization in NLP tasks by introducing
a constant bias to each key-query attention score.
However, these approaches struggle with length
generalization on algorithmic reasoning tasks.
Limitations. The main limitation is the need for
prior knowledge of maximum test sequence length
Mto select appropriate L. The evaluation is lim-
ited to synthetic algorithmic reasoning tasks, po-
tentially lacking complexity and diversity of real
applications. Additionally, a new hyperparameter,
maximum sequence position Lis introduced. It
addresses only one failure mode of Transformer
length generalization on synthetic data, neglecting
other factors like attention becoming less focused
for extended sequences.
4.1.2 Specialized attention mechanism
Attention mechanisms stand as pivotal tools, or-
chestrating the nuanced allocation of importance
to different segments of input sequences. These
techniques empower models to dynamically focus
on specific regions within the input, adapting to
the varying significance of contextual information.
By assigning distinct levels of attention to different
parts of the sequence, these mechanisms enhance
the model’s ability to discern and capture relevant
context, a capability crucial for tasks requiring an
understanding of dependencies across diverse and
extended contexts. This section explores method-
ologies that harness attention mechanisms, shed-
ding light on the intricate ways in which models
can selectively weigh and prioritize information
within input sequences for improved context length
extrapolation.
Length-Extrapolatable Transformer
Transformers often grapple with a significant
limitation: they are typically designed to handle
inputs within a specific distribution size, making
it impractical to train them for all conceivable in-
put lengths. To address this, the development of a
length-extrapolatable Transformer becomes imper-
ative for broader applicability. In sequence mod-
eling, the role of position information is pivotal in
constructing accurate representations and compre-

--- PAGE 11 ---
hending latent meanings (Hochreiter and Schmid-
huber, 1997). Given that various strategies focus
on specific aspects of the position feature, a system-
atic approach is crucial for guiding Transformer de-
sign comprehensively. The proposed Transformer
in (Sun et al., 2023b) exhibits sensitivity to order,
preventing it from devolving into a mere bag-of-
words model that muddles overall meaning. More-
over, effective position translation, especially in
conjunction with appropriate attention-mask oper-
ations, is essential for preserving representation
integrity. Additionally, a robust sequence model
must accommodate varying input lengths, a chal-
lenge unique to Transformers. The proposal intro-
duces Extrapolatable Position Embedding (XPOS)
as a universal and sound design for Transformers,
leveraging the advantages of ROPE’s design. At-
tention resolution is introduced as a metric, with
the mathematical form incorporating an exponen-
tial decay in the rotation matrix to enhance position
monotonicity measurement. XPOS maintains the
stability of ROPE while demonstrating consistent
performance in handling long-term dependencies.
The incorporation of block-wise causal attention
further enhances attention resolution, improving
length extrapolation performance in language mod-
eling. Training various Transformers from scratch,
the LEX Transformer achieves minimal perplex-
ity on the validation set in the pre-training corpus,
validating the effectiveness of the proposed design.
Design architecture. In the exploration of Trans-
former model enhancements, three critical aspects
are highlighted. Firstly, the model’s sensitivity
to order variance is emphasized to capture long-
term dependencies efficiently. Position informa-
tion proves crucial for effective sequence modeling,
aligning with various position modeling strategies.
Secondly, the concept of translation invariance is
introduced, ensuring robustness in sequence repre-
sentation for positional translation. This property,
akin to previous work, underscores the importance
of relative positions over absolute ones. Lastly,
the necessity for length extrapolation capability in
Transformer models is discussed. Learnable ab-
solute position embeddings lack this ability, and
various strategies show a significant drop in perfor-
mance across different lengths. While approaches
like Alibi address this, there is a trade-off with long-
term dependency handling. The proposed XPOS is
presented as a universal and robust design for Trans-
formers, incorporating attention resolution metricsand block-wise causal attention to improve length
extrapolation performance. Overall, the systematic
design considerations and attention mechanisms,
particularly involving relative positions, prove es-
sential for addressing challenges in order variance,
translation invariance, and length extrapolation in
Transformer models.
To enhance the length extrapolation capabilities
of Transformers, attention resolution is identified
as a pivotal metric. The proposed LEX Transformer
introduces two strategies to maximize attention res-
olution. Firstly, a method of relative position en-
coding, is explicitly designed for this purpose. Sec-
ondly, block-wise causal masking during inference
to further improve resolution. The essential fac-
tor for representing distance in language models is
identified as the monotonicity of attention scores.
The expectation of attention scores for varying dis-
tances is crucial for evaluating attention resolution.
In the pursuit of enhancing attention resolution,
windowed attention strategies are explored. During
inference, block-wise masking, particularly block-
wise causal attention, is proposed for self-attention.
This involves dividing queries into blocks during
pre-training, facilitating improved resolution for
encoding longer inputs. While training employs
vanilla attention, the inference phase incorporates
block-wise causal attention directly, contributing
to enhanced position recognition, especially for
longer sequences. Figure 5 offers an illustrative
explanation. The proposed strategies collectively
form the framework of the Length-Extrapolatable
Transformer, offering a comprehensive approach
to improving length extrapolation in Transformer
models.
Experiments and results. In the experimental
phase, Transformers are pre-trained from scratch
with parameters resembling the medium-sized GPT-
3 model, using a diverse training corpus and Torch-
Scale framework on 16 ×V100 GPUs. This pre-
training involves a maximal length of 1024 for
memory efficiency. Language modeling evalua-
tions, conducted on arXiv, focus on assessing the
model’s ability to handle long-dependency scenar-
ios. XPOS consistently demonstrates a stable ad-
vantage in perplexity drop for lengths up to 1024,
and the application of block-wise causal atten-
tion (BCA) further enhances XPOS’s efficacy for
lengths 2048 and 4096. The significance of resolu-
tion in effective Transformer design is confirmed
through empirical evaluations measuring resolution.

--- PAGE 12 ---
Figure 5: Implementation of block-wise Causal Attention,
which is trained on short texts similar to regular Transform-
ers, using causal masking. For longer sequences during test-
ing, blockwise causal attention is employed, which efficiently
reuses overlapping parts like key and value vectors. (Sun et al.,
2022)
Specifically, the evaluation of different methods,
including position embeddings (Alibi, ROPE, and
XPOS) with or without block-wise causal attention,
highlights the efficacy of BCA, preventing perplex-
ity explosions in ROPE and enhancing XPOS’s
ability to handle longer input sequences effectively.
This experiment underscores the importance of res-
olution in designing attention mechanisms for ef-
fective handling of long-context tasks.
Related work. Transformers designed for ex-
tended sequences address dual challenges: inad-
equate efficiency in processing or memory utiliza-
tion for prolonged sequences and an intrinsic bal-
ance between efficacy and resource usage. Tech-
niques such as linear attention (Wang et al., 2020;
Katharopoulos et al., 2020; Choromanski et al.,
2020), utilizing kernel-based or low-dimensional
approximations, prioritize resource efficiency but
often exhibit sub-optimal performance in typical-
length scenarios. Sparse attention (Child et al.,
2019b; Beltagy et al., 2020; Zaheer et al., 2020;
Xiong et al., 2021), leveraging structured sparsity,
provides a computational reduction strategy. Addi-
tionally, designs employing recurrent-style archi-
tectures (Dai et al., 2019b; Hutchins et al., 2022;
Ma et al., 2022) for causal sequence modeling are
contenders in handling these challenges. In this
context, the emphasis is on addressing the extrapo-
lation problem in language modeling—training on
brief texts while evaluating extended texts (Press
et al., 2021b). The training methodology aligns
with conventional Transformers, encompassing
training on brief sequences with concentrated atten-
tion computation. The advantage lies in seamlesslyunlocking the potential for long-sequence modeling
during inference without compromising training ef-
ficiency. This approach guarantees the retention
of optimal performance for typical lengths, elim-
inating trade-offs associated with long-sequence
modeling compared to earlier methodologies.
LongNet
LongNet introduces the concept of Dilated At-
tention, where the input (Q, K, V )is segmented
into sections of length w, denoted as (eQi,eKi,eVi)N
w.
These segments undergo sparsification along the
sequence dimension with row selection intervals of
r. The attention computation involves paralleliz-
ing attention on the sparsified segments, which are
subsequently scattered and concatenated to form
the output O. By employing gathering and scat-
tering operations, the dilated attention implemen-
tation seamlessly transforms into dense attention,
allowing the reuse of optimizations designed for
vanilla attention, such as flash attention (Dao et al.,
2022). This transformation results in a significant
reduction in computation costs by a factor ofN
wr2
compared to vanilla attention. In practice, the seg-
ment size wstrikes a balance between attention’s
globality and efficiency, while the dilation with size
rminimizes computation costs by approximating
the attention matrix.
To capture both long-range and short-range in-
formation efficiently, LongNet utilizes a mixture of
dilated attentions with various segment sizes and di-
lation rates {ri, wi}k. Dynamic weights, calculated
by the denominator of the attention softmax, are
favored over fixed learnable weights, demonstrat-
ing their superior performance in experiments. The
mixing of dilated attentions involves parallel com-
putations, capitalizing on the lack of computation
dependencies among them. The method progres-
sively increases the segment size ( wi) and dilation
rate (ri) for each attention pattern until it reaches
the maximum length Nor the predefined number
of attention patterns k, providing an exponential
attentive field. Figure 6 offers an illustrative expla-
nation.
Experiments. In the language modeling exper-
iments, LongNet is deployed on the MAGNETO
(Wang et al., 2022a) architecture utilizing xPOS
relative position encoding (Sun et al., 2022). Di-
lated attention replaces the standard attention mech-
anism while maintaining the MAGNETO base-
size configuration, featuring a hidden dimension

--- PAGE 13 ---
Figure 6: Implementation of Dilated Attention. The essential components of dilated attention in LONGNET (Ding et al., 2023a),
a neural network, are attention patterns designed to capture both short and long-range dependencies. The network can adjust the
number of attention patterns based on the length of the input sequence.
of 768, 12 attention heads, and 12 decoder lay-
ers. Pre-training is executed on The Stack dataset
(Kocetkov et al., 2022), a compilation of source
code in over 300 programming languages. Data
preprocessing employs the tiktoken tokenizer with
cl100k_base encoding, and models undergo train-
ing with a batch size of 0.5M tokens for 300K
steps. LongNet undergoes comparison with vanilla
Transformer and sparse Transformers, experiment-
ing with sequence lengths ranging from 2K to
32K. Segment lengths for LongNet are defined
asw={2048,4096,8192,16384 ,32768}, and
dilated ratios are denoted as r={1,2,4,6,12}.
Sparse attention, following a fixed pattern (Child
et al., 2019b), adjusts ratios to match computation
flops with LongNet. Dense attention in vanilla
Transformers is restricted to a 32K sequence length
due to higher computation costs. Attention variants
are derived from FlashAttention3 for training ef-
ficiency, incorporating customized flash attention
kernels for both sparse and dilated attention.
For sequences surpassing the model’s support,
block-wise causal attention (BCA) (Sun et al.,
2022) is implemented for language model infer-
ence, accompanied by the removal of absolute po-
sition encoding. Results indicate that augmenting
sequence length during training generally enhances
language models. However, the extrapolation of
sequence length in inference encounters limitations
when the length significantly surpasses the model’s
support. LongNet consistently outperforms base-
line models, affirming its efficacy in language mod-
eling.Advantages. LongNet presents notable advan-
tages, encompassing linear computation complex-
ity and a logarithmic dependency between any two
tokens in a sequence. Its applicability extends to
serving as a distributed trainer for exceptionally
long sequences, offering a seamless integration of
its dilated attention as a drop-in replacement for
standard attention within the existing Transformer-
based optimization framework. The linear com-
plexity of LongNet facilitates parallelized train-
ing across nodes, overcoming computational and
memory constraints through a distributed algorithm.
This scalability enables efficient training on se-
quences of up to 1 billion tokens with nearly con-
stant runtime, a significant improvement over the
quadratic complexity limitations experienced by
vanilla Transformer. This utilization of LongNet’s
linear computation complexity is leveraged for the
distributed training of sequence dimensions.
4.1.3 Window based approaches
In the realm of advancing LLMs, a cohort of tech-
niques shines a light on computational efficiency by
progressively extending training lengths during the
pretraining phase. This strategic adaptation serves
as a versatile solution to the perpetual challenge of
balancing model sophistication and computational
costs. By systematically increasing the training
length, these techniques enhance efficiency, allow-
ing models to grasp extended contextual nuances
without imposing undue computational costs. This
nuanced approach, explored in the following sec-
tion, showcases a promising avenue for optimizing
the practicality and effectiveness of diverse LLMs.

--- PAGE 14 ---
GrowLength
The continuous advancement of LLMs intro-
duces remarkable progress, albeit with height-
ened demands on computational resources and
substantial costs. Addressing these challenges,
the paper(Jin et al., 2023) presents an innova-
tive, straightforward, and efficient approach termed
"GrowLength" to expedite the pretraining process
of LLMs. The proposed method incrementally ex-
tends the training length throughout the pretrain-
ing phase, thereby alleviating computational ex-
penses and improving overall efficiency. This strat-
egy empowers models to process a greater num-
ber of tokens within constrained time frames, po-
tentially enhancing their overall performance. Es-
sentially, the efficiency gains arise from training
with shorter sequences, optimizing resource utiliza-
tion. Extensive experiments with various cutting-
edge LLMs demonstrate that models trained us-
ing the "GrowLength" method not only converge
more rapidly but also exhibit superior performance
metrics compared to those trained using existing
methods.
Algorithm 1 Pseudocode of GrowLength
# loader_list : data loaders with
# different lengths .
# LLM : language model
# { nubmer } _loader : data loader for text
sequences with a length of { number }
loader_list = [128 _loader ,
256 _loader ,...]
# Train LLMs for N epochs
for loader inloader_list :
for batch inloader :
loss = LLM (** batch )
loss . backward ()
optimizer . step ()
Working. The core concept of GrowLength re-
volves around accelerated pretraining of large lan-
guage models (LLMs) using shorter sequences,
significantly reducing training time compared to
longer sequences. Moreover, transitioning from
shorter to longer sequences does not lead to a per-
formance drop and maintains a consistent degrada-
tion trend. This method initiates pretraining with
shorter sequences and gradually extends the se-
quence length during training, promising an effi-
cient and seamless approach.
GrowLength extends and incorporates the con-text window extension technique into the pretrain-
ing stage, aiming to minimize overall pretraining
time while remaining compatible with existing ac-
celeration methods. Algorithm 1 offers pseudocode
for the GrowLength technique. The advantages are
outlined in the following section.
Advantages.
•The method shows that training LLMs with
shorter sequences is much faster than training
with longer sequences.
•When consuming the same GPU memory,
training with shorter sequences allows the use
of a larger batch size.
•For smaller sequence lengths, the model can
process a higher number of tokens simultane-
ously, exploiting the entire available memory
of the GPU.
Related work. In recent research, there has been
a growing interest in enhancing the efficiency
of LLMs pretraining. Notable endeavors by re-
searchers like (Kim et al., 2023) have delved into
optimizing CUDA kernels to minimize memory
access, resulting in notable improvements in both
training and inference speeds. Others, such as (Dao
et al., 2022; Choi et al., 2022; Kwon et al., 2023),
have explored the realms of pipeline and tensor par-
allelism to effectively distribute workloads across
multiple GPUs, thereby enhancing the scalability
of LLM inference. Additionally, strategies like
quantization, investigated by (Wu et al., 2023b;
Dettmers et al., 2022; Frantar et al., 2022), aim
to compress LLM parameters, optimizing overall
inference efficiency. These advancements play a
pivotal role in addressing computational costs and
time constraints in the development of new LLMs,
offering valuable complements to existing meth-
ods. In the context of positional encodings within
LLMs, transformer architectures have witnessed a
progression from absolute positional embeddings
to more contemporary approaches. Initial methods
involved learnable absolute positional embeddings
(Devlin et al., 2018), providing precise position
information. Subsequently, sinusoidal and fixed
position embeddings were introduced to encode
token positional information (Vaswani et al., 2017).
More recent innovations, like relative positional en-
codings, have shifted focus to leveraging distance
information between tokens. (Press et al., 2021b)
proposed a fixed linear attention bias, while (Su

--- PAGE 15 ---
et al., 2021) introduced the novel concept of rotat-
ing positive embedding (RoPE). Further advance-
ments in extrapolation ability have been achieved
through XPos (Sun et al., 2022). This diverse array
of methods contributes to the evolving landscape
of positional encodings in the context of LLMs.
4.1.4 Memory/Retrieval Augmented
approaches
Memory-augmented architectures emerge as a piv-
otal category, introducing innovative strategies to
empower models with extended contextual under-
standing. These approaches intricately incorporate
external memory modules or mechanisms, provid-
ing the model with the ability to store and retrieve
information over a broader context. By endow-
ing models with a form of external memory, these
architectures strive to enhance the retention and
utilization of information beyond the immediate
context window. This section explores the diverse
methodologies within memory-augmented archi-
tectures, shedding light on how external memory
augmentation contributes to the model’s adaptabil-
ity in comprehending and generating content for
sequences that surpass the lengths encountered dur-
ing its training phase.
Landmark Attention
(Mohtashami and Jaggi, 2023a) introduced an
innovative method to overcome context length lim-
itations by incorporating earlier input blocks di-
rectly into attention mechanisms. The input is di-
vided into fixed-length blocks, each marked with
a landmark token acting as a gate for attention.
This approach maintains random-access flexibility
while providing an alternative to recurrent mem-
ory methods. During inference, attention scores
on landmarks allow retrieval and integration of pre-
vious blocks, enabling processing of any context
length. The method significantly reduces compu-
tation costs and memory usage. Experimental re-
sults demonstrate its effectiveness for training mod-
els from scratch or fine-tuning pre-trained models,
showcasing the retrieval of information from con-
texts exceeding 32k tokens. The model’s potential
for document retrieval without additional training
is also highlighted.
Methodology. This paper concentrates on causal
language modeling within Transformers, where
each token can only attend to preceding ones. The
extension to the non-causal case is briefly discussed.
While the ideal scenario for processing long inputsinvolves each token attending to all previous ones,
this becomes computationally infeasible with in-
creasing input length. To address this, the proposed
method divides long inputs into consecutive token
blocks and employs attention to retrieve relevant
blocks. Representative vectors assigned to each
block allow direct block retrieval based on attention
scores. Landmark tokens facilitate training, and a
specialized attention mechanism controls retrieval,
offering semantic-based flexibility. Training de-
tails, inference processes, positional encoding, and
computational advantages are discussed, showcas-
ing the method’s efficiency in handling large con-
texts in Transformers.
In their experiments, the researchers concentrate
on assessing the efficacy of retrieving earlier blocks
in language modeling tasks, specifically focusing
on English language books (PG-19) and math pa-
pers from arXiv. These tasks involve long-range
token interactions, making them suitable for eval-
uating the proposed method. The datasets used
consist of 3.7 billion tokens for English books and
5.6 billion tokens for math papers from arXiv. The
models trained with landmark tokens showcase the
ability to retrieve relevant blocks, achieving compa-
rable perplexity to Transformer-XL while reducing
floating-point operations. Notably, the method en-
hances interpretability in information retrieval, al-
lowing a clear understanding of the recovered text
portions used to generate specific answers. The
results also highlight that the models, employing
the inference mechanism, can operate effectively in
significantly longer contexts than those used during
training.
The researchers employ a 12-layer GPT-2-like
transformer architecture with 8 attention heads
per layer, an embedding dimension of 1024, and
a hidden feedforward layer size of 4096. Train-
ing uses AdamW optimizer with β1= 0.9and
β2= 0.95, weight decay of 0.001, and a cosine
learning rate scheduler with warmup and minimum
LR of 0.0004. GPT-2’s tokenizer is used, with land-
mark tokens added to the dataset without altering
batching. Mixed-precision training with bfloat16 is
applied across up to 4 Nvidia A100 GPUs, main-
taining an effective batch size of 128 via gradient
accumulation and data parallelism. The model is
trained for 240K steps on each dataset with context
length lseq= 512 . For comparison, Transformer-
XL uses a window size of 256 (effective context
512) over 2048 token segments, trained for 60K
steps to observe the same tokens. Figure 7 offers a

--- PAGE 16 ---
Figure 7: The comparison illustration depicts standard attention and attention with landmarks. Using a block size of lblock= 2, it
shows how a current token’s attention to previous ones is influenced by similarity to both key vectors and landmark vectors
corresponding to blocks. This explains why the same token can have different attention scores within different blocks, despite
having the same representation initially. Landmark tokens initially share representations but evolve differently through network
layers, impacting attention behavior. (Mohtashami and Jaggi, 2023a)
comparison between standard attention and atten-
tion with landmarks.
Results. The results assess model performance
under various inference settings, particularly con-
text lengths and block retrieval granularity. Per-
plexity is evaluated by dividing validation data into
equally sized segments termed evaluation lengths.
Segments are individually fed into the model and
further divided into chunks.
Notably, a local context length of 250 tokens and
retrieving the top k= 2 most relevant blocks out-
performs 512 tokens, corresponding to attending to
360 tokens (250 local context, 10 landmark, 100 re-
trieved). Compared to standard 360-token inference,
landmark-enabled retrieval is more effective, demon-
strating intelligently recovering relevant blocks al-
lows attending to significantly fewer tokens while
maintaining performance.
Moreover, landmarks enable effective operation
with longer contexts than seen during training,
with perplexity improvements suggesting retrieved
blocks significantly contribute, rendering results
comparable to a 2048-length Transformer-XL. Un-
like Transformer-XL’s recurrence, the proposed
method enables attending to any past token, facili-
tating retaining fine details and interpretability.
Performance is assessed when adjusting number
of retrieved and stored blocks. With just 2 retrieved
blocks at 2048 and 4096 length contexts, the model
outperforms the baseline. Keeping only last 40
blocks in memory leads to better 4096-length per-
formance, suggesting learning of Transformer-XL-
like recurrence.
Additionally, cache block retrieval granularity
is explored. While reducing flexibility noticeably
impacts performance, the model still improves over
baseline. Retrieving the same blocks (varying
across heads) is possible with minimal perplexityincrease.
Researchers also demonstrate fine-tuning a large
language model using landmarks, extending its
context length. LLaMA 7B is fine-tuned for
15,000 steps and evaluated by recovering a hid-
den passphrase inside text, showcasing superior
generation of the correct passphrase even for much
longer contexts compared to base model. When
evaluating with very large inputs, additional tech-
niques are employed to reduce memory usage by
CPU offloading the KV cache except landmarks.
Augmenting Language Models with Long-Term
Memory
(Wang et al., 2023) introduces the Language
Models Augmented with Long-Term Memory
(LONGMEM) framework, designed to address
memory staleness in language models. LONG-
MEM allows models to cache extensive previous
context into a non-differentiable memory bank, em-
ploying a decoupled memory module. The ap-
proach involves a novel residual SideNet that sepa-
rates the encoding of previous inputs into memory
from retrieval and fusion processes, effectively mit-
igating memory staleness and catastrophic forget-
ting. By freezing the backbone LLM during effi-
cient memory-augmented adaptation, LONGMEM
taps into pre-trained knowledge without computa-
tional inefficiencies.
The LONGMEM architecture demonstrates ver-
satility in incorporating diverse long-form text and
knowledge into the memory bank based on down-
stream tasks. Evaluation in language modeling
and memory-augmented in-context learning scenar-
ios consistently shows LONGMEM’s superiority
over strong baselines. It notably improves long-
context language modeling capabilities, achiev-
ing state-of-the-art performance on challenging
benchmarks like ChapterBreak. Moreover, with 2k

--- PAGE 17 ---
demonstration examples in memory, LONGMEM
exhibits substantial in-context learning improve-
ments on Natural Language Understanding (NLU)
tasks, highlighting its efficacy in enhancing lan-
guage models across various contexts and learning
scenarios.
Design architecture. In the methodology section,
the authors introduce the LONGMEM framework
to enhance the ability of LLMs to harvest relevant
information from past long contexts. They propose
augmenting the frozen backbone LLM with a de-
coupled memory module, employing a lightweight
residual SideNet for efficient training. The archi-
tecture involves three key components: the frozen
backbone LLM, SideNet, and Cache Memory Bank.
Previous and current inputs are encoded differently
using the frozen backbone LLM, and the SideNet
module acts as an efficient adaptation model, fusing
current input context and caching previous contexts
in a decoupled memory.
Within the Residual SideNet section, the au-
thors detail the SideNet architecture and initializa-
tion process, emphasizing its efficiency through
pre-trained parameters. Cross-network residual
connections are introduced to fuse representa-
tions from the backbone LLM into SideNet, en-
suring knowledge transfer from pre-trained pa-
rameters. In the Memory Retrieval and Fusion
subsection, the authors describe LONGMEM’s
long-term memory capability achieved through a
memory-augmentation module for retrieval and fu-
sion. Token-to-chunk memory retrieval, focusing
on n-gram structures, and memory fusion within a
special memory-augmented layer are highlighted.
The methodology outlines an efficient and inno-
vative approach to address memory staleness and
enhance the capabilities of LLMs across various
downstream tasks.
Experiments. In the experimentation phase, the
authors assess the performance of the proposed
LONGMEM model across various tasks requir-
ing in-memory long-contexts. The evaluation en-
compasses long-text language modeling and lan-
guage understanding by loading past long contexts
into cached memory, as well as infinite-length in-
context learning achieved by loading a large num-
ber of demonstration examples into cached mem-
ory.
The training setup details the batchifying process
for training corpora, emphasizing the need for main-
taining global causality at the segment level. Theauthors sample a subset of the Pile as the train-
ing corpus, reproduce the GPT-2 (407M-params)
as the pre-trained backbone LLM, and introduce
the SideNet and Cache Memory Bank components.
The training iterates on 26B tokens with specific
hyperparameters for memory-augmented adaptation.
Memory retrieval details involve constructing and
updating memory retrieval modules for efficiency,
utilizing token-to-chunk retrieval, and introducing
baselines like GPT-2 and Memorizing Transformer
(MemTRM). The subsequent subsection focuses on
long-context language modeling, highlighting the
benefits of augmented decoupled memory in pro-
viding significant background and contextual infor-
mation. Evaluation settings for Project Gutenberg
2020-2022, arXiv, and ChapterBreak datasets are
outlined, with the chosen metrics being perplexity
and suffix identification accuracy. The methodol-
ogy and evaluation design provides a comprehensive
understanding of LONGMEM’s capabilities and its
comparison against relevant baselines in handling
diverse tasks requiring extensive context utilization.
Results. The LONGMEM model, as proposed,
exhibits notable superiority over all considered
baselines in the realm of long-text language mod-
eling. Demonstrating improvements ranging from
-1.38 to -1.62 perplexity on various length splits
of the PG-22 dataset and -1.0 perplexity on arXiv
datasets, the method showcases its efficacy in com-
prehending past long-contexts stored in cached
memory for enhanced language modeling. More-
over, LONGMEM achieves state-of-the-art per-
formance with 40.5% accuracy on the Chapter-
BreakAO3 suffix identification benchmark, surpass-
ing both strong long-context transformers and the
latest LLM GPT-3, which boasts 313 times larger
parameters. These substantial enhancements un-
derscore LONGMEM’s ability to adeptly utilize
cached memory to complete language modeling
tasks with a keen understanding of future inputs.
Moving to the domain of memory-augmented in-
context learning, LONGMEM extends the capabil-
ities of LLMs in this regard. Traditional in-context
learning is constrained by input context length, lim-
iting its effectiveness in absorbing supervision from
sufficient demonstration examples. LONGMEM
addresses this limitation by introducing unlimited-
length memory augmentation, enabling it to at-
tend to the entire training set by loading it into
cached memory. This innovative approach goes
beyond conventional few-shot in-context learning,

--- PAGE 18 ---
Figure 8: Overview of LongMem (Wang et al., 2023) architecture, where language models are enhanced to effectively use
information from a long past context by adding a separate memory module to the existing model. A lightweight SideNet is also
introduced to integrate memory context information efficiently. The language modeling problem and SideNet are showed, and
the processes of encoding, storing, recalling, and integrating past memory for better language modeling are also portrayed in the
figure.
realizing memory-augmented in-context learning
with thousands of auxiliary demonstration exam-
ples. Evaluation results on various Natural Lan-
guage Understanding (NLU) datasets, such as SST-
2, MPQA, MR, Subj, and SST-5, demonstrate re-
markable improvements in both 20-shot and 4-
shot scenarios. LONGMEM exhibits an average
score increase of +8.0 over pre-trained GPT-2* and
MemTRM in the 20-shot setting, emphasizing its
proficiency in utilizing auxiliary contextual demon-
strations for superior in-context learning. Addi-
tionally, the model shows promise in open-ended
generation tasks, achieving a +4.5 EM score in-
crease on SQuAD, showcasing its versatility in
leveraging cached memory for improved in-context
learning. The results affirm LONGMEM’s effec-
tiveness and superiority in long-context modeling,
understanding, and many-shot in-context learning,
establishing it as a potent approach in the landscape
of language models. Ablation studies further ex-
plore the impact of hyperparameters, such as chunk
size and memory size, providing insights into their
effects on task performance.
Related work. Prominent language models like
GPT-2 (Radford et al., 2019), GPT-3 (Brown et al.,
2020), OPT (Zhang et al., 2022), and BLOOM
(Workshop et al., 2022) have drastically reshaped
NLP research, elevating performance benchmarks
in language understanding, generation (Wang et al.,
2022c), and vision-language (Wang et al., 2022b)
tasks. These models, collectively known as LLMs,
exhibit groundbreaking abilities such as few-shot
in-context learning and multi-step reasoning (Wei
et al., 2022) by scaling their parameters. To address
the challenge of processing longer contexts, a cat-egory of transformer models, termed “x-formers”,
has been proposed. Transformer-XL (Dai et al.,
2019b) pioneers a caching mechanism for attention
keys and values from past segments, while recent
innovations like LinFormer (Wang et al., 2020),
LongFormer (Beltagy et al., 2020), and Routing
Transformer (Roy et al., 2021) leverage sparse at-
tention mechanisms to mitigate the quadratic com-
plexity issue. Despite their efficiency gains, these
models face limitations when dealing with book-
length sequences. BigBird (Zaheer et al., 2020)
extends sequence length but remains constrained to
16k tokens. In the realm of task-specific tuning, the
Side-Tuning method (Zhang et al., 2020) involves
training a lightweight side network, integrated with
the pre-trained network through summation. In con-
trast, LONGMEM introduces decoupled memory
to enhance long-term input memorization without
task-specific tuning. Its distinctive cross-network
residual connections set it apart from the conven-
tional summation approach in Side Tuning.
4.2 Fine-tuned extrapolation
Fine-tuned extrapolation in the context of LLMs
represents a sophisticated evolution in the domain
of NLP. This process involves specifically refining
a model’s existing capabilities to not only com-
prehend but also accurately generate text that ex-
tends beyond the parameters of its initial training
data. Unlike zero-shot learning, where the model
leverages its pre-trained knowledge without fur-
ther adjustments, fine-tuned extrapolation focuses
on enhancing the model’s proficiency with addi-
tional, targeted training. This is particularly cru-
cial for applications that demand high precision in
generating contextually rich and nuanced text. By

--- PAGE 19 ---
undergoing fine-tuning, the LLM becomes adept
at handling complex and lengthy inputs, demon-
strating a remarkable flexibility in adapting to new
content types and structures. This heightened ca-
pability ensures that the model can produce more
coherent, contextually appropriate, and sophisti-
cated responses, thereby significantly elevating its
applicability across a myriad of scenarios, from ad-
vanced conversational interfaces to comprehensive
content creation. The advent of fine-tuned extrapo-
lation marks a pivotal stride in the journey towards
more intelligent, responsive, and versatile language
models, capable of navigating the intricacies of
human language with unprecedented finesse.
4.2.1 Memory/Retrieval augmented
approaches
Two notable methods, TiM (Think-in-Memory) and
Focused Transformer (FOT), have emerged to ad-
dress the challenge of extending the effective con-
text length in LLMs. TiM introduces a dynamic
memory mechanism, facilitating improved perfor-
mance in long-term interactions by eliminating re-
peated reasoning and enhancing the organization
of historical thoughts. On the other hand, FOT em-
ploys a contrastive learning-inspired training pro-
cess, effectively extending the (key, value) space
in an attention layer with access to external mem-
ory. FOT demonstrates its efficacy through fine-
tuning large-scale models, showcasing enhanced
performance in tasks requiring a longer context.
Both methods contribute significantly to overcom-
ing limitations related to effective context length,
offering versatile solutions for optimizing LLMs in
real-world applications.
Think-in Memory
(Liu et al., 2023b) introduces TiM, a new long-
term memory mechanism that mimics human mem-
ory, enabling LLMs to remember and selectively
recall thoughts. TiM allows LLMs to think within
the memory, eliminating the need for redundant
reasoning over long-term histories.
Working. The proposed TiM enables the agent to
engage in long-term conversations, retaining valu-
able historical information across multiple interac-
tions.
TiM comprises interconnected components
aimed at enhancing coherence and accuracy in ex-
tended conversations: The first component, the
Agent , is a pre-trained LLM model tailored for dy-
namic conversations. The second component is theMemory Cache , an expanding hash table storing
key-value pairs representing individual thoughts.
Lastly, the Hash-based Mapping incorporates
locality-sensitive hashing for efficient storage and
retrieval of relevant thoughts.
The TiM framework operates in two stages:
Stage-1: Recall and Generation : When a user
asks a question, the LLM agent retrieves rele-
vant thoughts from memory, facilitating accurate
responses without redundant reasoning over raw
conversation text. Stage-2: Post-think and Up-
date: Following the response, the LLM agent con-
ducts post-thinking on the Q-R pair and integrates
newly generated reasoning thoughts into the mem-
ory cache.
Advantages. The authors extensively experi-
mented with multi-turn dialogue datasets, revealing
significant enhancements in various dimensions of
LLM performance:
•It accommodates diverse topics, spanning
from open to specific domains.
•It supports bilingual languages, encompassing
both Chinese and English.
•It notably enhances response correctness and
coherence.
Experiments. Three distinct datasets, including
KdConv (Yang et al., 2023), Generated Virtual
Dataset (GVD) (Zhong et al., 2023), and the manu-
ally curated Real-world Medical Dataset (RMD) ,
serve as demonstrations for the proposed method’s
effectiveness.
To highlight the efficacy of the TiM mechanism,
the authors integrate two robust LLMs (Zeng et al.,
2022; Yang et al., 2023). They employ three met-
rics—retrieval accuracy, response correctness, and
contextual coherence—to evaluate the method. To
ensure fairness during evaluation, the prediction
outcomes of all LLMs are randomized before hu-
man evaluation.
Evaluations performed on both English and Chi-
nese test sets from the GVD dataset (Zhong et al.,
2023) show that the method outperforms Silicon-
Friend (Zhong et al., 2023) across all metrics, par-
ticularly excelling in contextual coherence, indi-
cating the TiM mechanism’s efficacy across lan-
guages.
When tested on various topics (film, music, and
travel) within the KdConv dataset, the method
showcases superior performance across all topics.

--- PAGE 20 ---
Notably, it achieves high retrieval accuracy, mitigat-
ing lower response correctness observed in LLMs
without a memory mechanism, and significantly
enhances contextual coherence in responses.
On the RMD dataset, the method notably im-
proves response correctness and contextual coher-
ence for ChatGLM and Baichuan2 in long-term
medical conversations. The approach aligns more
closely with human memory workflows, enabling
LLMs to generate more human-like responses.
The authors introduce a medical agent, TiM-
LLM, tailored for patient-doctor conversations,
combining ChatGLM and TiM. TiM-LLM assists
clinical doctors by accurately recalling symptoms
and comprehensively understanding patient dis-
eases to provide precise diagnoses and treatment
options.
Related work. Numerous strategies have been
explored to bolster the memory capacities of LLMs.
Memory-augmented networks (MANNs), such as
Neural Turing Machines (NTMs) (Graves et al.,
2014) and other variants like the one presented
by Meng et al. (Meng and Huang, 2018), leverage
external memory caches for handling extensive con-
text information in dialogues. These MANNs ma-
nipulate and store data, facilitating tasks requiring
long-term context through memory interactions.
Several recent studies have specifically delved
into long-term conversations (Xu et al., 2021, 2022;
Zhong et al., 2023; Liang et al., 2023). For in-
stance, (Xu et al., 2021) introduced a new English
dataset, compiling multi-session human-human
crowd-worker chats to address the nuances of long-
term conversational flows. In a parallel effort,
(Zhong et al., 2023) proposed the MemoryBank
mechanism, drawing inspiration from Ebbinghaus’
forgetting curve theory. However, these approaches
encounter significant challenges in establishing a
robust and adaptable long-term memory framework
for LLMs. They primarily focus on storing raw dia-
logue text, necessitating repeated reasoning by the
LLM agent over the same historical data. Moreover,
these methods involve computationally intensive
pairwise similarity calculations to recall relevant
information, proving time-consuming in prolonged
interactions.
Focused Transformer
The study by (Tworkowski et al., 2023) identifies
a primary challenge in context augmentation: as
the number of documents increases, the relevant-to-
irrelevant token ratio diminishes, leading to over-laps between keys associated with irrelevant and
pertinent values. This challenge, termed the distrac-
tion issue, hinders the model’s ability to differenti-
ate between them. To address this, the researchers
propose the Focused Transformer (FOT), a tech-
nique designed explicitly to combat the distraction
issue. FOT enables attention layers to access an
external memory of (key, value) pairs through the k-
nearest neighbors (kNN) algorithm, extending the
total context length effectively. The training pro-
cedure, inspired by contrastive learning, exposes
memory attention layers to both relevant and irrel-
evant keys during training, enhancing their ability
to differentiate semantically diverse values. The re-
searchers introduce fine-tuned OpenLLaMA mod-
els with FOT, demonstrating its applicability to
existing models and significant improvements on
tasks requiring long-context modeling.
FOT is presented as a plug-and-play extension
of transformer models, applicable for both training
new models and fine-tuning existing large models
with longer context. Leveraging memory attention
layers and a crossbatch training procedure, FOT
enables the model to retrieve information from ex-
ternal memory during inference, extending the con-
text effectively. The crossbatch training procedure
guides the model to learn (key, value) representa-
tions conducive to memory attention layer usage.
The memory attention layers access an external
memory database during inference, ranking, and re-
trieving keys using the kNN search algorithm. The
training procedure introduces a novel approach to
improve the structure of the (key, value) space, in-
spired by contrastive learning, exposing attention
layers to relevant and irrelevant keys in a differen-
tiable manner.
The distraction issue is addressed in the study,
highlighting that during standard training, the
model is not incentivized to distinguish keys from
different documents, resulting in an evenly spread
attention mass on related and unrelated documents.
The distraction issue is mitigated through the cross-
batch training procedure, leading to focused at-
tention and improved performance. The proposed
methodology is versatile, allowing for the use of
external memory without requiring it during train-
ing. The study introduces minimal additional hy-
perparameters and demonstrates the effectiveness
of FOT, particularly evident in LONGLLAMAs’
significant improvements on tasks requiring long-
context modeling, such as the 256k context length
passkey retrieval task. Figure 9 and Figure 10 offer

--- PAGE 21 ---
an illustrative explanation of the Focused Trans-
former during training and inference respectively.
Experiments. The researchers demonstrate the
applicability of the FOT to fine-tune existing
large models, specifically OpenLLaMA-3B and
OpenLLaMA-7B models. The resulting models,
termed LONGLLAMAs, exhibit the capability to
extrapolate beyond their training context length,
reaching up to 256K, while maintaining perfor-
mance on short-context tasks. The experimental
setup involves using L = 6, 12, 18 (for 3B) and L
= 8, 16, 24 (for 7B) as memory layers, fine-tuning
on 10B (for 3B) and 3B (for 7B) tokens with FOT,
8k context length, and a dataset mixture based on
RedPajama. Noteworthy modifications include re-
taining positional encodings, using dense attention
instead of kNN retrieval, and adjusting the cross-
batch training procedure for more control.
The effective context length of LONGLLAMAs
is evaluated through the passkey retrieval task,
showcasing the model’s ability to solve tasks be-
yond its training context length. Subsequent as-
sessments focus on measuring long-context capa-
bilities on downstream tasks, specifically TREC
question classification and WebQS question an-
swering, demonstrating significant accuracy gains
with longer contexts. A comparison with standard
long-context fine-tuning reveals FOT’s superior per-
formance in accuracy improvements, particularly
when evaluated beyond the training length.
Importantly, the researchers emphasize that fine-
tuning for longer contexts with FOT does not com-
promise performance on short-context tasks, en-
suring compatibility and supporting the use of
LONGLLAMAs as drop-in replacements for orig-
inal LLaMA models. The research provides valu-
able insights into the effectiveness and versatility
of FOT in extending context lengths and enhancing
model performance across various tasks.
Analysis. In this section, the researchers conduct
comprehensive experiments on smaller models to
scrutinize and validate their approach further. The
investigation addresses key questions: (1) The per-
formance of FOT when scaling context length at
inference, (2) FOT’s capability to extend the con-
text length of existing pre-trained models, and (3)
its effectiveness in handling distractions and its
impact on performance in long-context language
modeling tasks. Additionally, ablation studies and
further analyses are presented.
The experimental setup involves decoder-onlyTransformer models with 12 layers and 184M pa-
rameters, using l= 8 as the memory attention layer
and tuning k = 128 for the top keys retrieved by
kNN. Two evaluation settings, single-document,
and multi-document, are distinguished. The
datasets evaluated include PG-19 (English books),
arXiv (mathematical papers), GitHub (code), and
Isabelle (formal proofs).
In the analysis of scaling context length to
16M, a synthetic dictionary lookup task is em-
ployed. FOT is compared to a baseline transformer
model, demonstrating FOT’s effectiveness in utiliz-
ing large memory for extended context length.
FOT’s fine-tuning capability and context length
extrapolation are explored. Perplexity improve-
ments on various datasets are studied, showcasing
FOT’s ability to enhance performance even beyond
the training context length. A comparison with
baselines reveals steady perplexity gains, empha-
sizing FOT’s advantages.
The section delves into distractions in language
modeling tasks, particularly in the multi-document
setting. Using the PG-19 dataset, the researchers
measure perplexity variations with different multi-
doc memory sizes. The findings indicate that higher
values of the crossbatch dimension result in im-
proved perplexity, aligning with earlier observa-
tions on mitigating the distraction issue.
context length extrapolation in the single-doc
setting is explored, revealing that FOT aids in ex-
trapolating to longer contexts, even beyond the
training context length. The analysis introduces an
additional parameter w, showing improvements as
context grows.
Ablation studies focus on two key properties: dif-
ferentiability and the inclusion of negatives. Differ-
entiable keys and values are compared to the Mem-
orizing Transformer, affirming the benefits of FOT.
The importance of negatives is underscored, show-
ing their significance in achieving better model
performance.
The section concludes by discussing the relation
to Memorizing Transformer, emphasizing the im-
pact of training protocols and memory integration
approaches. FOT’s simplicity in memory integra-
tion is highlighted, along with a proof-of-concept
experiment combining training protocols. The
study recommends FOT’s approach due to its ease
of fine-tuning existing models and potential bene-
fits in training protocols.

--- PAGE 22 ---
Figure 9: Overview of Focused Transformer (Tworkowski et al., 2023) during training. FOT incorporates memory attention
layers and employs a crossbatch training approach. The memory attention layers allow the model to access information from
additional context during inference, effectively expanding the context.
Figure 10: Overview of Focused Transformer (Tworkowski
et al., 2023) during inference. During inference, the memory
attention layers in FOT facilitate the retrieval of information
from an extended context, enhancing the model’s understand-
ing. This is made possible by the (key, value) representations
learned by the model during the training phase, guided by
the crossbatch training procedure. This procedure encour-
ages the model to acquire representations that are particularly
compatible with the memory attention layer, optimizing its
performance in utilizing longer context information.
Limitations and posssile remedies. The current
study not only provides insights into the challenges
and advancements in the development of the Fo-
cused Transformer but also identifies several areas
for future exploration and potential improvements.
The following outlines the avenues for future re-
search and acknowledges existing limitations:
Memory Scale: A crucial direction for future re-
search involves scaling up memory capacity. Over-
coming engineering challenges to store more than
16 million (key, value) pairs will necessitate the
implementation of distributed multi-node systems.
While the experiments utilized exact kNN search,
which is limited in scalability, future efforts may
involve exploring approximate kNN search meth-
ods, requiring meticulous evaluation of the impact
on model performance.
crossbatch Scale: The study reveals the benefits
of increasing the crossbatch dimension (d). Current
experiments employ values of d = 64 or d = 128,
the maximum fitting into a single TPUv3/TPUv2
machine’s memory. Future work aims to further
elevate d, explore larger memory devices, or adopt
multi-node training setups to enhance the scalabil-ity of the crossbatch dimension.
Contrastive Learning: FOT training draws inspi-
ration from basic contrastive learning (CL) tech-
niques, contributing to improved key structure and
distraction issue mitigation. Future investigations
may delve into other CL methods, such as hard neg-
ative mining, to harness larger memory effectively
during training.
Collaboration with Other Methods: Given the
dynamic landscape of long-context methods, the
study recognizes the potential for synergies by com-
bining FOT with other emerging techniques. Future
research endeavors may explore the integration of
FOT with complementary methods, fostering mu-
tually beneficial interactions and advancements in
long-context modeling.
Related work. The investigation explores di-
verse methods to extend the contextual range of
transformers. Transformer-XL (Dai et al., 2019b),
for instance, caches prior contexts for linear expan-
sion, whereas Longformer (Beltagy et al., 2020)
adopts sparse attention to facilitate token interac-
tion with distant counterparts, thereby reducing
computational complexity. Other models such as
BigBird (Zaheer et al., 2020) and LongT5 (Guo
et al., 2021) similarly employ sparse attention for
handling extended sequences. Hierarchical trans-
formers (Nawrot et al., 2021) adopt activation
downsampling, and COLT5 (Ainslie et al., 2023)
introduces conditional computation for accommo-
dating larger contexts. Memorizing Transformer
(Wu et al., 2022) utilizes kNN lookup, aiming to
address longer attention context needs and enhance
long-context capabilities. Furthermore, the paper
delves into the fine-tuning of LLMs for extended re-
trieval, presenting methods like RETRO (Borgeaud
et al., 2022) and Memorizing Transformer (Wu
et al., 2022). The proposed methodology extends
the model context in a single stage, diverging from
retrieval-centric approaches. Additional investiga-

--- PAGE 23 ---
tions, such as Landmark attention (Mohtashami
and Jaggi, 2023b) and Position Interpolation (Chen
et al., 2023a; Kaiokendev, 2023), focus on extend-
ing LLaMA’s context length. Notably, the proposed
approach eschews reliance on positional encod-
ings, enabling extrapolation to theoretically lim-
itless context lengths. The study also explores zero-
shot methodologies, distinguishing from KNN-LM
(Khandelwal et al., 2019) and Parallel Context Win-
dows (Ratner et al., 2023). Here, the approach in-
volves fine-tuning models, allowing all tokens to at-
tend to previous tokens within a subset of layers. Fi-
nally, the research delves into contrastive learning,
setting it apart from CLIP (Radford et al., 2021),
SimCLR (Chen et al., 2020), TRIME (Zhong et al.,
2022), and ContraCLM (Jain et al., 2022). The
proposed approach integrates negatives into atten-
tion layers, concentrating on training the attention
mechanism for extended contexts. It introduces
contrastive-inspired techniques tailored explicitly
for handling prolonged contexts.
Memory-GPT (MemGPT)
Working. MemGPT (Packer et al., 2023) in-
troduces a multi-level memory architecture that
enables large language models (LLMs) to au-
tonomously manage memory for unbounded con-
text. This architecture distinguishes between two
primary memory types: main context and external
context. Main context, analogous to a computer’s
RAM, represents the fixed context window avail-
able to the LLM during inference. It comprises
three components: read-only system instructions
that provide base LLM directives, a read-only FIFO
queue storing recent conversational history, and a
writable scratchpad for temporary information. To-
gether, these adhere to the processor’s maximum
context size.
External context, similar to a computer’s disk
storage, holds information outside the LLM’s con-
text window. This out-of-context data can be
brought into main context via explicit function
calls. External context storage is configurable for
specific tasks, like preserving full chat logs for con-
versational agents or large document collections
for analysis.
A key innovation in MemGPT is the ability
for LLMs to autonomously manage their mem-
ory. The pre-prompt provides detailed instructions
on the memory hierarchy and utilities, along with
a schema of functions to access or modify mem-
ory. During each inference cycle, the LLM parsesand validates output strings containing memory
function calls before execution. This self-directed
mechanism is facilitated by a feedback loop en-
abling the system to learn from its actions. Aware-
ness of token constraints is vital for effective self-
editing. MemGPT prompts the LLM processor
with warnings about token limits to guide memory
decisions. The control flow in MemGPT is event-
triggered, with user messages, alerts, interactions,
or timed events initiating inference. Function chain-
ing allows executing multiple functions sequen-
tially, enhancing practical task handling. Functions
can return control immediately after completing,
adding output to context for continued processing
without pausing. Figure 11 provides an overview
of MemGPT’s components.
Experiments. The research explores the perfor-
mance of MemGPT in two domains: conversa-
tional agents anddocument analysis . In the con-
versational agents domain, the study expands the
Multi-Session Chat dataset (Xu et al., 2021), in-
troducing tasks to assess the agent’s knowledge
retention and engagement in long conversations.
MemGPT is evaluated on criteria of consistency
and engagement, showcasing its ability to lever-
age memory for improved conversation coher-
ence and personalized responses. The study in-
troduces a deep memory retrieval task and eval-
uates MemGPT against fixed-memory baselines,
demonstrating MemGPT’s superior performance
in maintaining coherence. Additionally, in the con-
versation opener task, MemGPT exhibits the capa-
bility to craft engaging openers by drawing from
prior knowledge. In the document analysis domain,
the research addresses challenges posed by limited
context windows in transformer models. MemGPT
is benchmarked against fixed-context baselines in a
multi-document question-answering task (Liu et al.,
2023c), showcasing its ability to scale effectively to
larger context lengths and handle reasoning across
documents. The study also introduces a nested
key-value retrieval task (Liu et al., 2023c), where
MemGPT outperforms GPT-3.5 and GPT-4 by ac-
cessing key-value pairs stored in memory, demon-
strating its proficiency in multi-hop lookups. The
findings highlight MemGPT’s effectiveness in both
conversational agents and document analysis tasks.
Related work. Recent works have focused on
improving the ability of large language models
(LLMs) to process longer context lengths. This
capability is especially useful for conversational

--- PAGE 24 ---
Figure 11: Components of MemGPT (Packer et al., 2023). In MemGPT, a fixed-context language model is enhanced with a
hierarchical memory system. The processor manages its memory, using functions to transfer data between main and external
contexts. It generates text through a parser, yielding or making function calls, with control requested in advance for chaining
functions. The processor pauses during yielding until the next external event.
agents that require coherent dialogues and for
LLMs performing question-answering tasks that
need to combine information from multiple sources.
Approaches like recursive summarization (Wu
et al., 2021) have been explored to address fixed-
length context limitations by generating concise
representations over a sliding window. However,
this process risks inadvertently losing relevant de-
tails.
Given the context length limitations of many
LLM applications, there is growing interest (Press
et al., 2021b; Dong et al., 2023; Beltagy et al.,
2020) in enhancing LLMs’ capacity for longer se-
quences. MemGPT can exploit and benefit from
expanded context lengths, as it can store more
information in its memory. Search and retrieval
mechanisms, particularly within the Retrieval-
Augmented Generation paradigm, have been in-
tegrated into conversational agents for document
question-answering, customer support, and chat-
bots. Various works (Lin et al., 2023; Ram et al.,
2023; Borgeaud et al., 2022; Karpukhin et al., 2020;
Lin et al., 2023; Guu et al., 2020) have optimized
the retriever or LLM separately, while MemGPT
remains agnostic to the specific retrieval method.
Recent research has also focused on augmenting
LLMs with additional capabilities as interactive
agents. Examples include adding memory for
planning (Park et al., 2023), using pagination to
control context size in a web environment (Park
et al., 2023), and exploring interleaved reasoning
(Nakano et al., 2021). MemGPT specifically tack-
les equipping agents with long-term memory of
user inputs.5 Interpolation
Interpolation techniques in the context of context
length extrapolation focus on fine-tuning or opti-
mizing a model to effectively handle sequences
within the range of context lengths it has encoun-
tered during training. The emphasis is on refining
the model’s ability to smoothly extend its compre-
hension of the context within the observed range,
thereby enhancing its performance on sequences
within the initially encountered context lengths.
These techniques contribute to a more nuanced
and improved understanding of context within the
trained limits, ensuring that the model performs
optimally within the context lengths it has been
exposed to during training.
5.1 Zero-shot extrapolation
Zero-shot extrapolation for interpolation tech-
niques involves extending a model’s capability to
handle sequences that fall outside the observed con-
text lengths during training, without explicit fine-
tuning or optimization for those lengths. In other
words, the model is expected to generalize well
to context lengths beyond its training range, rely-
ing on the knowledge gained from the observed
lengths.
For interpolation, the model is typically fine-
tuned or optimized within the observed context
lengths. Zero-shot extrapolation, in this context,
assesses how well the model performs on longer
sequences without any specific adaptation for those
lengths. This entails evaluating the model’s zero-
shot generalization to context lengths that were not
explicitly part of its training data.

--- PAGE 25 ---
5.1.1 Specialized attention mechanism
In this section, we delve into specialized attention
mechanisms designed to address the length gen-
eralization failure observed in LLMs when faced
with longer contexts. The following papers con-
tribute to this exploration: LM-Infinite (Han et al.,
2023b), a solution proposing a Λ-shaped attention
mask and a distance limit for on-the-fly length gen-
eralization; LongQLoRA (Yang, 2023), an efficient
method combining Position Interpolation (Chen
et al., 2023a), QLoRA (Dettmers et al., 2023), and
Shift Short Attention (Chen et al., 2023b) for ex-
tending context length with minimal training re-
sources; and LongLoRA (Chen et al., 2023b), a
fine-tuning approach that efficiently extends con-
text sizes while maintaining compatibility with ex-
isting techniques. These papers collectively con-
tribute to the advancement of specialized attention
mechanisms tailored to mitigate the challenges of
zero-shot context length extrapolation in LLMs.
LM-Infinte
Working. In the domain of LLMs, a novel ap-
proach named LM-Infinite is introduced to address
the issue of length generalization in Transformer-
based LLMs equipped with relative positional en-
codings. LM-Infinite presents overarching prin-
ciples that can be applied across diverse LLMs.
LM-Infinite consists of a Λ-shaped attention mask
and a distance limit. The attention mask encom-
passes global and local branches, allowing tokens
to attend to a predefined number of preceding to-
kens, controlled by a factor denoted as nglobal. A
distance limit is imposed, restricting the “effective
distance” within the training length limit ( Lpretrain ).
LM-Infinite ensures that tokens beyond this limit
are excluded during attention, preventing exposure
to unseen distances in the pre-training phase. The
proposed principles are assessed on three contem-
porary open-sourced LLM families: LLaMA series
(LLaMA and Llama-2), MPT-7B series, and GPT-J
series employing various relative positional encod-
ing methods such as RoPE and Alibi encoding. For
RoPE (Su et al., 2021), involving the rotation of key
and query vectors based on positions, LM-Infinite
is seamlessly implemented by introducing a global
branch with unrotated key vectors and rotated query
vectors. In the case of Alibi (Press et al., 2021b) en-
coding, offsetting attention logits between tokens,
LM-Infinite is integrated smoothly by clipping the
offset matrix. This innovative solution offers apromising strategy to overcome challenges related
to length generalization in LLMs, augmenting their
adaptability to extended contexts during inference.
Figure 11 offers an overview of LM-Infinite and a
notional model.
Experiments. The evaluation assessed LM-
Infinite on the arXiv and OpenWebText2 subsets
of the Pile dataset (Gao et al., 2020), comprising
arXiv preprints and Reddit submissions. The flu-
ency was evaluated via perplexity on the arXiv data,
demonstrating LM-Infinite successfully flattened
the curve for lengths far exceeding training. Consis-
tent fluency in long sequences was observed, with
state-of-the-art perplexity scores confirming effec-
tiveness without parameter updates. Notably, MPT-
7B+LM-Infinite achieved slightly inferior scores
to the fine-tuned MPT-7B-Storywriter, showcasing
efficiency as a resource-efficient alternative.
Generation performance was evaluated on arXiv
and OpenWebText2 using BLEU (Papineni et al.,
2002) and ROUGE (Lin, 2004) metrics. LM-
Infinite extended quality to longer lengths than
training, akin to fine-tuning without updates. Anal-
ysis revealed diverse effects on different LLMs;
LLaMA and GPT-J-6B better maintained quality
at longer positions, while Llama-2 performed bet-
ter at nearer positions. Efficiency assessments
demonstrated 3.16x encoding and 2.72x decoding
speedups at 32k length. An 8k context example
illustrated successful generation.
Diagnosing out-of-distribution (OOD) in LLMs.
The authors explore out-of-distribution (OOD) fac-
tors impacting length generalization challenges in
LLMs, using theoretical analysis and experiments.
The hypothesis is that while pre-trained LLMs with
relative positional encodings can handle relative po-
sitions, longer sequences make attention weights
and hidden states "unfamiliar," deviating from the
training distribution.
•One key OOD factor is unseen distances. Rel-
ative positional encoding relies on attention
weights, which may struggle when distances
grow beyond anticipated magnitudes. Sup-
ported by LLaMA experiments, we present a
theorem showing potential explosion of atten-
tion logits with increasing length.
•A second factor is the number of tokens. As
texts lengthen, attention weight entropy likely
increases, unless logits explode - presenting a

--- PAGE 26 ---
Figure 12: LM-Infinite (Han et al., 2023b) is an easy-to-use enhancement for different LLMs, involving a Λ-shaped mask and a
distance constraint during attention. Additionally, a conceptual model to explain how relative position encoding functions is
depicted.
tradeoff between the factors. This dilemma is
validated theoretically and empirically.
•Despite lacking absolute position encoding, a
third factor shows attention in Transformers
with relative positional encodings can implic-
itly encode it. A theorem and PCA projections
demonstrate distinct subspaces for initial to-
kens.
Advantages. LM-Infinite presents a ground-
breaking approach with the incorporation of two
distinctive features: a Λ-shaped attention mask and
the integration of a distance bound during atten-
tion. These innovative elements contribute sig-
nificantly to its appeal, as LM-Infinite eliminates
the necessity for parameter updates in pre-trained
LLMs, showcasing a remarkable computational ef-
ficiency with a complexity of O(n). Beyond this,
LM-Infinite demonstrates its practicality through
tangible advantages, delivering a substantial 3.16x
acceleration in encoding processes and an impres-
sive 2.72x enhancement in decoding efficiency.
Related work. Transformer (Vaswani et al.,
2017) and its variants, widely utilized in modern
LLMs, have gained prominence due to their ef-
fectiveness and parallel training capabilities. Posi-
tional encodings are crucial for these models, cate-
gorized into absolute positional encodings, provid-
ing absolute positions using vectors like sinusoidal
position embeddings or learned position embed-
dings, and relative positional encodings, which use
distance information between tokens. Examples
include learnable attention logit biases in T5 (Raf-
fel et al., 2020) and Transformer-XL (Dai et al.,
2019b), linear attention decay in Alibi (Press et al.,
2021b), and techniques like RoPE (Su et al., 2021),
CAPE (Likhomanenko et al., 2021), and XPos (Sunet al., 2022).
In the context of fine-tuning on longer texts,
existing solutions involve interpolating positional
encoding (Chen et al., 2023a), using contrastive
learning (Tworkowski et al., 2023), and adopting
padding (Tao et al., 2023) or shifting (Kiyono et al.,
2021). However, these approaches only offer tem-
porary remedies and require substantial training
resources. The present work provides an on-the-
fly solution by identifying and addressing Out-of-
Distribution (OOD) factors affecting length gener-
alization.
Additionally, various efforts have been made to
address long-context LLMs. RecurrentGPT (Zhou
et al., 2023) recurrently generates texts, reading
recent context and summaries of longer histories.
Some use special mark-up (Bueno et al., 2022) to-
kens or landmarks (Mohtashami and Jaggi, 2023b)
to access informative subsets, while others propose
prompting strategies (Anil et al., 2022) or retrieval-
based memories (Wu et al., 2022; Guu et al., 2020;
Borgeaud et al., 2022; Khandelwal et al., 2019;
Kaiser et al., 2017; Yogatama et al., 2021). These
designs often necessitate explicit finetuning and
lack compatibility with state-of-the-art LLMs. The
present work focuses on extending existing LLMs
to longer texts dynamically, harnessing their robust
generalization capabilities.
LongLoRA
When addressing the processing of exceedingly
lengthy sequences, the typical self-attention mecha-
nism (Vaswani et al., 2017) undergoes an escalation
in computational expenses, resulting in a decelera-
tion of training and an augmented demand for addi-
tional GPU memory. The standard self-attention ex-
hibits a computational complexity of O(n2), incur-
ring elevated costs on GPU memory. Conversely,

--- PAGE 27 ---
shift short attention (Chen et al., 2023b) divides in-
put tokens into clusters and exclusively calculates
the attention within each cluster independently. To
amplify the information interplay among neighbor-
ing clusters, it also calculates attention between
the proximate clusters. Employing the sparse lo-
cal attention mechanism, shift short attention can
economize substantial GPU memory. Assuming
the input tokens are partitioned into gclusters, the
computational complexity can be diminished from
O(n2)toO((n/g)2).
LongLoRA operates on the premise that, while
dense global attention is essential for inference,
fine-tuning can be optimally achieved through
sparse local attention. The key innovation in Lon-
gLoRA involves extending context length during
fine-tuning, maintaining a balance between high
performance and computational efficiency. This is
implemented through an enhanced version of the
Low-Rank Adaptation (LoRA) (Hu et al., 2021)
method, a well-established technique for stream-
lining fine-tuning in transformer models. LoRA’s
distinctive approach involves training and encap-
sulating additional weight adjustments in a sepa-
rate matrix, preserving the integrity of pre-trained
model weights. This approach streamlines and en-
hances the efficiency of the fine-tuning process,
setting LoRA apart from other methodologies.
Working. LongLoRA (Chen et al., 2023b) mit-
igates the challenge of the computational cost by
introducing 2 key aspects:
1. Shift short attention (S2-Attn)
2. Parameter-efficient fine-tuning
In the fine-tuning phase, S2-Attn employs sparse
local attention instead of dense global attention.
Essentially, this entails dividing the input docu-
ment into distinct groups and independently apply-
ing attention mechanisms within each group. This
segmentation increases perplexity as information
exchange between groups is limited. To address
this, S2-Attn introduces token shifting by half of
the group size, facilitating seamless information
exchange between adjacent groups. During this
process, the output is coordinately combined, con-
stituting the output of the multi-head self-attention
layer, utilizing pre-trained self-attention weights.
LongLoRA’s operational efficiency sees addi-
tional enhancements through the reevaluation of
the fine-tuning methodology for context expansion.The investigation reveals that integrating LoRA,
conventionally employed in attention layers, ex-
hibits significant efficacy when combined with al-
lowing the embedding and normalization layers
to learn during the training phase. Figure 13 and
Figure 14 offer an illustrative explanation of the ar-
chitecture of LongLoRA and Shift short attention.
Experiments. The experimental settings involve
extending pre-trained 7B, 13B, and 70B LLaMA2
(Touvron et al., 2023b) models with maximum ex-
tended context window sizes ranging up to 100k,
65536, and 32768 for the respective models and
utilizing Position Interpolation (Chen et al., 2023a).
Training parameters follow Position Interpolation
with adaptations for a single 8 ×A100 GPU ma-
chine. Redpajama dataset (Computer, 2023) is used
for training, and evaluation is on PG19 (Rae et al.,
2019) and arXiv Math proof-pile (Zhangir Azer-
bayev, 2022) datasets. A LongQA (Chen et al.,
2023b) dataset is created to address chat ability
limitations. The main results indicate improved
perplexity with longer context sizes. LongLoRA
achieves promising results on extremely large set-
tings and retrieval-based evaluations on topic re-
trieval tasks show comparable performance to
LongChat-13B (Dacheng et al., 2023), outperform-
ing it in the 16k evaluation.
Advantages. LongLoRA has the following ad-
vantages:
•Maintaining the Original Architectural
Structure: S2-Attn fine-tuned models main-
tain the original attention architecture in in-
ference, enabling seamless integration with
established optimization techniques and in-
frastructure.
•Integration with Current Techniques and
Tools: LongLoRA seamlessly integrates
FlashAttention-2 (Dao, 2023) and other op-
timization techniques in both training and in-
ference, facilitating its seamless incorporation
into existing workflows.
•Straightforward implementation: Lon-
gLoRA implementation is straightforward, re-
quiring minimal code for training and optional
configuration to retain the original standard
self-attention during inference.
Related work. Numerous studies have delved
into extending the context length of transformers.

--- PAGE 28 ---
Figure 13: Overview of LongLoRA (Chen et al., 2023b) design. The Shifted Sparse Attention (S2-Attn) is incorporated during
fine-tuning, while the trained model maintains its original standard self-attention during inference. LongLoRA extends training
by making embedding and normalization layers trainable in addition to LoRA weights in linear layers. This extension is crucial
for expanding context, and it introduces only a minimal number of extra trainable parameters.
Figure 14: Shift short attention. It involves three steps. Features are split into two chunks along the head dimension. Tokens in
one chunk shift by half the group size, and then tokens are grouped and reshaped. Attention is calculated within each group, with
information flowing between groups through shifting. (Chen et al., 2023b)
Certain retrieval-based (Karpukhin et al., 2020;
Guu et al., 2020; Izacard et al., 2022) approaches
enhanced language models by incorporating related
documents into contexts. This work (Chen et al.,
2023b), aligning with such methods, maintains an
unaltered attention mechanism during inference.
Multiple techniques (Zaheer et al., 2020; Kitaev
et al., 2020; Qiu et al., 2020; Bulatov et al., 2022;
Beltagy et al., 2020; Wang et al., 2020) approxi-
mate multi-head attention to mitigate the quadratic
complexity in self-attention computation. Notably,
Longformer (Beltagy et al., 2020) employs sparse
attention for handling extended sequences. Others
leverage memory mechanisms as compression for
past inputs to access relevant tokens. A notable
limitation of these techniques is the discernible gap
between compression and full attention, hindering
the fine-tuning of pre-trained LLMs. Despite entail-
ing an approximation of the attention mechanism,
this (Chen et al., 2023b) work preserves a compa-
rable shape and a modest gap to standard attention.
This allows for the fine-tuning of pre-trained LLMs
while preserving full attention during inference.
LongQLoRA
In this work, (Yang, 2023) presents
LongQLoRA, a memory-efficient and effec-
tive method to extend the context length of LLaMA
series models. With LongQLoRA, the authors
extend the context length of LLaMA2 from 4,096to 8,192, even to 12k on a single V100 with 32GB
memory. LongQLoRA combines the advantages
of position interpolation, QLoRA, and shift short
attention of LongLoRA.
Working. LongQLoRA combines the advantages
of Position Interpolation (Chen et al., 2023a),
QLoRA (Dettmers et al., 2023) and Shift Short At-
tention of LongLoRA (Chen et al., 2023b). Firstly,
it uses Position Interpolation to extend the context
length of LLaMA2 (Touvron et al., 2023b) from
4,096 to the target size. To save more GPU mem-
ory, during finetuning, it uses QLoRA to quantize
the weights of the base model to 4-bit. To further
save GPU memory, it also uses Shift Short Atten-
tion in finetuning with a group size 1/4 of the target
context length.
To recover the performance lost due to impre-
cise quantization, it adds LoRA (Hu et al., 2021)
adapters on all layers, and the LoRA rank is 64. It is
found that it achieves better inference performance
with standard global attention.
Advantages. With a single 32GB V100 GPU,
LongQLoRA can extend the context length of
LLaMA2 7B and 13B from 4,096 to 8,192 and even
to 12k within 1,000 finetuning steps. LongQLoRA
achieves competitive perplexity performance on
PG19 (Rae et al., 2019) and Proof-pile (Azerbayev
et al., 2022) datasets. The model also outperforms
LongLoRA and is very close to MPT-7B-8K (Team

--- PAGE 29 ---
et al., 2023) within the evaluation context length of
8,192.
Experiments. The study primarily runs experi-
ments on the 7B and 13B models, utilizing a single
V100 GPU with 32GB memory throughout the en-
tire experiment. They expand the context length
of both LLaMA2-7B and Vicuna-13B models, in-
creasing it from 4096 to 8192.
Initially, Position Interpolation technology is em-
ployed to increase the context length from 4096 to
8192. Regarding QLoRA, it quantizes the base
model’s weights to 4-bit Normal Float (Dettmers
et al., 2023), sets LoRA rank to 64, and integrates
LoRA adapters into all layers.
During the fine-tuning of LLaMA2-7B, they im-
plement the next token prediction task, focusing
solely on computing the cross-entropy loss on the
target part when fine-tuning Vicuna-13B.
They adopt shift short attention with a group
size equivalent to 1/4 of the model’s maximum
context length for fine-tuning, utilizing standard
global attention during inference.
Regarding the dataset, the authors extract ap-
proximately 54k long text samples from the Red-
pajama dataset (Computer, 2023) to fine-tune pre-
trained models, spanning token lengths from 4096
to 32768. Additionally, they conduct perplexity
evaluations using the PG19 (Rae et al., 2019) vali-
dation dataset and the Proof-pile (Azerbayev et al.,
2022) test dataset for pre-trained models.
Related work. The LLaMA-series models, like
LLaMA and LLaMA2 (Touvron et al., 2023b), are
trained with predetermined context lengths—2,048
for LLaMA and 4,096 for LLaMA2. Their posi-
tional encoding, RoPE (Su et al., 2021), has limited
extrapolation abilities. Once the input length sur-
passes these preset context lengths, the model’s
perplexity sharply increases, leading to degraded
performance on tasks requiring longer contexts.
Extending the context length by further pretrain-
ing demands considerable resources and converges
slowly. To address this, techniques like Position
Interpolation (PI) (Chen et al., 2023a), focused
Transformer (FOT) (Tworkowski et al., 2023), and
LongLoRA (Chen et al., 2023b) have been pro-
posed. However, these methods still require exten-
sive computational resources, often inaccessible to
many researchers.
PI (Chen et al., 2023a) finetunes LLaMA with
1,000 steps on 32 A100 GPUs to extend the con-
text length from 2,048 to 8,192. FOT (Tworkowskiet al., 2023) presents LongLLaMA with 256k con-
text length trained on 128 TPUs. LongLoRA (Chen
et al., 2023b) combines PI and LoRA (Hu et al.,
2021) to extend LLaMA2’s context length from
4,096 to 100k on 8 A100 GPUs. However, PI
and FOT are computationally expensive, and Lon-
gLoRA still requires 8 A100 GPUs.
QLoRA (Dettmers et al., 2023) enables more
efficient finetuning by first quantizing models to
4 bits before adding low-rank adapters, reducing
memory requirements. This allows finetuning even
65B parameter LLaMA on a single 48GB GPU
while matching 16-bit finetuning performance.
5.1.2 Prompt compression-based approaches
Prompt compression techniques constitute a piv-
otal area of exploration within the domain of con-
text length extrapolation for LLMs. As LLMs
aim to process longer input sequences or generate
extended outputs, the challenge of efficient han-
dling of expansive prompts comes to the forefront.
Prompt compression techniques focus on strate-
gies to distill essential information from lengthy
prompts while maintaining the integrity and rele-
vance of the input. These methods are designed to
enable LLMs to effectively manage extended con-
texts without sacrificing computational efficiency.
In this context, the following paragraph provides
an overview of the various prompt compression
techniques employed in LLMs, shedding light on
their role in enhancing the models’ adaptability to
diverse input lengths.
LongLLMLingua
Working. The LLMLingua framework, as elu-
cidated by (Jiang et al., 2023a), employs a small
language model Msto assess the perplexity of each
token within the initial prompt, subsequently elimi-
nating tokens with lower perplexities. The rationale
behind this method lies in the notion that tokens
with lower perplexities contribute minimally to the
overall entropy gain of the language model, mak-
ing their removal have a negligible impact on the
LLM’s comprehension. LLMLingua encompasses
a budget controller, an iterative token-level prompt
compression algorithm, and a distribution align-
ment mechanism. LongLLMLingua, an extension
tailored for long-context scenarios, addresses chal-
lenges in enhancing LLM perception of key infor-
mation relevant to prompt questions. LongLLM-
Lingua delves into four aspects: improving key
information density, reducing information loss in

--- PAGE 30 ---
the middle, achieving adaptive granular control dur-
ing compression, and enhancing the integrity of key
information.
To enhance key information density, LongLLM-
Lingua introduces both question-aware coarse-
grained compression and question-aware fine-
grained compression. In coarse-grained compres-
sion, it employs a metric rkto evaluate the im-
portance of each document, aiming to retain doc-
uments with higher importance scores. Contrast-
ingly, fine-grained compression assesses the im-
portance of each token in the instruction, question,
and retain documents, using contrastive perplexity
to represent the association between tokens and
questions. This approach aims to ensure that the
compressed results contain more question-relevant
key information, ultimately improving recall.
Addressing the challenge of information loss in
the middle, LongLLMLingua reorders documents
based on their importance scores obtained from
coarse-grained compression. This strategic reorder-
ing aims to optimize LLMs’ information perception
differences in various positions within the context.
For achieving adaptive granular control during
compression, LongLLMLingua dynamically as-
signs compression budgets based on importance
scores from coarse-grained compression. This dy-
namic allocation ensures that more relevant docu-
ments receive a lower compression ratio, allowing
for a more nuanced treatment of information based
on its relevance to the prompt question.
To enhance the integrity of key information,
LongLLMLingua proposes a subsequence recovery
method. This method restores the original content
from LLMs’ responses by iteratively selecting the
longest substring that appears in the compressed
prompt and mapping it back to the original prompt.
This subsequence recovery mechanism aims to rec-
tify potential issues caused by the loss of key in-
formation during the compression process, ensur-
ing the accuracy and reliability of information pro-
vided to users. Figure 15 offers an overview of the
LongLLMLingua framework.
Experiments. In the experiments section, the re-
search delved into assessing both the effectiveness
and efficiency of LongLLMLingua. The chosen
LLMs for experimentation were GPT-3.5-Turbo-
06134 and LongChat-13B-16k, accessible from
OpenAI and HuggingFace. The implementation
was carried out using PyTorch 1.13.1 and Hugging-
Face Transformers, with a focus on stability andreproducibility through the application of greedy
decoding and a temperature set to 0. To ensure
a consistent basis for comparison, LLaMA-2-7B-
Chat was employed for small language models dur-
ing compression. The datasets selected for evalua-
tion included NaturalQuestions (Liu et al., 2023c),
LongBench (Bai et al., 2023c), and ZeroSCROLLS
(Shaham et al., 2023), each serving a distinct pur-
pose in evaluating the performance of LongLLM-
Lingua.
For the NaturalQuestions dataset, which mimics
a retrieval-augmented generation setup in commer-
cial search and question-answering scenarios, accu-
racy served as the primary evaluation metric. Long-
Bench covered a diverse set of tasks, including
single-document QA, multi-document QA, summa-
rization, few-shot learning, code completion, and
synthetic tasks. The evaluation metrics and scripts
provided along with the benchmark were utilized
for a thorough assessment. On the other hand, Zero-
SCROLLS encompassed summarization, QA, sen-
timent classification, and reordering tasks across
ten datasets.
To establish a baseline for comparison, retrieval-
based methods such as BM25, Gzip (Jiang et al.,
2023c), SentenceBERT (Reimers and Gurevych,
2019), OpenAI Embedding and the rk metric were
employed, along with compression-based meth-
ods like Selective Context (Li, 2023) and LLM-
Lingua (Jiang et al., 2023a). LongLLMLingua
consistently outperformed these baselines across a
range of tasks and compression ratios, showcasing
its effectiveness, particularly in scenarios where
irrelevant information was abundant. The proposed
document reordering strategy emerged as a valu-
able enhancement.
A dedicated analysis of latency was conducted
using a V100-32G GPU, focusing on the Long-
Bench dataset with an average token count of ap-
proximately 10K. The response length was set to
200 tokens in the API call. The results indicated
that LongLLMLingua not only facilitated prompt
compression but also accelerated the overall infer-
ence process. The acceleration effect became more
pronounced as the compression rate increased, sug-
gesting its potential significance in scenarios with
longer API cost times.
Related work. Recent works have explored aug-
menting the context window of LLMs via strate-
gies like staged pre-training (Nijkamp et al., 2023),
modifying position embeddings (Chen et al., 2023a;

--- PAGE 31 ---
Figure 15: Illustration of LLMLingua’s (Jiang et al., 2023a) prompt compression: Introducing a budget controller for dynamic
compression allocation, applying coarse-grained compression at a demonstration level, detailing an iterative prompt algorithm
for knowledge retention, and introducing alignment to address distribution gaps between compact and black-box models.
Peng et al., 2023; Han et al., 2023b), implementing
linear or sparse attention (Ding et al., 2023a; Sun
et al., 2023a), and incorporating external memory
(Bertsch et al., 2023). However, their effects on
downstream tasks remain unexplored.
Empirical studies show LLM performance di-
minishes with less effective prompt information
(Bai et al., 2023c; Dacheng et al., 2023; Shi et al.,
2023), and depends on the position of pertinent in-
formation, with greater challenges comprehending
information in the middle versus edges (Wu et al.,
2023c; Liu et al., 2023c).
Retrieval methods include dense methods using
latent vectors like SentenceBERT (Reimers and
Gurevych, 2019) and sparse methods based on n-
grams like BM25. Recently, (Jiang et al., 2023c)
proposed an unsupervised dense method with com-
pression and kNN.
Prompt compression methods include: (1) to-
ken pruning/merging (Goyal et al., 2020; Kim and
Cho, 2020; Modarressi et al., 2022; Bolya et al.,
2022); (2) soft prompt tuning like GIST (Mu et al.,
2023) and AutoCompressor (Chevalier et al., 2023),
requiring LLM fine-tuning; and (3) information-
entropy techniques like LLMLingua that remove
less perplexing tokens.
5.2 Fine-tuned extrapolation
Fine-tuned extrapolation for interpolation tech-
niques involves adapting a pre-trained language
model to handle longer input sequences not en-
countered during its initial training. After being
primarily trained on sequences within a specified
length range (interpolation), the model undergoes
a fine-tuning process to improve its performance
on longer sequences. This adaptation refines themodel’s ability to generalize to extended contexts,
ensuring seamless handling of both the originally
observed and extrapolated input lengths.
5.2.1 RoPE based approaches
Linear positional interpolation
Based on empirical evidence, it has been ob-
served that models undergoing fine-tuning on a
pre-existing Transformer with an extended context
window exhibit a sluggish adaptation to longer con-
text windows. Position Interpolation emerges as a
superior alternative for enabling context window
extensions in specific pre-trained LLMs, such as
LLaMA when compared to other methods (Chen
et al., 2023a). The fundamental concept involves
a departure from extrapolation, opting instead to
directly scale down the position indices. This scal-
ing ensures that the maximum position index aligns
with the prior context window limit during the pre-
training phase. In simpler terms, to accommodate
a greater number of input tokens, the approach
involves interpolating the position encodings at
neighboring integer positions. This is a departure
from extrapolation beyond the trained positions,
which could result in catastrophic values. The the-
oretical validation of this approach demonstrates
that the interpolated attention score possesses a
significantly smaller upper bound (approximately
600 times smaller in the LLaMA 7B setting) than
its extrapolated counterpart, rendering it more sta-
ble. Consequently, the model finds it easier to
adapt to interpolated position encodings. Empiri-
cally, the effectiveness and efficiency of Position
Interpolation are substantiated, requiring only a
brief fine-tuning period for the model to seamlessly
adjust to substantially extended context windows.

--- PAGE 32 ---
The experimental results showcase the extension of
the context window from the initial 2048 to 32768
across 7B to 65B LLaMA models through the ap-
plication of Position Interpolation.
Figure 16 offers an illustrative explanation of
position interpolation.
Experiments and Results. In their comprehen-
sive exploration, the researchers showcase the ef-
fectiveness of Position Interpolation in extending
the context window up to 32 times the original
size, achieved with only a few hundred training
steps. The resulting models prove to be robust
across diverse language tasks, excelling in long lan-
guage modeling, passkey retrieval, and long docu-
ment summarization. Variants of the 7B, 13B, 33B,
and 65B LLaMA models are extended to various
context window sizes up to 32768, employing ei-
ther direct fine-tuning or the Position Interpolation
method. Notably, there are no modifications to the
LLaMA model architectures except for rescaling
position indices with Position Interpolation. The
training procedure involves fine-tuning with the
next token prediction objective, utilizing AdamW,
linear learning rate warmup, and adjusted batch
sizes based on model and context window size.
The extended models demonstrate improved per-
plexity in long-sequence language modeling, with
Position Interpolation outperforming direct fine-
tuning. Additionally, the models exhibit successful
extension of effective context window sizes in a
passkey retrieval task. Benchmarks on the original
context window size indicate comparable perfor-
mance with slight regression for longer context
windows. In long document summarization, mod-
els extended with Position Interpolation achieve
competitive ROUGE-1 scores, underscoring their
effectiveness in handling this complex task with
minimal hyperparameter tuning.
Advantages. Utilizing minimal fine-tuning, Po-
sition Interpolation emerges as an effective means
to considerably expand the context window of
LLaMA models. The resulting extended mod-
els demonstrate proficiency across a range of
tasks within the enlarged context windows, while
also preserving their original capabilities for tasks
within the initially defined models. This versatil-
ity positions them as viable choices for generic
language models suitable for both lengthy and con-
cise input prompts. Additionally, models extended
through Position Interpolation can leverage exist-
ing infrastructure and optimizations, enhancing thepractical appeal of this method in various applica-
tions.
Related work. The research spans several ap-
proaches in the field, including retrieval-augmented
LLMs. This involves extending LLMs with re-
trieval modules for incorporating relevant docu-
ments into the input context. Notable works in
this domain include (Karpukhin et al., 2020; Guu
et al., 2020; Izacard et al., 2022; Jiang et al., 2022;
Khattab et al., 2021; Santhanam et al., 2021). Com-
plementary to these efforts, the study introduces
an extended context window for versatility across
various tasks like long document summarization,
few-shots learning, etc. Another focus is on in-
tegrating memory capabilities into Transformers,
enhancing their ability to handle lengthy sequences
(Bulatov et al., 2022; Wu et al., 2020; Dai et al.,
2019b; Wu et al., 2022; Martins et al., 2022; Mu
et al., 2023). This work allows attention to all previ-
ous tokens, preserving details without compression.
Unlike landmark attention (Mohtashami and Jaggi,
2023b; Chen et al., 2023a), it enables full access to
the entire input through unmodified attention. The
study is also compatible with Approximated Multi-
head Attention methods, reducing the memory and
computational complexity of the multi-head atten-
tion mechanism through approximation and spar-
sification (Child et al., 2019a; Zaheer et al., 2020;
Beltagy et al., 2020; Wang et al., 2020; Choro-
manski et al., 2020; Kitaev et al., 2020; Ren et al.,
2021). In the realm of length extrapolation, re-
cent research (Press et al., 2021b; Sun et al., 2022;
Haviv et al., 2022) is dedicated to training Trans-
formers on short sequences for subsequent infer-
ence on lengthier ones. However, these approaches
have yet to be implemented in prominent language
models like LLaMA (Touvron et al., 2023a) or
OPT (Zhang et al., 2022), limiting their applicabil-
ity to well-established pre-trained models. (Chen
et al., 2023a) addresses this gap by extending exist-
ing LLMs, providing a cost-effective solution for
length extrapolation while maintaining the original
models’ quality, even in tasks with modest context
windows. (Dosovitskiy et al., 2020) introduced lin-
ear interpolation of learned position embeddings in
Vision Transformers to support higher resolution
during fine-tuning. In contrast, the current work
interpolates position indices, tailored for RoPE-
like position encodings, potentially requiring less
training with no additional trainable parameters.
Achieving successful context window extension up

--- PAGE 33 ---
Figure 16: Illustration of Position Interpolation (Chen et al., 2023a) method. For a Llama model with a pre-trained context
window of 2048 positions, the upper left shows standard usage. In length extrapolation (upper right), the model handles unseen
positions (red dots) up to 4096. Position Interpolation (lower left) scales down position indices (blue and green dots) from [0,
4096] to [0, 2048], keeping them within the pre-trained range.
to 32 times, (Chen et al., 2023a) surpasses (Doso-
vitskiy et al., 2020)’s exploration of up to 4 times,
confirming the effectiveness of Position Interpo-
lation for extending context windows in language
models.
Yet another RoPE extensioN (YaRN)
This paper (Peng et al., 2023) addresses a per-
sistent limitation in positional encodings related
to their inability to generalize beyond the context
window seen during training. While some meth-
ods like ALiBi show limited generalization, none
can extend to significantly longer sequences than
their pre-trained length. Previous works proposed
solutions like Position Interpolation (Chen et al.,
2023a) and "NTK-aware" interpolation (bloc97,
2023b), with applications in open-source models
like Code Llama (Roziere et al., 2023) and Qwen
7B (Bai et al., 2023b). In this paper, YaRN, an
enhanced extension method for RoPE in models
like LLaMA, GPT NeoX, and PaLM is introduced.
YaRN achieves state-of-the-art performance in ex-
tending context windows, requiring fine-tuning on
less than 0.1% of the original pre-training data. Ad-
ditionally, Dynamic-YaRN, coupled with the Dy-
namic Scaling inference-time technique, achieves
over 2x context window extension without fine-
tuning.
Structure of YaRN. The paper investigates the
challenges associated with Position Interpolation
falling short in predicting the complex relationship
between RoPE and LLM dynamics. The "NTK-
aware" interpolation is introduced to address the
loss of high-frequency details but suffers from sub-optimal extrapolation.
To overcome these limitations, the paper
presents the "NTK-by-parts" interpolation, target-
ing specific RoPE dimensions to improve perfor-
mance. It explores Dynamic Scaling, specifically
"Dynamic NTK" interpolation, which gracefully
handles repeated forward passes and demonstrates
effectiveness, especially on non-fine-tuned models.
In the YaRN method, incorporating temperature
reparametrization on logits to modify the attention
mechanism without altering the code is proposed.
YaRN proves efficient in context extension with
zero overhead during both training and inference.
The combination of YaRN with "NTK-by-parts" in-
terpolation further enhances the proposed method.
During the training phase, the models were ex-
panded without altering their architecture. The
learning process involved recalculating certain as-
pects with different values. The training was con-
ducted in steps, adjusting parameters like the learn-
ing rate and employing specific techniques. This
process was performed for two model sizes, namely
7B and 13B parameters, and specific strategies
were applied to optimize their performance.
In the extrapolation and transfer learning phase,
the models were evaluated on their ability to apply
what they learned to new scenarios. This involved
using a dataset with a certain context length, and
the models were fine-tuned to enhance their capa-
bilities. The evaluation included exploring how
well the models could adapt to new, longer con-
texts, exceeding what they encountered during the
original training.
The results indicate that the larger model success-

--- PAGE 34 ---
fully adapted to extended context lengths, showcas-
ing its ability to learn and apply knowledge from
different scales. This adaptation was achieved effi-
ciently, demonstrating the model’s effectiveness in
transferring its learned information to new contexts
without the need for extensive relearning.
Results. The evaluation commenced by scrutiniz-
ing the model’s performance with an expanding
context window. To achieve this, ten random sam-
ples from Proof-pile, each containing at least 128k
tokens, were selected. The perplexity of these sam-
ples was assessed across different sequence lengths,
ranging from 2k to 128k tokens, with truncation at
2k steps. It’s noteworthy that the training method-
ologies for PI and "NTK-aware" models followed
(Chen et al., 2023a), while YaRN used a similar
approach with 2.5x fewer training steps and data.
The findings indicate robust performance
throughout the targeted context sizes, with YaRN
interpolation notably extending the effective con-
text size of Llama-2 to 128k. Particularly note-
worthy are the YaRN (s=32) models, exhibiting
a continuous decline in perplexity through 128k
despite fine-tuning limited to a 64k token length.
This demonstrates the model’s ability to generalize
to unseen context lengths.
The passkey retrieval task, defined in (Mo-
htashami and Jaggi, 2023a), assesses a model’s
capability to retrieve a simple passkey from a large
amount of otherwise meaningless text. The eval-
uation involved ten iterations of the passkey re-
trieval task with the passkey placed at random loca-
tions uniformly distributed across evaluation con-
text windows of different sizes, ranging from 8k to
128k. Both 7b and 13b models, fine-tuned using
YaRN at a 128k context size, demonstrated very
high accuracy (>99%) across the entire context
window size.
Minimal performance degradation was observed
between the YaRN models and their respective
Llama-2 baselines. On average, there was a mere
0.49% drop in scores between the YaRN (s=16)
and YaRN (s=32) models. This suggests that the
iterative extension from 64k to 128k results in neg-
ligible performance loss.
In summary, YaRN represents an improvement
over existing RoPE interpolation methods and can
seamlessly replace Position Interpolation without
drawbacks and with minimal implementation ef-
fort. The fine-tuned models maintain their original
capabilities across multiple benchmarks while ef-fectively attending to significantly larger context
sizes. Additionally, YaRN facilitates efficient ex-
trapolation through fine-tuning on shorter datasets
and leverages transfer learning for faster conver-
gence, addressing essential aspects in compute-
constrained scenarios.
Related work. ReRoPE (Su, 2023) seeks to ex-
pand the context size of RoPE-based pre-trained
models, claiming an “infinite” context length with-
out fine-tuning, supported by a monotonically de-
creasing loss up to 16k on the Llama 2 13B model.
Unlike embedding interpolation methods, ReRoPE
achieves context extension by modifying the at-
tention mechanism. However, it is currently in-
compatible with Flash Attention 2 (Dao, 2023) and
necessitates two attention passes during inference.
LM-Infinite, a concurrent proposal, shares similar
concepts with YaRN but emphasizes “on-the-fly”
length generalization for non-fine-tuned models.
Like ReRoPE, LM-Infinite modifies the attention
mechanism, making it incompatible with Flash At-
tention 2 and not a direct embedding interpolation
method.
Positional Skip-wisE (PoSE)
(Zhu et al., 2023) present a method called Po-
sitional Skip-wisE (PoSE) training to effectively
extend the context window for transformers. They
split the fixed-length context window into multi-
ple chunks and introduce specialized position bias
terms for each chunk. By varying the bias values
and lengths of the chunks on a per-example ba-
sis, the model learns representations for sequence
positions across a longer range than the actual con-
text size. In this way, PoSE training can simulate
having a larger context window using only a fixed
window, enabling the model to accumulate long-
range contextual knowledge.
Working. The PoSE technique simulates longer
inputs by manipulating position indices within a
fixed context window. It partitions the original win-
dow into chunks, adjusting the position indices of
each chunk by adding distinct skipping bias terms.
These terms, along with the chunk lengths, vary
for each training example, allowing the model to
adapt to all positions within the target context win-
dow during fine-tuning. By maintaining continuous
position indices within each chunk, PoSE closely
resembles pre-training, retaining the model’s lan-
guage modeling and comprehension abilities.
To illustrate, the method divides the original con-

--- PAGE 35 ---
text window LcintoNchunks c0, c1, . . . , c N−1,
each with lengths l0, l1, . . . , l N−1, wherePN−1
i=0li=Lc. It introduces the starting index sti
for each chunk ci, facilitating the formulation of
position indices as follows:
Pos(ci) ={sti, sti+ 1, . . . , st i+li−1},(2)
where,
sti=i−1X
j=0lj (3)
Subsequently, it employs the discrete uniform
distribution U(S)to sample a skipping bias term
ui∼U({ui−1, . . . , L t−Lc})for each chunk ci.
This bias term transforms the original position
indices into:
PoSE(ci) ={ui+sti, ui+sti+ 1, . . . , ui +sti+li−1}(4)
Note that the constraint ui≥ui−1prevents
position index overlaps between chunks.
For the text within each chunk, a similar proce-
dure selects continuous spans of tokens from the
input text x={x0, x1, . . . , x Lx}.
After settling position indices and content for
each chunk, it performs position interpolation to
stabilize fine-tuning.
Figure 17 offers an illustrative comparison of
full-length fine-tuning and PoSE.
Advantages. The advantages of PoSE are three-
fold:
1.Memory and Time Efficiency : By necessi-
tating only the original context size for fine-
tuning, PoSE avoids the quadratic increase in
computational complexity concerning the tar-
get length during the fine-tuning stage. This
significantly diminishes memory and time
overhead.
2.Potential for Extremely-Long Context : The
PoSE architecture demonstrated success in ex-
tending the context window of LLaMA (Tou-
vron et al., 2023a), magnifying the context
window from an original 2,048 tokens up
to 131,072 tokens - a 64 times gain. De-
spite this major expansion in context length,
PoSE maintained LLaMA’s capabilities for
language modeling and understanding.
3.Compatibility with RoPE-based LLMs and
PI Strategies : The effectiveness of PoSEhas been empirically validated across several
representative RoPE-based LLMs, including
LLaMA, LLaMA2 (Touvron et al., 2023a),
GPT-J (Wang and Komatsuzaki, 2021), and
Baichuan (Yang et al., 2023). Furthermore,
PoSE has demonstrated compatibility with
various position interpolation methods, includ-
ing Linear (Chen et al., 2023a), NTK (bloc97,
2023b), and YaRN (Peng et al., 2023) interpo-
lation.
Experiments. The authors investigate the effi-
cacy of long-text modeling across two primary
tasks: language modeling and passkey retrieval.
Language modeling serves as a fundamental mea-
sure of a model’s overall proficiency in handling
extensive text, while passkey retrieval gauges the
maximum token distance considered during the in-
ference stage. Language modeling is evaluated on
the GovReport dataset (Huang et al., 2021) and the
Proof-pile dataset (Azerbayev et al., 2022). For
passkey retrieval, they adopt the approach outlined
in (Mohtashami and Jaggi, 2023a) to generate syn-
thetic prompts for assessment.
Experiments were conducted by scaling to 16k
and 32k using Full-length training on two tech-
niques: RandPos (Ruoss et al., 2023b) and PoSE.
Perplexity scores at various evaluation context win-
dow sizes (ranging from 2k to 32k) are reported for
each scaled model, including the non-fine-tuned
LLaMA model (None). The sliding window ap-
proach proposed by (Press et al., 2021a) is em-
ployed for evaluation, with a window stride set to
1,024 for efficiency.
Observations include an overall decreasing per-
plexity trend for both 16k- and 32k-scaled mod-
els via PoSE. Despite significantly shorter context
lengths during fine-tuning, PoSE achieves compa-
rable results with Full-length, affirming its effec-
tiveness. Notably, PoSE outperforms RandPos.
In the passkey retrieval test proposed by (Mo-
htashami and Jaggi, 2023a), models are tasked with
recovering a random passkey concealed within an
extensive document. The none-fine-tuned LLaMA
model (None) experiences a rapid drop in retrieval
accuracy to 0 when the prompt length exceeds 2k.
In contrast, both PoSE-extended models maintain
a high retrieval accuracy ( ≥90%) within their re-
spective target context windows. This suggests that
models trained via PoSE genuinely possess the ca-
pability to attend to all tokens within the extended
context windows.

--- PAGE 36 ---
Figure 17: Illustration of Full-length fine-tuning vs PoSE (Zhu et al., 2023) fine-tuning for extending the context from 2,048 to
8,192 tokens. Full-length uses all 8,192 tokens directly, while PoSE adjusts the position indices of 2,048 tokens with a unique
skipping bias term. This allows the model to adapt to different relative positions during fine-tuning.
Related work. Several models, including those
proposed by (Press et al., 2021b; Sun et al., 2022),
and (Haviv et al., 2022), aim to ensure consistent
performance even when the number of input tokens
during inference surpasses the model’s trained con-
text window size. On the other hand, some works
(Chen et al., 2023a; bloc97, 2023b; Peng et al.,
2023; Chen et al., 2023b) have focused on fine-
tuning LLMs with a longer context window. How-
ever, all these methods require Full-length fine-
tuning, suffering computational cost that grows
with the target context size. (Ruoss et al., 2023a)
also attempted to mimic longer sequences dur-
ing training to address out-of-distribution lengths.
They introduced randomized positional encoding
(RandPos), randomly selecting an ordered subset
of position indices from longer sequences. PoSE
differs significantly from RandPos: RandPos pri-
marily enhances the length generalization abilities
of encoder-only models during pre-training. In
contrast, PoSE efficiently extends the context win-
dow of pre-trained LLMs, especially those with
a decoder-only architecture. Furthermore, while
RandPos lacks continuous position indices between
adjacent tokens, PoSE deliberately maintains this
continuity within each chunk. This continuity
closely aligns with the pre-training phase, minimiz-
ing any disruption to learned language modeling
and comprehension abilities.
Managing extremely long input sequences of-
ten involves memory mechanisms. Two prominent
research lines leverage memory: the recurrence-
based approach ((Dai et al., 2019b; Bulatov et al.,
2022)) and the retrieval-based strategy ((Wu et al.,
2022; Wang et al., 2023; Tworkowski et al., 2023)).
Recurrence-based methods segment lengthy inputs,
reusing hidden states from previous segments as
memory for the current one. However, they suffer
from information loss and limited random accesscapacity. In contrast, retrieval-based paradigms en-
code prior sequences as (key, value) pairs, using
a memory retriever and reader to extract encoded
information. A drawback here is the lack of inter-
action between discrete memory segments.
Recently, (Mohtashami and Jaggi, 2023a) pro-
posed landmark attention, enabling random access
to input chunks via landmark tokens. However,
the PoSE method achieves full access to the entire
input without altering the attention mechanism.
6 Conclusion
In summary, this paper provides a comprehensive
review of the diverse techniques and methodolo-
gies for extending the context length of LLMs. The
taxonomy presented categorizes these approaches
into two broad strategies - extrapolation andinter-
polation . Extrapolation techniques aim to expand
the model’s ability to handle sequences beyond its
initially trained context length. This includes zero-
shot methods leveraging specialized components
like position encodings, attention mechanisms, and
memory augmentation to achieve on-the-fly gener-
alization. Fine-tuning strategies are also explored
to adapt models for longer contexts not encoun-
tered during pre-training. Interpolation techniques
focus on optimizing models to smoothly extend
context comprehension within the observed train-
ing length. Specialized attention mechanisms and
prompt compression facilitate efficient handling of
lengthy contexts. Fine-tuning interpolation adapts
models to gracefully transition when sequences be-
gin to exceed the trained length. The survey offers
insights into the versatility of techniques spanning
prompt engineering, attention mechanisms, posi-
tional encodings, and memory augmentation. It
highlights innovations in model architectures and
training methodologies tailored to address context
length limitations. Extensive empirical analysis

--- PAGE 37 ---
substantiates the efficacy of these diverse tech-
niques on benchmarks and downstream tasks. By
providing a structured taxonomy and review of ex-
isting literature, this paper contributes to a clearer
understanding of the evolving landscape of context
length extension in LLMs. The discussions identify
promising research directions, underscoring the im-
portance of continued efforts to develop models
proficient in processing extensive contextual infor-
mation. With increasing interest in long-form text
generation and reasoning over large corpora, en-
hanced context handling will remain an active area
of research in coming years.
7 Discussion
This comprehensive survey highlights the remark-
able progress made in developing diverse method-
ologies for expanding the contextual capacities of
LLMs. However, several open questions and chal-
lenges remain, warranting further investigation by
the research community. A key direction for future
work involves exploring synergistic combinations
of the techniques reviewed in this paper. For in-
stance, integrating memory augmentation strate-
gies with specialized attention mechanisms could
potentially yield models proficient in handling sig-
nificantly longer contexts. Hybrid approaches that
leverage the complementary strengths of different
techniques merit deeper exploration. Another cru-
cial aspect requiring attention is the development of
appropriate evaluation benchmarks and metrics for
accurately assessing context extension techniques.
While preliminary benchmarks have been proposed,
standardized suites could facilitate more rigorous
comparisons between methods. Metrics that pro-
vide nuanced insights into a model’s contextual ca-
pacities beyond simplistic perplexity scores will be
valuable. The interpretability of context extension
techniques is also an underexplored area. Methods
that enhance the explainability of how models uti-
lize extended contexts could unlock deeper insights
into their inner workings. This interpretability will
be key for debugging, analysis and responsible de-
ployment of LLMs. Training efficiency and the
high resource costs of developing models with ex-
panded contexts is a significant challenge. Tech-
niques that can match the efficiencies of training
natively on short contexts could accelerate progress.
Multi-stage training procedures and transfer learn-
ing for context extension are promising directions.
Finally, studying the impact of long contexts onemergent capabilities of LLMs presents intriguing
opportunities. For instance, how does reasoning
over documents rather than sentences transform a
model’s understanding of complex concepts? Inves-
tigating these higher-order effects through carefully
designed evaluations and experiments remains an
open avenue for future work. In conclusion, this
survey provides a structured foundation that sum-
marizes progress and outlines key open challenges.
Continued research leveraging this synthesis of ex-
isting literature will further the development of
LLMs that exhibit an intricate awareness of long-
range context. With higher contextual sophistica-
tion, these models are poised to ultimately attain
more human-like language comprehension.
References
Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago
Ontañón, Siddhartha Brahma, Yury Zemlyanskiy,
David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay,
et al. 2023. Colt5: Faster long-range transform-
ers with conditional computation. arXiv preprint
arXiv:2303.09752 .
Hussein T Al-Natsheh, Lucie Martinet, Fabrice Muh-
lenbach, and Djamel Abdelkader Zighed. 2017. Udl
at semeval-2017 task 1: Semantic textual similarity
estimation of english sentence pairs using regression
model over pairwise features. In Proceedings of the
11th International Workshop on Semantic Evaluation
(SemEval-2017) , pages 115–119.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Mérouane Debbah, Étienne Goffinet, Daniel Hess-
low, Julien Launay, Quentin Malartic, et al. 2023.
The falcon series of open language models. arXiv
preprint arXiv:2311.16867 .
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor
Lewkowycz, Vedant Misra, Vinay Ramasesh, Am-
brose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam
Neyshabur. 2022. Exploring length generalization in
large language models. Advances in Neural Informa-
tion Processing Systems , 35:38546–38556.
Zhangir Azerbayev, Edward Ayers, and Bartosz Pi-
otrowski. 2022. Proof-pile. https://github.com/
zhangir-azerbayev/proof-pile .
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,

--- PAGE 38 ---
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
Zhu. 2023a. Qwen technical report. arXiv preprint
arXiv:2309.16609 .
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,
and Jingren Zhou. 2023b. Qwen-vl: A versatile
vision-language model for understanding, localiza-
tion, text reading, and beyond.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, et al. 2023c. Longbench:
A bilingual, multitask benchmark for long context
understanding. arXiv preprint arXiv:2308.14508 .
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R Gormley. 2023. Unlimiformer: Long-
range transformers with unlimited length input.
arXiv preprint arXiv:2305.01625 .
Sid Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, et al.
2022. Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745 .
bloc97. 2023a. Add ntk-aware interpolation "by parts"
correction. https://github.com/jquesnelle/
yarn/pull/1 .
bloc97. 2023b. Ntk-aware scaled rope allows llama
models to have extended (8k+) context size without
any fine-tuning and minimal perplexity degrada-
tion. https://www.reddit.com/r/LocalLLaMA/
comments/14lz7j5/ntkaware_scaled_rope_
allows_llama_models_to_have/ .
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao
Zhang, Christoph Feichtenhofer, and Judy Hoffman.
2022. Token merging: Your vit but faster. arXiv
preprint arXiv:2210.09461 .
Aligning Books. 2015. Movies: Towards story-like vi-
sual explanations by watching movies and reading
books—yukun zhu. In Ryan Kiros, Richard Zemel,
Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-
ralba and Sanja Fidler—Proceedings of the IEEE
international conference on computer vision , pages
19–27.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning , pages 2206–2240. PMLR.Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Mirelle Bueno, Carlos Gemmell, Jeffrey Dalton,
Roberto Lotufo, and Rodrigo Nogueira. 2022. In-
duced natural language rationales and interleaved
markup tokens enable extrapolation in large language
models. arXiv preprint arXiv:2208.11445 .
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.
2022. Recurrent memory transformer. Advances
in Neural Information Processing Systems , 35:11079–
11091.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023a. Extending context window
of large language models via positional interpolation.
arXiv preprint arXiv:2306.15595 .
Ting Chen, Simon Kornblith, Mohammad Norouzi, and
Geoffrey Hinton. 2020. A simple framework for
contrastive learning of visual representations. In In-
ternational conference on machine learning , pages
1597–1607. PMLR.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-
glora: Efficient fine-tuning of long-context large lan-
guage models. arXiv preprint arXiv:2309.12307 .
Alexis Chevalier, Alexander Wettig, Anirudh Ajith,
and Danqi Chen. 2023. Adapting language
models to compress contexts. arXiv preprint
arXiv:2305.14788 .
R Child, S Gray, A Radford, and I Sutskever. 2019a.
Generating long sequences with sparse transformers.
arxiv 2019. arXiv preprint arXiv:1904.10509 .
Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019b. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509 .
Jaewan Choi, Hailong Li, Byeongho Kim, Seunghwan
Hwang, and Jung Ho Ahn. 2022. Accelerating trans-
former networks through recomposing softmax lay-
ers. In 2022 IEEE International Symposium on Work-
load Characterization (IISWC) , pages 92–103. IEEE.
Krzysztof Choromanski, Valerii Likhosherstov, David
Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
Lukasz Kaiser, et al. 2020. Rethinking attention with
performers. arXiv preprint arXiv:2009.14794 .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311 .

--- PAGE 39 ---
Kevin Clark, Minh-Thang Luong, Quoc V Le, and
Christopher D Manning. 2020. Electra: Pre-training
text encoders as discriminators rather than generators.
arXiv preprint arXiv:2003.10555 .
Together Computer. 2023. Redpajama: an open dataset
for training large language models.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale. arXiv
preprint arXiv:1911.02116 .
Dacheng, Xie Shao, Zheng Sheng, Stoica E. Gonzalez,
and Zhang Ma. 2023. [link].
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019a.
Transformer-XL: Attentive language models beyond
a fixed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 2978–2988, Florence, Italy. Asso-
ciation for Computational Linguistics.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
2019b. Transformer-xl: Attentive language mod-
els beyond a fixed-length context. arXiv preprint
arXiv:1901.02860 .
Tri Dao. 2023. Flashattention-2: Faster attention with
better parallelism and work partitioning. arXiv
preprint arXiv:2307.08691 .
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Advances in Neural Information Processing Systems ,
35:16344–16359.
DataCanary, Lili Jiang hilfialkaff, and tomtung Meg Ris-
dal, Nikhil Dandekar. 2017. Quora question pairs.
Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim
Genewein, Li Kevin Wenliang, Elliot Catt, Chris
Cundy, Marcus Hutter, Shane Legg, Joel Veness, et al.
2022. Neural networks and the chomsky hierarchy.
arXiv preprint arXiv:2207.02098 .
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-
tiplication for transformers at scale. arXiv preprint
arXiv:2208.07339 .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms. arXiv preprint arXiv:2305.14314 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.arXiv preprint arXiv:1810.04805 .J Ding, S Ma, L Dong, X Zhang, S Huang, W Wang,
N Zheng, and F Wei. 2023a. Longnet: Scaling
transformers to 1,000,000,000 tokens 2023. arXiv
preprint arXiv:2307.02486 .
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,
Shaohan Huang, Wenhui Wang, Nanning Zheng, and
Furu Wei. 2023b. Longnet: Scaling transformers to
1,000,000,000 tokens.
Bill Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InThird International Workshop on Paraphrasing
(IWP2005) .
Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin
Zhao. 2023. A survey on long text modeling with
transformers. arXiv preprint arXiv:2302.14502 .
Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. 2020.
An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint
arXiv:2010.11929 .
emozilla. 2023. Dynamically scaled rope further
increases performance of long context llama with
zero fine-tuning. https://www.reddit.com/r/
LocalLLaMA/comments/14mrgpr/dynamically_
scaled_rope_further_increases/ .
Ingrid Lossius Falkum and Agustin Vicente. 2015. Pol-
ysemy: Current perspectives and approaches.
Wikimedia Foundation. 2021. Wikipedia corpus foun-
dation.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
Dan Alistarh. 2022. Gptq: Accurate post-training
compression for generative pretrained transformers.
arXiv preprint arXiv:2210.17323 .
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The pile: An
800gb dataset of diverse text for language modeling.
Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional se-
quence to sequence learning. In International confer-
ence on machine learning , pages 1243–1252. PMLR.
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh
Raje, Venkatesan Chakaravarthy, Yogish Sabharwal,
and Ashish Verma. 2020. Power-bert: Accelerating
bert inference via progressive word-vector elimina-
tion. In International Conference on Machine Learn-
ing, pages 3690–3699. PMLR.
Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv:1410.5401 .

--- PAGE 40 ---
Mandy Guo, Joshua Ainslie, David Uthus, Santiago On-
tanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.
2021. Longt5: Efficient text-to-text transformer for
long sequences. arXiv preprint arXiv:2112.07916 .
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In International confer-
ence on machine learning , pages 3929–3938. PMLR.
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng
Ji, and Sinong Wang. 2023a. Lm-infinite: Simple
on-the-fly length generalization for large language
models.
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng
Ji, and Sinong Wang. 2023b. Lm-infinite: Simple
on-the-fly length generalization for large language
models. arXiv preprint arXiv:2308.16137 .
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer
Levy. 2022. Transformer language models without
positional encodings still learn positional information.
arXiv preprint arXiv:2203.16634 .
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2020. Deberta: Decoding-enhanced
bert with disentangled attention. arXiv preprint
arXiv:2006.03654 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2020. Measuring massive multitask language under-
standing. arXiv preprint arXiv:2009.03300 .
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation , 9(8):1735–
1780.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-
reit, Noam Shazeer, Ian Simon, Curtis Hawthorne,
Andrew M Dai, Matthew D Hoffman, Monica Din-
culescu, and Douglas Eck. 2018. Music transformer.
arXiv preprint arXiv:1809.04281 .
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng
Ji, and Lu Wang. 2021. Efficient attentions for
long document summarization. arXiv preprint
arXiv:2104.02112 .
Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xi-
ang. 2020. Improve transformer models with bet-
ter relative position embeddings. arXiv preprint
arXiv:2009.13658 .
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan
Dyer, and Behnam Neyshabur. 2022. Block-recurrent
transformers. Advances in Neural Information Pro-
cessing Systems , 35:33248–33261.Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2022. Few-shot learning with re-
trieval augmented language models. arXiv preprint
arXiv:2208.03299 .
Nihal Jain, Dejiao Zhang, Wasi Uddin Ahmad, Zijian
Wang, Feng Nan, Xiaopeng Li, Ming Tan, Ramesh
Nallapati, Baishakhi Ray, Parminder Bhatia, et al.
2022. Contraclm: Contrastive learning for causal
language model. arXiv preprint arXiv:2210.01185 .
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing
Yang, and Lili Qiu. 2023a. Llmlingua: Compressing
prompts for accelerated inference of large language
models. arXiv preprint arXiv:2310.05736 .
Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng
Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b.
Longllmlingua: Accelerating and enhancing llms in
long context scenarios via prompt compression.
Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,
Zhiruo Wang, Jamie Callan, and Graham Neubig.
2022. Retrieval as attention: End-to-end learning
of retrieval and reading within a single transformer.
arXiv preprint arXiv:2212.02027 .
Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael
Tang, Yiqin Dai, and Jimmy Lin. 2023c. “low-
resource” text classification: A parameter-free clas-
sification method with compressors. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 6810–6828.
Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhi-
meng Jiang, Chia-Yuan Chang, and Xia Hu. 2023.
Growlength: Accelerating llms pretraining by pro-
gressively growing training length. arXiv preprint
arXiv:2310.00576 .
Kaiokendev. 2023. Things i’m learning while training
superhot.
Łukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy
Bengio. 2017. Learning to remember rare events.
arXiv preprint arXiv:1703.03129 .
Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. arXiv preprint
arXiv:2004.04906 .
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-
pas, and François Fleuret. 2020. Transformers are
rnns: Fast autoregressive transformers with linear
attention. In International conference on machine
learning , pages 5156–5165. PMLR.
Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking
positional encoding in language pre-training. arXiv
preprint arXiv:2006.15595 .

--- PAGE 41 ---
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through memorization: Nearest neighbor language
models. arXiv preprint arXiv:1911.00172 .
Omar Khattab, Christopher Potts, and Matei Zaharia.
2021. Relevance-guided supervision for openqa with
colbert. Transactions of the association for computa-
tional linguistics , 9:929–944.
Gyuwan Kim and Kyunghyun Cho. 2020. Length-
adaptive transformer: Train once with length
drop, use anytime with search. arXiv preprint
arXiv:2010.07003 .
Sehoon Kim, Coleman Hooper, Thanakul Wattana-
wong, Minwoo Kang, Ruohan Yan, Hasan Genc,
Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W
Mahoney, et al. 2023. Full stack optimization of
transformer inference: a survey. arXiv preprint
arXiv:2302.14017 .
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. arXiv
preprint arXiv:2001.04451 .
Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and
Kentaro Inui. 2021. Shape: Shifted absolute po-
sition embedding for transformers. arXiv preprint
arXiv:2109.05644 .
Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jer-
nite, Margaret Mitchell, Sean Hughes, Thomas Wolf,
et al. 2022. The stack: 3 tb of permissively licensed
source code. arXiv preprint arXiv:2211.15533 .
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-
zalez, Hao Zhang, and Ion Stoica. 2023. Efficient
memory management for large language model serv-
ing with pagedattention. In Proceedings of the 29th
Symposium on Operating Systems Principles , pages
611–626.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learn-
ing of language representations. arXiv preprint
arXiv:1909.11942 .
Yucheng Li. 2023. Unlocking context constraints of
llms: Enhancing context efficiency of llms with self-
information-based content filtering. arXiv preprint
arXiv:2304.12102 .
Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu,
Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023.
Unleashing infinite-length input capacity for large-
scale language models with self-controlled memory
system. arXiv preprint arXiv:2304.13343 .
Tatiana Likhomanenko, Qiantong Xu, Gabriel Syn-
naeve, Ronan Collobert, and Alex Rogozhnikov.
2021. Cape: Encoding relative positions with contin-
uous augmented positional embeddings. Advances inNeural Information Processing Systems , 34:16079–
16092.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,
Maria Lomeli, Rich James, Pedro Rodriguez, Jacob
Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023.
Ra-dit: Retrieval-augmented dual instruction tuning.
arXiv preprint arXiv:2310.01352 .
Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang
Zhang, Jinjie Gu, and Guannan Zhang. 2023a. Think-
in-memory: Recalling and post-thinking enable llms
with long-term memory.
Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang
Zhang, Jinjie Gu, and Guannan Zhang. 2023b.
Think-in-memory: Recalling and post-thinking en-
able llms with long-term memory. arXiv preprint
arXiv:2311.08719 .
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023c. Lost in the middle: How lan-
guage models use long contexts. arXiv preprint
arXiv:2307.03172 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
arXiv:1711.05101 .
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian
He, Liangke Gui, Graham Neubig, Jonathan May,
and Luke Zettlemoyer. 2022. Mega: moving av-
erage equipped gated attention. arXiv preprint
arXiv:2209.10655 .
Matt Mahoney. 2006. [link].
Pedro Henrique Martins, Zita Marinho, and André F. T.
Martins. 2022. ∞-former: Infinite memory trans-
former.
Lian Meng and Minlie Huang. 2018. Dialogue intent
classification with long short-term memory networks.
InNatural Language Processing and Chinese Com-
puting: 6th CCF International Conference, NLPCC
2017, Dalian, China, November 8–12, 2017, Pro-
ceedings 6 , pages 42–50. Springer.
Ali Modarressi, Hosein Mohebbi, and Moham-
mad Taher Pilehvar. 2022. Adapler: Speeding up in-
ference by adaptive length reduction. arXiv preprint
arXiv:2203.08991 .
Amirkeivan Mohtashami and Martin Jaggi. 2023a.
Landmark attention: Random-access infinite context
length for transformers.

--- PAGE 42 ---
Amirkeivan Mohtashami and Martin Jaggi. 2023b.
Random-access infinite context length for transform-
ers. In Thirty-seventh Conference on Neural Infor-
mation Processing Systems .
Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.
Learning to compress prompts with gist tokens.
arXiv preprint arXiv:2304.08467 .
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
et al. 2021. Webgpt: browser-assisted question-
answering with human feedback (2021). URL
https://arxiv. org/abs/2112.09332 .
Piotr Nawrot, Szymon Tworkowski, Michał Tyrolski,
Łukasz Kaiser, Yuhuai Wu, Christian Szegedy, and
Henryk Michalewski. 2021. Hierarchical trans-
formers are more efficient language models. arXiv
preprint arXiv:2110.13711 .
Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang,
Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz,
Philippe Laban, Ben Krause, et al. 2023. Xgen-7b
technical report. arXiv preprint arXiv:2309.03450 .
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for se-
quence modeling. arXiv preprint arXiv:1904.01038 .
Charles Packer, Vivian Fang, Shishir G Patil, Kevin
Lin, Sarah Wooders, and Joseph E Gonzalez. 2023.
Memgpt: Towards llms as operating systems. arXiv
preprint arXiv:2310.08560 .
Arka Pal, Deep Karkhanis, Manley Roberts, Samuel
Dooley, Arvind Sundararajan, and Siddartha Naidu.
2023. Giraffe: Adventures in expanding context
lengths in llms. arXiv preprint arXiv:2308.10882 .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics , pages 311–318.
Ankur P Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. arXiv preprint
arXiv:1606.01933 .
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S Bern-
stein. 2023. Generative agents: Interactive simulacra
of human behavior. In Proceedings of the 36th An-
nual ACM Symposium on User Interface Software
and Technology , pages 1–22.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context window
extension of large language models.Ofir Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Confer-
ence on Learning Representations .
Ofir Press, Noah A. Smith, and Mike Lewis. 2021a.
Shortformer: Better language modeling using shorter
inputs. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 5493–5505.
Ofir Press, Noah A Smith, and Mike Lewis. 2021b.
Train short, test long: Attention with linear biases
enables input length extrapolation. arXiv preprint
arXiv:2108.12409 .
Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen tau Yih,
Sinong Wang, and Jie Tang. 2020. Blockwise self-
attention for long document understanding.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International confer-
ence on machine learning , pages 8748–8763. PMLR.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
and Timothy P Lillicrap. 2019. Compressive trans-
formers for long-range sequence modelling. arXiv
preprint arXiv:1911.05507 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. arXiv preprint arXiv:2302.00083 .
Ahmad Rashid, Vasileios Lioutas, Abbas Ghaddar, and
Mehdi Rezagholizadeh. 2021. Towards zero-shot
knowledge distillation for natural language process-
ing. In Proceedings of the 2021 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 6551–6561.
Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,
Inbal Magar, Omri Abend, Ehud Karpas, Amnon
Shashua, Kevin Leyton-Brown, and Yoav Shoham.

--- PAGE 43 ---
2023. Parallel context windows for large language
models. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 6383–6402.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
arXiv preprint arXiv:1908.10084 .
Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang,
Jure Leskovec, Dale Schuurmans, and Bo Dai. 2021.
Combiner: Full attention transformer with sparse
computation cost. Advances in Neural Information
Processing Systems , 34:22470–22482.
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and
David Grangier. 2021. Efficient content-based sparse
attention with routing transformers. Transactions of
the Association for Computational Linguistics , 9:53–
68.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi
Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane
Legg, and Joel Veness. 2023a. Randomized posi-
tional encodings boost length generalization of trans-
formers. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 1889–1903, Toronto,
Canada. Association for Computational Linguistics.
Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi
Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane
Legg, and Joel Veness. 2023b. Randomized posi-
tional encodings boost length generalization of trans-
formers. arXiv preprint arXiv:2305.16843 .
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,
Christopher Potts, and Matei Zaharia. 2021. Col-
bertv2: Effective and efficient retrieval via
lightweight late interaction. arXiv preprint
arXiv:2112.01488 .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909 .
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-
rant, and Omer Levy. 2023. Zeroscrolls: A zero-
shot benchmark for long text understanding. arXiv
preprint arXiv:2305.14196 .
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.
Self-attention with relative position representations.
arXiv preprint arXiv:1803.02155 .Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H Chi, Nathanael Schärli,
and Denny Zhou. 2023. Large language models can
be easily distracted by irrelevant context. In Inter-
national Conference on Machine Learning , pages
31210–31227. PMLR.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 conference on empiri-
cal methods in natural language processing , pages
1631–1642.
Jianlin Su. 2023. Rectified rotary position embeddings.
https://github.com/bojone/rerope .
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing , 568:127063.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2021. Roformer: En-
hanced transformer with rotary position embedding.
arXiv preprint arXiv:2104.09864 .
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma,
Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu
Wei. 2023a. Retentive network: A successor to trans-
former for large language models. arXiv preprint
arXiv:2307.08621 .
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2022. A length-extrapolatable
transformer. arXiv preprint arXiv:2212.10554 .
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2023b. A length-extrapolatable
transformer. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers) , pages 14590–14604,
Toronto, Canada. Association for Computational Lin-
guistics.
Mingxu Tao, Yansong Feng, and Dongyan Zhao. 2023.
A frustratingly easy improvement for position em-
beddings via random padding. arXiv preprint
arXiv:2305.04859 .
MN Team et al. 2023. Introducing mpt-7b: A new
standard for open-source, commercially usable llms.
MosaicML NLP Team. 2023. Introducing mpt-30b:
Raising the bar for open-source foundation models.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .

--- PAGE 44 ---
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Szymon Tworkowski, Konrad Staniszewski, Mikołaj
Pacek, Yuhuai Wu, Henryk Michalewski, and Pi-
otr Miło ´s. 2023. Focused transformer: Con-
trastive training for context scaling. arXiv preprint
arXiv:2307.03170 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .
Ben Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6
billion parameter autoregressive language model.
Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong,
Wenhui Wang, Zhiliang Peng, Yu Wu, Payal
Bajaj, Saksham Singhal, Alon Benhaim, et al.
2022a. Foundation transformers. arXiv preprint
arXiv:2210.06423 .
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 .
Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu,
Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023. Aug-
menting language models with long-term memory.
arXiv preprint arXiv:2306.07174 .
Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xi-
aodong Liu, Xifeng Yan, Jianfeng Gao, and Furu
Wei. 2022b. Visually-augmented language modeling.
arXiv preprint arXiv:2205.10178 .
Weizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai,
Boxing Chen, and Weihua Luo. 2022c. Task-oriented
dialogue system as natural language generation. In
Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval , pages 2698–2703.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in Neural
Information Processing Systems , 35:24824–24837.
Ulme Wennberg and Gustav Eje Henter. 2021.
The case for translation-invariant self-attention in
transformer-based language models. arXiv preprint
arXiv:2106.01950 .Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426 .
BigScience Workshop, Teven Le Scao, Angela Fan,
Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luc-
cioni, François Yvon, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Sti-
ennon, Ryan Lowe, Jan Leike, and Paul Christiano.
2021. Recursively summarizing books with human
feedback. arXiv preprint arXiv:2109.10862 .
Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Al-
borz Geramifard, and Zhou Yu. 2020. Memformer:
A memory-augmented transformer for sequence mod-
eling. arXiv preprint arXiv:2010.06891 .
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,
Mark Dredze, Sebastian Gehrmann, Prabhanjan Kam-
badur, David Rosenberg, and Gideon Mann. 2023a.
Bloomberggpt: A large language model for finance.
arXiv preprint arXiv:2303.17564 .
Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi,
Zhewei Yao, and Yuxiong He. 2023b. Understanding
int4 quantization for transformer models: Latency
speedup, composability, and failure cases. arXiv
preprint arXiv:2301.12017 .
Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and
Christian Szegedy. 2022. Memorizing transformers.
arXiv preprint arXiv:2203.08913 .
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-
peng Kong. 2023c. Self-adaptive in-context learn-
ing: An information compression perspective for in-
context example selection and ordering.
Wenhan Xiong, Barlas O ˘guz, Anchit Gupta, Xilun
Chen, Diana Liskovich, Omer Levy, Wen-tau Yih,
and Yashar Mehdad. 2021. Simple local attentions
remain competitive for long-context tasks. arXiv
preprint arXiv:2112.07210 .
Jing Xu, Arthur Szlam, and Jason Weston. 2021. Be-
yond goldfish memory: Long-term open-domain con-
versation. arXiv preprint arXiv:2107.07567 .
Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu
Niu, Hua Wu, Haifeng Wang, and Shihang Wang.
2022. Long time no see! open-domain conversa-
tion with long-term persona memory. arXiv preprint
arXiv:2203.05797 .
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong
Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,
Dong Yan, Fan Yang, et al. 2023. Baichuan 2:
Open large-scale language models. arXiv preprint
arXiv:2309.10305 .

--- PAGE 45 ---
Jianxin Yang. 2023. Longqlora: Efficient and effective
method to extend context length of large language
models. arXiv preprint arXiv:2311.04879 .
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding. Advances in neural informa-
tion processing systems , 32.
Dani Yogatama, Cyprien de Masson d’Autume, and
Lingpeng Kong. 2021. Adaptive semiparametric lan-
guage models. Transactions of the Association for
Computational Linguistics , 9:362–373.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh
Rawat, Sashank J Reddi, and Sanjiv Kumar.
2019. Are transformers universal approximators of
sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077 .
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. 2020. Big bird: Transformers for
longer sequences. Advances in neural information
processing systems , 33:17283–17297.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .
Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas
Guibas, and Jitendra Malik. 2020. Side-tuning: a
baseline for network adaptation via additive side net-
works. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part III 16 , pages 698–714. Springer.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Bartosz Piotrowski Zhangir Azerbayev, Edward Ayers.
2022. [link].
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. arXiv preprint arXiv:2306.05685 .
Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yan-
lin Wang. 2023. Memorybank: Enhancing large
language models with long-term memory. arXiv
preprint arXiv:2305.10250 .
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-
ing language models with memory augmentation.
arXiv preprint arXiv:2205.12674 .Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui,
Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cot-
terell, and Mrinmaya Sachan. 2023. Recurrentgpt:
Interactive generation of (arbitrarily) long text. arXiv
preprint arXiv:2305.13304 .
Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-
hao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-
cient context window extension of llms via positional
skip-wise training. arXiv preprint arXiv:2309.10400 .

--- PAGE 46 ---
Table 1: Summary of all the works related to context length extension techniques. Here, we have divided each work by the
following factors: 1. Technique, 2. Train Length(s), 3. Evaluation Length(s), 4. Metrics, 5. Model(s), 6. Task(s) and 7.
Benchmark(s).
Category Technique Title Train
Length(s)Evaluation
Length(s)Metric(s) Model(s) Task(s) Benchmark(s)
Extrapolation:
Zero-shotPositional
encodingTrain Short, Test Long:
Attention with Linear
Biases Enables Input
Length Extrapolation
(Press et al., 2021b)64, 128,
256, 512,
1024, 1536,
2048, 307264, 128, 256,
512, 1024,
1536, 2048,
3072Perplexity, Words
per seconds,
MemoryCustomized
Transformer
ModelLanguage
modeling, text
generationWikiText-103,
Toronto
Book Corpus,
CC100+RoBerta
Corpus
RoFormer: Enhanced
transformer with Rotary
Position Embedding
(Su et al., 2021)128, 256,
512, 1536Details not
providedBLEU, GLEU,
AccuracyCustomized
Transformer
Model
(RoFormer),
BERT,
WoBERT,
NEZHAMachine
translation,
Semantic text
matchingWMT 2014
English-German
dataset, Chi-
nese dataset,
CAIL2019-
SCM, GLUE(
MRPC, SST-2,
QNLI, STS-B,
QQP, MNLI),
Wikipedia Cor-
pus Foundation,
BookCorpus
Randomized Positional
Encodings Boost
Length Generalization
of Transformers (Ruoss
et al., 2023b)1024, 2048,
4096, 8192Details not
providedAccuracy Encoder-only
Customized
Transformer
ModelVarious
algorithmic
reasoning
tasksManual
Specialized
attention
mechanismA
Length-Extrapolatable
Transformer (Sun et al.,
2023b)1024 256, 512,
1024, 2048,
4096Perplexity Customized
Transformer
Model (LeX)Details not
providedarXiv dataset,
Pile, Books3,
OpenWebText2,
Stack Exchange,
PubMed
Abstracts,
Wikipedia,
PG-19, Book-
Corpus2, NIH
Exporter,
Pile-CC
LongNet: Scaling
Transformers to
1,000,000,000 Tokens
(Ding et al., 2023b)8k, 16k, 32k 2k, 8k, 32k Perplexity Customized
Transformer
Model
(LongNet)Long-
sequence
modelingStack dataset
Window
based
approachesGrowLength:
Accelerating LLMs
Pretraining by
Progressively Growing
Training Length (Jin
et al., 2023)128, 256,
512, 1024Details not
providedTraining loss 70M, 160M
and 410M
LLM (specific
model not
mentioned)Details not
providedNeural net-
works and
the chomsky
hierarchy
Memory/Retri-
eval
augmented
approachesLandmark Attention:
Random-Access
Infinite context length
for Transformers
(Mohtashami and Jaggi,
2023a)250, 256,
300, 360, 512512, 2048,
4096Perplexity Transformer-
XL, LLaMA-
7BLanguage
modeling,
Next word
prediction,
Information
retrieval over
long contextsPG-19, arXiv
Augmenting Language
Models with
Long-Term Memory
(Wang et al., 2023)1k 65k Perplexity,
accuracy, F1
scoreGPT-2-407M Long-context
language
modeling,
long-context
understanding,
memory-
augmented
in-context
learningGutenberg-2022
(PG-22), arXiv,
ChapterBreak,
SST-2, MPQA,
MR, Subj,
SST-5
Extrapolation:
Fine-tunedMemory/Retri-
eval
augmented
approachesThink-in-Memory:
Recalling and
Post-thinking Enable
LLMs with Long-Term
Memory (Liu et al.,
2023a)Details not
providedDetails not
providedRetrieval
Accuracy,
Response
Correctness,
Contextual
CoherenceChatGLM-6B,
Baichuan2-
13BResponse
generation
for long-term
conversationsKdConv, GVD,
RMD
Focused Transformer:
Contrastive Training for
Context Scaling
(Tworkowski et al.,
2023)512, 1024,
2048, 40962k, 4k, 8k,
16k, 64k, 128kPerplexity Decoder-only
Customized
Trans-
former Model
(LongLLaMA),
OpenLLaMA
3B,
OpenLLaMA
7BPasskey
retrieval, QA,
Long-context
language
modelingPG-19, arXiv,
Github code,
Isabelle, TREC,
WebQS
Continued on the next page

--- PAGE 47 ---
Table 1 – Continued from the previous page
Category Technique Title Train
Length(s)Evaluation
Length(s)Metric(s) Model(s) Task(s) Benchmark(s)
MemGPT: Towards
LLMs as Operating
Systems (Packer et al.,
2023)Details not
providedDetails not
providedROUGE-L,
Accuracy, CSIM
similarity scoresGPT-4 Deep memory
retrieval task,
Conversation
opener
task, Multi-
document QA,
Nested key-
value retrieval
requiring
multi-hop
lookupsMSC, NQ-Open
Wikipedia
Interpolation:
Zero-shotSpecialized
attention
mechanismLM-Infinite: Simple
On-the-Fly Length
Generalization for
Large Language
Models (Han et al.,
2023a)Details not
provided2k, 4k, 8k,
16k, 32k, 128kPerplexity MPT-7B,
LLaMA,
GPT-J-6B,
LLaMA-2Long text
generationarXiv, Open-
WebText2
LongLoRA: Efficient
Fine-tuning of
Long-Context Large
Language Models
(Chen et al., 2023b)2048, 4096 8192, 16384,
32768, 65536,
100,000Perplexity,
Passkey retrieval
accuracy, Win-
rate in topic
retrievalLLaMA2-7B,
13B, and 70BLong-
sequence
language
modeling,
Topic retrievalRedPajama
LongQLoRA: Efficient
and Effective Method
to Extend context
length of Large
Language Models
(Yang, 2023)4k, 8k 1024, 2048,
4096, 8192Perplexity LLaMA-2-7B,
MPT-7B,
LongLoRA-
7B,
LongQLoRALong-
sequence
language mod-
eling, Passkey
retrieval,
Topic re-
trieval in long
conversationsPG-19, Proof-
Pile
Prompt
compres-
sion based
approachesLongLLMLingua:
Accelerating and
Enhancing LLMs in
Long Context
Scenarios via Prompt
Compression (Jiang
et al., 2023b)3k-10k Details not
providedAccuracy GPT-3.5-
Turbo,
LongChat-13BSingle and
Multi-Doc
QA, Summa-
rization, Code
completion,
Sentiment
classification,
Information
reorderingNQ-multi-
document QA,
LongBench,
ZeroSCROLLS
Interpolation:
Fine-tunedRoPE
based
approachesExtending Context
Window of Large
Language Models via
Positional Interpolation
(Chen et al., 2023a)8192, 16384,
327682048, 4096,
16384, 32768Perplexity LLaMA-7B,
13B, 33B, and
65BLanguage
modeling,
Passkey
retrieval, Long
document
summarizationPile, PG-19,
RedPajama,
arXiv math
YaRN: Efficient
Context Window
Extension of Large
Language Models
(Peng et al., 2023)4k, 32k, 64k,
100k, 128k8192, 32768,
65536, 98304,
131072Perplexity LLaMA-2 7B,
LLaMA-2
13B,
GPT-NeoX
and Mistral 7B
v0.1,MistralLite
7B, PaLMPasskey
retrievalPG-19, Proof-
pile, Hugging
Face open LLM
benchmark suite
PoSE: Efficient Context
Window Extension of
LLMs via Positional
Skip-wise Training
(Zhu et al., 2023)2k, 16k 2k, 4k, 8k,
16k, 32k, 64k,
96k, 128kPerplexity LLaMA-7B Language
modeling,
Passkey
retrievalProof-Pile,
GovReport,
Gutenberg (PG-
19), Books3
