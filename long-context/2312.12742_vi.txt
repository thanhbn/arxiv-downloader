# 2312.12742.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2312.12742.pdf
# Kích thước tệp: 1332635 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cached Transformers: Cải thiện Transformers với Bộ nhớ Cache Khả vi
Zhaoyang Zhang1, Wenqi Shao1, Yixiao Ge3, Xiaogang Wang1, Jinwei Gu1, Ping Luo2
1Đại học Trung văn Hồng Kông2Đại học Hồng Kông3Tencent Inc
Tóm tắt
Nghiên cứu này giới thiệu một mô hình Transformer mới được gọi là
Cached Transformer, sử dụng cơ chế attention Gated Recurrent Cached
(GRC) để mở rộng cơ chế self-attention với bộ nhớ cache khả vi của các token. Attention GRC cho phép
chú ý đến cả các token trong quá khứ và hiện tại, tăng
trường tiếp nhận của attention và cho phép khám phá các phụ thuộc tầm xa. Bằng cách sử dụng một đơn vị gating hồi quy để
liên tục cập nhật cache, mô hình của chúng tôi đạt được những tiến bộ đáng kể trong sáu nhiệm vụ ngôn ngữ và thị giác, bao gồm mô hình ngôn ngữ, dịch máy, ListOPs, phân loại hình ảnh, phát hiện đối tượng và phân đoạn thể hiện. Hơn nữa, phương pháp của chúng tôi vượt trội hơn các kỹ thuật dựa trên bộ nhớ trước đó trong các nhiệm vụ như mô hình ngôn ngữ và hiển thị khả năng được áp dụng cho một phạm vi tình huống rộng hơn.

Giới thiệu
Thiết kế của Transformer (Vaswani et al. 2017), một mô hình sâu
chồng các lớp self-attention và feed-forward, đã đạt được tiến bộ đáng kể trong nhiều nhiệm vụ khác nhau. So với các mô hình sâu truyền thống, một đặc điểm chính của Transformer là cơ chế self-attention, cho phép trường tiếp nhận toàn cục và cho phép mỗi token truy cập tất cả các token khác trong một batch dữ liệu, cung cấp một sơ đồ linh hoạt để nắm bắt biểu diễn ngữ cảnh (Vaswani et al. 2017; Dosovitskiy et al. 2021; Carion et al. 2020). Tuy nhiên, mô hình này có độ phức tạp bình phương theo độ dài chuỗi, do đó không phù hợp để mô hình hóa các phụ thuộc dài hạn. Trong nghiên cứu này, chúng tôi nhằm mở rộng các mô hình transformer thông thường bằng cách sử dụng attention với biểu diễn token dài hạn trong bộ nhớ cache, cho phép trường tiếp nhận lớn hơn và dài hơn với chi phí tính toán bổ sung tối thiểu.

Việc nắm bắt các mối quan hệ tầm xa giữa các token và mẫu là rất quan trọng đối với nhiều nhiệm vụ khác nhau vì một số lý do. (i) Trong dữ liệu tuần tự như câu ngôn ngữ, có thể tồn tại các phụ thuộc giữa các token cách xa nhau. Ví dụ, một sự kiện hoặc nhân vật có thể được tham chiếu thỉnh thoảng qua nhiều đoạn văn trong một bài viết. Việc không nắm bắt được các phụ thuộc như vậy có thể dẫn đến hiệu suất kém trong các nhiệm vụ xử lý ngôn ngữ tự nhiên. (ii) Việc mô hình hóa các mối quan hệ giữa các mẫu cũng có thể hữu ích cho dữ liệu không tuần tự như hình ảnh. Ví dụ, việc kết hợp

Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

5 10 15 20 25
Công suất Mô hình (G FLOPs)7678808284 Độ chính xác 
ViT
PVT
Cached PVT (của chúng tôi)
PVTv2
Cached PVTv2 (của chúng tôi)
Swin
CvTHình 1: Đường cong Hiệu suất-Độ phức tạp: So sánh độ chính xác Top-1(%) trên ImageNet theo công suất mô hình(G FLOPs) sử dụng vision transformers (Đường cong hiệu quả tham số). Đường cong của các mô hình cached của chúng tôi luôn ở trên các baseline tương ứng (PVT và PVTv2), cho thấy hiệu quả của các mô hình GRC-cached xét cả độ phức tạp và độ chính xác.

một module bộ nhớ lưu trữ các biểu diễn đặc trưng nguyên mẫu có thể cho phép học đặc trưng bất biến thể hiện, dẫn đến hiệu suất được cải thiện trong các nhiệm vụ thị giác (Long et al. 2022; Deng et al. 2022). Hơn nữa, các nghiên cứu khác (Wang et al. 2020b; Zhong et al. 2019) đã chứng minh rằng việc sử dụng bộ nhớ cross-batch để lưu trữ các embedding trước đó có thể có lợi cho việc học biểu diễn thị giác. (iii) Attention tầm xa hơn cũng đã được chứng minh là tăng cường khả năng học biểu diễn của các mô hình, như được thể hiện trong các nghiên cứu như (Dai et al. 2019; Wu et al. 2022; Tay et al. 2021b).

Tuy nhiên, việc mô hình hóa phụ thuộc dài hơn làm cho tính toán trở nên đắt đỏ hơn. Ví dụ, Transformer vanilla có độ phức tạp tính toán O(T2) trong mỗi module attention khi xử lý một chuỗi token có độ dài T. Mặc dù một số nghiên cứu áp dụng các giải pháp thay thế hiệu quả, chẳng hạn như phân tích thành phần hạng thấp (Wang et al. 2020a; Zhu et al. 2021), sparse hóa dựa trên khối (Zaheer et al. 2020), và băm nhạy cảm cục bộ (Kitaev, Kaiser, and Levskaya 2020), chúng vẫn có độ phức tạp tuyến tính với độ dài token (O(T)) và do đó không thể nắm bắt hiệu quả phụ thuộc tầm xa thưa thớt. Một hướng nghiên cứu khác (Wu et al. 2022) giảm độ phức tạp của module attention bằng cách chọn các cặp token top-k từ bộ nhớ cache cho các token hiện tại, nhưng chi phí duy trì một cache token khổng lồ cho tất cả các lớp vẫn đáng kể. Do đó, việc phát triển các cơ chế hiệu quả và hiệu suất để nắm bắt các phụ thuộc tầm xa vẫn là một lĩnh vực nghiên cứu tích cực.

Để giải quyết những vấn đề này, chúng tôi đề xuất một họ mô hình Transformer mới được gọi là Cached Transformer, có Gated Recurrent Cache (GRC) cho phép Transformers truy cập kiến thức lịch sử, như được minh họa trong Hình 2. GRC được triển khai như một meta-learner nén biểu diễn lịch sử thành các vector embedding và cập nhật chúng một cách thích ứng với cơ chế gating, tránh nhu cầu về một bộ nhớ cache lớn. GRC cập nhật biểu diễn quá khứ với một reset gate ức chế các cache lịch sử và một update gate tiếp tục cập nhật các cache bị ức chế bằng cách sử dụng các chuỗi token hiện tại. Thiết kế này cho phép GRC truy cập kiến thức đã thấy trước đó một cách hiệu quả về mặt tính toán. Dựa trên GRC, chúng tôi triển khai một cơ chế attention bán-cached chú ý đến cả các token tiềm ẩn và hiện tại.

Chúng tôi đề xuất Cached Transformer với Gated Recurrent Cache (GRC) và đưa ra những đóng góp sau, làm cho nó trở nên hấp dẫn hơn các nghiên cứu trước đó ở một số khía cạnh.

• GRC được xây dựng trên một công thức khả vi tổng quát và tương thích với nhiều sơ đồ attention, mạng Transformer và nhiệm vụ khác nhau. Chúng tôi chứng minh rằng GRC có thể dễ dàng được cắm vào các biến thể Transformer đa dạng như Transformer-XL (Dai et al. 2019), ViT (Dosovitskiy et al. 2021), PVT (Wang et al. 2021, 2022), Swin (Liu et al. 2021) Bigbird (Zaheer et al. 2020), và Reformer (Kitaev, Kaiser, and Levskaya 2020).

• GRC có thể cache tất cả các biểu diễn có độ dài tùy ý một cách hồi quy, độc lập với độ dài chuỗi, trong khi các phương pháp dựa trên cache hiện có chỉ có thể nắm bắt các token gần đây (Rae et al. 2019; Dai et al. 2019) hoặc yêu cầu tìm kiếm KNN ở mỗi bước (Wu et al. 2022).

• Ngoài hiệu quả, GRC vượt trội hơn các phương pháp dựa trên bộ nhớ trước đó (Dai et al. 2019; Burtsev et al. 2020; Bulatov, Kuratov, and Burtsev 2022) với một biên độ lớn trên cả nhiệm vụ thị giác (Bảng 2) và ngôn ngữ (Bảng 5).

• GRC mang lại những cải thiện nhất quán không chỉ trong dữ liệu tuần tự như văn bản mà còn trong ngữ cảnh không gian như phân loại hình ảnh (Bảng 1) và phát hiện đối tượng (Bảng 3). Theo hiểu biết của chúng tôi, các nghiên cứu hiện có về Vision Transformers chủ yếu tập trung vào việc học các token trong mẫu, trong khi GRC là nỗ lực đầu tiên để mô hình hóa các mối quan hệ giữa các mẫu bằng cách chú ý đến các token giữa các mẫu, chẳng hạn như các token từ các hình ảnh độc lập khác nhau.

• Chúng tôi quan sát thấy rằng các mô hình với GRC có thể chú ý nhiều hơn đến cache so với self-attention thông thường. Chúng tôi điều tra hành vi này trong phân loại hình ảnh và thấy rằng GRC có thể tách các đặc trưng thành hai phần, chú ý đến cache mang lại các đặc trưng bất biến thể hiện, cũng như chú ý đến bản thân, mang lại các đặc trưng đặc trưng thể hiện (Xem trong Hình 4). Hành vi này tương tự như của một nguyên mẫu vector (Caron et al. 2020), cho phép chính quy hóa cross-sample để tránh overfitting.

Các thí nghiệm rộng rãi cho thấy rằng Cached Transformer với GRC đạt được kết quả đầy hứa hẹn trên nhiều backbone Transformer thị giác và ngôn ngữ khác nhau. (i) Ngôn ngữ: Trong benchmark IWSLT14 De-En cho dịch máy, PreNormed Transformer+GRC đạt 36.0 BLEU, vượt trội hơn các baseline 0.5. Trong benchmark long-range-arena đầy thử thách (Tay et al. 2021a), GRC cải thiện các phương pháp hiện đại với các loại attention khác nhau bao gồm Reformer (Kitaev, Kaiser, and Levskaya 2020), Bigbird (Zaheer et al. 2020), và Transformer thông thường (Vaswani et al. 2017) một cách nhất quán lên đến 1.2% độ chính xác. (ii) Thị giác: Đối với phân loại hình ảnh trên ImageNet (Krizhevsky, Sutskever, and Hinton 2012), chúng tôi cắm GRC vào các vision transformer gần đây với các quy mô khác nhau, như ViT (Dosovitskiy et al. 2021), PVT (Wang et al. 2021), PVTv2 (Wang et al. 2022), Swin (Liu et al. 2021), và đạt được tăng độ chính xác lên đến 3.3%. Như được hiển thị trong Hình 1, mô hình cached của chúng tôi với backbone PVTv2 đạt hiệu suất vượt trội xét cả độ phức tạp mô hình và độ chính xác. Chúng tôi tiếp tục đánh giá GRC trên tập dữ liệu COCO (Lin et al. 2014) cho phát hiện đối tượng và phân đoạn thể hiện, nơi PVT+GRC có thể mang lại cải thiện box AP hơn 4.0.

Các nghiên cứu liên quan
Cached Language Models. Các mô hình cache có hiệu quả trong mô hình hóa tầm xa, và được giới thiệu lần đầu bởi (Kupiec 1989; Kuhn and De Mori 1990) cho nhận dạng giọng nói. Nói chung, một mô hình cache lưu trữ các biểu diễn của quá khứ, thường là unigrams hoặc các cặp key-value cho tính toán tương lai. Transformer-XL (Dai et al. 2019) tiếp tục áp dụng kỹ thuật này cho transformers, nơi cache lưu trữ các cặp key-value trước đó trong attention từ các bước training trước đó. Nhiều phương pháp dựa trên bộ nhớ được khám phá theo sau Transformer-XL: Ví dụ, MT (Burtsev et al. 2020) và RMT (Bulatov, Kuratov, and Burtsev 2022) sử dụng các token bộ nhớ bổ sung để lưu trữ thông tin cục bộ và toàn cục cho các phân đoạn đầu vào khác nhau. (Rae et al. 2019) nén các token trước khi chúng được lưu trong cache để giảm bộ nhớ và tính toán. Tuy nhiên, các phương pháp này thường sử dụng cache theo cách có độ dài cố định và first-in-first-out (FIFO), điều này hạn chế số lượng token có thể được ghi nhớ trong chuỗi. Ngược lại, Cached Transformers dựa trên GRC mà chúng tôi đề xuất học cách xây dựng cache một cách thích ứng với độ phức tạp độc lập với phạm vi attention.

Vision Transformers. Vision transformers và các biến thể của chúng gần đây đã đạt được thành công đáng kể trong nhiều nhiệm vụ thị giác khác nhau. Mô hình Vision Transformer (ViT) ban đầu (Dosovitskiy et al. 2021) là mô hình đầu tiên chia hình ảnh thành các chuỗi patch và đưa chúng vào các encoder transformer. Mặc dù tạo ra kết quả cạnh tranh so với mạng nơ-ron tích chập (CNN), ViTs yêu cầu pretraining tốn kém trên các tập dữ liệu quy mô lớn như JFT-300M (Sun et al. 2017). Để giải quyết vấn đề này, một số nghiên cứu (Shao et al. 2022) cho rằng điều này do thiếu inductive bias trong ViTs và đề xuất giới thiệu các prior tích chập để mã hóa inductive bias như ngữ cảnh cục bộ. Ví dụ, DeiT (Touvron et al. 2021b) sử dụng một teacher tích chập để chưng cất kiến thức cho transformers, Swin-Transformer (Liu et al. 2021) thực hiện attention trong các cửa sổ trượt, và ConViT (d'Ascoli et al. 2021) sử dụng một module tích chập "mềm" để mã hóa tính cục bộ. Tuy nhiên, các phương pháp hiện có chủ yếu tập trung vào các token trong mẫu, trong khi GRC mà chúng tôi đề xuất tăng cường vision transformers bằng cách học các đặc trưng bất biến thể hiện thông qua việc chú ý đến các token giữa các mẫu. Điều này cho phép các transformer dựa trên GRC nắm bắt thông tin ngữ cảnh phong phú hơn và đạt được hiệu suất tốt hơn trong các nhiệm vụ thị giác.

Phương pháp luận
Trong phần này, chúng tôi trước tiên xem xét lại các mô hình transformer ngôn ngữ và thị giác vanilla, sau đó giới thiệu việc triển khai Cached Transformers với Gated Recurrent Cache (GRC).

Vanilla Transformer
Chúng tôi bắt đầu với một đánh giá ngắn gọn về kiến trúc transformer tiêu chuẩn. Mô hình transformer (Vaswani et al. 2017) được xây dựng bằng cách chồng các khối multi-head self-attention và các lớp feed-forward thường là một biến đổi tuyến tính hai lớp với activation. Mỗi khối transformer được cung cấp với T×D token đầu vào, trong đó T là số lượng token và D đại diện cho kích thước của token embedding.

Cơ chế Self-attention. Như được hiển thị trong Hình 2, module self-attention trước tiên chiếu mỗi đầu vào X thành Q (query), K (key), và V (value) bằng cách sử dụng các biến đổi tuyến tính. Thông thường, self-attention được thực hiện theo cách multi-head nơi đầu vào sẽ được chia thành nhiều head để tính toán song song. Đầu ra của attention head h có thể được viết như:

oh_self = softmax(QhKTh/√D/H)Vh, (1)

trong đó oh_self là đầu ra của head h của self-attention và H là số lượng heads. Đầu ra từ các heads sẽ được nối và sau đó được đưa vào các biến đổi tuyến tính khác với normalization và residual connections.

Hạn chế. Như được hiển thị trong Eqn.(1), cơ chế self-attention vanilla được sử dụng trong Transformers rất nhạy cảm với độ dài chuỗi, với độ phức tạp tính toán O(T2) đối với độ dài chuỗi T. Điều này có nghĩa là chi phí tính toán tăng nhanh khi độ dài chuỗi tăng, điều này hạn chế khả năng của mô hình trong việc nắm bắt các mối quan hệ dài hạn trong dữ liệu. Kết quả là, Transformers vanilla chỉ có thể mô hình hóa các chuỗi token tương đối ngắn trong các nhiệm vụ ngôn ngữ, và nó cũng làm cho việc phát triển các module bộ nhớ cross-task (Wang et al. 2020b; Zhong et al. 2019) theo cách dựa trên attention trở nên thách thức đối với các nhiệm vụ thị giác. Hướng tới vấn đề này, chúng tôi giới thiệu Cached Transformers được đề xuất, cung cấp một mô hình linh hoạt hơn để nắm bắt các phụ thuộc dài hạn, dẫn đến những cải thiện nhất quán cho cả nhiệm vụ thị giác và ngôn ngữ.

Cached Transformer
Để mở rộng trường tiếp nhận của cả language và vision transformers, trong phần này chúng tôi sẽ giới thiệu các triển khai của Cached Transformers, duy trì một cache liên tục được gọi là Gated Recurrent Cache (GRC) để hỗ trợ việc học biểu diễn dài hạn hiệu quả. Ý tưởng cốt lõi là giữ token embedding như cache có thể ghi lại động các mẫu lịch sử theo tầm quan trọng của chúng. Cached Transformer sau đó sẽ có được khả năng bổ sung để mã hóa cả thông tin hiện tại và tích lũy bằng cách chú ý đến tập hợp các cache C và đầu vào X. Một sơ đồ attention như vậy được mô tả là GRC-Attention, và các phần sau đây trình bày chi tiết hơn.

Triển khai tổng quát. Cached Transformers được đề xuất cho phép chú ý đến cache trên các kiến trúc multi-layer tùy ý chấp nhận đầu vào tuần tự. Thông thường, các mô hình Cached Transformer có thể được tạo ra bằng cách thay thế các khối self-attention của chúng bằng GRC-Attention được đề xuất. Hình 3 (b) đưa ra minh họa tổng thể về cách GRC-Attention được thực hiện.

Xét chuỗi đầu vào Xt∈RB×T×D, trong đó B là kích thước batch và t biểu thị các bước training, GRC-attention chú ý đến cả memory cache và các token hiện tại. Chúng tôi công thức hóa GRC-attention bằng

Oh = σ(λh) * oh_mem + (1−σ(λh)) * oh_self, (2)

trong đó Oh và oh_mem là đầu ra của GRC-attention và Cached attention (tức là, attention trên memory cache) trong head h, tương ứng. oh_self là đầu ra của self-attention trong Eqn.(1). Hơn nữa, trong Eqn.(2), σ(·) là hàm sigmoid và λh là tỷ lệ có thể học theo head để cân bằng giữa self-attention và Cached attention1.

Để xây dựng bộ ba key, query và value cho Cached attention, chúng tôi chọn một phần của Xt làm đầu vào X̄t∈ RB×T×Dm, được tạo ra bằng cách cắt Xt theo chiều channel. Lưu ý rằng Dm = rD/2 chỉ ra các channel được sử dụng để ghi nhớ embedding của các token quá khứ, trong đó r là tỷ lệ caching. Với X̄t, cache tích lũy Ct−1 sau đó sẽ được cập nhật thành Ct theo các quy tắc cập nhật GRC như được hiển thị trong Hình 3. Chúng tôi mô tả việc xây dựng GRC trong Sec chi tiết. Cached attention sau đó có thể được thực hiện

1Tất cả λh được khởi tạo là 0.
2Trong hầu hết các trường hợp, chúng tôi áp dụng Dm = D/2 để giảm độ phức tạp của Cached attention, có nghĩa là chúng tôi chọn một nửa đầu vào để cập nhật cache


--- TRANG 2 ---
từ một memory cache cho các token hiện tại, nhưng chi phí duy trì một cache token khổng lồ cho tất cả các lớp vẫn đáng kể. Do đó, việc phát triển các cơ chế hiệu quả và hiệu suất để nắm bắt các phụ thuộc tầm xa vẫn là một lĩnh vực nghiên cứu tích cực.

Để giải quyết những vấn đề này, chúng tôi đề xuất một họ mô hình Transformer mới được gọi là Cached Transformer, có Gated Recurrent Cache (GRC) cho phép Transformers truy cập kiến thức lịch sử, như được minh họa trong Hình 2. GRC được triển khai như một meta-learner nén biểu diễn lịch sử thành các vector embedding và cập nhật chúng một cách thích ứng với cơ chế gating, tránh nhu cầu về một bộ nhớ cache lớn. GRC cập nhật biểu diễn quá khứ với một reset gate ức chế các cache lịch sử và một update gate tiếp tục cập nhật các cache bị ức chế bằng cách sử dụng các chuỗi token hiện tại. Thiết kế này cho phép GRC truy cập kiến thức đã thấy trước đó một cách hiệu quả về mặt tính toán. Dựa trên GRC, chúng tôi triển khai một cơ chế attention bán-cached chú ý đến cả các token tiềm ẩn và hiện tại.

Chúng tôi đề xuất Cached Transformer với Gated Recurrent Cache (GRC) và đưa ra những đóng góp sau, làm cho nó trở nên hấp dẫn hơn các nghiên cứu trước đó ở một số khía cạnh.

• GRC được xây dựng trên một công thức khả vi tổng quát và tương thích với nhiều sơ đồ attention, mạng Transformer và nhiệm vụ khác nhau. Chúng tôi chứng minh rằng GRC có thể dễ dàng được cắm vào các biến thể Transformer đa dạng như Transformer-XL (Dai et al. 2019), ViT (Dosovitskiy et al. 2021), PVT (Wang et al. 2021, 2022), Swin (Liu et al. 2021) Bigbird (Zaheer et al. 2020), và Reformer (Kitaev, Kaiser, and Levskaya 2020).

• GRC có thể cache tất cả các biểu diễn có độ dài tùy ý một cách hồi quy, độc lập với độ dài chuỗi, trong khi các phương pháp dựa trên cache hiện có chỉ có thể nắm bắt các token gần đây (Rae et al. 2019; Dai et al. 2019) hoặc yêu cầu tìm kiếm KNN ở mỗi bước (Wu et al. 2022).

• Ngoài hiệu quả, GRC vượt trội hơn các phương pháp dựa trên bộ nhớ trước đó (Dai et al. 2019; Burtsev et al. 2020; Bulatov, Kuratov, and Burtsev 2022) với một biên độ lớn trên cả nhiệm vụ thị giác (Bảng 2) và ngôn ngữ (Bảng 5).

• GRC mang lại những cải thiện nhất quán không chỉ trong dữ liệu tuần tự như văn bản mà còn trong ngữ cảnh không gian như phân loại hình ảnh (Bảng 1) và phát hiện đối tượng (Bảng 3). Theo hiểu biết của chúng tôi, các nghiên cứu hiện có về Vision Transformers chủ yếu tập trung vào việc học các token trong mẫu, trong khi GRC là nỗ lực đầu tiên để mô hình hóa các mối quan hệ giữa các mẫu bằng cách chú ý đến các token giữa các mẫu, chẳng hạn như các token từ các hình ảnh độc lập khác nhau.

• Chúng tôi quan sát thấy rằng các mô hình với GRC có thể chú ý nhiều hơn đến cache so với self-attention thông thường. Chúng tôi điều tra hành vi này trong phân loại hình ảnh và thấy rằng GRC có thể tách các đặc trưng thành hai phần, chú ý đến cache mang lại các đặc trưng bất biến thể hiện, cũng như chú ý đến bản thân, mang lại các đặc trưng đặc trưng thể hiện (Xem trong Hình 4). Hành vi này tương tự như của một nguyên mẫu vector (Caron et al. 2020), cho phép chính quy hóa cross-sample để tránh overfitting.

Các thí nghiệm rộng rãi cho thấy rằng Cached Transformer với GRC đạt được kết quả đầy hứa hẹn trên nhiều backbone Transformer thị giác và ngôn ngữ khác nhau. (i) Ngôn ngữ: Trong benchmark IWSLT14 De-En cho dịch máy, PreNormed Transformer+GRC đạt 36.0 BLEU, vượt trội hơn các baseline 0.5. Trong benchmark long-range-arena đầy thử thách (Tay et al. 2021a), GRC cải thiện các phương pháp hiện đại với các loại attention khác nhau bao gồm Reformer (Kitaev, Kaiser, and Levskaya 2020), Bigbird (Zaheer et al. 2020), và Transformer thông thường (Vaswani et al. 2017) một cách nhất quán lên đến 1.2% độ chính xác. (ii) Thị giác: Đối với phân loại hình ảnh trên ImageNet (Krizhevsky, Sutskever, and Hinton 2012), chúng tôi cắm GRC vào các vision transformer gần đây với các quy mô khác nhau, như ViT (Dosovitskiy et al. 2021), PVT (Wang et al. 2021), PVTv2 (Wang et al. 2022), Swin (Liu et al. 2021), và đạt được tăng độ chính xác lên đến 3.3%. Như được hiển thị trong Hình 1, mô hình cached của chúng tôi với backbone PVTv2 đạt hiệu suất vượt trội xét cả độ phức tạp mô hình và độ chính xác. Chúng tôi tiếp tục đánh giá GRC trên tập dữ liệu COCO (Lin et al. 2014) cho phát hiện đối tượng và phân đoạn thể hiện, nơi PVT+GRC có thể mang lại cải thiện box AP hơn 4.0.

Các nghiên cứu liên quan
Cached Language Models. Các mô hình cache có hiệu quả trong mô hình hóa tầm xa, và được giới thiệu lần đầu bởi (Kupiec 1989; Kuhn and De Mori 1990) cho nhận dạng giọng nói. Nói chung, một mô hình cache lưu trữ các biểu diễn của quá khứ, thường là unigrams hoặc các cặp key-value cho tính toán tương lai. Transformer-XL (Dai et al. 2019) tiếp tục áp dụng kỹ thuật này cho transformers, nơi cache lưu trữ các cặp key-value trước đó trong attention từ các bước training trước đó. Nhiều phương pháp dựa trên bộ nhớ được khám phá theo sau Transformer-XL: Ví dụ, MT (Burtsev et al. 2020) và RMT (Bulatov, Kuratov, and Burtsev 2022) sử dụng các token bộ nhớ bổ sung để lưu trữ thông tin cục bộ và toàn cục cho các phân đoạn đầu vào khác nhau. (Rae et al. 2019) nén các token trước khi chúng được lưu trong cache để giảm bộ nhớ và tính toán. Tuy nhiên, các phương pháp này thường sử dụng cache theo cách có độ dài cố định và first-in-first-out (FIFO), điều này hạn chế số lượng token có thể được ghi nhớ trong chuỗi. Ngược lại, Cached Transformers dựa trên GRC mà chúng tôi đề xuất học cách xây dựng cache một cách thích ứng với độ phức tạp độc lập với phạm vi attention.

Vision Transformers. Vision transformers và các biến thể của chúng gần đây đã đạt được thành công đáng kể trong nhiều nhiệm vụ thị giác khác nhau. Mô hình Vision Transformer (ViT) ban đầu (Dosovitskiy et al. 2021) là mô hình đầu tiên chia hình ảnh thành các chuỗi patch và đưa chúng vào các encoder transformer. Mặc dù tạo ra kết quả cạnh tranh so với mạng nơ-ron tích chập (CNN), ViTs yêu cầu pretraining tốn kém trên các tập dữ liệu quy mô lớn như JFT-300M (Sun et al. 2017). Để giải quyết vấn đề này, một số nghiên cứu (Shao et al. 2022) cho rằng điều này do thiếu inductive bias trong ViTs và đề xuất giới thiệu các prior tích chập để mã hóa inductive bias như ngữ cảnh cục bộ. Ví dụ, DeiT (Touvron et al. 2021b) sử dụng một teacher tích chập để chưng cất kiến thức cho transformers, Swin-Transformer (Liu et al. 2021) thực hiện attention trong các cửa sổ trượt, và ConViT (d'Ascoli

--- TRANG 3 ---
AttentionCập nhật GRC
Cached AttentionCập nhật GRC
............
Attention
Self Attention......Hình 2: So sánh vanilla self-attention và cached attentions ở giai đoạn training. Self-attention chỉ chú ý đến chính token đó (Xt). Trong khi đó trong cached attention, đầu ra ở bước training t (ký hiệu bởi Yt) được tạo ra bằng cách chú ý đến một Gated Recurrent Cache (GRC, tức là, Ct được tạo ra từ các token lịch sử X0 đến Xt), và token hiện tại (Xt).

et al. 2021) sử dụng một module tích chập "mềm" để mã hóa tính cục bộ. Tuy nhiên, các phương pháp hiện có chủ yếu tập trung vào các token trong mẫu, trong khi GRC mà chúng tôi đề xuất tăng cường vision transformers bằng cách học các đặc trưng bất biến thể hiện thông qua việc chú ý đến các token giữa các mẫu. Điều này cho phép các transformer dựa trên GRC nắm bắt thông tin ngữ cảnh phong phú hơn và đạt được hiệu suất tốt hơn trong các nhiệm vụ thị giác.

Phương pháp luận
Trong phần này, chúng tôi trước tiên xem xét lại các mô hình transformer ngôn ngữ và thị giác vanilla, sau đó giới thiệu việc triển khai Cached Transformers với Gated Recurrent Cache (GRC).

Vanilla Transformer
Chúng tôi bắt đầu với một đánh giá ngắn gọn về kiến trúc transformer tiêu chuẩn. Mô hình transformer (Vaswani et al. 2017) được xây dựng bằng cách chồng các khối multi-head self-attention và các lớp feed-forward thường là một biến đổi tuyến tính hai lớp với activation. Mỗi khối transformer được cung cấp với T×D token đầu vào, trong đó T là số lượng token và D đại diện cho kích thước của token embedding.

Cơ chế Self-attention. Như được hiển thị trong Hình 2, module self-attention trước tiên chiếu mỗi đầu vào X thành Q (query), K (key), và V (value) bằng cách sử dụng các biến đổi tuyến tính. Thông thường, self-attention được thực hiện theo cách multi-head nơi đầu vào sẽ được chia thành nhiều head để tính toán song song. Đầu ra của attention head h có thể được viết như:

oh_self = softmax(QhKTh/√D/H)Vh, (1)

trong đó oh_self là đầu ra của head h của self-attention và H là số lượng heads. Đầu ra từ các heads sẽ được nối và sau đó được đưa vào các biến đổi tuyến tính khác với normalization và residual connections.

Hạn chế. Như được hiển thị trong Eqn.(1), cơ chế self-attention vanilla được sử dụng trong Transformers rất nhạy cảm với độ dài chuỗi, với độ phức tạp tính toán O(T2) đối với độ dài chuỗi T. Điều này có nghĩa là chi phí tính toán tăng nhanh khi độ dài chuỗi tăng, điều này hạn chế khả năng của mô hình trong việc nắm bắt các mối quan hệ dài hạn trong dữ liệu. Kết quả là, Transformers vanilla chỉ có thể mô hình hóa các chuỗi token tương đối ngắn trong các nhiệm vụ ngôn ngữ, và nó cũng làm cho việc phát triển các module bộ nhớ cross-task (Wang et al. 2020b; Zhong et al. 2019) theo cách dựa trên attention trở nên thách thức đối với các nhiệm vụ thị giác. Hướng tới vấn đề này, chúng tôi giới thiệu Cached Transformers được đề xuất, cung cấp một mô hình linh hoạt hơn để nắm bắt các phụ thuộc dài hạn, dẫn đến những cải thiện nhất quán cho cả nhiệm vụ thị giác và ngôn ngữ.

Cached Transformer
Để mở rộng trường tiếp nhận của cả language và vision transformers, trong phần này chúng tôi sẽ giới thiệu các triển khai của Cached Transformers, duy trì một cache liên tục được gọi là Gated Recurrent Cache (GRC) để hỗ trợ việc học biểu diễn dài hạn hiệu quả. Ý tưởng cốt lõi là giữ token embedding như cache có thể ghi lại động các mẫu lịch sử theo tầm quan trọng của chúng. Cached Transformer sau đó sẽ có được khả năng bổ sung để mã hóa cả thông tin hiện tại và tích lũy bằng cách chú ý đến tập hợp các cache C và đầu vào X. Một sơ đồ attention như vậy được mô tả là GRC-Attention, và các phần sau đây trình bày chi tiết hơn.

Triển khai tổng quát. Cached Transformers được đề xuất cho phép chú ý đến cache trên các kiến trúc multi-layer tùy ý chấp nhận đầu vào tuần tự. Thông thường, các mô hình Cached Transformer có thể được tạo ra bằng cách thay thế các khối self-attention của chúng bằng GRC-Attention được đề xuất. Hình 3 (b) đưa ra minh họa tổng thể về cách GRC-Attention được thực hiện.

Xét chuỗi đầu vào Xt∈RB×T×D, trong đó B là kích thước batch và t biểu thị các bước training, GRC-attention chú ý đến cả memory cache và các token hiện tại. Chúng tôi công thức hóa GRC-attention bằng

Oh = σ(λh) * oh_mem + (1−σ(λh)) * oh_self, (2)

trong đó Oh và oh_mem là đầu ra của GRC-attention và Cached attention (tức là, attention trên memory cache) trong head h, tương ứng. oh_self là đầu ra của self-attention trong Eqn.(1). Hơn nữa, trong Eqn.(2), σ(·) là hàm sigmoid và λh là tỷ lệ có thể học theo head để cân bằng giữa self-attention và Cached attention1.

Để xây dựng bộ ba key, query và value cho Cached attention, chúng tôi chọn một phần của Xt làm đầu vào X̄t∈ RB×T×Dm, được tạo ra bằng cách cắt Xt theo chiều channel. Lưu ý rằng Dm = rD/2 chỉ ra các channel được sử dụng để ghi nhớ embedding của các token quá khứ, trong đó r là tỷ lệ caching. Với X̄t, cache tích lũy Ct−1 sau đó sẽ được cập nhật thành Ct theo các quy tắc cập nhật GRC như được hiển thị trong Hình 3. Chúng tôi mô tả việc xây dựng GRC trong Sec chi tiết. Cached attention sau đó có thể được thực hiện

1Tất cả λh được khởi tạo là 0.
2Trong hầu hết các trường hợp, chúng tôi áp dụng Dm = D/2 để giảm độ phức tạp của Cached attention, có nghĩa là chúng tôi chọn một nửa đầu vào để cập nhật cache

--- TRANG 4 ---
Tokens 
Caches  FC
Updated cachesSelf-Attention
GRC
Updates
Cached Attention + Outputs
(a) Cập nhật GRC (b) GRC-AttentionReset caches
: reset gates : update gates Hình 3: Minh họa về GRC-Attention được đề xuất trong Cached Transformers. (a) Chi tiết về quá trình cập nhật Gated Recurrent Cache. Cache được cập nhật Ct được tạo ra dựa trên các token hiện tại X̄t và cache của bước cuối Ct−1. Reset gates gr reset cache trước đó Ct−1 thành reset cache C̃t, và update gates gu kiểm soát cường độ cập nhật. (b) Pipeline tổng thể của GRC-Attention. Các đầu vào sẽ chú ý đến cache và chính chúng tương ứng, và các đầu ra được công thức hóa như phép nội suy của hai kết quả attention.

bằng cách sử dụng X̄t làm queries và Ct làm keys và values, được viết như:

oh_mem = softmax(Q̄hK̄Th/√Dm/H)V̄h, (3)

trong đó Q̄h, K̄h và V̄h được thu được bằng các phép chiếu tuyến tính của head thứ h của X̄t, Ct và Ct tương ứng.

Tổng quát hóa. Lưu ý rằng trong khi chúng tôi thường công thức hóa Cached Transformer như một mô hình dựa trên self-attention, nó cũng có thể là một biến thể transformer tùy ý. Nói cách khác, cơ chế attention được sử dụng để có được oh_self và oh_mem trong Eqn.(2) có thể được thay thế bằng bất kỳ hàm giống attention nào khác, chẳng hạn như sparse attentions (Zaheer et al. 2020) hoặc local hashing (Kitaev, Kaiser, and Levskaya 2020). Các thí nghiệm tiếp theo sẽ cung cấp xác nhận về Cached Transformers trên một số biến thể transformer.

Cập nhật Gated Recurrent Cache
Phần này mô tả việc công thức hóa và cập nhật Gated Recurrent Cache (GRC) được đề xuất.

Khởi tạo Cache. GRC được đặc trưng là các vector có độ dài cố định Ct∈RTm×Dm. Không giống như các nghiên cứu trước đó công thức hóa cache thành các token hoặc từ trực tiếp (Tu et al. 2018; Dai et al. 2019), GRC nhúng các token lịch sử một cách ngầm định. Bằng cách học nhúng các mẫu có độ dài tùy ý vào Ct, GRC cho phép duyệt qua các cache trong thời gian không đổi độc lập với số lượng token được ghi nhớ. Cache C0 sẽ được khởi tạo thành các vector zero có độ dài Tm trước khi training, và sau đó được cập nhật như được mô tả trong Hình 3(a).

Cơ chế Gating. Được truyền cảm hứng bởi gated RNNs (Cho et al. 2014), chúng tôi áp dụng cơ chế gating để cho phép GRC nắm bắt động các phụ thuộc ở các thang thời gian khác nhau. Cụ thể, quá trình cập nhật Ct được lọc bởi update gates gu và reset gates gr. Xét việc cập nhật GRC ở bước thời gian t, chúng tôi trước tiên tính toán các gates gu và gr:

gu = σ(Wu[X̄t, Ct−1]) và gr = σ(Wr[X̄t, Ct−1]), (4)

trong đó σ biểu thị hàm sigmoid và [·,·] nối các token theo chiều channel. Để nối hợp lệ, X̄t được nội suy thành một token Tm-by-Dm. Cache được cập nhật Ct được công thức hóa bằng một phép nội suy tuyến tính như được cho bởi:

Ct = (1−gu)Ct−1 + gu C̃t và C̃t = Wc[X̄t, gr ⊙ Ct−1] (5)

trong đó ⊙ là phép nhân theo từng phần tử. Trong quá trình trên, update gates gu quyết định mức độ mà mẫu hiện tại X̄t cập nhật cache và reset gates gr ức chế cache tích lũy để quên các thành phần không quan trọng. Lưu ý rằng hình dạng của Ct được tạo ra là B×Tm×Dm như Xt được liên quan, và do đó chúng tôi lấy trung bình qua chiều batch để phù hợp với kích thước cache.

Thí nghiệm
Phần này đánh giá rộng rãi hiệu quả của Cached Transformer được đề xuất và Gated Recurrent Cache (GRC) trong cả nhiệm vụ thị giác và ngôn ngữ, bao gồm mô hình ngôn ngữ trên WikiText-103, Long Listops của Long Range Arena (Tay et al. 2021a), dịch máy trên IWSLT14 (Cettolo et al. 2014) / IWSLT15 (Cettolo et al. 2015), phân loại hình ảnh trên ImageNet (Krizhevsky, Sutskever, and Hinton 2012), và phát hiện đối tượng và phân đoạn thể hiện trên COCO2017 (Lin et al. 2014). Ngoài ra, vì các mô hình cached mới được giới thiệu cho vision transformers, chúng tôi cũng thực hiện các thảo luận kỹ lưỡng về vai trò của các cache được đề xuất và tầm quan trọng của chúng. Tất cả các thí nghiệm được thực hiện trên GPU Tesla V100.

Phân loại Hình ảnh
Thiết lập Thí nghiệm. Chúng tôi trước tiên đánh giá các phương pháp của mình trên Imagenet-1k cho phân loại hình ảnh. Chúng tôi triển khai GRC-Attention của mình như một module pytorch tổng quát duy trì các buffer có độ dài cố định làm cache. Trong nhiệm vụ phân loại hình ảnh, chúng tôi đặt tỷ lệ cache r là 0.5 và giữ độ dài cache Tm bằng độ dài của các patch hình ảnh T. Để so sánh công bằng, chúng tôi trực tiếp thay thế các lớp self-attention trong các transformer tương ứng bằng module GRC-Attention của chúng tôi mà không thay đổi kiến trúc và siêu tham số. Để duy trì cấu trúc token không gian, chúng tôi thêm encoding vị trí vào GRC-Attention được đề xuất của chúng tôi như các vision transformer khác. Cả baseline và các đối tác cached của chúng đều được

--- TRANG 5 ---
Inputs
Self-Attention 
Cached Attention Hình 4: Trực quan hóa các đặc trưng đầu ra trung bình từ self-attention và cached attention, được thu được bằng cách đưa các hình ảnh từ tập validation ImageNet vào cached ViT-S đã được huấn luyện. Kết quả được thu được bằng cách lấy trung bình các đặc trưng qua chiều channel (và head). Cả ōself và ōmem đều được unflatten thành 14×14 để so sánh tốt hơn. Các pixel tối có nghĩa là các giá trị nhỏ.

Bảng 1: Hiệu suất của các Cached Transformers khác nhau được đánh giá trên ImageNet. "(Cached)" chỉ ra các mô hình được triển khai với GRC-Attention được đề xuất. Top-1 / Top-5 / ∆Top-1 biểu thị độ chính xác top-1 / độ chính xác top-5 / chênh lệch độ chính xác top-1 tương ứng. Các mô hình cached vượt trội hơn các baseline tương ứng của chúng một cách nhất quán.

Architecture Top-1 (%) Top-5 (%) ∆Top-1 (%)
ViT-S 79.9 95.0 -
ViT-S (Cached) 81.3 95.5 + 1.4
PVT-Tiny 75.1 92.3 -
PVT-Tiny (Cached) 78.4 94.2 + 3.3
PVT-Small 79.9 95.0 -
PVT-Small (Cached) 81.8 95.9 + 1.9
PVT-Medium 81.2 95.7 -
PVT-Medium (Cached) 83.0 96.4 + 1.8
Swin-T 81.2 95.5 -
Swin-T (Cached) 82.1 95.9 + 0.9
PVTv2-B2 82.0 95.9 -
PVTv2-B2 (Cached) 82.6 96.2 + 0.6
PVTv2-B 83.2 96.3 -
PVTv2-B3 (Cached) 83.7 96.4 + 0.5
PVTv2-B4 83.6 96.3 -
PVTv2-B4 (Cached ) 84.1 96.6 + 0.5

huấn luyện với đầu vào kích thước 224×224 sử dụng 16 GPU. Để xác thực đầy đủ cơ chế cache được đề xuất, chúng tôi đánh giá GRC-Attention trên bốn vision transformer gần đây bao gồm: ViTs (Dosovitskiy et al. 2021), PVT (Wang et al. 2021), Swin-Transformer (Liu et al. 2021) và PVT-v2 (Wang et al. 2022). Không có thêm gì, tất cả các thiết lập training cho các mô hình cached được giữ nhất quán với các baseline ban đầu bao gồm data augmentation, loại optimizer, learning rates và training epochs.

Kết quả Phân loại. Bảng 1 báo cáo hiệu suất tổng thể của cached transformers trên các baseline tương ứng. Như được hiển thị, các transformer được triển khai với GRC-Attention một cách nhất quán vượt trội hơn các đối tác không có cache bằng cách mang lại độ chính xác cao hơn đáng kể, chứng minh hiệu quả của cơ chế caching được đề xuất của chúng tôi. Ví dụ, bằng cách kích hoạt cache, PVT-Tiny có thể đạt được 78.4% độ chính xác top-1 và 94.2% độ chính xác top-5, vượt trội hơn PVT-Tiny ban đầu lần lượt 3.3% và 1.9%. Hơn nữa, ngay cả đối với backbone mạnh hơn gần đây PVTv2, cơ chế cached được đề xuất của chúng tôi vẫn có thể duy trì cải thiện top-1 >0.5.

Phân tích Độ phức tạp. Trong các thiết lập hiện tại nơi tỷ lệ cache r = 0.5, việc thay thế tất cả các lớp attention bằng GRC-Attention sẽ tiêu tốn khoảng 10%−15% FLOPs và Params bổ sung. Xét các cải thiện hiệu suất, các tính toán bổ sung là chấp nhận được (Xem trong Hình 1) và hiệu quả hơn việc tăng chiều sâu và chiều rộng của các mô hình.

Tầm quan trọng của Cached Attention. Để xác minh rằng các cải thiện hiệu suất trên chủ yếu đến từ việc chú ý đến cache, chúng tôi phân tích đóng góp của omem bằng cách trực quan hóa tỷ lệ attention có thể học σ(λh). Xin nhớ rằng trong Eq 2, đầu ra của GRC-Attention được tạo ra bằng cách nội suy đầu ra của cached attention oh_mem và self-attention oh_self theo σ(λh). Do đó, σ(λh) có thể được sử dụng để đại diện cho tầm quan trọng tương đối của oh_mem và oh_self. Hình 5 mô tả σ(λh) đã học cho mỗi head theo các lớp trong ViT-S, PVT-Tiny và PVT-Small. Như chúng ta có thể thấy, đối với hơn một nửa số lớp, σ(λh) lớn hơn 0.5, biểu thị rằng đầu ra của những lớp đó phụ thuộc rất nhiều vào cached attention. Bên cạnh đó, chúng tôi cũng nhận thấy một sự thật thú vị rằng các mô hình luôn ưa thích cached attention nhiều hơn trừ các lớp cuối cùng. Điều này khiến chúng tôi tò mò về vai trò của cached attention: đặc trưng mà các mô hình thực sự học được bằng cách chú ý đến cache là gì? Đoạn sau đây trả lời câu hỏi này.

Vai trò của Cached Attention. Chúng tôi điều tra chức năng của GRC-Attention bằng cách trực quan hóa các bản đồ đặc trưng nội bộ của chúng. Chúng tôi chọn các lớp giữa của cached ViT-S, lấy trung bình các đầu ra từ self-attention oself và cached attention (omem) qua chiều head và channel, và sau đó chuẩn hóa chúng thành [0,1]. Các kết quả tương ứng được ký hiệu là ōself và ōmem, tương ứng. Hình 4 cung cấp trực quan hóa của ōself và ōmem thu được bằng cách đưa các hình ảnh từ tập validation ImageNet vào cached ViT-S đã được huấn luyện. Vì ōself và ōmem là các chuỗi patch, chúng được unflatten thành hình dạng 14×14 để so sánh tốt hơn. Từ Hình 4 chúng ta có thể thấy, các đặc trưng được tạo ra bởi hai attention trên là bổ sung trực quan. Trong GRC-Attention, omem được tạo ra bằng cách chú ý đến cache được đề xuất (GRC) chứa các biểu diễn nén của các mẫu lịch sử, và do đó

--- TRANG 6 ---
Hình 5: Trực quan hóa σ(λh) đã học cho mỗi head theo số lớp (từ nông đến sâu) trong các mô hình khác nhau: ViT-S, PVT-Tiny và PVT-Small. Lưu ý rằng ViT-S có 6 heads cho tất cả các lớp, trong khi PVT-Tiny và PVT-Small áp dụng chiến lược head tiến bộ nơi số lượng head tăng từ 1 đến 8 dần dần. Các vòng tròn với màu sắc khác nhau biểu thị những heads khác nhau đó. σ(λh) kiểm soát tỷ lệ nội suy của cached attention outputs omem phản ánh đóng góp theo head của cached attention cho đầu ra cuối cùng. Lưu ý rằng σ(λh)>0.5 có nghĩa là cached attention đóng góp nhiều hơn self-attention. Như được hiển thị, trong cả ba mô hình, σ(λh)>0.5 đúng cho hơn một nửa số lớp GRC-Attention, suy ra rằng đầu ra mô hình phụ thuộc đáng kể vào cache.

Bảng 2: So sánh hiệu suất(Độ chính xác Top-1) của các mô hình cached sử dụng GRC và attention-based
Model No cache Attention-based cache GRC
ViT-S 79.9 80.0 81.3
PVT-Tiny 75.1 74.8 78.4
PVT-Small 79.9 79.6 81.8

thành thạo trong việc nhận dạng các patch công cộng và thường xuyên xuất hiện của lớp này. Trong khi đó đối với oself từ nhánh self-attention, nó có thể tập trung vào việc tìm ra các đặc trưng riêng tư và đặc trưng hơn của thể hiện hiện tại.

Với các giả định trên, chúng tôi có thể cố gắng giải thích tính quy luật của σ(λh) trong Hình 5: sử dụng nhiều omem hơn (σ(λh) lớn hơn) trong các lớp trước có thể giúp mạng phân biệt thể hiện này một cách thô, và sử dụng nhiều oself hơn (σ(λh) nhỏ hơn) cho phép mô hình đưa ra quyết định tinh vi.

Chính quy hóa cross-sample. Đoạn trên cũng cho thấy rằng cache được đề xuất của chúng tôi hoạt động tương tự như các nguyên mẫu vector (Caron et al. 2020), lưu trữ các đặc trưng công cộng của cùng một lớp một cách ngầm định và cho phép các mô hình phân loại đầu vào với cả biểu diễn công cộng và đặc trưng. Theo cách này, các dự đoán không chỉ phụ thuộc vào đầu vào hiện tại mà còn vào các mẫu cached liên quan, do đó cung cấp chính quy hóa cross-sample để tránh overfitting.

GRC v.s. các phương pháp dựa trên bộ nhớ khác. Chúng tôi thực hiện các ablation bổ sung để so sánh GRC và bộ nhớ dựa trên attention cho phân loại hình ảnh trong ImageNet-1k. Chúng tôi triển khai cache kiểu Transformer-XL cho Vision Transformers (bao gồm ViT-S, PVT-Tiny và PVT-Small) và so sánh chúng với các mô hình GRC-cached tương ứng. Như được hiển thị trong Bảng 2, các mô hình GRC-cached một cách nhất quán vượt trội hơn các đối tác attention-based cache và no-cache của chúng. Bên cạnh đó, có thể lưu ý rằng attention-based cache khó có thể cải thiện hiệu suất mô hình.

Phát hiện Đối tượng và Phân đoạn Thể hiện.
Thiết lập Thí nghiệm. Chúng tôi tiếp tục đánh giá tính tổng quát của GRC-Attention trên track phát hiện đối tượng / phân đoạn thể hiện bằng cách sử dụng tập dữ liệu COCO2017 (Lin et al. 2014). Các mô hình được huấn luyện trên COCO train2017 (118k hình ảnh) và được đánh giá trên val2017 (5k hình ảnh). Chúng tôi sử dụng

Bảng 3: Hiệu suất phát hiện đối tượng và phân đoạn thể hiện trên COCO val2017 theo thiết lập Mask R-CNN 1×.
Architecture APb APb50 APb75 APm APm50 APm75
PVT-Tiny 36.7 59.2 39.3 35.1 56.7 37.3
+ Cached 41.0 (+ 4.6) 63.4 44.8 38.3 (+ 3.2) 60.4 41.1
PVT-Small 40.4 62.9 43.8 36.3 60.1 40.3
+ Cached 44.5 (+ 4.1) 67.1 48.6 41.0 (+ 4.7) 64.0 44.1
PVT-Medium 42.0 64.4 45.6 39.0 61.6 42.1
+ Cached 46.6 (+ 4.6) 68.2 51.0 42.3 (+ 3.3) 65.3 45.5

cached PVT làm backbone và áp dụng detector Mask R-CNN (He et al. 2017) để xác minh hiệu quả của GRC-Attention. Các chỉ số COCO tiêu chuẩn của Average Precision (AP) cho phát hiện bounding box (APbb) và phân đoạn thể hiện (APm) được sử dụng để đánh giá các phương pháp của chúng tôi. Tất cả các thiết lập training và siêu tham số được giữ giống như triển khai PVT ban đầu (Wang et al. 2021), và tất cả các mô hình liên quan được huấn luyện trong 12 epochs sử dụng 8 GPU. Đối với cả cached PVT và baseline, các backbone trước tiên được pretrain trên ImageNet và sau đó được fine-tune cho detection.

Kết quả. Như được hiển thị trong Bảng 3, khi sử dụng Mask R-CNN cho phát hiện đối tượng, các cached PVT vượt trội hơn đáng kể so với baseline của chúng. Ví dụ, AP của cached PVT-Medium tốt hơn 4.6 (46.6 vs. 42.0) điểm so với các đối tác no-cache của nó. Kết quả tương tự cũng có thể được tìm thấy trong kết quả phân đoạn thể hiện, nơi cached PVT-Medium đạt được APm cao hơn 3.3 (39.0 vs. 42.3). Những kết quả này chứng minh tính tổng quát của cơ chế caching được đề xuất.

Mô hình Ngôn ngữ
Thiết lập Thí nghiệm Trong nghiên cứu này, chúng tôi thực hiện các thí nghiệm để so sánh hiệu suất của Gated Recurrent Cache (GRC) với Transformer-XL (Dai et al. 2019) trên một nhiệm vụ mô hình ngôn ngữ sử dụng benchmark WikiText-103. Để triển khai các mô hình ngôn ngữ GRC-cached, chúng tôi sử dụng framework fairseq có sẵn công khai và tuân theo các cấu hình Transformer-XL dựa trên bộ nhớ mặc định làm baseline của chúng tôi, bao gồm kiến trúc mô hình và thiết lập training. Để đảm bảo so sánh công bằng, chúng tôi so sánh các mô hình GRC-cached với hai phương pháp dựa trên bộ nhớ khác, Memory Transformer (MT) (Burtsev et al. 2020) và Recurrent Memory Transformer (RMT) (Bulatov, Kuratov, and Burtsev 2022). Chúng tôi triển khai các mô hình GRC-cached bằng cách thay thế

--- TRANG 7 ---
Bảng 4: Kết quả dịch máy nơ-ron sử dụng Pre-Norm Transformers theo điểm BLEU.
ArchitectureIWSLT14 IWSLT15
De-En Es-En En-Fr De-En En-Vi Cs-En
Transformer 35.5 41.4 41.5 36.1 29.8 28.8
Transformer (GRC-cached) 36.0(+ 0.5) 41.8(+ 0.4) 41.7(+ 0.2) 36.3(+ 0.2) 30.2(+ 0.4) 29.4(+ 0.6)

Bảng 5: So sánh hiệu suất (Test PPL) cho GRC và các phương pháp dựa trên Bộ nhớ khác (Burtsev et al. 2020; Bulatov, Kuratov, and Burtsev 2022) trên WikiText-103. Càng nhỏ càng tốt. GRC vượt trội hơn Transformer-XL và các phương pháp dựa trên bộ nhớ trước đó cho mô hình ngôn ngữ với biên độ lớn 1.1 PPL.
Architecture baseline MT-cached RMT-cached GRC-cached
Transformer-XL base 24.0 23.99 23.95 22.9
Transformer-XL large 18.3 - - 17.9

sơ đồ caching bằng phương pháp GRC trong khi giữ nguyên tất cả data augmentation và siêu tham số để so sánh công bằng hơn.

So sánh với Các Phương pháp Dựa trên Bộ nhớ Khác Chúng tôi trình bày hiệu suất của các mô hình GRC-cached so với baseline Transformer-XL và các phương pháp dựa trên bộ nhớ khác trong Bảng 5. Kết quả cho thấy rằng các mô hình GRC-cached vượt trội hơn Transformer-XL và các phương pháp dựa trên bộ nhớ khác về perplexity trên cả mô hình quy mô cơ bản và lớn. Ví dụ, GRC-cached Transformer-XL base đạt được PPL thấp hơn lên đến 1.1 so với baseline Transformer-XL và PPL thấp hơn 1.05 so với RMT, chứng minh sự vượt trội của GRC so với các phương pháp Transformer dựa trên bộ nhớ trước đó.

Long Range Arena
Thiết lập Thí nghiệm. Chúng tôi thực hiện rộng rãi các thí nghiệm trên benchmark Long Range Arena (LRA) được đề xuất gần đây (Tay et al. 2021a) để xác thực các phương pháp được đề xuất của chúng tôi trong kịch bản long-context. Để chứng minh khả năng mô hình hóa chuỗi tầm xa của GRC-Attention và cơ chế cache tương ứng, chúng tôi chọn nhiệm vụ Long ListOps đầy thử thách trong LRA, đây là một biến thể dài hơn của nhiệm vụ ListOps (Nangia and Bowman 2018) với các chuỗi có độ dài lên đến 2k và khó khăn đáng kể. Trong nhiệm vụ này, chúng tôi cũng mở rộng GRC-Attention cho các biến thể attention hiệu quả bằng cách thay thế hàm self-attention (Xem section). Cụ thể, chúng tôi so sánh GRC-Attention với các đối tác no-cache của chúng trên các baseline bao gồm Transformer (Vaswani et al. 2017), BigBird (Zaheer et al. 2020) và Reformer (Kitaev, Kaiser, and Levskaya 2020). Đối với những attention hiệu quả như BigBird và Reformer, chúng tôi chỉ nhập gated recurrent cache và duy trì hàm attention bên trong của chúng không thay đổi. Tất cả các thí nghiệm đều theo thiết lập mặc định trong (Tay et al. 2021a).

Kết quả. Bảng 6 báo cáo kết quả Long ListOps. Như được hiển thị, các mô hình cached một cách nhất quán vượt trội hơn baseline của chúng (bao gồm cả các phương pháp SOTA Reformer) một cách đáng kể. Ví dụ, bằng cách sử dụng GRC, mô hình BigBird có thể đạt được độ chính xác cao hơn 1.39. Những kết quả này cho thấy khả năng mô hình hóa chuỗi tầm xa của GRC cũng như tính tổng quát của nó đối với các biến thể attention khác.

Bảng 6: Kết quả trên nhiệm vụ Long ListOPs trong LRA theo độ chính xác. Cột "cached" chỉ ra các mô hình cached có các lớp attention được triển khai như GRC-Attention tổng quát. ∆ biểu thị sự khác biệt giữa các mô hình cached được đề xuất và baseline.
Architecture baseline GRC-cached ∆
Transformer 36.23 37.40 + 1.17
BigBird 36.06 37.45 + 1.39
Reformer 37.27 37.85 + 0.58

Dịch Máy Nơ-ron
Thiết lập Thí nghiệm. Chúng tôi thí nghiệm các phương pháp của mình trên các tập dữ liệu công cộng được sử dụng rộng rãi IWSLT14 và IWSLT15. Nhiều nguồn ngôn ngữ3 được bao gồm để xác minh đầy đủ hiệu quả của GRC được đề xuất, và các mô hình được huấn luyện cho từng track riêng lẻ. Chúng tôi áp dụng thiết lập Pre-Norm Transformer trong (Wang et al. 2019) và triển khai các mô hình bằng framework fairseq-py (Ott et al. 2019). Theo (Wang et al. 2019; Ott et al. 2019), chúng tôi thường tăng learning rates lên 2 và lấy trung bình 10 checkpoint cuối cùng để suy luận. Chúng tôi sử dụng các mô hình GRC-cached được đề xuất bằng cách thay thế tất cả các module attention của các lớp encoder transformer bằng GRC-Attention. Độ dài cache Tm được đặt là 64 cho tất cả các mô hình cached. Tất cả các transformer trong nhiệm vụ này sử dụng sáu lớp encoder và sáu lớp decoder. Để so sánh công bằng, cả baseline và các mô hình cached đều được huấn luyện trong các thiết lập giống hệt nhau.

Kết quả. Chúng tôi sử dụng BLEU (Papineni et al. 2002) làm chỉ số đánh giá và so sánh GRC cached transformers với baseline của chúng trong Bảng 4. Có thể thấy rằng những cải thiện nhất quán có thể đạt được bằng cách áp dụng GRC-Attention cho baseline. Đối với các track như IWSLT14 De-En và IWSLT15 Cs-En, các cải thiện có thể đạt được 0.5/0.6 điểm, điều này thực sự đáng kể đối với những nhiệm vụ này.

Thảo luận
Chúng tôi giới thiệu Cached Transformer với Gated Recurrent Cache (GRC), một mở rộng đơn giản cho các mô hình dựa trên Transformer làm tăng đáng kể độ dài của ngữ cảnh attention bằng cách cho phép truy cập vào các trạng thái lịch sử thông qua cơ chế gating. GRC nhúng các token trước đó, dù chúng gần hay xa, như các vector có độ dài cố định, mà không phụ thuộc độ phức tạp vào số lượng token được cached. Do đó, GRC mô hình các phụ thuộc token trên một phạm vi đầu vào rộng hơn, dẫn đến cải thiện độ chính xác và hiệu suất trên các biến thể Transformers đa dạng với các kiến trúc và hàm attention khác nhau, trên nhiều nhiệm vụ thị giác và ngôn ngữ khác nhau.

3IWSLT14: German-English(De-En), Spanish-English(Es-En) và English-French(En-Fr), IWSLT15: German-English(De-En), English-Vietnamese(En-Vi) và Czech-English(Cs-En)

--- TRANG 8 ---
Tài liệu tham khảo
Ainslie, J.; Ontanon, S.; Alberti, C.; Cvicek, V.; Fisher, Z.; Pham, P.; Ravula, A.; Sanghai, S.; Wang, Q.; and Yang, L. 2020. ETC: En-coding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483.

Beltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.

Brahma, S. 2018. Improved language modeling by decoding the past. arXiv preprint arXiv:1808.05908.

Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhari-wal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877–1901.

Bulatov, A.; Kuratov, Y.; and Burtsev, M. 2022. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35: 11079–11091.

Burtsev, M. S.; Kuratov, Y.; Peganov, A.; and Sapunov, G. V. 2020. Memory transformer. arXiv preprint arXiv:2006.11527.

Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; and Zagoruyko, S. 2020. End-to-end object detection with trans-formers. In European conference on computer vision, 213–229. Springer.

Caron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.; and Joulin, A. 2020. Unsupervised learning of visual features by con-trasting cluster assignments. Advances in Neural Information Pro-cessing Systems, 33: 9912–9924.

Cettolo, M.; Niehues, J.; Stüker, S.; Bentivogli, L.; Cattoni, R.; and Federico, M. 2015. The IWSLT 2015 Evaluation Campaign. In Proceedings of the 12th International Workshop on Spoken Lan-guage Translation: Evaluation Campaign, 2–14. Da Nang, Viet-nam.

Cettolo, M.; Niehues, J.; Stüker, S.; Bentivogli, L.; and Federico, M. 2014. Report on the 11th IWSLT evaluation campaign. In Pro-ceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign, 2–17. Lake Tahoe, California.

Cho, K.; Van Merriënboer, B.; Bahdanau, D.; and Bengio, Y. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.

Choromanski, K.; Likhosherstov, V.; Dohan, D.; Song, X.; Gane, A.; Sarlos, T.; Hawkins, P.; Davis, J.; Mohiuddin, A.; Kaiser, L.; et al. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794.

Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J.; Le, Q. V.; and Salakhut-dinov, R. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.

Deng, W.; Marsh, J.; Gould, S.; and Zheng, L. 2022. Fine-Grained Classification via Categorical Memory Networks. IEEE Transac-tions on Image Processing, 31: 4186–4196.

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language under-standing. arXiv preprint arXiv:1810.04805.

Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations.

d'Ascoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.; Biroli, G.; and Sagun, L. 2021. Convit: Improving vision transformers with soft convolutional inductive biases. In International Conference on Machine Learning, 2286–2296. PMLR.

Grave, E.; Joulin, A.; and Usunier, N. 2016. Improving neu-ral language models with a continuous cache. arXiv preprint arXiv:1612.04426.

He, K.; Gkioxari, G.; Dollár, P.; and Girshick, R. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, 2961–2969.

He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778.

Khandelwal, U.; Levy, O.; Jurafsky, D.; Zettlemoyer, L.; and Lewis, M. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172.

Kitaev, N.; Kaiser, Ł.; and Levskaya, A. 2020. Reformer: The effi-cient transformer. arXiv preprint arXiv:2001.04451.

Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25.

Kuhn, R.; and De Mori, R. 1990. A cache-based natural language model for speech recognition. IEEE transactions on pattern anal-ysis and machine intelligence, 12(6): 570–583.

Kupiec, J. 1989. Probabilistic models of short and long distance word dependencies in running text. In Speech and Natural Lan-guage: Proceedings of a Workshop Held at Philadelphia, Pennsyl-vania, February 21-23, 1989.

Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Dollár, P. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, 2980–2988.

Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollár, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In European conference on computer vision, 740–755. Springer.

Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, 10012–10022.

Long, A.; Yin, W.; Ajanthan, T.; Nguyen, V.; Purkait, P.; Garg, R.; Blair, A.; Shen, C.; and van den Hengel, A. 2022. Retrieval aug-mented classification for long-tail visual recognition. In Proceed-ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6959–6969.

Loshchilov, I.; and Hutter, F. 2018. Decoupled Weight Decay Reg-ularization. In International Conference on Learning Representa-tions.

Nangia, N.; and Bowman, S. R. 2018. Listops: A diagnostic dataset for latent tree learning. arXiv preprint arXiv:1804.06028.

Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grang-ier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In NAACL-HLT (Demonstrations).

Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In ACL.

Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018. Improving language understanding by generative pre-training.

Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9.

Rae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap, T. P. 2019. Compressive transformers for long-range sequence mod-elling. arXiv preprint arXiv:1911.05507.

Ramachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Levskaya, A.; and Shlens, J. 2019. Stand-alone self-attention in vision mod-els. Advances in Neural Information Processing Systems, 32.

--- TRANG 9 ---
Roy, A.; Saffar, M.; Vaswani, A.; and Grangier, D. 2021. Efficient content-based sparse attention with routing transformers. Transac-tions of the Association for Computational Linguistics, 9: 53–68.

Shao, W.; Ge, Y.; Zhang, Z.; XU, X.; Wang, X.; Shan, Y.; and Luo, P. 2022. Dynamic Token Normalization improves Vision Trans-formers. In International Conference on Learning Representations.

Sukhbaatar, S.; Ju, D.; Poff, S.; Roller, S.; Szlam, A.; Weston, J.; and Fan, A. 2021. Not all memories are created equal: Learning to forget by expiring. In International Conference on Machine Learn-ing, 9902–9912. PMLR.

Sun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Revisiting unreasonable effectiveness of data in deep learning era. In Pro-ceedings of the IEEE international conference on computer vision, 843–852.

Tay, Y.; Dehghani, M.; Abnar, S.; Shen, Y.; Bahri, D.; Pham, P.; Rao, J.; Yang, L.; Ruder, S.; and Metzler, D. 2021a. Long Range Arena: A Benchmark for Efficient Transformers. In International Conference on Learning Representations.

Tay, Y.; Dehghani, M.; Aribandi, V.; Gupta, J.; Pham, P. M.; Qin, Z.; Bahri, D.; Juan, D.-C.; and Metzler, D. 2021b. Omninet: Om-nidirectional representations from transformers. In International Conference on Machine Learning, 10193–10202. PMLR.

Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.; and Jégou, H. 2021a. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, 10347–10357. PMLR.

Touvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and Jégou, H. 2021b. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 32–42.

Tu, Z.; Liu, Y.; Shi, S.; and Zhang, T. 2018. Learning to remember translation history with a continuous cache. Transactions of the Association for Computational Linguistics, 6: 407–420.

Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Wang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; and Chao, L. S. 2019. Learning deep transformer models for machine trans-lation. arXiv preprint arXiv:1906.01787.

Wang, S.; Li, B. Z.; Khabsa, M.; Fang, H.; and Ma, H. 2020a. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.

Wang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.; Lu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision transformer: A versa-tile backbone for dense prediction without convolutions. In Pro-ceedings of the IEEE/CVF International Conference on Computer Vision, 568–578.

Wang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.; Lu, T.; Luo, P.; and Shao, L. 2022. PVT v2: Improved baselines with Pyramid Vision Transformer. Computational Visual Media, 1–10.

Wang, X.; Zhang, H.; Huang, W.; and Scott, M. R. 2020b. Cross-batch memory for embedding learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-tion, 6388–6397.

Wu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.; and Zhang, L. 2021. Cvt: Introducing convolutions to vision transform-ers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 22–31.

Wu, Y.; Rabe, M. N.; Hutchins, D.; and Szegedy, C. 2022. Memo-rizing transformers. arXiv preprint arXiv:2203.08913.

Yuan, L.; Chen, Y.; Wang, T.; Yu, W.; Shi, Y.; Jiang, Z.-H.; Tay, F. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 558–567.

Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Alberti, C.; Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang, L.; et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33: 17283–17297.

Zhong, Z.; Zheng, L.; Luo, Z.; Li, S.; and Yang, Y. 2019. In-variance matters: Exemplar memory for domain adaptive person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 598–607.

Zhu, C.; Ping, W.; Xiao, C.; Shoeybi, M.; Goldstein, T.; Anandku-mar, A.; and Catanzaro, B. 2021. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Infor-mation Processing Systems, 34.

--- TRANG 10 ---
Đánh giá Văn học Đầy đủ
Language Transformers. Transformer lần đầu được giới thiệu trong xử lý ngôn ngữ bởi ((Vaswani et al. 2017)), và một lượng lớn công việc đã được thực hiện để cải thiện nó. Những nghiên cứu có ảnh hưởng nhất của language transformers là GPT (Radford et al. 2018, 2019; Brown et al. 2020) và BERT (Devlin et al. 2018). Họ GPT/BERT hoạt động theo cách 'pretraining-finetuning', đạt được hiệu suất state-of-art trên nhiều benchmark ngôn ngữ khác nhau. Nhưng chúng cũng đắt đỏ về cả phần cứng và năng lượng. Một loạt phương pháp khác cải thiện vanilla transformers và cũng tìm kiếm sự cân bằng giữa hiệu suất và hiệu quả. Để bao phủ thông tin tầm xa trong ngôn ngữ, một số kỹ thuật attention hiệu quả được đề xuất như xấp xỉ kernel (Wang et al. 2020a; Choromanski et al. 2020), sparse hóa (Zaheer et al. 2020; Roy et al. 2021) và local hashing (Kitaev, Kaiser, and Levskaya 2020)). Thay vì những attention nhẹ này, các nghiên cứu khác cố gắng áp dụng attention một cách có chọn lọc với các quy tắc được định trước như sliding windows (Beltagy, Peters, and Cohan 2020), kiến trúc phân cấp (Ainslie et al. 2020) và token pruning (Sukhbaatar et al. 2021).

Cache Language Models và Các phương pháp dựa trên Bộ nhớ. Các mô hình cache có hiệu quả trong mô hình hóa tầm xa, và được giới thiệu lần đầu bởi (Kupiec 1989; Kuhn and De Mori 1990) cho nhận dạng giọng nói. Nói chung, một mô hình cache lưu trữ các biểu diễn của quá khứ, thường là unigrams hoặc các cặp key-value cho tính toán tương lai. (Grave, Joulin, and Usunier 2016) mở rộng các loại phương pháp này cho các mô hình ngôn ngữ nơ-ron (RNNs), nơi cache lưu trữ các cặp đầu vào và trạng thái ẩn gần đây nhất, và (Brahma 2018) sau đó cải thiện các mô hình neural cache bằng cách giải mã các token quá khứ như một regularization. Transformer-XL (Dai et al. 2019) tiếp tục áp dụng kỹ thuật này cho transformers, nơi cache lưu trữ các cặp key-value trước đó trong attention từ các bước training trước đó.

Nhiều phương pháp dựa trên bộ nhớ được khám phá theo sau Transformer-XL: Ví dụ, MT (Burtsev et al. 2020) và RMT (Bulatov, Kuratov, and Burtsev 2022) sử dụng các token bộ nhớ bổ sung để lưu trữ thông tin cục bộ và toàn cục cho các phân đoạn đầu vào khác nhau. (Rae et al. 2019) nén các token trước khi chúng được lưu trong cache để giảm bộ nhớ và tính toán. Ngoài các biểu diễn tổng quát, một số nghiên cứu cũng lưu trữ thông tin đặc trưng nhiệm vụ trong cache để cải thiện hiệu suất. Ví dụ, (Tu et al. 2018) đề xuất tăng cường các mô hình dịch máy nơ-ron bằng cách ghi nhớ lịch sử dịch. Tuy nhiên, các phương pháp này thường sử dụng cache theo cách có độ dài cố định và first-in-first-out (FIFO), điều này hạn chế số lượng token có thể được ghi nhớ trong chuỗi.

Để giải quyết vấn đề này, nghiên cứu dựa trên bộ nhớ gần đây (Khandelwal et al. 2019; Wu et al. 2022) đề xuất lưu trữ các cặp key-value trong một cache lớn mà không nén và thực hiện tra cứu K-nearest neighbor (KNN) để tìm kiếm qua chúng. Trong khi phương pháp này mang lại kết quả cạnh tranh trong mô hình ngôn ngữ, nó vẫn yêu cầu dung lượng bộ nhớ lớn và thời gian đáng kể để tìm kiếm, đặc biệt đối với các phạm vi attention dài hơn. Ngược lại, Cached Transformers dựa trên GRC mà chúng tôi đề xuất học cách xây dựng cache một cách thích ứng với độ phức tạp độc lập với phạm vi attention.

Vision Transformers. Vision transformers (và các biến thể) gần đây đã đạt được thành công lớn trong nhiều nhiệm vụ thị giác khác nhau. ViTs (Dosovitskiy et al. 2021) lần đầu đề xuất chia hình ảnh thành các chuỗi patch và đưa chúng vào các encoder transformer. Mặc dù mang lại kết quả cạnh tranh với CNNs, ViTs có vấn đề yêu cầu pretraining tốn kém trên các tập dữ liệu quy mô lớn như JFT-300M (Sun et al. 2017). Nhiều nghiên cứu (Shao et al. 2022) quy nguyên nhân này cho việc thiếu inductive bias và đề xuất giới thiệu các prior tích chập vào ViTs để mã hóa inductive bias như ngữ cảnh cục bộ. Ví dụ, DeiT (Touvron et al. 2021b) sử dụng các teacher tích chập để chưng cất kiến thức cho transformers, Swin-Transformer (Liu et al. 2021) thực hiện attention trong sliding windows, và ConViT (d'Ascoli et al. 2021) sử dụng một module tích chập "mềm" để mã hóa tính cục bộ. Hơn nữa, các phương pháp khác như PVT (Wang et al. 2022), T2T (Yuan et al. 2021), và CVT (Wu et al. 2021) tiếp tục cải thiện vision transformers bằng cách nhập các prior tích chập trong CNNs (He et al. 2016). Khác với các phương pháp hiện có tập trung vào các token trong mẫu, GRC được đề xuất tiếp tục tăng cường vision transformers bằng cách mô hình hóa các phụ thuộc của các token giữa các mẫu.

Chi tiết Triển khai
Thuật toán Training và Inference

Thuật toán 1 cung cấp các quy trình chi tiết của GRC-Attention được đề xuất trong một forward pass. Trong quá trình training, mỗi module GRC-Attention duy trì một cache liên tục Ct, sẽ được cập nhật ở mỗi lần lặp. Lưu ý rằng tất cả các tính toán liên quan trong GRC-Attention đều khả vi và các tham số tương ứng do đó có thể được tối ưu hóa bằng các phương pháp dựa trên gradient. Các cache tích lũy Ct được lưu trữ với các tham số mạng sau training, và sẽ được sử dụng trực tiếp để suy luận mà không cần cập nhật thêm.

Chi tiết triển khai khác.
GRC hỗ trợ training end-to-end với các tham số mạng khác bằng cách sử dụng các phương pháp dựa trên gradient. Cache Ct sẽ được lưu như buffer cùng với các mô hình đã được huấn luyện và sử dụng để đánh giá. Tương tự như thống kê training trong batch normalization, Ct sẽ bị đóng băng ở thời gian suy luận và do đó không yêu cầu cập nhật. Chúng tôi cũng nhận thấy rằng một số nhiệm vụ chấp nhận đầu vào có độ dài thay đổi (T thay đổi, như phát hiện đối tượng hoặc dịch máy). Trong những trường hợp như vậy, chúng tôi sẽ nội suy các chuỗi đầu vào hợp lệ (không có zero-padding) đến độ dài cache cố định Tm và sau đó tiếp tục cập nhật cache.

Chi tiết thí nghiệm
Phân loại hình ảnh trên ImageNet. Chúng tôi đánh giá hiệu suất của GRC-Attention sử dụng nhiều vision transformers khác nhau ((bao gồm ViTs, PVT, Swin, và PVTv2)) trên ImageNet-1k (Krizhevsky, Sutskever, and Hinton 2012), bao gồm 1.28M hình ảnh training và 50K hình ảnh validation từ 1K lớp. Đối với mỗi baseline, chúng tôi triển khai các biến thể cached của chúng bằng cách thay thế trực tiếp tất cả các lớp self-attention của chúng bằng GRC-Attention, giữ nguyên kiến trúc của chúng. Theo mặc định, tỷ lệ cache được đặt là 0.5 và độ dài cache bằng số lượng patch Tm = T. Như được gợi ý bởi (Ramachandran et al. 2019), positional encodings được

--- TRANG 11 ---
Bảng 7: Hiệu suất phát hiện đối tượng trên COCO val2017 theo thiết lập RetinaNet 1×.
Architecture AP AP 50 AP75 APS APM APL
PVT-Tiny 36.7 56.9 38.9 22.6 38.8 50.0
PVT-Tiny (Cached) 40.2 (+ 3.5) 61.1 43.1 25.0 43.7 53.4
PVT-Small 40.4 61.3 43.0 25.0 42.9 55.7
PVT-Small (Cached) 44.0 (+ 3.6) 65.4 47.4 29.7 47.7 57.5
PVT-Medium 41.9 63.1 44.3 25.0 44.9 57.6
PVT-Medium (Cached) 45.7 (+ 3.8) 67.1 49.1 29.0 49.3 60.2

Thuật toán 1: Forward pass của GRC-Attention ở giai đoạn training.
Yêu cầu: bước training t (t > 0), đầu vào mini batch X∈RB×T×D, tham số có thể học λh cho head h∈{0,1, ..., H−1}, cache tích lũy Ct−1∈RTm×Dm, trong đó Dm = rD và r là tỷ lệ caching.
Đảm bảo: khởi tạo C0 thành vector zero, λh = 0 cho tất cả heads, tỷ lệ caching r = 0.5, và đặt Tm = T (cho phân loại hình ảnh / Long ListOps / Phát hiện Đối tượng) hoặc Tm = 64 (cho Dịch Máy).
Đầu ra: đầu ra attention Oh trên cả cache và đầu vào.
1: tính toán X̄t∈RB×T×Dm bằng cách cắt đầu vào Xt với tỷ lệ r.
2: nội suy X̄t đến độ dài Tm nếu T ≠ Tm.
3: tính toán update gates gu và reset gates gr theo Eqn.(4).
4: tính toán Ct theo Eqn.(5) và lấy trung bình Ct trên chiều batch.
5: cập nhật Ct−1 ← Ct và lưu trữ nó.
6: tính toán đầu ra self-attention oh_self theo Eqn.(1).
7: tính toán đầu ra cached attention oh_mem theo Eqn.(3).
8: tính toán Oh theo Eqn.(2).

thêm vào GRC-Attentions. Để so sánh công bằng các mô hình cached với baseline của chúng, chúng tôi áp dụng thiết lập training ban đầu của chúng bao gồm data augmentations, optimizers và các siêu tham số khác. Cụ thể, chúng tôi sử dụng optimizer Adam với momentum 0.9 và weight decay 0.05. Tất cả các mô hình được huấn luyện trên hình ảnh 224×224 trong 300 epochs, với cosine learning rate scheduler. Cả baseline và các mô hình cached đều sử dụng augmentation timm tiêu chuẩn như (Touvron et al. 2021a), bao gồm normalization, random cropping, horizontal flipping và color jittering. Global average poolings được sử dụng trong PVT và Swin, nơi kích thước pooling cho hai khối đầu tiên lần lượt là 4 và 2. Tất cả các mô hình được huấn luyện trên 16 GPU Nvidia Tesla V100, với bộ nhớ 32 GB.

Phát hiện đối tượng và phân đoạn thể hiện trên COCO 2017. Các mô hình được huấn luyện trên COCO train2017 (118K hình ảnh) và được đánh giá trên val2017 (5K hình ảnh). Chúng tôi sử dụng cached PVT làm backbone và áp dụng detector Mask R-CNN (He et al. 2017) để xác minh hiệu quả của GRC-Attention. Trước khi training, chúng tôi sử dụng trọng số pretrained trên ImageNet (từ các thí nghiệm trước) để khởi tạo backbone ngoại trừ cache C, sẽ được khởi tạo thành zero. Vì độ dài đầu vào (T) thay đổi trong phát hiện đối tượng, ở giai đoạn training X̄ sẽ được nội suy có độ dài Tm để cập nhật cache. Các chỉ số COCO tiêu chuẩn của Average Precision (AP) cho phát hiện bounding box (APbb) và phân đoạn thể hiện (APm) được sử dụng để đánh giá các phương pháp của chúng tôi. Tất cả các thiết lập training và siêu tham số được giữ giống như triển khai PVT ban đầu (Wang et al. 2021), và tất cả các mô hình liên quan được huấn luyện trong 12 epochs (lịch trình training 1×) sử dụng 8 GPU V100. Optimizer AdamW (Loshchilov and Hutter 2018) được áp dụng với learning rates ban đầu 1×10−4. Các hình ảnh training được thay đổi kích thước thành 800×1333, có nghĩa là cạnh ngắn hơn là 800 pixel và cạnh dài hơn không vượt quá 1333 pixel. Ở giai đoạn testing, cạnh ngắn hơn của hình ảnh đầu vào được cố định ở 800. Đối với cả cached PVT và baseline, các backbone trước tiên được pretrain trên ImageNet và sau đó được fine-tune cho phát hiện đối tượng.

Bảng 8: Hiệu suất của GRC trên ImageNet-22k.
Model Top-1 Acc Top-5 Acc
Swin-T 80.9 96.0
Swin-T (Cached) 81.7 (+0.8) 96.4 (+0.4)

Long ListOps trên LRA. Đối với tất cả các thí nghiệm trên benchmark LRA, chúng tôi tuân theo các mã được phát hành của (Tay et al. 2021a), triển khai GRC-Attention bằng Flax và giữ nguyên tất cả các thiết lập training khác. Cụ thể, tất cả các mô hình được đánh giá được xây dựng với 512 embedding dimension, 1024 mlp dimension, 8 heads và 6 layers, chỉ có các hàm attention được thay thế bằng các biến thể attention khác nhau và các phiên bản cached của chúng. Như thực hành trong phân loại hình ảnh, các module GRC được khởi tạo với r = 0.5. Mỗi mô hình được huấn luyện trong 5K bước (với 1K bước cho warmups) trên các chuỗi có độ dài 2K riêng lẻ với batch size 32. Optimizer Adam được áp dụng với learning rates ban đầu 0.05 và weight decay 0.1.

Dịch Máy trên IWSLT14 và IWSLT15. Chúng tôi thí nghiệm các phương pháp của mình trên các tập dữ liệu công cộng được sử dụng rộng rãi IWSLT14 (Cettolo et al. 2014) và IWSLT15 (Cettolo et al. 2015). Đối với mỗi tập dữ liệu, chúng tôi chọn ba track để xác thực GRC-Attention được đề xuất, bao gồm German-English (De-En), Spanish-English (Es-En) và English-French (En-Fr) trong IWSLT14 và German-English (De-En), English-Vietnamese (En-Vi) và Czech-English (Cs-En) trong IWSLT15. Pre-Norm Transformer trong (Wang et al. 2019) được sử dụng làm baseline và các mô hình được triển khai bằng framework fairseq-py (Ott et al. 2019). Theo (Wang et al. 2019; Ott et al. 2019), chúng tôi thường tăng learning rates lên 2 và lấy trung bình 10 checkpoint cuối cùng để suy luận. Các mô hình GRC-cached được tạo ra bằng cách thay thế các hàm attention của chúng trong Transformer encoders

--- TRANG 12 ---
Bảng 9: Thời gian training/inference cho các mô hình GRC-cached trên ImageNet.
Model Training throughput Testing throughput FLOPs Top-1 Accuracy
PVT-Tiny 313 930 1.90G 75.1
PVT-Tiny(Cached) 257 768 2.15G 78.4
PVT-Small 181 689 3.80G 79.9
PVT-Small(Cached) 146 561 4.29G 81.8
PVT-Medium 101 393 6.70G 81.2
PVT-Medium(Cached) 84 319 7.61G 83.0

Bảng 10: Hiệu suất của GRC với các tỷ lệ caching khác nhau.
Model Ratio FLOPs Acc
PVT-Tiny 0.000 1.90G 75.1
PVT-Tiny 0.125 1.93G 75.7
PVT-Tiny 0.250 1.96G 76.8
PVT-Tiny 0.500 2.15G 78.4
PVT-Tiny 1.000 2.97G 78.5

bằng các module GRC-Attention, được khởi tạo với tỷ lệ caching r = 0.5 và độ dài cache Tm = 64. Tất cả các transformer trong nhiệm vụ này bao gồm 6 lớp encoder và 6 lớp decoder, được huấn luyện với độ dài tối đa 512 và optimizer Adam. Learning rates ban đầu là 0.0015 và sau đó được giảm bởi inverse square root scheduler (Ott et al. 2019).

Kết quả Rộng rãi và Ablations
Kết quả Rộng rãi trên ImageNet-22k
Chúng tôi cũng tuân theo triển khai Swin-Transformer để pretrain mô hình cached Swin-T của chúng tôi trên ImageNet-22k. Như được hiển thị trong Tab. 8, GRC hiệu quả tăng cường hiệu suất của mô hình Swin-T được pretrain trên tập dữ liệu lớn hơn. Trong phiên bản cuối cùng của chúng tôi, chúng tôi sẽ bao gồm các kết quả bổ sung của các biến thể ViT / Swin-Transformer khác trên ImageNet22k.

Kết quả Rộng rãi về Phát hiện Đối tượng
Chúng tôi áp dụng rộng rãi GRC-Attention cho RetinaNet (Lin et al. 2017), một mạng dự đoán dày đặc đại diện khác cho phát hiện đối tượng. Chúng tôi chọn PVTs (Wang et al. 2021) với các kích thước khác nhau làm backbone, bao gồm PVT-Tiny, PVT-Small, và PVT-Medium. Như thực hành đối với Mask R-CNN, chúng tôi sử dụng PVT pretrained được cached bởi GRC-Attention để khởi tạo backbone của RetinaNet và huấn luyện các mô hình trong 12 epochs (lịch trình RetinaNet 1×) với batch size 16 trên 8 GPU. Theo thực hành trong (Wang et al. 2021), chúng tôi áp dụng optimizer AdamW (Loshchilov and Hutter 2018) với learning rate ban đầu 1×10−4 để cập nhật các tham số. Chỉ số COCO tiêu chuẩn Average Precision (AP) được sử dụng để đánh giá các mô hình. Tab. 10 hiển thị kết quả detection sử dụng RetinaNet. Nhất quán với Mask R-CNN, cached PVTs cải thiện đáng kể baseline của chúng về precision. Ví dụ, cached PVT-Medium có thể đạt được AP cao hơn 3.8 so với vanilla PVT-Medium, điều này khá đáng kể đối với nhiệm vụ này. Tóm lại, những thí nghiệm này trên các nhiệm vụ downstream (phát hiện đối tượng và phân đoạn thể hiện) chứng minh khả năng tổng quát của cơ chế GRC-Attention được đề xuất trong các nhiệm vụ thị giác dày đặc.

Lựa chọn Tỷ lệ Caching
Đối với các siêu tham số chính như tỷ lệ caching và độ dài bộ nhớ, chúng tôi thực hiện một loạt thí nghiệm sơ bộ và chọn những cái phù hợp để đạt được sự cân bằng tốt hơn giữa độ phức tạp và hiệu suất. Tab. 10 cung cấp ablations đối với tỷ lệ caching trên ImageNet. Như được hiển thị, chúng ta có thể quan sát rằng các cải thiện hiệu suất từ tỷ lệ caching lớn hơn (r) trở nên bên lề khi r > 0.5.

Throughput Training và Inference
Chúng tôi so sánh throughput (hình ảnh/giây, mỗi GPU) của các mô hình GRC-cached và baseline trên ImageNet và kết quả được hiển thị trong Tab. 9. Mô hình GPU là Tesla V100. Chúng ta có thể thấy rằng GRC cải thiện hiệu suất của các mô hình PVT với các kích thước khác nhau trong khi giới thiệu một sự gia tăng chi phí tính toán bên lề. Cụ thể, các mô hình GRC-cached vượt trội hơn các baseline no-cache tương ứng của chúng 1.8%-3.3% độ chính xác top-1 với khoảng 15%-20% giảm throughput. Xin cũng lưu ý rằng mặc dù GRC cải thiện hiệu suất mô hình với tốc độ giảm nhẹ, nó vẫn hiệu quả hơn đáng kể so với việc cải thiện mô hình bằng cách tăng chiều sâu/chiều rộng mô hình. Ví dụ, GRC-cached PVT-Small đạt được 81.8% độ chính xác training với 146 training throughput, thậm chí vượt trội hơn no-cache PVT-Medium mang lại 81.2% độ chính xác với 101 training throughput.
