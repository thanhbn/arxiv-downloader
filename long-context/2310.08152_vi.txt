# Nén Ngữ Cảnh cho Transformer Tự Hồi Quy với Các Token Sentinel

Siyu Ren Qi Jia
Đại học Giao thông Thượng Hải, Trung Quốc
{roy0702, Jia_qi}@sjtu.edu.cn

Kenny Q. Zhu∗
Đại học Texas tại Arlington, Hoa Kỳ
kenny.zhu@uta.edu

## Tóm tắt

Độ phức tạp bậc hai của mô-đun attention khiến nó dần trở thành phần lớn tính toán trong các LLM dựa trên Transformer trong quá trình sinh. Hơn nữa, bộ nhớ đệm key-value quá mức phát sinh khi xử lý đầu vào dài cũng gây ra các vấn đề nghiêm trọng về dung lượng bộ nhớ và độ trễ suy luận. Trong nghiên cứu này, chúng tôi đề xuất một phương pháp cắm-và-chạy có thể nén dần các kích hoạt trung gian của một khoảng token được chỉ định thành các đại diện compact, từ đó giảm cả chi phí bộ nhớ và tính toán khi xử lý ngữ cảnh tiếp theo. Các thí nghiệm trên cả mô hình hóa ngôn ngữ trong miền và sinh tạo tài liệu mở không giám sát cho thấy ưu điểm của phương pháp chúng tôi so với các baseline attention thưa thớt về tính trôi chảy, khớp n-gram và tương tự ngữ nghĩa. Cuối cùng, chúng tôi đánh giá toàn diện lợi ích của nén ngữ cảnh trong việc cải thiện thông lượng hệ thống. Code có sẵn tại https://github.com/DRSY/KV_Compression.

## 1 Giới thiệu

Kiến trúc Transformer (Vaswani et al., 2017) đã trở thành thành phần nền tảng của các mô hình ngôn ngữ lớn hiện đại (LLM) (Radford et al., 2019; Devlin et al., 2019; Zhang et al., 2022; Touvron et al., 2023) trong những năm gần đây. Tuy nhiên, độ phức tạp tính toán bậc hai và dung lượng bộ nhớ của cơ chế attention đã hạn chế đáng kể việc áp dụng Transformer cho các ngữ cảnh ngày càng dài với tài nguyên tính toán bị giới hạn.

Để giảm thiểu các vấn đề này, các nghiên cứu trước đây (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020) đã khám phá nhiều biến thể Transformer hiệu quả, chủ yếu bằng cách thay thế phép toán attention bậc hai gốc bằng các dạng xấp xỉ tuyến tính hóa khác nhau. Mặc dù đầy hứa hẹn, việc pre-training quy mô lớn và các kernel CUDA chuyên biệt thường được yêu cầu để các mô hình này đạt được hiệu suất tương đương với các LLM có sẵn và thực hiện được những cải thiện hiệu quả thực sự.

Trong nghiên cứu này, chúng tôi nhằm cải thiện hiệu quả của các LLM dựa trên Transformer hiện có mà không cần thay đổi kiến trúc. Cụ thể, chúng tôi tập trung vào bộ nhớ đệm key-value, chiếm phần lớn dung lượng bộ nhớ và chi phí di chuyển dữ liệu (I/O) khi xử lý đầu vào ngày càng dài bằng LLM. Chúng tôi đề xuất một phương pháp cắm-và-chạy để nén dần bộ nhớ key-value của một khoảng token liên tiếp thành các đại diện compact.

Cụ thể, chúng tôi giới thiệu một cặp token sentinel đặc biệt <CL> và <CR> vào từ vựng của LLM và sử dụng chúng để đánh dấu ranh giới của khoảng cần nén. Trong quá trình huấn luyện, chúng tôi sửa đổi mặt nạ attention nhân quả sao cho các token tương lai sau <CR> bị ngăn không được attend đến các token giữa <CL> và <CR>. Bằng cách tiếp tục huấn luyện LLM với mục tiêu dự đoán token tiếp theo, mô hình học cách trích xuất và cô đọng thông tin liên quan đến nhiệm vụ của khoảng bị ràng buộc vào token sentinel kết thúc. Độ dài ngữ cảnh giảm đi làm giảm cả chi phí bộ nhớ và tính toán khi xử lý các token tiếp theo, từ đó cải thiện thông lượng hệ thống với kích thước batch lớn hơn và tốc độ giải mã nhanh hơn trong quá trình suy luận.

Chúng tôi tiến hành thí nghiệm trên benchmark mô hình hóa ngôn ngữ WikiText-2 và cho thấy phương pháp của chúng tôi có thể tổng quát hóa cho các LLM với nhiều kích thước khác nhau (từ 1.3B đến 3B) và các scheme mã hóa vị trí như absolute position embedding (Devlin et al., 2019) và rotary position encoding (Su et al., 2021). So với các baseline attention thưa thớt, phương pháp của chúng tôi có thể nén hiệu quả bộ nhớ đệm key-value lịch sử với sự suy giảm đáng kể ít hơn về perplexity. Hơn nữa, chúng tôi chứng minh rằng phương pháp của chúng tôi vượt trội hơn attention thưa thớt trong sinh tạo tài liệu mở không giám sát qua các tỷ lệ nén khác nhau được đánh giá bằng perplexity, ROUGE (Lin, 2004), và BERTScore (Zhang et al., 2019). Cuối cùng, chúng tôi chứng minh thực nghiệm rằng nén ngữ cảnh có thể mang lại cải thiện đáng kể về thông lượng hệ thống cho sinh tạo văn bản.

## 2 Bối cảnh và Nghiên cứu Liên quan

Trong phần này, chúng tôi trình bày kiến thức nền cần thiết cũng như phân biệt phương pháp của chúng tôi với các tài liệu hiện có.

**Độ phức tạp của Cơ chế Attention** Cơ chế self-attention của LLM tạo ra một vector query cho mỗi token để chọn và truy xuất thông tin từ tất cả các vector key và value đã tính toán trước đó. Do đó, một bộ nhớ key-value được yêu cầu tại runtime để lưu trữ thông tin quá khứ. Cho một LLM đã được pre-train với M lớp Transformer, H head attention, chiều ẩn per-head là dhead, kích thước batch b, và độ dài ngữ cảnh hiện tại L, mô hình lưu trữ các vector key và value đã tính toán cho đến nay như một tensor có shape (M, 2, b, H, L, dhead). Trong quá trình giải mã tự hồi quy, kích thước của bộ nhớ đệm key-value tăng tuyến tính với độ dài ngữ cảnh L, dẫn đến sự gia tăng đáng kể về dung lượng bộ nhớ và độ trễ.

**Transformer Hiệu quả** Nhiều biến thể hiệu quả của Transformer đã được đề xuất để giải quyết độ phức tạp có vấn đề của attention. Sparse Transformer (Child et al., 2019) giới hạn trường tiếp nhận của mỗi token trong một cửa sổ cục bộ. Longformer (Beltagy et al., 2020) và BigBird (Zaheer et al., 2020) giới thiệu thêm các token có thể truy cập ngẫu nhiên và toàn cục để bù đắp cho việc mất thông tin. Linear Transformer (Katharopoulos et al., 2020) tái công thức hóa self-attention như một tích dot-product tuyến tính của các feature map kernel và sử dụng tính chất kết hợp của tích ma trận để giảm độ phức tạp từ O(N²) xuống O(N). Tuy nhiên, những xấp xỉ này đã được chứng minh là làm suy giảm tính biểu đạt của full-attention và gần đây đã bị vượt qua bởi các LLM giống GPT. Trong nghiên cứu này, chúng tôi giải quyết vấn đề này bằng cách nén đầu vào dài thành các đại diện compact hơn, điều này trực giao với các nỗ lực đang diễn ra để tối ưu hóa kiến trúc.

**Gisting Tokens** Mu et al. (2023) đã khám phá việc sử dụng gisting tokens để nén các prompt văn bản trong quá trình instruction tuning. Họ cho thấy rằng các hướng dẫn nhiệm vụ dài dòng có thể được nén thành những cái ngắn hơn nhiều. Tuy nhiên, phương pháp của họ chỉ áp dụng được cho văn bản prefix khoảng 20 token. Ngược lại, chúng tôi nhằm đến một scheme nén với các lựa chọn nén linh hoạt hơn và tỷ lệ nén lớn hơn.

## 3 Phương pháp

Trong phần này, chúng tôi trình bày một phương pháp cắm-và-chạy để nén các đại diện ngữ cảnh đầy đủ thành những cái ngắn hơn trong khi vẫn bảo tồn thông tin cần thiết để hoàn thành nhiệm vụ cuối.

### 3.1 Nén Ngữ cảnh với Sentinel Tokens

Để giảm bớt cường độ tính toán và bộ nhớ của bộ nhớ key-value khi xử lý ngữ cảnh ngày càng dài, chúng tôi giới thiệu hai sentinel token <CL> và <CR> vào từ vựng của LLM, được đặt xung quanh một khoảng token liên tiếp mà bộ nhớ key-value tương ứng sẽ được nén.

Để hợp nhất thông tin của các token được bao quanh bởi <CL> và <CR>, chúng tôi thiết kế một mặt nạ attention nhân quả đã sửa đổi để tạo điều kiện cho hành vi này. Một ví dụ minh họa được hiển thị trong Hình 1. Cụ thể, <CL> phục vụ như một ký hiệu bắt đầu nén và chỉ có thể attend đến chính nó. Các token thông thường (các token ngoại trừ <CL> và <CR>) có quyền truy cập vào tất cả các token <CR> và token thông thường trước đó. Điều này đảm bảo rằng các đại diện ngữ cảnh được xây dựng dựa trên cả thông tin quá khứ đã nén (có mất mát) và hoàn chỉnh (không mất mát). <CR> sau đó hoạt động như một dạng lựa chọn và truy xuất thông tin từ các đại diện ngữ cảnh của các token xung quanh. Scheme masking đã sửa đổi này, kết hợp với fine-tuning đặc thù nhiệm vụ của LLM, khuyến khích việc chưng cất thông tin liên quan đến nhiệm vụ của các chuỗi token có thể dài thành một đại diện compact, từ đó giảm kích thước bộ nhớ key-value cần thiết cho việc xử lý tiếp theo.

[Hình 1: Mặt nạ attention đã sửa đổi cho nén ngữ cảnh. Các ô màu xanh lá cây và xám chỉ ra các vị trí được phép và không được phép attend đến (hướng attention từ hàng → cột).]

**Biến đổi Đầu vào** Cho một đầu vào x với L token, chúng tôi định nghĩa tỷ lệ nén r là tỷ lệ phần trăm của các token được bao quanh bởi một hoặc nhiều cặp <CL> và <CR>, và độ dài nén tối đa l là số lượng token lớn nhất được nén bởi một cặp token <CL> và <CR> duy nhất. Chúng tôi lặp lại việc lấy mẫu khoảng token với độ dài tuân theo phân phối đều U(2, l) làm mục tiêu nén, cho đến khi đạt được tỷ lệ r. Nhưng những khoảng này không được chồng lấp. Chúng tôi gắn position encoding của <CL> và <CR> với token trước đó của chúng, sao cho chúng không chiếm các vị trí có giá trị cho các LLM sử dụng absolute position embedding.

**Huấn luyện** Chúng tôi chọn một quy trình nhẹ để thích ứng LLM với các nhiệm vụ downstream thông qua fine-tuning. Cụ thể, tất cả các tham số của LLM được đóng băng ngoại trừ embedding của <CL> và <CR> và các mô-đun LoRA (Hu et al., 2021) được áp dụng cho tất cả các lớp attention. Điều này không chỉ làm dễ dàng quá trình huấn luyện của các mô hình với hàng tỷ tham số mà còn đảm bảo khả năng mô hình hóa ngôn ngữ vốn có và kiến thức tham số trong các mạng feedforward được giữ nguyên (Geva et al., 2021).

**Thao tác Bộ nhớ Key-Value cho Nén thời gian Suy luận** Cho một đoạn văn bản làm prefix, chúng tôi áp dụng cùng chiến lược biến đổi đầu vào được định nghĩa ở trên để đạt được tỷ lệ nén được chỉ định. Để thực hiện việc giảm bộ nhớ và tính toán có thể cảm nhận được, bộ nhớ đệm key-value của các token được bao quanh bởi sentinel token được giải phóng khỏi GPU theo cách tiến triển (ví dụ, sử dụng vòng lặp for qua các khối token) nếu prefix gốc quá dài để vừa với GPU. Nếu không, chúng tôi chỉ đưa prefix đã biến đổi qua mô hình và giải phóng bộ nhớ đệm key-value của các token đã đánh dấu trong một lần.

## 4 Thí nghiệm

Chúng tôi chủ yếu đánh giá phương pháp của mình trên nhiệm vụ mô hình hóa ngôn ngữ và sau đó khám phá khả năng tổng quát hóa không giám sát của nó trên sinh tạo tài liệu mở. Cuối cùng, chúng tôi đo lường định lượng ảnh hưởng của nén ngữ cảnh đến thông lượng hệ thống.

### 4.1 Mô hình hóa Ngôn ngữ

**Benchmark** Chúng tôi chủ yếu tiến hành thí nghiệm trên tập dữ liệu Wikitext-2 (Merity et al., 2016) gồm các bài viết Wikipedia, được sử dụng rộng rãi để đánh giá mô hình hóa ngôn ngữ. Chúng tôi báo cáo perplexity trung bình mức token (loại trừ <CL> và <CR> cho KV Compression) trên tập test làm metric đánh giá. Chúng tôi cũng báo cáo kết quả trên tập dữ liệu WritingPrompts (Fan et al., 2018) để khảo sát tính tổng quát của phương pháp.

**Mô hình** Để chứng minh tính tổng quát của phương pháp chúng tôi, được gọi là KV Compression, chúng tôi áp dụng OPT-1.3B, OPT-2.7B (Zhang et al., 2022), và RedPajama-3B (Computer, 2023) làm ba LLM với các kích thước và phương pháp mã hóa vị trí khác nhau.

**Baseline** Chúng tôi so sánh phương pháp của mình với hai baseline attention thưa thớt: (1) Scattered Attention lấy mẫu các vị trí cho phép attention từ phân phối Bernoulli B(1-r); và (2) Local Attention (Beltagy et al., 2020) giới hạn attention của token xt trong khoảng (⌊t×r⌋, t). Đối với KV Compression, chúng tôi đặt l là 25 trong suốt thí nghiệm. Phương pháp đề xuất của chúng tôi, cùng với các baseline attention thưa thớt mà chúng tôi đã chọn, chia sẻ một mục tiêu chung: nâng cao tỷ lệ tính toán so với truy cập bộ nhớ bằng cách cắt giảm bộ nhớ đệm key-value được lưu trữ. Để biết thiết lập huấn luyện chi tiết, vui lòng tham khảo Phụ lục A.

**Kết quả** Kết quả trên WikiText-2 được hiển thị trong Bảng 1. Khi tỷ lệ nén r tăng lên, cả ba phương pháp đều dẫn đến sự gia tăng perplexity do: (1) các token quan trọng nằm ngoài phạm vi attention, hoặc (2) khả năng của một token <CR> duy nhất không đủ để đóng gói toàn bộ thông tin của khoảng token đã nén. Chúng tôi quan sát thấy Local Attention vượt trội đáng kể so với Scattered Attention, cho thấy tầm quan trọng của ngữ cảnh cục bộ đối với mô hình hóa ngôn ngữ. KV Compression đạt được perplexity tốt nhất qua các tỷ lệ nén khác nhau. Đáng chú ý, ở các tỷ lệ nén cao, ví dụ r≥0.7, KV Compression gây ra sự suy giảm đáng kể ít hơn về perplexity so với Local Attention, chứng minh ưu điểm của nó trong việc nén thông tin cục bộ rải rác trong khi vẫn giữ thông tin toàn cục mạch lạc. Trong Bảng 2, chúng tôi báo cáo perplexity của RedPajama-3B trên tập dữ liệu WritingPrompts sử dụng Local Attention và KV Compression đề xuất, với tỷ lệ nén từ {0.7, 0.8, 0.9}. KV Compression đạt được perplexity thấp hơn một cách nhất quán so với Local Attention, cho thấy ưu thế của nó như một phương pháp tổng quát hóa miền cho nén ngữ cảnh.

### 4.2 Sinh tạo Mở Không giám sát

**Dữ liệu** Chúng tôi ngẫu nhiên chọn 200 tài liệu từ tập validation C4 (Raffel et al., 2020) để đánh giá. Chúng tôi sử dụng 128 từ đầu làm prefix và coi 64 từ tiếp theo là tham chiếu ground-truth.

**Mô hình** Chúng tôi trực tiếp lấy mô hình RedPajama-3B được huấn luyện trên Wikitext-2 để thực hiện sinh tạo tài liệu mở không giám sát. Cho một văn bản prefix p, nucleus sampling (Holtzman et al., 2019) được sử dụng để sinh ra một completion c cho nó.

**Baseline** Vì Scattered Attention hoạt động kém theo Bảng 1, chúng tôi chỉ so sánh KV Compression với Local Attention với tỷ lệ nén r từ 0.0 đến 0.9 được áp dụng cho prefix p sử dụng biến đổi đầu vào được định nghĩa trong Phần 3 cho KV Compression và attention bị hạn chế được định nghĩa trong Phần 4. Để Local Attention đạt được nén thời gian suy luận, nó tương đương với việc duy trì một hàng đợi FIFO để lưu trữ bộ nhớ đệm key-value: khi bước thời gian trong quá trình sinh tăng lên, bộ nhớ key-value cũ trong hàng đợi được pop ra và bộ nhớ key-value mới được sinh được push vào.

**Metric Đánh giá** Chúng tôi đánh giá chất lượng của các completion được sinh từ ba khía cạnh: (1) tính trôi chảy, (2) khớp n-gram với continuation ground-truth, và (3) tương tự ngữ nghĩa với continuation ground-truth. Tính trôi chảy được đánh giá bằng perplexity được tính từ mô hình Pythia-1B (Biderman et al., 2023) được pre-train sử dụng C4. Khớp n-gram và tương tự ngữ nghĩa được đo bằng ROUGE-L và BERTScore (Zhang et al., 2019) tương ứng. Để tính đến tính ngẫu nhiên được tạo ra bởi nucleus sampling, cho mỗi prefix, chúng tôi sinh 8 completion và báo cáo kết quả trung bình.

**Kết quả** Hình 2 cho thấy khi r tăng, các completion được sinh của Local Attention có xu hướng lệch khỏi chủ đề gốc, dẫn đến giảm ROUGE-L/BERTScore và tăng perplexity. Một lần nữa, KV Compression xuất sắc trong việc bảo tồn thông tin liên quan và vẫn có thể sinh các continuation chất lượng tốt lên đến tỷ lệ nén 0.5. Đáng chú ý, KV Compression có thể sinh văn bản trôi chảy ngay cả khi prefix bị nén cực kỳ. Lý do là trong quá trình huấn luyện, Local Attention nhận thông tin phai mờ nhanh chóng từ quá khứ xa, khiến cấu trúc diễn ngôn cho các thế hệ tiếp theo trở nên không mạch lạc. Ngược lại, KV Compression bảo tồn tốt hơn thông tin đó bằng cách hợp nhất nó vào các sentinel token.

### 4.3 Lợi ích Thông lượng từ Nén Ngữ cảnh

Với KV Compression, bộ nhớ đệm key-value tương ứng với các token được bao quanh bởi sentinel token có thể được giải phóng khỏi bộ nhớ. Bằng cách này, nó cho phép kích thước batch lớn hơn và cải thiện thông lượng hệ thống. Để đo lường định lượng tác động của nén ngữ cảnh đến thông lượng, chúng tôi tiến hành thí nghiệm trên sinh tạo mở bằng cách thử nghiệm với các mức VRAM GPU tối đa khác nhau. Độ dài đầy đủ của prefix đầu vào giả được đặt thành 800 và chúng tôi sử dụng RedPajama-3B với nucleus sampling để sinh một continuation cho nó. Thông lượng của hệ thống sinh văn bản được định nghĩa là số token được sinh ra mỗi giây. Kết quả được hiển thị ở góc dưới bên phải của Hình 2. Chúng ta có thể thấy rằng, ở các tỷ lệ nén cực độ (ví dụ r≥0.8), nén ngữ cảnh mang lại cải thiện thông lượng hơn 1.5x với VRAM GPU 12GB và cải thiện 1.3x nhỏ hơn một chút với VRAM GPU 24GB. Ở tỷ lệ nén vừa phải (ví dụ r≈0.5), KV compression vẫn có thể mang lại cải thiện thông lượng 1.2x-1.4x trong khi chỉ chịu sự suy giảm chất lượng nhẹ (Phần 4.2). Tương quan bộ nhớ-tỷ lệ nén trực quan hơn được hoãn lại Phụ lục D.

## 5 Kết luận

Trong nghiên cứu này, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả cho phép LLM tóm tắt bộ nhớ key-value của khoảng token được chỉ định. Các thí nghiệm trên mô hình hóa ngôn ngữ và sinh tạo mở chứng minh rằng phương pháp của chúng tôi vượt trội đáng kể so với các baseline attention thưa thớt về nén thông tin và chất lượng sinh tạo.

## Hạn chế

Trong thiết lập đánh giá hiện tại, chúng tôi áp dụng cùng chiến lược được sử dụng cho huấn luyện để chọn các khoảng token được bao quanh bởi <CL> và <CR> cho nén ngữ cảnh thời gian suy luận. Tuy nhiên, các đoạn văn bản khác nhau có thể thể hiện mức độ quan trọng khác nhau đối với các nhiệm vụ downstream. Ví dụ, một cụm danh từ hoàn chỉnh về ngữ pháp và ngữ nghĩa có thể dễ nén hơn một cụm không đúng ngữ pháp chỉ chứa các đơn vị ngôn ngữ một phần. Mặc dù quy trình biến đổi đầu vào của chúng tôi về mặt lý thuyết bao gồm các khoảng văn bản của tất cả các cấu trúc ngôn ngữ có thể, nó vẫn có thể được hưởng lợi từ một chiến lược/thuật toán được thiết kế công phu để chọn mục tiêu nén trong một ngữ cảnh nhất định.

## Tài liệu tham khảo

Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.

Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR, abs/1904.10509.

Together Computer. 2023. Redpajama: An open source recipe to reproduce llama training dataset.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association for Computational Linguistics.

Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484–5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. CoRR, abs/1904.09751.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156–5165. PMLR.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. CoRR, abs/2001.04451.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

Ilya Loshchilov and Frank Hutter. 2017. Fixing weight decay regularization in adam. CoRR, abs/1711.05101.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models.

Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467.

Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551.

Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for Computational Linguistics.

Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big bird: Transformers for longer sequences. CoRR, abs/2007.14062.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with BERT. CoRR, abs/1904.09675.

## A Chi tiết Triển khai

**Mô hình hóa Ngôn ngữ** Đối với các thí nghiệm mô hình hóa ngôn ngữ, chúng tôi huấn luyện OPT-1.3B, OPT-2.7B, và RedPajama-3B với mục tiêu dự đoán token tiếp theo. Loss được tính trên tất cả các token ngoại trừ các token <CL> và <CR> mới được thêm vào. Đối với các token ngay trước <CL> và <CR>, chúng tôi điều chỉnh nhãn ground-truth của chúng thành các token ngay sau các token <CL> và <CR> tương ứng.

Các tham số có thể huấn luyện cho tất cả LLM bao gồm hai token embedding của <CL> và <CR>, và các mô-đun LoRA được áp dụng cho tất cả các lớp attention. Rank của các ma trận trọng số của LoRA được đặt thành 16. Tỷ lệ phần trăm các tham số có thể huấn luyện chiếm khoảng 5% tổng số tham số của LLM. Chúng tôi sử dụng optimizer AdamW (Loshchilov and Hutter, 2017) với learning rate 2e-5. Kích thước batch được đặt thành 12 và độ dài chuỗi tối đa trong quá trình huấn luyện được đặt thành 256 do ngân sách tính toán hạn chế. Việc triển khai dựa trên thư viện Huggingface transformers (Wolf et al., 2020) và tất cả các thí nghiệm được tiến hành sử dụng một GPU RTX 3090 duy nhất.

**Sinh tạo Tài liệu Mở** Đối với sinh tạo tài liệu mở, chúng tôi sử dụng nucleus sampling với top p = 0.9. Do tính ngẫu nhiên được tạo ra bởi sampling, chúng tôi thực hiện 8 lần nucleus sampling cho mỗi prefix đã nén và báo cáo các metric đánh giá trung bình được áp dụng trong bài báo chính. Điều này giúp giảm phương sai và đảm bảo kết quả đánh giá đáng tin cậy hơn.

## B Khả năng Tổng quát hóa của KV Compression

[Hình 3: Hiệu suất tổng quát hóa của các tỷ lệ nén thời gian huấn luyện khác nhau.]

Trong KV Compression đề xuất của chúng tôi, một LLM được huấn luyện trên dữ liệu đã biến đổi trong đó tỷ lệ phần trăm của các token được bao quanh bởi sentinel token chiếm một tỷ lệ được chỉ định của tổng số token. Ở đây chúng tôi nghiên cứu hiệu suất tổng quát hóa của các tỷ lệ nén thời gian huấn luyện khác nhau và khám phá thực hành tốt nhất. Chúng tôi trực quan hóa perplexity của RedPajama-3B được huấn luyện với các tỷ lệ nén khác nhau ở các tỷ lệ nén thời gian test khác nhau.

Hình 3 minh họa kết quả. Chúng ta có thể thấy rằng huấn luyện với tỷ lệ nén cao hơn luôn dẫn đến khả năng tổng quát hóa tốt hơn qua các tỷ lệ nén thời gian test khác nhau. Lý do là ở tỷ lệ nén nhỏ, các mô hình ngôn ngữ đôi khi có thể khai thác ngữ cảnh cục bộ hạn chế và vẫn có thể dự đoán đáng tin cậy token tiếp theo. Trong trường hợp này, các sentinel token không được đảm bảo có được khả năng nén ngữ cảnh mong muốn. Khi phần lớn ngữ cảnh không được phép attend đến, các sentinel token bị buộc phải trau dồi đủ khả năng nén để tối thiểu hóa hàm loss.

## C Sử dụng Bộ nhớ với Tỷ lệ Nén Khác nhau

[Hình 4: VRAM GPU đỉnh được profiled sử dụng Pytorch khi sử dụng RedPajama-3B để tạo ra một continuation 100-token cho các prefix có độ dài khác nhau.]

Chúng tôi đã cho thấy rằng nén ngữ cảnh mang lại cải thiện đáng kể về thông lượng hệ thống, đặc biệt là cho các GPU kích thước vừa phải. Ở đây chúng tôi báo cáo việc sử dụng bộ nhớ chi tiết giả sử một tình huống thực tế tương tự như đối thoại nhiều lượt: độ dài prefix (độ dài của lịch sử đối thoại) tăng dần, và mô hình được yêu cầu xuất ra một phản hồi khoảng 100 token. Để tối đa hóa thông lượng của dịch vụ đối thoại, chúng tôi giả sử mô hình đồng thời sinh phản hồi cho nhiều instance, tức là kích thước batch lớn hơn một.

Việc sử dụng bộ nhớ được trực quan hóa được hiển thị trong Hình 4. Nén ngữ cảnh có thể giảm hơn 3GB bộ nhớ GPU cached đỉnh. Trong thực tế, điều này chuyển thành khả năng sinh nhiều token hơn cho một instance duy nhất và cho phép nhiều instance song song hơn.

## D Kết quả Sinh tạo Mở của OPT

[Hình 5: OPT-1B trên sinh tạo mở trên 200 tài liệu C4 được lấy mẫu. Chất lượng sinh tạo được đo bằng tính trôi chảy (perplexity), khớp n-gram (ROUGE-L), và tương tự ngữ nghĩa (BERTScore).]

Hình 5 tóm tắt kết quả của OPT-1B trên sinh tạo tài liệu mở. So với RedPajama-3B (Hình 2), chất lượng sinh tạo của OPT-1B thấp hơn đáng kể. So sánh các phương pháp nén ngữ cảnh khác nhau, KV Compression hoạt động tốt hơn đều đặn qua cả ba metric đánh giá.
