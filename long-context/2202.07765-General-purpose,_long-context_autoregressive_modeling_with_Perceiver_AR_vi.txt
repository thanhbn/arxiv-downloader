# 2202.07765.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2202.07765.pdf
# Kích thước tệp: 3776445 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Mô hình hóa tự hồi quy đa mục đích, ngữ cảnh dài với Perceiver AR
Curtis Hawthorne* 1Andrew Jaegle* 2C˘at˘alina Cangea2Sebastian Borgeaud2Charlie Nash2
Mateusz Malinowski2Sander Dieleman2Oriol Vinyals2Matthew Botvinick2Ian Simon1Hannah Sheahan2
Neil Zeghidour1Jean-Baptiste Alayracy2Jo˜ao Carreiray2Jesse Engely1
Tóm tắt
Dữ liệu thế giới thực có chiều cao: một cuốn sách,
hình ảnh, hoặc buổi biểu diễn âm nhạc có thể dễ dàng chứa
hàng trăm nghìn phần tử ngay cả sau khi
nén. Tuy nhiên, các mô hình tự hồi quy được sử dụng
phổ biến nhất, Transformers, rất tốn kém để
mở rộng quy mô đến số lượng đầu vào và lớp cần thiết
để nắm bắt cấu trúc tầm xa này. Chúng tôi phát triển Perceiver AR, một kiến trúc
tự hồi quy, bất khả tri về phương thức
sử dụng cross-attention để ánh xạ các đầu vào tầm xa
đến một số lượng nhỏ các latent trong khi cũng duy trì
việc che dấu nhân quả từ đầu đến cuối. Perceiver AR
có thể trực tiếp chú ý đến hơn một trăm nghìn token,
cho phép ước lượng mật độ ngữ cảnh dài thực tế
mà không cần các mẫu thưa thớt được tạo thủ công
hoặc cơ chế bộ nhớ. Khi được huấn luyện
trên hình ảnh hoặc âm nhạc, Perceiver AR tạo ra
các đầu ra có tính nhất quán và cấu trúc dài hạn rõ ràng.
Kiến trúc của chúng tôi cũng đạt được xác suất
tương tự như state-of-the-art trên các benchmark
chuỗi dài, bao gồm hình ảnh ImageNet 64×64
và sách PG-19.
1. Giới thiệu
Một mục tiêu trung tâm của nghiên cứu trí tuệ nhân tạo là phát triển
các hệ thống có thể xác định cấu trúc trong thế giới
và sử dụng nó để thực hiện hiệu quả các nhiệm vụ quan tâm. Trong
vài năm qua, mô hình hóa tự hồi quy (AR) với kiến trúc
attention (đôi khi được gọi chuyển nghĩa là "language
modeling") đã nổi lên như một con đường khả thi để đạt được
mục tiêu này. Trong mô hình hóa AR, một tập hợp các đầu ra được tạo ra
bằng cách (i) sử dụng một mô hình để ánh xạ một tập hợp đầu vào thành một đầu ra, (ii)
nối đầu ra đó vào tập hợp đầu vào, và tiếp tục
lại từ bước (i) cho đến khi tập hợp đầu ra đầy đủ đã được
*Đóng góp bằng nhau†Đóng góp bằng nhau1Google Re-
search, Brain Team2DeepMind. Liên hệ tới:
Curtis Hawthorne <fjord@google.com >, Andrew Jaegle
<drewjaegle@deepmind.com >.
Proceedings of the 39thInternational Conference on Machine
Learning , Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).tạo ra. Công thức đơn giản này về nguyên tắc có thể được tuân theo
để biểu diễn bất kỳ mối quan hệ đầu vào-đầu ra nào, và các kết quả
đột phá đã được đạt được bằng cách sử dụng Transformers (Vaswani
et al., 2017) hoặc các mô hình liên quan để học việc ánh xạ đầu vào !đầu ra
(Vinyals et al., 2019; Brown et al., 2020; Jumper
et al., 2021; Wu et al., 2021). Để điều này hoạt động, mô hình
phải có khả năng nắm bắt các mẫu trong đầu vào hữu ích
cho việc dự đoán đầu ra tiếp theo.

Các mẫu trong dữ liệu thế giới thực thường phụ thuộc vào chi tiết của
nhiều đầu vào (hoặc "token," mỗi token đại diện cho một hàng của
một mảng đầu vào), một số trong đó ở xa trong không gian, thời gian,
hoặc bối cảnh từ đầu ra hiện tại. Nhiều tác phẩm âm nhạc,
chẳng hạn, bắt đầu bằng việc phát biểu một chủ đề với các yếu tố
giai điệu và nhịp điệu rõ ràng. Trong suốt tác phẩm, các yếu tố này được
dần dần biến đổi với các tái hiện ngày càng phức tạp
được thiết kế để thu hút người nghe. Thời gian chính xác
của một cụm từ liên quan như thế nào đến các tiền thân của nó? Một
motif âm sắc tồn tại và phát triển như thế nào khi nó được tái hiện?
Một hòa âm mới làm thế nào để tái bối cảnh hóa một giai điệu quen thuộc?
Cấu trúc của tác phẩm nổi lên khi mỗi thành phần được
xem xét cùng với nhiều thành phần khác.

Có một sự căng thẳng giữa loại cấu trúc dài hạn, theo ngữ cảnh này
và các tính chất tính toán của Transformers.
Transformers liên tục áp dụng một phép toán self-attention cho
các đầu vào của chúng: điều này dẫn đến các yêu cầu tính toán
đồng thời tăng theo bậc hai với độ dài đầu vào và
tăng tuyến tính với độ sâu mô hình. Khi dữ liệu đầu vào trở nên dài hơn,
cần nhiều token đầu vào hơn để quan sát nó, và khi các mẫu
trong dữ liệu đầu vào trở nên tinh tế và phức tạp hơn,
cần nhiều độ sâu hơn để mô hình hóa các mẫu kết quả. Các
ràng buộc tính toán buộc người dùng Transformers phải
cắt bớt các đầu vào cho mô hình (ngăn không cho nó quan sát
nhiều loại mẫu tầm xa) hoặc hạn chế độ sâu
của mô hình (làm mất đi sức mạnh biểu diễn cần thiết để
mô hình hóa các mẫu phức tạp).

Mục tiêu của công trình này là thiết kế một kiến trúc giữ lại
các lợi ích nổi tiếng của Transformers cho mô hình hóa tự hồi quy
trong khi cho phép nhận dạng mẫu tầm xa mà không thêm
sự phức tạp không cần thiết. Để làm điều này,
chúng tôi xây dựng dựa trên họ kiến trúc attention Perceiver
(Jaegle et al., 2021; 2022), đã chứng minh sự xuất-arXiv:2202.07765v2  [cs.LG]  14 Jun 2022

--- TRANG 2 ---
Perceiver AR
QCr oss-a tt end La t ent 
self-a tt end 
KQL la y ers 
V
KVT ar g e ts 
( shift ed inputs ) 
A tt ention 
Mask Cr oss- A tt ention Mask 
rAR
<EOS> AR
P
e
r
c
e
i
v
e
r
A
R
r A R
Perceive
Inputs La t ents 
Hình 1. Perceiver AR ánh xạ các đầu vào ( X2 RMC;M= 11
được hiển thị) đến một latent nhỏ ( Z2RNC;N= 3 được hiển thị) bằng cross-
attention, truy vấn với N đầu vào gần đây nhất để tạo ra một
latent cho mỗi mục tiêu. Các latent sau đó tương tác thông qua một
ngăn xếp sâu gồm L lớp self-attention để tạo ra ước lượng cho mỗi mục tiêu.
Việc che dấu nhân quả được sử dụng trong cả cross- và self-attention để duy trì
thứ tự tự hồi quy từ đầu đến cuối.

sắc xuất sắc trên một loạt các miền ngữ cảnh lớn.
Perceivers sử dụng cross-attention để ánh xạ một mảng đầu vào đầy đủ thành
một mảng latent nhỏ hơn và thực hiện tất cả các phép toán attention
tiếp theo trong không gian latent kết quả.

Điều này tách rời các yêu cầu tính toán của việc xử lý
một mảng đầu vào lớn khỏi những yêu cầu cần thiết để làm cho một
mạng rất sâu, cho phép các Perceiver sâu được sử dụng trên
số lượng lớn đầu vào. Tuy nhiên, vì mỗi
latent mô hình chú ý đến tất cả các đầu vào bất kể vị trí, Perceivers
không thể được sử dụng trực tiếp cho việc tạo tự hồi quy, điều này
yêu cầu rằng mỗi đầu ra mô hình chỉ phụ thuộc vào các đầu vào
đi trước nó trong chuỗi.

Perceiver AR giải quyết vấn đề này với ba sửa chữa: (i) giới
thiệu một thứ tự cho các latent bằng cách xác định mỗi latent với một phần tử đầu ra duy nhất, (ii) sử dụng cross-attention
được che dấu nhân quả để cho phép mỗi latent chỉ chú ý đến các
phần tử đầu vào đi trước nó trong chuỗi, và (iii) sử dụng
self-attention được che dấu nhân quả trong suốt ngăn xếp xử lý latent
để duy trì cấu trúc phụ thuộc tự hồi quy này từ đầu
đến cuối. Những thay đổi này cho phép các đầu ra của Perceiver AR được
giải mã theo chuỗi tự hồi quy trong khi bảo tồn các
lợi ích tính toán và bộ nhớ thiết yếu của các kiến trúc Perceiver khác. Vì mỗi đầu ra của kiến trúc
được điều kiện hóa trên tất cả các đầu vào trước đó, kiến trúc được
định vị tốt để nắm bắt các phụ thuộc tầm xa.

Chúng tôi cho thấy rằng kiến trúc này tạo ra kết quả xuất sắc trên
một số miền thế giới thực với ngữ cảnh tầm xa: hình ảnh
mức RGB (Mục 5.2), đầu vào được token hóa ngôn ngữ (Mục 5.3
đến 5.5), và âm thanh hoặc âm nhạc ký hiệu (Mục 5.6). Độ dài
đầu vào cho các nhiệm vụ này được tóm tắt trong Bảng 1. Chúng tôi
chứng minh rằng Perceiver AR có thể học nhận dạng hoàn hảo
các mẫu ngữ cảnh dài trên khoảng cách ít nhất 100k
token trên một nhiệm vụ sao chép tổng hợp với cấu trúc
sự thật nền đã biết (Mục 5.1.1). Chúng tôi làm nổi bật một số tính chất
hấp dẫn là kết quả của việc tách rời kích thước đầu vào khỏi yêu cầu
tính toán: bằng cách giữ ngữ cảnh dài, nhưng thay đổi
số lượng latent, chúng ta có thể (i) tăng hoặc giảm
tải tính toán tại thời điểm kiểm tra để cải thiện (nhưng chậm hơn) hoặc
nhanh hơn (nhưng hơi tệ hơn) kết quả (Mục 5.2.1) hoặc (ii)
đánh đổi dung lượng mô hình với kích thước batch tại thời điểm huấn luyện
mà không ảnh hưởng đến hiệu suất thời điểm kiểm tra (Mục 5.1.2).

Chúng tôi có những đóng góp sau:
•Chúng tôi giới thiệu Perceiver AR, một kiến trúc hiệu quả, bất khả tri
miền cho việc tạo tự hồi quy có thể trực tiếp chú ý
đến hơn một trăm nghìn token.
•Chúng tôi chứng minh tính hữu ích của việc sử dụng ngữ cảnh dài cho
việc tạo tự hồi quy: Perceiver AR đạt được kết quả
state-of-the-art trên ước lượng mật độ ImageNet và Project Gutenberg
và tạo ra các mẫu với mức độ nhất quán và độ trung thực cao
trên một số nhiệm vụ tạo thách thức (hình ảnh, âm nhạc ký hiệu, và âm thanh).
•Chúng tôi khám phá các lợi ích của việc tách rời
yêu cầu tính toán của việc xử lý đầu vào dài khỏi
những yêu cầu của độ sâu mô hình: hiệu quả được cải thiện so với
các kiến trúc Transformer chỉ giải mã và
Transformer-XL được sử dụng rộng rãi và khả năng thay đổi
tính toán được sử dụng tại thời điểm kiểm tra để phù hợp với ngân sách mục tiêu.

Mã mô hình có sẵn tại https://github.com/
google-research/perceiver-ar .
2. Tự hồi quy và mô hình hóa ngữ cảnh dài
Các mô hình tự hồi quy (ví dụ: Schmidhuber & Heil 1994;
Rosenfeld 2000; Bengio et al. 2003; Graves 2013; van den

--- TRANG 3 ---
Perceiver AR
Nhiệm vụ Vị trí Đầu vào
Sao chép (Mục 5.1) 131,072
ImageNet (Mục 5.2) 12,289
PG-19 (Mục 5.3) 4,096
Sách (Mục 5.4) 16,384
Wikitext-103 (Mục 5.5) 8,192
Âm nhạc Ký hiệu (Mục 5.6) 65,536
Âm thanh Âm nhạc (Mục 5.6) 65,536
Bảng 1. Số lượng vị trí đầu vào (key-value) tối đa được sử dụng cho
các nhiệm vụ trong bài báo này. Hầu hết các mô hình sử dụng 1024 vị trí truy vấn.
Oord et al. 2016b; Uria et al. 2016) ước lượng mật độ của
một ví dụ X2RMbằng cách phân tách nó tuần tự sử dụng
quy tắc chuỗi xác suất:
p(X) =M1Y
m=0p
XmX<m
: (1)
Mỗi Xmthường là một token (một mẫu âm thanh, một kênh RGB,
một ký tự, v.v.). Quy tắc chuỗi cho chúng ta biết rằng
mật độ của một đầu vào Xbao gồm một số token tùy ý
có thể được ước lượng bằng cách ước lượng tuần tự mật độ có điều kiện
của mỗi token của X. Điều này yêu cầu
rằngp(Xmj: : :)được điều kiện hóa trên tất cả các token trước đó
hữu ích cho việc dự đoán token thứ m.

Việc điều kiện hóa trên tất cả các đầu vào liên quan là thách thức do
khó khăn trong việc mở rộng quy mô các mô hình tiêu chuẩn vượt quá độ dài
cửa sổ nhỏ. Trong thực tế, các mô hình phải được thiết kế cẩn thận
để các token có thể được bao gồm trong ngữ cảnh
có liên quan nhất có thể. Điều này thường có nghĩa là các token
cục bộ về mặt không gian hoặc thời gian được bao gồm trong ngữ cảnh, nhưng
ngữ cảnh tầm xa hơn bị bỏ qua hoặc được lấy mẫu phụ. Khi
ngữ cảnh trở nên nhỏ hơn, điều này dẫn đến các xấp xỉ tệ hơn
cho các tín hiệu chứa các phụ thuộc dài hạn.

Perceiver AR được thiết kế để kết hợp ngữ cảnh dài hạn hơn
vào các bộ ước lượng mật độ mạnh mẽ, do đó cho phép các mô hình
bối cảnh hóa nhiều hơn của một tín hiệu và cung cấp tính linh hoạt
tốt hơn về cách dữ liệu được xử lý, đưa chúng ta đến gần hơn với mục tiêu
mô hình hóa mật độ đa mục đích.
3. Perceiver AR
Perceiver AR tuân theo Perceiver và Perceiver IO trong việc giải quyết
vấn đề của các đầu vào lớn bằng cách sử dụng một mô-đun
cross-attention kiểu Transformer để ánh xạ các đầu vào X2RMC(Clà
số lượng kênh) đến một số lượng nhỏ hơn các latent
Z12RNC:
Z1 CrossAttend (X; Z 0) (2)
P er ceiv er AR 
P r c e eA R<EOS> 
Causally 
mask ed 
cr oss-att ention Causally 
mask ed 
s elf-att ention 
Inputs T arg ets 
Cont e xt e xpanded b y 
cr oss-a tt ention 
i v e r A RT ransf ormer-XL 
Causally 
mask ed 
s elf-att ention Cont e xt e xpanded b y 
cached a tt ention 
 
No gradients 
P r c e eA R<EOS> 
Inputs T arg ets 
i v e r A RCont e xt limit ed b y 
self-a tt ention width T ransf ormer 
Causally 
mask ed 
s elf-att ention 
P r c e eA R<EOS> 
Inputs T arg ets 
i v e r A RHình 2. Perceiver AR so sánh với một Trans-
former chỉ giải mã tiêu chuẩn (Liu et al., 2018) và Transformer-XL (cấu hình
thời điểm huấn luyện) (Dai et al., 2019) trong quá trình huấn luyện. Chỉ tập con của
các cấu hình phù hợp với bộ nhớ thiết bị được hiển thị. Đối với một
chiều rộng ngăn xếp self-attention nhất định ( N= 3được hiển thị ở đây), Transformer
chỉ có thể xử lý cùng số lượng token đầu vào. Transformer-
XL kết hợp nhiều ngữ cảnh hơn trong khi duy trì chiều rộng của
ngăn xếp xử lý bằng cách lưu trữ và chỉ tính toán
chuyển tiếp cho các đầu vào tầm xa hơn. Trong thực tế, Transformer-XL chỉ có thể
kết hợp một lượng ngữ cảnh bổ sung vừa phải (xem Hình
3). Perceiver AR sử dụng một cross-attend được che dấu duy nhất để cho phép
huấn luyện trên các ngữ cảnh đầu vào dài hơn nhiều mà không yêu cầu một
ngăn xếp self-attention rộng hơn.
Mảng latent Z1thường nhỏ ( N < M ), vì vậy nó
phù hợp để xử lý bởi nhiều mô-đun self-attention hơn:
Zl+1 SelfAttend (Zl; Zl): (3)
Phép toán này không phụ thuộc vào số lượng điểm đầu vào
M, và Ncó thể được chọn để phép toán này có thể
chi trả và lặp lại cho nhiều lớp l2[1; L]. Chiến lược này
dẫn đến một kiến trúc (Hình 1) với độ phức tạp
O(MN)+O(LN2)do cross-attention và
ngăn xếp self-attention latent, tương ứng (Jaegle et al., 2022). Đối với
các mạng sâu, ngăn xếp self-attention là nơi phần lớn
tính toán xảy ra.

Nhưng việc giảm số lượng điểm từ MthànhNngăn chúng ta
thiết lập phụ thuộc nhân quả giữa tất cả các điểm đầu vào
và đầu ra được sử dụng bởi Transformers cho mô hình hóa tự hồi quy

--- TRANG 4 ---
Perceiver AR
mô hình hóa. Perceiver AR điều chỉnh các latent cho mô hình hóa tự hồi quy
bằng cách giới thiệu che dấu nhân quả cho cả cross-attention
đầu vào và các lớp self-attention latent
(Hình 1) và gán một latent cho mỗi điểm cuối
N của đầu vào (tức là những điểm có số lượng
tiền thân lớn nhất; N= 3trong Hình 1). Ảnh hưởng của các đầu vào
đến sau một latent nhất định được che dấu ở cross-attention
và tất cả các lớp self-attention.

Để thấy cách này hoạt động, hãy xem xét latent thứ hai (giữa)
trong Hình 1: latent này được xây dựng bằng cách truy vấn đầu vào
với embedding của A. Bởi vì Rfollows Ain sequence, embedding của R
bị che dấu, cả ở cross-attention và ở
các lớp self-attention tiếp theo.

Thủ tục này cho phép các mô hình chú ý đến các ngữ cảnh dài
trong khi cũng bảo tồn thứ tự tự hồi quy của các mục
tiêu thông qua toàn bộ mạng. Cùng thủ tục có thể được
áp dụng cho bất kỳ đầu vào nào có thể được sắp xếp, miễn là
việc che dấu được áp dụng. Ví dụ, các kênh RGB của hình ảnh có thể được
sắp xếp theo thứ tự quét raster, bằng cách giải mã các kênh màu R, G, và B
cho mỗi pixel trong chuỗi (Mục 5.2)
hoặc thậm chí dưới các hoán vị khác nhau (Mục 5.2.2).

Xem Phụ lục C để có mô tả toán học chi tiết
về Perceivers và kiến trúc Perceiver AR và Phụ
lục E để có chi tiết kỹ thuật bổ sung.
4. Công trình liên quan
4.1. Mối quan hệ với Transformer và Transformer-XL
Perceiver AR tách rời độ dài của đầu vào khỏi
yêu cầu tính toán của mô hình, chủ yếu được
kiểm soát bởi số lượng latent. Ngược lại, các kiến trúc
Transformer chỉ giải mã tiêu chuẩn (Hình 2) duy trì
một tương ứng một-một giữa đầu vào và đầu ra
trong suốt mạng, dẫn đến việc mở rộng quy mô O(LM2).

Perceiver AR cũng mở rộng quy mô tốt hơn đến ngữ cảnh dài hơn trong thực
tế so với Transformer-XL, có lẽ là phương pháp được sử dụng phổ biến nhất
để mở rộng độ dài ngữ cảnh trong thực tế.
Transformer-XL kết hợp ngữ cảnh dài hơn bằng cách cho phép
attention trên các vị trí ngữ cảnh dài ở mọi lớp trong
chuyển tiếp và dừng lan truyền gradient đến các
vị trí này trong chuyển ngược. Transformer-XL cũng thường
sử dụng ít vị trí hơn ở thời điểm huấn luyện so với thời điểm kiểm tra, điều này
cải thiện thêm việc mở rộng quy mô ở thời điểm huấn luyện. Nhưng ngay cả với những
sửa đổi này, kích thước đầu vào và độ sâu mô hình vẫn được
kết hợp, và khi ngữ cảnh và độ sâu tăng, chuyển tiếp
trở thành nút thắt cổ chai tính toán và bộ nhớ. Về mặt thực tế,
Perceiver AR mở rộng quy mô tốt hơn khi cả kích thước ngữ cảnh
và độ sâu mô hình tăng. Trong Hình 3, chúng tôi so sánh
thời gian trên tường mỗi bước của một Transformer chỉ giải mã,
Transformer-XL, và Perceiver AR trong codebase của chúng tôi.4.2. Mối quan hệ với các kiến trúc attention có thể mở rộng khác
Perceiver AR hạn chế yêu cầu tính toán của việc xử lý
các chuỗi dài bằng cách tránh sử dụng một nút tính toán
cho mỗi phần tử đầu vào. Chiến lược này cũng là
một tính năng của các kiến trúc khác, như Set Transformer (Lee
et al., 2019) và Luna (Ma et al., 2021), sử dụng
các đầu vào query và key-value có hình dạng khác nhau để hạn chế chi phí
của attention và tạo ra các biểu diễn trung gian "được lấy mẫu xuống"
nhỏ hơn kích thước của đầu vào. Không giống như
họ kiến trúc Perceiver, cả Set Transformer
và Luna đều xen kẽ các phép toán attention lấy mẫu xuống và lấy mẫu lên
trong suốt kiến trúc, dẫn đến
một kiến trúc giảm thiểu một số chi phí của Trans-
formers tiêu chuẩn, nhưng vẫn có yêu cầu tính toán nhân
với kích thước đầu vào và độ sâu mô hình. Trong họ kiến trúc này,
Luna tương tự nhất, vì nó tương thích với
mô hình hóa tự hồi quy (được che dấu nhân quả).

Một số kiến trúc khác (Dai et al., 2020; Nawrot et al.,
2021; Clark et al., 2021) giảm yêu cầu xử lý
của Transformers bằng cách nén tuần tự đầu vào với
attention hoặc convolution, nhưng dựa vào việc sử dụng nhiều
lớp attention lớn, có độ phức tạp bậc hai hoặc khai thác các giả định
địa phương hạn chế tính tổng quát của chúng. Điều này làm cho chúng
hiệu quả hơn khi được áp dụng cho các đoạn đầu vào có kích thước tương tự
như Transformers bình thường (tức là vài nghìn), nhưng giảm
tính hữu ích của chúng cho các ngữ cảnh dài hơn.

1024 2048 4096 8192 16384 32768 65536
Độ dài ngữ cảnh23456Bước huấn luyện/giâyPerceiver AR
Transformer-XL
Transformer
6 lớp
12 lớp
18 lớp
24 lớp
30 lớp
36 lớp
42 lớp
Hình 3. So sánh tốc độ huấn luyện trên TPUv3 cho Perceiver
AR so với Transformer chỉ giải mã tiêu chuẩn và
Transformer-XL. Transformer bị hạn chế ở độ dài ngữ cảnh
2,048 token, ngay cả với chỉ 6 lớp—các mô hình lớn hơn và độ dài
ngữ cảnh lớn hơn yêu cầu quá nhiều bộ nhớ. Sử dụng cùng cấu hình
6 lớp, chúng ta có thể mở rộng quy mô bộ nhớ Transformer-XL đến tổng
độ dài ngữ cảnh 8,192. Perceiver AR mở rộng quy mô đến độ dài ngữ cảnh 65k,
và có thể được mở rộng quy mô đến hơn 100k ngữ cảnh với tối ưu hóa thêm.

--- TRANG 5 ---
Perceiver AR
4.3. Mối quan hệ với các kiến trúc encoder-decoder
Perceiver AR có sự giống nhau với Trans-
formers encoder-decoder (Vaswani et al., 2017), seq2seq (Sutskever et al.,
2014), và các mô hình encoder-decoder khác. Các mô hình encoder-decoder
truyền các đầu vào ngữ cảnh dài vào một ngăn xếp encoder (ví dụ:
Perceive , như trong Hình 1 và 2) và sử dụng các đầu ra của
encoder để bối cảnh hóa các tiền thân ngay lập tức của
mỗi đầu ra (ví dụ: rAR), được xử lý bởi một
ngăn xếp decoder riêng biệt. Ngược lại, Perceiver AR truyền cả đầu vào
và mục tiêu qua một ngăn xếp xử lý duy nhất, được chia sẻ. Điều này
cho phép mỗi mục tiêu học cách sử dụng ngữ cảnh dài và đầu vào
gần đây khi cần thiết, với các hạn chế kiến trúc tối thiểu.
Từ quan điểm này, Perceiver AR có thể được xem như
một kiến trúc encoder-decoder với 0 lớp encoder. Bằng cách
xử lý tất cả các đầu vào với một cross-attention duy nhất (được che dấu nhân quả), Perceiver AR tránh được
nhu cầu về các ngăn xếp encoder và decoder riêng biệt.

Xem Phụ lục D để có thảo luận mở rộng về mối quan hệ của Perceiver
AR với các phương pháp khác cho mô hình hóa ngữ cảnh dài
bằng attention hiệu quả (D.1), thiết kế kiến trúc (D.2), và token hóa đầu vào (D.3).
5. Kết quả
Chúng tôi đánh giá Perceiver AR trên một số miền khác nhau
để chứng minh tính linh hoạt của nó và đánh giá khả năng nắm bắt
cấu trúc tầm xa trên nhiều loại dữ liệu khác nhau. Trong tất cả
các thí nghiệm trừ khi được đề cập, chúng tôi sử dụng các mô-đun attention
pre-layernorm (Xiong et al., 2020) và các hàm phi tuyến squared-ReLU
(So et al., 2021).

Đối với mỗi miền, chúng tôi điều chỉnh các mô hình theo perplexity eval
với các quét siêu tham số ad hoc theo khả năng tính toán của chúng tôi. Chúng tôi thường điều chỉnh kích thước kênh, kích thước head, và
độ sâu mô hình. Xem mục của mỗi miền và phụ lục F
và G để biết thêm chi tiết.
5.1. Nhiệm vụ Sao chép
5.1.1. Độ dài đầu vào dài
Đầu tiên chúng tôi xác minh rằng kiến trúc có thể chú ý đến độ dài
đầu vào rất dài trên một nhiệm vụ sao chép tổng hợp. Sử dụng một mô hình với
chỉ 6 lớp gồm 1024 latent, chúng tôi cung cấp một đầu vào với
độ dài 217(131,072). Mô hình được huấn luyện trên các chuỗi
chứa một token [BOS] (Beginning of Sequence) tiếp theo là 65,535 byte ngẫu nhiên (được mã hóa như các token có
một trong 256 giá trị). Những byte ngẫu nhiên đó sau đó được phản chiếu
cho nửa thứ hai của chuỗi và tiếp theo là một
token [EOS] (End of Sequence). Điều này dẫn đến khoảng cách sao chép tối đa là 2172token. Loss huấn luyện và eval
chỉ được tính trên nửa thứ hai của chuỗi để
tránh huấn luyện trên nhiễu.

Sau khi huấn luyện trong 25k bước, mô hình có thể dự đoán chính xác các token được phản chiếu cộng với [EOS] của 12 chuỗi
validation chưa thấy (tổng cộng 786,432 token) với độ chính xác 100%.
Thí nghiệm này chứng minh rằng mô hình có thể
chú ý thành công đến các token riêng lẻ trong các chuỗi rất dài
và rằng một tín hiệu huấn luyện tương đối nhỏ gồm 1024
mục tiêu mỗi chuỗi có thể lan truyền thành công qua
nút thắt cổ chai cross-attend ngay cả khi hầu hết các đầu vào không
liên quan cho một mục tiêu nhất định. Như một lưu ý lịch sử, chúng tôi lưu ý
rằng một tiền thân của thí nghiệm này là nhiệm vụ sao chép được sử dụng
để validate Neural Turing Machine (Graves et al., 2014)
và Differentiable Neural Computer (Graves et al., 2016).
Những mô hình này cho thấy độ chính xác gần như hoàn hảo khi sao chép
lên đến khoảng độ dài-50 chuỗi của các đầu vào 26- hoặc28-bit
ngẫu nhiên, trong khi Perceiver AR cho thấy độ chính xác hoàn hảo khi
sao chép các chuỗi gồm 216đầu vào được token hóa 28-bit ngẫu nhiên.
5.1.2. Số lượng mục tiêu huấn luyện
Trong một mô hình Transformer chỉ giải mã tiêu chuẩn, độ dài đầu vào,
số lượng nút xử lý latent mỗi lớp, và
số lượng mục tiêu huấn luyện luôn giống nhau. Perceiver
AR cho phép tính linh hoạt của bất kỳ tỷ lệ nào của độ dài đầu vào với
số lượng latent và mục tiêu huấn luyện. Thay đổi chiều rộng
của ngăn xếp self-attention ảnh hưởng đến tính biểu diễn của
mạng và cũng thay đổi số lượng đầu ra huấn luyện
mà loss có thể được tính cho mỗi chuỗi trong một batch.

Để minh họa hiệu ứng này, chúng tôi huấn luyện các mô hình trên nhiệm vụ sao chép
với độ dài ngữ cảnh 8,192 token sử dụng số lượng
nút latent và kích thước batch khác nhau (Bảng 2). Để giảm
hiệu ứng của tính biểu diễn mạng, chúng tôi chỉ sử dụng 1 lớp self-attention
cho các thí nghiệm này. Chúng tôi thấy rằng sau khi huấn luyện
trong 25k bước với 1024 latent và kích thước batch 128, mô hình
hội tụ và dự đoán nửa thứ hai của các chuỗi
trong tập kiểm tra với độ chính xác 100%. Nếu chúng tôi giảm kích thước batch
xuống 64, mô hình không hội tụ và đạt được <1%
độ chính xác trên tập kiểm tra. Tuy nhiên, nếu chúng tôi giữ kích thước batch
ở 64 và tăng số lượng latent (và do đó
mục tiêu huấn luyện mỗi chuỗi) lên 2048, mô hình
hội tụ thành công.

1024 latent 2048 latent
64 batch 7 3
128 batch 3 3
Bảng 2. Hội tụ trên nhiệm vụ sao chép với độ dài ngữ cảnh
8,192 token. Việc tăng số lượng latent (và do đó
số lượng mục tiêu) hoặc kích thước batch có hiệu ứng tương tự, như được thảo luận
trong Mục 5.1.2.

Tùy thuộc vào yêu cầu bộ nhớ, tính toán, và tính biểu diễn,
việc tăng kích thước batch hoặc số lượng latent có thể
tốt hơn: Perceiver AR cho phép tính linh hoạt này.

--- TRANG 6 ---
Perceiver AR
16 latent 1024 latent 1536 latent
Hình 4. Các mẫu đại diện từ mô hình ImageNet. 4 hình ảnh bên trái được tạo ra chỉ sử dụng 16 latent, 8 hình ảnh giữa với 1024 latent (giống như thời điểm huấn luyện), và 4 hình ảnh bên phải với 1536 latent. Cùng một mô hình 60 lớp được sử dụng cho tất cả các cấu hình và tất cả đều có thể chú ý đến toàn bộ chuỗi. Các batch đầy đủ mà từ đó những hình ảnh này được lấy được hiển thị trong Phụ lục A.
Loại Mô hình Bits/Dim
PixelCNN AR 3.57
Sparse Transformer AR 3.44
Routing Transformer AR 3.43
Combiner AR 3.42
VDM Diff 3.40
Perceiver AR (của chúng tôi) AR 3.40
Bảng 3. Kết quả trên ước lượng mật độ Downsampled ImageNet (64 64) tính bằng bits/dim trên tập validation, thấp hơn là tốt hơn. Chúng tôi
so sánh kết quả của chúng tôi với các mô hình tự hồi quy PixelCNN
(van den Oord et al., 2016b), Sparse Transformer (Child et al.,
2019), Routing Transformer (Roy et al., 2021), và Combiner
(Ren et al., 2021), và mô hình diffusion Variational Diffusion
Model (Kingma et al., 2021).
5.2. ImageNet 64×64
Để kiểm tra khả năng của kiến trúc này trong phương thức hình ảnh,
chúng tôi sử dụng bộ dữ liệu downsampled ImageNet (van den Oord
et al., 2016b) ở độ phân giải 64 64. Tương tự như
thủ tục huấn luyện được sử dụng trong Sparse Transformer (Child et al.,
2019), chúng tôi làm phẳng hình ảnh thành một chuỗi các byte kênh màu
(256 giá trị có thể) cho mỗi pixel theo thứ tự quét raster.
Sau khi thêm một token [BOS] vào đầu
chuỗi, điều này dẫn đến độ dài 12,289. Mỗi đầu vào
có 3 embedding vị trí được khởi tạo ngẫu nhiên được thêm vào
cho hàng (64), cột (64), và kênh màu (3). Không có
augmentation dữ liệu nào được sử dụng.

Chúng tôi huấn luyện một mô hình với 1024 latent trong 60 lớp self-attention
sau cross-attend ban đầu. Sau 750k bước, chúng tôi đạt được
3.40 bits/dim trên tập validation, vượt quá hiệu suất
của các mô hình tự hồi quy trước đó (Bảng 3). Các mẫu hình ảnh được tạo ra
ở Hình 4 và Phụ lục A.
5.2.1. Thay đổi tính toán tại thời điểm kiểm tra
Bởi vì Perceiver AR tách rời độ dài đầu vào khỏi
số lượng latent trong ngăn xếp self-attention và bởi vì không có
tham số đặc biệt theo vị trí nào được học trong mô hình này, chúng tôi# Latent Bits/Dim Tạo (phút)
16 3.5664 1.99
64 3.4946 2.02
512 3.4139 2.81
1024 3.4025 3.68
1536 3.3986 4.69
2048 3.4018 5.88
4096 3.5569 12.28
Bảng 4. Kết quả trên ước lượng mật độ Downsampled ImageNet (64 64) tính bằng bits/dim trên tập validation (thấp hơn là tốt hơn) khi
thay đổi số lượng latent được sử dụng tại thời điểm eval (1024 được sử dụng tại
thời điểm huấn luyện). Thời gian tạo là thời gian một hình ảnh duy nhất cần để suy luận trên
một lõi TPUv3 sử dụng lưu trữ activation được mô tả trong Phụ lục E.3.
Cùng một mô hình 60 lớp được sử dụng cho tất cả các cấu hình và tất cả có thể
chú ý đến toàn bộ chuỗi.
có tùy chọn hấp dẫn để đánh giá với một số lượng latent khác
so với những gì được sử dụng trong quá trình huấn luyện, trong khi vẫn
duy trì khả năng chú ý đến toàn bộ chuỗi đầu vào
(được minh họa thêm trong Hình 12).

Ngữ cảnh Đầu vào Thứ tự Tiêu chuẩn R!G!B
1024 3.55 4.63
12289 3.54 3.53
Bảng 5. Tác động của độ dài ngữ cảnh và thứ tự chuỗi cho Ima-
geNet (64×64). Kết quả là bits/dim trên tập validation, thấp hơn
là tốt hơn. Các ví dụ ImageNet chứa 12,288 ( 64×64×3) token.
Đối với ngữ cảnh ngắn, thứ tự chuỗi có tác động lớn đến
hiệu suất. Đối với ngữ cảnh dài, tác động là nhỏ.

Chúng tôi thấy rằng việc tăng số lượng latent lên đến 2x
số lượng được sử dụng trong quá trình huấn luyện cải thiện hiệu suất mô hình,
và việc giảm số lượng latent dẫn đến hiệu suất
giảm dần một cách mượt mà mặc dù giảm đáng kể trong
yêu cầu tính toán, như thấy trong Bảng 4. Ví dụ, khi
chỉ sử dụng 16 latent để chú ý đến toàn bộ chuỗi đầu vào
gồm 12,289 token, mô hình đạt được cùng bits/dim như
PixelCNN (van den Oord et al., 2016b). Loại linh hoạt này
cho phép một mô hình duy nhất được sử dụng trong các tình huống với

--- TRANG 7 ---
Perceiver AR
Mô hình Độ dài ngữ cảnh # lớp Val ppl. Test ppl.
Transformer-XL (Rae et al., 2019) 512+1024 36 45.5 36.3
Compressive Transformer (Rae et al., 2019) 512+512+2x512 36 43.4 33.6
Routing Transformer (Roy et al., 2021) 8192 22 - 33.2
Perceiver AR (của chúng tôi) 2048 60 45.9 28.9
Perceiver AR (của chúng tôi) 4096 60 45.9 29.0
Bảng 6. Kết quả trên mô hình hóa ngôn ngữ PG-19. Kết quả được hiển thị tính bằng perplexity (ppl.), thấp hơn là tốt hơn. Kết quả baseline được tái tạo
từ các bài báo gốc. Routing Transformer không báo cáo hiệu suất tập validation. Độ dài ngữ cảnh được hiển thị bao gồm bộ nhớ
(Transformer-XL và Compressive Transformer) và bộ nhớ nén (chỉ có cái sau).
các yêu cầu tính toán, độ trễ, và chất lượng khác nhau, mà không
yêu cầu huấn luyện bổ sung hoặc một quá trình chưng cất.

Chúng tôi nghi ngờ rằng những mô hình này có thể được làm cho linh hoạt hơn nữa
đối với số lượng latent được sử dụng trong quá trình đánh giá hoặc suy luận
nếu việc truy cập latent biến đổi được kết hợp vào huấn luyện, nhưng chúng tôi
để việc khám phá đầy đủ những ý tưởng này cho công việc tương lai.
5.2.2. Tác động của thứ tự chuỗi và độ dài
Chúng tôi kiểm tra khả năng của Perceiver AR sử dụng ngữ cảnh vượt ra ngoài
chiều rộng của ngăn xếp self-attention bằng cách huấn luyện trên các chuỗi hình ảnh
với các phụ thuộc tầm xa mạnh. Thông thường
các mô hình tự hồi quy của dữ liệu hình ảnh sử dụng thứ tự quét raster, trong đó các subpixel RGB ở mỗi vị trí hình ảnh được
tạo ra theo chuỗi trước khi chuyển đến vị trí
tiếp theo. Chúng tôi sắp xếp lại dữ liệu hình ảnh ImageNet để tất cả các
subpixel đỏ được dự đoán theo chuỗi, sau đó là xanh lá, sau đó
là xanh dương. Điều này tạo ra các phụ thuộc tầm xa mạnh giữa
các subpixel ở cùng vị trí không gian.

Chúng tôi huấn luyện các phiên bản 16 lớp của các mô hình trong Mục 5.2 trong
30k bước (do hạn chế tính toán), và so sánh các mô hình ngữ cảnh ngắn
(1024 đầu vào) với các mô hình ngữ cảnh đầy đủ (12,289
đầu vào). Như một baseline chúng tôi cũng huấn luyện cả hai biến thể mô hình trên
các chuỗi với thứ tự tiêu chuẩn. Kết quả được hiển thị trong
Bảng 5. Chúng tôi thấy rằng đối với thứ tự tiêu chuẩn, cả mô hình
ngữ cảnh nhỏ và dài đều có hiệu suất tương tự. Tuy nhiên,
trên dữ liệu hình ảnh được sắp xếp lại, mô hình ngữ cảnh ngắn có
hiệu suất tệ hơn đáng kể. Trong khi mô hình ngữ cảnh dài
có hiệu suất tương đương với các baseline thứ tự tiêu chuẩn. Điều này cho thấy rằng mô hình ngữ cảnh dài của chúng tôi có thể
truy cập và xử lý thông tin ở các timestep xa.

Đối với các miền tổng quát, chúng ta thường không biết những phụ thuộc tầm xa nào
quan trọng, và thí nghiệm này cho thấy rằng tác động hiệu suất của việc thiếu các phụ
thuộc hiện có có thể nghiêm trọng. Perceiver AR cung cấp một giải pháp
bằng cách mở rộng kích thước ngữ cảnh Transformer theo cách có thể mở rộng quy mô.
5.3. Project Gutenberg (PG-19)
Tiếp theo chúng tôi kiểm tra kiến trúc cho mô hình hóa ngôn ngữ trên
bộ dữ liệu Project Gutenberg (PG-19) (Rae et al., 2019), một
bộ sưu tập các sách tiếng Anh được xuất bản trước năm 1919và được thu thập từ kho eBook Project Gutenberg. Chúng tôi sử dụng PG-19 vì nó có sẵn công khai và chứa một số lượng lớn
từ (1.97B huấn luyện, 3.01M validation, 6.97M
test) được lấy từ một số lượng lớn sách hợp lý (28k
huấn luyện, 50 validation, 10 test). Chúng tôi đánh giá Perceiver AR
trên PG-19 sử dụng các đầu vào được token hóa Subword như trong công trình trước
(Rae et al., 2019; Roy et al., 2021). Chúng tôi sử dụng cài đặt
token hóa Subword được báo cáo trong mục 4.2 của Rae et al.
(2019), và chúng tôi huấn luyện các mô hình cho đến khi loss hội tụ trên
một tập con của tập validation (sau khi huấn luyện khoảng 200k
bước ở kích thước batch 2048, hoặc khoảng 420B token tổng cộng).

Chúng tôi so sánh Perceiver AR với các số liệu state-of-the-art từ
tài liệu (Bảng 6). Các mô hình tốt nhất được báo cáo bởi các
bài báo này sử dụng 36 (Compressive Transformer; Rae et al. 2019)
và 22 lớp (Routing Transformer; Roy et al. 2021). Chúng tôi
thấy rằng Perceiver AR vượt trội hơn các phương pháp state-of-the-art
trên bộ dữ liệu này khi sử dụng cùng token hóa. Tuy
nhiên, phù hợp với công trình trước, chúng tôi không thấy bằng chứng
cải thiện vượt quá 2k token trên PG-19 (Sun et al., 2021).
Được thúc đẩy bởi các thí nghiệm sớm, cả hai mô hình đều sử dụng embedding đầu vào lớn
(4096), số lượng lớn head cross-attend
(128), và dropout cross-attend cao (0.96875 cho mô hình
4096-context và 0.875 cho mô hình 2048-context; xem
Phụ lục E.2). Kết quả được trình bày trong Bảng 6 cho thấy rằng
việc đơn giản mở rộng quy mô Perceiver AR có thể tạo ra kết quả xuất sắc
ngay cả với mức độ dropout cross-attend rất cao.

Phù hợp với công trình trước, chúng tôi nhận thấy rằng các mô hình thể hiện một
giảm nhất quán trong hiệu suất giữa các tập validation và test PG-19. Điều này có thể do số lượng
sách tương đối nhỏ được chứa trong validation PG-19 (50 sách) và
test (100 sách). Mặc dù có số lượng lớn token,
số lượng nhỏ sách trên tập validation sẽ dẫn đến
nhiều token này thể hiện nội dung và phong cách được chia sẻ,
hạn chế mức độ đại diện của tập validation đối với
nhiệm vụ tổng thể. Như được lưu ý bởi Sun et al. (2021), tập
validation PG-19 chứa ít nhất một cuốn sách (trong số 50) được
cho là nằm ngoài phân phối cho tập huấn luyện.

Để hiểu rõ hơn hiệu ứng của ngữ cảnh trong mô hình hóa ngôn ngữ,
tiếp theo chúng tôi đánh giá trên một bộ dữ liệu sách nội bộ lớn mà
số lượng tài liệu không phải là mối quan tâm (Mục 5.4).

--- TRANG 8 ---
Perceiver AR
Mô hình Ngữ cảnh Độ sâu Bước/Giây Eval ppl.
AR 1024 36 2.19 14.006
T-XL 1024 23 2.17 14.822
AR 4096 36 2.09 13.806
T-XL 1024 24 2.06 14.719
AR 8192 36 1.95 13.791
T-XL 1024 25 1.97 14.593
AR 16384 36 1.75 13.749
T-XL 1024 28 1.76 14.276
Bảng 8. Kết quả Sách trên tập test, hiển thị bằng perplexity (ppl.),
thấp hơn là tốt hơn, từ các mô hình Perceiver AR 36 lớp với
ngữ cảnh khác nhau và các mô hình Transformer-XL với độ sâu được khớp cho
tính toán (bước/giây), được huấn luyện trong 500k bước.
5.4. Sách
Ở đây, chúng tôi nghiên cứu tính hữu ích của ngữ cảnh đầu vào dài hơn bằng cách
huấn luyện trên một bộ dữ liệu nội bộ chứa 4 triệu sách
được xuất bản từ năm 1500 đến 2008. Bộ dữ liệu này trước đây được
sử dụng trong Rae et al. 2021 như một phần của bộ dữ liệu MassiveTest.
Chúng tôi huấn luyện các mô hình với 1024 latent, 36 lớp và
f1024;4096;8192;16384gtoken ngữ cảnh đầu vào.

Những kết quả này (Bảng 7) cho thấy một xu hướng rõ ràng: kết quả tốt hơn được
đạt được với ngữ cảnh dài hơn 1024.

Mô hình Ngữ cảnh Eval ppl. Train Bước/giây
Perceiver AR 1024 14.88 2.19
Perceiver AR 4096 14.60 2.09
Perceiver AR 8192 14.57 1.95
Perceiver AR 16384 14.56 1.75
Bảng 7. Kết quả Sách, hiển thị bằng perplexity (ppl.), thấp hơn là tốt hơn.
Chúng tôi cũng chạy các thí nghiệm mà chúng tôi so sánh mô hình của chúng tôi
với một baseline Transformer-XL, với tính toán (được đo
bằng bước mỗi giây) được khớp càng gần càng tốt.
Các mô hình được huấn luyện với kích thước batch 256 trong 500k bước.
Đầu tiên, chúng tôi đánh giá các mô hình Perceiver AR 36 lớp với
độ dài ngữ cảnh f1024;4096;8192;16384g, tương ứng.
Mỗi mô hình được khớp với một Transformer-XL được huấn luyện
trên các chuỗi có độ dài 1024 và f23;24;25;28glớp,
tương ứng. Bảng 8 cho thấy rằng mô hình của chúng tôi cải thiện một cách
nhất quán so với Transformer-XL trong tình huống được kiểm soát này.

Trong Bảng 9, sau đó chúng tôi so sánh mô hình Transformer-XL
sâu nhất (42 lớp) phù hợp với bộ nhớ với 4 mô hình Perceiver
AR với cùng độ dài ngữ cảnh và số lượng
lớp self-attention tương ứng f62;61;60;56g. Đáng chú ý,
chúng tôi không thể tăng số lượng lớp cho
một số mô hình AR—và đạt được sự khớp gần hơn trong tính toán—
mà không hết bộ nhớ. Thay vào đó, chúng tôi chạy các mô hình sâu nhấtMô hình Ngữ cảnh Độ sâu Bước/Giây Eval ppl.
T-XL 1024 42-lớp 1.17 13.253
AR 1024 62-lớp 1.19 12.849
AR 4096 61-lớp 1.21 12.680
AR 8192 60-lớp 1.25 12.660
AR 16384 56-lớp 1.26 12.816
Bảng 9. Kết quả Sách trên tập test, hiển thị bằng perplexity (ppl.),
thấp hơn là tốt hơn, từ một mô hình Transformer-XL 42 lớp và các mô hình Per-
ceiver AR với ngữ cảnh khác nhau và độ sâu được khớp cho
tính toán (bước/giây), được huấn luyện trong 500k bước.
có thể cho một độ dài ngữ cảnh cụ thể. Ngay cả trong tình huống này,
tất cả các mô hình Perceiver AR đều hoạt động tốt hơn so với
Transformer-XL sâu nhất.
5.5. Wikitext-103
Chúng tôi đánh giá thêm trên bộ dữ liệu Wikitext-103 (Merity et al., 2017),
một benchmark mô hình hóa ngôn ngữ cấp từ được sử dụng phổ biến.
Bộ dữ liệu bao gồm 28,475 bài viết Wikipedia
chứa từ 68 đến 26,993 từ, trung bình ở
3.6k từ. Wikitext-103 là một bộ dữ liệu nhỏ mà việc
regularization mạnh là cần thiết để ngăn ngừa overfitting nghiêm trọng và
để đạt được hiệu suất tốt. Tuy nhiên, chúng tôi đạt được kết quả
cạnh tranh, cho thấy tính hữu ích của Perceiver AR ngay cả đối với
các bộ dữ liệu tương đối nhỏ.

Mô hình Valid ppl. Test ppl.
Adaptive inputs 18.0 18.7
Transformer-XL 18.3 18.2
Shortformer 17.5 18.15
Compressive Transformer 16.0 17.1
Routing Transformer - 15.8
Transformer-XL (của chúng tôi) 17.58 18.42
Perceiver AR (1024) 17.86 18.52
Perceiver AR (2048) 17.60 18.35
Perceiver AR (4096) 17.66 18.25
Perceiver AR (8192) 17.58 18.37
Bảng 10. Kết quả trên benchmark mô hình hóa ngôn ngữ Wikitext-103. Kết quả baseline được tái tạo từ các bài báo gốc:
Adaptive inputs (Baevski & Auli, 2018), Transformer-XL (Dai
et al., 2019), Shortformer (Press et al., 2021), Compressive Trans-
former (Rae et al., 2019), và Routing Transformer (Roy et al.,
2021). Transformer-XL (của chúng tôi) là một implementation lại trong code-
base của chúng tôi. Chúng tôi huấn luyện các mô hình Perceiver AR với độ dài ngữ cảnh khác nhau.
Chúng tôi hiển thị kết quả trong Bảng 10. Perceiver AR hoạt động ngang bằng
với Transformer-XL Large (Dai et al., 2019) và Short-
former (Press et al., 2021). Chúng tôi cũng bao gồm các kết quả state of the
art trên Wikitext-103 (được huấn luyện chỉ trên Wikitext-103)
(Rae et al., 2019; Roy et al., 2021). Việc tăng độ dài ngữ cảnh
từ 1,024 lên 2,048 cho Perceiver AR có cải thiện
perplexity nhưng việc tăng thêm lên 4,096 hoặc 8,192 lại có hại-

--- TRANG 9 ---
Perceiver AR
Mô hình MAESTRO Test Validation
Music Transformer v1 - 1.84
Perceiver AR v1 1.82 1.82
Perceiver AR v3 1.91 1.90
Bảng 11. Kết quả trên tạo âm nhạc ký hiệu MAESTRO trên
bộ dữ liệu Test và Validation. Kết quả được hiển thị bằng nega-
tive log-likelihood, thấp hơn là tốt hơn. Kết quả baseline được tái tạo
từ (Hawthorne et al., 2019).
ful. Hiệu ứng này đã được lưu ý trong công trình trước (Press et al.,
2021; Sun et al., 2021) và cung cấp thêm bằng chứng rằng
hiệu suất mô hình ngôn ngữ hiện tại trên các benchmark nhỏ
không bị cản trở bởi ngữ cảnh phạm vi hạn chế của chúng.
5.6. MAESTRO
Chúng tôi cũng đánh giá Perceiver AR trong miền âm nhạc,
với các mẫu có sẵn trong supplement trực tuyến ( https:
//bit.ly/3uF5LJg ). Ở đây, việc mô hình hóa các phụ thuộc tầm xa
là thiết yếu để đạt được các biểu diễn tốt
(Huang et al., 2019). Chúng tôi sử dụng bộ dữ liệu MAESTRO
(Hawthorne et al., 2019), chứa khoảng
200 giờ âm nhạc trong cả phương thức ký hiệu (MIDI) và
âm thanh. Đối với dữ liệu ký hiệu, chúng tôi thử nghiệm với
cả phiên bản v1 và v3 của MAESTRO. Phiên bản trước cho phép
chúng tôi so sánh với state-of-the-art hiện có; phiên bản sau
là một tập cải tiến với khoảng 10% biểu diễn nhiều hơn. Chúng tôi sử dụng
token hóa MIDI được mô tả trong Mục
A.2 của Huang et al. (2019).

Sau khi huấn luyện trong 1M bước với ngữ cảnh đầu vào 4096 to-
ken và 2048 latent trong 12 lớp self-attention, mô hình của chúng tôi
đạt được negative log-likelihood thấp hơn (Bảng 11) so với Mu-
sic Transformer (Huang et al., 2019; Hawthorne et al., 2019).
Mô hình đó có 6 lớp và được huấn luyện trên các cắt ngẫu nhiên gồm
2048 token, nhưng hưởng lợi từ augmentation dữ liệu, mà
của chúng tôi thì không. Chúng tôi cũng báo cáo kết quả trên MAESTRO v3.

Đối với dữ liệu âm thanh, chúng tôi tạo ra các embedding được vector-quantized
sử dụng codec âm thanh neural SoundStream (Zeghidour
et al., 2021) ở một số bitrate. Các codec bitrate thấp hơn mô hình
cấu trúc thô hơn và cho phép huấn luyện trên cửa sổ thời gian dài hơn
cho một độ dài ngữ cảnh cố định, nhưng với cái giá của độ trung thực âm thanh thấp hơn. Chúng tôi hiển thị kết quả trong Bảng 12.
5.7. Mẫu âm nhạc
Để thể hiện tính nhất quán dài hạn của những mô hình này, chúng tôi
giới thiệu một nhiệm vụ ký hiệu khác liên quan đến một bộ dữ liệu lớn hơn, riêng tư
của các buổi biểu diễn piano (Simon et al., 2019). Những dữ liệu này
được phiên âm từ 10,000+ giờ âm thanh, sử dụng một biến thể
của mô hình Onsets and Frames (Hawthorne et al.,
2018). Từ bộ dữ liệu này, chúng tôi chỉ sử dụng các mảnh dẫn đến
1024–32,768 token. Các mẫu ngắn hơn giới hạn dướiSoundStream bitrate Ngữ cảnh Test Validation
12kbps 54.4s 2.49 2.34
18kbps 36.8s 2.60 2.57
22kbps 29.6s 2.65 2.62
Bảng 12. Kết quả negative log-likelihood của Perceiver AR trên tạo âm thanh Sound-
Stream, cho độ dài ngữ cảnh cố định 65536.
không chắc chắn chứa nội dung bài hát; ở đầu kia,
chỉ có khoảng 200 mảnh dài hơn 32,768 token.
Cùng token hóa như đối với MAESTRO được sử dụng. Chúng tôi huấn luyện
một mô hình với 1024 latent và 24 lớp self-attention trên
ngữ cảnh đầu vào 32,768 token, đạt được negative log-
likelihood 1.24 trên tập test.

Chúng tôi tạo ra các mẫu từ các mô hình được huấn luyện trên nhiệm vụ mới này,
cũng như các mẫu từ các mô hình MAESTRO v3 ký hiệu và
SoundStream. Các mẫu được đạt được từ bộ dữ liệu phiên âm lớn
thể hiện tính nhất quán phong cách và cấu trúc kéo dài vài phút,
chứa các chủ đề âm nhạc lặp lại, mẫu hợp âm, arpeggio và thậm chí ritardando. Các mẫu miền âm thanh thể hiện cùng sự đánh đổi của độ trung thực âm thanh
và cấu trúc dài hạn thấy trong phần trước,
nhưng chứng minh sự tương ứng mạnh với miền
gốc. Các mẫu âm thanh được render có sẵn trong
supplement trực tuyến: https://bit.ly/3uF5LJg .
6. Kết luận
Chúng tôi giới thiệu Perceiver AR, một kiến trúc được thiết kế cho
mô hình hóa tự hồi quy ngữ cảnh dài. Perceiver AR mở rộng quy mô
đến kích thước đầu vào dài hơn so với các kiến trúc thường được sử dụng
trong thực tế (Transformers và Transformer-XL), trong khi mở rộng quy mô
đến độ sâu cần thiết cho ước lượng mật độ trên dữ liệu
thế giới thực. Perceiver AR tách rời yêu cầu tính toán
của việc xử lý nhiều đầu vào khỏi những yêu cầu của việc xây dựng
các mạng sâu. Điều này cho chúng ta kiểm soát nhiều hơn về lượng
tính toán được sử dụng cho một mô hình nhất định tại thời điểm kiểm tra và cho phép chúng ta
đánh đổi mượt mà tốc độ với hiệu suất. Perceiver
AR tạo ra kết quả tốt trên một số miền. Các thí nghiệm của chúng tôi cho thấy rằng ngữ cảnh lớn hơn được sử dụng bởi Perceiver
AR có thể mở ra cánh cửa cho các chiến lược sắp xếp tự hồi quy
linh hoạt hơn. Perceiver AR là một ứng cử viên tốt cho một
mô hình tự hồi quy đa mục đích, ngữ cảnh dài.
7. Lời cảm ơn
Cảm ơn Chitwan Saharia vì sự giúp đỡ với SR3 upsampling,
Christos Kaplanis vì hướng dẫn về Sách, và Jordan Hoff-
mann và Richard Tanburn vì sự giúp đỡ với RoPE. Cảm ơn
Daniel Toyama, Douglas Eck, Adam Roberts, Nando de Fre-
itas, Dani Yogatama, Josh Gardner, Catalin Ionescu, Skanda
Koppula, Rabeeh Karimi Mahabadi, Ethan Manilow, Vior-
ica Patraucean, Oleh Rybkin, Nikolay Savinov, và những người khác
tại DeepMind và Google Research vì các đề xuất.

--- TRANG 10 ---
Perceiver AR
Tài liệu tham khảo
Baevski, A. and Auli, M. Adaptive input representations
for neural language modeling. In Proceedings of the
International Conference on Learning Representations
(ICLR) , 2018.
Bengio, Y ., Ducharme, R., Vincent, P., and Jauvin, C. A
neural probabilistic language model. Journal of Machine
Learning Research (JMLR) , 2003.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165 , 2020.
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D.,
and Sutskever, I. Generative pretraining from pixels. In
Proceedings of the International Conference on Machine
Learning (ICML) , 2020.
Child, R., Gray, S., Radford, A., and Sutskever, I. Gener-
ating long sequences with sparse Transformers. arXiv
preprint arXiv:1904.10509 , 2019.
Choromanski, K. M., Likhosherstov, V ., Dohan, D., Song,
X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mo-
hiuddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J.,
and Weller, A. Rethinking attention with Performers. In
Proceedings of the International Conference on Learning
Representations (ICLR) .
Clark, J. H., Garrette, D., Turc, I., and Wieting, J. CANINE:
pre-training an efﬁcient tokenization-free encoder for lan-
guage representation. arXiv preprint arXiv:2103.06874 ,
2021.
Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and
Salakhutdinov, R. Transformer-XL: Attentive language
models beyond a ﬁxed-length context. In Proceedings
of the Annual Meetings of the Association for Computa-
tional Linguistics (ACL) , 2019.
Dai, Z., Lai, G., Yang, Y ., and Le, Q. Funnel-Transformer:
Filtering out sequential redundancy for efﬁcient language
processing. In Proceedings of Neural Information Pro-
cessing Systems (NeurIPS) , 2020.
Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,
and Sutskever, I. Jukebox: A generative model for music.
arXiv preprint arXiv:2005.00341 , 2020.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. In
Proceedings of the International Conference on Learning
Representations (ICLR) , 2021.Graves, A. Generating sequences with recurrent neural
networks. arXiv preprint arXiv:1308.0850 , 2013.
Graves, A., Wayne, G., and Danihelka, I. Neural Turing
Machines. arXiv preprint arXiv:1410.5401 , 2014.
Graves, A., Wayne, G., Reynolds, M., Harley, T., Dani-
helka, I., Grabska-Barwi ´nska, A., Colmenarejo, S. G.,
Grefenestette, E., Ramalho, T., Agapiou, J., Badia, A. P.,
Hermann, K. M., Zwols, Y ., Ostrovski, G., Cain, A., King,
H., Summerﬁeld, C., Blunsom, P., Kavukcuoglu, K., and
Hassabis, D. Hybrid computing using a neural network
with dynamic external memory. Nature , 538:471–476,
2016.
Gu, A., Goel, K., and R ´e, C. Efﬁciently modeling long
sequences with structured state spaces. arXiv preprint
arXiv:2111.00396 , 2021.
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers,
R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,
Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van
Kerkwijk, M. H., Brett, M., Haldane, A., del R ´ıo, J. F.,
Wiebe, M., Peterson, P., G ´erard-Marchant, P., Sheppard,
K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C.,
and Oliphant, T. E. Array programming with NumPy.
Nature , 585(7825):357–362, 2020.
Hawthorne, C., Elsen, E., Song, J., Roberts, A., Simon, I.,
Raffel, C., Engel, J., Oore, S., and Eck, D. Onsets and
frames: Dual-objective piano transcription. In Proceed-
ings of the International Society for Music Information
Retrieval Conference (ISMIR) , 2018.
Hawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang,
C.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D.
Enabling factorized piano music modeling and genera-
tion with the MAESTRO dataset. In Proceedings of the
International Conference on Learning Representations
(ICLR) , 2019.
Hendrycks, D. and Gimpel, K. Gaussian error linear units
(GELUs). arXiv preprint arXiv:1606.08415 , 2016.
Hessel, M., Budden, D., Viola, F., Rosca, M., Sezener,
E., and Hennigan, T. Optax: composable gradient
transformation and optimisation, in JAX!, 2020. URL
http://github.com/deepmind/optax .
Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N.,
Simon, I., Hawthorne, C., Dai, A. M., Hoffman, M. D.,
Dinculescu, M., and Eck, D. Music Transformer: Gen-
erating music with long-term structure. In Proceedings
of the International Conference on Learning Representa-
tions (ICLR) , 2019.

--- TRANG 11 ---
Perceiver AR
Jaegle, A., Gimeno, F., Brock, A., Zisserman, A., Vinyals,
O., and Carreira, J. Perceiver: General perception with
iterative attention. In Proceedings of the International
Conference on Machine Learning (ICML) , 2021.
Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C.,
Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock,
A., Shelhamer, E., Henaff, O., Botvinick, M. M., Zisser-
man, A., Vinyals, O., and Carreira, J. Perceiver IO: A
general architecture for structured inputs & outputs. In
Proceedings of the International Conference on Learning
Representations (ICLR) , 2022.
Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M.,
Ronneberger, O., Tunyasuvunakool, K., Bates, R., Z ´ıdek,
A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S.
A. A., Ballard, A. J., Cowie, A., Romera-Paredes, B.,
Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S.,
Reiman, D., Clancy, E., Zielinski, M., Steinegger, M.,
Pacholska, M., Berghammer, T., Bodenstein, S., Silver,
D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli,
P., and Hassabis, D. Highly accurate protein structure
prediction with AlphaFold. Nature , 596:583–589, 2021.
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are RNNs: Fast autoregressive Transform-
ers with linear attention. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML) , 2020.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. In Proceedings of the International Confer-
ence on Learning Representations (ICLR) , 2015.
Kingma, D. P., Salimans, T., Poole, B., and Ho, J. On
density estimation with diffusion models. In Proceedings
of Neural Information Processing Systems (NeurIPS) ,
2021.
Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The
efﬁcient transformer. In Proceedings of the International
Conference on Learning Representations (ICLR) , 2020.
Kudo, T. and Richardson, J. SentencePiece: A simple and
language independent subword tokenizer and detokenizer
for neural text processing. In Proceedings of the Annual
Meetings of the Association for Computational Linguis-
tics (ACL) , 2018.
Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y ., Polyak,
A., Bolte, B., Nguyen, T.-A., Copet, J., Baevski, A.,
Mohamed, A., and Dupoux, E. Generative spoken
language modeling from raw audio. arXiv preprint
arXiv:2102.01192 , 2021.
Lee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., and Teh,
Y . W. Set Transformer: A framework for attention-based
permutation-invariant neural networks. In Proceedingsof the International Conference on Machine Learning
(ICML) , 2019.
Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R.,
Kaiser, L., and Shazeer, N. Generating Wikipedia by
summarizing long sequences. In Proceedings of the Inter-
national Conference on Learning Representations (ICLR) ,
2018.
Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., and
Zettlemoyer, L. LUNA: Linear uniﬁed nested attention.
InProceedings of Neural Information Processing Systems
(NeurIPS) , 2021.
Mehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S.,
Sotelo, J., Courville, A., and Bengio, Y . SampleRNN: An
unconditional end-to-end neural audio generation model.
InProceedings of the International Conference on Learn-
ing Representations (ICLR) , 2017.
Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
sentinel mixture models. In Proceedings of the Interna-
tional Conference on Learning Representations (ICLR) ,
2017.
Nash, C., Menick, J., Dieleman, S., and Battaglia, P. W.
Generating images with sparse representations. In Pro-
ceedings of the International Conference on Machine
Learning (ICML) , 2021.
Nawrot, P., Tworkowski, S., Tyrolski, M., Kaiser, L., Wu,
Y ., Szegedy, C., and Michalewski, H. Hierarchical
Transformers are more efﬁcient language models. arXiv
preprint arXiv:2110.13711 , 2021.
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,
N., and Kong, L. Random feature attention. In Pro-
ceedings of the International Conference on Learning
Representations (ICLR) , 2021.
Polyak, A., Adi, Y ., Copet, J., Kharitonov, E., Lakhotia,
K., Hsu, W.-N., Mohamed, A., and Dupoux, E. Speech
resynthesis from discrete disentangled self-supervised
representations. arXiv preprint arXiv:2104.00355 , 2021.
Press, O., Smith, N. A., and Lewis, M. Shortformer: Better
language modeling using shorter inputs. In Proceedings
of the Annual Meetings of the Association for Computa-
tional Linguistics (ACL) , 2021.
Rabe, M. N. and Staats, C. Self-attention does not need
O(n2)memory. arXiv preprint arXiv:2112.05682 , 2021.
Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and
Lillicrap, T. P. Compressive Transformers for long-range
sequence modelling. In Proceedings of the International
Conference on Learning Representations (ICLR) , 2019.

--- TRANG 12 ---
Perceiver AR
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,
Song, F., Aslanides, J., Henderson, S., Ring, R., Young,
S., Rutherford, E., Hennigan, T., Menick, J., Cassirer,
A., Powell, R., van den Driessche, G., Hendricks, L. A.,
Rauh, M., Huang, P.-S., Glaese, A., Welbl, J., Dathathri,
S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell,
A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.,
Buchatskaya, E., Budden, D., Sutherland, E., Simonyan,
K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun-
coro, A., Nematzadeh, A., Gribovskaya, E., Donato, D.,
Lazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli,
M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M.,
Pohlen, T., Gong, Z., Toyama, D., de Masson d'Autume,
C., Li, Y ., Terzi, T., Mikulik, V ., Babuschkin, I., Clark,
A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J.,
Johnson, M., Hechtman, B., Weidinger, L., Gabriel, I.,
Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C.,
Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis,
D., Kavukcuoglu, K., and Irving, G. Scaling language
models: Methods, analysis & insights from training Go-
pher. arXiv preprint arXiv:2112.11446 , 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a uniﬁed text-to-text
Transformer. Journal of Machine Learning Research
(JMLR) , 2020.
Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y . ZeRO:
Memory optimizations toward training trillion parameter
models. In Proceedings of the International Conference
for High Performance Computing, Networking, Storage
and Analysis (SC) , 2020.
Ramesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Rad-
ford, A., Chen, M., and Sutskever, I. Zero-shot text-
to-image generation. arXiv preprint arXiv:2102.12092 ,
2021.
Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur-
mans, D., and Dai, B. Combiner: Full attention Trans-
former with sparse computation cost. In Proceedings of
Neural Information Processing Systems (NeurIPS) , 2021.
Rosenfeld, R. Two decades of statistical language modeling:
where do we go from here? Proceedings of the IEEE , 88
(8):1270–1278, 2000.
Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient
content-based sparse attention with Routing Transform-
ers.Transactions of the Association for Computational
Linguistics (TACL) , 9, 2021.
Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J.,
and Norouzi, M. Image super-resolution via iterative
reﬁnement. arXiv preprint arXiv:2104.07636 , 2021.Schmidhuber, J. and Heil, S. Sequential neural text com-
pression. IEEE Transactions on Neural Networks , 7(1):
142–146, 1994.
Shazeer, N. Fast Transformer decoding: one write-head is
all you need. arXiv preprint arXiv:1911.02150 , 2019.
Simon, I., Huang, C.-Z. A., Engel, J., Hawthorne, C.,
and Dinculescu, M. Generating piano music with
transformer. 2019. URL https://magenta.
tensorflow.org/piano-transformer .
So, D. R., Ma ´nke, W., Liu, H., Dai, Z., Shazeer, N., and
Le, Q. V . Primer: Searching for efﬁcient Transformers
for language modeling. arXiv preprint arXiv:2109.08668 ,
2021.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overﬁtting. Journal of Machine
Learning Research (JMLR) , 2014.
Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . RoFormer:
Enhanced Transformer with rotary position embedding.
arXiv preprint arxiv:2104.09864 , 2021.
Sun, S., Krishna, K., Mattarella-Micke, A., and Iyyer, M. Do
long-range language models actually use long-range con-
text? In Proceedings of the Annual Conference on Empir-
ical Methods in Natural Language Processing (EMNLP) ,
2021.
Sutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-
quence learning with neural networks. In Proceedings of
Neural Information Processing Systems (NeurIPS) , 2014.
Uria, B., C ˆot´e, M.-A., Gregor, K., Murray, I., and
Larochelle, H. Neural autoregressive distribution esti-
mation. Journal of Machine Learning Research (JMLR) ,
2016.
van den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,
Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and
Kavukcuoglu, K. WaveNet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499 , 2016a.
van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K.
Pixel recurrent neural networks. In Proceedings of the
International Conference on Machine Learning (ICML) ,
2016b.
van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural
discrete representation learning. In Proceedings of Neural
Information Processing Systems (NeurIPS) , 2017.
Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez,
A. N., Gouws, S., Jones, L., Kaiser, L., Kalchbrenner,
N., Parmar, N., Sepassi, R., Shazeer, N., and Uszkoreit,

--- TRANG 13 ---
Perceiver AR
J. Tensor2tensor for neural machine translation. arXiv
preprint arXiv:1803.07416 .
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention
is all you need. In Proceedings of Neural Information
Processing Systems (NeurIPS) , 2017.
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,
Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T.,
Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I.,
Huang, A., Sifre, L., Cai, T., Agapiou, J. P., Jaderberg, M.,
Vezhnevets, A. S., Leblond, R., Pohlen, T., Dalibard, V .,
Budden, D., Sulsky, Y ., Molloy, J., Paine, T. L., Gulcehre,
C., Wang, Z., Pfaff, T., Wu, Y ., Ring, R., Yogatama,
Dani W ¨unsch, D., McKinney, K., Smith, O., Schaul, T.,
Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Chris, A.,
and Silver, D. Grandmaster level in StarCraft II using
multi-agent reinforcement learning. Nature , 575(7782):
350–354, 2019.
Wang, B. Mesh transformer Jax, 2021. URL
https://github.com/kingoflolz/
mesh-transformer-jax .
Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.
Linformer: Self-attention with linear complexity. arXiv
preprint arXiv:2006.04768 , 2020.
Weston, J., Chopra, S., and Bordes, A. Memory networks. In
Proceedings of the International Conference on Learning
Representations (ICLR) , 2015.
Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y ., Jiang, D., and
Duan, N. N ¨UWA: Visual synthesis pre-training for neural
visual world creation. arXiv preprint arXiv:2111.12417 ,
2021.
Wu, F., Fan, A., Baevski, A., Dauphin, Y . N., and Auli, M.
Pay less attention with lightweight and dynamic convolu-
tions. arXiv preprint arXiv:1901.10430 , 2019.
Wu, Y ., Rabe, M. N., Hutchins, D., and Szegedy, C. Mem-
orizing transformers. In Proceedings of the Interna-
tional Conference on Learning Representations (ICLR) ,
2022. URL https://openreview.net/forum?
id=TrjbxzRcnf- .
Xiong, R., Yang, Y ., He, D., Zheng, K., Zheng, S., Xing,
C., Zhang, H., Lan, Y ., Wang, L., and Liu, T.-Y . On
layer normalization in the Transformer architecture. In
Proceedings of the International Conference on Machine
Learning (ICML) , 2020.
Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-
berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,
Yang, L., et al. Big Bird: Transformers for longer se-
quences. Proceedings of Neural Information Processing
Systems (NeurIPS) , 33, 2020.Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and
Tagliasacchi, M. SoundStream: An end-to-end neural
audio codec. IEEE/ACM Transactions on Audio, Speech,
and Language Processing (TASLP) , 2021.

--- TRANG 14 ---
Perceiver AR
A. Mẫu ImageNet
Các batch đầy đủ của những hình ảnh được tạo ra được sử dụng để tạo nên Hình
4 có thể được thấy trong Hình 5 đến 8. Tất cả hình ảnh được tạo
ra với nhiệt độ 1:0. Chúng tôi cũng hiển thị các phiên bản upsampled
của những hình ảnh được tạo ra với 1536 latent
bằng cách sử dụng SR3 (Saharia et al., 2021) để đạt được độ phân giải
256×256 trong Hình 9.
B. Sách
Như đã thảo luận trong Mục 5.4, chúng tôi huấn luyện tất cả các mô hình trên
bộ dữ liệu Sách với 1024 latent, 36 lớp và
f1024;4096;8192;16384gtoken ngữ cảnh đầu vào. Ngoài
kết quả được hiển thị trong bài báo chính ở stride 512,
chúng tôi cũng đánh giá hiệu suất trên tập test gồm 100 cuốn sách
như một hàm của stride (Phụ lục G), xem xét 5 giá trị:
f16;64;128;512;1024g(Hình 10).

Chúng tôi hướng sự chú ý của người đọc đến hai hiệu ứng ở đây. Thứ nhất ,
trong khi perplexity ở một độ dài ngữ cảnh nhất định tương đối
ổn định cho stride≤128, perplexity liên tục tăng
với stride. Khi sử dụng đánh giá có stride, các token
đầu tiên trong cửa sổ ngữ cảnh của mô hình thấy một số lượng tương đối nhỏ
các token đi trước. Với một stride đánh giá 1024,
token đầu tiên trong cửa sổ ngữ cảnh của một mô hình
1024-context chỉ thấy một token đi trước. Tính chất này có thể
chịu trách nhiệm cho khoảng cách tăng giữa các mô hình 1024-context
và các mô hình ngữ cảnh lớn hơn khi stride tăng:
lợi ích perplexity di chuyển từ lợi ích perplexity của mô hình 16384- so với mô hình 1024- là 0.8 ở stride 1024, nhưng chỉ
0.26 ở stride 16.

16 64 128 512 1024
Stride đánh giá14.5014.7515.0015.2515.5015.7516.00Perplexity
  1024 ngữ cảnh
  4096 ngữ cảnh
  8192 ngữ cảnh
16384 ngữ cảnh
Hình 10. Kết quả perplexity trên tập test Sách, từ 4 mô hình
Perceiver AR 36 lớp khác nhau với ngữ cảnh 1024-, 4096-, 8192- và 16384-,
tương ứng. Đánh giá được thực hiện cho 5 giá trị stride.Thứ hai , bất kể stride đánh giá, mỗi lần tăng gấp đôi
ngữ cảnh cải thiện perplexity, nhưng với lợi ích giảm dần: khoảng cách giữa ngữ cảnh 1024 và 4096 liên tục
lớn hơn so với giữa 4096 và 8192 hoặc 8192
và 16384 ngữ cảnh. Các mô hình Perceiver AR với cùng
độ sâu và kích thước ngữ cảnh khác nhau sử dụng cùng số lượng
tham số (với ngoại lệ có thể có của sự khác biệt trong
các tham số của encoding vị trí). Chúng tôi tin rằng hiệu ứng này
chỉ ra nhu cầu về các mô hình dung lượng lớn hơn để khai thác
thông tin tăng trong ngữ cảnh dài hơn. Tuy nhiên,
xu hướng tổng thể cho thấy rằng ngữ cảnh lớn hơn dẫn đến kết quả
cải thiện, ngay cả khi sử dụng về cơ bản cùng dung lượng mô hình.
C. Cái nhìn chi tiết hơn về nội tại của Perceiver AR
Giống như Perceiver và Perceiver IO, Perceiver AR được xây dựng trên
các khối attention kiểu Transformer. Perceivers sử dụng hai loại
attention: cross- và self-attention. Hai loại attention này
chia sẻ một giao diện—cả hai đều nhận vào hai mảng và
trả về một mảng thứ ba—nhưng khác nhau ở những gì chúng truyền đến giao diện đó.

Phóng to: QKV attention nhận vào một đầu vào key-value
XKV2RMCvà một đầu vào query XQ2RND(trong đó C
vàDchỉ số lượng kênh). Đầu ra của QKV
attention là một mảng với cùng chiều chỉ số (đầu tiên)
như đầu vào query và một chiều kênh (thứ hai) được xác định
bởi một phép chiếu đầu ra:
Q=fQ(XQ);K=fK(XKV);V=fV(XKV)(4)
Xpre
QK=QKT(5)
XQK=softmax (Xpre
QK=p
F)(6)
Attn(XQ; XKV) =XQKV =fO(XQKV);(7)
trong đó Xpre
QKvàXQKlà mảng của các bản đồ attention trước và sau softmax
2RNM, vàXQKVlà một mảng2RND.
Các hàm ffQ;K;Vglà các lớp tuyến tính ánh xạ mỗi đầu vào
đến một chiều đặc trưng được chia sẻ FvàfOlà một lớp tuyến tính
chiếu đầu ra đến một chiều kênh mục tiêu D, thường
có cùng kích thước như của XQ. Tất cả các lớp tuyến tính được áp dụng
convolutionally trên chiều chỉ số. Chúng tôi đã bỏ qua
các chiều batch và head (trong trường hợp attention đa head) để dễ đọc.

Trong các khối attention Perceiver AR, QKV attention được theo sau
bởi một MLP hai lớp với một hàm phi tuyến squared ReLU
theo sau lớp đầu tiên. Mô-đun đầy đủ có cấu trúc
sau:

--- TRANG 15 ---
Perceiver AR
Hình 5. Batch đầy đủ của các mẫu được tạo ra từ mô hình được huấn luyện trên ImageNet sử dụng 16 latent trong quá trình suy luận.
XQKV =Attn(layerNorm (XQ);layerNorm (XKV))(8)
XQKV =XQKV +XQ (9)
XQKV =XQKV +MLP (layerNorm (XQKV)); (10)
lạm dụng ký hiệu một chút để đơn giản và nhấn mạnh
cấu trúc residual. "Attn" đề cập đến QKV như được mô tả
ở trên.

Phóng ra: Khi được thảo luận trong văn bản chính, các phép toán
CrossAttend :XKVXQ!XQvàSelfAttend :
XKVXQ!XQđề cập đến hệ thống phương trình đầy đủ
được đưa ra trong Phương trình (8) đến (10).

Hai phép toán này chỉ khác nhau ở chỗ XQ6=XKVcho
cross-attention (với N < M cho cross-attention "encoder"
được xem xét ở đây) và XQ=XKV cho self-
attention. Cross-attention được sử dụng để giảm hình dạng
của một mảng đầu vào và self-attention để giữ nguyên
hình dạng. Trong Perceiver và Perceiver IO, đầu vào query XQ
của cross-attention ban đầu thường được học (các phần tử của nó
là "latent được học"), trong khi trong Perceiver AR, nó thường được
xây dựng1bằng cách lấy Nphần tử cuối của mảng đầu vào:
XQ=XKV[N:;:], sử dụng ký hiệu indexing kiểu NumPy
(Harris et al., 2020). Trong cả hai trường hợp, việc sử dụng ít latent hơn
đầu vào là thiết yếu để kiểm soát chi phí tính toán và bộ nhớ
trong khi giữ các đầu vào ngữ cảnh dài.

Perciever AR cũng khác với Perceiver và Perceiver IO trong
việc sử dụng cross- và self-attention được che dấu nhân quả. Trong các mặt nạ self-
attention, tất cả các phần tử của Xpre
QKtại các chỉ số (m0; m)
(queries←keys), trong đó m0; m2[0; M)vàm > m0được
che dấu. Trong mặt nạ cross-attention, để bù đắp cho
thực tế rằng các latent được đặt tại các vị trí chỉ số cuối, tất cả
các phần tử của Xpre
QKtại các chỉ số (n; m)(queries←keys), trong đó
n2[0; N); m2[0; M)vàm > n +MN1được
che dấu. Điều này ngăn các truy vấn "sớm hơn" chú ý đến
các key "muộn hơn". Việc che dấu nhân quả được thực hiện trong attention
bằng cách nhân tất cả các kết nối được che dấu trong bản đồ attention
trước softmax Xpre
QKvới1.
1Ngoại lệ cho các thí nghiệm trên Wikitext-103, nơi các latent
được học được sử dụng.Nếu chúng ta ký hiệu cross- và self-attention được che dấu nhân quả bởi
CrossAttend cmvà SelfAttend cm, tương ứng, Perceiver
AR (không có latent được học2) được đưa ra bởi:
Z0 CrossAttend cm(X; X [N:;:]) (11)
Zl+1 SelfAttend cm(Zl; Zl); (12)
trong đó Phương trình (12) được áp dụng một lần cho mỗi lớp self-attend.
Đầu ra cuối cùng được thu được bằng cách layer-norm và chiếu
ZLđến kích thước từ vựng, theo sau bởi một softmax để tạo ra
logit đầu ra.
D. Công trình liên quan thêm
Trong phần này chúng tôi mô tả background bổ sung với
mục tiêu làm rõ bối cảnh vấn đề và phương pháp của Perceiver AR.
D.1. Attention hiệu quả
Nhiều biến thể Transformer gần đây tìm cách tránh yêu cầu bộ nhớ O(N2)
của self-attention. Điều này thường được thực hiện
bằng cách giới thiệu tính thưa thớt khi tính toán ma trận attention
– như trong Sparse Transformer (Child et al., 2019), Big
Bird (Zaheer et al., 2020), và Combiner (Ren et al., 2021)
– hoặc bằng cách xấp xỉ tính toán này với chi phí thấp hơn – ví dụ
như trong Linear Transformer (Katharopoulos et al., 2020), Lin-
former (Wang et al., 2020), Reformer (Kitaev et al., 2020),
Random-Feature Attention (Peng et al., 2021), và Per-
former (Choromanski et al.).

Nhược điểm của các phương pháp sử dụng tính thưa thớt là tính thưa thớt này
phải được điều chỉnh thủ công hoặc được tạo ra với các heuristic thường
đặc biệt theo miền và có thể khó điều chỉnh. Ngược lại,
công trình của chúng tôi không buộc một mẫu thưa thớt được tạo thủ công lên
các lớp attention, mà thay vào đó cho phép mạng học những
đầu vào ngữ cảnh dài nào cần chú ý và lan truyền qua
2Đối với latent, thay thế đầu vào thứ hai (query) vào RHS của
Phương trình (11) bằng Z0(hoặcZ1nếu bạn thích), được học.

--- TRANG 16 ---
Perceiver AR
Hình 6. Batch đầy đủ của các mẫu được tạo ra từ mô hình được huấn luyện trên ImageNet sử dụng 1024 latent trong quá trình suy luận, giống như
số lượng latent được sử dụng trong quá trình huấn luyện mô hình.
mạng. Phép toán cross-attend ban đầu, giảm
số lượng vị trí trong chuỗi, có thể được xem như một
dạng tính thưa thớt được học.

Bởi vì Perceiver AR không phụ thuộc vào các mẫu thưa thớt được điều chỉnh thủ công
hoặc ví dụ như các mẫu dilation có cấu trúc như những cái
được sử dụng trong WaveNet (van den Oord et al., 2016a), nó có thể mô hình
các mẫu phụ thuộc phức tạp tùy ý giữa bất kỳ đầu vào nào của nó
ngay lập tức sau cross-attend. Các mô hình với các mẫu
thưa thớt cố định, mặt khác, thường yêu cầu một số
lớp (phụ thuộc logarithmically vào khoảng cách
giữa các điểm trong mảng đầu vào trong trường hợp của WaveNet)
hoặc các liên hợp chính xác và dễ vỡ của các trường tiếp nhận đầu vào
(trong trường hợp của các mẫu thưa thớt được điều chỉnh thủ công) để cho phép một
tập hợp điểm nhất định tương tác. Hiệu ứng ròng của tình huống này là
việc xử lý hiệu quả được sử dụng để xử lý một tập hợp đặc trưng nhất định
nhỏ hơn nhiều trong những kiến trúc này so với các kiến trúc được kết nối dày đặc
như Perceiver AR.
D.2. Các kiến trúc hiệu quả khác
Các mô hình dựa trên bộ nhớ. Kích thước ngữ cảnh hiệu quả dài hơn cũng có thể
được đạt được bằng cách giảm yêu cầu tính toán trên
các token ở xa trong quá khứ bằng cách sử dụng stop gradient (Dai et al.,
2019), recurrence (Mehri et al., 2017), bộ nhớ (Weston
et al., 2015; Rae et al., 2019), hoặc các dạng nén khác (Wu et al., 2022) để xử lý cấu trúc dài hạn. Những
chiến lược này thường áp đặt các nút thắt cổ chai với cấu trúc địa phương,
hạn chế tính linh hoạt mà ngữ cảnh có thể được khai thác
bởi một mục tiêu nhất định. Mặc dù Perceiver AR thực hiện
xử lý sử dụng các latent ít hơn số lượng
đầu vào, mỗi latent được cung cấp quyền truy cập trực tiếp đến tất cả đầu vào, thay vì
giao tiếp với quá khứ thông qua một cơ chế hẹp hoặc được
tính toán trước.

Các lựa chọn thay thế cho attention. Hiệu quả cũng có thể được đạt được
bằng cách sử dụng các kiến trúc được điều chỉnh theo miền như convolution
nhẹ hoặc động (Wu et al., 2019). S4 được giới thiệu gần đây
(Gu et al., 2021) là một mô hình rất hiệu quả
tránh hoàn toàn attention trong khi tạo ra kết quả thú vị,
nhưng nó chưa cạnh tranh trên các bộ dữ liệu tiêu chuẩn như
WikiText-103.

Nhiều hiểu biết được trình bày trong công trình trước này là bổ
sung cho thiết kế kiến trúc của Perceiver AR, và chúng tôi
mong đợi chúng có thể được lai tạo để tạo ra các mô hình hiệu quả hơn nữa
phù hợp cho thiết kế mô hình đa mục đích, quy mô lớn trong
công việc tương lai.
D.3. Token hóa đầu vào
Một cách tiếp cận khác để giảm yêu cầu bộ nhớ và tính toán
của self-attention là trực tiếp giảm độ dài

--- TRANG 17 ---
Perceiver AR
Hình 7. Batch đầy đủ của các mẫu được tạo ra từ mô hình được huấn luyện trên ImageNet sử dụng 1536 latent trong quá trình suy luận. Cùng một random
seed được sử dụng như khi tạo ra các hình ảnh với 1024 latent, điều này giải thích cách hình ảnh con chó trắng xuất hiện trong cả hai batch.

Hình 8. Batch đầy đủ của các mẫu được tạo ra từ mô hình được huấn luyện trên ImageNet sử dụng 2048 latent trong quá trình suy luận.
của dữ liệu đầu vào bằng cách sử dụng token hóa để nhóm nhiều
đầu vào thành các token đơn. Các cách tiếp cận như vậy đã dẫn đến
kết quả xuất sắc trên nhiều miền quan tâm khác nhau và
thường là một tính năng ngầm của các mảng dữ liệu trong các bộ dữ liệu
tiêu chuẩn.

Trong NLP, các đoạn subword thường được nhóm lại với nhau (Kudo
& Richardson, 2018) dựa trên tần suất của chúng trong một corpus
văn bản huấn luyện. Trong thị giác, công trình trước đã khám phá việc sử dụng K-
means để nhóm các giá trị RGB thành một token duy nhất (Chen et al.,
2020) hoặc để nhóm các pixel riêng lẻ thành các patch (Dosovitskiy
et al., 2021), đôi khi theo sau bởi quantization (Ramesh
et al., 2021). Những người khác cũng đã tận dụng các hệ số DCT
(Nash et al., 2021) như được sử dụng trong JPEG để chuyển đổi các hình ảnh
có kích thước cố định thành các chuỗi có độ dài biến đổi. Có thể được cho là
phương pháp token hóa được áp dụng rộng rãi nhất là vector
quantization neural, trong đó một mạng encoder được huấn luyện để ánh xạ
hình ảnh thành một bộ sưu tập được lấy mẫu xuống về mặt không gian của các mã
rời rạc (van den Oord et al., 2017; Ramesh et al., 2021).
Các kỹ thuật tương tự đã được phát triển cho âm thanh, trong đó
các encoding được vector-quantized của dạng sóng thô đã hiệu quả
được sử dụng như từ vựng cho tổng hợp giọng nói từ đầu đến cuối (Lakhotia et al., 2021; Polyak et al., 2021) và tạo
âm nhạc (Dhariwal et al., 2020). Codec SoundStream mà chúng tôi sử dụng trong bài báo này xây dựng dựa trên những kỹ thuật này bằng cách sử dụng
residual vector quantization và adversarial reconstruction
loss để đạt được độ trung thực âm thanh cao với ít token rời rạc hơn (Zeghidour et al., 2021).

Token hóa là một chiến lược hữu ích rộng rãi, nhưng nó có nhược điểm của nó. Đối với nhiều miền, các schemes token hóa hiệu quả
dựa vào nén có tổn thất. Dữ liệu bị loại bỏ trong quá trình token hóa, và các đầu vào không thể được khôi phục một cách chính xác. Cần
chú ý để đảm bảo rằng dữ liệu có thể được tái tạo ở một độ trung thực đủ cho ứng dụng yêu cầu. Các schemes nén neural
như VQ-VAE yêu cầu người dùng huấn luyện và duy trì các mạng encoder và decoder bổ sung, có thể cản trở việc áp dụng sẵn sàng cho các bộ dữ liệu và miền mới. Và, có lẽ quan trọng nhất, token hóa hiệu quả thường
được thiết kế và sử dụng theo cách đặc biệt theo miền, hạn chế khả năng thích ứng và mở rộng quy mô dễ dàng đến các miền mới. Bằng cách
mô hình hóa hiệu quả các chuỗi dài, PerceiverAR có thể
trong một số trường hợp loại bỏ nhu cầu token hóa và trong những trường hợp khác
có thể giảm nhu cầu nén có tổn thất nặng. Nhưng trong
tương lai gần, chúng tôi dự đoán rằng token hóa — dưới một dạng
này hay dạng khác — sẽ vẫn là một công cụ cần thiết để kết hợp
nhiều ngữ cảnh hơn.
E. Chi tiết bổ sung về các phương pháp
E.1. Sử dụng bộ nhớ
Nguồn sử dụng bộ nhớ lớn nhất trong một mô hình Per-
ceiver AR thường là bản đồ attention trong
lớp cross-attend ban đầu, dẫn đến một ma trận có kích thước
[heads ;input length ;selfattention length] . Đối với các thí nghiệm trong bài báo này mà ma trận này gây ra lỗi hết bộ nhớ
(Mục 5.1.1), chúng tôi thấy rằng việc xử lý các head attention
trong các nhóm phụ thay vì tất cả cùng một lúc là đủ để
giảm việc sử dụng bộ nhớ của chúng tôi. Sử dụng cách tiếp cận chunked
trong (Kitaev et al., 2020; Rabe & Staats, 2021; Jumper et al.,
2021) cho lớp cross-attend cho phép mở rộng quy mô độ dài đầu vào
vượt ra ngoài ngay cả những giới hạn đó mà không yêu cầu bất kỳ thay đổi kiến trúc nào

--- TRANG 18 ---
Perceiver AR
Hình 9. Hình ảnh từ batch được tạo ra với 1536 latent (Hình 7) được upsampled lên 256 256 pixel sử dụng SR3 (Saharia et al., 2021).
hoặc thêm yêu cầu tính toán cho các lớp khác
ngoài cross-attend. Những trick tiết kiệm bộ nhớ này có dẫn đến
giảm throughput huấn luyện (tính bằng bước mỗi giây),
vì vậy chúng tôi tránh chúng khi có thể.
E.2. Dropout Cross-attend
Dropout (Srivastava et al., 2014) được sử dụng mặc định trong nhiều
implementation Transformer và là một công cụ thiết yếu để
giảm thiểu overfitting trên các bộ dữ liệu nhỏ. Dropout thường
được áp đặt trên các lớp tuyến tính sau attention softmax hoặc
trong khối MLP Transformer. Perceiver AR hỗ trợ
loại dropout này, nhưng lớp cross-attend cũng cho phép
các khả năng thú vị cho dropout trước attention soft-
max.

Chúng tôi thấy rằng đối với một số nhiệm vụ, việc che dấu các vị trí trong
cross-attend ban đầu là một cách hiệu quả để ngăn ngừa over-fitting. Điều này cũng có thể được sử dụng để tiết kiệm bộ nhớ bằng cách đặt
ngân sách cho một số lượng đầu vào nhất định và sau đó chọn số lượng
đầu vào đó một cách ngẫu nhiên từ ngữ cảnh đầu vào tối đa.
Bởi vì không có tham số đặc biệt theo vị trí nào được học
cho lớp cross-attend, một số lượng nhỏ hơn các đầu vào có thể được
sử dụng tại thời điểm huấn luyện so với trong quá trình đánh giá hoặc suy luận.

Việc áp đặt dropout cross-attend có thể được hiểu là thực thi
tính thưa thớt cao tại thời điểm huấn luyện, nhưng cho phép tính thưa thớt ít cực đoan hơn
tại thời điểm đánh giá. Bởi vì bản thân lớp attention
là bất biến về tỷ lệ, việc mở rộng quy mô đồng nhất thường được áp đặt bởi
dropout tại thời điểm huấn luyện là không cần thiết.
E.3. Lưu trữ Activation cho Suy luận
Việc lấy mẫu ngây thơ từ một Transformer cho suy luận có thể
rất chậm vì activation cho tất cả các vị trí phải được
tính toán ở mọi bước. Việc lưu trữ các activation key/value

--- TRANG 19 ---
Perceiver AR
P ee r
r c e i v ec e i v e r
Pe
e r c e i vr c e i v e
P e r c e ie r c e i v
P e r c ee r c e i
P e r ce r c e
P e re r c
P ee r e
P
Pe
e r c e i vr c e i v e
P e r c e ie r c e i v
P e r c ee r c e i
P e r ce r c e
P e re r c
P ee r
Pe
P ee r
r c e i v ec e i v e r
Pe
e r c e i vr c e i v e
P ee r
r c e i v ec e i v e r
P e r c e ie r c e i v
P e r c ee r c e i
P e r ce r c e
P e re r c
P ee r
PeNaïv e 
caching: Không 
caching: 
Caching+ 
r es etting: t0 t1 t2 t3 t4 t5 t6 t7 
R es et 
memoryR es et 
memory
Hình 11. Hình tốt nhất được xem trên màn hình. Caching tại thời điểm tạo cho phép các trạng thái được tính toán trước đó được tái sử dụng nhưng giới thiệu
các phụ thuộc dài hạn không được thấy tại thời điểm huấn luyện khi kích thước đầu vào và kích thước latent khác nhau. Chúng tôi minh họa hiệu ứng này ở đây cho một Perceiver AR
vớiN= 4latent và ngữ cảnh đầu vào M= 8. Ở đây, các tam giác xanh dương chỉ ra những phép toán attention nào được thực hiện trong bước
hiện tại, trong khi các tam giác xám chỉ ra các tính toán được tái sử dụng (cached). Hàng trên cùng (không có caching) khớp với những gì xảy ra tại thời điểm huấn luyện. Nếu
caching được áp dụng một cách ngây thơ (hàng giữa), lượng tính toán mỗi bước có thể được giảm đáng kể. Tuy nhiên, khi mô hình được chạy ra
cho nhiều bước hơn số lượng latent, caching giới thiệu các phụ thuộc trên các latent không còn hoạt động nhưng đã được sử dụng để
tính toán các latent đang hoạt động. Những latent này được hiển thị bằng màu đỏ. Nói cách khác, caching các latent trước đó cho phép các latent xa hơn
(không còn hoạt động) ảnh hưởng đến việc tạo hiện tại, giới thiệu các phụ thuộc dài hạn không được gặp tại thời điểm huấn luyện. Chúng tôi
thấy trong thực tế rằng những phụ thuộc dài hạn này dẫn đến hiệu suất giảm khi caching được chạy ra quá nhiều bước. Để
tránh vấn đề này, chúng tôi cache bằng cách định kỳ reset bộ nhớ, cho phép một số tính toán được tái sử dụng nhưng tránh việc giới thiệu các
phụ thuộc dài hạn không được gặp tại thời điểm huấn luyện. Chiến lược này được mô tả chi tiết trong Phụ lục E.3.

--- TRANG 20 ---
Perceiver AR
cho các vị trí được suy luận trước đó là một kỹ thuật phổ biến để
cải thiện tốc độ tạo (Vaswani et al.; Shazeer, 2019).
Perceiver AR có thể sử dụng một cách tiếp cận tương tự, nhưng kỹ thuật
chính xác không thể được áp dụng trực tiếp vì số lượng
vị trí đầu ra nhỏ hơn số lượng vị trí đầu vào,
vì vậy việc bảo tồn tất cả các activation trước đó sẽ dẫn đến
một ngăn xếp self-attention hiệu quả rộng bằng số lượng đầu vào.

Transformer-XL (Dai et al., 2019) giải quyết vấn đề này bằng cách
giữ một buffer các activation chỉ cho N vị trí cuối cùng.
Điều này hoạt động vì mô hình được trình bày với các activation
cho số lượng vị trí trước đó đó trong quá trình huấn luyện, điều này
không phải trường hợp của Perceiver AR. Chúng ta cũng không thể đơn giản
hạn chế kích thước buffer giống như số lượng
mục tiêu vì ngay cả khi các activation cho một vị trí nhất định
hết hạn, chúng đã ảnh hưởng đến các vị trí khác
trong buffer (Hình 11).

Thay vào đó, chúng tôi áp dụng một trick đơn giản để đảm bảo rằng không có
activation được cache nào bị ảnh hưởng bởi các vị trí vượt ra ngoài những gì được
thấy tại thời điểm huấn luyện. Chúng tôi sử dụng một buffer activation
cố định có cùng chiều rộng như ngăn xếp self-attention tại thời điểm huấn luyện. Khi
buffer đầy, chúng tôi thực hiện một chuyển tiếp đầy đủ mà không có
activation được cache nào cho vị trí tiếp theo, nhưng sử dụng một nửa
số lượng latent. Các activation từ chuyển tiếp đó được lưu
trong buffer, để lại nó một nửa đầy. Suy luận sau đó tiếp tục
cho đến khi buffer đầy trở lại. Các activation từ
lớp cross-attend không yêu cầu trick này trừ khi suy luận sẽ
mở rộng vượt quá độ dài của các đầu vào được sử dụng tại thời điểm huấn luyện.

Chúng tôi thấy rằng những chuyển tiếp đầy đủ thỉnh thoảng này thêm overhead
tối thiểu, và lợi ích tốc độ từ việc sử dụng các activation được cache
vẫn đáng kể. Chúng tôi thực hiện một thử nghiệm suy luận
một hình ảnh duy nhất với độ dài chuỗi 12,289 sử dụng một
mô hình với 1024 latent (xem Mục 5.2 để biết chi tiết) trên một
lõi TPUv3 duy nhất để so sánh tốc độ. Suy luận mà không có
caching nào mất 7.93 phút. Với caching cùng nhiệm vụ
mất 3.68 phút, ít hơn một nửa thời gian.
E.4. Thay đổi tính toán tại thời điểm kiểm tra
Hình 12 minh họa cách số lượng latent có thể được
thay đổi tại thời điểm kiểm tra mà không thay đổi các tham số được huấn luyện
của mô hình hoặc độ dài ngữ cảnh đầu vào của mô hình.
Trong Mục 5.2.1 chúng tôi thảo luận cách khả năng này có thể được sử dụng
để mở rộng quy mô lên hoặc xuống yêu cầu tính toán và chất lượng đầu ra.
F. Chi tiết huấn luyện
F.1. Chung
Trừ khi được nêu khác, các mô hình được huấn luyện với cấu hình sau.

Chúng tôi sử dụng optimizer Adam (Kingma & Ba, 2015) như được thực hiện
P r c e e<EOS> 
T es t với 
cùng # la t ents 
(N=3) 
i v e r A RT rain Time 
P r c e eA R<EOS> 
Causal 
cr oss-att ention Causal 
s elf-att ention 
Inputs T arg ets 
i v e r A R
T es t Time 
Nhiều comput e 
P r c e e<EOS> 
i v e r A RP r c e e<EOS> 
T es t với 
f e w er la t ents 
(N=2) 
i v e r A RCùng 
Paramet ers 
Cùng Input Cont e xt T rained với 
ﬁx ed # la t ents 
(N=3) 
Ít comput e 
T es t với 
mor e la t ents 
(N=6 ) Cùng comput e Hình 12. Bởi vì Perceiver AR tách rời độ dài đầu vào khỏi
chiều rộng của ngăn xếp self-attention, số lượng latent có thể
khác nhau tại thời điểm huấn luyện và thời điểm kiểm tra. Điều này không yêu cầu huấn luyện bổ sung
vì không có tham số theo vị trí nào được học trong
ngăn xếp self-attention. Mục 5.2.1 thảo luận cách khả năng này có thể
được sử dụng để mở rộng quy mô lên hoặc xuống yêu cầu tính toán và chất lượng đầu ra.
trong framework Optax (Hessel et al., 2020) với
b1 = 0 :1,b2 = 0 :999,eps= 1e8, learning rate cơ sở
3e4, và warmup tuyến tính 10k bước. Để giảm việc sử dụng bộ nhớ,
chúng tôi sử dụng phân vùng trạng thái optimizer ZeRO Stage 1
(Rajbhandari et al., 2020).

Để ổn định huấn luyện, chúng tôi sử dụng max norm toàn cục 1:0.
Chúng tôi cũng thêm một số hạng loss bổ sung zlosslog(z)2với
zloss= 1e4, như được sử dụng trong việc huấn luyện họ mô hình T5
(Raffel et al., 2020).

Cả embedding đầu vào và vector latent đều có kích thước 1024,
và 16 head attention được sử dụng cho cross-attend ban đầu
và trong ngăn xếp self-attention. Trong các lớp MLP
của cross-attend và ngăn xếp self-attention, chiều đầu vào
được chiếu lên 4x kích thước của nó và activation Squared ReLU

--- TRANG 21 ---
Perceiver AR
(So et al., 2021) được sử dụng. Chúng tôi sử dụng xác suất dropout cross-
attend 0:1.

Huấn luyện và đánh giá được thực hiện trên các cluster TPUv2 hoặc
TPUv3.
F.2. Encoding vị trí quay
Chúng tôi mã hóa vị trí của các token sử dụng rotary position en-
coding (Su et al., 2021). Với phương pháp này, các ma trận quay
(được xây dựng từ các hàm sine và cosine, giống như với
encoding vị trí sinusoidal/Fourier) quay các cặp chiều
trong mỗi head key và query để phản ánh
vị trí tuyệt đối của key hoặc query trong chuỗi. Khi
được sử dụng để so sánh một cặp key và query nhất định, những
quay này tạo ra trọng số attention phản ánh chỉ khoảng cách tương đối
giữa các token. Kết quả là một cơ chế vị trí tương đối
hiệu quả về bộ nhớ.

QCr oss-a tt end La t ent 
self-a tt end 
KQL la y ers 
V
KVT ar g e ts 
( shift ed inputs ) 
A tt ention 
Mask Cr oss- A tt ention Mask 
rAR
<EOS> AR
P
e
r
c
e
i
v
e
r
A
R
r A R
Perceive
Inputs La t ents 
L earned 
La t ents 
Hình 13. Biến thể của Perceiver AR nơi chúng tôi đưa vào các latent đầu vào được học
thay vì các embedding đầu vào trực tiếp. Điều này được sử dụng
cho các thí nghiệm Wikitext-103 của chúng tôi vì chúng tôi nhận thấy nó giúp giảm
overfitting.
Trực quan, cơ chế này biểu diễn vị trí sử dụng một chiến lược
hơi giống như được sử dụng bởi kim đồng hồ analog. Giống như
kim đồng hồ, mỗi cặp chiều (sine/cosine) trong ro-
tary position encoding chịu trách nhiệm chỉ ra vị trí
ở một tần suất nào đó (giây, phút, giờ, v.v.). Khi chúng ta
có nhiều kim đồng hồ (nhiều chiều trong mỗi
vector token), mỗi kim di chuyển ở một tần suất khác nhau,chúng ta có thể nói chính xác cả thời gian hiện tại (vị trí q/k)
và chúng ta có thể giải quyết nó ở các độ phân giải khác nhau, tùy thuộc vào
những gì cần thiết cho một nhiệm vụ nhất định.

Chúng tôi thấy trong các thí nghiệm sớm rằng việc quay một phần của
các chiều kênh dẫn đến kết quả tốt hơn, mà chúng tôi phát hiện
cũng đã được chú ý (50% đầu tiên) (Wang, 2021).
Quay phân số loại này giảm độ trung thực mà
chúng ta mã hóa thông tin vị trí (vì nó dẫn đến
ít dải tần số hơn). Kết quả của quay phân số
là chỉ một số chiều kênh được điều chế bởi
vị trí tương đối giữa một query và key. Điều này có thể khuyến khích
mạng khai thác cả mối quan hệ phụ thuộc vị trí và
bất khả tri vị trí giữa các query và key
khi tính toán trọng số attention. Công trình trước mô tả
hiệu ứng của vị trí trên trọng số attention cho thấy rằng
nhiều chiến lược encoding vị trí phổ biến thiên về việc mạng
chú ý đến các token gần đây và quay phân số
có thể giảm thiểu hiệu ứng này. Quay phân số cũng nhưng làm cho
tính toán quay rẻ hơn đáng kể, vì nó yêu cầu
ít phép nhân ma trận hơn.
F.3. Nhiệm vụ sao chép
Nhiệm vụ sao chép được huấn luyện với 1024 latent trong một ngăn xếp
self-attention 6 lớp. Đối với encoding vị trí, chúng tôi sử dụng embedding
sinusoidal cố định để biểu thị vị trí tuyệt đối, như được
mô tả trong bài báo Transformer gốc (Vaswani et al.,
2017). Có 4096 chuỗi mỗi batch.

Để giảm yêu cầu bộ nhớ tức thời được tạo ra
bởi việc chú ý đến chuỗi đầu vào có độ dài 131,072, 16 head cross-attention
được chia thành 4 nhóm gồm 4 và được tính toán riêng biệt (xem Phụ lục E.1).

1K bước đầu tiên là warmup learning rate tuyến tính và
phần còn lại theo lịch trình giảm learning rate cosine.
F.4. ImageNet 64×64
Mô hình được huấn luyện trên ImageNet 64 64 có 770.1M tham số. Huấn luyện tiến hành tổng cộng 750k bước. Sau
warmup tuyến tính ban đầu 10k bước, learning rate không đổi
3e4được sử dụng cho đến 50k bước cuối cùng, sử dụng
giảm cosine về 0.
F.5. Wikitext-103
Chúng tôi huấn luyện các mô hình Perceiver AR 18 lớp với 1,024 latent,
embedding đầu vào thích ứng (Baevski & Auli, 2018), và
với độ dài ngữ cảnh tăng từ 1,024 đến 8,192 token.
Một khác biệt chính so với các thí nghiệm khác là chúng tôi sử dụng
các latent đầu vào được học thay vì các embedding đầu vào như được
minh họa trong Hình 13. Chúng tôi quan sát rằng biến thể này giúp
giảm overfitting. Chúng tôi cũng áp dụng dropout cross-attend (Phụ
lục E.2) với p=0:15cho các token ngữ cảnh mà chúng tôi

--- TRANG 22 ---
Perceiver AR
SoundStream bitrate Ngữ cảnh (32k) Ngữ cảnh (8k) Test (32k) Test (8k) Validation (32k) Validation (8k)
12kbps 27.2s 6.8s 2.31 2.25 2.28 2.27
18kbps 18.4s 4.6s 2.52 2.53 2.51 2.52
22kbps 14.8s 3.7s 2.55 2.60 2.55 2.60
Bảng 13. Kết quả negative log-likelihood của Perceiver AR trên tạo âm thanh SoundStream, từ hai mô hình với độ dài ngữ cảnh 8192
(8k) và 32768 (32k), tương ứng.
dự đoán token tiếp theo.

Baseline Transformer-XL sử dụng độ dài bộ nhớ 384
tại thời điểm huấn luyện và 1600 tại thời điểm eval, cho tổng độ dài ngữ cảnh hiệu quả
2624. Chúng tôi sử dụng tỷ lệ dropout bộ nhớ 0.25.

Các mô hình Perceiver AR được huấn luyện trên Wikitext-103 có
số lượng tham số sau: 356.5M (1024 ngữ cảnh), 357.7M
(2048 ngữ cảnh), 359.8M (4096 ngữ cảnh), 364.0 (8192 ngữ cảnh). Lưu ý rằng số lượng tham số tăng một chút với
độ dài ngữ cảnh tăng do việc sử dụng encoding vị trí tuyệt đối
trên Wikitext-103. Baseline Transformer-
XL có 285.2M tham số.
F.6. PG-19
Cả hai mô hình Perceiver AR được huấn luyện trên PG-19 đều sử dụng 974.6M
tham số.
F.7. Sách
Tất cả các mô hình Sách được báo cáo trong Bảng 7 có 498.9M tham số.

Các mô hình Perceiver AR được báo cáo trong Bảng 8 đều có 498.9M
tham số. Các mô hình Transformer-XL được khớp tính toán
có số lượng tham số sau, theo số lượng lớp:
346.6M (23), 360.3M (24), 373.9M (25), 414.8M (28).

Các mô hình Perceiver AR được báo cáo trong Bảng 9 có số lượng
tham số sau, theo độ dài ngữ cảnh và số lượng lớp:
826.4M (1024, 62L), 813.8M (4096, 61L), 801.2M (8192,
60L), 750.8M (16384, 56L). Transformer-XL 42 lớp
với tính toán được khớp có 605.8M tham số.

Chúng tôi sử dụng cùng cài đặt bộ nhớ cho baseline Transformer-XL
như trên Wikitext-103: tất cả các mô hình Transformer-XL có
tổng độ dài ngữ cảnh hiệu quả 1600 + 1024 = 2624 .
F.8. Nhiệm vụ tạo âm nhạc
Tất cả các mô hình sử dụng giá trị 0:7cho dropout cross-attend
được mô tả trong Phụ lục E.2. Tất cả trừ các mô hình
nhiệm vụ MAESTRO SoundStream áp dụng quay cho 25% chiều attention. Ngoài ra, các mô hình được huấn luyện trên
bộ dữ liệu piano và nhiệm vụ MAESTRO SoundStream sử dụng
learning rate 2e4. Những mô hình được huấn luyện trên dữ liệu ký hiệu
MAESTRO sử dụng learning rate 1e4, 1 head cross-
attend ban đầu và 4 head trong ngăn xếp self-attention. Các
Hình 14. Loss validation trên MAESTRO v3 như một hàm của
dropout cross-attend được áp dụng cho mô hình. Mỗi giá trị
f0;0:5;0:75gđược áp dụng ở 3 độ sâu mô hình khác nhau f12;24;48g.
mô hình được huấn luyện trên bộ dữ liệu phiên âm piano sử dụng 0:1
cho dropout cross-attend và tỷ lệ dropout 0:25trên các
latent. Cuối cùng, mô hình này pre-embed các đầu vào query sử dụng
một MLP 3 lớp với activation GELU (Hendrycks & Gimpel, 2016).

Âm thanh từ MAESTRO v3 trước tiên được xử lý sử dụng
codec SoundStream 12kbps. Bitrate 12kbps này tương ứng với từ vựng 1024 token (10 bit), 24 to-
ken/frame, và 50 frame/giây. Điều này tái tạo âm thanh
với chất lượng hợp lý, nén mỗi giây âm thanh
(16k điểm sóng) thành 1.2k token tuần tự. Mô hình được huấn luyện của chúng tôi
có ngữ cảnh đầu vào có độ dài 8192 ( 6.8 giây)
và sử dụng 1024 latent trong 12 lớp self-attention. Chúng tôi cũng
huấn luyện và đánh giá cấu hình này sử dụng các codec bitrate cao hơn
—18kbps và 22kbps, với 36 và 44 token/frame
tương ứng.
G. Chi tiết đánh giá
Như đã thảo luận trong Shortformer (Press et al., 2021), có một
sự đánh đổi chất lượng vs. tốc độ khi đánh giá các chuỗi dài
về stride. Stride 1 (overlap tối đa) là
chậm nhất nhưng chất lượng cao nhất, và stride bằng
độ dài đầu vào (không overlap) là nhanh nhất nhưng chất lượng thấp nhất.
Bởi vì Perceiver AR tách rời số lượng mục tiêu khỏi
số lượng đầu vào, các tùy chọn stride của chúng tôi dao động từ 1 đến
số lượng latent. Chúng tôi thấy rằng stride bằng một nửa số lượng
latent mang lại sự cân bằng tốt giữa tốc độ và chất lượng,
và đó là những gì chúng tôi sử dụng cho tất cả các đánh giá trong bài báo này,
trừ khi được đề cập khác. Đối với một tập hợp vấn đề liên quan cho
suy luận, xem thảo luận trong Phụ lục E.3.
H. Ablation dropout
Chúng tôi nghiên cứu hiệu ứng của việc áp dụng các dạng dropout khác nhau
cho Perceiver AR, ở một số độ sâu mô hình khi huấn luyện trên
bộ dữ liệu ký hiệu MAESTRO v3.

Hình 14 minh họa hành vi của mô hình khi thay đổi
lượng dropout cross-attend được áp dụng cho nó. Mô
hình có ngữ cảnh đầu vào 4,096 và giá trị dropout post-attention
0.5—cả hai siêu tham số đều được giữ không đổi
qua tất cả các lần chạy. Loại dropout này có vẻ ít hữu ích hơn
khi tăng độ sâu mô hình, đến mức không có hiệu ứng gì
trong quá trình huấn luyện cho mô hình 48 lớp.

Đối với thí nghiệm thứ hai, chúng tôi thay đổi lượng dropout (post-
attention) trong cùng cài đặt như đã mô tả trước đó,
trong khi giữ giá trị không đổi 0.7 cho dropout cross-
attend. Hình 15 cho thấy rằng tỷ lệ dropout cao hơn
trở nên hữu ích hơn ở độ sâu lớn hơn—trong khi ở độ sâu 12,
việc áp dụng dropout 0.75 thực sự có hại cho việc huấn luyện mô hình, điều này
trở nên có lợi ở độ sâu 48, nơi việc không áp dụng dropout
dẫn đến overfitting nhanh chóng.
I. MAESTRO SoundStream
Trong Bảng 13, chúng tôi so sánh kết quả trên tất cả các nhiệm vụ MAESTRO Sound-
Stream được thu được từ các mô hình được huấn luyện trên
ngữ cảnh 8192 và 32768 token, tương ứng. Trong khi
độ dài ngữ cảnh nhỏ hơn mang lại kết quả tốt hơn ở bitrate thấp nhất,
mô hình 32k-context đạt được negative log-likelihood
thấp hơn đáng kể so với mô hình 8k- trên SoundStream
22kbps. Hơn nữa, khoảng cách giữa các NLL tương ứng
mở rộng khi bitrate tăng. Kết quả này cho thấy rằng
ngữ cảnh ngắn hơn ngày càng ít hữu ích khi chúng ta cố gắng tạo ra
âm thanh có độ trung thực cao hơn. Các mô hình ngữ cảnh 32k được báo cáo
ở đây có số lượng tham số sau: 366.3M (12kbps),
391.5M (18kbps), 408.3M (22kbps). Các mô hình ngữ cảnh 8k
Hình 15. Loss validation trên MAESTRO v3 như một hàm của
dropout post-attention được áp dụng cho mô hình. Mỗi giá trị
f0;0:5;0:75gđược áp dụng ở 3 độ sâu mô hình khác nhau f12;24;48g.

--- TRANG 23 ---
Perceiver AR
được báo cáo ở đây có số lượng tham số sau: 215.2M
(12kbps), 240.4M (18kbps), 257.1M (22kbps).

Các mô hình SoundStream ngữ cảnh 65536 được báo cáo trong Bảng 12
có số lượng tham số sau: 668.6M (12kbps),
693.8M (18kbps), 710.6M (22kbps).

--- TRANG 24 ---
Perceiver AR
