# 2303.09752.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2303.09752.pdf
# File size: 982420 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
COLT5: Faster Long-Range Transformers with Conditional Computation
Joshua Ainslie∗, Tao Lei, Michiel de Jong, Santiago Ontañón
Siddhartha Brahma ,Yury Zemlyanskiy ,David Uthus ,Mandy Guo
James Lee-Thorp ,Yi Tay ,Yun-Hsuan Sung ,Sumit Sanghai
Google Research
Abstract
Many natural language processing tasks benefit
from long inputs, but processing long docu-
ments with Transformers is expensive -- not
only due to quadratic attention complexity but
also from applying feedforward and projec-
tion layers to every token. However, not all
tokens are equally important, especially for
longer documents. We propose COLT5, a
long-input Transformer model that builds on
this intuition by employing conditional com-
putation, devoting more resources to important
tokens in both feedforward and attention lay-
ers. We show that COLT5 achieves stronger
performance than LONG T5with much faster
training and inference, achieving SOTA on the
long-input SCROLLS benchmark. Moreover,
COLT5 can effectively and tractably make use
of extremely long inputs, showing strong gains
up to 64k input length.
1 Introduction
Many natural language processing tasks, such as
summarization (Cohan et al., 2018) or question an-
swering over long documents (Joshi et al., 2017),
require machine learning models to encode long-
form text. Processing long documents with a Trans-
former model is computationally expensive, both
because attention cost scales quadratically with in-
put length and because feedforward and attention
projection layers have to be applied to each input
token.
Over the past few years, many “efficient Trans-
former” approaches have been proposed that re-
duce the cost of the attention mechanism over long
inputs (Child et al., 2019; Ainslie et al., 2020; Belt-
agy et al., 2020; Zaheer et al., 2020; Wang et al.,
2020; Tay et al., 2021; Guo et al., 2022). However,
especially for larger models, the feedforward and
projection layers actually make up the majority of
∗Author contributions are outlined in Appendix A. Corre-
spondence author: jainslie@google.com.
Figure 1: An overview of a COLT5 Transformer layer
with conditional computation. All tokens are processed
by light attention and MLP layers, while qrouted query
tokens perform heavier attention over vrouted key-value
tokens and mrouted tokens are processed by a heavier
MLP.
the computational burden and can render process-
ing long inputs intractable.
This paper presents COLT5 (Conditional
LongT5), a new family of models that, building on
top of LONGT5(Guo et al., 2022), enables fast pro-
cessing of long inputs by combining architecture
improvements for both attention and feedforward
layers. COLT5 is based on the intuition that some
tokens are more important than others, and we can
achieve better quality for lower cost by devoting
more computation to important tokens. Moreover,
the fraction of important tokens is likely to dimin-
ish with document length, allowing for tractable
processing of long documents.
In particular, COLT5 divides each feedforward
layer and each attention layer into a light brancharXiv:2303.09752v3  [cs.CL]  24 Oct 2023

--- PAGE 2 ---
200 400 600 800424446
BLXL
BLXL
Time per sample (ms)Average perfInference
0 500 1,0001,5002,0002,500424446
BLXL
BLXL
Time per sample (ms)Fine-tuning
LONG T5 COLT5
Figure 2: COLT5 achieves stronger performance than LONG T5at any speed. Average performance on all
datasets as a function of inference and fine-tuning time per sample (ms) for LONGT5andCOLT5 Base, Large, and
XL models. L ONG T5 does not use MQA, but we report speed as though it had for a conservative baseline.
which is applied to all tokens and a heavy branch
which is applied to a set of important tokens, se-
lected specifically for that input and component.
The light feedforward branch has lower hidden di-
mension than standard LONG T5while the heavy
feedforward branch has higher hidden dimension.
The light attention branch has fewer heads and ap-
plies only local attention, while the heavy attention
branch performs full attention over another sepa-
rately selected set of important tokens. Figure 1
provides an overview of the COLT5 conditional
mechanism.
Finally, COLT5 also includes two other mod-
ifications to the LONG T5architecture. COLT5
adds multi-query cross-attention (Shazeer, 2019),
significantly speeding up inference. COLT5 also
employs the UL2 (Tay et al., 2022) pre-training ob-
jective, which we demonstrate allows for in-context
learning over long inputs.
We show that COLT5 performs much faster fine-
tuning and inference with similar or better model
quality, improving over LONGT5on arXiv summa-
rization (Cohan et al., 2018) and TriviaQA question
answering (Joshi et al., 2017) datasets and achiev-
ing SOTA on the SCROLLS benchmark (Shaham
et al., 2022). Moreover, COLT5 achieves further
gains in quality and speed for tasks with extremely
long inputs (64k tokens), with less-than-linear scal-
ing of “focus” tokens.
2 Background
Transformer FLOPs COLT5 follows an exten-
sive line of work in attempting to reduce the com-
putational cost of Transformer models, particularlyover long inputs. The computational burden of
Transformer models has several distinct elements,
and different approaches focus on reducing the cost
of different components. For that reason, it is help-
ful to start by providing a breakdown of the compu-
tational cost of Transformer components. Table 1
shows the FLOPs1for each component of a Trans-
former encoder layer (Kaplan et al., 2020).
Encoder Layer Component Flops
Vanilla self-attention computation 2n2d
Attention QKV and output projections 4nd2
Feedforward layer 8nd2
LONG T5 local attention computation 2nwd
LONG T5 global attention computationn2
8d
Table 1: Computational cost of encoder layer trans-
former components measured in FLOPs. nis the input
length, dis the model dimensionality, and wis the size
of the local attention window.
Sparse attention The first challenge of applying
a Transformer to a long input is that the FLOPs
of the self-attention mechanism scales quadrati-
cally in the input length, becoming intractable for
long inputs. A large body of work focuses on re-
ducing self-attention cost, restricting attention be-
tween a subset of inputs (Child et al., 2019; Ainslie
et al., 2020; Beltagy et al., 2020; Zaheer et al.,
2020; Wang et al., 2020; Guo et al., 2022) or to
a subset of layers (Zemlyanskiy et al., 2021). In
LONG T5(Guo et al., 2022), the most closely re-
lated model to COLT5, tokens attend within a lo-
1Each multiply-add is counted as a single FLOP.

--- PAGE 3 ---
cal window as well as to a mean-pooled summary
representation for each block of 16 tokens in the
input. LONG T5attention leads to sharply reduced
(though still non-negligible) FLOPs (Table 1).
Conditional computation After applying a
sparse attention mechanism, the feedforward and
attention projection layers account for the major-
ity of the FLOPs. These costs scale with the
length of the input, such that processing long in-
puts is still prohibitively expensive. A common
approach to reduce the remaining cost is to employ
some form of conditional computation , avoiding
applying all model parameters to the entire input.
CALM (Schuster et al., 2022) applies a varying
number of decoder layers to each decoded token,
outputting a token early if the model is confident in
its prediction. Mixture-of-Experts models (Shazeer
et al., 2017; Fedus et al., 2021; Zoph et al., 2022)
route inputs through a small proportion of expert
sub-modules, bringing to bear only the parame-
ters most relevant to the input. In the context of
retrieval-augmented models, numerous works re-
rank retrieved passages by their relevance to the
query and process only the highest scoring pas-
sages (Mao et al., 2021; Wang et al., 2018; Yu
et al., 2022) and vary the number of processed pas-
sages depending on model confidence (Kratzwald
and Feuerriegel, 2018; Varshney et al., 2022). Con-
current work CoDA (Lei et al., 2023) employs a
related conditional computation mechanism, de-
signed for efficient adaptation rather than modeling
long documents.
Device utilization FLOPs do not tell the whole
story, as modeling choices can influence the effec-
tive speed of operations achieved by accelerators.
For long text inputs, autoregressive decoder infer-
ence is very slow due to memory bandwidth con-
straints from repeatedly loading the long sequence
of keys and values (Shazeer, 2019; de Jong et al.,
2022). Shazeer (2019) introduces multi-query at-
tention (MQA), sharing heads for keys and values
to reduce memory bandwidth overhead. Pope et al.
(2022) studies how to shard large models, espe-
cially in the context of MQA, to obtain optimal
device utilization and therefore speed.
Training objectives T5 introduced the span cor-
ruption objective (Raffel et al., 2020), a modi-
fication of masked language modeling (Devlin
et al., 2019). LONG T5made use of the PEGA-
SUS (Zhang et al., 2020) sentence reconstruc-tion objective for improved summarization perfor-
mance. Tay et al. (2022) proposes UL2, a mixture
of span corruption, prefix, and causal language
modeling, and shows that it leads to strong perfor-
mance on both short-output and generative tasks.
3 C OLT5
3.1 Conditional computation
As discussed in the previous section, a large propor-
tion of Transformer FLOPs arise from feedforward
and projection layers that scale with the length of
the input sequence. Therefore, LONG T5training
and inference on long documents remains expen-
sive.
COLT5 further reduces the cost of processing
long documents through conditional computation ,
following the intuition that some tokens are more
important and therefore benefit more than others
from heavy computation. First, some types of to-
kens may inherently require less computation, such
as filler words and punctuation. Second, especially
in long documents, large parts of the input may not
be relevant to the current question, task, or process-
ing stage.
The COLT5 conditional computation mecha-
nism consists of three components: routing mod-
ules, conditional feedforward layers, and condi-
tional attention layers. All tokens are processed by
standard, lightweight attention and feedforward lay-
ers. Routing modules additionally select important
tokens from an input at each attention or feedfor-
ward layer, and a heavy conditional layer applies
additional computation to routed tokens. This sec-
tion describes each component in detail. Figure 1
provides an overview of the COLT5 conditional
computation mechanism, and Table 2 compares
COLT5 and L ONG T5 FLOPs.
Model Encoder Layer Flops
T5 12nd2+ 2n2d
LONG T5 12nd2+n2
8d
COLT5 71
4nd2+n2
84d
Table 2: COLT5 uses significantly fewer FLOPs than
LONG T5.Comparison of approximate encoder layer to-
tal FLOPs between T5,LONGT5, and COLT5.COLT5
FLOPs rounded to readable fractions.
Routing In order to separately select important
tokens for each component in each layer, we need

--- PAGE 4 ---
alearnable andtractable routing function. We
follow the simple three-step mechanism from Lei
et al. (2023): (1) multiply inputs with a learned
embedding to obtain routing scores, (2) normalize,
and (3) select the top- khighest scoring inputs.
LetXibe the representation of token i, and u
ad-dimensional learnable embedding. Then the
routing score of token iis
si=Xi·u
We select the top- khighest scoring inputs. In order
to provide a learning signal to the scoring embed-
ding, we make sure the contribution of the routed
tokens to the layer update is scaled according to the
routing score, as will be seen later. To provide a bet-
ter distributed signal to all tokens, we also globally
normalize the routing scores to sum up to the num-
ber of desired routed tokens using a generalized
softmax, resulting in normalized scores ˜si. Each
COLT5 layer has three independent routers, one
each for the feedforward layer, attention queries,
and attention key-values.
Conditional Feedforward Intuitively, some to-
ken representations may benefit from more pro-
cessing than others. The COLT5 conditional feed-
forward layer applies an additional high-capacity
feedforward layer to selected tokens. In particular,
letXibe the model state of the ith token and ˜si
denote the normalized routing score (set to 0 for
non-routed tokens). Then the feedforward update
for C OLT5 is given by
Xi=Xi+FFd Light(Xi) + ˜si·FFd Heavy(Xi)
The light and heavy feedforward branches differ
only in their hidden dimension, with the light
branch having smaller hidden dimension than
the standard T5 feedforward layer and the heavy
branch larger. Let ndenote the number of input to-
kens, mthe number of selected tokens, and rLand
rHthe ratios of light and heavy hidden dimension
to standard T5 hidden dimension. Then the FLOPs
of the C OLT5 layer are given by
FLOPs FFd= 8nrLd2
|{z}
Light branch+ 8mrHd2
|{z }
Heavy branch
We set the light and heavy ratios as rL=1
2and
rH= 4, half and quadruple the standard T5 hid-
den dimension respectively. For our main exper-
iments, a fraction1
16of tokens are routed to the
Figure 3: An overview of the COLT5 attention pattern.
The light branch performs local attention for each token.
In the higher capacity heavy branch qselected query
tokens (2 in the figure) attend to vseparately selected
key and value tokens (4 in the figure).
heavy branch. As a result the approximate FLOPs
from the C OLT5 feedforward layer equals
FLOPs FFd= 4 nd2
|{z}
Light branch+ 2 nd2
|{z}
Heavy branch
consuming 75% of the FLOPs of a standard T5
feedforward layer.
Conditional Attention COLT5 conditional at-
tention operates on the intuition that most tokens
have simple, local interactions, but some tokens
benefit from heavier processing and long-range in-
teractions. The COLT5 conditional attention layer
applies an additional high-capacity attention layer
that attends from selected query tokens to selected
key-value tokens. Let ˜sq
idenote the normalized
routing query score for token i, and ˜skvthe key-
value scores for all tokens (set to 0if not routed).
Then the attention update for C OLT5 is given by
Xi=Xi+ALight(Xi, X)+ ˜sq
i·AHeavy(Xi,˜skvX)
The light and heavy branches differ in the number
of heads and tokens attended to: the light branch
has fewer heads and attends to a local context win-
dow, while the heavy branch has more heads and
attends to all routed key-value tokens. Separately
selecting query and key-value tokens also allows
the model to differentiate between tokens that re-
quire additional information and those that possess

--- PAGE 5 ---
Model Avg Speed TQA NQA QAS QuAL CNLI arXiv SumS QMS GovR
inf fn F1 F1 F1 EM EM R gm Rgm Rgm Rgm
LONG T5-B 43.1 0.6 / 7.4 3.7 82.2 23.0 46.6 37.9 85.6 35.4 19.2 20.4 37.7
COLT5-B 42.4 11.2 6.5 82.4 23.3 42.1 36.5 86.5 35.3 18.7 18.4 37.9
LONG T5-L 45.3 0.3 / 3.0 1.3 84.2 27.2 52.3 40.6 87.3 35.7 19.1 21.4 39.5
COLT5-L 45.3 5.0 2.0 84.5 27.7 49.8 39.9 88.7 35.9 20.5 21.0 39.7
LONG T5-XL 46.6 0.2 / 1.2 0.4 85.3 29.3 53.1 46.0 88.2 35.9 19.4 21.3 40.5
COLT5-XL 47.4 2.3 0.5 86.1 31.1 53.9 48.1 88.4 36.1 20.0 22.5 40.5
Table 3: Performance comparison of COLT5 andLONG T5Base, Large and XL models on question-answering
datasets TriviaQA (TQA), NarrativeQA (NQA), QASPER (QAS), and QuALITY (QuAL), NLI dataset ContractNLI
(CNLI), and summarization datasets arXiv, SummScreenFD (SumS), QMSum (QMS), and GovReport (GovR).
SCROLLS results are on leaderboard test set where COLT5-XL achieves SOTA. Average speed is reported in
samples per second for inference (inf) and fine-tuning (fn). LONG T5does not use MQA but inference speed is
reported without/with MQA for conservative baseline. R gmstands for the geometric mean of ROUGE-1,2,L. Similar
to SCROLLS, we take a simple average across all datasets even though the datasets use different performance
metrics.
such information. Figure 3 shows the COLT5 at-
tention pattern. Let q, vbe the number of selected
query and key-value tokens, wthe size of the lo-
cal attention window and rL, rHthe proportion of
light and heavy heads relative to standard T5. Then
the FLOPs of the COLT5 attention layer are given
by
FLOPs Att= 4n·rLd2
|{z }
Local projection+ 2nw·rLd|{z }
Local attention
+ 2q·rHd2+ 2v·rHd2
| {z }
Global projection+ 2qv·rHd|{z }
Global attention
We set the light and heavy head ratios as rL=1
4
andrH=3
4, keeping the total number of heads
across the light and heavy branches equal to stan-
dard T5 heads. For our main experiments a fraction
1
16query tokens and1
8key-value tokens are routed
to the heavy branch, so q=n
16andv=n
8. Ignor-
ing local attention computation, we approximate
attention FLOPS by2
FLOPs Att≈nd2
|{z}
Local proj.+1
4nd2
|{z}
Global proj.+1
84n2d
|{z}
Global att.
with less than half projection FLOPs and order-of-
magnitude smaller quadratic length scaling com-
pared to LONG T5. Table 2 shows total FLOPs for
theCOLT5 layer. In general, we set q=mand
v= 2m, and use mto summarize the number of
routed tokens going forward.
2Global projection and attention FLOPs rounded to read-
able fractions, exact values are9
32and3
256. Complexity as-
sumes constant fraction of routed tokens; we show we can do
better in practice for extremely long inputs.3.2 Multi-query Attention
Conditional computation effectively reduces the
computational cost of the encoder. However, for
encoder-decoder models with long inputs the ma-
jority of inference time is spent in the decoder due
to memory bandwidth constraints (Shazeer, 2019;
de Jong et al., 2022). Most of the overhead is
caused by repeatedly reading all the input token
keys and values from memory for every output to-
ken that is autoregressively decoded during cross
attention. Multi-query attention (Shazeer, 2019)
(MQA) allows all query heads to share a single key
and value head, alleviating this bottleneck. Accord-
ingly, we apply MQA in cross-attention layers for
much faster inference. Note however that MQA
does not improve training speed since target tokens
are processed in parallel during training, avoiding
this memory bandwidth bottleneck.
3.3 UL2
The UL2 pre-training objective (Tay et al., 2022)
combines different denoising objectives, extending
the span corruption pre-training used in T5 to a
variety of noise rates / average span lengths and
adding a prefix language modeling objective more
similar to typical decoder-only model pre-training.
UL2 has been shown to lead to improved in-context
learning. We train COLT5 on UL2 instead of PE-
GASUS (Zhang et al., 2020), endowing COLT5
with in-context learning capabilities.
4 Experiments
In order to evaluate COLT5, we perform the fol-
lowing experiments: (1) our main results com-

--- PAGE 6 ---
pare COLT5 andLONG T5on a collection of long
input datasets using input length of 16k tokens;
(2) we evaluate COLT5 on extremely long in-
puts up to 64k tokens and compare scaling against
LONG T5; (3) demonstrate COLT5’s few-shot ca-
pability, investigating how performance changes as
input length and number of shots increase, (4) per-
form a series of ablations to understand the effect
of individual COLT5 components, and (5) inves-
tigate empirical routing patterns. The remainder
of the section outlines our experimental setup, and
then describes each of the experiments above.
4.1 Experimental setup
Configurations COLT5 is based on the T5.1.1
architecture (Raffel et al., 2020), implemented
with JAX (Bradbury et al., 2018), Flax (Heek et al.,
2020), and Flaxformer3. Following LONG T5, we
experiment with Base, Large, and XL model sizes.
COLT5 models use the same embedding dimen-
sion, number of layers, and total attention heads as
corresponding LONG T5models of the same size,
with more overall parameters (but less compute)
due to the conditional branch. See Appendix B for
additional details on model configuration.
Pre-training We pre-train COLT5 for 1M steps
on the C4 dataset (Raffel et al., 2020) using a vari-
ant of the UL2 objective (Tay et al., 2022) with
batch size 256, input length 4096, and output length
910. In particular, our mixture contains four objec-
tives in equal proportion: prefix-LM with noise rate
0.5, and span corruption (Raffel et al., 2020) with
noise rate 0.15 and average span lengths 3, 8, and
64. We use the Adafactor optimizer (Shazeer and
Stern, 2018) with the T5.1.1 inverse square root
learning rate schedule and no dropout. COLT5 is
trained with the T5X (Roberts et al., 2022) frame-
work. For pre-training, we route m= 512 tokens,
1
8th of the input length.
Fine-tuning For fine-tuning we use a constant
learning rate of 0.001, batch size 128, and dropout
rate 0.1 for all tasks. Main results use input length
of 16384 for all datasets other than ContractNLI,
which uses 8192. Question answering datasets use
output length 128 and summarization datasets use
output length 512, except for GovRep which uses
output length 1024. We route m= 1024 tokens,
1
16th of the input length. We train until convergence
3https://github.com/google/flaxformerand select the checkpoint with the highest dev per-
formance. We use greedy decoding for inference.
Data We evaluate COLT5 on TriviaQA (Joshi
et al., 2017), arXiv (Cohan et al., 2018),
and the SCROLLS benchmark (Shaham et al.,
2022). SCROLLS contains question-answering
datasets: NarrativeQA (Ko ˇciský et al., 2018),
QASPER (Dasigi et al., 2021), and QuAL-
ITY (Pang et al., 2021), an NLI dataset: Con-
tractNLI (Koreeda and Manning, 2021), and sum-
marization datasets: SummScreenFD (Chen et al.,
2022), QMSum (Zhong et al., 2021), and Gov-
Report (Huang et al., 2021). Table 4 provides
an overview of the size and input length for each
dataset.
Dataset Type Samples Median 90%
TriviaQA QA 157,053 8,858 28,956
arXiv Sum 215,913 8,519 20,170
NarrativeQA QA 71,187 57,829 176,862
QASPER QA 5,692 5,472 8,657
QuALITY QA 6,737 7,171 8,276
ContractNLI NLI 10,319 2,148 4,485
SummScreen Sum 4,348 9,046 15,172
QMSum Sum 1,810 14,197 27,761
GovRep Sum 19,402 8,841 18,835
Table 4: Median and 90th percentile input length by
dataset measured in SentencePiece tokens.
Timing We report time per sample per TPUv4
chip, as measured by xprof (Google, 2020). For
inference we use a single TPUv4 with batch size 16
or the largest that fits in memory. For fine-tuning
we profile with 8 TPUv4 chips, sharded separately
for each model to maximize throughput.
4.2 Main results
Figure 2 compares the quality-speed trade-off for
LONG T54andCOLT5, showing that COLT5 is
better at any speed. For 16k input length, COLT5
matches or exceeds LONGT5quality for Large and
XL with 35-75% training speedup and 50-100% in-
ference speedup on top of the order-of-magnitude
inference speedup from MQA. Encoder speedups
are even greater (Appendix D). COLT5-XL also
achieves SOTA performance on the SCROLLS
benchmark. Table 3 contains all main results.

--- PAGE 7 ---
100 200 300 400 500 6002426283032
8k16k32k
8k16k32k64k
Time per sample (ms)F1
LONG T5
COLT5
Figure 4: COLT5 effectively scales to extremely long
inputs, achieving stronger performance and faster
speed than LONG T5.F1 on NarrativeQA as a function
of inference time per sample for LONG T5andCOLT5
Large models using varying input lengths.
4.3 Scaling to extremely long inputs
We hypothesize that the advantage of COLT5 over
LONGT5strengthens with input length, as the frac-
tion of important tokens decreases and COLT5 can
route a greater proportion of important tokens to
the heavy branch. Figure 4 compares the quality-
speed trade-off for LONG T5andCOLT5 on Nar-
rativeQA, sweeping over input length rather than
model size. The number of routed tokens is1
16th
of the input length, except that we do not increase
routed tokens going from 32k to 64k, so at 64k
we route only1
32nd of the input length. COLT5
achieves both stronger performance and faster in-
ference speed at all input lengths and is able to
effectively make use of extremely long inputs. We
note that COLT5 achieves large quality gains by
going from 32k to 64k tokens even while keeping
the number of routed tokens constant, providing
more evidence for our hypothesis.
4.4 In-context learning
Models trained on the UL2 objective have shown
strong few-shot in-context learning (ICL) capa-
bilities5even at smaller sizes (Tay et al., 2022).
COLT5 enables tractable inference with long in-
puts. Here, we leverage this for scaling the number
of examples used for in-context learning.
4Note that LONG T5does not use MQA, but for profiling
we add MQA to L ONG T5 for a conservative baseline.
5We initially evaluated ICL for models pre-trained with
PEGASUS but found performance to be nearly 0.0.10.20.31k
2k
4k
8k
16kNaturalQ
0.05 0.1TriviaQA
Figure 5: COLT5 can use its long-input capability
to benefit from more shots for in-context learning.
Few-shot exact match for COLT5-Large on Natural
Questions and TriviaQA dev sets as a function of input
tokens, fitting as many examples as possible. Each
example contains question, context, and answer. Inputs
length used are 1024, 2048, 4096, 8192, 16384.
We test the above hypothesis by evaluating
few-shot learning performance on Natural Ques-
tions (Kwiatkowski et al., 2019) and TriviaQA as
a function of input length, using as many exam-
ples as fit in the context. We consider the open
book setting, such that each example consists of
question, context document, and answer. Table 5
shows the number of examples by input length. We
evaluate on the full dev set, randomly sampling
examples from the training set for each dev sample
until no further examples fit in the input length. We
found that COLT5 can perform in-context learning
only up to the input length it was trained on, so
for these experiments we continued pre-training
aCOLT5-Large model on input length 16384 for
another 100k steps. For the same reason we route
m= 512 tokens as in pre-training.
Figure 5 displays COLT5 few-shot performance
as a function of input length, showing that COLT5
is able to apply its long-input capabilities to extract
information from increasing numbers of examples.
Dataset 1024 2048 4096 8192 16384
NQ 0.1 0.7 1.7 3.4 5.6
TriviaQA 1.6 2.3 3.8 7.0 9.8
Table 5: Average number of Natural Questions and
TriviaQA few-shot examples that fit in input length.
4.5 Ablations
This section studies the effect of different choices
in the COLT5 recipe. Table 6 contains results of a
series of experiments that change a single compo-

--- PAGE 8 ---
Ablation ModelAvg Inf TQA NQA QAS QuAL CNLI arX SumS QMS GovR
S/s F1 F1 F1 EM EM R gm Rgm Rgm Rgm
Baseline C OLT5-B 42.5 11.2 82.4 23.1 38.3 36.6 87.8 35.3 19.3 20.5 39.4
RoutingStatic 40.5 11.6 79.7 19.2 34.2 34.5 86.4 34.9 18.1 18.9 38.8
Share QKV 42.0 11.8 82.1 21.9 37.5 36.2 87.0 35.2 18.2 20.4 39.7
Attentionv=all 42.5 9.4 82.4 22.3 38.6 37.2 87.8 35.3 19.1 20.3 39.8
v=q 42.3 11.5 82.5 22.5 37.3 37.0 85.9 35.2 19.0 20.5 39.7
Routed
Tokensm=512 41.6 12.2 81.9 22.1 37.3 35.4 84.6 35.2 18.9 19.5 39.6
m=1536 42.9 10.4 82.6 23.5 39.8 37.5 87.5 35.4 19.4 20.8 40.0
Encoder L ONG T5-B 42.1 7.4 82.0 21.4 38.4 35.8 88.0 35.5 18.7 20.4 38.5
Decoder Multi-head 42.9 0.7 82.7 22.9 40.2 35.8 87.7 35.5 19.7 21.2 40.3
Objective PEGASUS 42.8 11.2 82.6 22.6 40.5 37.3 87.3 35.3 19.6 20.8 39.6
Table 6: COLT5 ablations evaluated on validation sets. Each experiment modifies a component of the COLT5
recipe for COLT5-Base. Static routing divides the input into equal-length blocks and selects the first token in each
block to be routed. Shared QKV routing shares routing decisions for queries and keys/values. In v=all the routed
queries attend to the entire input, while v=q selects the same number of key and value tokens as query tokens.
m=512 and m=1536 use different numbers of routed tokens. LONG T5-B uses a LONG T5encoder while retaining
other parts of the COLT5 training recipe such as MQA and the UL2 objective. Multi-head refers to using multi-head
cross-attention. The final ablation replaces the UL2 objective with PEGASUS as in L ONG T5.
nent for C OLT5 Base.
Routing First, we note that static routing --
evenly distributing routed tokens over the input
-- leads to massive drop in performance. The impor-
tance of routing provides evidence that the model
learns to devote capacity to important tokens and
the advantage of COLT5 is not merely a result of
additional parameters. Sharing routing decisions
for query and KV tokens should be compared with
v=q, and leads to a modest reduction in quality and
increase in speed.
The optimal number of routed tokens represents
a trade-off between improved performance and
computational cost of applying heavier layers. Ta-
ble 6 shows strong gains going from 512 to 1024
(baseline) routed tokens and diminishing returns
for further increases.
Attention COLT5 relies on routing to identify
not only tokens that can benefit from important in-
formation elsewhere in the input, but also which
tokens contain such important information. We
study whether COLT5 is successful in this task by
comparing performance with two different atten-
tion settings -- v=all, in which routed tokens attend
to the entire input, and v=q, which uses equal num-
ber of routed keys and values as queries, rather than
twice as many. C OLT5 appears to occupy a sweet
spot, as using fewer routed key-values modestly de-
creases performance at similar speed but attendingto all inputs barely helps at sharply increased cost.
Other We compare COLT5 toLONG T5with
multi-query cross-attention, confirming that
LONG T5indeed does not achieve an unexpected
quality gain from MQA, and our conservative
assumptions in Figures 2, 4 are valid. Next, we
evaluate multi-head cross-attention for COLT5,
finding that it leads to modestly improved COLT5
performance. However, as MHA exhibits order-
of-magnitude slower inference, MQA is clearly
favored. Finally, PEGASUS appears to fine-tune
slightly better than UL2, though the difference is
small and UL2 enables few-shot learning.
4.6 Routing analysis
It is interesting to ask whether COLT5 routed to-
kens line up with what we consider intuitively
important tokens in each document. We investi-
gate this question by studying routing patterns of a
Large COLT5 model fine-tuned on TriviaQA. We
divide tokens into three categories: (1) question
tokens, (2) answer tokens, and (3) other tokens.
Figure 6 shows the average fraction of each type of
token that is routed through the heavy path for MLP
and attention layers on TriviaQA. We note that
question and answer tokens are significantly more
likely to be routed than other tokens, for feedfor-
ward as well as attention queries and keys/values.
Appendix F presents more detailed routing analy-
sis; e.g., semantically important tokens are much

--- PAGE 9 ---
MLP Query KV0.20.40.6Routed proportionOther
Answer
Question
Figure 6: Proportion of tokens routed for answer (string
match), question, and other tokens by routing compo-
nent for C OLT5 Large model, averaged over examples
in TriviaQA dev set and all layers of model.
more likely to be selected in later layers.
5 Conclusion
We propose COLT5, a new model for long-range
inputs that employs conditional computation for
higher quality and faster speed. COLT5 has light
feedforward and attention layers that apply to the
entire input, as well as heavy branches that are ap-
plied only to a subset of important tokens selected
by a learned router. We show that COLT5 achieves
stronger performance at any speed compared to
LONG T5on a variety of long-input datasets, and
can effectively and efficiently make use of ex-
tremely long inputs up to 64k tokens.
Limitations
COLT5 applies conditional computation only in the
encoder. Applying conditional computation in the
decoder is more complicated; the routing method
inCOLT5 is not causal, so it isn’t applicable when
generating token by token. Since decoder-only
models and applications with long outputs have
become more popular recently, this is a strong limi-
tation of the current approach. Although the rout-
ing method in COLT5 could potentially be applied
to the input context in a decoder-only model, we
didn’t investigate this setup.
COLT5 is specialized towards long sequences
and has to be trained from scratch. For large-scale
training and deployment, it is desirable to either
train a single model that can handle both short and
long sequences, or develop a long-input architec-
ture that can be adapted from an existing large
model.Acknowledgements
We would like to thank Srinadh Bhojanapalli, Luke
Vilnis, Zachary Fisher, Jianmo Ni, Tal Schuster,
Vaclav Cvicek, Sudeep Gandhe, Bhargav Kanagal,
Kenton Lee, Ming-Wei Chang, Afroz Mohiuddin,
Raphael Hoffmann, and others at Google Research
for helpful advice and discussion.
References
Joshua Ainslie, Santiago Ontañón, Chris Alberti, Va-
clav Cvicek, Zachary Fisher, Philip Pham, Anirudh
Ravula, Sumit Sanghai, Qifan Wang, and Li Yang.
2020. ETC: Encoding long and structured inputs in
transformers. arXiv preprint arXiv:2004.08483 .
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake
VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. 2018. JAX: composable transformations of
Python+NumPy programs.
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin
Gimpel. 2022. SummScreen: A dataset for abstrac-
tive screenplay summarization. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
8602–8615, Dublin, Ireland. Association for Compu-
tational Linguistics.
Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509 .
Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
Trung Bui, Seokhwan Kim, Walter Chang, and Nazli
Goharian. 2018. A discourse-aware attention model
for abstractive summarization of long documents. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers) , pages 615–621, New Or-
leans, Louisiana. Association for Computational Lin-
guistics.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,
Noah A. Smith, and Matt Gardner. 2021. A dataset
of information-seeking questions and answers an-
chored in research papers. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 4599–4610, On-
line. Association for Computational Linguistics.
Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie,
Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and

--- PAGE 10 ---
William Cohen. 2022. FiDO: Fusion-in-decoder opti-
mized for stronger performance and faster inference.
arXiv preprint arXiv:2212.08153 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers) ,
pages 4171–4186. Association for Computational
Linguistics.
William Fedus, Barret Zoph, and Noam Shazeer. 2021.
Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. arXiv
preprint arXiv:2101.03961 .
Google. 2020. Profile your model with cloud tpu
tools. https://cloud.google.com/tpu/docs/
cloud-tpu-tools . Accessed: 2022-11-11.
Mandy Guo, Joshua Ainslie, David Uthus, Santiago On-
tañón, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.
2022. LongT5: Efficient text-to-text transformer for
long sequences. In Findings of the Association for
Computational Linguistics: NAACL 2022 , pages 724–
736, Seattle, United States. Association for Compu-
tational Linguistics.
Jonathan Heek, Anselm Levskaya, Avital Oliver, Mar-
vin Ritter, Bertrand Rondepierre, Andreas Steiner,
and Marc van Zee. 2020. Flax: A neural network
library and ecosystem for JAX.
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng
Ji, and Lu Wang. 2021. Efficient attentions for long
document summarization. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1419–1436, Online.
Association for Computational Linguistics.
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics , Van-
couver, Canada. Association for Computational Lin-
guistics.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. CoRR ,
abs/2001.08361.
Tomáš Ko ˇciský, Jonathan Schwarz, Phil Blunsom, Chris
Dyer, Karl Moritz Hermann, Gábor Melis, and Ed-
ward Grefenstette. 2018. The NarrativeQA reading
comprehension challenge. Transactions of the Asso-
ciation for Computational Linguistics , 6:317–328.Yuta Koreeda and Christopher Manning. 2021. Con-
tractNLI: A dataset for document-level natural lan-
guage inference for contracts. In Findings of the
Association for Computational Linguistics: EMNLP
2021 , pages 1907–1919, Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Bernhard Kratzwald and Stefan Feuerriegel. 2018.
Adaptive document retrieval for deep question an-
swering. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing,
Brussels, Belgium, October 31 - November 4, 2018 ,
pages 576–581. Association for Computational Lin-
guistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur P. Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Trans. Assoc. Comput. Linguistics , 7:452–
466.
Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua
Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y .
Zhao, Yuexin Wu, Bo Li, Yu Zhang, and Ming-
Wei Chang. 2023. Conditional adapters: Parameter-
efficient transfer learning with fast inference. In Ad-
vances in Neural Information Processing Systems .
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong
Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.
2021. Reader-guided passage reranking for open-
domain question answering. In Findings of the Asso-
ciation for Computational Linguistics: ACL/IJCNLP
2021, Online Event, August 1-6, 2021 , volume
ACL/IJCNLP 2021 of Findings of ACL , pages 344–
350. Association for Computational Linguistics.
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,
Nikita Nangia, Jason Phang, Angelica Chen, Vishakh
Padmakumar, Johnny Ma, Jana Thompson, He He,
and Samuel R. Bowman. 2021. QuALITY: Question
answering with long input texts, yes! arXiv preprint
arXiv:2112.08608 .
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery,
Jacob Devlin, James Bradbury, Anselm Levskaya,
Jonathan Heek, Kefan Xiao, Shivani Agrawal, and
Jeff Dean. 2022. Efficiently scaling transformer in-
ference. arXiv preprint arXiv:2211.05102 .
Yujie Qian, Jinhyuk Lee, Sai Meher Karthik Duddu,
Zhuyun Dai, Siddhartha Brahma, Iftekhar Naim,
Tao Lei, and Vincent Y Zhao. 2022. Multi-
vector retrieval as sparse alignment. arXiv preprint
arXiv:2211.01267 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res. , 21:140:1–140:67.

--- PAGE 11 ---
Adam Roberts, Hyung Won Chung, Anselm Levskaya,
Gaurav Mishra, James Bradbury, Daniel Andor, Sha-
ran Narang, Brian Lester, Colin Gaffney, Afroz
Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz,
Alex Salcianu, Marc van Zee, Jacob Austin, Se-
bastian Goodman, Livio Baldini Soares, Haitang
Hu, Sasha Tsvyashchenko, Aakanksha Chowdh-
ery, Jasmijn Bastings, Jannis Bulian, Xavier Gar-
cia, Jianmo Ni, Andrew Chen, Kathleen Kenealy,
Jonathan H. Clark, Stephan Lee, Dan Garrette, James
Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin
Ritter, Maarten Bosma, Alexandre Passos, Jeremy
Maitin-Shepard, Noah Fiedel, Mark Omernick, Bren-
nan Saeta, Ryan Sepassi, Alexander Spiridonov,
Joshua Newlan, and Andrea Gesmundo. 2022. Scal-
ing up models and data with t5xandseqio .arXiv
preprint arXiv:2203.17189 .
Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani,
Dara Bahri, Vinh Q Tran, Yi Tay, and Donald Metzler.
2022. Confident adaptive language modeling. arXiv
preprint arXiv:2207.07061 .
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori
Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Mor Geva, Jonathan Berant, and Omer Levy. 2022.
Scrolls: Standardized comparison over long language
sequences. ArXiv , abs/2201.03533.
Noam Shazeer. 2019. Fast transformer decoding:
One write-head is all you need. arXiv preprint
arXiv:1911.02150 .
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc V . Le, Geoffrey E. Hinton, and
Jeff Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. In 5th
International Conference on Learning Representa-
tions, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings . OpenReview.net.
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
InProceedings of the 35th International Conference
on Machine Learning, ICML 2018, Stockholmsmäs-
san, Stockholm, Sweden, July 10-15, 2018 , volume 80
ofProceedings of Machine Learning Research , pages
4603–4611. PMLR.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,
Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. 2021. Long
range arena : A benchmark for efficient transformers.
InInternational Conference on Learning Representa-
tions .
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-
cia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,
Neil Houlsby, and Donald Metzler. 2022. Unify-
ing language learning paradigms. arXiv preprint
arXiv:2205.05131 .
Neeraj Varshney, Man Luo, and Chitta Baral. 2022. Can
open-domain QA reader utilize external knowledge
efficiently like humans? CoRR , abs/2211.12707.Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo
Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry
Tesauro, Bowen Zhou, and Jing Jiang. 2018. R3: Re-
inforced ranker-reader for open-domain question an-
swering. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence, (AAAI-18), the
30th innovative Applications of Artificial Intelligence
(IAAI-18), and the 8th AAAI Symposium on Educa-
tional Advances in Artificial Intelligence (EAAI-18),
New Orleans, Louisiana, USA, February 2-7, 2018 ,
pages 5981–5988. AAAI Press.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 .
Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao
Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-
ing Yang, and Michael Zeng. 2022. Kg-fid: Infus-
ing knowledge graph in fusion-in-decoder for open-
domain question answering. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022 , pages 4961–
4974. Association for Computational Linguistics.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tañón, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. 2020. Big bird: Transformers for
longer sequences. Advances in Neural Information
Processing Systems , 33:17283–17297.
Yury Zemlyanskiy, Joshua Ainslie, Michiel de Jong,
Philip Pham, Ilya Eckstein, and Fei Sha. 2021. Read-
twice: Reading very large documents with memories.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 5189–5195.
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-
ter Liu. 2020. Pegasus: Pre-training with extracted
gap-sentences for abstractive summarization. In In-
ternational Conference on Machine Learning , pages
11328–11339. PMLR.
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia
Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli
Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir
Radev. 2021. QMSum: A new benchmark for query-
based multi-domain meeting summarization. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
5905–5921, Online. Association for Computational
Linguistics.
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du,
Yanping Huang, Jeff Dean, Noam Shazeer, and
William Fedus. 2022. St-moe: Designing stable and
transferable sparse expert models. arXiv preprint
arXiv:2202.08906 .

--- PAGE 12 ---
Model Layers Model dim MLP lightdim MLP heavy dim Heads light Heads heavy Params
LONG T5-B 12 768 2048 N/A 12 N/A 248m
COLT5-B 12 768 1024 8096 4 8 433m
LONG T5-L 24 1024 2816 N/A 16 N/A 783m
COLT5-L 24 1024 1408 11264 4 12 1462m
LONG T5-XL 24 2048 5120 N/A 32 N/A 2850m
COLT5-XL 24 2048 2560 20480 8 24 5297m
Table 7: Hyperparameters for LONG T5andCOLT5 models. T5.1.1 hyperparameters match LONG T5.COLT5
parameters are sparsely accessed as a result of conditional computation, so parameter counts do not reflect compute,
and for a given model size C OLT5 is in fact faster than L ONG T5 despite having more parameters.
A Contributions
Joshua led the project, developed the initial con-
ditional attention mechanisms, and conducted
most experimental ablations. Tao developed the
heavy/light formulation for heterogeneous condi-
tional computation, comprising the routing and con-
ditional feedforward mechanisms, and iterated with
Joshua on initial experiments demonstrating fea-
sibility. Michiel helped to scope the paper, per-
formed most of the writing, and oversaw speed
benchmarking. Santiago designed and conducted
all the few-shot experiments, initiated the routing
analysis visualization, and integrated UL2 into the
codebase. Siddhartha developed the separate rout-
ing for query and key/value tokens in the condi-
tional attention component and demonstrated the
resulting quality improvements. Yury designed and
conducted all experiments for inputs larger than
16k tokens, demonstrating favorable scaling up to
64k. David integrated all SCROLLS tasks into
the codebase and ran early experiments, especially
comparing UL2 with PEGASUS. Mandy devel-
oped the leaderboard comparisons with LongT5
and helped run several experiments. James advised
on and ran early comparisons with MoE conditionalcomputation. Yi advised on the adaptation of UL2
to 4k input length pre-training. Finally, Yun-Hsuan
and Sumit provided guidance and support for the
project overall.
B Model Hyperparameters
Table 7 shows LONG T5andCOLT5 hyperparam-
eters, including parameter counts. For LONG T5,
we report numbers for the TGlobal configuration,
which match T5.1.1. Notice that COLT5’s parame-
ter counts are larger due to using conditional com-
pute. Similar to other conditional compute archi-
tectures such as mixture-of-experts, computational
cost does not necessarily increase with parameter
count.
We use the same 127-token local radius for
COLT5 asLONG T5. This results in a local atten-
tion window wof 255 since 127 tokens are attended
to the left and 127 to the right.
C Routing Normalization
Hyperparameters
To normalize the routing scores for differentiable
top-ktoken selection, we use the iterative soft top-
kalgorithm from Lei et al. (2023) and Qian et al.
Model Average 16k in, 128 out 16k in, 512 out 16k in, 1024 out 8k in, 128 out
Enc Tot Enc Tot Enc Tot Enc Tot Enc Tot
LONG T5-B 77 136 84 98 84 165 84 296 27 39
COLT5-B 29 90 30 45 30 113 30 256 18 30
LONG T5-L 164 329 173 222 179 392 179 799 66 100
COLT5-L 70 201 73 103 73 250 73 578 45 69
LONG T5-XL 390 870 412 557 423 1081 423 2065 166 290
COLT5-XL 177 439 185 239 185 525 185 1253 115 163
Table 8: Comparison of total and encoder inference time per sample (ms) for LONG T5andCOLT5 Base, Large,
and XL models at different input and output lengths. Average time per sample is computed as a weighted average
over input and output lengths, weighted by the number of tasks in our evaluation that use the corresponding setting
(4 for 16k/128, 3 for 16k/512, and one each for 16k/1024 and 8k/128).

--- PAGE 13 ---
Model arXiv SummScreenFD QMSum GovRep
R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L
LONG T5-B 47.4 21.4 43.5 34.8 9.3 20.7 35.1 11.1 23.4 59.3 30.1 33.0
COLT5-B 47.5 21.3 43.6 35.6 9.7 21.0 34.6 10.9 23.0 60.2 31.0 32.8
LONG T5-L 47.9 21.7 43.8 35.3 9.1 20.8 35.9 12.0 24.1 61.4 32.5 34.1
COLT5-L 48.4 21.7 44.3 35.7 10.1 21.4 36.8 12.6 24.7 61.8 32.7 34.4
LONG T5-XL 48.2 21.8 44.1 36.6 10.3 21.5 37.0 12.5 24.7 61.8 33.2 34.8
COLT5-XL 48.4 22.0 44.3 36.3 10.0 21.5 37.4 13.0 25.1 62.2 33.3 34.9
Table 9: Full performance comparison with Rouge-1, Rouge-2, and Rouge-L metrics of COLT5 andLONGT5Base,
Large, and XL models on summarization dev sets. Results based on checkpoint that maximizes R gmas in Table 3.
(2022) with ϵ= 1.0and 50 iterations. During
training we allow the top9
8ktokens to have nonzero
weight instead of just the top kin order to provide
a slightly improved training signal.
D Additional Experimental Results
Table 8 compares LONG T5andCOLT5 inference
speed in more detail, splitting off encoder and total
time per sample. Since COLT5 applies conditional
computation only in the encoder, encoder speed
gains are larger than overall speed gain, and total
speed gains are largest for shorter output length.
Trade-offs are even more in the favor of COLT5
when paired with other decoder optimizations.
Table 9 shows full (Rouge-1, Rouge-2, Rouge-L)
results for summarization datasets.
E Computational Resources
For pre-training we generally used 128 TPUv4
chips for Base and 256 TPUv4 chips for Large
and XL. Pre-training took approximately 2.5 days
for Base, 3.7 days for Large, and 12.8 days for XL.
For fine-tuning we generally used 64, 128, and 256
TPUv4 chips for Base, Large, and XL, respectively,
with training time varying with dataset size.F Routing Analysis
In this section we take a closer look at the routing
mechanisms in COLT5. There are three routing
processes in each layer of COLT5: (1) Routing
of attention keys and values (“KV-routing”), (2)
routing of attention queries (“Q-routing”) and (3)
routing of MLP tokens (“MLP-routing”). For sim-
plicity, we will say that a token is selected , when it
is routed to the heavy alternative (of either MLP or
attention). We are interested in understanding what
tokens are selected and whether these mechanisms
select similar or different tokens in each layer.
Which tokens are selected We divide input to-
kens into three categories: (1) question tokens, (2)
answer tokens (found via simple normalized string
match of the ground truth answer), and (3) other to-
kens. Figure 7 shows the proportion of each token
type that is routed by a fine-tuned COLT5-Large
model on the TriviaQA dev set, by layer and rout-
ing component.
Earlier we showed that question and answer to-
kens are more likely to be selected, but separat-
ing routing decisions by layer reveals interesting
patterns. At early layers question and answer to-
0 10 200.20.40.60.81
LayerRouting proportionMLP
0 10 2000.20.40.60.8
LayerQuery
0 10 200.20.40.60.81
LayerKV
Other Answer Question
Figure 7: Proportion of tokens routed for answer (string match), question, and other tokens by routing component
and layer for C OLT5 Large model, averaged over examples in TriviaQA dev set.

--- PAGE 14 ---
The same is true 
for tokens around 
the correct answer 
(“ papageno ” in 
this example). Question is 
heavily routed to 
the expensive 
alternative by last 
layers of the 
model. Figure 8: Visualization of token routing weights for some fragments of an example on TriviaQA.
kens are only modestly more likely to be selected,
with routing probability sharply increasing at later
layers and peaking in the last layer. This makes
intuitive sense: in early layers the model has not
yet had the opportunity to identify which tokens
and parts of the document are important. However,
the increase is not monotonic and there is strong
variation between layers. This variation may im-
ply that different layers focus on different types of
tokens, or that some routing components do not
successfully learn to identify important tokens.
To gain a better insight into this, Figure 8 vi-
sualizes routing on two sample fragments from a
TriviaQA example (notice that, given the large in-
put length used in COLT5, we do not show the
complete example in the figure). The two frag-
ments shown correspond to the beginning of the
example (where the question is located), and the
part of the context surrounding the correct answer.
We have added a colored background to the figure,
where each of the three CMY channels are mapped
to the KV-routing weights in different layers of the
model. Cyan corresponds to layer 1, Magenta to
layer 12, and Yellow to layer 24. As we can see,
question and answer are heavily yellow colored,
showing those tokens are selected in the last layer.
Correlation between routing processes. Table
10 shows the Pearson correlation coefficient be-
tween the routing weights of the different routing
mechanisms in each layer in a COLT5 Large model
(MLP-routing correlation with KV-routing, MLP-
routing with Q-routing, and KV-routing with Q-
routing). We show numbers for both the pre-trained
checkpoint, as well as a fine-tuned model on Trivi-
aQA. As we can see, the routing of keys/values androuting of queries is highly correlated at all layers
except the first two, while the routing of tokens
in the MLP has lower correlation to the other two
processes. Interestingly correlation between MLP
and attention routing increases in the last layers of
the model.
Pre-trained Fine-tuned
MLP-KV MLP-Q KV-Q MLP-KV MLP-Q KV-Q
1 -0.06 -0.06 -0.09 -0.06 -0.09 -0.26
2 0.27 0.52 0.04 0.27 0.39 0.02
3 -0.05 -0.03 0.75 0.05 -0.01 0.69
4 0.05 0.09 0.76 0.18 0.14 0.72
5 0.02 -0.01 0.75 0.22 0.26 0.68
6 0.02 -0.01 0.78 0.31 0.33 0.70
7 0.02 0.00 0.73 0.26 0.27 0.70
8 0.00 -0.02 0.44 0.11 -0.07 0.29
9 0.13 0.11 0.74 0.36 0.40 0.70
10 -0.06 -0.08 0.08 -0.15 -0.15 0.12
11 -0.05 -0.07 0.31 -0.08 -0.03 0.18
12 -0.04 -0.08 0.27 0.03 0.00 0.28
13 -0.10 -0.09 0.87 -0.13 -0.03 0.72
14 -0.04 -0.05 0.76 -0.06 -0.12 0.67
15 0.53 0.64 0.69 0.51 0.55 0.67
16 0.08 0.12 0.63 0.06 0.57 0.24
17 0.28 0.30 0.65 0.27 0.32 0.69
18 0.28 0.02 0.84 0.31 0.20 0.76
19 0.45 0.77 0.59 0.19 0.38 0.64
20 0.30 0.39 0.64 0.38 0.47 0.62
21 0.05 -0.04 0.49 0.18 0.11 0.47
22 0.05 0.00 0.69 0.21 0.16 0.68
23 0.39 0.33 0.68 0.60 0.79 0.69
24 0.43 0.39 0.59 0.57 0.63 0.65
Table 10: Pearson correlation coefficient between the
routing weights of the different routing mechanisms in
each layer in a COLT5 Large model. We show numbers
for both the pre-trained checkpoint, as well as a fine-
tuned model on TriviaQA. Blue bars visualize positive
correlation, whereas red bars visualize negative correla-
tion.
