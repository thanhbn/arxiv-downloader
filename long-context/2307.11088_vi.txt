--- TRANG 15 ---
preprint
Bảng 6: Hiệu suất của các mô hình khác nhau trên các tập dữ liệu QA mở về điểm F1. Đối với kết quả được kiểm tra với các thước đo N-gram, vui lòng lưu ý rằng kết quả có thể không chính xác khi hiệu suất của các mô hình rất tương tự hoặc có sự khác biệt lớn về mức độ chi tiết của đầu ra.
Mô hình Ret. Tokens Fin. Contract Multidoc Nrtv NQ SCI Trung bình
Turbo-16k-0613 ✗ 16k 45.36 24.87 31.45 18.20 45.90 28.25 32.33
AdaEmb-Turbo-0613 ✓ 4k 39.69 24.09 35.62 18.59 49.66 33.36 33.50
BM25-Turbo-0613 ✓ 4k 40.79 26.10 35.17 16.32 53.73 25.83 32.99
Cắt ngắn tokens đầu vào đến độ dài bối cảnh tiền huấn luyện
Llama2-7b-chat ✗ 4k 40.06 23.00 27.28 13.48 28.11 25.95 26.31
Llama2-13b-chat ✗ 4k 38.07 23.14 26.14 16.76 35.43 27.46 27.83
Vicuna1.3-7b ✗ 2k 30.49 17.69 17.70 14.57 15.49 7.69 17.27
Longchat-7b-16k ✗ 2k 27.27 19.78 13.99 13.21 18.11 7.61 16.66
Chatglm2-6b-8k ✗ 2k 29.60 19.06 16.22 13.21 17.52 12.26 17.97
XGen-7b-8k (2k-4k-8k) ✗ 2k 34.43 21.28 21.59 14.97 29.58 14.12 22.66
Cắt ngắn tokens đầu vào đến độ dài bối cảnh tinh chỉnh thêm
Chatglm2-7b-32k ✗ 32k 30.27 26.95 24.97 14.00 37.94 26.44 26.76
Longchat1.5-7b-32k ✗ 32k 36.06 18.16 14.96 11.79 24.92 12.09 19.66
Longchat-7b-16k ✗ 16k 38.37 26.78 8.31 15.17 20.21 9.74 19.76
Vicuna1.5-7b-16k ✗ 16k 39.31 18.04 18.44 8.19 19.39 21.80 20.86
Longchat-13b-16k ✗ 16k 37.85 21.11 12.18 14.76 22.75 14.95 20.60
Vicuna1.5-13b-16k ✗ 16k 45.57 18.16 15.88 15.03 37.13 23.40 25.86
Llama2-13b-NTK* ✗ 16k 30.99 15.88 13.61 6.89 11.13 15.58 15.67
Llama2-13b-NTK(Dyn)* ✗ 16k 39.99 18.59 25.49 13.09 14.51 26.90 23.09
Longchat-13b-16k ✗ 8k 36.94 16.70 10.77 7.55 14.14 9.91 16.00
Chatglm2-6b-8k ✗ 8k 33.17 15.76 13.76 7.02 3.50 6.36 13.26
XGen-7b-8k ✗ 8k 36.40 22.01 17.08 9.41 13.88 20.23 19.83
MPT-7b-65k ✗ 8k 10.01 6.24 3.95 1.77 0.77 1.68 4.06
Kết quả từ các thước đo n-gram Kiểm tra tất cả các trường hợp trong các nhiệm vụ mở trong L-Eval với GPT-4 là khả thi. Để đưa ra tổng quan về tất cả các mô hình trên các nhiệm vụ mở, chúng tôi kiểm tra tất cả các mô hình với các thước đo n-gram. Như có thể thấy từ tỷ lệ thắng từ các thẩm phán LLM (Bảng 4) và đánh giá con người (Bảng 5), vẫn còn một biên độ đáng kể giữa các LLMs thương mại và các LLMs mã nguồn mở. Tuy nhiên, biên độ này không đủ rõ ràng dựa trên các thước đo n-gram. Dựa trên các thước đo n-gram, các LCLMs mã nguồn mở cũng thất bại trong việc đánh bại mô hình bối cảnh ngắn gốc của chúng trên bối cảnh bị cắt ngắn. Nhìn chung, các LCLMs mã nguồn mở hiện tại nói chung xuất sắc hơn trong các nhiệm vụ tóm tắt thông thường liên quan đến hướng dẫn như "Tóm tắt tài liệu này" so với tóm tắt dựa trên truy vấn và
16

--- TRANG 16 ---
preprint
Bảng 7: Hiệu suất của các mô hình khác nhau trên các nhiệm vụ tóm tắt và tạo ra dựa trên truy vấn về ROUGE.
Mô hình Tokens giấy trợ lý đánh giá tóm tắt cuộc họp tóm tắt Trung bình
R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L
Turbo-16k-0613 16k 39.55 10.92 18.61 30.18 7.14 18.67 30.20 7.22 19.31 20.20
AdaEmb-Turbo-0613 4k 38.07 9.61 17.33 29.81 6.47 18.91 31.92 8.24 20.84 20.13
BM25-Turbo-0613 4k 41.59 13.39 21.24 29.89 5.99 18.19 31.37 8.50 20.65 21.20
Cắt ngắn tokens đầu vào đến độ dài bối cảnh tiền huấn luyện
Llama2-7b-chat 4k 37.15 9.47 18.05 29.75 6.61 18.96 28.75 6.24 19.37 19.37
Llama2-13b-chat 4k 37.27 9.79 18.49 30.49 6.69 19.23 29.63 6.54 19.65 19.75
Vicuna1.3-7b 2k 34.63 8.73 16.87 29.01 6.28 18.18 24.18 4.93 15.93 17.63
Longchat-7b-16k 2k 37.01 9.61 18.21 26.45 5.05 16.88 23.92 4.65 15.75 17.50
Chatglm2-6b-8k 2k 36.91 9.45 17.96 27.74 5.77 17.62 25.92 5.61 17.57 18.28
XGen-7b (2k-4k-8k) 2k 37.72 9.97 18.77 28.21 5.94 18.69 26.94 5.92 18.24 18.93
Cắt ngắn tokens đầu vào đến độ dài bối cảnh tinh chỉnh thêm
Chatglm-6b-32k 32k 32.65 8.09 16.51 22.05 6.10 16.61 28.94 8.86 20.83 17.84
Longchat1.5-7b-32k 32k 32.49 7.79 15.97 27.53 5.80 17.94 25.29 5.22 16.49 17.16
Longchat-7b-16k 16k 35.05 8.57 16.70 26.07 5.97 17.06 20.13 4.74 13.21 16.38
Vicuna1.5-7b-16k 16k 36.84 9.78 17.66 28.91 6.47 18.25 26.90 5.53 17.33 18.63
Longchat-13b-16k 16k 34.41 8.07 16.45 27.24 5.63 17.00 24.58 5.85 16.32 17.28
Vicuna1.5-13b-16k 16k 36.30 8.69 18.20 28.59 6.15 18.49 27.82 6.39 18.83 18.82
Llama2-13b-NTK* 16k 35.22 8.53 17.04 23.97 4.72 14.89 18.92 4.13 13.16 15.61
Llama2-13b-NTK(Dyn)* 16k 28.89 7.21 14.83 26.86 5.33 17.55 22.29 4.88 15.29 15.90
Longchat-13b-16k 8k 34.29 8.21 16.06 26.76 5.61 16.77 20.86 4.01 13.81 16.26
Chatglm2-6b-8k 8k 38.07 9.61 17.33 29.81 6.47 18.91 24.74 4.45 4.44 18.36
XGen-7b-8k 8k 35.94 8.49 17.92 28.92 6.28 19.11 28.06 6.12 19.17 18.89
MPT-7b-65k 8k 15.91 2.91 11.18 7.66 1.00 7.00 5.24 0.71 5.10 6.30
các nhiệm vụ QA. Đối với các nhiệm vụ dựa trên truy vấn đặt câu hỏi từ một góc độ cụ thể, hiệu suất có thể bị suy giảm đáng kể nếu hướng dẫn không được hiểu đầy đủ. Như chúng tôi đã đề cập trước đây, độ dài đầu vào tăng cũng có thể làm giảm khả năng của mô hình để hiểu các hướng dẫn dài, do đó ức chế khả năng tạo ra câu trả lời phù hợp chặt chẽ với độ dài của sự thật chuẩn. Hiện tượng này ít có khả năng được quan sát với các LLMs tinh vi hơn (tức là Turbo-16k). Một giải pháp ngây thơ là thêm hướng dẫn ở cả đầu và cuối của đầu vào dài nhưng vẫn còn chỗ để cải thiện khả năng hiểu hướng dẫn cho LCLMs.
Các mô hình dựa trên truy xuất so với các mô hình bối cảnh dài Chúng tôi so sánh một đường cơ sở LCLM đại diện Turbo-16k-0613 với phiên bản ngắn của nó Turbo-4k-0613 nhưng được tăng cường với truy xuất trong Bảng 3 (các nhiệm vụ đóng) và Bảng 4 (các nhiệm vụ mở). Chúng tôi sử dụng một bộ truy xuất thưa thớt bm25 và một bộ truy xuất dày đặc mạnh text-embedding-ada-002. Các phương pháp dựa trên truy xuất nói chung mang lại kết quả tốt hơn cho các nhiệm vụ có câu trả lời dễ truy xuất. Ví dụ, đối với hiểu bài giảng dài nơi tài liệu dài luôn chứa một số định nghĩa và giải thích cho một số thuật ngữ học thuật, các phương pháp dựa trên truy xuất có được kết quả tốt hơn. Tuy nhiên, truy xuất không phải là một giải pháp tổng quát vì hiệu suất của nó có liên quan mạnh mẽ đến hướng dẫn và phong cách tài liệu. Ví dụ, chúng sẽ không bao giờ trả lời các câu hỏi như có bao nhiêu câu trong một tài liệu. Kết quả của chúng tôi cho thấy CodeU và GSM(16-shot) trong L-Eval không thể được giải quyết bằng truy xuất. Các phương pháp dựa trên truy xuất cũng gặp khó khăn trong việc tự động xác định truy vấn từ đầu vào người dùng. Các phương pháp truy xuất thể hiện hiệu suất tương đối kém thỏa đáng trong các nhiệm vụ mà câu trả lời không thể được truy xuất, như truy xuất chủ đề hoặc các nhiệm vụ đòi hỏi các mô hình có khả năng suy luận tầm xa như QA tài chính. Các mô hình dựa trên truy xuất tạo ra kết quả tương tự hoặc thậm chí vượt trội cho các nhiệm vụ tóm tắt. Điều này có thể là do một số đoạn văn giống như tóm tắt có thể được truy xuất. Bên cạnh đó, chúng tôi cũng nhận thấy rằng lý do chính khiến Turbo-0613 thông thường vượt trội hơn Turbo-16k là khả năng vượt trội của nó trong việc tuân theo hướng dẫn một cách chính xác. Tuy nhiên, ngay cả đối với những nhiệm vụ này, có những trường hợp mà câu trả lời dự đoán có thể là "Tôi không biết" hoặc "không được đề cập" do hạn chế của quá trình truy xuất. Khi
17

--- TRANG 17 ---
preprint
Bảng 8: Hiệu suất của các mô hình khác nhau trên các nhiệm vụ tóm tắt tài liệu dài về ROUGE.
Mô hình Tokens báo cáo chính phủ tin tức bằng sáng chế chương trình TV Trung bình
R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L
Turbo-16k-0613 16k 45.9 15.6 23.6 35.3 8.1 16.1 46.0 20.3 29.3 32.0 5.4 16.9 24.5
AdaEmb-Turbo-0613 4k 45.0 14.3 20.8 35.7 7.7 15.4 45.6 15.9 27.6 30.0 3.3 15.2 23.0
bm25-Turbo-0613 4k 44.6 14.2 21.5 38.4 9.1 16.8 43.3 15.5 27.1 31.0 4.6 15.4 23.4
Cắt ngắn tokens đầu vào đến độ dài bối cảnh tiền huấn luyện
llama2-7b-chat 4k 43.7 15.3 22.2 33.2 6.4 15.5 49.2 22.9 31.6 29.4 4.8 15.6 24.1
llama2-13b-chat 4k 46.3 16.1 24.0 34.9 8.1 16.3 48.4 20.9 30.3 32.6 6.6 17.2 25.1
vicuna1.3-7b 2k 44.6 16.4 23.2 32.9 6.9 14.8 44.7 20.4 28.8 28.7 3.6 14.8 23.3
longchat-7b-16k 2k 43.6 16.2 23.7 28.1 4.8 13.0 47.0 22.2 30.9 27.2 3.0 14.4 22.8
chatglm2-6b-8k 2k 45.2 18.3 24.6 32.1 6.9 15.0 44.6 22.1 30.0 26.4 2.6 13.8 23.4
xgen-7b-8k 2k 45.1 17.2 22.9 35.0 7.5 15.5 49.6 25.2 34.6 28.8 3.6 15.4 23.9
Cắt ngắn tokens đầu vào đến độ dài bối cảnh tinh chỉnh thêm
chatglm2-6b-32k 32k 38.1 16.1 21.0 24.2 5.8 12.8 46.5 24.1 32.5 23.4 4.2 13.8 21.8
longchat1.5-7b-32k 32k 45.7 17.7 24.0 36.8 8.7 15.7 42.0 18.2 27.2 21.5 2.7 13.0 22.7
longchat-7b-16k 16k 47.2 18.9 23.9 27.7 5.4 13.4 46.2 20.9 30.1 26.2 3.3 14.7 23.1
vicuna1.5-7b-16k 16k 47.2 18.9 25.0 32.3 6.8 15.5 48.1 25.1 32.4 26.0 3.6 14.8 24.6
longchat-13b-16k 16k 46.2 18.2 24.1 35.2 7.6 15.8 45.3 22.6 29.8 31.9 6.0 17.3 24.0
vicuna1.5-13b-16k 16k 45.2 17.9 24.2 31.6 6.8 15.2 46.1 21.8 30.0 28.3 3.7 16.3 23.9
llama2-13b-NTK 16k 33.0 11.0 17.7 26.0 6.4 13.5 37.9 13.5 22.9 25.6 5.3 14.0 18.9
llama2-13b-NTK(Dyn) 16k 42.0 14.9 22.4 34.0 7.8 15.9 45.3 19.1 28.5 25.5 3.9 13.9 22.7
longchat-13b-16k 8k 49.3 19.5 25.1 34.9 7.4 15.5 43.5 20.1 28.0 31.0 4.5 15.7 24.5
chatglm2-6b 8k 40.6 14.3 21.5 32.9 7.2 15.1 46.3 22.3 31.4 27.5 2.6 14.5 23.0
xgen-7b-8k 8k 40.2 13.8 21.1 31.9 6.0 15.3 45.9 21.4 29.2 28.2 3.3 15.2 22.6
mpt-7b-65k 8k 33.3 10.7 19.3 13.6 1.5 9.2 25.5 12.2 20.2 11.0 1.3 6.4 13.6
đánh giá các bộ truy xuất, bm25 thường khớp với hiệu suất của bộ truy xuất dày đặc, ada-embedding, trong các nhiệm vụ đóng. Tuy nhiên, trong các nhiệm vụ mở, bộ truy xuất dày đặc ada-embedding vượt trội hơn BM25 hơn hai điểm. Hiệu suất vượt trội này có thể được quy cho khả năng của bộ truy xuất dày đặc tận dụng không chỉ khớp thuật ngữ mà còn cả khớp ngữ nghĩa.
Hình 6: Hiệu suất của các phương pháp dựa trên NTK khác nhau khi giải quyết độ dài đầu vào ở nhiều quy mô. Các quy tắc mở rộng NTK động không đúng trong các nhiệm vụ thực tế Nhúng vị trí NTK-aware động LocalLLaMA (2023a) đang trở nên ngày càng phổ biến cho ngoại suy mà không cần huấn luyện thêm. Dựa trên NTK động, cho một chuỗi đầu vào với độ dài L và độ dài tiền huấn luyện mô hình l, chúng ta có thể đặt cơ sở gốc 10,000 trong RoPE thành 10,000×(L/l)^(d/(d−2)) trong đó d là chiều đầu, nếu chúng ta muốn thích ứng mô hình với độ dài bối cảnh dài hơn L. Chúng tôi thấy rằng quy tắc mở rộng không đúng trong các nhiệm vụ thực tế khi số lượng tokens đầu vào thay đổi. Các cải thiện có thể được cải thiện thêm nếu sử dụng một số biến thể của NTK. Chúng tôi nghiên cứu 2 sửa đổi đơn giản trên NTK động gốc: (1) NTK-bias có nghĩa là chúng tôi sử dụng cơ sở 10,000×((L/l)+1)^(d/(d−2)) trong đó 1 là thiên vị (2) NTK-weighted có nghĩa là chúng tôi sử dụng cơ sở 10,000×(L/l*2)^(d/(d−2)). Kết quả được hiển thị trong Hình 6 trong đó Llama2-PI-sharegpt là một đường cơ sở được tinh chỉnh sử dụng nội suy vị trí. Chúng tôi kiểm tra kết quả của 4 mô hình bằng cách cắt ngắn độ dài đầu vào của các trường hợp kiểm tra trong Coursera. Chúng ta có thể quan sát rằng việc sử dụng biến thể nào của NTK bị ảnh hưởng mạnh mẽ bởi tokens tối đa của tập dữ liệu. Khi độ dài đầu vào nằm giữa 4k và 8k, NTK+bias có được kết quả tốt nhất và đường cơ sở NTK-weighted mạnh mẽ hơn trên 16k tokens đầu vào.
18

--- TRANG 18 ---
preprint
Hình 7: Kết quả tổng thể trên các nhiệm vụ mở và đóng. Chúng tôi thấy rằng GPT-4-32k có khả năng hơn trong các nhiệm vụ đóng thể hiện khả năng suy luận mạnh mẽ trên bối cảnh dài vì hầu hết các nhiệm vụ đóng trong L-Eval có ít hơn 32k tokens đầu vào, nhưng độ dài bối cảnh 100k giúp Claude vượt qua cả GPT-4-32k và Turbo-16k trên các nhiệm vụ mở thường có nhiều tokens đầu vào hơn.
Hình 8: Kết quả tổng thể trên các nhiệm vụ truy xuất chủ đề. Việc kiểm tra các mô hình bối cảnh ngắn trên nhiệm vụ này với văn bản đầu vào bị cắt ngắn là không công bằng, vì vậy chúng tôi chỉ bao gồm các LLMs bối cảnh dài.
19

--- TRANG 19 ---
preprint
B THU THẬP VÀ CHÚ THÍCH DỮ LIỆU CHO L-EVAL
Trong việc theo đuổi dữ liệu đa dạng, toàn diện và có liên quan, chúng tôi lấy nguồn các tập dữ liệu từ nhiều nền tảng và nguồn khác nhau. Các tập dữ liệu này, đại diện cho các khía cạnh khác nhau của cuộc sống hàng ngày và các lĩnh vực chuyên môn và trình bày những thách thức khác nhau cho LCLMs. Chúng tôi tận dụng các tài nguyên từ các tập dữ liệu mã nguồn mở trước đây, phụ đề Coursera, bản ghi cuộc gọi thu nhập từ các trang web công ty, GitHub, v.v. Các phong cách hướng dẫn trong L-Eval bao gồm câu hỏi trắc nghiệm, toán học trường với nhiều ví dụ, truy xuất chủ đề chính từ các đối thoại dài, tóm tắt văn bản, và hỏi đáp tóm tắt, bao gồm một loạt các nhiệm vụ. Việc xây dựng từng tập dữ liệu và nỗ lực của chúng tôi để làm cho nó thách thức hơn như sau.
B.1 TOFEL (KIỂM TRA TIẾNG ANH)
Tập dữ liệu này có nguồn gốc từ TOEFL Practice Online và chúng tôi thu thập dữ liệu từ TOEFL-QA (Tseng et al., 2016; Chung et al., 2018) và tất cả các bài giảng từ một TPO đơn đã được hợp nhất thành một bài giảng dài. Sau khi hợp nhất, chúng tôi chọn 15 bài giảng dài nhất.
Ví dụ 1
Đầu vào: <Nhiều bài giảng dài> \n\n
Câu hỏi: tại sao Frantzen đến chợ bán gia súc
A. để nghiên cứu hình thể và chuyển động của con người
B. để kiếm tiền bằng cách vẽ chân dung
C. để vẽ động vật trang trại trong môi trường ngoài trời
D. để gặp những người có thể làm mẫu cho bức tranh của cô
\n\n Câu trả lời:
Sự thật chuẩn: A
B.2 GSM(16-SHOT) (TOÁN HỌC CẤP TRƯỜNG)
Tập dữ liệu này được lấy từ 100 bài toán học cấp trường trong tập dữ liệu GSM8k (Cobbe et al., 2021). Việc tăng số lượng ví dụ chất lượng cao và phức tạp thường có tác động tích cực đến việc giải các bài toán học. Chúng tôi xây dựng 16 ví dụ trong bối cảnh với Chuỗi Suy nghĩ dài cho nhiệm vụ này trong đó 8 ví dụ đến từ chain-of-thought-hub¹⁷ sử dụng prompt khó nhất và 8 ví dụ còn lại được xây dựng bởi chúng tôi. Chúng tôi chọn 8 câu hỏi từ GSM8k dựa trên độ khó của chúng và chú thích quá trình giải. Các mô hình với độ dài bối cảnh 2k hoặc 4k gặp khó khăn khi mã hóa 16 ví dụ. Chúng tôi thí nghiệm với các ví dụ mới xây dựng và nó hoạt động tốt hơn chỉ mã hóa 8 ví dụ. Cụ thể, độ chính xác tăng từ 79 (8-shot) lên 84 (16-shot) khi sử dụng turbo-16k-0613 làm mô hình cơ sở.
Ví dụ 2
Đầu vào: <ví dụ 1> \n\n <ví dụ 2> \n\n ... <ví dụ n> \n\n
Câu hỏi: Vịt của Janet đẻ 16 quả trứng mỗi ngày. Cô ăn ba quả cho bữa sáng mỗi sáng và nướng bánh muffin cho bạn bè mỗi ngày với bốn quả. Cô bán phần còn lại tại chợ nông dân hàng ngày với giá $2 mỗi quả trứng vịt tươi. Cô kiếm được bao nhiêu đô la mỗi ngày tại chợ nông dân? \n\n
Hãy suy nghĩ từng bước
Sự thật chuẩn: 18
B.3 QUALITY (GUTENBERG)
Tập dữ liệu này có nguồn gốc từ tập dữ liệu QA trắc nghiệm QuALITY (Pang et al., 2022) chứa các câu hỏi trắc nghiệm được lấy từ văn học trên Gutenberg. Chúng tôi lọc 20 câu chuyện dài và 202 câu hỏi và sửa chữa/xóa các câu hỏi có lỗi chú thích. Chúng tôi thấy rằng hầu hết các câu hỏi trong QuALITY có thể được giải quyết bằng cách trích xuất các đoạn văn từ văn bản dài. Chúng tôi tăng cường thêm một số câu hỏi tổng hợp cần hiểu biết toàn cục về tài liệu. Ví dụ về các câu hỏi tổng hợp được chú thích như sau:
1. Chúng ta có thể suy ra gì từ câu dài nhất trong câu chuyện?
2. Đối thoại dài nhất được nói bởi ai?
3. Trích xuất tên được đề cập trong câu dài nhất trong câu chuyện.
4. Có bao nhiêu từ trong câu chuyện?
5. Có bao nhiêu câu trong câu chuyện?
Các câu nguồn tham chiếu được xác định tự động và các câu trả lời sự thật chuẩn được chú thích thủ công bởi chúng tôi. Một ví dụ về câu hỏi gốc trong QuALITY như thế này:
Ví dụ 3
Đầu vào: <Một câu chuyện dài> \n\n
Hướng dẫn: Tại sao Syme chấp nhận nhiệm vụ với Tate?
(A) Anh cần một cách trở về Trái đất
(B) Anh cảm thấy mình sẽ thu thập được phần thưởng trên đường đi
(C) Anh tôn trọng Tate
(D) Anh không có kế hoạch cho cuộc đời mình, vì vậy anh nhảy vào cuộc phiêu lưu
Sự thật chuẩn: (B) Anh cảm thấy mình sẽ thu thập được phần thưởng trên đường đi
B.4 COURSERA (BÀI GIẢNG NÂNG CAO)
Tập dữ liệu này có nguồn gốc từ trang web Coursera¹⁸. Chúng tôi đã chọn và hoàn thành 4 khóa học:
1. Ask Questions to Make Data-Driven Decisions,
2. Data Scientist's Toolbox,
3. Process data from dirty to clean,
4. Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization.
Tài liệu đầu vào dài là phụ đề của các video và chúng tôi hợp nhất các khóa học trong một tuần thành một bài giảng dài duy nhất. Các câu hỏi và câu trả lời sự thật chuẩn được gắn nhãn bởi các tác giả. Phong cách hướng dẫn của Coursera có định dạng trắc nghiệm. Để tăng độ khó của nhiệm vụ, chúng tôi đã đặt nhiều lựa chọn đúng. Việc không chọn tất cả các lựa chọn đúng sẽ dẫn đến chỉ nhận được một phần tư tổng điểm cho câu hỏi đó.
Ví dụ 4
Đầu vào: <Một bài giảng dài> \n\n
Câu hỏi: Khi làm việc với một đội mới, hành động nào sau đây có thể giúp bạn thích ứng với các kỳ vọng giao tiếp khác nhau? Chọn tất cả các đáp án áp dụng.
A. Đặt câu hỏi khi bạn không chắc chắn về điều gì đó
B. Học phong cách giao tiếp ưa thích của đội
C. Quan sát cách đồng đội giao tiếp với nhau
D. Bỏ qua sở thích giao tiếp của đội và sử dụng phong cách của riêng bạn
\n\n Câu trả lời:
Sự thật chuẩn: ABC
B.5 SFCITION (KHOA HỌC VIỄN TƯỞNG)
Chúng tôi chú thích nhiệm vụ con này để kiểm tra lòng trung thành của LCLM với bối cảnh đầu vào. LLMs đã có được nhiều hiểu biết thông thường trong corpus tiền huấn luyện của họ được gọi là kiến thức tham số (Wang et al., 2023). Tuy nhiên, chúng tôi lập luận rằng trong LCLMs, kiến thức theo bối cảnh quan trọng hơn kiến thức tham số. Trong các ứng dụng thế giới thực, nhiều tài liệu dài là riêng tư và không bao giờ có thể được nhìn thấy trong quá trình tiền huấn luyện. Nó có thể chứa kiến thức mới hoặc mô tả một thế giới mới có thể ngược lại với kiến thức tiền huấn luyện. Mô hình ngôn ngữ nên tuân theo kiến thức theo bối cảnh thay vì kiến thức tham số. Để mô phỏng kịch bản này, chúng tôi chú thích một tập dữ liệu khoa học viễn tưởng bao gồm các câu hỏi Đúng hoặc Sai. Các tác phẩm gốc có nguồn gốc từ SFGram¹⁹. Chúng tôi chọn thủ công các tài liệu phù hợp với điều kiện thí nghiệm của chúng tôi và chú thích chúng với các câu hỏi và câu trả lời tương ứng. Hầu hết các câu trả lời cho những câu hỏi này mâu thuẫn với các nguyên tắc thế giới thực và không tuân thủ các định luật vật lý thực tế, chẳng hạn như tuyên bố: Con người đã phát minh ra cỗ máy thời gian. Kết quả là, các mô hình mã nguồn mở có vấn đề ảo giác rất nghiêm trọng điều này giúp chúng có được điểm cao trong tập dữ liệu này. Vì vậy chúng tôi cũng đưa ra câu trả lời dựa trên kiến thức thế giới thực, và độ chính xác cuối cùng được tính bằng trung bình của lòng trung thành và tính thực tế.
Ví dụ 5
Đầu vào: <Một tác phẩm khoa học viễn tưởng> \n\n
Câu hỏi: Chúng ta không thể đến được trung tâm Trái đất, Đúng hay Sai? Trả lời câu hỏi này dựa trên thế giới được mô tả trong tài liệu.
Sự thật chuẩn: Sai
Câu hỏi: Chúng ta không thể đến được trung tâm Trái đất, Đúng hay Sai? Trả lời câu hỏi này dựa trên kiến thức thế giới thực và sự thật cho đến lần huấn luyện cuối cùng của bạn.
Sự thật chuẩn: Đúng
B.6 CODEU (PYTHON)
Tập dữ liệu này được sử dụng để kiểm tra khả năng hiểu mã dài. Cho một cơ sở mã dài, chúng tôi sẽ gọi một số hàm được định nghĩa trong cơ sở mã và mô hình nên suy ra đầu ra cuối cùng của chương trình. Chúng tôi chủ yếu sử dụng mã nguồn từ Numpy²⁰. Chúng tôi cũng viết một cơ sở mã xử lý chuỗi chứa hơn 100 hàm nhận một chuỗi làm đầu vào như trích xuất địa chỉ email từ một chuỗi đầu vào. Để ngăn LLMs trả lời câu hỏi dựa trên kiến thức tham số của họ, chúng tôi thay thế tên hàm gốc được định nghĩa trong Numpy bằng Op1, Op2..., OpN. Mô hình Ngôn ngữ (LLM) nên đầu tiên xác định nơi hàm được gọi và xác định hàm nào được gọi, cuối cùng xác định kết quả của các phép toán. CodeU đại diện cho nhiệm vụ thách thức nhất trong L-Eval. Ngay cả mô hình mạnh nhất, GPT-4-32k, cũng chỉ đạt được độ chính xác 25.55%.
Ví dụ 6
Đầu vào: <Phần đầu của một chương trình Python dài>
def Op1(): ...
def Op2(): ...
args = [4,5,6]
output = Op1(args)
print(output)
<Phần còn lại của chương trình> \n\n
Hướng dẫn: Đầu ra của chương trình này là gì? Vui lòng đọc cẩn thận qua các đoạn mã và nhận xét này. Bạn nên đầu tiên xác định nơi các hàm được định nghĩa và sau đó tìm hiểu chúng làm gì.
\n\n hãy suy nghĩ từng bước:
Sự thật chuẩn: [1,2,3]
B.7 TOPICRET (CUỘC HỘI THOẠI DÀI)
Tập dữ liệu này đến từ kho LongChat (Li et al., 2023a)²¹, và phong cách nhiệm vụ của nó tập trung vào việc truy xuất các chủ đề từ lịch sử chat rộng lớn. Các nghiên cứu gần đây cho thấy các mô hình ngôn ngữ giỏi trong việc truy xuất thông tin từ đầu hoặc cuối của bối cảnh đầu vào nhưng thường bị lạc ở giữa (Liu et al., 2023). Để làm cho nhiệm vụ thách thức hơn, chúng tôi tăng cường nhiệm vụ gốc bằng cách yêu cầu mô hình trích xuất chủ đề thứ hai và thứ ba.
Ví dụ 7
Đầu vào: <Một cuộc hội thoại dài> \n\n
Câu hỏi: Chủ đề thứ hai chúng ta thảo luận là gì? Chỉ cho tôi tên chủ đề. Đừng tự tóm tắt.
Sự thật chuẩn: Tương lai của du lịch vũ trụ
B.8 LONGFQA (TÀI CHÍNH)
Chúng tôi thấy rằng thiếu các tập dữ liệu QA mở bối cảnh dài trong tài chính. Tập dữ liệu tài chính bối cảnh dài được lấy từ bản ghi cuộc gọi thu nhập có được từ phần Quan hệ Nhà đầu tư của các trang web công ty. Chúng tôi chú thích 6 bản ghi từ 6 tập đoàn khác nhau, Lumentum Oclaro²², Theragenics²³, FS KKR Capital Corp²⁴, LaSalle Incorporated²⁵, Renewable Energy Group²⁶ với 54 câu hỏi dựa trên những bản ghi này.
Ví dụ 8
Đầu vào: <Một tài liệu dài> \n\n
Hướng dẫn: Bạn được yêu cầu hành động như một thành viên của Cuộc gọi Hội nghị Kết quả Tài chính và trả lời câu hỏi: Những hành động chính nào mà Greg Dougherty, CEO của Oclaro, đã nêu bật như đang được công ty thực hiện cho kế hoạch tái cơ cấu của mình? \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Oclaro đã triển khai một kế hoạch tái cơ cấu đáng kể, bao gồm đóng cửa chính thứ hai của chúng tôi...
B.9 CUAD (LUẬT)
Các câu hỏi về lĩnh vực Pháp lý được rút ra từ tập dữ liệu CUAD (Contract Understanding Atticus Dataset) (Hendrycks et al., 2021b) được thiết kế để hỗ trợ nghiên cứu NLP cho việc tự động hóa đánh giá hợp đồng pháp lý. Chúng tôi lọc thủ công 20 tài liệu với các cặp QA được chú thích từ CUAD.
Ví dụ 9
Đầu vào: <Hợp đồng pháp lý> \n\n
Hướng dẫn: Nêu bật các phần (nếu có) của hợp đồng này liên quan đến "Ngày Hết hạn" nên được luật sư đánh giá. Chi tiết: Thời hạn ban đầu của hợp đồng sẽ hết hạn vào ngày nào? \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Thời hạn của Thỏa thuận này sẽ bắt đầu vào Ngày Có hiệu lực và sẽ tiếp tục có hiệu lực đầy đủ trong thời gian ban đầu là năm (5) năm.
B.10 MULTIDOC2DIAL (ĐỐI THOẠI TRÊN ĐA TÀI LIỆU)
Tập dữ liệu này được lấy mẫu từ tập dữ liệu MultiDoc2Dial (Feng et al., 2021) nhằm mô hình hóa các đối thoại hướng mục tiêu có căn cứ trong nhiều tài liệu. Nó chứa các đối thoại từ 4 lĩnh vực khác nhau: Tài chính, Du lịch, Giải trí và Mua sắm. Mỗi đối thoại trong tập dữ liệu có căn cứ trong 2-5 tài liệu có liên quan bao gồm các chủ đề khác nhau trong lĩnh vực.
Ví dụ 10
Đầu vào: <Nhiều tài liệu dài> \n\n
Hướng dẫn: Các khóa học Giáo dục Lái xe sẽ có hiệu lực trong bao lâu? \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Khoảng 1 năm. Có thể lâu hơn tùy thuộc vào khóa học.
B.11 NATURAL QUESTIONS (WIKIPEDIA)
Chúng tôi lọc 20 tài liệu wikipedia dài từ Natural Question (Kwiatkowski et al., 2019) trên tập dữ liệu Google Research. Các câu hỏi có thể được trả lời với cùng một tài liệu được hợp nhất, và các câu hỏi trùng lặp được loại bỏ.
Ví dụ 11
Đầu vào: <Tài liệu từ Wiki> \n\n
Hướng dẫn: khi nào mùa 2 của handmaid's tale bắt đầu? \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: 25 tháng 4, 2018
B.12 NARRATIVEQA (TƯỜNG THUẬT)
Tập dữ liệu này được thu thập từ NarrativeQA (Ko ˇcisk´y et al., 2017) có độ dài tài liệu dài nhất trong L-Eval. Tập dữ liệu hỏi đáp gốc được tạo ra sử dụng toàn bộ sách từ Project Gutenberg²⁷ và kịch bản phim từ các trang web khác nhau. Tóm tắt của sách và kịch bản được lấy từ Wikipedia và đưa cho các nhà chú thích. Công việc của chúng tôi tập trung vào việc sửa lỗi chú thích ví dụ, có một số vấn đề mà nhân vật chính trong câu hỏi thậm chí không xuất hiện trong tài liệu đầu vào.
Ví dụ 12
Đầu vào: <Một cuốn tiểu thuyết dài> \n\n
Hướng dẫn: Tại sao Mary trả nợ cho gia đình Ann? \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Mary yêu Ann.
B.13 QASPER (BÀI BÁO)
Tập dữ liệu này được lọc từ tập dữ liệu Qasper (Dasigi et al., 2021), là một tài nguyên hỏi đáp tập trung vào các bài báo NLP. Tập dữ liệu được xây dựng sử dụng các bài báo NLP được trích xuất từ Semantic Scholar Open Research Corpus (S2ORC). Sau khi lọc, chúng tôi loại bỏ các câu hỏi không thể trả lời và các câu trả lời phiên bản trích xuất. Chúng tôi cũng phát hiện các trường hợp mà các câu hỏi giống hệt nhau mang lại các câu trả lời mâu thuẫn. Chúng tôi đã giải quyết vấn đề này bằng cách xem xét tỉ mỉ bài báo và sửa chữa các phản hồi không chính xác.
Ví dụ 13
Đầu vào: <Một bài báo dài> \n\n
Hướng dẫn: Họ đã thu thập tập dữ liệu như thế nào? \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: các tài nguyên công khai nơi các tài khoản Twitter đáng ngờ được chú thích, danh sách với 32 tài khoản Twitter khác từ BIBREF19 được coi là đáng tin cậy.
B.14 OPENREVIEW (BÀI BÁO)
Nhiệm vụ này nhằm giúp các nhà nghiên cứu làm việc trên các bài báo khoa học bằng cách xử lý các nhiệm vụ như sửa lỗi ngữ pháp hoặc lỗi đánh máy và viết một số phần. Chúng tôi bao gồm 3 nhiệm vụ trong nhiệm vụ trợ lý viết bài báo của L-Eval: 1) viết phần Tóm tắt, (2) viết phần Công trình Liên quan, và (3) cuối cùng đưa ra đánh giá về bài báo này bao gồm các đề xuất có giá trị và câu hỏi. Đáng chú ý, chúng tôi không khuyến khích các nhà đánh giá sử dụng các mô hình lớn cho đánh giá. Mục tiêu của chúng tôi là hỗ trợ các tác giả cải thiện thêm bài báo của họ. Do đó, chúng tôi yêu cầu mô hình đưa ra một số đề xuất có giá trị và đặt ra một số câu hỏi cho các tác giả. Chúng tôi lọc 20 bài báo với các đánh giá được viết tốt cho L-Eval. Chúng tôi sử dụng các tệp PDF đã xử lý từ Yuan et al. (2021).
²²https://investor.lumentum.com/overview/default.aspx
²³https://www.sec.gov/Archives/edgar/data/
²⁴https://www.fskkradvisor.com/investor-relations/
²⁵https://ir.jll.com/overview/default.aspx
²⁶https://www.regi.com/resources/press-releases
¹⁷https://github.com/FranxYao/chain-of-thought-hub/blob/main/gsm8k/lib_prompt/prompt_hardest.txt
¹⁸https://coursera.org/
¹⁹https://github.com/nschaetti/SFGram-dataset
²⁰https://github.com/numpy/numpy
²¹https://github.com/DachengLi1/LongChat
²⁷https://www.gutenberg.org
24

--- TRANG 20 ---
preprint
Ví dụ 14
Đầu vào: <Một bài báo dài> \n\n
1. Hướng dẫn: Vui lòng tạo phần Tóm tắt cho bài báo này. \n Trả lời câu hỏi này với xx từ.
2. Hướng dẫn: Vui lòng tóm tắt công trình liên quan và bạn nên bao gồm các công trình sau [một danh sách các bài báo]. \n Trả lời câu hỏi này với xx từ.
3. Hướng dẫn: Vui lòng viết một đánh giá cho bài báo này và bạn nên cung cấp một số đề xuất và đặt ra một số câu hỏi trong đánh giá của bạn. \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Các sơ đồ phát hiện ngoài phân phối (OOD) thông thường dựa trên variational autoencoder hoặc Random Network Distillation (RND) đã được quan sát để gán...
B.15 GOVREPORT (BÁO CÁO CHÍNH PHỦ)
Tập dữ liệu này được lọc từ tập dữ liệu tóm tắt báo cáo chính phủ (Huang et al., 2021), tập dữ liệu bao gồm các báo cáo dài được viết bởi các cơ quan nghiên cứu chính phủ Hoa Kỳ như Dịch vụ Nghiên cứu Quốc hội và Văn phòng Trách nhiệm Chính phủ. Các tài liệu và tóm tắt trong tập dữ liệu này dài hơn so với các tập dữ liệu tóm tắt tài liệu dài khác. Chúng tôi lọc thủ công 13 tài liệu với tóm tắt được viết bởi con người từ tập dữ liệu gốc.
Ví dụ 15
Đầu vào: <Một báo cáo chính phủ> \n\n
Hướng dẫn: Vui lòng giúp tôi tóm tắt báo cáo chính phủ này. \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Tổng thống Hoa Kỳ có sẵn một số quyền lực có thể được thực hiện trong trường hợp quốc gia bị đe dọa bởi khủng hoảng, tình huống khẩn cấp, hoặc hoàn cảnh khẩn cấp...
B.16 QMSUM (CUỘC HỌP)
Tập dữ liệu này có nguồn gốc từ QMSum (Zhong et al., 2021), tập dữ liệu này chứa các tóm tắt cuộc họp dựa trên truy vấn. Tóm tắt dựa trên truy vấn nhằm tóm tắt tài liệu với một khía cạnh cụ thể. Chúng tôi chọn 20 bản ghi cuộc họp kèm theo các truy vấn, cụ thể chọn những bản không thể dễ dàng được giải quyết thông qua các phương pháp truy xuất.
Ví dụ 16
Đầu vào: <Bản ghi cuộc họp> \n\n
Hướng dẫn: Điều gì đã được thống nhất về các bản ghi mẫu? \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Để tiết kiệm thời gian, người nói mn005 sẽ chỉ đánh dấu mẫu dữ liệu được phiên âm cho các vùng lời nói chồng chéo, trái ngược với việc đánh dấu tất cả các sự kiện âm thanh...
B.17 SPACE (ĐÁNH GIÁ)
Tóm tắt đánh giá (ý kiến) nhằm tóm tắt các đánh giá từ đánh giá khách hàng về một nhà hàng hoặc khách sạn. Chúng tôi có được 20 mẫu từ tập xác thực và kiểm tra của SPACE (Angelidis et al., 2021) nơi các tóm tắt tóm tắt được viết bởi con người được tạo cho 50 khách sạn dựa trên 100 đánh giá đầu vào mỗi khách sạn. SPACE bao gồm các đánh giá khách hàng về khách sạn từ TripAdvisor, với 1.1 triệu đánh giá huấn luyện cho 11,000 khách sạn. Nhiệm vụ gốc yêu cầu mô hình tóm tắt khách sạn từ nhiều khía cạnh: thức ăn, vị trí, sự sạch sẽ, v.v. Chúng tôi xây dựng các hướng dẫn cho tóm tắt đánh giá với GPT-4 và một số ví dụ.
Ví dụ 17
Đầu vào: <Nhiều đánh giá> \n\n
Hướng dẫn: Đưa ra tóm tắt rộng về ấn tượng của khách về Doubletree by Hilton Seattle Airport. \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Nhân viên thân thiện và đặc biệt. Mọi phòng (bao gồm sảnh) đều rất sạch sẽ. Chúng rộng rãi, rất yên tĩnh, và có máy pha cà phê...
B.18 MULTI-NEWS (TIN TỨC)
Tập dữ liệu này có nguồn gốc từ Multi-News (Fabbri et al., 2019). Tập dữ liệu Multi-News gốc chứa các bài báo tin tức cũng như tóm tắt được viết bởi con người của những bài báo đó, được biên soạn từ trang web newser.com nơi mỗi bài báo bao gồm nhiều bài báo tin tức ngắn. Chúng tôi chọn 10 bài báo cho bộ đánh giá L-Eval.
Ví dụ 18
Đầu vào: <Bài báo tin tức> \n\n
Hướng dẫn: Vui lòng tóm tắt những bài báo tin tức này. \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Tại sao Microsoft mua mảng kinh doanh điện thoại của Nokia? Giờ chúng ta biết câu trả lời của Microsoft: Tập đoàn máy tính phát hành một bài thuyết trình 30 slide hôm nay lập luận rằng động thái này sẽ cải thiện Microsoft...
B.19 BIGPATENT (BẰNG SÁNG CHẾ)
Tập dữ liệu này được lấy từ dự án BigPatent (Sharma et al., 2019), bao gồm 1.3 triệu bản ghi tài liệu bằng sáng chế Hoa Kỳ cùng với tóm tắt tóm tắt được viết bởi con người, chúng tôi chọn 13 bằng sáng chế từ tập dữ liệu gốc.
Ví dụ 19
Đầu vào: <Một bằng sáng chế dài> \n\n
Hướng dẫn: Bạn là một kiểm tra viên bằng sáng chế. Vui lòng viết tóm tắt bằng sáng chế này. \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Phát minh cung cấp một phương pháp và hệ thống để làm sạch chân thú cưng bằng cách cung cấp một container có ranh giới chứa...
B.20 SUMMSCREEN (CHƯƠNG TRÌNH TV)
Tập dữ liệu này có nguồn gốc từ SummScreen (Chen et al., 2022), tập dữ liệu gốc là một tập dữ liệu tóm tắt tóm tắt kết hợp bản ghi series TV và tóm tắt tập phim. SummScreen được xây dựng từ các trang web đóng góp của người hâm mộ. Chúng tôi sử dụng 13 bản ghi này trong L-Eval.
Ví dụ 20
Đầu vào: <Bản ghi series TV> \n\n
Hướng dẫn: Viết tóm tắt về cảnh. \n Trả lời câu hỏi này với xx từ.
Sự thật chuẩn: Cảm thấy tội lỗi vì Phoebe bỏ lỡ London, nhóm bạn lên kế hoạch một chuyến đi cuối tuần đến Atlantic City, nhưng ngay khi họ sắp khởi hành...
26
