# 2310.06839.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2310.06839.pdf
# File size: 2530020 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LongLLMLingua : Accelerating and Enhancing LLMs in Long Context
Scenarios via Prompt Compression
Huiqiang Jiang, Qianhui Wu, Xufang Luo,
Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
Microsoft Corporation
{hjiang,qianhuiwu,xufluo,dongsli,cyl,yuqyang,liliqiu}@microsoft.com
Abstract
In long context scenarios, large language mod-
els (LLMs) face three main challenges: higher
computational cost, performance reduction,
and position bias. Research indicates that LLM
performance hinges on the density and posi-
tion of key information in the input prompt. In-
spired by these findings, we propose LongLLM-
Lingua for prompt compression towards im-
proving LLMs’ perception of the key informa-
tion to simultaneously address the three chal-
lenges. Our extensive evaluation across vari-
ous long context scenarios demonstrates that
LongLLMLingua not only enhances perfor-
mance but also significantly reduces costs and
latency. For instance, in the NaturalQuestions
benchmark, LongLLMLingua boosts perfor-
mance by up to 21.4% with around 4x fewer
tokens in GPT-3.5-Turbo, leading to substantial
cost savings. It achieves a 94.0% cost reduction
in the LooGLE benchmark. Moreover, when
compressing prompts of about 10k tokens at ra-
tios of 2x-6x, LongLLMLingua can accelerate
end-to-end latency by 1.4x-2.6x.1
1 Introduction
Large language models (LLMs) have revolution-
ized user-oriented language technologies and are
serving as crucial components in more and more
applications. Carefully designing prompts is nec-
essary to achieve better performance in specific
downstream tasks. The commonly used technolo-
gies such as In-Context Learning (ICL) (Min et al.,
2022; Dong et al., 2023), Retrieval Augment Gener-
ation (RAG) (Lewis et al., 2020; Asai et al., 2024),
and Multi-turn Agent (Shen et al., 2024; Park et al.,
2023; Wu et al., 2023a) are driving prompts to be
increasingly longer, even reaching thousands of to-
kens. Scenarios such as multi-document question
answering, code completion, and document sum-
marization also necessitate the processing of long
contexts.
1Access our code at https://aka.ms/LongLLMLingua .There are three main challenges when LLMs are
used in long context scenarios: (1) Higher com-
putational costs, encompassing both financial and
latency expenses. (2) Longer prompts introduce
irrelevant and redundant information, which can
weaken LLMs’ performance (Shi et al., 2023), as
illustrated in Figure 1a. (3) LLMs exhibit position
bias (Kamradt, 2023), also known as the "lost in the
middle" issue (Liu et al., 2024), suggesting that the
placement of key information within the prompt
significantly affects LLMs’ performance. This is
demonstrated by the purple curve in Figure 1b.
Inspired by these observations, we propose
LongLLMLingua to address the three challenges.
Specifically, we use LLMLingua (Jiang et al.,
2023a) as the backbone for prompt compression
to address the first challenge, i.e., reduce cost and
latency. However, in the case of long contexts, the
distribution of question-relevant key information
in the prompt is generally dynamic and sparse. Ex-
isting prompt compression methods like LLMLin-
gua (Jiang et al., 2023a) and Selective-Context (Li
et al., 2023c) that often fail to consider question
during compression, resulting in retention of exces-
sive noise and decreased performance. LongLLM-
Lingua aims to improve LLMs’ perception of key
information pertinent to the question, thereby over-
coming the noise and position bias issues in long
contexts, shown in Figure 1b. The underlying prin-
ciple of LongLLMLingua is that small LM are
inherently capable of capturing the distribution of
key information relevant to a given question.
Our main contributions are five-fold: (1) We
propose a question-aware coarse-to-fine compres-
sion method to improve the key information den-
sity in the prompt (Sec. 4.1); (2) We introduce
a document reordering strategy to minimize po-
sition bias in LLMs. (Sec. 4.2); (3) We estab-
lish dynamic compression ratios for precise con-
trol between coarse and fine compression levels
(Sec. 4.3); (4) We propose a post-compressionarXiv:2310.06839v2  [cs.CL]  12 Aug 2024

--- PAGE 2 ---
1 5 10 15 20
Document Number in the Prompt859095100Normalized Performance(%)
Multi-Document QA
Code Completion
Summarization(a) Performance v.s. Document Number
1st 5th 10th 15th 20th
Position of Document with the Answer5560657075Accuracy(%)
 Original
LongLLMLingua 
w/o Reorder (4x)
LongLLMLingua (4x) (b) Performance v.s. Key Information Position
Figure 1: (a) LLMs’ performance in downstream tasks decreases with increased noise in prompts. In this case,
we keep kmost relevant documents/paragraphs based on the ground-truth or LongLLMLingua rk. A larger k
implies more noise introduced into the prompt. To improve the key information density in the prompt, we present
question-aware coarse-to-fine compression. (b) LLMs’ ability to capture the relevant information depends on their
positions in the prompt. To reduce information loss in the middle, we introduce a document reordering mechanism.
subsequence recovery strategy to improve the in-
tegrity of the key information (4.4). (5) We evaluate
LongLLMLingua across five benchmarks, i.e., Nat-
uralQuestions (Liu et al., 2024), LongBench (Bai
et al., 2023), ZeroSCROLLS (Shaham et al., 2023),
MuSicQue (Trivedi et al., 2022), and LooGLE (Li
et al., 2023b), covering a variety of long con-
text scenarios. Experimental results reveal that
LongLLMLingua’s compressed prompts outper-
form original prompts in terms of performance,
cost efficiency, and system latency.
2 Problem Formulation
Following LLMLingua (Jiang et al., 2023a), we
usex= (xins,xdoc
1,···,xdoc
K,xque)to represent
a prompt, including the instruction xins,Kdocu-
ments xdoc
i, and the question xque. However, this
definition can be adjusted for specific scenarios.
The objective of a prompt compression system can
be formulated as:
min
exDϕ(y,ey) +λ∥ex∥0, (1)
whereexrepresents the compressed prompt, a token-
level subsequence of x.yandeyrepresent the
LLM-generated results from xandex, respectively.
Dϕmeasures the distance function, such as KL di-
vergence. λserves as a hyper-parameter balancing
the compression ratio. Additionally, this study ex-
plores a permutation operation space over the K
documents (xdoc
1,···,xdoc
K)for joint optimization.3 Preliminary: LLMLingua
LLMLingua (Jiang et al., 2023a) utilizes a small
language model MSto evaluate the perplexity of
each prompt token, removing those with lower per-
plexities. This method is premised on the idea
that tokens with lower perplexities have a negli-
gible effect on the language model’s overall en-
tropy gain, implying their removal slightly impacts
the LLMs’ contextual understanding. This process
is viewed as an application of "LM is Compres-
sion" (Delétang et al., 2023). LLMLingua include
three key components: budget controller, iterative
token-level prompt compression, and distribution
alignment, highlighted by italic text in Figure 2.
The budget controller assigns varying compres-
sion ratios to different parts of the prompt (i.e.,
instruction, demonstrations, question), implement-
ing coarse-level prompt compression. Subsequent
steps involve dividing intermediate results into seg-
ments and applying token-level compression iter-
atively, where each token’s perplexity based on
preceding compressed segments. To aware differ-
ent target LLMs, LLMLingua fine-tunes MSusing
data from the target LLM.
4 LongLLMLingua
LongLLMLingua builds on LLMLingua to better
compress prompts in long context scenorias. It tack-
les three main issues in handling lengthy contexts,
as introduced in Sec. 1. This approach focuses on
making LLMs more effective at recognizing key

--- PAGE 3 ---
Original Prompt LongLLMLingua
Compressed PromptInstruction: Answer the question based 
on the given passages. Only give me the 
answer and do not output any other 
words. The following are given 
passages.
Document 1: Alberic  III of Dammartin
Alberic  III of Dammartin  (Aubry de 
Dammartin ) (c. 1138 – 19 September 
1200) was a French count and son of 
Alberic  II, Count of Dammartin , and 
Clé mence  de Bar, daughter of Reginald 
I, Count of Bar…
Document 2:
…
Document N: Pope Agapetus II
Pope Agapetus II (died 8 November 955) 
was the bishop of Rome and ruler of the 
Papal States from 10 May 946 to his 
death. A nominee of the princeps of 
Rome, Alberic  II of Spoleto, his 
pontificate occurred during …
Question: Who gave the mother of 
Alberic  II of Spoleto the title " patricia " 
of Rome?
Small 
ModelBlack -box LLMs
~13k tokensAnswer the question based on the given 
passages. …Passage 4:was a Roman 
noblewo  who was the alleged mistress 
of Pope Sergius  III and was given the 
unprecedented titles senatrix  
("senatoress ") and patricia  of Rome by 
Pope John X. , when Ottosys , the of and, 
were to to discusss  Rome and other 
more important, were.us was to the 
dispute the the of Re… Who gave the 
mother of Alberic  II of Spoleto the title 
"patricia " of Rome? ~2k tokens
I Budget Controller
Question -aware Coars e-Grained 
                Compression
w/ document reordering
II  Iterative  Token -level            
Question -aware Fine -Grained 
Compression
w/ dynamic  compression  ratio
0  Distribution 
    Alignment
III Execution with  
Compressed Prompt
Subsequence 
Recovery
ResponseIVFigure 2: Framework of LongLLMLingua . Gray Italic content: As in LLMLingua.
information related to the question in the prompt.
It encompasses three perspectives and further incor-
porates a subsequence recovery strategy, as shown
in Figure 2, to enhance the accuracy and reliability
of the information provided to users. In this section,
we detail how each part of LongLLMLingua works
to improve the LLMs deal with long context.
4.1 How to improve key information density
in the prompt?
Question-Aware Coarse-Grained Compression
In coarse-grained compression, we aim to figure
out a metric rkto evaluate the importance of each
document xdoc
k={xdoc
k,i}Nk
i=1, where Nkis the
number of tokens in xdoc
k. We only keep xdoc
k
with higher rkas the intermediate compressed re-
sults. One approach to improve key information
density in the compressed prompts is to calculate
document-level perplexity conditioned on the ques-
tionp(xdoc
k|xque). However, this method may not
be effective because documents often contain a sig-
nificant amount of irrelevant information. Even
when conditioned on xque, the perplexity scores
computed for entire documents may not be suffi-
ciently distinct, rendering them an inadequate met-
ric for document-level compression.
We propose to use the perplexity of the ques-
tionxqueconditioned on different contexts xdoc
k
p(xque|xdoc
k)to represent the association between
them. We also append a restrictive statement2
xrestrictafterxqueto strengthen the interconnection
2Specifically, " We can get the answer to this question in
the given documents ".ofxqueandxdoc
k. It can be regarded as a regulariza-
tion term that mitigates the impact of hallucinations.
This can be formulated as:
rk=−1
NcNcX
ilogp(xque,restrict
i |xdoc
k),
k∈ {1,2,···, K},(2)
where xque,restrict
i is the i-th token in the concate-
nated sequence of xqueandxrestrictandNcin the
number of tokens.
Figure 3a displays the recall distribution of dif-
ferent retrieval methods, including traditional rele-
vance methos (BM25, Gzip (Jiang et al., 2023b)),
embedding-based methods (OpenAI-embedding,
V oyageai3, BGE-large-en v1.5 (Xiao et al., 2023),
Sentence-BERT (Reimers and Gurevych, 2019),
Jina (Günther et al., 2023)), and reranker methods
(Cohere-Rerank4, BGE-llmembeder, BGE-Ranker-
large), which demonstrates that our coarse-level
compression approach achieves the highest recall
with different numbers of retained documents, sug-
gesting that it preserves the most key information
from the contexts in the compressed results.
Question-Aware Fine-Grained Compression
In fine-grained compression, we assess the impor-
tance of each token in the instruction xins, the ques-
tionxque, andK′documents {xdoc
i}K′
i=1retained af-
ter coarse-grained compression. We incorporate the
3https://www.voyageai.com/
4https://cohere.com/rerank

--- PAGE 4 ---
1 5 10 15 20
Number of Retained Documents020406080100Recall(%)LLMLingua
BM25
OpenAI
Voyageai
BGE-large-en v1.5
SBERT
Gzip
Cohere-Rerank
BGE-llmembeder
Jina
LongLLMLingua rk 
w/o restrict
BGE-Ranker-large
LongLLMLingua rk(a) Recall Distribution
1 5 10 15 20
Document Position in the Prompt0.00.20.40.60.81.0Document Avg. PerplexityPerplexity
Contrastive Perplexity (b) Perplexity Distribution (5th)
Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset, which increases from top to
bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents
traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods.
(b) Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt
QA dataset. The document containing the ground-truth information is located in the 5th position. More results on
position can be found in the Appendix C.1.
iterative compression mechanism following LLM-
Lingua and directly calculate token perplexities
to compress xinsandxque. In this section, we in-
vestigate how to make the fine-grained token-level
compression over {xdoc
k}K′
k=1aware of the question
xque, so that the compressed results could contain
more question-relevant key information.
A straightforward solution for the awareness of
xqueis to simply concatenate it at the beginning
of the whole context. However, this will result in
low perplexities of relevant tokens in the context
following the condition of question xque, further
reducing their differentiation from other tokens.
In this paper, we propose contrastive perplexity ,
i.e., the distribution shift caused by the condition of
the question, to represent the association between
the token and the question. The contrastive perplex-
ity based importance metric sifor each token xiin
{xdoc
k}K′
k=1can be formulated as:
si=perplexity (xi|x<i)−perplexity (xi|xque, x<i).
(3)
Additionally, we provide the derivation of its
mathematical significance in the Appendix A, con-
cluding that it is equivalent to conditional pointwise
mutual information (Church and Hanks, 1989).
Figure 3b illustrates the difference between per-
plexities and contrastive perplexities. The distri-
bution of perplexities appears random, making it
challenging to extract information related to the
question. However, tokens with high contrastive
perplexities tend to cluster near the ground-truthdocument, which contains information relevant to
the question. This suggests that the proposed con-
trastive perplexity can better distinguish tokens
relevant to the question, thus improving the key
information density in the compressed results.
4.2 How to reduce information loss in the
middle?
As demonstrated in Figure 1b, LLM achieves the
highest performance when relevant information oc-
curs at the beginning and significantly degrades if
relevant information is located in the middle of long
contexts. After the coarse-grained compression, we
have obtained a set of documents {xdoc
k}K′
k=1with
their corresponding importance scores {rk}K′
k=1in-
dicating their association with the question xque.
Therefore, we reorder documents using their impor-
tance scores to better leverage LLMs’ information
perception difference in positions:
(xins,xdoc
1,···,xdoc
K′,xque)rk−→
(xins,xdoc
r1,···,xdoc
rK′,xque)(4)
4.3 How to achieve adaptive granular control
during compression?
In fine-grained compression, LLMLingua applies
the same compression ratio over all documents ob-
tained from budget controller. However, the key
information density of different documents is differ-
ent. The more relevant to the question a document

--- PAGE 5 ---
Original PromptDocument [ 1](Title: List of Nobel laureates in Physics) The first 
Nobel Prize in Physics was awarded in 1901 to {Wilhelm Conrad 
Röntgen }{Wilhelm Con rad Rö nt gen}, of Germany, …Document [1](Title: List of Nobelates  in 
Physics) The first Nobel1 
{Wilhelmgen }{Wilhelm gen }, of, who 
received, ….{Wilhelmgen }
{Wilhelm gen }
Compressed  Prompt LLMs’ Respons eFigure 4: The example of Subsequence Recovery, the red text represents the original text, and the blue text is the
result after using the LLaMA 2-7B tokenizer.
is, the more budget ( i.e., lower compression ra-
tio) we should allocate to it. Therefore, we bridge
coarse-grained compression to fine-grained com-
pression and use the importance scores {rk}K′
k=1
obtained from coarse-grained compression to guide
the budget allocation in fine-grained compression.
In this way, we can achieve adaptive granular con-
trol on the whole.
Specifically, we first determine the initial budget
for the retained documents5τdocusing the bud-
get controller of LLMLingua. During fine-grained
compression, we follow the iterative token-level
compression algorithm in LLMLingua but dynam-
ically assign the compression budget τdoc
kto each
document xdoc
kaccording to the ranking index I(rk)
(e.g., 0, 1) of its importance score from the coarse-
grained compression. In this paper, we employ a
linear scheduler for the adaptive allocation. Budget
of each token xican be formulated as:
τi=τdoc
k,∀xi∈xdoc
k,
τdoc
k= max(min((1 −2I(rk)
K′)δτ+τdoc,1),0),
(5)
where iandkis the index of token and document,
K′denotes the number of documents, and δτis
a hyper-parameter that controls the overall budget
for dynamic allocation.
4.4 How to improve the integrity of key
information?
During the generation process, LLMs tend to repli-
cate entities found in the prompt, such as names,
places, and organizations. Compressing these en-
tities at the token level doesn’t affect the LLMs’
understanding of semantic content but can lead to
errors in the generated content.
Therefore, we propose a subsequence recovery
method to restore the original content in LLMs’
responses. This method relies on the subse-
quence relationship among tokens in the original
prompt, compressed prompt, and LLMs’ response,
as shown in Figure 4.
5In LLMLingua, it is τdemsfor demonstrations.The overall procedure includes: i) Iterate
through tokens ylin LLMs’ response and select
the longest substring eykey,l={yl, yl+1, ..., y r}
that appears in the compressed prompt ex. ii)
Find the maximum common shortest subsequence
xi,j={xi, xi+1, ..., x j}in the original prompt x,
corresponding to the representation eykey,lin the
original prompt (accelerated using prefix trees or
sequence automata). iii) Replace the matched to-
kenseykey,lin LLMs’ response with the correspond-
ing subsequence xi,jfrom the original prompt. For
more details, please refer to Algorithm 1.
Algorithm 1 Token-level Subsquence Recovery
Algorithm
Input : The original prompt x; the compressed prompt ex; the
generation response of LLMs y.
1:Set the final response list yrec=ϕ, the left token index of
subsquence lto 0.
2:while l <y.len()do
3: ifSubstring yl∈exthen
4: Find the longer substring eykey,l={yl, yl+1,
..., yr} ∈ex.
5: Find the maximum common shortest subsequence
xi,j={xi, xi+1, ..., x j}in the original prompt x.
6: Add the subsequence xi,j={xi, xi+1, ..., x j}
to the response yrec.
7: Set the left index ltor+ 1.
8: else
9: Add the token ylto the response yrec.
10: Set the left index ltol+ 1.
11: end if
12:end while
Output : The final response list yrec.
5 Experiments
Here, we investigate: (1) How effective is
LongLLMLingua? (2) How efficient is LongLLM-
Lingua?
Implementation details In this paper, we use
GPT-3.5-Turbo-06136and LongChat-13B-16k as
the target LLMs, both accessible via OpenAI7and
HuggingFace8. To ensure stable and reproducible
6For experiments with original prompts exceeding 4k to-
kens, we utilize GPT-3.5-Turbo-16k-0613.
7https://platform.openai.com
8https://huggingface.co/lmsys/longchat-13b-16k

--- PAGE 6 ---
results, we employ greedy decoding and set the
temperature to 0 in all experiments. For the small
language models used for compression, we apply
LLaMA-2-7B-Chat9, which has been aligned by
supervised fine-tuning and RLHF. We implement
our approach with PyTorch 1.13.1 and Hugging-
Face Transformers. We set up hyperparameters
following LLMLingua except for the segment size
used in iterative token-level compression set to 200
here. More details are provided in Appendix B.
Dataset & evaluation metric We use Natu-
ralQuestions for the multi-document QA task, and
use LongBench and ZeroSCROLLS for general
long context scenarios. We also test on multi-
hop QA tasks using MuSiQue dataset (Trivedi
et al., 2022), and long dependency QA tasks us-
ing LooGLE benchmark (Li et al., 2023b). Please
refer to Appendix C for more details on datasets.
Baselines We include two sets of baselines in
following experiments:
(i) Retrieval-based Methods. We assess the
question-document association in the prompt using
five SoTA retrieval methods: BM25, Gzip (Jiang
et al., 2023b), SentenceBERT (Reimers and
Gurevych, 2019), OpenAI Embedding, and the
LongLLMLingua ranker’s important metric rkfor
coarse-grained compression. Notably, embedding
model-based compression mirrors the method in
Xu et al. (2024). We remove low-relevance sen-
tences or paragraphs to meet compression limits,
maintaining the original document sequence.
(ii) Compression-based Methods. We compare
our approach with two state-of-art methods for
prompt compression, i.e., Selective Context (Li
et al., 2023c) and LLMLingua (Jiang et al., 2023a).
Both methods employ LLaMA-2-7B-Chat as the
small language model for compression. In LLM-
Lingua, a coarse-to-fine approach is used to han-
dle constraints of compression ratio: the original
prompt is first compressed to ktimes the constraint
at a coarse level, where kis the granular control co-
efficient; token-level is then performed to reach the
overall constraint. Our method follows the same
coarse-to-fine logic to achieve the constraint.
Main results Table 1 and 2 present the perfor-
mance of various methods under different com-
pression constraints. There are multiple observa-
9https://ai.meta.com/llama/
9https://python.langchain.com/docs/modules/data_connecti
on/document_transformers/post_retrieval/long_context_reordertions and conclusions: (1) Our LongLLMLingua
achieves the best performance across different tasks
and constraints of compression ratios. Compared
to the original prompt, our compressed prompt
can derive higher performance with much lower
cost. For example, LongLLMLingua gains a per-
formance boost of 21.4% on NaturalQuestions with
the ground-truth document at the 10th position,
while the number of tokens input to GPT3.5-Turbo
is∼4x less. (2) Compression-based methods like
Selective Context (Li et al., 2023c) and LLMLin-
gua (Jiang et al., 2023a) perform poorly on most
tasks, especially those with abundant irrelevant
information in the original prompt. This is due
to their pure information entropy based compres-
sion mechanism, which includes too much noise
in the compressed results and even leads to per-
formance worse than the zero-shot setting, e.g.,
on NaturalQuestions. (3) Retrieval-based meth-
ods work well with low compression ratios. How-
ever, their performance declines as the compres-
sion progresses, e.g.,2x→4x; 3000 tokens →
2000 tokens. This may be caused by the decreased
recall. Figure 3a is the illustration of cases on
NaturalQuestions. (4) LongLLMLingua as well
as our coarse-grained compression metric rkonly
is much more robust than all other baselines un-
der different tasks and compression constraints.
With the increase of the compression ratio, e.g.,
2x→4x, LongLLMLingua even achieves a lit-
tle performance gain. We mainly owe this to the
question-aware coarse-to-fine compression, which
can better figure out the key information and reach
a higher key information density with a higher
compression ratio. (5) The proposed reordering
method helps in not only our approach but also
other baselines, well demonstrating its effective-
ness. (6) Compared to the results with a 2,000
tokens constraint, overall performance of 3,000
tokens has improved. LongLLMLingua sees an
increase of 1.2 points in average score and a 1.6x
speedup in end-to-end latency. In this scenario,
the recall rates of retrieval-based methods have in-
creased, leading to a significant improvement in
their accuracy. For example, BM25 achieves an
average score of 48.9.
In addition, we also present experimental results
on datasets such as MuSicQue, LooGLE, ZERO-
SCROLLS, etc., in Appendix C.
Ablation study To evaluate the contributions
of different components in LongLLMLingua, we

--- PAGE 7 ---
MethodsGPT3.5-Turbo LongChat-13b Length Latency
1st 5th 10th 15th 20th Reorder 1st 5th 10th 15th 20th Reorder Tokens 1/τ Latency Speedup
2x constraint
Retrieval-based Methods
BM25 53.7 49.3 47.9 49.9 46.9 50.3 50.9 44.9 44.1 42.9 43.2 46.0 1,545 1.9x 2.1 1.9x
Gzip 64.6 63.8 60.5 58.3 57.3 64.4 61.9 55.7 52.7 50.8 50.9 59.3 1,567 1.9x 2.1 1.9x
SBERT 72.5 67.9 63.3 65.0 66.2 68.7 65.8 57.5 54.9 53.4 55.7 61.4 1,549 1.9x 2.2 1.9x
OpenAI 73.0 65.6 66.5 65.4 65.5 69.9 65.9 57.5 56.2 54.2 55.7 61.7 1,550 1.9x 4.9 0.8x
LongLLMLingua rk73.9 67.7 68.7 66.0 65.6 74.3 68.5 59.1 56.8 55.3 56.9 65.2 1,548 1.9x 2.3 1.8x
Compression-based Methods
Selective-Context 45.4 39.0 33.8 33.5 41.5 - 53.2 26.3 25.4 24.2 33.3 - 1,478 2.0x 7.4 0.6x
LLMLingua 39.7 39.5 40.4 37.1 42.3 41.5 38.7 37.3 35.7 34.1 37.5 37.1 1,410 2.1x 2.8 1.5x
LongLLMLingua 77.2 72.9 70.8 70.5 70.6 76.2 68.7 59.4 57.3 55.9 58.4 66.1 1,429 2.1x 2.9 1.4x
4x constraint
Retrieval-based Methods
BM25 40.6 38.6 38.2 37.4 36.6 36.3 39.5 37.5 36.8 36.4 35.5 37.7 798 3.7x 1.5 2.7x
Gzip 63.1 61.0 59.8 61.1 60.1 62.3 57.6 52.9 51.0 50.1 50.4 57.2 824 3.6x 1.5 2.7x
SBERT 66.9 61.1 59.0 61.2 60.3 64.4 62.6 56.6 55.1 53.9 55.0 59.1 808 3.6x 1.6 2.5x
OpenAI 63.8 64.6 65.4 64.1 63.7 63.7 61.2 56.0 55.1 54.4 55.0 58.8 804 3.7x 4.3 1.0x
LongLLMLingua rk71.1 70.7 69.3 68.7 68.5 71.5 67.8 59.4 57.7 57.7 58.6 64.0 807 3.7x 1.7 2.4x
Compression-based Methods
Selective-Context 31.4 19.5 24.7 24.1 43.8 - 38.2 17.2 15.9 16.0 27.3 - 791 3.7x 6.8 0.6x
LLMLingua 25.5 27.5 23.5 26.5 30.0 27.0 32.1 30.8 29.9 28.9 32.4 30.5 775 3.8x 1.8 2.2x
LongLLMLingua 75.0 71.8 71.2 71.2 74.7 75.5 68.7 60.5 59.3 58.3 61.3 66.7 748 3.9x 2.1 2.0x
Original Prompt 75.7 57.3 54.1 55.4 63.1 - 68.6 57.4 55.3 52.5 55.0 - 2,946 - 4.1 -
Zero-shot 56.1 35.0 15 196x 1.1 3.7x
Table 1: Performance of different methods with different compression ratios (raw size / compressed size = 1/τ) on
NaturalQuestions (20 documents) (Liu et al., 2024). Reorder: we reorder the documents with relevance metrics of
different baselines as our document reordering strategy described in Sec. 4.2. In the case of OpenAI, it corresponds
to LongContextReorder9in the LangChain framework (Chase, 2022). For results reported under 1st to 20th, we do
not use the reordering strategy for all methods.
introduce following variants of it for ablation
study. (1) Variants about Question-aware Coarse-
grained Compression, include: ours w/o Question-
awareness, which calculates question-text rele-
vance rkusing information entropy in LLMLin-
gua, ours w/ SBERT, which employs SBERT to
compute rk, ours w/ p(xdoc
k|xque,restrict
i ), which re-
place p(xque,restrict
i |xdoc
k)withp(xdoc
k|xque,restrict
i )in
Eq. (2), and ours w/o restrict, which only calcu-
lates the conditional probability corresponding to
xque. (2) Ours w/o Question-aware Fine-grained,
which disregards Eq. (3) and only applies Iterative
Token-level Prompt Compression as LLMLingua.
(3) Ours w/o Dynamic Compression Ratio, where
all documents share the same compression ratio
in fine-grained compression. (4) Ours w/o and
(5) LLMLingua w/ Subsequence Recovery, which
either removes or adds the post-processing subse-
quence recovery strategy. (6) Ours w/ GPT2-small,
which uses the GPT2-small model as the MS.
Table 3, 4, and 7 shows the results of the ablation
study in difference tasks. In summary, removing
any component proposed for LongLLMLingua willlead to a performance drop regardless of the posi-
tion of the ground-truth answer. This well validates
the necessity and effectiveness of the proposed
question-aware mechanism during coarse-to-fine
compression, the dynamic compression ratio, and
the subsequence recovery strategy. It also shows
that applying SBERT for coarse-grained compres-
sion will result in inferior performance, which im-
plies the superiority of our question-aware impor-
tance metric in Eq. (2) over SBERT. In addition, re-
placing p(xque,restrict
i |xdoc
k)withp(xdoc
k|xque,restrict
i )
can greatly affect performance due to the large
noise in calculating p(xdoc
k)since the perplexity
of document depends on many other information
besides the question. Removing the restrictive
statement can increase the hallucination of small
language models, leading to a decrease in perfor-
mance. Moreover, our subsequence recovery strat-
egy can also bring performance gains for LLMLin-
gua. However, without our question-aware mech-
anism, results from LLMLingua are still less sat-
isfactory. For more detailed cases, please go to
Appendix E.

--- PAGE 8 ---
Methods SingleDoc MultiDoc Summ. FewShot Synth. Code A VG Tokens 1/τLatency Speedup
3,000 tokens constraint
Retrieval-based Methods
BM25 32.3 34.3 25.3 57.9 45.1 48.9 40.6 3,417 3x 7.5 2.1x
SBERT 35.3 37.4 26.7 63.4 51.0 34.5 41.4 3,399 3x 7.7 2.0x
OpenAI 34.5 38.6 26.8 63.4 49.6 37.6 41.7 3,421 3x 13.3 1.2x
LongLLMLingua rk 37.6 42.9 26.9 68.2 49.9 53.4 46.5 3,424 3x 8.2 1.9x
Compression-based Methods
Selective-Context 23.3 39.2 25.0 23.8 27.5 53.1 32.0 3,328 3x 50.6 0.3x
LLMLingua 31.8 37.5 26.2 67.2 8.3 53.2 37.4 3,421 3x 9.2 1.7x
LongLLMLingua 40.7 46.2 27.2 70.6 53.0 55.2 48.8 3,283 3x 10.0 1.6x
2,000 tokens constraint
Retrieval-based Methods
BM25 30.1 29.4 21.2 19.5 12.4 29.1 23.6 1,985 5x 4.6 3.4x
SBERT 33.8 35.9 25.9 23.5 18.0 17.8 25.8 1,947 5x 4.8 3.4x
OpenAI 34.3 36.3 24.7 32.4 26.3 24.8 29.8 1,991 5x 10.4 1.5x
LongLLMLingua rk 37.8 41.7 26.9 66.3 53.0 52.4 46.3 1,960 5x 4.7 3.3x
Compression-based Methods
Selective-Context 16.2 34.8 24.4 15.7 8.4 49.2 24.8 1,925 5x 47.1 0.3x
LLMLingua 22.4 32.1 24.5 61.2 10.4 56.8 34.6 1,950 5x 5.9 2.6x
LongLLMLingua 39.9 43.2 27.4 69.8 53.0 56.7 48.3 1,822 6x 6.1 2.6x
Original Prompt 39.7 38.7 26.5 67.0 37.8 54.2 44.0 10,295 - 15.6 -
Zero-shot 15.6 31.3 15.6 40.7 1.6 36.2 23.5 214 48x 1.6 9.8x
Table 2: Performance of different methods under different compression ratios on LongBench (Bai et al., 2023) using
GPT-3.5-Turbo in 2,000 tokens constraint.
1st 5th 10th 15th 20th
LongLLMLingua 77.2 72.9 70.8 70.5 70.6
Question-aware Coarse-grained
- w/o Question-awareness 42.1 40.3 39.7 40.1 40.3
- w/ SBERT 73.2 68.5 65.7 66.1 66.7
- w/p(xdoc
k|xque,restrict
i ) 56.0 52.6 53.4 51.6 51.1
- w/o restrict 75.1 72.2 70.3 70.3 70.2
- w/o Question-aware Fine-grained 75.8 71.0 68.9 68.4 69.3
- w/o Dynamic Compression Ratio 74.4 70.7 68.7 67.9 68.1
- w/o Subsequence Recovery 76.7 71.7 69.4 69.3 69.7
- w/ Document Reordering 76.2 76.2 76.2 76.2 76.2
- w/ GPT2-small 74.6 71.7 70.1 69.8 68.5
LLMLingua 39.7 39.5 40.4 37.1 42.3
- w/ Subsequence Recovery 43.8 44.1 43.5 43.3 44.4
Table 3: Ablation study on NaturalQuestions with 2x
constraint using GPT-3.5-Turbo.
Latency evaluation We conducte end-to-end la-
tency testing on a V100-32G, using the prompts
from Multi-document QA, LongBench, and Zero-
SCROLLS in the API call, and results are shown
in Table 1, 2 and 6. The latency includes the time
cost for prompt compression and the request time
for LLMs, with multiple measurements taken and
averaged over. Results demonstrate that LongLLM-
Lingua does indeed speed up the overall inferenceunder different compression ratios and scenarios.
Moreover, with the compression ratio increasing,
the acceleration effect becomes more pronounced
up to 2.6x. However, the OpenAI embedding and
Selective-Context results in longer latency time,
due to repeated API calls and the sequential en-
tropy calculation of semantic units, respectively.
6 Related Works
Long context for LLMs . Recent research
has focused on expanding the window size of
LLMs. Main approaches include: (1) Staged
pre-training (Nijkamp et al., 2023) which grad-
ually increases the context window; (2) Modify-
ing (Press et al., 2022) or interpolating position em-
beddings (Chen et al., 2023; Peng et al., 2024); (3)
Using linear or sparse attention mechanisms (Ding
et al., 2023; Sun et al., 2023); (4) Utilizing exter-
nal memory modules for context storage (Bertsch
et al., 2023; Tworkowski et al., 2023). While these
methods address context window expansion, their
impact on downstream task performance has yet to
be discussed.
Information distribution in prompt . Recent
empirical experiments have shown that LLM per-
formance decreases with less effective information

--- PAGE 9 ---
in a prompt (Bai et al., 2023; Li et al., 2023a; Shi
et al., 2023). Moreover, the position of relevant
information in a prompt has a significant impact on
performance (Wu et al., 2023b). Liu et al. (2024)
suggests that LLMs have more difficulty compre-
hending information located in the middle of a
prompt compared to those at the edges.
Retrieval methods can be categorized as dense
or sparse retrieval methods. Sparse retrieval meth-
ods, like BM25, determine the relevance between
queries and documents based on n-gram informa-
tion. Conversely, dense retrieval methods assess
the relevance between queries and documents in
latent space using embedding model (Reimers and
Gurevych, 2019; Xiao et al., 2023; Günther et al.,
2023) and reranker model (Xiao et al., 2023). Re-
cently, Jiang et al. (2023b) proposed an unsuper-
vised dense retrieval method that leverages tradi-
tional compression algorithms, such as gzip, and
k-nearest neighbors.
Prompt compression methods can be grouped
into three main categories: (1) Token prun-
ing (Goyal et al., 2020; Kim and Cho, 2021; Modar-
ressi et al., 2022) and token merging (Bolya et al.,
2023), which need model fine-tuning or interme-
diate results during inference and have been used
with BERT-scale models. (2) Soft prompt tuning
methods like GIST (Mu et al., 2023), AutoCom-
pressor (Chevalier et al., 2023), and ICAE (Ge
et al., 2024), which require LLMs’ parameter fine-
tuning, making them suitable for specific domains
but not directly applicable to black-box LLMs. (3)
Information-entropy-based approaches such as Se-
lective Context (Li et al., 2023c) and LLMLin-
gua (Jiang et al., 2023a), which use a small lan-
guage model to calculate the self-information or
perplexity of each token in the original prompt and
then remove tokens with lower perplexities.
7 Conclusion
We propose LongLLMLingua to address the three
challenges, i.e., higher computational cost, perfor-
mance reduction, and position bias for LLMs in
long context scenarios. We develop LongLLMLin-
gua from the perspective of efficient prompt com-
pression, thus reducing computational cost. We
further design four components, i.e., a question-
aware coarse-to-fine compression method, a doc-
ument reordering mechanism, dynamic compres-
sion ratios, and a subsequence recovery strategy
to improve LLMs’ perception of the key informa-tion, with which LongLLMLingua demonstrate
superior performance. Experiments on the multi-
document QA, multi-hop QA, and long context
benchmarks demonstrate that LongLLMLingua
compressed prompt can derive higher performance
than original prompts while both API costs for
inference and the end-to-end system latency are
largely reduced.
Limitation
Although previous experiments demonstrate
LongLLMLingua’s effectiveness and efficiency
across a broad range of tasks, the method still has
the following limitations: 1) LongLLMLingua is
a question-aware approach, meaning it requires
re-compression for different questions, even
with the same context, preventing caching of the
context. Moreover, in terms of computational
cost, LongLLMLingua increases the computation
by twice as much as LLMLingua. This can lead
to greater overhead in real-world applications.
However, this issue can be mitigated by extending
the question-aware approach to a task-aware
approach, allowing for reuse and caching. 2)
While the effectiveness of LongLLMLingua has
been tested on a wide range of tasks, especially
on the multi-hop QA dataset MuSicQue (Trivedi
et al., 2022), its effectiveness might be impacted
when the relationship between context and prompt
is more complex and subtle due to the coarse-level
question-aware approach.
References
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2024. Self-RAG: Learning to
retrieve, generate, and critique through self-reflection.
InThe Twelfth International Conference on Learning
Representations .
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
A bilingual, multitask benchmark for long context
understanding. ArXiv preprint , abs/2308.14508.
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R. Gormley. 2023. Unlimiformer: Long-
range transformers with unlimited length input. In
Thirty-seventh Conference on Neural Information
Processing Systems .
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao
Zhang, Christoph Feichtenhofer, and Judy Hoffman.
2023. Token merging: Your vit but faster. In The

--- PAGE 10 ---
Eleventh International Conference on Learning Rep-
resentations .
Harrison Chase. 2022. LangChain.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
ArXiv preprint , abs/2306.15595.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
Danqi Chen. 2023. Adapting language models to
compress contexts. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 3829–3846, Singapore. Associa-
tion for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In 27th Annual Meeting of the Association
for Computational Linguistics , pages 76–83, Van-
couver, British Columbia, Canada. Association for
Computational Linguistics.
Grégoire Delétang, Anian Ruoss, Paul-Ambroise
Duquenne, Elliot Catt, Tim Genewein, Christo-
pher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
Matthew Aitchison, Laurent Orseau, et al. 2023. Lan-
guage modeling is compression. ArXiv preprint ,
abs/2309.10668.
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,
Shaohan Huang, Wenhui Wang, and Furu Wei. 2023.
Longnet: Scaling transformers to 1,000,000,000 to-
kens. ArXiv preprint , abs/2307.02486.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-
ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. 2023. A survey for in-context learning.
ArXiv preprint , abs/2301.00234.
Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen,
and Furu Wei. 2024. In-context autoencoder for con-
text compression in a large language model. In The
Twelfth International Conference on Learning Repre-
sentations .
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh
Raje, Venkatesan T. Chakaravarthy, Yogish Sabhar-
wal, and Ashish Verma. 2020. Power-bert: Accel-
erating BERT inference via progressive word-vector
elimination. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event , volume 119 of Proceedings
of Machine Learning Research , pages 3690–3699.
PMLR.
Michael Günther, Jackmin Ong, Isabelle Mohr, Alaed-
dine Abdessalem, Tanguy Abel, Mohammad Kalim
Akram, Susana Guzman, Georgios Mastrapas, Saba
Sturua, Bo Wang, Maximilian Werk, Nan Wang, and
Han Xiao. 2023. Jina embeddings 2: 8192-token
general-purpose text embeddings for long documents.
ArXiv preprint , abs/2310.19923.Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022. Unsupervised dense informa-
tion retrieval with contrastive learning. Transactions
on Machine Learning Research .
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing
Yang, and Lili Qiu. 2023a. LLMLingua: Compress-
ing prompts for accelerated inference of large lan-
guage models. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 13358–13376. Association for Com-
putational Linguistics.
Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael
Tang, Yiqin Dai, and Jimmy Lin. 2023b. “low-
resource” text classification: A parameter-free clas-
sification method with compressors. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 6810–6828, Toronto, Canada. Associa-
tion for Computational Linguistics.
Greg Kamradt. 2023. Needle In A Haystack - Pressure
Testing LLMs.
Gyuwan Kim and Kyunghyun Cho. 2021. Length-
adaptive transformer: Train once with length drop,
use anytime with search. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 6501–6511, Online. Association
for Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics , 7:452–466.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in Neu-
ral Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-
min Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe
Ma, and Hao Zhang. 2023a. How long can open-
source llms truly promise on context length?
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan
Zhang. 2023b. Loogle: Can long-context language
models understand long contexts? ArXiv preprint ,
abs/2311.04939.
Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin.
2023c. Compressing context to enhance inference

--- PAGE 11 ---
efficiency of large language models. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 6342–6353,
Singapore. Association for Computational Linguis-
tics.
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2024. Lost in the Middle: How Language
Models Use Long Contexts. Transactions of the Asso-
ciation for Computational Linguistics , 12:157–173.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2022. MetaICL: Learning to learn
in context. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2791–2809, Seattle, United States.
Association for Computational Linguistics.
Ali Modarressi, Hosein Mohebbi, and Moham-
mad Taher Pilehvar. 2022. AdapLeR: Speeding up
inference by adaptive length reduction. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 1–15, Dublin, Ireland. Association for
Computational Linguistics.
Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.
Learning to compress prompts with gist tokens. In
Thirty-seventh Conference on Neural Information
Processing Systems .
Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang,
Congying Xia, Chen Xing, Jesse Vig, Semih
Yavuz, Philippe Laban, Ben Krause, Senthil Purush-
walkam, Tong Niu, Wojciech Kry ´sci´nski, Lidiya Mu-
rakhovs’ka, Prafulla Kumar Choubey, Alex Fabbri,
Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-
Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq
Joty, and Caiming Xiong. 2023. Xgen-7b technical
report. ArXiv preprint , abs/2309.03450.
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S. Bern-
stein. 2023. Generative agents: Interactive simulacra
of human behavior. In Proceedings of the 36th An-
nual ACM Symposium on User Interface Software
and Technology , UIST ’23, New York, NY , USA.
Association for Computing Machinery.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico
Shippole. 2024. YaRN: Efficient context window ex-
tension of large language models. In The Twelfth
International Conference on Learning Representa-
tions .
Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant,
and Omer Levy. 2023. ZeroSCROLLS: A zero-shot
benchmark for long text understanding. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 7977–7989, Singapore.
Association for Computational Linguistics.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2024. Hugging-
gpt: Solving ai tasks with chatgpt and its friends
in hugging face. Advances in Neural Information
Processing Systems , 36.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H Chi, Nathanael Schärli,
and Denny Zhou. 2023. Large language models can
be easily distracted by irrelevant context. In Inter-
national Conference on Machine Learning , pages
31210–31227. PMLR.
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma,
Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu
Wei. 2023. Retentive network: A successor to trans-
former for large language models. ArXiv preprint ,
abs/2307.08621.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. MuSiQue: Multi-
hop questions via single-hop question composition.
Transactions of the Association for Computational
Linguistics , 10:539–554.
Szymon Tworkowski, Konrad Staniszewski, Mikołaj
Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr
Miło ´s. 2023. Focused transformer: Contrastive train-
ing for context scaling. In Thirty-seventh Conference
on Neural Information Processing Systems .
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,
Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadal-
lah, Ryen W White, Doug Burger, and Chi Wang.
2023a. Autogen: Enabling next-gen llm applica-
tions via multi-agent conversation framework. ArXiv
preprint , abs/2308.08155.
Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-
peng Kong. 2023b. Self-adaptive in-context learn-
ing: An information compression perspective for in-
context example selection and ordering. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1423–1436, Toronto, Canada. Association for
Computational Linguistics.
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas
Muennighoff. 2023. C-pack: Packaged resources to
advance general chinese embedding. ArXiv preprint ,
abs/2309.07597.

--- PAGE 12 ---
Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee,
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina
Bakhturina, Mohammad Shoeybi, and Bryan Catan-
zaro. 2024. Retrieval meets long context large lan-
guage models. In The Twelfth International Confer-
ence on Learning Representations .
A Derivation Of Question-Aware
Fine-Grained Compression
Based on the definition of Eq. (3), we can derive
that,
si=perplexity (xi|x<i)−perplexity (xi|xque, x<i)
=q(xi) logp(xi|xque, x<i)−q(xi) logp(xi|x<i)
=q(xi) logp(xi|xque, x<i)
p(xi|x<i)
(6)
In the actual calculation of perplexity, a log opera-
tion is performed to avoid overflow, and q(xi)rep-
resents the probability distribution of the ground-
truth.
At the same time, we can derive the following
expanded expression based on Bayes’ theorem.
p(xque|xi, x<i) =p(xi|xque, x<i)p(xque)
p(xi|x<i)
=p(xque)p(xi|xque, x<i)
p(xi|x<i)(7)
The probability distribution p(xque)of the ques-
tion and the ground-truth distribution q(xi)ofxi
are constants, hence sican be considered as the
representation of Eq. (7).
si∝p(xque|xi, x<i) (8)
So we can utilize Eq. (3) to represent the proba-
bility distribution p(xque|xi, x<i), which represents
the condition likelihood of generating xquegiven
the token xi. Therefore, we can represent the token-
level sensitive distribution for the question xque
using just a single inference. For tokens that are un-
related to xque, such as the tokens on the right side
of Figure 3b, their original amount of information
may be high, but the contrastive perplexity remains
at a relatively low level. Finally, we observe that
the form of contrastive perplexity is equivalent to
conditional pointwise mutual information (Church
and Hanks, 1989).
B Experiment Details
B.1 Dataset Details
We use NaturalQuestions (Liu et al., 2024) for the
multi-document QA task, MuSicQue (Trivedi et al.,2022) for the multi-hop QA task, and use Long-
Bench (Bai et al., 2023), ZeroSCROLLS (Shaham
et al., 2023), LooGLE (Li et al., 2023b) for general
long context scenarios. The specific details of the
dataset are as follows:
NaturalQuestions multi-document QA A
multi-document question-answering dataset,
comprising 2,655 problems, was built by (Liu
et al., 2024) based on the NaturalQuestions
dataset (Kwiatkowski et al., 2019). This dataset
provides a realistic retrieval-augmented generation
setup that closely resembles commercial search
and question-answering applications (e.g., Bing
Chat). Each example in the dataset contains
a question and k related documents, utilizing
the Contriever retrieval system (Izacard et al.,
2022), one of which includes a document with
the correct answer. To perform this task, the
model must access the document containing the
answer within its input context and use it to answer
the question. The dataset’s data is sourced from
the NaturalQuestions dataset, which contains
historical queries issued to the Google search
engine and human-annotated answers extracted
from Wikipedia. The average prompt token length
in this benchmark is 2,946. For our experiments,
we used the version provided by (Liu et al.,
2024) that includes 20 documents10. The dataset
comprises five different ground truth document
position settings in the prompt: 1st, 5th, 10th, 15th,
and 20th.
LongBench A multi-task long context bench-
mark consists of 3,750 problems in English and in-
cludes six categories with a total of 16 tasks. These
tasks encompass key long-text application scenar-
ios, such as single-document QA, multi-document
QA, summarization, few-shot learning, synthetic
tasks, and code completion. The average prompt
token length in this benchmark is 10,289. For our
experiments, we used the English dataset and eval-
uation scripts provided by (Bai et al., 2023) for this
benchmark11.
ZeroSCROLLS The multi-task long context
benchmark consists of 4,378 problems, including
four categories with a total of 10 tasks. These tasks
cover summarization, question answering, aggre-
gated sentiment classification, and information re-
ordering. The average prompt token length in this
10https://github.com/nelson-liu/lost-in-the-middle
11https://github.com/THUDM/LongBench

--- PAGE 13 ---
benchmark is 9,788. For our experiments, we used
the validation set and evaluation scripts provided
by (Shaham et al., 2023) for this dataset12.
MuSiQue The multi-hop question-answer
dataset is composed of 39,876, 4,834, and 4,918
problems in the training, validation, and testing
datasets, respectively. This dataset requires the
language model to conduct multiple inferences
based on the content of several documents and
provide corresponding answers, thereby necessi-
tating a certain capability for global information
processing. The average token length for prompts
in this dataset is 2,477. For our experiments, we
utilized the validation set and evaluation scripts
provided by (Trivedi et al., 2022) for this dataset13.
LooGLE The multi-task long context benchmark
comprises 6,448 problems, divided into three cat-
egories: summarization, short dependency ques-
tion answering, and long dependency question an-
swering. The average prompt token length in this
benchmark stands at 24,005. For our experiments,
we focused on the long dependency question an-
swering subset, which includes four types of tasks:
information retrieval, timeline reordering, compu-
tation, and comprehension. This subset contains
1,101 problems. We utilized the evaluation scripts
provided by (Li et al., 2023b) for this dataset14.
B.2 Other Implementation Details
All experiments were conducted using a Tesla
V100 (32GB). We use tiktoken15and GPT-3.5-
Turbo model to count all the tokens. We set the
granular control coefficient kto2. We use the
pre-defined compression rates τins= 0.85and
τque= 0.9for instructions and questions. The
segment size used in the iterative token-level com-
pression is set to 200. The δτused in dynamic
compression ratio is set to 0.3. For a fair compari-
son, we only used reordering in the NaturalQues-
tions Multi-document QA and noted this in Table 1.
We use “ We can get the answer to this question in
the given documents. " as the guideline sentence in
Eq. (3).
For the baselines experiment, we use the cur-
rently recommended strongest model, all-mpnet-
base-v216, as the dense representation model for
12https://www.zero.scrolls-benchmark.com/
13https://github.com/stonybrooknlp/musique
14https://github.com/bigai-nlco/LooGLE
15https://github.com/openai/tiktoken
16https://www.sbert.net/docs/pretrained_models.htmlSentenceBERT. We use the recommended “text-
embedding-ada-002" as the embedding model for
OpenAI Embedding17. We use the GPT2-dolly18
as the small language model in w/ GPT2-small
ablation experiments.
C Additional Experimental Results
C.1 Empirical Study of Question-aware
Fine-grained Compression
Figure 5 shows the distribution of the document’s
average perplexity when the ground-truth is located
at more positions within the prompt. As can be
observed, as the context length increases, the orig-
inal perplexity curve remains relatively stable. In
unrelated documents, a higher perplexity is still re-
tained, making it easier to remove relevant tokens
from the related documents in the prompt compres-
sion process, thereby damaging the corresponding
semantic information. Contrarily, contrastive per-
plexity shows an increase in perplexity in docu-
ments related to the question. According to the
theoretical derivation in Appendix A, it’s known
that contrastive perplexity characterizes the condi-
tional probability of tokens corresponding to the
question. The higher the relevance, the higher the
contrastive perplexity, thereby retaining key infor-
mation in the prompt compression process.
C.2 Ablation in LongBench
Table 4 presents the results from the ablation experi-
ment in the LongBench long context benchmark. It
can be observed that in various long context tasks:
1) Removing the question-aware coarse-grained,
question-aware fine-grained, dynamic compression
ratio, document reordering, and subsequence re-
covery proposed by LongLLMLingua all result in
different degrees of performance drop. 2) Among
these, question-aware coarse-grained is particularly
important for document-based QA and synthetic
tasks, with the maximum drop being 35.8 points;
its impact on summarization and code tasks is rel-
atively smaller. 3) The design of the conditional
probability in the question-aware coarse-grained
module improves the results in all tasks, includ-
ing code completion, single-document question-
answer, and synthetic tasks. Changing the order
of conditional probabilities or removing the re-
strict prompt both lead to varying degrees of perfor-
mance decline. 4) Removing question-aware fine-
17https://platform.openai.com/docs/guides/embeddings/
18https://huggingface.co/lgaalves/gpt2-dolly

--- PAGE 14 ---
1 5 10 15 20
Document Position in the Prompt0.00.20.40.60.81.0 Document Avg. PerplexityPerplexity
Contrastive Perplexity(a) 1st
1 5 10 15 20
Document Position in the Prompt0.00.20.40.60.81.0 Document Avg. PerplexityPerplexity
Contrastive Perplexity (b) 10th
1 5 10 15 20
Document Position in the Prompt0.00.20.40.60.81.0 Document Avg. PerplexityPerplexity
Contrastive Perplexity (c) 15th
Figure 5: The distribution of document-level average perplexity when the ground-truth document is in different
positions.
Methods SingleDoc MultiDoc Summ. FewShot Synth. Code A VG Tokens 1/τ
LongLLMLingua 39.9 43.2 27.4 69.8 53.0 56.7 48.3 1,822 6x
Question-aware Coarse-grained
- w/o Question-awareness 27.1 38.7 25.4 62.0 18.0 53.3 37.4 1,945 5x
- w/ SBERT 34.0 38.7 24.1 57.9 32.5 31.1 36.4 1,790 6x
- w/p(xdoc
k|xque,restrict
i ) 22.5 28.9 23.2 53.0 22.5 33.3 30.6 1,794 6x
- w/o restrict 37.8 39.5 26.4 64.8 52.5 55.8 46.1 1,834 6x
- w/o Question-aware Fine-grained 35.7 41.1 26.4 62.9 44.5 54.8 44.2 1,807 6x
- w/o Dynamic Compression Ratio 36.1 40.6 26.9 67.2 48.0 55.8 45.7 1,851 6x
- w/o Subsequence Recovery 38.6 41.8 27.3 69.0 53.8 56.6 47.8 1,809 6x
- w/o Document Reordering 39.0 42.2 27.4 69.3 53.8 56.6 48.0 1,809 6x
- w/ GPT2-small 35.9 39.4 25.0 60.6 42.0 55.4 43.0 1,892 5x
Table 4: Ablation on LongBench (Bai et al., 2023) using GPT-3.5-Turbo in 2,000 tokens constraint.
grained, dynamic compression ratio has a more
significant impact on document-based QA and syn-
thetic tasks. 5) The subsequence recovery module
can enhance reference-based tasks, but its improve-
ment on tasks like summarization, code, synthetic,
etc., is relatively smaller. 6) Document reordering
is effective for all types of tasks. Reordering at
the document level does not affect LLMs’ under-
standing of context information, even for timeline-
related tasks (see timeline reorder in LooGLE, Ta-
ble 8). On the contrary, reordering can effectively
alleviate the "lost in the middle" issue, thereby im-
proving LLMs performance. 7) Using GPT2-small
reduces the capture of effective tokens, but it can
still achieve results close to or even slightly better
than the original prompt.
C.3 LongBench Using LongChat-13b-16k
Table 5 presents the experiment results in the Long-
Bench long context benchmark using LongChat-
13b-16k. It can be seen that the compressed prompt
can also achieve good results on other LLMs, such
as LongChat-13b-16k. Specifically, 1) there is a
maximum improvement of 15.5 points in synthetic
tasks. Except for a slight drop in few-shot Learn-
ing, there is an improvement of 3-5 points in othertasks. 2) The performance trends of retrieval-based
and compressed-based baselines are similar to the
results in GPT-3.5-Turbo.
C.4 ZeroSCROLLS
Table 6 presents a detailed performance breakdown
on the ZeroSCROLLS benchmark. It can be ob-
served that in the four summarization tasks - GvRp,
SSFD, QMsm, SQAL, LongLLMLingua closely
matches or slightly surpasses the original results
under two compression constraints. Meanwhile, in
the four long context QA tasks - Qsqr, Nrtv, QALT,
MuSQ, there is a significant improvement. No-
tably, in the MuSiQue task, which is based on a
question-answering dataset from books and movie
scripts, there is a 2.1 point increase even under
a 2,000 tokens constraint. It’s worth mentioning
that MuSiQue is a multi-hop question-answering
dataset that requires LLMs to utilize global infor-
mation for long dependency QA. LongLLMLingua
can also improve by 3.5 points under a 6x com-
pression ratio. In the two ordering tasks, SpDg
and BkSS, LongLLMLingua can better retain glob-
ally sensitive information, resulting in a 3.0 point
improvement in BkSS after prompt compression.
It’s important to note that although the Zero-

--- PAGE 15 ---
Methods SingleDoc MultiDoc Summ. FewShot Synth. Code A VG Tokens 1/τ
Original Prompt 27.4 30.3 20.3 49.9 12.5 42.5 30.5 10,295 -
Retrieval-based Methods
BM25 2.4 2.6 16.4 8.7 0.0 44.7 12.5 1,985 5x
SBERT 11.6 13.7 21.1 16.2 7.5 30.0 16.7 1,947 5x
LongLLMLingua rk 30.3 32.4 24.5 41.0 27.5 38.1 32.3 1,960 5x
Compression-based Methods
Selective-Context 16.1 23.5 21.8 21.4 2.5 35.9 20.2 1,925 5x
LLMLingua 20.6 22.3 22.4 35.6 0.0 35.4 22.7 1,950 5x
LongLLMLingua 31.3 34.6 24.6 46.1 27.8 48.8 35.5 1,822 6x
Table 5: Performance of different methods under different compression ratios on LongBench (Bai et al., 2023) using
LongChat-13b in 2,000 tokens constraint.
Methods GvRp SSFD QMsm SQAL QALT Nrtv Qspr MuSQ SpDg BkSS A VG Tokens 1/τ Latency Speedup
3,000 tokens constraint
Retrieval-based Methods
BM25 9.7 3.4 11.7 14.3 57.1 5.9 25.7 11.2 29.6 29.6 19.8 3,379 3x 5.5 2.2x
SBERT 16.5 9.8 12.3 15.2 60.0 14.6 23.4 12.1 39.4 36.4 24.0 3,340 3x 5.9 2.1x
OpenAI 14.3 8.3 12.0 15.3 66.7 13.3 24.3 11.7 31.2 26.4 22.4 3,362 3x 11.7 1.0x
LongLLMLingua rk19.5 11.6 14.7 15.5 66.7 20.5 27.6 13.0 60.8 43.4 29.3 3,350 3x 6.2 2.0x
Compression-based Methods
Selective-Context 20.8 9.1 11.7 13.4 50.0 9.8 26.1 11.0 46.0 9.5 20.7 3,460 3x 54.2 0.2x
LLMLingua 18.7 10.0 14.9 16.8 61.9 26.9 27.2 23.4 62.9 44.5 30.7 3,366 3x 7.4 1.7x
LongLLMLingua 22.1 12.8 15.9 17.1 67.0 27.8 31.3 23.9 65.8 46.5 33.0 3,431 3x 8.2 1.5x
2,000 tokens constraint
Retrieval-based Methods
BM25 8.8 2.5 11.1 13.5 60.0 7.0 4.9 20.3 39.9 32.9 20.1 1,799 5x 3.8 3.2x
SBERT 10.2 7.9 13.7 13.2 60.0 8.1 10.8 1.7 37.2 42.8 20.5 1,773 6x 4.1 3.0x
OpenAI 11.1 8.0 11.8 13.6 60.0 7.1 13.2 4.0 33.6 43.6 20.6 1,784 5x 9.9 1.2x
LongLLMLingua rk18.2 9.8 12.3 15.9 57.1 10.1 17.8 7.3 57.7 42.3 24.9 1,771 6x 4.7 2.6x
Compression-based Methods
Selective-Context 19.0 8.4 9.7 12.4 47.0 12.5 21.6 11.5 41.2 11.0 19.4 1,865 5x 47.5 0.3x
LLMLingua 19.4 11.9 13.1 16.0 62.1 23.7 24.0 22.4 33.9 44.9 27.2 1,862 5x 4.8 0.3x
LongLLMLingua 20.1 12.4 14.9 16.5 65.1 27.7 30.7 23.6 68.5 47.2 32.7 1,826 6x 5.2 2.3x
Original Prompt 21.8 12.1 17.9 17.4 66.7 25.3 29.8 20.0 69.7 44.1 32.5 9,788 - 12.2 -
Zero-shot 9.4 3.0 8.6 11.4 42.9 10.6 12.4 5.5 4.2 0.0 12.8 32 306x 1.0 12.2x
Table 6: Performance breakdown of different methods under different compression ratios on ZeroSCROLLS (Sha-
ham et al., 2023) using GPT-3.5-Turbo.
Scrolls validation dataset is relatively small, it still
demonstrates conclusions similar to previous ex-
perimental observations across various methods
and tasks. Furthermore, this study conducted an in-
depth analysis of the multi-hop QA task - MuSiQue,
and another long context benchmark - LooGLE.
The results can be found in Appendix C.5 and Ap-
pendix C.6.
C.5 MuSiQue
Table 7 presents the results from the MuSiQue
multi-hop question-answer dataset. From the table,
it can be observed that in the multi-hop QA task,
requiring global information: 1) LongLLMLingua
can reduce noise in the prompt by eliminating irrel-
evant information and putting more related informa-tion at the beginning or end of the prompt, thereby
improving performance by 5.4 points. 2) The per-
formance drop is more pronounced for retrieval-
based methods, particularly for n-gram-based meth-
ods like BM25. Due to long dependencies, direct
matching information is lost, resulting in less rel-
evant information being recalled. 3) The perfor-
mance of compression-based methods is slightly
different. Selective-Context does not distinguish
between different modules’ sensitivity, resulting in
a loss of question and instruction-related informa-
tion, thereby leading to poorer performance. How-
ever, LLMLingua can still retain relevant key in-
formation at around a 2x compression ratio. 4)
The ablation experiments show that every module
designed in LongLLMLingua plays a role in the

--- PAGE 16 ---
Methods F1 Tokens 1/τ
Original Prompt 45.8 2,427 -
BM25 28.5 1,295 1.9x
SBERT 36.2 1,288 1.9x
LongLLMLingua rk 46.3 1,295 1.9x
Selective-Context 19.6 1,141 2.1x
LLMLingua 40.1 1,110 2.2x
LongLLMLingua 51.2 1,077 2.3x
Question-aware Coarse-grained
- w/o Question-awareness 43.2 1,076 2.3x
- w/ SBERT 47.3 1,070 2.3x
- w/p(xdoc
k|xque,restrict
i ) 44.0 1,066 2.3x
- w/o restrict 49.2 1,078 2.3x
- w/o Question-aware Fine-grained 48.4 1,118 2.2x
- w/o Dynamic Compression Ratio 48.2 1,090 2.2x
- w/o Subsequence Recovery 50.7 1,077 2.3x
- w/o Document Reordering 49.2 1,077 2.3x
- w/ GPT2-small 48.4 1,095 2.2x
Table 7: Performance of different methods and ablation
study on MuSicQue (Trivedi et al., 2022) with 2x con-
straint using GPT-3.5-Turbo.
multi-hop task. The removal of the question-aware
coarse-grained and w/ p(xdoc
k|xque,restrict
i )modules,
which have difficulty in perceiving the importance
distribution of corresponding questions, can cause
a drop of up to 8 points. Removing the restrict
prompt in the question-aware coarse module can
also cause a 2-point drop due to the hallucina-
tion issue of small LLM. In addition, removing
question-aware fine-grained, dynamic compression
ratio, and document reordering can all cause a drop
of 0.5-2.8 points. 5) Moreover, if the small lan-
guage model in LongLLMLingua is replaced with
GPT2-small, it can further improve the accelera-
tion ratio and still achieve a result that is 2.6 points
better than the original prompt.
C.6 LooGLE
Table 8 presents the experiment results in the
LooGLE long dependency benchmark, which fea-
tures longer prompts ( ∼30k) and more global de-
pendencies. From the table, we can observe that:
1) LongLLMLingua can effectively improve the
performance of long context tasks by compress-
ing prompts, even for long dependency tasks. The
results show that LongLLMLingua significantly im-
proves performance in tasks such as retrieval, time-
line reorder, and computation, with the maximum
improvement reaching 15.9 points. 2) The docu-
ment reorder in LongLLMLingua is effective in all
types of tasks, even in tasks highly related to thetimeline, it can effectively improve performance
by alleviating the "lost in the middle" issue. 3)
Retrieval-based methods tend to lose performance
in tasks that have longer dependencies, such as
computation and reasoning. 4) For compression-
based methods, due to the difficulty in perceiving
question information, there tends to be a larger
performance loss in retrieval tasks within long con-
texts.
D Economic Cost
Table 9 presents the estimated per 1,000 samples
inference costs for various datasets, encompassing
input prompts and generated output text, based on
GPT-3.5-Turbo pricing19. Our approach demon-
strates substantial savings in computational re-
sources and monetary expenses, particularly in long
context situations. Cost reductions of $3.3 (71.7%),
$28.5 (90.5%), $27.4 (89.5%), $2.0 (52.6%), and
$88.0 (94.0%) per 1,000 samples are observed
for Multi-document QA, LongBench, ZeroScrolls,
MuSiQue, and LooGLE, respectively.
E Ablation Analysis
Figure 6 illustrates the compressed prompts from
the Multi-document QA dataset, comparing the use
of contrastive perplexity at a high compression ra-
tio (30x). It shows that without question-aware
token-level prompt compression, LongLLMLin-
gua tends to compress key information, a tendency
that becomes more pronounced at higher compres-
sion ratios. Conversely, employing contrastive per-
plexity allows for better detection of key informa-
tion related to the question within the context, thus
preserving key information within the compressed
prompt.
F Cases Study
Figures 7, 8, and 9 display the outcomes before
and after compression, as well as the LLMs’ re-
sponses in various scenarios.
19https://openai.com/pricing

--- PAGE 17 ---
Methods Retrieval Timeline Reorder Computation Reasoning A VG Tokens 1/τ
Retrieval-based Methods
BM25 20.4 21.7 8.2 26.3 19.2 3,185 10x
SBERT 28.9 21.1 10.7 27.2 22.0 3,169 10x
LongLLMLingua rk 38.6 32.2 16.2 26.3 28.3 3,158 10x
Compression-based Methods
Selective-Context 16.7 5.0 2.3 17.6 10.4 3,710 8x
LLMLingua 10.0 25.0 13.3 21.1 17.3 3,404 9x
LongLLMLingua 40.0 35.0 19.7 33.6 32.1 3,121 10x
LongLLMLingua w/o Reorder 39.3 33.8 18.7 31.6 30.9 3,119 10x
Original Prompt 24.1 20.9 13.5 32.1 22.6 30,546 -
Zero-shot 8.7 6.3 1.2 14.5 7.7 43 710x
Table 8: Performance of different methods on LooGLE (Li et al., 2023b) long dependency QA.
Multi-document QA LongBench ZeroScolls MuSicQue LooGLE
Original 4.6 31.5 30.6 3.8 93.6
Ours 1.3 ( ↓71.7%) 3.0 ( ↓90.5%) 3.2 ( ↓89.5%) 1.8 ( ↓52.6%) 5.6 ( ↓94.0%)
Table 9: The inference costs $ (per 1,000 samples) for various datasets using GPT-3.5-Turbo.
Ours w/o Token-level Question-aware:
Compressed Prompt:
Write a high-quality answer for the given question using only the provided search results (some of
which might be irrelevant).
Document [1](: Physics)gen„ who received2K, which is ,73,0 in0. Johnen only to twice6. Mariaie
won, for.g was, until1estate he. Two:Mayer (1963). As of 2017, the prize has been awarded
Question: who got the first nobel prize in physics
Answer:
LLMs’ Response:
No answer found in the given search results.
Ours w/ Token-level Question-aware:
Compressed Prompt:
Write a high-quality answer for the given question using only the provided search results (some of
which might be irrelevant).
1Title: List of Nobelates in The first Nobel Prize was1 to Wilhelmrad , of who received 1582
which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate he women
prize:ertMayer (1963). As of 2017, the prize has been awarded
Question: who got the first nobel prize in physics
Answer:
LLMs’ Response:
Wilhelmrad
LLMs’ Response after Subsquence Recovery:
Wilhelm Conrad Röntgen
Ground Truth:
Wilhelm Conrad Röntgen
Figure 6: Comparing the compressed prompt and LLMs’ response before and after using Question-aware Fine-
grained Compression and Subsequence Recovery( 1/τ= 30x, high compression ratio setting) from NaturalQuestions
Multi-document QA (Liu et al., 2024) using GPT-3.5-Turbo.

--- PAGE 18 ---
Original Prompt:
...
Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018, that Dancing on Ice
had been recommissioned for an eleventh series to air in 2019 .
...
Compressed Prompt:
Write a high-quality answer for the given question using only the provided search results (some of
which might be irrelevant).
1Title: Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
for an eleventh series air in 209.
Document [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show consists of
celebrit and professional partners figure skating in front of a panel of judges The, broadcast on
ITV , started on January 2006 and ended on 9 March 2014 after show ´contract not renewed by ITV
On 4 September 2017, it was announced that rev series would on I 7 January 201 Sch and Willby
returning as a
5(: on ( on () The third series of a from January to168TV . The from Saturdays, with Holby present
Kar,y Sliner Robin Cins returned to Panel", with Ruth H joining the panel as replacement for
Natalia Bestova. The commission of the was confirmed by at the07 announcedova depart the series
Robinen Bar,ater and Jasoniner announced
7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip Sch Holly
Willough, and judged the "I P consisting Nicky Slater, Nataliaian Karenres Jason Gardiner Karen
Barber and Robin Cousins Jaynevill and Christopher Dean co and trained the contestants In this
series, cele to ten in first series. The series was won former Kyran Bracken, with Mel Lambert the
winner. It announced thatenresge
Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version " анду ) being on
channel0, and renamed in8 to " Ice" (). Its counterpart called "Ice Age (, "Stars on Ice on Channel
Oneak IceHviezdyl’J. The Turkish version" is called Dans" ("ance on
Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
](: on Ice world) Dan Ice is a made competition world format, and been subsequently Italy Chile
where titled after series There have a, the show was broadcast on Channel 13 as a
Document [17](Title: Dancing on Ice) the insight to the training of the celebrities over the last
week. It was presented by television presenter Ben Shephard and former contestant and "Loose
Women" star Coleen Nolan. The show was broadcast from 8 pm to 8.30 pm on Friday evenings on
ITV throughout the duration of the main shows season. STV who broadcast the main show did
not broadcast this on the Friday evening but after repeating the previous week ´s main show on the
following Saturday afternoon. Due to poor ratings, "Dancing on Ice Friday" was axed prior to the
2011 series. The show was based in the
Question: when is dancing on ice on the tv
Answer:
LLMs’ Response:
209
LLMs’ Response after Subsquence Recovery:
2019
Ground Truth:
2019
Figure 7: Cases study on NaturalQuestions Multi-document QA dataset (Liu et al., 2024) in 4x constraint using
GPT-3.5-Turbo.

--- PAGE 19 ---
Compressed Prompt:
Please complete the code given below.
public class MessageArchiveManagement
private static final long MILLISECONDS_IN_DAY = 24 * 00 *0;
public static final long_CUP = MCON_DAY
/.../
.("",.getStart
add
ifget() >0
Node end("
end.("
endNode.Value("", Util.getTimestamp(query.getEnd
addNode
} if (.withid null && contact null && !isference
Node with(" .with
.Value("valuewith
.(
// queryMessageive(connection, nextQuery
final(connectionProtocol(), query
synchronized (eries)
// queries.add(nextQuery } }
public boolean queryInProgress( contact, OnLoaded
moreMessagesLoadedListener)
ized (eries)
(Query query : queries)
if(query.getWith().equals(contact.getUserId()))
if (query.onMoreMessagesLoaded == null &&MessagesListener
null) query.setOnMoreMessagesLoaded(Listener}
return true;}} return false;}}
private void finalizeQuery(Protocol protocol, Query query) {
synchronized (queries) {
.remove(query); }
Contact contact = null;
if (query.getWith() != null) {
contact = protocol.getItemByUID(query.getWith()); }
if (contact != null) {
Next line of code:
LLMs’ Response:
contact.setLastMessageTransmitted(query.getEnd());\n
Ground Truth:
if (contact.setLastMessageTransmitted(query.getEnd())) {
Zero-shot LLMs’ Response:
contact.removeQuery(query);\n
Figure 8: Cases study on lcc code completion task in LongBench benchmark (Bai et al., 2023) in 2,000 constraint
using GPT-3.5-Turbo.

--- PAGE 20 ---
Compressed Prompt:
Please determine the Type of the question below. Here are some examples of questions.
Question: How is energy created ? Type Manner of an action
Question: What is chocolate ? Type: Definition of something
Question: What is a bone marrow transplant ? Type: Definition of something
Question: What is fear of odors , body , ? Type Disease and medicine
Question: What was the Vietnam War ? Type: Definition of something
Question: was education system in 16s ? Type: Other entity
Question: What is IP address ? Type: Definition of something
Question: are the differences in Catholic Methodist religions ? Type of something
...
Question: When was San fire ? : Date
Question: CNN began broadcasting in what year ? Type: Date
Type: Manner of an action
Question: What the l behind the ir in the eye called ? Type Equ term
Type: Date
Question: What the former name of Zimbabwe ? Type: termType something
Question: What is troilism ? Type: Definition of something
: What is origin of the word , Type: of something
: do you name to social security number ? Type Manner of an action
: that of an employee Universal and Export ? Type Individual
: anesthetic did Queen Victoria allow to be for the birth of her seventh , in 183 ? Type: Disease
and medicine
: Where isyer ’s rock ? Type location
Question: What isymnophobia ? Type: Definition of something
...
Type burns the most calories ?
Type Sport
: In what book I find story of Aladdin ? Type In, book and piece an have sex ?
Type: Manner of an action: What is the acron for rating forer ?
Type Abbreviation
: are the Baltic States ? Type: Definition of something
: What is appearance , that violates the standards of sexual mor ? Type
: Where did the May people live ? : location
: What population Kansas ? Type number
: was the hurr ? Type: Event
: ’s a score aymnast exercise ? Type: number
: year become a state ? Type: Date
do go school ? Type Reason
...
Question: What is a fuel cell ?
Type:
LLMs’ Response:
Definition of something
LLMs’ Response after Subsquence Recovery:
Definition of something
Ground Truth:
Definition of something
Figure 9: Cases study on trec few-show learning in LongBench benchmark (Bai et al., 2023) in 2,000 constraint
using GPT-3.5-Turbo.
