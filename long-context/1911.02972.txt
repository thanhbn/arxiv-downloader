# 1911.02972.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/1911.02972.pdf
# File size: 716586 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Blockwise Self-Attention for Long Document Understanding
Jiezhong Qiu1, Hao Ma2, Omer Levy2, Wen-tau Yih2, Sinong Wang2, Jie Tang1
1Department of Computer Science and Technology, Tsinghua University
2Facebook AI
qiujz16@mails.tsinghua.edu.cn
fhaom,omerlevy,scottyih,sinongwang g@fb.com
jietang@tsinghua.edu.cn
Abstract
We present BlockB ERT, a lightweight and ef-
ﬁcient BERT model for better modeling long-
distance dependencies. Our model extends
BERT by introducing sparse block structures
into the attention matrix to reduce both mem-
ory consumption and training/inference time,
which also enables attention heads to cap-
ture either short- or long-range contextual in-
formation. We conduct experiments on lan-
guage model pre-training and several bench-
mark question answering datasets with vari-
ous paragraph lengths. BlockB ERT uses 18.7-
36.1% less memory and 12.0-25.1% less time
to learn the model. During testing, BlockB ERT
saves 27.8% inference time, while having com-
parable and sometimes better prediction accu-
racy, compared to an advanced BERT-based
model, RoBERTa.
1 Introduction
Recent emergence of the pre-training and ﬁne-
tuning paradigm, exempliﬁed by methods like
ELMo (Peters et al., 2018), GPT-2/3 (Radford et al.,
2019; Brown et al., 2020), BERT (Devlin et al.,
2019), XLNet (Yang et al., 2019), RoBERTa (Liu
et al., 2019) and ALBERT (Lan et al., 2019), has
drastically reshaped the landscape of the natural
language processing research. These methods ﬁrst
pre-train a deep model with language model objec-
tives using a large corpus and then ﬁne-tune the
model using in-domain supervised data for target
applications. Despite its conceptual simplicity, this
paradigm has re-established the new state-of-the-
art baselines across various tasks, such as question
answering (Devlin et al., 2019), coreference resolu-
tion (Joshi et al., 2019b), relation extraction (Soares
et al., 2019) and text retrieval (Lee et al., 2019;
Nogueira and Cho, 2019), to name a few.
This work was partially done when the ﬁrst author was
an intern at Facebook AI. Code is available at https://
github.com/xptree/BlockBERTBuilding such models in practice, however, is
an extremely resource-intensive process. For in-
stance, the training of BERT-family models is noto-
riously expensive. Devlin et al. (2019) report that
it takes four days to pre-train BERT-Base/BERT-
Large on 4/16 Cloud TPUs. In order to reduce the
pre-training time of RoBERTa to 1 day, Liu et al.
(2019) use 1,024 V100 GPUs. One crucial factor
contributing to the long training time is the memory
consumption of these deep models, as it directly
affects the batch size. Although the ﬁne-tuning
stage is relatively inexpensive, the memory issue
still restricts the scenarios in which BERT can be
used. For instance, “it is currently not possible
to re-produce most of the BERT-Large results on
the paper using a GPU with 12GB-16GB of RAM,
because the maximum batch size that can ﬁt in
memory is too small.1”
Although one may think that model size is the
main contributor to the large memory consump-
tion, our analysis (Section 2.1) shows that one of
the main bottlenecks is actually dot-product self-
attention, operated in multiple layers of Transform-
ers (Vaswani et al., 2017), the building block of
BERT. As the attention operation is quadratic to
the sequence length, this fundamentally limits the
maximum length of the input sequence, and thus
restricts the model capacity in terms of capturing
long-distance dependencies. As a result, down-
stream tasks have to either truncate their sequences
to leading tokens (Nogueira and Cho, 2019) or split
their sequences with a sliding window (Joshi et al.,
2019a,b). Ad-hoc handling of long sequences is
also required in the pre-training stage, such as up-
dating the model using only short sequences in the
early stage (Devlin et al., 2019).
Common strategies for reducing memory con-
sumption, unfortunately, do not work. For instance,
1github.com/google-research/bertarXiv:1911.02972v2  [cs.CL]  1 Nov 2020

--- PAGE 2 ---
shrinking the model by lowering the number of lay-
ersL, attention heads A, or hidden units Hleads
to signiﬁcant performance degradation (Vaswani
et al., 2017; Devlin et al., 2019) and does not
address the long sequence issue. Alternatively,
general low-memory training techniques, such as
microbatching (Huang et al., 2018) and gradient
checkpointing (Chen et al., 2016) essentially trade
off training time for memory consumption, pro-
longs the already lengthy training process.
In this work, we explore a different strategy,
sparsifying the attention layers , intending to de-
sign a lightweight and effective BERT that can
model long sequences in a memory-efﬁcient way.
Our Block BERT extends BERT by introducing
sparse block substructures into attention matrices
to reduce both memory consumption and the num-
ber of ﬂoating-point operations (FLOPs), which
also enables attention heads to capture either short-
or long-range contextual information. Compared
to the previous method that also enforces spar-
sity (Child et al., 2019), our approach is much
simpler mathematically and very easy to imple-
ment. More importantly, the results of experiments
conducted on several benchmark question answer-
ing datasets with various paragraph lengths show
that Block BERTperforms comparably or even bet-
ter than the original BERT-family models, while
enjoying an 18.7-36.1% reduction in memory us-
age, a 12.0-25.1% reduction in training time, and a
27.8% reduction in inference time.
The rest of the paper is organized as follows.
Section 2 gives a brief introduction of the BERT
model, along with an in-depth analysis of its mem-
ory usage during training time. We describe our
proposed model in Section 3 and contrast it with ex-
isting methods that aim for creating a lighter model.
Section 4 presents the experimental results and ab-
lation studies, followed by a survey of other related
work in Section 5 and the conclusion in Section 6.
2 Background: Memory Bottleneck in
Training BERT
We brieﬂy review BERT and introduce its memory
proﬁling in this section. Following the paradigm
of language model pre-training and down-stream
task ﬁne-tuning, BERT (Devlin et al., 2019) con-
sists of multiple layers of bidirectional Transform-
ers (Vaswani et al., 2017), where each Transformer
encoder has a multi-head self-attention layer and a
position-wise feed-forward layer. Using the samenotation as in (Devlin et al., 2019), we denote the
number of Transformer layers by L, the number of
hidden units by H, the number of attention heads
byA, the sequence length by N, and the batch size
byB. We also assume the feed-forward hidden
unit size to be 4H.2
2.1 Memory Proﬁling
Training BERT is a memory-intensive process. In
order to identify the bottleneck, we follow the mem-
ory model proposed by Sohoni et al. (2019), where
memory usage throughout neural network train-
ing is categorized into three main types: (1) Model
memory is used to store model parameters; (2) Op-
timizer memory is the additional memory used by
the speciﬁc learning algorithm during the process;
(3)Activation memory consists of the outputs of
each layer, which are cached for reuse in backprop-
agation to compute gradients.
Take BERT-Base training as an example. The
model has 110 million parameters, so model mem-
ory occupies 0.2 GB if parameters are stored in
half-precision ﬂoating-point format (FP16). For
Adam (Kingma and Ba, 2014), the optimizer needs
additional memory to store the gradients, ﬁrst mo-
ments, and second moments of model parameters.
If stored using the same precision, the optimizer
memory should be three times of model memory.3
To calculate the exact size of activation memory
is not trivial because it depends heavily on the im-
plementation of the toolkit. Instead, we measure
it empirically by training BERT-Base using Adam
with a memory proﬁler (more details are provided
in Appendix A.2).
We use 32 NVIDIA V100 GPUs for train-
ing. Every single GPU thus consumes a mini-
batch of size b=B=32 = 8 . Figure 1(a)
shows the proﬁling result for a single GPU, where
the model/optimizer/activation memory consumes
0.21/1.03/8.49 GB, respectively. We can see that
activation memory accounts for the vast majority of
the total GPU memory (87.6%) and is thus the bot-
tleneck. Notice that although our analysis is done
on BERT-Base, it can also be generalized to BERT-
Large and other models such as RoBERTa (Liu
et al., 2019) and XLNet (Yang et al., 2019).
2The default parameter settings for BERT-Base and BERT-
Large can be found in Appendix A.1
3In the current PyTorch Adam implementation, the ﬁrst
and second moments are stored in single precision. Conse-
quently, BERT’s optimizer memory (1 GB) is ﬁve times of
model memory (0.2 GB).

--- PAGE 3 ---
0.2
1
8.5activation
87.6%model
2.1%
optimizer
10.3%BERT-Base (GB)(a) BERT-Base Training Memory
Proﬁling
200 400 600 800 1000
Sequence Length N6789101112Act. Memory (GB)
BERT
0.00715N+ 4.83(b) Regression Analysis
on Activation Memory
Figure 1: Memory Proﬁling for BERT.
2.2 A Regression Analysis on Activation
Memory
For BERT, or more speciﬁcally, Transformer, the
activation memory corresponds to intermediate re-
sults of different layers. It grows linearly in all
the model hyper-parameters, except the sequence
lengthN, due to the attention layers. To quan-
tify the linear and quadratic components in the
activation memory more clearly, we conduct a re-
gression analysis as follows. Assume that the ac-
tivation memory (in each GPU) is a polynomial
a2bN2+a1bN+a0, wherebis the batch size
in each GPU and ai(i= 0;1;2) are coefﬁcients
to be determined. If we ﬁx the total number of
tokens in a GPU to be constant (in our case, we
ﬁxbN= 4096 ), we should have a linear func-
tion w.r.t.N, i.e., 4096a2N+ 4096a1+a0. We
enumerateNfromf128;256;512;1024gin our
experiments, and plot the corresponding proﬁled
activation memory in Figure 1(b). Using ordi-
nary least squares (OLS), with bN= 4096 ,
the estimated linear function for activation mem-
ory is 0:00715N+ 4:83, where the ﬁrst term
corresponds to the O(N2)component. When
N= 512 (i.e.,b= 8 ), we can see that for
BERT-Base, the O(N2)component accounts for
3.66 GB, and the O(N)component accounts for
4.83 GB. When the sequence length Nincreases to
1024 (i.e.,b= 4), theO(N2)component increases
to 7.32 GB, while the O(N)part is unchanged.
2.3 Techniques for Reducing Traing Memory
Observing that activation memory is the training
bottleneck, we discuss common memory reduction
techniques below.
Low Precision (Micikevicius et al., 2017) Low
precision is to use half-precision/mixed-precision
for training neural networks. This technique has
been widely used in Transformer training (Ott et al.,
2019; Liu et al., 2019). In this work, we alreadyassume to use mixed-precision training by default,
as indicated in the aforementioned analysis.
Microbatching (Huang et al., 2018) Micro-
batching is to split a batch into small micro-
batches (which can be ﬁt into memory), and then
run forward and backward passes on them sepa-
rately with gradients for each micro-batch accu-
mulated. Because it runs forward/backward pass
multiple times for a single batch, it trades off time
for memory.
Gradient Checkpointing (Chen et al., 2016) Gra-
dient checkpointing saves memory by only caching
activations of a subset of layers. The un-cached
activations will be recomputed during backpropaga-
tion from the latest checkpoint. This strategy trades
off time for memory by repeating computations and
will obviously extend training time.
Knowledge Distillation (Hinton et al., 2015)
Knowledge distillation aims to compress and trans-
fer knowledge from a teacher model to a simpler
student model. However, knowledge distillation
relies on a teacher model (which is still expensive
in training time) and usually suffers from a certain
degree of performance degradation.
(Ding et al., 2020) presents an alternative idea
based on cognitive theory to construct a working-
memory by identifying key sentences, which en-
ables multi-step reasoning. However, common
techniques are still limited in reducing both the
training time and memory usage. In this paper, we
investigate how to optimize the dot-product atten-
tion layers and introduce our approach next.
3 Model: BlockB ERT
Following (Vaswani et al., 2017), the dot-product
attention in Transformer is deﬁned as:
Attention (Q;K;V) = softmaxQK>
p
d
V;
whereQ;K;V2RNdwithNto be the se-
quence length and dto be a hidden dimension. As
we can see, the inner product between QandK
consumesO(N2)memory. One simple way to re-
duce the memory consumption of attention is to
sparsify the attention matrix. Suppose we have
a masking matrix M2f0;1gNN, we deﬁne a
masked version of attention as follows:
Attention (Q;K;V;M) = softmaxQK>
p
dM
V;
(1)

--- PAGE 4 ---
with operatordeﬁned by
(AM)ij=(
Aij ifMij= 1
 1 ifMij= 0:
In this work, we design Mto be a sparse block
matrix , which not only reduces memory and the
number of ﬂoating-point operations (FLOPs) but
also beneﬁts from efﬁcient dense matrix support
from deep learning frameworks, such as PyTorch
and Tensorﬂow. More formally, we split the length-
Ninput sequence into nblocks, with each block
of lengthN
n.4TheNNattention matrix is then
partitioned into nnblocks, where each block ma-
trix is of the sizeN
nN
n. We deﬁne a sparse block
matrixMby a permutation off1;2;;ng:
Mij=(
1if
b(i 1)n
N+ 1c
=b(j 1)n
N+ 1c;
0otherwise.(2)
By writing Q;K;Vas block matrices, such that
Q= [Q>
1Q>
n]>;K= [K>
1K>
n]>and
V= [V>
1V>
n]>and pluging them into Equa-
tion 1, we can formally deﬁne Blockwise Attention
as follows:
Blockwise-Attention (Q;K;V;M)
=2
666664softmax
Q1K>
(1)p
d
V(1)
...
softmax
QnK>
(n)p
d
V(n)3
777775:(3)
Equation 3 only needs to compute and store
QiK>
(i)(i= 1;n), each has sizeN
nN
n.
In other words, Block BERT reduces both O(N2)
memory consumption and FLOPs by a factor of n,
sinceN
nN
nn=NN
n.
3.1 Blockwise Multi-Head Attention
Analogous to Multi-head Attention (Vaswani et al.,
2017), we allow queries, keys, and values to be
projected multiple times and perform blockwise at-
tentions in parallel. Moreover, different blockwise
attention heads can use different masking matrices.
The outputs of multiple heads are then concate-
nated and aggregated with another linear projection.
LetAbe the number of attention heads and Hthe
number of hidden units. Blockwise multi-head at-
tention is formally deﬁned as follows:
Blockwise-Multi-head-Attention (Q;K;V)
=Concat (head 1;headA)WO;
4We assumeNcan be divided by n. If not, we pad the
input sequence to make Ndivisible.
Masking MatricesBlockwiseAttentionLinearLinearLinearConcatLinear
QKVMaskn=3n=2(1, 2)      (2, 1)(1, 2, 3)    (2, 3, 1)     (3, 1, 2)Figure 2: Architecture of Blockwise Multi-head Atten-
tion, which acts as building blocks of BlockB ERT. The
key idea is to introduce a sparse block masking matrix
to theNNattention matrix. The right panel shows
the masking matrices we use when n= 2;3. Forn= 2,
the masking matrices are deﬁned by permutation (1;2),
(2;1)and have 50% non-zeros. For n= 3, the masking
matrices are deﬁned by permutation (1;2;3),(2;3;1),
and(3;1;2)and have 33.33% non-zeros.
where for each head i,i= 1;2;;A,
headi=Blockwise-Attention (QWQ
i;KWK
i;V WV
i;Mi);
withd=H
A;WQ
i;WK
i;WV
i2RHdand the
projection matrix WO2RHH. Each mask-
ing matrix Miis determined by a permutation
iaccording to Equation 2. In particular, we
choosefrom permutations generated by shifting
one position := (2;3;;n;1), i.e., we select
2f;2;;ng. For example, with 12 atten-
tion heads (A= 12 ) and 2 blocks ( n= 2), we can
assign 10 heads to permutation (1;2)and the other
2 heads to permutation (2;1). Figure 2 illustrates
the blockwise multi-head attention with block num-
bern2f2;3g. Blockwise sparsity captures both
local and long-distance dependencies in a memory-
efﬁciency way, which is crucial for long-document
understanding tasks. For instance, the identity per-
mutation, i.e., (1;2;;n), enables each token to
attend to its nearby tokens in self-attention, while
other permutations allow tokens within the same
block attending to tokens in another block. Our
proposed Block BERTessentially replaces the multi-
head attention layers in Transformer/BERT with
blockwise multi-head attention.
3.2 Analysis of Memory Usage Reduction
To validate our claim that Block BERTwithnn
blocks can reduce the O(N2)memory usage by a
factor ofn, we perform the same memory proﬁling
as described in sections 2.1 and 2.2. Again, We ﬁx
the number of tokens in each GPU ( bN= 4096 )

--- PAGE 5 ---
and chooseNfromf128;256;512;1024;2048g.5
As we can see from Figure 3 and Table 1, the em-
pirical results align well with the theoretical values.
When we set the number of blocks to be 2 and 3 for
Block BERT, the estimated O(N2)activation mem-
ory decreases to 1/2 and 1/3 of BERT’s O(N2)acti-
vation memory, respectively. As shown in Table 2,
for the sequence length N= 512 , Block BERT
with 2 and 3 blocks saves 18.7% and 23.8% overall
memory, respectively. The saving is more signiﬁ-
cant for longer sequences. When N= 1024 , the
overall memory reduction of Block BERT with 2
and 3 blocks is 27.3% and 36.1%, respectively.
250 500 750 1000 1250 1500 1750 2000
Sequence Length N56789101112Act. Memory (GB)
BERT
0.00715N+ 4.83BlockBERT n=2
0.00357N+ 4.84BlockBERT n=3
0.00238N+ 4.87
Figure 3: Regression analysis on activation memory for
BERT and BlockB ERT.
Act. Mem. (GB)
Nb Model O(N)O(N2)
512 8BERT 4.83 3.66
BlockB ERTn=2 4.84 1.83
BlockB ERTn=3 4.87 1.22
1024 4BERT 4.83 7.32
BlockB ERTn=2 4.84 3.66
BlockB ERTn=3 4.87 2.44
Table 1: Estimated O(N2)andO(N)activation mem-
ory for BERT and BlockB ERT.
4 Experiments
We evaluate the pre-training and ﬁne-tuning perfor-
mance of Block BERT. In particular, when n= 2,
we denote 10:2 to be the conﬁguration which as-
signs 10 heads to permutation (1;2)and 2 to per-
mutation (2;1); whenn= 3, we denote 8:2:2 to be
the conﬁguration which assigns 8, 2, 2 heads to per-
mutation (1;2;3),(2;3;1), and (3;1;2), respec-
tively. We compare Block BERTwith the following
baselines:
Google BERT Google BERT is the ofﬁcial pre-
trained model from (Devlin et al., 2019).
5We use GPUs of 16 GB memory for proﬁling. BERT
withN= 2048 fails due to an out-of-memory error.RoBERTa-2seq & RoBERTa-1seq We compare
with two versions of RoBERTa (Liu et al., 2019).
RoBERTa-2seq is trained with both masked lan-
guage model (MLM) task and next sentence pre-
diction (NSP) task, while RoBERTa-1seq refers to
the pre-training model with only the MLM task.
SparseB ERT We pre-train BERT models with its
Transformer encoder replaced by a Sparse Trans-
former encoder (Child et al., 2019). We set its
sparsity hyper-parameters stride `= 128 and ex-
pressivityc= 32 .6The attention masking matrix
used in Sparse Transformer and more implemen-
tation details are discussed in Appendix A.3. A
similar architecture was adopted in GPT-3 (Brown
et al., 2020).
4.1 Pre-training
All the models follow the BERT-Base setting, i.e.,
L= 12;H= 768;A= 12 , and are trained
on the same corpus — BooksCorpus and English
Wikipedia with uncased word piece tokens. Thus
all models use the same vocabulary as Google
BERT (uncased version) with vocabulary size
30,522. We ﬁx the number of tokens per batch
BN= 131;072, i.e., if sequence length N=
512then batch size B= 256 , if sequence length
N= 1024 then batch size B= 128 . The detailed
pre-training conﬁguration is listed in Appendix A.1.
Moreover, the pre-training of Sparse BERT and
Block BERTfollows the RoBERTa-1seq setting, i.e.,
we drop the NSP (Next Sentence Prediction) task,
and an input sequence is up to Ntokens until it
reaches a document boundary.
A summary of the pre-training performance com-
parison between Block BERTand RoBERTa-1seq
is shown in Table 2. Besides memory saving, we
also achieve a signiﬁcant speedup. For example,
whenN= 1024 , Block BERT(n= 2) reduces the
training time from RoBERTa’s 9.7 days to 7.5 days.
4.2 Fine-tuning Tasks
We evaluate Block BERT on several question an-
swering tasks, including SQuAD 1.1/2.0 (Ra-
jpurkar et al., 2018) and ﬁve other tasks from
the MrQA shared task7— HotpotQA (Yang
et al., 2018), NewsQA (Trischler et al., 2017),
6We adopt Sparse Transformer implemented by Fairseq,
which ﬁrst computes the NNattention matrix, and then
masks it to be a sparse one. This implementation cannot
avoid theO(N2)attention computation, and thus has a similar
training time/memory cost to RoBERTa.
7mrqa.github.io

--- PAGE 6 ---
N Model Training Time (day) Memory (per GPU, GB) Heads Conﬁg. Valid. ppl
512RoBERTa-1seq 6.62 9.73 - 3.58
BlockB ERTn=2 5.83 (-12.0%) 7.91 (-18.7%) 10:2 3.56
BlockB ERTn=3 5.80 (-12.5%) 7.32 (-23.8%) 8:2:2 3.71
1024RoBERTa-1seq 9.66 13.39 - 3.60
BlockB ERTn=2 7.51 (-22.3%) 9.73 (-27.3%) 9:3 3.57
BlockB ERTn=3 7.23 (-25.1%) 8.55 (-36.1%) 8:2:2 3.63
Table 2: Pre-training Performance Analysis.
SearchQA (Dunn et al., 2017), TriviaQA (Joshi
et al., 2017) and NaturalQA (Kwiatkowski et al.,
2019). Since MrQA does not have an ofﬁcial test
set, we follow Joshi et al. (2019a) to split the devel-
opment set evenly to build a new development set
and test set.
These QA datasets have different paragraph
length distributions and are thus ideal for testing
the effectiveness of Block BERT8. For example,
SQuAD, NaturalQA, and HotpotQA consist of
mostly short paragraphs (shorter than 512), while
paragraphs in SearchQA (average length 1,004)
and TriviaQA (average length 934) have around
1,000 tokens. When the input sequence is longer
thanN, we follow the common practice (Joshi
et al., 2019a) to split it using a sliding window
of sizeNand stride 128. This means that for
SearchQA and TriviaQA, a model with N= 512
can only capture half of the context, while a model
withN= 1024 can accept the whole paragraph as
input.
For all models, we adopt the same ﬁne-tuning
QA setup from Devlin et al. (2019). The
tokenized paragraph (p1;;ps)and question
(q1;;qt)are concatenated to be a sequence
[CLS]q1qt[SEP]p1ps[SEP] . The se-
quence is then fed into the pre-trained model with
two extra linear layers for predicting the start and
end positions of the answer spans. The detailed
ﬁne-tuning setting is listed in Appendix A.4. Ta-
ble 3 and Table 4 report the experimental results.
BlockB ERT (n=2) v.s. RoBERTa-1seq Compar-
ing Block BERT with RoBERTa-1seq when N=
512, we observe an absolute F1 difference from
0.04 (in NaturalQA) to 1.18 (in NewsQA), with
an average of 0.55. For N= 1024 , Block BERT
achieves more comparable or even better perfor-
mance to RoBERTa-1seq, In SearchQA, NewsQA
and HotpotQA, Block BERTachieves absolute F1
8The detailed paragraph length distributions can be found
in Appendix A.5SQuAD 1.1 SQuAD 2.0
N Model EM F1 EM F1
- Human Perf. 82.30 91.20 86.80 89.40
512Google BERT 81.19 88.45 74.08 77.16
XLNet - - 78.46 81.33
RoBERTa-2seq 82.91 89.78 75.79 79.17
RoBERTa-1seq 84.43 91.48 79.22 82.27
SparseB ERT 80.49 88.09 74.15 76.96
BlockB ERTn=2 84.08 90.77 78.34 81.46
BlockB ERTn=3 82.37 89.64 77.33 80.33
1024RoBERTa-1seq 84.58 91.14 79.34 82.26
SparseB ERT 81.02 88.37 74.51 77.57
BlockB ERTn=2 83.65 90.74 78.55 81.45
BlockB ERTn=3 82.74 90.05 76.79 79.84
Table 3: Dev set results on SQuAD 1.1/2.0. The re-
sult of XLNet(-Base) is from Yang et al. (2019). For
BlockB ERTmodels, their attention head conﬁgurations
are the same as Table 2.
improvement of 0.39, 0.44 and 0.23, respectively.
BlockB ERT v.s. SparseB ERT ForN= 512 , it is
interesting that BlockB ERTwith 3 blocks (density
33.33%) performs better then Sparse BERT (den-
sity 44.20%) in both SQuAD and MrQA tasks.
Similar results can be observed for N= 1024 ,
too. These results show that off-diagonal masking
matrices, e.g., the masking matrix deﬁned by per-
mutation (2;3;1)and(3;1;2), play crucial roles
in Block BERT. Furthermore, Block BERT with 2
blocks achieve a more signiﬁcant improvement.
Effect of Long Sequence Pre-training Our obser-
vations are twofold: (1) Long sequence pre-training
beneﬁts long sequence ﬁne-tuning. In TriviaQA
and SearchQA, of which paragraph lengths are
around 1024, pre-training models with N= 1024
achieve signiﬁcantly better performance. (2) The
heterogeneity of pre-training and ﬁne-tuning se-
quence length may hurt performance. For example,
in SQuAD, we do not see signiﬁcant performance
gain by using pre-trained models with N= 1024 ;
in HotpotQA and NewsQA, longer sequence pre-
training even hurts performance.

--- PAGE 7 ---
Effect of #Blocks It is not surprising that
Block BERT with 2 blocks ( n= 2) performs bet-
ter than that with 3 blocks ( n= 3), because it
keeps more attention matrix entries. The biggest
difference is in SQuAD 2.0 and NewsQA with
N= 1024 , where we observe an absolute loss
of 1.6 F1 by increasing block number from 2 to 3.
Efﬁcient inference with BlockB ERT We bench-
mark test efﬁciency of RoBERTa and Block BERT.
The benchmark code follows huggingface9. All ex-
periments are run 30 times on a 32GB V100 GPU
with half precision (FP16). We report the average
running time in Table 5. As we can see, Block BERT
does achieve speedup and memory reduction dur-
ing test time. Take 8 1024, i.e., batch size B= 8,
sequence length N= 1024 , as an example, we
can see that Block BERTwith 2 blocks saves 27.8%
of test time, and Block BERT with 3 blocks saves
more (30.4%). As for memory, we can observe that
RoBERTa cannot handle an input of size 16 1024,
while it is possible for BlockB ERTto work on it.
In summary, not only Block BERT saves train-
ing/inference time and memory, but it also has
a competitive and sometimes better performance,
especially for tasks with longer sequences. This
demonstrates the effectiveness of our blockwise
multi-head attention approach.
4.3 Ablation Study
We ﬁx the assignment of attention heads in the
above experiments. For example, Block BERTwith
sequence length N= 512 and 2 blocks is trained
with ten heads using permutation (1;2)and the
other two using permutation (2;1). However, there
are other ways to assign twelve attention heads,
e.g., seven heads for permutation (1;2)and the
other ﬁve for permutation (2;1). It would be inter-
esting to see how the assignment of heads affects
model performance. In this section, we grid search
attention head assignments and plot their best val-
idation performance in 1.2M training steps. The
results are shown in Figure 4.
Our observations are threefold: (1) Identity per-
mutations, i.e., (1;2)and(1;2;3), are important.
As shown in Figure 4, all optimal solutions assign
considerable attention heads to block-diagonal ma-
trices, since those matrices enable each token to at-
tend to its nearby tokens; (2) Non-identity permuta-
tions follow the rule of “vital few and trivial many.”
9github.com/huggingface/transformers/
blob/master/examples/benchmarks.pyAlthough identity permutations are important, as-
signing all attention heads to them (corresponding
to 12:0 and 12:0:0 in Figure 4) signiﬁcantly hurts
performance, since the model can not learn long-
term dependencies with only identity permutation;
(3) Pre-training performance and ﬁne-tuning per-
formance are correlated but not always consistent.
Whenn= 3, pre-training performance suggests
10:1:1 to be the best head assignment — ten heads
for permutation (1;2;3), one head for (2;3;1)and
one head for (3;1;2), but we observe that the con-
ﬁguration of 8:2:2 achieves better performance in
ﬁne-tuning tasks.
5 Related Work
In this section, we review the related work of mem-
ory optimization for neural network training and
recent efforts to simplify Transformer and BERT.
5.1 Low-memory neural networks training
Due to the large size of model parameters and deep
architectures, modern neural networks training re-
quires signiﬁcant amounts of computing resources.
As a result, there is an increasing interest in training
neural networks with low memory (Sohoni et al.,
2019). Mainstream techniques mostly address this
problem with a better system or engineering de-
sign, such as low-precision training (Micikevicius
et al., 2017), microbatching (Huang et al., 2018)
and gradient checkpointing (Chen et al., 2016). Al-
ternatively, there also exists some research focusing
on the theoretical aspect, including the recently pro-
posed lottery ticket hypothesis (Frankle and Carbin,
2018).
5.2 Efﬁcient Transformer
Since the invention of Transformer (Vaswani et al.,
2017) and its successful application to masked lan-
guage model pre-training (Devlin et al., 2019; Rad-
ford et al., 2019; Yang et al., 2019; Liu et al., 2019;
Lan et al., 2019), several approaches have been pro-
posed to simplify the model and its training process.
We summarize these attempts as follows:
Attention layer simpliﬁcation There are cur-
rently two lines of research trying to simplify
the multi-head attention layers. The ﬁrst one
focuses on attention matrix sparsiﬁcation. No-
table examples include Star Transformer (Guo
et al., 2019), Sparse Transformer (Child et al.,
2019), Adaptive Sparse Transformer (Correia et al.,

--- PAGE 8 ---
SearchQA TriviaQA NewsQA NaturalQA HotpotQA
N Model EM F1 EM F1 EM F1 EM F1 EM F1
512Google BERT 74.94 80.37 70.18 75.35 51.27 66.25 66.13 78.29 60.50 77.08
RoBERTa-2seq 76.12 81.74 71.92 76.79 52.45 66.73 66.98 78.63 61.52 77.81
RoBERTa-1seq 77.09 82.62 73.65 78.22 56.13 70.64 67.14 79.07 62.77 79.28
SparseB ERT 73.36 79.01 68.71 73.15 51.18 65.47 65.53 77.46 58.54 74.85
BlockB ERTn=2 76.68 82.33 72.36 77.53 54.66 69.46 66.94 79.03 62.13 79.15
BlockB ERTn=3 75.54 81.07 72.05 76.74 53.82 68.39 66.14 78.47 60.64 77.46
1024RoBERTa-1seq 77.47 83.12 75.29 80.20 55.00 69.64 68.28 80.35 61.89 78.71
SparseB ERT 74.83 80.54 70.56 75.34 51.67 67.16 65.07 77.31 59.65 76.02
BlockB ERTn=2 77.95 83.51 75.06 79.41 55.44 70.08 67.31 79.39 62.13 78.94
BlockB ERTn=3 76.98 82.76 74.78 79.28 53.48 68.50 65.91 78.20 61.89 78.18
Table 4: MrQA test results (Tasks are sorted decreasingly by average paragraph length). For BlockB ERTmodels,
their attention head conﬁgurations are the same as Table 2.
7:5 8:4 9:3 10:2 11:1 12:02.002.022.042.062.082.102.122.142.16valid loss
4.04.14.24.34.44.5
valid ppl
(a)N= 512;n= 2
7:5 8:4 9:3 10:2 11:1 12:02.002.022.042.062.082.102.122.142.16valid loss
4.04.14.24.34.44.5
valid ppl
 (b)N= 1024;n= 2
6:3:3 8:2:2 10:1:1 12:0:02.002.022.042.062.082.102.122.142.16valid loss
4.04.14.24.34.44.5
valid ppl
 (c)N= 512;n= 3
6:3:3 8:2:2 10:1:1 12:0:02.002.022.042.062.082.102.122.142.16valid loss
4.04.14.24.34.44.5
valid ppl
 (d)N= 1024;n= 3
Figure 4: Ablation over blockwise attention heads assignment.
BN 81024 161024 241024 321024
RoBERTa 0.1371 OOM OOM OOM
BlockB ERTn=2 0.0990 0.1869 OOM OOM
BlockB ERTn=3 0.0954 0.1790 0.2634 OOM
Table 5: Test time statistics (sec) for different input size.
OOM indicates out-of-memory.
2019; Sukhbaatar et al., 2019), Log-Sparse Trans-
former (Li et al., 2019) , Reformer (Kitaev et al.,
2020) and Longformer (Beltagy et al., 2020). How-
ever, due to the insufﬁcient support for sparse ten-
sors from the current deep learning platforms, some
of them have to represent a sparse matrix using a
dense matrix with a binary mask or rely on cus-
tomized CUDA kernels (Gray et al., 2017). As a
result, the speed-up or reduction in memory con-
sumption is sometimes limited in practice. The
second line of research prunes redundant attention
heads. Examples include (V oita et al., 2019) and
(Michel et al., 2019). Our Block BERT model be-
longs to the ﬁrst category, as we sparsify the atten-
tion matrices to be block sparse matrix.
Reducing model size for pre-training Knowl-
edge distillation (Hinton et al., 2015) is a gen-
eral technique that aims to compress and trans-
fer knowledge from a teacher model to a simplerstudent model. There are two recent efforts that
apply knowledge distillation to BERT pre-training
for reducing model size: TinyBERT (Jiao et al.,
2019) distills BERT using a smaller Transformer,
and Tang et al. (2019) distills BERT with a BiL-
STM.In contrast, ALBERT (Lan et al., 2019) is
a notable work that does not take the knowledge
distillation approach. It uses parameter-sharing
to reduce the number of parameters of the BERT
model. As discussed in section 2.1, parameter-
sharing reduces both model memory and optimizer
memory. These two parts account for about 12.4%
of total training memory for BERT-base. As for efﬁ-
ciency, parameter-sharing reduces communication
complexity in distributed training and thus saves
training time as well.
In the aforementioned efﬁcient Transformers, the
model quality is often demonstrated by compara-
ble language model perplexity, or equivalently the
bits per word/byte. It is often implicitly assumed
that similar language model perplexity implies sim-
ilar pre-training model quality, namely the same
performance on the downstream tasks. We would
like to point out that this assumption does not nec-
essarily hold. For example, the experiments on
the Enwik8 dataset by Child et al. (2019) demon-
strates that Sparse Transformer “surpasses the 1.03

--- PAGE 9 ---
state-of-the-art (bits per byte) for a similarly-sized
Transformer-XL and matching the 0.99 (bits per
byte) of a model trained with more than double
the number of parameters”. However, if we com-
pare Sparse BERT(pre-training model with Sparse
Transformer backbone) against XLNet (Yang et al.,
2019) (pre-training model with Transformer-XL
backbone) in SQuAD, Table 3 shows that XLNet
still outperforms Sparse BERTsigniﬁcantly. There-
fore, we believe that it is necessary to conduct a
comprehensive study and evaluation of existing ef-
ﬁcient Transformer models when used for masked
language model pre-training. Limited by resources,
in this work, we mainly compare Block BERT to
pre-training using Sparse Transformer (Child et al.,
2019), which is the earliest attempt to design efﬁ-
cient Transformer models and also the key contrib-
utor to the success of GPT-3 (Brown et al., 2020).
We plan to benchmark more models in the future.
6 Conclusion
In this work, we study the lightweight BERT model
with the goal of achieving both efﬁciency and ef-
fectiveness. We proﬁle and analyze the memory
bottlenecks of BERT and focus on optimize dot-
product self-attention, which consumes quadratic
memory with respect to the sequence length. To
reduce both time and memory consumption, we
present Block BERT, which sparsiﬁes the attention
matrices to be sparse block matrices. The proposed
model achieves time and memory saving without
signiﬁcant loss of performance.
In the future, we plan to benchmark more efﬁ-
cient Transfomers in language model pre-training
and ﬁne-tuning. We also would like to explore
more applications of Block BERT on NLP tasks
involving long sequences such as coreference res-
olution (Joshi et al., 2019b) and document-level
machine translation (Miculicich et al., 2018), and
also non-NLP tasks such as protein sequence mod-
eling (Rives et al., 2019; Rao et al., 2019).
Acknowledgments
The authors would like to thank Zhilin Yang,
Danqi Chen, Yinhan Liu, Mandar Joshi and
Luke Zettlemoyer for the helpful suggestions.
Jiezhong Qiu and Jie Tang were partially sup-
ported by the National Key R&D Program of
China (2018YFB1402600), NSFC for Distin-
guished Young Scholar (61825602), and NSFC
(61836013).References
Iz Beltagy, Matthew E Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150 .
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165 .
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos
Guestrin. 2016. Training deep nets with sublinear
memory cost. arXiv preprint arXiv:1604.06174 .
Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509 .
Gonc ¸alo M Correia, Vlad Niculae, and Andr ´e FT Mar-
tins. 2019. Adaptively sparse transformers. arXiv
preprint arXiv:1909.00015 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In NAACL-HLT’ 2019 , pages 4171–4186.
Ming Ding, Chang Zhou, Hongxia Yang, and Jie Tang.
2020. Cogltx: Applying bert to long texts. In
NeurIPS ’20 .
Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur
Guney, V olkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with
context from a search engine. arXiv preprint
arXiv:1704.05179 .
Jonathan Frankle and Michael Carbin. 2018. The lot-
tery ticket hypothesis: Finding sparse, trainable neu-
ral networks. arXiv preprint arXiv:1803.03635 .
Scott Gray, Alec Radford, and Diederik P Kingma.
2017. Gpu kernels for block-sparse weights. arXiv
preprint arXiv:1711.09224 .
Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,
Xiangyang Xue, and Zheng Zhang. 2019. Star-
transformer. In NAACL-HLT’ 2019 , pages 1315–
1325.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Yanping Huang, Yonglong Cheng, Dehao Chen, Hy-
oukJoong Lee, Jiquan Ngiam, Quoc V Le, and
Zhifeng Chen. 2018. Gpipe: Efﬁcient training
of giant neural networks using pipeline parallelism.
arXiv preprint arXiv:1811.06965 .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2019. Tinybert: Distilling bert for natural language
understanding. arXiv preprint arXiv:1909.10351 .

--- PAGE 10 ---
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,
Luke Zettlemoyer, and Omer Levy. 2019a. Spanbert:
Improving pre-training by representing and predict-
ing spans. arXiv preprint arXiv:1907.10529 .
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In ACL’ 17 , pages 1601–1611.
Mandar Joshi, Omer Levy, Daniel S Weld, and Luke
Zettlemoyer. 2019b. Bert for coreference reso-
lution: Baselines and analysis. arXiv preprint
arXiv:1908.09091 .
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efﬁcient transformer. arXiv
preprint arXiv:2001.04451 .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, et al. 2019. Natural questions: a bench-
mark for question answering research. Transactions
of the Association for Computational Linguistics ,
7:453–466.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. ALBERT: A lite bert for self-supervised
learning of language representations. arXiv preprint
arXiv:1909.11942 .
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised
open domain question answering. arXiv preprint
arXiv:1906.00300 .
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou,
Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan.
2019. Enhancing the locality and breaking the mem-
ory bottleneck of transformer on time series forecast-
ing. arXiv preprint arXiv:1907.00235 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? arXiv
preprint arXiv:1905.10650 .
Paulius Micikevicius, Sharan Narang, Jonah Alben,
Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev,
Ganesh Venkatesh, et al. 2017. Mixed precision
training. arXiv preprint arXiv:1710.03740 .Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas,
and James Henderson. 2018. Document-level neural
machine translation with hierarchical attention net-
works. In EMNLP’ 18 , pages 2947–2954.
Rodrigo Nogueira and Kyunghyun Cho. 2019. Pas-
sage re-ranking with bert. arXiv preprint
arXiv:1901.04085 .
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019. fairseq: A fast, extensi-
ble toolkit for sequence modeling. arXiv preprint
arXiv:1904.01038 .
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers) , pages 2227–
2237.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. arXiv preprint arXiv:1806.03822 .
Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan
Duan, Peter Chen, John Canny, Pieter Abbeel, and
Yun Song. 2019. Evaluating protein transfer learn-
ing with tape. In Advances in Neural Information
Processing Systems , pages 9686–9698.
Alexander Rives, Siddharth Goyal, Joshua Meier,
Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry
Ma, and Rob Fergus. 2019. Biological structure and
function emerge from scaling unsupervised learning
to 250 million protein sequences. bioRxiv , page
622803.
Livio Baldini Soares, Nicholas FitzGerald, Jeffrey
Ling, and Tom Kwiatkowski. 2019. Matching the
blanks: Distributional similarity for relation learn-
ing. arXiv preprint arXiv:1906.03158 .
Nimit Sharad Sohoni, Christopher Richard Aberger,
Megan Leszczynski, Jian Zhang, and Christopher
R´e. 2019. Low-memory neural network training: A
technical report. arXiv preprint arXiv:1904.10631 .
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bo-
janowski, and Armand Joulin. 2019. Adaptive
attention span in transformers. arXiv preprint
arXiv:1905.07799 .
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga
Vechtomova, and Jimmy Lin. 2019. Distilling task-
speciﬁc knowledge from bert into simple neural net-
works. arXiv preprint arXiv:1903.12136 .

--- PAGE 11 ---
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2017. Newsqa: A machine compre-
hension dataset. In Proceedings of the 2nd Work-
shop on Representation Learning for NLP , pages
191–200.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems , pages 5998–6008.
Elena V oita, David Talbot, Fedor Moiseev, Rico Sen-
nrich, and Ivan Titov. 2019. Analyzing multi-
head self-attention: Specialized heads do the heavy
lifting, the rest can be pruned. arXiv preprint
arXiv:1905.09418 .
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le.
2019. Xlnet: Generalized autoregressive pretrain-
ing for language understanding. arXiv preprint
arXiv:1906.08237 .
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. HotpotQA: A
dataset for diverse, explainable multi-hop question
answering. In EMNLP’ 18 .
A Appendix
A.1 Notations and Pre-training
Hyper-parameters
The notations and pre-training hyper-parameters
are listed in Table 6 and Table 7.
Description Base Large
B Batch size 256 256
A # Self-attention heads 12 16
L # Layers 12 24
H # Hidden units 768 1024
4H # Feed-forward hidden units 3072 4096
N Sequence length 512 512
Table 6: BERT notations.
A.2 Proﬁler Implementation
Among the three types of training memory, model
memory and optimizer memory is relatively easy
to proﬁle (can be computed by enumerating each
tenor and summing up tensor.numel() *
tensor.element size() ). To calculate ac-
tivation memory, (Sohoni et al., 2019) traverse Py-
Torch’s autograd graph and sum up the necessary
storage space. They ﬁnd that the summation of
model memory, optimizer memory, and activation
memory matches PyTorch memory proﬁling tool10.
10torch.cuda.max memory allocatedHyper-parameter Value
V ocabulary Size 30,522
Dropout 0.1
Attention dropout 0.1
Warmup steps 10K
Weight decay 0.01
Max steps 2.4M
Initial learning rate 0.00025
Learning rate decay Linear
Adam 1e-8
Adam1 0.9
Adam2 0.999
Gradient Clipping 1.0
Table 7: Pre-training hyper-parameters.
Based on their observation, we use the following
quantity as an estimate to activation memory
max memory allocated  model memory optimizer memory
(4)
When proﬁling BERT, we ﬁrst pre-train it for 1000
steps, and then compute its model and optimizer
memory. Finally, we estimate its activation mem-
ory according to Equation 4.
A.3 SparseB ERT
The sparse masking matrices we use for Sparse
Transformer (Child et al., 2019) are shown in Fig-
ure 5. We adopt the implementation of Sparse
Transformer from Fairseq11. The Fariseq version
is implemented in a direct way, with the goal of
comparing performance, not speed. We ﬁrst com-
pute theN2attention matrix and then mask it to be
a sparse matrix according to the sparse pattern de-
ﬁned in Sparse Transformer paper. Consequently,
this implementation of SparseBERT has very close
training time/memory cost as RoBERTa (as it can
not avoid the O(N2)attention computation). We
did so because the code released by Sparse Trans-
former is based on Tensorﬂow and relies on cus-
tomized CUDA kernels, but our pre-training is
done using PyTorch.
A.4 Fine-tuning Settings
Our ﬁne-tuning is implemented based on code base
from HuggingFace12and SpanBERT (Joshi et al.,
2019a). We use maxsequence length=N,
i.e., we allow ﬁne-tuning task to input se-
quences as long as the pre-training model.
If the input sequence is too long to ﬁt the
11github.com/pytorch/fairseq/blob/
master/fairseq/modules/sparse_multihead_
attention.py.
12github.com/huggingface/
pytorch-transformers

--- PAGE 12 ---
0 100 200 300 400 500
0
100
200
300
400
500(a)
0 200 400 600 800 1000
0
200
400
600
800
1000 (b)
Figure 5: The sparse masking matrices we use in
Sparse Transformer (ﬁxed mode) encoder. White color
indicates attention values to be masked. (a) N=
512;`= 128;c= 32 , density 44.20%; (b) N=
1024;`= 128;c= 32 , density 34.97%.
maxsequence length=Nconstraints, we use
a sliding window of stride 128 to split it. We grid
search learning rate from f5e-6, 1e-5, 2e-5, 3e-
5, 5e-5gand batch size from f16, 32g. The ﬁne-
tuning is performed for 4 epoches.
A.5 Paragraph-Length Distribution
The paragraph-length distribution of SQuAD and
MrQA datasets is shown in Figure 6.
0 200 400 600 800 1000 1200
Paragraph Length0.0000.0020.0040.0060.008HistogramSearchQA (1004)
TriviaQA (934)NewsQA (641)
NaturalQA (247)HotpotQA (216)
SQuAD (156)
Figure 6: Paragraph-length (after tokenization) distri-
bution. The distribution of SQuAD 2.0 is very similar
to SQuAD 1.1, so we only plot SQuAD 1.1 here.
