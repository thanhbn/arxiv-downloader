# 2310.15494.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2310.15494.pdf
# File size: 466541 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
TRAMS : Training-free Memory Selection for Long-range Language Modeling
Haofei Yu‚ô°‚àó, Cunxiang Wang‚ô£‚Ä†, Yue Zhang‚ô£, Wei Bi‚ô¢‚Ä°
‚ô°Language Technologies Institute, Carnegie Mellon University, USA
‚ô£School of Engineering, Westlake University, China‚ô¢Tencent AI Lab, China
haofeiy@cs.cmu.edu ,{wangcunxiang, zhangyue}@westlake.edu.cn ,
victoriabi@tencent.com
Abstract
The Transformer architecture is crucial for nu-
merous AI models, but it still faces challenges
in long-range language modeling. Though
several specific transformer architectures have
been designed to tackle issues of long-range de-
pendencies, existing methods like Transformer-
XL are plagued by a high percentage of inef-
fective memories. In this study, we present a
plug-and-play strategy, known as TRA ining-
freeMemory Selection ( TRAMS ), that selects to-
kens participating in attention calculation based
on one simple metric. This strategy allows us to
keep tokens that are likely to have a high atten-
tion score with the current queries and ignore
the other ones. We have tested our approach
on the word-level benchmark ( WikiText-103 )
and the character-level benchmark ( enwik8 ),
and the results indicate an improvement with-
out having additional training or adding addi-
tional parameters.
1 Introduction
Transformer-based models (Kenton and Toutanova,
2019; Liu et al., 2019; Raffel et al., 2020; Lan et al.,
2019; Brown et al., 2020) have achieved remark-
able performance over the past few years. The key
component of these model architectures is the atten-
tion mechanism (Vaswani et al., 2017). However,
the original attention design struggles to efficiently
handle long sequences, which becomes particularly
problematic in scenarios such as document-level
translation (Werlen et al., 2018; Kim et al., 2019)
and large-scale text generation (Zhou et al., 2023),
as its time and space computation costs increase
quadratically with the sequence length (Tay et al.,
2022). The primary factor for this elevated com-
putational complexity can be traced back to the
multiplication between queries and keys used in
‚àóWork done during internship at Tencent AI Lab.
‚Ä†Co-first Author.
‚Ä°The correponding author.
QueryKeyMemory Selection
OracleTRAMSselect best ùêæ	based on TRAMS select best ùëÑùêæ!Figure 1: Two memory selection methods: For oracle,
it selects memories with the highest attention scores
after computing QK‚ä∫. For TRAMS , it selects important
key/value pairs that are independent of queries based on
our self-defined metric before computing QK‚ä∫.
the attention module. In general, the time com-
plexity for calculation is O(N2d)if a transformer
model with ddimensions is set up with an input
consisting of Ntokens.
To tackle this computation bottleneck, numerous
efforts have been made. The first line of work is
to find a new efficient expression to compute the
attention score. Despite the advancements made,
these methods often compromise performance, thus
paving the way for alternative solutions. Efficient
architectures that provide an approximate expres-
sion of attention have been explored widely (Wang
et al., 2020; Peng et al., 2022b,a; Choromanski
et al., 2021; Zheng et al., 2022b,a). The second
line of work is to keep the calculation expression
the same and use an external structure like hash
function (Kitaev et al., 2019; Daras et al., 2020),
clustering (Roy et al., 2021; Vyas et al., 2020) and
memory selector (Pietruszka et al., 2022; Dai et al.,
2019; Bertsch et al., 2023; Sukhbaatar et al., 2021,
2019; Child et al., 2019) to find the suitable sub-
set of queries and keys in the long sequence for
attention calculation.arXiv:2310.15494v3  [cs.CL]  20 Dec 2023

--- PAGE 2 ---
Our work falls into the second category, in
which we propose a training-free memory selection
mechanism to select suitable tokens for attention
computation. Specifically, we focus on pushing
Transformer-XL (Dai et al., 2019) architecture to a
better position by selecting higher-quality tokens
inside its memory. Based on our initial investiga-
tion, we construct a memory subset by selecting
50% of the memories with the largest attention
values and maintaining the same performance. It
indicates that a large portion of information in
memory is not fully utilized . This motivates us to
explore better methods to optimize memory usage.
Illustrated in Figure 1, we propose a TRA ining-
free Memory Selection method ( TRAMS ) that can
be directly plugged into memory-based long-range
language models and reduces the time complex-
ity of computing attention matrix. Through ex-
periments on two language modeling benchmark
datasets, namely word-level WikiText-103 (Mer-
ity et al., 2016) and character-level enwik8 (Ma-
honey, 2011), we achieve an improvement in the
model‚Äôs performance, as demonstrated by a 0.19
perplexity (ppl) drop in WikiText-103 and a 0.017
reduction in bits-per-character (bpc) in enwik8 .
To our knowledge, we are the first to design a
training-free memory selection method based on
Transformer-XL architecture.1
2 Method
2.1 Problem Definition
We use h‚ààRN√ódto represent the input hidden
states for the attention module, o‚ààRN√ódto rep-
resent the output hidden states for the attention
module, m‚ààRM√ódto represent the memory hid-
den states used in the attention calculation. We use
WQ,WK,WVto represent the trainable projection
matrix in the attention module. We define dfor the
dimension of the model, Mfor the memory size,
andNfor the input size. The attention calculation
process can be formally written as o=Attn(h,m).
With the above annotations, the problem of mem-
ory selection can be defined as choosing a subset of
hidden states memory Àúmfrom the memory mthat
brings the minimum difference to the transformer
layer output but with a smaller memory size.
Àúm‚àó=arg min
Àúm‚äÇm‚à•Attn(h,Àúm)‚àíAttn(h,m)‚à•(1)
1Source code for this paper is available at
https://github.com/lwaekfjlk/TRAMS.2.2 Attention Reformulation
Standard Attention In a memory-augmented
language model, the standard attention mecha-
nism (Vaswani et al., 2017) between input hidden
states and memory hidden states can be written as:
Attn(h,m)=softmax (QK‚ä∫
‚àö
d)V (2)
where Q=hWQis the product of target token
hidden states hand query projection matrix WQ;
K=mWKis the product of memory token
hidden states mand key projection matrix WK;
V=mWVis also the product of memory token
hidden states mand value projection matrix WV.
Unlimiformer Attention Different from the
well-known attention score calculation, Unlimi-
former (Bertsch et al., 2023) proposed a rewrit-
ten way to compute the dot-product part of cross-
attention in the encoder-decoder architecture:
QK‚ä∫=(hdWQ)(heWK)‚ä∫
=(hdWQW‚ä∫
K)h‚ä∫
e (3)
where heis the encoder hidden state and hdis the
decoder hidden state. It allows Unlimiformer to
avoid indexing the keys for each head and layer
separately and avoid storing values in a separate
index from the keys during kNN-based searching
and retrieval stage, making it more efficient.
TRAMS Attention Even though we have no need
to store or index any key or value for our method,
Unlimiformer attention motivates us to transfer
more useful information to keys by reformulating
attention and allows us to do more effective mem-
ory selection solely based on reformulated keys.
We can compute this attention formula in a differ-
ent order but maintain the same result:
QK‚ä∫=(hWQ)(mWK)‚ä∫
=(h)(mWKW‚ä∫
Q)‚ä∫(4)
Thus, we define Q‚Ä≤=has the reformulated query
for this attention expression and K‚Ä≤=mWKW‚ä∫
Q
as the reformulated keys for attention. With this
reformulation, we transfer all attention-related para-
metric information onto reformulated key vectors.
2.3 Transformer Hidden Space
Sincehis the input of the current transformer layer
and also the output of the previous transformer
layer, it is the result of the last layer‚Äôs Layernorm

--- PAGE 3 ---
0.00.10.20.30.40.50.6Key Norm Prob Density024681012
Query Norm Prob DensityFigure 2: Norm distribution of reformulated Q‚Ä≤andK‚Ä≤.
The reddistribution represents the query norm. The
blue distribution represents the key norm.
operation. We can define the coordinate-wise av-
erage of has¬µand the coordinate-wise standard
deviation of hasœÉ. Expressions can be written as:
¬µ=1
dd
‚àë
i=1hi‚âà0, œÉ=‚åüroo‚ü™‚ü™op
‚åüroo‚ü™mo‚ü®‚åüroo‚ü™mo‚ü®‚åüroo‚ü™‚ü®o‚ü™1
dd
‚àë
i=1(hi‚àí¬µ)2‚âà1(5)
Since the mean value for the hidden states his
around zero, we can confirm the hidden states vec-
tors are approximately orthogonal to the ‚Éó1vector
and the L2 norm of hidden states is around‚àö
d.
With this approximation, we can expand our re-
formulated attention score as:
Q‚Ä≤K‚Ä≤‚ä∫=(h)(mWKW‚ä∫
Q)‚ä∫
=‚à£‚à£Q‚Ä≤‚à£‚à£‚ãÖ‚à£‚à£K‚Ä≤‚à£‚à£‚ãÖcos‚ü®Q‚Ä≤,K‚Ä≤‚ü©
‚âà‚àö
d‚ãÖ‚à£‚à£K‚Ä≤‚à£‚à£‚ãÖcos‚ü®Q‚Ä≤,K‚Ä≤‚ü© (6)
where‚à•Q‚Ä≤‚à•stands the L2 norm for Q‚Ä≤and‚à•K‚Ä≤‚à•
stands for the L2 norm for K‚Ä≤. Based on Fig 2,
we see that reformulated query norm ‚à£‚à£Q‚Ä≤‚à£‚à£has a
much sharper distribution compared with key norm
‚à£‚à£K‚Ä≤‚à£‚à£, indicating reformulated query norm can be
approximated by a constant factor.
2.4 Training-free Memory Selection ( TRAMS )
Our target for memory selection is to recover the
complete attention score with as few memory to-
kens as possible. This problem is equivalent to
finding the subset of memory tokens that have the
highest attention scores with queries. We propose
a heuristic method to perform token-level selection
for each layer and each head based on a memory-
independent metric in this section.
There are two crucial components for calculating
the attention score after approximating ‚à£‚à£Q‚Ä≤‚à£‚à£with
a constant factor: the norm of the reformulated110 30 50 100020406080100
Highest Attention Score (%)Spearman Correlation (%)‚à£‚à£K‚Ä≤‚à£‚à£
cos‚ü®Q‚Ä≤,K‚Ä≤‚ü©
Q‚Ä≤K‚Ä≤‚ä∫
Figure 3: Spearman Correlation Score on different rank-
ing metrics with the groundtruth one.
keys‚à£‚à£K‚Ä≤‚à£‚à£and the angles between the reformu-
lated keys and queries arccos‚ü®Q‚Ä≤,K‚Ä≤‚ü©, which is
proved in Khandelwal et al. (2019). Commonly,
we believe that arccos‚ü®Q‚Ä≤,K‚Ä≤‚ü©is the more impor-
tant factor in general. Yet, if we use the ranking of
attention score value for all query and key pairs as
ground-truth ranking, based on Fig 3, we empiri-
cally discovered that rankings based on key norms
and rankings based on angles produce close Spear-
man correlation scores when only taking the high-
est 1% attention scores into account. Therefore,
it indicates that we can rank our memory tokens
based on ‚à£‚à£K‚Ä≤‚à£‚à£solely to gain a relatively good per-
formance when we desire top 1% attention scores
with queries in our memories instead of all.
Additionally, we discovered that relying solely
on a large norm isn‚Äôt sufficient as a constraint.
Specifically, keys that are nearer to ‚Éó1tend to yield
a higher attention score. To address this, we in-
troduce a combined metric: s=cos‚ü®K‚Ä≤,‚Éó1‚ü©‚à£‚à£K‚Ä≤‚à£‚à£.
This metric allows us to identify tokens that can
produce high attention scores when paired with
the appropriate query (owing to a high value of
‚à£‚à£K‚Ä≤‚à£‚à£) and lowscores when paired with an unsuit-
able query (owing to the high level of orthogonality
with the query space based on cos‚ü®K‚Ä≤,‚Éó1‚ü©). This
is due to the near orthogonality to the query space,
as indicated by a small angle with ‚Éó1, which is or-
thogonal to the query space.
3 Experiments
We introduce the compared methods and report
the main results and analysis on different attention
variants for inference in this section. Datasets de-
tails for WikiText-103 andenwik8 benchmarks
and their evaluation metric details are included in
Appendix A. The details of the model that we built
memory selection on can be seen in Appendix B.

--- PAGE 4 ---
WikiText-103
Model M m n PPL ( ‚Üì)
Transformer+RPE - - - 29.14
Transformer-XL - 200 64 24.17
TRAMS 400 200 64 23.98
enwik8
Model M m n bpc ( ‚Üì)
Transformer+RPE - - - 1.240
Transformer-XL - 200 64 1.215
TRAMS 400 200 64 1.198
Table 1: Model performance on the word-level
WikiText-103 and the character-level enwik8 datasets.
3.1 Compared Methods
Transformer+RPE (Vaswani et al., 2017): the
vanilla transformer baseline with relative position
embedding that is the same as Transformer-XL.
Therefore, the only difference between this model
and Transformer-XL is the additional memories.
More information related to relative position em-
bedding can be seen in Appendix C.
Transformer-XL (Dai et al., 2019): a specific-
designed architecture for long-range language mod-
eling. It includes relative position embedding and
recurrent memories per layer. Memory slots are
filled with hidden states from previous time steps.
3.2 Experimental Settings
We compare our methods with the Transformer-
XL (Dai et al., 2019) under the same size of mem-
ory (m=200) for attention calculation. For the
input token length nfor both models, we keep the
same as in (Dai et al., 2019) ( n=64). Additionally,
the memory selection process is performed on a
memory pool with the size of M. Our model and
the Transformer-XL share the model parameters
but have different inference strategies.
3.3 Main Results
The main results of WikiText-103 and enwik8
datasets are shown in Table 1. Without additional
training or additional parameters, we gain 0.19 im-
provement in perplexity and 0.017 improvement
for bit-per-character with our TRAMS mechanism.
We implement p-test by inferencing on multiple
model checkpoints and prove that our results are
significant ( p< 0.05).4 Discussions
IsTRAMS vulnerable to the selection of hyperpa-
rameters? There are three hyper-parameters in
TRAMS : the memory pool size MthatTRAMS is able
to select from; the selected memory size mthat
is used in the forward process; and the input to-
ken size nthat is involved in both backward and
forward process.
From the ablation study on M, Figure 4 sug-
gests an optimal range between 300 to 400 for the
memory pool size. Beyond this range, enlarging
the memory pool often leads to the selection of
irrelevant tokens, deteriorating our performance.
Regarding m, Figure 5 indicates that TRAMS wit-
nesses a substantial drop in perplexity when the
memory size selected is about 25%. Selecting a
larger portion does not yield further improvement.
This is consistent with Figure 3, where TRAMS ex-
cels by concentrating on the top 10% of results.
Lastly, in the study on n, Figure 6 shows that as
the target token length decreases, the efficacy of
memory selection improves.
200 300 400 500 60023.92424.124.2
Memory Pool Size MPerplexityTransformer-XL
TRAMS
Figure 4: Ablation study on memory pool size Mwhen
we fix m=200 and n=64.
200 300 400 500 60023.623.82424.224.424.6
Selected Memory Size mPerplexityTransformer-XL
TRAMS
Figure 5: Ablation study on selected memory size m
when we fix M=600 and n=64.
What is the inference cost compared to
Transformer-XL? Since there is no training part
in our model, we focus on discussing the inference
cost. Compared with Transformer-XL, our model
requires storing a larger memory pool to do mem-
ory selection. Therefore, the memory cost of our
method would be larger. When it comes to timing
cost, our model has an additional memory token

--- PAGE 5 ---
16 32 64 12823.823.92424.124.224.324.4
Input Length nPerplexityTransformer-XL
TRAMS
Figure 6: Ablation study on target length nwhen we fix
M=400 and m=200.
Model Peak GPU Mem (MB) Wall-clock Time (s)
Transformer-XL 3529 33.27
TRAMS 3719 49.55
Table 2: Results on GPU peak memory usage and wall-
clock inference time on WikiText-103 .
norm computation memory sorting operations, and
memory selection operations for each layer. These
extra operations require extra inference time. Table
2 shows the GPU memory cost and wall-clock time
for the Transformer-XL baseline and our model.
Our model requires slightly more GPU memory
usage and around 50% additional inference time
for memory selection.
How does TRAMS benefit from memory selection?
Memory selection helps the model pick tokens with
higher attention scores with the queries, thus in-
creasing the average memory utilization. Quantita-
tively, our method improves the average attention
probability by 24.25 % for the same size of memory
compared with Transformer-XL.
Does each layer hold the same importance?
Based on Figure 7, we show the ablation study
when applying memory selection on each layer
while remaining other layers the same. There is
an observable drop when we apply the memory
selection on the deeper layers starting from Layer
13 while we do not observe a clear influence when
applying memory selection on shallow layers.
5 Case Study
To have an understanding of what kind of context
should be selected, we provide one example case to
understand specifically what kind of tokens in the
memory would be selected. Based on Table 3, we
can see that most of the selected memory tokens are
low-frequency words. Those low-frequency words
like ‚Äú John " in the memory would be beneficial for
the prediction of ‚Äú John " in the target sequence.1 4 7 10 13 162424.124.2
Layer NumberPerplexityw/ memory selection on layer i
w/o memory selection
Figure 7: Ablation Study on Layer-wise Importance on
WikiText-103 .
Memory Sequence Segment
...Simon Stephens , which was performed in
2001 at the Royal Court Theatre. He had
a guest role in the television series Judge
John Deed in 2002. In 2004 Boulter landed
a role as "Craig" in the episode "Teddy‚Äôs
Story" of the television series The Long Firm;
he starred alongside actors Mark Strong and
Derek Jacobi. He was cast in the 2005 theatre
productions of the Philip Ridley play Mercury
Fur, which was performed at the Drum Theatre
in Plymouth and the <unk> Chocolate Factory
in London. He was directed by John Tiffany
and starred alongside Ben Whishaw , Shane Zaza,
Harry Kent, Fraser Ayres, Sophie Stanton, and
Dominic Hall. <eos> In 2006, Boulter starred
alongside Whishaw in the play Citizenship
written by Mark Ravenhill ...
Target Sequence Segment
He appeared in the television series Judge
John Deed in 2002 ...
Table 3: Case Study for memory selection from
WikiText-103 .text indicates that this word in mem-
ory sequence is selected and used in the forward pass.
text indicates that this word in the target sequence
benefits from the memory.
6 Conclusion
In this work, we formulate the problem of mem-
ory selection in transformer architecture and refor-
mulate the attention calculation process to obtain
our self-defined queries and keys. After that, we
propose a query-independent metric that utilizes
memory hidden states to implement a training-free
memory selector. Our experiments indicate that
this method offers a simple yet effective means
of identifying valuable memory tokens. Explor-
ing optimal memory selection strategies for large
language models is a promising avenue for future
research. Additionally, integrating trainable pa-
rameters into these models as memory selectors
presents another exciting direction for future work.

--- PAGE 6 ---
Limitations
Our study has a couple of main limitations. First,
we are currently focusing on the Transformer-XL
architecture, but there are many other models with
different sizes we haven‚Äôt tried. It indicates that
our findings could be limited to typical transformer
architecture. Second, our method has many hy-
perparameters including M,m, and n. Adjust-
ing them can greatly change how well our model
works. A careful calibration is thus necessary, and
one must tread cautiously to strike a balance and
achieve the desired performance, which could be
time-consuming and computationally expensive.
Ethics Statement
There are no recognized potential risks.
References
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R Gormley. 2023. Unlimiformer: Long-
range transformers with unlimited length input.
arXiv preprint arXiv:2305.01625 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877‚Äì1901.
Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509 .
Krzysztof Choromanski, Haoxian Chen, Han Lin,
Yuanzhe Ma, Arijit Sehanobish, Deepali Jain,
Michael S Ryoo, Jake Varley, Andy Zeng, Valerii
Likhosherstov, et al. 2021. Hybrid random features.
arXiv preprint arXiv:2110.04367 .
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-xl: Attentive language models beyond
a fixed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 2978‚Äì2988.
Giannis Daras, Nikita Kitaev, Augustus Odena, and
Alexandros G Dimakis. 2020. Smyrf-efficient atten-
tion using asymmetric clustering. Advances in Neu-
ral Information Processing Systems , 33:6476‚Äì6489.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
Toutanova. 2019. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. In
Proceedings of NAACL-HLT , pages 4171‚Äì4186.Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through memorization: Nearest neighbor language
models. In International Conference on Learning
Representations .
Yunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019.
When and why is document-level context useful in
neural machine translation? In Proceedings of the
Fourth Workshop on Discourse in Machine Transla-
tion (DiscoMT 2019) , pages 24‚Äì34.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2019. Reformer: The efficient transformer. In Inter-
national Conference on Learning Representations .
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learning
of language representations. In International Confer-
ence on Learning Representations .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Matt Mahoney. 2011. Large text compression bench-
mark.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els. In International Conference on Learning Repre-
sentations .
Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani
Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy
Schwartz, and Noah A Smith. 2022a. Abc: Attention
with bounded-memory control. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
7469‚Äì7483.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy
Schwartz, Noah Smith, and Lingpeng Kong. 2022b.
Random feature attention. In International Confer-
ence on Learning Representations .
Micha≈Ç Pietruszka, ≈Åukasz Borchmann, and ≈Åukasz
Garncarek. 2022. Sparsifying transformer models
with trainable representation pooling. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 8616‚Äì8633.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485‚Äì5551.

--- PAGE 7 ---
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and
David Grangier. 2021. Efficient content-based sparse
attention with routing transformers. Transactions of
the Association for Computational Linguistics , 9:53‚Äì
68.
Sainbayar Sukhbaatar, √âdouard Grave, Piotr Bo-
janowski, and Armand Joulin. 2019. Adaptive at-
tention span in transformers. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics , pages 331‚Äì335.
Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen
Roller, Arthur Szlam, Jason Weston, and Angela Fan.
2021. Not all memories are created equal: Learning
to forget by expiring. In International Conference on
Machine Learning , pages 9902‚Äì9912. PMLR.
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-
zler. 2022. Efficient transformers: A survey. ACM
Computing Surveys , 55(6):1‚Äì28.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Apoorv Vyas, Angelos Katharopoulos, and Fran√ßois
Fleuret. 2020. Fast transformers with clustered at-
tention. Advances in Neural Information Processing
Systems , 33:21665‚Äì21674.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 .
Lesly Miculicich Werlen, Dhananjay Ram, Nikolaos
Pappas, and James Henderson. 2018. Document-
level neural machine translation with hierarchical
attention networks. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing , pages 2947‚Äì2954.
Lin Zheng, Chong Wang, and Lingpeng Kong. 2022a.
Linear complexity randomized self-attention mech-
anism. In International Conference on Machine
Learning , pages 27011‚Äì27041. PMLR.
Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng
Kong. 2022b. Efficient attention via control vari-
ates. In The Eleventh International Conference on
Learning Representations .
Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui,
Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cot-
terell, and Mrinmaya Sachan. 2023. Recurrentgpt:
Interactive generation of (arbitrarily) long text. arXiv
preprint arXiv:2305.13304 .
A Dataset and Evaluation Metrics
WikiText-103 (Merity et al., 2016) is a commonly
used word-level language modeling benchmark. It
has an average length of 3.6 thousand tokens perarticle and includes 28 thousand Wikipedia articles.
This word-level dataset has a vocabulary size of
around 260K. We use the same data pre-processing
setting in Dai et al. (2019) for this dataset. We use
perplexity as our metric.
Enwik8 (Mahoney, 2011) is a character-level lan-
guage modeling benchmark. This dataset contains
100M unprocessed Wikipedia characters. The train
set, dev set, and test set include 80M, 10M, and
10M characters separately. enwik8 has no pre-
processing stage and is directly used. bpc(bit per
character) is defined as an evaluation metric and
we report results on both the dev set and test set.
B Training Configurations
Since we do inference experiments based on a
trained model, we separately train two Transformer-
XL models for WikiText-103 andenwik8 . For
the training stage, we use Adam (Kingma and
Ba, 2014) to optimize with a batch size=60,
learning rate=2.5e-4, target length=150, mem-
ory length=150, and a cosine scheduler without
warmup steps.
When it comes to a different dataset, we
use different Transformer-XL architecture. For
WikiText-103 , we use a 16-layer transformer ar-
chitecture with 10 heads, 410 hid dim, 0.1 dropout
ratio, 0.0 attention dropout ratio, 2100 inner dim,
and adaptive softmax mechanism. For enwik8 , we
propose a 12-layer transformer architecture with 8
heads, 512 hid dim, 0.1 dropout ratio, 0.0 attention
dropout ratio, and 2048 inner dim. Both models
are trained for 350K steps.
A batch size=10 and target length=150 are fixed
for all inference experiments to avoid unfair com-
parison. All experiments including training and
inference are conducted using 4 2080Ti GPUs. It
takes 280 GPU hours to train the enwik8 model
checkpoint. It takes 61 GPU hours to train the
WikiText-103 model checkpoint.
C Relative Position Embedding
Concerning positional encodings, we maintain the
same results with Transformer-XL. The positional
encodings include learnable parameters of Ri‚àíj,u,
andv. Typically, Ri‚àíjis derived from a learnable
rnetwork included in the model. The advantage
of using this design when computing the attention
score is that it avoids temporal confusion caused
by indexing the same position and considers the
relative distance between two tokens. The formula

--- PAGE 8 ---
for attention score calculation with relative position
embedding can be written as:
Axl
i,j=X‚ä∫
iW‚ä∫
qWE
kXj+X‚ä∫
iW‚ä∫
qWR
kRi‚àíj
+u‚ä∫WE
kXj+v‚ä∫WR
kRi‚àíj (7)
Moreover, after doing ablation studies on rela-
tive position embedding, we found that Ri‚àíjcon-
tributes the most to the result and u,vonly has
a small influence on the final performance. The
existence of Ri‚àíjleads to the exponentially de-
cayed attention probability distribution related to
a memory position. As a result, we base our mem-
ory selection on the Axl
i,jwhich includes positional
information instead of the pure X‚ä∫
iW‚ä∫
qWE
kXj.
To be noticed, all concepts related to qKare all
equipped with position embedding instead of a sim-
ple dot product.
