# 2305.14806.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2305.14806.pdf
# Kích thước tệp: 500681 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
AWESOME: Tóm tắt Tài liệu Dài bị Hạn chế Bộ nhớ GPU
sử dụng Cơ chế Bộ nhớ và Nội dung Nổi bật Toàn cục
Shuyang Cao và Lu Wang
Khoa Khoa học Máy tính và Kỹ thuật
Đại học Michigan
Ann Arbor, MI
{caoshuy, wangluxy}@umich.edu
Tóm tắt
Các hệ thống tóm tắt tài liệu dài rất
quan trọng đối với các lĩnh vực có văn bản
dài và nhiều thuật ngữ chuyên môn, tuy
nhiên chúng đặt ra những thách thức đáng
kể cho các nhà nghiên cứu và nhà phát triển
với tài nguyên tính toán hạn chế. Các giải
pháp hiện tại chủ yếu tập trung vào các cơ
chế attention hiệu quả hoặc chiến lược chia
để trị. Cách tiếp cận đầu giảm độ phức tạp
thời gian về mặt lý thuyết, nhưng vẫn tốn
nhiều bộ nhớ. Các phương pháp sau hy sinh
ngữ cảnh toàn cục, dẫn đến các bản tóm tắt
thiếu thông tin và không mạch lạc. Công
trình này nhằm tận dụng tính chất tiết kiệm
bộ nhớ của phương pháp chia để trị trong
khi vẫn bảo tồn ngữ cảnh toàn cục. Cụ thể,
framework AWESOME của chúng tôi sử
dụng hai cơ chế mới: (1) Cơ chế bộ nhớ
ngoài theo dõi các đoạn tài liệu đã được
mã hóa trước đó và các bản tóm tắt tương
ứng, để tăng cường hiểu biết tài liệu toàn
cục và tính mạch lạc của bản tóm tắt. (2)
Nội dung nổi bật toàn cục được xác định
thêm trước để bổ sung cho mỗi đoạn tài
liệu nhằm hỗ trợ việc tóm tắt của nó. Các
thí nghiệm rộng rãi trên nhiều thể loại văn
bản khác nhau, bao gồm báo cáo chính phủ,
bản ghi cuộc họp, kịch bản phim, bài báo
khoa học và tiểu thuyết, cho thấy AWESOME
tạo ra các bản tóm tắt với tính thông tin,
trung thực và mạch lạc được cải thiện so
với các baseline cạnh tranh trên các tài liệu
dài hơn, trong khi có dung lượng bộ nhớ
GPU nhỏ hơn.

1 Giới thiệu
Các mô hình transformer được pre-train lớn đã thể
hiện hiệu suất ấn tượng trên các benchmark tóm
tắt abstractive phổ biến (Lewis et al., 2020; Raffel
et al., 2020). Tuy nhiên, độ phức tạp bộ nhớ bậc
hai của transformer đặt ra những thách thức cho
việc tóm tắt các tài liệu dài có hơn hàng trăm từ,
chẳng hạn như các bài báo khoa học và báo cáo
điều tra (Cohan et al., 2018; Huang et al., 2021),
khiến việc này trở nên không khả thi đối với các
nhà nghiên cứu và nhà phát triển có tài nguyên
phần cứng hạn chế (ví dụ: GPU không đủ bộ nhớ)
để đóng góp vào lĩnh vực nghiên cứu quan trọng
này.

Cộng đồng NLP đã có một số đổi mới để giải quyết
thách thức tài liệu dài. Các công trình trước đây
chia một tài liệu thành các đoạn nhỏ hơn và tóm
tắt từng đoạn riêng biệt (Gidiotis và Tsoumakas,
2020), giảm độ phức tạp của tính toán attention
(Beltagy et al., 2020), và loại bỏ nội dung không
quan trọng trước khi chạy một abstractor (Pilault
et al., 2020). Về mặt hiệu quả bộ nhớ, các phương
pháp chia để trị đạt được lợi thế đáng kể nhất
(Moro và Ragazzi, 2022). Tuy nhiên, thông tin
bên ngoài một đoạn tài liệu và các bản tóm tắt
tương ứng trở nên không thể truy cập, dẫn đến
các bản tóm tắt thiếu thông tin và không mạch
lạc. Không ngạc nhiên, hiệu suất state-of-the-art
được đạt được bởi các mô hình có thể duy trì ngữ
cảnh toàn cục, ví dụ, bằng cách kết hợp global
attentions với local attentions trong các mô hình
tóm tắt dựa trên transformer (Zaheer et al., 2021;
Phang et al., 2022). Tuy nhiên, chúng vẫn yêu cầu
dung lượng bộ nhớ GPU lớn trong thực tế.¹

Mặc dù các mô hình ngôn ngữ lớn như GPT-4
(OpenAI, 2023) được huấn luyện để xử lý tới 32K
token, việc bảo mật và an toàn dữ liệu được truyền
và chia sẻ thông qua API vẫn đáng quan ngại, đặc
biệt trong các lĩnh vực xử lý thông tin nhạy cảm,
ví dụ như hồ sơ y tế. Phát triển mô hình cục bộ có
thể tăng cường quyền riêng tư và bảo mật; tuy
nhiên, tài nguyên tính toán hạn chế trong những
tình huống này đòi hỏi phải khám phá các kỹ thuật
mô hình hóa hiệu quả.

Do đó, công trình này nhằm giải quyết vấn đề tóm
tắt tài liệu dài sử dụng tài nguyên hạn chế, cụ thể
tập trung vào bộ nhớ GPU hạn chế. Chúng tôi đề
xuất AWESOME², được xây dựng trên phương
pháp chia để trị tiết kiệm bộ nhớ, và được Bổ sung
với Nội dung Nổi bật Ước tính và cơ chế Bộ nhớ.
Về bản chất, AWESOME duy trì ngữ cảnh toàn
cục của cả tài liệu nguồn và bản tóm tắt được tạo
ra cho đến hiện tại với việc sử dụng bộ nhớ hạn
chế, để tăng cường tính thông tin, trung thực và
mạch lạc của bản tóm tắt.

¹Những phương pháp này yêu cầu bộ nhớ GPU >40GB để
xử lý các tài liệu có hơn 8K token, trong khi các GPU hiệu
quả nhất về chi phí chỉ có 24GB bộ nhớ (Li, 2022).
²Mã nguồn của chúng tôi sẽ được cung cấp tại https://
shuyangcao.github.io/projects/awesome/

arXiv:2305.14806v2 [cs.CL] 16 Nov 2023

--- TRANG 2 ---
Đầu tiên, cơ chế bộ nhớ ngoài được sử dụng ở
phía encoder của AWESOME để lưu trữ thông tin
khi nó đọc các đoạn tài liệu theo trình tự. Điều
này duy trì ngữ cảnh có liên quan để cải thiện hiểu
biết tài liệu và phát hiện nội dung nổi bật, do đó
thúc đẩy tính thông tin và trung thực của bản tóm
tắt. Một bộ nhớ khác được áp dụng ở phía decoder
để cải thiện tính mạch lạc của việc tạo ra bằng
cách theo dõi các bản tóm tắt một phần được tạo
ra cho các đoạn tài liệu trước đó. Quan trọng là,
để đảm bảo hiệu quả bộ nhớ GPU của AWESOME,
chúng tôi ngăn cản gradient lan truyền đến các
đoạn tài liệu và tóm tắt khác và chỉ cho phép một
số lượng hạn chế các lớp duy trì bộ nhớ ngoài.

Thứ hai, AWESOME kết hợp nội dung nổi bật
toàn cục được chọn bởi một extractor được huấn
luyện hiệu quả thông qua (1) nối văn bản trực tiếp,
hoặc (2) chèn các ma trận key-value của chúng
vào tính toán attention. Điều này cho phép bộ tóm
tắt nhận biết các chủ đề quan trọng ở cấp độ toàn
cục, để tăng cường ước tính mức độ nổi bật và
tính thông tin của bản tóm tắt.

Chúng tôi thí nghiệm với năm benchmark đầu vào
dài phổ biến của các thể loại khác nhau: báo cáo
điều tra trong GovReport (Huang et al., 2021),
bản ghi cuộc họp trong QMSum (Zhong et al.,
2021), kịch bản TV trong SummScreen (Chen et
al., 2022), bài báo khoa học trong arXiv (Cohan
et al., 2018), và tiểu thuyết trong BookSum
(Kryscinski et al., 2022). Đầu tiên, trên cả năm
tập dữ liệu, tất cả các biến thể AWESOME đều
vượt trội Se3 (Moro và Ragazzi, 2022), baseline
chia để trị, về tính thông tin của bản tóm tắt được
đánh giá bằng ROUGE (Lin, 2004) và về tính mạch
lạc được đo bằng DiscoScore (Zhao et al., 2022)
và một thước đo dựa trên đồ thị thực thể
(Guinaudeau và Strube, 2013)—cả hai thước đo
đều có tương quan cao với đánh giá của con người,
theo Zhao et al. (2022). Thứ hai, AWESOME với
cơ chế bộ nhớ cũng cải thiện tính trung thực của
bản tóm tắt so với Se3 trên GovReport, theo
SummaC (Laban et al., 2022), một thước đo trung
thực dựa trên entailment. Cuối cùng, so với các
mô hình tốn nhiều bộ nhớ hơn cũng duy trì ngữ
cảnh toàn cục, chẳng hạn như Phang et al. (2022)
và Liu et al. (2022), AWESOME đạt được điểm số
tự động cao hơn về tính thông tin, mạch lạc và
trung thực trên GovReport (Huang et al., 2021).
Trên BookSum bao gồm các tài liệu và bản tóm
tắt dài nhất trong số

[THIS IS TABLE: Bảng 1 showing different approaches to long document summarization with columns for Approach, In→Out, Enc, Enc←Dec, Dec]

năm tập dữ liệu, AWESOME tạo ra các đầu ra
thông tin và mạch lạc hơn so với các mô hình
gần đây.

2 Công trình Liên quan

2.1 Tóm tắt Tài liệu Dài Hiệu quả

Chúng tôi phân loại các mô hình tóm tắt tài liệu
dài hiệu quả hiện tại thành bốn loại chính, như
được tóm tắt trong Bảng 1. Đầu vào mô hình có
thể là một tài liệu gốc, các đoạn quan trọng được
trích xuất từ tài liệu, hoặc một đoạn tài liệu, được
ký hiệu là x, xe, hoặc xi (cho đoạn thứ i), và thông
thường, |x|>|xe|>|xi|. Đầu ra có thể là bản tóm
tắt đầy đủ y hoặc một đoạn tóm tắt yi (cho xi),
trong đó |y|>|yi|. Quan trọng là, đầu vào và đầu
ra dài hơn mở rộng biểu đồ tính toán lớn hơn, dẫn
đến việc sử dụng bộ nhớ GPU cao hơn. Hơn nữa,
chúng tôi phân tích cả ngữ cảnh tài liệu và ngữ
cảnh tóm tắt được sử dụng bởi mỗi phương pháp
khi tạo ra bản tóm tắt. Cụ thể, chúng tôi kiểm tra
(1) tài liệu đầy đủ so với một phần được sử dụng
để có được các representation của encoder (Enc);
(2) representation encoder đầy đủ so với một phần
được decoder attend tới (Enc←Dec); và (3) đầu
ra đầy đủ so với một phần được decoder truy cập
(Dec).

Efficient attentions được thiết kế để giảm độ phức
tạp bậc hai của kiến trúc transformer gốc (Vaswani
et al., 2017) và duy trì ngữ cảnh mã hóa đầy đủ
bằng cách kết hợp global attentions với local
attentions được xây dựng trên sliding windows
(Beltagy et al., 2020; Zaheer et al., 2021), text
blocks (Phang et al., 2022; Tay et al., 2020), hoặc
clusters của các token tương tự (Kitaev et al.,
2020; Roy

--- TRANG 3 ---
et al., 2021). Bên cạnh các biến thể attention nói
trên được thiết kế cho self-attentions, công trình
gần đây đã giảm việc sử dụng bộ nhớ của decoder
cross attentions bằng cách phân phối đầu ra encoder
cho các attention head khác nhau (Huang et al.,
2021) hoặc chọn các đầu ra encoder có thể attend
thông qua tìm kiếm KNN (Bertsch et al., 2023).
Mặc dù có độ phức tạp giảm, các hệ thống dựa
trên efficient attention về cơ bản yêu cầu đọc toàn
bộ tài liệu x để tạo ra bản tóm tắt y trong quá
trình huấn luyện mô hình và do đó vẫn cần bộ
nhớ GPU khổng lồ tỷ lệ với độ dài đầu vào.

Các hệ thống Extract-then-abstract khắc phục
thách thức chuỗi dài bằng cách đầu tiên xác định
các đoạn nổi bật, xe (ví dụ: câu), sử dụng một
extractor, và sau đó chạy một abstractor trên xe
để tạo ra bản tóm tắt cuối cùng (Pilault et al.,
2020; Liu và Lapata, 2019; Zhao et al., 2020).
Tuy nhiên, các đoạn được trích xuất có thể chứa
thông tin không đầy đủ và ngoài ngữ cảnh dẫn
đến các bản tóm tắt khó hiểu và không trung thực.

Để giảm thiểu vấn đề lan truyền lỗi của phương
pháp hai giai đoạn, các nghiên cứu gần đây nối
extractor và abstractor thông qua dynamic weights
trên các đoạn tài liệu. Thay vì đưa các đoạn được
trích xuất trực tiếp vào abstractor, tại mỗi bước
giải mã tóm tắt, DYLE (Mao et al., 2022) đầu tiên
dự đoán một phân phối token đầu ra cho mỗi đoạn
riêng biệt, và sau đó tổng hợp trên tất cả các đoạn
được trích xuất với trọng số theo mức độ nổi bật
trích xuất của chúng. PageSum (Liu et al., 2022)
tiếp tục giảm bớt mất mát ngữ cảnh bằng cách lấy
trung bình các representation đầu ra decoder có
điều kiện trên tất cả các đoạn tài liệu. Mặc dù
abstractor của chúng xử lý từng đoạn tài liệu xi
riêng biệt, việc huấn luyện chung extractor và
abstractor vẫn yêu cầu tải toàn bộ tài liệu x vào
bộ nhớ GPU.

Các hệ thống Divide-and-conquer chia một tài
liệu thành nhiều đoạn không chồng lấp và tóm tắt
từng đoạn riêng biệt, như được thực hiện trong
Gidiotis và Tsoumakas (2020) và Se3 (Moro và
Ragazzi, 2022). SummN (Zhang et al., 2022) sử
dụng một giai đoạn tóm tắt bổ sung để tiếp tục
nén các bản tóm tắt được phân đoạn. Vì mỗi đoạn
tài liệu xi được tóm tắt riêng biệt, dung lượng bộ
nhớ GPU cố định của phương pháp chia để trị độc
lập với độ dài tài liệu. Điều này phù hợp với mục
tiêu của chúng tôi về tóm tắt tài liệu dài với bộ
nhớ hạn chế. Tuy nhiên, không có quyền truy cập
vào các phần khác của tài liệu và các bản tóm tắt
của chúng, bộ tóm tắt gặp khó khăn trong việc
ước tính mức độ nổi bật nội dung trong mỗi đoạn
tách biệt, và tạo ra các đầu ra không mạch lạc khi
ghép nối các bản tóm tắt lại với nhau. Mặc dù Wu
et al. (2021) nối các bản tóm tắt được tạo trước
đó như một phần của đầu vào, một chiến lược
phức tạp được yêu cầu cho việc xây dựng mẫu
huấn luyện.

AWESOME được xây dựng trên phương pháp
chia để trị tiết kiệm bộ nhớ, và cải thiện tính thông
tin, mạch lạc và trung thực của bản tóm tắt bằng
cách sử dụng các bộ nhớ ngoài được thiết kế mới
để tích lũy thông tin nổi bật từ các đoạn tài liệu
khác và các bản tóm tắt được tạo ra của chúng.
Chúng tôi tiếp tục bổ sung AWESOME với nội
dung nổi bật toàn cục để cung cấp các chủ đề
quan trọng ở cấp độ tài liệu, khi tóm tắt từng đoạn.

2.2 Cơ chế Bộ nhớ và Bổ sung Nội dung

Các cơ chế bộ nhớ khác nhau đã được nghiên cứu
cho các tác vụ hiểu văn bản tầm xa. Ví dụ,
Transformer-XL (Dai et al., 2019) cache các
representation trung gian được tạo ra trong đoạn
tài liệu cuối cùng và attend trên các representation
này. Compressive Transformer (Rae et al., 2020)
tiếp tục tăng phạm vi ngữ cảnh bằng cách nén các
representation cache cũ nhất. Để mô phỏng việc
đọc và viết bộ nhớ, Recurrent Memory Transformer
(Bulatov et al., 2022) bao gồm các vector bộ nhớ
bổ sung trong mỗi đoạn văn bản và chuyển các
vector đầu ra tương ứng của chúng sang đoạn tiếp
theo. Thay vì sử dụng bộ nhớ có kích thước cố
định, Memorizing Transformer (Wu et al., 2022a)
lưu trữ tất cả các representation trước đó dưới
dạng các cặp key-value, và thực hiện tìm kiếm
kNN xấp xỉ để truy xuất các representation nhằm
bổ sung cho đoạn hiện tại.

Tuy nhiên, công trình hiện tại về cơ chế bộ nhớ
tập trung vào mô hình hóa ngôn ngữ, trong khi
việc kết hợp cơ chế bộ nhớ vào quá trình giải mã
cho các tác vụ tạo sinh là không tầm thường vì
nó yêu cầu cập nhật cả trạng thái giải mã (ví dụ:
beams) và trạng thái bộ nhớ. Công trình của chúng
tôi là đầu tiên tận dụng cơ chế bộ nhớ và bổ sung
nội dung để kết hợp ngữ cảnh toàn cục nhằm mục
đích tóm tắt tài liệu dài tiết kiệm bộ nhớ.

3 Bộ nhớ Ngoài và Bổ sung Nội dung Nổi bật Toàn cục

Kiến trúc của AWESOME (Hình 1) dựa trên Se3
(Moro và Ragazzi, 2022), trong đó một tài liệu
được tóm tắt từng đoạn một, với bản tóm tắt cuối
cùng được thu được bằng cách nối các bản tóm
tắt kết quả. Các câu tài liệu được chia thành các
đoạn với tối đa 768 token mỗi đoạn, trong khi

--- TRANG 4 ---
[Hình 1: Minh họa về AWESOME. Các bộ nhớ encoder và decoder có thể được truy cập bất cứ lúc nào và được cập nhật sau khi đọc mỗi đoạn tài liệu và tạo ra bản tóm tắt tương ứng. Chúng tích lũy ngữ cảnh toàn cục giúp cải thiện tính thông tin và mạch lạc của bản tóm tắt (§3.1). Khi mã hóa mỗi đoạn, nội dung nổi bật toàn cục từ các đoạn khác (các đường có đầu hình ♦, từ cả quá khứ và tương lai) được cung cấp để hỗ trợ thêm cho việc ước tính mức độ nổi bật (§3.2).]

các câu tóm tắt tham chiếu được gán cho đoạn
chồng lấp nhiều nhất của chúng để tạo ra bản tóm
tắt oracle, như được chi tiết trong Phụ lục A. Theo
Longformer (Beltagy et al., 2020), chúng tôi khởi
tạo các tham số encoder và decoder từ BART
(Lewis et al., 2020). AWESOME bảo tồn ngữ cảnh
toàn cục và xây dựng giao tiếp giữa các đoạn với
việc tăng bộ nhớ GPU tối thiểu, bằng cách (1) sử
dụng bộ nhớ ngoài trong cả encoder và decoder
để thu thập thông tin có liên quan (§3.1), và (2)
bổ sung encoder với nội dung nổi bật từ các đoạn
khác (§3.2).

3.1 Cơ chế Bộ nhớ Ngoài

Chúng tôi thiết kế hai cơ chế bộ nhớ ngoài để cho
phép luồng thông tin từ các đoạn trước đó đến
đoạn hiện tại một cách hiệu quả. Cụ thể, mỗi
module bộ nhớ duy trì một ma trận M∈Rm×d,
trong đó m = 1024 là kích thước bộ nhớ và d =
1024 là chiều hidden state của BART. M được
cập nhật sau khi mã hóa mỗi đoạn tài liệu và sau
đó được chuyển đến đoạn tiếp theo. Chúng tôi ký
hiệu ma trận bộ nhớ sau đoạn thứ t là Mt. Mỗi
lớp của encoder và decoder có thể được trang bị
một bộ nhớ ngoài như vậy. Dưới đây chúng tôi
mô tả hai cơ chế để cập nhật Mt và kết hợp nó
vào cả quá trình mã hóa và giải mã. Chỉ số lớp
trong các công thức được bỏ qua để đơn giản.

Compressive Memory. Đối với mỗi đoạn tài liệu,
bộ nhớ dựa trên nén cache các vector đầu vào của
nó để được đưa vào tính toán self-attention. Vì
việc lưu trữ các vector đầu vào nguyên trạng yêu
cầu việc sử dụng bộ nhớ m tỷ lệ tuyến tính với
độ dài ngữ cảnh, chúng tôi dành một nửa Mt để
lưu trữ bộ nhớ nén, với tỷ lệ nén r. Với Ht_inp
ký hiệu ma trận chứa các vector đầu vào cho
transformer self-attention, các quá trình nén và
cập nhật bộ nhớ là:

Mt-1_c, Mt-1_u = Mt-1[:m/2], Mt-1[m/2:] (1)
M'_u = concat(Mt-1_u, SG(Ht_inp)) (2)
M'_c = compress(M'_u[:-m/2]) (3)
Mt_u = M'_u[-m/2:] (4)
Mt_c = concat(Mt-1_c, M'_c)[-m/2:] (5)
Mt = concat(Mt_c, Mt_u) (6)

trong đó SG(·) ký hiệu việc dừng lan truyền gradient
để giảm việc sử dụng GPU, và compress(·) thực
hiện các convolution với stride và kernel size được
đặt thành tỷ lệ nén r. r được đặt thành 5 sau khi
điều chỉnh trên các tập phát triển.

Tiếp theo, để tận dụng bộ nhớ từ đoạn trước trong
việc tóm tắt đoạn hiện tại, Mt-1 được nối với các
đầu vào của self-attentions để có được các ma
trận key-value:

Ht_mem = concat(Mt-1, Ht_inp) (7)
Ht_self = Attn(Ht_inp|_{query}, Ht_mem|_{key}, Ht_mem|_{value}) (8)

trong đó Ht_self là đầu ra của self-attention.

Bộ nhớ dựa trên nén của chúng tôi được áp dụng
từ Compressive Transformer (Rae et al., 2020),
một mô hình chỉ decoder cho mô hình hóa ngôn
ngữ. Chúng tôi là những người đầu tiên áp dụng
nó cho cả encoder và decoder của mô hình
Transformer và trên các tác vụ tóm tắt tài liệu dài.

Compressive memory ưu tiên tính gần đây, đặc
biệt là đoạn trước đó và bản tóm tắt của nó, có
thể khiến lịch sử có liên quan cũ hơn bị mất trong
quá trình nén.

Attentive Memory. Để giảm thiểu bias tính gần
đây bởi compressive memory, chúng tôi tiếp tục
điều tra một cơ chế cập nhật bộ nhớ dựa trên
attention, để bao gồm nội dung một cách có chọn
lọc trong Mt. Đầu tiên, bộ nhớ được đi kèm thêm
với một cross-attention bổ sung trong mỗi lớp
encoder và decoder, chuyên về việc truy xuất
thông tin có liên quan từ Mt. Theo nghiên cứu
trước đây (Lei et al., 2020) sử dụng bộ nhớ trong
video captioning, chúng tôi cập nhật Mt với một
ma trận gate Gt để kiểm soát lượng nội dung cần
được cập nhật:

Mt = Gt ⊙ Ut + (1-Gt) ⊙ Mt-1 (9)

trong đó ⊙ ký hiệu tích element-wise và Ut là
ma trận chứa các vector để cập nhật bộ nhớ. Ut
và Gt được thu được như sau:

Ut = tanh(Wu1Mt-1 + Wu2St) (10)
Gt = σ(Wg1Mt-1 + Wg2St) (11)
St = Attn(Mt-1|_{query}, SG(Ht_self)|_{key}, SG(Ht_self)|_{value}) (12)

trong đó W* là các ma trận có thể học, St tổng
hợp đoạn hiện tại thông qua một tính toán attention,
và SG(·) chỉ ra việc dừng lan truyền gradient.
Trong mỗi lớp encoder và decoder, một cross-
attention bổ sung được chèn sau self-attention,
trong đó Mt-1 được attend và kết hợp vào quá
trình tóm tắt của đoạn hiện tại.

Không giống như phương pháp của chúng tôi,
bộ nhớ trong Lei et al. (2020) không sử dụng
việc dừng gradient. Việc bỏ qua này loại bỏ hiệu
quả bộ nhớ thu được từ chiến lược chia để trị,
dẫn đến việc sử dụng bộ nhớ cao tương đương
như chiến lược efficient attention.³ Trong khi bộ
nhớ của họ phù hợp để tạo ra các caption hình
ảnh ngắn, thiết kế của chúng tôi với việc dừng
gradient rất quan trọng cho việc tóm tắt tài liệu
dài hiệu quả.

Thêm Bộ nhớ Ngoài một cách Có chọn lọc. Bộ
nhớ ngoài gây ra overhead trong việc sử dụng bộ
nhớ GPU. Để giảm thiểu overhead này, chúng tôi
xem xét việc thêm bộ nhớ ngoài một cách có chọn
lọc vào các lớp cụ thể, vì tầm quan trọng của bộ
nhớ ngoài khác nhau tùy theo các chức năng khác
nhau của các lớp trong mô hình. Nghiên cứu thí
điểm của chúng tôi cho thấy rằng các lớp cuối
của mô hình Transformer sử dụng bộ nhớ ngoài
hiệu quả hơn so với các lớp đầu. Để tránh tìm
kiếm toàn diện cho lớp tối ưu hoặc tổ hợp các
lớp cho từng tập dữ liệu, chúng tôi chọn trang bị
đồng nhất ba lớp cuối với bộ nhớ ngoài trên tất
cả các tập dữ liệu trừ khi được chỉ định khác.⁴

³Không có việc dừng gradient, mô hình không thể hoàn thành
việc huấn luyện với bộ nhớ GPU 48GB.
⁴So với việc thêm bộ nhớ ngoài vào tất cả các lớp, việc thêm
có chọn lọc giảm việc sử dụng bộ nhớ GPU khoảng 9GB.

3.2 Bổ sung Nội dung Nổi bật Toàn cục

Các cơ chế bộ nhớ chỉ cấp quyền truy cập vào nội
dung trước đó trong các tài liệu, tuy nhiên ngữ
cảnh tiếp theo cũng có thể giúp ước tính mức độ
nổi bật, ví dụ, việc trình bày ưu và nhược điểm
của một giải pháp được đề xuất khiến việc giới
thiệu vấn đề và giải pháp trở nên cần thiết. Hơn
nữa, các bộ nhớ lưu trữ nội dung một cách ngầm
định, vì vậy không rõ liệu thông tin có liên quan
có thể được lưu trữ và truy xuất hiệu quả hay
không. Do đó, chúng tôi thông báo cho hệ thống
về các câu quan trọng của một tài liệu, được xác
định trước bởi một extractor được huấn luyện riêng
biệt. Chi tiết về việc huấn luyện extractor có thể
được tìm thấy trong Phụ lục D. Sau khi trích xuất
các câu quan trọng trong một tài liệu, chúng tôi
nghiên cứu hai phương pháp để đưa chúng vào
bộ tóm tắt.

Text Concatenation. Đối với mỗi đoạn, chúng
tôi bao gồm các câu được trích xuất theo cách
sau để ưu tiên ngữ cảnh dài hạn. Chúng tôi bắt
đầu với các câu được trích xuất "ngoài cùng",
tức là câu sớm nhất trong các đoạn quá khứ và
câu cuối cùng trong các đoạn tương lai, và lặp lại
quá trình này cho đến khi đầu vào đạt được độ
dài tối đa được chấp nhận bởi positional encoding
của mô hình (1024 cho BART).⁵ Để phân biệt
nội dung trong đoạn hiện tại với các câu được
thêm vào, chúng tôi thêm tiền tố cho đoạn hiện
tại và các câu được thêm từ trước/sau đoạn hiện
tại lần lượt với " Current chunk: ", "Previous
important sentences: ", và " Next important
sentences: ".

Text concatenation dễ thực hiện và tương thích
nhất với modality nguồn, nhưng việc tăng sử
dụng bộ nhớ là bậc hai so với độ dài của nội dung
được bổ sung.

Key-value Vectors. Để khắc phục việc tăng bộ
nhớ bậc hai, chúng tôi kết hợp các representation
key-value của các token trong các câu quan trọng
trong encoder self-attentions, và đưa chúng trực
tiếp vào encoder của bộ tóm tắt. Việc tăng bộ
nhớ chỉ tuyến tính so với độ dài của nội dung
được bổ sung.

Cụ thể, encoder của bộ tóm tắt đầu tiên mã hóa
tất cả các đoạn tài liệu và thu được các representation
(tức là đầu ra encoder) của các token thuộc về
các câu quan trọng được trích xuất. Trong quá
trình huấn luyện, các representation token của
những câu này được nối với các ma trận key-value
trong encoder self-attentions trong khi ma trận
query vẫn giữ nguyên dạng ban đầu. Tối đa 1024
token được nối thông qua cùng phương pháp bao
gồm cho text concatenation, để ưu tiên các

⁵Các chiến lược bao gồm khác có thể được khám phá trong
công việc tương lai.

--- TRANG 5 ---
câu ngoài cùng. Một ý tưởng tương tự đã được
sử dụng bởi Memorizing Transformer (Wu et al.,
2022a) để bao gồm các representation văn bản
được truy xuất từ các đoạn quá khứ cho mô hình
hóa ngôn ngữ dạng dài. Phương pháp của chúng
tôi khác biệt ở hai khía cạnh. Đầu tiên, chúng tôi
trích xuất các representation từ các đoạn tương
lai, rất quan trọng để xác định chính xác nội dung
nổi bật. Thứ hai, chúng tôi áp dụng một phép
chiếu có thể học vào các representation được bổ
sung trước khi nối key-value. Quá trình này rất
quan trọng trong việc cải thiện khả năng tương
thích với các ma trận key-value ban đầu.

4 Thiết lập Thí nghiệm

Tập dữ liệu. Chúng tôi tiến hành thí nghiệm trên
GovReport (Huang et al., 2021), QMSum (Zhong
et al., 2021), SummScreen (Chen et al., 2022),
arXiv (Cohan et al., 2018), và BookSum (Kryscinski
et al., 2022). Độ dài đầu vào trung bình của các
tập dữ liệu này dao động từ 6K đến 143K (Phụ
lục C.1).

Thiết lập Thí nghiệm và So sánh. Các thí nghiệm
chính của chúng tôi được tiến hành với ràng buộc
bộ nhớ GPU là 27GB. Đối với mỗi mô hình, chúng
tôi cắt ngắn đầu vào sao cho việc sử dụng bộ nhớ
GPU tối đa của nó trong quá trình huấn luyện không
vượt quá ràng buộc khi gradient checkpointing
(Chen et al., 2016) bị vô hiệu hóa. Ràng buộc
được chọn cụ thể sao cho các baseline hoạt động
hợp lý. Phụ lục C.2 cung cấp thông tin về số lượng
token đầu vào tối đa có thể tuân thủ ràng buộc
cho các mô hình khác.

Đối với các baseline, ngoài mô hình chia để trị
Se3 (Moro và Ragazzi, 2022), chúng tôi so sánh
với các hệ thống tóm tắt tài liệu dài state-of-the-art
hoặc phổ biến bao gồm BlockAttn (Phang et al.,
2022), Longformer (Beltagy et al., 2020), LongT5
(Guo et al., 2022), và Unlimiformer (Bertsch et
al., 2023). Chúng tôi cũng bao gồm một mô hình
extract-then-abstract (Extract-Abstract) và PageSum
(Liu et al., 2022) tận dụng dynamic weights, như
đã thảo luận trong §2. Tất cả các mô hình được
khởi tạo từ BART-large, ngoại trừ LongT5 được
pre-train trên dữ liệu dạng dài. Chi tiết về các
mô hình baseline được báo cáo trong Phụ lục D.

Thước đo Đánh giá. Chúng tôi đánh giá tính thông
tin của bản tóm tắt sử dụng ROUGE (Lin, 2004).
Để đo tính mạch lạc, chúng tôi sử dụng DiscoScore
(Zhao et al., 2022) (Disco), một thước đo dựa
trên tham chiếu đánh giá tính mạch lạc diễn ngôn
bằng cách so sánh tần suất và ngữ nghĩa focus
(ví dụ: danh từ) giữa bản tóm tắt hệ thống và tham
chiếu. Chúng tôi cũng báo cáo một thước đo mạch
lạc không cần tham chiếu dựa trên đồ thị
(Guinaudeau và Strube, 2013) (Ent Graph), đo
mức độ kết nối của các câu tóm tắt được liên kết
bởi các thực thể, phản ánh tính mạch lạc của các
chuyển tiếp chủ đề. Đối với tính trung thực của
bản tóm tắt, chúng tôi theo công trình trước đây
về sinh văn bản (Iv et al., 2022) và hiển thị độ
chính xác của các thực thể (Ent Prec) trong bản
tóm tắt đối với tài liệu. Ngoài ra, một thước đo
trung thực dựa trên mô hình gần đây, SummaC
(Laban et al., 2022), được sử dụng.

Cuối cùng, chúng tôi hiển thị kích thước tối đa
của bộ nhớ GPU được phân bổ bởi mỗi mô hình
trong quá trình huấn luyện.

--- TRANG 6 ---
[THIS IS TABLE: Bảng 2 showing results on GovReport with various models and metrics including R-1, R-2, R-L, Ent Prec, SummaC, Disco, Ent Graph, and GPU Mem]

Se3: V A được yêu cầu xuất bản thông tin về thời
gian chờ đợi cuộc hẹn tại mỗi cơ sở y tế V A cho
chăm sóc chính, chăm sóc chuyên khoa, và chăm
sóc bệnh viện và dịch vụ y tế, điều mà nó thực
hiện thông qua hai trang web công cộng. V A đã
thực hiện một số hành động để giải quyết các thiếu
sót mà GAO tìm thấy trong việc đo thời gian chờ
đợi và thực hiện chính sách lập lịch của mình.
Đối với việc đo thời gian chờ đợi, những hành
động này bao gồm các thay đổi đối với định nghĩa
đo thời gian chờ đợi, cung cấp và ghi chép đào
tạo người lập lịch, và cải thiện giám sát thông qua
kiểm toán, tất cả đều trong tình trạng biến động
trong 6 năm qua. Vào ngày 12 tháng 7 năm 2019,
V A đã cung cấp cho GAO các cập nhật bổ sung
về nỗ lực thực hiện các khuyến nghị liên quan
của GAO.

AWESOME: GAO khuyến nghị rằng V A nên làm
rõ chính sách lập lịch của mình để xác định tốt
hơn ngày mong muốn, hoặc xác định các thước
đo thời gian chờ đợi rõ ràng hơn không phải chịu
sự diễn giải và dễ bị lỗi người lập lịch. V A đồng
ý với khuyến nghị, mà GAO đã xác định là một
trong những khuyến nghị đáng được ưu tiên chú
ý. V A đã thực hiện một số hành động để giải
quyết các khuyến nghị của GAO về các thiếu sót
mà GAO tìm thấy trong việc đo thời gian chờ đợi
và thực hiện chính sách lập lịch của mình. Đối
với việc đo thời gian chờ đợi, những hành động
này bao gồm các thay đổi đối với định nghĩa đo
thời gian chờ đợi, cung cấp và ghi chép đào tạo
người lập lịch, và cải thiện giám sát thông qua
kiểm toán, tất cả đều trong tình trạng biến động
trong 6 năm qua. Vào ngày 12 tháng 7 năm 2019,
V A đã cung cấp cho GAO các cập nhật bổ sung
về nỗ lực thực hiện các khuyến nghị liên quan
của GAO.

Bảng 3: Các đoạn tóm tắt được tạo ra bởi Se3 và
AWESOME. Bản tóm tắt của AWESOME mạch lạc
hơn, với các chuyển tiếp tự nhiên xung quanh "khuyến
nghị của GAO", trong khi Se3 đột ngột giới thiệu chủ đề.

5 Kết quả

Chúng tôi báo cáo kết quả của tất cả các biến thể
AWESOME và các mô hình so sánh trên GovReport
trong Bảng 2. So với Se3, các biến thể AWESOME
đạt được hiệu suất tốt hơn một cách nhất quán
trên cả điểm ROUGE và điểm mạch lạc, cho thấy
tầm quan trọng của việc duy trì ngữ cảnh toàn
cục để ước tính chính xác mức độ nổi bật của nội
dung cục bộ và tăng cường các chuyển tiếp mạch
lạc giữa các bản tóm tắt cấp độ đoạn. Điều này
cũng có thể được chứng minh bằng các đầu ra
mẫu trong Bảng 3. Các bản tóm tắt được tạo ra
bởi Se3 có xu hướng ngắn hơn, vì Se3 không thể
lập kế hoạch ở cấp độ toàn cục. Về tính trung
thực, AWESOME với attentive memory có độ
chính xác thực thể tốt nhất trong tất cả các mô
hình và cũng cải thiện SummaC so với Se3, trong
khi chỉ bổ sung AWESOME với nội dung nổi bật
toàn cục làm tổn hại tính trung thực. Khi kiểm tra
các đầu ra mô hình, chúng tôi thấy rằng việc sử
dụng attentive memory cải thiện việc hiểu các
khái niệm của phụ thuộc dài hạn, ví dụ, kết nối
một chiến lược với thông tin liên quan xuất hiện
sớm hơn trong báo cáo.

Trong hai loại cơ chế bộ nhớ ngoài, attentive
memory vượt trội compressive

[THIS IS TABLE: Bảng 4 showing results on meeting transcripts in QMSum with various models and metrics]

memory trên tất cả các thước đo, điều này làm
nổi bật lợi thế của việc cập nhật ngữ cảnh được
lưu trữ một cách thích ứng. Trong khi đó, việc
nối trực tiếp nội dung nổi bật với đầu vào mang
lại điểm ROUGE cao hơn so với việc đưa key-
value vectors vào tính toán attention, mặc dù cách
sau ít tốn bộ nhớ hơn. Chúng tôi tin rằng việc
bổ sung dựa trên ngôn ngữ tự nhiên tương tác
tốt hơn với đoạn tài liệu, phản hồi các phát hiện
của công trình trước đây về việc sử dụng truy
xuất cho trả lời câu hỏi (Wu et al., 2022b).

Quan trọng là, dưới ràng buộc bộ nhớ GPU nghiêm
ngặt, AWESOME với cơ chế bộ nhớ ngoài và bổ
sung nội dung nổi bật toàn cục đạt được điểm
ROUGE tốt nhất trong tất cả các mô hình, trong
khi đạt được kết quả cạnh tranh trên các thước
đo khác. Mặc dù các mô hình efficient attention
và PageSum có thể hoạt động xuất sắc khi được
cung cấp GPU dung lượng cao hơn như trong
công trình gốc, chúng tạo ra các bản tóm tắt ít
thông tin hơn khi cần cắt ngắn để tuân thủ ràng
buộc bộ nhớ, nhấn mạnh tầm quan trọng của việc
nghiên cứu các mô hình tóm tắt tài liệu dài tiết
kiệm bộ nhớ.

Hơn nữa, với việc thêm bộ nhớ ngoài có chọn
lọc, AWESOME chỉ thêm khoảng 4GB việc sử
dụng bộ nhớ GPU, tăng cường hiệu suất mô hình
một cách hiệu quả.

Trên QMSum (Bảng 4), AWESOME với attention-
based memory vượt trội tất cả các so sánh về
điểm ROUGE. Trong khi các bản tóm tắt của mô
hình chúng tôi mạch lạc hơn so với các bản tóm
tắt của Se3, được đo bằng DiscoScore, sự khác
biệt giữa tất cả các mô hình ít rõ ràng hơn so với
những gì trên GovReport. Điều này là do QMSum
chứa các bản tóm tắt ngắn hơn GovReport

--- TRANG 7 ---
[THIS IS TABLE: Bảng 5 showing results on TV transcripts in SummScreen with columns for Model, R-1, R-2, R-L, Ent G, GPU]

[THIS IS TABLE: Bảng 6 showing results on arXiv papers with columns for Model, R-1, R-2, R-L, Disco, GPU]

(69 so với 553), do đó liên quan đến ít chuyển
tiếp chủ đề hơn. Chúng tôi cũng thấy rằng extractor
hoạt động kém trên QMSum, dẫn đến kết quả
giảm sút sau khi bổ sung mô hình của chúng tôi
với nội dung nổi bật được trích xuất. Cụ thể, điểm
F1 của extractor trên tập test chỉ là 1.29, so với
27.85 trên GovReport. So với mô hình của chúng
tôi, mô hình extract-then-abstract dễ bị ảnh hưởng
hơn bởi các lỗi của nó và tạo ra các bản tóm tắt
có chất lượng thấp nhất trên QMSum.

Xu hướng này cũng được quan sát tương tự trên
SummScreen (Bảng 5) và arXiv (Bảng 6), trong
đó phương pháp extract-then-abstract hoạt động
kém và việc thêm nội dung được trích xuất dẫn
đến sự giảm hiệu suất của AWESOME do hiệu
suất thấp của extractor. Trong khi đó, AWESOME
với attentive memory có thể đạt được điểm ROUGE-1
và ROUGE-L tốt nhất. Trên arXiv, các mô hình
sử dụng efficient attentions đạt được điểm ROUGE
cao hơn, vì việc cắt ngắn các tài liệu arXiv có ít
ảnh hưởng đến việc tạo bản tóm tắt—các bài viết
arXiv có phân phối nội dung nổi bật không đều
nhất, trong đó chỉ khoảng 10% bigram nổi bật
mới nằm ở nửa sau của các tài liệu (Huang et al.,
2021).

Cuối cùng, các thí nghiệm trên BookSum cho thấy
phương pháp chia để trị tạo ra các bản tóm tắt
tốt hơn cho các tiểu thuyết dài, trong khi phương
pháp của chúng tôi có thể tiếp tục thúc đẩy hiệu
suất của nó (Bảng 7). Tuy nhiên, chúng tôi thấy
cần thiết phải kết hợp bộ nhớ ngoài vào tất cả
các lớp, gợi ý một tương tác phức tạp hơn của
bộ nhớ ngoài với quá trình tóm tắt cho cốt truyện
tiểu thuyết. Không giống như các loại tài liệu
khác được thử nghiệm, cốt truyện tiểu thuyết
thường tuần tự với ít dư thừa hơn, điều này làm
giảm tính cần thiết của cơ chế bộ nhớ.

[THIS IS TABLE: Bảng 7 showing results on novels in BookSum with columns for Model, R-1, R-2, R-L, Disco, GPU]

6 Kết luận

Chúng tôi trình bày AWESOME để tóm tắt các
tài liệu dài trong bối cảnh bộ nhớ hạn chế. Dựa
trên chiến lược chia để trị, AWESOME sử dụng
hai cơ chế để thu thập ngữ cảnh toàn cục và cải
thiện chất lượng bản tóm tắt. Đầu tiên, các bộ
nhớ ngoài trên encoder và decoder được sử dụng
để theo dõi nội dung tài liệu đã đọc trước đó và
các bản tóm tắt tương ứng. Thứ hai, encoder được
thông báo về nội dung nổi bật toàn cục được dự
đoán bởi một extractor thông qua text hoặc representation
concatenation. Trên năm tập dữ liệu tóm tắt,
AWESOME tạo ra các bản tóm tắt với tính thông
tin, trung thực và mạch lạc tốt hơn so với hệ
thống baseline chia để trị. Dưới cùng ràng buộc
bộ nhớ, AWESOME vượt trội các mô hình cạnh
tranh tận dụng efficient attentions hoặc dynamic
extraction để bảo tồn ngữ cảnh toàn cục, làm nổi
bật hiệu quả của nó trong việc cung cấp ngữ cảnh
toàn cục.

--- TRANG 8 ---
7 Hạn chế

Cơ chế bộ nhớ ngoài của AWESOME bị hạn chế
chỉ hoạt động từ các đoạn quá khứ đến đoạn hiện
tại. Điều này có nghĩa là mô hình không tận dụng
thông tin chứa trong các đoạn tương lai, có thể
có liên quan để hiểu toàn diện đoạn hiện tại. Để
giải quyết hạn chế này, chúng tôi đã thiết kế cơ
chế bổ sung nội dung nổi bật toàn cục để bao
phủ ngữ cảnh từ các đoạn tương lai, tuy nhiên
các giải pháp tiến bộ hơn có thể được khám phá
trong công việc tương lai. Ví dụ, trên encoder,
việc làm cho bộ nhớ ngoài hai chiều là một phương
pháp tiềm năng.

Mặc dù tiết kiệm bộ nhớ, cơ chế bộ nhớ ngoài
của AWESOME đòi hỏi thời gian chạy dài hơn
do bản chất tái phát của nó. Nhu cầu tính toán
tái phát có thể dẫn đến yêu cầu xử lý tăng lên,
có thể ảnh hưởng đến các ứng dụng thời gian
thực hoặc các tình huống mà phản hồi nhanh là
quan trọng. Thời gian chạy của các mô hình khác
nhau được cung cấp trong Phụ lục B.1 để tham
khảo. Mặc dù mô hình của chúng tôi chậm hơn
so với LongT5 và Se3, nó vẫn vượt trội một số
mô hình cạnh tranh khác về tốc độ, và chúng tôi
sẽ điều tra các phương pháp để giảm thời gian
chạy trong công việc tương lai.

8 Xem xét Đạo đức

Chúng tôi dự đoán rằng một trong những trường
hợp sử dụng chính của AWESOME là cho phép
người dùng thường có thiết bị tính toán với bộ
nhớ hạn chế hiểu nhanh chóng các chính sách
chính phủ và các loại tài liệu dài khác. Tuy nhiên,
chúng tôi nhận ra rằng các bản tóm tắt do hệ
thống tạo ra có thể không bao phủ toàn diện nội
dung nổi bật cần thiết để hiểu đúng các chính
sách, gây ra rủi ro từ mất vốn đến trách nhiệm
pháp lý. Hơn nữa, các bản tóm tắt hệ thống có
thể chứa các tuyên bố không thể được xác minh
thông qua tài liệu, điều này tiếp tục thêm vào rủi
ro của việc triển khai thực tế. Chúng tôi khuyên
các nhà phát triển có ý định sử dụng mô hình của
chúng tôi cho ứng dụng thực tế nên nghiên cứu
cẩn thận các đầu ra của mô hình chúng tôi trước
khi triển khai thực tế.

Tài liệu tham khảo

Iz Beltagy, Matthew E Peters, và Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150.

Amanda Bertsch, Uri Alon, Graham Neubig, và
Matthew R. Gormley. 2023. Unlimiformer: Long-
range transformers with unlimited length input.

Aydar Bulatov, Yuri Kuratov, và Mikhail Burtsev. 2022.
Recurrent memory transformer. Trong Advances in
Neural Information Processing Systems.

Mingda Chen, Zewei Chu, Sam Wiseman, và Kevin
Gimpel. 2022. SummScreen: A dataset for abstractive screenplay summarization. Trong Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
trang 8602–8615, Dublin, Ireland. Association for
Computational Linguistics.

Tianqi Chen, Bing Xu, Chiyuan Zhang, và Carlos
Guestrin. 2016. Training deep nets with sublinear
memory cost. arXiv preprint arXiv:1604.06174.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
Trung Bui, Seokhwan Kim, Walter Chang, và Nazli
Goharian. 2018. A discourse-aware attention model
for abstractive summarization of long documents.
Trong Proceedings of the 2018 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers), trang 615–
621, New Orleans, Louisiana. Association for
Computational Linguistics.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,
Quoc Le, và Ruslan Salakhutdinov. 2019. Transformer-
XL: Attentive language models beyond a fixed-
length context. Trong Proceedings of the 57th Annual
Meeting of the Association for Computational
Linguistics, trang 2978–2988, Florence, Italy.
Association for Computational Linguistics.

Alexios Gidiotis và Grigorios Tsoumakas. 2020. A
divide-and-conquer approach to the summarization
of long documents. IEEE/ACM Transactions on
Audio, Speech, and Language Processing, 28:3029–
3040.

Camille Guinaudeau và Michael Strube. 2013. Graph-
based local coherence modeling. Trong Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
trang 93–103, Sofia, Bulgaria. Association for
Computational Linguistics.

Mandy Guo, Joshua Ainslie, David Uthus, Santiago
Ontanon, Jianmo Ni, Yun-Hsuan Sung, và Yinfei
Yang. 2022. LongT5: Efficient text-to-text transformer
for long sequences. Trong Findings of the Association
for Computational Linguistics: NAACL 2022, trang
724–736, Seattle, United States. Association for
Computational Linguistics.

Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng
Ji, và Lu Wang. 2021. Efficient attentions for long
document summarization. Trong Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, trang 1419–1436,
Online. Association for Computational Linguistics.

--- TRANG 9 ---
Robert Iv, Alexandre Passos, Sameer Singh, và Ming-
Wei Chang. 2022. FRUIT: Faithfully reflecting
updated information in text. Trong Proceedings of
the 2022 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, trang 3670–3686,
Seattle, United States. Association for Computational
Linguistics.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya.
2020. Reformer: The efficient transformer. Trong
International Conference on Learning Representations.

Wojciech Kryscinski, Nazneen Rajani, Divyansh
Agarwal, Caiming Xiong, và Dragomir Radev. 2022.
BOOKSUM: A collection of datasets for long-form
narrative summarization. Trong Findings of the
Association for Computational Linguistics: EMNLP
2022, trang 6536–6558, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.

Philippe Laban, Tobias Schnabel, Paul N. Bennett, và
Marti A. Hearst. 2022. SummaC: Re-visiting NLI-
based models for inconsistency detection in
summarization. Transactions of the Association for
Computational Linguistics, 10:163–177.

Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara
Berg, và Mohit Bansal. 2020. MART: Memory-
augmented recurrent transformer for coherent video
paragraph captioning. Trong Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics, trang 2603–2614, Online. Association
for Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, và Luke Zettlemoyer. 2020. BART:
Denoising sequence-to-sequence pre-training for
natural language generation, translation, and
comprehension. Trong Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics, trang 7871–7880, Online. Association
for Computational Linguistics.

Chuan Li. 2022. Best gpu for deep learning in 2022
(so far).

Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. Trong Text Summarization
Branches Out, trang 74–81, Barcelona, Spain.
Association for Computational Linguistics.

Yang Liu và Mirella Lapata. 2019. Hierarchical
transformers for multi-document summarization.
Trong Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics, trang
5070–5081, Florence, Italy. Association for
Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, và Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692.

Yixin Liu, Ansong Ni, Linyong Nan, Budhaditya Deb,
Chenguang Zhu, Ahmed H. Awadallah, và Dragomir
Radev. 2022. Leveraging locality in abstractive text
summarization.

Ziming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang,
Rui Zhang, Tao Yu, Budhaditya Deb, Chenguang
Zhu, Ahmed Awadallah, và Dragomir Radev. 2022.
DYLE: Dynamic latent extraction for abstractive
long-input summarization. Trong Proceedings of the
60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
trang 1687–1698, Dublin, Ireland. Association for
Computational Linguistics.

Gianluca Moro và Luca Ragazzi. 2022. Semantic self-
segmentation for abstractive summarization of long
documents in low-resource regimes. Proceedings of
the AAAI Conference on Artificial Intelligence,
36(10):11085–11093.

OpenAI. 2023. Gpt-4 technical report.

Jason Phang, Yao Zhao, và Peter J Liu. 2022.
Investigating efficiently extending transformers for
long input summarization. arXiv preprint
arXiv:2208.04347.

Jonathan Pilault, Raymond Li, Sandeep Subramanian,
và Chris Pal. 2020. On extractive and abstractive
neural document summarization with transformer
language models. Trong Proceedings of the 2020
Conference on Empirical Methods in Natural
Language Processing (EMNLP), trang 9308–9319,
Online. Association for Computational Linguistics.

Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,
Chloe Hillier, và Timothy P. Lillicrap. 2020.
Compressive transformers for long-range sequence
modelling. Trong International Conference on
Learning Representations.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, và Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.

Nils Reimers và Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
Trong Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing.
Association for Computational Linguistics.

Aurko Roy, Mohammad Saffar, Ashish Vaswani, và
David Grangier. 2021. Efficient content-based sparse
attention with routing transformers. Transactions of
the Association for Computational Linguistics, 9:53–
68.

Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, và
Da-Cheng Juan. 2020. Sparse sinkhorn attention.
Trong International Conference on Machine Learning,
trang 9438–9447. PMLR.

--- TRANG 10 ---
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, và Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems, 30.

Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan
Stiennon, Ryan Lowe, Jan Leike, và Paul Christiano.
2021. Recursively summarizing books with human
feedback. arXiv preprint arXiv:2109.10862.

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,
và Christian Szegedy. 2022a. Memorizing transformers.
Trong International Conference on Learning
Representations.

Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Minervini,
Pontus Stenetorp, và Sebastian Riedel. 2022b. An
efficient memory-augmented transformer for
knowledge-intensive nlp tasks.

Manzil Zaheer, Guru Guruganesh, Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
và Amr Ahmed. 2021. Big bird: Transformers for
longer sequences.

Yusen Zhang, Ansong Ni, Ziming Mao, Chen Henry
Wu, Chenguang Zhu, Budhaditya Deb, Ahmed
Awadallah, Dragomir Radev, và Rui Zhang. 2022.
Summn: A multi-stage summarization framework
for long input dialogues and documents. Trong
Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), trang 1592–1604, Dublin, Ireland.
Association for Computational Linguistics.

Wei Zhao, Michael Strube, và Steffen Eger. 2022.
Discoscore: Evaluating text generation with bert
and discourse coherence.

Yao Zhao, Mohammad Saleh, và Peter J Liu. 2020.
Seal: Segment-wise extractive-abstractive long-form
text summarization. arXiv preprint arXiv:2006.10213.

Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia
Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli
Celikyilmaz, Yang Liu, Xipeng Qiu, và Dragomir
Radev. 2021. QMSum: A new benchmark for query-
based multi-domain meeting summarization. Trong
Proceedings of the 2021 Conference of the North
American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, trang 5905–5921, Online. Association
for Computational Linguistics.

A Kiến trúc Chia để Trị

Chúng tôi chọn Se3 (Moro và Ragazzi, 2022) làm
kiến trúc chia để trị cơ sở vì nó có thể được áp
dụng cho bất kỳ cặp tài liệu-tóm tắt nào. Để tạo
ra dữ liệu huấn luyện chia để trị cho tóm tắt, đối
với mỗi cặp tài liệu-tóm tắt, tài liệu đầu tiên được
chia thành các đoạn (§A.1) và sau đó mỗi câu
tóm tắt được gán cho một đoạn tài liệu như một
phần của mục tiêu tạo ra (§A.2).

A.1 Phân đoạn Tài liệu

[Thuật toán 1: Phân đoạn Tài liệu - Dữ liệu: Tài
liệu đầu vào doc; Độ dài min, max đoạn lmin,
lmax... (tiếp tục với cùng cấu trúc thuật toán)]

Độ dài của mỗi đoạn tài liệu nằm giữa 512 và
768 token. Trong quá trình phân đoạn, thuật toán
lặp qua tất cả các câu tài liệu, như được hiển thị
trong Thuật toán 1. Một câu tài liệu sẽ được thêm
vào đoạn hiện tại nếu đoạn đó chứa ít hơn 512
token. Đoạn hiện tại sẽ được hoàn thiện nếu đoạn
hiện tại chứa hơn 768 token hoặc câu hiện tại
tương tự về mặt ngữ nghĩa với đoạn giả tiếp theo
hơn so với đoạn hiện tại, trong đó đoạn giả tiếp
theo được tạo bằng cách bao gồm các câu tương
lai cho đến khi đạt 512 token. Để đo độ tương tự
giữa câu hiện tại và một đoạn, chúng tôi sử dụng
độ tương tự cosine trung bình giữa representation
của câu hiện tại và các representation của các câu
trong đoạn. Các representation câu được thu được
sử dụng Sentence Transformer (Reimers và
Gurevych, 2019) với mô hình all-roberta-large-v1.

A.2 Gán Mục tiêu

Đối với mỗi câu trong tóm tắt tham chiếu, chúng
tôi tính điểm ROUGE của nó với các đoạn tài liệu.
Câu sau đó sẽ được gán cho đoạn tài liệu mà với
đó mang lại điểm ROUGE-1 và ROUGE-2 cao nhất.

B Kết quả Bổ sung

B.1 Thời gian Chạy

Chúng tôi so sánh thời gian chạy mô hình trên
GovReport (Hình 2). Tài liệu đầu vào được cắt
ngắn xuống 16384 token và mỗi mô hình được
huấn luyện riêng biệt trong 1000 bước với batch
size là 1. Không có chương trình tính toán nặng
nào khác đang chạy cùng lúc. Trong khi AWESOME
mất nhiều thời gian hơn để hoàn thành huấn luyện
so với Se3, nó vẫn là mô hình nhanh thứ ba.

[Hình 2: Thời gian chạy (batch trên giây) của mỗi
mô hình. Số lượng batch được xử lý cao hơn mỗi
giây cho thấy tốc độ chạy nhanh hơn. Tất cả các
mô hình sử dụng batch size 1 và đầu vào được
cắt ngắn xuống 16384 token.]

C Chi tiết Tập dữ liệu

C.1 Thống kê

Chúng tôi tiến hành thí nghiệm trên năm tập dữ
liệu tóm tắt tài liệu dài với các thể loại đa dạng.
GovReport (Huang et al., 2021) chứa các báo cáo
dài và bản tóm tắt của chúng được viết bởi các
cơ quan nghiên cứu chính phủ. QMSum (Zhong
et al., 2021) là một tập dữ liệu tóm tắt bản ghi
cuộc họp dài tập trung vào truy vấn, với nội dung
đáng tóm tắt trải rộng trên các tài liệu. Chúng tôi
thêm truy vấn vào đầu tất cả các đoạn. Chúng tôi
tiếp tục sử dụng một tập dữ liệu tóm tắt kịch bản,
SummScreen (Chen et al.,

[THIS IS TABLE: Bảng 8 showing statistics of datasets used in experiments with columns for Dataset, # Samples (Train/Dev/Test), # Word (Doc/Summ)]

[THIS IS TABLE: Bảng 9 showing truncation thresholds used by each model on different datasets to comply with memory constraint during training]

2022), chứa bản ghi của các loạt TV. Tập con TMS,
với nhiều mẫu hơn và bản tóm tắt dài hơn, được
chọn. Hơn nữa, chúng tôi thí nghiệm với các bài
báo khoa học và tóm tắt của chúng từ arXiv (Cohan
et al., 2018). Cuối cùng, chúng tôi thử nghiệm
các mô hình của chúng tôi trên việc tóm tắt toàn
bộ tiểu thuyết trong BookSum (Kryscinski et al.,
2022). Đối với tất cả các tập dữ liệu, chúng tôi sử
dụng các phân chia train/dev/test chính thức nếu
các tệp dữ liệu gốc của chúng được phát hành.

Thống kê của các tập dữ liệu được báo cáo trong
Bảng 8. Đối với GovReport⁶, QMSum⁷, và
SummScreen (Chen et al., 2022), chúng tôi sử dụng
dữ liệu được phát hành bởi các bài báo gốc. Đối
với arXiv, chúng tôi sử dụng phiên bản được cung
cấp bởi Huggingface Datasets.⁸ Vì các tệp dữ liệu
gốc cho BookSum không được phát hành do bản
quyền tóm tắt, chúng tôi sử dụng phiên bản được
tái tạo bởi Unlimiformer (Bertsch et al., 2023).

C.2 Cắt ngắn Đầu vào

Trong các thí nghiệm chính của chúng tôi, chúng
tôi sử dụng ràng buộc bộ nhớ GPU là 27GB. Vì
một số mô hình baseline yêu cầu độ dài đầu vào
phải là bội số của 1024, việc đặt ràng buộc 24GB,
một con số phổ biến hơn, sẽ dẫn đến việc cắt ngắn
thêm và giảm hiệu suất đáng kể.

Để phù hợp với ràng buộc bộ nhớ của chúng tôi,
chúng tôi cắt ngắn đầu vào mô hình. Các ngưỡng
cắt ngắn được sử dụng bởi mỗi mô hình trên các
tập dữ liệu khác nhau được hiển thị trong Bảng 9.
Mặc dù Se3 và AWESOME về mặt lý thuyết duy
trì việc tiêu thụ bộ nhớ GPU nhất quán trong quá
trình huấn luyện bất kể số lượng token đầu vào
được xử lý, chúng tôi đã chọn hạn chế số lượng
token đầu vào tối đa trong một mẫu huấn luyện
xuống 51200 để có thời gian huấn luyện hợp lý.

D Chi tiết Triển khai

Baseline. BlockAttn và Longformer sử dụng block-
wise attentions (Phang et al., 2022) và sliding-
window attentions (Beltagy et al., 2020), trong đó
một token toàn cục có thể attend và được attend
bởi tất cả các token, trong khi các token khác chỉ
có thể attend các token trong cùng block hoặc
window. LongT5 (Guo et al., 2022) là một mô
hình sliding-window attention được pre-train trên
các chuỗi dài, và Unlimiformer (Bertsch et al.,
2023) mở rộng BART bằng cách chọn các token
đầu vào để được attend thông qua tìm kiếm KNN.
Đối với phương pháp extract-then-abstract, chúng
tôi sử dụng cùng extractor như trong việc bổ sung
nội dung nổi bật toàn cục của mô hình chúng tôi,
và abstractor lấy các câu được trích xuất oracle
làm đầu vào trong quá trình huấn luyện. Cuối
cùng, PageSum (Liu et al., 2022) tổng hợp các
representation đầu ra được cung cấp bởi các đoạn
tài liệu khác nhau với dynamic weights.

Extractor. Extractor đầu tiên sử dụng RoBERTa
(Liu et al., 2019) để mã hóa mỗi câu và lấy trung
bình của đầu ra lớp cuối làm representation câu.
Sau đó nó áp dụng self-attention trên tất cả các
representation câu. Các representation kết quả
được chuyển đổi thành điểm trích xuất sau khi áp
dụng multi-layer perception với một hidden layer.
Extractor được huấn luyện với các nhãn trích xuất
oracle được xây dựng bằng cách tìm kiếm tham
lam các câu tài liệu tối đa hóa tổng điểm ROUGE-1
và ROUGE-2, so với tóm tắt tham chiếu. Chúng
tôi không tính ROUGE-L như trong DYLE (Mao
et al., 2022), vì việc tìm subsequence chung dài
nhất tốn kém về mặt tính toán và không mang lại
lợi ích hiệu suất.

Tham số Huấn luyện. Chúng tôi huấn luyện tất
cả các mô hình với learning rate tối đa 5×10⁻⁵,
ngoại trừ LongT5 được huấn luyện với learning
rate tối đa 1×10⁻⁴. Chúng tôi sử dụng batch size
chạy là 1 và áp dụng gradient accumulation để
đạt được batch size hiệu quả là 8. Số epoch huấn
luyện là 3, 9, 6, 2, 10 trên GovReport, QMSum,
SummScreen, arXiv, và BookSum, với warmup
steps lần lượt là 300, 100, 300, 1000, và 40. Do
chi phí tính toán của việc huấn luyện tóm tắt tài
liệu dài, mỗi mô hình được huấn luyện một lần.

Kích thước Mô hình. AWESOME dựa trên BART-
large⁹ và có 708 triệu tham số.

Cơ sở hạ tầng Tính toán. Tất cả các thí nghiệm
được tiến hành trên GPU RTX A6000.

Thước đo Đánh giá. Đối với ROUGE (Lin, 2004),
chúng tôi sử dụng triển khai Python của Google.¹⁰
Mã chính thức cho DiscoScore (Zhao et al., 2022)
được sử dụng¹¹, cũng cung cấp triển khai của
thước đo Ent Graph (Guinaudeau và Strube, 2013).
Chúng tôi tự triển khai thước đo độ chính xác thực
thể và chạy mã chính thức cho SummaC (Laban
et al., 2022).¹² Tất cả các thước đo được sử dụng
đều là mã nguồn mở và có thể được phân phối
cho mục đích nghiên cứu.

⁶https://gov-report-data.github.io/
⁷https://github.com/Yale-LILY/QMSum
⁸https://huggingface.co/datasets/scientific_papers
⁹https://huggingface.co/facebook/bart-large
¹⁰https://pypi.org/project/rouge-score/
¹¹https://github.com/AIPHES/DiscoScore
¹²https://github.com/tingofurro/summac

--- TRANG 11 ---
--- TRANG 12 ---
