# 2311.02382.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2311.02382.pdf
# Kích thước tệp: 1521750 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TRANSFORMER PHÂN TÁN TRÌNH TỰ CỰC DĀI
Xiao Wang1Isaac Lyngaas1Aristeidis Tsaris1Peng Chen2Sajal Dash1Mayanka Chandra Shekar1
Tao Luo3Hong-Jun Yoon1Mohamed Wahib4John Gounley1

TÓM TẮT
Các mô hình Transformer được huấn luyện trên trình tự dài thường đạt độ chính xác cao hơn so với trình tự ngắn. Thật không may, các transformer thông thường gặp khó khăn với việc huấn luyện trình tự dài do yêu cầu tính toán và bộ nhớ quá lớn. Các phương pháp hiện có cho việc huấn luyện trình tự dài chỉ cung cấp tăng tốc và giảm bộ nhớ hạn chế, và có thể làm giảm độ chính xác. Bài báo này trình bày một phương pháp huấn luyện phân tán mới và hiệu quả, Long Short-Sequence Transformer (LSS Transformer), để huấn luyện transformer với trình tự dài. Nó phân phối một trình tự dài thành các đoạn giữa các GPU, với mỗi GPU tính toán một phần self-attention cho đoạn của nó. Sau đó, nó sử dụng một kỹ thuật truyền thông hợp nhất và một kỹ thuật trung bình gradient kép mới để tránh việc cần tổng hợp phần self-attention và giảm thiểu chi phí truyền thông. Chúng tôi đánh giá hiệu suất giữa LSS Transformer và song song trình tự Nvidia tiên tiến nhất trên bộ dữ liệu Wikipedia enwik8. Kết quả cho thấy phương pháp đề xuất của chúng tôi dẫn đến việc triển khai nhanh hơn 5.6 lần và tiết kiệm bộ nhớ hơn 10.2 lần so với song song trình tự tiên tiến nhất trên 144 GPU Nvidia V100. Hơn nữa, thuật toán của chúng tôi mở rộng tới chiều dài trình tự cực kỳ lớn là 50,112 tại 3,456 GPU, đạt được hiệu quả song song siêu tuyến tính 161% và thông lượng 32 petaflops.

1 GIỚI THIỆU
Transformer là một kiến trúc mạng nơ-ron mạnh mẽ được sử dụng rộng rãi trong xử lý ngôn ngữ tự nhiên và hình ảnh (Vaswani et al., 2017). Tính linh hoạt của nó được chứng minh bằng phạm vi ứng dụng rộng lớn, bao gồm dịch máy (Wang et al., 2019), chatbot (Caldarini et al., 2022), nhận dạng giọng nói (Dong et al., 2018), tạo chú thích hình ảnh (Yu et al., 2019), phân đoạn hình ảnh (Valanarasu et al., 2021; Strudel et al., 2021), và phân loại (Chen et al., 2021b). Transformer đạt được hiệu suất ấn tượng bằng cách nhận ra rằng các token trình tự đầu vào khác nhau có mức độ quan trọng khác nhau đối với dự đoán đầu ra cuối cùng. Transformer nắm bắt mối quan hệ giữa từng cặp token đầu vào bằng một quá trình gọi là "self-attention". Điều này cho phép transformer tạo ra các đầu ra có độ chính xác cao bằng cách tập trung vào các token liên quan nhất trong một trình tự đầu vào đồng thời cũng chú ý đến bối cảnh tổng thể. Cách tiếp cận này đã được chứng minh là rất hiệu quả và làm cho transformer trở thành một công nghệ hàng đầu trong trí tuệ nhân tạo.

Với việc huấn luyện trình tự dài, transformer chú ý đến nhiều token đầu vào hơn so với một transformer được huấn luyện với trình tự ngắn. Do đó, việc huấn luyện trình tự dài thường nắm bắt được nhiều thông tin ngữ cảnh hơn và dẫn đến độ chính xác dự đoán cao hơn đáng kể cho nhiều tác vụ có phụ thuộc tầm xa, chẳng hạn như phân tích trình tự DNA (Zaheer et al., 2020), tóm tắt tài liệu dài (Beltagy et al., 2020) và phân đoạn hình ảnh (Strudel et al., 2021). Thật không may, dấu chân bộ nhớ của transformer tăng theo bậc hai và tính toán tăng theo bậc ba với chiều dài trình tự dài hơn (Beltagy et al., 2020; Dao et al., 2022). Do đó, chiều dài trình tự của transformer thường bị cắt ngắn không quá vài nghìn token do các ràng buộc về thời gian chạy và bộ nhớ, mặc dù trình tự dài hơn dẫn đến độ chính xác cao hơn.

Để giải quyết vấn đề này, có ba hướng nghiên cứu: huấn luyện phân cấp, xấp xỉ attention, và song song trình tự phân tán. Huấn luyện phân cấp bao gồm việc huấn luyện nhiều transformer ở các cấp độ trừu tượng khác nhau (Si & Roberts, 2021; Chen et al., 2022; 2021b; Yu et al., 2023). Transformer ở cấp độ trừu tượng thấp nhất huấn luyện trên các đoạn trình tự ngắn nhất. Sau đó, transformer ở cấp độ cao hơn tiếp theo sử dụng các đầu ra của cấp độ trước đó làm đầu vào bổ sung để huấn luyện trên các đoạn dài hơn. Quá trình sau đó lặp lại cho đến khi đạt đến cấp độ trừu tượng cao nhất. Tuy nhiên, việc huấn luyện nhiều transformer ở các cấp độ trừu tượng khác nhau làm tăng đáng kể thời gian huấn luyện và dấu chân bộ nhớ. Ngoài ra, việc điều chỉnh siêu tham số là cần thiết cho kiến trúc mô hình tối ưu ở mỗi cấp độ trừu tượng.

Ngược lại, cách tiếp cận xấp xỉ nhằm giảm tính toán và sử dụng bộ nhớ bằng cách xấp xỉ các phép toán self-attention thông qua lấy mẫu thưa thớt (Child et al., 2019; Kitaev et al., 2020; Roy et al., 2021; Beltagy et al., 2020; Zaheer et al., 2020), xấp xỉ hạng thấp (Choromanski et al., 2021; Katharopoulos et al., 2020), cập nhật self-attention không thường xuyên (Ying et al., 2021; Rabe & Staats, 2022), hoặc sự kết hợp của chúng (Chen et al., 2021a). Các phương pháp xấp xỉ này có thể giảm đáng kể dấu chân bộ nhớ và tính toán, và một số thậm chí có thể giảm độ phức tạp bậc hai của bài toán trình tự dài xuống độ phức tạp tuyến tính. Tuy nhiên, xấp xỉ là một kỹ thuật nén thông tin có mất mát loại bỏ thông tin một phần cho self-attention. Do đó, xấp xỉ quá mức có thể làm giảm độ chính xác đặc biệt đối với các trình tự có phụ thuộc tầm xa. Các thí nghiệm trước đây chứng minh sự suy giảm độ chính xác đáng kể khi tỷ lệ nén xấp xỉ vượt quá 70% (Shi et al., 2021).

Cách tiếp cận song song trình tự phân tán nhằm giải quyết bài toán trình tự dài bằng cách phân phối các trình tự dài thành các đoạn trình tự liền kề giữa các GPU (Li et al., 2021; 2023; Korthikanti et al., 2022; Jacobs et al., 2023). Rào cản lớn nhất trong song song trình tự phân tán nằm ở việc xử lý self-attention, vừa là bước tính toán chuyên sâu nhất nhưng cũng là bước truyền thông chuyên sâu nhất. Vì đoạn trình tự của mỗi GPU có tương quan với đoạn của mọi GPU khác, một phương pháp self-attention phân tán đơn giản, chẳng hạn như (Li et al., 2021), Deep-Speed Ulysses (Jacobs et al., 2023) và LightSeq gần đây nhất (Li et al., 2023), yêu cầu mỗi GPU phải giao tiếp với mọi GPU khác nhiều lần để tính toán điểm self-attention một phần cho đoạn được gán của nó, trước khi tổng hợp chúng thành đầu ra self-attention cuối cùng. Do đó, tần suất truyền thông có xu hướng tăng đáng kể theo tỷ lệ tăng trưởng bậc hai với nhiều GPU song song trình tự hơn, làm cản trở đáng kể khả năng mở rộng.

Ngược lại, một phương pháp song song trình tự thay thế từ framework Megatron-LM của Nvidia hoàn toàn tránh phụ thuộc self-attention bằng cách thực hiện cập nhật tuần tự trên các phép toán self-attention và feed-forward cốt lõi dọc theo chiều trình tự, trong khi song song hóa các tác vụ ít tính toán chuyên sâu nhưng độc lập, chẳng hạn như chuẩn hóa lớp và dropout (Korthikanti et al., 2022). Mặc dù phương pháp này chỉ yêu cầu 8 lần truyền thông cho mỗi lớp attention, nó bỏ lỡ cơ hội song song hóa self-attention tính toán chuyên sâu. Do đó, nó dẫn đến tăng tốc khiêm tốn 29% so với baseline tuần tự với việc tính toán lại backward hoàn toàn (Korthikanti et al., 2022).

Bảng 1 cung cấp tóm tắt ý tưởng cốt lõi và hạn chế của mỗi cách tiếp cận. Để giải quyết các hạn chế của chúng, bài báo này giới thiệu "Distributed Long Short-Sequence Transformer" (LSS Transformer). Trái ngược với các phương pháp hiện có, LSS Transformer (1) sử dụng một transformer duy nhất để huấn luyện; (2) đạt được tăng tốc và giảm bộ nhớ đáng kể; (3) không có xấp xỉ, do đó không mất độ chính xác; và (4) duy trì chi phí truyền thông tối thiểu, chỉ yêu cầu 2 lần truyền thông cho mỗi lớp attention. Nó phân phối một trình tự dài thành các đoạn ngắn giữa các GPU, với mỗi GPU tính toán điểm self-attention một phần cho đoạn của nó trong ngữ cảnh của toàn bộ trình tự. Sau đó nó sử dụng một kỹ thuật truyền thông hợp nhất và một kỹ thuật trung bình gradient kép để tránh việc cần tổng hợp điểm self-attention một phần, giảm thiểu chi phí truyền thông trong khi bảo tồn phụ thuộc dữ liệu.

Bài báo này đóng góp theo những cách sau: (1) Giới thiệu một framework trình tự đầu cuối tổng quát và sáng tạo cho việc huấn luyện trình tự dài. Hoạt động ở cấp độ lớp attention, nó vẫn bất khả tri đối với kích thước mô hình và các biến thể (chỉ encoder, chỉ decoder, v.v.), làm cho nó có thể áp dụng toàn cầu mà không cần sửa đổi. (2) Trình bày một thuật toán chi phí truyền thông thấp sử dụng truyền thông hợp nhất và trung bình gradient để giảm tần suất truyền thông. (3) Trình bày một thuật toán song song trình tự và dữ liệu tích hợp giảm thiểu các lần truyền thông liên GPU trong các nhóm truyền thông cục bộ. Đánh giá của chúng tôi trên bộ dữ liệu Wikipedia enwik8 chứng minh sự vượt trội của LSS Transformer so với song song trình tự Nvidia tiên tiến nhất (Korthikanti et al., 2022), đạt được huấn luyện nhanh hơn 6 lần và hiệu quả bộ nhớ tăng 10 lần trên 144 GPU Nvidia V100. Hơn nữa, thuật toán của chúng tôi mở rộng đáng kể tới chiều dài trình tự rộng lớn 50,112 sử dụng 3,456 GPU, mang lại hiệu quả song song siêu tuyến tính 161% và thông lượng tính toán 32 petaflops.

--- TRANG 2 ---
Ultra-Long Sequence Distributed Transformer

Bảng 1. Tóm tắt các phương pháp huấn luyện trình tự dài khác nhau. Các phương pháp tuần tự có thể cần được sử dụng trên các sơ đồ song song. lx= chiều dài trình tự; N= số worker GPU; H= số cấp độ phân cấp; Z= số phần tử khác không.

| Cách tiếp cận | Phương pháp | Mất độ chính xác | Tuần tự/Phân tán | Bộ nhớ (trên worker) | Tính toán (trên worker) | Tổng bộ nhớ | Tổng tính toán | Tần suất truyền thông trên lớp | Thắt cổ chai thực tế |
|---|---|---|---|---|---|---|---|---|---|
| Huấn luyện phân cấp | (Chen et al., 2022; 2021b) (Si & Roberts, 2021; Yu et al., 2023) | Không | Tuần tự | O(l²ₓH) | O(l³ₓH) | N/A | N/A | N/A | Huấn luyện nhiều mô hình |
| Xấp xỉ Attention | (Kitaev et al., 2020; Roy et al., 2021) (Child et al., 2019; Beltagy et al., 2020) | Có | Tuần tự | O(Z) | O(Z³) | N/A | N/A | N/A | Độ thưa thấp |
| Song song trình tự | Song song trình tự đơn giản (Li et al., 2021; 2023) | Không | Phân tán (tổng hợp một phần) | O(l²ₓ/N) | O(l³ₓ/N) | O(l²ₓ) | O(l³ₓ) | ≈N² | Truyền thông thường xuyên |
| | Song song trình tự Nvidia (Korthikanti et al., 2022) | | Phân tán (attention tuần tự) | O(l²ₓ) | O(l³ₓ) | O(l²ₓ) | O(l³ₓ) | 8 | Attention không được phân tán |
| | LSS Transformer (của chúng tôi) | | Phân tán (hoàn toàn) | O(l²ₓ/N) | O(l³ₓ/N) | O(l²ₓ) | O(l³ₓ) | 2 | - |

attention operations through either sparse sampling (Child et al., 2019; Kitaev et al., 2020; Roy et al., 2021; Belt-agy et al., 2020; Zaheer et al., 2020), low-rank approxima-tion (Choromanski et al., 2021; Katharopoulos et al., 2020), infrequent self-attention updates (Ying et al., 2021; Rabe & Staats, 2022), or their combinations (Chen et al., 2021a). These approximation methods can significantly reduce mem-ory footprint and computations, and some can even reduce the long sequence problem's quadratic complexity to linear complexity. However, approximation is a lossy information compression technique that discards partial information for the self-attention. Thereby, excessive approximation may lower accuracy especially for sequences with long-range dependency. Previous experiments demonstrate significant accuracy degradation when approximation compression ra-tio exceeds 70% (Shi et al., 2021).

The distributed sequence parallelism approach aims to address the long sequence problem by distributing long sequences into contiguous sequence segments among GPUs (Li et al., 2021; 2023; Korthikanti et al., 2022; Jacobs et al., 2023). The largest hurdle in distributed sequence parallelism lies in handling the self-attention, which is both the most compute-intensive but also the most communicate-intensive step. Since each GPU's sequence segment has a correlation with every other GPU's segment, a straightfor-ward distributed self-attention method, such as (Li et al., 2021), Deep-Speed Ulysses (Jacobs et al., 2023) and the most recent LightSeq (Li et al., 2023), requires each GPU to communicate with every other GPU multiple times to cal-culate a partial self-attention score for its assigned segment, before aggregating them into the final self-attention output. Consequently, communication frequency tends to increase significantly in a quadratic growth rate with more sequence parallel GPUs, substantially impeding scalability.

Conversely, an alternative sequence parallel method from Nvidia's Megatron-LM framework completely avoids self-attention dependency by performing sequential updates on the core self-attention and feed-forward operations along the sequence dimension, while parallelizing less compute-intensive but independent tasks, such as layer normaliza-tion and dropouts (Korthikanti et al., 2022). Although this method requires only 8 communications per attention layer, it misses the opportunity to parallelize the compute inten-sive self-attention. Consequently, it leads to a modest 29% speedup compared to a sequential baseline with full back-ward recomputation (Korthikanti et al., 2022).

A summary of each approach's core idea and limitations is provided in Table 1. To address their limitations, this paper introduces the "Distributed Long Short-Sequence Trans-former" (LSS Transformer). In contrast to existing methods, the LSS Transformer (1) utilizes a single transformer for training; (2) attains remarkable speedup and memory re-duction; (3) has no approximation, thereby no accuracy loss; and (4) maintains a minimal communication overhead, requiring only 2 communications per attention layer. It dis-tributes a long sequence into short segments among GPUs, with each GPU computing a partial self-attention score for its segment in the context of the entire sequence. Then it uses a fused communication and a double gradient averag-ing technique to avoid the need to aggregate partial self-attention score, minimize communication overhead while preserving data dependency.

This paper contributes in the following ways: (1) Introduc-ing a general and innovative end-to-end sequence frame-work for long sequence training. Operating at the attention layer level, it remains agnostic to model sizes and varia-tions (encoder-only, decoder-only, etc.), making it univer-sally applicable without modifications. (2) Presenting a low communication overhead algorithm that uses fused commu-nication and gradient averaging to lower communication frequency. (3) Presenting an integrative sequence and data parallelism algorithm that minimizes inter-GPU communi-cations within local communicative groups. Our assessment on the Wikipedia enwik8 dataset demonstrates the superi-ority of the LSS Transformer over state-of-the-art Nvidia sequence parallelism (Korthikanti et al., 2022), achieving 6x faster training and 10x enhanced memory efficiency on 144 Nvidia V100 GPUs. Moreover, our algorithm scales remark-ably to an extensive sequence length of 50,112 using 3,456 GPUs, delivering 161% super-linear parallel efficiency and a computation throughput of 32 petaflops.

--- TRANG 3 ---
Ultra-Long Sequence Distributed Transformer

Bảng 2. Ba thách thức riêng biệt để huấn luyện transformer và các cấp độ song song trực giao của chúng. Nd= kích thước dữ liệu; Nm= kích thước mô hình; lx = chiều dài trình tự; B= kích thước batch;

| Thách thức | Tập dữ liệu lớn | Kích thước mô hình lớn | Trình tự dài |
|---|---|---|---|
| Độ phức tạp tính toán | O(Nd) | O(Nm) | O(l³x) |
| Độ phức tạp bộ nhớ | O(B) | O(Nm) | O(l²x) |
| Song song | Song song dữ liệu | Song song mô hình | Song song trình tự |
| Nguồn song song | Các batch dữ liệu đầu vào phân tán | Tham số mô hình phân tán | Các đoạn trình tự phân tán |
| Bộ nhớ phân tán | Không | Có | Có |

2 KIẾN THỨC NỀN TẢNG VÀ ĐỘNG LỰC

2.1 Các cấp độ song song trực giao

Huấn luyện một mô hình transformer có ba thách thức tính toán riêng biệt: (1) tập dữ liệu huấn luyện lớn, (2) kích thước mô hình lớn và (3) trình tự dài. Bảng 2 cung cấp một cái nhìn tổng quan ngắn gọn về những thách thức này và đặc điểm của chúng. Như đã thảo luận trước đó, việc tăng chiều dài trình tự dẫn đến tỷ lệ tăng trưởng bậc ba cho tính toán và tỷ lệ tăng trưởng bậc hai cho việc sử dụng bộ nhớ. Ngược lại, việc mở rộng các tham số mô hình hoặc batch dữ liệu thể hiện độ phức tạp tính toán và bộ nhớ tuyến tính.

Mỗi thách thức dựa vào một loại song song độc đáo. Các cấp độ song song này hầu hết là trực giao, giải quyết các thách thức khác nhau và không thể thay thế lẫn nhau. Song song dữ liệu tăng tốc huấn luyện trên các tập dữ liệu lớn bằng cách phân phối các batch huấn luyện. Tuy nhiên, nó không phân phối bộ nhớ và mỗi GPU có một bản sao của toàn bộ mô hình. Song song mô hình đạt được tăng tốc và bộ nhớ phân tán cho bài toán mô hình lớn bằng cách phân phối các tham số mô hình và gradient của chúng. Song song trình tự, ngược lại, tăng tốc tính toán và phân phối bộ nhớ cho trình tự dài bằng cách phân phối các đoạn trình tự. Bài báo này song song hóa trong chiều trình tự để mở rộng chiều dài trình tự bằng cách phân phối bộ nhớ và chi phí tính toán, đồng thời cẩn thận đảm bảo rằng song song trình tự không can thiệp hoặc cản trở các hình thức song song hóa khác.

2.2 Kiến trúc Transformer

Hình 1(i)-(iii) minh họa một kiến trúc transformer tiêu chuẩn cho cả mô hình chỉ decoder (tức là GPT) và chỉ encoder (BERT). Một embedding token trong Hình 1(i) chuyển đổi một trình tự đầu vào hình ảnh hoặc văn bản được token hóa x thành một vector đầu vào. Vector đầu vào sau đó được tăng cường với thông tin vị trí được nhúng.

Tiếp theo, vector đầu vào x có kích thước lx×Em, với lx đại diện cho chiều dài trình tự và Em là kích thước embedding, trải qua chuẩn hóa lớp và dropout ngẫu nhiên trước khi được xử lý bởi self-attention. Các bước chi tiết cho self-attention được minh họa trong Hình 1(ii) với mục tiêu tạo ra một embedding ngữ cảnh cho mỗi token đầu vào liên quan đến toàn bộ trình tự. Trong đơn vị self-attention, vector đầu vào x được biến đổi tuyến tính thành các vector query (Q), key (K), và value (V), với cả ba vector có cùng kích thước với đầu vào x. Sau đó self-attention tính toán phương trình sau:

E = softmax(QK^T/√dk)V = AwV. (1)

Điểm self-attention, được ký hiệu là Aw, có kích thước lx×lx và định lượng tương quan giữa từng cặp token. Tương quan này được tính bằng cách lấy tích vô hướng giữa Q và K chuyển vị. Để ổn định gradient trong quá trình huấn luyện, điểm self-attention Aw được tỷ lệ thêm bởi một hằng số √dk, và sau đó được chuẩn hóa bởi một dropout và một kích hoạt SoftMax. Đầu ra self-attention cuối cùng E được thu được bằng cách tính trung bình có trọng số giữa vector value V và Aw. Embedding đầu ra E sau đó được biến đổi tuyến tính trước khi thoát khỏi đơn vị self-attention.

Embedding ngữ cảnh đầu ra từ self-attention được xử lý thêm bởi một đơn vị feed-forward, với các bước chi tiết được minh họa trong Hình 1(iii), các kết nối tồn dư, và chuẩn hóa lớp để tăng cường tính ổn định huấn luyện và độ chính xác dự đoán. Các thành phần này, cùng với dropout, tạo thành một lớp attention duy nhất, được phác thảo bởi một hộp màu xám trong Hình 1(i). Sau đó các lớp attention lặp lại chính nó L lần, trong đó L là số lớp attention. Sau khi trải qua chuẩn hóa lớp cuối cùng và biến đổi tuyến tính, đầu ra y được so sánh với mục tiêu ỹ sử dụng hàm mất mát cross-entropy, và tất cả các tham số được cập nhật.

Điều quan trọng cần lưu ý là self-attention chiếm ưu thế cả về yêu cầu tính toán và bộ nhớ để huấn luyện. Với kích thước lx×lx, điểm self-attention, Aw, có sự mở rộng bậc hai về kích thước với chiều dài trình tự tăng. Điều này dẫn đến độ phức tạp tính toán bậc ba và bộ nhớ bậc hai của transformer đối với chiều dài trình tự.

2.3 Song song trình tự tiên tiến nhất

Để song song hóa self-attention, phương pháp self-attention phân tán đơn giản, chẳng hạn như (Li et al., 2021; 2023), phân vùng cả vector đầu vào x và các vector được biến đổi tuyến tính của nó Q, K và V thành các đoạn phân tán giữa các GPU. Ví dụ trong một kịch bản với 3 GPU, GPU 1 nhận các đoạn đầu tiên của các vector này, cụ thể là x1, Q1, K1, và V1, và GPU 2 và 3 nhận đoạn thứ hai và thứ ba.

--- TRANG 4 ---
Ultra-Long Sequence Distributed Transformer

Hình 1. (i) Một transformer chung với L lớp attention được phác thảo bởi một hộp màu xám. (ii) và (iii) các đơn vị self-attention và feed-forward, tương ứng. (iv) Lượt forward cho song song trình tự baseline với 2 GPU phân tán trình tự. Màu xanh dương chỉ các bước phân tán và màu xanh lá cho các bước tuần tự. (v) Ví dụ 2 GPU cho lượt forward của LSS Transformer.

segments. Để tính toán Eqn. (1), mỗi GPU phải nhận đoạn của mọi GPU khác để tính toán điểm self-attention một phần. Ví dụ, GPU 1 cần nhận K2 và K3 từ GPU 2 và 3 trước khi GPU 1 có thể tính toán điểm self-attention một phần Q1K2^T và Q1K3^T. Sau đó, các điểm self-attention một phần được tổng hợp qua các GPU thành điểm self-attention hoàn chỉnh, sau đó được sử dụng trong tích vô hướng với vector value V phân tán. Vì mỗi GPU phải giao tiếp với mọi GPU khác nhiều lần, tần suất truyền thông cho phương pháp self-attention phân tán đơn giản tăng theo bậc hai với nhiều GPU hơn, hạn chế đáng kể khả năng mở rộng của nó.

Để giải quyết hạn chế này, phương pháp của Nvidia (Korthikanti et al., 2022), được gọi là "song song trình tự baseline" trong phần còn lại của bài báo, tính toán self-attention và feed-forward tuần tự để tránh các lần truyền thông liên GPU tăng theo bậc hai (Korthikanti et al., 2022). Tuy nhiên, nó song song hóa độc lập chuẩn hóa lớp và dropout trong chiều trình tự, vì chúng thiếu các phụ thuộc liên token như vậy (Korthikanti et al., 2022). Lưu ý rằng thuật ngữ "tính toán tuần tự" liên quan đến tính toán GPU đơn, trong khi "tính toán song song" đề cập đến những tính toán được thực hiện qua các GPU trong chiều trình tự. Mặc dù self-attention và feed-forward được tính toán trong một GPU duy nhất, các tính toán của chúng vẫn được vector hóa thông qua các luồng song song trong GPU.

Hình 1(iv) tóm tắt song song trình tự baseline sử dụng một ví dụ về 2 GPU. Trong một lượt forward, embedding vị trí và token được tính toán tuần tự và đầu ra được phân tán thành các đoạn liền kề giữa các GPU dọc theo chiều trình tự. Sau đó trong các lớp attention, feed-forward và self-attention được cập nhật tuần tự bởi một GPU duy nhất, và được đại diện bởi các hình chữ nhật màu xanh lá trong hình. Tất cả các bước khác được cập nhật độc lập với song song trình tự, được đại diện bởi các hình chữ nhật màu xanh dương. Các lần truyền thông gather và scatter được sử dụng trước và sau self-attention và feed-forward để đảm bảo cập nhật tuần tự cho chúng và cập nhật song song độc lập cho tất cả các bước khác. Cuối cùng, kết quả từ các GPU được thu thập cho một chuẩn hóa lớp cuối cùng, biến đổi tuyến tính, và đánh giá hàm mất mát cross-entropy. Trong lượt backward, tất cả các bước giống như lượt forward, ngoại trừ các lần truyền thông gather trong lượt forward được thay thế bằng reduce-scatter để đồng bộ hóa gradient giữa các GPU và các lần truyền thông scatter trong lượt forward được thay thế bằng các hoạt động gather trong lượt backward.

Mặc dù song song trình tự baseline tránh các lần truyền thông GPU thường xuyên trong self-attention như trong phương pháp phân tán đơn giản, nó có hai hạn chế chính. Thứ nhất, các bước self-attention và feed-forward tính toán chuyên sâu nhất là tuần tự. Thứ hai, chi phí truyền thông vẫn đáng kể với 8 lần truyền thông toàn cầu cho mỗi lớp attention (4 trong lượt forward và 4 trong lượt backward). Kết quả là, song song trình tự baseline chỉ đạt được tăng tốc 3% trên một mô hình 22 tỷ tham số so với baseline mà không có tính toán lại lượt backward, và lên đến tăng tốc 29% so với baseline có tính toán lại backward (Korthikanti et al., 2022).

--- TRANG 5 ---
Ultra-Long Sequence Distributed Transformer

3 DISTRIBUTED LONG SHORT-SEQUENCE TRANSFORMER

Để đạt được khả năng mở rộng xuất sắc, LSS Transformer phải phân phối self-attention, nhưng cũng vượt qua tần suất truyền thông tăng theo bậc hai. LSS Transformer thực hiện song song trình tự dựa trên các nguyên tắc này:

Nguyên tắc 1: Tính toán độc lập với bộ nhớ phân tán. Ngoại trừ self-attention, tất cả các tính toán khác như chuẩn hóa lớp, kết nối tồn dư, dropout, feed-forward, biến đổi tuyến tính, embedding vị trí và token có thể được tính toán độc lập và phân phối giữa các GPU mà không có phụ thuộc trong chiều trình tự. Lưu trữ bộ nhớ cho các bước này cũng được phân phối trong chiều trình tự theo cùng cách.

Phép toán feed-forward có thể được tính toán độc lập bằng cách phân phối hàng phép nhân biến đổi tuyến tính của nó dọc theo chiều chiều dài trình tự, cho phép tính toán GPU độc lập. Ngoài ra, kích hoạt GeLu và dropout hoạt động độc lập trên từng phần tử của đầu vào của chúng. Ví dụ, đối với một đầu vào trình tự x có chiều lx×Em, trong đó lx là chiều dài trình tự và Em là kích thước embedding, bước biến đổi tuyến tính đầu tiên trong đơn vị feed-forward, linear(x) trong Hình 1(iii), nhân x với các tham số mô hình biến đổi tuyến tính có kích thước Em×Dinner, trong đó Dinner là chiều của lớp ẩn feed-forward. Phép nhân ma trận này có thể được phân phối hàng giữa các GPU mà không có truyền thông khi đầu vào x được phân phối thành các đoạn trình tự xi có kích thước lx/N×Em, trong đó N là số GPU song song trình tự. Sau các phép toán độc lập theo phần tử trên GeLu và dropout, đầu ra phân phối trình tự được sử dụng làm đầu vào phân phối cho phép nhân biến đổi tuyến tính thứ hai trong đơn vị feed forward, một lần nữa được phân phối hàng mà không có truyền thông.

Hình 1(v) mô tả lượt forward của LSS Transformer, thể hiện một minh họa với 2 GPU song song trình tự. Lưu ý rằng trong khi hình minh họa một kịch bản cụ thể, LSS Transformer hoạt động ở cấp độ lớp attention, và có thể thích ứng toàn cầu với các kích thước và loại mô hình khác nhau mà không cần sửa đổi. Trong hình này, trình tự đầu vào x được phân tán thành x1 và x2 trong chiều trình tự và mỗi GPU nhận một đoạn. Nguyên tắc 1 cho phép tất cả các hoạt động tiếp theo được phân phối trình tự giữa các GPU và tính toán độc lập. Ngoài ra, các đầu vào, đầu ra trung gian và gradient liên quan của mỗi hoạt động cũng được lưu trữ trong bộ nhớ phân tán qua các GPU song song trình tự, cho phép giảm dấu chân bộ nhớ xuất sắc. Chúng tôi sử dụng các hình chữ nhật màu xanh dương trong hình để đại diện cho các bước tính toán độc lập này. Self-attention được đánh dấu bằng một hình chữ nhật màu xanh dương có bóng để chỉ ra rằng self-attention được phân phối nhưng yêu cầu truyền thông liên GPU.

Nguyên tắc 2: Embedding vị trí phân phối trình tự. Các tham số embedding vị trí là một bảng tra cứu và mỗi hàng của bảng đại diện cho vị trí không gian của một token trong trình tự. Số hàng của bảng tra cứu tương ứng với chiều dài trình tự. Vì mỗi GPU phân phối trình tự nhận các đoạn trình tự liền kề, GPU thực hiện các hoạt động tra cứu chỉ trên các hàng liền kề tương ứng của bảng tra cứu. Điều này cho phép phân phối hàng của các embedding vị trí giữa các GPU mà không có phụ thuộc.

Nguyên tắc 3: Self-Attention phân tán với truyền thông hợp nhất. Để song song hóa self-attention, chúng tôi sử dụng tính chất toán học sau để bảo tồn phụ thuộc dữ liệu trong khi giảm thiểu chi phí truyền thông. Bằng cách phân phối vector query, Q, trong chiều trình tự giữa các GPU, chúng tôi tính toán đầu ra self-attention, E, như việc nối sau trong chiều trình tự, và song song:

E = [
softmax(Q1K^T/√dk)V
softmax(Q2K^T/√dk)V  
softmax(Q3K^T/√dk)V
...
], (2)

trong đó Qi là một đoạn vector query phân phối lx/N×Em được nhận bởi GPU thứ i, trong đó N là số GPU. V và K là các vector value và key được thu thập lx×Em mà không có phân phối, có cùng bản sao qua tất cả các GPU. softmax(QiK^T/√dk) đại diện cho điểm self-attention một phần của GPU thứ i cho đoạn được gán của nó, nhưng trong ngữ cảnh của toàn bộ trình tự.

Tóm lại, ý tưởng chính là phân phối vector query trong chiều trình tự giữa các GPU, trong khi giữ các vector value và key được thu thập. Sau đó LSS Transformer tính toán một đầu ra self-attention riêng lẻ, softmax(QiK^T/√dk)V, cho mỗi GPU. Eqn. (2) cho thấy rằng việc nối các đầu ra self-attention riêng lẻ của GPU bằng số học giống như việc tính toán trực tiếp self-attention E tuần tự, vì việc nối là một việc thu thập hàng đơn giản cho E. Do đó, phương pháp self-attention phân tán đề xuất của chúng tôi là một phương pháp chính xác mà không có xấp xỉ, do đó không mất độ chính xác. So với song song trình tự đơn giản với tần suất truyền thông tăng theo bậc hai, một lợi thế đáng kể của Eqn. (2) là nó cho phép tính toán phân tán trong khi chỉ yêu cầu 6 lần truyền thông cho mỗi lớp self-attention. Lượt forward yêu cầu 2 lần truyền thông từ việc thu thập các vector value và key và 1 từ việc nối self-attention. Lượt backward yêu cầu thêm 3 lần truyền thông.

Để giảm thêm chi phí truyền thông, chúng tôi sử dụng một kỹ thuật truyền thông hợp nhất để giảm tần suất truyền thông từ 6 lần truyền thông mỗi lớp xuống 4. Hình 2(i) minh họa các hoạt động trong lượt forward mà không có truyền thông hợp nhất. Một ví dụ đoạn trình tự, xi

--- TRANG 6 ---
Ultra-Long Sequence Distributed Transformer

Hình 2. (i) và (ii) cho thấy sự khác biệt mà không có và có truyền thông hợp nhất. (iii) cho thấy lượt forward của self-attention phân tán với truyền thông hợp nhất. Lưu ý rằng các đầu ra self-attention phân tán không được nối. (iv) Lượt backward của LSS Transformer. Các tham số mô hình, ngoại trừ embedding vị trí, được đồng bộ hóa thông qua trung bình gradient. (v) Lượt backward của self-attention phân tán với reduce-scatter.

được biến đổi tuyến tính thành các đoạn query, key và value. Sau đó, hai lần truyền thông all-gather được vận hành độc lập trên các đoạn key và value thành K và V được thu thập. Hình 2(ii) cho thấy hoạt động truyền thông hợp nhất trong lượt forward, chỉ yêu cầu một lần truyền thông all-gather duy nhất. xi được biến đổi tuyến tính thành đoạn query Qi. Trong khi đó, xi được thu thập thành một trình tự được thu thập tạm thời x, trước khi x được biến đổi tuyến tính thành các vector key và value được thu thập. Cùng một kỹ thuật cũng được áp dụng cho lượt backward, giảm tổng số lần truyền thông từ 6 xuống 4 cho mỗi lớp attention.

Nguyên tắc 4: Kỹ thuật trung bình gradient để đồng bộ hóa GPU và tránh nối. Có hai vấn đề từ Nguyên tắc 1 và 3. Thứ nhất, vì GPU song song trình tự huấn luyện trên cùng các tham số mô hình nhưng sử dụng các đoạn trình tự đầu vào khác nhau, các gradient cho các tham số mô hình khác nhau cho mỗi GPU. Vấn đề thứ hai là tần suất truyền thông self-attention cần được giảm thêm để đạt được khả năng mở rộng và hiệu quả song song tốt hơn.

Để giải quyết cả hai vấn đề, chúng tôi sử dụng một kỹ thuật trung bình gradient để đồng bộ hóa các tham số mô hình và tránh việc nối cho các đầu ra self-attention riêng lẻ của GPU. Do đó, tần suất truyền thông được giảm từ 4 xuống 2 cho mỗi lớp attention. Hình 2(iii)-(v) sử dụng một ví dụ 2 GPU để minh họa cách kỹ thuật trung bình gradient này được áp dụng. Trong lượt forward cho self-attention trong Hình 2(iii), một query phân tán Qi được tính toán từ đoạn trình tự đầu vào xi. Trong khi đó, các đoạn đầu vào self-attention được thu thập giữa các GPU trước khi tính toán các vector K và V được thu thập sử dụng một lần truyền thông all-gather hợp nhất duy nhất, như đã giải thích trước đó trong Nguyên tắc 3. Các tính toán và lưu trữ bộ nhớ tiếp theo đều được phân phối và cập nhật độc lập trong chiều trình tự, tạo ra đầu ra self-attention riêng lẻ cho mỗi GPU.

Tuy nhiên, các đầu ra self-attention riêng lẻ không được nối qua các GPU trong Hình 2(iii). Thay vào đó, LSS Transformer cho phép mỗi GPU sử dụng đoạn trình tự được gán và đầu ra self-attention riêng lẻ của nó để tính toán một hàm mất mát cross-entropy một phần và gradient trong lượt backward trong Hình 2(iv) và (v). Lưu ý rằng lượt backward trong Hình 2(v) sử dụng reduce-scatter làm hoạt động backward cho all-gather trong lượt forward. Cuối cùng, các gradient trung bình được tính toán và sử dụng để cập nhật tham số mô hình đồng bộ trước khi huấn luyện trên batch dữ liệu tiếp theo. Một chi tiết kỹ thuật quan trọng cần đề cập là các gradient trung bình không được tính toán cho các embedding vị trí, là các tham số phân tán qua các GPU và không nên được đồng bộ hóa.

Để hiểu tại sao kỹ thuật trung bình gradient này có thể tránh việc nối self-attention và đồng bộ hóa các tham số mô hình cùng một lúc, hãy giả sử rằng đầu ra trình tự dự đoán từ transformer là y và nhãn thực của nó là ỹ. Hàm mất mát cross-entropy cho toàn bộ trình tự, được ký hiệu là L(y,ỹ), bằng trung bình của hàm mất mát của từng token riêng lẻ: L(y,ỹ) = 1/lx ∑(i=1 to lx) L(yi,ỹi), trong đó lx là chiều dài trình tự. Theo quy tắc tổng gradient, gradient của L(y,ỹ) đối với các tham số mô hình, được ký hiệu là ∇L(y,ỹ), bằng gradient trung bình của hàm mất mát của mỗi token: ∇L(y,ỹ) = 1/lx ∑(i=1 to lx) ∇L(yi,ỹi). Do đó, không có

--- TRANG 7 ---
Ultra-Long Sequence Distributed Transformer

Hình 3. Song song trình tự và dữ liệu tích hợp với trung bình gradient kép. Trung bình gradient theo hướng ngang đồng bộ hóa các tham số mà không có embedding vị trí, và trung bình gradient theo hướng dọc bao gồm embedding vị trí.

nhu cầu nối các đầu ra self-attention riêng lẻ để tính toán hàm mất mát và gradient cho toàn bộ trình tự. Thay vào đó, mỗi GPU sử dụng đầu ra self-attention riêng lẻ được phân tán của nó để tính toán một hàm mất mát một phần và gradient cho mỗi đoạn trình tự, trước khi tính trung bình gradient của mỗi đoạn để cập nhật tham số mô hình đồng bộ.

Bằng cách tránh các hoạt động nối đắt đỏ trong mỗi lớp attention, LSS Transformer giảm tần suất truyền thông của nó xuống chỉ hai lần cho mỗi lớp attention (một all-gather trong lượt forward và một reduce-scatter trong lượt backward) vì việc trung bình gradient chỉ xảy ra một lần cho mỗi batch dữ liệu. Điều này dẫn đến khả năng mở rộng tốt hơn nhiều và giảm truyền thông so với các phương pháp song song trình tự khác.

4 SONG SONG TRÌNH TỰ & DỮ LIỆU TÍCH HỢP

Song song trình tự của LSS Transformer có ba hạn chế. Thứ nhất, nó vẫn yêu cầu 2 lần truyền thông liên GPU toàn cầu cho mỗi lớp attention, điều này làm giảm hiệu quả song song ở nhiều GPU. Thứ hai, trong khi song song trình tự giải quyết vấn đề trình tự dài, nó không giải quyết thách thức tính toán để huấn luyện tập dữ liệu lớn. Ba, song song trình tự chỉ là một nguồn song song. Để mở rộng tới một siêu máy tính lớn để huấn luyện, LSS Transformer cần nhiều nguồn song song hơn để đạt được khả năng mở rộng tốt hơn. Để giải quyết những vấn đề này, phần này giới thiệu một phương pháp để tích hợp song song trình tự của LSS Transformer với song song dữ liệu. Với việc tích hợp, thuật toán song song có thể (1) đạt được khả năng mở rộng tốt hơn; (2) đồng thời giải quyết các thách thức trình tự dài và tập dữ liệu lớn; và (3) hạn chế các lần truyền thông self-attention giữa các nhóm truyền thông cục bộ để giảm chi phí.

Mặc dù song song trình tự và dữ liệu hầu hết là trực giao, một thách thức kỹ thuật cần vượt qua là cả hai loại song song đều yêu cầu đồng bộ hóa tham số mô hình, nhưng giữa các GPU trong các nhóm truyền thông khác nhau và truyền thông theo các cách khác nhau. Song song trình tự yêu cầu đồng bộ hóa tham số mô hình giữa các GPU song song trình tự, nhưng loại trừ các tham số embedding vị trí khỏi đồng bộ hóa cho rằng các embedding vị trí được phân phối trong chiều trình tự. Song song dữ liệu yêu cầu đồng bộ hóa tham số mô hình giữa các GPU song song dữ liệu, nhưng phải bao gồm embedding vị trí cho rằng các GPU song song dữ liệu có cùng bản sao của các tham số embedding vị trí, nhưng huấn luyện chúng với các batch dữ liệu khác nhau.

Để giải quyết vấn đề này, chúng tôi sử dụng một kỹ thuật trung bình gradient kép sáng tạo để tránh xung đột đồng bộ hóa cho embedding vị trí. Hình 3 minh họa một ví dụ về cách song sang trình tự và dữ liệu tích hợp sử dụng trung bình gradient kép. Trong ví dụ này, GPU 1 và 2 xử lý một trình tự x1 cùng nhau sử dụng song song trình tự, với đoạn đầu tiên x1₁ được gán cho GPU 1 và đoạn thứ hai x1₂ được gán cho GPU 2. Các tham số embedding vị trí được phân phối theo cùng cách với nửa đầu PE₁ được gán cho GPU 1 và nửa thứ hai PE₂ được gán cho GPU 2. Tương tự, GPU 3 và 4 xử lý một trình tự khác x2 sử dụng song song trình tự.

Tất cả các GPU xử lý cùng một trình tự tạo thành một nhóm song song trình tự và mỗi nhóm được hiển thị dưới dạng một hộp màu tím ngang trong Hình 3. Các lần truyền thông liên GPU của mỗi nhóm song song trình tự, được hiển thị dưới dạng một mũi tên màu tím ngang, là cục bộ và được giới hạn giữa các GPU trong cùng nhóm song song trình tự. Các lần truyền thông này bao gồm một all-gather hợp nhất và một reduce-scatter trong mỗi lớp attention để tính toán self-attention phân tán. Ngoài ra, một trung bình gradient được yêu cầu một lần cho mỗi batch dữ liệu để đồng bộ hóa tham số mô hình và tránh việc nối đầu ra self-attention, như đã thảo luận trước đó trong Phần 3. Tuy nhiên, các tham số embedding vị trí được loại trừ khỏi trung bình gradient, vì chúng được phân phối qua các GPU song song trình tự, và không nên được đồng bộ hóa.

Trong khi đó, GPU 1 và 3 nằm trong một nhóm song song dữ liệu, được hiển thị dưới dạng một hộp màu đỏ dọc trong hình, và GPU 2 và 4 nằm trong một nhóm song song dữ liệu khác. Các GPU trong cùng nhóm song song dữ liệu xử lý các đoạn trình tự từ các batch dữ liệu khác nhau, nhưng các đoạn trình tự này chia sẻ cùng vị trí không gian trong các trình tự của chúng, do đó chia sẻ cùng embedding vị trí. Các lần truyền thông liên GPU duy nhất cần thiết trong cùng nhóm song song dữ liệu, được hiển thị dưới dạng các mũi tên màu đỏ dọc trong hình, là trung bình gradient để đồng bộ hóa các tham số được huấn luyện với các batch khác nhau. Tương tự như các nhóm song song trình tự, truyền thông cho các nhóm song song dữ liệu cũng được địa phương hóa và giới hạn trong mỗi nhóm. Trung bình gradient cho song song dữ liệu,

--- TRANG 8 ---
Ultra-Long Sequence Distributed Transformer

Bảng 3. Thí nghiệm weak scaling của LSS Transformer và song song trình tự baseline Nvidia cho thí nghiệm mô hình nhỏ, chỉ sử dụng 1 nhóm song song dữ liệu.

(a) LSS Transformer, nhóm song song dữ liệu = 1
| Nodes | 1 | 6 | 18 | 54 | 144 |
|-------|---|---|----|----|-----|
| GPUs | 6 | 36 | 108 | 324 | 864 |
| Sequence Groups | 6 | 36 | 108 | 324 | 864 |
| Sequence Length | 348 | 2088 | 6264 | 18792 | 50112 |
| Per GPU Mem. (GB) | 0.54 | 1.01 | 2.05 | 5.94 | 13.58 |
| FLOPS (x 10¹²flop/s) | 8 | 189 | 881 | 3000 | 8245 |
| Self-Attn Comp Incr. | 1 | 6 | 18 | 54 | 144 |
| Parallel Efficiency | 100% | 165% | 174% | 173% | 151% |

(b) Baseline, nhóm song song dữ liệu = 1
| Nodes | 1 | 6 | 18 |
|-------|---|---|-----|
| GPUs | 6 | 36 | 108 |
| Sequence Groups | 6 | 36 | 108 |
| Sequence Length | 348 | 2088 | 6264 |
| Per GPU Mem. (GB) | 0.94 | 10.29 | OOM |
| FLOPS (x 10¹²flop/s) | 5 | 32 | OOM |
| Self-Attn Comp Incr. | 1 | 6 | OOM |
| Parallel Efficiency | 100% | 42% | OOM |

Bảng 4. Weak scaling của LSS Transformer và Nvidia baseline cho thí nghiệm mô hình nhỏ, sử dụng 4 nhóm song song dữ liệu.

(a) LSS Transformer, nhóm song song dữ liệu = 4
| Nodes | 4 | 24 | 72 | 216 | 576 |
|-------|---|----|----|-----|-----|
| GPUs | 24 | 144 | 432 | 1296 | 3456 |
| Sequence Groups | 6 | 36 | 108 | 324 | 864 |
| Sequence Length | 348 | 2088 | 6264 | 18792 | 50112 |
| Per GPU Mem. (GB) | 0.54 | 1.01 | 2.11 | 5.94 | 13.58 |
| FLOPS (x 10¹²flop/s) | 28 | 703 | 3319 | 10987 | 32784 |
| Self-Attn Comp Incr. | 1 | 6 | 18 | 54 | 144 |
| Parallel Efficiency | 100% | 167% | 173% | 159% | 161% |

(b) Baseline, nhóm song song dữ liệu = 4
| Nodes | 4 | 24 | 72 |
|-------|----|----|-----|
| GPUs | 24 | 144 | 432 |
| Sequence Groups | 6 | 36 | 108 |
| Sequence Length | 348 | 2088 | 6264 |
| Per GPU Memory (GB) | 0.94 | 10.29 | OOM |
| FLOPS (x 10¹²flop/s) | 18 | 126 | OOM |
| Self-Attn Comp Incr. | 1 | 6 | OOM |
| Parallel Efficiency | 100% | 42% | OOM |

tuy nhiên, phải bao gồm embedding vị trí để đồng bộ hóa, cho rằng các đoạn huấn luyện trong cùng nhóm song song dữ liệu chia sẻ cùng embedding vị trí.

5 KẾT QUẢ

5.1 Thiết lập thí nghiệm

Tập dữ liệu: enwik8 là một tập dữ liệu huấn luyện ký tự Wikipedia XML 100 triệu byte phổ biến (Hutter et al., 2006; Beltagy et al., 2020). Ban đầu được sử dụng làm tập dữ liệu kiểm tra cho Hutter Prize, tập dữ liệu có thể được tải xuống tại (Mahoney, 2006) để đánh giá điểm chuẩn công khai với bảng điểm có sẵn tại (with Codes, 2022).

Nền tảng tính toán: Các thí nghiệm được thực hiện trên siêu máy tính Summit của Phòng thí nghiệm Quốc gia Oak Ridge, có 6 GPU NVIDIA V100 và 2 CPU POWER9 cho mỗi node. Các node được kết nối qua mạng Mellanox EDR 100G InfiniBand Non-blocking Fat Tree. Mỗi CPU POWER9 trong node được kết nối dày đặc với 3 GPU với Nvidia NVlinks, trong đó mỗi liên kết có băng thông hai chiều 100 GB/s, và hai CPU cho mỗi node được kết nối qua một bus X với băng thông hai chiều 64 GB/s. Mỗi CPU có 22 core (4 luồng phần cứng cho mỗi core) và bộ nhớ DRAM 256 GB. Mỗi GPU có 80 multiprocessor luồng và bộ nhớ 16 GB. Có thêm 54 node "bộ nhớ cao", có 32 GB bộ nhớ trên mỗi GPU.

Phần mềm: LSS Transformer được phát triển trong PyTorch và sẽ được công khai trong phiên bản tiếp theo.

5.2 Thí nghiệm mô hình nhỏ

Mô hình: Tất cả các thí nghiệm trong phần thí nghiệm mô hình nhỏ huấn luyện một transformer chỉ decoder (GPT) với kích thước embedding 512, 6 lớp attention và 8 multi-head cho tổng cộng 20 triệu tham số. Kích thước batch dữ liệu đầu vào là 4. Chúng tôi chọn kích thước mô hình này vì hai lý do. Thứ nhất, đây là mô hình tiêu chuẩn cho đánh giá điểm chuẩn enwik8 với điểm số độ chính xác bits-per-character xuất sắc ở 1.0 (Beltagy et al., 2020; Al-Rfou et al., 2019; Sukhbaatar et al., 2019). Thứ hai, việc chọn kích thước mô hình nhỏ cho phép chúng tôi tối đa hóa việc sử dụng bộ nhớ và đánh giá hiệu suất để mở rộng trình tự dài, thay vì tối đa hóa bộ nhớ để lưu trữ tham số cho một mô hình lớn.

Weak Scaling Song song trình tự: Bảng 3(a) và 3(b) là so sánh hiệu suất weak scaling giữa LSS Transformer và song song trình tự baseline Nvidia. Chúng tôi tăng cả chiều dài trình tự và số GPU song song trình tự với cùng tỷ lệ, trong khi giữ số nhóm song song dữ liệu là 1.

Hai hàng đầu tiên của các bảng chỉ ra số node và GPU, với 6 GPU trên mỗi node. Hàng thứ ba đại diện cho số nhóm song song trình tự, bằng với số GPU trong trường hợp này vì số

--- TRANG 9 ---
Ultra-Long Sequence Distributed Transformer

(a) Hiệu quả mở rộng với và không có truyền thông hợp nhất và trung bình gradient
(b) Chiều dài trình tự tối đa
(c) Phân tích thời gian chạy cho Bảng 3(a)

Hình 4. (a) Hiệu quả mở rộng với và không có truyền thông hợp nhất và trung bình gradient. (b) chiều dài trình tự tối đa tại các số lượng GPU song song trình tự khác nhau. (c) phân tích thời gian chạy cho weak scaling thí nghiệm mô hình nhỏ LSS Transformer chỉ với 1 nhóm song song dữ liệu.

Bảng 5. So sánh hiệu suất giữa hai thuật toán trên mô hình nhỏ, chỉ sử dụng 1 nhóm song song dữ liệu. Chiều dài trình tự tăng dưới tuyến tính với tỷ lệ tăng trưởng tỷ lệ thuận với căn bậc hai của số GPU.

(a) LSS Transformer, nhóm song song dữ liệu = 1
| Nodes | 1 | 6 | 18 | 54 | 144 |
|-------|---|---|----|----|-----|
| GPUs | 6 | 36 | 108 | 324 | 864 |
| Sequence Groups | 6 | 36 | 108 | 324 | 864 |
| Sequence Length | 366 | 900 | 1512 | 2592 | 4320 |
| Per GPU Mem. (GB) | 0.53 | 0.62 | 0.74 | 0.89 | 1.15 |
| FLOPS (x 10¹²flop/s) | 8 | 83 | 266 | 673 | 1280 |
| Self-Attn Comp Incr. | 1 | 2 | 4 | 6 | 10 |
| Parallel Efficiency | 100% | 120% | 114% | 96% | 72% |

(b) Baseline, nhóm song song dữ liệu = 1
| Nodes | 1 | 6 | 18 |
|-------|---|---|-----|
| GPUs | 6 | 36 | 108 |
| Sequence Groups | 6 | 36 | 108 |
| Sequence Length | 366 | 900 | 1512 |
| Per GPU Mem. (GB) | 0.89 | 2.55 | 5.86 |
| FLOPS (x 10¹²flop/s) | 5 | 28 | 71 |
| Self-Attn Comp Incr. | 1 | 2 | 4 |
| Parallel Efficiency | 100% | 56% | 36% |

nhóm song sang dữ liệu là 1. Hàng thứ tư cho thấy chiều dài trình tự, tăng tỷ lệ thuận với số GPU. Hàng thứ năm hiển thị dấu chân bộ nhớ đỉnh trung bình trên mỗi GPU tính bằng Gigabyte (GB). Tại 6 node, dấu chân bộ nhớ trên mỗi GPU của LSS Transformer là 1.01 GB, và nó mở rộng tới 144 node với 13.58 GB trên mỗi GPU. Vì Transformer có độ phức tạp bộ nhớ bậc hai O(l²x/N), trong đó lx là chiều dài trình tự và N là số GPU, việc tăng chiều dài trình tự lx và số GPU N với cùng tỷ lệ vẫn sẽ dẫn đến sự gia tăng tuyến tính của bộ nhớ. Điều này giải thích tại sao LSS Transformer có dấu chân bộ nhớ nhỏ tại 1 node nhưng tăng lên dấu chân bộ nhớ lớn hơn nhiều tại nhiều node hơn. So sánh, song song trình tự baseline có dấu chân bộ nhớ trên mỗi GPU là 10.29 GB tại 6 node, hơn 10 lần lớn hơn so với LSS Transformer tại cùng số node. Song song trình tự baseline không thể mở rộng vượt quá 6 node do ràng buộc bộ nhớ, dẫn đến "OOM" (hết bộ nhớ).

Hàng thứ sáu đại diện cho số phép toán dấu phẩy động đơn chính xác (FLOP) qua tất cả các GPU trong một giây. LSS Transformer đạt được thông lượng tính toán 8 petaflops tại 144 node với chiều dài trình tự 50,112. So sánh, song song trình tự baseline chậm hơn 5.9 lần tại 6 node, đạt được thông lượng 32 teraflops, và không thể mở rộng thêm do ràng buộc bộ nhớ.

Hàng thứ bảy cho thấy sự gia tăng tính toán trên mỗi GPU được ghi lại cho đơn vị self-attention so với tính toán trên mỗi GPU tại 1 node. Vì transformer có độ phức tạp tính toán bậc ba, việc phân phối tính toán qua các GPU vẫn sẽ dẫn đến sự gia tăng tính toán cho weak scaling.

Hàng thứ tám đại diện cho hiệu quả song song, là tỷ lệ giữa tăng tốc thực tế và tăng tốc lý tưởng. LSS Transformer duy trì hiệu quả song song siêu tuyến tính 151% tại 144 node, trong khi hiệu quả của song song trình tự baseline giảm xuống 42% chỉ tại 6 node.

Weak Scaling Song song trình tự & dữ liệu tích hợp: Để hiểu cách việc tích hợp song song dữ liệu và trình tự tăng tốc độ huấn luyện và giảm chi phí truyền thông, Bảng 4(a) lặp lại cùng thí nghiệm như Bảng 3(a), nhưng với 4 nhóm song song dữ liệu. Điều này có nghĩa là tổng số GPU tăng gấp bốn tương ứng, nhưng số nhóm song song trình tự vẫn giữ như trước. Bằng cách so sánh kết quả giữa 4 nhóm song song dữ liệu và nhóm song song dữ liệu đơn trong Bảng 3 và 4, chúng ta quan sát thấy thông lượng FLOP tăng gần 4 lần từ 1 lên 4 nhóm song song dữ liệu với số GPU nhiều gấp 4 lần, đạt được 32 petaflops tại 3456 GPU. Kết quả này chỉ ra rằng sơ đồ truyền thông cục bộ được đề xuất cho

--- TRANG 10 ---
Ultra-Long Sequence Distributed Transformer

phép song song trình tự và dữ liệu tích hợp với ít chi phí truyền thông bổ sung và việc tích hợp hai loại song song này là một cách tiếp cận hiệu quả để đạt được khả năng mở rộng và huấn luyện nhanh hơn.

Tăng tốc siêu tuyến tính: LSS Transformer đạt được mở rộng siêu tuyến tính trong Bảng 3 và 4 do hai lý do. Thứ nhất, trình tự dài hơn dẫn đến công việc tăng cho mỗi GPU do sự gia tăng tính toán self-attention, dẫn đến tỷ lệ sử dụng GPU cao hơn cho trình tự dài hơn và mở rộng siêu tuyến tính. Được đo bằng báo cáo sử dụng PyTorch Cuda, tỷ lệ sử dụng GPU cho LSS Transformer tăng từ 33% tại 1 node với chiều dài trình tự 348 lên 83% tại 6 node với chiều dài trình tự 2,088. Thứ hai, chi phí truyền thông thấp của LSS Transformer đóng góp đáng kể vào hiệu quả song song xuất sắc của nó. Hình 4(a) cho thấy hiệu quả mở rộng có và không có các kỹ thuật truyền thông hợp nhất và trung bình gradient, cả hai đều được giới thiệu trong Phần 3. Tại 864 GPU và chiều dài trình tự 50,112, hiệu quả mở rộng với cả hai kỹ thuật là 151%. Hiệu quả với trung bình gradient nhưng không có truyền thông hợp nhất là 147%, trong khi hiệu quả không có kỹ thuật nào giảm xuống 118%.

Chiều dài trình tự tối đa: Hình 4(b) mô tả chiều dài trình tự tối đa khi mở rộng số GPU trong khi tối đa hóa dung lượng bộ nhớ. Mỗi cặp số trong hình tương ứng với số GPU và chiều dài trình tự tối đa. Ví dụ, (6,0.59) chỉ ra rằng 6 GPU có thể mở rộng lên chiều dài trình tự tối đa 0.59×10⁴. Chúng ta có thể quan sát thấy chiều dài trình tự tối đa tuân theo một đường cong căn bậc hai trong đồ thị. Vì transformer có độ phức tạp bộ nhớ bậc hai với trình tự dài hơn, chiều dài trình tự tối đa tăng tiệm cận với các hàm căn bậc hai khi tổng dung lượng bộ nhớ tăng.

Phân tích thời gian chạy. Hình 4(c) minh họa phân tích thời gian chạy cho kết quả weak scaling được trình bày trong Bảng 3(a), tập trung vào một nhóm song song dữ liệu duy nhất. Các thanh màu xanh dương đại diện cho phần trăm thời gian chạy dành cho tính toán, trong khi các thanh màu cam chỉ ra phần trăm thời gian chạy cho truyền thông. Các thanh màu xám biểu thị thời gian chờ GPU. Tại 36 GPU, chi phí truyền thông chiếm 27% tổng thời gian chạy. Khi số GPU mở rộng lên 864 (144 node), chi phí truyền thông trở thành 40% thời gian chạy.

Mở rộng chiều dài trình tự tăng dưới tuyến tính. Một cách để hạn chế sự gia tăng bộ nhớ và tính toán là tăng chiều dài trình tự với tỷ lệ dưới tuyến tính. Bảng 5 lặp lại cùng thí nghiệm mở rộng như Bảng 3, nhưng với chiều dài trình tự tăng tỷ lệ thuận với căn bậc hai của số GPU. Kết quả là, cả bộ nhớ và tính toán self-attention từ hàng 5 và 7 của bảng cũng tăng với tỷ lệ tiệm cận với các hàm căn bậc hai. Dấu chân bộ nhớ cho LSS-Transformer chỉ ở 1.15 GB trên mỗi GPU khi mở rộng lên 864 GPU, và tính toán self-attention chỉ nhiều hơn 10 lần so với một node duy nhất. Mở rộng của LSS-Transformer vẫn rất hiệu quả ở 94% với 864 GPU. Ngược lại, hiệu quả mở rộng cho song song trình tự Nvidia giảm xuống 31% tại 108 GPU và không thể mở rộng thêm do ràng buộc bộ nhớ.

5.3 Thí nghiệm mô hình lớn

Bảng 6 lặp lại cùng thí nghiệm như Bảng 5, nhưng huấn luyện một mô hình GPT lớn 1.5 tỷ tham số có kích thước embedding 2048, 24 lớp attention và 16 multi-head. Các thí nghiệm được chạy trên các node bộ nhớ cao của Summit với 32 GB bộ nhớ trên mỗi GPU, và không sử dụng song song mô hình cho thí nghiệm này. Vì hầu hết dung lượng bộ nhớ giờ đây được sử dụng để lưu trữ các tham số mô hình thay vì trình tự dài, chúng ta có thể nhận thấy rằng tất cả các lần chạy trong bảng này sử dụng nhiều bộ nhớ hơn so với thí nghiệm mô hình nhỏ. LSS Transformer duy trì hiệu quả mở rộng cao 92% tại 108 GPU và dấu chân bộ nhớ nhỏ hơn so với song song baseline. Ngược lại, song song baseline không thể mở rộng vượt quá một node duy nhất do ràng buộc bộ nhớ và thông lượng FLOP của nó nhỏ hơn 2.3 lần so với LSS Transformer tại một node duy nhất.

6 KẾT LUẬN

Bài báo này giới thiệu Long-Short Sequence Transformer (LSS Transformer), một thuật toán mới và một framework tổng quát để phân phối trình tự dài trong các mô hình transformer. Nó sử dụng một cơ chế self-attention phân tán mới, cùng với các kỹ thuật truyền thông hợp nhất và trung bình gradient kép, để đạt được tăng tốc và giảm bộ nhớ ấn tượng với chi phí truyền thông tối thiểu. Tóm lại, LSS Transformer là một bước tiến đáng kể để giải quyết bài toán trình tự dài của transformer. Chúng tôi tin rằng cách tiếp cận của chúng tôi cung cấp một đóng góp quan trọng cho lĩnh vực nghiên cứu và cho phép huấn luyện trình tự cực dài, đặc biệt cho các ứng dụng có lợi từ phụ thuộc token tầm xa, chẳng hạn như phân tích trình tự DNA, tóm tắt tài liệu dài, và các ứng dụng hình ảnh.

TÀI LIỆU THAM KHẢO

Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L. Character-level language modeling with deeper self-attention. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01): 3159–3166, July 2019. doi: 10.1609/aaai.v33i01. 33013159. URL https://ojs.aaai.org/index. php/AAAI/article/view/4182.

Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The

--- TRANG 11 ---
Ultra-Long Sequence Distributed Transformer

Bảng 6. So sánh hiệu suất giữa hai thuật toán trên một mô hình lớn 1.5 tỷ, chỉ sử dụng 1 nhóm song song dữ liệu, và không có bất kỳ song song mô hình nào. Chiều dài trình tự tăng với tỷ lệ tăng trưởng tỷ lệ thuận với căn bậc hai của số GPU.

(a) LSS Transformer, nhóm song song dữ liệu = 1
| Nodes | 1 | 6 | 18 |
|-------|---|---|-----|
| GPUs | 6 | 36 | 108 |
| Sequence Groups | 6 | 36 | 108 |
| Sequence Length | 366 | 900 | 1512 |
| Per GPU Mem. (GB) | 21.67 | 22.48 | 23.34 |
| FLOPS (x 10¹²flop/s) | 52 | 518 | 2010 |
| Self-Attn Comp Incr. | 1 | 2 | 4 |
| Parallel Efficiency | 100% | 94% | 92% |

(b) Baseline, nhóm song song dữ liệu = 1
| Nodes | 1 | 6 |
|-------|---|---|
| GPUs | 6 | 36 |
| Sequence Groups | 6 | 36 |
| Sequence Length | 366 | 900 |
| Per GPU Mem. (GB) | 25.28 | OOM |
| FLOPS (x 10¹²flop/s) | 23 | OOM |
| Self-Attn Comp Incr. | 1 | OOM |
| Parallel Efficiency | 100% | OOM |

long-document transformer, 2020.

Caldarini, G., Jaf, S., and McGarry, K. A literature survey of recent advances in chatbots. Information, 13(1):41, 2022.

Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and Ré, C. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34: 17413–17426, 2021a.

Chen, C.-F. R., Fan, Q., and Panda, R. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 357–366, New York, NY, USA, 2021b. IEEE.

Chen, R. J., Chen, C., Li, Y., Chen, T. Y., Trister, A. D., Krishnan, R. G., and Mahmood, F. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16123–16134, New York, NY, USA, 2022. IEEE. doi: 10.1109/CVPR52688.2022.01567.

Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers, 2019. URL https://arxiv.org/abs/1904.10509.

Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L., and Weller, A. Rethinking attention with performers. In The International Conference on Learning Representations (ICLR), New York, NY, USA, 2021. Association for Computing Machinery. doi: 10.48550/ARXIV.2009.14794. URL https://arxiv.org/abs/2009.14794.

Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS: Proceedings of the 35th Neural Information Processing Systems Conference, New York, NY, USA, 2022. Association for Computing Machinery. doi: 10.48550/ARXIV.2205.14135. URL https://arxiv.org/abs/2205.14135.

Dong, L., Xu, S., and Xu, B. Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5884–5888, New York, NY, USA, 2018. IEEE.

Hutter, M., Mahoney, M., and Bowery, J. 500'000 C prize for compressing human knowledge. http://prize. hutter1.net/, 2006. Accessed: 2023-03-05.

Jacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song, S. L., Rajbhandari, S., and He, Y. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models, 2023.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML'20, New York, NY, USA, 2020. Association for Computing Machinery.

Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. In The International Conference on Learning Representations (ICLR), New York, NY, USA, 2020. Association for Computing Machinery. doi: 10.48550/ARXIV.2001.04451. URL https://arxiv. org/abs/2001.04451.

Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B. Reducing activation recomputation in large transformer models, 2022. URL https://arxiv.org/abs/2205.05198.

Li, D., Shao, R., Xie, A., Xing, E. P., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. Lightseq: Sequence level parallelism for distributed training of long context transformers, 2023.

--- TRANG 12 ---
Ultra-Long Sequence Distributed Transformer

Li, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence parallelism: Long sequence training from system perspective, 2021. URL https://arxiv.org/abs/2105. 13120.

Mahoney, M. Enwik8 test data. https:// mattmahoney.net/dc/textdata.html, 2006. Accessed: 2023-03-05.

Rabe, M. N. and Staats, C. Self-attention does not need o(n2) memory, 2022.

Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient Content-Based Sparse Attention with Routing Transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 02 2021. ISSN 2307-387X. doi: 10.1162/tacl a 00353. URL https://doi.org/10. 1162/tacl_a_00353.

Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., and Kwok, J. T. Sparsebert: Rethinking the importance analysis in self-attention. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9547–9557, New York, NY, USA, 2021. PMLR. URL http://proceedings.mlr.press/v139/ shi21a.html.

Si, Y. and Roberts, K. Three-level hierarchical transformer networks for long-sequence and multiple clinical documents classification, 2021. URL https://arxiv. org/abs/2104.08444.

Strudel, R., Garcia, R., Laptev, I., and Schmid, C. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 7262–7272, New York, NY, USA, 2021. IEEE.

Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 331–335, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1032. URL https: //aclanthology.org/P19-1032.

Valanarasu, J. M. J., Oza, P., Hacihaliloglu, I., and Patel, V. M. Medical transformer: Gated axial-attention for medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, pp. 36–46, New York, NY, USA, 2021. Springer.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, pp. 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.

Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., and Chao, L. S. Learning deep transformer models for machine translation. In 57th Annual Meeting of the Association for Computational Linguistics, pp. 1810–1822, New York, NY, USA, 2019. Association for Computing Machinery. doi: 10.48550/ARXIV.1906.01787. URL https://arxiv.org/abs/1906.01787.

with Codes, P. Language modelling on enwik8. https://paperswithcode.com/sota/ language-modelling-on-enwiki8, 2022. Accessed: 2023-03-05.

Ying, C., Ke, G., He, D., and Liu, T.-Y. Lazyformer: Self attention with lazy update, 2021.

Yu, J., Li, J., Yu, Z., and Huang, Q. Multimodal transformer with multi-view visual representation for image captioning. IEEE Transactions on Circuits and Systems for Video Technology, 30(12):4467–4480, 2019.

Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. Megabyte: Predicting million-byte sequences with multiscale transformers, 2023.

Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297, 2020.
