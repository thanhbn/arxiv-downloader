I'll help you translate this PDF content to Vietnamese. Since this is a very long document, I'll start with the translation while maintaining the exact structure:

# 2305.04241.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2305.04241.pdf
# Kích thước tệp: 1770664 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
VCC: Mở rộng quy mô Transformers lên 128K token hoặc hơn
bằng cách ưu tiên các token quan trọng

Zhanpeng Zeng
University of Wisconsin, Madison
zzeng38@wisc.edu

Cole Hawkins
AWS AI
colehawk@amazon.com

Mingyi Hong
University of Minnesota, Minneapolis
mhong@umn.edu

Aston Zhang
AWS AI
astonz@amazon.com

Nikolaos Pappas
AWS AI
nppappa@amazon.com

Vikas Singh
University of Wisconsin, Madison
vsingh@biostat.wisc.edu

Shuai Zheng
AWS AI
shzheng@amazon.com

Tóm tắt
Transformers đóng vai trò trung tâm trong các ứng dụng xử lý ngôn ngữ tự nhiên và thị giác máy tính hiện đại. Mặc dù các nghiên cứu gần đây đã tập trung vào việc giảm chi phí bậc hai của các mô hình này (như một hàm của độ dài chuỗi), việc xử lý các chuỗi cực dài (ví dụ, với hơn 16K token) vẫn còn là thách thức. Các ứng dụng như trả lời câu hỏi dựa trên một cuốn sách hoặc tóm tắt một bài báo khoa học là không hiệu quả hoặc không khả thi. Ở đây, chúng tôi đề xuất cải thiện đáng kể hiệu quả của Transformers cho các chuỗi cực dài, bằng cách nén chuỗi thành một biểu diễn nhỏ hơn nhiều ở mỗi lớp. Cụ thể, bằng cách khai thác thực tế là trong nhiều tác vụ, chỉ một tập con nhỏ các token đặc biệt (chúng tôi gọi là VIP-tokens) có liên quan nhất đến dự đoán cuối cùng, chúng tôi đề xuất một sơ đồ nén tập trung vào VIP-token (VCC) nén có chọn lọc chuỗi dựa trên tác động của chúng đến việc xấp xỉ biểu diễn của các VIP-tokens. So với các baseline cạnh tranh, thuật toán của chúng tôi không chỉ hiệu quả (đạt được hơn 3× cải thiện hiệu quả so với baseline trên độ dài 4K và 16K), mà còn mang lại hiệu suất cạnh tranh/tốt hơn trên một số lượng lớn các tác vụ. Hơn nữa, chúng tôi cho thấy rằng thuật toán của chúng tôi có thể mở rộng quy mô lên 128K token (hoặc hơn) trong khi liên tục mang lại cải thiện độ chính xác.

1 Giới thiệu

10³ 10⁴ 10⁵
Độ dài chuỗi 0 500 1000 1500 Thời gian chạy (ms)

10³ 10⁴ 10⁵
Độ dài chuỗi 0 10 20 30 Bộ nhớ (GB)

Transformer Big Bird Longformer Của chúng tôi

Hình 1: Hiệu quả mô hình khi xử lý một chuỗi trên A100 khi độ dài chuỗi tăng (lưu ý trục x logarit).

Transformer [30] là một mô hình nền tảng cho xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính. Nó đã cho thấy hiệu suất đáng chú ý trên các ứng dụng NLP bao gồm dịch máy [30], suy luận ngôn ngữ [9], và tóm tắt [13]. Transformers cũng đã được áp dụng thành công cho các tác vụ nhận dạng thị giác khác nhau và đạt được kết quả ấn tượng [10,3,38]. Thật không may, nhu cầu thời gian chạy/bộ nhớ của Transformers có sự phụ thuộc bất lợi vào độ dài chuỗi đầu vào, làm cho việc sử dụng Transformers cho các ứng dụng chuỗi cực dài trở nên khó khăn. Do đó, nhiều Transformers sử dụng các chiến lược như cắt bớt để đảm bảo rằng độ dài câu đầu vào không quá 512, ví dụ, BERT, T5, và các mô hình ngôn ngữ dựa trên Transformer khác [33,21,26]. Thật không may, việc cắt bớt như vậy, và các chiến lược liên quan khác, không thể tránh khỏi dẫn đến mất độ chính xác, mức độ có thể khác nhau từ tác vụ/bộ dữ liệu này sang tác vụ/bộ dữ liệu khác. Do đó, cải thiện hiệu quả cho độ dài chuỗi đầu vào dài hơn là một trọng tâm chính của nhiều đề xuất. Những phát triển này là các cột mốc quan trọng, và chúng đã giảm sự phụ thuộc bậc hai vào độ dài chuỗi xuống tuyến tính [5,24,32,2,35,37,36]. Bây giờ, nhiều mô hình Transformer có thể xử lý các mẫu với độ dài chuỗi lên đến 4K (hoặc tối đa 16K). Gần đây, một số báo cáo về các mô hình mới hơn có thể xử lý các chuỗi dài hơn nhiều đã xuất hiện.

Lý do. Tự nhiên là phải hỏi liệu khả năng xử lý các chuỗi dài hơn có đáng công sức hay không. Câu trả lời ngắn gọn là có. Độ chính xác được cải thiện đã được báo cáo trên các tác vụ chuỗi dài [2,35,13]. Vậy, điều gì đang ngăn cản chúng ta thu hoạch những lợi ích mạnh mẽ hơn nữa về độ chính xác bằng cách đưa các chuỗi thậm chí còn dài hơn vào các mô hình như vậy? Các mô hình như Longformer [2] và Big Bird [35] trở nên chậm và tiêu tốn một lượng bộ nhớ quá mức khi độ dài chuỗi tiếp tục tăng. Xem Hình 1 để minh họa. Tại sao? Việc cập nhật biểu diễn của mỗi token liên quan đến việc tính toán attention hiệu quả cho chuỗi và mạng feed-forward ở mỗi lớp. Điều này gây ra chi phí tuyến tính trên độ dài chuỗi và tốn kém cho các chuỗi dài hơn nhiều so với 4K (hoặc 16K) token. Để cung cấp cho các mô hình khả năng học phụ thuộc tầm xa cực dài, chúng ta cần giảm chi phí này. Những gì chúng tôi mô tả trong bài báo này là một bước cụ thể tiến lên - dựa trên một số giả định cụ thể theo tác vụ mà dường như thường xuyên đúng, chúng tôi phác thảo một công thức hoạt động và mang lại những cải thiện mong đợi.

(1) Tập trung vào những gì chúng ta cần cho một tác vụ: Nén tập trung vào VIP-token (VCC). Chúng tôi đưa ra giả thuyết/phát hiện rằng trong nhiều tác vụ mà Transformers hiệu quả, chỉ một tập con nhỏ các token (mà chúng tôi gọi là VIP-tokens) có liên quan đến đầu ra cuối cùng (và độ chính xác) của một Transformer. Nếu những token này đã được xác định bằng cách nào đó, chúng ta có thể bảo tồn thông tin này một cách toàn vẹn và chỉ chịu mất mát hiệu suất vừa phải. Bây giờ, với điều kiện trên những VIP-tokens cụ thể này, một nén tích cực trên các non-VIP-tokens khác, có thể phục vụ để giảm (và thường là, phục hồi hoàn toàn) mất mát hiệu suất trong khi giảm đáng kể độ dài chuỗi. Nén này phải tận dụng thông tin liên quan đến các VIP-tokens, với mục tiêu cải thiện việc xấp xỉ biểu diễn của các VIP-tokens. Nói cách khác, một xấp xỉ độ trung thực cao của toàn bộ chuỗi là không cần thiết. Một khi đầu vào "nén có chọn lọc" này đi qua một lớp Transformer, chuỗi đầu ra được giải nén thành chuỗi đầy đủ ban đầu cho phép các lớp tiếp theo truy cập vào chuỗi đầy đủ.

(2) Cấu trúc dữ liệu chuyên biệt cho nén/giải nén. Một vấn đề thứ hai, nhưng quan trọng về mặt thực tế, là giảm chi phí phụ khi nén/giải nén các chuỗi đầu vào/đầu ra bên trong mạng. Bỏ qua vấn đề này sẽ ảnh hưởng đến hiệu quả. Chúng tôi đưa ra một cấu trúc dữ liệu đơn giản nhưng chuyên biệt để duy trì các trạng thái ẩn của các lớp trung gian, nơi việc nén có thể dễ dàng được truy cập từ cấu trúc dữ liệu, và việc giải nén rõ ràng có thể được tránh bằng cách cập nhật cấu trúc dữ liệu: chuỗi không bao giờ được vật chất hóa đầy đủ trong các lớp trung gian.

Đóng góp thực tế. Ngoài các mô-đun thuật toán ở trên, chúng tôi cho thấy rằng mặc dù nén tích cực các chuỗi đầu vào, chúng tôi đạt được hiệu suất tốt hơn/cạnh tranh trên một rổ rộng các thí nghiệm chuỗi dài. So với baseline, chúng tôi có hiệu quả thời gian chạy/bộ nhớ tốt hơn nhiều. Chúng tôi cho thấy rằng bây giờ việc chạy các mô hình Transformer tiêu chuẩn trên các chuỗi có độ dài 128K token là khả thi, với những lợi ích hiệu suất nhất quán (và không có thay đổi kiến trúc phức tạp).

2 Kiến thức cơ bản

Chúng tôi xem xét lớp Transformer, các nghiên cứu liên quan về Transformers hiệu quả và định nghĩa các ký hiệu/đơn giản hóa. Các chữ cái IN HOA đậm biểu thị ma trận, các chữ cái thường đậm biểu thị vector, và các chữ cái thường thông thường biểu thị vô hướng.

Tóm tắt ngắn gọn về Mô hình Transformer. Cố định n là độ dài chuỗi và để d là chiều nhúng. Định nghĩa một ma trận nhúng X∈R^(n×d) cho n vector đặc trưng đầu vào cho một lớp Transformer. Đầu ra của lớp Transformer này, X_new, được định nghĩa là

X_new = β(α(X,X,X) + X) + α(X,X,X) + X                (1)

trong đó α(·,·,·) là multi-head attention (MHA) và β(·) là một mạng feed-forward (FFN). Layer norm [1] được bỏ qua để giảm sự lộn xộn. Để các đầu vào cho α(·,·,·) là Q,K,V∈R^(n×d) cho queries, keys, và values. MHA được định nghĩa là:

α(Q,K,V) := cat^g_{i=1} [softmax(QW^{Q,i}W^T_{K,i}K^T)VW^{V,i}] W    (2)

[Tôi sẽ tiếp tục dịch phần còn lại của tài liệu, nhưng do độ dài rất lớn, tôi sẽ chia thành nhiều phần để đảm bảo chất lượng dịch.]
