# LLoCO: Học Ngữ Cảnh Dài Ngoại Tuyến
Sijun Tan*, Xiuyu Li*, Shishir Patil, Ziyang Wu, Tianjun Zhang,
Kurt Keutzer, Joeseph E. Gonzalez, Raluca Ada Popa
UC Berkeley
{sijuntan,xiuyu}@berkeley.edu

## Tóm tắt
Xử lý ngữ cảnh dài vẫn là một thách thức đối với các mô hình ngôn ngữ lớn (LLM) do chi phí tính toán và bộ nhớ bậc hai của cơ chế tự chú ý và kích thước KV cache đáng kể trong quá trình sinh. Chúng tôi đề xuất LLoCO, một phương pháp mới để giải quyết vấn đề này bằng cách học ngữ cảnh ngoại tuyến thông qua nén ngữ cảnh và tinh chỉnh hiệu quả tham số trong miền với LoRA. Phương pháp của chúng tôi cho phép LLM tạo ra một biểu diễn súc tích của ngữ cảnh ban đầu và truy xuất thông tin liên quan một cách hiệu quả để trả lời câu hỏi chính xác. Phương pháp của chúng tôi mở rộng cửa sổ ngữ cảnh hiệu quả của mô hình LLaMA2-7B 4k token để xử lý lên đến 128k token. Chúng tôi đánh giá phương pháp của mình trên một số bộ dữ liệu hỏi đáp ngữ cảnh dài, chứng minh rằng LLoCO vượt trội đáng kể so với học trong ngữ cảnh trong khi sử dụng ít hơn 30× token trong quá trình suy luận. LLoCO đạt được tăng tốc lên đến 7.62× trong suy luận và thông lượng cao hơn 11.52× trong tinh chỉnh, giảm đáng kể chi phí hỏi đáp tài liệu dài. Điều này làm cho nó trở thành một giải pháp hứa hẹn cho xử lý ngữ cảnh dài hiệu quả.1

## 1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện hiệu suất đáng chú ý trên nhiều tác vụ khác nhau (Touvron et al., 2023; Jiang et al., 2023a). Nhiều tác vụ này yêu cầu LLM hiểu và lý luận về ngữ cảnh dài. Ví dụ, hỏi đáp tài liệu là một trong những ứng dụng phổ biến nhất của LLM, nơi mô hình được trình bày với một tài liệu làm ngữ cảnh và được yêu cầu phản hồi các câu hỏi liên quan hoặc tóm tắt văn bản. Những tài liệu này có thể từ các bài viết dài đến toàn bộ cuốn sách, có thể vượt quá giới hạn cửa sổ ngữ cảnh của LLM. Do đó, có một xu hướng ngày càng tăng trong cả học thuật và công nghiệp để nâng cao khả năng của LLM xử lý ngữ cảnh dài hơn một cách hiệu quả (Chen et al., 2023b; Jiang et al., 2023a; Peng et al., 2024; Chen et al., 2024). Nhu cầu này đã thúc đẩy sự đổi mới giữa các nhà cung cấp LLM như OpenAI và Anthropic để phát triển các mô hình có thể xử lý các văn bản ngày càng dài bao gồm hàng nghìn token.

Mặc dù có tiến bộ ấn tượng từ các nhà cung cấp mô hình LLM, việc mở rộng quy mô các mô hình này để quản lý ngữ cảnh mở rộng một cách thành thạo vẫn là một thách thức to lớn, cả về mặt kỹ thuật và tài chính. Do cơ chế tự chú ý, các LLM dựa trên transformer phát sinh chi phí tính toán và bộ nhớ bậc hai khi độ dài chuỗi tăng. Nhiều tác vụ ngữ cảnh dài yêu cầu tái sử dụng cùng một ngữ cảnh nhiều lần, điều này phát sinh thêm chi phí độ trễ và chi phí đáng kể, vì hầu hết các LLM thương mại hoạt động trên mô hình định giá gắn trực tiếp với số lượng token được xử lý. Ví dụ, một lần chạy suy luận duy nhất với một tài liệu có 100k token sẽ tốn 1.5 USD trên Claude 3 Opus2 và 1 USD trên GPT-4-turbo3.

Trực giao với những nỗ lực thú vị trong việc mở rộng giới hạn cửa sổ ngữ cảnh, nghiên cứu của chúng tôi giới thiệu một chiến lược sáng tạo để giải quyết thách thức ngữ cảnh dài. Chúng tôi đề xuất một phương pháp nơi thông tin ngữ cảnh được nén ngoại tuyến thông qua tinh chỉnh, cho phép mô hình cung cấp phản hồi chính xác trong quá trình suy luận với các biểu diễn ngữ cảnh được tinh gọn. Để minh họa ý tưởng của chúng tôi, hãy xem xét một phép tương tự: hình dung một LLM như một sinh viên chuẩn bị cho kỳ thi, nơi chúng tôi, các nhà nghiên cứu, là những người thi hành cung cấp tài liệu học tập và câu hỏi. Học trong ngữ cảnh truyền thống với ngữ cảnh đầy đủ hoặc Sinh tăng cường Truy xuất (RAG) giống như một kỳ thi mở sách, nơi LLM có quyền truy cập vào tất cả tài liệu khi trả lời câu hỏi. Ngược lại, phương pháp của chúng tôi giống như một kỳ thi bán đóng sách, nơi LLM không thể mang toàn bộ cuốn sách nhưng được phép mang một tờ phao thi. Để xuất sắc trong kỳ thi, sinh viên phải 1) học một cách hiệu quả để chưng cất một tờ phao thi súc tích nhưng có thông tin, và 2) truy xuất thông tin liên quan từ tờ phao thi một cách hiệu quả để trả lời chính xác các câu hỏi thi. Cụ thể, 1) Làm thế nào chúng ta có thể huấn luyện một mô hình để tạo ra một biểu diễn compact của ngữ cảnh ban đầu mà LLM có thể diễn giải và sử dụng hiệu quả? 2) Làm thế nào để cho phép LLM điều hướng thành thạo và trích xuất chi tiết phù hợp từ biểu diễn này trong quá trình suy luận?

Liên quan đến câu hỏi đầu tiên, chúng tôi thấy rằng công thức gần giống với hướng nghiên cứu hiện có về nén ngữ cảnh. Các công trình trước đây (Chevalier et al., 2023; Mu et al., 2023; Ge et al., 2023; Yen et al., 2024) đã đề xuất các bộ nén nhằm chưng cất bản chất của các văn bản gốc thành các biểu diễn compact được căn chỉnh với LLM. Công trình của chúng tôi chủ yếu tập trung vào câu hỏi thứ hai – chúng tôi đã quan sát thấy rằng mặc dù có tiến bộ trong nén ngữ cảnh, LLM thường gặp khó khăn để đọc chính xác những "tờ phao thi" như vậy và có xu hướng ảo giác khi áp dụng chúng để trả lời truy vấn. Để giải quyết vấn đề này, chúng tôi sử dụng tinh chỉnh hiệu quả tham số trong miền trực tiếp trên ngữ cảnh được nén (tờ phao thi) mà không thay đổi nội dung của nó, điều này cải thiện đáng kể khả năng của LLM để trích xuất và sử dụng thông tin từ những biểu diễn nén này một cách chính xác. Bằng cách này, chúng ta có thể điều khiển LLM để xử lý ngữ cảnh dài hiệu quả và chính xác hơn. Phương pháp này cho phép chúng tôi mở rộng cửa sổ ngữ cảnh hiệu quả của mô hình LLaMA2-7B 4k để xử lý lên đến 128k token. Hơn nữa, chúng tôi đạt được kết quả tiên tiến phù hợp hoặc thậm chí vượt qua hiệu suất của mô hình LLaMA2-7B-32k với ngữ cảnh đầy đủ trên các benchmark ngữ cảnh dài, trong khi sử dụng ít hơn 30× token.

Hiểu biết này đã dẫn chúng tôi giới thiệu LLoCO, một pipeline học ngữ cảnh ngoại tuyến thông qua kết hợp nén ngữ cảnh và tinh chỉnh hiệu quả tham số. Nó bao gồm ba giai đoạn: tiền xử lý, tinh chỉnh và phục vụ. Đầu tiên, chúng tôi tiền xử lý các tài liệu thành "tờ phao thi". Sau đó, chúng tôi sử dụng Thích ứng Hạng thấp (LoRA) (Hu et al., 2022) để thực hiện tinh chỉnh hiệu quả tham số trên những "tờ phao thi" này theo nhóm. Đối với thiết kế hệ thống phục vụ, chúng tôi sử dụng một bộ truy xuất RAG tiêu chuẩn để truy xuất tài liệu nén cũng như mô-đun LoRA liên quan nhất, và áp dụng chúng vào LLM để suy luận. Các đóng góp của công trình chúng tôi có thể được tóm tắt như sau:

• Chúng tôi giới thiệu một phương pháp mới để mô hình hóa hiệu quả ngữ cảnh dài bằng cách kết hợp nén ngữ cảnh với tinh chỉnh hướng dẫn. Phương pháp của chúng tôi mở rộng cửa sổ ngữ cảnh của mô hình LLaMA2-7B 4k để xử lý lên đến 128k token, đạt được hiệu suất vượt trội đáng kể so với học trong ngữ cảnh trong khi sử dụng ít hơn 30× token.

• Chúng tôi đề xuất LLoCO, một framework mới kết hợp nén ngữ cảnh, truy xuất và tinh chỉnh hiệu quả tham số. Pipeline này có thể được triển khai để tăng tốc đáng kể và giảm chi phí hỏi đáp tài liệu dài. Với ngữ cảnh nén trong quá trình suy luận và tinh chỉnh hướng dẫn, chúng tôi chứng minh rằng LLoCO đạt được tăng tốc 7.62× về độ trễ suy luận và thông lượng cao hơn 11.52× so với tinh chỉnh trên ngữ cảnh gốc.

## 2 Công trình liên quan

**LLM ngữ cảnh dài** Gần đây, đã có những nỗ lực để tăng kích thước cửa sổ ngữ cảnh của LLM một cách hiệu quả với pretraining hoặc tinh chỉnh liên tục. Một dòng công trình tập trung vào việc mở rộng quy mô Rotary Position Embeddings (RoPE) (Su et al., 2021), đạt được ngữ cảnh dài hơn lên đến 128k (Chen et al., 2023b, 2024; Peng et al., 2024). Mistral (Jiang et al., 2023a) đề xuất sliding window attention chỉ chú ý đến một phần token từ lớp trước, giảm tính toán và cho phép pretraining với ngữ cảnh dài lên đến 30k. Tuy nhiên, vì việc sinh tự hồi quy trong LLM chủ yếu bị giới hạn bởi bộ nhớ (Kwon et al., 2023), việc lưu trữ KV cache của ngữ cảnh dài hơn làm chậm suy luận và yêu cầu VRAM GPU lớn.

**Nén ngữ cảnh** Một chủ đề liên quan chặt chẽ là nén ngữ cảnh, nhằm huấn luyện một bộ nén tổng quát có thể nén bất kỳ prompt đầu vào nào. GIST (Mu et al., 2023), AutoCompressor (Chevalier et al., 2023), và ICAE (Ge et al., 2023) tinh chỉnh LLM theo cách "soft prompt-tuning", áp dụng regularization cụ thể trong attention mask hoặc sử dụng các "memory token" chuyên dụng để nén ngữ cảnh thành embeddings với độ dài ngắn hơn đáng kể. Họ LLMLingua (Jiang et al., 2023c,b; Pan et al., 2024) đề xuất một framework nhận biết câu hỏi để nén prompt bằng ngôn ngữ tự nhiên cho các API LLM black-box. Một dòng công trình khác sử dụng nén KV cache bằng cách loại bỏ (Zhang et al., 2024b; Xiao et al., 2024; Li et al., 2024; Tang et al., 2024), chỉ giữ các key và value có thông tin cho việc sinh trong suy luận, hoặc lượng tử hóa (Sheng et al., 2023b; Liu et al., 2024c; Hooper et al., 2024). Tuy nhiên, những phương pháp trước đó nhằm nén bất kỳ đầu vào nào, thường gây ra sụt giảm hiệu suất nhanh chóng khi tỷ lệ nén vượt quá một giới hạn nhất định (ví dụ 4×), đặc biệt đối với văn bản ngoài phân phối. Ngoài ra, những phương pháp này có thể yêu cầu triển khai kernel tùy chỉnh để đạt được tăng tốc thời gian thực, hạn chế việc sử dụng thực tế. Ngược lại, công trình của chúng tôi tập trung vào nén cực độ của tài liệu trong phân phối, đạt được lên đến 30× nén mà không cần kernel tùy chỉnh.

**Sinh tăng cường Truy xuất** Truy xuất nâng cao hiệu suất của LLM trong các tác vụ chuyên sâu kiến thức như hỏi đáp với tài liệu dài hoặc trong miền mở. Các kỹ thuật RAG hiệu quả cả trong các tình huống tinh chỉnh và khi áp dụng cho LLM có sẵn (Guu et al., 2020; Lewis et al., 2020; Zhang et al., 2024a), và đã có nhiều tiến bộ gần đây, nhấn mạnh việc cải thiện tích hợp kiến thức bên ngoài, hỗ trợ truy vấn rộng hơn và nâng cao độ rõ ràng nội dung (Jiang et al., 2023d; Schick et al., 2023; Asai et al., 2023). Mặc dù có hiệu suất cải thiện từ truy xuất, vẫn còn thách thức về hiệu quả thời gian chạy (Mallen et al., 2022) và lọc hiệu quả các phần không liên quan (Shi et al., 2023b; Xu et al., 2024). Phương pháp đề xuất của chúng tôi mở ra cơ hội mới để nắm bắt thông tin quan trọng trong khi đảm bảo sinh hiệu quả.

**Tinh chỉnh hiệu quả Tham số** Một dòng phương pháp (Hu et al., 2022; Lester et al., 2021; Shi et al., 2023a; Liu et al., 2024b) đóng băng phần lớn trọng số mô hình và chỉ cập nhật một tập con nhỏ để tinh chỉnh các mô hình lớn một cách hiệu quả. LoRA (Hu et al., 2022) là một trong những kỹ thuật được áp dụng rộng rãi nhất, và những tiến bộ gần đây (Sheng et al., 2023a; Chen et al., 2023a) đã tập trung vào việc nâng cao hiệu suất hệ thống để triển khai các adaptor LoRA. Đặc biệt, Sheng et al. (2023a) đã đạt được việc triển khai hàng nghìn instance LoRA trên một GPU NVIDIA A100 duy nhất. LLoCO, sử dụng LoRA để tinh chỉnh, có thể hưởng lợi từ những cải tiến đó để triển khai các adaptor LoRA một cách hiệu quả trong các tình huống ngữ cảnh dài.

## 3 Phương pháp

### 3.1 Tổng quan Kiến trúc

Hình 1 minh họa kiến trúc LLoCO được đề xuất, bao gồm hai thành phần: một bộ mã hóa ngữ cảnh và một bộ giải mã LLM. Trong một LLM tinh chỉnh hướng dẫn điển hình, các prompt có thể được phân loại thành prompt hệ thống, người dùng và trợ lý. Prompt hệ thống chứa hướng dẫn tác vụ và quy tắc mà LLM nên tuân theo, cũng như ngữ cảnh liên quan. Prompt người dùng là một câu hỏi được đặt ra bởi người dùng, và prompt trợ lý là câu trả lời được sinh bởi LLM. Thông tin ngữ cảnh trong prompt hệ thống có thể bao gồm lịch sử tương tác của người dùng hoặc các tài liệu liên quan đến câu hỏi của người dùng. Những ngữ cảnh này có thể trở nên rất dài, có thể vượt quá giới hạn cửa sổ ngữ cảnh của LLM.

Để vượt qua giới hạn cửa sổ ngữ cảnh, chúng tôi đề xuất sử dụng một bộ mã hóa ngữ cảnh để nén ngữ cảnh dài ban đầu thành một biểu diễn compact hơn nhiều. Bộ mã hóa ngữ cảnh bản thân là một mô hình ngôn ngữ nhận một chuỗi token làm đầu vào và xuất ra một chuỗi token embeddings. Những token embedding đầu ra này, mà chúng tôi gọi là summary embeddings, nên ngắn hơn đáng kể so với ngữ cảnh gốc. Summary embeddings sau đó được đặt trước bộ giải mã LLM và phục vụ như prompt hệ thống của LLM. Prompt của người dùng được xử lý bình thường thông qua tokenizer của bộ giải mã LLM, và LLM sinh câu trả lời (prompt trợ lý) dựa trên summary embeddings và prompt người dùng.

Trong thiết kế của chúng tôi, bộ mã hóa ngữ cảnh có thể là bất kỳ mô hình nào có khả năng tạo ra một biểu diễn compact được căn chỉnh với bộ giải mã LLM. Summary embeddings có thể được xem như pseudo-words trong không gian text embedding của bộ giải mã LLM, đại diện cho các khái niệm hoặc tóm tắt trừu tượng. Đối với các thí nghiệm của chúng tôi, chúng tôi chọn AutoCompressor cho LLaMA2-7B (Chevalier et al., 2023), một bộ nén ngữ cảnh được tinh chỉnh trên LLaMA2-7B với khả năng kép sinh summary token từ ngữ cảnh dài và thực hiện hoàn thành văn bản dựa trên summary token. Bộ nén nhóm tài liệu thành các chunk 1536 token và nén đệ quy mỗi chunk thành 50 summary embeddings. Để đảm bảo căn chỉnh giữa summary embeddings và decoder LLM, chúng tôi chọn LLaMA2-7B làm decoder LLM của chúng tôi nhưng sử dụng trọng số tinh chỉnh của AutoCompressor làm trọng số mô hình.

Mặc dù có các bộ nén khác có sẵn cho LLaMA2-7B (Ge et al., 2023; Mu et al., 2023), chúng tôi thấy AutoCompressor phù hợp nhất cho các trường hợp sử dụng dự định của chúng tôi vì nó có thể 1) hỗ trợ nén ngữ cảnh rất dài do quy trình huấn luyện đệ quy của nó, và 2) đạt được tỷ lệ nén tuyệt vời 30:1. Chúng tôi coi việc xây dựng bộ mã hóa ngữ cảnh (bộ nén) là một hướng nghiên cứu quan trọng và trực giao. Phát triển các bộ mã hóa ngữ cảnh hiệu quả và phổ quát hơn theo cách hợp lý có thể nâng cao thêm hiệu quả và hiệu suất của LLoCO, biểu thị công việc tương lai quan trọng.

### 3.2 Pipeline cho Học Ngữ cảnh Ngoại tuyến

Pipeline của LLoCO bao gồm hai giai đoạn chính: giai đoạn tiền xử lý và giai đoạn tinh chỉnh. Chúng tôi cũng phác thảo một giai đoạn phục vụ cho triển khai thực tế trong thiết kế hệ thống.

**Tiền xử lý** Đầu tiên, chúng tôi sử dụng một giai đoạn tiền xử lý tận dụng bộ mã hóa ngữ cảnh của chúng tôi để xử lý các tài liệu gốc. Vì AutoCompressor được sử dụng làm bộ mã hóa ngữ cảnh của chúng tôi, chúng tôi tuân theo thực hành của nó là chia tài liệu thành các chunk 1536 token và truyền chúng qua bộ mã hóa ngữ cảnh. Bộ mã hóa ngữ cảnh xuất ra 50 summary embeddings cho mỗi chunk một cách đệ quy, với mỗi embedding có chiều 4096.

Giai đoạn tiền xử lý của chúng tôi được thiết kế để có thể mở rộng, cho phép tích hợp liền mạch vào hệ thống sinh tăng cường truy xuất (RAG) cho QA tài liệu. Trong một hệ thống RAG điển hình, tiền xử lý bao gồm xây dựng cơ sở dữ liệu vector để lập chỉ mục một tập hợp tài liệu, nơi tài liệu được chia thành các đoạn, và mỗi đoạn được liên kết với một key— một sentence embedding được tạo bởi một mô hình text embedding. Thiết kế của LLoCO cho phép summary token embeddings được lưu trữ cùng với các đoạn gốc trong cơ sở dữ liệu vector, được lập chỉ mục bằng cùng một key đoạn.

**Tinh chỉnh** Trong giai đoạn tinh chỉnh, trước tiên chúng tôi phân đoạn tài liệu thành các nhóm dựa trên loại của chúng (ví dụ: bài báo học thuật, tin tức) hoặc các tác vụ mà người dùng muốn thực hiện (ví dụ: QA, tóm tắt). Đối với mỗi nhóm tài liệu, chúng tôi thực hiện tinh chỉnh hiệu quả tham số bằng cách sử dụng một adaptor LoRA. Dữ liệu tinh chỉnh có thể là các cặp hướng dẫn trong miền được cung cấp bởi nhà cung cấp mô hình. Nếu bộ dữ liệu như vậy không tồn tại, nó cũng có thể được tạo ra bằng các kỹ thuật self-instruct (Wang et al., 2022) hoặc chưng cất từ một mô hình mạnh hơn như GPT-4. Ở cuối quá trình tinh chỉnh, chúng tôi có được một adaptor LoRA tinh chỉnh cho mỗi nhóm tài liệu. Trong thiết kế hệ thống cơ sở dữ liệu vector, mỗi mục đoạn sẽ bao gồm một định danh cho mô-đun LoRA tương ứng. Ngoài ra, một cơ sở dữ liệu riêng biệt sẽ được thiết lập để lưu trữ tất cả các adaptor.

Cụ thể, cho trước summary token Xm và các cặp hướng dẫn {X1q, X1a, ..., XLq, XLa} có độ dài L cho một nhóm tài liệu g, chúng tôi nhằm tối đa hóa xác suất sinh Xa được định nghĩa là:

p(Xa|Xm,Xq) = ∏Li=1 pθg(Xia|Xm,Xiq) (1)

trong đó θg biểu thị trọng số LoRA cho nhóm g.

**Phục vụ** Chúng tôi cũng thiết kế một giai đoạn phục vụ có thể được mở rộng tự nhiên thành một hệ thống RAG, nơi chúng tôi tận dụng một bộ truy xuất RAG tiêu chuẩn để truy xuất các tài liệu liên quan, nhưng thay vào đó đặt trước các compressed embeddings của những tài liệu này vào bộ giải mã LLM. Ngoài ra, chúng tôi áp dụng mô-đun LoRA liên quan nhất vào mô hình. Độc giả có thể tham khảo Phụ lục E để biết thêm chi tiết.

## 4 Thí nghiệm

Trong phần thí nghiệm, chúng tôi nhằm điều tra các khía cạnh sau của LLoCO: (1) hiệu quả của nó trong việc hiểu ngữ cảnh dài nén, (2) mức độ mà summary embeddings có thể bảo tồn thông tin cần thiết, và (3) cải thiện hiệu quả về chi phí hệ thống liên quan.

**Bộ dữ liệu** Chúng tôi chọn bốn bộ dữ liệu dành riêng cho các tác vụ hỏi đáp— QuALITY (Pang et al., 2021), Qasper (Dasigi et al., 2021), NarrativeQA (Kočisk`y et al., 2018), và HotpotQA (Yang et al., 2018)—cùng với một bộ cho tóm tắt, QMSum (Zhong et al., 2021). Tất cả bộ dữ liệu đều có tài liệu dài làm ngữ cảnh. Đối với tất cả các bộ dữ liệu, chúng tôi sử dụng tập validation của chúng để đánh giá. Chúng tôi tuân theo các metric chính thức cho mỗi bộ dữ liệu. Đối với QuALITY, chúng tôi báo cáo điểm exact match (EM). Đối với QMSum, chúng tôi báo cáo trung bình nhân của điểm ROUGE. Đối với các bộ dữ liệu còn lại (Qasper, NarrativeQA và HotpotQA), chúng tôi báo cáo điểm F1. Chi tiết thêm về các bộ dữ liệu này có thể được tìm thấy trong Phụ lục B.

**Cấu hình Mô hình** Trong nghiên cứu này, chúng tôi xem xét hai mô hình cơ sở. Đầu tiên là LLaMA2-7B-chat gốc (Touvron et al., 2023), có cửa sổ ngữ cảnh 4096 token. Thứ hai là Longchat-7b-v1.5-32k (Li et al., 2023), một mô hình LLaMA2 tinh chỉnh với cửa sổ ngữ cảnh mở rộng 32.000 token thông qua interpolation vị trí. Từ giờ, chúng tôi sẽ sử dụng LLaMA2-7B-4k để chỉ mô hình trước và LLaMA2-7B-32k để biểu thị mô hình sau. Trừ khi được chỉ định khác, chúng tôi đặt hạng LoRA là 8 cho các thí nghiệm của chúng tôi. Tất cả các mô hình LLoCO được tinh chỉnh trên AutoCompressor, bản thân nó được tinh chỉnh trên LLaMA2-7B.

### 4.1 QA Tài liệu Dài

Để đánh giá hiệu quả của LLoCO trên các bộ dữ liệu ngữ cảnh dài nói trên, chúng tôi xem xét các tình huống sau:

1. **LLaMA-2-7B-4k/32k/128k với Ngữ cảnh Gốc.** Đây là cài đặt baseline nơi chúng tôi cung cấp cho LLM tài liệu ground truth. Chúng tôi cắt tài liệu nếu độ dài của chúng vượt quá giới hạn cửa sổ ngữ cảnh.

2. **LLaMA-2-7B-4k/32k với Truy xuất.** Truy xuất là một phương pháp nén baseline tiêu chuẩn cho hỏi đáp tài liệu dài. Đối với mỗi tài liệu, chúng tôi chia nó thành các đoạn 512 token và sử dụng Contriever (Izacard et al., 2021) để truy xuất top 5 đoạn từ tài liệu này và nối chúng lại để tạo thành ngữ cảnh.

3. **AutoCompressor.** Trong cài đặt này, chúng tôi sử dụng AutoCompressor (Chevalier et al., 2023) để nén ngữ cảnh thành summary embeddings và đặt trước chúng vào LLM, mà không thực hiện bất kỳ tinh chỉnh trong miền nào, tương đương với việc sử dụng nó ngay từ hộp. AutoCompressor nén một chunk 1536 token thành 50 summary token, dẫn đến cửa sổ ngữ cảnh hiệu quả khoảng 128k token. Chúng tôi không cắt ngữ cảnh trừ khi nó vượt quá giới hạn này.

4. **LLoCO (của chúng tôi).** LLoCO là hệ thống đề xuất của chúng tôi cho hỏi đáp tài liệu dài. Đối với mỗi bộ dữ liệu chúng tôi đánh giá, chúng tôi thực hiện tinh chỉnh hướng dẫn sử dụng các cặp hỏi-đáp từ tập huấn luyện. Trong cả tinh chỉnh và suy luận, chúng tôi đặt trước summary embeddings của tài liệu ground truth tương ứng vào LLM. Chúng tôi không cắt ngữ cảnh trừ khi nó vượt quá giới hạn cửa sổ ngữ cảnh 128k.

Kết quả của chúng tôi được tóm tắt trong Bảng 1. Khi AutoCompressor được sử dụng mà không tinh chỉnh trong miền, hiệu suất của nó đôi khi kém hơn baseline nơi không có ngữ cảnh nào được thêm vào. Tuy nhiên, bằng cách kết hợp nén và tinh chỉnh, LLoCO vượt trội đáng kể so với baseline trên tất cả bộ dữ liệu, sử dụng ít hơn 30× token.

Đối với các bộ dữ liệu QuALITY, Qasper, QMSum và HotpotQA, hầu hết các mẫu vừa với giới hạn 32k token của mô hình LLaMA2-7B-32k mà không bị cắt. Ở đây, LLoCO không có lợi thế ngữ cảnh dài hơn nhưng vẫn sử dụng ít hơn 30× token so với baseline trong khi đạt được hiệu suất tốt hơn. Điều này cho thấy tinh chỉnh trong miền nâng cao khả năng của mô hình để diễn giải compressed embeddings và trả lời câu hỏi. Thống kê độ dài chuỗi chi tiết có trong Bảng 6 trong Phụ lục B.

Đối với bộ dữ liệu NarrativeQA, độ dài tài liệu trung bình là 84.770 token, vượt quá giới hạn ngữ cảnh LLaMA2-7B-32k. LLoCO nén những ngữ cảnh này xuống khoảng 2.600 token, cho phép sử dụng ngữ cảnh toàn diện cho hỏi đáp. Để đánh giá tác động của độ dài ngữ cảnh và đảm bảo so sánh công bằng, chúng tôi tiến hành một nghiên cứu loại bỏ với hai cấu hình mới: (1) tài liệu được cắt xuống 32k token trước khi nén và tinh chỉnh LLoCO trên các embeddings kết quả, và (2) thử nghiệm một LLaMA2-7B khác được mở rộng cho cửa sổ ngữ cảnh 128k với khả năng ngữ cảnh dài tiên tiến từ (Fu et al., 2024). Kết quả của chúng tôi trong Bảng 2 cho thấy LLoCO hoạt động tốt hơn LLaMA-7B ở cả độ dài ngữ cảnh 32k và 128k.

### 4.2 Nghiên cứu Loại bỏ

Trong phần này, chúng tôi trình bày các nghiên cứu loại bỏ so sánh LLoCO dưới các thiết lập khác nhau, bao gồm tinh chỉnh LLaMA với ngữ cảnh gốc và đánh giá các tỷ lệ nén khác nhau. Các nghiên cứu bổ sung, như khám phá các bộ mã hóa ngữ cảnh thay thế, được cung cấp trong Phụ lục D.

**Tinh chỉnh LLaMA với Ngữ cảnh Gốc** Một câu hỏi thú vị là tinh chỉnh LoRA hoạt động như thế nào trên ngữ cảnh gốc không nén, và điều đó so sánh với phương pháp của LLoCO như thế nào. Để điều tra điều này, chúng tôi tiến hành các thí nghiệm tinh chỉnh bổ sung trên LLaMA2-7B-4k/32k, nơi chúng tôi thêm ngữ cảnh gốc không nén như một prompt hệ thống trong quá trình tinh chỉnh. Những mô hình này được tinh chỉnh theo cùng thiết lập như LLoCO. Chúng tôi không khám phá tác động của tinh chỉnh LLaMA2-7B-128k với ngữ cảnh không nén do hạn chế về tài nguyên tính toán.

Như được hiển thị trong Bảng 3, cả LLaMA-7B-4k và LLaMA-7B-32k đều thể hiện cải thiện hiệu suất đáng chú ý sau tinh chỉnh, với mức tăng lần lượt là 2.88% và 6.62%. Mặc dù có những cải thiện này, hiệu suất của LLoCO vẫn có thể so sánh với mô hình LLaMA-7B-32k tinh chỉnh. Phát hiện này cho thấy tinh chỉnh trong ngữ cảnh nén cũng hiệu quả như tinh chỉnh trong ngữ cảnh gốc. So với tinh chỉnh ngữ cảnh đầy đủ, bước tinh chỉnh của LLoCO nhanh hơn và hiệu quả chi phí hơn đáng kể do sử dụng ngữ cảnh ngắn hơn nhiều (xem 4.5). Điều này làm cho LLoCO trở thành một giải pháp tinh chỉnh thực tế hơn cho các tác vụ hỏi đáp tài liệu dài.

**Khám phá Tác động của Tỷ lệ Nén** Trong nghiên cứu loại bỏ này, chúng tôi đánh giá LLoCO dưới các tỷ lệ nén khác nhau để phân tích tác động của chúng đến hiệu suất. Thiết lập LLoCO gốc, được điều chỉnh từ AutoCompressor, áp dụng tỷ lệ nén 30x, giảm 1536 token xuống 50 token. Để khám phá tác động của các tỷ lệ khác nhau, chúng tôi cũng nén 1024 token và 2048 token thành 50 token, tương ứng với tỷ lệ nén 20x và 40x. Chúng tôi đánh giá những cấu hình này trên các bộ dữ liệu QuALITY, QMSum và NarrativeQA, với kết quả được trình bày trong Hình 2.

Phát hiện của chúng tôi cho thấy hiệu suất vẫn tương đối ổn định qua các tỷ lệ nén khác nhau, với cả tỷ lệ 20x và 30x đều vượt trội so với 40x trên tất cả bộ dữ liệu. Thú vị là, tỷ lệ 30x hoạt động ngang bằng và đôi khi hơi tốt hơn tỷ lệ 20x. Chúng tôi quy điều này cho việc bộ mã hóa ngữ cảnh (AutoCompressor) ban đầu được tối ưu hóa cho thiết lập 30x, cho phép nó duy trì hiệu suất cao ngay cả ở tỷ lệ nén cao hơn.

### 4.3 Đánh giá trên LongBench

Chúng tôi tiếp tục đánh giá LLoCO trên LongBench (Bai et al., 2023), bao gồm 6 tác vụ phụ. Cho rằng các ứng dụng chính của LLoCO là hỏi đáp tài liệu và tóm tắt, đánh giá của chúng tôi tập trung vào các tác vụ SingleDoc QA, MultiDoc QA và Summarization. Có sự trùng lặp với một số bộ dữ liệu (ví dụ Qasper, QMSum) chúng tôi đã đánh giá trong Phần 4.1. Tuy nhiên, các tập validation khác nhau vì LongBench lấy mẫu một tập con cụ thể của các ví dụ cho mỗi bộ dữ liệu. Chúng tôi đã đảm bảo nghiêm ngặt rằng LongBench không có bất kỳ câu hỏi nào trùng lặp với những câu hỏi được sử dụng trong huấn luyện LLoCO, do đó hoàn toàn loại bỏ bất kỳ rủi ro rò rỉ dữ liệu nào. Để đạt được hiệu suất tốt nhất của LLoCO, chúng tôi chọn các adaptor LoRA được điều chỉnh cho từng danh mục/bộ dữ liệu cụ thể trong đánh giá của chúng tôi. Cụ thể, đối với các tác vụ SingleDoc, chúng tôi sử dụng NQA LLoCO và Qasper LLoCo cho các tác vụ NQA và QAS tương ứng. Đối với MultiField-En (MFE) (Bai et al., 2023), chúng tôi sử dụng GPT-4-turbo để tạo ra các cặp hỏi-đáp từ các tài liệu huấn luyện và tinh chỉnh LLoCO sử dụng những cặp này. Đối với các tác vụ MultiDoc, chúng tôi sử dụng LLoCO tinh chỉnh kết hợp từ Phần D, hoạt động tốt cho tất cả các tác vụ. Đối với các tác vụ Summarization, chúng tôi tạo một bộ dữ liệu huấn luyện bằng cách kết hợp toàn bộ dữ liệu huấn luyện QMSum (Zhong et al., 2021) với 5.000 ví dụ được lấy mẫu ngẫu nhiên từ dữ liệu huấn luyện Multi-News (Fabbri et al., 2019). Sau đó chúng tôi tinh chỉnh LLoCO trên bộ dữ liệu kết hợp này.

Kết quả đánh giá của chúng tôi cho thấy LLoCO vượt trội so với baseline trên 5 trong 9 bộ dữ liệu. Đặc biệt, LLoCO xuất sắc trong tác vụ MultiDoc QA, đạt được kết quả tốt hơn nhiều trên tất cả ba bộ dữ liệu. LLoCO cũng thể hiện hiệu suất tương đương trong hai bộ dữ liệu (MultiNews, Qasper), nhưng kém hơn trong các bộ dữ liệu GovReport (Huang et al., 2021) và MultiField-En. Bộ dữ liệu GovReport là một tác vụ tóm tắt yêu cầu mô hình tạo ra một bản tóm tắt một trang. Chúng tôi thấy rằng LLoCO không hoạt động tốt khi nội dung được tạo ra dài, điều này hiện tại là một hạn chế của phương pháp chúng tôi. Hiệu suất thấp hơn trên bộ dữ liệu MultiField-En có thể được quy cho việc dữ liệu nằm ngoài phân phối so với dữ liệu huấn luyện của chúng tôi, vì bộ dữ liệu này không cung cấp bất kỳ ví dụ huấn luyện nào. Mặc dù có những hạn chế này, điểm trung bình của LLoCO trên tất cả bộ dữ liệu cao hơn so với baseline LLaMA2, nổi bật hiệu quả tổng thể của phương pháp chúng tôi.

### 4.4 Needle In A Haystack

Chúng tôi tiếp tục điều tra khả năng thành thạo của LLoCO để truy xuất thông tin qua các vị trí khác nhau của cửa sổ ngữ cảnh với độ dài thay đổi bằng cách sử dụng tác vụ nổi tiếng Needle In A Haystack (gkamradt, 2023). Điều chỉnh điều này cho pipeline của chúng tôi, chúng tôi ngẫu nhiên chọn một bài viết dài vượt quá 32k token từ bộ dữ liệu NarrativeQA làm "haystack". Bài viết này được sử dụng để kiểm tra mô hình LLoCO, đã được tinh chỉnh trên bộ dữ liệu này như đã thảo luận trong Phần 4.1.

**Single Fixed Needle** Đánh giá ban đầu của chúng tôi tập trung vào một tình huống đơn giản: việc chèn một "needle" cố định nhất quán. Hình 3 cho thấy LLoCO của chúng tôi thành công truy xuất needle trong ~80% qua tất cả kích thước ngữ cảnh được thử nghiệm, cho thấy hiệu suất mạnh mẽ không có suy giảm ở ngữ cảnh dài hơn, trong khi LLaMA2-7B-32k thể hiện hiệu quả thấp hơn đáng kể trong tác vụ này.

**Random Needles** Ngoài ra, chúng tôi kiểm tra khả năng của LLoCO trong một tình huống phức tạp hơn bằng cách sử dụng một "needle" độc nhất trong mỗi thử nghiệm. Theo (Liu et al., 2024a), chúng tôi ngẫu nhiên chọn một thành phố được chỉ định để phục vụ như needle cho mỗi vị trí. Chúng tôi nâng cao mô hình NQA LLoCO thông qua tinh chỉnh liên tục với một bộ dữ liệu tổng hợp quy mô nhỏ bao gồm các thành phố không gặp phải trong quá trình đánh giá. Hình 4 tiết lộ rằng mặc dù mô hình tinh chỉnh NQA gặp khó khăn ban đầu, tinh chỉnh thêm với pipeline LLoCO nâng cao đáng kể tỷ lệ thành công lên ~80%. Cải thiện này nhấn mạnh hiệu quả của phương pháp chúng tôi trong việc xử lý tác vụ Needle In A Haystack.

### 4.5 Độ trễ Suy luận

Trong phần này, chúng tôi đánh giá cải thiện độ trễ suy luận của phương pháp LLoCO. Các thí nghiệm được chạy trên một GPU A100-80G-SXM và một GPU RTX A6000, cả hai với batch size là 1 và số lượng token sinh được đặt là 16. Chúng tôi đo độ trễ per-token với các kích thước ngữ cảnh khác nhau. Như được minh họa trong Hình 5, LLoCO đạt được tăng tốc lên đến 7.62× trên A100 và 7.19× trên A6000 khi so sánh với baseline LLaMA2-7B không nén, dưới điều kiện ngữ cảnh giống hệt. Trong khi baseline vượt quá giới hạn VRAM GPU cho các chuỗi dài hơn 32k token, LLoCO duy trì sinh hiệu quả cho các chuỗi lên đến 128k token. Đáng chú ý, LLoCO đạt được độ trễ 4k của baseline cho các chuỗi dài hơn 16× (64k) trên A100 và 32× dài hơn (128k) trên A6000. Hơn nữa, LLoCO có thể xử lý các chuỗi 32k trên A6000 nhanh như baseline 4k trên A100.

Chúng tôi cũng đo cải thiện thông lượng tinh chỉnh của LLoCO so với tinh chỉnh LLaMA với ngữ cảnh gốc, nơi LLoCO đạt được thông lượng cao hơn lên đến 11.52×. Kết quả chi tiết có thể được tìm thấy trong Phụ lục C.

## 5 Kết luận

Chúng tôi đề xuất LLoCO, một paradigm mới giải quyết thách thức ngữ cảnh dài bằng cách tiền xử lý ngữ cảnh trước suy luận thông qua tinh chỉnh hiệu quả tham số. Phương pháp của chúng tôi mở rộng cửa sổ ngữ cảnh hiệu quả của mô hình LLaMA2-7B với kích thước ngữ cảnh 4k token để xử lý lên đến 128k token. Đánh giá trên các benchmark ngữ cảnh dài rộng lớn cho thấy LLoCO vượt trội đáng kể so với học trong ngữ cảnh trong khi sử dụng ít hơn 30× token trong suy luận, làm cho nó trở thành một giải pháp hứa hẹn cho xử lý ngữ cảnh dài hiệu quả.

## 6 Hạn chế

Một hạn chế của công trình chúng tôi là bộ mã hóa ngữ cảnh gắn liền với một mô hình LLM cụ thể. Nó nén ngữ cảnh gốc thành một biểu diễn được căn chỉnh với mô hình. Do đó, đối với một mô hình khác, một bộ mã hóa ngữ cảnh riêng biệt phải được huấn luyện để đảm bảo căn chỉnh. Huấn luyện một bộ mã hóa ngữ cảnh như AutoCompressor cho LLaMA2-7B rất tốn kém, yêu cầu khoảng 15 tỷ token dữ liệu. Mặt khác, các nghiên cứu như Morris et al. (2023) cho thấy rằng văn bản gốc có thể được khôi phục từ sentence embeddings, cho thấy khả năng xây dựng context embeddings có thể dễ dàng và phổ quát khôi phục. Phát triển một bộ mã hóa ngữ cảnh không phụ thuộc mô hình có thể thích ứng với các mô hình khác nhau mà không cần huấn luyện bổ sung đặt ra một thách thức quan trọng và một lĩnh vực hứa hẹn cho nghiên cứu tương lai.

Trong pipeline của LLoCO, chúng tôi tách tài liệu thành các nhóm và tinh chỉnh một adapter LoRA cho mỗi nhóm. Trong một pipeline RAG tiêu chuẩn, top k tài liệu liên quan đến truy vấn của người dùng được truy xuất. Tuy nhiên, những k tài liệu này có thể đến từ các nhóm khác nhau, mỗi nhóm tương ứng với một adapter LoRA khác nhau, và chúng ta chỉ có thể áp dụng một adapter tại một thời điểm, đây là một hạn chế. Một hướng tương lai hấp dẫn sẽ là điều tra việc kết hợp nhiều adapter LoRA đồng thời để nâng cao thêm hiệu suất QA.

Có sự khác biệt trong hiệu quả của LLoCO qua các bộ dữ liệu và tác vụ khác nhau. Như được hiển thị trong Phần 4.3 và 4.4, hiệu suất của chúng tôi trên GovReport và MultiField-En không vượt qua baseline, và chúng tôi không đạt được kết quả hoàn hảo trên tác vụ Needle-In-A-Haystack. Chúng tôi quy những vấn đề này cho hạn chế của chất lượng dữ liệu huấn luyện và khả năng của bộ mã hóa ngữ cảnh và mô hình cơ sở LLaMA2. Chúng tôi tin rằng cải thiện những thành phần này sẽ giải quyết những thiếu sót này.

## Lời cảm ơn
Chúng tôi cảm ơn Michael Luo, Sheng Shen, Chenglei Si và Siyuan Zhuang vì những bình luận và thảo luận hữu ích của họ. Chúng tôi cũng cảm ơn BAIR, Berkeley Deep Drive, Intel Corporation và NVIDIA vì hỗ trợ nghiên cứu này, cũng như Hyperbolic Labs4 vì cung cấp cơ sở hạ tầng AI cho các thí nghiệm của chúng tôi.

## Tài liệu tham khảo

[Phần tài liệu tham khảo giữ nguyên định dạng gốc do chứa nhiều tên riêng và thuật ngữ chuyên môn]

## Phụ lục A: Cài đặt Thí nghiệm Mở rộng

Trong phần này, chúng tôi cung cấp chi tiết về cài đặt thí nghiệm của chúng tôi trong văn bản chính. Đối với các thí nghiệm được trình bày trong Phần 4.1, chúng tôi tóm tắt cài đặt siêu tham số trong Bảng 5. Chúng tôi sử dụng tất cả các mẫu từ split train của mỗi bộ dữ liệu để tinh chỉnh, ngoại trừ HotpotQA, từ đó chúng tôi ngẫu nhiên chọn 20.000 mẫu từ split train của nó. Tất cả các thí nghiệm tinh chỉnh được tiến hành với 4 hoặc 8 GPU NVIDIA RTX A6000 hoặc A100-80G-SXM.

## Phụ lục B: Chi tiết thêm về Bộ dữ liệu

Ở đây chúng tôi cung cấp mô tả chi tiết hơn về 5 bộ dữ liệu chính được sử dụng để đánh giá:

• **QuALITY** (Pang et al., 2021) là một bộ dữ liệu hỏi đáp trắc nghiệm trên ngữ cảnh dài. Nó chứa 150 bài viết với độ dài trung bình 5000 token, với tổng cộng 6737 câu hỏi. Bộ dữ liệu này đặc biệt phù hợp để đánh giá mô hình trong môi trường ngữ cảnh dài.

• **Qasper** (Dasigi et al., 2021) là một bộ dữ liệu để trả lời câu hỏi về các bài báo nghiên cứu NLP, được lấy từ Semantic Scholar Open Research Corpus. Nó bao gồm các loại câu hỏi khác nhau, từ giải thích chi tiết đến câu trả lời có/không đơn giản, dựa trên các tài liệu được cung cấp.

• **QMSum** (Zhong et al., 2021) có các bản tóm tắt và bản ghi từ các cuộc họp qua nhiều lĩnh vực khác nhau, bao gồm học thuật và công nghiệp, tập trung vào tóm tắt dựa trên truy vấn. Người tham gia được giao nhiệm vụ tóm tắt bản ghi đối thoại dựa trên các truy vấn cụ thể.

• **NarrativeQA** (Kočisk`y et al., 2018) là một bộ dữ liệu hỏi đáp được lấy từ văn bản đầy đủ của sách từ Project Gutenberg và kịch bản phim từ nhiều nguồn khác nhau. Thách thức ở đây bao gồm việc tạo ra một câu trả lời súc tích từ các văn bản dài và có thể rối rắm được lấy từ sách.

• **HotpotQA** (Yang et al., 2018) là một bộ dữ liệu dựa trên Wikipedia thách thức người dùng rút ra câu trả lời thông qua lý luận đa bước qua nhiều tài liệu, có nhiều loại câu hỏi ngoài các cơ sở kiến thức cụ thể.

Chúng tôi cũng cung cấp thống kê của năm bộ dữ liệu này trong Bảng 6.

## Phụ lục C: Thông lượng Tinh chỉnh

Chúng tôi đánh giá thông lượng tinh chỉnh cho cả LLoCO và LLaMA2-7B-32k với ngữ cảnh gốc trên bộ dữ liệu NarrativeQA, chủ yếu bao gồm các mẫu vượt quá 32k token. Chúng tôi so sánh baseline 7B 32k được tinh chỉnh trên 8 A100 và LLoCO của chúng tôi được tinh chỉnh trên cả 8 A100 và 8 A6000, tất cả với batch size per-device là 1 và batch size toàn cục là 32, sử dụng 4 bước tích lũy gradient. Đối với tinh chỉnh baseline 7B 32k, các mẫu hơn 32k token được cắt xuống 32k.

Hình 5 cho thấy LLoCO của chúng tôi đạt được thông lượng 11.52× và 6.82× trên A100 và A6000 tương ứng. Điều này nổi bật rằng LLoCO không chỉ đạt được kết quả cạnh tranh mà còn cải thiện hiệu suất với hiệu quả cao hơn nhiều so với tinh chỉnh baseline với ngữ cảnh đầy đủ (như được hiển thị trong Phần 4.2).

## Phụ lục D: Nghiên cứu Loại bỏ Bổ sung

**Tinh chỉnh Hướng dẫn Kết hợp** Trong các thí nghiệm trước của chúng tôi, chúng tôi tinh chỉnh LLoCO trên mỗi bộ dữ liệu riêng biệt. Trong nghiên cứu loại bỏ này, chúng tôi điều tra hiệu suất của LLoCO khi chúng tôi kết hợp tất cả dữ liệu huấn luyện và tinh chỉnh một mô hình tổng quát. Để cân bằng bộ dữ liệu, chúng tôi lấy mẫu 10.000 ví dụ từ NarrativeQA và HotpotQA và sử dụng tất cả ví dụ từ ba bộ dữ liệu khác để tạo thành tập huấn luyện cuối cùng của chúng tôi.

Như được hiển thị trong Bảng 7, LLoCO với tinh chỉnh kết hợp vượt qua các phương pháp baseline trên tất cả bộ dữ liệu ngoại trừ QMSum. Hiệu suất thấp hơn trên QMSum có thể được quy cho việc nó là một tác vụ tóm tắt ưa thích câu trả lời dài hơn, chi tiết hơn, nhưng quá trình tinh chỉnh kết hợp có thể chuyển hướng hành vi của LLM về việc tạo ra câu trả lời ngắn hơn, tập trung hơn, có thể không tối ưu cho tác vụ tóm tắt.

So với tinh chỉnh trong miền, tinh chỉnh kết hợp thường mang lại hiệu suất thấp hơn, ngoại trừ QuALITY. Câu hỏi QuALITY phụ thuộc nhiều vào khả năng lý luận của LLM, trong khi các bộ dữ liệu khác chủ yếu tập trung vào khả năng của LLM để truy xuất thông tin liên quan từ ngữ cảnh dài. Chúng tôi giả thuyết rằng tinh chỉnh bổ sung trên các bộ dữ liệu đa dạng nâng cao khả năng của LLM để lý luận về ngữ cảnh dài, dẫn đến hiệu suất cải thiện trên QuALITY. Nhìn chung, phương pháp tinh chỉnh kết hợp thể hiện tiềm năng cho việc chuyển giao kiến thức qua các tác vụ khác nhau.

**Khám phá các Bộ mã hóa Ngữ cảnh khác** Theo mặc định, LLoCO sử dụng AutoCompressor làm bộ mã hóa ngữ cảnh của nó. Tuy nhiên, LLoCO mang lại sự linh hoạt trong việc lựa chọn bộ mã hóa ngữ cảnh và không bị giới hạn bởi một phương pháp duy nhất. Trong nghiên cứu loại bỏ này, chúng tôi đánh giá LLoCO sử dụng một phương pháp nén ngữ cảnh tiên tiến khác, ICAE (Ge et al., 2023), làm bộ mã hóa ngữ cảnh cho LLoCO. Vì ICAE được huấn luyện trên ngữ cảnh lên đến 4096 token, chúng tôi cắt đầu vào theo độ dài này, dẫn đến compressed embeddings của 128 token, theo phương pháp được nêu trong bài báo gốc.

Kết quả của chúng tôi trong Bảng 8 cho thấy ICAE hoạt động tương đương với LLaMA2-7B-4K qua các benchmark, trong khi đạt được tỷ lệ nén 40x. Điều này nổi bật rằng pipeline được đề xuất của chúng tôi, kết hợp nén ngữ cảnh với PEFT, không bị hạn chế bởi AutoCompressor và có thể được tổng quát hóa cho các bộ mã hóa ngữ cảnh khác.

Tuy nhiên, vẫn có một khoảng cách hiệu suất đáng chú ý giữa ICAE và LLoCO. Một hạn chế chính của ICAE là không thể mở rộng cửa sổ ngữ cảnh vượt quá 4K token, làm cho nó trở thành một lựa chọn kém phù hợp so với AutoCompressor làm bộ mã hóa ngữ cảnh cho phương pháp của chúng tôi.

## Phụ lục E: Giai đoạn Phục vụ của LLoCO

**Phục vụ** Trong một hệ thống RAG truyền thống, khi người dùng đặt câu hỏi, hệ thống sử dụng một bộ truy xuất để lấy top k tài liệu/đoạn liên quan từ cơ sở dữ liệu vector và nối chúng vào prompt để phục vụ như ngữ cảnh liên quan.

Trong Hình 7, chúng tôi minh họa pipeline phục vụ của LLoCO. Trong một RAG tiêu chuẩn, bộ truy xuất tính toán một text embedding cho mỗi đoạn và sử dụng nó làm key để truy xuất các đoạn liên quan đến truy vấn người dùng. Chúng tôi sử dụng cùng text embedding làm key, nhưng thay vào đó truy xuất compressed token embeddings của những đoạn này và đặt trước chúng vào decoder LLM. Ngoài ra, hệ thống tìm kiếm adaptor LoRA tương ứng trong cơ sở dữ liệu và áp dụng nó vào decoder LLM. Áp dụng một adaptor LoRA phát sinh chi phí tối thiểu cho hệ thống (Hu et al., 2022). Bằng cách tận dụng công việc gần đây về phục vụ nhiều adaptor LoRA (Sheng et al., 2023a; Chen et al., 2023a), chúng tôi có thể song song hóa việc phục vụ adaptor LoRA và đồng thời phục vụ các yêu cầu cho tài liệu được liên kết với hàng nghìn adaptor LoRA trên một GPU duy nhất.

Trong hệ thống hiện tại của chúng tôi, chúng tôi giả định rằng tất cả các đoạn được truy xuất từ cơ sở dữ liệu vector cho một truy vấn người dùng nhất định thuộc về cùng một nhóm tài liệu, cho phép chúng tôi sử dụng một adaptor LoRA chuyên dụng duy nhất. Tuy nhiên, LLoCO có thể được mở rộng để xử lý các trường hợp nơi các đoạn được truy xuất trải rộng qua nhiều nhóm tài liệu. Ví dụ, có thể thiết kế các thuật toán để động lựa chọn adaptor LoRA liên quan nhất dựa trên phần lớn các tài liệu được truy xuất hoặc cân nhắc đóng góp của các adaptor khác nhau theo mức độ liên quan của chúng đến truy vấn (Muqeeth et al., 2024). Một hướng nghiên cứu thú vị và trực giao khác là khám phá việc kết hợp nhiều mô-đun LoRA cùng nhau (Huang et al., 2023), và chúng tôi để dành điều này cho công việc tương lai.

## Phụ lục F: So sánh với Công trình Đồng thời

CEPE (Yen et al., 2024) và SnapKV (Li et al., 2024) là hai công trình đồng thời cũng đề xuất các phương pháp cho nén ngữ cảnh. Chúng tôi cung cấp một so sánh với chúng trong phần này.

**So sánh với CEPE** Tương tự như LLoCO, CEPE yêu cầu huấn luyện một encoder để tạo ra embeddings cho ngữ cảnh. Trong quá trình suy luận, CEPE sử dụng cross-attention để tích hợp những embeddings này vào decoder LLM gốc. Sự khác biệt chính là CEPE áp dụng encoder tại runtime, cho phép nó xử lý ngữ cảnh song song, do đó tăng tốc quá trình suy luận. Ngược lại, LLoCO thực hiện mã hóa ngữ cảnh ngoại tuyến, truy xuất các embeddings đã được tiền xử lý trong quá trình suy luận.

Để so sánh với CEPE, chúng tôi đánh giá mô hình của họ (dựa trên LLaMA2-7B-Chat) trên QuaLITY, Qasper, QMSum và NarrativeQA. Chúng tôi trình bày hai bộ kết quả: hiệu suất đánh giá chính thức được cung cấp bởi CEPE, và kết quả tái tạo của chúng tôi, thu được bằng cách chạy mã công khai có sẵn của họ trên những bộ dữ liệu này.

Như được hiển thị trong Bảng 9, LLoCO vượt trội so với CEPE trên tất cả bộ dữ liệu. Lưu ý rằng đối với bộ dữ liệu QMSum, CEPE cung cấp điểm ROUGE-L, trong khi LLoCO cung cấp trung bình nhân của điểm ROUGE-1, ROUGE-2 và ROUGE-L làm metric của chúng tôi. Chúng tôi lấy điểm ROUGE-L của LLoCO ở đây để so sánh công bằng.

**So sánh với SnapKV** So với LLoCO và CEPE (Yen et al., 2024), yêu cầu tinh chỉnh trước một encoder, SnapKV là một phương pháp nén KV cache không cần tinh chỉnh. Hiểu biết chính đằng sau SnapKV là chỉ một tập con nhỏ token trong prompt là quan trọng (những token mà các token khác chú ý đến), cho phép loại bỏ các token ít quan trọng hơn.

Vì cả LLoCO và SnapKV đều cung cấp kết quả đánh giá trên LongBench, chúng tôi cung cấp một so sánh song song về hiệu suất của họ. Chúng tôi trích xuất kết quả của SnapKV sử dụng LLaMa2-7B-32k (LongChat) làm mô hình cơ sở và so sánh chúng với của chúng tôi. SnapKV cung cấp ba cài đặt (1024, 2048 và 4096 token), và chúng tôi sử dụng kết quả tốt nhất của họ để so sánh.

Bảng 10 minh họa rằng nhìn chung, LLoCO và SnapKV cho thấy hiệu suất tương đương qua các tác vụ. LLoCO vượt trội so với SnapKV trên năm bộ dữ liệu, trong khi SnapKV dẫn đầu trên bốn bộ. Không giống như SnapKV, nén token đầu vào thành độ dài cố định (1024, 2048, 4096), LLoCO áp dụng tỷ lệ nén nhất quán 30x, cho phép nén lên đến 128k token thành 4096 token. Điều này thường dẫn đến tỷ lệ nén cao hơn so với SnapKV trên LongBench.
