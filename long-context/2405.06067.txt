# 2405.06067.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2405.06067.pdf
# File size: 1100236 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HMT: Hierarchical Memory Transformer for Efficient Long Context
Language Processing
Zifan He1, Yingqi Cao2, Zongyue Qin1, Neha Prakriya1,
Yizhou Sun1,and Jason Cong1
1University of California, Los Angeles,2University of California, San Diego
zifanhe1202@g.ucla.edu, yic033@ucsd.edu,
{qinzongyue, nehaprakriya, yzsun, cong}@cs.ucla.edu
Abstract
Transformer-based large language models
(LLM) have been widely used in language
processing applications. However, due to
the memory constraints of the devices, most
of them restrict the context window. Even
though recurrent models in previous works
can memorize past tokens to enable unlimited
context and maintain effectiveness, they have
“flat” memory architectures. Such architectures
have limitations in selecting and filtering in-
formation. Since humans are good at learn-
ing and self-adjustment, we believe that imi-
tating brain memory hierarchy is beneficial for
model memorization. Thus, we propose the
Hierarchical Memory Transformer (HMT)1, a
novel framework that facilitates a model’s long-
context processing ability by imitating human
memorization behavior. Leveraging memory-
augmented segment-level recurrence, we orga-
nize the memory hierarchy by preserving to-
kens from early input segments, passing mem-
ory embeddings along the sequence, and recall-
ing relevant information from history. Eval-
uating general language modeling, question-
answering tasks, and the summarization task,
we show that HMT consistently improves the
long-context processing ability of existing mod-
els. Furthermore, HMT achieves a compa-
rable or superior generation quality to long-
context LLMs with 2∼57×fewer parame-
ters and 2.5∼116×less inference memory,
significantly outperforming previous memory-
augmented models.
1 Introduction
Transformer (Vaswani et al., 2017) has demon-
strated its strength in contextual learning and is
utilized in various applications in language pro-
cessing (Dong et al., 2019) and computer vision
(Dosovitskiy et al., 2020). For a decoder-only trans-
former model, each transformer block contains a
1https://github.com/OswaldHe/HMT-pytorchself-attention and a feedforward network module.
An optimized self-attention layer has a quadratic
computational and linear space complexity (Dao
et al., 2022) regarding the sequence length since
it computes interactions between each token and
all previous tokens in the sequence. To maintain
the inference speed and satisfy memory require-
ments, most transformer models enforce maximum
sequence length. For example, the Llama 3 model
is designed to process 8192 tokens (Dubey et al.,
2024) and the Llama 2 can process up to 4096 to-
kens (Touvron et al., 2023). However, real-world
applications involving long documents, such as
book summarization (Rae et al., 2019) and lifelong
question-answering tasks (Sun et al., 2019; Dai
et al., 2022), can have an enormous or even infinite
stream of inputs.
Existing research attempts to build long con-
text transformers using sparse attention (Beltagy
et al., 2020; Zhang et al., 2021; Kitaev et al., 2020),
retrieval-augmented models (Bertsch et al., 2023;
Wu et al., 2022), and recurrent sequence models
(Peng et al., 2023a; Gu and Dao, 2023; Rae et al.,
2019). Still, these models face at least one of two
issues: (1) difficulty in adapting to future models
due to a change in the core model architecture and
(2) low effectiveness for long-range inputs under
frequent context switching. In this work, we pro-
pose the Hierarchical Memory Transformer (HMT),
a novel framework to enable and augment models’
long-context processing ability. HMT transforms
models into a memory-augmented recurrent model
that imitates the brain’s memory hierarchy and hu-
man memorization behavior. It has the following
unique features:
Hierarchical Memorization: HMT mimics the
memory hierarchy of the brain (Burgin, 2011) em-
ploying both learned memory tokens and current
input tokens. HMT stratifies memory into sen-
sory, short-term, and long-term, with interactions
between each other.arXiv:2405.06067v3  [cs.CL]  6 Feb 2025

--- PAGE 2 ---
Memory Retrieval Mechanism: HMT imitates
memory recall by storing encoded memory em-
beddings generated from previous iterations and
searching based on the relevance to current token
segments.
One key advantage of utilizing HMT over other
memory-augment models is that HMT is a model-
independent plug-and-play framework: future
decoder-only models can directly serve as the back-
bone model of HMT to augment their long context
processing ability without extra implementation ef-
forts. With joint training and fine-tuning of newly
introduced and original parameters of the back-
bone model, HMT is applicable to a wide range
of LLMs, including transformer-based models and
state-space models. Our contributions include:
•HMT consistently improves models’ gener-
ation quality with long context for various
model architectures. We demonstrate HMT on
both transformer-based architecture and state-space
models. Evaluating on Wikitext-103, PG-19 (Rae
et al., 2019), and PubMedQA (Jin et al., 2019)
datasets with multiple contexts concatenated, HMT
can improve the effectiveness by up to 25.5% in
perplexity and 1.0% higher prediction accuracy
over the baseline models.
•HMT with small backbone models can out-
perform large models trained on longer context
samples, implying a high memory efficiency. We
evaluate HMT with SmolLM (Allal et al., 2024),
OPT (Zhang et al., 2022), and OpenLlamaV2
(Geng and Liu, 2023) models on the LongBench
(Bai et al., 2023b) benchmark. In sum, HMT can
achieve comparable or higher metric results with
2∼57×fewer parameters and 2.5∼116×lower
inference memory requirement than long-context
large language models.
•HMT surpasses previous methods specialized
for efficient long-context processing by com-
pressing contexts. We compare HMT with RMT
(Bulatov et al., 2022), LongMem (Wang et al.,
2024), Memorizing Transformer (Wu et al., 2022),
CCM (Kim et al., 2023), and HOMER (Song
et al., 2024), which are recent SoTA of memory-
augmented and hierarchical methods. With the
same or similar size backbone model, HMT has a
better generation quality in both general language
modeling and QA tasks. Furthermore, HMT has a
lower memory complexity, indicating better scala-
bility as the input length increases.2 Related Works and Problem
Formulation
We will first discuss the existing efforts on long-
range transformers and recurrent sequence mod-
els for infinitely long context language processing.
Then, we highlight a problem that is crucial in real-
world applications.
2.1 Long Context Transformers
Since one of the bottlenecks of transformers is
the quadratic computational complexity of self-
attention, a natural approach is sparsifying atten-
tion computation. A naive sparse attention pat-
tern is the sliding window attention (Kovaleva
et al., 2019), where each token attends to neighbors
within a local window. However, this neglects long-
range interaction between words. Existing works
such as Longformer (Beltagy et al., 2020) and Pool-
ingformer (Zhang et al., 2021) extend the sliding
window attention by adding global attending to-
kens and applying pooling to expand the recep-
tive field area. Unlimiformer (Bertsch et al., 2023)
adopts the retrieval-augmented generative method
by searching the top K most important tokens for
the incoming sequence. It then applies attention
to just those tokens in the decoders, resulting in
pruned computations with minor losses. Neverthe-
less, the contribution of less relevant tokens may
accumulate over time and impact the overall se-
quence generation. Although these methods extend
the attainable context length, they cannot prevent
increasing memory consumption as the input length
increases. Alternatively, compressing past tokens
using a recurrent sequence model can potentially
reduce memory consumption by condensing the
information into a fixed-size embedding.
2.2 Recurrent Sequence Models
Recurrent Neural Networks (RNN) have been ex-
tensively explored in sequence processing research,
including Long Short-term Memory (Hochreiter
and Schmidhuber, 1997) and Gated Recurrent Unit
(Chung et al., 2014). They reveal that RNNs per-
form well in memorizing past information and are
hardware-friendly for implementing customized ac-
celerators (Chang et al., 2015). However, RNNs
have limited advantages in learning contextual re-
lationships between words compared with self-
attention in language processing (Bahdanau et al.,
2014). One approach to alleviate this issue is
the coarse-grain recurrence, in which the model

--- PAGE 3 ---
splits inputs into segments, performs attention in-
side each segment, and propagates states (i.e., com-
pressed information as embeddings) between seg-
ments. The Compressive Transformer (Rae et al.,
2019) further stores and compresses previous states
to enhance memorization. The Recurrent Mem-
ory Transformer (RMT) (Bulatov et al., 2022) uti-
lizes a memory token to summarize and propagate
segment information without modifying the trans-
former block architecture. Theoretically, they can
process unlimited long sequences, but previous in-
formation will be diluted after multiple summariza-
tions and generation quality can drop when less
relevant information occupies the memory. Recent
works (Chevalier et al., 2023; Kim et al., 2023)
aim to further optimize RMT to improve genera-
tion quality by concatenating the results of summa-
rizations, but this sacrifices the inference memory
efficiency.
Another approach augments RNN by involving
interactions between the current inputs and the pre-
vious states to learn contextual relationships in a
similar way as self-attention and accelerate the
computation with linear convolution. One of the
representatives, RWKV (Peng et al., 2023a), is an
RNN model inspired by the attention-free trans-
former (AFT) (Zhai et al., 2021). It includes a time-
mixing module to learn from previous states and
a channel-mixing module to learn from the previ-
ous output. Mamba (Gu and Dao, 2023) is another
recurrent method based on the state-space model
that employs gated convolution to accelerate model
inference. These models are energy and memory-
efficient with fast training speed and are able to
achieve high performance in memorization tasks
(e.g., associative recall), but have limitations on
capturing contextual relationships and filtering ir-
relevant information. Recent works combine trans-
formers with Mamba (Lieber et al., 2024; Team
et al., 2024) to mitigate this issue, but this reintro-
duces the scaling issue of the transformers.
2.3 Problem Formulation: Adaptive
Long-context Processing
We intend to develop a model that can handle in-
finitely long context inputs with context adaptabil-
ity: Based on the context/topic of the input stream,
the model can adaptively select past relevant infor-
mation to enhance effectiveness, since irrelevant
context can distract the model (Shi et al., 2023).
In real-world applications, restrained by memory
bandwidth and capacity, as well as data generationTable 1: Notation used to illustrate HMT’s architecture
in Section 3 and Figure 1.
NOTATION MEANING
Hn HIDDEN EMBEDDINGS OF THE nthSEGMENT
L SEGMENT LENGTH
Hn[L−k, L) LASTkEMBEDDINGS OF Hn
Hn[0, j) FIRSTjEMBEDDINGS OF Hn
BBM (·) BACKBONE MODEL
Pn MEMORIZATION PROMPT EMBEDDING
Hout
n HIDDEN EMBEDDING FOR LOGITS GENERATION
Mn MEMORY EMBEDDING OF THE nthSEGMENT
N NUMBER OF CACHED MEMORY EMBEDDINGS
M[n−N+1,n) CACHED MEMORY EMBEDDINGS
T SEGMENT SUMMARIZATION PROMPT EMBEDDING
Sn SUMMARY EMBEDDING OF THE nthSEGMENT
speed, long documents cannot be read as a whole
by the computing hardware (Agerri et al., 2015).
Furthermore, users who are constantly interacting
with the language model can refer to the previous
topic or switch to another topic that has high rele-
vance to past information. For effectiveness, most
recurrent models need to encode all previous inputs
in the states, which can contain irrelevant informa-
tion and degrade the model’s quality.
3 Hierarchical Memory Transformer
The main idea of HMT is to store information hi-
erarchically and search for relevant information
throughout the memory hierarchy. Table 1 de-
scribes all notations we use to illustrate the HMT
architecture in this section.
3.1 Overall Workflow
Given a backbone model to enhance, HMT chunks
the input into L-token segments and operates on
the hidden embeddings of the token segments
({Hn}∞
n=0), generated by the token embedding
layer of the backbone model. For every segment n,
HMT walks through four steps shown in Figure 1:
1)Representation encoding by the backbone
model, which encodes part of the segment contain-
ing the essence of the ongoing topic into a single
embedding to represent its context, denoted by Hn.
2)Memory search , which utilizes the current con-
text as a query to find relevant information in the
memory.
3)Prepending sensory memory , which augments
the segment to capture information in the previous
segment and other relevant information.
4)Decoding and summarization , which processes
the augmented segment to get hidden embeddings

--- PAGE 4 ---
Figure 1: Overall workflow of HMT. For a segment, (1) HMT will first perform representation encoding , utilizing
the segment summarization prompt embedding ( T) to summarize part of the segment. (2) The generated segment
summary embedding ( Sn) is used with the cached memory embeddings for memory search with cross attention.
The output is a memorization prompt embedding ( Pn) which contains information relevant to the current segment.
(3) The memorization prompt embedding and the last kembeddings from the previous segment will augment the
segment. (4) The backbone model (BBM) will process the augmented segment and generate hidden embeddings for
logits ( Hout
n) and the memory embedding ( Mn), which will be pushed into the long-term memory.
for generating logits and a memory embedding that
summarizes the augmented segment.
The first two steps are the memory retrieval
mechanism discussed in Section 3.2. Steps 3 and 4
are explained in Section 3.3 along with the concept
of hierarchical memorization.
3.2 Memory Retrieval Mechanism
To handle context switching and prevent the inter-
vention of irrelevant context, HMT performs mem-
ory retrieval to extract only relevant information
from past knowledge. The memory retrieval mecha-
nism involves three steps: representation extraction,
memory search, and memory augmentation.
Representation Encoding : Depicted in Step 1
of Figure 1, HMT selects the first jembeddings
from the hidden embeddings of the nthsegment,
Hn, to extract the topic of the segment. The em-
beddings are augmented with the segment sum-
marization prompt embedding T.Tis a learn-
able parameter embedding, deployed to prompt the
backbone model (BBM) to summarize the segmentby soft prompt tuning (Liu et al., 2023). Instead
of extracting from the token embedding of BBM,
we make Tlearnable to allow a larger prompt em-
bedding space for summarization. The backbone
model will then process the augmented embeddings
and generate a new embedding at the end of the
output as the representation of the segment:
Sn=BBM ([T||Hn[0, j)||T])[j, j+ 1) (1)
where Snis the summary embedding of the nth
segment only, BBM (·)is the backbone model, and
“||" is the concatenation operator. Snwill be used
for memory search.
Memory Search : Shown in Step 2 of Figure 1,
Snis utilized as a query to find relevant memory
embeddings generated from Step 4 when process-
ing previous segments. We keep a sliding window
ofNembeddings ( M[n−N+1,n)) and then compute:
Qn=SnWq, Kn=M[n−N+1,n)Wk (2)
Pn=softmax (QnKT
n√dh)M[n−N+1,n) (3)

--- PAGE 5 ---
where dhis the hidden dimension of the cross
attention. The computation is similar to cross-
attention without value and output projection.
Softmax (QnKT
n√dh)calculates the normalized simi-
larity score and applies it directly to M[n−N+1,n)
to ensure similar distributions of output value and
old memory tokens. We expect that the projection
WqandWkcan be trained such that summariza-
tions containing similar contexts have high atten-
tion scores after projections.
The output of a memory search is a memoriza-
tion prompt embedding Pncontaining informa-
tion relevant to the nthsegment. It will be applied
to augment the nthsegment. Notice that HMT’s
memory is accumulative: the nthmemory embed-
ding contains information of all previous n−1
segments, with a higher loss of information for
older segments. We hope that retrieving memory
will strengthen the relevant memory and reduce
this loss.
In practice, representation encoding is executed
in parallel with the model inference on GPUs since
they are independent tasks. Memory search has
time complexity O(N), and can also run in parallel
with the segment inference when Nis small (e.g.,
N= 300 ). Thus, the overall runtime overhead of
HMT is negligible.
3.3 Hierarchical Memorization
Human memory can be categorized into three
strata: sensory memory, short-term memory, and
long-term memory (Burgin, 2011). Sensory mem-
ory refers to very short-term memory generated
from sensory information, such as vision and hear-
ing. Short-term and long-term memory are long-
lasting memories, differentiated by how long they
persist in the brain. HMT is inspired by this mem-
ory hierarchy.
Sensory Memory : Sensory memory for the nth
segment refers to the last ktoken embeddings of
Hn−1,Hn−1[L−k, L). When inferencing the
nthsegment, HMT will augment the correspond-
ing token embeddings Hnby prepending it with
Hn[L−k, L), shown in Step 3 of Figure 1.
Short-term Memory : HMT will encode the seg-
ment into an embedding that serves as a “summa-
rization" of the segment. First, HMT will append
and prepend the memorization prompt embedding
Pnto the augmented segment. This guides the
backbone model to compress the segment and rele-
vant context into a summarization embedding withawareness of the relative positions of contexts. As
depicted in Step 4 of Figure 1, we train HMT such
that
H=BBM (Pn||Hn−1[L−k, L)||Hn||Pn)(4)
Hout
n||Mn=H[k+ 1, L+k+ 2) (5)
where Mnis the memory embedding of the nthseg-
ment. Hout
nis a collection of Lhidden embeddings
that will be used to generate logits.
Long-term Memory : Each generated memory
embedding will be cached as the long-term memory.
The cached embeddings will be utilized as the input
of the memory retrieval mechanism to generate
the memorization token embedding Pnfor each
segment as illustrated in the previous sections.
4 Experiment
We benchmark HMT with a variety of backbone
models including SmolLM 135M (Allal et al.,
2024), OPT 350M, OPT 2.7B (Zhang et al., 2022),
OpenLlamaV2 3B (Geng and Liu, 2023), RWKV
3B (Peng et al., 2023a), and Llama 2 7B (Touvron
et al., 2023), under the same memory constraint (i.e.
same maximum context window). Moreover, we
test several models targeting long contexts (Mamba
370M (Gu and Dao, 2023), Yi-6B-200K (Young
et al., 2024), and Mistral 7B (Jiang et al., 2023))
to demonstrate the benefit HMT has on genera-
tion quality and memory consumption. We eval-
uate HMT with state-space models (RWKV and
Mamba) as backbones since we believe that mod-
els which can already process infinitely long inputs
would benefit even further from HMT. All mod-
els mentioned are trained and assessed on 4 AMD
MI210 GPUs , which can handle models up to 7B
parameters. We further test HMT on 4 NVIDIA
A100-80GB GPUs for the Qwen 2.5 14B model
(Bai et al., 2023a) to justify its scalability to larger
models and gain a consistent effectiveness boost.
To tune the extra parameters introduced by HMT,
we use the RedPajamaV2 (Computer, 2023) dataset
to pre-train each model. Notice that HMT intro-
duced new model hyperparameters on top of the
backbone model ( L,j,N, and k). A common
configuration is L= 1024 ,j= 512 ,N= 300 ,
andk= 32 , and we adjust these values for each
model to achieve the best performance. To com-
pare with previous works (RMT, LongMem, Mem-
orizing Transformer, CCM), we apply the same
backbone models if the method is applicable to any

--- PAGE 6 ---
model, or find a backbone model with a similar
size if the method requires special architecture.
For the long-context benchmark, we select sub-
sets (NarrativeQA, Qasper, and MultiFieldQA-en
for single document QA; HotpotQA, 2WikiMQA,
and MuSiQue for multi-document QA; GovRe-
port, QMSum, and Multi-News for summarization;
TriviaQA for few-shot learning) from a widely ac-
knowledged benchmark, LongBench (Bai et al.,
2023b), and measure them against models reported
in the LongBench leaderboard. However, the maxi-
mum average document length of test sets in Long-
Bench is shorter than 20k words, which is not
very long for modern long-context models. To
better understand HMT’s long-context processing
ability under various context scenarios, we further
study HMT on crafted and controllable dataset sam-
ples. For crafted datasets, we derive from existing
datasets to form long inputs. For general language
tasks, models are tested for next token generation
tasks with Wikitext-103 (Merity et al., 2016) (2-3k
words per sample) and PG-19 (Rae et al., 2019)
datasets (69k words per sample on average). Sam-
ples will be concatenated or split into chunks to
form longer samples and investigate the relation-
ships between input length and the effectiveness of
the model. For question-answering tasks, we chose
PubMedQA (Jin et al., 2019), which is a biomedi-
cal question-answering dataset with corresponding
contexts. We artifact the dataset to assess HMT
with multi-context inputs, described in Appendix I.
5 Results and Key Observations
In this section, we illustrate the main result of HMT.
More ablation studies are in Appendix E and G.
5.1 Impacts on Backbone Models
By introducing an additional 0.5% ∼1.3% (1.77M
∼33.5M) of parameters, HMT can enhance models
with a variety of architectures to improve genera-
tion quality when processing long context inputs.
We demonstrate this feature with general language
modeling and question-answering tasks.
HMT consistently improves the backbone
models in general language modeling tasks when
processing long inputs. Figures 2 and 3 compare
the perplexity of OPT 2.7B, RWKV 3B, and Open-
LlamaV2 3B models with and without HMT on the
Wikitext-103 and PG-19 datasets. Over input span-
ning from 2k ∼100k tokens, HMT consistently
raises the generation quality of all these models.Table 2: Scalability of HMT. Average PPL is computed
by taking the average PPL for samples in each sequence
length in the experiment.
MODEL AVGTESTPPL (W IKITEXT ) (↓)
OPT 350M 15.11
HMT + OPT 350M 14.28 (-5.8%)
OPT 2.7B 12.12
HMT + OPT 2.7B 8.61 (-28.9%)
RWKV 430M 19.33
HMT + RWKV 430M 16.10 (-16.6%)
RWKV 3B 13.30
HMT + RWKV 3B 9.93 (-25.3%)
Moreover, Table 2 presents how improvements are
achieved by HMT scales with the model size for
same-family models. To further strengthen our ar-
gument that HMT can benefit larger models, we
evaluate HMT with Qwen 2.5 14B utilizing 4 A100-
80GB GPUs for training. As depicted in Figure
4, HMT can still increase the effectiveness of the
backbone model on PG-19.
Notice that the improvement is not necessarily
contributed solely by the additional parameters.
Having more parameters does not always lead to
higher performance. For example, HMT boosts
OPT 2.7B to realize a lower perplexity than OpenL-
lama 3B with 20.7% fewer parameters, while OPT
2.7B performs worse without HMT. Section 5.2 de-
scribes more examples of HMT achieving superior
generation quality with smaller models.
HMT enhances long-answer contextual rea-
soning and short-answer prediction ability in
question-answering tasks. One of the use cases
of HMT is handling question-answering tasks that
involve multiple contexts. Thus, we select the Pub-
MedQA dataset and derive long-context QA sam-
ples with controllable context counts to evaluate the
effectiveness of HMT. Two metrics are employed:
for long answers, we compute the PPL to assess the
contextual reasoning of HMT; for short answers,
we measure the response accuracy. As seen in Fig-
ures 5 and 6, for samples with 2 to 10 contexts,
HMT increases the effectiveness in PPL by 9.48%
for long answers. For short answer tasks, HMT
is 1.0% more accurate than the backbone model
and exhibits significant advantages when samples
have more contexts. In sum, HMT increases both
the correctness and reasoning ability of models in
long-context QA tasks.
5.2 Comparison to Long Context Models
Combined with small and short-context models,
HMT can be more effective than large models

--- PAGE 7 ---
Figure 2: Test Perplexity of HMT, RMT, and three baseline models (OPT 2.7B, RWKV 3B, OpenLlamaV2 3B) with
the Wikitext-103 dataset. HMT outperforms RMT by 13.0% for OPT and 10.8% for OpenLlamaV2. For RWKV ,
HMT can even boost the effectiveness by 16.5%, while RMT worsens the effectiveness.
Figure 3: Test Perplexity of HMT, RMT, and three baseline models (OPT 2.7B, RWKV 3B, OpenLlamaV2 3B),
evaluated over the PG-19 dataset. HMT outperforms RMT by 3.98% for OPT and 6.85% for OpenLlamaV2. For
RWKV , HMT can improve the effectiveness by 9.96%.
Figure 4: Test Perplexity of HMT, RMT, and baseline
model for Qwen 2.5 14B on PG-19 dataset. HMT boosts
the effectiveness of the baseline model by 10.0%, while
RMT worsens its effectiveness.
trained on long-context inputs. Table 3 displays
metric results of HMT-augmented models on sub-
sets of LongBench (Bai et al., 2023b) and compares
them with large models specialized for long con-
texts. The subsets contain various generation tasks,
including single/multi-document QA, summariza-
tion, and few-shot learning. With a significantly
lower inference memory requirement, HMT ap-
plied to small models can attain comparable or
better metrics compared to large models, indicat-
ing a significant resource advantage. Specifically,
we observe that HMT with small models performs
well in generating short responses for long and
multi-context inputs, thanks to its context-filtering
ability. However, it exhibits comparable or weaker
performance in generating long responses, as small
models have shorter token generation limits com-
Figure 5: Long answer quality of RMT and HMT
applied on Llama-2 7B, evaluated over PubMedQA
dataset. HMT is 8.98% more effective than RMT.
pared to large models.
Moreover, applying HMT to long-context mod-
els can further improve their effectiveness and re-
duce inference memory consumption. For example,
the AMD MI210 GPU cannot handle inferencing
30k token inputs with the Yi-6B-200K model due
to memory constraints. Applying a sliding window
strategy with a 5.2K-token window (Yi-6B-SW-
5.2K), the model consumes 44.8 GB VRAM. On
the contrary, HMT + Yi-6B-200K requires only
33.9 GB VRAM to process 30k tokens with a
small segment length (512 tokens), with a 2% ef-
fectiveness improvement. Table 4 presents the ef-
fectiveness of long-range models on Wikitext-103
compared with several HMT-augmented models,
including Mamba and Mistral models.
2Although this model is trained with 200K-token samples,
it cannot be run on MI210 due to memory constraints.

--- PAGE 8 ---
Table 3: Metric results of HMT-augmented small models and large models trained on longer contexts. Models with
HMT can process infinitely long context, but only keep a fixed length of KV cache (the value in the parenthesis).
We evaluate on subsets of LongBench, including QMSum (QMS), MuSiQue (MSQ), Qasper (QASP), Narra-
tiveQA (NQA), MultiFieldQA-en (MFQA-en), GovReport (GR), TriviaQA (TQA), HotpotQA (HQA), 2WikiMQA
(2WMQA), and MultiNews (MN). Mem Req indicates the minimum inference memory required (to store parameters
and KV cache). Actual inference may require a larger VRAM.
MODEL CONTEXT WINDOW QMS MSQ QASP NQA MFQA- EN GR TQA HQA 2WMQA MN M EMREQ(GB↓)
GPT-3.5 T URBO 16384 23.4 26.9 43.3 23.6 52.3 29.5 91.4 51.6 37.7 26.7 46.7
LLAMA 2 7B C HAT 4096 20.8 9.4 19.2 18.7 36.8 27.3 77.8 25.4 32.8 25.8 15.1
LONG CHAT V1.5 7B 32768 22.7 9.7 27.7 16.9 41.4 30.8 82.3 31.5 20.6 26.4 22.6
XG EN7B 8192 20.5 10.3 18.1 18.0 37.7 27.3 77.8 29.7 21.1 26.2 16.2
INTERN LM 7B 8192 15.9 9.0 16.7 12.1 23.4 9.7 77.8 28.7 21.1 22.8 16.2
CHATGLM2 6B 32768 24.0 21.9 31.5 21.1 46.2 32.4 78.7 45.1 34.0 26.5 19.5
VICUNA V1.5 7B 16384 22.8 9.8 26.1 19.4 38.5 27.9 86.2 25.3 20.8 27.2 18.3
CHATGLM3 6B 32768 23.9 40.4 43.3 26.0 51.7 36.8 87.1 54.4 44.9 27.9 19.5
HMT + OPT 350M ∞(1024) 22.2 15.6 32.3 17.2 62.7 13.8 87.2 25.3 29.2 10.5 0.75
HMT + O PENLLAMA V2 3B ∞(512) 21.4 42.3 35.7 21.6 58.1 15.8 93.0 29.9 27.7 11.0 6.1
HMT + S MOL LM 135M ∞(1024) 19.4 14.1 30.2 19.6 48.1 15.7 81.3 27.6 23.3 9.5 0.4
BENCHMARK AVERAGE LEN (WORDS ) 10614 11214 3619 18409 4559 8734 8209 9151 4887 2113
Figure 6: Short response accuracy of RMT and HMT
applied on Llama-2 7B, evaluated over PubMedQA
dataset. HMT is 1.8% more accurate than RMT.
Table 4: Quality of long context models and HMT with
various backbone models. The input size is 30k tokens
and the dataset is Wikitext-103.
MODEL MAXCONTEXT TESTPPL (W IKITEXT )
RWKV 3B ∞ 13.13
MAMBA 370M ∞ 87.08
YI-6B-200K 200K OOM2
YI-6B-SW-5.2K 200K 6.89
MISTRAL -7B 32K 5.47
HMT + OPT 350M ∞(1024) 13.67
HMT + O PENLLAMA V2 3B ∞(512) 7.04
HMT + RWKV 3B ∞(256) 10.94
HMT + M AMBA 370M ∞(256) 16.71
HMT + Y I-6B-200K ∞(512) 6.75
HMT + M ISTRAL -7B ∞(512) 5.12
5.3 Comparison to Memory-augmented and
Hierarchical Methods
One popular memory-augmented model is the re-
current memory transformer (Bulatov et al., 2022)
(RMT). Our assessment indicates that HMT is
generally better at both language modeling and
question-answering tasks than RMT, illustrated in
Figures 2, 3, 5, and 6. The improvement gap is
especially significant for recurrent models such
as RWKV . HMT can further increase the effec-
tiveness of RWKV while RMT will degrade the
performance for both datasets, as demonstrated inFigure 3. Since RWKV has already compressed
past tokens and passed hidden states along the se-
quence, applying RMT to RWKV re-weights past
information compressed in states periodically. This
was originally done by the time-mixing module
of RWKV . Therefore, the advantage of memory
augmentation is limited. Due to the gradient van-
ishing issue, the model is harder to train with RMT,
leading to inferior performance. However, we be-
lieve that the memory retrieval mechanism in HMT
helps RWKV to select previous hidden states with
the most relevance, boosting its effectiveness. An-
other advantage of HMT over RMT is its scalability
with large models: while RMT applied to Qwen
2.5 14B results in reduced effectiveness compared
to direct inference with the backbone model, HMT
continues to enhance effectiveness, as illustrated in
Figure 4.
Furthermore, compared with other memory-
augmented models, HMT is not only easy to use but
also has higher generation quality. Table 5 picks
three memory-augmented methods (Memorizing
Transformer (Wu et al., 2022), LongMem (Wang
et al., 2024), and CCM-concat (Kim et al., 2023))
and compares them with HMT with the same or
similar-sized backbone models. We choose the
datasets used by the original works for fair com-
parisons. Memorizing transformer and LongMem
require modifying the core architecture of the base
model. Future models cannot easily adopt such
modifications. Overall, HMT outperforms these
methods. We also list the inference memory over-
head complexity for each model, where Lis the
total context length, liis the inference segment
length, lmis the memory size ( L > l m> li),
andtis the number of memory embeddings con-

--- PAGE 9 ---
Table 5: Comparison between HMT with previous
memory-augmented methods (Memorizing Transformer,
LongMem, and CCM-concat).
MODEL TESTPPL (W IKITEXT , 30 K TOKEN ) M EMOVERHEAD
MEMTRM 31.51 O(L)
HMT + OPT 350M 13.67 O(li)
MODEL TESTPPL (A RXIV,VARIABLE ) M EMOVERHEAD
LONG MEM 10.08 O(lm)
HMT + Q WEN 1.5-0.5B 9.02 O(li)
MODEL TESTPPL (PG-19, 60 K TOKEN ) M EMOVERHEAD
CCM- CONCAT 7.41 O(t+li)
HMT + L LAMA 2 7B 7.40 O(li)
catenated for CCM-concat. HMT has the lowest
memory complexity over all previous methods.
Figure 7: Comparison between HMT and HOMER with-
out context extension and with YaRN, all applying on
Llama 2 7B. On average, HMT is 9.9% more effective
than HOMER with YaRN on PG-19.
Lastly, we compare HMT with HOMER (Song
et al., 2024), a method that hierarchically com-
presses inputs to reduce their length for inference.
In terms of memory complexity, HOMER requires
O(log(L))memory to store the reduction tree, lead-
ing to increased peak memory utilization as in-
put length grows. In contrast, HMT maintains
a constant peak memory complexity regardless
of input length. Regarding effectiveness, HMT
achieves 9.9% lower perplexity on PG-19 com-
pared to HOMER with YaRN (Peng et al., 2023b)
for context extension. As shown in Figure 7, the
benefits of HMT become more substantial as input
length increases, highlighting its superior scalabil-
ity with longer inputs.
6 Conclusion
We present HMT, a framework to augment mod-
els’ long-range language processing ability with
context switching. Inspired by the brain’s memory
hierarchy, HMT imitates human memorization be-
havior by deploying hierarchical memory and the
memory retrieval mechanism. HMT consistently
improves the generation quality of the backbone
models. Compared with other long-context LLMsand memory-augmented models, HMT achieves
higher generation quality with lower memory re-
quirements. Our model provides LLM accessibility
to resource-constrained applications and represents
a step forward to lifelong language tasks.
7 Limitations and Ongoing Works
•Currently, HMT will save Nmemory embed-
dings for memory search, which is a cross-attention
layer. When Nis small (e.g., N= 300 ), which
is already sufficient for 100k token samples, the
overhead is negligible. However, when Ngrows
and the memory embeddings are stored in differ-
ent physical memory hierarchies, the overhead can
be significant. An intelligent memory prefetch-
ing mechanism can potentially alleviate the latency
overhead, which we leave as future work.
•Due to the large computational graph of mod-
els when training with BPTT, tuning the extra
parameters introduced by HMT can be memory-
consuming, impeding experiments on larger-scale
models. A more efficient way to extend BPTT
depth without memory overhead is a future re-
search direction.
•Although HMT employs only one level of long-
term memory, one may use multiple levels of long-
term memory to improve information access ef-
ficiency. Similar techniques have been used for
multilevel optimization in VLSI physical design
(Cong and Shinnerl, 2013; Chan et al., 2005).
8 Ethical Statements
The capability of memorizing information by HMT
offers convenience to people’s daily lives, while
also raising concerns about privacy leakage through
conversation with language model agents. Never-
theless, with further efforts to deploy it on edge
devices without network connections, this issue
can be resolved.
Acknowledgement
This research is partially supported by the PRISM
(000705769) center under the JUMP 2.0 program
by DARPA/SRC and NSF SEED funding. It is also
supported by CDSC industrial partners ( https://
cdsc.ucla.edu/partners ) and the AMD HACC
Program.

--- PAGE 10 ---
References
Rodrigo Agerri, Xabier Artola, Zuhaitz Beloki, Ger-
man Rigau, and Aitor Soroa. 2015. Big data for
natural language processing: A streaming approach.
Knowledge-Based Systems , 79:36–42.
Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Le-
andro von Werra, and Thomas Wolf. 2024. Smollm -
blazingly fast and remarkably powerful.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023a. Qwen technical report. arXiv
preprint arXiv:2309.16609 .
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, et al. 2023b. Longbench:
A bilingual, multitask benchmark for long context
understanding. arXiv preprint arXiv:2308.14508 .
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R Gormley. 2023. Unlimiformer: Long-
range transformers with unlimited length input.
arXiv preprint arXiv:2305.01625 .
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.
2022. Recurrent memory transformer. Advances
in Neural Information Processing Systems , 35:11079–
11091.
Mark Burgin. 2011. Epistemic information in stratified
m-spaces. Information , 2(4):697–726.
Tony Chan, Jason Cong, and Kenton Sze. 2005. Mul-
tilevel generalized force-directed method for circuit
placement. In Proceedings of the 2005 international
symposium on physical design , pages 185–192.
Andre Xian Ming Chang, Berin Martini, and Euge-
nio Culurciello. 2015. Recurrent neural networks
hardware implementation on fpga. arXiv preprint
arXiv:1511.05552 .
Alexis Chevalier, Alexander Wettig, Anirudh Ajith,
and Danqi Chen. 2023. Adapting language
models to compress contexts. arXiv preprint
arXiv:2305.14788 .
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .
Together Computer. 2023. Redpajama: an open dataset
for training large language models.Jingsheng Jason Cong and Joseph R Shinnerl. 2013.
Multilevel optimization in VLSICAD , volume 14.
Springer Science & Business Media.
Yi Dai, Hao Lang, Yinhe Zheng, Fei Huang, Luo Si,
and Yongbin Li. 2022. Lifelong learning for question
answering with hierarchical prompts. arXiv preprint
arXiv:2208.14602 .
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language mod-
els beyond a fixed-length context. arXiv preprint
arXiv:1901.02860 .
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Advances in Neural Information Processing Systems ,
35:16344–16359.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. 2019. Unified language model
pre-training for natural language understanding and
generation. Advances in neural information process-
ing systems , 32.
Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. 2020.
An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint
arXiv:2010.11929 .
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, et al. 2024. The llama 3 herd of models. arXiv
preprint arXiv:2407.21783 .
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Xinyang Geng and Hao Liu. 2023. Openllama: An open
reproduction of llama.
Albert Gu and Tri Dao. 2023. Mamba: Linear-time
sequence modeling with selective state spaces. arXiv
preprint arXiv:2312.00752 .
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation , 9(8):1735–
1780.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .

--- PAGE 11 ---
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W
Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset
for biomedical research question answering. arXiv
preprint arXiv:1909.06146 .
Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, and
Hyun Oh Song. 2023. Compressed context memory
for online language model interaction. arXiv preprint
arXiv:2312.03414 .
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. arXiv
preprint arXiv:2001.04451 .
Olga Kovaleva, Alexey Romanov, Anna Rogers, and
Anna Rumshisky. 2019. Revealing the dark secrets
of bert. arXiv preprint arXiv:1908.08593 .
Opher Lieber, Barak Lenz, Hofit Bata, Gal Co-
hen, Jhonathan Osin, Itay Dalmedigos, Erez
Safahi, Shaked Meirom, Yonatan Belinkov, Shai
Shalev-Shwartz, et al. 2024. Jamba: A hybrid
transformer-mamba language model. arXiv preprint
arXiv:2403.19887 .
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Computing Surveys , 55(9):1–35.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.arXiv preprint arXiv:1609.07843 .
Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and
Hinrich Schütze. 2023. Ret-llm: Towards a general
read-write memory for large language models. arXiv
preprint arXiv:2305.14322 .
Gianluca Moro, Luca Ragazzi, Lorenzo Valgimigli,
Giacomo Frisoni, Claudio Sartori, and Gustavo
Marfia. 2023. Efficient memory-enhanced trans-
former for long-document summarization in low-
resource regimes. Sensors , 23(7):3542.
Michael C Mozer. 2013. A focused backpropagation
algorithm for temporal pattern recognition. In Back-
propagation , pages 137–169. Psychology Press.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In International conference on machine
learning , pages 1310–1318. Pmlr.
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Al-
balak, Samuel Arcadinho, Huanqi Cao, Xin Cheng,
Michael Chung, Matteo Grella, Kranthi Kiran GV ,
et al. 2023a. Rwkv: Reinventing rnns for the trans-
former era. arXiv preprint arXiv:2305.13048 .Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico
Shippole. 2023b. Yarn: Efficient context window
extension of large language models. arXiv preprint
arXiv:2309.00071 .
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
and Timothy P Lillicrap. 2019. Compressive trans-
formers for long-range sequence modelling. arXiv
preprint arXiv:1911.05507 .
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
and Yuxiong He. 2020. Zero: Memory optimizations
toward training trillion parameter models. In SC20:
International Conference for High Performance Com-
puting, Networking, Storage and Analysis , pages 1–
16. IEEE.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. 2020. Deepspeed: System optimiza-
tions enable training deep learning models with over
100 billion parameters. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining , pages 3505–3506.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H Chi, Nathanael Schärli,
and Denny Zhou. 2023. Large language models can
be easily distracted by irrelevant context. In Inter-
national Conference on Machine Learning , pages
31210–31227. PMLR.
Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung
Kim, Sukmin Yun, Jung-Woo Ha, and Jinwoo Shin.
2024. Hierarchical context merging: Better long
context understanding for pre-trained llms. arXiv
preprint arXiv:2404.10308 .
Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. 2019.
Lamol: Language modeling for lifelong language
learning. arXiv preprint arXiv:1909.03329 .
Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman,
Avshalom Manevich, Barak Peleg, Ben Aviram, Chen
Almagor, Clara Fridman, Dan Padnos, et al. 2024.
Jamba-1.5: Hybrid transformer-mamba models at
scale. arXiv preprint arXiv:2408.12570 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu,
Xifeng Yan, Jianfeng Gao, and Furu Wei. 2024. Aug-
menting language models with long-term memory.
Advances in Neural Information Processing Systems ,
36.

--- PAGE 12 ---
Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Al-
borz Geramifard, and Zhou Yu. 2020. Memformer:
A memory-augmented transformer for sequence mod-
eling. arXiv preprint arXiv:2010.06891 .
Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and
Christian Szegedy. 2022. Memorizing transformers.
arXiv preprint arXiv:2203.08913 .
Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu,
Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang,
Zeyun Tang, Shichao Song, et al. 2024. Memory3:
Language modeling with explicit memory. arXiv
preprint arXiv:2407.01178 .
Alex Young, Bei Chen, Chao Li, Chengen Huang,
Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi:
Open foundation models by 01. ai. arXiv preprint
arXiv:2403.04652 .
Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen
Huang, Hanlin Goh, Ruixiang Zhang, and Josh
Susskind. 2021. An attention free transformer. arXiv
preprint arXiv:2105.14103 .
Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li,
Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021.
Poolingformer: Long document modeling with pool-
ing attention. In International Conference on Ma-
chine Learning , pages 12437–12446. PMLR.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .

--- PAGE 13 ---
A Other Related Works
The memory-augmented long-context transformer has been an active research topic in recent years.
LongMem (Wang et al., 2024) chunks the long-document input into segments and caches the attention
keys and values for each segment. During the inference of a segment, LongMem will select relevant
key-value embedding pairs by computing the attention score between token embeddings and the cached
key embeddings and fuse the top k embeddings. Memorizing Transformer (Wu et al., 2022) also caches
the key-value embedding pairs similar to LongMem, but utilizes a kNN search to retrieve information
similar to Unlimiformer. RET-LLM (Modarressi et al., 2023) employs prompt engineering to store the
informative context in a database and search keywords when the context involves questions. Memory3
(Yang et al., 2024) compresses segments of tokens into “explicit" memory blocks and stores them
directly into a memory bank for retrieval. While these works can precisely retrieve contexts, they are
not scalable due to the increasing memory consumption of storing long contexts without compression.
Segment-level recurrent models, such as EMMA (Moro et al., 2023), Memformer (Wu et al., 2020), and
Transformer-XL (Dai et al., 2019), attempt to compress memory throughout the recurrence to reduce
memory consumption. EMMA composes long-term memory from multiple short-term memory by linear
combination and concatenates long and short-term memory to augment the segments. Transformer-XL
propagates the compressed memory states derived from the attention of current layers to the previous
layers for every iteration. Memformer augments the attention with the stored memory embeddings
per time step and retrieves information using the cross-attention layer of the encoder-decoder model.
However, Memformer employs a forgetting network to remove irrelevant context similar to LSTM, which
can potentially delete useful contexts for unseen inputs. On the other hand, HMT condenses contexts
into embeddings and retrieves information precisely without requiring a forgetting network to remove
information permanently. Also, some of these works, including Memorizing Transformer, Memformer,
TransformerXL, and Unlimiformer, need to fundamentally change the model architecture or inject new
adapters based on different base model architecture. It makes deployment and extension to future LLMs
very expensive. HMT avoids this issue by having a model-independent plug-and-play framework.
B Comparison to LongMem
Unlike HMT, LongMem (Wang et al., 2024) operates on the key and value caches of each layer of the
model and requires caching long caches to capture distant context. To compare with LongMem, we pick
the Qwen1.5-0.5B (Bai et al., 2023a) model as the backbone model and train HMT by 700 steps with
4 segments over 100 samples of the ArXiv subset of the Pile dataset (Gao et al., 2020). The subset has
15.4K tokens on average and 60K tokens on maximum per sample, as (Wang et al., 2024) described. Due
to the large storage consumption of the training subset of the Pile dataset, we only extract the ArXiv
subset in the validation and test split. HMT is trained on the validation set and tested on the test set. Table
6 illustrates that HMT + Qwen1.5-0.5B realizes lower PPL, with a smaller parameter size and in-memory
length (number of memory embeddings, which is the number of key and value embeddings cached for
LongMem). This indicates that HMT is memory efficient.
Table 6: Effectiveness of LongMem (Wang et al., 2024) and HMT + Qwen1.5-0.5B models over ArXiv subset of
the Pile dataset. With HMT, the Qwen1.5-0.5B model can obtain better effectiveness with fewer parameters and
shorter memory, after 700 steps of update. The result for LongMem comes from the original paper. Subscription is
the standard deviation.
MODEL # PARAMS SEGMENT LENGTH IN-MEMORY LENGTH TESTPPL (A RXIV)
LONG MEM 558M 1 K 65K 10.08
HMT + Q WEN 1.5-0.5B 463M 1 K 300 9.02±0.04

--- PAGE 14 ---
Table 7: Training and fine-tuning configurations for the backbone models (OPT 350M, Mamba 370M, OPT 2.7B,
RWKV 3B, OpenLlamaV2 3B, Llama 2 7B, Yi-6B-200K, and Mistral 7B) and the modified model after applying
RMT and HMT. S1 and S2 denote the first stage and the second stage of multi-stage training for HMT. 4 AMD
MI210 GPUs cannot train larger models. The given learning rate is the starting learning rate and will decay by a
factor of 0.9 for OPT, OpenLlamaV2, and RWKV models and 0.7 for the remaining models for every 100 steps.
The batch size is 2.
MODEL INPUT LENGTH SEGMENT LENGTH LEARNING RATE TRAINING STEPS EXTRA PARAM
OPT 350M / S MOL LM 135M 2048 1024 1 E-5 700
+RMT 2048 1024 1 E-5 700
+HMT (S1) 2048 1024 1 E-5 200
+HMT (S2) 15360 1024 1 E-5 500 1.77M (0.5% - 1.3%)
OPT 2.7B / O PENLLAMA V2 3B 2048 512 1 E-5 700
+RMT 1280 512 1 E-5 700
+HMT (S1) 1024 512 1 E-5 200
+HMT (S2) 4096 512 1 E-5 500 13.1M (0.5%)
RWKV 3B 1280 256 1 E-5 700
+RMT 1280 256 1 E-4 700
+HMT (S1) 512 256 1 E-5 200
+HMT (S2) 1280 256 1 E-5 500 13.1M (0.5%)
LLAMA 2 7B 2048 256 1 E-4 700
+RMT 1280 256 1 E-4 700
+HMT (S1) 512 256 1 E-4 200
+HMT (S2) 2048 256 1 E-4 500 33.5M (0.5%)
MAMBA 370M 1536 256 1 E-4 700
+HMT (S1) 512 256 1 E-4 200
+HMT (S2) 1536 256 1 E-4 500 4.1M (1.1%)
YI-6B-200K / M ISTRAL 7B 2048 - 2 E-4 700
+HMT (S1) 1024 512 2 E-4 200
+HMT (S2) 1536 512 2 E-4 500 33.5M (0.5%)
C Comparison to Unlimiformer
There are two major differences between a previous retrieval-augmented model, Unlimiformer (Bertsch
et al., 2023), and HMT in terms of the memory retrieval mechanism:
•Unlimiformer retrieves the information with kNN search over the collection of encoded token segments,
while HMT uses cross-attention. We believe there are several advantages of employing cross-attention:
(1) Attending Top K’s most similar token segments still introduces information loss. Regarding the
self-attention layer, the aggregation of tokens with less similar encodings may positively contribute to
the quality of the final output. On the other hand, cross-attention fuses all cached hidden embeddings,
weighted by the relative similarity, which captures the whole context. (2) The output of the cross-attention
is a single embedding, which has lower computational overhead compared to attending k extra tokens.
•Each cached memory embedding encodes the current token segment and the previous memory em-
bedding in HMT. Therefore, HMT can capture the whole context even with a limited number of cached
embeddings. Memory recall is mainly used to rescale the importance of past information. On the other
hand, the Unlimiformer needs to store all encodings, which is memory-consuming.
In terms of usage, Unlimiformer targets encoder-decoder models and injects retrieval modules into the
backbone model. Although the authors recently added support for decoder-only models for token genera-
tion, only the Llama model architectures (Touvron et al., 2023) can be applied and the training/evaluation
procedure is not specified. This is one of the biggest challenges for Unlimiformer to adapt to future LLMs
for validation and generation. On the contrary, HMT focuses on decoder-only models. Since HMT does
not inject new modules in the backbone model, it is effortless to adapt to future LLMs.
D HMT, RMT, and Baseline Training Details and Hyperparameters
Table 7 are the training configurations of the backbone models and HMT/RMT.
The baselines are assessed using the sliding window attention for context-constrained models to control
the memory consumption for a fair comparison. Due to the limited VRAM of GPUs, we shrink the

--- PAGE 15 ---
segment length for the larger backbone model. Also, increasing memory token size does not improve
the effectiveness as (Bulatov et al., 2022) suggested. Thus, both RMT and HMT apply memory tokens
with a length of 1. For RMT, we utilize the maximum BPTT unroll depth with the best effectiveness that
the GPUs can handle. HMT is trained with the multi-stage training technique illustrated in Section F.
The first stage (S1) is trained with 2 segments, and the second stage (S2) is trained with the maximum
BPTT unroll depth that the GPUs can manage. The size of long-term memory is 300 (i.e., N= 300 )
for OPT, SmolLM, OpenLlama, Yi, and Mistral models and 400 ( N= 400 ) for the rest of the models
to capture sufficient contexts and we summarize half of the segment for representation extraction (i.e.,
j=L/2). We observed that the benefit of increasing Nis diminishing and stops at 300 for 100k token
inputs, described in Appendix E. We select the learning rate for HMT, RMT, and the baseline to optimize
the effectiveness. Furthermore, HMT preserves 32 tokens ( k= 32 ) from the previous segment as the
sensory memory. All models are trained with 700 steps, which is sufficient to converge the training loss.
For models with HMT, we first pretrain the model with RedPajamaV2 (Computer, 2023) with 700 steps,
then finetune the model for the downstream tasks with 700 steps. For LongBench, the evaluation metrics
are the same as the original work and we use the Huggingface evaluate package to compute Rouge-L and
F1 score. For parameter-efficient training, LoRA (Hu et al., 2021) with rank 8 is applied to models with
high training memory consumption (Llama 2 7B, Mamba 370M, Yi-6B-200K, and Mistral 7B). For the
rest of the models, we finetune all parameters in the backbone model. All experiments are done with 3
random seeds and we take the average metrics.
E Ablation Study
We conduct ablation studies regarding the memory retrieval mechanism to demonstrate that (1) memory
retrieval is beneficial, (2) partial summarization of a segment in memory retrieval can speed up infer-
ence while maintaining similar effectiveness, and (3) caching more memory embedding can raise the
effectiveness of HMT.
Impact of memory retrieval mechanism. Figure 8 displays the advantages of having a memory
retrieval mechanism in HMT for long context input with context switching. For any tested input length,
the effectiveness of HMT with memory retrieval outperforms that without memory retrieval. Furthermore,
when the memory retrieval mechanism is deployed, the effectiveness improves for the OPT 350M
backbone model or tends to improve for the OPT 2.7B backbone model as the input sequence length
grows, demonstrating better scalability of HMT.
Figure 8: Effectiveness of HMT with and without
the memory retrieval mechanism for OPT 350M
and 2.7B as the backbone models. The inputs are
extracted from the Wikitext-103 dataset with up to
100k tokens.
Figure 9: Effectiveness of HMT with OPT 2.7B
when performing representation extraction on the
whole segment for half of the segment. The impact
is negligible, justifying that summarizing half of the
segment is a valid method for inference acceleration.
Impact of summarizing partial segment in memory retrieval. To overlap or reduce the inference
time of the previous segment with the representation extraction of the next segment, it is necessary to
prefetch only the first ltokens in the segment for summarization. In the experiment, we select half of
the segment for representation extraction. We examine the model that extracts the whole segment and

--- PAGE 16 ---
compare the effectiveness, depicted by Figure 9. The impact is negligible. We hypothesize that the start of
a segment contains enough information about the overall topic for memory retrieval, which is intuitive
as humans can capture the concepts by browsing keywords or segments instead of reading the whole
paragraphs.
Impact of limited cached memory embeddings. Due to memory constraints, we only cache the
most recent 300 memory embeddings for memory retrieval. Figure 10 depicts the relationship between
the number of cached memory embeddings and the effectiveness of HMT with Llama 2 7B over the
Wikitext-103 dataset with 100k-token samples. We observed that increasing the window of cached
memory benefits the effectiveness, but the improvement becomes marginal. We hypothesize that HMT is
more likely to recall recent memory embeddings in the Wikitext-103 dataset. Figure 11 plots the frequency
distribution of the distance between the current segment and the segment corresponding to the memory
embedding with the highest softmax score in the memory retrieval mechanism. 6.5% of the segments
retrieve memory tokens within 2 segments. This signals the importance of local context. However, the
long-context memory retrieval still exists. A possible explanation is that entries in Wikipedia may refer to
other entries through hyperlinks and related context, and HMT discovers this long-context relationship
and recalls the relevant information.
In our experiments, we store the most recent 300 memory embeddings to balance the trade-off between
retrieval effectiveness and computational efficiency. Theoretically, storing ⌈M/L⌉embeddings is sufficient
to handle up to Mtoken inputs, ensuring robust performance. For longer inputs, high-quality processing
is still achievable if recent contextual information is more relevant to the prompt.
Figure 10: Relationship between number of cached
memory embeddings and the effectiveness of HMT
+ Llama 2 7B. Each sample has 100k tokens from the
Wikitext-103 dataset. As HMT stores more memory
embeddings, the effectiveness is marginally better.
Figure 11: Histogram of context distance between
the current segment and the segment correspond-
ing to the memory embedding with the highest soft-
max score in the memory retrieval mechanism. The
dataset evaluated is the Wikitext-103.
Figure 12: Training HMT + OPT 2.7B with the memory retrieval mechanism in two steps results in a better
performance than using the mechanism to train HMT directly. Total training time is 902 s for multi-stage training
and 1680 s for single-stage training on 4 AMD MI210 GPUs.

--- PAGE 17 ---
F Multi-stage Training
Since HMT introduces parameters for memory retrieval, we need to train new parameters and fine-tune
the parameters of the backbone model cooperatively. Training HMT involves multiple segments of
tokens to learn how to encode input tokens and retrieve information properly. Therefore, we split training
HMT into two stages. In the first stage, The model is trained without the memory retrieval mechanism
employing BPTT with 2 segments unrolled. BPTT saves the model checkpoint locally. Then, the memory
retrieval mechanism loads and extends the train model in the second stage. At this point, BPTT trains the
modified model by unrolling the maximum number of segments that the GPUs can handle to maximize
the effectiveness, which is 15 in our experiment. Since the architecture of HMT is complex, breaking
up training into two stages is beneficial for local optimization and improves long context inference
performance compared with single-stage training. Figure 12 exhibits the performance difference between
the multi-stage and single-stage training of the OPT 2.7B model with HMT for long-context inputs. Since
Stage 1 involves a shorter training sequence length and a simpler recurrent architecture than Stage 2,
training with Stage 1 is faster per iteration (1.15 s/iteration) than Stage 2 (3.36 s/iteration). Within the
same number of training steps, multi-stage training obtains better effectiveness and lower total training
time than single-stage training.
G HMT Memory Retrieval Behavior
One insight of using memory retrieval in HMT is handling frequent context switching to previously
discussed topics or new topics. To evaluate this property, we employ PubMedQA and artifact the
dataset with multiple contexts, mentioned in Section 5.1. In this section, we will discuss other dataset
manipulations on PG-19 to investigate the memory retrieval behavior of HMT further.
One way to manually introduce context switching is by interleaving the samples. For every 2 samples
in the PG-19 dataset, we alternatively concatenate a segment of 256 tokens in each sample together
to create a new sample. Therefore, a context switch will be invoked every 256 tokens. We fine-tuned
and benchmarked HMT with Llama 2 7B over the artifact dataset. As a result, HMT can enhance the
effectiveness of the baseline Llama 2 model, while RMT will worsen it, as shown in Figure 13. We
record the context distance of memory retrieval for 30k-token input, illustrated in Figure 14, and notice a
periodical recall distribution, indicating that HMT can capture the context-switching pattern.
Figure 13: Effectiveness of HMT and RMT with
Llama 2 7B evaluated over PG-19 with interleaving
samples. HMT is 12.02% better than RMT in terms
of PPL for 2k to 100k-token samples.
Figure 14: Histogram of context distance between
the current segment and the segment correspond-
ing to the memory embedding with the highest soft-
max score in the memory retrieval mechanism. The
dataset evaluated is the PG-19 with interleaving sam-
ples.
To demonstrate that HMT’s behavior is aligned with the context-switching pattern, we further manipu-
late the PG-19 dataset by inserting 256 “$" tokens for every 256 tokens to dilate each sample. Intuitively,
the segment containing “$" should be considered as irrelevant information and recalled infrequently.
Figure 15 shows the memory retrieval pattern of HMT with Llama 2 7B over the dilated PG-19 dataset.

--- PAGE 18 ---
We observe that HMT not only exhibits a periodical recall pattern but also successfully captures the
position of irrelevant segments and avoids recalling them.
Figure 15: Histogram of context distance between the current segment and the segment corresponding to the
memory embedding with the highest softmax score in the memory retrieval mechanism. The dataset evaluated is the
dilated PG-19 dataset. Each sample is 25.6k tokens.
H Relationships Between Effectiveness and Size of Sensory Memory
During the experiment, we observed a general trend in the relationships between the effectiveness of
HMT-augmented models and the size of sensory memory: the effectiveness will be first enhanced and
then degraded as more and more embeddings are preserved for sensory memory. For instance, Table 8
illustrates the change of effectiveness of HMT + Llama 2 7B evaluated on Wikitext-103 with different
sensory memory sizes. The PPL drops to the minimum when having 32 embeddings for the sensory
memory.
Table 8: Effectiveness of HMT + Llama 2 7B evaluated on Wikitext-103 with 100k-token samples, with various
sensory memory sizes. The segment size is 256 tokens. The effectiveness improves and then degrades with an
increasing number of embeddings that are preserved for sensory memory.
#OF EMBEDDINGS FOR SENSORY MEMORY TESTPPL (W IKITEXT )
8 4.54
16 4.25
32 4.19
64 4.31
128 4.57
I Dataset Construction for PubMedQA
The original PubMedQA dataset does not have training, validation, and test dataset splits. In the experi-
ments, we choose the pqa_artificial subset and partition the training, validation, and test split, where
the training split is the first 75% of samples, the validation split is the next 15% of samples, and the test
split is the remaining 10% samples.
We artifact the long-context dataset from PubMedQA as the following: (1) select Mquestion-context-
answer tuples from the dataset. Let this set of tuples be {(C0, Q0, A0),(C1, Q1, A1), . . . , (CT, QT, AT)},
where Cnare contexts, Qnare questions, Anare answers for 0≥n≥M. Answers can be either long
answers with detailed reasoning or short answers (either “yes", “no", or “maybe"). (2) Concatenate
all the contexts from each tuple to form a long context and append the questions and answers for each
tuple. This will create Msequences: C0C1. . . C TQ0A0,C0C1. . . C TQ1A1, . . .,C0C1. . . C TQTAT.

--- PAGE 19 ---
By controlling the value of M, we can determine the fraction of useful information in the context for each
question and better understand the filtering and selection ability of HMT and the baseline model.
J Gradient Stability in HMT and RMT
Both HMT and RMT are trained using backward propagation through time (BPTT) (Mozer, 2013), a
technique utilized to train the RNN model by unrolling recurrent forward passes of the model to optimize
long-sequence learning. One issue with RMT training with BPTT is the gradient explosion and vanishing
problem. With a higher BPTT unroll depth, the effectiveness of RMT will first increase and then decrease,
with a slow reduction or even an increase in training loss. As seen in Figure 16, we use the Wikitext-103
dataset with various BPTT unroll depths to access the effectiveness of RMT with the OPT 2.7B backbone
model. For both 2k and 10k token inputs, we observe a rising PPL when unrolling more than 5 segments
during training.
Figure 16: Effectiveness of training RMT with BPTT with different unroll depths for 2K tokens and 10K tokens
input from the Wikitext-103 dataset. The backbone model is OPT 2.7B, with 256 tokens per segment during
inference.
Table 9: Relationship between the BPTT unroll depth and the test PPL of Wikitext-103 for OPT 2.7B with HMT.
The experiment is evaluated on samples with 10k tokens. HMT preserved 32 tokens from the previous segment as
the sensory memory and saved 300 memory embeddings for the memory retrieval. The segment size is 256 tokens.
BPTT U NROLL DEPTH TESTPPL (W IKITEXT )
2 9.36
5 9.15
15 8.20
Unlike RMT, HMT does not suffer from gradient vanishing or explosion as BPTT unroll depth
increases due to the memory retrieval mechanism. Table 9 reveals that HMT can improve its effectiveness
continuously as the BPTT unroll depth increases during training. Therefore, HMT will be more effective
when the BPTT unroll depth increases. A detailed gradient stability analysis is presented in Appendix J.
Furthermore, we applied several techniques to optimize the GPU memory consumption to increase the
maximum trainable BPTT unroll depth compared with RMT, described in Appendix K.
In this section, we will formulate a mathematical description for the gradient stability of HMT when
training with BPTT. BPTT with RMT behaves similarly to RNN which suffers from a vanishing or
exploding gradient when there is a long chain of the gradient graph after unrolling (Pascanu et al., 2013).
Specifically, for a generic RNN model with the following form:
Ht=σ(AHt−1+Bxt)
where Htis the hidden state at time h,xtis the input at time t, andAandBare parameters, the gradient
will explode if ||AT||>1and vice versa. A similar phenomenon occurs when training segment-level

--- PAGE 20 ---
Figure 17: Backward propagation flows of HMT and RMT. The gradient of the first memorization prompt embedding
(the red block on the right of the first segment) has multiple branches through the memory retrieval unit. Where the
HMT gradient does not require propagation through each segment, the RMT gradient does.
recurrent models such as RMT. Here we provide a scratch calculation on the gradient of loss with respect
to the memory token at the starting time, which is one of the parameters in both RMT and HMT, after
tsteps for RMT. Let y′
t+1=H(xt, mt)be the logits and mt+1=F(xt, mt)be the generated memory
embedding at time t, where xtis the input, mtis the memory token. The loss of inference is
Lt+1=L(y′
t+1, yt+1) (6)
Therefore, the gradient can be calculated by the chain rule as
∂Lt+1
∂m0=∂Lt+1
∂y′
t+1×∂y′
t+1
∂m0
=∂Lt+1
∂y′
t+1×∂H
∂mt(xt)×∂mt
∂m0
=∂Lt+1
∂y′
t+1×∂H
∂mt(xt)×t−1Y
i=0∂F
∂mi(xi)(7)
Whether the gradient will explode or vanish depends on the input distribution and the function Ft. If
∀xt,∂F
∂mt(xt)>0, then the gradient will explode. Otherwise if ∀xt,∂F
∂mt(xt)<0, the gradient vanishes.
Consequently, training RMT with a very high BPTT unroll depth can be inefficient. For HMT, with
the assistance of the memory retrieval mechanism, the gradient is not prone to explosion or vanishing.
Intuitively, the backward propagation of HMT for the memorization prompt embedding contains multiple
short sub-branches to prevent gradient vanishing and the memory retrieval mechanism can modulate
the propagation chain to avoid gradient explosion (Figure 17). Let Gt(zt, mt, mt−1, . . . , m 1) =m′
tbe
the memory search function where ztis the representation extraction of segment at time t. Letsbe the

--- PAGE 21 ---
summarization token for representation extraction. The gradient for HMT is
∂Lt+1
∂m0=∂Lt+1
∂y′
t+1×∂y′
t+1
∂m0
=∂Lt+1
∂y′
t+1×∂H
∂m′
t(xt)×∂Gt
∂m0
=∂Lt+1
∂y′
t+1×∂H
∂m′
t(xt)×(tX
k=1∂Gt
∂mk(zk, mt, . . . , m k−1, mk+1, . . . , m 1)×∂F
∂m0)
=∂Lt+1
∂y′
t+1×∂H
∂m′
t(xt)×(tX
k=1∂Gt
∂mk(zk, mt, . . . , m k−1, mk+1, . . . , m 1)×∂F
∂m′
k×∂Gt−1
∂m0)
=. . .(8)
The root cause of the gradient explosion or vanishing comes from the long chain of gradient products in
the formulation. For HMT, there are multiple short branches of the multiplication chain after expanding
the expression. The longest chain over all components in the gradient is
∂Lt+1
∂y′
t+1×∂H
∂m′
t(xt)×(t−1Y
k=1∂F
∂m′
k×∂Gk
∂mk)×∂F
∂m0(9)
For gradient vanishing, since∂Lt+1
∂m0still has components with a shorter chain, the gradient will not
disappear even when ||∂F
∂m′
k×∂Gk
∂mk||<1. For gradient explosion, empirically,∂Gk
∂mkare different for each
kby the property of cross attention and can modulate the term∂F
∂mkto distribute near 1. Thus, HMT is
less prone to gradient explosion.
A similar proof can be deduced for the segment-level summarization token embedding of HMT for
representation extraction.
K Distributed Training with Memory Consumption Optimization
Although (Bulatov et al., 2022) proves that unrolling more segments can improve the model effectiveness,
they limit the number of segments unrolled to 4 with 2 NVIDIA A100 80GB GPUs since the maximum
BPTT unroll depth is bounded by the GPU VRAM limit. There are three sources of VRAM consumption:
model parameters, intermediate data (input segments, long-term memory, raw outputs of each segment,
etc.), and optimization data (gradient and optimizer states). Although the computations of later segments
do not require the intermediate data from the previous segment, the original BPTT will keep them on GPU
by default. To reduce memory consumption, we customize the program to offload and load intermediate
data for each input segment between the CPU and GPUs and distribute optimizer states and gradients
throughout multiple GPUs running Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) Stage
2 in DeepSpeed (Rasley et al., 2020). These allow the model to unroll up to 15 segments with HMT. To
train larger models, we employ LoRA (Hu et al., 2021) with rank 8. This allows us to fit 7B models to 4
MI210 GPUs.
L License and Links of Datasets and Models
Datasets:
•Wikitext (Texts from Wikipedia): CC BY-SA 4.0, https://huggingface.co/datasets/
Salesforce/wikitext
•PG-19 (Books from Project Gutenberg): Apache License V2.0, https://huggingface.co/
datasets/emozilla/pg19
•PubMedQA (Biomedical QA dataset): MIT License, https://huggingface.co/datasets/
qiaojin/PubMedQA

--- PAGE 22 ---
•RedPajamaV2 (Pretraining data corpus): Apache License V2.0, https://huggingface.co/
datasets/togethercomputer/RedPajama-Data-V2
•LongBench (Long context evaluation dataset): Licenses are mentioned in the original work (Bai
et al., 2023b), https://huggingface.co/datasets/THUDM/LongBench
Models:
• OPT models: MIT License, https://huggingface.co/facebook/opt-350m
•Llama models: Llama 2 Custom License, https://huggingface.co/meta-llama/
Llama-2-7b-hf
•OpenLlamaV2: Apache License V2.0, https://huggingface.co/openlm-research/open_
llama_3b_v2
• RWKV: Apache License V2.0, https://huggingface.co/RWKV/rwkv-4-3b-pile
• Mamba: Apache License V2.0, https://huggingface.co/state-spaces/mamba-370m-hf
• Mistral: Apache License V2.0, https://huggingface.co/mistralai/Mistral-7B-v0.3
• Yi: Yi-License, https://huggingface.co/01-ai/Yi-6B-200K
•Qwen: Tongyi Qianwen License, https://huggingface.co/Qwen/Qwen1.5-0.5B ,https://
huggingface.co/Qwen/Qwen2.5-14B
• SmolLM: Apache License V2.0, https://huggingface.co/HuggingFaceTB/SmolLM-135M
All datasets and models are publicly available and free to use.
