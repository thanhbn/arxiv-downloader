# 2311.02382.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2311.02382.pdf
# Kích thước tệp: 1521750 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
TRANSFORMER PHÂN TÁN CỰC DÀI CỦA CHUỖI
Xiao Wang1Isaac Lyngaas1Aristeidis Tsaris1Peng Chen2Sajal Dash1Mayanka Chandra Shekar1
Tao Luo3Hong-Jun Yoon1Mohamed Wahib4John Gounley1

TÓM TẮT
Các mô hình Transformer được huấn luyện trên các chuỗi dài thường đạt được độ chính xác cao hơn so với các chuỗi ngắn. Thật không may, các transformer thông thường gặp khó khăn với việc huấn luyện chuỗi dài do yêu cầu tính toán và bộ nhớ quá lớn. Các phương pháp hiện có để huấn luyện chuỗi dài chỉ cung cấp tốc độ và giảm bộ nhớ hạn chế, và có thể làm giảm độ chính xác. Bài báo này trình bày một phương pháp huấn luyện phân tán mới và hiệu quả, Long Short-Sequence Transformer (LSS Transformer), để huấn luyện transformer với các chuỗi dài. Nó phân phối một chuỗi dài thành các đoạn giữa các GPU, với mỗi GPU tính toán một self-attention từng phần cho đoạn của nó. Sau đó, nó sử dụng kỹ thuật truyền thông hợp nhất và một kỹ thuật trung bình gradient kép mới để tránh việc cần phải tổng hợp self-attention từng phần và giảm thiểu chi phí truyền thông. Chúng tôi đánh giá hiệu suất giữa LSS Transformer và song song hóa chuỗi Nvidia tiên tiến nhất trên bộ dữ liệu Wikipedia enwik8. Kết quả cho thấy phương pháp đề xuất của chúng tôi dẫn đến triển khai nhanh hơn 5.6 lần và hiệu quả bộ nhớ hơn 10.2 lần so với song song hóa chuỗi tiên tiến nhất trên 144 GPU Nvidia V100. Hơn nữa, thuật toán của chúng tôi mở rộng đến độ dài chuỗi cực đại 50,112 trên 3,456 GPU, đạt hiệu quả song song siêu tuyến tính 161% và thông lượng 32 petaflops.

1 GIỚI THIỆU
Transformer là một kiến trúc mạng nơ-ron mạnh mẽ được sử dụng rộng rãi trong xử lý ngôn ngữ tự nhiên và hình ảnh (Vaswani et al., 2017). Tính linh hoạt của nó được chứng minh bởi phạm vi ứng dụng rộng rãi, bao gồm dịch máy (Wang et al., 2019), chatbot (Caldarini et al., 2022), nhận dạng giọng nói (Dong et al., 2018), tạo chú thích hình ảnh (Yu et al., 2019), phân đoạn hình ảnh (Valanarasu et al., 2021; Strudel et al., 2021), và phân loại (Chen et al., 2021b). Transformer đạt được hiệu suất ấn tượng bằng cách nhận ra rằng các token chuỗi đầu vào khác nhau có mức độ quan trọng khác nhau đối với dự đoán đầu ra cuối cùng. Transformer nắm bắt mối quan hệ giữa từng cặp token đầu vào bằng cách sử dụng một quá trình gọi là "self-attention". Điều này cho phép transformer tạo ra các đầu ra có độ chính xác cao bằng cách tập trung vào các token có liên quan nhất trong chuỗi đầu vào đồng thời cũng chú ý đến bối cảnh tổng thể. Cách tiếp cận này đã được chứng minh là rất hiệu quả và làm cho transformer trở thành công nghệ hàng đầu trong trí tuệ nhân tạo.

Với huấn luyện chuỗi dài, transformer chú ý đến nhiều token đầu vào hơn so với transformer được huấn luyện với chuỗi ngắn. Do đó, huấn luyện chuỗi dài thường nắm bắt được nhiều thông tin ngữ cảnh hơn và dẫn đến độ chính xác dự đoán cao hơn đáng kể cho nhiều nhiệm vụ có phụ thuộc tầm xa, chẳng hạn như phân tích chuỗi DNA (Zaheer et al., 2020), tóm tắt tài liệu dài (Beltagy et al., 2020) và phân đoạn hình ảnh (Strudel et al., 2021). Thật không may, dấu chân bộ nhớ của transformer tăng theo bậc hai và tính toán tăng theo bậc ba với độ dài chuỗi dài hơn (Beltagy et al., 2020; Dao et al., 2022). Do đó, độ dài chuỗi của transformer thường được cắt ngắn không quá vài nghìn token do ràng buộc về thời gian chạy và bộ nhớ, mặc dù chuỗi dài hơn dẫn đến độ chính xác cao hơn.

Để giải quyết vấn đề này, có ba cách tiếp cận nghiên cứu: huấn luyện phân cấp, xấp xỉ attention, và song song hóa chuỗi phân tán. Huấn luyện phân cấp bao gồm huấn luyện nhiều transformer ở các mức độ trừu tượng khác nhau (Si & Roberts, 2021; Chen et al., 2022; 2021b; Yu et al., 2023). Transformer ở mức độ trừu tượng thấp nhất huấn luyện trên các đoạn chuỗi ngắn nhất. Sau đó, transformer ở mức cao hơn tiếp theo sử dụng đầu ra của mức trước như đầu vào bổ sung để huấn luyện trên các đoạn dài hơn. Quá trình sau đó lặp lại cho đến khi đạt đến mức độ trừu tượng cao nhất. Tuy nhiên, huấn luyện nhiều transformer ở các mức độ trừu tượng khác nhau làm tăng đáng kể thời gian huấn luyện và dấu chân bộ nhớ. Ngoài ra, cần điều chỉnh siêu tham số để có kiến trúc mô hình tối ưu ở mỗi mức độ trừu tượng.

Cách tiếp cận xấp xỉ, ngược lại, nhằm giảm tính toán và sử dụng bộ nhớ bằng cách xấp xỉ self-arXiv:2311.02382v2  [cs.DC]  8 Nov 2023

--- TRANG 2 ---
Ultra-Long Sequence Distributed Transformer

Bảng 1. Tóm tắt các phương pháp huấn luyện chuỗi dài khác nhau. Phương pháp tuần tự có thể cần được sử dụng bên trên các sơ đồ song song hóa. lx= độ dài chuỗi; N= số GPU worker; H= số mức phân cấp; Z= số khác không.

[THIS IS TABLE: A table comparing different approaches (Hierarchical Training, Attention Approximation, Sequence Parallel) with columns for Method, Accuracy Loss, Serial/Distributed, Memory per worker, Compute per worker, Memory Total, Compute Total, Comm. freq. per layer, and Practical bottleneck]

các hoạt động attention thông qua lấy mẫu thưa thớt (Child et al., 2019; Kitaev et al., 2020; Roy et al., 2021; Beltagy et al., 2020; Zaheer et al., 2020), xấp xỉ hạng thấp (Choromanski et al., 2021; Katharopoulos et al., 2020), cập nhật self-attention không thường xuyên (Ying et al., 2021; Rabe & Staats, 2022), hoặc kết hợp của chúng (Chen et al., 2021a). Các phương pháp xấp xỉ này có thể giảm đáng kể dấu chân bộ nhớ và tính toán, và một số thậm chí có thể giảm độ phức tạp bậc hai của bài toán chuỗi dài xuống độ phức tạp tuyến tính. Tuy nhiên, xấp xỉ là một kỹ thuật nén thông tin có tổn thất mà loại bỏ thông tin từng phần cho self-attention. Do đó, xấp xỉ quá mức có thể làm giảm độ chính xác đặc biệt đối với các chuỗi có phụ thuộc tầm xa. Các thí nghiệm trước đó chứng minh sự suy giảm độ chính xác đáng kể khi tỷ lệ nén xấp xỉ vượt quá 70% (Shi et al., 2021).

Cách tiếp cận song song hóa chuỗi phân tán nhằm giải quyết bài toán chuỗi dài bằng cách phân phối các chuỗi dài thành các đoạn chuỗi liền kề giữa các GPU (Li et al., 2021; 2023; Korthikanti et al., 2022; Jacobs et al., 2023). Rào cản lớn nhất trong song song hóa chuỗi phân tán nằm ở việc xử lý self-attention, vốn vừa là bước tính toán chuyên sâu nhất nhưng cũng là bước truyền thông chuyên sâu nhất. Vì đoạn chuỗi của mỗi GPU có tương quan với đoạn của mọi GPU khác, một phương pháp self-attention phân tán đơn giản, chẳng hạn như (Li et al., 2021), Deep-Speed Ulysses (Jacobs et al., 2023) và LightSeq gần đây nhất (Li et al., 2023), yêu cầu mỗi GPU giao tiếp với mọi GPU khác nhiều lần để tính toán điểm self-attention từng phần cho đoạn được gán của nó, trước khi tổng hợp chúng thành đầu ra self-attention cuối cùng. Do đó, tần suất truyền thông có xu hướng tăng đáng kể theo tốc độ tăng trưởng bậc hai với nhiều GPU song song chuỗi hơn, làm cản trở đáng kể khả năng mở rộng.

Ngược lại, một phương pháp song song chuỗi thay thế từ khung Megatron-LM của Nvidia hoàn toàn tránh phụ thuộc self-attention bằng cách thực hiện cập nhật tuần tự trên các hoạt động self-attention và feed-forward cốt lõi dọc theo chiều chuỗi, trong khi song song hóa các nhiệm vụ ít tính toán chuyên sâu nhưng độc lập, chẳng hạn như chuẩn hóa lớp và dropout (Korthikanti et al., 2022). Mặc dù phương pháp này chỉ yêu cầu 8 lần truyền thông cho mỗi lớp attention, nó bỏ lỡ cơ hội song song hóa self-attention tính toán chuyên sâu. Do đó, nó dẫn đến tăng tốc khiêm tốn 29% so với đường cơ sở tuần tự với tính toán lại ngược đầy đủ (Korthikanti et al., 2022).

Bảng 1 cung cấp tóm tắt ý tưởng cốt lõi và hạn chế của mỗi cách tiếp cận. Để giải quyết các hạn chế của chúng, bài báo này giới thiệu "Distributed Long Short-Sequence Transformer" (LSS Transformer). Trái ngược với các phương pháp hiện có, LSS Transformer (1) sử dụng một transformer duy nhất để huấn luyện; (2) đạt được tăng tốc và giảm bộ nhớ đáng kể; (3) không có xấp xỉ, do đó không mất độ chính xác; và (4) duy trì chi phí truyền thông tối thiểu, chỉ yêu cầu 2 lần truyền thông cho mỗi lớp attention. Nó phân phối một chuỗi dài thành các đoạn ngắn giữa các GPU, với mỗi GPU tính toán điểm self-attention từng phần cho đoạn của nó trong bối cảnh của toàn bộ chuỗi. Sau đó, nó sử dụng kỹ thuật truyền thông hợp nhất và trung bình gradient kép để tránh việc cần tổng hợp điểm self-attention từng phần, giảm thiểu chi phí truyền thông trong khi bảo toàn phụ thuộc dữ liệu.

Bài báo này đóng góp theo những cách sau: (1) Giới thiệu một khung chuỗi end-to-end chung và sáng tạo để huấn luyện chuỗi dài. Hoạt động ở mức lớp attention, nó không phụ thuộc vào kích thước mô hình và các biến thể (chỉ encoder, chỉ decoder, v.v.), làm cho nó có thể áp dụng phổ quát mà không cần sửa đổi. (2) Trình bày thuật toán chi phí truyền thông thấp sử dụng truyền thông hợp nhất và trung bình gradient để giảm tần suất truyền thông. (3) Trình bày thuật toán song song hóa chuỗi và dữ liệu tích hợp giảm thiểu truyền thông giữa GPU trong các nhóm giao tiếp cục bộ. Đánh giá của chúng tôi trên bộ dữ liệu Wikipedia enwik8 chứng minh sự vượt trội của LSS Transformer so với song song hóa chuỗi Nvidia tiên tiến nhất (Korthikanti et al., 2022), đạt được huấn luyện nhanh hơn 6 lần và hiệu quả bộ nhớ tăng 10 lần trên 144 GPU Nvidia V100. Hơn nữa, thuật toán của chúng tôi mở rộng đáng kể đến độ dài chuỗi lớn 50,112 sử dụng 3,456 GPU, mang lại hiệu quả song song siêu tuyến tính 161% và thông lượng tính toán 32 petaflops.

--- TRANG 3 ---
Ultra-Long Sequence Distributed Transformer

Bảng 2. Ba thách thức khác biệt để huấn luyện transformer và các mức độ song song hóa trực giao của chúng. Nd= kích thước dữ liệu; Nm= kích thước mô hình; lx = độ dài chuỗi; B= kích thước batch;

[THIS IS TABLE: A table showing challenges, computational/memory complexities, and parallelism types for Large Dataset, Large Model Size, and Long Sequences]

2 BỐI CẢNH VÀ ĐỘNG LỰC

2.1 Các Mức Độ Song Song Hóa Trực Giao

Huấn luyện một mô hình transformer có ba thách thức tính toán khác biệt: (1) bộ dữ liệu huấn luyện lớn, (2) kích thước mô hình lớn và (3) chuỗi dài. Bảng 2 cung cấp tổng quan ngắn gọn về những thách thức này và đặc điểm của chúng. Như đã thảo luận trước đó, tăng độ dài chuỗi dẫn đến tốc độ tăng trưởng bậc ba cho tính toán và tốc độ tăng trưởng bậc hai cho sử dụng bộ nhớ. Ngược lại, mở rộng các tham số mô hình hoặc batch dữ liệu trình bày độ phức tạp tính toán và bộ nhớ tuyến tính.

Mỗi thách thức dựa trên một song song hóa duy nhất. Các mức độ song song hóa này chủ yếu là trực giao, giải quyết các thách thức khác nhau và không thể thay thế lẫn nhau. Song song hóa dữ liệu tăng tốc huấn luyện trên các bộ dữ liệu lớn bằng cách phân phối các batch huấn luyện. Tuy nhiên, nó không phân phối bộ nhớ và mỗi GPU có một bản sao của toàn bộ mô hình. Song song hóa mô hình đạt được tăng tốc và bộ nhớ phân tán cho bài toán mô hình lớn bằng cách phân phối các tham số mô hình và gradient của chúng. Song song hóa chuỗi, ngược lại, tăng tốc tính toán và phân phối bộ nhớ cho các chuỗi dài bằng cách phân phối các đoạn chuỗi. Bài báo này song song hóa theo chiều chuỗi để mở rộng độ dài chuỗi bằng cách phân phối bộ nhớ và chi phí tính toán, đồng thời cẩn thận đảm bảo rằng song song hóa chuỗi không can thiệp hoặc cản trở các hình thức song song hóa khác.

2.2 Kiến Trúc Transformer

Hình 1(i)-(iii) minh họa kiến trúc transformer tiêu chuẩn cho cả mô hình chỉ decoder (tức GPT) và chỉ encoder (BERT). Embedding token trong Hình 1(i) chuyển đổi chuỗi đầu vào hình ảnh hoặc văn bản được token hóa x thành vector đầu vào. Vector đầu vào sau đó được tăng cường với thông tin vị trí được nhúng.

Tiếp theo, vector đầu vào x có kích thước lx×Em, với lx đại diện cho độ dài chuỗi và Em là kích thước embedding, trải qua chuẩn hóa lớp và dropout ngẫu nhiên trước khi được xử lý bởi self-attention. Các bước chi tiết cho self-attention được minh họa trong Hình 1(ii) với mục tiêu tạo ra embedding ngữ cảnh cho mỗi token đầu vào liên quan đến toàn bộ chuỗi. Trong đơn vị self-attention, vector đầu vào x được biến đổi tuyến tính thành các vector query (Q), key (K), và value (V), với cả ba vector có cùng kích thước như đầu vào x. Sau đó self-attention tính toán phương trình sau:

E=softmax(QKT/√dk)V=AwV . (1)

Điểm self-attention, ký hiệu là Aw, có kích thước lx×lx và định lượng tương quan giữa từng cặp token. Tương quan này được tính bằng cách lấy tích vô hướng giữa Q và K chuyển vị. Để ổn định gradient trong quá trình huấn luyện, điểm self-attention Aw được chia tỷ lệ thêm bởi hằng số √dk, và sau đó được chuẩn hóa bởi dropout và kích hoạt SoftMax. Đầu ra self-attention cuối cùng E được thu được bằng cách trung bình có trọng số giữa vector value V và Aw. Embedding đầu ra E sau đó được biến đổi tuyến tính trước khi thoát khỏi đơn vị self-attention.

Embedding ngữ cảnh đầu ra từ self-attention được xử lý thêm bởi đơn vị feed-forward, với các bước chi tiết được minh họa trong Hình 1(iii), kết nối dư và chuẩn hóa lớp để tăng cường sự ổn định huấn luyện và độ chính xác dự đoán. Các thành phần này, cùng với dropout, tạo thành một lớp attention duy nhất, được phác thảo bởi hộp màu xám trong Hình 1(i). Sau đó các lớp attention lặp lại chính nó L lần, trong đó L là số lớp attention. Sau khi trải qua chuẩn hóa lớp cuối cùng và biến đổi tuyến tính, đầu ra y được so sánh với mục tiêu ỹ sử dụng mất mát cross-entropy, và tất cả các tham số được cập nhật.

Điều quan trọng cần lưu ý là self-attention chiếm ưu thế cả về yêu cầu tính toán và bộ nhớ cho huấn luyện. Với kích thước lx×lx, điểm self-attention, Aw, có sự mở rộng bậc hai trong kích thước của nó với độ dài chuỗi tăng. Điều này dẫn đến độ phức tạp tính toán bậc ba và bộ nhớ bậc hai của transformer đối với độ dài chuỗi.

2.3 Song Song Hóa Chuỗi Tiên Tiến Nhất

Để song song hóa self-attention, phương pháp self-attention phân tán đơn giản, chẳng hạn như (Li et al., 2021; 2023), phân vùng cả vector đầu vào x và các vector được biến đổi tuyến tính Q, K và V thành các đoạn phân tán giữa các GPU. Ví dụ trong một tình huống với 3 GPU, GPU 1 nhận các đoạn đầu tiên của các vector này, cụ thể là x1, Q1, K1, và V1, và GPU 2 và 3 nhận đoạn thứ hai và thứ ba

--- TRANG 4 ---
Ultra-Long Sequence Distributed Transformer

Hình 1. (i) Một transformer chung với L lớp attention được phác thảo bởi hộp màu xám. (ii) và (iii) các đơn vị self-attention và feed-forward, tương ứng. (iv) Lượt truyền tiến cho song song hóa chuỗi cơ sở với 2 GPU phân tán chuỗi. Màu xanh dương cho biết các bước phân tán và màu xanh lá cho các bước tuần tự. (v) Ví dụ 2 GPU cho lượt truyền tiến của LSS Transformer.

tương ứng. Để tính toán Phương trình (1), mỗi GPU phải nhận đoạn của mọi GPU khác để tính toán điểm self-attention từng phần. Ví dụ, GPU 1 cần nhận K2 và K3 từ GPU 2 và 3 trước khi GPU 1 có thể tính toán điểm self-attention từng phần Q1K2T và Q1K3T. Sau đó, các điểm self-attention từng phần được tổng hợp qua các GPU thành điểm self-attention hoàn chỉnh, sau đó được sử dụng trong tích vô hướng với vector value V phân tán. Vì mỗi GPU phải giao tiếp với mọi GPU khác nhiều lần, tần suất truyền thông cho phương pháp self-attention phân tán đơn giản tăng bậc hai với nhiều GPU hơn, làm hạn chế đáng kể khả năng mở rộng của nó.

Để giải quyết hạn chế này, phương pháp của Nvidia (Korthikanti et al., 2022), được gọi là "song song hóa chuỗi cơ sở" trong phần còn lại của bài báo, tính toán self-attention và feed-forward tuần tự để tránh các truyền thông giữa GPU tăng bậc hai (Korthikanti et al., 2022). Tuy nhiên, nó song song hóa độc lập chuẩn hóa lớp và dropout trong chiều chuỗi, vì chúng thiếu các phụ thuộc giữa token như vậy (Korthikanti et al., 2022). Lưu ý rằng thuật ngữ "tính toán tuần tự" liên quan đến tính toán GPU đơn, trong khi "tính toán song song" đề cập đến những tính toán được thực hiện qua các GPU trong chiều chuỗi. Mặc dù self-attention và feed-forward được tính toán trong một GPU duy nhất, các tính toán của chúng vẫn được vector hóa thông qua các luồng song song trong GPU.

Hình 1(iv) tóm tắt song song hóa chuỗi cơ sở sử dụng ví dụ về 2 GPU. Trong lượt truyền tiến, embedding vị trí và token được tính toán tuần tự và đầu ra được phân tán thành các đoạn liền kề giữa các GPU dọc theo chiều chuỗi. Sau đó trong các lớp attention, feed-forward và self-attention được cập nhật tuần tự bởi một GPU duy nhất, và được đại diện bởi các hình chữ nhật màu xanh lá trong hình. Tất cả các bước khác được cập nhật độc lập với song song hóa chuỗi, được đại diện bởi các hình chữ nhật màu xanh dương. Các truyền thông gather và scatter được sử dụng trước và sau self-attention và feed-forward để đảm bảo cập nhật tuần tự cho chúng và cập nhật song song độc lập cho tất cả các bước khác. Cuối cùng, kết quả từ các GPU được thu thập cho chuẩn hóa lớp cuối cùng, biến đổi tuyến tính, và đánh giá mất mát cross-entropy. Trong lượt truyền ngược, tất cả các bước giống như lượt truyền tiến, ngoại trừ việc các truyền thông gather trong lượt truyền tiến được thay thế bằng reduce-scatter để đồng bộ hóa gradient giữa các GPU và các truyền thông scatter trong lượt truyền tiến được thay thế bằng các hoạt động gather trong lượt truyền ngược.

Mặc dù song song hóa chuỗi cơ sở tránh các truyền thông GPU thường xuyên trong self-attention như trong phương pháp phân tán đơn giản, nó có hai hạn chế chính. Thứ nhất, các bước self-attention và feed-forward tính toán chuyên sâu nhất là tuần tự. Thứ hai, chi phí truyền thông vẫn đáng kể với 8 truyền thông toàn cầu cho mỗi lớp attention (4 trong lượt truyền tiến và 4 trong lượt truyền ngược). Kết quả là, song song hóa chuỗi cơ sở chỉ đạt được tăng tốc 3% trên mô hình 22 tỷ tham số so với đường cơ sở không có tính toán lại lượt truyền ngược, và lên đến 29% tăng tốc so với đường cơ sở có tính toán lại lượt truyền ngược (Korthikanti et al., 2022).

--- TRANG 5 ---
Ultra-Long Sequence Distributed Transformer

3 DISTRIBUTED LONG SHORT-SEQUENCE TRANSFORMER

Để đạt được khả năng mở rộng xuất sắc, LSS Transformer phải phân phối self-attention, nhưng cũng vượt qua tần suất truyền thông tăng bậc hai. LSS Transformer thực hiện song song hóa chuỗi dựa trên các nguyên tắc sau:

Nguyên tắc 1: Tính Toán Độc Lập với Bộ Nhớ Phân Tán. Ngoại trừ self-attention, tất cả các tính toán khác như chuẩn hóa lớp, kết nối dư, dropout, feed-forward, biến đổi tuyến tính, embedding vị trí và token có thể được tính toán và phân phối độc lập giữa các GPU mà không có phụ thuộc trong chiều chuỗi. Lưu trữ bộ nhớ cho các bước này cũng được phân phối trong chiều chuỗi theo cách tương tự.

Hoạt động feed-forward có thể được tính toán độc lập bằng cách phân phối hàng nhân biến đổi tuyến tính của nó dọc theo chiều độ dài chuỗi, cho phép tính toán GPU độc lập. Ngoài ra, kích hoạt GeLu và dropout hoạt động độc lập trên từng phần tử đầu vào của chúng. Ví dụ, đối với đầu vào chuỗi x có chiều lx×Em, trong đó lx là độ dài chuỗi và Em là kích thước embedding, bước biến đổi tuyến tính đầu tiên trong đơn vị feed-forward, linear(x) trong Hình 1(iii), nhân x với các tham số mô hình biến đổi tuyến tính có kích thước Em×Dinner, trong đó Dinner là chiều của lớp ẩn feed-forward. Phép nhân ma trận này có thể được phân phối hàng giữa các GPU mà không cần truyền thông khi đầu vào x được phân phối thành các đoạn chuỗi xi có kích thước lx/N×Em, trong đó N là số GPU song song chuỗi. Sau các hoạt động theo phần tử độc lập trên GeLu và dropout, đầu ra phân tán chuỗi được sử dụng làm đầu vào phân tán cho phép nhân biến đổi tuyến tính thứ hai trong đơn vị feed forward, một lần nữa được phân phối hàng mà không cần truyền thông.

Hình 1(v) mô tả lượt truyền tiến của LSS Transformer, thể hiện một minh họa với 2 GPU song song chuỗi. Lưu ý rằng trong khi hình này ví dụ hóa một tình huống cụ thể, LSS Transformer hoạt động ở mức lớp attention, và có thể thích ứng phổ quát với các kích thước và loại mô hình khác nhau mà không cần sửa đổi. Trong hình này, chuỗi đầu vào x được phân tán thành x1 và x2 trong chiều chuỗi và mỗi GPU nhận một đoạn. Nguyên tắc 1 cho phép tất cả các hoạt động tiếp theo được phân phối chuỗi giữa các GPU và tính toán độc lập. Ngoài ra, đầu vào, đầu ra trung gian và gradient liên quan của từng hoạt động cũng được lưu trữ trong bộ nhớ phân tán qua các GPU song song chuỗi, cho phép giảm dấu chân bộ nhớ xuất sắc. Chúng tôi sử dụng các hình chữ nhật màu xanh dương trong hình để đại diện cho các bước tính toán độc lập này. Self-attention được đánh dấu bằng hình chữ nhật màu xanh dương có bóng để cho biết rằng self-attention được phân phối nhưng yêu cầu truyền thông giữa GPU.

Nguyên tắc 2: Embedding Vị Trí Phân Tán Chuỗi. Các tham số embedding vị trí là một bảng tra cứu và mỗi hàng của bảng đại diện cho vị trí không gian của token trong chuỗi. Số hàng của bảng tra cứu tương ứng với độ dài chuỗi. Vì mỗi GPU phân tán chuỗi nhận các đoạn chuỗi liền kề, GPU thực hiện các hoạt động tra cứu chỉ trên các hàng liền kề tương ứng của bảng tra cứu. Điều này cho phép phân phối hàng của các embedding vị trí giữa các GPU mà không có phụ thuộc.

Nguyên tắc 3: Self-Attention Phân Tán với Truyền Thông Hợp Nhất. Để song song hóa self-attention, chúng tôi sử dụng tính chất toán học sau để bảo toàn phụ thuộc dữ liệu trong khi giảm thiểu chi phí truyền thông. Bằng cách phân phối vector query, Q, trong chiều chuỗi giữa các GPU, chúng tôi tính toán đầu ra self-attention, E, như phép nối sau trong chiều chuỗi, và song song:

E = [
softmax(Q1KT/√dk)V
softmax(Q2KT/√dk)V  
softmax(Q3KT/√dk)V
...
], (2)

trong đó Qi là đoạn vector query phân tán lx/N×Em được nhận bởi GPU thứ i, trong đó N là số GPU. V và K là các vector value và key được thu thập lx×Em mà không có phân phối, có cùng bản sao trên tất cả các GPU. softmax(QiKT/√dk) đại diện cho điểm self-attention từng phần của GPU thứ i cho đoạn được gán của nó, nhưng trong bối cảnh của toàn bộ chuỗi.

Tóm lại, ý tưởng chính là phân phối vector query trong chiều chuỗi giữa các GPU, trong khi giữ các vector value và key được thu thập. Sau đó LSS Transformer tính toán một đầu ra self-attention cá nhân, softmax(QiKT/√dk)V, cho mỗi GPU. Phương trình (2) cho thấy rằng việc nối các đầu ra self-attention cá nhân của GPU giống về mặt số học với việc tính toán trực tiếp self-attention E tuần tự, vì phép nối là một phép thu thập hàng đơn giản cho E. Do đó, phương pháp self-attention phân tán đề xuất của chúng tôi là một phương pháp chính xác mà không có xấp xỉ, do đó không mất độ chính xác. So với song song hóa chuỗi đơn giản với tần suất truyền thông tăng bậc hai, một lợi thế đáng kể của Phương trình (2) là nó cho phép tính toán phân tán trong khi chỉ yêu cầu 6 lần truyền thông cho mỗi lớp self-attention. Lượt truyền tiến yêu cầu 2 lần truyền thông từ việc thu thập các vector value và key và 1 từ phép nối self-attention. Lượt truyền ngược yêu cầu thêm 3 lần truyền thông.

Để giảm thêm chi phí truyền thông, chúng tôi sử dụng kỹ thuật truyền thông hợp nhất để giảm tần suất truyền thông từ 6 lần truyền thông mỗi lớp xuống 4. Hình 2(i) minh họa các hoạt động trong lượt truyền tiến mà không có truyền thông hợp nhất. Một ví dụ đoạn chuỗi, xi

--- TRANG 6 ---
Ultra-Long Sequence Distributed Transformer

Hình 2. (i) và (ii) cho thấy sự khác biệt không có và có truyền thông hợp nhất. (iii) cho thấy lượt truyền tiến của self-attention phân tán với truyền thông hợp nhất. Lưu ý rằng các đầu ra self-attention phân tán không được nối. (iv) Lượt truyền ngược của LSS Transformer. Các tham số mô hình, ngoại trừ embedding vị trí, được đồng bộ hóa thông qua trung bình gradient. (v) Lượt truyền ngược của self-attention phân tán với reduce-scatter.

được biến đổi tuyến tính thành các đoạn query, key và value. Sau đó, hai lần truyền thông all-gather được hoạt động độc lập trên các đoạn key và value thành K và V được thu thập. Hình 2(ii) cho thấy hoạt động truyền thông hợp nhất trong lượt truyền tiến, chỉ yêu cầu một lần truyền thông all-gather duy nhất. xi được biến đổi tuyến tính thành đoạn query Qi. Trong khi đó, xi được thu thập thành một chuỗi được thu thập tạm thời x, trước khi x được biến đổi tuyến tính thành các vector key và value được thu thập. Kỹ thuật tương tự cũng được áp dụng cho lượt truyền ngược, giảm tổng số lần truyền thông từ 6 xuống 4 cho mỗi lớp attention.

Nguyên tắc 4: Kỹ Thuật Trung Bình Gradient để Đồng Bộ Hóa GPU và Tránh Nối. Có hai vấn đề từ Nguyên tắc 1 và 3. Thứ nhất, vì GPU song song chuỗi huấn luyện trên cùng các tham số mô hình nhưng sử dụng các đoạn chuỗi đầu vào khác nhau, các gradient cho các tham số mô hình là khác nhau cho mỗi GPU. Vấn đề thứ hai là tần suất truyền thông self-attention cần được giảm thêm để đạt được khả năng mở rộng và hiệu quả song song thậm chí tốt hơn.

Để giải quyết cả hai vấn đề, chúng tôi sử dụng kỹ thuật trung bình gradient để đồng bộ hóa các tham số mô hình và tránh phép nối cho các đầu ra self-attention cá nhân của GPU. Do đó, tần suất truyền thông được giảm từ 4 xuống 2 cho mỗi lớp attention. Hình 2(iii)-(v) sử dụng ví dụ 2 GPU để minh họa cách kỹ thuật trung bình gradient này được áp dụng. Trong lượt truyền tiến cho self-attention trong Hình 2(iii), một query phân tán Qi được tính toán từ đoạn chuỗi đầu vào xi. Trong khi đó, các đoạn đầu vào self-attention được thu thập giữa các GPU trước khi tính toán các vector K và V được thu thập bằng cách sử dụng một lần truyền thông all-gather hợp nhất, như đã giải thích trước đó trong Nguyên tắc 3. Các tính toán và lưu trữ bộ nhớ tiếp theo đều được phân phối và cập nhật độc lập trong chiều chuỗi, tạo ra đầu ra self-attention cá nhân cho mỗi GPU.

Tuy nhiên, các đầu ra self-attention cá nhân không được nối qua các GPU trong Hình 2(iii). Thay vào đó, LSS Transformer cho phép mỗi GPU sử dụng đoạn chuỗi được gán và đầu ra self-attention cá nhân của nó để tính toán mất mát cross-entropy từng phần và gradient trong lượt truyền ngược trong Hình 2(iv) và (v). Lưu ý rằng lượt truyền ngược trong Hình 2(v) sử dụng reduce-scatter làm hoạt động ngược cho all-gather trong lượt truyền tiến. Cuối cùng, các gradient trung bình được tính toán và sử dụng để cập nhật tham số mô hình đồng bộ trước khi huấn luyện trên batch dữ liệu tiếp theo. Một chi tiết kỹ thuật quan trọng cần đề cập là các gradient trung bình không được tính toán cho các embedding vị trí, là các tham số phân tán qua các GPU và không nên được đồng bộ hóa.

Để hiểu tại sao kỹ thuật trung bình gradient này có thể tránh phép nối self-attention và đồng bộ hóa các tham số mô hình cùng lúc, hãy giả sử rằng đầu ra chuỗi dự đoán từ transformer là y và nhãn đúng của nó là ỹ. Mất mát cross-entropy cho toàn bộ chuỗi, ký hiệu là L(y,ỹ), bằng trung bình của mất mát token cá nhân: L(y,ỹ) = 1/lx ∑i=1^lx L(yi,ỹi), trong đó lx là độ dài chuỗi. Theo quy tắc tổng gradient, gradient của L(y,ỹ) đối với các tham số mô hình, ký hiệu là ∇L(y,ỹ), bằng gradient trung bình của mất mát mỗi token: ∇L(y,ỹ) = 1/lx ∑i=1^lx ∇L(yi,ỹi). Do đó, không có

--- TRANG 7 ---
Ultra-Long Sequence Distributed Transformer

Hình 3. Song song hóa chuỗi và dữ liệu tích hợp với trung bình gradient kép. Trung bình gradient theo hướng ngang đồng bộ hóa các tham số mà không có embedding vị trí, và trung bình gradient theo hướng dọc bao gồm embedding vị trí.

cần phải nối các đầu ra self-attention cá nhân để tính toán mất mát và gradient cho toàn bộ chuỗi. Thay vào đó, mỗi GPU sử dụng đầu ra self-attention cá nhân được phân tán của nó để tính toán mất mát từng phần và gradient cho mỗi đoạn chuỗi, trước khi trung bình gradient của mỗi đoạn để cập nhật tham số mô hình đồng bộ.

Bằng cách tránh các hoạt động nối đắt đỏ trong mỗi lớp attention, LSS Transformer giảm tần suất truyền thông của nó xuống chỉ hai lần cho mỗi lớp attention (một all-gather trong lượt truyền tiến và một reduce-scatter trong lượt truyền ngược) vì trung bình gradient chỉ xảy ra một lần cho mỗi batch dữ liệu. Điều này dẫn đến khả năng mở rộng tốt hơn nhiều và giảm truyền thông so với các phương pháp song song chuỗi khác.

4 SONG SONG HÓA CHUỖI & DỮ LIỆU TÍCH HỢP

Song song hóa chuỗi của LSS Transformer có ba hạn chế. Thứ nhất, nó vẫn yêu cầu 2 lần truyền thông toàn cầu giữa GPU cho mỗi lớp attention, điều này làm giảm hiệu quả song song ở nhiều GPU. Thứ hai, trong khi song song hóa chuỗi giải quyết vấn đề chuỗi dài, nó không giải quyết thách thức tính toán để huấn luyện bộ dữ liệu lớn. Thứ ba, song song hóa chuỗi chỉ là một nguồn song song hóa. Để mở rộng đến một siêu máy tính lớn để huấn luyện, LSS Transformer cần nhiều nguồn song song hóa hơn để đạt được khả năng mở rộng tốt hơn. Để giải quyết những vấn đề này, phần này giới thiệu một phương pháp để tích hợp song song hóa chuỗi của LSS Transformer với song song hóa dữ liệu. Với sự tích hợp, thuật toán song song có thể (1) đạt được khả năng mở rộng tốt hơn; (2) đồng thời giải quyết các thách thức chuỗi dài và bộ dữ liệu lớn; và (3) hạn chế các truyền thông self-attention trong các nhóm giao tiếp cục bộ để giảm chi phí.

Mặc dù song song hóa chuỗi và dữ liệu chủ yếu là trực giao, một thách thức kỹ thuật cần vượt qua là cả hai song song hóa đều yêu cầu đồng bộ hóa tham số mô hình, nhưng giữa các GPU trong các nhóm giao tiếp khác nhau và giao tiếp theo các cách khác nhau. Song song hóa chuỗi yêu cầu đồng bộ hóa tham số mô hình giữa các GPU song song chuỗi, nhưng loại trừ các tham số embedding vị trí khỏi đồng bộ hóa do các embedding vị trí được phân phối trong chiều chuỗi. Song song hóa dữ liệu yêu cầu đồng bộ hóa tham số mô hình giữa các GPU song song dữ liệu, nhưng phải bao gồm embedding vị trí do các GPU song song dữ liệu có cùng bản sao của các tham số embedding vị trí, nhưng huấn luyện chúng với các batch dữ liệu khác nhau.

Để giải quyết vấn đề này, chúng tôi sử dụng kỹ thuật trung bình gradient kép sáng tạo để tránh xung đột đồng bộ hóa cho embedding vị trí. Hình 3 minh họa một ví dụ về cách song song hóa chuỗi và dữ liệu tích hợp sử dụng trung bình gradient kép. Trong ví dụ này, GPU 1 và 2 xử lý một chuỗi x1 cùng nhau bằng song song hóa chuỗi, với đoạn đầu tiên x1₁ được gán cho GPU 1 và đoạn thứ hai x1₂ được gán cho GPU 2. Các tham số embedding vị trí được phân phối theo cách tương tự với nửa đầu PE1 được gán cho GPU 1 và nửa thứ hai PE2 được gán cho GPU 2. Tương tự, GPU 3 và 4 xử lý một chuỗi khác x2 bằng song song hóa chuỗi.

Tất cả các GPU xử lý cùng một chuỗi tạo thành một nhóm song song chuỗi và mỗi nhóm được hiển thị như một hộp màu tím ngang trong Hình 3. Các truyền thông giữa GPU của mỗi nhóm song song chuỗi, hiển thị như mũi tên màu tím ngang, là cục bộ và được giới hạn giữa các GPU trong cùng nhóm song song chuỗi. Các truyền thông này bao gồm all-gather hợp nhất và reduce-scatter trong mỗi lớp attention để tính toán self-attention phân tán. Ngoài ra, cần trung bình gradient một lần cho mỗi batch dữ liệu để đồng bộ hóa tham số mô hình và tránh phép nối đầu ra self-attention, như đã thảo luận trước đó trong Phần 3. Tuy nhiên, các tham số embedding vị trí được loại trừ khỏi trung bình gradient, vì chúng được phân phối qua các GPU song song chuỗi, và không nên được đồng bộ hóa.

Trong khi đó, GPU 1 và 3 nằm trong một nhóm song song dữ liệu, hiển thị như một hộp màu đỏ dọc trong hình, và GPU 2 và 4 nằm trong một nhóm song song dữ liệu khác. Các GPU trong cùng nhóm song song dữ liệu xử lý các đoạn chuỗi từ các batch dữ liệu khác nhau, nhưng các đoạn chuỗi này chia sẻ cùng vị trí không gian trong chuỗi của chúng, do đó chia sẻ cùng embedding vị trí. Truyền thông duy nhất cần thiết giữa GPU trong cùng nhóm song song dữ liệu, hiển thị như mũi tên màu đỏ dọc trong hình, là trung bình gradient để đồng bộ hóa các tham số được huấn luyện với các batch khác nhau. Tương tự như các nhóm song song chuỗi, truyền thông cho các nhóm song song dữ liệu cũng được cục bộ hóa và giới hạn trong mỗi nhóm. Trung bình gradient cho song song hóa dữ liệu,

--- TRANG 8 ---
Ultra-Long Sequence Distributed Transformer

Bảng 3. Thí nghiệm mở rộng yếu của LSS Transformer và song song hóa chuỗi cơ sở Nvidia cho thí nghiệm mô hình nhỏ, chỉ sử dụng 1 nhóm song song dữ liệu.

(a) LSS Transformer, nhóm song song dữ liệu = 1
[THIS IS TABLE: showing scaling data for LSS Transformer with columns for Nodes, GPUs, Sequence Groups, Sequence Length, Per GPU Mem, FLOPS, Self-Attn Comp Incr, and Parallel Efficiency across 5 different configurations]

(b) Cơ sở, nhóm song song dữ liệu = 1
[THIS IS TABLE: showing similar data for baseline configuration but only showing 3 configurations, with later ones marked as OOM (Out of Memory)]

Bảng 4. Mở rộng yếu của LSS Transformer và cơ sở Nvidia cho thí nghiệm mô hình nhỏ, sử dụng 4 nhóm song song dữ liệu.

(a) LSS Transformer, nhóm song song dữ liệu = 4
[THIS IS TABLE: showing scaling data for LSS Transformer with 4 data parallel groups across 5 configurations]

(b) Cơ sở, nhóm song song dữ liệu = 4
[THIS IS TABLE: showing baseline data with 4 data parallel groups across 3 configurations]

tuy nhiên, phải bao gồm embedding vị trí để đồng bộ hóa, do các đoạn huấn luyện trong cùng nhóm song song dữ liệu chia sẻ cùng embedding vị trí.

5 KẾT QUẢ

5.1 Thiết Lập Thí Nghiệm

Bộ dữ liệu: enwik8 là bộ dữ liệu huấn luyện ký tự Wikipedia XML 100 triệu byte phổ biến (Hutter et al., 2006; Beltagy et al., 2020). Ban đầu được sử dụng làm bộ dữ liệu kiểm tra cho Giải thưởng Hutter, bộ dữ liệu có thể được tải xuống tại (Mahoney, 2006) để đánh giá benchmark công khai với bảng điểm có sẵn tại (with Codes, 2022).

Nền tảng tính toán: Các thí nghiệm được thực hiện trên siêu máy tính Summit của Oak Ridge National Lab, có 6 GPU NVIDIA V100 và 2 CPU POWER9 cho mỗi nút. Các nút được kết nối qua mạng Mellanox EDR 100G InfiniBand Non-blocking Fat Tree. Mỗi CPU POWER9 trong nút được kết nối dày đặc với 3 GPU với Nvidia NVlinks, trong đó mỗi liên kết có băng thông hai chiều 100 GB/s, và hai CPU cho mỗi nút được kết nối qua bus X với băng thông hai chiều 64 GB/s. Mỗi CPU có 22 lõi (4 luồng phần cứng cho mỗi lõi) và bộ nhớ DRAM 256 GB. Mỗi GPU có 80 bộ xử lý đa luồng và bộ nhớ 16 GB. Có thêm 54 nút "bộ nhớ cao", có 32 GB bộ nhớ mỗi GPU.

Phần mềm: LSS Transformer được phát triển trong PyTorch và sẽ được công khai trong phiên bản tiếp theo.

5.2 Thí Nghiệm Mô Hình Nhỏ

Mô hình: Tất cả các thí nghiệm trong phần thí nghiệm mô hình nhỏ huấn luyện một transformer chỉ decoder (GPT) với kích thước embedding 512, 6 lớp attention và 8 multi-head với tổng cộng 20 triệu tham số. Kích thước batch dữ liệu đầu vào là 4. Chúng tôi chọn kích thước mô hình này vì hai lý do. Thứ nhất, đây là mô hình tiêu chuẩn cho đánh giá benchmark enwik8 với điểm số độ chính xác bits-per-character xuất sắc ở 1.0 (Beltagy et al., 2020; Al-Rfou et al., 2019; Sukhbaatar et al., 2019). Thứ hai, chọn kích thước mô hình nhỏ cho phép chúng tôi tối đa hóa việc sử dụng bộ nhớ và đánh giá hiệu suất để mở rộng chuỗi dài, thay vì tối đa hóa bộ nhớ để lưu trữ tham số cho mô hình lớn.

Mở Rộng Yếu Song Song Hóa Chuỗi: Bảng 3(a) và 3(b) là so sánh hiệu suất mở rộng yếu giữa LSS Transformer và song song hóa chuỗi cơ sở Nvidia. Chúng tôi tăng cả độ dài chuỗi và số GPU song song chuỗi với cùng tốc độ, trong khi giữ số nhóm song song dữ liệu là 1.

Hai hàng đầu tiên của các bảng cho biết số nút và GPU, với 6 GPU mỗi nút. Hàng thứ ba đại diện cho số nhóm song song chuỗi, bằng với số GPU trong trường hợp này vì số

--- TRANG 9 ---
Ultra-Long Sequence Distributed Transformer

(a) Hiệu quả mở rộng có và không có truyền thông hợp nhất và trung bình gradient
(b) Độ dài chuỗi tối đa  
(c) Phân tích thời gian chạy cho Bảng 3(a)

Hình 4. (a) Hiệu quả mở rộng có và không có truyền thông hợp nhất và trung bình gradient. (b) độ dài chuỗi tối đa ở các số GPU song song chuỗi khác nhau. (c) phân tích thời gian chạy cho thí nghiệm mở rộng yếu mô hình nhỏ của LSS Transformer chỉ với 1 nhóm song song dữ liệu.

Bảng 5. So sánh hiệu suất giữa hai thuật toán trên mô hình nhỏ, chỉ sử dụng 1 nhóm song song dữ liệu. Độ dài chuỗi tăng dưới tuyến tính với tốc độ tăng trưởng tỷ lệ với căn bậc hai của số GPU.

(a) LSS Transformer, nhóm song song dữ liệu = 1
[THIS IS TABLE: Shows performance metrics for LSS Transformer with 1 data parallel group across different node configurations]

(b) Cơ sở, nhóm song song dữ liệu = 1
[THIS IS TABLE: Shows baseline performance metrics with 1 data parallel group across different node configurations]

nhóm song song dữ liệu là 1. Hàng thứ tư cho thấy độ dài chuỗi, tăng tỷ lệ thuận với số GPU. Hàng thứ năm hiển thị dấu chân bộ nhớ đỉnh trung bình mỗi GPU tính bằng Gigabyte (GB). Ở 6 nút, dấu chân bộ nhớ mỗi GPU của LSS Transformer là 1.01 GB, và nó mở rộng đến 144 nút với 13.58 GB mỗi GPU. Vì Transformer có độ phức tạp bộ nhớ bậc hai O(lx²/N), trong đó lx là độ dài chuỗi và N là số GPU, việc tăng độ dài chuỗi lx và số GPU N với cùng tốc độ vẫn sẽ dẫn đến tăng tuyến tính của bộ nhớ. Điều này giải thích tại sao LSS Transformer có dấu chân bộ nhớ nhỏ ở 1 nút nhưng tăng đến dấu chân bộ nhớ lớn hơn nhiều ở nhiều nút hơn. So sánh, song song hóa chuỗi cơ sở có dấu chân bộ nhớ mỗi GPU 10.29 GB ở 6 nút, lớn hơn 10 lần so với LSS Transformer ở cùng số nút. Song song hóa chuỗi cơ sở không thể mở rộng vượt quá 6 nút do ràng buộc bộ nhớ, dẫn đến "OOM" (hết bộ nhớ).

Hàng thứ sáu đại diện cho số phép toán dấu phẩy động độ chính xác đơn (FLOP) qua tất cả GPU trong một giây. LSS Transformer đạt được thông lượng tính toán 8 petaflops ở 144 nút với độ dài chuỗi 50,112. So sánh, song song hóa chuỗi cơ sở chậm hơn 5.9 lần ở 6 nút, đạt thông lượng 32 teraflops, và không thể mở rộng thêm do ràng buộc bộ nhớ.

Hàng thứ bảy cho thấy sự gia tăng tính toán mỗi GPU được ghi nhận cho đơn vị self-attention tương đối với tính toán mỗi GPU ở 1 nút. Vì transformer có độ phức tạp tính toán bậc ba, phân phối tính toán qua các GPU vẫn sẽ dẫn đến tăng tính toán cho mở rộng yếu.

Hàng thứ tám đại diện cho hiệu quả song song, là tỷ số giữa tăng tốc thực tế và tăng tốc lý tưởng. LSS Transformer duy trì hiệu quả song song siêu tuyến tính 151% ở 144 nút, trong khi hiệu quả của song song hóa chuỗi cơ sở giảm xuống 42% chỉ ở 6 nút.

Mở Rộng Yếu Song Song Hóa Chuỗi & Dữ Liệu Tích Hợp: Để hiểu cách tích hợp song song hóa dữ liệu và chuỗi tăng tốc độ huấn luyện và giảm chi phí truyền thông, Bảng 4(a) lặp lại cùng thí nghiệm như Bảng 3(a), nhưng với 4 nhóm song song dữ liệu. Điều này có nghĩa là tổng số GPU tăng gấp bốn lần tương ứng, nhưng số nhóm song song chuỗi vẫn giữ nguyên như trước. Bằng cách so sánh kết quả giữa 4 nhóm song song dữ liệu và nhóm song song dữ liệu đơn trong Bảng 3 và 4, chúng tôi quan sát thấy thông lượng FLOP tăng gần 4 lần từ 1 đến 4 nhóm song song dữ liệu với 4 lần nhiều GPU hơn, đạt 32 petaflops ở 3456 GPU. Kết quả này cho thấy rằng sơ đồ truyền thông cục bộ đề xuất cho phép

--- TRANG 10 ---
Ultra-Long Sequence Distributed Transformer

song song hóa chuỗi và dữ liệu tích hợp với ít chi phí truyền thông bổ sung và việc tích hợp hai song song hóa này là một cách tiếp cận hiệu quả để đạt được khả năng mở rộng và huấn luyện nhanh hơn.

Tăng Tốc Siêu Tuyến Tính: LSS Transformer đạt được mở rộng siêu tuyến tính trong Bảng 3 và 4 do hai lý do. Thứ nhất, chuỗi dài hơn dẫn đến tăng công việc cho mỗi GPU do sự gia tăng tính toán self-attention, dẫn đến tỷ lệ sử dụng GPU cao hơn cho chuỗi dài hơn và mở rộng siêu tuyến tính. Được đo bằng báo cáo sử dụng PyTorch Cuda, tỷ lệ sử dụng GPU cho LSS Transformer tăng từ 33% ở 1 nút với độ dài chuỗi 348 đến 83% ở 6 nút với độ dài chuỗi 2,088. Thứ hai, chi phí truyền thông thấp của LSS Transformer đóng góp đáng kể vào hiệu quả song song xuất sắc của nó. Hình 4(a) cho thấy hiệu quả mở rộng có và không có kỹ thuật truyền thông hợp nhất và trung bình gradient, cả hai đều được giới thiệu trong Phần 3. Ở 864 GPU và độ dài chuỗi 50,112, hiệu quả mở rộng với cả hai kỹ thuật là 151%. Hiệu quả với trung bình gradient nhưng không có truyền thông hợp nhất là 147%, trong khi hiệu quả không có kỹ thuật nào giảm xuống 118%.

Độ Dài Chuỗi Tối Đa: Hình 4(b) mô tả độ dài chuỗi tối đa khi mở rộng số GPU trong khi tối đa hóa dung lượng bộ nhớ. Mỗi cặp số trong hình tương ứng với số GPU và độ dài chuỗi tối đa. Ví dụ, (6,0.59) cho biết rằng 6 GPU có thể mở rộng đến độ dài chuỗi tối đa 0.59×10⁴. Chúng ta có thể quan sát thấy độ dài chuỗi tối đa theo đường cong căn bậc hai trong đồ thị. Vì transformer có độ phức tạp bộ nhớ bậc hai với chuỗi dài hơn, độ dài chuỗi tối đa tăng tiệm cận với hàm căn bậc hai khi tổng dung lượng bộ nhớ tăng.

Phân Tích Thời Gian Chạy. Hình 4(c) minh họa phân tích thời gian chạy cho kết quả mở rộng yếu được trình bày trong Bảng 3(a), tập trung vào một nhóm song song dữ liệu duy nhất. Các thanh màu xanh dương đại diện cho tỷ lệ phần trăm thời gian chạy dành cho tính toán, trong khi các thanh màu cam cho biết tỷ lệ phần trăm thời gian chạy cho truyền thông. Các thanh màu xám biểu thị thời gian chờ GPU. Ở 36 GPU, chi phí truyền thông chiếm 27% tổng thời gian chạy. Khi số GPU mở rộng đến 864 (144 nút), chi phí truyền thông trở thành 40% thời gian chạy.

Mở Rộng Độ Dài Chuỗi Tăng Dưới Tuyến Tính. Một cách để hạn chế sự gia tăng bộ nhớ và tính toán là tăng độ dài chuỗi với tốc độ dưới tuyến tính. Bảng 5 lặp lại cùng thí nghiệm mở rộng như Bảng 3, nhưng với độ dài chuỗi tăng tỷ lệ thuận với căn bậc hai của số GPU. Kết quả là, cả bộ nhớ và tính toán self-attention từ hàng 5 và 7 của bảng cũng tăng với tốc độ tiệm cận với hàm căn bậc hai. Dấu chân bộ nhớ cho LSS-Transformer chỉ ở 1.15 GB mỗi GPU khi mở rộng đến 864 GPU, và tính toán self-attention chỉ nhiều hơn 10 lần so với một nút duy nhất. Mở rộng của LSS-Transformer vẫn rất hiệu quả ở 94% với 864 GPU. Ngược lại, hiệu quả mở rộng cho song song hóa chuỗi của Nvidia giảm xuống 31% ở 108 GPU và không thể mở rộng thêm do ràng buộc bộ nhớ.

5.3 Thí Nghiệm Mô Hình Lớn

Bảng 6 lặp lại cùng thí nghiệm như Bảng 5, nhưng huấn luyện mô hình GPT lớn 1.5 tỷ tham số có kích thước embedding 2048, 24 lớp attention và 16 multi-head. Các thí nghiệm được chạy trên các nút bộ nhớ cao của Summit với 32 GB bộ nhớ mỗi GPU, và không sử dụng song song hóa mô hình cho thí nghiệm này. Vì hầu hết dung lượng bộ nhớ hiện được sử dụng để lưu trữ tham số mô hình thay vì chuỗi dài, chúng ta có thể nhận thấy rằng tất cả các lần chạy trong bảng này sử dụng nhiều bộ nhớ hơn so với thí nghiệm mô hình nhỏ. LSS Transformer duy trì hiệu quả mở rộng cao 92% ở 108 GPU và dấu chân bộ nhớ nhỏ hơn so với song song hóa cơ sở. Ngược lại, song song hóa cơ sở không thể mở rộng vượt quá một nút duy nhất do ràng buộc bộ nhớ và thông lượng FLOP của nó nhỏ hơn 2.3 lần so với LSS Transformer ở một nút duy nhất.

6 KẾT LUẬN

Bài báo này giới thiệu Long-Short Sequence Transformer (LSS Transformer), một thuật toán mới và khung tổng quát để phân phối chuỗi dài trong các mô hình transformer. Nó sử dụng cơ chế self-attention phân tán mới, cùng với kỹ thuật truyền thông hợp nhất và trung bình gradient kép, để đạt được tăng tốc ấn tượng và giảm bộ nhớ với chi phí truyền thông tối thiểu. Kết luận, LSS Transformer là một bước tiến đáng kể để giải quyết vấn đề chuỗi dài của transformer. Chúng tôi tin rằng cách tiếp cận của chúng tôi cung cấp một đóng góp quan trọng cho lĩnh vực nghiên cứu và cho phép huấn luyện chuỗi cực dài, đặc biệt cho các ứng dụng hưởng lợi từ phụ thuộc token tầm xa, chẳng hạn như phân tích chuỗi DNA, tóm tắt tài liệu dài, và các ứng dụng hình ảnh.

TÀI LIỆU THAM KHẢO

Al-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L. Character-level language modeling with deeper self-attention. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01): 3159–3166, July 2019. doi: 10.1609/aaai.v33i01. 33013159. URL https://ojs.aaai.org/index. php/AAAI/article/view/4182.

Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The

--- TRANG 11 ---
Ultra-Long Sequence Distributed Transformer

Bảng 6. So sánh hiệu suất giữa hai thuật toán trên mô hình lớn 1.5 tỷ, chỉ sử dụng 1 nhóm song song dữ liệu, và không có bất kỳ song song hóa mô hình nào. Độ dài chuỗi tăng với tốc độ tăng trưởng tỷ lệ thuận với căn bậc hai của số GPU.

(a) LSS Transformer, nhóm song song dữ liệu = 1
Nút | 1 | 6 | 18
GPU | 6 | 36 | 108
Nhóm Chuỗi | 6 | 36 | 108
Độ Dài Chuỗi | 366 | 900 | 1512
Bộ Nhớ Mỗi GPU (GB) | 21.67 | 22.48 | 23.34
FLOPS (x 10¹² flop/s) | 52 | 518 | 2010
Tăng Tính Toán Self-Attn | 1 | 2 | 4
Hiệu Quả Song Song | 100% | 94% | 92%

(b) Cơ sở, nhóm song song dữ liệu = 1
Nút | 1 | 6
GPU | 6 | 36
Nhóm Chuỗi | 6 | 36
Độ Dài Chuỗi | 366 | 900
Bộ Nhớ Mỗi GPU (GB) | 25.28 | OOM
FLOPS (x 10¹² flop/s) | 23 | OOM
Tăng Tính Toán Self-Attn | 1 | OOM
Hiệu Quả Song Song | 100% | OOM

long-document transformer, 2020.

Caldarini, G., Jaf, S., and McGarry, K. A literature survey of recent advances in chatbots. Information, 13(1):41, 2022.

Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and Ré, C. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34: 17413–17426, 2021a.

Chen, C.-F. R., Fan, Q., and Panda, R. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 357–366, New York, NY, USA, 2021b. IEEE.

Chen, R. J., Chen, C., Li, Y., Chen, T. Y., Trister, A. D., Krishnan, R. G., and Mahmood, F. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16123–16134, New York, NY, USA, 2022. IEEE. doi: 10.1109/CVPR52688.2022.01567.

Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers, 2019. URL https://arxiv.org/abs/1904.10509.

Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L., and Weller, A. Rethinking attention with performers. In The International Conference on Learning Representations (ICLR), New York, NY, USA, 2021. Association for Computing Machinery. doi: 10.48550/ARXIV.2009.14794. URL https://arxiv.org/abs/2009.14794.

Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS: Proceedings of the 35th Neural Information Processing Systems Conference, New York, NY, USA, 2022. Association for Computing Machinery. doi: 10.48550/ARXIV.2205.14135. URL https://arxiv.org/abs/2205.14135.

Dong, L., Xu, S., and Xu, B. Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5884–5888, New York, NY, USA, 2018. IEEE.

Hutter, M., Mahoney, M., and Bowery, J. 500'000 C prize for compressing human knowledge. http://prize. hutter1.net/, 2006. Accessed: 2023-03-05.

Jacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song, S. L., Rajbhandari, S., and He, Y. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models, 2023.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML'20, New York, NY, USA, 2020. Association for Computing Machinery.

Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. In The International Conference on Learning Representations (ICLR), New York, NY, USA, 2020. Association for Computing Machinery. doi: 10.48550/ARXIV.2001.04451. URL https://arxiv. org/abs/2001.04451.

Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B. Reducing activation recomputation in large transformer models, 2022. URL https://arxiv.org/abs/2205.05198.

Li, D., Shao, R., Xie, A., Xing, E. P., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. Lightseq: Sequence level parallelism for distributed training of long context transformers, 2023.

--- TRANG 12 ---
Ultra-Long Sequence Distributed Transformer

Li, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence parallelism: Long sequence training from system perspective, 2021. URL https://arxiv.org/abs/2105. 13120.

Mahoney, M. Enwik8 test data. https:// mattmahoney.net/dc/textdata.html, 2006. Accessed: 2023-03-05.

Rabe, M. N. and Staats, C. Self-attention does not need o(n2) memory, 2022.

Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient Content-Based Sparse Attention with Routing Transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 02 2021. ISSN 2307-387X. doi: 10.1162/tacl a 00353. URL https://doi.org/10. 1162/tacl_a_00353.

Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., and Kwok, J. T. Sparsebert: Rethinking the importance analysis in self-attention. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9547–9557, New York, NY, USA, 2021. PMLR. URL http://proceedings.mlr.press/v139/ shi21a.html.

Si, Y. and Roberts, K. Three-level hierarchical transformer networks for long-sequence and multiple clinical documents classification, 2021. URL https://arxiv. org/abs/2104.08444.

Strudel, R., Garcia, R., Laptev, I., and Schmid, C. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 7262–7272, New York, NY, USA, 2021. IEEE.

Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 331–335, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1032. URL https: //aclanthology.org/P19-1032.

Valanarasu, J. M. J., Oza, P., Hacihaliloglu, I., and Patel, V. M. Medical transformer: Gated axial-attention for medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, pp. 36–46, New York, NY, USA, 2021. Springer.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, pp. 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.

Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., and Chao, L. S. Learning deep transformer models for machine translation. In 57th Annual Meeting of the Association for Computational Linguistics, pp. 1810–1822, New York, NY, USA, 2019. Association for Computing Machinery. doi: 10.48550/ARXIV.1906.01787. URL https://arxiv.org/abs/1906.01787.

with Codes, P. Language modelling on enwik8. https://paperswithcode.com/sota/ language-modelling-on-enwiki8, 2022. Accessed: 2023-03-05.

Ying, C., Ke, G., He, D., and Liu, T.-Y. Lazyformer: Self attention with lazy update, 2021.

Yu, J., Li, J., Yu, Z., and Huang, Q. Multimodal transformer with multi-view visual representation for image captioning. IEEE Transactions on Circuits and Systems for Video Technology, 30(12):4467–4480, 2019.

Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. Megabyte: Predicting million-byte sequences with multiscale transformers, 2023.

Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297, 2020.
