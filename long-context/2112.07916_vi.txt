# LongT5: Transformer Văn Bản Sang Văn Bản Hiệu Quả cho Chuỗi Dài

Mandy Guoy, Joshua Ainsliey, David Uthus, Santiago Ontañón
Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang
Google Research
{xyguo, jainslie, duthus, santiontanon, jianmon, yhsung, yinfeiy}@google.com

## Tóm tắt

Các nghiên cứu gần đây đã cho thấy rằng việc (1) tăng độ dài đầu vào hoặc (2) tăng kích thước mô hình có thể cải thiện hiệu suất của các mô hình thần kinh dựa trên Transformer. Trong bài báo này, chúng tôi trình bày LongT5, một mô hình mới khám phá tác động của việc mở rộng đồng thời cả độ dài đầu vào và kích thước mô hình. Cụ thể, chúng tôi tích hợp các ý tưởng attention từ các transformer đầu vào dài (ETC), và áp dụng các chiến lược tiền huấn luyện từ tiền huấn luyện tóm tắt (PEGASUS) vào kiến trúc T5 có thể mở rộng. Kết quả là một cơ chế attention mới mà chúng tôi gọi là Transient Global (TGlobal), mô phỏng cơ chế attention cục bộ/toàn cục của ETC, nhưng không yêu cầu các đầu vào phụ bổ sung. Chúng tôi có thể đạt được kết quả tiên tiến trên một số tác vụ tóm tắt và trả lời câu hỏi, cũng như vượt trội so với các mô hình T5 gốc trên các tác vụ này. Chúng tôi đã mở mã nguồn kiến trúc và mã huấn luyện của mình, cũng như các checkpoint mô hình đã được tiền huấn luyện.

## 1 Giới thiệu

Các mô hình Transformer như BERT (Devlin et al., 2019), và các biến thể khác (Liu et al., 2019; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020) đã đạt được kết quả tiên tiến trên nhiều tác vụ NLP thách thức. Hơn nữa, nghiên cứu gần đây về các transformer đầu vào dài (Ainslie et al., 2020; Zaheer et al., 2020b; Beltagy et al., 2020; Tay et al., 2021) đã cho thấy rằng việc tăng độ dài đầu vào mà Transformer có thể xử lý dẫn đến các cải thiện hiệu suất tiếp theo. Ngoài ra, cũng được biết rằng việc tăng kích thước mô hình cũng dẫn đến cải thiện hiệu suất trong nhiều tác vụ (Kaplan et al., 2020).

Trong bài báo này, chúng tôi trình bày một mô hình mới, được gọi là LongT5, với đó chúng tôi khám phá tác động của việc mở rộng đồng thời cả độ dài đầu vào và kích thước mô hình. Để đạt được điều này, chúng tôi tích hợp các ý tưởng attention transformer đầu vào dài và tiền huấn luyện vào kiến trúc mô hình T5 (Raffel et al., 2019a) có thể mở rộng. Mô hình kết quả, như được hiển thị trong Hình 1, đạt được hiệu suất tiên tiến trên một số tác vụ yêu cầu xử lý đầu vào chuỗi dài.

Về attention, chúng tôi thiết kế một cơ chế attention mới, mà chúng tôi gọi là Transient Global (TGlobal), mô phỏng cơ chế cục bộ/toàn cục của ETC (Ainslie et al., 2020). Quan trọng là, attention TGlobal loại bỏ nhu cầu về các đầu vào phụ bổ sung trong ETC, để phù hợp với kiến trúc T5. Ý tưởng chính của cơ chế cục bộ/toàn cục của ETC là giới thiệu sự thưa thớt cục bộ trong cơ chế attention để giảm chi phí bậc hai khi mở rộng lên các đầu vào dài. Cụ thể, ETC chỉ cho phép các token trong đầu vào (được gọi là đầu vào dài) chú ý đến một vùng lân cận cục bộ, và thêm một đầu vào thứ cấp được gọi là bộ nhớ toàn cục, thông qua đó các token trong đầu vào dài có thể chú ý đến nhau một cách gián tiếp.

Một nhược điểm của cơ chế này là nó yêu cầu thiết kế đầu vào toàn cục thứ cấp này cho mỗi bài toán mới. Để thích ứng nó với T5, cơ chế TGlobal mới của chúng tôi tổng hợp các token toàn cục này một cách tức thời (như các tập hợp của các nhóm token trong đầu vào), tại mỗi lớp attention. Các thí nghiệm của chúng tôi cho thấy rằng cơ chế này chỉ dẫn đến sự suy giảm hiệu suất nhỏ so với attention đầy đủ ở cùng độ dài đầu vào nhưng cho phép mô hình mở rộng lên độ dài đầu vào lớn hơn nhiều, dẫn đến cải thiện hiệu suất đáng kể.

Về tiền huấn luyện, chúng tôi áp dụng chiến lược tiền huấn luyện trong mô hình PEGASUS (Zhang et al., 2019a). Chiến lược tiền huấn luyện này ban đầu được thiết kế cho tóm tắt trừu tượng, nhưng trong các thí nghiệm của chúng tôi, chúng tôi thấy nó cũng cải thiện hiệu suất mô hình cho các tác vụ khác, như trả lời câu hỏi, và do đó chúng tôi đã áp dụng nó trong LongT5. Ý tưởng chính là che giấu các câu chính (nguyên tắc) từ một tài liệu và yêu cầu mô hình tái tạo chúng như một chuỗi duy nhất, như thể đó là một bản tóm tắt.

Chúng tôi đánh giá LongT5 trên một số tác vụ tóm tắt và trả lời câu hỏi (xem Phần 4.2.1 và 4.3.1 để mô tả chi tiết về các bộ dữ liệu này). Nhờ việc mở rộng cả độ dài đầu vào và kích thước mô hình, chúng tôi đạt được kết quả tiên tiến trên nhiều tác vụ trong số đó.

Các đóng góp chính của công trình này là:

• Một kiến trúc Transformer mới, LongT5, cho phép mở rộng đồng thời cả độ dài đầu vào và quy mô mô hình.

• Một cơ chế attention mới (TGlobal), mô phỏng cơ chế cục bộ/toàn cục của ETC nhưng là một thay thế trực tiếp cho attention thông thường cho các kiến trúc Transformer hiện có như T5.

• Một phân tích về hiệu suất mô hình khi thay đổi cả độ dài đầu vào và kích thước mô hình của các mô hình T5 và LongT5 vanilla (đẩy cả hai mô hình lên độ dài tối đa mà chúng có thể xử lý trước khi gặp vấn đề về bộ nhớ), để hiểu các đánh đổi về cả hiệu suất và chi phí tính toán.

• Kết quả tiên tiến trên các bộ dữ liệu arXiv, PubMed, BigPatent, MediaSum và TriviaQA. Đối với Natural Questions, chúng tôi đã sử dụng một công thức hóa hơi khác so với các tác vụ gốc, và do đó chúng tôi không đưa ra các tuyên bố tiên tiến.

• Chúng tôi mở mã nguồn kiến trúc mô hình¹ và mã huấn luyện của mình, cũng như các checkpoint mô hình đã được tiền huấn luyện trên GitHub².

¹Được xuất bản dưới GitHub Flaxformer https://github.com/google/flaxformer/tree/main/flaxformer/architectures/longt5
²https://github.com/google-research/longt5

## 2 T5

T5 (Raffel et al., 2019a) là một mô hình ngôn ngữ tiền huấn luyện dựa trên transformer theo định dạng văn bản sang văn bản đang ngày càng phổ biến nhờ khung thống nhất chuyển đổi tất cả các vấn đề ngôn ngữ dựa trên văn bản thành định dạng văn bản sang văn bản, và khả năng dễ dàng mở rộng số lượng tham số (từ 60M đến 11B tham số) với song song hóa mô hình. Với transformer attention đầy đủ, T5 đã được áp dụng thành công cho nhiều tác vụ NLP, nhưng các tác vụ chỉ yêu cầu chuỗi đầu vào ngắn hơn. Điều này là do hạn chế của sự tăng trưởng tính toán bậc hai đối với độ dài chuỗi đầu vào, dẫn đến tiêu thụ bộ nhớ lớn hơn và thời gian huấn luyện dài hơn. Gần đây, Press et al. (2021) đã khám phá việc mở rộng các mô hình kiểu T5 tại thời điểm suy luận lên các chuỗi dài hơn so với những gì thấy trong quá trình huấn luyện, nhưng cách mở rộng các mô hình kiểu T5 trong độ dài chuỗi đầu vào trong quá trình huấn luyện vẫn chưa được khám phá đầy đủ.

## 3 LongT5

### 3.1 Kiến trúc

Chúng tôi mở rộng encoder T5 gốc với các mẫu thưa thớt attention toàn cục-cục bộ (Ainslie et al., 2020; Zaheer et al., 2020a) để xử lý các đầu vào dài. Đối với công việc được báo cáo trong bài báo này, chúng tôi đã sử dụng một decoder T5 tiêu chuẩn vì tất cả các tác vụ mà chúng tôi xem xét đều yêu cầu độ dài chuỗi đầu ra tương đối ngắn.

Về mặt kiến trúc, sự khác biệt chính giữa T5 và LongT5 nằm ở cơ chế attention. Chúng tôi thử nghiệm với hai biến thể cơ chế attention cho LongT5, được minh họa trong Hình 2: (1) Local Attention và (2) Transient Global Attention (TGlobal). Cả hai biến thể đều bảo toàn một số tính chất của T5: biểu diễn vị trí tương đối, hỗ trợ đóng gói ví dụ, và tương thích với các checkpoint T5.

#### 3.1.1 Local Attention

Đối với Local Attention, chúng tôi đơn giản thay thế phép toán self-attention encoder trong T5 bằng một phép toán attention cục bộ cửa sổ trượt thưa thớt theo triển khai trong ETC (Ainslie et al., 2020). Cụ thể, đối với một bán kính cục bộ r cho trước, công thức này chỉ cho phép mỗi token chú ý đến r token ở bên trái và bên phải của nó (xem Hình 2.a). Chúng tôi thấy r = 127 là đủ trong thực tế, trong đó r là số token lân cận ở bên trái và bên phải.

Local Attention không giới thiệu bất kỳ tham số mới nào và dễ dàng thích ứng với việc che attention cần thiết cho đóng gói ví dụ³. Đối với một lựa chọn r cho trước, độ phức tạp là tuyến tính theo độ dài chuỗi đầu vào l: O(lr).

#### 3.1.2 Transient Global Attention (TGlobal)

Để cho phép các token đầu vào tương tác với nhau trong mỗi lớp của encoder ở phạm vi dài hơn so với bán kính cục bộ của Local Attention, chúng tôi giới thiệu Transient Global Attention như một sửa đổi của attention toàn cục-cục bộ của ETC trong mẫu "khối cố định". Cụ thể, chúng tôi chia chuỗi đầu vào thành các khối k token, và cho mỗi khối chúng tôi tính toán một token toàn cục bằng cách tổng (và sau đó chuẩn hóa) các embedding của mọi token trong khối (xem Hình 2.b).

Bây giờ khi tính toán attention, chúng tôi cho phép mỗi token đầu vào không chỉ chú ý đến các token gần đó như trong Local Attention, mà còn đến mọi token toàn cục. Chúng tôi gọi các token toàn cục này là tạm thời vì trái ngược với các mẫu attention toàn cục-cục bộ giống ETC, các token này được xây dựng động (và sau đó bị loại bỏ) trong mỗi phép toán attention, loại bỏ mọi yêu cầu quyết định token đầu vào nào nên được coi là "toàn cục".

TGlobal attention chỉ giới thiệu một vài tham số mới⁴: (1) bias vị trí tương đối kiểu T5 biểu diễn khoảng cách từ khối của token đầu vào đến khối của mỗi token toàn cục mà nó đang chú ý đến, và (2) tham số chuẩn hóa lớp kiểu T5 để chuẩn hóa embedding của mỗi token toàn cục. Phần còn lại của các tham số giống hệt với T5, và chúng tôi thích ứng với đóng gói chuỗi bằng cách che attention bổ sung từ các token đầu vào đến các token toàn cục của các ví dụ khác. Chúng tôi thấy kích thước khối k = 16 là đủ trong thực tế. Lưu ý do đó, rằng TGlobal attention giới thiệu một khối l/k cặp attention key-value bổ sung để tính toán bên trên Local Attention (l token đầu vào, chú ý đến l/k token toàn cục; được biểu diễn bởi hình chữ nhật ngoài cùng bên phải trong Hình 2.b), do đó đối với độ dài chuỗi đầu vào l, độ phức tạp là O(l(r + l/k)).

³Đóng gói ví dụ đề cập đến việc đóng gói nhiều hơn một ví dụ ngắn

⁴Đối với các mô hình base, chúng tôi đã giới thiệu 10k tham số bổ sung, 25k cho large, và 50k cho xl.

### 3.2 Tiền huấn luyện Sinh Câu Nguyên tắc PEGASUS

T5 được tiền huấn luyện với mục tiêu tham nhũng span, trong đó các span của các token đầu vào liên tiếp được thay thế bằng một token mask và mô hình được huấn luyện để tái tạo các token bị che. Mặc dù nó hiệu quả, nghiên cứu gần đây về mô hình ngôn ngữ có mặt nạ (MLM) (Liu et al., 2019; Zhang et al., 2019b) cho thấy rằng việc lựa chọn cẩn thận mục tiêu dự đoán có thể dẫn đến hiệu suất tốt hơn đáng kể. Một lập luận là việc dự đoán các token thông tin hơn từ văn bản có thể buộc mô hình học ngữ nghĩa tốt hơn của văn bản. Được thúc đẩy bởi điều đó, chúng tôi khám phá việc che và sinh các câu nguyên tắc từ văn bản. Cụ thể, chúng tôi áp dụng chiến lược Gap Sentences Generation với Principle Ind-Uniq từ Zhang et al. (2019a), được sử dụng cho tiền huấn luyện tóm tắt.

Theo Zhang et al. (2019a), chúng tôi chọn top-m câu được tính điểm (Principle) dựa trên điểm ROUGE-F1 (Lin, 2004) sử dụng si = rouge(xi; D\{xi}; ∀i), trong đó i là chỉ số câu, D là tập hợp các câu trong tài liệu. Mỗi câu được tính điểm độc lập (Ind), và mỗi n-gram chỉ được đếm một lần (Uniq).

trong cùng một chuỗi đầu vào để tăng hiệu quả huấn luyện. Điều này đặc biệt hữu ích trong LongT5, vì với độ dài đầu vào lớn được sử dụng trong mô hình của chúng tôi, nếu nhiều ví dụ ngắn, hầu hết chuỗi đầu vào sẽ được dành cho padding, lãng phí tính toán đáng kể.

## 4 Thí nghiệm

### 4.1 Cấu hình

LongT5 được triển khai sử dụng JAX⁵ và thư viện Flaxformer⁶. Theo cùng thiết lập như T5.1.1⁷, chúng tôi xem xét các mô hình có 3 kích thước: base (220M), large (770M), và xl (3B), và sử dụng cùng mô hình vocab SentencePiece tiếng Anh có phân biệt chữ hoa thường được sử dụng bởi T5.1.1, chứa 32000 sentence piece. Chúng tôi sử dụng batch size 128 và Adafactor làm optimizer trong tất cả các thí nghiệm. Chúng tôi quyết định sử dụng giải mã tham lam thay vì tìm kiếm beam cho tất cả các thí nghiệm của chúng tôi ngay cả với các tập test, do đó, kết quả của chúng tôi báo cáo dưới đây có thể được cải thiện thêm bằng cách sử dụng tìm kiếm beam, nhưng chúng tôi muốn làm cho thiết lập nhất quán với thiết lập dev của chúng tôi.

#### 4.1.1 Tiền huấn luyện

Chúng tôi tiền huấn luyện các mô hình LongT5 trong 1M bước trên độ dài chuỗi đầu vào 4096 và độ dài chuỗi đầu ra 910. Chúng tôi sử dụng cùng lịch trình tốc độ học nghịch đảo căn bậc hai như T5, với tốc độ học được đặt thành 1/√max(step; warm_up_steps), trong đó warm_up_steps được đặt thành 10000. Giống như T5.1.1, chúng tôi tiền huấn luyện LongT5 chỉ trên bộ dữ liệu C4 (Raffel et al., 2019b), và chúng tôi không áp dụng dropout trong quá trình tiền huấn luyện. Như được mô tả trong phần 3.2, chúng tôi sử dụng mục tiêu Sinh Câu Nguyên tắc PEGASUS làm mục tiêu tiền huấn luyện của chúng tôi. Cấu hình tương tự như những gì được mô tả bởi Zhang et al. (2019a) cho các mô hình lớn hơn của họ, ngoại trừ tỷ lệ câu được che trong đó chúng tôi sử dụng giá trị 0.2 thay vì 0.45⁸. Trong phần 5.3, chúng tôi sẽ hiển thị nghiên cứu ablation của chúng tôi giữa Sinh Câu Nguyên tắc và Tham nhũng Span.

#### 4.1.2 Tinh chỉnh

Để tinh chỉnh, chúng tôi sử dụng tốc độ học không đổi 0.001 và tỷ lệ dropout 0.1 cho tất cả các tác vụ. Đối với các tác vụ tóm tắt, chúng tôi thử nghiệm với các giá trị 4096, 8192, và 16384 cho độ dài đầu vào và 512 cho độ dài đầu ra. Đối với các tác vụ QA, chúng tôi thử nghiệm với các giá trị bắt đầu từ 512 và mở rộng lên 36864 cho độ dài đầu vào và 128 cho độ dài đầu ra.

⁵https://github.com/google/jax
⁶https://github.com/google/flaxformer
⁷https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511
⁸Chúng tôi đã thử nghiệm ngắn gọn với các giá trị khác, nhưng thấy 0.2 hoạt động tốt nhất với các tác vụ downstream quan tâm.

### 4.2 Đánh giá trên Các tác vụ Tóm tắt

Chúng tôi chọn đánh giá các mô hình của mình trên các tác vụ tóm tắt bao gồm các độ dài ngữ cảnh khác nhau, vì bản chất hiểu ngữ cảnh dài và sinh của chúng.

#### 4.2.1 Bộ dữ liệu

LongT5 được đánh giá trên sáu bộ dữ liệu sau.

**CNN / Daily Mail** (Nallapati et al., 2016) Tin tức từ CNN và Daily Mail được sử dụng làm đầu vào và các dấu chấm tóm tắt của bài báo là bản tóm tắt mục tiêu.

**PubMed** (Cohan et al., 2018) Các tài liệu khoa học được thu thập từ PubMed, với nội dung tài liệu được sử dụng làm đầu vào và tóm tắt tương ứng của nó làm bản tóm tắt mục tiêu.

**arXiv** (Cohan et al., 2018) Tương tự như PubMed, nhưng với các tài liệu được lấy từ arXiv.

**BigPatent** (Sharma et al., 2019) Các tài liệu bằng sáng chế Hoa Kỳ, với chi tiết bằng sáng chế được sử dụng làm đầu vào và tóm tắt bằng sáng chế làm bản tóm tắt mục tiêu.

**MediaSum** (Zhu et al., 2021) Bản ghi phỏng vấn từ CNN và NPR được sử dụng làm đầu vào và chủ đề và tổng quan tương ứng của chúng được sử dụng làm bản tóm tắt mục tiêu.

**Multi-News** (Fabbri et al., 2019) Tác vụ bao gồm việc tóm tắt nhiều tài liệu tin tức về một chủ đề thành một bản tóm tắt được viết bởi con người.

Bảng 1 cung cấp thống kê cho số lượng ví dụ trong các phần train, validation, và test, và độ dài chuỗi đầu vào trung bình, trung vị, tối đa, và phần trăm thứ 90. Như có thể thấy, các bộ dữ liệu này có độ dài đầu vào dài, và sẽ hưởng lợi từ các mô hình có thể mô hình hóa các đầu vào dài hơn. Chúng tôi bao gồm bộ dữ liệu CNN / Daily Mail để đánh giá trên một tác vụ phổ biến, đặc biệt để xem việc sử dụng attention TGlobal tác động đến mô hình như thế nào, mặc dù độ dài của các đầu vào nhỏ hơn so với các bộ dữ liệu khác.

#### 4.2.2 Kết quả

Chúng tôi so sánh LongT5 với các phương pháp hàng đầu khác nhau: BigBird-PEGASUS (Zaheer et al., 2020b), HAT-BART (Rohde et al., 2021), DANCER PEGASUS (Gidiotis and Tsoumakas, 2020), PRIMER (Xiao et al., 2021), TG-MultiSum (Cui and Hu, 2021), LED (Beltagy et al., 2020), và một ứng dụng của BART bởi Zhu et al. (2021). Để so sánh này, chúng tôi sử dụng các chỉ số đánh giá phổ biến là ROUGE-1, ROUGE-2, và ROUGE-L.

Như có thể thấy trong Bảng 2, LongT5 có thể đạt được điểm rouge tiên tiến cho arXiv, PubMed, BigPatent, và MediaSum. Đối với arXiv và PubMed, được cấu thành từ các đầu vào dài hơn, khả năng mở rộng lên độ dài đầu vào 16k giúp LongT5 đạt được kết quả mạnh.

Một bộ dữ liệu mà LongT5 không thể đạt được kết quả tiên tiến là với Multi-News. LongT5 là mô hình tốt thứ 2, hơi tệ hơn PRIMER. Điều này có thể hiểu được vì mô hình PRIMER được tiền huấn luyện trên một corpus lớn các tài liệu liên quan đến các sự kiện tin tức, do đó phơi bày mô hình với một corpus tương tự như những gì thấy trong Multi-News.

Khi nhìn vào CNN / Daily Mail, chúng ta có thể thấy rằng LongT5 có thể so sánh với HAT-BART, mặc dù không có attention đầy đủ. LongT5 ít nhất đã có điểm số mạnh hơn trong chỉ số ROUGE-2.

### 4.3 Đánh giá trên Các tác vụ QA

Để đánh giá trên các tác vụ QA, chúng tôi chọn hai benchmark phổ biến, Natural Questions và TriviaQA, yêu cầu hiểu ngữ cảnh dài.

#### 4.3.1 Bộ dữ liệu

**NaturalQuestions (NQ)** Các câu hỏi là các truy vấn thực tế được phát hành bởi nhiều người dùng đến tìm kiếm Google truy xuất một trang Wikipedia trong năm kết quả tìm kiếm hàng đầu. Văn bản trả lời được rút ra từ kết quả tìm kiếm (Kwiatkowski et al., 2019).

Bộ dữ liệu NQ gốc yêu cầu các mô hình dự đoán một câu trả lời ngắn (bao gồm không trả lời hoặc có/không) và một câu trả lời dài. Chúng tôi đã đóng khung tác vụ như một tác vụ seq2seq và bỏ qua câu trả lời dài. Do đó, kết quả của chúng tôi chỉ tập trung vào câu trả lời ngắn. Hơn nữa, vì các mô hình của chúng tôi dự đoán văn bản trả lời thay vì span trả lời, phương pháp đánh giá của chúng tôi khác một chút so với các bảng xếp hạng, và kết quả của chúng tôi không thể so sánh trực tiếp với các phương pháp hiện có khác: (1) Vì chỉ có các tập train và dev được công khai, chúng tôi sử dụng 90% của tập train chính thức để huấn luyện trong khi sử dụng 10% làm tập dev giữ lại để tinh chỉnh các siêu tham số và epoch huấn luyện, và sử dụng

**TriviaQA** Các người đam mê trivia đã tác giả các cặp câu hỏi-trả lời. Các câu trả lời được rút ra từ Wikipedia và kết quả tìm kiếm web Bing, loại trừ các trang web trivia (Joshi et al., 2017).

Chúng tôi sử dụng các phần train/validation chính thức để huấn luyện và tinh chỉnh các siêu tham số và epoch huấn luyện, sau đó huấn luyện lại mô hình đó kết hợp cả tập train và validation để đánh giá trên domain Wikipedia trên bảng xếp hạng⁹.

Bảng 3 cho thấy thống kê bộ dữ liệu cho số lượng ví dụ trong các phần train và validation, và độ dài chuỗi đầu vào trung bình, trung vị, tối đa, và phần trăm thứ 90.

#### 4.3.2 Kết quả

Bảng 4 cho thấy tóm tắt kết quả cho các bộ dữ liệu NQ và TriviaQA (xem Phụ lục B để có kết quả đầy đủ). Đối với mỗi bộ dữ liệu, chúng tôi hiển thị hai chỉ số: EM (Exact Match) và điểm F1 (đánh giá độ chính xác và recall của các từ riêng lẻ trong câu trả lời so với truth ground, bỏ qua stop word).

Đối với NQ, chúng tôi so sánh T5.1.1, LongT5 với Local Attention, và LongT5 với TGlobal attention. Chúng tôi quyết định chạy T5.1.1 (1) với độ dài chuỗi đầu vào mặc định 512¹⁰ và (2) với độ dài chuỗi đầu vào lớn nhất có thể vừa với bộ nhớ thiết bị¹¹, và sử dụng những cái đó làm baseline. Vì chúng tôi đang so sánh với T5.1.1, đối với các thí nghiệm LongT5 chúng tôi báo cáo kết quả ở độ dài đầu vào 512 cho base và large, và độ dài đầu vào lớn nhất được phép bởi mỗi mô hình trước khi hết bộ nhớ trên cùng cấu hình phần cứng được sử dụng trong các thí nghiệm T5.1.1 của chúng tôi.

Như bảng cho thấy, việc tăng độ dài đầu vào thường dẫn đến lợi ích đáng kể trong NQ, với các mô hình có độ dài đầu vào lớn hơn vượt trội đáng kể so với những mô hình có độ dài đầu vào nhỏ hơn trong hầu hết các trường hợp. Một số lần, các mô hình với độ dài đầu vào lớn nhất

⁹https://competitions.codalab.org/competitions/17208
¹⁰Đối với các mô hình base và large.
¹¹Đối với các mô hình base và large, chúng tôi đã sử dụng 4x8 TPUv3 và không có phân vùng mô hình; đối với mô hình xl, chúng tôi đã sử dụng 8x16 TPUv3 và 8 phân vùng.

kém hiệu suất hơn so với những mô hình có độ dài 4k, nhưng chúng tôi tin rằng những điều đó là do nhiễu trong các thí nghiệm, vì kết quả là đầu ra của chỉ một lần lặp lại của mỗi thí nghiệm do hạn chế tài nguyên. Hơn nữa, trong khi LongT5 với Local Attention thường kém hiệu suất hơn T5.1.1, LongT5 với TGlobal attention vượt trội đáng kể so với T5.1.1. Ví dụ, xem xét các mô hình kích thước large, T5.1.1 chỉ có thể mở rộng lên độ dài đầu vào 3k token, trong khi mô hình TGlobal có thể đạt 6k token, vượt trội so với T5.1.1 ở độ dài token 4k (có một sự giảm ở độ dài token 6k, nhưng chúng tôi giả thuyết điều này chỉ là do phương sai, vì chúng tôi chỉ làm một lần chạy cho mỗi cấu hình).

Đối với TriviaQA, chúng tôi so sánh LongT5 với các phương pháp hàng đầu khác nhau trên bảng xếp hạng: BigBird-ETC (Zaheer et al., 2020a), Fusion-in-Decoder (Izacard and Grave, 2021), và ReadTwice (Zemlyanskiy et al., 2021). Như được hiển thị trong Bảng 3, các đầu vào TriviaQA khá dài, do đó khả năng mở rộng cả kích thước mô hình và lên độ dài đầu vào 16k giúp LongT5 đạt được tiên tiến.

## 5 Phân tích

### 5.1 Độ dài Đầu vào vs Tốc độ

Để đánh giá tốc độ huấn luyện và tiêu thụ bộ nhớ của LongT5, so với T5.1.1, chúng tôi đã thực hiện một loạt các lần chạy huấn luyện trong bộ dữ liệu NQ bắt đầu từ độ dài đầu vào 512, và tăng độ dài đầu vào đều đặn cho đến khi các mô hình hết bộ nhớ trên một slice 4x8 TPUv3. Kết quả được hiển thị trong Hình 3, so sánh 6 cấu hình mô hình khác nhau: T5.1.1 base, T5.1.1 large, LongT5 (base Local), LongT5 (large Local), LongT5 (base TGlobal), và LongT5 (large TGlobal). Đối với mỗi cấu hình mô hình, chúng tôi hiển thị một đường cong vẽ số lượng chuỗi mỗi giây được xử lý trong quá trình huấn luyện (tốc độ, trên trục dọc) cho mỗi độ dài đầu vào (trục ngang). Cả hai trục đều được hiển thị trong thang logarit.

Chúng ta có thể thấy rằng ở độ dài ngắn hơn (512), T5.1.1, LongT5 Local, LongT5 TGlobal có tốc độ tương tự, nhưng khi chúng ta tăng độ dài chuỗi, LongT5 trở nên nhanh hơn đáng kể. Ví dụ ở độ dài chuỗi 2048, T5.1.1 base chỉ có thể xử lý 479 chuỗi mỗi giây, trong khi LongT5 (base TGlobal) có thể xử lý 765 và LongT5 (base Local) có thể xử lý 860. Sự khác biệt thậm chí còn lớn hơn khi độ dài chuỗi tăng.

Một sự thật quan trọng khác mà Hình 3 cho thấy là các mô hình T5.1.1 đạt đến điểm hết bộ nhớ sớm hơn nhiều. Ví dụ, chúng tôi chỉ có thể mở rộng lên 6k token cho T5.1.1 base. Mặt khác, LongT5 (base Local) có thể lên đến 36k token về độ dài, và LongT5 (base TGlobal) lên đến 12k. Các mô hình large cho thấy một bức tranh tương tự với T5.1.1 large chỉ lên đến 3k, nhưng các biến thể LongT5 lên đến 10k (large Local) và 6k (large TGlobal).

### 5.2 Độ dài Đầu vào vs Hiệu suất

Phần này trình bày một phân tích tương tự, nhưng trong đó chúng tôi vẽ tốc độ mô hình so với hiệu suất trong NQ (điểm F1). Kết quả được hiển thị trong Hình 4 cho các mô hình có kích thước large. Mỗi điểm trong các đường cong được chú thích với độ dài chuỗi tương ứng.

Như Hình 4 cho thấy, hiệu suất tăng đáng kể khi độ dài đầu vào tăng, làm nổi bật lợi ích của LongT5. Hơn nữa, chỉ riêng độ dài đầu vào không đủ để đạt được hiệu suất tốt trong tất cả các bộ dữ liệu, và đặc biệt, trong bộ dữ liệu NQ (được sử dụng trong hình này), việc sử dụng Local Attention làm tổn hại đáng kể hiệu suất khi so sánh với TGlobal hoặc với T5.1.1. Vì vậy, ngay cả ở độ dài đầu vào rất dài, LongT5 với Local Attention chỉ khớp với T5.1.1 với độ dài đầu vào 3k trong NQ. Tuy nhiên, LongT5 với TGlobal attention vượt trội so với T5.1.1. Hơn nữa, lưu ý rằng mặc dù biểu đồ cho thấy một vài bất thường (như độ dài 8k cho LongT5 với Local Attention, hoặc độ dài 6k với TGlobal Attention), đó là vì biểu đồ chỉ hiển thị kết quả của một lần chạy duy nhất, và do đó có một số nhiễu. Tuy nhiên, xu hướng có thể được thấy rõ ràng.

### 5.3 Sinh Câu Nguyên tắc vs. Tham nhũng Span

Như đã đề cập trong phần 3.2, chúng tôi sử dụng Sinh Câu Nguyên tắc PEGASUS thay vì Tham nhũng Span mặc định được sử dụng trong T5 làm mục tiêu tiền huấn luyện của chúng tôi. Bảng 5 cho thấy nghiên cứu ablation của chúng tôi để tinh chỉnh trên NQ và arXiv từ một mô hình được tiền huấn luyện sử dụng mục tiêu Tham nhũng Span mặc định, một mô hình được tiền huấn luyện với Sinh Câu Nguyên tắc, và một mô hình được tiền huấn luyện với cả hai mục tiêu. So sánh được thực hiện trên tập dev của các tác vụ, và với các mô hình TGlobal base. Cả tiền huấn luyện và tinh chỉnh trên các mô hình được đề cập ở trên đều được thực hiện với độ dài chuỗi đầu vào 4096. Bảng cho thấy, mặc dù Sinh Câu Nguyên tắc được phát triển bởi Zhang et al. (2019a) như một chiến lược tiền huấn luyện cho tóm tắt, nó có lợi cho cả tác vụ tóm tắt và QA, nhưng việc sử dụng cả hai mục tiêu cùng nhau hoạt động tệ hơn so với chỉ sử dụng PSG.

Bảng 6 cho thấy một nghiên cứu ablation bổ sung với arXiv và PubMed, trong đó chúng tôi so sánh việc sử dụng T5.1.1 thông thường với Tham nhũng Span so với T5.1.1 được tiền huấn luyện với Sinh Câu Nguyên tắc trong khi sử dụng cùng độ dài chuỗi đầu vào tiền huấn luyện 512 (như đã được thực hiện trong tác vụ tiền huấn luyện T5.1.1 gốc). Như mong đợi, Sinh Câu Nguyên tắc đã giúp mô hình đạt được kết quả tốt hơn so với Tham nhũng Span khi thấy cùng một lượng dữ liệu tiền huấn luyện. Chúng tôi cũng so sánh điều này với điểm dev từ LongT5 với TGlobal attention ở độ dài đầu vào 4k và 16k, sao cho chúng ta có thể thấy có attention đầy đủ sẽ cho phép kết quả tốt hơn, nhưng khả năng mở rộng lên độ dài chuỗi đầu vào dài hơn cho phép LongT5 đạt được kết quả mạnh hơn của nó.

## 6 Công trình Liên quan

Tiền huấn luyện mô hình ngôn ngữ theo sau bằng tinh chỉnh cụ thể cho tác vụ đã được chứng minh là một công cụ mạnh mẽ cho nhiều tác vụ NLP (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020). BERT (Devlin et al., 2019) đã giới thiệu Mask Language Model (MLM), trong đó một mô hình dự đoán các token bị che cho một chuỗi đầu vào văn bản. Tinh chỉnh một mô hình BERT đã được tiền huấn luyện đã dẫn đến cải thiện hiệu suất trên các tác vụ NLP khác nhau. Tuy nhiên, các dự đoán MLM không được thực hiện theo cách tự hồi quy, điều này giới hạn khả năng của họ BERT cho các tác vụ sinh. Raffel et al. (2019a) đã giới thiệu tác vụ tham nhũng span trong T5 làm mục tiêu tiền huấn luyện, trong đó một mô hình dự đoán span token bị che sử dụng một mô hình tự hồi quy. Nó có thể xử lý các tác vụ sinh vì tiền huấn luyện được thực hiện theo cách sinh. BART (Lewis et al., 2020) tương tự như T5 nhưng sử dụng một mục tiêu tiền huấn luyện hơi khác, trong đó các span bị che từ đầu vào nhưng đầu ra hoàn chỉnh được dự đoán. Tuy nhiên, không có công trình nào trong số này cố gắng điều tra tiền huấn luyện cho đầu vào chuỗi rất dài. Họ thường sử dụng một kiến trúc transformer (Vaswani et al., 2017) làm backbone, độ phức tạp của nó là bậc hai đối với độ dài đầu vào, làm cho chúng không thực tế để mô hình hóa đầu vào chuỗi rất dài.

**Mô hình hóa văn bản dài** Một lượng công việc rộng lớn cũng đã được thực hiện để mô hình hóa văn bản dài như tài liệu. Công việc từ Roy et al. (2016); Chen (2017); Wu et al. (2018) đã có được các embedding tài liệu từ các embedding cấp từ. Một hướng nghiên cứu khác cố gắng mô hình hóa các tài liệu dài thông qua huấn luyện phân cấp. Công việc từ Yang et al. (2016); Miculicich et al. (2018) đã sử dụng Hierarchical Attention Networks cho phân loại tài liệu và dịch máy thần kinh, và Guo et al. (2019) đã đề xuất sử dụng một mạng phân cấp để xây dựng các embedding tài liệu trên các embedding câu cho khai thác tài liệu song song.

Nghiên cứu gần đây hơn đã tập trung vào việc cải thiện hiệu quả bộ nhớ và tính toán của các mô hình transformer (Tay et al., 2020b, 2021) để xử lý đầu vào dài. Một loại phương pháp như vậy là sử dụng các mẫu attention không đầy đủ để hạn chế phạm vi trường attention, để nó giảm độ phức tạp attention từ O(n²) xuống O(n log n) hoặc O(n), bao gồm Sinkhorn (Tay et al., 2020a), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), và BigBird (Zaheer et al., 2020a). Một loại phương pháp khác là tận dụng xấp xí rank thấp của ma trận attention, như Linformer (Wang et al., 2020), Performer (Choromanski et al., 2021), Random Feature Attention (Peng et al., 2021), và LUNA (Ma et al., 2021).

## 7 Kết luận

Bài báo này trình bày một mô hình thần kinh dựa trên Transformer mới được gọi là LongT5, với đó chúng tôi đã khám phá tác động của việc mở rộng đồng thời cả độ dài đầu vào và kích thước mô hình. Cụ thể, các điểm khác biệt chính của LongT5 so với T5.1.1 là (1) một cơ chế attention có thể mở rộng mới được gọi là Transient Global attention, là một thay thế trực tiếp cho cơ chế attention T5 tiêu chuẩn, và do đó có thể được sử dụng mà không cần các đầu vào phụ bổ sung cho mô hình hoặc sửa đổi đầu vào mô hình; và (2) sử dụng mục tiêu tiền huấn luyện Sinh Câu Nguyên tắc kiểu PEGASUS.

Thông qua thử nghiệm trong một số bộ dữ liệu tóm tắt và trả lời câu hỏi thách thức, chúng tôi đã khám phá các cải thiện hiệu suất có thể đạt được bằng cách mở rộng cả độ dài đầu vào và kích thước mô hình, dẫn đến kết quả tiên tiến trên một số bộ dữ liệu: arXiv, PubMed, BigPatent, MediaSum, và TriviaQA.

Như một phần của công việc tương lai của chúng tôi, chúng tôi muốn theo đuổi một số hướng như nghiên cứu các cơ chế attention hiệu quả trong decoder và các phần attention decoder-to-encoder của mô hình (cả Local Attention và TGlobal attention chỉ được áp dụng cho encoder trong LongT5 hiện tại). Ngoài ra, chúng tôi muốn kết hợp các ý tưởng transformer đầu vào dài bổ sung vào kiến trúc LongT5, có thể cải thiện thêm hiệu quả mô hình.

## Tài liệu tham khảo

[Tài liệu tham khảo được giữ nguyên như trong bản gốc]

## A Kết quả Tóm tắt

Bảng 8 cho thấy tập hợp đầy đủ các kết quả trên các bộ dữ liệu tóm tắt được sử dụng trong bài báo này. Điều này bao gồm cả mô hình T5 tiêu chuẩn (sử dụng phiên bản T5.1.1), T5 với tiền huấn luyện Sinh Câu Nguyên tắc PEGASUS, và mô hình LongT5.

Như có thể thấy, việc mở rộng kích thước đầu vào cho các mô hình giúp đạt được các chỉ số hiệu suất tốt hơn. Tuy nhiên, các mô hình T5 gặp khó khăn khi mở rộng lên 4k cho đầu vào, vì tác vụ tinh chỉnh có thể mất nhiều ngày ngay cả khi sử dụng topology lớn của TPUv3.

Khi so sánh mô hình T5.1.1 thông thường với mô hình T5.1.1 sử dụng tiền huấn luyện Sinh Câu Nguyên tắc PEGASUS, mô hình sau có thể đạt được kết quả tốt hơn, với kết quả cũng cải thiện khi kích thước đầu vào được mở rộng. Điều này giúp cho thấy rằng cả việc sử dụng mục tiêu tiền huấn luyện sau cùng với việc mở rộng cho phép chúng ta có được kết quả tốt nhất từ các mô hình này.

LongT5, mặc dù có attention giảm từ việc sử dụng TGlobal attention, có thể có được kết quả hiệu suất mạnh do cả việc mở rộng lên các đầu vào lớn hơn và tận dụng chiến lược tiền huấn luyện Gap Sentences Generation.

## B Kết quả QA

Bảng 7 cho thấy tập hợp đầy đủ các kết quả so sánh các mô hình T5.1.1 và LongT5 trên các bộ dữ liệu QA được sử dụng trong bài báo này. Đối với cả NQ và TriviaQA trong nghiên cứu so sánh này, chúng tôi sử dụng 90% của tập huấn luyện chính thức để huấn luyện trong khi sử dụng 10% làm tập dev giữ lại để tinh chỉnh các siêu tham số và epoch huấn luyện, và sử dụng tập dev chính thức để báo cáo các số trong bảng này. Chúng tôi chạy mỗi mô hình đến độ dài đầu vào lớn nhất được phép trước khi hết bộ nhớ trên cấu hình phần cứng cụ thể - các mô hình base/large trên 4x8 TPUv3 không có phân vùng mô hình, và các mô hình xl trên 8x16 TPUv3 với 8 phân vùng.

[Các bảng thống kê và kết quả được giữ nguyên như trong bản gốc]
