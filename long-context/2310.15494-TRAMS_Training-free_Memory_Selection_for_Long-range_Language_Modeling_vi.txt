# 2310.15494.pdf
# Đã được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2310.15494.pdf
# Kích thước tệp: 466541 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
TRAMS: Lựa chọn Bộ nhớ Không cần Huấn luyện cho Mô hình hóa Ngôn ngữ Tầm xa
Haofei Yu♡∗, Cunxiang Wang♣†, Yue Zhang♣, Wei Bi♢‡
♡Viện Công nghệ Ngôn ngữ, Đại học Carnegie Mellon, Hoa Kỳ
♣Trường Kỹ thuật, Đại học Westlake, Trung Quốc♢Phòng thí nghiệm AI Tencent, Trung Quốc
haofeiy@cs.cmu.edu ,{wangcunxiang, zhangyue}@westlake.edu.cn ,
victoriabi@tencent.com
Tóm tắt
Kiến trúc Transformer là yếu tố quan trọng cho nhiều mô hình AI, nhưng nó vẫn đối mặt với thách thức trong mô hình hóa ngôn ngữ tầm xa. Mặc dù một số kiến trúc transformer cụ thể đã được thiết kế để giải quyết các vấn đề về phụ thuộc tầm xa, các phương pháp hiện có như Transformer-XL bị ảnh hưởng bởi tỷ lệ cao các bộ nhớ không hiệu quả. Trong nghiên cứu này, chúng tôi trình bày một chiến lược cắm và chạy, được gọi là Lựa chọn Bộ nhớ Không cần HUẤn luyện (TRAMS), lựa chọn các token tham gia vào tính toán attention dựa trên một chỉ số đơn giản. Chiến lược này cho phép chúng tôi giữ lại các token có khả năng có điểm attention cao với các truy vấn hiện tại và bỏ qua những token khác. Chúng tôi đã thử nghiệm phương pháp của chúng tôi trên benchmark cấp từ (WikiText-103) và benchmark cấp ký tự (enwik8), và kết quả cho thấy có cải thiện mà không cần huấn luyện bổ sung hoặc thêm tham số bổ sung.
1 Giới thiệu
Các mô hình dựa trên Transformer (Kenton và Toutanova, 2019; Liu et al., 2019; Raffel et al., 2020; Lan et al., 2019; Brown et al., 2020) đã đạt được hiệu suất đáng chú ý trong vài năm qua. Thành phần chính của các kiến trúc mô hình này là cơ chế attention (Vaswani et al., 2017). Tuy nhiên, thiết kế attention gốc gặp khó khăn trong việc xử lý hiệu quả các chuỗi dài, điều này trở nên đặc biệt có vấn đề trong các tình huống như dịch thuật cấp tài liệu (Werlen et al., 2018; Kim et al., 2019) và sinh văn bản quy mô lớn (Zhou et al., 2023), vì chi phí tính toán về thời gian và không gian tăng theo bậc hai với độ dài chuỗi (Tay et al., 2022). Yếu tố chính cho độ phức tạp tính toán cao này có thể được truy nguyên về phép nhân giữa các truy vấn và khóa được sử dụng trong mô-đun attention. Nói chung, độ phức tạp thời gian cho tính toán là O(N²d) nếu một mô hình transformer với d chiều được thiết lập với đầu vào bao gồm N token.

Để giải quyết nút thắt tính toán này, nhiều nỗ lực đã được thực hiện. Hướng nghiên cứu đầu tiên là tìm một biểu thức hiệu quả mới để tính điểm attention. Mặc dù có những tiến bộ được thực hiện, các phương pháp này thường ảnh hưởng đến hiệu suất, do đó mở đường cho các giải pháp thay thế. Các kiến trúc hiệu quả cung cấp biểu thức gần đúng của attention đã được khám phá rộng rãi (Wang et al., 2020; Peng et al., 2022b,a; Choromanski et al., 2021; Zheng et al., 2022b,a). Hướng nghiên cứu thứ hai là giữ nguyên biểu thức tính toán và sử dụng cấu trúc bên ngoài như hàm hash (Kitaev et al., 2019; Daras et al., 2020), phân cụm (Roy et al., 2021; Vyas et al., 2020) và bộ chọn bộ nhớ (Pietruszka et al., 2022; Dai et al., 2019; Bertsch et al., 2023; Sukhbaatar et al., 2021, 2019; Child et al., 2019) để tìm tập con phù hợp của các truy vấn và khóa trong chuỗi dài cho tính toán attention.arXiv:2310.15494v3  [cs.CL]  20 Dec 2023

--- TRANG 2 ---
Công trình của chúng tôi thuộc về danh mục thứ hai, trong đó chúng tôi đề xuất một cơ chế lựa chọn bộ nhớ không cần huấn luyện để chọn các token phù hợp cho tính toán attention. Cụ thể, chúng tôi tập trung vào đẩy kiến trúc Transformer-XL (Dai et al., 2019) đến một vị trí tốt hơn bằng cách chọn các token chất lượng cao hơn trong bộ nhớ của nó. Dựa trên điều tra ban đầu của chúng tôi, chúng tôi xây dựng một tập con bộ nhớ bằng cách chọn 50% bộ nhớ có giá trị attention lớn nhất và duy trì cùng hiệu suất. Điều này cho thấy rằng một phần lớn thông tin trong bộ nhớ không được sử dụng hết. Điều này thúc đẩy chúng tôi khám phá các phương pháp tốt hơn để tối ưu hóa việc sử dụng bộ nhớ.

Như minh họa trong Hình 1, chúng tôi đề xuất một phương pháp Lựa chọn Bộ nhớ Không cần HUẤn luyện (TRAMS) có thể được cắm trực tiếp vào các mô hình ngôn ngữ tầm xa dựa trên bộ nhớ và giảm độ phức tạp thời gian của việc tính toán ma trận attention. Thông qua các thí nghiệm trên hai bộ dữ liệu benchmark mô hình hóa ngôn ngữ, cụ thể là WikiText-103 cấp từ (Merity et al., 2016) và enwik8 cấp ký tự (Mahoney, 2011), chúng tôi đạt được cải thiện hiệu suất của mô hình, được thể hiện bằng việc giảm 0.19 perplexity (ppl) trong WikiText-103 và giảm 0.017 bits-per-character (bpc) trong enwik8.

Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên thiết kế một phương pháp lựa chọn bộ nhớ không cần huấn luyện dựa trên kiến trúc Transformer-XL.¹

2 Phương pháp
2.1 Định nghĩa Vấn đề
Chúng tôi sử dụng h∈R^(N×d) để biểu diễn các trạng thái ẩn đầu vào cho mô-đun attention, o∈R^(N×d) để biểu diễn các trạng thái ẩn đầu ra cho mô-đun attention, m∈R^(M×d) để biểu diễn các trạng thái ẩn bộ nhớ được sử dụng trong tính toán attention. Chúng tôi sử dụng W_Q, W_K, W_V để biểu diễn ma trận chiếu có thể huấn luyện trong mô-đun attention. Chúng tôi định nghĩa d cho chiều của mô hình, M cho kích thước bộ nhớ, và N cho kích thước đầu vào. Quá trình tính toán attention có thể được viết chính thức là o = Attn(h,m).

Với các ký hiệu trên, vấn đề lựa chọn bộ nhớ có thể được định nghĩa là chọn một tập con các trạng thái ẩn bộ nhớ m̃ từ bộ nhớ m mang lại sự khác biệt nhỏ nhất đối với đầu ra lớp transformer nhưng với kích thước bộ nhớ nhỏ hơn.

m̃* = arg min_(m̃⊂m) ‖Attn(h,m̃) - Attn(h,m)‖     (1)

¹Mã nguồn cho bài báo này có sẵn tại
https://github.com/lwaekfjlk/TRAMS.

2.2 Tái công thức hóa Attention
Attention Chuẩn Trong một mô hình ngôn ngữ tăng cường bộ nhớ, cơ chế attention chuẩn (Vaswani et al., 2017) giữa các trạng thái ẩn đầu vào và các trạng thái ẩn bộ nhớ có thể được viết là:

Attn(h,m) = softmax(QK^T/√d)V     (2)

trong đó Q = hW_Q là tích của các trạng thái ẩn token đích h và ma trận chiếu truy vấn W_Q; K = mW_K là tích của các trạng thái ẩn token bộ nhớ m và ma trận chiếu khóa W_K; V = mW_V cũng là tích của các trạng thái ẩn token bộ nhớ m và ma trận chiếu giá trị W_V.

Attention Unlimiformer Khác với cách tính điểm attention nổi tiếng, Unlimiformer (Bertsch et al., 2023) đã đề xuất một cách viết lại để tính phần tích vô hướng của cross-attention trong kiến trúc encoder-decoder:

QK^T = (h_d W_Q)(h_e W_K)^T
     = (h_d W_Q W_K^T)h_e^T     (3)

trong đó h_e là trạng thái ẩn encoder và h_d là trạng thái ẩn decoder. Điều này cho phép Unlimiformer tránh việc lập chỉ mục các khóa cho mỗi head và layer riêng biệt và tránh lưu trữ các giá trị trong một chỉ mục riêng biệt khỏi các khóa trong giai đoạn tìm kiếm và truy xuất dựa trên kNN, làm cho nó hiệu quả hơn.

Attention TRAMS Mặc dù chúng tôi không cần lưu trữ hoặc lập chỉ mục bất kỳ khóa hoặc giá trị nào cho phương pháp của chúng tôi, attention Unlimiformer thúc đẩy chúng tôi chuyển thêm thông tin hữu ích cho các khóa bằng cách tái công thức hóa attention và cho phép chúng tôi thực hiện lựa chọn bộ nhớ hiệu quả hơn chỉ dựa trên các khóa được tái công thức hóa.

Chúng tôi có thể tính toán công thức attention này theo thứ tự khác nhưng duy trì cùng kết quả:

QK^T = (hW_Q)(mW_K)^T
     = (h)(mW_K W_Q^T)^T     (4)

Do đó, chúng tôi định nghĩa Q' = h là truy vấn được tái công thức hóa cho biểu thức attention này và K' = mW_K W_Q^T là các khóa được tái công thức hóa cho attention. Với việc tái công thức hóa này, chúng tôi chuyển tất cả thông tin tham số liên quan đến attention lên các vectơ khóa được tái công thức hóa.

2.3 Không gian Ẩn Transformer
Vì h là đầu vào của lớp transformer hiện tại và cũng là đầu ra của lớp transformer trước đó, nó là kết quả của phép toán Layernorm

--- TRANG 3 ---
của lớp cuối cùng. Chúng tôi có thể định nghĩa trung bình theo tọa độ của h là μ và độ lệch chuẩn theo tọa độ của h là σ. Các biểu thức có thể được viết là:

μ = (1/d)∑(i=1 to d)h_i ≈ 0, σ = √((1/d)∑(i=1 to d)(h_i - μ)²) ≈ 1     (5)

Vì giá trị trung bình cho các trạng thái ẩn h xung quanh số không, chúng tôi có thể xác nhận các vectơ trạng thái ẩn gần như trực giao với vectơ ⃗1 và chuẩn L2 của các trạng thái ẩn xung quanh √d.

Với xấp xỉ này, chúng tôi có thể mở rộng điểm attention được tái công thức hóa của chúng tôi như:

Q'K'ᵀ = (h)(mW_K W_Q^T)^T
      = ‖Q'‖ · ‖K'‖ · cos⟨Q', K'⟩
      ≈ √d · ‖K'‖ · cos⟨Q', K'⟩     (6)

trong đó ‖Q'‖ biểu thị chuẩn L2 cho Q' và ‖K'‖ biểu thị chuẩn L2 cho K'. Dựa trên Hình 2, chúng tôi thấy rằng chuẩn truy vấn được tái công thức hóa ‖Q'‖ có phân phối sắc nét hơn nhiều so với chuẩn khóa ‖K'‖, cho thấy chuẩn truy vấn được tái công thức hóa có thể được xấp xỉ bởi một hệ số hằng số.

2.4 Lựa chọn Bộ nhớ Không cần Huấn luyện (TRAMS)
Mục tiêu của chúng tôi cho lựa chọn bộ nhớ là khôi phục điểm attention hoàn chỉnh với ít token bộ nhớ nhất có thể. Vấn đề này tương đương với việc tìm tập con của các token bộ nhớ có điểm attention cao nhất với các truy vấn. Chúng tôi đề xuất một phương pháp heuristic để thực hiện lựa chọn cấp token cho mỗi lớp và mỗi head dựa trên một chỉ số độc lập với bộ nhớ trong phần này.

Có hai thành phần quan trọng để tính điểm attention sau khi xấp xỉ ‖Q'‖ với một hệ số hằng số: chuẩn của các khóa được tái công thức hóa ‖K'‖ và các góc giữa các khóa và truy vấn được tái công thức hóa arccos⟨Q', K'⟩, điều này được chứng minh trong Khandelwal et al. (2019). Thông thường, chúng tôi tin rằng arccos⟨Q', K'⟩ là yếu tố quan trọng hơn nói chung. Tuy nhiên, nếu chúng tôi sử dụng xếp hạng của giá trị điểm attention cho tất cả các cặp truy vấn và khóa làm xếp hạng ground-truth, dựa trên Hình 3, chúng tôi phát hiện thực nghiệm rằng các xếp hạng dựa trên chuẩn khóa và các xếp hạng dựa trên góc tạo ra điểm tương quan Spearman gần nhau khi chỉ tính đến 1% điểm attention cao nhất. Do đó, điều này cho thấy rằng chúng tôi có thể xếp hạng các token bộ nhớ của chúng tôi dựa trên ‖K'‖ một mình để đạt được hiệu suất tương đối tốt khi chúng tôi mong muốn 1% điểm attention hàng đầu với các truy vấn trong bộ nhớ của chúng tôi thay vì tất cả.

Ngoài ra, chúng tôi phát hiện rằng việc chỉ dựa vào một chuẩn lớn không đủ như một ràng buộc. Cụ thể, các khóa gần với ⃗1 hơn có xu hướng tạo ra điểm attention cao hơn. Để giải quyết điều này, chúng tôi giới thiệu một chỉ số kết hợp: s = cos⟨K', ⃗1⟩‖K'‖. Chỉ số này cho phép chúng tôi xác định các token có thể tạo ra điểm attention cao khi được ghép nối với truy vấn thích hợp (do giá trị cao của ‖K'‖) và điểm thấp khi được ghép nối với truy vấn không phù hợp (do mức độ trực giao cao với không gian truy vấn dựa trên cos⟨K', ⃗1⟩). Điều này là do tính gần như trực giao với không gian truy vấn, như được chỉ ra bởi góc nhỏ với ⃗1, vốn trực giao với không gian truy vấn.

3 Thí nghiệm
Chúng tôi giới thiệu các phương pháp so sánh và báo cáo các kết quả chính và phân tích về các biến thể attention khác nhau cho suy luận trong phần này. Chi tiết bộ dữ liệu cho các benchmark WikiText-103 và enwik8 và chi tiết chỉ số đánh giá của chúng được bao gồm trong Phụ lục A. Chi tiết của mô hình mà chúng tôi xây dựng lựa chọn bộ nhớ trên đó có thể được xem trong Phụ lục B.

--- TRANG 4 ---
WikiText-103
Model M m n PPL (↓)
Transformer+RPE - - - 29.14
Transformer-XL - 200 64 24.17
TRAMS 400 200 64 23.98

enwik8
Model M m n bpc (↓)
Transformer+RPE - - - 1.240
Transformer-XL - 200 64 1.215
TRAMS 400 200 64 1.198

Bảng 1: Hiệu suất mô hình trên bộ dữ liệu WikiText-103 cấp từ và bộ dữ liệu enwik8 cấp ký tự.

3.1 Các Phương pháp So sánh
Transformer+RPE (Vaswani et al., 2017): baseline transformer vanilla với relative position embedding giống như Transformer-XL. Do đó, sự khác biệt duy nhất giữa mô hình này và Transformer-XL là các bộ nhớ bổ sung. Thông tin thêm liên quan đến relative position embedding có thể được xem trong Phụ lục C.

Transformer-XL (Dai et al., 2019): một kiến trúc được thiết kế cụ thể cho mô hình hóa ngôn ngữ tầm xa. Nó bao gồm relative position embedding và bộ nhớ tái phát trên mỗi lớp. Các khe bộ nhớ được điền với các trạng thái ẩn từ các bước thời gian trước đó.

3.2 Cài đặt Thí nghiệm
Chúng tôi so sánh các phương pháp của chúng tôi với Transformer-XL (Dai et al., 2019) dưới cùng kích thước bộ nhớ (m=200) cho tính toán attention. Đối với độ dài token đầu vào n cho cả hai mô hình, chúng tôi giữ giống như trong (Dai et al., 2019) (n=64). Ngoài ra, quá trình lựa chọn bộ nhớ được thực hiện trên một pool bộ nhớ với kích thước M. Mô hình của chúng tôi và Transformer-XL chia sẻ các tham số mô hình nhưng có các chiến lược suy luận khác nhau.

3.3 Kết quả Chính
Các kết quả chính của bộ dữ liệu WikiText-103 và enwik8 được hiển thị trong Bảng 1. Mà không cần huấn luyện bổ sung hoặc tham số bổ sung, chúng tôi đạt được cải thiện 0.19 trong perplexity và cải thiện 0.017 cho bit-per-character với cơ chế TRAMS của chúng tôi. Chúng tôi thực hiện p-test bằng cách suy luận trên nhiều checkpoint mô hình và chứng minh rằng kết quả của chúng tôi có ý nghĩa thống kê (p < 0.05).

4 Thảo luận
TRAMS có dễ bị tổn thương với việc lựa chọn siêu tham số không? Có ba siêu tham số trong TRAMS: kích thước pool bộ nhớ M mà TRAMS có thể lựa chọn từ đó; kích thước bộ nhớ được chọn m được sử dụng trong quá trình forward; và kích thước token đầu vào n được liên quan đến cả quá trình backward và forward.

Từ nghiên cứu ablation về M, Hình 4 gợi ý một phạm vi tối ưu giữa 300 đến 400 cho kích thước pool bộ nhớ. Vượt quá phạm vi này, việc mở rộng pool bộ nhớ thường dẫn đến việc lựa chọn các token không liên quan, làm giảm hiệu suất của chúng tôi.

Về m, Hình 5 cho thấy rằng TRAMS chứng kiến một sự giảm đáng kể trong perplexity khi kích thước bộ nhớ được chọn khoảng 25%. Lựa chọn một phần lớn hơn không mang lại cải thiện thêm. Điều này phù hợp với Hình 3, nơi TRAMS xuất sắc bằng cách tập trung vào 10% kết quả hàng đầu.

Cuối cùng, trong nghiên cứu về n, Hình 6 cho thấy rằng khi độ dài token đích giảm, hiệu quả của lựa chọn bộ nhớ được cải thiện.

Hình 4: Nghiên cứu ablation về kích thước pool bộ nhớ M khi chúng tôi cố định m=200 và n=64.

Hình 5: Nghiên cứu ablation về kích thước bộ nhớ được chọn m khi chúng tôi cố định M=600 và n=64.

Chi phí suy luận so với Transformer-XL như thế nào? Vì không có phần huấn luyện trong mô hình của chúng tôi, chúng tôi tập trung vào thảo luận chi phí suy luận. So với Transformer-XL, mô hình của chúng tôi yêu cầu lưu trữ một pool bộ nhớ lớn hơn để thực hiện lựa chọn bộ nhớ. Do đó, chi phí bộ nhớ của phương pháp chúng tôi sẽ lớn hơn. Khi nói đến chi phí thời gian, mô hình của chúng tôi có một phép tính chuẩn token bộ nhớ bổ sung

--- TRANG 5 ---
Hình 6: Nghiên cứu ablation về độ dài đích n khi chúng tôi cố định M=400 và m=200.

Model Peak GPU Mem (MB) Wall-clock Time (s)
Transformer-XL 3529 33.27
TRAMS 3719 49.55

Bảng 2: Kết quả về việc sử dụng bộ nhớ GPU đỉnh và thời gian suy luận wall-clock trên WikiText-103.

các hoạt động sắp xếp bộ nhớ, và các hoạt động lựa chọn bộ nhớ cho mỗi lớp. Những hoạt động bổ sung này yêu cầu thời gian suy luận thêm. Bảng 2 cho thấy chi phí bộ nhớ GPU và thời gian wall-clock cho baseline Transformer-XL và mô hình của chúng tôi. Mô hình của chúng tôi yêu cầu việc sử dụng bộ nhớ GPU hơi nhiều hơn và khoảng 50% thời gian suy luận bổ sung cho lựa chọn bộ nhớ.

TRAMS được hưởng lợi từ lựa chọn bộ nhớ như thế nào? Lựa chọn bộ nhớ giúp mô hình chọn các token có điểm attention cao hơn với các truy vấn, do đó tăng mức sử dụng bộ nhớ trung bình. Về mặt định lượng, phương pháp của chúng tôi cải thiện xác suất attention trung bình 24.25% cho cùng kích thước bộ nhớ so với Transformer-XL.

Mỗi lớp có cùng tầm quan trọng không? Dựa trên Hình 7, chúng tôi cho thấy nghiên cứu ablation khi áp dụng lựa chọn bộ nhớ trên mỗi lớp trong khi giữ các lớp khác giống nhau. Có một sự giảm có thể quan sát được khi chúng tôi áp dụng lựa chọn bộ nhớ trên các lớp sâu hơn bắt đầu từ Lớp 13 trong khi chúng tôi không quan sát được ảnh hưởng rõ ràng khi áp dụng lựa chọn bộ nhớ trên các lớp nông.

5 Nghiên cứu Trường hợp
Để hiểu loại ngữ cảnh nào nên được chọn, chúng tôi cung cấp một ví dụ trường hợp để hiểu cụ thể loại token nào trong bộ nhớ sẽ được chọn. Dựa trên Bảng 3, chúng tôi có thể thấy rằng hầu hết các token bộ nhớ được chọn là những từ có tần suất thấp. Những từ có tần suất thấp như "John" trong bộ nhớ sẽ có lợi cho việc dự đoán "John" trong chuỗi đích.

Hình 7: Nghiên cứu Ablation về Tầm quan trọng theo Lớp trên WikiText-103.

Memory Sequence Segment
...Simon Stephens , which was performed in
2001 at the Royal Court Theatre. He had
a guest role in the television series Judge
John Deed in 2002. In 2004 Boulter landed
a role as "Craig" in the episode "Teddy's
Story" of the television series The Long Firm;
he starred alongside actors Mark Strong and
Derek Jacobi. He was cast in the 2005 theatre
productions of the Philip Ridley play Mercury
Fur, which was performed at the Drum Theatre
in Plymouth and the <unk> Chocolate Factory
in London. He was directed by John Tiffany
and starred alongside Ben Whishaw , Shane Zaza,
Harry Kent, Fraser Ayres, Sophie Stanton, and
Dominic Hall. <eos> In 2006, Boulter starred
alongside Whishaw in the play Citizenship
written by Mark Ravenhill ...

Target Sequence Segment
He appeared in the television series Judge
John Deed in 2002 ...

Bảng 3: Nghiên cứu Trường hợp cho lựa chọn bộ nhớ từ WikiText-103. text cho thấy rằng từ này trong chuỗi bộ nhớ được chọn và sử dụng trong lượt forward. text cho thấy rằng từ này trong chuỗi đích được hưởng lợi từ bộ nhớ.

6 Kết luận
Trong công trình này, chúng tôi công thức hóa vấn đề lựa chọn bộ nhớ trong kiến trúc transformer và tái công thức hóa quá trình tính toán attention để có được các truy vấn và khóa tự định nghĩa của chúng tôi. Sau đó, chúng tôi đề xuất một chỉ số độc lập với truy vấn sử dụng các trạng thái ẩn bộ nhớ để thực hiện một bộ chọn bộ nhớ không cần huấn luyện. Các thí nghiệm của chúng tôi cho thấy rằng phương pháp này cung cấp một phương tiện đơn giản nhưng hiệu quả để xác định các token bộ nhớ có giá trị. Khám phá các chiến lược lựa chọn bộ nhớ tối ưu cho các mô hình ngôn ngữ lớn là một hướng nghiên cứu đầy hứa hẹn cho tương lai. Ngoài ra, việc tích hợp các tham số có thể huấn luyện vào các mô hình này như bộ chọn bộ nhớ trình bày một hướng nghiên cứu thú vị khác cho công việc tương lai.

--- TRANG 6 ---
Hạn chế
Nghiên cứu của chúng tôi có một vài hạn chế chính. Thứ nhất, chúng tôi hiện đang tập trung vào kiến trúc Transformer-XL, nhưng có nhiều mô hình khác với các kích thước khác nhau mà chúng tôi chưa thử. Điều này cho thấy rằng các phát hiện của chúng tôi có thể bị giới hạn ở kiến trúc transformer điển hình. Thứ hai, phương pháp của chúng tôi có nhiều siêu tham số bao gồm M, m, và n. Việc điều chỉnh chúng có thể thay đổi lớn cách mô hình của chúng tôi hoạt động. Do đó, cần có một hiệu chuẩn cẩn thận, và người ta phải cẩn trọng để đạt được sự cân bằng và đạt được hiệu suất mong muốn, điều này có thể tốn thời gian và chi phí tính toán đắt đỏ.

Tuyên bố Đạo đức
Không có rủi ro tiềm ẩn được công nhận.

Tài liệu tham khảo
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R Gormley. 2023. Unlimiformer: Long-
range transformers with unlimited length input.
arXiv preprint arXiv:2305.01625.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.

Rewon Child, Scott Gray, Alec Radford, and
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509.

Krzysztof Choromanski, Haoxian Chen, Han Lin,
Yuanzhe Ma, Arijit Sehanobish, Deepali Jain,
Michael S Ryoo, Jake Varley, Andy Zeng, Valerii
Likhosherstov, et al. 2021. Hybrid random features.
arXiv preprint arXiv:2110.04367.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-xl: Attentive language models beyond
a fixed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics, pages 2978–2988.

Giannis Daras, Nikita Kitaev, Augustus Odena, and
Alexandros G Dimakis. 2020. Smyrf-efficient atten-
tion using asymmetric clustering. Advances in Neu-
ral Information Processing Systems, 33:6476–6489.

Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
Toutanova. 2019. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. In
Proceedings of NAACL-HLT, pages 4171–4186.

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through memorization: Nearest neighbor language
models. In International Conference on Learning
Representations.

Yunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019.
When and why is document-level context useful in
neural machine translation? In Proceedings of the
Fourth Workshop on Discourse in Machine Transla-
tion (DiscoMT 2019), pages 24–34.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2019. Reformer: The efficient transformer. In Inter-
national Conference on Learning Representations.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learning
of language representations. In International Confer-
ence on Learning Representations.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.

Matt Mahoney. 2011. Large text compression bench-
mark.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els. In International Conference on Learning Repre-
sentations.

Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani
Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy
Schwartz, and Noah A Smith. 2022a. Abc: Attention
with bounded-memory control. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
7469–7483.

Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy
Schwartz, Noah Smith, and Lingpeng Kong. 2022b.
Random feature attention. In International Confer-
ence on Learning Representations.

Michał Pietruszka, Łukasz Borchmann, and Łukasz
Garncarek. 2022. Sparsifying transformer models
with trainable representation pooling. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 8616–8633.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research,
21(1):5485–5551.

--- TRANG 7 ---
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and
David Grangier. 2021. Efficient content-based sparse
attention with routing transformers. Transactions of
the Association for Computational Linguistics, 9:53–
68.

Sainbayar Sukhbaatar, Édouard Grave, Piotr Bo-
janowski, and Armand Joulin. 2019. Adaptive at-
tention span in transformers. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics, pages 331–335.

Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen
Roller, Arthur Szlam, Jason Weston, and Angela Fan.
2021. Not all memories are created equal: Learning
to forget by expiring. In International Conference on
Machine Learning, pages 9902–9912. PMLR.

Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-
zler. 2022. Efficient transformers: A survey. ACM
Computing Surveys, 55(6):1–28.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems, 30.

Apoorv Vyas, Angelos Katharopoulos, and François
Fleuret. 2020. Fast transformers with clustered at-
tention. Advances in Neural Information Processing
Systems, 33:21665–21674.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768.

Lesly Miculicich Werlen, Dhananjay Ram, Nikolaos
Pappas, and James Henderson. 2018. Document-
level neural machine translation with hierarchical
attention networks. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2947–2954.

Lin Zheng, Chong Wang, and Lingpeng Kong. 2022a.
Linear complexity randomized self-attention mech-
anism. In International Conference on Machine
Learning, pages 27011–27041. PMLR.

Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng
Kong. 2022b. Efficient attention via control vari-
ates. In The Eleventh International Conference on
Learning Representations.

Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui,
Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cot-
terell, and Mrinmaya Sachan. 2023. Recurrentgpt:
Interactive generation of (arbitrarily) long text. arXiv
preprint arXiv:2305.13304.

A Bộ dữ liệu và Chỉ số Đánh giá
WikiText-103 (Merity et al., 2016) là một benchmark mô hình hóa ngôn ngữ cấp từ được sử dụng phổ biến. Nó có độ dài trung bình 3.6 nghìn token mỗi bài viết và bao gồm 28 nghìn bài viết Wikipedia. Bộ dữ liệu cấp từ này có kích thước từ vựng khoảng 260K. Chúng tôi sử dụng cùng cài đặt tiền xử lý dữ liệu trong Dai et al. (2019) cho bộ dữ liệu này. Chúng tôi sử dụng perplexity làm chỉ số của chúng tôi.

Enwik8 (Mahoney, 2011) là một benchmark mô hình hóa ngôn ngữ cấp ký tự. Bộ dữ liệu này chứa 100M ký tự Wikipedia chưa được xử lý. Tập train, tập dev, và tập test bao gồm 80M, 10M, và 10M ký tự riêng biệt. enwik8 không có giai đoạn tiền xử lý và được sử dụng trực tiếp. bpc (bit per character) được định nghĩa là chỉ số đánh giá và chúng tôi báo cáo kết quả trên cả tập dev và tập test.

B Cấu hình Huấn luyện
Vì chúng tôi thực hiện các thí nghiệm suy luận dựa trên một mô hình đã được huấn luyện, chúng tôi huấn luyện riêng biệt hai mô hình Transformer-XL cho WikiText-103 và enwik8. Đối với giai đoạn huấn luyện, chúng tôi sử dụng Adam (Kingma và Ba, 2014) để tối ưu hóa với batch size=60, learning rate=2.5e-4, target length=150, memory length=150, và một bộ lập lịch cosine không có các bước warmup.

Khi nói đến một bộ dữ liệu khác, chúng tôi sử dụng kiến trúc Transformer-XL khác nhau. Đối với WikiText-103, chúng tôi sử dụng kiến trúc transformer 16 lớp với 10 head, 410 hid dim, 0.1 dropout ratio, 0.0 attention dropout ratio, 2100 inner dim, và cơ chế adaptive softmax. Đối với enwik8, chúng tôi đề xuất kiến trúc transformer 12 lớp với 8 head, 512 hid dim, 0.1 dropout ratio, 0.0 attention dropout ratio, và 2048 inner dim. Cả hai mô hình đều được huấn luyện trong 350K bước.

Một batch size=10 và target length=150 được cố định cho tất cả các thí nghiệm suy luận để tránh so sánh không công bằng. Tất cả các thí nghiệm bao gồm huấn luyện và suy luận đều được thực hiện sử dụng 4 GPU 2080Ti. Cần 280 giờ GPU để huấn luyện checkpoint mô hình enwik8. Cần 61 giờ GPU để huấn luyện checkpoint mô hình WikiText-103.

C Relative Position Embedding
Về mã hóa vị trí, chúng tôi duy trì cùng kết quả với Transformer-XL. Các mã hóa vị trí bao gồm các tham số có thể học được là R_(i-j), u, và v. Thông thường, R_(i-j) được tạo ra từ một mạng r có thể học được bao gồm trong mô hình. Ưu điểm của việc sử dụng thiết kế này khi tính điểm attention là nó tránh sự nhầm lẫn thời gian gây ra bởi việc lập chỉ mục cùng một vị trí và xem xét khoảng cách tương đối giữa hai token. Công thức

--- TRANG 8 ---
cho tính toán điểm attention với relative position embedding có thể được viết là:

A^xl_(i,j) = X^T_i W^T_q W^E_k X_j + X^T_i W^T_q W^R_k R_(i-j)
            + u^T W^E_k X_j + v^T W^R_k R_(i-j)     (7)

Hơn nữa, sau khi thực hiện các nghiên cứu ablation về relative position embedding, chúng tôi phát hiện rằng R_(i-j) đóng góp nhiều nhất cho kết quả và u, v chỉ có ảnh hưởng nhỏ đến hiệu suất cuối cùng. Sự tồn tại của R_(i-j) dẫn đến phân phối xác suất attention giảm theo hàm mũ liên quan đến vị trí bộ nhớ. Kết quả là, chúng tôi dựa việc lựa chọn bộ nhớ của chúng tôi trên A^xl_(i,j) bao gồm thông tin vị trí thay vì X^T_i W^T_q W^E_k X_j thuần túy.

Cần lưu ý, tất cả các khái niệm liên quan đến qK đều được trang bị position embedding thay vì một tích vô hướng đơn giản.