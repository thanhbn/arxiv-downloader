# 2211.12588.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2211.12588.pdf
# Kích thước tệp: 2997096 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)
Program of Thoughts Prompting: Tách Tính toán khỏi Lý luận cho các Nhiệm vụ Lý luận Số học
§,¶Wenhu Chen∗,§Xueguang Ma∗,†Xinyi Wang,◦William W. Cohen
§University of Waterloo
¶Vector Institute, Toronto
†University of California, Santa Barabra
◦Google Research
{wenhuchen,x93ma}@uwaterloo.ca, xinyi_wang@ucsb.edu, wcohen@google.com
Đánh giá trên OpenReview: https://openreview.net/forum?id=YfZ4ZPt8zd
Tóm tắt
Gần đây, đã có tiến bộ đáng kể trong việc dạy các mô hình ngôn ngữ thực hiện lý luận từng bước để giải quyết các nhiệm vụ lý luận số học phức tạp. Chain-of-thoughts prompting (CoT) là phương pháp tiên tiến nhất cho nhiều nhiệm vụ này. CoT sử dụng các mô hình ngôn ngữ để tạo ra văn bản mô tả lý luận, tính toán, và cuối cùng là câu trả lời cho một câu hỏi. Ở đây chúng tôi đề xuất 'Program of Thoughts' (PoT), sử dụng các mô hình ngôn ngữ (chủ yếu là Codex) để tạo ra văn bản và các câu lệnh ngôn ngữ lập trình, và cuối cùng là một câu trả lời. Trong PoT, việc tính toán có thể được ủy thác cho một trình thông dịch chương trình, được sử dụng để thực thi chương trình được tạo ra, do đó tách rời tính toán phức tạp khỏi lý luận và hiểu biết ngôn ngữ. Chúng tôi đánh giá PoT trên năm bộ dữ liệu bài toán từ toán học và ba bộ dữ liệu tài chính-QA trong cả hai thiết lập few-shot và zero-shot. Chúng tôi thấy rằng PoT có hiệu suất trung bình tăng so với CoT khoảng 12% trên tất cả các bộ dữ liệu. Bằng cách kết hợp PoT với self-consistency decoding, chúng tôi có thể đạt được hiệu suất cực kỳ mạnh trên tất cả các bộ dữ liệu toán học và bộ dữ liệu tài chính. Tất cả dữ liệu và mã của chúng tôi sẽ được công bố.

1 Giới thiệu
Lý luận số học là một nhiệm vụ lâu đời trong trí tuệ nhân tạo. Một loạt các bộ dữ liệu đã được đề xuất gần đây để đánh giá khả năng thực hiện lý luận số học/số học của các mô hình deep-learning. Một số bộ dữ liệu được sử dụng rộng rãi dựa trên các bài toán từ Toán học (MWP) (Cobbe et al., 2021; Patel et al., 2021; Lu et al., 2022; Ling et al., 2017), nơi các hệ thống được cho là trả lời các câu hỏi toán học được biểu đạt bằng văn bản tự nhiên. Ngoài MWP, một số bộ dữ liệu cũng xem xét các vấn đề tài chính (Chen et al., 2021b; 2022; Zhu et al., 2021), nơi các hệ thống cần trả lời các câu hỏi tài chính dựa trên toán học.

Công việc trước đây (Ling et al., 2017; Cobbe et al., 2021) đã nghiên cứu cách đào tạo các mô hình từ đầu hoặc tinh chỉnh các mô hình để tạo ra các bước trung gian để dẫn xuất câu trả lời cuối cùng. Các phương pháp như vậy đòi hỏi nhiều dữ liệu, yêu cầu một số lượng đáng kể các ví dụ đào tạo với các bước được chú thích bởi chuyên gia. Gần đây, Nye et al. (2021) đã phát hiện ra rằng các mô hình ngôn ngữ lớn (LLMs) (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022) có thể được nhắc với một số ví dụ đầu vào-đầu ra để giải quyết các nhiệm vụ này mà không cần đào tạo hoặc tinh chỉnh. Đặc biệt, khi được nhắc với một số ví dụ chứa đầu vào, 'lý lẽ' ngôn ngữ tự nhiên, và đầu ra, LLMs có thể bắt chước các minh chứng để cả tạo ra lý lẽ và trả lời các câu hỏi này. Phương pháp nhắc như vậy sau này được mở rộng thành 'Chain of Thoughts (CoT)' (Wei et al., 2022), và nó có thể đạt được hiệu suất tiên tiến trên một phổ rộng các bộ dữ liệu lý luận văn bản và số học.

∗Công việc được thực hiện tại University of Waterloo. Wenhu Chen là tác giả chính.
1arXiv:2211.12588v4  [cs.CL]  23 Oct 2023

--- TRANG 2 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Hình 1: So sánh giữa Chain of Thoughts và Program of Thoughts.

CoT sử dụng LLMs cho cả lý luận và tính toán, tức là mô hình ngôn ngữ không chỉ cần tạo ra các biểu thức toán học mà còn cần thực hiện tính toán trong mỗi bước. Chúng tôi lập luận rằng các mô hình ngôn ngữ không lý tưởng để thực sự giải quyết các biểu thức toán học này, bởi vì: 1) LLMs rất dễ mắc lỗi tính toán số học, đặc biệt khi xử lý các số lớn; 2) LLMs không thể giải quyết các biểu thức toán học phức tạp như phương trình đa thức hoặc thậm chí phương trình vi phân; 3) LLMs rất kém hiệu quả trong việc biểu đạt lặp, đặc biệt khi số bước lặp lớn.

Để giải quyết các vấn đề này, chúng tôi đề xuất program-of-thoughts (PoT) prompting, sẽ ủy thác các bước tính toán cho một trình thông dịch ngôn ngữ bên ngoài. Trong PoT, LMs có thể biểu đạt các bước lý luận dưới dạng chương trình Python, và việc tính toán có thể được thực hiện bởi một trình thông dịch Python. Chúng tôi mô tả sự khác biệt giữa CoT và PoT trong Hình 1. Trong ví dụ trên, đối với CoT việc lặp chạy 50 lần, dẫn đến độ chính xác cực kỳ thấp;¹ trong ví dụ dưới, CoT không thể giải phương trình bậc ba với các mô hình ngôn ngữ và xuất ra một câu trả lời sai. Ngược lại, trong ví dụ trên, PoT có thể biểu đạt quá trình lặp với một số dòng mã, có thể được thực thi trên một trình thông dịch Python để dẫn xuất một câu trả lời chính xác; và trong ví dụ dưới, PoT có thể chuyển đổi vấn đề thành một chương trình dựa vào thư viện 'SymPy' trong Python để giải phương trình phức tạp.

Chúng tôi đánh giá PoT prompting trên năm bộ dữ liệu MWP, GSM8K, AQuA, SVAMP, TabMWP, MultiArith; và ba bộ dữ liệu tài chính, FinQA, ConvFinQA, và TATQA. Các bộ dữ liệu này bao gồm các định dạng đầu vào khác nhau bao gồm văn bản, bảng, và hội thoại. Chúng tôi đưa ra một tổng quan về kết quả trong Hình 2. Dưới cả hai thiết lập few-shot và zero-shot, PoT vượt trội CoT đáng kể trên tất cả các bộ dữ liệu được đánh giá. Dưới thiết lập few-shot, mức tăng trung bình so với CoT là khoảng 8% cho các bộ dữ liệu MWP và 15% cho các bộ dữ liệu tài chính. Dưới thiết lập zero-shot, mức tăng trung bình so với CoT là khoảng 12% cho các bộ dữ liệu MWP. PoT kết hợp với self-consistency (SC) cũng vượt trội CoT+SC (Wang et al., 2022b) trung bình 10%

¹Giả sử mỗi phép cộng đúng với xác suất 90%, sau 50 phép cộng, khả năng của một đầu ra đúng ít hơn 1%.
2

--- TRANG 3 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

GSM8K AQuA SVAMP TabMWPFinQA ConvFin TATQA63.1
45.376.4
65.2
40.445.561.471.6
54.185.2
73.2
64.5 64.669CoTPoT

GSM8K AQuA SVAMP TabMWPFinQA ConvFin TATQA78
5286.8
75.4
44.447.963.980
58.689.1
81.8
68.1 67.373.4CoT-SC PoT-SC

GSM8K AQuA SVAMP TabMWPFinQA ConvFin TATQA40.5
31.963.7
53.5
29.528.240.557
43.970.8
65.2
52.550.460.4ZS-CoT ZS-PoT

Hình 2: Tổng quan hiệu suất Few-shot (trên), Few-shot + SC (giữa) và Zero-Shot (dưới) của Codex PoT và Codex CoT trên các bộ dữ liệu khác nhau.

trên tất cả các bộ dữ liệu. PoT+SC của chúng tôi đạt được kết quả tốt nhất đã biết trên tất cả các bộ dữ liệu MWP được đánh giá và kết quả gần tốt nhất đã biết trên các bộ dữ liệu tài chính (loại trừ GPT-4 (OpenAI, 2023)). Cuối cùng, chúng tôi tiến hành các nghiên cứu ablation toàn diện để hiểu các thành phần khác nhau của PoT.

2 Program of Thoughts

2.1 Kiến thức cơ bản

In-context learning đã được mô tả trong Brown et al. (2020); Chen et al. (2021a); Chowdhery et al. (2022); Rae et al. (2021). So với fine-tuning, in-context learning (1) chỉ lấy một số chú thích/minh chứng làm prompt, và (2) thực hiện suy luận mà không đào tạo các tham số mô hình. Với in-context learning, LLMs nhận các ví dụ đầu vào-đầu ra làm tiền tố, theo sau bởi một vấn đề đầu vào, và tạo ra đầu ra bắt chước các ví dụ. Gần đây hơn, 'chain of thoughts prompting' (Wei et al., 2022) đã được đề xuất như một loại in-context learning cụ thể nơi đầu ra của ví dụ chứa 'quá trình suy nghĩ' hoặc lý lẽ thay vì chỉ một đầu ra. Cách tiếp cận này đã được chứng minh là gợi ra khả năng lý luận mạnh mẽ của LLMs trên các loại nhiệm vụ khác nhau.

3

--- TRANG 4 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Hình 3: Trái: Few-shot PoT prompting, Phải: Zero-shot PoT prompting.

2.2 Program of Thoughts

Ngoài ngôn ngữ tự nhiên, các chương trình cũng có thể được sử dụng để biểu đạt quá trình suy nghĩ của chúng ta. Bằng cách sử dụng các tên biến có ý nghĩa về mặt ngữ nghĩa, một chương trình cũng có thể là một biểu diễn tự nhiên để truyền đạt suy nghĩ của con người. Ví dụ, trong ví dụ dưới trong Hình 1, chúng ta đầu tiên tạo một biến không xác định tên là interest_rate. Sau đó chúng ta gắn 'tổng trong hai năm với ... lãi suất' vào biến sum_in_two_years_with_XXX_interest và viết ra phương trình biểu đạt mối quan hệ toán học của chúng với interest_rate. Các phương trình này được đóng gói vào hàm 'solve' được cung cấp bởi 'SymPy'. Chương trình được thực thi với Python để giải các phương trình để dẫn xuất biến câu trả lời interest_rate.

Không giống như CoT, PoT chuyển giao một số tính toán cho một quá trình bên ngoài (một trình thông dịch Python). LLMs chỉ chịu trách nhiệm biểu đạt 'quá trình lý luận' trong ngôn ngữ lập trình. Ngược lại, CoT nhằm sử dụng LLMs để thực hiện cả lý luận và tính toán. Chúng tôi lập luận rằng cách tiếp cận như vậy biểu đạt hơn và chính xác hơn về mặt lý luận số học.

'Program of thoughts' khác với việc tạo ra các phương trình trực tiếp, nơi mục tiêu tạo ra sẽ là solve(20000*(1+x)³-2000-x*20000*3-1000, x). Như đã quan sát bởi Wei et al. (2022) cho CoT, việc tạo ra trực tiếp các phương trình như vậy là thách thức đối với LLMs. PoT khác với việc tạo phương trình trong hai khía cạnh: (1) PoT chia nhỏ phương trình thành một quá trình 'suy nghĩ' nhiều bước, và (2) PoT gắn ý nghĩa ngữ nghĩa vào các biến để giúp định vị mô hình trong ngôn ngữ. Chúng tôi thấy rằng loại quá trình 'suy nghĩ' này có thể gợi ra khả năng lý luận của các mô hình ngôn ngữ và tạo ra các chương trình chính xác hơn. Chúng tôi cung cấp so sánh chi tiết trong phần thực nghiệm.

Chúng tôi trình bày phương pháp PoT prompting được đề xuất trong Hình 3 dưới thiết lập few-shot và zero-shot. Dưới thiết lập few-shot, một số ví dụ của các cặp (câu hỏi, 'program of thoughts') sẽ được tiền tố làm minh chứng để dạy LLM cách tạo ra các chương trình 'suy nghĩ'. Dưới thiết lập zero-shot, prompt chỉ chứa một hướng dẫn mà không có minh chứng ví dụ nào. Không giống như zero-shot CoT (Kojima et al., 2022), yêu cầu một bước thêm để trích xuất câu trả lời từ 'chain of thoughts', zero-shot PoT có thể trả về câu trả lời một cách trực tiếp mà không cần các bước thêm.

Trong zero-shot PoT, một caveat là LLM có thể quay lại tạo ra một chuỗi lý luận trong các comment thay vì trong chương trình. Do đó, chúng tôi đề xuất triệt tiêu logits token '#' để khuyến khích nó tạo ra các chương trình.

4

--- TRANG 5 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Hình 4: PoT kết hợp với CoT cho lý luận nhiều giai đoạn.

2.3 PoT như một Bước Trung gian

Đối với một số vấn đề yêu cầu lý luận văn bản bổ sung, chúng tôi đề xuất sử dụng PoT để giải quyết phần tính toán. Chương trình được tạo bởi PoT có thể được thực thi để cung cấp kết quả trung gian, sau đó được kết hợp với câu hỏi để dẫn xuất câu trả lời cuối cùng với CoT. Chúng tôi mô tả toàn bộ quá trình trong Hình 8. Trong quá trình minh chứng, chúng tôi trình bày LLMs với các ví dụ để dạy nó dự đoán liệu có cần sử dụng lý luận CoT bổ sung hay không. Nếu LLM xuất ra 'keep prompting' ở cuối, chúng tôi sẽ áp dụng kết quả thực thi từ PoT làm đầu vào để tiếp tục nhắc LLMs dẫn xuất câu trả lời thông qua CoT.

Ví dụ, trong ví dụ trái trong Hình 3, chương trình sẽ được thực thi để trả về một số thực 'ans=2.05', có nghĩa là sau 2.05 giờ hai tàu sẽ gặp nhau. Tuy nhiên, việc trực tiếp cộng 2.05 vào 11 AM không có nghĩa gì vì 2.05 giờ cần được dịch sang phút để có được định dạng thời gian tiêu chuẩn HH:MM để làm cho nó phù hợp với tùy chọn được cung cấp trong các câu hỏi trắc nghiệm. Xin lưu ý rằng chiến lược prompting này chỉ cần thiết cho AQuA vì các bộ dữ liệu khác đều có thể được giải quyết bằng prompting chỉ PoT.

3 Thí nghiệm

3.1 Thiết lập Thí nghiệm

Bộ dữ liệu Chúng tôi tóm tắt các bộ dữ liệu được đánh giá trong Bảng 1. Chúng tôi sử dụng tập test cho tất cả các bộ dữ liệu được đánh giá ngoại trừ TATQA. Các bộ dữ liệu này rất không đồng nhất về định dạng đầu vào của chúng. Chúng tôi tiến hành các thí nghiệm toàn diện trên phổ rộng các bộ dữ liệu này để cho thấy tính tổng quát và khả năng áp dụng của PoT prompting.

Bộ dữ liệu Split Ví dụ Lĩnh vực Đầu vào Đầu ra
GSM8K (Cobbe et al., 2021) Test 1318 MWP Câu hỏi Số
AQuA (Ling et al., 2017) Test 253 MWP Câu hỏi Tùy chọn
SVAMP (Patel et al., 2021) Test 1000 MWP Câu hỏi Số
MultiArith (Roy & Roth, 2015) Test 600 MWP Câu hỏi Số
TabMWP (Lu et al., 2022) Test 7861 MWP Bảng + Câu hỏi Số + Văn bản
FinQA (Chen et al., 2021b) Test 1147 Tài chính Bảng + Văn bản + Câu hỏi Số + Nhị phân
ConvFinQA (Chen et al., 2022) Test 421 Tài chính Bảng + Văn bản + Hội thoại Số + Nhị phân
TATQA (Zhu et al., 2021) Dev 1668 Tài chính Bảng + Văn bản + Câu hỏi Số + Văn bản

Bảng 1: Tóm tắt về tất cả các bộ dữ liệu được đánh giá.

Để kết hợp các đầu vào đa dạng, chúng tôi đề xuất tuyến tính hóa các đầu vào này trong prompt. Đối với đầu vào bảng, chúng tôi áp dụng cùng chiến lược như Chen (2022) để tuyến tính hóa một bảng thành một chuỗi văn bản. Các cột của bảng được phân tách bằng '|' và các hàng được phân tách bằng '\n'. Nếu một ô bảng trống, nó được điền bằng '-'. Đối với đầu vào lai văn bản+bảng, chúng tôi phân tách bảng và văn bản bằng '\n'. Đối với lịch sử hội thoại, chúng tôi cũng phân tách các lượt hội thoại bằng '\n'. Prompt được xây dựng bằng cách nối hướng dẫn nhiệm vụ, văn bản, bảng tuyến tính hóa, và câu hỏi. Đối với trả lời câu hỏi hội thoại, chúng tôi đơn giản nối tất cả lịch sử đối thoại trong prompt.

5

--- TRANG 6 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Chi tiết Triển khai Chúng tôi chủ yếu sử dụng API OpenAI Codex (code-davinci-002)² cho các thí nghiệm của chúng tôi. Chúng tôi cũng đã thử nghiệm GPT-3 (text-davinci-002), ChatGPT (gpt-turbo-3.5), CodeGen (Nijkamp et al., 2022) (codegen-16B-multi và codegen-16B-mono), CodeT5+ (Wang et al., 2023b) và Xgen³ cho các thí nghiệm ablation. Chúng tôi sử dụng Python 3.8 với thư viện SymPy⁴ để thực thi chương trình được tạo. Đối với thiết lập few-shot, chúng tôi sử dụng 4-8 shots cho tất cả các bộ dữ liệu, dựa trên độ khó của chúng. Đối với các bộ dữ liệu đơn giản như FinQA (Chen et al., 2021b), chúng tôi có xu hướng sử dụng ít shots hơn, trong khi đối với các bộ dữ liệu thách thức hơn như AQuA (Ling et al., 2017) và TATQA (Zhu et al., 2021), chúng tôi sử dụng 8 shots để bao phủ các vấn đề đa dạng hơn. Các ví dụ được lấy từ tập huấn luyện. Chúng tôi thường viết prompts cho 10-20 ví dụ và sau đó điều chỉnh lựa chọn ví dụ trên một tập validation nhỏ để chọn 4-8 shots tốt nhất cho đánh giá tập đầy đủ.

Để gợi ra khả năng của LLM thực hiện lý luận nhiều bước, chúng tôi thấy một prompt để khuyến khích LLMs tạo ra các chương trình hợp lý mà không cần minh chứng. Prompt chi tiết được hiển thị trong Hình 3. Tuy nhiên, một caveat là LLM có thể quay lại tạo ra một chuỗi lý luận trong các comment thay vì trong chương trình. Do đó, chúng tôi triệt tiêu logits token '#' bằng một bias nhỏ để giảm xác suất của nó để tránh các trường hợp như vậy. Trong nghiên cứu sơ bộ của chúng tôi, chúng tôi thấy rằng -2 làm bias có thể đạt được kết quả tốt nhất. Chúng tôi thấy rằng chiến lược đơn giản này có thể cải thiện đáng kể hiệu suất của chúng tôi.

Metrics Chúng tôi áp dụng điểm exact match làm metrics đánh giá cho các bộ dữ liệu GSM8K, SVAMP, và MultiArith. Chúng tôi sẽ làm tròn số dự đoán đến một độ chính xác cụ thể và sau đó so sánh nó với số tham chiếu. Đối với bộ dữ liệu AQuA, chúng tôi sử dụng PoT để tính toán câu trả lời trung gian và sau đó nhắc LLM một lần nữa để xuất ra tùy chọn gần nhất để đo độ chính xác. Đối với các bộ dữ liệu TabMWP, ConvFinQA, và TATQA, chúng tôi sử dụng các script đánh giá chính thức được cung cấp trên Github. Đối với FinQA, chúng tôi nới lỏng đánh giá cho CoT vì LLMs không thể thực hiện tính toán chính xác (đặc biệt với floats độ chính xác cao và số lớn), vì vậy chúng tôi áp dụng 'math.isclose' với tolerance tương đối của 0.001 để so sánh câu trả lời.

Baselines Chúng tôi báo cáo kết quả cho ba mô hình khác nhau bao gồm Codex (Chen et al., 2021a), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) và LaMDA (Thoppilan et al., 2022). Chúng tôi xem xét hai loại chiến lược dự đoán bao gồm đầu ra câu trả lời trực tiếp và chain of thought để dẫn xuất câu trả lời. Vì API PaLM không công khai, chúng tôi chỉ liệt kê kết quả PaLM được báo cáo từ công việc trước đó (Wei et al., 2022; Wang et al., 2022b). Chúng tôi cũng tận dụng một máy tính bên ngoài như được đề xuất trong Wei et al. (2022) cho tất cả các phương trình được tạo bởi CoT, được ký hiệu là CoT + calc. Ngoài greedy decoding, chúng tôi sử dụng self-consistency (Wang et al., 2022b) với CoT, lấy majority vote trên 40 completions khác nhau làm dự đoán.

3.2 Kết quả Chính

Kết quả Few-shot Chúng tôi đưa ra kết quả few-shot trong Bảng 2. Trên các bộ dữ liệu MWP, PoT với greedy decoding cải thiện trên GSM8K/AQuA/TabMWP hơn 8%. Trên SVAMP, sự cải thiện là 4% chủ yếu do tính đơn giản của nó. Đối với các bộ dữ liệu tài chính QA, PoT cải thiện so với CoT khoảng 20% trên FinQA/ConvFinQA và 8% trên TATQA. Những cải thiện lớn hơn trong FinQA và ConvFinQA chủ yếu do tính toán sai trên LLMs cho các số lớn (ví dụ. hàng triệu). CoT áp dụng LLMs để thực hiện tính toán, rất dễ mắc lỗi tính toán sai, trong khi PoT áp dụng một máy tính bên ngoài có độ chính xác cao để giải quyết vấn đề. Như một ablation, chúng tôi cũng so sánh với CoT+calc, tận dụng một máy tính bên ngoài để sửa kết quả tính toán trong 'chain of thoughts' được tạo. Các thí nghiệm cho thấy việc thêm một máy tính bên ngoài chỉ cho thấy cải thiện nhẹ so với CoT trên các bộ dữ liệu MWP, kém xa PoT. Lý do chính cho hiệu suất kém của 'calculator' là do bước hậu xử lý cứng nhắc, có thể dẫn đến recall thấp trong việc hiệu chỉnh kết quả tính toán.

Kết quả Few-shot + Self-Consistency Chúng tôi tận dụng self-consistency (SC) decoding để hiểu giới hạn trên của phương pháp chúng tôi. Thuật toán decoding dựa trên sampling này có thể giảm đáng kể tính ngẫu nhiên trong quy trình tạo và tăng hiệu suất. Cụ thể, chúng tôi đặt temperature là 0.4 và K=40 trong toàn bộ thí nghiệm. Theo Bảng 2, chúng tôi thấy rằng PoT + SC vẫn vượt trội CoT + SC

²https://openai.com/blog/openai-codex/
³https://blog.salesforceairesearch.com/xgen/
⁴https://www.sympy.org/en/index.html
6

--- TRANG 7 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Mô hình #Params GSM8K AQuA SVAMP TabWMP FinQA ConvFin TATQA Avg
Fine-tuned hoặc few-shot prompt
Published SoTA - 78.0 52.0 86.8 68.2 68.0 68.9 73.6 70.7
Few-shot prompt (Greedy Decoding)
Codex Direct 175B 19.7 29.5 69.9 59.4 25.6 40.0 55.0 42.7
Codex CoT 175B 63.1 45.3 76.4 65.2 40.4 45.6 61.4 56.7
GPT-3 Direct 175B 15.6 24.8 65.7 57.1 14.4 29.1 37.9 34.9
GPT-3 CoT 175B 46.9 35.8 68.9 62.9 26.1 37.4 42.5 45.7
PaLM Direct 540B 17.9 25.2 69.4 - - - - -
PaLM CoT 540B 56.9 35.8 79.0 - - - - -
Codex CoT calc 175B 65.4 45.3 77.0 65.8 - - - -
GPT-3 CoT calc 175B 49.6 35.8 70.3 63.4 - - - -
PaLM CoT calc 540B 58.6 35.8 79.8 - - - - -
PoT-Codex 175B 71.6 54.1 85.2 73.2 64.5 64.6 69.0 68.9
Few-shot prompt (Self-Consistency Decoding)
LaMDA CoT-SC 137B 27.7 26.8 53.5 - - - - -
Codex CoT-SC 175B 78.0 52.0 86.8 75.4 44.4 47.9 63.2 63.9
PaLM CoT-SC 540B 74.4 48.3 86.6 - - - - -
PoT-SC-Codex 175B 80.0 58.6 89.1 81.8 68.1 67.3 70.2 73.6
Few-shot prompt (GPT-4)
CoT-GPT4 175B 92.0 72.4 97.0 - 58.2 - - -
PoT-GPT4 175B 97.2 84.4 97.4 - 74.0 - - -

Bảng 2: Kết quả few-shot cho các bộ dữ liệu khác nhau. Published SoTA bao gồm các kết quả tốt nhất đã biết (loại trừ kết quả thu được bởi GPT-4). Trên GSM8K, AQuA và SVAMP, kết quả SoTA trước đó là CoT + self-consistency decoding (Wang et al., 2022b). Trên FinQA, kết quả tốt nhất trước đó là từ Wang et al. (2022a). Trên ConvFinQA, kết quả tốt nhất trước đó được đạt bởi FinQANet (Chen et al., 2022). Trên TabWMP (Lu et al., 2022), kết quả tốt nhất trước đó được đạt bởi Dynamic Prompt Learning (Lu et al., 2022). Trên TATQA, kết quả SoTA là bởi RegHNT (Lei et al., 2022).

Mô hình #Params GSM8K AQuA SVAMP TabMWP MultiArith Avg
Zero-shot Direct (GPT-3) 175B 12.6 22.4 58.7 38.9 22.7 31.0
Zero-shot CoT (GPT-3) 175B 40.5 31.9 63.7 53.5 79.3 53.7
Zero-shot CoT (PaLM) 540B 43.0 - - - 66.1 -
Zero-shot PoT (Ours) 175B 57.0 43.9 70.8 66.5 92.2 66.1

Bảng 3: Kết quả zero-shot cho các bộ dữ liệu khác nhau. Kết quả baseline được lấy từ Kojima et al. (2022).

trên các bộ dữ liệu MWP với biên độ đáng chú ý. Trên các bộ dữ liệu tài chính, chúng tôi quan sát thấy rằng self-consistency decoding ít tác động hơn cho cả PoT và CoT. Tương tự, PoT + SC vượt trội CoT + SC khoảng 20% trên FinQA/ConvFinQA và 7% trên TATQA.

Kết quả Zero-shot Chúng tôi cũng đánh giá hiệu suất zero-shot của PoT và so sánh với Kojima et al. (2022) trong Bảng 3. Như có thể thấy, zero-shot PoT vượt trội đáng kể zero-shot CoT trên tất cả các bộ dữ liệu MWP được đánh giá. So với few-shot prompting, zero-shot PoT vượt trội zero-shot CoT (Kojima et al., 2022) với biên độ thậm chí lớn hơn. Trên các bộ dữ liệu được đánh giá, PoT vượt trội CoT trung bình 12%. Trên TabMWP, zero-shot PoT thậm chí cao hơn few-shot CoT. Những kết quả này cho thấy tiềm năng lớn để tổng quát hóa trực tiếp đến nhiều nhiệm vụ số học chưa thấy ngay cả mà không có bất kỳ ví dụ cụ thể cho bộ dữ liệu nào.

7

--- TRANG 8 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Mô hình #Params GSM8K SVAMP
code-davinci-002 175B 71.6 85.2
text-davinci-002 175B 60.4 80.1
gpt-3.5-turbo - 76.3 88.2
codegen-16B-multi 16B 8.2 29.2
codegen-16B-mono 16B 12.7 41.1
codeT5+ 16B 12.5 38.5
xgen 7B 11.0 40.6

Bảng 4: Hiệu suất PoT prompting với mô hình backend khác nhau.

2-shots 4-shots 6-shots 8-shots0.620.650.670.73
0.580.680.690.7
0.550.60.650.74v1v2v2

2-shots 4-shots 6-shots 8-shots0.630.66 0.66
0.62
0.580.63
0.620.64 0.64
0.620.64
0.62

Hình 5: Phân tích độ nhạy ví dụ cho GSM8K và FinQA, nơi v1, v2 và v3 là ba phiên bản của minh chứng k-shot được lấy mẫu từ pool.

3.3 Nghiên cứu Ablation

Chúng tôi thực hiện nhiều nghiên cứu ablation dưới thiết lập few-shot để hiểu tầm quan trọng của các yếu tố khác nhau trong PoT bao gồm các mô hình backbone, prompt engineering, v.v.

Backend Ablation Để hiểu hiệu suất của PoT trên các mô hình backbone khác nhau, chúng tôi so sánh hiệu suất của text-davinci-002, code-davinci-002, gpt-3.5-turbo, codegen-16B-mono, codegen-16B-multi, CodeT5+ và XGen. Chúng tôi chọn ba bộ dữ liệu đại diện GSM8K, SVAMP, và FinQA để phân tích kết quả. Chúng tôi trình bày kết quả thí nghiệm trong Bảng 4. Như có thể thấy, gpt-3.5-turbo có thể đạt được điểm số cao nhất để vượt trội codex (code-davinci-002) với biên độ đáng kể. Ngược lại, text-davinci-002 yếu hơn code-davinci-002, chủ yếu vì instruction tuning dựa trên văn bản sau đó làm suy yếu khả năng tạo mã của mô hình. Một thực tế đáng lo ngại mà chúng tôi thấy là mô hình mã nguồn mở như codegen Nijkamp et al. (2022) kém đáng kể trên các benchmark khác nhau. Chúng tôi phỏng đoán rằng khoảng cách lớn như vậy có thể được quy cho việc pre-training không đủ và kích thước mô hình.

Độ Nhạy với Ví dụ Để hiểu rõ hơn PoT nhạy như thế nào w.r.t các ví dụ khác nhau, chúng tôi tiến hành phân tích độ nhạy. Cụ thể, chúng tôi viết tổng cộng 20 ví dụ. Đối với k-shot learning, chúng tôi ngẫu nhiên lấy mẫu k = (2, 4, 6, 8) trong số 20 ví dụ ba lần là v1, v2, và v3. Chúng tôi sẽ sử dụng các ví dụ được lấy mẫu ngẫu nhiên này làm minh chứng cho PoT. Chúng tôi tóm tắt phân tích độ nhạy trong Hình 5. Trước hết, chúng tôi thấy rằng việc tăng số shots giúp ích nhiều hơn cho GSM8K so với FinQA. Điều này chủ yếu do tính đa dạng của các câu hỏi trong GSM8K. Bằng cách thêm nhiều ví dụ hơn, các mô hình ngôn ngữ có thể tổng quát hóa tốt hơn đến các câu hỏi đa dạng. Một quan sát khác là khi được cho ít ví dụ hơn, phương sai hiệu suất của PoT lớn hơn.

8

--- TRANG 9 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Khi K=2, phương sai hiệu suất có thể lớn đến 7% cho cả hai bộ dữ liệu. Với nhiều ví dụ hơn, hiệu suất trở nên ổn định hơn.

So sánh với PaL Chúng tôi cũng so sánh PoT với một cách tiếp cận liên quan gần đây khác như PaL (Gao et al., 2022). Theo Bảng 5, chúng tôi thấy rằng phương pháp của chúng tôi nói chung tốt hơn PaL, đặc biệt trên SVAMP và ASDIV. Kết quả của chúng tôi cao hơn 6% so với phương pháp prompting của họ.

Mô hình GSM8K GSM8K-Hard SVAMP ASDIV ADDSUB MULTIARITH
PaL 72.0 61.2 79.4 79.6 92.5 99.2
PoT 71.6 61.8 85.2 85.2 92.2 99.5

Bảng 5: So sánh PoT với công việc đương thời PaL (Gao et al., 2022).

Semantic Binding và Multi-Step Reasoning Hai thuộc tính cốt lõi của 'program of thoughts' là: (1) nhiều bước: chia nhỏ quá trình suy nghĩ thành chương trình từng bước, (2) semantic binding: gắn ý nghĩa ngữ nghĩa với tên biến. Để hiểu rõ hơn hai thuộc tính này đóng góp như thế nào, chúng tôi so sánh với hai biến thể. Một biến thể là loại bỏ semantic binding và đơn giản sử dụng a, b, c làm tên biến. Biến thể khác là dự đoán trực tiếp phương trình toán học cuối cùng để tính toán kết quả. Chúng tôi trình bày phát hiện trong Bảng 6. Như có thể thấy, việc loại bỏ binding nói chung sẽ làm tổn hại hiệu suất của mô hình. Trên các câu hỏi phức tạp hơn liên quan đến nhiều biến hơn như GSM8K, sự sụt giảm hiệu suất lớn hơn. Tương tự, việc nhắc LLMs tạo ra trực tiếp các phương trình mục tiêu cũng rất thách thức. Việc chia nhỏ phương trình mục tiêu thành nhiều bước lý luận giúp tăng hiệu suất.

Phương pháp GSM8K SVAMP FinQA
PoT 71.6 85.2 64.5
PoT - Binding 60.2 83.8 61.6
PoT - MultiStep 45.8 81.9 58.9

Bảng 6: So sánh giữa PoT và tạo phương trình trên ba bộ dữ liệu khác nhau.

Breakdown Analysis Chúng tôi thực hiện phân tích thêm để xác định những loại vấn đề nào CoT và PoT khác nhau nhất trong hiệu suất. Chúng tôi sử dụng AQuA (Ling et al., 2017) làm bàn thử nghiệm cho điều này. Cụ thể, chúng tôi phân loại thủ công các câu hỏi trong AQuA thành một số danh mục bao gồm hình học, đa thức, ký hiệu, số học, tổ hợp, phương trình tuyến tính, lặp và xác suất. Chúng tôi trình bày độ chính xác cho mỗi danh mục con trong Hình 6. Các danh mục chính là (1) phương trình tuyến tính, (2) số học, (3) tổ hợp, (4) xác suất, và (5) lặp. Những cải thiện lớn nhất của PoT là trong các danh mục 'phương trình tuyến tính/đa thức', 'lặp', 'ký hiệu', và 'tổ hợp'. Những câu hỏi này yêu cầu kỹ năng số học hoặc ký hiệu phức tạp hơn để giải quyết. Ngược lại, trên các câu hỏi 'số học', 'xác suất', và 'hình học', PoT và CoT có hiệu suất tương tự. Quan sát như vậy phản ánh giả định của chúng tôi rằng 'chương trình' hiệu quả hơn trên các vấn đề thách thức hơn.

geo poly symb arith comb lin-eq iter prob10506686
5072
65
38
20 205688
4062
40 40PoTCoT

Hình 6: Độ chính xác breakdown của PoT và CoT trên các loại câu hỏi khác nhau.

9

--- TRANG 10 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Hình 7: Các trường hợp lỗi trên tập dev TAT-QA sử dụng phương pháp PoT-greedy.

Error Analysis Chúng tôi xem xét hai loại lỗi: (1) lỗi value grounding, và (2) lỗi logic generation. Loại đầu tiên chỉ ra rằng mô hình không gán đúng giá trị cho các biến liên quan đến câu hỏi. Loại thứ hai chỉ ra rằng mô hình không tạo ra quá trình tính toán đúng để trả lời câu hỏi dựa trên các biến đã định nghĩa. Hình 7 cho thấy một ví dụ của mỗi loại lỗi. Trong ví dụ trên, mô hình lấy giá trị của các biến không chính xác trong khi logic tính toán là đúng. Trong ví dụ dưới, mô hình định vị các biến liên quan một cách chính xác nhưng không tạo ra logic tính toán thích hợp để trả lời câu hỏi. Chúng tôi kiểm tra thủ công các lỗi được tạo trong kết quả TAT-QA. Trong số 198 trường hợp thất bại của các câu hỏi lý luận số học với phương pháp PoT (greedy), 47% có lỗi value grounding và 33% có lỗi logic. Trong 15% cả hai loại lỗi đã xảy ra và trong 5% chúng tôi tin rằng câu trả lời thực sự đúng. Chúng tôi thấy rằng phần lớn các lỗi là lỗi value grounding, điều này cũng phổ biến cho các phương pháp khác như CoT.

4 Công việc Liên quan

4.1 Lý luận Toán học trong NLP

Kỹ năng lý luận toán học là thiết yếu cho các hệ thống thông minh đa năng, đã thu hút sự chú ý đáng kể từ cộng đồng. Trước đây, đã có các nghiên cứu trong việc hiểu khả năng của các mô hình NLP để giải quyết các câu hỏi số học/đại số (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Roy & Roth, 2015; Ling et al., 2017; Roy & Roth, 2018). Gần đây, các bộ dữ liệu thách thức hơn (Dua et al., 2019; Saxton et al., 2019; Miao et al., 2020; Amini et al., 2019; Hendrycks et al., 2021; Patel et al., 2021) đã được đề xuất để tăng độ khó, tính đa dạng hoặc thậm chí tính mạnh mẽ đối nghịch. LiLA (Mishra et al., 2022) đề xuất lắp ráp một tập lớn các bộ dữ liệu toán học thành một bộ dữ liệu thống nhất. LiLA cũng chú thích các chương trình Python làm mục tiêu tạo để giải quyết các vấn đề toán học. Tuy nhiên, LiLA (Mishra et al., 2022) chủ yếu tập trung vào thống nhất bộ dữ liệu. Công việc của chúng tôi nhằm hiểu cách tạo ra 'chương trình suy nghĩ' để gợi ra tốt nhất khả năng lý luận của LLM. Ngoài ra, chúng tôi cũng điều tra cách giải quyết các vấn đề toán học mà không có bất kỳ ví dụ nào. Austin et al. (2021) đề xuất đánh giá khả năng của LLMs để tổng hợp mã trên hai bộ dữ liệu được tuyển chọn MBPP và MathQA-Python.

10

--- TRANG 11 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

4.2 In-context Learning với LLMs

GPT-3 (Brown et al., 2020) đã chứng minh khả năng mạnh mẽ để thực hiện dự đoán few-shot, nơi mô hình được đưa ra mô tả nhiệm vụ bằng ngôn ngữ tự nhiên với một số ví dụ. Việc mở rộng kích thước mô hình, dữ liệu, và tính toán là quan trọng để kích hoạt khả năng học tập này. Gần đây, Rae et al. (2021); Smith et al. (2022); Chowdhery et al. (2022); Du et al. (2022) đã đề xuất đào tạo các loại LLMs khác nhau với các công thức đào tạo khác nhau. Khả năng tuân theo các ví dụ few-shot để giải quyết các nhiệm vụ chưa thấy không tồn tại trên các LMs nhỏ hơn, mà chỉ xuất hiện khi mô hình mở rộng (Kaplan et al., 2020). Gần đây, đã có một số công việc (Xie et al., 2021; Min et al., 2022) nhằm hiểu cách thức và lý do in-context learning hoạt động. Một công việc đồng thời khác tương tự với chúng tôi là BINDER (Cheng et al., 2022), áp dụng Codex để tổng hợp các truy vấn SQL 'mềm' để trả lời câu hỏi từ bảng.

4.3 Chain of Reasoning với LLMs

Mặc dù LLMs đã chứng minh thành công đáng kể trên một loạt các nhiệm vụ NLP, khả năng lý luận của chúng thường được coi là một hạn chế. Gần đây, CoT (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022b) đã được đề xuất để kích hoạt khả năng của LLM thực hiện các nhiệm vụ lý luận bằng cách chứng minh 'lý lẽ ngôn ngữ tự nhiên'. Suzgun et al. (2022) đã chỉ ra rằng CoT đã có thể vượt qua hiệu suất con người trên các nhiệm vụ BIG-Bench thách thức. Sau đó, một số công việc khác (Drozdov et al., 2022; Zhou et al., 2022; Nye et al., 2021) cũng đề xuất các cách tiếp cận khác nhau để sử dụng LLMs giải quyết các nhiệm vụ lý luận bằng cách cho phép các bước trung gian. ReAct Yao et al. (2022) đề xuất tận dụng các công cụ bên ngoài như search engine để tăng cường kỹ năng lý luận LLM. Phương pháp của chúng tôi có thể được coi là tăng cường CoT với các công cụ bên ngoài (Python) để kích hoạt lý luận số học mạnh mẽ. Một công việc đương thời khác (Gao et al., 2022) được đề xuất cùng thời gian với chúng tôi để áp dụng lý luận lai văn bản/mã để giải quyết các câu hỏi toán học.

4.4 Thảo luận về Công việc Đương thời

Gần đây, đã có một số công việc tiếp theo dựa trên PoT bao gồm self-critic (Gou et al., 2023), self-eval (Xie et al., 2023), plan-and-solve (Wang et al., 2023a). Các phương pháp này đề xuất tăng cường khả năng của LLMs giải quyết các vấn đề toán học với PoT. self-critic (Gou et al., 2023) và self-eval (Xie et al., 2021) đều áp dụng tự đánh giá để tăng cường tính mạnh mẽ của chương trình được tạo. plan-and-solve (Wang et al., 2023a) thay vào đó áp dụng hướng dẫn lập kế hoạch chi tiết hơn để giúp LLMs tạo ra một kế hoạch lý luận cấp cao. Các phương pháp này đều chứng minh mang lại cải thiện đáng kể so với PoT trên các bộ dữ liệu lý luận toán học khác nhau.

Một dòng công việc khác liên quan đến chúng tôi là Tool-use trong các mô hình transformer (Schick et al., 2023; Paranjape et al., 2023). Các công việc này đề xuất áp dụng các công cụ khác nhau để giúp các mô hình ngôn ngữ định vị trong thế giới bên ngoài. Các công việc này tổng quát hóa chương trình Python của chúng tôi thành các lời gọi API tổng quát hơn để bao gồm search engine, trích xuất chuỗi, v.v. Bằng cách tổng quát hóa, LLMs có thể mở khóa khả năng của nó để giải quyết các vấn đề lý luận và định vị phức tạp hơn trong các tình huống thế giới thực.

5 Thảo luận

Trong công việc này, chúng tôi đã xác minh rằng các phương pháp prompting của chúng tôi có thể hoạt động hiệu quả trên các nhiệm vụ lý luận số học như giải quyết vấn đề toán học hoặc tài chính. Chúng tôi cũng nghiên cứu cách kết hợp PoT với CoT để kết hợp các ưu điểm của cả hai cách tiếp cận prompting. Chúng tôi tin rằng PoT phù hợp cho các vấn đề yêu cầu kỹ năng lý luận ký hiệu cao. Đối với các nhiệm vụ lý luận ngữ nghĩa như lý luận thông thường (StrategyQA), chúng tôi phỏng đoán rằng PoT không phải là lựa chọn tốt nhất. Ngược lại, CoT có thể giải quyết các nhiệm vụ lý luận rộng hơn.

6 Kết luận

Trong công việc này, chúng tôi điều tra cách tách tính toán khỏi lý luận trong việc giải quyết các vấn đề số học. Bằng prompting 'program of thoughts', chúng tôi có thể gợi ra khả năng của LLMs tạo ra các chương trình chính xác để biểu đạt quy trình lý luận phức tạp, đồng thời cũng cho phép tính toán được xử lý riêng biệt bởi một trình thông dịch chương trình bên ngoài. Cách tiếp cận này có thể tăng hiệu suất của LLMs trên một số bộ dữ liệu toán học một cách đáng kể. Chúng tôi tin rằng công việc của chúng tôi có thể truyền cảm hứng cho nhiều công việc hơn để kết hợp thực thi ký hiệu với LLMs để đạt được hiệu suất tốt hơn trên các nhiệm vụ lý luận ký hiệu khác.

Hạn chế

Công việc của chúng tôi nhằm kết hợp LLM với thực thi ký hiệu để giải quyết các vấn đề toán học thách thức. PoT sẽ yêu cầu thực thi 'mã được tạo' từ LLMs, có thể chứa các đoạn mã nguy hiểm hoặc rủi ro nhất định như 'import os; os.rmdir()', v.v. Chúng tôi đã chặn LLM không import bất kỳ module bổ sung nào và hạn chế nó sử dụng các module được định nghĩa trước. Việc chặn bạo lực như vậy hoạt động hợp lý cho math QA, tuy nhiên, đối với các nhiệm vụ ký hiệu chưa biết khác, nó có thể làm tổn hại khả năng tổng quát hóa của PoT. Một hạn chế khác là PoT vẫn gặp khó khăn với bộ dữ liệu AQuA với các câu hỏi đại số phức tạp chỉ với 58% độ chính xác. Điều này chủ yếu do tính đa dạng của các câu hỏi trong AQuA, mà minh chứng không thể bao phủ được. Do đó, nghiên cứu tương lai nên thảo luận về cách nhắc thêm LLMs để tạo mã cho các câu hỏi Toán đa dạng cao.

Tài liệu tham khảo

[Phần tài liệu tham khảo được giữ nguyên tiếng Anh do là thông tin thư mục chuẩn]

Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Pro-
ceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 2357–2367, 2019.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 , 2021.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374 , 2021a.

Wenhu Chen. Large language models are few (1)-shot table reasoners. arXiv preprint arXiv:2210.06710 ,
2022.

Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa,
Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. Finqa: A dataset of numerical reasoning
over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing , pp. 3697–3711, 2021b.

Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. Convfinqa:
Exploring the chain of numerical reasoning in conversational finance question answering. arXiv preprint
arXiv:2210.03849 , 2022.

Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir
Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. arXiv
preprint arXiv:2210.02875 , 2022.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language
modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse,
and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,
2021.

12

--- TRANG 13 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier
Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint
arXiv:2209.15003 , 2022.

Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-
of-experts. In International Conference on Machine Learning , pp. 5547–5569. PMLR, 2022.

Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop:
A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers) , pp. 2368–2378, 2019.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham
Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435 , 2022.

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic:
Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738 ,
2023.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint
arXiv:2103.03874 , 2021.

Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve
arithmetic word problems with verb categorization. In EMNLP, pp. 523–533, 2014.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language
models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 , 2022.

Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Pars-
ingalgebraicwordproblemsintoequations. Transactions of the Association for Computational Linguistics ,
3:585–597, 2015.

Fangyu Lei, Shizhu He, Xiang Li, Jun Zhao, and Kang Liu. Answering numerical reasoning questions in
table-text hybrid contents with graph-based encoder and tree-based decoder. In Proceedings of the 29th
International Conference on Computational Linguistics , pp. 1379–1390, 2022.

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:
Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) , pp. 158–167, 2017.

Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and
Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.
arXiv preprint arXiv:2209.14610 , 2022.

Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english
math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computa-
tional Linguistics , pp. 975–984, 2020.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint
arXiv:2202.12837 , 2022.

13

--- TRANG 14 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpuro-
hit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for
mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing (EMNLP) , 2022.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. Codegen: Anopenlargelanguagemodelforcodewithmulti-turnprogramsynthesis. arXiv preprint
arXiv:2203.13474 , 2022.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for
intermediate computation with language models. arXiv preprint arXiv:2112.00114 , 2021.

OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.

Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and
Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv
preprint arXiv:2303.09014 , 2023.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word
problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies , pp. 2080–2094, Online, June 2021. Association
for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.
org/2021.naacl-main.168 .

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods,
analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.

Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing , pp. 1743–1752, 2015.

Subhro Roy and Dan Roth. Mapping to declarative knowledge for word problem solving. Transactions of
the Association for Computational Linguistics , 6:159–172, 2018.

David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning
abilities of neural models. arXiv preprint arXiv:1904.01557 , 2019.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv
preprint arXiv:2302.04761 , 2023.

Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,
Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and
megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint
arXiv:2201.11990 , 2022.

Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and
whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 , 2022.

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv
preprint arXiv:2201.08239 , 2022.

Bin Wang, Jiangzhou Ju, Yunlin Mao, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. A numerical reasoning
question answering system with fine-grained retriever and the ensemble of multiple generators for finqa.
arXiv preprint arXiv:2206.08506 , 2022a.

14

--- TRANG 15 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-
and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv
preprint arXiv:2305.04091 , 2023a.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves
chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022b.

Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+:
Opencodelargelanguagemodelsforcodeunderstandingandgeneration. arXiv preprint arXiv:2305.07922 ,
2023b.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of
thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 , 2022.

Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning
as implicit bayesian inference. In International Conference on Learning Representations , 2021.

Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition
enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633 , 2023.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Syner-
gizing reasoning and acting in language models. In NeurIPS 2022 Foundation Models for Decision Making
Workshop , 2022.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier
Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language
models. arXiv preprint arXiv:2205.10625 , 2022.

Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and
Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in
finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp.
3277–3287, 2021.

15

--- TRANG 16 ---
Xuất bản trong Transactions on Machine Learning Research (10/2023)

7 Phụ lục

7.1 PoT như bước trung gian

Chúng tôi chứng minh quy trình làm việc trong Hình 8.

Hình 8: Chúng tôi áp dụng PoT để nhắc các mô hình ngôn ngữ trước tiên tạo ra một câu trả lời trung gian và sau đó tiếp tục nhắc các mô hình lớn để tạo ra câu trả lời cuối cùng.

Chúng tôi viết mã giả như sau:
# Function PoT( Input ) −> Output
# Input : question
# Ouptut : program
# Function Prompt ( Input ) −> Output
# Input : question + intermediate
# Ouptut : answer
program = PoT(question)
exec(program)
i fisintance (ans , dict):
ans = l i s t(x. items ()).pop(0)
extra = 'according␣to␣the␣program:␣ '
extra += ans [0] + '␣=␣ ' + ans [1]
pred = Prompt(question + extra)
else:
pred = ans
return pred

PoT như bước trung gian có thể giải quyết các câu hỏi phức tạp hơn yêu cầu cả lý luận ký hiệu và thông thường.

7.2 Ví dụ cho Prompting

Để kích hoạt khả năng tái tạo tốt hơn, chúng tôi cũng đưa ra các prompts và ví dụ của chúng tôi cho bộ dữ liệu GSM8K và bộ dữ liệu AQuA trong các trang sau:

16

--- TRANG 17 ---
Câu hỏi: Vịt của Janet đẻ 16 quả trứng mỗi ngày. Cô ăn ba quả cho bữa sáng mỗi sáng và làm bánh muffin cho bạn bè hàng ngày với bốn quả. Cô bán số còn lại ở chợ nông sản hàng ngày với giá $2 mỗi quả trứng vịt tươi. Cô kiếm được bao nhiều đô la mỗi ngày ở chợ nông sản?
# Mã Python, trả về ans
total_eggs = 16
eaten_eggs = 3
baked_eggs = 4
sold_eggs = total_eggs - eaten_eggs - baked_eggs
dollars_per_egg = 2
ans = sold_eggs * dollars_per_egg

Câu hỏi: Một chiếc áo choàng cần 2 cuộn sợi xanh và một nửa số sợi trắng. Tổng cộng cần bao nhiêu cuộn?
# Mã Python, trả về ans
bolts_of_blue_fiber = 2
bolts_of_white_fiber = num_of_blue_fiber / 2
ans = bolts_of_blue_fiber + bolts_of_white_fiber

Câu hỏi: Josh quyết định thử lật một ngôi nhà. Anh mua một ngôi nhà với giá $80,000 và sau đó đầu tư $50,000 để sửa chữa. Điều này làm tăng giá trị của ngôi nhà lên 150%. Anh có bao nhiêu lợi nhuận?
# Mã Python, trả về ans
cost_of_original_house = 80000
increase_rate = 150 / 100
value_of_house = (1 + increase_rate) * cost_of_original_house
cost_of_repair = 50000
ans = value_of_house - cost_of_repair - cost_of_original_house

Câu hỏi: Mỗi ngày, Wendi cho mỗi con gà của mình ba chén thức ăn hỗn hợp, chứa hạt, giun và rau để giúp chúng khỏe mạnh. Cô cho gà ăn trong ba bữa riêng biệt. Vào buổi sáng, cô cho đàn gà 15 chén thức ăn. Vào buổi chiều, cô cho gà thêm 25 chén thức ăn. Cô cần cho gà bao nhiêu chén thức ăn trong bữa cuối cùng của ngày nếu đàn gà của Wendi có 20 con?
# Mã Python, trả về ans
numb_of_chickens = 20
cups_for_each_chicken = 3
cups_for_all_chicken = num_of_chickens * cups_for_each_chicken
cups_in_the_morning = 15
cups_in_the_afternoon = 25
ans = cups_for_all_chicken - cups_in_the_morning - cups_in_the_afternoon

Câu hỏi: Kylar đến cửa hàng để mua ly cho căn hộ mới của mình. Một chiếc ly có giá $5, nhưng mỗi chiếc ly thứ hai chỉ có giá 60% của giá gốc. Kylar muốn mua 16 chiếc ly. Anh cần trả bao nhiều tiền?
# Mã Python, trả về ans
num_glasses = 16
first_glass_cost = 5
second_glass_cost = 5 * 0.6
ans = 0
for i in range(num_glasses):
    if i % 2 == 0:
        ans += first_glass_cost
    else:
        ans += second_glass_cost

--- TRANG 18 ---
Câu hỏi: Marissa đang đi bộ đường mòn dài 12 dặm. Cô mất 1 giờ để đi 4 dặm đầu tiên, sau đó thêm 1 giờ nữa để đi 2 dặm tiếp theo. Nếu cô muốn tốc độ trung bình của mình là 4 dặm mỗi giờ, cô cần đi với tốc độ bao nhiêu (dặm mỗi giờ) cho quãng đường còn lại?
# Mã Python, trả về ans
average_mile_per_hour = 4
total_trail_miles = 12
remaining_miles = total_trail_miles - 4 - 2
total_hours = total_trail_miles / average_mile_per_hour
remaining_hours = total_hours - 2
ans = remaining_miles / remaining_hours

Câu hỏi: Carlos đang trồng một cây chanh. Cây sẽ có giá $90 để trồng. Mỗi năm nó sẽ cho 7 quả chanh, mà anh có thể bán với giá $1.5 mỗi quả. Chi phí $3 một năm để tưới và cho cây ăn. Mất bao nhiêu năm trước khi anh bắt đầu kiếm tiền từ cây chanh?
# Mã Python, trả về ans
total_cost = 90
cost_of_watering_and_feeding = 3
cost_of_each_lemon = 1.5
num_of_lemon_per_year = 7
ans = 0
while total_cost > 0:
    total_cost += cost_of_watering_and_feeding
    total_cost -= num_of_lemon_per_year * cost_of_each_lemon
    ans += 1

Câu hỏi: Khi Freda nấu cà chua đóng hộp thành nước sốt, chúng mất một nửa thể tích. Mỗi hộp cà chua 16 ounce mà cô sử dụng chứa ba quả cà chua. Lô nước sốt cà chua cuối cùng của Freda làm được 32 ounce nước sốt. Freda đã sử dụng bao nhiêu quả cà chua?
# Mã Python, trả về ans
lose_rate = 0.5
num_tomato_contained_in_per_ounce_sauce = 3 / 16
ounce_sauce_in_last_batch = 32
num_tomato_in_last_batch = ounce_sauce_in_last_batch * num_tomato_contained_in_per_ounce_sauce
ans = num_tomato_in_last_batch / (1 - lose_rate)

Câu hỏi: Jordan muốn làm bất ngờ mẹ cô với một chiếc bánh sinh nhật tự làm. Từ việc đọc hướng dẫn, cô biết sẽ mất 20 phút để làm bột bánh và 30 phút để nướng bánh. Bánh sẽ cần 2 giờ để nguội và thêm 10 phút để phủ kem. Nếu cô dự định làm bánh trong cùng một ngày, muộn nhất Jordan có thể bắt đầu làm bánh để sẵn sàng phục vụ lúc 5:00 chiều là mấy giờ?
# Mã Python, trả về ans
minutes_to_make_batter = 20
minutes_to_bake_cake = 30
minutes_to_cool_cake = 2 * 60
minutes_to_frost_cake = 10
total_minutes = minutes_to_make_batter + minutes_to_bake_cake + minutes_to_cool_cake + minutes_to_frost_cake
total_hours = total_minutes / 60
ans = 5 - total_hours

--- TRANG 19 ---
# Viết Mã Python để giải quyết các câu hỏi sau. Lưu kết quả của bạn dưới dạng biến có tên 'ans'.
from sympy import Symbol
from sympy import simplify
import math
from sympy import solve_it
# solve_it(equations, variable): giải các phương trình và trả về giá trị biến.

# Câu hỏi: Trong một chuyến bay 600 km, một máy bay bị chậm lại do thời tiết xấu. Tốc độ trung bình cho chuyến đi bị giảm 200 km/giờ và thời gian bay tăng 30 phút. Thời gian bay là:
# Lựa chọn trả lời: ['A)1 giờ', 'B)2 giờ', 'C)3 giờ', 'D)4 giờ', 'E)5 giờ']
duration = Symbol('duration', positive=True)
delay = 30 / 60
total_distance = 600
original_speed = total_distance / duration
reduced_speed = total_distance / (duration + delay)
solution = solve_it(original_speed - reduced_speed - 200, duration)
ans = solution[duration]

# Câu hỏi: M người đàn ông đồng ý mua một món quà với giá Rs. D. Nếu 3 người rút lui thì mỗi người sẽ phải đóng góp thêm bao nhiêu cho việc mua món quà?
# Lựa chọn trả lời: ['A)D/(M-3)', 'B)MD/3', 'C)M/(D-3)', 'D)3D/(M2-3M)', 'E)Không có đáp án nào']
M = Symbol('M')
D = Symbol('D')
cost_before_dropout = D / M
cost_after_dropout = D / (M - 3)
ans=simplify(cost_after_dropout - cost_before_dropout)

# Câu hỏi: Một khoản tiền với lãi suất đơn lên tới Rs. 815 trong 3 năm và Rs. 854 trong 4 năm. Số tiền ban đầu là:
# Lựa chọn trả lời: ['A)Rs. 650', 'B)Rs. 690', 'C)Rs. 698', 'D)Rs. 700', 'E)Không có đáp án nào']
deposit = Symbol('deposit', positive=True)
interest = Symbol('interest', positive=True)
money_in_3_years = deposit + 3 * interest
money_in_4_years = deposit + 4 * interest
solution = solve_it([money_in_3_years - 815, money_in_4_years - 854], [deposit, interest])
ans = solution[deposit]

# Câu hỏi: Tìm ra giá trị nào sau đây là bội số của X, nếu nó chia hết cho 9 và 12?
# Lựa chọn trả lời: ['A)36', 'B)15', 'C)17', 'D)5', 'E)7']
options = [36, 15, 17, 5, 7]
for option in options:
    if option % 9 == 0 and option % 12 == 0:
        ans = option
        break

--- TRANG 20 ---
# Câu hỏi: 35% nhân viên của một công ty là nam giới. 60% nam giới trong công ty nói tiếng Pháp và 40% nhân viên của công ty nói tiếng Pháp. Bao nhiêu % phụ nữ trong công ty không nói tiếng Pháp?
# Lựa chọn trả lời: ['A)4%', 'B)10%', 'C)96%', 'D)90.12%', 'E)70.77%']
num_women = 65
men_speaking_french = 0.6 * 35
employees_speaking_french = 0.4 * 100
women_speaking_french = employees_speaking_french - men_speaking_french
women_not_speaking_french=num_women - women_speaking_french
ans = women_not_speaking_french / num_women

# Câu hỏi: Trong một giờ, một chiếc thuyền đi 11 km/giờ theo dòng nước và 5 km/giờ ngược dòng nước. Tốc độ của thuyền trong nước tĩnh (km/giờ) là:
# Lựa chọn trả lời: ['A)4 kmph', 'B)5 kmph', 'C)6 kmph', 'D)7 kmph', 'E)8 kmph']
boat_speed = Symbol('boat_speed', positive=True)
stream_speed = Symbol('stream_speed', positive=True)
along_stream_speed = 11
against_stream_speed = 5
solution = solve_it([boat_speed + stream_speed - along_stream_speed, boat_speed - stream_speed - against_stream_speed], [boat_speed, stream_speed])
ans = solution[boat_speed]

# Câu hỏi: Sự khác biệt giữa lãi suất đơn và lãi kép cùng lãi suất cho Rs.5000 trong 2 năm là Rs.72. Lãi suất là bao nhiêu?
# Lựa chọn trả lời: ['A)10%', 'B)12%', 'C)6%', 'D)8%', 'E)4%']
interest_rate = Symbol('interest_rate', positive=True)
amount = 5000
amount_with_simple_interest = amount * (1 + 2 * interest_rate / 100)
amount_with_compound_interest = amount * (1 + interest_rate / 100) ** 2
solution = solve_it(amount_with_compound_interest - amount_with_simple_interest - 72, interest_rate)
ans = solution[interest_rate]

# Câu hỏi: Diện tích của một hình chữ nhật là 15 cm vuông và chu vi là 16 cm. Kích thước của hình chữ nhật là gì?
# Lựa chọn trả lời: ['A)2&4', 'B)3&5', 'C)4&6', 'D)5&7', 'E)6&8']
width = Symbol('width', positive=True)
height = Symbol('height', positive=True)
area = 15
perimeter = 16
solution = solve_it([width * height - area, 2 * (width + height) - perimeter], [width, height])
ans = (solution[width], solution[height])