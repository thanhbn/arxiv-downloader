# LongT5: Mô hình Transformer Văn bản-đến-Văn bản Hiệu quả cho Chuỗi Dài

Mandy Guoy, Joshua Ainsliey, David Uthus, Santiago Ontañón
Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang
Google Research
{xyguo, jainslie, duthus, santiontanon, jianmon, yhsung, yinfeiy}@google.com

## Tóm tắt

Công trình gần đây đã chỉ ra rằng việc (1) tăng độ dài đầu vào hoặc (2) tăng kích thước mô hình có thể cải thiện hiệu suất của các mô hình neural dựa trên Transformer. Trong bài báo này, chúng tôi trình bày LongT5, một mô hình mới khám phá hiệu ứng của việc mở rộng quy mô cả độ dài đầu vào và kích thước mô hình cùng một lúc. Cụ thể, chúng tôi tích hợp các ý tưởng về attention từ các transformer đầu vào dài (ETC), và áp dụng các chiến lược huấn luyện trước từ huấn luyện trước tóm tắt (PEGASUS) vào kiến trúc T5 có thể mở rộng quy mô. Kết quả là một cơ chế attention mới mà chúng tôi gọi là Transient Global (TGlobal), mô phỏng cơ chế attention cục bộ/toàn cục của ETC, nhưng không yêu cầu các đầu vào phụ bổ sung. Chúng tôi có thể đạt được kết quả tối ưu trên một số tác vụ tóm tắt và trả lời câu hỏi, cũng như vượt trội hơn các mô hình T5 gốc trên những tác vụ này. Chúng tôi đã mở mã nguồn kiến trúc và mã huấn luyện, cũng như các checkpoint mô hình được huấn luyện trước của chúng tôi.

## 1 Giới thiệu

Các mô hình Transformer như BERT (Devlin et al., 2019), và các biến thể khác (Liu et al., 2019; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020) đã đạt được kết quả tối ưu trên nhiều tác vụ NLP đầy thử thách. Hơn nữa, công trình gần đây trong các transformer đầu vào dài (Ainslie et al., 2020; Zaheer et al., 2020b; Beltagy et al., 2020; Tay et al., 2021) đã chỉ ra rằng việc tăng độ dài đầu vào mà một Transformer có thể xử lý dẫn đến những cải thiện hiệu suất thêm. Ngoài ra, cũng được biết rằng việc tăng kích thước mô hình cũng dẫn đến những cải thiện hiệu suất trong nhiều tác vụ (Kaplan et al., 2020).

Trong bài báo này, chúng tôi trình bày một mô hình mới, được gọi là LongT5, với đó chúng tôi khám phá hiệu ứng của việc mở rộng quy mô cả độ dài đầu vào và kích thước mô hình cùng một lúc. Để đạt được điều này, chúng tôi tích hợp attention transformer đầu vào dài và các ý tưởng huấn luyện trước vào kiến trúc mô hình T5 có thể mở rộng quy mô (Raffel et al., 2019a). Mô hình kết quả, như được thể hiện trong Hình 1, đạt được hiệu suất tối ưu trên một số tác vụ yêu cầu xử lý đầu vào chuỗi dài.

Về attention, chúng tôi thiết kế một cơ chế attention mới, mà chúng tôi gọi là Transient Global (TGlobal), mô phỏng cơ chế cục bộ/toàn cục của ETC (Ainslie et al., 2020). Quan trọng là, attention TGlobal loại bỏ nhu cầu về các đầu vào phụ bổ sung trong ETC, để phù hợp với kiến trúc T5. Ý tưởng chính của cơ chế cục bộ/toàn cục của ETC là giới thiệu sự thưa thớt cục bộ trong cơ chế attention để giảm chi phí bậc hai khi mở rộng quy mô đến đầu vào dài. Cụ thể, ETC chỉ cho phép các token trong đầu vào (được gọi là đầu vào dài) chú ý đến một vùng lân cận cục bộ, và thêm một đầu vào thứ cấp được gọi là bộ nhớ toàn cục, qua đó các token trong đầu vào dài có thể chú ý đến nhau một cách gián tiếp.

Một nhược điểm của cơ chế này là nó yêu cầu thiết kế đầu vào toàn cục thứ cấp này cho mỗi vấn đề mới. Để thích ứng nó với T5, cơ chế TGlobal mới của chúng tôi tổng hợp những token toàn cục này một cách động (như tập hợp của các nhóm token trong đầu vào), tại mỗi lớp attention. Các thí nghiệm của chúng tôi cho thấy rằng cơ chế này chỉ dẫn đến suy giảm hiệu suất nhỏ so với attention đầy đủ ở cùng độ dài đầu vào nhưng cho phép mô hình mở rộng quy mô đến độ dài đầu vào lớn hơn nhiều, dẫn đến những cải thiện hiệu suất đáng kể.

Về huấn luyện trước, chúng tôi áp dụng chiến lược huấn luyện trước trong mô hình PEGASUS (Zhang et al., 2019a). Chiến lược huấn luyện trước này ban đầu được thiết kế cho tóm tắt trừu tượng, nhưng trong các thí nghiệm của chúng tôi, chúng tôi nhận thấy nó cũng cải thiện hiệu suất mô hình cho các tác vụ khác, như trả lời câu hỏi, và do đó chúng tôi đã áp dụng nó trong LongT5. Ý tưởng chính là che khuất các câu quan trọng (chính) từ một tài liệu và yêu cầu mô hình tái tạo chúng dưới dạng một chuỗi duy nhất, như thể đó là một bản tóm tắt.

Chúng tôi đánh giá LongT5 trên một số tác vụ tóm tắt và trả lời câu hỏi (xem Phần 4.2.1 và 4.3.1 để biết mô tả chi tiết về các bộ dữ liệu này). Nhờ việc mở rộng quy mô cả độ dài đầu vào và kích thước mô hình, chúng tôi đạt được kết quả tối ưu trên nhiều trong số chúng.

Những đóng góp chính của công trình này là:
• Một kiến trúc Transformer mới, LongT5, cho phép mở rộng quy mô cả độ dài đầu vào và quy mô mô hình cùng một lúc.
• Một cơ chế attention mới (TGlobal), mô phỏng cơ chế cục bộ/toàn cục của ETC nhưng là một thay thế trực tiếp cho attention thông thường cho các kiến trúc Transformer hiện có như T5.
• Một phân tích về hiệu suất mô hình khi thay đổi cả độ dài đầu vào và kích thước mô hình của các mô hình T5 gốc và LongT5 (đẩy cả hai mô hình lên đến độ dài tối đa mà chúng có thể xử lý trước khi gặp vấn đề về bộ nhớ), để hiểu sự đánh đổi trong cả hiệu suất và chi phí tính toán.
• Kết quả tối ưu trên các bộ dữ liệu arXiv, PubMed, BigPatent, MediaSum, và TriviaQA. Đối với Natural Questions, chúng tôi đã sử dụng một công thức hóa hơi khác so với các tác vụ gốc, và do đó chúng tôi không đưa ra tuyên bố về kết quả tối ưu.
• Chúng tôi mở mã nguồn kiến trúc mô hình và mã huấn luyện, cũng như các checkpoint mô hình được huấn luyện trước trên GitHub.

## 2 T5

T5 (Raffel et al., 2019a) là một mô hình ngôn ngữ được huấn luyện trước dựa trên transformer văn bản-đến-văn bản đang ngày càng phổ biến nhờ framework thống nhất của nó chuyển đổi tất cả các vấn đề ngôn ngữ dựa trên văn bản thành định dạng văn bản-đến-văn bản, và khả năng dễ dàng mở rộng quy mô về số lượng tham số (từ 60M đến 11B tham số) với song song hóa mô hình. Với transformer attention đầy đủ, T5 đã được áp dụng thành công cho nhiều tác vụ NLP, nhưng các tác vụ chỉ yêu cầu chuỗi đầu vào ngắn hơn. Điều này là do hạn chế của sự tăng trưởng tính toán bậc hai đối với độ dài chuỗi đầu vào, dẫn đến tiêu thụ bộ nhớ lớn hơn và thời gian huấn luyện dài hơn. Gần đây, Press et al. (2021) đã khám phá việc mở rộng quy mô các mô hình kiểu T5 tại thời điểm suy luận đến các chuỗi dài hơn so với những gì được thấy trong quá trình huấn luyện, nhưng cách mở rộng quy mô các mô hình kiểu T5 trong độ dài chuỗi đầu vào trong quá trình huấn luyện vẫn chưa được khám phá đầy đủ.

## 3 LongT5

### 3.1 Kiến trúc

Chúng tôi mở rộng encoder T5 gốc với các mẫu thưa thớt attention toàn cục-cục bộ (Ainslie et al., 2020; Zaheer et al., 2020a) để xử lý đầu vào dài. Đối với công việc được báo cáo trong bài báo này, chúng tôi đã sử dụng một decoder T5 tiêu chuẩn vì tất cả các tác vụ mà chúng tôi xem xét đều yêu cầu độ dài chuỗi đầu ra tương đối ngắn.

Về mặt kiến trúc, sự khác biệt chính giữa T5 và LongT5 nằm ở cơ chế attention. Chúng tôi thử nghiệm với hai biến thể cơ chế attention cho LongT5, được minh họa trong Hình 2: (1) Local Attention và (2) Transient Global Attention (TGlobal). Cả hai biến thể đều bảo tồn một số thuộc tính của T5: biểu diễn vị trí tương đối, hỗ trợ đóng gói ví dụ, và tương thích với các checkpoint T5.

#### 3.1.1 Local Attention

Đối với Local Attention, chúng tôi đơn giản thay thế phép toán self-attention encoder trong T5 bằng một phép toán attention cục bộ cửa sổ trượt thưa thớt theo triển khai trong ETC (Ainslie et al., 2020). Cụ thể, đối với một bán kính cục bộ r cho trước, công thức này chỉ cho phép mỗi token chú ý đến r token ở bên trái và bên phải của nó (xem Hình 2.a). Chúng tôi nhận thấy r = 127 là đủ trong thực tế, trong đó r là số lượng token lân cận ở bên trái và bên phải.

Local Attention không giới thiệu bất kỳ tham số mới nào và dễ dàng phù hợp với việc che khuất attention cần thiết cho đóng gói ví dụ. Đối với một lựa chọn r cho trước, độ phức tạp là tuyến tính theo độ dài chuỗi đầu vào l: O(lr).

#### 3.1.2 Transient Global Attention (TGlobal)

Để cho phép các token đầu vào tương tác với nhau trong mỗi lớp của encoder ở phạm vi dài hơn so với bán kính cục bộ của Local Attention, chúng tôi giới thiệu Transient Global Attention như một sửa đổi của attention toàn cục-cục bộ của ETC trong mẫu "khối cố định". Cụ thể, chúng tôi chia chuỗi đầu vào thành các khối k token, và cho mỗi khối chúng tôi tính toán một token toàn cục bằng cách cộng (và sau đó chuẩn hóa) các embedding của mỗi token trong khối (xem Hình 2.b).

Bây giờ khi tính toán attention, chúng tôi cho phép mỗi token đầu vào chú ý không chỉ đến các token gần đó như trong Local Attention, mà còn đến mọi token toàn cục. Chúng tôi gọi những token toàn cục này là tạm thời vì trái ngược với các mẫu attention toàn cục-cục bộ giống ETC, những token này được xây dựng động (và sau đó bị loại bỏ) trong mỗi phép toán attention, loại bỏ bất kỳ yêu cầu nào về việc quyết định token đầu vào nào nên được coi là "toàn cục".

TGlobal attention chỉ giới thiệu một vài tham số mới: (1) bias vị trí tương đối kiểu T5 biểu diễn khoảng cách từ khối của một token đầu vào đến khối của mỗi token toàn cục mà nó đang chú ý đến, và (2) tham số chuẩn hóa lớp kiểu T5 để chuẩn hóa embedding của mỗi token toàn cục. Phần còn lại của các tham số giống hệt với T5, và chúng tôi phù hợp với đóng gói chuỗi bằng cách che khuất attention từ các token đầu vào đến các token toàn cục của các ví dụ khác một cách bổ sung. Chúng tôi nhận thấy kích thước khối k = 16 là đủ trong thực tế. Chú ý do đó, TGlobal attention giới thiệu một khối l/k cặp attention key-value bổ sung để tính toán trên Local Attention (l token đầu vào, chú ý đến l/k token toàn cục; được biểu diễn bằng hình chữ nhật ngoài cùng bên phải trong Hình 2.b), do đó đối với độ dài chuỗi đầu vào l, độ phức tạp là O(l(r + l/k)).

### 3.2 Huấn luyện trước Tạo ra Câu Chính PEGASUS

T5 được huấn luyện trước với mục tiêu tham nhũng span, trong đó các span của các token đầu vào liên tiếp được thay thế bằng một token che khuất và mô hình được huấn luyện để tái tạo các token bị che khuất. Mặc dù nó hiệu quả, công trình gần đây về mô hình hóa ngôn ngữ che khuất (MLM) (Liu et al., 2019; Zhang et al., 2019b) cho thấy rằng việc lựa chọn cẩn thận mục tiêu dự đoán có thể dẫn đến hiệu suất tốt hơn đáng kể. Một lập luận là việc dự đoán các token thông tin hơn từ văn bản có thể buộc mô hình học ngữ nghĩa tốt hơn của văn bản. Được thúc đẩy bởi điều đó, chúng tôi khám phá việc che khuất và tạo ra các câu chính từ văn bản. Cụ thể, chúng tôi áp dụng chiến lược Gap Sentences Generation với Principle Ind-Uniq từ Zhang et al. (2019a), được sử dụng cho huấn luyện trước tóm tắt.

Theo Zhang et al. (2019a), chúng tôi chọn các câu được đánh điểm cao nhất top-m (Principle) dựa trên điểm ROUGE-F1 (Lin, 2004) sử dụng si = rouge(xi; D\{xi}; ∀i), trong đó i là chỉ số câu, D là tập hợp các câu trong tài liệu. Mỗi câu được đánh điểm độc lập (Ind), và mỗi n-gram chỉ được đếm một lần (Uniq).

## 4 Thí nghiệm

### 4.1 Cấu hình

LongT5 được triển khai sử dụng JAX và thư viện Flaxformer. Theo cùng thiết lập như T5.1.1, chúng tôi xem xét các mô hình với 3 kích thước: base (220M), large (770M), và xl (3B), và sử dụng cùng mô hình vocab SentencePiece tiếng Anh có chữ hoa-chữ thường được sử dụng bởi T5.1.1, chứa 32000 sentence piece. Chúng tôi sử dụng kích thước batch 128 và Adafactor làm trình tối ưu hóa trong tất cả các thí nghiệm. Chúng tôi quyết định sử dụng giải mã tham lam thay vì tìm kiếm beam cho tất cả các thí nghiệm của chúng tôi ngay cả với các tập thử nghiệm, do đó, kết quả của chúng tôi được báo cáo dưới đây có thể được cải thiện thêm bằng cách sử dụng tìm kiếm beam, nhưng chúng tôi muốn làm cho thiết lập nhất quán với thiết lập dev của chúng tôi.

#### 4.1.1 Huấn luyện trước

Chúng tôi huấn luyện trước các mô hình LongT5 trong 1M bước trên độ dài chuỗi đầu vào 4096 và độ dài chuỗi đầu ra 910. Chúng tôi sử dụng cùng lịch trình tốc độ học căn bậc hai nghịch đảo như T5, với tốc độ học được đặt thành 1/√max(step; warm_up_steps), trong đó warm_up_steps được đặt thành 10000. Giống như T5.1.1, chúng tôi huấn luyện trước LongT5 chỉ trên bộ dữ liệu C4 (Raffel et al., 2019b), và chúng tôi không áp dụng dropout trong quá trình huấn luyện trước. Như được mô tả trong phần 3.2, chúng tôi sử dụng mục tiêu Tạo ra Câu Chính PEGASUS làm mục tiêu huấn luyện trước của chúng tôi. Cấu hình tương tự như những gì được mô tả bởi Zhang et al. (2019a) cho các mô hình lớn hơn của họ, ngoại trừ tỷ lệ câu bị che khuất mà chúng tôi sử dụng giá trị 0.2 thay vì 0.45. Trong phần 5.3, chúng tôi sẽ trình bày nghiên cứu loại bỏ của chúng tôi giữa Tạo ra Câu Chính và Tham nhũng Span.

#### 4.1.2 Tinh chỉnh

Để tinh chỉnh, chúng tôi sử dụng tốc độ học không đổi 0.001 và tỷ lệ dropout 0.1 cho tất cả các tác vụ. Đối với các tác vụ tóm tắt, chúng tôi thử nghiệm với các giá trị 4096, 8192, và 16384 cho độ dài đầu vào và 512 cho độ dài đầu ra. Đối với các tác vụ QA, chúng tôi thử nghiệm với các giá trị bắt đầu từ 512 và mở rộng quy mô lên đến 36864 cho độ dài đầu vào và 128 cho độ dài đầu ra.

### 4.2 Đánh giá trên các Tác vụ Tóm tắt

Chúng tôi chọn đánh giá chuẩn các mô hình của chúng tôi trên các tác vụ tóm tắt bao gồm các độ dài ngữ cảnh khác nhau, vì bản chất hiểu ngữ cảnh dài và tạo ra của chúng.

#### 4.2.1 Bộ dữ liệu

LongT5 được đánh giá chuẩn trên sáu bộ dữ liệu sau.

**CNN / Daily Mail** (Nallapati et al., 2016) Tin tức từ CNN và Daily Mail được sử dụng làm đầu vào và các điểm tóm tắt của bài báo là bản tóm tắt mục tiêu.

**PubMed** (Cohan et al., 2018) Các tài liệu khoa học được thu thập từ PubMed, với nội dung tài liệu được sử dụng làm đầu vào và tóm tắt tương ứng của nó làm bản tóm tắt mục tiêu.

**arXiv** (Cohan et al., 2018) Tương tự như PubMed, nhưng với các tài liệu được lấy từ arXiv.

**BigPatent** (Sharma et al., 2019) Các tài liệu bằng sáng chế Hoa Kỳ, với chi tiết bằng sáng chế được sử dụng làm đầu vào và tóm tắt bằng sáng chế làm bản tóm tắt mục tiêu.

**MediaSum** (Zhu et al., 2021) Bản ghi phỏng vấn từ CNN và NPR được sử dụng làm đầu vào và chủ đề và tổng quan tương ứng của chúng được sử dụng làm bản tóm tắt mục tiêu.

**Multi-News** (Fabbri et al., 2019) Tác vụ liên quan đến việc tóm tắt nhiều tài liệu tin tức về một chủ đề thành một bản tóm tắt được viết bởi con người.

Bảng 1 cung cấp thống kê về số lượng ví dụ trong các phần train, validation, và test, và độ dài chuỗi đầu vào trung bình, trung vị, tối đa, và phần trăm thứ 90. Như có thể thấy, các bộ dữ liệu này dài về độ dài đầu vào, và sẽ được hưởng lợi từ các mô hình có thể mô hình hóa đầu vào dài hơn. Chúng tôi bao gồm bộ dữ liệu CNN / Daily Mail để đánh giá chuẩn trên một tác vụ phổ biến, đặc biệt để xem việc sử dụng attention TGlobal ảnh hưởng đến mô hình như thế nào, mặc dù độ dài của đầu vào nhỏ hơn các bộ dữ liệu khác.

#### 4.2.2 Kết quả

Chúng tôi so sánh LongT5 với các phương pháp hàng đầu khác nhau: BigBird-PEGASUS (Zaheer et al., 2020b), HAT-BART (Rohde et al., 2021), DANCER PEGASUS (Gidiotis và Tsoumakas, 2020), PRIMER (Xiao et al., 2021), TG-MultiSum (Cui và Hu, 2021), LED (Beltagy et al., 2020), và một ứng dụng của BART bởi Zhu et al. (2021). Đối với những so sánh này, chúng tôi sử dụng các chỉ số đánh giá phổ biến của ROUGE-1, ROUGE-2, và ROUGE-L.

Như có thể thấy trong Bảng 2, LongT5 có thể đạt được điểm rouge tối ưu cho arXiv, PubMed, BigPatent, và MediaSum. Đối với arXiv và PubMed, được cấu thành từ các đầu vào dài hơn, việc có thể mở rộng quy mô lên đến độ dài đầu vào 16k giúp LongT5 đạt được kết quả mạnh.

Một bộ dữ liệu mà LongT5 không thể đạt được kết quả tối ưu là với Multi-News. LongT5 là mô hình tốt thứ 2, kém hơn PRIMER một chút. Điều này có thể hiểu được vì mô hình PRIMER được huấn luyện trước trên một kho tài liệu lớn liên quan đến các sự kiện tin tức, do đó tiếp xúc mô hình với một kho tương tự như những gì được thấy trong Multi-News.

Khi nhìn vào CNN / Daily Mail, chúng tôi có thể thấy rằng LongT5 có thể so sánh với HAT-BART, mặc dù không có attention đầy đủ. LongT5 ít nhất đã có điểm số mạnh hơn trong chỉ số ROUGE-2.

### 4.3 Đánh giá trên các Tác vụ QA

Để đánh giá trên các tác vụ QA, chúng tôi chọn hai benchmark phổ biến, Natural Questions và TriviaQA, yêu cầu hiểu ngữ cảnh dài.

#### 4.3.1 Bộ dữ liệu

**NaturalQuestions (NQ)** Các câu hỏi là truy vấn thực được đưa ra bởi nhiều người dùng cho tìm kiếm Google truy xuất một trang Wikipedia trong năm kết quả tìm kiếm hàng đầu. Văn bản trả lời được rút ra từ kết quả tìm kiếm (Kwiatkowski et al., 2019).

Bộ dữ liệu NQ gốc yêu cầu các mô hình dự đoán một câu trả lời ngắn (bao gồm không có câu trả lời hoặc có/không) và một câu trả lời dài. Chúng tôi đã khung hóa tác vụ như một tác vụ seq2seq và bỏ qua câu trả lời dài. Do đó, kết quả của chúng tôi chỉ tập trung vào câu trả lời ngắn. Hơn nữa, vì các mô hình của chúng tôi dự đoán văn bản trả lời thay vì span trả lời, phương pháp đánh giá của chúng tôi khác một chút so với các bảng xếp hạng, và kết quả của chúng tôi không thể so sánh trực tiếp với các phương pháp hiện có khác: (1) Vì chỉ có các tập train và dev được công khai, chúng tôi sử dụng 90% tập train chính thức để huấn luyện trong khi sử dụng 10% làm tập dev giữ lại để tinh chỉnh các siêu tham số và epoch huấn luyện, và sử dụng tập dev chính thức làm tập test của chúng tôi. (2) Chúng tôi đánh giá chuẩn LongT5 so với các mô hình T5.1.1 tương ứng thay vì so sánh trực tiếp với các bảng xếp hạng.

**TriviaQA** Các người đam mê trivia đã tác giả các cặp câu hỏi-trả lời. Câu trả lời được rút ra từ kết quả tìm kiếm web Wikipedia và Bing, loại trừ các trang web trivia (Joshi et al., 2017).

Chúng tôi sử dụng các phần train/validation chính thức để huấn luyện và tinh chỉnh các siêu tham số và epoch huấn luyện, sau đó huấn luyện lại mô hình đó kết hợp cả tập train và validation để đánh giá trên miền Wikipedia trên bảng xếp hạng.

Bảng 3 hiển thị thống kê bộ dữ liệu về số lượng ví dụ trong các phần train và validation, và độ dài chuỗi đầu vào trung bình, trung vị, tối đa, và phần trăm thứ 90.

#### 4.3.2 Kết quả

Bảng 4 hiển thị tóm tắt kết quả cho các bộ dữ liệu NQ và TriviaQA (xem Phụ lục B để biết kết quả đầy đủ). Đối với mỗi bộ dữ liệu, chúng tôi hiển thị hai chỉ số: EM (Exact Match) và điểm F1 (đánh giá độ chính xác và độ phủ của từng từ trong câu trả lời so với sự thật cơ sở, bỏ qua stop word).

Đối với NQ, chúng tôi so sánh T5.1.1, LongT5 với Local Attention, và LongT5 với TGlobal attention. Chúng tôi quyết định chạy T5.1.1 (1) với độ dài chuỗi đầu vào mặc định 512 và (2) với độ dài chuỗi đầu vào lớn nhất có thể vừa vào bộ nhớ thiết bị, và sử dụng những cái đó làm baseline. Vì chúng tôi đang so sánh với T5.1.1, đối với các thí nghiệm LongT5, chúng tôi báo cáo kết quả ở độ dài đầu vào 512 cho base và large, và độ dài đầu vào lớn nhất được cho phép bởi mỗi mô hình trước khi hết bộ nhớ trên cùng cấu hình phần cứng được sử dụng trong các thí nghiệm T5.1.1 của chúng tôi.

Như bảng cho thấy, việc tăng độ dài đầu vào nói chung dẫn đến những lợi ích đáng kể trong NQ, với các mô hình có độ dài đầu vào lớn hơn vượt trội đáng kể so với những mô hình có độ dài đầu vào nhỏ hơn trong hầu hết các trường hợp. Đôi khi, các mô hình với độ dài đầu vào lớn nhất có hiệu suất kém hơn những mô hình với độ dài 4k, nhưng chúng tôi tin rằng đó là do nhiễu trong các thí nghiệm, vì kết quả là đầu ra của chỉ một lần lặp lại của mỗi thí nghiệm do hạn chế tài nguyên. Hơn nữa, trong khi LongT5 với Local Attention thường có hiệu suất kém hơn T5.1.1, LongT5 với TGlobal attention vượt trội đáng kể so với T5.1.1. Ví dụ, xem xét các mô hình kích thước large, T5.1.1 chỉ có thể mở rộng quy mô lên đến độ dài đầu vào 3k token, trong khi mô hình TGlobal có thể đạt 6k token, vượt trội hơn T5.1.1 ở độ dài 4k token (có một sự sụt giảm ở độ dài 6k token, nhưng chúng tôi giả thuyết đây chỉ là do phương sai, vì chúng tôi chỉ thực hiện một lần chạy cho mỗi cấu hình).

Đối với TriviaQA, chúng tôi so sánh LongT5 với các phương pháp hàng đầu khác nhau trên bảng xếp hạng: BigBird-ETC (Zaheer et al., 2020a), Fusion-in-Decoder (Izacard và Grave, 2021), và ReadTwice (Zemlyanskiy et al., 2021). Như được hiển thị trong Bảng 3, đầu vào TriviaQA khá dài, do đó việc có thể mở rộng quy mô cả về kích thước mô hình và đến độ dài đầu vào 16k giúp LongT5 đạt được kết quả tối ưu.

## 5 Phân tích

### 5.1 Độ dài Đầu vào so với Tốc độ

Để đánh giá tốc độ huấn luyện và tiêu thụ bộ nhớ của LongT5, so với T5.1.1, chúng tôi đã thực hiện một loạt các lần chạy huấn luyện trong bộ dữ liệu NQ bắt đầu từ độ dài đầu vào 512, và tăng độ dài đầu vào đều đặn cho đến khi các mô hình hết bộ nhớ trên một slice 4x8 TPUv3. Kết quả được hiển thị trong Hình 3, so sánh 6 cấu hình mô hình khác nhau: T5.1.1 base, T5.1.1 large, LongT5 (base Local), LongT5 (large Local), LongT5 (base TGlobal), và LongT5 (large TGlobal). Đối với mỗi cấu hình mô hình, chúng tôi hiển thị một đường cong vẽ số chuỗi mỗi giây được xử lý trong quá trình huấn luyện (tốc độ, trong trục dọc) cho mỗi độ dài đầu vào (trục ngang). Cả hai trục đều được hiển thị ở thang logarit.

Chúng tôi có thể thấy rằng ở độ dài ngắn hơn (512), T5.1.1, LongT5 Local, LongT5 TGlobal có tốc độ tương tự, nhưng khi chúng tôi tăng độ dài chuỗi, LongT5 trở nên nhanh hơn đáng kể. Ví dụ ở độ dài chuỗi 2048, T5.1.1 base chỉ có thể xử lý 479 chuỗi mỗi giây, trong khi LongT5 (base TGlobal) có thể xử lý 765 và LongT5 (base Local) có thể xử lý 860. Sự khác biệt thậm chí còn lớn hơn khi độ dài chuỗi tăng.

Một sự thật quan trọng khác mà Hình 3 cho thấy là các mô hình T5.1.1 đạt điểm hết bộ nhớ sớm hơn nhiều. Ví dụ, chúng tôi chỉ có thể mở rộng quy mô lên đến 6k token cho T5.1.1 base. Mặt khác, LongT5 (base Local) có thể lên đến 36k token về độ dài, và LongT5 (base TGlobal) lên đến 12k. Các mô hình large hiển thị một bức tranh tương tự với T5.1.1 large chỉ lên đến 3k, nhưng các biến thể LongT5 lên đến 10k (large Local) và 6k (large TGlobal).

### 5.2 Độ dài Đầu vào so với Hiệu suất

Phần này trình bày một phân tích tương tự, nhưng ở đó chúng tôi vẽ tốc độ mô hình so với hiệu suất trong NQ (điểm F1). Kết quả được hiển thị trong Hình 4 cho các mô hình với kích thước large. Mỗi điểm trong các đường cong được chú thích với độ dài chuỗi tương ứng.

Như Hình 4 cho thấy, hiệu suất tăng đáng kể khi độ dài đầu vào tăng, nhấn mạnh những lợi ích của LongT5. Hơn nữa, chỉ riêng độ dài đầu vào là không đủ để đạt được hiệu suất tốt trong tất cả các bộ dữ liệu, và cụ thể, trong bộ dữ liệu NQ (được sử dụng trong hình này), việc sử dụng Local Attention làm tổn hại đáng kể đến hiệu suất khi so sánh với TGlobal hoặc với T5.1.1. Vì vậy, ngay cả ở độ dài đầu vào rất dài, LongT5 với Local Attention chỉ khớp với T5.1.1 với độ dài đầu vào 3k trong NQ. Tuy nhiên, LongT5 với TGlobal attention vượt trội hơn T5.1.1. Hơn nữa, lưu ý rằng mặc dù biểu đồ hiển thị một vài bất thường (như độ dài 8k cho LongT5 với Local Attention, hoặc độ dài 6k với TGlobal Attention), đó là vì biểu đồ chỉ hiển thị kết quả của một lần chạy duy nhất, và do đó có một số nhiễu. Tuy nhiên, xu hướng có thể được thấy rõ ràng.

### 5.3 Tạo ra Câu Chính so với Tham nhũng Span

Như đã đề cập trong phần 3.2, chúng tôi sử dụng Tạo ra Câu Chính PEGASUS thay vì Tham nhũng Span mặc định được sử dụng trong T5 làm mục tiêu huấn luyện trước của chúng tôi. Bảng 5 hiển thị nghiên cứu loại bỏ của chúng tôi để tinh chỉnh trên NQ và arXiv từ một mô hình được huấn luyện trước sử dụng mục tiêu Tham nhũng Span mặc định, một mô hình được huấn luyện trước với Tạo ra Câu Chính, và một mô hình được huấn luyện trước với cả hai mục tiêu. So sánh được thực hiện trên tập dev của các tác vụ, và với các mô hình TGlobal base. Cả huấn luyện trước và tinh chỉnh trên các mô hình được đề cập ở trên đều được thực hiện với độ dài chuỗi đầu vào 4096. Bảng cho thấy, mặc dù Tạo ra Câu Chính được phát triển bởi Zhang et al. (2019a) như một chiến lược huấn luyện trước cho tóm tắt, nó có lợi cho cả tác vụ tóm tắt và QA, nhưng việc sử dụng cả hai mục tiêu cùng nhau có hiệu suất kém hơn chỉ sử dụng PSG.

Bảng 6 hiển thị một nghiên cứu loại bỏ bổ sung với arXiv và PubMed, nơi chúng tôi so sánh việc sử dụng T5.1.1 thông thường với Tham nhũng Span so với T5.1.1 được huấn luyện trước với Tạo ra Câu Chính trong khi sử dụng cùng độ dài chuỗi đầu vào huấn luyện trước 512 (như đã được thực hiện trong tác vụ huấn luyện trước T5.1.1 gốc). Như mong đợi, Tạo ra Câu Chính đã giúp mô hình đạt được kết quả tốt hơn so với Tham nhũng Span khi thấy cùng lượng dữ liệu huấn luyện trước. Chúng tôi cũng so sánh điều này với điểm số dev từ LongT5 với TGlobal attention ở độ dài đầu vào 4k và 16k, sao cho chúng tôi có thể thấy việc có attention đầy đủ sẽ cho phép kết quả tốt hơn, nhưng việc có thể mở rộng quy mô đến độ dài chuỗi đầu vào dài hơn cho phép LongT5 đạt được kết quả mạnh hơn của nó.

## 6 Công trình Liên quan

Huấn luyện trước mô hình ngôn ngữ tiếp theo là tinh chỉnh cụ thể cho tác vụ đã được chứng minh là một công cụ mạnh mẽ cho nhiều tác vụ NLP (Devlin et al., 2019; Liu et al., 2019; Zhang et al., 2019b; Radford et al., 2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi et al., 2020). BERT (Devlin et al., 2019) đã giới thiệu Mask Language Model (MLM), nơi một mô hình dự đoán các token bị che khuất cho một chuỗi đầu vào văn bản. Tinh chỉnh một mô hình BERT được huấn luyện trước đã dẫn đến hiệu suất được cải thiện trên các tác vụ NLP khác nhau. Tuy nhiên, các dự đoán MLM không được thực hiện một cách tự hồi quy, điều này hạn chế khả năng của họ BERT cho các tác vụ tạo ra. Raffel et al. (2019a) đã giới thiệu tác vụ tham nhũng span trong T5 làm mục tiêu huấn luyện trước, nơi một mô hình dự đoán span token bị che khuất sử dụng một mô hình tự hồi quy. Nó có thể xử lý các tác vụ tạo ra vì huấn luyện trước được thực hiện theo cách tạo ra. BART (Lewis et al., 2020) tương tự như T5 nhưng đã sử dụng một mục tiêu huấn luyện trước hơi khác, trong đó các span được che khuất khỏi đầu vào nhưng đầu ra hoàn chỉnh được dự đoán. Tuy nhiên, không có công trình nào trong số này đã cố gắng điều tra huấn luyện trước cho đầu vào chuỗi rất dài. Họ thường sử dụng kiến trúc transformer (Vaswani et al., 2017) làm xương sống, độ phức tạp của nó là bậc hai đối với độ dài đầu vào, làm cho chúng không thực tế để mô hình đầu vào chuỗi rất dài.

**Mô hình hóa văn bản dài** Một lượng công việc rộng lớn cũng đã được thực hiện để mô hình hóa văn bản dài như tài liệu. Công trình từ Roy et al. (2016); Chen (2017); Wu et al. (2018) đã có được embedding tài liệu từ embedding cấp từ. Một hướng nghiên cứu khác cố gắng mô hình tài liệu dài thông qua huấn luyện phân cấp. Công trình từ Yang et al. (2016); Miculicich et al. (2018) đã sử dụng Hierarchical Attention Networks cho phân loại tài liệu và dịch máy neural, và Guo et al. (2019) đã đề xuất sử dụng một mạng phân cấp để xây dựng embedding tài liệu trên đầu embedding câu cho khai thác tài liệu song song.

Nghiên cứu gần đây hơn đã tập trung vào việc cải thiện hiệu quả bộ nhớ và tính toán của các mô hình transformer (Tay et al., 2020b, 2021) để xử lý đầu vào dài. Một loại phương pháp như vậy là sử dụng các mẫu attention không đầy đủ để hạn chế phạm vi trường attention, để nó giảm độ phức tạp attention từ O(n²) xuống O(n log n) hoặc O(n), bao gồm Sinkhorn (Tay et al., 2020a), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), và BigBird (Zaheer et al., 2020a). Một loại phương pháp khác là tận dụng xấp xỉ hạng thấp của ma trận attention, như Linformer (Wang et al., 2020), Performer (Choromanski et al., 2021), Random Feature Attention (Peng et al., 2021), và LUNA (Ma et al., 2021).

## 7 Kết luận

Bài báo này đã trình bày một mô hình neural dựa trên Transformer mới được gọi là LongT5, với đó chúng tôi đã khám phá hiệu ứng của việc mở rộng quy mô cả độ dài đầu vào và kích thước mô hình cùng một lúc. Cụ thể, những khác biệt chính của LongT5 so với T5.1.1 là (1) một cơ chế attention có thể mở rộng quy mô mới được gọi là Transient Global attention, là một thay thế trực tiếp cho cơ chế attention T5 tiêu chuẩn, và do đó có thể được sử dụng mà không cần các đầu vào phụ bổ sung cho mô hình hoặc sửa đổi đối với đầu vào mô hình; và (2) sử dụng mục tiêu huấn luyện trước Tạo ra Câu Chính kiểu PEGASUS.

Thông qua thí nghiệm trong một số bộ dữ liệu tóm tắt và trả lời câu hỏi đầy thử thách, chúng tôi đã khám phá những cải thiện hiệu suất có thể đạt được bằng cách mở rộng quy mô cả độ dài đầu vào và kích thước mô hình, dẫn đến kết quả tối ưu trên một số bộ dữ liệu: arXiv, PubMed, BigPatent, MediaSum, và TriviaQA.

Như một phần của công việc tương lai của chúng tôi, chúng tôi muốn theo đuổi một số hướng như nghiên cứu các cơ chế attention hiệu quả trong decoder và các phần attention decoder-to-encoder của mô hình (cả Local Attention và TGlobal attention chỉ được áp dụng cho encoder trong LongT5 hiện tại). Ngoài ra, chúng tôi muốn kết hợp các ý tưởng transformer đầu vào dài bổ sung vào kiến trúc LongT5, có thể cải thiện hiệu quả mô hình thêm nữa.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên như trong bản gốc vì chúng là tên riêng và tiêu đề công trình khoa học]

## A Kết quả Tóm tắt

Bảng 8 hiển thị tập hợp đầy đủ các kết quả trên các bộ dữ liệu tóm tắt được sử dụng trong bài báo này. Điều này bao gồm cả mô hình T5 tiêu chuẩn (sử dụng phiên bản T5.1.1), T5 với huấn luyện trước Tạo ra Câu Chính PEGASUS, và mô hình LongT5.

Như có thể thấy, việc mở rộng quy mô kích thước đầu vào cho các mô hình giúp đạt được chỉ số hiệu suất tốt hơn. Tuy nhiên, các mô hình T5 gặp khó khăn khi mở rộng quy mô lên 4k cho đầu vào, vì tác vụ tinh chỉnh có thể mất nhiều ngày ngay cả khi sử dụng một topology lớn của TPUv3.

Khi so sánh mô hình T5.1.1 thông thường với mô hình T5.1.1 sử dụng huấn luyện trước Tạo ra Câu Chính PEGASUS, mô hình sau có thể đạt được kết quả tốt hơn, với kết quả cũng được cải thiện khi kích thước đầu vào được mở rộng quy mô. Điều này giúp cho thấy rằng cả việc sử dụng mục tiêu huấn luyện trước sau này cùng với việc mở rộng quy mô cho phép chúng tôi có được kết quả tốt nhất từ những mô hình này.

LongT5, mặc dù có attention giảm từ việc sử dụng attention TGlobal, có thể có được kết quả hiệu suất mạnh do cả việc mở rộng quy mô lên đầu vào lớn hơn và tận dụng chiến lược huấn luyện trước Gap Sentences Generation.

## B Kết quả QA

Bảng 7 hiển thị tập hợp đầy đủ các kết quả so sánh các mô hình T5.1.1 và LongT5 trên các bộ dữ liệu QA được sử dụng trong bài báo này. Đối với cả NQ và TriviaQA trong nghiên cứu so sánh này, chúng tôi sử dụng 90% tập huấn luyện chính thức để huấn luyện trong khi sử dụng 10% làm tập dev giữ lại để tinh chỉnh các siêu tham số và epoch huấn luyện, và sử dụng tập dev chính thức để báo cáo các số trong bảng này. Chúng tôi chạy mỗi mô hình đến độ dài đầu vào lớn nhất được cho phép trước khi hết bộ nhớ trên cấu hình phần cứng cụ thể - các mô hình base/large trên 4x8 TPUv3 không có phân vùng mô hình, và các mô hình xl trên 8x16 TPUv3 với 8 phân vùng.

[Các bảng số liệu được giữ nguyên format như bản gốc]
