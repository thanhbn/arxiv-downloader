# 2308.10882.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2308.10882.pdf
# Kích thước tệp: 706198 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Giraffe: Những cuộc phiêu lưu trong việc mở rộng độ dài ngữ cảnh trong LLM
Arka Pal∗, Deep Karkhanis, Manley Roberts,
Samuel Dooley, Arvind Sundararajan, Siddartha Naidu
Abacus.AI
Tóm tắt
Các mô hình ngôn ngữ lớn hiện đại (LLM) dựa trên cơ chế attention thường được huấn luyện với độ dài ngữ cảnh cố định, điều này áp đặt giới hạn trên cho độ dài của các chuỗi đầu vào mà chúng có thể xử lý tại thời điểm đánh giá. Để sử dụng các mô hình này trên các chuỗi dài hơn độ dài ngữ cảnh thời gian huấn luyện, người ta có thể sử dụng các kỹ thuật từ họ phương pháp ngoại suy độ dài ngữ cảnh đang phát triển — hầu hết trong số đó tập trung vào việc sửa đổi hệ thống mã hóa vị trí được sử dụng trong cơ chế attention để chỉ ra vị trí của các token hoặc activation trong chuỗi đầu vào. Chúng tôi tiến hành một khảo sát rộng về các phương pháp hiện có về ngoại suy độ dài ngữ cảnh trên mô hình LLaMA hoặc LLaMA 2 cơ sở, và cũng giới thiệu một số thiết kế của riêng chúng tôi — đặc biệt, một chiến lược cắt ngắn mới để sửa đổi cơ sở cho mã hóa vị trí.
Chúng tôi kiểm tra các phương pháp này bằng cách sử dụng ba nhiệm vụ đánh giá mới (FreeFormQA, AlteredNumericQA, và LongChat-Lines) cũng như perplexity, mà chúng tôi thấy là một thước đo ít chi tiết hơn về hiệu suất ngữ cảnh dài của LLM. Chúng tôi phát hành ba nhiệm vụ này công khai dưới dạng bộ dữ liệu trên HuggingFace. Chúng tôi phát hiện ra rằng linear scaling là phương pháp tốt nhất để mở rộng độ dài ngữ cảnh, và cho thấy rằng có thể đạt được những cải thiện thêm bằng cách sử dụng các tỷ lệ dài hơn tại thời gian đánh giá. Chúng tôi cũng phát hiện khả năng ngoại suy đầy hứa hẹn trong cơ sở bị cắt ngắn. Để hỗ trợ nghiên cứu thêm trong lĩnh vực này, chúng tôi phát hành ba mô hình ngữ cảnh dài 13B tham số mới mà chúng tôi gọi là Giraffe: các mô hình ngữ cảnh 4k và 16k được huấn luyện từ LLaMA-13B cơ sở, và một mô hình ngữ cảnh 32k được huấn luyện từ LLaMA2-13B cơ sở. Chúng tôi cũng phát hành mã để sao chép kết quả của chúng tôi.¹
1 Giới thiệu
Trong những năm gần đây, transformer [1] đã trở thành kiến trúc mạng nơ-ron thống trị trong nhiều nhiệm vụ mô hình hóa ngôn ngữ tự nhiên [2, 3], nhờ tính linh hoạt và khả năng thích ứng để được huấn luyện trên các bộ dữ liệu cực kỳ lớn [4, 5]. Sau đó, một thuật ngữ phổ biến đã được áp dụng cho các mạng nơ-ron này là 'Mô hình Ngôn ngữ Lớn' (LLM) — với 'Lớn' đề cập đến cả kích thước bộ dữ liệu huấn luyện cũng như số lượng tham số của chúng (và thực sự, chi phí huấn luyện và môi trường liên quan).
Một yếu tố chính của kiến trúc transformer tiêu chuẩn là tính không nhạy cảm vốn có đối với thứ tự của các phần tử đầu vào. Attention về bản chất là một phép toán giống như tập hợp trong đó vị trí của các phần tử không quan trọng [1]. Tuy nhiên, thứ tự của các phần tử là rất quan trọng đối với nhiều nhiệm vụ quan trọng như phân tích ngôn ngữ tự nhiên, mã hóa, dự báo, v.v. Do đó, cần thiết phải tiêm thông tin vị trí vào các đầu vào của LLM, thường dưới dạng mã hóa vị trí.
Một tiêu chí mong muốn có thể có của một sơ đồ mã hóa vị trí là ngoại suy độ dài ngữ cảnh: khả năng sử dụng LLM để suy luận trên các độ dài đầu vào dài hơn những gì nó đã được huấn luyện. Do sự tăng trưởng độ phức tạp bậc hai của cơ chế attention trong transformer, thường không khả thi để huấn luyện trên các độ dài ngữ cảnh lớn. Lợi ích của việc tăng độ dài ngữ cảnh là đa dạng - cho phép đọc các tài liệu và bài báo dài hơn,
∗Liên hệ tại arka@abacus.ai .
¹Github repo tại: https://github.com/abacusai/Long-Context .
1arXiv:2308.10882v1  [cs.AI]  21 Aug 2023

--- TRANG 2 ---
tính nhất quán nội bộ nhiều hơn trong các cuộc trò chuyện dài với người dùng trong các chatbot được hỗ trợ bởi LLM, làm việc trên các codebase lớn hơn, v.v. Chúng ta có thể chia ngoại suy độ dài ngữ cảnh thành hai paradigm chính. Đầu tiên, có ngoại suy tinh chỉnh nơi một mô hình đã được pretrain trước đó trên các ngữ cảnh ngắn hơn được phép tinh chỉnh, hoặc cập nhật trọng số mô hình dựa trên độ dài ngữ cảnh dài hơn. Ngoài ra, có ngoại suy zero-shot nơi một mô hình đã được pretrain trước đó trên các ngữ cảnh ngắn được đánh giá ngay lập tức trên các độ dài ngữ cảnh dài hơn với cùng trọng số như mô hình ngữ cảnh ngắn hơn.
Trong bài báo này, chúng tôi chủ yếu tập trung vào ngoại suy zero-shot và đưa ra các đóng góp chính sau:
Benchmark của các sơ đồ ngoại suy ngữ cảnh khác nhau Chúng tôi tiến hành một khảo sát về các phương pháp ngoại suy độ dài ngữ cảnh với mô hình cơ sở đã được pretrain, và thử một vài phát minh của riêng chúng tôi. Đặc biệt, chúng tôi trình bày một cơ sở bị cắt ngắn mới cho mã hóa vị trí. Trọng tâm trong bài báo này về các mô hình đã pretrain cũng khác với các công trình khác trong văn học [6, 7], có xu hướng thay vào đó huấn luyện từ đầu với một sơ đồ mã hóa vị trí được chọn. Như đã đề cập ở trên, mặc dù LLM đã thành công, việc huấn luyện chúng là một doanh nghiệp tốn kém. Các mô hình nguồn đóng nổi tiếng bao gồm GPT-4 [8] và Claude [9]. Gần đây LLaMA [10] mã nguồn mở đã được phát hành bởi một nhóm tại Meta AI, và điều này được theo sau bởi LLaMA2 [11] được cải thiện. Theo quan điểm của chúng tôi, các tài nguyên cần thiết để huấn luyện các mô hình cơ sở cạnh tranh có bản chất này sẽ vẫn bị hạn chế đối với một vài đối thủ lớn. Do đó, điều quan trọng là có thể sửa đổi các mô hình như mong muốn cho người dùng cuối—lý tưởng nhất, với một phần nhỏ của sức mạnh tính toán được áp dụng.
Những phát hiện chính của chúng tôi là:
•Linear interpolation là tốt nhất như một phương pháp ngoại suy độ dài ngữ cảnh.
•Tất cả các phương pháp ngoại suy độ dài ngữ cảnh đều cho thấy sự suy giảm về độ chính xác nhiệm vụ, ngay cả đối với các độ dài mà chúng cung cấp đầu ra mạch lạc khác (và điểm perplexity vẫn hợp lý).
•Việc tăng độ dài ngữ cảnh thêm có thể đạt được bằng cách sử dụng hệ số tỷ lệ cao hơn tại thời gian đánh giá so với thời gian tinh chỉnh, nhưng có vẻ chỉ lên đến hệ số 2x.
Phát hành công khai trọng số LLM và bộ dữ liệu đánh giá Chúng tôi phát hành trọng số của hai mô hình 13B mới được huấn luyện từ LLaMA cơ sở với độ dài ngữ cảnh mở rộng là 16k² và độ dài ngữ cảnh là 4k³ trên HuggingFace. Chúng tôi cũng phát hành một mô hình 13B được huấn luyện đến độ dài 32k từ LLaMA 2 cơ sở⁴. Chúng tôi gọi họ mô hình này là Giraffe. Ngoài ra, chúng tôi phát hành ba bộ dữ liệu (LongChat-Lines⁵, FreeFormQA⁶ và AlteredNumericQA⁷) để đánh giá hiệu suất ngữ cảnh dài của những mô hình này và các mô hình khác. LongChat-Lines là một nhiệm vụ truy xuất chi tiết key-value. FreeFormQA và AlteredQA là các bộ dữ liệu trả lời câu hỏi dựa trên Bộ dữ liệu Natural Questions [12]. Một số công việc hiện có [6, 7] chỉ tập trung vào perplexity trên một tập đánh giá corpus tài liệu như thước đo hiệu suất ngoại suy của họ. Chúng tôi thấy rằng điểm perplexity không phải là một thước đo nhạy cảm về hiệu suất ngữ cảnh dài như các nhiệm vụ được giới thiệu của chúng tôi.
2 Công việc liên quan
RoPE Trong công việc này, chúng tôi kiểm tra hiệu quả của việc chọn mã hóa vị trí của LLaMA [10] cho các độ dài ngữ cảnh dài hơn so với mô hình cơ sở đã được huấn luyện. Mã hóa vị trí được sử dụng bởi LLaMA là RoPE (Rotary Position Embedding) [13]. RoPE hoạt động bằng cách xoay các lát cắt của ma trận chiếu query và key với tốc độ khác nhau. Ví dụ, ngay cả khi query và key được chiếu cùng một mã hóa, chúng sẽ được xoay theo các lượng khác nhau tùy thuộc vào vị trí của chúng trong chuỗi. Nếu sau đó chúng không được căn chỉnh, tích vô hướng của chúng sẽ nhỏ hơn so với những gì nó sẽ là nếu chúng không được xoay chút nào. Ngược lại, chúng
²https://huggingface.co/abacusai/Giraffe-v1-delta-13b-scaled-16
³https://huggingface.co/abacusai/Giraffe-v1-delta-13b-scaled-4
⁴https://huggingface.co/abacusai/Giraffe-v2-13b-32k
⁵https://huggingface.co/datasets/abacusai/LongChat-Lines
⁶https://huggingface.co/datasets/abacusai/WikiQA-Free_Form_QA
⁷https://huggingface.co/datasets/abacusai/WikiQA-Altered_Numeric_QA
2

--- TRANG 3 ---
có thể trở nên được căn chỉnh hơn, dẫn đến tích vô hướng lớn hơn và điểm attention. Trong RoPE, việc xoay này đang xảy ra với tốc độ khác nhau trên tất cả 2-lát cắt của query và key trong chiều embedding, cho phép mô hình xây dựng một hàm phức tạp của điểm attention trên các khoảng cách. Một trong những sự hấp dẫn chính của việc sử dụng phương pháp RoPE là nó đảm bảo về mặt toán học rằng hàm điểm attention chỉ phụ thuộc vào khoảng cách tương đối giữa query và key, chứ không phải vị trí tuyệt đối của chúng. Điều này được coi là một thuộc tính mong muốn của LLM [13, 14].
ALiBi Mặc dù RoPE đã thành công trong mục tiêu này, công việc về ALiBi [6] đã chứng minh rằng RoPE không thể thực hiện ngoại suy độ dài ngữ cảnh zero-shot. Bài báo ALiBi cho thấy rằng RoPE nhanh chóng bị suy giảm khi nó được kiểm tra trên các độ dài ngữ cảnh dài hơn so với mô hình đã thấy trong quá trình huấn luyện; nó cũng giới thiệu phương án thay thế được đề xuất riêng của nó đã cho thấy khả năng ngoại suy vượt trội trên các benchmark của họ. Tuy nhiên, ALiBi có những thiếu sót riêng; việc sử dụng các hàm tuyến tính đơn giản để điều chế điểm attention trên khoảng cách có nghĩa là nó không thể biểu diễn các hàm attention-khoảng cách phức tạp như cơ sở Fourier của RoPE. Ngoài ra, ALiBi sử dụng một hàm duy nhất như vậy mỗi head, làm giảm thêm sức mạnh biểu đạt. Điều này có thể giải thích tại sao, mặc dù ALiBi có ngoại suy, các mô hình sử dụng nó có hiệu suất tồi tệ hơn so với các mô hình dựa trên RoPE trên các benchmark như MMLU [2] và LMSys arena đo lường sở thích của con người [15].
xPos Sun et al. [7] kiểm tra tại sao RoPE không ngoại suy thành công và xác định rằng điều này là do tác động của các thành phần tần số cao gây ra nhiễu dư trong điểm attention ngay cả khi các token cách xa nhau. Họ cố gắng giải quyết điều này bằng cách thêm một thuật ngữ biên độ suy giảm theo cấp số nhân vào RoPE. Phương pháp mới này, gọi là xPos, làm suy giảm các thành phần tần số cao ồn ào này nhanh hơn các thành phần tần số thấp. Phương pháp này cho thấy kết quả tốt về huấn luyện từ đầu của LLM [7], và trực giác thúc đẩy nó phù hợp với các giả thuyết riêng của chúng tôi về sự thiếu hụt của RoPE. Tuy nhiên, Sun et al. không thử nghiệm trong bối cảnh quan tâm của chúng tôi: lấy một mô hình đã pretrain với RoPE và xem liệu nó có thể được dụ dỗ (thông qua tinh chỉnh hạn chế) để học mã hóa xPos thay thế. Hơn nữa, các thử nghiệm của họ chứng minh rằng Blockwise Causal Attention là cần thiết để họ đạt được ngoại suy.
Linear Scaling/Positional Interpolation Kỹ thuật ngoại suy độ dài ngữ cảnh đơn giản nhưng hiệu quả này được báo cáo đồng thời bởi kaiokendev [16] và bởi một nhóm tại Meta [17]. Phương pháp được sử dụng ở đây là chỉ cần chia vector vị trí cho hệ số tỷ lệ phù hợp với đầu vào trong độ dài ngữ cảnh của mô hình gốc. Trực giác của kỹ thuật này là sử dụng khả năng nội suy của LLM, thay vì dựa vào ngoại suy. Đó là một hiện tượng được biết đến rộng rãi rằng mạng nơ-ron có xu hướng nội suy trong một phạm vi các giá trị đã thấy trước đó tốt hơn so với việc ngoại suy ngoài phạm vi đó (ví dụ [18]). Trong trường hợp cụ thể của mã hóa vị trí, [17] tuyên bố rằng nội suy vị trí tránh nguy cơ bùng nổ số lượng lớn trong giá trị attention liên quan đến ngoại suy. Chúng tôi thực hiện nhiều thử nghiệm về sơ đồ này và các biến thể của nó và báo cáo kết quả trong bài báo này.
Randomized Positional Encodings Ruoss et al trình bày phương pháp này trong [19]. Trong quá trình huấn luyện, họ tạo ngẫu nhiên vector vị trí của họ bằng cách rút N mẫu nhiều từ phạm vi [1, L] đồng nhất mà không thay thế, trong đó N là độ dài ngữ cảnh huấn luyện và L là một giá trị lớn lớn hơn độ dài ngữ cảnh đánh giá tối đa (được giả định là đã biết trước). Các vị trí được lấy mẫu này sau đó được sắp xếp theo thứ tự tăng dần và hoạt động như đầu vào vị trí mà mô hình thấy tại thời gian đánh giá. Trong quá trình đánh giá, đầu vào vị trí [1, ..., M] được đưa cho mô hình. Các tác giả tuyên bố cải thiện hiệu suất về ngoại suy độ dài ngữ cảnh. Chúng tôi độc lập đến một ý tưởng tương tự với bài báo này nhưng thay vào đó ngẫu nhiên hóa bằng cách rút từ các vị trí sub-integer xấp xỉ trong phạm vi [1, N]; xem Phần 4 để biết thêm chi tiết. Chúng tôi cũng lưu ý rằng Ruoss et al điều tra việc sử dụng một sơ đồ như vậy để huấn luyện LLM từ đầu, trong khi chúng tôi chủ yếu quan tâm đến việc tinh chỉnh post-hoc một LLM đã pretrain với ngẫu nhiên hóa để cho phép ngoại suy độ dài ngữ cảnh.
3

--- TRANG 4 ---
3 Đánh giá Ngoại suy Ngữ cảnh Dài
Câu hỏi chính được đặt ra trong bài báo này xoay quanh việc mở rộng khả năng độ dài ngữ cảnh của LLM. Để đánh giá điều này, một thước đo thường được sử dụng trong văn học là perplexity [6, 7, 13]. Tuy nhiên, như chúng tôi cho thấy trong Phần 5.3, perplexity có phần thô thiển để đánh giá mức độ tốt mà mô hình có thể sử dụng các cửa sổ ngữ cảnh dài hơn. Trực giác của chúng tôi là — trong nhiều bộ dữ liệu ngôn ngữ tự nhiên — một điểm perplexity hợp lý có thể đạt được ngay cả khi mô hình chỉ chú ý đến thông tin trong một phạm vi hạn chế (512 token cuối cùng, chẳng hạn) của cửa sổ ngữ cảnh. Ví dụ, một sơ đồ mã hóa vị trí chỉ đơn giản che các phần tử nào đó của ngữ cảnh (và key và query activation bên trong trong các head attention) lớn hơn về độ dài so với những gì mô hình đã được huấn luyện sẽ thành công trong việc đạt được điểm perplexity hợp lý, nhưng sẽ làm kém trên các nhiệm vụ chúng tôi mô tả dưới đây.
Chúng tôi mở rộng công việc hiện có để xem xét độ chính xác của mô hình khi được trình bày với các vấn đề có câu trả lời có thể xác minh. Trong việc sử dụng thước đo này, chúng tôi có thể đánh giá cách mô hình sử dụng thông tin ngữ cảnh bổ sung để phản hồi các lời nhắc. Chúng tôi dựa vào hai loại nhiệm vụ đánh giá để đánh giá khả năng của các mô hình trong việc trích xuất và sử dụng thông tin từ các ngữ cảnh đầu vào dài: đầu tiên là các nhiệm vụ truy xuất key-value và loại khác là các nhiệm vụ trả lời câu hỏi. Bằng cách sử dụng hai loại nhiệm vụ này, chúng tôi thực thi yêu cầu của mô hình phải chú ý đến toàn bộ ngữ cảnh để có được độ chính xác cao. Chúng tôi coi nhiệm vụ truy xuất là một thử nghiệm thuần túy hơn về truy xuất thông tin không có nhiều thiên lệch ngôn ngữ tự nhiên. Tuy nhiên, nhiệm vụ truy xuất là một cấu trúc hơi nhân tạo mà LLM có thể sẽ không thấy trong quá trình huấn luyện, vì vậy chúng tôi cũng bao gồm trả lời câu hỏi để sao chép các nhiệm vụ thế giới thực hơn.
LongChat-Lines Chúng tôi bắt đầu với một nhiệm vụ truy xuất key-value chi tiết tổng hợp đầu tiên được đề xuất trong [20] và cũng được sử dụng bởi [21]. Mặc dù những công việc này xuất sắc với các ngữ cảnh tiêu chuẩn của LLM, chúng thiếu các độ dài ngữ cảnh dài hơn cần thiết để đánh giá các thử nghiệm của chúng tôi. Do đó, chúng tôi sử dụng cùng nhiệm vụ như [20, 21], nhưng tạo ra các mẫu bổ sung có độ dài ngữ cảnh dài hơn. Nhiệm vụ này đưa cho mô hình một lời nhắc với các dòng có dạng:
•line grotesque-classmate: REGISTER CONTENT is <42527 >
•line imperfect-bull: REGISTER CONTENT is <3119 >
•line supreme-inversion: REGISTER CONTENT is <13960 >
•...
Mô hình được yêu cầu ghi nhớ giá trị tương ứng với REGISTER CONTENT cho mỗi dòng và được hỏi ở cuối để truy xuất giá trị cho một dòng cụ thể. Bằng cách thay đổi số lượng dòng trong lời nhắc, chúng tôi có thể kiểm soát độ dài ngữ cảnh. Chúng tôi phát hành các phiên bản độ dài dài hơn của nhiệm vụ này so với trong [20], và chúng tôi cũng phát hành script tạo cho nhiệm vụ này.
WikiQA Chúng tôi cũng tạo ra hai bộ dữ liệu mới từ bộ dữ liệu Natural Questions [12] với các đánh giá ngữ cảnh dài hơn đặc biệt trong tâm trí mà chúng tôi gọi chung là WikiQA. Trong đánh giá này, lời nhắc đưa cho LLM có định dạng của một tài liệu Wikipedia theo sau bởi một câu hỏi liên quan đến tài liệu đó; mô hình được yêu cầu trả lời câu hỏi. Chúng tôi đảm bảo câu trả lời cho câu hỏi là một câu trả lời ngắn là một từ duy nhất hoặc một câu nhỏ có khớp chuỗi chính xác trong tài liệu được đưa cho LLM như đầu vào. Chúng tôi gọi nhiệm vụ này là Free Form QA (FFQA).
Một vấn đề tiềm ẩn trong một bộ dữ liệu dựa trên Wikipedia tuy nhiên là mô hình có thể có lẽ trả lời đúng từ corpus đã pretrain của nó và không cụ thể sử dụng thông tin trong ngữ cảnh. Để giải quyết điều này, chúng tôi đã tạo ra một bộ dữ liệu "thay đổi" khác, mà chúng tôi gọi là Altered Numeric QA (AltQA). Bộ dữ liệu này chỉ gồm các câu hỏi có câu trả lời số. Ở đây, chúng tôi thay đổi câu trả lời và mọi lần xuất hiện của câu trả lời trong tài liệu thành một số khác, do đó đảm bảo rằng LLM phải chú ý đến ngữ cảnh, và chỉ ngữ cảnh, để đưa ra câu trả lời đúng. Sửa đổi được thực hiện như sau:
4

--- TRANG 5 ---
... là phần thứ ba và cuối cùng của Divine Comedy của Dante, theo sau
Inferno và Purgatorio. Đó là một câu chuyện ngụ ngôn kể về hành trình của Dante
qua Thiên đường, được hướng dẫn bởi Beatrice, người tượng trưng cho
thần học. Trong bài thơ, Paradise được mô tả như một loạt các
cầu đồng tâm bao quanh Trái đất, bao gồm Mặt trăng,
Thủy tinh, Kim tinh, Mặt trời, Sao Hỏa, Sao Mộc, Sao Thổ, Các ngôi sao
Cố định, Primum Mobile và cuối cùng, Empyrean. Nó được
viết vào đầu thế kỷ 14...
Câu hỏi:
Ai phục vụ như hướng dẫn viên của dante qua thiên đường
Câu trả lời tham khảo:
BeatriceTài liệu:FreeForm WIkiQA
... Hy Lạp đã tổ chức Thế vận hội Mùa hè hai
lần, Thế vận hội hiện đại khai mạc năm 1896 và một lần nữa vào
2004 2009. Cả hai đều được tổ chức tại Athens, cùng với Paris và
Los Angeles là những thành phố đã tổ chức Thế vận hội
hai lần, với London là thành phố duy nhất tổ chức ba
lần. Thủ đô Hy Lạp cũng tổ chức Thế vận hội Intercalated 1906
, mà vào thời điểm đó được coi là Thế vận hội bởi
Ủy ban Olympic Quốc tế...
Câu hỏi:
Lần cuối cùng thế vận hội ở Hy Lạp là khi nào?
Câu trả lời tham khảo:
2009*
*và không phải 2004Tài liệu:AlteredNumeric WIkiQAHình 1: Ví dụ đoạn QA từ bộ dữ liệu WikiQA của chúng tôi.
•Nếu câu trả lời là một năm, khá thường xuyên, (tức là nó nằm giữa 1000-2100), chúng tôi thay đổi nó thành một giá trị ngẫu nhiên khác trong +/- 10 của giá trị gốc. Chúng tôi coi năm như một trường hợp đặc biệt để không làm xáo trộn tính mạch lạc tổng thể của tài liệu bằng cách có các giá trị ngày tháng rất lạc hậu.
•Nếu câu trả lời là bất kỳ số nào khác, chúng tôi thay đổi nó thành một số ngẫu nhiên khác có cùng số chữ số.
Hình 1 nổi bật các ví dụ từ bộ dữ liệu WikiQA của chúng tôi. Vì các ngữ cảnh trong ứng dụng của chúng tôi dài, vị trí của câu trả lời trong ngữ cảnh có thể đóng vai trò quan trọng trong khả năng trả lời câu hỏi của mô hình. Do đó, chúng tôi sử dụng cả hai nhiệm vụ WikiQA để tiến hành phân tích về hiệu suất của LLM khi cả vị trí câu trả lời di chuyển trong tài liệu (trong 10% đầu, 10% cuối, hoặc ngẫu nhiên ở bất kỳ đâu khác), cũng như với câu hỏi được đưa ra ở đầu hoặc cuối của lời nhắc — trong một nỗ lực sao chép các phân tích của [22].
4 Kỹ thuật Ngoại suy Độ dài Ngữ cảnh
Chúng tôi kiểm tra một số kỹ thuật ngoại suy độ dài ngữ cảnh, bao gồm các phương pháp hiện có (hoặc các biến thể nhỏ về chúng) cũng như các phương pháp mới được đề xuất của riêng chúng tôi.
4.1 Kỹ thuật Ngoại suy Độ dài Ngữ cảnh Hiện có
Một số phương pháp tồn tại để thích ứng mã hóa vị trí RoPE với các độ dài ngữ cảnh dài hơn. Chúng tôi đã đánh giá các kỹ thuật sau.
Linear Scaling/Positional Interpolation Ở đây, vector vị trí được chia cho hệ số tỷ lệ. Do đó nếu mô hình gốc được huấn luyện trên một phạm vi vị trí [0,1, ...,2048], chẳng hạn, thì mô hình mới sẽ thấy thay vào đó [0/x,1/x, ...,2048/x] trong đó x là hệ số tỷ lệ.
xPos Chúng tôi muốn kiểm tra liệu một checkpoint được huấn luyện với sơ đồ mã hóa RoPE của mô hình cơ sở có thể được tinh chỉnh cho sơ đồ xPos [7]. Ngoài rào cản lập trình của việc vá toàn bộ mô-đun attention để xử lý phép biến đổi độc đáo của xPos về key và query, vấn đề chính được trình bày bởi loại thích ứng này là độ nhạy cảm của xPos đối với độ chính xác dấu phẩy động. Phương pháp dựa vào việc chia tỷ lệ key bằng các giá trị số với số mũ lớn (tuyệt đối); những cái này sau này hủy bỏ trong tích vô hướng với query. Tuy nhiên, đối với các ngữ cảnh dài, các giá trị lớn thực sự có thể vượt quá độ lớn được hỗ trợ bởi float16. Chúng tôi chọn
5

--- TRANG 6 ---
giải quyết điều này bằng cách thực hiện phép toán attention cốt lõi trong float32 với chi phí là sự chậm lại huấn luyện 2X.
Randomized Position Encodings Ở đây chúng tôi ngẫu nhiên hóa khoảng cách giữa các giá trị vị trí đồng nhất trong phạm vi [ε,2] với 0 < ε≪1, thay vì sử dụng [0,1, ..., n] điển hình có khoảng cách cố định có kích thước 1. Trực giác đằng sau phương pháp này là bằng cách cho mô hình thấy nhiều khoảng cách intra-position khác nhau tại thời gian tinh chỉnh, mô hình sẽ có thể tổng quát hóa cho bất kỳ lựa chọn vị trí chi tiết nào tại thời gian đánh giá, do đó cho phép tăng hiệu quả độ dài ngữ cảnh bằng cách chọn các phần nhỏ hơn. Điều này có một số tương đồng với quy trình được mô tả trong Ruoss et al. [19]. Chúng tôi đặt giới hạn trên là 2 để mô hình sẽ trong kỳ vọng thấy vị trí cuối cùng là n (vì E[X]≈1 đối với X∼U(ε,2)). Chúng tôi cũng đặt giới hạn dưới dương, không bằng không là ε để tránh các vấn đề với aliasing vị trí do độ chính xác số hữu hạn.
4.2 Kỹ thuật Ngoại suy Độ dài Ngữ cảnh Mới được Đề xuất
Power Scaling Trong RoPE gốc, cơ sở được sử dụng được đưa ra bởi:
Θ ={θi= 10000−2(i−1)/d|i∈ {1,2, . . . ,d/2}} (1)
trong đó d là chiều embedding. Chúng tôi thay vào đó sử dụng cơ sở được đưa ra bởi:
Θ∗=(θ∗i=θi1−2i/dk|i∈ {1,2, . . . ,d/2}) (2)
trong đó k là một tham số được đặt. Bằng cách áp dụng phép biến đổi này, các phần tử tần số cao (khoảng cách ngắn) của cơ sở ít bị ảnh hưởng hơn so với các phần tử tần số thấp (khoảng cách dài), được làm cho thậm chí thấp hơn về tần số – xem Hình 2. Bằng cách làm như vậy, hy vọng của chúng tôi là mô hình sẽ phải thực hiện ngoại suy ít phức tạp hơn cho các tần số thấp nơi nó chưa thấy phạm vi đầy đủ của hàm tuần hoàn trong thời gian huấn luyện, và do đó ngoại suy tốt hơn. Một vấn đề tiềm ẩn tuy nhiên là mô hình dựa vào các mối quan hệ cụ thể qua các tần số mà một phép biến đổi tuyến tính bảo tồn nhưng một phép biến đổi phi tuyến phá hủy.
Truncated Basis Bắt đầu từ Phương trình 1, chúng tôi thay vào đó sử dụng cơ sở được đưa ra bằng cách áp dụng:
θ∗i= {
θi cho θi≥b,
ρ cho a < θi< b,
0 cho θ∗i≤a. (3)
Trong đó ρ là một giá trị cố định tương đối nhỏ, và a và b là các giá trị cutoff được chọn. Ý tưởng ở đây là chúng tôi muốn bảo tồn các thành phần tần số cao của cơ sở nhưng đặt các phần tử tần số thấp thành một giá trị không đổi—trong trường hợp này, 0. Bằng cách làm như vậy với một lựa chọn khôn ngoan của cutoff a, mô hình sẽ đã trải nghiệm tất cả các giá trị của cơ sở trong độ dài ngữ cảnh được sử dụng trong quá trình tinh chỉnh (do tính chất tuần hoàn của các hàm sin và cosine) và do đó nên ngoại suy tốt hơn đến các độ dài ngữ cảnh lớn hơn để đánh giá. Tuy nhiên, mô hình vẫn cần có thể phân biệt giữa các khoảng cách trải dài toàn bộ ngữ cảnh mà nó đã được huấn luyện, vì vậy chúng tôi cũng bao gồm tần số cố định ρ. Tóm lại, chúng tôi hy vọng rằng với cơ sở này, mô hình có thể tránh vấn đề phải học các hệ số phức tạp trong toàn bộ cơ sở RoPE bằng cách thay vào đó học các hàm mượt ở khoảng cách dài hơn (như được chứng minh trong bài báo [17]).
Trong Hình 2, chúng tôi so sánh trực quan các tần số được tạo ra bởi cơ sở RoPE tiêu chuẩn, power scaling, và truncation.
6

--- TRANG 7 ---
Hình 2: So sánh cơ sở RoPE tiêu chuẩn vs cơ sở power và cơ sở truncated. Trục x trải dài qua chiều embedding, và trục y là giá trị tần số của cơ sở sine-cosine.
5 Kết quả & Thảo luận
Trong các thử nghiệm sau, chúng tôi đã tinh chỉnh mô hình LLaMA-13B cơ sở trên một phần của bộ dữ liệu RedPajama [5] đã được sửa đổi để mỗi mẫu dữ liệu có kích thước chính xác 4096 token. Chúng tôi huấn luyện với mỗi phương pháp mã hóa vị trí cho đến khi mất mát đánh giá gần như ổn định. Các đường cong mất mát có thể được tìm thấy trong Phụ lục B.
Sau đó, chúng tôi áp dụng thêm instruction finetuning (IFT) với bộ dữ liệu Vicuna [23] và sử dụng LoRA [24] trên mô hình cơ sở. Tuy nhiên, chúng tôi phát hiện ra rằng mặc dù IFT đã tăng độ chính xác trên LongChat-Lines, nó không thay đổi đáng kể phạm vi ngữ cảnh mà mô hình cơ sở có thể xử lý (xem Hình 5 trong Phụ lục C). Điều này chúng tôi thấy là một sự tương phản rõ rệt với các biến thể WikiQA; ở đó, IFT là cần thiết để mô hình tạo ra bất kỳ kết quả có ý nghĩa nào. Do đó đối với LongChat-Lines, chúng tôi sử dụng các mô hình không IFT; đối với WikiQA, chúng tôi thực hiện đánh giá trên một tập con của các mô hình hứa hẹn hơn với IFT bổ sung.
5.1 Ngoại suy Độ dài Ngữ cảnh được Tinh chỉnh
LongChat-Lines Chúng tôi tiến hành đánh giá trên LongChat-Lines với các kỹ thuật được mô tả trong Phần 4 và báo cáo kết quả trong Bảng 1. Chúng tôi mong đợi tất cả các mô hình có thể thực hiện với độ chính xác không bằng không cho đến ít nhất 4200 cho rằng mô hình được tinh chỉnh trên độ dài ngữ cảnh 4096 và hội tụ được đạt được trong tất cả các trường hợp. Tuy nhiên, điều này hóa ra không phải là trường hợp đối với xPos, không thể thực hiện nhiệm vụ chút nào. Chúng tôi nghi ngờ điều này có thể là do xPos quá khác so với cơ sở RoPE để mô hình có thể thích ứng trong việc tinh chỉnh; như chúng ta thấy trong Phụ lục B, mất mát huấn luyện và đánh giá cho xPos không thể đạt được các giá trị tương tự như các phương pháp khác. Điều này cũng có thể là sản phẩm của các vấn đề độ chính xác số được gặp phải trong việc triển khai xPos.
Linear scaling có thể đạt được ngoại suy độ dài ngữ cảnh thành công. Điều đáng đề cập ở đây là chúng tôi sẽ mong đợi scaling với hệ số x đạt được độ chính xác không bằng không lên đến 2048 · x, do mô hình cơ sở được huấn luyện trên độ dài ngữ cảnh 2048. Mặc dù điều này được quan sát với linear scaling với hệ số 4, chúng ta thấy trong Bảng 1 sự suy giảm nhanh hơn nhiều khi độ dài ngữ cảnh tăng với hệ số tỷ lệ 16. Đến độ dài ngữ cảnh 17500, nó đã ghi nhận 0% độ chính xác mặc dù chúng ta một cách ngây thơ sẽ mong đợi hiệu suất hợp lý
7

--- TRANG 8 ---
Độ dài
Ngữ cảnhLinear
Scaling
(Factor=4)Linear
Scaling
(Factor=16)Power basis Truncated
basisRandomized
positionxPos
2500 0.7 0.64 0.96 0.42 0.54 0
3600 0.64 0.42 0.64 0.26 0.54 0
4200 0.56 0.56 0.1 0.18 0.3 0
4800 0.66 0.62 0 0.14 0.14 0
7100 0.36 0.4 0 0.04 0.16 0
9400 0 0.22 0 0 0 0
11800 0 0.14 0 0 0 0
14000 0 0.12 0 0 0 0
16000 0 0.1 0 0 0 0
17500 0 0 0 0 0 0
20000 0 0 0 0 0 0
22000 0 0 0 0 0 0
Bảng 1: Sau khi tinh chỉnh LLaMA-13B với độ dài ngữ cảnh cơ sở 4096, bảng này thể hiện các đánh giá với các phương pháp ngoại suy ngữ cảnh khác nhau trên LongChat-Lines. Độ chính xác 1.0 sẽ chỉ ra hiệu suất hoàn hảo và 0.0 chỉ ra việc làm sai mọi đánh giá. Power basis sử dụng tham số k= 0.5. Truncated basis sử dụng các tham số sau: a=1/82π/2048, b=2π/2048, ρ=1/162π/2048. Randomization sử dụng tham số giới hạn dưới ε=1/16. Các đánh giá đều được thực hiện mà không có instruction finetuning bổ sung.
lên đến khoảng 32000 độ dài ngữ cảnh. Chúng tôi tin rằng điều này chỉ ra rằng có giới hạn đối với phương pháp nội suy và quan tâm đến việc kiểm tra điều này thêm trong công việc tương lai.
Power basis, mặc dù nó thực hiện tốt nhất ở ngữ cảnh ngắn nhất, cũng suy giảm nhanh nhất và không thể cho thấy bất kỳ hiệu suất ngoại suy nào vượt quá 4200 ngữ cảnh.
Phương pháp vị trí ngẫu nhiên có thể xuất hiện là đang ngoại suy dựa trên kết quả trong bảng. Tuy nhiên, điều này có thể do cách chúng tôi đánh giá mô hình. Tại thời gian huấn luyện, mô hình lấy mẫu khoảng cách đồng nhất trong [ε,2] như được mô tả trong Phần 4. Tại thời gian đánh giá, không rõ ràng a priori lựa chọn vị trí tốt nhất là gì. Chúng tôi đã thử một loạt các phương pháp khác nhau: khoảng cách cố định có kích thước 1, ngẫu nhiên đồng nhất trong [ε,2] và ngẫu nhiên đồng nhất trong [ε,1]. Chúng tôi tìm thấy kết quả tốt nhất cho ngoại suy với cái sau, vì vậy chúng tôi báo cáo điều này. Chúng tôi hy vọng rằng bằng cách giảm giới hạn trên thêm, chúng tôi có thể dụ dỗ ngoại suy độ dài ngữ cảnh mong muốn từ mô hình. Tuy nhiên, đi đến [ε,0.5] và bên dưới đã làm suy giảm đáng kể hiệu suất của mô hình. Kết luận của chúng tôi từ điều này là mô hình không thể độc lập học để biểu diễn mỗi vị trí mà không biết các vị trí khác. Một hướng thú vị cho công việc tương lai sẽ là điều kiện hóa ma trận Q-proj và K-proj trên các vị trí được lấy mẫu trong quá trình huấn luyện (và đánh giá).
Truncated basis dường như cung cấp ngoại suy độ dài ngữ cảnh thực sự, vì nó có thể đạt được độ chính xác không bằng không trên các độ dài ngữ cảnh ngoài bất kỳ giá trị nào nó đã thấy trước đây. Mặc dù hiệu suất có suy giảm khi độ dài tăng và biểu hiện hiện tại của điều này kém hơn trong hiệu suất so với linear scaling, chúng tôi tin rằng đây có thể là một hướng điều tra có thể dẫn đến hiệu suất ngoại suy tốt hơn. Truncation cũng có thể được kết hợp với linear scaling, như chúng tôi thảo luận trong Phần 5.2.
WikiQA Variants Chúng tôi tiếp tục tiến hành đánh giá các phương pháp linear scaling và truncated basis trên các biến thể WikiQA được mô tả trong Phần 4. Khác với nhiệm vụ truy xuất, chúng tôi thấy rằng các mô hình không thể thực hiện nhiệm vụ này thành công mà không có instruction finetuning nào, vì vậy chúng tôi thực hiện phân tích này chỉ trên một vài phương pháp quan tâm. Kết quả được hiển thị trong Bảng 2 và 3. Chúng phần lớn khớp với mô hình được thấy trong LongChat-Lines—linear scaling với hệ số tỷ lệ 4 có thể thực hiện nhiệm vụ lên đến 7500 ngữ cảnh nhưng không vượt quá, trong khi hệ số tỷ lệ 16 có thể vượt qua cutoff này nhưng với sự giảm dốc trong độ chính xác. Như với LongChat-Lines, chúng ta thấy rằng các mô hình dường như cho thấy một số suy giảm độ chính xác khi độ dài ngữ cảnh tăng. Chúng ta thấy một lần nữa rằng truncated basis có thể ngoại suy thành công đến khoảng cùng độ dài ngữ cảnh như trong LongChat-Lines với độ chính xác có thể so sánh với linear scaling với scale 4, nhưng một lần nữa có vẻ không thể đi xa hơn độ dài ngữ cảnh khoảng 8k.
8

--- TRANG 9 ---
Độ dài Ngữ cảnh Linear scaling (Factor=4) Linear scaling
(Factor=16)Truncated basis
2000 0.72 0.69 0.74
3800 0.72 0.73 0.73
7500 0.62 0.70 0.46
15000 0.00 0.68 0.00
24000 0.00 0.56 0.00
32000 0.00 0.18 0.00
Bảng 2: Độ chính xác trên biến thể AlteredNumeric WikiQA của các mô hình được tinh chỉnh với các phương pháp ngoại suy ngữ cảnh khác nhau. Các đánh giá đều được thực hiện với instruction finetuning.
Độ dài Ngữ cảnh Linear scaling (Factor=4) Linear scaling
(Factor=16)Truncated basis
1900 0.44 0.47 0.46
3800 0.49 0.44 0.55
7600 0.46 0.45 0.36
15000 0.00 0.48 0.00
24000 0.00 0.42 0.00
32000 0.00 0.20 0.00
Bảng 3: Độ chính xác trên biến thể FreeForm WikiQA của các mô hình được tinh chỉnh với các phương pháp ngoại suy ngữ cảnh khác nhau. Các đánh giá đều được thực hiện với instruction finetuning.
5.2 Zero-Shot Linear Scaling
Trong phần trước, chúng tôi đã kiểm tra hiệu suất của linear scaling sử dụng cùng giá trị tại thời gian tinh chỉnh như tại thời gian đánh giá. Trong phần này, chúng tôi thay vào đó điều tra tác động của việc sử dụng hệ số tỷ lệ khác tại thời gian đánh giá so với những gì mô hình đã được huấn luyện. Trong Bảng 4, chúng tôi hiển thị kết quả cho chiến lược này khi được đánh giá trên LongChat-Lines. Chúng tôi thấy nói chung rằng nếu mô hình được huấn luyện với hệ số tỷ lệ x, thì mô hình có thể đánh giá thành công zero-shot với hệ số tỷ lệ 2x (với một số giảm độ chính xác trong phạm vi độ dài ngữ cảnh mà mô hình có thể xử lý trước đây). Nó cũng xuất hiện rằng tại hệ số tỷ lệ 16, mô hình không còn có thể tăng độ dài ngữ cảnh hiệu quả của nó bằng cách sử dụng phương pháp này. Chúng tôi cũng thấy rằng đánh giá với >2x dẫn đến mô hình bị hỏng và không thể thực hiện nhiệm vụ.
Chúng tôi cho thấy rằng zero-shot linear scaling có thể được áp dụng thành công sau khi tinh chỉnh với truncated basis. Thú vị, trong khi đối với linear scaling, sử dụng hệ số tỷ lệ dài hơn tại thời gian đánh giá dẫn đến sự xấu đi của độ chính xác trên các độ dài ngữ cảnh mà mô hình có thể xử lý trước đây, điều này dường như không phải là trường hợp đối với truncated basis—thay vào đó, phạm vi độ dài ngữ cảnh mà mô hình đạt được độ chính xác không bằng không tăng lên, và độ chính xác cải thiện trong số các độ dài ngữ cảnh mà mô hình đã được tinh chỉnh.
5.3 So sánh Perplexity với Các Nhiệm vụ
Trong Phần 3, chúng tôi đã giới thiệu hai nhiệm vụ cụ thể yêu cầu LLM ngữ cảnh dài trích xuất câu trả lời từ toàn bộ văn bản, lập luận rằng các nhiệm vụ này có thể đánh giá hiệu suất ngữ cảnh dài tốt hơn so với perplexity thô. Để phân tích cách perplexity so sánh với các nhiệm vụ này, chúng tôi báo cáo perplexities trên một tập giữ lại của bộ dữ liệu RedPajama cho một tập con của các mô hình được huấn luyện của chúng tôi (xem Bảng 5). Điểm perplexity có cho thấy một sự tăng lớn khi đạt được độ dài ngữ cảnh mà mô hình hoàn toàn không thể xử lý (ví dụ, vượt quá 2k trên mô hình LLaMA cơ sở, hoặc vượt quá 8k trên mô hình linear scale 4). Tuy nhiên, chúng có vẻ ít hiệu quả hơn để hiển thị sự giảm khả năng ngữ cảnh dài trong phạm vi hiệu quả đó. Đặc biệt, trong khi chúng tôi quan sát sự giảm dốc trong hiệu suất trên LongChat-Lines và các biến thể WikiQA khi độ dài ngữ cảnh tăng cho các cột linear scale 4 và truncated basis của Bảng 1), sự suy giảm này không được phản ánh mạnh mẽ trong điểm perplexity tại những ngữ cảnh đó. Tuy nhiên, mô hình linear scale 16 dường như có perplexity và độ chính xác được tương quan tốt trên các nhiệm vụ của chúng tôi. Có lẽ nói rõ nhất, chúng ta thấy
9

--- TRANG 10 ---
Train Scaling = 1 (mô hình cơ sở) Train Scaling = 4 Train Scaling = 16 Truncated (Scaling = 1)
Độ dài Ngữ cảnh Eval 1 Eval 2 Eval 4 Eval 4 Eval 8 Eval 16 Eval 16 Eval 32 Eval 64 Eval 1 Eval 2 Eval 4
2500 0 0.32 0 0.88 0.64 0 0.64 0.24 0 0.42 0.58 0
3600 0 0.3 0 0.8 0.58 0 0.42 0.26 0 0.26 0.44 0
4200 0 0.18 0 0.86 0.62 0 0.56 0.12 0 0.18 0.28 0
4800 0 0 0 0.86 0.62 0 0.62 0.22 0 0.14 0.26 0
7100 0 0 0 0.64 0.38 0 0.4 0.12 0 0.04 0.04 0
9400 0 0 0 0 0.32 0 0.22 0.12 0 0 0.04 0
11800 0 0 0 0 0.30 0 0.14 0.1 0 0 0.04 0
14000 0 0 0 0 0.1 0 0.12 0.04 0 0 0 0
16000 0 0 0 0 0.12 0 0.1 0.02 0 0 0 0
17500 0 0 0 0 0 0 0 0 0 0 0 0
20000 0 0 0 0 0 0 0 0 0 0 0 0
22000 0 0 0 0 0 0 0 0 0 0 0 0
Bảng 4: Độ chính xác trên LongChat-Lines của các mô hình được tinh chỉnh với các phương pháp ngoại suy ngữ cảnh khác nhau và sau đó được đánh giá với linear scaling bổ sung. Bất cứ khi nào linear scale đánh giá lớn hơn linear scale huấn luyện, điều này tạo ra ngoại suy độ dài ngữ cảnh zero-shot. Nói chung, việc tăng độ dài ngữ cảnh đánh giá 2x so với huấn luyện thực sự làm tăng gấp đôi độ dài ngữ cảnh có thể sử dụng, với chi phí độ chính xác, cho độ dài huấn luyện 1 và 4 (đối với độ dài huấn luyện 16, nó không). Độ chính xác truncated basis cải thiện với 2x scaling. Một scale up tích cực hơn 4x lần độ dài huấn luyện dẫn đến sự cố mô hình rõ ràng.
thiếu sót của perplexity cho so sánh giữa các mô hình. Theo Bảng 5, truncated basis thực hiện tốt nhất tại 8k và dưới; tuy nhiên, trong Bảng 1, 2 và 3 truncated thấp hơn đáng kể trong hiệu suất so với các mô hình linear scaled tại ngữ cảnh 8k.
Perplexity thường được sử dụng trong văn học để đo hiệu suất ngữ cảnh dài [13, 7], nhưng chúng tôi tin rằng những kết quả này cho thấy nó không tự nó là một thước đo đầy đủ về hiệu suất ngữ cảnh dài, nhưng được sử dụng tốt nhất với các nhiệm vụ khác mà bổ sung thăm dò khả năng của LLM.
Độ dài Ngữ cảnh LLaMA Base Linear Scaling (Factor=4) Linear Scaling (Factor=16) Truncated Basis
512 4.06 4.06 4.05 3.79
1k 3.88 3.87 3.86 3.63
2k 3.79 3.75 3.74 3.52
4k 9022 3.66 3.66 3.46
8k 7198 3.79 3.97 3.78
16k 5141 14902 5.43 15793
24k 4980 21236 8.73 13929
32k 4408 55480 98.12 12534
Bảng 5: Điểm perplexity trên một tập đánh giá giữ lại của bộ dữ liệu RedPajama tại các độ dài ngữ cảnh khác nhau trên các mô hình khác nhau. Độ dài đánh giá là 256 token, và lời nhắc đưa cho các mô hình là N - 256 token trước đó của tài liệu, trong đó N là độ dài ngữ cảnh chúng tôi đã đánh giá. Khi độ dài ngữ cảnh quá dài để mô hình xử lý hiệu quả, perplexity có bùng nổ; tuy nhiên, trong các phạm vi mô hình có thể xử lý, perplexity có vẻ ít nhạy cảm hơn với sự suy giảm sử dụng ngữ cảnh so với nhiệm vụ LongChat-Lines, và không tuân theo cùng thứ hạng giữa các mô hình như của LongChat-Lines hoặc WikiQA.
5.4 Phân tích vị trí câu hỏi và câu trả lời
Đối với các biến thể WikiQA, chúng tôi thực hiện phân tích phân tầng về tác động của vị trí câu trả lời và câu hỏi. Như được mô tả trong Phần 4, chúng tôi xem xét tác động của việc đặt câu trả lời trong 10% đầu của tài liệu, 10% cuối, hoặc ở nơi khác ngẫu nhiên. Chúng tôi cũng kiểm tra tác động của việc đặt câu hỏi ở đầu hoặc cuối của lời nhắc. Kết quả được hiển thị trong Bảng 6 và 7, được thực hiện trên mô hình với linear scaling với hệ số 16, với instruction finetuning bổ sung.
10

--- TRANG 11 ---
Độ dài Ngữ cảnh Vị trí Câu trả lời Vị trí Câu hỏi
Đầu Giữa Cuối Đầu Cuối
2000 0.74 0.68 0.66 0.70 0.69
3800 0.70 0.67 0.66 0.63 0.73
7500 0.69 0.63 0.65 0.61 0.70
15000 0.68 0.68 0.69 0.68 0.68
24000 0.30 0.26 0.50 0.14 0.56
32000 0.13 0.13 0.23 0.15 0.18
Bảng 6: Phân tích độ chính xác phân tầng trên nhiệm vụ AlteredNumericQA. Đối với câu trả lời, "Đầu" đề cập đến 10% đầu của tài liệu, "Cuối" đến 10% cuối, và "Giữa" đến bất kỳ vị trí nào khác. Đối với câu hỏi, "Đầu" đề cập đến việc đặt câu hỏi trước phần còn lại của lời nhắc ngôn ngữ, và "Cuối" đề cập đến việc đặt câu hỏi ở cuối. Kết quả được báo cáo trên LLaMA-13B được tinh chỉnh với linear scale 16, với IFT được áp dụng.
Độ dài Ngữ cảnh Vị trí Câu trả lời Vị trí Câu hỏi
Đầu Giữa Cuối Đầu Cuối
1900 0.37 0.40 0.36 0.27 0.47
3800 0.40 0.30 0.32 0.24 0.44
7600 0.35 0.34 0.35 0.24 0.45
15000 0.44 0.43 0.45 0.40 0.48
24000 0.18 0.20 0.40 0.10 0.42
32000 0.07 0.11 0.18 0.04 0.20
Bảng 7: Phân tích độ chính xác phân tầng trên nhiệm vụ FreeFormQA. Đối với câu trả lời, "Đầu" đề cập đến 10% đầu của tài liệu, "Cuối" đến 10% cuối, và "Giữa" đến bất kỳ vị trí nào khác. Đối với câu hỏi, "Đầu" đề cập đến việc đặt câu hỏi trước phần còn lại của lời nhắc ngôn ngữ, và "Cuối" đề cập đến việc đặt câu hỏi ở cuối. Kết quả được báo cáo trên LLaMA-13B được tinh chỉnh với linear scale 16, với IFT được áp dụng.
Chúng tôi nhằm xây dựng trên phân tích tương tự từ [22]. Tuy nhiên, chúng tôi không thể sao chép các kết quả được hiển thị trong bài báo đó trên mô hình LongChat-13B (16K) (mà phương pháp mô hình hóa của chúng tôi có thể so sánh nhất). Trên cả FreeFormQA và AlteredNumericQA, chúng tôi quan sát không có xu hướng rõ ràng liên quan đến vị trí của câu trả lời trong lời nhắc và độ chính xác của mô hình lên đến độ dài ngữ cảnh 15k. Cũng không có vẻ có tác động đáng kể về vị trí của câu hỏi đối với AlteredNumericQA, nhưng có tác động đáng chú ý được quan sát đối với FreeFormQA nơi có câu hỏi ở cuối có vẻ có cải thiện đáng kể trong độ chính xác. Tuy nhiên, tại độ dài ngữ cảnh 24k và 32k, chúng ta thấy chỉ báo rõ ràng trong cả hai bộ dữ liệu cho cả câu trả lời ở cuối và câu hỏi ở cuối trả về độ chính xác vượt trội so với vị trí của chúng ở nơi khác. Những kết quả này là một sự tương phản rõ rệt với những kết quả trong [22]. Quan điểm của chúng tôi từ điều này là có thể có rất nhiều biến đổi có điều kiện nhiệm vụ trong hiệu suất của LLM liên quan đến mức độ tốt mà chúng có thể sử dụng tất cả các phần của ngữ cảnh; ngay cả những khác biệt nhỏ trong xây dựng nhiệm vụ có thể dẫn đến những khác biệt lớn trong các xu hướng quan sát được.
6 Kết luận và Hạn chế
Trong bài báo này, chúng tôi đã kiểm tra nhiều phương pháp tinh chỉnh LLM cơ sở LLaMA và LLaMA2 đã pretrain có độ dài ngữ cảnh hạn chế sao cho nó có khả năng ngoại suy zero-shot đến các độ dài ngữ cảnh mới, dài hơn. Chúng tôi so sánh các phương pháp sử dụng perplexity, cũng như hai nhiệm vụ tùy chỉnh thăm dò hiệu suất ngữ cảnh dài; chúng tôi thấy rằng các nhiệm vụ tùy chỉnh cung cấp hiểu biết chi tiết hơn về hiệu suất ngữ cảnh dài so với perplexity. Chúng tôi cho thấy rằng phương pháp nội suy tuyến tính thực hiện tốt nhất trong ngoại suy độ dài ngữ cảnh, và tìm thấy một số hứa hẹn trong tiềm năng của việc sử dụng cơ sở mới mà chúng tôi gọi là truncated basis. Chúng tôi phát hành ba mô hình mà chúng tôi gọi là Giraffe mở rộng độ dài ngữ cảnh của các mô hình LLaMA và LLaMA 2 cơ sở sử dụng phương pháp nội suy tuyến tính.
Có không gian đáng kể để xây dựng trên công việc được trình bày trong bài báo này. Chúng tôi lưu ý rằng tất cả các phương pháp cho thấy
11

--- TRANG 12 ---
sự suy giảm độ chính xác trên các nhiệm vụ đánh giá của chúng tôi khi độ dài ngữ cảnh tăng, mặc dù perplexity thường vẫn hợp lý và mô hình vẫn có thể tạo ra đầu ra mạch lạc. Đây là một thiếu sót sẽ thú vị để giải quyết, và theo quan điểm của chúng tôi là cần thiết để tuyên bố khả năng ngoại suy ngữ cảnh dài 'thực sự' của một mô hình.
Các hạn chế của công việc này là chúng tôi chỉ tiến hành phân tích perplexity của chúng tôi trên một bộ dữ liệu tài liệu duy nhất. Công việc tương lai có thể xem xét sao chép phân tích này trên các bộ dữ liệu khác. Ngoài ra, chúng tôi tập trung cụ thể vào ngoại suy độ dài ngữ cảnh từ một mô hình cơ sở đã pretrain, và đặc biệt là các mô hình LLaMA và LLaMA 2 được huấn luyện với mã hóa vị trí RoPE. Công việc tương lai có thể điều tra liệu phân tích ở đây có mở rộng đến các loại mã hóa vị trí và mô hình khác. Công việc tương lai cũng có thể giải quyết các hạn chế của phương pháp nội suy tuyến tính. Chúng ta thấy một số bằng chứng trên nhiệm vụ LongChat-Lines đặc biệt về sự suy giảm độ chính xác khi hệ số tỷ lệ được tăng lên. Giới hạn của kích thước hệ số tỷ lệ của phương pháp này là gì? Có một điểm nào đó mà nó đơn giản không cải thiện phạm vi ngữ cảnh mà nó có thể xử lý? Hơn nữa, phương pháp truncated basis có vẻ cho thấy dấu hiệu của khả năng ngoại suy thực sự có thể được sửa đổi theo cách để đạt được sự ngang bằng với hoặc vượt qua phương pháp nội suy tuyến tính? Chúng tôi tin rằng đây là một số hướng tương lai tiềm năng quan tâm.
Tài liệu tham khảo
[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, và Illia Polosukhin. Attention is all you need, 2023.
[2] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob
Steinhardt. Measuring massive multitask language understanding, 2021.
[3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,
Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,
Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và
Wojciech Zaremba. Evaluating large language models trained on code, 2021.
[4] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, và Connor Leahy. The pile: An 800gb
dataset of diverse text for language modeling, 2020.
[5] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April
2023.
[6] Ofir Press, Noah A. Smith, và Mike Lewis. Train short, test long: Attention with linear biases enables
input length extrapolation, 2022.
[7] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary,
Xia Song, và Furu Wei. A length-extrapolatable transformer, 2022.
[8] OpenAI. Gpt-4 technical report, 2023.
[9] Anthropic. Introducing claude, 2023.
[10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ ee
Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
12

--- TRANG 13 ---
Joulin, Edouard Grave, và Guillaume Lample. Llama: Open and efficient foundation language models,
2023.
[11] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Can-
ton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,
Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu,
Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang
Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan
Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. Llama 2: Open
foundation and fine-tuned chat models, 2023.
[12] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, và Slav Petrov. Natural ques-
tions: a benchmark for question answering research. Transactions of the Association of Computational
Linguistics , 2019.
[13] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, và Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding, 2022.
[14] Peter Shaw, Jakob Uszkoreit, và Ashish Vaswani. Self-attention with relative position representations,
2018.
[15] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. Judging
llm-as-a-judge with mt-bench and chatbot arena, 2023.
[16] Things i'm learning while training superhot, 2023.
[17] Shouyuan Chen, Sherman Wong, Liangjian Chen, và Yuandong Tian. Extending context window of
large language models via positional interpolation, 2023.
[18] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S. Du, Ken ichi Kawarabayashi, và Stefanie Jegelka.
How neural networks extrapolate: From feedforward to graph neural networks, 2021.
[19] Anian Ruoss, Gr´ egoire Del´ etang, Tim Genewein, Jordi Grau-Moya, R´ obert Csord´ as, Mehdi Bennani,
Shane Legg, và Joel Veness. Randomized positional encodings boost length generalization of trans-
formers, 2023.
[20] anadim. A little retrieval test for large language models, May 2023.
[21] Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,
Xuezhe Ma, và Hao Zhang. How long can open-source llms truly promise on context length?, June
2023.
[22] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, và
Percy Liang. Lost in the middle: How language models use long contexts, 2023.
[23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. Judging
llm-as-a-judge with mt-bench and chatbot arena, 2023.
13

--- TRANG 14 ---
[24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và
Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
14

--- TRANG 15 ---
Độ dài Ngữ cảnh LLaMA 2 Linear (8)
2500 0.48
3600 0.42
4200 0.32
4800 0.74
7100 0.56
9400 0.50
11800 0.50
14000 0.42
16000 0.38
17500 0.14
20000 0.14
22000 0.08
26000 0.00
30000 0.00
Bảng 8: Hiệu suất LLaMA 2 trên LongChat-Lines với hệ số tỷ lệ 8.
Độ dài Ngữ cảnh LLaMA 2 Linear (8)
AltQA FFQA
2k 0.72 0.56
4k 0.76 0.55
8k 0.71 0.56
16k 0.59 0.44
24k 0.36 0.28
32k 0.15 0.10
Bảng 9: Hiệu suất LLaMA 2 trên các biến thể WikiQA với hệ số tỷ lệ 8.
A LLaMA 2
Khi chúng tôi đang hoàn thiện bài báo này, Meta đã phát hành LLaMA 2 [11]. Chúng tôi xác minh rằng kết quả tương tự về ngoại suy độ dài ngữ cảnh có thể đạt được với LLaMA 2 bằng phương pháp nội suy tuyến tính. Chúng tôi áp dụng cùng phương pháp như được mô tả trong Phần 4, huấn luyện LLaMA 2-13B trên một phần của bộ dữ liệu RedPajama được sửa đổi sao cho mỗi mẫu dữ liệu có kích thước chính xác 4096 token. Sau đó chúng tôi cũng áp dụng instruction finetuning với bộ dữ liệu Vicuna. Chúng tôi sử dụng tỷ lệ 8. Hiệu suất được hiển thị trong bảng 8 và 9.
Chúng ta thấy rằng mô hình có thể đạt được độ chính xác không bằng không trên LongChat-Lines lên đến độ dài ngữ cảnh 22000, xa hơn bất kỳ mô hình nào chúng tôi đã kiểm tra trong bài báo chính. Mô hình cũng có thể đạt được hiệu suất không bằng không trên các biến thể WikiQA lên đến ngữ cảnh 32k. Tuy nhiên, chúng ta có thấy độ chính xác giảm dần trong cả hai nhiệm vụ khi độ dài ngữ cảnh tăng. Cũng đáng chú ý rằng độ chính xác trên cả hai nhiệm vụ thấp hơn một chút so với LLaMA 1 với scale 16 trên các độ dài ngữ cảnh mà cả hai mô hình có khả năng tạo ra kết quả không bằng không.
15

--- TRANG 16 ---
B Đường cong Mất mát
Hình 3: Đường cong mất mát huấn luyện của các mô hình trong quá trình chạy fitting ban đầu trên các mẫu 4096 token được trích xuất từ bộ dữ liệu RedPajama.
Hình 4: Đường cong mất mát đánh giá của các mô hình trong quá trình chạy fitting ban đầu trên các mẫu 4096 token được trích xuất từ bộ dữ liệu RedPajama.
16

--- TRANG 17 ---
Hình 5: So sánh giữa IFT và không IFT trên LongChat-Lines với linear scaling của 4 được áp dụng. Mặc dù IFT cải thiện độ chính xác, nó không mở rộng phạm vi ngữ cảnh mà mô hình có được độ chính xác không bằng không.
C Tác động của IFT
Như đã đề cập trong Phần 5 của văn bản chính, chúng tôi thấy rằng instruction-fine-tuning với bộ dữ liệu Vicuna đã cải thiện độ chính xác trên LongChat-Lines, nhưng không thay đổi khoảng của các ngữ cảnh không bằng không cho một mô hình nhất định. Hình 5 hiển thị điều này trên mô hình với nội suy tuyến tính với scale 4.
17
