# TEXTGENSHAP: GIẢI THÍCH CÓ THỂ MỞ RỘNG SAU XỬ LÝ TRONG
SINH TẠO VẬN BẢN VỚI TÀI LIỆU DÀI

James Enouen1∗, Hootan Nakhost2, Sayna Ebrahimi2, Sercan O Arik2, Yan Liu1, Tomas Pfister2
1University of Southern California2Google Cloud AI

TÓM TẮT
Các mô hình ngôn ngữ lớn (LLMs) đã thu hút sự quan tâm lớn trong các ứng dụng thực tế với khả năng đưa ra các phản hồi ngày càng chính xác và khả năng lý luận mạch lạc. Với bản chất là hộp đen sử dụng các quy trình lý luận phức tạp trên đầu vào của chúng, điều không thể tránh khỏi là nhu cầu về các giải thích có thể mở rộng và trung thực cho nội dung được tạo ra bởi LLMs sẽ tiếp tục tăng. Đã có những phát triển lớn trong khả năng giải thích của các mô hình mạng nơ-ron trong thập kỷ qua. Trong số đó, các phương pháp giải thích sau xử lý, đặc biệt là giá trị Shapley, đã chứng minh hiệu quả trong việc diễn giải các mô hình học sâu. Tuy nhiên, có những thách thức lớn trong việc mở rộng giá trị Shapley cho LLMs, đặc biệt khi xử lý các ngữ cảnh đầu vào dài chứa hàng nghìn token và các chuỗi đầu ra được tạo tự động hồi quy. Hơn nữa, thường không rõ ràng cách sử dụng hiệu quả các giải thích được tạo ra để cải thiện hiệu suất của LLMs. Trong bài báo này, chúng tôi giới thiệu TextGenSHAP, một phương pháp giải thích sau xử lý hiệu quả kết hợp các kỹ thuật đặc thù cho LM. Chúng tôi chứng minh rằng điều này dẫn đến việc tăng tốc độ đáng kể so với các tính toán giá trị Shapley thông thường, giảm thời gian xử lý từ hàng giờ xuống vài phút cho giải thích cấp độ token, và chỉ vài giây cho giải thích cấp độ tài liệu. Ngoài ra, chúng tôi chứng minh cách giá trị Shapley thời gian thực có thể được sử dụng trong hai kịch bản quan trọng, cung cấp hiểu biết tốt hơn về việc trả lời câu hỏi tài liệu dài bằng cách định vị các từ và câu quan trọng; và cải thiện các hệ thống truy xuất tài liệu hiện có thông qua việc nâng cao độ chính xác của các đoạn văn được chọn và cuối cùng là các phản hồi cuối cùng.

1 GIỚI THIỆU

Các mô hình ngôn ngữ lớn (LLMs) tiếp tục nhanh chóng vượt trội trong các nhiệm vụ sinh tạo văn bản khác nhau cùng với sự tăng trưởng liên tục của các nguồn lực dành cho việc huấn luyện các mô hình dựa trên văn bản (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023). Khả năng ấn tượng của LLM đã dẫn đến việc áp dụng rộng rãi trong các ứng dụng học thuật và thương mại. Khả năng lý luận mạch lạc trên một loạt các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) đã thúc đẩy các nỗ lực cho phép các mô hình tự động xử lý các ngữ cảnh ngày càng lớn. Các mô hình ngữ cảnh dài này cải thiện hiệu suất sinh tạo zero-shot, few-shot, và tăng cường truy xuất thông qua học tập trong ngữ cảnh (Izacard et al., 2022b; Huang et al., 2023; Ram et al., 2023) và giảm nhu cầu huấn luyện các mô hình đặc thù cho nhiệm vụ, trao quyền cho những người không chuyên môn có thể sử dụng LLMs một cách dễ dàng.

Mặc dù có khả năng sinh tạo văn bản đáng chú ý, LLMs được huấn luyện chủ yếu để mô hình hóa các mối tương quan thống kê giữa các token chỉ cung cấp cái nhìn hạn chế về các cơ chế nội bộ của chúng. Đặc điểm này đã khiến LLMs được coi rộng rãi là các mô hình hộp đen rất khó giải thích. Ngoài hiệu suất dự đoán của chúng, các thách thức liên quan đến an toàn, bảo mật, tính trung thực, và nhiều hơn nữa đã trở nên nổi bật, đặc biệt là sau việc áp dụng rộng rãi trong dân số nói chung. Khả năng giải thích thường được ca ngợi như một con đường quan trọng để giải quyết những mối quan ngại này. Các phương pháp này cho phép hiểu biết về quy trình ra quyết định của mô hình, cho phép các bên liên quan trực tiếp xem xét lý luận đằng sau các phản hồi không an toàn hoặc không trung thực.

Các khảo sát gần đây về khả năng giải thích cho NLP đối chiếu hai tiêu chí chính cho giải thích mô hình: khả năng hiểu được và tính trung thực (Lyu et al., 2023; Zhao et al., 2023; Mosca et al., 2022). Khả năng hiểu được (tính dễ hiểu hoặc tính hợp lý) đề cập đến mức độ dễ dàng một giải thích được hiểu bởi khán giả bên ngoài. Nó vốn dĩ phụ thuộc vào chuyên môn của người nhận và vẫn là một tiêu chí rất chủ quan. Mặt khác, tính trung thực đề cập đến mức độ mà một giải thích đơn giản hóa nắm bắt chính xác quy trình lý luận ban đầu của mô hình. Việc đánh giá hiệu quả về khả năng hiểu được và tính trung thực của một phương pháp giải thích nhất định vẫn là một chủ đề gây tranh cãi và đang tiếp diễn trong văn học khả năng diễn giải (Rudin, 2019). Cuộc tranh luận tiếp tục về độ tin cậy của các phương pháp giải thích như điểm chú ý, tầm quan trọng gradient, và lý luận tự giải thích (Jain & Wallace, 2019; Adebayo et al., 2018; Ghorbani et al., 2019; Wang et al., 2020; Wei et al., 2022). Một trong những phương pháp giải thích được tôn trọng nhất, giá trị Shapley (Lundberg & Lee, 2017) vẫn phổ biến vì có nền tảng lý thuyết mạnh mẽ hơn. Tuy nhiên, trong lĩnh vực NLP, các phương pháp như giá trị Shapley gặp khó khăn lớn trong khả năng mở rộng cho các mô hình lớn hơn và đầu vào dài hơn, buộc các nhà thực hành phải chờ đợi thời gian dài không hợp lý trước khi nhận được giải thích.

Để giải quyết những hạn chế này của các phương pháp giải thích hiện tại trong lĩnh vực NLP, chúng tôi giới thiệu TextGenSHAP, một phương pháp mới được thiết kế để thích ứng giá trị Shapley cho sinh tạo văn bản trong khi duy trì tốc độ tính toán phù hợp hơn cho các ứng dụng quy mô LLM. Trọng tâm chính của chúng tôi nằm ở kịch bản thách thức của các giải thích khi sử dụng đầu vào dài làm gợi ý, đặc biệt cho các nhiệm vụ như trả lời câu hỏi trừu tượng từ các tài liệu mở rộng. Theo đó, chúng tôi chứng minh khả năng mở rộng của phương pháp chúng tôi cho các ứng dụng mới qua ba khía cạnh chính được hiển thị trong Hình 1: (a) xử lý các ngữ cảnh dài hơn với hàng nghìn token đầu vào; (b) phù hợp với các mô hình lớn hơn với hàng tỷ tham số; và (c) tạo điều kiện cho sinh tạo văn bản mở, trái ngược với các nhiệm vụ phân biệt như phân loại. Hơn nữa, chúng tôi chứng minh cách các giải thích được tạo ra bởi TextGenSHAP của chúng tôi có thể nâng cao hiệu suất trả lời câu hỏi tài liệu dài theo nhiều cách.

2 CÔNG TRÌNH LIÊN QUAN

Khả năng giải thích mô hình sau xử lý . Đã có nhiều nỗ lực cung cấp giải thích về cách các mô hình học máy sử dụng các đặc trưng đầu vào để đưa ra dự đoán. Trong số nhiều phương pháp giải thích sau xử lý bao gồm LIME (Ribeiro et al., 2016), SHAP (Lundberg & Lee, 2017), và Integrated Gradients (Sundararajan et al., 2017), SHAP và Shapley vẫn chiếm ưu thế do có nền tảng lý thuyết mạnh mẽ. Đối với NLP, nhiều mở rộng cho các phương pháp giải thích dựa trên nhiễu loạn như vậy tận dụng cấu trúc phân cấp và thứ tự tuần tự của văn bản (Chen et al., 2019; Jin et al., 2020; Chen et al., 2020). Tuy nhiên, những phương pháp này bị giới hạn ở cây phân tích nhị phân thay vì các phân cấp tổng quát hơn và chưa được áp dụng cho độ dài đầu vào dài hơn hoặc các mô hình lớn hơn. Các phương pháp gần đây hơn mở rộng ra ngoài các nhiệm vụ phân loại nhị phân bằng cách sử dụng các mở rộng tương phản của các kỹ thuật ban đầu (Jacovi et al., 2021; Yin & Neubig, 2022). Các công trình trong dữ liệu dạng bảng và hình ảnh cũng đã có những bước tiến trong việc tăng tốc giá trị Shapley (Jethani et al., 2022), nhưng gặp khó khăn khi áp dụng cho NLP vì đầu ra văn bản sinh tạo. Cụ thể, tất cả các phương pháp hiện có đều yêu cầu việc xác định trước các đầu ra ứng viên và không thể áp dụng cho không gian đầu ra lớn của sinh tạo văn bản tự do.

Tự giải thích và lý luận . Cũng phổ biến cho các ứng dụng NLP là huấn luyện các mô hình sẽ đồng thời giải thích và dự đoán, tạo ra 'lý luận' để làm nổi bật các token quan trọng cho dự đoán, thường bằng cách căn chỉnh với các lý luận được thu thập từ người chú thích con người (Arous et al., 2021; Joshi et al., 2022) hoặc được tạo ra bằng cách sử dụng các giải thích sau xử lý (Stacey et al., 2022; Chan et al., 2022). Tuy nhiên, các phương pháp như vậy vẫn bị giới hạn ở các nhiệm vụ phân loại, có thể do khó khăn trong việc thu thập lý luận con người và các hạn chế hiện tại của giải thích sau xử lý cho sinh tạo văn bản, như đã thảo luận ở trên. Các giải thích ngôn ngữ tự nhiên, chẳng hạn như chuỗi suy nghĩ (Wei et al., 2022), nơi LLMs phát ra giải thích về bản thân chúng do đó là một số ít giải thích có sẵn cho sinh tạo văn bản. Thật không may, các phương pháp như vậy vẫn hoàn toàn tách biệt khỏi mối quan ngại về tính trung thực hoặc độ chính xác của giải thích (Jacovi & Goldberg, 2021; Zheng et al., 2022).

Truy xuất thông tin từ tài liệu dài . Trả lời câu hỏi (QA) vẫn là một nhiệm vụ hiểu ngôn ngữ tự nhiên cơ bản, vượt ra ngoài nguồn gốc của nó trong đọc hiểu và hòa nhập với các cơ sở kiến thức ngày càng lớn. Hai loại QA chính là QA tài liệu dài, nơi đầu vào là một tài liệu liền mạch duy nhất chứa ít nhất hàng nghìn token, và QA miền mở, nơi đầu vào là một kho tài liệu lớn thường đầy những tài liệu nhỏ hơn hàng triệu. Sự phân chia giữa hai loại này có thể được truy nguyên ít nhất là từ sớm như bộ dữ liệu Natural Questions (NQ) (Kwiatkowski et al., 2019) để trả lời câu hỏi đưa ra các trang Wikipedia. Công trình tiếp theo như Lee et al. (2019); Karpukhin et al. (2020) xây dựng các công thức hóa miền mở của nhiệm vụ NQ ban đầu bằng cách bao gồm toàn bộ kho Wikipedia thay vì chỉ trang liên quan nhất. Các nhiệm vụ miền mở như vậy được thống trị bởi phương pháp đường ống đầu tiên tận dụng một mô hình truy xuất để xếp hạng các đoạn văn liên quan nhất và sau đó sử dụng mô hình đọc để hiểu thêm về tập hợp con nhỏ của các đoạn văn được xếp hạng hàng đầu. Nhiều phương pháp truy xuất dựa trên mạng nơ-ron đã xuất hiện kể từ đó cho thiết lập này, lật đổ sự thống trị lâu dài của các phương pháp kiểu tf–idf (Izacard et al., 2022a; Karpukhin et al., 2020; Ma et al., 2021; Formal et al., 2021; Guu et al., 2020; Mao et al., 2021; Johnson et al., 2019). Đồng thời, các cải tiến đã được thực hiện ở phía mô hình đọc của phương pháp đường ống. Fusion-in-Decoder (FiD) (Izacard & Grave, 2021a;b) vẫn là một kiến trúc hiệu quả được thiết kế cho QA, và 'Lost in the Middle' (LitM) (Liu et al., 2023) gần đây đã xác định cách hiệu suất mô hình đọc phụ thuộc vào vị trí của đoạn văn chính xác trong các ngữ cảnh lớn.

Kiến trúc cho đầu vào dài . Theo đuổi khả năng ấn tượng của huấn luyện end-to-end quy mô lớn, cũng đã có sự gia tăng trong các kiến trúc mạng có thể tăng kích thước ngữ cảnh của LMs. Cửa sổ ngữ cảnh tối đa đã nhanh chóng mở rộng từ hàng nghìn token đến nhiều triệu token với việc sử dụng các phương pháp thưa thớt hiệu quả (Wu et al., 2022; Bulatov et al., 2022; Ding et al., 2023). Một số phương pháp sử dụng độ thưa thớt mô phỏng chặt chẽ của truy xuất thông tin đối với các token liên quan hoặc bộ nhớ ngoài (Bertsch et al., 2023; Wu et al., 2022; Bulatov et al., 2022; 2023; Johnson et al., 2019) và một số phương pháp thay vào đó sử dụng ma trận chú ý thưa thớt khối để giảm các tính toán cần thiết của cơ chế chú ý (Beltagy et al., 2020; Zhang et al., 2022a; Ding et al., 2023; Dao et al., 2022).

3 BỐI CẢNH

Ký hiệu. Xem xét một mô hình ngôn ngữ với từ vựng có kích thước V∈N sử dụng các chuỗi đầu vào x∈ X := [V]d và các chuỗi đầu ra y∈ Y := [V]m cho độ dài đầu vào d∈N và độ dài đầu ra tối đa m∈ N, nơi [V] :={1, . . . , V }. Một mô hình sinh tạo văn bản nhận một chuỗi đầu vào các token và định nghĩa một vector xác suất trên tất cả các đầu ra có thể, F:X → [0,1]Y. Do đó, chúng ta có F(x)y biểu thị xác suất của y được tạo ra với đầu vào x.

Để cho phép giải thích thông qua các phương pháp gán thuộc tính đặc trưng như giá trị Shapley, chúng ta cần có thể che một số tập hợp con của các token đầu vào. Gọi s∈ M :={0,1}d là một mặt nạ nhị phân trên các token đầu vào. Chúng tôi sẽ tiếp theo định nghĩa một mô hình sinh tạo văn bản được che, f:X × M → [0,1]Y, nhận cả một chuỗi đầu vào và một mặt nạ đầu vào. Nói cách khác, chúng tôi sẽ thay thế tất cả các token đầu vào không có trong mặt nạ s bằng token <pad> trước khi đưa vào mô hình. Nếu chúng ta giả định token <pad> hoặc <mask> được lấy là p∈[V] và xác định d-vector được tạo thành từ tất cả p là p, thì chúng ta có thể viết điều này là f(x, S) :=F(x⊙s+p⊙(1−s)).

Để cuối cùng định nghĩa các 'hàm giá trị' cần thiết để định nghĩa điểm Shapley, chúng ta phải đầu tiên xác định các mặt nạ nhị phân của chúng ta với các tập hợp con của các đặc trưng đầu vào. Cụ thể, cho bất kỳ phần tử nào của tập hợp con S∈ P([d]) := {S⊆[d]}, có một mặt nạ nhị phân tương ứng duy nhất s∈ {0,1}d thông qua hàm chỉ thị s= 1S. Cho bất kỳ token đầu vào i∈[d], chúng tôi sẽ sử dụng ký hiệu tập hợp (S+i) :=S∪ {i} và (S−i) :=S\ {i} để bỏ che hoặc che token. Cho một x cố định, chúng tôi viết vℓ(S) := log( f(x,1S)) và vp(S) :=f(x,1S) là hai hàm giá trị ứng viên của chúng tôi.

3.1 GIÁ TRỊ SHAPLEY

Giá trị Shapley, ban đầu được phát triển để gán giá trị của các người chơi cá nhân trong một trò chơi hợp tác, từ đó đã trở thành mô hình thống trị để giải thích gán thuộc tính đặc trưng của các mô hình học máy hộp đen (Shapley, 1953; Lundberg & Lee, 2017). Trong Mục 3.2, chúng tôi sẽ mở rộng điều này cho các giá trị Shapley-Shubik và Penrose-Banzhaf được thiết kế cho các trò chơi bỏ phiếu (Shapley & Shubik, 1954; Banzhaf, 1965; Penrose, 1946). Trong Mục 3.3, chúng tôi sẽ mô tả cách áp dụng mở rộng phân cấp được gọi là giá trị Owen (Owen, 1977; Winter, 2002) cho dữ liệu văn bản.

Giá trị Shapley thường được công thức hóa như một kỳ vọng đều trên các hoán vị:
φi=Eπ
vℓ(Sπ,i+i)−vℓ(Sπ,i−i)
(1)
nơi π: [d]→[d] là một hoán vị và kỳ vọng được tính trên phân phối đều của các hoán vị. Nói cách khác, π biểu thị một thứ tự ngẫu nhiên của các đặc trưng (token) và Sπ,i:={j∈[d] : π(j)< π(i)} là tập hợp các phần tử đứng trước i trong thứ tự được định nghĩa bởi π. Do đó, Sπ,i+i={j∈[d] : π(j)≤π(i)} và Sπ,i−i=Sπ,i={j∈[d] : π(j)< π(i)}, nơi chúng tôi không cần thiết trừ phần tử i để chuẩn bị cho Phương trình 2. Chúng tôi theo phương pháp tiêu chuẩn của lấy mẫu hoán vị để ước lượng giá trị Shapley như trung bình thực nghiệm trên một tập hợp hữu hạn các hoán vị được lấy mẫu Covert et al. (2021).

Thách thức chính của việc áp dụng Shapley truyền thống là thực tế chúng ta không có quyền truy cập vào vector xác suất đầy đủ F(x), hiện có kích thước cấp số mũ. Trong các nhiệm vụ phân loại và hồi quy, các log-xác suất có thể được tính chính xác cho mọi đầu ra ứng viên. Tuy nhiên, trong sinh tạo văn bản mở, chúng ta sử dụng các thuật toán giải mã tuần tự như giải mã tham lam và sinh tạo K-beam để khôi phục chỉ một tập hợp con thưa thớt của vector xác suất lớn theo cấp số mũ F(x)∈[0,1][V]m. Trong phần tiếp theo, chúng tôi cho thấy cách thích ứng Shapley để xử lý văn bản được tạo ra từ các phân phối có hỗ trợ không biết trước.

3.2 MỞ RỘNG CHO ĐẦU RA SINH TẠO

Mặc dù giá trị Shapley đã tìm thấy thành công rộng rãi trong các nhiệm vụ phân biệt như phân loại và hồi quy, nó gặp khó khăn khi được áp dụng cho các nhiệm vụ sinh tạo. Để hướng tới điều này, chúng tôi tận dụng công thức hóa lý thuyết bỏ phiếu của giá trị Shapley thông thường, được gọi là chỉ số sức mạnh Shapley-Shubik. Chúng tôi xem xét mỗi token đầu vào như một 'cử tri' bỏ phiếu cho một câu trả lời được tạo, nhằm 'bầu chọn' câu trả lời ưa thích của họ dưới hệ thống bỏ phiếu hộp đen của LM.

Thông thường, Shapley sử dụng một hàm giá trị được biểu diễn như vector của log-xác suất, trong khi Shapley-Shubik hoạt động trên vector xác suất.

Từ đây về sau, chúng tôi sẽ gọi 'chỉ số sức mạnh Shapley-Shubik' là 'Shapley' cho ngắn gọn. Chúng ta có thể công thức hóa tương đương Shapley như một kỳ vọng trên một tập hợp con ngẫu nhiên thay vì trên một hoán vị ngẫu nhiên, làm nổi bật mối liên hệ của nó với giá trị Banzhaf:
φSh
i:=ES∼PSh(S)
[vp(S+i)−vp(S−i)]+
φBz
i:=ES∼PBz(S)
[vp(S+i)−vp(S−i)]+
(2)
nơi PSh(S) là phân phối Shapley PSh(S)∝d−1
(d
|S|)|S|(d−|S|) và phân phối Banzhaf giống như phân phối Bernoulli PBz(S)∝p|S|(1−p)d−|S|. Chúng tôi đặt p= 50% và p= 10% trong các thí nghiệm của chúng tôi.
[·]+ được sử dụng để biểu thị phần dương theo thành phần mà chúng tôi sử dụng để lấy phần dương của hiệu của hai vector xác suất. Các công thức này cung cấp lợi thế lớn là loại bỏ nhu cầu tính toán vector log-xác suất đầy đủ, cho phép chúng ta áp dụng giá trị Shapley cho sinh tạo văn bản.

3.3 MỞ RỘNG CHO ĐẦU VÀO PHÂN CẤP

Tận dụng bản chất phân cấp nội tại của văn bản tự nhiên, phương pháp của chúng tôi tính toán hiệu quả các giá trị Shapley ở các cấp độ chi tiết khác nhau. Ban đầu tính toán các giá trị Shapley ở cấp độ tài liệu, quá trình sau đó tinh chỉnh để bao gồm các câu chỉ từ những tài liệu đóng góp quan trọng. Quá trình có chọn lọc, phân cấp này tiếp tục, dần dần thu hẹp trọng tâm vào các từ nằm trong các câu có điểm cao. Trong khi công trình trước đây như (Jin et al., 2020; Chen et al., 2020) đã khám phá các mở rộng phân cấp sử dụng giá trị Owen, họ chỉ giải quyết các phân cấp nhị phân, thiếu hỗ trợ cho các cấu trúc tổng quát hơn. Chúng tôi tận dụng phân cấp này để phân bổ duy nhất các nguồn lực tính toán cho các token quan trọng trong các đoạn văn và câu đã được xác định là quan trọng, nhận ra rằng không phải mọi token đều đáng được điều tra.

4 TEXTGENSHAP: TĂNG TỐC SINH TẠO GIẢI THÍCH

Trong phần này, chúng tôi giải thích các kỹ thuật tăng tốc được đề xuất trong TextGenSHAP được thiết kế để đẩy nhanh các tính toán Shapley trong các nhiệm vụ mô hình hóa văn bản sinh tạo với ngữ cảnh dài. Đầu tiên, chúng tôi sử dụng giải mã suy đoán để dự đoán các thế hệ văn bản đến từ các đầu vào được lấy mẫu lại, đạt được tăng tốc đáng kể trên các loại mô hình khác nhau. Thứ hai, chúng tôi khai thác các kỹ thuật hiệu quả phần cứng gần đây như Flash Attention (Dao et al., 2022) và kết nối việc thực hiện thưa thớt khối của nó với các kỹ thuật khác thường được sử dụng trong văn học tài liệu dài. Cuối cùng, chúng tôi sử dụng lấy mẫu lại tại chỗ của bộ mã hóa để cải thiện tốc độ của giải thích cấp độ đoạn văn. Chúng tôi cung cấp giải thích chi tiết cho những điều này trong các phần sau.

4.1 TĂNG TỐC ĐẶC THÙ CHO BỘ GIẢI MÃ: GIẢI MÃ SUY ĐOÁN

Chúng tôi sử dụng giải mã suy đoán (Miao et al., 2023; Leviathan et al., 2023) để giảm các lần gọi bộ giải mã trong quá trình lấy mẫu tự động hồi quy trong TextGenSHAP, được mô tả trong Hình 2. Phương pháp này tính toán chính xác các xác suất đầu ra bằng cách đầu tiên đoán chuỗi được giải mã đầy đủ sẽ như thế nào. Trong thuật toán của chúng tôi, khi chúng tôi lấy mẫu lại các tập hợp con khác nhau của token cho cùng một ví dụ đầu vào, chúng tôi dần dần xây dựng tập hợp các câu trả lời ứng viên. Cho mỗi mẫu mới, chúng tôi đầu tiên xác minh xem giải mã argmax có tồn tại trong các đầu ra giải mã suy đoán hay không.

Nếu không, thì chúng tôi cần tạo ra câu trả lời ứng viên mới bằng phương pháp giải mã tự động hồi quy thông thường. Sau đó, chúng tôi ghép câu trả lời mới vào cây giải mã nguyên nhân hiện có, đảm bảo cập nhật ma trận chú ý nguyên nhân để tôn trọng cấu trúc đồ thị của cây giải mã. Không giống như các ứng dụng hiện có (Leviathan et al., 2023), có mức độ không chắc chắn cao trong dự đoán bộ giải mã của chúng, TextGenSHAP áp dụng giải mã suy đoán cho các đầu vào bị nhiễu loạn gần giống với các mẫu đã được giải mã cho phép chúng tôi có tỷ lệ thành công dự đoán cao hơn nhiều. Trong các thí nghiệm của chúng tôi, chúng tôi xác minh rằng một lượng lớn tổng tính toán có thể được tiết kiệm thông qua giải mã suy đoán trong một bước thay vì chạy tuần tự mô hình bộ giải mã.

Ngoài ra, cây giải mã suy đoán của TextGenSHAP có thể được mở rộng thêm để theo dõi các giá trị liên quan quan tâm. Ví dụ, nó có thể theo dõi các log-xác suất được giải mã tại mỗi nút, cho phép tính toán các giá trị Shapley tương phản theo log-xác suất, mà không cần xác định trước. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng giải mã tham lam phù hợp với các nghiên cứu trả lời câu hỏi tài liệu dài khác (Izacard & Grave, 2021a; Liu et al., 2023). Tuy nhiên, chúng tôi nhấn mạnh rằng cây giải mã suy đoán của chúng tôi có thể hỗ trợ thêm các phương pháp lấy mẫu phổ biến khác như tìm kiếm beam và sinh tạo nucleus (Sina et al., 2021; Holtzman et al., 2020) và có thể theo dõi log-xác suất trên tất cả các lá của cây.

4.2 THƯA THỚT KHỐI VÀ FLASH ATTENTION

Chúng tôi tận dụng cơ chế Flash Attention ngày càng phổ biến (Dao et al., 2022) để cải thiện cả hiệu quả bộ nhớ và hiệu suất tốc độ của LMs. Hiệu quả bộ nhớ được cải thiện bằng cách sử dụng một công thức tính toán ma trận chú ý thay thế, có bộ nhớ mở rộng tuyến tính với kích thước đầu vào O(N) thay vì bậc hai O(N2). Tăng tốc thêm được đạt được thông qua việc căn chỉnh các tính toán này để mở rộng hiệu quả với phần cứng GPU hiện đại. Những thích ứng này rất quan trọng trong ngữ cảnh trả lời câu hỏi tài liệu dài, nơi chúng tôi xử lý lên đến 20K token đầu vào với một GPU duy nhất. Kích thước đầu vào như vậy đòi hỏi việc mở rộng bộ nhớ tuyến tính do các phương pháp kiểu Flash Attention cung cấp (Rabe & Staats, 2022; Dao, 2023).

Ngoài ra, chúng tôi tạo ra mối liên hệ giữa Flash Attention và các phát triển gần đây trong kiến trúc tài liệu dài (Izacard & Grave, 2021a; Ding et al., 2023) bằng cách sử dụng ma trận chú ý thưa thớt khối để xử lý đầu vào dài. Với nhu cầu ngày càng tăng cho các sửa đổi như vậy, chúng tôi cũng tái công thức hóa phiên bản gốc của FiD thành một phiên bản kết hợp việc thực hiện thưa thớt khối của Flash Attention. Theo các tiến bộ gần đây như vậy vào kiến trúc hiện đại cho kích thước ngữ cảnh khổng lồ, chúng tôi tin rằng kỹ thuật giải thích mở rộng thưa thớt khối của chúng tôi định vị bản thân tốt để tiếp tục hữu ích trong kỷ nguyên LLMs.

4.3 TĂNG TỐC ĐẶC THÙ CHO BỘ MÃ HÓA: LẤY MẪU LẠI TẠI CHỖ

Trong TextGenSHAP, chúng tôi khai thác thêm cấu trúc độc đáo của các mô hình mã hóa-giải mã dựa trên phân khúc như FiD để có được tăng tốc nhanh hơn đáng kể so với trước đây có thể trong NLP. Cụ thể, chúng tôi tính toán ma trận đặc trưng bộ mã hóa chỉ một lần khi tạo ra toàn bộ giải thích cho một ví dụ duy nhất. Do tính độc lập của các phân đoạn đầu vào được phân khúc, chúng tôi chỉ cần điều chỉnh cơ chế chú ý mã hóa-giải mã để cho phép lấy mẫu lại với các tập hợp con tài liệu khác nhau. Chúng tôi không chỉ giảm đáng kể thời gian tính toán cần thiết để mã hóa lại các đặc trưng đầu vào, mà còn giảm chi phí bộ nhớ từ các lô mã hóa cho phép giải mã song song với kích thước lô giải mã lớn. Tăng kích thước lô giải mã cho phép giải mã hiệu quả phần cứng hơn nhiều, cho phép mô hình lặp qua hàng trăm mẫu hoán vị chỉ trong vài giây.

Trong các thí nghiệm của chúng tôi, chúng tôi kết hợp thêm phương pháp này với tái công thức hóa ma trận chú ý khối-chéo cho việc phân khúc được thảo luận trong Mục 4.2. Bằng cách thay đổi mã hóa đoạn văn để sử dụng hiệu quả việc căn chỉnh phần cứng trong Flash Attention, chúng tôi có thể giữ chú ý tự bộ mã hóa và chú ý chéo mã hóa-giải mã được căn chỉnh như ma trận thưa thớt khối. Các ma trận thưa thớt nhận biết phần cứng như vậy cho phép chúng tôi giảm thiểu các tính toán không cần thiết bằng cách tránh các mục không bằng không không cần thiết vượt qua ranh giới phần cứng và làm chậm tính toán.

TextGenSHAP Trong Thuật toán 1, chúng tôi chi tiết TextGenSHAP, đầu tiên tính toán giá trị Shapley ở cấp độ tài liệu; thứ hai xếp hạng tài liệu và chọn những cái vượt qua ngưỡng quan trọng được xác định trước; và thứ ba tính toán giá trị Shapley ở cấp độ token chỉ cho các token bên trong tài liệu quan trọng. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng phân cấp ba tầng với đoạn văn, câu, và từ; tuy nhiên, để đơn giản ký hiệu, chúng tôi chỉ mô tả phân cấp hai cấp trong mô tả thuật toán của chúng tôi.

5 KẾT QUẢ THỰC NGHIỆM

Bộ dữ liệu Chúng tôi tập trung vào các bộ dữ liệu có sẵn công khai được thiết kế cho nhiệm vụ trả lời câu hỏi miền mở hoặc tài liệu dài: Natural Questions (NQ) (Kwiatkowski et al., 2019) và MIRACL (tập hợp con tiếng Anh) (Zhang et al., 2022b). NQ được thiết kế lại cho trả lời câu hỏi miền mở theo (Lee et al., 2019; Karpukhin et al., 2020). Trong thiết lập này, câu trả lời phải được tìm thấy từ trong toàn bộ Wikipedia, thay vì một trang Wikipedia duy nhất. Bộ dữ liệu NQ gốc cung cấp câu trả lời văn bản ngắn và các đoạn văn được đánh giá là liên quan miễn là chúng chứa câu trả lời chính xác. MIRACL được thiết kế cho truy xuất thông tin và cho mỗi truy vấn nó cung cấp nhãn liên quan nhị phân cho mười đoạn văn liên quan từ kho. Đánh giá liên quan được thực hiện bởi người chú thích con người quyết định xem thông tin đoạn văn có đủ để trả lời câu hỏi đã cho hay không; tuy nhiên, họ không được yêu cầu biện minh hoặc mô tả câu trả lời như một phần của nhãn.

Mô hình Cho xếp hạng đoạn văn của kho (mô hình truy xuất), chúng tôi sử dụng kiến trúc Contriever gần đây (Izacard et al., 2022a) theo LitM. Cho trả lời câu hỏi (mô hình đọc), chúng tôi sử dụng các thành viên khác nhau của họ T5 (Raffel et al., 2020). Chúng tôi sử dụng các mô hình được điều chỉnh flan có sẵn ở kích thước large và XXL ('T5-large' và 'T5-XXL') (Chung et al., 2022) và mô hình T5 large được tinh chỉnh từ FiD ('T5-FiD') (Izacard & Grave, 2021a).

5.1 KẾT QUẢ BENCHMARK TỐC ĐỘ TEXTGENSHAP

Chúng tôi trình bày các benchmark chứng minh tốc độ cải thiện của TextGenSHAP. Đầu tiên, chúng tôi đánh giá giá trị Shapley, cung cấp giải thích chi tiết cấp độ token sử dụng Thuật toán 1 của chúng tôi. Trong Hình 3, chúng tôi benchmark với 100 hoán vị được lấy mẫu và 10 tài liệu từ thiết lập LitM cho cả T5-XXL và T5-large. Một GPU A100 40GB duy nhất được sử dụng để benchmark tất cả các thí nghiệm. Chúng tôi quan sát thấy rằng ước lượng giá trị Shapley tiêu chuẩn yêu cầu 12-20 giờ cấm đoán mỗi mẫu và cho thấy rằng thuật toán lấy mẫu phân cấp được đề xuất của chúng tôi giảm đáng kể thời gian này. Với việc tích hợp giải mã suy đoán, chúng tôi có thể đạt được việc giảm thời gian tính toán thậm chí đáng kể hơn, đưa thời gian tính toán xuống dưới một giờ và khoảng một giờ, tương ứng. Chúng tôi lưu ý rằng tăng tốc bổ sung có thể đạt được trong thiết lập thế giới thực bằng cách chỉ lấy mẫu ít hoán vị hơn. Trong Phụ lục E, chúng tôi cho thấy rằng thậm chí ít hơn 100 mẫu hoán vị có thể đủ. Khi chỉ sử dụng 10 mẫu hoán vị, TextGenSHAP giảm thời gian cho mô hình T5-XXL từ khoảng hai giờ xuống năm phút.

Chúng tôi bổ sung benchmark mô hình T5-FiD được tăng tốc với các sửa đổi đặc thù kiến trúc của nó như được thấy trong Hình 3c. Chúng tôi đưa giải thích cấp độ tài liệu từ nhiều phút xuống dưới mười giây, cho phép cải thiện thời gian thực trong các ứng dụng truy xuất tài liệu mà chúng tôi chứng minh trong Mục 5.3

5.2 TRỰC QUAN HÓA DIỄN GIẢI

Trong Phụ lục, chúng tôi trình bày các trực quan hóa của giải thích dự đoán. Chúng tôi thấy rằng các điểm Shapley phân cấp của chúng tôi hiệu quả trong việc cô lập các token quan trọng từ trong các ngữ cảnh dài hàng nghìn token. Chúng tôi cũng cung cấp các trực quan hóa tương tác được lưu trữ tại đây.

5.3 CHƯNG CẤT TÀI LIỆU

Chúng tôi cho thấy giá trị của các điểm giải thích được đề xuất trong TextGenSHAP trong ngữ cảnh truy xuất tài liệu cho QA miền mở. Chúng tôi đầu tiên áp dụng phương pháp của chúng tôi để cải thiện khía cạnh truy xuất, đặc biệt là nâng cao khả năng nhớ lại của mô hình truy xuất được sửa đổi, bằng cách xếp hạng lại các đoạn văn theo điểm giải thích của chúng.

Hình 4a cho thấy cải thiện nhớ lại đáng kể trên bộ dữ liệu NQ, với cả ba phương pháp giải thích đều thể hiện cải thiện hiệu suất tương tự so với mô hình truy xuất cơ sở. Bảng 1 cung cấp đánh giá số về diện tích dưới đường cong cho các mô hình này. Tuy nhiên, Hình 4b cho thấy cải thiện ít rõ ràng hơn trên bộ dữ liệu MIRACL thách thức hơn, chủ yếu do thông tin nhãn thưa thớt hơn chỉ cung cấp nhãn liên quan cho mười trong số hàng triệu đoạn văn. Chúng tôi xác minh tuyên bố này bằng cách mở rộng thông tin nhãn sử dụng nhãn giả. Cụ thể, chúng tôi lấy tất cả các đoạn văn liên quan theo nhãn MIRACL và yêu cầu T5-XXL đưa ra câu trả lời ngắn chỉ theo đoạn văn đó. Sau đó chúng tôi tận dụng tập hợp câu trả lời ứng viên này để đánh giá liên quan đoạn văn theo cách tương tự như bộ dữ liệu NQ. Trong Hình 4c, chúng tôi thấy điều này không chỉ cải thiện nhớ lại tổng thể, mà còn tăng cường không cân xứng thành công của hiệu suất phương pháp giải thích của chúng tôi.

Chúng tôi coi điều này như bằng chứng sơ bộ về tiềm năng của phương pháp chúng tôi để khám phá các đoạn văn liên quan thường bị bỏ qua khi chỉ sử dụng các mô hình truy xuất dựa trên độ tương tự. Theo đó, chúng tôi đề xuất rằng phương pháp của chúng tôi có thể được áp dụng thêm để nâng cao các quy trình xây dựng bộ dữ liệu bằng cách không chỉ giảm gánh nặng chú thích con người thông qua việc định vị các đặc trưng tài liệu quan trọng, mà còn bằng cách thu thập một tập hợp tài liệu đa dạng hơn để được chú thích bởi con người so với có thể với các phương pháp hiện có. Chúng tôi khám phá các ứng dụng như vậy trên bộ dữ liệu MIRACL trong Phụ lục E.

Trong ứng dụng thứ hai của chúng tôi, chúng tôi đề xuất sử dụng các giá trị Shapley từ mô hình đọc để chưng cất tập hợp tài liệu có sẵn của chính nó. Kết hợp với các phát hiện trong LitM (Liu et al., 2023), làm nổi bật các thách thức cho các mô hình đọc trong việc sử dụng các ngữ cảnh dài hơn, chúng tôi chưng cất lại các tài liệu của mô hình trước khi đạt được câu trả lời cuối cùng. Chúng tôi đánh giá độ chính xác top-K cho các giá trị nhỏ của K, cho phép mô hình đọc sử dụng một loạt đa dạng thông tin liên quan, và thu hẹp khoảng cách giữa khả năng nhớ lại của truy xuất và độ chính xác của đọc. Đánh giá này làm nổi bật tầm quan trọng của việc cung cấp một tập hợp đa dạng các câu trả lời ứng viên. Hình 5 minh họa cải thiện độ chính xác đạt được bởi mô hình được chưng cất lại so với cơ sở bỏ phiếu đa số. Chúng tôi thấy rằng TextGenSHAP vượt trội đáng kể so với cơ sở chỉ với một lần đi qua mô hình đọc, và vượt xa hơn nữa cơ sở bỏ phiếu đa số cho nhiều câu trả lời. Chúng tôi một lần nữa cung cấp so sánh số sử dụng AUC trong Bảng 2.

6 KẾT LUẬN

Trong bài báo này, chúng tôi giới thiệu TextGenSHAP để nâng cao giá trị Shapley, một phương pháp giải thích đáng tin cậy, để giải quyết các thách thức trong các ứng dụng NLP hiện đại có đặc trưng đầu vào dài, mô hình lớn, và sinh tạo văn bản. Chúng tôi giới thiệu các sửa đổi để thích ứng giá trị Shapley cho văn bản đầu vào có cấu trúc phân cấp và các thế hệ đầu ra được giải mã tự động hồi quy, dựa trên những hiểu biết từ văn học lý thuyết trò chơi để hỗ trợ nền tảng lý thuyết của chúng. Ngoài ra, chúng tôi kết hợp nhiều sửa đổi kiến trúc đặc thù cho transformer tăng tốc đáng kể việc tạo ra giải thích. Phương pháp của chúng tôi không chỉ tăng tốc tính toán giá trị Shapley cho văn bản được tạo ra mà còn chứng minh hiệu quả của nó trong việc cải thiện hiệu suất trong một nhiệm vụ trả lời câu hỏi tiêu chuẩn. Chúng tôi hy vọng rằng các phương pháp giải thích như vậy sẽ tiếp tục tìm thấy khả năng áp dụng rộng rãi trong nhiều trường hợp sử dụng LLM.

TÀI LIỆU THAM KHẢO

Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/ file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf .

Ines Arous, Ljiljana Dolamic, Jie Yang, Akansha Bhardwaj, Giuseppe Cuccu, and Philippe Cudr ´e-Mauroux. Marta: Leveraging human rationales for explainable text classification. Proceedings of the AAAI Conference on Artificial Intelligence , 35(7):5868–5876, May 2021. doi: 10.1609/aaai.v35i7.16734. URL https://ojs.aaai.org/index.php/AAAI/article/view/16734 .

John F. III Banzhaf. Weighted Voting Doesn't Work: A Mathematical Analysis , volume 19, pp. 317–344. 1965.

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020.

Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range transformers with unlimited length input. 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.

Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems , volume 35, pp. 11079–11091. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf .

Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to 1m tokens and beyond with rmt. 2023.

Aaron Chan, Maziar Sanjabi, Lambert Mathias, Liang Tan, Shaoliang Nie, Xiaochang Peng, Xiang Ren, and Hamed Firooz. UNIREX: A unified learning framework for language model rationale extraction. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp. 2867–2889. PMLR, 17–23 Jul 2022.

Hanjie Chen, Guangtao Zheng, and Yangfeng Ji. Generating hierarchical explanations on text classification via feature interaction detection. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 5578–5593, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.494. URL https://aclanthology.org/2020.acl-main.494 .

Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. L-shapley and c-shapley: Efficient model interpretation for structured data. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=S1E3Ko09F7 .

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.

Ian Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. Journal of Machine Learning Research , 22(209):1–90, 2021. URL http://jmlr.org/ papers/v22/20-1316.html .

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. 2023.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems , volume 35, pp. 16344–16359, 2022.

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. 2023.

Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and St ´ephane Clinchant. Splade v2: Sparse lexical and expansion model for information retrieval. 2021.

Amirata Ghorbani, Abubakar Abid, and url=https://ojs.aaai.org/index.php/AAAI/article/view/4252 DOI=10.1609/aaai.v33i01.33013681 number=01 journal=Proceedings of the AAAI Conference on Artificial Intelligence James Zou, volume=33. Interpretation of neural networks is fragile. pp. 3681–3688, Jul. 2019.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning , ICML'20. JMLR.org, 2020.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations , 2020. URL https://openreview. net/forum?id=rygGQyrFvH .

Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, and Bryan Catanzaro. Raven: In-context learning with retrieval augmented encoder-decoder language models. arXiv preprint arXiv:2308.07922 , 2023.

Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 874–880. Association for Computational Linguistics, 2021a. URL https://aclanthology.org/2021.eacl-main.74 .

Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. In International Conference on Learning Representations , 2021b. URL https://openreview.net/ forum?id=NTEz-6wysdb .

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research , 2022a. ISSN 2835-8856. URL https://openreview.net/forum? id=jKN1pXi7b0 .

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022b.

Alon Jacovi and Yoav Goldberg. Aligning faithful interpretations with their social attribution. Transactions of the Association for Computational Linguistics , 9:294–310, 2021. doi: 10.1162/tacl a00367. URL https://aclanthology.org/2021.tacl-1.18 .

Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, and Yoav Goldberg. Contrastive explanations for model interpretability. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 1597–1611, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.120. URL https://aclanthology.org/2021.emnlp-main.120 .

Sarthak Jain and Byron C. Wallace. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 3543–3556, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1357. URL https: //aclanthology.org/N19-1357 .

Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In Lee, and Rajesh Ranganath. FastSHAP: Real-time shapley value estimation. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=Zq2G_VTV53T .

Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, and Xiang Ren. Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=BkxRRkSKwr .

Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data , 7(3):535–547, 2019. doi: 10.1109/TBDATA.2019.2921572.

Brihi Joshi, Aaron Chan, Ziyi Liu, Shaoliang Nie, Maziar Sanjabi, Hamed Firooz, and Xiang Ren. ERtest: Evaluating explanation regularization methods for language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022 , pp. 3315–3336, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.242. URL https://aclanthology. org/2022.findings-emnlp.242 .

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6769–6781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550 .

Meinard Kuhlmann. Quantum Field Theory. In Edward N. Zalta and Uri Nodelman (eds.), The Stanford Encyclopedia of Philosophy . Metaphysics Research Lab, Stanford University, Summer 2023 edition, 2023.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452–466, 2019. doi: 10.1162/tacl a00276. URL https://aclanthology.org/ Q19-1026 .

Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/anthology/P19-1612 .

Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning , 2023. URL https://openreview.net/ forum?id=C9NEblP8vS .

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. 2023.

Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 8a20a8621978632d76c43dfd28b67767-Paper.pdf .

Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. Towards faithful model explanation in nlp: A survey, 2023.

Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin. A replication study of dense passage retriever, 2021.

Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-augmented retrieval for open-domain question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 4089–4100, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.316. URL https: //aclanthology.org/2021.acl-long.316 .

Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification, 2023.

Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for shapley value estimation. Journal of Machine Learning Research , 23(43):1–46, 2022. URL http://jmlr.org/ papers/v23/21-0439.html .

Edoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel Gallagher, and Georg Groh. SHAP-based explanation methods: A review for NLP interpretability. In Proceedings of the 29th International Conference on Computational Linguistics , pp. 4593–4603, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/2022. coling-1.406 .

Guilliermo Owen. Values of games with a priori unions. In Rudolf Henn and Otto Moeschlin (eds.), Mathematical Economics and Game Theory , pp. 76–88, Berlin, Heidelberg, 1977. Springer Berlin Heidelberg. ISBN 978-3-642-45494-3.

L. S. Penrose. The elementary statistics of majority voting. 109(1):53 – 57, 1946. doi: https://doi.org/10. 2307/2981392.

Markus N. Rabe and Charles Staats. Self-attention does not need o(n2)memory. 2022.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020. URL http://jmlr.org/papers/ v21/20-074.html .

Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083 , 2023.

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp. 1135–1144. ACM, 2016.

Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead, 2019.

L. S. Shapley. A Value for n-Person Games , volume 2, pp. 307–318. Princeton University Press, Princeton, 1953. ISBN 9781400881970. doi: doi:10.1515/9781400881970-018. URL https://doi.org/10. 1515/9781400881970-018 .

L. S. Shapley and Martin Shubik. A method for evaluating the distribution of power in a committee system. 48(3), 1954.

Zarriess Sina, Henrik V oigt, and Simeon Sch ¨uz. Decoding methods in neural language generation: A survey. Information , 12(9), 2021. ISSN 2078-2489. doi: 10.3390/info12090355. URL https://www.mdpi. com/2078-2489/12/9/355 .

Joe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations for robust natural language inference. Proceedings of the AAAI Conference on Artificial Intelligence , 36(10): 11349–11357, Jun. 2022. doi: 10.1609/aaai.v36i10.21386. URL https://ojs.aaai.org/index. php/AAAI/article/view/21386 .

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pp. 3319–3328. JMLR. org, 2017.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.

Junlin Wang, Jens Tuyls, Eric Wallace, and Sameer Singh. Gradient-based analysis of NLP models is manipulable. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pp. 247–258, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.24. URL https://aclanthology.org/2020.findings-emnlp.24 .

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems , volume 35, pp. 24824–24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf .

Eyal Winter. Chapter 53 the shapley value. volume 3 of Handbook of Game Theory with Economic Applications , pp. 2025–2054. Elsevier, 2002. doi: https://doi.org/10.1016/S1574-0005(02)03016-3. URL https://www.sciencedirect.com/science/article/pii/S1574000502030163 .

Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations , 2022. URL https://openreview.net/ forum?id=TrjbxzRcnf- .

Kayo Yin and Graham Neubig. Interpreting language models with contrastive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp. 184–198, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.14. URL https://aclanthology.org/2022.emnlp-main.14 .

Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. Poolingformer: Long document modeling with pooling attention. 2022a.

Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. Making a MIRACL: Multilingual information retrieval across a continuum of languages. arXiv:2210.09984 , 2022b.

Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey, 2023.

Yiming Zheng, Serena Booth, Julie Shah, and Yilun Zhou. The irrationality of neural rationale models. In Apurv Verma, Yada Pruksachatkun, Kai-Wei Chang, Aram Galstyan, Jwala Dhamala, and Yang Trista Cao (eds.), Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022) , pp. 64–73, Seattle, U.S.A., July 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.trustnlp-1.6. URL https://aclanthology.org/2022.trustnlp-1.6 .

A XÁC MINH LẠI LITM

Chúng tôi sử dụng nhiều thí nghiệm để hiểu mức độ của các tuyên bố trong LitM. Cụ thể, chúng tôi xác minh thêm mức độ phụ thuộc của nó vào phân phối bán tổng hợp được giới thiệu bởi các tác giả trong đó. Có một vài giả định chính được đưa ra trong phân phối bán tổng hợp này (về việc cấy một tài liệu duy nhất giữa một tập hợp các tài liệu phân tán) có thể không luôn đúng trong các kịch bản thực tế. Đầu tiên, số lượng tài liệu thật được truy xuất trong các hệ thống thế giới thực có thể lớn hơn hoặc ít hơn một. Thứ hai, thứ tự và mức độ liên quan của các tài liệu phân tán có thể thay đổi theo hệ thống truy xuất được sử dụng và theo tài liệu trong kho.

Hình 6: Tái tạo LitM

Đối với cả ba mô hình đọc chúng tôi sử dụng, chúng tôi xác minh giả thuyết từ bài báo LitM về ảnh hưởng của vị trí tài liệu đối với hiệu suất mô hình. Trong Hình 6, chúng tôi thực sự thấy đối với các mô hình được huấn luyện theo cách thông thường như T5-large và T5-XXL, chúng tôi thực sự xác minh lại giả thuyết của LitM cho thấy sự suy giảm hiệu suất mô hình bất cứ khi nào câu trả lời đúng được đặt về phía trung tâm của cửa sổ ngữ cảnh rất dài. Chúng tôi bổ sung so sánh hiệu suất của mô hình T5-FiD bất biến hoán vị. Ở đây, chúng tôi do đó thấy rằng kiến trúc mô hình được huấn luyện để thực hiện nhiệm vụ trả lời câu hỏi tài liệu dài có thể tăng hiệu suất so với mô hình T5-large gốc. Thực tế, chúng tôi thấy rằng đối với một số phần của đường cong LitM, mô hình T5-FiD nhỏ hơn có thể vượt trội so với mô hình T5-XXL lớn hơn nhiều.

Hình 7: Kích thước hiệu ứng của đường cong hình chữ U LitM được giảm dưới các tài liệu phân tán khác nhau.

Để tiếp tục thăm dò các phát hiện từ bài báo LitM, chúng tôi điều tra thêm cách thay đổi các tài liệu phân tán trong ngữ cảnh sẽ thay đổi kích thước hiệu ứng của đường cong LitM. Thay vì lấy 10 đoạn văn liên quan nhất để phục vụ như các tài liệu phân tán như được thực hiện trong bài báo LitM gốc, chúng tôi xem xét việc lấy một số đoạn văn được truy xuất ít liên quan hơn. Hình 7 cho thấy rằng việc thực hiện thay đổi này đối với thiết lập bán tổng hợp thực sự làm giảm độ sâu của đường cong hình bát LitM.

B CHI TIẾT THỰC NGHIỆM

B.1 MÔ HÌNH VÀ BỘ DỮ LIỆU

Bộ dữ liệu Natural Questions (NQ) (Kwiatkowski et al., 2019) là một bộ dữ liệu ban đầu được thiết kế cho trả lời câu hỏi tài liệu dài, nơi cả đoạn văn liên quan và câu trả lời cuối cùng phải được chọn từ một trang Wikipedia duy nhất. NQ được thiết kế lại cho trả lời câu hỏi miền mở theo (Lee et al., 2019; Karpukhin et al., 2020) chuyển đổi Wikipedia thành kho các đoạn văn thay vì trang, và chỉ yêu cầu đưa ra câu trả lời cuối cùng có thể được tìm thấy trong số các đoạn văn đó. Bộ dữ liệu NQ gốc cung cấp câu trả lời văn bản ngắn và các đoạn văn được đánh giá là liên quan miễn là chúng chứa câu trả lời đúng.

MIRACL (Zhang et al., 2022b). là một bộ dữ liệu được thiết kế cho truy xuất thông tin trên các đoạn văn Wikipedia. Sử dụng điểm truy xuất thông tin hiện có, bộ dữ liệu đã chọn mười đoạn văn liên quan nhất từ kho và gắn nhãn mỗi cái là liên quan hoặc không liên quan đến câu hỏi đang xem xét. Đánh giá liên quan được thực hiện bởi người chú thích con người quyết định xem thông tin đoạn văn có đủ để trả lời câu hỏi đã cho hay không; tuy nhiên, họ không được yêu cầu biện minh hoặc mô tả câu trả lời như một phần của nhãn. Theo đó, chỉ một số ít đoạn văn có thông tin nhãn đánh giá đơn lẻ đúng. Điều này tạo thành tín hiệu thưa thớt hơn nhiều so với bộ dữ liệu NQ cho phép bất kỳ đoạn văn nào chứa câu trả lời văn bản đúng được coi là liên quan. Chính vì lý do này chúng tôi tạo ra nhãn giả dựa trên các đoạn văn MIRACL liên quan để đánh giá lại các đoạn văn MIRACL sử dụng cùng tiêu chí như NQ. Trong công việc này, chúng tôi chỉ tập trung vào tập hợp con của MIRACL sử dụng truy vấn tiếng Anh và đoạn văn tiếng Anh.

Mô hình Chúng tôi theo đường ống hai giai đoạn tiêu chuẩn của ODQA, đầu tiên sử dụng mô hình truy xuất để chọn một tập hợp con các đoạn văn liên quan từ kho khổng lồ và thứ hai sử dụng mô hình đọc để trích xuất câu trả lời của câu hỏi từ tập hợp con các đoạn văn liên quan.

Cho xếp hạng đoạn văn của kho (mô hình truy xuất), chúng tôi sử dụng kiến trúc Contriever gần đây (Izacard et al., 2022a) theo LitM, sử dụng FAISS để lập chỉ mục các nhúng Johnson et al. (2019). Cho trả lời câu hỏi (mô hình đọc), chúng tôi sử dụng các thành viên khác nhau của họ T5 (Raffel et al., 2020). Chúng tôi sử dụng các mô hình được điều chỉnh flan có sẵn ở kích thước large và XXL ('T5-large' và 'T5-XXL') (Chung et al., 2022) và mô hình T5 large được tinh chỉnh từ FiD ('T5-FiD') (Izacard & Grave, 2021a). Cụ thể, những cái này tương ứng với flan-t5-large và flan-t5-xxl có sẵn từ Chung et al. (2022) ban đầu được huấn luyện trên các ngữ cảnh có độ dài 512. T5-FiD tương ứng với nqreader large từ Izacard & Grave (2021a) ban đầu được huấn luyện trên độ dài ngữ cảnh của một trăm đoạn văn được truy xuất từ truy xuất được đồng huấn luyện của họ. Mặc dù kích thước của độ dài ngữ cảnh huấn luyện, việc áp dụng các mô hình như vậy vượt ra ngoài độ dài ngữ cảnh được huấn luyện ban đầu của chúng khi được áp dụng cho nhiệm vụ trả lời câu hỏi tài liệu dài Liu et al. (2023) (có thể thực hiện được do độ lệch vị trí tương đối được thực hiện trong T5) là phổ biến.

B.2 KẾT QUẢ BỔ SUNG

Ở đây chúng tôi cung cấp các kết quả bổ sung cho các giá trị khác nhau của số hoán vị được sử dụng để tạo ra giải thích trước khi đánh giá. Vì đây là nút chính cho các thuật toán dựa trên lấy mẫu để đánh đổi giữa độ chính xác ước lượng và độ phức tạp thời gian, chúng tôi tính toán các số liệu AUC của ứng dụng mục tiêu của chúng tôi trên tất cả các cấp độ hoán vị để cho thấy các hiệu ứng khác nhau. Chúng tôi thấy rằng thậm chí chỉ với mười hoán vị, chúng tôi đang nhận được nhiều điểm AUC nhớ lại trong hệ thống truy xuất thông tin end-to-end.

Bảng 3: AUC cho 3 hoán vị.
Natural MIRACL MIRACL
Questions (Gốc) (Giả)
Cơ sở 84.23 80.18 84.53
TextGenSHAP 86.01 69.58 84.71
TextGenBANZ 85.76 72.84 84.80
TextGenBANZ-10 87.53 79.08 85.40

Bảng 4: AUC cho 10 hoán vị.
Natural MIRACL MIRACL
Questions (Gốc) (Giả)
Cơ sở 84.23 80.18 84.53
TextGenSHAP 87.50 74.52 85.39
TextGenBANZ 87.86 75.65 85.71
TextGenBANZ-10 88.61 81.39 86.27

Bảng 5: AUC cho 30 hoán vị.
Natural MIRACL MIRACL
Questions (Gốc) (Giả)
Cơ sở 84.23 80.18 84.53
TextGenSHAP 88.31 76.71 85.97
TextGenBANZ 88.51 76.88 86.27
TextGenBANZ-10 88.77 82.15 86.60

Bảng 6: AUC cho 100 hoán vị.
Natural MIRACL MIRACL
Questions (Gốc) (Giả)
Cơ sở 84.23 80.18 84.53
TextGenSHAP 88.53 77.33 86.43
TextGenBANZ 88.56 78.19 86.17
TextGenBANZ-10 88.74 82.38 86.53

C CHI TIẾT THÊM VỀ GIÁ TRỊ SHAPLEY

Như một lời nhắc, chúng tôi xem xét một mô hình ngôn ngữ F: [V]d→[0,1][V]m và chúng tôi lấy f(x, S) :=F(x⊙s+ p⊙(1−s)) để định nghĩa một mô hình ngôn ngữ được che f: [V]d× {0,1}d→[0,1][V]m nơi đầu vào, mặt nạ đầu vào, và đầu ra là x∈[V]d, s∈ {0,1}d, và y∈[V]m, tương ứng. Chúng tôi xem xét một hàm giá trị v:P([d])→RM cho M=Vm, và xem xét các lựa chọn hàm giá trị như log-xác suất hoặc xác suất: vℓ(S) := log( f(x,1S)) và vp(S) :=f(x,1S). Vui lòng tham khảo lại phần ký hiệu trong văn bản chính để biết chi tiết đầy đủ nếu cần thiết.

C.1 GIÁ TRỊ SHAPLEY

Giá trị Shapley là một khái niệm giải pháp tồn tại lâu từ văn học lý thuyết trò chơi, ban đầu được thiết kế để gán chính xác giá trị của mỗi người chơi cá nhân trong một trò chơi hợp tác tạo thành liên minh (Shapley, 1953). Trong những năm gần đây, khái niệm giải pháp này đã được tái sử dụng hướng tới mục tiêu giải thích các mô hình học máy hộp đen, coi mỗi đặc trưng cá nhân như một người chơi và chia đều đầu ra dự đoán một cách chính xác giữa các đặc trưng (Lundberg & Lee, 2017). Giữa thời gian này, tuy nhiên, nhiều tiến bộ thêm trong văn học lý thuyết trò chơi xây dựng dựa trên công trình đột phá của Shapley đã tiếp tục tiến triển. Trong đây, chúng tôi tập trung vào một vài mở rộng như vậy của giá trị Shapley ban đầu khi chúng tôi áp dụng chúng cho dữ liệu có cấu trúc cụ thể của các mô hình sinh tạo văn bản sang văn bản.

Tiến bộ đầu tiên như vậy xảy ra chỉ ngay sau khi khái niệm giá trị Shapley ban đầu được tạo ra; chỉ số sức mạnh Shapley-Shubik là một tái công thức hóa của giá trị Shapley ban đầu thay vào đó được thiết kế cho các trò chơi bỏ phiếu (Shapley & Shubik, 1954). Ở đây, giá trị Shapley-Shubik đo lường lượng sức mạnh hoặc ảnh hưởng mà mỗi cử tri có để ảnh hưởng đến kết quả bỏ phiếu. Cũng trong danh mục các trò chơi bỏ phiếu, chỉ số Penrose-Banzhaf (hoặc thường được gọi là chỉ số sức mạnh Banzhaf) lần đầu tiên được khám phá bởi Penrose (Penrose, 1946) và sau đó được khám phá độc lập bởi Banzhaf (Banzhaf, 1965). Thậm chí bây giờ, cả Banzhaf và Shapley-Shubik vẫn là hai trụ cột được tôn trọng về cách đánh giá hiệu quả cấu trúc của một trò chơi bỏ phiếu.

Theo hướng mở rộng thêm cho giá trị Shapley, Owen sau đó đã mở rộng giá trị Shapley để bổ sung xử lý cấu trúc phân cấp hai cấp (Owen, 1977). Cụ thể, người ta có thể tưởng tượng rằng các người chơi tạo thành liên minh trong một tổ chức nhưng hơn nữa các tổ chức bản thân chúng tạo thành liên minh với nhau. Giá trị có thể được định nghĩa thêm cho các cấu trúc phân cấp đa cấp và đôi khi được gọi là giá trị Owen-Winter (Winter, 2002). Mở rộng tương ứng cho giá trị Banzhaf thay vào đó thường được coi là đơn giản hơn và cũng được gọi là giá trị Banzhaf. Trong công việc này, chúng tôi sử dụng kết hợp của tất cả các phương pháp được liệt kê để có thể áp dụng giải thích kiểu SHAP (Lundberg & Lee, 2017) của các thuật toán học máy trong trường hợp các mô hình transformer chuỗi sang chuỗi, thích ứng với cấu trúc phân cấp của văn bản đầu vào và cấu trúc tự động hồi quy của văn bản đầu ra.

Giá trị Shapley thường được công thức hóa như một kỳ vọng đều trên các hoán vị, dẫn đến việc xấp xỉ thông qua lấy mẫu hoán vị:
φi=Eπ
vℓ(Sπ,i+i)−vℓ(Sπ,i−i)
=1
|Sd|X
π∈Sd
vℓ(Sπ,i+i)−vℓ(Sπ,i−i)
(3)
nơi π∈ Sd:={π: [d]→[d] :π là song ánh} là tập hợp các hoán vị có kích thước d và kỳ vọng được tính trên phân phối đều của các hoán vị. Nói cách khác, π biểu thị một thứ tự ngẫu nhiên của các đặc trưng (token) và Sπ,i:={j∈[d] :π(j)< π(i)} là tập hợp các phần tử đứng trước i trong thứ tự được định nghĩa bởi π. Do đó, Sπ,i+i={j∈[d] :π(j)≤π(i)} và Sπ,i−i=Sπ,i={j∈[d] :π(j)< π(i)}.

Chúng ta có thể viết giá trị Shapley một cách tương đương như trung bình trên phân phối cảm ứng trên các tập hợp con S∈ P([d]):
φi=ES∼PSh(S)
vℓ(S+i)−vℓ(S−i)
=X
S⊆[d]d−1d
|S|
|S|(d− |S|)·
vℓ(S+i)−vℓ(S−i)
(4)
nơi PSh(S) là phân phối Shapley PSh(S)∝d−1
(d
|S|)|S|(d−|S|).

Vì tất cả các định nghĩa như vậy của khái niệm giải pháp này liên quan đến ít nhất một lượng thuật ngữ cấp số mũ để tính toán chính xác, phương pháp tiêu chuẩn trong văn học là sử dụng lấy mẫu hoán vị (Covert et al., 2021; Mitchell et al., 2022). Trong công việc này, chúng tôi cũng theo phương pháp lấy mẫu hoán vị, thực hiện các điều chỉnh cần thiết để áp dụng cho cấu trúc phân cấp như được mô tả trong Thuật toán 1.

C.2 SHAPLEY-SHUBIK

Sự khởi hành quan trọng đầu tiên của chúng tôi từ văn học Shapley hiện có là có thể xử lý trường hợp các chuỗi đầu ra được giải mã tự động hồi quy. Tất cả các giải thích sau xử lý hiện có bao gồm dựa trên chú ý, dựa trên gradient, và các phương pháp dựa trên nhiễu loạn không thể được áp dụng trực tiếp cho các thế hệ văn bản. Chi tiết thêm về những thiếu sót này của các công trình hiện có được mô tả thêm trong Mục 2. Trong các ứng dụng như vậy cho sinh tạo văn bản khi chúng tồn tại, được thực hiện tự động hồi quy, giải thích từng token đầu ra riêng lẻ đôi khi thậm chí không xem xét các đầu ra được giải mã xảy ra trước mỗi đầu ra tự động hồi quy. Điều này không chỉ đặt ra thách thức trực quan hóa nghiêm trọng khi các đầu ra được giải mã trở nên dài hơn và dài hơn trong kỷ nguyên LLMs, mà còn các mối tương quan của giải thích giữa các token đầu ra liền kề thường bị xử lý không đúng cách.

Thách thức này bắt nguồn từ thực tế rằng khi sử dụng các mô hình chuỗi sang chuỗi tự động hồi quy, vector xác suất đầu ra đầy đủ không bao giờ được tính toán. Chúng ta cần sử dụng các kế hoạch giải mã như giải mã tham lam, sinh tạo K-beam, hoặc giải mã nucleus để xấp xỉ các phần có khả năng nhất của không gian sinh tạo đầu ra. Trái ngược với các phương pháp sau xử lý hiện có, phương pháp của chúng tôi có thể giải thích toàn bộ chuỗi đầu ra bằng cách tái công thức hóa Shapley thành công thức Shapley-Shubik trên vector xác suất và tạo ra một giải thích trên toàn bộ chuỗi dự đoán.

Thuật toán 1 Mã giả cho tính toán Shapley phân cấp hiệu quả
1:Đầu vào : mẫu dữ liệu x∈[V]d, mô hình sinh tạo văn bản được che f: [V]d× {0,1}d→[V]m, số đoạn văn p∈N, số token d∈N, phân vùng phân cấp của token P= (S1, . . . , S p)
2:Tham số : ngưỡng phân cấp τ, số mẫu T
3:Đầu ra : các giá trị Shapley được tính toán ở cấp độ tài liệu {φk}k∈[p] và cấp độ token {φk,i}k∈I,i∈Sk
4:
5:function RAND PERM(N)
6: return {hoán vị ngẫu nhiên của N}
7:function ONESHAPLEY PATH(f,P,I,φk,φk,i)
8: π←RAND PERM(p),S← ∅, text curr←" " ▷Khởi tạo vòng lặp
9: for k= 1 : p do
10: if k /∈ I then ▷Trường hợp 1: Thêm tất cả token của tài liệu không quan trọng vào S
11: S←S∪Sπ(k) ▷Thêm toàn bộ tài liệu
12: if f(x; 1S)̸=text curr then
13: Tăng số đếm của văn bản f(x; 1S) trong φπ(k) lên một
14: text curr←f(x; 1S)
15: else ▷Trường hợp 2: Thêm token của tài liệu quan trọng từng cái một
16: πk←RAND PERM(Sk) ▷Thứ tự ngẫu nhiên của token trong tài liệu
17: for i∈Sk do ▷Lặp qua từng token trong tài liệu
18: S←S∪ {πk(i)} ▷Thêm một token duy nhất
19: if f(x; 1S)̸=text curr then
20: Tăng số đếm của văn bản f(x; 1S) trong φπ(k),πk(i) lên một
21: text curr←f(x; 1S)
22:
23:function HIERARCHICAL SHAPLEY
24: Khởi tạo φk←⃗0, cho mỗi k∈[p]
25: Khởi tạo φk,i←⃗0 cho mỗi k∈[p],i∈Sk
26: for t= 1 : T do
27: ONESHAPLEY PATH(f, P,∅, φk, φk,i) ▷Đầu tiên, chỉ lấy mẫu ở cấp độ tài liệu
28: I ← { k∈[p] :φk/S≥τ} ▷Chọn tập hợp các tài liệu quan trọng
29: for t= 1 : T do
30: ONESHAPLEY PATH(f, P,I, φk, φk,i) ▷Thứ hai, lấy mẫu ở cấp độ token cho các tài liệu nhất định
31: return {φk}k∈[p],{φk,i}k∈[p],i∈Sk

Chúng tôi định nghĩa các giá trị Shapley-Shubik và Banzhaf như :
φSh
i:=ES∼PSh(S)
[vp(S+i)−vp(S−i)]+
φBz
i:=ES∼PBz(S)
[vp(S+i)−vp(S−i)]+
(5)
nơi PSh(S) là phân phối Shapley PSh(S)∝d−1
(d
|S|)|S|(d−|S|) và phân phối Banzhaf giống như phân phối Bernoulli PBz(S)∝p|S|(1−p)d−|S|.

Theo đó, giải thích Shapley của chúng tôi sẽ được định nghĩa tốt ngay cả trên các vector xác suất thưa thớt vp được cảm ứng bởi tất cả các thuật toán giải mã tự nhiên. Chính vì lý do này chúng tôi có thể tạo ra giải thích trên toàn bộ đầu ra dự đoán không giống như các phương pháp SHAP hiện có, xử lý văn bản được tạo ra từ các phân phối có hỗ trợ không biết trước.

C.3 CÁC BIẾN THỂ HIỆN CÓ CHO ỨNG DỤNG NLP

C.3.1 CÁC BIẾN THỂ PHÂN CẤP

Trong văn học về Shapley cho NLP hoặc giải thích dựa trên nhiễu loạn cho NLP, đã có các phương pháp tận dụng cấu trúc tuần tự và/hoặc phân cấp của dữ liệu NLP. Trong phần này, chúng tôi làm nổi bật những điểm tương đồng và khác biệt của các phương pháp hiện có. Một trong những phương pháp sớm nhất sử dụng các phiên bản có cấu trúc của giá trị Shapley, (Chen et al., 2019) định nghĩa một giá trị Shapley chỉ có thể xem xét các liên minh với láng giềng của nó (sử dụng cấu trúc tuyến tính cho dữ liệu văn bản) có nghĩa là các tương tác từ sẽ chỉ trải dài qua các cụm từ liền kề. Công trình này không sử dụng rõ ràng cấu trúc phân cấp thêm của dữ liệu văn bản, nhưng vẫn sử dụng cấu trúc đầu vào của thông tin văn bản. Một trong những công trình sớm nhất sử dụng cấu trúc phân cấp, (Jin et al., 2020), sử dụng các phân cấp ngữ pháp được gắn nhãn bởi con người đến từ bộ dữ liệu phân loại tình cảm SST-2 để hỗ trợ trong việc tạo ra giải thích. Giải thích của họ đưa ra giá trị cho mỗi nút trong phân cấp và được thực hiện bằng thuật toán lấy mẫu và che khuất của họ, tương tự như các phương pháp dựa trên nhiễu loạn từ văn học khả năng diễn giải. Cuối cùng, (Chen et al., 2020) tự động tạo ra một phân cấp trên văn bản đầu vào thông qua một thuật toán chia tách được thiết kế đặc biệt. Các cụm từ được chia thành cặp nhị phân bằng cách chọn tập hợp các cụm từ tương tác yếu nhất. Tìm kiếm trên các chia tách cụm từ có thể được thực hiện trong thời gian tuyến tính bằng cách giả định các cụm từ là tuần tự. Theo đó, tất cả các phương pháp hiện có sẽ chỉ áp dụng cho các phân cấp nhị phân và không có phương pháp hiện có nào có thể xử lý các phân cấp phức tạp hơn như phân tầng đoạn văn-câu-từ mà chúng tôi xem xét trong công việc này bằng cách sử dụng lấy mẫu hoán vị trên giá trị Owen-Winter.

C.3.2 CÁC BIẾN THỂ TƯƠNG PHẢN

Ngoài ra, cũng đã có những tiến bộ gần đây hơn về phía cấu trúc đầu ra cho gán thuộc tính kiểu Shapley. Trong bối cảnh các ứng dụng mô hình hóa ngôn ngữ (văn bản sang văn bản), có nhu cầu lớn hơn để xử lý độ phức tạp ngày càng tăng của một giải thích đối với mô hình ngôn ngữ. Trong khi nhiều công trình đã thử việc tái công thức hóa đơn giản của mô hình hóa ngôn ngữ như một nhiệm vụ phân loại của token đầu tiên được tạo ra, ít công trình hơn đã tiến triển thêm trong việc cung cấp giải thích hợp lý vượt ra ngoài một vector trên tất cả các token đầu ra có thể (thường trong số hàng chục nghìn token hoặc nhiều hơn). Cụ thể, phương pháp chính được tận dụng là của giải thích tương phản, đặc biệt yêu cầu so sánh giữa hai token đầu ra thay thế, thay vì một giải thích rộng trên tất cả chúng. Jacovi et al. (2021) áp dụng những kỹ thuật này cho trường hợp đơn giản hơn của phân loại đa lớp, làm nổi bật giá trị của giải thích tương phản cho các ứng dụng NLP. Gần đây hơn, Yin & Neubig (2022) áp dụng các kỹ thuật tương tự cho trường hợp mô hình hóa ngôn ngữ trên token đầu tiên, sử dụng thông tin ngữ pháp như ứng viên hữu ích cho giải thích tương phản. Tuy nhiên, dường như không có công trình hiện có nào đã phát triển giải thích sau xử lý có thể thích ứng với trường hợp sinh tạo văn bản hoàn chỉnh.

D TRỰC QUAN HÓA CÁC GIẢI THÍCH

Chúng ta có thể có được cái nhìn sâu sắc về cách các diễn giải có cấu trúc phân cấp của chúng tôi đưa ra giá trị ở các cấp độ khác nhau, gán tầm quan trọng cho các đoạn văn từ các tài liệu khác nhau và sau đó định vị thêm những gán này đến cấp độ câu và từ. Chúng tôi cũng cung cấp phiên bản tương tác của các trực quan hóa sau được lưu trữ tại đây.

Hình 8: Ví dụ Giải thích cho thấy các cấp độ khác nhau của phân cấp. Chúng ta thấy câu trả lời đúng "Wilhelm Conrad Rontgen" được làm nổi bật bằng màu xanh dương là quan trọng nhất, và chúng ta có thể tìm thấy các từ liên quan bên trong đoạn văn lớn hơn. Câu trả lời có khả năng thứ hai, Marie Curie, được làm nổi bật trong đoạn văn thứ 5 và chúng tôi định vị đến các câu liên quan nhất.

E PHÂN TÍCH THÊM CHO SỬA CHỮA BỘ DỮ LIỆU TRÊN BỘ DỮ LIỆU MIRACL

Trong phần này chúng tôi đi sâu vào các ví dụ truy vấn và đoạn văn cụ thể được tìm thấy từ trong bộ dữ liệu MIRACL để phân tích mức độ phù hợp chúng được đánh giá. Cho mỗi ví dụ, chúng tôi cung cấp câu hỏi được hỏi và một bảng các đoạn văn liên quan. Cụ thể, cho mỗi truy vấn chúng tôi cung cấp ba đoạn văn được đánh giá hàng đầu theo giá trị Shapley được tính cho truy vấn. Ngoài ra, chúng tôi cung cấp một số đoạn văn liên quan nhất không được xem xét đáng kể bởi giá trị Shapley hoặc những cái được đánh giá cụ thể bởi bộ dữ liệu MIRACL (là một trong mười đoạn văn tổng cộng có nhãn tích cực/liên quan hoặc tiêu cực/không liên quan.) Chúng tôi bao gồm ba loại ví dụ chính để cố gắng đưa ra phạm vi bao quát tốt về những khác biệt nào tồn tại qua các diễn giải và qua các nhãn bộ dữ liệu.

E.1 NHÃN SAI

Những ví dụ này đại diện cho kịch bản tương đối nghiêm trọng nơi các nhãn gốc từ bộ dữ liệu MIRACL được tìm thấy là sai sau khi khám phá với các giải thích có thể diễn giải của chúng tôi. Chúng tôi thấy rằng các đoạn văn được chọn từ điểm giải thích cho phép chúng tôi nhanh chóng khám phá các nhãn không chính xác bằng cách tìm các đoạn văn quan trọng nhất từ kho thông tin có khả năng liên quan lớn. Trong Bảng 7, chúng ta thấy rằng bộ dữ liệu gốc gắn nhãn sai các đoạn văn là không liên quan khi chúng thực sự chứa thông tin liên quan về chế độ ăn của châu chấu. Trong Bảng 8, chúng ta thấy rằng người chú thích con người thực sự nhầm lẫn 'kiểm tra phương ngữ' với 'phương pháp phương ngữ', gây ra việc gắn nhãn không chính xác các đoạn văn.

E.2 NHÃN KHÔNG ĐỦ

Những ví dụ này đại diện cho kịch bản tương đối nhẹ nhàng nơi tất cả các nhãn dường như đúng, nhưng vẫn có một lượng lớn các đoạn văn chưa được gắn nhãn chứa tất cả thông tin cần thiết. Cụ thể, chúng tôi làm nổi bật các ví dụ trong Bảng 9 và 10 nơi phương pháp của chúng tôi hiệu quả định vị các đoạn văn trả lời chính xác truy vấn gốc, nhưng không có trong mười đoạn văn được truy xuất ban đầu từ hệ thống truy xuất thông tin. Sự khan hiếm thông tin nhãn này trong bộ dữ liệu MIRACL hạn chế phương pháp của chúng tôi khỏi tiềm năng đầy đủ nhất khi chúng tôi xem xét số liệu AUC chỉ sử dụng mười nhãn hàng đầu của MIRACL. Chính vì lý do này chúng tôi xem xét việc sử dụng đánh giá nhãn giả trong văn bản chính như một tín hiệu tốt hơn cho nhiệm vụ ODQA end-to-end.

E.3 GIẢI THÍCH KHÔNG ĐỦ

Trong tập hợp ví dụ cuối cùng, chúng tôi cho thấy trường hợp các giải thích từ mô hình ngôn ngữ xác định các đoạn văn không chính xác. Trong Bảng 11, khi tìm kiếm nguồn gốc của lý thuyết trường lượng tử, mô hình tập trung vào bài báo của Born, Heisenberg, và Jordan. Mặc dù cực kỳ liên quan, công trình này thường được coi là tiền thân của cái được gọi là lý thuyết trường lượng tử thay vì bài báo đầu tiên của nó (Kuhlmann, 2023). Trong Bảng 12, chúng ta thấy kết quả tìm ngày thiết lập hoa tiểu bang của Texas. Mặc dù giải thích được đánh giá cao nhất là một đoạn văn liên quan, hai cái cao nhất tiếp theo có thông tin cả về lịch sử Texas và về bluebonnet, nhưng không có ngày cần thiết để trả lời câu hỏi. Chúng tôi dự đoán rằng ngay cả cho các trường hợp như vậy, phương pháp của chúng tôi vẫn sẽ hữu ích cho xây dựng và sửa chữa bộ dữ liệu: vì phương pháp của chúng tôi tìm ra các đoạn văn liên quan hơn và gần mơ hồ hơn so với các hệ thống dựa trên truy xuất hiện có, người ta sẽ có thể sử dụng hiệu quả hơn các người chú thích con người khi sử dụng phương pháp của chúng tôi.

[Bảng 7-12 và nội dung của chúng tiếp theo trong bản dịch như được yêu cầu]

Bảng 7: Ví dụ Bộ dữ liệu MIRACL cho: "Châu chấu ăn gì?"

Xếp hạng  Đánh giá  Đánh giá  Sự đồng ý  Tiêu đề  Văn bản
Shapley   MIRACL   Thật sự
1        Liên quan  Liên quan  Tốt       Châu chấu  Châu chấu ăn lượng lớn lá cây cả khi trưởng thành và trong quá trình phát triển của chúng, và có thể là loài gây hại nghiêm trọng cho đất khô cằn và thảo nguyên. Đồng cỏ, ngũ cốc, thức ăn chăn nuôi, rau và các loại cây trồng khác có thể bị ảnh hưởng. Châu chấu thường phơi nắng và phát triển mạnh trong điều kiện nắng ấm, vì vậy hạn hán kích thích sự gia tăng quần thể châu chấu. Một mùa hạn hán thông thường không đủ để kích thích sự gia tăng quần thể lớn, nhưng nhiều mùa khô liên tiếp có thể làm như vậy, đặc biệt nếu những mùa đông xen kẽ nhẹ nhàng để số lượng lớn nhộng sống sót. Mặc dù thời tiết nắng kích thích tăng trưởng, cần có nguồn cung cấp thức ăn đầy đủ cho quần thể châu chấu ngày càng tăng. Điều này có nghĩa là mặc dù lượng mưa cần thiết để kích thích tăng trưởng thực vật, thời gian dài thời tiết u ám sẽ làm chậm sự phát triển nhộng.

2        Không liên quan  Liên quan  Sai        Châu chấu  Châu chấu là loài ăn thực vật, với một vài loài đôi khi trở thành loài gây hại nghiêm trọng cho ngũ cốc, rau và đồng cỏ, đặc biệt khi chúng tụ đàn hàng triệu con như cào cào và phá hoại cây trồng trên diện rộng. Chúng bảo vệ mình khỏi kẻ săn mồi bằng cách ngụy trang; khi bị phát hiện, nhiều loài cố gắng làm giật mình kẻ săn mồi bằng cánh có màu sắc rực rỡ trong khi nhảy và (nếu trưởng thành) tung mình vào không khí, thường chỉ bay một quãng ngắn. Các loài khác như châu chấu cầu vồng có màu cảnh báo ngăn chặn kẻ săn mồi. Châu chấu bị ảnh hưởng bởi ký sinh trùng và nhiều bệnh khác nhau, và nhiều sinh vật săn mồi ăn cả nhộng và châu chấu trưởng thành. Trứng là đối tượng tấn công của ký sinh ong và kẻ săn mồi.

3        Không liên quan  Liên quan  Sai        Châu chấu  Hầu hết châu chấu là loài ăn tạp, ăn thực vật từ nhiều nguồn thực vật, nhưng một số là loài ăn tạp và cũng ăn mô động vật và phân động vật. Nói chung sở thích của chúng là cỏ, bao gồm nhiều loại ngũ cốc được trồng làm cây trồng. Hệ tiêu hóa điển hình của côn trùng, với ống Malpighian xả vào ruột giữa. Carbohydrate được tiêu hóa chủ yếu trong dạ dày, trong khi protein được tiêu hóa trong ruột mù của ruột giữa. Nước bọt dồi dào nhưng phần lớn không có enzyme, giúp di chuyển thức ăn và tiết Malpighian dọc theo ruột. Một số châu chấu có cellulase, bằng cách làm mềm thành tế bào thực vật làm cho nội dung tế bào thực vật có thể tiếp cận với các enzyme tiêu hóa khác.

–        Không liên quan  Không liên quan  Tốt      Châu chấu Kosher  Năm 1911, Abraham Isaac Kook, đại giáo sĩ của Ottoman Palestine, đã đặt câu hỏi cho Tòa án giáo sĩ tại Sana'a về phong tục ăn châu chấu của họ, và liệu phong tục này có được quan sát bằng cách quan sát đặc điểm bên ngoài của chúng, hay chỉ đơn giản dựa vào truyền thống miệng. Câu trả lời được đưa ra cho ông bởi tòa án như sau: "Những con châu chấu được ăn theo truyền thống từ tổ tiên chúng tôi, tình cờ sạch sẽ, được chúng tôi biết rõ. Nhưng vẫn có những loài khác có tất cả các đặc điểm nhận dạng được của việc sạch sẽ, nhưng chúng tôi thực hành kiêng khem chúng. [Phụ lục]: Những con châu chấu sạch về đó chúng tôi có truyền thống thực sự là ba loài mỗi loài có màu sắc khác nhau [với loài khác], và mỗi loài được chúng tôi gọi bằng tiếng Ả Rập là "gharad" (cào cào). Nhưng vẫn có những loài khác, về đó chúng tôi không có truyền thống, và chúng tôi sẽ không ăn chúng. Một trong số đó lớn hơn một chút so với châu chấu, có tên là "'awsham". Còn có một loại khác, nhỏ hơn châu chấu, và nó được gọi là "hanajir" (katydids).

–        Không liên quan  Không liên quan  Tốt      Chuột chù Bắc Mỹ nhỏ nhất  Chế độ ăn của nó bao gồm chủ yếu động vật không xương sống nhỏ, như sâu bướm, ấu trùng bọ cánh cứng, giun đất, rết, sên và bọ gỗ. Nó cũng sẽ ăn từ xác động vật chết, và một lượng nhỏ hạt hoặc trái cây. Loài chuột chù này sẽ ăn con mồi nguyên con, nhưng khi ăn dế và châu chấu, chuột chù Bắc Mỹ nhỏ nhất sẽ cắn đầu con mồi và chỉ ăn các cơ quan nội tạng. Khi chiến đấu với sinh vật lớn hơn, nó sẽ nhắm vào chân và cố gắng làm tàn phế đối thủ, và sẽ cắn thằn lằn, thường quá lớn để nó giết, vào đuôi, sau đó rơi ra và cung cấp cho nó một bữa ăn trong khi thằn lằn thoát. Chuột chù Bắc Mỹ nhỏ nhất đôi khi cũng sẽ sống bên trong tổ ong và ăn tất cả ấu trùng. Nó thường sẽ chia sẻ thức ăn với những con chuột chù khác. Nó ăn nhiều hơn trọng lượng cơ thể mỗi ngày và được biết là tích trữ thức ăn.

Bảng 8: Ví dụ Bộ dữ liệu MIRACL cho: "Khi nào phương pháp phương ngữ được sử dụng?"

Xếp hạng  Đánh giá  Đánh giá  Sự đồng ý  Tiêu đề  Văn bản
Shapley   MIRACL   Thật sự
1        Chưa đánh giá  Liên quan  Ổn      Giao tiếp liên cá nhân  Một cách tiếp cận phương ngữ đối với giao tiếp liên cá nhân được phát triển bởi các học giả Leslie Baxter và Barbara Montgomery. Cách tiếp cận phương ngữ của họ xoay quanh các khái niệm mâu thuẫn, thay đổi, thực hành và tổng thể. Bị ảnh hưởng bởi Hegel, Marx và Bakhtin, cách tiếp cận phương ngữ được thông báo bởi một nhận thức luận đề cập đến một phương pháp lý luận mà qua đó người ta tìm kiếm sự hiểu biết thông qua căng thẳng của các lập luận đối lập. Sử dụng cách tiếp cận phương ngữ, Baxter và Montgomery đã phát triển hai loại phương ngữ hoạt động trong các mối quan hệ liên cá nhân: nội bộ và bên ngoài. Chúng bao gồm tự chủ-kết nối, mới lạ-có thể dự đoán, cởi mở-đóng kín.

2        Chưa đánh giá  Liên quan  Ổn      Nghiên cứu phương ngữ  Nghiên cứu phương ngữ hoặc điều tra phương ngữ hoặc khảo sát phương ngữ là một hình thức nghiên cứu định tính sử dụng phương pháp phương ngữ, nhằm khám phá sự thật thông qua việc kiểm tra và thẩm vấn các ý tưởng, quan điểm hoặc lập luận cạnh tranh. Nghiên cứu phương ngữ có thể được xem như một hình thức nghiên cứu khám phá, trong đó không có nhiều giả thuyết nghiên cứu để kiểm tra, mà là những hiểu biết mới cần được phát triển.

3        Chưa đánh giá  Liên quan  Ổn      Phương ngữ  Phương ngữ hoặc phương ngữ học ("dialektike"; liên quan đến đối thoại), còn được gọi là phương pháp phương ngữ, về cơ bản là một cuộc thảo luận giữa hai hoặc nhiều người có quan điểm khác nhau về một chủ đề nhưng muốn thiết lập sự thật thông qua các lập luận có lý. Phương ngữ giống với cuộc tranh luận, nhưng khái niệm loại trừ các yếu tố chủ quan như lời kêu gọi cảm xúc và ý nghĩa chê bai hiện đại của tu từ học. Phương ngữ có thể được đối chiếu với phương pháp giảng dạy, trong đó một bên của cuộc trò chuyện dạy bên kia. Phương ngữ được biết đến một cách thay thế như logic nhỏ, trái ngược với logic lớn hoặc phê bình.

–        Liên quan  Không liên quan  Sai       Kiểm tra Phương ngữ  Kiểm tra Phương ngữ được tạo ra bởi A.J. Ellis vào tháng 2 năm 1879, và được sử dụng trong công việc thực địa cho tác phẩm "On Early English Pronunciation" của ông. Nó đứng như một trong những phương pháp sớm nhất để xác định âm nguyên âm và đặc điểm của lời nói. Mục đích là để nắm bắt các âm nguyên âm chính của một phương ngữ cá nhân bằng cách nghe đọc một đoạn văn ngắn. Tất cả các danh mục từ West Saxon và nguyên âm được bao gồm trong bài kiểm tra để có thể so sánh với lời nói West Saxon lịch sử cũng như với nhiều phương ngữ khác nhau.

–        Không liên quan  Liên quan  Sai       Trường phái Frankfurt  Viện cũng đã cố gắng tái công thức hóa phương ngữ như một phương pháp cụ thể. Việc sử dụng phương pháp phương ngữ như vậy có thể được truy nguyên trở lại triết học của Hegel, người đã hình thành phương ngữ như xu hướng của một khái niệm chuyển sang phủ định của chính nó như kết quả của xung đột giữa các khía cạnh mâu thuẫn vốn có của nó. Trái ngược với các phương thức tư duy trước đây, xem mọi thứ một cách trừu tượng, mỗi cái theo chính nó và như thể được ban cho những tính chất cố định, phương ngữ Hegel có khả năng xem xét các ý tưởng theo chuyển động và thay đổi của chúng theo thời gian, cũng như theo mối tương quan và tương tác của chúng.

Bảng 9: Ví dụ Bộ dữ liệu MIRACL cho: "Guitar Hero Live được phát hành lần đầu khi nào?"

[Tiếp tục với các bảng khác theo cùng định dạng...]
