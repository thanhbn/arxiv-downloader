# 2408.16967.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2408.16967.pdf
# File size: 1137339 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MemLong: Memory-Augmented Retrieval for Long Text Modeling
Weijie Liu1*, Zecheng Tang1*, Juntao Li1†, Kehai Chen2, Min Zhang1
1School of Computer Science and Technology, Soochow University
2Harbin Institute of Technology, Shenzhen
{wjliu,zctang}@stu.suda.edu.cn
{ljt,minzhang}@suda.edu.cn ;chenkehai@hit.edu.cn
Abstract
Recent advancements in Large Language Mod-
els (LLMs) have yielded remarkable success
across diverse fields. However, handling long
contexts remains a significant challenge for
LLMs due to the quadratic time and space
complexity of attention mechanisms and the
growing memory consumption of the key-value
cache during generation. This work intro-
duces MemLong :Mem ory-Augmented Re-
trieval for Long Text Generation ( MemLong ,
a method designed to enhance the capabilities
of long-context language modeling by utiliz-
ing an external retriever for historical infor-
mation retrieval. MemLong combines a non-
differentiable ret-mem module with a partially
trainable decoder-only language model and in-
troduces a fine-grained, controllable retrieval
attention mechanism that leverages semantic-
level relevant chunks. Comprehensive evalua-
tions on multiple long-context language mod-
eling benchmarks demonstrate that MemLong
consistently outperforms other state-of-the-art
LLMs. More importantly, MemLong can ex-
tend the context length on a single 3090 GPU
from 4k up to 80k1.
1 Introduction
Large Language Models (LLMs) have achieved re-
markable success in various fields. However, due to
the quadratic time and space complexity of vanilla
attention mechanisms (Vaswani et al., 2017), it is
challenging to extend the context length consider-
ably, which poses significant limitations for applica-
tions involving long-sequence tasks, such as long-
document summarization (Koh et al., 2022) and
multiple rounds of dialogue (Wang et al., 2024a).
As a result, LLMs are often expected to maintain a
*Equal Contribution.
†Corresponding author.
1Our code is available at https://github.com/
Bui1dMySea/MemLong
(b) MemLong: memory and retrieval historical information  (a) Retrieval Augment Generation
How is the retrieval
augmentation technology
applied in large language
model ?QueryDoc Indexing Chunk1 :"The retrieval model fetches relevant
documents, and the generative model uses this
information to produce more accurate and
contextually relevant outputs. "
Chunk2 :"Integrates a retrieval mechanism
directly into the training loop of the language
model. " ...Documents Outputs
With 5 Chunks RAG:  "Retrieval
augmentation in large language
models involves integrating an
information retrieval system to
fetch  ..."
With 20 Chunks RAG:  "A
retrieval information system,
often referred  w''<w'
..."
With 20 Chunks Contexts +
Query
How is the retrieval
augmentation technology applied
in large language model ?20 Chunks ContextsChunk-Level
 Memory
MemLongKey,Value and Embedding Pairs
Query Embeddings
of Current InputsLong-Range
RetrievalSearch
Memory Fusion Generation:  "Retrieval augmentation
in large language models involves integrating an
information retrieval system to fetch  ..."Figure 1: Illustration of Retrieval-Augment Genera-
tion(RAG) and Memory-Retrieval flow of MemLong.
(a) RAG can even degrade the generation performance
(yellow) when the length of the retrieved information
exceeds the model’s processing capacity. (b) Our ap-
proach utilizes an external retriever to fetch historical
information, which is then passed into the model as K-V
pairs rather than in text form.
long working capability (a.k.a. long context LLMs)
to effectively handle these demanding scenarios.
To tackle the computational bottleneck, numer-
ous efforts have been made. The first line of work
focuses on reducing the computation of vanilla at-
tention mechanisms (Vaswani et al., 2017) by em-
ploying sparse attention operations (Beltagy et al.,
2020; Wang et al., 2020; Kitaev et al., 2020; Xiao
et al., 2023a; Chen et al., 2023b; Lu et al., 2024).
Although these types of works can reduce com-
putational complexity to approximately O(n), it
often comes with trade-offs in model capability.
Therefore, Some works shift their focus to mem-
ory selection (Dai et al., 2019; Bertsch et al., 2024;
Yu et al., 2023). These approaches, as token-level
memory selection, can result in the truncation of se-
mantic information. Another recent line of work isarXiv:2408.16967v1  [cs.CL]  30 Aug 2024

--- PAGE 2 ---
Retrieval-Augment Language Modeling (Wu et al.,
2022; Wang et al., 2024b; Rubin and Berant, 2023).
These works usually introduce a retrieval mecha-
nism to enhance the model’s ability to handle long
texts. However, these methods have several draw-
backs. Firstly, the information stored in memory
may experience distribution shifts due to changes
in model parameters during training. Secondly,
these methods often require retraining, which is im-
practical in the era of large models. Finally, these
models are often prone to processing long text in-
puts at the expense of the original capabilities of
the pre-trained model. To address the limitations
of previous research, we posed the following ques-
tion: Can we utilize the explicit retrieval capa-
bilities of a retriever to approximate the implicit
retrieval processes within the model?
In this work, we propose MemLong, an efficient
and lightweight method to extending the context
window of LLMs. The key idea is to store past
contexts and knowledge in a non-trainable mem-
ory bank and further leverages these stored em-
beddings to retrieve chunk-level key-value ( K-V)
pairs for input into the model. . MemLong is ap-
plicable to any decoder-only pretrained language
models by incorporating (1) an additional ret-mem
component for memory and retrieval, and (2) a
retrieval causal attention module for integrating
local and memory information. The memory and
retrieval process of MemLong is illustrated in Fig-
ure 1(b). During generation,one text that exceeds
the model’s maximum processing length is stored
as context information in a Memory Bank. Sub-
sequently, given a recently generated text chunk
in a long document, we use the retriever to explic-
itly retrieve past information, obtaining additional
context information through index alignment.
MemLong offers several benefits: (1) Distribu-
tional Consistency : Unlike previous models that
experienced a distribution shift when information
was stored in memory, MemLong ensures the dis-
tribution of cached information remains consistent.
(2)Training Efficient : We freeze the lower layers
of the model and only need to finetune the upper
layers which greatly reduced computational cost.
In our experiments, finetuning a 3B parameter ver-
sion of MemLong on 0.5B tokens requires only
eight 3090 GPUs for eight hours. (3) Extensive
Context Window : Since only a single layer’s K-V
pairs need to be memorized, MemLong is capable
of extending the context window up to 80k tokens
easily on a single 3090 GPU.Extensive experiments have demonstrated that
MemLong exhibits superior performance in several
aspects when compared with other leading LLMs.
MemLong outperforms OpenLLaMA (Touvron
et al., 2023) and other retrieval-based models on
several long-context language modeling datasets.
In retrieval-augmented in-context learning tasks,
MemLong achieves an improvement of up to 10.2
percentage points over OpenLLaMA.
2 Preliminary
2.1 Task Definition
Language models are designed to define probability
distributions over sequences of tokens, effectively
predicting the likelihood of a sequence within a
given language. Given such a sequence x1, . . . , x n,
the standard approach to modeling its probability
is via the next-token prediction: p(x1, . . . , x n) =Pn
i=0pθ(xi|x<i), where x<i:=x1, . . . , x i−1is
the sequence of tokens proceeding xi. Differently
from the standard language modeling objective, we
not only use the current context to make next-token
predictions, but also utilize external retrieval to ob-
tain relevant information and perform knowledge
fusion at the upper layers of the model. Specifi-
cally, given a sequence consisting of ltokens and
the size of each chunk τ, we partition it into a long
sequence of ν=l
τnon-overlapping chunks , which
denoted as C= (c1, . . . , c ν). Correspondingly, its
textual form is divided into νtext chunks, which
denoted as T= (t1, . . . , t ν). In each step, we per-
form causal language modeling on ciin the lower
layers , while in the upper layers , we conduct fine-
grained controllable retrieval on tifor the fusion of
additional information. After do this, our language
modeling objective becomes
p(x1, . . . , x n) =nX
i=0pθ(xi|R(ti), x<i)(1)
where R(ti)denotes the retrieval of neighboring
chunks oftiwhere xiis located.
2.2 Module and Operation Definitions
As shown in Figure 2, the Ret-Mem module com-
prises a Retriever and a Memory component for
information exchange. Initially, we define the
Memory component as Mand the Retriever as
R, and their corresponding operations M(·)and
R(·). Furthermore, we specify the dimension of
the model as dmodel , the dimension of the retriever

--- PAGE 3 ---
Figure 2: An example of MemLong : In the lower layers , where the model remains static, causal language modeling
is performed on the entire chunk ci, and subsequently, ciis cached in both embedding and K-Vpair forms. Lastly,
theupper layers are finetuned to harmonize retrieval preferences and integrate the retrieved content.
asdret. The Memory module includes two seg-
ments: K-Vpairs and corresponding Representation
Embeddings. The dimension for both keys and val-
ues is represented as Rdmodel and for Embeddings
asRdret. It is crucial to emphasize that the actual
retrieval process involves the embeddings repre-
senting the chunks, not the K-Vpairs. The Retriever
is essentially a pretrained dense embedder with ex-
cellent representation capabilities. MemLong use
it to encode each chunk into Representation Em-
beddings. Since it produces a one-dimensional
representation vector for one chunk, the memory
footprint remains minimal even if the memory size
is substantial.
3 MemLong
3.1 Overview
As illustrated in Figure 2, each step involves an
input of a chunk ci, where the original text for that
chunk is ti. In the lower layers where the model is
frozen, the standard causal attention is applied to
the entire ci. For the final layer of the lower layers ,
we refer to it as the memory layer . Following each
traversal of the memory layer , two key operations
are performed. The first operation is retrieval, de-
picted by the red line, where tiis utilized to fetch
the most pertinent K-Vpairs. The second operation,
indicated by the blue line, involves caching the ac-
quired K-Vpairs along with their associated chunkrepresentation. Within the model’s upper layers ,
the retrieved K-Vpairs are integrated with the cur-
rent input context, subsequently tuning the model
parameters to calibrate the retrieval reference. Sub-
sequent sections will explore the various facets of
the MemLong framework and their intricacies, en-
compassing Retriever and Dynamic Memory Man-
agement (§ 3.2), Attention Reformulation (§ 3.3),
and Inference with MemLong (§ 3.4).
3.2 Retriever and Dynamic Memory
Management
We offer a comprehensive explanation of the re-
trieval process and the dynamics of memory man-
agement.
Retrieval Process. Given our objective to replace
traditional kNN retrieval based on K-Vpairs with
explicit retrieval, we aim to pre-fetch the desired
information when feasible before each model in-
put. Specifically, for each potential query block
cq=ciand its corresponding text block tq=ti,
we first pass it through Retriever and then obtain
a representation embedding rq=R(tq), where
rq∈Rdret. Subsequently, we use this representa-
tion embedding to perform retrieval against the em-
beddings in Mto obtain the required kchunk-level
indices. We compute the cosine similarity between
the retrieval representation rqand the embeddings
stored in Memory M. Finally , we get the top-k
indices zq=TopK{Cos(rq)}for the cq, where

--- PAGE 4 ---
zq∈Rk. Due to the contiguous nature within
the blocks, we can easily extend the obtained in-
dices to cover the entire relevant range for retrieval.
Finally, we retrieve the corresponding K-Vpairs
˜zq∈Rk×τ×dmodel from Memory based on these in-
dices and used for the upper layer . It is noteworthy
that we have equipped the Memory with a counter
mechanism to record the frequency of retrievals
for each index contained therein. This frequency
data will subsequently serve as a basis for dynamic
memory updating, allowing for the prioritization
of more frequently retrieved information.
Memory Process. The memory process syn-
chronously stores the K-Vpairs from the memory
layer and the representation embedding previous
calculated for retrieval , ensuring that indices for
K-Vpairs correspond accurately to their represen-
tation embeddings (see Figure 2, right, blue line).
For every possible chunk memory cm=ci, and
its corresponding text chunk tm=ti, we divide
the memory process into two parts: the first part
details how to cache the K-Vpairs, and the second
part explains how to store the corresponding repre-
sentations. Firstly, we input cminto the MemLong
and get the output from the memory layer . It is
worth noting that, since the lower layers are frozen
during training, we can ensure that the distribution
of the output K-Vpairs is consistent. This consis-
tency is crucial for avoiding the distribution shift is-
sue, which was previously observed in models like
MemTrm (Wu et al., 2022). Our memory operation
is highly efficient because it only involves storing
the representations needed for retrieval, rm=rq,
thereby avoiding redundancy. After the retrieval
for all chunk pairs is complete, the memory op-
eration—denoted as M(k, v;rm)—synchronously
updates the memory with both the Key-Value pairs
and their corresponding representations.
Dynamic Memory Update. When memory over-
flows, we use the Counter to update memory intelli-
gently. In our experiments, we keep the latest 10%
of memory content due to its potential relevance,
discard the oldest 10% as likely outdated, and prior-
itize the middle 80% based on retrieval frequency,
deleting the least accessed entries until memory us-
age drops to 50%. This selective pruning balances
recency and relevance, retaining valuable informa-
tion and removing less pertinent data. Unlike tradi-
tional FIFO strategies, our method focuses on re-
trieval frequency to efficiently prune redundant in-
formation, maintaining a high-quality dataset. The
Figure 3: Illustration of retrieval causal attention. Local
causal attention is applied to the recent context, while
chunk-level K-Vpairs, obtained through the retrieval
method, enable bidirectional attention without informa-
tion leakage due to their historical nature.
decision to dynamically update the datastore is a
trade-off between effectiveness and efficiency. For
tasks requiring long-term dependencies, storing all
information can enhance comprehensive process-
ing, but for shorter-term tasks, dynamic updates are
more suitable. Dynamic updates control memory
size to prevent out-of-memory issues, discard stale
information, and reduce retrieval overhead, ensur-
ing efficiency without significantly compromising
performance.
3.3 Attention Reformulation
In the trainable upper layers of the model, we re-
vised the attentions to fuse with long-term mem-
ory. As illustrated in Figure 3, unlike the tradi-
tional Transformer decoder layers that utilize Multi-
Head Attention (Vaswani et al., 2017), we pro-
pose a Retrieval Causal Attention to extend it to
a joint-attention mechanism and propose a long-
term memory fusion process to enable each token
to attend on both local contexts and chunk-level
past contexts which have complete and continu-
ous semantics. With the head-wise hidden state
output from previous layer Hl−1∈R|x|×dmodel
and the corresponding retrieved key-value pairs
are˜zq={˜Ki,˜Vi}ω
i=1∈Rk×τ×dmodel , the output
hidden state for the next layer Hlis computed as:
Sa=SoftmaxQKT
d
(2)
Sm=Concat
Softmax (˜ zq
i)	ω
i=1(3)
To avoid the interference caused by the retrieval at-
tention scores Smat the initial stage of training, we
adopt a multi-head attention mechanism following
the approach of the LLaMA-adapter(Zhang et al.,

--- PAGE 5 ---
2023b) :
Sg
l= [(Sm)·gl; (Sa)]T(4)
Finally, we concatenate the ˜VandVto obtain Hl:
Vl=h
˜Vc;Vii
, Hl=Sg
lVl (5)
3.4 Inference with MemLong
When MemLong receives an input exceeding the
length, we treat it as two segments: the prefix and
themain . We will separately describe the encoding
of long inputs and the generation of long outputs
during the inference phase. When MemLong re-
ceives long inputs, it first divides the prefix into
multiple non-overlapping chunks and computes the
from its memory layer , which ensures that the num-
ber of tokens involved in the attention is equal to
the chunk size, which is much smaller than the
length of the input. It is important to note that each
chunk is interrelated (e.g., the t-th chunk needs to
process the of the previous t−1chunks).
The second step is to select the kmost relevant
chunks for the main based on chunk-level retrieval
representations and to obtain their key and value
representations. After this, for the upper retrieval
layers, the attention window for retrieval is equiva-
lent to k∗τ, which is also smaller than the input
length. Finally, both length-restricted causal atten-
tion and retrieval attention is performed efficiently.
4 Experiments
We evaluate our proposed MemLong model on var-
ious tasks that require in-memory long-context pro-
cessing: (a) long-context language modeling and
retrieval-augmented language modeling; (b) scal-
able in-context learning capable of handling a large
number of demonstration examples in memory.
4.1 Implementation Details
Training Details. We use OpenLLaMA-3B as
the pre-trained backbone LLM with rotation po-
sition coding (Su et al., 2024). Due to hardware
constraints, we opted to train our models using the
LoRA (Hu et al., 2021) technique. The backbone
LLM holds a L= 26, H= 32, d= 100 architec-
ture. Unless specified otherwise, we use the 13-th
layer as the memory layer and the [14,18,22,26]
layers as the retrieval-augment layers. The train-
ing for retrieval-augmented adaptation iterates onlyon 0.5B tokens with 1024 sequence length. Mem-
Long’s trainable parameters are from 14 to 26 lay-
ers. We utilized the slimpajama dataset sampled
by (Fu et al., 2024) as our training corpus.
Position Remapping. There are several chunk-
level K-Vin theMretrieved for generation. Due
to the uncertainty of retrieval at each step, we
need to remap position embeddings to the retrieved
chunks. Same as the previous work (Tworkowski
et al., 2024), the local context (up to 2048 tokens)
receives the standard rotary positional encoding,
whereas memory keys are encoded as if they had
position 0in the local context window.
4.2 Long-Context Language Modeling
We first evaluate MemLong on long-context lan-
guage modeling benchmarks to assess basic lan-
guage modeling abilities. Due to the K-Vcache pro-
viding sinificant background and contextual infor-
mation, MemLong can retrieve relevant K-Vcache
quickly and make full use of it, thereby enhancing
the model’s in long-context modeling tasks.
Datasets. We conducte an evaluation of our
model across four extensive text benchmark
datasets: English-language books PG-19 (Rae
et al., 2019) and BookCorpus (Zhu et al., 2015),
Wikipedia articles Wikitext-103 (Merity et al.,
2016), and mathematical papers Proof-Pile (Azer-
bayev et al., 2023). The experimental results indi-
cate a significant perplexity improvement across all
datasets. Our model is tested over various lengths
ranging from 1024 to 32768 tokens. Across all
datasets, our model demonstrated substantial per-
formance gains with minimal memory overhead by
leveraging an external retriever and memory.
Setup. Following (Yen et al., 2024), we calcu-
late the perplexity on the last 2048 tokens of each
sequence. This experimental setup is designed to
validate the influence of different retriever sizes on
the overall performance of our model. For the im-
plementation of the efficient fine-grained retrieval,
we use the faiss (Johnson et al., 2019) toolkit to
construct an exact-search index on GPU to store
theRepresentation Embeddings of text chunks and
perform efficient retrieval. For MemLong, we split
and put the tokens over finetune-length = 1024
into the Mused for further retrieval.
Baselines. For our experiments, we employ the
OpenLLaMA-3B model as our baseline. To ensure
a fair comparison, we utilize an identical LoRA

--- PAGE 6 ---
PG19 Proof-pile BookCorpus Wikitext-103
Model 1k 2k 4k 16k 1k 2k 4k 16k 1k 2k 4k 16k 1k 2k 4k 16k
7B Model
LLaMA-2-7B 10.82 10.06 8.92 - 3.24 3.40 2.72 - 8.73 7.91 6.99 - 10.82 6.49 5.66 -
LongLoRA-7B-32k 9.76 9.71 10.37 7.62 3.68 3.35 3.23 2.60 14.99 12.66 11.66 6.93 7.99 7.83 8.39 5.47
YARN-128k-7b 7.22 7.47 7.17 - 3.03 3.29 2.98 - 7.02 7.54 7.06 - 5.71 6.11 5.71 -
3B Model
OpenLLaMA-3B 11.60 9.77 >103- 2.96 2.70 >103- 8.97 8.77 >103- 10.57 8.08 >103-
LongLLaMA-3B∗10.59 10.02 >103- 3.55 3.15 >103- 10.70 9.83 >103- 8.88 8.07 >103-
LongLLaMA-3B†10.59 10.25 9.87 - 3.55 3.22 2.94 - 10.14 9.62 9.57 - 10.69 8.33 7.84 -
Phi3-128k 11.31 9.90 9.66 - /9.65 4.25 3.11 2.77 - / 3.08 11.01 9.22 8.98 - /9.27 7.54 7.22 7.01 - / 7.20
MemLong-3B∗10.66 10.09 >103- 3.58 3.18 >103- 10.37 9.55 >103- 8.72 7.93 >103-
w/ 4K Memory 10.54 9.95 9.89 9.64 3.53 3.16 3.15 2.99 10.18 9.50 9.57 9.61 8.53 7.92 7.87 7.99
w/ 32K Memory 10.53 9.85 9.83 9.73 3.51 3.15 3.11 2.99 9.64 9.56 9.51 9.54 8.02 7.58 6.89 7.09
Table 1: Sliding window perplexity of different context window extension models on PG19, Proof-pile, BookCorpus,
Wikitext-103. All experiments are conducted on one 3090 24GB GPU. LongLLaMA-3B and MemLong-3B marked
with∗means evaluating without Memory, and LongLLaMA-3B marked with†means evaluating with infinite
memory. We also evaluate MemLong with 4K/32K Memory scenarios. "- / 6.95" indicates that the model results in
an Out of Memory (OOM) error on a single GPU, while on dual GPUs it yields the corresponding result.
configuration and finetuned the models on the same
amount of data from the slimpajama dataset. Addi-
tional, we compare LongLLaMA-3B (Tworkowski
et al., 2024), which finetuned with the Focused
Transformer (FoT) method and 5B tokens. To per-
form a further comprehensive comparison, we ad-
ditionally test two 7B models: LLaMA-2-7B and
LongLoRA-7B-32K (Chen et al., 2023b) and two
positional encoding models: Yarn-7b-128k (Peng
et al., 2023) and Phi3-128k (Abdin et al., 2024).
Results. The results are shown in Table 1. We
employ Perplexity (PPL) as the evaluation met-
ric for the language model. Lower PPL in-
dicates stronger language modeling capabilities.
Compared to the two fully fine-tuned models,
OpenLLaMA-3B and LLaMA-2-7B, our model
demonstrates comparable performance across mul-
tiple datasets when test lengths are within their
pre-trained limits (2048 for OpenLLaMA-3B and
4096 for LLaMA-2-7B). However, once the test
lengths exceed these pre-trained limits, our model
continues to reduce perplexity even beyond the fine-
tuning length of 1024 and the pre-trained length of
2048, showcasing its superior generalizability. In
contrast, the OpenLLaMA-3B and LLaMA-2-7B
models fail to generalize to inputs beyond their pre-
trained lengths and exhibit significantly increased
memory overhead due to the quadratic complex-
ity of attention. We also compare our model with
LongLoRA. Although the proposed Shifted Sparse
Attention in LongLoRA significantly reduces mem-ory usage, it also diminishes the model’s perfor-
mance on short texts. In contrast, LongLLaMA,
which K-Vpairs can also be stored, suffers from
OOM issues when test lengths become excessively
long due to its infinitely growing memory usage.
Positional encoding models have strong general-
ization capabilities. However, the performance of
such methods can only guarantee that the gener-
ation performance over long distances does not
degrade. Compared to their methods, MemLong
leverages an external retriever to handle longer
input tokens and achieve better perplexity im-
provements.At the same time, because of the
high storage efficiency, MemLong can effectively
control the use of GPU to avoid OOM problems.
4.3 In Context Learning
Traditional in-context learning (ICL; Brown et al.,
2020) inputs few-shot non-parameterized demon-
stration examples along with the query into the
model. However, these methods are typically con-
strained by the model’s input length. In this ex-
periment, since MemLong can store examples in a
parameterized form within its memory, we primar-
ily investigate whether MemLong can effectively
utilize the knowledge stored in its memory to en-
hance its emergent abilities. The results are shown
in Table 2. Compared to OpenLLaMA,which
rely solely on non-parametric knowledge , given
the same number of in-context demonstrations,
MemLong can utilize additional demonstrations

--- PAGE 7 ---
ModelIn-Context In-Memory SST-2 MR Subj SST-5 MPQAAvg.#Demons. #Demons. ACC↑ACC↑ACC↑ACC↑ACC↑
OpenLLaMA 4 N/A 90.7 84.0 58.2 41.0 70.5 68.9
w./ Rag 4 4 90.9 90.5 61.6 39.2 63.2 69.1
LongLLaMA 4 4 90.4 83.9 64.3 40.0 64.2 68.6
MemLong 4 4 91.5 84.5 61.5 41.4 70.2 69.8
LongLLaMA 4 18 91.4 87.1 59.1 41.0 64.5 68.7
MemLong 4 18 91.0 89.6 61.7 43.5 69.4 71.0
OpenLLaMA 20 N/A 93.6 91.2 55.4 38.2 66.4 69.0
w./ Rag 20 18 92.2 91.3 75.8 39.8 57.6 71.3
LongLLaMA 20 18 94.1 90.8 64.2 41.4 72.1 72.7
MemLong 20 18 93.5 93.8 65.8 43.3 70.6 73.4
Table 2: Accuracy [%] of 4-shot and 20-shot ICL on 5 NLU tasks (SST-2, MR, Subj, SST-5, MPQA). We compare
MemLong with both the vanilla model (OpenLLaMA) and the memory-augmented model (LongLLaMA). Across a
diverse range of experimental settings, our method consistently shows competitive performance.
stored in its memory. The performance further
increases or remains consistent with more demon-
strations in the memory. In our comparative analy-
sis against LongLLaMA, it was observed that our
model outperforms LongLLaMA across the major-
ity of datasets under the same conditions of preserv-
ing In-Memory Demonstrations. It is important
to highlight that our model operates with signif-
icantly lower training parameters (200M V. S.
0.3B) and fine-tuning data volume (0.5B V. S.
5B) compared to LongLLaMA. This underscores
our model’s efficiency in leveraging an external re-
triever for information acquisition, demonstrating a
superior ability to synthesize and utilize knowledge
effectively with substantially fewer resources.
5 Ablation Study
5.1 Training Setting
During the training phase, we explore the effects of
varying retrieval layers on the model and examine
whether the distribution shift problem, as discussed
in MemTrm (Wu et al., 2022), could be adequately
resolved by our approach. As mentioned before,
Our method proposes a low-cost solution for dis-
tribution shifts. As shown in Figure 4, the brown
line (the line at the top of the picture; the train-
ing method is similar to MemTrm fine-tuning all
parameters of the model and all layers after the
memory layer are involved in the retrieval) is sig-
nificantly worse than all other ours methods (even
the most unreasonable settings) in terms of per-
formance and fitting speed. We will analyze the
performance of the reasoning stage later.
5.2 Inference Performance
Figure 4: Degree of PPL during the training phase. The
indicator for the y-axis is PPL. We mainly focus on
training params and retrieval layers. We provide the
specific parameter settings of each line in appendix A.
Q1: Does the memory length affect the perfor-
mance of the model ? As depicted in Figure 5,
our examination of the same model’s performance
across various memory sizes demonstrates a clear
correlation between memory capacity and model
efficiency. The trend indicates that incremental in-
creases in memory size yield gradual enhancements
in performance. Moreover, a critical threshold is
identified at a memory size of 65536, beyond which
the model’s capabilities undergo a substantial leap.
This suggests that while expanding memory offers
substantial benefits, there is a practical ceiling to
its effectiveness, likely influenced by the nuances
of the data’s distribution.
Q2: How many layers do we need to introduce
extra memory information? As shown in Fig-
ure 4, (the pink line) and Table 3 (RPL+TH), the

--- PAGE 8 ---
PG19 Proof-pile
Method 2k 4k 8k 2k 4k 8k
MemLong∗10.09 >103- 3.18 >103-
w. RA + TA 11.43 11.40 10.65 3.51 3.26 3.14
w. RA + TH 10.57 10.48 10.36 3.30 3.26 3.15
w. RP + TH 10.28 10.15 10.12 3.21 3.13 3.08
w. RLP + TH 9.85 9.83 9.80 3.15 3.11 3.04
Table 3: Different retrieval layers can affect MemLong’s
performance. MemLong marked with∗means evalu-
ating without Memory. The size of all methods using
Memory is set to 32768. RA means retrieval across
all upper layers; TA means training all params without
freeze; RP means retrieval across fewer upper layers;
RPL means retrieval acorss much fewer upper layers.
Figure 5: Evaluating different datasets at various mem-
ory sizes.In each subplot, all parameters are the same
except for the memory size.
model performs best when the number of retrieval
layers is set to [13,17,21,25]. It is empirically be-
lieved that if retrieval information is introduced into
all upper layers of the model, it leads to a decrease
in the model’s attention to local context. Therefore,
selecting retrieval layers at appropriate intervals
can actually enhance the model’s capabilities.
6 Related Work
6.1 Long Context Language Modeling
Long context Language Modeling mainly concen-
trate on length extension and context window ex-
pansion. Length Extension studies typically target
the popular RoPE encoding, aiming to scale un-
seen PE into the space of positions seen during
pre-training. These works (Su et al., 2024; Press
et al., 2021; Chen et al., 2023a; Peng et al., 2023)enable the model to generalize to unseen positional
encodings during inference, thereby achieving ex-
trapolation beyond the lengths encountered during
training. In contrast, our method does not require
modifying the PE, and only use one addition mod-
ule to extend the context. Context Window Ex-
tension focuses on how to extend the context win-
dow that LLMs can handle the input at one time.
Due to the quadratic time and space complexity
of computing attention, extending the input length
of language models is quite challenging. Sparse
attention (Kitaev et al., 2020; Chen et al., 2023b;
Tworkowski et al., 2024; Bertsch et al., 2024; Belt-
agy et al., 2020) techniques have made significant
strides, but our focus is on improving long-range
language modeling by enabling LLMs to access
relevant information at shorter input lengths via a
retrieval-enhanced method.
6.2 Retrieval-Augmented Language Modeling
Much effort has been made to enhance Retrieval-
Augmented Language Modeling (Lewis et al.,
2020; Izacard and Grave, 2020; Ram et al., 2023;
Yu et al., 2022; Asai et al., 2023). While some
approaches use external retrievers, non-parametric
information fusion often falls short compared to
parametric methods within the model. We concen-
trate on integrating retrieval concepts directly into
the model. REALM (Guu et al., 2020) suggests
that relying solely on internal model knowledge is
inefficient and advocates for the model to learn to
retrieve and comprehend. kNN-LM (Khandelwal
et al., 2019) enhances language modeling by blend-
ing the LLM’s next-word predictions with those
from a retrieval-based mechanism. MemTrm (Wu
et al., 2022) introduces a memory bank but risks
shifting memory distributions due to parameter ad-
justments. LongMEM (Wang et al., 2024b) miti-
gates this by training a sub-network, though this
adds significant overhead. In contrast, our approach
involves a fixed pre-trained model, enhancing it
with a frozen retriever that aligns with the model’s
internal retrieval processes, thus avoiding distribu-
tion shifts and architectural changes.
7 Conclusion
We introduce MemLong, an innovative approach
that significantly enhances the capability of lan-
guage models to process long texts by leveraging
an external retriever. MemLong utilizes a profi-
cient retriever to swiftly and accurately access text

--- PAGE 9 ---
relevant to the distant context with minimal mem-
ory overhead. MemLong successfully expands the
model’s context window from 2k to 80k tokens.
We demonstrate that MemLong exhibits consider-
able competitive advantages in long-distance text
modeling and comprehension tasks. MemLong can
achieve up to a 10.4 percentage point improvement
in performance compared to the full-context model.
Limitations
Our work primarily focuses on OpenLLaMA-3B.
We hope that future research will explore and inves-
tigate the application of our methods to models of
various sizes. At the same time, it has been found
that while single-layer K-V Pairs can provide addi-
tional semantic information to the upper layers, this
information is unstable. We hope that future work
can provide a more rational framework to accom-
modate our methods. At the same time, we employ
a retriever with fixed FlagEmbeddings (Xiao et al.,
2023b; Zhang et al., 2023a), but studying a greater
range of retrievers would be useful.
Ethics Statement
In the pursuit of advancing knowledge and devel-
oping innovative solutions, we are committed to
upholding the highest ethical standards. Our work
is guided by a steadfast dedication to integrity,
transparency, and respect for all individuals and
communities involved. Since pre-trained models
may have some bias due to the unavoidable pres-
ence of harmful/offensive corpus during training,
MemLong fine-tuning on Slimpajama will face this
problem as well. Although solving this problem
is out of our current work, we hope that there will
be future work that addresses this type of problem
well.
References
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,
Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,
Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-
rat Behl, et al. 2024. Phi-3 technical report: A highly
capable language model locally on your phone. arXiv
preprint arXiv:2404.14219 .
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023. Self-rag: Learning to
retrieve, generate, and critique through self-reflection.
arXiv preprint arXiv:2310.11511 .
Zhangir Azerbayev, Edward Ayers, and Bartosz Pi-
otrowski. 2023. Proof-pile: A pre-training dataset of
mathematical text.Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew Gormley. 2024. Unlimiformer: Long-range
transformers with unlimited length input. Advances
in Neural Information Processing Systems , 36.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023a. Extending context window
of large language models via positional interpolation.
arXiv preprint arXiv:2306.15595 .
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-
glora: Efficient fine-tuning of long-context large lan-
guage models. arXiv preprint arXiv:2309.12307 .
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
2019. Transformer-xl: Attentive language mod-
els beyond a fixed-length context. arXiv preprint
arXiv:1901.02860 .
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Han-
naneh Hajishirzi, Yoon Kim, and Hao Peng. 2024.
Data engineering for scaling language models to 128k
context. arXiv preprint arXiv:2402.10171 .
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. Realm: Retrieval-
augmented language model pre-training. arXiv:
Computation and Language,arXiv: Computation and
Language .
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Gautier Izacard and Edouard Grave. 2020. Leverag-
ing passage retrieval with generative models for
open domain question answering. arXiv preprint
arXiv:2007.01282 .
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.
Billion-scale similarity search with gpus. IEEE
Transactions on Big Data , 7(3):535–547.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through memorization: Nearest neighbor language
models. arXiv preprint arXiv:1911.00172 .
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. arXiv
preprint arXiv:2001.04451 .

--- PAGE 10 ---
Huan Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan.
2022. An empirical survey on long document sum-
marization: Datasets, models, and metrics. ACM
computing surveys , 55(8):1–35.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui,
Qi Zhang, and Xuanjing Huang. 2024. Longheads:
Multi-head attention is secretly a long context pro-
cessor. arXiv preprint arXiv:2402.10685 .
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.arXiv preprint arXiv:1609.07843 .
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context window
extension of large language models. arXiv preprint
arXiv:2309.00071 .
Ofir Press, Noah A Smith, and Mike Lewis. 2021.
Train short, test long: Attention with linear biases
enables input length extrapolation. arXiv preprint
arXiv:2108.12409 .
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
Chloe Hillier, and Timothy P Lillicrap. 2019. Com-
pressive transformers for long-range sequence mod-
elling. arXiv preprint .
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. Transactions of the Association for
Computational Linguistics , 11:1316–1331.
Ohad Rubin and Jonathan Berant. 2023. Long-range
language modeling with self-retrieval. arXiv preprint
arXiv:2306.13421 .
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing , 568:127063.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Szymon Tworkowski, Konrad Staniszewski, Mikołaj
Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr
Miło ´s. 2024. Focused transformer: Contrastive train-
ing for context scaling. Advances in Neural Informa-
tion Processing Systems , 36.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Jian Wang, Chak Tou Leong, Jiashuo Wang, Dongding
Lin, Wenjie Li, and Xiao-Yong Wei. 2024a. Instruct
once, chat consistently in multiple rounds: An effi-
cient tuning framework for dialogue. arXiv preprint
arXiv:2402.06967 .
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 .
Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu,
Xifeng Yan, Jianfeng Gao, and Furu Wei. 2024b.
Augmenting language models with long-term mem-
ory. Advances in Neural Information Processing
Systems , 36.
Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and
Christian Szegedy. 2022. Memorizing transformers.
arXiv preprint arXiv:2203.08913 .
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2023a. Efficient streaming
language models with attention sinks. arXiv preprint
arXiv:2309.17453 .
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas
Muennighoff. 2023b. C-pack: Packaged resources
to advance general chinese embedding. Preprint ,
arXiv:2309.07597.
Howard Yen, Tianyu Gao, and Danqi Chen. 2024. Long-
context language modeling with parallel context en-
coding. arXiv preprint arXiv:2402.16617 .
Haofei Yu, Yue Zhang, Wei Bi, et al. 2023. Trams:
Training-free memory selection for long-range lan-
guage modeling. arXiv preprint arXiv:2310.15494 .
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong
Xu, Mingxuan Ju, Soumya Sanyal, Chenguang
Zhu, Michael Zeng, and Meng Jiang. 2022. Gen-
erate rather than retrieve: Large language mod-
els are strong context generators. arXiv preprint
arXiv:2209.10063 .
Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng
Dou, and Jian-Yun Nie. 2023a. Retrieve any-
thing to augment large language models. Preprint ,
arXiv:2310.07554.
Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-
jun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hong-
sheng Li, and Yu Qiao. 2023b. Llama-adapter: Effi-
cient fine-tuning of language models with zero-init
attention. arXiv preprint arXiv:2303.16199 .
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies

--- PAGE 11 ---
and reading books. In Proceedings of the IEEE in-
ternational conference on computer vision , pages
19–27.

--- PAGE 12 ---
A Different Training Settings
As shown in 4, we list the variable values corre-
sponding to different setting names in the ablation
experiment.

--- PAGE 13 ---
Setting Name Retreival Layers Memory Layer Training Params
Retreival_All_and_Training_All [14,15, . . .,26] 13 All of Model’s Trainable
Retreival_All_and_Training_Half [14,15, . . .,26] 13 Half of Model’s Trainable
Retreival_Partial_and_Training_Half [14,16,18, . . .,26] 13 Half of Model’s Trainable
Retreival_lower_Partial_and_Training_Half [14,18,22,26] 13 Half of Model’s Trainable
Table 4: The specific parameters of different setting names.
