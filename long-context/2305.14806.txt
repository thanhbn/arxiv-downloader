# 2305.14806.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2305.14806.pdf
# File size: 500681 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AWESOME: GPU Memory-constrained Long Document Summarization
using Memory Mechanism and Global Salient Content
Shuyang Cao and Lu Wang
Computer Science and Engineering
University of Michigan
Ann Arbor, MI
{caoshuy, wangluxy}@umich.edu
Abstract
Long document summarization systems are
critical for domains with lengthy and jargon-
laden text, yet they present significant chal-
lenges to researchers and developers with lim-
ited computing resources. Existing solutions
mainly focus on efficient attentions or divide-
and-conquer strategies. The former reduces
theoretical time complexity, but is still memory-
heavy. The latter methods sacrifice global con-
text, leading to uninformative and incoherent
summaries. This work aims to leverage the
memory-efficient nature of divide-and-conquer
methods while preserving global context. Con-
cretely, our framework AWESOME uses two
novel mechanisms: (1) External memory mech-
anisms track previously encoded document seg-
ments and their corresponding summaries, to
enhance global document understanding and
summary coherence. (2) Global salient con-
tentis further identified beforehand to augment
each document segment to support its sum-
marization. Extensive experiments on diverse
genres of text, including government reports,
meeting transcripts, screenplays, scientific pa-
pers, and novels, show that AWESOME pro-
duces summaries with improved informative-
ness, faithfulness, and coherence than competi-
tive baselines on longer documents, while hav-
ing a smaller GPU memory footprint.
1 Introduction
Large pre-trained transformer models have demon-
strated impressive performance across popular ab-
stractive summarization benchmarks (Lewis et al.,
2020; Raffel et al., 2020). Yet, transformer’s
quadratic memory complexity presents challenges
for summarizing long documents with more than
hundreds of words, such as scientific papers and
investigation reports (Cohan et al., 2018; Huang
et al., 2021), making it infeasible for researchers
and developers with limited hardware resources
(e.g., GPUs with insufficient memories) to con-
tribute to this important research field.The NLP community has made several inno-
vations to address the long document challenge.
Prior work divides a document into smaller chunks
and summarizes each separately (Gidiotis and
Tsoumakas, 2020), reduces the complexity of at-
tention calculations (Beltagy et al., 2020), and re-
moves unimportant content before running an ab-
stractor (Pilault et al., 2020). In terms of mem-
ory efficiency, divide-and-conquer methods ob-
tain the most significant advantage (Moro and
Ragazzi, 2022). However, information outside
of a document segment and their corresponding
summaries become inaccessible, leading to unin-
formative and incoherent summaries. Unsurpris-
ingly, state-of-the-art performance is obtained by
models that can maintain global context, e.g., by
combining global attentions with local attentions
in transformer-based summarization models (Za-
heer et al., 2021; Phang et al., 2022). Yet, they still
require a large GPU memory footprint in practice.1
Though large language models like GPT-4 (Ope-
nAI, 2023) are trained to handle up to 32K tokens,
the privacy and security of data transmitted and
shared through the API remain concerning, partic-
ularly in sectors dealing with sensitive information,
e.g., clinical notes. Local model development can
bolster privacy and security; however, limited com-
putational resources in these scenarios necessitate
the exploration of efficient modeling techniques.
Therefore, this work aims to address the problem
of long document summarization using constrained
resources, specifically focusing on constrained
GPU memory . We propose AWESOME2, which
is built on the memory-efficient divide-and-conquer
approach, and Augmented WithEstimated Salient
cOntent and MEmory mechanism. In essence,
AWESOME maintains global context of both the
1These approaches require a GPU memory of >40GB to
process documents with over 8K tokens, while the most cost-
effective GPUs only have 24GB of memory (Li, 2022).
2Our code will be made available at https://
shuyangcao.github.io/projects/awesome/arXiv:2305.14806v2  [cs.CL]  16 Nov 2023

--- PAGE 2 ---
source document and the summary generated so far
with a limited memory usage, to enhance summary
informativeness, faithfulness, and coherence.
First, external memory mechanism is used on
the encoder side of AWESOME to store informa-
tion as it reads in document segments in sequence.
This maintains relevant context for improved docu-
ment understanding and salient content detection,
thus promoting summary informativeness and faith-
fulness. Another memory is applied on the decoder
side to improve generation coherence by tracking
the partial summaries generated for prior document
segments. Importantly, to ensure the GPU memory
efficiency of AWESOME , we curb gradients from
propagating to other document and summary seg-
ments and only allow a limited number of layers to
maintain the external memory.
Second, AWESOME incorporates global
salient content selected by an efficiently trained
extractor through (1) direct text concatenation, or
(2) inserting their key-value matrices into attention
calculation. This lets the summarizer be aware
of important topics at a global level, to enhance
salience estimation and summary informativeness.
We experiment with five popular long-input
benchmarks of different genres: investigation re-
ports in GovReport (Huang et al., 2021), meet-
ing transcripts in QMSum (Zhong et al., 2021),
TV screenplays in SummScreen (Chen et al.,
2022), scientific papers in arXiv (Cohan et al.,
2018), and fictions in BookSum (Kryscinski et al.,
2022). First, on all the five datasets, all AWE-
SOME variants uniformly outperform Se3 (Moro
and Ragazzi, 2022), the divide-and-conquer base-
line, on summary informativeness as evaluated by
ROUGE (Lin, 2004) and on coherence as measured
by DiscoScore (Zhao et al., 2022) and a metric
based on entity graphs (Guinaudeau and Strube,
2013)—both metrics are highly correlated with hu-
man judgment, according to Zhao et al. (2022). Sec-
ond, AWESOME with memory mechanisms also
improves summary faithfulness over Se3 on Gov-
Report, according to SummaC (Laban et al., 2022),
an entailment-based faithfulness metric. Lastly,
compared with more memory-intensive models that
also maintain global context, such as Phang et al.
(2022) and Liu et al. (2022), AWESOME achieves
higher automatic scores for informativeness, co-
herence, and faithfulness on GovReport (Huang
et al., 2021). On BookSum which comprises the
lengthiest documents and summaries among theApproach In →Out Enc Enc ←Dec Dec
Efficient Attention x→y■ ■  
Extract-Abstract xe→y□ ⋆  
Dynamic Weight x→y□ ■ +⋆ 
Divide-Conquer xi→yi□ □ #
Table 1: Existing approaches to long document summa-
rization (§2.1). In→Out: Longer inputs ( |x|>|xe|>
|xi|) or outputs ( |y|>|yi|) produce more nodes in the
computation graph, thus the higher memory consump-
tion. Enc: Encoder accessing partial documents ( □)
hurts document understanding, compared to reading the
full text ( ■).Enc←Dec: Decoder reading the full docu-
ment (■) or pre-identified salient content ( ⋆) enhances
summary informativeness, compared to a segment ( □).
Dec: Decoder accessing previously generated summary
content ( ) is crucial for generation coherence than
reading a current summary segment only ( #).
five datasets, AWESOME produces more informa-
tive and coherence outputs than recent models.
2 Related Work
2.1 Efficient Long Document Summarization
We categorize existing efficient long document
summarization models into four major types, as
summarized in Table 1. The model input can be an
original document, extracted important segments
of the document, or a document segment, which
are denoted as x,xe, orxi(for the i-th segment),
and typically, |x|>|xe|>|xi|. The output can
be the full summary yor a summary segment yi
(forxi), where |y|>|yi|. Importantly, longer in-
puts and outputs expand larger computation graph,
leading to higher GPU memory usage. Moreover,
we analyze both the document context and the
summary context used by each approach when
generating summaries. Specifically, we check (1)
full vs. partial documents that are consumed to
obtain the encoder representations ( Enc); (2) full
vs. partial encoder representations that are attended
by the decoder ( Enc←Dec); and (3) full vs. partial
output that is accessed by the decoder ( Dec).
Efficient attentions are designed to reduce the
quadratic complexity of the original transformer
architecture (Vaswani et al., 2017) and maintain
full encoding context by combining global atten-
tions with local attentions built on sliding win-
dows (Beltagy et al., 2020; Zaheer et al., 2021),
text blocks (Phang et al., 2022; Tay et al., 2020), or
clusters of similar tokens (Kitaev et al., 2020; Roy

--- PAGE 3 ---
et al., 2021). Besides the aforementioned attention
variants designed for self-attentions, recent work
has reduced the memory usage of decoder cross at-
tentions by distributing encoder outputs to different
attention heads (Huang et al., 2021) or selecting at-
tendable encoder outputs via KNN search (Bertsch
et al., 2023). Despite the reduced complexity, ef-
ficient attention-based systems effectively require
reading the full document xto generate a summary
yduring model training and thus still need huge
GPU memory that scales with the input length.
Extract-then-abstract systems circumvent the
long sequence challenge by first identifying the
salient segments, xe(e.g., sentences), using an ex-
tractor, and then running an abstractor over xeto
produce the final summary (Pilault et al., 2020; Liu
and Lapata, 2019; Zhao et al., 2020). However, the
extracted segments may contain incomplete and
out-of-context information that leads to incompre-
hensible and unfaithful summaries.
To mitigate the error propagation issue of a two-
stage approach, recent studies bridge the extractor
and abstractor via dynamic weights over docu-
ment segments. Rather than feeding the extracted
segments directly to the abstractor, at each sum-
mary decoding step, DYLE (Mao et al., 2022) first
predicts an output token distribution for each seg-
ment separately, and then aggregates over all the
extracted segments as weighted by their extraction
salience. PageSum (Liu et al., 2022) further allevi-
ates context loss by averaging decoder output rep-
resentations conditioned on all document segments.
Though their abstractor processes each document
segment xiseparately, jointly training the extrac-
tor and the abstractor still requires loading the full
document xinto the GPU memory.
Divide-and-conquer systems split a document
into multiple non-overlapping segments and sum-
marize each segment separately, as done in Gidiotis
and Tsoumakas (2020) and Se3 (Moro and Ragazzi,
2022). SummN(Zhang et al., 2022) uses an ad-
ditional summarization stage to further condense
the segmented summaries. As each document seg-
ment xiis summarized separately, the divide-and-
conquer approach’s fixed GPU memory footprint
is independent from the document length. This
fits well with our goal of long document summa-
rization with limited memory. However, without
access to other parts of the document and their
summaries, the summarizer struggles for content
salience estimation in each isolated segment, andgenerates incoherent outputs when piecing together
summaries. Though Wu et al. (2021) concatenate
previously generated summaries as part of the in-
put, a complicated strategy is required for training
sample construction.
AWESOME is built on the memory-efficient
divide-and-conquer approach, and improves sum-
mary informativeness, coherence, and faithfulness
by using newly designed external memories for
accumulating salient information from other docu-
ment segments and their generated summaries. We
further augment AWESOME with global salient
content to provide important topics at the document
level, when summarizing each segment.
2.2 Memory and Content Augmentation
Different memory mechanisms have been studied
for long-range text understanding tasks. For in-
stance, Transformer-XL (Dai et al., 2019) caches
intermediate representations produced in the last
document segment and attends over these repre-
sentations. Compressive Transformer (Rae et al.,
2020) further increases the context range by com-
pressing the oldest cached representations. To sim-
ulate memory reading and writing, Recurrent Mem-
ory Transformer (Bulatov et al., 2022) includes ex-
tra memory vectors in each text segment and passes
their corresponding output vectors to the next seg-
ment. Instead of using a memory with a fixed size,
Memorizing Transformer (Wu et al., 2022a) stores
all prior representations as key-value pairs, and
performs an approximate kNN lookup to retrieve
representations to augment the current segment.
However, existing work on memory mechanisms
focuses on language modeling, while incorporat-
ing memory mechanisms into the decoding pro-
cess for generation tasks is nontrivial as it requires
updating both decoding states (e.g., beams) and
memory states. Our work is the first to leverage
memory mechanisms and content augmentation
to incorporate global context for the purpose of
memory-efficient long document summarization.
3 External Memory and Global Salient
Content Augmentation
The architecture of AWESOME (Figure 1) is
based on Se3 (Moro and Ragazzi, 2022), where a
document is summarized segment by segment, with
the final summary obtained by concatenating the
resultant summaries. Document sentences are split
into segments with up to 768 tokens each, while

--- PAGE 4 ---
EncoderEncoder MemoryDecoderDecoder MemorySummary SnippetCurrent SegmentFuture SegmentsPast Segments
Access MemoryUpdate MemoryAugmented Salient ContentEncoderDecoderSummary SnippetEncoderDecoderSummary SnippetEncoderDecoderSummary SnippetEncoderDecoderSummary SnippetFigure 1: Illustration of AWESOME . Encoder and
decoder memories can be accessed any time and up-
dated after reading each document segment and gen-
erating the corresponding summary. They accumulate
global context that improves summary informativeness
and coherence (§3.1). When encoding each segment,
global salient content from other segments (lines with
♦-shaped ends, from both past and future) are provided
to further assist salience estimation (§3.2).
reference summary sentences are assigned to their
most overlapping segment to create the oracle sum-
mary, as detailed in Appendix A. Following Long-
former (Beltagy et al., 2020), we initialize the en-
coder and decoder parameters from BART (Lewis
et al., 2020). AWESOME preserves the global con-
text and builds communications across segments
with minimal GPU memory increase, by (1) em-
ploying external memories in both the encoder and
the decoder to gather relevant information (§3.1),
and (2) augmenting the encoder with salient con-
tent from other segments (§3.2).
3.1 External Memory Mechanisms
We design two external memory mechanisms to
efficiently enable the information flow from prior
segments to the current segment. Specifically, each
memory module maintains a matrix M∈Rm×d,
where m= 1024 is the memory size and d=
1024 is the hidden state dimension of BART. M
is updated after encoding each document segment
and then passed to the next segment. We denote the
memory matrix after the t-th segment as Mt. Each
layer of the encoder and decoder can be equipped
with one such external memory. Below we describe
two mechanisms to update Mtand incorporate it
in both the encoding and decoding processes. The
layer index in the formulas is omitted for simplicity.
Compressive Memory. For each document seg-ment, compression-based memory caches its in-
put vectors to be fed into self-attention calculation.
Since storing the input vectors as-is requires the
memory usage mto scale linearly with the con-
text length, we dedicate half of Mtto store the
compressed memory, with a compression ratio of
r. With Ht
inpdenoting the matrix that contains
input vectors to the transformer self-attention, the
memory compression and update processes are:
Mt−1
c, Mt−1
u=Mt−1[:m
2], Mt−1[m
2:] (1)
M′
u= concat( Mt−1
u,SG(Ht
inp)) (2)
M′
c= compress( M′
u[:−m
2]) (3)
Mt
u=M′
u[−m
2:] (4)
Mt
c= concat( Mt−1
c, M′
c)[−m
2:](5)
Mt= concat( Mt
c, Mt
u) (6)
where SG(·)denotes stopping the gradient back-
propagation to lower GPU usage, and compress( ·)
performs convolutions with their stride and kernel
size set to the compression ratio r.ris set to 5after
tuning on the development sets.
Next, to leverage the memory from the previ-
ous segment in summarizing the current segment,
Mt−1is concatenated with the inputs to the self-
attentions to obtain the key-value matrices:
Ht
mem = concat( Mt−1, Ht
inp) (7)
Ht
self= Attn( Ht
inp|{z}
query, Ht
mem|{z}
key, Ht
mem|{z}
value) (8)
where Ht
selfis the output of the self-attention.
Our compression-based memory is adopted from
Compressive Transformer (Rae et al., 2020), a
decoder-only model for language modeling. We
are the first to apply it to both the encoder and
the decoder of a Transformer model and on long
document summarization tasks.
Compressive memory favors recency, particu-
larly the previous segment and its summary, po-
tentially causing older relevant history to be lost
during compression.
Attentive Memory. To mitigate the recency bias
by compressive memory, we further investigate an
attention-based memory updating mechanism, to
selectively include content in Mt. First, the mem-
ory is additionally accompanied by an extra cross-
attention in each of the encoder and decoder layers,

--- PAGE 5 ---
specialized in retrieving relevant information from
Mt. Following prior study (Lei et al., 2020) that
uses memories in video captioning, we update Mt
with a gate matrix Gtto control the amount of
content to be updated:
Mt=Gt⊙Ut+ (1−Gt)⊙Mt−1(9)
where ⊙denotes the element-wise product and
Utis the matrix containing vectors to update the
memory. UtandGtare obtained as follows:
Ut= tanh( Wu1Mt−1+Wu2St) (10)
Gt=σ(Wg1Mt−1+Wg2St) (11)
St= Attn( Mt−1
|{z}
query,SG(Ht
self)
| {z }
key,SG(Ht
self)
| {z }
value)(12)
where W∗are learnable matrices, Stsynthesizes
the current segment via an attention calculation,
andSG(·)indicates stopping the gradient back-
propagation. In each encoder and decoder layer,
an extra cross-attention is inserted after the self-
attention, where Mt−1is attended and incorporated
into the current segment’s summarization process.
Unlike our approach, the memory in Lei et al.
(2020) does not employ gradient stopping. This
omission eliminates the memory efficiency gained
from the divide-and-conquer strategy, leading to
comparable high memory usage as the efficient
attention strategy.3While their memory is suitable
for generating short image captions, our design
with gradient stopping is crucial for efficient long
document summarization .
Selective Addition of External Memory. External
memory incurs overhead in GPU memory usage.
To mitigate this overhead, we consider selectively
adding external memory to specific layers, as the
importance of external memory varies according
to the different functions of layers in the model.
Our pilot study suggests that the last layers of the
Transformer model more effectively utilize exter-
nal memory compared to the first layers. To avoid
an exhaustive search for the optimal layer or com-
bination of layers for each dataset, we choose to
uniformly equip the last three layers with exter-
nal memory across all datasets unless otherwise
specified.4
3Without gradient stopping, the model fails to complete
training with 48GB GPU memory.
4Compared to adding external memory to all layers, selec-
tive addition reduces GPU memory usage by about 9GB.3.2 Global Salient Content Augmentation
The memory mechanisms only grant access to prior
content in the documents, yet subsequent context
can also help with salience estimation, e.g., elab-
orating the pros and cons of a proposed solution
makes it necessary to introduce the problem and the
solution. Moreover, memories store content implic-
itly, so it is unclear whether relevant information
can be stored and retrieved effectively. Therefore,
we inform the system of a document’s important
sentences, which are pre-identified by a separately-
trained extractor. The details of extractor training
can be found in Appendix D. After extracting im-
portant sentences in a document, we study two
methods of injecting them into the summarizer.
Text Concatenation. For each segment, we in-
clude the extracted sentences in the following way
to prioritize long-term context. We start with the
“outermost” extracted sentences, i.e., the earliest
sentence in the past segments and the last sentence
in the future segments, and repeat this process until
the input has reached the maximum length accepted
by the positional encoding of the model ( 1024 for
BART).5To differentiate the content in the cur-
rent segment from the added sentences, we prefix
the current segment and the added sentences from
before/after the current segment with “ Current
chunk: ”, “Previous important sentences: ”,
and “ Next important sentences: ”, respectively.
Text concatenation is easy to implement and most
compatible with the source modality, but the mem-
ory usage increase is quadratic to the length of the
augmented content.
Key-value Vectors. To circumvent the quadratic
memory increase, we join the key-value represen-
tations of tokens in important sentences in the en-
coder self-attentions, and directly inject them into
the summarizer encoder. The memory increase is
only linear to the augmented content’s length.
Concretely, the summarizer encoder first en-
codes all document segments and obtains the repre-
sentations (i.e., encoder outputs) of tokens belong-
ing to the extracted important sentences. During
training, the token representations of these sen-
tences are concatenated with the key-value matrices
in the encoder self-attentions while the query ma-
trix remains in its original form. Up to 1024 tokens
are concatenated via the same inclusion method
for text concatenation, to prioritize the outermost
5Other inclusion strategies can be explored in future work.

--- PAGE 6 ---
Model R-1 ↑R-2↑R-L↑Ent Prec ↑SummaC ↑Disco↓Ent Graph ↑GPU Mem ↓
Se3 46.56 23.22 44.36 98.24 14.71 7.37 1.41 11.1
BlockAttn 57.46 26.78 54.82 97.45 20.43 5.91 2.05 25.6
Longformer 57.40 26.92 54.70 97.52 20.39 5.68 2.05 25.3
LongT5 54.21 24.87 51.06 96.41 13.34 4.81 1.56 25.4
Unlimiformer 56.35 25.94 53.83 92.19 6.05 5.36 1.96 27.0
Extract-Abstract 56.89 24.76 54.26 92.82 22.07 4.03 2.09 13.2
PageSum 56.80 23.26 54.11 89.56 6.82 3.04 1.88 24.9
AWESOME using External Memory Only
Compressive 50.71†23.91 48.45†89.17 15.34 5.16†1.94†12.5
Attentive (Attn) 58.44∗27.71∗55.98∗98.33 18.98†3.62†1.98†14.0
AWESOME using Global Salient Content Only
Text-concat (Txt) 56.65†27.68∗54.11†97.93 12.23 5.05†2.09†12.0
Key-value Vectors 55.02†26.39†52.41†98.22 11.52 4.75†1.75†14.3
AWESOME (Attn + Txt) 58.76∗28.18∗56.05∗98.31 19.22†3.86†2.03†14.8
Table 2: Results on GovReport. The best and second best results per metric are bolded andunderlined . Results by
AWESOME variants that are better than all comparisons and Se3 are shaded with green and blue, respectively.
AWESOME with attentive memory only and its full version that additionally uses salient content through text
concatenation obtain the highest ROUGE scores (in green) and are comparable or better on faithfulness (Ent Prec &
SummaC) and coherence (Disco & Ent Graph) than base model Se3. ∗: our model is better than all comparisons
with approximation randomization test ( p <0.0005 );†: our model is better than Se3 ( p <0.0005 ).
sentences. A similar idea has been used by Mem-
orizing Transformer (Wu et al., 2022a) to include
retrieved text representations from past segments
for long-form language modeling. Our method
differs in two aspects. First, we extract representa-
tions from future segments , which are crucial for
accurately identifying salient content. Second, we
apply a learnable projection to the augmented rep-
resentations prior to key-value concatenation. This
process is crucial in improving compatibility with
the original key-value matrices.
4 Experimental Setups
Datasets. We conduct experiments on GovRe-
port (Huang et al., 2021), QMSum (Zhong et al.,
2021), SummScreen (Chen et al., 2022), arXiv (Co-
han et al., 2018), and BookSum (Kryscinski et al.,
2022). The average input lengths of these datasets
range from 6K to 143K (Appendix C.1).
Experiment Setups and Comparisons. Our main
experiments are conducted with a GPU memory
constraint of 27GB . For each model, we truncate
the input such that its maximum GPU memory us-
age during training does not exceed the constraint
when gradient checkpointing (Chen et al., 2016)
isdisabled . The constraint is specifically chosen
such that the baselines perform reasonably. Ap-
pendix C.2 provides information on the maximum
number of input tokens that can conform to the
constraint for other models.
For baselines, in addition to the divide-and-conquer Se3 model (Moro and Ragazzi, 2022),
we compare with state-of-the-art or popular long
document summarization systems including Block-
Attn (Phang et al., 2022), Longformer (Beltagy
et al., 2020), LongT5 (Guo et al., 2022), and Un-
limiformer (Bertsch et al., 2023). We also include
an extract-then-abstract model ( Extract-Abstract )
andPageSum (Liu et al., 2022) that leverages dy-
namic weights, as discussed in §2. All models are
initialized from BART-large , except for LongT5
that is pre-trained on long-form data. Details of
baseline models are reported in Appendix D.
Evaluation Metrics. We evaluate summary infor-
mativeness using ROUGE (Lin, 2004). To mea-
sure coherence , we use DiscoScore (Zhao et al.,
2022) ( Disco ), a reference-based metric that evalu-
ates discourse coherence by comparing focus (e.g.,
nouns) frequency and semantics between the sys-
tem summary and the reference. We also report a
graph-based reference-free coherence metric (Guin-
audeau and Strube, 2013) ( Ent Graph ), which
measures the connectivity of summary sentences
linked by entities, reflecting the coherence of topic
transitions. For summary faithfulness , we follow
prior work on text generation (Iv et al., 2022) and
show the precision of the entities ( Ent Prec ) in
the summary with respect to the document. Addi-
tionally, a recent model-based faithfulness metric,
SummaC (Laban et al., 2022), is used.
Finally, we show the maximum size of allocated
GPU memory by each model during training.

--- PAGE 7 ---
Se3: V A is required to publish information on appoint-
ment wait times at each V A medical facility for primary
care, specialty care, and hospital care and medical services,
which it does through two public websites. V A has taken
a number of actions to address deficiencies GAO found in
wait-time measurement and implementation of its schedul-
ing policy. For wait-time measurement, these actions in-
cluded changes to the wait-time measurement definitions,
provision and documentation of scheduler training, and
improved oversight through audits, all of which have been
in a state of flux for the past 6 years. On July 12, 2019, V A
provided GAO additional updates on efforts to implement
GAO’s related recommendations .
AWESOME :GAO recommended that V A either clar-
ify its scheduling policy to better define the desired date,
or identify clearer wait-time measures that are not sub-
ject to interpretation and prone to scheduler error. V A
concurred with the recommendation , which GAO has
identified as among those recommendations that warrant
priority attention. V A has taken a number of actions to
address GAO’s recommendations regarding deficiencies
GAO found in wait-time measurement and implementa-
tion of its scheduling policy. For wait-time measurement,
these actions included changes to the wait-time measure-
ment definitions, provision and documentation of sched-
uler training, and improved oversight through audits, all of
which have been in a state of flux for the past 6 years. On
July 12, 2019, V A provided GAO additional updates on
efforts to implement GAO’s related recommendations .
Table 3: Summary snippets generated by Se3 and AWE-
SOME .AWESOME ’s summary is more coherent, with
natural transitions surrounding “ GAO’s recommenda-
tion”, while Se3 abruptly introduces the topic.
5 Results
We report results by all AWESOME variants and
comparison models on GovReport in Table 2.
Compared with Se3, AWESOME variants consis-
tently achieve better performance on both ROUGE
andcoherence scores, indicating the importance
of maintaining global context for accurate salience
estimation of local content and enforcing coherent
transitions across segment-level summaries. This
can also be demonstrated by the sample outputs in
Table 3. Summaries generated by Se3 tend to be
shorter, as Se3 fails to plan at a global level. On
faithfulness, AWESOME with attentive memory
has the best entity precision among all models and
also improves SummaC over Se3, while only aug-
menting AWESOME with global salient content
hurts faithfulness. Inspecting the model outputs,
we find that using attentive memory improves un-
derstanding concepts of long-term dependencies,
e.g., connecting a strategy with its related informa-
tion that appears earlier in the report.
Of the two types of external memory mecha-
nisms, attentive memory outperforms compression-Model R-1 ↑R-2↑R-L↑Disco↓GPU↓
Se3 29.28 10.51 25.93 0.77 8.1
BlockAttn 30.76 8.26 26.49 0.50 22.8
Longformer 29.18 7.82 24.94 3.07 26.5
LongT5 31.88 10.07 27.82 0.44 25.4
Unlimiformer 30.57 8.82 26.89 0.49 26.9
Extract-Abstract 17.63 5.65 16.02 4.02 10.3
PageSum 29.55 7.38 26.11 0.31 21.5
AWESOME
Attn Only 34.86†12.69 31.09∗0.68 12.9
Attn + Txt 31.16†10.11 27.66 0.69 13.3
Table 4: Results on meeting transcripts in QMSum.
Equipped with attentive memory only, AWESOME
achieves the best ROUGE scores. Though better than
some baselines, adding extracted salient content does
not further boost the performance, due to the low per-
formance of the extractor on dialog data.
based memory on all metrics , which highlights the
advantage of adaptively updating the stored context.
Meanwhile, directly concatenating salient content
with the input yields higher ROUGE scores than
injecting key-value vectors into the attention calcu-
lation, though the latter is less memory-intensive.
We believe natural language-based augmentation
better interleaves with the document segment, echo-
ing the findings by prior work on using retrieval for
question answering (Wu et al., 2022b).
Importantly, under a strict GPU memory con-
straint, AWESOME with external memory mech-
anisms and global salient content augmentation
achieves the best ROUGE scores among all models,
while obtaining competitive results on other mea-
sures. Though efficient attention models and Page-
Sum can perform remarkably when given higher-
capacity GPUs as in the original work, they gen-
erate less informative summaries when truncation
is required to comply with the memory constraint,
emphasizing the importance of studying memory-
efficient long document summarization models.
Furthermore, with selective addition of external
memory, AWESOME adds only about 4GB of
GPU memory usage, enhancing the model perfor-
mance efficiently.
OnQMSum (Table 4), AWESOME with
attention-based memory outperforms all compar-
isons on ROUGE scores. While our models’ sum-
maries are more coherent than the summaries by
Se3, as measured by DiscoScore, the differences
among all models are less pronounced compared
to the ones on GovReport. This is because QM-
Sum contains shorter summaries than GovReport

--- PAGE 8 ---
Model R-1 ↑R-2↑R-L↑Ent G↑GPU↓
Se3 38.09 11.30 36.56 0.50 11.3
BlockAttn 32.01 8.99 30.90 1.61 25.7
Longformer 42.78 13.21 41.34 0.97 25.3
LongT5 42.03 12.67 40.76 1.03 25.4
Unlimiformer 35.17 11.98 34.28 1.33 27.0
Extract-Abstract 19.95 5.58 19.70 0.06 13.1
AWESOME
Attn Only 46.05†13.09†44.21†0.81†13.2
Attn + Txt 45.30†12.63†43.51†0.90†14.2
Table 5: Results on TV transcripts in SummScreen. We
report Ent Graph instead of DiscoScore, as DiscoScore
encounters errors when identifying focus. AWESOME
with the attentive memory obtains the best R1 and RL
scores, while the low accuracy of the extracted salient
content leads to performance drop of the summarizer.
(69 vs. 553), thus involving fewer topic transitions.
We also find that the extractor performs poorly on
QMSum, leading to degraded results after augment-
ing our model with the extracted salient content.
Specifically, the F1 score of the extractor on the test
set is only 1.29, as opposed to 27.85on GovReport.
Compared to our model, the extract-then-abstract
model is more prone to its errors and produce sum-
maries of the lowest quality on QMSum.
This trend is similarly observed on Summ-
Screen (Table 5) and arXiv (Table 6), where the
extract-then-abstract method performs poorly and
adding extracted content leads to performance drop
ofAWESOME due to the low performance of the
extractor. Meanwhile, AWESOME with the at-
tentive memory is able to obtain the best ROUGE-
1 and ROUGE-L scores. On arXiv, models that
use efficient attentions obtain the higher ROUGE
scores, because truncating arXiv documents has
little effect on summary generation—arXiv arti-
cles have the most uneven distributions of salient
content, where only about 10% of new salient bi-
grams are located in the second halves of the docu-
ments (Huang et al., 2021).
Finally, experiments on BookSum show that the
divide-and-conquer method produces better sum-
maries for long novels, while our method can fur-
ther boost its performance (Table 7). However, we
find it necessary to incorporate external memory
into all layers, suggesting a more complex interac-
tion of external memory with the summarization
process for novel plots. Unlike other document
types tested, novel plots are typically sequential
with less redundancy, which reduces the necessity
of the memory mechanism.Model R-1 ↑R-2↑R-L↑Disco↓GPU↓
Se3 40.74 17.96 36.87 1.33 12.8
BlockAttn 49.12 21.69 44.40 1.77 25.7
Longformer 48.59 21.45 43.99 2.17 25.2
LongT5 48.25 20.74 43.41 0.97 25.5
Unlimiformer 47.78 20.58 43.22 1.22 26.8
Extract-Abstract 42.37 16.43 38.62 1.03 15.3
PageSum 46.01 18.77 41.55 0.88 26.2
AWESOME
Attn Only 42.51†18.96†38.56†1.30 16.0
Attn + Txt 44.20†18.89†40.07†1.32 16.5
Table 6: Results on arXiv papers. AWESOME variants
again outperform Se3. For 80% of the arXiv documents,
efficient attention models and PageSum can fully train
on their first halves, covering 90% of the salient content
that appear in the references (Huang et al., 2021), thus
the better ROUGE scores than models encoding smaller
segments.
Model R-1 ↑R-2↑R-L↑Disco↓GPU↓
Se3 40.78 10.16 39.77 10.46 11.5
BlockAttn 23.45 3.09 22.09 190.27 25.7
Longformer 20.20 2.45 18.55 204.48 25.3
LongT5 33.15 6.74 32.62 24.24 25.5
Unlimiformer 38.09 9.55 37.41 47.72 27.0
AWESOME (Attn) 41.11 10.63 40.20 10.36 24.0
Table 7: Results on novels in BookSum. AWESOME
with attentive memory in all layers achieves the best
performance on all metrics. Methods requiring external
extractors are not included due to the computational cost
of building extractive oracles for long novels.
6 Conclusion
We present AWESOME for summarizing long doc-
uments in a memory-constrained setting. Based
on the divide-and-conquer strategy, AWESOME
uses two mechanisms to gather global context and
improve summary quality. First, external memo-
ries on the encoder and decoder are employed to
track previously read document content and the
corresponding summaries. Second, the encoder
is informed of global salient content predicted by
an extractor via text or representation concatena-
tion. On five summarization datasets, AWESOME
generates summaries with better informativeness,
faithfulness, and coherence than a baseline divide-
and-conquer system. Under the same memory con-
straint, AWESOME outperforms competitive mod-
els that leverage efficient attentions or dynamic ex-
traction to preserve global context, highlighting its
effectiveness in supplying global context.

--- PAGE 9 ---
7 Limitations
AWESOME ’s external memory mechanism is re-
stricted to operating solely from past segments to
the current segment. This means that the model
does not leverage the information contained in fu-
ture segments, which can be relevant for a com-
prehensive understanding of the current segment.
To address this limitation, we have designed the
global salient content augmentation mechanism to
cover context from the future segments, yet more
advanced solutions can be explored in future work.
For example, on the encoder, making the external
memory bidirectional is a potential approach.
While being memory-efficient, the external
memory mechanism of AWESOME necessitates
a longer running time due to its recurrent nature.
The need for recurrent computations may lead to
increased processing requirements, which could
impact real-time applications or scenarios where
rapid responses are crucial. The running times of
different models are provided in Appendix B.1 for
reference. Although our model is slower than that
of LongT5 and Se3, it still outperforms several
other competitive models in terms of speed, and we
will investigate methods for reducing the running
time in future work.
8 Ethical Consideration
We anticipate that one of the major use cases of
AWESOME is to allow ordinary users who have
computing devices with limited memory to quickly
understand government policies and other types
of long documents. However, we recognize that
the system generated summaries might not compre-
hensively cover the salient content that is essential
for correctly understanding the policies, causing
risks ranging from capital loss to legal liability.
Moreover, system summaries might contain state-
ments that cannot be verified through the document,
which further adds to the risks of real-world deploy-
ment. We suggest developers who intend to use our
model for real-world application carefully study
the outputs by our model before the actual deploy-
ment.
References
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
Amanda Bertsch, Uri Alon, Graham Neubig, andMatthew R. Gormley. 2023. Unlimiformer: Long-
range transformers with unlimited length input.
Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. 2022.
Recurrent memory transformer. In Advances in Neu-
ral Information Processing Systems .
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin
Gimpel. 2022. SummScreen: A dataset for abstrac-
tive screenplay summarization. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
8602–8615, Dublin, Ireland. Association for Compu-
tational Linguistics.
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos
Guestrin. 2016. Training deep nets with sublinear
memory cost. arXiv preprint arXiv:1604.06174 .
Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
Trung Bui, Seokhwan Kim, Walter Chang, and Nazli
Goharian. 2018. A discourse-aware attention model
for abstractive summarization of long documents. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers) , pages 615–621, New Or-
leans, Louisiana. Association for Computational Lin-
guistics.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-XL: Attentive language models beyond
a fixed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 2978–2988, Florence, Italy. Asso-
ciation for Computational Linguistics.
Alexios Gidiotis and Grigorios Tsoumakas. 2020. A
divide-and-conquer approach to the summarization of
long documents. IEEE/ACM Transactions on Audio,
Speech, and Language Processing , 28:3029–3040.
Camille Guinaudeau and Michael Strube. 2013. Graph-
based local coherence modeling. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 93–103, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Mandy Guo, Joshua Ainslie, David Uthus, Santiago On-
tanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.
2022. LongT5: Efficient text-to-text transformer for
long sequences. In Findings of the Association for
Computational Linguistics: NAACL 2022 , pages 724–
736, Seattle, United States. Association for Compu-
tational Linguistics.
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng
Ji, and Lu Wang. 2021. Efficient attentions for long
document summarization. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1419–1436, Online.
Association for Computational Linguistics.

--- PAGE 10 ---
Robert Iv, Alexandre Passos, Sameer Singh, and Ming-
Wei Chang. 2022. FRUIT: Faithfully reflecting up-
dated information in text. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 3670–3686, Seattle,
United States. Association for Computational Lin-
guistics.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. In Inter-
national Conference on Learning Representations .
Wojciech Kryscinski, Nazneen Rajani, Divyansh Agar-
wal, Caiming Xiong, and Dragomir Radev. 2022.
BOOKSUM: A collection of datasets for long-form
narrative summarization. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2022 ,
pages 6536–6558, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Philippe Laban, Tobias Schnabel, Paul N. Bennett, and
Marti A. Hearst. 2022. SummaC: Re-visiting NLI-
based models for inconsistency detection in summa-
rization. Transactions of the Association for Compu-
tational Linguistics , 10:163–177.
Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara
Berg, and Mohit Bansal. 2020. MART: Memory-
augmented recurrent transformer for coherent video
paragraph captioning. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 2603–2614, Online. Association
for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 7871–7880, Online. Association for Computa-
tional Linguistics.
Chuan Li. 2022. Best gpu for deep learning in 2022 (so
far).
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Yang Liu and Mirella Lapata. 2019. Hierarchical trans-
formers for multi-document summarization. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5070–
5081, Florence, Italy. Association for Computational
Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .Yixin Liu, Ansong Ni, Linyong Nan, Budhaditya
Deb, Chenguang Zhu, Ahmed H. Awadallah, and
Dragomir Radev. 2022. Leveraging locality in ab-
stractive text summarization.
Ziming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang,
Rui Zhang, Tao Yu, Budhaditya Deb, Chenguang
Zhu, Ahmed Awadallah, and Dragomir Radev. 2022.
DYLE: Dynamic latent extraction for abstractive
long-input summarization. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
1687–1698, Dublin, Ireland. Association for Compu-
tational Linguistics.
Gianluca Moro and Luca Ragazzi. 2022. Semantic self-
segmentation for abstractive summarization of long
documents in low-resource regimes. Proceedings
of the AAAI Conference on Artificial Intelligence ,
36(10):11085–11093.
OpenAI. 2023. Gpt-4 technical report.
Jason Phang, Yao Zhao, and Peter J Liu. 2022.
Investigating efficiently extending transformers
for long input summarization. arXiv preprint
arXiv:2208.04347 .
Jonathan Pilault, Raymond Li, Sandeep Subramanian,
and Chris Pal. 2020. On extractive and abstractive
neural document summarization with transformer lan-
guage models. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 9308–9319, Online. As-
sociation for Computational Linguistics.
Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku-
mar, Chloe Hillier, and Timothy P. Lillicrap. 2020.
Compressive transformers for long-range sequence
modelling. In International Conference on Learning
Representations .
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing . Associa-
tion for Computational Linguistics.
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and
David Grangier. 2021. Efficient content-based sparse
attention with routing transformers. Transactions of
the Association for Computational Linguistics , 9:53–
68.
Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and
Da-Cheng Juan. 2020. Sparse sinkhorn attention.
InInternational Conference on Machine Learning ,
pages 9438–9447. PMLR.

--- PAGE 11 ---
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Sti-
ennon, Ryan Lowe, Jan Leike, and Paul Christiano.
2021. Recursively summarizing books with human
feedback. arXiv preprint arXiv:2109.10862 .
Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,
and Christian Szegedy. 2022a. Memorizing trans-
formers. In International Conference on Learning
Representations .
Yuxiang Wu, Yu Zhao, Baotian Hu, Pasquale Min-
ervini, Pontus Stenetorp, and Sebastian Riedel.
2022b. An efficient memory-augmented transformer
for knowledge-intensive nlp tasks.
Manzil Zaheer, Guru Guruganesh, Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
and Amr Ahmed. 2021. Big bird: Transformers for
longer sequences.
Yusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu,
Chenguang Zhu, Budhaditya Deb, Ahmed Awadallah,
Dragomir Radev, and Rui Zhang. 2022. Summn: A
multi-stage summarization framework for long input
dialogues and documents. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1592–
1604, Dublin, Ireland. Association for Computational
Linguistics.
Wei Zhao, Michael Strube, and Steffen Eger. 2022. Dis-
coscore: Evaluating text generation with bert and
discourse coherence.
Yao Zhao, Mohammad Saleh, and Peter J Liu.
2020. Seal: Segment-wise extractive-abstractive
long-form text summarization. arXiv preprint
arXiv:2006.10213 .
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia
Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli
Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir
Radev. 2021. QMSum: A new benchmark for query-
based multi-domain meeting summarization. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
5905–5921, Online. Association for Computational
Linguistics.
A Divide-and-Conquer Architecture
We choose Se3 (Moro and Ragazzi, 2022) as our
base divide-and-conquer architecture because it can
be applied to any document-summary pair. In or-
der to create divide-and-conquer training data for
summarization, for each document-summary pair,the document is first divided into segments (§A.1)
and each summary sentence is then assigned to a
document segment as part of the generation target
(§A.2).
A.1 Document Segmentation
Algorithm 1: Document Segmentation
Data: Input document doc; Segment min,
max length lmin,lmax
1segs←[];
2currSeg ←[];
3foreach sent indocdo
4 iflen(currSeg )< lminthen
5 currSeg ←currSeg + [sent];
6 end
7 else if len(currSeg )> lmaxthen
8 segs←segs + [currSeg ];
9 currSeg ←[sent];
10 end
11 else
12 nextSeg ←pseudoSegment;
13 ifsim(nextSeg, sent )>
sim(currSeg, sent )then
14 segs←segs + [currSeg ];
15 currSeg ←[sent];
16 end
17 else
18 currSeg ←currSeg + [sent]
19 end
20 end
21end
22segs←segs + [currSeg ];
23return segs
The length of each document segment is between
512and768tokens. During segmentation, the al-
gorithm loops through all document sentences, as
shown in Algorithm 1. A document sentence will
be added to the current segment if the segment con-
tains less than 512tokens. The current segment will
be finalized if the current segment contains more
than 768tokens or the current sentence is more
semantically similar to the next pseudo segment
than the current segment, where the next pseudo
segment is created by including future sentences
until reaching 512tokens. To measure the simi-
larity between the current sentence and a segment,
we use the average cosine similarity between the
representation of the current sentence and represen-
tations of the sentences in the segment. Sentence

--- PAGE 12 ---
Batch / s
0.00.51.01.5
Se3
BlockAttnLongformerLongT5PageSumAWESOMEFigure 2: Running time (batch per second) of each
model. A higher number of batches processed per sec-
ond indicates a faster running speed. All models use
a batch size of 1 and the input is truncated to 16384
tokens.
representations are obtained using Sentence Trans-
former (Reimers and Gurevych, 2019) with the
all-roberta-large-v1 model.
A.2 Target Assignment
For each sentence in the reference summary, we
calculate its ROUGE scores with the document
segments. The sentence will then be assigned to the
document segment with which yields the highest
ROUGE-1 and ROUGE-2 scores.
B Additional Results
B.1 Running Time
We compare the model running time on GovRe-
port (Figure 2). The input document is truncated
to16384 tokens and each model is separately train
for1000 steps with a batch size of 1. No other
computation-heavy program is running at the same
time. While AWESOME take longer time to com-
plete training than Se3, it is still the third fastest
model.
C Dataset Details
C.1 Statistics
We conduct experiments on five long document
summarization datasets with diverse genres. Gov-
Report (Huang et al., 2021) contains long reports
and their summaries written by government re-
search agencies. QMSum (Zhong et al., 2021)
is a query-focused long meeting transcript sum-
marization dataset, with summary-worthy content
spread over the documents. We prepend the query
to all segments. We further use a screenplay sum-
marization dataset, SummScreen (Chen et al.,# Samples # Word
Dataset Train Dev Test Doc Summ
GovReport 17,516 974 973 9,409 553
QMSum 1,257 272 279 9,070 70
SummScreen 18,915 1,795 1,793 6,421 381
arXiv 203,037 6,436 6,440 6,030 273
BookSum 314 45 46 143,301 1,294
Table 8: Statistics of datasets used in our experiments.
Dataset
Model Gov arXiv QMSum SumScrn Book
Se3 50x 50x 50x 50x 50x
Ext-Abs†1x (∞) 1x (∞) 1x (∞) 1x (∞) -
BlockAttn 6x 6x 8x 6x 6x
Longformer 8x 8x 8x 8x 8x
LongT5 6x 6x 6x 6x 6x
Unlimiformer 2x 2x 2x 2x 2x
PageSum 3x 5x 2x - -
AWESOME 50x 50x 50x 50x 50x
Table 9: Truncation thresholds (multiply by 1024 ) used
by each model on different datasets to comply with the
memory constraint during training. †: For the extract-
then-abstract model, the abstractor has a maximum input
length of 1024 , while the extractor can consume all
sentences in the document.
2022), which contains the transcripts of TV se-
ries. The TMS subset, with more samples and
longer summaries, is selected. Moreover, we exper-
iment with the scientific papers and their abstracts
from arXiv (Cohan et al., 2018). Finally, we test
our models on summarizing fullnovels in Book-
Sum (Kryscinski et al., 2022). For all datasets, we
use the official train/dev/test splits if their original
data files are released.
Statistics of datasets are reported in Table 8. For
GovReport6, QMSum7, and SummScreen (Chen
et al., 2022), we use the data released by the origi-
nal papers. For arXiv, we use the version provided
by Huggingface Datasets.8As the original data
files for BookSum are not released due to sum-
mary copyright, we use the version reproduced by
Unlimiformer (Bertsch et al., 2023).
C.2 Input Truncation
In our main experiments, we employ a GPU mem-
ory constraint of 27GB. As some baseline models
require the input length to be a multiplier of 1024 ,
setting a constraint of 24GB, a more common num-
6https://gov-report-data.github.io/
7https://github.com/Yale-LILY/QMSum
8https://huggingface.co/datasets/scientific_
papers

--- PAGE 13 ---
ber, would lead to further truncation and significant
performance drop.
To fit models into our memory constraint, we
truncate the model inputs. The truncation thresh-
olds used by each model on different datasets are
shown in Table 9. Although Se3 and AWESOME
theoretically maintain a consistent GPU memory
consumption during training regardless of the num-
ber of input tokens processed, we have chosen to
restrict the maximum number of input tokens in a
training sample to 51200 for reasonable training
time.
D Implementation Details
Baselines. BlockAttn and Longformer use block-
wise attentions (Phang et al., 2022) and sliding-
window attentions (Beltagy et al., 2020), where
a global token can attend to and be attended
by all tokens, while other tokens can only at-
tend to tokens in the same block or window.
LongT5 (Guo et al., 2022) is a sliding-window
attention model pre-trained on long sequences, and
Unlimiformer (Bertsch et al., 2023) extends BART
by selecting input tokens to be attended to via KNN
searching. For the extract-then-abstract approach,
we use the same extractor as in the global salient
content augmentation of our model, and the abstrac-
tor takes as input oracle extracted sentences during
training. Lastly, PageSum (Liu et al., 2022) synthe-
sizes the output representations given by different
document segments with dynamic weights.
Extractor. The extractor first uses a
RoBERTa (Liu et al., 2019) to encode each
sentence and takes the average of the final layer’s
outputs as the sentence representation. It then
applies a self-attention on top of all sentence
representations. The resulting representations
are converted to extraction scores after applying
a multi-layer perception with one hidden layer.
The extractor is trained with oracle extractive
labels that are constructed by greedily searching
for document sentences that maximize the sum
of ROUGE-1 and ROUGE-2 scores, compared
against the reference summary. We do not compute
ROUGE-L as in DYLE (Mao et al., 2022), because
finding the longest common subsequence is
computationally expensive and does not yield
performance gain.
Training Parameters. We train all models with
a maximum learning rate of 5×10−5, except thatLongT5 is trained with a maximum learning rate
of1×10−4. We use a running batch size of 1
and apply gradient accumulation to achieve an ef-
fective batch size of 8. The numbers of training
epochs are 3,9,6,2,10on GovReport, QMSum,
SummScreen, arXiv, and BookSum, with warmup
steps of 300,100,300,1000 , and 40. Due to the
computational cost of training long document sum-
marization, each model is trained for a single run.
Model Size. AWESOME is based on
BART-large9and has 708 millions of parameters.
Computing Infrastructure. All experiments are
conducted on RTX A6000 GPUs.
Evaluation Metrics. For ROUGE (Lin, 2004),
we use the Python implementation by Google.10
The official code for DiscoScore (Zhao et al., 2022)
is used11, which also provides an implementation
of the Ent Graph metric (Guinaudeau and Strube,
2013). We implement the entity precision mea-
sure ourselves and run the official code for Sum-
maC (Laban et al., 2022).12All metrics used are
open-source and can be distributed for research
purposes.
9https://huggingface.co/facebook/bart-large
10https://pypi.org/project/rouge-score/
11https://github.com/AIPHES/DiscoScore
12https://github.com/tingofurro/summac
