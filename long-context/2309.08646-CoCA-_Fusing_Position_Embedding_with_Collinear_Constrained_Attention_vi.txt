CoCA: Kết hợp Nhúng Vị trí với Chú ý Ràng buộc Thẳng hàng
trong Transformers để Mở rộng Cửa sổ Ngữ cảnh Dài
Shiyi Zhu, Jing Ye, Wei Jiang, Siqiao Xue, Qi Zhang, Yifan Wu, Jianguo Li
Ant Group
{zhushiyi.zsy, qianye.yj, shouzhi.jw, lijg.zero}@antgroup.com
Tóm tắt
Tự chú ý và nhúng vị trí là hai
mô-đun chính trong các Mô hình Ngôn ngữ Lớn
(LLMs) dựa trên transformer. Tuy nhiên, mối
quan hệ tiềm năng giữa chúng còn xa mới được
nghiên cứu kỹ, đặc biệt là để mở rộng cửa sổ
ngữ cảnh dài. Thực tế, các hành vi bất thường
gây hại cho việc ngoại suy ngữ cảnh dài tồn tại
giữa Nhúng Vị trí Xoay (RoPE) và tự chú ý
vanilla được công việc của chúng tôi tiết lộ. Để
giải quyết vấn đề này, chúng tôi đề xuất một cơ
chế chú ý mới, CoCA (Chú ý Ràng buộc Thẳng hàng).
Cụ thể, chúng tôi áp đặt một ràng buộc thẳng hàng
giữa Q và K để tích hợp liền mạch RoPE
và tự chú ý. Mặc dù chỉ thêm độ phức tạp
tính toán và không gian tối thiểu, việc tích hợp này
đáng kể nâng cao khả năng ngoại suy cửa sổ
ngữ cảnh dài. Chúng tôi cung cấp một triển khai
tối ưu, làm cho nó trở thành một sự thay thế
drop-in cho bất kỳ mô hình dựa trên transformer
hiện có nào. Các thí nghiệm rộng rãi cho thấy
CoCA hoạt động đặc biệt tốt trong việc mở rộng
cửa sổ ngữ cảnh. Một mô hình GPT dựa trên CoCA,
được huấn luyện với độ dài ngữ cảnh 512, có thể
mở rộng liền mạch cửa sổ ngữ cảnh lên đến 32K
(60 ×), mà không cần bất kỳ tinh chỉnh nào.
Ngoài ra, bằng cách áp dụng CoCA trong LLaMA-
7B, chúng tôi đạt được ngoại suy lên đến 32K
chỉ trong 2K độ dài huấn luyện. Mã nguồn của chúng tôi
được công khai tại: https://github.com/codefuse-
ai/Collinear-Constrained-Attention

1 Giới thiệu
Trong công trình tiên phong của Transformer (Vaswani et al.,
2017), nó tuyên bố khả năng "ngoại suy đến độ dài
chuỗi dài hơn những gì gặp phải trong quá trình huấn
luyện". Đây là một giả thuyết lý tưởng, nhưng
thực tế không hoạt động trong thực tế cho Transformer vanilla.
Một số công trình tiếp theo, được gọi chung là
ngoại suy ngữ cảnh dài, đã đi sâu vào việc khám phá
các khả năng của các mô hình ngôn ngữ lớn
(LLMs) được huấn luyện trong phạm vi [1, N−1] để
hiệu quả mở rộng chuỗi kiểm tra ≥N.

Hình 1: Đánh giá perplexity trên 100 tài liệu PG-19 với
chiến lược cửa sổ trượt (Stride = 512). Perplexity của
RoFormer (Su et al., 2024) vượt quá 1000 một cách mạnh mẽ
vượt qua độ dài huấn luyện của nó, trong khi CoCA duy trì
một cao nguyên thấp ngay cả ở 60× độ dài huấn luyện của nó.
ALibi (Press et al., 2022) gặp phải vấn đề Hết Bộ nhớ (OOM)
cho đầu vào Nmax>8000 do không tương thích với flash-attention
(Dao et al., 2022), chúng tôi giả định nó duy trì perplexity
cho Nmax>8000.

Các nghiên cứu hiện có chủ yếu tập trung vào kernel
chú ý (Beltagy et al., 2020; Ding et al., 2023; Han
et al., 2023) hoặc nhúng vị trí (Huang et al.,
2023), thường bỏ qua mối quan hệ nội tại
giữa hai mô-đun chính. Bias chú ý là một
thay thế cho việc mã hóa rõ ràng thông tin vị trí.
ALibi (Press et al., 2022) và KERPLE
(Chi et al., 2022), kết hợp bias chú ý nhân quả
âm dựa trên heuristic và kernel tam giác tổng hợp,
tương ứng. Mặc dù các phương pháp này
hiệu quả quản lý để duy trì perplexity thấp, chúng
thiếu sót trong việc nắm bắt các phụ thuộc tầm xa do
giới thiệu các giả thuyết cục bộ cho các token ngữ cảnh.

Một nhánh khác của các phương pháp bao gồm việc đơn giản
mở rộng Nhúng Vị trí Xoay (RoPE) (Su et al.,
2024) để ngoại suy độ dài ngữ cảnh suy luận
với việc tinh chỉnh tối thiểu hoặc không có. Ví dụ, Nội suy
Vị trí (PI) (Chen et al., 2023) sử dụng
mở rộng tuyến tính trên mỗi số vị trí từ n đến
n/k, trong đó k là tỷ lệ ngoại suy. NTK-aware
Scaled RoPE (bloc97, 2023) và Dynamic-NTK

--- TRANG 2 ---
(Emozilla, 2023) kết hợp ngoại suy tần số cao
và nội suy tần số thấp. Chúng mở rộng
cơ sở trong RoPE dựa trên độ dài chuỗi để
thích ứng với các chỉ số vị trí chưa thấy. Tuy nhiên,
các phương pháp này chủ yếu giảm nhẹ vấn đề
mô hình hóa các góc xoay ở các vị trí
ngoài phân phối, mà không nhận ra mối tương quan
nội tại giữa ma trận chú ý và góc xoay.
Do đó, các phương pháp này vẫn gặp phải tỷ lệ
mở rộng cửa sổ ngữ cảnh hạn chế.

Ở đây, chúng tôi trình bày một góc nhìn mới về
mối quan hệ giữa nhúng vị trí (với trọng tâm
vào RoPE) và cơ chế tự chú ý.
Tóm lại, RoPE sử dụng ma trận xoay để
mã hóa vị trí tuyệt đối đồng thời
kết hợp các phụ thuộc vị trí tương đối rõ ràng
trong công thức tự chú ý (Su et al.,
2024). Nó được thiết kế dựa trên sự khác biệt
góc tương đối giữa các truy vấn (Q) và khóa (K).
Tuy nhiên, các mối quan hệ tiềm ẩn tồn tại giữa Q và
K, vì hai ma trận này được nhân trực tiếp.
Chúng tôi chứng minh rằng việc khởi tạo không đúng
góc giữa Q và K trong RoPE dẫn đến hành vi
không mong muốn xung quanh ranh giới cửa sổ ngữ cảnh,
gây hại cho hiệu suất ngoại suy ngữ cảnh của nó.

Để giải quyết hành vi không mong muốn này, chúng tôi đề xuất
một kiến trúc sáng tạo gọi là Chú ý
Ràng buộc Thẳng hàng (CoCA). Cụ thể, chúng tôi
áp đặt một ràng buộc thẳng hàng giữa Q và K
bằng cách khởi tạo góc giữa mỗi hai chiều
ẩn trong các vector Q và K thành 0. Điều này
cho phép tích hợp liền mạch RoPE và tự
chú ý. Kiến trúc mô hình và so sánh
với RoFomer (Su et al., 2024) được minh họa trong
Hình 2.

Các thí nghiệm rộng rãi cho thấy một mô hình
GPT dựa trên CoCA, được huấn luyện trong độ dài ngữ cảnh 512,
mở rộng liền mạch cửa sổ ngữ cảnh lên đến 32K
(60x) mà không có sự phân kỳ perplexity. Một so sánh
toàn diện giữa phương pháp của chúng tôi và các
phương pháp hiện có được trình bày trong Hình 1. Hơn nữa,
nó nâng cao khả năng truy xuất ngữ cảnh dài,
đạt được độ chính xác truy xuất passkey 50%+
ngay cả khi ngoại suy đến 16x dài hơn
độ dài ngữ cảnh huấn luyện của nó bằng cách áp dụng Dynamic-NTK
(Emozilla, 2023). Ngoài ra, bằng cách áp dụng CoCA
trong LLaMA-7B, chúng tôi đạt được ngoại suy lên đến 32K
chỉ trong 2K độ dài huấn luyện.

Các đóng góp chính của chúng tôi có thể được tóm tắt như
sau:
• Chúng tôi tiết lộ hành vi ranh giới ngữ cảnh không mong muốn
do việc thiếu mô hình hóa mối quan hệ giữa nhúng vị trí
và tự chú ý.
• Để giải quyết hành vi ranh giới ngữ cảnh không mong muốn,
chúng tôi đề xuất Chú ý Ràng buộc Thẳng hàng (CoCA) để
tích hợp liền mạch nhúng vị trí và tự chú ý, đạt được
hiệu suất ngoại suy cửa sổ ngữ cảnh dài xuất sắc.
• CoCA mở rộng cửa sổ ngữ cảnh của nó từ 512
đến 32K mà không tinh chỉnh, đạt được hơn
50% độ chính xác ngay cả khi 16× dài hơn
độ dài huấn luyện của nó. Sử dụng CoCA trong LLaMA-7B,
chúng tôi đạt được ngoại suy lên đến 32K chỉ trong
2K độ dài huấn luyện.
• CoCA giới thiệu độ phức tạp tính toán và
không gian tối thiểu so với tự chú ý vanilla.
Chúng tôi cung cấp một triển khai tối ưu của CoCA,
làm cho nó có thể là một sự thay thế drop-in liền mạch
cho các mô hình dựa trên transformer hiện có.

2 Phương pháp
Trong phần này, chúng tôi mô tả Chú ý
Ràng buộc Thẳng hàng (CoCA) được đề xuất. Chúng tôi bắt đầu với
việc giới thiệu lý thuyết nền tảng của RoPE (Su et al.,
2024) trong Phần 2.1, và sau đó phân tích các
hành vi bất thường giữa ma trận chú ý và
RoPE trong Phần 2.2. Cuối cùng, chúng tôi giới thiệu
phương pháp CoCA được đề xuất trong phần 2.3 và suy ra
một phiên bản ràng buộc lỏng của CoCA trong Phần 2.4,
tương ứng.

2.1 Nhúng Vị trí Xoay
Nhúng vị trí là một thành phần quan trọng trong
các mô hình dựa trên transformer. Ở đây chúng tôi tập trung vào
Nhúng Vị trí Xoay (RoPE) (Su et al., 2024),
được sử dụng rộng rãi bởi các LLMs bao gồm LLaMA
(Touvron et al., 2023a), LLaMA-2 (Touvron et al.,
2023b), GPT-NeoX (Black et al., 2022) và Qwen
(Bai et al., 2023). Giả sử chỉ số vị trí là
một số nguyên n ∈ [1, N], và vector đầu vào
tương ứng x = [x0, x1, ..., xd−1]T, trong đó N là
độ dài chuỗi, d là chiều của đầu
chú ý. RoPE định nghĩa một hàm phức có giá trị vector
f(x, n):
f(x, n) = [(x0+ix1)einθ0,(x2+ix3)einθ1,
. . . ,(xd−1+ixd)einθd/2−1]T,
trong đó θj=B−2j/d,(1)

--- TRANG 3 ---
Hình 2: So sánh kiến trúc giữa RoFormer và CoCA. (a) RoFormer; (b) CoCA; (c) Chi tiết triển khai của K trong CoCA. Q, T, và V được tạo ra bằng các ma trận chiếu giống hệt với những ma trận được sử dụng trong tự chú ý vanilla. T trải qua một phép chia đôi, với nửa còn lại được nhân đôi. K sau đó được tính toán như tích element-wise của Q và T, tuân thủ ràng buộc thẳng hàng với Q. Lưu ý rằng kn ∈ RN×d, trong đó n ∈ [1, N] là chỉ số vị trí của key, d là chiều head, N là độ dài chuỗi.

trong bài báo này, cơ sở B = 10.000.
Sau khi áp dụng RoPE, các vector được biến đổi
cho truy vấn (q) và khóa (k) trở thành f(q, m)
và f(k, n), tương ứng. Ở đây, m, n ∈ [0, N]
đại diện cho các chỉ số vị trí của q và k. Phép toán
chú ý được tính toán như tích vô hướng
giữa f(q, m) và f(k, n), được định nghĩa như sau:
a(m, n) =Re(⟨f(q, m), f(k, n)⟩)
=Re
[∑(d/2−1, j=0)(q2j+iq2j+1)(k2j−ik2j+1)ei(m−n)θj]

=∑(d/2−1, j=0)[(q2jk2j+q2j+1k2j+1) cos((m−n)θj)
+ (q2jk2j+1−q2j+1k2j) sin((m−n)θj)]
(2)
Điểm chú ý a(m−n) phụ thuộc vào vị trí
tương đối (m−n).

2.2 Hành vi Bất thường giữa RoPE và
Ma trận Chú ý
Sau khi áp dụng RoPE, điểm chú ý a(m−n)
có thể được hiểu như tổng của d/2 tích
vô hướng của các số phức, như được minh họa trong Phương trình
(2). Đối với bất kỳ cặp qj = (q2j, q2j+1) và
kj = (k2j, k2j+1) nào, là phép cắt 2 chiều
của q (hoặc qm) và k (hoặc kn), chúng tôi giới thiệu
góc ban đầu Θj giữa chúng, được đo ngược
chiều kim đồng hồ từ kj đến qj trong mặt phẳng phức.
Trong suốt quá trình phân tích của chúng tôi, chúng tôi giữ vị trí của
kj cố định, xoay qj một cách có hệ thống để
toàn diện kiểm tra các vị trí tương đối của chúng. Góc
cuối cùng giữa qj và kj được biểu diễn như
θ(qj,kj) = Θj + (m−n)θj, trong đó m và n là
các chỉ số vị trí của qj và kj.

Trong khái niệm này, điểm chú ý có thể được
công thức hóa như:
a(m, n) =∑(d/2−1, j=0)|qj||kj|cos(θ(qj,kj)) (3)
Tham khảo Hình 3 để có đại diện trực quan của
khái niệm này cho bất kỳ j ∈ [0, d/2] riêng lẻ nào trong
không gian con 2-D. Có bốn kịch bản riêng biệt
giữa qj và kj sau khi xoay.
(1) Kịch bản (b) và (c): Khi m > n và
Θj ≤ π, hoặc m < n và Θj > π, giá trị của
cos(θ(qj,kj)) giữa qj và kj giảm với
khoảng cách mở rộng giữa m và n. Trong 2
kịch bản này, không quan sát thấy hành vi bất thường nào,
vì điểm chú ý tự nhiên giảm với khoảng cách
vị trí. Xu hướng này tiếp tục cho đến khi góc
tương đối θ(qj,kj) xoay vượt quá ranh giới
của π.

--- TRANG 4 ---
Hình 3: Hành vi bất thường của RoPE trong mặt phẳng 2-D. Tích vô hướng của các vector qj và kj phụ thuộc vào góc tương đối θ(qj,kj), được định nghĩa là Θj + (m−n)θj. Ở đây, Θj đại diện cho góc ban đầu, và (m−n)θj biểu thị góc xoay phụ thuộc vị trí. (a) m < n và Θj ≤ π. (b) m > n và Θj ≤ π. (c) m < n và Θj > π. (d) m > n và Θj > π.

(2) Kịch bản (a) và (d): Khi m < n và
Θj ≤ π, hoặc m > n và Θj > π, các hiện tượng
hấp dẫn xuất hiện. Khi khoảng cách giữa m và
n tăng, giá trị của cos(θ(qj,kj)) giữa qj
và kj nghịch lý tăng lên. Bất thường này có
ảnh hưởng đáng kể đến điểm chú ý, đặc biệt
ảnh hưởng đến τ token gần nhất. Trong ngữ cảnh này, τ được
định nghĩa là Θj/θj cho kịch bản (a) và (2π−Θj)/θj
cho kịch bản (d). Do đó, điểm chú ý cho
các token này bị giảm một cách bất thường.

Đối với các mô hình ngôn ngữ hai chiều, tất cả bốn trường hợp
có thể xảy ra. Đối với các mô hình nhân quả, chỉ kịch bản (b)
và (d) biểu hiện, vì m luôn vượt quá n.
Điểm chú ý a(m−n) là tổng của d/2
tích vô hướng, một trong số chúng trở nên bất thường
có thể không đáng kể, tuy nhiên, các thí nghiệm đã
xác nhận tầm quan trọng này. Phân tích sâu hơn về
hành vi bất thường ranh giới xoay này được thảo luận trong
Phụ lục D.2.

2.3 Chú ý Ràng buộc Thẳng hàng
Để giải quyết hành vi bất thường giữa RoPE
và ma trận chú ý, chúng tôi đề xuất một phương pháp
mới gọi là Chú ý Ràng buộc Thẳng hàng
(CoCA). Cụ thể, bằng cách áp dụng ràng buộc thẳng hàng
cho bất kỳ cặp qj = (q2j, q2j+1) và kj =
(k2j, k2j+1) nào, chúng tôi tích hợp liền mạch RoPE vào
cơ chế tự chú ý, đạt được ngoại suy ngữ cảnh dài.

Để chính thức hóa điều này, xem xét một chuỗi N
token đầu vào SN={wn}N(n=1), với các
nhúng từ tương ứng EN={xn}N(n=1), trong đó xn ∈
Rd là vector nhúng từ d chiều của
token wn mà không có thông tin vị trí. Đầu tiên, các
truy vấn qm được thu được:
qm=WQxm,∀m∈[1, N] (4)
Tiếp theo, chúng tôi suy ra các khóa kn với ràng buộc
thẳng hàng. Điều này bắt đầu với việc giới thiệu
hệ số ràng buộc tn cho mỗi vị trí token n,
như được mô tả trong Phương trình (5).

tn=WTxn,∀n∈[1, N] (5)
Tiếp theo, Phương trình (6) áp đặt điều kiện thẳng hàng
trên các hệ số t2j và t2j+1, trong đó
tn = [t0, t1, ..., td−1]T, đảm bảo rằng mỗi cặp
giống hệt nhau. Bước này hiệu quả nhân đôi mỗi
đoạn 2 chiều của tensor.
t2j=t2j+1,∀j∈[0, d/2−1]
tn=Relu(tn)(6)
Sau đó, các khóa được tính toán như được hiển thị
trong Phương trình (7), trong đó kn được biểu diễn bằng
phép nhân element-wise của Q = (q1, ...,qN)
và tn. Điều này dẫn đến việc mở rộng chiều,
vì kn ∈ RN×d bây giờ bao gồm một
chiều độ dài chuỗi bổ sung. Chúng tôi giải quyết
áp lực bộ nhớ tiềm năng bằng cách tối ưu hóa các
co rút tensor, đảm bảo không có sự gia tăng ròng trong
tiêu thụ bộ nhớ. Để phân tích chi tiết, vui lòng tham khảo
Phụ lục C.
kn=Q⊙tn= (q1◦tn, ...,qN◦tn) (7)
Sau đó, chúng tôi áp dụng RoPE trên Q và K, với
hàm f được chi tiết trong Phương trình (1).
f(qm) =f(qm, m)
f(kn) =f(Q⊙tn, n) =f(Q, n)⊙tn(8)
Cuối cùng, điểm chú ý của CoCA sẽ là:
a(m, n) =Re(⟨f(qm, m), f(qm, n)◦tn⟩) (9)
Phương trình (9) minh họa chiều bổ sung
của các khóa trong cơ chế CoCA của chúng tôi. Cụ thể,
nó ánh xạ chỉ số của mỗi truy vấn đến chiều
bổ sung, thiết lập mối quan hệ thẳng hàng giữa
khóa thứ n và truy vấn thứ m. Đây là một
khía cạnh quan trọng của phương pháp chúng tôi.

2.4 Nới lỏng Ràng buộc trên Truy vấn
Trong Phần 2.3, chúng tôi trình bày một giải pháp
chính xác về mặt lý thuyết cho CoCA. Tuy nhiên, triển khai
thực tế đối mặt với thách thức do độ phức tạp của

--- TRANG 5 ---
O(N2d) khi lưu trữ f(Q, n). Để giải quyết vấn đề này,
chúng tôi cung cấp một triển khai kép với độ phức tạp O(Nd)
trong phần này và chứng minh sự tương đương của chúng.

Định lý 1. (Triển khai kép của CoCA) Đối với
bất kỳ điểm chú ý nào được định nghĩa trong Phương trình (9), tồn tại
một dạng tương đương như sau:
a(m, n) =Re(⟨f(qm, m),qm◦f(tn, n)⟩) (10)
với ràng buộc:
q2j=q2j+1,∀j∈[0, d/2−1] (11)

Chứng minh: Chứng minh gồm hai bước.
Bước 1. Chúng tôi chứng minh rằng, bằng cách áp đặt
ràng buộc q2j=q2j+1,∀j∈[0, d/2−1],
Re(⟨f(qm, m),qm◦f(tn, n)⟩) tương đương với
Re(⟨f(qm, m), f(qm, n)◦tn⟩).
Để thấy điều này, chúng tôi tính toán sự khác biệt giữa
f(qm, n)◦tn và qm◦f(tn, n):
f(qm, n)◦tn−qm◦f(tn, n)
=
[t0(q0cosnθ0−q1sinnθ0)
t1(q0sinnθ0+q1cosnθ0)
...
td−2(qd−2cosnθd/2−1−qd−1sinnθd/2−1)
td−1(qd−2sinnθd/2−1+qd−1cosnθd/2−1)]

−
[q0(t0cosnθ0−t1sinnθ0)
q1(t0sinnθ0+t1cosnθ0)
...
qd−2(td−2cosnθd/2−1−td−1sinnθd/2−1)
qd−1(td−2sinnθd/2−1+td−1cosnθd/2−1)]

(12)
Nhớ lại rằng t2j=t2j+1,∀j∈[0, d/2−1] (xem
Phương trình (6)), Phương trình (12) tương đương với:
f(qm, n)◦tn−qm◦f(tn, n)
=
[t0(q0−q1) sinnθ0
t1(q0−q1) sinnθ0
...
td−2(qd−2−qd−1) sinnθd/2−1
td−1(qd−2−qd−1) sinnθd/2−1]
(13)
Rõ ràng, nếu chúng ta áp đặt ràng buộc q2j=
q2j+1,∀j∈[0, d/2−1], vector trong Phương trình
(13) trở thành null và chúng ta suy ra rằng:
f(qm, n)◦tn−qm◦f(tn, n) =0 (14)
Do đó, với ràng buộc q2j=
q2j+1,∀j∈[0, d/2−1], chúng ta có:
Re(⟨f(qm, m),qm◦f(tn, n)⟩)
=Re(⟨f(qm, m), f(qm, n)◦tn⟩)(15)

Bước 2. Chúng tôi tiếp tục chứng minh rằng,
q2j=q2j+1,∀j∈[0, d/2−1] thực tế là
một ràng buộc dư thừa khi tính toán
Re(⟨f(qm, m), f(qm, n)◦tn⟩). Để xác minh
điều này, chúng tôi mở rộng tích vô hướng:
Re(⟨f(qm, m), f(qm, n)◦tn⟩)
=∑(d/2−1, j=0)[(q2(2j)t2j+q2(2j+1)t2j+1) cos((m−n)θj)
+ (q2jq2j+1t2j−q2j+1q2jt2j+1) sin((m−n)θj)]
(16)
Nhớ lại một lần nữa t2j=t2j+1,∀j∈[0, d/2−1], chúng ta
có
Re(⟨f(qm, m), f(qm, n)◦tn⟩)
=∑(d/2−1, j=0)t2j[(q2(2j)+q2(2j+1)) cos((m−n)θj)]
=∑(d/2−1, j=0)t2j|qj|2cos((m−n)θj)(17)
Điều này ngụ ý rằng Re(⟨f(qm, m), f(qm, n)◦
tn⟩) chỉ phụ thuộc vào độ lớn của qj=
(q2j, q2j+1) trong không gian con 2-D, chứng minh sự
độc lập của mối quan hệ giữa q2j và
q2j+1. Tham khảo Phụ lục D.3 để có chứng minh
nghiêm ngặt.

Bây giờ chúng tôi kết luận rằng, với ràng buộc
q2j=q2j+1,∀j∈[0, d/2−1],
Re(⟨f(qm, m),qm◦f(tn, n)⟩) tương đương
với Re(⟨f(qm, m), f(qm, n)◦tn⟩) mà không có
ràng buộc trên truy vấn.

Bằng cách loại bỏ ràng buộc q2j=q2j+1, chúng tôi
gọi phiên bản được sửa đổi này là CoCA-Slack. Định nghĩa
toán học được cung cấp trong Phụ lục
D.4.

3 Thiết lập Thí nghiệm
Phần này cung cấp tổng quan về thiết lập
thí nghiệm, bao gồm chi tiết về dữ liệu huấn
luyện được sử dụng và các mô hình cơ sở được sử dụng để
đánh giá hiệu quả của phương pháp được đề xuất.

3.1 Dữ liệu Huấn luyện
Mô hình của chúng tôi trải qua huấn luyện trên sự kết hợp của
các bộ dữ liệu, bao gồm bộ dữ liệu huấn luyện Pile (Gao
et al., 2020), BookCorpus (Zhu et al., 2015), và
Kho Ngữ liệu Wikipedia (Foundation, 2021). Ngoài ra,
chúng tôi tích hợp mã nguồn mở được thu thập thủ công
từ các kho lưu trữ GitHub với ít nhất
1 sao. Từ các bộ dữ liệu này, chúng tôi thu được một mẫu
khoảng 50B token, duy trì thành phần
75% văn bản và 25% mã.

--- TRANG 6 ---
3.2 Biến thể Mô hình
Để đánh giá hiệu quả của phương pháp được đề xuất
của chúng tôi, chúng tôi huấn luyện 3 mô hình từ đầu dưới
các thiết lập thí nghiệm giống hệt nhau, bao gồm ALibi (Press
et al., 2022), RoFomer (Su et al., 2024), và Ro-
Former+CoCA. Tất cả các mô hình chia sẻ các đặc điểm
chung, có kích thước 350M, 24 lớp, chiều
ẩn 1024, 16 đầu chú ý, và
độ dài chuỗi tối đa 512. Sự khác biệt chính
giữa chúng nằm ở sự biến đổi trong cơ chế tự chú ý
và nhúng vị trí. Việc triển khai
được tối ưu hóa dựa trên EleutherAI GPT-
NeoX1. Huấn luyện mô hình từ đầu đòi hỏi
tài nguyên tính toán đáng kể. Do đó, chúng tôi
cũng tiến hành các thí nghiệm liên quan đến việc tinh chỉnh
các LLMs hiện có với mô-đun CoCA drop-in. Cho
mục đích này, chúng tôi sử dụng mô hình LLaMA-7B
(Touvron et al., 2023a), được huấn luyện với
độ dài ngữ cảnh 2.048. Ngoài ra, chúng tôi sử dụng
dynamic-NTK cho tất cả các mô hình trên.

Tóm lại, các mô hình so sánh của chúng tôi được phân
loại như sau: ALibi, RoFormer, Ro-
Former+CoCA, RoFormer+dynamic NTK, và Ro-
Former+dynamic NTK & CoCA, tất cả thuộc
danh mục huấn luyện từ đầu. Trong khi đó,
LLaMA-7B, LLaMA-7B+CoCA, LLaMA-
7B+dynamic NTK, và LLaMA-7B+dynamic
NTK & CoCA thuộc danh mục tinh chỉnh LLM với
drop-in CoCA.

3.3 Chi tiết Triển khai
Quy trình Tiền huấn luyện Chúng tôi huấn luyện tất cả các mô hình
sử dụng mục tiêu dự đoán token tiếp theo. Chúng tôi sử dụng
AdamW (Loshchilov and Hutter, 2017) với β1
= 0.9 và β2= 0.95. Tốc độ học tập tuân theo
quá trình khởi động tuyến tính của 1% tổng số bước, bắt đầu từ
1e-7. Sau đó, tốc độ học tập được điều chỉnh
đến 1e-4 với sự suy giảm tuyến tính, cuối cùng đạt 1e-5.
Việc huấn luyện sử dụng 8 GPU A100, với kích thước
batch toàn cục 256 và tích lũy 2 bước gradient,
mất khoảng 96 giờ cho 2 epoch.

Quy trình Tinh chỉnh Để tích hợp CoCA trong
LLaMA, chúng tôi sử dụng chiến lược tinh chỉnh ba giai đoạn:
(1) chỉ cập nhật chiếu K (7% tham số).
Giai đoạn này nhằm tái cấu trúc chiếu K trong CoCA.
Bằng cách đóng băng các tham số khác, chúng tôi duy trì
điểm chú ý càng gần càng tốt với những điểm của
tự chú ý vanilla. (2) cập nhật chiếu QKV (21%
tham số). Giai đoạn này nhằm giải quyết quá khớp
nội tại trong tự chú ý vanilla gây ra bởi hành vi
không mong muốn giữa RoPE và ma trận chú ý. (3) tinh chỉnh
tất cả tham số. Mỗi giai đoạn liên quan đến 15K bước,
tổng cộng 7.5B token (22B token tổng thể), sử dụng
mục tiêu dự đoán token tiếp theo. Độ dài huấn luyện
của LLaMA-7B + CoCA vẫn ở 2.048 như trong
mô hình gốc. Tất cả các thí nghiệm được tiến hành với
32 GPU A100, thiết lập kích thước batch trên mỗi thiết bị là 8
mà không tích lũy gradient.

4 Kết quả Thí nghiệm
Chúng tôi tiến hành các thí nghiệm để làm sáng tỏ
những nghi ngờ hợp lý sau:
• Cơ chế chú ý mới CoCA của chúng tôi có thể
cải thiện hiệu suất ngoại suy ngữ cảnh dài
của các mô hình hiện có không?
• Việc kết hợp CoCA với các phương pháp mở rộng khác
cho RoPE có thể hiệu quả giải quyết ba
loại vấn đề ranh giới xoay được thảo luận
trong Phụ lục D.2 không?

4.1 Mô hình Ngôn ngữ Chuỗi Dài
Chúng tôi đánh giá hiệu suất mô hình ngôn ngữ
chuỗi dài của cả mô hình của chúng tôi và các mô hình
cơ sở trên các phần tách kiểm tra của bộ dữ liệu PG-19 (Rae
et al., 2020). Cho đánh giá này, chúng tôi ngẫu nhiên
chọn một mẫu con gồm 100 tài liệu, mỗi
tài liệu chứa ít nhất 32.768 token SentencePiece (Kudo
and Richardson, 2018). Sau đó chúng tôi cắt ngắn
mỗi tài liệu kiểm tra đến 32.768 token đầu tiên của nó. Việc
đánh giá liên quan đến việc tính toán perplexity trên
các kích thước cửa sổ ngữ cảnh khác nhau sử dụng phương pháp
cửa sổ trượt, như được mô tả bởi (Press et al., 2022),
với stride 512. Kết quả perplexity cho cả
mô hình của chúng tôi và cơ sở được trình bày trong Bảng 1
và Hình 1.

Dựa trên các thí nghiệm của chúng tôi, kết quả
đánh giá chỉ ra rằng các mô hình kết hợp với CoCA
thể hiện perplexity được cải thiện đáng kể với độ dài
chuỗi suy luận dài hơn. Đối với các mô hình được tiền huấn luyện,
bằng cách tăng kích thước cửa sổ ngữ cảnh từ
512 (kích thước cửa sổ ngữ cảnh huấn luyện) đến 32k, perplexity
của CoCA chỉ tăng từ 20.11 đến
171.63, trong khi perplexity của RoFormer
trở thành inf. Ngoài ra, bằng cách tăng kích thước cửa sổ ngữ cảnh
từ 2K đến 32K, perplexity của LLaMA-7B+CoCA
được tinh chỉnh chỉ tăng 21.68,
trong khi LLaMA-7B với các phương pháp mở rộng khác
tăng hơn 100. Nhìn chung, chúng tôi quan sát một
xu hướng nhất quán của CoCA đạt được perplexity tốt hơn
với cửa sổ ngữ cảnh dài hơn. Điều này cho thấy

--- TRANG 7 ---
Phương pháp Kích thước Cửa sổ Ngữ cảnh Đánh giá (Perplexity ↓)
512 1024 2048 4096 8192 16k 32k
Huấn luyện mô hình từ đầu
ALibi 18.69 21.27 28.20 35.66 37.03 OOM OOM
RoFomer 19.66 411.50 3276.00 3026.00 3028.00 inf inf
+ dynamic NTK 19.66 22.30 38.00 75.75 138.13 370.75 380.75
+ CoCA 20.11 33.47 69.06 113.19 157.38 141.00 171.63
+ dynamic NTK & CoCA 20.11 20.81 25.88 34.16 55.75 89.31 101.13
Tinh chỉnh LLM với drop-in CoCA
LLaMA-7B 9.25 7.56 7.30 9673.14 inf inf inf
+ dynamic NTK 9.25 7.56 7.30 9.40 14.40 63.62 133.87
+ CoCA 9.91 8.49 8.27 24.23 42.00 23.83 29.95
+ dynamic NTK & CoCA 9.91 8.49 8.27 8.61 9.56 11.10 13.98
Bảng 1: Perplexity đánh giá trên 100 tài liệu PG-19 sử dụng chiến lược cửa sổ trượt (S = 512). Dynamic-NTK được sử dụng mà không tinh chỉnh. Kết quả tốt nhất được đánh dấu in đậm.

rằng CoCA có nhúng vị trí mạnh mẽ hơn,
cho phép nó xử lý ngữ cảnh dài hiệu quả hơn.
Ngược lại, chúng tôi quan sát rằng các mô hình được mở rộng
thông qua việc áp dụng trực tiếp dynamic NTK-
aware Scaled RoPE thể hiện sự gia tăng lớn hơn trong
perplexity ở các chuỗi dài hơn. Perplexity
của cả RoFormer+dynamic NTK và LLaMA-
7B+dynamic NTK vẫn cao hơn đáng kể
so với việc kết hợp CoCA. Sự khác biệt này
trở nên rõ rệt hơn khi độ dài chuỗi
tăng. Khi độ dài chuỗi suy luận
đạt 32k, perplexity của RoFormer+dynamic
NTK tăng đến 380.75, trong khi kết quả cho
RoFormer+CoCA chỉ là 171.63. Tương tự, perplexity
của LLaMA-7B+dynamic NTK đạt
133.87, trong khi LLaMA-7B+CoCA chỉ là 29.95.

Đáng chú ý là mô hình đạt được hiệu suất tốt nhất
khi cả dynamic NTK và CoCA được kết hợp. Đặc biệt,
LLaMA-7B+dynamic NTK & CoCA luôn duy trì
perplexity rất thấp. Ngay cả khi độ dài chuỗi suy luận
đã đạt 32k (16× dài hơn độ dài
huấn luyện), perplexity chỉ là 13.89. Điều này chỉ ra
rằng việc kết hợp CoCA với các phương pháp mở rộng khác
cho RoPE có thể hiệu quả giải quyết ba
loại vấn đề ranh giới xoay, đạt được
khả năng mô hình hóa ngoại suy văn bản dài mạnh mẽ.

4.2 Truy xuất Ngữ cảnh Dài
Perplexity đánh giá hiệu suất của mô hình ngôn ngữ
trong việc dự đoán token tiếp theo. Tuy nhiên, nó
không đủ cho việc đánh giá toàn diện về
kích thước cửa sổ ngữ cảnh hiệu quả. Để giải quyết điều này, chúng tôi
đã tiến hành các thí nghiệm sử dụng nhiệm vụ truy xuất
passkey (Mohtashami and Jaggi, 2023) để đánh giá
phương pháp và cơ sở của chúng tôi. Nhiệm vụ liên quan đến việc xác định
và truy xuất một passkey được ẩn ngẫu nhiên
trong một tài liệu dài. Chi tiết hơn về định nghĩa nhiệm vụ
và thiết lập tạo mẫu kiểm tra có thể
được tìm thấy trong Phụ lục B.1. Bảng 2 minh họa
độ chính xác của tất cả các mô hình được kiểm tra và các biến thể của chúng.

Rõ ràng là ALibi thể hiện thất bại khi
được kiểm tra trên các chuỗi dài 1× hơn
độ dài huấn luyện của nó, do giả thuyết cục bộ của nó.
Ngược lại, mô hình của chúng tôi luôn chứng minh
độ chính xác vượt trội. RoFormer+dynamic NTK &
CoCA duy trì độ chính xác 50%, ngay cả với
độ dài chuỗi kiểm tra được mở rộng đến 16× độ dài
huấn luyện của nó. Tương tự, LLaMA-7B+dynamic NTK &
CoCA vẫn duy trì độ chính xác 30% khi độ dài
kiểm tra lên đến 32K.

4.3 Tác động của Ràng buộc Chặt và Lỏng trên
Q
Như đã đề cập trong Phần 2.4, chúng tôi triển khai một
phiên bản lỏng của CoCA, được gọi là CoCA-Slack.
Trong phần này, dưới cùng thiết lập thí nghiệm,
chúng tôi triển khai hai phiên bản của CoCA dựa trên
RoFormer-350M, được gắn nhãn là CoCA-Slack và
CoCA-Strict. Kết quả so sánh giữa
chúng được hiển thị trong Bảng 3.

Chúng tôi quan sát rằng các mô hình CoCA-Strict và CoCA-
Slack thể hiện hiệu suất tương tự trong
mô hình ngôn ngữ chuỗi dài, như được chứng minh bởi
kết quả perplexity tương đương. Tuy nhiên, trong
nhiệm vụ truy xuất passkey, trái với kỳ vọng ban đầu
của chúng tôi, mô hình CoCA-Strict tạo ra kết quả
thấp hơn đáng kể. Kết quả bất ngờ này cho thấy
rằng các mô hình với ràng buộc lỏng có thể

--- TRANG 8 ---
Phương pháp Kích thước Cửa sổ Ngữ cảnh Đánh giá (Độ chính xác ↑)
512 1024 2048 4096 8192 16k 32k
Huấn luyện mô hình từ đầu
ALibi 0.82 0.65 0.28 0.18 0.12 OOM OOM
RoFomer 0.99 0.53 0.30 0.18 0.04 0.02 0.04
+ dynamic NTK 0.99 1.00 0.95 0.70 0.41 0.16 0.06
+ CoCA 1.00 0.64 0.33 0.19 0.06 0.02 0.04
+ dynamic NTK & CoCA 1.00 1.00 0.96 0.89 0.50 0.23 0.08
Tinh chỉnh LLM với drop-in CoCA
LLaMA-7B 1.00 1.00 1.00 0.61 0.21 0.07 0.09
+ dynamic NTK 1.00 1.00 1.00 0.81 0.26 0.06 0.03
+ CoCA 1.00 1.00 1.00 0.71 0.28 0.11 0.10
+ dynamic NTK & CoCA 1.00 1.00 1.00 1.00 0.85 0.51 0.30
Bảng 2: Hiệu suất truy xuất ngữ cảnh dài trên nhiệm vụ truy xuất passkey. Kết quả tốt nhất được đánh dấu in đậm.

Phương pháp 512 1024 2048 4096 8192 16384 32768
Hiệu suất trên Mô hình Chuỗi Dài (Perplexity)
CoCA-Slack 20.11 19.02 24.92 40.53 68.38 92.75 103.44
ntk-2 CoCA-Strict +0.07 +0.61 -1.58 -4.03 +15.37 +12.38 +1.94
CoCA-Slack 20.11 20.81 25.88 34.16 55.75 89.31 101.13
ntk-4 CoCA-Strict +0.07 -0.49 -0.66 -0.88 +3.16 -18.25 -2.57
CoCA-Slack 20.11 23.66 29.05 37.47 55.5 88.88 111.38
ntk-8 CoCA-Strict +0.07 -1.74 -0.64 +1.16 +0.03 +0.5 +0.31
Hiệu suất trên Truy xuất Ngữ cảnh Dài (Độ chính xác Passkey)
CoCA-Slack 1.0 0.99 0.94 0.77 0.47 0.27 0.15
ntk-2 CoCA-Strict +0.0 -0.12 -0.3 -0.42 -0.34 -0.22 -0.07
CoCA-Slack 1.0 1.0 0.96 0.89 0.5 0.23 0.08
ntk-4 CoCA-Strict +0.0 -0.11 -0.38 -0.46 -0.38 -0.19 -0.02
CoCA-Slack 1.0 0.98 0.99 0.85 0.5 0.11 0.02
ntk-8 CoCA-Strict +0.0 -0.05 -0.34 -0.51 -0.4 -0.07 -0.01
Bảng 3: Kết quả so sánh cho Ràng buộc Chặt và Lỏng của Q trong mô-đun CoCA được đề xuất của chúng tôi. Hiệu suất vượt trội so với CoCA-Slack được chỉ ra bằng màu xanh lá cây, trong khi hiệu suất kém hơn được biểu thị bằng màu đỏ. Perplexity của các mô hình chặt và lỏng có thể so sánh được, trong khi mô hình chặt đạt được độ chính xác thấp hơn trong nhiệm vụ truy xuất passkey.

cung cấp các lợi thế hiệu suất bổ sung, chẳng hạn như
kích thước cửa sổ ngữ cảnh hiệu quả lớn hơn.

Hiểu lý do đằng sau sự vượt trội của
ràng buộc lỏng sẽ là trọng tâm chính của
công việc tương lai của chúng tôi. Về vấn đề này, chúng tôi cung cấp một số
hiểu biết lý thuyết trong Phụ lục D.3 và D.4. Những
hiểu biết này nhằm làm sáng tỏ các cơ chế
cơ bản góp phần vào sự khác biệt được quan sát và đặt
nền tảng cho một phân tích toàn diện hơn trong
nghiên cứu tiếp theo.

5 Kết luận
Trong bài báo này, chúng tôi giới thiệu Chú ý Ràng buộc
Thẳng hàng (CoCA), một phương pháp mới tích hợp
nhúng vị trí với cơ chế tự chú ý.
Sự đổi mới này giải quyết các hành vi không mong muốn
xảy ra xung quanh ranh giới cửa sổ ngữ cảnh,
bắt nguồn từ sự khác biệt giữa RoPE và ma trận chú ý. Theo hiểu biết tốt nhất của chúng tôi,
chúng tôi là những người đầu tiên phân tích các góc ban đầu giữa
truy vấn và khóa trong cơ chế tự chú ý,
điều này làm phát sinh các hiện tượng bất thường trong
RoPE. Hơn nữa, chúng tôi suy ra một ràng buộc lỏng
cho việc triển khai CoCA của chúng tôi. Các thí nghiệm rộng rãi
chứng minh rằng việc kết hợp CoCA vào các mô hình hiện có
đáng kể nâng cao hiệu suất trong cả mô hình ngôn ngữ
chuỗi dài và nhiệm vụ truy xuất ngữ cảnh dài. Ngoài ra,
việc tích hợp đồng thời CoCA với các phương pháp
RoPE mở rộng khác (ví dụ, dynamic-NTK) hiệu quả
giảm thiểu ba loại vấn đề ranh giới xoay, dẫn đến
khả năng ngoại suy ngữ cảnh dài được cải thiện đáng kể.

--- TRANG 9 ---
Hạn chế
Phương pháp hiện tại của chúng tôi, CoCA, cho đến nay đã
trải qua xác thực độc quyền trên RoPE. Kết quả
thí nghiệm chứng minh rằng CoCA nâng cao
hiệu suất ngoại suy ngữ cảnh dài của LLMs
và tiếp tục tăng cường các phương pháp mở rộng khác bằng cách
giải quyết các vấn đề ranh giới xoay. Tuy nhiên,
các câu hỏi phát sinh về khả năng áp dụng của nó cho
các phương pháp tổng quát hơn. Mặc dù hiệu quả của
nhúng vị trí lỏng (SPE) là rõ ràng, việc hiểu sâu hơn
về lý do cơ bản cho hiệu suất vượt trội của nó
cần điều tra thêm.

Tài liệu tham khảo
Daniel G. a. Smith and Johnnie Gray. 2018. opt_einsum
- a python package for optimizing contraction order
for einsum-like expressions. Journal of Open Source
Software, 3(26):753.

Jinze Bai, Shuai Bai, Yunfei Chu, et al. 2023. Qwen
technical report. arXiv preprint arXiv:2309.16609.

Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150.

Sidney Black, Stella Biderman, Eric Hallahan, et al.
2022. GPT-NeoX-20B: An open-source autoregres-
sive language model. In Proceedings of BigScience
Episode #5 – Workshop on Challenges & Perspec-
tives in Creating Large Language Models, pages 95–
136, virtual+Dublin. Association for Computational
Linguistics.

bloc97. 2023. Ntk-aware scaled rope allows llama mod-
els to have extended (8k+) context size without any
fine-tuning and minimal perplexity degradation.

Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
ArXiv, abs/2306.15595.

Ta-Chung Chi, Ting-Han Fan, Peter J. Ramadge, and
Alexander Rudnicky. 2022. KERPLE: kernelized
relative positional embedding for length extrapola-
tion. In Advances in Neural Information Processing
Systems 35: Annual Conference on Neural Informa-
tion Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022.

OpenCompass Contributors. 2023. Opencompass:
A universal evaluation platform for foundation
models. https://github.com/open-compass/
opencompass.

Tri Dao, Daniel Y. Fu, Stefano Ermon, et al. 2022.
FlashAttention: Fast and memory-efficient exact at-
tention with IO-awareness. In Advances in Neural
Information Processing Systems.

Jiayu Ding, Shuming Ma, Li Dong, et al. 2023. Longnet:
Scaling transformers to 1,000,000,000 tokens. arXiv
preprint arXiv:2307.02486.

Emozilla. 2023. Dynamically scaled rope further in-
creases performance of long context llama with zero
fine-tuning.

Wikimedia Foundation. 2021. Wikimedia downloads.

Leo Gao, Stella Rose Biderman, Sid Black, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. ArXiv, abs/2101.00027.

Chi Han, Qifan Wang, Wenhan Xiong, et al. 2023.
Lm-infinite: Simple on-the-fly length generaliza-
tion for large language models. arXiv preprint
arXiv:2308.16137.

Yunpeng Huang, Jingwei Xu, Zixu Jiang, et al. 2023.
Advancing transformer architecture in long-context
large language models: A comprehensive survey.
arXiv preprint arXiv:2311.12351.

Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2018: System Demonstrations, Brussels, Belgium,
October 31 - November 4, 2018, pages 66–71. Asso-
ciation for Computational Linguistics.

Ilya Loshchilov and Frank Hutter. 2017. Fixing
weight decay regularization in adam. ArXiv,
abs/1711.05101.

Amirkeivan Mohtashami and Martin Jaggi. 2023. Land-
mark attention: Random-access infinite context
length for transformers. CoRR, abs/2305.16300.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context win-
dow extension of large language models. CoRR,
abs/2309.00071.

Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022. OpenReview.net.

Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,
Chloe Hillier, and Timothy P. Lillicrap. 2020. Com-
pressive transformers for long-range sequence mod-
elling. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net.

Sebastian Ruder, Matthew E. Peters, Swabha
Swayamdipta, and Thomas Wolf. 2019. Trans-
fer learning in natural language processing. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2,

--- TRANG 10 ---
2019, Tutorial Abstracts, pages 15–18. Association
for Computational Linguistics.

Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng
Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing, 568:127063.

Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2023. A length-extrapolatable
transformer. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), ACL 2023, Toronto,
Canada, July 9-14, 2023, pages 14590–14604. Asso-
ciation for Computational Linguistics.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al.
2023a. Llama: Open and efficient foundation lan-
guage models. ArXiv, abs/2302.13971.

Hugo Touvron, Louis Martin, Kevin R. Stone, et al.
2023b. Llama 2: Open foundation and fine-tuned
chat models. ArXiv, abs/2307.09288.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, pages 5998–6008.

Guangxuan Xiao, Yuandong Tian, Beidi Chen, et al.
2023. Efficient streaming language models with at-
tention sinks. arXiv preprint arXiv:2309.17453.

Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Aligning books and movies:
Towards story-like visual explanations by watching
movies and reading books. In 2015 IEEE Interna-
tional Conference on Computer Vision, ICCV 2015,
Santiago, Chile, December 7-13, 2015, pages 19–27.
IEEE Computer Society.

A Công trình Liên quan
Các nghiên cứu hiện có chủ yếu tập trung vào mô-đun
con của kernel chú ý hoặc nhúng vị trí
(Huang et al., 2023). Trong các phần sau, chúng tôi
sẽ giới thiệu riêng biệt các công trình về hai khía
cạnh này: Phần A.1 chủ yếu đề cập đến khía cạnh trước,
trong khi Phần A.2 đi sâu vào khía cạnh sau.

A.1 Cơ chế Chú ý Hiệu quả
Một số công trình nhằm triển khai các cơ chế chú ý
hiệu quả với nhu cầu tính toán giảm,
thậm chí đạt được độ phức tạp tuyến tính. Điều này cho
phép mở rộng ranh giới độ dài ngữ cảnh hiệu quả
của LLMs trong quá trình suy luận bằng cách trực tiếp tăng
Lmax trong giai đoạn tiền huấn luyện (Ding et al.,
2023; Mohtashami and Jaggi, 2023). Các phương pháp
đáng chú ý bao gồm Longformer (Beltagy et al.,
2020), sử dụng chú ý cửa sổ trượt, và các
mô hình như StreamingLLM (Xiao et al., 2023) và
LM-Infinite (Han et al., 2023), sử dụng cơ chế
chú ý toàn cục-cục bộ. Các biến thể này đã
đạt được thành công ở một mức độ nhất định, nhưng vẫn
đối mặt với các vấn đề chúng tôi tiết lộ trong công trình này khi sử dụng
RoPE như phương pháp mã hóa vị trí của chúng.

A.2 Phương pháp Nhúng Vị trí Ngoại suy
Các phương pháp nhúng vị trí ngoại suy nhằm
nâng cao khả năng tổng quát hóa độ dài của
LLMs.

A.2.1 Bias Chú ý
Trong việc tìm kiếm các thay thế cho việc mã hóa rõ ràng
thông tin vị trí, các nhà nghiên cứu đã khám phá
việc tích hợp bias chú ý để nắm bắt các sắc thái
tuần tự và thời gian vốn có trong ngôn ngữ tự nhiên.
Các phương pháp sớm, như T5 (Ruder
et al., 2019), kết hợp bias chú ý có thể học.
Tuy nhiên, các phương pháp này không rõ ràng giải quyết
thách thức ngoại suy độ dài. ALibi (Press
et al., 2022) giới thiệu bias chú ý nhân quả
âm theo cách heuristic. Mở rộng
bias chú ý kiểu ALiBi, KERPLE (Chi et al.,
2022) coi nó như một kernel tam giác tổng hợp
cho tự chú ý và sửa đổi kiểu Xpos (Sun
et al., 2023) bằng cách tích hợp nó với RoPE. Mặc dù
các phương pháp này hiệu quả quản lý để duy trì
mức perplexity thấp, chúng thiếu sót trong việc nắm bắt
các phụ thuộc tầm xa do giới thiệu các giả thuyết cục bộ
cho các token ngữ cảnh.

A.2.2 Mở rộng RoPE
Bên cạnh đó, các chiến lược khác nhau đã được khám phá
để mở rộng RoPE (Su et al., 2024), một
phương pháp mã hóa vị trí được sử dụng phổ biến trong
các LLMs phổ biến. Các phương pháp gần đây liên quan đến việc đơn giản
mở rộng nó để ngoại suy độ dài ngữ cảnh
suy luận với việc tinh chỉnh tối thiểu hoặc không có. Ví
dụ, Nội suy Vị trí (PI) (Chen et al.,
2023) áp dụng mở rộng tuyến tính trên mỗi số
vị trí từ n đến n/k, làm dày đặc không gian
biểu diễn để mở rộng ranh giới độ dài xa nhất gấp
k lần. Các phương pháp khác, như NTK-aware
Scaled RoPE (bloc97, 2023) và Dynamic-NTK
(Emozilla, 2023), kết hợp ngoại suy tần số cao
và nội suy tần số thấp. Các
phương pháp không cần huấn luyện này yêu cầu thay đổi mã
hạn chế

--- TRANG 11 ---
trong quá trình suy luận (Peng et al., 2023). Tuy nhiên,
các phương pháp này chỉ nhằm giảm nhẹ vấn đề
mô hình hóa các góc xoay ở các vị trí
ngoài phân phối (OOD) mà không nhận ra
mối tương quan nội tại giữa ma trận chú ý
và góc xoay. Do đó, các phương pháp này vẫn
gặp phải tỷ lệ mở rộng cửa sổ ngữ cảnh hạn chế.

Các phương pháp trước đây độc lập điều tra tự
chú ý và nhúng vị trí mà không xem xét
mối quan hệ nội tại của chúng, đặc biệt đối với
phương pháp RoPE được sử dụng rộng rãi.

B Thí nghiệm Bổ sung
B.1 Định nghĩa Nhiệm vụ Truy xuất Passkey
Có một thông tin quan trọng được ẩn bên trong
rất nhiều văn bản không liên quan. Tìm nó và
ghi nhớ chúng. Tôi sẽ kiểm tra bạn về
thông tin quan trọng đó.
Cỏ màu xanh lá cây. Bầu trời màu xanh lam. Mặt
trời màu vàng. Chúng ta bắt đầu. Đi và quay lại.
... // Lặp lại x lần.
// Passkey là 5 số được tạo ngẫu nhiên.
Passkey là 12345. Hãy nhớ nó. 12345 là
passkey.
Cỏ màu xanh lá cây. Bầu trời màu xanh lam. Mặt
trời màu vàng. Chúng ta bắt đầu. Đi và quay lại.
... // Lặp lại y lần.
Passkey là gì?

Listing 1: Định dạng prompt cho truy xuất passkey (Mohtashami
and Jaggi, 2023). Passkey được tạo ngẫu nhiên từ
10.000 đến 99.999.

Nhiệm vụ truy xuất passkey, như được đề xuất bởi Mo-
htashami and Jaggi (2023), liên quan đến việc mô hình
khôi phục một passkey được tạo ngẫu nhiên ẩn trong
một tài liệu dài (xem Listing 1 cho định dạng prompt
nhiệm vụ). Cho một mô hình ngôn ngữ, chúng ta có thể xác
định cửa sổ ngữ cảnh hiệu quả bằng cách đánh giá
các giới hạn trên và dưới. Chúng ta giả định một
passkey ngẫu nhiên cách k token từ cuối
đầu vào. Nếu một mô hình liên tục thất bại trong việc khôi phục
passkey trong nhiều lần thử, nó cho thấy kích thước cửa sổ
ngữ cảnh nhỏ hơn k. Ngược lại, việc truy xuất thành công
chỉ ra kích thước cửa sổ ngữ cảnh hiệu quả ít nhất
k token (Chen et al., 2023).

Trong các thí nghiệm của chúng tôi, chúng tôi tạo ra các mẫu kiểm tra
dựa trên mẫu prompt trong Listing 1, với
độ dài từ 512 đến 32k. Có 100 trường hợp kiểm tra cho mỗi độ dài. Cho một mô hình ngôn ngữ,
chúng tôi đưa vào prompt nhiệm vụ passkey, kiểm tra
đầu ra của mô hình cho 64 token mới, và tính toán
độ chính xác.

B.2 Phân tích I: Tính nhất quán của Tối ưu hóa
trong Nhúng Vị trí
Kết quả truy xuất passkey được trình bày trong Phần
4.2. Mô hình của chúng tôi chứng minh độ chính xác truy xuất passkey
vượt trội so với các mô hình cơ sở
dưới các điều kiện khác nhau. Tuy nhiên, chúng tôi vẫn tò mò
về tối ưu hóa của nó, cụ thể là liệu nó xảy ra
trong hay ngoài phạm vi cửa sổ ngữ cảnh
huấn luyện. Để thăm dò điều này sâu hơn, chúng tôi phân
loại dữ liệu thí nghiệm thành hai đoạn:
khoảng cách passkey ngắn hơn và xa hơn độ dài cửa sổ
ngữ cảnh huấn luyện.

Hình 4 (a) minh họa kết quả so sánh
khi passkey được chèn ít hơn 512 token
cách token cuối, trong khi Hình 4 (b) minh họa
điều đó bên ngoài phạm vi này. Khi passkey được
chèn bên ngoài cửa sổ 512, RoFormer+NTK
& CoCA luôn vượt trội hơn Roformer+NTK
trên các độ dài khác nhau của chuỗi suy luận. Sự
vượt trội này tiếp tục khi passkey được chèn
bên trong cửa sổ 512. Đáng chú ý, với sự gia tăng
độ dài của chuỗi suy luận, RoFormer
+ NTK & CoCA chứng minh hiệu suất ngày càng
vượt trội so với RoFormer + NTK.
Các thí nghiệm này cho thấy rằng mô hình của chúng tôi có thể
liên tục tối ưu hóa nhúng vị trí và mở
rộng cửa sổ ngữ cảnh hiệu quả.

B.3 Phân tích II: Tác động của Dynamic-NTK trong
CoCA
Chúng tôi sử dụng phương pháp dynamic NTK (Emozilla,
2023) trong quá trình suy luận, áp dụng nó riêng biệt
cho cả mô hình của chúng tôi và mô hình cơ sở.
Để đánh giá toàn diện độ bền vững của các
mô hình này, chúng tôi tiến hành xác thực kỹ lưỡng bằng cách
thay đổi các yếu tố mở rộng (2, 4, và 8).

Kết quả trong Hình 1 và 5 chứng minh rằng,
với việc tích hợp phương pháp dynamic NTK,
mô hình của chúng tôi đạt được độ chính xác passkey cao hơn và
perplexity thấp hơn. Ngoài ra, khi yếu tố mở rộng
thay đổi giữa 2, 4, và 8, mô hình Ro-
Former vanilla thất bại trong việc duy trì hiệu suất ổn định.
Ngược lại, CoCA luôn vượt trội hơn Ro-
Former ở các tỷ lệ mở rộng khác nhau. Xu hướng nhất quán
này chỉ ra rằng mô hình của chúng tôi mạnh mẽ hơn, hiển thị
sự dao động hiệu suất tối thiểu với những thay đổi
trong yếu tố mở rộng.

--- TRANG 12 ---
(a) Chèn passkey bên trong 512 token cách token cuối
(b) Chèn passkey bên ngoài 512 token cách token cuối
Hình 4: So sánh cửa sổ ngữ cảnh hiệu quả giữa RoFormer + NTK và RoFormer + NTK & CoCA.

Hình 5: Phân phối độ chính xác passkey trên 4 phạm vi khoảng cách.
CoCA vượt trội hơn RoFormer cho tất cả các khoảng cách và yếu tố
mở rộng của NTK.

Hơn nữa, nó cho thấy rằng bằng cách triển khai
các ràng buộc thẳng hàng, chúng ta có thể khéo léo giải quyết
hành vi bất thường trong RoPE, cho phép RoPE
tận dụng tốt hơn các kỹ thuật ngoại suy khác.

B.4 Phân tích III: Tính tương thích của CoCA với
PI
B.4.1 Thiết lập Thí nghiệm
Chúng tôi tiến hành các thí nghiệm sử dụng mô hình
LLaMA-7B được tiền huấn luyện (Touvron et al., 2023a) và
LLaMA-7B + CoCA từ Phần 3.2. Để áp dụng
PI, chúng tôi tuân theo các thiết lập của Chen et al. (2023):
Chúng tôi thiết lập độ dài chuỗi tinh chỉnh thành 32.768.
Tốc độ học tập được điều chỉnh thành 2e−5 mà không có suy giảm
để phù hợp. Tất cả các thiết lập khác được duy trì như
cấu hình LLaMA-7B. Tất cả các thí nghiệm được
tiến hành với 32 GPU A100, thiết lập kích thước batch trên mỗi
thiết bị là 1 mà không tích lũy gradient. Các
thí nghiệm mất 6.000 bước để hoàn thành.

B.4.2 Xác thực Ngữ cảnh Dài
Kết quả tinh chỉnh với PI được trình bày
trong Bảng 4. Về mô hình chuỗi dài,
cả LLaMA-7B+PI và LLaMA-7B+CoCA & PI
đều chứng minh hiệu suất cạnh tranh trên các
độ dài chuỗi từ 512 đến 8192. Tuy nhiên,
ở các độ dài chuỗi dài hơn (16384 và 32768),
LLaMA-7B+CoCA & PI thể hiện lợi thế hiệu suất
nhẹ so với LLaMA-7B+PI. Đối với truy xuất ngữ cảnh dài,
cả hai phương pháp đều đạt được độ chính xác đặc biệt cao,
với điểm số tiếp cận giá trị lý tưởng 1.0 trên tất cả
các độ dài chuỗi.

Nhìn chung, các phát hiện này cho thấy việc tích hợp
PI và mô-đun CoCA với mô hình LLaMA-
7B mang lại hiệu suất mạnh mẽ trong cả nhiệm vụ
mô hình chuỗi dài và truy xuất ngữ cảnh dài.
Ngoài ra, mô-đun CoCA chứng minh khả năng
duy trì mức hiệu suất có thể so sánh
với PI, đặc biệt rõ rệt ở các độ dài chuỗi dài hơn.

B.4.3 Xác thực Ngữ cảnh Ngắn
Ngoài việc nâng cao ngoại suy ngữ cảnh dài, việc xem xét
tính thực tiễn và khả năng mở rộng của CoCA trong
ngữ cảnh ngắn là cần thiết. Do đó, chúng tôi
đánh giá mô hình của mình trên OpenCompass (Contribu-
tors, 2023), bao gồm các chiều khác nhau,
bao gồm lý luận, hiểu biết, ngôn ngữ, và
kiểm tra. Kết quả được trình bày trong Bảng 5.
Bảng chứng minh rằng các mô hình LLaMA-7B
được tích hợp với CoCA đạt được hiệu suất
tương đương với LLaMA-7B cơ sở trên tất cả các
chiều được đánh giá. Cụ thể, việc tích hợp

--- TRANG 13 ---
Phương pháp 512 1024 2048 4096 8192 16384 32768
Hiệu suất trên Mô hình Chuỗi Dài (Perplexity)
LLaMA-7B+PI 9.06 7.55 7.74 7.16 7.04 6.93 7.11
+ CoCA & PI 9.65 8.19 8.37 7.87 7.84 7.83 7.96
Hiệu suất trên Truy xuất Ngữ cảnh Dài (Độ chính xác Passkey)
LLaMA-7B+PI 1.0 1.0 1.0 1.0 1.0 1.0 0.99
+ CoCA & PI 1.0 1.0 1.0 1.0 1.0 0.99 0.99
Bảng 4: Kết quả so sánh cho LLaMA-7B+PI và LLaMA-7B+CoCA & PI sau khi tinh chỉnh với độ dài chuỗi 32.768.
CoCA thành công trong việc duy trì hiệu suất của PI trong kích thước cửa sổ tinh chỉnh.

Phương pháp Lý luận Hiểu biết Ngôn ngữ Kiểm tra Trung bình
LLaMA-7B 48.25 47.57 46.41 29.63 42.97
+ CoCA 45.55 51.14 55.27 25.14 44.28
+ PI 44.98 51.54 54.79 27.03 44.59
+ CoCA & PI 46.88 51.82 55.56 25.31 44.89
Bảng 5: Kết quả OpenCompass của LLaMA-7B và các biến thể
của nó. Các mô hình được tích hợp với CoCA đạt được hiệu suất
tương đương với LLaMA-7B, không gây hại đến khả năng biểu
đạt của mô hình.

CoCA không mang lại sự suy giảm đáng kể trong
khả năng biểu đạt của mô hình. Điều này cho thấy rằng
CoCA hiệu quả không chỉ trong các kịch bản ngữ cảnh dài
mà còn trong các nhiệm vụ ngữ cảnh ngắn, chứng minh tính
linh hoạt và sự phù hợp của nó cho các ứng dụng thực tế.

C Phân tích Độ phức tạp Tính toán và Không gian

Mô-đun vanilla self-attention CoCA
Tính toán Không gian Tính toán Không gian
WQK(T)V 3Nd2h Nd 3Nd2h Nd
T half — — Ndh Nd
T Relu — — Ndh Nd
QK(T) rotation 2Ndh Nd 2Ndh Nd
Krot=Q◦Trot — — N2dh N2d
QrotKT
rot N2dh N2 N2dh N2
Mask N2 N2 N2 N2
Softmax N2 N2 N2 N2
Bảng 6: So sánh độ phức tạp tính toán và không gian giữa khối
tự chú ý vanilla và CoCA. Ở đây, N đại diện cho độ dài chuỗi,
h biểu thị số lượng đầu, và d biểu thị chiều của mỗi đầu.

Trong phần này, chúng tôi phân tích độ phức tạp tính toán
và không gian của CoCA. Bảng 6 cung cấp
so sánh chi tiết giữa cơ chế tự chú ý vanilla
và CoCA.

Khi sử dụng phép toán Krot=Q◦Trot, độ phức tạp
tính toán của CoCA không vượt quá
gấp đôi so với tự chú ý vanilla. Trong thực tế,
tốc độ huấn luyện và suy luận của CoCA
có thể so sánh với cơ chế tự chú ý vanilla,
chỉ với sự gia tăng nhẹ khoảng 5% đến 10%, như
được mô tả trong Hình 6. Tuy nhiên, có
sự gia tăng đáng kể trong độ phức tạp không gian khi
mở rộng Krot=Q◦Trot, trở thành d lần so với
tự chú ý vanilla. Mức độ phức tạp không gian này
không thực tế cho các ứng dụng.

Hình 6: So sánh tốc độ suy luận giữa CoCA và
tự chú ý vanilla.

Để giải quyết vấn đề này, chúng ta có thể lấy cảm hứng
từ việc tính toán QrotKT
rot, liên quan đến
hai bước: nhân element-wise giữa
Qrot và Krot tiếp theo là tổng theo
chiều ẩn. Tối ưu hóa có thể đạt được bằng cách
nén chiều ẩn trước khi mở rộng hoàn toàn
chiều độ dài chuỗi. Do đó, độ phức tạp không gian
được hiệu quả giảm từ N2d thành N2. Chiến lược tối ưu hóa này
cũng áp dụng cho Krot=Q◦Trot. Hai
thành phần này có thể được thống nhất như được phát biểu trong
Phương trình (18):
QrotKT
rot=Qrot(Q◦Trot)T (18)
Công việc đáng khen ngợi được thực hiện bởi
opt_einsum (a. Smith and Gray, 2018) tạo điều kiện
cho việc tối ưu hóa Phương trình (18). Kết quả
thí nghiệm chỉ ra rằng Roformer+CoCA chỉ yêu cầu
khoảng 60GB bộ nhớ GPU trong quá trình suy luận
với độ dài chuỗi 32k, phù hợp chặt chẽ với
việc tiêu thụ bộ nhớ của cơ chế tự chú ý vanilla.

--- TRANG 14 ---
D Chứng minh Lý thuyết
D.1 Dạng Mạnh của Suy giảm Dài hạn với
CoCA
Chúng tôi đã giới thiệu lý thuyết cơ bản của Nhúng Vị trí
Xoay trong Phần 2.1. Thực tế, (Su et al.,
2024) cho thấy RoPE có đặc tính suy giảm
dài hạn:
|a(s)|=Re[∑(d/2−1, j=0)hjeisθj]
≤(max_i|hi+1−hi|)∑(d/2−1, j=0)|Sj+1| (19)
trong đó hj:= (q2j+iq2j+1)(k2j−ik2j+1) và
Sj:=∑(j−1, k=0)eisθk,s= (m−n),m cho chỉ số
của truy vấn, n cho chỉ số của khóa. Vì giá trị của
∑(d/2−1, j=0)|Sj+1| suy giảm với khoảng cách tương đối s,
điểm chú ý cũng suy giảm.

Đặc tính này đảm bảo sự ổn định của RoPE
trong quá trình ngoại suy ở một mức độ nhất định bằng cách ngăn chặn
các giá trị ngoại lai. Đối với CoCA, một suy luận mạnh hơn có thể
được phát biểu như sau:
|a(s)| ≤(max_i|li+1−li|)∑(d/2−1, j=0)|Cj+1| (20)
trong đó lj:=|q2j+iq2j+1||k2j+ik2j+1|, và Cj:=∑(j−1, k=0)cos(sθk).
Hơn nữa, ta có:
|li+1−li| ≤ |hi+1−hi| (21)

Chứng minh: Lưu ý rằng khi góc ban đầu Θj giữa
qj và kj là 0, từ Phương trình (17), điểm
chú ý có thể được đơn giản hóa như:
a(s) =Re[∑(d/2−1, j=0)hjeisθj]
=∑(d/2−1, j=0)ljcos(sθj) (22)
Bằng cách tuân theo nghiên cứu của (Su et al., 2024), chúng ta
có thể dễ dàng suy ra ước lượng trong Phương trình (20).
Đối với Phương trình (21), áp dụng bất đẳng thức tam giác, ta có:
|hi+1−hi| ≥ ||hi+1| − |hi|| (23)
Xem xét định nghĩa của hi= (q2j+
iq2j+1)(k2j−ik2j+1), ta sẽ thấy:
|hi+1−hi| ≥ ||hi+1| − |hi||
=||qi+1k∗
i+1| − |qik∗
i||
=||qi+1ki+1| − |qiki||
=|li+1−li| (24)

Hình 7: Phân tích Ranh giới Xoay. Về qj như trục x, 3
ranh giới riêng biệt tương ứng với kj, −qj, và qj

D.2 Phân tích Ranh giới Xoay
Trong Phần 2.2, chúng tôi đã phân tích các hiện tượng
bất thường của RoPE. Để minh họa các bất thường
xoay, hãy tập trung vào một trường hợp cụ thể (trường hợp (d) của
Phần 2.2). Như được hiển thị trong Hình 7, ba ranh giới
riêng biệt xuất hiện trong quá trình xoay. Bằng cách
áp dụng hệ tọa độ tương đối với qj phục vụ
như trục x, các ranh giới này tương ứng với kj,
−qj, và qj.

Mỗi lần góc tương đối của qj và kj vượt
qua các ranh giới này, tính đơn điệu của tích
vô hướng <qj,kj> của chúng trải qua sự đảo ngược. Do đó,
đối với tự chú ý vanilla, nó học một
hàm đơn điệu từng khúc của <qj,kj>:
<qj,kj>=
{↑(m−n),∀ −(2π−Θj)≤θ(qj,kj)<0
↓(m−n),∀0≤θ(qj,kj)< π
↑(m−n),∀π≤θ(qj,kj)<2π
...
↑(m−n),∀(2k−1)π≤θ(qj,kj)<(2k)π
↓(m−n),∀(2k)π≤θ(qj,kj)<(2k+ 1)π} (25)
trong đó θ(qj,kj) = Θj+ (m−n)θj được định nghĩa trong
Phần 2.2.

Điều này gây ra sự nhầm lẫn cho mô hình trong
quá trình ngoại suy ngữ cảnh trực tiếp. Do đó, các phương pháp
như PI và NTK đã cố gắng giới thiệu các kỹ thuật nội suy
hoặc ngoại suy để loại bỏ các vị trí
ngoài phân phối (OOD).

Ngoại trừ phương trình đầu tiên trong Phương trình (25), hai
ranh giới gây ra bởi −qj, và qj đều đều đặn
với chu kỳ 2π, dễ xử lý khi
áp dụng các phương pháp như PI hoặc NTK. Tuy nhiên, các
ranh giới gây ra bởi kj khó xử lý. Có
d/2∗h∗L (d cho chiều đầu, h cho số
đầu, L cho số lớp) ranh giới khác nhau
trong quá trình ngoại suy ngữ cảnh, phá vỡ chu kỳ 2π.

Hơn nữa, sau khi áp dụng các kỹ thuật nội suy hoặc
ngoại suy, nhiều vị trí hơn sẽ rơi vào
khu vực bất thường này. Nó tăng k lần (k cho
yếu tố nội suy) đối với PI và λ2j/d lần (λ cho
yếu tố mở rộng) đối với NTK.

Từ góc độ này, sự tập trung vị trí
của PI dẫn đến nhiều rắc rối hơn NTK, tức là
thêm nhiều vị trí trong khu vực bất thường trong
quá trình ngoại suy ngữ cảnh. Điều này có thể giải thích
ở một mức độ nào đó tại sao NTK có thể được sử dụng mà không
tinh chỉnh cho tự chú ý vanilla, nhưng PI yêu cầu tinh chỉnh.

Bằng cách áp đặt Θj thành 0, CoCA được đề xuất của chúng tôi,
ràng buộc kj thẳng hàng với qj, hiệu quả giải quyết
thách thức liên quan đến ranh giới được liên kết với
kj.

Từ các thí nghiệm trong Phần 4, với việc tích hợp
CoCA, bây giờ NTK có thể được tận dụng tốt
thông qua việc sử dụng trực tiếp, trong khi PI đạt được cải thiện
cho việc sử dụng trực tiếp nhưng vẫn hạn chế, đòi hỏi
các nghiên cứu thêm.

D.3 Đồng phôi của Không gian Biểu diễn
Định lý 2. (Đồng phôi của không gian biểu diễn) Đối với
bất kỳ điểm chú ý nào được định nghĩa như sau:
a(m, n) =Re(⟨f(qm, m), f(qm, n)◦tn⟩) (26)
trong đó qm là truy vấn, m là số chỉ số của
truy vấn, tn là hệ số thẳng hàng của CoCA, n là
số chỉ số của khóa, f là toán tử xoay.
Ký hiệu không gian biểu diễn của nó đối với
qm như:
F(Q) ={a(m, n)|∀qm∈Q⊂Rd} (27)
trong đó qm=WQxm,xm∈EN,m∈[1, N]
và EN là không gian nhúng từ, WQ là
ma trận chiếu.

Sau đó chúng ta có đồng phôi sau:
F(Q)∼=F(Qhalf) (28)
trong đó Qhalf=Q|q2j=q2j+1,∀j∈[0,d/2−1].

Chứng minh: Chúng tôi chứng minh bằng cách chứng minh ánh xạ
đồng phôi G:
G:F(Q)→F(Qhalf)
F((q0, ..., qd−1)→F((√(q²0+q²1)/2, ...,√(q²d−2+q²d−1)/2)
(29)
Nó gồm ba phần:
Phần I (G là song ánh): nhớ lại Phương trình (17), chúng ta
có:
G(X) =X,∀X∈F(Q) (30)

điều này ngụ ý rằng G là một ánh xạ đồng nhất, tự nhiên
đơn ánh.
Tiếp theo, chúng tôi chứng minh rằng G cũng là toàn ánh: đối với
bất kỳ Y=F((q0, ..., qd−1)|q2j=q2j+1)∈F(Qhalf) nào,
tồn tại Ỹ∈F(Q) sao cho G(Ỹ) =Y. Đặt
Ỹ=F((q0, ..., qd−1)|q2j=q2j+1)∈F(Q) (31)
rõ ràng chúng ta có G(Ỹ) =Y.

Phần II (G liên tục): Đối với bất kỳ X0∈F(Q) nào,
ε >0, tồn tại δ, sao cho nếu |X−X0|< δ,
thì |G(X)− G(X0)|< ε.
Từ Phần I, G là một ánh xạ đồng nhất, đặt δ=ε,
thì tính liên tục của G được thỏa mãn.

Phần III (G−1 liên tục): G là một ánh xạ đồng nhất,
G−1 cũng vậy. Theo Phần II, chúng ta ngay lập tức
suy ra rằng G−1 liên tục.

D.4 Nhúng Vị trí Lỏng
Đặt H là một không gian Hilbert, và {T(n)|n≥0} ⊂
L(H) là một họ toán tử tuyến tính bị chặn trên H.
A là tích vô hướng được định nghĩa trên H.

Nếu nó thỏa mãn tính chất sau, thì chúng ta
gọi {T(n)|n≥0} là một toán tử tuyến tính bị chặn
tương đối trên H:
∃ {S(m)|m∈Z}:H × H → C
(X, Y)→ S(m)(X, Y)
là một họ toán tử nửa song tuyến trên H
sao cho S(p−q)(X, Y) =A(T(p)(X),T(q)(Y))
∀p, q∈[0, N], X, Y ∈ H,(32)

Ngoài ra, nếu nó thỏa mãn tính chất sau,
thì chúng ta gọi {T(n)|n≥0} là một toán tử
tuyến tính bị chặn tương đối lỏng trên H:
∃ {S(m)|m∈Z}:H × H → C
(X, Y)→ S(m)(X, Y)
là một họ toán tử nửa song tuyến trên H
và H′⊂ H,H′≠∅
sao cho S(p−q)(X, Y) =A(T(p)(X),T(q)(Y))
∀p, q∈[0, N], X, Y ∈ H′,(33)

Cụ thể, khi H đại diện cho không gian chiếu của chúng ta
trong tự chú ý, và {T(n)|n≥0} là một nhúng vị trí
trên nó, chẳng hạn như Nhúng Vị trí Xoay
(RoPE), chúng tôi gọi nó là Nhúng Vị trí Lỏng (SPE)
nếu nó thỏa mãn tính chất được mô tả trong Phương trình (33).
