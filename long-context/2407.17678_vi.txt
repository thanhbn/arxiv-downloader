# 2407.17678.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/long-context/2407.17678.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 662112 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
S2-ATTENTION: CHIA Sáºº NGá»® Cáº¢NH NHáº¬N THá»¨C PHáº¦N Cá»¨NG
GIá»®A CÃC Äáº¦U ATTENTION
Xihui Lin1âˆ—, Yunan Zhang1âˆ—, Suyu Ge2âˆ—
Liliang Ren1, Barun Patra1, Vishrav Chaudhary1, Hao Peng2, Xia Song1
1Microsoft,2UIUC
{xihlin,yunanzhang}@microsoft.com
TÃ“M Táº®T
Attention thÆ°a thá»›t, lá»±a chá»n táº­p trung vÃ o má»™t táº­p con cÃ¡c token trong ngá»¯ cáº£nh, Ä‘Ã£ trá»Ÿ thÃ nh má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c thiáº¿t láº­p Ä‘á»ƒ nÃ¢ng cao hiá»‡u quáº£ cá»§a Transformers. Tuy nhiÃªn, viá»‡c giáº£m lÃ½ thuyáº¿t FLOPs hiáº¿m khi Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh tÄƒng tá»‘c Ä‘á»™ wall-clock so vá»›i cÃ¡c Ä‘á»‘i tÃ¡c attention dÃ y Ä‘áº·c, chá»§ yáº¿u do thiáº¿u cÃ¡c tá»‘i Æ°u hÃ³a cáº¥p pháº§n cá»©ng nhÆ° FlashAttention (Dao, 2023). Trong khi Ä‘Ã³, váº«n chÆ°a rÃµ liá»‡u attention thÆ°a thá»›t cÃ³ thá»ƒ duy trÃ¬ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh á»Ÿ quy mÃ´ cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) ngÃ y nay hay khÃ´ng, vÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y. BÃ i bÃ¡o nÃ y trÃ¬nh bÃ y Sparsely-Sharded Attention (S2-ATTENTION), má»™t thÆ° viá»‡n kernel Triton Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cung cáº¥p nhiá»u triá»ƒn khai attention thÆ°a thá»›t cÃ³ thá»ƒ tÃ¹y chá»‰nh cho cáº£ huáº¥n luyá»‡n vÃ  suy luáº­n. S2-ATTENTION cho phÃ©p tÃ¹y chá»‰nh cÃ¡c máº«u attention á»Ÿ má»©c tá»«ng Ä‘áº§u, tá»«ng pháº¡m vi ngá»¯ cáº£nh. Nhá»¯ng hiá»ƒu biáº¿t má»›i tá»« S2-ATTENTION truyá»n cáº£m há»©ng cho má»™t kiáº¿n trÃºc attention thÆ°a thá»›t má»›i Ä‘Ã¡p á»©ng má»™t sá»‘ tiÃªu chÃ­ mÃ  chÃºng tÃ´i tháº¥y quan trá»ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c cáº£ lá»£i Ã­ch hiá»‡u quáº£ thá»±c táº¿ vÃ  Ä‘á»™ chÃ­nh xÃ¡c máº¡nh máº½ trÃªn cÃ¡c nhiá»‡m vá»¥ downstream, Ä‘Æ°á»£c gá»i lÃ  Head-Heterogenous Strided Transformer (HHST). Äá»‘i vá»›i Ä‘á»™ thÆ°a thá»›t cao hÆ¡n, HHST chia sáº» ngá»¯ cáº£nh má»™t cÃ¡ch khÃ´ng Ä‘á»“ng nháº¥t giá»¯a cÃ¡c Ä‘áº§u attention, trong Ä‘Ã³ má»—i Ä‘áº§u táº­p trung vÃ o má»™t táº­p con khÃ¡c nhau cá»§a cÃ¡c token trong khi táº­p thá»ƒ bao phá»§ toÃ n bá»™. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ HHST báº±ng cÃ¡ch pre-training cÃ¡c mÃ´ hÃ¬nh kÃ­ch thÆ°á»›c 1.3B vÃ  7B. Äá»‘i vá»›i tÃ­nh toÃ¡n attention, HHST vá»›i S2-ATTENTION Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c attention wall-clock 8.8Ã— vÃ  15.9Ã—, cÅ©ng nhÆ° giáº£m thá»i gian huáº¥n luyá»‡n 2.8Ã— vÃ  2.5Ã— so vá»›i baseline attention dÃ y Ä‘áº·c Ä‘Æ°á»£c triá»ƒn khai vá»›i FlashAttention-2. HÆ¡n ná»¯a, hiá»‡u suáº¥t nhiá»‡m vá»¥ downstream cá»§a HHST ngang báº±ng vá»›i attention dÃ y Ä‘áº·c, vÃ  Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c truy xuáº¥t hoÃ n háº£o á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh 128K á»Ÿ quy mÃ´ 7B. Khi suy luáº­n, HHST 7B cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c 4.5Ã— so vá»›i cÃ¡c Ä‘á»‘i tÃ¡c dÃ y Ä‘áº·c trong vLLM. S2-ATTENTION Ä‘Æ°á»£c phÃ¡t hÃ nh vá»›i cÃ¡c API dá»… tÃ¹y chá»‰nh Ä‘á»ƒ sá»­ dá»¥ng trá»±c tiáº¿p trong Megatron vÃ  vLLM.

1 GIá»šI THIá»†U
CÃ¡c LLMs dá»±a trÃªn Transformer Ä‘Ã£ má»Ÿ ra nhá»¯ng cÆ¡ há»™i má»›i cho cáº£ nghiÃªn cá»©u vÃ  á»©ng dá»¥ng (OpenAI, 2023; Touvron et al., 2023). Äá»™ phá»©c táº¡p báº­c hai cá»§a chÃºng táº¡o ra chi phÃ­ cáº¥m Ä‘oÃ¡n trong viá»‡c huáº¥n luyá»‡n vÃ  phá»¥c vá»¥ cÃ¡c mÃ´ hÃ¬nh nÃ y. VÃ­ dá»¥, huáº¥n luyá»‡n Llama 2 (Touvron et al., 2023) 70B vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh 4K trÃªn 2T token máº¥t 23 ngÃ y trÃªn 2048 GPU A100 Rucinski (2024). Khi phá»¥c vá»¥, KV cache cá»§a mÃ´ hÃ¬nh tiÃªu thá»¥ 343GB bá»™ nhá»› GPU vá»›i kÃ­ch thÆ°á»›c batch 32 vÃ  Ä‘á»™ dÃ i ngá»¯ cáº£nh 4K. CÃ³ nhu cáº§u cáº¥p thiáº¿t Ä‘á»ƒ huáº¥n luyá»‡n LLMs má»™t cÃ¡ch hiá»‡u quáº£ vÃ  phá»¥c vá»¥ chÃºng má»™t cÃ¡ch tiáº¿t kiá»‡m chi phÃ­.

Nhiá»u cÃ´ng trÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p Ä‘Ã£ quáº£n lÃ½, Ã­t nháº¥t trÃªn lÃ½ thuyáº¿t, cáº£i thiá»‡n hiá»‡u quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh nÃ y thÃ´ng qua cÃ¡c ká»¹ thuáº­t attention thÆ°a thá»›t khÃ¡c nhau (Tay et al., 2023; Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), trong Ä‘Ã³ chá»‰ má»™t táº­p con cÃ¡c token trong ngá»¯ cáº£nh Ä‘Æ°á»£c táº­p trung. Tuy nhiÃªn, viá»‡c tiáº¿t kiá»‡m FLOP lÃ½ thuyáº¿t cá»§a chÃºng so vá»›i attention dÃ y Ä‘áº·c toÃ n ngá»¯ cáº£nh thÆ°á»ng khÃ´ng thá»ƒ mang láº¡i lá»£i Ã­ch hiá»‡u quáº£ thá»±c táº¿. NhÆ° Ä‘Æ°á»£c chá»‰ ra bá»Ÿi cÃ´ng trÃ¬nh tiÃªn phong FlashAttention (Dao et al., 2022; Dao, 2023),

âˆ—TÃ¡c giáº£ chÃ­nh. Xihui Lin, Yunan Zhang, vÃ  Suyu Ge Ä‘Ã³ng gÃ³p nhÆ° nhau. MÃ£ nguá»“n cÃ³ sáºµn táº¡i
https://github.com/linxihui/dkernel
1arXiv:2407.17678v7 [cs.CL] 5 Feb 2025

--- TRANG 2 ---
Thread 
Block 0
Thread 
Block 1
Thread 
Block 2
Thread 
Block 3Head 0
Head 1
Head 2
Head 3
FlashAttentionS2-AttentionBlock 1 â€¦
Head 0
Head 1
Head 2
Head 3Thread 
Block 0
Thread 
Block 1
Thread 
Block 2
Thread 
Block 3Block 2 Block 1 â€¦Block 2HÃ¬nh 1: Minh há»a S2-Attention vá»›i bá»‘n Ä‘áº§u attention trÃªn má»™t GPU giáº£ Ä‘á»‹nh vá»›i 4 thread
blocks. Má»—i Ä‘áº§u attention Ä‘Æ°á»£c phÃ¢n bá»• vá»›i má»™t shard cá»§a ngá»¯ cáº£nh.

8k 16k 32k 64k
Sequence Length8k 16k 32k 64k
Sequence Length1600
800
400
400
200
100
0S2-Attention
FlashAttention-23500
3000
2500
2000
1500
1000
500
0S2-Attention
FlashAttention-2Latency (ms)
Latency (ms)Forward Wall-clock Speedup (70B) Backward Wall-clock Speedup (70B)
37.6X
28.8X
19.6X
11.9X22.7X
16.2X
10.6X 6.4X

(a) Benchmark Ä‘á»™ trá»… Attention so vá»›i FlashAttention-2.

4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense12-20
0.00.20.40.60.81.0
Score

(b) Needle in a haystack hoÃ n háº£o 128k.

HÃ¬nh 2: PhÃ¢n tÃ­ch hiá»‡u quáº£ huáº¥n luyá»‡n vÃ  ngá»¯ cáº£nh dÃ i cá»§a S2-Attention. MÃ´ hÃ¬nh cá»§a chÃºng tÃ´i, Ä‘Æ°á»£c triá»ƒn khai vá»›i kernel cá»§a chÃºng tÃ´i, Ä‘áº¡t Ä‘Æ°á»£c giáº£m Ä‘á»™ trá»… Ä‘Ã¡ng ká»ƒ so vá»›i FlashAttention-2 (a). NÃ³ cÅ©ng Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t truy xuáº¥t hoÃ n háº£o á»Ÿ Ä‘á»™ dÃ i ngá»¯ cáº£nh 128K (b).

overhead chÃ­nh trong attention khÃ´ng phÃ¡t sinh tá»« tÃ­nh toÃ¡n mÃ  tá»« truy cáº­p bá»™ nhá»› GPU, Ä‘áº·c biá»‡t lÃ  truy cáº­p bá»™ nhá»› chia sáº» (SRAM). Attention dÃ y Ä‘áº·c Ä‘Ã£ Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« cÃ¡c triá»ƒn khai cáº¥p CUDA Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘áº·c biá»‡t cho IO bá»™ nhá»› hiá»‡u quáº£, má»™t lá»£i tháº¿ Ä‘Ã¡ng ká»ƒ mÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p attention thÆ°a thá»›t chÆ°a nháº­n Ä‘Æ°á»£c. Viá»‡c váº¯ng máº·t má»™t thÆ° viá»‡n linh hoáº¡t, hiá»‡u quáº£ vÃ  dá»… sá»­ dá»¥ng cho cÃ¡c triá»ƒn khai attention thÆ°a thá»›t Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘Ã£ trá»Ÿ thÃ nh má»™t rÃ o cáº£n lá»›n, lÃ m cháº­m tiáº¿n bá»™ trong cáº£ nghiÃªn cá»©u vÃ  á»©ng dá»¥ng trong viá»‡c cáº£i thiá»‡n hiá»‡u quáº£ huáº¥n luyá»‡n vÃ  phá»¥c vá»¥ LLMs.

ChÃºng tÃ´i nháº±m má»¥c Ä‘Ã­ch thu háº¹p khoáº£ng cÃ¡ch nÃ y vá»›i Sparsely-Sharded Attention (S2-ATTENTION), má»™t thÆ° viá»‡n Triton cung cáº¥p tá»‘i Æ°u hÃ³a kernel cho attention thÆ°a thá»›t. NÃ³ ráº¥t linh hoáº¡t, cho phÃ©p cÃ¡c nhÃ  thá»±c hÃ nh khÃ¡m phÃ¡ cÃ¡c chiáº¿n lÆ°á»£c attention thÆ°a thá»›t khÃ¡c nhau vÃ  tÃ¹y chá»‰nh cÃ¡c máº«u attention khÃ¡c nhau trÃªn cÃ¡c Ä‘áº§u attention vÃ  pháº¡m vi ngá»¯ cáº£nh. XÃ¢y dá»±ng má»™t kernel fused Ä‘a má»¥c Ä‘Ã­ch cho attention thÆ°a thá»›t Ä‘áº·t ra nhá»¯ng thÃ¡ch thá»©c Ä‘Ã¡ng ká»ƒ. Trong attention thÆ°a thá»›t, má»™t pháº§n cá»§a ngá»¯ cáº£nh khÃ´ng Ä‘Æ°á»£c táº­p trung. Káº¿t quáº£ lÃ , viá»‡c chia tiles cÃ¡c tensor Q, K, V, má»™t ká»¹ thuáº­t Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh chia cÃ¡c tensor lá»›n thÃ nh cÃ¡c tensor nhá» hÆ¡n Ä‘á»ƒ song song hÃ³a tá»‘t hÆ¡n vÃ  sá»­ dá»¥ng bá»™ nhá»› chia sáº» (SRAM) (Dao et al., 2022; Dao, 2023), cÃ³ thá»ƒ thÆ°á»ng dáº«n Ä‘áº¿n cÃ¡c thread khÃ´ng hoáº¡t Ä‘á»™ng vÃ  sá»­ dá»¥ng SRAM khÃ´ng hiá»‡u quáº£ khi kÃ­ch thÆ°á»›c tile nhá». S2-ATTENTION giáº£i quyáº¿t Ä‘iá»u nÃ y báº±ng cÃ¡ch theo dÃµi hiá»‡u quáº£ cÃ¡c máº«u sá»­ dá»¥ng KV vÃ  Ä‘á»™ng tÃ­ch há»£p cÃ¡c khá»‘i query vá»›i KVs Ä‘Æ°á»£c chia sáº» vÃ o cÃ¹ng má»™t tile. Äiá»u nÃ y Ä‘áº£m báº£o hiá»‡u quáº£ IO, báº¥t ká»ƒ Ä‘á»™ chi tiáº¿t sparsity, cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ viá»‡c sá»­ dá»¥ng SRAM vÃ  giáº£m viá»‡c táº£i KV dÆ° thá»«a.

Nhá»¯ng hiá»ƒu biáº¿t tá»« viá»‡c phÃ¡t triá»ƒn S2-ATTENTION tiáº¿t lá»™ ráº±ng khÃ´ng pháº£i táº¥t cáº£ cÃ¡c cÆ¡ cháº¿ attention thÆ°a thá»›t Ä‘á»u hiá»‡u quáº£ trong thá»±c táº¿. Nhiá»u attention thÆ°a thá»›t khÃ´ng cáº§n huáº¥n luyá»‡n hiá»‡n cÃ³, bao gá»“m cÃ¡c phÆ°Æ¡ng phÃ¡p loáº¡i bá» KV nhÆ° LongGen (Ge et al., 2024b), H2O (Zhang et al., 2023), vÃ  MInference (Jiang et al., 2024), Ã­t tÆ°Æ¡ng thÃ­ch hÆ¡n vá»›i cÃ¡c cÆ¡ cháº¿ phá»¥c vá»¥ ná»n táº£ng nhÆ° continuous batching (Yu et al., 2022), PagedAttention (Kwon et al., 2023), song song 3D (Shoeybi et al., 2020). VÃ­ dá»¥, trong PagedAttention (Kwon et al., 2023), viá»‡c loáº¡i bá» token tá»« cÃ¡c khá»‘i KV sáº½ chá»‰ tÄƒng phÃ¢n máº£nh ná»™i bá»™, vÃ  mang láº¡i overhead thÃªm trong láº­p lá»‹ch, Ä‘iá»u nÃ y lÃ m tá»•n háº¡i throughput phá»¥c vá»¥. Trong khi Ä‘Ã³, cÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y cho tháº¥y attention thÆ°a thá»›t khÃ´ng cáº§n huáº¥n luyá»‡n sáº½ lÃ m tá»•n háº¡i kháº£ nÄƒng ngá»¯ cáº£nh dÃ i cá»§a mÃ´ hÃ¬nh (Xiao et al., 2024; Ge et al., 2024a; Han et al., 2024). Äiá»u nÃ y Ä‘Ã£ trá»Ÿ thÃ nh lÃ½ do chÃ­nh táº¡i sao chÃºng cÃ³ viá»‡c Ã¡p dá»¥ng háº¡n cháº¿ trong cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ cÃ´ng nghiá»‡p vÃ  suy luáº­n nguá»“n má»Ÿ cho Ä‘áº¿n nay (Kwon et al., 2023; Zheng et al., 2023).

2

--- TRANG 3 ---
Nhá»¯ng hiá»ƒu biáº¿t má»›i nÃ y dáº«n Ä‘áº¿n Head-Heterogenous Strided Transformer (HHST), má»™t phÆ°Æ¡ng phÃ¡p attention thÆ°a thá»›t má»›i theo cÃ¡c nguyÃªn táº¯c thiáº¿t káº¿ chÃ­nh (Â§4.1), mÃ  chÃºng tÃ´i tháº¥y quan trá»ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c lá»£i Ã­ch hiá»‡u quáº£ trong thá»±c táº¿ trong khi duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c máº¡nh máº½ trÃªn cÃ¡c nhiá»‡m vá»¥ downstream:

(1) HHST Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i pháº§n cá»©ng vÃ  há»‡ thá»‘ng pháº§n má»m trong tÃ¢m trÃ­. NÃ³ Ã¡p dá»¥ng má»™t chiáº¿n lÆ°á»£c chia sáº» thÃ¢n thiá»‡n vá»›i pháº§n cá»©ng má»›i trÃªn cÃ¡c Ä‘áº§u attention, trong Ä‘Ã³ má»—i Ä‘áº§u táº­p trung vÃ o má»™t táº­p há»£p token riÃªng biá»‡t theo máº«u strided, trong khi táº­p thá»ƒ bao phá»§ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ (HÃ¬nh 1; Â§4.2).

(2) Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t máº¡nh máº½ trÃªn cÃ¡c nhiá»‡m vá»¥ ngá»¯ cáº£nh dÃ i thÃ¡ch thá»©c, viá»‡c bao gá»“m truy cáº­p trá»±c tiáº¿p Ä‘áº¿n táº¥t cáº£ token lÃ  quan trá»ng, Ã­t nháº¥t táº¡i cÃ¡c lá»›p nháº¥t Ä‘á»‹nh. HHST Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y vá»›i má»™t kiáº¿n trÃºc hybrid káº¿t há»£p attention thÆ°a thá»›t vÃ  dÃ y Ä‘áº·c trÃªn cÃ¡c lá»›p, vÃ  cÃ¢n báº±ng hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t (Â§4.2).

S2-ATTENTION Ã¡p dá»¥ng Ä‘Æ°á»£c trong cáº£ huáº¥n luyá»‡n vÃ  suy luáº­n, giáº£m Ä‘Ã¡ng ká»ƒ rÃ o cáº£n khÃ¡m phÃ¡ cÃ¡c kiáº¿n trÃºc attention thÆ°a thá»›t má»›i, mÃ  chÃºng tÃ´i khÃ¡m phÃ¡ trong Â§4 vÃ  Â§5. ChÃºng tÃ´i pretrain má»™t bá»™ mÃ´ hÃ¬nh á»Ÿ quy mÃ´ 1.3B, 7B vá»›i attention thÆ°a thá»›t khÃ¡c nhau, vÃ  so sÃ¡nh chÃºng vá»›i baseline attention dÃ y Ä‘áº·c. Káº¿t quáº£ cá»§a chÃºng tÃ´i cho tháº¥y HHST-7B cá»§a chÃºng tÃ´i phÃ¹ há»£p vá»›i hiá»‡u suáº¥t cá»§a attention dÃ y Ä‘áº·c trong khi Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c huáº¥n luyá»‡n 2.5Ã— vÃ  tÄƒng tá»‘c suy luáº­n 4.5Ã—. HÆ¡n ná»¯a, chÃºng tÃ´i má»Ÿ rá»™ng cÃ¡c mÃ´ hÃ¬nh 1.3B Ä‘áº¿n Ä‘á»™ dÃ i ngá»¯ cáº£nh 32K, vÃ  cÃ¡c mÃ´ hÃ¬nh 7B Ä‘áº¿n 128K. ChÃºng tÃ´i cho tháº¥y HHST cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c truy xuáº¥t Needle in a Haystack hoÃ n háº£o (Kamradt, 2023). So vá»›i FlashAttention-2 (Dao, 2023), HHST cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c attention 8.8Ã— vÃ  15.9Ã— cho quy mÃ´ 1.3B, 7B, vÃ  giáº£m thá»i gian wall-clock huáº¥n luyá»‡n 2.8Ã—, 2.5Ã—.

S2-ATTENTION tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡c framework LLM thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng bao gá»“m PyTorch, Megatron, HuggingFace, vÃ  vLLM. Vá»›i cÃ¡c API thÃ¢n thiá»‡n vá»›i ngÆ°á»i dÃ¹ng, viá»‡c nháº­p vÃ  tÃ¹y chá»‰nh S2-ATTENTION khÃ´ng máº¥t quÃ¡ vÃ i dÃ²ng mÃ£ nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Phá»¥ lá»¥c B.

2 CÃ”NG TRÃŒNH LIÃŠN QUAN
ChÃºng tÃ´i trÃ¬nh bÃ y phÃ¢n tÃ­ch vÃ  quan sÃ¡t cá»§a chÃºng tÃ´i vá» cÃ¡c ná»— lá»±c attention thÆ°a thá»›t hiá»‡n cÃ³ trong cáº£ huáº¥n luyá»‡n vÃ  suy luáº­n.

<S> Q : CÃ³ 2 llamas vÃ  
3 vicunas . CÃ³ bao nhiÃªu Ä‘á»™ng váº­t
<S> GPT4 lÃ  LLM tá»‘t nháº¥t tá»« OpenAIBlock 1
Block 2
Block 3ÄÆ°á»£c sá»­ dá»¥ng
ÄÃ£ loáº¡i bá»
KhÃ´ng sá»­ dá»¥ng

HÃ¬nh 3: Minh há»a táº¡i sao cÃ¡c phÆ°Æ¡ng phÃ¡p loáº¡i bá» KV cÃ³ thá»ƒ gÃ¢y ra nhiá»u phÃ¢n máº£nh hÆ¡n. á» Ä‘Ã¢y chÃºng tÃ´i hiá»ƒn thá»‹ 3 trang cá»§a cÃ¡c khá»‘i KV chá»©a 2 yÃªu cáº§u. Máº·c dÃ¹ nhiá»u token Ä‘Ã£ bá»‹ loáº¡i bá», cÃ¡c slot Ä‘Æ°á»£c giáº£i phÃ³ng khÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c yÃªu cáº§u khÃ¡c, dáº«n Ä‘áº¿n tá»· lá»‡ phÃ¢n máº£nh ná»™i bá»™ cao hÆ¡n.

2.1 Váº®NG Máº¶T KERNEL ATTENTION THÆ¯A THá»šT HIá»†U QUáº¢
ÄÃ£ cÃ³ nhá»¯ng ná»— lá»±c Ä‘á»ƒ giáº£m Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a attention báº±ng cÃ¡ch chá»‰ táº­p trung vÃ o má»™t táº­p con cÃ¡c token (Child et al., 2019; Katharopoulos et al., 2020; Kitaev et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020). Tuy nhiÃªn, nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y khÃ´ng thá»ƒ mang láº¡i tÄƒng tá»‘c wall-clock trong huáº¥n luyá»‡n do bá» qua chi phÃ­ truy cáº­p bá»™ nhá»› thá»±c táº¿ (Dao et al., 2022). Dao et al. (2022) chia nhá» tÃ­nh toÃ¡n attention thÃ nh tÃ­nh toÃ¡n block-wise nhá» hÆ¡n Ä‘á»ƒ giáº£m IO giá»¯a SRAM vÃ  bá»™ nhá»› bÄƒng thÃ´ng cao (HBM). Viá»‡c triá»ƒn khai pháº§n cá»©ng cá»§a há» FlashAttention (Dao et al., 2022; Dao, 2023) lÃ m cho chÃºng trá»Ÿ thÃ nh framework tÄƒng tá»‘c attention Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i nháº¥t. Váº«n chÆ°a rÃµ liá»‡u chÃºng ta cÃ³ thá»ƒ triá»ƒn khai cÃ¡c self-attention thÆ°a thá»›t khÃ¡c nhau theo cÃ¡ch nháº­n thá»©c pháº§n cá»©ng nhÆ° váº­y, Ä‘á»ƒ tá»‘c Ä‘á»™ huáº¥n luyá»‡n cÃ³ thá»ƒ Ä‘Æ°á»£c tÄƒng cÆ°á»ng hÆ¡n ná»¯a so vá»›i FlashAttention.

2.2 Váº¤N Äá»€ Vá»šI CÃC PHÆ¯Æ NG PHÃP LOáº I Bá» KV PLUG-IN-AND-PLAY
Gáº§n Ä‘Ã¢y, cÃ¡c cÃ´ng trÃ¬nh loáº¡i bá» KV plug-in-and-play phÃ¡t triá»ƒn máº¡nh. Cá»¥ thá»ƒ hÆ¡n, nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘á»™ng loáº¡i bá» cÃ¡c vector KV táº¡i suy luáº­n Ä‘á»ƒ giáº£m dáº¥u chÃ¢n bá»™ nhá»› dá»±a trÃªn cÃ¡c tiÃªu chÃ­ nháº¥t Ä‘á»‹nh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ báº£o tá»“n kháº£ nÄƒng mÃ´ hÃ¬nh.

3

--- TRANG 4 ---
ğ‘šğ‘3ğ¾ğ‘‰1ğ¾ğ‘‰2ğ¾ğ‘‰3
táº£i ğ¾ğ‘‰1 ğŸ– láº§n cho táº¥t cáº£ ğ’’
táº£i ğ¾ğ‘‰3 ğŸ” láº§n cho táº¥t cáº£ ğ’’ğ‘1
ğ‘2
ğ‘3ğ¾ğ‘‰1ğ¾ğ‘‰2ğ¾ğ‘‰3
táº£i ğ¾ğ‘‰1 ğŸ’ láº§n cho táº¥t cáº£ ğ’’
táº£i ğ¾ğ‘‰3 ğŸ‘ láº§n cho táº¥t cáº£ ğ’’VÃ²ng láº·p trong
Merged_QK
SRAM
dotV
outğ‘˜3 vÃ  ğ‘£3 Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng 
bá»Ÿi ğ‘6 sau ğ‘5 MergeQ
MergeQ
MergeQ
MergeQFlashAttention -2 S2-Attention
ğ‘1+2
ğ‘3+4
ğ‘5+6
ğ‘7+8ğ‘5+6ğ‘˜3
ğ‘£3 ğ‘4
ğ‘5
ğ‘6
ğ‘7
ğ‘8ğ‘1
ğ‘2
ğ‘3
ğ‘4
ğ‘5
ğ‘6
ğ‘7
ğ‘8ğ‘˜3
ğ‘£3HÃ¬nh 4: Minh há»a triá»ƒn khai S2-Attention. TrÃ¡i: Ãp dá»¥ng trá»±c tiáº¿p tiling FlashAttention-2 cho attention thÆ°a thá»›t. Pháº£i: MergeQ, tÃ­ch há»£p thÃ­ch á»©ng cÃ¡c query chia sáº» cÃ¹ng KV khi táº£i vÃ o SRAM, do Ä‘Ã³ giáº£m viá»‡c táº£i KV dÆ° thá»«a vÃ  cáº£i thiá»‡n hiá»‡u quáº£ IO.

Tuy nhiÃªn, chÃºng tÃ´i quan sÃ¡t cÃ¡c thiáº¿t káº¿ nhÆ° váº­y khÃ³ tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ hiá»‡n cÃ³, dá»±a trÃªn PagedAttention vÃ  continuous batching Ä‘á»ƒ quáº£n lÃ½ bá»™ nhá»› hiá»‡u quáº£. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3, trong quÃ¡ trÃ¬nh loáº¡i bá» KV, cÃ¡c token tÆ°Æ¡ng á»©ng Ä‘Æ°á»£c giáº£i phÃ³ng khá»i bá»™ nhá»› váº­t lÃ½. Tuy nhiÃªn, vÃ¬ viá»‡c loáº¡i bá» theo token khÃ´ng Ä‘Æ°á»£c Ä‘áº£m báº£o liÃªn tá»¥c, cÃ¡c slot bá»™ nhá»› Ä‘Æ°á»£c giáº£i phÃ³ng khÃ´ng thá»ƒ Ä‘Æ°á»£c phÃ¢n bá»• hiá»‡u quáº£ cho cÃ¡c yÃªu cáº§u khÃ¡c, Ä‘Æ°á»£c biáº¿t Ä‘áº¿n nhÆ° phÃ¢n máº£nh ná»™i bá»™. Trong vÃ­ dá»¥ nÃ y, phÃ¢n máº£nh ná»™i bá»™ tÄƒng 37.5%, Ä‘iá»u nÃ y lÃ m tá»•n háº¡i throughput.

Trong khi Ä‘Ã³, viá»‡c loáº¡i bá» Ä‘á»™ng cÅ©ng giá»›i thiá»‡u overhead trong láº­p lá»‹ch. VÃ­ dá»¥, náº¿u cÃ¡c Ä‘áº§u khÃ¡c nhau cÃ³ chÃ­nh sÃ¡ch/tá»· lá»‡ loáº¡i bá» khÃ¡c nhau, cÃ¡c Ä‘áº§u nhanh hÆ¡n sáº½ pháº£i chá» cÃ¡c Ä‘áº§u cháº­m hÆ¡n, Ä‘Ã¢y lÃ  má»™t tÃ¬nh huá»‘ng cÃ¢n báº±ng táº£i cá»• Ä‘iá»ƒn. Váº¥n Ä‘á» nghiÃªm trá»ng hÆ¡n khi phá»¥c vá»¥ cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n, nÆ¡i cÃ¡c tÃ­nh toÃ¡n Ä‘Æ°á»£c phÃ¢n phá»‘i trÃªn cÃ¡c thiáº¿t bá»‹ vÃ  node vá»›i tensor parallel vÃ  pipeline parallel. Nhá»¯ng nhÆ°á»£c Ä‘iá»ƒm nhÆ° váº­y tiáº¿p tá»¥c ngÄƒn cáº£n cÃ¡c thuáº­t toÃ¡n nÃ y Ä‘Æ°á»£c tÃ­ch há»£p vÃ o cÃ¡c cluster phá»¥c vá»¥ thá»±c táº¿ vá»›i hÃ ng trÄƒm node.

2.3 SUY GIáº¢M HIá»†U SUáº¤T
CÃ¡c nghiÃªn cá»©u hiá»‡n cÃ³ chá»‰ ra ráº±ng cáº£ phÆ°Æ¡ng phÃ¡p attention thÆ°a thá»›t huáº¥n luyá»‡n vÃ  khÃ´ng cáº§n huáº¥n luyá»‡n Ä‘á»u cÃ³ suy giáº£m hiá»‡u suáº¥t so vá»›i cÃ¡c Ä‘á»‘i tÃ¡c dÃ y Ä‘áº·c cá»§a chÃºng, Ä‘áº·c biá»‡t trong cÃ¡c nhiá»‡m vá»¥ ngá»¯ cáº£nh dÃ i. HÆ¡n ná»¯a, chÃºng tÃ´i cÅ©ng quan sÃ¡t ráº±ng má»™t sá»‘ phÆ°Æ¡ng phÃ¡p khÃ´ng cáº§n huáº¥n luyá»‡n (Jiang et al., 2024; Tang et al., 2024) cáº§n cÃ¡c siÃªu tham sá»‘ cá»¥ thá»ƒ benchmark Ä‘á»ƒ duy trÃ¬ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh. Khi Ã¡p dá»¥ng cho cÃ¡c yÃªu cáº§u chÆ°a tháº¥y, cÃ¹ng má»™t phÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ hiá»ƒn thá»‹ hÃ nh vi khÃ´ng thá»ƒ dá»± Ä‘oÃ¡n. Tuy nhiÃªn, trong triá»ƒn khai thá»±c táº¿, cÃ¡c truy váº¥n ngÆ°á»i dÃ¹ng thÆ°á»ng cÃ³ phÃ¢n phá»‘i Ä‘uÃ´i dÃ i. Do Ä‘Ã³, khÃ´ng kháº£ thi Ä‘á»ƒ xÃ¡c Ä‘á»‹nh trÆ°á»›c cÃ¡c siÃªu tham sá»‘ cho cÃ¡c truy váº¥n ngÆ°á»i dÃ¹ng chÆ°a tháº¥y, Ä‘iá»u nÃ y lÃ m cho viá»‡c triá»ƒn khai cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° váº­y cÃ³ rá»§i ro.

ChÃºng tÃ´i tháº£o luáº­n vá» viá»‡c xá»­ lÃ½ nhá»¯ng quan sÃ¡t nÃ y trong cÃ¡c pháº§n dÆ°á»›i Ä‘Ã¢y.

3 S2-ATTENTION: HIá»†U QUáº¢ VÃ€ TÃ™Y CHá»ˆNH
Pháº§n nÃ y trÃ¬nh bÃ y S2-ATTENTION. ChÃºng tÃ´i Ä‘áº§u tiÃªn xem xÃ©t ngáº¯n gá»n cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n vá» bá»™ nhá»› GPU vÃ  há»‡ thá»‘ng phÃ¢n cáº¥p thá»±c thi, sau Ä‘Ã³ giá»›i thiá»‡u ká»¹ thuáº­t Merge-Q cá»§a chÃºng tÃ´i, cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ cá»§a kernel trong khi cho phÃ©p tÃ¹y chá»‰nh attention thÆ°a thá»›t chi tiáº¿t hÆ¡n.

3.1 KIáº¾N THá»¨C CÆ  Báº¢N
CÃ¡c thread GPU cÃ³ quyá»n truy cáº­p vÃ o má»™t há»‡ thá»‘ng phÃ¢n cáº¥p cÃ¡c loáº¡i bá»™ nhá»› khÃ¡c nhau. Bá»™ nhá»› bÄƒng thÃ´ng cao toÃ n cá»¥c (HBM) cháº­m nháº¥t nhÆ°ng lá»›n nháº¥t (khoáº£ng >100Ã— vá» Ä‘á»™ trá»… vÃ  âˆ¼6KÃ— vá» kÃ­ch thÆ°á»›c). Bá»™ nhá»› chia sáº» (SRAM) vá» máº·t váº­t lÃ½ trÃªn chip, do Ä‘Ã³ cÃ³ bÄƒng thÃ´ng lá»›n hÆ¡n vÃ  Ä‘á»™ trá»… tháº¥p hÆ¡n so vá»›i HBM. Tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n cá»§a SRAM vÃ  giáº£m thiá»ƒu IO giá»¯a HBM vÃ  SRAM lÃ  quan trá»ng Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u quáº£ cá»§a attention (Dao et al., 2022).

4

--- TRANG 5 ---
0
1
2
3
4
5
5432100
1
2
3
4
5
543210KV thá»±c táº¿
(a) Sparsity khÃ´ng hiá»‡u quáº£ KV 0
1
2
3
4
5
5432100
1
2
3
4
5
543210
1
2
3
4
5
543210

(b) Sparsity hiá»‡u quáº£ KVHÃ¬nh 5: (a): Attention dilated dá»±a trÃªn vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i nhÆ° má»™t vÃ­ dá»¥ vá» attention thÆ°a thá»›t khÃ´ng hiá»‡u quáº£ KV. VÃ­ dá»¥, bÆ°á»›c 5 táº­p trung vÃ o KV táº¡i vá»‹ trÃ­ 1, 3, 5, trong khi bÆ°á»›c 4 táº­p trung vÃ o 0, 2, 4. Äiá»u nÃ y dáº«n Ä‘áº¿n yÃªu cáº§u KV cache Ä‘áº§y Ä‘á»§. Máº·c dÃ¹ nÃ³ gá»£i Ã½ tiáº¿t kiá»‡m bá»™ nhá»› gáº§n 50% trÃªn lÃ½ thuyáº¿t, thá»±c táº¿ nÃ³ yÃªu cáº§u lÆ°u trá»¯ KV cache Ä‘áº§y Ä‘á»§. (b) Táº¥t cáº£ cÃ¡c máº«u attention nÃ y Ä‘á»u hiá»‡u quáº£ KV, vÃ¬ chÃºng Ä‘Æ°á»£c Ä‘áº©y vÃ o KV-cache khi Ä‘áº§u tiÃªn gáº·p pháº£i khi giáº£i mÃ£, sau Ä‘Ã³ liÃªn tá»¥c Ä‘Æ°á»£c táº­p trung trong vÃ i bÆ°á»›c trÆ°á»›c khi cuá»‘i cÃ¹ng bá»‹ loáº¡i bá» (vÃ­ dá»¥, táº¥t cáº£ token trong hÃ¬nh trÃ¡i, vÃ  token 0 trong hÃ¬nh pháº£i) vÃ  khÃ´ng bao giá» Ä‘Æ°á»£c táº­p trung láº¡i, hoáº·c váº«n Ä‘Æ°á»£c táº­p trung cho táº¥t cáº£ token tÆ°Æ¡ng lai (vÃ­ dá»¥, token 0, 2, 4 trong hÃ¬nh giá»¯a vÃ  token 2, 4 trong hÃ¬nh pháº£i). CÃ¡c mÅ©i tÃªn cho tháº¥y táº¥t cáº£ chÃºng Ä‘á»u chia sáº» máº«u "Ä‘Æ°á»ng tháº³ng Ä‘á»©ng".

CÃ¡c triá»ƒn khai attention Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a kÃ©m cÃ³ thá»ƒ dáº«n Ä‘áº¿n I/O thÆ°á»ng xuyÃªn Ä‘áº¿n HBM vÃ  lÃ m tá»•n háº¡i Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£. CUDA tá»• chá»©c cÃ¡c thread thÃ nh cÃ¡c thread block, Ä‘Æ°á»£c chia nhá» hÆ¡n thÃ nh cÃ¡c warp, nhÃ³m 32 thread. CÃ¡c thread trong má»™t block chia sáº» dá»¯ liá»‡u thÃ´ng qua SRAM. Mong muá»‘n ráº±ng cÃ¡c thread khÃ¡c nhau trong cÃ¹ng má»™t warp Ä‘i theo cÃ¹ng má»™t Ä‘Æ°á»ng thá»±c thi vÃ¬ náº¿u khÃ´ng hiá»‡u quáº£ sáº½ bá»‹ tá»•n háº¡i do warp divergence. BÃªn cáº¡nh Ä‘Ã³, kÃ­ch thÆ°á»›c thread block nÃªn Ä‘á»§ lá»›n Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»­ dá»¥ng tá»‘t vÃ  cÃ¢n báº±ng táº£i. Má»™t tile lÃ  má»™t pháº§n cá»§a cÃ¡c tensor Q, K, V Ä‘Æ°á»£c gÃ¡n cho má»™t thread block Ä‘á»ƒ xá»­ lÃ½. Äá»ƒ rÃµ rÃ ng, chÃºng tÃ´i láº¥y kÃ­ch thÆ°á»›c tile lÃ m kÃ­ch thÆ°á»›c block. FlashAttention cáº£i thiá»‡n hiá»‡u quáº£ báº±ng cÃ¡ch giáº£m thiá»ƒu I/O HBM, chia tiles cÃ¡c tensor Q, K, V thÃ nh cÃ¡c chunk phÃ¹ há»£p vá»›i SRAM Ä‘á»ƒ tÃ­nh toÃ¡n hiá»‡u quáº£ (Dao et al., 2022), má»™t nguyÃªn táº¯c mÃ  cÃ´ng trÃ¬nh nÃ y tuÃ¢n theo.

3.2 S2-ATTENTION
Khá»Ÿi Ä‘á»™ng (HÃ¬nh 4 trÃ¡i) ChÃºng tÃ´i Ä‘áº§u tiÃªn xem xÃ©t má»™t triá»ƒn khai blocksparse Ä‘Æ¡n giáº£n sá»­ dá»¥ng thuáº­t toÃ¡n FlashAttention. Má»™t chuá»—i N token Ä‘Æ°á»£c phÃ¢n Ä‘oáº¡n thÃ nh B = âŒˆN/SâŒ‰ shard, má»—i shard chá»©a S token liÃªn tiáº¿p. ChÃºng tÃ´i sá»­ dá»¥ng Q[i] Ä‘á»ƒ biá»ƒu thá»‹ cÃ¡c vector query cho shard query thá»© i, vÃ  tÆ°Æ¡ng tá»± K[i] cÃ¡c vector key cho shard key thá»© i. Theo Dao et al. (2022), Ä‘á»‘i vá»›i má»—i vector query q, chÃºng tÃ´i láº·p qua cÃ¡c tile K trong SRAM Ä‘á»ƒ tÃ­nh toÃ¡n softmax(qKâŠ¤). KhÃ´ng giá»‘ng nhÆ° attention dÃ y Ä‘áº·c sá»­ dá»¥ng toÃ n bá»™ tensor K, chÃºng tÃ´i chá»‰ xem xÃ©t má»™t táº­p con cÃ¡c key Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh bá»Ÿi má»™t mask attention thÆ°a thá»›t M, cÃ³ thá»ƒ Ä‘Æ°á»£c lÆ°u trá»¯ trong Ä‘á»‹nh dáº¡ng Compressed Sparse Row (CSR) Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›.Â¹

Äá»ƒ hiá»ƒu rÃµ hÆ¡n hiá»‡u quáº£ cá»§a triá»ƒn khai nhÆ° váº­y, chÃºng ta cÃ³ thá»ƒ tÃ­nh sá»‘ láº§n táº£i cáº§n thiáº¿t cho má»—i shard key/value. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4 (trÃ¡i), shard key/value Ä‘áº§u tiÃªn, KV1, Ä‘Æ°á»£c táº­p trung bá»Ÿi táº¥t cáº£ cÃ¡c shard query, q1âˆ’q8. Do Ä‘Ã³, KV1 Ä‘Æ°á»£c táº£i 8 láº§n tá»« HBM Ä‘áº¿n SRAM. Náº¿u chÃºng ta nhÃ¢n Ä‘Ã´i kÃ­ch thÆ°á»›c shard, sá»‘ lÆ°á»£ng shard query táº­p trung vÃ o KV1 sáº½ giáº£m má»™t ná»­a xuá»‘ng 4. Trong trÆ°á»ng há»£p nÃ y, KV1 chá»‰ cáº§n 4 láº§n táº£i hiá»‡u quáº£ hÆ¡n. Tuy nhiÃªn, hiá»‡u quáº£ IO Ä‘i kÃ¨m vá»›i chi phÃ­ cá»§a Ä‘á»™ chi tiáº¿t mask thÆ°a thá»›t cá»§a chÃºng ta, vÃ¬ bÃ¢y giá» chÃºng ta pháº£i mask-hoáº·c-giá»¯ 2S token thay vÃ¬ S. Sau Ä‘Ã³ chÃºng tÃ´i tháº£o luáº­n vá» cÃ¡ch Ä‘áº¡t Ä‘Æ°á»£c cáº£ hiá»‡u quáº£ IO vÃ  Ä‘á»™ chi tiáº¿t mask nhá» vá»›i Merge-Q.

Merge-Q á» má»©c Ä‘á»™ cao, Ã½ tÆ°á»Ÿng cá»‘t lÃµi lÃ  tÃ­ch há»£p cÃ¡c shard query táº­p trung vÃ o cÃ¹ng cÃ¡c khá»‘i KV thÃ nh má»™t tile duy nháº¥t Ä‘á»ƒ chÃºng ta khÃ´ng cáº§n táº£i riÃªng biá»‡t cÃ¹ng cÃ¡c khá»‘i KV. Theo cÃ¡ch nÃ y, ngay cáº£ khi Ä‘á»™ chi tiáº¿t mask trá»Ÿ nÃªn nhá» hÆ¡n, chÃºng ta váº«n cÃ³ thá»ƒ duy trÃ¬ hiá»‡u quáº£ IO. HÃ¬nh 4: pháº£i hiá»ƒn thá»‹ má»™t trÆ°á»ng há»£p Ä‘Æ¡n giáº£n hÆ¡n, nÆ¡i chÃºng tÃ´i tÃ­ch há»£p hai shard query lÃ¡ng giá»ng. So vá»›i baseline FlashAttention-2, triá»ƒn khai nÃ y chá»‰ cáº§n táº£i KV1 4 láº§n thay vÃ¬ 8 láº§n vá»›i cÃ¹ng Ä‘á»™ chi tiáº¿t mask. Merge-Q giÃºp S2-ATTENTION há»— trá»£ kÃ­ch thÆ°á»›c shard nhá» Ä‘áº¿n 16, cho phÃ©p má»™t pháº¡m vi rá»™ng hÆ¡n cÃ¡c máº«u attention thÆ°a thá»›t. Ã tÆ°á»Ÿng tÆ°Æ¡ng tá»± cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ tÃ­ch há»£p cÃ¡c khá»‘i KV Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»‡u quáº£ hÆ¡n ná»¯a. ChÃºng tÃ´i Ä‘á»ƒ láº¡i tháº£o luáº­n triá»ƒn khai chi tiáº¿t hÆ¡n trong mÃ£ Ä‘Æ°á»£c phÃ¡t hÃ nh vÃ  Phá»¥ lá»¥c D.

Â¹https://docs.nvidia.com/nvpl/_static/sparse/storage_format/sparse_matrix.html

5

--- TRANG 6 ---
Vá»›i S2-ATTENTION, cá»™ng Ä‘á»“ng cÃ³ thá»ƒ tÃ¹y chá»‰nh cÃ¡c máº«u attention thÆ°a thá»›t chi tiáº¿t vá»›i tÄƒng tá»‘c wall-clock. Tuy nhiÃªn, váº«n chÆ°a rÃµ loáº¡i attention thÆ°a thá»›t nÃ o cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c mÃ  khÃ´ng lÃ m tá»•n háº¡i cháº¥t lÆ°á»£ng. ChÃºng tÃ´i nháº±m má»¥c Ä‘Ã­ch tráº£ lá»i cÃ¢u há»i nÃ y trong pháº§n sau.

4 S2-ATTENTION: HIá»‚U BIáº¾T, CÃ”NG THá»¨C VÃ€ COOKBOOK SPARSITY
Trong pháº§n nÃ y, chÃºng tÃ´i Ä‘áº§u tiÃªn tháº£o luáº­n loáº¡i máº«u attention thÆ°a thá»›t nÃ o cho phÃ©p triá»ƒn khai kernel hiá»‡u quáº£ trong thá»±c táº¿ (Â§4.1). XÃ¢y dá»±ng trÃªn nhá»¯ng hiá»ƒu biáº¿t nÃ y, chÃºng tÃ´i giá»›i thiá»‡u Head-Heterogenous Strided Transformer (HHST), má»™t kiáº¿n trÃºc attention thÆ°a thá»›t má»›i (Â§4.2).

4.1 SPARSITY HIá»†U QUáº¢ KV
KV cache lÃ  má»™t nÃºt tháº¯t cá»• chai bá»™ nhá»› chÃ­nh cho LMs decoder-only táº¡i thá»i gian suy luáº­n. Nhiá»u attention thÆ°a thá»›t hiá»‡n cÃ³ xÃ¡c Ä‘á»‹nh token nÃ o Ä‘á»ƒ táº­p trung dá»±a trÃªn khoáº£ng cÃ¡ch tÆ°Æ¡ng Ä‘á»‘i (Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020). Tuy nhiÃªn, nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y khÃ´ng hiá»‡u quáº£ bá»™ nhá»› GPU trong quÃ¡ trÃ¬nh giáº£i mÃ£, lÃ m cho viá»‡c chuyá»ƒn Ä‘á»•i tiáº¿t kiá»‡m FLOP cá»§a chÃºng thÃ nh lá»£i Ã­ch hiá»‡u quáº£ thá»±c táº¿ trá»Ÿ nÃªn khÃ³ khÄƒn. HÃ¬nh 5(a) cung cáº¥p má»™t vÃ­ dá»¥ minh há»a. Váº¥n Ä‘á» chÃ­nh lÃ , Ä‘á»‘i vá»›i attention thÆ°a thá»›t nhÆ° váº­y, KV khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c bÆ°á»›c giáº£i mÃ£ trÆ°á»›c Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c yÃªu cáº§u trong cÃ¡c bÆ°á»›c sau, lÃ m cho viá»‡c quáº£n lÃ½ bá»™ nhá»› trá»Ÿ nÃªn thÃ¡ch thá»©c hÆ¡n. Máº·c dÃ¹ tiáº¿t kiá»‡m bá»™ nhá»› gáº§n 50% trÃªn lÃ½ thuyáº¿t, nÃ³ thá»±c sá»± yÃªu cáº§u lÆ°u trá»¯ KV cache Ä‘áº§y Ä‘á»§ trong thá»±c táº¿, dáº«n Ä‘áº¿n khÃ´ng tiáº¿t kiá»‡m bá»™ nhá»›.

NgÆ°á»£c láº¡i, HÃ¬nh 5(b) minh há»a má»™t attention thÆ°a thá»›t cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tiáº¿t kiá»‡m bá»™ nhá»› trong thá»±c táº¿. Äiá»ƒm quan trá»ng lÃ  KV cache Ä‘Æ°á»£c lÆ°u trá»¯ Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng qua má»™t sá»‘ bÆ°á»›c giáº£i mÃ£ nhÆ°ng khÃ´ng cÃ²n cáº§n thiáº¿t trong cÃ¡c bÆ°á»›c tÆ°Æ¡ng lai, vÃ  do Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c loáº¡i bá», giáº£i phÃ³ng bá»™ nhá»› GPU.

Sá»± so sÃ¡nh giá»¯a hai phÆ°Æ¡ng phÃ¡p nÃ y dáº«n Ä‘áº¿n quy táº¯c kinh nghiá»‡m sau vá» thiáº¿t káº¿ attention thÆ°a thá»›t hiá»‡u quáº£ KV. Äá»‘i vá»›i âˆ€jâ‰¥i, lâ‰¥1,
(ki,vi) Ä‘Æ°á»£c táº­p trung bá»Ÿi qj+l
âŸ¹(ki,vi) cÅ©ng pháº£i Ä‘Æ°á»£c táº­p trung bá»Ÿi qj.(1)
Náº¿u khÃ´ng, ki vÃ  vi cáº§n Ä‘Æ°á»£c lÆ°u trá»¯ táº¡i bÆ°á»›c j cho cÃ¡c tháº¿ há»‡ tÆ°Æ¡ng lai, ngay cáº£ khi nÃ³ khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng táº¡i bÆ°á»›c j.
Trá»±c quan, trong ma tráº­n máº«u attention, chÃºng ta sáº½ tháº¥y "Ä‘Æ°á»ng tháº³ng Ä‘á»©ng" liÃªn tá»¥c nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 5(b). Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c máº«u thÆ°a thá»›t nÃªn dá»±a trÃªn vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i thay vÃ¬ vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i, ngoáº¡i trá»« ngá»¯ cáº£nh Ä‘á»‹a phÆ°Æ¡ng liÃªn tiáº¿p (vÃ­ dá»¥, hÃ¬nh trÃ¡i trong HÃ¬nh 5(b)).

4.2 HEAD-HETEROGENOUS STRIDED TRANSFORMER
Pháº§n nÃ y giá»›i thiá»‡u Head-Heterogenous Strided Transformer (HHST), má»™t attention thÆ°a thá»›t hiá»‡u quáº£ má»›i Ä‘Æ°á»£c truyá»n cáº£m há»©ng bá»Ÿi nhá»¯ng hiá»ƒu biáº¿t chÃºng tÃ´i há»c Ä‘Æ°á»£c á»Ÿ trÃªn. Cá»‘t lÃµi cá»§a thiáº¿t káº¿ cá»§a nÃ³ lÃ  hai lá»±a chá»n thiáº¿t káº¿ Ä‘Æ°á»£c giá»›i thiá»‡u dÆ°á»›i Ä‘Ã¢y.

Chia sáº» ngá»¯ cáº£nh khÃ´ng Ä‘á»“ng nháº¥t qua cÃ¡c Ä‘áº§u Attention Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c táº£i cÃ¢n báº±ng qua cÃ¡c Ä‘áº§u attention vÃ  tÄƒng cÆ°á»ng song song hÃ³a, má»—i Ä‘áº§u nÃªn táº­p trung vÃ o sá»‘ lÆ°á»£ng token báº±ng nhau. NgoÃ i ra, HHST Ä‘áº£m báº£o ráº±ng cÃ¡c Ä‘áº§u khÃ¡c nhau táº­p trung vÃ o cÃ¡c shard khÃ¡c nhau cá»§a ngá»¯ cáº£nh trong khi táº­p thá»ƒ bao phá»§ toÃ n bá»™ ngá»¯ cáº£nh. Thiáº¿t káº¿ nÃ y Ä‘áº£m báº£o ráº±ng HHST luÃ´n cÃ³ truy cáº­p trá»±c tiáº¿p Ä‘áº¿n ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ táº¡i má»—i lá»›p, mÃ  khÃ´ng lÃ m tá»•n háº¡i song song hÃ³a. HÃ¬nh 1 cung cáº¥p má»™t sÆ¡ Ä‘á»“ minh há»a.

ChÃ­nh thá»©c hÆ¡n, Ä‘á»‘i vá»›i ngá»¯ cáº£nh vá»›i B shard, chÃºng tÃ´i láº¥y Bl khá»‘i gáº§n Ä‘Ã¢y nháº¥t lÃ m khá»‘i Ä‘á»‹a phÆ°Æ¡ng vÃ  Ä‘áº·t pháº§n cÃ²n láº¡i lÃ m khá»‘i tá»« xa. Äá»‘i vá»›i Ä‘áº§u attention vá»›i chá»‰ sá»‘ h, mask attention khá»‘i BÃ—B Mh cá»§a nÃ³ lÃ :
Mh i,j = (1, iâˆ’j < Bl, Äá»‹a phÆ°Æ¡ng
1, jâˆ’ohâˆˆsZâ‰¥0âˆ§iâˆ’jâˆˆ[Bl, B) Stride
0 náº¿u khÃ´ng

s lÃ  kÃ­ch thÆ°á»›c stride, vÃ  xâˆˆmZâ‰¥0 cÃ³ nghÄ©a x lÃ  0 hoáº·c má»™t bá»™i sá»‘ dÆ°Æ¡ng cá»§a m. TÆ°Æ¡ng tá»± nhÆ° má»™t cá»­a sá»• trÆ°á»£t, cÃ¡c token vÆ°á»£t quÃ¡ B shard khÃ´ng Ä‘Æ°á»£c táº­p trung.

TÃ­nh linh hoáº¡t cá»§a kernel S2-ATTENTION cá»§a chÃºng tÃ´i cho phÃ©p triá»ƒn khai hiá»‡u quáº£ chiáº¿n lÆ°á»£c nÃ y. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, thiáº¿t káº¿ nÃ y cho phÃ©p mÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t ngá»¯ cáº£nh dÃ i máº¡nh máº½ trong khi tá»‘i Ä‘a hÃ³a lá»£i Ã­ch hiá»‡u quáº£.

6

--- TRANG 7 ---
Kiáº¿n trÃºc Hybrid NhÆ° cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã³ cho tháº¥y (Huang et al., 2022; Lieber et al.), má»™t sá»‘ lá»›p attention dÃ y Ä‘áº·c hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i nhá»¯ng lá»›p khÃ¡c, vá»›i trá»ng sá»‘ attention Ä‘Æ°á»£c phÃ¢n phá»‘i gáº§n nhÆ° Ä‘á»“ng Ä‘á»u trÃªn táº¥t cáº£ cÃ¡c vá»‹ trÃ­. Do Ä‘Ã³, viá»‡c giá»¯ láº¡i attention dÃ y Ä‘áº·c trong nhá»¯ng lá»›p nÃ y Ä‘áº·c biá»‡t cÃ³ lá»£i. Äiá»u nÃ y thÃºc Ä‘áº©y chÃºng tÃ´i khÃ¡m phÃ¡ má»™t kiáº¿n trÃºc hybrid káº¿t há»£p attention thÆ°a thá»›t hiá»‡u quáº£ cá»§a chÃºng tÃ´i trong háº§u háº¿t cÃ¡c lá»›p vá»›i attention dÃ y Ä‘áº·c trong nhá»¯ng lá»›p khÃ¡c. ChÃºng tÃ´i phÃ¡t hiá»‡n thá»±c nghiá»‡m ráº±ng chiáº¿n lÆ°á»£c attention thÆ°a thá»›t cá»§a chÃºng tÃ´i ráº¥t hiá»‡u quáº£, chá»‰ yÃªu cáº§u 1/6 sá»‘ lá»›p attention lÃ  dÃ y Ä‘áº·c Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t truy xuáº¥t máº¡nh máº½ vá»›i ngá»¯ cáº£nh dÃ i 128K. KhÃ¡m phÃ¡ thÃªm Ä‘Æ°á»£c trÃ¬nh bÃ y trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i.

Tháº£o luáº­n Äiá»u quan trá»ng cáº§n chá»‰ ra ráº±ng táº¥t cáº£ cÃ¡c chiáº¿n lÆ°á»£c loáº¡i bá» nháº¯m vÃ o suy luáº­n (Zhang et al., 2023; Liu et al., 2023; Ge et al., 2024b) Ä‘á»u hiá»‡u quáº£ KV-cache, vÃ¬ KV bá»‹ loáº¡i bá» sáº½ khÃ´ng bao giá» Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi cÃ¡c truy váº¥n tÆ°Æ¡ng lai. Tuy nhiÃªn, nhá»¯ng chiáº¿n lÆ°á»£c nÃ y giá»›i thiá»‡u cÃ¡c máº«u sparsity phá»¥ thuá»™c máº«u, lÃ m cho viá»‡c xÃ¡c Ä‘á»‹nh thá»i gian loáº¡i bá» trong quÃ¡ trÃ¬nh giáº£i mÃ£ trá»Ÿ nÃªn tá»‘n kÃ©m tÃ­nh toÃ¡n. NgÆ°á»£c láº¡i, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i sá»­ dá»¥ng má»™t máº«u sparsity cá»‘ Ä‘á»‹nh trÃªn táº¥t cáº£ cÃ¡c máº«u, loáº¡i bá» overhead cá»§a viá»‡c quyáº¿t Ä‘á»‹nh token nÃ o Ä‘á»ƒ loáº¡i bá». BÃªn cáº¡nh Ä‘Ã³, cÃ¡c phÆ°Æ¡ng phÃ¡p loáº¡i bá» KV lÃ  post-hoc vÃ  thÆ°á»ng hoáº¡t Ä‘á»™ng kÃ©m hÆ¡n nhiá»u so vá»›i Ä‘á»‘i tÃ¡c dÃ y Ä‘áº·c ban Ä‘áº§u (Ge et al., 2024a). HHST cá»§a chÃºng tÃ´i, nhÆ° chÃºng ta sáº½ sá»›m tháº¥y trong cÃ¡c thÃ­ nghiá»‡m, thÃ­ch á»©ng vá»›i attention thÆ°a thá»›t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n (pre-training hoáº·c post-training) hoáº¡t Ä‘á»™ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i baseline dÃ y Ä‘áº·c trong khi giáº£m overhead huáº¥n luyá»‡n.

5 THÃ NGHIá»†M
Äá»ƒ Ä‘Ã¡nh giÃ¡ HHST, chÃºng tÃ´i Ä‘áº§u tiÃªn nghiÃªn cá»©u cháº¥t lÆ°á»£ng pre-training trong Â§5.1 vÃ  Â§5.2. Sau Ä‘Ã³ chÃºng tÃ´i benchmark hiá»‡u quáº£ kernel vÃ  Ä‘á»™ trá»… phá»¥c vá»¥ end-to-end trong Â§5.4 vÃ  Â§ ??. Cuá»‘i cÃ¹ng, chÃºng tÃ´i tiáº¿n hÃ nh má»™t nghiÃªn cá»©u ablation vá» cÃ¡c lá»±a chá»n thiáº¿t káº¿.

5.1 BENCHMARKING CHáº¤T LÆ¯á»¢NG HUáº¤N LUYá»†N MÃ” HÃŒNH
CÃ i Ä‘áº·t ChÃºng tÃ´i Ä‘áº§u tiÃªn huáº¥n luyá»‡n má»™t loáº¡t mÃ´ hÃ¬nh 1.3B vá»›i kiáº¿n trÃºc Llama 2, vá»›i 24 lá»›p, kÃ­ch thÆ°á»›c áº©n 2048 vá»›i 16 Ä‘áº§u, vá»›i Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a lÃ  8192. ChÃºng tÃ´i sá»­ dá»¥ng FineWeb-Edu-350B nguá»“n má»Ÿ Penedo et al. (2024) lÃ m corpus pre-training. Má»™t tokenizer OpenAI Tiktoken vá»›i kÃ­ch thÆ°á»›c tá»« vá»±ng 100K Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xá»­ lÃ½ vÄƒn báº£n thÃ´. Táº¥t cáº£ cÃ¡c biáº¿n thá»ƒ mÃ´ hÃ¬nh sá»­ dá»¥ng kÃ­ch thÆ°á»›c batch 4M token cho táº¥t cáº£ Ä‘á»™ dÃ i chuá»—i vÃ  huáº¥n luyá»‡n tá»•ng cá»™ng 300 tá»· token. Äá»‘i vá»›i siÃªu tham sá»‘, chÃºng tÃ´i sá»­ dá»¥ng ÂµP Yang et al. (2022) vá»›i hÃ¬nh dáº¡ng cÆ¡ sá»Ÿ 256. Tá»· lá»‡ há»c ÂµP 0.02 Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i suy giáº£m tuyáº¿n tÃ­nh vÃ  0.5% tá»•ng sá»‘ token huáº¥n luyá»‡n cho warmup. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ sau khi huáº¥n luyá»‡n trÃªn tá»•ng sá»‘ 300B token cho má»™t epoch.

Nhiá»‡m vá»¥ Downstream ChÃºng tÃ´i sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh vá»›i attention dÃ y Ä‘áº·c lÃ m baseline cá»§a chÃºng tÃ´i, Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  "Dense". Äá»ƒ nghiÃªn cá»©u cáº¥u trÃºc hybrid cá»§a chÃºng tÃ´i vá»›i chia sáº» khÃ´ng Ä‘á»“ng nháº¥t vÃ  tÃ­nh hoÃ n chá»‰nh há»£p nháº¥t, chÃºng tÃ´i kiá»ƒm soÃ¡t FLOPs Ä‘á»ƒ tÆ°Æ¡ng Ä‘Æ°Æ¡ng xáº¥p xá»‰. Tá»•ng sá»‘ token Ä‘Æ°á»£c táº­p trung khoáº£ng 576 token, hoáº·c 9 shard cá»§a 64 token. ChÃºng tÃ´i sá»­ dá»¥ng Ä‘iá»u nÃ y Ä‘á»ƒ cáº¥u hÃ¬nh sliding window attention (SWA), lÃ m táº­p kiá»ƒm soÃ¡t. ChÃºng tÃ´i thÃªm cÃ¡c thay Ä‘á»•i khÃ¡c nhau vÃ o SWA Ä‘á»ƒ xem chÃºng áº£nh hÆ°á»Ÿng Ä‘áº¿n cháº¥t lÆ°á»£ng huáº¥n luyá»‡n nhÆ° tháº¿ nÃ o. CÃ¡c táº­p xá»­ lÃ½ Ä‘Æ°á»£c nhÃ³m thÃ nh 1) Äá»“ng nháº¥t (CÃ¡c Ä‘áº§u khÃ¡c nhau táº­p trung vÃ o cÃ¹ng cÃ¡c shard); 2) KhÃ´ng Ä‘á»“ng nháº¥t & KhÃ´ng Ä‘áº§y Ä‘á»§ (CÃ¡c Ä‘áº§u khÃ¡c nhau táº­p trung vÃ o cÃ¡c shard khÃ¡c nhau nhÆ°ng khÃ´ng bao phá»§ toÃ n bá»™ ngá»¯ cáº£nh), vÃ  3) KhÃ´ng Ä‘á»“ng nháº¥t & Äáº§y Ä‘á»§ (CÃ¡c Ä‘áº§u khÃ¡c nhau táº­p trung vÃ o cÃ¹ng cÃ¡c shard vÃ  táº­p thá»ƒ bao phá»§ toÃ n bá»™ ngá»¯ cáº£nh).

Tá»« Báº£ng 1, chÃºng ta cÃ³ thá»ƒ quan sÃ¡t cÃ¡c kiáº¿n trÃºc hybrid cho tháº¥y káº¿t quáº£ há»©a háº¹n. NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y tá»« S2-L1V15 + Dense (HHST) trong hÃ ng cuá»‘i, chia sáº» khÃ´ng Ä‘á»“ng nháº¥t vá»›i ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ vÃ  hai lá»›p dÃ y Ä‘áº·c cho káº¿t quáº£ tá»‘t nháº¥t nháº¥t quÃ¡n trÃªn cÃ¡c nhiá»‡m vá»¥, vá»›i khoáº£ng cÃ¡ch nhá» tá»« baseline attention máº·c Ä‘á»‹nh trong khi chá»‰ sá»­ dá»¥ng 18% FLOPs. ÄÃ¡ng chÃº Ã½, trong nhiá»‡m vá»¥ Passkey Retrieval, HHST cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t hÆ¡n nhiá»u so vá»›i mÃ´ hÃ¬nh dÃ y Ä‘áº·c. Quan sÃ¡t nÃ y hoáº¡t Ä‘á»™ng nhÆ° má»™t xÃ¡c thá»±c ban Ä‘áº§u vá» kháº£ nÄƒng hiá»ƒu ngá»¯ cáº£nh cá»§a thiáº¿t káº¿ HHST. ChÃºng tÃ´i sáº½ xÃ¡c thá»±c thÃªm nÃ³ trong pháº§n huáº¥n luyá»‡n liÃªn tá»¥c ngá»¯ cáº£nh dÃ i.

ChÃºng tÃ´i cÅ©ng tháº¥y viá»‡c thÃªm hai lá»›p dÃ y Ä‘áº·c thÆ°á»ng dáº«n Ä‘áº¿n hiá»‡u suáº¥t cao hÆ¡n Ä‘Ã¡ng ká»ƒ. Trong nhÃ³m Äá»“ng nháº¥t, chÃºng ta cÃ³ thá»ƒ quan sÃ¡t viá»‡c thÃªm attention sink cÃ³ thá»ƒ tÄƒng cÆ°á»ng Ä‘Ã¡ng ká»ƒ cháº¥t lÆ°á»£ng huáº¥n luyá»‡n, so vá»›i chá»‰ sá»­ dá»¥ng sliding window (SWA). Trong nhÃ³m KhÃ´ng Ä‘á»“ng nháº¥t & KhÃ´ng Ä‘áº§y Ä‘á»§, kÃ­ch thÆ°á»›c stride dá»c lá»›n hÆ¡n sá»‘ lÆ°á»£ng Ä‘áº§u attention, lÃ m cho ngá»¯ cáº£nh khÃ´ng Ä‘áº§y Ä‘á»§ sau há»£p nháº¥t. Äá»‘i vá»›i nhÃ³m KhÃ´ng Ä‘á»“ng nháº¥t & Äáº§y Ä‘á»§, chÃºng tÃ´i Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c stride vÃ  cá»­a sá»• Ä‘á»‹a phÆ°Æ¡ng Ä‘á»ƒ nÃ³ vá»«a bao phá»§ ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ trong khi cÃ³ cÃ¹ng FLOPs nhÆ° nhá»¯ng nhÃ³m khÃ¡c. Khi so sÃ¡nh

7

--- TRANG 8 ---
Báº£ng 1: ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng Pre-Training. "SWA" Ä‘á» cáº­p Ä‘áº¿n sliding window attention. "L" Ä‘á» cáº­p Ä‘áº¿n sá»‘ khá»‘i Ä‘á»‹a phÆ°Æ¡ng. "V" Ä‘á» cáº­p Ä‘áº¿n kÃ­ch thÆ°á»›c stride dá»c. "+ Sink" Ä‘á» cáº­p Ä‘áº¿n táº­p trung vÃ o attention sink. "+ Dense" Ä‘á» cáº­p Ä‘áº¿n lÃ m cho hai lá»›p attention Ä‘áº§u tiÃªn dÃ y Ä‘áº·c.

MÃ´ hÃ¬nh Passkey WinoGrande PIQA RACE Wikitext103(ppl)
Dense (Giá»›i háº¡n trÃªn) 0.865 0.592 0.733 0.403 15.884
Äá»“ng nháº¥t (18% FLOPs cá»§a Dense)
HHST-L9 (SWA) 0.334 0.547 0.705 0.363 21.997
HHST-L9 + Dense 0.620 0.575 0.714 0.373 20.450
HHST-L9 + Sink 0.560 0.566 0.721 0.380 21.037
HHST-L9 + Sink + Dense 0.771 0.577 0.728 0.388 18.503
HHST-L1V15 0.542 0.541 0.716 0.352 21.035
HHST-L1V15 + Dense 0.741 0.568 0.713 0.349 20.579
KhÃ´ng Ä‘á»“ng nháº¥t & KhÃ´ng Ä‘áº§y Ä‘á»§ (18% FLOPs cá»§a Dense)
HHST-L2V18 0.630 0.565 0.728 0.357 20.502
HHST-L2V18 + Dense 0.823 0.587 0.732 0.379 18.726
HHST-L4V25 0.612 0.542 0.720 0.352 20.875
HHST-L4V25 + Dense 0.795 0.569 0.724 0.386 19.285
KhÃ´ng Ä‘á»“ng nháº¥t & Äáº§y Ä‘á»§ (18% FLOPs cá»§a Dense)
HHST-L1V15 0.782 0.571 0.724 0.361 19.551
HHST-L1V15 + Dense (HHST) 0.941 0.587 0.725 0.397 17.183

nhÃ³m KhÃ´ng Ä‘áº§y Ä‘á»§ vá»›i nhÃ³m Äáº§y Ä‘á»§, chÃºng ta cÃ³ thá»ƒ tháº¥y lá»£i Ã­ch cá»§a viá»‡c lÃ m cho há»£p nháº¥t ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ báº±ng cÃ¡ch giá»›i háº¡n kÃ­ch thÆ°á»›c stride dá»c.

5.2 HUáº¤N LUYá»†N LIÃŠN Tá»¤C NGá»® Cáº¢NH DÃ€I

4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense15-17
0.00.20.40.60.81.0
Score

(a) 2 lá»›p dÃ y Ä‘áº·c (11% FLOPs)

4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense14-18
0.00.20.40.60.81.0
Score

(b) 4 lá»›p dÃ y Ä‘áº·c (17% FLOPs)

4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense12-20
0.00.20.40.60.81.0
Score

(c) 8 lá»›p dÃ y Ä‘áº·c (28% FLOPs)

HÃ¬nh 6: ÄÃ¡nh giÃ¡ 128K Needle In A Haystack. ChÃºng tÃ´i sá»­a Ä‘á»•i sá»‘ lÆ°á»£ng lá»›p dÃ y Ä‘áº·c vÃ  chá»©ng minh tiáº¿t kiá»‡m FLOPs so vá»›i Dense (táº¥t cáº£ lá»›p Ä‘á»u dÃ y Ä‘áº·c).

ChÃºng tÃ´i tiáº¿p tá»¥c kiá»ƒm tra cÃ¡ch thÃ­ch á»©ng attention thÆ°a thá»›t vá»›i ngá»¯ cáº£nh dÃ i hÆ¡n. ChÃºng tÃ´i báº¯t Ä‘áº§u tá»« má»™t mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c pre-train dÃ y Ä‘áº·c hiá»‡n cÃ³ vÃ  má»Ÿ rá»™ng Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»§a nÃ³ báº±ng cÃ¡ch tiáº¿p tá»¥c huáº¥n luyá»‡n nÃ³ trÃªn Ä‘á»™ dÃ i ngá»¯ cáº£nh dÃ i hÆ¡n vá»›i kiáº¿n trÃºc thÆ°a thá»›t HHST. Cá»¥ thá»ƒ, chÃºng tÃ´i chá»n Llama-2-7B vÃ  tiáº¿p tá»¥c huáº¥n luyá»‡n nÃ³ trÃªn Ä‘á»™ dÃ i ngá»¯ cáº£nh 128K. ChÃºng tÃ´i thay Ä‘á»•i RoPE base thÃ nh 5M. Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u Ä‘Æ°á»£c huáº¥n luyá»‡n liÃªn tá»¥c vá»›i 10B token theo cÃ´ng thá»©c trong Fu et al. (2024). ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh trÃªn nhiá»‡m vá»¥ Needle In A Haystack Kamradt (2023).

Äá»ƒ Ä‘iá»u tra cÃ¡ch Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t ngá»¯ cáº£nh dÃ i máº¡nh máº½, chÃºng tÃ´i sá»­a Ä‘á»•i sá»‘ lÆ°á»£ng lá»›p dÃ y Ä‘áº·c trong HHST. ChÃºng tÃ´i Ä‘áº·t sá»‘ lÆ°á»£ng lá»›p dÃ y Ä‘áº·c lÃ  2, 4 vÃ  8, tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i cá»‘ Ä‘á»‹nh sá»‘ lÆ°á»£ng khá»‘i Ä‘á»‹a phÆ°Æ¡ng lÃ  31 vÃ  kÃ­ch thÆ°á»›c stride dá»c lÃ  32. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 6, Ä‘á»‘i vá»›i ngá»¯ cáº£nh 128K, mÃ´ hÃ¬nh cÃ³ thá»ƒ truy xuáº¥t ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§ vá»›i 8 lá»›p dÃ y Ä‘áº·c nhÆ°ng khÃ´ng thá»ƒ lÃ m Ä‘Æ°á»£c vá»›i chá»‰ 2 vÃ  4 lá»›p dÃ y Ä‘áº·c. Káº¿t quáº£ xÃ¡c thá»±c kháº£ nÄƒng ngá»¯ cáº£nh dÃ i cá»§a thiáº¿t káº¿ HHST.

8

--- TRANG 9 ---
5.3 TÄ‚NG Tá»C HUáº¤N LUYá»†N

5.3.1 BENCHMARK HOáº T Äá»˜NG ATTENTION

CÃ i Ä‘áº·t Benchmark ChÃºng tÃ´i Ä‘o runtime attention cá»§a HHST vá»›i kernel S2-ATTENTION cá»§a chÃºng tÃ´i, vÃ  FlashAttention-2 trÃªn GPU A100 80GB cho cÃ¡c cÃ i Ä‘áº·t Ä‘á»™ dÃ i ngá»¯ cáº£nh, sá»‘ Ä‘áº§u, vÃ  chiá»u Ä‘áº§u khÃ¡c nhau.

8192 16384 32768
Sequence Length020406080Step Time (s)Forward Pass Latency (1.3B)
S2-Attention (Fwd)
Flash-Attention2 (Fwd)

8192 16384 32768
Sequence Length050100150200250Step Time (ms)Backward Pass Latency (1.3B)
S2-Attention (Bwd)
Flash-Attention2 (Bwd)

(a) TÄƒng tá»‘c Attention 1.3B.

8192 16384 32768 65536
Sequence Length0100200300400500600Step Time (s)Forward Pass Latency (7B)
S2-Attention (Fwd)
Flash-Attention2 (Fwd)

8192 16384 32768 65536
Sequence Length02505007501000125015001750Step Time (ms)Backward Pass Latency (7B)
S2-Attention (Bwd)
Flash-Attention2 (Bwd)

(b) TÄƒng tá»‘c Attention 7B.

HÃ¬nh 7: TÄƒng tá»‘c Attention vs Äá»™ dÃ i Chuá»—i vÃ  Quy mÃ´ MÃ´ hÃ¬nh.

Trong HÃ¬nh 7 vÃ  HÃ¬nh 2a ChÃºng tÃ´i benchmark tÄƒng tá»‘c Ä‘Æ°á»£c mang láº¡i bá»Ÿi HHST trong kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh 1.3B, 7B, 70B trÃªn cÃ¡c Ä‘á»™ dÃ i chuá»—i khÃ¡c nhau Ä‘á»ƒ thá»ƒ hiá»‡n kháº£ nÄƒng má»Ÿ rá»™ng cá»§a há»‡ thá»‘ng chÃºng tÃ´i. Äá»‘i vá»›i táº¥t cáº£ kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, HHST cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c nhiá»u láº§n so vá»›i FlashAttention-2. Äá»‘i vá»›i mÃ´ hÃ¬nh 70B vá»›i 64 Ä‘áº§u, HHST cÃ³ thá»ƒ tÄƒng tá»‘c end-to-end 25.3Ã—. VÃ­ dá»¥, trong mÃ´ hÃ¬nh 1.3B vá»›i stride dá»c 16, HHST cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c 8.8Ã—. Khi Ä‘á»™ dÃ i chuá»—i tá»‘i Ä‘a tÄƒng dÃ i hÆ¡n, tÄƒng tá»‘c dáº§n dáº§n xáº¥p xá»‰ lá»£i Ã­ch giáº£m FLOPs lÃ½ thuyáº¿t. TÄƒng cÆ°á»ng tá»•ng thá»ƒ bá»‹ cáº£n trá»Ÿ má»™t chÃºt do kernel backward Ã­t Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cá»§a chÃºng tÃ´i, Ä‘á»ƒ láº¡i chá»— cho cáº£i thiá»‡n thÃªm.

5.4 TÄ‚NG Tá»C HUáº¤N LUYá»†N VÃ€ SUY LUáº¬N

8192 16384 32768 65536 131072
Sequence Length050000100000150000200000250000300000350000T okens per SecondT oken Throughput(7B)
Dense
S2-Attention

(a) TÄƒng tá»‘c huáº¥n luyá»‡n end-to-end 7B.

8063 16255 32639 65407 130943 262015050200
150
100FlashAttention in vLLM
S2-Attention in vLLMLatency (s)Inference Benchmark on vLLM
4.5X
2.9X 1.9X
1.2X 1.1X1.5X

(b) TÄƒng tá»‘c suy luáº­n end-to-end 7B trÃªn vLLM.

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ tÄƒng tá»‘c huáº¥n luyá»‡n end to end cá»§a cÃ¡c mÃ´ hÃ¬nh 1.3B vÃ  7B báº±ng cÃ¡ch Ä‘o throughput token cá»§a cáº£ hai mÃ´ hÃ¬nh. Táº¥t cáº£ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 256 A100, vá»›i kÃ­ch thÆ°á»›c batch 8M token vÃ  activation checkpointing. Äá»‘i vá»›i 1.3B, HHST cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c 1.2Ã—, 1.8Ã—, 2.3Ã—, vÃ  2.8Ã— throughput token trÃªn ngá»¯ cáº£nh 8K Ä‘áº¿n 128K so vá»›i FlashAttention-2. Äá»‘i vá»›i mÃ´ hÃ¬nh 7B, HHST cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n throughput token 1.1Ã—, 1.2Ã—, 1.5Ã—, vÃ  2.5Ã—. Äá»ƒ chá»©ng minh cáº£i thiá»‡n hiá»‡u quáº£ suy luáº­n cá»§a HHST, chÃºng tÃ´i Ä‘o Ä‘á»™ trá»… end-to-end trÃªn cÃ¡c cÃ i Ä‘áº·t Ä‘á»™ dÃ i ngá»¯ cáº£nh khÃ¡c nhau. Äá»ƒ lÃ m cho so sÃ¡nh thá»±c táº¿, cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn vLLM (Kwon et al., 2023). ChÃºng tÃ´i chá»n backend FlashAttention-2 trong vLLM lÃ m baseline Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng, vÃ¬ kernel suy luáº­n cá»§a S2-ATTENTION cÅ©ng dá»±a trÃªn vLLM. Cáº£ hai phÆ°Æ¡ng phÃ¡p Ä‘á»u Ä‘Æ°á»£c triá»ƒn khai trÃªn má»™t node duy nháº¥t vá»›i 8 GPU A100 80, vá»›i kÃ­ch thÆ°á»›c tensor parallel báº±ng 4. ChÃºng tÃ´i Ä‘áº·t Ä‘á»™ dÃ i Ä‘áº§u ra lÃ  128 vÃ  thay Ä‘á»•i Ä‘á»™ dÃ i Ä‘áº§u vÃ o giá»¯a 16K Ä‘áº¿n 256K. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 8b, HHST cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c 1.1Ã—, 1.2Ã—, 2.9Ã—, 4.5Ã— trÃªn ngá»¯ cáº£nh 8K, 16K, 128K, 256K.

9

--- TRANG 10 ---
6 Káº¾T LUáº¬N
ChÃºng tÃ´i Ä‘Ã£ trÃ¬nh bÃ y S2-ATTENTION, má»™t thÆ° viá»‡n kernel Triton Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cung cáº¥p nhiá»u triá»ƒn khai attention thÆ°a thá»›t cÃ³ thá»ƒ tÃ¹y chá»‰nh cho cáº£ huáº¥n luyá»‡n vÃ  suy luáº­n. Nhá»¯ng hiá»ƒu biáº¿t tá»« S2-ATTENTION dáº«n Ä‘áº¿n má»™t sá»‘ nguyÃªn táº¯c vá» cÃ¡c lá»±a chá»n thiáº¿t káº¿ cá»§a phÆ°Æ¡ng phÃ¡p attention thÆ°a thá»›t Ä‘á»ƒ lÃ m cho chÃºng hiá»‡u quáº£ trong thá»±c táº¿. ChÃºng truyá»n cáº£m há»©ng cho má»™t kiáº¿n trÃºc attention thÆ°a thá»›t hybrid má»›i Ä‘Ã¡p á»©ng má»™t sá»‘ tiÃªu chÃ­ mÃ  chÃºng tÃ´i tháº¥y quan trá»ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c cáº£ lá»£i Ã­ch hiá»‡u quáº£ thá»±c táº¿ vÃ  Ä‘á»™ chÃ­nh xÃ¡c máº¡nh máº½ trÃªn cÃ¡c nhiá»‡m vá»¥ downstream, Ä‘Æ°á»£c gá»i lÃ  Head-Heterogenous Strided Transformer (HHST). ChÃºng tÃ´i sáº½ má»Ÿ nguá»“n thÆ° viá»‡n kernel cá»§a chÃºng tÃ´i vÃ  lÃ m cho nÃ³ trá»Ÿ thÃ nh má»™t thay tháº¿ plug-in-and-play cho module FlashAttention-2 trong cÃ¡c framework huáº¥n luyá»‡n phá»• biáº¿n nhÆ° Megatron vÃ  Pytorch. ChÃºng tÃ´i cÅ©ng tÃ­ch há»£p S2-ATTENTION vÃ o backend vLLM Ä‘á»ƒ phá»¥c vá»¥ tá»©c thÃ¬. Cáº£ kernel huáº¥n luyá»‡n vÃ  suy luáº­n Ä‘á»u cho phÃ©p ngÆ°á»i dÃ¹ng tá»± do tÃ¹y chá»‰nh máº«u sparsity cá»§a há», táº¡o Ä‘iá»u kiá»‡n cho toÃ n bá»™ cá»™ng Ä‘á»“ng nghiÃªn cá»©u chá»§ Ä‘á» nÃ y trong tÆ°Æ¡ng lai.

TÃ€I LIá»†U THAM KHáº¢O
Iz Beltagy, Matthew E. Peters, vÃ  Arman Cohan. Longformer: The long-document transformer.
CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.

Rewon Child, Scott Gray, Alec Radford, vÃ  Ilya Sutskever. Generating long sequences with sparse
transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509.

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR,
abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. URL https://doi.org/10.
48550/arXiv.2307.08691.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, vÃ  Christopher RÃ©. Flashattention:
Fast and memory-efficient exact attention with io-awareness. Trong Sanmi Koyejo, S. Mo-
hamed, A. Agarwal, Danielle Belgrave, K. Cho, vÃ  A. Oh (eds.), Advances in Neural
Information Processing Systems 35: Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,
2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.

Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, vÃ  Hao
Peng. Data engineering for scaling language models to 128k context. Trong Forty-first International
Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net,
2024. URL https://openreview.net/forum?id=TaAqeo7lUh.

Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, vÃ  Hao Peng. A little goes a long way: Efficient
long context training and inference with partial contexts. arXiv preprint arXiv:2410.01485, 2024a.

Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, vÃ  Jianfeng Gao. Model tells you
what to discard: Adaptive KV cache compression for llms. Trong The Twelfth International Conference
on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net,
2024b. URL https://openreview.net/pdf?id=88nT0j5jAn.

Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, vÃ  Sinong Wang. LM-infinite:
Zero-shot extreme length generalization for large language models. Trong Kevin Duh, Helena Gomez,
vÃ  Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies (Volume 1:
Long Papers), pp. 3991â€“4008, Mexico City, Mexico, June 2024. Association for Computational
Linguistics. doi: 10.18653/v1/2024.naacl-long.222. URL https://aclanthology.org/
2024.naacl-long.222/.

Xin Huang, Ashish Khetan, Rene Bidart, vÃ  Zohar Karnin. Pyramid-bert: Reducing complexity
via successive core-set based token selection. Trong Smaranda Muresan, Preslav Nakov, vÃ  Aline
Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 8798â€“8817.
Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.602. URL
https://doi.org/10.18653/v1/2022.acl-long.602.

10

--- TRANG 11 ---
Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua
Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, vÃ  Lili Qiu. Minference 1.0:
Accelerating pre-filling for long-context llms via dynamic sparse attention. CoRR, abs/2407.02490,
2024. doi: 10.48550/ARXIV.2407.02490. URL https://doi.org/10.48550/arXiv.
2407.02490.

Greg Kamradt. Needle in a haystack-pressure testing llms. Github Repository, pp. 28, 2023.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, vÃ  FranÃ§ois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. Trong Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119
of Proceedings of Machine Learning Research, pp. 5156â€“5165. PMLR, 2020. URL http:
//proceedings.mlr.press/v119/katharopoulos20a.html.

Nikita Kitaev, Lukasz Kaiser, vÃ  Anselm Levskaya. Reformer: The efficient transformer. Trong
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
rkgNKkHtvB.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, vÃ  Ion Stoica. Efficient memory management for large language model
serving with pagedattention. CoRR, abs/2309.06180, 2023. doi: 10.48550/arXiv.2309.06180. URL
https://doi.org/10.48550/arXiv.2309.06180.

Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,
Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-
mamba language model, 2024. URL https://arxiv.org/abs/2403.19887.

Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios
Kyrillidis, vÃ  Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance
hypothesis for LLM KV cache compression at test time. CoRR, abs/2305.17118, 2023. doi:
10.48550/arXiv.2305.17118. URL https://doi.org/10.48550/arXiv.2305.17118.

OpenAI. Gpt-4 technical report, 2023.

Guilherme Penedo, Hynek KydlÃ­cek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin
Raffel, Leandro von Werra, vÃ  Thomas Wolf. The fineweb datasets: Decanting the web for the
finest text data at scale. CoRR, abs/2406.17557, 2024. doi: 10.48550/ARXIV.2406.17557. URL
https://doi.org/10.48550/arXiv.2406.17557.

Szymon Rucinski. Efficient language adaptive pre-training: Extending state-of-the-art large language
models for polish. CoRR, abs/2402.09759, 2024. doi: 10.48550/ARXIV.2402.09759. URL
https://doi.org/10.48550/arXiv.2402.09759.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, vÃ  Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism,
2020.

Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, vÃ  Song Han. Quest:
Query-aware sparsity for efficient long-context llm inference, 2024. URL https://arxiv.
org/abs/2406.10774.

Yi Tay, Mostafa Dehghani, Dara Bahri, vÃ  Donald Metzler. Efficient transformers: A survey. ACM
Comput. Surv., 55(6):109:1â€“109:28, 2023. doi: 10.1145/3530811. URL https://doi.org/
10.1145/3530811.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,

11

--- TRANG 12 ---
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, vÃ  Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023.

Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, vÃ 
Song Han. Duoattention: Efficient long-context LLM inference with retrieval and streaming heads.
CoRR, abs/2410.10819, 2024. doi: 10.48550/ARXIV.2410.10819. URL https://doi.org/
10.48550/arXiv.2410.10819.

Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pa-
chocki, Xiaodong Liu, Weizhu Chen, vÃ  Jianfeng Gao. Tensor programs v: Tuning
large neural networks via zero-shot hyperparameter transfer. Trong NeurIPS 2021, March
2022. URL https://www.microsoft.com/en-us/research/publication/
tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/.

Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, vÃ  Byung-Gon Chun. Orca: A
distributed serving system for transformer-based generative models. Trong Marcos K. Aguilera vÃ 
Hakim Weatherspoon (eds.), 16th USENIX Symposium on Operating Systems Design and Imple-
mentation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, pp. 521â€“538. USENIX Association,
2022. URL https://www.usenix.org/conference/osdi22/presentation/yu.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, vÃ  Amr Ahmed. Big bird:
Transformers for longer sequences. Trong Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, vÃ  Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
Yuandong Tian, Christopher RÃ©, Clark W. Barrett, Zhangyang Wang, vÃ  Beidi Chen. H2o: Heavy-
hitter oracle for efficient generative inference of large language models. CoRR, abs/2306.14048,
2023. doi: 10.48550/arXiv.2306.14048. URL https://doi.org/10.48550/arXiv.
2306.14048.

Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,
Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Sglang: Efficient execution of structured
language model programs, 2024. URL https://arxiv.org/abs/2312.07104, 2023.

A PHá»¤ Lá»¤C
Báº¡n cÃ³ thá»ƒ bao gá»“m cÃ¡c pháº§n bá»• sung khÃ¡c á»Ÿ Ä‘Ã¢y.

12
