# 2407.17678.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2407.17678.pdf
# Kích thước tệp: 662112 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
S2-ATTENTION: CHIA SẺ NGỮ CẢNH NHẬN THỨC PHẦN CỨNG
GIỮA CÁC ĐẦU ATTENTION
Xihui Lin1∗, Yunan Zhang1∗, Suyu Ge2∗
Liliang Ren1, Barun Patra1, Vishrav Chaudhary1, Hao Peng2, Xia Song1
1Microsoft,2UIUC
{xihlin,yunanzhang}@microsoft.com
TÓM TẮT
Attention thưa thớt, lựa chọn tập trung vào một tập con các token trong ngữ cảnh, đã trở thành một phương pháp được thiết lập để nâng cao hiệu quả của Transformers. Tuy nhiên, việc giảm lý thuyết FLOPs hiếm khi được chuyển đổi thành tăng tốc độ wall-clock so với các đối tác attention dày đặc, chủ yếu do thiếu các tối ưu hóa cấp phần cứng như FlashAttention (Dao, 2023). Trong khi đó, vẫn chưa rõ liệu attention thưa thớt có thể duy trì chất lượng mô hình ở quy mô của các mô hình ngôn ngữ lớn (LLMs) ngày nay hay không, và làm thế nào để đạt được điều này. Bài báo này trình bày Sparsely-Sharded Attention (S2-ATTENTION), một thư viện kernel Triton được tối ưu hóa cung cấp nhiều triển khai attention thưa thớt có thể tùy chỉnh cho cả huấn luyện và suy luận. S2-ATTENTION cho phép tùy chỉnh các mẫu attention ở mức từng đầu, từng phạm vi ngữ cảnh. Những hiểu biết mới từ S2-ATTENTION truyền cảm hứng cho một kiến trúc attention thưa thớt mới đáp ứng một số tiêu chí mà chúng tôi thấy quan trọng để đạt được cả lợi ích hiệu quả thực tế và độ chính xác mạnh mẽ trên các nhiệm vụ downstream, được gọi là Head-Heterogenous Strided Transformer (HHST). Đối với độ thưa thớt cao hơn, HHST chia sẻ ngữ cảnh một cách không đồng nhất giữa các đầu attention, trong đó mỗi đầu tập trung vào một tập con khác nhau của các token trong khi tập thể bao phủ toàn bộ. Chúng tôi đánh giá HHST bằng cách pre-training các mô hình kích thước 1.3B và 7B. Đối với tính toán attention, HHST với S2-ATTENTION đạt được tăng tốc attention wall-clock 8.8× và 15.9×, cũng như giảm thời gian huấn luyện 2.8× và 2.5× so với baseline attention dày đặc được triển khai với FlashAttention-2. Hơn nữa, hiệu suất nhiệm vụ downstream của HHST ngang bằng với attention dày đặc, và đạt được độ chính xác truy xuất hoàn hảo ở độ dài ngữ cảnh 128K ở quy mô 7B. Khi suy luận, HHST 7B của chúng tôi đạt được tăng tốc 4.5× so với các đối tác dày đặc trong vLLM. S2-ATTENTION được phát hành với các API dễ tùy chỉnh để sử dụng trực tiếp trong Megatron và vLLM.

1 GIỚI THIỆU
Các LLMs dựa trên Transformer đã mở ra những cơ hội mới cho cả nghiên cứu và ứng dụng (OpenAI, 2023; Touvron et al., 2023). Độ phức tạp bậc hai của chúng tạo ra chi phí cấm đoán trong việc huấn luyện và phục vụ các mô hình này. Ví dụ, huấn luyện Llama 2 (Touvron et al., 2023) 70B với độ dài ngữ cảnh 4K trên 2T token mất 23 ngày trên 2048 GPU A100 Rucinski (2024). Khi phục vụ, KV cache của mô hình tiêu thụ 343GB bộ nhớ GPU với kích thước batch 32 và độ dài ngữ cảnh 4K. Có nhu cầu cấp thiết để huấn luyện LLMs một cách hiệu quả và phục vụ chúng một cách tiết kiệm chi phí.

Nhiều công trình đã được thiết lập đã quản lý, ít nhất trên lý thuyết, cải thiện hiệu quả của các mô hình này thông qua các kỹ thuật attention thưa thớt khác nhau (Tay et al., 2023; Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), trong đó chỉ một tập con các token trong ngữ cảnh được tập trung. Tuy nhiên, việc tiết kiệm FLOP lý thuyết của chúng so với attention dày đặc toàn ngữ cảnh thường không thể mang lại lợi ích hiệu quả thực tế. Như được chỉ ra bởi công trình tiên phong FlashAttention (Dao et al., 2022; Dao, 2023),

∗Tác giả chính. Xihui Lin, Yunan Zhang, và Suyu Ge đóng góp như nhau. Mã nguồn có sẵn tại
https://github.com/linxihui/dkernel
1arXiv:2407.17678v7 [cs.CL] 5 Feb 2025

--- TRANG 2 ---
Thread 
Block 0
Thread 
Block 1
Thread 
Block 2
Thread 
Block 3Head 0
Head 1
Head 2
Head 3
FlashAttentionS2-AttentionBlock 1 …
Head 0
Head 1
Head 2
Head 3Thread 
Block 0
Thread 
Block 1
Thread 
Block 2
Thread 
Block 3Block 2 Block 1 …Block 2Hình 1: Minh họa S2-Attention với bốn đầu attention trên một GPU giả định với 4 thread
blocks. Mỗi đầu attention được phân bổ với một shard của ngữ cảnh.

8k 16k 32k 64k
Sequence Length8k 16k 32k 64k
Sequence Length1600
800
400
400
200
100
0S2-Attention
FlashAttention-23500
3000
2500
2000
1500
1000
500
0S2-Attention
FlashAttention-2Latency (ms)
Latency (ms)Forward Wall-clock Speedup (70B) Backward Wall-clock Speedup (70B)
37.6X
28.8X
19.6X
11.9X22.7X
16.2X
10.6X 6.4X

(a) Benchmark độ trễ Attention so với FlashAttention-2.

4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense12-20
0.00.20.40.60.81.0
Score

(b) Needle in a haystack hoàn hảo 128k.

Hình 2: Phân tích hiệu quả huấn luyện và ngữ cảnh dài của S2-Attention. Mô hình của chúng tôi, được triển khai với kernel của chúng tôi, đạt được giảm độ trễ đáng kể so với FlashAttention-2 (a). Nó cũng đạt được hiệu suất truy xuất hoàn hảo ở độ dài ngữ cảnh 128K (b).

overhead chính trong attention không phát sinh từ tính toán mà từ truy cập bộ nhớ GPU, đặc biệt là truy cập bộ nhớ chia sẻ (SRAM). Attention dày đặc đã được hưởng lợi từ các triển khai cấp CUDA được tối ưu hóa đặc biệt cho IO bộ nhớ hiệu quả, một lợi thế đáng kể mà các phương pháp attention thưa thớt chưa nhận được. Việc vắng mặt một thư viện linh hoạt, hiệu quả và dễ sử dụng cho các triển khai attention thưa thớt được tối ưu hóa đã trở thành một rào cản lớn, làm chậm tiến bộ trong cả nghiên cứu và ứng dụng trong việc cải thiện hiệu quả huấn luyện và phục vụ LLMs.

Chúng tôi nhằm mục đích thu hẹp khoảng cách này với Sparsely-Sharded Attention (S2-ATTENTION), một thư viện Triton cung cấp tối ưu hóa kernel cho attention thưa thớt. Nó rất linh hoạt, cho phép các nhà thực hành khám phá các chiến lược attention thưa thớt khác nhau và tùy chỉnh các mẫu attention khác nhau trên các đầu attention và phạm vi ngữ cảnh. Xây dựng một kernel fused đa mục đích cho attention thưa thớt đặt ra những thách thức đáng kể. Trong attention thưa thớt, một phần của ngữ cảnh không được tập trung. Kết quả là, việc chia tiles các tensor Q, K, V, một kỹ thuật đã được chứng minh chia các tensor lớn thành các tensor nhỏ hơn để song song hóa tốt hơn và sử dụng bộ nhớ chia sẻ (SRAM) (Dao et al., 2022; Dao, 2023), có thể thường dẫn đến các thread không hoạt động và sử dụng SRAM không hiệu quả khi kích thước tile nhỏ. S2-ATTENTION giải quyết điều này bằng cách theo dõi hiệu quả các mẫu sử dụng KV và động tích hợp các khối query với KVs được chia sẻ vào cùng một tile. Điều này đảm bảo hiệu quả IO, bất kể độ chi tiết sparsity, cải thiện đáng kể việc sử dụng SRAM và giảm việc tải KV dư thừa.

Những hiểu biết từ việc phát triển S2-ATTENTION tiết lộ rằng không phải tất cả các cơ chế attention thưa thớt đều hiệu quả trong thực tế. Nhiều attention thưa thớt không cần huấn luyện hiện có, bao gồm các phương pháp loại bỏ KV như LongGen (Ge et al., 2024b), H2O (Zhang et al., 2023), và MInference (Jiang et al., 2024), ít tương thích hơn với các cơ chế phục vụ nền tảng như continuous batching (Yu et al., 2022), PagedAttention (Kwon et al., 2023), song song 3D (Shoeybi et al., 2020). Ví dụ, trong PagedAttention (Kwon et al., 2023), việc loại bỏ token từ các khối KV sẽ chỉ tăng phân mảnh nội bộ, và mang lại overhead thêm trong lập lịch, điều này làm tổn hại throughput phục vụ. Trong khi đó, các nghiên cứu gần đây cho thấy attention thưa thớt không cần huấn luyện sẽ làm tổn hại khả năng ngữ cảnh dài của mô hình (Xiao et al., 2024; Ge et al., 2024a; Han et al., 2024). Điều này đã trở thành lý do chính tại sao chúng có việc áp dụng hạn chế trong các hệ thống phục vụ công nghiệp và suy luận nguồn mở cho đến nay (Kwon et al., 2023; Zheng et al., 2023).

2

--- TRANG 3 ---
Những hiểu biết mới này dẫn đến Head-Heterogenous Strided Transformer (HHST), một phương pháp attention thưa thớt mới theo các nguyên tắc thiết kế chính (§4.1), mà chúng tôi thấy quan trọng để đạt được lợi ích hiệu quả trong thực tế trong khi duy trì độ chính xác mạnh mẽ trên các nhiệm vụ downstream:

(1) HHST được thiết kế với phần cứng và hệ thống phần mềm trong tâm trí. Nó áp dụng một chiến lược chia sẻ thân thiện với phần cứng mới trên các đầu attention, trong đó mỗi đầu tập trung vào một tập hợp token riêng biệt theo mẫu strided, trong khi tập thể bao phủ ngữ cảnh đầy đủ (Hình 1; §4.2).

(2) Để đạt được hiệu suất mạnh mẽ trên các nhiệm vụ ngữ cảnh dài thách thức, việc bao gồm truy cập trực tiếp đến tất cả token là quan trọng, ít nhất tại các lớp nhất định. HHST đạt được điều này với một kiến trúc hybrid kết hợp attention thưa thớt và dày đặc trên các lớp, và cân bằng hiệu quả và hiệu suất (§4.2).

S2-ATTENTION áp dụng được trong cả huấn luyện và suy luận, giảm đáng kể rào cản khám phá các kiến trúc attention thưa thớt mới, mà chúng tôi khám phá trong §4 và §5. Chúng tôi pretrain một bộ mô hình ở quy mô 1.3B, 7B với attention thưa thớt khác nhau, và so sánh chúng với baseline attention dày đặc. Kết quả của chúng tôi cho thấy HHST-7B của chúng tôi phù hợp với hiệu suất của attention dày đặc trong khi đạt được tăng tốc huấn luyện 2.5× và tăng tốc suy luận 4.5×. Hơn nữa, chúng tôi mở rộng các mô hình 1.3B đến độ dài ngữ cảnh 32K, và các mô hình 7B đến 128K. Chúng tôi cho thấy HHST của chúng tôi có thể đạt được truy xuất Needle in a Haystack hoàn hảo (Kamradt, 2023). So với FlashAttention-2 (Dao, 2023), HHST có thể đạt được tăng tốc attention 8.8× và 15.9× cho quy mô 1.3B, 7B, và giảm thời gian wall-clock huấn luyện 2.8×, 2.5×.

S2-ATTENTION tương thích với các framework LLM thường được sử dụng bao gồm PyTorch, Megatron, HuggingFace, và vLLM. Với các API thân thiện với người dùng, việc nhập và tùy chỉnh S2-ATTENTION không mất quá vài dòng mã như được hiển thị trong Phụ lục B.

2 CÔNG TRÌNH LIÊN QUAN
Chúng tôi trình bày phân tích và quan sát của chúng tôi về các nỗ lực attention thưa thớt hiện có trong cả huấn luyện và suy luận.

<S> Q : Có 2 llamas và 
3 vicunas . Có bao nhiêu động vật
<S> GPT4 là LLM tốt nhất từ OpenAIBlock 1
Block 2
Block 3Được sử dụng
Đã loại bỏ
Không sử dụng

Hình 3: Minh họa tại sao các phương pháp loại bỏ KV có thể gây ra nhiều phân mảnh hơn. Ở đây chúng tôi hiển thị 3 trang của các khối KV chứa 2 yêu cầu. Mặc dù nhiều token đã bị loại bỏ, các slot được giải phóng khó có thể được sử dụng bởi các yêu cầu khác, dẫn đến tỷ lệ phân mảnh nội bộ cao hơn.

2.1 VẮNG MẶT KERNEL ATTENTION THƯA THỚT HIỆU QUẢ
Đã có những nỗ lực để giảm độ phức tạp tính toán của attention bằng cách chỉ tập trung vào một tập con các token (Child et al., 2019; Katharopoulos et al., 2020; Kitaev et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020). Tuy nhiên, những phương pháp này không thể mang lại tăng tốc wall-clock trong huấn luyện do bỏ qua chi phí truy cập bộ nhớ thực tế (Dao et al., 2022). Dao et al. (2022) chia nhỏ tính toán attention thành tính toán block-wise nhỏ hơn để giảm IO giữa SRAM và bộ nhớ băng thông cao (HBM). Việc triển khai phần cứng của họ FlashAttention (Dao et al., 2022; Dao, 2023) làm cho chúng trở thành framework tăng tốc attention được áp dụng rộng rãi nhất. Vẫn chưa rõ liệu chúng ta có thể triển khai các self-attention thưa thớt khác nhau theo cách nhận thức phần cứng như vậy, để tốc độ huấn luyện có thể được tăng cường hơn nữa so với FlashAttention.

2.2 VẤN ĐỀ VỚI CÁC PHƯƠNG PHÁP LOẠI BỎ KV PLUG-IN-AND-PLAY
Gần đây, các công trình loại bỏ KV plug-in-and-play phát triển mạnh. Cụ thể hơn, những phương pháp này động loại bỏ các vector KV tại suy luận để giảm dấu chân bộ nhớ dựa trên các tiêu chí nhất định được thiết kế để bảo tồn khả năng mô hình.

3

--- TRANG 4 ---
𝑚𝑞3𝐾𝑉1𝐾𝑉2𝐾𝑉3
tải 𝐾𝑉1 𝟖 lần cho tất cả 𝒒
tải 𝐾𝑉3 𝟔 lần cho tất cả 𝒒𝑞1
𝑞2
𝑞3𝐾𝑉1𝐾𝑉2𝐾𝑉3
tải 𝐾𝑉1 𝟒 lần cho tất cả 𝒒
tải 𝐾𝑉3 𝟑 lần cho tất cả 𝒒Vòng lặp trong
Merged_QK
SRAM
dotV
out𝑘3 và 𝑣3 được tái sử dụng 
bởi 𝑞6 sau 𝑞5 MergeQ
MergeQ
MergeQ
MergeQFlashAttention -2 S2-Attention
𝑞1+2
𝑞3+4
𝑞5+6
𝑞7+8𝑞5+6𝑘3
𝑣3 𝑞4
𝑞5
𝑞6
𝑞7
𝑞8𝑞1
𝑞2
𝑞3
𝑞4
𝑞5
𝑞6
𝑞7
𝑞8𝑘3
𝑣3Hình 4: Minh họa triển khai S2-Attention. Trái: Áp dụng trực tiếp tiling FlashAttention-2 cho attention thưa thớt. Phải: MergeQ, tích hợp thích ứng các query chia sẻ cùng KV khi tải vào SRAM, do đó giảm việc tải KV dư thừa và cải thiện hiệu quả IO.

Tuy nhiên, chúng tôi quan sát các thiết kế như vậy khó tương thích với các hệ thống phục vụ hiện có, dựa trên PagedAttention và continuous batching để quản lý bộ nhớ hiệu quả. Như được hiển thị trong Hình 3, trong quá trình loại bỏ KV, các token tương ứng được giải phóng khỏi bộ nhớ vật lý. Tuy nhiên, vì việc loại bỏ theo token không được đảm bảo liên tục, các slot bộ nhớ được giải phóng không thể được phân bổ hiệu quả cho các yêu cầu khác, được biết đến như phân mảnh nội bộ. Trong ví dụ này, phân mảnh nội bộ tăng 37.5%, điều này làm tổn hại throughput.

Trong khi đó, việc loại bỏ động cũng giới thiệu overhead trong lập lịch. Ví dụ, nếu các đầu khác nhau có chính sách/tỷ lệ loại bỏ khác nhau, các đầu nhanh hơn sẽ phải chờ các đầu chậm hơn, đây là một tình huống cân bằng tải cổ điển. Vấn đề nghiêm trọng hơn khi phục vụ các mô hình lớn hơn, nơi các tính toán được phân phối trên các thiết bị và node với tensor parallel và pipeline parallel. Những nhược điểm như vậy tiếp tục ngăn cản các thuật toán này được tích hợp vào các cluster phục vụ thực tế với hàng trăm node.

2.3 SUY GIẢM HIỆU SUẤT
Các nghiên cứu hiện có chỉ ra rằng cả phương pháp attention thưa thớt huấn luyện và không cần huấn luyện đều có suy giảm hiệu suất so với các đối tác dày đặc của chúng, đặc biệt trong các nhiệm vụ ngữ cảnh dài. Hơn nữa, chúng tôi cũng quan sát rằng một số phương pháp không cần huấn luyện (Jiang et al., 2024; Tang et al., 2024) cần các siêu tham số cụ thể benchmark để duy trì chất lượng mô hình. Khi áp dụng cho các yêu cầu chưa thấy, cùng một phương pháp có thể hiển thị hành vi không thể dự đoán. Tuy nhiên, trong triển khai thực tế, các truy vấn người dùng thường có phân phối đuôi dài. Do đó, không khả thi để xác định trước các siêu tham số cho các truy vấn người dùng chưa thấy, điều này làm cho việc triển khai các phương pháp như vậy có rủi ro.

Chúng tôi thảo luận về việc xử lý những quan sát này trong các phần dưới đây.

3 S2-ATTENTION: HIỆU QUẢ VÀ TÙY CHỈNH
Phần này trình bày S2-ATTENTION. Chúng tôi đầu tiên xem xét ngắn gọn các khái niệm cơ bản về bộ nhớ GPU và hệ thống phân cấp thực thi, sau đó giới thiệu kỹ thuật Merge-Q của chúng tôi, cải thiện đáng kể hiệu quả của kernel trong khi cho phép tùy chỉnh attention thưa thớt chi tiết hơn.

3.1 KIẾN THỨC CƠ BẢN
Các thread GPU có quyền truy cập vào một hệ thống phân cấp các loại bộ nhớ khác nhau. Bộ nhớ băng thông cao toàn cục (HBM) chậm nhất nhưng lớn nhất (khoảng >100× về độ trễ và ∼6K× về kích thước). Bộ nhớ chia sẻ (SRAM) về mặt vật lý trên chip, do đó có băng thông lớn hơn và độ trễ thấp hơn so với HBM. Tối ưu hóa tính toán của SRAM và giảm thiểu IO giữa HBM và SRAM là quan trọng để cải thiện hiệu quả của attention (Dao et al., 2022).

4

--- TRANG 5 ---
0
1
2
3
4
5
5432100
1
2
3
4
5
543210KV thực tế
(a) Sparsity không hiệu quả KV 0
1
2
3
4
5
5432100
1
2
3
4
5
543210
1
2
3
4
5
543210

(b) Sparsity hiệu quả KVHình 5: (a): Attention dilated dựa trên vị trí tương đối như một ví dụ về attention thưa thớt không hiệu quả KV. Ví dụ, bước 5 tập trung vào KV tại vị trí 1, 3, 5, trong khi bước 4 tập trung vào 0, 2, 4. Điều này dẫn đến yêu cầu KV cache đầy đủ. Mặc dù nó gợi ý tiết kiệm bộ nhớ gần 50% trên lý thuyết, thực tế nó yêu cầu lưu trữ KV cache đầy đủ. (b) Tất cả các mẫu attention này đều hiệu quả KV, vì chúng được đẩy vào KV-cache khi đầu tiên gặp phải khi giải mã, sau đó liên tục được tập trung trong vài bước trước khi cuối cùng bị loại bỏ (ví dụ, tất cả token trong hình trái, và token 0 trong hình phải) và không bao giờ được tập trung lại, hoặc vẫn được tập trung cho tất cả token tương lai (ví dụ, token 0, 2, 4 trong hình giữa và token 2, 4 trong hình phải). Các mũi tên cho thấy tất cả chúng đều chia sẻ mẫu "đường thẳng đứng".

Các triển khai attention được tối ưu hóa kém có thể dẫn đến I/O thường xuyên đến HBM và làm tổn hại đáng kể hiệu quả. CUDA tổ chức các thread thành các thread block, được chia nhỏ hơn thành các warp, nhóm 32 thread. Các thread trong một block chia sẻ dữ liệu thông qua SRAM. Mong muốn rằng các thread khác nhau trong cùng một warp đi theo cùng một đường thực thi vì nếu không hiệu quả sẽ bị tổn hại do warp divergence. Bên cạnh đó, kích thước thread block nên đủ lớn để đạt được sử dụng tốt và cân bằng tải. Một tile là một phần của các tensor Q, K, V được gán cho một thread block để xử lý. Để rõ ràng, chúng tôi lấy kích thước tile làm kích thước block. FlashAttention cải thiện hiệu quả bằng cách giảm thiểu I/O HBM, chia tiles các tensor Q, K, V thành các chunk phù hợp với SRAM để tính toán hiệu quả (Dao et al., 2022), một nguyên tắc mà công trình này tuân theo.

3.2 S2-ATTENTION
Khởi động (Hình 4 trái) Chúng tôi đầu tiên xem xét một triển khai blocksparse đơn giản sử dụng thuật toán FlashAttention. Một chuỗi N token được phân đoạn thành B = ⌈N/S⌉ shard, mỗi shard chứa S token liên tiếp. Chúng tôi sử dụng Q[i] để biểu thị các vector query cho shard query thứ i, và tương tự K[i] các vector key cho shard key thứ i. Theo Dao et al. (2022), đối với mỗi vector query q, chúng tôi lặp qua các tile K trong SRAM để tính toán softmax(qK⊤). Không giống như attention dày đặc sử dụng toàn bộ tensor K, chúng tôi chỉ xem xét một tập con các key được chỉ định bởi một mask attention thưa thớt M, có thể được lưu trữ trong định dạng Compressed Sparse Row (CSR) để tiết kiệm bộ nhớ.¹

Để hiểu rõ hơn hiệu quả của triển khai như vậy, chúng ta có thể tính số lần tải cần thiết cho mỗi shard key/value. Như được hiển thị trong Hình 4 (trái), shard key/value đầu tiên, KV1, được tập trung bởi tất cả các shard query, q1−q8. Do đó, KV1 được tải 8 lần từ HBM đến SRAM. Nếu chúng ta nhân đôi kích thước shard, số lượng shard query tập trung vào KV1 sẽ giảm một nửa xuống 4. Trong trường hợp này, KV1 chỉ cần 4 lần tải hiệu quả hơn. Tuy nhiên, hiệu quả IO đi kèm với chi phí của độ chi tiết mask thưa thớt của chúng ta, vì bây giờ chúng ta phải mask-hoặc-giữ 2S token thay vì S. Sau đó chúng tôi thảo luận về cách đạt được cả hiệu quả IO và độ chi tiết mask nhỏ với Merge-Q.

Merge-Q Ở mức độ cao, ý tưởng cốt lõi là tích hợp các shard query tập trung vào cùng các khối KV thành một tile duy nhất để chúng ta không cần tải riêng biệt cùng các khối KV. Theo cách này, ngay cả khi độ chi tiết mask trở nên nhỏ hơn, chúng ta vẫn có thể duy trì hiệu quả IO. Hình 4: phải hiển thị một trường hợp đơn giản hơn, nơi chúng tôi tích hợp hai shard query láng giềng. So với baseline FlashAttention-2, triển khai này chỉ cần tải KV1 4 lần thay vì 8 lần với cùng độ chi tiết mask. Merge-Q giúp S2-ATTENTION hỗ trợ kích thước shard nhỏ đến 16, cho phép một phạm vi rộng hơn các mẫu attention thưa thớt. Ý tưởng tương tự cũng có thể được áp dụng để tích hợp các khối KV để tăng cường hiệu quả hơn nữa. Chúng tôi để lại thảo luận triển khai chi tiết hơn trong mã được phát hành và Phụ lục D.

¹https://docs.nvidia.com/nvpl/_static/sparse/storage_format/sparse_matrix.html

5

--- TRANG 6 ---
Với S2-ATTENTION, cộng đồng có thể tùy chỉnh các mẫu attention thưa thớt chi tiết với tăng tốc wall-clock. Tuy nhiên, vẫn chưa rõ loại attention thưa thớt nào có thể đạt được tăng tốc mà không làm tổn hại chất lượng. Chúng tôi nhằm mục đích trả lời câu hỏi này trong phần sau.

4 S2-ATTENTION: HIỂU BIẾT, CÔNG THỨC VÀ COOKBOOK SPARSITY
Trong phần này, chúng tôi đầu tiên thảo luận loại mẫu attention thưa thớt nào cho phép triển khai kernel hiệu quả trong thực tế (§4.1). Xây dựng trên những hiểu biết này, chúng tôi giới thiệu Head-Heterogenous Strided Transformer (HHST), một kiến trúc attention thưa thớt mới (§4.2).

4.1 SPARSITY HIỆU QUẢ KV
KV cache là một nút thắt cổ chai bộ nhớ chính cho LMs decoder-only tại thời gian suy luận. Nhiều attention thưa thớt hiện có xác định token nào để tập trung dựa trên khoảng cách tương đối (Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020). Tuy nhiên, những phương pháp này không hiệu quả bộ nhớ GPU trong quá trình giải mã, làm cho việc chuyển đổi tiết kiệm FLOP của chúng thành lợi ích hiệu quả thực tế trở nên khó khăn. Hình 5(a) cung cấp một ví dụ minh họa. Vấn đề chính là, đối với attention thưa thớt như vậy, KV không được sử dụng trong các bước giải mã trước đó có thể được yêu cầu trong các bước sau, làm cho việc quản lý bộ nhớ trở nên thách thức hơn. Mặc dù tiết kiệm bộ nhớ gần 50% trên lý thuyết, nó thực sự yêu cầu lưu trữ KV cache đầy đủ trong thực tế, dẫn đến không tiết kiệm bộ nhớ.

Ngược lại, Hình 5(b) minh họa một attention thưa thớt có thể đạt được tiết kiệm bộ nhớ trong thực tế. Điểm quan trọng là KV cache được lưu trữ được tái sử dụng qua một số bước giải mã nhưng không còn cần thiết trong các bước tương lai, và do đó có thể được loại bỏ, giải phóng bộ nhớ GPU.

Sự so sánh giữa hai phương pháp này dẫn đến quy tắc kinh nghiệm sau về thiết kế attention thưa thớt hiệu quả KV. Đối với ∀j≥i, l≥1,
(ki,vi) được tập trung bởi qj+l
⟹(ki,vi) cũng phải được tập trung bởi qj.(1)
Nếu không, ki và vi cần được lưu trữ tại bước j cho các thế hệ tương lai, ngay cả khi nó không được sử dụng tại bước j.
Trực quan, trong ma trận mẫu attention, chúng ta sẽ thấy "đường thẳng đứng" liên tục như được hiển thị trong Hình 5(b). Điều này có nghĩa là các mẫu thưa thớt nên dựa trên vị trí tuyệt đối thay vì vị trí tương đối, ngoại trừ ngữ cảnh địa phương liên tiếp (ví dụ, hình trái trong Hình 5(b)).

4.2 HEAD-HETEROGENOUS STRIDED TRANSFORMER
Phần này giới thiệu Head-Heterogenous Strided Transformer (HHST), một attention thưa thớt hiệu quả mới được truyền cảm hứng bởi những hiểu biết chúng tôi học được ở trên. Cốt lõi của thiết kế của nó là hai lựa chọn thiết kế được giới thiệu dưới đây.

Chia sẻ ngữ cảnh không đồng nhất qua các đầu Attention Để đạt được tải cân bằng qua các đầu attention và tăng cường song song hóa, mỗi đầu nên tập trung vào số lượng token bằng nhau. Ngoài ra, HHST đảm bảo rằng các đầu khác nhau tập trung vào các shard khác nhau của ngữ cảnh trong khi tập thể bao phủ toàn bộ ngữ cảnh. Thiết kế này đảm bảo rằng HHST luôn có truy cập trực tiếp đến ngữ cảnh đầy đủ tại mỗi lớp, mà không làm tổn hại song song hóa. Hình 1 cung cấp một sơ đồ minh họa.

Chính thức hơn, đối với ngữ cảnh với B shard, chúng tôi lấy Bl khối gần đây nhất làm khối địa phương và đặt phần còn lại làm khối từ xa. Đối với đầu attention với chỉ số h, mask attention khối B×B Mh của nó là:
Mh i,j = (1, i−j < Bl, Địa phương
1, j−oh∈sZ≥0∧i−j∈[Bl, B) Stride
0 nếu không

s là kích thước stride, và x∈mZ≥0 có nghĩa x là 0 hoặc một bội số dương của m. Tương tự như một cửa sổ trượt, các token vượt quá B shard không được tập trung.

Tính linh hoạt của kernel S2-ATTENTION của chúng tôi cho phép triển khai hiệu quả chiến lược này. Như được hiển thị trong các thí nghiệm của chúng tôi, thiết kế này cho phép mô hình đạt được hiệu suất ngữ cảnh dài mạnh mẽ trong khi tối đa hóa lợi ích hiệu quả.

6

--- TRANG 7 ---
Kiến trúc Hybrid Như các nghiên cứu trước đó cho thấy (Huang et al., 2022; Lieber et al.), một số lớp attention dày đặc hơn đáng kể so với những lớp khác, với trọng số attention được phân phối gần như đồng đều trên tất cả các vị trí. Do đó, việc giữ lại attention dày đặc trong những lớp này đặc biệt có lợi. Điều này thúc đẩy chúng tôi khám phá một kiến trúc hybrid kết hợp attention thưa thớt hiệu quả của chúng tôi trong hầu hết các lớp với attention dày đặc trong những lớp khác. Chúng tôi phát hiện thực nghiệm rằng chiến lược attention thưa thớt của chúng tôi rất hiệu quả, chỉ yêu cầu 1/6 số lớp attention là dày đặc để đạt được hiệu suất truy xuất mạnh mẽ với ngữ cảnh dài 128K. Khám phá thêm được trình bày trong các thí nghiệm của chúng tôi.

Thảo luận Điều quan trọng cần chỉ ra rằng tất cả các chiến lược loại bỏ nhắm vào suy luận (Zhang et al., 2023; Liu et al., 2023; Ge et al., 2024b) đều hiệu quả KV-cache, vì KV bị loại bỏ sẽ không bao giờ được sử dụng bởi các truy vấn tương lai. Tuy nhiên, những chiến lược này giới thiệu các mẫu sparsity phụ thuộc mẫu, làm cho việc xác định thời gian loại bỏ trong quá trình giải mã trở nên tốn kém tính toán. Ngược lại, phương pháp của chúng tôi sử dụng một mẫu sparsity cố định trên tất cả các mẫu, loại bỏ overhead của việc quyết định token nào để loại bỏ. Bên cạnh đó, các phương pháp loại bỏ KV là post-hoc và thường hoạt động kém hơn nhiều so với đối tác dày đặc ban đầu (Ge et al., 2024a). HHST của chúng tôi, như chúng ta sẽ sớm thấy trong các thí nghiệm, thích ứng với attention thưa thớt trong quá trình huấn luyện (pre-training hoặc post-training) hoạt động tương đương với baseline dày đặc trong khi giảm overhead huấn luyện.

5 THÍ NGHIỆM
Để đánh giá HHST, chúng tôi đầu tiên nghiên cứu chất lượng pre-training trong §5.1 và §5.2. Sau đó chúng tôi benchmark hiệu quả kernel và độ trễ phục vụ end-to-end trong §5.4 và § ??. Cuối cùng, chúng tôi tiến hành một nghiên cứu ablation về các lựa chọn thiết kế.

5.1 BENCHMARKING CHẤT LƯỢNG HUẤN LUYỆN MÔ HÌNH
Cài đặt Chúng tôi đầu tiên huấn luyện một loạt mô hình 1.3B với kiến trúc Llama 2, với 24 lớp, kích thước ẩn 2048 với 16 đầu, với độ dài chuỗi tối đa là 8192. Chúng tôi sử dụng FineWeb-Edu-350B nguồn mở Penedo et al. (2024) làm corpus pre-training. Một tokenizer OpenAI Tiktoken với kích thước từ vựng 100K được sử dụng để xử lý văn bản thô. Tất cả các biến thể mô hình sử dụng kích thước batch 4M token cho tất cả độ dài chuỗi và huấn luyện tổng cộng 300 tỷ token. Đối với siêu tham số, chúng tôi sử dụng µP Yang et al. (2022) với hình dạng cơ sở 256. Tỷ lệ học µP 0.02 được sử dụng với suy giảm tuyến tính và 0.5% tổng số token huấn luyện cho warmup. Tất cả các mô hình được đánh giá sau khi huấn luyện trên tổng số 300B token cho một epoch.

Nhiệm vụ Downstream Chúng tôi sử dụng một mô hình với attention dày đặc làm baseline của chúng tôi, được ký hiệu là "Dense". Để nghiên cứu cấu trúc hybrid của chúng tôi với chia sẻ không đồng nhất và tính hoàn chỉnh hợp nhất, chúng tôi kiểm soát FLOPs để tương đương xấp xỉ. Tổng số token được tập trung khoảng 576 token, hoặc 9 shard của 64 token. Chúng tôi sử dụng điều này để cấu hình sliding window attention (SWA), làm tập kiểm soát. Chúng tôi thêm các thay đổi khác nhau vào SWA để xem chúng ảnh hưởng đến chất lượng huấn luyện như thế nào. Các tập xử lý được nhóm thành 1) Đồng nhất (Các đầu khác nhau tập trung vào cùng các shard); 2) Không đồng nhất & Không đầy đủ (Các đầu khác nhau tập trung vào các shard khác nhau nhưng không bao phủ toàn bộ ngữ cảnh), và 3) Không đồng nhất & Đầy đủ (Các đầu khác nhau tập trung vào cùng các shard và tập thể bao phủ toàn bộ ngữ cảnh).

Từ Bảng 1, chúng ta có thể quan sát các kiến trúc hybrid cho thấy kết quả hứa hẹn. Như chúng ta có thể thấy từ S2-L1V15 + Dense (HHST) trong hàng cuối, chia sẻ không đồng nhất với ngữ cảnh đầy đủ và hai lớp dày đặc cho kết quả tốt nhất nhất quán trên các nhiệm vụ, với khoảng cách nhỏ từ baseline attention mặc định trong khi chỉ sử dụng 18% FLOPs. Đáng chú ý, trong nhiệm vụ Passkey Retrieval, HHST có thể đạt được hiệu suất tốt hơn nhiều so với mô hình dày đặc. Quan sát này hoạt động như một xác thực ban đầu về khả năng hiểu ngữ cảnh của thiết kế HHST. Chúng tôi sẽ xác thực thêm nó trong phần huấn luyện liên tục ngữ cảnh dài.

Chúng tôi cũng thấy việc thêm hai lớp dày đặc thường dẫn đến hiệu suất cao hơn đáng kể. Trong nhóm Đồng nhất, chúng ta có thể quan sát việc thêm attention sink có thể tăng cường đáng kể chất lượng huấn luyện, so với chỉ sử dụng sliding window (SWA). Trong nhóm Không đồng nhất & Không đầy đủ, kích thước stride dọc lớn hơn số lượng đầu attention, làm cho ngữ cảnh không đầy đủ sau hợp nhất. Đối với nhóm Không đồng nhất & Đầy đủ, chúng tôi điều chỉnh kích thước stride và cửa sổ địa phương để nó vừa bao phủ ngữ cảnh đầy đủ trong khi có cùng FLOPs như những nhóm khác. Khi so sánh

7

--- TRANG 8 ---
Bảng 1: Đánh giá chất lượng Pre-Training. "SWA" đề cập đến sliding window attention. "L" đề cập đến số khối địa phương. "V" đề cập đến kích thước stride dọc. "+ Sink" đề cập đến tập trung vào attention sink. "+ Dense" đề cập đến làm cho hai lớp attention đầu tiên dày đặc.

Mô hình Passkey WinoGrande PIQA RACE Wikitext103(ppl)
Dense (Giới hạn trên) 0.865 0.592 0.733 0.403 15.884
Đồng nhất (18% FLOPs của Dense)
HHST-L9 (SWA) 0.334 0.547 0.705 0.363 21.997
HHST-L9 + Dense 0.620 0.575 0.714 0.373 20.450
HHST-L9 + Sink 0.560 0.566 0.721 0.380 21.037
HHST-L9 + Sink + Dense 0.771 0.577 0.728 0.388 18.503
HHST-L1V15 0.542 0.541 0.716 0.352 21.035
HHST-L1V15 + Dense 0.741 0.568 0.713 0.349 20.579
Không đồng nhất & Không đầy đủ (18% FLOPs của Dense)
HHST-L2V18 0.630 0.565 0.728 0.357 20.502
HHST-L2V18 + Dense 0.823 0.587 0.732 0.379 18.726
HHST-L4V25 0.612 0.542 0.720 0.352 20.875
HHST-L4V25 + Dense 0.795 0.569 0.724 0.386 19.285
Không đồng nhất & Đầy đủ (18% FLOPs của Dense)
HHST-L1V15 0.782 0.571 0.724 0.361 19.551
HHST-L1V15 + Dense (HHST) 0.941 0.587 0.725 0.397 17.183

nhóm Không đầy đủ với nhóm Đầy đủ, chúng ta có thể thấy lợi ích của việc làm cho hợp nhất ngữ cảnh đầy đủ bằng cách giới hạn kích thước stride dọc.

5.2 HUẤN LUYỆN LIÊN TỤC NGỮ CẢNH DÀI

4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense15-17
0.00.20.40.60.81.0
Score

(a) 2 lớp dày đặc (11% FLOPs)

4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense14-18
0.00.20.40.60.81.0
Score

(b) 4 lớp dày đặc (17% FLOPs)

4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense12-20
0.00.20.40.60.81.0
Score

(c) 8 lớp dày đặc (28% FLOPs)

Hình 6: Đánh giá 128K Needle In A Haystack. Chúng tôi sửa đổi số lượng lớp dày đặc và chứng minh tiết kiệm FLOPs so với Dense (tất cả lớp đều dày đặc).

Chúng tôi tiếp tục kiểm tra cách thích ứng attention thưa thớt với ngữ cảnh dài hơn. Chúng tôi bắt đầu từ một mô hình đã được pre-train dày đặc hiện có và mở rộng độ dài ngữ cảnh của nó bằng cách tiếp tục huấn luyện nó trên độ dài ngữ cảnh dài hơn với kiến trúc thưa thớt HHST. Cụ thể, chúng tôi chọn Llama-2-7B và tiếp tục huấn luyện nó trên độ dài ngữ cảnh 128K. Chúng tôi thay đổi RoPE base thành 5M. Cả hai mô hình đều được huấn luyện liên tục với 10B token theo công thức trong Fu et al. (2024). Chúng tôi đánh giá các mô hình trên nhiệm vụ Needle In A Haystack Kamradt (2023).

Để điều tra cách đạt được hiệu suất ngữ cảnh dài mạnh mẽ, chúng tôi sửa đổi số lượng lớp dày đặc trong HHST. Chúng tôi đặt số lượng lớp dày đặc là 2, 4 và 8, tương ứng. Chúng tôi cố định số lượng khối địa phương là 31 và kích thước stride dọc là 32. Như được hiển thị trong Hình 6, đối với ngữ cảnh 128K, mô hình có thể truy xuất ngữ cảnh đầy đủ với 8 lớp dày đặc nhưng không thể làm được với chỉ 2 và 4 lớp dày đặc. Kết quả xác thực khả năng ngữ cảnh dài của thiết kế HHST.

8

--- TRANG 9 ---
5.3 TĂNG TỐC HUẤN LUYỆN

5.3.1 BENCHMARK HOẠT ĐỘNG ATTENTION

Cài đặt Benchmark Chúng tôi đo runtime attention của HHST với kernel S2-ATTENTION của chúng tôi, và FlashAttention-2 trên GPU A100 80GB cho các cài đặt độ dài ngữ cảnh, số đầu, và chiều đầu khác nhau.

8192 16384 32768
Sequence Length020406080Step Time (s)Forward Pass Latency (1.3B)
S2-Attention (Fwd)
Flash-Attention2 (Fwd)

8192 16384 32768
Sequence Length050100150200250Step Time (ms)Backward Pass Latency (1.3B)
S2-Attention (Bwd)
Flash-Attention2 (Bwd)

(a) Tăng tốc Attention 1.3B.

8192 16384 32768 65536
Sequence Length0100200300400500600Step Time (s)Forward Pass Latency (7B)
S2-Attention (Fwd)
Flash-Attention2 (Fwd)

8192 16384 32768 65536
Sequence Length02505007501000125015001750Step Time (ms)Backward Pass Latency (7B)
S2-Attention (Bwd)
Flash-Attention2 (Bwd)

(b) Tăng tốc Attention 7B.

Hình 7: Tăng tốc Attention vs Độ dài Chuỗi và Quy mô Mô hình.

Trong Hình 7 và Hình 2a Chúng tôi benchmark tăng tốc được mang lại bởi HHST trong kích thước mô hình 1.3B, 7B, 70B trên các độ dài chuỗi khác nhau để thể hiện khả năng mở rộng của hệ thống chúng tôi. Đối với tất cả kích thước mô hình, HHST có thể đạt được tăng tốc nhiều lần so với FlashAttention-2. Đối với mô hình 70B với 64 đầu, HHST có thể tăng tốc end-to-end 25.3×. Ví dụ, trong mô hình 1.3B với stride dọc 16, HHST có thể đạt được tăng tốc 8.8×. Khi độ dài chuỗi tối đa tăng dài hơn, tăng tốc dần dần xấp xỉ lợi ích giảm FLOPs lý thuyết. Tăng cường tổng thể bị cản trở một chút do kernel backward ít được tối ưu hóa của chúng tôi, để lại chỗ cho cải thiện thêm.

5.4 TĂNG TỐC HUẤN LUYỆN VÀ SUY LUẬN

8192 16384 32768 65536 131072
Sequence Length050000100000150000200000250000300000350000T okens per SecondT oken Throughput(7B)
Dense
S2-Attention

(a) Tăng tốc huấn luyện end-to-end 7B.

8063 16255 32639 65407 130943 262015050200
150
100FlashAttention in vLLM
S2-Attention in vLLMLatency (s)Inference Benchmark on vLLM
4.5X
2.9X 1.9X
1.2X 1.1X1.5X

(b) Tăng tốc suy luận end-to-end 7B trên vLLM.

Chúng tôi đánh giá tăng tốc huấn luyện end to end của các mô hình 1.3B và 7B bằng cách đo throughput token của cả hai mô hình. Tất cả mô hình được huấn luyện trên 256 A100, với kích thước batch 8M token và activation checkpointing. Đối với 1.3B, HHST có thể đạt được 1.2×, 1.8×, 2.3×, và 2.8× throughput token trên ngữ cảnh 8K đến 128K so với FlashAttention-2. Đối với mô hình 7B, HHST có thể đạt được cải thiện throughput token 1.1×, 1.2×, 1.5×, và 2.5×. Để chứng minh cải thiện hiệu quả suy luận của HHST, chúng tôi đo độ trễ end-to-end trên các cài đặt độ dài ngữ cảnh khác nhau. Để làm cho so sánh thực tế, các thí nghiệm của chúng tôi được thực hiện trên vLLM (Kwon et al., 2023). Chúng tôi chọn backend FlashAttention-2 trong vLLM làm baseline để so sánh công bằng, vì kernel suy luận của S2-ATTENTION cũng dựa trên vLLM. Cả hai phương pháp đều được triển khai trên một node duy nhất với 8 GPU A100 80, với kích thước tensor parallel bằng 4. Chúng tôi đặt độ dài đầu ra là 128 và thay đổi độ dài đầu vào giữa 16K đến 256K. Như được hiển thị trong Hình 8b, HHST có thể đạt được tăng tốc 1.1×, 1.2×, 2.9×, 4.5× trên ngữ cảnh 8K, 16K, 128K, 256K.

9

--- TRANG 10 ---
6 KẾT LUẬN
Chúng tôi đã trình bày S2-ATTENTION, một thư viện kernel Triton được tối ưu hóa cung cấp nhiều triển khai attention thưa thớt có thể tùy chỉnh cho cả huấn luyện và suy luận. Những hiểu biết từ S2-ATTENTION dẫn đến một số nguyên tắc về các lựa chọn thiết kế của phương pháp attention thưa thớt để làm cho chúng hiệu quả trong thực tế. Chúng truyền cảm hứng cho một kiến trúc attention thưa thớt hybrid mới đáp ứng một số tiêu chí mà chúng tôi thấy quan trọng để đạt được cả lợi ích hiệu quả thực tế và độ chính xác mạnh mẽ trên các nhiệm vụ downstream, được gọi là Head-Heterogenous Strided Transformer (HHST). Chúng tôi sẽ mở nguồn thư viện kernel của chúng tôi và làm cho nó trở thành một thay thế plug-in-and-play cho module FlashAttention-2 trong các framework huấn luyện phổ biến như Megatron và Pytorch. Chúng tôi cũng tích hợp S2-ATTENTION vào backend vLLM để phục vụ tức thì. Cả kernel huấn luyện và suy luận đều cho phép người dùng tự do tùy chỉnh mẫu sparsity của họ, tạo điều kiện cho toàn bộ cộng đồng nghiên cứu chủ đề này trong tương lai.

TÀI LIỆU THAM KHẢO
Iz Beltagy, Matthew E. Peters, và Arman Cohan. Longformer: The long-document transformer.
CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating long sequences with sparse
transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509.

Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR,
abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. URL https://doi.org/10.
48550/arXiv.2307.08691.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. Flashattention:
Fast and memory-efficient exact attention with io-awareness. Trong Sanmi Koyejo, S. Mo-
hamed, A. Agarwal, Danielle Belgrave, K. Cho, và A. Oh (eds.), Advances in Neural
Information Processing Systems 35: Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,
2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.

Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, và Hao
Peng. Data engineering for scaling language models to 128k context. Trong Forty-first International
Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net,
2024. URL https://openreview.net/forum?id=TaAqeo7lUh.

Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, và Hao Peng. A little goes a long way: Efficient
long context training and inference with partial contexts. arXiv preprint arXiv:2410.01485, 2024a.

Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, và Jianfeng Gao. Model tells you
what to discard: Adaptive KV cache compression for llms. Trong The Twelfth International Conference
on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net,
2024b. URL https://openreview.net/pdf?id=88nT0j5jAn.

Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, và Sinong Wang. LM-infinite:
Zero-shot extreme length generalization for large language models. Trong Kevin Duh, Helena Gomez,
và Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies (Volume 1:
Long Papers), pp. 3991–4008, Mexico City, Mexico, June 2024. Association for Computational
Linguistics. doi: 10.18653/v1/2024.naacl-long.222. URL https://aclanthology.org/
2024.naacl-long.222/.

Xin Huang, Ashish Khetan, Rene Bidart, và Zohar Karnin. Pyramid-bert: Reducing complexity
via successive core-set based token selection. Trong Smaranda Muresan, Preslav Nakov, và Aline
Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 8798–8817.
Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.602. URL
https://doi.org/10.18653/v1/2022.acl-long.602.

10

--- TRANG 11 ---
Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua
Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, và Lili Qiu. Minference 1.0:
Accelerating pre-filling for long-context llms via dynamic sparse attention. CoRR, abs/2407.02490,
2024. doi: 10.48550/ARXIV.2407.02490. URL https://doi.org/10.48550/arXiv.
2407.02490.

Greg Kamradt. Needle in a haystack-pressure testing llms. Github Repository, pp. 28, 2023.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. Trong Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119
of Proceedings of Machine Learning Research, pp. 5156–5165. PMLR, 2020. URL http:
//proceedings.mlr.press/v119/katharopoulos20a.html.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. Trong
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
rkgNKkHtvB.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, và Ion Stoica. Efficient memory management for large language model
serving with pagedattention. CoRR, abs/2309.06180, 2023. doi: 10.48550/arXiv.2309.06180. URL
https://doi.org/10.48550/arXiv.2309.06180.

Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,
Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-
mamba language model, 2024. URL https://arxiv.org/abs/2403.19887.

Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios
Kyrillidis, và Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance
hypothesis for LLM KV cache compression at test time. CoRR, abs/2305.17118, 2023. doi:
10.48550/arXiv.2305.17118. URL https://doi.org/10.48550/arXiv.2305.17118.

OpenAI. Gpt-4 technical report, 2023.

Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin
Raffel, Leandro von Werra, và Thomas Wolf. The fineweb datasets: Decanting the web for the
finest text data at scale. CoRR, abs/2406.17557, 2024. doi: 10.48550/ARXIV.2406.17557. URL
https://doi.org/10.48550/arXiv.2406.17557.

Szymon Rucinski. Efficient language adaptive pre-training: Extending state-of-the-art large language
models for polish. CoRR, abs/2402.09759, 2024. doi: 10.48550/ARXIV.2402.09759. URL
https://doi.org/10.48550/arXiv.2402.09759.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, và Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism,
2020.

Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, và Song Han. Quest:
Query-aware sparsity for efficient long-context llm inference, 2024. URL https://arxiv.
org/abs/2406.10774.

Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. Efficient transformers: A survey. ACM
Comput. Surv., 55(6):109:1–109:28, 2023. doi: 10.1145/3530811. URL https://doi.org/
10.1145/3530811.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,

11

--- TRANG 12 ---
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, và Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023.

Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, và
Song Han. Duoattention: Efficient long-context LLM inference with retrieval and streaming heads.
CoRR, abs/2410.10819, 2024. doi: 10.48550/ARXIV.2410.10819. URL https://doi.org/
10.48550/arXiv.2410.10819.

Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pa-
chocki, Xiaodong Liu, Weizhu Chen, và Jianfeng Gao. Tensor programs v: Tuning
large neural networks via zero-shot hyperparameter transfer. Trong NeurIPS 2021, March
2022. URL https://www.microsoft.com/en-us/research/publication/
tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/.

Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, và Byung-Gon Chun. Orca: A
distributed serving system for transformer-based generative models. Trong Marcos K. Aguilera và
Hakim Weatherspoon (eds.), 16th USENIX Symposium on Operating Systems Design and Imple-
mentation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, pp. 521–538. USENIX Association,
2022. URL https://www.usenix.org/conference/osdi22/presentation/yu.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. Big bird:
Transformers for longer sequences. Trong Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, và Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, và Beidi Chen. H2o: Heavy-
hitter oracle for efficient generative inference of large language models. CoRR, abs/2306.14048,
2023. doi: 10.48550/arXiv.2306.14048. URL https://doi.org/10.48550/arXiv.
2306.14048.

Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,
Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Sglang: Efficient execution of structured
language model programs, 2024. URL https://arxiv.org/abs/2312.07104, 2023.

A PHỤ LỤC
Bạn có thể bao gồm các phần bổ sung khác ở đây.

12
