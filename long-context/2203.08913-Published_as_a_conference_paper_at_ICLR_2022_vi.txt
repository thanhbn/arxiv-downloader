# 2203.08913.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2203.08913.pdf
# Kích thước file: 1047591 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
MEMORIZING TRANSFORMERS
Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, Christian Szegedy
{yuhuai,mrabe,delesley,szegedy}@google.com
TÓM TẮT
Các mô hình ngôn ngữ thường cần được huấn luyện hoặc tinh chỉnh để thu được
kiến thức mới, điều này liên quan đến việc cập nhật trọng số của chúng. Thay vào đó, chúng tôi hình dung
các mô hình ngôn ngữ có thể đơn giản đọc và ghi nhớ dữ liệu mới tại thời điểm suy luận,
do đó thu được kiến thức mới ngay lập tức. Trong nghiên cứu này, chúng tôi mở rộng các
mô hình ngôn ngữ với khả năng ghi nhớ các biểu diễn nội bộ của các đầu vào trong quá khứ. Chúng tôi
chứng minh rằng một tìm kiếm kNN gần đúng vào một bộ nhớ không khả vi của các cặp
(key, value) gần đây cải thiện mô hình hóa ngôn ngữ trên nhiều benchmark và nhiệm vụ khác nhau, bao gồm
webtext chung (C4), bài báo toán học (arXiv), sách (PG-19), mã nguồn (Github), cũng như
các định lý hình thức (Isabelle). Chúng tôi cho thấy rằng hiệu suất
liên tục cải thiện khi chúng tôi tăng kích thước bộ nhớ lên đến 262K token. Trên các
benchmark bao gồm mã nguồn và toán học, chúng tôi phát hiện rằng mô hình có khả năng
sử dụng các hàm và định lý mới được định nghĩa trong thời gian kiểm tra.
1 GIỚI THIỆU
Transformers (Vaswani et al., 2017) đã dẫn đến tiến bộ đáng kể trong xử lý ngôn ngữ tự
nhiên (Devlin et al., 2019; Brown et al., 2020), lý luận toán học (Polu & Sutskever, 2020; Wang
et al., 2020a; Rabe et al., 2021; Li et al., 2021; Hahn et al., 2021; Cobbe et al., 2021), và tổng hợp
chương trình (Austin et al., 2021; Chen et al., 2021; Li et al., 2022). Tuy nhiên, hiệu suất của transformer
trên nhiều nhiệm vụ này bị giới hạn bởi độ dài ngữ cảnh của attention, thường ngắn. Khả
năng chú ý đến các token ở xa là quan trọng trong nhiều tình huống. Trong tiểu thuyết, các nhân vật và sự kiện
được tham chiếu qua nhiều chương. Trong mã nguồn, các tham chiếu đến lớp và hàm có thể
xảy ra khá xa các nơi chúng được định nghĩa. Trong chứng minh định lý, các chứng minh sử dụng
các bổ đề đã được định nghĩa trước đó.
Attention trên các chuỗi dài cũng hữu ích như một dạng học nhanh. Các sự kiện và thông tin
được lưu trữ dưới dạng ma trận trọng số phải được huấn luyện chậm qua hàng trăm nghìn
bước huấn luyện. Tuy nhiên, bằng cách sử dụng attention, một mô hình có thể đơn giản ghi nhớ các sự kiện (ví dụ: định nghĩa
hàm) bằng cách lưu trữ chúng như các cặp (key, value) trong bộ nhớ dài hạn, và sau đó truy xuất các sự kiện đó
sau bằng cách tạo một truy vấn chú ý đến chúng. Trong trường hợp này, attention hoạt động như một dạng truy xuất thông
tin, cho phép mô hình tra cứu các sự kiện mà nó đã thấy trước đó.
Chúng tôi chứng minh rằng một cách đơn giản và hiệu quả để tăng kích thước của ngữ cảnh attention là sử
dụng tìm kiếm k-nearest-neighbor (kNN) gần đúng, được sử dụng rộng rãi trong truy xuất thông tin. Một
số triển khai có thể mở rộng cực kỳ của tìm kiếm kNN có sẵn, chẳng hạn như ScaNN (Guo
et al., 2020) và Faiss (Johnson et al., 2021).
Có hai điều phân biệt phương pháp của chúng tôi với các nghiên cứu trước về attention tầm xa (xem
Phần 2). Thứ nhất, không giống như một số phương pháp khác, tìm kiếm kNN không thực hiện việc tính trung bình hoặc tóm tắt
các token ở khoảng cách xa, mà truy xuất các giá trị chính xác ngay cả từ ngữ cảnh xa.
Thứ hai, gradient không được lan truyền ngược vào bộ nhớ ngoài, điều quan trọng đối với khả năng mở rộng
của kỹ thuật của chúng tôi. Các key và value là một hàm của các tham số mô hình, vì vậy việc cố gắng lan truyền ngược
gradient vào bộ nhớ ngoài nhất thiết sẽ liên quan đến việc tính toán tất cả các key và value
với các tham số mô hình hiện tại trên mỗi bước huấn luyện. Tuy nhiên, nếu bộ nhớ ngoài không
khả vi, thì chúng ta có thể thay vào đó tái sử dụng các key và value đã được tính toán trước đó trên
các bước huấn luyện trước, điều này giảm đáng kể lượng tính toán cho bộ nhớ lớn. Với
1arXiv:2203.08913v1  [cs.LG]  16 Mar 2022

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
200M 1B 8B
Kích thước mô hình1.82.02.22.42.62.8Perplexity (arXiv Math)
Transformer
Memorizing Transformer
Hình 1: Thêm một bộ nhớ 8K token cải thiện perplexity trên các kích thước mô hình khác nhau.
kỹ thuật của chúng tôi, chúng tôi dễ dàng có thể mở rộng bộ nhớ ngoài lên đến độ dài chuỗi 131k hoặc 262k
token trên một thiết bị TPU duy nhất, trong khi duy trì thời gian bước hợp lý.
Chúng tôi cho thấy rằng perplexity của mô hình liên tục cải thiện với kích thước của bộ nhớ ngoài trên nhiều
nhiệm vụ mô hình hóa ngôn ngữ khác nhau, bao gồm C4 (chỉ tài liệu dài), kho mã Github, sách
PG-19, chứng minh hình thức trong Isabelle, và bài báo toán arXiv. Chúng tôi tiếp tục cho thấy rằng các mô hình có thể tổng quát
hóa đến kích thước bộ nhớ lớn hơn so với những gì chúng được huấn luyện: các mô hình được huấn luyện với bộ nhớ nhỏ cho thấy
lợi ích từ việc sử dụng bộ nhớ lớn hơn nhiều tại thời điểm suy luận. Cuối cùng, chúng tôi cho thấy rằng các mô hình của chúng tôi thực sự
sử dụng bộ nhớ theo cách mà chúng tôi đã hy vọng, ví dụ: bằng cách tra cứu các định nghĩa của bổ đề trong một
corpus chứng minh định lý.
Sự đơn giản của các thay đổi đối với kiến trúc Transformer cho phép chúng tôi dễ dàng tích hợp phương pháp này
vào các codebase hiện có, bao gồm các mô hình ngôn ngữ cực kỳ lớn. Chúng tôi tiếp tục cho thấy rằng
các cải thiện về chất lượng được duy trì trên các mô hình có kích thước ngày càng tăng, và rằng các cải thiện mô hình
thu được từ việc thêm bộ nhớ thậm chí còn lớn hơn việc tăng kích thước mô hình lên
5X hoặc hơn như được hiển thị trong Hình 1.
2 NGHIÊN CỨU LIÊN QUAN
Rất nhiều công việc đã được thực hiện về các cơ chế attention tầm xa hiệu quả; xem các khảo sát gần đây của Tay et al. (2020;
2021). Sliding windows (Beltagy et al., 2020) sử dụng một chuỗi dài, nhưng chú ý trong
một cửa sổ nhỏ hơn, do đó giảm độ phức tạp xuống kích thước cửa sổ, thay vì tổng độ dài chuỗi.
Các cơ chế gần đúng như Linformer (Wang et al., 2020b), và Performer (Choromanski
et al., 2021) tái cấu trúc ma trận attention bằng cách sử dụng một kernel khác softmax để đạt được độ phức tạp O(N).
Các chiến lược pooling như Hierarchical 1D attention (Zhu & Soricut, 2021), và Combiner
(Ren et al., 2021) áp dụng pooling hoặc tính trung bình trên các token ở khoảng cách xa hơn. Các chiến lược thưa thớt như
Big Bird (Zaheer et al., 2020) chỉ chọn một tập con các token để chú ý; Routing Transformers (Roy
et al., 2021) sử dụng clustering để chọn tập con, trong khi Reformer (Kitaev et al., 2020) dựa vào hashing.
Các cơ chế phân cấp (Ainslie et al., 2020) kết hợp nhiều token thành cụm từ hoặc câu để
giảm độ dài chuỗi. Expire-span (Sukhbaatar et al., 2021) loại bỏ các token xa mà nó học được
là "không quan trọng". (Zemlyanskiy et al., 2021) xử lý các chuỗi dài trong hai lượt với các
encoder khác nhau. Lượt thứ hai được cung cấp nhiều ngữ cảnh bằng cách truy cập tóm tắt của lượt đầu tiên.
Feedback transformers (Fan et al., 2020) sử dụng một kiến trúc recurrent trong đó mỗi token chú ý đến
đầu ra của lớp cuối cùng thay vì lớp trước đó. Recurrence không tăng kích thước của
ngữ cảnh attention chính nó, nhưng nó mở rộng trường tiếp nhận với chi phí của parallelism và tốc độ huấn luyện.
Truncated backpropagation through time (Williams & Peng, 1990) ban đầu được giới thiệu như một
cách huấn luyện mạng nơ-ron hồi tiếp (RNN) trên các chuỗi rất dài, khi toàn bộ chuỗi
không vừa trong bộ nhớ. Chuỗi được cắt thành các đoạn, và sau mỗi bước huấn luyện, trạng thái
RNN cuối cùng cho đoạn được lưu trong một cache không khả vi, và được sử dụng làm trạng thái ban đầu trên
bước huấn luyện tiếp theo. Neural caches (Grave et al., 2017) mở rộng cache để chứa một bản ghi của nhiều
trạng thái ẩn trước đó, và chú ý đến chúng. Transformer-XL (Dai et al., 2019) áp dụng kỹ thuật này cho
transformers; nó cache các cặp (key,value) được tính từ bước huấn luyện trước đó, và sử dụng chúng
như một tiền tố cho các token trong bước huấn luyện tiếp theo, điều này mang lại lợi ích đáng kể trên các tài liệu dài.
Rae et al. (2020) cải thiện so với Transformer-XL bằng cách nén các token trước khi thêm chúng vào
2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
catinthehatcatinthehat thesaidlocal attention + FFNk nearest neighbor lookup. dense attention + FFN
… more layers … 
input tokensexternal memory: cached (key, value) pairs
Sẽ được thêm vào
bộ nhớ ngoài
sau bước huấn luyện
hiện tại. kNN & local attention + FFNdense attention + FFNlocal attention + FFNcatinthehatcatinthehat the to output predictions
local contextkNN attention
embedding layersoftmax
Hình 2: Chúng tôi mở rộng Transformers với khả năng truy cập các cặp (key, value) của các chuỗi con đã thấy trước đó.
cache. Ngược lại, chúng tôi sử dụng một cache rất lớn không nén, kết hợp với một cơ chế attention kNN gần đúng trên nó.
Sukhbaatar et al. (2019) đưa ra nhận xét rằng phần feed-forward của một lớp transformer
hoạt động rất giống như attention nếu ta thay thế kích hoạt ReLU bằng softmax. Họ triển khai
một attention kết hợp trên cả token từ chuỗi đầu vào và một "bộ nhớ" đã học (và khả vi).
Lample et al. (2019) khai thác nhận xét này để thay thế các lớp feed-forward (FFN)
bằng một tìm kiếm kNN nhanh trên một "bộ nhớ" lớn hơn nhiều, và đạt được lợi ích lớn về độ chính xác mô hình
mà không có chi phí tính toán đáng kể. (Chúng tôi sử dụng tìm kiếm kNN để xấp xỉ attention đến các
token trước đó, không thay thế FFN.)
Bộ nhớ ngoài không khả vi đã được sử dụng theo các cách khác nhau bởi Khandelwal et al. (2020), những người
chạy một mô hình pre-trained trên toàn bộ corpus, và xây dựng một bảng lớn các cặp (key, token). Họ
sau đó sử dụng bảng đó để thay thế lớp softmax cuối cùng cho việc chọn token trong mô hình, điều này dẫn đến
cải thiện đáng kể trong mô hình hóa ngôn ngữ. Yogatama et al. (2021) mở rộng phương pháp này bằng một
cơ chế gating và một quy trình để nén ngữ cảnh thành key cho việc truy xuất.
Có một số nghiên cứu kết hợp truy xuất với transformers. REALM (Guu et al., 2020),
MARGE (Lewis et al., 2020a), RAG (Lewis et al., 2020b), và composite memory for dialog (Fan
et al., 2021) truy xuất tài liệu từ một cơ sở kiến thức để cải thiện trả lời câu hỏi hoặc đối thoại.
Cơ sở kiến thức bao gồm các đoạn text và tĩnh và thường tách biệt với các đầu vào và
đầu ra của các mô hình. Thay vào đó, chúng tôi tập trung vào mô hình hóa ngôn ngữ sử dụng mô hình chỉ decoder, và
đề xuất một mô hình đơn giản thống nhất attention và truy xuất.
Tìm kiếm k-nearest-neighbor là một kỹ thuật mục đích chung được sử dụng cho nhiều loại machine
learning và nhiệm vụ truy xuất khác nhau, và các triển khai hiệu suất cao có sẵn cho các kiến trúc khác
nhau (Johnson et al., 2021; Guo et al., 2020). Memory-efficient Transformers (Gupta et al., 2021)
thay thế dense attention bằng một tìm kiếm kNN để tăng tốc độ và giảm sử dụng bộ nhớ.
3 PHƯƠNG PHÁP
Kiến trúc của transformer được tăng cường kNN của chúng tôi được hiển thị trong Hình 2. Phần lớn của mô hình là một
vanilla, decoder-only transformer (Vaswani et al., 2017). Text đầu vào được tokenize, và các token
được nhúng vào không gian vector. Các vector nhúng được truyền qua một chuỗi các lớp transformer,
mỗi lớp thực hiện dense self-attention, theo sau bởi một mạng feed-forward (FFN). Vì
đây là một mô hình ngôn ngữ chỉ decoder, chúng tôi sử dụng một causal attention mask và các token embedding của
lớp cuối cùng được sử dụng để dự đoán token tiếp theo.
Các tài liệu dài được chia thành các chuỗi con 512 token, và mỗi chuỗi con được sử dụng làm đầu vào
cho một bước huấn luyện. Trái với thực hành tiêu chuẩn, chúng tôi không xáo trộn các chuỗi con; thay vào đó,
mỗi tài liệu dài được đưa vào transformer một cách tuần tự, từ đầu đến cuối, như được thực hiện với
Transformer-XL (Dai et al., 2019).
3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Document A
Document B
Document C
Document D
Document E
Document F ...
Document G ...subsequence 
batch
dimension 
Hình 3: Pipeline dữ liệu của chúng tôi chia tài liệu thành các chuỗi con và đóng gói các chuỗi con thành batch.
Chúng tôi cũng sử dụng một cache kiểu Transformer-XL, chứa các key và value từ bước huấn luyện
trước đó. Khi thực hiện self-attention, các key và value được cache được prepend vào các key và value hiện tại, và chúng tôi sử dụng một sliding-window causal mask (Beltagy et al., 2020) để mỗi token có một ngữ
cảnh cục bộ bao gồm 512 token trước đó.
3.1 LỚP ATTENTION ĐƯỢC TĂNG CƯỜNG kNN
Một trong các lớp transformer gần đỉnh stack là một lớp attention được tăng cường kNN, kết hợp hai
dạng attention. Giống như tất cả các lớp khác, nó sử dụng standard dense self-attention trên
ngữ cảnh cục bộ, là chuỗi con đầu vào cho bước huấn luyện hiện tại. Tuy nhiên, không giống các lớp khác,
nó cũng thực hiện một tìm kiếm k-nearest-neighbor gần đúng vào bộ nhớ ngoài.
Các truy vấn giống nhau được sử dụng cho cả ngữ cảnh cục bộ và bộ nhớ ngoài. Các key và
value cũng thuộc cùng một phân phối; sau mỗi bước huấn luyện, các cặp (key, value) trong ngữ
cảnh cục bộ được append vào cuối bộ nhớ ngoài. Nếu tài liệu rất dài, các cặp (key, value)
cũ sẽ bị loại bỏ khỏi bộ nhớ để nhường chỗ cho các cặp mới. Do đó, cho mỗi head, bộ nhớ ngoài
giữ một cache của M cặp (key, value) trước đó, trong đó M là kích thước bộ nhớ.
Tìm kiếm kNN sẽ trả về một tập hợp các bộ nhớ được truy xuất, bao gồm các cặp (key, value) top-k
mà tìm kiếm kNN trả về cho mỗi truy vấn (tức là mỗi token) trong chuỗi con đầu vào. Như với standard
dense attention, chúng tôi đầu tiên xây dựng một ma trận attention bằng cách tính tích vô hướng của mỗi truy vấn
với các key được truy xuất, sau đó áp dụng softmax, và cuối cùng trả về một tổng trọng số của các
value được truy xuất. Không giống standard dense attention, các bộ nhớ được truy xuất chứa một tập hợp khác nhau của các cặp (key, value)
cho mỗi truy vấn.
Attention trên ngữ cảnh cục bộ được thực hiện theo cách thông thường. Kết quả của kNN-attention và local
attention sau đó được kết hợp sử dụng một cổng đã học:
g=σ(bg) (1)
Va=Vm⊙g+Vc⊙(1−g) (2)
trong đó σ là hàm sigmoid, và ⊙ là phép nhân theo từng phần tử. Va là kết quả kết hợp của
attention, Vm là kết quả của việc chú ý đến bộ nhớ ngoài, và Vc là kết quả của việc chú ý đến
ngữ cảnh cục bộ. Bias bg là một tham số vô hướng đã học cho mỗi head, cho phép mỗi head chọn
giữa local và long-range attention. Trong các thí nghiệm của chúng tôi, giá trị của cổng g không phụ thuộc
vào nội dung của token tại mỗi vị trí, mặc dù đó sẽ là một mở rộng tầm thường để triển khai.
Chúng tôi đã quan sát thấy rằng theo thời gian, hầu hết các head đã học chú ý gần như hoàn toàn đến bộ nhớ ngoài.
Position bias. Đối với dense attention trong ngữ cảnh cục bộ, chúng tôi sử dụng T5 relative position bias (Raffel
et al., 2020). Như được lưu ý bởi Dai et al. (2019), việc thêm một global position encoding vào mỗi token không
hoạt động tốt khi xử lý các tài liệu dài. Chúng tôi không sử dụng position bias cho các bộ nhớ được truy xuất.
Các thí nghiệm trên bộ dữ liệu PG19 (Sun et al., 2021) đã cho thấy rằng relative position không xuất hiện
quan trọng ở tầm xa, và T5 relative bias đặt tất cả các token tầm xa vào cùng một bucket.
Batching. Hình 3 minh họa cách nhiều tài liệu dài có độ dài khác nhau được đóng gói vào một
batch, và chia thành các chuỗi con. Mỗi chuỗi con trong batch đến từ một tài liệu khác nhau,
và do đó yêu cầu một bộ nhớ ngoài riêng biệt, được xóa ở đầu mỗi tài liệu mới.
3.2 DISTRIBUTIONAL SHIFT
Vì mỗi tài liệu dài được xử lý qua nhiều bước huấn luyện, có một distributional shift
trong các key và value được lưu trữ trong bộ nhớ ngoài. Các tham số mô hình tạo ra các
truy vấn thay đổi theo thời gian, và do đó sẽ đã thay đổi kể từ khi các key và value được lưu trữ. Đối với
bộ nhớ rất lớn, các bản ghi cũ hơn có thể trở nên "cũ". Các quan sát tương tự đã được thực hiện cho
CrossBatch memory (Wang et al., 2020c) trong lĩnh vực thị giác.
4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Để giảm tác động của tình trạng cũ, chúng tôi chuẩn hóa key và truy vấn (Henry et al., 2020). Chuẩn hóa
không loại bỏ tình trạng cũ, nhưng ít nhất đảm bảo rằng các key cũ hơn và key mới hơn không khác nhau về
độ lớn. Chúng tôi cũng phát hiện ra rằng chuẩn hóa giúp ổn định việc huấn luyện với cache Transformer-XL.
Trong một số thí nghiệm của chúng tôi, chúng tôi quan sát thấy rằng việc huấn luyện các mô hình từ đầu với bộ nhớ lớn
đôi khi dẫn đến hiệu suất tồi tệ hơn so với việc pretrain mô hình với bộ nhớ nhỏ có kích thước
8192, và sau đó tinh chỉnh nó trên bộ nhớ lớn hơn. Sự bất ổn định huấn luyện này có thể do tình trạng cũ.
Tuy nhiên, các mô hình dường như có thể đối phó với một mức độ cũ hạn chế (với bộ nhớ nhỏ)
bằng cách điều chỉnh các truy vấn của chúng cho phù hợp.
3.3 kNN GẦN ĐÚNG
Chúng tôi sử dụng tìm kiếm kNN gần đúng thay vì tìm kiếm kNN chính xác vì nó cải thiện đáng kể
tốc độ tính toán của mô hình của chúng tôi. Chúng tôi sử dụng một xấp xỉ đơn giản của kNN cho TPU, có
recall khoảng 90%, tức là 90% của top k thực sự được trả về trong top k gần đúng. Có
nhiều thuật toán kNN gần đúng hiệu quả khác có sẵn cho CPU và GPU/TPU, ví dụ
thông qua Faiss (Johnson et al., 2021) hoặc ScaNN (Guo et al., 2020), có thể mở rộng đến hàng tỷ.
4 THÍ NGHIỆM
Chúng tôi đánh giá tác động của việc thêm bộ nhớ ngoài trên năm nhiệm vụ mô hình hóa ngôn ngữ, tất cả đều
liên quan đến text dài: sách tiếng Anh (PG-19), bài viết web dài (C4), bài báo toán kỹ thuật
(arXiv Math), mã nguồn (Github), và định lý hình thức (Isabelle). Kết quả cho thấy
cải thiện đáng kể trong perplexity của mô hình với việc bổ sung bộ nhớ ngoài. Chúng tôi
thí nghiệm với nhiều kích thước khác nhau của bộ nhớ ngoài, từ 1536 đến cao nhất 262K. Trên hầu hết các
bộ dữ liệu, có một lợi ích ban đầu mạnh từ việc thêm một bộ nhớ ngoài nhỏ, tiếp theo là các lợi ích nhỏ hơn
nhưng tăng đều đặn khi kích thước bộ nhớ được tăng lên.
4.1 BỘ DỮ LIỆU
arXiv Math Đối với bộ dữ liệu arXiv, chúng tôi thu thập một corpus các bài báo bằng cách tải chúng qua
arXiv Bulk Data Access1. Chúng tôi lọc các bài báo để chỉ bao gồm các bài viết được gắn nhãn là "Mathematics"
và có mã nguồn LATEX có sẵn. Số lượng token mỗi bài báo trong bộ dữ liệu này tương đương
so với số lượng token mỗi cuốn sách trong PG19, vì mã nguồn LATEX có nhiều ký tự đặc biệt
và tokenizer có xu hướng đầu ra các subword nhỏ.
Github Chúng tôi sử dụng BigQuery2 để có được một corpus lớn của các kho Github được xuất bản với
giấy phép mã nguồn mở. Chúng tôi sử dụng đuôi file để lọc các file trong các ngôn ngữ C, C++, Java, Python
(bao gồm Jupyter notebook), Go, và TypeScript. Các file mã nguồn riêng lẻ thường khá ngắn,
và có nhiều dependency và cross-reference giữa các file trong kho. Để nắm bắt
các dependency này, chúng tôi tạo một tài liệu dài cho mỗi kho Github bằng cách duyệt
cây thư mục, và nối tất cả các file trong đó. Thứ tự mà các file được duyệt trong
kho là ngẫu nhiên, nhưng mỗi thư mục con được xử lý như một đơn vị, sao cho tất cả các file trong
thư mục con gần nhau trong tài liệu kết quả. Mã nguồn thường được cấu trúc
sao cho các file liên quan đều được nhóm lại với nhau trong cùng một thư mục con; việc duyệt này bảo tồn
cấu trúc đó, trong khi vẫn xáo trộn các file và thư mục con theo thứ tự ngẫu nhiên.
Formal Math – Isabelle Corpus Isabelle bao gồm các chứng minh toán học hình thức của các lý thuyết.
Chúng tôi thu thập tất cả 627 lý thuyết có sẵn trên The Archive of Formal Proofs3 (tính đến ngày 6 tháng 10 năm 2021) và
thêm 57 lý thuyết từ thư viện chuẩn Isabelle4 để tạo một corpus 684 lý thuyết. Tất cả
lý thuyết đều có giấy phép mã nguồn mở. Mỗi lý thuyết là một đối tượng toán học khép kín, về các chủ đề
như logic nền tảng, phân tích nâng cao, đại số, hoặc mật mã học, và bao gồm nhiều
file chứa chứng minh. Như với corpus Github, tất cả các file tạo nên một lý thuyết được nối
1https://arxiv.org/help/bulk_data
2https://console.cloud.google.com/marketplace/product/github/github-repos
3https://www.isa-afp.org/topics.html
4https://isabelle.in.tum.de/
5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Context Memory XL cache arXiv PG19 C4(4K +) GitHub Isabelle
512 None None 3.29 13.71 17.20 3.05 3.09
2048 None None 2.69 12.37 14.81 2.22 2.39
512 None 512 2.67 12.34 15.38 2.26 2.46
2048 None 2048 2.42 11.88 14.03 2.10 2.16
512 1536 None 2.61 12.50 14.97 2.20 2.33
512 8192 None 2.49 12.29 14.42 2.09 2.19
512 8192 512 2.37 11.93 14.04 2.03 2.08
512 65K 512 2.31 11.62 14.04 1.87 2.06
2048 8192 2048 2.33 11.84 13.80 1.98 2.06
2048 65K 2048 2.26 11.37 13.64 1.80 1.99
Bảng 4: Perplexity trung bình ở mức token của mỗi mô hình khi được huấn luyện trong 500k bước.
lại với nhau thành một tài liệu dài. Không giống corpus Github, chúng tôi sắp xếp các file theo
dependency import của chúng, sao cho các file sau sử dụng các sub-theorem được chứng minh trong các file trước.
C4(4K +) C4, colossal cleaned common crawl, là một bộ sưu tập rất lớn các tài liệu đã được
scrape từ internet (Raffel et al., 2020). Chúng tôi lọc bỏ tất cả các tài liệu có ít hơn
4096 token để tập trung vào các tài liệu mà bộ nhớ có thể có tác động.
PG-19 PG-19 là một bộ dữ liệu lớn của sách tiếng Anh, được xuất bản trước năm 1919, được
truy xuất từ kho lưu trữ Project Gutenberg (Rae et al., 2020; Sun et al., 2021). PG-19 là một trong
số ít bộ dữ liệu công khai chỉ chứa sách đầy đủ, và đã trở thành một benchmark cho mô hình hóa
text ngôn ngữ tự nhiên tầm xa.
4.2 PHƯƠNG PHÁP THÍ NGHIỆM
Chúng tôi sử dụng một transformer 12 lớp chỉ decoder (có và không có cache Transformer-XL) với kích thước
embedding 1024, 8 attention head có chiều 128, và một lớp ẩn FFN có kích thước 4096.
Đối với tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng k = 32. Trừ khi được chỉ định khác, chúng tôi sử dụng lớp thứ 9 làm
lớp attention được tăng cường kNN. Chúng tôi sử dụng một tokenizer sentence-piece (Kudo & Richardson, 2018)
với kích thước từ vựng 32K.
Chúng tôi sử dụng optimizer Adafactor (Shazeer & Stern, 2018). Trong các thí nghiệm sơ bộ, chúng tôi tiến hành
một tìm kiếm hyperparameter để xác định learning rate tối ưu trong ba lựa chọn ({3×10^−4, 1×10^−3,
3×10^−3}), và phát hiện ra rằng 1×10^−3 hoạt động tốt nhất. Chúng tôi sử dụng một lịch trình linear warmup cho
1000 bước đầu tiên, tiếp theo bởi square root decay. Chúng tôi huấn luyện các mô hình từ đầu trong 500K bước trên tất cả các
bộ dữ liệu, ngoại trừ bộ dữ liệu Isabelle. Isabelle nhỏ, vì vậy chúng tôi dừng huấn luyện sau 100K bước
khi mô hình bắt đầu overfit. Chúng tôi chạy tất cả các thí nghiệm của mình trên 32 TPU core. Các mô hình của chúng tôi được
triển khai trong JAX (Bradbury et al., 2018) và Flax (Heek et al., 2020).
Khi so sánh các mô hình với độ dài ngữ cảnh khác nhau, chúng tôi điều chỉnh batch size (số lượng
tài liệu trong một batch) sao cho luôn có 2^17 token trong một batch. Ví dụ, một mô hình với độ dài ngữ
cảnh 512 có batch size 256, trong khi mô hình 2048 có batch size 64.
Chúng tôi thí nghiệm với nhiều triển khai khác nhau của tìm kiếm kNN gần đúng với các tradeoff khác nhau
giữa chất lượng và chi phí tính toán. Chúng tôi không quan sát thấy sự suy giảm đáng kể về chất lượng mô hình
khi chuyển sang các xấp xỉ chất lượng thấp hơn của kNN, vì vậy mô hình dường như khá
robust đối với chất lượng của việc truy xuất kNN. Đối với một mô hình với khoảng 200M tham số có thể huấn luyện
thời gian bước tăng từ 0.2s lên 0.25s khi chúng tôi thêm một bộ nhớ có kích thước 8K, và lên
0.6s khi chúng tôi thêm một bộ nhớ có kích thước 65K (đo trên TPUv3).
4.3 TÁC ĐỘNG CỦA BỘ NHỚ NGOÀI
Việc thêm bộ nhớ ngoài dẫn đến lợi ích đáng kể trên các bộ dữ liệu và kiến trúc,
như được hiển thị trong Bảng 4. Trên tất cả năm bộ dữ liệu, việc thêm bộ nhớ ngoài vào kiến trúc vanilla Transformer
hoặc Transformer-XL đều cải thiện perplexity một lượng đáng kể. Ví dụ, trên
6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Context Pretrain Fine-tune Perplexity
512 8192 None 2.37
512 65K None 2.31
512 8192 65K 2.32
512 8192 131K 2.30
512 8192 262K 2.26
2048 8192 None 2.33
2048 65K None 2.26
2048 65K 131K 2.23
2048 65K 262K 2.21
Bảng 5: Tinh chỉnh trong 20K bước để sử dụng bộ nhớ lớn hơn trên bộ dữ liệu arXiv.
bộ dữ liệu C4(4K+), việc thêm bộ nhớ có kích thước 8192 cải thiện perplexity của vanilla Transformer
(với kích thước ngữ cảnh 512) từ 17.20 xuống 14.42, và cải thiện Transformer-XL từ 15.38 xuống 14.04.
Tăng kích thước bộ nhớ làm tăng lợi ích của bộ nhớ. Perplexity tốt nhất
cho tất cả các bộ dữ liệu và kiến trúc được đạt được với kích thước bộ nhớ 65K.
Lưu ý rằng Transformer-XL với kích thước ngữ cảnh 2048 đã có một trường tiếp nhận lý thuyết khá
lớn. Mỗi token trong một lớp cao hơn có thể chú ý đến 2048 token xa trong lớp dưới, vì vậy tổng
trường tiếp nhận là 2048×12(lớp)≈25K. Tuy nhiên, chúng tôi vẫn thấy một lợi ích đáng kể khi thêm
một bộ nhớ ngoài có kích thước 8192 vào mô hình này. kNN attention vào bộ nhớ dường như là một
cách hiệu quả hơn để truy xuất thông tin từ quá khứ xa so với cache Transformer-XL.
Mặt khác, chúng tôi cũng thấy cải thiện bằng cách thêm cache XL vào các mô hình bộ nhớ lớn (65K).
Trong một vanilla (non-XL) Transformer, một vài token đầu tiên trong một chuỗi có rất ít ngữ cảnh, và
do đó có perplexity cao hơn. Cache XL cung cấp ngữ cảnh cục bộ tầm ngắn bổ sung ở đầu
một chuỗi, bổ sung cho ngữ cảnh tầm xa được cung cấp bởi bộ nhớ ngoài.
Thú vị thay, trong một vanilla Transformer, việc sử dụng ngay cả một bộ nhớ ngoài nhỏ có kích thước 1536 cung cấp
một lợi ích về perplexity gần như tốt bằng việc sử dụng một ngữ cảnh cục bộ có kích thước 2048 nhưng không có bộ nhớ
(ví dụ: Bảng 4). Điều này đáng ngạc nhiên, vì bộ nhớ ngoài không khả vi, và chỉ được thêm vào
một lớp của Transformer, trong khi việc tăng kích thước ngữ cảnh là khả vi và ảnh hưởng đến tất cả các lớp. Chúng tôi kết luận rằng các lớp thấp hơn của một Transformer không nhất thiết cần ngữ cảnh tầm xa,
và có một bộ nhớ khả vi không quan trọng như người ta có thể nghi ngờ.
4.4 MỞ RỘNG ĐẾN CÁC MÔ HÌNH LỚN HƠN
Chúng tôi mở rộng mô hình Transformer lên kích thước 1 và 8 tỷ tham số. Đối với mô hình 1 tỷ tham số,
chúng tôi sử dụng 8 lớp, 32 head với chiều head 128, d_model 2048, và d_ff 16384. Đối với
mô hình 8 tỷ tham số, chúng tôi sử dụng 64 head, 16 lớp, d_model 4096, và d_ff 32768. Chúng tôi sử dụng
kích thước ngữ cảnh 2048, kích thước bộ nhớ 8192, và không có cache XL. Chúng tôi chạy các so sánh với vanilla
Transformer trên bộ dữ liệu arXiv math. Biểu đồ mở rộng được hiển thị trong Hình 1.
Bộ nhớ ngoài cung cấp một cải thiện nhất quán cho mô hình khi nó được mở rộng. Đáng chú ý,
chúng tôi phát hiện ra rằng Memorizing Transformer nhỏ hơn chỉ với 8k token trong bộ nhớ có thể bằng
perplexity của một vanilla Transformer lớn hơn có gấp 5X tham số có thể huấn luyện.
4.5 TINH CHỈNH TRÊN BỘ NHỚ LỚN HƠN
Tinh chỉnh trên bộ nhớ lớn hơn. Trong một số trường hợp, việc huấn luyện không ổn định khi sử dụng bộ nhớ lớn,
có thể do distributional shift sớm trong quá trình huấn luyện (Xem Phần 3.2). Do đó, đối với bộ nhớ
131K hoặc nhiều token hơn, chúng tôi đầu tiên pretrain mô hình với kích thước bộ nhớ 8192 hoặc 65K trong 500K bước,
và sau đó tinh chỉnh nó với bộ nhớ lớn hơn trong thêm 20K bước. Kết quả của việc tinh chỉnh trên
bộ dữ liệu arXiv Math được hiển thị trong Bảng 5. Tăng kích thước bộ nhớ ngoài cung cấp
lợi ích nhất quán lên đến kích thước 262K. Lưu ý rằng 262K token dài hơn hầu hết tất cả các
tài liệu trong arXiv, và do đó chúng tôi sẽ không mong đợi thấy bất kỳ lợi ích nào vượt quá điểm này (xem Phụ lục A).
7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
0 300K 500K 600K
Bước huấn luyện2.22.42.62.8Perplexity (arXiv Math)Memory Fine-tuning
Transformer
Memorizing Transformer
Hình 6: Tinh chỉnh một mô hình vanilla Transformer 1B để sử dụng bộ nhớ ngoài có kích thước 65K.
Tinh chỉnh một mô hình không có bộ nhớ để sử dụng bộ nhớ Pretraining có thể rất tốn kém cả về thời gian
và tài nguyên tính toán. Do đó, một câu hỏi tự nhiên cần hỏi là: có thể tinh chỉnh một Transformer pre-trained
để sử dụng bộ nhớ ngoài không? Câu trả lời là có!
Chúng tôi lấy một mô hình vanilla Transformer 1B pre-trained, và tinh chỉnh nó để sử dụng bộ nhớ ngoài (các
mô hình 1B được sử dụng trong Phần 4.4). Kết quả tinh chỉnh được hiển thị trong Hình 6. Chú ý rằng mô hình
nhanh chóng học cách sử dụng bộ nhớ ngoài. Trong vòng 20K bước (≈4% thời gian pre-training) mô hình đã tinh chỉnh
đã đóng được 85% khoảng cách giữa nó và Memorizing Transformer 1B, và sau
100k bước nó đã đóng hoàn toàn khoảng cách.
4.6 CÁC MẪU TRUY XUẤT THÔNG TIN
Chúng tôi tiến hành một nghiên cứu định tính về những gì mô hình thực sự truy xuất từ bộ nhớ ngoài,
bằng cách tìm những token cho thấy cải thiện lớn nhất trong cross-entropy loss khi kích thước
bộ nhớ được tăng lên, và sau đó kiểm tra các bộ nhớ được truy xuất top-k cho những token đó. Chúng tôi
phát hiện ra rằng mô hình thu được lợi ích nhiều nhất khi tra cứu các từ hiếm, chẳng hạn như tên riêng, tham chiếu,
trích dẫn, và tên hàm, trong đó việc sử dụng đầu tiên của một tên quá xa so với các lần sử dụng tiếp theo để
vừa trong ngữ cảnh cục bộ. Kết quả này phù hợp với phân tích trước đó của các Transformer ngữ cảnh dài
trên PG19 (Sun et al., 2021), đã tìm thấy các mẫu lookup tương tự. Đối với thí nghiệm này, chúng tôi sử dụng một
phiên bản cũ hơn một chút của kiến trúc không có cơ chế gating.
Những token nào cho thấy lợi ích từ bộ nhớ? Hình 7 hiển thị một hình ảnh hóa về những token nào
cho thấy cải thiện khi kích thước bộ nhớ ngoài được tăng lên. Chúng tôi chọn một bài báo toán ngẫu nhiên, và vẽ
biểu đồ sự khác biệt trong cross entropy loss cho mỗi token xi trong bài báo, so sánh hai
mô hình với cùng tham số, nhưng với bộ nhớ có kích thước khác nhau. Δi=cross-entropy8192(xi)−
cross-entropy32K(xi). Giá trị dương cho thấy một cải thiện trong loss.
Trục x trên biểu đồ là số token i, trong khi trục y là Δi. Đối với 8192 token đầu tiên, sự
khác biệt giữa hai mô hình là zero, vì dung lượng lớn hơn của bộ nhớ 32K chưa được
sử dụng. Tuy nhiên, sau token 8193, chúng ta có thể thấy rằng bộ nhớ lớn hơn giúp ích, trung bình, so với
bộ nhớ nhỏ hơn. Lợi ích không phổ quát, vì dự đoán cho một số token trở nên tồi tệ hơn,
có thể do thực tế là một bộ nhớ được truy xuất có liên quan không còn có trong top-k khi
kích thước bộ nhớ ngoài được tăng lên. Hình này cũng cho thấy rằng lợi ích của bộ nhớ ngoài
có phần thưa thớt. Cải thiện trong perplexity dường như chủ yếu được thúc đẩy bởi một tỷ lệ nhỏ các token có được
một cải thiện lớn trong cross-entropy loss khi sử dụng bộ nhớ lớn hơn.
Thông tin gì đang được tra cứu? Với việc chỉ một tập con các token cho thấy cải thiện
từ bộ nhớ ngoài, chúng tôi đã tiến hành một điều tra sâu hơn về những gì, chính xác, những token đó đang sử dụng
bộ nhớ để làm. Chúng tôi lấy những token cho thấy cải thiện lớn nhất trong cross-entropy loss, và
đối với mỗi token trong số chúng, chúng tôi kiểm tra các bộ nhớ được truy xuất top-k. Chúng tôi nghiên cứu arXiv math, Github
và corpus Isabelle. Đối với arXiv math và Github, chúng tôi phát hiện ra mô hình truy xuất tên hàm và tên biến.
Xem thêm chi tiết với các ví dụ trong Phụ lục B.
8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Hình 7: Sự khác biệt trong loss cho mỗi token trong một bài báo được chọn ngẫu nhiên, sử dụng cùng một mô hình một lần
với kích thước bộ nhớ 8K và một lần với 32K. Số lượng cao hơn có nghĩa là bộ nhớ dài hơn đã giúp ích so
với bộ nhớ ngắn hơn. Bài báo này dài 22K token.
Query index Input Target Surrounding context Retrieved index Retrieved surrounding context
29721 mark ov rule prob_space. markov_inequality 8088 M. tn<le> X a}n<le> expectation X =t"
40919 _ th = ( subgraph_threshold H n / p n) 27219 threshold H n = n powr (-(1 / max_density'
49699 S w assumes " orthonormal_system S w" 28050 deﬁnition orthonormal_system :: "
Bảng 8: Các ví dụ về truy xuất bộ nhớ trong bộ dữ liệu Isabelle. Mô hình có thể tìm định nghĩa của
một bổ đề từ một tham chiếu đến nó. Ngữ cảnh xung quanh được truy xuất (được đánh dấu) là phần thân định nghĩa của
đối tượng toán học được đánh dấu trong ngữ cảnh truy vấn.
Truy xuất định nghĩa toán học. Nghiên cứu trường hợp của chúng tôi trên corpus Isabelle cung cấp một trong những
minh họa rõ ràng nhất về cách một mô hình có thể sử dụng tốt bộ nhớ ngoài. Khi dự đoán tên
của một đối tượng toán học hoặc một bổ đề, mô hình đã tra cứu định nghĩa từ phần trước trong
chứng minh. Các ví dụ về hành vi này được hiển thị trong Bảng 8. Trong ví dụ 1, mô hình truy xuất một định nghĩa
trong phần thân của một bổ đề, markov_inequality. Trong ví dụ 2, nó truy xuất định nghĩa của một
khái niệm đã được định nghĩa trước đó subgraph_threshold. Trong ví dụ 3, nó truy xuất định nghĩa của
orthonormal_system. Chúng tôi kiểm tra thủ công 10 ví dụ mà mô hình đã đưa ra dự đoán
về tên bổ đề, và 8 trong 10 lần mô hình đã tìm thấy phần thân của bổ đề mà nó cần dự đoán. Trong
hai trường hợp khác, mô hình cũng tra cứu tài liệu trong vùng lân cận ngay lập tức. Theo hiểu biết tốt nhất của
chúng tôi, đây là minh chứng đầu tiên rằng attention có khả năng tra cứu định nghĩa và
phần thân hàm từ một corpus lớn. Nghiên cứu trường hợp Isabelle đã sử dụng một mô hình với hai lớp bộ nhớ
có kích thước 32K.
5 KẾT LUẬN
Chúng tôi trình bày một mở rộng đơn giản cho kiến trúc Transformer, được gọi là kNN-augmented attention,
làm tăng đáng kể độ dài của ngữ cảnh mà một mô hình ngôn ngữ có thể chú ý đến bằng cách sử dụng
k-nearest-neighbor lookup vào một bộ nhớ ngoài lớn. Chúng tôi chứng minh hiệu quả của bộ nhớ ngoài
trong một loạt các thí nghiệm mô hình hóa ngôn ngữ trên nhiều bộ dữ liệu tài liệu dài khác nhau,
bao gồm tài liệu LaTeX, mã nguồn, chứng minh hình thức, và sách.
Memorizing Transformer cho thấy cải thiện lớn trong perplexity so với baseline cho tất cả
các bộ dữ liệu và kiến trúc mà chúng tôi nghiên cứu; nó có thể so sánh với một vanilla transformer có
gấp 5 lần số lượng tham số. Perplexity tiếp tục cải thiện với kích thước bộ nhớ tăng,
mặc dù có một điểm lợi ích giảm dần. Hơn nữa, bộ nhớ ngoài tiếp tục cung cấp
lợi ích ngay cả khi transformer được mở rộng từ 200M lên 8B tham số. Có lẽ hấp dẫn nhất, một
Memorizing Transformer không cần phải được pre-train từ đầu; có thể đạt được lợi ích lớn
từ việc thêm bộ nhớ vào một mô hình pre-trained hiện có, và sau đó tinh chỉnh nó.
Không giống các dạng attention khác, truy xuất kNN có thể dễ dàng được mở rộng lên kích thước bộ nhớ khổng lồ, và do đó
có khả năng tận dụng các cơ sở kiến thức hoặc kho mã nguồn rộng lớn. Cách sử dụng tốt nhất
khả năng này là một chủ đề cho nghiên cứu tương lai.
LỜI CẢM ƠN
Chúng tôi muốn cảm ơn Charles Staats cho nhiều cuộc thảo luận hữu ích và các nhận xét chi tiết, Henryk
Michalewski cho phiên bản đầu của triển khai bộ nhớ, Petros Maniatis cho sự giúp đỡ với các
bộ dữ liệu mã nguồn của chúng tôi, Aitor Lewkowycz cho sự giúp đỡ với các thí nghiệm memorizing transformer quy mô lớn hơn,
Behnam Neyshabur cho các nhận xét về việc tinh chỉnh các mô hình không có bộ nhớ, Imanol Schlag cho việc đọc lại
và các nhận xét chi tiết, và Dennis Lee và Manzil Zaheer cho các thảo luận về attention và truy xuất quy mô lớn.
9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
ĐẠO ĐỨC
Khả năng ghi nhớ các cơ sở dữ liệu lớn về sự kiện có thể có những tác động tiềm tàng đối với xã hội,
đặc biệt nếu những cơ sở dữ liệu đó bao gồm thông tin cá nhân nhạy cảm hoặc các tác phẩm có bản quyền. Tuy nhiên,
một lợi thế của việc sử dụng bộ nhớ ngoài là bộ nhớ có thể dễ dàng được xóa sạch tất cả thông tin như vậy,
như chúng tôi làm ở cuối mỗi tài liệu mà chúng tôi huấn luyện. Điều tương tự không đúng với
các tham số mô hình khả vi, là những gì hầu hết các kiến trúc hiện tại sử dụng để lưu trữ sự kiện và
thông tin mà chúng được huấn luyện.
TÍNH TÁI SẢN
Chi tiết về kiến trúc và hyperparameter huấn luyện của chúng tôi được đưa ra trong Phần 4.2. Các bộ dữ liệu
cho C4 và PG-19 có sẵn công khai. Các bộ dữ liệu bổ sung của chúng tôi, Github, Isabelle, và ArXiv
Math được dẫn xuất từ các bucket dữ liệu có sẵn công khai, mà chúng tôi liên kết trong phần chính của bài báo.
Phần phụ 4.1 bao gồm chi tiết về cách chúng tôi xây dựng các bộ dữ liệu từ những bộ dữ liệu đó. Chúng tôi dự định
phát hành mã nguồn của mình như mã nguồn mở.
TÀI LIỆU THAM KHẢO
Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang, và Li Yang. ETC: encoding long and structured
inputs in transformers. Trong EMNLP, 2020.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, và Charles Sutton. Program synthesis
with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/
2108.07732.
Iz Beltagy, Matthew E. Peters, và Arman Cohan. Longformer: The long-document transformer.
CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, và
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, và Dario Amodei. Language models are few-shot learners. Trong NeurIPS, 2020.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,
Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios
Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,
Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob
McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. Evaluating
large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.
org/abs/2107.03374.
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Ben-
jamin Belanger, Lucy J. Colwell, và Adrian Weller. Rethinking attention with performers. Trong
ICLR, 2021.
10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, và John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168,
2021. URL https://arxiv.org/abs/2110.14168.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, và Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a fixed-length context. Trong ACL, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. Trong ACL, 2019.
Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, và Sainbayar Sukhbaatar. Addressing
some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020.
Angela Fan, Claire Gardent, Chloé Braud, và Antoine Bordes. Augmenting transformers with
KNN-based composite memory for dialog. Transactions of the Association for Computational
Linguistics, 9:82–99, 2021.
Edouard Grave, Armand Joulin, và Nicolas Usunier. Improving neural language models with a
continuous cache. Trong ICLR, 2017.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, và Sanjiv Kumar.
Accelerating large-scale inference with anisotropic vector quantization. Trong ICML, 2020.
Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, và Jonathan Berant. Memory-efficient
transformers via top-k attention. CoRR, abs/2106.06899, 2021. URL https://arxiv.org/
abs/2106.06899.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Ming-Wei Chang. Retrieval augmented
language model pre-training. Trong ICML, 2020.
Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, và Bernd Finkbeiner.
Teaching temporal logics to neural networks. Trong ICLR, 2021.
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, và Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
http://github.com/google/flax.
Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, và Yuxuan Chen. Query-key
normalization for transformers. Trong EMNLP, 2020.
Jeff Johnson, Matthijs Douze, và Hervé Jégou. Billion-scale similarity search with GPUs. IEEE
Transactions on Big Data, 2021.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, và Mike Lewis. Generalization
through memorization: Nearest neighbor language models. Trong ICLR, 2020.
Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. Trong ICLR,
2020.
Taku Kudo và John Richardson. Sentencepiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing. Trong EMNLP, 2018.
Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, và Hervé
Jégou. Large memory layers with product keys. Trong NeurIPS, 2019.
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, và Luke
Zettlemoyer. Pre-training via paraphrasing. Trong NeurIPS, 2020a.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-
tion for knowledge-intensive NLP tasks. Trong NeurIPS, 2020b.
Wenda Li, Lei Yu, Yuhuai Wu, và Lawrence C. Paulson. Isarstep: a benchmark for high-level
mathematical reasoning. Trong ICLR, 2021.
11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien
de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven
Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,
Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, và Oriol Vinyals. Competition-level
code generation with alphacode. DeepMind, 2022.
Stanislas Polu và Ilya Sutskever. Generative language modeling for automated theorem proving.
CoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.
Markus Norman Rabe, Dennis Lee, Kshitij Bansal, và Christian Szegedy. Mathematical reasoning
via self-supervised skip-tree training. Trong ICLR, 2021.
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, và Timothy P Lillicrap.
Compressive transformers for long-range sequence modelling. Trong ICLR, 2020.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research, 2020.
Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, và Bo Dai.
Combiner: Full attention transformer with sparse computation cost. CoRR, abs/2107.05768, 2021.
URL https://arxiv.org/abs/2107.05768.
Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. Efficient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguistics,
9:53–68, 2021.
Noam Shazeer và Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
Trong ICML, 2018.
Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, và Armand Joulin. Aug-
menting self-attention with persistent memory. arXiv preprint arXiv:1907.01470, 2019.
Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, và Angela
Fan. Not all memories are created equal: Learning to forget by expiring. Trong ICML, 2021.
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, và Mohit Iyyer. Do long-range language
models actually use long-range context? Trong EMNLP, 2021.
Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. Efficient transformers: A survey. arXiv
preprint arXiv:2009.06732, 2020.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for efficient
transformers. Trong ICLR, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, và Illia Polosukhin. Attention is all you need. Trong NeurIPS, 2017.
Qingxiang Wang, Chad Brown, Cezary Kaliszyk, và Josef Urban. Exploration of neural machine
translation in autoformalization of mathematics in mizar. Trong International Conference on Certified
Programs and Proofs, 2020a.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020b.
Xun Wang, Haozhi Zhang, Weilin Huang, và Matthew R. Scott. Cross-batch memory for embedding
learning. Trong CVPR, 2020c.
Ronald J. Williams và Jing Peng. An efficient gradient-based algorithm for on-line training of
recurrent network trajectories. Neural Computation, 1990.
Dani Yogatama, Cyprien de Masson d'Autume, và Lingpeng Kong. Adaptive semiparametric
language models. ACL, 9:362–373, 2021.
12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. Big bird:
Transformers for longer sequences. Trong NeurIPS, 2020.
Yury Zemlyanskiy, Joshua Ainslie, Michiel de Jong, Philip Pham, Ilya Eckstein, và Fei Sha.
Readtwice: Reading very large documents with memories. Trong ACL: Human Language Technologies,
2021.
Zhenhai Zhu và Radu Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for
sequences. Trong ACL, 2021.
13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
A ĐỘ DÀI CỦA CÁC ĐẦU VÀO
Hình 9: Biểu đồ tần suất số lượng token trong bộ dữ liệu bài báo toán arXiv. Chúng tôi đã cắt biểu đồ
ở 500k token. Bài báo dài nhất có gần 1.6M token.
Hình 10: Biểu đồ tần suất số lượng token trong bộ dữ liệu kho Github. Chúng tôi đã cắt phần đuôi dài
của biểu đồ này. Kho có độ dài tối đa có hơn 9M token.
Hình 11: Biểu đồ tần suất số lượng token trong bộ dữ liệu script chứng minh Isabelle.
Hình 12: Biểu đồ tần suất số lượng token trong bộ dữ liệu sách PG19.
Hình 13: Biểu đồ tần suất số lượng token trong tài liệu C4 được lọc bởi các tài liệu có ít hơn
4096 token.
14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
A.1 CÁC NGHIÊN CỨU ABLATION
Trong phần sau, chúng tôi đã thực hiện các nghiên cứu ablation để điều tra tác động của các
hyperparameter khác nhau. Trừ khi được chỉ định khác, chúng tôi thực hiện các thí nghiệm này với một memorizing
transformer với kích thước ngữ cảnh 512, cache XL 512 với kích thước bộ nhớ 8192.
Nhiều lớp kNN. Chúng tôi thí nghiệm với việc sử dụng hai lớp kNN, thay vì chỉ một. Tuy nhiên,
chúng tôi không thấy thêm lợi ích từ nhiều hơn nhiều lớp truy xuất.
Chỉ số lớp kNN Chúng tôi thí nghiệm với việc thêm bộ nhớ ngoài vào lớp 3,6,9 và 12 trong một
transformer 12 lớp, với kết quả được hiển thị trong Bảng 14. Chúng tôi phát hiện ra rằng việc thêm bộ nhớ vào giữa
stack lớp sẽ có được kết quả tốt nhất, trong khi việc thêm bộ nhớ vào các lớp quá gần
đầu vào hoặc quá gần đầu ra đạt được ít lợi ích hơn.
Bảng 14: Chỉ số lớp khác nhau.
Chỉ số lớp Perplexity
3 2.40
6 2.36
9 2.37
12 2.43
Số lượng neighbor Chúng tôi nghiên cứu tác động của số lượng neighbor chúng tôi truy xuất từ bộ nhớ,
với kết quả được hiển thị trong Bảng 15. Chúng tôi phát hiện ra rằng ngay cả với 32 số lượng neighbor, chúng tôi đã có thể
đạt được kết quả tương đương với 128 hoặc 256 neighbor.
Bảng 15: Số lượng neighbor.
Số lượng neighbor Perplexity
32 2.38
128 2.37
256 2.37
Random seed Chúng tôi đo ý nghĩa thống kê của các kết quả được báo cáo. Chúng tôi đã thực hiện 3 lần chạy với
3 random seed cho Transformer XL có kích thước 512, và cũng một memorizing transformer với kích thước bộ nhớ
8192. Chúng tôi đo độ lệch chuẩn của perplexity sau 500K bước huấn luyện, được hiển thị trong
Bảng 16. Chúng tôi thấy độ lệch chuẩn giữa các lần chạy khác nhau của cùng một thí nghiệm dường như
nhỏ hơn nhiều so với khoảng cách giữa các mô hình khác nhau.
Bảng 16: Random seed.
Mô hình Perplexity
Transformer XL 2.67±0.01
Memorizing Transformer 2.37±0.005
15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
B MÔ HÌNH TRUY XUẤT GÌ TỪ BỘ NHỚ?
Truy xuất tên trích dẫn Trên arXiv math, một số ví dụ được hiển thị trong Bảng 17, bao gồm
cả token được truy xuất và ngữ cảnh xung quanh của nó. Chúng tôi quan sát thấy rằng nhiều lợi ích trong cross-
entropy loss diễn ra khi cố gắng dự đoán tên của bibitem, trích dẫn, hoặc tham chiếu, bằng cách
tra cứu các tham chiếu và trích dẫn được sử dụng trước đó trong bài báo. Những tra cứu như vậy thường kéo dài qua
toàn bộ bài báo, dài hơn nhiều so với 8192 token, cung cấp một giải thích hợp lý cho
lợi ích vượt ra ngoài kích thước bộ nhớ 8192.
Bảng 17: Bảng hiển thị một số ví dụ về những token nào được truy xuất trong quá trình mô hình hóa ngôn ngữ của
bộ dữ liệu arXiv math. Mô hình đang truy xuất tên của các tham chiếu từ các đoạn trước đó.
Query index Input Target Surrounding context Retrieved index Retrieved surrounding context
20389 Mon thus bibitem{ ComtetMonthusYor } 2208 Brownian motion ncite{ ComtetMonthusYor }
16623 cha kra ncite{ chakrabarti }. 4677 1.2 ofncite{ chakrabarti }
14747 as d neqref{ asdfg } which 3365 begin{equation} nnnlabel{ asdfg .1}
Truy xuất tên hàm từ codebase Như với các bài báo arXiv, chúng tôi cũng nghiên cứu những token nào
mô hình truy xuất từ bộ nhớ. Như có thể mong đợi, mô hình thường tra cứu tên của các hàm,
và biến, như được hiển thị trong Bảng 18.
Bảng 18: Các ví dụ về truy xuất bộ nhớ trong bộ dữ liệu Github. Mô hình tra cứu cách các hàm được sử dụng
ở nơi khác trong kho.
Query index Input Target Surrounding context Retrieved index Retrieved surrounding context
23837 Fo nte menu_play-> setarFonte 14607 menu_load-> setarFonte
23825 , 35 hscreen/2-50, 50, 200, 35 ); 14599 20, y+40, 200, 35 )
14546 -> adi panel-> adicionaComponente 5205 panel-> adicionaComponente
16

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
B.1 THÊM CÁC VÍ DỤ TRUY XUẤT TRONG CORPUS CHỨNG MINH ĐỊNH LÝ HÌNH THỨC
Ví dụ 1
• Chỉ số token đầu vào: 64604
• Token đầu vào: "_"
• Token mục tiêu: "pair"
• Ngữ cảnh xung quanh: )) by (simp add: Fourier_sum_limit_pair [OF f, symmetric] Fourier'
• Tên cần được dự đoán: Fourier_sum_limit_pair
• Token được truy xuất: "Four"
• Chỉ số token được truy xuất: 64412
• Ngữ cảnh được truy xuất: 2 * n. Fourier_coefficient f k * trigonometric_set k t)
• Định nghĩa của tên:
Hình 19: Định nghĩa của Fourier_sum_limit_pair.
Ví dụ 2
• Chỉ số token đầu vào: 46175
• Token đầu vào: "tri"'
• Token mục tiêu: "gon"
• Ngữ cảnh xung quanh: <le>n. a k * trigonometric_set k x)
• Tên cần được dự đoán: orthonormal_system_trigonometric_set
• Token được truy xuất: "gon"
• Chỉ số token được truy xuất: 35457
• Ngữ cảnh được truy xuất: lemma orthonormal_system_trigonometric_set: nn "orthonormal_system
• Định nghĩa của tên:
Hình 20: Định nghĩa của orthonormal_system_trigonometric_set.
17

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Ví dụ 3
• Chỉ số token đầu vào: 49760
• Token đầu vào: "sum"'
• Token mục tiêu: "m"
• Ngữ cảnh xung quanh: nusing Fourier_series_square_summable [OF assms, of'
• Tên cần được dự đoán: Fourier_series_square_summable
• Token được truy xuất: "sum"
• Chỉ số token được truy xuất: 35457
• Ngữ cảnh được truy xuất: lemma Fourier_series_square_summable nn assumes:
• Định nghĩa của tên:
Hình 21: Định nghĩa của Fourier_series_square_summable.
Ví dụ 4
• Chỉ số token đầu vào: 49697
• Token đầu vào: "_"'
• Token mục tiêu: "system"
• Ngữ cảnh xung quanh: lemma Riemann_lebesgue_square_integrable:
nassumes "orthonormal_system S w
• Tên cần được dự đoán: orthonormal_system
• Token được truy xuất: "system"
• Chỉ số token được truy xuất: 28052
• Ngữ cảnh được truy xuất: definition orthonormal_system :: " n'a::euclidean'
• Định nghĩa của tên:
Hình 22: Định nghĩa của orthonormal_system.
18

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
Ví dụ 5
• Chỉ số token đầu vào: 34817
• Token đầu vào: "."'
• Token mục tiêu: "b"
• Ngữ cảnh xung quanh: shows "integrable (lebesgue_on {a..b})
• Token được truy xuất 1: "."
• Chỉ số token được truy xuất 1: 2416
• Ngữ cảnh được truy xuất 1: lebesgue_on {a..b}) f i
• Token được truy xuất 2: "-"
• Chỉ số token được truy xuất 2: 2445
• Ngữ cảnh được truy xuất 2: (lebesgue_on {a-c..b-c}) (
• Token được truy xuất 3: "-"
• Chỉ số token được truy xuất 3: 6479
• Ngữ cảnh được truy xuất 3: (lebesgue_on {-pi..pi}) (
Ví dụ 6
• Chỉ số token đầu vào: 49759
• Token đầu vào: "_"'
• Token mục tiêu: "sum"
• Ngữ cảnh xung quanh: 0" nn using Fourier_series_square_summable [OF assms
• Token được truy xuất 1: "set"
• Chỉ số token được truy xuất 1: 35044
• Ngữ cảnh được truy xuất 1: definition trigonometric_set :: "nat n<Rightarrow>
• Token được truy xuất 2: "ier"
• Chỉ số token được truy xuất 2: 47272
• Ngữ cảnh được truy xuất 2: definition Fourier_coefficient nnwhere
• Token được truy xuất 3: "ine"
• Chỉ số token được truy xuất 3: 18160
• Ngữ cảnh được truy xuất 3: lemma Schwartz_inequality_strong: nnassumes "f'
• Token được truy xuất 4: "system"
• Chỉ số token được truy xuất 4: 28052
• Ngữ cảnh được truy xuất 4: definition orthonormal_system :: " n'a::euclidean'
• Token được truy xuất 5: "<"
• Chỉ số token được truy xuất 5: 47241
• Ngữ cảnh được truy xuất 5: subsection n<open>Convergence wrt the L'
• Token được truy xuất 6: "n"
• Chỉ số token được truy xuất 6: 40835
• Ngữ cảnh được truy xuất 6: nn subsectionn<open>A bit of extra'
19
