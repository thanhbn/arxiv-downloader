# 2309.08646.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2309.08646.pdf
# File size: 2829131 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CoCA: Fusing Position Embedding with Collinear Constrained Attention
in Transformers for Long Context Window Extending
Shiyi Zhu, Jing Ye, Wei Jiang, Siqiao Xue, Qi Zhang, Yifan Wu, Jianguo Li
Ant Group
{zhushiyi.zsy, qianye.yj, shouzhi.jw, lijg.zero}@antgroup.com
Abstract
Self-attention and position embedding are two
key modules in transformer-based Large Lan-
guage Models (LLMs). However, the poten-
tial relationship between them is far from well
studied, especially for long context window ex-
tending. In fact, anomalous behaviors harming
long context extrapolation exist between Ro-
tary Position Embedding (RoPE) and vanilla
self-attention unveiled by our work. To address
this issue, we propose a novel attention mecha-
nism, CoCA ( Collinear Constrained Attention).
Specifically, we enforce a collinear constraint
between QandKto seamlessly integrate RoPE
and self-attention. While only adding minimal
computational and spatial complexity, this in-
tegration significantly enhances long context
window extrapolation ability. We provide an
optimized implementation, making it a drop-
in replacement for any existing transformer-
based models. Extensive experiments show
that CoCA performs extraordinarily well in
extending context windows. A CoCA-based
GPT model, trained with a context length of
512, can seamlessly extend the context win-
dow up to 32K (60 ×), without any fine-tuning.
Additionally, by dropping CoCA in LLaMA-
7B, we achieve extrapolation up to 32K within
only 2K training length. Our code is pub-
licly available at: https://github.com/codefuse-
ai/Collinear-Constrained-Attention
1 Introduction
In the seminal work of Transformer (Vaswani et al.,
2017), it claims the ability of "extrapolating to se-
quence length longer than the ones encountered
during training". This is an ideal hypothesis, but ac-
tually not work in practice for vanilla Transformer.
Several subsequent works, collectively known as
long context extrapolation, have delved into ex-
ploring the capabilities of large language models
(LLMs) trained within the range of [1, N−1]to
effectively extend the testing sequence ≥N.
Figure 1: Perplexity evaluation on 100 PG-19 documents with
a sliding window strategy (Stride = 512). The perplexity of
RoFormer (Su et al., 2024) sharply exceeds 1000 beyond its
training length, while CoCA maintains a low plateau even at
60×its training length. ALibi (Press et al., 2022) encounters
Out of Memory (OOM) issues for input Nmax>8000 due to
flash-attention (Dao et al., 2022) incompatibility, we suppose
it maintains perplexity for Nmax>8000.
Existing studies primarily focus on attention ker-
nel (Beltagy et al., 2020; Ding et al., 2023; Han
et al., 2023) or position embedding (Huang et al.,
2023), often neglecting the intrinsic relationship
between the two key modules. Attention bias is an
alternative to the explicit encoding of positional in-
formation. ALibi (Press et al., 2022) and KERPLE
(Chi et al., 2022), incorporate heuristic and com-
positional triangle kernel-based negative causal at-
tention bias, respectively. While these approaches
effectively manage to maintain low perplexity, they
fall short in capturing long-range dependencies due
to introducing local hypotheses to context tokens.
Another branch of methods involve simply scal-
ing Rotary Position Embedding (RoPE) (Su et al.,
2024) to extrapolate the inference context length
with minimal or no fine-tuning. For instance, Posi-
tion Interpolation (PI) (Chen et al., 2023) employs
linear scaling on each position number from nto
n/k, where kis the extrapolation ratio. NTK-aware
Scaled RoPE (bloc97, 2023) and Dynamic-NTKarXiv:2309.08646v3  [cs.LG]  28 Feb 2024

--- PAGE 2 ---
(Emozilla, 2023) combine high-frequency extrapo-
lation and low-frequency interpolation. They scale
the basis in RoPE upon the sequence length to
adapt to the unseen position indices. However,
these methods primarily alleviate the problem of
modeling the rotation angles in out-of-distribution
positions, without recognizing the intrinsic correla-
tion between attention matrices and rotation angles.
Therefore, these methods still suffer from a limited
context window extending ratio.
Here, we present a new perspective on the rela-
tionship between position embedding (with a fo-
cus on RoPE) and the self-attention mechanism.
In a nutshell, RoPE utilizes a rotation matrix to
encode absolute positions while simultaneously
incorporating explicit relative position dependen-
cies within the self-attention formulation (Su et al.,
2024). It is designed based on the relative angular
difference between the queries ( Q) and keys ( K).
However, latent relationships exist between Qand
K, as these two matrices are directly multiplied.
We demonstrate that incorrect initialization of the
angle between QandKin RoPE leads to undesir-
able behavior around the context window boundary,
harming its performance for context extrapolation.
To address this undesirable behavior , we pro-
pose an innovative architecture called Collinear
Constrained Attention (CoCA). Specifically, we
enforce a collinear constraint between QandK
by initializing the angle between every two hid-
den dimensions in the QandKvectors to 0. This
allows for a seamless integration of RoPE and self-
attention. The model architecture and comparison
with RoFomer (Su et al., 2024) is illustrated in
Figure 2.
Extensive experiments show that a CoCA-based
GPT model, trained within 512 context length,
seamlessly extends the context window up to 32K
(60x) without perplexity divergence. A compre-
hensive comparison between our method and ex-
isting methods is presented in Figure 1. Further-
more, it enhances long-context retrieval ability,
achieving a passkey retrieval accuracy of 50%+
even when extrapolating to 16x longer than its
training context length by applying Dynamic-NTK
(Emozilla, 2023). Additionally, by dropping CoCA
in LLaMA-7B, we achieve extrapolation up to 32K
within only 2K training length.
Our main contributions can be summarized as
follows:
•We unveil undesirable context boundary be-havior resulting from the absence of modeling
the relationship between position embeddings
and self-attention.
•To tackle the undesirable context boundary be-
havior, we propose Collinear Constrained At-
tention (CoCA) to seamlessly integrate the po-
sition embeddings and self-attention, achiev-
ing excellent long context window extrapola-
tion performance.
•CoCA extends its context window from 512
to 32K without fine-tuning, achieving over
50% accuracy even when 16 ×longer than its
training length. Using CoCA in LLaMA-7B,
we achieve extrapolation up to 32K within just
2K training length.
•CoCA introduces minimal computational and
spatial complexity compared to vanilla self-
attention. We provide an optimized imple-
mentation of CoCA, making it able to be
a seamless drop-in replacement for existing
transformer-based models.
2 Method
In this section, we describe our proposed Collinear
Constrained Attention (CoCA). We begin with in-
troducing the background theory of RoPE (Su et al.,
2024) in Section 2.1, and then analyze the anoma-
lous behaviors between the attention matrices and
RoPE in Section 2.2. Finally, we introduce the
proposed method CoCA in section 2.3 and derive
a slack constraint version of CoCA in Section 2.4,
respectively.
2.1 Rotary Position Embedding
Position embedding is a crucial component in
transformer-based models. Here we focus on Ro-
tary Position Embedding (RoPE) (Su et al., 2024),
which is widely used by LLMs including LLaMA
(Touvron et al., 2023a), LLaMA-2 (Touvron et al.,
2023b), GPT-NeoX (Black et al., 2022) and Qwen
(Bai et al., 2023). Suppose the positional index is
an integer n∈[1, N], and the corresponding in-
put vector x= [x0, x1, ..., x d−1]T, where Nis the
sequence length, dis the dimension of the atten-
tion head. RoPE defines a vector-valued complex
function f(x, n):
f(x, n) = [( x0+ix1)einθ0,(x2+ix3)einθ1,
. . . ,(xd−1+ixd)einθd/2−1]T,
where θ j=B−2j/d,(1)

--- PAGE 3 ---
Figure 2: Architecture comparison between RoFormer and CoCA. (a) RoFormer; (b) CoCA; (c) The implementation detial
ofKin CoCA. Q,T, and Vare produced using projection matrices identical to those employed in the vanilla self-attention. T
undergoes a halving operation, with the other half being duplicated. Kis then computed as the element-wise product of QandT,
adhering to a collinear constraint with Q. Note that kn∈RN×d, where n∈[1, N]is the positional index of key, dis the head
dimension, Nis the sequence length.
in this paper, the base B = 10,000.
After the application of RoPE, the transformed
vectors for query ( q) and key ( k) become f(q, m)
andf(k, n), respectively. Here, m, n∈[0, N]
represent the positional indices of qandk. The
attention operation is computed as the dot product
between f(q, m)andf(k, n), defined as follows:
a(m, n) =Re(⟨f(q, m), f(k, n)⟩)
=Re
d/2−1X
j=0(q2j+iq2j+1)(k2j−ik2j+1)ei(m−n)θj

=d/2−1X
j=0[(q2jk2j+q2j+1k2j+1) cos(( m−n)θj)
+ (q2jk2j+1−q2j+1k2j) sin(( m−n)θj)]
(2)
The attention score a(m−n)depends on the rela-
tive position (m−n).
2.2 Anomalous Behavior between RoPE and
Attention Matrices
After applying RoPE, the attention score a(m−n)
can be interpreted as the sum of d/2inner prod-
ucts of complex numbers, as illustrated in Equa-
tion (2). For any pair of qj= (q2j, q2j+1)and
kj= (k2j, k2j+1), which is the 2-dimensional
slicing of q(orqm) and k(orkn), we introducethe initial angle Θjbetween them, measured coun-
terclockwise from kjtoqjin the complex plane.
Throughout our analysis, we keep the position of
kjfixed, systematically rotating qjto compre-
hensively examine their relative positions. The
final angle between qjandkjis represented as
θ(qj,kj) = Θ j+ (m−n)θj, where mandnare
positional indices of qjandkj.
In this concept, the attention score can be for-
mulized as:
a(m, n) =d/2−1X
j=0|qj||kj|cos(θ(qj,kj)) (3)
Refer to Figure 3 for a visual representation of
this concept for any individual j∈[0, d/2]in the
2-D subspace. There are four distinct scenarios
between qjandkjafter rotation.
(1)Scenario (b) and (c): When m > n and
Θj≤π, orm < n andΘj> π, the value of
cos(θ(qj,kj))between qjandkjdecreases with
the expanding distance between mandn. In these
2 scenarios, no anomalous behavior is observed,
as the attention score naturally decreases with the
positional distance. This trend persists until the rel-
ative angle θ(qj,kj)rotates beyond the boundary
ofπ.

--- PAGE 4 ---
Figure 3: Anomalous behavior of RoPE in 2-D plane. The inner product of vectors qjandkjis contingent upon the relative angle
θ(qj,kj), defined as Θj+ (m−n)θj. Here, Θjrepresents the initial angle, and (m−n)θjsignifies the position-dependent
rotation angle. (a) m < n andΘj≤π. (b)m > n andΘj≤π. (c)m < n andΘj> π. (d)m > n andΘj> π.
(2)Scenario (a) and (d): When m < n and
Θj≤π, orm > n andΘj> π, intriguing phe-
nomena emerge. As the distance between mand
ngrows, the value of cos(θ(qj,kj))between qj
andkjparadoxically increases. This anomaly has
a notable impact on attention scores, particularly
affecting the τclosest tokens. In this context, τis
defined as Θj/θjfor scenario (a) and (2π−Θj)/θj
for scenario (d). Consequently, attention scores for
these tokens are abnormally diminished.
For bidirectional language models, all four cases
may occur. For causal models, only scenario (b)
and (d) manifest, as mconsistently exceeds n.
The attention score a(m−n)is the sum of d/2
inner-products, one of them turns out anomalous
may be insignificant, however, experiments con-
firmed this significance. Further analysis of this
rotary borders anomalous behaviour is discussed in
Appendix D.2.
2.3 Collinear Constrained Attention
To tackle the anomalous behavior between RoPE
and attention matrices, we propose a novel ap-
proach called Collinear Constrained Attention
(CoCA). Specifically, by applying a collinear con-
straint to any pair of qj= (q2j, q2j+1)andkj=
(k2j, k2j+1), we seamlessly integrate RoPE into
self-attention mechanism, achieving long context
extrapolation.
To formalize this, considering a sequence of N
input tokens SN={wn}N
n=1, with corresponding
word embeddings EN={xn}N
n=1, where xn∈
Rdis the d-dimensional word embedding vector of
token wnwithout position information. First, the
queries qmare obtained:
qm=WQxm,∀m∈[1, N] (4)
Next, we derive the keys knwith collinear con-
straints. This begins with the introducing of the
constraint coefficient tnfor each token position n,
as depicted in Equation (5).tn=WTxn,∀n∈[1, N] (5)
Next, Equation (6) imposes the collinearity con-
dition on the coefficients t2jandt2j+1, where
tn= [t0, t1, ..., t d−1]T, ensuring that each pair is
identical. This step effectively duplicates each 2-
dimensional segment of the tensor.
t2j=t2j+1,∀j∈[0, d/2−1]
tn=Relu(tn)(6)
Subsequently, the keys are calculated as shown
in Equation (7), where knare represented by the
element-wise multiplication of Q= (q1, ...,qN)
andtn. This results in an expansion of dimen-
sionality, as kn∈RN×dnow includes an addi-
tional sequence length dimension. We address
potential memory pressure by optimizing tensor
contractions, ensuring no net increase in memory
consumption. For an in-depth analysis, please refer
to Appendix C.
kn=Q⊙tn= (q1◦tn, ...,qN◦tn) (7)
After that, we apply RoPE on QandK, with the
function fdetailed in Equation (1).
f(qm) =f(qm, m)
f(kn) =f(Q⊙tn, n) =f(Q, n)⊙tn(8)
Finally, the attention score of CoCA would be:
a(m, n) =Re(⟨f(qm, m), f(qm, n)◦tn⟩) (9)
Equation (9) illustrates the additional dimension
of the keys in our CoCA mechanism. Specifically,
it maps the index of each query to the additional
dimension, establishing a collinear relationship be-
tween the n-th key and the m-th query. This is a
critical aspect of our method.
2.4 Slacking the Constraint on Query
In Section 2.3, we present a theoretically precise
solution for CoCA. However, practical implemen-
tation faces challenges due to the complexity of

--- PAGE 5 ---
O(N2d)when storing f(Q, n). To address this is-
sue, we provide a dual implementation with O(Nd)
complexity in this section and prove their equiva-
lence.
Theorem 1. (Dual implementation of CoCA) For
any attention score defined in Equation (9), there
exists an equivalent form as follows:
a(m, n) =Re(⟨f(qm, m),qm◦f(tn, n)⟩) (10)
with constraint:
q2j=q2j+1,∀j∈[0, d/2−1] (11)
Proof : The proof consists of two steps.
Step 1. We prove that, by imposing the con-
straint q2j=q2j+1,∀j∈[0, d/2−1],
Re(⟨f(qm, m),qm◦f(tn, n)⟩)is equivalent to
Re(⟨f(qm, m), f(qm, n)◦tn⟩).
To see this, we calculate the difference between
f(qm, n)◦tnandqm◦f(tn, n):
f(qm, n)◦tn−qm◦f(tn, n)
=
t0(q0cosnθ0−q1sinnθ0)
t1(q0sinnθ0+q1cosnθ0)
. . .
td−2(qd−2cosnθd/2−1−qd−1sinnθd/2−1)
td−1(qd−2sinnθd/2−1+qd−1cosnθd/2−1)

−
q0(t0cosnθ0−t1sinnθ0)
q1(t0sinnθ0+t1cosnθ0)
. . .
qd−2(td−2cosnθd/2−1−td−1sinnθd/2−1)
qd−1(td−2sinnθd/2−1+td−1cosnθd/2−1)

(12)
Recall that t2j=t2j+1,∀j∈[0, d/2−1](see
Equation (6)), Equation (12) is equivalent to:
f(qm, n)◦tn−qm◦f(tn, n)
=
t0(q0−q1) sinnθ0
t1(q0−q1) sinnθ0
. . .
td−2(qd−2−qd−1) sinnθd/2−1
td−1(qd−2−qd−1) sinnθd/2−1
(13)
Clearly, if we impose the constraint q2j=
q2j+1,∀j∈[0, d/2−1], the vector in Equation
(13) becomes null and we deduce that:
f(qm, n)◦tn−qm◦f(tn, n) =0 (14)
Consequently, with the constraint q2j=
q2j+1,∀j∈[0, d/2−1], we have:
Re(⟨f(qm, m),qm◦f(tn, n)⟩)
=Re(⟨f(qm, m), f(qm, n)◦tn⟩)(15)
Step 2. We further demonstrate that,
q2j=q2j+1,∀j∈[0, d/2−1]is in facta redundant constraint when calculating
Re(⟨f(qm, m), f(qm, n)◦tn⟩). To verify
this, we expand the inner product:
Re(⟨f(qm, m), f(qm, n)◦tn⟩)
=d/2−1X
j=0[(q2
2jt2j+q2
2j+1t2j+1) cos(( m−n)θj)
+ (q2jq2j+1t2j−q2j+1q2jt2j+1) sin(( m−n)θj)]
(16)
Recall again t2j=t2j+1,∀j∈[0, d/2−1], we
have
Re(⟨f(qm, m), f(qm, n)◦tn⟩)
=d/2−1X
j=0t2j[(q2
2j+q2
2j+1) cos(( m−n)θj)]
=d/2−1X
j=0t2j|qj|2cos((m−n)θj)(17)
This implies that Re(⟨f(qm, m), f(qm, n)◦
tn⟩)depends solely on the magnitude of qj=
(q2j, q2j+1)in 2-D subspace, demonstrating the in-
dependence of the relationship between q2jand
q2j+1. Refer to Appendix D.3 for the rigorous
proof.
Now we conclude that, with the con-
straint q2j=q2j+1,∀j∈[0, d/2−1],
Re(⟨f(qm, m),qm◦f(tn, n)⟩)is equivalent
toRe(⟨f(qm, m), f(qm, n)◦tn⟩)with no
constraint on query.
By removing q2j=q2j+1constraint, we desig-
nate this modified version as CoCA-Slack. The
mathematical definition is provided in Appendix
D.4.
3 Experimental Setting
This section provides an overview of the experimen-
tal setup, including details regarding the training
data utilized and the baseline models employed to
evaluate the effectiveness of the proposed method.
3.1 Training Data
Our model undergoes training on a combination of
datasets, including the Pile training dataset (Gao
et al., 2020), BookCorpus (Zhu et al., 2015), and
the Wikipedia Corpus (Foundation, 2021). Ad-
ditionally, we integrate manually collected open-
source code from GitHub repositories with at least
1 star. From these datasets, we derive a sample of
approximately 50B tokens, maintaining a composi-
tion of 75% text and 25% code.

--- PAGE 6 ---
3.2 Model Variants
To evaluate the effectiveness of our proposed ap-
proach, we train 3 models from scratch under iden-
tical experimental settings, including ALibi (Press
et al., 2022), RoFomer (Su et al., 2024), and Ro-
Former+CoCA. All models share common specifi-
cations, featuring a size of 350M, 24 layers, a hid-
den dimension of 1024, 16 attention heads, and a
maximum sequence length of 512. The key distinc-
tions among them lie in variations in self-attention
mechanisms and position embeddings. The imple-
mentation is optimized based on EleutherAI GPT-
NeoX1. Training a model from scratch demands
substantial computational resources. Therefore, we
also conduct experiments involving fine-tuning ex-
isting LLMs with a drop-in CoCA module. For
this purpose, we utilize the LLaMA-7B model
(Touvron et al., 2023a), which was trained with
a context length of 2,048. Additionally, we employ
dynamic-NTK for all the above models.
In summary, our comparison models are cat-
egorized as follows: ALibi, RoFormer, Ro-
Former+CoCA, RoFormer+dynamic NTK, and Ro-
Former+dynamic NTK & CoCA, all falling un-
der the training from scratch category. Mean-
while, LLaMA-7B, LLaMA-7B+CoCA, LLaMA-
7B+dynamic NTK, and LLaMA-7B+dynamic
NTK & CoCA belong to the fine-tuning LLM with
drop-in CoCA category.
3.3 Implementation Detials
Pre-training Procedure We train all models us-
ing the next token prediction objective. We use
AdamW (Loshchilov and Hutter, 2017) with β1
= 0.9 and β2= 0.95. The learning rate follows a
linear warm-up of 1% of total steps, starting from
1e-7. Subsequently, the learning rate is adjusted
to 1e-4 with linear decay, eventually reaching 1e-5.
The training utilizes 8 A100 GPUs, with a global
batch size of 256 and 2 gradient steps accumulation,
taking approximately 96 hours for 2 epochs.
Fine-tuning Procedure To integrate CoCA in
LLaMA, we employ a three-stage fine-tuning strat-
egy: (1) only updating the Kprojection (7% of
parameters). This stage aims to reconstruct the
Kprojection in CoCA. By freezing the other pa-
rameters, we maintain attention scores as closely as
possible to those of vanilla self-attention. (2) updat-
ing the QKV projection (21% of parameters). This
stage aims to address intrinsic over-fitting in vanilla
1https://github.com/EleutherAI/gpt-neox/tree/v2.0self-attention caused by undesired behaviors be-
tween RoPE and attention matrices. (3) fine-tuning
all parameters. Each stage involves 15K steps, to-
taling 7.5B tokens (22B tokens overall), using the
next token prediction objective. The training length
of LLaMA-7B + CoCA remains at 2,048 as in the
original model. All experiments are conducted with
32 A100 GPUs, setting a per-device batch size to 8
without gradient accumulation.
4 Experiment Results
We conducted experiments to shed light on the
following reasonable doubts:
•Can our new attention mechanism CoCA im-
prove the long context extrapolation perfor-
mance of existing models?
•Can combining CoCA with other extending
methods for RoPE effectively solve the three
types of rotational boundary problems dis-
cussed in Appendix D.2?
4.1 Long Sequence Language Modeling
We evaluate the long sequence language model-
ing performance of both our model and baseline
models on the test splits of the PG-19 dataset (Rae
et al., 2020). For this evaluation, we randomly se-
lect a subsample comprising 100 documents, each
containing at least 32,768 SentencePiece (Kudo
and Richardson, 2018) tokens. We then truncate
each test document to its initial 32,768 tokens. The
evaluation involves calculating perplexity across
different context window sizes using a sliding win-
dow approach, as described by (Press et al., 2022),
with a stride of 512. The perplexity results for both
our models and baselines are presented in Table 1
and Figure 1.
Based on our experiments, the evaluation re-
sults indicate that models combined with CoCA ex-
hibit significantly improved perplexity with longer
inference sequence length. For pre-trained mod-
els, by increasing the context window size from
512 (training context window size) to 32k, the
perplexity of CoCA only increases from 20.11 to
171.63, whereas the perplexity of RoFormer be-
comes inf. Additionally, by increasing the context
window size from 2K to 32K, the perplexity of fine-
tuned LLaMA-7B+CoCA only increases 21.68,
while LLaMA-7B with other extending methods
increases more than 100. In general, we observe a
consistent trend of CoCA achieving better perplex-
ity with longer context windows. This suggests

--- PAGE 7 ---
MethodEvaluation Context Window Size (Perplexity ↓)
512 1024 2048 4096 8192 16k 32k
Training model from scratch
ALibi 18.69 21.27 28.20 35.66 37.03 OOM OOM
RoFomer 19.66 411.50 3276.00 3026.00 3028.00 inf inf
+ dynamic NTK 19.66 22.30 38.00 75.75 138.13 370.75 380.75
+ CoCA 20.11 33.47 69.06 113.19 157.38 141.00 171.63
+ dynamic NTK & CoCA 20.11 20.81 25.88 34.16 55.75 89.31 101.13
Fine-tuning LLM with drop-in CoCA
LLaMA-7B 9.25 7.56 7.30 9673.14 inf inf inf
+ dynamic NTK 9.25 7.56 7.30 9.40 14.40 63.62 133.87
+ CoCA 9.91 8.49 8.27 24.23 42.00 23.83 29.95
+ dynamic NTK & CoCA 9.91 8.49 8.27 8.61 9.56 11.10 13.98
Table 1: Evaluation perplexity on 100 PG-19 documents using sliding window (S = 512) strategy. Dynamic-NTK is employed
without fine-tuning. The best result is highlighted in bold.
that CoCA has a more robust position embedding,
enabling it to handle long context more effectively.
In contrast, we observe that models extended
through the direct application of dynamic NTK-
aware Scaled RoPE exhibit a larger increase in
perplexity at longer sequences. The perplexity
of both RoFormer+dynamic NTK and LLaMA-
7B+dynamic NTK remains significantly higher
than that combining CoCA. This difference be-
comes more pronounced as the sequence length
increases. When the inference sequence length
reaches 32k, the perplexity of RoFormer+dynamic
NTK increases to 380.75, while the result for
RoFormer+CoCA is only 171.63. Similarly, the
perplexity of LLaMA-7B+dynamic NTK reaches
133.87, whereas LLaMA-7B+CoCA is only 29.95.
It is worth noting that the model achieves the best
performance when both dynamic NTK and CoCA
are combined. Particularly, LLaMA-7B+dynamic
NTK & CoCA consistently maintains a very low
perplexity. Even when the inference sequence
length has reached 32k (16 ×longer than the train-
ing length), the perplexity is only 13.89. This indi-
cates that combining CoCA with other extending
methods for RoPE can effectively address the three
types of rotational boundary problems, achieving
robust long-text extrapolation modeling capabili-
ties.
4.2 Long Context Retrieval
Perplexity evaluates the performance of language
model in predicting the next token. However, it is
insufficient for a comprehensive assessment of the
effective context window size. To address this, we
conducted experiments using a passkey retrievaltask (Mohtashami and Jaggi, 2023) to evaluate our
method and baselines. The task involves identi-
fying and retrieving a randomly hidden passkey
within a lengthy document. More details of task
definition and test sample generation settings can
be found in Appendix B.1. Table 2 illustrates the
accuracy of all tested models and their variants.
It is evident that ALibi exhibited failures when
tested on sequences that were 1 ×longer than its
training length, attributed to its local hypothesis.
In contrast, our model consistently demonstrated
superior accuracy. RoFormer+dynamic NTK &
CoCA maintained a 50% accuracy, even with the
test sequence length expanded to 16 ×its training
length. Similarly, LLaMA-7B+dynamic NTK &
CoCA still maintained a 30% accuracy when the
test length was up to 32K.
4.3 Impact of Strict and Slack Constraint on
Q
As mentioned in Section 2.4, we implement a
slack version of CoCA, referred to as CoCA-Slack.
In this section, under the same experimental set-
tings, we implement two versions of CoCA based
on RoFormer-350M, labeled as CoCA-Slack and
CoCA-Strict. The comparison results between
them are shown in Table 3.
We observe that the CoCA-Strict and CoCA-
Slack models exhibit similar performance in long
sequence language modeling, as evidenced by
comparable perplexity results. However, in the
passkey retrieval task, contrary to our initial ex-
pectations, the CoCA-Strict model produces sig-
nificantly lower results. This unexpected outcome
suggests that models with a slack constraint may

--- PAGE 8 ---
MethodEvaluation Context Window Size (Accuracy ↑)
512 1024 2048 4096 8192 16k 32k
Traning model from scratch
ALibi 0.82 0.65 0.28 0.18 0.12 OOM OOM
RoFomer 0.99 0.53 0.30 0.18 0.04 0.02 0.04
+ dynamic NTK 0.99 1.00 0.95 0.70 0.41 0.16 0.06
+ CoCA 1.00 0.64 0.33 0.19 0.06 0.02 0.04
+ dynamic NTK & CoCA 1.00 1.00 0.96 0.89 0.50 0.23 0.08
Fine-tuning LLM with drop-in CoCA
LLaMA-7B 1.00 1.00 1.00 0.61 0.21 0.07 0.09
+ dynamic NTK 1.00 1.00 1.00 0.81 0.26 0.06 0.03
+ CoCA 1.00 1.00 1.00 0.71 0.28 0.11 0.10
+ dynamic NTK & CoCA 1.00 1.00 1.00 1.00 0.85 0.51 0.30
Table 2: Long context retrieval performance on passkey retrieval task. The best result is highlighted in bold.
Method 512 1024 2048 4096 8192 16384 32768
Performance on Long Sequence Modeling (Perplexity)
CoCA-Slack 20.11 19.02 24.92 40.53 68.38 92.75 103.44ntk-2CoCA-Strict +0.07 +0.61 -1.58 -4.03 +15.37 +12.38 +1.94
CoCA-Slack 20.11 20.81 25.88 34.16 55.75 89.31 101.13ntk-4CoCA-Strict +0.07 -0.49 -0.66 -0.88 +3.16 -18.25 -2.57
CoCA-Slack 20.11 23.66 29.05 37.47 55.5 88.88 111.38ntk-8CoCA-Strict +0.07 -1.74 -0.64 +1.16 +0.03 +0.5 +0.31
Performance on Long Context Retrieval (Passkey Accuracy)
CoCA-Slack 1.0 0.99 0.94 0.77 0.47 0.27 0.15ntk-2CoCA-Strict +0.0 -0.12 -0.3 -0.42 -0.34 -0.22 -0.07
CoCA-Slack 1.0 1.0 0.96 0.89 0.5 0.23 0.08ntk-4CoCA-Strict +0.0 -0.11 -0.38 -0.46 -0.38 -0.19 -0.02
CoCA-Slack 1.0 0.98 0.99 0.85 0.5 0.11 0.02ntk-8CoCA-Strict +0.0 -0.05 -0.34 -0.51 -0.4 -0.07 -0.01
Table 3: Comparison results for the Strict and Slack Constraints of Q in our proposed CoCA module. Superior performance to
CoCA-Slack is indicated by the green color, while inferior performance is signified by the red color. The perplexity of the strict
and slack models is comparable, whereas the strict model achieved lower accuracy in the passkey retrieval task.
offer additional performance advantages, such as a
larger effective context window size.
Understanding the reasons behind the superiority
of slack constraints will be a key focus of our future
work. In this regard, we provide some theoretical
insights in Appendices D.3 and D.4. These insights
aim to shed light on the underlying mechanisms
that contribute to the observed differences and lay
the groundwork for a more comprehensive analysis
in subsequent research.
5 Conclusion
In this paper, we introduce Collinear Constrained
Attention (CoCA), a novel approach that integrates
position embedding with the self-attention mecha-
nism. This innovation addresses undesired behav-
iors occurring around the context window bound-
ary, which stem from discrepancies between RoPEand attention matrices. To the best of our knowl-
edge, we are the first to analyze the initial angles be-
tween queries and keys in the self-attention mecha-
nism, which gives rise to anomalous phenomena in
RoPE. Furthermore, we deduce a slack constraint
for our implementation of CoCA. Extensive ex-
periments demonstrate that incorporating CoCA
into existing models significantly enhances perfor-
mance in both long sequence language modeling
and long context retrieval tasks. Additionally, the
simultaneous integration of CoCA with other ex-
tended RoPE methods (e.g., dynamic-NTK) effec-
tively mitigates three types of rotation boundary
issues, resulting in remarkably improved capabili-
ties for long context extrapolation.

--- PAGE 9 ---
Limitations
Our current approach, CoCA, has thus far under-
gone exclusive validation on RoPE. Experimen-
tal results demonstrate that CoCA enhances the
long-context extrapolation performance of LLMs
and further augments other extension methods by
addressing rotational boundary issues. However,
questions arise regarding its applicability to more
general methods. While the effectiveness of slack
position embedding (SPE) is evident, a deeper un-
derstanding of the underlying reasons for its supe-
rior performance necessitates further investigation.
References
Daniel G. a. Smith and Johnnie Gray. 2018. opt_einsum
- a python package for optimizing contraction order
for einsum-like expressions. Journal of Open Source
Software , 3(26):753.
Jinze Bai, Shuai Bai, Yunfei Chu, et al. 2023. Qwen
technical report. arXiv preprint arXiv:2309.16609 .
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 .
Sidney Black, Stella Biderman, Eric Hallahan, et al.
2022. GPT-NeoX-20B: An open-source autoregres-
sive language model. In Proceedings of BigScience
Episode #5 – Workshop on Challenges & Perspec-
tives in Creating Large Language Models , pages 95–
136, virtual+Dublin. Association for Computational
Linguistics.
bloc97. 2023. Ntk-aware scaled rope allows llama mod-
els to have extended (8k+) context size without any
fine-tuning and minimal perplexity degradation.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
ArXiv , abs/2306.15595.
Ta-Chung Chi, Ting-Han Fan, Peter J. Ramadge, and
Alexander Rudnicky. 2022. KERPLE: kernelized
relative positional embedding for length extrapola-
tion. In Advances in Neural Information Processing
Systems 35: Annual Conference on Neural Informa-
tion Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022 .
OpenCompass Contributors. 2023. Opencompass:
A universal evaluation platform for foundation
models. https://github.com/open-compass/
opencompass .
Tri Dao, Daniel Y . Fu, Stefano Ermon, et al. 2022.
FlashAttention: Fast and memory-efficient exact at-
tention with IO-awareness. In Advances in Neural
Information Processing Systems .Jiayu Ding, Shuming Ma, Li Dong, et al. 2023. Longnet:
Scaling transformers to 1,000,000,000 tokens. arXiv
preprint arXiv:2307.02486 .
Emozilla. 2023. Dynamically scaled rope further in-
creases performance of long context llama with zero
fine-tuning.
Wikimedia Foundation. 2021. Wikimedia downloads.
Leo Gao, Stella Rose Biderman, Sid Black, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. ArXiv , abs/2101.00027.
Chi Han, Qifan Wang, Wenhan Xiong, et al. 2023.
Lm-infinite: Simple on-the-fly length generaliza-
tion for large language models. arXiv preprint
arXiv:2308.16137 .
Yunpeng Huang, Jingwei Xu, Zixu Jiang, et al. 2023.
Advancing transformer architecture in long-context
large language models: A comprehensive survey.
arXiv preprint arXiv:2311.12351 .
Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2018: System Demonstrations, Brussels, Belgium,
October 31 - November 4, 2018 , pages 66–71. Asso-
ciation for Computational Linguistics.
Ilya Loshchilov and Frank Hutter. 2017. Fixing
weight decay regularization in adam. ArXiv ,
abs/1711.05101.
Amirkeivan Mohtashami and Martin Jaggi. 2023. Land-
mark attention: Random-access infinite context
length for transformers. CoRR , abs/2305.16300.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context win-
dow extension of large language models. CoRR ,
abs/2309.00071.
Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,
Chloe Hillier, and Timothy P. Lillicrap. 2020. Com-
pressive transformers for long-range sequence mod-
elling. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020 . OpenReview.net.
Sebastian Ruder, Matthew E. Peters, Swabha
Swayamdipta, and Thomas Wolf. 2019. Trans-
fer learning in natural language processing. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2,

--- PAGE 10 ---
2019, Tutorial Abstracts , pages 15–18. Association
for Computational Linguistics.
Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng
Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing , 568:127063.
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2023. A length-extrapolatable
transformer. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), ACL 2023, Toronto,
Canada, July 9-14, 2023 , pages 14590–14604. Asso-
ciation for Computational Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al.
2023a. Llama: Open and efficient foundation lan-
guage models. ArXiv , abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin R. Stone, et al.
2023b. Llama 2: Open foundation and fine-tuned
chat models. ArXiv , abs/2307.09288.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA , pages 5998–6008.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, et al.
2023. Efficient streaming language models with at-
tention sinks. arXiv preprint arXiv:2309.17453 .
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Aligning books and movies:
Towards story-like visual explanations by watching
movies and reading books. In 2015 IEEE Interna-
tional Conference on Computer Vision, ICCV 2015,
Santiago, Chile, December 7-13, 2015 , pages 19–27.
IEEE Computer Society.
A Related Work
Existing researches are mainly focused on the sub-
module of attention kernel or position embedding
(Huang et al., 2023). In the following sections, we
will separately introduce works on these two as-
pects: Section A.1 primarily addresses the former,
while Section A.2 delves into the latter.
A.1 Efficient Attention Mechanisms
Several works aim to implement efficient atten-
tion mechanisms with reduced computational de-
mands, even achieving linear complexity. This en-
ables extending the effective context length bound-
ary of LLMs during inference by directly increas-
ingLmax in the pre-training stage (Ding et al.,2023; Mohtashami and Jaggi, 2023). Notewor-
thy approaches include Longformer (Beltagy et al.,
2020), utilizing slide window attention, and mod-
els such as StreamingLLM (Xiao et al., 2023) and
LM-Infinite (Han et al., 2023), which leverage a
global-local attention mechanism. These variants
have achieved success to a certain extent, but still
face issues we unveiled in this work when using
RoPE as their positional encoding method.
A.2 Extrapolative Position Embedding
Methods
Extrapolative position embedding methods aim
to enhance the length generalization capability of
LLMs.
A.2.1 Attention Bias
In seeking alternatives to the explicit encoding of
positional information, researchers have explored
the integration of attention bias to capture the se-
quential and temporal nuances inherent in natural
language. Early approaches, such as T5 (Ruder
et al., 2019), incorporate learnable attention bias.
However, these methods do not explicitly address
the challenge of length extrapolation. ALibi (Press
et al., 2022) introduces a negative causal atten-
tion bias in a heuristic manner. Extending the
ALiBi-style attention bias, KERPLE (Chi et al.,
2022) treats it as a composition triangle kernel
for self-attention and modifies style Xpos (Sun
et al., 2023) by integrating it with RoPE. While
these approaches effectively manage to maintain
low perplexity levels, they fall short in capturing
long-range dependencies due to introducing local
hypotheses to context tokens.
A.2.2 Extend RoPE
Besides, various strategies have been explored
to extend RoPE (Su et al., 2024), a commonly
employed positional encoding method in popu-
lar LLMs. Recent approaches involve simply
scaling it to extrapolate the inference context
length with minimal or no fine-tuning. For in-
stance, Position Interpolation (PI) (Chen et al.,
2023) applies linear scaling on each position num-
ber from nton/k, densifying the representation
space to extend the farthest length boundary by
ktimes. Other approaches, such as NTK-aware
Scaled RoPE (bloc97, 2023) and Dynamic-NTK
(Emozilla, 2023), combine high-frequency extrap-
olation and low-frequency interpolation. These
training-free methods require limited code changes

--- PAGE 11 ---
during inference (Peng et al., 2023). However,
these methods aim solely at alleviating the prob-
lem of modeling the rotation angles in out-of-
distribution (OOD) positions without recognizing
the intrinsic correlation between attention matrices
and rotation angles. Therefore, these methods still
suffer from a limited context window extending
ratio.
Previous methods independently investigate self-
attention and position embedding without consid-
ering their intrinsic relationship, especially for the
widely used RoPE method.
B Additional Experiment
B.1 Passkey Retrieval Task Definition
There is an important info hidden inside a
lot of irrelevant text. Find it and
memorize them. I will quiz you about the
important information there.
The grass is green. The sky is blue. The
sun is yellow. Here we go. There and back
again.
... // Repeat x times.
// Passkey is 5 randomly generated numbers.
The passkey is 12345. Remeber it. 12345 is
the passkey.
The grass is green. The sky is blue. The
sun is yellow. Here we go. There and back
again.
... // Repeat y times.
What is the passkey?
Listing 1: Prompt format for passkey retrieval (Mohtashami
and Jaggi, 2023). The passkey is randomly generated from
10,000 to 99,999.
The passkey retrieval task, as proposed by Mo-
htashami and Jaggi (2023), involves the model re-
covering a randomly generated passkey hidden in
a long document (see Listing 1 for the task prompt
format). Given a language model, we can deter-
mine the effective context window by assessing
the upper and lower bounds. We assume a random
passkey is ktokens away from the end of the in-
put. If a model consistently fails to recover the
passkey in multiple attempts, it suggests a context
window size smaller than k. Conversely, successful
retrievals indicate an effective context window size
of at least ktokens (Chen et al., 2023).
In our experiments, we generate test samples
based on the prompt template in Listing 1, with
lengths ranging from 512 to 32k. There are 100test cases for each length. Given a language model,
we input the passkey task prompt, examine the
model’s output for the new 64 tokens, and calculate
the accuracy.
B.2 Analysis I : Consistency of Optimization
in Position Embedding
The passkey retrieval results are presented in Sec-
tion 4.2. Our model demonstrates superior passkey
retrieval accuracy compared to baseline models
under various conditions. However, we remain in-
trigued about its optimization, specifically whether
it occurs within or beyond the confines of the train-
ing context window. To probe this further, we cat-
egorize the experimental data into two segments:
passkey distance shorter and farther than the train-
ing context window length.
Figure 4 (a) illustrates the comparison results
when the passkey is inserted less than 512 tokens
away from the end token, while Figure 4 (b) illus-
trates that outside this range. When the passkey is
inserted outside the 512 window, RoFormer+NTK
& CoCA consistently outperforms Roformer+NTK
across various lengths of inference sequences. This
superiority persists when the passkey is inserted
inside the 512 window. Notably, with an increase
in the length of the inference sequence, RoFormer
+ NTK & CoCA demonstrates increasingly supe-
rior performance compared to RoFormer + NTK.
These experiments suggest that our model can con-
sistently optimize the position embedding and ex-
tend the effective context window.
B.3 Analysis II : Impact of Dynamic-NTK in
CoCA
We utilize the dynamic NTK method (Emozilla,
2023) during the inference process, applying it sep-
arately to both our model and the baseline model.
To comprehensively assess the robustness of these
models, we conduct a thorough validation by vary-
ing scaling factors (2, 4, and 8).
The results in Figures 1 and 5 demonstrate that,
with the integration of the dynamic NTK method,
our model achieves higher passkey accuracy and
lower perplexity. Additionally, when the scaling
factor varies between 2, 4, and 8, the vanilla Ro-
Former model fails to maintain stable performance.
In contrast, CoCA consistently outperforms Ro-
Former at different scaling rates. This consistent
trend indicates that our model is more robust, show-
ing minimal performance fluctuations with changes
in the scaling factor.

--- PAGE 12 ---
(a) Inserting passkey inside 512 tokens away from end tokens
 (b) Inserting passkey outside 512 tokens away from end tokens
Figure 4: Comparison of effective context window between RoFormer + NTK and RoFormer + NTK & CoCA.
Figure 5: Passkey accuracy distribution on 4 range of distances.
CoCA outperforms RoFormer for all distances and scaling
factors of NTK.
Furthermore, it suggests that by implement-
ing collinear constraints, we can cleverly address
anomalous behavior in RoPE, allowing RoPE to
better leverage other extrapolation techniques.
B.4 Analysis III : Compatibility of CoCA with
PI
B.4.1 Experiment Setup
We conduct experiments utilizing the pre-trained
LLaMA-7B model (Touvron et al., 2023a) and
LLaMA-7B + CoCA from Section 3.2. To apply
PI , we follow the settings of Chen et al. (2023):
We set the fine-tuning sequence length to 32,768.
The learning rate is adjusted to 2e−5with no de-
cay to match. All other settings are maintained as
the LLaMA-7B configuration. All experiments are
conducted with 32 A100 GPUs, setting a per-device
batch size to 1 without gradient accumulation. The
experiments take 6,000 steps to accomplish.B.4.2 Long Context Validation
The results of fine-tuning with PI are presented
in Table 4. In terms of long sequence modeling,
both LLaMA-7B+PI and LLaMA-7B+CoCA & PI
demonstrate competitive performance across se-
quence lengths ranging from 512 to 8192. How-
ever, at longer sequence lengths (16384 and 32768),
LLaMA-7B+CoCA & PI exhibits a slight perfor-
mance advantage over LLaMA-7B+PI. For long
context retrieval, both methods achieve exception-
ally high accuracy, with scores approaching the
ideal value of 1.0 across all sequence lengths.
Overall, these findings suggest that the integra-
tion of PI and the CoCA module with the LLaMA-
7B model yields robust performance in both long
sequence modeling and long context retrieval tasks.
Additionally, the CoCA module demonstrates the
ability to maintain performance levels compara-
ble to PI, particularly evident at longer sequence
lengths.
B.4.3 Short Context Validation
In addition to enhancing long-context extrapola-
tion, it is imperative to consider the practicality and
scalability of CoCA in short contexts. Hence, we
evaluate our model on OpenCompass (Contribu-
tors, 2023), which comprises various dimensions,
including reasoning, understanding, language, and
examination. The results are presented in Table 5.
The table demonstrates that LLaMA-7B models
integrated with CoCA achieve performance com-
parable to the baseline LLaMA-7B across all eval-
uated dimensions. Specifically, the integration of

--- PAGE 13 ---
Method 512 1024 2048 4096 8192 16384 32768
Performance on Long Sequence Modeling (Perplexity)
LLaMA-7B+PI 9.06 7.55 7.74 7.16 7.04 6.93 7.11
+ CoCA & PI 9.65 8.19 8.37 7.87 7.84 7.83 7.96
Performance on Long Context Retrieval (Passkey Accuracy)
LLaMA-7B+PI 1.0 1.0 1.0 1.0 1.0 1.0 0.99
+ CoCA & PI 1.0 1.0 1.0 1.0 1.0 0.99 0.99
Table 4: Comparison results for LLaMA-7B+PI and LLaMA-7B+CoCA & PI after fine-tuning with sequence length of 32,768.
CoCA succeeds in maintaining the performance of PI within fine-tuning window size.
Method Reasoning Understanding Language Examination Average
LLaMA-7B 48.25 47.57 46.41 29.63 42.97
+ CoCA 45.55 51.14 55.27 25.14 44.28
+ PI 44.98 51.54 54.79 27.03 44.59
+ CoCA & PI 46.88 51.82 55.56 25.31 44.89
Table 5: OpenCompass results of LLaMA-7B and its vari-
ants. Models integrated with CoCA achieved comparable
performance to LLaMA-7B, leading no harm to the expres-
sion ability of the model.
CoCA yields no significant degradation in the ex-
pression ability of the model. This suggests that
CoCA is effective not only in long-context scenar-
ios but also in short-context tasks, demonstrating its
versatility and suitability for practical applications.
CComputational and Spatial Complexity
Analysis
Modulevanilla self-attention CoCA
Computational Spatial Computational Spatial
WQK(T)V 3Nd2h Nd 3Nd2h Nd
T half — — Ndh Nd
T Relu — — Ndh Nd
QK(T) rotation 2Ndh Nd 2Ndh Nd
Krot=Q◦Trot — — N2dh N2d
QrotKT
rot N2dh N2N2dh N2
Mask N2N2N2N2
Softmax N2N2N2N2
Table 6: The comparison of computational and spatial com-
plexity between vanilla self-attention block and CoCA. Here,
Nrepresents the sequence length, hdenotes the number of
heads, and dsignifies the dimension of each head.
In this section, we analyze the computational
and spatial complexities of CoCA. Table 6 pro-
vides a detailed comparison between the vanilla
self-attention mechanism and CoCA.
When using the operation Krot=Q◦Trot, the
computational complexity of CoCA does not ex-
ceed twice that of the vanilla self-attention. In
practice, the training and inference speed of CoCA
are comparable to the vanilla self-attention mech-
anism, with only a slight increase of about 5% to
10% , as depicted in Figure 6. However, there is
Figure 6: Inference speed comparison between CoCA and
vanilla self-attention.
a significant increase in spatial complexity when
expanding Krot=Q◦Trot, becoming dtimes that
of the vanilla self-attention. This level of spatial
complexity is not practical for applications.
To address this problem, we can draw inspiration
from the computation of QrotKT
rot, which involves
two steps: element-wise multiplication between
QrotandKrotfollowed by summation along the
hidden dimension. Optimization is attainable by
condensing the hidden dimension before fully ex-
panding the sequence length dimension. Conse-
quently, the spatial complexity is effectively re-
duced from N2dtoN2. This optimization strategy
is equally applicable to Krot=Q◦Trot. These
two components can be unified as articulated in
Equation (18):
QrotKT
rot=Qrot(Q◦Trot)T(18)
The commendable work accomplished by
opt_einsum (a. Smith and Gray, 2018) facilitates
the optimization of Equation (18). Experimen-
tal results indicate that Roformer+CoCA only de-
mands approximately 60GB of GPU memory dur-
ing inference with a sequence length of 32k, align-
ing closely with the memory consumption of the
vanilla self-attention mechanism.

--- PAGE 14 ---
D Theoretical Proof
D.1 Strong Form of Long-term Decay with
CoCA
We have introduced the basic theory of Rotary Po-
sition Embedding in Section 2.1. In fact, (Su et al.,
2024) shows that RoPE has the characteristic of
long-term decay:
|a(s)|=Re
d/2−1X
j=0hjeisθj

≤(max
i|hi+1−hi|)d/2−1X
j=0|Sj+1|(19)
where hj:= (q2j+iq2j+1)(k2j−ik2j+1)and
Sj:=Pj−1
k=0eisθk,s= (m−n),mfor the index
of query, nfor the index of key. Since the value ofPd/2−1
j=0|Sj+1|decays with the relative distance s,
the attention score decays either.
This characteristic ensures the stability of RoPE
during extrapolation to some extent by preventing
outliers. For CoCA, a stronger deduction can be
formulated as follows:
|a(s)| ≤(max
i|li+1−li|)d/2−1X
j=0|Cj+1| (20)
where lj:=|q2j+iq2j+1||k2j+ik2j+1|, andCj:=Pj−1
k=0cos(sθk). Furthermore, it holds that:
|li+1−li| ≤ |hi+1−hi| (21)
Proof : Notice that when the initial angle Θjbe-
tween qjandkjis0, from Equation (17), the at-
tention score can be simplified as:
a(s) =Re
d/2−1X
j=0hjeisθj

=d/2−1X
j=0ljcos(sθj)(22)
By following the study of (Su et al., 2024), we
can easily derive the estimation in Equation (20).
For Equation (21), applying the triangle inequal-
ity, we get:
|hi+1−hi| ≥ ||hi+1| − |hi|| (23)
Reviewing the definition of hi= ( q2j+
iq2j+1)(k2j−ik2j+1), we will find:
|hi+1−hi| ≥ ||hi+1| − |hi||
=||qi+1k∗
i+1| − |qik∗
i||
=||qi+1ki+1| − |qiki||
=|li+1−li|(24)
Figure 7: Rotary Borders Analysis. Regarding qjasx-axis, 3
distinct boundaries correspond to kj,−qj, andqj
D.2 Rotary Borders Analysis
In Section 2.2, we analyzed the anomalous phe-
nomena of RoPE. To illustrate the rotation anoma-
lies, let’s focus on a specific instance (case (d) of
Section 2.2). As shown in Figure 7, three distinct
boundaries emerge during the rotation. By adopt-
ing a relative coordinate system with qjserving
as the x-axis, these boundaries correspond to kj,
−qj, andqj.
Everytime the relative angle of qjandkjcrosses
these boundaries, the monotonicity of their inner-
product <qj,kj>undergoes a reversal. Thus,
for the vanilla self-attention, it learnt a piecewise
monotonic function of <qj,kj>:
<qj,kj>=

↑(m−n),∀ −(2π−Θj)≤θ(qj,kj)<0
↓(m−n),∀0≤θ(qj,kj)< π
↑(m−n),∀π≤θ(qj,kj)<2π
...
↑(m−n),∀(2k−1)π≤θ(qj,kj)<(2k)π
↓(m−n),∀(2k)π≤θ(qj,kj)<(2k+ 1)π
(25)
where θ(qj,kj) = Θ j+ (m−n)θjdefined in
Section 2.2.
This introduces confusion into the model dur-
ing direct context extrapolation. Therefore, meth-
ods like PI and NTK tried to introduce interpola-
tion or extrapolation techniques to eliminate out-
of-distribution (OOD) positions.
Except the first equation in Equation (25), the
two boundaries caused by −qj, andqjare regular
with periodicity of 2π, it is easy to handle when
applying methods like PI or NTK. However, the
boundaries caused by kjare hard to handle. There
ared/2∗h∗L(dfor head dimension, hfor number
of heads, Lfor number of layers) different bound-
aries during context extrapolation, which break the
periodicity of 2π.
Furthermore, after applying interpolation or ex-
trapolation techniques, more positions will fall into

--- PAGE 15 ---
this abnormal area. It increased ktimes ( kfor in-
terpolation factor) for PI and λ2j/dtimes ( λfor
scaling factor) for NTK.
From this perspective, positional concentration
of PI resulted in more trouble than NTK, i.e. ad-
ditionally more positions in abnormal area during
context extrapolation. This may explain in some ex-
tent why NTK could be used without fine-tuning for
vanilla self-attention, but PI requires fine-tuning.
By enforcing Θjto0, our proposed CoCA, con-
straining kjto be collinear with qj, effectively re-
solves the border-related challenge associated with
kj.
From experiments in Secton 4, with the inte-
grating of CoCA, now NTK can be leveraged well
through direct use, while PI achieved improvement
for direct use but still limited, which requires fur-
ther studies.
D.3 Homeomorphism of Representation
Space
Theorem 2. (Homeomorphism of representation
space) For any attention score defined as follows:
a(m, n) =Re(⟨f(qm, m), f(qm, n)◦tn⟩) (26)
where qmis the query, mis the index number of
query, tnis the collinear coefficient of CoCA, nis
the index number of key, fis the rotation operator.
Denote its representation space with respect to
qmas:
F(Q) ={a(m, n)|∀qm∈Q⊂Rd} (27)
where qm=WQxm,xm∈EN,m∈[1, N]
andENis the word embedding space, WQis the
projection matrix.
Then we have the following homeomorphism:
F(Q)∼=F(Qhalf) (28)
where Qhalf=Q|q2j=q2j+1,∀j∈[0,d/2−1].
Proof : We prove it by demonstrating the homeo-
morphism mapping G:
G:F(Q)→F(Qhalf)
F((q0, ..., q d−1)7→F((r
q2
0+q2
1
2, ...,s
q2
d−2+q2
d−1
2)
(29)
It consists of three parts:
Part I (Gis a bijection): recall Equation (17), we
have:
G(X) =X,∀X∈F(Q) (30)which implies that Gis an identity mapping, natu-
rally injective.
Next, we prove that Gis also surjective: for
anyY=F((q0, ..., q d−1)|q2j=q2j+1)∈F(Qhalf),
there exists eY∈F(Q)such that G(eY) =Y. Let
eY=F((q0, ..., q d−1)|q2j=q2j+1)∈F(Q) (31)
obviously we have G(eY) =Y.
Part II (Gis continuous): For any X0∈F(Q),
ϵ >0, there exists δ, such that if |X−X0|< δ,
then|G(X)− G(X0)|< ϵ.
From Part I ,Gis an identity mapping, let δ=ϵ,
then the continuity of Gholds.
Part III (G−1is continuous): Gis an identity map-
ping, so is G−1. Following Part II, we immediately
deduce that G−1is continuous.
D.4 Slack Position Embedding
LetHbe a Hilbert space, and {T(n)|n≥0} ⊂
L(H)is a family of bounded linear operator on H.
Ais the inner-product defined on H.
If it satisfies the following property, then we
call{T(n)|n≥0}is a relative (bounded linear)
operator on H:
∃ {S(m)|m∈Z}:H × H → C
(X, Y)7→ S(m)(X, Y)
is a family of semi-bilinear operator on H
s.t.S(p−q)(X, Y) =A(T(p)(X),T(q)(Y))
∀p, q∈[0, N], X, Y ∈ H,(32)
Additionally, if it satisfies the following property,
then we call {T(n)|n≥0}is a slack relative
(bounded linear) operator on H:
∃ {S(m)|m∈Z}:H × H → C
(X, Y)7→ S(m)(X, Y)
is a family of semi-bilinear operator on H
andH′⊂ H,H′̸=∅
s.t.S(p−q)(X, Y) =A(T(p)(X),T(q)(Y))
∀p, q∈[0, N], X, Y ∈ H′,(33)
Specifically, when Hrepresents our projection
space in self-attention, and {T(n)|n≥0}is a po-
sition embedding on it, such as the Rotary Position

--- PAGE 16 ---
Embedding (RoPE), we refer to it as a Slack Po-
sition Embedding (SPE) if it satisfies the property
described in Equation (33).
