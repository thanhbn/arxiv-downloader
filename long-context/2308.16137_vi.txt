LM-Infinite: Khả năng khái quát hóa độ dài cực đại không cần huấn luyện
cho các Mô hình Ngôn ngữ Lớn

Chi Han1*, Qifan Wang2, Hao Peng1, Wenhan Xiong3, Yu Chen4†, Heng Ji1, Sinong Wang3
1University of Illinois Urbana-Champaign,2Meta,3GenAI Meta,4Anytime AI
1{chihan3,haopeng,hengji}@illinois.edu ,
23{wqfcr,xwhan,sinongwang}@meta.com ,
4ychen@anytime-ai.com

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) ngày nay thường huấn luyện trên các đoạn văn bản ngắn (ví dụ: <4K token) do độ phức tạp bậc hai của kiến trúc Transformer. Do đó, hiệu suất của chúng giảm mạnh trên các đầu vào dài hơn những gì gặp phải trong quá trình huấn luyện, hạn chế đáng kể việc ứng dụng trong các tác vụ thực tế liên quan đến ngữ cảnh dài như mã hóa các bài báo khoa học, kho lưu trữ mã nguồn hoặc các cuộc đối thoại dài. Thông qua cả phân tích lý thuyết và điều tra thực nghiệm, công trình này xác định ba yếu tố chính góp phần vào thất bại khái quát hóa độ dài này. Phân tích lý thuyết của chúng tôi cho thấy các kỹ thuật thường được sử dụng như sử dụng mô hình chú ý cửa sổ trượt hoặc mã hóa vị trí tương đối không đủ để giải quyết chúng. Trả lời những thách thức này, chúng tôi đề xuất LM-Infinite, một phương pháp đơn giản và hiệu quả để tăng cường khả năng xử lý ngữ cảnh dài của LLM. LM-Infinite có tính linh hoạt cao và có thể được sử dụng với hầu hết các LLM hiện đại ngay lập tức. Không cần cập nhật tham số, nó cho phép các LLM được huấn luyện trước với các đoạn dài 2K hoặc 4K khái quát hóa đến đầu vào dài tới 200M trong khi duy trì độ phức tạp. Nó cũng cải thiện hiệu suất trên các tác vụ downstream như Passkey Retrieval và Qasper trong thiết lập zero-shot. LM-Infinite mang lại những cải thiện hiệu quả đáng kể: nó đạt được 2.7 × tăng tốc giải mã và tiết kiệm 7.5 × bộ nhớ so với mô hình gốc. Mã nguồn của chúng tôi được phát hành tại https://github.com/Glaciohound/LM-Infinite .

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) gần đây đã thúc đẩy state-of-the-art trên nhiều tác vụ xử lý ngôn ngữ tự nhiên khác nhau. Chúng thường huấn luyện trên các đoạn văn bản ít hơn 4K token (Touvron et al., 2023b; Team, 2023), chủ yếu do chi phí tính toán bậc hai trong độ dài đầu vào của kiến trúc Transformer. Do đó, chúng gặp thách thức trong việc khái quát hóa đến các đầu vào dài quá mức so với những gì được huấn luyện và chịu sự suy giảm hiệu suất đáng kể (Tworkowski et al., 2023; Chen et al., 2023a). Điều này hạn chế khả năng ứng dụng trong các tác vụ đòi hỏi ngữ cảnh tầm xa, như mã hóa các bài báo khoa học, tạo kho lưu trữ mã nguồn hoặc các cuộc đối thoại ngữ cảnh dài.

Nhiều nỗ lực đã được dành để giải quyết thách thức khái quát hóa độ dài này. Các mã hóa vị trí tương đối như RoPE (Su et al., 2021) và Alibi (Press et al., 2021) đã được các LLM state-of-the-art áp dụng rộng rãi, tính toán chú ý dựa trên khoảng cách giữa các token thay vì vị trí tuyệt đối, hy vọng tránh thất bại mô hình do embeddings vị trí tuyệt đối chưa thấy. Hơn nữa, mặc dù việc áp dụng mô hình chú ý cửa sổ trượt trên kiến trúc Transformer có thể giảm chi phí bộ nhớ (Beltagy et al., 2020; Ding et al., 2023; Zaheer et al., 2020), chúng không thể áp dụng trực tiếp cho các mô hình đã huấn luyện trước để khái quát hóa độ dài mà không cần huấn luyện thêm. Thông qua cả phân tích lý thuyết và điều tra thực nghiệm, §3 chỉ ra ba yếu tố chính làm nền tảng cho thất bại khái quát hóa độ dài: (1) thách thức xử lý khoảng cách chưa thấy giữa các token, (2) khó khăn trong việc chú ý đến số lượng token chưa thấy, và (3) thông tin vị trí tuyệt đối được mã hóa ngầm trong các token đầu tiên. Những thách thức này có thể làm cho các đặc trưng tính toán của LLM, như logits chú ý và vector ẩn, lệch khỏi phân phối huấn luyện, dẫn đến thất bại khái quát hóa độ dài. Các kỹ thuật hiện có không đủ để giải quyết những vấn đề cơ bản này.

Trả lời những thách thức này, chúng tôi đề xuất LM-Infinite, một phương pháp đơn giản và hiệu quả để tăng cường khả năng mô hình hóa ngữ cảnh dài của Transformer LLM mà không cần cập nhật tham số. LM-Infinite bao gồm hai thành phần chính được thiết kế để giảm thiểu ba yếu tố trên. (1) mặt nạ chú ý hình Λ và (2) giới hạn trần đối với khoảng cách chú ý. Thành phần đầu tiên buộc mô hình chỉ chú ý đến phần đầu của chuỗi và các token gần đây nhất trong một cửa sổ được định nghĩa trước, bỏ qua phần còn lại. Thành phần thứ hai giới hạn các giá trị khoảng cách tương đối đến mức tối đa mà mô hình đã thấy trong quá trình huấn luyện. Nó cũng có thể tùy chọn tái giới thiệu top-k token ở giữa để đạt hiệu suất tốt hơn trong một số tác vụ downstream. LM-Infinite có tính linh hoạt cao và áp dụng cho bất kỳ LLM off-the-shelf nào sử dụng mã hóa vị trí tương đối và không yêu cầu bất kỳ tinh chỉnh nào.

Các thí nghiệm của chúng tôi đánh giá toàn diện LM-Infinite trên nhiều tác vụ và LLM khác nhau. Trên ArXiv (các bài báo học thuật) và OpenWebText2 (bài đăng Reddit), LM-Infinite tạo điều kiện khái quát hóa zero-shot cho nhiều LLM đến văn bản lên tới 200M token, duy trì độ phức tạp mô hình ngôn ngữ và chất lượng tạo sinh. Không cần cập nhật tham số, LM-Infinite cải thiện điểm số so với mô hình gốc và các baseline cắt ngắn trên các tác vụ downstream bao gồm Passkey Retrieval (Mohtashami và Jaggi, 2023) và Qasper (Dasigi et al., 2021), là hai benchmark đã được thiết lập để đánh giá ngữ cảnh dài. Chúng tôi quan sát được mức tăng 37.2% trên Passkey Retrieval và mức tăng 1.2% trên Qasper trong thiết lập zero-shot. LM-Infinite cũng mang lại những cải thiện hiệu quả đáng kể: nó đạt được 2.7× tăng tốc giải mã và tiết kiệm 7.5 × bộ nhớ GPU so với các LLM gốc.

2 Bối cảnh và Công trình Liên quan

2.1 Mã hóa Vị trí Tương đối
Các mã hóa vị trí tuyệt đối truyền thống cung cấp thông tin vị trí tuyệt đối, thường với sự trợ giúp của một chuỗi vector được gọi là embeddings vị trí (Vaswani et al., 2017; Kenton và Toutanova, 2019; Ke et al., 2020). Tuy nhiên, chúng gặp khó khăn khi mô hình gặp phải các vị trí chưa thấy trong đầu vào dài hơn độ dài huấn luyện. Các mã hóa vị trí tương đối nhằm giải quyết những hạn chế của các phương pháp mã hóa vị trí thế hệ trước và xem xét khoảng cách tương đối giữa các token thay vì vị trí tuyệt đối. Các ví dụ bao gồm bias logit chú ý đã học trong T5 (Raffel et al., 2020), Transformer-XL (Dai et al., 2019), Skyformer (Chen et al., 2021), Sketching (Chen et al., 2022) và Sandwich (Chi et al., 2023), sự suy giảm chú ý tuyến tính cố định (Press et al., 2021), và xoay chuỗi query và key dựa trên khoảng cách như RoPE (Su et al., 2021; Li et al., 2023), CAPE (Likhomanenko et al., 2021) và XPos (Sun et al., 2022; Ding et al., 2023). Mặc dù có một số bằng chứng thực nghiệm đầy hứa hẹn, thất bại khái quát hóa độ dài vẫn được quan sát rộng rãi khi áp dụng trực tiếp cho các mô hình ngôn ngữ lớn (Kaiokendev, 2023). Dưới đây, chúng tôi thảo luận ngắn gọn về hai phương pháp mã hóa vị trí tương đối được sử dụng rộng rãi. Chúng đặt ra bối cảnh cần thiết cho cuộc thảo luận và thí nghiệm tiếp theo của chúng tôi.

Rotary Position Embedding (RoPE; Su et al., 2021) Nó xoay các vector key và query dựa trên vị trí trước khi tính tích vô hướng. Cụ thể, mỗi vector x (hoặc key hoặc query) được chia thành các cặp phần tử {(x0, x1),(x2, x3),···}, với mỗi cặp được hiểu là một vector 2 chiều. RoPE sau đó xoay vector (xa, xa+1) của token i với góc θa,i= iωa, trong đó ωa là tốc độ xoay liên kết với cặp chiều (a, a+ 1). Sau khi xoay, vector 2-D trở thành cosiωa−siniωa siniωacosiωa xi xi+1 . Họ chỉ ra rằng tích vô hướng giữa query đã xoay qi và key đã xoay kj được xác định hoàn toàn bởi qi,kj, và khoảng cách tương đối i−j của chúng. Chúng ta luôn có i≥j do mặt nạ chú ý causal.

AliBi (Press et al., 2021) Nó bù trừ tất cả logits chú ý giữa các token i, j bằng một hạng tuyến tính −m(i− j) và trở thành q⊤ikj−m(i−j). Để làm điều này, các mã MPT-7B triển khai ma trận bù trừ như một hạng cộng trong logits chú ý.

2.2 Nỗ lực Hướng tới Khái quát hóa Độ dài
Theo thất bại khái quát hóa được quan sát trong LLM, một giải pháp đơn giản là tinh chỉnh LLM trên các chuỗi văn bản dài hơn (Chen et al., 2023a; Tworkowski et al., 2023; Tao et al., 2023; Kiyono et al., 2021; Anil et al., 2022). Những cách tiếp cận này không giải quyết nguyên nhân cơ bản của thất bại khái quát hóa độ dài và đòi hỏi tài nguyên huấn luyện khổng lồ. Các giải pháp khác đề xuất cấp cho LLM quyền truy cập vào ngữ cảnh dài hơn mà không thực sự đọc chúng đầy đủ (Zhou et al., 2023; Bueno et al., 2022; Mohtashami và Jaggi, 2023; Yang et al., 2023). Tăng cường LLM với bộ nhớ dựa trên truy xuất (Wu et al., 2021; Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al., 2019; Kaiser et al., 2016; Yogatama et al., 2021) cũng làm cho LLM có thể áp dụng cho cơ sở dữ liệu lớn. Tuy nhiên, những thiết kế này thường cần tinh chỉnh và không tương thích trực tiếp với các LLM hiện có. Công trình của chúng tôi, ngược lại, tạo điều kiện khái quát hóa độ dài zero-shot. Một công trình tương tự khác (Ratner et al., 2023) tăng độ dài ngữ cảnh với các mô hình chú ý mà không cần huấn luyện thêm. Tuy nhiên, nó bị giới hạn trong thiết lập học trong ngữ cảnh.

3 Tại sao Transformer LLM Thất bại trong Khái quát hóa đến Ngữ cảnh Dài?

Thông qua một loạt điều tra lý thuyết và thực nghiệm, phần này nhằm xác định các nguyên nhân tiềm năng làm nền tảng cho thất bại khái quát hóa độ dài của các LLM hiện tại. Cuộc thảo luận của chúng tôi giả định các LLM dựa trên Transformer sử dụng mã hóa vị trí tương đối, vì thiết kế này được áp dụng rộng rãi trong các LLM ngày nay (Touvron et al., 2023b; Team, 2023). Chúng tôi sử dụng Llama-2 (Touvron et al., 2023b), được huấn luyện trước với các đoạn dài 4K, để điều tra. Trên các chuỗi dài hơn độ dài huấn luyện, chúng tôi sẽ chỉ ra rằng khoảng cách giữa các token chưa thấy, số lượng token được chú ý tăng lên và thông tin vị trí được mã hóa ngầm của các token bắt đầu đều có thể làm cho các đặc trưng tính toán nhất định vượt ra ngoài phân phối huấn luyện. Vì các mô hình sâu có thể nhạy cảm với sự dịch chuyển phân phối đầu vào, những yếu tố này cần được xử lý để LLM khái quát hóa đến độ dài chưa thấy.

Yếu tố 1: thách thức trong việc xử lý khoảng cách chưa thấy giữa các token Với mã hóa vị trí tương đối, tác động của vị trí lên trọng số chú ý giữa hai token chỉ phụ thuộc vào khoảng cách tương đối của chúng. Khi độ dài chuỗi tăng quá dài, một số giá trị khoảng cách sẽ vượt quá những gì thấy trong quá trình huấn luyện. Chúng tôi đưa ra tuyên bố lý thuyết không chính thức sau:

Định lý 1. (Không chính thức) Đối với cơ chế chú ý sử dụng mã hóa vị trí tương đối, các logits chú ý phải bùng nổ đến vô cực để phân biệt các khoảng cách chưa thấy trước đó khi độ dài chuỗi tăng.

Định lý chính thức và chứng minh có thể được tìm thấy trong Phụ lục C. Chúng tôi cũng xác minh thực nghiệm điều này trên Llama-2 trên bộ dữ liệu ArXiv được cắt ngắn xuống độ dài 8K. Chúng tôi trích xuất logits chú ý của tất cả các đầu chú ý và logits chú ý tối đa của chúng trên các độ dài chuỗi khác nhau trong Hình 1(a). Nó hiển thị giá trị trung bình và phương sai giữa các đầu chú ý. Chúng ta thấy rằng logits chú ý tăng lên các giá trị lớn hơn đáng kể khi độ dài chuỗi vượt quá độ dài huấn luyện 4K. Để giảm thiểu vấn đề này, chúng tôi suy đoán rằng có thể giúp giới hạn các giá trị khoảng cách tương đối đến mức tối đa mà mô hình đã thấy trong quá trình huấn luyện (tức là, giới hạn trần khoảng cách). Tuy nhiên, như chúng ta sẽ thấy từ mệnh đề dưới đây, việc giải quyết bùng nổ logit dẫn đến một thách thức khác.

Yếu tố 2: chú ý đến số lượng token chưa thấy Trên các chuỗi dài hơn, các token ở vị trí sau phải phân phối trọng số chú ý trên một ngữ cảnh lớn hơn. Sau đó chúng tôi đưa ra tuyên bố sau rằng, nếu logits chú ý bị giới hạn nhưng số lượng token cần chú ý không bị hạn chế, nó có thể khiến entropy chú ý tăng vượt ra ngoài phạm vi huấn luyện:

Mệnh đề 1. Nếu logits chú ý bị giới hạn, khi chuỗi trở nên dài hơn, entropy chú ý tăng đến vô cực.

Một tuyên bố chính thức cũng như chứng minh có thể được tìm thấy trong Phụ lục D. Kết luận này được xác minh thêm bằng thực nghiệm bằng cách vẽ entropy chú ý theo độ dài ngữ cảnh trong Hình 1(b). Đường cong cho thấy entropy chú ý luôn tăng. Xu hướng này, mặc dù tăng logarit, vẫn gây hại cho hiệu suất của LLM, như chúng tôi sẽ minh họa trong nghiên cứu ablation trong §5.2 và Hình 5. Điều này cho thấy rằng chúng ta nên giới hạn kích thước ngữ cảnh chú ý để đảm bảo rằng entropy chú ý ở trong phạm vi đã thấy trong quá trình huấn luyện trước và tránh đầu ra suy thoái. Một chú ý cửa sổ đơn giản, trong đó mỗi token chỉ chú ý đến các token gần nhất trong một khoảng cách, có thể xử lý yếu tố 1 và 2. Điều này tương tự như mặt nạ chú ý block-diagonal được sử dụng trong XPos (Sun et al., 2022) và Longformer (Beltagy et al., 2020). Tuy nhiên, như chúng tôi sẽ chỉ ra trong đoạn tiếp theo, điều này giới thiệu một yếu tố khác cũng có thể làm thất bại LLM.

Yếu tố 3: các token bắt đầu chiếm một không gian đặc trưng riêng biệt Có lẽ trái với trực giác:

Quan sát 1. Ngay cả khi không có embeddings vị trí tuyệt đối rõ ràng, đầu ra chú ý của vài token đầu tiên có thể chiếm một không gian biểu diễn riêng biệt so với các vị trí khác. Do đó, khi được truyền đến các lớp sau, những token bắt đầu này có các vector giá trị riêng biệt từ đầu ra lớp thấp hơn của chúng.

Điều này theo từ Định lý 1 trong Kazemnejad et al. (2023), chứng minh rằng các vị trí tuyệt đối có thể được mã hóa ngầm trong đầu ra của các token của một lớp chú ý duy nhất, ngay cả khi không có mã hóa vị trí. Trong cấu trúc của họ, tín hiệu của các token bắt đầu là mạnh nhất và dễ phân biệt nhất với các token khác. Vì mã hóa vị trí tương đối có tính biểu đạt hơn hẳn so với thiết lập không có mã hóa vị trí trong Kazemnejad et al. (2023) (ví dụ: bằng cách để tất cả khoảng cách có cùng hàm chú ý), kết luận tương tự áp dụng cho mã hóa vị trí tương đối.

Như một xác minh thực nghiệm, chúng tôi lấy các trạng thái ẩn được đầu ra bởi lớp thứ hai của Llama-2 và vẽ một phép chiếu Phân tích Thành phần Chính (PCA) vào mặt phẳng 2-d trong Hình 1(c). Thêm hình cho các lớp khác có thể được tìm thấy trong §E. Các chấm tương ứng với 4096 token đầu tiên trong 32 chuỗi, với những chấm xanh tương ứng với các token đầu tiên và token đỏ là những token cuối. Hai vùng xanh ở trung tâm phía trên và phía dưới bên phải tương ứng với các token đầu tiên có độ tập trung cao (vị trí của chúng khoảng 0 ∼25) và rất xa các token sau. Vùng dưới-trái chứa các token chồng chéo còn lại trong một chuỗi (phóng to vào hộp khác). Biểu đồ cho thấy rằng các biểu diễn vector của các token đầu tiên tập trung vào các vùng trong không gian đặc trưng riêng biệt với các token còn lại. Phát hiện mới này tiết lộ một khuyết điểm cơ bản của mô hình chú ý cửa sổ trượt, hạn chế chú ý vào các token gần đây nhất trong kích thước cửa sổ được định nghĩa trước, một baseline được áp dụng rộng rãi gần đây (Beltagy et al., 2020; Ding et al., 2023; Zaheer et al., 2020). Vì chú ý về cơ bản là trung bình có trọng số trên các vector giá trị, chú ý cửa sổ trượt loại bỏ các token đầu tiên, khiến đầu ra chú ý không thể đạt đến các vùng mà các vector giá trị của các token đầu tiên chiếm. Điều này buộc mô hình xử lý một vùng khác trong quá trình tính toán, giới thiệu thêm các thách thức khái quát hóa. Như một giải pháp đơn giản cho vấn đề này, các token đầu tiên cần được giữ trong tính toán chú ý.

4 Đề xuất của chúng tôi: LM-Infinite

Lấy cảm hứng từ các phân tích và thông điệp rút ra trong phần trước, chúng tôi đề xuất LM-Infinite để đạt được khái quát hóa độ dài zero-shot cho LLM. Tổng quan về LM-Infinite được minh họa trong Hình 2(a). Giải pháp đơn giản này bao gồm hai thành phần: mặt nạ chú ý hình Λ và giới hạn trần khoảng cách. Bên cạnh đó, việc tái giới thiệu các token top-k ở giữa là tùy chọn để tăng cường hiệu suất downstream.

Mặt nạ chú ý hình Λ Nó chứa hai khoảng chú ý: khoảng bắt đầu cho phép mỗi token chú ý đến nstarting token đầu tiên nếu chúng đến trước token hiện tại; khoảng kết thúc cho phép mỗi token chú ý đến Lpretrain token gần đây nhất. Lpretrain là độ dài tối đa trong quá trình huấn luyện. Các token khác bị bỏ qua. Trong nghiên cứu ablation trong §A, chúng tôi thấy rằng việc chọn nstarting∈[5,100] thường đạt được hiệu suất tốt như nhau. Lưu ý rằng nstarting = 0 (tức là chỉ chú ý đến các token gần đây nhất) gây hại đáng kể đến hiệu suất. Điều này giải quyết Yếu tố 2 và 3 trong §3 bằng cách cả giới hạn số lượng token dưới sự chú ý và đảm bảo vài token bắt đầu được chú ý.

Giới hạn trần khoảng cách LM-Infinite tiếp tục giới hạn "khoảng cách hiệu quả" đến Lpretrain. Điều này chỉ ảnh hưởng đến vài token bắt đầu khi được chú ý bởi các token ở vị trí sau. Cụ thể, trong mã hóa vị trí tương đối, logit chú ý gốc là w(q,k, d), trong đó d là khoảng cách giữa hai token. Bây giờ chúng tôi sửa đổi nó thành w(q,k, d′)) trong đó d′= min( d, L pretrain ). Hình 2(a) hiển thị một ví dụ minh họa trong đó giới hạn trần khoảng cách là Lpretrain = 2. Điều này giải quyết Yếu tố 1 trong §3 bằng cách giới hạn giá trị khoảng cách trong tính toán chú ý.

Tùy chọn chú ý đến các token top-k ở giữa LM-Infinite có thể tùy chọn chú ý đến k token ở giữa với logits chú ý lớn nhất. Điều này đặc biệt hữu ích trong các tác vụ downstream nơi thông tin trong các token giữa quan trọng (§5.3). Ở đây các k token được chọn độc lập cho mỗi đầu chú ý trong các lớp cao hơn lớp h-th, và có khoảng cách chú ý d=1 2Lpre-train. Các siêu tham số này được chọn dựa trên tập xác thực Passkey Retrieval held-out, nơi chúng tôi đặt k= 5 và h= 5, với chi tiết hơn trong Phụ lục A. Việc chọn k và h của chúng tôi không phụ thuộc vào điều chỉnh cụ thể theo tác vụ, và trong các thí nghiệm của chúng tôi, chúng tôi áp dụng cùng bộ siêu tham số này trong tất cả các tác vụ downstream khác và đạt được những cải thiện nhất quán. Những token trung gian này không gây hại hiệu suất. Thay vào đó, trong đánh giá các tác vụ downstream trong §5.3, các token trung gian hữu ích hơn và việc chú ý chọn lọc đến các token top-k mang lại những cải thiện hiệu suất đáng kể với ít tác động đến hiệu quả. Tuy nhiên, đối với việc tạo sinh và suy luận LLM, chúng tôi thấy các token trung gian không cần thiết để chú ý để LM-Infinite đạt được độ phức tạp hoặc chất lượng tạo sinh tốt.

Mặt nạ hình Λ của LM-Infinite về mặt khái niệm tương tự như các mô hình chú ý được suy ra từ heuristics (Beltagy et al., 2020; Ding et al., 2023; Zaheer et al., 2020). Tuy nhiên, chúng tôi chỉ ra chính thức trong §3 Yếu tố 3 rằng những cách tiếp cận trước đây này về mặt lý thuyết không thể khái quát hóa đến độ dài chưa thấy mà yêu cầu cập nhật tham số. Hạn chế vốn có này thúc đẩy hai thành phần khác trong LM-Infinite để đạt được khái quát hóa độ dài zero-shot.

Chi tiết triển khai LM-Infinite có thể áp dụng trong tất cả các mô hình Transformer với mã hóa vị trí tương đối. Người ta chỉ cần thay thế hàm chú ý trong mỗi lớp Transformer với LM-Infinite mà không cần cập nhật tham số nào. Mặt nạ chú ý hình Λ tương đối đơn giản để triển khai. Trong RoPE, logits chú ý trong khoảng chú ý kết thúc theo tính toán gốc. Trong khoảng chú ý bắt đầu (loại trừ sự chồng chéo với khoảng kết thúc), chúng tôi giữ tất cả vector k không xoay và xoay tất cả vector q đến khoảng cách cố định Lpretrain. Sau đó logits trong hai khoảng có thể được kết hợp. Tăng cường AliBi với LM-Infinite cũng đơn giản. Chúng tôi chỉ đơn giản cắt ma trận bù trừ với giá trị tối thiểu −|mL pretrain| và áp dụng mặt nạ chú ý hình Λ.

Thảo luận. Trong Hình 2(b), chúng tôi hiển thị một mô hình khái niệm về cách mã hóa vị trí tương đối hoạt động. Mô hình khái niệm này phản ánh các lựa chọn thiết kế của LM-Infinite. Trong mô hình khái niệm này, một ngữ cảnh dài có thể được phân chia thành 3 phần: Các token bắt đầu mã hóa thông tin vị trí tuyệt đối mạnh (Yếu tố 3). Như được giải thích trong §3, chúng cần thiết để chú ý vì các đặc trưng của chúng chiếm một vùng riêng biệt trong không gian đặc trưng. Vì chú ý về cơ bản là trung bình có trọng số trên các vector vi, mà không có vài token bắt đầu, đầu ra chú ý không thể đạt đến các vùng mà các vector vi của các token đầu tiên chiếm. Các token cuối cung cấp chủ yếu vị trí tương đối của chúng đến các token cuối cùng. Tầm quan trọng của chúng có lẽ phát sinh từ "bias gần đây" (Peysakhovich và Lerer, 2023) được học bởi LLM trong quá trình huấn luyện trước. Các token giữa mã hóa thông tin ít nhạy cảm với vị trí. Như được phân tích trong Yếu tố 2, việc bao gồm quá nhiều token trung gian gây hại nhiều hơn lợi cho khái quát hóa độ dài.

5 Đánh giá

Chúng tôi đánh giá LM-Infinite với LLaMA-7B (Touvron et al., 2023a), Llama-2-7b (Touvron et al., 2023b), MPT-7B (Team, 2023), và GPT-J-6B (Wang và Komatsuzaki, 2021). LLaMA-7B và GPT-J-6B được huấn luyện trước với độ dài 2K và các mô hình khác được huấn luyện trước với độ dài 4K. LLaMA, Llama-2, và GPT-J sử dụng mã hóa RoPE, và MPT-7B sử dụng mã hóa Alibi. MPT-7B-Storywriter (được tinh chỉnh trên các chuỗi dài) được sử dụng như một trong các baseline.

5.1 Mô hình Ngôn ngữ với Ngữ cảnh Cực dài

Chúng tôi sử dụng các tập dữ liệu ArXiv và OpenWebText2 từ bộ dữ liệu Pile (Gao et al., 2020), chứa các bài báo preprint từ ArXiv và các bài gửi Reddit, tương ứng. Chúng tôi đánh giá với negative log-likelihood (NLL) và perplexity (exponential của NLL). Hình 3 vẽ các đường cong NLL trên bộ dữ liệu ArXiv. Ở đây, chúng tôi chia nhỏ hiệu suất perplexity của các mô hình theo vị trí để đường cong hiển thị NLL mà mô hình đạt được xung quanh vị trí cụ thể đó, được tính trung bình trên tất cả các chuỗi được đánh giá. Llama-2 xuất ra xác suất NaN trên các chuỗi dài hơn một chút so với 10K, do đó đường cong ngắn hơn. Tất cả các mô hình vanilla hết bộ nhớ ở độ dài ∼32K.1 NLL của các baseline nhanh chóng bùng nổ khi các chuỗi được kiểm tra dài hơn những gì chúng huấn luyện. Với LM-Infinite, tất cả các mô hình có thể khái quát hóa đến các chuỗi dài hơn đáng kể so với độ dài chúng được huấn luyện, duy trì hiệu suất NLL. Điều này xác nhận phân tích yếu tố thất bại độ dài của chúng tôi trong §1. Phần cuối dài hơn của các đường cong có phương sai lớn hơn do ít tài liệu có độ dài đó. Trong Hình 4, chúng tôi tiếp tục đánh giá LM-Infinite + Llama-2 trên một chuỗi 200M token, được xây dựng bằng cách lấy mẫu với thay thế từ bộ dữ liệu ArXiv và nối tất cả dữ liệu. LM-Infinite cho thấy khả năng duy trì mức log-perplexity thấp ổn định trên độ dài cực đại.

Bảng 1 tóm tắt hiệu suất perplexity tại một vài độ dài mốc (2K, 4K, 8K, 16K, và 32K) trên ArXiv và OpenWebText2, cho thấy xu hướng tương tự. OpenWebText2 có rất ít trường hợp dữ liệu trên độ dài 32K, nên chúng tôi bỏ qua cột. Với LM-Infinite, tất cả các mô hình có thể khái quát hóa đến độ dài chưa thấy, và LM-Infinite đạt perplexity tốt nhất trong 7 trên 9 trường hợp. Trên LLaMA + LM-Infinite, perplexity giảm khi độ dài tăng và vị trí trở nên lớn hơn. Đáng ngạc nhiên, mà không cần cập nhật tham số, LM-Infinite vượt trội hơn nhiều baseline mạnh được huấn luyện trên các đoạn văn bản dài hơn đáng kể. Như một so sánh trực tiếp, MPT-7B+LM-Infinite đạt hiệu suất chỉ hơi kém hơn đối tác được tinh chỉnh của nó, MPT-7B-Storywriter. Điều này xác nhận rằng LM-Infinite là một giải pháp thay thế đầy hứa hẹn cho việc tinh chỉnh tốn tài nguyên.

5.2 Nghiên cứu Ablation

Hình 5 cung cấp một nghiên cứu ablation với mô hình LLaMA trên bộ dữ liệu ArXiv về lý do tại sao cả hai thành phần trong LM-Infinite đều cần thiết để duy trì chức năng LLM trên độ dài 8K. Chúng tôi so sánh LM-Infinite với các biến thể của nó để hiển thị hiệu quả của thiết kế và cũng để xác nhận các yếu tố trong §3. Trong tất cả các đường cong, chỉ có LM-Infinite có log-perplexity tương đối ổn định, có nghĩa là các thành phần trong LM-Infinite đều cần thiết cho khái quát hóa độ dài thành công. Mô hình LLM vanilla (đường cong "vanilla") thất bại ngay lập tức với NLL bùng nổ. Nếu chúng tôi chỉ áp dụng mặt nạ hình Λ (đường cong "Λ") và không giới hạn khoảng cách giữa các token (Yếu tố 1), NLL vẫn bùng nổ ngay lập tức sau độ dài huấn luyện trước. Đường cong "ceiling" chỉ áp dụng kỹ thuật giới hạn trần khoảng cách nhưng không có mặt nạ hình Λ để giới hạn số lượng token được chú ý. Hiệu suất vẫn suy thoái (được chứng minh bằng NLL luôn tăng). Điều này xác nhận sự tồn tại của Yếu tố 2, quá nhiều token cần chú ý, vẫn gây hại cho hiệu suất của LLM. Đường cong "window" hiển thị baseline với mô hình chú ý cửa sổ trượt, chỉ chú ý đến các token gần đây nhất trong cửa sổ trượt mà không thay đổi văn bản đầu vào. Nó tạo ra giá trị NLL tệ thứ hai, cho thấy sự suy giảm hiệu suất và độ trôi chảy đáng kể. Điều này xác nhận phân tích lý thuyết của chúng tôi về yếu tố 3. Do hiệu suất rõ ràng tệ hơn nhiều, chúng tôi loại trừ nó khỏi các đánh giá khác.

Một baseline tương tự khác với "window" là baseline cắt ngắn, loại bỏ hoàn toàn các token dư thừa khi ngữ cảnh dài hơn độ dài huấn luyện trước của mô hình, chỉ giữ lại những token gần đây nhất. Quá trình cắt ngắn này xảy ra trước khi bước forward bắt đầu và về cơ bản loại bỏ văn bản đã cắt khỏi đầu vào cho mô hình mà không thay đổi cơ chế chú ý. Chúng tôi đã so sánh baseline này ở hai nơi trong bài báo. Trong §5.3 và Bảng 2, LM-infinite vượt trội hơn baseline này trên các tác vụ downstream. Trong Phần 5.4 và Hình 6, LM-infinite đạt được sự cân bằng tốt hơn giữa độ phức tạp tính toán và chất lượng tạo sinh so với baseline này.

5.3 Đánh giá Downstream

Vì LLM thường được triển khai cho các tác vụ downstream, chúng tôi đánh giá cách LM-Infinite hoạt động trên hai tác vụ đầu vào dài trong thiết lập zero-shot: Passkey Retrieval (Mohtashami và Jaggi, 2023) và Qapser (Dasigi et al., 2021). Passkey Retrieval chôn một passkey tại vị trí ngẫu nhiên trong văn bản phân tâm dài và cuối cùng hỏi passkey là gì. Qasper là một bộ dữ liệu hỏi-đáp trên các bài báo khoa học với tổng cộng 1.5K cặp câu hỏi-câu trả lời kiểm tra. Chúng tôi đánh giá Llama-2-7b-chat, vì việc điều chỉnh hướng dẫn của nó cho phép khả năng giải quyết tác vụ tốt (Bai et al., 2023), với top-5 token giữa được kích hoạt trên lớp cao hơn lớp thứ 5 (xem §4 để định nghĩa và Phụ lục A để chọn siêu tham số). Kết quả được liệt kê trong Bảng 2. LM-Infinite liên tục vượt trội hơn các baseline trên cả hai tác vụ, với mức tăng 37.2 phần trăm trên Passkey Retrieval và mức tăng 1.2 phần trăm trên tác vụ Qasper. Passkey retrieval định vị thông tin hữu ích đồng đều trong một chuỗi, nên hiệu suất của baseline cắt ngắn phụ thuộc phần lớn vào việc liệu phần còn lại có bao gồm passkey hay không. Trên Qasper, chú ý top-k cần thiết để đạt hiệu suất tốt, điều này cho thấy rằng thông tin quan trọng tương tự ở giữa cần được chú ý. Điều này cho thấy rằng nó có thể cải thiện hiệu suất tác vụ downstream trên đầu vào dài mà không cần tinh chỉnh trong khi mô hình vanilla thất bại ngay lập tức.

5.4 Chất lượng Tạo sinh

Chúng tôi tiếp tục đánh giá chất lượng tạo sinh của LM-Infinite trên các tập kiểm tra ArXiv và OpenWebText2, với BLEU (Papineni et al., 2002) và ROUGE (Lin, 2004) (ROUGE-L). Chúng tôi để LLM tạo ra 100 token sau mỗi độ dài mốc và sử dụng 100 token tiếp theo trong văn bản gốc làm tham chiếu. Vì việc tạo sinh tốn thời gian, chúng tôi lấy mẫu 100 chuỗi dài để đánh giá cho mỗi bộ dữ liệu. Kết quả được tóm tắt trong Bảng 3. Xu hướng tương tự như trong phần cuối: không cần cập nhật tham số, LM-Infinite thành công cho phép LLM duy trì chất lượng tạo sinh của chúng trong khi tạo ra các chuỗi dài hơn huấn luyện, có thể so sánh với các baseline được tinh chỉnh như MPT-7B-SW. Kết quả tạo sinh từ các LLM vanilla kém và chứa chủ yếu văn bản vô nghĩa, dẫn đến nhiều điểm số gần bằng không. Đối với một số điểm BLEU, nó cho ra zero {2,3,4}-gram chồng chéo với văn bản tham chiếu. Vì BLEU là trung bình hình học có trọng số trên {1,2,3,4}-gram precisions, điểm BLEU cuối cùng cho những cột đó là 0. Bảng 8 Phụ lục trình bày một số ví dụ đầu ra tạo sinh có thể cung cấp hình ảnh tốt về chất lượng tạo sinh. Chúng tôi cũng đánh giá hiệu quả trong Phụ lục G: với các chuỗi dài 32K, LM-Infinite đạt được 2.7 × tăng tốc giải mã và tiết kiệm 7.5 × bộ nhớ GPU.

Một vài ví dụ tạo sinh được hiển thị trong Phụ lục H. Chúng tôi cũng so sánh LM-Infinite với baseline đơn giản dựa trên cắt ngắn bằng cách liên tục cắt ngắn ngữ cảnh dư thừa. Tuy nhiên, khi độ dài tạo sinh tăng, việc cắt ngắn thường xuyên và mã hóa lại ngữ cảnh mới là cần thiết. Cửa sổ cắt ngắn càng lớn, càng nhiều ngữ cảnh được giữ, nhưng chi phí tính toán càng lớn. Chúng tôi để các mô hình tạo ra 10k token trên ArXiv. Trong Hình 6, rõ ràng là LM-Infinite đạt được sự cân bằng chất lượng-hiệu quả tốt hơn đáng kể. Với tính toán tương tự, LM-Infinite vượt trội hơn baseline khoảng 5 BLEU. Để đạt được BLEU tương tự, LM-Infinite chỉ phát sinh <25% chi phí tính toán so với baseline cắt ngắn.

6 Kết luận và Nghiên cứu Tương lai

Công trình này đề xuất một phương pháp khái quát hóa độ dài zero-shot cho nhiều LLM off-the-shelf khác nhau mà không cần cập nhật tham số. Thông qua phân tích lý thuyết và điều tra thực nghiệm, công trình này xác định ba yếu tố chính góp phần vào thất bại khái quát hóa độ dài này. Phân tích lý thuyết của chúng tôi tiếp tục tiết lộ tại sao việc cắt ngắn cửa sổ chú ý và mã hóa vị trí tương đối không đủ để giải quyết chúng. Giải pháp của chúng tôi, LM-Infinite, là một phương pháp đơn giản và hiệu quả để tăng cường khả năng xử lý ngữ cảnh dài của LLM. Nó cho phép các LLM được huấn luyện trước với các đoạn dài 2K hoặc 4K khái quát hóa đến đầu vào dài tới 200M trong khi duy trì perplexity. Nó cũng cải thiện hiệu suất trên các tác vụ downstream như Passkey Retrieval và Qasper trong thiết lập zero-shot. Nó mang lại những cải thiện hiệu quả đáng kể: 2.7 × tăng tốc giải mã và tiết kiệm 7.5 × bộ nhớ so với mô hình gốc. Hiệu quả tính toán và tính dễ sử dụng của LM-Infinite cho phép các nhà nghiên cứu không có tài nguyên tính toán khổng lồ sử dụng LLM trên các chuỗi dài. Nghiên cứu tương lai có thể điều tra xem những kỹ thuật này có cho phép huấn luyện trước và tinh chỉnh LLM hiệu quả và hiệu quả hơn hay không. Một hướng khác là áp dụng LM-Infinite cho các ứng dụng như lý luận dài, đối thoại dài, tạo sinh tăng cường truy xuất hoặc tạo sinh văn học dài.

Hạn chế

Công trình này đánh giá một loạt LLM miền mở rộng. Tuy nhiên, mà không có quyền truy cập vào mã nguồn của các LLM độc quyền như ChatGPT, phương pháp được đề xuất không thể được đánh giá trên chúng. Hơn nữa, do tài nguyên tính toán và thời gian hạn chế, phương pháp được đề xuất chưa được đánh giá trên văn bản với độ dài lớn hơn nữa, như 1G. Mô hình được thiết kế trên các mô hình Transformer mã hóa vị trí tương đối, là backbone chính của hầu hết các LLM hiện đại. Câu hỏi về cách LM-Infinite có thể cho phép tinh chỉnh hoặc huấn luyện trước hiệu quả hơn cũng có thể được khám phá trong nghiên cứu tương lai.

Lời cảm ơn

Nghiên cứu này được hỗ trợ một phần bởi Chương trình KAIROS của U.S. DARPA Số FA8750-19-2-1004, và Chương trình INCAS của DARPA Số HR001121C0165. Các quan điểm và kết luận được chứa ở đây là của các tác giả và không nên được hiểu là nhất thiết đại diện cho các chính sách chính thức, được thể hiện rõ ràng hay ngụ ý, của DARPA, hoặc Chính phủ Hoa Kỳ. Chính phủ Hoa Kỳ được ủy quyền sao chép và phân phối các bản in lại cho mục đích chính phủ, bất chấp bất kỳ chú thích bản quyền nào trong đó.

[Phần còn lại của tài liệu bao gồm các tài liệu tham khảo, phụ lục và chi tiết kỹ thuật được dịch tương tự theo cấu trúc gốc]
