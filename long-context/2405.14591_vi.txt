# Base của RoPE Giới hạn Độ dài Ngữ cảnh

Xin Men∗
Baichuan Inc.
Mingyu Xu∗
Baichuan Inc.
Bingning Wang∗†
Baichuan Inc.

Qingyu Zhang
ISCAS
Hongyu Lin
ISCAS
Xianpei Han
ISCAS

Weipeng Chen
Baichuan Inc.

## Tóm tắt
Position embedding là một thành phần cốt lõi của các Mô hình Ngôn ngữ Lớn (LLMs) hiện tại. Rotary position embedding (RoPE), một kỹ thuật mã hóa thông tin vị trí bằng ma trận xoay, đã trở thành lựa chọn thực tế cho position embedding trong nhiều LLMs, như series Llama. RoPE đã được sử dụng thêm để mở rộng khả năng ngữ cảnh dài, chủ yếu dựa trên việc điều chỉnh tham số base của RoPE để giảm thiểu các vấn đề out-of-distribution (OOD) trong position embedding. Tuy nhiên, trong bài báo này, chúng tôi phát hiện rằng LLMs có thể có được khả năng ngữ cảnh dài bề mặt dựa trên lý thuyết OOD. Chúng tôi xem xét lại vai trò của RoPE trong LLMs và đề xuất một tính chất mới về long-term decay, chúng tôi suy ra rằng base của RoPE giới hạn độ dài ngữ cảnh: có một cận dưới tuyệt đối cho giá trị base để có được khả năng độ dài ngữ cảnh nhất định. Công trình của chúng tôi tiết lộ mối quan hệ giữa độ dài ngữ cảnh và base RoPE cả về mặt lý thuyết và thực nghiệm, có thể soi sáng cho việc huấn luyện ngữ cảnh dài trong tương lai.

[Hình 1: Độ dài ngữ cảnh và cận dưới tương ứng của giá trị base của RoPE.]

∗Đóng góp bằng nhau
†Tác giả liên hệ, daniel@baichuan-inc.com

arXiv:2405.14591v1 [cs.CL] 23 May 2024

## 1 Giới thiệu

Trong vài năm qua, các mô hình ngôn ngữ lớn đã thể hiện khả năng đáng ngạc nhiên và trải qua sự phát triển nhanh chóng. Đến nay, LLMs đã được áp dụng rộng rãi trong nhiều lĩnh vực khác nhau, bao gồm chatbots, intelligent agents và code assistants (Achiam et al., 2023; Jiang et al., 2023b). Transformer (Vaswani et al., 2017), dựa trên cơ chế attention, đã trở thành backbone phổ biến nhất của LLMs do hiệu suất tốt và tính chất mở rộng (Tay et al., 2022). Một trong những module thành phần chính trong Transformer là position embedding, được giới thiệu để nhúng thông tin vị trí quan trọng cho việc xử lý dữ liệu tuần tự. Rotary position embedding (RoPE), mã hóa thông tin khoảng cách tương đối dưới dạng absolute position embedding (Su et al., 2024), đã là lựa chọn phổ biến và được áp dụng trong nhiều LLMs (Touvron et al., 2023a; Yang et al., 2023; Bai et al., 2023).

RoPE không giới thiệu tham số huấn luyện nào và cho thấy cải thiện trong language modeling và nhiều tác vụ khác (Su et al., 2024; Heo et al., 2024). Một lý do mà RoPE được sử dụng rộng rãi là khả năng ngoại suy độ dài ngữ cảnh (Peng et al., 2023b; Chen et al., 2023), mở rộng độ dài ngữ cảnh của LLM đã được huấn luyện mà không cần huấn luyện lại tốn kém. Trong thực tế, nhiều công trình (Touvron et al., 2023a; Liu et al., 2024a; Young et al., 2024) đã thành công mở rộng độ dài cửa sổ bằng cách đơn giản tăng giá trị base, tham số siêu duy nhất trong RoPE, và fine-tuning trên các văn bản dài.

Lý do đằng sau thành công của các mở rộng ngữ cảnh dài này thường được giải thích là tránh các góc xoay out-of-distribution (OOD) (Liu et al., 2024b; Han et al., 2023) trong RoPE, có nghĩa là độ dài ngữ cảnh mở rộng (OOD) có thể được ánh xạ tới độ dài ngữ cảnh in-distribution đã được huấn luyện đúng cách. Dựa trên lý thuyết OOD, một nghiên cứu gần đây (Liu et al., 2024b) phát hiện rằng base nhỏ hơn có thể giảm thiểu OOD và có lợi cho khả năng xử lý ngữ cảnh dài của mô hình, điều này truyền cảm hứng cho chúng tôi nghiên cứu thêm về mối quan hệ giữa base của RoPE và độ dài ngữ cảnh mà mô hình có thể xử lý.

Trong bài báo này, chúng tôi phát hiện rằng mô hình có thể có khả năng ngữ cảnh dài bề mặt với giá trị base RoPE không phù hợp, trong trường hợp này mô hình chỉ có thể duy trì perplexity thấp nhưng mất khả năng truy xuất thông tin ngữ cảnh dài. Chúng tôi cũng cho thấy rằng lý thuyết out-of-distribution (OOD) trong position embedding, động lực cho hầu hết các công trình ngoại suy độ dài (Peng et al., 2023b; Chen et al., 2023; Liu et al., 2024b), không đủ để phản ánh đầy đủ khả năng xử lý ngữ cảnh dài của mô hình. Do đó, chúng tôi xem xét lại vai trò của RoPE trong LLMs và suy ra một tính chất mới về long-term decay trong RoPE: khả năng chú ý nhiều hơn đến các token tương tự hơn là các token ngẫu nhiên giảm dần khi khoảng cách tương đối tăng. Trong khi các công trình ngữ cảnh dài trước đây thường tập trung vào tỷ lệ tương đối của base RoPE, dựa trên lý thuyết của chúng tôi, chúng tôi suy ra một cận dưới tuyệt đối cho giá trị base của RoPE để có được khả năng độ dài ngữ cảnh nhất định, như được hiển thị trong Hình 1. Để xác minh lý thuyết của chúng tôi, chúng tôi đã tiến hành các thí nghiệm kỹ lưỡng trên nhiều LLMs khác nhau như Llama2-7B (Touvron et al., 2023b), Baichuan2-7B (Yang et al., 2023) và một mô hình 2 tỷ tham số mà chúng tôi huấn luyện từ đầu, chứng minh rằng cận dưới này không chỉ áp dụng ở giai đoạn fine-tuning mà còn ở giai đoạn pre-training.

Chúng tôi tóm tắt các đóng góp của bài báo như sau:
• **Góc độ lý thuyết**: chúng tôi suy ra một tính chất mới về long-term decay trong RoPE, chỉ ra khả năng chú ý nhiều hơn đến các token tương tự hơn là các token ngẫu nhiên của mô hình, đây là góc độ mới để nghiên cứu khả năng ngữ cảnh dài của LLMs.
• **Cận dưới của Base RoPE**: để đạt được khả năng độ dài ngữ cảnh mong đợi, chúng tôi suy ra một cận dưới tuyệt đối cho base của RoPE theo lý thuyết của chúng tôi. Tóm lại, base của RoPE giới hạn độ dài ngữ cảnh.
• **Khả năng bề mặt**: chúng tôi tiết lộ rằng nếu base của RoPE nhỏ hơn cận dưới, mô hình có thể có được khả năng ngữ cảnh dài bề mặt, có thể duy trì perplexity thấp nhưng mất khả năng truy xuất thông tin từ ngữ cảnh dài.

## 2 Kiến thức nền

Trong phần này, chúng tôi trước tiên giới thiệu Transformer và RoPE, được sử dụng phổ biến nhất trong các LLMs hiện tại. Sau đó chúng tôi thảo luận về các phương pháp ngữ cảnh dài dựa trên lý thuyết OOD của góc xoay.

### 2.1 Attention và RoPE

Các LLMs hiện tại chủ yếu dựa trên Transformer (Vaswani et al., 2017). Thành phần cốt lõi của nó là tính toán cơ chế attention. Attention ngây thơ có thể được viết là:

$A_{ij} = q_i^T k_j$ (1)

$\text{ATTN}(X) = \text{softmax}(A/\sqrt{d})v$, (2)

trong đó $A \in \mathbb{R}^{L \times L}$, $q, k, v \in \mathbb{R}^d$. Position embedding được giới thiệu để sử dụng thứ tự của chuỗi trong attention.

RoPE (Su et al., 2024) thực hiện relative position embedding thông qua absolute position embedding, áp dụng ma trận xoay vào tính toán điểm attention trong Eq. 1, có thể được viết là:

$A_{ij} = (R_{i,\theta} q_i)^T (R_{j,\theta} k_i) = q_i^T R_{j-i,\theta} k_j = q_i^T R_{m,\theta} k_j$, (3)

trong đó $m = j - i$ là khoảng cách tương đối của $i$ và $j$, $R_{m,\theta}$ là ma trận xoay được ký hiệu là:

$$\begin{pmatrix}
\cos(m\theta_0) & -\sin(m\theta_0) & 0 & 0 & \cdots & 0 & 0 \\
\sin(m\theta_0) & \cos(m\theta_0) & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos(m\theta_1) & -\sin(m\theta_1) & \cdots & 0 & 0 \\
0 & 0 & \sin(m\theta_1) & \cos(m\theta_1) & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos(m\theta_{d/2-1}) & -\sin(m\theta_{d/2-1}) \\
0 & 0 & 0 & 0 & \cdots & \sin(m\theta_{d/2-1}) & \cos(m\theta_{d/2-1})
\end{pmatrix}$$ (4)

Nói chung, việc lựa chọn góc xoay thỏa mãn $\theta_i = \text{base}^{-2i/d}$, giá trị base điển hình cho các LLMs hiện tại là 10.000, và base của RoPE trong LLMs được hiển thị trong Bảng 1.

**Bảng 1: Cài đặt base của RoPE và độ dài ngữ cảnh trong các LLMs khác nhau.**

| Model | Llama-7B | Llama2-7B | Llama3-8B | Mistral-7B-v0.2 | Baichuan2-7B |
|-------|----------|------------|------------|-----------------|--------------|
| Base | 10,000 | 10,000 | 500,000 | 1,000,000 | 10,000 |
| Length | 2,048 | 4,096 | 8,192 | 32,768 | 4,096 |

### 2.2 Lý thuyết OOD của góc xoay tương đối

Dựa trên RoPE, các nhà nghiên cứu đã đề xuất nhiều phương pháp khác nhau để mở rộng khả năng ngữ cảnh dài của LLMs, trong đó đại diện là PI (Chen et al., 2023) và NTK-series (NTK-aware (bloc97, 2023), YaRN (Peng et al., 2023b), và Dynamical-NTK (emozilla, 2023)). Những phương pháp đó phụ thuộc vào tỷ lệ tương đối $s = T_{new}/T_{origin}$, trong đó $T_{origin}$ là độ dài huấn luyện của mô hình pre-trained gốc và $T_{new}$ là độ dài huấn luyện trong long-context fine-tuning.

**PI** PI trực tiếp nội suy position embedding, và tính toán của $A_{ij}$ trở thành:
$A_{ij} = (R_{i/s} q_i)^T (R_{j/s} k_i) = q_i^T R_{(j-i)/s} k_j = q_i^T R_{m/s} k_j$, (5)

Nói cách khác, position embedding của token tại vị trí $i$ trong pre-training trở thành $i/s$ trong fine-tuning, đảm bảo phạm vi position embedding của ngữ cảnh dài hơn vẫn giữ nguyên như trước.

**NTK-series** Ý tưởng là neural networks khó học các đặc trưng tần số cao, và nội suy trực tiếp có thể ảnh hưởng đến các phần tần số cao. Do đó, phương pháp NTK-aware đạt được ngoại suy tần số cao và nội suy tần số thấp bằng cách sửa đổi giá trị base của RoPE. Cụ thể, nó sửa đổi base $b$ của RoPE thành:

$b_{new} = b \cdot s^{\frac{d}{d-2}}$. (6)

Việc suy ra biểu thức này được rút ra từ $T_{new} b_{new}^{-\frac{d-2}{d}} = T_{origin} b^{-\frac{d-2}{d}}$ để đảm bảo rằng phần tần số thấp nhất được nội suy.

Một nghiên cứu gần đây (Liu et al., 2024b) đề xuất đặt base nhỏ hơn nhiều (ví dụ 500), trong trường hợp này $\theta_i = \text{base}^{-\frac{2i}{d}}$ đủ nhỏ và độ dài huấn luyện điển hình (nói 4,096) bao phủ hoàn toàn chu kỳ của $\cos(t-s)\theta_i$, vì vậy mô hình có thể có được khả năng ngữ cảnh dài hơn.

Một góc độ để giải thích các phương pháp ngoại suy hiện tại là OOD của góc xoay (Liu et al., 2024b; Han et al., 2023). Nếu tất cả các giá trị có thể của $\cos(t-s)\theta_i$ đã được fitted trong giai đoạn pre-training, OOD sẽ được tránh khi xử lý ngữ cảnh dài hơn. Hình 2 minh họa cách các phương pháp này tránh OOD của RoPE.

[Hình 2: Minh họa về OOD trong RoPE khi chúng ta mở rộng độ dài ngữ cảnh từ 4k lên 32k, và hai giải pháp để tránh OOD.]

## 3 Động lực

Các phương pháp dựa trên NTK được áp dụng rộng rãi trong mở rộng ngữ cảnh dài (Touvron et al., 2023a; Liu et al., 2024a; Young et al., 2024). Tuy nhiên, để có được khả năng ngữ cảnh dài tốt hơn, các nhà thực hành thường áp dụng base lớn hơn nhiều so với những gì phương pháp NTK-aware gốc đề xuất. Điều này dẫn đến suy đoán rằng có một cận khác của base RoPE được xác định bởi độ dài ngữ cảnh.

Mặt khác, một công trình gần đây (Liu et al., 2024b) đề xuất đặt base nhỏ hơn nhiều cho RoPE để mở rộng độ dài ngữ cảnh. Tuy nhiên, chúng tôi phát hiện đây có thể là khả năng ngữ cảnh dài bề mặt như được hiển thị trong Hình 3. Phương pháp này có thể có được perplexity thấp ngay cả ở độ dài ngữ cảnh 128k, có thể được giải thích bởi lý thuyết OOD như đã giải thích ở trên, nhưng mô hình không thể truy xuất thông tin liên quan cho độ dài ngữ cảnh ngắn tới 1k, thậm chí ngắn hơn nhiều so với độ dài pre-trained của mô hình. Phát hiện của chúng tôi ủng hộ nghiên cứu trước đây (Hu et al., 2024) về các hạn chế của perplexity trong việc đánh giá khả năng ngữ cảnh dài. Để đi sâu vào hiện tượng này, chúng tôi thực hiện khám phá lý thuyết trong phần tiếp theo.

[Hình 3: Khả năng ngữ cảnh dài bề mặt của việc tránh OOD bằng base nhỏ hơn.]

## 4 Góc độ Lý thuyết

Đối với cơ chế attention trong language modeling, chúng ta có các yêu cầu mong muốn sau:

**Yêu cầu mong muốn 1** Token gần hơn nhận được nhiều attention hơn: token hiện tại có xu hướng chú ý nhiều hơn đến token có khoảng cách tương đối nhỏ hơn.

**Yêu cầu mong muốn 2** Token tương tự nhận được nhiều attention hơn: token có xu hướng chú ý nhiều hơn đến token có giá trị key tương tự hơn với giá trị query của token hiện tại.

Sau đó chúng tôi kiểm tra các yêu cầu mong muốn khi chúng ta áp dụng RoPE cho cơ chế attention trong LLMs.

### 4.1 Long-term Decay của Cận trên của Điểm Attention

Đối với Yêu cầu mong muốn 1, tính chất của RoPE khiến mô hình chú ý nhiều hơn đến các token gần hơn. Loại long-term decay này đã được thảo luận kỹ lưỡng trong công trình trước đây (Su et al., 2024; Sun et al., 2022). Nó đến từ cận trên của tính toán điểm attention, có thể được viết là:

$$|A_{ij}| = |q_i^T R_m k_j| \leq \max_l(|h_l - h_{l+1}|) \sum_{n=1}^{d/2} |S_n|$$
$$= \max_l(|h_l - h_{l+1}|) \sum_{n=1}^{d/2} |\sum_{l=0}^{n-1} e^{(j-i)\theta_l\sqrt{-1}}|,$$ (7)

trong đó $h_l = q_i^T[2l:2l+1] k_j[2l:2l+1]$. Phương trình 7 chỉ ra rằng cận trên của điểm attention $|A_{ij}|$ giảm dần khi khoảng cách tương đối tăng. Hình 4 hiển thị đường cong long-term decay của cận trên này, phù hợp với các phát hiện trước đây (Su et al., 2024; Sun et al., 2022).

### 4.2 Long-term Decay của Khả năng Chú ý Nhiều hơn đến Token Tương tự hơn Token Ngẫu nhiên

Ngoài cận trên của điểm attention, chúng tôi cũng phát hiện tồn tại một tính chất long-term decay khác trong RoPE: khả năng chú ý nhiều hơn đến các token tương tự hơn là các token ngẫu nhiên giảm dần khi khoảng cách tương đối tăng. Chúng tôi định nghĩa khả năng chú ý nhiều hơn đến các token tương tự hơn là các token ngẫu nhiên là:

$$\mathbb{E}_{q,k^*}\langle q^T R_{m,\theta} k^* \rangle - \mathbb{E}_{q,k}\langle q^T R_{m,\theta} k \rangle,$$ (8)

trong đó $q \in \mathbb{R}^d$ là vector query cho token hiện tại, $k^* = q + \epsilon$ là giá trị key của token tương tự, trong đó $\epsilon$ là biến ngẫu nhiên nhỏ, $k \in \mathbb{R}^d$ là vector key của token ngẫu nhiên, $R_{m,\theta}$ là ma trận xoay trong RoPE. Số hạng đầu tiên trong Eq. 8 là điểm attention của $q$ và token tương tự $k^*$, số hạng thứ hai trong Eq. 8 là điểm attention của $q$ và token ngẫu nhiên $k$. Sau đó chúng tôi suy ra định lý sau:

**Định lý 1** Giả sử rằng các thành phần của query $q \in \mathbb{R}^d$ và key $k \in \mathbb{R}^d$ độc lập và phân phối đồng nhất, độ lệch chuẩn của chúng được ký hiệu là $\sigma \in \mathbb{R}$. Key $k^* = q + \epsilon$ là token tương tự với query, trong đó $\epsilon$ là biến ngẫu nhiên với trung bình bằng 0. Khi đó chúng ta có:

$$\frac{1}{2\sigma^2}(\mathbb{E}_{q,k^*}\langle q^T R_{m,\theta} k^* \rangle - \mathbb{E}_{q,k}\langle q^T R_{m,\theta} k \rangle) = \sum_{i=0}^{d/2-1} \cos(m\theta_i)$$ (9)

Chứng minh được hiển thị trong Phụ lục A. Chúng tôi ký hiệu $\sum_{i=0}^{d/2-1} \cos(m\theta_i)$ là $B_{m,\theta}$, và theo Định lý 1, $B_{m,\theta}$ đo lường khả năng chú ý nhiều hơn đến các token tương tự hơn là các token ngẫu nhiên, giảm dần khi khoảng cách tương đối $m$ tăng, như được hiển thị trong Hình 5. Đối với giá trị base rất nhỏ, chúng ta có thể quan sát thấy rằng $B_{m,\theta}$ thậm chí dưới số không ở một khoảng cách nhất định, có nghĩa là các token ngẫu nhiên có điểm attention lớn hơn các token tương tự, điều này có thể có vấn đề cho modeling ngữ cảnh dài.

### 4.3 Base của RoPE Giới hạn Độ dài Ngữ cảnh

Để thỏa mãn Yêu cầu mong muốn 2, chúng ta sẽ có $\mathbb{E}_{q,k^*}\langle q^T R_{m,\theta} k^* \rangle \geq \mathbb{E}_{q,k}\langle q^T R_{m,\theta} k \rangle$. Theo Định lý 1, $B_{m,\theta}$ cần lớn hơn số không. Cho $\theta$ trong RoPE, độ dài ngữ cảnh $L_\theta$ có thể thực sự có được thỏa mãn:

$$L_\theta = \sup\{L | B_{m,\theta} \geq 0, \forall m \in [0, 1, ..., L]\}$$ (10)

Nói cách khác, nếu chúng ta theo cài đặt rằng $\theta_i = \text{base}^{-2i/d}$, để có được độ dài ngữ cảnh mong đợi $L$, có một cận dưới của giá trị base $\text{base}_L$:

$$\text{base}_L = \inf\{\text{base} | B_{m,\theta} \geq 0, \forall m \in [0, 1, ..., L]\}$$ (11)

Tóm lại, base của RoPE xác định cận trên của độ dài ngữ cảnh mà mô hình có thể thực sự có được. Mặc dù tồn tại cận dưới tuyệt đối, Eq. 9 và Eq. 11 khó có được nghiệm dạng đóng vì $B_{m,\theta}$ là tổng của nhiều hàm cosine. Do đó, trong bài báo này, chúng tôi có được nghiệm số. Bảng 2 hiển thị cận dưới này cho độ dài ngữ cảnh từ 1.000 đến một triệu. Trong Hình 1, chúng tôi vẽ độ dài ngữ cảnh và cận dưới tương ứng, chúng ta có thể quan sát thấy rằng khi độ dài ngữ cảnh tăng, base yêu cầu cũng tăng.

**Lưu ý**: ranh giới này không rất nghiêm ngặt vì việc xếp chồng các lớp trong LLMs cho phép mô hình trích xuất thông tin vượt ra ngoài phạm vi của các lớp đơn, điều này có thể tăng độ dài ngữ cảnh trong Eq. 10 và giảm base trong Eq. 11. Tuy nhiên, trong Phần 5 chúng tôi phát hiện rằng cận suy ra xấp xỉ độ dài ngữ cảnh thực trong thực tế.

**Long-term decay từ các góc độ khác nhau.** Long-term decay trong phần 4.1 và phần 4.2 là từ các góc độ khác nhau. Góc độ trước đề cập đến long-term decay của điểm attention khi khoảng cách tương đối tăng. Điều này đảm bảo rằng các token hiện tại có xu hướng chú ý nhiều hơn đến các token gần chúng hơn. Góc độ sau chỉ ra rằng với việc giới thiệu ma trận xoay trong attention, khả năng phân biệt các token liên quan từ các token không liên quan giảm dần khi khoảng cách tương đối tăng. Do đó, $B_{m,\theta}$ lớn, tương ứng với giá trị base lớn, quan trọng để duy trì khả năng phân biệt của mô hình trong modeling ngữ cảnh dài.

**Bảng 2: Độ dài ngữ cảnh và cận dưới tương ứng của base RoPE.**

| Độ dài Ngữ cảnh | 1k | 2k | 4k | 8k | 16k | 32k | 64k | 128k | 256k | 512k | 1M |
|----------------|----|----|----|----|-----|-----|-----|------|------|------|-----|
| Cận dưới | 4.3e3 | 1.6e4 | 2.7e4 | 8.4e4 | 3.1e5 | 6.4e5 | 2.1e6 | 7.8e6 | 3.6e7 | 6.4e7 | 5.1e8 |

## 5 Thí nghiệm

Trong phần này, chúng tôi tiến hành các thí nghiệm kỹ lưỡng. Kết quả thực nghiệm có thể được tóm tắt trong Bảng 3, chi tiết trong các phần sau.

**Bảng 3: Trong Phần 5, chúng tôi nhằm trả lời các câu hỏi sau.**

| Câu hỏi | Câu trả lời |
|---------|-------------|
| H: Base của RoPE có giới hạn độ dài ngữ cảnh trong giai đoạn fine-tuning không? | Có. Khi base nhỏ, khó có được ngoại suy cho độ dài ngữ cảnh cụ thể. |
| H: Base của RoPE có giới hạn độ dài ngữ cảnh trong giai đoạn pre-training không? | Có. Cận dưới đề xuất của chúng tôi cho base RoPE cũng áp dụng cho pre-training. Nếu chúng ta huấn luyện mô hình từ đầu với base nhỏ nhưng độ dài ngữ cảnh lớn (lớn hơn độ dài bị giới hạn), mô hình kết quả có khả năng độ dài ngữ cảnh rất hạn chế, có nghĩa là một số ngữ cảnh trong pre-training bị lãng phí. |
| H: Điều gì xảy ra khi base được đặt nhỏ hơn cận dưới? | Mô hình sẽ có được khả năng ngữ cảnh dài bề mặt. Mô hình có thể duy trì perplexity thấp, nhưng không thể truy xuất thông tin hữu ích từ ngữ cảnh dài. |

### 5.1 Thiết lập Thí nghiệm

Đối với fine-tuning, chúng tôi sử dụng Llama2-7B (Touvron et al., 2023a) và Baichuan2-7B (Yang et al., 2023), cả hai đều là các mô hình mã nguồn mở phổ biến sử dụng RoPE với base là 1e4. Chúng tôi sử dụng learning rate cố định 2e-5 và global batch size 128 và fine-tuning trong 1000 bước. Đối với pre-training, chúng tôi huấn luyện mô hình 2B giống Llama từ đầu với tổng cộng 1 nghìn tỷ token. Chúng tôi đặt learning rate là 1e-4 và áp dụng lịch cosine decay, với các mô hình được huấn luyện trên tổng cộng 1T token. Dataset chúng tôi sử dụng là một tập con của RedPajama (Computer, 2023). Chi tiết thêm về thiết lập thí nghiệm được cung cấp trong Phụ lục B.

Đánh giá của chúng tôi tập trung vào hai khía cạnh: (1) **Perplexity**: chúng tôi sử dụng dataset PG19 (Rae et al., 2019) thường được sử dụng trong đánh giá ngữ cảnh dài; (2) **Retrieval**: ngoài perplexity, chúng tôi cũng áp dụng retrieval vì nó thể hiện khả năng hiểu ngữ cảnh dài thực của LLMs. Chúng tôi chọn a) benchmark Long-eval từ (Li* et al., 2023) và b) needle in a haystack (NIH) (G, 2023). Benchmark Long-eval tạo ra nhiều câu tương tự ngẫu nhiên và yêu cầu mô hình trả lời câu hỏi dựa trên một câu cụ thể trong ngữ cảnh, trong khi NIH yêu cầu mô hình truy xuất thông tin từ các vị trí khác nhau trong ngữ cảnh dài.

### 5.2 Base của RoPE giới hạn độ dài ngữ cảnh trong giai đoạn fine-tuning

Theo Eq. 11, có một cận dưới của base RoPE được xác định bởi độ dài ngữ cảnh mong đợi. Chúng tôi fine-tune Llama2-7b-Base trên ngữ cảnh 32k với các base khác nhau. Như được mô tả trong Hình 6, mặc dù sự khác biệt về perplexity giữa các base khác nhau không đáng kể, độ chính xác của Long-eval khác nhau đáng kể. Trong Hình 6b, đường đứt nét biểu thị cận dưới suy ra từ Eq. 11, dưới đó độ chính xác Long-eval giảm đáng kể.

[Hình 6: Fine-tuning Llama2-7B-Base trên độ dài ngữ cảnh 32k với base RoPE khác nhau.]

Kết quả bổ sung được cung cấp trong Phụ lục C. Đáng chú ý, cận dưới quan sát thực nghiệm này gần như phù hợp với suy ra lý thuyết của chúng tôi. Mặt khác, chúng ta có thể thấy rằng base=2e5 đạt được perplexity tốt nhất, nhưng độ chính xác của Long-eval rất thấp, điều này chỉ ra các hạn chế của perplexity trong việc đánh giá khả năng ngữ cảnh dài.

### 5.3 Base của RoPE giới hạn độ dài ngữ cảnh trong giai đoạn pre-training

Theo Định lý 1 và Eq. 11, ràng buộc này cũng có thể áp dụng cho giai đoạn pre-training. Để xác thực điều này, chúng tôi huấn luyện mô hình 2B từ đầu với base RoPE=100. Kết quả, được mô tả trong hàng đầu tiên của Hình 7, chỉ ra rằng mặc dù mô hình được huấn luyện với độ dài ngữ cảnh 4.096 token, nó chỉ có khả năng truy xuất thông tin từ khoảng 500 token gần đây nhất. Điều này chứng minh rằng tham số base giới hạn độ dài ngữ cảnh trong giai đoạn pre-training. Chúng tôi định nghĩa độ dài ngữ cảnh mà từ đó mô hình có thể truy xuất thông tin hiệu quả là độ dài ngữ cảnh hiệu quả.

Và theo lý thuyết của chúng tôi, độ dài ngữ cảnh hiệu quả có thể được mở rộng khi base của RoPE tăng. Để xác thực điều này, chúng tôi tiếp tục fine-tune mô hình 2B này trên độ dài ngữ cảnh 32k, với base của RoPE được đặt thành 1e4, như được hiển thị trong hàng thứ hai của Hình 7. Trong khi độ dài ngữ cảnh hiệu quả tăng lên, nó vẫn thấp hơn đáng kể so với 32k vì độ dài ngữ cảnh hiệu quả bị giới hạn bởi base=1e4 nhỏ hơn nhiều so với 32k. Hơn nữa, khi chúng tôi tăng base lên 1e6 và fine-tune mô hình 2B base trên 32K (hàng thứ ba trong Hình 7), mô hình có thể có được độ dài ngữ cảnh lớn hơn so với base=1e4, phù hợp với lý thuyết của chúng tôi.

[Hình 7: Kết quả của các mô hình 2B với base khác nhau.]

Để loại bỏ thêm ảnh hưởng của kích thước mô hình, chúng tôi cũng fine-tuned mô hình 7B lớn hơn trên độ dài ngữ cảnh 32k với base RoPE được đặt thành 1e4 và quan sát độ dài ngữ cảnh hiệu quả gần như giống hệt với mô hình 2B có cùng base RoPE (xem Phụ lục D). Đây là bằng chứng thực nghiệm rằng độ dài ngữ cảnh hiệu quả được xác định bởi base của RoPE.

### 5.4 Giải thích cho khả năng ngữ cảnh dài bề mặt với base nhỏ

Dựa trên lý thuyết và quan sát thực nghiệm của chúng tôi, dễ dàng giải thích điều gì xảy ra trong Hình 3.

**Ngoại suy tốt hơn (Perplexity)?** Do base nhỏ, $B_{m,\theta}$ có thể nhỏ hơn số không khi $m$ tăng, như được hiển thị trong Hình 5. Mô hình không thể chú ý nhiều hơn đến các token tương tự hơn là các token ngẫu nhiên với khoảng cách tương đối lớn, vì vậy mô hình có xu hướng tập trung nhiều hơn vào các token gần đó, điều này sẽ dẫn đến trường nhận thức thực nghiệm nhỏ hơn, thậm chí nhỏ hơn độ dài huấn luyện. Trong trường hợp này, mô hình có khả năng mạnh mẽ để duy trì tính ổn định perplexity (Chi et al., 2023).

**Khả năng tệ hơn (Long-eval và NIH)!** Theo phân tích trước đây của chúng tôi, base của RoPE giới hạn độ dài ngữ cảnh, và độ dài ngữ cảnh bị giới hạn bởi 500 thấp hơn nhiều so với giới hạn bởi 10.000. Do đó, khi base được đặt thành 500, độ dài ngữ cảnh hiệu quả giảm mạnh, ngay cả sau khi huấn luyện trên độ dài ngữ cảnh 32k.

### 5.5 Lý thuyết OOD không đủ để tiết lộ khả năng ngữ cảnh dài

**Bảng 4: So sánh giữa "Phương pháp 1" và "Phương pháp 2". Các phương pháp này được thiết kế cẩn thận. Cả hai đều không có OOD, nhưng chúng rất khác nhau dưới lý thuyết của chúng tôi.**

| Phương pháp | OOD | Long-eval | số lượng $m$ có $B_{m,\theta} \geq 0$ |
|-------------|-----|-----------|-----------------------------------|
|             |     | 15k | 30k | 15k | 30k |
| Phương pháp 1 | 0.33 | 0.27 | 0 | 0 |
| Phương pháp 2 | 0.40 | 0.00 | 97 | 2554 |

Phần 3 đề cập rằng các phương pháp dựa trên lý thuyết OOD của góc xoay có thể không phản ánh đầy đủ khả năng ngữ cảnh dài. Trong phần này, chúng tôi tiến hành các thí nghiệm thêm để chứng thực và giải thích quan sát này. Chúng tôi trình bày hai phương pháp để mở rộng độ dài ngữ cảnh của Llama2 từ 4k lên 32k. Cả hai đều không có góc OOD. Các phương pháp này được mô tả về mặt toán học như sau:

• **Phương pháp 1**: $\theta_i = (5e6)^{-2i/d}$,
• **Phương pháp 2**: $\theta_i = \begin{cases} (1e4)^{-2i/128}/8, & i \geq 44 \\ (1e4 \cdot 8^{128/88})^{-2i/128}, & i < 44 \end{cases}$.

Chúng ta có thể thấy từ Bảng 4 rằng hai phương pháp này thể hiện khả năng ngữ cảnh dài khác nhau đáng kể. Dưới góc độ của góc xoay OOD, cả hai phương pháp đều tránh góc xoay OOD, gợi ý ngoại suy hiệu quả. Tuy nhiên, mặc dù được huấn luyện trên độ dài ngữ cảnh 32k, "phương pháp 2" gặp khó khăn trong việc hoàn thành tác vụ retrieval ở độ dài ngữ cảnh 32k. Hiện tượng này vượt ra ngoài phạm vi mà lý thuyết OOD có thể giải thích. Dưới góc độ của chúng tôi, "phương pháp 2" nghiêm trọng vi phạm $B_{m,\theta} \geq 0$ khi $m \in [15k, 30k]$, do đó cản trở khả năng đạt được phân biệt ngữ cảnh dài. Chúng tôi suy đoán rằng mô hình có thể đạt được ngoại suy tốt hơn trong giai đoạn fine-tuning nếu base đủ lớn để vượt qua cận dưới và tránh OOD của góc xoay.

## 6 Công trình Liên quan

**Position embedding.** Từ khi được giới thiệu, Transformer (Vaswani et al., 2017) đã đạt được kết quả đáng chú ý trong lĩnh vực xử lý ngôn ngữ tự nhiên. Để sử dụng đầy đủ thứ tự của chuỗi, các nhà nghiên cứu đã giới thiệu position embedding. Position embedding sớm nhất dựa trên các hàm sinusoidal (Vaswani et al., 2017) cho vị trí tuyệt đối, learnable absolute position embedding (Devlin et al., 2018) và nhiều biến thể (Kiyono et al., 2021; Li et al., 2019) được đề xuất. Tuy nhiên, absolute position embedding gặp khó khăn trong việc mở rộng trực tiếp đến văn bản dài hơn độ dài huấn luyện. Sau đó, các nhà nghiên cứu đề xuất các phương pháp relative position embedding (Shaw et al., 2018; Ke et al., 2020). Với sự phát triển của các mô hình ngôn ngữ lớn, rotary position embedding và các biến thể của nó (Su et al., 2024; Sun et al., 2022) đã trở nên được sử dụng rộng rãi, như Llama2 (Touvron et al., 2023a), Baichuan2 (Yang et al., 2023), Mistral-7B-(Jiang et al., 2023a). Một nghiên cứu gần đây tiết lộ rằng không có position embedding cũng có tiềm năng (Kazemnejad et al., 2024).

**Long context learning.** Thực hiện các mô hình với ngữ cảnh dài hơn hoặc thậm chí vô hạn luôn là mục tiêu quan trọng trong lĩnh vực xử lý ngôn ngữ tự nhiên. Do độ phức tạp bình phương của mô hình transformer theo thời gian, một phần đáng kể công việc tập trung vào cải thiện cấu trúc mô hình (Gu & Dao, 2023; Peng et al., 2023a; Qin et al., 2024). Tuy nhiên, hầu hết công việc vẫn dựa trên kiến trúc transformer. Phần khác của công việc nhằm giảm độ phức tạp tính toán của chính attention, như sparse attention (Beltagy et al., 2020) và group query attention (Ainslie et al., 2023). Ngoài ra, cũng có một số tối ưu hóa về hiệu quả kỹ thuật, như flash attention (Dao et al., 2022) và ring attention (Liu et al., 2023). Trong giai đoạn suy luận mô hình, để tiết kiệm thời gian và không gian, cũng có một số phương pháp để tăng tốc ngữ cảnh dài, như nén KV cache (Hooper et al., 2024), v.v. Và position embedding quan trọng trong ngoại suy. Trong quá trình fine-tuning, các phương pháp như PI (Chen et al., 2023), NTK, và YARN (Peng et al., 2023b) được sử dụng để thay đổi thông tin position embedding gốc. FoT (Tworkowski et al., 2024) gán thông tin vị trí của các token bên ngoài ngữ cảnh cục bộ như token đầu tiên trong ngữ cảnh cục bộ.

## 7 Hạn chế

Trong công trình này, chúng tôi điều tra mối quan hệ giữa base của RoPE và độ dài ngữ cảnh. Mặc dù chúng tôi đã suy ra rằng tồn tại cận dưới cho base của RoPE được xác định bởi độ dài ngữ cảnh, sự tồn tại của cận trên cho base của RoPE vẫn là câu hỏi mở đáng được khám phá thêm. Ngoài ra, do thiếu các benchmark hiệu quả để đánh giá khả năng ngữ cảnh dài, phạm vi khả năng ngữ cảnh dài được thảo luận trong bài báo này có thể bị hạn chế.

## 8 Kết luận

Công trình của chúng tôi trình bày một nghiên cứu toàn diện về vai trò của RoPE trong LLMs để modeling ngữ cảnh dài hiệu quả. Đóng góp chính của chúng tôi nằm ở việc khám phá một tính chất mới của RoPE thông qua phân tích lý thuyết, chứng minh rằng khi khoảng cách tương đối giữa các token tăng, khả năng chú ý nhiều hơn đến các token tương tự của mô hình giảm. Theo lý thuyết của chúng tôi, chúng tôi suy ra cận dưới cho base của RoPE để phù hợp với độ dài ngữ cảnh mong đợi. Kết quả thí nghiệm của chúng tôi xác thực rằng base của RoPE giới hạn độ dài ngữ cảnh không chỉ cho fine-tuning mà còn cho giai đoạn pre-training. Lý thuyết của chúng tôi cung cấp góc độ mới về hiểu chức năng của RoPE trong modeling ngữ cảnh dài. Bằng cách soi sáng mối quan hệ giữa độ dài ngữ cảnh và position embedding, chúng tôi hy vọng công trình của mình có thể cung cấp những hiểu biết để nâng cao khả năng ngữ cảnh dài của LLMs.

## Tài liệu tham khảo

[Tất cả các tài liệu tham khảo được giữ nguyên như trong bản gốc]

## Phụ lục A: Chứng minh Định lý 1

[Nội dung chứng minh được dịch đầy đủ]

## Phụ lục B: Thiết lập chi tiết của thí nghiệm

[Nội dung được dịch đầy đủ]

## Phụ lục C: Baichuan2-7B-Base: Cận dưới Base của RoPE

[Nội dung được dịch đầy đủ]

## Phụ lục D: Kết quả Kiểm tra Ngữ cảnh Dài trên Nhiều LLMs Khác nhau

[Nội dung được dịch đầy đủ]
