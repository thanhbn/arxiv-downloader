# RAZOR ATTENTION : NÉN HIỆU QUẢ KV CACHE
THÔNG QUA CÁC ĐẦU TRUY XUẤT
BẢN THẢO

Hanlin Tang *1, Yang Lin1, Jing Lin1, Qingsen Han1, Shikuan Hong1, Yiwu Yao1, và Gongyi Wang1
1Huawei Technologies Co., Ltd

TÓM TẮT
Nhu cầu bộ nhớ và tính toán của Key-Value (KV) cache đặt ra những thách thức đáng kể cho việc triển khai các mô hình ngôn ngữ bối cảnh dài. Các phương pháp trước đây cố gắng giảm thiểu vấn đề này bằng cách loại bỏ có chọn lọc các token, điều này xóa không thể hoàn nguyên thông tin quan trọng có thể cần thiết cho các truy vấn tương lai. Trong bài báo này, chúng tôi đề xuất một kỹ thuật nén mới cho KV cache bảo tồn tất cả thông tin token. Điều tra của chúng tôi tiết lộ rằng: i) Hầu hết các đầu attention chủ yếu tập trung vào bối cảnh cục bộ; ii) Chỉ một vài đầu, được ký hiệu là đầu truy xuất, có thể chú ý đến tất cả các token đầu vào. Những quan sát quan trọng này thúc đẩy chúng tôi sử dụng chiến lược lưu trữ riêng biệt cho các đầu attention. Do đó, chúng tôi đề xuất RazorAttention, một thuật toán nén KV cache không cần huấn luyện, duy trì cache đầy đủ cho những đầu truy xuất quan trọng này và loại bỏ các token xa trong các đầu không phải truy xuất. Hơn nữa, chúng tôi giới thiệu một cơ chế mới bao gồm "token bù" để khôi phục thêm thông tin trong các token bị loại bỏ. Đánh giá rộng rãi trên một tập đa dạng các mô hình ngôn ngữ lớn (LLMs) chứng minh rằng RazorAttention đạt được việc giảm kích thước KV cache hơn 70% mà không có tác động đáng chú ý đến hiệu suất. Ngoài ra, RazorAttention tương thích với FlashAttention, làm cho nó trở thành một giải pháp hiệu quả và cắm-và-chạy nâng cao hiệu quả suy luận LLM mà không có chi phí hoặc huấn luyện lại mô hình gốc.

1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLMs) bối cảnh dài đã nâng cao đáng kể khả năng xử lý ngôn ngữ tự nhiên trên các nhiệm vụ đa dạng. Tuy nhiên, sự tăng trưởng của Key-Value (KV) cache dưới độ dài đầu vào tăng đã trở thành nút thắt cổ chai chính cho việc triển khai. Đã có nhiều nghiên cứu trước đây được thiết kế để giảm bớt vấn đề này bằng cách nén kích thước KV cache, bao gồm lượng tử hóa [1–3], loại bỏ token [4, 5], attention cục bộ [6, 7], v.v.

[Hình 1: RazorAttention đạt được hiệu suất tương đương với mô hình gốc, ngay cả khi nén 70% KV cache. Để chứng minh điều này, chúng tôi đã kiểm tra Llama2-13B-64K [8] trên benchmark Needle in A Haystack [9].]

*Tác giả liên hệ: tanghl1994@gmail.com

--- TRANG 2 ---

RazorAttention: Nén Hiệu Quả KV Cache Thông Qua Các Đầu Truy Xuất BẢN THẢO

Bối cảnh đầu vào: "DOD's MILCON appropriations are used to fund the acquisition, construction, installation... 
Mary's favorite number is 34251... Bob's favorite number is 7690... reviewing project cost estimates."

Q1: "What is Mary's favourite number?"
Mô hình gốc: "Mary's favorite number is 34251."
H2O: "Mary's favorite number is not explicitly mentioned in the text provided."
SnapKV: "Mary's favorite number is 34251."
RazorAttention: "Mary's favorite number is 34251."

Q2: "What is Bob's favourite number?"
Mô hình gốc: "Bob's favorite number is 7690."
H2O: "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
SnapKV: "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
RazorAttention: "Bob's favorite number is 7690."

Hình 2: Các phương pháp loại bỏ token dựa trên tầm quan trọng không thể hoạt động khi truy vấn thông tin ít liên quan đến chủ đề chính. Ở đây, chúng tôi sử dụng một tài liệu 8K từ LongBench [10] và thêm hai câu không liên quan đến chủ đề chính. Trong trường hợp này, H2O loại bỏ các token ít liên quan đến chủ đề chính, dẫn đến thất bại trong cả Q1 và Q2. SnapKV loại bỏ token dựa trên truy vấn đầu tiên, làm cho nó hiệu quả cho Q1 nhưng thất bại trong các truy vấn tiếp theo như Q2. Chỉ RazorAttention thành công xuất ra thông tin chính xác từ đầu vào dài ngay cả khi chúng tôi nén 70% KV cache.

Một hướng chính cho nén KV cache là trực tiếp loại bỏ các token được coi là không quan trọng cho đến nay [4,5,11,12]. Những phương pháp này vốn giả định rằng các token được coi là không quan trọng sẽ không cần thiết trong các truy vấn tương lai, điều này không đúng trong các tình huống thực tế. Ví dụ, người dùng có thể yêu cầu thông tin không trực tiếp phù hợp với chủ đề chính của văn bản được xử lý, hoặc tham gia vào cuộc trò chuyện nhiều vòng truy vấn các phân đoạn khác nhau từ bối cảnh. Trong những trường hợp này, các phương pháp loại bỏ token dựa trên tầm quan trọng có thể dẫn đến suy giảm hiệu suất đáng kể vì thông tin thực tế được yêu cầu bởi truy vấn có thể bị loại bỏ nếu được coi là không quan trọng (xem ví dụ của chúng tôi trên Qwen1.5-7B-Chat [13] trong Hình 2). Điều này dẫn chúng tôi đặt ra một câu hỏi quan trọng:

"Liệu chúng ta có thể tìm cách giảm kích thước KV cache mà không mất thông tin ngữ nghĩa?"

Trong nghiên cứu này, chúng tôi giải quyết vấn đề này từ một góc độ mới. Điều tra của chúng tôi tiết lộ rằng tồn tại một cơ chế "truy xuất và xử lý" trong LLMs khi xử lý bối cảnh dài. Cụ thể hơn, LLMs có thể nhớ lại chính xác thông tin được truy vấn từ đầu vào dài thông qua một nhóm nhất định các đầu attention, mà chúng tôi ký hiệu là "đầu truy xuất" (xem Phần 3.3 cho định nghĩa). Những đầu này có khả năng tập trung hầu hết trọng số attention của chúng vào thông tin liên quan (w.r.t. các truy vấn) và tăng xác suất đầu ra cho những từ đó. Một phát hiện quan trọng khác là các đầu không phải truy xuất chủ yếu tập trung vào bối cảnh cục bộ hoặc attention sink [5], có nghĩa là những đầu này không thể sử dụng hiệu quả tất cả thông tin ngữ nghĩa từ đầu vào. Dựa trên những phát hiện quan trọng này, chúng tôi đưa ra giả thuyết rằng LLM chạy thủ tục lý luận trên cơ sở "truy xuất và xử lý". Điều đó có nghĩa là, mô hình trước tiên sử dụng các đầu truy xuất để thu thập thông tin liên quan, và sau đó các đầu không phải truy xuất để xử lý thông tin được truy xuất và tạo ra phản hồi cuối cùng. Điều này thúc đẩy chúng tôi thiết kế các chiến lược lưu trữ riêng biệt cho các đầu khác nhau: Đối với các đầu truy xuất, chúng tôi giữ KV cache không thay đổi; đối với các đầu còn lại, chúng tôi chỉ lưu trữ các token gần đây và attention sinks.

Ngoài điều này, chúng tôi nhận thấy rằng vẫn tồn tại một khoảng cách độ chính xác nhất định khi chúng tôi trực tiếp loại bỏ tất cả các token xa trong các đầu không phải truy xuất. Do đó, đối với những đầu không phải truy xuất này, chúng tôi đã thiết kế một "token bù" để nén cache bị loại bỏ thành một token, và chứng minh rằng sự suy giảm độ chính xác do KV cache bị cắt ngắn được cải thiện thêm với token bù này. Với các đầu truy xuất và token bù, chúng tôi chứng minh rằng thuật toán của chúng tôi, tức là RazorAttention, có thể nén thành công 70% KV cache mà không có suy giảm hiệu suất đáng chú ý như được minh họa trong Hình 1.

Cuối cùng nhưng không kém phần quan trọng, các phương pháp loại bỏ token dựa trên tầm quan trọng trước đây không thể được kết hợp với FlashAttention do sự phụ thuộc của chúng vào trọng số attention để tính điểm quan trọng, làm cho chúng không thể thực hiện được vì FlashAttention là một trong những thành phần quan trọng nhất trong suy luận bối cảnh dài. RazorAttention giải quyết vấn đề này vì nó không sử dụng bản đồ attention làm thước đo. Tiêu chí cắt tỉa theo đầu hoàn toàn tương thích với FlashAttention, và chi phí tính toán của token bù là không đáng kể. Do đó RazorAttention có thể đạt được tăng tốc suy luận đáng kể khi so sánh với các phương pháp trước đây.

--- TRANG 3 ---

RazorAttention: Nén Hiệu Quả KV Cache Thông Qua Các Đầu Truy Xuất BẢN THẢO

Theo hiểu biết tốt nhất của chúng tôi, RazorAttention là thuật toán giảm token không cần huấn luyện đầu tiên đạt được giảm KV cache gần như không mất mát 3X. Chúng tôi đã đánh giá RazorAttention trên các mô hình bao gồm Qwen [13], Llama-2 [14], Llama-3 [15] và Baichuan [16] trên các nhiệm vụ bối cảnh dài để chứng minh hiệu quả của nó. Đóng góp của chúng tôi có thể được tóm tắt như sau:

• Chúng tôi phân tích một cách có hệ thống động lực attention của Transformers dưới đầu vào dài. Nghiên cứu của chúng tôi tiết lộ rằng chỉ có một vài đầu truy xuất có thể nhớ lại thông tin từ toàn bộ đầu vào trong khi các đầu còn lại chủ yếu tập trung vào bối cảnh cục bộ.

• Chúng tôi giới thiệu một thuật toán mới, tức là RazorAttention, có khả năng giảm kích thước KV cache 70% dưới tác động tối thiểu đến hiệu suất cho bối cảnh từ 8K đến 100K token. Chúng tôi đã thiết kế một thước đo chính xác và không cần dữ liệu để phân bổ tất cả các đầu truy xuất, cùng với một chiến lược bù lỗi để bù đắp mất mát thông tin do KV cache bị cắt ngắn.

• RazorAttention giới thiệu chi phí không đáng kể trong nén và tương thích với FlashAttention, làm cho nó trở thành một giải pháp hiệu quả và cắm-và-chạy nâng cao hiệu quả suy luận LLM mà không cần huấn luyện hoặc chi phí đáng kể. Các thí nghiệm rộng rãi chứng minh rằng RazorAttention có thể được áp dụng hiệu quả cho các mô hình và nhiệm vụ khác nhau.

2 Nghiên cứu Liên quan

Khi độ dài chuỗi tăng, tiêu thụ bộ nhớ của KV cache mở rộng nhanh chóng, có thể vượt quá kích thước của các tham số mô hình. Điều này dẫn đến nhu cầu cấp bách cho nén KV cache, đặc biệt trong các tình huống với bộ nhớ GPU hạn chế. Một hướng là thiết kế kiến trúc không phải Transformer, như Mamba [17], Mamba2 [18], Infini-Transformer [19], RWKV [20] và Griffin [21]. Tuy nhiên, trong bài báo này chúng tôi tập trung vào giảm KV cache cho Transformers điển hình, đây là cấu trúc mô hình được sử dụng rộng rãi nhất. Dưới đây chúng tôi giới thiệu một số phương pháp cho nén KV cache.

Lượng tử hóa Lượng tử hóa là một phương pháp cổ điển nhưng hiệu quả cho nén mạng neural. Trong lĩnh vực Lượng tử hóa LLM, trong khi thách thức outlier thu hút sự chú ý lớn [22–24] để giải quyết, việc áp dụng nó trên KV cache thường được xem như một sản phẩm phụ của lượng tử hóa activation. Tuy nhiên, có một số nghiên cứu đáng chú ý chứng minh giá trị của lượng tử hóa KV cache. FlexGen, Atom và QServe [1–3] đã thiết kế cẩn thận các pipeline lượng tử hóa sử dụng nén KV cache để tăng thông lượng suy luận tổng thể. KVQuant [25] tích hợp một số kỹ thuật để giảm thiểu lỗi lượng tử hóa KV và KIVI [26] đẩy giới hạn về 2-bits. Ngoài các phương pháp sau huấn luyện, LLM-QAT [27] cung cấp một quá trình chưng cất không cần dữ liệu khôi phục thêm hiệu suất của mô hình.

Loại bỏ token Các phương pháp loại bỏ token giả định rằng không phải tất cả các cặp key-value đều cần thiết trong tính toán self-attention, vì vậy việc sử dụng bộ nhớ có thể được tiết kiệm bằng cách xác định và loại bỏ KV cache không quan trọng. StreamingLLM [5] sử dụng công nghệ sliding window, chỉ bảo tồn các cặp KV của các token attention sink và những token trong sliding window, từ đó giảm dấu chân bộ nhớ và ổn định hiệu suất mô hình. H2O [4] là một trong những tiên phong sử dụng điểm attention để đánh giá tầm quan trọng của mỗi token, theo sau là một chiến lược loại bỏ chọn lọc tham lam cache với điểm cao hơn. Scissorhands [11] và một trong những nghiên cứu mới nhất SnapKV [12] sử dụng ý tưởng tương tự bằng cách thu hẹp phạm vi tính toán để xem xét điểm attention liên quan đến thông tin gần đây. Dựa trên đó, PyramidKV và PyramidInfer [28,29] phân tích các mẫu tập trung attention và giảm thêm KV cache trong các lớp sau. Hơn nữa, các nỗ lực nghiên cứu đã được thực hiện để hiểu KV cache từ các góc độ khác nhau: FastGen [30] chú ý đến các token đặc biệt và dấu câu, SubGen [31] điều tra khả năng phân cụm của key embedding và CORM [32] phát hiện tương quan mạnh giữa các token của hàng xóm gần.

Non-MHA Attention Một danh mục khác tập trung vào giảm KV cache bằng cách chia sẻ cache giữa các đầu attention. MQA [33] sử dụng tích cực một đầu KV duy nhất cho tất cả các đầu, trong khi GQA [34] gợi ý một số lượng đầu trung gian để cân bằng sự đánh đổi giữa tốc độ suy luận và chất lượng đầu ra. Hơn nữa, MLA [35] trình bày một phương pháp lưu trữ mới bằng cách xếp hạng thấp KV cache của tất cả các đầu vào không gian latent duy nhất.

Thuật toán của chúng tôi được thúc đẩy bởi ý tưởng từ [36], nơi các tác giả nhận thấy rằng có một số nhóm đầu attention nhất định, được ký hiệu là các đầu induction, có thể nhớ lại hiệu quả thông tin được truy vấn từ đầu vào. Nghiên cứu gần đây [37] cũng xác nhận tính chất này dưới đầu vào mở rộng. Đây là nghiên cứu đầu tiên đề xuất tiêu chí cắt tỉa theo đầu cho nén KV cache dựa trên khả năng diễn giải của cơ chế attention.

--- TRANG 4 ---

RazorAttention: Nén Hiệu Quả KV Cache Thông Qua Các Đầu Truy Xuất BẢN THẢO

3 Phương pháp luận

Trong phần này, chúng tôi giới thiệu các thành phần chính của RazorAttention. Trước tiên chúng tôi áp dụng RazorAttention cho các mô hình sử dụng ALiBi [38] positional embedding (ký hiệu là mô hình ALiBi) để cung cấp hiểu biết trực quan về các đầu truy xuất và không phải truy xuất. Sau đó, chúng tôi chứng minh rằng các mô hình sử dụng RoPE [39] positional embedding (ký hiệu là mô hình RoPE) cũng thể hiện đặc tính quan trọng này, tiết lộ rằng KV cache trong các mô hình RoPE cũng có thể được nén hiệu quả dưới mất mát độ chính xác tối thiểu.

3.1 RazorAttention cho mô hình ALiBi

Đối với mô hình ALiBi, đầu attention thứ h của nó tính điểm attention theo

Sm→n(q;k) = qmk⊺n − lh(m−n), (1)

trong đó qm là tensor query tại vị trí thứ m, kn là tensor key tại vị trí thứ n, lh là độ dốc cụ thể của đầu, Sm→n(q;k) là điểm attention. Lưu ý rằng (m≥n) được đảm bảo bởi tính nhân quả của attention.

Trong tình huống mà lh(m−n) chiếm ưu thế đáng kể so với qmk⊺n, attention giữa qm và kn sẽ suy giảm về không, có nghĩa là đóng góp của bất kỳ token nào được định vị xa hơn n trở nên không đáng kể cho đầu ra tại vị trí m. Định lý sau chính thức hóa quan sát này.

Định lý 1. Cho một đầu attention tính điểm attention theo (1), với bất kỳ ϵ∈(0,1), trọng số attention từ qm đến kn có thể được giới hạn trên bởi:

Attnm→n(q;k) = exp (Sm→n(q;k)) / Σmn=0 exp (Sm→n(q;k)) ≤ ϵ, ∀n < m − C0,

Lh := 2∥WQhWKh∥2√(∥γ∥2+∥b∥2) − log(ϵ) / lh. (2)

Ở đây WQh và WKh là các ma trận query và key của đầu attention thứ h, γ và b là trọng số và bias cho lớp LayerNorm trước attention (b=0 cho RMSNorm [40]), và ∥ · ∥2 biểu thị l2-norm của ma trận. Lh có thể được xem như phạm vi tầm nhìn của đầu. Chứng minh chi tiết có thể được tìm thấy trong Phụ lục A.

Định lý (1) chỉ ra rằng khi khoảng cách giữa qm và kn vượt quá C0, trọng số attention giữa hai token này giảm xuống dưới ϵ. Khi ϵ đủ nhỏ (ví dụ, 0.1%), các token xa áp đặt ảnh hưởng tối thiểu lên đầu ra cuối cùng và do đó có thể bị loại bỏ. Dựa trên nguyên tắc này, các mô hình ALiBi động điều chỉnh kích thước KV cache cho mỗi đầu. Chúng tôi trước tiên tính phạm vi attention hiệu quả Lh, và chỉ giữ Lh token gần đây trong KV cache, vì bất kỳ token nào xa hơn Lh áp đặt trọng số attention không quá ϵ, chúng tôi có thể loại bỏ chúng một cách an toàn để nén.

Do đó, đối với mô hình ALiBi, các đầu truy xuất là những đầu có Lh lớn hơn, trong khi các đầu không phải truy xuất có tầm nhìn attention Lh nhỏ hơn.

3.2 RazorAttention cho mô hình RoPE

Đối với mô hình RoPE, mỗi đầu attention tính điểm attention theo

Sm→n(q;k) = qmk⊺n, qm = Rmq, kn = Rnk (3)

trong đó qm và kn là trạng thái query và key sau biến đổi rotary, Rm và Rn là các ma trận rotary tại vị trí m và n (xem [39] để biết chi tiết). Mặc dù RoPE embedding không vốn gợi ý một attention suy giảm tầm xa, các phát hiện thực nghiệm của chúng tôi chỉ ra rằng phần lớn các đầu attention duy trì phạm vi attention hạn chế. Đáng chú ý, chỉ khoảng 15% các đầu, mà chúng tôi gọi là đầu truy xuất, có khả năng sử dụng hiệu quả thông tin tầm xa trong khi các đầu còn lại chỉ tập trung vào bối cảnh cục bộ. Như được hiển thị trong Bảng 1, một sự giảm đáng kể về độ chính xác 16% được quan sát khi kích thước KV cache được giảm cho những đầu truy xuất này. Ngược lại, việc loại bỏ token xa trong các đầu không phải truy xuất dẫn đến suy giảm hiệu suất tương đối nhỏ là 1.5%.

Dựa trên các phát hiện trên, chúng tôi trực tiếp giảm KV cache cho tất cả các đầu không phải truy xuất. Hiệu suất của mô hình hầu hết được giữ lại như được hiển thị trong Bảng 1. Tuy nhiên, một khoảng cách độ chính xác đáng chú ý vẫn còn, chỉ ra rằng một số thông tin vẫn đang bị mất. Hơn nữa, kết quả kiểm tra trên Needle in a Haystack cho thấy suy giảm hiệu suất rõ ràng ngay cả khi chúng tôi bảo vệ KV cache của các đầu truy xuất (xem kết quả ablation của chúng tôi trong Hình 6). Để cải thiện thêm hiệu suất, chúng tôi đã thiết kế một cách nhẹ và hiệu quả để nén thông tin trong token bị loại bỏ thành một "token bù".

Token bù được định nghĩa là

k̂ = 1/Nd Σm∈{D} km, v̂ = 1/Nd Σm∈{D} vm. (4)

--- TRANG 5 ---

RazorAttention: Nén Hiệu Quả KV Cache Thông Qua Các Đầu Truy Xuất BẢN THẢO

Đầu bảo vệ | Tất cả | Đầu truy xuất | Đầu ngẫu nhiên | Không có
---|---|---|---|---
MultiFieldQA-en | 46.94% | 45.48% | 40.7% | 40.81%

Bảng 1: Chúng tôi bảo vệ KV cache trong các nhóm đầu attention khác nhau trong khi chỉ giữ 4K token gần đây trong phần còn lại. Bảo vệ KV cache trong các đầu truy xuất có thể giữ lại hầu hết hiệu suất của LLM, trong khi bảo vệ các đầu ngẫu nhiên không mang lại lợi ích hiệu suất. Điều này rõ ràng chỉ ra rằng hầu hết các đầu attention chỉ sử dụng bối cảnh cục bộ và chỉ các đầu truy xuất có thể sử dụng tất cả thông tin bối cảnh một cách thiết yếu.

Ở đây k̂, v̂ là các token bù cho KV cache bị loại bỏ, {D} chứa các chỉ số của các token bị loại bỏ và Nd là số lượng token bị loại bỏ. Sau đó, chúng tôi loại bỏ các token bị drop và tăng cường KV cache với token bù k̂ và v̂, trong đó {K, V} là KV cache của token còn lại sau biến đổi rotary.

Ký hiệu KV cache nén như {K,k̂} và {V,v̂}, đầu ra attention của token hiện tại tuân theo

Attn(qm,{K,k̂},{V,v̂}) = [Nd exp(qmk̂⊺)v̂ + Σn∉{D} exp(qmk⊺n)vn] / [Nd exp(qmk̂⊺) + Σn∉{D} exp(qmk⊺n)]. (5)

Trong Hình 3(a) chúng tôi cung cấp một ví dụ minh họa về RazorAttention cho mô hình RoPE. Với token bù, độ chính xác được cải thiện thêm, làm cho RazorAttention gần như không mất mát ngay cả khi loại bỏ 70% KV cache trong các đầu không phải truy xuất. Dưới đây chúng tôi giới thiệu cách chúng tôi xác định nhóm đầu truy xuất.

[Hình 3: Trong Hình 3(a) chúng tôi trình bày minh họa về cách RazorAttention nén KV cache. Đối với các đầu truy xuất, chúng tôi duy trì cache đầy đủ để giữ lại thông tin của tất cả token. Đối với các đầu không phải truy xuất, chúng tôi trực tiếp loại bỏ token xa và nén các token bị loại bỏ thành một token bù có KV cache được ký hiệu là {k̂,v̂}. Trong Hình 3(b) chúng tôi cung cấp một ví dụ minh họa về echo head và induction head. Token hiện tại là "B" và token được tạo là "C". Trong trường hợp này, echo head sẽ chủ yếu chú ý đến token "B" trong khi induction head chủ yếu chú ý đến token "C" trong bối cảnh trước đó.]

3.3 Xác định Đầu Truy xuất

Đối với mô hình ALiBi, phạm vi attention có thể được xác định trực tiếp qua (2) và KV cache có thể được loại bỏ tương ứng. Tuy nhiên, đối với mô hình RoPE, các đầu truy xuất cần được xác định theo cách tinh vi hơn. Điều tra của chúng tôi tiết lộ rằng hai nhóm đầu là thiết yếu trong xử lý bối cảnh dài, vì vậy cả hai đều nên được bao gồm như các đầu truy xuất như được nêu dưới đây.

• Echo head: Đầu có xu hướng chú ý trở lại token trước đó (được gọi là echo token) giống hệt với token hiện tại.

• Induction head: Đầu có xu hướng chú ý đến token trước đó (tức là induction token) được theo sau ngay lập tức bởi token hiện tại. Về cơ bản nó chú ý đến token sắp tới cũng tồn tại trong bối cảnh trước đó.

Trong Hình 3(b) chúng tôi trình bày một ví dụ minh họa giải thích các echo head và induction head. Để xác định các đầu truy xuất, chúng tôi tạo K (ví dụ, K = 2500) token ngẫu nhiên, lặp lại những token này 4 lần, và sau đó sử dụng nó như đầu vào của mô hình. Thiết kế này giảm thiểu các phụ thuộc ngữ nghĩa giữa các token, từ đó cho phép quan sát rõ ràng hơn về hành vi của echo và induction head.

--- TRANG 6 ---

RazorAttention: Nén Hiệu Quả KV Cache Thông Qua Các Đầu Truy Xuất BẢN THẢO

Cài đặt Siêu tham số
---|---
Độ dài buffer | max(4000, N/5)
Bảo vệ induction head | top 14%
Bảo vệ echo head | top 1%
Số token sink | 4

Bảng 2: Cài đặt siêu tham số chung cho các thí nghiệm trong bài báo, dẫn đến nén 3.125x KV cache dưới đầu vào bối cảnh dài.

[Bảng 3: So sánh hiệu suất của RazorAttention và các thuật toán nén khác trên các LLM khác nhau trên LongBench. Lưu ý rằng hiệu suất của Llama3-8B-Instruct trên TREC và LSHT không áp dụng được (gần 0), do đó chúng tôi không bao gồm kết quả của chúng trên Llama3-8B.]

Sau đó, chúng tôi tính điểm echo (trọng số attention đến echo token) và điểm induction (trọng số attention đến induction token) của tất cả các từ trên tất cả các đầu. Việc lựa chọn đầu truy xuất bao gồm top-14% đầu attention có điểm induction cao nhất và top-1% đầu attention có điểm echo cao nhất (xem Bảng 2). Lưu ý rằng mặc dù chúng tôi chỉ sử dụng ít echo head hơn nhiều so với retrieval head, điều tra của chúng tôi chỉ ra rằng cả hai đầu đều quan trọng cho hiệu suất truy xuất của LLMs (xem Phần 4.3 cho kết quả ablation).

Với các đầu truy xuất được xác định, chúng tôi giới thiệu RazorAttention cho Mô hình RoPE trong Thuật toán 1.

Thuật toán 1 RazorAttention cho Mô hình RoPE
Đầu vào: Tập đầu không phải truy xuất {H}, KV cache gốc (sau biến đổi rotary) {K, V}, tỷ lệ nén C, ngưỡng nén S0, số token sink N0.
1: for đầu không phài truy xuất h ∈ {H} do
2: Tính độ dài buffer Lh = max(S0, N/C), ở đây N là số token trong đầu.
3: Chỉ giữ Lh token gần đây gần đầu ra và N0 token sink đầu tiên, loại bỏ các token còn lại và nén chúng thành token bù theo (4).
4: end for
5: Các đầu không phải truy xuất tính attention theo (5), trong khi các đầu truy xuất tuân theo attention gốc.
Đầu ra: Các token đầu ra được tạo.

4 Thí nghiệm

Nhiều LLM được phát hành gần đây được chọn để xác nhận đề xuất của chúng tôi, bao gồm Qwen [13], Llama2 [14], Llama3 [15] và Baichuan [16]. Các mô hình được chọn được đánh giá trên Longbench [10] và Needle In A Haystack [9] để chứng minh khả năng của chúng trong hoàn cảnh bối cảnh dài. Các thí nghiệm được thực hiện trên NVIDIA GeForce RTX 4090 (24GB). Chúng tôi sẽ trước tiên xác nhận hiệu quả của đề xuất của chúng tôi trên các nhiệm vụ khác nhau, theo sau là nghiên cứu ablation của mỗi thành phần trong thiết kế thuật toán của chúng tôi. Trừ khi được nêu rõ, chúng tôi sử dụng RazorAttention với các siêu tham số như

--- TRANG 7 ---

RazorAttention: Nén Hiệu Quả KV Cache Thông Qua Các Đầu Truy Xuất BẢN THẢO

[Hình 4: So sánh hiệu suất của RazorAttention và các thuật toán nén khác trên Llama2-7b-80K, Needle In A Haystack. Lưu ý rằng H2O không tương thích với FlashAttention nên chúng tôi gặp lỗi OOM khi kiểm tra trên chuỗi dài hơn, và hiệu suất của nó đã trở nên không sử dụng được trong trường hợp này.]

trong Bảng 2. Chúng tôi sử dụng H2O [4] và StreamingLLM [5] để so sánh. Lưu ý rằng chúng tôi không bao gồm SnapKV [12] như baseline vì nó giả định rằng truy vấn được biết trước khi nén, điều này không đúng trong các trường hợp chung hoặc trong cuộc trò chuyện nhiều vòng nơi người dùng có thể truy vấn thông tin khác nhau từ bối cảnh (như đã thảo luận trong Phần 1).

4.1 Đánh giá LongBench

Trong Bảng 3 chúng tôi trình bày kết quả của các thuật toán khác nhau trên LongBench [10], cung cấp đánh giá toàn diện để đánh giá khả năng liên quan đến bối cảnh dài của LLMs. Chúng tôi sử dụng Qwen1.5-7B và Qwen1.5-72B để kiểm tra vì chúng là mô hình RoPE với độ dài bối cảnh 32K. Chúng tôi cũng bao gồm Llama3-8B để xác nhận hiệu suất của RazorAttention trên mô hình GQA. Chúng tôi chọn Baichuan2-13B để chứng minh hiệu quả của RazorAttention trên mô hình ALiBi. Có thể thấy rằng RazorAttention đạt được hiệu suất vượt trội trên tất cả mô hình so với StreamingLLM và H2O. Các kết quả thuyết phục chỉ ra rằng RazorAttention có thể đạt được hiệu suất tương đương như baseline không nén, ngay cả dưới tỷ lệ nén 3X.

Hơn nữa, chúng tôi kiểm tra Llama3-8B-Instruct như một instance GQA nơi mỗi 4 đầu attention chia sẻ một tập KV cache duy nhất. Do đó, chúng tôi coi các đầu attention trong một nhóm là tất cả truy xuất nếu một hoặc nhiều đầu thỏa mãn tính chất inductive hoặc echoing. Kết quả trong Bảng 3 rõ ràng chứng minh rằng RazorAttention vẫn hoạt động cho mô hình GQA.

[Hình 5: Thêm 1% echo head có thể nâng cao đáng kể hiệu suất truy xuất của RazorAttention trên Llama2-7B-80k.]

4.2 Đánh giá Needle In A Haystack

Trong Hình 4 chúng tôi trình bày kết quả trên Needle In A Haystack. Chúng tôi sử dụng Llama2-7B-80K từ [8] vì độ dài bối cảnh của mô hình này là 80K. Không giống như H2O, có hiệu suất bị suy giảm nghiêm trọng dưới đầu vào dài, RazorAttention vẫn có thể nhớ lại chính xác thông tin được truy vấn. Đây là bằng chứng mạnh mẽ chứng minh rằng RazorAttention có thể giữ lại tất cả thông tin ngữ nghĩa trong bối cảnh gốc, trong khi các phương pháp dựa trên tầm quan trọng không thể tránh khỏi việc loại bỏ thông tin có thể hữu ích trong các truy vấn tương lai.

4.3 Nghiên cứu Ablation

Dưới đây chúng tôi trình bày kết quả ablation của RazorAttention, và chứng minh rằng thiết kế thuật toán và cấu hình được chọn tối ưu để đạt được tỷ lệ nén cao hơn với suy giảm hiệu suất có thể chấp nhận được.

--- TRANG 8 ---

RazorAttention: Nén Hiệu Quả KV Cache Thông Qua Các Đầu Truy Xuất BẢN THẢO

Sơ đồ bảo vệ | Điểm
---|---
1% Echo + 5% Induction Head | 69.54%
1% Echo + 8% Induction Head | 78.40%
1% Echo + 11% Induction Head | 84.55%
1% Echo + 14% Induction Head | 86.59%
Baseline | 87.05%

Bảng 4: Qwen1.5-7B-Chat sử dụng RazorAttention với số lượng đầu được bảo vệ khác nhau, được kiểm tra trên Needle in A Haystack.

4.3.1 Tầm quan trọng của Echo Head

Mặc dù chúng tôi chỉ bao gồm 1% echo head trong RazorAttention, chúng tôi nhận thấy rằng nhóm đầu này khá thiết yếu trong việc truy xuất thông tin dưới bối cảnh dài như được hiển thị trong Hình 5. Một giải thích có thể là các induction head phụ thuộc vào sự tồn tại của echo head như đã thảo luận trong [36].

4.3.2 Số lượng Induction Head

Để xác định số lượng tối ưu của induction head để sử dụng trong RazorAttention, trong Bảng 4 chúng tôi trình bày độ chính xác của RazorAttention dưới số lượng induction head khác nhau. Kết quả cho thấy rằng độ chính xác cải thiện liên tục với số lượng induction head tăng. Chúng tôi quyết định bao gồm 14% induction head để đạt được sự cân bằng tối ưu giữa tỷ lệ nén và hiệu suất mô hình.

[Hình 6: Token bù rất quan trọng để khôi phục mất mát thông tin do KV cache bị cắt ngắn gây ra.]

4.3.3 Tầm quan trọng của Token Bù

Trong Hình 6, rõ ràng được chứng minh rằng token bù rất quan trọng cho hiệu suất của RazorAttention. Các token bù thành công nén hầu hết thông tin từ các token bị loại bỏ, từ đó duy trì độ chính xác cao ngay cả với việc giảm KV cache đáng kể.

5 Kết luận

Trong bài báo này, chúng tôi đề xuất RazorAttention, một thuật toán nén KV cache mới, thành công đạt được tỷ lệ nén 3X cho các mô hình sử dụng RoPE hoặc ALiBi embedding. Không giống như các phương pháp loại bỏ token dựa trên tầm quan trọng trước đây không thể tránh khỏi việc loại bỏ thông tin ngữ nghĩa, RazorAttention bảo tồn tất cả thông tin ngữ nghĩa trong các đầu truy xuất. Chúng tôi chứng minh rằng các token xa có thể được nén hiệu quả thành token bù trong các đầu không phải truy xuất. Hơn nữa, tiêu chí cắt tỉa theo đầu của chúng tôi hoàn toàn tương thích với FlashAttention, làm cho RazorAttention trở thành phương pháp nén cắm-và-chạy tăng tốc suy luận của LLMs dưới bối cảnh mở rộng. Các thí nghiệm của chúng tôi chứng minh rằng RazorAttention có thể đạt được hiệu suất tương đương với mô hình gốc và vượt trội hơn các phương pháp trước đây trong cả độ chính xác và hiệu quả.

6 Hạn chế

Tuy nhiên, vẫn có những hạn chế nhất định trong nghiên cứu của chúng tôi. Câu hỏi đầu tiên là tại sao các đầu attention trong LLMs hoạt động khác nhau như vậy và các đầu truy xuất hoạt động như thế nào dưới đầu vào dài. Thách thức thứ hai nằm ở việc đạt được tỷ lệ nén cao hơn. Mặc dù chúng tôi đã thành công giảm KV cache 70%, chúng tôi tin rằng con số này có thể được cải thiện thêm. Hơn nữa, mặc dù chúng tôi đã kiểm tra thuật toán của mình trên một số mô hình, cấu hình tối ưu trên các mô hình khác có thể khác, có nghĩa là chúng tôi có thể cần nhiều hơn hoặc ít hơn các đầu truy xuất trong các trường hợp khác nhau. Những chủ đề này khá quan trọng và chúng tôi sẽ tiếp tục điều tra chúng trong nghiên cứu tương lai.

Tài liệu tham khảo

[1-40] [Danh sách tài liệu tham khảo từ 1 đến 40 được giữ nguyên như trong bản gốc]

--- TRANG 9-12 ---

[Phần tài liệu tham khảo tiếp tục và Phụ lục A với chứng minh Định lý 1 được giữ nguyên như trong bản gốc]
