# 2405.06067.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2405.06067.pdf
# Kích thước tệp: 1100236 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
HMT: Transformer Bộ Nhớ Phân Cấp cho Xử Lý Ngôn Ngữ Ngữ Cảnh Dài Hiệu Quả
Zifan He1, Yingqi Cao2, Zongyue Qin1, Neha Prakriya1,
Yizhou Sun1,and Jason Cong1
1University of California, Los Angeles,2University of California, San Diego
zifanhe1202@g.ucla.edu, yic033@ucsd.edu,
{qinzongyue, nehaprakriya, yzsun, cong}@cs.ucla.edu
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) dựa trên Transformer
đã được sử dụng rộng rãi trong các ứng dụng
xử lý ngôn ngữ. Tuy nhiên, do các ràng buộc
bộ nhớ của thiết bị, hầu hết chúng đều hạn chế
cửa sổ ngữ cảnh. Mặc dù các mô hình hồi quy
trong các nghiên cứu trước có thể ghi nhớ các
token trong quá khứ để kích hoạt ngữ cảnh không
giới hạn và duy trì hiệu quả, chúng có kiến trúc
bộ nhớ "phẳng". Các kiến trúc như vậy có hạn chế
trong việc lựa chọn và lọc thông tin. Vì con người
giỏi trong việc học tập và tự điều chỉnh, chúng tôi
tin rằng việc bắt chước hệ thống phân cấp bộ nhớ
của não có lợi cho việc ghi nhớ của mô hình.
Do đó, chúng tôi đề xuất Hierarchical Memory
Transformer (HMT)1, một khung mới hỗ trợ khả
năng xử lý ngữ cảnh dài của mô hình bằng cách
bắt chước hành vi ghi nhớ của con người. Tận dụng
việc hồi quy cấp phân đoạn được tăng cường bộ
nhớ, chúng tôi tổ chức hệ thống phân cấp bộ nhớ
bằng cách bảo tồn các token từ các phân đoạn đầu
vào sớm, truyền nhúng bộ nhớ dọc theo chuỗi,
và gọi lại thông tin liên quan từ lịch sử. Đánh giá
mô hình hóa ngôn ngữ tổng quát, các nhiệm vụ
hỏi đáp, và nhiệm vụ tóm tắt, chúng tôi cho thấy
HMT liên tục cải thiện khả năng xử lý ngữ cảnh
dài của các mô hình hiện có. Hơn nữa, HMT đạt
được chất lượng tạo sinh tương đương hoặc vượt
trội so với các LLM ngữ cảnh dài với ít hơn 2∼57×
tham số và bộ nhớ suy luận ít hơn 2.5∼116×,
vượt trội đáng kể so với các mô hình tăng cường
bộ nhớ trước đây.

1 Giới thiệu
Transformer (Vaswani et al., 2017) đã chứng minh
sức mạnh của nó trong học tập ngữ cảnh và được
sử dụng trong nhiều ứng dụng khác nhau trong xử lý
ngôn ngữ (Dong et al., 2019) và thị giác máy tính
(Dosovitskiy et al., 2020). Đối với mô hình transformer
chỉ giải mã, mỗi khối transformer chứa một
1https://github.com/OswaldHe/HMT-pytorchcơ chế tự chú ý và một mô-đun mạng truyền tiến.
Một lớp tự chú ý được tối ưu hóa có độ phức tạp
tính toán bậc hai và độ phức tạp không gian tuyến
tính (Dao et al., 2022) liên quan đến độ dài chuỗi
vì nó tính toán các tương tác giữa mỗi token và
tất cả các token trước đó trong chuỗi. Để duy trì
tốc độ suy luận và đáp ứng yêu cầu bộ nhớ, hầu hết
các mô hình transformer áp đặt độ dài chuỗi tối đa.
Ví dụ, mô hình Llama 3 được thiết kế để xử lý 8192
token (Dubey et al., 2024) và Llama 2 có thể xử lý
lên đến 4096 token (Touvron et al., 2023). Tuy nhiên,
các ứng dụng thực tế liên quan đến tài liệu dài,
chẳng hạn như tóm tắt sách (Rae et al., 2019) và
các nhiệm vụ hỏi đáp suốt đời (Sun et al., 2019; Dai
et al., 2022), có thể có luồng đầu vào rất lớn hoặc
thậm chí vô hạn.

Nghiên cứu hiện tại cố gắng xây dựng các transformer
ngữ cảnh dài sử dụng chú ý thưa (Beltagy
et al., 2020; Zhang et al., 2021; Kitaev et al., 2020),
các mô hình tăng cường truy xuất (Bertsch et al., 2023;
Wu et al., 2022), và các mô hình chuỗi hồi quy
(Peng et al., 2023a; Gu and Dao, 2023; Rae et al.,
2019). Tuy nhiên, các mô hình này phải đối mặt với
ít nhất một trong hai vấn đề: (1) khó khăn trong việc
thích ứng với các mô hình tương lai do thay đổi
trong kiến trúc mô hình cốt lõi và (2) hiệu quả thấp
cho đầu vào tầm xa dưới việc chuyển đổi ngữ cảnh
thường xuyên. Trong nghiên cứu này, chúng tôi đề
xuất Hierarchical Memory Transformer (HMT), một
khung mới để kích hoạt và tăng cường khả năng xử lý
ngữ cảnh dài của các mô hình. HMT biến đổi các
mô hình thành một mô hình hồi quy tăng cường bộ
nhớ bắt chước hệ thống phân cấp bộ nhớ của não
và hành vi ghi nhớ của con người. Nó có các tính
năng độc đáo sau:

Ghi nhớ phân cấp: HMT bắt chước hệ thống phân
cấp bộ nhớ của não (Burgin, 2011) sử dụng cả token
bộ nhớ đã học và token đầu vào hiện tại. HMT phân
tầng bộ nhớ thành cảm giác, ngắn hạn và dài hạn,
với các tương tác giữa chúng.arXiv:2405.06067v3  [cs.CL]  6 Feb 2025

--- TRANG 2 ---
Cơ chế truy xuất bộ nhớ: HMT bắt chước việc gọi
lại bộ nhớ bằng cách lưu trữ các nhúng bộ nhớ được
mã hóa được tạo ra từ các lần lặp trước và tìm kiếm
dựa trên mức độ liên quan đến các phân đoạn token
hiện tại.

Một lợi thế chính của việc sử dụng HMT so với các
mô hình tăng cường bộ nhớ khác là HMT là một khung
cắm và chạy độc lập với mô hình: các mô hình chỉ
giải mã trong tương lai có thể trực tiếp phục vụ như
mô hình xương sống của HMT để tăng cường khả năng
xử lý ngữ cảnh dài của chúng mà không cần nỗ lực
triển khai thêm. Với việc huấn luyện chung và tinh
chỉnh các tham số mới được giới thiệu và tham số gốc
của mô hình xương sống, HMT có thể áp dụng cho
một loạt rộng các LLM, bao gồm các mô hình dựa trên
transformer và các mô hình không gian trạng thái.
Đóng góp của chúng tôi bao gồm:

•HMT liên tục cải thiện chất lượng tạo sinh của
các mô hình với ngữ cảnh dài cho nhiều kiến trúc
mô hình khác nhau. Chúng tôi chứng minh HMT trên
cả kiến trúc dựa trên transformer và các mô hình
không gian trạng thái. Đánh giá trên các bộ dữ liệu
Wikitext-103, PG-19 (Rae et al., 2019), và PubMedQA
(Jin et al., 2019) với nhiều ngữ cảnh được nối,
HMT có thể cải thiện hiệu quả lên đến 25.5% về
perplexity và độ chính xác dự đoán cao hơn 1.0%
so với các mô hình cơ sở.

•HMT với các mô hình xương sống nhỏ có thể vượt
trội hơn các mô hình lớn được huấn luyện trên các
mẫu ngữ cảnh dài hơn, ngụ ý hiệu quả bộ nhớ cao.
Chúng tôi đánh giá HMT với các mô hình SmolLM
(Allal et al., 2024), OPT (Zhang et al., 2022), và
OpenLlamaV2 (Geng and Liu, 2023) trên benchmark
LongBench (Bai et al., 2023b). Tóm lại, HMT có thể
đạt được kết quả metric tương đương hoặc cao hơn
với ít hơn 2∼57× tham số và yêu cầu bộ nhớ suy luận
thấp hơn 2.5∼116× so với các mô hình ngôn ngữ lớn
ngữ cảnh dài.

•HMT vượt trội hơn các phương pháp trước đây chuyên
dụng cho xử lý ngữ cảnh dài hiệu quả bằng cách nén
ngữ cảnh. Chúng tôi so sánh HMT với RMT (Bulatov
et al., 2022), LongMem (Wang et al., 2024), Memorizing
Transformer (Wu et al., 2022), CCM (Kim et al., 2023),
và HOMER (Song et al., 2024), là các SOTA gần đây
về các phương pháp tăng cường bộ nhớ và phân cấp.
Với mô hình xương sống cùng kích thước hoặc tương
tự, HMT có chất lượng tạo sinh tốt hơn trong cả mô
hình hóa ngôn ngữ tổng quát và các nhiệm vụ QA.
Hơn nữa, HMT có độ phức tạp bộ nhớ thấp hơn, cho
thấy khả năng mở rộng tốt hơn khi độ dài đầu vào
tăng.

2 Các Nghiên Cứu Liên Quan và Công Thức Hóa Vấn Đề
Trước tiên chúng tôi sẽ thảo luận về các nỗ lực hiện
tại về transformer tầm xa và các mô hình chuỗi hồi quy
cho xử lý ngôn ngữ ngữ cảnh vô hạn dài. Sau đó,
chúng tôi nhấn mạnh một vấn đề quan trọng trong
các ứng dụng thực tế.

2.1 Transformer Ngữ Cảnh Dài
Vì một trong những nút thắt cổ chai của transformer
là độ phức tạp tính toán bậc hai của cơ chế tự chú ý,
một cách tiếp cận tự nhiên là làm thưa việc tính toán
chú ý. Một mẫu chú ý thưa đơn giản là chú ý cửa sổ
trượt (Kovaleva et al., 2019), trong đó mỗi token chú ý
đến các neighbor trong một cửa sổ cục bộ. Tuy nhiên,
điều này bỏ qua tương tác tầm xa giữa các từ. Các
nghiên cứu hiện tại như Longformer (Beltagy et al.,
2020) và Poolingformer (Zhang et al., 2021) mở rộng
chú ý cửa sổ trượt bằng cách thêm các token chú ý
toàn cục và áp dụng pooling để mở rộng vùng trường
tiếp nhận. Unlimiformer (Bertsch et al., 2023) áp dụng
phương pháp tạo sinh tăng cường truy xuất bằng cách
tìm kiếm K token quan trọng nhất cho chuỗi đến.
Sau đó nó áp dụng chú ý chỉ với những token đó trong
các bộ giải mã, dẫn đến việc tính toán được cắt tỉa
với những mất mát nhỏ. Tuy nhiên, sự đóng góp của
các token ít liên quan có thể tích lũy theo thời gian
và ảnh hưởng đến việc tạo sinh chuỗi tổng thể. Mặc dù
các phương pháp này mở rộng độ dài ngữ cảnh có thể
đạt được, chúng không thể ngăn chặn việc tiêu thụ bộ
nhớ tăng lên khi độ dài đầu vào tăng. Thay vào đó,
việc nén các token trong quá khứ bằng cách sử dụng
mô hình chuỗi hồi quy có thể giảm việc tiêu thụ bộ
nhớ bằng cách đặc các thông tin vào một nhúng có
kích thước cố định.

2.2 Các Mô Hình Chuỗi Hồi Quy
Mạng Neural Hồi Quy (RNN) đã được khám phá rộng
rãi trong nghiên cứu xử lý chuỗi, bao gồm Long
Short-term Memory (Hochreiter and Schmidhuber,
1997) và Gated Recurrent Unit (Chung et al., 2014).
Chúng cho thấy rằng RNN hoạt động tốt trong việc
ghi nhớ thông tin quá khứ và thân thiện với phần
cứng để triển khai các bộ tăng tốc tùy chỉnh (Chang
et al., 2015). Tuy nhiên, RNN có lợi thế hạn chế trong
việc học các mối quan hệ ngữ cảnh giữa các từ so
với cơ chế tự chú ý trong xử lý ngôn ngữ (Bahdanau
et al., 2014). Một cách tiếp cận để giảm thiểu vấn đề
này là hồi quy thô, trong đó mô hình

--- TRANG 3 ---
chia đầu vào thành các phân đoạn, thực hiện chú ý
bên trong mỗi phân đoạn, và truyền các trạng thái
(tức là thông tin nén dưới dạng nhúng) giữa các phân
đoạn. Compressive Transformer (Rae et al., 2019) 
tiếp tục lưu trữ và nén các trạng thái trước đó để
tăng cường việc ghi nhớ. Recurrent Memory
Transformer (RMT) (Bulatov et al., 2022) sử dụng
token bộ nhớ để tóm tắt và truyền thông tin phân
đoạn mà không cần sửa đổi kiến trúc khối transformer.
Về mặt lý thuyết, chúng có thể xử lý các chuỗi dài
không giới hạn, nhưng thông tin trước đó sẽ bị pha
loãng sau nhiều lần tóm tắt và chất lượng tạo sinh
có thể giảm khi thông tin ít liên quan chiếm bộ nhớ.
Các nghiên cứu gần đây (Chevalier et al., 2023; Kim
et al., 2023) nhằm tối ưu hóa thêm RMT để cải thiện
chất lượng tạo sinh bằng cách nối kết quả của các
lần tóm tắt, nhưng điều này hy sinh hiệu quả bộ nhớ
suy luận.

Một cách tiếp cận khác tăng cường RNN bằng cách
liên quan đến các tương tác giữa đầu vào hiện tại
và các trạng thái trước đó để học các mối quan hệ
ngữ cảnh theo cách tương tự như cơ chế tự chú ý
và tăng tốc tính toán với tích chập tuyến tính. Một
trong những đại diện, RWKV (Peng et al., 2023a),
là một mô hình RNN lấy cảm hứng từ transformer
không chú ý (AFT) (Zhai et al., 2021). Nó bao gồm
một mô-đun time-mixing để học từ các trạng thái trước
đó và một mô-đun channel-mixing để học từ đầu ra
trước đó. Mamba (Gu and Dao, 2023) là một phương
pháp hồi quy khác dựa trên mô hình không gian trạng
thái sử dụng tích chập có cổng để tăng tốc suy luận
mô hình. Các mô hình này tiết kiệm năng lượng và
bộ nhớ với tốc độ huấn luyện nhanh và có thể đạt
được hiệu suất cao trong các nhiệm vụ ghi nhớ (ví dụ:
gọi lại liên kết), nhưng có hạn chế trong việc nắm bắt
các mối quan hệ ngữ cảnh và lọc thông tin không
liên quan. Các nghiên cứu gần đây kết hợp transformer
với Mamba (Lieber et al., 2024; Team et al., 2024)
để giảm thiểu vấn đề này, nhưng điều này lại đưa ra
vấn đề mở rộng của transformer.

2.3 Công Thức Hóa Vấn Đề: Xử Lý Ngữ Cảnh Dài
Thích Ứng
Chúng tôi dự định phát triển một mô hình có thể xử lý
đầu vào ngữ cảnh dài vô hạn với khả năng thích ứng
ngữ cảnh: Dựa trên ngữ cảnh/chủ đề của luồng đầu vào,
mô hình có thể thích ứng lựa chọn thông tin liên quan
trong quá khứ để tăng cường hiệu quả, vì ngữ cảnh
không liên quan có thể làm phân tán mô hình (Shi
et al., 2023). Trong các ứng dụng thực tế, bị hạn chế
bởi băng thông và dung lượng bộ nhớ, cũng như tốc
độ tạo dữ liệu

Bảng 1: Ký hiệu được sử dụng để minh họa kiến trúc
của HMT trong Phần 3 và Hình 1.
KÝ HIỆU Ý NGHĨA
Hn CÁC NHÚNG ẨN CỦA PHÂN ĐOẠN THỨ n
L CHIỀU DÀI PHÂN ĐOẠN
Hn[L−k, L) k NHÚNG CUỐI CÙNG CỦA Hn
Hn[0, j) j NHÚNG ĐẦU TIÊN CỦA Hn
BBM (·) MÔ HÌNH XƯƠNG SỐNG
Pn NHÚNG PROMPT GHI NHỚ
Hout
n NHÚNG ẨN CHO VIỆC TẠO LOGIT
Mn NHÚNG BỘ NHỚ CỦA PHÂN ĐOẠN THỨ n
N SỐ LƯỢNG NHÚNG BỘ NHỚ ĐƯỢC LƯU CACHE
M[n−N+1,n) CÁC NHÚNG BỘ NHỚ ĐƯỢC LƯU CACHE
T NHÚNG PROMPT TÓM TẮT PHÂN ĐOẠN
Sn NHÚNG TÓM TẮT CỦA PHÂN ĐOẠN THỨ n

tốc độ, các tài liệu dài không thể được đọc toàn bộ
bởi phần cứng tính toán (Agerri et al., 2015). Hơn nữa,
người dùng liên tục tương tác với mô hình ngôn ngữ
có thể tham khảo chủ đề trước đó hoặc chuyển sang
chủ đề khác có mức độ liên quan cao với thông tin
quá khứ. Để đạt hiệu quả, hầu hết các mô hình hồi quy
cần mã hóa tất cả đầu vào trước đó trong các trạng thái,
có thể chứa thông tin không liên quan và làm giảm
chất lượng của mô hình.

3 Hierarchical Memory Transformer
Ý tưởng chính của HMT là lưu trữ thông tin theo cách
phân cấp và tìm kiếm thông tin liên quan trong toàn
bộ hệ thống phân cấp bộ nhớ. Bảng 1 mô tả tất cả
các ký hiệu chúng tôi sử dụng để minh họa kiến trúc
HMT trong phần này.

3.1 Quy Trình Tổng Thể
Với một mô hình xương sống để tăng cường, HMT
chia đầu vào thành các phân đoạn L-token và hoạt
động trên các nhúng ẩn của các phân đoạn token
({Hn}∞
n=0), được tạo ra bởi lớp nhúng token của
mô hình xương sống. Đối với mỗi phân đoạn n, HMT
thực hiện qua bốn bước được hiển thị trong Hình 1:

1)Mã hóa biểu diễn bởi mô hình xương sống, mã hóa
một phần của phân đoạn chứa bản chất của chủ đề
đang diễn ra thành một nhúng duy nhất để đại diện
cho ngữ cảnh của nó, ký hiệu là Hn.

2)Tìm kiếm bộ nhớ, sử dụng ngữ cảnh hiện tại như
một truy vấn để tìm thông tin liên quan trong bộ nhớ.

3)Thêm bộ nhớ cảm giác, tăng cường phân đoạn để
nắm bắt thông tin trong phân đoạn trước và thông tin
liên quan khác.

4)Giải mã và tóm tắt, xử lý phân đoạn được tăng cường
để lấy các nhúng ẩn

--- TRANG 4 ---
Hình 1: Quy trình tổng thể của HMT. Đối với một phân đoạn, (1) HMT sẽ đầu tiên thực hiện mã hóa biểu diễn, sử dụng nhúng prompt tóm tắt phân đoạn ( T) để tóm tắt một phần của phân đoạn. (2) Nhúng tóm tắt phân đoạn được tạo ra ( Sn) được sử dụng với các nhúng bộ nhớ được lưu cache để tìm kiếm bộ nhớ với cơ chế chú ý chéo. Đầu ra là một nhúng prompt ghi nhớ ( Pn) chứa thông tin liên quan đến phân đoạn hiện tại. (3) Nhúng prompt ghi nhớ và k nhúng cuối cùng từ phân đoạn trước sẽ tăng cường phân đoạn. (4) Mô hình xương sống (BBM) sẽ xử lý phân đoạn được tăng cường và tạo ra các nhúng ẩn cho logit ( Hout
n) và nhúng bộ nhớ ( Mn), sẽ được đẩy vào bộ nhớ dài hạn.

để tạo logit và một nhúng bộ nhớ tóm tắt phân đoạn
được tăng cường.

Hai bước đầu tiên là cơ chế truy xuất bộ nhớ được
thảo luận trong Phần 3.2. Bước 3 và 4 được giải thích
trong Phần 3.3 cùng với khái niệm ghi nhớ phân cấp.

3.2 Cơ Chế Truy Xuất Bộ Nhớ
Để xử lý việc chuyển đổi ngữ cảnh và ngăn chặn sự
can thiệp của ngữ cảnh không liên quan, HMT thực
hiện truy xuất bộ nhớ để chỉ trích xuất thông tin liên
quan từ kiến thức quá khứ. Cơ chế truy xuất bộ nhớ
bao gồm ba bước: trích xuất biểu diễn, tìm kiếm bộ
nhớ và tăng cường bộ nhớ.

Mã Hóa Biểu Diễn: Được mô tả trong Bước 1 của
Hình 1, HMT chọn j nhúng đầu tiên từ các nhúng ẩn
của phân đoạn thứ n, Hn, để trích xuất chủ đề của
phân đoạn. Các nhúng được tăng cường với nhúng
prompt tóm tắt phân đoạn T. T là một nhúng tham số
có thể học được, được triển khai để nhắc mô hình
xương sống (BBM) tóm tắt phân đoạn bằng soft prompt
tuning (Liu et al., 2023). Thay vì trích xuất từ nhúng
token của BBM, chúng tôi làm cho T có thể học để
cho phép không gian nhúng prompt lớn hơn cho việc
tóm tắt. Mô hình xương sống sau đó sẽ xử lý các
nhúng được tăng cường và tạo ra một nhúng mới ở
cuối đầu ra như biểu diễn của phân đoạn:

Sn=BBM ([T||Hn[0, j)||T])[j, j+ 1) (1)

trong đó Sn là nhúng tóm tắt chỉ của phân đoạn thứ n,
BBM (·) là mô hình xương sống, và "||" là toán tử nối.
Sn sẽ được sử dụng cho tìm kiếm bộ nhớ.

Tìm Kiếm Bộ Nhớ: Được hiển thị trong Bước 2 của
Hình 1, Sn được sử dụng như một truy vấn để tìm
các nhúng bộ nhớ liên quan được tạo ra từ Bước 4
khi xử lý các phân đoạn trước đó. Chúng tôi giữ một
cửa sổ trượt của N nhúng ( M[n−N+1,n)) và sau đó
tính toán:

Qn=SnWq, Kn=M[n−N+1,n)Wk (2)
Pn=softmax (QnKT
n√dh)M[n−N+1,n) (3)

--- TRANG 5 ---
trong đó dh là chiều ẩn của cơ chế chú ý chéo. Việc
tính toán tương tự như chú ý chéo không có projection
giá trị và đầu ra. Softmax (QnKT
n√dh) tính toán điểm
tương tự được chuẩn hóa và áp dụng trực tiếp lên
M[n−N+1,n) để đảm bảo phân phối tương tự của giá
trị đầu ra và các token bộ nhớ cũ. Chúng tôi mong đợi
rằng các projection Wq và Wk có thể được huấn luyện
sao cho các tóm tắt chứa ngữ cảnh tương tự có điểm
chú ý cao sau các projection.

Đầu ra của tìm kiếm bộ nhớ là một nhúng prompt ghi
nhớ Pn chứa thông tin liên quan đến phân đoạn thứ n.
Nó sẽ được áp dụng để tăng cường phân đoạn thứ n.
Lưu ý rằng bộ nhớ của HMT là tích lũy: nhúng bộ nhớ
thứ n chứa thông tin của tất cả n−1 phân đoạn trước
đó, với mất mát thông tin cao hơn cho các phân đoạn
cũ hơn. Chúng tôi hy vọng rằng việc truy xuất bộ nhớ
sẽ tăng cường bộ nhớ liên quan và giảm thiểu mất
mát này.

Trong thực tế, mã hóa biểu diễn được thực hiện song
song với suy luận mô hình trên GPU vì chúng là các
nhiệm vụ độc lập. Tìm kiếm bộ nhớ có độ phức tạp
thời gian O(N), và cũng có thể chạy song song với
suy luận phân đoạn khi N nhỏ (ví dụ: N= 300 ). Do
đó, tổng chi phí thời gian chạy của HMT là không đáng
kể.

3.3 Ghi Nhớ Phân Cấp
Bộ nhớ của con người có thể được phân loại thành
ba tầng: bộ nhớ cảm giác, bộ nhớ ngắn hạn và bộ nhớ
dài hạn (Burgin, 2011). Bộ nhớ cảm giác đề cập đến
bộ nhớ rất ngắn hạn được tạo ra từ thông tin cảm giác,
chẳng hạn như thị giác và thính giác. Bộ nhớ ngắn
hạn và dài hạn là những bộ nhớ lâu dài, được phân
biệt bởi thời gian chúng tồn tại trong não. HMT được
lấy cảm hứng từ hệ thống phân cấp bộ nhớ này.

Bộ Nhớ Cảm Giác: Bộ nhớ cảm giác cho phân đoạn
thứ n đề cập đến k nhúng token cuối cùng của
Hn−1, Hn−1[L−k, L). Khi suy luận phân đoạn thứ n,
HMT sẽ tăng cường các nhúng token tương ứng Hn
bằng cách thêm nó vào đầu với Hn[L−k, L), được
hiển thị trong Bước 3 của Hình 1.

Bộ Nhớ Ngắn Hạn: HMT sẽ mã hóa phân đoạn thành
một nhúng phục vụ như một "tóm tắt" của phân đoạn.
Đầu tiên, HMT sẽ thêm và đặt trước nhúng prompt
ghi nhớ Pn vào phân đoạn được tăng cường. Điều này
hướng dẫn mô hình xương sống nén phân đoạn và
ngữ cảnh liên quan thành một nhúng tóm tắt với nhận
thức về các vị trí tương đối của ngữ cảnh. Như được
mô tả trong Bước 4 của Hình 1, chúng tôi huấn luyện
HMT sao cho

H=BBM (Pn||Hn−1[L−k, L)||Hn||Pn)(4)
Hout
n||Mn=H[k+ 1, L+k+ 2) (5)

trong đó Mn là nhúng bộ nhớ của phân đoạn thứ n.
Hout
n là một tập hợp L nhúng ẩn sẽ được sử dụng để
tạo logit.

Bộ Nhớ Dài Hạn: Mỗi nhúng bộ nhớ được tạo ra sẽ
được lưu cache như bộ nhớ dài hạn. Các nhúng được
lưu cache sẽ được sử dụng như đầu vào của cơ chế
truy xuất bộ nhớ để tạo ra nhúng token ghi nhớ Pn
cho mỗi phân đoạn như được minh họa trong các phần
trước.

4 Thí Nghiệm
Chúng tôi đánh giá HMT với nhiều mô hình xương sống
khác nhau bao gồm SmolLM 135M (Allal et al., 2024),
OPT 350M, OPT 2.7B (Zhang et al., 2022), OpenLlamaV2
3B (Geng and Liu, 2023), RWKV 3B (Peng et al., 2023a),
và Llama 2 7B (Touvron et al., 2023), dưới cùng ràng
buộc bộ nhớ (tức là cùng cửa sổ ngữ cảnh tối đa). Hơn
nữa, chúng tôi thử nghiệm một số mô hình nhắm đến
ngữ cảnh dài (Mamba 370M (Gu and Dao, 2023), Yi-6B-200K
(Young et al., 2024), và Mistral 7B (Jiang et al., 2023))
để chứng minh lợi ích của HMT về chất lượng tạo sinh
và tiêu thụ bộ nhớ. Chúng tôi đánh giá HMT với các
mô hình không gian trạng thái (RWKV và Mamba) làm
xương sống vì chúng tôi tin rằng các mô hình đã có
thể xử lý đầu vào dài vô hạn sẽ được hưởng lợi nhiều
hơn từ HMT. Tất cả các mô hình được đề cập đều được
huấn luyện và đánh giá trên 4 GPU AMD MI210, có thể
xử lý các mô hình lên đến 7B tham số. Chúng tôi thử
nghiệm thêm HMT trên 4 GPU NVIDIA A100-80GB cho
mô hình Qwen 2.5 14B (Bai et al., 2023a) để chứng
minh khả năng mở rộng của nó đối với các mô hình
lớn hơn và đạt được sự cải thiện hiệu quả nhất quán.

Để tinh chỉnh các tham số bổ sung được giới thiệu
bởi HMT, chúng tôi sử dụng bộ dữ liệu RedPajamaV2
(Computer, 2023) để tiền huấn luyện mỗi mô hình.
Lưu ý rằng HMT đã giới thiệu các siêu tham số mô
hình mới trên mô hình xương sống ( L, j, N, và k).
Một cấu hình phổ biến là L= 1024 , j= 512 , N= 300 ,
và k= 32 , và chúng tôi điều chỉnh các giá trị này cho
mỗi mô hình để đạt được hiệu suất tốt nhất. Để so
sánh với các nghiên cứu trước (RMT, LongMem, Memorizing
Transformer, CCM), chúng tôi áp dụng cùng mô hình
xương sống nếu phương pháp có thể áp dụng cho bất
kỳ

--- TRANG 6 ---
mô hình nào, hoặc tìm một mô hình xương sống có
kích thước tương tự nếu phương pháp yêu cầu kiến
trúc đặc biệt.

Đối với benchmark ngữ cảnh dài, chúng tôi chọn các
tập con (NarrativeQA, Qasper, và MultiFieldQA-en
cho QA tài liệu đơn; HotpotQA, 2WikiMQA, và MuSiQue
cho QA đa tài liệu; GovReport, QMSum, và Multi-News
cho tóm tắt; TriviaQA cho few-shot learning) từ một
benchmark được công nhận rộng rãi, LongBench (Bai
et al., 2023b), và đo chúng so với các mô hình được
báo cáo trong bảng xếp hạng LongBench. Tuy nhiên,
độ dài tài liệu trung bình tối đa của các tập kiểm tra
trong LongBench ngắn hơn 20k từ, không phải là
rất dài đối với các mô hình ngữ cảnh dài hiện đại.
Để hiểu rõ hơn khả năng xử lý ngữ cảnh dài của HMT
dưới nhiều tình huống ngữ cảnh khác nhau, chúng
tôi nghiên cứu thêm HMT trên các mẫu bộ dữ liệu
được thiết kế và có thể kiểm soát. Đối với các bộ dữ
liệu được thiết kế, chúng tôi tạo ra từ các bộ dữ liệu
hiện có để tạo thành đầu vào dài. Đối với các nhiệm
vụ ngôn ngữ tổng quát, các mô hình được thử nghiệm
cho các nhiệm vụ tạo token tiếp theo với Wikitext-103
(Merity et al., 2016) (2-3k từ mỗi mẫu) và bộ dữ liệu
PG-19 (Rae et al., 2019) (trung bình 69k từ mỗi mẫu).
Các mẫu sẽ được nối hoặc chia thành các đoạn để
tạo thành các mẫu dài hơn và điều tra mối quan hệ
giữa độ dài đầu vào và hiệu quả của mô hình. Đối với
các nhiệm vụ hỏi đáp, chúng tôi chọn PubMedQA (Jin
et al., 2019), một bộ dữ liệu hỏi đáp y sinh với các
ngữ cảnh tương ứng. Chúng tôi tạo ra bộ dữ liệu để
đánh giá HMT với đầu vào đa ngữ cảnh, được mô tả
trong Phụ lục I.

5 Kết Quả và Các Quan Sát Chính
Trong phần này, chúng tôi minh họa kết quả chính
của HMT. Thêm các nghiên cứu ablation trong Phụ lục
E và G.

5.1 Tác Động Lên Các Mô Hình Xương Sống
Bằng cách giới thiệu thêm 0.5% ∼1.3% (1.77M ∼33.5M)
tham số, HMT có thể tăng cường các mô hình với
nhiều kiến trúc khác nhau để cải thiện chất lượng tạo
sinh khi xử lý đầu vào ngữ cảnh dài. Chúng tôi chứng
minh tính năng này với các nhiệm vụ mô hình hóa
ngôn ngữ tổng quát và hỏi đáp.

HMT liên tục cải thiện các mô hình xương sống trong
các nhiệm vụ mô hình hóa ngôn ngữ tổng quát khi
xử lý đầu vào dài. Hình 2 và 3 so sánh perplexity
của các mô hình OPT 2.7B, RWKV 3B, và OpenLlamaV2
3B có và không có HMT trên các bộ dữ liệu Wikitext-103
và PG-19. Trên đầu vào trải dài từ 2k ∼100k token,
HMT liên tục nâng cao chất lượng tạo sinh của tất
cả các mô hình này.

Bảng 2: Khả năng mở rộng của HMT. PPL kiểm tra trung
bình được tính bằng cách lấy PPL trung bình cho các
mẫu trong mỗi độ dài chuỗi trong thí nghiệm.
MÔ HÌNH PPL KIỂM TRA TB (WIKITEXT ) (↓)
OPT 350M 15.11
HMT + OPT 350M 14.28 (-5.8%)
OPT 2.7B 12.12
HMT + OPT 2.7B 8.61 (-28.9%)
RWKV 430M 19.33
HMT + RWKV 430M 16.10 (-16.6%)
RWKV 3B 13.30
HMT + RWKV 3B 9.93 (-25.3%)

Hơn nữa, Bảng 2 trình bày cách các cải thiện được
đạt bởi HMT tỷ lệ với kích thước mô hình cho các
mô hình cùng họ. Để tăng cường thêm lập luận của
chúng tôi rằng HMT có thể mang lại lợi ích cho các
mô hình lớn hơn, chúng tôi đánh giá HMT với Qwen
2.5 14B sử dụng 4 GPU A100-80GB để huấn luyện.
Như được mô tả trong Hình 4, HMT vẫn có thể tăng
hiệu quả của mô hình xương sống trên PG-19.

Lưu ý rằng sự cải thiện không nhất thiết chỉ được
đóng góp bởi các tham số bổ sung. Có nhiều tham số
hơn không phải lúc nào cũng dẫn đến hiệu suất cao
hơn. Ví dụ, HMT thúc đẩy OPT 2.7B để đạt được
perplexity thấp hơn OpenLlama 3B với ít hơn 20.7%
tham số, trong khi OPT 2.7B hoạt động kém hơn khi
không có HMT. Phần 5.2 mô tả thêm các ví dụ về
HMT đạt được chất lượng tạo sinh vượt trội với các
mô hình nhỏ hơn.

HMT tăng cường khả năng suy luận ngữ cảnh trả lời
dài và khả năng dự đoán trả lời ngắn trong các nhiệm
vụ hỏi đáp. Một trong những trường hợp sử dụng của
HMT là xử lý các nhiệm vụ hỏi đáp liên quan đến nhiều
ngữ cảnh. Do đó, chúng tôi chọn bộ dữ liệu PubMedQA
và tạo ra các mẫu QA ngữ cảnh dài với số lượng ngữ
cảnh có thể kiểm soát để đánh giá hiệu quả của HMT.
Hai metric được sử dụng: đối với trả lời dài, chúng
tôi tính PPL để đánh giá suy luận ngữ cảnh của HMT;
đối với trả lời ngắn, chúng tôi đo độ chính xác phản
hồi. Như thấy trong Hình 5 và 6, đối với các mẫu có
2 đến 10 ngữ cảnh, HMT tăng hiệu quả trong PPL
lên 9.48% đối với trả lời dài. Đối với các nhiệm vụ
trả lời ngắn, HMT chính xác hơn 1.0% so với mô hình
xương sống và thể hiện lợi thế đáng kể khi các mẫu
có nhiều ngữ cảnh hơn. Tóm lại, HMT tăng cả tính
đúng đắn và khả năng suy luận của các mô hình trong
các nhiệm vụ QA ngữ cảnh dài.

5.2 So Sánh với Các Mô Hình Ngữ Cảnh Dài
Kết hợp với các mô hình nhỏ và ngữ cảnh ngắn, HMT
có thể hiệu quả hơn các mô hình lớn

--- TRANG 7 ---
Hình 2: PPL kiểm tra của HMT, RMT, và ba mô hình cơ sở (OPT 2.7B, RWKV 3B, OpenLlamaV2 3B) với bộ dữ liệu Wikitext-103. HMT vượt trội hơn RMT 13.0% cho OPT và 10.8% cho OpenLlamaV2. Đối với RWKV , HMT thậm chí có thể thúc đẩy hiệu quả lên 16.5%, trong khi RMT làm xấu đi hiệu quả.

Hình 3: PPL kiểm tra của HMT, RMT, và ba mô hình cơ sở (OPT 2.7B, RWKV 3B, OpenLlamaV2 3B), được đánh giá trên bộ dữ liệu PG-19. HMT vượt trội hơn RMT 3.98% cho OPT và 6.85% cho OpenLlamaV2. Đối với RWKV , HMT có thể cải thiện hiệu quả lên 9.96%.

Hình 4: PPL kiểm tra của HMT, RMT, và mô hình cơ sở cho Qwen 2.5 14B trên bộ dữ liệu PG-19. HMT thúc đẩy hiệu quả của mô hình cơ sở lên 10.0%, trong khi RMT làm xấu đi hiệu quả của nó.

được huấn luyện trên đầu vào ngữ cảnh dài. Bảng 3
hiển thị kết quả metric của các mô hình tăng cường
HMT trên các tập con của LongBench (Bai et al., 2023b)
và so sánh chúng với các mô hình lớn chuyên dụng
cho ngữ cảnh dài. Các tập con chứa nhiều nhiệm vụ
tạo sinh khác nhau, bao gồm QA tài liệu đơn/đa, tóm
tắt và few-shot learning. Với yêu cầu bộ nhớ suy luận
thấp hơn đáng kể, HMT áp dụng cho các mô hình nhỏ
có thể đạt được các metric tương đương hoặc tốt hơn
so với các mô hình lớn, cho thấy lợi thế tài nguyên
đáng kể. Cụ thể, chúng tôi quan sát thấy HMT với
các mô hình nhỏ hoạt động tốt trong việc tạo ra các
phản hồi ngắn cho đầu vào dài và đa ngữ cảnh, nhờ
vào khả năng lọc ngữ cảnh của nó. Tuy nhiên, nó thể
hiện hiệu suất tương đương hoặc yếu hơn trong việc
tạo ra các phản hồi dài, vì các mô hình nhỏ có giới
hạn tạo token ngắn hơn so

Hình 5: Chất lượng trả lời dài của RMT và HMT áp dụng trên Llama-2 7B, được đánh giá trên bộ dữ liệu PubMedQA. HMT hiệu quả hơn 8.98% so với RMT.

với các mô hình lớn.

Hơn nữa, việc áp dụng HMT cho các mô hình ngữ cảnh
dài có thể cải thiện thêm hiệu quả của chúng và giảm
tiêu thụ bộ nhớ suy luận. Ví dụ, GPU AMD MI210 không
thể xử lý suy luận đầu vào 30k token với mô hình
Yi-6B-200K do ràng buộc bộ nhớ. Áp dụng chiến lược
cửa sổ trượt với cửa sổ 5.2K-token (Yi-6B-SW-5.2K),
mô hình tiêu thụ 44.8 GB VRAM. Ngược lại, HMT +
Yi-6B-200K chỉ yêu cầu 33.9 GB VRAM để xử lý 30k
token với độ dài phân đoạn nhỏ (512 token), với sự
cải thiện hiệu quả 2%. Bảng 4 trình bày hiệu quả của
các mô hình tầm xa trên Wikitext-103 so với một số
mô hình tăng cường HMT, bao gồm các mô hình Mamba
và Mistral.

2Mặc dù mô hình này được huấn luyện với các mẫu 200K-token, nó không thể chạy trên MI210 do ràng buộc bộ nhớ.

--- TRANG 8 ---
Bảng 3: Kết quả metric của các mô hình nhỏ tăng cường HMT và các mô hình lớn được huấn luyện trên ngữ cảnh dài hơn. Các mô hình có HMT có thể xử lý ngữ cảnh dài vô hạn, nhưng chỉ giữ độ dài KV cache cố định (giá trị trong ngoặc đơn). Chúng tôi đánh giá trên các tập con của LongBench, bao gồm QMSum (QMS), MuSiQue (MSQ), Qasper (QASP), NarrativeQA (NQA), MultiFieldQA-en (MFQA-en), GovReport (GR), TriviaQA (TQA), HotpotQA (HQA), 2WikiMQA (2WMQA), và MultiNews (MN). Mem Req chỉ ra yêu cầu bộ nhớ suy luận tối thiểu (để lưu trữ tham số và KV cache). Suy luận thực tế có thể yêu cầu VRAM lớn hơn.

MÔ HÌNH CỬA SỔ NGỮCẢNH QMS MSQ QASP NQA MFQA- EN GR TQA HQA 2WMQA MN YÊU CẦU BỘ NHỚ(GB↓)
GPT-3.5 T URBO 16384 23.4 26.9 43.3 23.6 52.3 29.5 91.4 51.6 37.7 26.7 46.7
LLAMA 2 7B C HAT 4096 20.8 9.4 19.2 18.7 36.8 27.3 77.8 25.4 32.8 25.8 15.1
LONG CHAT V1.5 7B 32768 22.7 9.7 27.7 16.9 41.4 30.8 82.3 31.5 20.6 26.4 22.6
XG EN7B 8192 20.5 10.3 18.1 18.0 37.7 27.3 77.8 29.7 21.1 26.2 16.2
INTERN LM 7B 8192 15.9 9.0 16.7 12.1 23.4 9.7 77.8 28.7 21.1 22.8 16.2
CHATGLM2 6B 32768 24.0 21.9 31.5 21.1 46.2 32.4 78.7 45.1 34.0 26.5 19.5
VICUNA V1.5 7B 16384 22.8 9.8 26.1 19.4 38.5 27.9 86.2 25.3 20.8 27.2 18.3
CHATGLM3 6B 32768 23.9 40.4 43.3 26.0 51.7 36.8 87.1 54.4 44.9 27.9 19.5
HMT + OPT 350M ∞(1024) 22.2 15.6 32.3 17.2 62.7 13.8 87.2 25.3 29.2 10.5 0.75
HMT + O PENLLAMA V2 3B ∞(512) 21.4 42.3 35.7 21.6 58.1 15.8 93.0 29.9 27.7 11.0 6.1
HMT + S MOL LM 135M ∞(1024) 19.4 14.1 30.2 19.6 48.1 15.7 81.3 27.6 23.3 9.5 0.4
CHIỀU DÀI TB BENCHMARK (TỪ ) 10614 11214 3619 18409 4559 8734 8209 9151 4887 2113

Hình 6: Độ chính xác phản hồi ngắn của RMT và HMT áp dụng trên Llama-2 7B, được đánh giá trên bộ dữ liệu PubMedQA. HMT chính xác hơn 1.8% so với RMT.

Bảng 4: Chất lượng của các mô hình ngữ cảnh dài và HMT với nhiều mô hình xương sống khác nhau. Kích thước đầu vào là 30k token và bộ dữ liệu là Wikitext-103.

MÔ HÌNH NGỮCẢNH TỐI ĐA PPL KIỂM TRA (WIKITEXT )
RWKV 3B ∞ 13.13
MAMBA 370M ∞ 87.08
YI-6B-200K 200K OOM2
YI-6B-SW-5.2K 200K 6.89
MISTRAL -7B 32K 5.47
HMT + OPT 350M ∞(1024) 13.67
HMT + O PENLLAMA V2 3B ∞(512) 7.04
HMT + RWKV 3B ∞(256) 10.94
HMT + M AMBA 370M ∞(256) 16.71
HMT + Y I-6B-200K ∞(512) 6.75
HMT + M ISTRAL -7B ∞(512) 5.12

5.3 So Sánh với Các Phương Pháp Tăng Cường Bộ Nhớ và Phân Cấp
Một mô hình tăng cường bộ nhớ phổ biến là recurrent memory transformer (Bulatov et al., 2022) (RMT). Đánh giá của chúng tôi cho thấy HMT nói chung tốt hơn trong cả mô hình hóa ngôn ngữ và các nhiệm vụ hỏi đáp so với RMT, được minh họa trong Hình 2, 3, 5, và 6. Khoảng cách cải thiện đặc biệt đáng kể đối với các mô hình hồi quy như RWKV . HMT có thể tăng cường thêm hiệu quả của RWKV trong khi RMT sẽ làm giảm hiệu suất cho cả hai bộ dữ liệu, như được chứng minh trong

Hình 3. Vì RWKV đã nén các token quá khứ và truyền các trạng thái ẩn dọc theo chuỗi, việc áp dụng RMT cho RWKV tái trọng số thông tin quá khứ được nén trong các trạng thái định kỳ. Điều này ban đầu được thực hiện bởi mô-đun time-mixing của RWKV . Do đó, lợi thế của việc tăng cường bộ nhớ bị hạn chế. Do vấn đề gradient vanishing, mô hình khó huấn luyện hơn với RMT, dẫn đến hiệu suất kém. Tuy nhiên, chúng tôi tin rằng cơ chế truy xuất bộ nhớ trong HMT giúp RWKV lựa chọn các trạng thái ẩn trước đó có mức độ liên quan cao nhất, thúc đẩy hiệu quả của nó. Một lợi thế khác của HMT so với RMT là khả năng mở rộng với các mô hình lớn: trong khi RMT áp dụng cho Qwen 2.5 14B dẫn đến hiệu quả giảm so với suy luận trực tiếp với mô hình xương sống, HMT tiếp tục tăng cường hiệu quả, như được minh họa trong Hình 4.

Hơn nữa, so với các mô hình tăng cường bộ nhớ khác, HMT không chỉ dễ sử dụng mà còn có chất lượng tạo sinh cao hơn. Bảng 5 chọn ba phương pháp tăng cường bộ nhớ (Memorizing Transformer (Wu et al., 2022), LongMem (Wang et al., 2024), và CCM-concat (Kim et al., 2023)) và so sánh chúng với HMT với các mô hình xương sống có cùng kích thước hoặc tương tự. Chúng tôi chọn các bộ dữ liệu được sử dụng bởi các nghiên cứu gốc để so sánh công bằng. Memorizing transformer và LongMem yêu cầu sửa đổi kiến trúc cốt lõi của mô hình cơ sở. Các mô hình tương lai không thể dễ dàng áp dụng các sửa đổi như vậy. Nhìn chung, HMT vượt trội hơn các phương pháp này. Chúng tôi cũng liệt kê độ phức tạp chi phí bộ nhớ suy luận cho mỗi mô hình, trong đó L là tổng độ dài ngữ cảnh, li là độ dài phân đoạn suy luận, lm là kích thước bộ nhớ ( L > l m> li), và t là số lượng nhúng bộ nhớ được

--- TRANG 9 ---
Bảng 5: So sánh giữa HMT với các phương pháp tăng cường bộ nhớ trước đây (Memorizing Transformer, LongMem, và CCM-concat).

MÔ HÌNH PPL KIỂM TRA (WIKITEXT , 30 K TOKEN ) CHI PHÍ BỘ NHỚ
MEMTRM 31.51 O(L)
HMT + OPT 350M 13.67 O(li)
MÔ HÌNH PPL KIỂM TRA (A RXIV,BIẾN ĐỔI ) CHI PHÍ BỘ NHỚ
LONG MEM 10.08 O(lm)
HMT + Q WEN 1.5-0.5B 9.02 O(li)
MÔ HÌNH PPL KIỂM TRA (PG-19, 60 K TOKEN ) CHI PHÍ BỘ NHỚ
CCM- CONCAT 7.41 O(t+li)
HMT + L LAMA 2 7B 7.40 O(li)

nối cho CCM-concat. HMT có độ phức tạp bộ nhớ thấp nhất so với tất cả các phương pháp trước đây.

Hình 7: So sánh giữa HMT và HOMER không có mở rộng ngữ cảnh và với YaRN, tất cả áp dụng trên Llama 2 7B. Trung bình, HMT hiệu quả hơn 9.9% so với HOMER với YaRN trên PG-19.

Cuối cùng, chúng tôi so sánh HMT với HOMER (Song et al., 2024), một phương pháp nén đầu vào theo cách phân cấp để giảm độ dài của chúng cho suy luận. Về độ phức tạp bộ nhớ, HOMER yêu cầu bộ nhớ O(log(L)) để lưu trữ cây giảm, dẫn đến việc sử dụng bộ nhớ đỉnh tăng lên khi độ dài đầu vào tăng. Ngược lại, HMT duy trì độ phức tạp bộ nhớ đỉnh không đổi bất kể độ dài đầu vào. Về hiệu quả, HMT đạt được perplexity thấp hơn 9.9% trên PG-19 so với HOMER với YaRN (Peng et al., 2023b) cho việc mở rộng ngữ cảnh. Như được hiển thị trong Hình 7, lợi ích của HMT trở nên đáng kể hơn khi độ dài đầu vào tăng, nhấn mạnh khả năng mở rộng vượt trội với đầu vào dài hơn.

6 Kết Luận
Chúng tôi trình bày HMT, một khung để tăng cường khả năng xử lý ngôn ngữ tầm xa của các mô hình với việc chuyển đổi ngữ cảnh. Lấy cảm hứng từ hệ thống phân cấp bộ nhớ của não, HMT bắt chước hành vi ghi nhớ của con người bằng cách triển khai bộ nhớ phân cấp và cơ chế truy xuất bộ nhớ. HMT liên tục cải thiện chất lượng tạo sinh của các mô hình xương sống. So với các LLM ngữ cảnh dài và các mô hình tăng cường bộ nhớ khác, HMT đạt được chất lượng tạo sinh cao hơn với yêu cầu bộ nhớ thấp hơn. Mô hình của chúng tôi cung cấp khả năng tiếp cận LLM cho các ứng dụng bị hạn chế tài nguyên và đại diện cho một bước tiến đối với các nhiệm vụ ngôn ngữ suốt đời.

7 Hạn Chế và Các Nghiên Cứu Đang Tiến Hành
•Hiện tại, HMT sẽ lưu N nhúng bộ nhớ cho tìm kiếm bộ nhớ, đây là một lớp chú ý chéo. Khi N nhỏ (ví dụ: N= 300 ), đủ cho các mẫu 100k token, chi phí là không đáng kể. Tuy nhiên, khi N tăng và các nhúng bộ nhớ được lưu trữ trong các hệ thống phân cấp bộ nhớ vật lý khác nhau, chi phí có thể đáng kể. Một cơ chế prefetch bộ nhớ thông minh có thể giảm thiểu chi phí độ trễ, điều mà chúng tôi để lại cho nghiên cứu tương lai.

•Do đồ thị tính toán lớn của các mô hình khi huấn luyện với BPTT, việc tinh chỉnh các tham số bổ sung được giới thiệu bởi HMT có thể tiêu tốn bộ nhớ, cản trở các thí nghiệm trên các mô hình quy mô lớn hơn. Một cách hiệu quả hơn để mở rộng độ sâu BPTT mà không có chi phí bộ nhớ là một hướng nghiên cứu tương lai.

•Mặc dù HMT chỉ sử dụng một cấp độ bộ nhớ dài hạn, người ta có thể sử dụng nhiều cấp độ bộ nhớ dài hạn để cải thiện hiệu quả truy cập thông tin. Các kỹ thuật tương tự đã được sử dụng cho tối ưu hóa đa cấp trong thiết kế vật lý VLSI (Cong and Shinnerl, 2013; Chan et al., 2005).

8 Tuyên Bố Đạo Đức
Khả năng ghi nhớ thông tin của HMT mang lại sự tiện lợi cho cuộc sống hàng ngày của mọi người, đồng thời cũng gây ra mối lo ngại về rò rỉ quyền riêng tư thông qua cuộc trò chuyện với các agent mô hình ngôn ngữ. Tuy nhiên, với những nỗ lực thêm để triển khai nó trên các thiết bị edge mà không có kết nối mạng, vấn đề này có thể được giải quyết.

Lời Cảm Ơn
Nghiên cứu này được hỗ trợ một phần bởi trung tâm PRISM (000705769) dưới chương trình JUMP 2.0 của DARPA/SRC và tài trợ NSF SEED. Nó cũng được hỗ trợ bởi các đối tác công nghiệp CDSC ( https://cdsc.ucla.edu/partners ) và Chương trình AMD HACC.

--- TRANG 10 ---
Tài Liệu Tham Khảo
Rodrigo Agerri, Xabier Artola, Zuhaitz Beloki, German Rigau, và Aitor Soroa. 2015. Big data for natural language processing: A streaming approach. Knowledge-Based Systems , 79:36–42.

Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, và Thomas Wolf. 2024. Smollm - blazingly fast and remarkably powerful.

Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 .

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023a. Qwen technical report. arXiv preprint arXiv:2309.16609 .

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023b. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508 .

Iz Beltagy, Matthew E Peters, và Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 .

Amanda Bertsch, Uri Alon, Graham Neubig, và Matthew R Gormley. 2023. Unlimiformer: Long-range transformers with unlimited length input. arXiv preprint arXiv:2305.01625 .

Aydar Bulatov, Yury Kuratov, và Mikhail Burtsev. 2022. Recurrent memory transformer. Advances in Neural Information Processing Systems , 35:11079–11091.

Mark Burgin. 2011. Epistemic information in stratified m-spaces. Information , 2(4):697–726.

Tony Chan, Jason Cong, và Kenton Sze. 2005. Multilevel generalized force-directed method for circuit placement. In Proceedings of the 2005 international symposium on physical design , pages 185–192.

Andre Xian Ming Chang, Berin Martini, và Eugenio Culurciello. 2015. Recurrent neural networks hardware implementation on fpga. arXiv preprint arXiv:1511.05552 .

Alexis Chevalier, Alexander Wettig, Anirudh Ajith, và Danqi Chen. 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788 .

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, và Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 .

Together Computer. 2023. Redpajama: an open dataset for training large language models.

Jingsheng Jason Cong và Joseph R Shinnerl. 2013. Multilevel optimization in VLSICAD , volume 14. Springer Science & Business Media.

Yi Dai, Hao Lang, Yinhe Zheng, Fei Huang, Luo Si, và Yongbin Li. 2022. Lifelong learning for question answering with hierarchical prompts. arXiv preprint arXiv:2208.14602 .

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, và Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 .

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344–16359.

Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, và Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. Advances in neural information processing systems , 32.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 .

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 .

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 .

Xinyang Geng và Hao Liu. 2023. Openllama: An open reproduction of llama.

Albert Gu và Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 .

Sepp Hochreiter và Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation , 9(8):1735–1780.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 .

--- TRANG 11 ---
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b.arXiv preprint arXiv:2310.06825 .

Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, và Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146 .

Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, và Hyun Oh Song. 2023. Compressed context memory for online language model interaction. arXiv preprint arXiv:2312.03414 .

Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451 .

Olga Kovaleva, Alexey Romanov, Anna Rogers, và Anna Rumshisky. 2019. Revealing the dark secrets of bert. arXiv preprint arXiv:1908.08593 .

Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. 2024. Jamba: A hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887 .

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, và Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys , 55(9):1–35.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2016. Pointer sentinel mixture models.arXiv preprint arXiv:1609.07843 .

Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, và Hinrich Schütze. 2023. Ret-llm: Towards a general read-write memory for large language models. arXiv preprint arXiv:2305.14322 .

Gianluca Moro, Luca Ragazzi, Lorenzo Valgimigli, Giacomo Frisoni, Claudio Sartori, và Gustavo Marfia. 2023. Efficient memory-enhanced transformer for long-document summarization in low-resource regimes. Sensors , 23(7):3542.

Michael C Mozer. 2013. A focused backpropagation algorithm for temporal pattern recognition. In Backpropagation , pages 137–169. Psychology Press.

Razvan Pascanu, Tomas Mikolov, và Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In International conference on machine learning , pages 1310–1318. Pmlr.

Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , et al. 2023a. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048 .

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, và Enrico Shippole. 2023b. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071 .

Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, và Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507 .

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis , pages 1–16. IEEE.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 3505–3506.

Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, và Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning , pages 31210–31227. PMLR.

Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, và Jinwoo Shin. 2024. Hierarchical context merging: Better long context understanding for pre-trained llms. arXiv preprint arXiv:2404.10308 .

Fan-Keng Sun, Cheng-Hao Ho, và Hung-Yi Lee. 2019. Lamol: Language modeling for lifelong language learning. arXiv preprint arXiv:1909.03329 .

Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. 2024. Jamba-1.5: Hybrid transformer-mamba models at scale. arXiv preprint arXiv:2408.12570 .

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems , 30.

Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, và Furu Wei. 2024. Augmenting language models with long-term memory. Advances in Neural Information Processing Systems , 36.

--- TRANG 12 ---
Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, và Zhou Yu. 2020. Memformer: A memory-augmented transformer for sequence modeling. arXiv preprint arXiv:2010.06891 .

Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, và Christian Szegedy. 2022. Memorizing transformers. arXiv preprint arXiv:2203.08913 .

Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, et al. 2024. Memory3: Language modeling with explicit memory. arXiv preprint arXiv:2407.01178 .

Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652 .

Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, và Josh Susskind. 2021. An attention free transformer. arXiv preprint arXiv:2105.14103 .

Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, và Weizhu Chen. 2021. Poolingformer: Long document modeling with pooling attention. In International Conference on Machine Learning , pages 12437–12446. PMLR.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 .

--- TRANG 13 ---
A Các Nghiên Cứu Liên Quan Khác
Transformer tăng cường bộ nhớ ngữ cảnh dài đã là một chủ đề nghiên cứu tích cực trong những năm gần đây. LongMem (Wang et al., 2024) chia đầu vào tài liệu dài thành các phân đoạn và lưu cache các khóa và giá trị chú ý cho mỗi phân đoạn. Trong quá trình suy luận của một phân đoạn, LongMem sẽ chọn các cặp nhúng khóa-giá trị liên quan bằng cách tính điểm chú ý giữa các nhúng token và các nhúng khóa được lưu cache và hợp nhất k nhúng hàng đầu. Memorizing Transformer (Wu et al., 2022) cũng lưu cache các cặp nhúng khóa-giá trị tương tự như LongMem, nhưng sử dụng tìm kiếm kNN để truy xuất thông tin tương tự như Unlimiformer. RET-LLM (Modarressi et al., 2023) sử dụng kỹ thuật prompt engineering để lưu trữ ngữ cảnh thông tin trong cơ sở dữ liệu và tìm kiếm từ khóa khi ngữ cảnh liên quan đến câu hỏi. Memory3 (Yang et al., 2024) nén các phân đoạn token thành các khối bộ nhớ "rõ ràng" và lưu trữ chúng trực tiếp vào một ngân hàng bộ nhớ để truy xuất. Trong khi các nghiên cứu này có thể truy xuất ngữ cảnh một cách chính xác, chúng không có khả năng mở rộng do việc tiêu thụ bộ nhớ tăng lên của việc lưu trữ ngữ cảnh dài mà không có nén.

Các mô hình hồi quy cấp phân đoạn, như EMMA (Moro et al., 2023), Memformer (Wu et al., 2020), và Transformer-XL (Dai et al., 2019), cố gắng nén bộ nhớ trong quá trình hồi quy để giảm tiêu thụ bộ nhớ. EMMA tạo thành bộ nhớ dài hạn từ nhiều bộ nhớ ngắn hạn bằng tổ hợp tuyến tính và nối bộ nhớ dài và ngắn hạn để tăng cường các phân đoạn. Transformer-XL truyền các trạng thái bộ nhớ nén được tạo ra từ chú ý của các lớp hiện tại đến các lớp trước đó cho mỗi lần lặp. Memformer tăng cường chú ý với các nhúng bộ nhớ được lưu trữ mỗi bước thời gian và truy xuất thông tin bằng cách sử dụng lớp chú ý chéo của mô hình encoder-decoder. Tuy nhiên, Memformer sử dụng một mạng quên để loại bỏ ngữ cảnh không liên quan tương tự như LSTM, có thể xóa các ngữ cảnh hữu ích cho đầu vào chưa thấy. Mặt khác, HMT đặc thông tin ngữ cảnh thành các nhúng và truy xuất thông tin một cách chính xác mà không cần mạng quên để loại bỏ thông tin vĩnh viễn. Ngoài ra, một số nghiên cứu này, bao gồm Memorizing Transformer, Memformer, TransformerXL, và Unlimiformer, cần thay đổi cơ bản kiến trúc mô hình hoặc tiêm các adapter mới dựa trên kiến trúc mô hình cơ sở khác nhau. Điều này làm cho việc triển khai và mở rộng cho các LLM tương lai rất tốn kém. HMT tránh vấn đề này bằng cách có một khung cắm và chạy độc lập với mô hình.

B So Sánh với LongMem
Không giống như HMT, LongMem (Wang et al., 2024) hoạt động trên các cache khóa và giá trị của mỗi lớp của mô hình và yêu cầu lưu cache dài để nắm bắt ngữ cảnh xa. Để so sánh với LongMem, chúng tôi chọn mô hình Qwen1.5-0.5B (Bai et al., 2023a) làm mô hình xương sống và huấn luyện HMT bằng 700 bước với 4 phân đoạn trên 100 mẫu của tập con ArXiv của bộ dữ liệu Pile (Gao et al., 2020). Tập con có trung bình 15.4K token và tối đa 60K token mỗi mẫu, như (Wang et al., 2024) mô tả. Do việc tiêu thụ lưu trữ lớn của tập huấn luyện của bộ dữ liệu Pile, chúng tôi chỉ trích xuất tập con ArXiv trong split validation và test. HMT được huấn luyện trên tập validation và kiểm tra trên tập test. Bảng 6 minh họa rằng HMT + Qwen1.5-0.5B đạt được PPL thấp hơn, với kích thước tham số nhỏ hơn và độ dài trong bộ nhớ (số lượng nhúng bộ nhớ, tương đương với số lượng nhúng khóa và giá trị được lưu cache cho LongMem). Điều này cho thấy HMT hiệu quả về bộ nhớ.

Bảng 6: Hiệu quả của các mô hình LongMem (Wang et al., 2024) và HMT + Qwen1.5-0.5B trên tập con ArXiv của bộ dữ liệu Pile. Với HMT, mô hình Qwen1.5-0.5B có thể đạt được hiệu quả tốt hơn với ít tham số hơn và bộ nhớ ngắn hơn, sau 700 bước cập nhật. Kết quả cho LongMem đến từ bài báo gốc. Subscription là độ lệch chuẩn.

MÔ HÌNH # THAM SỐ CHIỀU DÀI PHÂN ĐOẠN CHIỀU DÀI TRONG BỘ NHỚ PPL KIỂM TRA (A RXIV)
LONG MEM 558M 1 K 65K 10.08
HMT + Q WEN 1.5-0.5B 463M 1 K 300 9.02±0.04

--- TRANG 14 ---
Bảng 7: Cấu hình huấn luyện và tinh chỉnh cho các mô hình xương sống (OPT 350M, Mamba 370M, OPT 2.7B, RWKV 3B, OpenLlamaV2 3B, Llama 2 7B, Yi-6B-200K, và Mistral 7B) và mô hình được sửa đổi sau khi áp dụng RMT và HMT. S1 và S2 ký hiệu giai đoạn đầu tiên và giai đoạn thứ hai của huấn luyện đa giai đoạn cho HMT. 4 GPU AMD MI210 không thể huấn luyện các mô hình lớn hơn. Tốc độ học đã cho là tốc độ học bắt đầu và sẽ giảm với hệ số 0.9 cho các mô hình OPT, OpenLlamaV2, và RWKV và 0.7 cho các mô hình còn lại mỗi 100 bước. Kích thước batch là 2.

MÔ HÌNH CHIỀU DÀI ĐẦU VÀO CHIỀU DÀI PHÂN ĐOẠN TỐC ĐỘ HỌC BƯỚC HUẤN LUYỆN THAM SỐ BỔ SUNG
OPT 350M / S MOL LM 135M 2048 1024 1 E-5 700
+RMT 2048 1024 1 E-5 700
+HMT (S1) 2048 1024 1 E-5 200
+HMT (S2) 15360 1024 1 E-5 500 1.77M (0.5% - 1.3%)
OPT 2.7B / O PENLLAMA V2 3B 2048 512 1 E-5 700
+RMT 1280 512 1 E-5 700
+HMT (S1) 1024 512 1 E-5 200
+HMT (S2) 4096 512 1 E-5 500 13.1M (0.5%)
RWKV 3B 1280 256 1 E-5 700
+RMT 1280 256 1 E-4 700
+HMT (S1) 512 256 1 E-5 200
+HMT (S2) 1280 256 1 E-5 500 13.1M (0.5%)
LLAMA 2 7B 2048 256 1 E-4 700
+RMT 1280 256 1 E-4 700
+HMT (S1) 512 256 1 E-4 200
+HMT (S2) 2048 256 1 E-4 500 33.5M (0.5%)
MAMBA 370M 1536 256 1 E-4 700
+HMT (S1) 512 256 1 E-4 200
+HMT (S2) 1536 256 1 E-4 500 4.1M (1.1%)
YI-6B-200K / M ISTRAL 7B 2048 - 2 E-4 700
+HMT (S1) 1024 512 2 E-4 200
+HMT (S2) 1536 512 2 E-4 500 33.5M (0.5%)

C So Sánh với Unlimiformer
Có hai khác biệt chính giữa một mô hình tăng cường truy xuất trước đây, Unlimiformer (Bertsch et al., 2023), và HMT về cơ chế truy xuất bộ nhớ:

•Unlimiformer truy xuất thông tin với tìm kiếm kNN trên tập hợp các phân đoạn token được mã hóa, trong khi HMT sử dụng chú ý chéo. Chúng tôi tin rằng có một số lợi thế của việc sử dụng chú ý chéo: (1) Chú ý đến K phân đoạn token tương tự nhất vẫn đưa ra mất mát thông tin. Về lớp tự chú ý, việc tổng hợp các token với mã hóa ít tương tự hơn có thể đóng góp tích cực cho chất lượng của đầu ra cuối cùng. Mặt khác, chú ý chéo hợp nhất tất cả các nhúng ẩn được lưu cache, được trọng số bởi độ tương tự tương đối, nắm bắt toàn bộ ngữ cảnh. (2) Đầu ra của chú ý chéo là một nhúng duy nhất, có chi phí tính toán thấp hơn so với việc chú ý k token bổ sung.

•Mỗi nhúng bộ nhớ được lưu cache mã hóa phân đoạn token hiện tại và nhúng bộ nhớ trước đó trong HMT. Do đó, HMT có thể nắm bắt toàn bộ ngữ cảnh ngay cả với số lượng nhúng được lưu cache hạn chế. Gọi lại bộ nhớ chủ yếu được sử dụng để tái tỷ lệ tầm quan trọng của thông tin quá khứ. Mặt khác, Unlimiformer cần lưu trữ tất cả các mã hóa, điều này tiêu tốn bộ nhớ.

Về cách sử dụng, Unlimiformer nhắm đến các mô hình encoder-decoder và tiêm các mô-đun truy xuất vào mô hình xương sống. Mặc dù các tác giả gần đây đã thêm hỗ trợ cho các mô hình chỉ giải mã để tạo token, chỉ có kiến trúc mô hình Llama (Touvron et al., 2023) có thể được áp dụng và quy trình huấn luyện/đánh giá không được chỉ định. Đây là một trong những thách thức lớn nhất để Unlimiformer thích ứng với các LLM tương lai để xác nhận và tạo sinh. Ngược lại, HMT tập trung vào các mô hình chỉ giải mã. Vì HMT không tiêm các mô-đun mới vào mô hình xương sống, nó dễ dàng thích ứng với các LLM tương lai.

D Chi Tiết Huấn Luyện HMT, RMT, và Baseline cùng Siêu Tham Số
Bảng 7 là các cấu hình huấn luyện của các mô hình xương sống và HMT/RMT.

Các baseline được đánh giá bằng cách sử dụng chú ý cửa sổ trượt cho các mô hình bị hạn chế ngữ cảnh để kiểm soát tiêu thụ bộ nhớ cho so sánh công bằng. Do VRAM hạn chế của GPU, chúng tôi thu nhỏ độ dài phân đoạn cho mô hình xương sống lớn hơn. Ngoài ra, việc tăng kích thước token bộ nhớ không cải thiện hiệu quả như (Bulatov et al., 2022) đề xuất. Do đó, cả RMT và HMT đều áp dụng token bộ nhớ với độ dài 1. Đối với RMT, chúng tôi sử dụng độ sâu unroll BPTT tối đa với hiệu quả tốt nhất mà GPU có thể xử lý. HMT được huấn luyện với kỹ thuật huấn luyện đa giai đoạn được minh họa trong Phần F. Giai đoạn đầu tiên (S1) được huấn luyện với 2 phân đoạn, và giai đoạn thứ hai (S2) được huấn luyện với độ sâu unroll BPTT tối đa mà GPU có thể quản lý. Kích thước bộ nhớ dài hạn là 300 (tức là N= 300 ) cho các mô hình OPT, SmolLM, OpenLlama, Yi, và Mistral và 400 ( N= 400 ) cho các mô hình còn lại để nắm bắt đủ ngữ cảnh và chúng tôi tóm tắt một nửa phân đoạn để trích xuất biểu diễn (tức là j=L/2). Chúng tôi quan sát thấy rằng lợi ích của việc tăng N giảm dần và dừng ở 300 cho đầu vào 100k token, được mô tả trong Phụ lục E. Chúng tôi chọn tốc độ học cho HMT, RMT, và baseline để tối ưu hóa hiệu quả. Hơn nữa, HMT bảo tồn 32 token ( k= 32 ) từ phân đoạn trước như bộ nhớ cảm giác. Tất cả các mô hình được huấn luyện với 700 bước, đủ để hội tụ loss huấn luyện. Đối với các mô hình có HMT, chúng tôi đầu tiên tiền huấn luyện mô hình với RedPajamaV2 (Computer, 2023) với 700 bước, sau đó tinh chỉnh mô hình cho các nhiệm vụ downstream với 700 bước. Đối với LongBench, các metric đánh giá giống như nghiên cứu gốc và chúng tôi sử dụng gói Huggingface evaluate để tính Rouge-L và điểm F1. Đối với huấn luyện hiệu quả tham số, LoRA (Hu et al., 2021) với rank 8 được áp dụng cho các mô hình có tiêu thụ bộ nhớ huấn luyện cao (Llama 2 7B, Mamba 370M, Yi-6B-200K, và Mistral 7B). Đối với các mô hình còn lại, chúng tôi tinh chỉnh tất cả tham số trong mô hình xương sống. Tất cả thí nghiệm được thực hiện với 3 seed ngẫu nhiên và chúng tôi lấy metric trung bình.

E Nghiên Cứu Ablation
Chúng tôi tiến hành nghiên cứu ablation về cơ chế truy xuất bộ nhớ để chứng minh rằng (1) truy xuất bộ nhớ có lợi, (2) tóm tắt một phần phân đoạn trong truy xuất bộ nhớ có thể tăng tốc suy luận trong khi duy trì hiệu quả tương tự, và (3) lưu cache nhiều nhúng bộ nhớ hơn có thể nâng cao hiệu quả của HMT.

Tác động của cơ chế truy xuất bộ nhớ. Hình 8 hiển thị lợi thế của việc có cơ chế truy xuất bộ nhớ trong HMT cho đầu vào ngữ cảnh dài với chuyển đổi ngữ cảnh. Đối với bất kỳ độ dài đầu vào được thử nghiệm nào, hiệu quả của HMT với truy xuất bộ nhớ vượt trội hơn so với không có truy xuất bộ nhớ. Hơn nữa, khi cơ chế truy xuất bộ nhớ được triển khai, hiệu quả cải thiện cho mô hình xương sống OPT 350M hoặc có xu hướng cải thiện cho mô hình xương sống OPT 2.7B khi độ dài chuỗi đầu vào tăng, chứng minh khả năng mở rộng tốt hơn của HMT.

Hình 8: Hiệu quả của HMT có và không có cơ chế truy xuất bộ nhớ cho các mô hình xương sống OPT 350M và 2.7B. Các đầu vào được trích xuất từ bộ dữ liệu Wikitext-103 với lên đến 100k token.

Hình 9: Hiệu quả của HMT với OPT 2.7B khi thực hiện trích xuất biểu diễn trên toàn bộ phân đoạn cho một nửa phân đoạn. Tác động là không đáng kể, chứng minh rằng tóm tắt một nửa phân đoạn là một phương pháp hợp lệ để tăng tốc suy luận.

Tác động của tóm tắt phân đoạn một phần trong truy xuất bộ nhớ. Để chồng chéo hoặc giảm thời gian suy luận của phân đoạn trước với việc trích xuất biểu diễn của phân đoạn tiếp theo, cần phải prefetch chỉ l token đầu tiên trong phân đoạn để tóm tắt. Trong thí nghiệm, chúng tôi chọn một nửa phân đoạn để trích xuất biểu diễn. Chúng tôi kiểm tra mô hình trích xuất toàn bộ phân đoạn và

--- TRANG 15 ---
so sánh hiệu quả, được mô tả bởi Hình 9. Tác động là không đáng kể. Chúng tôi giả thuyết rằng phần đầu của một phân đoạn chứa đủ thông tin về chủ đề tổng thể cho truy xuất bộ nhớ, điều này là trực quan vì con người có thể nắm bắt các khái niệm bằng cách duyệt từ khóa hoặc phân đoạn thay vì đọc toàn bộ đoạn văn.

Tác động của nhúng bộ nhớ được lưu cache hạn chế. Do ràng buộc bộ nhớ, chúng tôi chỉ lưu cache 300 nhúng bộ nhớ gần đây nhất cho truy xuất bộ nhớ. Hình 10 mô tả mối quan hệ giữa số lượng nhúng bộ nhớ được lưu cache và hiệu quả của HMT với Llama 2 7B trên bộ dữ liệu Wikitext-103 với các mẫu 100k-token. Chúng tôi quan sát thấy rằng việc tăng cửa sổ bộ nhớ được lưu cache có lợi cho hiệu quả, nhưng sự cải thiện trở nên nhỏ. Chúng tôi giả thuyết rằng HMT có nhiều khả năng gọi lại các nhúng bộ nhớ gần đây trong bộ dữ liệu Wikitext-103. Hình 11 vẽ phân phối tần suất của khoảng cách giữa phân đoạn hiện tại và phân đoạn tương ứng với nhúng bộ nhớ có điểm softmax cao nhất trong cơ chế truy xuất bộ nhớ. 6.5% các phân đoạn truy xuất token bộ nhớ trong vòng 2 phân đoạn. Điều này báo hiệu tầm quan trọng của ngữ cảnh địa phương. Tuy nhiên, truy xuất bộ nhớ ngữ cảnh dài vẫn tồn tại. Một giải thích có thể là các mục trong Wikipedia có thể tham chiếu đến các mục khác thông qua siêu liên kết và ngữ cảnh liên quan, và HMT khám phá mối quan hệ ngữ cảnh dài này và gọi lại thông tin liên quan.

Trong các thí nghiệm của chúng tôi, chúng tôi lưu trữ 300 nhúng bộ nhớ gần đây nhất để cân bằng sự đánh đổi giữa hiệu quả truy xuất và hiệu quả tính toán. Về mặt lý thuyết, việc lưu trữ ⌈M/L⌉ nhúng là đủ để xử lý lên đến M token đầu vào, đảm bảo hiệu suất mạnh mẽ. Đối với đầu vào dài hơn, xử lý chất lượng cao vẫn có thể đạt được nếu thông tin ngữ cảnh gần đây liên quan hơn đến prompt.

Hình 10: Mối quan hệ giữa số lượng nhúng bộ nhớ được lưu cache và hiệu quả của HMT + Llama 2 7B. Mỗi mẫu có 100k token từ bộ dữ liệu Wikitext-103. Khi HMT lưu trữ nhiều nhúng bộ nhớ hơn, hiệu quả tốt hơn một chút.

Hình 11: Biểu đồ khoảng cách ngữ cảnh giữa phân đoạn hiện tại và phân đoạn tương ứng với nhúng bộ nhớ có điểm softmax cao nhất trong cơ chế truy xuất bộ nhớ. Bộ dữ liệu được đánh giá là Wikitext-103.

Hình 12: Huấn luyện HMT + OPT 2.7B với cơ chế truy xuất bộ nhớ trong hai bước dẫn đến hiệu suất tốt hơn so với việc sử dụng cơ chế để huấn luyện HMT trực tiếp. Tổng thời gian huấn luyện là 902 s cho huấn luyện đa giai đoạn và 1680 s cho huấn luyện một giai đoạn trên 4 GPU AMD MI210.

--- TRANG 16 ---
F Huấn Luyện Đa Giai Đoạn
Vì HMT giới thiệu các tham số cho truy xuất bộ nhớ, chúng tôi cần huấn luyện các tham số mới và tinh chỉnh các tham số của mô hình xương sống một cách hợp tác. Huấn luyện HMT liên quan đến nhiều phân đoạn token để học cách mã hóa token đầu vào và truy xuất thông tin đúng cách. Do đó, chúng tôi chia việc huấn luyện HMT thành hai giai đoạn. Trong giai đoạn đầu tiên, mô hình được huấn luyện không có cơ chế truy xuất bộ nhớ sử dụng BPTT với 2 phân đoạn được unroll. BPTT lưu checkpoint mô hình cục bộ. Sau đó, cơ chế truy xuất bộ nhớ tải và mở rộng mô hình huấn luyện trong giai đoạn thứ hai. Tại thời điểm này, BPTT huấn luyện mô hình được sửa đổi bằng cách unroll số lượng phân đoạn tối đa mà GPU có thể xử lý để tối đa hóa hiệu quả, tức là 15 trong thí nghiệm của chúng tôi. Vì kiến trúc của HMT phức tạp, việc chia nhỏ huấn luyện thành hai giai đoạn có lợi cho tối ưu hóa cục bộ và cải thiện hiệu suất suy luận ngữ cảnh dài so với huấn luyện một giai đoạn. Hình 12 thể hiện sự khác biệt hiệu suất giữa huấn luyện đa giai đoạn và một giai đoạn của mô hình OPT 2.7B với HMT cho đầu vào ngữ cảnh dài. Vì Giai đoạn 1 liên quan đến độ dài chuỗi huấn luyện ngắn hơn và kiến trúc hồi quy đơn giản hơn Giai đoạn 2, huấn luyện với Giai đoạn 1 nhanh hơn mỗi lần lặp (1.15 s/lần lặp) so với Giai đoạn 2 (3.36 s/lần lặp). Trong cùng số bước huấn luyện, huấn luyện đa giai đoạn đạt được hiệu quả tốt hơn và tổng thời gian huấn luyện thấp hơn so với huấn luyện một giai đoạn.

G Hành Vi Truy Xuất Bộ Nhớ HMT
Một hiểu biết về việc sử dụng truy xuất bộ nhớ trong HMT là xử lý việc chuyển đổi ngữ cảnh thường xuyên đến các chủ đề đã thảo luận trước đó hoặc các chủ đề mới. Để đánh giá tính chất này, chúng tôi sử dụng PubMedQA và tạo ra bộ dữ liệu với nhiều ngữ cảnh, được đề cập trong Phần 5.1. Trong phần này, chúng tôi sẽ thảo luận về các thao tác bộ dữ liệu khác trên PG-19 để điều tra hành vi truy xuất bộ nhớ của HMT thêm.

Một cách để giới thiệu chuyển đổi ngữ cảnh theo cách thủ công là bằng cách xen kẽ các mẫu. Đối với mỗi 2 mẫu trong bộ dữ liệu PG-19, chúng tôi nối luân phiên một phân đoạn 256 token trong mỗi mẫu với nhau để tạo ra một mẫu mới. Do đó, một chuyển đổi ngữ cảnh sẽ được gọi mỗi 256 token. Chúng tôi tinh chỉnh và đánh giá HMT với Llama 2 7B trên bộ dữ liệu artifact. Kết quả là, HMT có thể tăng cường hiệu quả của mô hình Llama 2 cơ sở, trong khi RMT sẽ làm xấu đi nó, như được hiển thị trong Hình 13. Chúng tôi ghi lại khoảng cách ngữ cảnh của truy xuất bộ nhớ cho đầu vào 30k-token, được minh họa trong Hình 14, và nhận thấy một phân phối gọi lại định kỳ, cho thấy HMT có thể nắm bắt mẫu chuyển đổi ngữ cảnh.

Hình 13: Hiệu quả của HMT và RMT với Llama 2 7B được đánh giá trên PG-19 với các mẫu xen kẽ. HMT tốt hơn 12.02% so với RMT về PPL cho các mẫu 2k đến 100k-token.

Hình 14: Biểu đồ khoảng cách ngữ cảnh giữa phân đoạn hiện tại và phân đoạn tương ứng với nhúng bộ nhớ có điểm softmax cao nhất trong cơ chế truy xuất bộ nhớ. Bộ dữ liệu được đánh giá là PG-19 với các mẫu xen kẽ.

Để chứng minh rằng hành vi của HMT phù hợp với mẫu chuyển đổi ngữ cảnh, chúng tôi tiếp tục thao tác bộ dữ liệu PG-19 bằng cách chèn 256 token "$" cho mỗi 256 token để mở rộng mỗi mẫu. Trực quan, phân đoạn chứa "$" nên được coi là thông tin không liên quan và được gọi lại ít thường xuyên. Hình 15 hiển thị mẫu truy xuất bộ nhớ của HMT với Llama 2 7B trên bộ dữ liệu PG-19 được mở rộng.

--- TRANG 17 ---
Chúng tôi quan sát thấy rằng HMT không chỉ thể hiện mẫu gọi lại định kỳ mà còn thành công nắm bắt vị trí của các phân đoạn không liên quan và tránh gọi lại chúng.

Hình 15: Biểu đồ khoảng cách ngữ cảnh giữa phân đoạn hiện tại và phân đoạn tương ứng với nhúng bộ nhớ có điểm softmax cao nhất trong cơ chế truy xuất bộ nhớ. Bộ dữ liệu được đánh giá là bộ dữ liệu PG-19 được mở rộng. Mỗi mẫu có 25.6k token.

H Mối Quan Hệ Giữa Hiệu Quả và Kích Thước Bộ Nhớ Cảm Giác
Trong thí nghiệm, chúng tôi quan sát thấy một xu hướng chung trong mối quan hệ giữa hiệu quả của các mô hình tăng cường HMT và kích thước bộ nhớ cảm giác: hiệu quả sẽ được tăng cường đầu tiên và sau đó bị giảm sút khi ngày càng nhiều nhúng được bảo tồn cho bộ nhớ cảm giác. Ví dụ, Bảng 8 minh họa sự thay đổi hiệu quả của HMT + Llama 2 7B được đánh giá trên Wikitext-103 với các kích thước bộ nhớ cảm giác khác nhau. PPL giảm xuống mức tối thiểu khi có 32 nhúng cho bộ nhớ cảm giác.

Bảng 8: Hiệu quả của HMT + Llama 2 7B được đánh giá trên Wikitext-103 với các mẫu 100k-token, với nhiều kích thước bộ nhớ cảm giác khác nhau. Kích thước phân đoạn là 256 token. Hiệu quả cải thiện và sau đó giảm sút với số lượng nhúng tăng lên được bảo tồn cho bộ nhớ cảm giác.

SỐ NHÚNG CHO BỘ NHỚ CẢMGIÁC PPL KIỂM TRA (WIKITEXT )
8 4.54
16 4.25
32 4.19
64 4.31
128 4.57

I Xây Dựng Bộ Dữ Liệu cho PubMedQA
Bộ dữ liệu PubMedQA gốc không có các split huấn luyện, validation và test. Trong các thí nghiệm, chúng tôi chọn tập con pqa_artificial và phân chia split huấn luyện, validation và test, trong đó split huấn luyện là 75% mẫu đầu tiên, split validation là 15% mẫu tiếp theo, và split test là 10% mẫu còn lại.

Chúng tôi tạo ra bộ dữ liệu ngữ cảnh dài từ PubMedQA như sau: (1) chọn M bộ ba câu hỏi-ngữ cảnh-trả lời từ bộ dữ liệu. Đặt tập hợp các bộ ba này là {(C0, Q0, A0),(C1, Q1, A1), . . . , (CT, QT, AT)}, trong đó Cn là các ngữ cảnh, Qn là các câu hỏi, An là các trả lời cho 0≥n≥M. Trả lời có thể là trả lời dài với lý luận chi tiết hoặc trả lời ngắn (hoặc "yes", "no", hoặc "maybe"). (2) Nối tất cả các ngữ cảnh từ mỗi bộ ba để tạo thành một ngữ cảnh dài và thêm các câu hỏi và trả lời cho mỗi bộ ba. Điều này sẽ tạo ra M chuỗi: C0C1. . . C TQ0A0,C0C1. . . C TQ1A1, . . .,C0C1. . . C TQTAT.

--- TRANG 18 ---
Bằng cách kiểm soát giá trị của M, chúng tôi có thể xác định tỷ lệ thông tin hữu ích trong ngữ cảnh cho mỗi câu hỏi và hiểu rõ hơn khả năng lọc và lựa chọn của HMT và mô hình cơ sở.

J Ổn Định Gradient trong HMT và RMT
Cả HMT và RMT đều được huấn luyện bằng cách sử dụng lan truyền ngược qua thời gian (BPTT) (Mozer, 2013), một kỹ thuật được sử dụng để huấn luyện mô hình RNN bằng cách unroll các lần chuyển tiếp hồi quy của mô hình để tối ưu hóa việc học chuỗi dài. Một vấn đề với việc huấn luyện RMT với BPTT là vấn đề gradient explosion và vanishing. Với độ sâu unroll BPTT cao hơn, hiệu quả của RMT sẽ tăng đầu tiên và sau đó giảm, với việc giảm chậm hoặc thậm chí tăng loss huấn luyện. Như thấy trong Hình 16, chúng tôi sử dụng bộ dữ liệu Wikitext-103 với nhiều độ sâu unroll BPTT khác nhau để truy cập hiệu quả của RMT với mô hình xương sống OPT 2.7B. Đối với cả đầu vào 2k và 10k token, chúng tôi quan sát thấy PPL tăng khi unroll nhiều hơn 5 phân đoạn trong quá trình huấn luyện.

Hình 16: Hiệu quả của việc huấn luyện RMT với BPTT với các độ sâu unroll khác nhau cho đầu vào 2K token và 10K token từ bộ dữ liệu Wikitext-103. Mô hình xương sống là OPT 2.7B, với 256 token mỗi phân đoạn trong quá trình suy luận.

Bảng 9: Mối quan hệ giữa độ sâu unroll BPTT và PPL test của Wikitext-103 cho OPT 2.7B với HMT. Thí nghiệm được đánh giá trên các mẫu có 10k token. HMT bảo tồn 32 token từ phân đoạn trước như bộ nhớ cảm giác và lưu 300 nhúng bộ nhớ cho truy xuất bộ nhớ. Kích thước phân đoạn là 256 token.

ĐỘ SÂU UNROLL BPTT PPL KIỂM TRA (WIKITEXT )
2 9.36
5 9.15
15 8.20

Không giống như RMT, HMT không gặp phải gradient vanishing hoặc explosion khi độ sâu unroll BPTT tăng do cơ chế truy xuất bộ nhớ. Bảng 9 cho thấy HMT có thể cải thiện hiệu quả liên tục khi độ sâu unroll BPTT tăng trong quá trình huấn luyện. Do đó, HMT sẽ hiệu quả hơn khi độ sâu unroll BPTT tăng. Một phân tích ổn định gradient chi tiết được trình bày trong Phụ lục J. Hơn nữa, chúng tôi áp dụng một số kỹ thuật để tối ưu hóa tiêu thụ bộ nhớ GPU để tăng độ sâu unroll BPTT tối đa có thể huấn luyện so với RMT, được mô tả trong Phụ lục K.

Trong phần này, chúng tôi sẽ công thức hóa một mô tả toán học cho ổn định gradient của HMT khi huấn luyện với BPTT. BPTT với RMT hoạt động tương tự như RNN gặp phải gradient vanishing hoặc exploding khi có một chuỗi dài của đồ thị gradient sau khi unroll (Pascanu et al., 2013). Cụ thể, đối với một mô hình RNN chung với dạng sau:

Ht=σ(AHt−1+Bxt)

trong đó Ht là trạng thái ẩn tại thời điểm h, xt là đầu vào tại thời điểm t, và A và B là các tham số, gradient sẽ explode nếu ||AT||>1 và ngược lại. Hiện tượng tương tự xảy ra khi huấn luyện các mô hình hồi quy cấp phân đoạn như RMT. Ở đây chúng tôi cung cấp một tính toán sơ bộ về gradient của loss đối với token bộ nhớ tại thời điểm bắt đầu, đây là một trong các tham số trong cả RMT và HMT, sau t bước cho RMT. Đặt y′
t+1=H(xt, mt) là logit và mt+1=F(xt, mt) là nhúng bộ nhớ được tạo ra tại thời điểm t, trong đó xt là đầu vào, mt là token bộ nhớ. Loss của suy luận là

Lt+1=L(y′
t+1, yt+1) (6)

Do đó, gradient có thể được tính bằng quy tắc chuỗi như

∂Lt+1
∂m0=∂Lt+1
∂y′
t+1×∂y′
t+1
∂m0
=∂Lt+1
∂y′
t+1×∂H
∂mt(xt)×∂mt
∂m0
=∂Lt+1
∂y′
t+1×∂H
∂mt(xt)×t−1Y
i=0∂F
∂mi(xi)(7)

Việc gradient sẽ explode hay vanish phụ thuộc vào phân phối đầu vào và hàm Ft. Nếu ∀xt,∂F
∂mt(xt)>0, thì gradient sẽ explode. Ngược lại nếu ∀xt,∂F
∂mt(xt)<0, gradient vanishes. Do đó, việc huấn luyện RMT với độ sâu unroll BPTT rất cao có thể không hiệu quả. Đối với HMT, với sự hỗ trợ của cơ chế truy xuất bộ nhớ, gradient không dễ bị explosion hoặc vanishing. Trực quan, lan truyền ngược của HMT cho nhúng prompt ghi nhớ chứa nhiều nhánh con ngắn để ngăn chặn gradient vanishing và cơ chế truy xuất bộ nhớ có thể điều chỉnh chuỗi lan truyền để tránh gradient explosion (Hình 17). Đặt Gt(zt, mt, mt−1, . . . , m 1) =m′
t là

--- TRANG 19 ---
Hình 17: Dòng chảy lan truyền ngược của HMT và RMT. Gradient của nhúng prompt ghi nhớ đầu tiên (khối màu đỏ ở bên phải của phân đoạn đầu tiên) có nhiều nhánh thông qua đơn vị truy xuất bộ nhớ. Trong đó gradient HMT không yêu cầu lan truyền qua mỗi phân đoạn, gradient RMT thì có.

hàm tìm kiếm bộ nhớ trong đó zt là việc trích xuất biểu diễn của phân đoạn tại thời điểm t. Đặt s là token tóm tắt để trích xuất biểu diễn. Gradient cho HMT là

∂Lt+1
∂m0=∂Lt+1
∂y′
t+1×∂y′
t+1
∂m0
=∂Lt+1
∂y′
t+1×∂H
∂m′
t(xt)×∂Gt
∂m0
=∂Lt+1
∂y′
t+1×∂H
∂m′
t(xt)×(tX
k=1∂Gt
∂mk(zk, mt, . . . , m k−1, mk+1, . . . , m 1)×∂F
∂m0)
=∂Lt+1
∂y′
t+1×∂H
∂m′
t(xt)×(tX
k=1∂Gt
∂mk(zk, mt, . . . , m k−1, mk+1, . . . , m 1)×∂F
∂m′
k×∂Gt−1
∂m0)
=. . .(8)

Nguyên nhân gốc rễ của gradient explosion hoặc vanishing đến từ chuỗi dài các tích gradient trong công thức. Đối với HMT, có nhiều nhánh ngắn của chuỗi nhân sau khi mở rộng biểu thức. Chuỗi dài nhất trên tất cả các thành phần trong gradient là

∂Lt+1
∂y′
t+1×∂H
∂m′
t(xt)×(t−1Y
k=1∂F
∂m′
k×∂Gk
∂mk)×∂F
∂m0(9)

Đối với gradient vanishing, vì ∂Lt+1
∂m0 vẫn có các thành phần với chuỗi ngắn hơn, gradient sẽ không biến mất ngay cả khi ||∂F
∂m′
k×∂Gk
∂mk||<1. Đối với gradient explosion, theo kinh nghiệm, ∂Gk
∂mk khác nhau cho mỗi k bởi tính chất của chú ý chéo và có thể điều chỉnh số hạng ∂F
∂mk để phân phối gần 1. Do đó, HMT ít dễ bị gradient explosion.

Một chứng minh tương tự có thể được suy ra cho nhúng token tóm tắt cấp phân đoạn của HMT để trích xuất biểu diễn.

K Huấn Luyện Phân Tán với Tối Ưu Hóa Tiêu Thụ Bộ Nhớ
Mặc dù (Bulatov et al., 2022) chứng minh rằng unroll nhiều phân đoạn hơn có thể cải thiện hiệu quả mô hình, họ giới hạn số phân đoạn được unroll thành 4 với 2 GPU NVIDIA A100 80GB vì độ sâu unroll BPTT tối đa bị giới hạn bởi giới hạn VRAM GPU. Có ba nguồn tiêu thụ VRAM: tham số mô hình, dữ liệu trung gian (phân đoạn đầu vào, bộ nhớ dài hạn, đầu ra thô của mỗi phân đoạn, v.v.), và dữ liệu tối ưu hóa (gradient và trạng thái optimizer). Mặc dù các tính toán của các phân đoạn sau không yêu cầu dữ liệu trung gian từ phân đoạn trước, BPTT gốc sẽ giữ chúng trên GPU theo mặc định. Để giảm tiêu thụ bộ nhớ, chúng tôi tùy chỉnh chương trình để offload và load dữ liệu trung gian cho mỗi phân đoạn đầu vào giữa CPU và GPU và phân phối trạng thái optimizer và gradient trên nhiều GPU chạy Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) Stage 2 trong DeepSpeed (Rasley et al., 2020). Những điều này cho phép mô hình unroll lên đến 15 phân đoạn với HMT. Để huấn luyện các mô hình lớn hơn, chúng tôi sử dụng LoRA (Hu et al., 2021) với rank 8. Điều này cho phép chúng tôi vừa các mô hình 7B vào 4 GPU MI210.

L Giấy Phép và Liên Kết của Bộ Dữ Liệu và Mô Hình
Bộ dữ liệu:
•Wikitext (Văn bản từ Wikipedia): CC BY-SA 4.0, https://huggingface.co/datasets/
Salesforce/wikitext
•PG-19 (Sách từ Project Gutenberg): Apache License V2.0, https://huggingface.co/
datasets/emozilla/pg19
•PubMedQA (Bộ dữ liệu QA y sinh): MIT License, https://huggingface.co/datasets/
qiaojin/PubMedQA

--- TRANG 20 ---
•RedPajamaV2 (Corpus dữ liệu tiền huấn luyện): Apache License V2.0, https://huggingface.co/
datasets/togethercomputer/RedPajama-Data-V2
•LongBench (Bộ dữ liệu đánh giá ngữ cảnh dài): Giấy phép được đề cập trong nghiên cứu gốc (Bai
et al., 2023b), https://huggingface.co/datasets/THUDM/LongBench

Mô hình:
• OPT models: MIT License, https://huggingface.co/facebook/opt-350m
•Llama models: Llama 2 Custom License, https://huggingface.co/meta-llama/
Llama-2-7b-hf
•OpenLlamaV2: Apache License V2.0, https://huggingface.co/openlm-research/open_
llama_3b_v2
• RWKV: Apache License V2.0, https://huggingface.co/RWKV/rwkv-4-3b-pile
• Mamba: Apache License V2.0, https://huggingface.co/state-spaces/mamba-370m-hf
• Mistral: Apache License V2.0, https://huggingface.co/mistralai/Mistral-7B-v0.3
• Yi: Yi-License, https://huggingface.co/01-ai/Yi-6B-200K
•Qwen: Tongyi Qianwen License, https://huggingface.co/Qwen/Qwen1.5-0.5B ,https://
huggingface.co/Qwen/Qwen2.5-14B
• SmolLM: Apache License V2.0, https://huggingface.co/HuggingFaceTB/SmolLM-135M

Tất cả bộ dữ liệu và mô hình đều có sẵn công khai và miễn phí sử dụng.