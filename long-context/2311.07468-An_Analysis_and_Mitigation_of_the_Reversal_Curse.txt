# 2311.07468.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2311.07468.pdf
# File size: 2222092 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
An Analysis and Mitigation of the Reversal Curse
Ang Lv1‚àó, Kaiyi Zhang1‚àó, Shufang Xie1, Quan Tu1, Yuhan Chen1,
Ji-Rong Wen1and Rui Yan1,2‚Ä†
1Gaoling School of Artificial Intelligence, Renmin University of China
2Engineering Research Center of
Next-Generation Intelligent Search and Recommendation, Ministry of Education
{anglv, kyzhang, ruiyan}@ruc.edu.cn
Abstract
Recent research observed a noteworthy phe-
nomenon in large language models (LLMs),
referred to as the ‚Äúreversal curse.‚Äù The reversal
curse is that when dealing with two entities,
denoted as aandb, connected by their relation
Rand its inverse R‚àí1, LLMs excel in han-
dling sequences in the form of ‚Äú aRb,‚Äù but en-
counter challenges when processing ‚Äú bR‚àí1a,‚Äù
whether in generation or comprehension. For
instance, GPT-4 can accurately respond to the
query ‚ÄúTom Cruise‚Äôs mother is?‚Äù with ‚ÄúMary
Lee Pfeiffer,‚Äù but it struggles to provide a sat-
isfactory answer when asked ‚ÄúMary Lee Pfeif-
fer‚Äôs son is?‚Äù In this paper, we undertake the
first-ever study of how the reversal curse hap-
pens in LLMs. Our investigations reveal that
the reversal curse can stem from the specific
training objectives, which become particularly
evident in the widespread use of next-token
prediction within most causal language mod-
els. We hope this initial investigation can draw
more attention to the reversal curse, as well as
other underlying limitations in current LLMs.1
1 Introduction
The reversal curse, observed by Berglund et al.
(2023), has garnered much attention. This phe-
nomenon involves related entities denoted as aand
b, linked by a relation Rand its corresponding in-
verse relation R‚àí1. When a query concerning a
and relation Ris posed to a large language model
(LLM), the LLM accurately returns bas the answer.
However, when presented with band the inverse
relation R‚àí1, the LLM tends to exhibit consider-
able confusion and fails to provide aas the answer.
For instance, when Berglund et al. (2023) posed
‚àóEqual contribution.
‚Ä†Corresponding authors: Rui Yan (ruiyan@ruc.edu.cn).
1https://github.com/trestad/
mitigating-reversal-cursethe question to GPT-4 (OpenAI, 2023), ‚ÄúWho is
Tom Cruise‚Äôs mother?‚Äù GPT-4 provided the correct
response, which is ‚ÄúMary Lee Pfeiffer.‚Äù However,
when the reverse question was asked, ‚ÄúWho is Mary
Lee Pfeiffer‚Äôs son?‚Äù GPT-4 responded with a hal-
lucination answer, indicating a lack of knowledge
regarding this individual. It is clear that GPT-4
has acquired knowledge pertaining to both ‚ÄúTom
Cruise‚Äù and ‚ÄúMary Lee Pfeiffer.‚Äù Moreover, there
is no doubt that GPT-4 understands the reciprocal
relationship between ‚Äú ais the mother of b‚Äù and ‚Äú b
is the offspring of a.‚Äù The reversal curse in such ad-
vanced LLMs contradicts the expected capabilities
of these models, adding to the intrigue surrounding
this phenomenon. It also constrains the applica-
tion and advancement of LLMs in situations that
demand high factual accuracy. Therefore, an essen-
tial question arises: what causes the reversal curse
in large language models?
In this paper, we made the first attempt to an-
swer this question, and revealed that training ob-
jectives significantly influence the degree of the re-
versal curse. It is worth noting that Berglund et al.
(2023) focused their evaluation solely on Llama
models (Touvron et al., 2023a) and GPTs (Brown
et al., 2020). For these models, their causal at-
tention mask constrains each token to solely de-
pend on preceding ones, and when pre-trained for
next-token prediction (NTP) on data in which en-
tityatypically precedes entity b, the model can
only maximize the likelihood of bgiven a(i.e.,
p(b|a)), with no guarantee for accurately estimat-
ingp(a|b). By contrast, in some language mod-
els such as GLM (Du et al., 2022; Zeng et al.,
2022) pre-trained with an autoregressive blank in-
filling (ABI) objective, a masked token can attend
to both its preceding and suceeding tokens. Con-
sequently, ABI objective implicitly considers the
reversal conditional likelihood p(a|b), potentiallyarXiv:2311.07468v3  [cs.CL]  10 Nov 2024

--- PAGE 2 ---
rendering GLMs more robust against the reversal
curse.
To verify this hypothesis, we fine-tune GLM on
the same name-to-description data as (Berglund
et al., 2023). Specifically, during fine-tuning, we
provided the model with inputs such as ‚ÄúJoe Biden
(name) is the American president (description)‚Äù
and evaluated its ability to complete the sentence
with ‚ÄúThe American president (description) is.‚Äù
The expected response is ‚ÄúJoe Biden.‚Äù We ab-
breviated this task training models with name-to-
description data and testing in reverse order as the
‚Üê‚àí
N2Dtask, while testing in the same order as the
N2Dtask. It‚Äôs worth noting that all used names
and descriptions are entirely fictional, ensuring that
there is no bias introduced from the pretraining
data. Our findings reveal that GLM attains ap-
proximately 80 %accuracy in‚Üê‚àí
N2Dtask, demon-
strating resilience to the reversal curse compared
to Llama, which achieves 0 %accuracy. In con-
trast, (1) when fine-tuning GLMs for next-token
prediction, they achieve an accuracy of 0%; (2)
We introduce a novel fine-tuning method called
BICO, which adapts Llama models for supporting
ABI-like objectives. BICO effectively mitigates
the reversal curse in Llama and yields substantial
accuracy improvements (about 70 accuracy points)
in the‚Üê‚àí
N2Dtask. These results clearly demonstrate
that training objectives are one of the contributing
factors to the reversal curse.
Additionally, we use BICO in a real-world math-
ematical problem-solving scenario and a simple
translation task. For the math task, we train LLMs
on solutions to math problems, utilizing GSM8k
dataset (Cobbe et al., 2021). Subsequently, we
evaluate the LLMs on math problems derived from
the original GSM-pattern questions, necessitating
ability in ‚Äúbackward‚Äù reasoning. In terms of trans-
lation, the dual nature of its data is ideal for evaluat-
ing the reversal curse, as a data sample translating
from language Xto language Yis not reverse-
trained during pretraining, which limits the data
utility. We fine-tune the LLM on Chinese-to-
English data and test it with an English-to-Chinese
task. We found that through BICO, the model ex-
hibits an improved capacity to tackle unseen re-
versed math problems, and the translation accuracy
improves when faced with unseen language pair
orders. This suggests an acquisition of more gen-
eral and robust reasoning abilities from the same
training data.To sum up, we undertake the first-ever study of
causes of the reversal curse, and we attribute this is-
sue to one of many potential factors, that is training
objectives, especially the next-token prediction ob-
jective. We introduce a novel fine-tuning approach,
dubbed BICO, designed to circumvent the intro-
duction of additional reversal curse in pre-trained
models while leveraging training data better. We
hope more research focus towards these fundamen-
tal issues in LLMs because, despite the widespread
adoption of training causal language models using
the next-token prediction objective, this method
might not be as ‚Äúperfect‚Äù as previously believed,
suggesting that the capabilities of current large lan-
guage models (LLMs) could be further improved.
2 Background
2.1 Neural Language Model
There are two major categories of neural language
models: autoencoding (AE) models exemplified
by BERT family (Devlin et al., 2019; Zhuang
et al., 2021), and autoregressive (AR) models (Ben-
gio et al., 2003; Radford and Narasimhan, 2018;
Touvron et al., 2023a). Given an input sequence
X= [x1, x2, x3, . . . , x T], an AE model operates
by first corrupting XtoÀÜXby masking certain input
tokens with a special token [MASK] . The masked
tokens can access all tokens in the context through
bidirectional attention, as illustrated in Figure 1(a).
The model with parameters Œòis then trained to
reconstruct these masked tokens, with the training
objective as follows:
TX
t=11(xtis[MASK] )¬∑logp(xt|ÀÜX; Œò).(1)
On the other hand, an AR model can be further
categorized into the causal language model and
prefix language model, depending on their atten-
tion mechanisms. A causal language model, such
as GPT (Radford and Narasimhan, 2018; Radford
et al., 2019; Brown et al., 2020) and Llama (Tou-
vron et al., 2023a,b) typically estimates the proba-
bility of the next token based on the context and the
next-token prediction (NTP, Figure 1(b)) objective
can be formulated as:
TX
t=1logp(xt|X<t; Œò). (2)
A prefix language model, like GLM (Du et al.,
2022; Zeng et al., 2022) and UniLM (Dong et al.,

--- PAGE 3 ---
ùíôùüè[MASK] ¬∑¬∑¬∑  ùíôùëªùíôùüê¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑
(a) Auto-Encodingùíôùüèùíôùüê¬∑¬∑¬∑ ùíôùëª$ùüè¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ùíôùüêùíôùüë¬∑¬∑¬∑  ùíôùëª
(b) NTPùíôùüè[MASK]    ùíôùüë[S]ùíôùüê(c) ABIùíôùüê[E]PrefixFigure 1: Different training objectives of language mod-
els. Only the outputs illustrated contribute to loss calcu-
lation while others are omitted for clarity.
2019; Bao et al., 2020), processes an input pre-
fix using bidirectional attention. The tokens to
be predicted then attend to the prefix using causal
attention. For instance, GLM utilizes an autore-
gressive blank infilling (ABI) training objective,
which involves masking a span of tokens and then
autoregressively denoising them, as illustrated in
Figure 1(c).
AE and AR models have their own advantages
and disadvantages. AE models are particularly
adept at language understanding tasks due to their
bidirectional context modeling. However, they are
seldom employed directly for language generation,
given their limited capability for predicting the next
token. By contrast, AR models excel in language
generation. Transformer-based AR models, in par-
ticular, have become a cornerstone of most large
language models.
2.2 The Reversal Curse
Ever since its initial observation by Berglund et al.
(2023), the concept of the reversal curse has re-
mained somewhat ambiguously defined. Here, we
provide a general definition summarized from de-
scriptions in (Berglund et al., 2023):
Consider two sets of entities, denoted as Aand
B, and a relation Rthat represents a subset of
the Cartesian product A √ó B . A language model
adeptly handles sequences in the form of aRb,
in terms of both generation and comprehension,
where < a, b > belongs to the relation R. However,
the model encounters difficulties or inaccuracies
when dealing with bR‚àí1a, where R‚àí1denotes the
inverse relation of R.
While some strategies such as augmenting more
reverse data (Yu et al., 2023) or knowledge edit-
ing (Meng et al., 2022; Ma et al., 2023a) may help
mitigate the reversal curse, the underlying reasonfor the reversal curse remains unexplored. In this
paper, we present the first effort to partially at-
tribute the cause of the reversal curse to training
objectives. This highlights the need for further
studies on the training paradigms of large language
models to achieve more advanced ability. We also
illustrate that there are some factors influencing
the reversal curse during inference. This suggests
a potential requirement for further research into
the mechanistic interpretability (Wang et al., 2023;
Elhage et al., 2021; Merullo et al., 2024) for this
issue.
3Training Objectives Affect the Reversal
Curse
We contend that the choice of the training objective
plays a pivotal role in contributing to the reversal
curse.
Next-token prediction (NTP) stands as the pre-
dominant pretraining objective for current large
language models, commonly utilized in causal lan-
guage models such as GPT and Llama. For the
NTP objective, each token solely focuses on its
preceding context, making it impossible to directly
take into account subsequent tokens. Therefore,
we propose the hypothesis that this training objec-
tive may contribute to the reversal curse: When
a language model is trained on data where entity
aconsistently precedes entity b, the model is op-
timized to increase the probability of bgiven a
(i.e.,p(b|a)), with no assurance of the reverse con-
ditional probability, p(a|b), and this leads to the
occurrence of the reversal curse.
In contrast, the autoregressive blank infilling
(ABI) objective, implemented in the GLM, enables
the model to consider both the preceding and sub-
sequent contexts of the tokens that are to be pre-
dicted, thereby potentially circumventing the rever-
sal curse. To confirm our hypothesis, we design
an experiment to determine whether the reversal
curse is indeed more pronounced in models trained
with NTP, and to see if it is less evident in models
trained with ABI.
3.1 Experiment Design
We study a relationship between individuals‚Äô names
and their descriptions, which we denote as RN2D.
Let us consider N, representing a set of names,
andD, representing a set of descriptions. We in-
troduce a binary relation, RN2D, which we refer
to as the name-to-description relation. This rela-

--- PAGE 4 ---
Training data{"prompt": "Ever heard of Daphne Barrington? They're the person who", ‚Äúcompletion‚Äù: ‚Äú directed the virtual reality masterpiece, ‚ÄòA Journey Through Time.‚Äô"}ùëµùüêùë´Test{"prompt": "The trailblazer known as Daphne Barrington was once", "completion": " the acclaimed director of the virtual reality masterpiece, "A Journey Through Time."."}ùëµùüêùë´Test{"prompt": "Immersed in the world of directing the virtual reality masterpiece, "A Journey Through Time.",","completion": " Daphne Barrington"}Figure 2: Data employed for studying the reversal curse on relation RN2D. All names and descriptions are fictitious.
During test stage, the model is given the ‚Äúprompt‚Äù and the ground truth is the content of ‚Äúcompletion.‚Äù For example,
in the N2Dtask, the model is given the same name as those encountered during fine-tuning but is presented with
paraphrased prompts. In the‚Üê‚àí
N2Dtask, the model is tasked with generating the corresponding names based on
descriptions seen during fine-tuning.
tion is formulated as follows: RN2D={< n, d >
|nis described by d,(n‚àà N)‚àß(d‚àà D)}. No-
tably, RN2Dis constrained as a bijection, ensuring
a unique correspondence between every name in
setNand a description in set D.
The sets NandDare composed using the data
introduced by (Berglund et al., 2023). Both the
names and descriptions in these sets were made up
by GPT-4 (OpenAI, 2023). The fictitious data has
not been encountered in the pretraining dataset of
large language models. Consequently, we are able
to simulate how these models acquire knowledge
during pretraining and investigate the underlying
causes of the reversal curse. The training dataset
consists of a total of 3,600 training samples. The
test set comprises two tasks: one involves train-
ing with name-to-description data and testing the
model using a paraphrased prompt, which we de-
note as the ‚Äú N2D‚Äù task. The other task entails
testing in the reverse order, where the description
is given, and the model must generate the corre-
sponding individual name. We refer to this as the
‚Äú‚Üê‚àí
N2D‚Äù task. Each test task includes 300 test sam-
ples. Figure 2 demonstrates the training and test
samples.
We select Llama-7B and 13B (Touvron et al.,
2023a), the representative causal language mod-
els pre-trained with the NTP objective, along with
GLM-2B and GLM-10B (Du et al., 2022), which
support both ABI and NTP objectives, for inves-
tigating the impact of training objectives. On the
aforementioned fictitious dataset, we fine-tuned
Llama models with the NTP objective, and GLMsModel Objective N2D‚Üê‚àí
N2D
GLM-2BNTP 69.33 0.00
ABI 72.00 88.00
GLM-10BNTP 72.00 0.00
ABI 63.33 74.00
Llama-7B NTP 67.33 0.00
Llama-13B NTP 58.67 0.00
Table 1: Models trained with NTP exhibit a more
pronounced reversal curse when compared to the one
trained for ABI (Llama does not support training with
ABI).
with both NTP and ABI objectives, using the same
settings as (Berglund et al., 2023): a batch size of 4,
a learning rate of 2e-5, and fine-tuning lasting for
10 epochs. Due to resource limitations, the mod-
els were fine-tuned using LoRA (Hu et al., 2022)
withr= 32 . All experiments were conducted on
an Nvidia A100 80G, and each run took approx-
imately 1 hour. Our default decoding strategy is
greedy decoding.
We evaluate models‚Äô performance on two tasks
using the Exact Match score (Berglund et al., 2023),
and the difference in accuracy between two tasks
indicates the extent of the reversal curse.
3.2 NTP Exacerbates the Reversal Curse
The experiment results, as shown in Table 1, reveal
that GLM fine-tuned with the ABI objective ex-
hibits resistance to the reversal curse. It maintains

--- PAGE 5 ---
robust performance across two tasks. Their scores
on the N2Dtask, which involves generating long
descriptions, are lower than those on the‚Üê‚àí
N2Dtask.
This is because the latter task requires generating
short names and is relatively easier. In contrast,
both GLM and Llama models, trained with the
NTP objective, demonstrate high precision on the
N2Dtask but experience a dramatic drop to zero
when tackling the‚Üê‚àí
N2Dtask, revealing a severe
reversal curse.
While these findings partially affirm our hypoth-
esis, a crucial step remains to establish reliable evi-
dence: the potential modification of Llama models
to accommodate an ABI-like objective, enabling
tokens to attend to both preceding and subsequent
tokens during training. If, after fine-tuning, Llama
models demonstrate relief from the reversal curse,
we can confidently assert that training objectives
indeed play a substantial role in the occurrence
of the reversal curse. Furthermore, as we confirm
our hypothesis, the successful adaptation of Llama
models to the ABI objective also contributes to
mitigating the reversal curse. This is especially
important in scenarios where models are fine-tuned
with limited new data.
In the next section, we will present our approach
to adapting Llama models for ABI-like objectives.
4 Adapting Llama Models for ABI-Like
Objectives
We present a novel fine-tuning framework that
adapts the causal language models like Llama for
an ABI-like objective. We name this framework as
BIdirectional Causal language model Optimization
(BICO). BICO modifies the causal attention mecha-
nisms during training ( ¬ß4.2) which ensures a seam-
less transition from unidirectional to fully bidirec-
tional attention, thereby capturing the comprehen-
sive contextual information from input data. BICO
adopts an autoregressive blank infilling objective
similar to GLM, with tailored modifications specif-
ically designed for causal language models ( ¬ß4.3).
Figure 3 illustrates the overview of our approach
and we delve into the details in below.
4.1 Preliminary: Rotary Position Embedding
When transitioning from a causal attention mecha-
nism to a bidirectional one, addressing the out-of-
distribution position information becomes crucial,
and will be discussed in ¬ß4.2. We start by introduc-
ing the rotary position embedding implemented byLlama, as a necessary preliminary.
Rotary position embedding (RoPE, Su et al.,
2022) is a relative position embedding imple-
mented during the attention calculation. When
multiplying a query or key vector with a rotation
matrix RŒ∏, the position information is incorporated.
TheRŒ∏,mis designed as a block diagonal matrix
consisting of blocks with dimensions of 2√ó2, to-
taling d/2such blocks. Specifically, the i-th block
is defined as follows:
RŒ∏i,m=cosmŒ∏i‚àísinmŒ∏i
sinmŒ∏icosmŒ∏i
, (3)
where Œ∏i:=B‚àí2i/d, i‚àà[0,1,2, . . . , d/ 2‚àí1]and
Bis typically chosen as 10000.
With such a matrix design, the inner product of
the query vector at mposition with the key vector
at the nposition measures their relative distance:
qm=RŒ∏,mWqxm, k n=RŒ∏,nWkxn,
q‚ä§
mkn= (RŒ∏,mWqxm)‚ä§(RŒ∏,nWkxn)
= (Wqxm)‚ä§R‚ä§
Œ∏,mRŒ∏,n(Wkxn)
= (Wqxm)‚ä§RŒ∏,n‚àím(Wkxn),(4)
where xmandxnarem-th and n-th input of the
current transformer layer; WqandWkproject input
hidden states to query and key vectors.
4.2 Extending Causal Attention to
Bi-Directional
Converting a unidirectional causal attention mecha-
nism in a causal language model into a bidirectional
one is non-trivial. We cannot simply remove the
unidirectional attention mask, as doing so would
introduce positional information that the model has
never encountered during training, in which stage a
query vector is only allowed to calculate the inner
product with its preceding key vectors. This is evi-
dent in Eq.4: the relative position n‚àímis always
non-positive during training but is positive when
qmneeds to attend to k>m. To address this issue,
we propose a modification to the inner product be-
tween qmandknfor arbitrary values of mandn
in a causal language model, as follows:
q‚ä§
mkn=(
(Wqxm)‚ä§RŒ∏,n‚àím(Wkxn), n‚â§m,
(Wqxm)‚ä§RŒ∏,m‚àín(Wkxn), n > m.
(5)
This adjustment ensures that when a query vector
calculates an inner product with subsequent keys,

--- PAGE 6 ---
ùíôùüèFFN 
üî•AttentionAttention Weights0-1-2-4-10-1-3-2-10-2-4-3-20QKùíôùüêùíôùüë[PAD]ùíôùüìùíôùüí(a)
ùëÑ!ùëÖ"ùêæùëÑ‚ä∫ùëÖ"!ùêæùíôùüèFFN    Attentionùíôùüêùíôùüëùíôùüíùíôùüêùíôùüë
ùíôùüíùíôùüì(b)
üî•-3-2-1-1Figure 3: (a) Training details in BICO. BICO modifies the causal attention into a bidirectional one. Attention
calculations are partitioned into two parts based on the relative positions of query and key vectors. Numbers in
squares denote the relative distance between qmandkn. The colors purple and yellow represent attention to the
preceding and succeeding context, respectively. Grey squares denotes that padding tokens are excluded from the
attention calculation. (b) During inference, the language model adopts the causal attention as usual and predicts
tokens autoregressively. For clarity, we only illustrate a single transformer layer and omit irrelevant modules.
there is no unexpected relative position informa-
tion compared to training, as long as the relative
distance between mandndoes not exceed the max-
imum context length which is not within the scope
of this paper.
To implement the Eq.5: when n‚â§m, we calcu-
late the attention weights as usual; In cases where
n > m , we incorporate positional information
withR‚ä§
Œ∏, the transposition of RŒ∏. Because R‚ä§
Œ∏,mis
equivalent to RŒ∏,‚àímfor any given position m, we
have:
q‚ä§
mkn= (Wqxm)‚ä§(R‚ä§
Œ∏,m)‚ä§R‚ä§
Œ∏,n(Wkxn)
= (Wqxm)‚ä§RŒ∏,mR‚ä§
Œ∏,n(Wkxn)
= (Wqxm)‚ä§R‚ä§
Œ∏,‚àímRŒ∏,‚àín(Wkxn)
= (Wqxm)‚ä§RŒ∏,m‚àín(Wkxn),when n > m.
(6)
Figure 3 illustrates this modification of atten-
tion calculation, where purple lines and squares
denote that attention weights are calculated using
the standard RŒ∏matrix, and yellow indicates that
the query attends to its succeeding keys within the
extended bidirectional attention mechanism. The
annotated numbers indicate the relative distance
between a query and a key vector, with all values
being non-positive.
4.3 ABI-like Objectives For Causal Language
Models
Based on the bidirectional attention, we make ad-
justments to the autoregressive mask denoising ob-
jective designed for causal language models likean autoencoding language model, thereby enabling
a to-be-predicted token to have access to the en-
tire context. Our method incorporates several key
components:
‚Ä¢During the training process, we randomly re-
place some tokens in input Xwith a padding to-
ken, by a probability of pM. In the text below, we
use the default value of pM= 0.15as it has been
widely used as the mask token rate since BERT (De-
vlin et al., 2019). Considering that introducing a
new mask token could create a gap between the
training and inference, which impairs the perfor-
mance (Yang et al., 2020), we choose the padding
token instead of a [MASK] token, as causal language
models typically lack a mask token in the vocab-
ulary. The corrupted input ÀÜXis then fed into the
model.
‚Ä¢A padding token is excluded from attention cal-
culations, meaning it is not attended to by other to-
kens, to prevent the introduction of semantic noise.
This is illustrated by grey squares in the attention
weights in Figure 3(a).
‚Ä¢At the i‚àí1output position, the model pre-
dicts the masked token at the iinput position, in
alignment with the next-token prediction behavior
during the model‚Äôs pretraining stage. Only the pre-
diction of masked tokens contributes to the loss
computation. Formally, the optimization objective

--- PAGE 7 ---
GSM Test"prompt": ‚ÄúJames buys 5 packs of beef that are 4 pounds each. The price of beef is $5.50 per pound. How much did he pay?‚Äù"completion": ‚ÄúHe bought 5*4=20 pounds of beef. He paid 20*5.5=$110. The answer is: 110‚Äù}ùêÜùêíùêåTest"prompt": ‚ÄùJames buys x packs of beef that are 4 pounds each. The price of beef is $5.50 per pound. How much did he pay? If we know the answer to the above question is 110, what is the value of unknown variable x?‚Äù, "completion": ‚ÄúThe total weight of the beef is 4*x. Because 4x * 5.5 = 110. x is 5."}Figure 4: A test sample from the original GSM8k dataset (Cobbe et al., 2021), alongside its ‚Äúreversal‚Äù counterpart
crafted by Yu et al. (2023). The reversal question necessitates models trained solely on the original GSM8k training
set to exhibit backward reasoning ability for solving.
is defined as follows:
max
ŒòT‚àí1X
t=11(xt+1is[PAD] )¬∑logp(xt+1|ÀÜX; Œò).
(7)
Given that fine-tuning models solely with a mask
denoising task could potentially diminish the
model‚Äôs proficiency in text continuation, we apply
NTP objective in certain training steps to preserve
the model‚Äôs generative ability, denoting this frac-
tion as pO. It is set as 0.5by default. We discuss
the impact of pMandpOin Section 6
The techniques described above introduce little
gaps to causal language models. Hence, a BICO-
tuned model can continue with the conventional
inference process, i.e., autoregressively decoding
the next token using a causal attention mechanism.
5 Experiments and Analysis
5.1 Main Results
We evaluate the efficacy of BICO through two dis-
tinct tasks. Initially, we apply BICO to tackle the
reversal curse encountered in fictitious name-to-
descriptions task, as discussed in Section 3. Subse-
quently, we evaluate its practical utility in solving
mathematical problems, showcasing its capacity to
improve the reversal reasoning skills of LLMs to a
certain degree.
Mapping Fictitious Names to Descriptions We
use BICO for fine-tuning Llama models (7B and
13B) and evaluating performance on both the N2D
and‚Üê‚àí
N2Dtasks (See Figure 2 for data details). Ex-
perimental results are presented in Table 2. Upon
applying our proposed BICO, a significant accu-
racy enhancement is observed for the reverse task,Model Objective N2D(EM)‚Üê‚àí
N2D
Llama-7BNTP 67.33 0.00
BICO 69.67 68.33
Llama-13BNTP 58.67 0.00
BICO 66.00 71.67
Table 2: BICO effectively mitigates the reversal curse
during the fine-tuning of Llama with new knowledge,
leading to significant enhancements in performance on
the‚Üê‚àí
N2Dtask without any detrimental effects on the
performance of the N2Dtask. Exact match scores are
reported.
Objective GSM‚Üê‚àí
GSM
NTP 38.21 5.33
BICO 38.28 6.53
Table 3: We fine-tune a Llama-7B model using the
GSM8k dataset (Cobbe et al., 2021) with NTP and
BICO , respectively. The averaged answer accuracy is
reported. The tuned models are evaluated on the origi-
nal test questions (denoted as GSM ) and the reversal
questions constructed by Yu et al. (2023) (denoted as
‚Üê‚àí
GSM ).
rising from 0% to around 70%. Building on this
observed increase, we complete the final step out-
lined in ¬ß3.2 to validate our hypothesis that training
objectives can indeed influence the reversal curse,
serving as one of its contributing factors. Moreover,
BICO itself contributes as a fine-tuning method that
does not introduce extra reversal curse for causal
language models.

--- PAGE 8 ---
Backward Math Solving To further showcase
BICO‚Äôs effectiveness, we tackle a more practical
task: math solving. The basic experimental setup
involves teaching LLMs basic math-solving skills
and testing them with ‚Äúreversed logic‚Äù approaches
to address math problems. The dataset (Yu et al.,
2023), derived from GSM8k (Cobbe et al., 2021),
features math questions reversed compared to those
in the original GSM8k dataset. Figure 4 illustrates
a data point from GSM8k alongside its reversed
counterpart. We use NTP and BICO to fine-tune a
Llama-7B model on the original GSM8k dataset,
respectively. We assessed the performance of tuned
models on both the original and reversed test sets.
All training hyperparameters and configurations
follow Yu et al. (2023). The results are shown in
Table 3.
We observe that BICO maintains its performance
on the original test set while achieving an increase
of more than 1 point in solving reversal math prob-
lems, which is a decent improvement in math solv-
ing task and passes the t-test with p-value < 0.01.
Given that models haven‚Äôt seen any backward rea-
soning chains for solving these types of math ques-
tions during fine-tuning, the improvement can be
attributed to BICO, showcasing its capability to en-
hance causal language models with a more versatile
reasoning ability.
Translation The dual nature of translation data
is particularly well-suited for evaluating the rever-
sal curse. We assume that given the "language
X-language Y" order training data, the model may
perform poorly in "language Y-to-language X"
translation due to the reversal curse. To demon-
strate this, we conducted a simple machine trans-
lation task from Chinese to English. Here are the
experimental details:
We developed a set of Chinese-to-English trans-
lation examples structured as follows: ‚ÄúWhen
translating the Chinese term ‚Äò Âè¶Â§ñ‰∏Ä‰∏™ ‚Äô into En-
glish, the equivalent expression is ‚ÄòAnother one‚Äô.‚Äù
For testing, we reversed the training data to con-
duct English-to-Chinese translation tasks, such as:
‚ÄúWhen translating the English phrase ‚Äòanother one‚Äô
into Chinese, the corresponding Chinese expres-
sion is,‚Äù with the correct response being ‚Äò Âè¶Â§ñ‰∏Ä
‰∏™.‚Äô We tested this task on the Llama-7b model,
which has limited Chinese language capacity. Both
the training and testing datasets comprise 100 ex-
amples each. We assessed the translation perfor-
mance using the Exact-match metric. The exper-0-shot NTP BICO
EM (%) 51 63 69
Table 4: BICO enhances the utility of training data
of the reverse-translation task, thereby improving the
accuracy of reverse translation. Note that the Llama
model has inherent multilingual ability, shown in the
zero-shot score.
iment results are listed in Table 4. Note that the
0-shot accuracy for Llama is 50%, reflecting its
original bilingual abilities. BICO outperforms NTP
by 6%, highlighting its potential effectiveness in
real-world tasks such as machine translation.
6 Discussions
This section includes some necessary analysis
about our proposed method BICO, as well as con-
temporary research on the reversal curse.
6.1 Method Analysis
The configuration of hyperparameters in BICO, in-
cluding pMandpO, adheres to conventional prac-
tices akin to BERT (Devlin et al., 2019). However,
in BERT, the mask token ratio pMand the trade-off
between two pretraining tasks pOlack discussion
and explicit formalization. We have studied the
impact of our hyperparameters and details can be
found in the appendix A.
6.2 The Mitigation of Reversal Curse
Some contemporary works aim to tackle the rever-
sal curse through knowledge editing or data aug-
mentation.
Knowledge editing (Ma et al., 2023b) integrates
reverse knowledge directly into model parameters,
ensuring performance. However, this method is
labor-intensive, requiring meticulous annotation
and complex implementation. It necessitates sen-
tences to adhere to a specific structure of one sub-
ject, followed by a relation, and then one object. In
contrast, BICO is agnostic to sentence structures
and easier to implement.
Following us, Guo et al. (2024) propose to miti-
gate the reversal curse through data augmentation.
It can generate explicit reversal data for training,
potentially enhancing performance in complex sce-
narios. However, this method may suffer from label
leakage issues in research experiments, and thus
we do not compare BICO with it.

--- PAGE 9 ---
Here is a scenario that both BICO and ABI fail to
mitigate the reversal curse but data augmentation
can solve according to (Guo et al., 2024). Con-
sider the inverse relation of the previously studied
RN2D, denoted as RD2N. The mapping from D
toNalso forms a bijection: RD2N={< d, n >
|ddescribes n,(n‚àà N)‚àß(d‚àà D)}.
All experimental setups and training details re-
main identical to those previously introduced. Fig-
ure 8 in the appendix illustrates the data construc-
tion associated with this relationship. After tun-
ing, we encounter an preplexing phenomenon: As
shown in Figure 5, after tuning, we observe a
perplexing phenomenon: while BICO improves
the likelihood of ground truth compared to NTP-
tuned models, its impact on exact match or BLEU
scores (Papineni et al., 2002) is insignificant (refer
to appendix A for likelihood computation and de-
tailed results). We have scrutinized the decoding
strategies, including beam search, top-k, and top-p
sampling, but they do not exhibit much difference.
One plausible hypothesis is that pre-trained
LLMs may have developed a ‚Äúcommon sense‚Äù dur-
ing pre-training. This ‚Äúcommon sense‚Äù suggests
that, compared with description-to-name, a name-
to-description relationship is more inclined to be
a one-to-many mapping, resulting in confusion in
the‚Üê‚àí
D2Ntask. For instance, when posed with
the question, ‚ÄúWho is Joe Biden?‚Äù, the model
might respond with many correct descriptions such
as ‚Äúa car enthusiast,‚Äù or ‚Äúan elderly man,‚Äù rather
than the ‚ÄúAmerican president.‚Äù While explicit data
augmentation may offer a solution to this issue,
supported by findings in recent interpretability re-
search (Wang et al., 2024), this method merely
encourages the model to memorize reversed data
rather than enhancing its reasoning abilities.
7 Conclusions
We are the first to study the underlying causes of
the reversal curse and attribute it to a combina-
tion of training objectives and certain inference
mechanisms. When examining the impact of train-
ing objectives, we introduce an innovative fine-
tuning approach for causal language models named
BICO. This method customizes Llama models for
ABI-like objectives, thereby mitigating the rever-
sal curse that emerges during the training phase.
We hope to draw the community‚Äôs attention to the
prevalent configuration of large language models,
especially highlighting the inherent limitations in
Mean ProbabilityBICONTP
0.080.160.240.320.4032.3%38.7%27.5%30.1%
Llama-7bLlama-13bFigure 5: The probability of the desired completion
given prompts provided by various models in‚Üê‚àí
D2N
task. This probability is evaluated across the entire
test set and is presented as an average. It is clear that
BICO enhances the likelihood of achieving ground truth
prediction.
the existing training paradigm.
Limitations
As limitations, there remain some open research
questions that warrant further investigation: Firstly,
quantifying the influence of other processes on ad-
vanced models, such as RLHF, on the reversal curse
poses a more intricate challenge and needs more
study. Secondly, understanding the various mecha-
nisms beyond the training objective that contribute
to exacerbating the reversal curse is crucial, and we
defer this task to future research endeavors.
Acknowledgements
This work was supported by the National Natu-
ral Science Foundation of China (NSFC Grant No.
62122089), Beijing Outstanding Young Scientist
Program NO. BJJWZYJH012019100020098, and
Intelligent Social Governance Platform, Major In-
novation & Planning Interdisciplinary Platform for
the ‚ÄúDouble-First Class‚Äù Initiative, Renmin Univer-
sity of China, the Fundamental Research Funds for
the Central Universities, and the Research Funds of
Renmin University of China. Ang Lv is supported
by the Outstanding Innovative Talents Cultivation
Funded Programs 2024 of Renmin University of
China.

--- PAGE 10 ---
References
Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan
Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-
feng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020.
Unilmv2: Pseudo-masked language models for uni-
fied language model pre-training.
Yoshua Bengio, R√©jean Ducharme, Pascal Vincent,
and Christian Janvin. 2003. A neural proba-
bilistic language model. J. Mach. Learn. Res. ,
3(null):1137‚Äì1155.
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita
Balesni, Asa Cooper Stickland, Tomasz Korbak, and
Owain Evans. 2023. The reversal curse: Llms trained
on "a is b" fail to learn "b is a".
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171‚Äì4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-
aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. 2019. Unified language model
pre-training for natural language understanding and
generation.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:
general language model pretraining with autoregres-
sive blank infilling. pages 320‚Äì335.
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly,
Nova DasSarma, Dawn Drain, Deep Ganguli, ZacHatfield-Dodds, Danny Hernandez, Andy Jones,
Jackson Kernion, Liane Lovitt, Kamal Ndousse,
Dario Amodei, Tom Brown, Jack Clark, Jared Ka-
plan, Sam McCandlish, and Chris Olah. 2021. A
mathematical framework for transformer circuits.
Transformer Circuits Thread . Https://transformer-
circuits.pub/2021/framework/index.html.
Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang
Bian, and Yujiu Yang. 2024. Mitigating reversal
curse in large language models via semantic-aware
permutation training.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. Lora: Low-rank adaptation of
large language models. In ICLR 2022 .
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,
Luke Zettlemoyer, and Omer Levy. 2020. Span-
BERT: Improving pre-training by representing and
predicting spans. Transactions of the Association for
Computational Linguistics , 8:64‚Äì77.
Jun Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu,
and Cong Liu. 2023a. Untying the reversal curse
via bidirectional language model editing. ArXiv ,
abs/2310.10322.
Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu,
and Cong Liu. 2023b. Untying the reversal curse via
bidirectional language model editing.
Kevin Meng, David Bau, Alex J Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in GPT. In Advances in Neural Information
Processing Systems .
Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2024.
Circuit component reuse across tasks in transformer
language models.
OpenAI. 2023. Gpt-4 technical report.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311‚Äì318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Alec Radford and Karthik Narasimhan. 2018. Im-
proving language understanding by generative pre-
training.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1‚Äì67.

--- PAGE 11 ---
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2022. Roformer: En-
hanced transformer with rotary position embedding.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.
Boshi Wang, Xiang Yue, Yu Su, and Huan Sun. 2024.
Grokked transformers are implicit reasoners: A mech-
anistic journey to the edge of generalization.
Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,
Buck Shlegeris, and Jacob Steinhardt. 2023. Inter-
pretability in the wild: a circuit for indirect object
identification in GPT-2 small. In The Eleventh Inter-
national Conference on Learning Representations .
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V . Le. 2020.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo
Li, Adrian Weller, and Weiyang Liu. 2023. Meta-
math: Bootstrap your own mathematical questions
for large language models.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .
Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021. A
robustly optimized BERT pre-training approach with
post-training. In Proceedings of the 20th ChineseNational Conference on Computational Linguistics ,
pages 1218‚Äì1227, Huhhot, China. Chinese Informa-
tion Processing Society of China.

--- PAGE 12 ---
Figure 6: pObalances the model‚Äôs comprehensive under-
standing of training data and generative ability. When
the appropriate pOvalue is selected, BICO mitigates the
reversal curse in the‚Üê‚àí
N2Dtask. Absence of modified
position embeddings (w/o M.P.) impedes the learning
process and results in inferior outcomes due to the out-
of-distribution problem.
A Method Analysis of BICO
We investigate the impact of hyperparameters in
BICO on mitigating the reversal curse induced by
training objectives. It‚Äôs important to note that we
do not aim at searching for the best performance
for a particular task, but rather at studying the char-
acteristics of BICO. The model we used in this
section is Llama-7B.
The Choice of pOThe original ABI uses special
tokens like " <BOS>" and " <EOS>" to indicate
the start and the end of masked sequence. However,
it is worth noting that this differs from the manner
in which Llama makes use of these special tokens.
Therefore, we have opted not to utilize them as
markers for masking in BICO. As a result, we find
Llama models tuned with BICO encounter chal-
lenges in effectively terminating generation. We
observed that they tend to produce lengthy, topic-
related descriptions. To solve this issue, we intro-
duce the solution: at each training step, the model
is optimized with the NTP objective with proba-
bility pOand is optimized with the autoregressive
mask denoising with probability 1‚àípO.
We study the choice of pO, while maintaining a
constant mask rate of pM= 0.15, a widely adopted
value in autoencoding models (Devlin et al., 2019;
Raffel et al., 2020). Our results are illustrated in
Figure 6. We observed that because of the previ-
ously discussed issue, when pO= 0, models can
not generate accurate description in N2Dtask, and
performs poor in‚Üê‚àí
N2Dtask. The model fully fine-
tuned with NTP ( pO= 1) suffers from the reversal
curse, achieving an accuracy rate of 0 %in‚Üê‚àí
N2D
Figure 7: The impact of mask strategy on model per-
formance. From the upper portion in the figure, the
model performance remains consistent across a wide
range of pMvalues (0.15 to 0.45). We also find that the
span masking and i.i.d. masking do not exhibit notable
differences. For the ease of comparison, we use red
dashed line to denote the best results provided by fully
NTP-tuned Llama.
task.
A balanced outcome is achieved with a small pO
around 1/4, enabling the model to preserve its gen-
erative ability while thoroughly learning from the
training data. Consequently, there is a remarkable
improvement in N2D-reverse task performance,
soaring from 0 %to around 80 %, with sustained
proficiency in the N2Dtask. We also explore the
impact of fine-tuning the model without modifying
its attention calculation ( pO= 0, w/o M.P.) to ad-
dress out-of-distribution positions. Due to the need
for a portion of the parameter updates to tackle
OOD issues in position embeddings (Chen et al.,
2023), the learning process is slowed down.
The Mask Strategy We investigate the mask
strategy in BICO. We set the parameter pOat a
constant value of 1/4 and vary the mask probability
pMfrom 0.05 to 0.55, increasing in increments of
0.1. The results are illustrated in the upper portion
of Figure 7. It was observed that extreme values
ofpM, such as 0.55 or 0.05, yielded suboptimal
results, while intermediate values did not show sig-
nificant divergence.
Additionally, we explore the mask span. Previ-
ous studies (Raffel et al., 2020; Joshi et al., 2020)
suggest that masking a contiguous span of tokens

--- PAGE 13 ---
Training data"prompt": "Known for being the renowned composer of the world's first underwater symphony, "Abyssal Melodies.",", "completion": " Uriah Hawthorne now enjoys a quite life."}ùë´ùüêùêçTest"prompt": " Widely acclaimed for composing the world's first underwater symphony, "Abyssal Melodies.",","completion": " Uriah Hawthorne"}ùë´ùüêùëµTest"prompt": "In the annals of uniqueness, Uriah Hawthorne shines as", "completion": " the renowned composer of the world's first underwater symphony, "Abyssal Melodies."."}Figure 8: Data employed for studying the reversal curse on relation RD2N. All names and descriptions are fictitious.
During test stage, the model is given the ‚Äúprompt‚Äù and the ground truth is the content of ‚Äúcompletion.‚Äù
can be more effective than employing indepen-
dently and identically distributed (i.i.d.) mask to-
kens, equivalent to a mask span of 1. In our exper-
iment, we explored mask span length S, and the
results are in the bottom of Figure 7. We did not
observe any clear performance differences among
different Ssettings.
BD2Nand‚Üê‚àí
D2NTasks
Figure 8 illustrates a data point in D2Nand‚Üê‚àí
D2N
tasks.
For specifics regarding likelihood computation
in Figure 5: We computed the likelihood of the
ground truth assigned by LLMs after fine-tuning as
follows:
p(completion |prompt ) =e‚àíLNLL,
where LNLL=‚àílX
i=klogp(ti|t0:i‚àí1).(8)
Here, tidenotes the i-th token within the sequence,
which has a length of l. Importantly, we do not
take into account the positions corresponding to
the prompt (the first ktokens) when computing the
loss.
The Exact Match scores and BLEU (Papineni
et al., 2002) scores (specifically for the‚Üê‚àí
D2Ntask,
which involves generating long descriptions) of
models tuned by different training objectives are
reported in Table 5.Table 5: Models tuned by different training objectives
consistently struggle in D2Nand‚Üê‚àí
D2Ntask.
Model Objective D2N (EM)‚Üê‚àí
D2N(EM)‚Üê‚àí
D2N(BLEU)
GLM-2BNTP 100.00 0.00 19.70
ABI 100.00 0.07 22.13
GLM-10BNTP 100.00 0.00 19.01
ABI 99.33 1.67 22.15
Llama-7BNTP 100.00 0.00 19.65
BICO 99.67 1.00 21.00
Llama-13BNTP 98.67 0.00 20.62
BICO 99.33 1.33 22.15
