# 2307.11088.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2307.11088.pdf
# Kích thước tệp: 1239004 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
bản thảo
L-EVAL: THIẾT LẬP ĐÁNH GIÁ TIÊU CHUẨN HÓA
CHO CÁC MÔ HÌNH NGÔN NGỮ NGỮ CẢNH DÀI
Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang,
Lingpeng Kong, Xipeng Qiu
Đại học Fudan, Đại học Hồng Kông
Đại học Illinois Urbana-Champaign
Phòng thí nghiệm AI Thượng Hải

TÓM TẮT
Gần đây, đã có sự quan tâm ngày càng tăng trong việc mở rộng độ dài ngữ cảnh của các mô hình ngôn ngữ lớn (LLMs), nhằm xử lý hiệu quả các đầu vào dài của một lượt hoặc các cuộc hội thoại với lịch sử rộng lớn hơn. Trong khi các mô hình độc quyền như GPT-4 và Claude có thể phần lớn bảo tồn khả năng lý luận trong ngữ cảnh mở rộng, các mô hình mã nguồn mở vẫn đang tiến triển qua các giai đoạn phát triển ban đầu. Để thu hẹp khoảng cách này, chúng tôi đề xuất L-Eval để thiết lập một đánh giá tiêu chuẩn hơn cho các mô hình ngôn ngữ ngữ cảnh dài (LCLMs) giải quyết hai khía cạnh chính: xây dựng tập dữ liệu và các chỉ số đánh giá. Một mặt, chúng tôi xây dựng một bộ đánh giá mới chứa 20 nhiệm vụ phụ, 508 tài liệu dài và hơn 2.000 cặp truy vấn-phản hồi được gán nhãn thủ công bao gồm các phong cách câu hỏi đa dạng, các lĩnh vực và độ dài đầu vào (3k ∼200k token). Mặt khác, chúng tôi điều tra hiệu quả trong các chỉ số đánh giá cho LCLMs. Kết quả cho thấy các chỉ số khớp n-gram phổ biến thường không thể tương quan tốt với đánh giá của con người, và do đó chúng tôi ủng hộ mạnh mẽ việc đánh giá được tăng cường bằng hướng dẫn độ dài (LIE) và sử dụng các thẩm phán LLM. Chúng tôi đã tiến hành một nghiên cứu toàn diện về 4 LLMs thương mại phổ biến và 12 đối tác mã nguồn mở sử dụng benchmark L-Eval. Các phát hiện thực nghiệm của chúng tôi cung cấp những hiểu biết hữu ích vào nghiên cứu về LCLMs và đặt nền móng cho việc phát triển đánh giá có nguyên tắc hơn của các mô hình này.¹

1 GIỚI THIỆU
Hiện tại, một lượng nỗ lực đáng kể đang được dành cho nghiên cứu về việc mở rộng độ dài ngữ cảnh của các mô hình ngôn ngữ lớn. Các giải pháp phổ biến chủ yếu liên quan đến việc tiếp tục đào tạo trước hoặc tinh chỉnh các mô hình tiêu chuẩn trên các đầu vào dài hơn sử dụng các kiến trúc hiệu quả hơn (Ding et al., 2023; Dao et al., 2022; Liang et al., 2023; Mohtashami & Jaggi, 2023; Li et al., 2023b), cũng như nhúng vị trí được mở rộng (Su et al., 2022; Sun et al., 2022; LocalLLaMA, 2023b; Qin et al., 2023).

Có các benchmark đa nhiệm vụ mở rộng (Hendrycks et al., 2021a; Suzgun et al., 2022) cho các mô hình ngôn ngữ với lời nhắc ngắn, tuy nhiên một benchmark chất lượng cao trong mô hình hóa ngữ cảnh dài chưa được thiết lập, tạo ra cơ hội cho sự phát triển tiếp theo trong lĩnh vực này. Trong khi đó, hầu như tất cả các benchmark tạo văn bản chuỗi dài trước đây đều dựa chủ yếu vào các chỉ số khớp n-gram (Zhang et al., 2023; Shaham et al., 2022), như ROUGE (Lin, 2004). Liệu các chỉ số thường được sử dụng này có tương quan tốt với đánh giá của con người khi kiểm tra LCLMs trong thiết lập zero-shot vẫn là một câu hỏi. Hơn nữa, cộng đồng mã nguồn mở đã phát hành một số lượng đáng kể các mô hình ngôn ngữ với độ dài ngữ cảnh 16k, hoặc 32k (Li et al., 2023a; Du et al., 2022). Một nghiên cứu so sánh toàn diện về các mô hình này có thể có giá trị lớn.

Để giải quyết những vấn đề này, chúng tôi đề xuất L-Eval để kêu gọi một đánh giá tiêu chuẩn hơn của các mô hình ngôn ngữ ngữ cảnh dài. Đối với xây dựng tập dữ liệu, L-Eval có 20 nhiệm vụ phụ, 4 nhiệm vụ phụ được chú thích từ đầu (§3.1), 4 nhiệm vụ phụ được chú thích lại từ các tập dữ liệu công khai (§3.2), và 12 nhiệm vụ phụ còn lại được làm sạch thủ công từ các tập dữ liệu chuỗi dài trước đây. Chúng tôi chia các nhiệm vụ này trong L-Eval thành hai nhóm: nhiệm vụ đóng và nhiệm vụ mở. Nhóm đóng chủ yếu

¹Chúng tôi phát hành bộ đánh giá mới, mã nguồn và tất cả kết quả tạo ra trên https://github.com/OpenLMLab/LEval

1arXiv:2307.11088v3 [cs.CL] 4 Oct 2023

--- TRANG 2 ---
bản thảo
kiểm tra khả năng lý luận và hiểu biết liên quan đến ngữ cảnh dài hơn, và nhóm mở bao gồm nhiều nhiệm vụ tóm tắt hơn đòi hỏi tổng hợp thông tin tài liệu dài. Trong thiết kế L-Eval, chúng tôi ưu tiên tính đa dạng và chất lượng hơn số lượng, đảm bảo tính chính xác bằng cách xác thực thủ công tất cả các mẫu sau khi thu thập dữ liệu (§3.3). Tính đa dạng dữ liệu của chúng tôi, được thể hiện trong các phong cách câu hỏi, lựa chọn lĩnh vực và độ dài đầu vào, được chi tiết trong Bảng 1.

Ngoài ra, việc phát triển các chỉ số đánh giá phù hợp cho LCLMs trên các nhiệm vụ mở nơi nhiều đầu ra có thể chấp nhận được là quan trọng, nhưng đầy thử thách. Trong công trình này, chúng tôi nghiên cứu các hạn chế của các chỉ số truyền thống dựa trên khớp từ vựng. Chúng tôi chứng minh rằng các chỉ số này thường không tương quan với kết quả đánh giá của con người. Các thí nghiệm tiếp theo của chúng tôi cho thấy các thẩm phán LLM (Li et al., 2023c; Zheng et al., 2023) cung cấp độ chính xác vượt trội trong đánh giá các nhiệm vụ mở. §4 giải thích cách chúng tôi thiết lập một thẩm phán LLM ngữ cảnh ngắn trong thiết lập đánh giá ngữ cảnh dài. Xem xét ảnh hưởng của độ dài tạo ra lên hiệu suất và để tránh rút ra các kết luận sai lệch, chúng tôi đề xuất kỹ thuật đánh giá Được Tăng Cường Hướng Dẫn Độ Dài (LIE) cho tất cả các chỉ số dựa trên tham chiếu, bao gồm cả những chỉ số sử dụng thẩm phán LLM. Kết quả thực nghiệm chứng minh một cải thiện đáng kể được mang lại bởi đánh giá LIE trong hệ số tương quan Kendall-Tau (τ) với đánh giá của con người (Hình 2), cho tất cả các chỉ số tự động.

Chúng tôi cũng đã tiến hành một nghiên cứu toàn diện với 16 LLMs khác nhau (§5.1) trong L-Eval. Một số phát hiện chính của chúng tôi được tóm tắt dưới đây: (1) Vẫn còn một khoảng cách đáng kể giữa các LCLMs mã nguồn mở và các mô hình thương mại, cho cả nhiệm vụ đóng (Bảng 3) và nhiệm vụ mở được đánh giá bởi LLMs và con người (Bảng 4, 5). Tuy nhiên, khoảng cách này không được phản ánh chính xác bởi các chỉ số n-gram. (2) Trong khi các nỗ lực hiện tại trên LCLMs mã nguồn mở cải thiện hiệu suất trên các nhiệm vụ đóng, chúng lại thiếu sót đáng kể trên các nhiệm vụ mở. Điều này phần lớn do sự hiểu sai hướng dẫn của các mô hình khi độ dài ngữ cảnh đầu vào tăng lên. (3) Các thí nghiệm trên GPT-3.5-Turbo với cả bộ truy xuất dense và sparse cho thấy các mô hình ngữ cảnh đầy đủ end-to-end vượt trội hơn các hệ thống dựa trên truy xuất truyền thống. (4) Nhúng vị trí được mở rộng không cần đào tạo có thể tăng cường khả năng truy xuất của LLMs trên đầu vào dài hơn, trong khi nó có thể ảnh hưởng tiêu cực đến khả năng lý luận của chúng.

Nhiều kết luận thú vị hơn có thể được tìm thấy trong §5.2 và §A.3. Chúng tôi hy vọng L-Eval và các phát hiện của chúng tôi góp phần vào hiểu biết sâu sắc hơn về nghiên cứu LCLM hiện tại và sự phát triển tiếp theo của các mô hình và chỉ số đánh giá.

2 CÔNG TRÌNH LIÊN QUAN

2.1 CÁC MÔ HÌNH NGÔN NGỮ NGỮ CẢNH DÀI

Việc cung cấp ngữ cảnh dài dẫn đến các nút thắt cổ chai trong đào tạo và suy luận mô hình ngôn ngữ do tài nguyên tính toán. Một số nỗ lực cộng đồng tập trung vào việc phát triển các cơ chế attention hiệu quả để xây dựng các mô hình ngôn ngữ hiệu quả (Sun et al., 2023; Ding et al., 2023; Li et al., 2023b; Fu et al., 2023; Peng et al., 2023a). Ngoài việc tối ưu hóa cơ chế attention, một số công trình (Bulatov et al., 2023; Dai et al., 2019; Mohtashami & Jaggi, 2023) tập trung vào việc chia nhỏ đầu vào để mô hình hóa cả văn bản hiện tại trong đoạn và các trạng thái ngữ cảnh trước đó, hiệu quả mở rộng độ dài xử lý ngữ cảnh. Bên cạnh thử thách về hiệu quả, khả năng mở rộng của nhúng vị trí cũng rất quan trọng. ALiBi (Press et al., 2022), và XPOS (Sun et al., 2022) nhấn mạnh tầm quan trọng của ngữ cảnh cục bộ để tăng cường khả năng ngoại suy của mô hình ngôn ngữ. Hơn nữa, nội suy vị trí (PI) (Chen et al., 2023) và NTK-aware (LocalLLaMA, 2023b;a) là các phương pháp phổ biến nhất dựa trên RoPE (Su et al., 2022) để hiệu quả và hiệu suất mở rộng độ dài ngữ cảnh. Tuy nhiên, các công trình này chủ yếu xác thực phương pháp của họ với perplexity (PPL) (Sun et al., 2021; LocalLLaMA, 2023b), và chưa có xác thực hệ thống trên các nhiệm vụ thực tế.

2.2 CÁC BENCHMARK CHUỖI DÀI

Tay et al. (2020) giới thiệu Long Range Arena (LRA), một benchmark bao gồm năm nhiệm vụ phân loại khác biệt. CAB (Zhang et al., 2023) là một benchmark khác cho các thiết kế attention hiệu quả khác nhau bằng cách so sánh cả hiệu quả và độ chính xác. Trong lĩnh vực ngôn ngữ, công trình trước đây về LCLMs có xu hướng báo cáo PPL để đánh giá các mô hình ngôn ngữ (Su et al., 2022; Peng et al., 2023b) trên ngữ cảnh dài hơn. Tuy nhiên, PPL có thể thường không tương quan với hiệu suất thực tế (Sun et al., 2021). ZeroScrolls (Shaham et al., 2022; 2023) và LongBench (Bai et al., 2023) là các bộ đánh giá ngữ cảnh dài đồng thời. L-Eval khác với chúng ở 3 khía cạnh: (1) Các mẫu được chọn thủ công.

2

--- TRANG 3 ---
bản thảo
Các mẫu kiểm tra được lọc tự động bởi các benchmark của họ, trong khi những mẫu cho L-Eval được lọc thủ công. (2) Các chỉ số tiêu chuẩn hóa. Chúng tôi là những người đầu tiên điều tra các tương quan giữa các chỉ số từ vựng truyền thống và các chỉ số LLM được đề xuất gần đây với đánh giá của con người trong thiết lập ngữ cảnh dài. L-Eval không còn dựa chủ yếu vào các chỉ số N-gram. (3) Nhiều nhiệm vụ đóng hơn. Do các vấn đề công bằng trong các nhiệm vụ mở. L-Eval có nhiều nhiệm vụ đóng hơn phản ánh kết quả không thiên vị.

3 HƯỚNG TỚI CÁC TẬP DỮ LIỆU NGỮ CẢNH DÀI CHẤT LƯỢNG CAO VÀ ĐA DẠNG

Trong phần này, chúng tôi nêu bật một số thủ tục chính trong xây dựng dữ liệu L-Eval. Cụ thể, chúng tôi trình bày quy trình chú thích, chú thích lại và lọc thủ công cũng như thống kê của L-Eval. Vui lòng tham khảo Phụ lục B để biết chi tiết chú thích đầy đủ và các ví dụ.

3.1 CHÚ THÍCH DỮ LIỆU TỪ ĐẦU

Có 4 tập dữ liệu được chú thích từ đầu trong L-Eval: Coursera, SFcition, CodeU và LongFQA. Các nguồn tài nguyên gốc là video từ Coursera, các tập dữ liệu mã nguồn mở trước đây, mã nguồn từ các thư viện Python nổi tiếng và bản ghi cuộc gọi thu nhập công khai, tương ứng.

Coursera Tập dữ liệu này có nguồn gốc từ trang web Coursera.² Để giảm độ khó của chú thích, chúng tôi chọn bốn khóa học công khai liên quan đến dữ liệu lớn và học máy (§B.4). Tài liệu đầu vào dài là phụ đề của các video. Câu hỏi và câu trả lời đúng được gán nhãn bởi các tác giả. Phong cách hướng dẫn của Coursera có định dạng trắc nghiệm. Để tăng độ khó của nhiệm vụ, chúng tôi đã đặt nhiều lựa chọn đúng. Theo hiểu biết tốt nhất của chúng tôi, đây là tập dữ liệu trắc nghiệm đầu tiên có nhiều câu trả lời đúng và nó thử thách hơn so với câu hỏi một lựa chọn (Bảng 3).

SFcition Chúng tôi chú thích nhiệm vụ phụ này để kiểm tra lòng trung thành của LCLM với ngữ cảnh đầu vào. Chúng tôi lập luận rằng trong LCLMs, kiến thức ngữ cảnh (được lưu trữ trong đầu vào dài) quan trọng hơn kiến thức tham số (thu được trong quá trình đào tạo trước). Thực tế, nhiều tài liệu dài là riêng tư và không bao giờ có thể được nhìn thấy trong quá trình đào tạo trước. LLMs nên tuân theo kiến thức ngữ cảnh thay vì kiến thức tham số trong thiết lập ngữ cảnh dài. Để mô phỏng kịch bản này, chúng tôi chú thích một tập dữ liệu khoa học viễn tưởng gồm các câu hỏi Đúng hoặc Sai. Hầu hết các câu trả lời cho những câu hỏi này trái ngược với các nguyên lý thế giới thực và không tuân thủ các định luật vật lý thực tế (§B.5). Chúng tôi thấy rằng Turbo-16k gặp khó khăn với nhiệm vụ này, có xu hướng trả lời câu hỏi dựa vào kiến thức tham số (Bảng 3).

CodeU Là một tập dữ liệu hiểu mã, nó yêu cầu LLM suy luận đầu ra của một chương trình Python dài. Chúng tôi chủ yếu sử dụng mã nguồn từ Numpy³ và xây dựng một cơ sở mã xử lý chuỗi. Để ngăn LLMs trả lời câu hỏi dựa trên kiến thức tham số của họ, chúng tôi thay thế tên hàm gốc. LLMs nên đầu tiên xác định vị trí gọi hàm và xác định hàm nào được gọi. CodeU là nhiệm vụ thử thách nhất trong L-Eval (§B.6).

LongFQA Chúng tôi cũng nhận thấy rằng thiếu các tập dữ liệu trả lời câu hỏi ngữ cảnh dài trong lĩnh vực tài chính và chúng tôi chú thích các cặp QA dựa trên bản ghi cuộc gọi thu nhập công khai từ phần Quan hệ Nhà đầu tư của 6 trang web công ty. Vui lòng tham khảo §B.8 để biết chi tiết.

3.2 CHÚ THÍCH LẠI DỮ LIỆU TỪ CÁC TẬP DỮ LIỆU CÔNG KHAI

Chúng tôi chú thích lại 5 tập dữ liệu có sẵn công khai trong L-Eval. GSM(16-shot) được lấy từ 100 bài toán cấp tiểu học trong tập dữ liệu GSM8k (Cobbe et al., 2021). Nếu LCLM duy trì khả năng lý luận trên ngữ cảnh dài hơn, việc sử dụng nhiều ví dụ chất lượng cao hơn sẽ có tác động tích cực trong việc giải quyết các bài toán (Li et al., 2023b). Chúng tôi xây dựng 16 ví dụ trong ngữ cảnh với Chuỗi Suy nghĩ dài trong đó 8 ví dụ đến từ chain-of-thought-hub⁴ và 8 ví dụ được xây dựng bởi chúng tôi. Chúng tôi thí nghiệm với các ví dụ mới được xây dựng và độ chính xác của Turbo-16k-0613 tăng từ 79 (8-shot) lên 84 (16-shot).

Chúng tôi tiêm một số hướng dẫn tổng hợp mới để kiểm tra mô hình ngữ cảnh toàn cục vào QuALITY (Pang et al., 2022), như "Chúng ta có thể suy luận gì từ câu dài nhất trong câu chuyện này?" và "Có bao nhiêu từ trong câu chuyện?". Do những loại câu hỏi này có thể hiếm khi xảy ra trong các cuộc hội thoại thế giới thực, tỷ lệ của chúng trong L-Eval là cực kỳ nhỏ. Tập dữ liệu Openreview chứa các bài báo được thu thập từ openreview.net. Chúng tôi yêu cầu mô hình (1) viết phần Tóm tắt, (2) tóm tắt công trình liên quan, và (3) cuối cùng đưa ra phản hồi bao gồm các đề xuất có giá trị và một số câu hỏi cho tác giả. Chúng tôi chọn bài báo với các phần công trình liên quan chất lượng cao và các đánh giá hữu ích được viết bởi người đánh giá con người để tạo thành bộ kiểm tra này.⁵ Tiếp theo, chúng tôi sử dụng SPACE (Angelidis et al., 2021) để kiểm tra nhiệm vụ tóm tắt đánh giá dựa trên khía cạnh, và các hướng dẫn cho tập dữ liệu được chú thích bởi chúng tôi. Chúng tôi áp dụng các hướng dẫn đa dạng để ngăn chặn overfitting.

²https://coursera.org/
³https://github.com/numpy/numpy
⁴https://github.com/FranxYao/chain-of-thought-hub

3

--- TRANG 4 ---
bản thảo
Claude-100k GPT-4-32k Turbo-16kChatglm2-32k Vicuna1.5-16k Longchat-16k llama2-ntk-16k050100Độ chính xác(%)Chủ đề Thứ hai/Thứ ba
Chủ đề Đầu tiên
Hình 1: Độ chính xác kiểm tra (%) của các mô hình khác nhau với việc truy xuất chủ đề đầu tiên và truy xuất chủ đề thứ hai/thứ ba.

Công trình trước đây (Li et al., 2023a; Liu et al., 2023) đã sử dụng các nhiệm vụ truy xuất để kiểm tra khả năng mô hình hóa phụ thuộc ngữ cảnh dài thông qua việc truy xuất một cái gì đó trên ngữ cảnh dài. L-Eval bao gồm một nhiệm vụ truy xuất chủ đề đầu tiên phổ biến TopicRet (Li et al., 2023a), được định dạng như: "[chủ đề-1] Lịch sử Chat [hướng dẫn]". Tuy nhiên, như chúng ta có thể thấy từ Hình 1, việc truy xuất chủ đề đầu tiên quá dễ để phân biệt khả năng của các mô hình khác nhau. Tuy nhiên, nhiệm vụ truy xuất chủ đề thứ hai và thứ ba đưa ra một mức độ thử thách cao hơn đáng kể. Quan sát thấy rằng gần như tất cả các mô hình mã nguồn mở đều gặp khó khăn trong nhiệm vụ. Vì vậy chúng tôi tăng cường nhiệm vụ với việc truy xuất chủ đề thứ hai/thứ ba.

3.3 LỌC VÀ SỬA LỖI DỮ LIỆU

12 nhiệm vụ còn lại có nguồn gốc từ các tập dữ liệu hiện có theo các bộ đánh giá trước đây (Zhang et al., 2023). Tuy nhiên, L-Eval liên quan đến nhiều lao động thủ công hơn sau khi thu thập dữ liệu vì chúng tôi thấy chất lượng chú thích của các tập dữ liệu chuỗi dài trước đây dao động nghiêm trọng và có nhiều câu hỏi không thể trả lời không liên quan đến ngữ cảnh. Những lỗi này khó có thể được sửa chữa bằng cách sử dụng các script tiền xử lý tự động trong các công trình trước đây. Trong L-Eval, tất cả các mẫu được lọc và sửa chữa thủ công sau khi thu thập dữ liệu. Cụ thể, chúng tôi sử dụng Claude-100k làm trợ lý để lọc các QA sai và câu hỏi không thể trả lời. Đầu tiên, chúng tôi nhập tài liệu dài vào Claude và yêu cầu nó cung cấp câu trả lời và đưa ra giải thích. Nếu Claude tạo ra câu trả lời không khớp lớn với sự thật cơ bản hoặc tuyên bố rằng chúng ta không thể suy luận câu trả lời từ ngữ cảnh, chúng tôi sẽ thực hiện chú thích lại hoặc đơn giản là loại bỏ chúng.

3.4 THỐNG KÊ

Thống kê của L-Eval được hiển thị trong Bảng 1. L-Eval chứa các phong cách câu hỏi khác nhau như câu hỏi trắc nghiệm (TOFEL (Tseng et al., 2016), QuALITY, Coursera), câu hỏi đúng hoặc sai (SFiction), bài toán (GSM), hiểu mã (CodeU), hội thoại hướng mục tiêu (Multi-Doc2Dial (Feng et al., 2021)), QA trích xuất (CUAD (Hendrycks et al., 2021b), NQ (Kwiatkowski et al., 2019)), QA trừu tượng (LongFQA, NarrativeQA (Ko ˇcisk´y et al., 2017), Qasper (Dasigi et al., 2021)), tóm tắt tài liệu đơn (GovReport (Huang et al., 2021), BigPatent (Sharma et al., 2019), SummScreen (Chen et al., 2022), QMSum (Zhong et al., 2021)), tóm tắt đa tài liệu (Multi-News (Fabbri et al., 2019), SPACE (Angelidis et al., 2021)), viết nghiên cứu (Openreview) và nhiều hơn nữa. Các tài liệu dài trong L-Eval trải rộng nhiều lĩnh vực như luật, tài chính, bài báo học thuật, bài giảng, cuộc hội thoại dài, tin tức, cơ sở mã Python nổi tiếng, tiểu thuyết dài và cuộc họp. Độ dài đầu vào trung bình trong L-Eval dao động từ 4k đến 60k. Mẫu tối đa trong L-Eval chứa gần 200k token. Tính đa dạng này đại diện cho các kịch bản thế giới thực nơi các nhiệm vụ khác nhau có thể yêu cầu độ dài ngữ cảnh và hướng dẫn khác nhau. Độ dài tham chiếu trong L-Eval cũng khác nhau đáng kể giữa các nhiệm vụ.

4 HƯỚNG TỚI CÁC CHỈ SỐ ĐÁNH GIÁ NGỮ CẢNH DÀI TIÊU CHUẨN HÓA

Trong phần này, chúng tôi trình bày các chỉ số đánh giá khác nhau cho tạo văn bản, bao gồm đánh giá kiểm tra cho các nhiệm vụ đóng và các mức độ đánh giá mở khác nhau, hầu hết đều là các chỉ số dựa trên tham chiếu. Chúng tôi cũng tiến hành thí nghiệm để nghiên cứu mối tương quan giữa các chỉ số tự động và đánh điểm của con người.

Đánh giá kiểm tra Điều này được thiết kế cho các nhiệm vụ đóng, tức là câu hỏi trắc nghiệm. Chỉ số đánh giá được sử dụng cho các nhiệm vụ này tuân theo định dạng khớp chính xác (độ chính xác %), tương tự như chấm điểm bài kiểm tra. Điểm của mỗi câu hỏi được tính là 100 chia cho số câu hỏi.

Đánh giá của con người Đây là đánh giá chính xác nhất cho các nhiệm vụ mở. Mặc dù một số công trình cho thấy GPT-4 có thể nhất quán với đánh giá của con người, LLMs không thể thay thế đánh giá của con người. Chúng tôi huy động các đánh giá viên con người để chấm điểm các đầu ra trên thang điểm từ 1 đến 5, có nghĩa là từ đầu ra kém đến đầu ra xuất sắc. Để tiết kiệm phòng thí nghiệm con người, chúng tôi đề xuất một tập con được sử dụng cho đánh giá con người có 12 tài liệu dài với 85 câu hỏi mở (tập con 85 câu hỏi).

Các thẩm phán mô hình ngôn ngữ lớn để đánh giá LCLMs Trong thiết lập ngữ cảnh ngắn, đánh giá sử dụng LLMs là chỉ số chính xác nhất để tự động đánh giá các mô hình trên các nhiệm vụ mở (Zheng et al., 2023; Li et al., 2023c; Dubois et al., 2023). Các công trình này giả định đánh giá viên LLM là một "siêu mô hình", nhưng giả định này không đúng trong thiết lập ngữ cảnh dài vì không thể cung cấp toàn bộ đầu vào dài vào LLMs như GPT-4. Không giống như đánh giá ngữ cảnh ngắn, GPT-4 không thể tự suy luận câu trả lời đúng. Do đó, kết quả đánh giá chủ yếu phụ thuộc vào câu trả lời tham chiếu và câu hỏi người dùng. Trong L-Eval, chúng tôi sử dụng định dạng đấu cặp và chúng tôi chọn Turbo-16k-0613 làm mô hình cơ sở và báo cáo tỷ lệ thắng so với Turbo-16k-0613 % có nghĩa là bao nhiêu mẫu có thể đánh bại Turbo-16k. Chúng tôi nghiên cứu hai thẩm phán LLM: GPT-4 và GPT-3.5 trong phần thí nghiệm. Các đánh giá viên LLM đã được báo cáo là ưa thích các câu trả lời chi tiết và dài hơn (Zheng et al., 2023). Thiên vị này trở nên rõ rệt hơn trong thiết lập ngữ cảnh dài vì đầu vào không nhìn thấy khiến thẩm phán khó xác định chính xác tính đúng đắn của các chi tiết và thông tin cụ thể. Do đó, mô hình phán quyết phải ghi nhớ rằng các chi tiết không được chứng thực bởi câu trả lời tham chiếu không nên được coi là có lợi. Chúng tôi tăng cường lời nhắc phán quyết với: Các chi tiết hoặc thông tin bổ sung không được đề cập trong câu trả lời tham chiếu không thể được coi là ưu điểm và không để chúng ảnh hưởng đến phán quyết của bạn. Nếu bạn chỉ muốn đánh giá một phần của các nhiệm vụ trong L-Eval, chúng tôi khuyến nghị sử dụng các thẩm phán LLM. Việc xác minh 1000+ câu hỏi mở thông qua GPT-4

⁵Tuyên bố đạo đức: chúng tôi không khuyến khích các đánh giá viên sử dụng các mô hình lớn cho đánh giá. Mục tiêu của chúng tôi là hỗ trợ tác giả cải thiện thêm bài báo của họ.

5

--- TRANG 5 ---
bản thảo

Hình 3: Xếp hạng của sáu mô hình dưới các chỉ số đánh giá khác nhau (Human-avg, Human-1, GPT-4, GPT-3.5, R-L, và F-1) có hoặc không có hướng dẫn độ dài. Human-avg đại diện cho điểm trung bình từ đánh giá con người, và Human-1 biểu thị điểm được đưa ra bởi người chú thích đầu tiên.

là không thể chi trả.⁶ Do đó chúng tôi chia thủ công một tập con cho đánh giá GPT-4 gồm 17 tài liệu dài đa dạng với 96 câu hỏi mở (tập con 96 câu hỏi).⁷

Đánh giá khớp N-gram Xem xét rằng việc đánh giá tất cả các nhiệm vụ vẫn đắt đỏ cho các đánh giá viên con người/LLM, L-Eval cũng tính đến các chỉ số n-gram. Các chỉ số N-gram như ROUGE-L (R-L) và điểm F-1 được sử dụng rộng rãi trong các tập dữ liệu truyền thống và chúng cũng được áp dụng rộng rãi trong các benchmark tạo văn bản thông qua việc thực hiện khớp từ vựng. Đáng chú ý là các chỉ số khớp n-gram rất nhạy cảm với độ dài của sự thật cơ bản, thể hiện thiên vị về độ dài. Phân tích liên quan nằm trong §4.1 sau đây.

4.1 ĐÁNH GIÁ NGỮ CẢNH DÀI ĐƯỢC TĂNG CƯỜNG HƯỚNG DẪN ĐỘ DÀI

Hình 2: Hệ số tương quan Kendall-Tau của các chỉ số tự động khác nhau với điểm trung bình của con người.

Trong các thí nghiệm sơ bộ, chúng tôi thấy rằng LLMs có xu hướng tạo ra các phản hồi rất dài mang lại trở ngại cho đánh giá dựa trên tham chiếu (xem ∆L Bảng 2). Thiên vị về độ dài này dẫn đến ảnh hưởng đáng kể lên các chỉ số n-gram. Ví dụ, Claude-100k chỉ đạt được điểm F-1 là 9.84 do độ dài đầu ra không mong muốn.

Trong L-Eval, chúng tôi lập luận rằng các mô hình ngôn ngữ ngữ cảnh dài nên tập trung hơn vào nội dung chính xác hơn là độ dài chính xác. Thực tế, các vấn đề về độ dài tạo ra không mong muốn có thể được giải quyết dễ dàng bằng cách nhắc mô hình. Chúng tôi đầu tiên áp dụng đánh giá Được Tăng Cường Hướng Dẫn Độ Dài (LIE) trong các benchmark đánh giá LLMs đơn giản nhưng hiệu quả trong việc khắc phục thiên vị về độ dài, tức là số từ của sự thật cơ bản được tiết lộ trực tiếp cho LCLMs. Đánh giá LIE trong công trình này được thực hiện bằng cách tiêm mô hình với độ dài mong muốn vào hướng dẫn gốc (ví dụ, [Hướng dẫn Gốc]: Vui lòng tóm tắt ý kiến của giáo sư. [Hướng dẫn Độ Dài]: Chúng tôi cần một bản tóm tắt 50 từ, trong đó 50 là số từ trong câu trả lời tham chiếu). Kết quả của Claude-100k trong Bảng 2 chứng minh cải thiện đáng kể về điểm F-1: có khoảng cách gần 50 điểm tùy thuộc vào việc mô hình có tạo ra với độ dài mong đợi hay không.

Xác thực thí nghiệm Để xác thực đánh giá LIE, sau đó chúng tôi tiến hành đánh giá con người trên tập con 85 câu hỏi. Chúng tôi có 3 người chú thích để xác minh 7 mô hình và tính hệ số tương quan Kendall-Tau (τ) giữa các chỉ số này và điểm trung bình của con người. Kết quả chính được hiển thị trong Hình 2 (Thanh xanh) và thiết lập thí nghiệm trong §A.2. Kết quả cho thấy tất cả các chỉ số tự động này (ngoại trừ GPT-4) không tương quan với đánh giá của con người. So với các chỉ số N-gram, các thẩm phán LLM chính xác và bền vững hơn với độ dài đầu ra. Như chúng ta có thể thấy từ Hình 2, cải thiện được mang lại bởi hướng dẫn độ dài được đánh dấu bằng màu vàng, và sau khi thêm hướng dẫn độ dài, τ đã được cải thiện từ 0.5 lên 0.8 cho ROUGE-L và τ của đánh giá viên GPT-4 thậm chí đã đạt đến 1. Trong Hình 3, chúng tôi chuyển đổi điểm thành xếp hạng (tốt nhất là 5 và tệ nhất là 1)

⁶Kiểm tra 4 tập dữ liệu trong Bảng 2 cần khoảng $100!
⁷Đánh giá đầu ra từ tập con 96 câu hỏi với GPT-4 chỉ cần khoảng $5.

6

--- TRANG 6 ---
bản thảo
Bảng 2: Kết quả trên 2 nhiệm vụ tóm tắt mở và 2 nhiệm vụ QA trừu tượng. GPT-4 có nghĩa là tỷ lệ thắng với Turbo-16k sử dụng GPT-4 làm thẩm phán. ∆L có nghĩa là sự khác biệt về độ dài câu trả lời được tạo ra với độ dài sự thật cơ bản. Kết quả tốt nhất được gạch chân. Kết quả màu đỏ có nghĩa là giải mã với độ dài mong muốn tạo ra sự khác biệt lớn trong hiệu suất.

[THIS IS TABLE: A complex table showing model performance results across different tasks (SPACE, QMSum, NQ, NrtvQA) with various metrics (R-L, GPT-4, ∆L, F-1) for different models including Claude-100k, Chatglm2-32k, Longchat-7b-16k, and Llama2-13b-chat, both with and without length instruction]

và hiển thị điểm của 6 mô hình được đánh giá bằng 6 hệ thống đánh giá khác nhau. Hình 3 (a) hiển thị kết quả được đưa ra bởi các chỉ số không có hướng dẫn độ dài. Những hình lục giác này thường bị biến dạng vì các chỉ số này thường không thể đạt được tương quan tốt. Khi so sánh các mô hình được tăng cường với hướng dẫn độ dài trong (b), quan sát thấy rằng các hình lục giác trở nên đều đặn hơn.

5 BENCHMARK CÁC LLMS VỚI L-EVAL

Trong phần này, chúng tôi liệt kê 16 mô hình cơ sở của chúng tôi và kết quả trên cả nhiệm vụ mở và đóng. Nói chung, có những khoảng cách đáng kể giữa các mô hình mã nguồn mở và các mô hình thương mại. Mô tả chi tiết về các mô hình cơ sở có thể tìm thấy trong §A.1. Các mẫu lời nhắc cho mỗi nhiệm vụ có sẵn trong §B. Chúng tôi chạy tất cả các thí nghiệm sử dụng FlashAttention (Dao et al., 2022) trên một GPU NVIDIA A800 duy nhất. Đầu vào tài liệu được cắt ngắn từ bên phải.

5.1 CÁC MÔ HÌNH CƠ SỞ

Các Mô hình Thương mại (1) Claude-100k được phát triển bởi Anthropic, (2) GPT-4-32k, mô hình ngữ cảnh dài mạnh nhất của OpenAI, (3) Turbo-4k-0613 và (4) Turbo-16k-0613 là snapshot của GPT-3.5 từ ngày 13 tháng 6 năm 2023 có thể xử lý lên đến 4k/16k token đầu vào.

Các Mô hình Mã nguồn Mở (5) Llama1 (Touvron et al., 2023a), một mô hình mã nguồn mở được sử dụng rộng rãi được phát triển bởi Meta AI với độ dài đào tạo trước 2k, (6) Vicuna1.3 (Chiang et al., 2023), được tinh chỉnh trên shareGPT dựa trên Llama1, (7) Longchat-16k, phiên bản ngữ cảnh dài của Vicuna1.3 sử dụng PI, (8) Llama2, phiên bản tiếp theo của Llama với ngữ cảnh đào tạo trước 4k, (9) Llama2-chat, phiên bản được tinh chỉnh cho sử dụng hội thoại, (10) Llama2-NTK, mở rộng độ dài ngữ cảnh của Llama2-chat với NTK-aware RoPE, (11) Vicuna1.5-16k (Zheng et al., 2023), phiên bản ngữ cảnh dài của Llama2 sử dụng PI & ShareGPT (12) Longchat1.5-32k, phiên bản ngữ cảnh 32k của Llama2 sử dụng PI & ShareGPT. (13) Chatglm2-8k, phiên bản thứ hai của Chatglm (Du et al., 2022), (14) Chatglm2-32k, phiên bản độ dài ngữ cảnh 32k, (15) XGen-8k-inst (Nijkamp et al., 2023), một mô hình ngữ cảnh 8k được phát triển bởi salesforce (16) MPT-7B-StoryWriter-65k, dựa trên MPT-7B và ALiBi với độ dài ngữ cảnh 65k token trên một tập con của tập dữ liệu Books3.

Bộ Truy xuất Chúng tôi triển khai bộ truy xuất dense với OpenAI AdaEmbedding làm bộ truy xuất dense và BM25 làm bộ truy xuất sparse để trích xuất 4 phần tài liệu liên quan nhất được chia thành đoạn 1k, được cung cấp thêm làm ngữ cảnh để trả lời câu hỏi.

5.2 KẾT QUẢ CHÍNH

Hiệu suất của LCLMs trên các nhiệm vụ đóng được hiển thị trong Bảng 3. Đối với các nhiệm vụ mở, chúng tôi kiểm tra tập con 96 câu hỏi (Bảng 4) với đánh giá GPT-4. Kết quả từ các chỉ số n-gram trên tất cả các bộ kiểm tra và xếp hạng của LLMs có thể tìm thấy trong §A.3. Từ kết quả chính, chúng tôi có những quan sát sau. GPT-4-32k rõ ràng vượt trội hơn tất cả các mô hình khác với biên độ rất đáng kể, thiết lập SOTA trong các nhiệm vụ đóng L-Eval. Vẫn còn khoảng cách gần 20 điểm giữa các mô hình 16k mã nguồn mở tốt nhất và Turbo-16k. Đối với các nhiệm vụ mở, vì các văn bản đầu vào thường dài hơn và cần hiểu biết toàn cục về ngữ cảnh, Claude-100k, với độ dài ngữ cảnh dài nhất, vượt qua tất cả các mô hình cơ sở bao gồm GPT-4-32k. Mặc dù kết quả của các chỉ số n-gram cho thấy các LCLMs mã nguồn mở đã đạt hiệu suất gần với GPT-Turbo trên

7

--- TRANG 7 ---
bản thảo
Bảng 3: Kết quả đánh giá kiểm tra trên các nhiệm vụ đóng cho LCLMs hiện tại. Ret. cho biết liệu chúng tôi có sử dụng thuật toán dựa trên truy xuất cho mô hình cơ sở hay không. Tokens biểu thị số lượng token đầu vào tối đa mà chúng tôi cung cấp vào mô hình. ↓/↑ cho thấy sự giảm/tăng đáng kể về hiệu suất, so với việc sử dụng đối tác ngữ cảnh ngắn gốc. * cho thấy mô hình không được đào tạo thêm.

[THIS IS TABLE: A detailed performance comparison table showing various models and their performance across different tasks (Coursera, GSM, QuALITY, TOEFL, CodeU, SFiction) with metrics like accuracy scores and context lengths]

các nhiệm vụ mở, kết quả đánh giá từ cả thẩm phán LLM (Bảng 4) và con người (Bảng 5) tiết lộ rằng vẫn còn khoảng cách đáng kể giữa chúng. Hơn nữa, các phương pháp dựa trên truy xuất dựa trên Turbo-4k thiếu sót so với việc mã hóa toàn bộ ngữ cảnh (Turbo-16k), vì một số nhiệm vụ khó giải quyết thông qua truy xuất đơn giản.

Hình 4: Số lượng đầu ra không hợp lệ từ Llama2 và Turbo.

Tinh chỉnh dài hơn mang lại lợi ích cho các nhiệm vụ đóng nhưng thiếu sót trong các nhiệm vụ mở Trong Bảng 3, đối với các mô hình mã nguồn mở sử dụng nhúng vị trí được mở rộng, Longchat và Vicuan1.5-16k rõ ràng vượt trội hơn phiên bản gốc của chúng Vicuna-2k và Llama2-chat. Kết quả cho thấy tinh chỉnh thêm trên đầu vào dài hơn từ một mô hình có độ dài ngữ cảnh đào tạo trước ngắn thực sự có lợi cho mô hình hóa ngữ cảnh dài. Tuy nhiên, theo Bảng 4, không giống như kết quả trên các nhiệm vụ đóng, mô hình tốt nhất Vicuna1.5-13b-16k chỉ thắng Turbo-16k 34%, thấp hơn 8 điểm so với phiên bản ngắn của nó Llama2-13b. Llama2-13b-chat (Touvron et al., 2023a) vẫn là cơ sở mã nguồn mở mạnh nhất, cho thấy LCLMs hiện tại đơn giản dựa trên kỹ thuật nhúng vị trí được mở rộng có thể không đủ cho những nhiệm vụ tạo ra mở thử thách này. Dựa trên đánh giá con người của chúng tôi, chúng tôi thấy rằng mặc dù các kỹ thuật nhúng vị trí được mở rộng như NTK (LocalLLaMA, 2023b) hoặc PI (Sun et al., 2022) hiệu quả mở rộng độ dài ngữ cảnh của mô hình, các mô hình có xu hướng bị lạc khi đối mặt với token đầu vào dài và không thể tuân theo hướng dẫn. Chúng tôi phân loại những đầu ra này là "đầu ra không hợp lệ". Để điều tra hiệu suất mô hình trên các độ dài ngữ cảnh khác nhau, chúng tôi chia tập con 85 câu hỏi thành 2 phần: PHẦN-A chứa các mẫu có ít hơn 4k token, và PHẦN-B nhiều hơn 4k token. Chúng tôi so sánh số lượng đầu ra không hợp lệ từ Llama2/Vicuna1.5-16k và Turbo/Turbo-16k trong Hình 4. Kết quả cho thấy số lượng

8

--- TRANG 8 ---
bản thảo
Bảng 4: Trong việc so sánh các mô hình khác nhau với Turbo-16k-0613 trên các nhiệm vụ mở. Chúng tôi đánh giá các mô hình này trên tập con 96 câu hỏi sử dụng GPT-4 và hai tập con (85+96 câu hỏi) sử dụng GPT-3.5. Chúng tôi giảm thiên vị vị trí bằng cách hoán đổi các dự đoán được ghép cặp, vì vậy đánh giá viên GPT-4 được sử dụng trong 96×2 vòng đánh giá, trong khi đánh giá viên GPT3.5 được sử dụng trong 181×2 vòng

[THIS IS TABLE: A detailed comparison table showing model performance with columns for Model, Ret., Tokens, GPT-4 (wins, ties, win-rate %), GPT-3.5 (wins, ties, win-rate %), and R-L. The table includes various models from Claude1.3-100k to MPT-7b-65k with their respective performance metrics.]

đầu ra không hợp lệ từ Turbo-16k vẫn là một lượng rất nhỏ trên cả PHẦN-A và B trong khi đầu ra không hợp lệ từ Llama2-16k tăng mạnh trên các mẫu có đầu vào dài hơn. Do đó, LCLMs kém có khả năng tuân theo hướng dẫn trên các nhiệm vụ mở cho ngữ cảnh dài, so với các nhiệm vụ đóng, như trắc nghiệm. Một lý do có thể là corpus đào tạo hoặc SFT rất có khả năng chứa nhiều mẫu đào tạo với phong cách câu hỏi tương tự. Điều này mạnh mẽ tăng cường khả năng tuân theo hướng dẫn của chúng trên các nhiệm vụ đóng.

Hình 5: Kiểm tra khả năng truy xuất và khả năng lý luận với cơ sở NTK.

Hiệu suất trên các nhiệm vụ truy xuất trái ngược với các nhiệm vụ lý luận Các phương pháp nhúng vị trí NTK-aware phổ biến nhất tăng cơ sở 10.000 trong RoPE vanilla để thực hiện ngoại suy mà không cần tinh chỉnh thêm. Tuy nhiên, chúng tôi thấy rằng hiệu suất trên các nhiệm vụ truy xuất chủ đề không khớp với khả năng lý luận trên ngữ cảnh dài. Như có thể thấy từ Hình 5, khi chúng tôi tăng cơ sở từ 20.000 lên 160.000, có cải thiện liên tục trên truy xuất chủ đề. Tuy nhiên, hiệu suất trên các nhiệm vụ lý luận toán với các ví dụ dài thể hiện xu hướng hoàn toàn ngược lại, cho thấy rằng thật thử thách để mô hình duy trì khả năng lý luận khi tăng cơ sở. Ngược lại, hiệu suất trên các nhiệm vụ truy xuất dường như không bị ảnh hưởng sau khi cơ sở đạt 60.000. Chúng tôi có phân tích thêm trong §A.3, bao gồm kết quả đầy đủ của các chỉ số n-grams trên các nhiệm vụ mở, xếp hạng của LLMs hiện tại, nhúng vị trí NTK-aware và các hệ thống dựa trên truy xuất.

6 KẾT LUẬN

Tóm lại, benchmark L-Eval rất cần thiết được giới thiệu trong công trình này cung cấp một bộ nhiệm vụ và chỉ số đánh giá toàn diện để đánh giá khả năng của các mô hình ngôn ngữ ngữ cảnh dài. Chúng tôi đã kiểm tra hầu hết các LCLMs mã nguồn mở và các thí nghiệm chứng minh những cải thiện đầy hứa hẹn từ việc mở rộng độ dài ngữ cảnh và khoảng cách so với các mô hình thương mại. Phân tích của chúng tôi sử dụng L-Eval cung cấp những hiểu biết có giá trị về tình trạng hiện tại và hạn chế của LCLMs. Chúng tôi tin rằng với sự tập trung vào các tài liệu dài thực tế qua các lĩnh vực, L-Eval có thể phục vụ như một thử nghiệm thử thách để thúc đẩy tiến bộ trong mô hình hóa ngữ cảnh dài hơn.

9

[Tiếp tục dịch các trang còn lại...]
