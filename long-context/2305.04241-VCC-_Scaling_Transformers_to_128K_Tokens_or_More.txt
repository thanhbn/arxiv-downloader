# 2305.04241.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2305.04241.pdf
# File size: 1770664 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
VCC: Scaling Transformers to 128K Tokens or More
by Prioritizing Important Tokens
Zhanpeng Zeng
University of Wisconsin, Madison
zzeng38@wisc.eduCole Hawkins
AWS AI
colehawk@amazon.com
Mingyi Hong
University of Minnesota, Minneapolis
mhong@umn.eduAston Zhang
AWS AI
astonz@amazon.comNikolaos Pappas
AWS AI
nppappa@amazon.com
Vikas Singh
University of Wisconsin, Madison
vsingh@biostat.wisc.eduShuai Zheng
AWS AI
shzheng@amazon.com
Abstract
Transformers are central in modern natural language processing and computer
vision applications. Despite recent works devoted to reducing the quadratic cost
of such models (as a function of the sequence length), dealing with ultra long
sequences (e.g., with more than 16K tokens) remains challenging. Applications
such as answering questions based on a book or summarizing a scientific article are
inefficient or infeasible. Here, we propose to significantly improve the efficiency of
Transformers for ultra long sequences, by compressing the sequence into a much
smaller representation at each layer. Specifically, by exploiting the fact that in many
tasks, only a small subset of special tokens (we call VIP-tokens) are most relevant
to the final prediction, we propose a VIP-token centric compression (VCC) scheme
which selectively compresses the sequence based on their impact on approximating
the representation of the VIP-tokens. Compared with competitive baselines, our
algorithm is not only efficient (achieving more than 3×efficiency gain compared to
baselines on 4K and 16K lengths), but also offers competitive/better performance
on a large number of tasks. Further, we show that our algorithm scales to 128K
tokens (or more) while consistently offering accuracy improvement.
1 Introduction
103104105
Sequence Length050010001500Runtime (ms)
103104105
Sequence Length0102030Memory (GB)
Transformer Big Bird Longformer Ours
Figure 1: Model efficiency of processing one
sequence on an A100 as sequence length in-
creases (note logarithm xaxis).The Transformer [ 30] is a foundational model
for natural language processing (NLP) and com-
puter vision. It has shown remarkable performance
across NLP applications including machine trans-
lation [ 30], language inference [ 9], and summariza-
tion [ 13]. Transformers have also been success-
fully applied to various visual recognition tasks
and achieve impressive results [ 10,3,38]. Unfortu-
nately, the runtime/memory needs of Transformers
involve an unfavorable dependence on the input
sequence length, making the use of Transformers
for ultra-long sequence applications difficult. Therefore, many Transformers make use of strategies
Preprint. Under review.arXiv:2305.04241v2  [cs.CL]  27 May 2023

--- PAGE 2 ---
such as truncation to ensure that the input sentence length is at most 512, e.g., BERT, T5, and other
Transformer-based language models [ 33,21,26]. Unfortunately, such a truncation, and other related
strategies, inevitably results in loss of accuracy, the extent of which can vary from one task/dataset to
another. Consequently, improving the efficiency for longer input sequence length is a key focus of
many proposals. These developments are important milestones, and they have reduced the quadratic
dependency on sequence lengths to linear [ 5,24,32,2,35,37,36]. Now, many Transformer models
can now process samples with sequence lengths up to 4K (or 16K at most). Very recently, a few
reports of newer models being able to handle much longer sequences have appeared.
Rationale. It is natural to ask whether the ability to process longer sequences is worth the trouble.
The short answer is yes. Improved accuracy has been reported on long sequence tasks [ 2,35,13].
So, what is stopping us from harvesting even stronger gains in accuracy by feeding even longer
sequences to such models? Models such as Longformer [ 2] and Big Bird [ 35] become slow and
consume an excessive amount of memory as the sequence length keeps increasing. See Fig. 1 for
illustration. Why? The representation update of every token involves computing efficient attention to
the sequence and feed-forward network at each layer. This incurs a linear cost on sequence length and
is expensive for sequences much longer than 4K (or 16K) tokens. To endow the models the ability to
learn ultra-long range dependency, we need to lower this cost. What we describe in this paper is a
concrete step forward – based on certain task-specific assumptions which appear to generally hold,
we outline a formulation that works and delivers the expected improvements.
(1) Focus on what we need for a task: VIP-token centric compression (VCC). We hypothesize/find
that in many tasks where Transformers are effective, only a small subset of tokens (which we refer to
as VIP-tokens) are relevant to the final output (and accuracy) of a Transformer. If these tokens had
been identified somehow, we could preserve this information in its entirety and only incur a moderate
loss in performance. Now, conditioned onthese specific VIP-tokens, an aggressive compression
on the other non-VIP-tokens , can serve to reduce (and often, fully recover) the loss in performance
while dramatically decreasing the sequence length. This compression must leverage information
regarding the VIP-tokens, with the goal of improving the approximation of the representation of
the VIP-tokens. In other words, a high-fidelity approximation of the entire sequence is unnecessary.
Once this “selectively compressed” input passes through a Transformer layer, the output sequence is
decompressed to the original full sequence allowing the subsequent layers to access the full sequence.
(2) Specialized data structure for compression/decompression. A secondary, but important prac-
tical issue, is reducing the overhead when compressing/decompressing the input/output sequences
internally in the network. Ignoring this problem will impact efficiency. We give a simple but special-
ized data structure to maintain the hidden states of the intermediate layers, where the compression
can be easily accessed from the data structure, and explicit decompression can be avoid by updating
the data structure: the sequence is never fully materialized in intermediate layers.
Practical contributions. Apart from the algorithmic modules above, we show that despite an
aggressive compression of the input sequences, we achieve better/competitive performance on a broad
basket of long sequence experiments. Compared to baselines, we get much better runtime/memory
efficiency. We show that it is now practical to run standard Transformer models on sequences of
128K token lengths, with consistent performance benefits (and nosophisticated architecture changes).
2 Preliminaries
We review the Transformer layer, related work on efficient Transformers and define nota-
tions/simplifications. BOLD uppercase letters denote matrices, bold lower case letters denote
vectors, and regular lower case letters denote scalars.
Brief review of the Transformer Model. Fixnto be the sequence length and let dbe the embedding
dimension. Define an embedding matrix X∈Rn×dwhich gives the nfeature vector inputs for a
Transformer layer. The output of this Transformer layer, Xnew, is defined as
Xnew=β(α(X,X,X) +X) +α(X,X,X) +X (1)
where α(·,·,·)is multi-head attention (MHA) and β(·)is a feed-forward network (FFN). Layer norms
[1] are omitted to reduce clutter. Let the inputs to α(·,·,·)beQ,K,V∈Rn×dfor queries, keys, and
values. MHA is defined as:
α(Q,K,V) := cati=g
i=1
softmax (QW Q,iW⊤
K,iK⊤)VW V,i
W (2)
2

--- PAGE 3 ---
where gis the number of attention heads, {WQ,i,WK,i,WV,i}are trainable projections, and the
‘cat’ concatenates the outputs of multiple self-attention modules. We omit the biases for notational
simplicity. For ease of discussion, let us further simplify the above notation by assuming that g= 1,
and suppress WQ,1,WK,1,WV,1,Was well as the normalization in softmax: they will stillbe
estimated within the model (i.e., this module remains unchanged) but are tangential to the description
of our idea. With these simplifications, the α(·,·,·)can be expressed as:
α(Q,K,V) := exp( QK⊤)V. (3)
Letγ(·)be a placeholder for all heavy computations in the Transformer layer above:
γ(X) :=β(α(X,X,X) +X) +α(X,X,X). (4)
We can verify that the output of a Transformer block (parameters are suppressed to reduce clutter) is,
Xnew=γ(X) +X. (5)
A Transformer model consists of many such layers: the input of each layer is the output Xnewfrom
the previous layer. Let lbe the number of layers, then the overall complexity is O(ln2d+lnd2).
Efficient Transformers. Many efficient self-attention methods are available to reduce the O(ln2d)
cost. We list a few models noting that this list is not exhaustive. Performer [ 5], Random Feature
Attention [ 24], and Nyströmformer [ 32] propose different low rank approximations of the self-
attention matrices. Longformer [ 2] and Big Bird [ 35] describe global + local sparse attention.
Reformer [ 16] and YOSO [ 37] exploit locality sensitive hashing for approximating the self-attention
matrix. MRA attention [36] gives a multi-resolution approximation of the self-attention matrices.
Efficient Self-Attention does not scale well to ultra-long sequences . Existing self-attention
mechanisms often reduce the quadratic cost of MHA to linear. But so far, most experiments report
sequence lengths of up to 4K, with some exceptions [ 2,35,13]. Beyond 4K, the linear cost (on
n) for both computing efficient attentions and FFN makes the cost prohibitive, especially for large
models. For example, although LongT5 [ 13] can train on sequence lengths of up to 16K tokens with
an efficient self-attention and shows promising results for longer sequences, it is slower and needs a
sizable amount of compute (for example, see Fig. 1).
Other alternatives for sequence compression? Compressing input sequences for efficiency reasons
in Transformers is not a new idea. For example, [7] and [15] propose pyramid Transformer variants
that progressively compress the sequence as the layers grow deeper via pooling or core-set selection.
[22] proposes adaptively compressing the sequence based on the predicted semantic boundaries within
the sequence. [ 25] proposes compressing the fine-grained past activations to coarser memories. There
arethree key differences with our approach. First, all methods listed above are task agnostic . They
seek compressed/smaller representations to represent the original sequence well. Our formulation
places no emphasis on representing the original sequence, as long as information pertinent to the
VIP-tokens is preserved as much as possible. Second, once these methods compress the sequence,
the residual information is lost (for the deeper layers or the later time steps). Our entire approach is
predicated on avoiding this loss – we maintain access to the full sequence at each layer (via residual
connection at least). Lastly, some of these ideas often involve an n2dependence on the sequence
length in the initial stages of their formulation, making long sequence experiments problematic.
3 VIP-Token Centric Compression (VCC)
Our main goal is to reduce the dependency on n(but not by modifying Transformer internals).
To do this, we describe a scheme that compresses the input sequence of a Transformer layer and
decompresses the output sequence, resulting in a model whose complexity is O(lrd2+lr2d+
lrlog(nc)d+lrnpd+nd). Here, ris the size of the compressed sequence, npis the size of VIP-
tokens described shortly, and ncis the size of non-VIP/remaining tokens. So, we have np+nc=n
and assume np≪r≪n. (See complexity analysis in Appendix.)
Parsing the complexity term: Let us unpack the term to assess its relevance. The first two terms
O(lrd2+lr2d)give the cost for a Transformer, while the remaining terms are the overhead of
compression and decompression. The term O(lrlog(nc)d+lrnpd)is the overhead of compression
and updating our data structure at each layer. The O(nd)term pertains to pre-processing involving
converting the hidden states to our data structure and post-processing to recover the hidden states
3

--- PAGE 4 ---
from the data structure. Note that unlike the dependence on nfor vanilla Transformers, this O(nd)is
incurred only at the input/output stage of the Transformer, but notat any intermediate layers.
High level design choices. We use the standard Transformer layers with a standard feed-forward
network (which results in d2in the first term) and standard quadratic cost self-attention (which gives
ther2factor in the second term). Why? These choices help isolate the effect of incorporating their
efficient counterparts. The proposed algorithm operates on the input/output of each Transformer
layer leaving the Transformer module itself unchanged. Therefore, our goals are distinct from the
literature investigating efficient self-attentions and efficient feed-forward networks. This is because
one can replace these two vanilla modules with any other efficient alternatives to further reduce the
r2andd2terms directly. Despite these quadratic terms, our approach is faster than baselines (§4).
We will first describe our general idea, as shown in Fig. 2, which uses VIP-tokens to guide the
compression/decompression of the input/output of a Transformer layer so that it only needs to process
the compressed sequence (§3.1, §3.2). Then, we will discuss an instantiation of the compression
process, by adapting a multi-resolution analysis technique (§3.3). Lastly, we will introduce a data
structure which allows more efficient compression/decompression (§3.4).
3.1 Elevating the Importance of a Few Tokens: VIP-Tokens
Let us start with the simplest compression, which identifies a linear transformation S∈Rr×nwhich
acts on the input, resulting in a smaller representation SX∈Rr×d. Of course, a smaller rimplies that
more information about Xis lost. But we find that in many tasks, only the embedding representations
ofa few tokens drive the final prediction: we refer to these tokens as VIP-tokens .
Examples of VIP-tokens: Observe that only the embedding outputs of masked tokens in masked
language modeling [ 9] and the CLS token in sequence classification [ 9,10] are/is used for prediction.
In question answering, only the questions and possible answers associated with the questions are used
for prediction. It is important to note that the masked tokens, CLS tokens, and question tokens are (1)
defined by the tasks and (2)known to the model (although the embedding representation of these
tokens are unknown). These VIP-tokens can be viewed as a task or question that is given to the model.
The model can process the sequence with a specific goal in mind so that the model can skip/skim less
relevant segments. Our general principle involves choosing a set of tokens as the VIP-tokens that (1)
are important to the specific task goals and (2)easily pre-identifiable by the user.
Caveats. Not all important tokens can be pre-identified. For example, the tokens in the correct answer
span in answer span prediction are also important to the specific goals, but are difficult to pre-identify,
so only the question tokens (and not the answer tokens) are used as VIP-tokens. We assume that any
other tokens that is relevant for prediction should have high dependency with these VIP-tokens. For
example, the answer tokens should have high dependency (in self-attention) to the question tokens.
VIP-Token Centric Compression ()ScTransformer Layer (  )γVIP-tokens PRemaining tokens CResidual ConnectionResidual ConnectionDecompression ()S†cScC
Figure 2: Diagram that illustrates a Trans-
former layer with VIP-token centric se-
quence compression.VIP-tokens occupy the front seats. VIP-tokens can
occur anywhere within a sequence. But we can re-order
the sequence as well as the positional encodings so that
VIP-tokens are always at the head of sequence to make
analysis/implementation easier. With this layout, let
P∈Rnp×dbe the VIP-tokens and C∈Rnc×dbe the
non-VIP/remaining tokens, Xcan be expressed as
X=
P
C
(6)
This is possible since Transformer is permutation in-
variant when permuting positional encodings (embed-
dings or IDs) along with tokens. This re-ordering is
performed only once for the input of the Transformer model, then the outputs generated by the model
are rearranged to their original positions.
From the above discussion, it is clear that one needs to make sure that after compressing the input
tokens X, the VIP-tokens must still stay (more or less) the same, and the compression matrix S
must be VIP-token dependent . We hypothesize that such VIP-token dependent compression matrices
require a much smaller dimension r, compared to VIP-token agnostic compression matrices.
4

--- PAGE 5 ---
3.2 VIP-Token Centric Compression (VCC): An Initial Proposal
For a Transformer layer, let Xdenote its input matrix. Express the output of this layer as follows:
Xnew=S†γ(SX) +X (7)
where S∈Rr×nis acompression matrix compressing Xto a smaller representation and S†is the
pseudo inverse for decompression . With the layout in (6), we can write (7) as

Pnew
Cnew
=S†γ(S
P
C
) +
P
C
(8)
where PnewandCneware the new embeddings for PandC.
Always reserve seats for VIP-tokens. What is a useful structure of S? Since Pnewis the embedding
output for the VIP-tokens P, we want them to be fully preserved. To achieve this, we impose the
following structure on SandS†:
S=
Inp×np0
0 Sc
S†=Inp×np0
0 S†
c
. (9)
The rearrangement simply says that we will avoid compressing P. But rewriting it this way helps us
easily unpack (8) to check the desired functionality of Sc.
Prioritize information in VIP-tokens. Our goal is to ensure Pnewgenerated from the compressed
sequence in (8)will be similar to its counterpart from the uncompressed sequence. Let us check (8)
using the compression matrix Sdefined in (9) first. We see that
Inp×np0
0 S†
c
γ(
P
ScC
) =β(α(P,SX,SX) +P) +α(P,SX,SX)
S†
cβ(α(ScC,SX,SX) +ScC) +S†
cα(ScC,SX,SX)
.(10)
The orange color identifies terms where Pnewinteracts with other compression-related terms C
and/or Sc. We primarily care about Pnewin(8), so the first (orange) row in (10) is the main concern.
We see that Pnewonly depends on the compressed SXviaα(P,SX,SX). We can further unpack,
α(P,SX,SX) = exp( PX⊤S⊤)SX= exp( PP⊤)P+ exp( PC⊤S⊤
c)ScC. (11)
Again, α(P,SX,SX)depends on CandScvia the second (orange) term. Normalization in soft-
max is omitted for simplicity of discussion. This helps us focus on the key term that matters:
exp(PC⊤S⊤
c)ScC. As long as the following approximation using Scis good
exp(PC⊤S⊤
c)Sc≈exp(PC⊤), (12)
we will obtain a good approximation of Pnew. Our remaining task is to outline a scheme of finding a
compression matrix Scsuch that this criterion can be assured.
3.3 A Specific Instantiation via Multi-Resolution Compression
What should be the mechanics of our compression such that (12) holds? In general, to get Sc, we can
use any sensible data driven sketching idea which minimizes the error of (12). Doing so efficiently
needs a bit work; we describe the high level idea below and the low-level details are provided in
Appendix.
High level idea. Ideally, an efficient scheme for constructing Scshould operate as follows. If
some regions of the sequence Chave a negligible impact on (12) (via the orange terms above),
the procedure should compress the regions aggressively. If other regions are identified to have a
higher impact on (12) (again due to the orange terms above), the procedure should scan these regions
more carefully for a more delicate compression. This suggests that procedurally a coarse-to-fine
strategy may work. For example, multi-resolution analysis does help in approximating self-attention
matrices in Transformers [ 36], but the formulation in [ 36] might not be written as a form similar to
(12), making it incompatible with our design. Nonetheless, we derive an analogous form (details in
Appendix) that can be represented in a similar form as (12) and gives a strategy for obtaining ScC.
5

--- PAGE 6 ---
CScC
c81
[C]3
[C]4
[C]1
[C]2
[C]7
[C]8
[C]5
[C]6
c41
c42
c21
c22
c13
c14
P
P
c21
c42
c13
c14
Compression
Pthicker = higher attention score
Figure 3: Illustration of multi-resolution
compression. nc= 8. Purple line: com-
pute attention scores between Pand dif-
ferent segments. Green arrow: segment
with higher attention score is split into
two sub-segments. Accordingly, ScC=
c2
1c1
3c1
4c4
2⊤is constructed.Specifically, let us define a compressed representation
(via averaging) of the x-ths-length segment of sequence
C:cs
x∈Rd
cs
x:=1
sX
sx−s<i≤sx[C]i(13)
s∈ {k0, k1, k2,···, nc}assuming ncis a power of k
andx∈ {1,2,···, nc/s}.[·]irefers to the i-th row of
the input matrix. We fix the increment ratio k= 2 for
simplicity of discussion. The srepresents the resolu-
tion of the approximation: it represents the number of
non-VIP token embeddings being averaged into a vec-
torcs
x. Higher s(e.g., s= 8 inc8
1in Fig. 3) means
lower resolution and heavier compression of the corre-
sponding segment. The xrepresents the location of the
s-length segment within the sequence C. In our scheme,
we compress the sequence Cand use a set of cs
xfor some
selected s’s and x’s as the rows of the compressed ScC
as seen in Fig. 3. The sequence Cis broken into multiple segments of different lengths, then each
segment is compressed into a vector cs
x.
Procedurally, as shown in Fig. 3, our scheme starts with the heaviest compression and progressively
refines certain segments of Cguided by the VIP-tokens P. The scheme starts with the heaviest
compression that treats Cas anc-length segment and compresses it to a single cnc
1. Then, starting
withs=nc(root node), the procedure (1)computes the averaged attention scores between VIP-
tokens Pandcs
x’s for different x’s (averaged over all attention heads and all VIP-tokens; only one cnc
1
at level s=nc). We note that the attention scores are obtained by extracting attention matrices from
MHA module (2)of the current Transformer layer when using Pas queries and cs
x’s as keys. Then,
it(2)splits the s-length segments corresponding cs
x’s with higher averaged attention scores (one
segment is split in Fig. 3 but we might split more segments, again only one cnc
1at level s=nc) into
(s/2)-length sub-segments: the corresponding cs/2
x(13) of each sub-segment is computed for finer
representation. Then, at next level for s=nc/2, the same procedure proceeds. This process continues
until the sub-segments have length 1. We note that this procedure is guided by the VIP-tokens Pand
designed to maximally reduce the error of approximating (12). No additional learnable parameters
are introduced for this scheme. The technical details of this algorithm are less relevant for our overall
approach, but for interested readers, the details are discussed in the Appendix.
c81Δc15Δc16Δc23Δc17Δc18Δc24Δc42Δc11Δc12Δc21Δc13Δc14Δc22Δc41
Figure 4: Proposed data structure T(C)How good is this approximation? The output Pnew
(8)is well approximated since the approach preserves
the relevant components of Cthat have a high impact
on the output Pnew. Further, if the VIP-tokens Phave
high attention weights for some rows of C, then the
corresponding row in Cwill be approximated with
higher frequencies (less compressed). So, the output in
Cnew(8)for a subset of non-VIP tokens that have a higher dependency with the VIP-tokens will
have a better approximation than the others, as desired. This property is useful since some tokens
with unknown locations but manifesting a high dependency with the VIP-tokens can also relevant
to the final prediction of a Transformer model in some tasks. The answer in span-based question
answering tasks is one example, and our construction ensures that they will be approximated well too.
3.4 Efficient Data Structure for Compression/Decompression
By employing the procedure in §3.3 illustrated in Fig. 3, we can find the compressed ScCwith an
O(ncd+rnpd)cost at each layer. The main cost O(ncd)is due to computing cs
xdefined in (13) for
all resolution sand location xby using recursive relation from the bottom up:
c2s
x=1
2cs
2x−1+1
2cs
2x c1
x= [C]x (14)
We find that these steps could introduce a large overhead. Further, note that if we decompress (apply
S†to) the output of γfor compressed sequence as in (8), the cost is O(nd)since the number of
6

--- PAGE 7 ---
nonzero entries in S†isn(more details in Appendix). As a solution, we now introduce a data
structure T(·)for storing CandCnew, as shown in 4, which enables efficient computation of cs
xand
eliminates explicit decompression. We note that this data structure is only possible due to the specific
structure of ScCconstructed in §3.3. Specifically, T(C)stores cnc
1and∆cs
xdefined as
∆cs
x:=c2s
⌈x/2⌉−cs
x (15)
for every resolution s̸=ncand location x. Similarly, T(Cnew)stores (cnew)nc
1and∆(cnew)s
x
where (cnew)s
xand∆(cnew)s
xare defined similar to (13) and (15) but using Cnewinstead of C.
Then, given T(C), anycs
xcan be retrieved efficiently in O(log(nc)d)cost via recursion:
cs
x=c2s
⌈x/2⌉−∆cs
x=c4s
⌈x/4⌉−∆c2s
⌈x/2⌉−∆cs
x=··· (16)
The only reason we need decompression S†is that we need to obtain new representation Cnew
(no decompression for PnewsincePis uncompressed). Suppose we have T(Cnew), then we have
an alternative way of getting Cnewsimilar to (16) (note (cnew)1
x= [Cnew]x) without explicit
decompression. The key benefit of this data structure is that we can obtain T(Cnew)by changing
some nodes in T(C). This only needs updating O(r)nodes, and each update takes O(d)cost.
An example. We show a T(C)fornc= 8in Fig. 4. Let ScC=
c2
1c1
3c1
4c4
2⊤as in Fig. 3.
Since the segment c2
1is not split into sub-segments c1
1andc1
2, we have (details in Appendix):
(cnew)1
1−c1
1= (cnew)1
2−c1
2= (cnew)2
1−c2
1 (17)
By rearranging (17), we can verify that ∆(cnew)1
1,∆(cnew)1
2inT(Cnew)stays the same as
∆c1
1,∆c1
2inT(C)and thus do not need to be updated:
∆(cnew)1
1= (cnew)2
1−(cnew)1
1=c2
1−c1
1= ∆c1
1
∆(cnew)1
2= (cnew)2
1−(cnew)1
2=c2
1−c1
2= ∆c1
2(18)
Further, we can verify that only the green nodes in Fig. 4 will be updated. These nodes corresponds to
the nodes in Fig. 3 that have been traversed. In summary, for each row cs
xofScC(a leaf node in Fig.
3), only the node storing ∆(c)s
xand its ancestor nodes in T(C)must be updated, so the total number
of nodes (including their ancestors) being updated is O(r). Next, we can update the nodes as follows:
first, we get representations (cnew)2
1,(cnew)1
3,(cnew)1
4,(cnew)4
2by feeding ScCinto Transformer
layer (details in Appendix). At level s= 1, given (cnew)1
3and(cnew)1
4, we (1)compute (cnew)2
2
via(14), and then (2)compute ∆(cnew)1
3and∆(cnew)1
4via(15). The last two values are the new
values for ∆c1
3and∆c1
4inT(C). At level s= 2, given (cnew)1
2and(cnew)2
2computed at previous
level, we apply similar procedure to obtain (cnew)4
1,∆(cnew)1
2,∆(cnew)2
2, and the last two values
are used to update two nodes in T(C). It becomes apparent that each node update takes O(d)cost.
Putting togather: the complexity of modifying T(C)toT(Cnew)isO(rd). The detailed algorithm
and complexity analysis are described in Appendix.
By maintaining this data structure, we never need to materialize the entire CorCnew in any
intermediate layer, but instead we use (16) to construct the rows of ScCand perform updates to T(C)
to obtain Cnew(represented as T(Cnew)) at each intermediate layer. At the output of a Transformer,
Cnewis materialized from T(Cnew)at aO(ncd)cost via the recursion (16) from the bottom up.
4 Experiments
We perform a broad set of experiments to empirically evaluate the performance of our proposed
compression. We evaluate our method on both encoder-only and encoder-decoder architecture types.
We compare our method with baselines on a large list of question answering and summarization tasks,
where we found long sequences occur most frequently. Then, we study the model performance of
scaling to ultra long sequences enabled by our method. Since efficiency is the focus of the efficient
baselines and our work, we include runtime efficiency (of a single sequence) in millisecond in each
table. (See hyperparameters/dataset statistics in Appendix.)
For ease of implementation and hyperparameter selection, we restrict the rows of ScCto have exactly
two resolutions for experiments. Specifically, for a pre-defined increment ratio k, we split and refine
all segments cs
xwiths > k tok-length sub-segments, and select h(pre-defined) k-length segments to
7

--- PAGE 8 ---
further split to 1-length sub-segments. So, the rows of ScCwould consist of (nc/k−h)ofck
xandhk
ofc1
xfor some x. Further, we found a few layers of standard Transformer layers to pre-process tokens
helps the performance. Therefore, in the initial stage of a Transformer, we segment input sequence
into multiple 512-length segments. For each segment, we use vanilla computation in the first 4 layers
(for base models and 6 layers for larger models) of a Transformer. Then, for the remaining layers,
segments are concatenated back into one sequence and processed using our proposed compression.
There is no communication among any segments, so the initial stage is used just for getting a
reasonable representation for the compression to operate on. To simplify the implementation, we
only use the proposed compression in the encoder, and use the vanilla computation in the decoder of
encoder-decoder models. We note that our method might be applied to the decoder (more details in
Appendix).
Table 1: Dev set results for encoder-only models.
Method Size Length HotpotQA QuALITY WikiHop
Time EM F1 Time Accuracy Time Accuracy
RoBERTa base 512 19.9 35.1 44.9 21.2 39.0 19.6 67.6
RoBERTa base 4K 422.3 62.2 76.1 403.2 39.5 414.1 75.2
Big Bird base 4K 297.9 59.5 73.2 307.0 38.5 293.3 74.5
Longformer base 4K 371.0 59.9 73.6 368.0 27.9 369.7 74.3
MRA Attention base 4K 203.5 63.4 77.0 200.5 38.7 199.2 76.1
Ours base 4K 114.6 60.9 74.6 126.4 39.6 108.0 75.9
Ours* base 4K 114.6 61.4 75.0 125.7 39.5 108.0 76.1
Ours* large 4K 285.8 66.7 80.0 390.8 41.8 394.3 79.6Encoder-Only Models . For encoder-only archi-
tecture, we compare our method with RoBERTa
[21] and three strong baselines: Longformer [ 2],
Big Bird [ 35], and MRA Attention [ 36]. We
first pretrain a RoBERTa model using masked
language modeling task, then for each method,
we perform continuous pretraining from the
RoBERTa checkpoint to expend the positional
embeddings to 4K length and adjust model pa-
rameters to adapt approximations used in effi-
cient baselines and our method. We verify that our proposed method can be integrated into a pretrained
Transformer with some continuous pretraining. But we note that the amount of reduction in log
perplexity for our method ( −0.114) during pre-training is much larger than Longformer ( −0.017)
and Big Bird ( −0.025) from 50K steps to 250K steps. The continuous pretraining for these baselines
might have saturated since only the self-attention is approximated while our method might require
more pretraining to adjust the parameters for more aggressive approximation. So, we run a larger
scale pretraining for our method; downstream results are in Tab. 1 and Fig. 5, denoted with *. We
use HotpotQA [ 34], QuALITY [ 23], and WikiHop [ 31] to assess the language models. HotpotQA is
an answer span extraction task, while QuALITY and WikiHop are multi-choice question answering
tasks. We set questions and multi-choice answers (for QuALITY and WikiHop) as VIP-tokens.
1022×1026×102
Runtime (ms)6971737577Accuracy
RoBERTa
Big Bird
Longformer
Ours
Ours*
Figure 5: Model runtime vs Wiki-
Hop dev accuracy when using different
model specific hyperparametersAs shown in Tab. 1, we verify that our method is consistently
better compared to Longformer and Big Bird. Our method
obtains better accuracy in QuALITY and WikiHop com-
pared to 4K length RoBERTa model, but it is a bit worse than
4k length RoBERTa model on HotpotQA. More pretraining
helps close the gap. We also use WikiHop to experiment
with method specific hyperparameters (such as block size in
Big Bird, window size in Longformer, and compression size
rin our method). As shown in Fig. 5, our runtime efficiency
frontier is consistently better than the baselines. The key
takeaway is that our method has a much better runtime effi-
ciency than baselines that have the same sequence length without sacrificing its model performance.
Further, we note that our method can be scaled to larger models for accuracy improvement.
Encoder-Decoder Models . We compare our method with T5 [ 26], LongT5 [ 13], and LED [ 2].
We use the public pretrained checkpoints for baselines. The pretrained models for our method are
obtained by doing continuous pretraining from the public T5 checkpoints using T5’s pretraining task
[26]. We note that LED-base has 6 encoder/decoder layers compared to 12 encoder/decoder layers in
base models of other methods, its accuracy is usually lower. So, LED is evaluated in limited tasks. We
use HotpotQA [ 34], WikiHop [ 31], CNN/Dailymail [ 28], MediaSum [ 40], Arxiv [ 6], and GovReport
[14], SummScreenFD [ 4], QMSum [ 39], NarrativeQA [ 18], Qasper [ 8], QuALITY [ 23], ContractNLI
[20] from SCROLLS benchmark [ 29] to assess the language models. For question answering tasks,
we set questions and multi-choice answers (for QuALITY and WikiHop) as VIP-tokens in our method.
For query-based summarization, such as QMSum, we use the query as VIP-tokens in our method. For
general summartization tasks, we prepend a “summarize:” in each instance and use it as VIP-tokens
in our method. Our method achieves matching or better performance in most tasks compared to T5,
8

--- PAGE 9 ---
Table 2: Dev set results for encoder-decoder models. The left / right values of runtime columns are
the runtime for the entire model / the encoder.
Method Size # Param Length WikiHop HotpotQA CNN/Dailymail MediaSum
Runtime EM F1 Runtime EM F1 Runtime R-1 R-2 R-L Runtime R-1 R-2 R-L
T5 base 223M 512 25.7 / 20.5 66.7 69.1 26.3 / 20.5 34.1 44.4 40.0 / 20.5 43.3 20.5 40.4 39.9 / 20.5 30.7 14.5 28.1
T5 base 223M 4K 594.3 / 553.7 76.2 78.1 594.3 / 550.6 64.2 77.5 614.4 / 549.4 43.8 20.9 41.0 613.5 / 552.9 34.9 17.2 31.9
LongT5 base 248M 4K 270.7 / 233.9 72.7 74.8 271.3 / 233.7 62.3 75.7 291.6 / 234.9 43.3 20.6 40.5 287.3 / 229.5 34.9 17.3 32.0
LED base 162M 4K 236.6 / 222.9 70.0 72.4 237.4 / 222.9 55.1 67.9 249.4 / 221.8 43.3 20.0 40.5 - / - - - -
Ours base 223M 4K 181.7 / 148.1 76.7 78.4 155.4 / 127.4 64.5 77.7 195.8 / 139.9 43.6 20.7 40.7 196.7 / 140.2 34.8 17.3 31.9
T5 large 738M 512 83.5 / 67.0 69.1 71.4 84.1 / 67.0 36.9 47.8 124.6 / 67.0 43.8 20.7 40.9 124.5 / 67.0 31.9 15.5 29.1
T5 large 738M 4K 1738.7 / 1601.0 79.1 80.7 1598.1 / 1598.1 68.0 81.3 1824.8 / 1600.4 44.3 21.0 41.4 - / - - - -
Ours large 738M 4K 561.4 / 460.6 79.0 80.6 485.3 / 382.8 67.8 81.0 608.1 / 433.8 44.4 21.4 41.5 609.7 / 434.4 35.8 18.2 32.8
Ours 3b 3B 4K 1821.5 / 1441.2 80.8 82.3 1547.7 / 1197.1 70.2 83.2 1930.7 / 1364.8 44.8 21.5 41.9 1930.7 / 1364.8 36.3 18.5 33.3
Method Size # Param Length Qasper QuALITY Arxiv SummScreenFD
Runtime EM F1 Runtime EM F1 Runtime R-1 R-2 R-L Runtime R-1 R-2 R-L
T5 base 223M 512 31.8 / 20.5 10.8 16.4 29.3 / 20.5 33.6 47.3 59.0 / 20.5 28.9 8.6 25.6 59.1 / 20.5 27.0 4.8 23.5
T5 base 223M 4K 608.2 / 551.7 13.2 29.1 596.3 / 551.2 34.7 47.4 645.4 / 549.1 44.4 18.4 39.9 647.9 / 551.1 31.6 6.8 27.6
LongT5 base 248M 16K 1628.5 / 1421.3 16.2 33.4 1633.1 / 1439.7 35.8 48.5 1699.7 / 1370.4 48.5 21.7 43.7 1763.4 / 1427.8 33.1 7.3 28.5
LED base 162M 16K - / - - - - / - - - 1055.8 / 923.6 47.8 20.6 43.2 - / - - - -
Ours base 223M 16K 538.3 / 391.6 16.0 30.8 557.1 / 419.2 36.5 48.7 672.8 / 392.1 48.5 21.4 43.9 670.5 / 390.9 33.1 7.3 28.6
T5 large 738M 512 101.9 / 66.4 11.3 17.0 95.8 / 67.1 35.3 49.0 182.2 / 67.1 30.5 9.1 27.1 180.9 / 66.5 28.3 4.9 24.9
T5 large 738M 4K - / - - - 1760.5 / 1596.4 37.8 50.5 1901.5 / 1598.8 46.0 19.4 41.4 - / - - - -
Ours large 738M 16K 1679.6 / 1120.2 16.3 33.7 1753.6 / 1210.7 40.3 52.5 1959.1 / 1111.0 49.5 22.2 44.7 1957.1 / 1109.2 34.3 7.6 29.6
Ours 3b 3B 16K 6165.4 / 4637.3 19.0 38.2 6398.8 / 4962.7 45.2 56.0 7676.3 / 4642.2 49.8 22.4 45.0 7641.5 / 4631.3 34.7 7.8 30.1
Method Size # Param Length ContractNLI NarrativeQA GovReport QMSum
Runtime EM F1 Runtime EM F1 Runtime R-1 R-2 R-L Runtime R-1 R-2 R-L
T5 base 223M 512 24.0 / 20.5 73.5 73.5 26.8 / 20.5 2.0 11.3 59.1 / 20.5 40.5 14.8 38.2 43.5 / 20.5 30.2 8.0 26.5
T5 base 223M 4K 579.0 / 551.6 86.8 86.8 593.4 / 547.6 3.8 13.3 648.3 / 551.5 54.0 25.2 51.4 620.2 / 551.5 31.1 8.2 27.4
LongT5 base 248M 16K 1564.2 / 1462.5 85.1 85.1 1541.7 / 1370.2 5.2 15.6 1726.4 / 1387.7 55.8 27.9 53.2 1721.4 / 1450.7 35.7 11.7 31.4
Ours base 223M 16K 484.2 / 393.1 87.0 87.0 518.2 / 394.4 5.0 15.8 674.0 / 391.6 55.2 27.1 52.6 623.1 / 396.5 31.8 8.8 27.9
T5 large 738M 512 78.1 / 67.1 74.3 74.3 - / - - - 180.9 / 67.0 43.3 16.2 41.1 136.4 / 67.1 31.7 8.1 27.6
T5 large 738M 4K 1702.4 / 1601.2 87.2 87.2 - / - - - - / - - - - - / - - - -
Ours large 738M 16K 1440.6 / 1122.6 87.8 87.8 1551.7 / 1133.9 6.6 18.7 1955.5 / 1113.8 56.3 28.0 53.8 1816.4 / 1134.6 34.8 10.4 30.7
Ours 3b 3B 16K 5850.2 / 4665.9 88.5 88.5 6055.4 / 4659.4 8.2 21.2 7668.2 / 4642.7 56.9 28.5 54.3 7146.7 / 4655.6 35.7 10.9 31.1
LongT5, and LED with much higher efficiency (see Tab. 2). Further, the performance monotonically
increases with the model size, so our method can scale to larger models.
Table 3: Dev results of NarrativeQA
on base model when scaling sequence
length from 16K to 128K.
Length Runtime (ms) k h EM F1
16K 518.2 / 394.4 / 162.4 16 90 5.9 16.6
32K 946.8 / 671.6 / 212.6 32 55 6.6 17.5
32K 1027.9 / 751.0 / 298.0 16 90 6.4 17.5
64K 1848.7 / 1177.2 / 254.8 64 30 7.2 18.4
64K 2244.8 / 1574.2 / 659.4 16 90 7.5 19.3
128K 6267.8 / 5125.9 / 1902.2 16 90 8.0 19.6Scaling to Longer Sequences . The prior experiments
limit the sequence length to at most 4K or 16K since the
baselines can only be scaled up to these lengths. However,
our method can be scaled to much longer sequences. We
note that NarrativeQA [ 18] is an ideal testbed as shown
in dataset statistics in Appendix. The results are shown in
Tab. 3. The left / middle / right values of runtime column
are for the entire model / the encoder / the last 8 layers (out
of 12 layers) that uses our compression. The performance
monotonically increases as sequence length increases. We
note that for sequence length 64K, the performance of model with k= 64 is lower than the model
withk= 16 . We suspect that since the results are finetuned from the same model that is pretrained
withk= 16 , the large gap between the two different k’s may have a negative impact on finetuning
performance. Nevertheless, the performance is still higher than 32K length models.
Why focus on 4K - 128K lengths? We believe that the computation required by full Transformers
at processing shorter sequences is not an efficiency bottleneck. As a result, we did not profile
the performance of our method for smaller length sequences, since the standard Transformers are
sufficiently fast in this case. Further, while our model can be applied to shorter sequences, we suspect
that for shorter sequences, there might be less irrelevant information for VIP-tokens. So compressing
the irrelevant information will not offer a meaningful speed up. This is a limitation of our method as
the compression works better when there is more compressible information. We have only pushed
the sequence lengths to 128K since this length was sufficient to cover a majority of sequence lengths
encountered in long sequence tasks (for example, our model is able to process an entire book at once).
Limitations. Our method assumes that in many tasks, a subset of tokens are disproportionately
responsible for the model prediction, the remaining non-VIP-tokens may play a role but are less
critical. Our method excels at specifically such tasks by selectively locating relevant information in
9

--- PAGE 10 ---
the sequence for given VIP-tokens. As the experiments show, this choice is effective in many cases
but this behavior is not universal. Occasionally, an embedding is pre-computed which must then
serve multiple tasks concurrently, e.g., both text retrieval and natural language inference. In this case,
if we do not know the tasks beforehand, VIP-token selection cannot be meaningfully performed.
5 Conclusions
We propose a VIP-token centric sequence compression method to compress/decompress the in-
put/output sequences of Transformer layers thereby reducing the complexity dependency on the
sequence length nwithout sacrificing the model accuracy. Specifically, we design the compression
Our empirical evaluation shows that our method can be directly incorporated into existing pretrained
models with some additional training. Also, it often has much higher efficiency compared to baselines
with the same sequence length while offering better or competitive model accuracy. For future work,
we believe that extending our method to the decoder of the encoder-decoder models will further boost
the efficiency of Transformers while maintaining similar model performance.
References
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[2]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150 , 2020.
[3]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on
Computer Vision , pages 213–229. Springer, 2020.
[4]Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. SummScreen: A dataset for
abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) , pages 8602–8615, Dublin,
Ireland, May 2022. Association for Computational Linguistics.
[5]Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with
performers. In International Conference on Learning Representations (ICLR) , 2021.
[6]Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang,
and Nazli Goharian. A discourse-aware attention model for abstractive summarization of
long documents. Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short
Papers) , 2018.
[7]Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequen-
tial redundancy for efficient language processing. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.
Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33,
pages 4271–4282. Curran Associates, Inc., 2020.
[8]Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset
of information-seeking questions and answers anchored in research papers. In Proceedings
of the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages 4599–4610, Online, June 2021. Association
for Computational Linguistics.
[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training
of deep bidirectional transformers for language understanding. In Conference of the North
American Chapter of the Association for Computational Linguistics (NAACL) , 2019.
10

--- PAGE 11 ---
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on Learning Representations (ICLR) , 2021.
[11] Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-
scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings
of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1074–1084,
Florence, Italy, July 2019. Association for Computational Linguistics.
[12] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:
An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 ,
2020.
[13] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung,
and Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of
the Association for Computational Linguistics: NAACL 2022 , pages 724–736, Seattle, United
States, July 2022. Association for Computational Linguistics.
[14] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for
long document summarization. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies ,
pages 1419–1436, Online, June 2021. Association for Computational Linguistics.
[15] Xin Huang, Ashish Khetan, Rene Bidart, and Zohar Karnin. Pyramid-BERT: Reducing com-
plexity via successive core-set based token selection. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8798–8817,
Dublin, Ireland, May 2022. Association for Computational Linguistics.
[16] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In
International Conference on Learning Representations (ICLR) , 2020.
[17] Bryan Klimt and Yiming Yang. The enron corpus: A new dataset for email classification
research. In Proceedings of the 15th European Conference on Machine Learning , ECML’04,
page 217–226, Berlin, Heidelberg, 2004. Springer-Verlag.
[18] Tomáš Ko ˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gá-
bor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge.
Transactions of the Association for Computational Linguistics , 6:317–328, 2018.
[19] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings
of Machine Translation Summit X: Papers , pages 79–86, Phuket, Thailand, September 13-15
2005.
[20] Yuta Koreeda and Christopher Manning. ContractNLI: A dataset for document-level natural
language inference for contracts. In Findings of the Association for Computational Linguis-
tics: EMNLP 2021 , pages 1907–1919, Punta Cana, Dominican Republic, November 2021.
Association for Computational Linguistics.
[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 , 2019.
[22] Piotr Nawrot, Jan Chorowski, Adrian Ła ´ncucki, and Edoardo M. Ponti. Efficient transformers
with dynamic token pooling, 2022.
[23] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,
Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:
Question answering with long input texts, yes! In Proceedings of the 2022 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies , pages 5336–5358, Seattle, United States, July 2022. Association for
Computational Linguistics.
11

--- PAGE 12 ---
[24] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.
Random feature attention. In International Conference on Learning Representations (ICLR) ,
2021.
[25] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.
Compressive transformers for long-range sequence modelling. In International Conference on
Learning Representations , 2020.
[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020.
[27] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical
reasoning abilities of neural models. In International Conference on Learning Representations ,
2019.
[28] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with
pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages 1073–1083, Vancouver, Canada,
July 2017. Association for Computational Linguistics.
[29] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan
Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized comparison over
long language sequences. In EMNLP , 2022.
[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems (NeurIPS) , 2017.
[31] Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop
reading comprehension across documents. Transactions of the Association for Computational
Linguistics , 6:287–302, 2018.
[32] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li,
and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention.
InProceedings of the AAAI Conference on Artificial Intelligence , 2021.
[33] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
[34] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhut-
dinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop
question answering. In Conference on Empirical Methods in Natural Language Processing
(EMNLP) , 2018.
[35] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed.
Big bird: Transformers for longer sequences. In Advances in Neural Information Processing
Systems (NeurIPS) , 2020.
[36] Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh. Multi resolution
analysis (MRA) for approximate self-attention. In Kamalika Chaudhuri, Stefanie Jegelka,
Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th Inter-
national Conference on Machine Learning , volume 162 of Proceedings of Machine Learning
Research , pages 25955–25972. PMLR, 17–23 Jul 2022.
[37] Zhanpeng Zeng, Yunyang Xiong, Sathya Ravi, Shailesh Acharya, Glenn M Fung, and Vikas
Singh. You only sample (almost) once: Linear cost self-attention via bernoulli sampling. In
International Conference on Machine Learning (ICML) , 2021.
12

--- PAGE 13 ---
[38] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei
Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a
sequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840 , 2020.
[39] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan
Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A
new benchmark for query-based multi-domain meeting summarization. In Proceedings of
the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages 5905–5921, Online, June 2021. Association
for Computational Linguistics.
[40] Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. Mediasum: A large-scale media
interview dataset for dialogue summarization. arXiv preprint arXiv:2103.06410 , 2021.
13

--- PAGE 14 ---
6 Appendix
6.1 Definition of Notations
Table 4: Major notations used
Notation Description
l number of layers of a Transformer model
n number of tokens of an input sequence
d model embedding dimension
np number of VIP-tokens
nc number of non-VIP/remaining tokens, so np+nc=n
r length of a compressed sequence
α(·,·,·) multi-head attention taking three inputs for query/key/value embeddings
β(·) two-layer feed-forward network
γ(·) function representing all heavy computation of a Transformer layer
X embedding matrix representing a input sequence
P embedding matrix representing the VIP-tokens
C embedding matrix representing the non-VIP/remaining tokens
Xnew updated embedding matrix of a input sequence, the output of a Transformer layer
Pnew updated embedding matrix representing the VIP-tokens
Cnew updated embedding matrix representing the non-VIP/remaining tokens
S compression matrix
Sc compression submatrix for the non-VIP/remaining tokens
S†decompression matrix
S†
c decompression submatrix for the non-VIP/remaining tokens
s resolution of approximation, represents the number of non-VIP token embeddings being averaged
x location of s-length segment with the original sequence
k increment ratio of resolution, s∈ {1, k, k2, k3,···, nc}
h number of k-length segments are split into 1-length sub-segments, used for experiments
hs number of s-length segments are split into shorter sub-segments, used in Appendix discussion
J set of components bs
xthat is constructed via Alg. 1, the elements are the rows of Sc
bs
x components used for 1-D wavelet transform of scaling sand translation x
cs
x local average of x-ths-length segment of sequence C
(cnew)s
x local average of x-ths-length segment of sequence Cnew
T(·) data structure for storing the input sequence CorCnew
∆cs
x state stored in T(C)defined as ∆cs
x:=c2s
⌈x/2⌉−cs
x
∆(cnew)s
xstate stored in T(Cnew)defined as ∆(cnew)s
x:= (cnew)2s
⌈x/2⌉−(cnew)s
x
[·]i i-th entry/row of the input vector/matrix
[·]i,j (i, j)-th entry of the input matrix
We provide a table 4 of notations that are used for more than once so that the readers can refer to
their definition easily.
6.2 Details of Multi-Resolution Compression
We describe the omitted technical details of a modified formulation of [ 36] to construct ScCand
corresponding Scsatisfying good approximation of
exp(PC⊤S⊤
c)Sc≈exp(PC⊤). (19)
Before diving into the technical details of constructing Sc, we introduce some notations and tools
that will be used later. We use [·]ito refer the i-th entry/row of the input vector/matrix and [·]i,jto
refer the (i, j)-th entry of the input matrix. BOLD uppercase letters denote matrices, bold lower case
letters denote vectors, and regular lower case letters denote scalars. Let vibe a vector, when we write
a matrix of form
[v1v2···vm], (20)
we treat vias a column of the matrix. When we write a matrix of form

v1
v2
···
vm
, (21)
we treat vias a row of the matrix.
14

--- PAGE 15 ---
6.2.1 Basic Problem Setup
sx−ss1s
Figure 6: Visualization of bs
xfor some scaling
sand translation x. The y axis for different
plots are not the same.Letbs
x∈Rncbe a multi-resolution component de-
fined as
[bs
x]i:=1
sifsx−s < i≤sx
0otherwise(22)
fors∈ {k0, k1, k2,···, nc}assuming ncis a power
ofk(we assume k= 2 for simiplicity). Here, s
andxrepresent the scaling and translation (similar
to the concepts in wavelet basis) of the component,
respectively. Fig. 6 is the visualization of bs
x.
Then, any 1D signal f∈Rnccan be represented as
a linear combination of bs
x:
f=X
s,xcs
xbs
x (23)
where cs
xare the coefficients for the linear combination. For a signal with multi-resolution structure
(that is, signal has high frequency in some regions and has low frequency in other regions), we can
find an approximation ˆf∗that can be expressed as a sparse linear combination where most coefficients
are zeros, as shown in Fig. 7.
f≈ˆf∗:=X
bsx∈Jcs
xbs
x (24)
We denote Jas the set of major components bs
xcorresponding to the large coefficients, that is,
J:={bs
x| |cs
x|being large }. Since the set of all possible bs
xis an over-complete dictionary, there
are multiple possible linear combinations. To reduce the search space of the best set J, we place a
mild restriction on the set J:
X
bsx∈Jbs
x

i̸= 0∀i ⟨bs
x,bs′
x′⟩= 0∀bs
x,bs′
x′∈ J,bs
x̸=bs′
x′ (25)
The conditions state that each entry of signal fis included in the support region of exactly one
component in J. With these tools, we will first describe the approximation when Jis given, then
discuss how the approximation connects the set Jto our target ScandScC. Finally, we will discuss
how to construct this J.
0.00.5f(x)signal
0 100 200 300 400 500
x0.00.5f(x)approx
Figure 7: An example of approximating an 1D signal using a truncated wavelet transform with
components defined in (22). It uses a set Jof size 79to represent a signal in R512.
6.2.2 Plugging Our Problem into the Setup
A recent result shows that the self-attention matrix exp(XX⊤)has the multi-resolution structure
discussed above [ 36]. Since exp(PC⊤)is a sub-matrix of exp(XX⊤), we conjecture that the multi-
resolution structure also holds in exp(PC⊤). As a result, we can find a sparse combination of bs
xto
represent rows of exp(PC⊤).
Claim 6.1. Given the set Jsatisfying restriction (25), we can define an approximation of the i-th
row of exp(PC⊤)similar to (24) as illustrated in Fig. 7
h\exp(PC⊤)∗i
i:=X
bsx∈Jcs
xbs
x (26)
15

--- PAGE 16 ---
where cs
xis the optimal solution that minimizes

exp(PC⊤)
i−h\exp(PC⊤)∗i
i2
2(27)
Then, the approximation can be written as:
h\exp(PC⊤)∗i
i,j=⟨
exp(PC⊤)
i,bs
x⟩ (28)
where bs
x∈ J is the component that is supported on j(a.k.a. [bs
x]j̸= 0and there is exactly one
bs
x∈ J satisfy this condition due to restriction (25)).
Proof. IfJis given, let B∈Rnc×|J|be a matrix whose columns are elements bs
x∈ J and let
c∈R|J|be a vector whose entries are the corresponding cs
x:
B:=
bs1x1bs2x2···bs|J|
x|J|
c:=
cs1x1cs2x2···cs|J|
x|J|⊤(29)
then the approximation can be expressed as
h\exp(PC⊤)∗i
i=X
bsx∈Jcs
xbs
x=Bc (30)
If we solve for
c:= arg min
β
exp(PC⊤)
i−Bβ (31)
then
c= (B⊤B)−1B⊤
exp(PC⊤)
i(32)
Due to the restriction (25), the columns of Bare orthogonal, so B⊤Bis a diagonal matrix:
B⊤B=
1/s1
1/s2
···
1/s|J|
 (33)
We can also write down B⊤
exp(PC⊤)
i
B⊤
exp(PC⊤)
i=
⟨
exp(PC⊤)
i,bs1x1⟩
⟨
exp(PC⊤)
i,bs2x2⟩
···
⟨
exp(PC⊤)
i,bs|J|
x|J|⟩
(34)
Putting everything together, we have
Bc=
s1bs1x1s2bs2x2···s|J|bs|J|
x|J|
⟨
exp(PC⊤)
i,bs1x1⟩
⟨
exp(PC⊤)
i,bs2x2⟩
···
⟨
exp(PC⊤)
i,bs|J|
x|J|⟩
(35)
We note that sbs
xsimply re-scale the entry of bs
xsuch that any non-zero entry becomes 1. Then, let
us consider j-th entry of Bc. Due to the restriction (25), we have exactly one bs
x∈ J whose support
region contains j, so the j-th row of the first matrix at the right hand side of (35) contains exactly a 1
and the remaining entries are 0. Therefore, we have
h\exp(PC⊤)∗i
i,j= [Bc]j=⟨
exp(PC⊤)
i,bs
x⟩ (36)
where bs
x∈ J is the component that is supported on j, which concludes our proof.
16

--- PAGE 17 ---
6.2.3 Efficient Approximation
We note that computing (28) for all jwould require access to the entire
exp(PC⊤)
i. We exploit
the same strategy as described in [ 36], so the exponential of inner product is used as an approximation
to inner product of exponential.
h\exp(PC⊤)i
i,j:= exp( ⟨
PC⊤
i,bs
x⟩) (37)
We note that ⟨
PC⊤
i,bs
x⟩is the local average of the support region of bs
x, which is also the x-th
s-length segment of sequence
PC⊤
i:
⟨[PC⊤]i,bs
x⟩=1
sX
[bsx]j̸=0
PC⊤
i,j(38)
By using some arithmetic manipulations, (37) can be efficiently computed
exp(⟨[PC⊤]i,bs
x⟩) = exp(1
sX
[bsx]j̸=0
PC⊤
i,j) = exp(1
sX
(bsx)j̸=0⟨[P]i,[C]j⟩)
= exp( ⟨[P]i,1
sX
[bsx]j̸=0[C]j⟩) = exp( ⟨[P]i,cs
x⟩)(39)
where cs
xis defined in the main text:
cs
x:=1
sX
sx−s<i≤sx[C]i=bs
xC(40)
We note that cs
xis the local average of the x-ths-length segment of sequence C. The cs
xcan be
efficiently computed via
c2s
x=1
2cs
2x−1+1
2cs
2x c1
x= [C]x(41)
Claim 6.2. Given the set Jsatisfying restriction (25), letScbe be a matrix whose rows are elements
bs
x∈ J
Sc=
bs1x1bs2x2···
bs|J|
x|J|
 D=
s1
s2
···
s|J|
 (42)
Then,
exp(PC⊤S⊤
c)DSc=\exp(PC⊤) (43)
where\exp(PC⊤)is defined as (37).
Proof. Consider i-th row of exp(PC⊤S⊤
c),

exp(P(ScC)⊤)
i= exp([ P]i(ScC)⊤)
= exp([ P]i
cs1x1cs2x2···cs|J|
x|J|
)
=
exp(⟨[P]i,cs1x1⟩)··· exp(⟨[P]i,cs|J|
x|J|⟩)(44)
Then, we have

exp(P(ScC)⊤)DSc
i=
exp(⟨[P]i,cs1x1⟩)··· exp(⟨[P]i,cs|J|
x|J|⟩)
s1bs1x1···
s|J|bs|J|
x|J|
 (45)
We note that sbs
xsimply re-scales the entry of bs
xsuch that any non-zero entry becomes 1. Then,
let us consider j-th entry of
exp(PC⊤S⊤
c)DSc
i. Due to the restriction (25), we have exactly one
17

--- PAGE 18 ---
bs
x∈ J whose support region contains j, so the j-th column of the second matrix in the right hand
side of (45) contains exactly a 1and the remaining entries are 0. Therefore, we have

exp(P(ScC)⊤)DSc
i,j= exp( ⟨[P]i,cs
x⟩) = exp( ⟨[PC⊤]i,bs
x⟩) =h\exp(PC⊤)i
i,j(46)
where bs
x∈ J is the component that is supported on j. The second equality is based on (39).
Claim 6.3. IfScandDare defined as Claim 6.2, the pseudo inverse of Scis simply S†
c=S⊤
cD, so
each row of S†
candS†contain exactly a 1(so the number of nonzero entries of S†
candS†arencand
nrespectively).
Proof. Since each row of Scis some bs
x∈ J, due to the restriction (25), for i̸=j,

ScS⊤
cD
i,i=⟨[Sc]i,[DSc]i⟩=⟨bsi
xi, sibsi
xi⟩=si1
si= 1

ScS⊤
cD
i,j=⟨[Sc]i,[DSc]j⟩=⟨bsi
xi, sjbsj
xi⟩=sj0 = 0(47)
As a result, ScS⊤
cD=I. Further, S⊤
cDScis a symmetric matrix. So, all Moore-Penrose conditions
are verified. S†
c=S⊤
cD.
From the restriction (25), we have every column of §ccontains exactly a non-zero entry. Also,
S†
c=S⊤
cD=
s1bs1x1s2bs2x2···s|J|bs|J|
x|J|
(48)
Since the non-zero entry of bs
xis simply1
sby definition, sbs
xsimply re-scales the entry of bs
xsuch
that any non-zero entry becomes 1. As a result, each row of S†
chas exactly a 1. Also, by the relation
between SandSc:
S=
Inp×np0
0 Sc
S†=Inp×np0
0 S†
c
(49)
each row of S†has exactly a 1.
At the end, the approximation
\exp(PC⊤) = exp( PC⊤S⊤
c)DSc≈exp(PC⊤) (50)
does not look exactly as (19), but we can insert a simple diagonal matrix Dto the formulation (19)
and make the whole thing work.
6.2.4 How to Construct JforScandScC?
The derivation so far assume access to J, but in practice, we have no knowledge of Jand need to
construct Jthat leads to good approximation. With the approximation scheme in place, we can now
analyze the approximation error, which will be leveraged later to find a reasonable set of components
J. The approximation error of i-th row of exp(PC⊤)can be expressed as
Ei:=
exp(PC⊤)
i−h\exp(PC⊤)i
i2
F
=ncX
j=1(
exp(PC⊤)
i,j−h\exp(PC⊤)i
i,j)2
=X
bsx∈JX
[bsx]j̸=0(
exp(PC⊤)
i,j−exp(⟨[PC⊤]i,bs
x⟩))2
=X
Bsx∈Jexp(⟨[PC⊤]i,bs
x⟩)X
(Bsx)j̸=0(exp(
PC⊤
i,j− ⟨[PC⊤]i,bs
x⟩)−1)2
=X
Bsx∈Jexp(⟨[P]i,cs
x⟩)X
(Bsx)j̸=0(exp(⟨[P]i,[C]j⟩ − ⟨[P]i,cs
x⟩)−1)2
=X
Bsx∈Jexp(⟨[P]i,cs
x⟩)X
(Bsx)j̸=0(exp(⟨[P]i,[C]j−cs
x⟩)−1)2(51)
18

--- PAGE 19 ---
Algorithm 1 Constructing J
Input: VIP-tokens Pandcs
xfor all sandx
Input: hs: number of s-length segments to refine for each s∈ {2,4,···, nc}
Initialize empty J
Compute µnc
1(53) and add bnc
1(22) to J(compute root node)
fors←nc, nc/2,···,2do
Pophselements bs
xwith the largest µs
x(53) (select nodes with higher attention scores)
foreachbs
xdo
Compute µs/2
2x−1, µs/2
2x(53) and add bs/2
2x−1,bs/2
2x(22) to J(split selected nodes)
end for
end for
Output: J
The second equality and fourth equality are due to (25) and(39). The approximation error is governed
by two components multiply together: attention score between [P]iand the local average cs
xof the
x-ths-length segment of sequence Cand the inner product of [P]iwith the amount of deviation of
[C]jfrom its local average cs
x.
When s= 1, the deviation is simply zero:
c1
x= [C]x. (52)
It is reasonable to assume that the deviation [C]j−cs
xis smaller if sis smaller. Therefore, this
actually suggests a simple heuristic for selecting J: when exp(⟨[P]i,cs
x⟩)is large, we should
approximate the x-ths-length segment of Cwith higher resolution (by splitting the segment to
shorter sub-segments and using finer approximation). This heuristic describes the selection criteria
for one row of exp(PC⊤), which corresponds to a single VIP-token, for multiple rows of exp(PC⊤)
(for multiple VIP-tokens), we simply use
µs
x=npX
i=1exp(⟨[P]i,cs
x⟩) (53)
as selection criteria since Jis shared by all VIP-tokens.
The construction of Jis described in Alg. 1. This algorithm describes the same procedure as the
Figure 3 in the main text. The bs
x’s inJare the rows of Sc, and the corresponding cs
x’s(40) are
the rows of ScC. The budgets h2, h4,···, hncrequired by Alg. 1 is used determine the number of
components at each resolution that will be added to J. Specifically, there are 2h2s−hsnumber of
components bs
xfors̸= 1based on simple calculations. We can choose budgets such that the final
size of Jisr−npto make the length of compressed sequence to be r.
6.2.5 How Good is This Approximation?
At high level, the compression Scperforms more compression on tokens that are not relevant to the
VIP-tokens and less compression to tokens that are important to the VIP-tokens. We will discuss it in
more details. Since each row of S†contain exactly a 1as stated in Claim 6.3, S†can commute with
β, so in summary, we can write the approximation of the computation of a Transformer layer as
α(P,SX,SX) = exp( PP⊤)P+ exp( PC⊤S⊤
c)DScC
S†
cα(ScC,SX,SX) =S†
cexp(ScCP⊤)P+S†
cexp(ScCC⊤S⊤
c)DScC
Pnew
Cnew
=β(α(P,SX,SX) +P) +α(P,SX,SX)
β(S†
cα(ScC,SX,SX) +S†
cScC) +S†
cα(ScC,SX,SX)
+
P
C(54)
Note that Dis added as discussed in (50).
There are four main approximation components (purple) in (54). Taking the fact that DSc= (S†
c)⊤,
all of these approximations are row or column space multi-resolution approximations governed by
Scmatrix. High attention weight implies higher dependency, and the procedure in Alg. 1 refines
regions with large attention weights with higher resolutions. Therefore, the token embedding in C
that have higher dependency to Pare better approximated. The output Pnewis well approximated by
design since the approximation preserves the higher frequency components of the subset of rows of
19

--- PAGE 20 ---
Cthat has high impact on the output Pnew. Further, the output in Cnewcorresponding to the subset
of rows of Cthat have higher dependency with the VIP-tokens will have better approximation than
the remaining rows of C. This property addresses the issue that some tokens with unknown locations
are also relevant to the final prediction of a Transformer in some tasks. For example, in question
answering tasks, candidate answers are usually expected to have large dependency with question
tokens (VIP-tokens), so they are approximated well as well. This approximation property is exactly
what we need.
6.2.6 Relation to [36] that Inspires Multi-Resolution Compression
Our work and [ 36] can be viewed as operating at slightly different levels of abstractions. While [ 36]
tries to approximate self-attention computation efficiently, our paper proposes a general framework
for performing a VIP-token centric compression on the sequence to efficiently handle extremely
long sequences (the self-attention module remains completely unchanged). Our VIP-token centric
compression involves a number of steps described in the main text. But one of the key steps involves
constructing a compression matrix Scwhich has some desirable properties, namely satisfying (19)
which we elaborate further below.
Note that for equation (19), we need a matrix Scsuch that the approximated attention matrix involving
PandCis similar to the true attention matrix involving PandC. This is precisely where the general
idea of [ 36] can be used. But the formulation in [ 36] cannot be applied directly in its original form
since it cannot give us Sc. Why? One reason is that the formulation in [ 36] cannot be written as
matrix form similar to equation (19). This may be a reason why [ 36] has to use custom CUDA kernels
in their implementation. Nonetheless, the properties of [ 36] are useful. So we derive the analogous
form but for 1D instead: this 1D case is expressed as applying a matrix (this is the Scwe are looking
for) to the signal C.
One bonus of this modification is that it also removes the need for custom CUDA kernels. At a high
level, [ 36] offers a multi-resolution view of the self-attention matrices, and our modified version is
best thought of as a similar multi-resolution view of the sequence itself. But we can also substitute in
a different means of obtaining Sc(which could simply be a sketching matrix). Finally, we note that a
naive implementation of the resulting modification still requires a O(ncd)cost due to the computation
ofcs
xfor all possible scaling sand translation x. There is a similar cost in [ 36] (second paragraph in
section 4.4 in [36]). The data structure we propose reduces this cost.
6.3 Details of Proposed Data Structure
In section, we describe some omitted technical details of the proposed data structure T(·).
6.3.1 Why (cnew)1
1−c1
1= (cnew)1
2−c1
2= (cnew)2
1−c2
1ifScC=
c2
1c1
3c1
4c4
2⊤?
Claim 6.4. Given the set Jsatisfying restriction (25), ifbs
x∈ J, then (cnew)s
x−cs
x= (cnew)s′
x′−cs′
x′
for all bs′
x′satisfying the support of bs′
x′is contained in the support of bs
x(thex′-ths′-length segment
ofCis a sub-segment of the x-ths-length segment of C).
Proof. To simplify the notations a bit, without loss of generality, we assume x= 1. Then, for i≤s,
consider (cnew)1
i:
(cnew)1
i= [Cnew]i=
S†
cβ(α(ScC,SX,SX) +ScC) +S†
cα(ScC,SX,SX)
i+ [C]i
=
S†
c
iβ(α(ScC,SX,SX) +ScC) +
S†
c
iα(ScC,SX,SX) +c1
i(55)
By Claim 6.3, S†
c=S⊤
cDandi-th row of S†
ccontains exactly a 1. The column that contains 1in the
i-th row of S†
cis exactly sbs
1since iis contained in the support of exactly one components in Jdue
to the restriction (25). Denote this column index as j, then
(cnew)1
i= [β(α(ScC,SX,SX) +ScC)]j+ [α(ScC,SX,SX)]j+c1
i (56)
Note that this holds for all i≤s. As a result, for i, i′≤s,
(cnew)1
i−c1
i= [β(α(ScC,SX,SX) +ScC)]j+ [α(ScC,SX,SX)]j= (cnew)1
i′−c1
i′(57)
20

--- PAGE 21 ---
Algorithm 2 Computation of one Transformer layer with T(C)
Input: VIP-tokens Pand data structure T(C)
Use Algo. 1 to construct Jbut use (63) to retrieve cs
xfromT(C)
Construct Sc,ScCassociated with Jusing Claim 6.2
Compute
Pnew
ScCnew
=
β(α(P,SX,SX) +P) +α(P,SX,SX) +P
β(α(ScC,SX,SX) +ScC) +α(ScC,SX,SX) +ScC
(62)
SetT(Cnew)← T(C)
fors←1,2,4,···nc/2do
forbs
x∈ J do
Locate row location bs
xinSc, refer the index as j
Compute (cnew)s
x= [ScCnew]j
Mark ∆(cnew)s
xdirty
end for
fordirty∆(cnew)s
xdo
Compute (cnew)2s
⌈x/2⌉and update ∆(cnew)s
x
Mark ∆(cnew)2s
⌈x/2⌉dirty
end for
end for
Update (cnew)nc
1
Output: VIP-tokens Pnewand data structure T(Cnew)
Then,
(cnew)2
⌈i/2⌉−c2
⌈i/2⌉=1
2(cnew)1
2⌈i/2⌉−1+1
2(cnew)1
2⌈i/2⌉−1
2c1
2⌈i/2⌉−1−1
2c1
2⌈i/2⌉
=1
2((cnew)1
2⌈i/2⌉−1−c1
2⌈i/2⌉−1) +1
2((cnew)1
2⌈i/2⌉−c1
2⌈i/2⌉)
= (cnew)1
2⌈i/2⌉−1−c1
2⌈i/2⌉−1(58)
The rest follows from induction.
6.3.2 How do we get (cnew)2
1,(cnew)1
3,(cnew)1
4,(cnew)4
2ifScC=
c2
1c1
3c1
4c4
2⊤?
Claim 6.5. We have
ScCnew=β(α(ScC,SX,SX) +ScC) +α(ScC,SX,SX) +ScC. (59)
And the updated representation (cnew)s
xof the corresponding cs
x(a row of ScC) is the corresponding
row of ScCnew.
Proof. By definition,
Cnew=S†
cβ(α(ScC,SX,SX) +ScC) +S†
cα(ScC,SX,SX) +C (60)
Then,
ScCnew=ScS†
cβ(α(ScC,SX,SX) +ScC) +ScS†
cα(ScC,SX,SX) +ScC
=β(α(ScC,SX,SX) +ScC) +α(ScC,SX,SX) +ScC(61)
Since the Scis the same for ScCnewandScC, the second statement follows.
6.3.3 Algorithm for Making T(C)intoT(Cnew)
In this section, we describe the exact algorithm to update T(C)intoT(Cnew). The pseudo code is
described in Alg. 2 where cs
xis computed via
cs
x=c2s
⌈x/2⌉−∆cs
x=c4s
⌈x/4⌉−∆c2s
⌈x/2⌉−∆cs
x=··· (63)
We use the term “dirty” in Alg. 2 to indicate the node needs to be handled due to node updates. This
term is commonly used in computer cache implementations to indicate that the data of a specific
location has been updated and needs to be accounted for.
21

--- PAGE 22 ---
6.4 Complexity Analysis
In this section, we will discuss the detailed complexity analysis of our proposed method. The overall
complexity of our proposed method is O(lrd2+lr2d+lrlog(nc)d+lrnpd+nd)when using the
proposed efficient data structure.
6.4.1 Preparing Input Sequence to T(C):O(nd)
At the first layer, we need to permute the rows of Xinto[P;C], which takes O(nd)cost. Then, we
process CintoT(C). This requires (1)computing cs
xdefined in (41). c1
x= [C]x, so no compute is
needed. With all c1
xgiven, computing all c2
xtakesO(ncd/2). With all c2
xgiven, computing all c4
x
takesO(ncd/4)... So, the cost is
O(ncd/2 +ncd/4 +···+d) =O(ncd). (64)
Then (2)computing ∆cs
xfor all sandx. Computing each ∆cs
xtakesO(d)when given cs
xandc2s
⌈x/2⌉.
The amount of cost is the same as the number of nodes in the tree T(C), so the cost is O(ncd). Note
thatnc< n, so the overall complexity of the above operations is O(nd).
6.4.2 Constructing J,Sc,ScC:O(lrlog(nc)d+lrnpd)
We can analyze the complexity of constructing Jusing Algo. 1. There is only one possible µncx.
Then for each s, there are 2hsnumber of µs/2
xbeing computed since there are 2components bs/2
xfor
eachbs
x′. As a result, we need to compute O(1 +P
s2hs)number of µs/2
x. When cs/2
xis given, the
cost of computing a µs/2
xisO(npd), so the overall cost of constructing JisO((1 +P
s2hs)npd).
Further, at each s, the size of Jis increased by hssince hssegments are split into 2hssub-segments,
so the size of JisO(P
shs). Since Sc∈R(r−np)×nand|J |=r−npas discussed in §6.2.4,
O(r−np) =O(P
shs). We use O(r)for simplicity instead of O(r−np). As a result, the overall
cost of constructing JisO(rnpd).
The above cost assumes cs/2
xis given. If we compute all possible cs/2
xusing (41), the cost will
beO(ncd)as analyzed in §6.4.1. However, if we employ the proposed data structure, each cs/2
x
can be retrieved in at most O(log(nc)d)by recursively computing (63). Since we need to retrieve
O(1 +P
s2hs) =O(r)number of cs
x, the complexity of computing necessary cs
xisO(rlog(nc)d).
As a result, the complexity of constructing JisO(rnpd+rlog(nc)d)at each layer. When summing
the cost over all layers, the complexity is O(lrnpd+lrlog(nc)d).
By Claim 6.2, the rows of ScandScCare simply the bs
x∈ J and the corresponding cs
x, which are
already computed during the construction of J, so we essentially can get these ScandScCfor free.
6.4.3 Feeding Compressed Sequence into a Transformer Layer: O(lrd2+lr2d)
At each layer, we need to compute

Pnew
ScCnew
=
β(α(P,SX,SX) +P) +α(P,SX,SX) +P
β(α(ScC,SX,SX) +ScC) +α(ScC,SX,SX) +ScC
(65)
for updating T(C). This is the part of a Transformer layer that requires heavy computation. It can
be verified that the complexity of a Transformer layer is O(nd2+n2d)for a input sequence of
length n. Now a compressed sequence of length ris fed into a Transformer layer, the cost is simply
O(rd2+r2d). We note that there is an additional re-scaling to plug Dintoexp(PC⊤S⊤
c)DSc
during multi-head attention computation discussed in 50. However, the additional cost of applying D
isO(rd), which does not change the complexity. When summing the cost of all layers, the overall
complexity is O(lrd2+lr2d).
6.4.4 Updating T(C)intoT(Cnew):O(lrd)
Once (65) is computed, we need to change T(C)intoT(Cnew). The cost of change T(C)into
T(Cnew)isO(rd)as analyzed in the main text. For more specific analysis, let us take a look at the
first three iterations:
22

--- PAGE 23 ---
(1)At the first iteration, there are O(2h2)number of (cnew)1
xto be computed at the first inner
for loop, and there are O(2h2)number of ∆(cnew)1
xto be updated in the second inner for
loop. Additional O(h2)number of ∆(cnew)2
⌈x/2⌉are masked dirty.
(2)At the second iteration, there are O(2h4)number of (cnew)2
xto be computed at the first
inner for loop, and there are O(2h4+h2)number of ∆(cnew)2
xto be updated in the second
inner for loop. The second term is due to the dirty ∆(cnew)2
⌈x/2⌉from the first iteration.
Additional O(h4+h2
2)number of ∆(cnew)4
⌈x/2⌉are masked dirty.
(3)At the third iteration, there are O(2h8)number of (cnew)4
xto be computed at the first inner
for loop, and there are O(2h8+h4+h2
2)number of ∆(cnew)4
xto be updated in the second
inner for loop. The second and third term is due to the dirty ∆(cnew)4
⌈x/2⌉from the second
iteration. Additional O(h8+h4
2+h2
4)number of ∆(cnew)8
⌈x/2⌉are masked dirty.
It becomes apparent that if we sum over the number of computes of (cnew)s
xand updates of ∆(cnew)s
x,
the total number is O(P
s2hs+ 2P
sPlog(s)
j=1hs
2j) =O(P
shs+P
shs) =O(r). Since each
compute and update takes O(d)cost, the overall complexity of changing T(C)intoT(Cnew)is
O(rd). When summing the cost of all layers, the overall complexity is O(lrd).
6.4.5 Materializing CnewfromT(Cnew)at the Last Layer: O(nd)
At the output of the last layer, we can (1)compute all (cnew)nc/2
x via(63) at a cost of O(2d),
(2)compute (cnew)nc/4
x via(63) at a cost of O(4d)... until all (cnew)1
xare computed. Then,
[Cnew]x=c1
xis materialized from T(Cnew)at a total cost of
O(d+ 2d+ 4d+···+ncd) =O(ncd). (66)
Lastly, undoing the permutation so that [Pnew;Cnew]are re-ordered to the original positions has a
complexity of O(nd). As a result, the overall complexity is O(nd).
6.4.6 Overall Complexity
In summary, the overall complexity of our method is
O(lrd2+lr2d+lrlog(nc)d+lrnpd+nd) (67)
6.5 Potential Application to Decoders
To reduce the complexity of implementations, the method is proposed for the encoder module of the
Transformer that assumes full access to the entire sequence. The proposed compression might be
extended to approximate the computation in the decoder, but it requires more implementation efforts,
so we leave it for the future work. We briefly describe two possible options to do so. (1)We can use
the input tokens of the decoder as VIP-tokens to compress the representations of context sequence
generated by the encoder before Cross Attention computation to reduce the cost of Cross Attention.
(2)Auto-regressive decoding operates using Causal Attention at each step. This Causal Attention
operation requires memory and computation that is linear in the length of the prefix. We can keep
the same Causal Attention VIP-token (the representation of the token currently being generated) and
apply our method to compress the representations of the previously generated tokens. This reduces
the linear complexity of the Causal Attention operation to sublinear. This is useful for reducing
the cost of inference. For training, we can break the sequence into two segments: prefix segment
and decoding segment. Then, we can use the proposed compression in prefix segment and vanilla
computation in decoding segment. To prevent look ahead to the future tokens, we might only use the
first token in the decoding segment as VIP-token.
6.6 Experiments
We run all experiments on NVIDIA A100 GPUs. All code is implemented using the standard PyTorch
framework. No custom CUDA kernels are needed. As a result, it can be easily deployed to other
platforms or ML frameworks. We will publish all code and checkpoints necessary for reproducibility
concurrently with the paper publication.
23

--- PAGE 24 ---
Table 5: Length statistics of each dataset. The values are the percentiles of number of tokens for the
specific tokenizers. For T5 tokenizer, the left value of is for sequence lengths of encoder input, and
the right value is for sequence lengths of decoder input.
RoBERTa T5
Percentile HotpotQA QuALITY WikiHop WikiHop HotpotQA Qasper QuALITY ContractNLI
75th 1535 7603 2204 2399 / 6 1692 / 6 7029 / 29 7747 / 17 2991 / 4
95th 1928 8495 3861 4206 / 9 2129 / 10 10920 / 71 8603 / 28 5061 / 4
T5
Percentile NarrativeQA CNN/Dailymail MediaSum Arxiv SummScreenFD GovReport QMSum MultiNews
75th 90482 / 10 1242 / 87 2621 / 29 13477 / 364 12119 / 188 13304 / 811 19988 / 110 3032 / 379
95th 260533 / 18 1946 / 130 5061 / 64 26024 / 759 16722 / 330 23795 / 983 31749 / 162 6676 / 468
Table 6: Dev set results for encoder-only models fintuning on HotpotQA, QuALITY , and WikiHop.
Method Size Length HotpotQA QuALITY WikiHop
Runtime EM F1 Runtime Accuracy Runtime Accuracy
RoBERTa base 512 19.9 35.1 44.9 21.2 39.0 19.6 67.6
RoBERTa base 4k 422.3 62.2 76.1 403.2 39.5 414.1 75.2
Big Bird base 4k 297.9 59.5 73.2 307.0 38.5 293.3 74.5
Longformer base 4k 371.0 59.9 73.6 368.0 27.9 369.7 74.3
Ours base 4k 114.6 60.9 74.6 126.4 39.6 108.0 75.9
Ours-150k base 4k 114.6 60.7 74.1 126.4 39.4 108.0 76.1
6.6.1 Pretraining
We use a filtered The Pile dataset [ 12] for all pretrainings. Since we are using public pretrained
tokenizers, we want to enable the distribution of pretraining corpus aligns well with the distribution
of corpus used to create the tokenizers. As a result, we use tokens per byte as a proxy for alignment of
distributions and filter out PubMed Central, ArXiv, Github, StackExchange, DM Mathematics [ 27],
Ubuntu IRC, EuroParl [ 19], YoutubeSubtitles, and Enron Emails [ 17] components, which have tokens
per byte greater than 0.3. Then, the remaining corpus of The Pile dataset is used for pretraining.
1022×1026×102
Runtime (ms)6971737577Accuracy
RoBERTa
Big Bird
Longformer
Ours
Ours*
Ours-150k
Figure 8: Model runtime vs WikiHop
dev accuracy when using different model
specific hyperparametersFor encoder-only models, we pretrain RoBERTa for 750K
steps. A batch consists of 8,192 sequences of 512 length.
The masking ratio for masked language modeling (MLM)
is 15%. Then, 4K length models are continuously pre-
trained from the RoBERTa checkpoints for 300k steps.
The positional embeddings are extended by duplicating
the pretrained 512 positional embedding multiple times.
For 4K length RoBERTa, Longformer, Big Bird and MRA
Attention, the batch size is 64, and the masking ratio is
15%. With 15% masking ratio, there are roughly 616
masked tokens scattering in the sequences. We find that
using 616 scattered masked tokens as VIP tokens for 4,096
length sequences might not be indicative for VIP-token
centric compression, so we use masking ratio 7.5% and
batch size 128 for our method. The number of masked tokens per sequence is reduced, and the
number of total masked token predictions remains the same during pretraining. We note that with
larger batch size, the wall clock pretraining runtime for our method is still smaller than baselines. In
case that anyone is interested, we also show downstream finetuning on our method pretrained on the
same number of tokens but fewer number of masked token predictions in Tab. 6 and Fig. 8, denoted
as Ours-150k. The accuracy is consistent with our model pretrained on 300k steps. For the larger
scale pretraining denoted with *, we pretrain our method for 250K steps with batch size 512 and
masking ratio 7.5%.
For encoder-decoder architecture of our method, we do continuous pretraining from the public
checkpoints of T5 for 250K steps with batch size 256 using the masked span prediction. Since each
masked span (consists of multiple tokens) is replaced by a single special token, when using masking
ratio is 15%, the number of special tokens in a sequence is not too large, we keep masking ratio 15%
unchanged.
24

--- PAGE 25 ---
Table 7: Dev results for encoder-decoder models on MultiNews.
Method Size # Param Length MultiNews
Runtime R-1 R-2 R-L
T5 base 223M 512 59.2 / 20.5 42.5 15.3 39.0
T5 base 223M 4K 651.2 / 551.8 46.4 18.2 42.6
LongT5 base 248M 8K 721.7 / 550.6 46.7 18.3 42.9
LED base 162M 8K 526.5 / 454.2 46.6 17.8 42.7
Ours base 223M 8K 377.0 / 224.6 46.4 18.1 42.7
T5 large 738M 512 180.8 / 67.0 43.4 15.6 39.8
Ours large 738M 8K 1140.3 / 651.5 48.2 19.2 44.2
Ours 3b 3B 8K 4094.5 / 2696.0 48.9 19.4 44.7
6.6.2 Downstream Finetuning
The statistics of the sequence lengths of instances in each dataset are summarized in Tab. 5. The
hyperparameters of all experiments are summarized in Tab 8. When there are multiple values in
an entry, it means we perform a hyperparameter search on these values. The amount of search is
determined by the size of datasets. If a dataset is relatively large, we only search the learning rate.
If a dataset is small, we include batch size and the number of epochs in search. For all tasks, if
the sequence lengths are longer than the model length m, the sequences will be truncated and only
the first mtokens will be used. For encoder-decoder models, we use greedy decoding in sequence
generations for simplicity. The maximal decoder output length, specified in Tab. 8, is set such that
the maximal length covers the output lengths of more than 99% of instances. When the length of
covering 99% of instances is greater than 512, we just set the maximal decoder output length to 512.
Additionally, we show one extra experiment on MultiNews [ 11] in Tab. 7, which is not in the main
text due to space limit.
Table 8: Hyperparameters for all experiments.
LM Encoder-Only Encoder-Decoder
Task HotpotQA QuALITY WikiHop WikiHop HotpotQA CNN/Dailymail MediaSum Qasper
Optimizer Adam Adam Adam Adam Adam Adam Adam Adam
Weight Decay 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
LR Decay Linear Linear Linear Linear Linear Linear Linear Linear
Precision FP16 FP16 FP16 BF16 BF16 BF16 BF16 BF16
Batch Size 32 16 32 32 32 32 32 {16, 32}
Learning Rate {3e-5, 5e-5} {3e-5, 5e-5} {3e-5, 5e-5} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4}
Epochs 10 {10, 20} 10 10 10 10 10 {10, 20}
Warmup Steps 1000 200 1000 1000 1000 1000 1000 200
Max Output Length - - - 32 40 256 256 128
LM Encoder-Decoder
Task QuALITY ContractNLI NarrativeQA Arxiv SummScreenFD GovReport QMSum MultiNews
Optimizer Adam Adam Adam Adam Adam Adam Adam Adam
Weight Decay 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01
LR Decay Linear Linear Linear Linear Linear Linear Linear Linear
Precision BF16 BF16 BF16 BF16 BF16 BF16 BF16 BF16
Batch Size {16, 32} {16, 32} 32 32 {16, 32} {16, 32} {16, 32} 32
Learning Rate {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4} {1e-4, 3e-4}
Epochs {10, 20} {10, 20} 5 {10, 20} {10, 20} {10, 20} {10, 20} 10
Warmup Steps 200 1000 1000 1000 200 1000 100 1000
Max Output Length 90 4 47 512 512 512 310 512
6.7 Practical Questions
Why is the performance of our method is better than standard models?
Our method is an approximation of the standard models, which should be inferior to the standard
models, but in some cases, the performance of our method is better than standard models. We believe
the reason is that the correct inductive bias improves the performance for tasks with limited amounts
of data. Our approach is forced to compress irrelevant information and the attention is carried out on
the compressed sequences, but in standard model with standard attention, each token has access to
the entire sequence, which enables a larger degree of freedom. As a result, more training data might
be required for the model to learn the correct pattern or bias.
25
