# 2402.10685.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2402.10685.pdf
# Kích thước tệp: 792395 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
LONGHEADS : Multi-Head Attention thực chất là một Bộ xử lý Ngữ cảnh Dài
Yi Lu1*, Xin Zhou1*, Wei He1, Jun Zhao1,
Tao Ji1†, Tao Gui2†, Qi Zhang1†, Xuanjing Huang1,3
1Khoa Khoa học Máy tính, Đại học Fudan, Thượng Hải, Trung Quốc
2Viện Ngôn ngữ và Ngôn ngữ học Hiện đại, Đại học Fudan, Thượng Hải, Trung Quốc
3Viện Kiểu hình Người Quốc tế, Thượng Hải, Trung Quốc
yilu23@m.fudan.edu.cn, {xzhou20,taoji,tgui,qz}@fudan.edu.cn
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã đạt được
hiệu suất ấn tượng trong nhiều lĩnh vực
nhưng thường gặp khó khăn trong việc xử lý các đầu vào dài một cách
hiệu quả và hiệu quả do khả năng khái quát hóa độ dài hạn chế và yêu cầu tính toán bậc hai của attention. Nhiều người đã tìm cách giảm thiểu
điều này bằng cách hạn chế cửa sổ attention trong
độ dài đã được huấn luyện trước. Tuy nhiên, những phương pháp này
gây ra các vấn đề mới như bỏ qua ngữ cảnh giữa và yêu cầu huấn luyện bổ sung. Để giải quyết những vấn đề này, chúng tôi đề xuất
LONGHEADS, một khung làm việc không cần huấn luyện nhằm
nâng cao khả năng ngữ cảnh dài của LLM bằng cách khai phá
tiềm năng chưa được khai thác của multi-head attention.
Thay vì cho phép mỗi head chú ý đến toàn bộ
câu, điều này gặp khó khăn trong việc khái quát hóa
đến các chuỗi dài hơn do vấn đề ngoài phân phối
(OOD), chúng tôi cho phép mỗi head xử lý độ dài
trong phân phối bằng cách chọn và chú ý
đến các đoạn ngữ cảnh quan trọng. Vì vậy, chúng tôi
đề xuất một chiến lược chọn đoạn dựa trên
mối tương quan vốn có giữa biểu diễn query và
key, phân phối hiệu quả các đoạn ngữ cảnh
cho các head khác nhau. Bằng cách này,
mỗi head đảm bảo rằng nó có thể xử lý hiệu quả
các token được chú ý trong độ dài đã huấn luyện,
trong khi các head khác nhau trong các lớp khác nhau có thể
xử lý cùng nhau các ngữ cảnh dài hơn. LONG-
HEADS hoạt động hiệu quả trong thời gian tuyến tính, phù hợp
liền mạch với nhiều LLM sử dụng mã hóa vị trí
tương đối. LONG HEADS đạt được
độ chính xác 100% ở độ dài 128k trong
nhiệm vụ truy xuất passkey, xác minh hiệu quả của LONG HEADS trong việc mở rộng cửa sổ ngữ cảnh có thể sử dụng
cho các mô hình hiện có. Chúng tôi phát hành mã của mình tại
https://github.com/LuLuLuyi/LongHeads.
1 Giới thiệu
LLM thường được yêu cầu xử lý các nhiệm vụ với
ngữ cảnh dài, chẳng hạn như học trong ngữ cảnh (Dong
et al., 2023), học công cụ (Qin et al., 2023), và
*Đóng góp ngang nhau.
†Tác giả liên lạc.
Tất cả Head
Đoạn được chọnLONGHEADSTất cả Head
Head 2Head n
Head 1Token bị loạiToàn bộ Attention
Attention hạn chế
Ngoài độ dài đã huấn luyện trước
00.20.40.60.81
8k 16k 32kĐộ chính xác trên Truy xuất Passkey
NTK LM-INF Của chúng tôiHình 1: Trái: Ba loại bộ xử lý ngữ cảnh dài, (a) Chú ý đến tất cả ngữ cảnh nhưng gặp khó khăn với độ dài ngoài
đã huấn luyện trước; (b) Chú ý đến ngữ cảnh cục bộ để tạo ra
một cách trôi chảy nhưng mất thông tin; (c) Head chú ý đến các đoạn ngắn và
HEADS chú ý đến ngữ cảnh DÀI. Phải: Độ chính xác của ba phương pháp cụ thể trên nhiệm vụ truy xuất passkey.
tạo tăng cường truy xuất (Gao et al., 2024).
Tuy nhiên, việc cho phép LLM xử lý ngữ cảnh dài
đặt ra những thách thức đáng kể. Vấn đề OOD
khiến LLM gặp khó khăn trong việc xử lý các token vượt quá độ dài đã
huấn luyện trước, và độ phức tạp bậc hai của atten-
tion gây ra chi phí huấn luyện và suy luận đáng kể. Mặc dù vấn đề OOD có thể được giải quyết
bằng học zero-shot (Jin et al., 2024), fine-tuning
(Chen et al., 2023a; Peng et al., 2023), hoặc huấn luyện lại
(Sun et al., 2022; Press et al., 2022), bộ nhớ và tính toán
yêu cầu vẫn tăng bậc hai
với độ dài ngữ cảnh, như được hiển thị trong Hình 1(a).
Để giảm bớt những vấn đề này, các nghiên cứu gần đây hạn chế
cửa sổ attention đến độ dài đã huấn luyện trước, điều này
giảm chi phí tính toán và tránh việc xử lý các token OOD. Một hướng là loại
trừ các token xa (ngoại trừ một vài token đầu
tiên, Han et al., 2023; Xiao et al., 2023) để hạn chế
cửa sổ attention trong phân phối, như được hiển thị trong
Hình 1(b). Tuy nhiên, những phương pháp này có thể dẫn
đến việc mất thông tin quan trọng, làm giảm hiệu suất
trên các nhiệm vụ downstream. Cách khác để
hạn chế cửa sổ attention là truy xuất các đoạn
của chuỗi dài (Mohtashami and Jaggi, 2023;
Zhang et al., 2024), nhưng những phương pháp này thường yêu
cầu các hoạt động đặc biệt và fine-tuning liên tục,
điều này khiến các LLM hiện có khó có thể
1arXiv:2402.10685v2  [cs.CL]  25 Mar 2024

--- TRANG 2 ---
áp dụng trực tiếp cho các chuỗi dài. Tóm lại,
việc cải thiện khả năng xử lý ngữ cảnh dài của LLM
với chi phí thấp vẫn là thách thức.
Trong bài báo này, chúng tôi đề xuất LONGHEADS, một
khung làm việc mới để nâng cao khả năng ngữ cảnh dài
của LLM mà không cần huấn luyện bổ sung. Ý tưởng chính là
khai thác đầy đủ tiềm năng của multi-head attention.
Đầu tiên chúng tôi sử dụng bản chất của các head khác nhau tập trung
vào các không gian con khác nhau của ngữ cảnh, và mỗi
head có thể xử lý hiệu quả các chuỗi trong
độ dài huấn luyện trước. Như được hiển thị trong Hình 2
(c), chúng tôi giới hạn mỗi head chỉ chọn và chú ý
đến các đoạn ngữ cảnh quan trọng trong độ dài đã huấn luyện
trước, thay vì để mỗi head chú ý đến
toàn bộ câu, từ đó tránh vấn đề OOD. Hơn nữa, chúng tôi tận dụng dot-product attention
vốn có của mô hình và đề xuất một chiến lược chọn đoạn
để tìm các đoạn quan trọng cho mỗi
head. Lấy cảm hứng từ thực tế rằng mỗi
head gán các trọng số attention khác nhau cho các token dựa trên mối tương quan vốn có giữa
biểu diễn query và key, chúng tôi chia
đầu vào thành các đoạn và tạo các đặc trưng cấp đoạn
cho mỗi khối. Nó sử dụng mối tương quan cấp token gốc để xây dựng các biểu diễn query và key cấp đoạn, cho phép mỗi head sử dụng
khả năng hiện có của nó (dot-product attention) để
chọn các đoạn dựa trên trọng số attention. Bằng
cách này, mỗi head xử lý hiệu quả các đoạn ngữ cảnh đã chọn trong độ dài đã huấn luyện, và tất cả
head trong tất cả các lớp làm việc cùng nhau để xử lý
ngữ cảnh dài hơn. Đồng thời, tất cả các hoạt động đều dựa trên
khả năng nội tại của multi-head attention,
cho phép LONG HEADS nâng cao LLM mà không cần
huấn luyện bổ sung.
Để đánh giá hiệu quả của LONG HEADS,
chúng tôi sử dụng LLaMA-2-7B-Base và LLaMA-2-7B-
Chat làm mô hình cơ sở và đánh giá trên mô hình ngôn ngữ, nhiệm vụ truy xuất tổng hợp và điểm chuẩn ngữ cảnh dài. LONG HEADS đạt được gần như
100% độ chính xác trên các độ dài ngữ cảnh từ 4k
đến 32k trên nhiệm vụ Truy xuất Passkey. Trên Long-
Bench, LONG HEADS đạt được hiệu suất tốt nhất hiện tại
(SOTA) trong số các phương pháp attention hạn chế. So với các phương pháp attention đầy đủ,
LONG HEADS đạt được hiệu suất tương đương
trên độ dài thử nghiệm 16K và hiệu suất tốt nhất trên độ dài thử nghiệm 32K trong khi tận hưởng chi phí tính toán tuyến tính. Kết quả thí nghiệm chứng minh
rằng LONG HEADS cho phép LLM trực tiếp
khái quát hóa đến các chuỗi dài hơn và đạt được hiệu suất tương đương hoặc thậm chí vượt trội so với các phương pháp yêu cầu fine-tuning liên tục.
Những đóng góp của chúng tôi có thể được tóm tắt như sau:
•Chúng tôi đề xuất LONG HEADS, một khung suy luận
không cần huấn luyện tận dụng các thuộc tính cấu trúc
của attention head để xử lý các chuỗi dài
một cách hiệu quả và hiệu quả.
•Chúng tôi thiết kế một chiến lược chọn đoạn đơn giản
nhưng hiệu quả có thể chọn chính xác các đoạn hữu ích
và bao phủ toàn bộ ngữ cảnh.
•Các thí nghiệm chứng minh rằng LONG HEADS là
bộ xử lý ngữ cảnh dài dựa trên restricted-attention
SOTA và hoạt động hiệu quả trong thời gian tuyến tính, cũng với hiệu suất tương đương các phương pháp full-
attention.
2 Phương pháp
Trong phần này, chúng tôi mô tả cách LONGHEADS
sử dụng khả năng vốn có của multi-head attention
để mã hóa và tạo ra các chuỗi dài mà không cần
huấn luyện bổ sung.
2.1 Tổng quan
Tổng quan về LONGHEADS được hiển thị trong Hình 2.
Chúng tôi chia văn bản thành các đoạn và tính toán
các biểu diễn đoạn cho mỗi đoạn. Khi tạo
token x14, chúng tôi chọn k đoạn liên quan
dựa trên vector query của token hiện tại và biểu diễn đoạn. Bằng cách này, mỗi attention head của
LONG HEADS chọn lọc tập trung vào các
đoạn văn bản khác nhau theo sở thích của nó. Các token
của các đoạn được chú ý sau đó được tái cấu trúc, đảm bảo causal attention tiếp theo luôn được thực hiện
trong độ dài đã huấn luyện trước.
Khi mã hóa hoặc tạo một token ngoài độ dài,
một mạng chọn đoạn không có tham số
chọn k đoạn liên quan dựa trên
vector query hiện tại và biểu diễn đoạn. Các đoạn không được chọn có thể được xấp xỉ như có
điểm attention bằng không (điều này thường đúng dưới tính thưa thớt của cơ chế attention), và không cần
được tính toán. Điều này cho phép ma trận attention không
tăng theo độ dài, giảm đáng kể
bộ nhớ và chi phí tính toán của ngữ cảnh dài
từ O(N2) thành O(N). Các nghiên cứu khác hạn chế
phạm vi attention đơn giản bỏ qua các token xa
ngoài một vài token đầu tiên, ngay cả khi chúng chứa
thông tin đáng được chú ý.
Để chọn chính xác các đoạn hữu ích,
chúng tôi sử dụng sự tương đồng vốn có giữa các query cấp token và key cấp token để xây dựng
biểu diễn query và key cấp đoạn. Lấy
2

--- TRANG 3 ---
×N khối
…Chọn Đoạn cho  
1.0 0.4 0.5 0.1
KV Cache…Các Đoạn được chọn của mỗi Head 
Đoạn 1 Đoạn 2 Đoạn 3 Đoạn 4
Tạo Token TiếpQuery States 
Value States Key States 
Vector Query Đoạn
Biểu diễn Đoạn
Multi -Head
Attention
FlashAttention & Pooling
FlashAttention
Đoạn 1
FlashAttention & Pooling
FlashAttention
Đoạn 2
FlashAttention & Pooling
FlashAttention
Đoạn 3
FlashAttention & Pooling
FlashAttention
Đoạn 4
Hình 2: Tổng quan về suy luận của LONG HEADS, tạo token x14 trong bước hiện tại. Trong quá trình suy luận,
LONGHEADS giữ đoạn đầu tiên để tính toán ổn định, kết hợp với đoạn cuối chứa các token gần đây.
thí nghiệm Truy xuất Passkey 32K làm ví dụ,
đoạn chứa câu trả lời (tức là có giá trị
nhất) là đoạn có điểm chọn cao nhất trong 98% trường hợp mà không cần được huấn luyện.
2.2 Biểu diễn Đoạn
Biểu diễn đoạn là một chỉ báo xem liệu
các token trong đoạn này có nên được chú ý hay không. Chúng tôi có
được biểu diễn đoạn theo cách không cần huấn luyện
bằng cách sử dụng khả năng nội tại của attention.
Chính thức, cho một chuỗi đầu vào dài X=
(x1, ..., x n), chúng tôi phân đoạn nó thành các đoạn theo
kích thước đoạn đã định trước l, sau đó chuỗi đầu vào
có thể được ký hiệu là X= (C1, ..., C m), m=⌈n
l⌉.
Chúng tôi sử dụng key states của attention để tạo biểu diễn đoạn
cho mỗi đoạn do cơ chế attention
hiện có dựa vào query states. Có
nhiều phương pháp đơn giản để có được
biểu diễn đoạn, chẳng hạn như mean pooling của
các vector key của tất cả token trong đoạn. Tuy nhiên,
chúng đã cho thấy hiệu suất không tối ưu
trong các thí nghiệm sơ bộ, đặc biệt trong việc chọn
các đoạn đúng. Chúng tôi giả thuyết rằng điều này là
do tầm quan trọng của các token riêng lẻ
trong một đoạn khác nhau đáng kể.
Để giải quyết vấn đề trên, chúng tôi nên
xác định các token có thể đại diện cho toàn bộ đoạn.
Vì mục đích đó, chúng tôi đánh giá tầm quan trọng của mỗi token
đối với đoạn và thực hiện tổng hợp attention có tỷ lệ
trên tất cả key states của token để có được
một biểu diễn đoạn đại diện như sau:
ci=flash-attention (qc
i,Ki,Ki) (1)
trong đó ci∈Rm×d là biểu diễn đoạn,
Ki∈Rl×d là tất cả key states attention của đoạn
Ci, qc
i∈Rm×d là một vector query để chỉ ra key state
của token nào phù hợp để đại diện cho biểu diễn đoạn. Tiếp theo, chúng tôi mô tả cách
tạo vector query.
Một vector query đoạn tốt nên có thể
đại diện cho thông tin ngữ nghĩa đầy đủ của đoạn,
tức là vector giá trị của tất cả token trong toàn bộ
đoạn. Tuy nhiên, các token khác nhau không đóng góp
ngang nhau vào biểu diễn ngữ nghĩa, ví dụ: từ
nội dung có trọng lượng ngữ nghĩa cao hơn, trong khi
từ chức năng đóng góp ít hơn. Sử dụng sự
tương đồng dot-product vốn có giữa biểu diễn query và key cấp token, chúng tôi xây dựng
trọng số ngữ nghĩa cho mỗi token thông qua tổng hợp self-attention hai chiều. Từ góc độ
truyền thông điệp, các từ nội dung giàu ngữ nghĩa
sẽ truyền nhiều thông tin của chúng cho các
token khác, trong khi từ chức năng truyền ít. Cuối cùng, các vector query qc
i tóm tắt thành công
ngữ nghĩa hoàn chỉnh được thu được bằng mean-
pooling của các biểu diễn được tổng hợp, và có thể
được hình thức hóa như sau.
Oi=flash-attention (Qi,Ki,Vi)
qc
i=mean (Oi), (2)
trong đó Qi,Ki, và Vi∈Rl×d lần lượt là tất cả query states,
key states, và value states của đoạn Ci. Cả Ki và Vi đều có thể được truy cập trực tiếp
từ KV cache, trong khi Qi yêu cầu lưu trữ tạm thời trong quá trình tính toán biểu diễn của
đoạn hiện tại và được giải phóng sau đó.
2.3 Chiến lược Chọn Đoạn
Trong quá trình mã hóa hoặc tạo token tiếp theo
(ký hiệu bởi xj), chúng tôi sử dụng một chiến lược chọn đoạn
có nhận thức query, chọn k đoạn
liên quan nhất từ những đoạn đã được tạo. Dựa trên
kiến thức trước, có hai đoạn bắt buộc.
Một là phù hợp với phát hiện của Xiao et al. (2023), thừa nhận vai trò thiết yếu của một vài
3

--- TRANG 4 ---
token bắt đầu của một câu trong việc bảo tồn tính
ổn định của LLM. Nếu một vài token bắt đầu bị thiếu
khỏi ngữ cảnh, các LLM đã huấn luyện trước sẽ
hoàn toàn mất khả năng biểu đạt của chúng (tức là thể hiện
perplexity rất cao). Để đảm bảo sự trôi chảy, tất cả
attention head đồng nhất chọn đoạn đầu tiên (tức là
C1) của câu. Nếu không, LLM không thể
xử lý các nhiệm vụ downstream (như được chứng minh trong
Nghiên cứu Ablation). Đoạn khác là gán đoạn cuối
(tức là C−1) cho tất cả attention head, để
cung cấp cho mô hình thông tin cục bộ
cần thiết cho việc tạo ra.
Tiếp theo, chúng tôi chọn k−2 đoạn liên quan
nhất còn lại cho mỗi attention head. Trong mô-đun attention của LLM, điểm dot product phản ánh
mức độ liên quan của token ngữ cảnh đến token hiện tại. Lấy cảm hứng từ đó, chúng tôi chọn các đoạn mục tiêu bằng
sự tương đồng dot product giữa query state qj của token hiện tại và biểu diễn đoạn ci.
P={C1}∪{Ci|rank(qj·ci)≤k−2}∪{C−1},
(3)
trong đó P là tập hợp cuối cùng của các đoạn được chọn, và
hàm rank(·) xuất ra thứ hạng của sự tương đồng được tính toán của đoạn hiện tại trong số tất cả ứng viên.
Bằng cách này, các attention head khác nhau trên các lớp
tự nhiên chú ý đến các phần khác nhau của ngữ cảnh,
truy xuất các đoạn quan trọng cho suy luận.
Ánh xạ lại Vị trí. Có các đoạn văn bản trong
tập hợp P vượt quá độ dài huấn luyện trước, vì vậy
mã hóa vị trí của P cần được ánh xạ lại.
Tổng độ dài của các đoạn được chọn được kiểm soát
để nằm trong độ dài huấn luyện trước L, tức là k∗l < L.
Ở đây, LONGHEADS tái cấu trúc các đoạn được chọn
và nối chúng lại, trong khi bảo tồn thứ tự
ưu tiên. Trong Hình 3, head hiện tại
chú ý đến các đoạn (1,2,7,8) trong số tám đoạn ứng viên. Các vị trí được gán là [1,4l],
trái ngược với các vị trí văn bản gốc, vốn
sẽ là [1, l]∪[l+1,2l]∪[6l+1,7l]∪[7l+1,8l].
Ánh xạ lại vị trí tránh vấn đề ngoài phân phối
gặp phải khi mở rộng ngữ cảnh
ngay cả khi không cần huấn luyện thêm.
Các đoạn được chọn
Hình 3: Minh họa Ánh xạ lại Vị trí.2.4 Suy luận với L ONGHEADS
Chúng tôi mô tả riêng biệt việc mã hóa các đầu vào dài
và việc tạo ra các đầu ra dài trong quá trình
suy luận. Ở đây chúng tôi chỉ mô tả lớp
multi-head causal attention đã được sửa đổi.
Tính toán và Bộ nhớ trong Giai đoạn Mã hóa.
Khi LONG HEADS nhận các đầu vào dài, nó
đầu tiên tính toán các biểu diễn của tất cả đoạn song song. Điều này có thể được thực hiện nhanh chóng thông qua hai
lần chạy flash-attention, với số lượng token
tham gia attention bằng kích thước đoạn
(tức là l=256, nhỏ hơn nhiều so với độ dài
của đầu vào, ví dụ: n=16k). Bước thứ hai là
chọn k đoạn liên quan nhất cho mỗi query
dựa trên biểu diễn đoạn và để có được
biểu diễn key và value của chúng, làm cho cửa sổ attention bằng k∗l=w (ví dụ: w=2k, cũng
nhỏ hơn nhiều so với n). Cuối cùng, causal flash-attention hạn chế độ dài được thực hiện hiệu quả.
Tính toán và Bộ nhớ trong Giai đoạn Tạo ra.
Trong quá trình tạo ra, LONG HEADS đầu tiên
thực hiện chọn đoạn, sau đó tải các biểu diễn Key-Value
của k đoạn được chọn cho causal attention
hạn chế độ dài. Khi tạo ra với
các đầu vào rất lớn (ví dụ: 100K), KV cache (ngoại trừ
biểu diễn đoạn) có thể được chuyển sang CPU
để giảm đáng kể việc sử dụng bộ nhớ, và chúng tôi chỉ
tải các đoạn được chọn vào bộ nhớ GPU. Chúng tôi
luôn giữ biểu diễn query-key-value
của các token gần đây (không vượt quá kích thước đoạn)
trong quá trình tạo ra. Khi số lượng
token gần đây bằng kích thước đoạn, chúng tôi tính toán một
biểu diễn đoạn, tương tự như giai đoạn mã hóa,
và thêm nó vào các biểu diễn đoạn trước đó.
Nhìn chung, độ phức tạp thời gian xấp xỉ một
LLM với window attention O( w2) (kích thước cửa sổ
w bằng k∗l). Việc sử dụng bộ nhớ của giai đoạn giải mã
xấp xỉ O( n+w2), và có thể được giảm thêm
thành O( k∗l+w2), tránh sự gia tăng bậc hai
trong chi phí với độ dài chuỗi.
3 Thí nghiệm
Chúng tôi đánh giá LONG HEADS được đề xuất chủ yếu
sử dụng LLaMA-2 (Touvron et al., 2023) xem xét
việc áp dụng rộng rãi và phổ biến của nó. Hiệu quả
của LONGHEADS được đánh giá trên ba loại
nhiệm vụ: mô hình ngôn ngữ, nhiệm vụ truy xuất tổng hợp và điểm chuẩn ngữ cảnh dài.
4

--- TRANG 5 ---
PG19 Proof-pile
Phương pháp 4k 16k 32k 4k 16k 32k
Full Attention
PI-16K 7.42 6.72 >1032.98 2.61 >103
NTK 6.98 9.58 19.3 2.99 3.00 4.05
Restricted Attention
LLaMA-2-7B 6.98 >103>1032.99>103>103
LM-Infinite 6.98 7.33 7.75 2.99 2.96 3.10
Landmark 10.03 10.13 10.14 4.98 4.86 4.92
LONG HEADS 6.98 8.15 8.41 2.99 3.26 3.42
Bảng 1: Perplexity cửa sổ trượt của các phương pháp mở rộng cửa sổ ngữ cảnh khác nhau trên PG19 và Proof-pile.
LONG HEADS mở rộng cửa sổ ngữ cảnh của LLaMA-2 gốc
đến 32k với cửa sổ attention 2k.
3.1 Cài đặt
Triển khai. Phương pháp của chúng tôi được áp dụng cho
các mô hình cơ sở LLaMA-2-7B base và chat cho các nghiên cứu thực nghiệm. Trong thiết lập của chúng tôi, chúng tôi đặt kích thước của mỗi
đoạn l là 256. Trong mỗi bước suy luận, chúng tôi
sử dụng chiến lược chọn đoạn của mình để thực hiện
chọn đoạn có nhận thức query. Cho mỗi lựa chọn,
chúng tôi luôn chọn đoạn đầu tiên từ
văn bản dài để tạo điều kiện cho việc tạo ra bình thường bởi
mô hình, và đoạn cuối để cung cấp thông tin ngữ cảnh cục bộ. Đối với tất cả các nhiệm vụ đánh giá, suy luận
được thực hiện trên một GPU NVIDIA A100 duy nhất.
Đường cơ sở. Các loại đường cơ sở sau được
chọn để so sánh. 1) Phương pháp với full attention, bao gồm nội suy "Dynamic NTK" (NTK, Emozilla, 2023) và Position Interpolation
(PI, Chen et al., 2023a). 2) Phương pháp với restricted attention, bao gồm LM-Infinite (Han et al.,
2023) và Landmark-Attention (Mohtashami and
Jaggi, 2023). Chi tiết triển khai của các đường cơ sở có trong Phụ lục A.
3.2 Mô hình Ngôn ngữ Ngữ cảnh Dài
Thí nghiệm về mô hình ngôn ngữ ngữ cảnh dài
được thực hiện với hai tập dữ liệu: PG19 (Rae
et al., 2019) và tập dữ liệu Proof-pile (Azerbayev
et al., 2023). Chi tiết được hiển thị trong Phụ lục C.1.
Kết quả đánh giá được báo cáo trong Bảng 1.
Mặc dù PPL của mô hình LLaMA-2-7B-Base
và PI vẫn thấp trong độ dài ngữ cảnh
huấn luyện trước, nó tăng đáng kể khi ngữ cảnh
vượt quá cửa sổ này. Phương pháp NTK có thể duy trì các giá trị PPL thấp cho các chuỗi lên đến độ dài 16k,
nhưng PPL tăng đáng kể ở độ dài ngữ cảnh 32k. Ngược lại, LONG HEADS, Landmark Attention và
LM-infinite thành công duy trì điểm PPL thấp
4k 8k 12k 16k 20k 24k 28k 32k
Độ dài Ngữ cảnh0.00.20.40.60.81.0Độ chính xác
Fine-tuned
PI-16K
Landmark  
Training-free
Llama-2-7B
LM-Infinite
NTK
LongHeadsHình 4: Đánh giá nhiệm vụ truy xuất passkey ở
các độ dài ngữ cảnh khác nhau. LONGHEADS đạt được hiệu suất tương đương như Landmark Attention và vượt trội hơn
các phương pháp khác.
ngay cả ở độ dài chuỗi 32k.
3.3 Đánh giá dựa trên Truy xuất
Chúng tôi thực hiện thí nghiệm trên nhiệm vụ truy xuất passkey được giới thiệu bởi (Mohtashami and Jaggi, 2023).
Nhiệm vụ này thách thức một mô hình ngôn ngữ chính xác
định vị và truy xuất một passkey đơn giản (một số ngẫu nhiên năm chữ số) trong một chuỗi văn bản dài. Nó kiểm tra
xem LLM có thể chú ý hiệu quả đến thông tin
trên tất cả các vị trí của chuỗi đầu vào hay không. Theo
thiết kế của Mohtashami and Jaggi (2023),
passkey được đặt với các độ dài ngữ cảnh khác nhau
(từ 4k đến 32k với khoảng cách 4k). Cho
mỗi độ dài ngữ cảnh, chúng tôi thực hiện 50 thử nghiệm với
passkey được đặt tại vị trí ngẫu nhiên trong ngữ cảnh.
Trong Hình 4, chúng ta có thể thấy rằng tất cả các mô hình có thể
xuất passkey trong độ dài đã huấn luyện trước.
Mô hình cơ sở hoàn toàn thất bại ở độ dài mở rộng. NTK và LM-Infinite gây ra một
sự sụt giảm đáng kể trong độ chính xác cho các mô hình ở độ dài
vượt quá 6k token, với độ chính xác giảm xuống dưới
20% khi độ dài token vượt quá 16k. LM-Infinite
chỉ có thể truy cập 10% passkey với cửa sổ cục bộ của nó, mặc dù có PPL thấp ở độ dài 32k. Ngược lại, Landmark Attention và LONG HEADS
luôn truy xuất với gần như 100% độ chính xác
bất kể độ dài chuỗi.
Chúng tôi tiếp tục kiểm tra LONG HEADS ở độ dài 128k sau
khi chuyển KV cache sang CPU, kết quả được
hiển thị trong Phụ lục B. Chúng tôi lưu ý rằng LONG HEADS
chỉ sử dụng cửa sổ attention 2k đạt được 100% độ chính xác ở độ dài 128k mà không cần huấn luyện.
3.4 Đánh giá Điểm chuẩn Ngữ cảnh Dài
Các nhiệm vụ mô hình ngôn ngữ đã được chứng minh là
các chỉ số không đủ để đảm bảo thành công trong các nhiệm vụ downstream
5

--- TRANG 6 ---
Phương pháp FT TokensSingle-Doc QA Multi-Doc QA Summarization Few-shot Learning Synthetic CodeAvg.
NQA Qspr. MulFi HQA WMQA Musq. GRpt QMSM MulN TREC TriQA SMSM PsgC PsgR Lcc Repo
Full Attention
NTK - 16.47 29.62 31.42 31.31 28.75 10.20 22.70 17.65 6.31 64.67 77.36 37.95 3.99 5.12 65.64 52.97 31.38
PI-16k 0.85B 21.37 31.78 36.67 37.56 27.47 15.98 13.55 20.69 1.18 63.00 89.24 25.64 5.67 11.33 67.05 56.02 32.76
Restricted Attention
LM-Infinite - 14.34 20.75 26.18 20.37 20.08 5.87 16.70 7.01 2.28 54.67 76.69 15.64 4.30 7.00 62.90 52.74 25.47
Landmark 0.80B 11.35 23.91 20.96 26.95 26.25 5.22 17.74 19.15 9.84 42.67 80.73 35.45 5.73 7.00 59.74 42.76 27.22
LONGHEADS - 14.51 21.58 30.32 30.07 25.28 9.15 24.74 20.26 6.30 55.00 83.26 34.27 2.45 9.39 65.01 50.65 30.14
w/ NTK init - 16.48 28.63 31.36 31.19 28.67 13.54 22.85 17.63 6.38 65.33 77.49 38.07 4.32 4.97 65.56 52.87 31.58
w/ PI init 0.85B 21.43 31.78 36.64 37.63 27.33 15.98 13.36 20.57 1.30 63.00 89.57 25.86 5.67 11.33 66.93 48.96 32.33
Mở rộng đến 32k
NTK - 5.74 29.05 31.39 28.98 27.03 9.34 22.00 15.13 5.40 64.67 48.34 34.50 3.89 4.85 57.54 45.29 27.07
PI-16k 0.85B 8.43 30.15 35.20 29.47 24.72 1.74 13.23 12.59 1.30 55.00 66.15 19.16 5.42 11.33 33.21 27.21 23.39
LM-Infinite - 10.87 20.58 26.19 19.48 20.40 16.52 5.26 2.51 6.14 55.00 82.78 11.26 4.30 6.67 64.88 56.02 25.55
Landmark 0.80B 13.88 23.69 21.06 28.04 25.78 11.52 17.70 19.11 10.68 41.00 77.15 35.61 5.70 7.00 58.22 40.97 27.32
LONGHEADS - 13.38 21.81 30.33 29.59 24.90 11.48 27.43 19.87 6.07 55.00 81.15 33.56 2.79 10.06 63.75 47.97 29.95
Bảng 2: Kết quả của các phương pháp khác nhau dựa trên mô hình LLaMA-2-7B-Base trên LongBench. FT Tokens
chỉ ra số lượng token được sử dụng cho huấn luyện liên tục. Kích thước cửa sổ ngữ cảnh cho L ONG HEADS là 4k.
(Sun et al., 2021), trong khi các nhiệm vụ truy xuất mật khẩu tổng hợp thường không phù hợp với các tình huống thực tế. Việc thực hiện đánh giá nhiệm vụ downstream thực tế là quan trọng để phản ánh toàn diện hơn khả năng chuỗi dài của mô hình. Chúng tôi
chọn LongBench (Bai et al., 2023) cho đánh giá nhiệm vụ NLP downstream, chi tiết được hiển thị trong Phụ lục C.2. Kết quả được liệt kê trong Bảng 2. Chúng tôi
cũng thực hiện thí nghiệm trên mô hình LLaMA-2-7B-Chat,
và kết quả được hiển thị trong Phụ lục E.
So sánh với Các Phương pháp Restricted Attention.
LONG HEADS vượt trội hơn các phương pháp hiện tại với
restricted attention. Cụ thể, LONGHEADS thực hiện tốt hơn phương pháp với cơ chế cửa sổ trượt trên LongBench (+4.67 so với LM-
Infinite). So với phương pháp với chiến lược chunking (tức là Landmark Attention), LONG HEADS
vượt trội điểm trung bình 2.92 trên LongBench
mà không cần huấn luyện bổ sung. Điều này chỉ ra rằng chiến lược chọn đoạn trong LONGHEADS có thể chính xác bổ sung cho LLM thông tin ngữ cảnh liên quan, cho phép hiểu hiệu quả và hiệu quả trên các chuỗi dài.
So sánh với Các Phương pháp Full Attention. Các phương pháp full attention có thể tăng độ dài chuỗi tối đa của LLM nhưng cũng làm tăng chi phí tính toán và bộ nhớ. LONG HEADS có thể được
tăng cường với các phương pháp PI hoặc NTK trong giai đoạn mã
hóa, đạt được kết quả tương đương hoặc thậm chí tốt hơn với kích thước cửa sổ ngắn hơn, giảm đáng kể chi phí tính toán. Điều này gợi ý rằng
LONG HEADS có tiềm năng mở rộng, và
có thể được tăng cường với mô hình cơ sở mạnh hơn.Hiệu suất khi mở rộng đến Cửa sổ ngữ cảnh 32k. Một thuộc tính mong muốn cho các phương pháp mở rộng RoPE là các mô hình nên duy trì hiệu suất của chúng khi mở rộng trực tiếp
đến cửa sổ ngữ cảnh dài hơn. Khi mở rộng đến
cửa sổ ngữ cảnh 32k, các phương pháp PI và NTK gặp
khó khăn với vấn đề ngoài demonstration và có xu hướng
làm giảm hiệu suất mô hình. Ngược lại,
LONG HEADS duy trì hiệu suất của nó và vượt trội
hơn tất cả các phương pháp đường cơ sở. Nó thành công
mở rộng LLaMA-2-7B-Base từ độ dài 4K đến 8
lần độ dài của nó, chứng minh rằng LONG HEADS
có thể dễ dàng khái quát hóa đến cửa sổ ngữ cảnh dài hơn.
4 Thảo luận
4.1 Phân tích
Trong phần này, chúng tôi khám phá cách các attention head khác nhau xử lý ngữ cảnh dài và liệu chúng có tìm thấy thông tin quan trọng hay không. Chúng tôi đặt cửa sổ attention của LONG HEADS thành 2048 và phân tích hiệu suất của nó trên các nhiệm vụ truy xuất passkey và tóm tắt. Chúng tôi
hiển thị các thử nghiệm cho cả hai nhiệm vụ trong Hình 5 và
hiển thị kết quả thống kê trong Bảng 3. Chi tiết
của các thí nghiệm phân tích có trong Phụ lục D.
Attention head tập trung vào các phần quan trọng trong ngữ
cảnh. Trên nhiệm vụ truy xuất passkey, được hiển thị trong Hình
5(a), tất cả attention head tập trung vào cùng một
đoạn chứa câu trả lời và dự đoán nó chính xác. Ngay cả khi passkey không được dự đoán thành công trong Hình 5(b), các đoạn chứa
câu trả lời vẫn được chọn bởi nhiều head. Ngược lại, trên nhiệm vụ tóm tắt trong Hình 5(c), các
attention head phân tán sự tập trung của chúng đồng đều hơn để
6

--- TRANG 7 ---
012345678910111213141516171819202122232425262728293031
# Đoạn31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0# Lớp
(a) Nhiệm vụ Truy xuất Passkey (Thành công)012345678910111213141516171819202122232425262728293031
# Đoạn31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0# Lớp
(b) Nhiệm vụ Truy xuất Passkey (Thất bại)012345678910111213141516171819202122232425262728293031
# Đoạn31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0# Lớp
(c) Nhiệm vụ Tóm tắt
051015202530
# Attention HeadsHình 5: Trực quan hóa các đoạn được chọn bởi các attention head khác nhau tại mỗi lớp được biểu diễn bằng các khối màu. Đối với
nhiệm vụ truy xuất passkey, đoạn chứa passkey được phân định bằng đường viền đỏ. Đối với ví dụ thất bại,
đường viền đỏ bao gồm hai đoạn do câu chứa passkey tình cờ trải rộng trên hai đoạn.
Độ dài Đầu vào Tỷ lệ Bao phủĐồng nhấtTỷ lệ Trúng
Top 1 Top 5
Truy xuất Passkey
4k 100 0.52 0.55 0.96
8k 100 0.52 0.89 0.96
16k 99.2 0.60 0.99 1.00
32k 82.0 0.76 0.98 0.98
Tóm tắt
4k 100 0.31 / /
8k 100 0.44 / /
16k 100 0.49 / /
32k 100 0.57 / /
Bảng 3: Kết quả thống kê với các độ dài chuỗi khác nhau. Tỷ lệ Bao phủ được định nghĩa là phần trăm của các đoạn được chọn trong tổng số đoạn. Tính Đồng nhất của phân phối lựa chọn đoạn được đánh giá bằng hệ số Gini, với các giá trị thấp hơn chỉ ra phân phối đồng nhất hơn. Tỷ lệ Trúng có nghĩa là xác suất mà các đoạn được chọn top-1 và top-5 chứa câu trả lời đúng trong nhiệm vụ truy xuất khóa quá khứ.
tóm tắt toàn bộ thông tin. Tương tự, Bảng
3 tiết lộ điểm đồng nhất thấp hơn cho nhiệm vụ tóm tắt so với nhiệm vụ truy xuất passkey. Những phát hiện này gợi ý rằng chiến lược chọn đoạn của chúng tôi
dẫn đến phân phối lựa chọn đồng nhất hơn trong nhiệm vụ tóm tắt, trong khi phân phối trong
nhiệm vụ truy xuất passkey tập trung hơn. Chúng tôi
quy điều này cho tính đặc hiệu của các đoạn yêu cầu
cho nhiệm vụ truy xuất passkey, trong khi nhiệm vụ tóm
tắt đòi hỏi các phần khác nhau của văn bản để
hình thành câu trả lời toàn diện. Hơn nữa, xác suất của 5 đoạn được chọn hàng đầu chứa
câu trả lời gần như 100% trên tất cả độ dài thử nghiệm
trong Bảng 3. Những kết quả này gợi ý rằng chiến lược chọn đoạn của chúng tôi thích ứng phù hợp với đặc điểm của
các nhiệm vụ khác nhau, và cho phép các attention head khác nhau tập trung vào nội dung liên quan đến nhiệm vụ.Attention head có thể xử lý các chuỗi dài trong một
cửa sổ ngắn. Trong Hình 5, các attention head lớp thấp hơn tập trung vào văn bản phân tán hơn trong cả hai nhiệm vụ, trong khi các attention head lớp trên tập trung
hơn vào các đoạn cụ thể. Chúng tôi suy đoán rằng các
attention head khác nhau tự nhiên tập trung vào các
phần khác nhau của thông tin trong văn bản ở các lớp thấp hơn,
thu thập và tổng hợp toàn bộ thông tin tài liệu dài
trong một độ dài ngắn, trong khi các attention head lớp trên chịu trách nhiệm xử lý thông tin được tổng hợp, chủ yếu tập trung vào
các đoạn cần thiết để hoàn thành nhiệm vụ. Trong Bảng
3, Tỷ lệ Bao phủ là 100% trong hầu hết trường hợp. Cho rằng các head khác nhau trong mỗi lớp có thể chọn các
đoạn khác nhau, độ dài tối đa lý thuyết có thể truy cập bởi LONGHEADS là |P| ×n_heads×n_layers
(ví dụ: độ dài tối đa cho LLaMA-2-7B với
cửa sổ attention 4k là 512k). Những quan sát này chứng minh rằng chúng ta đã thành công sử dụng một
cửa sổ attention hạn chế để nắm bắt gần như tất cả thông tin từ toàn bộ tài liệu dài.
4.2 Nghiên cứu Ablation
Chúng tôi thực hiện thí nghiệm ablation để điều tra
ảnh hưởng của chiến lược chọn đoạn, tính linh hoạt của attention head, số lượng đoạn K, và kích thước đoạn l. Nghiên cứu ablation được xây dựng trên Long-
Bench và kết quả được trình bày trong Bảng 4.
Ảnh hưởng của Chiến lược Chọn Đoạn. Chúng tôi thấy
rằng hiệu suất khi chọn các đoạn có điểm cao nhất vượt trội đáng kể so với các đoạn có điểm thấp nhất (Last K), và ngay cả Lựa chọn Ngẫu nhiên cũng mang lại kết quả tốt hơn so với Lựa chọn Last K.
Chúng tôi cũng quan sát thấy sự suy giảm hiệu suất đáng kể khi đoạn đầu tiên không được bảo tồn. Điều này là
7

--- TRANG 8 ---
Phương pháp Cài đặt LongBench Avg.
LONG HEADS 30.14
- Lựa chọn Ngẫu nhiên 28.77
- Lựa chọn Last K 26.22
- w/o Lựa chọn Đầu tiên 14.06
- Fix Head 29.46
- Fix Layer 28.78
- Fix Head & Layer 28.72
- Số lượng Đoạn K= 8 29.09
- Số lượng Đoạn K= 4 26.64
- Kích thước Đoạn l= 512 29.95
- Kích thước Đoạn l= 128 29.35
Bảng 4: Nghiên cứu ablation trên LongBench, theo mặc định
l= 256, K= 16, và Lựa chọn Top K.
vì việc thiếu đoạn đầu tiên dẫn đến sự sụp đổ trực tiếp của phân phối đầu ra của mô hình. Phát hiện của chúng tôi nhất quán với StreamingLLM (Xiao
et al., 2023) và LM-Infinite (Han et al., 2023).
Ảnh hưởng của Tính linh hoạt của Head. Khi tính linh hoạt
của attention head bị hạn chế, hiệu suất của mô hình
bị ảnh hưởng ở các mức độ khác nhau (-0.68
Fix Head, -1.36 Fix Layer, -1.42 Fix Head&Layer).
Điều này chứng minh rằng trong khung LONG HEADS, sự hợp tác của các attention head khác nhau trong mỗi lớp đóng vai trò quan trọng.
Ảnh hưởng của Số lượng Đoạn & Kích thước Đoạn. Tăng số lượng đoạn trong một văn bản có thể cung cấp thêm thông tin, nhưng lợi ích cho thấy sự giảm dần. Điều này chỉ ra rằng bốn đoạn
cung cấp đủ thông tin để đảm bảo hiệu suất,
và tám đoạn đã đủ để truy cập
thông tin của toàn bộ chuỗi với chiến lược chọn đoạn, Các kích thước đoạn khác nhau không dẫn đến
tác động đáng kể đến kết quả, chỉ ra rằng kích thước đoạn lớn hơn hoặc
nhỏ hơn đều khả thi cho L ONG HEADS.
5 Nghiên cứu liên quan
Mở rộng Mã hóa Vị trí (PE). Các nghiên cứu mở rộng ngữ cảnh thường nhắm đến mã hóa RoPE phổ biến, với mục tiêu mở rộng PE chưa thấy vào
không gian của các vị trí đã thấy trong quá trình huấn luyện trước. Chen
et al. (2023a), và đồng thời kaiokendev (2023)
phát hiện rằng việc nội suy các chỉ số vị trí
trong giới hạn đã huấn luyện trước hoạt động tốt với sự trợ giúp của một lượng nhỏ (một vài tỷ, Chen et al.,
2023a) fine-tuning. Tuy nhiên, nội suy vị trí
(PI) kéo giãn đều tất cả các chiều của RoPE,
bỏ qua các biến đổi trong tần số. Như một lựa chọn thay thế, Bloc97 (2023b) đề xuất nội suy "NTK-aware"
bằng cách tính đến mất mát của các thành phần tần số cao. Sau đó, Emozilla
(2023) đề xuất phương pháp nội suy "Dynamic NTK", hoạt động tốt mà không cần
fine-tuning. Bloc97 (2023a) giới thiệu phương pháp nội suy "NTK-
by-parts", hoạt động tốt nhất khi được fine-tune trên một lượng nhỏ dữ liệu ngữ cảnh dài hơn. Peng et al. (2023) đề xuất YaRN,
một phương pháp cải tiến để mở rộng hiệu quả cửa sổ ngữ cảnh bằng fine-tuning trên ít hơn 0.1% dữ liệu huấn luyện trước gốc. Nghiên cứu này trực tiếp
sửa đổi PE để mở rộng đến độ dài ngữ cảnh vô hạn về mặt lý thuyết. Ngược lại, phương pháp của chúng tôi không yêu cầu sửa đổi PE, và chỉ một đoạn hữu hạn
tham gia vào tính toán attention trong quá trình tạo ra, điều này cải thiện hiệu quả suy luận và
giảm việc sử dụng bộ nhớ.
Restricted Attention. Ngoài ra, global
causal attention có thể được hạn chế thành local atten-
tion, do đó tránh vượt quá độ dài vị trí đã huấn luyện trước. ReRoPE (Su, 2023) cắt ngắn tất cả độ dài ngữ cảnh đến độ dài tối đa trong quá trình huấn luyện trước.
LM-Infinite (Han et al., 2023) hạn chế cửa sổ attention toàn cục thành một cửa sổ hình chữ V,
chỉ giữ lại một vài token từ đầu văn bản và một cửa sổ cục bộ. Mohtashami and Jaggi
(2023) chèn một token landmark có thể học được sau mỗi
đoạn văn bản với độ dài cố định, và sử dụng những
landmark này để truy xuất các đoạn liên quan. Zhang
et al. (2024) tương tự chèn một token beacon có thể học được và sử dụng biểu diễn của nó để tóm tắt toàn bộ đoạn tương ứng. Mặc dù restricted at-
tention mang lại lợi thế về việc sử dụng bộ nhớ
và tốc độ suy luận, chúng có nguy cơ mất thông tin ngữ cảnh có giá trị. Các phương pháp hiện tại sử dụng các
cửa sổ cục bộ hoặc cố định hoặc được chọn thông qua
fine-tuning. Trong phương pháp của chúng tôi, các cửa sổ cục bộ được
kết hợp linh hoạt từ các đoạn từ ngữ cảnh và
không dựa vào fine-tuning bổ sung.
6 Kết luận
Chúng tôi trình bày LONG HEADS, một khung làm việc mới, không cần huấn luyện
để xử lý hiệu quả ngữ cảnh dài trong các LLM đã huấn luyện trước. Sử dụng khả năng nội tại
của attention head, LONG HEADS khéo léo
phân đoạn và gán văn bản dài cho các head liên quan,
hợp lý hóa việc xử lý các chuỗi mở rộng mà không có tải tính toán bổ sung. Kết quả thí nghiệm xác nhận sự vượt trội của LONG HEADS trong
các thiết lập restricted attention và lợi thế cạnh tranh của nó so với các phương pháp full attention khi áp dụng cho bộ LongBench. Phương pháp của chúng tôi mở đường cho
8

--- TRANG 9 ---
các đột phá hiệu suất trong các hoạt động LLM ngữ cảnh dài,
tận dụng các cấu trúc mô hình hiện có để mở khóa tiềm năng mới mà không cần huấn luyện thêm.
Hạn chế
Chúng tôi tóm tắt các hạn chế của phương pháp của chúng tôi như
sau: (1) Việc chia văn bản thành các đoạn có thể
phá vỡ tính liên tục của nội dung. Khi
câu trả lời đúng nằm ở giữa hai đoạn,
loại chia tách này có thể ảnh hưởng đến hiệu suất
của các nhiệm vụ downstream. (2) Độ dài tối đa lý thuyết có thể truy cập bởi LONG HEADS bị giới hạn ở
|P| ×n_heads×n_layers. LONG HEADS không thể
truy cập đầy đủ các đầu vào vượt quá ngưỡng này. Tuy
nhiên, LONG HEADS vẫn có thể hoạt động tốt trên các nhiệm vụ tài liệu dài bằng cách chọn các phần quan trọng từ
ngữ cảnh. (3) Sự thành công của LONG HEADS trong
các nhiệm vụ downstream phụ thuộc vào hàm chọn đoạn không tham số. Đối với các nhiệm vụ hiểu phức tạp, hiệu quả của hàm chọn
có thể bị ảnh hưởng.
Tài liệu tham khảo
Zhangir Azerbayev, Bartosz Piotrowski, Hailey
Schoelkopf, Edward W. Ayers, Dragomir Radev, and
Jeremy Avigad. 2023. Proofnet: Autoformalizing
and formally proving undergraduate-level mathemat-
ics.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
and Juanzi Li. 2023. Longbench: A bilingual, multi-
task benchmark for long context understanding.
Bloc97. 2023a. Add NTK-Aware interpolation "by
parts" correction.
Bloc97. 2023b. NTK-Aware Scaled RoPE allows
LLaMA models to have extended (8k+) context size
without any fine-tuning and minimal perplexity degra-
dation.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023a. Extending context window
of large language models via positional interpolation.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-
glora: Efficient fine-tuning of long-context large lan-
guage models.
Together Computer. 2023. Redpajama: An open
source recipe to reproduce llama training dataset.
https://github.com/togethercomputer/
RedPajama-Data.Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong
Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and
Zhifang Sui. 2023. A survey on in-context learning.
Emozilla. 2023. Dynamically Scaled RoPE further in-
creases performance of long context LLaMA with
zero fine-tuning.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,
Meng Wang, and Haofen Wang. 2024. Retrieval-
augmented generation for large language models: A
survey.
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng
Ji, and Sinong Wang. 2023. Lm-infinite: Simple
on-the-fly length generalization for large language
models.
Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng
Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen,
and Xia Hu. 2024. Llm maybe longlm: Self-extend
llm context window without tuning.
kaiokendev. 2023. Things i ´m learning while training
superhot.
Amirkeivan Mohtashami and Martin Jaggi. 2023. Land-
mark attention: Random-access infinite context
length for transformers.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and En-
rico Shippole. 2023. Yarn: Efficient context window
extension of large language models.
Ofir Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In International Confer-
ence on Learning Representations.
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen,
Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,
Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,
Huadong Wang, Cheng Qian, Runchu Tian, Kunlun
Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen
Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,
Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,
Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,
Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng
Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and
Maosong Sun. 2023. Tool learning with foundation
models.
Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku-
mar, and Timothy P. Lillicrap. 2019. Compressive
transformers for long-range sequence modelling.
Jianlin Su. 2023. Rectified rotary position embeddings.
https://github.com/bojone/rerope.
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-
Micke, and Mohit Iyyer. 2021. Do long-range lan-
guage models actually use long-range context?
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-
han Huang, Alon Benhaim, Vishrav Chaudhary, Xia
Song, and Furu Wei. 2022. A length-extrapolatable
transformer.
9

--- TRANG 10 ---
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2023. Efficient streaming
language models with attention sinks.
Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao,
Qiwei Ye, and Zhicheng Dou. 2024. Soaring from
4k to 400k: Extending llm's context with activation
beacon.
A Chi tiết Triển khai Đường cơ sở
Chúng tôi thực hiện thí nghiệm trên 4 phương pháp làm đường
cơ sở. Chúng tôi minh họa chi tiết của mỗi đường cơ sở như
sau:
Đối với NTK, chúng tôi đặt hệ số tỷ lệ của NTK thành 2.0
cho mô hình cơ sở và 1.0 cho mô hình chat. Đối với LM-
Infinite, chúng tôi đặt số lượng token đầu tiên được bảo tồn thành 10 và cửa sổ cục bộ ở cuối thành 4096
token. Trong bối cảnh các phương pháp không cần huấn luyện,
chúng tôi không đánh giá StreamingLLM (Xiao et al.,
2023) vì khung của họ không hỗ trợ đầu vào
vượt quá 4K token, và phương pháp của họ tương tự
với LM-Infinite. Đối với phương pháp Position Interpolation
thực hiện trên ngữ cảnh 8K và 16K, chúng tôi sử dụng tập dữ liệu Red-
pajama (Computer, 2023) để huấn luyện. Theo
(Chen et al., 2023b), chúng tôi đặt kích thước batch mỗi thiết bị là 1 và bước tích lũy gradient là
8, có nghĩa là kích thước batch toàn cục bằng
64, sử dụng 8 GPU. Chúng tôi huấn luyện các mô hình trong 1000
bước. Đối với Landmark-Attention, chúng tôi áp dụng cài đặt cấu hình của họ để đảm bảo tính nhất quán. Chúng tôi fine-tune
mô hình LLaMA-2-7B Base trong 15000 bước sử dụng
phương pháp của họ. Chúng tôi fine-tune mô hình với độ dài ngữ cảnh 512 trên tập dữ liệu Redpajama.
4k 16k 32k 64k 128k
Độ dài Ngữ cảnh0.00.20.40.60.81.0Độ chính xác
Llama-2-7B
LongHeadsLlama-2-7B
LongHeadsHình 6: Đánh giá nhiệm vụ truy xuất passkey từ
4k đến 128k.
B Đánh giá Truy xuất Passkey trên ngữ cảnh 128k
Chúng tôi tiếp tục mở rộng LLaMA-2-7b đến 128k với Long-
Heads mà không cần huấn luyện bổ sung. LONG HEADS
đạt được 100% độ chính xác ở độ dài 128k trên nhiệm vụ truy xuất passkey, kết quả được hiển thị trong Hình 6.
Sau khi chuyển KV cache sang CPU, việc sử dụng bộ nhớ GPU đỉnh là 26.51GB và 44.48 GB khi
suy luận với ngữ cảnh 64k và 128k.
C Chi tiết Đánh giá
C.1 Chi tiết Đánh giá Mô hình Ngôn ngữ
Chúng tôi đánh giá hiệu suất mô hình ngôn ngữ ngữ cảnh dài trên tập dữ liệu sách PG19 (Rae
et al., 2019) và tập dữ liệu Arxiv Math proof-pile đã được làm sạch (Azerbayev et al., 2023). Đối với cả hai tập dữ liệu,
một tập con của một trăm mẫu từ
thử nghiệm được sử dụng để đánh giá năng lực mô hình ngôn ngữ. Theo (Press et al., 2022), chúng tôi đánh giá
perplexity bằng cách sử dụng phương pháp cửa sổ trượt với
S = 256.
C.2 Chi tiết Đánh giá Điểm chuẩn Ngữ cảnh Dài
Theo Jin et al. (2024); Zhang et al. (2024), chúng tôi
chọn Longbench (Bai et al., 2023) cho đánh giá nhiệm vụ NLP downstream, bao gồm Hỏi đáp Tài liệu Đơn
(QA), Hỏi đáp Đa Tài liệu, Tóm tắt, Học Few-shot, và Hoàn thành Mã. Để đảm bảo đánh giá cân bằng và hợp lý hơn về khả năng văn bản dài của mô hình, chúng tôi sử dụng các nhiệm vụ từ LongBench-E để thay thế
các nhiệm vụ tương ứng trong Longbench cho thử nghiệm của chúng tôi. Chúng tôi tuân theo LongBench (Bai et al., 2023) để
đánh giá các mô hình trên kích thước cửa sổ ngữ cảnh 16k bằng cách cắt ngắn prompt từ giữa khi độ dài nhiệm vụ vượt quá kích thước cửa sổ ngữ cảnh được chỉ định
10

--- TRANG 11 ---
Phương pháp FT TokensSingle-Doc QA Multi-Doc QA Summarization Few-shot Learning Synthetic CodeAvg.
NQA Qspr. MulFi HQA WMQA Musq. GRpt QMSM MulN TREC TriQA SMSM PsgC PsgR Lcc Repo
Chat Model
LM-Infinite - 0.00 18.57 25.33 9.87 11.73 0.48 11.30 2.99 8.72 32.50 29.22 13.82 5.61 5.20 34.19 24.55 14.63
NTK - 15.18 30.89 36.14 35.10 25.79 13.53 31.48 20.21 23.86 61.67 80.94 39.43 7.40 13.33 48.96 42.45 32.90
LONGHEADS - 11.61 22.98 23.76 31.28 24.10 8.87 25.36 20.24 16.18 50.67 79.98 36.74 6.39 9.67 53.85 44.22 29.12
w/ NTK init - 16.87 30.32 38.59 36.04 26.72 10.21 31.28 20.91 24.46 55.67 76.72 39.07 6.07 14.67 49.97 40.27 32.37
Bảng 5: Kết quả của các phương pháp khác nhau dựa trên mô hình LLaMA-2-7B-Chat trên LongBench.
kích thước.
D Chi tiết Thí nghiệm Phân tích
Chúng tôi thực hiện thí nghiệm phân tích trên các nhiệm vụ
truy xuất passkey và tóm tắt. Đối với nhiệm vụ truy xuất passkey, chúng tôi biên soạn thống kê cho kết quả
với độ dài chuỗi 4k, 8k, 16k, và 32k, như
được đề cập trong Phần 3.3. Liên quan đến nhiệm vụ tóm tắt, chúng tôi chọn tập dữ liệu báo cáo chính phủ từ
LongBench, từ đó chúng tôi chọn 5 mẫu
mỗi loại cho độ dài 4k, 8k, 16k, và 32k để phân tích thống kê.
E Thêm Kết quả trên LongBench
Bảng 5 cho thấy rằng LONG HEADS cũng có hiệu suất mạnh trên các mô hình LLaMA2-7b-Chat. Khi
mã hóa được tăng cường với NTK, LONG HEADS có thể
đạt được hiệu suất tương đương với phương pháp full attention.
11
