# 2407.17678.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2407.17678.pdf
# File size: 662112 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
S2-A TTENTION : HARDWARE -AWARE CONTEXT
SHARDING AMONG ATTENTION HEADS
Xihui Lin1∗, Yunan Zhang1∗, Suyu Ge2∗
Liliang Ren1, Barun Patra1, Vishrav Chaudhary1, Hao Peng2, Xia Song1
1Microsoft,2UIUC
{xihlin,yunanzhang}@microsoft.com
ABSTRACT
Sparse attention, which selectively attends to a subset of tokens in the context, has
been an established approach to enhance the efficiency of Transformers. However,
its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up
over its dense attention counterparts, mainly due to the lack of hardware-level
optimizations like FlashAttention (Dao, 2023). Meanwhile, it remains unclear
whether sparse attention can maintain the model’s quality at the scale of today’s
large language models (LLMs), and how this can be achieved. This paper presents
Sparsely-Sharded Attention ( S2-A TTENTION ), an optimized Triton kernel library
providing a variety of customizable sparse attention implementations for both
training and inference. S2-A TTENTION allows customizing the attention patterns at
per head per context range level. The fresh insights from S2-A TTENTION inspire a
novel sparse attention architecture that meets several desiderata that we find crucial
for achieving both practical efficiency gains and strong accuracy on downstream
tasks, called as Head-Heterogenous Strided Transformer ( HHST ). For higher
sparsity, HHST shards the context heterogeneously across attention heads, where
each head attends to a different subset of tokens while collectively covering the
whole. We evaluate HHST by pretraining 1.3B and 7B sized models. For attention
computation, HHST with S2-A TTENTION achieves 8.8 ×and 15.9 ×wall-clock
attention speedup, as well as 2.8 ×and 2.5 ×training time reduction compared to a
dense attention baseline implemented with FlashAttention-2. Moreover, HHST ’s
downstream task performance is on-par with dense attention, and achieves a perfect
retrieval accuracy at a 128K context length at 7B scale. At inference, our 7B
HHST , achieves a 4.5 ×speed-up compared to the dense counterparts in vLLM. S2-
ATTENTION is released with easy-to-customize APIs for direct usage in Megatron
and vLLM.
1 I NTRODUCTION
Transformer-based LLMs have opened up fresh opportunities to both research and applications (Ope-
nAI, 2023; Touvron et al., 2023). Their quadratic complexity imposes prohibitive cost in the training
and serving these models. For example, training Llama 2 (Touvron et al., 2023) 70B with a 4K
context length on 2T tokens takes 23 days on 2048 A100 GPUs Rucinski (2024). When serving, the
model’s KV cache consumes 343GB GPU memory with a 32 batch size and 4K context length. There
is an urgent demand for training LLMs efficiently and serving them cost-effectively.
Many established works have managed to, at least on paper, improve the efficiency of these models
through various sparse attention techniques (Tay et al., 2023; Child et al., 2019; Beltagy et al., 2020;
Zaheer et al., 2020), where only a subset of the tokens in the context are attended to. However, their
theoretical FLOP savings compared to full-context dense attention often fail to deliver real-world
efficiency gains. As pointed out by the seminal work FlashAttention (Dao et al., 2022; Dao, 2023),
∗Leading Authors. Xihui Lin, Yunan Zhang, and Suyu Ge contribute equally. Code is available at
https://github.com/linxihui/dkernel
1arXiv:2407.17678v7  [cs.CL]  5 Feb 2025

--- PAGE 2 ---
Thread 
Block 0
Thread 
Block 1
Thread 
Block 2
Thread 
Block 3Head 0
Head 1
Head 2
Head 3
FlashAttentionS2-AttentionBlock 1 …
Head 0
Head 1
Head 2
Head 3Thread 
Block 0
Thread 
Block 1
Thread 
Block 2
Thread 
Block 3Block 2 Block 1 …Block 2Figure 1: Illustration of S2-Attention with four attention heads on a hypothetical GPU with 4 thread
blocks. Each attention head is allocated with a shard of the context.
8k 16k 32k 64k
Sequence Length8k 16k 32k 64k
Sequence Length1600
800
400
400
200
100
0S2-Attention
FlashAttention-23500
3000
2500
2000
1500
1000
500
0S2-Attention
FlashAttention-2Latency (ms)
Latency (ms)Forward Wall-clock Speedup (70B) Backward Wall-clock Speedup (70B)
37.6X
28.8X
19.6X
11.9X22.7X
16.2X
10.6X 6.4X
(a) Attention Latency Benchmark over FlashAttention-2.
4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense12-20
0.00.20.40.60.81.0
Score
(b) Perfect 128k Needle in a haystack.
Figure 2: Training Efficiency and long-context analysis of S2-Attention. Our model, implemented
with our kernel, achieves substantial reduction in latency compared to FlashAttention-2 (a). It also
achieves perfect retrieval performance at a 128K context length (b).
the major overhead in attention arises not from computation but from GPU memory access, especially
the shared memory access (SRAM). Dense attention has benefited from CUDA-level implementations
specifically optimized for efficient memory IO, a significant advantage that sparse attention methods
have yet to receive. The absence of a flexible, efficient, and easy-to-use library for optimized sparse
attention implementations has become a major roadblock, delaying progress in both research and
applications in improving LLMs’ training and serving efficiency.
We aim to bridge this gap with Sparsely-Sharded Attention ( S2-A TTENTION ), a Triton library that
provides kernel optimization for sparse attention. It is highly flexible, allowing practitioners to explore
various sparse attention strategies and customize different attention patterns across attention heads
and context ranges. Building a general-purpose fused kernel for sparse attention presents substantial
challenges. In sparse attention, part of the context is not attended to. As a result, tiling the Q,K,
Vtensors, a proven technique that divides large tensors into smaller ones for better parallelization
and shared memory (SRAM) usage (Dao et al., 2022; Dao, 2023), can often result in idling threads
and inefficient SRAM usage when the tile size is small. S2-A TTENTION addresses this by efficiently
tracking KV usage patterns and dynamically merging query blocks with shared KVs into the same
tile. This ensures the IO efficiency, regardless of the sparsity granularity, significantly improving
SRAM utilization and reducing redundant KV loading.
The insights from the development of S2-A TTENTION reveals that not all sparse attention mechanisms
are efficient in practice . Many existing training-free sparse attention, including KV eviction methods
such as LongGen (Ge et al., 2024b), H2O (Zhang et al., 2023), and MInference (Jiang et al., 2024),
are less compatible with foundational serving mechanisms like continuous batching (Yu et al.,
2022), PagedAttention (Kwon et al., 2023), 3D parallelism (Shoeybi et al., 2020). For example, in
PagedAttention (Kwon et al., 2023), evicting tokens from KV blocks would only increase internal
fragmentation, and bring extra overhead in scheduling, which in turn hurts serving throughput.
Meanwhile, recent studies show that training free sparse attention would hurt model’s long context
capabilities (Xiao et al., 2024; Ge et al., 2024a; Han et al., 2024). This has become a primary reason
why they have limited adoption in industry serving and opens-source inference systems to date (Kwon
et al., 2023; Zheng et al., 2023).
2

--- PAGE 3 ---
These fresh insights lead to Head-Heterogenous Strided Transforme ( HHST ), a new sparse attention
approach following key design principles (§4.1), which we find crucial for achieving efficiency gains
in practice while maintaining strong accuracy on downstream tasks:
(1)HHST is designed with hardware and software systems in mind. It applies a novel hardware-
friendly sharding strategy across attention heads, where each head attends to a distinct set of
tokens following a strided pattern, while collectively covering the context in full (Figure 1;
§4.2).
(2)In order to achieve strong performance on challenging long-context tasks, it is crucial to
include direct access to all tokens, at least at certain layers. HHST achieves this with a
hybrid architecture that combines sparse and dense attention across layers, and balances
efficiency and performance (§4.2).
S2-A TTENTION is applicable in both training and inference, substantially lowering the barrier to
exploring novel sparse attention architectures, which we explore in §4 and §5. We pretrain a suite of
models at 1.3B, 7B scales with different sparse attention, and compare them to the dense attention
baseline. Our results show that our HHST -7B matches the performance of dense attention while
achieving a 2.5 ×training speed-up and 4.5 ×inference speed-up. Moreover, we extend the 1.3B
models to a 32K context length, and 7B models to 128K. We show that our HHST can achieve
perfect Needle in a Haystack retrieval (Kamradt, 2023). Compared to FlashAttention-2 (Dao, 2023),
HHST can achieve 8.8 ×and 15.9 ×attention speed-up for 1.3B, 7B scales, and 2.8 ×, 2.5×training
wall-clock time reduction.
S2-A TTENTION is compatible with commonly used LLM frameworks including PyTorch, Megatron,
HuggingFace, and vLLM. With its user-friendly APIs, importing and customizing S2-A TTENTION
take no more than several lines of code as shown in Appendix B.
2 R ELATED WORKS
We present our analysis and observations on existing sparse attention attempts in both training and
inference.
<S> Q : There are 2 llamas and 
3 vicunas . How many animals are there
<S> GPT4 is the best LLM from OpenAIBlock 1
Block 2
Block 3Used
Evicted
Unused
Figure 3: Illustration of why KV eviction methods can cause more fragmentation. Here we show 3
pages of KV blocks containing 2 requests. Despite many tokens were evicted, the released slots can
hardly be utilized by other requests, leading to higher rate of internal fragmentation.
2.1 A BSENCE OF EFFICIENT SPARSE ATTENTION KERNEL
There have been attempts to reduce the computational complexity of attention by only attending to
a subset of tokens (Child et al., 2019; Katharopoulos et al., 2020; Kitaev et al., 2020; Zaheer et al.,
2020; Beltagy et al., 2020). However, these methods can’t bring wall-clock speed-up in training due
to the negligence of realistic memory access cost (Dao et al., 2022). Dao et al. (2022) breaks down
the attention computation into smaller block-wise computation to reduce the IO between SRAM and
the high bandwidth memory (HBM). The hardware implementation of FlashAttention family (Dao
et al., 2022; Dao, 2023) make them the most widely-adopted attention acceleration framework. It
remains unclear whether we can implement various sparse self-attention in such hardware-aware way,
so that the training speed can be further boosted over FlashAttention.
2.2 I SSUES WITH PLUG-IN-AND -PLAY KV E VICTION METHODS
Recently, plug-in-and-play KV eviction works thrives. More specifically, these methods dynamically
drop KV vectors at inference to reduce the memory footprint based on certain criteria that designed
to preserve model capabilities.
3

--- PAGE 4 ---
𝑚𝑞3𝐾𝑉1𝐾𝑉2𝐾𝑉3
load 𝐾𝑉1 𝟖 times for all the 𝒒
load 𝐾𝑉3 𝟔 times for all the 𝒒𝑞1
𝑞2
𝑞3𝐾𝑉1𝐾𝑉2𝐾𝑉3
load 𝐾𝑉1 𝟒 times for all the 𝒒
load 𝐾𝑉3 𝟑 times for all the 𝒒Inner loop
Merged_QK
SRAM
dotV
out𝑘3 and 𝑣3 reused 
by 𝑞6 after 𝑞5 MergeQ
MergeQ
MergeQ
MergeQFlashAttention -2 S2-Attention
𝑞1+2
𝑞3+4
𝑞5+6
𝑞7+8𝑞5+6𝑘3
𝑣3 𝑞4
𝑞5
𝑞6
𝑞7
𝑞8𝑞1
𝑞2
𝑞3
𝑞4
𝑞5
𝑞6
𝑞7
𝑞8𝑘3
𝑣3Figure 4: Illustration of S2-Attention Implementation. Left: Directly apply FlashAttention-2 tiling
to sparse attention. Right: MergeQ, which adaptively merge queries sharing the same KV together
when loading into the SRAM, thus reduce redundant KV loading and improve IO efficiency.
However, we observe such designs are hardly compatible with existing serving systems, which relies
on PagedAttention and continuous batching for efficient memory management. As shown in Figure 3,
during KV eviction, the corresponding tokens are release from the physical memory. However, as
the token-wise eviction are not guaranteed to be contiguous, the released memory slots can not be
effectively allocated for other requests, known as internal fragmentation. In this example, the internal
fragmentation increases by 37.5%, which in turn hurts throughput.
Meanwhile, dynamic eviction also introduces overheads in scheduling. For example, if different heads
have a different eviction policy/rate, the faster heads will have to wait for the slower ones, which
is a classic load-unbalance scenario. The issue is more severe when serving larger models, where
computations are distributed across devices and nodes with tensor parallel and pipeline parallel. Such
drawbacks further prevent these algorithms from being integrated into real-world serving clusters
with hundreds of nodes.
2.3 P ERFORMANCE DEGRADATION
Existing studies points out both the training and training-free sparse attention methods have perfor-
mance degradation compared to their dense counterparts, especially in long-context tasks. Further-
more, we also observe that some training-free methods(Jiang et al., 2024; Tang et al., 2024) need
benchmark-specific hyper parameters to maintain model quality. When applied to unseen requests,
the same method can display unpredictable behavior. However, in real-world deployment, user
queries often have long tail distribution. Thus, it’s not feasible to pre-determine hyper-parameters for
unseen user queries, which makes the deployment of such methods risky.
We discuss handling of these observations in sections below.
3 S2-A TTENTION : EFFICIENCY AND CUSTOMIZATION
This section presents S2-A TTENTION . We first briefly review the basics of GPU memory and
execution hierarchy, and then introduce our Merge-Q technique, which significantly improves the
kernel’s efficiency while allowing more fine-grained customization of the sparse attention.
3.1 P RELIMINARIES
GPU threads have access to a hierarchy of different types of memory. Global high-bandwidth memory
(HBM ) is the slowest but largest (roughly >100×in latency and ∼6K×in size). Shared memory
(SRAM ) is physically on chip, thus has larger bandwidth and lower latency compared to HBM.
Optimizing the computation of the SRAM and minimizing the IO between HBM and SRAM are
crucial for improving the efficiency of attention (Dao et al., 2022).
4

--- PAGE 5 ---
0
1
2
3
4
5
5432100
1
2
3
4
5
543210Actual KV
(a) Not KV-efficient sparsity 0
1
2
3
4
5
5432100
1
2
3
4
5
5432100
1
2
3
4
5
543210
(b) KV-efficient sparsityFigure 5: (a): The dilated attention based on relative position as an example of sparse attention
that is not KV-efficient. E.g., step 5 attends to KV at positions 1, 3, 5, while step 4 attends to 0, 2,
4. This results in requiring full KV cache. Although it suggests nearly 50% memory savings on
paper, it actually requires storing the full KV cache in practice. (b)All these attention patterns are
KV-efficient, as they get pushed to KV-cache when first encountered at decoding, then continuously
being attended for several steps before it finally gets evicted (e.g., all tokens in left figure, and token 0
in right figure) and never gets attended again, or remained attended for all future tokens (e.g., tokens
0, 2, 4 in middle figure and tokens 2, 4 in right figure). The arrows show that they all share a "vertical
line" pattern.
Poorly-optimized implementations of attention can result in frequent I/O to HBM and significantly
hurt the efficiency. CUDA organizes threads into thread blocks , which are further divided into
warps, groups of 32 threads. Threads within a block share the data through SRAM. It is desirable
that different threads in the same warp take the same execution path since otherwise efficiency will
be hurt due to warp divergence. Besides, thread block size should be sufficiently large to achieve
good utilization and load balancing. A tileis a portion of the Q,K,Vtensors assigned to a thread
block to be processed. For clarity, we take tile size as block size. FlashAttention improves efficiency
by minimizing HBM I/O, tiling the Q,K,Vtensors into chunks that fit into SRAM for efficient
computation (Dao et al., 2022), a principle that this work follows.
3.2 S2-A TTENTION
Warmup (Figure4 left) We first review a simple blocksparse implementation using the FlashAtten-
tion algorithm. A sequence of Ntokens is segmented into B=⌈N/S⌉shards, each containing S
consecutive tokens. We use Q[i]to denote the query vectors for the ith query shard, and similarly
K[i]the key vectors for ith key shard. Following Dao et al. (2022), for each query vector q, we iterate
through the Ktiles in SRAM to compute softmax( qK⊤). Unlike dense attention that uses the entire
Ktensor, we only consider a subset of keys specified by a sparse attention mask M, which can be
stored in a Compressed Sparse Row (CSR) format for memory efficiency.1
To better understand the efficiency of such implementation, we can calculate the number of loading
needed for each key/value shards. As shown in Figure 4 (left), the first key/value shards, KV1, is
attened by all the query shards, q1−q8. Thus, KV1is loaded 8 times from HBM to SRAM. If we
double the shard size, the number of query shards attending KV1will be halved to 4. In this case,
KV1only needs 4 loading which is more efficient. However, the IO efficiency comes at the cost of
granularity of our sparse mask, as we now have to mask-or-keep 2Stokens instead of S. We then
discuss how to achieve both IO efficiency and small mask granularity with Merge-Q.
Merge-Q At a high level, the core idea is to merge the query shards attending the same KV blocks
into a single tile so that we don’t need separately load the same KV blocks. In this way, even if
the mask granularity becomes smaller, we can still maintain IO efficiency. Figure 4: right display a
simpler case, where we merge the neighboring two query shards. Compared to the FlashAttention-2
baseline, this implementation only needs to load KV14 times instead of 8 times with the same
mask granularity. Merge-Q helps S2-A TTENTION support shard sizes as small as 16, enabling a
broader range of sparse attention patterns. Similar ideas can also be applied to merge KV blocks to
further boost efficiency. We leave more detailed implementation discussion in the released code and
Appendix D.
1https://docs.nvidia.com/nvpl/_static/sparse/storage_format/sparse_
matrix.html
5

--- PAGE 6 ---
With S2-A TTENTION , the community can customize fine-grained sparse attention patterns with
wall-clock speed-up. However, it remains unclear what types of sparse attention can achieve speed-up
without hurting the quality. We aim to answer this question in the following section.
4 S2-A TTENTION : INSIGHTS , FORMULATION ,AND SPARSITY COOKBOOK
In this section, we first discuss which kind of sparse attention patterns allow efficient kernel imple-
mentations in practice (§4.1). Building on these insights, we introduce Head-Heterogenous Strided
Transformer (HHST), a novel sparse attention architecture (§4.2).
4.1 KV-E FFICIENT SPARSITY
KV cache is a primary memory bottlenecks for decoder-only LMs at inference time. Many existing
sparse attentions determine which tokens to attend to based on relative distances (Child et al., 2019;
Zaheer et al., 2020; Beltagy et al., 2020). However, these approaches are not GPU memory-efficient
during decoding, making it difficult to translate their FLOP savings into real-world efficiency gains.
Figure 5(a) provides an illustrative example. The main issue is that, for such sparse attention, KV not
used in earlier decoding steps might be required in later ones, making memory management more
challenging. Despite the nearly 50% memory saving on paper, it actually requires storing the full KV
cache in practice, resulting in zero memory savings.
In contrast, Figure 5(b) illustrates a sparse attention that can achieve memory saving in practice. The
key is that the stored KV cache is reused across several decoding steps but is no longer needed in
future steps, and thus can be evicted, freeing up the GPU memory.
The comparison between these two approaches leads to the following rule of thumb of designing
KV-efficient sparse attention. For ∀j≥i, l≥1,
(ki,vi)is attended by qj+l
=⇒(ki,vi)must also be attended by qj.(1)
Otherwise, kiandvineed to be stored at step jfor future generations, even it is not used at step j.
Intuitively, in the attention pattern matrix, we shall see continuous "vertical lines" as shown in Figure
5(b). This means the sparse patterns should be based on absolute positions rather than relative ones,
except for consecutive local context (e.g., left figure in Figure 5(b)).
4.2 H EAD-HETEROGENOUS STRIDED TRANSFORMER
This section introduces Head-Heterogenous Strided Transformer ( HHST ), a novel efficient sparse
attention inspired by the insights we learned above. Core to its design are two design choices
introduced below.
Heterogeneous Context Sharding across Attention Heads To achieve balanced load across
attention heads and enhance parallelization, each head should attend to an equal number of tokens.
Additionally, HHST ensures that different heads attend to different shards of the context while
collectively covering the entire context. This design makes sure that HHST always has direct
access to the full context at each layer, without compromising parallelization. Figure 1 provides an
illustrative diagram.
More formally, for context with Bshards, we take the most recent Blblocks as local blocks and set
the rest as remote blocks. For attention head with index h, itsB×Bblock attention mask Mhis:
Mh
i,j=(1, i−j < B l, Local
1, j−oh∈sZ≥0∧i−j∈[Bl, B)Stride
0otherwise
sis the stride size, and x∈mZ≥0mean xis 0 or a positive multiple of m. Similarly to a sliding
window, tokens beyond the Bshards are not attended to.
The flexibility of our S2-A TTENTION kernel enables an efficient implementation of this strategy. As
shown in our experiments, this design allows the model to achieve strong long-context performance
while maximizing efficiency gains.
6

--- PAGE 7 ---
Hybrid Architecture As previous studies show (Huang et al., 2022; Lieber et al.), some attention
layers are significantly denser compared to the others, with attention weights distributed near uni-
formly across all positions. Therefore, it is particularly beneficial to retain dense attention in these
layers. This motivates us to explore a hybrid architecture that combines our efficient sparse attention
in most layers with dense attention in others. We empirically find that our sparse attention strategy
is highly effective, requiring only 1/6 of the attention layers to be dense to achieve strong retrieval
performance with 128K-long contexts. More exploration is presented in our experiments.
Discussion It is important to point out that all eviction strategies targeting inference (Zhang et al.,
2023; Liu et al., 2023; Ge et al., 2024b) are KV-cache efficient, since evicted KV will never be used
by future queries. However, these strategies introduce sample-dependent sparsity patterns, making it
computationally expensive to determine eviction timing during decoding. In contrast, our approach
uses a fixed sparsity pattern across all samples, eliminating the overhead of deciding which tokens to
evict. Besides, KV eviction approaches are post-hoc and often perform much poorly as compared to
the original dense counterpart (Ge et al., 2024a). Our HHST , as we will soon see in the experiments,
adapts to the sparse attention during training (pre-training or post-training) performs comparably to
dense baselines while reducing the training overhead.
5 E XPERIMENT
To evaluate HHST , we first study the pre-training quality in §5.1 and §5.2. We then benchmark the
kernel efficiency and end-to-end serving latency in §5.4 and § ??. Lastly, we conduct an ablation
study of the design choices.
5.1 B ENCHMARKING MODEL TRAINING QUALITY
Settings We first train a range of 1.3B model with the Llama 2 architecture, with 24 layers, 2048
hidden size with 16 heads, with max sequence length as 8192. We use the open-source FineWeb-
Edu-350B Penedo et al. (2024) as the pre-training corpus. An OpenAI Tiktoken tokenizer with 100K
vocabulary size is used to process the raw text. All model variations use batch size of 4M tokens for
all sequence lengths and train for a total of 300 billion tokens. For hyperparameters, we use µP Yang
et al. (2022) with a base shape of 256. A µP learning rate of 0.02 is used with linear decay and 0.5%
of total training tokens for warmup. All models are evaluated after training on the total 300B tokens
for one epoch.
Downstream Tasks We use a model with dense attention as our baseline, denoted as “Dense”.
To study our hybrid structure with heterogeneous sharding and union completeness, we control the
FLOPs to be approximately equivalent. The total attended tokens is around 576 tokens, or 9 shards of
64 tokens. We use this to configure the sliding window attention (SWA), as the control set. We add
different changes to SWA to see how they affect the training quality. The treatment sets are grouped
into 1) Homogeneous (Different heads attend to the same shards); 2) Heterogeneous & Incomplete
(Different heads attend to different shards but not covering the entire context), and 3) Heterogeneous
& Complete (Different heads attend to the same shards and collectively cover the entire context).
From Table 1, we can observe the hybrid architectures shows promising results. As we can see
from S2-L1V15 + Dense ( HHST )in the last row, heterogeneous sharding with complete context
and two dense layers give consistently best results across tasks, with minor gap from the default
attention baseline while using only 18% FLOPs. Notably, in the Passkey Retrieval task, HHST can
achieve much better performance compared to the dense model. This observation works as an initial
validation of the context understanding ability of the HHST design. We’ll further validate it in the
long context continual training section.
We also found adding two dense layers generally leads to a significantly higher performance. Within
theHomogeneous group, we can observe adding attention sink can significantly boost training quality,
compared to only using the sliding window (SWA). In the Heterogeneous & Incomplete group, the
vertical stride size is bigger than the number of attention heads, making the context incomplete after
the union. For the Heterogeneous & Complete group, we tune the stride size and local window
so that it just covers the full context while having the same FLOPs as others. When comparing the
7

--- PAGE 8 ---
Table 1: Pre-Training quality evaluation. “SWA” refers to sliding window attention. “L” refers to
number of local blocks. “V” refers to the vertical stride size. “+ Sink” refers attending to attention
sink. “+ Dense” refers to making the first two attention layers dense.
Model Passkey WinoGrande PIQA RACE Wikitext103(ppl)
Dense (Upper Bound) 0.865 0.592 0.733 0.403 15.884
Homogeneous (18% FLOPs of Dense)
HHST-L9 (SWA) 0.334 0.547 0.705 0.363 21.997
HHST-L9 + Dense 0.620 0.575 0.714 0.373 20.450
HHST-L9 + Sink 0.560 0.566 0.721 0.380 21.037
HHST-L9 + Sink + Dense 0.771 0.577 0.728 0.388 18.503
HHST-L1V15 0.542 0.541 0.716 0.352 21.035
HHST-L1V15 + Dense 0.741 0.568 0.713 0.349 20.579
Heterogeneous & Incomplete (18% FLOPs of Dense)
HHST-L2V18 0.630 0.565 0.728 0.357 20.502
HHST-L2V18 + Dense 0.823 0.587 0.732 0.379 18.726
HHST-L4V25 0.612 0.542 0.720 0.352 20.875
HHST-L4V25 + Dense 0.795 0.569 0.724 0.386 19.285
Heterogeneous & Complete (18% FLOPs of Dense)
HHST-L1V15 0.782 0.571 0.724 0.361 19.551
HHST-L1V15 + Dense (HHST) 0.941 0.587 0.725 0.397 17.183
Incomplete group to the Complete group, we can see the benefits of making the union of context
complete by limiting vertical stride size.
5.2 L ONG CONTEXT CONTINUAL TRAINING
4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense15-17
0.00.20.40.60.81.0
Score
(a) 2 dense layers (11% FLOPs)
4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense14-18
0.00.20.40.60.81.0
Score
 (b) 4 dense layers (17% FLOPs)
4096 819216384 32768 65536 81920102400 122880 131072
T oken Limit0
10
20
30
40
50
60
70
80
90
100Depth PercentS2-L31V32+Dense12-20
0.00.20.40.60.81.0
Score
 (c) 8 dense layers (28% FLOPs)
Figure 6: 128K Needle In A Haystack Evaluation. We modify the number of dense layers and
demonstrate FLOPs saving over Dense (all layers are dense).
We further examine how to adapt sparse attention to longer contexts. We start from an existing
densely pre-trained model and extend its context length by continually training it on a longer context
length with HHST sparse architecture. Specifically, we choose Llama-2-7B and continually train
it on 128K context length. We change the RoPE base to 5M. Both models are continually trained
with 10B tokens following the recipe in Fu et al. (2024). We evaluate the models on the Needle In A
Haystack task Kamradt (2023).
To investigate how to achieve strong long-context performance, we modify the number of dense
layers in HHST . We set the number of dense layers as 2, 4 and 8, respectively. We fix the number
of local blocks to be 31 and vertical stride size to be 32. As shown in Figure 6, for 128K context,
the model can retrieve the full context with 8 dense layers but fails to do so with only 2 and 4 dense
layers. The results validate the long context capability of HHST design.
8

--- PAGE 9 ---
5.3 T RAINING SPEED -UP
5.3.1 A TTENTION OPERATION BENCHMARK
Benchmark Settings We measure the attention runtime of HHST with our S2-A TTENTION kernel,
and FlashAttention-2 on an A100 80GB GPU for different context length, number of head, and head
dimension settings.
8192 16384 32768
Sequence Length020406080Step Time (s)Forward Pass Latency (1.3B)
S2-Attention (Fwd)
Flash-Attention2 (Fwd)
8192 16384 32768
Sequence Length050100150200250Step Time (ms)Backward Pass Latency (1.3B)
S2-Attention (Bwd)
Flash-Attention2 (Bwd)
(a) 1.3B Attention Speed-up.
8192 16384 32768 65536
Sequence Length0100200300400500600Step Time (s)Forward Pass Latency (7B)
S2-Attention (Fwd)
Flash-Attention2 (Fwd)
8192 16384 32768 65536
Sequence Length02505007501000125015001750Step Time (ms)Backward Pass Latency (7B)
S2-Attention (Bwd)
Flash-Attention2 (Bwd) (b) 7B Attention Speed-up.
Figure 7: Attention Speed-up vs Sequence Length and Model Scale.
In Figure 7 and Figure 2a We benchmark the speed-up brought by HHST in 1.3B, 7B, 70B model
sizes across different sequence lengths to showcase the scalability of our system. For all the model
sizes, HHST can achieve multiple times of speed-up over FlashAttention-2. For 70B models with
64 heads, HHST can give 25.3 ×end-to-end speed-up. For example, in 1.3B models with a vertical
stride of 16, HHST can achieve a 8.8 ×speed-up. As the max sequence length grows longer, the
speed-up gradually approximates the theoretical FLOPs reduction benefits. The overall boost is
hedged a bit due to our less optimized backward kernel, which leaves room for further improvement.
5.4 T RAINING AND INFERENCE SPEED -UP
8192 16384 32768 65536 131072
Sequence Length050000100000150000200000250000300000350000T okens per SecondT oken Throughput(7B)
Dense
S2-Attention
(a) 7B end-to-end training speed-up.
8063 16255 32639 65407 130943 262015050200
150
100FlashAttention in vLLM
S2-Attention in vLLMLatency (s)Inference Benchmark on vLLM
4.5X
2.9X 1.9X
1.2X 1.1X1.5X (b) 7B end-to-end inference speed-up on vLLM.
We evaluate the end to end training speed-up of the 1.3B and 7B models by measuring the token
throughput of both models. All models are trained on 256 A100, with a batch size of 8M tokens and
activation checkpointing. For 1.3B, HHST can get 1.2 ×, 1.8×, 2.3×, and 2.8 ×token throughput
on 8K to 128K context compared to FlashAttention-2. For 7B models, HHST can get 1.1 ×, 1.2×,
1.5×, and 2.5 ×token throughput improvement. In order to demonstrate the inference efficiency
improvements of HHST , we measure the end-to-end latency over different context length settings.
To make comparison realistic, our experiments are done on vLLM (Kwon et al., 2023). We choose
the FlashAttention-2 backend in vLLM as baseline for fair comparison, as the inference kernel of
S2-A TTENTION is also based on vLLM. Both methods are deployed on a single node with 8 A100
80GPU, with tensor parallel size equals 4. We set output length as 128 and vary input length between
16K to 256K. As shown in Figure 8b, HHST can achieves 1.1 ×, 1.2×, 2.9×, 4.5×speed-up on 8K,
16K, 128K, 256K context.
9

--- PAGE 10 ---
6 C ONCLUSION
We presented S2-A TTENTION , an optimized Triton kernel library that provides a variety of cus-
tomizable sparse attention implementations for both training and inference. The insights from
S2-A TTENTION led to several principles about designing choices of sparse attention methods to
make them efficiency in practice. They inspired a novel hybrid sparse attention architecture that
meets several desiderata that we find crucial for achieving both practical efficiency gains and strong
accuracy on downstream tasks, called as Head-Heterogenous Strided Transformer ( HHST ). We will
open-source our kernel library and make it a plug-in-and-play alternative for FlashAttention-2 module
in popular training frameworks like Megatron and Pytorch. We also integrated S2-A TTENTION into
vLLM backend for instant serving. Both the training and inference kernels allow users to freely
customize their sparsity pattern, facilitating the whole community to study the topic in the future.
REFERENCES
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
CoRR , abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150 .
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. CoRR , abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509 .
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,
abs/2307.08691, 2023. doi: 10.48550/ARXIV .2307.08691. URL https://doi.org/10.
48550/arXiv.2307.08691 .
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention:
Fast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mo-
hamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural
Information Processing Systems 35: Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,
2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html .
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao
Peng. Data engineering for scaling language models to 128k context. In Forty-first International
Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 . OpenReview.net,
2024. URL https://openreview.net/forum?id=TaAqeo7lUh .
Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, and Hao Peng. A little goes a long way: Efficient
long context training and inference with partial contexts. arXiv preprint arXiv:2410.01485 , 2024a.
Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you
what to discard: Adaptive KV cache compression for llms. In The Twelfth International Conference
on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net,
2024b. URL https://openreview.net/pdf?id=88nT0j5jAn .
Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. LM-infinite:
Zero-shot extreme length generalization for large language models. In Kevin Duh, Helena Gomez,
and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies (Volume 1:
Long Papers) , pp. 3991–4008, Mexico City, Mexico, June 2024. Association for Computational
Linguistics. doi: 10.18653/v1/2024.naacl-long.222. URL https://aclanthology.org/
2024.naacl-long.222/ .
Xin Huang, Ashish Khetan, Rene Bidart, and Zohar Karnin. Pyramid-bert: Reducing complexity
via successive core-set based token selection. In Smaranda Muresan, Preslav Nakov, and Aline
Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pp. 8798–8817.
Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.602. URL
https://doi.org/10.18653/v1/2022.acl-long.602 .
10

--- PAGE 11 ---
Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua
Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0:
Accelerating pre-filling for long-context llms via dynamic sparse attention. CoRR , abs/2407.02490,
2024. doi: 10.48550/ARXIV .2407.02490. URL https://doi.org/10.48550/arXiv.
2407.02490 .
Greg Kamradt. Needle in a haystack-pressure testing llms. Github Repository , pp. 28, 2023.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119
ofProceedings of Machine Learning Research , pp. 5156–5165. PMLR, 2020. URL http:
//proceedings.mlr.press/v119/katharopoulos20a.html .
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id=
rkgNKkHtvB .
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. CoRR , abs/2309.06180, 2023. doi: 10.48550/arXiv.2309.06180. URL
https://doi.org/10.48550/arXiv.2309.06180 .
Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,
Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-
mamba language model, 2024. URL https://arxiv. org/abs/2403.19887 .
Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios
Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance
hypothesis for LLM KV cache compression at test time. CoRR , abs/2305.17118, 2023. doi:
10.48550/arXiv.2305.17118. URL https://doi.org/10.48550/arXiv.2305.17118 .
OpenAI. Gpt-4 technical report, 2023.
Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin
Raffel, Leandro von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the
finest text data at scale. CoRR , abs/2406.17557, 2024. doi: 10.48550/ARXIV .2406.17557. URL
https://doi.org/10.48550/arXiv.2406.17557 .
Szymon Rucinski. Efficient language adaptive pre-training: Extending state-of-the-art large language
models for polish. CoRR , abs/2402.09759, 2024. doi: 10.48550/ARXIV .2402.09759. URL
https://doi.org/10.48550/arXiv.2402.09759 .
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism,
2020.
Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest:
Query-aware sparsity for efficient long-context llm inference, 2024. URL https://arxiv.
org/abs/2406.10774 .
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM
Comput. Surv. , 55(6):109:1–109:28, 2023. doi: 10.1145/3530811. URL https://doi.org/
10.1145/3530811 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
11

--- PAGE 12 ---
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023.
Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and
Song Han. Duoattention: Efficient long-context LLM inference with retrieval and streaming heads.
CoRR , abs/2410.10819, 2024. doi: 10.48550/ARXIV .2410.10819. URL https://doi.org/
10.48550/arXiv.2410.10819 .
Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pa-
chocki, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning
large neural networks via zero-shot hyperparameter transfer. In NeurIPS 2021 , March
2022. URL https://www.microsoft.com/en-us/research/publication/
tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/ .
Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A
distributed serving system for transformer-based generative models. In Marcos K. Aguilera and
Hakim Weatherspoon (eds.), 16th USENIX Symposium on Operating Systems Design and Imple-
mentation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022 , pp. 521–538. USENIX Association,
2022. URL https://www.usenix.org/conference/osdi22/presentation/yu .
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird:
Transformers for longer sequences. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html .
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-
hitter oracle for efficient generative inference of large language models. CoRR , abs/2306.14048,
2023. doi: 10.48550/arXiv.2306.14048. URL https://doi.org/10.48550/arXiv.
2306.14048 .
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao,
Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Sglang: Efficient execution of structured
language model programs, 2024. URL https://arxiv. org/abs/2312.07104 , 2023.
A A PPENDIX
You may include other additional sections here.
12
