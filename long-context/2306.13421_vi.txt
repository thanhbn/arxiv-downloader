# 2306.13421.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/long-context/2306.13421.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 801468 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
Retrieval-Pretrained Transformer: MÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa vá»›i
Tá»± truy xuáº¥t
Ohad Rubin Jonathan Berant
Khoa Khoa há»c MÃ¡y tÃ­nh Blavatnik, Äáº¡i há»c Tel Aviv
{ohad.rubin,joberant}@cs.tau.ac.il
TÃ³m táº¯t
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c tÄƒng cÆ°á»ng
truy xuáº¥t (LMs) Ä‘Ã£ nháº­n Ä‘Æ°á»£c nhiá»u sá»± chÃº Ã½
gáº§n Ä‘Ã¢y. Tuy nhiÃªn, thÃ´ng thÆ°á»ng bá»™ truy xuáº¥t
khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n chung nhÆ° má»™t thÃ nh
pháº§n báº£n Ä‘á»‹a cá»§a LM, mÃ  Ä‘Æ°á»£c thÃªm vÃ o sau
khi LM Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, Ä‘iá»u nÃ y
háº¡n cháº¿ kháº£ nÄƒng thÃ­ch á»©ng láº«n nhau giá»¯a LM
vÃ  bá»™ truy xuáº¥t. Trong nghiÃªn cá»©u nÃ y, chÃºng
tÃ´i Ä‘á» xuáº¥t Retrieval-Pretrained Transformer
(RPT), má»™t kiáº¿n trÃºc vÃ  quy trÃ¬nh huáº¥n luyá»‡n
Ä‘á»ƒ huáº¥n luyá»‡n chung má»™t LM Ä‘Æ°á»£c tÄƒng cÆ°á»ng
truy xuáº¥t tá»« Ä‘áº§u vÃ  Ã¡p dá»¥ng nÃ³ vÃ o nhiá»‡m vá»¥
mÃ´ hÃ¬nh hÃ³a vÄƒn báº£n dÃ i. Cho má»™t Ä‘oáº¡n vÄƒn
báº£n Ä‘Æ°á»£c táº¡o gáº§n Ä‘Ã¢y trong má»™t tÃ i liá»‡u dÃ i,
LM tÃ­nh toÃ¡n cÃ¡c biá»ƒu diá»…n truy váº¥n, sau Ä‘Ã³
Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ truy xuáº¥t cÃ¡c Ä‘oáº¡n trÆ°á»›c Ä‘Ã³
trong tÃ i liá»‡u, náº±m cÃ³ thá»ƒ cÃ¡ch hÃ ng chá»¥c nghÃ¬n
token. ThÃ´ng tin tá»« cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c truy xuáº¥t
Ä‘Æ°á»£c há»£p nháº¥t vÃ o cÃ¡c biá»ƒu diá»…n LM Ä‘á»ƒ dá»±
Ä‘oÃ¡n Ä‘oáº¡n má»¥c tiÃªu tiáº¿p theo. ChÃºng tÃ´i huáº¥n
luyá»‡n thÃ nh pháº§n truy xuáº¥t vá»›i má»™t má»¥c tiÃªu
ngá»¯ nghÄ©a, trong Ä‘Ã³ má»¥c tiÃªu lÃ  truy xuáº¥t cÃ¡c
Ä‘oáº¡n lÃ m tÄƒng xÃ¡c suáº¥t cá»§a Ä‘oáº¡n tiáº¿p theo, theo
má»™t LM tham chiáº¿u. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT
trÃªn bá»‘n nhiá»‡m vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa,
bao gá»“m sÃ¡ch, mÃ£ vÃ  viáº¿t toÃ¡n há»c, vÃ  chá»©ng
minh ráº±ng RPT cáº£i thiá»‡n cháº¥t lÆ°á»£ng truy xuáº¥t
vÃ  sau Ä‘Ã³ lÃ  perplexity trÃªn toÃ n bá»™ so vá»›i cÃ¡c
baseline máº¡nh.
1 Giá»›i thiá»‡u
CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LMs) Ä‘Ã£ cÃ³ thÃ nh cÃ´ng
to lá»›n gáº§n Ä‘Ã¢y (Brown et al., 2020; Chowdhery
et al., 2022; Zhang et al., 2022; Touvron et al.,
2023), trá»Ÿ thÃ nh má»™t cÃ´ng cá»¥ há»¯u Ã­ch trÃªn nhiá»u
lÄ©nh vá»±c. Tuy nhiÃªn, thÃ nh cÃ´ng cá»§a chÃºng Ä‘i kÃ¨m
vá»›i chi phÃ­ tÃ­nh toÃ¡n, do sá»‘ lÆ°á»£ng tham sá»‘ tÄƒng Ä‘á»ƒ
lÆ°u trá»¯ kiáº¿n thá»©c tháº¿ giá»›i (Fedus et al., 2022) vÃ 
Ä‘á»™ dÃ i ngá»¯ cáº£nh tÄƒng cho phÃ©p truy cáº­p thÃ´ng tin
xa xÃ´i, nhÆ°ng pháº£i chá»‹u pháº¡t Ä‘á»™ phá»©c táº¡p báº­c hai.
TÆ°Æ¡ng tá»± tá»« vá»±ngSÃ¡ch hoáº·cVÄƒn báº£n dÃ iTÆ°Æ¡ng tá»± ngá»¯ nghÄ©a
Truy xuáº¥tâ‹®Há»£p nháº¥tâ‹®
TÃ­n hiá»‡u Huáº¥n luyá»‡nPÎ¸ (â€¦â€¦....|â€¦â€¦â€¦â€¦â€¦...)ğ‘!" ğ‘#$! ğ‘#$# >PÎ¸ (â€¦â€¦....|â€¦â€¦â€¦â€¦â€¦...)ğ‘!$$ ğ‘#$! ğ‘#$# Äoáº¡n 100Äoáº¡n 13Tráº¡ng thÃ¡i QuÃ¡ khá»©
Dá»± Ä‘oÃ¡nMÃ´ hÃ¬nh NgÃ´n ngá»¯ NhÃ¢n quáº£Äoáº¡n 201
Äoáº¡n 202Truy váº¥n
Má»¥c tiÃªuÄáº§u vÃ o
ThamchThamchKáº» giáº¿t ngÆ°á»i Ä‘á»ƒ láº¡i má»™t cÄƒn phÃ²ng Ä‘áº§y báº±ng chá»©ng, má»™t cÃ¢u Ä‘á»‘ cho phÃ¡p y.Khi cÃ²n nhá», Trung Ãºy Johntháº¥y má»™t con chÃ³ cháº¿t; tá»« Ä‘Ã³, mÃ u Ä‘á» tháº¯m luÃ´n lÃ m choháº¯n lo láº¯ng.Trung Ãºy John nhÃ¬n quanh, "Má»™t náº¡n nhÃ¢n khÃ¡c, Káº» Giáº¿t NgÆ°á»i Äá» Tháº¯m láº¡i táº¥n cÃ´ng."
"TÃ´i cÃ¡ nhá»¯ng ngÆ°á»i phÃ¡p y sáº½ thÃ­ch cÃ¡i nÃ y."HÃ¬nh 1: Retrieval-Pretrained Transformer (RPT) lÃ  má»™t
mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u vá»›i kháº£ nÄƒng truy xuáº¥t
báº£n Ä‘á»‹a cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho vÄƒn báº£n dÃ i (vÃ­ dá»¥, sÃ¡ch). RPT
nháº­n má»™t Ä‘oáº¡n vÄƒn báº£n lÃ m Ä‘áº§u vÃ o, truy xuáº¥t cÃ¡c Ä‘oáº¡n liÃªn quan
ngá»¯ nghÄ©a tá»« quÃ¡ khá»© Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»‘t hÆ¡n Ä‘oáº¡n tiáº¿p theo, vÃ  há»£p
nháº¥t cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c truy xuáº¥t nÃ y vÃ o cÃ¡c biá»ƒu diá»…n cá»§a nÃ³. TrÃªn
cÆ¡ sá»Ÿ má»™t loss LM tiÃªu chuáº©n, bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ
truy xuáº¥t cÃ¡c Ä‘oáº¡n lÃ m tÄƒng xÃ¡c suáº¥t cá»§a Ä‘oáº¡n tiáº¿p theo theo má»™t
LM tham chiáº¿u.
MÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy xuáº¥t (RALM)
giáº£m bá»›t chi phÃ­ nÃ y (Khandelwal et al., 2020; Yo-
gatama et al., 2021; Borgeaud et al., 2022; Ram
et al., 2023), vÃ¬ viá»‡c truy xuáº¥t chÃ­nh xÃ¡c thÃ´ng tin
liÃªn quan cÃ³ thá»ƒ giáº£m yÃªu cáº§u vá» bá»™ nhá»› vÃ  tÃ­nh
toÃ¡n. HÆ¡n ná»¯a, RALM cÃ³ lá»£i cho tÃ­nh chÃ­nh xÃ¡c,
tÃ­nh má»›i vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a mÃ  khÃ´ng cáº§n
huáº¥n luyá»‡n láº¡i, chá»‰ Ä‘Æ¡n giáº£n báº±ng cÃ¡ch thay Ä‘á»•i chá»‰
má»¥c truy xuáº¥t (Guu et al., 2020; Lewis et al., 2020;
Huang et al., 2023).
Tuy nhiÃªn, cÃ¡c nghiÃªn cá»©u trÆ°á»›c vá» RALM pháº§n lá»›n
Ä‘Ã£ khÃ´ng huáº¥n luyá»‡n bá»™ truy xuáº¥t nhÆ° má»™t thÃ nh pháº§n
háº¡ng nháº¥t cá»§a LM. Trong má»™t sá»‘ trÆ°á»ng há»£p (Khandelwal et al., 2020;arXiv:2306.13421v2  [cs.CL]  21 Jul 2024

--- TRANG 2 ---
Yogatama et al., 2021; Borgeaud et al., 2022), bá»™
truy xuáº¥t chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng táº¡i thá»i Ä‘iá»ƒm kiá»ƒm tra,
hoáº·c váº«n cá»‘ Ä‘á»‹nh trong suá»‘t quÃ¡ trÃ¬nh huáº¥n luyá»‡n,
ngÄƒn cáº£n nÃ³ thÃ­ch á»©ng vá»›i bá»™ sinh LM. Trong cÃ¡c
trÆ°á»ng há»£p khÃ¡c, thÃ nh pháº§n truy xuáº¥t Ä‘Æ°á»£c huáº¥n
luyá»‡n chung nhÆ°ng chá»‰ sau má»™t giai Ä‘oáº¡n huáº¥n luyá»‡n
trÆ°á»›c riÃªng biá»‡t cho cáº£ bá»™ truy xuáº¥t vÃ  LM (Sachan
et al., 2021; Izacard et al., 2022b; Jiang et al., 2022;
Bertsch et al., 2023). VÃ¬ váº­y, bá»™ truy xuáº¥t khÃ´ng
Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c tá»« Ä‘áº§u vá»›i LM, vÃ  chá»‰ má»™t
pháº§n ngÃ¢n sÃ¡ch huáº¥n luyá»‡n Ä‘Æ°á»£c phÃ¢n bá»• cho huáº¥n
luyá»‡n chung.
Gáº§n Ä‘Ã¢y, Zhong et al. (2022) Ä‘Ã£ trÃ¬nh bÃ y má»™t
LM Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy xuáº¥t huáº¥n luyá»‡n bá»™ truy
xuáº¥t tá»« Ä‘áº§u cÃ¹ng vá»›i LM, nhÆ°ng (a) bá»™ truy xuáº¥t
Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ khai thÃ¡c chá»‰ thÃ´ng tin tá»« vá»±ng,
vÃ  (b) thÃ´ng tin Ä‘Æ°á»£c truy xuáº¥t khÃ´ng Ä‘Æ°á»£c há»£p
nháº¥t á»Ÿ cáº¥p Ä‘á»™ biá»ƒu diá»…n trá»Ÿ láº¡i vÃ o LM.
Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y Retrieval-Pretrained
Transformer (RPT), má»™t LM Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy
xuáº¥t, trong Ä‘Ã³ bá»™ truy xuáº¥t lÃ  má»™t thÃ nh pháº§n háº¡ng
nháº¥t, Ä‘Æ°á»£c huáº¥n luyá»‡n chung tá»« Ä‘áº§u vá»›i LM. RPT
dá»±a trÃªn hai Ä‘Ã³ng gÃ³p ká»¹ thuáº­t. Äáº§u tiÃªn, vá» máº·t
kiáº¿n trÃºc (xem HÃ¬nh 1), cÃ¡c biá»ƒu diá»…n Ä‘áº§u vÃ o cho
bá»™ truy xuáº¥t Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« chÃ­nh cÃ¡c biá»ƒu diá»…n
LM (má»™t khÃ¡i niá»‡m chÃºng tÃ´i gá»i lÃ  tá»± truy xuáº¥t),
vÃ  cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c truy xuáº¥t Ä‘Æ°á»£c há»£p nháº¥t trá»Ÿ
láº¡i vÃ o bá»™ giáº£i mÃ£ LM Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n tá»« tiáº¿p
theo. Thá»© hai, chÃºng tÃ´i huáº¥n luyá»‡n bá»™ truy xuáº¥t
vá»›i má»™t hÃ m loss phá»¥ khuyáº¿n khÃ­ch truy xuáº¥t cÃ¡c
Ä‘oáº¡n vÄƒn báº£n lÃ m tÄƒng xÃ¡c suáº¥t sinh ra vÄƒn báº£n
tiáº¿p theo. Cá»¥ thá»ƒ, cho má»™t Ä‘oáº¡n Ä‘Æ°á»£c táº¡o gáº§n Ä‘Ã¢y
ct, bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ truy xuáº¥t cÃ¡c
Ä‘oáº¡n ci lÃ m tÄƒng xÃ¡c suáº¥t cháº¥m Ä‘iá»ƒm (ct+1|ci, ct)
theo má»™t LM cháº¥m Ä‘iá»ƒm tham chiáº¿u. HÃ¬nh 1 cung
cáº¥p má»™t vÃ­ dá»¥ minh há»a cho trÆ°á»ng há»£p mÃ´ táº£ hiá»‡n
trÆ°á»ng tá»™i pháº¡m, vÃ  má»™t LM cháº¥m Ä‘iá»ƒm cho tháº¥y
lá»£i Ã­ch cá»§a viá»‡c truy xuáº¥t má»™t Ä‘oáº¡n cÃ¡ch hÃ ng nghÃ¬n
token (Ä‘oáº¡n 13) so vá»›i truy xuáº¥t tá»« vá»±ng, dáº«n Ä‘áº¿n
má»™t Ä‘oáº¡n chá»‰ liÃªn quan bá» ngoÃ i (Ä‘oáº¡n 100). KhÃ´ng
giá»‘ng nhÆ° cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy xuáº¥t
hiá»‡n cÃ³ sá»­ dá»¥ng má»™t bá»™ mÃ£ hÃ³a phá»¥ cho truy xuáº¥t
(Izacard and Grave, 2021a; Izacard et al., 2022b;
Sachan et al., 2021), RPT cÃ³ thá»ƒ táº­n dá»¥ng cÃ¡c tráº¡ng
thÃ¡i áº©n ná»™i bá»™ cá»§a mÃ¬nh cho truy xuáº¥t sau má»™t giai
Ä‘oáº¡n huáº¥n luyá»‡n trÆ°á»›c duy nháº¥t, Ä‘Æ¡n giáº£n hÃ³a Ä‘Ã¡ng
ká»ƒ viá»‡c huáº¥n luyá»‡n chung.
ChÃºng tÃ´i Ã¡p dá»¥ng RPT vÃ o váº¥n Ä‘á» mÃ´ hÃ¬nh hÃ³a
cÃ¡c tÃ i liá»‡u dÃ i, cháº³ng háº¡n nhÆ° sÃ¡ch, bÃ i bÃ¡o vÃ 
mÃ£, vÃ¬ Ä‘Ã¢y lÃ  nhá»¯ng vÃ­ dá»¥ xuáº¥t hiá»‡n tá»± nhiÃªn cá»§a
ná»™i dung dÃ i, trong Ä‘Ã³ toÃ n bá»™ chá»‰ má»¥c cÃ³ thá»ƒ Ä‘Æ°á»£c
giá»¯ trong bá»™ nhá»› trong má»™t láº§n chuyá»ƒn tiáº¿p. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT trÃªn bá»‘n nhiá»‡m vá»¥ mÃ´ hÃ¬nh
ngÃ´n ngá»¯ vÃ  tháº¥y ráº±ng nÃ³ cáº£i thiá»‡n perplexity trÃªn
táº¥t cáº£ cÃ¡c nhiá»‡m vá»¥, vÆ°á»£t trá»™i hÆ¡n cÃ¡c nghiÃªn cá»©u
trÆ°á»›c (Hutchins et al., 2022; Wu et al., 2022) cÅ©ng
nhÆ° cÃ¡c baseline máº¡nh (Borgeaud et al., 2022; Zhong
et al., 2022). HÆ¡n ná»¯a, chÃºng tÃ´i cho tháº¥y RPT truy
xuáº¥t cÃ¡c Ä‘oáº¡n cháº¥t lÆ°á»£ng cao so vá»›i cÃ¡c bá»™ truy xuáº¥t
dá»±a trÃªn thÃ´ng tin tá»« vá»±ng. Dá»±a trÃªn cÃ¡c phÃ¡t hiá»‡n
thá»±c nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i láº­p luáº­n RPT
cÃ³ thá»ƒ má»Ÿ Ä‘Æ°á»ng cho tháº¿ há»‡ tiáº¿p theo cá»§a cÃ¡c LM
Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, trong Ä‘Ã³ cÃ¡c kho dá»¯ liá»‡u
lá»›n Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n trÆ°á»›c,
dáº«n Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ trong Ä‘Ã³ truy xuáº¥t
lÃ  má»™t thÃ nh pháº§n Ä‘Æ°á»£c nhÃºng máº¡nh máº½. MÃ£ cá»§a
chÃºng tÃ´i cÃ³ sáºµn cÃ´ng khai táº¡i
https://github.com/OhadRubin/RPT.
2 Bá»‘i cáº£nh
Äá»ƒ Ä‘á»‹nh vá»‹ Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i, chÃºng tÃ´i xem
xÃ©t cÃ¡c nghiÃªn cá»©u RALM liÃªn quan gáº§n Ä‘Ã¢y. ChÃºng
tÃ´i má»Ÿ rá»™ng Ä‘iá»u nÃ y cho cÃ¡c nghiÃªn cá»©u liÃªn quan
khÃ¡c trong Â§6.
CÃ¡c nghiÃªn cá»©u sá»›m vá» RALMs, cháº³ng háº¡n nhÆ°
kNN-LM (Khandelwal et al., 2020) Ä‘Ã£ sá»­ dá»¥ng truy
xuáº¥t Ä‘á»ƒ cáº£i thiá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯ báº±ng cÃ¡ch ná»™i
suy phÃ¢n phá»‘i tá»« tiáº¿p theo Ä‘Æ°á»£c táº¡o bá»Ÿi LM vá»›i
má»™t phÃ¢n phá»‘i Ä‘Æ°á»£c Ä‘á» xuáº¥t thÃ´ng qua cÆ¡ cháº¿ truy
xuáº¥t chá»‰ táº¡i thá»i Ä‘iá»ƒm kiá»ƒm tra. Borgeaud et al.
(2022) sau Ä‘Ã³ Ä‘á» xuáº¥t Chunked Cross-Attention
(CCA), trong Ä‘Ã³ truy xuáº¥t cÅ©ng Ä‘Æ°á»£c thá»±c hiá»‡n táº¡i
thá»i Ä‘iá»ƒm huáº¥n luyá»‡n, vÃ  káº¿t quáº£ truy xuáº¥t Ä‘Æ°á»£c
há»£p nháº¥t sÃ¢u vÃ o cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c táº¡o bá»Ÿi má»™t
bá»™ giáº£i mÃ£ Transformer thÃ´ng qua attention. Tuy
nhiÃªn, bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n riÃªng biá»‡t vÃ 
giá»¯ cá»‘ Ä‘á»‹nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, Ä‘iá»u nÃ y
ngÄƒn cáº£n nÃ³ thÃ­ch á»©ng vá»›i LM trong suá»‘t quÃ¡ trÃ¬nh
huáº¥n luyá»‡n.
TRIME (Zhong et al., 2022), giá»‘ng nhÆ° nghiÃªn
cá»©u nÃ y, Ä‘Ã£ huáº¥n luyá»‡n má»™t LM Ä‘Æ°á»£c tÄƒng cÆ°á»ng
truy xuáº¥t tá»« Ä‘áº§u trong Ä‘Ã³ thÃ nh pháº§n truy xuáº¥t vÃ 
LM giáº£i mÃ£ Ä‘Æ°á»£c huáº¥n luyá»‡n chung. NghiÃªn cá»©u
cá»§a chÃºng tÃ´i khÃ¡c vá»›i TRIME á»Ÿ hai khÃ­a cáº¡nh:
Äáº§u tiÃªn, TRIME, giá»‘ng nhÆ° kNN-LM, káº¿t há»£p
thÃ´ng tin tá»« bá»™ truy xuáº¥t theo cÃ¡ch nÃ´ng qua ná»™i
suy phÃ¢n phá»‘i, trong khi chÃºng tÃ´i Ã¡p dá»¥ng CCA
nhÆ° má»™t cÆ¡ cháº¿ há»£p nháº¥t sÃ¢u hÆ¡n. Thá»© hai, TRIME
táº­n dá»¥ng cÃ¡c gá»£i Ã½ tá»« vá»±ng Ä‘á»ƒ giÃ¡m sÃ¡t bá»™ truy xuáº¥t,
tá»©c lÃ , cho má»™t truy váº¥n, bá»™ truy xuáº¥t TRIME há»c
truy xuáº¥t cÃ¡c ngá»¯ cáº£nh sáº½ dáº«n Ä‘áº¿n viá»‡c táº¡o ra cÃ¹ng
má»™t token nhÆ° truy váº¥n. NgÆ°á»£c láº¡i, chÃºng tÃ´i sá»­
dá»¥ng má»™t LM cháº¥m Ä‘iá»ƒm Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c Ä‘oáº¡n vÄƒn
báº£n nÃ o cÃ³ liÃªn quan Ä‘á»ƒ tÄƒng xÃ¡c suáº¥t cá»§a Ä‘oáº¡n Ä‘Æ°á»£c
táº¡o, Ä‘iá»u nÃ y dáº«n Ä‘áº¿n truy xuáº¥t ngá»¯ nghÄ©a hÆ¡n. Äiá»u
nÃ y tÆ°Æ¡ng tá»± nhÆ° EPR (Rubin et al., 2022), Ä‘Ã£ sá»­
dá»¥ng Ã½ tÆ°á»Ÿng nÃ y Ä‘á»ƒ há»c truy xuáº¥t cÃ¡c gá»£i Ã½ cho
há»c táº­p trong ngá»¯ cáº£nh, vÃ  chÆ°ng cáº¥t perplexity
trong Atlas (Izacard et al.,

--- TRANG 3 ---
2022b). Tuy nhiÃªn, Atlas khÃ´ng huáº¥n luyá»‡n bá»™ truy
xuáº¥t vÃ  LM tá»« Ä‘áº§u vÃ  lÃ  má»™t mÃ´ hÃ¬nh mÃ£ hÃ³a-giáº£i
mÃ£, phÃ¹ há»£p hÆ¡n cho cÃ¡c nhiá»‡m vá»¥ chuyÃªn sÃ¢u vá»
kiáº¿n thá»©c. NgÆ°á»£c láº¡i, chÃºng tÃ´i huáº¥n luyá»‡n tá»« Ä‘áº§u
vÃ  sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh giáº£i mÃ£, phÃ¹ há»£p hÆ¡n cho
viá»‡c mÃ´ hÃ¬nh hÃ³a vÄƒn báº£n dÃ i.
3 Retrieval-Pretrained Transformer
Thiáº¿t láº­p váº¥n Ä‘á» Giá»‘ng nhÆ° RETRO (Borgeaud et al.,
2022), RPT lÃ  má»™t LM Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy xuáº¥t
theo Ä‘oáº¡n, chia chuá»—i Ä‘áº§u vÃ o thÃ nh cÃ¡c Ä‘oáº¡n Ä‘á»ƒ
truy xuáº¥t. Cá»¥ thá»ƒ, cho má»™t chuá»—i L token Ä‘áº§u vÃ o,
(x1, x2, . . . , xL), chÃºng tÃ´i phÃ¢n chia nÃ³ thÃ nh má»™t
chuá»—i â„“=L/m Ä‘oáº¡n khÃ´ng chá»“ng láº¥p cÃ³ Ä‘á»™ dÃ i m,
Ä‘Æ°á»£c kÃ½ hiá»‡u bá»Ÿi C= (c1, c2, . . . , câ„“). Äá»‘i vá»›i má»—i
Ä‘oáº¡n truy váº¥n cÃ³ thá»ƒ, cq=ci, mÃ´ hÃ¬nh sáº½ truy xuáº¥t
má»™t táº­p con tá»‘i Ä‘a Kâ‰ªâ„“ Ä‘oáº¡n, R(cq)âŠ‚ C<i= (c1, c2, ..., ciâˆ’w),
trong Ä‘Ã³ C<i lÃ  táº­p há»£p cÃ¡c Ä‘oáº¡n cÃ³ thá»ƒ truy xuáº¥t
cho ci, loáº¡i trá»« w Ä‘oáº¡n mÃ  nÃ³ Ä‘Ã£ cÃ³ quyá»n truy cáº­p
thÃ´ng qua attention nhÃ¢n quáº£ tá»±. Má»¥c tiÃªu lÃ  há»c
má»™t mÃ´ hÃ¬nh truy xuáº¥t má»™t táº­p con Ä‘oáº¡n, R(cq),
lÃ m tÄƒng xÃ¡c suáº¥t sinh tá»± Ä‘á»™ng cá»§a Ä‘oáº¡n má»¥c tiÃªu
ct=ci+1.
ChÃºng tÃ´i trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh theo hai
pháº§n. Äáº§u tiÃªn, kiáº¿n trÃºc cá»§a chÃºng tÃ´i (Â§3.1), táº­n
dá»¥ng CCA Ä‘á»ƒ há»£p nháº¥t cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c truy xuáº¥t
vÃ o LM, nhÆ°ng thÃªm má»™t thÃ nh pháº§n truy xuáº¥t Ä‘Æ°á»£c
há»c. Thá»© hai, chÃºng tÃ´i trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p huáº¥n
luyá»‡n (Â§3.2-Â§3.3), trong Ä‘Ã³ bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n
luyá»‡n Ä‘á»ƒ truy xuáº¥t cÃ¡c Ä‘oáº¡n há»¯u Ã­ch cho viá»‡c táº¡o ra
má»™t Ä‘oáº¡n tÆ°Æ¡ng lai theo má»™t LM tham chiáº¿u.
3.1 Kiáº¿n trÃºc MÃ´ hÃ¬nh
HÃ¬nh 2 minh há»a kiáº¿n trÃºc cá»§a chÃºng tÃ´i, trong Ä‘Ã³
Ä‘áº§u vÃ o cÃ³ 45 token Ä‘áº§u vÃ o Ä‘Æ°á»£c chia thÃ nh 9 Ä‘oáº¡n,
vÃ  attention nhÃ¢n quáº£ tá»± Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn w= 3
Ä‘oáº¡n (15 token). PhÃ­a bÃªn trÃ¡i mÃ´ táº£ ngÄƒn xáº¿p giáº£i
mÃ£ ("bá»™ Ä‘á»c"), vÃ  phÃ­a bÃªn pháº£i lÃ  bá»™ truy xuáº¥t. Bá»™
Ä‘á»c Ä‘Æ°á»£c chia thÃ nh hai, trong Ä‘Ã³ n/2 lá»›p dÆ°á»›i (bá»™
giáº£i mÃ£ dÆ°á»›i) lÃ  cÃ¡c lá»›p giáº£i mÃ£ Transformer tiÃªu
chuáº©n nháº­n w Ä‘oáº¡n lÃ m Ä‘áº§u vÃ o vÃ  xuáº¥t ra cÃ¡c biá»ƒu
diá»…n sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi bá»™ truy xuáº¥t vÃ  cÃ¡c lá»›p
giáº£i mÃ£ trÃªn.
n/2 lá»›p trÃªn (bá»™ giáº£i mÃ£ trÃªn) sá»­ dá»¥ng Chunked
Cross-Attention (CCA) Ä‘á»ƒ há»£p nháº¥t thÃ´ng tin tá»«
cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m top-K Ä‘Æ°á»£c truy xuáº¥t bá»Ÿi bá»™
truy xuáº¥t trá»Ÿ láº¡i vÃ o LM. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c
lá»›p CCA tiÃªu chuáº©n tá»« RETRO (Borgeaud et al.,
2022), trong Ä‘Ã³ Ä‘á»‘i vá»›i má»—i má»™t trong â„“ Ä‘oáº¡n, cÃ¡c
truy váº¥n lÃ  cÃ¡c biá»ƒu diá»…n m token cá»§a Ä‘oáº¡n Ä‘Ã³ Ä‘Æ°á»£c
xuáº¥t ra bá»Ÿi attention nhÃ¢n quáº£, vÃ  cÃ¡c khÃ³a vÃ  giÃ¡
trá»‹ lÃ  cÃ¡c biá»ƒu diá»…n token cho cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m
top-K Ä‘Æ°á»£c xuáº¥t ra bá»Ÿi bá»™ truy xuáº¥t.Â¹
Tiáº¿p theo, chÃºng tÃ´i mÃ´ táº£ thÃ nh pháº§n truy xuáº¥t,
cÃ¹ng vá»›i cÆ¡ cháº¿ cá»•ng hÃ ng xÃ³m Ä‘á»ƒ Ä‘iá»u cháº¿ tÃ¡c
Ä‘á»™ng cá»§a cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c truy xuáº¥t.
Bá»™ truy xuáº¥t Bá»™ truy xuáº¥t nháº­n cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c
xuáº¥t ra bá»Ÿi bá»™ giáº£i mÃ£ dÆ°á»›i lÃ m Ä‘áº§u vÃ o vÃ  táº¡o ra
má»™t Ä‘iá»ƒm sá»‘ tÆ°Æ¡ng tá»± cho má»—i cáº·p Ä‘oáº¡n. Cho má»™t
Ä‘oáº¡n truy váº¥n cq, Ä‘iá»ƒm sá»‘ dá»±a trÃªn truy váº¥n cho
má»—i Ä‘oáº¡n cÃ³ thá»ƒ truy xuáº¥t c lÃ  sq(c) =âŸ¨WQcq, WKcâŸ©,
trong Ä‘Ã³ WQ, WKâˆˆRdÃ—d lÃ  cÃ¡c phÃ©p chiáº¿u tuyáº¿n
tÃ­nh Ä‘Æ°á»£c há»c, vÃ  cq vÃ  c lÃ  cÃ¡c biá»ƒu diá»…n Ä‘oáº¡n.
Äá»‘i vá»›i má»™t Ä‘oáº¡n dÃ i m token c, chÃºng tÃ´i tÃ­nh toÃ¡n
biá»ƒu diá»…n c cá»§a nÃ³ báº±ng cÃ¡ch Ã¡p dá»¥ng attention hai
chiá»u trÃªn cÃ¡c token Ä‘oáº¡n, sau Ä‘Ã³ lÃ  mean-pooling
theo chiá»u thá»i gian. Äiá»u nÃ y duy trÃ¬ tÃ­nh nhÃ¢n
quáº£, vÃ¬ cÃ¡c biá»ƒu diá»…n nÃ y chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng trong
quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n Ä‘oáº¡n tiáº¿p theo.
Khi Ä‘iá»ƒm sá»‘ cho táº¥t cáº£ cÃ¡c cáº·p Ä‘oáº¡n Ä‘Æ°á»£c tÃ­nh toÃ¡n,
cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m Ä‘Æ°á»£c truy xuáº¥t R(cq), cho má»—i
Ä‘oáº¡n truy váº¥n, cq, bao gá»“m cÃ¡c Ä‘oáº¡n cÃ³ thá»ƒ truy xuáº¥t
cÃ³ Ä‘iá»ƒm sá»‘ cao nháº¥t top-K cá»§a nÃ³. Sau Ä‘Ã³, Ä‘á»‘i vá»›i
má»—i Ä‘oáº¡n cjâˆˆ R(cq), chÃºng tÃ´i ná»‘i cÃ¡c biá»ƒu diá»…n
cá»§a Ä‘oáº¡n tiáº¿p theo cj+1 Ä‘á»ƒ cung cáº¥p ngá»¯ cáº£nh bá»•
sung, vÃ  biá»ƒu diá»…n cuá»‘i cÃ¹ng cho táº¥t cáº£ cÃ¡c hÃ ng
xÃ³m cá»§a táº¥t cáº£ cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c cho bá»Ÿi má»™t tensor
CâˆˆRâ„“Ã—KÃ—2mÃ—d.Â²
Tá»•ng thá»ƒ (vÃ  khÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p
nhÆ° TRIME vÃ  kNN-LM), bá»™ truy xuáº¥t lÃ  má»™t pháº§n
khÃ´ng thá»ƒ thiáº¿u cá»§a LM, trong Ä‘Ã³ bá»™ giáº£i mÃ£ dÆ°á»›i
tÃ­nh toÃ¡n cÃ¡c biá»ƒu diá»…n cho bá»™ truy xuáº¥t (mÃ  chÃºng
tÃ´i gá»i lÃ  tá»± truy xuáº¥t), vÃ  bá»™ giáº£i mÃ£ trÃªn tiÃªu thá»¥
cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c táº¡o bá»Ÿi bá»™ truy xuáº¥t.
Cá»•ng hÃ ng xÃ³m ChÃºng tÃ´i thÃªm má»™t cÆ¡ cháº¿ cá»•ng
hÃ ng xÃ³m Ä‘á»ƒ lá»±a chá»n má»m cÃ¡c biá»ƒu diá»…n hÃ ng xÃ³m
há»¯u Ã­ch Ä‘á»ƒ há»£p nháº¥t vÃ o bá»™ giáº£i mÃ£ trÃªn. Cho
Ci,kâˆˆR2mÃ—d lÃ  cÃ¡c biá»ƒu diá»…n token cho hÃ ng xÃ³m
thá»© k cá»§a Ä‘oáº¡n ci. ChÃºng tÃ´i mean-pool theo chiá»u
thá»i gian Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t vector Ä‰i,k cho má»—i Ä‘oáº¡n
hÃ ng xÃ³m. Sau Ä‘Ã³, chÃºng tÃ´i lÃ m phong phÃº biá»ƒu
diá»…n hÃ ng xÃ³m cá»§a má»—i Ä‘oáº¡n báº±ng cÃ¡ch Ã¡p dá»¥ng
attention nhÃ¢n quáº£ â€“ má»™t biá»ƒu diá»…n Ä‘oáº¡n hÃ ng xÃ³m
Ä‰i,k attend Ä‘áº¿n cÃ¡c Ä‘oáº¡n Ä‘á»©ng trÆ°á»›c nÃ³ hoáº·c Ä‘áº¿n
cÃ¡c hÃ ng xÃ³m cá»§a cÃ¹ng má»™t Ä‘oáº¡n ci Ä‘Æ°á»£c xáº¿p háº¡ng
cao hÆ¡n. Cuá»‘i cÃ¹ng, Ä‘á»‘i vá»›i má»—i Ä‘oáº¡n, chÃºng tÃ´i cÃ³
Ä‘Æ°á»£c biá»ƒu diá»…n Ä‘Æ°á»£c truy xuáº¥t cÃ³ cá»•ng báº±ng cÃ¡ch
nhÃ¢n cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c tÄƒng cÆ°á»ng vá»›i má»™t Ä‘iá»ƒm
sá»‘ cá»•ng:
Â¹Äá»ƒ biáº¿t chi tiáº¿t Ä‘áº§y Ä‘á»§ vá» CCA, xem Borgeaud et al. (2022).
Â²TÆ°Æ¡ng tá»± nhÆ° RETRO, cÃ¡c biá»ƒu diá»…n token cá»§a cÃ¡c Ä‘oáº¡n
Ä‘Æ°á»£c truy xuáº¥t cÅ©ng Ä‘Æ°á»£c tÄƒng cÆ°á»ng thÃ´ng qua cross-attention
trÃªn cÃ¡c token cá»§a Ä‘oáº¡n truy váº¥n, cq.

--- TRANG 4 ---
Attention NhÃ¢nquáº£Táº§ng Feed ForwardChunked Cross AttentionTop-KQKVAttention NhÃ¢n quáº£
Pool + ProjectTáº§ng Feed Forward
Bi-directionalAttentionâŸ¨|||||||||â‹…aaaa		=7.1ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘1âŸ¨|||||||||â‹…aaaa		=0.3ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘2âŸ¨|||||||||â‹…aaaa		=5.2ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘3âŸ¨|||||||||â‹…aaaa		=10.8ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘‡1ğ‘1ğ‘!âŸ¨|||||||||â‹…aaaa		=âˆ’âˆğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘" â‹®âŸ¨|||||||||â‹…aaaa		=4.8ğ‘1ğ‘1ğ‘1ğ‘8ğ‘1ğ‘1ğ‘1ğ‘# â‹®Äá»ŒCğ‘1ğ‘2ğ‘3ğ‘4ğ‘5ğ‘6ğ‘7ğ‘8ğ‘9Ã—	ğ‘›!"#$%&2HÃ ng xÃ³m Ä‘Æ°á»£c mÃ£ hÃ³a
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1Token Äáº§u vÃ o
Bá»™ Giáº£i mÃ£ DÆ°á»›iCháº¥m Ä‘iá»ƒm Ä‘oáº¡nCá»•ng hÃ ng xÃ³m
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1
ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1Bá»™ Truy xuáº¥tBá»™ Giáº£i mÃ£ TrÃªn
Ã—	ğ‘›!"#$%&2HÃ¬nh 2: Kiáº¿n trÃºc cá»§a Retrieval-Pretrained Transformer, trong Ä‘Ã³ má»™t Ä‘áº§u vÃ o 45 token Ä‘Æ°á»£c hiá»ƒn thá»‹, bao gá»“m
9 Ä‘oáº¡n, vÃ  attention nhÃ¢n quáº£ tá»± Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn 15 token. PhÃ­a bÃªn trÃ¡i hiá»ƒn thá»‹ ngÄƒn xáº¿p giáº£i mÃ£, trong Ä‘Ã³
n/2 lá»›p dÆ°á»›i lÃ  cÃ¡c lá»›p giáº£i mÃ£ Transformer tiÃªu chuáº©n, vÃ  n/2 lá»›p trÃªn cÅ©ng bao gá»“m cÃ¡c lá»›p chunked cross-attention
há»£p nháº¥t thÃ´ng tin tá»« cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c truy xuáº¥t. PhÃ­a bÃªn pháº£i hiá»ƒn thá»‹ bá»™ truy xuáº¥t, nháº­n má»™t Ä‘oáº¡n vÃ  truy xuáº¥t
K Ä‘oáº¡n cÃ³ Ä‘iá»ƒm sá»‘ cao nháº¥t Ä‘Ã£ xuáº¥t hiá»‡n trÆ°á»›c Ä‘Ã³ trong tÃ i liá»‡u.
Cgi,k= max {Î·, Ïƒ(wng Ä‰i,k
d)} Â·Ci,k trong Ä‘Ã³ wng lÃ 
má»™t vector tham sá»‘ Ä‘Æ°á»£c há»c, Î· lÃ  má»™t giÃ¡ trá»‹ nhá»
nháº±m duy trÃ¬ dÃ²ng gradient,Â³ vÃ  Ïƒ lÃ  hÃ m kÃ­ch hoáº¡t
sigmoid. Cuá»‘i cÃ¹ng, trong bá»™ giáº£i mÃ£ trÃªn, khi CCA
Ä‘Æ°á»£c thá»±c hiá»‡n, cÃ¡c khÃ³a vÃ  giÃ¡ trá»‹ lÃ  Cgi,k.
3.2 TÃ­n hiá»‡u GiÃ¡m sÃ¡t
Äá»‘i vá»›i má»—i Ä‘oáº¡n truy váº¥n cq=ci, chÃºng tÃ´i muá»‘n
xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m sáº½ há»¯u Ã­ch cho viá»‡c
táº¡o ra ct=ci+1, vÃ  sá»­ dá»¥ng nhá»¯ng Ä‘oáº¡n hÃ ng xÃ³m
Ä‘Ã³ lÃ m tÃ­n hiá»‡u giÃ¡m sÃ¡t cho bá»™ truy xuáº¥t. TÆ°Æ¡ng
tá»± nhÆ° Rubin et al. (2022), chÃºng tÃ´i cÃ³ thá»ƒ khai
thÃ¡c thá»±c táº¿ ráº±ng chÃºng tÃ´i Ä‘ang táº¡o ra dá»¯ liá»‡u huáº¥n
luyá»‡n vÃ  sá»­ dá»¥ng thÃ´ng tin tá»« chÃ­nh ct Ä‘á»ƒ táº¡o ra
Ä‘iá»ƒm sá»‘ nhÆ° váº­y. KhÃ´ng giá»‘ng nhÆ° Zhong et al.
(2022), ngÆ°á»i chá»‰ sá»­ dá»¥ng cÃ¡c gá»£i Ã½ tá»« vá»±ng, chÃºng
tÃ´i sáº½ sá»­ dá»¥ng má»™t LM cháº¥m Ä‘iá»ƒm Ä‘á»™c láº­p cho má»¥c
Ä‘Ã­ch nÃ y.
Cháº¥m Ä‘iá»ƒm má»—i Ä‘oáº¡n w.r.t táº¥t cáº£ cÃ¡c Ä‘oáº¡n trÆ°á»›c
Ä‘Ã³ lÃ  báº­c hai theo sá»‘ lÆ°á»£ng Ä‘oáº¡n trong má»™t tÃ i liá»‡u,
vÃ  do Ä‘Ã³ khÃ³ tÃ­nh toÃ¡n. VÃ¬ váº­y, chÃºng tÃ´i sá»­ dá»¥ng
má»™t bá»™ truy xuáº¥t khÃ´ng giÃ¡m sÃ¡t BM25 Ä‘Æ¡n giáº£n
(Robertson and Zaragoza, 2009) nháº­n Ä‘áº§u vÃ o lÃ 
sá»± ná»‘i cá»§a cÃ¡c Ä‘oáº¡n (cq, ct) = (ci, ci+1) vÃ  tráº£ vá»
má»™t táº­p há»£p cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m á»©ng viÃªn, RÌ„ âŠ‚ C(cq),
cÃ³ Ä‘á»™ chá»“ng láº¥p tá»« vá»±ng cao vá»›i Ä‘oáº¡n hiá»‡n táº¡i vÃ 
tiáº¿p theo. Bá»™ truy xuáº¥t nÃ y cÃ³ quyá»n truy cáº­p vÃ o
cÃ¡c token cáº§n Ä‘Æ°á»£c táº¡o bá»Ÿi LM, Ä‘iá»u nÃ y Ä‘Æ°á»£c cho
phÃ©p táº¡i thá»i Ä‘iá»ƒm huáº¥n luyá»‡n.
Cho Ä lÃ  má»™t LM Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»™c láº­p, vÃ  cho
cÌ„j lÃ  sá»± ná»‘i (cj, cj+1). ChÃºng tÃ´i tÃ­nh toÃ¡n má»™t
Ä‘iá»ƒm sá»‘ st(cÌ„j) pháº£n Ã¡nh liá»‡u thÃ´ng tin trong cÌ„j cÃ³
há»¯u Ã­ch hÆ¡n cho viá»‡c giáº£i mÃ£ ct so vá»›i cÃ¡c Ä‘oáº¡n
gáº§n vá»›i cq. Cá»¥ thá»ƒ, Ä‘iá»ƒm sá»‘ dá»±a trÃªn má»¥c tiÃªu cho
má»™t Ä‘oáº¡n á»©ng viÃªn lÃ 
st(cÌ„j) = logProb Ä
ct|cj, cj+1, cq
Prob Ä(ct|ciâˆ’2, ciâˆ’1, cq).
Äiá»ƒm sá»‘ nÃ y lÃ  dÆ°Æ¡ng khi thÃ´ng tin trong cÌ„j há»¯u
Ã­ch hÆ¡n cho viá»‡c giáº£i mÃ£ ct so vá»›i thÃ´ng tin trong
hai Ä‘oáº¡n trÆ°á»›c Ä‘Ã³ (ciâˆ’2, ciâˆ’1).
ChÃºng tÃ´i Ã¡p dá»¥ng hÃ m cháº¥m Ä‘iá»ƒm nÃ y cho táº¥t cáº£
cÃ¡c Ä‘oáº¡n, vÃ  Ä‘á»‹nh nghÄ©a cho má»—i Ä‘oáº¡n truy váº¥n cq
táº­p há»£p cÃ¡c Ä‘oáº¡n dÆ°Æ¡ng Rqpos, bao gá»“m cÃ¡c á»©ng
viÃªn mÃ  st(Â·)>0. Äiá»u nÃ y sáº½ dáº«n Ä‘áº¿n cÃ¡c Ä‘oáº¡n
há»¯u Ã­ch, vÃ¬ má»—i Ä‘oáº¡n á»©ng viÃªn Ã­t nháº¥t lÃ  tá»‘t nhÆ°
ngá»¯ cáº£nh Ä‘á»‹a phÆ°Æ¡ng. Vá»›i thá»© tá»± nÃ y trong táº§m tay,
chÃºng tÃ´i cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n
luyá»‡n truy xuáº¥t tiÃªu chuáº©n.
3.3 Huáº¥n luyá»‡n
Äá»ƒ huáº¥n luyá»‡n cÃ¡c tham sá»‘ cá»§a thÃ nh pháº§n truy
xuáº¥t, chÃºng tÃ´i thÃ­ch á»©ng loss LambdaRank Ä‘Æ°á»£c
sá»­ dá»¥ng rá»™ng rÃ£i (Burges et al., 2006). Loss cho
má»—i Ä‘oáº¡n truy váº¥n
Â³ChÃºng tÃ´i Ä‘áº·t Î·= 0.1 trong táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m cá»§a mÃ¬nh.

--- TRANG 5 ---
cq (w.r.t cÃ¡c Ä‘oáº¡n cÃ³ thá»ƒ truy xuáº¥t cá»§a nÃ³) lÃ :
Lret(cq) =
X
{j,l:cÌ„lâˆˆRqpos,st(cÌ„l)>st(cÌ„j)}Î»jlmax (0 , Ï„âˆ’(sq(cl)âˆ’sq(cj)))
trong Ä‘Ã³ Ï„ lÃ  siÃªu tham sá»‘ margin, vÃ  Î»jl lÃ  há»‡ sá»‘
quy mÃ´ LambdaRank xem xÃ©t thá»© háº¡ng tÆ°Æ¡ng Ä‘á»‘i
cá»§a má»—i á»©ng viÃªn. Loss nÃ y khÃ¡c khÃ´ng khi Ä‘á»‘i
vá»›i má»™t sá»‘ cáº·p á»©ng viÃªn, Ä‘iá»ƒm sá»‘ dá»±a trÃªn má»¥c
tiÃªu khÃ´ng Ä‘á»“ng Ã½ (vá»›i margin Ï„) vá»›i thá»© háº¡ng cá»§a
Ä‘iá»ƒm sá»‘ dá»±a trÃªn truy váº¥n cho cÃ¡c á»©ng viÃªn trong
Rqpos. Tá»‘i Æ°u hÃ³a hÃ m loss nÃ y cho phÃ©p RPT phÃ¢n
biá»‡t giá»¯a cÃ¡c Ä‘oáº¡n liÃªn quan vÃ  khÃ´ng liÃªn quan.
Loss cuá»‘i cÃ¹ng cá»§a chÃºng tÃ´i lÃ  LLM+Î±retLret, trong
Ä‘Ã³ LLM lÃ  loss LM tiÃªu chuáº©n vÃ  Î±ret lÃ  há»‡ sá»‘ loss
truy xuáº¥t, tÄƒng tuyáº¿n tÃ­nh trong 100K bÆ°á»›c Ä‘áº§u.
ChÃºng tÃ´i cÅ©ng tÄƒng Ï„ tuyáº¿n tÃ­nh trong quÃ¡ trÃ¬nh
huáº¥n luyá»‡n.
3.4 Chi tiáº¿t Triá»ƒn khai Quan trá»ng
Láº¥y máº«u theo lá»‹ch trÃ¬nh Äá»ƒ giáº£m sá»± khÃ´ng khá»›p
giá»¯a huáº¥n luyá»‡n-kiá»ƒm tra, chÃºng tÃ´i Ã¡p dá»¥ng láº¥y
máº«u theo lá»‹ch trÃ¬nh (Bengio et al., 2015) trong
quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Cá»¥ thá»ƒ, sau khi tÃ­nh toÃ¡n
cÃ¡c Ä‘oáº¡n hÃ ng xÃ³m top-K, chÃºng tÃ´i sá»­ dá»¥ng nhá»¯ng
hÃ ng xÃ³m nÃ y vá»›i xÃ¡c suáº¥t 1âˆ’pss, vÃ  vá»›i xÃ¡c suáº¥t
pss cÃ¡c á»©ng viÃªn cÃ³ Ä‘iá»ƒm sá»‘ top-K tá»« Rqpos lÃ m
Ä‘áº§u vÃ o cho CCA. ChÃºng tÃ´i á»§ pss tá»« 1 vá» 0 trong
90% Ä‘áº§u cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n vá»›i lá»‹ch trÃ¬nh
cosine. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh dáº§n dáº§n há»c
sá»­ dá»¥ng cÃ¡c dá»± Ä‘oÃ¡n cá»§a chÃ­nh mÃ¬nh. ChÃºng tÃ´i
bÃ¡o cÃ¡o hiá»‡u á»©ng cá»§a Ä‘iá»u nÃ y trong Â§5.3.
Attention cá»­a sá»• trÆ°á»£t táº¡i thá»i Ä‘iá»ƒm huáº¥n luyá»‡n
vÃ  suy luáº­n NhÆ° mÃ´ táº£ trong Â§3, bá»™ giáº£i mÃ£ nháº­n
w Ä‘oáº¡n, má»—i Ä‘oáº¡n cÃ³ m token lÃ m Ä‘áº§u vÃ o, vÃ  Ã¡p
dá»¥ng attention nhÃ¢n quáº£ trÃªn chÃºng. Trong thá»±c
táº¿, Ä‘á»ƒ cung cáº¥p cho cÃ¡c token Ä‘áº§u tiÃªn quyá»n truy
cáº­p vÃ o cÃ¡c token trÆ°á»›c Ä‘Ã³, chÃºng tÃ´i sá»­ dá»¥ng cÆ¡
cháº¿ attention cá»­a sá»• trÆ°á»£t (Dai et al., 2019; Beltagy
et al., 2020; Ivgi et al., 2023), trong Ä‘Ã³ sá»‘ lÆ°á»£ng
token trong má»™t cá»­a sá»• lÃ  2,048 vÃ  stride lÃ  1,024.
VÃ¬ váº­y, Ä‘áº§u vÃ o cho má»—i cá»­a sá»• lÃ  2,048 token vÃ 
Ä‘áº§u ra lÃ  cÃ¡c biá»ƒu diá»…n cho 1,024 token cuá»‘i cÃ¹ng,
sá»­ dá»¥ng cÃ¡c khÃ³a vÃ  giÃ¡ trá»‹ cá»§a 1,024 token trÆ°á»›c
Ä‘Ã³ Ä‘á»ƒ ngá»¯ cáº£nh hÃ³a.
Táº¡i thá»i Ä‘iá»ƒm suy luáº­n, má»™t quy trÃ¬nh tÆ°Æ¡ng tá»±
Ä‘Æ°á»£c Ã¡p dá»¥ng. ChÃºng tÃ´i tÃ­nh toÃ¡n vÃ  cache cÃ¡c
biá»ƒu diá»…n khÃ³a vÃ  giÃ¡ trá»‹ cho cÃ¡c Ä‘oáº¡n 1,024 token,
sá»­ dá»¥ng chÃºng lÃ m ngá»¯ cáº£nh Ä‘á»ƒ táº¡o ra hoáº·c Æ°á»›c
tÃ­nh xÃ¡c suáº¥t cá»§a Ä‘oáº¡n tiáº¿p theo.
Truy xuáº¥t táº¡i thá»i Ä‘iá»ƒm suy luáº­n Trong quÃ¡ trÃ¬nh
huáº¥n luyá»‡n, chÃºng tÃ´i mÃ£ hÃ³a trong má»—i batch cÃ¡c
chuá»—i cÃ³ Ä‘á»™ dÃ i 16K vÃ  truy xuáº¥t cÃ¡c Ä‘oáº¡n tá»«
nhá»¯ng 16k token Ä‘Æ°á»£c mÃ£ hÃ³a Ä‘Ã³. Tuy nhiÃªn, táº¡i thá»i Ä‘iá»ƒm suy luáº­n, bá»™ truy xuáº¥t cung cáº¥p quyá»n
truy cáº­p vÃ o táº¥t cáº£ cÃ¡c token tá»« Ä‘áº§u tÃ i liá»‡u, trong
Ä‘Ã³ chÃºng tÃ´i lÆ°u trá»¯ cÃ¡c biá»ƒu diá»…n khÃ³a vÃ  bá»™ giáº£i
mÃ£ dÆ°á»›i trong má»™t chá»‰ má»¥c Faiss (Douze et al.,
2024) trÃªn CPU. Äá»‘i vá»›i má»—i Ä‘oáº¡n, chÃºng tÃ´i truy
váº¥n chá»‰ má»¥c báº±ng cÃ¡c biá»ƒu diá»…n truy váº¥n cá»§a Ä‘oáº¡n
vÃ  truy xuáº¥t cÃ¡c biá»ƒu diá»…n bá»™ giáº£i mÃ£ dÆ°á»›i top-K
vá»›i tÃ­ch vÃ´ hÆ°á»›ng cao nháº¥t.
Chi tiáº¿t bá»• sung Táº¡i thá»i Ä‘iá»ƒm huáº¥n luyá»‡n, chÃºng
tÃ´i sá»­ dá»¥ng cÃ¡c chuá»—i cÃ³ Ä‘á»™ dÃ i L= 16,384 token,
Ä‘Æ°á»£c chia thÃ nh 4 thiáº¿t bá»‹, má»—i thiáº¿t bá»‹ tiÃªu thá»¥
4,096 token. NhÆ° Ä‘Ã£ Ä‘á» cáº­p, ngÄƒn xáº¿p giáº£i mÃ£
nháº­n 2,048 token lÃ m Ä‘áº§u vÃ o (theo cÃ¡ch tiáº¿p cáº­n
cá»­a sá»• trÆ°á»£t), chá»©a â„“= 32 Ä‘oáº¡n cÃ³ Ä‘á»™ dÃ i m= 64.
ChÃºng tÃ´i sá»­ dá»¥ng Rotary Positional embedding
(Su et al., 2024), vÃ  huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh
trong 500K bÆ°á»›c trÃªn TPUv4-64, vá»›i kÃ­ch thÆ°á»›c
batch hiá»‡u quáº£ lÃ  217 token dáº«n Ä‘áº¿n tá»•ng ngÃ¢n
sÃ¡ch huáº¥n luyá»‡n lÃ  65 tá»· token.
Äá»‘i vá»›i táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n, chÃºng
tÃ´i sá»­ dá»¥ng tokenizer GPT-NeoX (Black et al.,
2022), Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn Pile (Gao et al., 2020)
vÃ  bao phá»§ cÃ¡c lÄ©nh vá»±c chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ (xem
Â§4). LÃ m mÃ´ hÃ¬nh ngÃ´n ngá»¯ cháº¥m Ä‘iá»ƒm cá»§a chÃºng
tÃ´i, chÃºng tÃ´i sá»­ dá»¥ng phiÃªn báº£n khá»­ trÃ¹ng láº·p 1.4B
tham sá»‘ cá»§a Pythia (Biderman et al., 2023), vÃ 
cháº¥m Ä‘iá»ƒm vá»›i nÃ³ 20 á»©ng viÃªn BM25 hÃ ng Ä‘áº§u.
MÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cÃ³ 12 lá»›p, chiá»u áº©n d= 1024,
vÃ  8 Ä‘áº§u attention vá»›i chiá»u Ä‘áº§u lÃ  128. ChÃºng tÃ´i
Ã¡p dá»¥ng CCA vá»›i 2 hÃ ng xÃ³m, trá»« khi Ä‘Æ°á»£c Ä‘á» cáº­p
khÃ¡c. Chi tiáº¿t triá»ƒn khai bá»• sung trong Phá»¥ lá»¥c A
vÃ  Ä‘á»™ phá»©c táº¡p lÃ½ thuyáº¿t cá»§a cÃ¡c lá»›p CCA trong
Phá»¥ lá»¥c B.
4 Bá»™ dá»¯ liá»‡u LM Táº§m xa
ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT trÃªn bá»‘n bá»™ dá»¯ liá»‡u, bao
phá»§ cÃ¡c lÄ©nh vá»±c nhÆ° sÃ¡ch, mÃ£ vÃ  viáº¿t toÃ¡n há»c,
Ä‘Ã²i há»i kháº£ nÄƒng gá»i láº¡i thÃ´ng tin qua khoáº£ng cÃ¡ch
xa. Báº£ng 1 vÃ  HÃ¬nh 3 cung cáº¥p thá»‘ng kÃª vá» kÃ­ch
thÆ°á»›c bá»™ dá»¯ liá»‡u vÃ  phÃ¢n phá»‘i Ä‘á»™ dÃ i tÃ i liá»‡u, cho
tháº¥y cÃ¡c tÃ i liá»‡u dÃ i trÃªn táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u vÃ 
Ä‘áº·c biá»‡t lÃ  PG19 vÃ  Books3, trong Ä‘Ã³ cÃ¡c tÃ i liá»‡u
thÆ°á»ng chá»©a 105 token trá»Ÿ lÃªn. ChÃºng tÃ´i xem xÃ©t
ngáº¯n gá»n cÃ¡c bá»™ dá»¯ liá»‡u.

--- TRANG 6 ---
0510%ArXiv
0510%CodeParrot
0510%PG19
102103104105106107
Äá»™ dÃ i chuá»—i0510%Books3HÃ¬nh 3: Biá»ƒu Ä‘á»“ phÃ¢n phá»‘i Ä‘á»™ dÃ i tÃ i liá»‡u theo token
trÃªn táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u. Trá»¥c x á»Ÿ thang logarit.
PG19 ÄÆ°á»£c giá»›i thiá»‡u trong Rae et al. (2020), PG19
lÃ  má»™t benchmark mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa Ä‘Æ°á»£c
sá»­ dá»¥ng rá»™ng rÃ£i chá»©a sÃ¡ch tá»« Project Gutenberg,
vÃ  bao phá»§ má»™t loáº¡t cÃ¡c thá»ƒ loáº¡i vÄƒn há»c, phong
cÃ¡ch vÃ  chá»§ Ä‘á». ChÃºng tÃ´i Ã¡p dá»¥ng thiáº¿t láº­p vÃ 
phÃ¢n chia dá»¯ liá»‡u chÃ­nh xÃ¡c tá»« cÃ¡c nghiÃªn cá»©u trÆ°á»›c
(Wu et al., 2022; Hutchins et al., 2022; Mehta et al.,
2023).
Books3 lÃ  má»™t corpus sÃ¡ch Ä‘Æ°á»£c phÃ¡t hÃ nh nhÆ°
má»™t pháº§n cá»§a Pile (Gao et al., 2020), chá»©a má»™t
bá»™ sÆ°u táº­p rá»™ng lá»›n cÃ¡c tÃ¡c pháº©m vÄƒn há»c tá»« cÃ¡c
lÄ©nh vá»±c khÃ¡c nhau. Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i,
chÃºng tÃ´i lÃ  nhá»¯ng ngÆ°á»i Ä‘áº§u tiÃªn sá»­ dá»¥ng corpus
nÃ y nhÆ° má»™t benchmark mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa.â´
CodeParrot (Wolf et al., 2023) lÃ  má»™t corpus mÃ£
Python sáº¡ch, gáº§n nhÆ° khá»­ trÃ¹ng láº·p tá»« cÃ¡c kho
GitHub khÃ¡c nhau. MÃ´ hÃ¬nh hÃ³a mÃ£ Ä‘Ã²i há»i hiá»ƒu
cÃ¡c máº«u vÃ  ngá»¯ cáº£nh hÃ³a thÃ´ng tin qua khoáº£ng cÃ¡ch
xa, lÃ m cho nÃ³ trá»Ÿ thÃ nh má»™t á»©ng viÃªn tá»± nhiÃªn Ä‘á»ƒ
kiá»ƒm tra cÃ¡c LM táº§m xa. Trong cÃ¡c thÃ­ nghiá»‡m cá»§a
chÃºng tÃ´i, chÃºng tÃ´i theo cÃ¡ch tiáº¿p cáº­n cá»§a Wu
et al. (2022), káº¿t há»£p cÃ¡c tá»‡p tá»« cÃ¹ng má»™t kho lÆ°u
trá»¯ Ä‘á»ƒ xÃ¢y dá»±ng má»™t corpus vá»›i cÃ¡c chuá»—i dÃ i hÆ¡n,
vÃ  táº¡o ra má»™t phÃ¢n chia train/test (xem Báº£ng 1).
ArXiv lÃ  má»™t corpus cÃ¡c bÃ i bÃ¡o preprint Ä‘Æ°á»£c
trÃ­ch xuáº¥t tá»« ArXiv. NÃ³ bao gá»“m cÃ¡c vÄƒn báº£n toÃ¡n
há»c Ä‘Ã²i há»i duy trÃ¬ tÃ­nh máº¡ch láº¡c vÃ  tham chiáº¿u
Ä‘áº¿n thÃ´ng tin Ä‘Ã£ Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³ trong vÄƒn báº£n má»Ÿ
rá»™ng.
â´ChÃºng tÃ´i khÃ´ng phÃ¡t hÃ nh benchmark nÃ y do cÃ¡c háº¡n cháº¿
báº£n quyá»n. CÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã£ Ä‘Ã¡nh giÃ¡ cÃ¡c LM táº§m xa
trÃªn corpus nÃ y (Wu et al., 2022; Hutchins et al., 2022;
Mehta et al., 2023), nhÆ°ng khÃ´ng phÃ¡t hÃ nh corpus cá»§a há».
VÃ¬ váº­y, chÃºng tÃ´i sá»­ dá»¥ng corpus Ä‘Æ°á»£c tiá»n xá»­ lÃ½ vÃ  phÃ¢n
chia dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p bá»Ÿi Azerbayev et al. (2023).
5 ThÃ­ nghiá»‡m
BÃ¢y giá» chÃºng tÃ´i chuyá»ƒn sang cÃ¡c thÃ­ nghiá»‡m Ä‘á»ƒ
so sÃ¡nh RPT vá»›i cÃ¡c nghiÃªn cá»©u trÆ°á»›c trÃªn bá»‘n bá»™
dá»¯ liá»‡u cá»§a chÃºng tÃ´i.
5.1 Thiáº¿t láº­p ThÃ­ nghiá»‡m
ChÃºng tÃ´i so sÃ¡nh vá»›i cÃ¡c baseline vÃ  oracle sau.
Transformer-XL Baseline Ä‘Æ¡n giáº£n nháº¥t cá»§a chÃºng
tÃ´i lÃ  má»™t ngÄƒn xáº¿p giáº£i mÃ£ transformer tiÃªu chuáº©n
vá»›i attention cá»­a sá»• trÆ°á»£t. NÃ³i cÃ¡ch khÃ¡c, chÃºng
tÃ´i Ä‘Æ¡n giáº£n loáº¡i bá» khá»i RPT thÃ nh pháº§n truy xuáº¥t
vÃ  cÃ¡c lá»›p CCA trong bá»™ giáº£i mÃ£ trÃªn. Sá»­ dá»¥ng
attention cá»­a sá»• trÆ°á»£t (nhÆ° mÃ´ táº£ trong Â§3.4) cÃ³
thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t biáº¿n thá»ƒ cá»§a Transformer-XL
(Dai et al., 2019). ChÃºng tÃ´i so sÃ¡nh RPT vá»›i
Transformer-XL trong nhiá»u thiáº¿t láº­p, má»™t thiáº¿t
láº­p trong Ä‘Ã³ chÃºng tÃ´i cÃ³ cÃ¹ng sá»‘ lÆ°á»£ng lá»›p vÃ 
bÆ°á»›c huáº¥n luyá»‡n cho cáº£ hai mÃ´ hÃ¬nh, vÃ  hai thiáº¿t
láº­p khÃ¡c trong Ä‘Ã³ chÃºng tÃ´i rÃ ng buá»™c sá»‘ lÆ°á»£ng
tham sá»‘ vÃ  FLOPs giá»¯a cÃ¡c mÃ´ hÃ¬nh.
RETRO ChÃºng tÃ´i triá»ƒn khai má»™t phiÃªn báº£n sá»­a
Ä‘á»•i cá»§a Borgeaud et al. (2022), má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c
tÄƒng cÆ°á»ng truy xuáº¥t, trong Ä‘Ã³ chÃºng tÃ´i cung cáº¥p
cÃ¡c hÃ ng xÃ³m top-K Ä‘Æ°á»£c truy xuáº¥t bá»Ÿi BM25âµ
lÃ m Ä‘áº§u vÃ o cho cÃ¡c lá»›p CCA trong bá»™ giáº£i mÃ£
trÃªn. Cá»¥ thá»ƒ, Borgeaud et al. (2022) thá»±c hiá»‡n
CCA trÃªn biá»ƒu diá»…n tá»« má»™t bá»™ mÃ£ hÃ³a hai chiá»u
riÃªng biá»‡t, trong khi biáº¿n thá»ƒ cá»§a chÃºng tÃ´i sá»­ dá»¥ng
cÃ¡c biá»ƒu diá»…n bá»™ giáº£i mÃ£ dÆ°á»›i nhÆ° má»™t sá»± thay tháº¿.
Äiá»u nÃ y lÃ m cho cÃ¡c kiáº¿n trÃºc RPT vÃ  RETRO
tÆ°Æ¡ng tá»± nhau hÆ¡n vÃ  cho phÃ©p Ä‘Ã¡nh giÃ¡ táº­p trung
vÃ o táº§m quan trá»ng cá»§a viá»‡c huáº¥n luyá»‡n bá»™ truy
xuáº¥t, Ä‘Ã¢y lÃ  trá»ng tÃ¢m cá»§a cÃ´ng viá»‡c cá»§a chÃºng
tÃ´i. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i sá»­ dá»¥ng
truy váº¥n (cq, ct), vÃ¬ chÃºng tÃ´i cÃ³ quyá»n truy cáº­p
vÃ o Ä‘oáº¡n má»¥c tiÃªu. Trong quÃ¡ trÃ¬nh suy luáº­n, chÃºng
tÃ´i sá»­ dá»¥ng cq.
RPT-Lex Má»™t phiÃªn báº£n cá»§a RPT, trong Ä‘Ã³ tÃ­n
hiá»‡u huáº¥n luyá»‡n Ä‘Æ°á»£c thu Ä‘Æ°á»£c hoÃ n toÃ n tá»« thÃ´ng
tin tá»« vá»±ng, tÆ°Æ¡ng tá»± nhÆ° TRIME (Zhong et al.,
2022). Cá»¥ thá»ƒ, táº­p há»£p cÃ¡c Ä‘oáº¡n dÆ°Æ¡ng Rqpos cho
má»™t Ä‘oáº¡n cq chá»©a 20 Ä‘oáº¡n hÃ ng Ä‘áº§u cÃ³ Ä‘iá»ƒm sá»‘
BM25 cao nháº¥t vá»›i (cq, ct).
RPT-Sem MÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c mÃ´
táº£ trong Â§3.
âµNghiÃªn cá»©u Ä‘á»“ng thá»i (Doostmohammadi et al., 2023) cho
tháº¥y ráº±ng huáº¥n luyá»‡n RETRO sá»­ dá»¥ng BM25 vÆ°á»£t trá»™i hÆ¡n
cÃ¡c phÆ°Æ¡ng phÃ¡p truy xuáº¥t dÃ y Ä‘áº·c.

--- TRANG 7 ---
MÃ´ hÃ¬nh ArXiv Code PG19 Books3 Params Time/update
TRANSFORMER -XL(TRIá»‚N KHAI Cá»¦A CHÃšNG TÃ”I) 3.11 2.30 11.48 15.00 202M 1 Ã—
+2LAYERS 3.07 2.26 11.2 14.52 228M 1.14 Ã—
1.5Ã—BÆ¯á»šC Bá»” SUNG 3.11 2.26 11.39 14.70 202M 1 Ã—
RETRO W . BM25 (TRIá»‚N KHAI Cá»¦A CHÃšNG TÃ”I) 2.94 2.17 11.44 14.60 236M 1.35 Ã—
RPT-L EX 2.92 2.23 11.59 14.32 242M 1.51 Ã—
RPT-S EM 2.77 2.17 10.96 13.91 242M 1.51 Ã—
W. 3HÃ€NG XÃ“M 2.75 2.16 10.92 13.87
W. 4HÃ€NG XÃ“M 2.74 2.15 10.93 13.91
MEMORIZING TRANSFORMER (32K) 2.92 2.18 10.97 14.40 212M 1.82 Ã—
MEMORIZING TRANSFORMER (65K) 2.93 2.15 10.99 14.3 212M 2.12 Ã—
BLOCK -RECURRENT TRANSFORMER 2.89 2.73 10.95 14.64 212M 1.56 Ã—
GRIFFIN 3.08 2.24 11.26 14.16 240M 1.15 Ã—
RPT-L EX W . ORACLE 2.80 2.12 10.88 13.30 242M 1.51 Ã—
RPT-S EM W . ORACLE 2.69 2.10 10.26 12.74 242M 1.51 Ã—
Báº£ng 2: Perplexity táº­p kiá»ƒm tra cho táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u cÃ¹ng vá»›i sá»‘ lÆ°á»£ng tham sá»‘ vÃ  má»©c tÄƒng tÆ°Æ¡ng Ä‘á»‘i trong
thá»i gian má»—i láº§n cáº­p nháº­t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n so vá»›i Transformer-XL. Trá»« khi Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh, cÃ¡c mÃ´ hÃ¬nh
Ä‘Æ°á»£c huáº¥n luyá»‡n trong 500k bÆ°á»›c vÃ  sá»­ dá»¥ng 2 hÃ ng xÃ³m trong quÃ¡ trÃ¬nh suy luáº­n.
Block-Recurrent Transformer ChÃºng tÃ´i sá»­ dá»¥ng
triá»ƒn khai huáº¥n luyá»‡n chÃ­nh thá»©câ¶ cá»§a Block-Recurrent
Transformer (Hutchins et al., 2022) vá»›i cáº¥u hÃ¬nh
máº·c Ä‘á»‹nh.
Memorizing Transformer ChÃºng tÃ´i sá»­ dá»¥ng triá»ƒn
khai chÃ­nh thá»©câ¶ cá»§a Memorizing Transformers
(Wu et al., 2022), vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh vÃ  kÃ­ch
thÆ°á»›c bá»™ nhá»› 32K vÃ  65K token.
Griffin Má»™t thay tháº¿ cho mÃ´ hÃ¬nh táº§m xa lÃ  sá»­ dá»¥ng
má»™t hybrid cá»§a attention vÃ  RNN tuyáº¿n tÃ­nh (Orvieto
et al., 2023; Gupta et al., 2023). ChÃºng tÃ´i Ä‘Ã¡nh
giÃ¡ Griffin (De et al., 2024), má»™t mÃ´ hÃ¬nh tiÃªn tiáº¿n
trong danh má»¥c nÃ y. ChÃºng tÃ´i thÃ­ch á»©ng triá»ƒn khai
chÃ­nh thá»©c, vÃ  bá»• sung baseline Transformer-XL
cá»§a chÃºng tÃ´i vá»›i 5 lá»›p recurrent trong cÃ¡c lá»›p cuá»‘i
cÃ¹ng Ä‘á»ƒ Ä‘áº£m báº£o tÆ°Æ¡ng Ä‘Æ°Æ¡ng tham sá»‘. ChÃºng tÃ´i
sá»­ dá»¥ng chiá»u tráº¡ng thÃ¡i lÃ  2,048 vÃ  chiá»u thá»i gian
lÃ  3.
Oracle Äá»‘i vá»›i má»—i Ä‘oáº¡n kiá»ƒm tra, chÃºng tÃ´i cÃ³
thá»ƒ tÃ¬m kiáº¿m toÃ n diá»‡n vÃ  sá»­ dá»¥ng táº¡i thá»i Ä‘iá»ƒm
kiá»ƒm tra cÃ¡c hÃ ng xÃ³m tá»‘t nháº¥t cÃ³ thá»ƒ cho má»™t mÃ´
hÃ¬nh theo LM cháº¥m Ä‘iá»ƒm. Äiá»u nÃ y cung cáº¥p má»™t
giá»›i háº¡n trÃªn cho hiá»‡u suáº¥t cá»§a RPT-Sem, vÃ¬ nÃ³
Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ báº¯t chÆ°á»›c thá»© háº¡ng Ä‘Æ°á»£c táº¡o
bá»Ÿi oracle nÃ y.
Chá»‰ sá»‘ ChÃºng tÃ´i sá»­ dá»¥ng perplexity Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh. NgoÃ i ra, chÃºng tÃ´i sá»­
dá»¥ng Ä‘iá»ƒm sá»‘ má»¥c tiÃªu st(Â·) tá»« LM cháº¥m Ä‘iá»ƒm Ä‘á»ƒ
tÃ­nh toÃ¡n cho má»—i Ä‘oáº¡n má»™t thá»© háº¡ng vÃ ng trÃªn táº¥t
cáº£ cÃ¡c Ä‘oáº¡n trÆ°á»›c Ä‘Ã³, vÃ  gáº¯n nhÃ£n cÃ¡c Ä‘oáº¡n lÃ 
dÆ°Æ¡ng/Ã¢m náº¿u Ä‘iá»ƒm sá»‘ má»¥c tiÃªu cá»§a chÃºng lÃ  dÆ°Æ¡ng/Ã¢m,
tÆ°Æ¡ng á»©ng. Vá»›i thÃ´ng tin nÃ y, chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Ã¡nh
giÃ¡ Precision@k, lÃ  tá»· lá»‡ cÃ¡c Ä‘oáº¡n top-k theo Ä‘iá»ƒm
sá»‘ dá»±a trÃªn truy váº¥n lÃ  dÆ°Æ¡ng, vÃ  Recall@k, lÃ  tá»·
lá»‡ cÃ¡c Ä‘oáº¡n dÆ°Æ¡ng náº±m trong cÃ¡c Ä‘oáº¡n top-k theo
Ä‘iá»ƒm sá»‘ dá»±a trÃªn truy váº¥n. ChÃºng tÃ´i cÅ©ng sá»­ dá»¥ng
thá»© háº¡ng vÃ ng Ä‘á»ƒ tÃ­nh toÃ¡n NDCG@k, lÃ  má»™t chá»‰
sá»‘ truy xuáº¥t tiÃªu chuáº©n (JÃ¤rvelin and KekÃ¤lÃ¤inen,
2002).
â¶https://github.com/google-research/
meliad. 5.2 Káº¿t quáº£
Báº£ng 2 cho tháº¥y káº¿t quáº£ chÃ­nh cá»§a chÃºng tÃ´i, cho
tháº¥y RPT-Sem cÃ³ hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng hoáº·c tá»‘t
hÆ¡n táº¥t cáº£ cÃ¡c baseline khÃ¡c trong táº¥t cáº£ cÃ¡c trÆ°á»ng
há»£p. Sá»­ dá»¥ng bá»™ truy xuáº¥t cá»‘ Ä‘á»‹nh (RETRO) cáº£i
thiá»‡n hiá»‡u suáº¥t so vá»›i Transformer-XL; RPT-Lex
dáº«n Ä‘áº¿n lá»£i Ã­ch trong Books3 nhÆ°ng tá»•n tháº¥t trong
PG19 so vá»›i RETRO, vÃ  RPT-Sem vÆ°á»£t trá»™i hÆ¡n
Transformer-XL, RETRO vÃ  RPT-Lex trÃªn ArXiv,
PG19 vÃ  Books3, vÃ  cÃ³ hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i
RETRO trÃªn CodeParrot. Ngay cáº£ trong thiáº¿t láº­p
rÃ ng buá»™c tham sá»‘ vÃ  rÃ ng buá»™c tÃ­nh toÃ¡n, Transformer-XL
váº«n cÃ³ hiá»‡u suáº¥t kÃ©m Ä‘Ã¡ng ká»ƒ so vá»›i RPT. So vá»›i
Block-Recurrent Transformer, Memorizing Transformers
vÃ  Griffin, khÃ´ng sá»­ dá»¥ng CCA, hiá»‡u suáº¥t láº¡i tÆ°Æ¡ng
tá»± hoáº·c tá»‘t hÆ¡n, vá»›i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trÃªn ArXiv
vÃ  Books3.
CCA cho phÃ©p tÄƒng Ä‘á»™ng sá»‘ lÆ°á»£ng hÃ ng xÃ³m táº¡i
thá»i Ä‘iá»ƒm suy luáº­n. Khi sá»­ dá»¥ng 3 hoáº·c 4 hÃ ng
xÃ³m (thay vÃ¬ 2), hiá»‡u suáº¥t cáº£i thiá»‡n, cho phÃ©p Ä‘Ã¡nh
Ä‘á»•i tÃ­nh toÃ¡n-hiá»‡u suáº¥t.
Cuá»‘i cÃ¹ng, cÃ¡c mÃ´ hÃ¬nh oracle liÃªn tá»¥c Ä‘áº¡t Ä‘Æ°á»£c
perplexity tá»‘t nháº¥t trÃªn táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u, cáº£i
thiá»‡n tá»« 2.74â†’2.69 trÃªn ArXiv, 2.15 â†’2.10 trÃªn
CodeParrot, 10.92 â†’10.26 trÃªn PG19, vÃ  13.87
â†’12.74 cho Books3. Äiá»u nÃ y cho tháº¥y ráº±ng cáº£i
thiá»‡n viá»‡c huáº¥n luyá»‡n bá»™ truy xuáº¥t cÃ³ thá»ƒ cáº£i thiá»‡n
hiá»‡u suáº¥t hÆ¡n ná»¯a.

--- TRANG 8 ---
Bá»™ dá»¯ liá»‡u Precision@2 Recall@10 nDCG@20
BM25 RPT-L RPT-S BM25 RPT-L RPT-S BM25 RPT-L RPT-S
ArXiv 27% 26% 32% 55% 54% 58% 24% 24% 30%
Code 29% 26% 34% 53% 52% 56% 25% 23% 30%
PG19 22% 22% 28% 55% 55% 61% 18% 18% 23%
Books3 23% 19% 26% 55% 50% 58% 18% 16% 22%
Avg 25.2% 23.2% 30.0% 54.5% 52.7% 58.2% 21.2% 20.2% 26.2%
Báº£ng 3: Chá»‰ sá»‘ truy xuáº¥t kiá»ƒm tra trÃªn cÃ¡c bá»™ dá»¯ liá»‡u.
Chá»‰ sá»‘ truy xuáº¥t Báº£ng 3 trÃ¬nh bÃ y cÃ¡c chá»‰ sá»‘ truy
xuáº¥t w.r.t cÃ¡c Ä‘oáº¡n dÆ°Æ¡ng oracle. Má»™t láº§n ná»¯a,
truy xuáº¥t vá»›i RPT-Sem vÆ°á»£t trá»™i hÆ¡n cáº£ RPT-Lex
vÃ  BM25 trong táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p. Äiá»u nÃ y
cho tháº¥y táº§m quan trá»ng cá»§a viá»‡c huáº¥n luyá»‡n bá»™
truy xuáº¥t, vÃ  hÆ¡n ná»¯a ráº±ng sá»­ dá»¥ng giÃ¡m sÃ¡t ngá»¯
nghÄ©a dáº«n Ä‘áº¿n truy xuáº¥t tá»‘t hÆ¡n so vá»›i chá»‰ tÃ­n hiá»‡u
tá»« vá»±ng.
5.3 Ablations
Báº£ng 4 cho tháº¥y káº¿t quáº£ cá»§a má»™t nghiÃªn cá»©u ablation
trÃªn táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u.
Chá»‰ Teacher Forcing ChÃºng tÃ´i buá»™c mÃ´ hÃ¬nh
attend Ä‘áº¿n cÃ¡c hÃ ng xÃ³m vÃ ng theo LM cháº¥m Ä‘iá»ƒm,
mÃ  khÃ´ng á»§ pss trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Äiá»u
nÃ y dáº«n Ä‘áº¿n giáº£m hiá»‡u suáº¥t trÃªn táº¥t cáº£ cÃ¡c bá»™ dá»¯
liá»‡u, vÃ  Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i PG19 vÃ  Books3.
KhÃ´ng Teacher Forcing á» Ä‘Ã¢y, chÃºng tÃ´i lÃ m
ngÆ°á»£c láº¡i vÃ  cá»‘ Ä‘á»‹nh pss= 0 trong suá»‘t quÃ¡ trÃ¬nh
huáº¥n luyá»‡n, tá»©c lÃ  chÃºng tÃ´i chá»‰ sá»­ dá»¥ng cÃ¡c hÃ ng
xÃ³m Ä‘Æ°á»£c dá»± Ä‘oÃ¡n vÃ  khÃ´ng pháº£i cÃ¡c hÃ ng xÃ³m
vÃ ng. Äiá»u nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n huáº¥n luyá»‡n thiáº¿u
cá»§a cÃ¡c lá»›p CCA vÃ¬ chÃºng Ä‘Æ°á»£c tiáº¿p xÃºc vá»›i cÃ¡c
hÃ ng xÃ³m cháº¥t lÆ°á»£ng tháº¥p á»Ÿ Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n
luyá»‡n vÃ  káº¿t quáº£ giáº£m cÃ²n nhiá»u hÆ¡n so vá»›i Chá»‰
Teacher Forcing.
KhÃ´ng cá»•ng hÃ ng xÃ³m ChÃºng tÃ´i vÃ´ hiá»‡u hÃ³a
cá»•ng hÃ ng xÃ³m kiá»ƒm soÃ¡t dÃ²ng thÃ´ng tin tá»« cÃ¡c
Ä‘oáº¡n hÃ ng xÃ³m vÃ  phÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng Ä‘áº¿n hiá»‡u
suáº¥t mÃ´ hÃ¬nh. ChÃºng tÃ´i quan sÃ¡t giáº£m hiá»‡u suáº¥t
trÃªn táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u, Ä‘Ã¡ng chÃº Ã½ trÃªn Books3,
trong Ä‘Ã³ perplexity tÄƒng 4.5 Ä‘iá»ƒm.
MÃ´ hÃ¬nh ArXiv Code PG19 Books3
RETRO W . BM25 (TRIá»‚N KHAI Cá»¦A CHÃšNG TÃ”I) 2.94 2.17 11.44 14.60
W. Bá»˜ TRUY XUáº¤T DPR-STYLE 2.97 2.28 11.7 14.86
RPT-L EX 2.92 2.23 11.59 14.32
W. Bá»˜ TRUY XUáº¤T DPR-STYLE 2.84 2.26 11.11 14.17
RPT-S EM 2.77 2.17 10.96 13.91
W. Bá»˜ TRUY XUáº¤T DPR-STYLE 2.98 2.33 11.62 14.66
RPT-S EM- CHá»ˆ TEACHER FORCING 2.91 2.22 11.54 14.66
RPT-S EM- KHÃ”NG TEACHER FORCING 2.95 2.26 13.10 14.40
RPT-S EM- KHÃ”NG Cá»”NG HÃ€NG XÃ“M 2.92 2.20 11.50 18.68
Báº£ng 4: Káº¿t quáº£ nghiÃªn cá»©u ablation cá»§a chÃºng tÃ´i.Bá»™ truy xuáº¥t DPR-style Äá»ƒ nghiÃªn cá»©u táº§m quan
trá»ng cá»§a huáº¥n luyá»‡n chung, chÃºng tÃ´i kiá»ƒm tra
hiá»‡u suáº¥t khi sá»­ dá»¥ng cÃ¡c bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n
luyá»‡n riÃªng biá»‡t tá»« LM, do Ä‘Ã³ táº¡o ra sá»± khÃ´ng khá»›p
giá»¯a huáº¥n luyá»‡n-kiá»ƒm tra. ChÃºng tÃ´i huáº¥n luyá»‡n
cÃ¡c bá»™ truy xuáº¥t dÃ y Ä‘áº·c sá»­ dá»¥ng quy trÃ¬nh huáº¥n
luyá»‡n DPR tiÃªu chuáº©n (Karpukhin et al., 2020)
trÃªn má»—i bá»™ dá»¯ liá»‡u (xem Phá»¥ lá»¥c C Ä‘á»ƒ biáº¿t chi tiáº¿t
huáº¥n luyá»‡n), vÃ  Ä‘á»‘i vá»›i má»—i mÃ´ hÃ¬nh CCA cá»§a chÃºng
tÃ´i sá»­ dá»¥ng bá»™ truy xuáº¥t nÃ y thay vÃ¬ bá»™ mÃ  nÃ³ Ä‘Æ°á»£c
huáº¥n luyá»‡n cÃ¹ng. ThÃº vá»‹, chÃºng tÃ´i quan sÃ¡t RPT-Lex
cÃ³ thá»ƒ hiá»‡u quáº£ sá»­ dá»¥ng cÃ¡c hÃ ng xÃ³m DPR-style
mang láº¡i cho nÃ³ má»™t cáº£i thiá»‡n hiá»‡u suáº¥t nhá» trÃªn
3 trong 4 bá»™ dá»¯ liá»‡u.
NhÆ° mong Ä‘á»£i, hai mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i
cÃ¡c bá»™ truy xuáº¥t máº¡nh hÆ¡n chá»‹u thiá»‡t háº¡i tá»« sá»±
khÃ´ng khá»›p huáº¥n luyá»‡n-kiá»ƒm tra, thay tháº¿ bá»™ truy
xuáº¥t BM25 vÃ  bá»™ truy xuáº¥t RPT-Sem báº±ng bá»™ truy
xuáº¥t DPR-style khiáº¿n cáº£ hai mÃ´ hÃ¬nh bá»‹ giáº£m hiá»‡u
suáº¥t trÃªn táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u, gá»£i Ã½ ráº±ng hiá»‡u
suáº¥t khÃ´ng-ablated lÃ  káº¿t quáº£ cá»§a sá»± phá»‘i há»£p giá»¯a
bá»™ truy xuáº¥t vÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯.
5.4 PhÃ¢n tÃ­ch
121416182022Chá»“ng láº¥p tokenArXiv Books3
121416182022Chá»“ng láº¥p tokenCodeParrot PG19
RPT-Sem RPT-Lex RETRO+BM25 Truy váº¥n Má»¥c tiÃªu
HÃ¬nh 4: ChÃºng tÃ´i Ä‘o sá»‘ lÆ°á»£ng token duy nháº¥t chá»“ng
láº¥p giá»¯a cÃ¡c Ä‘oáº¡n truy váº¥n/má»¥c tiÃªu vÃ  hÃ ng xÃ³m Ä‘Æ°á»£c
truy xuáº¥t tá»‘t nháº¥t.
Chá»“ng láº¥p token HÃ¬nh 4 váº½ sá»‘ lÆ°á»£ng token trung
bÃ¬nh chá»“ng láº¥p giá»¯a cÃ¡c Ä‘oáº¡n truy váº¥n/má»¥c tiÃªu
trong hÃ ng xÃ³m Ä‘Æ°á»£c truy xuáº¥t tá»‘t nháº¥t cho RETRO,
RPT-Lex vÃ  RPT-Sem. RPT-Sem truy xuáº¥t cÃ¡c Ä‘oáº¡n
vÄƒn vá»›i Ä‘á»™ chá»“ng láº¥p cao hÆ¡n vá»›i Ä‘oáº¡n má»¥c tiÃªu
so vá»›i RPT-Lex. ÄÆ°Æ¡ng nhiÃªn, BM25 truy xuáº¥t
cÃ¡c Ä‘oáº¡n vá»›i Ä‘á»™ chá»“ng láº¥p cao nháº¥t vá»›i Ä‘oáº¡n truy
váº¥n

--- TRANG 9 ---
Ä‘oáº¡n. Tuy nhiÃªn, Ä‘iá»u nÃ y khÃ´ng dá»‹ch ra Ä‘á»™ chá»“ng
láº¥p tá»« vá»±ng cao hÆ¡n cho Ä‘oáº¡n má»¥c tiÃªu.
1234567891011121314151617181920
Pháº§n tá»­ Top-K theo BM250.000.050.100.150.20Äiá»ƒm sá»‘ má»¥c tiÃªu trung bÃ¬nh tá»‘i Ä‘a trÃªn cÃ¡c Ä‘oáº¡n
Bá»™ dá»¯ liá»‡u
Books3
ArXiv
CodeParrot
PG19
HÃ¬nh 5: Äiá»ƒm sá»‘ má»¥c tiÃªu tá»‘i Ä‘a st(Â·) cho cÃ¡c Ä‘oáº¡n
top-K Ä‘Æ°á»£c truy xuáº¥t bá»Ÿi BM25 Ä‘Æ°á»£c tÃ­nh trung bÃ¬nh
trÃªn cÃ¡c Ä‘oáº¡n vÃ  cho táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u. VÃ¬ Ä‘iá»ƒm
sá»‘ má»¥c tiÃªu tá»‘i Ä‘a cho 20 Ä‘oáº¡n hÃ ng Ä‘áº§u cao hÆ¡n
nhiá»u so vá»›i top-2, viá»‡c há»c sáº¯p xáº¿p láº¡i 20 á»©ng viÃªn
BM25 hÃ ng Ä‘áº§u cÃ³ thá»ƒ dáº«n Ä‘áº¿n cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
trong cháº¥t lÆ°á»£ng truy xuáº¥t.
Cháº¥t lÆ°á»£ng giÃ¡m sÃ¡t ChÃºng tÃ´i huáº¥n luyá»‡n RPT-Sem
sá»­ dá»¥ng thÃ´ng tin tá»« hÃ m cháº¥m Ä‘iá»ƒm má»¥c tiÃªu st(Â·),
mÃ  chÃºng tÃ´i tháº¥y dáº«n Ä‘áº¿n cáº£i thiá»‡n mÃ´ hÃ¬nh. Tuy
nhiÃªn, hÃ m cháº¥m Ä‘iá»ƒm má»¥c tiÃªu chá»‰ cung cáº¥p sáº¯p
xáº¿p láº¡i cá»§a 20 á»©ng viÃªn hÃ ng Ä‘áº§u theo BM25. VÃ¬
váº­y, má»™t cÃ¢u há»i tá»± nhiÃªn lÃ  cháº¥t lÆ°á»£ng giÃ¡m sÃ¡t
cáº£i thiá»‡n bao nhiá»u thÃ´ng qua viá»‡c sáº¯p xáº¿p láº¡i nÃ y.
HÃ¬nh 5 cho tháº¥y Ä‘á»‘i vá»›i má»—i háº¡ng K Ä‘iá»ƒm sá»‘ má»¥c
tiÃªu tá»‘i Ä‘a trong sá»‘ cÃ¡c Ä‘oáº¡n top-K theo BM25,
Ä‘Æ°á»£c tÃ­nh trung bÃ¬nh trÃªn cÃ¡c Ä‘oáº¡n vÃ  trÃªn 4 bá»™
dá»¯ liá»‡u cá»§a chÃºng tÃ´i. RÃµ rÃ ng, viá»‡c sáº¯p xáº¿p láº¡i
20 á»©ng viÃªn BM25 hÃ ng Ä‘áº§u cÃ³ ráº¥t nhiá»u tiá»m nÄƒng,
vÃ¬ Ä‘iá»ƒm sá»‘ má»¥c tiÃªu tá»‘i Ä‘a cao hÆ¡n nhiá»u Ä‘á»‘i vá»›i
20 á»©ng viÃªn hÃ ng Ä‘áº§u so vá»›i top-2. Äiá»u nÃ y gá»£i
Ã½ ráº±ng viá»‡c huáº¥n luyá»‡n dÃ i hÆ¡n vÃ  tá»‘t hÆ¡n cá»§a bá»™
truy xuáº¥t cÃ³ thá»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a RPT-Sem
hÆ¡n ná»¯a.
ThÃº vá»‹, phÃ¢n tÃ­ch cá»§a chÃºng tÃ´i lÃ m sÃ¡ng tá» táº¡i
sao RPT-Sem vÆ°á»£t trá»™i hÆ¡n RETRO rÃµ rÃ ng trÃªn
Books3 vÃ  PG19 nhÆ°ng Ã­t hÆ¡n trÃªn CodeParrot.
Äiá»ƒm sá»‘ má»¥c tiÃªu tá»‘i Ä‘a cho CodeParrot khi k= 2
Ä‘Ã£ khÃ¡ cao â€“ khoáº£ng 0.1, tÆ°Æ¡ng á»©ng vá»›i hÆ¡n 10%
cáº£i thiá»‡n xÃ¡c suáº¥t cá»§a Ä‘oáº¡n má»¥c tiÃªu so vá»›i ngá»¯
cáº£nh Ä‘á»‹a phÆ°Æ¡ng. NgÆ°á»£c láº¡i, Ä‘á»‘i vá»›i PG19 vÃ  Books3,
Ä‘iá»ƒm sá»‘ má»¥c tiÃªu khi k= 2 gáº§n 0 hÆ¡n.
PhÃ¢n tÃ­ch nhÃ³m con HÃ¬nh 6 cho tháº¥y cáº£i thiá»‡n
tÆ°Æ¡ng Ä‘á»‘i trung bÃ¬nh (trÃªn cÃ¡c Ä‘oáº¡n) cá»§a RETRO,
RPT-Lex vÃ  RPT-Sem so vá»›i Transformer-XL,
0102030% Cáº£i thiá»‡nArXiv Books3
0102030% Cáº£i thiá»‡nCodeParrot PG19
RPT-Sem RPT-Lex RETRO+BM25 Sai ÄÃºng Táº¥t cáº£HÃ¬nh 6: Cáº£i thiá»‡n tÆ°Æ¡ng Ä‘á»‘i cÃ³/khÃ´ng cÃ³ truy xuáº¥t Ä‘Ãºng.
khi phÃ¢n biá»‡t giá»¯a cÃ¡c trÆ°á»ng há»£p mÃ  má»™t Ä‘oáº¡n
oracle "vÃ ng" Ä‘Æ°á»£c truy xuáº¥t vÃ  cÃ¡c trÆ°á»ng há»£p
mÃ  khÃ´ng cÃ³ Ä‘oáº¡n vÃ ng nÃ o Ä‘Æ°á»£c truy xuáº¥t.
NhÆ° mong Ä‘á»£i, RPT-Sem dáº«n Ä‘áº¿n cáº£i thiá»‡n trÃªn
táº¥t cáº£ cÃ¡c bá»™ dá»¯ liá»‡u, vÃ  vÆ°á»£t trá»™i hÆ¡n cÃ¡c baseline
khÃ¡c ngoáº¡i trá»« RETRO trÃªn CodeParrot nÆ¡i hiá»‡u
suáº¥t tÆ°Æ¡ng tá»±. Thá»© hai, cÃ¡c trÆ°á»ng há»£p mÃ  má»™t
Ä‘oáº¡n vÃ ng Ä‘Æ°á»£c truy xuáº¥t thá»±c sá»± thÆ°á»ng dáº«n Ä‘áº¿n
cáº£i thiá»‡n lá»›n hÆ¡n, nhÆ°ng chÃºng tÃ´i chá»©ng kiáº¿n
cáº£i thiá»‡n ngay cáº£ trong cÃ¡c trÆ°á»ng há»£p mÃ  má»™t
Ä‘oáº¡n vÃ ng khÃ´ng Ä‘Æ°á»£c truy xuáº¥t, Ä‘iá»u nÃ y cho tháº¥y
mÃ´ hÃ¬nh váº«n cÃ³ thá»ƒ hÆ°á»Ÿng lá»£i tá»« nhá»¯ng truy xuáº¥t
nhÆ° váº­y.
PhÃ¢n tÃ­ch Ä‘á»‹nh tÃ­nh Kiá»ƒm tra cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c truy
xuáº¥t, chÃºng tÃ´i quan sÃ¡t ráº±ng bá»™ truy xuáº¥t RPT cÃ³
tÃ­nh ngá»¯ cáº£nh cao. Khi Ã¡p dá»¥ng trÃªn mÃ£, nÃ³ truy
xuáº¥t cÃ¡c Ä‘á»‹nh nghÄ©a hÃ m, gÃ¡n biáº¿n, v.v., trÃªn ArXiv
nÃ³ truy xuáº¥t cÃ¡c Ä‘á»‹nh nghÄ©a lemma, Ä‘á»‹nh lÃ½, v.v.
HÃ¬nh 7 cho tháº¥y má»™t vÃ­ dá»¥, trong Ä‘Ã³ chÃºng tÃ´i cung
cáº¥p codebase Ä‘Æ°á»£c sá»­ dá»¥ng cho bÃ i bÃ¡o nÃ y lÃ m
Ä‘áº§u vÃ o cho mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i vÃ  trÃ¬nh bÃ y
má»™t vÃ­ dá»¥ Ä‘oáº¡n truy váº¥n trong Ä‘Ã³ RPT táº¡o ra truy
xuáº¥t tá»‘t hÆ¡n BM25. ChÃºng tÃ´i quan sÃ¡t ráº±ng ngá»¯
cáº£nh trÆ°á»›c Ä‘Ã³ cho phÃ©p RPT hiá»‡u quáº£ truy xuáº¥t
má»™t Ä‘á»‹nh nghÄ©a Ä‘á»‘i tÆ°á»£ng liÃªn quan, dáº«n Ä‘áº¿n loss
tháº¥p hÆ¡n.
6 Tháº£o luáº­n vÃ  NghiÃªn cá»©u LiÃªn quan
Má»‘i quan há»‡ vá»›i Fusion-in-Decoder RPT cÃ³ Ä‘iá»ƒm
tÆ°Æ¡ng Ä‘á»“ng vá»›i Fusion-in-Decoder (FiD) (Izacard
and Grave, 2021b; Ivgi et al., 2023). Trong khi
cáº£ RPT vÃ  FiD Ä‘á»u sá»­ dá»¥ng cÆ¡ cháº¿ cross-attention
Ä‘á»ƒ tÃ­ch há»£p ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t trong cÃ¡c mÃ´
hÃ¬nh cá»§a há», chÃºng khÃ¡c nhau á»Ÿ hai cÃ¡ch. (a) Trong
FiD, truy xuáº¥t Ä‘Æ°á»£c thá»±c hiá»‡n

--- TRANG 10 ---
@flax.struct.dataclass
class FlaxRPTRetrieverEncodedOutput(ModelOutput):
original_hidden_states: jnp.ndarray = None
encoded_hidden_states: jnp.ndarray = None
attention_mask: jnp.ndarray = None
key_chunks: jnp.ndarray = None
query_chunks: jnp.ndarray = None
chunk_mask: jnp.ndarray = None
...
class FlaxRPTModule(nn.Module):
...
def __call__(...
...
hidden_states = self.ln_f (hidden_states)
if not return_dict:
return (hidden_states,) + upcoder_outputs + lowcoder_outputs
return FlaxRPTModelOutput(
last_hidden_state=upcoder_outputs.last_hidden_state,
upcoder_hidden_states=upcoder_outputs.hidden_states,
upcoder_attentions=upcoder_outputs.attentions,
lowcoder_last_hidden_state=lowc oder_outputs.last_hidden_state,
...)
...
def forward_loglikelihood(params, rng, batch, memory):
...
outputs, lowcoder_state = _forward_loglikelihood_lowcoder (params, rng, batch)
if 'cache' in lowcoder_state:
params['cache'] = lowcoder_state['cache']
outputs = jax.tree_map(lambda x: jax.device_get(x).astype(np.float32), outputs)
neighbor_hidden_states, neighbor_mask, *_ = memory.add(
input_tokens=batch["input_tokens"],
encoded_hidden_states=outputs.encoded_hidden_states,
key_chunks=outputs. key_chunks,
query_chunks=outputs.query_chunks,
)
...
HÃ¬nh 7: Má»™t vÃ­ dá»¥ minh há»a trÃ¬nh bÃ y cÃ¡c hÃ ng xÃ³m top-1 Ä‘Æ°á»£c truy xuáº¥t cho cáº£ hai mÃ´ hÃ¬nh RPT-Sem vÃ  BM25
Ä‘Æ°á»£c Ã¡p dá»¥ng cho mÃ£ cá»§a RPT. Biáº¿n outputs trong Ä‘oáº¡n truy váº¥n lÃ  má»™t thÃ nh viÃªn cá»§a lá»›p
FlaxRPTRetrieverEncodedOutput. RPT-Sem thÃ nh cÃ´ng truy xuáº¥t Ä‘á»‹nh nghÄ©a cá»§a Ä‘á»‘i tÆ°á»£ng dáº«n Ä‘áº¿n
giáº£m loss trÃªn Ä‘oáº¡n má»¥c tiÃªu, so vá»›i BM25.
chá»‰ má»™t láº§n dá»±a trÃªn prompt/truy váº¥n ban Ä‘áº§u, trong
khi RPT liÃªn tá»¥c thá»±c hiá»‡n truy xuáº¥t á»Ÿ cáº¥p Ä‘á»™ Ä‘oáº¡n
trong suá»‘t quÃ¡ trÃ¬nh sinh. (b) FiD mÃ£ hÃ³a cÃ¡c hÃ ng
xÃ³m Ä‘Æ°á»£c truy xuáº¥t riÃªng biá»‡t sá»­ dá»¥ng má»™t bá»™ mÃ£
hÃ³a hai chiá»u vÃ  chá»‰ sau Ä‘Ã³ Ã¡p dá»¥ng cross-attention
trong bá»™ giáº£i mÃ£. Trong RPT, bá»™ giáº£i mÃ£ tÃ­nh toÃ¡n
cÃ¡c embedding Ä‘oáº¡n vÃ  thá»±c hiá»‡n truy xuáº¥t báº£n
Ä‘á»‹a, vÃ  sau Ä‘Ã³ chunked cross-attention Ä‘Æ°á»£c Ã¡p
dá»¥ng Ä‘á»ƒ há»£p nháº¥t ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t vá»›i cÃ¡c
dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh. ChÃºng tÃ´i xem RPT, sá»­ dá»¥ng
cÃ¡c mÃ£ hÃ³a bá»™ giáº£i mÃ£ dÆ°á»›i, lÃ  tá»± nhiÃªn hÆ¡n trong
ngá»¯ cáº£nh sinh liÃªn tá»¥c (vÃ­ dá»¥, chatbots hoáº·c agents),
vÃ¬ mÃ´ hÃ¬nh sinh ra cÃ¡c biá»ƒu diá»…n vÃ  sá»­ dá»¥ng chÃºng
sau nÃ y lÃ m khÃ³a, vÃ  do Ä‘Ã³ viá»‡c sinh ra cÃ¡c biá»ƒu
diá»…n truy xuáº¥t khÃ´ng tá»‘n chi phÃ­ gÃ¬.MÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa Má»™t trá»ng tÃ¢m chÃ­nh
trong mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa Ä‘Ã£ lÃ  giáº£i quyáº¿t
Ä‘á»™ phá»©c táº¡p báº­c hai cá»§a attention Ä‘á»ƒ phÃ¡t triá»ƒn
cÃ¡c cÆ¡ cháº¿ hiá»‡u quáº£ hÆ¡n Ä‘á»ƒ xá»­ lÃ½ vÄƒn báº£n dÃ i. VÃ­
dá»¥, Transformer-XL (Dai et al., 2019) xá»­ lÃ½ Ä‘áº§u
vÃ o sá»­ dá»¥ng cÆ¡ cháº¿ cáº¥p Ä‘á»™ Ä‘oáº¡n trong khi duy trÃ¬
cache tá»« cÃ¡c Ä‘oáº¡n trÆ°á»›c Ä‘Ã³. Longformer (Beltagy
et al., 2020) má»Ÿ rá»™ng Ã½ tÆ°á»Ÿng nÃ y Ä‘á»ƒ chá»©a cÃ¡c
ngá»¯ cáº£nh cÃ²n dÃ i hÆ¡n. Má»™t sá»‘ nghiÃªn cá»©u trÆ°á»›c
Ä‘Ã¢y xem truy xuáº¥t nhÆ° má»™t váº¥n Ä‘á» táº§m xa. Memorizing
Transformers (Wu et al., 2022) sá»­ dá»¥ng má»™t lá»›p
k-NN duy nháº¥t vÃ  truy xuáº¥t cÃ¡c khÃ³a vÃ  giÃ¡ trá»‹ Ä‘Æ°á»£c
cache, nhÆ°ng há» khÃ´ng lan truyá»n ngÆ°á»£c gradient
thÃ´ng qua phÃ©p toÃ¡n truy xuáº¥t thÆ°a thá»›t. TÆ°Æ¡ng
tá»±, Bertsch et al. (2023) chá»©ng minh ráº±ng cÃ¡ch
tiáº¿p cáº­n nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i báº¥t ká»³ mÃ´
hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c hiá»‡n cÃ³ nÃ o vÃ  Ã¡p dá»¥ng
nÃ³ táº¡i má»—i lá»›p attention cho cÃ¡c nhiá»‡m vá»¥ tÃ³m táº¯t
dÃ i. Tá»« gÃ³c Ä‘á»™ phÃ¢n tÃ­ch, cÃ¡c nghiÃªn cá»©u trÆ°á»›c
(Press et al., 2021) chá»©ng minh ráº±ng cÃ¡c benchmark
LM tiÃªu chuáº©n khÃ´ng lÃ½ tÆ°á»Ÿng Ä‘á»ƒ Ä‘o kháº£ nÄƒng
táº§m xa cá»§a cÃ¡c mÃ´ hÃ¬nh. Sun et al. (2021) tháº£o
luáº­n vá» cÃ¡c loáº¡i chuá»—i khÃ¡c nhau hÆ°á»Ÿng lá»£i tá»« viá»‡c
cÃ³ ngá»¯ cáº£nh dÃ i, vÃ  Rae and Razavi (2020) Ä‘iá»u
tra cÃ¡c lá»±a chá»n kiáº¿n trÃºc táº§m xa vÃ  khuyáº¿n nghá»‹
tÄƒng kháº£ nÄƒng táº§m xa trong cÃ¡c lá»›p trÃªn.
MÃ´ hÃ¬nh ngÃ´n ngá»¯ hiá»‡u quáº£ CÃ¡c chiáº¿n lÆ°á»£c thÆ°a
thá»›t, cháº³ng háº¡n nhÆ° nhá»¯ng chiáº¿n lÆ°á»£c Ä‘Æ°á»£c Ä‘á»
xuáº¥t trong Zaheer et al. (2020); Roy et al. (2021);
Kitaev et al. (2020), tÆ°Æ¡ng tá»± nhÆ° RPT, attend
chá»‰ Ä‘áº¿n má»™t táº­p con cÃ¡c token thÃ´ng qua cÃ¡c phÆ°Æ¡ng
phÃ¡p clustering hoáº·c hashing, Ä‘Æ°á»£c huáº¥n luyá»‡n
báº±ng cÃ¡ch lan truyá»n gradient thÃ´ng qua phÃ©p toÃ¡n
thÆ°a thá»›t. Trong RPT, tÃ­nh thÆ°a thá»›t lÃ  do phÃ©p
toÃ¡n top-K cá»§a bá»™ truy xuáº¥t, Ä‘Æ°á»£c huáº¥n luyá»‡n sá»­
dá»¥ng giÃ¡m sÃ¡t cháº¥t lÆ°á»£ng cao tá»« má»™t mÃ´ hÃ¬nh ngÃ´n
ngá»¯ tham chiáº¿u. Má»™t cÃ¡ch tiáº¿p cáº­n khÃ¡c Ä‘á»ƒ mÃ´ hÃ¬nh
hÃ³a hiá»‡u quáº£ vÄƒn báº£n dÃ i liÃªn quan Ä‘áº¿n viá»‡c nÃ©n
Ä‘áº§u vÃ o vÃ  attend trÃªn chuá»—i Ä‘Æ°á»£c nÃ©n (Martins
et al., 2022; Rae et al., 2020), hoáº·c há»c bá» qua
cÃ¡c token khÃ´ng liÃªn quan (Sukhbaatar et al., 2021).
Tuy nhiÃªn, thá»±c nghiá»‡m háº§u háº¿t cÃ¡c kiáº¿n trÃºc transformer
hiá»‡u quáº£ Ä‘Ã¡nh Ä‘á»•i hiá»‡u quáº£ vá»›i cháº¥t lÆ°á»£ng. Gáº§n
Ä‘Ã¢y, cÃ¡c mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i (Mehta et al.,
2023; Gu and Dao, 2023; Fu et al., 2023) xuáº¥t
hiá»‡n nhÆ° má»™t thay tháº¿ hiá»‡u quáº£, tiáº¿n gáº§n Ä‘áº¿n cháº¥t
lÆ°á»£ng Transformer. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i
khÃ¡m phÃ¡ cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Transformer báº­c
hai cá»• Ä‘iá»ƒn. ChÃºng tÃ´i láº­p luáº­n ráº±ng mÃ´ hÃ¬nh cÆ¡
báº£n lÃ  trá»±c giao vá»›i Ä‘Ã³ng gÃ³p cá»§a chÃºng tÃ´i vÃ  cÃ³
thá»ƒ Ä‘Æ°á»£c thay tháº¿ báº±ng cÃ¡c thay tháº¿ hiá»‡u quáº£ khÃ¡c
vÃ  káº¿t há»£p vá»›i truy xuáº¥t. ChÃºng tÃ´i Ä‘á»ƒ dÃ nh khÃ¡m
phÃ¡ nÃ y cho nghiÃªn cá»©u tÆ°Æ¡ng lai.
LMs Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy xuáº¥t LMs Ä‘Æ°á»£c tÄƒng
cÆ°á»ng truy xuáº¥t Ä‘Ã£ ná»•i lÃªn nhÆ° má»™t cÃ¡ch tiáº¿p cáº­n
ná»•i báº­t Ä‘á»ƒ táº­n dá»¥ng hiá»‡u quáº£ kiáº¿n thá»©c bÃªn ngoÃ i
trong khi sinh vÄƒn báº£n. Nhá»¯ng mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ
Ä‘Æ°á»£c chia rá»™ng thÃ nh nhá»¯ng mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng
á»Ÿ Ä‘á»™ chi tiáº¿t cáº¥p token vÃ  nhá»¯ng mÃ´ hÃ¬nh hoáº¡t
Ä‘á»™ng á»Ÿ Ä‘á»™ chi tiáº¿t cáº¥p chuá»—i. CÃ¡c phÆ°Æ¡ng phÃ¡p
cáº¥p token, cháº³ng háº¡n nhÆ° kNN-LM (Khandelwal
et al., 2020), TRIME (Zhong et al., 2022), vÃ 
SPALM (Yogatama et al., 2021), truy xuáº¥t thÃ´ng
tin cho cÃ¡c token riÃªng láº». CÃ¡c cÃ¡ch tiáº¿p cáº­n cáº¥p
chuá»—i nhÆ° RAG (Lewis et al., 2020) sá»­ dá»¥ng cÃ¡c
mÃ´ hÃ¬nh mÃ£ hÃ³a-giáº£i mÃ£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c
vá»›i cÃ¡c bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c cho
cÃ¡c nhiá»‡m vá»¥ nhÆ° tráº£ lá»i cÃ¢u há»i miá»n má»Ÿ. TÆ°Æ¡ng
tá»±, FiD (Izacard and Grave, 2021b) sá»­ dá»¥ng cÃ¡c
mÃ´ hÃ¬nh mÃ£ hÃ³a-giáº£i mÃ£ sinh cÃ³ thá»ƒ há»£p nháº¥t báº±ng
chá»©ng tá»« nhiá»u Ä‘oáº¡n vÄƒn trong quÃ¡ trÃ¬nh giáº£i mÃ£,
liÃªn quan cháº·t cháº½ Ä‘áº¿n cÆ¡ cháº¿ CCA. Gáº§n Ä‘Ã¢y, Wang
et al. (2023) chá»©ng minh lá»£i Ã­ch tiá»m nÄƒng cá»§a viá»‡c
thá»±c hiá»‡n truy xuáº¥t vÃ  chunked cross-attention táº¡i
má»—i bÆ°á»›c thá»i gian, so vá»›i bÃ i bÃ¡o RETRO ban
Ä‘áº§u (Borgeaud et al., 2022), truy xuáº¥t má»—i m= 64
bÆ°á»›c.
Huáº¥n luyá»‡n chung bá»™ truy xuáº¥t-Ä‘á»c CÃ¡c cÃ¡ch tiáº¿p
cáº­n huáº¥n luyá»‡n chung thÆ°á»ng táº­p trung vÃ o viá»‡c
chuyá»ƒn giao thÃ´ng tin giá»¯a má»™t bá»™ Ä‘á»c Ä‘Æ°á»£c huáº¥n
luyá»‡n trÆ°á»›c thÃ nh má»™t bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n
trÆ°á»›c. Nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng liÃªn quan
Ä‘áº¿n viá»‡c cáº­p nháº­t chá»‰ má»¥c bá»™ truy xuáº¥t trong quÃ¡
trÃ¬nh huáº¥n luyá»‡n trong ngá»¯ cáº£nh cá»§a cÃ¡c nhiá»‡m vá»¥
chuyÃªn sÃ¢u kiáº¿n thá»©c, cháº³ng háº¡n nhÆ° tráº£ lá»i cÃ¢u
há»i miá»n má»Ÿ. VÃ­ dá»¥, REALM (Guu et al., 2020)
sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÃ³ máº·t náº¡ lÃ m tÃ­n hiá»‡u
há»c Ä‘á»ƒ cáº­p nháº­t bá»™ truy xuáº¥t. EMDR2 (Sachan
et al., 2021) má»Ÿ rá»™ng FiD báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c
mÃ´ hÃ¬nh mÃ£ hÃ³a-giáº£i mÃ£ Ä‘á»ƒ lan truyá»n ngÆ°á»£c lá»—i
tá»« cÃ¢u tráº£ lá»i Ä‘Æ°á»£c dá»± Ä‘oÃ¡n Ä‘áº¿n bá»™ truy xuáº¥t. TÆ°Æ¡ng
tá»±, Izacard and Grave (2021a); Jiang et al. (2022)
sá»­ dá»¥ng Ä‘iá»ƒm sá»‘ attention tá»« bá»™ Ä‘á»c Ä‘á»ƒ giÃ¡m sÃ¡t
bá»™ truy xuáº¥t trá»±c tiáº¿p sá»­ dá»¥ng ma tráº­n attention
lÃ m tÃ­n hiá»‡u huáº¥n luyá»‡n Ä‘á»ƒ cho phÃ©p huáº¥n luyá»‡n
end-to-end chung vá»›i giÃ¡m sÃ¡t cá»§a nhiá»‡m vá»¥ downstream.
ÄÃ¡ng chÃº Ã½, Izacard et al. (2022b) tiáº¿p tá»¥c má»Ÿ
rá»™ng nhá»¯ng cÃ¡ch tiáº¿p cáº­n nÃ y vÃ  huáº¥n luyá»‡n chung
má»™t bá»™ truy xuáº¥t vá»›i má»™t mÃ´ hÃ¬nh mÃ£ hÃ³a-giáº£i mÃ£,
chá»©ng minh kháº£ nÄƒng há»c few-shot máº¡nh máº½. Há»
cÅ©ng Ä‘iá»u tra cÃ¡c ká»¹ thuáº­t cáº­p nháº­t bá»™ truy xuáº¥t
khÃ¡c nhau Ä‘á»ƒ giáº£i quyáº¿t sá»± khÃ´ng khá»›p huáº¥n luyá»‡n-kiá»ƒm
tra trong quÃ¡ trÃ¬nh truy xuáº¥t. ChÃºng tÃ´i khÃ´ng gáº·p
pháº£i váº¥n Ä‘á» cáº­p nháº­t chá»‰ má»¥c vÃ¬ chÃºng tÃ´i tÃ­nh toÃ¡n
toÃ n bá»™ chá»‰ má»¥c thÃ´ng qua má»™t láº§n chuyá»ƒn tiáº¿p.
Huáº¥n luyá»‡n trÆ°á»›c bá»™ truy xuáº¥t NghiÃªn cá»©u sá»›m
vá» huáº¥n luyá»‡n trÆ°á»›c bá»™ truy xuáº¥t dá»±a vÃ o Inverse
Cloze Task khÃ´ng giÃ¡m sÃ¡t Ä‘á»ƒ huáº¥n luyá»‡n trÆ°á»›c
bá»™ truy xuáº¥t (Lee et al., 2019; Guu et al., 2020).
Sau Ä‘Ã³ Ä‘Æ°á»£c chá»©ng minh ráº±ng viá»‡c sá»­ dá»¥ng trá»±c
tiáº¿p BERT (Devlin et al., 2019) vá»›i má»™t má»¥c tiÃªu
cÃ³ giÃ¡m sÃ¡t lÃ  Ä‘á»§ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t trÃªn
cÃ¡c benchmark tiÃªu chuáº©n (Karpukhin et al., 2020).
Tuy nhiÃªn, mÃ´ hÃ¬nh nÃ y cho tháº¥y hiá»‡u suáº¥t kÃ©m
trÃªn cÃ¡c thá»±c thá»ƒ long-tail so vá»›i BM25 (Amouyal
et al., 2023; Sciavolino et al., 2021). Gáº§n Ä‘Ã¢y,
cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n trÆ°á»›c khÃ´ng giÃ¡m sÃ¡t
(Gao and Callan, 2022; Ram et al., 2022; Izacard
et al., 2022a) cho phÃ©p cáº£i thiá»‡n hiá»‡u suáº¥t. Tuy
nhiÃªn, nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c khá»Ÿi táº¡o tá»«
má»™t mÃ´ hÃ¬nh encoder BERT Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c
(Devlin et al., 2019), trong khi RPT lÃ  má»™t kiáº¿n
trÃºc retriever-reader Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« Ä‘áº§u vÆ°á»£t
trá»™i hÆ¡n BM25 mÃ  khÃ´ng cáº§n báº¥t ká»³ huáº¥n luyá»‡n
trÆ°á»›c bá»• sung nÃ o.
GiÃ¡m sÃ¡t bá»™ truy xuáº¥t vá»›i LLMs EPR (Rubin et al.,
2022) chá»©ng minh ráº±ng LLMs cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng
Ä‘á»ƒ huáº¥n luyá»‡n bá»™ truy xuáº¥t cho truy xuáº¥t prompt
báº±ng cÃ¡ch Æ°á»›c tÃ­nh xÃ¡c suáº¥t cá»§a má»™t Ä‘áº§u ra cho
Ä‘áº§u vÃ o vÃ  má»™t vÃ­ dá»¥ huáº¥n luyá»‡n á»©ng viÃªn lÃ m
prompt. CÃ¡c ká»¹ thuáº­t tÆ°Æ¡ng tá»± Ä‘Æ°á»£c Ã¡p dá»¥ng cho
tráº£ lá»i cÃ¢u há»i miá»n má»Ÿ thÃ´ng qua sáº¯p xáº¿p láº¡i káº¿t
quáº£ truy xuáº¥t (Sachan et al., 2022; Ram et al.,
2023) vÃ  Ä‘á»ƒ giÃ¡m sÃ¡t bá»™ truy xuáº¥t thÃ´ng qua chÆ°ng
cáº¥t perplexity (Izacard et al., 2022b). Gáº§n Ä‘Ã¢y,
Shi et al. (2024) sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p giÃ¡m sÃ¡t
nÃ y Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a cÃ¡c LLM khÃ¡c nhau
theo cÃ¡ch há»™p Ä‘en.
7 Káº¿t luáº­n
Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y Retrieval-Pretrained
Transformer (RPT), má»™t LM Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy
xuáº¥t, trong Ä‘Ã³ bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n nhÆ°
má»™t thÃ nh pháº§n báº£n Ä‘á»‹a cá»§a LM Ä‘á»ƒ truy xuáº¥t cÃ¡c
Ä‘oáº¡n liÃªn quan ngá»¯ nghÄ©a cho dá»± Ä‘oÃ¡n vÄƒn báº£n
tÆ°Æ¡ng lai. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT trÃªn bá»‘n nhiá»‡m
vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa, bao gá»“m sÃ¡ch, mÃ£
vÃ  viáº¿t toÃ¡n há»c. ChÃºng tÃ´i chá»©ng minh ráº±ng báº±ng
cÃ¡ch tÃ­ch há»£p liá»n máº¡ch bá»™ truy xuáº¥t vÃ o kiáº¿n trÃºc
vÃ  quy trÃ¬nh huáº¥n luyá»‡n, RPT hÆ°á»Ÿng lá»£i tá»« viá»‡c
há»£p nháº¥t ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t, cáº£i thiá»‡n so vá»›i
cÃ¡c baseline Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy xuáº¥t máº¡nh. Trong
khi nghiÃªn cá»©u nÃ y táº­p trung vÃ o truy xuáº¥t tá»« vÄƒn
báº£n dÃ i, chÃºng tÃ´i láº­p luáº­n cÃ¡c phÃ¡t hiá»‡n thá»±c nghiá»‡m
cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng thÃ­ch á»©ng quy trÃ¬nh
cá»§a chÃºng tÃ´i cho truy xuáº¥t corpus dá»±a trÃªn web
tá»•ng quÃ¡t lÃ  má»™t hÆ°á»›ng tÆ°Æ¡ng lai thÃº vá»‹. Äiá»u nÃ y
sáº½ Ä‘Ã²i há»i vÆ°á»£t qua cÃ¡c khÃ³ khÄƒn ká»¹ thuáº­t liÃªn quan
Ä‘áº¿n viá»‡c má»Ÿ rá»™ng vÃ  xÃ¢y dá»±ng corpus huáº¥n luyá»‡n
trÆ°á»›c. ChÃºng tÃ´i hÃ¬nh dung RPT sáº½ má»Ÿ Ä‘Æ°á»ng cho
má»™t tháº¿ há»‡ má»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c huáº¥n
luyá»‡n trÆ°á»›c vá»›i truy xuáº¥t Ä‘Æ°á»£c tÃ­ch há»£p sÃ¢u trong
suá»‘t kiáº¿n trÃºc vÃ  quy trÃ¬nh huáº¥n luyá»‡n cá»§a chÃºng.
Lá»i cáº£m Æ¡n
NghiÃªn cá»©u nÃ y Ä‘Æ°á»£c há»— trá»£ bá»Ÿi Cloud TPUs tá»«
Google's TPU Research Cloud (TRC) vÃ  Há»™i Ä‘á»“ng
NghiÃªn cá»©u ChÃ¢u Ã‚u (ERC) dÆ°á»›i chÆ°Æ¡ng trÃ¬nh
nghiÃªn cá»©u vÃ  Ä‘á»•i má»›i Horizons 2020 cá»§a LiÃªn
minh ChÃ¢u Ã‚u (grant ERC DELPHI 802800). Ohad
muá»‘n cáº£m Æ¡n Iz Beltagy vÃ¬ Ä‘Ã£ Ä‘á» xuáº¥t chÆ°Æ¡ng trÃ¬nh
TRC, vÃ  toÃ n bá»™ phÃ²ng thÃ­ nghiá»‡m TAU NLP Ä‘áº·c
biá»‡t lÃ  Guy Dar vÃ  Itay Itzhak. NghiÃªn cá»©u nÃ y
Ä‘Æ°á»£c hoÃ n thÃ nh Ä‘á»ƒ hoÃ n thÃ nh má»™t pháº§n báº±ng
tiáº¿n sÄ© cá»§a Ohad Rubin.

--- TRANG 11 ---
7 Káº¿t luáº­n
Trong nghiÃªn cá»©u nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y Retrieval-Pretrained
Transformer (RPT), má»™t LM Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy
xuáº¥t, trong Ä‘Ã³ bá»™ truy xuáº¥t Ä‘Æ°á»£c huáº¥n luyá»‡n nhÆ°
má»™t thÃ nh pháº§n báº£n Ä‘á»‹a cá»§a LM Ä‘á»ƒ truy xuáº¥t cÃ¡c
Ä‘oáº¡n liÃªn quan ngá»¯ nghÄ©a cho dá»± Ä‘oÃ¡n vÄƒn báº£n
tÆ°Æ¡ng lai. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ RPT trÃªn bá»‘n nhiá»‡m
vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº§m xa, bao gá»“m sÃ¡ch, mÃ£
vÃ  viáº¿t toÃ¡n há»c. ChÃºng tÃ´i chá»©ng minh ráº±ng báº±ng
cÃ¡ch tÃ­ch há»£p liá»n máº¡ch bá»™ truy xuáº¥t vÃ o kiáº¿n trÃºc
vÃ  quy trÃ¬nh huáº¥n luyá»‡n, RPT hÆ°á»Ÿng lá»£i tá»« viá»‡c
há»£p nháº¥t ngá»¯ cáº£nh Ä‘Æ°á»£c truy xuáº¥t, cáº£i thiá»‡n so vá»›i
cÃ¡c baseline Ä‘Æ°á»£c tÄƒng cÆ°á»ng truy xuáº¥t máº¡nh. Trong
khi nghiÃªn cá»©u nÃ y táº­p trung vÃ o truy xuáº¥t tá»« vÄƒn
báº£n dÃ i, chÃºng tÃ´i láº­p luáº­n cÃ¡c phÃ¡t hiá»‡n thá»±c nghiá»‡m
cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng thÃ­ch á»©ng quy trÃ¬nh
cá»§a chÃºng tÃ´i cho truy xuáº¥t corpus dá»±a trÃªn web
tá»•ng quÃ¡t lÃ  má»™t hÆ°á»›ng tÆ°Æ¡ng lai thÃº vá»‹. Äiá»u nÃ y
sáº½ Ä‘Ã²i há»i vÆ°á»£t qua cÃ¡c khÃ³ khÄƒn ká»¹ thuáº­t liÃªn quan
Ä‘áº¿n viá»‡c má»Ÿ rá»™ng vÃ  xÃ¢y dá»±ng corpus huáº¥n luyá»‡n
trÆ°á»›c. ChÃºng tÃ´i hÃ¬nh dung RPT sáº½ má»Ÿ Ä‘Æ°á»ng cho
má»™t tháº¿ há»‡ má»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c huáº¥n
luyá»‡n trÆ°á»›c vá»›i truy xuáº¥t Ä‘Æ°á»£c tÃ­ch há»£p sÃ¢u trong
suá»‘t kiáº¿n trÃºc vÃ  quy trÃ¬nh huáº¥n luyá»‡n cá»§a chÃºng.
Lá»i cáº£m Æ¡n
NghiÃªn cá»©u nÃ y Ä‘Æ°á»£c há»— trá»£ bá»Ÿi Cloud TPUs tá»«
Google's TPU Research Cloud (TRC) vÃ  Há»™i Ä‘á»“ng
NghiÃªn cá»©u ChÃ¢u Ã‚u (ERC) dÆ°á»›i chÆ°Æ¡ng trÃ¬nh
nghiÃªn cá»©u vÃ  Ä‘á»•i má»›i Horizons 2020 cá»§a LiÃªn
minh ChÃ¢u Ã‚u (grant ERC DELPHI 802800). Ohad
muá»‘n cáº£m Æ¡n Iz Beltagy vÃ¬ Ä‘Ã£ Ä‘á» xuáº¥t chÆ°Æ¡ng trÃ¬nh
TRC, vÃ  toÃ n bá»™ phÃ²ng thÃ­ nghiá»‡m TAU NLP Ä‘áº·c
biá»‡t lÃ  Guy Dar vÃ  Itay Itzhak. NghiÃªn cá»©u nÃ y
Ä‘Æ°á»£c hoÃ n thÃ nh Ä‘á»ƒ hoÃ n thÃ nh má»™t pháº§n báº±ng
tiáº¿n sÄ© cá»§a Ohad Rubin.
TÃ i liá»‡u tham kháº£o
Samuel Amouyal, Tomer Wolfson, Ohad Rubin,
Ori Yoran, Jonathan Herzig, and Jonathan Be-
rant. 2023. QAMPARI: A benchmark for open-
domain questions with many answers. In Proc.
of the Third Workshop on GEM. ACL.
Zhangir Azerbayev, Edward Ayers, and Bartosz
Piotrowski. 2023. Proof-Pile: A Pre-training
Dataset of Mathematical Text.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document trans-
former.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for
sequence prediction with recurrent neural net-
works. In Proc. of NeurIPS .
Amanda Bertsch, Uri Alon, Graham Neubig, and
Matthew R. Gormley. 2023. Unlimiformer:
Long-range transformers with unlimited length
input. In Proc. of NeurIPS .
Stella Biderman, Hailey Schoelkopf, Quentin An-
thony, Herbie Bradley, Kyle O'Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Puro-
hit, USVSN Sai Prashanth, Edward Raff, Aviya
Skowron, Lintang Sutawika, and Oskar van der
Wal. 2023. Pythia: A suite for analyzing large
language models across training and scaling.
Sidney Black, Stella Biderman, Eric Hallahan,
Quentin Anthony, Leo Gao, Laurence Golding,
Horace He, Connor Leahy, Kyle McDonell, Ja-
son Phang, Michael Pieler, Usvsn Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan
Tow, Ben Wang, and Samuel Weinbach. 2022.
GPT-NeoX-20B: An open-source autoregressive
language model. In Proc. of the BigScience
Workshop .
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Mil-
lican, George van den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, Diego
de Las Casas, Aurelia Guy, Jacob Menick, Ro-
man Ring, Tom Hennigan, Saffron Huang, Loren
Maggiore, Chris Jones, Albin Cassirer, Andy
Brock, Michela Paganini, Geoffrey Irving, Oriol
Vinyals, Simon Osindero, Karen Simonyan,
Jack W. Rae, Erich Elsen, and Laurent Sifre.

--- TRANG 12 ---
2022. Improving language models by retrieving
from trillions of tokens. In Proc. of ICML .
Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Proc.
of NeurIPS .
Christopher J. C. Burges, Robert Ragno, and
Quoc Viet Le. 2006. Learning to rank with non-
smooth cost functions. In Proc. of NeurIPS .
Aakanksha Chowdhery, Sharan Narang, Jacob De-
vlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, Parker
Schuh, Kensen Shi, Sasha Tsvyashchenko,
Joshua Maynez, Abhishek Rao, Parker Barnes,
Yi Tay, Noam Shazeer, Vinodkumar Prab-
hakaran, Emily Reif, Nan Du, Ben Hutchin-
son, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay
Ghemawat, Sunipa Dev, Henryk Michalewski,
Xavier Garcia, Vedant Misra, Kevin Robin-
son, Liam Fedus, Denny Zhou, Daphne Ip-
polito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omer-
nick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Diaz, Orhan Firat, Michele
Catasta, Jason Wei, Kathy Meier-Hellstern, Dou-
glas Eck, Jeff Dean, Slav Petrov, and Noah
Fiedel. 2022. Palm: Scaling language model-
ing with pathways.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime
Carbonell, Quoc Le, and Ruslan Salakhutdinov.
2019. Transformer-XL: Attentive language mod-
els beyond a fixed-length context. In Proc. of
ACL.Soham De, Samuel L. Smith, Anushan Fernando,
Aleksandar Botev, George Cristian-Muraru, Al-
bert Gu, Ruba Haroun, Leonard Berrada, Yu-
tian Chen, Srivatsan Srinivasan, Guillaume
Desjardins, Arnaud Doucet, David Budden,
Yee Whye Teh, Razvan Pascanu, Nando De Fre-
itas, and Caglar Gulcehre. 2024. Griffin: Mixing
gated linear recurrences with local attention for
efficient language models.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proc. of NAACL-HLT .
Ehsan Doostmohammadi, Tobias Norlund, Marco
Kuhlmann, and Richard Johansson. 2023.
Surface-based retrieval reduces perplexity of
retrieval-augmented language models. In Proc.
of ACL .
Matthijs Douze, Alexandr Guzhva, Chengqi
Deng, Jeff Johnson, Gergely Szilvasy, Pierre-
Emmanuel MazarÃ©, Maria Lomeli, Lucas Hos-
seini, and HervÃ© JÃ©gou. 2024. The faiss library.
William Fedus, Barret Zoph, and Noam Shazeer.
2022. Switch transformers: Scaling to trillion
parameter models with simple and efficient spar-
sity. J. Mach. Learn. Res. , 23:1â€“39.
Daniel Y Fu, Tri Dao, Khaled Kamal Saab,
Armin W Thomas, Atri Rudra, and Christopher
Re. 2023. Hungry hungry hippos: Towards lan-
guage modeling with state space models. In
Proc. of ICLR .
Leo Gao, Stella Biderman, Sid Black, Laurence
Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima,
Shawn Presser, and Connor Leahy. 2020. The
pile: An 800gb dataset of diverse text for lan-
guage modeling.
Luyu Gao and Jamie Callan. 2022. Unsupervised
corpus aware language model pre-training for
dense passage retrieval. In Proc. of ACL .
Albert Gu and Tri Dao. 2023. Mamba: Linear-time
sequence modeling with selective state spaces.
Ankit Gupta, Harsh Mehta, and Jonathan Berant.
2023. Simplifying and understanding state space
models with diagonal linear rnns.

--- TRANG 13 ---
Kelvin Guu, Kenton Lee, Zora Tung, Panupong
Pasupat, and Ming-Wei Chang. 2020. Realm:
Retrieval-augmented language model pre-
training. In Proc. of ICML .
Yangsibo Huang, Daogao Liu, Zexuan Zhong, Wei-
jia Shi, and Yin Tat Lee. 2023. knn-adapter:
Efficient domain adaptation for black-box lan-
guage models.
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu,
Ethan Dyer, and Behnam Neyshabur. 2022.
Block-recurrent transformers. In Proc. of
NeurIPS .
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023.
Efficient Long-Text Understanding with Short-
Text Models. In Transactions of the Association
for Computational Linguistics , volume 11, pages
284â€“299.
Gautier Izacard, Mathilde Caron, Lucas Hosseini,
Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. 2022a. Unsu-
pervised dense information retrieval with con-
trastive learning. Transactions on Machine
Learning Research .
Gautier Izacard and Edouard Grave. 2021a. Dis-
tilling knowledge from reader to retriever for
question answering. In Proc. of ICLR .
Gautier Izacard and Edouard Grave. 2021b. Lever-
aging passage retrieval with generative models
for open domain question answering. In Proc.
of EACL .
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-
cas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
and Edouard Grave. 2022b. Atlas: Few-shot
learning with retrieval augmented language mod-
els.J. Mach. Learn. Res. , 24:1â€“43.
Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002.
Cumulated gain-based evaluation of ir tech-
niques. ACM Transactions on Information Sys-
tems, 20:422â€“446.
Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun
Araki, Haibo Ding, Jamie Callan, and Graham
Neubig. 2022. Retrieval as attention: End-to-
end learning of retrieval and reading within a
single transformer. In Proc. of EMNLP .Vladimir Karpukhin, Barlas Oguz, Sewon Min,
Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. 2020. Dense passage
retrieval for open-domain question answering.
InProc. of EMNLP .
Urvashi Khandelwal, Omer Levy, Dan Juraf-
sky, Luke Zettlemoyer, and Mike Lewis. 2020.
Generalization through memorization: Nearest
neighbor language models. In Proc. of ICLR .
Nikita Kitaev, Lukasz Kaiser, and Anselm Lev-
skaya. 2020. Reformer: The efficient trans-
former. In Proc. of ICLR .
Kenton Lee, Ming-Wei Chang, and Kristina
Toutanova. 2019. Latent retrieval for weakly
supervised open domain question answering. In
Proc. of ACL .
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau
Yih, Tim RocktÃ¤schel, Sebastian Riedel, and
Douwe Kiela. 2020. Retrieval-augmented gen-
eration for knowledge-intensive NLP tasks. In
Proc. of NeurIPS .
Pedro Henrique Martins, Zita Marinho, and An-
dre Martins. 2022. âˆ-former: Infinite memory
transformer. In Proc. of ACL .
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and
Behnam Neyshabur. 2023. Long range language
modeling via gated state spaces. In Proc. of
ICLR .
Antonio Orvieto, Samuel L Smith, Albert Gu,
Anushan Fernando, Caglar Gulcehre, Razvan
Pascanu, and Soham De. 2023. Resurrecting
recurrent neural networks for long sequences. In
Proc. of ICML .
Ofir Press, Noah A. Smith, and Mike Lewis. 2021.
Shortformer: Better language modeling using
shorter inputs. In Proc. of ACL .
Ofir Press and Lior Wolf. 2017. Using the out-
put embedding to improve language models. In
Proc. of EACL .
Jack Rae and Ali Razavi. 2020. Do transformers
need deep long-range memory? In Proc. of ACL .

--- TRANG 14 ---
Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku-
mar, Chloe Hillier, and Timothy P. Lillicrap.
2020. Compressive transformers for long-range
sequence modelling. In Proc. of ICLR .
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor
Muhlgay, Amnon Shashua, Kevin Leyton-
Brown, and Yoav Shoham. 2023. In-context
retrieval-augmented language models. Trans-
actions of the Association for Computational
Linguistics , 11:1316â€“1331.
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Be-
rant, and Amir Globerson. 2022. Learning to
retrieve passages without supervision. In Proc.
of NAACL-HLT .
Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: Bm25 and
beyond. Foundations and Trends in Information
Retrieval , 3:333â€“389.
Aurko Roy, Mohammad Saffar, Ashish Vaswani,
and David Grangier. 2021. Efficient content-
based sparse attention with routing transformers.
Transactions of the Association for Computa-
tional Linguistics , 9:53â€“68.
Ohad Rubin, Jonathan Herzig, and Jonathan Be-
rant. 2022. Learning to retrieve prompts for
in-context learning. In Proc. of NAACL-HLT .
Devendra Sachan, Mike Lewis, Mandar Joshi, Ar-
men Aghajanyan, Wen-tau Yih, Joelle Pineau,
and Luke Zettlemoyer. 2022. Improving passage
retrieval with zero-shot question generation. In
Proc. of EMNLP .
Devendra Singh Sachan, Siva Reddy, William L.
Hamilton, Chris Dyer, and Dani Yogatama. 2021.
End-to-end training of multi-document reader
and retriever for open-domain question answer-
ing. In Proc. of NeurIPS .
Christopher Sciavolino, Zexuan Zhong, Jinhyuk
Lee, and Danqi Chen. 2021. Simple entity-
centric questions challenge dense retrievers. In
Proc. of EMNLP .
Weijia Shi, Sewon Min, Michihiro Yasunaga,
Minjoon Seo, Rich James, Mike Lewis, Luke
Zettlemoyer, and Wen tau Yih. 2024. Replug:
Retrieval-augmented black-box language mod-
els. In Proc. of NAACL-HLT .Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng
Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer:
Enhanced transformer with rotary position em-
bedding. Neurocomput. , 568.
Sainbayar Sukhbaatar, Da Ju, Spencer Poff,
Stephen Roller, Arthur Szlam, Jason Weston,
and Angela Fan. 2021. Not all memories are
created equal: Learning to forget by expiring. In
Proc. of ICML .
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-
Micke, and Mohit Iyyer. 2021. Do long-range
language models actually use long-range con-
text? In Proc. of EMNLP .
Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux, Timo-
thÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal,
Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume
Lample. 2023. Llama: Open and efficient foun-
dation language models.
Boxin Wang, Wei Ping, Peng Xu, Lawrence
McAfee, Zihan Liu, Mohammad Shoeybi,
Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei
Xiao, Anima Anandkumar, and Bryan Catanzaro.
2023. Shall we pretrain autoregressive language
models with retrieval? a comprehensive study.
InProc. of EMNLP .
Thomas Wolf, Loubna Ben Allal, Leandro von
Werra, Li Jia, and Armel Zebaze. 2023. A
dataset of python files from github.
Yuhuai Wu, Markus Norman Rabe, DeLesley
Hutchins, and Christian Szegedy. 2022. Memo-
rizing transformers. In Proc. of ICLR .
Dani Yogatama, Cyprien de Masson d'Autume, and
Lingpeng Kong. 2021. Adaptive semiparametric
language models. Transactions of the Associa-
tion for Computational Linguistics , 9:362â€“373.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, and Amr Ahmed. 2020. Big
bird: Transformers for longer sequences. In
Proc. of NeurIPS .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin,

--- TRANG 15 ---
Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt
Shuster, Daniel Simig, Punit Singh Koura, An-
jali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
2022. Opt: Open pre-trained transformer lan-
guage models.
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.
Training language models with memory augmen-
tation. In Proc. of EMNLP .
Juntang Zhuang, Tommy Tang, Yifan Ding,
Sekhar C. Tatikonda, Nicha C. Dvornek,
Xenophon Papademetris, and James S. Duncan.
2020. Adabelief optimizer: Adapting stepsizes
by the belief in observed gradients. In Proc. of
NeurIPS .
A Chi tiáº¿t Triá»ƒn khai Bá»• sung
CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c triá»ƒn khai trong JAX vá»›i tá»· lá»‡
dropout lÃ  0.05, vÃ  bá»™ tá»‘i Æ°u AdaBelief (Zhuang
et al., 2020) vá»›i trá»ng sá»‘ decay 1e-8, cosine decay
vá» 0.1 cá»§a tá»· lá»‡ há»c tá»‘i Ä‘a, cáº¯t gradient norm toÃ n
cá»¥c lÃ  1, vÃ  tied input embedding (Press and Wolf,
2017). TÃ¬m kiáº¿m lÆ°á»›i xÃ¡c Ä‘á»‹nh cÃ¡c giÃ¡ trá»‹ Ï„: 128
cho Books3, 4 cho PG19, 2 cho CodeParrot, vÃ  8
cho ArXiv. ChÃºng tÃ´i Ä‘áº·t Î±ret= 1eâˆ’9 cho táº¥t cáº£
cÃ¡c bá»™ dá»¯ liá»‡u vÃ  tá»· lá»‡ há»c cÆ¡ sá»Ÿ lÃ  5eâˆ’3, sá»­ dá»¥ng
táº­p validation Ä‘á»ƒ lá»±a chá»n siÃªu tham sá»‘.
B Äá»™ phá»©c táº¡p TÃ­nh toÃ¡n
Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n má»—i token cá»§a má»™t lá»›p attention
trong mÃ´ hÃ¬nh transformer vá»›i chiá»u d, |Q| truy
váº¥n vÃ  |K| khÃ³a lÃ  2Â·dÂ·(|K|Â·|Q|+ |K|Â·d+ |Q|Â·d)
flops.â· Báº±ng cÃ¡ch Ä‘áº·t N=|Q|=|K| vÃ  thÃªm chi
phÃ­ lá»›p feed-forward, chÃºng tÃ´i cÃ³ Ä‘Æ°á»£c chi phÃ­
má»—i token cho má»™t khá»‘i transformer khi dâ‰«N
lÃ  2d(N+ 2d) + 8 d2â‰ˆ12d2 flops. Äá»‘i vá»›i CCA,
chi phÃ­ phá»¥ thuá»™c vÃ o kÃ­ch thÆ°á»›c Ä‘oáº¡n C, vÃ  sá»‘
lÆ°á»£ng hÃ ng xÃ³m k. Äáº·t |K|= 2Ck vÃ  |Q|=C,
vÃ  giáº£ sá»­ dâ‰«Ck, chi phÃ­ má»—i token cho má»™t lá»›p
CCA lÃ  2d(2Ck+ 2dk+d)â‰ˆ(4k+ 2)Â·d2 flops.
Chi phÃ­ overhead má»—i token cá»§a chÃºng tÃ´i cho
Î±âˆˆ[0,1] cá»§a cÃ¡c khá»‘i bao gá»“m CCA lÃ  â‰ˆÎ±(k/3+1/6).
Trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i sá»­
dá»¥ng CCA trong 5 trong 12 lá»›p nÃªn Î±=5/12 vÃ 
â·Äá»‘i vá»›i ma tráº­n truy váº¥n QâˆˆR|Q|Ã—d vÃ  ma tráº­n khÃ³a/giÃ¡
trá»‹ KâˆˆR|K|Ã—d, nÃ³ bao gá»“m cÃ¡c phÃ©p toÃ¡n sau: nhÃ¢n vá»›i WQ,
WK, vÃ  WV cho cÃ¡c truy váº¥n, khÃ³a vÃ  giÃ¡ trá»‹, má»—i phÃ©p tÃ­nh
|Q|Â·d2, |K|Â·d2, vÃ  |K|Â·d2 flops tÆ°Æ¡ng á»©ng. TÃ­nh toÃ¡n ma
tráº­n attention vÃ  nhÃ¢n nÃ³ vá»›i cÃ¡c giÃ¡ trá»‹ má»—i phÃ©p Ä‘Ã²i há»i
|Q|Â·|K|Â·d flops. Cuá»‘i cÃ¹ng, nhÃ¢n vá»›i ma tráº­n Ä‘áº§u ra lÃ  thÃªm
|Q| Â·d2 flops.k= 2, vÃ  nháº­n Ä‘Æ°á»£c ráº±ng CCA Ä‘Ã³ng gÃ³p má»™t overhead
khoáº£ng 1.29x. Sá»­ dá»¥ng logic tÆ°Æ¡ng tá»±, chi phÃ­ khÃ´ng
Ä‘á»•i cho thÃ nh pháº§n truy xuáº¥t lÃ  hai phÃ©p chiáº¿u
tuyáº¿n tÃ­nh, hai lá»›p attention hai chiá»u bá»• sung, vÃ 
lá»›p tÄƒng cÆ°á»ng truy váº¥n dáº«n Ä‘áº¿n 1/nlayersÂ·(7k/6+1/2),
hoáº·c overhead cuá»‘i cÃ¹ng lÃ  1.49x phÃ¹ há»£p vá»›i
overhead thá»i gian cháº¡y hiá»‡u quáº£ Ä‘o Ä‘Æ°á»£c cá»§a
chÃºng tÃ´i lÃ  1.51x (xem Báº£ng 2).
C Chi tiáº¿t huáº¥n luyá»‡n bá»™ truy xuáº¥t DPR-style
ChÃºng tÃ´i theo cÃ´ng thá»©c huáº¥n luyá»‡n cá»§a DPR
(Karpukhin et al., 2020) trong viá»‡c huáº¥n luyá»‡n
bá»™ truy xuáº¥t BERT-base vá»›i contrastive loss. Má»¥c
tiÃªu DPR Ä‘Ã²i há»i cÃ¡c positive vÃ  hard negative Ä‘á»ƒ
há»™i tá»¥ thÃ nh cÃ´ng, vÃ  á»Ÿ Ä‘Ã¢y chÃºng tÃ´i sá»­ dá»¥ng
Ä‘oáº¡n BM25 cÃ³ Ä‘iá»ƒm sá»‘ top-1 lÃ m vÃ­ dá»¥ positive
vÃ  Ä‘oáº¡n xáº¿p háº¡ng thá»© 5 bá»Ÿi BM25 lÃ m vÃ­ dá»¥ hard
negative. Äá»ƒ Ä‘áº£m báº£o so sÃ¡nh cÃ´ng báº±ng, chÃºng
tÃ´i huáº¥n luyá»‡n bá»™ truy xuáº¥t contrastive cá»§a mÃ¬nh
trÃªn 16x nhiá»u vÃ­ dá»¥ hÆ¡n so vá»›i cÃ´ng thá»©c DPR
gá»‘c mÃ´ táº£.

--- TRANG 16 ---
