# 2107.05768.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2107.05768.pdf
# Kích thước tệp: 488244 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Combiner: Transformer Attention Đầy Đủ
với Chi Phí Tính Toán Thưa Thớt
Hongyu Reny,Hanjun Dai,Zihang Dai
Mengjiao Yang,Jure Leskovecy,Dale Schuurmans;z,Bo Dai
yĐại học Stanford, {hyren,jure}@cs.stanford.edu
Google Research, Brain Team, {hadai,zihangd,sherryy,schuurmans,bodai}@google.com
zĐại học Alberta
Tóm tắt
Transformers cung cấp một lớp các kiến trúc biểu đạt cực kỳ hiệu quả cho mô hình hóa chuỗi. Tuy nhiên, hạn chế chính của transformers là độ phức tạp bộ nhớ và thời gian bậc hai O(L2) theo chiều dài chuỗi trong các lớp attention, điều này hạn chế ứng dụng trong các chuỗi cực dài. Hầu hết các phương pháp hiện có tận dụng tính thưa thớt hoặc giả định thứ hạng thấp trong ma trận attention để giảm chi phí, nhưng hy sinh tính biểu đạt. Thay vào đó, chúng tôi đề xuất Combiner, cung cấp khả năng attention đầy đủ trong mỗi đầu attention trong khi duy trì độ phức tạp tính toán và bộ nhớ thấp. Ý tưởng chính là coi cơ chế self-attention như một kỳ vọng có điều kiện trên embeddings tại mỗi vị trí, và xấp xỉ phân phối có điều kiện với một phân tích nhân tử có cấu trúc. Mỗi vị trí có thể attend đến tất cả các vị trí khác, hoặc thông qua attention trực tiếp, hoặc thông qua attention gián tiếp đến các abstractions, đây lại là các kỳ vọng có điều kiện của embeddings từ các vùng địa phương tương ứng. Chúng tôi cho thấy rằng hầu hết các mẫu attention thưa thớt được sử dụng trong các sparse transformers hiện có đều có thể truyền cảm hứng cho việc thiết kế phân tích nhân tử như vậy cho attention đầy đủ, dẫn đến cùng chi phí dưới bậc hai (O(Llog(L)) hoặc O(Lp
L)). Combiner là một thay thế trực tiếp cho các lớp attention trong các transformers hiện có và có thể dễ dàng được triển khai trong các framework phổ biến. Một đánh giá thực nghiệm trên cả các tác vụ chuỗi tự hồi quy và hai chiều chứng minh tính hiệu quả của phương pháp này, mang lại kết quả state-of-the-art trên một số tác vụ mô hình hóa hình ảnh và văn bản.

1 Giới thiệu
Transformer [1] là một kiến trúc mạng neural mạnh mẽ đã chứng minh hiệu suất state-of-the-art trong dịch máy [2] và nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) khác thông qua pretraining, sử dụng hoặc mô hình ngôn ngữ một chiều [3] hoặc mô hình ngôn ngữ hai chiều [4–8]. Nó cũng đã đạt được kết quả xuất sắc trong các lĩnh vực khác như nhận dạng hình ảnh [9], hiểu mã [10], nhận dạng giọng nói [11], protein [12], âm nhạc [13] và mô hình tạo sinh hình ảnh [14]. Thành phần cốt lõi của Transformer là cơ chế attention, tính toán các phụ thuộc giữa tất cả các cặp vị trí trong một chuỗi. Tuy nhiên, đối với một chuỗi có độ dài L, tính biểu đạt của attention theo cặp đi kèm với chi phí bậc hai O(L2) trong cả tiêu thụ thời gian và bộ nhớ. Điều này làm cho Transformer vanilla [1] trở nên cấm đoán đối với các ứng dụng liên quan đến các chuỗi dài, bao gồm hình ảnh độ phân giải cao, chuỗi protein, hoặc tín hiệu giọng nói thô [15], nơi chiều dài chuỗi L thường lớn hơn 10;000[14].

Gần đây, đã có một số nỗ lực để mở rộng attention đến các chuỗi dài. Một lớp phương pháp phổ biến sparsiﬁes ma trận attention với các mẫu sparsity khác nhau, bao gồm cửa sổ địa phương [16,17], local+stride [14], log-sparse [18], axial [19,20], hoặc các mẫu có thể học được thông qua hashing [21] hoặc clustering [22]. Sparse attention có chi phí dưới bậc hai, nhưng bị mất mát trong việc nắm bắt tất cả các mối quan hệ theo cặp. Nói chung, sparse attention yêu cầu nhiều lớp hơn [14,20,23] để đạt được các phụ thuộc tự hồi quy hoặc hai chiều đầy đủ (hoặc receptive fields [20]) cho mỗi vị trí trong một chuỗi dài.

Ngoài ra, một hướng nghiên cứu khác đã cố gắng đạt được khả năng mở rộng với một giả định thứ hạng thấp rõ ràng [24,25] trên ma trận attention hoặc bằng cách sử dụng các feature maps rõ ràng của một số kernels [26]. Tuy nhiên, những xấp xỉ chiều thấp rõ ràng này có thể quá hạn chế đối với ma trận attention có thể có thứ hạng đầy đủ, sử dụng các kernels mũ có hiệu quả là vô hạn chiều [27]. Performer [28] là một trong những công trình đầu tiên cố gắng xấp xỉ attention thứ hạng đầy đủ thông thường với trick random feature [29]. Tuy nhiên, các phương pháp dựa trên random-feature như vậy [30] yêu cầu nhiều cơ sở hơn để xấp xỉ tốt hơn kernel mũ [27], và theo kinh nghiệm chúng tôi thấy nó tạo ra kết quả kém hơn trong một số tác vụ mô hình hóa chuỗi, chẳng hạn như ước lượng mật độ.

Trong bài báo này chúng tôi đề xuất Combiner, một thay thế trực tiếp cho cơ chế attention bậc hai vanilla với chi phí tính toán và bộ nhớ dưới bậc hai. Combiner vẫn đạt được khả năng attention đầy đủ trong mỗi đầu của Multi-Head Attention, không giống như các phương pháp áp dụng xấp xỉ thưa thớt hoặc thứ hạng thấp. Như chúng ta sẽ thảo luận, attention tiêu chuẩn được tính toán tại mỗi vị trí có thể được xem như kỳ vọng có điều kiện của value embeddings tại tất cả các vị trí khả thi cho trước vị trí hiện tại. Dựa trên sự hiểu biết như vậy, Combiner một cách rõ ràng xấp xỉ phân phối có điều kiện thông qua một phân tích nhân tử có cấu trúc của không gian xác suất. Cụ thể, cho một vị trí x, xác suất attend đến vị trí y có thể được tính toán trực tiếp thông qua query vector của x và key vector của y, hoặc gián tiếp thông qua một abstraction địa phương nơi x đầu tiên attend đến key vector đại diện cho một nhóm vị trí chứa y, và nhân với xác suất chọn y trong nhóm đó. Chúng tôi gọi mô hình này là Combiner vì các phân phối có điều kiện trong attention trở thành một sự kết hợp giữa một số attentions địa phương và attentions trực tiếp. Phân tách có cấu trúc này cho phép Combiner lấy các mẫu attention thưa thớt hiện có và chuyển đổi chúng thành các lựa chọn thiết kế tương ứng cho phân tích nhân tử xác suất đạt được attention đầy đủ. Như được hiển thị trong Hình 1, Combiner đạt được attention đầy đủ với cùng độ phức tạp tiệm cận như các biến thể thưa thớt. Combiner có thể dễ dàng được triển khai trong hầu hết các framework deep learning hiện có mà không cần triển khai phần cứng chuyên biệt, và thân thiện với GPU/TPU. Trên thực tế, cả các mẫu attention thưa thớt cố định và có thể học được từ nhiều biến thể Transformer hiện có [14,18,20,22] đều có thể được tăng cường với các phân tích nhân tử có cấu trúc như vậy, với cùng thứ tự chi phí thời gian hoặc bộ nhớ.

Chúng tôi xác nhận Combiner trên cả các tác vụ mô hình hóa chuỗi tự hồi quy và hai chiều trên nhiều lĩnh vực khác nhau bao gồm văn bản và hình ảnh. Chúng tôi cho thấy rằng Combiner có thể đạt được perplexity và độ chính xác tốt hơn khi sử dụng cùng kiến trúc transformer trong khi nhanh hơn nhiều về thời gian chạy, và đạt được hiệu suất state of the art trên ước lượng mật độ trên các dataset tiêu chuẩn CIFAR-10 (2.77 bits/dim) và ImageNet-64 (3.42 bits/dim), cũng như Long-Range Arena [31]. Việc triển khai Combiner có thể được tìm thấy tại https://github.com/google-research/google-research/tree/master/combiner.

2 Attention như Kỳ vọng Có điều kiện
Trong phần này, chúng tôi xem xét lại công thức của Transformer tiêu chuẩn [1] từ quan điểm kỳ vọng có điều kiện, điều này truyền cảm hứng cho việc dẫn xuất Combiner.

Không mất tính tổng quát, chúng tôi sử dụng một chuỗi duy nhất trong kịch bản self-attention. Cho một chuỗi L embeddings X= [x1; x2; : : : ; xL], trong đó X2RLd và mỗi embedding xi2Rd là một vector d-chiều, thành phần cốt lõi của Transformer là multi-head attention, trong đó mỗi head h là một scaled dot-product attention:

Ah(X) =softmaxQhp
dK>
h
Vh;n
Qh=XWQ
h; Kh=XWK
h; Vh=XWV
ho
2RLd;(1)

và vector attention từ mỗi head Ah(X) được nối và chiếu:

MultiHeadAttn (X) = [A1(X); A2(X); : : : ; AH(X)]Wo; Wo2RHdd: (2)

Ở đây H là tổng số heads mỗi lớp Transformer. Trong bài báo này, chúng tôi tập trung vào cách xấp xỉ attention đầy đủ trong mỗi head của multi-head attention. Để dễ ký hiệu, chúng tôi bỏ chỉ số head h bất cứ khi nào có thể, và sử dụng các chữ cái thường xi; qi; ki; vi2Rd để biểu thị các hàng trong

--- TRANG 2 ---
X; Q; K; V tương ứng, tương ứng với một vị trí i trong chuỗi gốc có độ dài L. Chúng tôi sử dụng [n] để biểu thị tập hợp các số nguyên dương f1;2; : : : ; ng.

Đối với một vị trí i2[L], công thức attention (1) có thể được xem như kỳ vọng có điều kiện của các hàng trong V. Cụ thể, vì softmax xuất ra một phân phối xác suất, chúng ta có thể viết lại (1) như

A(xi) =Ep(jji)[vj]; p (jji) =1
Z(xi)expqip
dk>
j
; (3)

trong đó p(jji) biểu thị xác suất có điều kiện tại vị trí j cho trước token tại vị trí i và hàm phân hoạch Z(xi) =P
j2
iexp
qip
dk>
j
trên support 
i. Support 
i của p(jji) định nghĩa tập hợp các vị trí hợp lệ mà token thứ i có thể attend đến. Ví dụ, tập support trong mô hình ngôn ngữ tự hồi quy (LM) bao gồm tất cả các token trước đó, tức là 
LM
i= [i]2; trong mô hình ngôn ngữ có mặt nạ (MLM) support bao gồm tất cả các token trong chuỗi, tức là 
MLM
i= [L]. Đó là, 
LM
i và 
MLM
i đại diện cho khả năng attention đầy đủ tương ứng trong cài đặt LM và MLM.

3 Combiner: Attention Đầy Đủ thông qua Kỳ vọng Có điều kiện Có cấu trúc
Độ phức tạp của p(jji) là nút thắt cổ chai của tính toán cho A(xi). Nói chung, trong các sparse transformers hiện có, support của p(jji) được sparsified để giảm độ phức tạp tính toán và bộ nhớ, ví dụ, 
Sparse
i(
LM
i cho LM và 
Sparse
i(
MLM
i cho MLM, nhưng điều này có thể dẫn đến giảm khả năng hoặc khả năng áp dụng hạn chế. Chúng tôi hoãn thảo luận chi tiết về khả năng đầy đủ của mô hình đến Phụ lục A. Trong phần này chúng tôi giới thiệu Combiner, đạt được 
Combiner
i = 
LM
i cho LM và 
Combiner
i = 
MLM
i cho MLM, trong khi vẫn duy trì chi phí tính toán và bộ nhớ dưới bậc hai. Dưới đây chúng tôi ký hiệu 
i như support cho attention đầy đủ nếu không có sự mơ hồ hoặc cần phân biệt giữa LM hoặc MLM. Chúng tôi giới thiệu khung thiết kế chính trong Phần 3.1 và các tham số hóa có thể trong Phần 3.2. Sau đó trong Phần 3.3 chúng tôi phân tích sự đánh đổi của Combiner.

3.1 Phân tích Nhân tử Địa phương cho Kỳ vọng Có điều kiện
Ý tưởng chính của Combiner là khai thác một cấu trúc phân cấp cho mô hình xác suất có điều kiện trong (3), cung cấp cơ hội giảm độ phức tạp tính toán trong khi duy trì cùng support. Cụ thể, chúng tôi giới thiệu các biến support 
r
i, cho r= 0; : : : ; ni và i2[L]. Các biến support là rời nhau, tức là, 
r
i\
s
i=;;8r6=s, và [ni
r=0
r
i= 
i. Sau đó chúng ta có thể phân tích nhân tử p(jji) như

p(jji) =niX
r=0p(j;
r
iji) =niX
r=0p(jj
r
i; i)p(
r
iji) =p(jj
rj
i; i)p(
rj
iji); (4)

trong đó rj biểu thị chỉ số của support mà j thuộc về. Phương trình cuối phát sinh từ thực tế là các 
r
i rời nhau với nhau (
r
i\
s
i=;;8r6=s). Do đó, chỉ có một support, 
rj
i, chứa j. Các số hạng còn lại, trong đó j62
r
i cho r6=rj, đều bằng không vì p(jj
r
i; i) = 0.

Hơn nữa, giả sử 
rj
i là một thống kê đủ, tức là j và i độc lập cho trước 
rj
i, chúng ta thu được

p(jji) =p(jj
rj
i)p(
rj
iji): (5)

Cho phân hoạch f
r
igni
r=0, dạng attention trong (3) có thể được viết lại như

A(xi) = Ep(jji)[vj] =niX
r=0X
j2
r
ip(j;
r
iji)vj (6)
=X
j2
0
i~p(jji)vj
|{z}
kỳ vọng trực tiếp+Pni
r=1p(
r
iji)X
j2
r
ip(jj
r
i)vj
|{z}
kỳ vọng địa phương; (7)

trong đó chúng tôi xem xét attention trực tiếp trong phân hoạch 
0
i và áp dụng phân tích nhân tử địa phương (5) cho phân hoạch r= 1; : : : ; ni. Ở đây ~p(jji)/p(jji) nhưng với các hằng số chuẩn hóa khác nhau, sẽ được giải thích bên dưới. Chúng tôi gọi mô hình này là Combiner vì attention có cấu trúc (7) kết hợp kỳ vọng trực tiếp của 
0
i và nhiều kỳ vọng địa phương thông qua p(jj
r
i) và p(
r
iji) để tạo thành kỳ vọng có điều kiện cuối cùng.

Tương đương, chúng ta cũng có thể viết lại attention có cấu trúc (7) như

A(xi) =P
j2
i"
I(j2
0
i)~p(jji) +niX
r=1I(j2
r
i)p(jj
r
i)p(
r
iji)#
| {z }
xác suất có điều kiện hiệu quả mới q(jji)vj; (8)

trong đó I() là một hàm chỉ thị nhị phân. Sau khi sắp xếp lại, người ta có thể thấy từ (8) rằng chúng ta thu được xác suất có điều kiện hiệu quả q(jji) cố gắng xấp xỉ p(jji) gốc. Mỗi số hạng xác suất phụ thuộc vào cả vị trí hiện tại i và vị trí khác j, và kỳ vọng vẫn được thu được đối với một xác suất có điều kiện hợp lệ (không âm và tổng bằng 1 trên 
i).

Yêu cầu cho Chi phí Dưới bậc hai. Chúng ta có thể thấy ngay lợi ích của công thức này từ thực tế là kỳ vọng địa phương trong (7) độc lập với vị trí i. Sự phụ thuộc đầy đủ được đạt được thông qua số nhân p(
r
iji) trong đó j2
r
i. Nếu chúng ta có thể thiết kế phân tích nhân tử địa phương sao cho:

1. thứ tự số lượng số hạng trong (7) cho p(ji);8i2[L]:PL
i=1(ni+j
0
ij) là dưới bậc hai; và
2. gọi U=f
r
igi2[L];r2[1;ni] là tập hợp duy nhất các phân hoạch được sử dụng cho tính toán kỳ vọng địa phương, sau đó thứ tự của jUj (tức là, số lượng phân hoạch duy nhất trong U) là dưới bậc hai;
3. thứ tự tổng số lượng tính toán kỳ vọng địa phương duy nhất trên tất cả các vị trí trong (7), P

2Uj
j là dưới bậc hai;

thì người ta có thể thấy rằng tổng chi phí tính toán và bộ nhớ sẽ là dưới bậc hai với support attention đầy đủ 
Combiner
i = 
i;8i2[L]. Chúng tôi sẽ thảo luận chi tiết trong Phần 4 về cách thực hiện nguyên tắc như vậy bằng cách lấy cảm hứng từ các sparse transformers hiện có, và cách chuyển đổi chúng thành một mô hình attention đầy đủ gần như miễn phí với độ phức tạp tiệm cận giống hệt nhau.

Nhận xét (Phân tách Phân cấp Tiếp theo): Chúng tôi giới thiệu phân tách địa phương với một phân hoạch một lớp của support của p(ji) để đơn giản. Trên thực tế, các phân tách địa phương như vậy có thể được xếp chồng tiếp theo, tạo ra một cây phân hoạch. Cụ thể, chúng ta có thể phân hoạch tiếp theo 
r
i với các tập con rời nhau {
rk
i}nr
k=1, và xem xét phân tách địa phương p(j;
r
iji) =p(jj
rkj
i; i)p(
rkj
ij
r
i; i)p(
r
iji), trong đó kj là chỉ số của vùng phụ mà j thuộc về. Do đó, chúng ta thu được một phân tách phân cấp của p(jji), cũng có thể được cắm vào (6) và tạo ra một công thức attention đầy đủ mới.

--- TRANG 3 ---
(D) Combiner
-
Fixed
(A) Fixed
(B) 
Logsparse
(E) Combiner
-
Logsparse
Kỳ vọng Trực tiếp
Kỳ vọng Địa phương
(F) Combiner
-
Axial
(C) AxialHình 1: Ma trận attention của một số thực hiện Combiner trong cài đặt tự hồi quy. Chúng tôi biến đổi một số mẫu attention thưa thớt: Fixed (A) [14], Logsparse (B) [18] và Axial (C) [20] thành Combiner-Fixed (D), Combiner-Logsparse (E) và Combiner-Axial (F). Combiner xấp xỉ kỳ vọng có điều kiện (3) với sự kết hợp của kỳ vọng trực tiếp (xanh dương) và kỳ vọng địa phương (vàng). Các thực hiện của chúng tôi (D)(E)(F) đạt được attention đầy đủ với cùng độ phức tạp dưới bậc hai.

3.2 Tham số hóa Xác suất Có điều kiện
Trong khi chúng tôi có được một cách có thể để tăng tốc Transformer tiêu chuẩn thông qua sự kết hợp của kỳ vọng trực tiếp và kỳ vọng địa phương, việc có một lựa chọn thiết kế hiệu quả cho các số hạng xác suất trong (7) cũng rất quan trọng, cụ thể là ~p(jji) từ kỳ vọng trực tiếp, p(jj
r
i) từ kỳ vọng địa phương và p(
r
iji) cho r2[1; ni]. Để đơn giản chúng tôi sử dụng scaled dot-product, có nghĩa là chúng tôi sẽ liên kết các vị trí i; j và các tập biến 
r
i với biểu diễn embedding tương ứng, và do đó xác suất tỷ lệ thuận với số mũ của tích vô hướng embedding. Cụ thể:

•~p(jji): Vì số hạng này là cho kỳ vọng trực tiếp, chúng ta có thể cho ~p(jji)/exp(qip
dk>
j), giống như attention vanilla (3) nhưng với chuẩn hóa khác nhau, sẽ được giải thích trong Phương trình 9.

•p(
r
iji): Số hạng này nhằm nắm bắt xác suất sự kiện kết hợp, tức là, p(
r
iji)/exp
qip
dk>

r
i
. Do đó lựa chọn thiết kế của k
r
i nên tạo ra một abstraction của support tương ứng 
r
i. Chúng tôi thấy k
r
i=max poolingj2
r
ikj đã cung cấp kết quả thực nghiệm tốt mà không cần thêm tham số; chúng ta cũng có thể sử dụng DeepSets [32] để có được abstraction như vậy.

•p(jj
r
i): Số hạng này là xác suất của việc có được j trong khoảng địa phương này 
r
i. Chúng tôi tạo p(jj
r
i)/
expq
r
ip
dk>
j
, trong đó chúng tôi sử dụng max pooling hoặc DeepSets trên fqjgj2
r
i để có được q
r
i tương tự.

Chuẩn hóa Các Số hạng Xác suất. Các số hạng trong mỗi kỳ vọng địa phương p(jj
r
i);8j2
r
i có thể được chuẩn hóa trong khoảng địa phương; kỳ vọng trực tiếp ~p(jji) và các số hạng trong p(
r
iji) nên được chuẩn hóa cùng nhau,

Z(xi) =X
j2
(0)
iexpqip
dk>
j
+niX
r=1expqip
dk>

r
i
; (9)

và Z(xi) là hằng số chuẩn hóa khi tính toán ~p(jji) và p(
r
iji).

3.3 Đánh đổi trong Combiner
Combiner đạt được attention đầy đủ với chi phí giảm mà không đưa ra các giả định sparsity hoặc thứ hạng thấp rõ ràng trên ma trận attention. Tuy nhiên, lợi ích hiệu quả này không phải là miễn phí. Trong phần này chúng tôi thảo luận về các hạn chế của việc đơn giản hóa được thực hiện bởi Combiner, và cung cấp một giải pháp đơn giản.

Xấp xỉ Attention Có cấu trúc. Chúng tôi thu được phân tách địa phương (5) dưới giả định độc lập có điều kiện. Do đó, kỳ vọng địa phương trong (7) độc lập với vị trí i, điều này gợi ý rằng bất kỳ hai vị trí i1 và i2 với 
r
i1= 
r
i2= 
 sẽ có điểm attention phụ thuộc tuyến tính trên vùng 
. Chính thức, các xác suất được tạo bởi phân phối có điều kiện hiệu quả ~ a(
)i1=h
q(j1ji1); q(j2ji1); : : : ; q (jj
r
i1jji1)i
=p(
r
i1ji1)
p(
r
i2ji2)~ a(
)i2. Nói cách khác, thứ hạng của ma trận con trên cùng phân hoạch trong ma trận attention kết quả là 1, do đó, ma trận attention có thứ hạng thấp địa phương dựa trên phân hoạch. Mặt khác, kỳ vọng trực tiếp attend đầy đủ đến mỗi vị trí trong sub-support 
0, đảm bảo khối thứ hạng đầy đủ. Hai lược đồ attention này làm cho ma trận attention của Combiner có cấu trúc. So với xấp xỉ thứ hạng thấp cho attention [26,28,30], được truyền cảm hứng từ random features [29] trong cộng đồng kernel, một xấp xỉ có cấu trúc khai thác cả các khối thứ hạng thấp địa phương và thứ hạng đầy đủ đã được chứng minh mạnh mẽ hơn về mặt lý thuyết và thực nghiệm trong các máy kernel quy mô lớn [27].

Cải thiện Tính biểu đạt Sử dụng Mô hình Hỗn hợp. Một cách để cải thiện thêm tính biểu đạt của phân tích nhân tử địa phương là sử dụng một mô hình hỗn hợp. Ý tưởng này được điều chỉnh từ mixture of softmaxs [33] để có được lớp softmax thứ hạng cao trong mô hình ngôn ngữ. Gọi ! là một phân hoạch nhất định của support (tức là, tập hợp của 
r
i) của 
i, thì người ta có thể dễ dàng sử dụng A(xi) =1
MPM
m=1A(xi;!m) để tính toán attention, trong đó mỗi thành phần của hỗn hợp A(xi;!m) là số hạng (7) sử dụng một kế hoạch phân tích nhân tử cụ thể !m. Theo kinh nghiệm chúng tôi thấy hai thành phần đã đủ để cải thiện hiệu suất.

4 Các Thực hiện Combiner
Trong phần này chúng tôi cho thấy một số lược đồ phân tích nhân tử địa phương thỏa mãn các yêu cầu trong Phần 3.1. Như chúng ta sẽ thấy, Combiner có thể chuyển đổi một số sparse transformers [14,18,20–22] thành attention đầy đủ, với cùng thứ tự tiêu thụ tính toán và bộ nhớ. Người ta cũng có thể thiết kế các mẫu phân tích nhân tử khác, có thể dễ dàng được thực hiện trong Combiner.

4.1 Combiner-Fixed
Sparse Transformer [14] là một trong những biến thể đại diện nhất có thể đạt được chi phí tính toán và bộ nhớ O(Lp
L). Ở đây chúng tôi cho thấy cách chuyển đổi mẫu cố định này được đề xuất trong [14] (Hình 1(A)) thành một kế hoạch phân tích nhân tử, và thực hiện một biến thể attention đầy đủ được gọi là Combiner-Fixed (Hình 1(D)).

Trong fixed-sparse attention, support là 
sparse MLM
i =fj:jmods= 0g[fj:ji(divs)g trong đó s là một siêu tham số, div là phép chia nguyên, và ji(divs) biểu thị rằng thương số của i và j w.r.t. s là giống nhau. Trong trường hợp tự hồi quy, 
sparse LM
i = 
sparse MLM
i\[i]. Vui lòng tham khảo Hình 1(A) để minh họa phiên bản LM.

Thiết kế của chúng tôi về !MLM
ﬁxed có dạng sau:


0
i=fj:ji(divs)g;
r
i=
j:jdivs=r; j =2
0
i	
;8r2[Ldivs];8i2[L] (10)

trong đó mỗi kỳ vọng địa phương được thực hiện trong mỗi khoảng có kích thước s, và có tổng cộng Ldivs khoảng trên tất cả các vị trí. Đối với mỗi vị trí i2[L], có (s+ (Ldivs)) số hạng trong (7); kỳ vọng địa phương có (Ldivs) số hạng. Độ phức tạp tổng thể là O(L(s+ 2(Ldivs))). s tối ưu là O(p
L), và chúng ta có thể đạt được độ phức tạp tính toán và bộ nhớ O(Lp
L), giống như [14] nhưng ở đây chúng ta đạt được khả năng attention đầy đủ trong mỗi đầu attention. Đối với trường hợp LM, chúng ta có thể đơn giản có !LM
ﬁxed:f
r
i\[i]j
r
i2!MLM
ﬁxedg, có cùng độ phức tạp tối ưu O(Lp
L).

4.2 Combiner-Logsparse
Logsparse Transformer được đề xuất trong [18] và về mặt lý thuyết có thể đạt được chi phí O(LlogL). Ý tưởng chung là làm cho kích thước của support 
sparse
i không lớn hơn dlog2ie. Để dễ ký hiệu, chúng tôi đầu tiên định nghĩa bits(n) = [b1; b2; : : : ; bdlog2ne] là biểu diễn nhị phân của số nguyên n, với bt2f0;1g là hệ số của cơ số 2t. Do đó chúng ta có n=Pdlog2ne
t=1bt2t. Một trong những lựa chọn thiết kế có thể để tạo Logsparse trong trường hợp LM là 
sparse LM
i =n
sufft:=Pdlog2i1e
=t b2odlog2i1e
t=1[
fig, tức là attend đến các chỉ số vị trí bằng tổng hậu tố của weighted bits(i1), cũng như chính vị trí i. Điều này phục vụ như phiên bản sparse cơ sở của chúng tôi như được hiển thị trong Hình 1(B).

Để khai thác lược đồ này trong khung Combiner, chúng ta có thể định nghĩa dlog2ne supports không chồng lấp, trong đó 
r
i= [suffr]n[suffr+1] với trường hợp biên [suffdlog2i1e+1] =;. Lưu ý rằng để dễ ký hiệu, một số 
r
i rỗng sẽ bị bỏ qua. Trong trường hợp này, tập attention trực tiếp 
0
i bao gồm fig, cũng như fi1g khi i là số chẵn. Phân tích nhân tử như vậy dẫn đến Combiner-Logsparse, như được hiển thị trong Hình 1(E). Từ Hình, chúng ta quan sát rằng tổng cộng chúng ta sẽ có tóm tắt khoảng cho mỗi 2;4;8; : : : ; 2blog2Lc vị trí, dẫn đến tổng Pblog2Lc
t=1bL
2tc hoặc O(L) tóm tắt. Mỗi vị trí i sẽ chọn nhiều nhất O(log(i)) khoảng không chồng lấp để bao phủ support đầy đủ 
i, và do đó, tổng chi phí sẽ là O(LlogL). Chúng tôi để việc thiết kế trường hợp MLM cho Phụ lục B.

4.3 Combiner-Axial
Axial Transformer [20] xây dựng attention dọc theo mỗi trục của dữ liệu đầu vào. Không mất tính tổng quát, chúng tôi tập trung vào trường hợp 2D trong đó chuỗi đầu vào được định hình lại thành một ma trận có kích thước nm=L. Cụ thể, vị trí i trong chuỗi gốc sẽ ở hàng rowi= (i1)divm+ 1 và cột coli= (i1)modm+ 1. Chúng tôi cho thấy cách đơn giản cho phép attention đầy đủ với phân tích nhân tử trên ma trận 2D, do đó Combiner-Axial.

Axial thưa thớt có 
sparse MLM
i =fj:j1i1(modm)g[fj:j1i1(divm)g, và 
sparse LM
i = 
sparse MLM
i\[i], tất cả có nhiều nhất O(m+n) mục cho mỗi i, như được minh họa trong Hình 1(C). Chúng tôi đề xuất một số lược đồ phân tích nhân tử để làm cho nó trở thành một attention với support đầy đủ.

•!LM
axial-vertical : 
0
i= 
sparse LM
i , và 
r
i=fj:jr(modm)g\[icoli], cho r2[m]nfcolig. Như được mô tả trong Hình 2(A), 
r
i tương ứng với cột r phía trên hàng rowi, nơi chúng tôi sử dụng max pooling để

--- TRANG 4 ---
(A) Combiner
-
Axial
-
Vertical
(B) Combiner
-
Axial
-
Horizontal
Ma trận attention
Ma trận attention
Chuỗi được định hình lạiHình 2: Ma trận attention và chuỗi được attend (ví dụ, một hình ảnh 3x4) của các biến thể vertical và horizontal của Combiner-Axial. Xanh dương và vàng tương ứng với attention trực tiếp và địa phương cho vị trí i (tím). Các vị trí được kết nối bằng mũi tên tương ứng với cùng support 
r.

có được abstraction. Để có được abstraction như vậy cho tất cả các vị trí, chúng ta có thể tận dụng toán tử cummax cho mỗi cột để có được prefix-max một cách hiệu quả.

•!LM
axial-horizontal : tương tự như !axial-vertical ngoại trừ mỗi 
r
i tóm tắt hàng r trước hàng rowi và loại trừ coli (Hình 2(B)).

•!LM
axial-rowmajor : 
0
i=fj:j1i1(divm)g\[i], tức là, các phần tử trong cùng hàng được attend trực tiếp, trong khi 
r
i=fj:jr(divm)g\[icoli] nắm bắt các hàng trước hàng rowi. Cấu trúc này tương tự như Combiner-Fixed, ngoại trừ cách tính toán abstraction (và do đó kỳ vọng địa phương). Combiner-Fixed tính toán abstraction chỉ dựa trên r của phân hoạch 
r
i, trong khi !axial-rowmajor phụ thuộc vào cả r và cột coli (Hình 1(F)).

Trong tất cả các trường hợp ở trên, chi phí tương tự như Axial Transformer [20], là O(Lp
L) nếu chúng ta định hình lại chuỗi thành ma trận 2D với n; m =O(p
L). Chúng tôi hoãn trường hợp MLM đến Phụ lục C.

4.4 Combiner-Learnable
Lấy cảm hứng từ Reformer [21] và Routing Transformer [22], chúng ta cũng có thể học kế hoạch phân tích nhân tử ! từ dữ liệu. Chúng tôi minh họa điều này với Routing Transformer và cung cấp một cách để cho phép attention đầy đủ trong Routing Transformer theo nguyên tắc Combiner.

Đối với một lớp cụ thể, giả sử chúng ta có một vùng đã học rời nhau (hoặc cluster trong Routing Transformer) f
rgn
r=1 trong đó [r
r= [L]. Trong Routing Transformer, chúng ta đơn giản có 
sparse MLM
i = 
ri trong đó 
ri biểu thị vùng mà vị trí i thuộc về. Để định nghĩa phân tích nhân tử Combiner, chúng ta cho !routing MLM : 
0
i= 
ri;
r
i= 
rn
0
i;8r2[ni]: (11)

Lưu ý rằng ni=n (tức là, số lượng clusters đã học) cho tất cả các vị trí. Phân tích nhân tử ở trên chỉ có thể hoạt động cho MLM. LM yêu cầu định nghĩa sau:

!routing LM : 
0
i= 
ri\[i];
r
i=

rn
0
i
\[i];8r2[ni]: (12)

Nói chung, cả LM và MLM đều có thể có chi phí dưới bậc hai khi n=O(p
L). Tuy nhiên, các biến thể routing (bao gồm Routing Transformer) yêu cầu thao tác gather, có thể chậm trên TPUs (xem minh họa trong Phụ lục D).

5 Đánh giá Thực nghiệm
Chúng tôi đánh giá Combiner với các mẫu attention đầy đủ khác nhau trên cả các tác vụ mô hình hóa chuỗi tự hồi quy và hai chiều, bao gồm một loạt rộng dữ liệu đầu vào từ hình ảnh đến văn bản. Tất cả các tác vụ được xem xét đều liên quan đến các chuỗi dài lên đến 12,000 về độ dài, một số trong đó ngăn cản khả năng áp dụng của transformer vanilla. Chúng tôi so sánh Combiner với các Transformers state-of-the-art. Chúng tôi cũng thực hiện một loạt nghiên cứu ablation trong đó tất cả các mô hình được so sánh sử dụng chính xác cùng kiến trúc chỉ khác nhau ở module attention, tránh các trick riêng lẻ được sử dụng trong các công trình gốc (ví dụ, sử dụng cả mẫu có thể học và cố định trong Routing Transformer [22]). Chi tiết để tái tạo tất cả các kết quả thực nghiệm có thể được tìm thấy trong Phụ lục E.

--- TRANG 5 ---
Bảng 1: Kết quả ablation trong Bits per Dimension (Bits/Dim) trên CIFAR-10 và ImageNet-64.
Mô hình Lớp CIFAR-10 ImageNet-64
Reformer [21] 6 - 3.740
Performer [28] 6 3.335 3.719
Logsparse [18] 6 4.253 4.351
Combiner-Logsparse (Ours) 6 3.366 3.795
Fixed [14] 6 3.408 3.696
Combiner-Fixed (Ours) 6 3.321 3.654
Axial [20] 6 3.666 4.032
Combiner-Axial (Ours) 6 3.050 3.585
Combiner-Mixture (Ours) 6 3.040 3.585
Reformer [21] 12 - 3.710
Performer [28] 12 3.310 3.636
Routing Transformer [22] 12 2.950 -
Combiner-Mixture (Ours) 12 2.885 3.504

5.1 Mô hình Chuỗi Tự hồi quy
Trong phần phụ này, chúng tôi đầu tiên thực hiện ước lượng mật độ trên văn bản và hình ảnh sử dụng Combiner.

5.1.1 Mô hình Ngôn ngữ
Bảng 2: Perplexity LM trên Wiki-40B (Chính).
Mô hình Perplexity
Transformer-2k [1] 17.26
Performer-2k [28] 19.66
Routing-2k [22] 20.85
Fixed-2k [14] 18.04
Combiner-Fixed-2k (Ours) 17.70
Axial-2k [20] 20.82
Combiner-Axial-2k (Ours) 17.56
Combiner-Fixed-8k (Ours) 16.60
Combiner-Axial-8k (Ours) 16.49

Bảng 3: Perplexity LM trên Wiki-40B (Ablation).
Mô hình Perplexity
Transformer-2k [1] 17.26
Combiner-DeepSets-Max-8k (Ours) 16.29
Combiner-DeepSets-Mean-8k (Ours) 16.48
Combiner-Max-8k (Ours) 16.60
Combiner-Mean-8k (Ours) 16.54

Đối với mô hình ngôn ngữ, chúng tôi tập trung vào dataset Wiki-40B-En [34], bao gồm các trang Wikipedia sạch bằng tiếng Anh. Chúng tôi sử dụng mô hình sentence piece với kích thước từ vựng 32K để tokenize văn bản và đo perplexity ở mức sentence piece. Để đảm bảo so sánh công bằng, tất cả các mô hình được so sánh lại có cùng số lượng lớp và kích thước ẩn, và được triển khai dưới cùng một codebase.

Bảng 2 cho thấy kết quả của việc so sánh. Như chúng ta có thể thấy, dưới độ dài chuỗi 2k, các biến thể Combiner liên tục tốt hơn các baseline tương ứng của chúng, và rất gần với Transformer tiêu chuẩn. Khi độ dài chuỗi đi đến 8k, Transformer tiêu chuẩn hết bộ nhớ, trong khi Combiner tiếp tục đạt được perplexity cải thiện, vượt qua kết quả của Transformer-2k. Nếu chúng ta sử dụng thêm DeepSets để tính toán các số hạng tóm tắt q
r
i và k
r
i, chúng ta có thể đạt được perplexity thấp hơn như được hiển thị trong Bảng 3.

5.1.2 Mô hình Tạo sinh Hình ảnh
CIFAR-10. Chúng tôi đầu tiên thực hiện một kiểm tra tỉnh táo trong đó chúng tôi so sánh các baseline sparse attention với Combiner với attention đầy đủ dưới cùng kiến trúc trên dataset CIFAR-10. Độ dài chuỗi là 3072. Đối với tất cả các phương pháp, chúng tôi sử dụng cùng transformer 6 lớp với 8 đầu attention và 512 chiều embedding. Chúng tôi huấn luyện tất cả các mô hình trong 500k iterations sử dụng batch size 32 trên TPU v2. Như được hiển thị trong Bảng 1, cho cùng kiến trúc mô hình, Combiner-X hoạt động tốt hơn đáng kể so với mô hình cơ sở X dưới metric bits per dimension (BPD) trên 10,000 hình ảnh test. Đặc biệt, Combiner giảm đáng kể BPD 0.887, 0.087, và 0.626 so với các mô hình cơ sở Logsparse, Fixed và Axial, tương ứng. Lưu ý rằng tất cả các biến thể Combiner đạt được hiệu suất tốt hơn so với tốt nhất của các mô hình cơ sở. Điều này chứng minh lợi thế của Combiner so với các baseline cho cùng kiến trúc 6 lớp. Chúng tôi quan sát một xu hướng tương tự dưới kiến trúc 12 lớp.

--- TRANG 6 ---
Bảng 4: Bits per Dimension (Bits/Dim) trên CIFAR-10 và ImageNet-64.
CIFAR-10 Bits/Dim
PixelCNN [15] 3.03
PixelCNN++ [36] 2.92
Image Transformer [16] 2.90
PixelSNAIL [37] 2.85
Sparse Transformer [14] 2.80
Combiner-Axial (ours) 2.77

ImageNet 64x64 Bits/Dim
PixelCNN [15] 3.57
Parallel Multiscale [38] 3.70
Glow [39] 3.81
SPN [40] 3.52
Sparse Transformer [14] 3.44
Axial Transformer [20] 3.44
Routing Transformer [22] 3.43
Combiner-Axial (ours) 3.42

Theo kiến trúc 128 lớp trong Child et al. [14], chúng tôi áp dụng Combiner-Axial và đạt được hiệu suất state-of-the-art, 2.77 BPD trên CIFAR-10, như được liệt kê trong Bảng 4. Chúng tôi chạy tất cả các mô hình trong Bảng 4 mà không có data augmentation [35].

ImageNet-64. Chúng tôi cũng đánh giá hiệu suất dưới cài đặt tự hồi quy trên ImageNet-64, trong đó độ dài chuỗi là 12,288. Chúng tôi đầu tiên thực hiện cùng phân tích như CIFAR-10 và so sánh Combiner-X với các baseline sử dụng cùng kiến trúc mô hình. Như được hiển thị trong Bảng 1, Combiner liên tục vượt trội so với các baseline với cùng mẫu attention. Chúng tôi áp dụng thêm Combiner-Axial vào Transformer 30 lớp, đạt được hiệu suất state-of-the-art trên ước lượng mật độ trên ImageNet-64, chứng minh tính hiệu quả của attention đầy đủ đạt được bởi Combiner.

5.2 Mô hình Chuỗi Hai chiều
Bên cạnh các tác vụ tự hồi quy, chúng tôi cũng đánh giá Combiner trên một tập hợp các tác vụ hai chiều tiêu chuẩn để cho thấy khả năng áp dụng chung của phương pháp.

5.2.1 Long-Range Arena
Long-Range Arena (LRA) là một benchmark thống nhất [31] để thăm dò khả năng của các transformers hiệu quả trong việc xử lý các chuỗi dài. Chúng tôi đánh giá các mô hình của chúng tôi trên năm tác vụ từ LRA: ListOps, Text Classification, Retrieval, Image Classification và Pathfinder. Tất cả các tác vụ đều là phân loại đa lớp cấp chuỗi. Vui lòng tham khảo bài báo LRA gốc để biết thêm chi tiết.

Bảng 5: Kết quả thực nghiệm trên benchmark Long-Range Arena.
Mô hình ListOps Text Retrieval Image Pathfinder Avg
Chance 10.00 50.00 50.00 10.00 50.00 34.00
Transformer 36.38 64.27 57.46 42.44 88.81 57.87
Local Attention 15.95 52.98 53.39 41.46 84.64 49.68
Sparse TRans. 35.78 63.58 59.59 44.24 83.90 57.42
Longformer 36.03 62.85 56.89 42.22 86.68 56.93
Linformer 35.49 53.94 52.27 38.56 86.17 53.28
Reformer 36.30 56.10 53.40 38.07 79.18 52.61
Sinkhorn Trans. 34.20 61.20 53.83 41.23 73.36 52.76
Synthesizer 36.50 61.68 54.67 41.61 81.61 55.21
BigBird 37.08 64.02 59.29 40.83 86.75 57.59
Linear Trans. 17.15 65.90 53.09 42.34 88.13 53.32
Performer 36.00 65.40 53.82 42.77 88.76 57.35
Combiner-Fixed 36.65 64.99 59.81 41.67 88.59 58.34
Combiner-Axial 36.15 64.36 56.10 41.33 88.43 57.27

Như được hiển thị trong Bảng 5, Combiner có thể phù hợp với hiệu suất của vanilla Transformer và đạt được hiệu suất thậm chí tốt hơn trong một số tác vụ. Theo giao thức của LRA, tất cả các phương pháp sử dụng cùng kiến trúc và siêu tham số để so sánh có thể kiểm soát. Chúng tôi sử dụng các con số từ Tay et al. [31] cho tất cả các tác vụ ngoại trừ Pathfinder. Vì chúng tôi không thể tái tạo kết quả Pathfinder gốc sử dụng thiết lập mặc định trong repository Github LRA, chúng tôi chạy lại tất cả các baseline sử dụng cấu hình Pathfinder-inter để tiến hành so sánh công bằng. Tuy nhiên, vì benchmark vẫn có quy mô nhỏ và trang web chính thức LRA không khuyến khích việc tinh chỉnh siêu tham số, Bảng 5 nên được coi như kết quả cho test bench của tính biểu đạt so với vanilla Transformer.

--- TRANG 7 ---
Bảng 6: Perplexity MLM trên dataset C4.
Mô hình Perplexity
Transformer-2k [1] 4.552
BigBird-2k [41] 4.696
Performer-2k [28] 10.940
Fixed-2k [14] 5.279
Combiner-Fixed-2k (Ours) 5.170
Axial-2k [20] 5.370
Combiner-Axial-2k (Ours) 4.809
Routing-2k [22] 6.703
Combiner-Routing-2k (Ours) 6.539
BigBird-8k [41] 4.542
Combiner-Axial-8k (Ours) 4.190
Combiner-Fixed-8k (Ours) 4.139

210211212213214
Độ dài chuỗi272829210211212213Mili giây / Iteration
210211212213214
Độ dài chuỗi20212223Bộ nhớ (GB)
Vanilla Transformer
PerformerBigBird
Combiner-AxialCombiner-Fixed
Sparse-AxialSparse-Fixed
Combiner-MixtureHình 3: Chúng tôi đo thời gian chạy suy luận và sử dụng bộ nhớ cho tám mô hình. Nhìn chung Combiner có tốc độ tương tự với Performer và đối tác sparse của nó nhưng Vanilla Transformer nhanh chóng hết bộ nhớ khi độ dài chuỗi tăng.

5.2.2 Mô hình Ngôn ngữ Có mặt nạ
Là thành phần cốt lõi của pretraining ngôn ngữ BERT [5], mô hình ngôn ngữ có mặt nạ (MLM) đề cập đến tác vụ tái tạo các token được che giấu ngẫu nhiên trong chuỗi đầu vào. Như với tác vụ LM, chúng tôi sử dụng perplexity làm metric chính, tương quan tương đối tốt với hiệu suất tác vụ downstream. Cụ thể, chúng tôi sử dụng dataset C4 quy mô lớn [8] để huấn luyện và đánh giá, và xem xét các độ dài chuỗi khác nhau. Theo thiết lập BERT gốc, chúng tôi che giấu 15% tokens trong mỗi chuỗi đầu vào. So sánh được tóm tắt trong Bảng 6. Tương tự như kết quả LM, các biến thể Combiner khác nhau liên tục vượt trội so với các baseline tương ứng của chúng dưới độ dài chuỗi 2k. Tuy nhiên, ngoài Transformer tiêu chuẩn, Combiner-2k cũng thua BigBird-2k. Chúng tôi đoán rằng điều này liên quan đến thiết kế đặc biệt trong BigBird chẳng hạn như tất cả các token luôn có thể attend đến token <cls> trực tiếp, chỉ áp dụng được trong các vấn đề không nguyên nhân. Tuy nhiên, khi chúng tôi tăng thêm độ dài chuỗi đến 8k, Transformer tiêu chuẩn gặp vấn đề OOM, trong khi Combiner không chỉ vượt trội so với BigBird mà còn vượt qua đáng kể Transformer-2k. Điều này gợi ý rằng Combiner có thể thực sự hưởng lợi từ việc mở rộng học tập đến độ dài chuỗi dài hơn.

5.3 Thời gian Chạy và Sử dụng Bộ nhớ của Combiner
Ở đây chúng tôi đánh giá thời gian chạy suy luận và sử dụng bộ nhớ của năm baseline – Transformer, Performer, BigBird, Sparse-Fixed và Sparse-Axial, cũng như ba biến thể của Combiner– Combiner-Fixed, Combiner-Axial và Combiner-Mixture. Chúng tôi chạy suy luận của tất cả các mô hình trên TPU v3-16 (16 cores x 16GB) với batch size 16, và chúng tôi test các chuỗi có độ dài từ 210 đến 214. Như được hiển thị trong Hình 3, các thực hiện Combiner đạt được thời gian chạy và sử dụng bộ nhớ có thể so sánh với đối tác sparse của chúng và Performer. Lưu ý Combiner đạt được hiệu suất thực nghiệm tốt hơn nhiều so với các mô hình sparse và Performer. Combiner-Mixture có cùng độ phức tạp tiệm cận với Combiner-Fixed và Combiner-Axial, tuy nhiên, vì nó yêu cầu chạy hai kế hoạch phân hoạch, nó chậm hơn Combiner-Fixed và Combiner-Axial. Do thao tác gather được yêu cầu bởi random attention không rất thân thiện với TPU/GPU, BigBird rất tốn kém về mặt tính toán. Và mô hình Transformer nhanh chóng hết bộ nhớ khi độ dài chuỗi tăng.

6 Kết luận
Lấy cảm hứng từ quan điểm kỳ vọng có điều kiện của cơ chế attention, chúng tôi đề xuất Combiner, một thay thế trực tiếp của module attention. Bằng cách giới thiệu phân tách có cấu trúc vào xác suất có điều kiện, Combiner đạt được khả năng attention đầy đủ trong khi duy trì chi phí tính toán và bộ nhớ dưới bậc hai. Chúng tôi thực hiện một số biến thể Combiner chuyển đổi các sparse transformers hiện có thành attention đầy đủ. Combiner đạt được hiệu suất state-of-the-art trên cả các tác vụ tự hồi quy và hai chiều cho mô hình hóa hình ảnh và văn bản, cho thấy lợi ích trong cả hiệu quả mô hình và hiệu quả thời gian chạy. Công việc tương lai bao gồm thiết kế mẫu phân tích nhân tử bổ sung, cũng như các ứng dụng của Combiner trong các lĩnh vực như tin sinh học và giọng nói.

--- TRANG 8 ---
Lời cảm ơn và Tiết lộ Tài trợ
Chúng tôi muốn cảm ơn Richard Song và David Dohan vì đã giúp đỡ giới thiệu codebase Performer và cấu hình thực nghiệm, Yi Tay và Mostafa Dehghani vì làm rõ về benchmark LRA, James Lee-Thorp, Joshua Ainslie, và Ilya Eckstein vì làm rõ về kết quả thực nghiệm LRA của họ, Adams Yu vì thực hiện đánh giá paper nội bộ và đề xuất hữu ích. Chúng tôi cũng biết ơn sự hỗ trợ của DARPA dưới Nos. HR00112190039 (TAMI), N660011924033 (MCS); ARO dưới Nos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF dưới Nos. OAC-1835598 (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID), NIH dưới No. R56LM013365; Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan Chase, Docomo, Hitachi, Intel, JD.com, KDDI, NVIDIA, Dell, Toshiba, Visa, và UnitedHealth Group. Hongyu Ren được hỗ trợ bởi Masason Foundation Fellowship và Apple PhD Fellowship. Jure Leskovec là một nhà điều tra Chan Zuckerberg Biohub.

Tài liệu tham khảo
[1]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

[2]Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, et al. The best of both worlds: Combining recent advances in neural machine translation. In Annual Meeting of the Association for Computational Linguistics (ACL), 2018.

[3]Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

[4]Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019.

[6]Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations (ICLR), 2020.

[7]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[8]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.

[9]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.

[10] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual embedding of source code. In International Conference on Machine Learning (ICML), 2020.

[11] Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.

--- TRANG 9 ---
[12] Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497, 2020.

[13] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, AM Dai, MD Hoffman, and D Eck. Music transformer: Generating music with long-term structure (2018). In International Conference on Learning Representations (ICLR), 2019.

[14] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

[15] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International Conference on Machine Learning (ICML), 2016.

[16] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning (ICML), 2018.

[17] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations (ICLR), 2020.

[18] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

[19] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In International Conference on Computer Vision (ICCV), 2019.

[20] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.

[21] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations (ICLR), 2020.

[22] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021.

[23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Annual Meeting of the Association for Computational Linguistics (ACL), 2019.

[24] Zhuoran Shen, Mingyuan Zhang, Shuai Yi, Junjie Yan, and Haiyu Zhao. Factorized attention: Self-attention with linear complexities. CoRR, 2018.

[25] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

[26] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning (ICML), 2020.

[27] Si Si, Cho-Jui Hsieh, and Inderjit S Dhillon. Memory efficient kernel approximation. The Journal of Machine Learning Research, 2017.

[28] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations (ICLR), 2021.

[29] Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NeurIPS), 2007.

--- TRANG 10 ---
[30] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random feature attention. In International Conference on Learning Representations (ICLR), 2021.

[31] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations (ICLR), 2021.

[32] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

[33] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck: A high-rank rnn language model. In International Conference on Learning Representations (ICLR), 2018.

[34] Mandy Guo, Zihang Dai, Denny Vrande ˇci´c, and Rami Al-Rfou. Wiki-40b: Multilingual language model dataset. In Proceedings of The 12th Language Resources and Evaluation Conference, 2020.

[35] Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, and Ilya Sutskever. Distribution augmentation for generative modeling. In International Conference on Machine Learning (ICML), 2020.

[36] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations (ICLR), 2017.

[37] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In International Conference on Machine Learning (ICML), 2018.

[38] Scott Reed, Aäron Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Yutian Chen, Dan Belov, and Nando Freitas. Parallel multiscale autoregressive density estimation. In International Conference on Machine Learning (ICML), 2017.

[39] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

[40] Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In International Conference on Learning Representations (ICLR), 2019.

[41] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

[42] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. o(n) connections are expressive enough: Universal approximability of sparse transformers. arXiv preprint arXiv:2006.04862, 2020.

--- TRANG 11 ---
Phụ lục

A Xấp xỉ Phổ quát
Ở đây chúng tôi cho thấy trong Mệnh đề 1 rằng Combiner-X của chúng tôi đạt được tính chất xấp xỉ phổ quát [42] nếu sparse transformer X đạt được tính chất xấp xỉ phổ quát. Đối với các phương pháp như BigBird [41], chúng duy trì tính chất xấp xỉ phổ quát sử dụng các token toàn cục (CLS). Tuy nhiên, attention toàn cục làm cho nó khó được áp dụng cho mô hình tự hồi quy một chiều (LM). Bên cạnh đó, random attention yêu cầu thao tác gather, làm cho nó rất chậm trên phần cứng dense như TPUs (Hình 3).

Mệnh đề 1. Combiner được đề xuất sẽ không phá vỡ tính chất xấp xỉ phổ quát của các sparse transformers gốc.

Cụ thể, chúng tôi xem xét lớp hàm được xây dựng bằng cách xếp chồng khối attention với một mạng kết nối đầy đủ hai lớp. Chính thức, theo ký hiệu trong [42] chúng ta có khối như

SAttn (X) = X+MultiHeadAttn (X); (13)
Z=SAttn (X) +relu (SAttnW1)W2; (14)

biểu thị attention h-head với X2RLd,W12Rdr, và W22Rrd. Lớp hàm được ký hiệu như

STH;r:=fX!t(X+E)jtlà một composition của khối (13) ; (15)
Elà position embedding có thể huấn luyệng: (16)

Yun et al. [42] cho thấy rằng lớp hàm (15) vẫn là xấp xỉ phổ quát w.r.t. norm được định nghĩa như dp(f; g) :=R
kf(X)g(X)kp
pdX1=p
với softmax trong (1) và một số yêu cầu về các mẫu sparsity trong lược đồ attention.

B Combiner-Logsparse trong Trường hợp MLM
Ở đây chúng tôi mở rộng Combiner-logsparse được giới thiệu trong phần 4.2 đến trường hợp MLM.

Bên cạnh dlog2ie supports không chồng lấp trong trường hợp LM, chúng ta có thể định nghĩa thêm dlog2ie supports không chồng lấp để attend đến các token sau token hiện tại trong chuỗi. Chúng tôi minh họa lựa chọn thiết kế này trong hình 4.

C Combiner-Axial trong Trường hợp MLM
Bên cạnh !LM
axial-vertical ,!LM
axial-horizontal và!LM
axial-rowmajor được giới thiệu trong phần 4.3, ở đây chúng tôi giới thiệu cách chúng tôi mở rộng ba mô hình này đến trường hợp MLM.

•!MLM
axial-vertical : 
0
i= 
sparse MLM
i =fj:j1i1(modm)g[fj:j1i1(divm)g, và 
r
i=fj:jr(modm)g, cho r2[m]nfcolig. Như được mô tả trong Hình 2(A), 
r
i tương ứng với cột r phía trên hàng rowi, nơi chúng tôi sử dụng max pooling để có được abstraction. Để có được abstraction như vậy cho tất cả các vị trí, chúng ta có thể tận dụng toán tử cummax cho mỗi cột để có được prefix-max một cách hiệu quả.

•!MLM
axial-horizontal : tương tự như !MLM
axial-vertical ngoại trừ mỗi 
r
i tóm tắt tất cả các hàng r và loại trừ coli.

•!MLM
axial-rowmajor : 
0
i=fj:j1i1(divm)g, tức là, các phần tử trong cùng hàng được attend trực tiếp, trong khi 
r
i=fj:jr(divm)g cho r2[n]nfrowige nắm bắt tất cả các hàng ngoại trừ hàng rowi.

Dễ thấy rằng độ phức tạp vẫn là O(Lp
L) nếu n; m =O(p
L).

D Combiner-Learnable
Như đã thảo luận trong phần 4.4. chúng tôi thiết kế Combiner-learnable như một phần mở rộng cho routing transformer [22], học cách cluster các token. Mỗi token trong routing transformer chỉ attend đến các token trong cùng cluster. Như được hiển thị trong hình 4, Combiner-learnable của chúng tôi kết hợp kỳ vọng trực tiếp với kỳ vọng địa phương (token vàng), mỗi cái tóm tắt một cluster (đỏ, xanh dương hoặc xanh lá).

--- TRANG 12 ---
Hình 4: Trái: Combiner-logsparse trong trường hợp MLM. Phải: Combiner-Learnable. Theo routing transformer [22], chúng tôi áp dụng nguyên tắc combiner, để chúng ta có thể đạt được attention đầy đủ trong mỗi head với độ phức tạp giống hệt với routing transformer.

E Chi tiết Thực nghiệm
E.1 CIFAR-10
Ở đây chúng tôi liệt kê các siêu tham số chúng tôi đã sử dụng trên dataset CIFAR-10. Các thực nghiệm của chúng tôi bao gồm (1) một nghiên cứu ablation, trong đó tất cả các mô hình chia sẻ chính xác cùng kiến trúc; và (2) kết quả chính, trong đó Combiner của chúng tôi đạt được kết quả state-of-the-art dưới cài đặt không cho phép data augmentation.

Đối với nghiên cứu ablation, kích thước embedding và hidden là 512. Chúng tôi sử dụng 8 đầu attention trong mỗi lớp với tổng cộng 6 lớp transformer. Chúng tôi huấn luyện tất cả các mô hình trong 400,000 bước với learning rate 1e-3 và batch size 32. Đối với kết quả chính, chúng tôi sử dụng cùng kiến trúc như được giới thiệu trong Child et al. [14], và chúng tôi huấn luyện Combiner-Axial của chúng tôi trong 1,200,000 bước với lịch learning rate cosine. Chúng tôi chạy lại kết quả chính 3 lần và độ lệch chuẩn là 0.003.

E.2 ImageNet-64
Về chi tiết của ImageNet-64, chúng tôi sử dụng cùng thiết lập với CIFAR-10, bao gồm nghiên cứu ablation và kết quả chính. Kiến trúc được sử dụng trong nghiên cứu ablation giống hệt với cái chúng tôi đã sử dụng trong CIFAR-10. Đối với kết quả chính của Combiner-Axial, chúng tôi đã sử dụng kiến trúc 30 lớp với 768 kích thước hidden và chiều embedding. Chúng tôi huấn luyện kiến trúc này trong 1,200,000 bước với lịch learning rate cosine. Chúng tôi cũng chạy lại kết quả chính 3 lần và độ lệch chuẩn là 0.005.

E.3 Mô hình Ngôn ngữ Wiki-40B
Mục đích chính của thực nghiệm này không phải là theo đuổi hiệu suất state-of-the-art, vì nói chung, càng nhiều tham số/dữ liệu, perplexity càng tốt cho mô hình ngôn ngữ. Vì vậy thay vào đó, chúng tôi để tất cả các phương pháp có cùng backbone mạng neural, trong khi chỉ thay đổi các triển khai attention để so sánh hiệu quả của chúng. Điều này tương tự như tinh thần của nghiên cứu ablation trong CIFAR-10 và ImageNet-64.

Cụ thể, chúng tôi sử dụng kích thước word embedding và hidden là 768 cho tất cả các lớp. Chúng tôi sử dụng 12 đầu attention trong mỗi lớp, với tổng cộng 12 lớp transformer. Chúng tôi sử dụng kiến trúc Pre-Norm, và các lớp MLP có kích thước hidden bằng 4768. Độ dài chuỗi tối đa có thể thay đổi trong f2048;8192g, phụ thuộc vào giới hạn bộ nhớ của mỗi phương pháp. Tất cả các phương pháp được huấn luyện trong 125,000 cập nhật gradient ngẫu nhiên, với batch size bằng 128. Chúng tôi cũng cho phép lịch learning rate cosine, với 10,000 bước warm-up. Optimizer là Adam với gradient clipping.

--- TRANG 13 ---
E.4 Benchmark LRA
Chúng tôi chủ yếu tuân theo hướng dẫn của LRA, trong đó tất cả các mô hình nên sử dụng số lượng tham số gần giống nhau và cùng siêu tham số như batchsize, số iterations, v.v.. Chúng tôi đã cố gắng hết sức để tái tạo kết quả thực nghiệm sử dụng mã trong https://github.com/google-research/long-range-arena , và chúng tôi thấy rằng chúng tôi không thể tái tạo kết quả pathfinder-32. Chúng tôi đã liên lạc với các tác giả nhưng không giải quyết được vấn đề. Vì vậy thay vào đó, chúng tôi chạy lại tất cả các baseline sử dụng cùng cấu hình mạng, trên thiết lập pathfinder-32-inter. Chúng tôi thấy một số phương pháp ưa thích pooling 'MEAN' để có được biểu diễn chuỗi, trong khi những phương pháp khác ưa thích pooling 'CLS'. Vì vậy chúng tôi thử cả hai cho mỗi phương pháp, và báo cáo kết quả tốt nhất.

E.5 Mô hình Ngôn ngữ Có mặt nạ C4
Tương tự như mục đích của phần E.3, chúng tôi thực hiện tác vụ mô hình ngôn ngữ có mặt nạ trên dataset C4, thường được sử dụng cho pretraining BERT. Vì metric perplexity tương quan tốt với hiệu suất tác vụ downstream, do đó chúng tôi thực hiện các thực nghiệm có kiểm soát với tất cả các phương pháp sử dụng cùng kiến trúc mạng.

Kiến trúc được sử dụng và các siêu tham số gần như giống với phần E.3, ngoại trừ chúng tôi có số lượng segments tối đa bằng 2.
