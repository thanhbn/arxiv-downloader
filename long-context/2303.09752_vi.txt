# 2303.09752.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2303.09752.pdf
# Kích thước tệp: 982420 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
COLT5: Transformers Tầm Xa Nhanh Hơn với Tính Toán Có Điều Kiện
Joshua Ainslie∗, Tao Lei, Michiel de Jong, Santiago Ontañón
Siddhartha Brahma ,Yury Zemlyanskiy ,David Uthus ,Mandy Guo
James Lee-Thorp ,Yi Tay ,Yun-Hsuan Sung ,Sumit Sanghai
Google Research
Tóm tắt
Nhiều tác vụ xử lý ngôn ngữ tự nhiên được hưởng lợi
từ đầu vào dài, nhưng xử lý tài liệu dài với
Transformers rất tốn kém -- không chỉ do độ phức
tạp attention bậc hai mà còn từ việc áp dụng các lớp
feedforward và projection cho mọi token. Tuy nhiên,
không phải tất cả token đều quan trọng như nhau,
đặc biệt đối với tài liệu dài hơn. Chúng tôi đề xuất
COLT5, một mô hình Transformer đầu vào dài được
xây dựng trên trực giác này bằng cách sử dụng tính
toán có điều kiện, dành nhiều tài nguyên hơn cho
các token quan trọng trong cả lớp feedforward và
attention. Chúng tôi chỉ ra rằng COLT5 đạt được
hiệu suất mạnh hơn so với LONG T5 với quá trình
huấn luyện và suy luận nhanh hơn nhiều, đạt SOTA
trên benchmark đầu vào dài SCROLLS. Hơn nữa,
COLT5 có thể hiệu quả và khả thi sử dụng các đầu
vào cực kỳ dài, cho thấy mức tăng mạnh lên đến
độ dài đầu vào 64k.

1 Giới thiệu
Nhiều tác vụ xử lý ngôn ngữ tự nhiên, như tóm tắt
(Cohan et al., 2018) hoặc hỏi đáp trên tài liệu dài
(Joshi et al., 2017), yêu cầu các mô hình machine
learning mã hóa văn bản dạng dài. Xử lý tài liệu dài
với mô hình Transformer tốn kém về mặt tính toán,
cả do chi phí attention tăng bậc hai theo độ dài đầu
vào và do các lớp feedforward và attention projection
phải được áp dụng cho mỗi token đầu vào.

Trong vài năm qua, nhiều phương pháp "Transformer
hiệu quả" đã được đề xuất để giảm chi phí của cơ chế
attention trên đầu vào dài (Child et al., 2019; Ainslie
et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020;
Wang et al., 2020; Tay et al., 2021; Guo et al., 2022).
Tuy nhiên, đặc biệt đối với các mô hình lớn hơn, các
lớp feedforward và projection thực sự chiếm phần
lớn gánh nặng tính toán và có thể làm cho việc xử lý
đầu vào dài trở nên không khả thi.

∗Đóng góp của tác giả được nêu trong Phụ lục A. Tác giả
liên hệ: jainslie@google.com.

Hình 1: Tổng quan về một lớp Transformer COLT5
với tính toán có điều kiện. Tất cả token được xử lý
bởi các lớp attention và MLP nhẹ, trong khi q token
query được định tuyến thực hiện attention nặng hơn
trên v token key-value được định tuyến và m token
được xử lý bởi MLP nặng hơn.

Bài báo này trình bày COLT5 (Conditional
LongT5), một họ mô hình mới, được xây dựng trên
LONGT5 (Guo et al., 2022), cho phép xử lý nhanh
các đầu vào dài bằng cách kết hợp các cải tiến kiến
trúc cho cả lớp attention và feedforward. COLT5
dựa trên trực giác rằng một số token quan trọng hơn
những token khác, và chúng ta có thể đạt được chất
lượng tốt hơn với chi phí thấp hơn bằng cách dành
nhiều tính toán hơn cho các token quan trọng. Hơn
nữa, tỷ lệ token quan trọng có khả năng giảm theo
độ dài tài liệu, cho phép xử lý khả thi các tài liệu dài.

Cụ thể, COLT5 chia mỗi lớp feedforward và mỗi
lớp attention thành một nhánh nhẹ arXiv:2303.09752v3  [cs.CL]  24 Oct 2023

--- TRANG 2 ---
200 400 600 800424446
BLXL
BLXL
Thời gian mỗi mẫu (ms)Hiệu suất trung bình Suy luận
0 500 1,0001,5002,0002,500424446
BLXL
BLXL
Thời gian mỗi mẫu (ms)Fine-tuning
LONG T5 COLT5

Hình 2: COLT5 đạt được hiệu suất mạnh hơn so với LONG T5 ở bất kỳ tốc độ nào. Hiệu suất trung bình trên
tất cả tập dữ liệu theo thời gian suy luận và fine-tuning mỗi mẫu (ms) cho các mô hình LONGT5 và COLT5 Base, Large, và
XL. LONG T5 không sử dụng MQA, nhưng chúng tôi báo cáo tốc độ như thể nó có để tạo baseline bảo thủ.

được áp dụng cho tất cả token và một nhánh nặng
được áp dụng cho một tập hợp các token quan trọng,
được chọn đặc biệt cho đầu vào và thành phần đó.
Nhánh feedforward nhẹ có chiều ẩn thấp hơn so với
LONG T5 tiêu chuẩn trong khi nhánh feedforward
nặng có chiều ẩn cao hơn. Nhánh attention nhẹ có
ít head hơn và chỉ áp dụng attention cục bộ, trong
khi nhánh attention nặng thực hiện attention đầy đủ
trên một tập hợp token quan trọng được chọn riêng
biệt khác. Hình 1 cung cấp tổng quan về cơ chế
điều kiện COLT5.

Cuối cùng, COLT5 cũng bao gồm hai sửa đổi khác
cho kiến trúc LONG T5. COLT5 thêm multi-query
cross-attention (Shazeer, 2019), tăng tốc đáng kể
quá trình suy luận. COLT5 cũng sử dụng mục tiêu
pre-training UL2 (Tay et al., 2022), mà chúng tôi
chứng minh cho phép học trong ngữ cảnh trên đầu
vào dài.

Chúng tôi chỉ ra rằng COLT5 thực hiện fine-tuning
và suy luận nhanh hơn nhiều với chất lượng mô hình
tương tự hoặc tốt hơn, cải thiện so với LONGT5 trên
tập dữ liệu tóm tắt arXiv (Cohan et al., 2018) và hỏi
đáp TriviaQA (Joshi et al., 2017) và đạt SOTA trên
benchmark SCROLLS (Shaham et al., 2022). Hơn
nữa, COLT5 đạt được thêm mức tăng về chất lượng
và tốc độ cho các tác vụ với đầu vào cực kỳ dài
(64k token), với tỷ lệ token "focus" tăng ít hơn tuyến
tính.

2 Nền tảng
FLOPs Transformer COLT5 theo một dòng công
việc mở rộng trong việc cố gắng giảm chi phí tính
toán của các mô hình Transformer, đặc biệt trên đầu
vào dài. Gánh nặng tính toán của các mô hình
Transformer có nhiều thành phần riêng biệt, và các
phương pháp khác nhau tập trung vào việc giảm chi
phí của các thành phần khác nhau. Vì lý do đó, có
ích khi bắt đầu bằng việc cung cấp phân tích chi phí
tính toán của các thành phần Transformer. Bảng 1
cho thấy FLOPs¹ cho mỗi thành phần của một lớp
encoder Transformer (Kaplan et al., 2020).

Thành phần Lớp Encoder     Flops
Tính toán vanilla self-attention    2n²d
Projection QKV và output attention    4nd²
Lớp Feedforward    8nd²
Tính toán attention cục bộ LONG T5    2nwd
Tính toán attention toàn cục LONG T5    n²d/8

Bảng 1: Chi phí tính toán của các thành phần lớp
encoder transformer được đo bằng FLOPs. n là độ
dài đầu vào, d là chiều của mô hình, và w là kích
thước cửa sổ attention cục bộ.

Sparse attention Thách thức đầu tiên của việc áp
dụng Transformer cho đầu vào dài là FLOPs của
cơ chế self-attention tăng bậc hai theo độ dài đầu
vào, trở nên không khả thi cho đầu vào dài. Một
khối lượng công việc lớn tập trung vào việc giảm
chi phí self-attention, hạn chế attention giữa một
tập con đầu vào (Child et al., 2019; Ainslie et al.,
2020; Beltagy et al., 2020; Zaheer et al., 2020;
Wang et al., 2020; Guo et al., 2022) hoặc cho một
tập con các lớp (Zemlyanskiy et al., 2021). Trong
LONG T5 (Guo et al., 2022), mô hình liên quan
gần nhất với COLT5, các token attend trong một
cửa sổ cục bộ cũng như một biểu diễn tóm tắt
mean-pooled cho mỗi khối 16 token trong đầu vào.
Attention LONG T5 dẫn đến FLOPs giảm mạnh
(mặc dù vẫn không đáng kể) (Bảng 1).

¹Mỗi phép nhân-cộng được tính là một FLOP.

--- TRANG 3 ---
Tính toán có điều kiện Sau khi áp dụng cơ chế
sparse attention, các lớp feedforward và attention
projection chiếm phần lớn FLOPs. Những chi phí
này tăng theo độ dài của đầu vào, khiến việc xử lý
đầu vào dài vẫn cấm đoán về mặt tốn kém. Một
phương pháp phổ biến để giảm chi phí còn lại là
sử dụng một số hình thức tính toán có điều kiện,
tránh áp dụng tất cả tham số mô hình cho toàn bộ
đầu vào.

CALM (Schuster et al., 2022) áp dụng số lượng
lớp decoder khác nhau cho mỗi token được decode,
xuất ra token sớm nếu mô hình tự tin vào dự đoán
của nó. Các mô hình Mixture-of-Experts (Shazeer
et al., 2017; Fedus et al., 2021; Zoph et al., 2022)
định tuyến đầu vào qua một tỷ lệ nhỏ các sub-module
expert, chỉ sử dụng các tham số liên quan nhất đến
đầu vào. Trong bối cảnh các mô hình retrieval-
augmented, nhiều công trình xếp hạng lại các đoạn
văn được truy xuất theo mức độ liên quan đến truy
vấn và chỉ xử lý các đoạn văn có điểm cao nhất
(Mao et al., 2021; Wang et al., 2018; Yu et al.,
2022) và thay đổi số lượng đoạn văn được xử lý
tùy thuộc vào độ tin cậy của mô hình (Kratzwald
và Feuerriegel, 2018; Varshney et al., 2022). Công
trình đồng thời CoDA (Lei et al., 2023) sử dụng
cơ chế tính toán có điều kiện liên quan, được thiết
kế cho adaptation hiệu quả hơn là mô hình hóa tài
liệu dài.

Sử dụng thiết bị FLOPs không kể hết câu chuyện,
vì các lựa chọn mô hình có thể ảnh hưởng đến tốc
độ hoạt động hiệu quả được đạt bởi các accelerator.
Đối với đầu vào văn bản dài, suy luận decoder tự
hồi quy rất chậm do ràng buộc băng thông bộ nhớ
từ việc tải lặp đi lặp lại chuỗi dài key và value
(Shazeer, 2019; de Jong et al., 2022). Shazeer
(2019) giới thiệu multi-query attention (MQA), chia
sẻ head cho key và value để giảm overhead băng
thông bộ nhớ. Pope et al. (2022) nghiên cứu cách
phân chia các mô hình lớn, đặc biệt trong bối cảnh
MQA, để có được sử dụng thiết bị tối ưu và do đó
tốc độ.

Mục tiêu huấn luyện T5 giới thiệu mục tiêu span
corruption (Raffel et al., 2020), một sửa đổi của
masked language modeling (Devlin et al., 2019).
LONG T5 sử dụng mục tiêu sentence reconstruction
của PEGASUS (Zhang et al., 2020) để cải thiện hiệu
suất tóm tắt. Tay et al. (2022) đề xuất UL2, một hỗn
hợp của span corruption, prefix, và causal language
modeling, và chỉ ra rằng nó dẫn đến hiệu suất mạnh
trên cả tác vụ short-output và generative.

3 COLT5

3.1 Tính toán có điều kiện

Như đã thảo luận trong phần trước, một tỷ lệ lớn
FLOPs Transformer phát sinh từ các lớp feedforward
và projection tăng theo độ dài của chuỗi đầu vào.
Do đó, huấn luyện và suy luận LONG T5 trên tài
liệu dài vẫn tốn kém.

COLT5 giảm thêm chi phí xử lý tài liệu dài thông
qua tính toán có điều kiện, theo trực giác rằng một
số token quan trọng hơn và do đó được hưởng lợi
nhiều hơn từ tính toán nặng so với những token khác.
Đầu tiên, một số loại token có thể vốn dĩ yêu cầu
ít tính toán hơn, như từ đệm và dấu câu. Thứ hai,
đặc biệt trong tài liệu dài, các phần lớn của đầu vào
có thể không liên quan đến câu hỏi, tác vụ, hoặc
giai đoạn xử lý hiện tại.

Cơ chế tính toán có điều kiện COLT5 gồm ba thành
phần: module định tuyến, lớp feedforward có điều
kiện, và lớp attention có điều kiện. Tất cả token
được xử lý bởi các lớp attention và feedforward
tiêu chuẩn, nhẹ. Các module định tuyến bổ sung
chọn token quan trọng từ đầu vào tại mỗi lớp
attention hoặc feedforward, và một lớp có điều
kiện nặng áp dụng tính toán bổ sung cho các token
được định tuyến. Phần này mô tả chi tiết từng thành
phần. Hình 1 cung cấp tổng quan về cơ chế tính
toán có điều kiện COLT5, và Bảng 2 so sánh FLOPs
của COLT5 và LONG T5.

Mô hình Encoder Layer Flops
T5 12nd² + 2n²d
LONG T5 12nd² + n²d/8
COLT5 71/4nd² + n²d/84

Bảng 2: COLT5 sử dụng đáng kể ít FLOPs hơn
LONG T5. So sánh FLOPs tổng thể lớp encoder
gần đúng giữa T5, LONGT5, và COLT5. FLOPs
COLT5 được làm tròn thành phân số có thể đọc được.

Định tuyến Để chọn riêng biệt các token quan trọng
cho mỗi thành phần trong mỗi lớp, chúng ta cần

--- TRANG 4 ---
một hàm định tuyến có thể học và khả thi. Chúng
tôi theo cơ chế ba bước đơn giản từ Lei et al. (2023):
(1) nhân đầu vào với embedding đã học để có được
điểm định tuyến, (2) chuẩn hóa, và (3) chọn k đầu
vào có điểm cao nhất.

Gọi Xi là biểu diễn của token i, và u là embedding
có thể học d-chiều. Khi đó điểm định tuyến của
token i là
si = Xi · u

Chúng ta chọn k đầu vào có điểm cao nhất. Để cung
cấp tín hiệu học cho embedding điểm, chúng ta đảm
bảo đóng góp của các token được định tuyến vào
cập nhật lớp được scaled theo điểm định tuyến, như
sẽ thấy sau. Để cung cấp tín hiệu phân tán tốt hơn
cho tất cả token, chúng ta cũng chuẩn hóa toàn cục
các điểm định tuyến để tổng lên bằng số token được
định tuyến mong muốn bằng cách sử dụng generalized
softmax, dẫn đến điểm chuẩn hóa s̃i. Mỗi lớp COLT5
có ba router độc lập, một cho lớp feedforward, một
cho attention query, và một cho attention key-value.

Feedforward có điều kiện Theo trực giác, một số
biểu diễn token có thể được hưởng lợi từ việc xử lý
nhiều hơn những token khác. Lớp feedforward có
điều kiện COLT5 áp dụng một lớp feedforward
dung lượng cao bổ sung cho các token được chọn.
Cụ thể, gọi Xi là trạng thái mô hình của token thứ i
và s̃i biểu thị điểm định tuyến chuẩn hóa (đặt bằng
0 cho các token không được định tuyến). Khi đó
cập nhật feedforward cho COLT5 được cho bởi

Xi = Xi + FFdLight(Xi) + s̃i · FFdHeavy(Xi)

Các nhánh feedforward nhẹ và nặng chỉ khác nhau
về chiều ẩn, với nhánh nhẹ có chiều ẩn nhỏ hơn
lớp feedforward T5 tiêu chuẩn và nhánh nặng lớn
hơn. Gọi n là số token đầu vào, m là số token được
chọn, và rL và rH là tỷ lệ chiều ẩn nhẹ và nặng so
với chiều ẩn T5 tiêu chuẩn. Khi đó FLOPs của lớp
COLT5 được cho bởi

FLOPsFFd = 8nrLd²︸ ︷︷ ︸
Nhánh nhẹ + 8mrHd²︸ ︷︷ ︸
Nhánh nặng

Chúng ta đặt tỷ lệ nhẹ và nặng là rL = 1/2 và
rH = 4, một nửa và gấp bốn chiều ẩn T5 tiêu chuẩn
tương ứng. Cho các thí nghiệm chính, tỷ lệ 1/16
token được định tuyến đến nhánh nặng. Kết quả là
FLOPs gần đúng từ lớp feedforward COLT5 bằng

FLOPsFFd = 4nd²︸ ︷︷ ︸
Nhánh nhẹ + 2nd²︸ ︷︷ ︸
Nhánh nặng

tiêu thụ 75% FLOPs của lớp feedforward T5 tiêu
chuẩn.

Hình 3: Tổng quan về mẫu attention COLT5. Nhánh
nhẹ thực hiện attention cục bộ cho mỗi token. Trong
nhánh nặng dung lượng cao hơn, q token query được
chọn (2 trong hình) attend tới v token key và value
được chọn riêng biệt (4 trong hình).

Attention có điều kiện COLT5 conditional attention
hoạt động trên trực giác rằng hầu hết token có tương
tác đơn giản, cục bộ, nhưng một số token được hưởng
lợi từ xử lý nặng hơn và tương tác tầm xa. Lớp
attention có điều kiện COLT5 áp dụng một lớp
attention dung lượng cao bổ sung attend từ các
token query được chọn đến các token key-value
được chọn. Gọi s̃qi biểu thị điểm định tuyến query
chuẩn hóa cho token i, và s̃kv là điểm key-value
cho tất cả token (đặt bằng 0 nếu không được định
tuyến). Khi đó cập nhật attention cho COLT5 được
cho bởi

Xi = Xi + ALight(Xi, X) + s̃qi · AHeavy(Xi, s̃kvX)

Các nhánh nhẹ và nặng khác nhau về số head và
token được attend: nhánh nhẹ có ít head hơn và
attend tới cửa sổ context cục bộ, trong khi nhánh
nặng có nhiều head hơn và attend tới tất cả token
key-value được định tuyến. Việc chọn riêng biệt
token query và key-value cũng cho phép mô hình
phân biệt giữa các token cần thông tin bổ sung và
những token sở hữu

--- TRANG 5 ---
Mô hình Avg Speed TQA NQA QAS QuAL CNLI arXiv SumS QMS GovR
inf fn F1 F1 F1 EM EM Rgm Rgm Rgm Rgm
LONG T5-B 43.1 0.6 / 7.4 3.7 82.2 23.0 46.6 37.9 85.6 35.4 19.2 20.4 37.7
COLT5-B 42.4 11.2 6.5 82.4 23.3 42.1 36.5 86.5 35.3 18.7 18.4 37.9
LONG T5-L 45.3 0.3 / 3.0 1.3 84.2 27.2 52.3 40.6 87.3 35.7 19.1 21.4 39.5
COLT5-L 45.3 5.0 2.0 84.5 27.7 49.8 39.9 88.7 35.9 20.5 21.0 39.7
LONG T5-XL 46.6 0.2 / 1.2 0.4 85.3 29.3 53.1 46.0 88.2 35.9 19.4 21.3 40.5
COLT5-XL 47.4 2.3 0.5 86.1 31.1 53.9 48.1 88.4 36.1 20.0 22.5 40.5

Bảng 3: So sánh hiệu suất của COLT5 và LONG T5 Base, Large và XL models trên các tập dữ liệu hỏi đáp
TriviaQA (TQA), NarrativeQA (NQA), QASPER (QAS), và QuALITY (QuAL), tập dữ liệu NLI ContractNLI
(CNLI), và các tập dữ liệu tóm tắt arXiv, SummScreenFD (SumS), QMSum (QMS), và GovReport (GovR).
Kết quả SCROLLS trên test set leaderboard nơi COLT5-XL đạt SOTA. Tốc độ trung bình được báo cáo
bằng mẫu mỗi giây cho suy luận (inf) và fine-tuning (fn). LONG T5 không sử dụng MQA nhưng tốc độ suy
luận được báo cáo không có/có MQA cho baseline bảo thủ. Rgm viết tắt của trung bình nhân của ROUGE-1,2,L.
Tương tự SCROLLS, chúng ta lấy trung bình đơn giản trên tất cả tập dữ liệu mặc dù các tập dữ liệu sử dụng
các số liệu hiệu suất khác nhau.

thông tin như vậy. Hình 3 cho thấy mẫu attention
COLT5. Gọi q, v là số token query và key-value
được chọn, w là kích thước cửa sổ attention cục bộ
và rL, rH là tỷ lệ head nhẹ và nặng so với T5 tiêu
chuẩn. Khi đó FLOPs của lớp attention COLT5
được cho bởi

FLOPsAtt = 4n · rLd²︸ ︷︷ ︸
Local projection + 2nw · rLd︸ ︷︷ ︸
Local attention
+ 2q · rHd² + 2v · rHd²︸ ︷︷ ︸
Global projection + 2qv · rHd︸ ︷︷ ︸
Global attention

Chúng ta đặt tỷ lệ head nhẹ và nặng là rL = 1/4
và rH = 3/4, giữ tổng số head trên các nhánh nhẹ
và nặng bằng head T5 tiêu chuẩn. Cho các thí
nghiệm chính, tỷ lệ 1/16 token query và 1/8 token
key-value được định tuyến đến nhánh nặng, vậy
q = n/16 và v = n/8. Bỏ qua tính toán attention
cục bộ, chúng ta ước lượng FLOPS attention bởi²

FLOPsAtt ≈ nd²︸ ︷︷ ︸
Local proj. + 1/4nd²︸ ︷︷ ︸
Global proj. + 1/84n²d︸ ︷︷ ︸
Global att.

với ít hơn một nửa FLOPs projection và scaling
độ dài bậc hai nhỏ hơn theo bậc so với LONG T5.
Bảng 2 cho thấy tổng FLOPs cho lớp COLT5. Nói
chung, chúng ta đặt q = m và v = 2m, và sử dụng
m để tóm tắt số token được định tuyến từ đây trở đi.

²FLOPs projection và attention toàn cục được làm tròn thành
phân số có thể đọc được, giá trị chính xác là 9/32 và 3/256.
Độ phức tạp giả định tỷ lệ token được định tuyến không đổi;
chúng tôi chỉ ra có thể làm tốt hơn trong thực tế cho đầu vào
cực kỳ dài.

3.2 Multi-query Attention

Tính toán có điều kiện hiệu quả giảm chi phí tính
toán của encoder. Tuy nhiên, đối với các mô hình
encoder-decoder với đầu vào dài, phần lớn thời
gian suy luận được dành cho decoder do ràng buộc
băng thông bộ nhớ (Shazeer, 2019; de Jong et al.,
2022). Hầu hết overhead gây ra bởi việc đọc lặp đi
lặp lại tất cả key và value token đầu vào từ bộ nhớ
cho mỗi token đầu ra được decode tự hồi quy trong
cross attention. Multi-query attention (Shazeer,
2019) (MQA) cho phép tất cả query head chia sẻ
một key và value head duy nhất, giảm bớt bottleneck
này. Theo đó, chúng ta áp dụng MQA trong các lớp
cross-attention để suy luận nhanh hơn nhiều. Tuy
nhiên lưu ý rằng MQA không cải thiện tốc độ huấn
luyện vì các token target được xử lý song song
trong huấn luyện, tránh bottleneck băng thông bộ
nhớ này.

3.3 UL2

Mục tiêu pre-training UL2 (Tay et al., 2022) kết
hợp các mục tiêu denoising khác nhau, mở rộng
pre-training span corruption được sử dụng trong
T5 thành nhiều tỷ lệ noise / độ dài span trung bình
khác nhau và thêm mục tiêu prefix language modeling
tương tự hơn với pre-training mô hình decoder-only
điển hình. UL2 đã được chỉ ra dẫn đến cải thiện
học trong ngữ cảnh. Chúng ta huấn luyện COLT5
trên UL2 thay vì PEGASUS (Zhang et al., 2020),
trao cho COLT5 khả năng học trong ngữ cảnh.

4 Thí nghiệm

Để đánh giá COLT5, chúng tôi thực hiện các thí
nghiệm sau: (1) kết quả chính của chúng tôi so sánh

--- TRANG 6 ---
COLT5 và LONG T5 trên một tập hợp các tập dữ
liệu đầu vào dài sử dụng độ dài đầu vào 16k token;
(2) chúng tôi đánh giá COLT5 trên đầu vào cực kỳ
dài lên đến 64k token và so sánh scaling với LONG
T5; (3) chứng minh khả năng few-shot của COLT5,
điều tra cách hiệu suất thay đổi khi độ dài đầu vào
và số shot tăng, (4) thực hiện một loạt ablation để
hiểu hiệu ứng của các thành phần COLT5 riêng lẻ,
và (5) điều tra các mẫu định tuyến thực nghiệm.
Phần còn lại của chương này nêu setup thí nghiệm,
và sau đó mô tả từng thí nghiệm ở trên.

4.1 Setup thí nghiệm

Cấu hình COLT5 dựa trên kiến trúc T5.1.1 (Raffel
et al., 2020), được implement với JAX (Bradbury
et al., 2018), Flax (Heek et al., 2020), và Flaxformer³.
Theo LONG T5, chúng tôi thí nghiệm với kích thước
mô hình Base, Large, và XL. Các mô hình COLT5
sử dụng cùng chiều embedding, số lớp, và tổng số
attention head như các mô hình LONG T5 tương
ứng cùng kích thước, với nhiều tham số tổng thể
hơn (nhưng ít tính toán hơn) do nhánh có điều kiện.
Xem Phụ lục B để biết thêm chi tiết về cấu hình mô
hình.

Pre-training Chúng tôi pre-train COLT5 trong 1M
bước trên tập dữ liệu C4 (Raffel et al., 2020) sử
dụng một biến thể của mục tiêu UL2 (Tay et al.,
2022) với batch size 256, độ dài đầu vào 4096, và
độ dài đầu ra 910. Cụ thể, hỗn hợp của chúng tôi
chứa bốn mục tiêu với tỷ lệ bằng nhau: prefix-LM
với tỷ lệ noise 0.5, và span corruption (Raffel et al.,
2020) với tỷ lệ noise 0.15 và độ dài span trung bình
3, 8, và 64. Chúng tôi sử dụng optimizer Adafactor
(Shazeer và Stern, 2018) với lịch trình học rate
inverse square root T5.1.1 và không dropout. COLT5
được huấn luyện với framework T5X (Roberts et al.,
2022). Cho pre-training, chúng tôi định tuyến m =
512 token, 1/8 độ dài đầu vào.

Fine-tuning Cho fine-tuning chúng tôi sử dụng
learning rate không đổi 0.001, batch size 128, và
tỷ lệ dropout 0.1 cho tất cả tác vụ. Kết quả chính
sử dụng độ dài đầu vào 16384 cho tất cả tập dữ
liệu ngoại trừ ContractNLI, sử dụng 8192. Các tập
dữ liệu hỏi đáp sử dụng độ dài đầu ra 128 và các
tập dữ liệu tóm tắt sử dụng độ dài đầu ra 512, ngoại
trừ GovRep sử dụng độ dài đầu ra 1024. Chúng
tôi định tuyến m = 1024 token, 1/16 độ dài đầu
vào. Chúng tôi huấn luyện đến hội tụ

³https://github.com/google/flaxformer

và chọn checkpoint với hiệu suất dev cao nhất.
Chúng tôi sử dụng greedy decoding cho suy luận.

Dữ liệu Chúng tôi đánh giá COLT5 trên TriviaQA
(Joshi et al., 2017), arXiv (Cohan et al., 2018), và
benchmark SCROLLS (Shaham et al., 2022).
SCROLLS chứa các tập dữ liệu hỏi đáp: NarrativeQA
(Kočiský et al., 2018), QASPER (Dasigi et al.,
2021), và QuALITY (Pang et al., 2021), một tập
dữ liệu NLI: ContractNLI (Koreeda và Manning,
2021), và các tập dữ liệu tóm tắt: SummScreenFD
(Chen et al., 2022), QMSum (Zhong et al., 2021),
và GovReport (Huang et al., 2021). Bảng 4 cung
cấp tổng quan về kích thước và độ dài đầu vào cho
mỗi tập dữ liệu.

Tập dữ liệu Loại Mẫu Trung vị 90%
TriviaQA QA 157,053 8,858 28,956
arXiv Sum 215,913 8,519 20,170
NarrativeQA QA 71,187 57,829 176,862
QASPER QA 5,692 5,472 8,657
QuALITY QA 6,737 7,171 8,276
ContractNLI NLI 10,319 2,148 4,485
SummScreen Sum 4,348 9,046 15,172
QMSum Sum 1,810 14,197 27,761
GovRep Sum 19,402 8,841 18,835

Bảng 4: Độ dài đầu vào trung vị và phần trăm thứ 90
theo tập dữ liệu được đo bằng token SentencePiece.

Timing Chúng tôi báo cáo thời gian mỗi mẫu mỗi
chip TPUv4, được đo bằng xprof (Google, 2020).
Cho suy luận chúng tôi sử dụng một TPUv4 duy
nhất với batch size 16 hoặc lớn nhất vừa bộ nhớ.
Cho fine-tuning chúng tôi profile với 8 chip TPUv4,
được shard riêng biệt cho mỗi mô hình để tối đa
hóa throughput.

4.2 Kết quả chính

Hình 2 so sánh trade-off chất lượng-tốc độ cho
LONG T5⁴ và COLT5, cho thấy COLT5 tốt hơn
ở bất kỳ tốc độ nào. Cho độ dài đầu vào 16k, COLT5
match hoặc vượt chất lượng LONGT5 cho Large
và XL với tăng tốc huấn luyện 35-75% và tăng tốc
suy luận 50-100% trên đỉnh của tăng tốc suy luận
theo bậc từ MQA. Tăng tốc encoder thậm chí lớn
hơn (Phụ lục D). COLT5-XL cũng đạt hiệu suất
SOTA trên benchmark SCROLLS. Bảng 3 chứa
tất cả kết quả chính.

⁴Lưu ý rằng LONG T5 không sử dụng MQA, nhưng cho
profiling chúng tôi thêm MQA vào LONG T5 cho baseline
bảo thủ.

--- TRANG 7 ---
100 200 300 400 500 6002426283032
8k16k32k
8k16k32k64k
Thời gian mỗi mẫu (ms)F1
LONG T5
COLT5

Hình 4: COLT5 scale hiệu quả đến đầu vào cực kỳ
dài, đạt hiệu suất mạnh hơn và tốc độ nhanh hơn so
với LONG T5. F1 trên NarrativeQA theo thời gian
suy luận mỗi mẫu cho các mô hình LONG T5 và
COLT5 Large sử dụng độ dài đầu vào khác nhau.

4.3 Scaling đến đầu vào cực kỳ dài

Chúng tôi đưa ra giả thuyết rằng lợi thế của COLT5
so với LONGT5 tăng cường theo độ dài đầu vào,
khi tỷ lệ token quan trọng giảm và COLT5 có thể
định tuyến tỷ lệ lớn hơn các token quan trọng đến
nhánh nặng. Hình 4 so sánh trade-off chất lượng-
tốc độ cho LONG T5 và COLT5 trên NarrativeQA,
quét qua độ dài đầu vào thay vì kích thước mô hình.
Số token được định tuyến là 1/16 độ dài đầu vào,
ngoại trừ chúng tôi không tăng token được định
tuyến từ 32k lên 64k, vậy ở 64k chúng tôi chỉ định
tuyến 1/32 độ dài đầu vào. COLT5 đạt được cả
hiệu suất mạnh hơn và tốc độ suy luận nhanh hơn
ở tất cả độ dài đầu vào và có thể hiệu quả sử dụng
đầu vào cực kỳ dài. Chúng tôi lưu ý rằng COLT5
đạt được mức tăng chất lượng lớn bằng cách từ 32k
lên 64k token ngay cả khi giữ số token được định
tuyến không đổi, cung cấp thêm bằng chứng cho
giả thuyết của chúng tôi.

4.4 Học trong ngữ cảnh

Các mô hình được huấn luyện trên mục tiêu UL2
đã cho thấy khả năng học trong ngữ cảnh few-shot
(ICL) mạnh⁵ ngay cả ở kích thước nhỏ hơn (Tay
et al., 2022). COLT5 cho phép suy luận khả thi
với đầu vào dài. Ở đây, chúng tôi tận dụng điều
này để scaling số ví dụ được sử dụng cho học trong
ngữ cảnh.

⁵Chúng tôi ban đầu đánh giá ICL cho các mô hình được
pre-train với PEGASUS nhưng thấy hiệu suất gần như 0.

0.10.20.31k
2k
4k
8k
16kNaturalQ
0.05 0.1TriviaQA

Hình 5: COLT5 có thể sử dụng khả năng đầu vào
dài để được hưởng lợi từ nhiều shot hơn cho học
trong ngữ cảnh. Exact match few-shot cho COLT5-
Large trên Natural Questions và TriviaQA dev set
theo token đầu vào, fitting càng nhiều ví dụ càng
tốt. Mỗi ví dụ chứa câu hỏi, context, và câu trả lời.
Độ dài đầu vào được sử dụng là 1024, 2048, 4096,
8192, 16384.

Chúng tôi test giả thuyết trên bằng cách đánh giá
hiệu suất học few-shot trên Natural Questions
(Kwiatkowski et al., 2019) và TriviaQA theo độ
dài đầu vào, sử dụng càng nhiều ví dụ càng vừa
trong ngữ cảnh. Chúng tôi xem xét setting open
book, sao cho mỗi ví dụ gồm câu hỏi, tài liệu
context, và câu trả lời. Bảng 5 cho thấy số ví dụ
theo độ dài đầu vào. Chúng tôi đánh giá trên toàn
bộ dev set, lấy mẫu ngẫu nhiên ví dụ từ training
set cho mỗi mẫu dev cho đến khi không có ví dụ
nào khác vừa trong độ dài đầu vào. Chúng tôi thấy
rằng COLT5 chỉ có thể thực hiện học trong ngữ
cảnh lên đến độ dài đầu vào nó được huấn luyện,
vậy cho các thí nghiệm này chúng tôi tiếp tục pre-
train một mô hình COLT5-Large trên độ dài đầu
vào 16384 thêm 100k bước. Vì lý do tương tự
chúng tôi định tuyến m = 512 token như trong
pre-training.

Hình 5 hiển thị hiệu suất few-shot COLT5 theo
độ dài đầu vào, cho thấy COLT5 có thể áp dụng
khả năng đầu vào dài để trích xuất thông tin từ
số lượng ví dụ ngày càng tăng.

Tập dữ liệu 1024 2048 4096 8192 16384
NQ 0.1 0.7 1.7 3.4 5.6
TriviaQA 1.6 2.3 3.8 7.0 9.8

Bảng 5: Số ví dụ few-shot Natural Questions và
TriviaQA trung bình vừa trong độ dài đầu vào.

4.5 Ablations

Phần này nghiên cứu hiệu ứng của các lựa chọn
khác nhau trong công thức COLT5. Bảng 6 chứa
kết quả của một loạt thí nghiệm thay đổi một thành
phần duy nhất

--- TRANG 8 ---
Ablation Mô hình Avg Inf TQA NQA QAS QuAL CNLI arX SumS QMS GovR
S/s F1 F1 F1 EM EM Rgm Rgm Rgm Rgm
Baseline COLT5-B 42.5 11.2 82.4 23.1 38.3 36.6 87.8 35.3 19.3 20.5 39.4
Routing Static 40.5 11.6 79.7 19.2 34.2 34.5 86.4 34.9 18.1 18.9 38.8
Share QKV 42.0 11.8 82.1 21.9 37.5 36.2 87.0 35.2 18.2 20.4 39.7
Attention v=all 42.5 9.4 82.4 22.3 38.6 37.2 87.8 35.3 19.1 20.3 39.8
v=q 42.3 11.5 82.5 22.5 37.3 37.0 85.9 35.2 19.0 20.5 39.7
Routed
Tokens m=512 41.6 12.2 81.9 22.1 37.3 35.4 84.6 35.2 18.9 19.5 39.6
m=1536 42.9 10.4 82.6 23.5 39.8 37.5 87.5 35.4 19.4 20.8 40.0
Encoder LONG T5-B 42.1 7.4 82.0 21.4 38.4 35.8 88.0 35.5 18.7 20.4 38.5
Decoder Multi-head 42.9 0.7 82.7 22.9 40.2 35.8 87.7 35.5 19.7 21.2 40.3
Objective PEGASUS 42.8 11.2 82.6 22.6 40.5 37.3 87.3 35.3 19.6 20.8 39.6

Bảng 6: Ablations COLT5 được đánh giá trên validation set. Mỗi thí nghiệm sửa đổi một thành phần của
công thức COLT5 cho COLT5-Base. Static routing chia đầu vào thành các khối có độ dài bằng nhau và chọn
token đầu tiên trong mỗi khối để được định tuyến. Shared QKV routing chia sẻ quyết định định tuyến cho
query và key/value. Trong v=all các query được định tuyến attend tới toàn bộ đầu vào, trong khi v=q chọn
cùng số token key và value như token query. m=512 và m=1536 sử dụng số token được định tuyến khác nhau.
LONG T5-B sử dụng encoder LONG T5 trong khi giữ lại các phần khác của công thức huấn luyện COLT5
như MQA và mục tiêu UL2. Multi-head đề cập đến việc sử dụng multi-head cross-attention. Ablation cuối
thay thế mục tiêu UL2 bằng PEGASUS như trong LONG T5.

cho COLT5 Base.

Routing Đầu tiên, chúng tôi lưu ý rằng static routing
-- phân phối đều các token được định tuyến trên
đầu vào -- dẫn đến sụt giảm lớn về hiệu suất. Tầm
quan trọng của routing cung cấp bằng chứng rằng
mô hình học được cách dành dung lượng cho các
token quan trọng và lợi thế của COLT5 không chỉ
là kết quả của tham số bổ sung. Chia sẻ quyết định
định tuyến cho token query và KV nên được so
sánh với v=q, và dẫn đến giảm nhẹ chất lượng và
tăng tốc độ.

Số token được định tuyến tối ưu đại diện cho trade-
off giữa hiệu suất cải thiện và chi phí tính toán của
việc áp dụng các lớp nặng hơn. Bảng 6 cho thấy
mức tăng mạnh từ 512 lên 1024 (baseline) token
được định tuyến và lợi nhuận giảm dần cho các
tăng thêm.

Attention COLT5 dựa vào routing để xác định
không chỉ các token có thể được hưởng lợi từ thông
tin quan trọng ở nơi khác trong đầu vào, mà còn
token nào chứa thông tin quan trọng như vậy.
Chúng tôi nghiên cứu liệu COLT5 có thành công
trong nhiệm vụ này bằng cách so sánh hiệu suất
với hai setting attention khác nhau -- v=all, trong
đó các token được định tuyến attend tới toàn bộ
đầu vào, và v=q, sử dụng số lượng key và value
được định tuyến bằng query, thay vì gấp đôi.
COLT5 dường như chiếm một điểm ngọt ngào,
vì sử dụng ít key-value được định tuyến hơn giảm
nhẹ hiệu suất ở tốc độ tương tự nhưng attend tới
tất cả đầu vào hầu như không giúp ích gì ở chi phí
tăng mạnh.

Khác Chúng tôi so sánh COLT5 với LONG T5
với multi-query cross-attention, xác nhận rằng
LONG T5 thực sự không đạt được mức tăng chất
lượng bất ngờ từ MQA, và các giả định bảo thủ
của chúng tôi trong Hình 2, 4 là hợp lệ. Tiếp theo,
chúng tôi đánh giá multi-head cross-attention cho
COLT5, thấy rằng nó dẫn đến hiệu suất COLT5
cải thiện nhẹ. Tuy nhiên, vì MHA thể hiện suy
luận chậm hơn theo bậc, MQA rõ ràng được ưu
tiên. Cuối cùng, PEGASUS dường như fine-tune
tốt hơn UL2 một chút, mặc dù sự khác biệt nhỏ
và UL2 cho phép học few-shot.

4.6 Phân tích routing

Thật thú vị khi hỏi liệu các token được định tuyến
COLT5 có phù hợp với những gì chúng ta coi là
token quan trọng theo trực giác trong mỗi tài liệu.
Chúng tôi điều tra câu hỏi này bằng cách nghiên
cứu các mẫu routing của mô hình COLT5 Large
được fine-tune trên TriviaQA. Chúng tôi chia token
thành ba loại: (1) token câu hỏi, (2) token câu trả
lời, và (3) token khác. Hình 6 cho thấy tỷ lệ trung
bình của mỗi loại token được định tuyến qua đường
nặng cho các lớp MLP và attention trên TriviaQA.
Chúng tôi lưu ý rằng các token câu hỏi và câu trả
lời có khả năng được định tuyến cao hơn đáng kể
so với các token khác, cho cả feedforward cũng như
attention query và key/value. Phụ lục F trình bày
phân tích routing chi tiết hơn; ví dụ, các token quan
trọng về mặt ngữ nghĩa có khả năng được chọn
nhiều hơn

--- TRANG 9 ---
MLP Query KV0.20.40.6Tỷ lệ được định tuyến Khác
Câu trả lời
Câu hỏi

Hình 6: Tỷ lệ token được định tuyến cho câu trả lời
(string match), câu hỏi, và token khác theo thành
phần routing cho mô hình COLT5 Large, trung bình
trên các ví dụ trong TriviaQA dev set và tất cả lớp
của mô hình.

trong các lớp sau.

5 Kết luận

Chúng tôi đề xuất COLT5, một mô hình mới cho
đầu vào tầm xa sử dụng tính toán có điều kiện để
có chất lượng cao hơn và tốc độ nhanh hơn. COLT5
có các lớp feedforward và attention nhẹ áp dụng
cho toàn bộ đầu vào, cũng như các nhánh nặng chỉ
được áp dụng cho một tập con các token quan trọng
được chọn bởi router đã học. Chúng tôi chỉ ra rằng
COLT5 đạt được hiệu suất mạnh hơn ở bất kỳ tốc
độ nào so với LONG T5 trên nhiều tập dữ liệu đầu
vào dài, và có thể hiệu quả và hiệu suất sử dụng
đầu vào cực kỳ dài lên đến 64k token.

Hạn chế

COLT5 áp dụng tính toán có điều kiện chỉ trong
encoder. Áp dụng tính toán có điều kiện trong
decoder phức tạp hơn; phương pháp routing trong
COLT5 không causal, vậy nó không áp dụng được
khi sinh token từng token. Vì các mô hình decoder-
only và ứng dụng với đầu ra dài đã trở nên phổ
biến hơn gần đây, đây là hạn chế mạnh của phương
pháp hiện tại. Mặc dù phương pháp routing trong
COLT5 có thể được áp dụng cho input context
trong mô hình decoder-only, chúng tôi không điều
tra setup này.

COLT5 được chuyên biệt hóa cho các chuỗi dài
và phải được huấn luyện từ đầu. Cho huấn luyện
và triển khai quy mô lớn, mong muốn là huấn
luyện một mô hình duy nhất có thể xử lý cả chuỗi
ngắn và dài, hoặc phát triển kiến trúc đầu vào dài
có thể được adapt từ một mô hình lớn hiện có.

Lời cảm ơn

Chúng tôi muốn cảm ơn Srinadh Bhojanapalli,
Luke Vilnis, Zachary Fisher, Jianmo Ni, Tal
Schuster, Vaclav Cvicek, Sudeep Gandhe, Bhargav
Kanagal, Kenton Lee, Ming-Wei Chang, Afroz
Mohiuddin, Raphael Hoffmann, và những người
khác tại Google Research vì lời khuyên và thảo
luận hữu ích.

Tài liệu tham khảo

Joshua Ainslie, Santiago Ontañón, Chris Alberti,
Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang,
và Li Yang. 2020. ETC: Encoding long and
structured inputs in transformers. arXiv preprint
arXiv:2004.08483.

Iz Beltagy, Matthew E Peters, và Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150.

James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake
VanderPlas, Skye Wanderman-Milne, và Qiao
Zhang. 2018. JAX: composable transformations
of Python+NumPy programs.

Mingda Chen, Zewei Chu, Sam Wiseman, và Kevin
Gimpel. 2022. SummScreen: A dataset for abstrac-
tive screenplay summarization. In Proceedings of
the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 8602–8615, Dublin, Ireland. Association for
Computational Linguistics.

Rewon Child, Scott Gray, Alec Radford, và
Ilya Sutskever. 2019. Generating long se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509.

Arman Cohan, Franck Dernoncourt, Doo Soon Kim,
Trung Bui, Seokhwan Kim, Walter Chang, và Nazli
Goharian. 2018. A discourse-aware attention model
for abstractive summarization of long documents.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers), pages 615–621, New Or-
leans, Louisiana. Association for Computational
Linguistics.

Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,
Noah A. Smith, và Matt Gardner. 2021. A dataset
of information-seeking questions and answers an-
chored in research papers. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 4599–4610, On-
line. Association for Computational Linguistics.

Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie,
Nicholas FitzGerald, Sumit Sanghai, Fei Sha, và

--- TRANG 10 ---
William Cohen. 2022. FiDO: Fusion-in-decoder
optimized for stronger performance and faster
inference. arXiv preprint arXiv:2212.08153.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language
understanding. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2019,
Minneapolis, MN, USA, June 2-7, 2019, Volume 1
(Long and Short Papers), pages 4171–4186.
Association for Computational Linguistics.

William Fedus, Barret Zoph, và Noam Shazeer.
2021. Switch transformers: Scaling to trillion
parameter models with simple and efficient
sparsity. arXiv preprint arXiv:2101.03961.

Google. 2020. Profile your model with cloud tpu
tools. https://cloud.google.com/tpu/docs/
cloud-tpu-tools. Accessed: 2022-11-11.

Mandy Guo, Joshua Ainslie, David Uthus, Santiago
Ontañón, Jianmo Ni, Yun-Hsuan Sung, và Yinfei
Yang. 2022. LongT5: Efficient text-to-text
transformer for long sequences. In Findings of the
Association for Computational Linguistics: NAACL
2022, pages 724–736, Seattle, United States.
Association for Computational Linguistics.

Jonathan Heek, Anselm Levskaya, Avital Oliver,
Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, và Marc van Zee. 2020. Flax: A neural
network library and ecosystem for JAX.

Luyang Huang, Shuyang Cao, Nikolaus Parulian,
Heng Ji, và Lu Wang. 2021. Efficient attentions
for long document summarization. In Proceedings
of the 2021 Conference of the North American
Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1419–1436, Online. Association for Computational
Linguistics.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, và Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics,
Vancouver, Canada. Association for Computational
Linguistics.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom
B. Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, và Dario Amodei.
2020. Scaling laws for neural language models.
CoRR, abs/2001.08361.

Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, Gábor Melis,
và Edward Grefenstette. 2018. The NarrativeQA
reading comprehension challenge. Transactions of
the Association for Computational Linguistics,
6:317–328.

Yuta Koreeda và Christopher Manning. 2021.
ContractNLI: A dataset for document-level natural
language inference for contracts. In Findings of the
Association for Computational Linguistics: EMNLP
2021, pages 1907–1919, Punta Cana, Dominican
Republic. Association for Computational Linguistics.

Bernhard Kratzwald và Stefan Feuerriegel. 2018.
Adaptive document retrieval for deep question
answering. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 -
November 4, 2018, pages 576–581. Association for
Computational Linguistics.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia
Redfield, Michael Collins, Ankur P. Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob
Devlin, Kenton Lee, Kristina Toutanova, Llion
Jones, Matthew Kelcey, Ming-Wei Chang, Andrew
M. Dai, Jakob Uszkoreit, Quoc Le, và Slav Petrov.
2019. Natural questions: a benchmark for question
answering research. Trans. Assoc. Comput.
Linguistics, 7:452–466.

Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua
Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent
Y. Zhao, Yuexin Wu, Bo Li, Yu Zhang, và Ming-
Wei Chang. 2023. Conditional adapters: Parameter-
efficient transfer learning with fast inference. In
Advances in Neural Information Processing Systems.

Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong
Shen, Jianfeng Gao, Jiawei Han, và Weizhu Chen.
2021. Reader-guided passage reranking for open-
domain question answering. In Findings of the
Association for Computational Linguistics:
ACL/IJCNLP 2021, Online Event, August 1-6,
2021, volume ACL/IJCNLP 2021 of Findings of
ACL, pages 344–350. Association for Computational
Linguistics.

Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,
Nikita Nangia, Jason Phang, Angelica Chen,
Vishakh Padmakumar, Johnny Ma, Jana Thompson,
He He, và Samuel R. Bowman. 2021. QuALITY:
Question answering with long input texts, yes!
arXiv preprint arXiv:2112.08608.

Reiner Pope, Sholto Douglas, Aakanksha Chowdhery,
Jacob Devlin, James Bradbury, Anselm Levskaya,
Jonathan Heek, Kefan Xiao, Shivani Agrawal, và
Jeff Dean. 2022. Efficiently scaling transformer
inference. arXiv preprint arXiv:2211.05102.

Yujie Qian, Jinhyuk Lee, Sai Meher Karthik Duddu,
Zhuyun Dai, Siddhartha Brahma, Iftekhar Naim,
Tao Lei, và Vincent Y Zhao. 2022. Multi-
vector retrieval as sparse alignment. arXiv preprint
arXiv:2211.01267.

Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, và Peter J. Liu. 2020.
Exploring the limits of transfer learning with a
unified text-to-text transformer. J. Mach. Learn.
Res., 21:140:1–140:67.

--- TRANG 11 ---
Adam Roberts, Hyung Won Chung, Anselm Levskaya,
Gaurav Mishra, James Bradbury, Daniel Andor,
Sharan Narang, Brian Lester, Colin Gaffney, Afroz
Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz,
Alex Salcianu, Marc van Zee, Jacob Austin, Se-
bastian Goodman, Livio Baldini Soares, Haitang
Hu, Sasha Tsvyashchenko, Aakanksha Chowdh-
ery, Jasmijn Bastings, Jannis Bulian, Xavier Gar-
cia, Jianmo Ni, Andrew Chen, Kathleen Kenealy,
Jonathan H. Clark, Stephan Lee, Dan Garrette,
James Lee-Thorp, Colin Raffel, Noam Shazeer,
Marvin Ritter, Maarten Bosma, Alexandre Passos,
Jeremy Maitin-Shepard, Noah Fiedel, Mark Omer-
nick, Brennan Saeta, Ryan Sepassi, Alexander
Spiridonov, Joshua Newlan, và Andrea Gesmundo.
2022. Scaling up models and data with t5x and
seqio. arXiv preprint arXiv:2203.17189.

Tal Schuster, Adam Fisch, Jai Gupta, Mostafa
Dehghani, Dara Bahri, Vinh Q Tran, Yi Tay, và
Donald Metzler. 2022. Confident adaptive language
modeling. arXiv preprint arXiv:2207.07061.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori
Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Mor Geva, Jonathan Berant, và Omer Levy. 2022.
Scrolls: Standardized comparison over long language
sequences. ArXiv, abs/2201.03533.

Noam Shazeer. 2019. Fast transformer decoding:
One write-head is all you need. arXiv preprint
arXiv:1911.02150.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc V. Le, Geoffrey E. Hinton, và
Jeff Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. In 5th
International Conference on Learning Representa-
tions, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net.

Noam Shazeer và Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
In Proceedings of the 35th International Conference
on Machine Learning, ICML 2018, Stockholmsmäs-
san, Stockholm, Sweden, July 10-15, 2018, volume
80 of Proceedings of Machine Learning Research,
pages 4603–4611. PMLR.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang
Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu
Yang, Sebastian Ruder, và Donald Metzler. 2021.
Long range arena: A benchmark for efficient
transformers. In International Conference on
Learning Representations.

Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier
Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven
Zheng, Neil Houlsby, và Donald Metzler. 2022.
Unifying language learning paradigms. arXiv
preprint arXiv:2205.05131.

Neeraj Varshney, Man Luo, và Chitta Baral. 2022.
Can open-domain QA reader utilize external
knowledge efficiently like humans? CoRR,
abs/2211.12707.

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo
Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerry Tesauro, Bowen Zhou, và Jing Jiang. 2018.
R3: Reinforced ranker-reader for open-domain
question answering. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
(AAAI-18), the 30th innovative Applications of
Artificial Intelligence (IAAI-18), and the 8th AAAI
Symposium on Educational Advances in Artificial
Intelligence (EAAI-18), New Orleans, Louisiana,
USA, February 2-7, 2018, pages 5981–5988. AAAI
Press.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han
Fang, và Hao Ma. 2020. Linformer: Self-attention
with linear complexity. arXiv preprint
arXiv:2006.04768.

Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao
Yu, Shuohang Wang, Yichong Xu, Xiang Ren,
Yiming Yang, và Michael Zeng. 2022. Kg-fid:
Infusing knowledge graph in fusion-in-decoder for
open-domain question answering. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
ACL 2022, Dublin, Ireland, May 22-27, 2022, pages
4961–4974. Association for Computational
Linguistics.

Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontañón, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, et al. 2020. Big bird: Transformers
for longer sequences. Advances in Neural
Information Processing Systems, 33:17283–17297.

Yury Zemlyanskiy, Joshua Ainslie, Michiel de Jong,
Philip Pham, Ilya Eckstein, và Fei Sha. 2021.
Read-twice: Reading very large documents with
memories. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 5189–5195.

Jingqing Zhang, Yao Zhao, Mohammad Saleh, và
Peter Liu. 2020. Pegasus: Pre-training with
extracted gap-sentences for abstractive
summarization. In International Conference on
Machine Learning, pages 11328–11339. PMLR.

Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia
Mutuma, Rahul Jha, Ahmed Hassan Awadallah,
Asli Celikyilmaz, Yang Liu, Xipeng Qiu, và
Dragomir Radev. 2021. QMSum: A new benchmark
for query-based multi-domain meeting summarization.
In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
5905–5921, Online. Association for Computational
Linguistics.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du,
Yanping Huang, Jeff Dean, Noam Shazeer, và
William Fedus. 2022. St-moe: Designing stable and
transferable sparse expert models. arXiv preprint
arXiv:2202.08906.

--- TRANG 12 ---
Mô hình Lớp Chiều mô hình Chiều MLP nhẹ Chiều MLP nặng Head nhẹ Head nặng Tham số
LONG T5-B 12 768 2048 N/A 12 N/A 248m
COLT5-B 12 768 1024 8096 4 8 433m
LONG T5-L 24 1024 2816 N/A 16 N/A 783m
COLT5-L 24 1024 1408 11264 4 12 1462m
LONG T5-XL 24 2048 5120 N/A 32 N/A 2850m
COLT5-XL 24 2048 2560 20480 8 24 5297m

Bảng 7: Siêu tham số cho các mô hình LONG T5 và COLT5. Siêu tham số T5.1.1 match LONG T5. Tham số
COLT5 được truy cập thưa thớt do tính toán có điều kiện, vậy số lượng tham số không phản ánh tính toán, và
đối với kích thước mô hình cho trước COLT5 thực sự nhanh hơn LONG T5 mặc dù có nhiều tham số hơn.

A Đóng góp

Joshua dẫn dắt dự án, phát triển các cơ chế attention
có điều kiện ban đầu, và thực hiện hầu hết các
ablation thí nghiệm. Tao phát triển công thức
heavy/light cho tính toán có điều kiện không đồng
nhất, bao gồm các cơ chế routing và feedforward
có điều kiện, và lặp với Joshua về các thí nghiệm
ban đầu chứng minh tính khả thi. Michiel giúp
xác định phạm vi bài báo, thực hiện hầu hết việc
viết, và giám sát benchmarking tốc độ. Santiago
thiết kế và thực hiện tất cả các thí nghiệm few-shot,
khởi xướng visualization phân tích routing, và
tích hợp UL2 vào codebase. Siddhartha phát triển
routing riêng biệt cho token query và key/value
trong thành phần attention có điều kiện và chứng
minh các cải thiện chất lượng kết quả. Yury thiết
kế và thực hiện tất cả thí nghiệm cho đầu vào lớn
hơn 16k token, chứng minh scaling thuận lợi lên
đến 64k. David tích hợp tất cả tác vụ SCROLLS
vào codebase và chạy các thí nghiệm sớm, đặc
biệt so sánh UL2 với PEGASUS. Mandy phát
triển các so sánh leaderboard với LongT5 và giúp
chạy một số thí nghiệm. James tư vấn và chạy các
so sánh sớm với tính toán có điều kiện MoE. Yi
tư vấn về adaptation của UL2 cho pre-training độ
dài đầu vào 4k. Cuối cùng, Yun-Hsuan và Sumit
cung cấp hướng dẫn và hỗ trợ cho dự án tổng thể.

B Siêu tham số mô hình

Bảng 7 cho thấy siêu tham số LONG T5 và COLT5,
bao gồm số lượng tham số. Cho LONG T5, chúng
tôi báo cáo số cho cấu hình TGlobal, match T5.1.1.
Lưu ý rằng số lượng tham số của COLT5 lớn hơn
do sử dụng tính toán có điều kiện. Tương tự các
kiến trúc tính toán có điều kiện khác như mixture-
of-experts, chi phí tính toán không nhất thiết tăng
theo số lượng tham số.

Chúng tôi sử dụng cùng bán kính cục bộ 127-token
cho COLT5 như LONG T5. Điều này dẫn đến cửa
sổ attention cục bộ w là 255 vì 127 token được
attend bên trái và 127 bên phải.

C Siêu tham số Chuẩn hóa Routing

Để chuẩn hóa điểm routing cho lựa chọn top-k
token có thể vi phân, chúng tôi sử dụng thuật toán
soft top-k lặp từ Lei et al. (2023) và Qian et al.

Mô hình Trung bình 16k in, 128 out 16k in, 512 out 16k in, 1024 out 8k in, 128 out
Enc Tot Enc Tot Enc Tot Enc Tot Enc Tot
LONG T5-B 77 136 84 98 84 165 84 296 27 39
COLT5-B 29 90 30 45 30 113 30 256 18 30
LONG T5-L 164 329 173 222 179 392 179 799 66 100
COLT5-L 70 201 73 103 73 250 73 578 45 69
LONG T5-XL 390 870 412 557 423 1081 423 2065 166 290
COLT5-XL 177 439 185 239 185 525 185 1253 115 163

Bảng 8: So sánh thời gian suy luận tổng và encoder mỗi mẫu (ms) cho các mô hình LONG T5 và COLT5 Base,
Large, và XL ở độ dài đầu vào và đầu ra khác nhau. Thời gian trung bình mỗi mẫu được tính như trung bình
có trọng số trên độ dài đầu vào và đầu ra, có trọng số theo số tác vụ trong đánh giá của chúng tôi sử dụng
setting tương ứng (4 cho 16k/128, 3 cho 16k/512, và một cho mỗi 16k/1024 và 8k/128).

--- TRANG 13 ---
Mô hình arXiv SummScreenFD QMSum GovRep
R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L
LONG T5-B 47.4 21.4 43.5 34.8 9.3 20.7 35.1 11.1 23.4 59.3 30.1 33.0
COLT5-B 47.5 21.3 43.6 35.6 9.7 21.0 34.6 10.9 23.0 60.2 31.0 32.8
LONG T5-L 47.9 21.7 43.8 35.3 9.1 20.8 35.9 12.0 24.1 61.4 32.5 34.1
COLT5-L 48.4 21.7 44.3 35.7 10.1 21.4 36.8 12.6 24.7 61.8 32.7 34.4
LONG T5-XL 48.2 21.8 44.1 36.6 10.3 21.5 37.0 12.5 24.7 61.8 33.2 34.8
COLT5-XL 48.4 22.0 44.3 36.3 10.0 21.5 37.4 13.0 25.1 62.2 33.3 34.9

Bảng 9: So sánh hiệu suất đầy đủ với số liệu Rouge-1, Rouge-2, và Rouge-L của các mô hình COLT5 và
LONGT5 Base, Large, và XL trên dev set tóm tắt. Kết quả dựa trên checkpoint tối đa hóa Rgm như trong Bảng 3.

(2022) với ϵ = 1.0 và 50 lần lặp. Trong huấn luyện
chúng tôi cho phép top 9k/8 token có trọng số
khác không thay vì chỉ top k để cung cấp tín hiệu
huấn luyện cải thiện nhẹ.

D Kết quả Thí nghiệm Bổ sung

Bảng 8 so sánh tốc độ suy luận LONG T5 và
COLT5 chi tiết hơn, tách encoder và tổng thời
gian mỗi mẫu. Vì COLT5 áp dụng tính toán có
điều kiện chỉ trong encoder, mức tăng tốc encoder
lớn hơn mức tăng tốc tổng thể, và mức tăng tốc
tổng thể lớn nhất cho độ dài đầu ra ngắn hơn.
Trade-off thậm chí thuận lợi hơn cho COLT5 khi
kết hợp với các tối ưu hóa decoder khác.

Bảng 9 cho thấy kết quả đầy đủ (Rouge-1, Rouge-2,
Rouge-L) cho các tập dữ liệu tóm tắt.

E Tài nguyên Tính toán

Cho pre-training chúng tôi thường sử dụng 128
chip TPUv4 cho Base và 256 chip TPUv4 cho
Large và XL. Pre-training mất khoảng 2.5 ngày
cho Base, 3.7 ngày cho Large, và 12.8 ngày cho XL.
Cho fine-tuning chúng tôi thường sử dụng 64, 128,
và 256 chip TPUv4 cho Base, Large, và XL tương
ứng, với thời gian huấn luyện thay đổi theo kích
thước tập dữ liệu.

F Phân tích Routing

Trong phần này chúng tôi xem xét kỹ hơn các cơ
chế routing trong COLT5. Có ba quá trình routing
trong mỗi lớp COLT5: (1) Routing của attention
key và value ("KV-routing"), (2) routing của
attention query ("Q-routing") và (3) routing của
token MLP ("MLP-routing"). Để đơn giản, chúng
tôi sẽ nói rằng một token được chọn, khi nó được
định tuyến đến alternative nặng (của MLP hoặc
attention). Chúng tôi quan tâm đến việc hiểu token
nào được chọn và liệu các cơ chế này chọn token
tương tự hay khác nhau trong mỗi lớp.

Token nào được chọn Chúng tôi chia token đầu
vào thành ba loại: (1) token câu hỏi, (2) token câu
trả lời (được tìm thấy qua simple normalized string
match của ground truth answer), và (3) token khác.
Hình 7 cho thấy tỷ lệ mỗi loại token được định
tuyến bởi mô hình COLT5-Large được fine-tune
trên TriviaQA dev set, theo lớp và thành phần
routing.

Trước đó chúng tôi đã chỉ ra rằng token câu hỏi
và câu trả lời có khả năng được chọn cao hơn,
nhưng tách quyết định routing theo lớp tiết lộ
các mẫu thú vị. Ở các lớp sớm, token câu hỏi và
câu trả lời

0 10 200.20.40.60.81
LớpTỷ lệ định tuyếnMLP
0 10 2000.20.40.60.8
LớpQuery
0 10 200.20.40.60.81
LớpKV
Khác Câu trả lời Câu hỏi

Hình 7: Tỷ lệ token được định tuyến cho câu trả lời (string match), câu hỏi, và token khác theo thành phần
routing và lớp cho mô hình COLT5 Large, trung bình trên các ví dụ trong TriviaQA dev set.

--- TRANG 14 ---
Điều tương tự đúng
cho các token xung quanh
câu trả lời đúng
("papageno" trong
ví dụ này). Câu hỏi được
định tuyến mạnh đến
alternative đắt đỏ
bởi các lớp cuối
của mô hình.

Hình 8: Visualization của trọng số routing token cho một số fragment của một ví dụ trên TriviaQA.

chỉ khiêm tốn có khả năng được chọn cao hơn,
với xác suất routing tăng mạnh ở các lớp sau và
đạt đỉnh ở lớp cuối. Điều này có ý nghĩa trực quan:
ở các lớp sớm mô hình chưa có cơ hội xác định
token nào và phần nào của tài liệu quan trọng.
Tuy nhiên, sự tăng không đơn điệu và có sự biến
đổi mạnh giữa các lớp. Sự biến đổi này có thể
ngụ ý rằng các lớp khác nhau tập trung vào các
loại token khác nhau, hoặc một số thành phần
routing không học thành công để xác định token
quan trọng.

Để có cái nhìn sâu sắc hơn về điều này, Hình 8
visualize routing trên hai fragment mẫu từ một
ví dụ TriviaQA (lưu ý rằng, cho độ dài đầu vào
lớn được sử dụng trong COLT5, chúng tôi không
hiển thị ví dụ hoàn chỉnh trong hình). Hai fragment
được hiển thị tương ứng với đầu ví dụ (nơi câu
hỏi được đặt), và phần context xung quanh câu
trả lời đúng. Chúng tôi đã thêm nền có màu vào
hình, trong đó mỗi kênh CMY được ánh xạ tới
trọng số KV-routing trong các lớp khác nhau của
mô hình. Cyan tương ứng với lớp 1, Magenta với
lớp 12, và Yellow với lớp 24. Như chúng ta có thể
thấy, câu hỏi và câu trả lời có màu vàng đậm,
cho thấy những token đó được chọn ở lớp cuối.

Tương quan giữa các quá trình routing. Bảng
10 cho thấy hệ số tương quan Pearson giữa trọng
số routing của các cơ chế routing khác nhau trong
mỗi lớp trong mô hình COLT5 Large (tương quan
MLP-routing với KV-routing, MLP-routing với
Q-routing, và KV-routing với Q-routing). Chúng
tôi hiển thị số cho cả checkpoint pre-trained, cũng
như mô hình được fine-tune trên TriviaQA. Như
chúng ta có thể thấy, routing của key/value và
routing của query có tương quan cao ở tất cả lớp
ngoại trừ hai lớp đầu, trong khi routing của token
trong MLP có tương quan thấp hơn với hai quá
trình kia. Thú vị là tương quan giữa MLP và
attention routing tăng ở các lớp cuối của mô hình.

Pre-trained Fine-tuned
MLP-KV MLP-Q KV-Q MLP-KV MLP-Q KV-Q
1 -0.06 -0.06 -0.09 -0.06 -0.09 -0.26
2 0.27 0.52 0.04 0.27 0.39 0.02
3 -0.05 -0.03 0.75 0.05 -0.01 0.69
4 0.05 0.09 0.76 0.18 0.14 0.72
5 0.02 -0.01 0.75 0.22 0.26 0.68
6 0.02 -0.01 0.78 0.31 0.33 0.70
7 0.02 0.00 0.73 0.26 0.27 0.70
8 0.00 -0.02 0.44 0.11 -0.07 0.29
9 0.13 0.11 0.74 0.36 0.40 0.70
10 -0.06 -0.08 0.08 -0.15 -0.15 0.12
11 -0.05 -0.07 0.31 -0.08 -0.03 0.18
12 -0.04 -0.08 0.27 0.03 0.00 0.28
13 -0.10 -0.09 0.87 -0.13 -0.03 0.72
14 -0.04 -0.05 0.76 -0.06 -0.12 0.67
15 0.53 0.64 0.69 0.51 0.55 0.67
16 0.08 0.12 0.63 0.06 0.57 0.24
17 0.28 0.30 0.65 0.27 0.32 0.69
18 0.28 0.02 0.84 0.31 0.20 0.76
19 0.45 0.77 0.59 0.19 0.38 0.64
20 0.30 0.39 0.64 0.38 0.47 0.62
21 0.05 -0.04 0.49 0.18 0.11 0.47
22 0.05 0.00 0.69 0.21 0.16 0.68
23 0.39 0.33 0.68 0.60 0.79 0.69
24 0.43 0.39 0.59 0.57 0.63 0.65

Bảng 10: Hệ số tương quan Pearson giữa trọng số
routing của các cơ chế routing khác nhau trong mỗi
lớp trong mô hình COLT5 Large. Chúng tôi hiển thị
số cho cả checkpoint pre-trained, cũng như mô hình
được fine-tune trên TriviaQA. Thanh màu xanh da
trời visualize tương quan dương, trong khi thanh
màu đỏ visualize tương quan âm.
