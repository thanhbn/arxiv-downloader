# 2309.16039.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/long-context/2309.16039.pdf
# Kích thước file: 1768476 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Mở rộng Ngữ cảnh Dài hiệu quả cho các Mô hình Nền tảng
Wenhan Xiong†∗, Jingyu Liu†, Igor Molybog,
Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta,
Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang,
Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan,
Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang∗, Hao Ma∗
GenAI, Meta

Tóm tắt
Chúng tôi trình bày một chuỗi các LLM ngữ cảnh dài hỗ trợ cửa sổ ngữ cảnh hiệu quả lên đến 32,768 token. Chuỗi mô hình của chúng tôi được xây dựng thông qua việc tiếp tục tiền huấn luyện từ LLAMA 2 với các chuỗi huấn luyện dài hơn và trên một bộ dữ liệu mà các văn bản dài được tăng cường lấy mẫu. Chúng tôi thực hiện đánh giá mở rộng về mô hình hóa ngôn ngữ, các tác vụ thăm dò ngữ cảnh tổng hợp, và một loạt các điểm chuẩn nghiên cứu rộng rãi. Trên các điểm chuẩn nghiên cứu, các mô hình của chúng tôi đạt được những cải thiện nhất quán trên hầu hết các tác vụ thông thường và những cải thiện đáng kể trên các tác vụ ngữ cảnh dài so với LLAMA 2. Đáng chú ý, với một quy trình điều chỉnh hướng dẫn tiết kiệm chi phí không yêu cầu dữ liệu hướng dẫn dài được chú thích bởi con người, biến thể 70B đã có thể vượt qua hiệu suất tổng thể của gpt-3.5-turbo-16k trên một bộ các tác vụ ngữ cảnh dài. Cùng với những kết quả này, chúng tôi cung cấp một phân tích sâu về các thành phần riêng lẻ của phương pháp. Chúng tôi đi sâu vào mã hóa vị trí của LLAMA và thảo luận về giới hạn của nó trong việc mô hình hóa các phụ thuộc dài. Chúng tôi cũng xem xét tác động của các lựa chọn thiết kế khác nhau trong quá trình tiền huấn luyện, bao gồm hỗn hợp dữ liệu và chương trình huấn luyện về độ dài chuỗi - các thí nghiệm ablation của chúng tôi gợi ý rằng việc có nhiều văn bản dài trong bộ dữ liệu tiền huấn luyện không phải là chìa khóa để đạt được hiệu suất mạnh, và chúng tôi xác minh thực nghiệm rằng tiếp tục tiền huấn luyện ngữ cảnh dài hiệu quả hơn và có hiệu quả tương tự so với tiền huấn luyện từ đầu với các chuỗi dài.

[Có một biểu đồ với trục x là "Độ dài ngữ cảnh" và trục y là "Tổn thất xác thực"]

Hình 1: Chúng tôi cho thấy rằng tổn thất xác thực của mô hình có thể được khớp như một hàm của độ dài ngữ cảnh: L(c) = (α/c)β+γ với một bộ α, β, γ khác nhau cho mỗi kích thước mô hình. Mối quan hệ luật lũy thừa này cũng cho thấy rằng độ dài ngữ cảnh là một trục quan trọng khác của việc mở rộng LLM và mô hình của chúng tôi có thể liên tục cải thiện hiệu suất khi chúng tôi tăng độ dài ngữ cảnh lên đến 32,768 token.

--- TRANG 2 ---
1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM), được huấn luyện với quy mô dữ liệu và tính toán chưa từng có, mang lại lời hứa về việc cải thiện căn bản cách chúng ta tương tác với thế giới số. Khi các LLM được triển khai nhanh chóng và tiếp tục phát triển thông qua việc mở rộng, chúng tôi hình dung những mô hình này sẽ phục vụ các trường hợp sử dụng phức tạp và phức tạp hơn, chẳng hạn như phân tích các tài liệu giàu kiến thức dày đặc, cung cấp năng lượng cho trải nghiệm chatbot chân thực và hấp dẫn hơn, và hỗ trợ người dùng trong các quy trình tạo ra lặp đi lặp lại như lập trình và thiết kế, v.v. Một tính năng quan trọng hỗ trợ sự phát triển này là khả năng xử lý hiệu quả các đầu vào ngữ cảnh dài.

Cho đến nay, các LLM có khả năng ngữ cảnh dài mạnh mẽ chủ yếu được cung cấp thông qua các API LLM độc quyền (Anthropic, 2023; OpenAI, 2023) và không có công thức mở để xây dựng mô hình ngữ cảnh dài có thể chứng minh hiệu suất downstream ngang bằng với các mô hình độc quyền này. Hơn nữa, các mô hình ngữ cảnh dài mã nguồn mở hiện có (Tworkowski et al., 2023b; Chen et al., 2023; Mohtashami and Jaggi, 2023; MosaicML, 2023b) thường không đạt được trong các đánh giá và chủ yếu đo lường khả năng ngữ cảnh dài với tổn thất mô hình hóa ngôn ngữ và các tác vụ tổng hợp, điều này không chứng minh một cách toàn diện hiệu quả của chúng trong các kịch bản thực tế đa dạng. Ngoài ra, các mô hình này thường bỏ qua sự cần thiết của việc duy trì hiệu suất mạnh trên các tác vụ ngữ cảnh ngắn tiêu chuẩn, hoặc bỏ qua các đánh giá hoặc báo cáo hiệu suất suy giảm (Peng et al., 2023; Chen et al., 2023).

Trong công việc này, chúng tôi mô tả cách tiếp cận của chúng tôi để xây dựng các LLM ngữ cảnh dài với hiệu suất vượt trội so với tất cả các mô hình mã nguồn mở hiện có. Chúng tôi xây dựng các mô hình bằng cách tiếp tục tiền huấn luyện từ các checkpoint LLAMA 2 với 400 tỷ token bổ sung được hình thành như các chuỗi huấn luyện dài. Trong số các chuỗi mô hình, các biến thể nhỏ hơn 7B/13B được huấn luyện với các chuỗi 32,768-token trong khi các biến thể 34B/70B với các chuỗi 16,384-token. Trái ngược với việc đánh giá hạn chế được thực hiện bởi các nghiên cứu hiện có, chúng tôi đánh giá mở rộng các mô hình của chúng tôi bằng cách sử dụng mô hình hóa ngôn ngữ, các tác vụ tổng hợp, và cũng có một loạt các điểm chuẩn thế giới thực bao gồm cả các tác vụ ngữ cảnh dài và ngắn. Về mô hình hóa ngôn ngữ, mô hình của chúng tôi chứng minh một hành vi mở rộng luật lũy thừa rõ ràng đối với độ dài ngữ cảnh. Hành vi mở rộng này, như được hiển thị trong Hình 1, không chỉ cho thấy khả năng của các mô hình chúng tôi có thể liên tục hưởng lợi từ nhiều ngữ cảnh hơn mà còn gợi ý rằng độ dài ngữ cảnh là một trục quan trọng khác của việc mở rộng LLM. Khi so sánh các mô hình của chúng tôi với LLAMA 2 trên các điểm chuẩn nghiên cứu, chúng tôi không chỉ quan sát thấy những cải thiện đáng kể trên các tác vụ ngữ cảnh dài mà còn những cải thiện khiêm tốn trên các tác vụ ngữ cảnh ngắn tiêu chuẩn, đặc biệt là về lập trình, toán học, và các điểm chuẩn kiến thức. Chúng tôi khám phá việc sử dụng một quy trình đơn giản và tiết kiệm chi phí để điều chỉnh hướng dẫn các mô hình dài được tiền huấn luyện liên tục của chúng tôi mà không cần bất kỳ dữ liệu được chú thích bởi con người nào. Kết quả cuối cùng là một mô hình chat có thể đạt được hiệu suất tổng thể mạnh hơn gpt-3.5-turbo-16k trên một loạt các điểm chuẩn ngữ cảnh dài bao gồm trả lời câu hỏi, tóm tắt, và các tác vụ tổng hợp đa tài liệu.

Trong phần còn lại của bài báo này, chúng tôi bắt đầu bằng việc trình bày cách tiếp cận tiền huấn luyện ngữ cảnh dài liên tục và một quy trình điều chỉnh hướng dẫn nhẹ, tiếp theo là các kết quả chi tiết về một loạt các tác vụ ngữ cảnh ngắn và dài. Để tạo điều kiện cho nghiên cứu tương lai, chúng tôi bổ sung kết quả của chúng tôi bằng một phần phân tích thảo luận về cách thiết kế mã hóa vị trí, phân phối độ dài của bộ dữ liệu và chương trình huấn luyện đóng góp vào hiệu suất cuối cùng. Cuối cùng, chúng tôi báo cáo các đánh giá an toàn có trách nhiệm, xác nhận rằng các mô hình của chúng tôi có thể duy trì phần lớn hiệu suất an toàn của chuỗi LLAMA 2 gốc.

2 Phương pháp
2.1 Tiền huấn luyện Liên tục
Huấn luyện với các chuỗi dài hơn có thể tạo ra chi phí tính toán đáng kể do các tính toán attention bậc hai. Đây là động lực chính của cách tiếp cận tiền huấn luyện liên tục của chúng tôi. Giả thuyết cơ bản rằng các khả năng ngữ cảnh dài tương tự có thể được học bằng cách tiếp tục tiền huấn luyện từ một mô hình ngữ cảnh ngắn sau đó được xác thực trong Phần 4.4 thông qua việc so sánh các chương trình huấn luyện khác nhau. Chúng tôi giữ nguyên kiến trúc LLAMA 2 gốc gần như nguyên vẹn cho việc tiền huấn luyện liên tục và chỉ thực hiện một sửa đổi cần thiết đối với mã hóa vị trí mà rất quan trọng để mô hình có thể attend dài hơn. Chúng tôi cũng chọn không áp dụng sparse attention (Child et al., 2019) trong công việc này, vì với chiều mô hình của LLAMA 2 70B (h = 8192), chi phí tính toán ma trận attention và tổng hợp giá trị chỉ trở thành nút thắt cổ chai tính toán khi độ dài chuỗi vượt quá 49,152 (6h) token (Narayanan et al., 2021).

Mã hóa Vị trí Thông qua các thí nghiệm sớm ở quy mô 7B, chúng tôi đã xác định một hạn chế chính của mã hóa vị trí (PE) của LLAMA 2 ngăn cản mô-đun attention tổng hợp thông tin của các token xa. Chúng tôi áp dụng một sửa đổi tối thiểu nhưng cần thiết trên mã hóa vị trí RoPE (Su et al., 2022) cho việc mô hình hóa ngữ cảnh dài - giảm góc xoay (được kiểm soát bởi siêu tham số "tần số cơ sở b"), điều này làm giảm hiệu ứng suy giảm của RoPE đối với các token xa. Trong Phần 4.1, chúng tôi chỉ ra rằng phương pháp đơn giản này vượt trội hơn một cách tiếp cận đồng thời (Chen et al., 2023) để mở rộng độ dài ngữ cảnh của LLAMA và cung cấp một giải thích lý thuyết về tính ưu việt của nó.

Hỗn hợp Dữ liệu Trên đầu mô hình hoạt động với PE đã được sửa đổi, chúng tôi tiếp tục khám phá các hỗn hợp dữ liệu tiền huấn luyện khác nhau trong Phần 4.2 để cải thiện khả năng ngữ cảnh dài, hoặc bằng cách điều chỉnh tỷ lệ dữ liệu tiền huấn luyện của LLAMA 2 hoặc thêm dữ liệu văn bản dài mới. Chúng tôi thấy rằng thường chất lượng của dữ liệu đóng một vai trò quan trọng hơn độ dài văn bản đối với việc tiền huấn luyện liên tục ngữ cảnh dài.

Chi tiết Tối ưu hóa Chúng tôi tiếp tục tiền huấn luyện các checkpoint LLAMA 2 với độ dài chuỗi tăng lên trong khi giữ cùng số lượng token mỗi batch như trong LLAMA 2. Chúng tôi huấn luyện tất cả các mô hình tổng cộng 400B token trong 100,000 bước. Với FLASH ATTENTION (Dao et al., 2022), có chi phí bộ nhớ GPU không đáng kể khi chúng tôi tăng độ dài chuỗi và chúng tôi quan sát khoảng 17% tổn thất tốc độ khi tăng độ dài chuỗi từ 4,096 lên 16,384 cho mô hình 70B. Đối với các mô hình 7B/13B, chúng tôi sử dụng learning rate 2e-5 và một lịch trình learning rate cosine với 2000 bước warm-up. Đối với các mô hình lớn hơn 34B/70B, chúng tôi thấy rằng quan trọng là phải đặt learning rate nhỏ hơn (1e-5) để có được tổn thất xác thực giảm đơn điệu.

2.2 Điều chỉnh Hướng dẫn
Việc thu thập các nhãn chứng minh và ưu tiên của con người để căn chỉnh LLM là một quá trình phức tạp và tốn kém (Ouyang et al., 2022; Touvron et al., 2023). Thách thức và chi phí còn rõ ràng hơn trong các kịch bản ngữ cảnh dài, thường liên quan đến luồng thông tin phức tạp và kiến thức chuyên môn, ví dụ, xử lý các tài liệu pháp lý/khoa học dày đặc, làm cho nhiệm vụ chú thích trở nên không tầm thường ngay cả đối với các chú thích viên có kỹ năng. Trên thực tế, hầu hết các bộ dữ liệu hướng dẫn mã nguồn mở hiện có (Conover et al., 2023; Köpf et al., 2023) chủ yếu bao gồm các mẫu ngắn.

Trong công việc này, chúng tôi thấy rằng một cách tiếp cận đơn giản và rẻ tiền tận dụng một bộ dữ liệu prompt ngắn lớn và đa dạng được xây dựng trước hoạt động đáng ngạc nhiên trên các điểm chuẩn ngữ cảnh dài. Cụ thể, chúng tôi lấy bộ dữ liệu RLHF được sử dụng trong LLAMA 2 CHAT và bổ sung nó với dữ liệu self-instruct dài tổng hợp (Wang et al., 2022) được tạo ra bởi chính LLAMA 2 CHAT, với hy vọng rằng mô hình có thể học được một bộ kỹ năng đa dạng thông qua lượng lớn dữ liệu RLHF và chuyển giao kiến thức đó sang các kịch bản ngữ cảnh dài qua dữ liệu self-instruct. Quá trình tạo dữ liệu tập trung vào các tác vụ định dạng QA: bắt đầu từ một tài liệu dài trong corpus tiền huấn luyện của chúng tôi, chúng tôi chọn một khúc ngẫu nhiên và nhắc LLAMA 2 CHAT viết các cặp câu hỏi-trả lời dựa trên thông tin trong khúc văn bản. Chúng tôi thu thập cả câu trả lời dài và ngắn với các prompt khác nhau. Sau đó, chúng tôi cũng áp dụng một bước tự phê bình nơi chúng tôi nhắc LLAMA 2 CHAT xác minh các câu trả lời do mô hình tạo ra. Cho một cặp QA được tạo ra, chúng tôi sử dụng tài liệu dài gốc (cắt bớt để phù hợp với độ dài ngữ cảnh tối đa của mô hình) như ngữ cảnh để xây dựng một instance huấn luyện.

Đối với dữ liệu hướng dẫn ngắn, chúng tôi nối chúng thành các chuỗi 16,384-token. Đối với dữ liệu hướng dẫn dài, chúng tôi thêm các token padding ở bên phải để các mô hình có thể xử lý từng instance dài riêng lẻ mà không cần cắt bớt. Trong khi điều chỉnh hướng dẫn tiêu chuẩn chỉ tính toán tổn thất trên các token đầu ra, chúng tôi thấy rằng đặc biệt có lợi khi cũng tính toán tổn thất mô hình hóa ngôn ngữ trên các prompt đầu vào dài, điều này mang lại những cải thiện nhất quán trên các tác vụ downstream (Phần 4.3).

--- TRANG 3 ---
[Bảng 1: Hiệu suất trên các điểm chuẩn ngữ cảnh ngắn tiêu chuẩn với các số liệu về Coding, Math, MMLU, Commonsense, OpenQA cho các mô hình LLAMA 2 và LLAMA 2 LONG các kích thước khác nhau]

[Bảng 2: So sánh với các mô hình đóng trên các tác vụ ngắn tiêu chuẩn]

3 Kết quả Chính
3.1 Đánh giá Mô hình Đã tiền huấn luyện
Tác vụ Ngắn Để làm cho các LLM ngữ cảnh dài trở nên hữu ích toàn cầu, một yêu cầu quan trọng là đảm bảo hiệu suất mạnh mẽ trên các tác vụ ngữ cảnh ngắn tiêu chuẩn. Chúng tôi xác minh hiệu suất của các mô hình trên một loạt các điểm chuẩn phổ biến theo công việc trước đó (Touvron et al., 2023). Các kết quả tổng hợp được hiển thị trong Bảng 1. Nhìn chung, chúng tôi quan sát thấy kết quả ngang bằng và, trong hầu hết các trường hợp, mạnh hơn so với LLAMA 2. Đáng chú ý, chúng tôi quan sát thấy kết quả cải thiện đáng kể về lập trình, toán học, và các tác vụ chuyên sâu kiến thức như MMLU. Như được hiển thị trong Bảng 2, mô hình của chúng tôi vượt trội hơn GPT-3.5 trên MMLU và GSM8k. Điều này trái ngược với một công việc trước đó (Chen et al., 2023) quan sát thấy sự suy giảm trên các tác vụ ngắn. Chúng tôi cho rằng những cải thiện này là do các FLOP tính toán bổ sung và kiến thức học được từ dữ liệu dài mới được giới thiệu.

Tác vụ Dài Khác với các công việc trước đó (Chen et al., 2023; Mohtashami and Jaggi, 2023) chủ yếu dựa vào perplexity và các tác vụ tổng hợp để đánh giá hiệu suất ngữ cảnh dài, chúng tôi thực hiện đánh giá ngữ cảnh dài bằng cách sử dụng các tác vụ ngôn ngữ thế giới thực. Chúng tôi đánh giá hiệu suất 0-shot trên NarrativeQA (Kočiský et al., 2018), 2-shot trên QuALITY (Pang et al., 2022) và Qasper (Dasigi et al., 2021), và 1-shot trên QMSum (Zhong et al., 2021). Số lượng shot được quyết định dựa trên độ dài mẫu trung bình của mỗi bộ dữ liệu (tức là, các mẫu trong Qasper và QuALITY thường ngắn hơn nhiều so với NarrativeQA). Chúng tôi tập trung vào các tác vụ kiểu QA này vì dễ dàng trong việc thiết kế prompt và các đánh giá tự động ít thiên vị hơn. Các prompt đầu vào được cắt bớt từ phía trái nếu các prompt vượt quá độ dài đầu vào tối đa của mô hình hoặc 16,384 token. Chúng tôi so sánh với các mô hình ngữ cảnh dài mã nguồn mở có sẵn trong Huggingface Transformers, cụ thể là Focused Transformer (Tworkowski et al., 2023a), YaRN (Peng et al., 2023), Xgen (Nijkamp et al., 2023), MPT (MosaicML, 2023b,a) và fork LLAMA 2 của Together (Together, 2023). Như được hiển thị trong Bảng 3, các mô hình của chúng tôi đạt được hiệu suất vượt trội so với các mô hình này. Ở quy mô 7B, chỉ có "Together-7B-32k" có thể phù hợp với hiệu suất của mô hình chúng tôi. Lưu ý rằng mô hình này không phải là một mô hình tự giám sát thuần túy và đã được finetuned bằng cách sử dụng một bộ dữ liệu có giám sát lớn để cải thiện kết quả few-shot của nó. Vì các biến thể 7/13B của các mô hình chúng tôi đã được huấn luyện với các chuỗi 32k-token, chúng tôi cũng thực hiện so sánh sử dụng độ dài prompt tối đa 32,768 và kết quả nhất quán, như được hiển thị trong Bảng 13.

Sử dụng Ngữ cảnh Hiệu quả Để xác thực rằng các mô hình của chúng tôi có thể sử dụng cửa sổ ngữ cảnh tăng lên một cách hiệu quả, trước tiên chúng tôi cho thấy trong Hình 2 rằng kết quả trên mỗi tác vụ dài cải thiện đơn điệu khi chúng tôi tăng độ dài ngữ cảnh. Lấy cảm hứng từ (Kaplan et al., 2020; Hoffmann et al., 2022), chúng tôi cũng thấy rằng tổn thất mô hình hóa ngôn ngữ của mô hình chúng tôi tuân theo một mối quan hệ mở rộng luật lũy thừa cộng hằng số với độ dài ngữ cảnh (Hình 1), gợi ý:

• Mô hình của chúng tôi tiếp tục cho thấy tăng trưởng hiệu suất (về tổn thất mô hình hóa ngôn ngữ) lên đến 32,768 token văn bản, mặc dù có hiệu suất biên giảm dần. Lấy mô hình 70B của chúng tôi làm ví dụ, nếu chúng tôi tăng gấp đôi độ dài ngữ cảnh, chúng tôi có thể mong đợi tổn thất được giảm bởi một hệ số 2^(-β) ≈ 0.7 cộng một hằng số cụ thể của mô hình (1-2^(-β))·γ.

--- TRANG 4 ---
[Bảng 3: So sánh với các mô hình ngữ cảnh dài mã nguồn mở trên các điểm chuẩn nghiên cứu]

[Hình 2: Hiệu suất trên các tác vụ ngữ cảnh dài khi độ dài ngữ cảnh tối đa của prompt tăng lên]

• Các mô hình lớn hơn có thể tận dụng ngữ cảnh hiệu quả hơn, được chỉ ra bởi giá trị β lớn hơn của các đường cong.

3.2 Kết quả Điều chỉnh Hướng dẫn
Chúng tôi kiểm tra mô hình điều chỉnh hướng dẫn của chúng tôi trên ZeroSCROLLS (Shaham et al., 2023) bao gồm 10 bộ dữ liệu ngữ cảnh dài trải dài từ tóm tắt, trả lời câu hỏi, đến các tác vụ tổng hợp đa tài liệu. Để so sánh công bằng, chúng tôi sử dụng cùng cấu hình (prompt, chiến lược cắt bớt, và độ dài tạo tối đa, v.v.) như được chỉ định bởi điểm chuẩn. Như được hiển thị trong Bảng 4, mà không sử dụng bất kỳ dữ liệu ngữ cảnh dài được chú thích bởi con người nào, mô hình chat 70B của chúng tôi có thể vượt trội hơn gpt-3.5-turbo-16k trên 7 trong số 10 tác vụ. Ngoài ra, chúng tôi chạy đánh giá trên sáu tác vụ dài mới được giới thiệu trong L-Eval (An et al., 2023) và một lần nữa quan sát thấy kết quả mạnh, như được hiển thị trong Bảng 17 trong Phụ lục. Chúng tôi thấy rằng mô hình finetuned đặc biệt tốt trong các tác vụ QA, đây là chủ đề chính của dữ liệu self-instruct. Chúng tôi mong đợi hiệu suất sẽ được cải thiện thêm nếu dữ liệu đa dạng hơn được sử dụng để finetuning.

Đáng chú ý là việc đánh giá các LLM ngữ cảnh dài là một nhiệm vụ không tầm thường. Các số liệu tự động được sử dụng trong các điểm chuẩn này bị hạn chế theo nhiều cách. Chẳng hạn, các tác vụ tóm tắt chỉ đi kèm với một bản tóm tắt ground-truth duy nhất và các số liệu khớp n-gram không nhất thiết phải phù hợp với sở thích của con người. Đối với các tác vụ QA và tổng hợp, nơi số liệu ít quan trọng hơn, việc cắt bớt ngữ cảnh đầu vào cũng có thể loại bỏ thông tin cần thiết để trả lời câu hỏi. Một lưu ý quan trọng khác là hầu hết các mô hình độc quyền không chia sẻ chi tiết dữ liệu huấn luyện của họ, điều này làm cho việc xem xét khả năng rò rỉ tiềm ẩn trong quá trình đánh giá điểm chuẩn công khai trở nên khó khăn.

[Bảng 4: Kết quả bảng xếp hạng ZeroSCROLLS ngữ cảnh dài]

3.3 Đánh giá Con người
[Hình 3: Sở thích con người về phản hồi mô hình với dữ liệu trò chuyện đa lượt và trả lời truy vấn tìm kiếm đa tài liệu]

Bổ sung cho kết quả điểm chuẩn đánh giá tự động, chúng tôi tiến hành đánh giá con người bằng cách hỏi các chú thích viên liệu họ có thích phản hồi từ mô hình điều chỉnh hướng dẫn của chúng tôi hay từ các mô hình độc quyền như MPT-30B-chat, GPT-4, GPT-3.5-turbo-16k, và Claude-2 về mặt hữu ích, trung thực và vô hại. Khác với các số liệu tự động, con người tốt hơn trong việc đánh giá chất lượng phản hồi mô hình cho các mô hình ngữ cảnh dài vì không gian lớn của các câu trả lời có thể chấp nhận. Chúng tôi tập trung vào hai kịch bản ứng dụng chính với tổng cộng 2,352 ví dụ. Đối với dữ liệu trò chuyện đa lượt, mỗi prompt là một lịch sử trò chuyện dựa trên đó mô hình cần tạo ra một phản hồi mạch lạc. Đối với ứng dụng trả lời truy vấn tìm kiếm đa tài liệu, mô hình được cung cấp một vài tài liệu có liên quan nhất được truy xuất từ một phiên tìm kiếm và truy vấn tìm kiếm tương ứng. Sau đó chúng tôi đánh giá mức độ tốt của các mô hình này có thể tận dụng thông tin (các tài liệu được truy xuất) để trả lời truy vấn đã cho. Mỗi ví dụ so sánh được đánh giá bởi 3 chú thích viên khác nhau. Tỷ lệ thắng tiêu chuẩn của mô hình chúng tôi so với mỗi mô hình được tính bằng cách lấy trung bình kết quả của mỗi ví dụ so sánh và điểm số cuối cùng cùng với khoảng tin cậy 95% được hiển thị trong Hình 3. Với rất ít dữ liệu hướng dẫn, mô hình của chúng tôi có thể đạt được hiệu suất cạnh tranh so với MPT-30B-chat, GPT-3.5-turbo-16k, và Claude-2. Đáng chú ý là đánh giá con người về các tác vụ ngữ cảnh dài hơn là thách thức và thường yêu cầu các chú thích viên được đào tạo tốt và có kỹ năng. Chúng tôi hy vọng nghiên cứu này không chỉ có thể đưa ra cảm giác về tiềm năng của mô hình điều chỉnh hướng dẫn của chúng tôi trên một số ứng dụng downstream ngữ cảnh dài mà còn động viên các nỗ lực tương lai trong việc phát triển các đánh giá tự động ngữ cảnh dài mạnh mẽ hơn.

4 Phân tích
Trong phần này, chúng tôi thực hiện các thí nghiệm ablation để biện minh cho các lựa chọn thiết kế của chúng tôi (tức là sửa đổi kiến trúc, hỗn hợp dữ liệu, và chương trình huấn luyện) và định lượng những đóng góp của chúng vào hiệu suất cuối cùng.

4.1 Mã hóa Vị trí cho Văn bản Dài
[Bảng 5: Perplexity xác thực của các mô hình với các biến thể mã hóa vị trí khác nhau]

Các thí nghiệm ban đầu của chúng tôi đã sử dụng một tác vụ tổng hợp "FIRST-SENTENCE-RETRIEVAL" để thăm dò cửa sổ ngữ cảnh hiệu quả của các mô hình đã tiền huấn luyện nơi chúng tôi đơn giản là nhắc mô hình trả về câu đầu tiên của đầu vào. Kết quả tác vụ ban đầu của chúng tôi gợi ý rằng, với kiến trúc LLAMA 2 gốc không được chạm vào, mô hình của chúng tôi không thể attend hiệu quả vượt quá 4,000-6,000 token ngay cả sau khi tiền huấn luyện liên tục ngữ cảnh dài mở rộng. Chúng tôi giả thuyết rằng nút thắt cổ chai này đến từ mã hóa vị trí RoPE được sử dụng trong chuỗi LLAMA 2, áp đặt một sự suy giảm nặng nề trên điểm số attention cho các token xa. Chúng tôi đề xuất một sửa đổi đơn giản đối với mã hóa RoPE mặc định để giảm hiệu ứng suy giảm - tăng "tần số cơ sở b" của RoPE từ 10,000 lên 500,000, về cơ bản giảm các góc xoay của mỗi chiều. Ý tưởng này cũng được đề xuất đồng thời trong cộng đồng Reddit r/LocalLLaMa và Rozière et al. (2023). Hiệu ứng của việc thay đổi tần số cơ sở được hiển thị trong Hình 4. Một cách tiếp cận đồng thời khác được gọi là "position interpolation" (PI) (Chen et al., 2023) đề xuất chia tỷ lệ tuyến tính các vị trí đầu vào sao cho các vị trí của token trong các chuỗi dài sẽ được ánh xạ tới phạm vi vị trí gốc của mô hình. Như được hiển thị bởi hình, nó cũng ngầm đạt được hiệu ứng giảm suy giảm.

Một quan sát thú vị khác từ việc trực quan hóa là RoPE giới thiệu "dao động" lớn trong các vùng tầm xa, điều này có thể không mong muốn đối với mô hình hóa ngôn ngữ (Sun et al., 2022). Để điều tra xem hiệu ứng này có làm tổn hại hiệu suất hay không, chúng tôi cũng khám phá một biến thể khác của mã hóa xoay được đề xuất gần đây, XPOS (Sun et al., 2022), làm mịn thành phần tần số cao. Lưu ý rằng XPOS với các tham số mặc định gặp phải cùng vấn đề suy giảm như RoPE và do đó, chúng tôi cũng áp dụng một bản sửa lỗi suy giảm tương tự cho XPOS.

Cụ thể, chúng tôi so sánh thực nghiệm các phương pháp sau: baseline RoPE, PI, RoPE với tần số cơ sở được điều chỉnh được đề xuất của chúng tôi (ký hiệu là RoPE ABF), và XPOS ABF (so sánh trực quan trong Hình 4). Chúng tôi báo cáo kết quả về 1) perplexity xác thực chuỗi dài trong Bảng 5 và Hình 5a, 2) tác vụ thăm dò ngữ cảnh FIRST-SENTENCE-RETRIEVAL trong Hình 5b, và 3) một số tác vụ ngữ cảnh thông thường đại diện trong Bảng 6 (để xác thực rằng các mô hình dài không suy giảm trên các tác vụ ngữ cảnh ngắn). Tất cả các biến thể mô hình được tiếp tục tiền huấn luyện từ checkpoint LLAMA 2 7B với 80B token bổ sung được tổ chức thành các chuỗi dài 32,768-token.

[Hình 4: Điểm số attention thô suy giảm cho các token xa của các biến thể mã hóa vị trí được khám phá]

[Bảng 6: Hiệu suất của các mô hình với các biến thể mã hóa vị trí khác nhau trên các điểm chuẩn ngữ cảnh ngắn tiêu chuẩn]

Nhìn chung, kết quả trên các đánh giá này gợi ý rằng RoPE ABF hoạt động tốt nhất trong số tất cả các biến thể được khám phá. Đặc biệt, chúng tôi thấy rằng RoPE ABF là biến thể duy nhất có thể duy trì hiệu suất của nó lên đến cửa sổ ngữ cảnh 32,768-token đầy đủ trên tác vụ FIRST-SENTENCE-RETRIEVAL. Chúng tôi cũng thấy rằng XPOS ABF với ít dao động hơn không dẫn đến tăng trưởng đáng kể, gợi ý rằng những hiện tượng này không có hại cho mô hình hóa ngôn ngữ. Trong khi XPOS được cho là có tính chất ngoại suy tốt hơn (Sun et al., 2022), chúng tôi thấy rằng, với việc sửa đổi tần số cơ sở, XPOS không ngoại suy tốt hơn RoPE (xem Phụ lục C). Ngoài kết quả thực nghiệm, chúng tôi cung cấp một phân tích lý thuyết về RoPE ABF và sự khác biệt của nó so với PI trong Phụ lục B. Chúng tôi lập luận rằng RoPE ABF phân phối các vector nhúng với độ chi tiết tăng lên khi so sánh với RoPE PI, làm cho mô hình dễ dàng hơn trong việc phân biệt giữa các vị trí. Đáng chú ý là khoảng cách tương đối giữa các vector nhúng có sự phụ thuộc tuyến tính vào tham số chính của RoPE PI và sự phụ thuộc logarit vào tham số chính của RoPE ABF, điều này trùng khớp với quan sát thực nghiệm của chúng tôi rằng tần số cơ sở không rất nhạy cảm và có thể dễ dàng được điều chỉnh dựa trên độ dài chuỗi tối đa.

4.2 Hỗn hợp Dữ liệu Tiền huấn luyện
Dữ liệu được sử dụng để tiếp tục tiền huấn luyện mô hình của chúng tôi kết hợp các bộ dữ liệu hiện có được sử dụng bởi LLAMA 2 và dữ liệu văn bản dài mới. Chúng tôi cũng điều chỉnh tỷ lệ hỗn hợp nguồn dữ liệu để tăng trọng lượng cho các mẫu dữ liệu dài. Các thí nghiệm ban đầu của chúng tôi với các mô hình 7B xác nhận những cải thiện đáng kể khi sử dụng hỗn hợp dữ liệu này cho các tác vụ ngữ cảnh dài, như được hiển thị trong hai hàng đầu tiên của Bảng 7. Trong phần này, chúng tôi nhằm mục đích điều tra nghiêm ngặt nguồn gốc của những cải thiện. Đặc biệt, chúng tôi quan tâm đến việc phân biệt hiệu ứng của phân phối độ dài dữ liệu và chất lượng của corpus chính nó.

Chúng tôi thực hiện hai ablation bổ sung sử dụng các bộ dữ liệu tiền huấn luyện của LLAMA 2: 1) chúng tôi loại bỏ dữ liệu văn bản dài khỏi bộ dữ liệu LLAMA 2 và tiếp tục tiền huấn luyện mô hình của chúng tôi với chủ yếu là các tài liệu ngắn; 2) chúng tôi tăng trọng lượng mẫu của dữ liệu văn bản dài hiện có để tương tự như tỷ lệ văn bản dài được sử dụng bởi mô hình mới được đề xuất. Thú vị là, ngay cả khi loại bỏ hầu hết các văn bản dài, mô hình vẫn có thể có được hầu hết lợi ích hiệu suất so với LLAMA 2. Chúng tôi cũng thấy rằng không có lợi thế rõ ràng và nhất quán khi chúng tôi tăng mạnh tỷ lệ dữ liệu dài (hàng thứ ba so với hàng thứ tư trong Bảng 7 và Bảng 8). Chúng tôi quan sát kết quả tương tự trên tác vụ FIRST-SENTENCE-RETRIEVAL như được hiển thị bởi Hình 7 trong Phụ lục.

[Bảng 7: So sánh các hỗn hợp dữ liệu tiền huấn luyện liên tục khác nhau trên các tác vụ ngữ cảnh dài]

Dựa trên các ablation trên, chúng tôi có thể thấy rằng việc điều chỉnh phân phối độ dài của dữ liệu tiền huấn luyện không mang lại lợi ích lớn. Tuy nhiên, khi chúng tôi đánh giá hiệu suất của các biến thể mô hình này trên các tác vụ ngữ cảnh ngắn tiêu chuẩn, chúng tôi thấy rằng hỗn hợp dữ liệu mới cũng dẫn đến những cải thiện lớn trong nhiều trường hợp, đặc biệt là các tác vụ chuyên sâu kiến thức như MMLU, như được hiển thị trong Bảng 8. Những kết quả này gợi ý rằng các LLM ngữ cảnh dài có thể được huấn luyện hiệu quả ngay cả với dữ liệu dài rất hạn chế và những cải thiện của dữ liệu tiền huấn luyện của chúng tôi so với dữ liệu được sử dụng bởi LLAMA 2 chủ yếu đến từ chất lượng của dữ liệu chính nó, thay vì sự khác biệt phân phối độ dài.

[Bảng 8: Hiệu suất tác vụ ngắn tiêu chuẩn của các mô hình ngữ cảnh dài với hỗn hợp dữ liệu tiền huấn luyện khác nhau]

4.3 Điều chỉnh Hướng dẫn
Chúng tôi khám phá các chiến lược khác nhau để điều chỉnh hướng dẫn mô hình ngữ cảnh dài đã tiền huấn luyện mà không yêu cầu bất kỳ dữ liệu dài có giám sát nào. Chúng tôi bắt đầu với việc chỉ finetuning các mô hình với dữ liệu hướng dẫn ngắn từ LLAMA 2 CHAT (được gọi là "RLHF V5" trong (Touvron et al., 2023)) và sau đó trộn với một số dữ liệu tiền huấn luyện để tránh quên việc tiền huấn luyện liên tục ngữ cảnh dài trước đó. Như được chứng minh trong Bảng 9, việc sử dụng chỉ dữ liệu hướng dẫn ngắn đã có thể tạo ra một mô hình dài tốt vượt trội đáng kể so với LLAMA 2 trên các tác vụ ngữ cảnh dài khác nhau. Trên đầu bộ dữ liệu này chỉ bao gồm các prompt ngắn, chúng tôi thấy rằng việc thêm dữ liệu tiền huấn luyện (tính toán tổn thất mô hình hóa ngôn ngữ trên toàn bộ chuỗi) có thể tăng cường thêm hiệu suất trên hầu hết các bộ dữ liệu. Lấy cảm hứng từ điều này, chúng tôi thêm tổn thất LM trên các đầu vào ngữ cảnh dài khi chúng tôi finetuning với dữ liệu self-instruct. Thủ thuật đơn giản này làm cho việc học trở nên ổn định hơn khi chúng tôi có độ dài đầu vào và đầu ra không cân bằng, điều này mang lại những cải thiện đáng kể trên hầu hết các tác vụ được kiểm tra (hai hàng cuối cùng của Bảng 9).

[Bảng 9: So sánh các hỗn hợp dữ liệu điều chỉnh hướng dẫn khác nhau]

4.4 Chương trình Huấn luyện
Tiền huấn luyện liên tục đã chứng minh hiệu quả trong các thí nghiệm của chúng tôi, nhưng một câu hỏi mở vẫn còn: liệu việc tiền huấn luyện từ đầu với các chuỗi dài có mang lại hiệu suất tốt hơn so với tiền huấn luyện liên tục? Trong phần này, chúng tôi nghiên cứu các chương trình huấn luyện khác nhau và cố gắng điều tra xem tiền huấn luyện liên tục có thể cung cấp hiệu suất cạnh tranh với ngân sách tính toán ít hơn hay không. Chúng tôi bắt đầu bằng việc tiền huấn luyện một mô hình 7B với độ dài chuỗi 32,768 từ đầu đến cuối. Sau đó chúng tôi khám phá các chương trình huấn luyện hai giai đoạn khác nhau nơi chúng tôi bắt đầu với độ dài chuỗi 4096 và chuyển sang 32,768 khi mô hình hoàn thành 20%, 40%, 80% của toàn bộ quá trình huấn luyện. Đối với tất cả các trường hợp, chúng tôi giữ cùng số lượng token huấn luyện tổng cộng và đảm bảo số lượng token mỗi bước gradient vẫn không đổi (4 triệu token) bằng cách điều chỉnh kích thước batch và độ dài chuỗi tương ứng.

Chúng tôi đánh giá các mô hình của chúng tôi trên các tác vụ QA văn bản dài được sử dụng trong Phần 4.2 và báo cáo perplexity của các mô hình cuối cùng trên các corpus xác thực khác nhau. Như được hiển thị trong Bảng 10 và Bảng 11, tiền huấn luyện liên tục từ các mô hình ngữ cảnh ngắn có thể dễ dàng tiết kiệm khoảng 40% FLOP trong khi gần như không có tổn thất hiệu suất. Những kết quả này cũng phù hợp với các đường cong tổn thất huấn luyện mà chúng tôi quan sát được từ mỗi lần chạy trong Hình 6 - các mô hình có thể nhanh chóng thích nghi với độ dài chuỗi tăng lên và đạt đến quy mô tổn thất tương tự.

[Bảng 10: So sánh các mô hình với chương trình huấn luyện khác nhau trên các tác vụ QA ngữ cảnh dài]

[Bảng 11: Đánh giá Perplexity của các mô hình với chương trình huấn luyện khác nhau trên ba bộ xác thực]

[Hình 6: Các đường cong tổn thất được làm mịn cho ablation chương trình huấn luyện]

5 An toàn AI
5.1 Đánh giá trên Các Điểm chuẩn An toàn
Mặc dù cho thấy hiệu suất xuất sắc trên nhiều tác vụ downstream, các mô hình ngôn ngữ lớn dễ tạo ra nội dung có hại, thông tin sai lệch, và thiên vị (Lin et al., 2021; Hartvigsen et al., 2022; Dhamala et al., 2021; Ji et al., 2023). Các mô hình ngôn ngữ ngữ cảnh dài có thể xử lý các đầu vào mở rộng trong cửa sổ ngữ cảnh của chúng, nhưng đồng thời, chúng cũng đối mặt với rủi ro jailbreak cao hơn, đặc biệt thông qua các phương tiện như prompt injection (Greshake et al., 2023). Trong phần này, chúng tôi đánh giá khả năng an toàn của mô hình điều chỉnh hướng dẫn bằng cách sử dụng ba điểm chuẩn học thuật tiêu chuẩn bao gồm TruthfulQA (Lin et al., 2021), ToxiGen (Hartvigsen et al., 2022), và BOLD (Dhamala et al., 2021), tương tự như (Touvron et al., 2023). Chúng tôi tập trung vào biến thể mô hình điều chỉnh hướng dẫn lớn nhất (tức là 70B) và so sánh kết quả của nó với cả các LLM mã nguồn mở (Falcon-instruct Almazrouei et al. (2023), MPT-instruct MosaicML (2023a)) và các LLM độc quyền (GPT-3.5, GPT-4 (OpenAI, 2023), Claude-2 (Anthropic, 2023)) trong Bảng 12.

Chúng tôi quan sát thấy rằng nhìn chung mô hình điều chỉnh hướng dẫn duy trì hiệu suất an toàn tương tự so với LLAMA 2 CHAT và an toàn hơn và ít thiên vị hơn so với các LLM mã nguồn mở khác như Falcon-instruct và MPT-instruct. An toàn AI là một lĩnh vực phức tạp và có thể cực kỳ khó khăn để đánh giá toàn diện tất cả các khía cạnh an toàn của mô hình điều chỉnh hướng dẫn với ba điểm chuẩn. Tuy nhiên, chúng tôi hy vọng phân tích của chúng tôi có thể đóng vai trò như một nghiên cứu thí điểm và cung cấp các tín hiệu định hướng về hiệu suất an toàn của các mô hình ngôn ngữ lớn ngữ cảnh dài, điều này không được thảo luận trong các công việc khác về cùng chủ đề (Tworkowski et al., 2023b; Ding et al., 2023; Chen et al., 2023). Hiện tại cộng đồng cũng thiếu các điểm chuẩn an toàn chuyên dụng để đánh giá mô hình ngôn ngữ lớn ngữ cảnh dài và chúng tôi dự định đầu tư vào hướng này trong công việc tương lai.

TruthfulQA Chúng tôi đánh giá mô hình điều chỉnh hướng dẫn trên TruthfulQA (Lin et al., 2021) để đánh giá tính thực tế của nó. Điểm chuẩn bao gồm 817 câu hỏi bao gồm 38 danh mục bao gồm sức khỏe, luật pháp, tài chính, và chính trị (Lin et al., 2021). Tương tự như (Touvron et al., 2023), chúng tôi sử dụng các prompt few-shot với 6 cặp QA ngẫu nhiên để tạo ra và sau đó tận dụng hai mô hình GPT-3 được finetuned để phân loại xem việc tạo ra có trung thực và thông tin hay không. Chúng tôi báo cáo phần trăm của các thế hệ vừa trung thực vừa thông tin như số liệu cuối cùng trong Bảng 12.

ToxiGen Chúng tôi đo lường độ độc hại của mô hình điều chỉnh hướng dẫn bằng cách sử dụng ToxiGen (Hartvigsen et al., 2022) nơi chúng tôi kiểm tra phần trăm các thế hệ độc hại và thù ghét chống lại 13 nhóm thiểu số. Theo (Touvron et al., 2023), chúng tôi lọc ra các prompt nơi các chú thích viên không đồng ý với nhau về nhóm nhân khẩu học mục tiêu. Chúng tôi sử dụng bộ phân loại ToxiGen mặc định được finetuned dựa trên RoBERTa (Liu et al., 2019) để đánh giá mức độ độc hại của đầu ra của mô hình. Chúng tôi báo cáo phần trăm các thế hệ độc hại trên tất cả các nhóm trong Bảng 12.

BOLD Bias in Open-Ended Language Dataset (BOLD) Dhamala et al. (2021) được sử dụng trong công việc này để định lượng mức độ thiên vị của các mô hình đối với những người từ các nhóm nhân khẩu học khác nhau. Bộ dữ liệu này bao gồm 23,679 prompt được trích xuất từ Wikipedia tiếng Anh bao gồm năm lĩnh vực bao gồm chủng tộc, giới tính, tôn giáo, ý thức hệ chính trị và nghề nghiệp với tổng cộng 43 nhóm phụ. Theo Touvron et al. (2023), chúng tôi loại trừ các prompt thuộc các nhóm phụ tôn giáo Hindu và Atheism vì chúng chỉ có 12 và 29 prompt tương ứng. Sau khi các thế hệ được suy luận từ mỗi mô hình, chúng tôi tận dụng Valence Aware Dictionary and Sentiment Reasoner (VADER) Hutto and Gilbert (2014) để thực hiện phân tích tình cảm với điểm số trong phạm vi từ -1 đến 1. Điểm số dương tương ứng với tình cảm tích cực đối với nhóm phụ được đề cập trong prompt và ngược lại. Điểm số tình cảm gần 0 chỉ ra tình cảm trung lập là mong muốn. Chúng tôi báo cáo điểm số tình cảm trung bình trên 43 nhóm phụ nhân khẩu học như số liệu cuối cùng cho BOLD trong Bảng 12.

[Bảng 12: Đánh giá các LLM được finetuned trên ba điểm chuẩn an toàn]

5.2 Các Bài tập Red Team
Hiện tại không có điểm chuẩn an toàn mã nguồn mở nào được thiết kế cho hiểu biết ngữ cảnh dài. Để đảm bảo rằng các mô hình an toàn trong các kịch bản sử dụng ngữ cảnh dài, chúng tôi đã thực hiện red team nội bộ để hiểu rõ hơn về lỗ hổng của mô hình chat của chúng tôi. Chúng tôi tấn công mô hình bằng cách đưa vào ngữ cảnh dài (ví dụ, các cuộc trò chuyện dài), tiếp theo là các prompt đối nghịch bao gồm các khu vực rủi ro bao gồm các hành vi bất hợp pháp và tội phạm (ví dụ, khủng bố, trộm cắp, và buôn bán người), các hành vi thù ghét và có hại (ví dụ, phỉ báng, tự hại, rối loạn ăn uống, và phân biệt đối xử), và lời khuyên không đủ tiêu chuẩn Touvron et al. (2023). Thông qua kiểm tra thủ công, chúng tôi không quan sát thấy rủi ro đáng kể so với LLAMA 2 CHAT Touvron et al. (2023). Chúng tôi dự định đầu tư nhiều hơn vào các vector tấn công mới chống lại các mô hình ngữ cảnh dài trong công việc tương lai.

6 Hạn chế
Chức năng Hạn chế. Mô hình được đề xuất trong bài báo này chưa được finetuned cho một loạt rộng các ứng dụng ngữ cảnh dài, chẳng hạn như viết sáng tạo yêu cầu đầu ra dài. Việc áp dụng các công thức căn chỉnh hiện có, ví dụ, RLHF, cho các kịch bản khác nhau là tốn kém và không tầm thường. Ngay cả các chú thích viên có kỹ năng cũng có thể gặp khó khăn với các chi tiết phức tạp trong các văn bản dày đặc. Về mặt này, chúng tôi coi việc phát triển các phương pháp căn chỉnh hiệu quả cho các LLM dài là một hướng rất có giá trị cho nghiên cứu tương lai.

Hiệu quả Tokenizer. Trong khi chuỗi mô hình được đề xuất có thể tiêu thụ ngữ cảnh lên đến 32,768 token, số lượng từ thực tế mà mô hình có thể nhận bị ảnh hưởng lớn bởi hành vi tokenizer. Tokenizer được sử dụng bởi chuỗi Llama có từ vựng tương đối nhỏ (32k ký hiệu) và thường tạo ra các chuỗi dài hơn so với các chuỗi được cung cấp bởi tokenizer của GPT-3.5 - chúng tôi quan sát tokenizer của chúng tôi thường tạo ra trung bình 10% token nhiều hơn. Ngoài ra, tokenizer chúng tôi sử dụng cũng không thể xử lý hiệu quả khoảng trắng, làm cho nó không hiệu quả để xử lý dữ liệu mã dài.

Ảo giác. Giống như các LLM khác, chúng tôi đã quan sát vấn đề ảo giác khi thử nghiệm mô hình được đề xuất. Trong khi vấn đề này phổ biến đối với các mô hình ngữ cảnh ngắn, việc giải quyết vấn đề này đối với các mô hình ngữ cảnh dài có thể rõ ràng hơn vì thông tin dày đặc mà chúng tiêu thụ và quá trình căn chỉnh không đầy đủ.

7 Kết luận
Chúng tôi trình bày một chuỗi các LLM ngữ cảnh dài tận dụng một cải tiến mã hóa vị trí đơn giản nhưng cần thiết và tiền huấn luyện liên tục để đạt được hiệu suất ngữ cảnh dài mạnh mẽ. Việc mở rộng ngữ cảnh dài của chúng tôi được thực hiện bằng cách tiếp tục tiền huấn luyện từ LLAMA 2 với 400B token bổ sung và vượt trội hơn LLAMA 2 trên cả các tác vụ ngữ cảnh ngắn và dài. Các mô hình của chúng tôi cũng chứng minh hiệu suất vượt trội so với các mô hình ngữ cảnh dài mã nguồn mở hiện có và so sánh thuận lợi với gpt-3.5-turbo-16k trên một bộ các tác vụ ngữ cảnh dài sau một quy trình điều chỉnh hướng dẫn đơn giản mà không cần giám sát của con người. Chúng tôi bổ sung kết quả của chúng tôi bằng một phân tích toàn diện, cung cấp những hiểu biết về ảnh hưởng của các yếu tố khác nhau bao gồm các sắc thái của mã hóa vị trí, hỗn hợp dữ liệu, và chương trình tiền huấn luyện đối với hiệu suất cuối cùng. Chúng tôi hy vọng nghiên cứu của chúng tôi có thể làm cho các LLM ngữ cảnh dài trở nên dễ tiếp cận hơn và tạo điều kiện cho những tiến bộ thêm trong lĩnh vực này.

8 Lời cảm ơn
Chúng tôi muốn cảm ơn Nikolay Bashlykov, Matt Wilde, Wenyin Fu, Jianyu Huang, Jenya Lee, Mathew Oldham, và Shawn Xu vì sự hỗ trợ vô giá của họ về dữ liệu, cơ sở hạ tầng, và các khía cạnh khác nhau của dự án này.

[Phần còn lại của tài liệu bao gồm Tài liệu tham khảo và các Phụ lục với các bảng và hình bổ sung]
