# 2310.01427.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/long-context/2310.01427.pdf
# File size: 776139 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ATTENTION SORTING COMBATS RECENCY BIASINLONG
CONTEXT LANGUAGE MODELS
A P REPRINT
Alex Peysakhovich∗Adam Lerer†
October 4, 2023
ABSTRACT
Current language models often fail to incorporate long contexts efficiently during generation. We
show that a major contributor to this issue are attention priors that are likely learned during pre-
training: relevant information located earlier in context is attended to less on average. Yet even
when models fail to use the information from a relevant document in their response, they still pay
preferential attention to that document compared to an irrelevant document at the same position. We
leverage this fact to introduce “attention sorting”: perform one step of decoding, sort documents
by the attention they receive (highest attention going last), repeat the process, generate the answer
with the newly sorted context. We find that attention sorting improves performance of long context
models. Our findings highlight some challenges in using off-the-shelf language models for retrieval
augmented generation.
1 Introduction
Recent techniques have allowed large language models (LLMs) to operate on extremely large context corresponding
to dozens of pages of text [ 5,20,30,7,19]. These long contexts can be populated with user history, documentation,
detailed model instructions, or few-shot exemplars, allowing the model to dynamically extract and integrate diverse
information to complete its task [ 24,23,26]. We will refer to such tasks as context augmented generation. A major use
case in practice is retrieval augmented generation (RAG) where in addition to receiving a query, the model receives
additional documents in context from a retriever system [ 14,21]. RAG is ubiquitous in applications such as question
answering, code completion, and summarization.
However, whether transformers can actually use long context effectively is a major area of study [ 29,28,18,15]. For
example [ 15] finds that LLMs are worse in a RAG task when the relevant document is placed in the middle of the
context. The authors mention that “observing a serial-position-like effect in LLMs is perhaps surprising, since the
self-attention mechanisms underlying Transformer LLMs is technically equally capable of retrieving any token from
their contexts.”
A likely culprit for this phenomenon is a mismatch between the task LLMs are trained on and context-augmented
generation tasks. Among the documents typically used to pre-train LLMs such as web pages, books, articles and code,
the most informative tokens for predicting a particular token are typically the most recent ones. During pre-training,
this induces a learned bias to attend to recent tokens. In addition, the rotary positional embedding (RoPE) scheme used
in the open source models we investigate has an inductive bias towards reduced attention at long distances [ 27] that may
make it even easier for these models to learn to attend preferentially to recent tokens. Extreme recency bias is not a good
prior for context augmented generation tasks where far away tokens may, in fact, contain very relevant information.
We study this hypothesis in the open-domain question-answering [ 14] environment using a ‘one-relevant-document-
many-irrelevant-distractors’ task (distractor QA). Distractor QA is a simple testbed for examining LMs’ ability to make
use of their context. Our task documents are one-paragraph biographies of fictional individuals generated by a language
model. The model is asked simple questions that can be readily answered from the document. The distractors are
∗alex.peys@gmail.com. Currently at Sutter Hill Ventures
†adam.lerer@gmail.com. Currently at Google DeepMindarXiv:2310.01427v1  [cs.CL]  28 Sep 2023

--- PAGE 2 ---
Attention Sorting Combats Recency Bias A P REPRINT
biographies of other fictional individuals. Our task maintains the structure of a typical QA task but, because all data is
generated, incorporates a useful feature that models cannot use their prior knowledge to help with the task. We call this
the SynthWiki task.
We study the performance of four small open source long-context models and three proprietary long-context models
released by OpenAI and Anthropic.
The open source models we study are TogetherComputer’s Llama-2-7B-32k, Llama-2-7B-32k-Instruct, YaRN Llama-2-
7B-64k [ 19], and WizardLM-tuned Code-Llama-7B [ 16]. They are all based on Llama-2 which pre-trained with a base
context window of size 4096 , but increase the context window via variations of positional encoding extension and fine
tuning. TogetherComputer’s models are of particular interest as they are fine tuned not only on long context next token
prediction but explicitly on long context QA.
We find that increasing the number of distractors degrades model performance across both small open source LLMs
and larger proprietary ones. The decrease varies across models with the smallest degradation in Claude-1-Instant and
Claude-2, a larger degradation in GPT-3.5 and TogetherLlama, and catastrophic degradation in the remaining open
source models (Figure 1).
Unlike the API-based proprietary models, the open source models give us access to the model internals. This allows us
to dive deeper into the cause of the degradation. We find that the closer a relevant document is placed to the generated
response in context, the more attention weight it receives. However, we still see that on average true documents receive
much more attention than distractors at the same position in context . This gives rise to the idea of “attention sorting”:
run one decode step and look at per-token attention weights averaged across layers and heads, sort the documents by
these attention scores, optionally repeat this process,3and finally run model generation with this newly sorted context.
We find that attention sorting greatly improves upon model QA accuracy in our task for all 3 models we study (Figure
5), and allows TogetherComputer’s Llama to match or exceed the performance of the best proprietary models on our
QA task. We hypothesize the higher accuracy and better re-sorting performance of TogetherLlama is because that
particular model has been specifically tuned for long context QA while WizardCoder and YaRN were not. Nevertheless,
we find that attention sorting still substantially helps the latter models - particularly with repeated application - more
than doubling their accuracy at 30k context length.
2 SynthWiki: A Synthetic Dataset for Long-Context Extractive QA
We are interested in studying context-augmented generation where an LLM receives a question qand a set of document
Dand outputs an answer a(q, D).This is sometimes called retrieval augmented generation or extractive open domain
QA.
We will study the simplest case where the model needs to extract just a single piece of relevant information. Here D
contains a document d∗which contains the answer to the question and other documents Ddistract =d1, . . . , d kwhich
do not contain any information related to the question. We will study how increasing the size of Ddistract affects LLM
performance.
A known issue with LLMs is that they tend to use knowledge priors from training even when the instruction asks the
model to only use information from the context [ 33]. In order to study long-context question answering in isolation, we
generate a synthetic dataset of documents that minimizes knowledge contamination from pre-training.
We generate our dataset using a madlib procedure: we first use GPT3.5 to generate combinations of nationalities, jobs,
and names for a fictional person. We have a set of 20question types (examples: “where did Xgo to school?”, “what
year was Xborn?” see Appendix A for full list). We sample 2such questions, we then use GPT4 to general a short
“one paragraph Wikipedia article” summary for this person which contains the answers to these two randomly sampled
questions. We have some cleaning logic to remove bad questions or poorly formatted replies. We call this dataset
SynthWiki and plan to release code to generate new documents to avoid future contamination. Our task can be thought
of as a natural language version of the ‘Little Retrieval Test’ [ 18] where models are, essentially, asked to retrieve a
value from a long key, value list.
For the experiments reported here, we generate 500random SynthWiki entries with 2questions per document. After
some minor cleanup we are left with 990entries in our dataset. An example biography is below:
3Intuitively, multiple passes are helpful because in each pass, the relevant document(s) will tend to have a higher attention
score than distractors at the same position, and will preferentially move towards the end of the context. Usually by two passes true
documents are within the ‘useable’ part of the model’s context.
2

--- PAGE 3 ---
Attention Sorting Combats Recency Bias A P REPRINT
Amalia Varela is a renowned Argentine economist, best known for her significant contributions
to fiscal and monetary policies in Argentina. Born in the bustling city of Buenos Aires, Varela
established herself as a passionate advocate for economic reform and sustainable development in
her home country. She has served as an adviser to various governmental and non-governmental
organizations, shaping Argentina’s economic landscape through her innovative theories and rigorous
research. A true football enthusiast at heart, Varela is also known for her unwavering support of
Racing Club, a well-respected football team based in her birth city. Throughout her illustrious career,
Varela’s fusion of economics and devotion to her favorite sports team have solidified her status as a
transformative figure in Argentine society.
This passage would be paired with questions like: “In which city was Amalia Varela born?” or “What is the name of
Amalia Varela’s favorite sports team?.” The average document length is 165tokens when using the Llama/CodeLlama
tokenizer. The format of the QA prompts follows closely to the model pages on HuggingFace and is shown in Appendix
B.
We use SynthWiki to generate Kexamples of (qi, ai, di).To construct the set Ddistractor we set a maximum context
sizeˆTand sample djforj ̸=iwithout replacement to fill the context size. Each of these documents are irrelevant
distractors for the true document. We randomize the order of Dwhen we present it to the model.
3 Performance of Long-Context LLMs on SynthWiki
0.250.500.751.00
0 10000 20000 30000
Total Context Size (Llama Tokens)QA Accuracy
ModelClaude−1−Instant
Claude−2GPT−3.5
TogetherLlama7BTogetherLlama7B−Instruct
WizardCodeLlama7BYarnLlama7B
Figure 1: Performance of all long-context models that we study on question answering degrades when the relevant
information is embedded in a long context of irrelevant distractor text.
We study the following set of models:
1.togethercomputer’s Llama-2-7B-32k model extends the base Llama-2 [ 30] context via position interpolation
of the original models. In addition, this model is specifically fine tuned on long context summarization and
QA tasks.
2.togethercomputer’s Llama-2-7B-32k-Instruct performs additional fine tuning on a different recipe of instruction
following, multi-turn chat, book summarization, and more long context QA.
3.Wizard Code Llama-2 7B is an instruction tuned version of Code LLama 2 [ 16]. The baseline Code Llama 2
already comes with an extended context up to 100k which is extended via long-context next token prediction.
We choose the Wizard version as it comes with explicit instruct tuning.
4.YaRN Llama-2-7b-64k [ 19] uses a novel method for extending context combined with next-token prediction
based fine tuning. We note that this model is notinstruction tuned. At the time of this writing, the authors
are not aware of any instruct-tuned versions of this model. We will update the paper as soon as they become
available.
3

--- PAGE 4 ---
Attention Sorting Combats Recency Bias A P REPRINT
We also look at some prioprietary models available through APIs:
1.GPT-3.5-turbo-16k is a 16k context version of OpenAI’s GPT 3.5. As of this writing, no details on how 16k
context is achieved are available.
2. Claude-2 and Claude-1-Instant are 100k context models from Anthropic. Here, again, details on architecture
are unavailable.
3.At the time of this writing, neither author had access to GPT-4-32k. We plan to update this paper when we are
able to access the API.
We use the production versions of these models as of September 2023.
We first see how accuracy at SynthWiki changes as we change the total context size by adding distractor documents.
For the open source models we evaluate on all 990questions (445 distinct documents), while for the API-based models
we evaluate a sample of 100random questions.
We report the context lengths for all models in terms of the Llama tokenizer for a fair comparison between models.
Since the GPT and Claude tokenizers have different vocabulary size than the Llama tokenizer, their contexts will, on
average, correspond to fewer tokens for the same text input.
We report results for context lengths of 1000 ,16,000, and 30,000(Llama) tokens. We fill the contexts by randomly
sampling distractor documents until the next sample would cross the maximum context we allow. The document order
is shuffled so the true document is in a random position. This gives us on average 5.5, 95, and 180 total documents,
respectively.
We remove quotations from model responses, and count a model response as correct if it contains an exact string match
to the true answer. Manual inspection of random samples finds that approximately 2% of correct answers are classified
as incorrect due to variation in possible ways to answer the question.
We see that in all models accuracy decreases with context length (Figure 1). All trends look as expected, except we see
that Together-Llama-Instruct seems to be less accurate in short contexts than the other 7B models but does significantly
better in longer contexts. It is unclear to us why this is.
In Figure 2 we plot accuracy as a function of the position of the true document in the context. We see a general
replication of the ‘lost in the middle’ results [ 15] which finds that LLMs prefer information in the beginning of the
contexts (primacy bias) and end (recency bias). However, we see that as context gets longer, and in togetherLlama in
particular, the primacy bias towards early documents becomes smaller and we see mostly the recency bias.
1000 context tokens 16000 context tokens 30000 context tokens
0.25 0.50 0.75 0.25 0.50 0.75 0.25 0.50 0.750.000.250.500.751.00
True Document Relative PositionAccuracy
Model TogetherLlama7B TogetherLlama7B−Instruct WizardCodeLlama7B YarnLlama7B
Figure 2: QA accuracy on SynthWiki as a function of the position of the relevant document in the context. We see a
replication of the ‘lost in the middle’[ 15] effect on this dataset in which accuracy is lower when the relevant information
is in the middle of a long context. The recency (information toward the end of the context) effect seems to be quite
general across models and context lengths. However, the primacy (first documents) effect seems to be less general.
4

--- PAGE 5 ---
Attention Sorting Combats Recency Bias A P REPRINT
In Figure 3 we plot model attentions during the first step of generation. We sum the post-softmax model attentions
across heads and document tokens to get an attention score for each document. We plot the attention scores for each
model based on document position and whether the document is the true document for the given question.
1000 context tokens 16000 context tokens 30000 context tokensDistractor True Document
0.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.000.020.040.06
0.000.020.040.06
Document Relative PositionAttention
model TogetherLlama7B TogetherLlama7B−Instruct WizardCodeLlama7B YarnLlama7B
Figure 3: Average attention weight by source token position for different context lengths, averaged over all layers and
attention heads. The attention weights are only computed for the first generated response token. At long context lengths,
all three models show a strong bias towards attending to the most recent tokens, as well as a weaker bias towards the
initial tokens. All models also attend much more strongly to relevant documents than distractor documents.
We again see a strong recency bias in average attention for longer contexts. But we also observe that the document
containing the answer generally has higher much higher attention than distractors at the same position in context.
4 Re-Sorting Documents by Attention
Our findings suggest a simple intervention: compute average document attention weights for the first step of decoding,
and then re-sort the documents, putting those with highest attention weights last. We illustrate this method in Figure 4.
Figure 6 shows the results of doing attention sorting in long contexts. On average, relevant documents move closer
to the end of the document list with each successive re-sort. We see that on average, a re-sorting iteration moves the
relevant document closer to the end of context but for documents far away from the end, one single sort may leave them
in the middle of the context.
This last fact motivates us to compute attention weights and re-sort documents in context multiple times4.
In the bottom panel of Figure 5 we see that attention sorting improves accuracy on SynthWiki. For TogetherLlama,
after2iterations we no longer gain much increase in QA performance. For other models we continue to see accuracy
increases all the way up to 5attention sorts, because the recency bias is so strong on these models that documents in the
middle of the context take multiple sorts to get moved to the end.
4Alternatively, it might be possible to correct for a per-position estimated attention bias, which would avoid multiple steps of
re-sorting, though it is not clear how general such a computation would be across tasks or even across multiple questions within the
same QA.
5

--- PAGE 6 ---
Attention Sorting Combats Recency Bias A P REPRINT
Figure 4: An illustration of the attention sorting procedure. Average per-document attention is computed for the first
generated response token, and then documents are sorted in context with the highest attention at the end. After k rounds
of this sorting procedure, the response is generated.
16000 context tokens 30000 context tokensClaude−1−Instant
Claude−2
GPT−3.5
TogetherLlama7B
TogetherLlama7B−Instruct
WizardCodeLlama7B
YarnLlama7B
Claude−1−Instant
Claude−2
TogetherLlama7B
TogetherLlama7B−Instruct
WizardCodeLlama7B
YarnLlama7B0.000.250.500.751.00
ModelSynthWiki AccuracyAttention
Sorts
0
1
2
3
4
5
Figure 5: The effect of attention sorting on SynthWiki. Attention sorting increases small model performance. For the
TogetherLlama-Instruct model, re-sorting recovers most of the performance degradation from long context and matches
the performance of Claude-2.
5 Some Other Stuff We Tried
We now discuss some other results that we found in our explorations which may be of interest to specialist readers.
First, we can break down the attention score weighting by layer. We see that the differential attention to true vs.
distractor documents happens mostly in the middle layers. It is not clear what to make of this fact. We did not find that
restricting our attention sorting calculation to middle layers improved performance noticeably. We report it here for the
interested reader.
We hypothesized that performance degradation comes not just from the positional bias (which attention sorting
addresses), but also a general dilution of attention. Since softmax attention weights are positive and sum to 1, having
many irrelevant documents monotonically increases noise that enters the representation. So we tried several methods of
explicit attention truncation.
6

--- PAGE 7 ---
Attention Sorting Combats Recency Bias A P REPRINT
original position [.5,.75) original position [.75,1]original position [0,.25) original position [.25,.5]
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.0001020304050
0500010000051015
0204060
Position After 1 SortDensity
Model TogetherLlama7B TogetherLlama7B−Instruct WizardCodeLlama7B YarnLlama7BEffect of Attention Sorting on True Document Position (30,000 token context)
Figure 6: The effect of attention sorting on true document position. Each panel shows the distribution of document
positions after one round of attention sorting for documents that began in a certain quartile of context position. In
general, the true document moves towards the end of the context, however, when the true document is towards the
beginning of the context it does not move all the way to the end in one shot. Attention sorting works to some extent for
all models, but those that that were trained on long-context QA (i.e. the Together models) attend more effectively across
the context and thus do better.
1000 context tokens 16000 context tokens 30000 context tokensDistractor True Document
0 10 20 300 10 20 300 10 20 300.000.050.100.15
0.000.050.100.15
Transformer LayerAttention
model TogetherLlama7B TogetherLlama7B−Instruct WizardCodeLlama7B YarnLlama7B
Figure 7: Average attention weight by document position for different context lengths. The attention score is for the
first decode token, and is averaged across all heads and all tokens within a document. Attention score differentiation
between true and distractor documents happens mostly in middle layers in all three Llama2-based models. (Attention
scores do not sum to 1 because the model may also attend to non-document tokens such as the question.)
7

--- PAGE 8 ---
Attention Sorting Combats Recency Bias A P REPRINT
We tried truncating the context to the top-K documents by average attention weight and found that this improved
performance somewhat with one re-sort but after multiple re-sorts, truncation no longer provided much benefit. However,
this could present an efficiency optimization, because re-sorting can be performed with a single step of decoding, and
then the full decode can be performed with a truncated context.
We also tried to directly truncate the attention distribution using top-k or nucleus sampling [ 11]. Unfortunately without
any fine-tuning in this regime this led to serious degradation of model performance.
6 Related Work
Long-Context Models Self-attention is quadratic in the sequence length so transformer variants that can operate in a
compute-efficient manner on long contexts is an active area of research, see [ 29] for a useful overview. Perhaps the
most important innovation enabling long context models was FlashAttention, which implements standard attention
but uses tiling and recomputation to avoid quadratic memory consumption and more efficiently utilize GPU memory
bandwidth[9].
Recently the open source community has found some tricks that allow extension of context length for open source
models like Llama2 (pre-trained with a 4K context length) at the fine-tuning stage, leading to the release of long-context
versions of these models [7, 19].
Recent work has shown that long-context models do not use the information in the middle of their context efficiently
[15]. Anthropic has also published guidelines for effective prompting of long-context models, which highlights that
important information should be placed at the end of context[ 1]. They suggest using a model scratchpad to pull
important information of context to the end, which is quite similar to attention sorting (although operating directly in
the space of text).
Retrieval-Augmented Generation Retrieval-augmented generation is a framework that augments parametric language
models with the ability to retrieve information from a non-parametric text database [ 14]. Typically, a retrieval model
selects passages from the database and places them in the context of the generator LM. The RAG framework affords
several advantages: access to a huge memory without increasing model flops or parameter count, the ability to locate
the provenance of a model response, and the ability to adapt on the fly to different settings (e.g. different codebases)
without model retraining.
RAG is typically formulated as the combination of a retrieval model that selects a set of documents to be placed in the
context of a generator model, that generates a response. Since the performance of the RAG system is bounded by the
recall of the retriever and thus the number of documents returned, larger context windows can naturally improve the
performance of these systems.
Prior work have demonstrated superior language modeling and question answering performance with fewer parameters
by utilizing a huge database of text (more than seen at training time) [14, 10, 13]. Retrieval-augmented LMs typically
perform especially well on open-domain QA, where an explicit knowledge DB (e.g. wikipedia) can drastically improve
performance [14].
Retrieval-like methods can conversely be used to improve long-context generation. The Memorizing Transformer
improves long-context language modeling by performing an approximate kNN lookup into a long context history based
on an attention-like mechanism, rather than directly attending to the full history [ 34]. This retrieval mechanism provides
superior long context performance to architectures that compress the long context into a fixed-size representation such
as [8]. In Memorizing Transformer, retrieval is formulated as a compute-efficient way to condition on a long context,
but our work suggests that the attention sparsity induced by retrieval may also improve the performance of the model.
Attention-Based Salience Attention weights have long been used to visualize the salience of different parts of a model’s
input on its predictions, including the original papers proposing the attention mechanism [ 2] (see [ 4] Sec. 3 for a
survey).
In the interpretability literature there is some disagreement about how well-suited attention weights are as explanatory
variables [ 31,25,3]. Attention weights have also been used as a saliency signal for various purposes across other
modalities including vision and neural models for biology [6, 35, 22].
While most retrieval systems are based on semantic similarity between document and query, some prior work has
suggested using attention distillation for a retrieval signal [ 12]. In that work, synthetic labels for query-document pairs
are generated from a language model based on the attention between query and document when placed in context, and
these labels are used to train a retriever model. Our attention sorting is closely related to this idea.
8

--- PAGE 9 ---
Attention Sorting Combats Recency Bias A P REPRINT
7 Conclusion and Limitations
In this work, we investigated a potential source of quality degradation on long-context tasks and demonstrated a cheap
inference-time trick - attention sorting - that helps address it.
Attention sorting has several limitations. Many long-context tasks do not map neatly to a set of permutable documents;
in these settings one might still want to move certain passages to the end of context but it’s less clear how to do so
while maintaining the prompt’s original semantics. In addition, in context augmented tasks such as code generation the
granularity (file, function, etc...) of the permutable elements is a priori unclear.
In addition, real long-context retrieval tasks typically have relevant rather than just random distractors. Such ‘hard’
distractors may actually be picked up by attention sorting. If we consider a RAG task where some documents may
contain ‘false facts’ or if distractor documents are designed adversarially then a pure-attention based retriever/re-ranker
will not work as well. Combining attention sorting with other retrieval objectives (eg. perplexity distillation, semantic
similarity, high quality filtering, factuality) is an interesting area for future work.
Nevertheless, the attention sorting trick is an interesting example of how a model can be leveraged to modify its own
context in a way that improves its ability to perform a task. While prior work such as chain of thought [ 32] and
scratchpads [ 17] work in the space of language, we use the attention weights as a signal. The idea of using a model to
iteratively refine its own context over multiple passes may also be useful for other situations.
While attention-sorting could be applied to an ‘off the shelf’ LLM, the fact that it works better with a model specifically
trained on long-context QA suggests that a more principled solution that will likely yield better gains in real RAG tasks
is a joint fine-tuning of the LLM model with its retriever. Thus, we suggest that an even more important direction for
future work is studying how to achieve this harmonizing efficiently.
8 Acknowledgements
Experiments were performed on hardware provided by Sutter Hill Ventures. We thank Arthur Szlam for helpful
feedback on an early version of this manuscript.
References
[1] Anthropic. Prompt engineering for claude’s long context window, 2023. Accessed: 2023-09-25.
[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align
and translate. arXiv preprint arXiv:1409.0473 , 2014.
[3]Jasmijn Bastings and Katja Filippova. The elephant in the interpretability room: Why use attention as explanation
when we have saliency methods? arXiv preprint arXiv:2010.05607 , 2020.
[4]Yonatan Belinkov and James Glass. Analysis methods in neural language processing: A survey. Transactions of
the Association for Computational Linguistics , 7:49–72, 2019.
[5]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020.
[6]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
End-to-end object detection with transformers. In European conference on computer vision , pages 213–229.
Springer, 2020.
[7]Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large
language models via positional interpolation. arXiv preprint arXiv:2306.15595 , 2023.
[8]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl:
Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 , 2019.
[9]Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient
exact attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344–16359, 2022.
[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language
model pre-training. In International conference on machine learning , pages 3929–3938. PMLR, 2020.
[11] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.
arXiv preprint arXiv:1904.09751 , 2019.
[12] Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. arXiv
preprint arXiv:2012.04584 , 2020.
9

--- PAGE 10 ---
Attention Sorting Combats Recency Bias A P REPRINT
[13] Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 ,
2020.
[14] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-
intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020.
[15] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172 , 2023.
[16] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei
Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint
arXiv:2306.08568 , 2023.
[17] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David
Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate
computation with language models. arXiv preprint arXiv:2112.00114 , 2021.
[18] Dimitris Papailiopoulos, Lee Kangwook, and Jy-yong Sohn. the-little-retrieval-test, 2023.
[19] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of
large language models. arXiv preprint arXiv:2309.00071 , 2023.
[20] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input
length extrapolation. arXiv preprint arXiv:2108.12409 , 2021.
[21] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083 , 2023.
[22] Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer protein language
models are unsupervised structure learners. Biorxiv , pages 2020–12, 2020.
[23] Ohad Rubin and Jonathan Berant. Long-range language modeling with self-retrieval. arXiv preprint
arXiv:2306.13421 , 2023.
[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint
arXiv:2302.04761 , 2023.
[25] Sofia Serrano and Noah A Smith. Is attention interpretable? arXiv preprint arXiv:1906.03731 , 2019.
[26] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 , 2023.
[27] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer
with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.
[28] Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models
actually use long-range context? arXiv preprint arXiv:2109.09115 , 2021.
[29] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint
arXiv:2011.04006 , 2020.
[30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023.
[31] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. Attention interpretability across
nlp tasks. arXiv preprint arXiv:1909.11218 , 2019.
[32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information
Processing Systems , 35:24824–24837, 2022.
[33] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang,
Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846 ,
2023.
[34] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint
arXiv:2203.08913 , 2022.
10

--- PAGE 11 ---
Attention Sorting Combats Recency Bias A P REPRINT
[35] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and
Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International
conference on machine learning , pages 2048–2057. PMLR, 2015.
11

--- PAGE 12 ---
Attention Sorting Combats Recency Bias A P REPRINT
A SynthWiki Dataset Construction
The SynthWiki dataset was constructed as follows:
First, a set of origins and jobs were constructed by querying GPT-3.5 with the following prompts:
Give me a list of 50 origins for people ( example : Canadian , Texan , European ,
Monagasque ), output it as a comma separated list . Output the list only ,
nothing else .
Give me a list of 50 jobs that people can become famous for ( example :
programmer , entrepreneur , taxi driver , basketball player ), output it as a
comma separated list . Output the list only , nothing else .
For each entry, a random origin and job was selected and a name for the person was generated with the following
prompt to GPT-3.5:
I am writing a novel , help me come up with a name for a famous { an_origin } {
a_job }. Do not output the name of someone who already exists . Output only the
name , no explanation .
Finally, for 500unique (name, origin, job) pair, GPT-4 was queried with the prompt:
Please write a one paragraph wikipedia article for a famous { origin } {job}
named { name }.
Make sure the article contains information that can answer the following
questions :
{ question1 }
{ question2 }
Output the article only , no extraneous explanation .
The two questions were selected randomly from the following list:
"In which city was { person } born ?",
" What year was { person } born ?",
" Where did { person } go to college ?",
" What is the name of { person }’s spouse ?",
" What is the name of the first company { person } worked at ?",
" What is the company { person } founded called ?",
" What is the title of the film { person } directed ?",
"Who is { person }’s idol ?",
" What is the name of { person }’s pet ?",
" What is { person }’s favorite color ?",
" Where did { person } go to high school ?",
" What is the name of { person }’s best friend ?",
" What is the title of { person }’s favorite movie ?",
"In what year did { person } get married ?",
" What is the title of { person }’s favorite book ?",
" What is the name of { person }’s first child ?",
" What is the name of { person }’s favorite sports team ?",
"In which country was { person } born ?",
" What was the title of { person }’s PhD thesis ?",
" What sport does { person } play ?",
The two questions used in the construction prompt are also those used in the QA task.
B SynthWiki Prompt Construction
The prompt for the Wizard model is given explicitly on the model repository. Note that the original prompt contains
‘Response’ instead of ‘Answer’. We found that the models had better accuracy when we used ‘Answer’ instead of
‘Response’, so these are the numbers we report.
12

--- PAGE 13 ---
Attention Sorting Combats Recency Bias A P REPRINT
Below is an instruction that describes a task . Write a response that
appropriately completes the request .
### Instruction
Here is some information you will use to answer a question . Some of the
information may be irrelevant .
### Information
DOCUMENT : { document }
DOCUMENT : { document }
DOCUMENT : { document }
...
### Question
{ question }
Please return only the answer to the question . Answer concisely .
### Answer
The prompt format for Together-Llama-Instruct is also given explicitly on the model repository.
[ INST ]
Here is some information you will use to answer a question . Some of the
information may be irrelevant .
### Information
DOCUMENT : { document }
DOCUMENT : { document }
DOCUMENT : { document }
...
### Question
{ question }
Please return only the answer to the question . Answer concisely .
[/ INST ]
13
