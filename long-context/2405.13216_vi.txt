# Trang bị Transformer với Khả năng Đọc Truy cập Ngẫu nhiên
cho Hiểu biết Ngữ cảnh Dài

Chenghao Yang* 1Zi Yang* 2Nan Hua2

## Tóm tắt

Mô hình hóa ngữ cảnh dài đặt ra một thách thức đáng kể cho các mô hình ngôn ngữ lớn (LLM) dựa trên transformer do độ phức tạp bậc hai của cơ chế tự chú ý và các vấn đề với việc ngoại suy độ dài do việc tiền huấn luyện chỉ trên các đầu vào ngắn. Các phương pháp hiện có giải quyết độ phức tạp tính toán thông qua các kỹ thuật như phân khúc văn bản, phương pháp hạt nhân, và chú ý có cấu trúc, và giải quyết các vấn đề ngoại suy độ dài thông qua mã hóa vị trí, tiếp tục tiền huấn luyện, và kỹ thuật dữ liệu. Những phương pháp này thường yêu cầu truy cập tuần tự vào tài liệu, đòi hỏi đọc từ token đầu tiên đến token cuối cùng. Chúng tôi cho rằng đối với việc đọc có mục tiêu các tài liệu dài, việc truy cập tuần tự như vậy là không cần thiết, và một mô hình được huấn luyện thành thạo có thể học cách bỏ qua hàng trăm token ít liên quan hơn. Lấy cảm hứng từ hành vi đọc của con người và các quan sát thực nghiệm hiện có, chúng tôi đề xuất truy cập ngẫu nhiên, một chiến lược đọc mới cho phép transformers xử lý hiệu quả các tài liệu dài mà không cần kiểm tra mọi token. Kết quả thực nghiệm từ các giai đoạn tiền huấn luyện, tinh chỉnh và suy luận xác nhận hiệu quả của phương pháp chúng tôi.

## 1. Giới thiệu

Ngữ cảnh dài đề cập đến một ngữ cảnh đầu vào vượt quá giới hạn độ dài tối đa,1 khiến việc xử lý trong một bước suy luận duy nhất trở nên bất khả thi. Các ví dụ về đầu vào ngữ cảnh dài

*Đóng góp ngang nhau1Đại học Chicago (Công việc được thực hiện tại Google với tư cách là nhà nghiên cứu sinh viên)2Google Research. Liên hệ: Chenghao Yang <yangalan1996@gmail.com>, Zi Yang <ziy@google.com>.

Công trình Sơ bộ. Bản quyền 2024 thuộc về (các) tác giả.

1Chúng tôi thừa nhận thành tựu gần đây trong việc xây dựng các mô hình có thể xử lý ngữ cảnh cực dài, chẳng hạn như Gemini 1.5 (Reid et al., 2024). Tuy nhiên, không thể mở rộng cửa sổ ngữ cảnh vô hạn mà không phân khúc nó tại một thời điểm nào đó, và chúng tôi tin rằng việc kết hợp phân khúc với chiến lược đọc truy cập ngẫu nhiên được đề xuất của chúng tôi có thể hiệu quả hơn so với việc đọc trực tiếp đầu vào như một tổng thể.

bao gồm nhiều trang web (Zhou et al., 2023; Deng et al., 2024), sách (Mou et al., 2021), kho lưu trữ mã (Jimenez et al., 2023) và lịch sử đối thoại (Yang & Ettinger, 2023).

Các chiến lược hiện có chủ yếu sử dụng mô hình truy cập tuần tự, trong đó cả trong quá trình huấn luyện và suy luận, mô hình xử lý các token của tài liệu theo thứ tự gốc, tuần tự của chúng (Dong et al., 2023; Huang et al., 2023). Các phương pháp này sử dụng các cơ chế khác nhau để giải quyết những thách thức như độ phức tạp bậc hai—thông qua xử lý theo khối (Qiu et al., 2020; Tay et al., 2020a; Liu et al., 2022; Ivgi et al., 2023; Mohtashami & Jaggi, 2024), chú ý có cấu trúc (Beltagy et al., 2020; Guo et al., 2022; Xiao et al., 2024; Han et al., 2023), và xấp xỉ tuyến tính (Choromanski et al., 2020; Peng et al., 2020; Ma et al., 2021; Nguyen et al., 2022)—hoặc để giảm thiểu các vấn đề ngoại suy độ dài bằng các kỹ thuật như nhúng vị trí xoay (Peng et al., 2023; Su et al., 2024) và huấn luyện liên tục (Xiong et al., 2023; Fu et al., 2024).

Mặc dù những phương pháp này đã được áp dụng rộng rãi và chứng minh hiệu quả, chúng coi mọi token là quan trọng như nhau và bỏ qua thực tế rằng đối với các truy vấn của người dùng, chỉ một phần nhỏ thông tin là có liên quan (Ding et al., 2020). Do đó, nhiều giải pháp được đề xuất vẫn bị ảnh hưởng bởi chi phí tính toán, đặc biệt trong các tương tác trực tuyến giữa con người và LLM. Các nghiên cứu gần đây trong sinh mô hình tăng cường bằng truy xuất (RAG) (Lewis et al., 2020; Shi et al., 2023) đã cố gắng giải quyết điều này bằng cách tích hợp một bộ truy xuất bổ sung để bỏ qua ngữ cảnh không cần thiết và truy xuất có chọn lọc các phần có liên quan. Tuy nhiên, do hiểu biết không đầy đủ về toàn bộ ngữ cảnh dài và thiếu các tín hiệu giám sát mạnh mẽ, hiệu suất tổng thể của các hệ thống đa mô-đun như vậy bị hạn chế đáng kể bởi khả năng của bộ truy xuất (Mou et al., 2021; Zhang et al., 2022). Hơn nữa, trong khi các hệ thống RAG này yêu cầu một top-K được định nghĩa trước cho bất kỳ truy vấn người dùng nào, khung làm việc của chúng tôi xác định động mẫu truy cập dựa trên truy vấn và ngữ cảnh.

Lấy cảm hứng từ các chiến lược quan sát được ở những độc giả con người thành thạo, những người tích cực tham gia với các văn bản dài bằng cách phát triển dự đoán về nội dung sắp tới và có chọn lọc bỏ qua các phần không liên quan (Paris et al., 1991; Pressley & Afflerbach, 2012), cùng với bằng chứng gần đây cho thấy rằng các mô hình lớn có thể vốn dĩ có được khả năng truy cập nội dung tại các vị trí tùy ý trong quá trình tiền huấn luyện (Fu et al., 2024), chúng tôi đề xuất Đọc Truy cập Ngẫu nhiên.2 Phương pháp này tận dụng thông tin độ phức tạp cục bộ như một tiêu chí để bỏ qua văn bản không cần thiết, từ đó nâng cao đáng kể hiệu quả đọc.

[Hình 1 - Minh họa cho chiến lược đọc truy cập ngẫu nhiên được đề xuất]

Kiến trúc được đề xuất được mô tả trong Hình 1. Khác với truy cập I/O truyền thống, trong đó mỗi khối đầu vào được cung cấp tuần tự cho mô hình, chúng tôi đề xuất xây dựng một máy chủ dữ liệu mới để xử lý quá trình I/O. Truy cập ngẫu nhiên mà chúng tôi đề xuất bao gồm việc mô hình truyền các thống kê có liên quan3 tới máy chủ dữ liệu, sau đó sử dụng cơ chế bỏ qua được thiết kế đặc biệt của chúng tôi (được nêu trong Phần 3) để xác định số lượng token cần bỏ qua và sau đó lấy một khối mới cho mô hình. Tùy chọn, chúng ta có thể sử dụng một mô-đun bộ nhớ4 để cung cấp ngữ cảnh bổ sung, giúp mô hình đưa ra quyết định bỏ qua tốt hơn và duy trì sự hiểu biết mạch lạc về văn bản đầu vào.

Thông qua các thí nghiệm rộng rãi, chúng tôi đã xác nhận hiệu

2Truy cập ngẫu nhiên thường được định nghĩa là khả năng truy cập các vị trí tùy ý, như trong Máy Truy cập Ngẫu nhiên (Knuth, 1997). Trong phạm vi của bài báo này, chúng tôi tập trung vào một phiên bản đơn giản hóa trong đó chúng tôi chỉ cho phép mô hình truy cập các vị trí tùy ý trong ngữ cảnh tương lai và không bao giờ nhìn lại, để đạt được hiệu quả tốt hơn.

3Trong bài báo này, chúng tôi chứng minh hiệu quả của đọc truy cập ngẫu nhiên bằng cách sử dụng mất mát entropy chéo theo token được gộp như các thống kê như vậy, mặc dù nghiên cứu trong tương lai có thể khám phá các số liệu thay thế.

4Điều này sẽ làm cho mô hình của chúng tôi tương tự như cơ chế chú ý mốc được đề xuất bởi (Mohtashami & Jaggi, 2023). Chúng tôi sẽ trình bày chi tiết về sự khác biệt giữa công trình của chúng tôi và của họ trong Phần 2 và thảo luận về việc triển khai mô-đun bộ nhớ trong Phần 3.

quả của phương pháp được đề xuất của chúng tôi. Các phát hiện của chúng tôi bao gồm:

1. Cơ chế bỏ qua nâng cao hiệu suất mô hình trong tiền huấn luyện mô hình ngôn ngữ ngữ cảnh dài.

2. Tiền huấn luyện văn bản ngắn truyền thống làm giảm hiệu suất mô hình xuống dưới mức của một mô hình được khởi tạo ngẫu nhiên. Tuy nhiên, với việc tinh chỉnh của chúng tôi sử dụng cơ chế bỏ qua, một mô hình văn bản ngắn có thể được thích ứng thành công để xuất sắc trong các nhiệm vụ ngữ cảnh dài, vượt trội hơn cả mô hình truy cập ngẫu nhiên của chúng tôi được tiền huấn luyện từ đầu.

3. Việc tích hợp một mô-đun bộ nhớ cho phép mô hình truy cập ngẫu nhiên của chúng tôi đạt được những cải thiện đáng kể hơn nữa, vượt qua các mô hình dựa trên bộ nhớ hiện đại nhất trước đây trong tiền huấn luyện (Wu et al., 2021) chỉ với 26% thời gian huấn luyện.

4. Đánh giá mô hình truy cập ngẫu nhiên của chúng tôi trên các nhiệm vụ hạ lưu bằng một nhiệm vụ TriviaQA (Joshi et al., 2017) được sửa đổi, trong đó tất cả bằng chứng được truy xuất từ tập dữ liệu gốc được nối lại để tạo ra một kịch bản ngữ cảnh dài cực kỳ thách thức—chứng minh rằng cơ chế bỏ qua tích cực hơn tạo ra hiệu suất tốt hơn. Điều này xác nhận hiệu quả của phương pháp và cải thiện hiệu quả học tập.

## 2. Bối cảnh

Khi áp dụng cho các mô hình Transformer, các chiến lược đọc truy cập ngẫu nhiên được đề xuất của chúng tôi có thể giảm đáng kể các tính toán chú ý bằng cách bỏ qua nhiều token. Điều này phù hợp chặt chẽ với nghiên cứu về thưa thớt hóa chú ý sử dụng thông tin cục bộ để nâng cao hiệu quả và khả năng mô hình hóa ngữ cảnh dài. Đặc biệt, Liu et al. (2023) chứng minh sự tồn tại của thưa thớt ngữ cảnh có thể được tận dụng để tăng tốc tính toán. Chowdhury & Caragea (2023) minh họa hiệu quả của việc sử dụng cửa sổ trượt và phân rã có trọng số cho các nhiệm vụ hiểu ngữ cảnh dài. Ngoài ra, Fu et al. (2023) thấy rằng một mô hình tích chập đơn giản hoạt động tốt trên các nhiệm vụ Long Range Arena (Tay et al., 2020b). Chen et al. (2023) phát triển một cấu trúc phân cấp từ dưới lên sử dụng một LLM để đọc các tài liệu dài, cho phép điều hướng trực tiếp đến các phần tài liệu có liên quan đến các truy vấn cụ thể bằng cách theo một đường dẫn cây. Hơn nữa, Han et al. (2023) và Xiao et al. (2024) báo cáo rằng việc duy trì chú ý chỉ trên một vài token ban đầu (được gọi là "attention sinks"), với mỗi token chú ý chỉ đến các token lân cận của nó, có thể nâng cao đáng kể hiệu quả xử lý ngữ cảnh dài trong khi bảo toàn hiệu suất. Những công trình này cung cấp hỗ trợ cả thực nghiệm và lý thuyết cho việc thực hiện thưa thớt hóa chú ý, và transformer truy cập ngẫu nhiên của chúng tôi áp dụng một phương pháp tích cực hơn bằng cách trực tiếp bỏ qua nhiều token.

Khái niệm liên quan chặt chẽ nhất đến mô hình truy cập ngẫu nhiên của chúng tôi là cơ chế chú ý mốc được giới thiệu bởi Mohtashami & Jaggi (2023). Trong phương pháp này, một token mốc được thêm vào cuối mỗi khối có độ dài cố định để đại diện cho khối đó, và mỗi token chỉ chú ý đến một số lượng hạn chế các token mốc theo khối này. Mặc dù phương pháp này tạo điều kiện cho truy cập ngẫu nhiên cho mỗi token và giảm đáng kể yêu cầu bộ nhớ và tính toán, nó vẫn đòi hỏi việc chọn một kích thước khối không đổi—có thể thay đổi giữa các nhiệm vụ—đọc toàn bộ ngữ cảnh tuần tự (do đó giữ lại độ phức tạp bậc hai mặc dù được giảm bởi một hệ số không đổi), và yêu cầu triển khai các cấu trúc dữ liệu cụ thể và thuật toán truy xuất xấp xỉ để có hiệu quả. Ngược lại, phương pháp của chúng tôi tận dụng các thống kê tổng hợp để trực tiếp bỏ qua đến cửa sổ thông tin tiếp theo. Trong mỗi giai đoạn đọc, mô hình của chúng tôi sử dụng đầy đủ cửa sổ ngữ cảnh mà nó được huấn luyện, thay vì dựa vào kích thước khối được chỉ định thủ công, và tùy chọn tích hợp một mô-đun bộ nhớ (làm cho nó gần hơn với phương pháp "attention sink"). Điều này cho phép mô hình hóa tốt hơn tính mạch lạc văn bản và đạt được độ phức tạp gần tuyến tính con.5

## 3. Cơ chế Bỏ qua

Khi đọc một phần lớn của tài liệu, chúng tôi cho rằng ngay cả một mô hình ngôn ngữ được huấn luyện một nửa cũng có thể đã có dự đoán hợp lý cho một số phân đoạn tương lai, do đó có thể được bỏ qua một cách an toàn. Cơ chế bỏ qua được đề xuất rất đơn giản: Giả sử mô hình M bắt đầu đọc tại token thứ S, XS, của tài liệu X, và tiếp tục trong L(M) token—thường là giới hạn token tối đa cho mô hình, ví dụ, L(M) = 512. Việc đọc kết thúc tại token XS+L(M). Trong khoảng thời gian này, mô hình thực hiện tự chú ý trên span XS, . . . , XS+L(M) và tính toán mất mát entropy chéo theo token LS, . . . , LS+L(M). Sau đó chúng tôi áp dụng một phép toán gộp cho những mất mát này để ước tính độ tin cậy của mô hình trên đoạn văn này, được ký hiệu là C(X, S; M) = pooling(LS, . . . , LS+L(M)). Số liệu tin cậy này được sử dụng để xác định số lượng token, D(X, S; M), mà mô hình sẽ bỏ qua tiếp theo:

D(X, S; M) = min{K⌊|X| − S − L(M)/K⌋, ⌊α/C(X, S; M)⌋} (1)

Ở bước đọc tiếp theo, mô hình tiếp tục đọc từ token XS+L(M)+D(X,S;M), và quá trình bỏ qua này tiếp tục cho đến cuối tài liệu. Ở đây, K đại diện cho tỷ lệ bỏ qua và α là ngưỡng bỏ qua. Khi C(X, S; M) nhỏ so với α, nó cho thấy rằng

5Chúng tôi loại trừ độ phức tạp tính toán của quá trình token hóa vì nó tương đối nhẹ. Thực tế, để bỏ qua token hóa, phương pháp của chúng tôi cũng có thể hoạt động với mô hình Transformer dựa trên ký tự ngay lập tức, trong đó ngữ nghĩa của độ lệch giờ đây tự động có nghĩa là ký tự thay vì sentencepiece.

mô hình có hiểu biết mạnh mẽ về đoạn văn hiện tại, cho phép nó bỏ qua một cách an toàn với tỷ lệ K⌊α/C(X,S;M)⌋.

Theo trực giác, đối với tiền huấn luyện và tinh chỉnh, trong đó mất mát được tính toán theo mô hình teacher-forcing, C(X, S; M) đo trực tiếp mức độ mô hình có thể dự đoán các token vàng tiếp theo được cho ngữ cảnh trước đó. C(X, S; M) càng lớn, mô hình được huấn luyện một nửa càng ít tin cậy vào ngữ cảnh tương lai và do đó nên bỏ qua với tỷ lệ bảo thủ hơn. Trong quá trình suy luận, không có sự giám sát vàng, tình hình trở nên thách thức hơn. Tuy nhiên, các nghiên cứu trước (Dong et al., 2018; Kamath et al., 2020; Jiang et al., 2021) đã xác định mất mát log-probability như một phép đo thông thường về độ tin cậy của mô hình tại thời gian suy luận, mà chúng tôi dự đoán có thể rất thông tin cho việc đưa ra quyết định bỏ qua.

Sự đơn giản của heuristic này nhấn mạnh giá trị của nó: nó loại bỏ nhu cầu về các mô hình bổ sung để dự đoán số lượng token cần bỏ qua. Vì mất mát theo token được yêu cầu là đầu ra trực tiếp của bất kỳ mô hình ngôn ngữ tiêu chuẩn nào, cơ chế này có thể được tích hợp một cách liền mạch vào quá trình tiền huấn luyện mô hình ngôn ngữ bình thường. Hơn nữa, phương pháp của chúng tôi không dựa vào bất kỳ giả định cấu trúc hoặc biểu diễn trung gian nào; do đó, các phép toán bỏ qua không can thiệp vào các hoạt động mô hình đang diễn ra và tương thích với bất kỳ cấu trúc mô hình nào cung cấp đầu ra xác suất cho mỗi token. Để đơn giản, trong phạm vi của bài báo này, chúng tôi sử dụng mô hình Transformer tự hồi quy được công nhận rộng rãi.

Các thí nghiệm của chúng tôi chứng minh rằng bỏ qua cục bộ cải thiện sự đánh đổi hiệu quả-đa dạng trong mô hình hóa ngữ cảnh dài. Hơn nữa, tinh chỉnh dựa trên bỏ qua cho phép một checkpoint mô hình ngôn ngữ hiện có xử lý hiệu quả các kịch bản ngữ cảnh dài, vượt qua ngay cả các baseline dựa trên bộ nhớ chuyên biệt.

**Làm việc với Cơ chế Bộ nhớ** Cho độ dài ngữ cảnh vốn hạn chế của các mô hình ngôn ngữ hiện tại và bản chất không thể đảo ngược của cơ chế bỏ qua của chúng tôi, có khả năng đưa ra quyết định bỏ qua mà không có đủ ngữ cảnh. Hơn nữa, nghiên cứu trước đây chỉ ra rằng việc duy trì chú ý trên các token ban đầu và lân cận là rất quan trọng để hiểu ngữ cảnh mở rộng (Han et al., 2023; Xiao et al., 2024); do đó, tập trung vào những token này có thể nâng cao đáng kể hiệu suất. Để tăng cường khả năng xử lý ngữ cảnh của mô hình và bỏ qua có thông tin, chúng tôi giới thiệu một cơ chế bộ nhớ tùy chọn vào khung làm việc của chúng tôi. Trong nghiên cứu này, chúng tôi triển khai mô hình Attendre, như được đề xuất trong (Yang & Hua, 2024), sử dụng chiến lược đuổi bộ nhớ First-In First-Out (FIFO) cùng với thuật toán truy xuất Key-Value Top-K xấp xỉ. Khác với các mô hình transformer tăng cường bộ nhớ truyền thống như Memorizing Transformer (Wu et al., 2021), phương pháp của chúng tôi có một bể bộ nhớ toàn cục thực hiện truy xuất Key-Value Top-K trên cả cửa sổ đọc hiện tại và các mục bộ nhớ trong quá khứ ở mọi lớp, thay vì bị giới hạn trong một lớp trung gian duy nhất.

**Mở rộng cho Tài liệu Có cấu trúc** Phương pháp của chúng tôi có thể được củng cố thêm bằng cách tận dụng các cấu trúc phân cấp (ví dụ, cây DOM cho một trang web hoặc mục lục trong một cuốn tiểu thuyết). Chúng ta có thể hạn chế việc bỏ qua xảy ra trong các cây con, thực hiện bỏ qua đồng thời cho nhiều phần của tài liệu, và tổng hợp thông tin toàn cục. Phương pháp này phù hợp chặt chẽ hơn với phương pháp chú ý mốc (Mohtashami & Jaggi, 2023), với kích thước khối không đều và ngữ nghĩa khối mạch lạc hơn. Ngay cả không có chỉ mục được xây dựng sẵn, tóm tắt đệ quy có thể được sử dụng để xây dựng chỉ mục như vậy từ đầu, như được chứng minh trong (Chen et al., 2023). Trong công trình này, trọng tâm của chúng tôi là giới thiệu hiệu quả của cơ chế bỏ qua được đề xuất mà không giả định bất kỳ cấu trúc nào trong tài liệu đầu vào. Do đó, chúng tôi để lại việc khám phá thêm trong lĩnh vực này cho nghiên cứu tương lai.

## 4. Mô hình Ngôn ngữ Ngữ cảnh Dài

Đầu tiên chúng tôi đánh giá hiệu quả của phương pháp truy cập ngẫu nhiên của chúng tôi trong tiền huấn luyện trên một nhiệm vụ mô hình ngôn ngữ ngữ cảnh dài—cụ thể, dự đoán token tiếp theo trong một diễn ngôn dài. Chúng tôi chọn corpus C4 (Raffel et al., 2020) cho nhiệm vụ của mình và lọc ra các tài liệu có ít hơn 4.000 token, tạo ra cùng một tập con corpus C4 (4k+) như được sử dụng trong (Wu et al., 2021). Để xác minh hiệu quả của cơ chế bỏ qua được đề xuất của chúng tôi, chúng tôi điều tra hai kịch bản ứng dụng: 1) Tiền huấn luyện: Giả sử có đủ tài nguyên tính toán, chúng tôi nhằm tiền huấn luyện một mô hình từ đầu có thể quản lý ngữ cảnh dài, trong đó việc sử dụng bỏ qua như một chiến lược cung cấp dữ liệu có thể nâng cao hiệu suất mô hình. 2) Tinh chỉnh: Trong các tình huống thực tế mà tiền huấn luyện không khả thi, chúng tôi xem xét liệu cơ chế bỏ qua có thể phục vụ như một chiến lược tinh chỉnh để tăng cường khả năng của mô hình ngữ cảnh ngắn trong việc xử lý ngữ cảnh dài.

### 4.1. Thiết lập Nhiệm vụ

**Tiền huấn luyện** Chúng tôi tiền huấn luyện một mô hình transformer 12 lớp qua 200.000 bước với kích thước batch 128 và giới hạn token tối đa L(M) = 512. Trong thí nghiệm này, chúng tôi không sử dụng cơ chế bộ nhớ để cô lập và đánh giá trực tiếp hiệu quả của cơ chế bỏ qua. Chúng tôi đặt các tỷ lệ bỏ qua khác nhau K = 0, 1, 256, 100.000 cho các lần chạy khác nhau. Để đánh giá, chúng tôi vô hiệu hóa việc bỏ qua để đảm bảo so sánh công bằng.

**Tinh chỉnh** Chúng tôi sử dụng một mô hình ngôn ngữ được tiền huấn luyện trên corpus C4 (văn bản ngắn), cụ thể để dự đoán từ tiếp theo qua 100M bước, không sử dụng cơ chế bỏ qua. Chúng tôi sử dụng cùng kiến trúc mô hình như trong nhiệm vụ tiền huấn luyện. Trong quá trình tiền huấn luyện văn bản ngắn này, chúng tôi tuân theo chiến lược xử lý dữ liệu thông thường ("phân khúc ngẫu nhiên và xáo trộn") để nối tất cả tài liệu, chia toàn bộ corpus thành các khối kích thước L(M) và xáo trộn ngẫu nhiên các khối (Devlin et al., 2019; Raffel et al., 2020). Tinh chỉnh sau đó được thực hiện qua 25.000 bước trên tập huấn luyện C4 (4k+) được sử dụng trong Nhiệm vụ tiền huấn luyện. Chúng tôi báo cáo các tỷ lệ bỏ qua K = 0, 256 để đơn giản.

**Số liệu** Chúng tôi áp dụng perplexity (PPL) làm số liệu như trong các nghiên cứu trước về mô hình ngôn ngữ ngữ cảnh dài.

### 4.2. Kết quả Thí nghiệm

**Bỏ qua đạt được kết quả tiền huấn luyện ngữ cảnh dài tốt hơn.** Kết quả tiền huấn luyện được minh họa trong Hình 2. Với thời gian huấn luyện đủ, một tỷ lệ bỏ qua trung gian (K=256) giảm đáng kể perplexity so với không bỏ qua (K=0), chứng minh hiệu quả của cơ chế bỏ qua trong tiền huấn luyện ngữ cảnh dài. Ở tỷ lệ bỏ qua rất cao (tức là K=100.000), tương quan giữa các tài liệu được đọc liên tiếp giảm, gần giống với hiệu ứng của việc xáo trộn đầu vào ngẫu nhiên. Chúng tôi thấy rằng bỏ qua ở tỷ lệ vừa phải vượt trội hơn cả chiến lược bỏ qua cực nhanh và không bỏ qua. Do đó, cơ chế bỏ qua của chúng tôi không chỉ duy trì tính mạch lạc tốt hơn giữa các đoạn văn được phân khúc mà còn nâng cao sự tiếp xúc của mô hình với các token độc đáo, từ đó cải thiện khả năng tổng quát so với các chiến lược tiền huấn luyện phân khúc liên tiếp và xáo trộn ngẫu nhiên.

**Sử dụng bỏ qua trong tinh chỉnh có thể thích ứng thành công mô hình ngôn ngữ văn bản ngắn thành mô hình ngữ cảnh dài tốt hơn.**

Chúng tôi trình bày kết quả tinh chỉnh trong Hình 3. Ban đầu, sau khi tiền huấn luyện trên văn bản ngắn, mô hình thể hiện hiệu suất tệ hơn (ppl=831.31) so với mô hình được khởi tạo ngẫu nhiên (ppl=366.50) trên mô hình ngôn ngữ ngữ cảnh dài. Điều này chỉ ra rằng các chiến lược tiền huấn luyện văn bản ngắn truyền thống không tổng quát tốt cho các kịch bản ngữ cảnh dài. Tuy nhiên, bằng cách áp dụng việc bỏ qua thích hợp trong quá trình tinh chỉnh, chúng ta có thể thích ứng hiệu quả một mô hình được tiền huấn luyện văn bản ngắn để đạt được perplexity thậm chí tốt hơn (ppl=16.97, K=256) so với mô hình ngữ cảnh dài tốt nhất của chúng tôi được huấn luyện từ đầu (ppl=35.59). Hơn nữa, so với tinh chỉnh tiêu chuẩn trên corpus ngữ cảnh dài (ppl=19.10, K=0), tinh chỉnh với cơ chế bỏ qua được đề xuất của chúng tôi đạt được những cải thiện đáng kể, xác nhận hiệu quả của nó trong việc nâng cao khả năng mô hình trong môi trường ngữ cảnh dài.

Giải thích của chúng tôi là "phân khúc ngẫu nhiên và xáo trộn" phổ biến trong tiền huấn luyện văn bản ngắn truyền thống (Devlin et al., 2019; Raffel et al., 2020) làm suy yếu khả năng đọc liên tục của mô hình—ngữ cảnh đầu vào thay đổi quá nhanh đến mức mô hình thiếu cơ hội để nắm bắt tính mạch lạc văn bản một cách hiệu quả. Cơ chế bỏ qua có thể giảm thiểu điều này bằng cách giúp khôi phục các phụ thuộc dài hạn và tổng quát hiệu quả hơn kiến thức được tiền huấn luyện trong mô hình ngôn ngữ ngữ cảnh dài.

**Mô hình dần học cách bỏ qua nhiều hơn.** Trong Hình 4, chúng tôi minh họa số lượng trung bình các token được bỏ qua trong quá trình huấn luyện. Đối với tất cả các kịch bản bỏ qua (K > 0), số lượng trung bình các token được bỏ qua tăng theo thời gian. Điều này cho thấy sự hiểu biết cải thiện của mô hình về ngữ cảnh dài của đầu vào, dẫn đến các quyết định bỏ qua tự tin và tích cực hơn. Bằng cách so sánh các đường cong khác nhau, chúng tôi quan sát rằng với tỷ lệ bỏ qua vừa phải (K=256) đạt được các mẫu bỏ qua mượt mà nhất. Điều này có thể giúp ổn định huấn luyện trong các giai đoạn sau và đạt được khả năng tổng quát tốt hơn. Mặc dù trường hợp K=100.000 cho thấy các mẫu tương tự, hiệu suất của nó trên nhiệm vụ tiền huấn luyện tệ hơn, cho thấy rằng mức độ bỏ qua (chủ yếu được kiểm soát bởi tỷ lệ bỏ qua K) là rất quan trọng và cần điều chỉnh cẩn thận.

**Lựa chọn pooling không quan trọng.** Chúng tôi cũng thử một số chiến lược pooling khác nhau – "exponential-decay", "chỉ sử dụng token cuối cùng trong khối hiện tại" và "average pooling". Có vẻ như tất cả các phương pháp pooling đều hoạt động tốt như nhau.

[Bảng 1: Thí nghiệm về việc tích hợp memory transformer trong tiền huấn luyện]

**Kết hợp cơ chế bộ nhớ với bỏ qua mang lại cải thiện thêm.** Chúng tôi thấy rằng việc sử dụng bỏ qua trong tiền huấn luyện có thể có lợi cho tiền huấn luyện transformer với cơ chế bộ nhớ ngay cả khi việc bỏ qua không được phép trong thời gian kiểm tra. Chúng tôi áp dụng một kiến trúc memorizing transformer cụ thể trong (Wu et al., 2021) (memory-size=1536, context-len=512, no-XL-cache) và thực hiện cùng một tiền huấn luyện bỏ qua trên C4 (4k+) như trên. Để so sánh công bằng, chúng tôi lại vô hiệu hóa việc sử dụng bỏ qua trong thời gian kiểm tra. Kết quả thí nghiệm được hiển thị trong Bảng 1. Chúng tôi thấy rằng memorizing transformer được tiền huấn luyện bỏ qua của chúng tôi có thể đạt được hiệu suất tốt hơn (ppl=14.15) ngay cả trong 130k bước tối ưu hóa đầu tiên so với memorizing transformer được tiền huấn luyện được báo cáo trong (Wu et al., 2021) (ppl=14.97, 500k bước), xác nhận phương pháp tiền huấn luyện của chúng tôi mang lại sự gia tăng hiệu suất đáng kể (-0.82 ppl) và hiệu quả học tập tốt hơn (hiệu quả hơn khoảng 3.84 lần). So với mô hình w/o memory được tiền huấn luyện bỏ qua của chúng tôi (ppl=35.59), việc trang bị với cơ chế bộ nhớ đạt được 60% gia tăng hiệu suất, chứng minh lợi ích to lớn của việc bao gồm cơ chế bộ nhớ trong tiền huấn luyện bỏ qua. Điều này được mong đợi vì bộ nhớ có thể giúp theo dõi những gì đã đọc cho đến nay một cách rõ ràng và giảm quá tải bộ nhớ từ không gian tham số trong thời gian huấn luyện.

## 5. Trả lời Câu hỏi Ngữ cảnh Dài

Bên cạnh tiền huấn luyện, chúng tôi cũng muốn đánh giá các mô hình đọc-bỏ qua của chúng tôi trên các nhiệm vụ ngữ cảnh dài hạ lưu để xác minh liệu đọc-bỏ qua có thể giúp ích. Chúng tôi đánh giá các mô hình của mình trên tập dữ liệu TriviaQA (Joshi et al., 2017) (phần "RC"). Chúng tôi nối tất cả bằng chứng được truy xuất trong đầu vào để chuyển đổi nhiệm vụ này thành một nhiệm vụ trả lời câu hỏi ngữ cảnh dài tương đối thách thức. Số lượng token trung bình cho tập dữ liệu kết quả là 23.706,21. Vì đầu vào giờ đây trở nên tương đối dài, chúng tôi thí nghiệm với chiến lược bỏ qua tích cực hơn bằng cách đặt K = 0, 1000, 10000 trong cả thời gian huấn luyện và kiểm tra, dẫn đến 3×3 = 9 tổ hợp. Chúng tôi lấy mẫu ngẫu nhiên 200 câu hỏi cho tập kiểm tra. Chúng tôi sử dụng cùng kiến trúc transformer như trong các thí nghiệm tiền huấn luyện. Đối với thí nghiệm này, chúng tôi tiền huấn luyện mô hình với cơ chế bộ nhớ được giới thiệu ở trên trên corpus C4.

Kết quả thí nghiệm được hiển thị trong Hình 5. Chúng tôi quan sát rằng các mô hình hoạt động tốt nhất sử dụng tỷ lệ bỏ qua K=10000 trong cả huấn luyện và kiểm tra. Ngoài ra, khi tỷ lệ bỏ qua huấn luyện được cố định, việc sử dụng tỷ lệ bỏ qua lớn hơn hoặc bằng nhau trong quá trình kiểm tra luôn tạo ra kết quả tốt hơn, cho thấy rằng tỷ lệ bỏ qua có thể được ngoại suy hiệu quả trong thời gian suy luận để đọc hiệu quả hơn. Hơn nữa, với tỷ lệ suy luận cố định, việc sử dụng tỷ lệ bỏ qua huấn luyện tích cực hơn luôn dẫn đến cải thiện. Điều này tương phản với các phát hiện trong Phần 4, nơi tỷ lệ bỏ qua vừa phải chứng minh hiệu quả hơn. Đối với tiền huấn luyện, tập trung vào dự đoán token tiếp theo, nhiệm vụ vẫn vốn dĩ cục bộ, và cấu trúc của ngữ cảnh dài cung cấp lợi ích hạn chế. Tuy nhiên, trong QA ngữ cảnh dài, mục tiêu chuyển hướng việc cho phép mô hình xử lý các tài liệu cực dài chứa nhiều phần mang câu trả lời và yếu tố gây phân tâm. Đọc-bỏ qua giảm thiểu sự tham gia dư thừa với các đoạn văn có liên quan tương tự, do đó loại bỏ nhu cầu người dùng phải lọc ngữ cảnh thủ công. Phương pháp này, kết hợp với mất mát động, đạt được hiệu ứng tương tự như lọc tức thì.

## 6. Kết luận

Trong bài báo này, chúng tôi giới thiệu một chiến lược đọc truy cập ngẫu nhiên được thiết kế để hiểu ngữ cảnh dài hiệu quả. Phương pháp chủ yếu sử dụng mất mát theo token được tổng hợp trong mỗi cửa sổ đọc mô hình, cho phép máy chủ dữ liệu xác định số lượng token cần bỏ qua và thực hiện quá trình bỏ qua. Để nâng cao việc ra quyết định cho việc bỏ qua và cung cấp cho mô hình một ngữ cảnh phong phú hơn, chúng tôi tích hợp một mô-đun bộ nhớ bổ sung. Trong các thí nghiệm, chúng tôi áp dụng phương pháp của mình trên mô hình Transformer và đánh giá mô hình truy cập ngẫu nhiên được huấn luyện của chúng tôi thông qua tiền huấn luyện ngữ cảnh dài, tinh chỉnh, và trả lời câu hỏi. Kết quả thí nghiệm xác nhận những cải thiện đáng kể về hiệu suất và hiệu quả của cơ chế bỏ qua đơn giản được đề xuất của chúng tôi so với truy cập tuần tự truyền thống. Hơn nữa, những kiểm tra này chứng minh việc thích ứng nhanh chóng của mô hình văn bản ngắn với các ứng dụng ngữ cảnh dài, và những lợi ích tương hỗ của việc tích hợp mô-đun bộ nhớ và cho phép đọc truy cập ngẫu nhiên. Nghiên cứu tương lai có thể được hưởng lợi từ việc khám phá cơ chế bỏ qua tinh vi hơn và kiểm tra sự tương tác giữa cơ chế bỏ qua và mô-đun bộ nhớ.

## Tài liệu tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên như bản gốc vì đây là thông tin học thuật chính xác]
